<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 11 Aug 2024 12:27:40 GMT</lastBuildDate>
    <item>
      <title>批量数据的 SGD 优化器设置</title>
      <link>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</link>
      <description><![CDATA[我是 ML 的新手，刚刚开始学习。我正在学习 Joh Krohn 的数学 ML 入门课程。课程解释得很清楚，但有一件事让我很困惑。在这个任务中 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynb 我们使用了 torch.optim.SGD torch SGD，它运行了所有示例数据。
optimizer = torch.optim.SGD([m,b], lr = 0.01)
epochs = 999
for epoch in range(epochs): 

optimizer.zero_grad() # 将梯度重置为零；否则它们会累积

yhats = 回归（xs，m，b）# 步骤 1
C = mse（yhats，ys）# 步骤 2

C.backward() # 步骤 3

optimizer.step() # 步骤 4

在第二个练习中，我们进行了学习率调度 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/learning-rate-scheduling.ipynb
有 8.000.000 个数据点，因此数据被设置为批处理，并且代码在这些样本上轮流运行，而不是在所有数据上按时期运行。然而，这不是用 torch.optim.SGD 完成的，而是在代码上显示以查看数学是如何工作的。我正在努力用 torch.optim.SGD 运行它。如何编写代码来运行它，而不是像下面这样编写大型数学方程式，其中已经创建了所有方程式，例如梯度、theta：
n = 8000000
x = torch.linspace(0., 8., n)
y = -0.5*x + 2 + torch.normal(mean=torch.zeros(n), std=1)
indices = np.random.choice(n, size=2000, replace=False)
gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 
lr = 0.01
new_theta = theta - lr*gradient
C = mse(regression(x[batch_indices], m, b), y[batch_indices])
b.requires_grad_()
m.requires_grad_()

def return(my_x, my_m, my_b):
return my_m*my_x + my_b

m = torch.tensor([0.9]).requires_grad_()
b = torch.tensor([0.1]).requires_grad_()

batch_size = 32 # 模型超参数
batch_indices = np.random.choice(n, size=batch_size, replace=False)
yhat = return(x[batch_indices], m, b)

yhat = return(x[batch_indices], m, b)

def mse(my_yhat, my_y): 
sigma = torch.sum((my_yhat - my_y)**2)
return sigma/len(my_y)

C = mse(yhat, y[batch_indices])

C.backward()
m.grad
b.grad

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T

theta = torch.tensor([[b, m]]).T 

lr = 0.01
new_theta = theta - lr*gradient
new_theta

b = new_theta[0]
m = new_theta[1]

C = mse(regression(x[batch_indices], m, b), y[batch_indices])

rounds = 100 

for r in range(rounds): 

# 这个采样步骤很慢；稍后我们将介绍更快的批量采样： 
batch_indices = np.random.choice(n, size=batch_size, replace=False)

yhat = return(x[batch_indices], m, b) # 步骤 1
C = mse(yhat, y[batch_indices]) # 步骤 2

C.backward() # 步骤 3

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 

new_theta = theta - lr*gradient # 步骤 4

b = new_theta[0].requires_grad_()
m = new_theta[1].requires_grad_()
]]></description>
      <guid>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</guid>
      <pubDate>Sun, 11 Aug 2024 11:20:48 GMT</pubDate>
    </item>
    <item>
      <title>解决自动标记（优化）+分类问题</title>
      <link>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</link>
      <description><![CDATA[我知道这不太侧重于编程，但我不知道还有什么地方可以问这个问题。这更多的是关于方法而不是技术问题。
上下文
我有一个优化 + 分类任务。因此，本质上，我的数据具有以下列：
[&#39;Model ID&#39;, &#39;Q&#39;, &#39;refinement&#39;, &#39;avg_time&#39;, &#39;lattice&#39;, &#39;radius&#39;]（还有更多，但为了简洁起见，我们只保留这些）

Model ID：代表“设计”，每个 Model ID 将有多行

Q：这是目标变量

refinement：这是一个设置变量；它可以取 1-8 的值，这直接影响 Q，（模型 ID，细化）对是唯一的。因此，模型 ID 将具有多行，细化程度各不相同

avg_time：这是模拟完成所需的时间，仅受细化的影响，细化程度越高，所需的时间越长。此值与设计无关，它仅取决于细化，因此特定细化的所有设计都具有相同的时间。

lattice 和 radius：这些代表“设计”，本质上更改它们将更改 Q


数据集
我的数据集来自随机设计的模拟。对于每个设计，我们可以有以下行为：

持续增加（每次细化时的 Q 值高于上一个细化级别）
持续减少（每次细化时的 Q 值低于上一个细化级别）
之字形，其中 Q 值遵循此当前模式（高，低，高，低）或（低，高，低，高）
碗形，其中 Q 值遵循此当前模式：（高，低，低，高）
梯形，其中 Q 值遵循此当前模式：（低，高，高，低）
我有代码可以检测这些形状并返回布尔值：

def is_zigzag(q_values)

def is_bowl(q_values)

def is_trapezoid(q_values)

数据集中细化的值范围是 2-5，但细化可以取 1-8 的值。
任务
因此，我试图实现的是自动标记每个模型 ID（通过对行进行分组）和最佳细化值（范围为 1-8），以最大化 Q 的变化（增量越大越好）并最小化所花费的时间（越低越好）。问题是由于数据是在细化级别 5 处切割的，所以我想到使用概率方法（例如 MLE）来创建未来细化的预期变化和预期所花费的时间。但我似乎无法“调整”它，所以它很有用。获得预期值后，我需要一个成本函数来计算（优化）Q 的回报与完成该细化级别所花费的增加时间的比较。
在下一部分中，我将开发一个分类器，它将采用设计和最佳细化。从理论上讲，它应该可以预测未见过的设计的最佳细化级别
我很感激任何有关解决这个问题的指导/帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</guid>
      <pubDate>Sun, 11 Aug 2024 11:01:40 GMT</pubDate>
    </item>
    <item>
      <title>处理缺失数据并建立具有不完整信息的预测模型？</title>
      <link>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</link>
      <description><![CDATA[我正在为涉及 20 个影响点的供水网络开发一个预测模型。但是，我只有这 20 个点中的 10 个的历史数据。
我想知道如何在这个不完整的数据集下构建预测模型。具体来说：
我可以使用哪些方法来处理剩余 10 个点的缺失数据？在这种情况下，是否有任何标准技术或最佳实践来处理缺失数据？
我如何有效地将我拥有的 10 个点的数据合并到模型中？我可以采用哪些策略来确保有效利用可用数据进行准确预测？
是否有特定的技术或模型可以帮助在数据不完整的情况下进行预测？我对可以有效管理和利用不完整数据的方法感兴趣。
&quot;我还没有具体的方法。&quot;]]></description>
      <guid>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</guid>
      <pubDate>Sun, 11 Aug 2024 10:43:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 hub.KerasLayer 使用 tf.keras.sequential 制作深度学习模型时出错</title>
      <link>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</link>
      <description><![CDATA[我是刚开始使用 keras 的，这里我遇到了一些问题：
创建一个构建 Keras 模型的函数
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
print(&quot;Building model with:&quot;, MODEL_URL)

model = tf.keras.Sequential([
hub.KerasLayer(MODEL_URL), # 第 1 层（输入层）
tf.keras.layers.Dense(units=OUTPUT_SHAPE, 
activation=&quot;softmax&quot;) # 第 2 层（输出层）
])

# 编译模型
model.compile(
loss=tf.keras.losses.CategoricalCrossentropy(), # 我们的模型想要减少这个（它的猜测有多错误）
optimizer=tf.keras.optimizers.Adam()，# 一个朋友告诉我们的模型如何改进它的猜测
metrics=[&quot;accuracy&quot;] # 我们希望这个数字上升
)

# 构建模型
model.build(INPUT_SHAPE) # 让模型知道它将获得什么样的输入

返回模型

我有上面的函数，当我运行下面的其他程序时
model = create_model()
model.summary()

它会产生一些错误
TypeError：添加的层必须是 Layer 类的实例。收到：layer= 类型为 &lt;class &#39;keras.src.layers.core.dense.Dense&#39;&gt;。
所以我错了什么，我一直在搜索这个，但问题和我得到的不一样]]></description>
      <guid>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</guid>
      <pubDate>Sun, 11 Aug 2024 08:05:45 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 CV-k 折叠预测“是”“否”的均方误差？</title>
      <link>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</link>
      <description><![CDATA[为了根据满意度指数和参与度指数预测客户是否会购买产品，我们使用了 k 近邻法。
如何用 4 倍交叉验证过程评估预测均方误差？它不需要在 R 中。我只需要如何计算它的理论。
我知道如何用数字计算，但不知道如何用字符串计算]]></description>
      <guid>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</guid>
      <pubDate>Sun, 11 Aug 2024 05:16:27 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 SGDRegressor 模型性能</title>
      <link>https://stackoverflow.com/questions/78857401/how-to-improve-sgdregressor-model-performance</link>
      <description><![CDATA[我正在做一个个人项目，比较 OLS 模型和 SGDRessor 模型之间的模型性能。OLS 模型并不完美，但运行良好。SGDR 模型预测偏差很大。我检查了迭代过程中的成本降低情况。结果表明学习率太高。然而，降低学习率似乎会使成本降低变得更糟。
我是机器学习的新手，我不知道如何改进 SGDR 模型。任何建议都将不胜感激。
这是我的代码：Google Colab 链接。您可以重点关注“梯度下降方法”部分和“排除 SGDR 故障”。
提前感谢您的时间和帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78857401/how-to-improve-sgdregressor-model-performance</guid>
      <pubDate>Sun, 11 Aug 2024 02:34:35 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard 中的 add_hparams() 函数无法正常工作</title>
      <link>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</link>
      <description><![CDATA[我试图向此 SummaryWriter 添加指标，但不起作用。
我正在使用 SummaryWriter 的 add_hparams() 函数，其详细信息可在此处找到：https://pytorch.org/docs/stable/tensorboard.html。
我这样做：
 writer = SummaryWriter(f&#39;runs/lstm_experiment_final&#39;)

for e in tqdm(range(num_epochs)):
tr_loss, tr_f1, tr_precision, tr_recall = training_loop(model, train_dataloader, loss_function, optimizer, e, writer)
val_loss, val_f1, val_precision, val_recall = validation_loop(model, test_dataloader, loss_function, e, writer)

metric_dict = {&#39;Loss/train&#39;: tr_loss, &#39;Loss/valid&#39;: val_loss,
&#39;F1/train&#39;: tr_f1, &#39;F1/valid&#39;: val_f1,
&#39;Precision/train&#39;: tr_precision, &#39;Precision/valid&#39;: val_precision,
&#39;Recall/train&#39;: tr_recall, &#39;Recall/valid&#39;: val_recall}
writer.add_hparams(best_params, metric_dict, global_step=num_epochs-1)
writer.close()

这就是正在发生的事情。
在此处输入图片描述
换句话说，超参数确实记录在 TensorBoard 上，但度量值却没有记录。
我希望有人已经看到我的问题并知道如何解决这个问题。
提前谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</guid>
      <pubDate>Sun, 11 Aug 2024 00:21:56 GMT</pubDate>
    </item>
    <item>
      <title>启用 GPU 2024-OSError：[WinError 127] 找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</link>
      <description><![CDATA[我正在尝试启用 GPU 进行机器学习，但遇到了这个问题：

OSError：[WinError 127] 找不到指定的程序。错误
加载
“C:\Users\name\anaconda3\Lib\site-packages\torch\lib\c10_cuda.dll”
或其依赖项之一。

目前，我正在使用配备 NVIDIA 3050 GPU 的 Windows 笔记本电脑，以 Python 作为我的主要开发语言。

注意到该文件存在，因为 torch 库是使用 pip 直接下载的。因此，从技术上讲，此错误不应该发生。

最初，我收到了类似的错误“[WinError 126] 找不到指定的过程。”但我已经更新并安装了 VisualStudio 2022 的 C/C++ 编译器，它解决了 [WinError 126}，但我得到的却是 [WinError 127]。
要安装和启用 GPU，我将按照此教程执行以下步骤。

安装 VisualStudio -&gt; 全部下载

安装 Pytorch (pip) -&gt; CUDA 12.4

安装 CUDA 工具包 (12.6)

下载 cuDNN“下载 cuDNN v8.9.7 (2023 年 12 月 5 日)，适用于 CUDA 12.x”

将 cuDNN 的内容按照各自的文件夹名称粘贴到“NVIDIA GPU 计算工具包”中。


已验证已安装的 CUDA 已添加到环境中

我也按照这个最近更新的视频的建议下载 C/C++ 编译器。然而，这并没有解决我的问题。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</guid>
      <pubDate>Sat, 10 Aug 2024 16:59:08 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    <item>
      <title>构建 ML 模型时如何选择合适的标签</title>
      <link>https://stackoverflow.com/questions/78854998/how-to-choose-the-appropriate-label-when-building-a-ml-model</link>
      <description><![CDATA[我正在尝试为特定任务训练模型。
这里有一个简单的描述：
image1
image2
以下是两个不同数据集的屏幕截图：图 1 中的数据顺序正确，没有错误，也没有缺失数据。另一方面，图 2 中的数据是无序的，包含噪音，并且有缺失数据。
我想要训练一个模型，当给出图 2 中所示类型的数据作为输入时，该模型可以返回图 1 中所示类型的数据。
我尝试使用 RF 和 CNN 模型，但结果并没有像我预期的那样发展。我在想这可能是由于标签选择不正确造成的。
其实从图1中，很容易就能发现其中的联系。
例如，
1 2 3 4
A A-1 B B-1
A2 A2-1 B2 B2-1
A3 A3-1 B3 B3-1
A4 A4-1 B4 B4-1
图1这类数据中，A=A2-1，A2=A3-1\
因此，我希望模型能够学习到这种关系，然后在乱序的数据（图2）中找出正确的顺序。一旦确定了一个正确的序列，就可以通过递归得到正确且唯一的顺序。由于行与行之间是相互对应的（即 A A-1 B B-1 固定在同一行），一旦正确确定了一列的序列，整个序列也就正确确定了。
所以我尝试使用模型来解决这个问题。该模型能够运行并学到了一些东西，但没有学到任何有用的东西。我开始意识到问题可能在于标签选择。（事实上，这个问题可能可以用算法来解决，但我想用机器学习来实现它。）
如何选择标签并分割训练集和验证集？]]></description>
      <guid>https://stackoverflow.com/questions/78854998/how-to-choose-the-appropriate-label-when-building-a-ml-model</guid>
      <pubDate>Sat, 10 Aug 2024 01:22:04 GMT</pubDate>
    </item>
    <item>
      <title>在扩展中访问 NetLogo 扩展</title>
      <link>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</link>
      <description><![CDATA[我正在尝试开发一个 NetLogo 扩展来与不同的 LLMS（在线、离线）进行通信。LLM 调用返回 JSON 格式的字符串。我想解析 JSON 并将其转换为嵌套的 TABLE 对象，以访问 NetLogo Table 扩展。
是否有办法让一个扩展访问和使用另一个扩展中的类？]]></description>
      <guid>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</guid>
      <pubDate>Fri, 09 Aug 2024 03:21:02 GMT</pubDate>
    </item>
    <item>
      <title>在依赖项解析中强制使用标签</title>
      <link>https://stackoverflow.com/questions/78849363/enforce-labels-in-dependency-parsing</link>
      <description><![CDATA[我正在使用少量数据在 spaCy 中训练依赖解析器，如果我可以强制解析器

每个句子只有一个词根
将相同的标签应用于相同的词干/词形（例如 u 应始终为 cc）。

有办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78849363/enforce-labels-in-dependency-parsing</guid>
      <pubDate>Thu, 08 Aug 2024 15:56:21 GMT</pubDate>
    </item>
    <item>
      <title>我正在训练一个 VAE，但在我的批次中，我的 KLD 术语在执行几个步骤后就消失了</title>
      <link>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</link>
      <description><![CDATA[这是我的 VAE 损失代码：
def loss_function(x, x_hat, mean, logvar, beta=1.0):
    criterion = nn.MSELoss(reduction=&quot;mean&quot;)
    rebuilding_loss = criterion(x_hat, x)

    KLD = - 0.5 * torch.mean(1+ logvar - mean.pow(2) - logvar.exp())
    print(f&quot;KLD = {KLD}&quot;)
    return rebuilding_loss + KLD*beta

这是我的 VAE（简化版）：
class VariationalAutoEncoder(nn.Module):
def __init__(self, latent_shape):
super(VariationalAutoEncoder, self).__init__()

self.encoder = nn.Sequential(

nn.Linear(18,5184), # new
nn.LeakyReLU(0.2),
nn.Linear(5184,128), # new

)

# 潜在均值和方差对数 

self.mean_layer = nn.Linear(128, latent_shape)
self.logvar_layer = nn.Linear(128,latent_shape)

# 解码器
self.decoder = nn.Sequential(
nn.Linear(latent_shape,128),

nn.LeakyReLU(0.2),
nn.Linear(128,5184),

nn.LeakyReLU(0.2),
nn.Linear(5184,18), # new
nn.Sigmoid(),

)
def encode(self, x):
x = self.encoder(x)
mean, logvar = self.mean_layer(x), self.logvar_layer(x)
return mean, logvar

def reparameterization(self, mean, logvar):
std = torch.exp(0.5 * logvar)
epsilon = torch.randn_like(std).to(device) 
z = mean + std*epsilon
return z

def decrypt(self, z):
return self.decoder(z)

def forward(self, x):
mean, logvar = self.encode(x)
z = self.reparameterization(mean, logvar)

x_hat = self.decode(z)
return x_hat, mean, logvar


在我使用 beta 值后，KLD 的值会达到 10^-7 的数量级10^16。我不确定为什么会这样。我该怎么办？
我目前的超参数是：Adm Optimizer 的权重衰减为 10，lr 为 0.01，ReduceLROnPlateau 调度程序的耐心为 3，因子为 0.5。我在代码中使用梯度裁剪，max_norm 为 0.5，训练 25 个时期。
潜在形状为 10。
我尝试使用较小的值进行 beta 退火和循环退火，甚至使用 10^16 作为 beta 值的 beta KLD，但 KLD 项仍然消失为 10^-8。这就像它在我的任务上随机运行一样。我尝试添加一个卷积层，因为输入是一张大小约为 18 x 18 的图像。没有变化。]]></description>
      <guid>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</guid>
      <pubDate>Thu, 08 Aug 2024 12:10:40 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量</title>
      <link>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</link>
      <description><![CDATA[我无法加载我的模型，它一直显示错误
ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]
这是我的代码
image_generator = ImageDataGenerator(
rescale=1./255,
rotation_range=20,
zoom_range=0.2,
width_shift_range=0.2,
height_shift_range=0.2,
Horizo​​ntal_flip=True,
validation_split=0.2
)

train_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;training&#39;
)

validation_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;validation&#39;
)

# 加载数据集中子文件夹中的 (num_classes) 类
num_classes = len(train_dataset.class_indices)

from tensorflow.keras.applications.mobilenet import MobileNet

# 加载 MobileNet 模型
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
include_top=False,
weights=&#39;imagenet&#39;)

pre_trained_model.summary()

# 打印数据集信息以供调试
print(f&quot;训练数据集形状：{train_dataset.image_shape}&quot;)
print(f&quot;验证数据集形状：{validation_dataset.image_shape}&quot;)

pre_trained_model.trainable = False

# 为预训练模型添加自定义层
model = tf.keras.Sequential([
pre_trained_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(1024,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;) 
])

# 编译模型
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

batch=40
history = model.fit(train_dataset,
validation_data=validation_dataset,
epochs=20,
steps_per_epoch = train_dataset.samples//batch,
validation_steps =validation_dataset.samples//batch,
verbose = 1
)

# 加载模型
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;

# 加载模型，确保必要时已编译
loaded_model = tf.keras.models.load_model(model_save_path) 

# 现在您可以根据需要修改已加载的模型
# 例如，如果您想提取子模型：
input_layer_index = 0 # 替换为实际索引
dense_2_index = 3 # 替换为实际索引
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
outputs=loaded_model.layers[dense_2_index].output)

# 检查已加载模型的配置
for i, layer in enumerate(loaded_model.layers):
print(f&quot;Layer {i}: {layer.name} - 输入形状：{layer.input_shape} - 输出形状：{layer.output_shape}&quot;)

print(&quot;已成功加载修订模型。&quot;)

我尝试加载模型，并希望它能够加载以进行测试]]></description>
      <guid>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</guid>
      <pubDate>Thu, 08 Aug 2024 07:06:54 GMT</pubDate>
    </item>
    <item>
      <title>将自定义模型和配置注册到 AutoModel 和 AutoConfig</title>
      <link>https://stackoverflow.com/questions/77428197/registering-custom-model-and-config-to-automodel-and-autoconfig</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77428197/registering-custom-model-and-config-to-automodel-and-autoconfig</guid>
      <pubDate>Mon, 06 Nov 2023 00:48:18 GMT</pubDate>
    </item>
    </channel>
</rss>