<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 17 Jan 2024 18:17:56 GMT</lastBuildDate>
    <item>
      <title>召回分数 != 在 Scikit-Learn 中使用 fusion_matrix 手动计算</title>
      <link>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix-in-scikit-learn</link>
      <description><![CDATA[我目前正在使用 Scikit-Learn 进行机器学习项目，我遇到了一个问题，即使用recall_score(y, y_pred) 获得的召回分数与使用confusion_matrix 手动计算的值不匹配。
不仅如此，召回率与特异性（我已经计算过）的值完全相同，我也在下面手动计算了该值。
代码：
这是我正在使用的相关代码：
recall = recall_score(y, y_pred) &lt;-- 不同分数

conf_matrix = fusion_matrix(y, y_pred)
tn, fp, fn, tp = conf_matrix.ravel()
Manual_recall = tp / (tp + fn) &lt;-- 达到这个分数
特异性 = tn / (tn + fp) &lt;-- 与上面的分数相同

这是发生这种情况的终端中打印的混淆矩阵的示例：
[[34 6]
[20 20]] 或
[[29 11]
[9 31]]
问题：
我面临的问题是，recall 和 manual_recall 不会产生相同的值，即使我期望它们匹配。
问题：
为什么recall_score和使用confusion_matrix手动计算会产生不同的召回分数结果？
更多信息...
我正在处理的数据集是一个二元分类问题。
我正在评估此函数中的多个分类器 - 该函数只是将所有经过调整的模型运行到训练数据
我正在使用recall_score 的默认阈值。
我正在计算的所有其他指标 - 准确度、精确度、ROC AUC 和 F1 - 都是不同的。
在评估模型的这一步之前，我将模型拟合到训练数据，并通过网格搜索找到要使用的最佳参数。然后，我似乎在相同的训练数据上再次使用这些模型，但这次进行了 10 倍交叉验证；然后在测试数据上进行测试。虽然这并不理想，但我认为它不会导致这个问题......只是想找出这个错误。

我尝试打印出手动计算结果和recall_score。
我已尝试确定混淆表是否准确（确实如此）。
实际上只是recall_score。
]]></description>
      <guid>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix-in-scikit-learn</guid>
      <pubDate>Wed, 17 Jan 2024 18:08:23 GMT</pubDate>
    </item>
    <item>
      <title>将大型数据集分类为不同的类别[关闭]</title>
      <link>https://stackoverflow.com/questions/77834092/categorizing-a-large-dataset-into-different-classes</link>
      <description><![CDATA[我有一个数据集，其中有月球陨石坑的直径。我需要将它们分为不同的类别。（使用 python）
我的数据中的列名称是 ID、纬度、经度、直径和深度。用空格分隔的值
例如：有一个直径为 980m 的陨石坑，则它属于直径小于 1 km 的陨石坑类别（让我们将该类别命名为 SET1），类似地，有一个直径为 40 km 的陨石坑，则它是属于直径小于50公里但大于30公里的陨石坑类别（让我们将其命名为SETX）。
我需要创建这样的类别并将所有这些陨石坑分类到其中。
我还需要计算每个此类类别中的陨石坑数量。
另请注意，我的数据中有近 80 万个陨石坑。
我需要想法或解决方案来解决上述问题。]]></description>
      <guid>https://stackoverflow.com/questions/77834092/categorizing-a-large-dataset-into-different-classes</guid>
      <pubDate>Wed, 17 Jan 2024 16:43:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 sagemaker 使用开放搜索索引</title>
      <link>https://stackoverflow.com/questions/77833977/consume-opensearch-indices-with-sagemaker</link>
      <description><![CDATA[我需要从 AWS OpenSearch 中的索引中提取数据以进行聚合并分析异常/变化。
PS：我认为逻辑是有效的......
我正在考虑使用 Sagemaker + OpenSearch。逻辑如下：Sagemaker 连接到 Opensearch，从索引中获取实时数据，对其进行处理并通过 Sagemaker 端点使其可用，但出现了以下问题：
使用“OpenSearch 服务 AI 连接器&quot;并使用下面的代码？
连接示例：
from opensearchpy import OpenSearch、RequestsHttpConnection、AWSV4SignerAuth
导入boto3

凭证 = boto3.Session().get_credentials()
主机 = OPENSEARCH_HOSTNAME[“*.es.amazon.com”]

auth = AWSV4SignerAuth(凭证, &#39;us-east-1&#39;, &#39;es&#39;)
# 使用opensearch Python客户端连接opensearch集群。

os_client = OpenSearch(hosts=[{&#39;主机&#39;: 主机, &#39;端口&#39;: 443}],
 http_auth = 身份验证，
 use_ssl=真，
 verify_certs=真，
 connection_class=RequestsHttpConnection,
 超时=30
 ）

打印（os_client）

打印（os_client.ping（））
]]></description>
      <guid>https://stackoverflow.com/questions/77833977/consume-opensearch-indices-with-sagemaker</guid>
      <pubDate>Wed, 17 Jan 2024 16:26:06 GMT</pubDate>
    </item>
    <item>
      <title>训练时张量流损失不会超过 100 [关闭]</title>
      <link>https://stackoverflow.com/questions/77833956/tensorflow-loss-doesnt-get-past-100-when-training</link>
      <description><![CDATA[我有一个用于股票预测人工智能的张量流代码，由于某种原因，我无法让损失低于 100。我已经尝试了各种方法，例如增加模型大小、减少、学习率调度、提前停止、密集、调整学习速率和纪元。但我还是没能突破 100 的亏损大关。有人可以告诉我我是否做错了什么并帮助我解决问题。
这是代码：
导入 csv
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集，批量标准化，丢弃
从tensorflow.keras.optimizers导入Adam
从tensorflow.keras.callbacks导入ReduceLROnPlateau，EarlyStopping

# 用于训练或评估的用户输入
choice = input(&#39;火车或 eva: &#39;)

如果选择==“火车”：
    # 初始化列表来存储输入数组和输出数组
    输入数组 = []
    输出数组 = []

    # 打开 CSV 文件和格式。
    以 open(r&quot;C:\Users\eldo7\Downloads\HistoricalData_1698728285806.csv&quot;, newline=&#39;&#39;) 作为 csvfile：
        读者 = csv.DictReader(csvfile)

        收盘价 = []

        对于反转行（列表（读者））：
            close_price_str = row.get(“收盘/最后”)
            如果 close_price_str 不为 None：
                close_price = float(close_price_str.strip(&#39;$&#39;).replace(&#39;,&#39;, &#39;&#39;))
                close_prices.append(close_price)

                # 如果我们收集了 6 个值（5 个用于输入，1 个用于输出），将它们添加到各自的数组中
                如果 len(close_prices) == 6:
                    input_arrays.append(close_prices[:5])
                    output_array.append(close_prices[-1])
                    close_prices.pop(0) # 删除第一个值以保持5个连续值

    # 打乱数据同时保持对的顺序
    索引 = 列表（范围（len（input_arrays）））
    np.random.shuffle(索引)

    shuffled_input_arrays = [input_arrays[i] 对于索引中的 i]
    shuffled_output_array = [output_array[i] 对于索引中的 i]

    # 将输入和输出数据转换为NumPy数组
    shuffled_input_arrays = np.array(input_arrays, dtype=np.float32)
    shuffled_output_array = np.array(output_array, dtype=np.float32)

    # 将数据分为训练集和验证集
    分割比率 = 0.8
    split_index = int(split_ratio * len(shuffled_input_arrays))

    train_input = shuffled_input_arrays[:split_index]
    train_output = shuffled_output_array[:split_index]
    val_input = shuffled_input_arrays[split_index:]
    val_output = shuffled_output_array[split_index:]

    自定义学习率 = 0.0001

    优化器 = Adam(learning_rate=custom_learning_rate)

    模型=顺序（[
        密集（128，激活=&#39;relu&#39;，input_shape=（5，）），
        批量归一化(),
        辍学率（0.3），

        密集（64，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（32，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（16，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（8，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集(1)
    ]）

    # 使用自定义优化器和 MSE 作为损失函数来编译模型
    model.compile（优化器=优化器，损失=&#39;mean_squared_error&#39;，指标=[&#39;mae&#39;]）

    模型.summary()

    # 学习率调度
    reduce_lr =ReduceLROnPlateau(监视器=&#39;val_loss&#39;,因子=0.5,耐心=100,min_lr=0.00001,min_delta=100)

    # 提前停止回调
    Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 100，restore_best_weights = True）

    历史= model.fit（train_input，train_output，epochs = 3000，batch_size = 64，validation_data =（val_input，val_output），callbacks = [reduce_lr，early_stopping]）

    model.save(“stockPredictorV2.h5”)

    # 绘制训练和验证指标
    将 matplotlib.pyplot 导入为 plt

    plt.plot(history.history[&#39;loss&#39;], label=&#39;训练损失&#39;)
    plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证损失&#39;)
    plt.图例()
    plt.show()

elif 选择==“eva”：
    模型 = keras.models.load_model(“stockPredictorV2.h5”)
    # 创建自定义输入数组
    custom_input = np.array([193.60, 193.05, 193.15, 193.58, 192.53], dtype=np.float32)

    # 使用自定义输入进行预测
    自定义输入 = 自定义输入.reshape(1, 5)
    自定义预测 = model.predict(自定义输入)

    print(&quot;自定义输入：&quot;, custom_input)
    # 打印自定义预测
    print(&quot;自定义预测：&quot;, custom_prediction)
]]></description>
      <guid>https://stackoverflow.com/questions/77833956/tensorflow-loss-doesnt-get-past-100-when-training</guid>
      <pubDate>Wed, 17 Jan 2024 16:22:15 GMT</pubDate>
    </item>
    <item>
      <title>Mit App Inventor：ML4K 的 API 集成存在问题 [已关闭]</title>
      <link>https://stackoverflow.com/questions/77833902/mit-app-inventor-troubles-with-ml4ks-api-integration</link>
      <description><![CDATA[ML4K Api 集成问题。
我复制了“找到它”项目，集成 API，下载 ml4k.axi 文件夹，但无论如何，当应用程序运行时它会出现错误。
我相信还是 API 问题，因为代码是从 ML4K 复制的
第一个错误第二个错误]]></description>
      <guid>https://stackoverflow.com/questions/77833902/mit-app-inventor-troubles-with-ml4ks-api-integration</guid>
      <pubDate>Wed, 17 Jan 2024 16:13:33 GMT</pubDate>
    </item>
    <item>
      <title>我目前在 google colab 上运行 FAISS 并且遇到了这个问题</title>
      <link>https://stackoverflow.com/questions/77833829/i-am-currently-running-faiss-on-google-colab-and-i-encountered-this-problem</link>
      <description><![CDATA[从 langchain.vectorstores 导入 FAISS
vectorStore = FAISS.from_texts(块，嵌入)

基本上我正在尝试在我的 mac (google-colab) 上运行它。 (https://github.com/Selim321/Langchain-QnA -falcon/blob/main/Langchain_QnA_falcon.ipynb)
并且 google colab 会无限期地运行。难道我做错了什么？给定的代码有优化吗？我已经导入了faiss_cpu、langchain和sentence_transformers。]]></description>
      <guid>https://stackoverflow.com/questions/77833829/i-am-currently-running-faiss-on-google-colab-and-i-encountered-this-problem</guid>
      <pubDate>Wed, 17 Jan 2024 16:03:00 GMT</pubDate>
    </item>
    <item>
      <title>人脸图像检测CNN [关闭]</title>
      <link>https://stackoverflow.com/questions/77833016/face-image-detection-cnn</link>
      <description><![CDATA[我正在尝试训练一个简单的图像分类模型，用于根据面部图像对年龄进行分类。我收到这样的错误：“RuntimeError：权重张量应该为所有 64 个类定义，或者没有类，但得到形状的权重张量：[99]”。由于某种原因，我的自定义图像文件夹类将父文件夹 face_age 作为类别之一，我不确定为什么。我怀疑这可能与尺寸不兼容有关。
这是我的 Kaggle 笔记本：
https://www.kaggle.com/code/gustavo9898/face -年龄检测]]></description>
      <guid>https://stackoverflow.com/questions/77833016/face-image-detection-cnn</guid>
      <pubDate>Wed, 17 Jan 2024 13:58:48 GMT</pubDate>
    </item>
    <item>
      <title>如何训练一个接受十六进制代码的模型，并且它应该输出给定数量的对比或相似的十六进制代码</title>
      <link>https://stackoverflow.com/questions/77830997/how-do-i-train-a-model-that-takes-in-a-hex-code-and-it-should-output-a-given-num</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77830997/how-do-i-train-a-model-that-takes-in-a-hex-code-and-it-should-output-a-given-num</guid>
      <pubDate>Wed, 17 Jan 2024 08:40:22 GMT</pubDate>
    </item>
    <item>
      <title>查找输入列中的值的优化组合，以生成输出列中的值[关闭]</title>
      <link>https://stackoverflow.com/questions/77830783/find-optimized-combinations-of-values-in-input-columns-that-produces-the-values</link>
      <description><![CDATA[我有多个生成输出列的输入列。我想找到输入列中的值的优化组合，以生成输出列中的值。
例如：像这个表所以输出将是这样的：
抄送-&gt;输出4
AA，XX -&gt;输出1
等等。
我正在尝试使用关联规则学习来查找多个输入列和一个输出列（所有文本列）之间的规则或关系。我尝试过 pycaret.arules 但在 pycaret==2.3.10 之后它不可用，并且此版本或以下版本与 Python 3.10 不兼容。所以，我不能使用它。
有没有其他方法可以解决这个问题。我尝试过决策树，但我想要一些完全适合我的数据而无需任何额外节点的东西。
到目前为止，我已经研究了 apyori、efficient-apriori 等不同的 apriori 实现。它们都不完全符合我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/77830783/find-optimized-combinations-of-values-in-input-columns-that-produces-the-values</guid>
      <pubDate>Wed, 17 Jan 2024 08:01:36 GMT</pubDate>
    </item>
    <item>
      <title>如何仅使用组件在 azure ml Designer 中训练和部署 ml 模型？</title>
      <link>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</link>
      <description><![CDATA[我在 azure ml Designer 中创建了一个训练管道。现在，我需要通过添加用于注册和部署的组件来部署此模型。我想我可以使用“执行 python 脚本”组件来执行此操作。但是我不知道如何将“训练的最佳模型”（“调整模型超参数”组件的输出）与“执行 python 脚本”组件连接起来。那么，知道如何完成这项任务吗？我将非常感谢您的帮助。
这是我的管道：
训练管道]]></description>
      <guid>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</guid>
      <pubDate>Tue, 16 Jan 2024 17:39:49 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试使用 Tensorflow mnist 进行数字识别时遇到的问题</title>
      <link>https://stackoverflow.com/questions/77822681/problem-i-face-while-trying-number-recognition-using-tensorflow-mnist</link>
      <description><![CDATA[在我的 OpenCV 项目中，我的目标是读取数独表，将其划分为单独的单元格，并识别数字。剩下的大部分是数独解决。
我一直在识别数字部分。我决定使用 minst 数据集来训练模型。由于我无法弄清楚的原因，该程序总是识别错误的数字。我将用下面的代码进一步解释。
这是我的识别部分的代码：
(ds_train, ds_test), ds_info = tfds.load(
    &#39;姆尼斯特&#39;,
    split=[&#39;训练&#39;, &#39;测试&#39;],
    shuffle_files=真，
    as_supervised=真，
    with_info=真，
）


def Normalize_img(图像, 标签):
    &quot;&quot;&quot;标准化图像：`uint8` -&gt; `float32`。&quot;&quot;&quot;
    返回 tf.cast(image, tf.float32) / 255., 标签


ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

模型 = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(10)
]）
模型.编译(
    优化器=tf.keras.optimizers.legacy.Adam(0.001),
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    指标=[tf.keras.metrics.SparseCategoricalAccuracy()],
）

模型.拟合(
    ds_火车，
    纪元=6，
    验证数据=ds_test，
）
样本 = cv.cvtColor(样本, cv.COLOR_BGR2GRAY)
样本 = cv.resize(样本, (28, 28))

视图图像（样本）
样本 = np.invert(np.array([样本]))

预测 = model.predict(样本)
打印（np.argmax（预测））

样本 = np.invert(np.array([样本])).reshape((28, 28, 1))
cv.imshow(“测试”, 样本)
CV.waitKey(0)

这是我处理之前和之后的图像。
我的图像的第一种形式：

重新缩放至 28x28 后的图像：

模型不断预测我的图像为 3。我不知道这背后的原因。
我尝试实现一个解决方案，其中我训练了尺寸为 300x300 的模型，并将图像转换为 300x300 而不是 28x28，但结果没有改变。
我不知道我是否实施了错误的解决方案，但我希望您能帮助我。
编辑
大小 = 300
(ds_train, ds_test), ds_info = tfds.load(
    &#39;姆尼斯特&#39;,
    split=[&#39;训练&#39;, &#39;测试&#39;],
    shuffle_files=真，
    as_supervised=真，
    with_info=真，
）


def resize_img(图像, 标签):
    返回 tf.image.resize(图像, (尺寸, 尺寸)), 标签


ds_train = ds_train.map(resize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.map(resize_img, num_parallel_calls=tf.data.AUTOTUNE)


def Normalize_img(图像, 标签):
    &quot;&quot;&quot;标准化图像：`uint8` -&gt; `float32`。&quot;&quot;&quot;
    返回 tf.cast(image, tf.float32) / 255., 标签


ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

模型 = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(大小, 大小, 1)),
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(10)
]）
模型.编译(
    优化器=tf.keras.optimizers.legacy.Adam(0.001),
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    指标=[tf.keras.metrics.SparseCategoricalAccuracy()],
）

模型.拟合(
    ds_火车，
    纪元=6，
    验证数据=ds_test，
）
样本 = cv.cvtColor(样本, cv.COLOR_BGR2GRAY)
样本 = cv.resize(样本, (大小, 大小))

视图图像（样本）
样本 = np.invert(np.array([样本]))

预测 = model.predict(样本)
打印（np.argmax（预测））

对于 class_index，枚举中的概率（预测[0]）：
    print(f&#39;类 {class_index}: 概率 {prob}&#39;)

样本 = np.invert(np.array([样本])).reshape((大小, 大小, 1))
cv.imshow(“测试”, 样本)
CV.waitKey(0)

这是我编写的用于处理所有 300x300 格式图像的代码。当然，它比 28x28 慢，但我认为它会提高准确性。不幸的是，它没有改变任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/77822681/problem-i-face-while-trying-number-recognition-using-tensorflow-mnist</guid>
      <pubDate>Mon, 15 Jan 2024 22:44:03 GMT</pubDate>
    </item>
    <item>
      <title>当尝试使用tuner.search运行GridTuner类时我遇到了问题</title>
      <link>https://stackoverflow.com/questions/77821202/when-trying-to-run-gridtuner-class-using-tuner-search-%c4%b1-am-having-problem</link>
      <description><![CDATA[类 GridTuner(keras_tuner.GridSearch):
def __init__(self, 超模型, \*\*kwargs):
super().__init__(超级模型，\*\*kwargs)

    def run_Trial(自我, 审判, *args, **kwargs):
        hp = 试验.超参数
        模型 = self.hypermodel.build(hp)
        返回 self.hypermodel.fit(hp, model, *args, **kwargs)
                调谐器 = GridTuner(
                构建模型，
                目标=&#39;val_loss&#39;,
                覆盖=真，
                目录=“D:\\kaggle\\working\\hyperparameters”,
                project_name=f“driams-{有机体}-{抗菌剂}”，
）
                

                tuner.search_space_summary()



                调谐器. 搜索(
                    X_火车，
                    y_火车，
                    验证数据=（X_val，y_val），
                    批量大小=128，
                    纪元=100，
                    类别权重=类别权重，
                    回调=[提前停止(耐心=15)]
）

            best_hp =tuner.get_best_hyperparameters()[0]
            best_model =tuner.hypermodel.build(best_hp)
            best_model.summary()

我正在尝试运行有关超参数的试验，但出现以下错误：
文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\streamlit\\runtime\\scriptrunner\ \script_runner.py”，第 534 行，在 \_run_script exec(code, module.__dict__) 文件“C:\\Users\\90507\\OneDrive\\Masaüstü\\demo\\app.py”，第 266 行，在\&lt;模块\&gt;; main() 文件“C:\\Users\\90507\\OneDrive\\Masaüstü\\demo\\app.py”，第 234 行，在 maintuner.search( 文件“C:\\Users\\90507”中\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py”，第 234 行，在搜索 self.on_Trial_end(Trial)文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py”，第 338 行，在 on_Trial_end self.oracle.end_Trial(Trial) 文件 &quot;C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner \\src\\engine\\oracle.py”，第 108 行，wrapped_func ret_val = func(\*args, \*\*kwargs) ^^^^^^^^^^^^^^^^^^ ^^^ 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\tuners\\gridsearch.txt” py”，第 318 行，在 end_Trial super().end_Trial(Trial) 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages \\keras_tuner\\src\\engine\\oracle.py”，第 108 行，wrapped_func ret_val = func(\*args, \*\*kwargs) ^^^^^^^^^^^^^^^^ ^^^^^^ 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\ \oracle.py”，第 586 行，end_Trial self.\_check_consecutive_failures() 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site- packages\\keras_tuner\\src\\engine\\oracle.py”，第 543 行，在 \_check_consecutive_failures 中引发 RuntimeError( ValueError: 无法将 NumPy 数组转换为张量（不支持的对象类型 NoneType）。
我尝试查看数据集内部，检查导入、库并尝试更改代码。我正在尝试这个 https://www.kaggle.com/code/ hlysine/driams-maldi-tof-classifier 代码来制作有关 ML 的 Web 应用程序，我正在使用 Streamlit。]]></description>
      <guid>https://stackoverflow.com/questions/77821202/when-trying-to-run-gridtuner-class-using-tuner-search-%c4%b1-am-having-problem</guid>
      <pubDate>Mon, 15 Jan 2024 16:43:50 GMT</pubDate>
    </item>
    <item>
      <title>在使用pywinauto生成的多个系统中使用Wrapper_objects</title>
      <link>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</link>
      <description><![CDATA[我在使用 pywinauto 执行操作时得到了一个包装器。
现在我想在不同的系统上使用同一应用程序上的相同包装器来播放它。
可能吗？
我正在使用它创建包装对象：
from ctypes.wintypes import tagPOINT
导入 pywinauto
导入时间
时间.睡眠(2)
def get_ElementFromPoint(x,y):
     elem = pywinauto.uia_defines.IUIA().iuia.ElementFromPoint(tagPOINT(x, y))
     元素 = pywinauto.uia_element_info.UIAElementInfo(elem)
     包装器 = pywinauto.controls.uiawrapper.UIAWrapper(元素)
     返回包装器

创建的对象示例如下：


如何在不同的系统中使用此包装器来自动执行我的任务？]]></description>
      <guid>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</guid>
      <pubDate>Mon, 15 Jan 2024 11:49:34 GMT</pubDate>
    </item>
    <item>
      <title>InvalidArgumentError：图形执行错误：在推理部分（使用张量流为时间序列构建转换器）</title>
      <link>https://stackoverflow.com/questions/76786484/invalidargumenterror-graph-execution-error-in-inference-part-building-a-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76786484/invalidargumenterror-graph-execution-error-in-inference-part-building-a-trans</guid>
      <pubDate>Fri, 28 Jul 2023 09:26:31 GMT</pubDate>
    </item>
    <item>
      <title>面对强化学习的问题</title>
      <link>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</link>
      <description><![CDATA[导入健身房
从 stable_baselines3 导入 A2C

env =gym.make(&#39;LunarLander-v2&#39;, render_mode=&quot;人类&quot;)
env.reset()

模型 = A2C(“MlpPolicy”, env, verbose=1)
model.learn(total_timesteps=1000)

集数 = 10

对于范围内的 ep（剧集）：
    obs = env.reset()
    完成=假
    虽然没有完成：
        动作、_状态、_情节、_determ = model.predict(obs)
        obs、奖励、完成、info = env.step(action)
        env.render()

env.close()

我上面的代码产生以下输出：
DeprecationWarning：“np.bool8”是“np.bool_”的已弃用别名。 （已弃用 NumPy 1.24）
  如果不是 isinstance(终止, (bool, np.bool8)):
------------------------------------------------
|推出/ | |
| ep_len_mean | 89.2 | 89.2
| ep_rew_mean | -227 | -227
|时间/ | |
|帧率 | 43 | 43
|迭代| 100 | 100
|已用时间 | 11 | 11
|总时间步数 | 500 | 500
|火车/ | |
|熵损失 | -1.29 | -1.29
|解释方差 | -0.0216 | -0.0216
|学习率 | 0.0007 | 0.0007
| n_更新 | 99 | 99
|政策损失 | 2.79 | 2.79
|价值损失 | 12.3 | 12.3
------------------------------------------------
------------------------------------------------
|推出/ | |
| ep_len_mean | 107 | 107
| ep_rew_mean | -209 | -209
|时间/| |
|帧率 | 45 | 45
|迭代| 200 | 200
|已用时间 | 21 | 21
|总时间步数 | 1000 | 1000
|火车/ | |
|熵损失 | -0.864 |
|解释方差 | -0.00161 | -0.00161
|学习率 | 0.0007 | 0.0007
| n_更新 | 199 | 199
|政策损失 | -16.6 | -16.6
|价值损失 | 228 | 228

随后出现此错误：
&lt;前&gt;&lt;代码&gt;------------------------------------
回溯（最近一次调用最后一次）：
  文件“c:\Appu\Courses\Fun items\Reinforcement Learning\c1.py”，第 17 行，位于  中。
    动作、_states、_episode、_determ = model.predict(obs)
                                         ^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\base_class.py”，第 555 行，在预测中
    返回 self.policy.predict（观察、状态、episode_start、确定性）
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 346 行，在预测中
    观察，vectorized_env = self.obs_to_tensor(观察)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 260 行，位于 obs_to_tensor 中
    观察 = np.array(观察)
                  ^^^^^^^^^^^^^^^^^^^^^^^
ValueError：使用序列设置数组元素。请求的数组在 1 维之后具有不均匀的形状。检测到的形状为(2,)+不均匀部分。

当我运行代码时，它会运行几个时间步，然后退出并出现上述错误。有什么解决办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</guid>
      <pubDate>Sat, 15 Jul 2023 17:51:40 GMT</pubDate>
    </item>
    </channel>
</rss>