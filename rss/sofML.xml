<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Feb 2025 12:33:33 GMT</lastBuildDate>
    <item>
      <title>如何训练自定义 Core ML 模型以实现卡通/动漫风格的图像转换？</title>
      <link>https://stackoverflow.com/questions/79417865/how-can-i-train-a-custom-core-ml-model-for-cartoon-anime-style-image-transformat</link>
      <description><![CDATA[我正在开发一款 iOS 应用，用于将用户提供的图像转换为卡通或动漫风格的版本。虽然有几种预先训练的模型可用于风格转换，但我发现它们无法满足我的质量和性能需求。因此，我正在考虑训练自己的模型并使用 Core ML 进行集成。
以下是我面临的一些具体挑战和问题：
选择正确的架构：

当前构建有效执行风格转换的模型的最佳实践是什么（例如，使用 GAN、编码器-解码器架构或其他方法）？
是否有任何值得注意的研究论文或实现专注于将图像转换为卡通/动漫风格，我可以将其用作起点？

训练流程和数据准备：

假设我有一个由原始用户图像及其相应的卡通/动漫风格图像组成的数据集，建议采取哪些预处理步骤来确保高质量的训练结果？
是否有任何数据增强技术特别适用于风格转换任务？

转换为 Core ML：

将自定义训练模型（来自 TensorFlow、PyTorch 等框架）转换为 Core ML 模型的推荐流程是什么？
是否有任何工具或最佳实践可以帮助在转换过程中保持性能和准确性？

iOS 上的性能和优化：

如何平衡模型复杂性（实现高质量风格转换）和移动设备性能限制之间的权衡？
是否有已知的特定优化技术或模型架构可以与 iOS 上的 Core ML 配合良好？

我将不胜感激任何见解、资源推荐或代码示例，它们可以帮助指导我完成为此应用程序训练和部署自定义 Core ML 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79417865/how-can-i-train-a-custom-core-ml-model-for-cartoon-anime-style-image-transformat</guid>
      <pubDate>Thu, 06 Feb 2025 12:13:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的糖尿病预测项目中 sns.pairplot 会出现 TypeError 问题，我应该抑制警告吗？</title>
      <link>https://stackoverflow.com/questions/79417806/why-am-i-getting-a-typeerror-with-sns-pairplot-in-my-diabetes-prediction-project</link>
      <description><![CDATA[我正在学习机器学习。 
GeeksforGeeks 有很多项目要练习，其中之一就是这个教程。
在教程中，他编写了这个代码，可以运行，但我的不行
sns.pairplot(data, hue=&#39;Outcome&#39; , data = data) 
plt.show()

TypeError: pairplot() 获得了参数“data”的多个值

Chatgpt 给出了解决方案，但只显示警告：
sns.pairplot(data, hue=&#39;Outcome&#39;)
plt.show()

我应该抑制警告？]]></description>
      <guid>https://stackoverflow.com/questions/79417806/why-am-i-getting-a-typeerror-with-sns-pairplot-in-my-diabetes-prediction-project</guid>
      <pubDate>Thu, 06 Feb 2025 11:54:47 GMT</pubDate>
    </item>
    <item>
      <title>NameError: 尝试运行 def __init__(self, width, height, inter=cv2.INTER_AREA) 时未定义名称“cv2”：[关闭]</title>
      <link>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</link>
      <description><![CDATA[我尝试使用 cv2 编写一些神经网络代码，但出现错误
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError：名称“cv2”未定义

knn.py --dataset ./datasets/animals
回溯（最近一次调用）：
文件“/Users/test/Desktop/CODE/knn.py”，第 6 行，位于&lt;module&gt;
来自 pyimagesearch.preprocessing 导入 SimplePreprocessor
文件“/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py”，第 4 行，位于&lt;module&gt;
类 SimplePreprocessor:
文件 &quot;/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py&quot;，第 5 行，在 SimplePreprocessor 中
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError: 名称 &#39;cv2&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</guid>
      <pubDate>Thu, 06 Feb 2025 08:53:50 GMT</pubDate>
    </item>
    <item>
      <title>XGboost 在不同的标记器上具有不同的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</link>
      <description><![CDATA[我有一个transformer bodomerka/Mil_class_exp_sber_balanssedclass，我在sberbank-ai/ruBert-base的基础上对其进行了训练。额外训练的本质是，该模型可以对用俄语写的文本进行分类，并分类是否是军事经验（0或1）。
而且我还想训练Xgboost模型。我用transformer中的tokenizer对文本进行了token化。
最初，当我使用sberbank-ai/ruBert-base tokenizer时，准确率为0.86。但是，当我将其更改为bodomerka/Mil_class_exp_sber_balanssedclass时，准确率上升到了0.96。这是为什么呢？]]></description>
      <guid>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</guid>
      <pubDate>Wed, 05 Feb 2025 20:12:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习数据集预处理问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</link>
      <description><![CDATA[我有这个数据集，它是一个多类分类问题，y_train 高度不平衡。我想对此应用 smote，但它会抛出 NaN 和无穷大值错误。我尝试了 smote-variants 库下可用的所有 smote 技术，但没有成功
我计划在基于 IoT 的生产环境中部署它，所以我不想有任何偏差
任何建议或潜在解决方案都很好
这是标签编码后 y_train 的值计数
当然！以下是相同格式的标签分布，但采用 markdown 格式：
平衡和编码后的标签分布：
4 617
12 432
11 391
6 357
10 353
7 336
19 299
9 290
18 235
17 180
20 86
0 77
22 72
21 63
5 44
27 31
23 30
2 23
13 22
15 13
24 9
25 5
16 4
3 3
8 3
26 2
28 1
1 1
14 1
名称：count，dtype：int64

这是使用 mix-max 缩放数据集后的结果，我已估算缺失值值。
在应用任何 smote 之前没有 NaN 或 Infinity 值，在应用时，它会抛出此错误。
我添加了例外以删除 NaN 和无限值，但现在 smote 技术不起作用，任何可能起作用的技术建议
或者我应该只创建较小值数据的副本
例如这里的 14 只是 1，我会多次复制它，那么它会更正确，我不知道这是否是一种正确的方法]]></description>
      <guid>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</guid>
      <pubDate>Wed, 05 Feb 2025 19:00:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow Lite 将 ML 模型实现到 Android 应用中</title>
      <link>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</guid>
      <pubDate>Wed, 05 Feb 2025 16:29:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Java 从 Android 中的扫描文档中检测实心圆圈（单选按钮）？</title>
      <link>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</link>
      <description><![CDATA[我正在开发一款使用 GmsDocumentScanner 扫描纸质文档的 Android 应用。我的目标是检测哪些圆圈被填充，类似于表单上的单选按钮。
圆圈预先印在纸上，用户填充其中一个圆圈来标记他们的选择。扫描文档后，我需要检测标记的圆圈以及与之相关的文本或图标。为了便于理解，我在这里分享了一个片段。
此片段清晰地显示了扫描的纸张，应进一步处理以检测圆圈中的颜色标记。
我尝试了这些步骤

使用 GmsDocumentScanning 扫描文档
尝试使用像素强度分析检测填充的圆圈

private void understandText(Bitmap bitmap) {
InputImage image = InputImage.fromBitmap(bitmap, 0);
TextRecognizer understander = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS);

识别器.处理（图像）
.addOnSuccessListener（结果 -&gt; {
for（Text.TextBlock block : result.getTextBlocks()) {
detectRadioButtons（block, bitmap);
}
})
.addOnFailureListener（e -&gt; Log.e（&quot;错误&quot;, &quot;文本识别失败&quot;, e));
}


private void detectRadioButtons（Text.TextBlock textBlock, Bitmap bitmap）{
for（Text.Line line : textBlock.getLines()) {
Rect boundingBox = line.getBoundingBox();
if (isRadioButtonChecked（boundingBox, bitmap)) {
Log.i（&quot;检测&quot;, &quot;选中的单选按钮：&quot; + line.getText());
}
}
}


private boolean isRadioButtonChecked(Rect rect, Bitmap bitmap) {
int checkedPixelThreshold = 100;
int checkedPixels = 0;
int totalPixels = rect.width() * rect.height();

for (int x = rect.left; x &lt; rect.right; x++) {
for (int y = rect.top; y &lt; rect.bottom; y++) {
int pixel = bitmap.getPixel(x, y);
if (Color.red(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.green(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.blue(pixel) &lt; checkedPixelThreshold) {
checkedPixels++;
}
}
}
return checkedPixels &gt; (totalPixels * 0.5); // 超过 50% 的像素已填充
}


预期结果是
检测扫描纸上的圆圈
识别哪个圆圈已填充
将填充的圆圈与正确的文本标签或图标关联]]></description>
      <guid>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</guid>
      <pubDate>Wed, 05 Feb 2025 12:44:42 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 Keras、Tensorflow 实现重现性？</title>
      <link>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</link>
      <description><![CDATA[每次运行以下代码时，我获得的准确率和损失都不一样。我按照之前帖子中的说明操作，但无法解决。问题可能出在哪里？
import os
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = &quot;1&quot;
os.environ[&quot;TF_CUDNN_DETERMINISTIC&quot;] = &quot;1&quot;
...
...

SEED=65
tf.keras.utils.set_random_seed(SEED) 
tf.config.experimental.enable_op_determinism()
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

训练，测试，训练目标，测试目标 = train_test_split((df.loc[:,&quot;input_alarm_1&quot;:&quot;input_alarm_&quot;+str(num_backtracking_events)]), df.loc[:,&quot;output_failure&quot;], test_size=0.25, random_state=SEED)

训练 = np.asarray(training).astype(&#39;float32&#39;)
训练目标 = np.asarray(trainingtarget).astype(&#39;float32&#39;)
test = np.asarray(test).astype(&#39;float32&#39;)
testtarget = np.asarray(testtarget).astype(&#39;float32&#39;)

initializer = tf.keras.initializers.GlorotUniform(seed=SEED)

model = keras.Sequential(
[
layer.Dense(600, 激活=&quot;relu&quot;, input_shape=(num_backtracking_events,), kernel_initializer=initializer),
layer.Dense(300, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(100, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(1, 激活=&quot;sigmoid&quot;, kernel_initializer=initializer),
#dropout?
]
)

#编译模型
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

#fit
history = model.fit(training, trainingtarget, batch_size=10, epochs=50, validation_split=0.1)

# 评估 keras 模型
test_loss, test_acc = model.evaluate(test, testtarget)
print(&#39;Accuracy: %.2f&#39; % (test_acc*100))
print(&#39;Loss: %.2f&#39; % (test_loss*100))
]]></description>
      <guid>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</guid>
      <pubDate>Wed, 05 Feb 2025 08:09:02 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>VotingClassifier 仅支持二分类或多分类。不支持多标签和多输出分类</title>
      <link>https://stackoverflow.com/questions/79411556/votingclassifier-only-supports-binary-or-multiclass-classification-multilabel-a</link>
      <description><![CDATA[我收到有关 KerasClassifer 和 VotingClassifer 的错误。
首先，我使用 MNIST 数据集并将其拆分，然后通过 Voting 分类器拟合 xtrain 和 ytrain。如果我将它们的估计器类型放入来自不同模型的分类器中，我得到
&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;

# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

model1._estimator_type = &quot;classifier&quot;
model2._estimator_type = &quot;classifier&quot;
model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

但是当我注释掉 _estimator_type 时，我得到了 ** VotingClassifier 仅支持二分类或多分类。不支持多标签和多输出分类。 **
# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

# model1._estimator_type = &quot;classifier&quot;
# model2._estimator_type = &quot;classifier&quot;
# model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

我也尝试了 Pipeline 和 skl2onnx，但得到了相同的结果。
这是我的所有代码
import numpy as np
import pandas as pd

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import optimizers
from scikeras.wrappers import KerasClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
来自 sklearn.datasets 导入 load_digits、load_iris、fetch_openml
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.metrics 导入 accuracy_score

导入请求
来自 tensorflow.keras.utils 导入 to_categorical

# 使用 tensorflow.keras.datasets 加载 MNIST 数据集
来自 tensorflow.keras.datasets 导入 mnist

# 下载 MNIST 数据集
url = &#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz&#39;
response = request.get(url)
使用 open(&#39;mnist.npz&#39;, &#39;wb&#39;) 作为 f:
f.write(response.content)

# 加载数据集
使用 np.load(&#39;mnist.npz&#39;) 作为数据：
X_train, y_train = data[&#39;x_train&#39;], data[&#39;y_train&#39;]
X_test, y_test = data[&#39;x_test&#39;], data[&#39;y_test&#39;]

# 标准化数据
X_train = X_train.reshape((X_train.shape[0], -1)) / 255.0
X_test = X_test.reshape((X_test.shape[0], -1)) / 255.0

# 将标签转换为分类（独热编码）
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

def mlp_model()：
model = Sequential()

model.add(Dense(50, input_shape = (784, )))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(10))
model.add(Activation(&#39;softmax&#39;))

sgd = optimizers.SGD(learning_rate = 0.001, motivation=0.0)
model.compile(optimizer = sgd, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

return model

from scikeras.wrappers import KerasClassifier
from sklearn.ensemble import VotingClassifier

# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

# model1._estimator_type = &quot;classifier&quot;
# model2._estimator_type = &quot;classifier&quot;
# model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

y_pred = ensemble_clf.predict(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79411556/votingclassifier-only-supports-binary-or-multiclass-classification-multilabel-a</guid>
      <pubDate>Tue, 04 Feb 2025 11:40:49 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型在二元分类中仅预测一个类（猫）[关闭]</title>
      <link>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</link>
      <description><![CDATA[我使用 TensorFlow/Keras 训练了一个二元分类 CNN，以区分猫和狗。然而，在测试数据集上进行评估时，该模型只为每张图片预测“猫”，尽管数据集包含这两个类别。
这是我的代码：
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import load_model

def normalizer(image, label):
aux = tf.cast(image, dtype=tf.float32)
image_norm = aux/255.0
return image_norm, label

train_data, valid_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/training&#39;,
validation_split=0.1, 
subset=&quot;both&quot;, 
seed=42, 
image_size=(150, 150), 
batch_size=32 
)

test_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/test&#39;, 
image_size=(150, 150), 
batch_size=32 
)

train = train_data.map(normalizer)
valid = valid_data.map(normalizer) 
test = test_data.map(normalizer)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),activation=&#39;relu&#39;, input_shape=(150,150,3)))
model.add(MaxPooling2D())

model.add(Conv2D(filters=64, kernel_size=(3,3),激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Flatten())

model.add(Dense(units=256, 激活=&#39;relu&#39;))
model.add(Dense(units=1, 激活=&#39;sigmoid&#39;))

model.compile(
optimizer=&#39;adam&#39;,
loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[&#39;accuracy&#39;],
)

print(model.summary())

hist = model.fit(
训练，
batch_size=32， 
epochs=20， 
shuffle=True，
validation_data=valid
)

plt.plot(hist.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_loss&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.title(&#39;训练和验证中的损失&#39;)
plt.show()

如果 hist.history 中有 &#39;accuracy&#39;: 
plt.plot(hist.history[&#39;accuracy&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_accuracy&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.legend()
plt.title(&#39;训练和验证的准确性&#39;)
plt.show()

model.save(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

new_model = load_model(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

loss, acc = new_model.evaluate(test, batch_size=32)

print(loss)
print(acc)

y_pred = new_model.predict(test) 

y_true = np.concatenate([y.numpy() for x, y in test], axis=0)

matrix = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(
matrix.numpy(), 
annot=True, 
fmt=&#39;d&#39;, 
cmap=&#39;Blues&#39;, 
xticklabels=[&#39;Cat&#39;, &#39;Dog&#39;], 
yticklabels=[&#39;Cat&#39;, &#39;Dog&#39;],
)

plt.ylabel(&#39;True Label&#39;)
plt.xlabel(&#39;Predicted Label&#39;)
plt.title(&#39;Confusion Matrix&#39;)
plt.show()

这是混淆矩阵
混淆矩阵
我怀疑的可能原因

类别不平衡 –&gt; 我的训练数据中猫和狗的数量大致相等，所以我不认为这是原因。
标签问题 –&gt;我检查并确认 y_true 既有 0 也有 1，所以标签应该没问题。
注：该模型在验证中的准确率达到了约 70%
]]></description>
      <guid>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</guid>
      <pubDate>Mon, 03 Feb 2025 20:03:37 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
销毁（游戏对象）；
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Swift、UIkit 和 CoreML 在 iOS 应用中访问图像分类器 ML 模型的预测结果</title>
      <link>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</link>
      <description><![CDATA[我正在尝试开发一款应用，使用经过 Apple CoreML 训练的模型对从相机拍摄的图像或从图像库中选择的图像进行分类。该模型经过了适当的训练和测试。在将其添加到 xcode 项目后，我使用 Preview 对其进行测试时，它没有显示任何问题。但是当我尝试使用 Swift 获取预测时，结果是错误的，与 Preview 显示的完全不同。感觉就像模型未经训练一样。
这是我访问模型所做预测的代码：
let pixelImage = buffer(from: (image ?? UIImage(named: &quot;imagePlaceholder&quot;))!)
self.imageView.image = image

guard let result = try? imageClassifier!.prediction(image: pixelImage!) else {
fatalError(&quot;发生意外错误&quot;)
}

let className: String = result.classLabel
let confidence: Double = result.classLabelProbs[result.classLabel] ?? 1.0
classifier.text = &quot;\(className)\nWith Confidence:\n\(confidence)&quot;

print(&quot;分类结果为：\(className)\n置信度为：\(confidence)&quot;)

imageClassifier 是我在代码段之前使用此行代码创建的模型：
let imageClassifier = try? myImageClassifier(configuration: MLModelConfiguration())

myImageClassifier 是我使用 CoreML 创建的 ML 模型的名称。
图像是正确的，即使我输入相同的图像，它也会显示与预览不同的结果。但必须将其转换为 UIImage 到 CVPixelBuffer 类型，因为预测只允许输入 CVPixelBuffer 类型。上面代码段中的 pixelImage 是更改为 CVPixelBuffer 类型后的图像。我使用这个 stackoverflow 问题中的解决方案进行转换。代码在这里以防出现问题：
func buffer(from image: UIImage) -&gt; CVPixelBuffer? {
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] 作为 CFDictionary
var pixelBuffer : CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault, Int(image.size.width), Int(image.size.height), kCVPixelFormatType_32ARGB, attrs, &amp;pixelBuffer)
guard (status == kCVReturnSuccess) else {
return nil
}

CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
让 context = CGContext(data: pixelData, width: Int(image.size.width), height: Int(image.size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue)

context?.translateBy(x: 0, y: image.size.height)
context?.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context!)
image.draw(in: CGRect(x: 0, y: 0, width: image.size.width, height: image.size.height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))

return pixelBuffer
}

我认为模型本身没有任何问题，只是我将其实现到应用程序中的方式有​​问题。
编辑：
我已经从 Apple 的教程中下载了一个示例项目，并将其模型 MobileNet 实现到我的项目中。代码执行没有错误，结果是正确的。我创建的模型可能出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</guid>
      <pubDate>Tue, 09 Nov 2021 13:30:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras，内存错误 - data = data.astype("float") / 255.0。无法为形状为 (13165, 32, 32, 3) 的数组分配 309.MiB</title>
      <link>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</link>
      <description><![CDATA[我目前正在研究 Smiles 数据集，然后应用深度学习来检测微笑是正面的还是负面的。我使用的机器是 Raspberry Pi 3，用于执行此程序的 Python 版本是 3.7（不是 2.7）
我的训练集中总共有 13165 张图像。我想将其存储到一个数组中。但是，我遇到了一个问题，就是分配一个形状为（13165, 32, 32, 3）的数组。
下面是源代码（shallownet_smile.py）：
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classes_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv.shallownet import ShallowNet
from keras.optimizers import SGD
from imutils import routes
import matplotlib.pyplot as plt
import numpy as np
import argparse

ap = argparse.ArgumentParser()
ap.add_argument(&quot;-d&quot;, &quot;--dataset&quot;, required=True, help=&quot;path to input dataset&quot;)
args = vars(ap.parse_args())

# 获取我们将要描述的图像列表
print(&quot;[INFO] loading images...&quot;)

imagePaths = list(paths.list_images(args[&quot;dataset&quot;]))

sp = SimplePreprocessor(32, 32)
iap = ImageToArrayPreprocessor()

sdl = SimpleDatasetLoader(preprocessors=[sp, iap])
(data, labels) = sdl.load(imagePaths, verbose=1)
# 将值转换为 0-1 之间的值
data = data.astype(&quot;float&quot;) / 255.0

# 将数据划分为训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25,
random_state=42)

# 将标签从整数转换为向量
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)

# 初始化优化器和模型
print(“INFO] 编译模型...”)

# 初始化随机梯度下降，学习率为 0.005
opt = SGD(lr=0.005)

model = ShallowNet.build(width=32, height=32,depth=3,classes=2)
model.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=opt,
metrics=[&quot;accuracy&quot;])

# 训练网络
print(“INFO] 训练网络...”)

H = model.fit(trainX, trainY,validation_data=(testX, testY),batch_size=32,
epochs=100, verbose=1)

print(“[INFO] 评估网络...”)

predictions = model.predict(testX, batch_size=32)

print(classification_report(
testY.argmax(axis=1),
predictions.argmax(axis=1),
target_names=[&quot;positive&quot;, &quot;negative&quot;]
))

plt.style.use(&quot;ggplot&quot;)
plt.figure()
plt.plot(np.arange(0, 100), H.history[&quot;loss&quot;], label=&quot;train_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;acc&quot;], label=&quot;train_acc&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_acc&quot;], label=&quot;val_acc&quot;)
plt.title(&quot;训练损失和准确率&quot;)
plt.xlabel(&quot;Epoch #&quot;)
plt.ylabel(&quot;损失/准确率&quot;)
plt.legend()
plt.show()

假设数据集位于我当前的目录中。以下是我得到的错误：

python3 shallownet_smile.py -d=datasets/Smiles

错误消息
我仍然感到困惑，不知道哪里出了问题。我将非常感谢任何专家或有深度学习/机器学习经验的人向我解释和澄清我做错了什么。
感谢您的帮助和关注。]]></description>
      <guid>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</guid>
      <pubDate>Sun, 05 Apr 2020 17:25:26 GMT</pubDate>
    </item>
    <item>
      <title>回归模型中成本函数用L1范数代替L2范数</title>
      <link>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</link>
      <description><![CDATA[我想知道 Python 中是否有一个函数可以完成与 scipy.linalg.lstsq 相同的工作，但使用“最小绝对偏差”回归而不是“最小二乘”回归 (OLS)。我想使用 L1 范数，而不是 L2 范数。
事实上，我有 3d 点，我想要它们的最佳拟合平面。常见的方法是通过最小二乘法，如 Github 链接。但众所周知，这并不总是能给出最佳拟合，尤其是当我们的数据集中有闯入者时。最好计算最小绝对偏差。 此处 更详细地解释了这两种方法之间的区别。
由于它是 Ax = b 矩阵方程，需要循环来最小化结果，因此无法通过 MAD 等函数解决。我想知道是否有人知道 Python 中的相关函数（可能是线性代数包中）可以计算“最小绝对偏差”回归？]]></description>
      <guid>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</guid>
      <pubDate>Thu, 16 Aug 2018 18:12:02 GMT</pubDate>
    </item>
    </channel>
</rss>