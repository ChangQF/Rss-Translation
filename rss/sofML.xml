<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 17 Jul 2024 01:06:59 GMT</lastBuildDate>
    <item>
      <title>如何将多头自注意力输出的形状更改为可以馈送到卷积层的形状</title>
      <link>https://stackoverflow.com/questions/78757193/how-to-change-output-shape-of-multi-head-self-attention-output-to-a-shape-that-c</link>
      <description><![CDATA[我遇到了这样的错误
MHSA（多头自注意力）的输出是这样的
torch.Size([20, 197, 768])

批量大小为 20
序列长度为 197（之前为 196，添加类标记后为 197）
嵌入维度为 768
我想将其重塑为这样的格式，以便将其馈送到卷积层
torch.Size([batch_size, channel, width, height])

我尝试通过添加新维度来使用类似的东西
torch.unsqueeze(1)
torch.transpose(1, 3)

它可以成功输入到卷积层。但我不知道这是否正确，所以如果不正确请纠正我
现在我正在尝试使用类似的东西
new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)

它给我一个错误，形状对于大小（some_number）的输入无效。发生这种情况是因为sequence_length（197）无法完美平方，结果会是十进制数，而视图函数需要int输入，因此改为int后平方运算结果为16，而batch_size * 768 * 16 * 16不等于前一个（batch_size * 197 * 768），导致错误
我的分析正确吗？如何解决这个问题，或者有更好的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78757193/how-to-change-output-shape-of-multi-head-self-attention-output-to-a-shape-that-c</guid>
      <pubDate>Wed, 17 Jul 2024 00:24:06 GMT</pubDate>
    </item>
    <item>
      <title>需要什么类型的相机或计算机视觉硬件/软件来跟踪寻找特定运动的人[关闭]</title>
      <link>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</link>
      <description><![CDATA[我需要找到一个摄像头和相应的计算机视觉技术，可以跟踪人类直到做出特定动作，在这种情况下它将触发另一个系统事件。例如，这个摄像头应该检测其框架内的人的存在，如果人坐下，它将告诉触发连接到摄像头系统的 LED 以打开。任何想法或指导都值得赞赏。
我研究过像 arduinos Nicla Vision 这样的计算机视觉系统，但并不认为它有能力检测到这种变化。]]></description>
      <guid>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</guid>
      <pubDate>Tue, 16 Jul 2024 23:02:38 GMT</pubDate>
    </item>
    <item>
      <title>当已知目标值的特征向量时，如何使用监督式机器学习进行时间序列预测？</title>
      <link>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</link>
      <description><![CDATA[我尝试使用 LSTM 预测仪器的连续“偏移”校准值。这些偏移值之前已被证明与用作特征的一对温度值有很好的相关性。这些偏移值显示出周期性，因此为模型选择了 LSTM。但是，我发现的所有 LSTM 示例都使用来自先前数据点序列的特征向量来预测目标。我觉得这可能无法捕捉序列中每个特征向量与其偏移之间的关系。而且由于在预测中只使用了先前数据点序列的特征向量，因此无法有效地利用该温度偏移关系。
为了解决这个问题，我已经将当前数据点的特征向量添加到用于训练模型的向量序列中，当然，也添加到用于预测目标偏移的序列中。
但是，我如何才能对要预测的偏移的特征向量赋予更大的权重，以使模型能够利用温度偏移关系？
谢谢！ 🖖
创建序列的代码如下所示：
def create_sequences(x, y, time_steps = 24):
xs, ys = [], []
for i in range(len(x) - time_steps - 1):
x_temp = x[i:(i + time_steps + 1)] #在目标值之前创建一个 time_steps 序列
xs.append(x_temp)
ys.append(y[i + time_steps]) #目标值是序列之后的值
return np.array(xs), np.array(ys)
]]></description>
      <guid>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</guid>
      <pubDate>Tue, 16 Jul 2024 22:49:31 GMT</pubDate>
    </item>
    <item>
      <title>词到词机器翻译的评估指标/算法</title>
      <link>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</link>
      <description><![CDATA[我正在寻找一种科学合理的方法来评估我的学士论文的单词翻译。在寻找的过程中，我发现大多数方法都是基于 n-gram 精度来进行句子评估。我正在将数据库列和表名从英语翻译成德语。我可以选择包含基本事实，但我更喜欢不包含它的方法。我的翻译和原文也包含缩写。
现在我使用 OpenAIs 最新的嵌入模型，然后计算嵌入之间的距离。但结果很大程度上取决于模型。我研究过使用字符 n-gram 的 ChrF。但如上所述，不需要基本事实的方法会更理想。]]></description>
      <guid>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</guid>
      <pubDate>Tue, 16 Jul 2024 19:42:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 GCP TPU 进行多处理，但 shell 意外死机</title>
      <link>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</link>
      <description><![CDATA[我可以访问美国地区的 32 个可抢占 Cloud TPU v4 芯片，并尝试使用以下 Python 代码运行我的 PyTorch 模型：
import os 
import sys 
import pickle 
import torch_xla.distributed.xla_multiprocessing as xmp 
from transformers 
import AutoTokenizer 
import main_utils as utils 
import multiprocessing as mp

lock = mp.Manager().Lock()

def _mp_fn(i): 

A_tasks, B_tasks, desire_output_lengths, keys = utils.get_total_tasks()

import torch
import torch_xla.core.xla_model as xm
from models_mamba import MambaForCausalLM

DEVICE_NAME = xm.xla_device()
tokenizer = AutoTokenizer.from_pretrained(&quot;~/Mamba-1B/&quot;)
model = MambaForCausalLM.from_pretrained(&quot;~/Mamba-1B/&quot;, torch_dtype=&quot;auto&quot;,
device_map=&quot;auto&quot;, low_cpu_mem_usage=True)
model = model.to_empty(device=&#39;cpu&#39;)
model.apply(lambda module: module.reset_parameters() if hasattr(module, &#39;reset_parameters&#39;) else None)
model = model.to(xm.xla_device())

params_to_save = [&quot;out_proj_y&quot;]

def generate_logits(task, logits_list, desire_output_length):
for i in range(48):
layer_to_save = [i + 1]
input_ids = tokenizer.encode(task, return_tensors=&quot;pt&quot;).to(设备名称)
model.saved_activation(params_to_save, layer_to_save, precision=&#39;r&#39;)
output = model.generate(input_ids, max_length=desired_output_length, no_repeat_ngram_size=2)

saved_dict = model.reset_everything_and_save()
param = saved_dict[f&quot;out_proj_y_{i + 1}&quot;][0, -1, :].to(设备名称)

xm.all_gather(param)
logits = model.get_unembed_for_layer(param, norm=&quot;False&quot;)

xm.all_gather(logits)
logits_list.append(logits.to(&quot;cpu&quot;))

del input_ids、output、saved_dict、param、logits

task_dict = {}
for A_task、B_task、dol、key in zip(A_tasks、B_tasks、desired_output_lengths、keys):
A_logits = []
B_logits = []
generate_logits(A_task、A_logits、dol)
generate_logits(B_task、B_logits、dol)

aux_dict = {&#39;A&#39;: A_logits, &#39;B&#39;: B_logits}
task_dict[key] = aux_dict

device = xm.xla_device()
with lock:
print(f&#39;Process {i}, Device {device}&#39;)

xm.mark_step()
with open(f&#39;~/main_file.pickle&#39;, &#39;wb&#39;) as handle:
pickle.dump(task_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

if name == &quot;main&quot;: 
xmp.spawn(_mp_fn, args=(), nprocs=8, start_method=&#39;fork&#39;)`

要执行此代码，我使用 Cloud Shell 和以下命令（设置 TPU 并在所有 8 个工作器上安装库之后）：
gcloud compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE} --project=${PROJECT_ID} --worker=all --command=&quot;PJRT_DEVICE=TPU python3 ~test.py&quot;
我希望模型在所有 TPU 上执行 _mp_fn() 中定义的任务，按照指定的方式收集和保存 logit，最后将结果存储在 main_file.pickle 中。但是，在 Cloud Shell 中启动命令后，shell 在模型加载期间打印了一些预期的警告，但没有继续进行。相反，过了一段时间，shell 会话意外终止。我不确定如何排除故障？]]></description>
      <guid>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</guid>
      <pubDate>Tue, 16 Jul 2024 17:38:54 GMT</pubDate>
    </item>
    <item>
      <title>我如何访问这些元素？[关闭]</title>
      <link>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</link>
      <description><![CDATA[LIVEKIT_URL= 

LIVEKIT_API_KEY= 

LIVEKIT_API_SECRET= 

DEEPGRAM_API_KEY= 

在此处输入图片描述 来自我的帐户的与项目相关的屏幕截图
页面顶部的 URL 是吗？我无法在 LIVEKIT 上创建 API 密钥。
这是与项目相关的 YouTube 链接。
https://www.youtube.com/watch?v=nvmV0a2geaQ]]></description>
      <guid>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</guid>
      <pubDate>Tue, 16 Jul 2024 15:20:52 GMT</pubDate>
    </item>
    <item>
      <title>使用张量流的问题（既不包含“saved_model.pb”也不包含“saved_model.pbtxt”）</title>
      <link>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</link>
      <description><![CDATA[我尝试运行代码，但出现错误。
ValueError：尝试加载不兼容/未知类型的模型。&#39;C:\Users\pm23821\AppData\Local\Temp\tfhub_modules\9616fd04ec2360621642ef9455b84f4b668e219e&#39; 既不包含 &#39;saved_model.pb&#39; 也不包含 &#39;saved_model.pbtxt&#39;

这是我第一次使用 tensorflow 和模型，所以我不知道我需要做什么才能运行我的代码。
目标是找到声音之间的差异，当检测到紧急警报器（警察、消防部门和医院）时，它需要看起来像已被识别一样。当听到家用警报器或鹦鹉的声音时，应该看起来它还没有被识别。
# 加载 YAMNet 模型
model = hub.load(&#39;https://tfhub.dev/google/yamnet/1&#39;)

# 加载 YAMNet 中的类名
def load_class_names(csv_path=&#39;yamnet_class_map.csv&#39;):
class_map = pd.read_csv(csv_path, sep=&#39;,&#39;, header=None)
class_names = class_map[1].tolist()
return class_names

class_names = load_class_names()

# 麦克风音频捕获函数
def record_audio(duration=5, fs=16000): # 录制时长为 5 秒
recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=&#39;float32&#39;)
sd.wait()
return recording.flatten()

# 预测音频类的函数
def predict_sound(audio_data):
scores, embeddings, spectrogram = model(audio_data)
prediction = np.argmax(scores, axis=1)[0]
return class_names[prediction]

# 循环主函数
while True:
print(&quot;Escutando...&quot;)
audio_data = record_audio()
audio_data /= np.max(np.abs(audio_data))

predict_class = predict_sound(audio_data)
print(&quot;预测的类：&quot;, predict_class)

if “/m/012n7d”在 Predicted_class 或“/m/03j1ly”中在 Predicted_class 或“/m/03kmc9”中in Predicted_class: print(&quot;Sirene detectorda!&quot;) else: print(&quot;Não é uma Sirene.&quot;) time.sleep(1) # Pausa de 1 segundo antes da próxima gravação ]]></description>
      <guid>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</guid>
      <pubDate>Tue, 16 Jul 2024 14:11:25 GMT</pubDate>
    </item>
    <item>
      <title>如何将每小时数据转换为每日最大值、平均值……和最小值</title>
      <link>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</link>
      <description><![CDATA[这是每小时的 TMP 数据，因此想将其转换为每日的最大值、平均值和最小值，我在 excel 中做过一次，但结果不合理。
此数据需要预处理，因此我只是应用了一些函数，例如查找平均值，我只是使用函数平均值来获取一天的平均数据，但结果并不好，因此我需要一些其他准确且最佳的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</guid>
      <pubDate>Tue, 16 Jul 2024 14:03:10 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和跨摄像头跟踪物体</title>
      <link>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</link>
      <description><![CDATA[我正在使用 YOLOV10 检测和深度排序来跟踪物体，当我们在第一个 CCTV 摄像机中时，它工作正常，当我们到达下一个 CCTV 摄像机时，物体来自不同的视角，类似的物体也很多，我将跟踪更改为 YOLOV10 本身，它们都无法解决在第二个摄像机中识别第一个摄像机中跟踪的物体的问题，因为第二个摄像机中的大小、视点和光照都发生了变化，现在如何给出与模型在第一个摄像机中跟踪的物体相同的 id，第二个摄像机中的 id 为 13，当物体到达时，它应该具有相同的 id，模型应该知道视点比例和光照是否也发生了变化
这个物体应该匹配
使用这个，而那里也有相同类型的汽车
在第一个摄像头中，物体向东行走，但在第二个摄像头中，物体从它的南边过来，所以我们无法跟踪位置
from ultralytics import YOLO
import cv2

# 全局变量
selected_bbox = None
tracking_id = None

# 鼠标回调函数用于选择要跟踪的对象
def select_object(event, x, y, flags, param):
global selected_bbox, tracking_id
if event == cv2.EVENT_LBUTTONDOWN:
for result in param:
bboxes = result.boxes.xyxy.cpu().numpy() # 边界框
ids = result.boxes.id.cpu().numpy() # 跟踪 ID
for bbox, id_ in zip(bboxes, ids):
if bbox[0] &lt;= x &lt;= bbox[2] and bbox[1] &lt;= y &lt;= bbox[3]:
selected_bbox = bbox
tracking_id = id_
return

print(&quot;starting&quot;)
# 加载 YOLOv10 模型
model = YOLO(&#39;yolov10m.pt&#39;)

# 加载视频
video_path = &#39;videos/1.mp4&#39;
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.100:554&quot;)

cv2.namedWindow(&#39;frame&#39;)
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=None)

ret = True
results = []

# 读取帧
while ret:
ret, frame = cap.read()

if ret:
# 检测和跟踪对象
results = model.track(frame, persist=True)

# 设置鼠标回调参数
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=results)

if tracking_id is not None:
# 为所选对象绘制边界框
for result in results:
bboxes = result.boxes.xyxy.cpu().numpy()
ids = result.boxes.id.cpu().numpy()
for bbox, id_ in zip(bboxes, ids):
if id_ == tracking_id:
cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)
cv2.putText(frame, f&quot;ID: {int(id_)}&quot;, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)

# 可视化
cv2.imshow(&#39;frame&#39;, frame)
if cv2.waitKey(25) &amp; 0xFF == ord(&#39;q&#39;):
break

else:
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.101:554&quot;)
ret = True

cap.release()
cv2.destroyAllWindows()
`
]]></description>
      <guid>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</guid>
      <pubDate>Tue, 16 Jul 2024 12:40:28 GMT</pubDate>
    </item>
    <item>
      <title>尝试在多 GPU 设置上训练机器翻译的 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</link>
      <description><![CDATA[我正在尝试训练机器翻译的变换模型。这是训练函数：
在单个 GPU 上训练时，我得到以下结果：
for src, tgt in train_dataloader:
try:
src = src.to(DEVICE)
tgt = tgt.to(DEVICE)

tgt_input = tgt[:-1, :]

src_mask, tgt_mask, \
src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

logits = model( src, tgt_input,
src_mask, tgt_mask,
src_padding_mask, tgt_padding_mask,
src_padding_mask
)

在Seq2SeqTransformer:
class Seq2SeqTransformer(nn.Module):
def forward( self,
src: Tensor,
trg: Tensor,
src_mask: Tensor,
tgt_mask: Tensor,
src_padding_mask: Tensor,
tgt_padding_mask: Tensor,
memory_key_padding_mask: Tensor ):

src_emb = self.positional_encoding(self.src_tok_emb(src))
tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )
return self.generator(outs)

在此行：
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )

我明白了：
&gt; sec_emb 的大小为 (31,31) tgt_emb 的大小为 (37,37)

训练运行顺利。
现在，我尝试使用以下设置在具有 8 个 GPU 的计算机上进行训练：
 transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

在调试模式下，我检查此行中的值：
 outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )


我明白了：
&gt; sec_emb 的大小为 (4,31) tgt_emb 的大小为 (5,37)

我认为这是因为事情是并行进行的。
但是，我遇到了此错误消息：
&gt; 文件
&gt; &quot;C:\Projects\MT005\.venv\Lib\site-packages\torch\nn\ functional.py&quot;,
&gt; 第 5382 行，在 multi_head_attention_forward 中
&gt;引发 RuntimeError(f&quot;2D attn_mask 的形状为 {attn_mask.shape}，但应为 {correct_2d_size}。&quot;) RuntimeError:
&gt; 2D attn_mask 的形状为 torch.Size([4, 31])，但应为
&gt; (4, 4)。

有人可以帮助我或提供一些指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</guid>
      <pubDate>Tue, 16 Jul 2024 12:00:27 GMT</pubDate>
    </item>
    <item>
      <title>优化 pgvector 以实现多用户文档存储：索引和分区的最佳实践</title>
      <link>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</link>
      <description><![CDATA[我使用 PostgreSQL 和 pgvector 以及 LangChain 进行文档存储和检索。我的设置：

许多用户，每个用户上传多个文档
文档具有带 userId 的元数据
需要按用户隔离用户文档并按 userId 进行过滤
使用 LangChain 进行带过滤器的检索

当前表结构：
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
);

优化可扩展性和查询性能的最佳方法是什么？

为每个用户创建一个新表？
添加 user_id 列并对表进行分区？
索引元数据 - &gt;&gt;&#39;userId&#39;？
另一种方法？

寻找一种可扩展性好且在使用用户特定过滤器进行向量相似性搜索时保持良好性能的解决方案。
感谢您的帮助！
我尝试根据存储在元数据 JSONB 字段中的 userId 在 document_vector 表上实现哈希分区，但不确定这是否是正确的方法。
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
) PARTITION BY HASH ((metadata-&gt;&gt;&gt;&#39;userId&#39;));
]]></description>
      <guid>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</guid>
      <pubDate>Tue, 16 Jul 2024 11:33:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用标签编码的情况下对 RandomForest 的非序数分类变量进行编码？</title>
      <link>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</guid>
      <pubDate>Fri, 12 Jul 2024 21:11:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOv6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有以下输出，但无法弄清楚如何评估，因为没有 F1 分数 或 混淆矩阵。
平均召回率 (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

平均召回率 (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

平均召回率 (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我训练了 400 个 epoch，这只是输出的一小部分。我也看不到 mAP。
我有这行代码要评估
!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0

有没有办法获得详细的评估指标，例如 F1 分数、混淆矩阵 和 mAP？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>获取“model.fit”keras API 参数的值</title>
      <link>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</link>
      <description><![CDATA[我正在尝试使用自定义回调函数获取 Kera 顺序模型的详细信息。我需要提取 model.fit() API 中设置的参数的所有值，例如 batch_size、epochs、validation_split 等。但我无法在 Keras 的回调中访问它们。您知道如何自动获取这些值吗？
我使用的是 Python 3.10 和 Keras 2.8。]]></description>
      <guid>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</guid>
      <pubDate>Thu, 30 Nov 2023 20:13:45 GMT</pubDate>
    </item>
    </channel>
</rss>