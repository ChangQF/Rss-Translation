<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 25 Jun 2024 03:17:44 GMT</lastBuildDate>
    <item>
      <title>我如何根据 1 分钟间隔内初始 5 个等距变化率来建模预测衰减趋势？</title>
      <link>https://stackoverflow.com/questions/78665223/how-can-i-model-to-predict-decay-trend-based-on-intial-5-equally-spaced-rate-of</link>
      <description><![CDATA[我有 30 条衰减轨迹。每条轨迹的变化率略有不同。我想基于每 1 分钟后的 5 个初始变化率进行建模（经典或 RNN）。因此，前 5 分钟我有 R1、R2、R3、R4、R5，然后想预测衰减轨迹的其余部分。对于训练，我们可以使用整个轨迹，在推理时输入将是 5 个速率。对此进行建模的可能方法有哪些？]]></description>
      <guid>https://stackoverflow.com/questions/78665223/how-can-i-model-to-predict-decay-trend-based-on-intial-5-equally-spaced-rate-of</guid>
      <pubDate>Tue, 25 Jun 2024 02:59:47 GMT</pubDate>
    </item>
    <item>
      <title>我想通过机器学习预测未来 60 天的股票价格</title>
      <link>https://stackoverflow.com/questions/78665213/i-want-to-predict-stock-price-in-next-60-days-by-machine-learning</link>
      <description><![CDATA[我想预测未来60天的股价，但是写完代码后，预测结果却向后。有人能指点我吗？我该怎么做？我修改了代码，但是不起作用。
我的代码是
import math
from mplfinance.original_flavor import candlestick_ohlc
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import mean_squared_error

Tencent2.head()
#定义特征和目标
##features = [&#39;Open&#39;, &#39;Vol.&#39;]
##target = &#39;Price&#39;

#绘制折线图
Tencent2[&#39;Date&#39;] = pd.to_datetime(Tencent2[&#39;日期&#39;])
plt.figure(figsize=(15,6))
plt.plot(Tencent2[&#39;日期&#39;], Tennis2[&#39;收盘价&#39;], marker=&#39;.&#39;)
plt.title(&#39;按月收盘价&#39;, fontsize=15)
plt.xlabel(&#39;日期&#39;, fontsize=13)
plt.ylabel(&#39;收盘价&#39;, fontsize=13)
plt.xticks(rotation=45)
plt.show()

#绘制蜡烛图
matplotlib_date = mdates.date2num(Tencent2[&#39;日期&#39;])
ohlc = np.vstack((matplotlib_date,Tencent2[&#39;Open&#39;],Tencent2[&#39;High&#39;],Tencent2[&#39;Low&#39;],Tencent2[&#39;Close&#39;])).T
plt.figure(figsize=(15,6))
ax = plt.subplot()
candlestick_ohlc(ax,ohlc,width=0.8,colorup=&#39;g&#39;,colordown=&#39;r&#39;)
ax.xaxis_date()
plt.title(&#39;收盘价变动&#39;)
plt.xlabel(&#39;日期&#39;)
plt.ylabel(&#39;收盘价&#39;)
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

#重置索引
Tencent2.set_index(&#39;Date&#39;, inplace=True)

#创建一个只有“Close”列的新数据框
data =腾讯2.filter([&#39;Close&#39;])

#将数据框转换为numpy数组
dataset = data.values
#获取训练模型的行数
training_data = math.ceil( len(dataset) * 0.7 )

#缩放数据
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)

#创建训练数据集
#创建缩放后的训练数据集
train_data = scaled_data[0:training_data, :]
#将数据拆分为x_train和y_train数据集
x_train = []
y_train = []
#我们创建一个循环
for i in range(60, len(train_data)):
x_train.append(train_data[i-60:i, 0]) 
y_train.append(train_data[i, 0]) 
if i &lt;= 60:
print(x_train)
print(y_train)
print()

#将 x_train 和 y_train 转换为 numpy 数组
x_train, y_train = np.array(x_train), np.array(y_train)

#重塑数据
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

#构建 LSTM 模型
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

#编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

#训练模型
model.fit(x_train, y_train, batch_size=1, epochs=1)

#创建测试数据集
#创建一个包含从索引 1738 到 2247 的缩放值的新数组
test_data = scaled_data[training_data - 60:]
#创建数据集 X_test 和 y_test
X_test = []
y_test = dataset[training_data:, :]
for i in range(60, len(test_data)):
X_test.append(test_data[i-60:i, 0])

#将数据转换为 numpy 数组
X_test = np.array(X_test)

#重塑数据
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

#获取模型对 X_test 数据集的预测价格值
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)

#评估模型（获取均方根误差 (RMSE)）
rmse = np.sqrt( np.mean( predictions - y_test )**2 )

#绘制数据
train = data[:training_data]
valid = data[training_data:]
valid[&#39;Predictions&#39;] = predictions
#可视化数据
plt.figure(figsize=(16,8))
plt.title(&#39;Model&#39;)
plt.xlabel(&#39;Date&#39;, fontsize=18)
plt.ylabel(&#39;Close Price&#39;, fontsize=18)
plt.plot(train[&#39;Close&#39;])
plt.plot(valid[[&#39;Close&#39;, &#39;Predictions&#39;]])
plt.legend([&#39;Train&#39;, &#39;Validation&#39;, &#39;Predictions&#39;], loc=&#39;lower right&#39;)
plt.show()


我想预测未来 60 天的价格，但价格是过去预测的。我该如何预测 2024-06 之后的价格？
感谢您的帮助。
我的示例数据是
]]></description>
      <guid>https://stackoverflow.com/questions/78665213/i-want-to-predict-stock-price-in-next-60-days-by-machine-learning</guid>
      <pubDate>Tue, 25 Jun 2024 02:51:50 GMT</pubDate>
    </item>
    <item>
      <title>对包含姓名、电子邮件、地址、出生日期、电话号码和一些二进制标志的数据库进行重复数据删除的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/78664960/best-way-to-deduplicate-a-database-which-has-name-email-address-dob-phone-nu</link>
      <description><![CDATA[我有一个数据库，其中有重复的记录（同一个客户创建了多个帐户）。它们的唯一标识符列值对于每个 NameID 都不同，但其他信息可能相同。
例如，有一个名叫 Mark 的人。他创建了两个来宾用户：
来宾用户 1：
姓名 ID= 12345
姓名 = Mark Thomas
IsOnlineshopper = 1
Isinstoreshopper = 0
出生日期 = 01/12/1998
电子邮件 = mark.thomas@hotmail.com
地址 = # 12 Bailey Street, New Jersey, USA
电话 =03248923423

来宾用户 2：
姓名 ID= 56789
姓名 = Mark Thomas
IsOnlineshopper = 0
Isinstoreshopper = 1
出生日期 = 01/12/1998
电子邮件 = mark.thomas@hotmail.com
地址 = # 12 Bailey Street, New Jersey, USA
电话 =03248923423

现在，这是其中一个场景&#39;IsOnlineshopper&#39; 和 &#39;Isinstoreshopper&#39; 不同，但其他所有参数都相同。可能会出现用户输入错误的电话号码而其他所有参数都相同的情况。有很多种情况，但我想从最简单的一种开始，即所有字段都匹配。
我的目标是实现：
NameID 100% 匹配，并根据以下算法将它们列为重复项（通过公共 ID 将所有匹配项链接在一起）：

如果名称匹配，则 100% 进入下一步，否则不匹配。
如果电子邮件匹配，则 100% 进入下一步，否则不匹配。
如果电话号码匹配，则 100% 进入下一步，否则不匹配。
如果地址匹配，则 100% 进入下一步，否则不匹配。
如果地址出生日期匹配，则 100% 进入下一步，否则不匹配。
根据以下优先级标记为主要和重复项



NameID 具有IsOnlineshopper 应标记为主要。
具有 Isinstoreshopper 的 NameID 应标记为重复。


在匹配和合并这些名称一次（清除系统）后，我想自动化该过程，但每天运行该过程，将所有传入的记录（重复项）与主要名称合并。
我在 Azure SQL 中有数据，我可以选择使用 Databricks（考虑到数据量巨大，可能需要更多的计算能力）。我已经创建了主要数据集的一个子集进行测试。但我不确定应该使用什么方法进行重复数据删除。
我做了一些研究，看起来机器学习可以用于这种情况。我也在考虑在 pyspark 中对此进行硬编码。但正如我提到的，我不确定哪种方法最好。
请分享您的想法。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78664960/best-way-to-deduplicate-a-database-which-has-name-email-address-dob-phone-nu</guid>
      <pubDate>Tue, 25 Jun 2024 00:17:16 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的总 epoch 数，对同一 epoch 得出不同的结果</title>
      <link>https://stackoverflow.com/questions/78664794/different-results-for-the-same-epoch-using-different-number-of-total-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78664794/different-results-for-the-same-epoch-using-different-number-of-total-epochs</guid>
      <pubDate>Mon, 24 Jun 2024 22:35:04 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的自定义评分功能无法按预期工作？</title>
      <link>https://stackoverflow.com/questions/78664597/custom-scoring-function-in-sklearn-does-not-work-as-intended</link>
      <description><![CDATA[大家好，我目前正在使用来自 kaggle 的葡萄酒质量数据集深入研究机器学习领域。
我查看了我的数据，识别并限制了异常值，缩放了数据，对其使用了 smote（因为有 100 种好葡萄酒和 900 种坏葡萄酒）
我发现 f2_score 最适合对我的模型的质量进行分类。
这些是我的 f2_score 函数和打印结果：
from sklearn.metrics import classes_report, confusion_matrix,precision_score, recall_score

def f2_score(y, y_pred, pos_label=1):
precision = precision_score(y, y_pred, pos_label=1)
recall = recall_score(y, y_pred, pos_label=1)
f2 = (5 * precision * recall) / (4 * 精度 + 召回率)
return f2

def print_results(y, y_pred):
print(&#39;__________________________________________________________________________________&#39;)
print(&#39;&#39;)
# 创建和导出分类报告
report = 分类报告(y, y_pred, zero_division=1)
print(&quot;分类报告:&quot;)
print(&#39;F2_Score:&#39;,f2_score(y,y_pred))
print(&#39;__________________________________________________________________________________&#39;)
print(report)

# 发现混淆矩阵
conf_matrix = 混淆矩阵(y, y_pred)

# 创建和导出图表
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix,
annot=True,
fmt=&#39;d&#39;,
cmap=&#39;Blues&#39;,
xticklabels=[&#39;Schlecht&#39;, &#39;Gut&#39;],
yticklabels=[&#39;Schlecht&#39;, &#39;Gut&#39;]
)

plt.xlabel(&#39;Vorhergesagte Labels&#39;)
plt.ylabel(&#39;Wahre Labels&#39;)
plt.title(&#39;Konfusionsmatrix&#39;)
plt.show()
print(conf_matrix)
print(&#39;__________________________________________________________________________________&#39;)

def print_best_score_and_params(model):
print(&#39;Best score: &#39; + str(model.best_score_))
print(&#39;Best params: &#39; + str(model.best_params_))

使用 SVC 的标准参数时，我已经获得了良好的结果：
来自 sklearn.svm导入 SVC
从 sklearn.metrics 导入 accuracy_score、f1_score、recall_score

svc = SVC(random_state=11, C = 1, gamma = &#39;scale&#39;) # C = 190 下一个更准确的结果 // 标准参数：C=1，gamma=&#39;scale&#39;

svc.fit(X_smote, y_smote)

y_pred = svc.predict(X_test)

print_and_write_results(0, y_test, y_pred)


--&gt; F2_Score：0.7196969696969696
但是，如果我现在尝试使用 RandomizedSearchCV 或 GridSearchCV，我总是会得到更差的分数，并且它不会搜索具有最高 f2_score 的选项。
甚至在调试几个小时后，我也不知道为什么。
来自 sklearn.metrics 导入 make_scorer，recall_score
来自 sklearn.model_selection 导入 KFold
来自 sklearn.model_selection 导入 RandomizedSearchCV

search_distribution = {&#39;C&#39;：[1,2,5,10,15,20]
}

scorer = make_scorer(f2_score, pos_label=1) # 创建一个基于召回率指标的 1 级评分器优化

kfold = KFold(n_splits=3, random_state=42, shuffle=True)

svc_search = RandomizedSearchCV(svc, search_distribution, random_state = 11, n_iter = 6,scoring = scorer, cv=kfold)

svc_search.fit(X_smote, y_smote)

print_best_score_and_params(svc_search)

print(&#39;最佳得分：&#39;, svc_search.best_score_)

y_pred = svc_search.predict(X_test)

print_and_write_results(0, y_test, y_pred)

使用仅具有不同 C 参数的随机搜索
我总是得到这样的结果：
最佳得分： 0.9587991499630526
最佳参数：{&#39;C&#39;: 20}
最佳得分：0.9587991499630526

分类报告：
F2_Score：0.6995884773662552
RandomizedSearchCV 使用的得分与我给出的 f2 得分无关，因此我无法使用 random_search 或 grid_search 来查找最佳参数？
您知道为什么得分与 f2 得分如此不同吗？
我尝试使用 f2_score 作为最佳拟合参数的指标。我认为 best_score 和 f2 得分相同？
我真的不知道这个超过 0.95 的值在哪里来自？]]></description>
      <guid>https://stackoverflow.com/questions/78664597/custom-scoring-function-in-sklearn-does-not-work-as-intended</guid>
      <pubDate>Mon, 24 Jun 2024 21:10:54 GMT</pubDate>
    </item>
    <item>
      <title>担任数据科学家需要具备哪些必备技能或知识？哪些技能或知识能让应届毕业生更有价值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78664514/what-are-all-the-mandatory-skills-or-knowledge-one-should-have-for-data-scientis</link>
      <description><![CDATA[我最近刚获得电子与通信工程 (ECE) 硕士学位，我渴望转型从事数据科学职业。我的背景包括数学概念、编程（主要是 Python）方面的坚实基础。我对利用数据来获得有意义的见解和解决复杂问题有着浓厚的兴趣。
鉴于我的技术背景，我渴望了解数据科学家角色所必需的基本技能和知识。根据您的经验，雇主在数据科学家身上寻找的关键能力是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78664514/what-are-all-the-mandatory-skills-or-knowledge-one-should-have-for-data-scientis</guid>
      <pubDate>Mon, 24 Jun 2024 20:36:16 GMT</pubDate>
    </item>
    <item>
      <title>Mediapipe - 机器模型的动态学习</title>
      <link>https://stackoverflow.com/questions/78664510/mediapipe-dynamic-learning-of-machine-models</link>
      <description><![CDATA[大家好，我现在正在使用 mediapipe，我想通过包含图片的状态动态训练我的姿势，但我对结果不太满意。你们尝试了什么来获得更好的姿势检测？
我尝试了我自己和其他人的多张照片，但它仍然检测到错误的姿势]]></description>
      <guid>https://stackoverflow.com/questions/78664510/mediapipe-dynamic-learning-of-machine-models</guid>
      <pubDate>Mon, 24 Jun 2024 20:35:04 GMT</pubDate>
    </item>
    <item>
      <title>DPO 训练期间预训练模型权重未更新</title>
      <link>https://stackoverflow.com/questions/78664372/pretrained-model-weights-not-updating-during-dpo-training</link>
      <description><![CDATA[我正在尝试将 DPO 应用于预训练模型。然而，在训练过程中，预训练模型和微调模型给出的分数相同，并且所有批次的损失都保持不变，这让我相信权重没有更新。我的训练方法如下。
def train(model, optimizer, pref_set, dispref_set, epochs, beta, bs):
model.train()
#print(list(model.parameters())[0])
#print(list(model.parameters())[0].grad)
for epoch in range(epochs):
cur_pref=[]
cur_dispref=[]
for i in range(len(pref_set)):
cur_pref.append(pref_set[i])
cur_dispref.append(dispref_set[i]) #收集首选和不首选的响应
if (i+1) % bs == 0:
make_fastas(cur_pref, cur_dispref) #设置必要的文件
run_mpnn(&#39;model-DPO&#39;) #对响应进行评分
optimizer.zero_grad()
b_ref, nb_ref, b_dpo, nb_dpo = collect_logps(cur_pref) #收集分数
loss = calc_loss(b_dpo, nb_dpo, b_ref, nb_ref, beta) #计算 DPO 损失
print(loss)
loss.backward()
optimizer.step()
print(optimizer)
torch.save({ #保存更新后的模型以进行下一轮评分
&#39;epoch&#39;: epoch+1,
&#39;step&#39;: i,
&#39;num_edges&#39; : 48,
&#39;noise_level&#39;: 0.2,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
}, &quot;../ProteinMPNN/vanilla_model_weights/model-DPO.pt&quot;)
print(loss)
cur_pref=[]
cur_dispref=[]

简而言之，我偏爱和不喜欢的回答的评分必须在单独的脚本中完成，这意味着我必须在每个批次之后保存更新后的模型，以便在下一轮评分中加载。但正如我所提到的，模型权重不会改变，参考模型和目标模型返回的分数总是相同的。任何有助于解决此问题的帮助都将不胜感激。
我已经检查以确保模型参数已正确初始化，其中 require_grad=True。它们在训练之前也没有梯度（list(model.parameters())[0].grad = None）。我还检查以确保我没有覆盖更新的模型权重，或者在评分期间意外加载原始权重。我仔细检查了我的损失函数，并尝试将损失和学习率设置为任意高值以强制更新权重。但是，评分没有发生变化。反向调用后的模型参数梯度仍然为 None，我不知道为什么。如前所述，所有模型参数都使用 require_grad=True 进行初始化。]]></description>
      <guid>https://stackoverflow.com/questions/78664372/pretrained-model-weights-not-updating-during-dpo-training</guid>
      <pubDate>Mon, 24 Jun 2024 19:48:48 GMT</pubDate>
    </item>
    <item>
      <title>mlflow 在记录图像时不会自动记录工件</title>
      <link>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</link>
      <description><![CDATA[我正在使用 Dagshub 免费 MLFlow 服务器进行实验。我偶然发现了一些奇怪的行为。当我运行一个简单的 Keras 模型时，它会使用 MLFlow Autolog 进行拟合，如下所示：
mlflow.tensorflow.autolog()
dagshub.init(&#39;my_experiment_name&#39;,
&#39;my_user_name&#39;,
mlflow=True)

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))
mlflow.end_run()

这会产生预期的行为。我可以在工件和指标中看到模型。

但是，当我尝试添加包含训练准确率和损失的图形时，工件仅包含图像。
mlflow.tensorflow.autolog()
dagshub.init(&#39;my_experiment_name&#39;,
&#39;my_user_name&#39;,
mlflow=True)

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))

##_________ 有问题的部分
fig, ax = plt.subplots(1,2,figsize=(10,4))
ax[0].plot(history.history[&#39;accuracy&#39;], label=&#39;Accuracy&#39; )
ax[0].plot(history.history[&#39;val_accuracy&#39;], label=&#39;Val Accuracy&#39; )
ax[0].set_title(&#39;Accuracy&#39;)
ax[0].legend(loc=&#39;best&#39;)
ax[1].plot(history.history[&#39;loss&#39;], label=&#39;Loss&#39; )
ax[1].plot(history.history[&#39;val_loss&#39;], label=&#39;Val Loss&#39; )
ax[1].set_title(&#39;Loss&#39;)
ax[1].legend(loc=&#39;best&#39;)
mlflow.log_figure(fig,&#39;training_history.png&#39;)
# _________

mlflow.end_run()

模型工件不存在。指标也没有记录。
我是否遗漏了一些简单的东西？

请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</guid>
      <pubDate>Mon, 24 Jun 2024 17:12:13 GMT</pubDate>
    </item>
    <item>
      <title>单头 Transformer 模型，输出和目标张量大小不匹配</title>
      <link>https://stackoverflow.com/questions/78663722/single-headed-transformer-model-output-and-target-tensor-sizes-dont-match</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78663722/single-headed-transformer-model-output-and-target-tensor-sizes-dont-match</guid>
      <pubDate>Mon, 24 Jun 2024 16:50:02 GMT</pubDate>
    </item>
    <item>
      <title>执行训练过程中遇到错误</title>
      <link>https://stackoverflow.com/questions/78663639/facing-errors-while-executing-the-training-process</link>
      <description><![CDATA[这是我的代码片段：
from transformers import TrainingArguments, Trainer

# 定义训练参数
args = TrainingArguments(
output_dir=&#39;./content/drive/MyDrive/phase ii/result&#39;, # 保存结果的目录
evaluation_strategy=&quot;epoch&quot;, # 在每个 epoch 结束时进行评估
save_strategy=&quot;epoch&quot;, # 在每个 epoch 结束时保存检查点
save_total_limit=3, # 限制检查点的总数
learning_rate=1e-5, # 设置学习率
per_device_train_batch_size=32, # 训练的批次大小
per_device_eval_batch_size=32, # 评估的批次大小
num_train_epochs=10, # 训练 epoch 的数量
report_to=&quot;none&quot;, # 禁用向外部系统报告
load_best_model_at_end=True, # 训练完成后加载最佳模型
metric_for_best_model=&quot;f1&quot; # 用于选择最佳模型的指标
)

执行上述代码时遇到此错误

我该如何解决给定的问题？您能简要解释一下必要的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78663639/facing-errors-while-executing-the-training-process</guid>
      <pubDate>Mon, 24 Jun 2024 16:31:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 和 DETR 优化视频处理</title>
      <link>https://stackoverflow.com/questions/78663466/optimizing-video-processing-with-opencv-and-detr</link>
      <description><![CDATA[我正在编写一个 Python 脚本，使用 OpenCV 处理视频文件，使用预训练的 DETR 模型注释检测到的对象，并保存注释后的视频。问题是处理部分需要 5 分钟，即使是 4 秒的视频。这是我找到的解决方案：
DEVICE = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
CHECKPOINT = &#39;facebook/detr-resnet-50&#39;
CONFIDENCE_THRESHOLD = 0.5
BATCH_SIZE = 4
image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)

id2label = {
0: &#39;potholes&#39;,
1: &#39;pothole&#39;
}

model = DetrForObjectDetection.from_pretrained(&quot;Models/potholes-model&quot;)
model.to(DEVICE)
model.eval()
box_annotator = sv.BoxAnnotator()

def process_video(video_path, output_path):
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
print(&quot;Error: 无法打开视频。&quot;)
return

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

fourcc = cv2.VideoWriter_fourcc(*&#39;mp4v&#39;)
out = cv2.VideoWriter(os.path.join(output_path, &quot;results.mp4&quot;), fourcc, fps, (width, height))

if not out.isOpened():
print(&quot;Error: 无法打开 VideoWriter。&quot;)
return

frames = []
while True:
ret, frame = cap.read()
if not ret:
break

frames.append(frame)
if len(frames) == BATCH_SIZE:
process_and_write_batch(frames, out)
frames = []

if frames:
process_and_write_batch(frames, out)

cap.release()
out.release()
cv2.destroyAllWindows()

我当时尝试处理一批帧，而不是一次处理一个：
def process_and_write_batch(frames, out):
with torch.no_grad():
输入 = image_processor(images=frames, return_tensors=&#39;pt&#39;).to(DEVICE)
输出 = model(**inputs)

target_sizes = torch.tensor([frame.shape[:2] for frame in frames]).to(DEVICE)
结果 = image_processor.post_process_object_detection(
output=outputs,
Threshold=CONFIDENCE_THRESHOLD,
target_sizes=target_sizes
)

for frame, result in zip(frames, results):
try:
detections = sv.Detections.from_transformers(transformers_results=result).with_nms(threshold=0.5)
labels = [f&quot;{id2label[class_id]} {confidence:.2f}&quot; for _, confidence, class_id, _ in detections]
annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections, labels=labels)
out.write(annotated_frame)
except:
out.write(frame)

即使对于短视频，该过程仍然需要很长时间。我可以做些什么来优化该过程？]]></description>
      <guid>https://stackoverflow.com/questions/78663466/optimizing-video-processing-with-opencv-and-detr</guid>
      <pubDate>Mon, 24 Jun 2024 15:46:23 GMT</pubDate>
    </item>
    <item>
      <title>有没有一种方法可以通过比率来测量用于创建第三个特征的两个特征的初始形状值？</title>
      <link>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</link>
      <description><![CDATA[在 Python 机器学习、回归或分类等背景下，我有时会对两个特征进行比率计算以生成第三个特征，并删除前两个特征。
示例：living_surface_ratio = residence_building__total_living_surface / building__footprint_surface

因此，在我的 data_processing_pipe 末尾，我只有 living_surface_ratio 作为一个特征。
我想知道是否有办法测量 residence_building__total_living_surface 和 building__footprint_surface 的形状值，尽管这些特征已被删除，因此不再可用。
也许：
shape(living_surface_ratio) = 0.3 * shape(residential_building__total_living_surface) + 0.7µbuilding__footprint_surface

我考虑过用两个特征，然后只使用比率，但困扰我的是，我并没有真正提供具有特征比率的模型的可解释性。
我看过形状值的实现，但修改起来似乎很复杂。
如果您有任何想法，我很乐意听取您的意见！
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</guid>
      <pubDate>Mon, 24 Jun 2024 13:36:28 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch Vision Transformer 模型中的异常、验证损失、测试准确度和正常准确度计算 [关闭]</title>
      <link>https://stackoverflow.com/questions/78659312/anomaly-in-pytorch-vision-transformer-model-validation-loss-testing-accuracy-a</link>
      <description><![CDATA[我正在尝试使用 PyTorch 为我的个人项目创建一个视觉变换模型。
问题是，当我运行测试代码时，我不确定我是否正确计算了训练损失、验证损失、训练准确率和测试（+ 前 2 名测试）准确率。
这是我的代码：
criterion = nn.CrossEntropyLoss()
optimizer = AdamW(vit.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# 训练循环
train_losses, val_losses, accuracies, top2_accuracies= [], [], [], []
training_start = time.time()

for epoch in range(NUM_EPOCHS):
log_str = write_and_print_str(log_str, f&quot;EPOCH [{epoch+1}/{NUM_EPOCHS}]&quot;)
start = time.time()
vit.train()
running_loss = []
for images, labels in train_dataloader:
images, labels = images.to(device), labels.to(device)
optimizer.zero_grad()
output = vit(images)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
running_loss.append(loss.item())

avg_train_loss = sum(running_loss) / len(running_loss)
train_losses.append(avg_train_loss)
log_str = write_and_print_str(log_str, f&#39;Loss: {avg_train_loss}&#39;)

vit.eval()
val_loss = []
correct = 0
top2_correct = 0
total = 0
with torch.no_grad():
for images, labels in test_dataloader:
images, labels = images.to(device), labels.to(device)
output = vit(images)
loss = criterion(outputs, labels)
val_loss.append(loss.item())
_, predicted = torch.max(outputs.data, 1)
total += labels.size(0)
correct += (predicted == labels).sum().item()

# 计算 top-2 准确率
top2_pred = torch.topk(outputs, 2, dim=1).indices
top2_correct += (top2_pred == labels.unsqueeze(1)).sum().item()
end = time.time()
avg_val_loss = sum(val_loss) / len(val_loss)
accuracy = 100 * correct / total
top2_accuracy = 100 * top2_correct / total

val_losses.append(avg_val_loss)
accuracies.append(accuracy)
top2_accuracies.append(top2_accuracy)

log_str = write_and_print_str(log_str, f&#39;验证损失：{avg_val_loss}，\n准确率：{accuracy}%，\nTop-2 准确率：{top2_accuracy}%\n时间：{round(end-start, 2)}\n\n&#39;)

training_end = time.time()

log_str = write_and_print_str(log_str, f&#39;训练持续时间：{round(training_end - training_start, 2)}\n&#39;)

print(&quot;EPOCHS 已成功保存到文件中&quot;)


我运行了 20 多次，并记录了所有运行广泛的方法。
在所有结果中，验证损失开始超过 1并降低到 0.20。但问题是在这些情况下我的准确率约为 90%，所以我认为我的代码在计算方面出了问题。
为了更详细地说明准确率和损失的数值，以下是我的一些 EPOCH 结果
EPOCH [1/50]
损失：1.4692728799123032
验证损失：1.1625839814995274，
准确率：49.58932238193019%，
Top-2 准确率：75.77002053388091%
时间：29.71

EPOCH [10/50]
损失：0.1079550055715327
验证损失： 0.5106942771059094，
准确率：83.26488706365502%，
Top-2 准确率：97.53593429158111%
时间：25.86

EPOCH [20/50]
损失：0.037730065656293076
验证损失：0.4059527646185774，
准确率：89.52772073921972%，
Top-2 准确率：97.94661190965093%
时间：26.12

EPOCH [30/50]
损失： 0.00011380775267753052
验证损失：0.22308006276955095，
准确率：94.6611909650924%，
Top-2 准确率：99.48665297741273%
时间：24.41

EPOCH [40/50]
损失：3.5449059315886606e-05
验证损失：0.23672400451808548，
准确率：94.76386036960986%，
Top-2 准确率：99.48665297741273%
时间：25.46

EPOCH [50/50]
损失：1.367992779425829e-05
验证损失：0.24671741761443572，
准确率：94.6611909650924%，
Top-2 准确率：99.48665297741273%
时间：25.66

希望您能帮我解决这个问题。我只需要澄清一下我计算损失和准确率指标的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78659312/anomaly-in-pytorch-vision-transformer-model-validation-loss-testing-accuracy-a</guid>
      <pubDate>Sun, 23 Jun 2024 16:52:16 GMT</pubDate>
    </item>
    <item>
      <title>如何验证 Coursera 中的 Jupyter 笔记本分数错误？[关闭]</title>
      <link>https://stackoverflow.com/questions/78655480/how-to-validate-jupyter-notebook-score-error-in-coursera</link>
      <description><![CDATA[课程：监督机器学习：回归和分类
平台：Coursera
提交时练习未正确验证，即使通过了测试用例场景，分数仍为“0”。
Jupyter 笔记本嵌入在 Coursera 应用程序中。
我尝试多次保存文件并重新运行服务器。
我希望文件能够针对给定的解决方案进行验证，从而获得分数。
我已附上已通过的测试用例场景练习。
最终成绩截图
练习 1
练习 2
练习 3
练习 4
练习 5
练习 6]]></description>
      <guid>https://stackoverflow.com/questions/78655480/how-to-validate-jupyter-notebook-score-error-in-coursera</guid>
      <pubDate>Sat, 22 Jun 2024 08:21:29 GMT</pubDate>
    </item>
    </channel>
</rss>