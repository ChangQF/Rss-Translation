<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 12 May 2024 12:25:58 GMT</lastBuildDate>
    <item>
      <title>少数镜头学习中对比损失的距离误差</title>
      <link>https://stackoverflow.com/questions/78467828/errors-in-the-distance-for-contrastive-loss-in-few-shot-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78467828/errors-in-the-distance-for-contrastive-loss-in-few-shot-learning</guid>
      <pubDate>Sun, 12 May 2024 12:19:51 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的张量图</title>
      <link>https://stackoverflow.com/questions/78467708/tensor-diagram-in-python</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;#plot
a=np.linspace(0,1,n)
图，ax=plt.subplots()
ax.plot(a,(U_pred).clone().detach()[0],&#39;g.&#39;,label=&#39;预测&#39;)
ax.plot(a,U_test,&#39;b.&#39;,label = &quot;测试&quot;)
斧头图例()
#ax.set_ylim([-1,1])“”“”

在此处输入图片描述
我画了一个数字张量，本来应该画成蓝色图的形式，但它是点的形式。您认为问题出在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/78467708/tensor-diagram-in-python</guid>
      <pubDate>Sun, 12 May 2024 11:44:17 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Model.fit() 打印训练数据集</title>
      <link>https://stackoverflow.com/questions/78467703/tensorflow-model-fit-printing-training-dataset</link>
      <description><![CDATA[我是 Tensorflow 和 ML 领域的新手，我对 Tensorflow 的体验很奇怪。
我正在尝试训练 Tensorflow 简单序列模型，但在训练阶段，它不会显示准确性或某些进度条，而是打印整个训练数据集。
这是我的完整代码
导入tensorflow为tf
从数据导入 get_data
从 sklearn 导入 model_selection

数据 = get_data(&#39;2Y&#39;)

x_train, x_test, y_train, y_test = model_selection.train_test_split(data.get_values([&#39;温度&#39;,&#39;风&#39;,&#39;湿度&#39;]), data.get_values(&#39;雨&#39;), test_size=0.1, shuffle=False)
模型 = tf.keras.Sequential([
    tf.keras.layers.Dense(8),
    tf.keras.layers.Dense(8)
]）

模型.编译(
    优化器=“亚当”，
    损失=“binary_crossentropy”，
    指标=[“准确度”]
）

model.fit(x_train、y_train、epochs=10、batch_size=64、validation_data=(x_test、y_test)、verbose=2)

我阅读了tensorflow文档，但没有提到在模型拟合期间打印整个数据集之类的事情。我不明白哪里出了问题。并且在 model.fit 之后的任何代码都被丢弃，就像我尝试使用 print() 语句一样，但它们从未运行。]]></description>
      <guid>https://stackoverflow.com/questions/78467703/tensorflow-model-fit-printing-training-dataset</guid>
      <pubDate>Sun, 12 May 2024 11:41:45 GMT</pubDate>
    </item>
    <item>
      <title>情感分析中朴素贝叶斯的特征选择</title>
      <link>https://stackoverflow.com/questions/78467290/feature-selection-for-naive-bayes-in-sentiment-analysis</link>
      <description><![CDATA[在 R 中训练朴素贝叶斯模型时，使用情感分析数据，我想知道哪些单词（特征）有助于模型将评论分类为正面 [1] 或负面 [ 0]。
我知道像 GNB 这样不支持决策树的模型无法立即找到最重要的特征。
无论如何，我尝试使用这个：
库（插入符号）
varImp（朴素贝叶斯模型）

给出了这个输出：
 没有适用于“varImp”的方法应用于类“naiveBayes”的对象

还有这样的方式：
库(rminer)
重要性（NBmodel，数据 = NBtrainer）

给出了这个输出：
重要性（naive_bayes_model，数据 = train_data）
PRED(M, data[(1:L), ]) 中的错误：找不到函数“PRED”

所以我尝试了不同的方法来解决这个问题，正如本文中所指出的，让我们说类似的问题，但接受的答案没有解决任何问题。
你能建议我如何修复这个错误吗？或者如何实现另一种查找特征重要性的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78467290/feature-selection-for-naive-bayes-in-sentiment-analysis</guid>
      <pubDate>Sun, 12 May 2024 09:18:26 GMT</pubDate>
    </item>
    <item>
      <title>这个模型是否过度拟合或者数据质量太差？</title>
      <link>https://stackoverflow.com/questions/78467117/is-this-model-overfitting-or-is-the-quality-of-the-data-to-bad</link>
      <description><![CDATA[我目前正在从事一个机器学习项目。这是一个监督学习问题。我的目标是预测动物的给定数据（饲养、大小、重量……）成分（能量、维生素等……）。首先，我清理了数据并使用 LabelEncoding 对分类特征进行了编码。我选择随机森林作为算法，因为我读到树对于混合数据（分类和继续）很有用。所以我用几个参数训练了模型，我注意到我得到了很好的训练结果，但测试结果非常糟糕。在我看来，这表明过度拟合。该模型正在学习噪声。所以我知道我有两个选择：更多数据和降低模型的复杂性。但我尝试过PCA，删除了一些功能，更改了超参数（max_深度为15）。但这些行动都没有帮助。我已经减少了 max_深度，但随后我得到了更高的训练误差，但仍然是一个巨大的高测试误差。
那么这里的问题可能是什么？该模型是否过度拟合或数据噪音太大？
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn.metrics 导入mean_absolute_error
从 sklearn.metrics 导入mean_squared_error, r2_score
从 sklearn.pipeline 导入 make_pipeline
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.neural_network 导入 MLPRegressor
从 sklearn.decomposition 导入 KernelPCA

参数网格 = {
    &#39;n_estimators&#39;: [i for i in range(50, 500, 50)],
    &#39;最大深度&#39;: [i 为范围 (5, 20, 5) 内的 i],
}

估计器 = RandomForestRegressor()
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=52)
X_train,scalerX = 归一化(X_train)
Y_train,scalerY = 归一化(Y_train)
X_test = scalerX.transform(X_test)
Y_test = scalerY.transform(Y_test)



gridModel = GridSearchCV(估计器=估计器，param_grid=param_grid，n_jobs=4，cv=5，评分=&#39;neg_mean_squared_error&#39;)
gridModel.fit(X_train,Y_train)


打印（gridModel.best_params_）

best_params：{&#39;max_深度&#39;：15，&#39;n_estimators&#39;：150}
将网格更改为 [i for i in range(5, 50, 5)] 时 best_params: {&#39;max_depth&#39;: 30, &#39;n_estimators&#39;: 50}
y_pred_test = gridModel.predict(X_test)
test_r2_score = r2_score(y_pred=y_pred_test,y_true=Y_test)

y_pred_train = gridModel.predict(X_train)
train_r2_score = r2_score(y_pred=y_pred_train,y_true=Y_train)

print(&quot;测试结果：&quot;,test_r2_score)
print(&quot;训练结果：&quot;,train_r2_score)

{&#39;最大深度&#39;：15，&#39;n_估计器&#39;：150}
结果测试：-2.952394644421328e+31
结果列车：0.8043381537451035
{&#39;最大深度&#39;：30，&#39;n_估计器&#39;：50}
结果测试：-7.37835882483847e+30
结果列车：0.9286384515560636]]></description>
      <guid>https://stackoverflow.com/questions/78467117/is-this-model-overfitting-or-is-the-quality-of-the-data-to-bad</guid>
      <pubDate>Sun, 12 May 2024 08:12:48 GMT</pubDate>
    </item>
    <item>
      <title>我们正在尝试训练 yolov7 来检测自定义数据集上的徽标，我们收到以下错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78467046/we-are-trying-to-train-yolov7-for-detecting-logos-on-a-custom-dataset-we-are-get</link>
      <description><![CDATA[train：扫描“data\train\labels.cache”图像和标签... 0 个已找到，9 个缺失，0 个为空，0 个已损坏：100%|█| 9/9 [0
回溯（最近一次调用最后一次）：
  文件“C:\YOLO\envs\yolov7_custom\yolov7-custom\train.py”，第 616 行，在  中
    火车（hyp、opt、设备、tb_writer）
  文件“C:\YOLO\envs\yolov7_custom\yolov7-custom\train.py”，第 245 行，火车中
    数据加载器，数据集= create_dataloader（train_path，imgsz，batch_size，gs，opt，
  文件“C:\YOLO\envs\yolov7_custom\yolov7-custom\utils\datasets.py”，第 69 行，在 create_dataloader 中
    数据集= LoadImagesAndLabels（路径，imgsz，batch_size，
  文件“C:\YOLO\envs\yolov7_custom\yolov7-custom\utils\datasets.py”，第 403 行，在 __init__ 中
    断言 nf &gt; 0 或不增强，f&#39;{prefix}{cache_path} 中没有标签。没有标签就无法训练。请参阅 {help_url}&#39;
断言错误：火车：data\train\labels.cache 中没有标签。没有标签就无法训练。请参阅 https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data

我们尝试通过在不同图像集上多次重新创建相应图像的标签来调试它]]></description>
      <guid>https://stackoverflow.com/questions/78467046/we-are-trying-to-train-yolov7-for-detecting-logos-on-a-custom-dataset-we-are-get</guid>
      <pubDate>Sun, 12 May 2024 07:47:27 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError: 'QuantumCircuit' 对象没有属性 'cnot' 使用 qiskit.QuantumCircuit.cnot 时收到此错误</title>
      <link>https://stackoverflow.com/questions/78466923/attributeerror-quantumcircuit-object-has-no-attribute-cnot-receiving-this-e</link>
      <description><![CDATA[def ans(n, 深度):
    qc = 量子电路(n)
    对于范围（深度）内的 j：
        对于范围 (n) 内的 i：
            param_name = f&#39;theta_{j}_{i}&#39;
            theta_param = 参数(param_name)
            qc.rx(theta_param, i)
            qc.ry(theta_param, i)
            qc.rz(theta_param, i)
    对于范围 (n) 内的 i：
        如果我==n-1：
            qc.cnot(i, 0)
        别的：
            qc.cnot(i, i+1)
    返回质量控制

此方法似乎已被弃用并删除。我在尝试使用来自 此帖子。
我想要新版本的 qiskit 中提供的替代功能]]></description>
      <guid>https://stackoverflow.com/questions/78466923/attributeerror-quantumcircuit-object-has-no-attribute-cnot-receiving-this-e</guid>
      <pubDate>Sun, 12 May 2024 06:52:33 GMT</pubDate>
    </item>
    <item>
      <title>使用混合深度学习算法进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78466722/face-recognition-using-hybrid-deep-learning-algorithms</link>
      <description><![CDATA[如果我们有“准确性”：0.812，“精度”：0.902247191011236，“Sensitivity_recall”：0.8882743362831859，“特异性”：0.09375，“F1_score”：0.895206243032，如何获得混淆矩阵3299
如果我们有 450000 个人脸数据集，应用 MTCNN、ArcFace 和 FaceNet 3 种算法，需要输出混淆矩阵]]></description>
      <guid>https://stackoverflow.com/questions/78466722/face-recognition-using-hybrid-deep-learning-algorithms</guid>
      <pubDate>Sun, 12 May 2024 04:47:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用pytorch计算矩阵和向量的部分积？</title>
      <link>https://stackoverflow.com/questions/78466709/how-to-use-pytorch-to-compute-partial-products-of-a-matrix-and-a-vector</link>
      <description><![CDATA[我想实现一个速度快且 GPU 内存使用量较低的 MoE 模型，但我遇到了可以通过以下方式抽象的问题：
假设 A[1:1024][1:1024] 是一个矩阵，b[1:1024] 是一个向量，c[1:1024] 是一个稀疏向量，最多有 100 个非零元素。我需要计算由 d[i]=(A*b)[i] * c[i] 定义的 d[1:1024]。
我需要，对于一个固定的 A 和许多对 (b,c)，计算 d。我应该如何利用cuda的并行能力在pytorch下计算d？
取 A 的右行组成一个新矩阵？这可能很快，但会使用大量不必要的 GPU 内存，并使最大可能的批量大小太小。
使用 for 循环在右侧索引上一一计算 d 的元素？这可以节省 GPU 内存，但速度太慢。
如何既节省内存又提高速度？
这个 cuda 函数似乎可以完成工作。但我想在 pytorch 中工作，而不是 cuda 编程。
__global__ voidcomputeDots(float *A, float *b, float *c, float *out, int *mask) {
    int idx = blockIdx.x * 100 + threadIdx.x;
    if (threadIdx.x &lt; 100) {
        float *a = A + mask[idx] * VECTOR_SIZE；
        for (int i = 0; i &lt; VECTOR_SIZE; i++) {
            out[idx] += a[i] * b[idx+i];
        }
        输出[idx] *= c[idx];
    }
}
]]></description>
      <guid>https://stackoverflow.com/questions/78466709/how-to-use-pytorch-to-compute-partial-products-of-a-matrix-and-a-vector</guid>
      <pubDate>Sun, 12 May 2024 04:36:53 GMT</pubDate>
    </item>
    <item>
      <title>基本线性回归不预测 y=x+5 模式</title>
      <link>https://stackoverflow.com/questions/78466532/basic-linear-regression-not-predicting-y-x5-pattern</link>
      <description><![CDATA[然而，我的成本不断下降，但我的线性回归无法预测简单的模式 y=x+5。
我尝试了 alpha 的所有值，得到的最低成本是 3.00。
&lt;前&gt;&lt;代码&gt;x=[]
y=[]
对于范围（1000）内的 i：
    x.追加(i)
    y.追加(i+5)
def计算成本（x，y，w，b）：
    m=len(x)
    总成本=0
    成本=0
    对于范围 (m) 内的 i：
        f_wb = w*x[i]+b
        成本 += (f_wb - y[i])**2
    总成本 = 成本/(2*m)
    返回总成本
defgradient_descent(x,y,iter=200):
    m=len(x)
    w=0
    b=0
    阿尔法=0.0000001
    对于范围内的 i（iter）：
        dj_dw,dj_db=计算梯度(x,y,w,b)
        w-=alpha*dj_dw
        b-=alpha*dj_db
        如果（i%10==0）：
            成本=compute_cost(x,y,w,b)
            print(f“迭代 {i} 时的成本是 {cost}”)
    返回w,b
def Compute_gradient(x,y,w,b):
    m=len(x)
    dj_dw=0
    dj_db=0
    对于范围 (m) 内的 i：
        y_预测=w*x[i]+b
        dj_dw +=(y_predict-y[i])*x[i]
        dj_db +=(y_predict-y[i])
    dj_dw /=m
    dj_db /=m
    返回 dj_dw,dj_db
w,b=梯度下降(x,y)
q=w*10+b
打印（q）


这里是输出
迭代 0 时的成本为 157869.16787934472
第 10 次迭代的成本为 80221.1618112745
第 20 次迭代的成本为 40765.111675992295
第 30 次迭代的成本为 20715.91825636264
第 40 次迭代的成本为 10528.123091503312
迭代 50 时的成本为 5351.297861860537
迭代 60 时的成本为 2720.746400189429
迭代 70 时的成本为 1384.058239492339
迭代 80 时的成本为 704.8336487635617
迭代 90 时的成本为 359.6925315439946
迭代 100 时的成本为 184.31255755759477
迭代 110 时的成本为 95.19499412720666
迭代 120 时的成本为 49.910803477032324
迭代 130 时的成本为 26.900098542186733
迭代 140 时的成本为 15.20744043888423
迭代 150 时的成本为 9.265933534732453
迭代 160 时的成本为 6.246816032905587
迭代 170 时的成本为 4.712681193345336
迭代 180 时的成本为 3.93312529712553
迭代 190 时的成本为 3.537001070649068
10.064986594681566

出了什么问题？
代码是用 python 编写的，具有计算成本之类的函数来计算成本，梯度下降也是如此。我使用损失函数作为均方误差]]></description>
      <guid>https://stackoverflow.com/questions/78466532/basic-linear-regression-not-predicting-y-x5-pattern</guid>
      <pubDate>Sun, 12 May 2024 02:05:07 GMT</pubDate>
    </item>
    <item>
      <title>使用torch音频库创建数据集时出错</title>
      <link>https://stackoverflow.com/questions/78466420/error-when-using-torch-audio-library-to-create-a-data-set</link>
      <description><![CDATA[我正在学习 YT 课程，研究使用 torch 音频的城市 8k 数据集。作者编写了完全相同的代码，但在我收到错误时能够获得输出。
以下是我收到的错误：
运行时错误：找不到适当的后端来处理 uri C:\Users\hbhavnag\Documents\Hussain\ASU\collision detector\urban sound\UrbanSound8K\audio\5\100263-2-0-121。 wav 和格式 无。

以下是我的代码：
`从 torch.utils.data 导入数据集
将 pandas 导入为 pd
导入火炬音频
导入操作系统
类 UrbanSoundDataset（数据集）：
def __init__(self,annotation_file,audio_dir):
    self.annotations = pd.read_csv(annotation_file)
    self.audio_dir = 音频_dir

def __len__(自身):
    返回 len(self.annotations)

def __getitem__(自身，索引)：
    audio_sample_path = self._get_audio_sample_path(索引)
    标签 = self._get_audio_sample_label(索引)
    信号，sr = torchaudio.load（audio_sample_path）
    返回信号、标签

def _get_audio_sample_path（自身，索引）：
    Fold = f“fold{self.annotations.iloc[index,5]}”
    路径 = os.path.join(self.audio_dir, 折叠, self.annotations.iloc[index,0])
    返回路径

def _get_audio_sample_label（自身，索引）：
    返回 self.annotations.iloc[index,6]

如果 __name__ == “__main__”：
    注释_文件 = r“C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\metadata\UrbanSound8K.csv”
    audio_dir = r&quot;C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\audio&quot;
    usd = UrbanSoundDataset（注释文件，音频目录）
    print (f“数据集中有 {len(usd)} 个样本”)

信号，标签=美元[2]`

我尝试查找 torch audio 的文档，但不确定是否有任何内容可以直接帮助我，我假设存在一些版本兼容性问题。
我使用的是 Windows。]]></description>
      <guid>https://stackoverflow.com/questions/78466420/error-when-using-torch-audio-library-to-create-a-data-set</guid>
      <pubDate>Sun, 12 May 2024 00:34:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HuggingFace Transformers Pipeline 在每个提示（如 vLLM）中生成多个文本补全而不触发错误？</title>
      <link>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</link>
      <description><![CDATA[我正在使用 HuggingFace Transformers Pipeline 库为给定提示生成多个文本完成。我的目标是利用像 GPT-2 这样的模型来生成不同的可能完成结果，例如 vLLM 中的默认值。但是，当我尝试指定 max_length 和 num_return_sequences 等参数时，我遇到了未使用 model_kwargs 的问题。
这是我正在使用的代码片段：
复制代码
从变压器导入 GPT2Tokenizer、GPT2LMHeadModel、管道
从输入导入列表，字典

def process_prompts(提示: List[str], 模型: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions: int = 3) -&gt;列表[列表[str]]：
    device = 0 if model.device.type == &#39;cuda&#39; else -1
    text_generator = pipeline(“文本生成”, model=model, tokenizer=tokenizer, device=device)
    输出 = []

    对于提示中的提示：
        尝试：
            结果= text_generator（提示，max_length = 50，num_return_sequences = num_completions，num_beams = num_completions）
            完成 = [结果[&#39;生成的文本&#39;] 结果中的结果]
            输出.追加（完成）
        除了异常 e：
            print(f&quot;处理错误提示{prompt}: {str(e)}&quot;)

    返回输出

如果 __name__ == “__main__”：
    tokenizer = GPT2Tokenizer.from_pretrained(“gpt2”)
    模型 = GPT2LMHeadModel.from_pretrained(“gpt2”)
    model.to(“cuda” if torch.cuda.is_available() else “cpu”)

    example_prompts = [“你好，你好吗？”]
    processed_outputs = process_prompts(example_prompts, model, tokenizer, num_completions=3)
    对于processed_outputs中的输出：
        打印（输出）

还有：
 results = text_generator(prompt, max_length=50, num_return_sequences=num_completions)

当我运行此程序时，出现以下错误：
模型不使用以下`model_kwargs`：[&#39;max_len&#39;]
注意：我知道生成参数中的拼写错误也可能触发此警告，但我已经检查并重新检查了参数名称。

和
 引发 ValueError(
ValueError：没有波束搜索的贪婪方法不支持不同于 1 的 `num_return_sequences`（得到 4）。

什么可能导致此错误，以及如何修复它以使用模型有效地生成多个完成？
交叉：https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without -触发错误/86297]]></description>
      <guid>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</guid>
      <pubDate>Sun, 12 May 2024 00:06:07 GMT</pubDate>
    </item>
    <item>
      <title>通过 k 均值和分类进行聚类</title>
      <link>https://stackoverflow.com/questions/78466355/clustering-by-k-means-and-classification</link>
      <description><![CDATA[任务分配：
分类是根据类别进行的，这绝对是无监督的（有 5 个类别需要帮助）。
我被分配了这个任务，其中我有一个包含多列形式的数据集.. .
&lt;前&gt;&lt;代码&gt;` D1_16 D1_17 D1_18 D1_19 D1_20 D1_23 D1_24 D1_25 D1_26 D1_27 \
1 1 1 1 1 1 1 1 1 1 1
2 1 1 1 1 1 0 1 0 1 1
3 0 0 0 0 0 0 0 0 0 0
4 0 1 0 1 0 0 1 1 1 0
5 0 0 0 0 0 0 0 0 0 0
……………………………………
29502 0 0 0 0 0 0 0 0 0 0
29504 0 0 0 0 0 0 0 0 0 0
29505 0 0 0 0 0 0 0 0 0 0
29506 0 0 0 0 0 0 0 0 0 0
29507 0 0 0 0 0 0 0 0 0 0

       ... D68_29 D68_30 D68_31 D68_32 D68_33 D68_34 D68_35 D68_36 \
1 ... 0 0 1 0 0 0 0 0
2 ... 0 0 0 0 0 1 0 0
3 ... 0 0 0 0 0 0 0 0
4 ... 0 0 0 0 0 0 0 0
5 ... 0 0 0 0 0 0 0 0
………………………………
29502 ... 0 0 0 0 0 0 0 0
29504 ... 0 0 0 0 0 0 0 0
29505 ... 0 0 0 0 0 0 0 0
29506 ... 0 0 0 0 0 0 0 0
29507 ... 0 0 0 0 0 0 0 1`

我是否正确理解，首先需要进行聚类，例如根据k-means并选择一个合适的类，在此基础上我将数据分为两部分并标记“1”和“0”例如，然后对树进行分类？或者您需要以不同的方式执行聚类。老实说，我不太明白我要做什么，如果有任何想法，我将不胜感激。
我的程序：
best_score = -1
最佳_k = 0

对于范围 (2, 10) 内的 k：
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(df)
    Silhouette_avg = Silhouette_score(df, kmeans.labels_)
    

    如果 Silhouette_avg &gt;最佳得分：
        最佳得分 = 轮廓平均数
        最佳_k = k

kmeans = KMeans(n_clusters=best_k)
kmeans.fit(df)
df[&#39;cluster&#39;] = kmeans.labels_

best_cluster = np.argmax(np.bincount(kmeans.labels_))
df[&#39;目标&#39;] = np.where(df[&#39;簇&#39;] == best_cluster, 0, 1)

X_train, X_test, y_train, y_test = train_test_split(df.drop([&#39;集群&#39;, &#39;目标&#39;], axis=1), df[&#39;目标&#39;], test_size=0.2, random_state=42)

clf = 决策树分类器()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
准确度=准确度_分数（y_test，y_pred）

数据集：https://filetransfer.io/data-package/Aiawe648#link
我的所有程序： https://onecompiler.com/python/42cxfy2vz
有关聚类和分类的建议]]></description>
      <guid>https://stackoverflow.com/questions/78466355/clustering-by-k-means-and-classification</guid>
      <pubDate>Sat, 11 May 2024 23:54:57 GMT</pubDate>
    </item>
    <item>
      <title>核逻辑回归 - 错误的预测</title>
      <link>https://stackoverflow.com/questions/78463519/kernel-logistic-regression-wrong-prediction</link>
      <description><![CDATA[我正在研究内核逻辑回归函数，但它们没有返回正确的预期预测。
它还会抛出指数溢出警告，目前已被抑制
&lt;前&gt;&lt;代码&gt;
    将 numpy 导入为 np
    进口警告

    warnings.filterwarnings(&#39;忽略&#39;)

    def monomial_kernel(d):
        def k(x, y, d=d):
            phi_x_y = 0
            prod_xy = np.dot(x.T,y)

            对于范围 (d+1) 中的 n：
                phi_x_y += (prod_xy ** n)

            返回 phi_x_y
            
        返回 k

    def rbf_kernel(西格玛):
        def k(x, y, 西格玛=西格玛):
            分子 = np.linalg.norm(x - y) **2
            分母 = 2 * (西格玛 ** 2)
            
            return np.exp(-分子/分母)

        返回 k

    定义 sigmoid(z):
        如果类型（z）== np.ndarray：
            z = z[0]
        尝试：
            返回 1 / (1 + np.exp(-z))
        除了：
            打印（z）

    def Logistics_regression_with_kernel(X, y, k, alpha, 迭代):

        n_samples, _ = X.shape
        偏差 = 0
        kernel_matrix = np.zeros((n_samples, n_samples))
        beta = np.zeros(n_samples)

        #创建核矩阵
        对于范围内的 i（n_samples）：
            对于 j 在范围内（n_samples）：
                kernel_matrix[i][j] = k(X[i], X[j])
        
        对于 _ 在范围内（迭代）：
            对于范围内的 i（n_samples）：
                总计 = 0
                对于 j 在范围内（n_samples）：
                    总计 += beta[j] * kernel_matrix[i][j]
                总计 += 偏差
                sigmoid_value = sigmoid(总计)
                t = y[i]

                beta += kernel_matrix[i] * alpha * (t - sigmoid_value)
                        
                偏差 += (alpha * (t - (sigmoid_value)))

        def 模型(x, beta=beta, 偏差=bias, k=k, ref=X):
            z = sum([k(ref[i], x) * beta[i] for i in range(ref.shape[0])]) + 偏差

            sig = sigmoid(z)
            # 打印（签名）
            回程(sig)
        返回模型



由于某种原因，它无法正确学习以下测试用例：
 def test4():
        
    f = lambda x, y, z, w: int(x*y*z - y**2*z*w/4 + x**4*w**3/8- y*w/2 &gt;= 0)

    训练示例 = [
        ([0.254, 0.782, 0.254, 0.569], 0),
        ([0.237, 0.026, 0.237, 0.638], 0),
        ([0.814, 0.18, 0.814, 0.707], 1),
        ([0.855, 0.117, 0.855, 0.669], 1),
        ([0.776, 0.643, 0.776, 0.628], 1),
        ([0.701, 0.71, 0.701, 0.982], 0),
        ([0.443, 0.039, 0.443, 0.356], 1),
        ([0.278, 0.105, 0.278, 0.158], 0),
        ([0.394, 0.203, 0.394, 0.909], 0),
        ([0.83, 0.197, 0.83, 0.779], 1),
        ([0.277, 0.415, 0.277, 0.357], 0),
        ([0.683, 0.117, 0.683, 0.455], 1),
        ([0.421, 0.631, 0.421, 0.015], 1)
    ]

    X, y = 地图(np.array, zip(*training_examples))

    h =logistic_regression_with_kernel(X, y, monomial_kernel(10), 0.01, 500)

    测试示例 = [
        ([0.157, 0.715, 0.787, 0.644], 0),
        ([0.79, 0.279, 0.761, 0.886], 1),
        ([0.903, 0.544, 0.138, 0.925], 0),
        ([0.129, 0.01, 0.493, 0.658], 0),
        ([0.673, 0.526, 0.672, 0.489], 1),
        ([0.703, 0.716, 0.088, 0.674], 0),
        ([0.276, 0.174, 0.69, 0.358], 1),
        ([0.199, 0.812, 0.825, 0.653], 0),
        ([0.332, 0.721, 0.148, 0.541], 0),
        ([0.51, 0.956, 0.023, 0.249], 0)
    ]
    print(f&quot;{&#39;x&#39;: ^30}{&#39;预测&#39;: ^11}{&#39;true&#39;: ^6}&quot;)
    对于 test_examples 中的 x、y：
        print(f&quot;{str(x) : ^30}{int(h(x)) : ^11}{y : ^6}&quot;)
    # x 预测为真
    # [0.157, 0.715, 0.787, 0.644] 0 0
    # [0.79, 0.279, 0.761, 0.886] 1 1
    # [0.903, 0.544, 0.138, 0.925] 0 0
    # [0.129, 0.01, 0.493, 0.658] 0 0
    # [0.673, 0.526, 0.672, 0.489] 1 1
    # [0.703, 0.716, 0.088, 0.674] 0 0
    # [0.276, 0.174, 0.69, 0.358] 1 1
    # [0.199, 0.812, 0.825, 0.653] 0 0
    # [0.332, 0.721, 0.148, 0.541] 0 0
    # [0.51, 0.956, 0.023, 0.249] 0 0

我的结果是：

我尝试将迭代次数增加到 4000，但仍然没有产生正确的结果]]></description>
      <guid>https://stackoverflow.com/questions/78463519/kernel-logistic-regression-wrong-prediction</guid>
      <pubDate>Sat, 11 May 2024 05:52:43 GMT</pubDate>
    </item>
    <item>
      <title>移动网络准确率较高，但 val_accuracy 趋于稳定</title>
      <link>https://stackoverflow.com/questions/78412765/mobile-net-high-accuracy-but-val-accuracy-is-plateauing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78412765/mobile-net-high-accuracy-but-val-accuracy-is-plateauing</guid>
      <pubDate>Wed, 01 May 2024 08:33:18 GMT</pubDate>
    </item>
    </channel>
</rss>