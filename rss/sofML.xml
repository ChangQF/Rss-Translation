<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 17 Feb 2025 03:23:47 GMT</lastBuildDate>
    <item>
      <title>Pytorch多类分类网络的异常损失和低精度</title>
      <link>https://stackoverflow.com/questions/79443990/abnormal-loss-and-low-accuracy-of-pytorch-multi-class-classification-network</link>
      <description><![CDATA[我是机器学习的新手，我很困惑为什么我的代码不起作用。数据集来自 https://www.kaggle.com/datasets/ yasserh/wine质量数据/数据。 （该功能标签应该是0-10，但只有7个独特的类，3至9，因此我只是设置[&#39;Quality&#39;] -3，因此所有内容均为0至6）。当我运行训练模型时，我看到损失异常高（整个时间为1.8至1.9）；我正在同时评估测试集上的模型，但准确性始终较低，大约为.2 -.3。当我在训练数据本身上测试HTE模型时，这是相同的精度（〜 -.24 -.25）。
我尝试重塑y_train和y_test集，将学习率更改为0.1至0.001，更改批次大小，在2-5个隐藏层之间进行测试，Min-Max归一化，不正常化，并标记了编码功能列的标签。但是，训练损失和测试准确性始终保持在相同的异常范围内。我不知道我在哪里做错了什么。
我的代码在：
。
供参考，这是我的训练循环：
  lose_fn = nn.crossentropyloss（）
优化器= Optim.Adam（model.parameters（），lr = 0.01）#lr可选，但通常更好地指定

DEF MULTICLASSNN_TRAIN（）：
        n_epochs = 100
        batch_size = 10
    
        对于范围（n_epochs）的时期：
            model.train（）
            total_loss = 0
            对于我的范围（0，len（x_train），batch_size）：
                xbatch = x_train [i：i+batch_size]
                ybatch = y_train [i：i+batch_size] .type（torch.long）
            
                优化器.zero_grad（）
                输出=模型（xbatch）
                损失= lose_fn（输出，ybatch）
                loss.backward（）
                优化器.step（）
            
                total_loss += lose.item（）
            
            avg_loss = total_loss /（len（x_train）// batch_size）
            打印（f&#39;epoch {epoch+1}，平均损失：{avg_loss：.4f}&#39;）
        
            ＃在每个时期之后对测试集进行评估
            model.eval（）
            使用Torch.no_grad（）：
                test_outputs =模型（x_test）
                test_loss = loss_fn（test_outputs，
                 y_test.type（torch.long））
                预测= torch.argmax（test_outputs，dim = 1）
                精度=（预测== y_test）.float（）。平均（）
                打印（f&#39;test损失：{test_loss：.4f}，准确性：                 
                {准确性：.4f}&#39;）
 
帮助您将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79443990/abnormal-loss-and-low-accuracy-of-pytorch-multi-class-classification-network</guid>
      <pubDate>Sun, 16 Feb 2025 21:44:28 GMT</pubDate>
    </item>
    <item>
      <title>将ADASYN应用于不平衡数据时，SVM分类精度会降低[关闭]</title>
      <link>https://stackoverflow.com/questions/79442860/svm-classification-accuracy-lowers-when-applying-adasyn-to-imbalanced-data</link>
      <description><![CDATA[我有一个使用台湾信用数据集预测贷款默认值的SVM模型。当我应用Adasyn时，该模型的总体性能下降了。 Adasyn是 *一种过采样方法，它为少数族裔类生成合成样本，平衡数据集并提高分类精度。同样，尽管存在不平衡，模型的总体准确性仍会下降。
无adasyn的代码：
 导入numpy作为np
导入大熊猫作为pd
来自sklearn.model_selection导入train_test_split
来自Sklearn.svm导入SVC
来自Sklearn.metrics Import Classification_Report，Confusion_matrix，roc_curve，auc
导入matplotlib.pyplot作为PLT
从sklearn.prepercorsing进口标准标准
进口海洋作为SNS
进口时间


＃加载数据集
df = pd.read_csv（r&#39;c：\ users \ adria \ downloads \ adriansvm_simulation \ taiwanese Credit Dataset \ Taiwan_credit1000_dataset.csv&#39;）

＃选择功能和目标变量
x = df [[&#39;x1&#39;，&#39;x2&#39;，&#39;x3&#39;，&#39;x4&#39;， 
        &#39;x5&#39;，&#39;x6&#39;，&#39;x7&#39;，&#39;x8&#39;， 
        &#39;x9&#39;，&#39;x10&#39;，&#39;x11&#39;，&#39;x12&#39;，， 
        &#39;x13&#39;，&#39;x14&#39;，&#39;x15&#39;，&#39;x16&#39;，
        &#39;x17&#39;，&#39;x18&#39;，&#39;x19&#39;，&#39;x20&#39;，，
        &#39;x21&#39;，&#39;x22&#39;，&#39;x23&#39;]＃包括新功能
y = df [&#39;y&#39;]

＃将数据集分为培训和测试集
x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.3，andural_state = 42）

＃标准化功能
sualer = StandardScaler（）
X_TRAIN_SCALED = SCALER.FIT_TRANSFORM（x_train）
X_TEST_SCALED = SCALER.TRANSFORM（x_test）

＃用默认的超参数定义SVM模型
svm_model = svc（kernel =&#39;rbf&#39;，c = 1.0，gamma =&#39;scale&#39;，andural_state = 42）

＃训练SVM模型
start_time = time.time（）
svm_model.fit（x_train_scaled，y_train）
end_time = time.time（）

＃评估模型
y_pred = svm_model.predict（x_test_scaled）

＃混乱矩阵
cm = Confusion_matrix（y_test，y_pred）
打印（“混乱矩阵：\ n＆quot”，cm）

＃分类报告
打印（分类报告：\ n＆quort; classification_report（y_test，y_pred））

alapsed_time = end_time -start_time
打印（F＆quot“训练SVM模型的时间：{elapsed_time：.4f}秒”;）
 
输出为：
 混乱矩阵：
 [[228 10]
 [57 5]]
分类报告：
               精确召回F1得分支持

           0 0.80 0.96 0.87 238
           1 0.33 0.08 0.13 62

    准确性0.78 300
   宏公平0.57 0.52 0.50 300
加权公平0.70 0.78 0.72 300

训练SVM型号花费的时间：0.0140秒
AUC：0.6731499051233395
 
与adasyn：的代码
 来自imblearn.over_sampling导入adasyn 

＃应用adasyn平衡培训数据
adasyn = adasyn（Random_State = 42）
x_train_resampled，y_train_resampled = adasyn.fit_resample（x_train，y_train）

＃在Adasyn之后检查课程分布
print（adasyn之后的“类发行”：\ n＆quort; pd.series（y_train_resampled）.value_counts（））

＃标准化功能
sualer = StandardScaler（）
X_TRAIN_SCALED = SCALER.FIT_TRANSFORM（X_TRAIN_RESAMPLED）
X_TEST_SCALED = SCALER.TRANSFORM（x_test）

＃使用RBF内核定义SVM模型
svm_model = svc（kernel =&#39;rbf&#39;，c = 1.0，gamma =&#39;scale&#39;，andural_state = 42）

＃训练SVM模型
start_time = time.time（）
svm_model.fit（x_train_scaled，y_train_resampled）
end_time = time.time（）

＃评估模型
y_pred = svm_model.predict（x_test_scaled）

＃混乱矩阵
cm = Confusion_matrix（y_test，y_pred）
打印（“混乱矩阵：\ n＆quot”，cm）
 
输出为：
 原始类分布：
 y
0 786
1 214
名称：count，dtype：int64
Adasyn之后的课堂分布：
 y
1 571
0 550
名称：count，dtype：int64
混乱矩阵：
 [[155 81]
 [24 40]]
分类报告：
               精确召回F1得分支持

           0 0.87 0.66 0.75 236
           1 0.33 0.62 0.43 64

    准确性0.65 300
   宏公平0.60 0.64 0.59 300
加权公平0.75 0.65 0.68 300

训练SVM型号的时间：0.0389秒
AUC：0.676708156779661
 
 fyi：我需要从向模型添加Adasyn至少提高准确性2％。我研究了修复数据不平衡可能导致准确性提高？
我说的是总体准确性，而不是AUC（曲线下的区域）。
无adasyn的代码-0.78 acc 
与Adasyn的代码-0.65 ACC ]]></description>
      <guid>https://stackoverflow.com/questions/79442860/svm-classification-accuracy-lowers-when-applying-adasyn-to-imbalanced-data</guid>
      <pubDate>Sun, 16 Feb 2025 09:12:01 GMT</pubDate>
    </item>
    <item>
      <title>使用numpy [封闭]实现感知器</title>
      <link>https://stackoverflow.com/questions/79442381/implementing-a-perceptron-using-numpy</link>
      <description><![CDATA[当使用符号z = xw+b时，我正在尝试使用numpy在python中实现一个感知器。在研究ML时，我确实看到Z = WX+B也很常见，尤其是在谈论神经网络时。问题在于矩阵的尺寸不累加，我尝试按照网络上的一些答案，但输出没有正确的尺寸。我也尝试询问chatgpt，但它仅在z = xw+b符号之后实现了代码。
这是我用于z = xw+b的代码：
 导入numpy作为NP

n_inpts = 10
in_feats = 5
n_hidden = 8
out_feats = 1

x = np.random.randn（n_inpts，in_feats）

w_x = np.random.randn（in_feats，n_hidden）

bias_h = np.random.randn（1，n_hidden）
h = np.dot（x，w_x） + bias_h
#H是NXH

relu = lambda x：max（0，x）
v_relu = np.Dectorize（relu）

h = v_relu（h）

w_h = np.random.randn（n_hidden，out_feats）

bias_o = np.random.randn（1，out_feats）

输出= np.dot（h，w_h） + bias_o
 
任何人都可以在使用z = wx+b时给我一个实现的实现吗？
我发现的每个实现都遵循z = xw+b符号。我想这取决于您如何指定X和W矩阵，但到目前为止，我还没有运气找到我的问题的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/79442381/implementing-a-perceptron-using-numpy</guid>
      <pubDate>Sat, 15 Feb 2025 23:25:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用WebAssembly和TensorFlow.js优化浏览器中的实时ML推断，而不会损害性能？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79442256/how-to-optimize-real-time-ml-inference-in-the-browser-using-webassembly-and-tens</link>
      <description><![CDATA[我正在开发一个实时的Web应用程序，该应用程序直接在浏览器中执行机器学习推断（使用自定义CNN模型）。该模型使用Rust（Wasm-Pack）编译为WebAssembly（WASM），我将其与TensorFlow.js集成在一起，以进行预处理和后处理。但是，当WASM模块和TensorFlow.js之间的数据传递时，我遇到了明显的延迟（每次推理200ms），尤其是使用高分辨率图像输入（1080x1080px）。&gt;
采取的步骤：
将Rust模型汇总到WASM，带有opt级= 3。
使用TF.Browser.Frompixels用于图像张量转换。
尝试了共享内存共享但面临安全限制（CORS和COOP标题）的共享arraybuffer。
基准的独立wasm（无tf.js）：〜50ms推理时间。
关键挑战：
内存转移开销：WASM和JS之间的数据序列化/次要化似乎很昂贵。
线程限制：WASM中的Rust的多线程不稳定，而TF.JS WebGL后端与同步斗争。
浏览器兼容性：Safari的WASM SIMD支持不一致。
在保持实时性能的同时，如何最大程度地减少WebAssembly和TensorFlow.js之间的延迟？是否有用于零拷贝数据传输或利用WebGPU进行并行处理的优化模式？]]></description>
      <guid>https://stackoverflow.com/questions/79442256/how-to-optimize-real-time-ml-inference-in-the-browser-using-webassembly-and-tens</guid>
      <pubDate>Sat, 15 Feb 2025 21:17:52 GMT</pubDate>
    </item>
    <item>
      <title>随着神经网络的训练，权重方差如何变化？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79440406/how-does-the-variance-of-weights-change-as-the-neural-network-is-trained</link>
      <description><![CDATA[变化在不同的ANN体系结构中会有所不同吗？
当我培训MLP网络时，我发现训练后的权重比训练之前大。标准偏差的增加是典型发生的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79440406/how-does-the-variance-of-weights-change-as-the-neural-network-is-trained</guid>
      <pubDate>Fri, 14 Feb 2025 19:31:29 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降的theta值不连贯</title>
      <link>https://stackoverflow.com/questions/79440021/theta-values-for-gradient-descent-not-coherent</link>
      <description><![CDATA[我制作了梯度下降代码，但似乎效果很好
 导入numpy作为NP
从随机导入兰特，随机
导入matplotlib。 pyplot作为plt


def calculh（theta，x）：
    h = 0
    h+= theta [0]*x＃w*x
    h += theta [-1]＃ +b
    返回h


Def Calculy（Sigma，h）：
    返回Sigma（H）＃Sigma Peut-Etre Tanh，Signoide等。


Def Erreurj（Sigma Theta）：
    somme = 0
    somme = 1/4*（sigma（theta [1]）** 2+sigma（theta [0]+theta [1]）** 2）
    返回索姆


def梯度（x，y，ysol，sigmaprime，h）：
    返回（（y-ysol）*sigmaprime（h）*x，（y-ysol）*sigmaprime（h）*1）
def Grad（theta）：
    w，b = theta [0]，theta [1]
    #print（theta）
    返回[2*b ** 3+3*b ** 2*w+3*b*w ** 2-2*b+w ** 3-w，b ** 3+3*b ** 2* W+3*B*W ** 2-B+W ** 3-W]
＃ *X对应A 0 OU 1：NOS 2Entrées; *1对应A de b

Def Pasfixe（Theta，Eta，Epsilon，X，Y，YSOL，Sigma，Sigmaprime，H）：
    n = 0
    而np.linalg.norm（渐变（x，y，ysol，sigmaprime，h））＆gt; Epsilon和N＆lt; 10000：
        对于我的范围（len（theta））：
            theta [i] = theta [i]  -  eta*渐变（x，y，ysol，sigmaprime，h）[i]
            h = calculh（theta，x）
            Y = Calculy（Sigma，h）
            n+= 1
            如果theta [i]＆gt; 100：### cas de Divergence
                返回[100,100]，y
    返回Theta，Y

Sigma = Lambda Z：Z ** 2-1
sigmaprime = lambda z：2*z
ETA = 0.1

x = 1
ysol = 0
Listey = []
ListEtheta = []
lst = [[3*random（）*（ -  1）** randint（0,1），3*random（）*（ -  1）** randint（0,1）]在范围内（5000）]
NB = 0
因为我在LST：
        NB+= 1
        如果NB％50 == 0：
            打印（NB）
        theta = i [：]
        h = calculh（theta，x）
        Y = Calculy（Sigma，h）
        calcultheta = pasfixe（theta，eta，10 ** -4，x，y，ysol，sigma，sigmaprime，h）
        listetheta.append（calcultheta [0]）
        listey.append（calcultheta [1]）


对于我的范围（Len（Listey））：
          Listey [i] = round（Listey [i]，2）
打印（Listey）

对于我的范围（Len（ListEtheta））：
      对于J范围（2）的J：
          listetheta [i] [j] = round（listetheta [i] [j]，2）
打印（ListEtheta）

对于我的范围（LEN（LST））：
    如果[[-2,1]]中的[int（listetheta [i] [0]），int（listetheta [i] [1]）]：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listetheta [i] [1]）在[[[2，-1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listetheta [i] [1]）在[[[0，-1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listEtheta [i] [1]）在[[[0,1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif int（listetheta [i] [0]）** 2 +int（listEtheta [i] [1]）** 2＆gt; = 10：
        plt.plot（lst [i] [0]，lst [i] [1]

plt.show（）
 
最后，i做一个具有偏差和权重值的图形，每个点在循环开始时给出的theta（重量，偏置）值的功能都呈上色。
我应该具有 的图形
我试图自己计算梯度，但也没有起作用。我应该得到这样的图]]></description>
      <guid>https://stackoverflow.com/questions/79440021/theta-values-for-gradient-descent-not-coherent</guid>
      <pubDate>Fri, 14 Feb 2025 16:41:23 GMT</pubDate>
    </item>
    <item>
      <title>将数据分类为多个独立类的最佳策略[封闭]</title>
      <link>https://stackoverflow.com/questions/79439543/optimal-strategy-for-classification-data-into-multiple-independent-set-of-classe</link>
      <description><![CDATA[需要分类的数据属于两个独立的类。为简单起见，让我们想象一个可以具有 Shape&gt; Shape 和 color 的项目。形状可以是 square ， circle  三角形，而颜色可以是 red ，绿色或蓝色。每个项目都完全属于这两组类之一（即一个可以具有蓝色三角形）。
问题是，通过机器学习对此类数据进行分类的最佳策略是什么。到目前为止，我可以看到三种方法：

  两个独立的神经网络 
这很简单。一个网络将对形状进行分类，而另一个网络为颜色。但是，这看起来有些效率。

  多类分类 
在这里，网络的输出将是矩阵3x3（形状次数的颜色数），作为所有可能组合的产物。 （单个）结果将确定特定类别作为对这两个正交碱基的投影。这里的问题是，必须拥有非独立的NXM输出类，因此问题不必要。

  多标签分类 
在这里，输出将是向量3+3，并且将允许网络选择多个输出。虽然问题的维度现在要低得多，但通过允许模型选择多种形状和/或多种颜色，我们使模型可以预测无法发生的情况。


因此，问题仍然存在，在这种情况下的最佳策略是什么？是否有一些对这种问题最佳的混合范式？]]></description>
      <guid>https://stackoverflow.com/questions/79439543/optimal-strategy-for-classification-data-into-multiple-independent-set-of-classe</guid>
      <pubDate>Fri, 14 Feb 2025 13:33:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在不同的数据范围内训练Sklearn模型？</title>
      <link>https://stackoverflow.com/questions/79439462/how-to-train-sklearn-model-in-different-dataframes</link>
      <description><![CDATA[我有一个用“ knn”制作的ML模型在Scikit-Learn中，注意到我的数据越多，我的模型就会越准确地说服它的预测。问题是，我有很多数据框架显示了我想预测的同一系统的不同情况。是否可以在那些不同的数据范围内训练模型？因为如果我致电.fit（），则将重置它是以前的培训。
  x_cleaned = x.dropna（）
y_cleaned = y [x_cleaned.index]
x_training，x_test，y_training，y_test = train_test_split（x_cleaned，y__cleaned，test_size = 0.15，Random_State = 0）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79439462/how-to-train-sklearn-model-in-different-dataframes</guid>
      <pubDate>Fri, 14 Feb 2025 13:01:45 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“ RuntimeRorr：张量A（64）的大小必须匹配非辛格尔顿尺寸1”的张量B（6）？</title>
      <link>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</guid>
      <pubDate>Tue, 11 Feb 2025 06:48:18 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>权重和偏见的结果不同</title>
      <link>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</guid>
      <pubDate>Tue, 04 Feb 2025 19:13:39 GMT</pubDate>
    </item>
    <item>
      <title>通过huggingface Trainer API进行微调“ Google/vit-base-patch16-224-In21k”微调时的Pytorch错误</title>
      <link>https://stackoverflow.com/questions/78320351/pytorch-error-when-fine-tuning-google-vit-base-patch16-224-in21k-on-video-data</link>
      <description><![CDATA[我试图在CSV文件中读取带有标签的自定义视频数据集，并使用Google/VIT-BASE-PATCH16-224-IN21K模型使用HuggingFace Trainer转移训练。
这是我到目前为止已经实施的，但遇到了一个密钥42错误代码。
 导入火炬
导入torchvision.transforms作为变换
导入torchvision.io.video作为视频
来自torch.utils.data导入数据集，dataloader
从变形金刚进口VitfeatureExtractor，VitforimageCecrification，Trainingarguments，培训师
来自sklearn.model_selection导入train_test_split
导入大熊猫作为pd
进口地球
导入AV
从TQDM导入TQDM

＃定义类
class = [&#39;class_0&#39;，&#39;class_1&#39;]

＃定义视频帧的自定义数据集
类VideoDataTaset（数据集）：
    def __init __（self，video_path，标签，变换=无）：
        self.video_paths = video_paths
        self.labels =标签
        self.transform =变换

    def __len __（自我）：
        返回len（self.video_paths）

    def __ getItem __（self，idx）：
        帧，_，_ = video.read_video（self.video_paths [idx]，pts_unit =&#39;sec&#39;）
        sample = {&#39;帧&#39;：框架，&#39;label&#39;：self.labels [idx]}

        如果self.transform：
            示例[&#39;frames&#39;] = [self.transform（帧）样本中的帧[&#39;帧&#39;]]

        返回样本

＃定义用于预处理帧的转换
transform = transforms.compose（[
    转换。升压（（224，224）），
    transforms.totensor（），
）））

＃功能来预处理每个帧
def preprocess_frame（帧）：
    帧= transforms.topilimage（）（帧）
    帧=变换（帧）
    返回框架

＃功能来预处理数据集
def preprocess_dataset（数据集）：
    返回数据集

＃定义培训论点
triending_args = triencharguments（
    output_dir =; ./受过训练
    per_device_train_batch_size = 4，
    num_train_epochs = 3，
    logging_dir =&#39;。/logs&#39;，
    logging_steps = 100，
）

＃加载VIT模型和功能提取器
型号= forforimageClassification.from_pretrated（“ Google/vit-base-patch16-224-In21k”）
feature_extractor = vitfeatureExtractor.from_pretrated（; google/vit-base-patch16-224-in21k＆quort＆quort;）

#load培训视频
train_videos = glob.glob（“ ./ dataset100/train/all/*。mp4＆quort”）

#load标签
train_df = pd.read_csv（&#39;./ train100.csv&#39;，names = header_list）
标签= train_df [&#39;tag&#39;]

#split数据集中到火车和验证集中
train_paths，val_paths，train_labels，val_labels = train_test_split（train_videos，labels，test_size = 0.2，randy_state = 42）

＃创建数据集并预处理
train_dataset = videodataset（train_paths，train_labels，transform = transform）
val_dataset = videodataset（val_paths，val_labels，transform = transform）
train_dataset = preprocess_dataset（train_dataset）
val_dataset = preprocess_dataset（val_dataset）

打印（训练视频的数量：＆quot; len（train_paths））
打印（验证视频的数量：＆quot; len（val_paths））
打印（“培训标签的数量：＆quot” len（train_labels））
打印（验证标签的数量：＆quot; len（val_labels））

打印（训练视频：＆quot; train_videos [：5]）
打印（训练标签：＆quot; train_labels [：5]）

＃定义功能以计算精度
DEF COMPUTE_ACCURACY（PRED）：
    标签= pred.label_ids
    preds = pred.predictions.argmax（-1）
    返回{准确性＆quot; ：（ preds == labels）.mean（）}

＃定义培训师
培训师=教练（
    模型=模型，
    args =训练_args，
    train_dataset = train_dataset，
    eval_dataset = val_dataset，
    COMPUTE_METRICS = COMPUTE_ACCURACY，“您的文本”
）

＃训练模型
Trainer.Train（）
 
这是我收到的错误消息，我真的很感谢您的帮助。
  --------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------
KeyError Trackback（最近的最新电话）
文件C：\ Users \ Code \ Venv \ lib \ lib \ site-packages \ pandas \ core \ indexes \ base.py：3791，in Index.get_loc（self，key）
   3790尝试：
 - ＆gt; 3791返回self._engine.get_loc（casted_key）
   3792除了keyError为err：

文件index.pyx：152，在pandas._libs.index.indexengine.get_loc（）

文件index.pyx：181，在pandas._libs.index.indexengine.get_loc（）

文件pandas \ _libs \ hashtable_class_helper.pxi：2606，in pandas._libs.hashtable.int64hashtable.get_item（）

文件pandas \ _libs \ hashtable_class_helper.pxi：2630，in pandas._libs.hashtable.int64hashtable.get_item（）

KeyError：42

上述例外是以下例外的直接原因：

KeyError Trackback（最近的最新电话）
[31]中的单元，第121行
    112培训师=教练（
    113模型=模型，
    114 args =训练_args，
   （...）
...
   3801＃InvalidIndexError。否则，我们跌倒并重新加重
   3802＃TypeError。
   3803 self._check_indexing_error（键）

KeyError：42
 ]]></description>
      <guid>https://stackoverflow.com/questions/78320351/pytorch-error-when-fine-tuning-google-vit-base-patch16-224-in21k-on-video-data</guid>
      <pubDate>Sat, 13 Apr 2024 11:00:26 GMT</pubDate>
    </item>
    <item>
      <title>如何修复HuggingFace培训师的学习率？</title>
      <link>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</link>
      <description><![CDATA[我正在使用以下参数进行培训模型：
  seq2seqtrainingarguments（
    output_dir =; ./ out＆quot; 
    operwrite_output_dir = true，
    do_train = true，
    do_eval = true，
    
    per_device_train_batch_size = 2， 
    gradient_accumulation_steps = 4，
    per_device_eval_batch_size = 8， 
    
    Learning_rate = 1.25e-5，
    热身_Steps = 1，
    
    save_total_limit = 1，
       
    evalution_strategy =&#39;epoch; quot;
    save_strategy =&#39;epoch; quot;
    logging_strategy =＆quot; epoch;  
    num_train_epochs = 5，   
    
    gradient_checkpointing = true，
    fp16 = true，    
        
    preditive_with_generate = true，
    generation_max_length = 225，
          
    report_to = [＆quot; tensorboard＆quot;]，
    load_best_model_at_end = true，
    metric_for_best_model =; wer; quot;
    大_is_better = false，
    push_to_hub = false，
）
 
我假设热身&gt; 1 修复了学习率。
但是，完成培训后，我正在查看文件 Trainer_state.json ，看来学习率并未确定。
这是 Learning_rate 和步骤的值：
 Learning_rate，步骤
  1.0006 E-05 1033
7.5062 E-06 2066
5.0058 E-06 3099
2.5053 E-06 4132
7.2618 E-09 5165
 
看来，学习率并未固定在 1.25e-5 上（步骤1之后）。我想念什么？我如何确定学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</guid>
      <pubDate>Wed, 10 Jan 2024 09:14:26 GMT</pubDate>
    </item>
    <item>
      <title>在CIFAR-10上的Keras中实施Alexnet的准确性差</title>
      <link>https://stackoverflow.com/questions/51403712/implementation-of-alexnet-in-keras-on-cifar-10-gives-poor-accuracy</link>
      <description><![CDATA[我尝试实现视频。请原谅我，如果我实施了错误，这是我在keras中实现的代码。
编辑：cifar-10 imagedatagenerator 
  cifar_generator = imagedatagenerator（）

cifar_data = cifar_generator.flow_from_directory（&#39;dataSets/cifar-10/train&#39;， 
                                                 batch_size = 32， 
                                                 target_size = input_size， 
                                                 class_mode =&#39;分类&#39;）
 
 Keras中描述的模型：

在

ADD（卷积2d（过滤器= 96，kernel_size =（11，11），input_shape =（227，227，3），步幅= 4，激活=&#39;relu&#39;））））
model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

ADD（卷积2d（过滤器= 256，kernel_size =（5，5），步幅= 1，padding =&#39;same&#39;，activation =&#39;relu&#39;））
model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

ADD（卷积2d（过滤器= 384，kernel_size =（3，3），步幅= 1，填充=&#39;same&#39;，activation =&#39;relu&#39;））））
ADD（卷积2d（过滤器= 384，kernel_size =（3，3），步幅= 1，填充=&#39;same&#39;，activation =&#39;relu&#39;））））
model.Add（卷积2d（滤波器= 256，kernel_size =（3，3），步幅= 1，padding =&#39;same&#39;，activation =&#39;relu&#39;））

model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

模型add（Flatten（））
model.Add（密集（单位= 4096））
model.Add（密集（单位= 4096））
model.Add（密度（单位= 10，activation =&#39;softmax&#39;））

model.compile（优化器=&#39;adam&#39;，loss =&#39;apcorical_crossentropopy&#39;，量表= [&#39;准确性&#39;]）
 
我已经使用Imagedatagenerator在CIFAR-10数据集上训练该网络。但是，我只能获得大约.20的准确性。我无法弄清楚我做错了什么。]]></description>
      <guid>https://stackoverflow.com/questions/51403712/implementation-of-alexnet-in-keras-on-cifar-10-gives-poor-accuracy</guid>
      <pubDate>Wed, 18 Jul 2018 13:47:55 GMT</pubDate>
    </item>
    <item>
      <title>返回标签及其在Sklearn LabelenCoder中的编码值</title>
      <link>https://stackoverflow.com/questions/48938905/return-the-labels-and-their-encoded-values-in-sklearn-labelencoder</link>
      <description><![CDATA[我正在使用  labelencoder  和  oneHotEncoder       sklearn  在机器学习项目中，用于编码数据集中的标签（国家名称）。一切都很好，我的模型运行得很好。该项目是根据包括客户国家在内的多个功能（数据）来对银行客户的持续或离开银行的分类。 
当我想预测（分类）新客户（仅一个）时，我的问题就会出现。新客户的数据仍未预处理（即，未编码国家名称）。如下：
  new_customer = np.array（[[[&#39;france&#39;，600，&#39;男性&#39;，40，3，60000，2，1,1，50000]]）
 
在我学习机器学习的在线课程中，讲师打开了包括编码数据的预处理数据集， 手动   检查了法国的代码并更新了它在 new_customer 中，如下：
  new_customer = np.Array（[[[0，0，600，&#39;Male&#39;，40，3，60000，2，1,1，50000]]）
 
我相信这是不切实际的，必须有一种方法可以自动编码法国在原始数据集中使用的相同代码，或者至少是返回国家列表及其编码值的方法。手动编码标签似乎乏味且容易出错。那么，如何自动化此过程或生成标签代码？提前致谢。 ]]></description>
      <guid>https://stackoverflow.com/questions/48938905/return-the-labels-and-their-encoded-values-in-sklearn-labelencoder</guid>
      <pubDate>Thu, 22 Feb 2018 23:28:02 GMT</pubDate>
    </item>
    </channel>
</rss>