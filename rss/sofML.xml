<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 25 Feb 2025 21:17:06 GMT</lastBuildDate>
    <item>
      <title>无法获得我重量的原始小数，在某个地方自动四舍五入？</title>
      <link>https://stackoverflow.com/questions/79467839/unable-to-get-the-raw-decimal-of-my-weights-auto-rounding-somewhere</link>
      <description><![CDATA[我正在做一个单层perceptron，我需要让我的模型预测用户是否使用网站上的某些按钮制作t还是l。 RN我正在尝试获得权重和偏见，以便我可以将它们实现到我的网站Pyscript代码中，因为我无法进行网站拟合和预测。但是，每当我试图打印重量时，它们会回到SCI符号。我已经尝试执行reter，但这是由于某种原因无法使用的。有人可以告诉我我要在哪里做错什么？
 导入numpy作为NP
导入大熊猫作为pd
来自sklearn.model_selection导入train_test_split
从Sklearn Import DataSet中
导入matplotlib.pyplot作为PLT

Def Activation_Function（Z）：
    返回np.Where（z＆gt; = 0，1，0）

＃将数据集作为熊猫数据框架
df = pd.read_csv（&#39;triendingdata.csv＆quot;）
df.head（）

#split培训数据到X和Y
x = df.iloc [：，1：]
打印（x）

y = df.iloc [：，0]
打印（y）


#convert y标签进入整数
y = y.map（{&#39;l&#39;：0，&#39;t&#39;：1}）

#Convert X和Y到Numpy数组以进行以后处理
x = x.to_numpy（dtype = np.float64）
y = y.to_numpy（dtype = np.int32）

打印（x）
打印（y）

打印（x.dtype）
打印（y.dtype）

#split培训数据并检查形状。由于数据集不是很大的原因（58个样本，16个功能），进行70/30拆分
x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.3，andural_state = 42）

打印（x_train.shape）
打印（x_test.shape）
打印（y_train.shape）
打印（y_test.shape）

＃适合该模型以重新考虑代码的Pyscript部分的权重和偏差
权重= np .eros（16）*1
偏差= 0

Learning_rate = 0.01
    
对于_范围（1000）：
    对于IDX，x_i enumerate（x_train）：
        
        linear_product = np.dot（x_i，weights） +偏见 
        
        y_pred = activation_function（linear_product）
        
        损失= y_pred -y_train [idx]
                
        权重 -  = Learning_rate *损失 * x_i
        
        偏见 -  = Learning_rate *损失

ret（（重量））
ret（（偏见））
 
，我希望假设我的转换和方程式是准确的，我希望能得到很大的小数。我已经尝试询问朋友，教授和chatgpt，并使用其他方法以及其他方法以正确的方式打印。]]></description>
      <guid>https://stackoverflow.com/questions/79467839/unable-to-get-the-raw-decimal-of-my-weights-auto-rounding-somewhere</guid>
      <pubDate>Tue, 25 Feb 2025 20:47:24 GMT</pubDate>
    </item>
    <item>
      <title>概率扩散模型的推导[封闭]</title>
      <link>https://stackoverflow.com/questions/79467325/derivation-of-probabilistic-diffusion-model</link>
      <description><![CDATA[在扩散模型教程中（ paper ） 22通过将概率过程从$$ q（x_（t）| x_（t-1））$$逆转到$$ q（x_（t-1）| x_t，x_0）$$。我了解个人术语是如何重新重新制定的，但对期望操作和KL分歧并不十分清楚。简而言之，如何将方程式（21）重新重新为等式（22）？？
我试图将方程21扩展到积分形式中，并将概率反向变为积分形式，但仍无法在等式22中获得结果。]]></description>
      <guid>https://stackoverflow.com/questions/79467325/derivation-of-probabilistic-diffusion-model</guid>
      <pubDate>Tue, 25 Feb 2025 16:57:48 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost在400个示例数据集上 - 我的模型过于拟合吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79467253/xgboost-on-400-sample-dataset-is-my-model-overfitting</link>
      <description><![CDATA[我正在处理一个回归任务，我可以从包含400个样本和15个功能的数据集中预测机器的生产持续时间（数小时）。这些功能包括数值和编码的分类变量。我正在使用XGBoost进行回归。
作为结果，我观察到以下指标：

训练R²：〜0.99 
测试R²：〜0.80-0.90 
当我从测试集中删除特定异常值时，其他错误指标（MSE，RMSE，MAE）较低，但Mape几乎保持不变。

学习曲线表明，即使有少量样本，训练分数也接近1.0。验证分数从0.65左右开始，攀升至0.90左右，但差距仍然存在。
学习曲线 
所以我认为我的模型过于适应，我知道这是一个很小的数据集，但这就是我所拥有的。我可以申请什么以减少过度拟合？
我已经尝试使用超参数调整。更改训练/测试拆分。]]></description>
      <guid>https://stackoverflow.com/questions/79467253/xgboost-on-400-sample-dataset-is-my-model-overfitting</guid>
      <pubDate>Tue, 25 Feb 2025 16:30:21 GMT</pubDate>
    </item>
    <item>
      <title>在线性调度程序训练DDPM后，如何实现其他类型的调度程序？</title>
      <link>https://stackoverflow.com/questions/79465469/how-do-i-implement-a-different-type-of-scheduler-after-training-my-ddpm-on-a-lin</link>
      <description><![CDATA[我使用以下代码定义的线性调度程序训练了扩散模型（DDPM）：
 调度程序= ddpmscheduler（
    num_train_timesteps = 1000，
    beta_start = 0.0001，
    beta_end = 0.02，
    beta_schedule =＆quot&#39;线性
）
 
但是，在训练此模型并使用代码加载之后：
  def load_checkpoint（模型，优化器，checkpoint_）：
    ““从检查点加载模型和优化器词典”。“”。
    checkpoint = torch.load（checkpoint_path，map_location =设备）
    model.load_state_dict（checkpoint [&#39;model_state_dict&#39;]）
    Optimizer.load_state_dict（checkpoint [&#39;Optimizer_state_dict&#39;]）
    start_epoch = checkpoint [&#39;epoch&#39;] + 1＃从下一个时代恢复。
    打印（f＆quot“从epoch {start_epoch}＆quot恢复）
    返回start_epoch
 
当我尝试切换到余弦调度程序以使用以下代码推理：
 调度程序= ddpmscheduler（
    num_train_timesteps = 1000，
    beta_schedule =＆quot; 
）
 
我有以下错误：
  notimplementedError：cosine未针对＆lt; class&#39;dribfusers.schedulers.scheduling_ddpm.ddpmm.ddpmscheduler&#39;＆gt; gt;
 
我尝试将DDPM调度程序更改为DDIM调度程序，将代码更改为：
 调度程序= ddimscheduler（
    num_train_timesteps = 1000，
    beta_schedule =＆quot;
）
 
，但我仍然有一个非常相似的错误：
  notimplementedError：cosine未针对＆lt; class&#39;drifusers.schedulers.scheduling_ddim.ddimscheduler&#39;＆gt; gt;
 
我不明白我在做什么错，因为我从多个来源读到，在一个调度程序上训练扩散模型是正常的（在我的情况下是线性），然后更改为推理的另一个调度程序（在我的情况，余弦）。我如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79465469/how-do-i-implement-a-different-type-of-scheduler-after-training-my-ddpm-on-a-lin</guid>
      <pubDate>Tue, 25 Feb 2025 05:14:09 GMT</pubDate>
    </item>
    <item>
      <title>ASR的单方言语音语料库[关闭]</title>
      <link>https://stackoverflow.com/questions/79464867/single-dialect-speech-corpus-for-asr</link>
      <description><![CDATA[我正在进行有关减轻自动语音识别系统中音调偏差的研究。我的研究涉及测试流行的语音助手和创建的ASR系统。但是，我正在努力寻找用于测试和培训数据集的语音数据集。因为我专注于音调，所以我想通过找到一个每个人都具有相同口音/英语方言的数据集来缩小变异性。由于大多数语音语料库都专注于ASR培训的不同演讲者，因此很难遇到这一点。我发现了一个免费的语料库，其中包含来自不列颠群岛的6个方言（）。但是，由于我仅利用一种方言，即使是最坚固的方言也没有提供来自高音的声音的足够数据。我正在寻找大约80-350 Hz的一系列音高。一些录音确实降落在300-350上方的范围内，但只有大约12个，这对我的数据集还不够。我花了无数小时的时间进行搜索，并努力寻找合适的语料库。我没有钱可以花在演讲语料库上，所以有人对我能做什么有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79464867/single-dialect-speech-corpus-for-asr</guid>
      <pubDate>Mon, 24 Feb 2025 21:45:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Docling库从DOCX文件中提取页面上的HTML内容，以检测页面断路？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</link>
      <description><![CDATA[我已经成功地使用 docling&gt; docling 和 pypdf2 。这是我当前代码对PDF的作用：

使用PYPDF2将PDF分为单个页面
使用Docling的 Document Converter  将每个页面转换为HTML
用嵌入式图像提取HTML内容
添加元数据（页码，文档ID，文件名）
将所有内容保存到JSON结构

重要说明：我首先将PDF划分为单个页面的原因是因为Docling的 save_as_html（） and  export_to_to_html（）函数在完整的文档对象上工作，而不是在单个页面。要获取页面html内容，我需要创建临时的单页PDF并分别转换一个。
这是我每页获得的示例JSON结构：
  {
    ＆quot“ page”：“第1页”
    ＆quot“ content＆quot”：＆quot＆lt; html内容＆gt;＆quot ,,
    “元数据：{{
        ＆quot“ docutsId”：; quot“ uuid; quot”
        “文件名”：“ document.pdf”
        &#39;page_number＆quot”：1，
        ＆quot“ total_pages”：总计
    }
}
 
现在，我需要为DOCX文件实现相同的功能。据我说，DOCX文件包含标题，页脚和页面断路等元素，我们可以使用这些页面中断将内容分为页面。
我正在使用Docling库进行转换（DOCX到HTML），但是我无法识别或检测到DOCX文件中的页面中断。由于Docling的HTML转换在完整的文档上起作用，因此我需要根据页面断路首先将DOCX内容拆分，类似于我处理PDF的方式。
问题：

如何使用Docling在DOCX文件中检测页面中断？
是否有一种方法可以根据这些页面断开以创建单独的文档对象来拆分DOCX内容？
如果Docling不直接支持此内容，是否还有其他python库，我应该与文档一起使用以检测和拆分页面中断？

我尝试查看文档文档，但找不到有关DOCX文件中处理页面中断的信息。
任何帮助或指导将不胜感激！
我正在使用的相关库：

  docling 
  python-docx （如果需要）
]]></description>
      <guid>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</guid>
      <pubDate>Sun, 23 Feb 2025 15:19:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用FBProphet使用多个回归剂预测值？</title>
      <link>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</link>
      <description><![CDATA[我想使用FBProphet模型预测小时温度值。到目前为止，我已经对DS和Y变量进行了培训，这给了我良好的结果。但是现在我想添加额外的回归器，然后执行预测。
将模型与额外的回归器拟合后，我在测试数据集上对其进行了测试，这使我准确。但是主要问题是如何预测未来（我的测试集超出我的测试集）
这是我到目前为止所做的。
 ＃数据准备和功能工程

temp = df [[[＆quot; weverip; quot; quot; quot;]]。应用（kelvintodegc）.copy（）
temp [&#39;hourlylag＆quot;] = temp [温度＆quot＆quot。shift（1）.bfill（）
temp [&#39;dailylag&#39;&#39;] = temp [温度＆quot＆quot。shift（24）.bfill（）
temp [＆quot; weeklylag＆quot;] = temp [温度＆quot＆quot＆quot。shift（24*7）.bfill（）
temp [movmean＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [mstd＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [ub＆quot; quot&#39;] = temp [movmean＆quot;] +（1.6 * temp [mstd;]）
temp [lb＆quot;] = temp [movmean;]  - （1.6 * temp [mstd;]）
temp [＆quot; devfromean＆quot; quot; temp [movemean＆quord; temp [温度＆quort; quort&#39;&#39;]
temp [＆quot; devfromub＆quot;] = temp [ub quot; quot; temp [temp;
temp [devfromlb; quot; quot; quot temp [lb＆quot&#39;]  - 温度；
temp [小时;
temp [&#39;Dayofyear＆quort;] = temp.index.day
temp [; quot; quot; quot＆quot temp.index.month
temp = temp.Reset_index（）
temp.rename（columns = {; date;：＆quord ds; quot; quot; quot; quot; quot; quot; quot; y y y}

模型=先知（）
model.Add_regressor（“ hourlylag”）
model.Add_regressor（“ Dailylag”）
model.Add_regressor（“每周”
model.add_regressor（“ movmean;）
Model.Add_Regressor（“ MSTD”）
model.Add_regressor（&#39;ub＆quot;）
model.add_regressor（&#39;lb＆quot;）
model.add_regressor（“ Devfromean＆quot”）
Model.Add_Regressor（“ DevFromub”）
Model.Add_regressor（&#39;Devfromlb＆quort;）
model.add_regressor（“小时”）
model.Add_regressor（“ Dayofyear”）
model.add_regressor（“月”）

型号（火车）

这些是MAE和MAPE分数
MAE：0.00
Mape：0.17％

现在未来= model.make_future_dataframe（周期= 24 * 365 * 3，freq =; h＆quot;）

我有这个错误

ValueError Trackback（最近的最新电话）
[68]中的单元，第2行
      1未来= model.make_future_dataframe（周期= 8760，freq =; h＆quot;）
----＆gt; 2预测=模型。预定（未来）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py.py：1270，in Prophet.prophet.predt.predict.predict.predict.predict.predict（self，df，df，vectorized，vectorized）
   1268如果DF.Shape [0] == 0：
   1269提高价值Error（“数据框架没有行。”）
 - ＆gt; 1270 df = self.setup_dataframe（df.copy（））
   1272 DF [&#39;趋势&#39;] = self.predict_trend（df）
   1273 sipersal_components = self.predict_seasonal_components（df）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py：297，in PropHet.set.setup.dataframe（self，ddf，diredize_scales）
    295在self.extra_regressor中名称：
    296如果不在DF中的名字：
 - ＆gt; 297提高价值Error（
    298&#39;回归器{name！r} dataframe中缺少
    299 .format（名称=名称）
    300）
    301 df [name] = pd.to_numeric（df [name]）
    302如果DF [name] .isnull（）。任何（）：

valueerror：dataFrame中缺少回归器“小时lag”
 ]]></description>
      <guid>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</guid>
      <pubDate>Fri, 21 Feb 2025 04:48:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上要插件代码：

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：

 file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py&quot;，第97行，in_dict 
 提高ValueError（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 

我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

提高ValueError（valueError：未知量化类型，获得FP8-支持类型为：[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，gptq&#39;，&#39;gptq&#39;，&#39;aqlm&#39;&#39;aqlm&#39;&#39;，&#39;aqlm&#39;，&#39; &#39;，&#39;hqq&#39;，&#39;compressed Tensors&#39;，&#39;fbgemm_fp8&#39;， &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在NLTK中下载Punkt Tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用安装了NLTK库
  PIP安装NLTK
 
使用lib 
 来自nltk.tokenize导入send_tokenize 
send_tokenize（文本）
 
我遇到此错误
  lookuperror： 
****************************************************** ********************
  找不到资源朋克。
  请使用NLTK下载器获取资源：

  ＆gt;＆gt;＆gt;导入NLTK
  ＆gt;＆gt;＆gt; nltk.download（&#39;punkt&#39;）
  
  有关更多信息，请参见：https：//www.nltk.org/data.html

  尝试加载dokenizers/punkt/English.pickle

  搜索：
     - &#39;c：\\用户\\ adars/nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ share \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ lib lib \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\漫游\\ nltk_data&#39;
     - &#39;c：\\ nltk_data&#39;
     - &#39;d：\\ nltk_data&#39;
     - &#39;e：\\ nltk_data&#39;
     - &#39;&#39;&#39;
 
因此，为了解决此错误，我尝试了
 导入NLTK
nltk.download（&#39;punkt&#39;）
 
但是我无法下载此软件包，因为每次运行时，我都会收到错误的错误
  [nltk_data]错误加载punkt：＆lt; urlopen错误[WinError 10060] a
[nltk_data]连接尝试失败，因为连接的聚会
[nltk_data]一段时间后没有正确响应，或者
[nltk_data]建立的连接失败，因为连接的主机
[nltk_data]未能响应＆gt;
 
请在这里帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Runtimeerror：Mat1＆Mat2形状无法乘以</title>
      <link>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</link>
      <description><![CDATA[我正在pytorch上建造一个CNN并收到以下错误消息：

 RuntimeError：MAT1和MAT2形状无法乘以（32x32768和
512x256）

我已经构建了以下模型：
  def classifier_block（输入，输出，kernel_size，stride，last_layer = false）：
  如果不是last_layer：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步，填充= 3），
        nn.batchnorm2d（输出），，
        nn.leakyrelu（0.2，intplophe = true）
    ）
  别的：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步），
        nn.maxpool2d（kernel_size = 3，步幅= 2，填充= 1）
    ）
  返回x

类分类器（nn.module）：
  def __init __（self，input_dim，输出）：
    超级（分类器，self）.__ init __（）
    self.classifier = nn。
        classifier_block（input_dim，64、7、2），
        classifier_block（64、64、3、2），
        classifier_block（64、128、3、2），
        classifier_block（128，256，3，2），
        classifier_block（256，512，3，2，true）
    ）
    打印（&#39;clf：&#39;，self.classifier）
    
    self.linear = nn.Sequepention（
        nn.linear（512，256），
        nn.relu（inplace = true），
        nn.linear（256，128），
        nn.relu（inplace = true），
        nn.linear（128，64），
        nn.relu（inplace = true），
        nn.linear（64，输出）
    ）
    打印（&#39;linear：&#39;，self.linear）
  
  向前（自我，图像）：
    打印（&#39;img：&#39;，image.shape）
    x = self.classifier（图像）
    打印（&#39;X：&#39;，X.Shape）
    返回self.linear（x.View（len（x），-1））
 
输入图像是大小 512x512 。这是我的训练障碍：
  loss_train = []
loss_val = []

对于范围（时期）的时期：
  print（&#39;epoch：{}/{}&#39;。格式（epoch，epochs））
  total_train = 0
  CRORCE_TRAIN = 0
  cumloss_train = 0
  classifier.train（）
  对于枚举（x，y）的批次（train_loader）：
    x = x.to（设备）
    打印（X.Shape）
    打印（y.形）
    输出=分类器（x）
    损失=标准（输出，y.to（设备））
    优化器.zero_grad（）
    loss.backward（）
    优化器.step（）

    打印（&#39;损失：{}&#39;。格式（损失））
 
任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</guid>
      <pubDate>Fri, 10 Mar 2023 06:48:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的基本CNN模型不适合分段图像数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/65707204/why-is-my-basic-cnn-model-not-overfitting-segmentation-image-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/65707204/why-is-my-basic-cnn-model-not-overfitting-segmentation-image-dataset</guid>
      <pubDate>Wed, 13 Jan 2021 17:49:43 GMT</pubDate>
    </item>
    <item>
      <title>我的培训数据集对于我的神经网络来说太复杂了吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/57224353/is-my-training-data-set-too-complex-for-my-neural-network</link>
      <description><![CDATA[我正在尝试解释我的回归模型的两个图。
 我的机器学习模型的培训错误和验证错误 
我的情况与此相似：训练Keras中的多回归模型的损失值非常大，但是我的MSE和RMSE非常高。
我的建模不足？如果是，我该怎么办才能解决这个问题？
这是我用于解决回归问题的神经网络
  def build_model（）：
模型= keras。
    layers.dense（128，激活= tf.nn.relu，input_shape = [len（train_dataset.keys（））），
    layers.dense（64，激活= tf.nn.relu），
    层。
）））
优化器= tf.keras.optimizers.rmsprop（0.001）

model.compile（loss =&#39;mean_squared_error&#39;，
              优化器=优化器，
              metrics = [&#39;mean_absolute_error&#39;，&#39;mean_squared_error&#39;]）
返回模型
 
和我的数据集
我有500个样本，10个功能和1个目标]]></description>
      <guid>https://stackoverflow.com/questions/57224353/is-my-training-data-set-too-complex-for-my-neural-network</guid>
      <pubDate>Fri, 26 Jul 2019 17:03:09 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn-型号不断拟合</title>
      <link>https://stackoverflow.com/questions/31956501/sklearn-model-keeps-overfitting</link>
      <description><![CDATA[我正在寻找有关当前机器学习问题的最佳前进方向的建议
问题的轮廓和我所做的是如下：

我有900多次脑电图数据的试验，其中每个试验长1秒。地面真相是每个人都知道的，并分类状态0和状态1（40-60％拆分）
每个试验都通过预处理进行过滤和提取某些频带的功率，这些频段构成了一组功能（功能矩阵：913x32）
然后，我使用Sklearn来训练模型。在使用测试尺寸为0.2的情况下，使用Cross_validation。分类器设置为使用RBF内核，C = 1，伽马= 1（我尝试过许多不同的值）

您可以在此处找到代码的缩短版本： http:/pastebin.com/xu13cil4  P&gt;

我的问题：

当我使用分类器预测测试集的标签时，每个预测为0 
火车准确性为1，而测试集精度约为0.56 
我的学习曲线情节看起来像这样：

    
现在，这似乎是在这里过度拟合的经典案例。但是，这里的过度拟合不太可能是由于样本数量不成比例的（32个功能，900个样本）引起的。我已经尝试了许多事情来缓解这个问题：

我尝试使用降低维度（PCA），以防万一，因为我对样本的数量有太多功能，但是准确的分数和学习曲线图看起来与上述相同。除非我将组件的数量设置为10以下，否则火车准确性开始下降，但是考虑到您开始丢失信息，这不是某种程度上吗？
我尝试将数据标准化和标准化。标准化（SD = 1）无助于更改火车或准确分数。标准化（0-1）将我的训练精度降低到0.6。
我已经为SVC尝试了各种C和伽马设置，但它们不会更改任何分数
尝试使用其他估计器（例如高斯人），甚至使用Adaboost等集合方法。没有更改
尝试使用LinareArsVC设置正规化方法，但没有改善情况
我尝试使用Theano通过神经网运行相同的功能，而我的火车准确性约为0.6，测试约为0.5 

我很高兴继续思考这个问题，但是在这一点上，我正在寻找适当方向的轻推。我的问题可能在哪里，我该怎么办？
我的一组功能完全可能不会区分这两个类别，但是我想在得出这个结论之前尝试其他选项。此外，如果我的功能没有区分，那么这可以解释低测试场得分，但是在这种情况下，您如何获得完美的训练场得分呢？这可能吗？]]></description>
      <guid>https://stackoverflow.com/questions/31956501/sklearn-model-keeps-overfitting</guid>
      <pubDate>Wed, 12 Aug 2015 05:10:59 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化[关闭]</title>
      <link>https://stackoverflow.com/questions/21554301/data-normalization</link>
      <description><![CDATA[当我想分类“好”时或“最佳”然后，我可以使用Facebook的计数或Twitter转发计数的计数。
但是有些社区的用户群很大，因此他们的链接获得了更多的喜欢或转发。我该如何“归一化”这些巨大的社区喜欢例如，像count这样的小得多的社区的类似新闻项目链接之类的链接？
这被称为正常化吗？我可以在哪种书籍中学习有关“质量”的这类算法。 （例如，在这种情况下）？无论如何，我想做什么？]]></description>
      <guid>https://stackoverflow.com/questions/21554301/data-normalization</guid>
      <pubDate>Tue, 04 Feb 2014 13:47:45 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化的参考文献[关闭]</title>
      <link>https://stackoverflow.com/questions/5652357/references-for-data-normalization</link>
      <description><![CDATA[对于NNS和其他机器学习算法，将数据标准化（不确定是否正确）的最佳实践是什么？  我的意思是您如何表示NN/Algo的数据。
例如，您如何表示商店代码？  商店555不大于或小于554，它只是一个分类。 NNS/ALGO模型只是单独过滤出来，还是您需要使它们进行分类而不是数学上的区别？
 编辑：感谢大家的答案。  我一直在挖掘很多数据挖掘书，尽管我发现了一些在预处理的数据主题上花了一两章的时间，但我对最掩饰的效果最大感到有些惊讶。  再次感谢。]]></description>
      <guid>https://stackoverflow.com/questions/5652357/references-for-data-normalization</guid>
      <pubDate>Wed, 13 Apr 2011 16:17:41 GMT</pubDate>
    </item>
    </channel>
</rss>