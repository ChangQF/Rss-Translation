<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ çš„æ´»è·ƒé—®é¢˜ - å †æ ˆå†…å­˜æº¢å‡º</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>æ¥è‡ª stackoverflow.com çš„æœ€æ–° 30 æ¡</description>
    <lastBuildDate>Sun, 13 Oct 2024 21:15:57 GMT</lastBuildDate>
    <item>
      <title>æˆ‘æ˜¯å¦åº”è¯¥å°†é—å¿˜é›†åˆ†æˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ä»¥ä½¿ç”¨ CNN å’Œ Celeba æ•°æ®é›†è¿›è¡Œåå­¦ä¹ å®éªŒï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ CelebA æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªç®€å•çš„ CNNï¼Œå¹¶å°è¯•é€šè¿‡åˆ›å»ºä¸€ä¸ªé—å¿˜é›†ï¼ˆæ¥è‡ª CelebA çš„éšæœºéƒ¨åˆ†å›¾åƒï¼‰æ¥è¿›è¡Œåå­¦ä¹ å®éªŒã€‚
CNN å·²åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿›è¡Œäº†å•ç‹¬çš„éªŒè¯å’Œæµ‹è¯•æ‹†åˆ†ä»¥è¿›è¡Œè¯„ä¼°ã€‚
æˆ‘æ˜¯å¦åº”è¯¥å°†é—å¿˜é›†æ‹†åˆ†ä¸ºè®­ç»ƒ/æµ‹è¯•/éªŒè¯å­é›†ï¼Œå¦‚æœæ˜¯ï¼Œä¸ºä»€ä¹ˆï¼Ÿï¼ˆé—å¿˜é›†çº¦å è®­ç»ƒé›†çš„ 10% - å…¶ä½™ 90% ä¸ºä¿ç•™é›†ï¼‰
åœ¨æˆ‘å½“å‰çš„åå­¦ä¹ å®éªŒä¸­ï¼Œæˆ‘åªå¯¹é—å¿˜é›†è¿›è¡Œåå­¦ä¹ ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚å› æ­¤ï¼Œæˆ‘åœ¨æ­¤è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨å®Œæ•´æ¨¡å‹å’Œé—å¿˜é›†ã€‚
æˆ‘å°è¯•ä½¿ç”¨æˆ‘ç›¸åº”æ‹†åˆ†çš„é—å¿˜é›†ï¼Œä½†æµ‹è¯•å˜å¾—æ£˜æ‰‹ï¼Œå¹¶ä¸”å¯èƒ½åªæœ‰ä¸€ä¸ªé—å¿˜é›†æ„Ÿè§‰åˆä¹é€»è¾‘ï¼ˆå› ä¸ºé—å¿˜é›†ä¸­ä¸æ˜¯åŒä¸€ä¸ªäººï¼Œå®ƒæ˜¯ä¸€ä¸ªæ··åˆåŒ…ï¼‰
å› æ­¤ï¼Œæ‹†åˆ†ä¸ºæµ‹è¯•å’ŒéªŒè¯æ„å‘³ç€å®ƒæ˜¯ä¸åŒé¢å­”çš„ä¸åŒåŒ…ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</guid>
      <pubDate>Sun, 13 Oct 2024 19:44:40 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆå°½ç®¡ä½¿ç”¨äº† RLZoo3 çš„æœ€ä½³è¶…å‚æ•°ï¼Œæˆ‘çš„ SB3 DQN ä»£ç†ä»æ— æ³•å­¦ä¹  CartPole-v1ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</link>
      <description><![CDATA[æˆ‘ä» RLZoo3 è·å¾—äº†ç”¨äºè®­ç»ƒ CartPole-v1 çš„æœ€ä½³è¶…å‚æ•°ã€‚æˆ‘åˆ›å»ºäº†ä¸€ä¸ªæœ€å°ç¤ºä¾‹æ¥å±•ç¤ºæˆ‘çš„ CartPole ä»£ç†çš„æ€§èƒ½ã€‚æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä»£ç†åº”è·å¾— 500 åˆ†ï¼Œæ‰èƒ½æˆåŠŸå®Œæˆä¸€é›†ã€‚ä¸å¹¸çš„æ˜¯ï¼Œåˆ†æ•°æ²¡æœ‰è¶…è¿‡ 300ã€‚
è¿™æ˜¯æˆ‘çš„ä»£ç  -
import gymnasium as gym
import numpy as np
import torch
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter
import os

def set_seed(seed):
np.random.seed(seed)
torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True

class TensorBoardCallback(BaseCallback):
def __init__(self, log_dir):
super().__init__()
self.writer = SummaryWriter(log_dir=log_dir)
self.episode_rewards = []
self.current_episode_reward = 0

def _on_step(self):
self.current_episode_reward += self.locals[&#39;rewards&#39;][0]

å¦‚æœ self.locals[&#39;dones&#39;][0]:
self.episode_rewards.append(self.current_episode_reward)
self.writer.add_scalar(&#39;train/episode_reward&#39;, self.current_episode_reward, self.num_timesteps)
self.current_episode_reward = 0

å¦‚æœ len(self.episode_rewards) &gt;= 100:
avg_reward = sum(self.episode_rewards[-100:]) / 100
self.writer.add_scalar(&#39;train/average_reward&#39;, avg_reward, self.num_timesteps)

return True

def on_training_end(self):
self.writer.close()

# è®¾ç½®æ—¥å¿—ç›®å½•
log_dir = &quot;tensorboard_logs&quot;
os.makedirs(log_dir, exist_ok=True)

# è®¾ç½®å¯é‡å¤æ€§çš„ç§å­
seed = 42
set_seed(seed)

# åˆ›å»ºç¯å¢ƒ
env = gym.make(&quot;CartPole-v1&quot;)
env = DummyVecEnv([lambda: env])

# ä½¿ç”¨æ¥è‡ª rlzoo3 çš„è¶…å‚æ•°åˆ›å»ºæ¨¡å‹
model = DQN(
policy=&quot;MlpPolicy&quot;,
env=env,
learning_rate=2.3e-3,
batch_size=64,
buffer_size=100000,
learning_starts=1000,
gamma=0.99,
target_update_interval=10,
train_freq=256,
gradient_steps=128,
exploration_fraction=0.16,
exploration_final_eps=0.04,
policy_kwargs=dict(net_arch=[256, 256]),
verbose=1,
tensorboard_log=log_dir,
seed=seed
)

# åˆ›å»ºå›è°ƒ
tb_callback = TensorBoardCallback(log_dir)

# è®­ç»ƒæ¨¡å‹
total_timesteps = 50000
model.learn(total_timesteps=total_timesteps, callback=tb_callback)

print(&quot;è®­ç»ƒå®Œæˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨ TensorBoard æŸ¥çœ‹ç»“æœã€‚&quot;)
print(f&quot;åœ¨æ‚¨çš„ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼štensorboard --logdir {log_dir}&quot;)

env.close()

è¿™æ˜¯æœ€ç»ˆç»“æœ -
]]></description>
      <guid>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</guid>
      <pubDate>Sun, 13 Oct 2024 19:23:17 GMT</pubDate>
    </item>
    <item>
      <title>ML æ¢¯åº¦ä¸‹é™ [å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79082800/ml-gradient-descent</link>
      <description><![CDATA[æ¢¯åº¦ä¸‹é™
æ¢¯åº¦ä¸‹é™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³•ä¹‹ä¸€ã€‚å®ƒé€šè¿‡è¿­ä»£è°ƒæ•´æ¨¡å‹å‚æ•°æ¥å¸®åŠ©æœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼ˆæˆ–æŸå¤±å‡½æ•°ï¼‰ã€‚
æ¢¯åº¦ä¸‹é™ä¸­çš„å¯¼æ•°ï¼šæ¢¯åº¦ï¼ˆåå¯¼æ•°å‘é‡ï¼‰æ˜¾ç¤ºæŸå¤±å‡½æ•°æœ€é™¡å³­çš„å¢é•¿æ–¹å‘ã€‚ä¸ºäº†æœ€å°åŒ–æŸå¤±ï¼Œå‚æ•°ä¼šæ²¿æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°ã€‚ä»æ•°å­¦ä¸Šæ¥è¯´ï¼Œå‚æ•°æ›´æ–°è§„åˆ™æ˜¯ï¼š
ğœƒ
ğœƒ
âˆ’
ğ›¼
âˆ‡
ğœƒ
ğ½
(
ğœƒ
)
Î¸=Î¸âˆ’Î±âˆ‡
Î¸
â€‹
J(Î¸)
å…¶ä¸­
ğœƒ
Î¸ è¡¨ç¤ºæ¨¡å‹å‚æ•°ï¼Œ
ğ›¼
Î± è¡¨ç¤ºå­¦ä¹ ç‡ï¼Œ
âˆ‡
ğœƒ
ğ½
(
ğœƒ
)
âˆ‡
Î¸
â€‹
J(Î¸) æ˜¯æˆæœ¬å‡½æ•°ç›¸å¯¹äº
ğœƒ
Î¸ çš„æ¢¯åº¦ã€‚
2. æˆæœ¬å‡½æ•°ï¼ˆæŸå¤±å‡½æ•°ï¼‰
æˆæœ¬å‡½æ•°æ˜¯è®­ç»ƒæœŸé—´è¦æœ€å°åŒ–çš„ç›®æ ‡å‡½æ•°ã€‚ç¤ºä¾‹åŒ…æ‹¬å›å½’ä»»åŠ¡çš„å‡æ–¹è¯¯å·® (MSE) å’Œåˆ†ç±»ä»»åŠ¡çš„äº¤å‰ç†µã€‚
å¾®åˆ†å­¦ç”¨äºè®¡ç®—æˆæœ¬å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼Œä»¥æŒ‡å¯¼å¦‚ä½•è°ƒæ•´å‚æ•°ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79082800/ml-gradient-descent</guid>
      <pubDate>Sun, 13 Oct 2024 09:18:27 GMT</pubDate>
    </item>
    <item>
      <title>Python åœ¨åˆ†é… numpy æ•°ç»„æ—¶æŠ›å‡º MemoryError</title>
      <link>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä»¥ç±»ä¼¼ sklearn çš„æ–¹å¼æ‹Ÿåˆ QML ç®—æ³•ï¼š
num_features = X_train.shape[1]

feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)
ansatz = RealAmplitudes(num_qubits=num_features, reps=3)
optimizer = COBYLA(maxiter=100)

vqc = VQC(
feature_map=feature_map,
ansatz=ansatz,
optimizer=optimizer
)

vqc.fit(X_train, y_train.to_numpy())

åœ¨æ‰§è¡Œ vqc.fit(X_train, y_train.to_numpy()) è¡Œæ—¶ï¼Œè§£é‡Šå™¨æŠ›å‡ºå¼‚å¸¸ï¼š
MemoryErrorï¼šæ— æ³•ä¸ºå½¢çŠ¶ä¸º (1048576,) ä¸”æ•°æ®ç±»å‹ä¸º &lt;U420  çš„æ•°ç»„åˆ†é… 1.64 GiB
é—®é¢˜æ˜¯ï¼Œæˆ‘æ­£åœ¨ä½¿ç”¨çš„æœºå™¨æœ‰ 120 GB çš„ RAMï¼Œæˆ‘ä¸æ˜ç™½å®ƒä¸ºä»€ä¹ˆä¸èƒ½ä¸ºæ•°ç»„åˆ†é… 1.64 GBã€‚ä½ èƒ½å¸®æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿæœ‰ä»€ä¹ˆæ–¹æ³•å¯ä»¥çªç ´è¿™ä¸ª RAM é™åˆ¶å—ï¼Ÿ
æˆ‘ä¸ç¡®å®šï¼Œä½†æˆ‘æƒ³è¯•è¯•è¿™ä¸ª https://stackoverflow.com/a/58686879ã€‚ç„¶è€Œï¼Œæˆ‘è®¤ä¸ºè¿™è¡Œä¸é€šï¼Œä¹Ÿè®¸ä½ æœ‰æ›´å¤šçš„æƒ³æ³•ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</guid>
      <pubDate>Sun, 13 Oct 2024 03:26:07 GMT</pubDate>
    </item>
    <item>
      <title>ç»“åˆå‡ ç§ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79081793/combining-several-supervised-learning-techniques</link>
      <description><![CDATA[æˆ‘çš„æ•°æ®é›†åŒ…å«å¤§é‡å›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼ˆå­˜å‚¨åœ¨ .csv æ–‡ä»¶ä¸­ï¼‰ã€‚æˆ‘æ‰“ç®—ä½¿ç”¨æˆ‘æ‹¥æœ‰çš„æ‰€æœ‰æ•°æ®ï¼ˆå›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼‰åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¯¹æ¡ˆä¾‹ A å’Œæ¡ˆä¾‹ B è¿›è¡Œåˆ†ç±»/è¯†åˆ«çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
æ˜¯å¦å¯ä»¥ç»“åˆå‡ ç§ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œä¾‹å¦‚ä½¿ç”¨ CNN å¤„ç†å›¾åƒï¼Œä½¿ç”¨éšæœºæ£®æ—åˆ†æåˆ¶è¡¨æ•°æ®ï¼Ÿç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå¯¹ä¸¤ç§ç±»å‹çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ç»„åˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å°†æ¡ˆä¾‹ A å’Œæ¡ˆä¾‹ B è¿›è¡Œåˆ†ç±»ã€‚
æˆ‘çš„é—®é¢˜æ˜¯è¿™ç§æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­æ˜¯å¦å¯è¡Œã€‚
æ˜¯å¦å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆå›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼Ÿ
å¯¹äºè¿™äº›ç±»å‹çš„æ•°æ®ï¼Œé€šå¸¸æ¨èä½¿ç”¨å“ªç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ
æˆ‘ç›¸ä¿¡ CNN æ˜¯å›¾åƒçš„ä¸é”™é€‰æ‹©ï¼Œä½†æˆ‘ä¸ç¡®å®šåˆ¶è¡¨æ•°æ®çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆã€‚
æœ€é€‚åˆç”¨äºæ­¤ä»»åŠ¡çš„ Python åŒ…æ˜¯ä»€ä¹ˆï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79081793/combining-several-supervised-learning-techniques</guid>
      <pubDate>Sat, 12 Oct 2024 19:52:24 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆé£æœºæ²¡æœ‰æ˜¾ç¤ºåœ¨ matplotlib å›¾ä¸­</title>
      <link>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å¯¹å…·æœ‰ 13 ä¸ªç‰¹å¾çš„æ³¢å£«é¡¿æˆ¿å±‹æ•°æ®é›†å®æ–½ SLPã€‚æˆ‘ä¸º X é€‰æ‹©â€œrmâ€å’Œâ€œznâ€ï¼Œä¸ºç›®æ ‡ Y é€‰æ‹©â€œmedvâ€ã€‚æˆ‘è¿˜ä»å¤´å®æ–½äº†ä¸€ä¸ªæ„ŸçŸ¥å™¨ç±»ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæˆ‘æœ‰ä¸€ä¸ªåä¸º plot_losses çš„å‡½æ•°ï¼Œå®ƒåœ¨ä¸€ä¸ªçª—å£ä¸­ç»˜åˆ¶é¢„æµ‹çº¿ï¼ˆ2dï¼‰å’ŒæŸå¤±ï¼Œè¿˜ç»˜åˆ¶ 3d å›¾çš„é¢„æµ‹å¹³é¢ï¼Œè¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ï¼Œå³ 3d éƒ¨åˆ†ã€‚
å¹³é¢æœªæ˜¾ç¤ºåœ¨ 3d æ•£ç‚¹å›¾ä¸Šã€‚
æ„ŸçŸ¥å™¨ç±»å®ç°ï¼š
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

æ„ŸçŸ¥å™¨ç±»ï¼š
def __init__(self, input_size, lr, epochs):
self.w = np.zeros(input_size)
self.b = 0
self.lr = lr
self.epochs = epochs
self.losses = []

def fit(self, X_train, Y_train):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
y_pred = np.dot(x, self.w) + self.b
error = y - y_pred

self.w = self.w + (error * x * self.lr)
self.b = self.b + (error * self.lr)

loss = np.mean(np.abs(error))
self.losses.append(loss)

def predict(self, X_test):
return np.dot(X_test, self.w) + self.b

def plot_losses(self, X_train, Y_train, ax1_title, ax2_title, plot_3d=False, plot_3d_title=&#39;3D Plot&#39;):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
Y_pred = np.dot(x, self.w) + self.b

if plot_3d:
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

X_feature1 = X_train[:, 0]
X_feature2 = X_train[:, 1]

ax.scatter(X_feature1, X_feature2, Y_train, color=&#39;blue&#39;, label=&#39;True Values&#39;)

X1_grid, X2_grid = np.meshgrid(
np.linspace(X_feature1.min(), X_feature1.max(), 20),
np.linspace(X_feature2.min(), X_feature2.max(), 20)
)

Z_pred = self.w[0] * X1_grid + self.w[1] * X2_grid + self.b

ax.plot_surface(X1_grid, X2_grid, Z_pred, color=&#39;red&#39;, alpha=0.5)
ax.set_xlabel(&quot;ç‰¹å¾ &#39;rm&#39;&quot;)
ax.set_ylabel(&quot;ç‰¹å¾ &#39;zn&#39;&quot;)
ax.set_zlabel(&quot;ç›®æ ‡ &#39;medvâ€‹â€‹&#39;&quot;)
ax.set_title(plot_3d_title)
ax.legend()
plt.show()
else:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))

ax1.scatter(X_train[:, 0], Y_train, color=&#39;blue&#39;, label=&#39;çœŸå€¼&#39;)
ax1.plot(X_train[:, 0], Y_pred, color=&#39;red&#39;, label=&#39;é¢„æµ‹å€¼Line&#39;)
ax1.set_title(ax1_title)
ax1.legend()

ax2.plot(self.losses)
ax2.set_title(ax2_title)
ax2.set_xlabel(&quot;Epochs&quot;)
ax2.set_ylabel(&quot;å‡æ–¹è¯¯å·® (MSE)&quot;)

plt.tight_layout()
plt.show()

æ³¢å£«é¡¿æˆ¿å±‹æ•°æ®é›†çš„çº¿æ€§å›å½’ï¼š
%matplotlib qt
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from perceptron import Perceptron
df_boston = pd.read_csv(&#39;input/BostonHousing.csv&#39;)
X = df_boston[[&#39;rm&#39;,&#39;zn&#39;]].values
Y = df_boston[&#39;medvâ€‹â€‹&#39;].values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=.2)

slp = Perceptron(2, .01, 100)
slp.fit(X_train, Y_train) 
slp.plot_losses(X_train,Y_train, &#39;å‘˜å·¥å·¥èµ„å’Œç»éªŒæ„ŸçŸ¥å™¨&#39;, &#39;æŸå¤±å€¼&#39;, plot_3d=True, plot_3d_title=&#39;æ³¢å£«é¡¿ä½æˆ¿æ„ŸçŸ¥å™¨&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</guid>
      <pubDate>Sat, 12 Oct 2024 19:31:21 GMT</pubDate>
    </item>
    <item>
      <title>æ— æ³•ä½¿ç”¨ load_state_dict åŠ è½½æˆ‘çš„æ¨¡å‹ï¼šRuntimeErrorï¼šä¸º UNetGenerator åŠ è½½ state_dict æ—¶å‡ºé”™ï¼šstate_dict ä¸­å‡ºç°æ„å¤–é”®</title>
      <link>https://stackoverflow.com/questions/79081715/cant-load-my-model-using-load-state-dict-runtimeerror-errors-in-loading-sta</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79081715/cant-load-my-model-using-load-state-dict-runtimeerror-errors-in-loading-sta</guid>
      <pubDate>Sat, 12 Oct 2024 19:09:08 GMT</pubDate>
    </item>
    <item>
      <title>ç»“åˆ CNN å’Œéšæœºæ£®æ—æ–¹æ³•ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79081396/combining-cnn-and-random-forest-approach</link>
      <description><![CDATA[æˆ‘çš„æ•°æ®é›†åŒ…å«å¤§é‡å›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼ˆå­˜å‚¨åœ¨ .csv æ–‡ä»¶ä¸­ï¼‰ã€‚æˆ‘æ‰“ç®—ä½¿ç”¨æˆ‘æ‹¥æœ‰çš„æ‰€æœ‰æ•°æ®ï¼ˆå›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼‰åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¯¹æ¡ˆä¾‹ A å’Œæ¡ˆä¾‹ B è¿›è¡Œåˆ†ç±»/è¯†åˆ«çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
æ˜¯å¦å¯ä»¥ç»“åˆå‡ ç§ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨ CNN å¤„ç†å›¾åƒï¼Œä½¿ç”¨éšæœºæ£®æ—åˆ†æåˆ¶è¡¨æ•°æ®ï¼Ÿç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå¯¹ä¸¤ç§ç±»å‹çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ç»„åˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å°†æ¡ˆä¾‹ A å’Œæ¡ˆä¾‹ B è¿›è¡Œåˆ†ç±»ã€‚
æˆ‘çš„é—®é¢˜æ˜¯è¿™ç§æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­æ˜¯å¦å¯è¡Œã€‚
æ˜¯å¦å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é›†æˆå›¾åƒå’Œåˆ¶è¡¨æ•°æ®ï¼Ÿ
å¯¹äºè¿™äº›ç±»å‹çš„æ•°æ®ï¼Œé€šå¸¸æ¨èä½¿ç”¨å“ªç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ
æˆ‘è®¤ä¸º CNN æ˜¯å›¾åƒçš„ä¸é”™é€‰æ‹©ï¼Œä½†æˆ‘ä¸ç¡®å®šåˆ¶è¡¨æ•°æ®çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆã€‚
æœ€é€‚åˆç”¨äºæ­¤ä»»åŠ¡çš„ Python åŒ…æ˜¯ä»€ä¹ˆï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79081396/combining-cnn-and-random-forest-approach</guid>
      <pubDate>Sat, 12 Oct 2024 16:08:46 GMT</pubDate>
    </item>
    <item>
      <title>ValueErrorï¼šX æœ‰ 14 ä¸ªç‰¹å¾ï¼Œä½† LogisticRegression éœ€è¦ 17128 ä¸ªç‰¹å¾ä½œä¸ºè¾“å…¥ [é‡å¤]</title>
      <link>https://stackoverflow.com/questions/79081243/valueerror-x-has-14-features-but-logisticregression-is-expecting-17128-feature</link>
      <description><![CDATA[# é¢„æµ‹ç³»ç»Ÿ
# content = input(&quot;è¾“å…¥æ‚¨çš„æ–‡æœ¬ï¼š&quot;)

# è®¿é—®æ–‡æœ¬å†…å®¹è€Œä¸æ˜¯ csr_matrix å…ƒç´ 
text_content = X_train[20].toarray()[0]

# å‡è®¾ X_train åŒ…å«æ–‡æœ¬æ•°æ®ï¼›å¦‚æœä¸åŒ…å«ï¼Œåˆ™è¿›è¡Œç›¸åº”è°ƒæ•´ã€‚
# å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸ºå¸¸è§„ Python å­—ç¬¦ä¸²ï¼š
text_content = &quot; &quot;.join([str(element) for element in text_content if element != 0])

processed_content = preprocess_and_translate(text_content)
print(&quot;å¤„ç†åçš„å†…å®¹ï¼ˆè‹±æ–‡ï¼‰ï¼š&quot;,processed_content)

vectorizer = TfidfVectorizer()

vectorizer.fit_transform([processed_content])

input_data = vectorizer.transform([processed_content])

prediction = model.predict(input_data)
if prediction[0] == 1:
print(&#39;å‡æ–°é—»&#39;)
else:
print(&#39;çœŸå®æ–°é—»&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79081243/valueerror-x-has-14-features-but-logisticregression-is-expecting-17128-feature</guid>
      <pubDate>Sat, 12 Oct 2024 14:51:02 GMT</pubDate>
    </item>
    <item>
      <title>ç”¨äºå¢é‡å­¦ä¹ çš„ Python éçº¿æ€§å›å½’å™¨</title>
      <link>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</link>
      <description><![CDATA[æˆ‘æƒ³çŸ¥é“ scikit-learn ä¸­æ˜¯å¦æœ‰ä¸€ä¸ªéçº¿æ€§å›å½’ç¨‹åºï¼Œå…è®¸å¢é‡å­¦ä¹ ï¼Œå³é€šè¿‡ partial_fit è°ƒç”¨ã€‚æˆ‘å‘ç° SGDRegressor å’Œ PassiveAggressiveRegressor éƒ½å…è®¸ partial_fitï¼Œä½†å®ƒä»¬æ˜¯çº¿æ€§çš„ï¼Œè€Œæˆ‘çš„æ•°æ®æ˜¾ç„¶æ˜¯éçº¿æ€§çš„ï¼Œå› æ­¤æ‹Ÿåˆæ•ˆæœå¹¶ä¸ç†æƒ³ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</guid>
      <pubDate>Mon, 07 Oct 2024 21:18:59 GMT</pubDate>
    </item>
    <item>
      <title>åªæœ‰è¾“å…¥å¼ é‡å¯ä»¥ä½œä¸ºä½ç½®å‚æ•°ä¼ é€’</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[ä» PIL å¯¼å…¥å›¾åƒ
å¯¼å…¥ matplotlib.pyplot ä½œä¸º plt
å¯¼å…¥ argparse
å¯¼å…¥ pickle
å¯¼å…¥ numpy
ä½œä¸º np
ä» tensorflow å¯¼å…¥ keras
ä» keras.applications.xception å¯¼å…¥ Xception
ä» keras.preprocessing.sequence å¯¼å…¥ pad_sequences
ä» tensorflow.keras.preprocessing.text å¯¼å…¥ Tokenizer
å¯¼å…¥ tensorflow ä½œä¸º tf

# ä½¿ç”¨ Lambda å®šä¹‰è‡ªå®šä¹‰å±‚ï¼ˆä¸å¸¦ name å‚æ•°ï¼‰
class NotEqual(tf.keras.layers.Layer):
def __init__(self, name=None):
super(NotEqual, self).__init__(name=name)

def call(self, x, y): # ä½¿ç”¨å…³é”®å­—å‚æ•°â€œxâ€å’Œâ€œyâ€
return tf.math.not_equal(x, y)

# å®šä¹‰ç”¨äºæå–ç‰¹å¾ã€ç”Ÿæˆæè¿°çš„å‡½æ•°ï¼Œå’Œå…¶ä»–å¿…è¦çš„å®ç”¨ç¨‹åº
def extract_features(filename, model):
try:
image = Image.open(filename)
except:
print(&quot;ERROR: æ— æ³•æ‰“å¼€å›¾åƒï¼è¯·ç¡®ä¿å›¾åƒè·¯å¾„å’Œæ‰©å±•åæ­£ç¡®&quot;)
image = image.resize((299, 299))
image = np.array(image)
if image.shape[2] == 4:
image = image[..., :3]
image = np.expand_dims(image, axis=0)
image = image / 127.5
image = image - 1.0
feature = model.predict(image)
return feature

def word_for_id(integer, tokenizer):
for word, index in tokenizer.word_index.items():
if index == integer:
return word
return None

def generate_desc(model, tokenizer, photo, max_length):
in_text = &#39;start&#39;
for i in range(max_length):
sequence = tokenizer.texts_to_sequences([in_text])[0]
sequence = pad_sequences([sequence], maxlen=max_length)
pred = model.predict({&#39;image_input&#39;: photo, &#39;text_input&#39;:sequence}) # å°†è¾“å…¥ä½œä¸ºå­—å…¸ä¼ é€’
pred = np.argmax(pred)
word = word_for_id(pred, tokenizer)
if word is None:
break
in_text += &#39; &#39; + word
if word == &#39;end&#39;:
break
return in_text

# è§£æå‚æ•°
ap = argparse.ArgumentParser()
ap.add_argument(&#39;-i&#39;, &#39;--image&#39;, required=True, help=&quot;Image Path&quot;)
args = vars(ap.parse_args())
img_path = args[&#39;image&#39;]

# åŠ è½½tokenizer
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;, &quot;rb&quot;))

# å®šä¹‰æ¨¡å‹çš„è·¯å¾„
model_path = &#39;models/model_9.h5&#39;
# ä½¿ç”¨è‡ªå®šä¹‰å¯¹è±¡ï¼ˆåŒ…æ‹¬ NotEqual å±‚ï¼‰åŠ è½½æ¨¡å‹
ä½¿ç”¨ keras.utils.custom_object_scope({&#39;NotEqual&#39;: NotEqual}):
model = tf.keras.models.load_model(model_path)

æˆ‘å°è¯•ä»¥å„ç§æ–¹å¼è¿è¡Œæ­¤ä»£ç ï¼Œä½†å‡ºç°é”™è¯¯ï¼š
 model = tf.keras.models.load_model(model_path)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
æ–‡ä»¶â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\saving\saving_api.pyâ€ï¼Œç¬¬ 183 è¡Œï¼Œä½äº load_model
return legacy_h5_format.load_model_from_hdf5(filepath)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
æ–‡ä»¶ â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\legacy_h5_format.pyâ€ï¼Œç¬¬ 133 è¡Œï¼Œä½äº load_model_from_hdf5
model = saving_utils.model_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
æ–‡ä»¶â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\saving_utils.pyâ€ï¼Œç¬¬ 85 è¡Œï¼Œä½äº model_from_config
return serialization.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
æ–‡ä»¶â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\serialization.pyâ€ï¼Œç¬¬ 495 è¡Œï¼Œä½äº deserialize_keras_object
deserialized_obj = cls.from_config(
^^^^^^^^^^^^^^^^^
æ–‡ä»¶â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\model.pyâ€ï¼Œç¬¬ 528 è¡Œï¼Œä½äº from_config ä¸­
return functional_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^
æ–‡ä»¶â€œC:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.pyâ€ï¼Œç¬¬ 528 è¡Œï¼Œä½äº functional_from_config ä¸­
process_node(layer, node_data)
æ–‡ä»¶&quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.py&quot;ï¼Œç¬¬ 475 è¡Œï¼Œä½äº process_node
layer(*args, **kwargs)
æ–‡ä»¶ &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;ï¼Œç¬¬ 122 è¡Œï¼Œä½äº error_handler
raise e.with_traceback(filtered_tb) from None
æ–‡ä»¶ &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\layers\layer.py&quot;ï¼Œç¬¬ 721 è¡Œï¼Œä½äº __call__
raise ValueError(
ValueError: åªèƒ½å°†è¾“å…¥å¼ é‡ä½œä¸ºä½ç½®å‚æ•°ä¼ é€’ã€‚ä»¥ä¸‹å‚æ•°å€¼åº”ä½œä¸ºå…³é”®å­—å‚æ•°ä¼ é€’ï¼š0ï¼ˆç±»å‹ä¸º &lt;class &#39;int&#39;&gt;ï¼‰
]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ Transformer éœ€è¦ä¸€å †ç¼–ç å™¨ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å…³æ³¨è¿™ç¯‡å…³äº transformers çš„åšå®¢
http://jalammar.github.io/illustrated-transformer/
æˆ‘å”¯ä¸€ä¸æ˜ç™½çš„æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ä¸€å †ç¼–ç å™¨æˆ–è§£ç å™¨ã€‚æˆ‘çŸ¥é“å¤šå¤´æ³¨æ„åŠ›å±‚æ•è·äº†é—®é¢˜çš„ä¸åŒè¡¨ç¤ºç©ºé—´ã€‚æˆ‘ä¸æ˜ç™½ä¸ºä»€ä¹ˆéœ€è¦ä¸€å †ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ä¸€ä¸ªç¼–ç å™¨/è§£ç å™¨å±‚ä¸è¡Œå—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</guid>
      <pubDate>Wed, 18 Dec 2019 00:57:26 GMT</pubDate>
    </item>
    <item>
      <title>CNN æ¶æ„ï¼šå¯¹â€œå¥½â€å›¾åƒå’Œâ€œåâ€å›¾åƒè¿›è¡Œåˆ†ç±»</title>
      <link>https://stackoverflow.com/questions/57943425/cnn-architecture-classifying-good-and-bad-images</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç ”ç©¶å®ç° CNN çš„å¯èƒ½æ€§ï¼Œä»¥ä¾¿å°†å›¾åƒåˆ†ç±»ä¸ºâ€œå¥½â€æˆ–â€œåâ€ï¼Œä½†æˆ‘ç›®å‰çš„æ¶æ„æ²¡æœ‰æˆåŠŸã€‚
è¡¨ç¤ºâ€œåâ€çš„ç‰¹å¾å›¾åƒï¼š

è¿‡åº¦æ›å…‰
è¿‡åº¦é¥±å’Œ
ç™½å¹³è¡¡ä¸æ­£ç¡®
æ¨¡ç³Š

æ ¹æ®è¿™äº›ç‰¹å¾å®ç°ç¥ç»ç½‘ç»œå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»æ˜¯å¦å¯è¡Œï¼Œè¿˜æ˜¯æœ€å¥½ä½¿ç”¨ä¼ ç»Ÿç®—æ³•ï¼Œè¯¥ç®—æ³•ä»…æŸ¥çœ‹æ•´ä¸ªå›¾åƒçš„äº®åº¦/å¯¹æ¯”åº¦å˜åŒ–å¹¶ä»¥æ­¤æ–¹å¼è¿›è¡Œåˆ†ç±»ï¼Ÿ
æˆ‘æ›¾å°è¯•ä½¿ç”¨ VGGNet æ¶æ„è®­ç»ƒ CNNï¼Œä½†æ— è®º epoch æ•°æˆ–æ­¥éª¤æ•°æœ‰å¤šå°‘ï¼Œæˆ‘ä¼¼ä¹æ€»æ˜¯å¾—åˆ°ä¸€ä¸ªæœ‰åå·®ä¸”ä¸å¯é çš„æ¨¡å‹ã€‚
ç¤ºä¾‹ï¼š

æˆ‘å½“å‰çš„æ¨¡å‹æ¶æ„éå¸¸ç®€å•ï¼ˆå› ä¸ºæˆ‘å¯¹æ•´ä¸ªæœºå™¨å­¦ä¹ ä¸–ç•Œè¿˜å¾ˆé™Œç”Ÿï¼‰ï¼Œä½†ä¼¼ä¹å¯ä»¥å¾ˆå¥½åœ°å¤„ç†å…¶ä»–åˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸”æˆ‘å¯¹å…¶è¿›è¡Œäº†è½»å¾®ä¿®æ”¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†è¿™ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼š
 # CONV =&gt; RELU =&gt; POOL å±‚é›†
# å®šä¹‰å·ç§¯å±‚ï¼Œä½¿ç”¨&quot;ReLU&quot;æ¿€æ´»å‡½æ•°
# å¹¶ä½¿ç”¨æ± åŒ–å±‚å‡å°‘ç©ºé—´å¤§å°ï¼ˆå®½åº¦å’Œé«˜åº¦ï¼‰
model.add(Conv2D(32, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 32 ä¸ª 3x3 è¿‡æ»¤å™¨ï¼ˆé«˜åº¦ã€å®½åº¦ã€æ·±åº¦ï¼‰
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆï¼ˆ25% çš„ç¥ç»å…ƒéšæœºæ–­å¼€è¿æ¥ï¼‰

# (CONV =&gt; RELU) * 2 =&gt; POOL å±‚é›†ï¼ˆéšç€ CNN çš„æ·±å…¥ï¼Œå±‚æ•°ä¼šå¢åŠ ï¼‰
model.add(Conv2D(64, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 64 ä¸ª 3x3 è¿‡æ»¤å™¨
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(64, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 64 ä¸ª 3x3 è¿‡æ»¤å™¨
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆ (25%éšæœºæ–­å¼€çš„ç¥ç»å…ƒæ•°é‡)

# (CONV =&gt; RELU) * 3 =&gt; POOL å±‚é›†ï¼ˆè¾“å…¥ä½“ç§¯å¤§å°è¶Šæ¥è¶Šå°ï¼‰
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 ä¸ª 3x3 æ»¤æ³¢å™¨
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 ä¸ª 3x3 æ»¤æ³¢å™¨
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 3x3 è¿‡æ»¤å™¨
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆï¼ˆ25% çš„ç¥ç»å…ƒéšæœºæ–­å¼€è¿æ¥ï¼‰

# ä»…è®¾ç½® FC =&gt; RELU å±‚
model.add(Flatten())
model.add(Dense(512))
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization())
model.add(Dropout(0.5))

# sigmoid åˆ†ç±»å™¨ï¼ˆè¾“å‡ºå±‚ï¼‰
model.add(Dense(classes))
model.add(Activation(&quot;sigmoid&quot;))

è¿™ä¸ªæ¨¡å‹æ˜¯å¦æœ‰ä»»ä½•æ˜æ˜¾çš„é—æ¼æˆ–é”™è¯¯ï¼Œæˆ–è€…æˆ‘æ ¹æœ¬æ— æ³•ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼ˆä½¿ç”¨æˆ‘å½“å‰çš„ GPUï¼ŒGTX 970ï¼‰è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ
è¿™æ˜¯æˆ‘ç¼–è¯‘/è®­ç»ƒæ¨¡å‹çš„ä»£ç ï¼š
# åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
print(&quot;[INFO] Training network...&quot;)
opt = SGD(lr=initial_lr, decay=initial_lr / epochs)
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;])

# è®¾ç½®æ£€æŸ¥ç‚¹
model_name = &quot;output/50_epochs_{epoch:02d}_{val_acc:.2f}.model&quot;
checkpoint = ModelCheckpoint(model_name, monitor=&#39;val_acc&#39;, verbose=1, 
save_best_only=True, mode=&#39;max&#39;)
reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.2,
waiting=5, min_lr=0.001)
tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(time()))
callbacks_list = [checkpoint, reduce_lr, tensorboard]

# è®­ç»ƒç½‘ç»œ
H = model.fit_generator(training_set, steps_per_epoch=500, epochs=50, validation_data=test_set, validation_steps=150, callbacks=callbacks_list)
]]></description>
      <guid>https://stackoverflow.com/questions/57943425/cnn-architecture-classifying-good-and-bad-images</guid>
      <pubDate>Sun, 15 Sep 2019 10:53:49 GMT</pubDate>
    </item>
    <item>
      <title>çº¿æ€§å›å½’çˆ†ç‚¸çš„æ¢¯åº¦ä¸‹é™</title>
      <link>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨æ­¤èµ„æºå®ç°çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼šhttps://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
æˆ‘çš„é—®é¢˜æ˜¯ï¼Œæˆ‘çš„æƒé‡æ­£åœ¨çˆ†ç‚¸å¼å¢é•¿ï¼ˆå‘ˆæŒ‡æ•°å¢é•¿ï¼‰ï¼Œå¹¶ä¸”æœ¬è´¨ä¸Šä¸é¢„æœŸç›¸åã€‚
é¦–å…ˆï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼š
def y(x, a):
return 2*x + a*np.random.random_sample(len(x)) - a/2

x = np.arange(20)
y_true = y(x,10)

çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

è¦ä¼˜åŒ–çš„çº¿æ€§å‡½æ•°ï¼š
def y_predict(x, m, b):
return m*x + b

å› æ­¤ï¼Œå¯¹äºä¸€äº›éšæœºé€‰æ‹©çš„å‚æ•°ï¼Œç»“æœå¦‚ä¸‹ï¼š
m0 = 1
b0 = 1

a = y_predict(x, m0, b0)

plt.scatter(x, y_true)
plt.plot(x, a)
plt.show()


ç°åœ¨æˆæœ¬çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
cost = (1/2)* np.sum((y_true - a) ** 2)

æˆæœ¬ç›¸å¯¹äºé¢„æµ‹ (dc_da) çš„åå¯¼æ•°ï¼š
dc_da = (a - y_true) # ä»ç„¶æ˜¯ä¸€ä¸ªå‘é‡

æˆæœ¬ç›¸å¯¹äºæ–œç‡å‚æ•° (dc_dm) çš„åå¯¼æ•°ï¼š
dc_dm = dc_da.dot(x) # ç°åœ¨æ˜¯ä¸€ä¸ªå¸¸æ•°

æˆæœ¬ç›¸å¯¹äº y æˆªè·å‚æ•° (dc_db) çš„åå¯¼æ•°ï¼š
dc_db = np.sum(dc_da) # ä¹Ÿæ˜¯ä¸€ä¸ªå¸¸æ•°

æœ€åæ˜¯æ¢¯åº¦ä¸‹é™çš„å®ç°ï¼š
iterations = 10

m0 = 1

b0 = 1

learning_rate = 0.1

N = len(x)

for i in range(iterations):

a = y_predict(x, m0, b0)

cost = (1/2) * np.sum((y_true - a) ** 2)

dc_da = (a - y_true)

mgrad = dc_da.dot(x)
bgrad = np.sum(dc_da)

m0 -= learning_rate * (2 / N) * mgrad
b0 -= learning_rate * (2 / N) * bgrad

if (i % 2 == 0):
print(&quot;Iteration {}&quot;.format(i))
print(&quot;Cost: {}, m: {}, b: {}\n&quot;.format(cost, m0, b0))

ç»“æœä¸ºï¼š
è¿­ä»£ 0
Cost: 1341.5241150881411, m: 26.02473879743261, b: 2.8683883457327797

è¿­ä»£ 2
Cost: 409781757.38124645, m: 13657.166910552878, b: 1053.5831308528543

è¿­ä»£ 4
Cost: 132510115599264.75ï¼Œmï¼š7765058.4350503925ï¼Œbï¼š598610.1166795876

è¿­ä»£ 6
æˆæœ¬ï¼š4.284947676217907e+19ï¼Œmï¼š4415631880.089208ï¼Œbï¼š340401694.5610262

è¿­ä»£ 8
æˆæœ¬ï¼š1.3856132043127762e+25ï¼Œmï¼š2510967578365.3584ï¼Œbï¼š193570850213.62192

æˆ‘çš„å®ç°æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</guid>
      <pubDate>Mon, 07 May 2018 16:54:52 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å°†ç”¨PCAå’Œéšæœºæ£®æ—è®­ç»ƒçš„æ¨¡å‹åº”ç”¨äºæµ‹è¯•æ•°æ®ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/36382572/how-to-apply-model-trained-with-pca-and-random-forest-to-test-data</link>
      <description><![CDATA[åœ¨è§£å†³ä¸€ä¸ªæœºå™¨å­¦ä¹ é—®é¢˜æ—¶ï¼Œæˆ‘åœ¨è®­ç»ƒæ•°æ®ä¸Šå®æ–½ PCAï¼Œç„¶åä½¿ç”¨ sklearn åœ¨è®­ç»ƒæ•°æ®ä¸Šåº”ç”¨ .transformã€‚è§‚å¯Ÿæ–¹å·®åï¼Œæˆ‘åªä¿ç•™æ–¹å·®è¾ƒå¤§çš„è½¬æ¢æ•°æ®ä¸­çš„é‚£äº›åˆ—ã€‚ç„¶åæˆ‘ä½¿ç”¨ RandomForestClassifier è®­ç»ƒæ¨¡å‹ã€‚ç°åœ¨ï¼Œæˆ‘å¾ˆå›°æƒ‘ï¼šå¦‚ä½•å°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºæµ‹è¯•æ•°æ®ï¼Œå› ä¸ºæµ‹è¯•æ•°æ®çš„åˆ—æ•°å’Œä¿ç•™çš„è½¬æ¢æ•°æ®ï¼ˆåº”ç”¨éšæœºæ£®æ—ï¼‰ä¸åŒï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/36382572/how-to-apply-model-trained-with-pca-and-random-forest-to-test-data</guid>
      <pubDate>Sun, 03 Apr 2016 07:07:01 GMT</pubDate>
    </item>
    </channel>
</rss>