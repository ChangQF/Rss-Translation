<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 27 Oct 2024 01:21:01 GMT</lastBuildDate>
    <item>
      <title>通过强化学习进行路径规划</title>
      <link>https://stackoverflow.com/questions/79129736/path-planning-with-reinforcement-learning</link>
      <description><![CDATA[我有一个包含地形高程信息的数据集，我正在尝试使用强化学习进行最佳路径规划。我对强化学习没有任何先验知识，有人能帮我看看我该怎么做吗？我正在尝试寻找有关它的教程和代码，但我被卡住了。
我尝试创建一个可以运行的环境，但我不知道]]></description>
      <guid>https://stackoverflow.com/questions/79129736/path-planning-with-reinforcement-learning</guid>
      <pubDate>Sun, 27 Oct 2024 01:02:07 GMT</pubDate>
    </item>
    <item>
      <title>NLP 疾病检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/79128775/nlp-diseases-detection</link>
      <description><![CDATA[我正在做一个项目，目标是从不同的句子中检测出疾病名称
我需要一些建议
我正在寻找可以从中获取疾病名称列表的库，这样我至少可以从我的数据框中检测出疾病
然后我想通过从这种疾病中训练来使用 NER 名称实体识别
希望有人能指导我并给我一些建议
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79128775/nlp-diseases-detection</guid>
      <pubDate>Sat, 26 Oct 2024 14:45:31 GMT</pubDate>
    </item>
    <item>
      <title>用于实时音频分类的深度学习预训练模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79128715/deep-learning-pre-trained-model-for-real-time-audio-classification</link>
      <description><![CDATA[对于 Raspberry Pi 3（可用的 mcu）等微控制器上的实时音频分类系统，最好的预训练深度学习模型是什么？如果模型可以进行音频到音频转换，则可获得额外奖励，这意味着输出取决于音频是否通过分类。
我正在尝试创建自己的模型。但是，使用预训练模型并将其用于我的情况似乎更好，因为训练我自己的模型需要花费大量时间。]]></description>
      <guid>https://stackoverflow.com/questions/79128715/deep-learning-pre-trained-model-for-real-time-audio-classification</guid>
      <pubDate>Sat, 26 Oct 2024 14:15:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 查找奇偶分类器 sin(w*x)^2 中 w 的值</title>
      <link>https://stackoverflow.com/questions/79128525/using-pytorch-to-find-value-of-w-in-sinwx2-for-an-even-odd-classifier</link>
      <description><![CDATA[这不是重复的，因为其他关于奇偶分类的问题都没有尝试使用这个特定的函数来学习，而是使用通常的 ReLU 或 sigmoid。
我正在尝试估计函数 x -&gt; sin(w*x)^2 中的参数 w，以将整数分类为偶数或奇数，使用 pytorch 作为自我分配的练习。当然，正确的 w 有多个可能的值，包括 w = pi/2。我初始化了我的网络（一个无偏差的线性网络，然后是 sin 激活，然后是平方），其中 w = 1.5 接近 pi/2，希望它收敛到 pi/1 = 1.507...，但无论我如何调整学习率或使用什么优化器，模型都没有学习。
class Net(torch.nn.Module):
def __init__(self):
super().__init__()
# 无偏差的线性网络 
self.fc1 = torch.nn.Linear(1, 1, bias=False)
# 初始化接近理论解
with torch.no_grad():
self.fc1.weight.data.fill_(1.5) # 接近 π/2 ≈ 1.57

def forward(self, x):
x = self.fc1(x)
return torch.sin(x)**2

net = Net()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.0004)

weights = []
for epoch in range(100):
net.train()

optimizer.zero_grad()
output = net(train_x)

loss = criterion(output, train_y)
loss.backward()
optimizer.step()

weights.append(w)

权重图表明没有趋向于任何一点。
我想相信我避免了常见的陷阱：我让输入和输出为 float 32，目标函数可以使用模型完美学习，我也尝试过其他损失函数，但失败了。
请帮我找到我犯了一个错误，这里是完整的代码（从 jupyter 笔记本导出）：
# %%
import torch
import numpy as np
import pandas as pd

# %%
# 生成数据并缩放输入
def generate_data(size):
x = np.random.randint(0, size, size) # 范围较小，可视化效果更好
return x.astype(float), (x % 2).astype(float)

# %%
# 生成数据集
train_x, train_y = generate_data(1000)
val_x, val_y = generate_data(1000)

# 转换为张量
train_x = torch.tensor(train_x, dtype=torch.float32).reshape(-1, 1)
train_y = torch.tensor(train_y, dtype=torch.float32).reshape(-1, 1)
val_x = torch.tensor(val_x, dtype=torch.float32).reshape(-1, 1)
val_y = torch.tensor(val_y, dtype=torch.float32).reshape(-1, 1)

# %%
class Net(torch.nn.Module):
def __init__(self):
super().__init__()
# 无偏差线性 
self.fc1 = torch.nn.Linear(1, 1, bias=False)
# 初始化接近理论解
with torch.no_grad():
self.fc1.weight.data.fill_(1.5) # 接近 π/2 ≈ 1.57

def forward(self, x):
x = self.fc1(x)
return torch.sin(x)**2

# %%
net = Net()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.0004)

# %%
weights = []
for epoch in range(100):
net.train()

optimizer.zero_grad()
output = net(train_x)

loss = criterion(output, train_y)
loss.backward()
optimizer.step()

net.eval()
with torch.no_grad():
val_output = net(val_x)
val_loss = criterion(val_output, val_y)

if epoch % 1 == 0:
print(f&quot;Epoch {epoch}&quot;)
print(f&quot;Loss: {loss.item():.8f} Val损失：{val_loss.item():.8f}&quot;)
w = net.fc1.weight.item()
print(f&quot;权重：{w:.8f} (目标：{np.pi/2:.8f})&quot;)
print(&quot;---&quot;)

weights.append(w)

# %%
# 绘制权重
import matplotlib.pyplot as plt
plt.plot(weights)
plt.plot([np.pi/2]*len(weights))

# %%
# 测试模型
w = net.fc1.weight.item()
print(&quot;\n最终参数：&quot;)
print(f&quot;权重：{w:.8f} (目标：{np.pi/2:.8f})&quot;)

# 对偶数和奇数进行测试
test_numbers = np.arange(0, 1500, 1)
net.eval()
使用 torch.no_grad():
for x in test_numbers:
test_input = torch.tensor([[float(x)]], dtype=torch.float32)
pred = net(test_input).item()
print(&quot;✅&quot; if (pred &lt; 0.5) == (x % 2 == 0) else &quot;❌&quot;, end=&quot;&quot;)
if (x+1) % 60 == 0:
print()

]]></description>
      <guid>https://stackoverflow.com/questions/79128525/using-pytorch-to-find-value-of-w-in-sinwx2-for-an-even-odd-classifier</guid>
      <pubDate>Sat, 26 Oct 2024 12:24:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个简单的机器学习代码会给出错误的答案？</title>
      <link>https://stackoverflow.com/questions/79127884/why-does-this-simple-machine-learning-code-give-the-wrong-answer</link>
      <description><![CDATA[我正在尝试学习一些时间序列神经网络 ML，但得到的解有些奇怪，因此我尝试对我能想到的最简单的非平凡情况进行建模，即预测 n+1 为序列 0,1,2,3,...n 中的下一个数字（使用 LSTM 模型）。
每个数据点的训练数据是一系列紧接在前的数字，我假设只要每个训练集的数据长度 &gt;= 2（因为它是一个算术序列），它就应该很容易解决模型。
无论训练系列的大小如何，下面的代码都会为所有测试数据返回一个常数。有人能解释一下我做错了什么吗？
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

import statistics

dim = 5

data = pd.Series(range(0,200))

# 设置 80% 的数据用于训练
training_data_len = math.ceil(len(data) * .8)

# 规范化数据
train_data = data[:training_data_len]

# 拆分数据集
train_data = data[:training_data_len]
test_data = data[training_data_len:]
print(train_data.shape, test_data.shape)

# 选择值
dataset_train = train_data.values 
# 将 1D 重塑为 2D 数组
dataset_train = np.reshape(dataset_train, (-1,1))

# 选择值
dataset_test = test_data.values
# 将 1D 数组重塑为 2D 数组
dataset_test = np.reshape(dataset_test, (-1,1)) 

X_train = []
y_train = []
for i in range(dim, len(dataset_train)):
X_train.append(dataset_train[i-dim:i, 0])
y_train.append(dataset_train[i, 0])

X_test = []
y_test = []
for i in range(dim, len(dataset_test)):
X_test.append(dataset_test[i-dim:i, 0])
y_test.append(dataset_test[i, 0])

# 将数据转换为 Numpy 数组
X_train, y_train = np.array(X_train), np.array(y_train)

#重塑
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
y_train = np.reshape(y_train, (y_train.shape[0],1))
print(&quot;X_train :&quot;,X_train.shape,&quot;y_train :&quot;,y_train.shape)

# 将数据转换为 numpy 数组
X_test, y_test = np.array(X_test), np.array(y_test)

#重塑
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))
y_test = np.reshape(y_test, (y_test.shape[0],1))
print(&quot;X_test :&quot;,X_test.shape,&quot;y_test :&quot;,y_test.shape)

# 导入库
从 keras.models 导入 Sequential
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense
从 keras.layers 导入 SimpleRNN
从 keras.layers 导入 Dropout
从 keras.layers 导入 GRU, Bidirectional
从 keras.optimizers 导入 SGD
从 sklearn 导入 metrics
从 sklearn.metrics 导入 mean_squared_error

# 初始化模型
regressorLSTM = Sequential()

# 添加 LSTM 层
regressorLSTM.add(LSTM(dim, 
return_sequences = True, 
input_shape = (X_train.shape[1],1)))
regressorLSTM.add(LSTM(dim, 
return_sequences = False))

#添加输出层
regressorLSTM.add(Dense(1))

#编译模型
regressorLSTM.compile(optimizer = &#39;adam&#39;,
loss = &#39;mean_squared_error&#39;,
metrics = [&quot;accuracy&quot;])

#拟合模型
regressorLSTM.fit(X_train, 
y_train, 
batch_size = 1, 
epochs = 4)
regressorLSTM.summary()

# 使用 X_test 数据的预测
y_LSTM = regressorLSTM.predict(X_test)

#绘制 LSTM 预测图
plt.plot(train_data.index[dim:], train_data[dim:], label = &quot;train_data&quot;, color = &quot;b&quot;)
plt.plot(test_data.index, test_data, label = &quot;test_data&quot;, color = &quot;g&quot;)
plt.plot(test_data.index[dim:], y_LSTM, label = &quot;y_LSTM&quot;, color = &quot;orange&quot;)
plt.legend()
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79127884/why-does-this-simple-machine-learning-code-give-the-wrong-answer</guid>
      <pubDate>Sat, 26 Oct 2024 05:28:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML.NET 中使用 CenterFace？模型预期形状为 (10, 3, 32, 32)</title>
      <link>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</link>
      <description><![CDATA[我尝试在 ML.NET 中使用 CenterFace ONNX，但一直出现各种错误，主要是关于输入大小的错误。
CenterFace 元数据指出，它应该有一个 10, 3, 32, 32 的输入，这对于图像检测来说已经毫无意义了 - 为算法提供 10 个批次（每个批次 32x32 像素）有什么意义？
这是我的主要代码：
 string modelPath = &quot;centerface.onnx&quot;;
var mlContext = new MLContext();

string imagePath = &quot;photo1.jpg&quot;;

var img = Image.FromFile(imagePath);
var DH = (int)(Math.Ceiling((float)img.Height / 32) * 32);
var DW = (int)(Math.Ceiling((float)img.Width / 32) * 32);

var inputData = new[] { new ModelInput { ImagePath = imagePath } };
IDataView imageData = mlContext.Data.LoadFromEnumerable(inputData);

var pipeline = mlContext.Transforms.LoadImages(outputColumnName: &quot;input.1&quot;, imageFolder: &quot;&quot;, inputColumnName: nameof(ModelInput.ImagePath))
.Append(mlContext.Transforms.ResizeImages(outputColumnName: &quot;input.1&quot;, imageWidth: DW, imageHeight: DH))
.Append(mlContext.Transforms.ExtractPixels(outputColumnName: &quot;input.1&quot;))
.Append(mlContext.Transforms.ApplyOnnxModel(
outputColumnNames: [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;],
inputColumnNames: [&quot;input.1&quot;],
modelFile: modelPath
));

var model = pipeline.Fit(imageData);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput, ModelOutput&gt;(mo​​del);
var prediction = predictionEngine.Predict(new ModelInput { ImagePath = imagePath });

使用我的 2 个模型类：
 public class ModelInput
{
public string ImagePath { get; set; }
}

public class ModelOutput
{
[ColumnName(&quot;537&quot;)] 
public float[] HeatMap { get; set; }

[ColumnName(&quot;538&quot;)]
public float[] Scale { get; set; }

[ColumnName(&quot;539&quot;)]
public float[] Offset { get; set; }

[ColumnName(&quot;540&quot;)]
public float[] Landmarks { get; set; }
}

但我确实一直收到有关输入大小的错误：

System.ArgumentException：“内存长度（3686400）必须与尺寸乘积（30720）匹配。”

30720 显然是 10x3x32x32。但同样，这有什么意义呢？
我认为我的 ONNX 坏了，但我确实有一个使用 OpenCVSharp 的工作实现：
// 这是计算机 DW 和 DH，与 ML.NET 示例中的方式相同
CenterFaceParams p = new(image, resizedSize.Width, resizedSize.Height, scoreThreshold, nmsThreshold);
Size size = new(p.DW, p.DH);

使用 Mat input = new();
Cv2.Resize(image, input, size);

使用 Mat blobInput = CvDnn.BlobFromImage(input, 1.0, size, new Scalar(0, 0, 0), true, false);
_net.SetInput(blobInput, &quot;input.1&quot;);

使用 (Mat heatMap = new())
使用 (Mat scale = new())
使用 (Mat offset = new())
使用 (Mat skylines = new())
{
_net.Forward([heatMap, scale, offset, skylines], [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;]);

CenterFaceDecodercoder = new(heatMap, scale, offset, skylines, p);
returncoder.GetOutput();
}

这个实现给了我所有 4 个层，建模后我得到了我想要的值。]]></description>
      <guid>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</guid>
      <pubDate>Thu, 24 Oct 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>summary() 函数在 cnn 中不起作用（ValueError：不支持未定义的形状。）</title>
      <link>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</link>
      <description><![CDATA[我正在尝试创建一个分类网络，用于识别来自 cifar10 数据集的图片。
当我尝试使用 summary() 函数时，我总是收到此错误。
ValueError Traceback (most recent call last)
Cell In[267], line 4
1 #base_model.summary()
2 #top_model.summary()
3 #print(base_model.output_shape)
----&gt; 4 model2.summary()

文件 c:\Users\noahc\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\noahc\anaconda3\Lib\site-packages\optree\ops.py:747，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
745 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
746 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 747 return treespec.unflatten(map(func, *flat_args))

ValueError：不支持未定义的形状。

代码如下...
import tensorflow as tf
from keras.applications import VGG16

base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3))

top_model = tf.keras.Sequential([
layer.Flatten(input_shape=base_model.output_shape[1:]),
layer.Dense(10,activation=&#39;softmax&#39;)
])

for layer in base_model.layers[:10]:
layer.trainable = False

model2 = tf.keras.models.Sequential([
base_model,
top_model
])

model2.summary() # 出现错误这里

我已经为基础模型和顶层模型做了总结，效果很好。但是当我测试 model2 时，出现了错误。不知道为什么。不确定“未定义”形状是什么意思。不知道还能尝试什么。当我只取 vgg16 的前 11 或 15 层时，总结就起作用了。我听说这可能是 python 版本本身的问题，但我不知道……]]></description>
      <guid>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</guid>
      <pubDate>Mon, 14 Oct 2024 05:50:22 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：mat1 和 mat2 形状无法相乘</title>
      <link>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</link>
      <description><![CDATA[我在 Pytorch 上构建 CNN 并收到以下错误消息：

RuntimeError：mat1 和 mat2 形状无法相乘（32x32768 和
512x256）

我已构建以下模型：
def classifier_block(input, output, kernel_size, stride, last_layer=False):
if not last_layer:
x = nn.Sequential(
nn.Conv2d(input, output, kernel_size, stride, padding=3),
nn.BatchNorm2d(output),
nn.LeakyReLU(0.2, inplace=True)
)
else:
x = nn.Sequential(
nn.Conv2d(input, output, kernel_size, stride),
nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)
返回 x

class Classifier(nn.Module):
def __init__(self, input_dim, output):
super(Classifier, self).__init__()
self.classifier = nn.Sequential(
classifier_block(input_dim, 64, 7, 2),
classifier_block(64, 64, 3, 2),
classifier_block(64, 128, 3, 2),
classifier_block(128, 256, 3, 2),
classifier_block(256, 512, 3, 2, True)
)
print(&#39;CLF: &#39;,self.classifier)

self.linear = nn.Sequential(
nn.Linear(512, 256),
nn.ReLU(inplace=True),
nn.Linear(256, 128),
nn.ReLU(inplace=True),
nn.Linear(128, 64),
nn.ReLU(inplace=True),
nn.Linear(64, output)
)
print(&#39;Linear: &#39;, self.linear)

def forward(self, image):
print(&#39;IMG: &#39;, image.shape)
x = self.classifier(image)
print(&#39;X: &#39;, x.shape)
return self.linear(x.view(len(x), -1))

输入图像的大小为 512x512。这是我的训练块：
loss_train = []
loss_val = []

for epoch in range(epochs):
print(&#39;Epoch: {}/{}&#39;.format(epoch, epochs))
total_train = 0
correct_train = 0
cumloss_train = 0
classifier.train()
for batch, (x, y) in enumerate(train_loader):
x = x.to(device)
print(x.shape)
print(y.shape)
output = classifier(x)
loss = criterion(output, y.to(device))
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(&#39;Loss: {}&#39;.format(loss))

任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</guid>
      <pubDate>Fri, 10 Mar 2023 06:48:16 GMT</pubDate>
    </item>
    <item>
      <title>如何自动判断深度学习模型的训练过程是否收敛？</title>
      <link>https://stackoverflow.com/questions/68707680/how-to-automatically-judge-whether-the-training-process-of-the-deep-learning-mod</link>
      <description><![CDATA[在训练深度学习模型的时候，需要看loss曲线和性能曲线来判断深度学习模型的训练过程是否收敛。
这浪费了我不少时间，有时候肉眼判断的收敛时间并不准确。
有没有一个算法或者包可以自动判断深度学习模型的训练过程是否收敛？]]></description>
      <guid>https://stackoverflow.com/questions/68707680/how-to-automatically-judge-whether-the-training-process-of-the-deep-learning-mod</guid>
      <pubDate>Mon, 09 Aug 2021 06:27:59 GMT</pubDate>
    </item>
    <item>
      <title>你能绘制预先训练模型的准确度图吗？</title>
      <link>https://stackoverflow.com/questions/65212386/can-you-plot-the-accuracy-graph-of-a-pre-trained-model</link>
      <description><![CDATA[我完成了一个耗时 8 小时的模型的训练，但在关闭 jupyter 笔记本之前我忘了绘制准确度图。
我需要绘制图表，并且我确实将模型保存到了硬盘中。但是如何绘制预训练模型的准确度图呢？我在网上搜索解决方案，但一无所获。]]></description>
      <guid>https://stackoverflow.com/questions/65212386/can-you-plot-the-accuracy-graph-of-a-pre-trained-model</guid>
      <pubDate>Wed, 09 Dec 2020 07:30:40 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 分类器对 mnist 数据不起作用</title>
      <link>https://stackoverflow.com/questions/64962318/pytorch-classifier-for-mnist-data-not-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/64962318/pytorch-classifier-for-mnist-data-not-work</guid>
      <pubDate>Mon, 23 Nov 2020 03:51:20 GMT</pubDate>
    </item>
    <item>
      <title>尝试拟合机器学习模型时出现错误 AttributeError: 'bool' 对象没有属性 'transpose'</title>
      <link>https://stackoverflow.com/questions/62520099/getting-error-attributeerror-bool-object-has-no-attribute-transpose-when-at</link>
      <description><![CDATA[我正在尝试创建一个机器学习模型来预测谁能在泰坦尼克号上幸存下来。每次我尝试拟合模型时，都会出现此错误：
 coordinates = np.where(mask.transpose())[::-1]
AttributeError: &#39;bool&#39; 对象没有属性 &#39;transpose&#39;

我正在运行的代码如下：

来自 xgboost 导入 XGBClassifier
来自 sklearn.preprocessing 导入 OneHotEncoder
来自 sklearn.compose 导入 ColumnTransformer
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.impute 导入 SimpleImputer
来自 sklearn.feature_selection 导入 SelectFromModel
来自 itertools 导入 combinations
导入 pandas 作为 pd
导入 numpy 作为 np

#读入数据
training_data = pd.read_csv(&#39;train.csv&#39;)
testing_data = pd.read_csv(&#39;test.csv&#39;)

#分离 X 和 Y
X_train_full = training_data.copy()
y = X_train_full.Survived
X_train_full.drop([&#39;Survived&#39;], axis=1, inplace=True)

y_test = testing_data

#获取所有 str 列
cat_columns1 = [cname for cname in X_train_full.columns if
X_train_full[cname].dtype == &quot;object&quot;]

interactions = pd.DataFrame(index= X_train_full)

#创建新特征
for combinations in combinations(cat_columns1,2):
imputer = SimpleImputer(strategy=&#39;constant&#39;)

new_col_name = &#39;_&#39;.join(combination)
col1 = X_train_full[combination[0]]
col2 = X_train_full[combination[1]]
col1 = np.array(col1).reshape(-1,1)
col2 = np.array(col2).reshape(-1,1)
col1 = imputer.fit_transform(col1)
col2 = imputer.fit_transform(col2)

new_vals = col1 + &#39;_&#39; + col2
OneHot = OneHotEncoder()

interactions[new_col_name] = OneHot.fit_transform(new_vals)

interactions = interactions.reset_index(drop = True)

#创建包含新功能的新数据框
new_df = X_train_full.join(interactions)

#对测试文件执行相同操作
interactions2 = pd.DataFrame(index= y_test)
for combinations in combinations(cat_columns1,2):
imputer = SimpleImputer(strategy=&#39;constant&#39;)

new_col_name = &#39;_&#39;.join(combination)
col1 = y_test[combination[0]]
col2 = y_test[combination[1]]
col1 = np.array(col1).reshape(-1,1)
col2 = np.array(col2).reshape(-1,1)
col1 = imputer.fit_transform(col1)
col2 = imputer.fit_transform(col2)

new_vals = col1 + &#39;_&#39; + col2

OneHot = OneHotEncoder()

interactions2[new_col_name] = OneHot.fit_transform(new_vals)

interactions2[new_col_name] = new_vals

interactions2 = interactions2.reset_index(drop = True)
y_test = y_test.join(interactions2)

#获取 cat 列的名称（添加了新功能）
cat_columns = [cname for cname in new_df.columns if
new_df[cname].dtype == &quot;object&quot;]

# 选择数值列
num_columns = [cname for cname in new_df.columns if 
new_df[cname].dtype in [&#39;int64&#39;, &#39;float64&#39;]]

#设置管道
numerical_transformer = SimpleImputer(strategy = &#39;constant&#39;)

categorical_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
(&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))
])

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, numeric_transformer, num_columns),
(&#39;cat&#39;, categorical_transformer, cat_columns)
])
model = XGBClassifier()

my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
(&#39;model&#39;, model)
])
#fit model
my_pipeline.fit(new_df,y)


我正在阅读的 csv 文件可从 Kaggle 的此链接获取：
https://www.kaggle.com/c/titanic/data
我无法弄清楚是什么导致了这个问题。任何帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/62520099/getting-error-attributeerror-bool-object-has-no-attribute-transpose-when-at</guid>
      <pubDate>Mon, 22 Jun 2020 17:24:31 GMT</pubDate>
    </item>
    <item>
      <title>SciKitLearn 中 MLPRegressor 的隐藏层大小是如何确定的？</title>
      <link>https://stackoverflow.com/questions/55786860/how-is-the-hidden-layer-size-determined-for-mlpregressor-in-scikitlearn</link>
      <description><![CDATA[假设我使用以下代码创建一个神经网络：
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(
hidden_​​layer_sizes=(100,),
activation=&#39;identity&#39;
)
model.fit(X_train, y_train)

对于 hidden_​​layer_sizes，我只是将其设置为默认值。但是，我真的不明白它是如何工作的。我的定义中隐藏层的数量是多少？是 100 吗？]]></description>
      <guid>https://stackoverflow.com/questions/55786860/how-is-the-hidden-layer-size-determined-for-mlpregressor-in-scikitlearn</guid>
      <pubDate>Sun, 21 Apr 2019 21:21:41 GMT</pubDate>
    </item>
    <item>
      <title>使用预训练模型开发深度学习图像识别系统</title>
      <link>https://stackoverflow.com/questions/49011306/developing-deep-learning-image-recognition-system-using-pre-trained-models</link>
      <description><![CDATA[我想在我的深度学习图像识别项目中使用预训练模型（例如 Xception、VGG16、ResNet50 等），以便在训练集上快速训练模型并实现高精度。我很难找到实现我的模型的确切代码。首先，根据 VGG16 模型的要求，我将训练数据的输入形状从 (256,256,3) 修改为 (224,224,3)。我使用了 keras 编程环境。我的模型代码如下
train_x = np.expand_dims(train_X, axis=2)
train_y = np.expand_dims(train_Y, axis=2)
print(train_X.shape) # 输出 - (670, 224, 224, 3)
print(train_Y.shape) # 输出 - (670, 224, 224, 1)
print(train_x.shape) # 输出 - (670, 224, 1, 224, 3)
print(train_y.shape) # 输出 - (670, 224, 1, 224, 1) 

def vgg16_(IMG_WIDTH=224,IMG_HEIGHT=224,IMG_CHANNELS=3):
inputs = Input(shape=(len(train_x[0]), 1))
x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, 名称=&#39;block1_conv1&#39;)(输入)
x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, 名称=&#39;block1_conv2&#39;)(x)
x = MaxPooling2D((2, 2), 步幅=(2, 2), 名称=&#39;block1_pool&#39;)(x)

# 块 2
x = Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, 名称=&#39;block2_conv1&#39;)(x)
x = Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, 名称=&#39;block2_conv2&#39;)(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block2_pool&#39;)(x)

# 块 3
x = Conv2D(256, (3, 3), 激活=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv1&#39;)(x)
x = Conv2D(256, (3, 3), 激活=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv2&#39;)(x)
x = Conv2D(256, (3, 3), 激活=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv3&#39;)(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block3_pool&#39;)(x)

# 块 4
x = Conv2D(512, (3, 3), 激活=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv1&#39;)(x)
x = Conv2D(512, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, name=&#39;block4_conv2&#39;)(x)
x = Conv2D(512, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, name=&#39;block4_conv3&#39;)(x)
x = MaxPooling2D((2, 2), 步幅=(2, 2), name=&#39;block4_pool&#39;)(x)

# 块 5
x = Conv2D(512, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, name=&#39;block5_conv1&#39;)(x)
x = Conv2D(512, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;, name=&#39;block5_conv2&#39;)(x)
x = Conv2D(512, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,name=&#39;block5_conv3&#39;)(x)
x = MaxPooling2D((2, 2),strides=(2, 2),name=&#39;block5_pool&#39;)(x)

x = Flatten()(x)
x = Dropout(0.2)(x)
x = Dense(100,activation=&#39;tanh&#39;)(x)
x = Reshape([len(train_x[0]),1])(x)
model = Model(inputs,reshape)
model.compile(loss=&#39;mse&#39;,optimizer=&#39;rmsprop&#39;)
return model

但是，不幸的是，我在训练数据上拟合这个模型时得到了这个错误
ValueError: 输入 0 与层 block1_conv1 不兼容：预期 ndim=4，发现ndim=3

我应该怎么做才能获得正确的输出？
此外，我尝试通过仅更改输出层来从以下代码运行。我收到此错误 ValueError：检查目标时出错：预期预测有 2 个维度，但得到的数组形状为 (670, 224, 224, 1)
model_vgg16_conv = VGG16(input_shape=(IMG_WIDTH,IMG_HEIGHT,3),weights=&#39;imagenet&#39;, include_top=False,pooling=max)
model_vgg16_conv.summary()
#print(&quot;ss&quot;)
#创建您自己的输入格式 
input = Input(shape=(IMG_WIDTH,IMG_HEIGHT,3),name = &#39;image_input&#39;)
#print(&quot;ss2&quot;)
#使用生成的模型 
output_vgg16_conv = model_vgg16_conv(input)
print(&quot;ss3&quot;)
#添加全连接层
x = Flatten(name=&#39;flatten&#39;)(output_vgg16_conv)
x = Dense(512,activation=&#39;relu&#39;,name=&#39;fc1&#39;)(x)
x = Dense(128,activation=&#39;relu&#39;,name=&#39;fc2&#39;)(x)
x = Dense(1,activation=&#39;sigmoid&#39;,name=&#39;predictions&#39;)(x)

#创建自己的模型
my_model = Model(input=input,output=x)

#在摘要中，VGG 部分的权重和层将被隐藏，但它们将在训练期间进行拟合
my_model.summary()

my_model.compile(loss=&#39;categorical_crossentropy&#39;,
optimizer=&#39;adam&#39;,
metrics=[&#39;accuracy&#39;])

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/49011306/developing-deep-learning-image-recognition-system-using-pre-trained-models</guid>
      <pubDate>Tue, 27 Feb 2018 14:42:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit learn 进行快速 ICA 重构误差分析</title>
      <link>https://stackoverflow.com/questions/45758280/fast-ica-using-scikit-learn-reconstruction-error-analysis</link>
      <description><![CDATA[我正在尝试在 scikitLearn 中使用 fastICA 程序。为了验证目的，我试图了解基于 PCA 和 ICA 的信号重建之间的区别。
观察到的信号的原始数量为 6，我尝试使用 3 个重建独立分量。问题是，无论我使用什么规范，ICA 和 PCA 都会导致相同的重建误差。有人能解释一下这里发生了什么吗？
代码如下：
 pca = PCA(n_components=3)
icamodel = FastICA(n_components=3,whiten=True)

Data = TrainingDataDict[YearSpan][RiskFactorNames]

PCR_Dict[YearSpan] = pd.DataFrame(pca.fit_transform(Data), 
columns=[&#39;PC1&#39;,&#39;PC2&#39;,&#39;PC3&#39;],index=Data.index)

ICR_Dict[YearSpan] = pd.DataFrame(icamodel.fit_transform(Data), 
columns=[&#39;IC1&#39;,&#39;IC2&#39;,&#39;IC3&#39;],index=Data.index)

&#39;------------------------IC 和 PC 的逆变换 -----------&#39;

PCA_New_Data_Df = pd.DataFrame(pca.inverse_transform(PCR_Dict[YearSpan]),
columns =[&#39;F1&#39;,&#39;F2&#39;,&#39;F3&#39;],index = Data.index)

ICA_New_Data_Df = pd.DataFrame(icamodel.inverse_transform(ICR_Dict[YearSpan]),
columns =[&#39;F1&#39;,&#39;F2&#39;,&#39;F3&#39;],index = Data.index)

下面是我测量重建误差的方法
&#39;-----------重建误差------------------&#39;
print &#39;PCA 重建误差 L2 范数：&#39;,np.sqrt((PCA_New_Data_Df - Data).apply(np.square).mean())

print &#39;ICA 重建误差 L2 范数：&#39;,np.sqrt((ICA_New_Data_Df - Data).apply(np.square).mean())

print &#39;PCA 重建误差 L1 范数：&#39;,(PCA_New_Data_Df - Data).apply(np.absolute).mean()

print &#39;ICA 重建误差 L1 范数：&#39;,(ICA_New_Data_Df - Data).apply(np.absolute).mean()

以下是 PC 和 IC 尾部的描述
PC 统计：(&#39;2003&#39;, &#39;2005&#39;) 
峰度 偏度
PCR_1 -0.001075 -0.101006
PCR_2 1.057140 0.316163
PCR_3 1.067471 0.047946 

IC 统计： (&#39;2003&#39;, &#39;2005&#39;) 
峰度 偏度
ICR_1 -0.221336 -0.204362
ICR_2 1.499278 0.433495
ICR_3 3.654237 0.072480 

以下是重建的结果
PCA 重建误差 L2 范数：
SPTR 0.000601
SPTRMDCP 0.001503
RU20INTR 0.000788
LBUSTRUU 0.002311
LF98TRUU 0.001811
NDDUEAFE 0.000135
dtype: float64 

ICA 重建误差 L2 范数：
SPTR 0.000601
SPTRMDCP 0.001503
RU20INTR 0.000788
LBUSTRUU 0.002311
LF98TRUU 0.001811
NDDUEAFE 0.000135

连L1规范都一样。我有点糊涂了！]]></description>
      <guid>https://stackoverflow.com/questions/45758280/fast-ica-using-scikit-learn-reconstruction-error-analysis</guid>
      <pubDate>Fri, 18 Aug 2017 13:48:17 GMT</pubDate>
    </item>
    </channel>
</rss>