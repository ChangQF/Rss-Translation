<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 24 Feb 2025 15:20:25 GMT</lastBuildDate>
    <item>
      <title>多标签分类任务的自动编码器</title>
      <link>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</guid>
      <pubDate>Mon, 24 Feb 2025 14:17:53 GMT</pubDate>
    </item>
    <item>
      <title>LLM是否在令牌集中包含很少使用的单词或字符？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</link>
      <description><![CDATA[我看到LLM几乎用所有语言给出答案，而且我很少看到使用英语词汇以及很少使用的汉字（我本人作为本地语说话的人甚至都不使用该角色）。
我的问题是：
当模型预测接下来的令牌时，它会计算概率分布。但这是多少个令牌的分布？
该概率分布的维度是多少？
如果它包含许多语言中的所有可能的单词或字符，则数组的长度太大。
如果他们使用相对较小的令牌集，那么这些稀有单词和汉字如何在答案中弹出？从这个意义上讲，考虑到许多语言的词汇和字符的数量，即使是令牌的大小也很小。。
他们用来解决此问题的技术方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</guid>
      <pubDate>Mon, 24 Feb 2025 10:45:19 GMT</pubDate>
    </item>
    <item>
      <title>候选算法算法实施无法正常工作</title>
      <link>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</link>
      <description><![CDATA[我正在尝试实现候选算法，以从数据集中学习概念。但是，我的代码没有产生预期的结果。以下是我的实现和我正在使用的数据集。
 我的代码： 
 将大熊猫作为pd导入
导入numpy作为NP
导入OS，系统

df = pd.read_csv（&#39;chumma.csv; quot;）

val_dict = {
    ＆quot” 0：[[一些，“许多”，“]，”，
    ＆quot“ 1＆quot”：[&#39;small&#39;，&#39;big&#39;，&#39;Medive&#39;]，
    &#39;2&#39;：[&#39;no&#39;]，
    &#39;3&#39;：[“负担得起”，“昂贵”]，
    &#39;4&#39;：[&#39;一个&#39;，&#39;许多&#39;，&#39;少数&#39;]
}

x = df.iloc [：，： -  1]。值
y = df.iloc [：， -  1]。值

s = [[&#39;$&#39;]*5]
对于我的范围（len（y））：
    如果y [i] ==&#39;是&#39;：
        s = x [i] .copy（）
        休息
    
g = [[[＆quot;？]*len（s）]

def is_consistent（x_data，y_data，g_hyp）：
    out_hyp = []
    对于g_hyp中的催眠：
        is_valid = true
        对于我的范围（len（x_data））：
            matches_hyp = true
            对于J范围（Len（Len）（Hyp））：
                如果用演[J]！=;？＆quot;和hyp [j]！= x_data [i] [j]：
                    matches_hyp = false
                    休息
            if（y_data [i] ==&#39;是&#39;而不是匹配的_hyp）或（y_data [i] ==&#39;no&#39;和matches_hyp）：
                is_valid = false
                休息
            
        如果IS_VALID和HYP不在out_hyp中：
            out_hyp.append（hyp）
            
    返回out_hyp
    
    
对于我，实例在枚举（x）中：
    如果y [i] ==&#39;是&#39;：
        对于J范围（LEN（S））的J：
            如果s [j]！=实例[J]：
                s [j] =;？？＆quot;
    别的：
        g_hyp = []
        对于G中的G：
            对于J范围（Len（g））的J：
                如果g [j] ==;
                    d_vals = val_dict [str（j）]
                    对于范围内的k（0，len（d_vals））：
                        如果d_va​​ls [k]！=实例[J]：
                            new_g = g.copy（）
                            new_g [j] = d_vals [k]
                            打印（new_g）
                            g_hyp.append（new_g）
            打印（）
            休息
        g = is_consistent（x [：i+1]，y [：i+1]，g_hyp）
        

打印（“最终特定假设：”，S）
打印（“最终的一般假设：”，G）
 
 数据集（chumma.csv）： 
 引用，尺寸，无盲，价格，版本，购买
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
 
 预期输出
该算法应输出特定的假设和一般假设（G），该假设代表从数据集中学到的概念
 问题
该代码似乎无法正确更新特定的假设（S）和一般假设（G）。具体：

 s尚未正确地为积极的例子正确概括。
 g无法正确专门用于负面示例。

我实施候选算法是什么问题，我该如何修复代码以正确更新S和G？]]></description>
      <guid>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</guid>
      <pubDate>Mon, 24 Feb 2025 10:26:17 GMT</pubDate>
    </item>
    <item>
      <title>语言模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</link>
      <description><![CDATA[ valueerror：太多值无法打开包装（预期2）
追溯：
file＆quot＆quot c：\ user \ chawk \ pycharmprojects \ pfe.venv \ lib \ lib \ site-packages \ runlit \ runtime \ runtime \ scriptrunner \ exec_code.code.code.py.py＆quote＆quort＆quote＆quort＆quort of exec_func_with_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_handling in
结果= func（）]]></description>
      <guid>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</guid>
      <pubDate>Mon, 24 Feb 2025 10:24:56 GMT</pubDate>
    </item>
    <item>
      <title>它只是说这个问题，请帮助如何克服这个[关闭]</title>
      <link>https://stackoverflow.com/questions/79462841/it-just-says-this-problem-please-help-how-to-overcome-this</link>
      <description><![CDATA[ 在此处输入图像描述为什么它不说属性svc？我在这里做什么？我总是遇到这些类型的问题。
请帮助我我绝望。预先感谢您
我尝试了其他解决方案，但没有任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/79462841/it-just-says-this-problem-please-help-how-to-overcome-this</guid>
      <pubDate>Mon, 24 Feb 2025 08:23:04 GMT</pubDate>
    </item>
    <item>
      <title>grpotrainer不支持iterabledataset</title>
      <link>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</link>
      <description><![CDATA[执行时以下程序崩溃
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
def data_generator（）：
    而真：
        在提示中s：
            产生{提示; ：S}
dataset = iterabledataset.from_generator（data_generator）


triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
导致以下迹线：
  trackback（最近的最新通话）：
  file＆quot＆quort＆quot＆quode/code/code/cs234/starter_code/trl_testing.py&quot;，第24行，in＆lt; module＆gt;
    Trainer.Train（）
  file＆quot＆quort＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot;，第2241号线
    return innion_training_loop（
  file＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&quot; line 2500，in _inner_training_training_loop in
    batch_samples，num_items_in_batch = self.get_batch_samples（epoch_iterator，num_batches）
  file＆quot＆quot＆quort＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot; line 5180，在get_batch_samples中
    batch_samples += [next（epoch_iterator）]
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    next_batch，next_batch_info = self._fetch_batches（main_iterator）
  file＆quot＆quort＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/accelerate/data_loader.py＆quot＆quot; line 812，in _fetch_batches
    批处理=连接（批次，dim = 0）
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot congatenate in Compatenate in Compatenate
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（OBJ）（生成器）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    提高typeerror（f＆quot“只能连接张量，但得到{type（data [0]）};）
typeError：只能连接张量，但可以得到＆lt; class&#39;Str&#39;＆gt;
 
但是，用类似的 dataset 替换 iterabledataset 解决了问题：
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
dataset = dataset.from_dict（{提示＆quot;：提示}）

triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
这已经在2个截然不同的系统上复制了，因此这不太可能是原因。
我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</guid>
      <pubDate>Mon, 24 Feb 2025 05:08:12 GMT</pubDate>
    </item>
    <item>
      <title>LSTM培训是否在恢复学习后重置？</title>
      <link>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</link>
      <description><![CDATA[我有一个称为processed_data的数据库，其中包含像这样构成的单元格：
  0.980999999999767 0.945912306864893 1
1.46300000000338 0.926617136227153 1
0.51199999995169 0.868790509137634 2
1.00600000000122 0.978074194186882 1
0.995999999999185 0.884817478795566 2
1.12400000000343 0.740093883803231 2
1.3539999999936 0.418494628137842 2
0.65399999994994 0.399199457500103 2
1.00600000000122 0.438938088213894 2
0.99000000003434 0.566427539286267 2
 
第一列表示自上一行以来已通过的秒数，第二列是在给定时间（归一化）的值，第三列包含分类值1和2。前两个列是预测指标，第三列是目标。
每个细胞代表一天，我有大约215天的数据，每个数据都有不同的观测值。在培训期间，我每天都会停止学习时，当达到给定的一天的最后一批时，请停止学习，然后在第二天加载数据并恢复培训。
问题在于，当训练加载第二天的数据后训练恢复时，好像网络已经完全重置并开始从头开始学习。它反复产生相同的精度结果（不包括第一次迭代），只有损失值的略有变化。其余迭代的准确性保持不变，总是产生相同的值，就好像网络根本没有学习。这是第1天输出的示例：
  1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.69781 40.625
           50 1 00:00:00 0.001 0.65881 64.844
          100 1 00:00:00 0.001 0.70176 50.781
          117 1 00:00:00 0.001 0.63057 69.531
训练停止：Max Epochs完成
...
...
...
1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.70017 41.406
           50 1 00:00:00 0.001 0.65913 64.844
          100 1 00:00:00 0.001 0.6985 50.781
          117 1 00:00:00 0.001 0.62994 69.531
训练停止：Max Epochs完成
...
...
...
1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.69753 42.188
           50 1 00:00:00 0.001 0.6619 64.844
          100 1 00:00:00 0.001 0.70356 50.781
          117 1 00:00:00 0.001 0.6291 69.531
训练停止：Max Epochs完成
...
...
...
 
这是我的代码段：
  %%定义培训选项 
train_opts =训练（...
    “亚当”，...
    初始learternrate = 0.001，...
    minibatchsize = 128，...
    图=; none＆quot; ...
    详细= true，...
    maxepochs = 1，...
    Shuffle =“从不
    指标=“准确性” ...
    ）；

%%定义网络。
net = dlnetwork;
temp_net = [
    sequenceInputlayer（2，“名称”，“输入”）
    lstmlayer（256，“ name”&#39;lstm; quot&#39;ouppotemode“
    dropoutlayer（0.5，“名称”，“ dropfout”）
    完整连接的layerer（2，“名称”，“输出”）
    SoftMaxlayer];
net = addlayers（net，temp_net）;
net =初始化（net）;

％清理助手变量
清除temp_net;

%%每天加载数据并训练网络。
num_of_epochs = 30;
train_data_length = round（长度（processed_data） * 0.9）;
train_data = processed_data（1：train_data_length）;

for epoch = 1：num_of_epochs
    一天= 1：train_data_length
        如果train_opts.verbose
            disp（Day +&#39;Day&#39;Day&#39;）
        结尾
        train_x = processed_data {day}（：，1：2）;
        train_x = dlarray（train_x，“ bct”）;
        train_y =分类（processed_data {day}（：，3））;
    
        net = trainnet（train_x，train_y，net，&#39;crossentropy＆quot; train_opts）;
    结尾
结尾
 
目标是创建一种基于预测因子的LSTM算法，可以预测该值将来是否会增加（2）或减少（1）。]]></description>
      <guid>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</guid>
      <pubDate>Sun, 23 Feb 2025 21:11:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Docling库从DOCX文件中提取页面上的HTML内容，以检测页面断路？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</link>
      <description><![CDATA[我使用Docling和PYPDF2成功实现了PDF文件的页面html提取。这是我当前代码对PDF的作用：

使用PYPDF2将PDF分为单个页面
使用Docling的DocumentConverter 将每个页面转换为HTML
用嵌入式图像提取HTML内容
添加元数据（页码，文档ID，文件名）
将所有内容保存到JSON结构

重要说明：我首先将pdf分为单个页面的原因是因为docling的save_as_html（）和export_to_html（）函数在完整的文档对象（而不是在单个页面上）起作用。要获取页面html内容，我需要创建临时的单页PDF并分别转换一个。
这是我每页获得的示例JSON结构：
{
＆quot“ page”：“第1页”
＆quot“ content＆quot”：“”
“元数据：{{
＆quot“ docutsId”：; quot“ uuid; quot”
“文件名”：“ document.pdf”
&#39;page_number＆quot”：1，
＆quot“ total_pages”：总计
}
} 
现在，我需要为DOCX文件实现相同的功能。对我来说，DOCX文件包含诸如标题，页脚和页面断开之类的元素，我们可以使用这些页面断开将内容分为页面。。
我正在使用Docling库进行转换（DOCX到HTML），但是我无法识别或检测到DOCX文件中的页面中断。由于Docling的HTML转换在完整的文档上起作用，因此我需要根据页面断路首先将DOCX内容拆分，类似于我处理PDF的方式。
问题：

如何在DOCX文件中检测到页面中断？
是否有一种方法可以根据这些页面中断进行拆分DOCX内容，以便我可以从DOCX中提取内容页面？

我正在使用的相关库：

文档
 python-docx（如果需要）
]]></description>
      <guid>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</guid>
      <pubDate>Sun, 23 Feb 2025 15:19:41 GMT</pubDate>
    </item>
    <item>
      <title>'microsoft.ml.transforms.timeseries.fftutils的类型初始化器抛出了例外。在ml.net中</title>
      <link>https://stackoverflow.com/questions/79457387/the-type-initializer-for-microsoft-ml-transforms-timeseries-fftutils-threw-an</link>
      <description><![CDATA[  int预测= 144;
 var datapoints = 480;

列表＆lt; pcidatapoint＆gt; pcidata = dataPoints.Select（x =＆gt; new PcidApoint {value =（float）x}）。tolist（）;
            idataview dataview = mlcontext.data.data.loadfromenumerable（pcidata）;
            
        //定义SSA预测模型
        var pipeline = mlcontext.forecasting.forecastbyssa（
            outputcolumnname：＆quot“ pciforecastedvalues＆quot＆quot＆quot＆quot of tougply starlically设置输出列
            InputColumnName：nameof（pcidatapoint.value），//正确输入列
            Windowsize：预测，// BeackBack窗口大小
            系列节目：datapoints.count，//整个系列长度
            trainsize：datapoints.count，//培训数据大小
            地平线：预测，//预测数量
            Concidencelevel：0.95F，//置信度
            ConcidencelowerBoundColumn：&#39;ConfidencelowerBound＆quot
            CressupupperboundColumn：“ CressialUperpBound”
        ）；

        //训练模型
        var model = pipeline.fit（dataview）;
 
获得错误的格式错误是错误的，在此我的代码中，我的系列长度是480，我的horizo​​n leth是144是正确的方法来提供获取前进值的详细信息]]></description>
      <guid>https://stackoverflow.com/questions/79457387/the-type-initializer-for-microsoft-ml-transforms-timeseries-fftutils-threw-an</guid>
      <pubDate>Fri, 21 Feb 2025 12:39:40 GMT</pubDate>
    </item>
    <item>
      <title>如何从PDF中提取表并将其转换为结构化的HTML（<table>，<tr>，<td>），同时保持原始布局和格式化？</title>
      <link>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</link>
      <description><![CDATA[  1] doc1的原始页面包含4个表 
  1] output .html doc1 of doc1 of doc1 of doc1 of doc1 of doc1 of doc1 of docc1 of doc1 notection .html doct of dectuts of dectuts of dectuts of dectuts dectuts dotection dotect of table，有时从表中提取文本 
  2] doc2的原始页面在表中包含图像     
  2]输出.html doc2的of Doc2，复杂的表具有带有嵌入式图像的复杂表格，无法正确处理＆amp;有时，对于简单表而言，在.html文件中也无法获得适当图像描述在这里“ src =” https://i.sstatic.net/feqdv0vo.jpg“ /&gt;  
我正在从PDF文件中提取内容，并将其转换为HTML格式，同时维护原始结构和格式。我正在为此目的使用文档库。
我正在获取与 .html 文件中原始PDF文件相同的内容流的输出。但是，我在保留输出html文件中保存表结构时面临问题。
 我期望发生的事情： 

 从PDF中提取正确的行和列结构。

 将表布局保留在＆lt; table; ，＆lt; tr＆gt; 和＆lt; td＆gt;  html标签中。 

 保持原始格式，对齐和单元格内容，如PDF所示。


 实际发生的事情： 

 表未正确检测到表 - 表显示在＆lt; p＆gt; 标签中，而不是正确的＆lt; table&gt;＆gt; 结构。

 未对准的表格 - 细胞内部的图像出现在表面 表。。

 带有嵌入式图像的复杂表无法正确保留。


 使用的代码： 
 来自docling.document_converter import docuctonverter，pdfformatoption
从docling.datamodel.pipeline_options导入pdfpipelineOptions
来自docling.datamodel.base_models导入inputformat
来自docling_core.types.doc导入imagerefmode
从pathlib导入路径
导入记录

＃设置记录
logging.basicconfig（level = logging.info）
log = logging.getLogger（__名称__）＃更正：_name_--＆gt; __姓名__

＃配置图像设置
image_resolution_scale = 2.0

＃通往PDF文件的路径
source = path（r＆quot; c：\ users \ downloads \ journal.pdf;）
output_path =路径（r＆quot; c：\ users \ desktop \ output20.html＆quort;）

＃配置图像处理的管道选项
pipeline_options = pdfpipelineOptions（）
pipeline_options.images_scale = image_resolution_scale
pipeline_options.generate_page_images = true
pipeline_options.generate_picture_images = true

＃创建带有图像选项的转换器实例
转换器= documentConverter（
    format_options = {
        inputformat.pdf：pdfformatoption（pipeline_options = pipeline_options）
    }
）

＃将PDF转换为文档
结果= converter.convert（源）

＃用嵌入式图像保存HTML
result.document.save_as_html（output_path，image_mode = imagerefmode.embedded）

log.Info（f＆quot&#39;html文件，创建的嵌入式图像：{output_path}＆quot;）
 
 我到目前为止尝试过的是： 

 检查了提取的HTML输出 - 缺少或错误地显示。

 在 pdfpipelineoptions（）中尝试了不同的管道选项以查看它们是否影响表提取。

 将输出与文档智能库进行了比较 - 它可以更好地提取标题/页脚，但仍在使用复杂的表格。


 关键挑战： 

 表不保留在＆lt; table; 标签中。

 细胞分裂问题 - 数据被错位或分为多个部分。

 表中的图像放错了位置（出现在上方/下方而不是内部表单元格）。


 其他观察： 

提取的HTML文件中的内容流与原始PDF文件**匹配，但是**表结构未正确格式。

 问题： 
我如何才能从PDF正确提取表并将其转换为结构化的HTML（＆lt; table; gt; ，＆lt; tr＆gt; ，＆lt; td＆gt＆gt; ）在维护原始布局和使用文档库的格式化时？]]></description>
      <guid>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</guid>
      <pubDate>Tue, 11 Feb 2025 13:21:50 GMT</pubDate>
    </item>
    <item>
      <title>我如何成功设置和检索元数据信息以在Huggingface Hub上的HuggingFacedatAset？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，我是从诸如此类的字典中创建的：
  info = datasetinfo（
        Description =&#39;我的快乐LIL数据集
        版本=; 0.0.1＆quot;
        homepage =＆quot; https：//www.myhomepage.co.uk＆quot;
    ）
train_dataset = dataset.from_dict（prepary_data（data [＆quot; train;]），info = info）
test_dataset = dataset.from_dict（prepary_data（数据[test; test;]），info = info）
验证_DATASET = DATASET.FROM_DICT（prepary_data（data [data [＆quot; quartation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quote = info = info）
 
 i然后将它们集合到数据集中。
 ＃创建一个datasetDict
dataset = datasetDict（
    {train＆quort＆quot; train_dataset，&#39;test;：test_dataset，;
）
 
到目前为止，一切都很好。如果我访问 dataset [&#39;train&#39;]。info.description 我看到“我的快乐lil dataset”的预期结果。
所以我推到轮毂上，就像：
  dataset.push_to_hub（f＆quot {agrompome}/{repo_name}＆quits＆quits_message =＆quort; some some commin
 
这也成功了。
但是，当我来将数据集从集线器中拉回并访问与之关联的信息时，而不是获取数据集的描述时，我只会得到一个空字符串；喜欢：
  pulled_data = full = load_dataset（＆quot; f {agrommy}/{repo_name}＆quort＆quort; use_auth_token = true）

＃我希望以下内容打印出来“我的快乐LIL数据集”。
print（pulled_data [&#39;train;]。info.Description）
＃但是，它返回&#39;&#39;
 
我是否错误地从集线器加载数据？我是只推出数据集而不是以某种方式推出信息吗？
我觉得我缺少一些明显的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>如何在NLTK中下载Punkt Tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用安装了NLTK库
  PIP安装NLTK
 
使用lib 
 来自nltk.tokenize导入send_tokenize 
send_tokenize（文本）
 
我遇到此错误
  lookuperror： 
****************************************************** ********************
  找不到资源朋克。
  请使用NLTK下载器获取资源：

  ＆gt;＆gt;＆gt;导入NLTK
  ＆gt;＆gt;＆gt; nltk.download（&#39;punkt&#39;）
  
  有关更多信息，请参见：https：//www.nltk.org/data.html

  尝试加载dokenizers/punkt/English.pickle

  在：
     - &#39;c：\\用户\\ adars/nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ share \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ lib lib \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\漫游\\ nltk_data&#39;
     - &#39;c：\\ nltk_data&#39;
     - &#39;d：\\ nltk_data&#39;
     - &#39;e：\\ nltk_data&#39;
     - &#39;&#39;&#39;
 
因此，为了解决此错误，我尝试了
 导入NLTK
nltk.download（&#39;punkt&#39;）
 
但是我无法下载此软件包，因为每次运行时，我都会收到错误的错误
  [nltk_data]错误加载punkt：＆lt; urlopen错误[WinError 10060] a
[nltk_data]连接尝试失败，因为连接的聚会
[nltk_data]一段时间后没有正确响应，或者
[nltk_data]建立的连接失败，因为连接的主机
[nltk_data]未能响应＆gt;
 
请在这里帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>在微调过程中，如何正确设置垫子令牌（不是EOS），以避免模型不预测EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在虚线文本验证码中找到轮廓图像</title>
      <link>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</link>
      <description><![CDATA[我是OpenCV的新手。我正在尝试找到验证码图像的轮廓。仅当我的验证码包含虚线文本时，它不起作用。
我已经完成了以下代码：
 导入numpy作为NP
导入CV2作为CV
导入imgaug.augmenters为IAA

im = cv.imread（&#39;dataset/1.jpg&#39;）
imgray = cv.cvtcolor（im，cv.color_bgr2gray）

imgray = cv.threshold（Imgray，127，255，0）[1]

dst = cv.canny（imgray，0,150）
bluret = cv.blur（dst，（5,5），0）
img_thresh = cv.AdaptivEthreshold（Blud，255，cv.Adaptive_thresh_gaussian_c，cv.thresh_binary_inv，11，2）

内核= cv.getStructuringElement（cv.morph_rect，（3,3））
阈值= cv.morphologyex（img_thresh，cv.morph_close，kernel）

轮廓，层次结构= cv.findcontours（dst，cv.retr_tree，cv.chain_approx_simple）
打印（Len（Contours））
＃cv.drawContours（IM，轮廓，-1，（0，255，0），3）

cv.imshow（“ img_thresh”，img_thresh）
cv.imshow（dst&#39;dst）
cv.imshow（“阈值”，阈值）
CV.Waitkey（0）
cv.destroyallwindows（）
 
有人可以帮忙吗？有什么方法可以在此图像中找到轮廓？
  ]]></description>
      <guid>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</guid>
      <pubDate>Fri, 25 Feb 2022 06:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何找到稀疏矢量的最近邻居</title>
      <link>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</link>
      <description><![CDATA[我有大约500个向量，每个向量是1500维矢量，
几乎每个向量都很稀疏 - 我的意思是，矢量的30-70维度不是0。
现在，问题在于，这里是一个给定的向量，也是1500个维度，我需要将其与500个向量进行比较，以查找500个最接近的矢量。（在Euclidean距离中）。。
毫无疑问，蛮力方法是一种解决方案，但是我需要计算500次的距离，这需要很长时间。
昨天，我读了一篇文章“用大词汇和快速的空间匹配”的文章，它说使用倒置索引会有所帮助，它说：
   
但是，在我的测试之后，几乎没有任何意义，想象一个1500矢量，其中50个尺寸并不为零，当涉及另一个尺寸时，它们可能总是具有相同的尺寸，而不是零。换句话说，这种算法只能排除一个小矢量，我仍然需要与剩下的许多向量进行比较。
我的问题：

 此算法是否有意义？

 还有其他方法可以做我想做的事吗？例如Flann或KD-Tree？
但是我想要精确的准确的最近邻居，大约是一个不够的

]]></description>
      <guid>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</guid>
      <pubDate>Tue, 05 Jan 2016 12:07:01 GMT</pubDate>
    </item>
    </channel>
</rss>