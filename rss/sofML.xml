<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 12 Feb 2024 18:16:24 GMT</lastBuildDate>
    <item>
      <title>OOM 问题和图执行问题</title>
      <link>https://stackoverflow.com/questions/77982891/oom-issue-and-graph-execution-issue</link>
      <description><![CDATA[当我使用胸部 X 射线数据集为 120000 的 capsnet 模型时，所有数据集的大小都调整为 256,256。
我收到此错误：
通过分配器 GPU_0_bfc 在 /job:localhost/replica:0/task:0/device:GPU:0 上分配形状为 [524288,10000] 的张量并键入 float 时出现 OOM
     [[{{节点模型/capsule_layer/MatMul}}]]
提示：如果您想在 OOM 发生时查看分配的张量列表，请将 report_tensor_allocations_upon_oom 添加到 RunOptions 以获取当前分配信息。在 Eager 模式下运行时，此功能不可用。
 [操作：__inference_train_function_853]

不知道如何继续前进。
模型未进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/77982891/oom-issue-and-graph-execution-issue</guid>
      <pubDate>Mon, 12 Feb 2024 16:29:36 GMT</pubDate>
    </item>
    <item>
      <title>DenseFeatures 层不需要字典或可迭代对象</title>
      <link>https://stackoverflow.com/questions/77982790/densefeatures-layer-dont-want-a-dictionary-or-an-iterable-either</link>
      <description><![CDATA[我想基于 tfrecord 文件（~200GB）训练 keras 模型，但似乎开放（功能）层不希望我这样做。我使用 tf.keras.layers.DenseFeatures(feature_columns=dense_columns) 添加功能层
数组（可迭代）版本
&lt;前&gt;&lt;代码&gt;dense_columns = []
对于功能，feature_info.items() 中的 data_type：
    ensemble_columns.append(tf.feature_column.numeric_column(feature))

interable-error - 在 model.fit(...) 方法中引发异常。显示第一个纪元，但进程在几秒钟后中止。他想要一本字典：
 ValueError：调用图层“dense_features”（类型 DenseFeatures）时遇到异常。
    
    我们期望这里有一本字典。相反，我们得到：
    
    调用层“dense_features”接收的参数（类型 DenseFeatures）：
      • features=tf.Tensor(shape=(None,), dtype=string)
      • cols_to_output_tensors=无
      • 训练=真

词典版本
dense_columns = {}
对于功能，feature_info.items() 中的 data_type：
    稠密列[特征] = tf.feature_column.numeric_column(特征)


字典错误 - 它在tf.keras.layers.DenseFeatures(feature_columns=dense_columns)处引发
ValueError：预期 feature_columns 是可迭代的，找到了 dict。

我做错了什么？
我尝试使用字典、数组，使用了网上的几个示例，但我无法继续进行。]]></description>
      <guid>https://stackoverflow.com/questions/77982790/densefeatures-layer-dont-want-a-dictionary-or-an-iterable-either</guid>
      <pubDate>Mon, 12 Feb 2024 16:12:31 GMT</pubDate>
    </item>
    <item>
      <title>尝试对象检测时出现 opencv 错误</title>
      <link>https://stackoverflow.com/questions/77982625/getting-opencv-errors-while-trying-object-detection</link>
      <description><![CDATA[这是我的代码：
&lt;前&gt;&lt;代码&gt;导入cv2
将 cvlib 导入为 cv

从 cvlib.object_detection 导入draw_bbox
将张量流导入为 tf
打印（tf.__版本__）
#从 gtts 导入 gTTS
#from Playsound 导入 Playsound

视频 = cv2.VideoCapture(0)

# 检查网络摄像头是否打开成功
如果不是 video.isOpened():
    print(“错误：无法打开网络摄像头。”)
    出口（）
别的：
    print(&quot;检测到相机&quot;)
而真实：
    ret, 帧 = video.read()
    打印（框架）
    bbox、标签、conf = cv.detect_common_objects(frame)
    输出图像=绘制_bbox（框架，bbox，标签，conf）

    cv2.imshow(“目标检测”,output_image)

    如果 cv2.waitKey(1) &amp; 0xFF == ord (“q”)：
        休息

我收到以下错误：cv2.error: OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\dnn\src\darknet\darknet_io.cpp： 705：错误：（-215：断言失败）separator_index &lt;函数“cv::dnn::darknet::ReadDarknetFromCfgStream”中的 line.size()
我尝试导航到错误中指定的文件，但我的计算机上没有 D 驱动器，因此我不知道如何找到它。我尝试重新安装 OpenCV 但没有帮助。我在网上看到，导航到该文件并直接编辑它可以解决人们的问题，但我首先如何导航到它。]]></description>
      <guid>https://stackoverflow.com/questions/77982625/getting-opencv-errors-while-trying-object-detection</guid>
      <pubDate>Mon, 12 Feb 2024 15:44:27 GMT</pubDate>
    </item>
    <item>
      <title>二元预测的预测结果是否定的</title>
      <link>https://stackoverflow.com/questions/77982419/the-predictions-of-a-binary-prediction-are-negative</link>
      <description><![CDATA[我正在努力创建一个二进制模型。我以为一切正常，但当我发现模型关闭的频率很奇怪时，但当我尝试调整阈值时，我发现没有任何变化，所以就在那时我开始调查。
我检查了二元分类的预测值，发现其中大多数都是负值。
这是我的模型：
 public ITransformer TrainCategorialModel(IEnumerabletrainingData)
    {

        var 列名称 = typeof(TrainingCategorial)
            .GetProperties()
            .Where(property =&gt; property.DeclaringType != typeof(TrainingCategorial))
            .Select(属性 =&gt; 属性.名称)
            .ToArray();

        // 检查训练数据中的空值
        
        if (trainingData.Any(item =&gt; item == null))
        {
            throw new ArgumentException(“训练数据包含空值。”);
        }

        var pipeline = mLContext.Transforms.Concatenate(“特征”, columnNames)
            .Append(mLContext.BinaryClassification.Trainers.SdcaNonCalibrate(labelColumnName: &quot;CHPlabels&quot;, featureColumnName: &quot;Features&quot;));

        var data = mLContext.Data.LoadFromEnumerable(trainingData);

        var model = pipeline.Fit(data);

        返回模型；
    }

我的特征基于另一个类中的参数模型。
我的预测如下：
 公共列表; PredictCategorialModel（ITransformer 模型，IEnumerable 输入）
    {
        // 4. 转换数据
        IDataView 测试数据 = mLContext.Data.LoadFromEnumerable(input);

        // 5. 根据特征预测新值。
        列表&lt;浮动&gt; PredictedValues = mLContext.Data.CreateEnumerable(
            model.Transform(testingData),reuseRowObject: false)
            .Select(row =&gt; row.LabelPrediction)
            .ToList();

        // 应用阈值（例如 0.5）将分数转换为布尔预测
        变量阈值 = 0.3；
            列表&lt;布尔&gt; PredictedLabels = PredictedValues.Select(LabelPrediction =&gt; LabelPrediction &gt; 阈值).ToList();

        返回预测标签；
    }

我检查了我的数据，看起来没问题，所以我真的不知道如何解决这个问题。如果您有任何提示或建议，请分享。谢谢！
更新：我认为问题出在模型中，我尝试了其他方法来创建预测布尔值，但遇到了相同的错误。我尝试过 LightGBM，因为我知道该模型不应该是线性的，但这产生了全新的问题（我对此也有一个未解答的问题）。有谁知道检查模型是否有效的好方法？]]></description>
      <guid>https://stackoverflow.com/questions/77982419/the-predictions-of-a-binary-prediction-are-negative</guid>
      <pubDate>Mon, 12 Feb 2024 15:09:31 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks MLFlow 中部署 ML 模型</title>
      <link>https://stackoverflow.com/questions/77982112/deploy-ml-model-in-databricks-mlflow</link>
      <description><![CDATA[我有一个在开发环境中训练的 ML 模型，现在我想将其部署在具有不同 URL 的 PROD databricks 中。我可以下载开发模型，然后手动将 pkl 文件上传到 PROD DBFS。下载模型后，如何在产品 MLFlow 中部署。
要下载我使用以下代码的模型
导入操作系统
导入流量
从 mlflow.store.artifact.models_artifact_repo 导入 ModelsArtifactRepository
mlflow.set_tracking_uri(“databricks”)
model_name = “测试模型”
model_stage = “无”
os.makedirs(“model2”,exist_ok=True)
local_path = ModelsArtifactRepository(f&#39;models:/{model_name}/{model_stage}&#39;).download_artifacts(“”, dst_path=&#39;model2&#39;)
print(f&#39;{model_stage} 模型 {model_name} 已在 {local_path} 下载&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77982112/deploy-ml-model-in-databricks-mlflow</guid>
      <pubDate>Mon, 12 Feb 2024 14:21:56 GMT</pubDate>
    </item>
    <item>
      <title>如何包装 keras 模型以供 scikit-learn 堆叠集成使用</title>
      <link>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</link>
      <description><![CDATA[我是机器学习领域的新手。我有一个已经训练过的 keras 模型列表。我想在 scikit learn 中将它们与 StackingClassifier 一起使用。由于 keras 没有 Predict_proba 方法，我创建了一个包装器。
如果使用我的为 VotingClassifier 包装的模型以及软方法和硬方法，它就可以工作。
但是当我使用堆叠模型时，第一次运行后，它会向我显示此错误。我没有找到任何相关信息。
感谢您的帮助
&lt;小时/&gt;
类 KerasWrapperWithEncoder(BaseEstimator, ClassifierMixin):
    def __init__(自身，keras_model，classes_)：
        self.keras_model = keras_model
        self.encoder = OneHotEncoder(sparse_output=False)
        # L&#39;encoder OneHotEncoder 已经过去了
        self.classes_ = classes_ # 定义可分配类

    def fit(自身, X, y):
        # 模型已安装，不再适合
        y_reshape = y.reshape(-1, 1)
        self.encoder.fit(y_reshape)
        返回自我

    def 预测（自身，X）：
        # 利用 les modèles entraînés pour faire des predictions
        预测 = self.keras_model.predict(X)
        np_argmax = np.argmax(预测，轴=1)
        打印（预测）
        打印（np_argmax）
        返回 np_argmax

    def Predict_proba(自身, X):
        # 返回分类模型的类别概率
        概率 = self.keras_model.predict(X)
        print(&quot;概率形状：&quot;, probabilities.shape) # 调试
        返回概率


keras_wrapped_models_with_encoder = [
    (name.replace(&#39; &#39;, &#39;_&#39;).replace(&#39;__&#39;, &#39;_&#39;), KerasWrapperWithEncoder(model, _target_classes_))
    对于 keras_models.items() 中的名称、模型
]

vote_clf = 投票分类器(
         估计器=all_估计器，
         投票=&#39;软&#39;，
         n_职位=3，
         详细=真）
vote_clf .fit(X_train, y_train) # 完美运行

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
keras_stacking_models_current_year = StackingClassifier(
    估计器=all_估计器，
    Final_estimator=LogisticRegression(),
    简历=简历，
    详细=3，
    # n_jobs=2
）


keras_stacking_models_current_year.fit(X_train, y_train) # 抛出错误


&lt;前&gt;&lt;代码&gt;========================================stacking_models_all_models===== =======================
12105/12105 [================================] - 25s 2ms/步
概率形状：(387348, 3)
训练折叠 (1) 中的类数与类总数 (3) 不匹配。结果可能不适合您的用例。要解决此问题，请使用交叉验证技术来产生正确分层的折叠
_enforce_prediction_order（类、预测、n_classes、方法）
   第1457章
   第1458章）
-&gt;第1459章
   第1460章 1460
   第1461章 回归预测

ValueError：形状不匹配：形状（387348,3）的值数组无法广播到形状（387348,1,3）的索引结果

我有一个已经训练过的 keras 模型列表。我想在 scikit learn 中将它们与 StackingClassifier 一起使用。由于 keras 没有 Predict_proba 方法，我创建了一个包装器。
如果使用我的为 VotingClassifier 包装的模型以及软方法和硬方法，它就可以工作。
但是当我使用堆叠模型时，第一次运行后，它会向我显示此错误。我没有找到任何相关信息。
感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</guid>
      <pubDate>Mon, 12 Feb 2024 14:12:27 GMT</pubDate>
    </item>
    <item>
      <title>torch.Sequential 中尺寸不兼容 [关闭]</title>
      <link>https://stackoverflow.com/questions/77981094/incompatibility-of-dimensions-in-torch-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77981094/incompatibility-of-dimensions-in-torch-sequential</guid>
      <pubDate>Mon, 12 Feb 2024 11:24:32 GMT</pubDate>
    </item>
    <item>
      <title>用大量数据训练tenserflow js模型</title>
      <link>https://stackoverflow.com/questions/77980320/training-the-tenserflow-js-model-with-a-large-amount-of-data</link>
      <description><![CDATA[大家好，感谢您花时间回答我的问题。
我正在使用node js、docker、redis bull、postgresql、tenserflow js编写一个项目来训练AI模型。我的服务器有12 个 CPU 核心和 16GB RAM。
执行以下步骤后出现问题。

我创建了一个包含 10080 元素的二维数据数组作为训练输入数据，并创建了一个包含 5040 元素的数组作为输出数据。所有数据都是数字
使用 redis bull，我创建了从数据库检索数据的队列，准备训练，并使用 .fit() 同步训练模型。
我使用队列在 CPU 核心之间均匀划分学习过程。

问题：

由于某种原因，系统仅使用一个CPU核心，并且正如我从监控中看到的，训练包太大并且堵塞了该CPU核心的100%。

也许我应该使用 .dataset() 和工作线程？我需要用 10080 个输入数据包训练 madel。当整个数据库的大小约为 5TB 数据时，使用一个核心，我将使用所有这些数据训练模型一年半(((
我不明白如何将训练分配给所有可能的 CPU 核心。 我会回答任何问题，并感谢任何建议
有一些我的代码：
 trainQueue.process(1, (有效负载, 完成) =&gt; {
      const train = async()=&gt;{
      尝试 {
        // console.log(payload.data);
        const test =等待trainTask（有效负载，完成）；
        如果（测试）{
          console.log(“完成！”);

        } 别的 {
          抛出新错误（“Не тут то было！”）

        }
        完毕（）;
      } 捕获（错误）{
        完成（错误）；
      }
    }
    火车（）
    });


 for (var i = 0; i &lt; q.length; i++) {
      等待延迟(100)

  
     trainQueue.add(q[i],
       {
       jobId: `汉堡#${Number(new Date())}`,
       尝试次数：5，
       退避：300000，
       延迟：1000，

    // })

    }


module.exports = async（有效负载，完成）=&gt; {
    尝试 {
        // 步骤1.bun

        console.log(payload.data.bun);
        Payload.log(`开始训练，${payload.data.sym}`,);
        const model =等待train.GetModel(payload)
        如果（！模型）{
            抛出新错误（“找不到模型”）
        }
        有效负载.进度(5);
        // 等待延迟(2000)
        const data =等待train.GetLastData(payload,payload.data.qtyitems,payload.data.sym)
        负载.log(数据.长度)
        有效负载.进度(15);

        if (payload.data.qtyitems == data.length) {
            Payload.log(new Date(Number(data[0].open_time)))
            Payload.log(new Date(Number(data[data.length - 1].open_time)))
            Payload.log((数据[0].trained))
            Payload.log((数据[data.length - 1].trained))
            const { 数据集，标签 } =等待train.PrepareData（有效负载，有效负载.data.qtyitems，数据）
            有效负载.进度(25);

            const { datasetTenser, labelsTenser } =等待train.getTensers(有效负载，数据集，标签)
            有效负载.进度(50);

            const训练=等待train.Train（有效负载，模型，datasetTenser，labelsTenser）
            const rr = data.splice(data.length / 2)
            有效负载.进度(75);

            const UpdateData =等待train.UpdateData（有效负载，数据）
            有效负载.进度(100);


        } 别的 {
            throw new Error(&#39;Не то количество минут, которое ожидали&#39;)
        }

        // 等待延迟(3000)

        等待有效负载.进度(100);
        // 完毕（）;
        返回真
    } 捕获（错误）{

        console.log(“错误”)
        返回完成（错误）；
    }
}

 const t = wait model.fit(数据集, 标签, {
        纪元：3，
        批量大小：32，
     
      });

    
      等待 model.save(&#39;file://model-js&#39;)
      Payload.log(&#39;Сохранил модель!&#39;)

      tf.dispose（模型）
      tf.dispose（数据集）
      tf.dispose（标签）
]]></description>
      <guid>https://stackoverflow.com/questions/77980320/training-the-tenserflow-js-model-with-a-large-amount-of-data</guid>
      <pubDate>Mon, 12 Feb 2024 09:03:42 GMT</pubDate>
    </item>
    <item>
      <title>何时应用假设检验以及选择哪一个，以及是否对所有类型的数据集进行假设检验[关闭]</title>
      <link>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</link>
      <description><![CDATA[我一直在研究 Kaggle 的太空泰坦尼克号数据集，并且我看过一个关于类似类型问题的代码笔记本，所以我尽管使用笔记本中的相同步骤，但他已经完成了所有步骤，例如加载数据、变量描述和单变量等。我理解了大部分步骤，甚至将它们应用于太空泰坦尼克号，但我不理解在双变量部分进行的假设检验，我很困惑他是如何应用这些类型的测试以及如何进行的我可以学习吗？
那么我可以跳过双变量分析的这一部分吗？
有人可以给我指出一些资源或者我可以在哪里理解假设检验吗？它是否适用于所有类型的数据集，例如 nlp、cv 等？]]></description>
      <guid>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</guid>
      <pubDate>Mon, 12 Feb 2024 06:53:48 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 训练在 CPU 和 GPU 上都很慢</title>
      <link>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</link>
      <description><![CDATA[我正在尝试使用 Tensorflow Keras 在我的机器上训练 CNN 模型。
这是我的机器规格：

CPU：Ryzen 9 3900x（12 核 24 线程）
GPU：GTX 970 4GB

模型 = models.Sequential([
    层.InputLayer(input_shape=input_shape),
    层.Conv2D（32，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Flatten(),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.Dense（n_classes，激活=&#39;softmax&#39;），
]）

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    火车发电机，
    每纪元的步数=153，
    批量大小=32，
    验证数据=验证生成器，
    验证步骤=23，
    详细=1，
    纪元=20，
）

我使用的是仅具有 CPU 访问权限的 Windows 11。使用 model.fit() 进行训练大约需要 1 小时（仅 CPU 训练）。我在同一台机器上双重启动 Linux Ubuntu，以获得更轻松的 GPU 支持。现在我已经确认了 Linux 环境中的 GPU 支持，但训练仍然需要大约 1 小时（仅 GPU 训练）。
CPU强而GPU弱，速度一样吗？还有其他可能出错的地方吗？
我确保在训练模型时使用 GPU。速度还是挺慢的。]]></description>
      <guid>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</guid>
      <pubDate>Mon, 12 Feb 2024 06:51:29 GMT</pubDate>
    </item>
    <item>
      <title>在 qiskit.algorithms.optimizers.ADAM 优化过程中获取中间步骤的方法</title>
      <link>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</link>
      <description><![CDATA[使用 Qiskit 内部 ADAM 优化器时，有什么方法可以获取中间步骤（类似于 Tensorflow 或 Pytorch Adams 优化器中的回调函数）？
我正在实现变分量子电路 (VQC)，以使用 Qiskit 的 Adam 优化器训练量子机器学习模型。
我浏览了 ADAM 的文档 Qiskit 中的优化器，但我找不到合适的方法来获取它们。
我可以使用 Pytorch Adam 优化器获取中间训练信息，但我想知道是否可以使用 Qiskit 来做到这一点。
我正在使用qiskit v0.45.2。]]></description>
      <guid>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</guid>
      <pubDate>Mon, 12 Feb 2024 00:13:43 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 得到了意外的关键字参数“units”</title>
      <link>https://stackoverflow.com/questions/77977628/lstm-got-an-unexpected-keyword-argument-units</link>
      <description><![CDATA[这段LSTM代码用于时间序列分析，在模型架构上遇到了错误。
def LSTM(X_train, X_test, y_train, y_test):
  缩放器 = MinMaxScaler()
  X_train_scaled = 缩放器.fit_transform(X_train)
  X_test_scaled = 缩放器.transform(X_test)
  y_train_scaled = 缩放器.fit_transform(y_train)
  y_test_scaled = 缩放器.transform(y_test)

  X_train_reshape = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1],1))
  X_test_reshape = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1],1))

  模型=顺序（）
  model.add(LSTM(单位=50, input_shape=(X_train_reshape.shape[1], X_train_reshape.shape[2])))
  model.add(密集(单位=1))
  model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

  model.fit（X_train_reshape，y_train_scaled，epochs = 50，batch_size = 32）

  损失 = model.evaluate(X_test_reshape, y_test_scaled)
  print(f“测试损失：{loss}”)

  Predictions_scaled = model.predict(X_test_reshape)
  预测=scaler.inverse_transform(predictions_scaled)

  rmse = np.sqrt(np.mean(np.square(预测 - y_test.values)))
  print(f“均方根误差 (RMSE): {rmse}”)


该函数导致以下错误 -
 12 模型 = Sequential()
---&gt; 13 model.add(LSTM(单位=50, input_shape=(X_train_reshape.shape[1], X_train_reshape.shape[2])))
     14 model.add(密集(单位=1))
     15 model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;）

TypeError：LSTM() 得到了意外的关键字参数“units”
]]></description>
      <guid>https://stackoverflow.com/questions/77977628/lstm-got-an-unexpected-keyword-argument-units</guid>
      <pubDate>Sun, 11 Feb 2024 17:05:10 GMT</pubDate>
    </item>
    <item>
      <title>如何对多变量函数使用拉格朗日插值多项式？</title>
      <link>https://stackoverflow.com/questions/77968655/how-to-use-lagrange-interpolation-polynomial-for-functions-of-multiple-variables</link>
      <description><![CDATA[在对单个变量的函数进行插值的情况下，事情相对简单：
def create_basic_polynomial(x_values, i):
    def basic_polynomial(x):
        分频器 = 1
        结果 = 1
        对于范围内的 j(len(x_values))：
            如果 j != i:
                结果 *= (x-x_values[j])
                除法器 *= (x_values[i]-x_values[j])
        返回结果/除法器
    返回基本多项式


def create_lagrange_polynomial(x_values, y_values):
    基本多项式 = []
    对于范围内的 i(len(x_values))：
        basic_polynomials.append(create_basic_polynomial(x_values, i))

    def lagrange_polynomial(x):
        结果 = 0
        对于范围内的 i(len(y_values))：
            结果+= y_values[i]*basic_polynomials[i](x)
        返回结果
    返回拉格朗日多项式


x_值 = [0, 2, 3, 5]
y 值 = [0, 1, 3, 2]

lag_pol = create_lagrange_polynomial(x_values, y_values)

对于 x_values 中的 x：
    print(&quot;x = {:.4f}\t y = {:4f}&quot;.format(x,lag_pol(x)))

x = 0.0000 y = 0.000000
x = 2.0000 y = 1.000000
x = 3.0000 y = 3.000000
x = 5.0000 y = 2.000000

但是我们如何实现处理多变量函数的逻辑呢？]]></description>
      <guid>https://stackoverflow.com/questions/77968655/how-to-use-lagrange-interpolation-polynomial-for-functions-of-multiple-variables</guid>
      <pubDate>Fri, 09 Feb 2024 13:59:16 GMT</pubDate>
    </item>
    <item>
      <title>将迁移学习应用于不同的骨干网[关闭]</title>
      <link>https://stackoverflow.com/questions/77179249/apply-transfer-learning-with-different-backbones</link>
      <description><![CDATA[我有一个使用 R50-FPN、COCO 数据集和 Mask R-CNN 训练的预训练权重。原始数据集不可用。
我正在考虑使用不同的主干网，例如 X101-FPN，并使用新数据集（我必须处理的图像）对预训练的权重应用迁移学习。
这对你来说有意义吗？我怀疑它是否适用于不同的主干结构，权重应该有很大不同，这很可能会使这个想法无效。
顺便说一句，这似乎是我想象的深度学习的一个非常常见的场景。比如说，尝试重用不同主干网的预训练权重，并且没有可用的原始数据集。人们如何处理它？就放弃吗？]]></description>
      <guid>https://stackoverflow.com/questions/77179249/apply-transfer-learning-with-different-backbones</guid>
      <pubDate>Tue, 26 Sep 2023 10:43:12 GMT</pubDate>
    </item>
    <item>
      <title>Scala 支持向量机库</title>
      <link>https://stackoverflow.com/questions/12536619/scala-support-vector-machine-library</link>
      <description><![CDATA[我需要一个 Scala 支持向量机库。
我想我应该看看 Scala 和 Java 实现，你建议我特别使用它们中的任何一个吗？]]></description>
      <guid>https://stackoverflow.com/questions/12536619/scala-support-vector-machine-library</guid>
      <pubDate>Fri, 21 Sep 2012 18:59:16 GMT</pubDate>
    </item>
    </channel>
</rss>