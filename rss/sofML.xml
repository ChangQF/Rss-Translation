<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 25 Nov 2024 15:19:11 GMT</lastBuildDate>
    <item>
      <title>OCR 验证码求解器模型在 Google Colab 和本地机器之间产生不一致的结果</title>
      <link>https://stackoverflow.com/questions/79222813/ocr-captcha-solver-model-produces-inconsistent-results-between-google-colab-and</link>
      <description><![CDATA[我正在训练一个用于解决验证码的 OCR 模型，它在 Google Colab 中直接使用时表现良好。但是，当我下载训练好的模型并在本地机器上使用它时，结果却大不相同。该模型无法正确预测任何验证码，准确率几乎降至零。
问题详情：

环境：

我本地机器上的 TensorFlow 和 Keras 版本与 Colab 中的版本匹配。
两个平台上都使用相同的脚本和代码。
使用相同 Colab 笔记本和流程训练的其他验证码模型在本地运行良好。


问题：

只有这个特定的验证码模型（在特定的验证码图像上训练）存在此问题。
在本地下载的模型上运行相同的代码会产生不正确的结果。
将相同的模型上传回 Google Colab 可恢复其原始准确率，预测。


突出显示问题的代码片段：
以下块在本地和 Colab 中生成错误的预测：

## -----------------------问题在这里 ------------
# 使用本地主机上的相同 python 脚本生成错误结果
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import asyncio
from CaptchaSolver import CaptchaSolver

captcha_solver：CaptchaSolver = CaptchaSolver（
captcha_model_path=save_model_name，
max_length=4，
image_shape=(62, 200)
)
image_path = &#39;game_vault_captcha_test_1.png&#39;
prediction_text：str = await captcha_solver.predict_from_image_path(
image_path=image_path
)
prediction_text



Google Colab Notebook:
这是我用来训练和测试模型的完整笔记本。

我尝试过的:

确保本地和 Colab 中的所有依赖项和版本都相同。
验证了在同一笔记本中训练的其他验证码模型在本地和 Colab 中都能正常工作。
多次重新下载模型以确保它不是损坏的文件。




问题：

什么原因导致 Colab 和我的本地机器之间的模型性能差异如此之大？
我应该采取哪些特定的调试步骤来查明问题？
这可能与保存的模型格式、环境变量或验证码图像的预处理有关吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79222813/ocr-captcha-solver-model-produces-inconsistent-results-between-google-colab-and</guid>
      <pubDate>Mon, 25 Nov 2024 11:20:23 GMT</pubDate>
    </item>
    <item>
      <title>比较两幅图像集分布？</title>
      <link>https://stackoverflow.com/questions/79221947/compare-two-images-sets-distirbutions</link>
      <description><![CDATA[我有两组患有某种疾病的人体皮肤图像。第 1 组的图像肤色较浅，但由于光线不好，许多图像显得较暗。我通过修改第 1 组图像的肤色使它们更暗，制作了另一组，即第 2 组。现在，我想比较这两组的分布。我应该使用哪种测量方法，例如 Wasserstein 距离、Kullback-Leibler 距离等？在测量距离之前我应该​​做什么，例如我应该制作两个大小为 256 个箱的直方图，其中每个像素代表其在两组中出现的概率并进行比较吗？我的图像是 RGB，如果是 RGB 图像，我应该如何测量？]]></description>
      <guid>https://stackoverflow.com/questions/79221947/compare-two-images-sets-distirbutions</guid>
      <pubDate>Mon, 25 Nov 2024 06:52:08 GMT</pubDate>
    </item>
    <item>
      <title>加载的模型数据与测试数据不同吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</link>
      <description><![CDATA[我将一个经过训练的模型加载到 R studio 4 中，当我使用 xgboost 进行预测时，它总是说：
 存储在对象和新数据中的特征名称不同

我检查了训练和测试的数据，列名和顺序相同，但测试数据有一个额外的列作为响应/依赖变量，用于预测结果或将预测与其进行比较。
没有它，你怎么知道要预测什么？
loadedmodel$feature_names（打印出 6 列）

test &lt;- as.matrix(test2)

test$feature_names（它说原子向量，实际上是 6 列和 1 个依赖列与预测结果相同）

dtest&lt;- xgb.DMatrix(test2, missing=NaN)

pred&lt;- predict(loadedmodel, dtest)

什么是错了吗？
MADZ]]></description>
      <guid>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</guid>
      <pubDate>Sun, 24 Nov 2024 17:33:43 GMT</pubDate>
    </item>
    <item>
      <title>当模型训练完成时，演员-评论家的标准差是否会收敛到 0？</title>
      <link>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</link>
      <description><![CDATA[我正在使用 Advantage Actor-Critic 算法来解决我的问题。
actor NN 预测我采样动作的平均值和标准偏差。
这是否意味着预测的标准偏差会随着时间的推移而减少，直到模型收敛时变得太小？
或者即使 A&amp;C 模型收敛，我也“注定”总是具有随机性？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</guid>
      <pubDate>Sun, 24 Nov 2024 16:32:22 GMT</pubDate>
    </item>
    <item>
      <title>在验证期间计算 mIoU</title>
      <link>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</link>
      <description><![CDATA[我正在研究二元分割任务，并已实施以下训练和验证循环。
我需要两点帮助：
如何计算每个时期后每个类别的 IoU，并打印第 1 类 IoU、第 2 类 IoU 和总体 mIoU 分数？
根据最佳 mIoU 分数或最低验证损失保存模型更好吗？
任何指导都将不胜感激。谢谢！
这是我的代码：
# 初始化列表以存储损失值
train_losses = []
val_losses = []

# 训练和验证循环
for epoch in range(n_eps):
model.train()
train_loss = 0.0

# 训练循环
for images, mask in tqdm(train_loader):
images, mask = images.to(device), mask.to(device)

optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, mask)
loss.backward()
optimizer.step()

train_loss += loss.item()

avg_train_loss = train_loss / len(train_loader)
train_losses.append(avg_train_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}], Train Loss: {avg_train_loss:.4f}&quot;)

model.eval()
val_loss = 0.0

# 验证循环
with torch.no_grad():
for images, mask in val_loader:
images, mask = images.to(device), mask.to(device)
output = model(images)
val_loss += criterion(outputs, mask).item()

avg_val_loss = val_loss / len(val_loader)
val_losses.append(avg_val_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}], Val Loss: {avg_val_loss:.4f}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</guid>
      <pubDate>Sun, 24 Nov 2024 15:59:00 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 可加性检查失败，RandomForestClassifier 的 SHAP 值达到天文数字</title>
      <link>https://stackoverflow.com/questions/79220150/shap-additivity-check-fails-with-astronomical-shap-values-for-randomforestclassi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79220150/shap-additivity-check-fails-with-astronomical-shap-values-for-randomforestclassi</guid>
      <pubDate>Sun, 24 Nov 2024 13:20:28 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Visual Studio 代码中训练监督和机器学习模型</title>
      <link>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</link>
      <description><![CDATA[我在 Nvidia RTX 4070 8Gb Vram 上运行 Visual Studio 代码。
我花了很长时间训练一些模型，而我朋友的 4070 上花费的时间比我少得多。
有什么建议吗？
更新了 CUDA 驱动程序和工具包。确实将我的电池设置为高性能模式（但我认为这不是问题所在）
检查 Python 包和框架版本是否与我朋友的设置相同。
从 cmd 检查 Nvidia-Smi 中的使用情况，但似乎没有使用 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</guid>
      <pubDate>Sun, 24 Nov 2024 06:39:27 GMT</pubDate>
    </item>
    <item>
      <title>Python scorecardpy：UnboundLocalError：赋值前引用了局部变量“card_df”</title>
      <link>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</link>
      <description><![CDATA[我使用 scorecardpy 函数来获取模型：
import scorecardpy as ac
card=sc.scorecard(bins_adj, lr, X_train.columns)

然后我尝试使用以下代码保存此模型：
import numpy as np
np.save(&#39;card.npy&#39;,card)

之后我尝试重新加载此模型：
card=np.load(&#39;card.npy&#39;,allow_pickle=True)

然后我想使用该模型获取分数：
score=sc.scorecard_ply(data_train, card, print_step=0)

但它给出了错误：
UnboundLocalError Traceback（最近一次调用最后一次）
单元格在 [91]，第 1 行
score=sc.scorecard_ply(data_train, card, print_step=0)

文件 ~/.local/lib/python3.9/site-packages/scorecardpy/scorecard.py:330，在 scorecard_ply(dt, card, only_total_score, print_step, replace_blank_na, var_kp)
card_df=card.copy(deep=True)
# x 变量
xs=card_df.loc[card_df.variable != &#39;basepoints&#39;, &#39;variable&#39;].unique()
# x 变量的长度
xs_len=len(xs)

UnboundLocalError：局部变量“card_df”在赋值前被引用

我无法解决这个问题问题。请帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</guid>
      <pubDate>Sun, 24 Nov 2024 04:16:39 GMT</pubDate>
    </item>
    <item>
      <title>错误：TypeError：无法将 cuda:0 设备类型张量转换为 numpy。首先使用 Tensor.cpu() 将张量复制到主机内存</title>
      <link>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</guid>
      <pubDate>Sat, 23 Nov 2024 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>具有动态约束的贷款人和借款人之间的多元化资金分配算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</guid>
      <pubDate>Sat, 23 Nov 2024 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>如何改进 CNN 的图像分类能力</title>
      <link>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</guid>
      <pubDate>Sat, 23 Nov 2024 15:26:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 RMSprop 优化器的动态学习率进行 Q 学习</title>
      <link>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。以下是我正在使用的 Q-learning 更新函数：
**
python 复制代码**
def update_q_table(self, state, action, reward, next_state):
&quot;&quot;&quot;使用 Q-learning 更新规则更新 Q-table。&quot;&quot;&quot;
def update_q_table(self, state, action, reward, next_state):
best_next_action = np.argmax(self.q_table[next_state, :])
td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
td_error = td_target - self.q_table[state, action]

# 更新 RMSprop 的平方梯度移动平均值 E[g^2]
self.gradient_Q[state, action] = (
self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2)
)

self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon)
self.learning_rate_history.append(self.learning_rate)

# 使用固定学习率和 TD 误差更新 Q 值
self.q_table[state, action] += self.learning_rate * td_error 

# 存储 E[g^2] 值以进行跟踪
self.gradient_history.append(self.gradient_Q[state, action])


我正在使用受 RMSprop 启发的动态学习率实现 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。这是我正在使用的 Q 学习更新函数：]]></description>
      <guid>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</guid>
      <pubDate>Mon, 11 Nov 2024 10:33:43 GMT</pubDate>
    </item>
    <item>
      <title>iOS 视觉框架 - 无法在 VNDetectHumanBodyPoseRequest 中设置请求</title>
      <link>https://stackoverflow.com/questions/70473725/ios-vision-framework-unable-to-setup-request-in-vndetecthumanbodyposerequest</link>
      <description><![CDATA[我使用 VNDetectHumanBodyPoseRequest 从 xcode 资源中的图像中检测身体（我从图像网站下载），但出现以下错误：

2021-12-24 21:50:19.945976+0800 Guess My Exercise[91308:4258893] [espresso] [Espresso::handle_ex_plan] exception=Espresso exception: &quot;I/O error&quot;: Missing weights path cnn_human_pose.espresso.weights status=-2
无法执行请求：错误域=com.apple.vis 代码=9 &quot;无法在 VNDetectHumanBodyPoseRequest 中设置请求&quot; UserInfo={NSLocalizedDescription=无法在 VNDetectHumanBodyPoseRequest 中设置请求}。

以下是我的代码：
 let image = UIImage(named: &quot;image2&quot;)
guard let cgImage = image?.cgImage else{return}

let requestHandler = VNImageRequestHandler(cgImage: cgImage)

let request = VNDetectHumanBodyPoseRequest(completionHandler: bodyPoseHandler)

do {
// 执行身体姿势检测请求。
try requestHandler.perform([request])
} catch {
print(&quot;无法执行请求：\(error).&quot;)
}

func bodyPoseHandler(request: VNRequest, error: Error?) {
guard let surveillance =
request.results as? [VNHumanBodyPoseObservation] else {
return
}

let poses = Pose.fromObservations(observations)

self.drawPoses(poses, onto: self.simage!)
// 处理每个观察结果以找到已识别的身体姿势点。
}
]]></description>
      <guid>https://stackoverflow.com/questions/70473725/ios-vision-framework-unable-to-setup-request-in-vndetecthumanbodyposerequest</guid>
      <pubDate>Fri, 24 Dec 2021 14:08:04 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 的 GridSearchCV 中评分</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我目前正在尝试首次使用 XGBoost 分析数据。我想使用 GridsearchCV 找到最佳参数。我想最小化均方根误差，为此，我使用“rmse”作为 eval_metric。但是，网格搜索中的评分没有这样的指标。我在这个网站上发现“neg_mean_squared_error”也有同样的效果，但我发现这给出的结果与 RMSE 不同。当我计算“neg_mean_squared_error”绝对值的根时，我得到的值约为 8.9，而另一个函数给出的 RMSE 约为 4.4。
我不知道哪里出了问题，或者我如何让这两个函数一致/给出相同的值？
由于这个问题，我得到了错误的值作为“best_params_”，这给了我一个比我最初开始调整的一些值更高的 RMSE。
有人能解释一下如何在网格搜索中获得 RMSE 的分数，或者为什么我的代码给出了不同的值吗？ 
提前致谢。
def modelfit(alg, trainx, trainy, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):
if useTrainCV:
xgb_param = alg.get_xgb_params()
xgtrain = xgb.DMatrix(trainx, label=trainy)
cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;], nfold=cv_folds,
metrics=&#39;rmse&#39;, early_stopping_rounds=early_stopping_rounds)
alg.set_params(n_estimators=cvresult.shape[0])

# 将算法拟合到数据上
alg.fit(trainx, trainy, eval_metric=&#39;rmse&#39;)

# 预测训练集：
dtrain_predictions = alg.predict(trainx)
# dtrain_predprob = alg.predict_proba(trainy)[:, 1]
print(dtrain_predictions)
print(np.sqrt(mean_squared_error(trainy, dtrain_predictions)))

# 打印模型报告：
print(&quot;\nModel Report&quot;)
print(&quot;RMSE : %.4g&quot; % np.sqrt(metrics.mean_squared_error(trainy, dtrain_predictions)))

param_test2 = {
&#39;max_depth&#39;:[6,7,8],
&#39;min_child_weight&#39;:[2,3,4]
}

grid2 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=2000, max_depth=5,
min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,
objective= &#39;reg:linear&#39;, nthread=4, scale_pos_weight=1, random_state=4),
param_grid = param_test2,scoring=&#39;neg_mean_squared_error&#39;, n_jobs=4,iid=False, cv=10, verbose=20)
grid2.fit(X_train,y_train)
# best_estimator 的平均交叉验证分数
print(grid2.best_params_, np.sqrt(np.abs(grid2.best_score_))), print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
modelfit(grid2.best_estimator_, X_train, y_train)
print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>Q 值无限增加，Q 学习中重复相同动作后重复奖励的结果</title>
      <link>https://stackoverflow.com/questions/13148934/unbounded-increase-in-q-value-consequence-of-recurrent-reward-after-repeating-t</link>
      <description><![CDATA[我正在开发一个简单的 Q-Learning 实现，用于一个普通的应用程序，但有些事情一直困扰着我。
让我们考虑一下 Q-Learning 的标准公式
Q(S, A) = Q(S, A) + alpha * [R + MaxQ(S&#39;, A&#39;) - Q(S, A)]

让我们假设存在这个状态 K，它有两个可能的操作，都通过 A 和 A&#39; 授予我们的代理奖励 R 和 R&#39;。
如果我们遵循几乎完全贪婪的方法（假设我们假设 0.1 epsilon），我将首先随机选择其中一个操作，例如 A。下一次，我可能会（90% 的时间）再次选择 A，这将导致 Q(K, A) 不断增长，即使我偶然尝试 A&#39;，因为它的奖励可能与 A 的奖励大小相同，我们也会陷入一种情况，在其余的学习过程中，几乎不可能从我们的第一个猜测中“恢复”。
我想这一定不是这样，否则代理基本上不会学习——它只是遵循一个简单的配方：像你第一次做的那样做所有事情。
我遗漏了什么吗？我知道我可以调整 alpha 值（通常是随着时间的推移而减少它），但这绝不会改善我们的情况。]]></description>
      <guid>https://stackoverflow.com/questions/13148934/unbounded-increase-in-q-value-consequence-of-recurrent-reward-after-repeating-t</guid>
      <pubDate>Tue, 30 Oct 2012 23:11:32 GMT</pubDate>
    </item>
    </channel>
</rss>