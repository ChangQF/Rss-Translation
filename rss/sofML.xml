<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Mar 2024 00:47:26 GMT</lastBuildDate>
    <item>
      <title>类型错误：fit() 缺少 1 个必需的位置参数 Y</title>
      <link>https://stackoverflow.com/questions/78118068/typeerror-fit-missing-1-required-positional-argument-y</link>
      <description><![CDATA[代码：
def _forward(self, x):
    self.trained_model = self.model.fit(x)
    返回 self.trained_model.labels_

我在这个项目中使用了很多模型，代码与 minibatchkmeans 和birthch 完美配合，但是在添加 kneighborclassifier 后，它开始显示错误消息，如下所示：
回溯（最近一次调用最后一次）：
  文件“E:\Desktop\customer_analysis\customer_analysis-main\main.py”，第 39 行，位于  中。
    主要的（）
  文件“E:\Desktop\customer_analysis\customer_analysis-main\main.py”，第 15 行，在 main 中
    预测标签 = cluster_model._forward(data_array)
  文件“E:\Desktop\customer_analysis\customer_analysis-main\dnn_module\dnn_model.py”，第 193 行，位于 _forward
    self.trained_model = self.model.fit(x)
类型错误：fit() 缺少 1 个必需的位置参数：&#39;y&#39;

如果我添加更多参数，例如纪元和批量大小，它会显示：
类型错误：fit() 获得意外的关键字参数“batch_size”

如果我添加另一个参数，例如 []：
self.trained_model = self.model.fit(x, [])

它会导致值错误。]]></description>
      <guid>https://stackoverflow.com/questions/78118068/typeerror-fit-missing-1-required-positional-argument-y</guid>
      <pubDate>Thu, 07 Mar 2024 00:06:38 GMT</pubDate>
    </item>
    <item>
      <title>训练预训练 CNN 模型时出现矩阵大小不兼容错误</title>
      <link>https://stackoverflow.com/questions/78116865/matrix-size-incompatible-error-while-training-a-pretrained-cnn-model</link>
      <description><![CDATA[矩阵大小不兼容：In[0]：[32,7776]，In[1]：[18816,512]
[[{{节点顺序_1/activation_1/aiRelu}}]]
[操作：__inference_train_function_3421]

值得注意的是，理解这个错误它可以在更少的数据集和更少的类的情况下顺利运行
# # 建模开始使用 CNN。

模型=顺序（）
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = &#39;相同&#39;,activation =&#39;relu&#39;, input_shape = (224,224,3)))
model.add(MaxPooling2D(pool_size=(2,2)))


model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = &#39;相同&#39;,activation =&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))


model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = &#39;相同&#39;,activation = &#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = &#39;相同&#39;,activation =&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

模型.add(压平())
模型.add（密集（512））
model.add(激活(&#39;relu&#39;))
model.add（密集（5，激活=“softmax”））

文本]]></description>
      <guid>https://stackoverflow.com/questions/78116865/matrix-size-incompatible-error-while-training-a-pretrained-cnn-model</guid>
      <pubDate>Wed, 06 Mar 2024 19:09:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的训练损失和验证损失非常低，但新数据集的均方误差却很高？</title>
      <link>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</link>
      <description><![CDATA[来自tensorflow.keras导入层
从tensorflow.keras.layers导入Dense
从tensorflow.keras.layers导入LSTM
从tensorflow.keras.models导入顺序
从tensorflow.keras导入正则化器

模型=顺序（[
       LSTM(单位=64, input_shape=(len(x.columns),1),
       kernel_regularizer=regularizers.l2(0.01)),
       密集(1)
     ]）
model.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer=tf.keras.optimizers.Adam())
历史= model.fit（x_train，y_train，validation_data =（x_val，y_val），epochs = 100，verbose = 1，batch_size = 32）

mse=model.evaluate(x_test, y_test)
打印（毫秒）

这是我得到的损失图：

我得到的值非常高（30.2344）]]></description>
      <guid>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</guid>
      <pubDate>Wed, 06 Mar 2024 18:14:44 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 与 RTX 4090 在训练时间上存在巨大差异 [关闭]</title>
      <link>https://stackoverflow.com/questions/78116561/pytorch-with-rtx-4090-huge-difference-in-training-time</link>
      <description><![CDATA[我在大学计算机上用 PyTorch 训练了一个模型 48 个 epoch，大约需要 8 分钟。
我现在在家装了一台新电脑，但训练时间明显更长，48 个 epoch 大约需要 17.5 分钟。
规格：

大学：

RTX 4090
i9-13900KF
64GB 内存


首页：

RTX 4090
i9-14900KF
48GB 内存



我在两者上使用相同的代码，并且没有更改训练的任何其他参数。
目前我已经安装：

CUDA 版本 12.4
驱动程序版本 551.61
火炬版本2.2.1+cu121
Python 3.11

在训练期间，没有一个组件得到充分利用：

内存 20/48GB
CPU 33%（70°C）
光盘 1%
GPU 50% (40°C)
显存 3.2/24GB

我使用 Cinebench、MemTest86 和 3DMark 测试了我的 CPU、GPU 和 RAM，所有测试结果均符合预期，因此硬件应该可以正常工作。游戏过程中的表现也符合预期。温度也还可以。
除了 PyCharm 之外，只有 Discord 和 Opera 正在运行，这不会显着降低训练速度。
我认为驱动程序可能存在问题，或者 torch、cuda 和驱动程序版本之间的兼容性有问题？
我还在两个系统上都做了测试，仅使用cpu进行训练。在这种情况下，我的计算机速度更快，大约为 5.5 次迭代/秒，而大学计算机大约为 2.3 次迭代/秒。那么问题一定出在 GPU 的某个地方？
您可以在此处访问测试代码。
训练在train.py中开始。]]></description>
      <guid>https://stackoverflow.com/questions/78116561/pytorch-with-rtx-4090-huge-difference-in-training-time</guid>
      <pubDate>Wed, 06 Mar 2024 18:12:11 GMT</pubDate>
    </item>
    <item>
      <title>强化学习神经网络概率没有改变</title>
      <link>https://stackoverflow.com/questions/78116374/reinforcement-learning-neural-network-probabilities-arent-changing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78116374/reinforcement-learning-neural-network-probabilities-arent-changing</guid>
      <pubDate>Wed, 06 Mar 2024 17:35:37 GMT</pubDate>
    </item>
    <item>
      <title>在 MLJ 中找不到适合我的数据类型的模型</title>
      <link>https://stackoverflow.com/questions/78116043/can-not-find-a-model-in-mlj-suitable-for-my-data-type</link>
      <description><![CDATA[我想在 MLJ 中使用 RandomForestRegressor，但我的数据类型总是出现错误。
这是我的数据类型：
# X 的类型
500×1矩阵{Float64}：
 0.002
 0.004
 0.006
 0.008
 0.01
 ⋮
 0.992
 0.994
 0.996
 0.998
 1.0

#y 的类型
500 元素向量{任意}：
 [-26.12471826402402、-0.00019238911241092893、-8.487354458186491e-6、-4.247928689027347e-6、-1.4735850473179823e-6、-5.5989652116 83904e-6]
 [-26.124690243484718、-2.813962224679223e-5、0.00014993993007544892、-3.972667350815584e-5、-4.415345127384285e-5、4.7755059188 858695e-5]
 [-26.124700578434716、0.00020304547366412073、-1.941266595367752e-5、-6.8228290228677935e-6、-2.8075749891054436e-6、-5.95726165 1756696e-7]
 [-26.12470127817773、-0.0002052884039169811、-1.53​​54293043945422e-5、5.457931474439626e-6、-1.0654739228677101e-6、1.3601831927 445573e-6]
 [-26.124689854622336、-0.0001879523626138191、5.983618996563411e-6、-2.412535654516823e-5、1.7942953590588395e-5、-1.3323422657 25206e-5]
 ⋮
 [-26.12470204971627、-0.00020642317364671925、-1.6756339903056805e-5、7.454364967629523e-6、-2.539052483596649e-6、1.8465582632 964939e-6]
 [-26.12469951981032、0.00020256742611590717、-1.9380595701112835e-5、-6.199351626712257e-6、-3.5935735922532075e-6、-1.153947443 044423e-6]
 [-26.124700578434716、0.00020304547366412073、-1.9412665953733033e-5、-6.8228290228677935e-6、-2.8075749891054436e-6、-5.9572616 58418034e-7]
 [-26.124701332004584、-0.00020670402366529395、-1.6873028843567006e-5、7.897781219456945e-6、-3.3466380342517255e-6、1.43543246 5890507e-6]
 [-26.124682551931944、0.0001869595100869592、5.027911548549646e-6、1.9320833163805062e-5、1.57188​​013177878e-5、6.48586574647 5028e-6]

上面的数据可以在DecisionTree.jl中使用，但在MLJ.jl中不起作用。
这是我的 DecisionTree.jl 代码，它运行良好
模型 = build_forest(y, X)
apply_forest（模型，[0.75]）

这是我的 MLJ.jl 代码。
RandomForest = MLJ.@load RandomForestRegressor pkg = DecisionTree
myrandforest = 随机森林()
马赫=机器（myrandforest，X，y）|&gt; MLJ.适合！

和错误消息：
导入 MLJDecisionTreeInterface ✔
┌ 警告：数据参数的数量和/或类型与指定模型不匹配
│ 支持。通过指定“scitype_check_level=0”来抑制此类型检查。
│
│ 运行“@doc DecisionTree.RandomForestRegressor”以了解有关模型要求的更多信息。
│
│ 通常但非唯一地，监督模型是使用以下语法构建的
│ `machine(model, X, y)` 或 `machine(model, X, y, w)` 而大多数其他模型是
│ 用 `machine(model, X)` 构造。这里“X”是特征，“y”是目标，“w”
│ 样本或类别权重。
│
│ 一般来说，`machine(model, data...)`中的数据预计满足
│
│ scitype(数据) &lt;: MLJ.fit_data_scitype(模型)
│
│ 在本案中：
│
│ scitype(数据) = Tuple{AbstractMatrix{连续}, AbstractVector{AbstractVector{连续}}}
│
│ fit_data_scitype(model) = Tuple{Table{&lt;:Union{AbstractVector{&lt;:连续}, AbstractVector{&lt;:Count}, AbstractVector{&lt;:OrderedFactor}}}, AbstractVector{连续}}
└ @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:231
错误：ArgumentError：“Matrix{Float64}”不是表；有关将 AbstractVecOrMat 视为表的方法，请参阅“?Tables.table”
堆栈跟踪：
  [1] 列(m::Matrix{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:6
  [2] Tables.Columns(x::Matrix{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/Tables.jl:271
  [3] 矩阵(表::Matrix{Float64};转置::Bool)
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:85
  [4] 矩阵(表::矩阵{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:84
  [5] 重新格式化(::MLJDecisionTreeInterface.RandomForestRegressor, X::Matrix{Float64}, y::Vector{Any})
    @ MLJDecisionTreeInterface ~/.julia/packages/MLJDecisionTreeInterface/kPIDf/src/MLJDecisionTreeInterface.jl:460
  [6] fit_only!(mach::Machine{…}; rows::Nothing，verbosity::Int64，force::Bool，composite::Nothing)
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:659
  [7] 只适合！
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:607 [内联]
  [8] #适合！#63
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:778 [内联]
  [9] 适合！
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:775 [内联]
 [10] |&gt;(x::Machine{MLJDecisionTreeInterface.RandomForestRegressor, true}, f::typeof(StatsAPI.fit!))
    @基地./operators.jl:917
 [11] Mainf()
    @主要~/Code/JuliaML/src/dt.jl:53
 [12] 顶级范围
    @〜/代码/JuliaML/src/dt.jl:58
某些类型信息被截断。使用 show(err) 查看完整类型。

它不适用于 MLJ.jl。我想使用 RandomForestRegressor 预测一个包含 5 或 6 个浮点数的向量，模型必须是 RF。]]></description>
      <guid>https://stackoverflow.com/questions/78116043/can-not-find-a-model-in-mlj-suitable-for-my-data-type</guid>
      <pubDate>Wed, 06 Mar 2024 16:41:59 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试运行高级自动训练时遇到错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78115600/i-am-getting-an-error-while-trying-to-run-autotrain-advanced</link>
      <description><![CDATA[我使用 Llama-2-7B-Chat-Hf 模型作为基础，Orca Math Word Problems 作为训练数据。我遇到错误 - UnicodeDecodeError: &#39;utf-8&#39; 编解码器无法解码位置 7 中的字节 0x92: 无效的起始字节。
如何解决这个问题？
这是我尝试的图像 - 我尝试构建的图像&lt; /p&gt;
这是完整的错误：
回溯（最近一次调用最后一次）：
  文件“/app/env/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py”，第 428 行，在 run_asgi 中
    结果=等待应用程序（＃类型：忽略[func-returns-value]
  文件“/app/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py”，第 78 行，在 __call__ 中
    返回等待 self.app（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/fastapi/applications.py”，第 1106 行，在 __call__ 中
    等待超级（）.__call__（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/applications.py”，第 122 行，在 __call__ 中
    等待 self.middleware_stack（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 184 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 162 行，在 __call__ 中
    等待 self.app（范围，接收，_发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/sessions.py”，第 86 行，在 __call__ 中
    等待 self.app（范围，接收，send_wrapper）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 79 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 68 行，在 __call__ 中
    等待 self.app（范围、接收、发送者）
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 20 行，在 __call__ 中
    提高e
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 17 行，在 __call__ 中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 718 行，在 __call__ 中
    等待route.handle（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 276 行，在句柄中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 66 行，在应用程序中
    响应=等待函数（请求）
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 274 行，在应用程序中
    raw_response = 等待 run_endpoint_function(
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 191 行，在 run_endpoint_function 中
    返回等待 dependent.call(**值)
  文件“/app/env/lib/python3.10/site-packages/autotrain/app.py”，第 452 行，handle_form
    dset = AutoTrainDataset(**dset_args)
  文件“”，第 13 行，位于 __init__ 中
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 204 行，在 __post_init__ 中
    self.train_df, self.valid_df = self._preprocess_data()
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 213 行，位于 _preprocess_data
    train_df.append(pd.read_csv(文件))
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1026 行，在 read_csv 中
    返回_read（文件路径或缓冲区，kwds）
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 620 行，在 _read 中
    解析器 = TextFileReader(filepath_or_buffer, **kwds)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1620 行，位于 __init__ 中
    self._engine = self._make_engine(f, self.engine)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1898 行，在 _make_engine 中
    返回映射[引擎](f, **self.options)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py”，第 93 行，在 __init__ 中
    self._reader = parsers.TextReader(src, **kwds)
  文件“parsers.pyx”，第 574 行，位于 pandas._libs.parsers.TextReader.__cinit__ 中
  文件“parsers.pyx”，第 663 行，位于 pandas._libs.parsers.TextReader._get_header
  文件“parsers.pyx”，第 874 行，位于 pandas._libs.parsers.TextReader._tokenize_rows 中
  文件“parsers.pyx”，第 891 行，位于 pandas._libs.parsers.TextReader._check_tokenize_status
  文件“parsers.pyx”，第 2053 行，位于 pandas._libs.parsers.raise_parser_error
UnicodeDecodeError：“utf-8”编解码器无法解码位置 7 中的字节 0x92：起始字节无效
]]></description>
      <guid>https://stackoverflow.com/questions/78115600/i-am-getting-an-error-while-trying-to-run-autotrain-advanced</guid>
      <pubDate>Wed, 06 Mar 2024 15:30:30 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程数据错误</title>
      <link>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</link>
      <description><![CDATA[如何在ma​​tlab的fitrgp函数中插入误差数组来进行高斯过程回归？我有一个数组 x、其他数组 y 以及与 y 关联的标准差数组 delta_y。
只有 x 和 y，我可以使用 gprMdl = fitrgp(x,y)，但是如何添加delta_y作为y的错误栏？]]></description>
      <guid>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</guid>
      <pubDate>Wed, 06 Mar 2024 15:02:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中生成 CSV 和 Excel 数据库的列名称？</title>
      <link>https://stackoverflow.com/questions/78114334/how-does-one-generate-column-names-for-csvs-and-excel-databases-in-python</link>
      <description><![CDATA[有一堆没有列名称的逗号分隔值 (CSV) 和 Excel 数据库，有没有办法使用列值通过 Python 自动生成名称？
示例：
输入=[(迈克尔, 18),(安娜, 19),(彼得, 20)

输出：
姓名、年龄
迈克尔，18 岁
安娜，19 岁
彼得，20 岁
]]></description>
      <guid>https://stackoverflow.com/questions/78114334/how-does-one-generate-column-names-for-csvs-and-excel-databases-in-python</guid>
      <pubDate>Wed, 06 Mar 2024 12:20:25 GMT</pubDate>
    </item>
    <item>
      <title>我的感知器算法总是给我错误的线来分离二维线性可分离数据</title>
      <link>https://stackoverflow.com/questions/78112634/my-perceptron-algorithm-keeps-giving-me-the-wrong-line-for-separating-2d-linearl</link>
      <description><![CDATA[我正在为虚拟 2D 数据实现感知器模型。以下是我生成数据的方式
&lt;前&gt;&lt;代码&gt;#numpoint
n = 15
#f(x) = w0 + ax1 + bx2
#那么如果f(x) = 0
#x2 = (-w0 - ax1)/b
截距 = 30
一个= 4
b = 2
#生成0-20之间的随机点
x1 = np.random.uniform(-20, 20, n) #返回一个np数组
x2 = np.random.uniform(-20, 20, n)
y = []
#绘制f(x)
plt.plot(x1, (-截距 - a*x1)/b, &#39;k-&#39;)
plt.ylabel(“x2”)
plt.xlabel(“x1”)

#绘制彩色点
对于范围内的 i(0, len(x1))：
    f = 截距 + a * x1[i] + b * x2[i]
    如果（f &lt;= 0）：
        plt.plot(x1[i], x2[i], &#39;ro&#39;)
        y.追加(-1)
    如果（f&gt;0）：
        plt.plot(x1[i], x2[i], &#39;bo&#39;)
        y.追加(1)
y = np.array(y)
# 添加x0作为阈值
x0 = np.ones(n)
stacked_x = np.stack((x0,x1,x2))
堆叠_x

这是数据的可视化
在此处输入图像描述
这是我的感知器模型
类 PLA():
    def __init__(self, numPredictors):
        self.w = np.random.rand(1,numPredictors+1) #(1, numPredictors+1)
        self.iter = 0
    def fitModel（自身，xData，yData）：
        而（真）：
            yhat = np.matmul(self.w, xData).squeeze() #从(1,n)到(,n)
            比较 = np.sign(yhat) == yData
            ind = [i for i in range(0,len(compare)) if Compare[i] == False] #分类错误的索引
            打印（长度（ind））
            如果 len(ind) == 0:
                休息
            对于 ind 中的 i：
                update = yData[i]* xData[:, i] #1d 数组
                self.w = self.w + np.transpose(update[:,np.newaxis]) #转置以匹配权重的形状
            self.iter += 1

当我可视化模型时
&lt;前&gt;&lt;代码&gt;pla1 = PLA(2)
pla1.fitModel(stacked_x, y)
#绘制彩色点
对于范围内的 i(0, len(x1))：
    如果（y[i]==-1）：
        plt.plot(x1[i], x2[i], &#39;ro&#39;)
    如果（y[i]==1）：
        plt.plot(x1[i], x2[i], &#39;bo&#39;)
plt.plot(x1, (-pla1.w[0][0] - pla1.w[0][1]*x1)/(pla1.w[0][1]), &#39;g-&#39;, 标签 = “解放军”）
plt.plot(x1, (-截距 - a*x1)/b, &#39;k-&#39;, label = &quot;f(x)&quot;)
plt.xlabel(“x1”)
plt.ylabel(“x2”)
plt.图例()

我从感知器算法得到的线是不正确的
在此处输入图像描述
这是使用不同数据参数和样本大小的另一次运行 (n = 30)
在此处输入图像描述
我尝试在每次迭代时打印出更新，它按我的预期工作。我不确定是什么导致我的算法停止，即使仍然存在错误分类的点。我已经被这个问题困扰了几天了。我非常感谢任何意见。]]></description>
      <guid>https://stackoverflow.com/questions/78112634/my-perceptron-algorithm-keeps-giving-me-the-wrong-line-for-separating-2d-linearl</guid>
      <pubDate>Wed, 06 Mar 2024 07:53:35 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降最小二乘代码问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</link>
      <description><![CDATA[我正在尝试在数据集上使用梯度下降。我写的是
&lt;前&gt;&lt;代码&gt;导入numpy
将 pandas 导入为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

数据 = pd.read_csv(&#39;C:/Users/Teacher/Downloads/data.csv&#39;)
X = data.iloc[:, 0] # 选择 data 中第一列的所有数据
Y = data.iloc[:, 1]
plt.scatter(X,Y)
plt.show()
n = 长度 (X)

a = 0
b = 0
L = .001

对于范围（1000）内的 i：
    y_预测 = a * X + b
    pd_a = (1 / n) * sum((y_预测 - Y) * X)
    pd_b = (1 / n) * sum(y_预测 - Y)
    a = a - L * pd_a
    b = b - L * pd_b
打印（a，b）
plt.scatter(X, Y)
c, d = numpy.polyfit(X, Y, 1)
打印（c，d）
plt.plot([min(X), max(X)], [a * x + b for x in [min(X), max(X)]], [c * x + d for x in [min( X), 最大值(X)]])
plt.show()

如果我定义 X 和 Y = np.random.rand(20)，那么一切似乎都工作正常，所以问题似乎出在 csv 的输入上。
然而，X 和 Y 的散点图仍然很好，即使我将它们定义为数据集的第一列和第二列，所以我不确定发生了什么。
编辑：这是定义 X = data.iloc[:, 0] 后的散点图图像
Y = data.iloc[:, 1]

这是代码末尾的绘图和线条的图像。

print(data.head())的结果：

编辑：仅读取 csv 的一行：

]]></description>
      <guid>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</guid>
      <pubDate>Wed, 06 Mar 2024 07:48:04 GMT</pubDate>
    </item>
    <item>
      <title>torchserve ：即使 config.properties 指定其他值，batch_size 也始终为 1</title>
      <link>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</link>
      <description><![CDATA[我认为我的 torchserve 正确加载了 config.properties，因为我设置的工作人员数量是 2。但batch_size是1而不是20。
任何人都知道可能会出现什么问题吗？谢谢！
我已经检查并torchserve正确加载config.properties，可惜它忽略了config.properties中指定的batch_size和max_batch_delay。
这是我的 config.properties 供参考
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
log_file=/ml_server/logs/torchserve.log
default_workers_per_model=2
number_of_netty_threads=32
作业队列大小=1000
批量大小=20
最大批量延迟=10

下面是日志，worker的batchSize：1
ml-server | 2024-03-06T00：11：11,091 [信息] W-9001-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
机器学习服务器 | 2024-03-06T00：11：11,091 [信息] W-9000-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
]]></description>
      <guid>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</guid>
      <pubDate>Wed, 06 Mar 2024 00:19:31 GMT</pubDate>
    </item>
    <item>
      <title>使用记分器实现 GridSearchCV 进行留一交叉验证</title>
      <link>https://stackoverflow.com/questions/60851884/implementing-gridsearchcv-with-scorer-for-leave-one-out-cross-validation</link>
      <description><![CDATA[我正在尝试实现 scikit-learn 的 GridSearchCV 用于高斯过程回归 (GPR)。我正在使用大约 200 个点的小数据集，并且希望使用 LOOCV 作为我的模型的性能评估器。我的设置是：
来自 sklearn.model_selection 导入 *
从 sklearn.ensemble 导入 *
从 sklearn.gaussian_process 导入 *

参数网格 = {
    &#39;内核&#39;:[kernels.RBF(),kernels.Matern(length_scale=0.1)],
    &#39;n_restarts_optimizer&#39;:[5,10,20,25],
    “随机状态”：[30]
}
res_GPR = GridSearchCV(估计器=GaussianProcessRegressor(),param_grid=param_grid,cv=LeaveOneOut(),verbose=20,n_jobs=-1)
res_GPR.fit(X,y)

其中 X 和 y 分别是我的数据点和目标值。
我知道 GPR 返回的评分方法是 r^2，这对于 LOOCV 情况是无法定义的（因为只有一个测试元素） - 这是通过获取拟合模型的 .best_score_ 属性的 NaN 来验证的。
因此，我希望仅使用每个测试用例的均方根误差 (RMSE) 对模型进行评分，并对所有迭代进行平均。我怎样才能做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/60851884/implementing-gridsearchcv-with-scorer-for-leave-one-out-cross-validation</guid>
      <pubDate>Wed, 25 Mar 2020 15:22:44 GMT</pubDate>
    </item>
    <item>
      <title>如何根据组 ID 生成训练-测试-分割？</title>
      <link>https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id</link>
      <description><![CDATA[我有以下数据：
 Group_ID Item_id 目标
0 1 1 0
1 1 2 0
2 1 3 1
3 2 4 0
4 2 5 1
5 2 6 1
6 3 7 0
7 4 8 0
8 5 9 0
9 5 10 1

我需要根据“Group_ID”将数据集分成训练集和测试集。这样 80% 的数据进入训练集，20% 进入测试集。
也就是说，我需要我的训练集看起来像：
 Group_ID Item_id 目标
0 1 1 0
1 1 2 0
2 1 3 1
3 2 4 0
4 2 5 1
5 2 6 1
6 3 7 0
7 4 8 0

和测试集：
 Group_ID Item_id 目标
8 5 9 0
9 5 10 1

最简单的方法是什么？据我所知，sklearn 中的标准 test_train_split 函数不支持按组拆分，而我也可以指示拆分的大小（例如 80/20）。]]></description>
      <guid>https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id</guid>
      <pubDate>Thu, 21 Feb 2019 00:45:21 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程的平方协方差函数</title>
      <link>https://stackoverflow.com/questions/44085001/squared-covariance-function-of-gaussian-process</link>
      <description><![CDATA[这是我第一次尝试编写协方差函数。我有以下价值观，
&lt;预&gt;&lt;代码&gt;x = [-1.50 -1.0 -.75 -.40 -.25 0.00];
SF=1.27；
埃尔 = 1;
SN = 0.3;

平方指数协方差函数的公式为

我编写的 matlab 代码为：
K = sf^2*exp(-0.5*(squareform(pdist(x)).^2)/ell^2)+(sn)^2*eye(Ntr,Ntr);

其中，sf 是信号标准差，ell 是特征长度尺度，sn 是噪声标准差，Ntr code&gt; 训练输入数据的长度x。
但这没有给我任何结果。我的编码有什么错误吗？
一旦我计算出来，我想总结成矩阵形式，如下所示，

如果x_ = 0.2那么我们如何计算：
a) K_ =[k(x_,x1) k(x_,x2).........k(x_,xn)] 和 
b) K__ = k(x_,x_)
使用matlab？]]></description>
      <guid>https://stackoverflow.com/questions/44085001/squared-covariance-function-of-gaussian-process</guid>
      <pubDate>Sat, 20 May 2017 10:53:42 GMT</pubDate>
    </item>
    </channel>
</rss>