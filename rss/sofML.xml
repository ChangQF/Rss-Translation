<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Mar 2024 09:12:23 GMT</lastBuildDate>
    <item>
      <title>R 中没有为任何变量提供梯度</title>
      <link>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</link>
      <description><![CDATA[我正在使用 R 和 MNIST 数据集进行深度学习。
我编写了这段代码来存储训练和测试数据，并定义和拟合模型：
库（keras）

#获取数据
mnist &lt;- dataset_mnist()
train_data &lt;- mnist$train$x
train_labels &lt;- mnist$train$y
test_data &lt;- mnist$test$x
test_labels &lt;- mnist$test$y

#重塑&amp;正常化
train_data &lt;- array_reshape(train_data,c(nrow(train_data), 784))
训练数据 &lt;- 训练数据 / 255
test_data &lt;- array_reshape(test_data,c(nrow(test_data), 784))
测试数据 &lt;- 测试数据 / 255

#一个热编码 train_labels &lt;- to_categorical(train_labels, 10)
测试标签 &lt;- to_categorical(测试标签, 10)

＃模型
模型 &lt;- keras_model_sequential()
模型％&gt;％layer_dense（单位= 128，激活=“relu”）％&gt;％
            Layer_dropout(率=0.3) %&gt;%
            Layer_dense(单位=64，激活=“relu”)%&gt;%
            Layer_dropout(率=0.2) %&gt;%
            Layer_dense（单位= 10，激活=“softmax”）

#编译
模型%&gt;%编译(loss=“categorical_crossentropy”,
                    优化器=“rmsprop”，
                    指标=“准确性”）
    
＃火车
历史 &lt;- 模型 %&gt;% fit(train_data,
                        火车标签，
                        纪元=10，
                        批量大小=784，
                        验证分割=0.2，
                        详细=2)
    
#评估与预测
模型 %&gt;% 评估(test_data, test_labels)
pred &lt;- 模型 %&gt;% 预测(test_data)
打印（表（预测= pred，实际= test_labels））

在R studio中运行时，出现以下错误：
ValueError：没有为任何变量提供渐变：([&#39;dense_124/kernel:0&#39;, &#39;dense_124/bias:0&#39;, &#39;dense_123/kernel:0&#39;, &#39;dense_123/bias:0&#39;, &#39;密集_122 /内核：0&#39;，&#39;密集_122 /偏差：0&#39;]，）。假设 `grads_and_vars` 为 ((None, ), (None, ), (无, ), (无, ), (无, ), (无, ))。

我认为问题可能在于输入数据和输入的形状冲突，但不知道如何解决这个问题。
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</guid>
      <pubDate>Sat, 30 Mar 2024 08:30:30 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有七个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数字特征是 [8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9] （我有该图所有点的坐标对 (x, y) 以两列 CSV 文件的形式存在，我也可以使用它们）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
例如，我编写了以下代码：
X = []
y = []
目录=“数据”；
对于 os.listdir（目录）中的 csv_file：
    data = pd.read_csv(f&quot;{目录}/{csv_file}&quot;)
    X.append(data.iloc[1:, :2].astype(float).values)
    y.append(data.iloc[0, 2:].astype(float).values)
X = np.array(X, dtype=np.float64) # X.shape: (50000, 253, 2)
y = np.array(y, dtype=np.float64) # y.shape: (50000, 7)

X_train = X[:40000,:,:]
X_val = X[40000:, :, :]
y_train = y[:40000,:]
y_val = y[40000:, :]

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_val_scaled = 缩放器.fit_transform(X_val)

输入 = keras.layers.Input(shape=(X.shape[1], X.shape[2]))
lstm_out = keras.layers.LSTM(32)(输入)
输出 = keras.layers.Dense(7)(lstm_out)

模型= keras.Model（输入=输入，输出=输出）
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=“mse”)
模型.summary()

历史=模型.fit(
    x=X_train,
    y = y_train，
    纪元=10，
）

损失非常高，而且一点也不好。
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 30 Mar 2024 08:11:00 GMT</pubDate>
    </item>
    <item>
      <title>处理用于骨折检测的 Flask 应用程序中的不相关上传</title>
      <link>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</link>
      <description><![CDATA[我正在开发一个用于骨折检测的 Flask 应用程序，用户可以在其中上传 X 射线图像，该应用程序会预测是否存在骨折。但是，我在处理不相关的上传时遇到问题，例如肘部、手和肩膀以外的身体部位的图像。
以下是我的 Flask 应用程序代码的概述：
Flask 应用代码
导入操作系统
从烧瓶导入烧瓶，请求，jsonify
从 werkzeug.utils 导入 secure_filename
从预测导入预测
从flask_cors导入CORS

应用程序=烧瓶（__名称__）
CORS(app) # 为所有路由启用 CORS
app.config[&#39;UPLOAD_FOLDER&#39;] = &#39;上传&#39;
app.config[&#39;ALLOWED_EXTENSIONS&#39;] = {&#39;png&#39;, &#39;jpg&#39;, &#39;jpeg&#39;}

def allowed_file(文件名):
    返回 &#39;​​。&#39;在 filename 和 filename.rsplit(&#39;.&#39;, 1)[1].lower() 中 app.config[&#39;ALLOWED_EXTENSIONS&#39;]

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def Predict_bone_fracture():
    如果“文件”不在 request.files 中：
        return jsonify({&#39;error&#39;: &#39;没有文件部分&#39;})

    文件 = request.files[&#39;文件&#39;]

    if file.filename == &#39;&#39;:
        return jsonify({&#39;error&#39;: &#39;没有选择文件&#39;})

    如果文件和 allowed_file(file.filename):
        文件名 = secure_filename(文件.文件名)
        filepath = os.path.join(app.config[&#39;UPLOAD_FOLDER&#39;], 文件名)
        文件.保存（文件路径）

        # 使用 Predictions.py 中的预测函数执行预测
        骨骼类型结果 = 预测（文件路径）
        结果=预测（文件路径，bone_type_result）

        # 您可以根据您的要求自定义响应
        返回jsonify（{&#39;bone_type&#39;：bone_type_result，&#39;结果&#39;：结果}）

    return jsonify({&#39;error&#39;: &#39;文件格式无效&#39;})

如果 __name__ == “__main__”：
    应用程序.run()


predict_bone_fracture() 函数接收上传的图像，将其保存到指定文件夹，然后使用外部模块 (predictions.py) 中的 Predict() 函数执行预测。如果上传的文件不是图像或格式不受支持，则返回错误响应。
我主要关心的是如何处理用户上传与指定身体部位（即肘部、手部、肩膀）不对应的图像的情况。例如，如果用户上传眼睛图像而不是骨骼图像，则应用程序应拒绝上传并提供适当的错误消息。
我相信我需要采用一种机制来检测上传图像中的相关身体部位，并验证它们是否与预期的预测身体部位（即肘部、手部、肩膀）相匹配。但是，我不确定实现此目的的最佳方法。
您能否提供有关如何解决此问题的建议或想法？具体来说，我正在寻找以下方面的指导：
实施一种机制来检测上传图像中的相关身体部位。
检查检测到的身体部位是否与预期的预测身体部位相匹配。
为不相关的上传提供适当的错误处理和消息。
任何见解或代码示例将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</guid>
      <pubDate>Sat, 30 Mar 2024 05:41:52 GMT</pubDate>
    </item>
    <item>
      <title>在进行二值图像分类时，设置为二值的类模式错误地标记了图像，但它在分类上是否正确</title>
      <link>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</link>
      <description><![CDATA[我目前正在研究二值图像分类。我的问题是，当我使用数据增强时，当它设置为二进制时，它会错误地标记图像。
我尝试过的事情：

在扩充我的数据之前，在图像上查找错误标记的类。不过这并没有什么问题。

将课程模式更改为分类模式。这是可行的，但是我不从事多类分类。

尝试使用图像数据生成器。没有任何效果。

寻找分类不平衡的情况。也没什么问题。


我还可以尝试什么？我应该在生成器上使用分类类模式吗？]]></description>
      <guid>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</guid>
      <pubDate>Sat, 30 Mar 2024 03:44:32 GMT</pubDate>
    </item>
    <item>
      <title>计算explained_variance_score，手动方法和函数调用结果不同</title>
      <link>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</link>
      <description><![CDATA[根据官方页面的公式
https://scikit-learn.org/stable/modules/ model_evaluation.html#explained-variance-score，计算数据集的以下 EVS：
y_true = [1, 2, 3, 4, 5] y_pred = [6, 7, 8, 9, 10]
手动：evs = 1 - var(y_true - y_pred)/var(y_true) = -11.5
使用代码：evs = 1
从 sklearn.metrics 导入解释_方差_分数

y_true = [1, 2, 3, 4, 5]
y_pred = [6, 7, 8, 9, 10]

解释的方差 = 解释的方差_分数(y_true, y_pred)

为什么结果不同？]]></description>
      <guid>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</guid>
      <pubDate>Sat, 30 Mar 2024 03:26:49 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn 排列和更新 Polars DataFrame</title>
      <link>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</link>
      <description><![CDATA[我正在尝试重写scikit-learn 排列重要性要实现的：

与 Polar 的兼容性
与功能集群的兼容性

将极坐标导入为 pl
将 Polars.selectors 导入为 cs
将 numpy 导入为 np

从 sklearn.datasets 导入 make_classification
从 sklearn.model_selection 导入 train_test_split

X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
feature_names = [f&quot;feature_{i}&quot;;对于范围内的 i(X.shape[1])]

X_train_polars = pl.DataFrame(X_train, schema=feature_names)
X_test_polars = pl.DataFrame(X_test, schema=feature_names)
y_train_polars = pl.Series(y_train, schema=[“目标”])
y_test_polars = pl.Series(y_test, schema=[“目标”])

为了获得一组特征的未来重要性，我们需要同时排列一组特征，然后传递给评分器以与基线分数进行比较。
但是，在检查特征簇时，我正在努力替换多个极坐标数据框列：
from sklearn.utils import check_random_state
随机状态=检查随机状态(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

X_train_permuted = X_train_polars.clone()
shuffle_arr = np.array(X_train_permuted[:, [“feature_0”, “feature_1”]])

random_state.shuffle(shuffle_arr)
X_train_permuted.replace_column( # 这个操作到位
                0,
                pl.Series(name=“feature_0”,values=shuffle_arr))

通常，shuffle_arr 的形状为 (n_samples,)，可以使用 polars.DataFrame.replace_column() 轻松替换 Polars 数据框中的相关列。在这种情况下，shuffle_arr 的多维形状为（簇中的 n_samples，n_features）。替换相关列的有效方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</guid>
      <pubDate>Sat, 30 Mar 2024 03:20:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络距离与性能之间的关系</title>
      <link>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</link>
      <description><![CDATA[我一直想知道“距离”与“距离”之间是否存在相关性。神经网络权重及其性能之间的关系。
为了详细说明，请考虑以下场景：
我们有三个模型：M1、M2 和 M3，它们都具有相同的结构，每个模型都在其各自的数据集 D1、D2 和 D3 上进行训练。训练后，让我们说一下“距离”。 M1 和 M2 之间的距离是 5，而 M1 和 M3 之间的距离是 20。本质上，M1 和 M2 “空间上”更接近。
我想说的是，如果我们在 D2 和 D3 上评估 M1，它在 D2 上的性能应该更高，因为 M1 更接近 M2，并且 M2 是在 D2 上训练的。然而，一些实验与这一假设相矛盾。
我故意将“距离”括起来用引号引起来，因为我不确定在这种情况下采用的适当指标。
我找到了一些关于该主题的论文，但它们似乎不能满足我的需求。
谁能帮助我更好地理解“距离”和“距离”之间是否存在关系？和性能？
非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</guid>
      <pubDate>Sat, 30 Mar 2024 01:27:55 GMT</pubDate>
    </item>
    <item>
      <title>jupyter笔记本中的tensorflow导入错误（ import tensorflow_io as tfio ）[关闭]</title>
      <link>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</link>
      <description><![CDATA[导入tensorflow_io as tfio 我在音频分类器深度学习项目的代码中遇到错误，有人可以帮助我吗？
来自nicholes renotte yt频道的项目，jupyter笔记本中的tensor_io代码存在问题。]]></description>
      <guid>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</guid>
      <pubDate>Fri, 29 Mar 2024 22:24:20 GMT</pubDate>
    </item>
    <item>
      <title>2类的组合</title>
      <link>https://stackoverflow.com/questions/78246119/combination-of-2-classes</link>
      <description><![CDATA[创建单独的数据生成器以进行训练和验证
train_data = data_generator.flow_from_directory(
火车路径，
目标大小=(img_size,img_size),
批量大小=批量大小_训练，
class_mode=&#39;分类&#39;,
类=[&#39;AKIEC et BCC&#39;,&#39;VASC et DF&#39;,&#39;MEL &amp; NV&amp; BKL&#39;]
）
找到属于 3 个类别的 0 张图片。]]></description>
      <guid>https://stackoverflow.com/questions/78246119/combination-of-2-classes</guid>
      <pubDate>Fri, 29 Mar 2024 22:04:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在Tensorflow中转换为对数梅尔谱图？</title>
      <link>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</link>
      <description><![CDATA[我尝试修改我的预处理函数来创建 log-mel-spectrogram，以便我的 CNN 模型可以在其上进行训练。
我尝试将我的频谱图转换为对数梅尔频谱图，但我无法做到这一点。但是，由于我使用的是tensorflow，所以这个转换过程需要使用tensorflow框架。
def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    
    返回频谱图、标签
]]></description>
      <guid>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 21:10:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中模拟 Microsoft Excel 的求解器功能（GRG 非线性）？</title>
      <link>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</link>
      <description><![CDATA[演示 Excel 求解器使用的屏幕截图：

我的任务是自动化某个 Excel 工作表。该工作表恰好使用名为 Solver 的 Excel 插件实现了逻辑。它使用单元格 $O$9 中的单个值 (-1.95624)（这是图中用红色和蓝色墨水突出显示的计算结果）作为输入值，然后使用名为的算法返回 C、B1 和 B2 的三个值“GRG非线性回归”。我的任务是用 Python 模拟这个逻辑。以下是我的尝试。主要问题是我没有得到与 Excel 的 Solver 插件计算出的 C、B1 和 B2 相同的值。
导入 numpy、scipy、matplotlib
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 scipy.optimize 导入 curve_fit
从 scipy.optimize 导入 Differential_evolution
进口警告

xData = numpy.array([-2.59772914040242,-2.28665528866907,-2.29176070881848,-2.31163972446061,-2.28369414349715,-2.27911303233721,-2.282 22332344644,-2.39089535619106,-2.32144325648778,-2.17235002006179,-2.22906032068685,-2.42044014499938,-2.71639505549322,-2.65 462061336346,- 2.47330475191616,-2.33132910807216,-2.33025978869114,-2.61175064230516,-2.92916553244925,-2.987503044973,-3.00367414706232, -1.45507812104723]) # 使用与参数相同的表名
yData = numpy.array([0.0692847120775066,0.0922342111029099,0.0918076382491768,0.0901635409944003,0.0924824386284127,0.092867647175396, 0.092605957740688,20.0838696111204451,0.0893625419994501,0.102261091024881,0.097171046758256,70.0816272542472914,0.0620128251 290935,0.0657047909578125,0.0777509345715382,0.088561321341585,0.088647672874835,90.0683859871424735,0.0507304952495273,0.047 9936476914665,0.0472601632188253,0.18922126828463 ]) # 使用与参数相同的表名

def func(x, a, b, Offset): # 带偏移量的 Sigmoid A 来自 zunzun.com
    返回 1.0 / (1.0 + numpy.exp(-a * (x-b))) + 偏移量


# 遗传算法最小化（误差平方和）的函数
def sumOfSquaredError(parameterTuple):
    warnings.filterwarnings(“ignore”) # 不通过遗传算法打印警告
    val = func(xData, *parameterTuple)
    返回 numpy.sum((yData - val) ** 2.0)


defgenerate_Initial_Parameters():
    # 用于边界的最小值和最大值
    maxX = max(x数据)
    minX = min(x数据)
    maxY = max(y数据)
    minY = min(yData)

    参数范围 = []
    parameterBounds.append([minX, maxX]) # 的搜索范围
    parameterBounds.append([minX, maxX]) # b 的搜索范围
    parameterBounds.append([0.0, maxY]) # Offset 的搜索范围

    #“种子”用于可重复结果的 numpy 随机数生成器
    结果 = Differential_evolution(sumOfSquaredError,parameterBounds,seed=3)
    返回结果.x

# 生成初始参数值
遗传参数=generate_Initial_Parameters()

# 曲线拟合测试数据
参数，协方差 = curve_fit（func，xData，yData，遗传参数，maxfev = 50000）

# 将参数转换为Python内置类型
params = [float(param) for param in params] # 将 numpy float64 转换为 Python float
C、B1、B2 = 参数
OutputDataSet = pd.DataFrame({“C”：[C]，“B1”：[B1]，“B2”：[B2]，“ProType”：[input_value_1]，“RegType”：[input_value_2 ]})


有什么想法会有帮助吗？提前致谢
这是我的尝试：
鉴于 xData 和 yData 的这些数据集，正确的输出应该是：
C= -2.35443383，B1 = -14.70820051，B2 = 0.0056217]]></description>
      <guid>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</guid>
      <pubDate>Fri, 29 Mar 2024 15:08:18 GMT</pubDate>
    </item>
    <item>
      <title>运行排列重要性时，我们是否会排列测试集中的列？</title>
      <link>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</link>
      <description><![CDATA[我一直在查看有关排列重要性的文档和相关教程，但似乎没有人清楚地了解他们实际排列的内容。
为了澄清，分步过程如下：

将数据集拆分为 X_train、X_val 和 X_test

在 X_train 上训练数据，使用 X_val 例如找到最佳纪元

在 X_test 上运行经过训练的模型，记下我们正在测量的指标

排列 X_test 中的特征，并在此排列后的 X_test 数据集上运行相同的模型

记下相同的指标并比较两者

对每个变量重复此操作，而不更改模型。


旁白问题：是否值得重复运行此排列过程，其中 X_train、X_val 和 X_test 随着每次重复而变化。我知道最终的模型会有所不同，但我想广泛了解通用模型（具有固定超参数）在不同数据集上训练时的表现，因为保持 X_test 固定可能会扭曲某些特征的感知重要性。]]></description>
      <guid>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</guid>
      <pubDate>Fri, 29 Mar 2024 13:25:13 GMT</pubDate>
    </item>
    <item>
      <title>用于确定 TRL（技术准备水平）的问答模型</title>
      <link>https://stackoverflow.com/questions/78243266/question-answering-model-for-determine-trltechnology-readiness-levels</link>
      <description><![CDATA[我们想要创建一个问答模型。如何创建像 chatgpt 这样的问答模型，根据对该模型提供的文章、专利或任何产品描述来确定技术准备程度（从 1 到 9）？
我们考虑使用 robeta 模型作为现成的模型，但是我们如何根据来自 API 的文章格式（标题、描述、作者等）集成掩蔽、微调和预训练阶段到这种格式并从这种格式中获取信息并确定文章在一定水平上的技术就绪程度？
我考虑过给每个部分，即标题部分中决定成熟度级别的单词赋予系数2，为描述部分中表示成熟度级别的单词赋予系数1.5，以及然后给潜文本中显示成熟度的单词赋予系数1，但是模型将如何在该部分学习这一点呢？我被困住了。如果每次冻结不同的结果，它就不会一致。您能帮我详细解释一下如何完成这些阶段吗？]]></description>
      <guid>https://stackoverflow.com/questions/78243266/question-answering-model-for-determine-trltechnology-readiness-levels</guid>
      <pubDate>Fri, 29 Mar 2024 10:34:46 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks AutoML 运行的最佳试用笔记本的“加载数据”部分中访问 df_loaded 和/或 run_id</title>
      <link>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</link>
      <description><![CDATA[下面的代码块是通过执行 Databricks AutoML 运行自动生成的最佳试用笔记本的一部分。
导入mlflow
导入操作系统
导入uuid
进口舒蒂尔
将 pandas 导入为 pd

# 创建临时目录以从 MLflow 下载输入数据
input_temp_dir = os.path.join(os.environ[&quot;SPARK_LOCAL_DIRS&quot;], &quot;tmp&quot;, str(uuid.uuid4())[:8])
os.makedirs(input_temp_dir)


# 下载工件并将其读入 pandas DataFrame
input_data_path = mlflow.artifacts.download_artifacts（run_id =“e2a4a93aafb24aa9956e83f6b7ab3e28”，artifact_path =“数据”，dst_path = input_temp_dir）

df_loaded = pd.read_parquet(os.path.join(input_data_path, “training_data”))
# 删除临时数据
Shutil.rmtree(input_temp_dir)

# 预览数据
df_loaded.head(5)


上面代码块中的 run_id e2a4a93aafb24aa9956e83f6b7ab3e28，我可以从运行 automl.regress 返回的 AutoMLSummary 中获取它吗？如果我使用summary.best_trial.mlflow_run_id，我会得到不同的值。那么这个 run_id 是什么以及如何获取它？

除了上面的代码块之外，还有没有办法获取已加载到 df_loaded 中的数据集？它本质上是我输入到 automl.regress 中的输入数据集，只不过它有一列指示每一行是否是训练、验证和测试子集的一部分。


我对 Databricks AutoML 相当陌生，因此不确定完成此任务的最佳方法是什么。
提前致谢。
正如我所提到的，我尝试从summary.best_trial.mlflow_run_id 中获取run_id，但值不匹配。我尝试阅读 automl 和 mlflow 的文档，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</guid>
      <pubDate>Thu, 28 Mar 2024 23:32:11 GMT</pubDate>
    </item>
    <item>
      <title>将 Detectron2 模型转换为 torchscript</title>
      <link>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</link>
      <description><![CDATA[我想将 detectorron2 &#39;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml 模型&#39; 转换为 torchscript。
我用过托克
我的代码如下。
&lt;前&gt;&lt;代码&gt;导入cv2

将 numpy 导入为 np

进口火炬
从 detector2 导入 model_zoo
从 detector2.config 导入 get_cfg
从 detectorron2.engine 导入 DefaultPredictor
从 detector2.modeling 导入 build_model
从 detectorron2.export.flatten 导入 TracingAdapter
导入操作系统

ModelPath=&#39;/home/jayasanka/working_files/create_torchsript/model.pt&#39;
将 open(&#39;savepic.npy&#39;, &#39;rb&#39;) 作为 f：
    图像 = np.load(f)

#------------------------------------------------- ------------------------------------------------

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(“COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml”))

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # 你的类数 + 1

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, ModelPath)

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.60 # 设置该模型的测试阈值

预测器 = DefaultPredictor(cfg)



我使用了 TracingAdapter 和跟踪函数。我不太了解其背后的概念是什么。
&lt;前&gt;&lt;代码&gt;# im = cv2.imread(图像)
im = torch.tensor(图像)

def inference_func（模型，图像）：
    输入= [{“图像”：图像}]
    返回 model.inference(inputs, do_postprocess=False)[0]

包装器= TracingAdapter（预测器，im，inference_func）
包装器.eval()
Traced_script_module= torch.jit.trace（包装器，（im，））
traced_script_module.save(“torchscript.pt”)

它给出了下面给出的错误。
回溯（最近一次调用最后一次）：
  文件“script.py”，第 49 行，位于  中。
    Traced_script_module= torch.jit.trace（包装器，（im，））
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 744 行，跟踪中
    _模块_类，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 959 行，在trace_module 中
    参数名称，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1051 行，在 _call_impl 中
    返回forward_call（*输入，**kwargs）
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1039 行，位于 _slow_forward
    结果 = self.forward(*输入, **kwargs)
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/detectron2/export/flatten.py”，第 294 行，向前
    输出 = self.inference_func(self.model, *inputs_orig_format)
  文件“script.py”，第 44 行，inference_func
    返回 model.inference(inputs, do_postprocess=False)[0]
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/yacs/config.py”，第 141 行，在 __getattr__ 中
    引发属性错误（名称）
属性错误：推理


你能帮我解决这个问题吗？
还有其他方法可以轻松做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</guid>
      <pubDate>Tue, 06 Sep 2022 08:50:15 GMT</pubDate>
    </item>
    </channel>
</rss>