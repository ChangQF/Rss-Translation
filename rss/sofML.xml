<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Jan 2024 15:13:17 GMT</lastBuildDate>
    <item>
      <title>我可以使用 google colab pro 作为人工智能服务器吗？</title>
      <link>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</link>
      <description><![CDATA[我正在尝试编写一个程序，该程序从用户那里获取文本，并在文本中找到食物的名称并将其提供给用户。现在我想知道我可以使用google colab pro作为人工智能服务器吗？
从用户那里获取文本并向其发送答案的最佳方式是什么？
快速 API 好吗？
我可以在 google colab 中运行我的小 llama 应用程序，但我需要在服务器上运行我的应用程序，以便我可以随时使用它。
这是小美洲驼代码：
从变压器导入 AutoTokenizer、FlaxLlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“afmck/testing-llama-tiny”)
模型 = FlaxLlamaForCausalLM.from_pretrained(“afmck/testing-llama-tiny”)

input = tokenizer(“你好，我的狗很可爱”，return_tensors =“np”)
输出=模型（**输入）

# 检索下一个令牌的日志
next_token_logits = 输出.logits[:, -1]
]]></description>
      <guid>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</guid>
      <pubDate>Sat, 20 Jan 2024 13:07:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 TreeExplainer 绘制瀑布图</title>
      <link>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</link>
      <description><![CDATA[在 SHAP 中使用 TreeExplainer，我无法绘制瀑布图。
错误消息：
&lt;块引用&gt;
---&gt; 17 shap.plots.waterfall(shap_values[0], max_display=14) TypeError: 瀑布图需要一个 Explanation 对象作为
shap_values 参数。

由于我的模型是基于树的，因此我使用 TreeExplainer（因为使用 xgb.XGBClassifier）。
如果我使用Explainer而不是TreeExplainer，我可以绘制瀑布图。
我的代码如下：
导入 pandas 作为 pd

数据 = {
    &#39;a&#39;: [1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8, 1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8],
    &#39;b&#39;: [2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10, 2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10],
    &#39;c&#39;: [1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1, 1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1],
    &#39;d&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],
    &#39;e&#39;: [1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1, 1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1],
    &#39;f&#39;: [1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1],
    &#39;g&#39;: [3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2],
    &#39;h&#39;: [1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5],
    &#39;我&#39;: [1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6],
    &#39;j&#39;: [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6],
    &#39;k&#39;: [3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1],
    &#39;r&#39;: [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(数据)

X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
y = df.iloc[:,11]

从 sklearn.model_selection 导入 train_test_split，GridSearchCV
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.30、random_state = 42)

将 xgboost 导入为 xgb
从sklearn.metrics导入accuracy_score，confusion_matrix，classification_report
从 sklearn.model_selection 导入 GridSearchCV

参数网格 = {
    &#39;最大深度&#39;：[6]，
    “n_估计器”：[500]，
    “学习率”：[0.3]
}


grid_search_xgboost = GridSearchCV(
    估计器 = xgb.XGBClassifier(),
    参数网格=参数网格，
    CV = 3,
    详细 = 2,
    职位数 = -1
）

grid_search_xgboost.fit(X_train, y_train)

print(&quot;最佳参数：&quot;, grid_search_xgboost.best_params_)
best_model_xgboost = grid_search_xgboost.best_estimator_

导入形状

解释器 = shap.TreeExplainer(best_model_xgboost)
shap_values = 解释器.shap_values(X_train)

shap.summary_plot（shap_values，X_train，plot_type =“条”）

shap.summary_plot(shap_values, X_train)

X_train.columns 中的名称：
    shap.dependence_plot(名称, shap_values, X_train)

shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0], matplotlib=True)

shap.decision_plot(explainer.expected_value, shap_values[:10], X_train.iloc[:10])

shap.plots.waterfall(shap_values[0], max_display=14)

问题出在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</guid>
      <pubDate>Sat, 20 Jan 2024 12:55:45 GMT</pubDate>
    </item>
    <item>
      <title>当机器学习回归的许多预测结果几乎相同时该怎么办？</title>
      <link>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-results-in-almost-the-same-number-fo</link>
      <description><![CDATA[我是机器学习新手
我有 200 行数据集 14 个每日传感器读数（第 1 天到第 14 天），因为这是一种罕见的现象，所以我只得到 200 个数据集行。
目标范围（来自数据集列）为 100 至 2500 和 600 标准差，呈正态分布。
回归结果总是接近 1000（这是目标数据的平均值/平均值和中位数）。
我已经尝试过：

（再次）检查并清理数据
使用 gridsearch 寻找最佳模型参数
尝试 3 种不同的模型（AdaBoost、梯度提升、随机森林）
回归结果仍然接近平均值/平均值&amp;目标数据集的中值。
]]></description>
      <guid>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-results-in-almost-the-same-number-fo</guid>
      <pubDate>Sat, 20 Jan 2024 11:59:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 训练文本生成模型时出现 MemoryError</title>
      <link>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</link>
      <description><![CDATA[我在尝试使用 TensorFlow 训练文本生成模型时遇到内存错误。该错误发生在 model.fit 阶段，特别是在尝试创建形状 (401233, 12512) 和数据类型 int32 的数组时。系统无法分配所需的 18.7 GiB 内存。
file_path = &#39;story.txt&#39;
最大读取字节数 = 2 * 1024 * 1024 # 5 MB
​
打开（file_path，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为文件：
    数据 = file.read(max_bytes_to_read)
分词器 = 分词器()
​
语料库 = data.lower().split(“\n”)
​
tokenizer.fit_on_texts（语料库）
​
总单词数 = len(tokenizer.word_index) + 1
​
输入序列 = []
​
对于语料库中的行：
  token_list = tokenizer.texts_to_sequences([行])[0]
  对于范围内的 i(1, len (token_list))：
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)
​
max_seq_len = max([len(i) for i in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences,maxlen= max_seq_len,padding=“pre”))
​
xs = 输入序列[:,:-1]
标签 = input_sequences[:,-1]
fromtensorflow.keras.utils import to_categorical # 专门针对 to_categorical 函数
ys = tf.keras.utils.to_categorical(标签, num_classes=total_words, dtype=&#39;int32&#39;)



模型=顺序（）
model.add(嵌入(total_words, 240, input_length=max_seq_len-1))
model.add(双向(LSTM(150)))
model.add（密集（total_words，激活=&#39;softmax&#39;））
​
​
model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit(xs, ys, epochs=10)
​

这是在 jupyter 笔记本中运行 model.fit 代码单元时出现的错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
MemoryError Traceback（最近一次调用）
[32] 中的单元格，第 8 行
      4 model.add（密集（total_words，激活=&#39;softmax&#39;））
      7 model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
----&gt; 8 model.fit(xs, ys, epochs=10)

文件 ~\anaconda3\lib\site-packages\keras\src\utils\traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件~\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py:86，在convert_to_eager_tensor(value, ctx, dtype)中
     66 &quot;&quot;&quot;将给定的“value”转换为“EagerTensor”。
     67
     68 请注意，此函数可以返回已创建常量的缓存副本
   （...）
     80 类型错误：如果 `dtype` 与 t 的类型不兼容。
     第81章
     82 if isinstance(值, np.ndarray):
     83 # 显式创建一个副本，因为 EagerTensor 可能共享底层
     84 # 内存与输入数组。如果没有此副本，用户将能够
     85 # 在创建后通过更改输入数组来修改 EagerTensor。
---&gt; 86 值 = value.copy()
     87 if isinstance(value, ops.EagerTensor):
     88 如果 dtype 不是 None 并且 value.dtype != dtype:

MemoryError：无法为形状为 (401233, 12512) 和数据类型为 int32 的数组分配 18.7 GiB
]]></description>
      <guid>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</guid>
      <pubDate>Sat, 20 Jan 2024 10:00:47 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归和决策树分类器之间的区别</title>
      <link>https://stackoverflow.com/questions/77850372/difference-between-decision-tree-regression-and-decision-tree-classifier-properl</link>
      <description><![CDATA[当输出变量是分类变量时使用分类树，而当输出变量是连续变量时使用回归树。我知道这是正确的，但我对这个主题还没有清楚的了解，如果您帮助我举例，请帮助我更好地获得最佳答案。]]></description>
      <guid>https://stackoverflow.com/questions/77850372/difference-between-decision-tree-regression-and-decision-tree-classifier-properl</guid>
      <pubDate>Sat, 20 Jan 2024 09:00:05 GMT</pubDate>
    </item>
    <item>
      <title>在拥抱脸上部署机器学习模型</title>
      <link>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</link>
      <description><![CDATA[任何人都可以帮助我部署在拥抱脸上使用 Streamlit 构建的机器学习模型吗？
我上传了app.py文件、.hdf5模型和requirements.txt文件。我收到构建错误。总是显示找不到 TensorFlow 模块。我包含了我在项目中使用的所有包。
所以我希望有人指导我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</guid>
      <pubDate>Sat, 20 Jan 2024 07:20:44 GMT</pubDate>
    </item>
    <item>
      <title>对于特征工程师来说，有哪些好主意可以为分类模型创建与目标特征的更多相关性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</link>
      <description><![CDATA[我正在开发一个分类模型，可以对客户进行分类，判断他们是否会成功付款或未能付款。
我尝试了 Scikit Learn 分类模型，但它的准确率略高于 50%。然后，我研究了 TensorFlow 分类模型，并获得了 67% 的准确率分数。
我正在努力提高准确率，希望模型的准确率能够达到 90% 以上。
我注意到的主要问题是没有一个特征与目标特征具有高相关性得分。
这是我尝试过的：
&lt;前&gt;&lt;代码&gt;df5.corr()

结果是

我尝试了多个 TensorFlow 模型，最高准确度得分为 67%
tf.random.set_seed(42)

log_model_8 = tf.keras.Sequential([
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

log_model_8.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                   优化器= tf.keras.optimizers.Adam(),
                   指标 = [&#39;准确性&#39;])

log_model_8.fit(X,y, epochs = 100, verbose = 1)

log_model_8.evaluate(X,y)


在这种情况下有什么好主意，如何才能提高模型准确率达到 90% 以上？]]></description>
      <guid>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</guid>
      <pubDate>Fri, 19 Jan 2024 21:03:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将字符串转换为浮点数，dtype='numeric' 与字节/字符串数组不兼容。将数据显式转换为数值</title>
      <link>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从sklearn导入预处理

cols = [&#39;国家&#39;, &#39;人口&#39;, &#39;中位数年龄&#39;]
col_types = {&#39;国家&#39;：str，&#39;人口&#39;：int，&#39;median_age&#39;：int}
数据库 = pd.read_csv(&#39;dataset.csv&#39;, dtype=col_types)

le = 预处理.LabelEncoder()
数据库[&#39;国家&#39;] = le.fit_transform(数据库[&#39;国家&#39;])

X = (数据库[[&#39;国家&#39;]])
y = database.drop(列=[&#39;国家&#39;])

模型 = DecisionTreeClassifier()
model.fit(X.值, y.值)
prevision = model.predict([[&#39;意大利&#39;]])
打印（预览）

我正在尝试使用机器学习编写一个程序，该程序以国家名称作为唯一输入，返回年龄中位数以及该国家有多少居民。问题是拟合和预测函数需要浮点数，但我的输入是字符串，因此我尝试使用标签编码器对其进行转换，但收到此错误：
dtype=&#39;numeric&#39; 与字节/字符串数组不兼容。
明确地将数据转换为数值

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</guid>
      <pubDate>Fri, 19 Jan 2024 20:44:26 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中具有多个层的简单 RNN，用于顺序预测</title>
      <link>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</link>
      <description><![CDATA[我得到了连续的时间序列数据。在每个时间戳，只有一个变量可供观察（如果我的理解是正确的，这意味着特征数量 = 1）。我想训练一个具有多个层的简单 RNN 来预测下一个观察结果。
我使用滑动窗口创建了训练数据，窗口大小设置为8。为了给出具体的想法，下面是我的原始数据、训练数据和目标。
示例数据
0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49
训练数据
&lt;前&gt;&lt;代码&gt;X =
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37


对应的目标是
&lt;前&gt;&lt;代码&gt;Y =
     0.53
     3.37
     0.49

我将批量大小设置为 3。但它给了我一个错误
运行时错误：input.size(-1) 必须等于 input_size。期望 8，得到 1
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入 torch.utils.data 作为数据
将 numpy 导入为 np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

类 RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        超级（RNNModel，自我）.__init__()
        self.hidden_​​dim = 3*input_sz
        self.n_layers = n_layers
        输出大小 = 1
        self.rnn = nn.RNN（input_sz，self.hidden_​​dim，num_layers = n_layers，batch_first = True）
        self.线性 = nn.Linear(self.hidden_​​dim, output_sz)

    def 前向（自身，x）：
        batch_sz = x.size(0)
        hide = torch.zeros(self.n_layers, batch_sz, self.hidden_​​dim) #初始化n_layer*batch_sz维度的隐藏状态数hidden_​​dim)
        out, 隐藏 = self.rnn(x, 隐藏)
        out = out.contigious().view(-1, self.hidden_​​dim)
        返回，隐藏

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型 = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(设备)
Y = Y.to(设备)
模型 = model.to(设备)
优化器 = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

加载器= data.DataLoader（data.TensorDataset（X，Y），shuffle=False，batch_size=3）

n_epoch = 10
对于范围内的历元（n_epoch）：
    模型.train()
    对于加载器中的 X_batch、Y_batch：
        Y_pred = 模型(X_batch)
        损失 = loss_fn(Y_pred,Y_batch)
        优化器.zero_grad()
        loss.backward()
        优化器.step()

    如果纪元 % 10 != 0:
        继续
        模型.eval()
        使用 torch.no_grad()：
            Y_pred = 模型(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(“纪元 %d: 训练 RMSE %.4f” % (纪元, train_rmse))


我做错了什么？谁能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</guid>
      <pubDate>Fri, 19 Jan 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>不平衡数据的隔离森林和SHAP过程</title>
      <link>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</link>
      <description><![CDATA[我想对类别为正常 99.93% 异常 0.07% 的数据使用隔离森林，并使用 SHAP 检查异常数据特征之间的相关性。
于是，我参考了下面kaggle网站上的方法继续学习。
kaggle
在这个 Kaggle 站点上，Class = 0 的数据和 Class = 1 的数据划分如下：
inliers = df[df.Class==0]
ins = inliers.drop([&#39;Class&#39;], axis=1)

离群值 = df[df.Class==1]
outs = outliers.drop([&#39;Class&#39;], axis=1)

为了查看学习中使用的特征与异常值（“Class == 1”的数据）之间的相关性，我按如下方式使用了 SHAP，并通过蜂群图检查了相关性。
&lt;前&gt;&lt;代码&gt;状态= 42
ISF = 隔离森林（random_state=状态）
ISF.fit(ins)

normal_isf = ISF.predict(ins)
欺诈_isf = ISF.predict(outs)

导入形状
解释器 = shap.TreeExplainer(ISF)
shap_values = 解释器(outs)
shap.plots.beeswarm(shap_values)

代码工作正常，但是 beeswarn 的结果与我使用 shap_values ​​=explainer(ins) 时类似，即正常数据。我犯错了吗？如果您能告诉我是否有任何需要改进的地方，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</guid>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：如何使用带有 scikit-learn 接口 .fit 的 DMatrix</title>
      <link>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</link>
      <description><![CDATA[我目前在我的项目中使用 XGBoost 的 scikit-learn 接口。但是，我有一个非常大的数据集，每次调用 .fit 时，数据都会转换为 DMatrix，这非常耗时，尤其是在使用训练相对较快的 GPU 时。我使用本机接口对每次拟合使用单个 DMatrix 进行基准测试，结果显示出显着差异（每次拟合 14 秒与每次拟合 0.9 秒）。问题是，我需要一个 scikit-learn 模型，以便它可以与我的程序的其余部分配合使用。
有没有办法在 XGBoost 中将 DMatrix 与 scikit-learn 接口结合使用，或者有任何解决方法来避免重复转换为 DMatrix，同时仍保持与 scikit-learn 的兼容性？
请参阅下面的代码以获取导致此问题的可重现方法。
from sklearn.datasets import make_classification
从 xgboost 导入 XGBClassifier
将 xgboost 导入为 xgb

# 大型综合数据集
X, y = make_classification(n_samples=500_0000, n_features=20,
                           n_informative=10，n_redundant=10，random_state=42）

# scikit 学习
t = 时间.time()
模型 = XGBClassifier(tree_method=“gpu_hist”, gpu_id=0,
                      预测器=“gpu_predictor”，max_bin=256）
模型.fit(X, y)
print(&quot;scikit-learn 接口：&quot;, time.time() - t)

# 再次进行 scikit-learn
t = 时间.time()
模型.fit(X, y)
print(&quot;scikit-learn (2nd) 接口：&quot;, time.time() - t)

打印（）

#DMatrix
dtrain = xgb.DMatrix(数据=X,标签=y)
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(&quot;本机接口：&quot;, time.time() - t)

#再次DMatrix
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(“本机（第二）接口::”, time.time() - t)

输出：
scikit-learn 接口：14.393212795257568
scikit-learn（第二）接口：14.048950433731079

本机接口：3.9494242668151855
本机（第二）接口:: 0.9888997077941895

如您所见，scikit-learn 和 Native 之间存在很大的时间差异。]]></description>
      <guid>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</guid>
      <pubDate>Sun, 18 Jun 2023 19:56:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 object_Detector.EfficientDetLite4Spec tensorflow lite 继续使用检查点进行训练</title>
      <link>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</link>
      <description><![CDATA[很重要的是，我已经在 config.yaml 中设置了我的 EfficientDetLite4 模型“grad_checkpoint=true”。并且它已经成功生成了一些检查点。但是，当我想继续基于这些检查点进行训练时，我不知道如何使用它们。
每次我训练模型时，它都会从头开始，而不是从检查点开始。
下图是我的colab文件系统结构：

下图显示了我的检查点存储的位置：

以下代码显示了我如何配置模型以及如何使用模型进行训练。
将 numpy 导入为 np
导入操作系统

从 tflite_model_maker.config 导入 ExportFormat
从 tflite_model_maker 导入 model_spec
从 tflite_model_maker 导入 o​​bject_ detector

将张量流导入为 tf
断言 tf.__version__.startswith(&#39;2&#39;)

tf.get_logger().setLevel(&#39;错误&#39;)
从absl导入日志记录
日志记录.set_verbosity（日志记录.错误）

训练数据、验证数据、测试数据 =
    object_Detector.DataLoader.from_csv(&#39;csv_path&#39;)

规格 = object_ detector.EfficientDetLite4Spec(
    uri=&#39;/内容/模型&#39;,
    model_dir=&#39;/content/drive/MyDrive/MathSymbolRecognition/CheckPoints/&#39;,
    hparams=&#39;grad_checkpoint=true,策略=gpus&#39;,
    epochs=50，batch_size=3，
    steps_per_execution=1， moving_average_decay=0，
    var_freeze_expr=&#39;(efficientnet|fpn_cells|resample_p6)&#39;,
    tflite_max_detections=25，策略=spec_strategy
）

model = object_ detector.create(train_data, model_spec=spec, batch_size=3,
    train_whole_model=True，validation_data=validation_data）
]]></description>
      <guid>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</guid>
      <pubDate>Tue, 05 Oct 2021 04:21:43 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验的计算</title>
      <link>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</link>
      <description><![CDATA[我试图了解如何针对以下输入计算 chi2 函数。
sklearn.feature_selection.chi2([[1, 2, 0, 0, 1],
                                [0, 0, 1, 0, 0],
                                [0, 0, 0, 2, 1]], [真, 假, 假])

对于 chi2，我得到以下结果 [2, 4, 0.5, 1, 0.25]。
我已经在维基百科上找到了以下计算公式（x_i 也称为观察值，m_i 称为预期值），但我不知道如何应用它。

我的理解是，我有三个类别的输入（行）和四个特征（列），chi2函数返回特征和类别之间是否存在相关性。第一列表示的特征在第一个类别中出现两次，并且 chi2 值为 4。
我想我已经弄清楚了

各列彼此独立，这是有道理的
如果我省略第三行，预期值将是列的总和，观察值只是相应单元格中的值，但这不适用于最后一列
带有 False 的两列似乎以某种方式组合在一起，但我还没有弄清楚如何组合。

如果有人可以帮助我，我将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</guid>
      <pubDate>Thu, 08 Oct 2020 23:27:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 cross_val_predict 与 cross_val_score 时，scikit-learn 分数不同</title>
      <link>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</link>
      <description><![CDATA[我预计这两种方法都会返回非常相似的错误，有人可以指出我的错误吗？
计算 RMSE...
rf = RandomForestRegressor(random_state=555，n_estimators=100，max_深度=8)
rf_preds = cross_val_predict(rf, train_, 目标, cv=7, n_jobs=7)
print(&quot;使用 cv preds 的 RMSE 分数：{:0.5f}&quot;.format(metrics.mean_squared_error(targets, rf_preds, squared=False)))

分数 = cross_val_score(rf, train_, 目标, cv=7, 评分=&#39;neg_root_mean_squared_error&#39;, n_jobs=7)
print(&quot;使用 cv_score 的 RMSE 分数: {:0.5f}&quot;.format(scores.mean() * -1))


使用 cv preds 的 RMSE 分数：0.01658
使用 cv_score 的 RMSE 分数：0.01073
]]></description>
      <guid>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</guid>
      <pubDate>Thu, 04 Jun 2020 18:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>