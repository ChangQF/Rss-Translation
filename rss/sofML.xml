<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 11 Jun 2024 15:15:51 GMT</lastBuildDate>
    <item>
      <title>YOLOX 训练时评估问题，操作数不能与形状 (0,5) 和 (0,) 一起广播</title>
      <link>https://stackoverflow.com/questions/78608411/yolox-evaluation-problem-when-training-operands-could-not-be-broadcast-together</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78608411/yolox-evaluation-problem-when-training-operands-could-not-be-broadcast-together</guid>
      <pubDate>Tue, 11 Jun 2024 15:11:33 GMT</pubDate>
    </item>
    <item>
      <title>如何才能使用完全 MLP 架构在 CIFAR10 上获得与原始论文类似的结果？</title>
      <link>https://stackoverflow.com/questions/78608160/how-can-one-get-similar-results-on-cifar10-with-the-fully-mlp-architecture-as-th</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78608160/how-can-one-get-similar-results-on-cifar10-with-the-fully-mlp-architecture-as-th</guid>
      <pubDate>Tue, 11 Jun 2024 14:23:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 Huggingface Trainer 进行多 GPU 训练时，如何避免内存使用不均衡？</title>
      <link>https://stackoverflow.com/questions/78608004/how-can-i-avoid-unbalanced-memory-usage-when-performing-multi-gpu-training-using</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78608004/how-can-i-avoid-unbalanced-memory-usage-when-performing-multi-gpu-training-using</guid>
      <pubDate>Tue, 11 Jun 2024 13:55:30 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 后台记录器</title>
      <link>https://stackoverflow.com/questions/78607472/tensorflow-under-the-hood-logger</link>
      <description><![CDATA[我必须根据项目日志记录样式（colorlogs、格式化程序等）配置 TensorFlow 日志。项目日志记录配置在 .yaml 文件中描述，然后传递给 logging.dictConfig() 方法。可以通过调用 tf.get_logger() 检索的 TensorFlow 记录器“tensorflow”遵循 logging 配置设置（由执行回调时调用的 WARNING 消息证明）。但是，底层 TensorFlow 记录器并未使用此方法进行配置。我所说的“幕后”是指在导入阶段调用的记录器（或其他日志记录实例），它会记录有用的统计数据：
2024-06-11 13:49:34.476834：I tensorflow/core/platform/cpu_feature_guard.cc:193] 此 TensorFlow 二进制文件使用 oneAPI 深度神经网络库 (oneDNN) 进行了优化，以便在性能关键型操作中使用以下 CPU 指令：AVX AVX2
要在其他操作中启用它们，请使用适当的编译器标志重建 TensorFlow。
2024-06-11 13:49:34.625877：W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] 覆盖 orig_value 设置，因为设置了 TF_FORCE_GPU_ALLOW_GROWTH 环境变量。原始配置值为 0。
2024-06-11 13:49:34.625992：I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] 创建了具有 5563 MB 内存的设备 /device:GPU:0：-&gt; 设备：0，名称：NVIDIA GeForce RTX 4060，pci 总线 ID：0000:01:00.0，计算能力：8.9


通过将 os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] 设置为高级别来在后台更改 TensorFlow 日志记录不是一个好的解决方案。我甚至尝试在导入 TensorFlow 之前和之后获取所有记录器并手动配置新添加的记录器（将它们添加到 .yaml 文件中），但这没有帮助（而且这完全没有必要，因为所有记录器都从具有定义样式的根记录器继承）。我猜想日志记录样式是在编译的 tf 模块中定义的（env 变量指的是 CPP）。您对如何在后台配置 TensorFlow 记录器有什么想法吗？（TensorFlow 版本 2.10，Python 3.10）]]></description>
      <guid>https://stackoverflow.com/questions/78607472/tensorflow-under-the-hood-logger</guid>
      <pubDate>Tue, 11 Jun 2024 12:24:46 GMT</pubDate>
    </item>
    <item>
      <title>在 MNIST 数据集和 Kaggle 数据集上训练同一模型时，MNIST 的准确率 >90，而 Kaggle 的准确率 <15</title>
      <link>https://stackoverflow.com/questions/78607458/when-training-the-same-model-on-an-mnist-dataset-and-a-kaggle-dataset-the-accur</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78607458/when-training-the-same-model-on-an-mnist-dataset-and-a-kaggle-dataset-the-accur</guid>
      <pubDate>Tue, 11 Jun 2024 12:22:05 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的模型没有做出正确的预测？</title>
      <link>https://stackoverflow.com/questions/78606937/why-is-my-model-not-making-good-predictions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78606937/why-is-my-model-not-making-good-predictions</guid>
      <pubDate>Tue, 11 Jun 2024 10:38:40 GMT</pubDate>
    </item>
    <item>
      <title>ResumeParcer 没有给出预期的结果，我希望在从简历中提取实体时至少得到 80%</title>
      <link>https://stackoverflow.com/questions/78606930/resumeparcer-is-not-giving-result-as-expected-i-want-atleast-80-per-while-doing</link>
      <description><![CDATA[我遇到了实体提取问题。
我没有获得预期的准确度。我想要至少 80% 的准确度，但每次（在输出中）都会缺少一些实体，如姓名、电子邮件或联系人
我在 Python 中尝试了很多方法，使用 nir 模型和 huggingFace 转换器，但结果中仍然缺少一些实体。我还尝试在 1500 多份简历上训练模型。但似乎这仍然没有足够的数据。
有什么办法吗？如果有，请帮帮我。
谢谢帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78606930/resumeparcer-is-not-giving-result-as-expected-i-want-atleast-80-per-while-doing</guid>
      <pubDate>Tue, 11 Jun 2024 10:36:29 GMT</pubDate>
    </item>
    <item>
      <title>在 Pandas 中创建 DataFrame 时出现“TypeError：类型‘DataFrame’不可下标”[关闭]</title>
      <link>https://stackoverflow.com/questions/78606786/typeerror-type-dataframe-is-not-subscriptable-when-creating-a-dataframe-in</link>
      <description><![CDATA[类型错误
我尝试使用以下代码在 pandas 中创建 DataFrame：
DataFrame 应该有两列：“y_test”和“y_pred”。 “y_test”列应包含来自我的模型的测试数据，而“y_pred”列应包含来自我的模型的预测值。
但是，当我运行此代码时，我收到 TypeError：类型“DataFrame”不可下标。我不确定为什么会发生这种情况。我认为我用来创建 DataFrame 的语法是正确的。
df_pred_test = pd.DataFrame[
{
&#39;y_test&#39;: y_test,
&#39;y_pred&#39;: model.predict(X_test)
}
]
df_pred_test.head()
]]></description>
      <guid>https://stackoverflow.com/questions/78606786/typeerror-type-dataframe-is-not-subscriptable-when-creating-a-dataframe-in</guid>
      <pubDate>Tue, 11 Jun 2024 10:06:41 GMT</pubDate>
    </item>
    <item>
      <title>除了降低神经网络的复杂度之外，卷积层还有其他用处吗？ANN 能达到与 CNN 模型相同的准确率吗？</title>
      <link>https://stackoverflow.com/questions/78606784/is-there-any-use-for-convolution-layers-apart-from-reducing-the-complexity-of-th</link>
      <description><![CDATA[对于图像分类任务，为什么 CNN 被如此广泛地使用？
卷积是一个函数，因此具有密集层的足够大的 ANN 应该能够使用相同的数据完成相同的工作（如果需要，可以使用更多的训练时间）？这不就是通用近似定理的意思吗？
相同的逻辑不能应用于 RNN，因为我们实际上从上一个时间步骤传递了一些额外的数据。但对于图像分类任务，CNN 和 ANN 都在相同的数据上运行。
当我尝试 MNIST 数字分类问题并使用 2 层 ANN 达到 95% 以上的准确率时，这个问题突然出现在我的脑海中。]]></description>
      <guid>https://stackoverflow.com/questions/78606784/is-there-any-use-for-convolution-layers-apart-from-reducing-the-complexity-of-th</guid>
      <pubDate>Tue, 11 Jun 2024 10:06:30 GMT</pubDate>
    </item>
    <item>
      <title>当比较不是以相同角度拍摄的图像时，如何处理角度不匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/78606026/how-can-i-handle-angle-mismatching-when-comparing-images-that-are-not-taken-at-t</link>
      <description><![CDATA[ aligned_image1, aligned_image2 = align_images(image1, image2)

# 将对齐的图像转换为灰度
gray1 = cv2.cvtColor(aligned_image1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(aligned_image2, cv2.COLOR_BGR2GRAY)

# 计算对齐图像之间的绝对差异
difference = cv2.absdiff(gray1, gray2)

# 计算两个灰度图像之间的 SSIM
(score, diff) = ssim(gray1, gray2, full=True)
print(&quot;SSIM: {}&quot;.format(score))

# 差异图像包含两个图像之间的差异
diff = (diff * 255).astype(&quot;uint8&quot;)

# 对差异图像进行阈值处理以获取差异区域
_, thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)

# 查找差异区域的轮廓
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 在对齐的图像上绘制边界矩形以可视化差异
output1 = aligned_image1.copy()
output2 = aligned_image2.copy()
contourArea = []
for contour in contours:
contourArea.append(cv2.contourArea(contour))
# (x, y, w, h) = cv2.boundingRect(contour)
# cv2.rectangle(output1, (x, y), (x + w, y + h), (0, 0, 255), 2)
# cv2.rectangle(output2, (x, y), (x + w, y + h), (0, 0, 255), 2)
print(contourArea)
major_changes = detect_major_changes(contourArea)

print(major_changes)
# 在检测到重大变化的轮廓周围绘制矩形
for i in range(len(contours)):
if i in [change[0] for change in major_changes]:
(x, y, w, h) = cv2.boundingRect(contours[i])
cv2.rectangle(output1, (x, y), (x + w, y + h), (0, 0, 255), 2)
cv2.rectangle(output2, (x, y), (x + w, y + h), (0, 0, 255), 2)
# 创建差异图像以可视化变化
difference_image = cv2.bitwise_xor(gray1, gray2)

# 绘制图像以可视化差异
fig, axis = plt.subplots(1, 4, figsize=(25, 10))
axis[0].imshow(cv2.cvtColor(output1, cv2.COLOR_BGR2RGB))
axis[0].set_title(&quot;Aligned Image 1 with Rectangles&quot;)
axis[0].axis(&quot;off&quot;)

axis[1].imshow(cv2.cvtColor(output2, cv2.COLOR_BGR2RGB))
axis[1].set_title(&quot;Aligned Image 2 with Rectangles&quot;)
axis[1].axis(&quot;off&quot;)

axis[2].imshow(diff, cmap=&#39;gray&#39;)
axis[2].set_title(&quot;SSIM差异”）
axes[2].axis(&quot;off&quot;)

axes[3].imshow(difference_image, cmap=&#39;gray&#39;)
axes[3].set_title(&quot;差异图像&quot;)
axes[3].axis(&quot;off&quot;)

plt.show()

我正在做一个实时图像比较项目，用户可以上传两幅图像。这些图像可能不是以相同的角度或相同的视觉条件拍摄的。在比较这些图像时，我该如何处理角度不匹配的问题？
问题：我正在使用 OpenCV 和 Python 进行图像处理和比较。挑战在于准确比较角度或视觉条件不完全对齐的图像。这会引入视角和光线的差异，影响比较的准确性。
当前方法：用户上传两幅图像。实时处理和比较图像。应解决角度和视觉条件不匹配问题，以确保准确的比较结果。

在比较不同角度拍摄的图像时，如何处理角度不匹配问题？
在图像比较过程中，应考虑哪些技术或算法来解决视角和光线差异问题？
OpenCV 或 Python 中是否有特定的库或工具可以帮助处理这些挑战？

目标：我想知道如何解决实时图像比较中的角度不匹配问题，以确保准确的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78606026/how-can-i-handle-angle-mismatching-when-comparing-images-that-are-not-taken-at-t</guid>
      <pubDate>Tue, 11 Jun 2024 07:36:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：从“y”的唯一值推断出的类无效。预期：[0 1 2]，结果为 ['Dropout' 'Enrolled' 'Graduate']</title>
      <link>https://stackoverflow.com/questions/78605622/valueerror-invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2</link>
      <description><![CDATA[我目前正在使用 XGBoost 分类器模型进行分类任务。我的数据集包含分类变量，我的目标类别（“辍学”、“入学”、“毕业”）。
from xgboost import XGBClassifier

xgb = XGBClassifier(
n_estimators=200,
max_depth=6, 
learning_rate=0.1, 
subsample=0.8,
colsample_bytree=0.8,
eval_metric=&#39;mlogloss&#39; 
)

xgb.fit(X_train, y_train)

我编写了上述代码，然后它显示：“ValueError：从 y 的唯一值推断出无效的类别。预期：[0 1 2]，得到 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;]&quot;
之后，我使用标签编码器技术；它工作正常。但我需要 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;] 这个生产部分的分类。在训练 XGBClassifier 之后，我如何将这个 [0 1 2] 更改为 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;]。
提前感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78605622/valueerror-invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2</guid>
      <pubDate>Tue, 11 Jun 2024 05:56:08 GMT</pubDate>
    </item>
    <item>
      <title>从 Azure 机器学习工作室笔记本直接将数据写入 Blob 存储</title>
      <link>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</link>
      <description><![CDATA[我正在 Azure 机器学习笔记本中进行一些交互式开发，我想将一些数据直接从 pandas DataFrame 保存到我默认连接的 blob 存储帐户中的 csv 文件中。我目前正在按以下方式加载一些数据：
import pandas as pd

uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df = pd.read_csv(uri)

加载这些数据没有问题，但经过一些基本转换后，我想将这些数据保存到我的存储帐户中。我发现的大多数（如果不是全部）解决方案都建议将此文件保存到本地目录，然后将此保存的文件上传到我的存储帐户。我发现的最佳解决方案是以下解决方案，它使用 tmpfile，因此我不必事后删除任何“本地”文件：
from azureml.core import Workspace
import tempfile

ws = Workspace.from_config()
datastore = ws.datastores.get(&quot;exampleblobstore&quot;)

with tempfile.TemporaryDirectory() as tmpdir:
tmpath = f&quot;{tmpdir}/example_file.csv&quot;
df.to_csv(tmpath)
datastore.upload_files([tmpath], target_path=&quot;path/to/target.csv&quot;, overwrite=True)

这是一个合理的解决方案，但我想知道是否有任何方法可以直接写入我的存储帐户，而无需先保存文件。理想情况下，我想做一些简单的事情：
target_uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df.to_csv(target_uri)

读了一些资料后，我认为 AzureMachineLearningFileSystem 类可能允许我以类似于在本地机器上开发时的方式读取和写入数据到我的数据存储，但是，似乎这个类不允许我写入数据，只能检查“文件系统”并从中读取数据。]]></description>
      <guid>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</guid>
      <pubDate>Mon, 10 Jun 2024 16:05:39 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归代码在生成的观测值超过约 43,500 个时停止工作</title>
      <link>https://stackoverflow.com/questions/75299046/logistic-regression-code-stops-working-above-43-500-generated-observations</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75299046/logistic-regression-code-stops-working-above-43-500-generated-observations</guid>
      <pubDate>Tue, 31 Jan 2023 15:00:25 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 模型输入形状</title>
      <link>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</link>
      <description><![CDATA[我加载了一个自定义 PyTorch 模型，我想找出它的输入形状。类似这样的内容：
model.input_shape

是否可以获取此信息？

更新： print() 和 summary() 不显示此模型的输入形状，因此它们不是我要找的。]]></description>
      <guid>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</guid>
      <pubDate>Fri, 05 Mar 2021 07:59:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有灰度图像的预训练神经网络？</title>
      <link>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</link>
      <description><![CDATA[我有一个包含灰度图像的数据集，我想在这些图像上训练最先进的 CNN。我非常想微调一个预先训练好的模型（比如这里的模型）。
问题是，我能找到权重的几乎所有模型都是在包含 RGB 图像的 ImageNet 数据集上训练的。
我无法使用其中一个模型，因为它们的输入层需要一批形状为 (batch_size, height, width, 3) 或 (64, 224, 224, 3) 的模型，但我的图像批次是 (64, 224, 224)。
我有什么办法可以使用其中一个模型吗？我曾考虑在加载权重后删除输入层并添加自己的输入层（就像我们对顶层所做的那样）。这种方法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</guid>
      <pubDate>Fri, 24 Aug 2018 00:33:04 GMT</pubDate>
    </item>
    </channel>
</rss>