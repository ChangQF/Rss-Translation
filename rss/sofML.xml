<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 06 Jan 2024 15:13:01 GMT</lastBuildDate>
    <item>
      <title>在Python中调整辅助矩阵的输出大小</title>
      <link>https://stackoverflow.com/questions/77769473/adjusting-output-size-of-auxiliary-matrix-in-python</link>
      <description><![CDATA[隐藏层激活后，我想创建一个辅助矩阵，以更好地捕获下面代码片段中数据的时间方面。返回变量的当前形状为 [out_channels , out_channels] 但我希望返回的形状为 [input_channels , out_channels] 。应该修改代码的哪一部分来实现所需的输出，同时保持想法/逻辑不变？
def my_fun( self, H: torch.FloatTensor,) -&gt;;火炬.FloatTensor：
    
    self.input_channels = 4
    self.out_channels = 16
    自我遗忘因子 = 0.92
    自我羔羊 = 0.01
    self.M = torch.inverse(self.lamb*torch.eye(self.out_channels))
    HH = self.calculateHiddenLayerActivation(H) # [4,16]
    Ht = HH.t() # [16 , 4]
    ######辅助矩阵的计算
    
    初始产品 = torch.mm((1 / self.forgettingFactor) * self.M, Ht) # [16, 4]
    中间矩阵 = torch.mm(HH, 初始产品 ) # [4, 4]
    sum_inside_pseudoinverse = torch.eye(self.input_channels) + middle_matrix # [4, 4]

    seudoinverse_sum = torch.pinverse(sum_inside_pseudoinverse) # [4, 4]
    产品内部表达式 = torch.mm(HH, (1/self.forgettingFactor) * self.M) # [4, 16]
    dot_product_pseudo = torch.mm(pseudoinverse_sum,product_inside_expression) # [4, 16]
    dot_product_with_hidden_​​matrix = torch.mm(Ht, dot_product_pseudo ) # [16, 16]

    res = (1/self.forgettingFactor) * self.M - torch.mm((1/self.forgettingFactor) * self.M, dot_product_with_hidden_​​matrix ) # [16,16]

    返回资源
]]></description>
      <guid>https://stackoverflow.com/questions/77769473/adjusting-output-size-of-auxiliary-matrix-in-python</guid>
      <pubDate>Sat, 06 Jan 2024 12:55:49 GMT</pubDate>
    </item>
    <item>
      <title>请帮助在 Siamese 网络中实现 Triplet loss</title>
      <link>https://stackoverflow.com/questions/77769407/please-help-implement-triplet-loss-in-siamese-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77769407/please-help-implement-triplet-loss-in-siamese-network</guid>
      <pubDate>Sat, 06 Jan 2024 12:38:05 GMT</pubDate>
    </item>
    <item>
      <title>RandomizedSearchCV 独立于集成中的模型</title>
      <link>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</link>
      <description><![CDATA[假设我构建了两个估计器的集合，其中每个估计器运行自己的参数搜索：
导入和回归数据集：
从 sklearn.ensemble 导入 VotingRegressor、StackingRegressor、RandomForestRegressor
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.datasets 导入 make_regression

从 sklearn.model_selection 导入 RandomizedSearchCV

X, y = make_regression()

定义两个自调整估计器，并将它们组合起来：
rf_param_dist = dict(n_estimators=[1, 2, 3, 4, 5])
rf_searcher = RandomizedSearchCV(RandomForestRegressor(), rf_param_dist, n_iter=5, cv=3)

dt_param_dist = dict(max_深度=[4, 5, 6, 7, 8])
dt_searcher = RandomizedSearchCV(DecisionTreeRegressor(), dt_param_dist, n_iter=5, cv=3)

合奏 = StackingRegressor(
    [（&#39;rf&#39;，rf_searcher），（&#39;dt&#39;，dt_searcher）]
).fit(X, y)

我的问题是关于sklearn如何处理ensemble的拟合。
Q1）我们有两个并行的未拟合估计器，并且都需要在 ensemble.predict(...) 工作之前进行拟合。但是，如果没有首先从整体中获得预测，我们就无法拟合任何估计器。 sklearn 如何处理这种循环依赖？
Q2）由于我们有两个运行独立调整的估计器，每个估计器是否会错误地假设另一个估计器的参数是固定的？因此，我们最终遇到了一个定义不明确的优化问题。
&lt;小时/&gt;
作为参考，我认为联合优化集成模型的正确方法是定义一个联合搜索所有参数的 CV，如下所示。但我的问题是关于 sklearn 如何处理前面描述的特殊情况。
#联合优化
合奏 = VotingRegressor(
    [ (&#39;rf&#39;, RandomForestRegressor()), (&#39;dt&#39;, DecisionTreeRegressor()) ]
）

jointsearch_param_dist = 字典(
    rf__n_estimators=[1, 2, 3, 4, 5],
    dt__max_深度=[4,5,6,7,8]
）

ensemble_jointsearch = RandomizedSearchCV(ensemble, jointsearch_param_dist)
]]></description>
      <guid>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</guid>
      <pubDate>Sat, 06 Jan 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>主动机器学习 - 在训练前预测模型的大部分信息数据</title>
      <link>https://stackoverflow.com/questions/77768656/active-machine-learning-predict-most-informative-data-for-the-model-before-tra</link>
      <description><![CDATA[我正在开展一个项目，我的目标是通过黑盒方法对电子电路进行建模（端到端建模，而不开发实际电路）。该电路接收四个输入，包括输入信号和范围在 0 到 5 之间的电位计（旋钮）值。
通过使用大量随机值，我的目标是开发一个能够将这些输入映射到输出的模型，并将电位计值用作模型的调节值。
然而，挑战是用最少的数据训练这个模型，因为数据收集非常耗时。我有兴趣找到一种方法，根据给定旋钮设置的过去和当前状态/损耗值，可以识别哪些旋钮值会产生最丰富的数据。这样，我可以将数据收集集中在这些特定值上，然后使用这些数据进行训练。
任何有关此主题的相关文献或资源的建议将不胜感激。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77768656/active-machine-learning-predict-most-informative-data-for-the-model-before-tra</guid>
      <pubDate>Sat, 06 Jan 2024 07:55:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在 aws sagemaker 无服务器推理上运行 Github 公共模型？</title>
      <link>https://stackoverflow.com/questions/77768620/how-to-run-github-public-models-on-aws-sagemaker-serverless-inference</link>
      <description><![CDATA[我想在 AWS Sagemaker 无服务器推理上运行来自 Github 的开源公共模型（例如  https://github.com/ai-forever/Kandinsky-3）。我希望它在 AWS Sagemaker 无服务器上运行，因为我希望在需要时使用 API，并且只需为推理付费。问题是我不知道从哪里开始。我问了 3 个不同的 AI，得到了 3 个不同的答案，但不知道哪一个是正确的。
有人可以帮我指出正确的方向吗？任何提示、教程、一般步骤列表等将不胜感激！谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77768620/how-to-run-github-public-models-on-aws-sagemaker-serverless-inference</guid>
      <pubDate>Sat, 06 Jan 2024 07:37:53 GMT</pubDate>
    </item>
    <item>
      <title>裂纹检测的预处理和算法</title>
      <link>https://stackoverflow.com/questions/77768422/pre-processing-and-algorithm-for-crack-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77768422/pre-processing-and-algorithm-for-crack-detection</guid>
      <pubDate>Sat, 06 Jan 2024 06:11:59 GMT</pubDate>
    </item>
    <item>
      <title>最适合分类器算法的音频文件格式是什么？</title>
      <link>https://stackoverflow.com/questions/77767853/what-is-the-best-suited-audio-file-format-for-a-classifier-algorithm</link>
      <description><![CDATA[用于分类器算法的长录音（约 2 小时）的最佳文件格式是什么？
我的用例：
&lt;块引用&gt;
假设我想训练一个分类器，它可以计算出
房间里正在交谈的人数。
我想我想要一个包含数小时对话音频的数据集
记录下来，每个时间戳都标记有多少人在
给定时间的房间。
然后，我可以对录制的文件进行监督学习，并且
训练一个模型，它可以告诉我有多少人拥有
在任何给定时间进行对话。

我最初的想法是录制WAV，但是这些会不会变得很大，有没有更适合的文件格式？]]></description>
      <guid>https://stackoverflow.com/questions/77767853/what-is-the-best-suited-audio-file-format-for-a-classifier-algorithm</guid>
      <pubDate>Sat, 06 Jan 2024 00:37:11 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型与 ReactNative 集成</title>
      <link>https://stackoverflow.com/questions/77764870/ml-model-integration-with-reactnative</link>
      <description><![CDATA[我正在尝试集成以检查 React Native 中的 ML 模型。我在 google colab 上制作了一个模型，并将训练后的模型保存为 Model.pkl 和 Model.joblib 现在我想将此模型集成到我的 React Native 应用程序中，但我不知道方法，请帮助我！
我一直在尝试在chatgpt的帮助下运行，但它对我不起作用]]></description>
      <guid>https://stackoverflow.com/questions/77764870/ml-model-integration-with-reactnative</guid>
      <pubDate>Fri, 05 Jan 2024 13:23:18 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 haar 级联找到使用网络摄像头的人的参与度？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77761102/how-to-find-the-engagement-level-of-a-person-using-webcam-with-haar-cascades</link>
      <description><![CDATA[我正在建立一个网站，在编写考试时跟踪用户以了解他们的参与程度，并为此找到了这篇有趣的论文。
https://www.sciencedirect .com/science/article/pii/S0045790621002597?ref=cra_js_challenge&amp;fr=RR-1
我想使用 opencv 和 tensorflow 构建完全相同的模型。有人可以帮我完成模型吗？我是机器学习领域的新手。刚刚建立了一些回归和分类模型。任何帮助都会非常有帮助。我无法理解如何从上述论文中计算聚焦概率。我希望了解如何构建模型以及如何计算值的详细步骤。
数据集链接：
FER 2013：https://www.kaggle.com/datasets/msambare/fer2013&lt; /a&gt;
MES数据集：https://github.com/Harsh9524/MES-Dataset
我已经使用 haar 级联检测到人脸并且它正在工作。下一个问题是找出情绪，我也做到了！但问题在于寻找 MES 和焦点概率。请帮我解决一下。]]></description>
      <guid>https://stackoverflow.com/questions/77761102/how-to-find-the-engagement-level-of-a-person-using-webcam-with-haar-cascades</guid>
      <pubDate>Thu, 04 Jan 2024 20:42:19 GMT</pubDate>
    </item>
    <item>
      <title>DataFrame'对象没有属性'符号</title>
      <link>https://stackoverflow.com/questions/77755413/dataframe-object-has-no-attribute-symbol</link>
      <description><![CDATA[我想使用机器学习创建股票价格预测，但出现“‘DataFrame’对象没有属性‘符号’”我的错误是什么以及如何修复它
将 numpy 导入为 np
将 pandas 导入为 pd
从sklearn导入预处理
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression

def prepare_data(df,forecast_col,forecast_out,test_size) :
    label = df[forecast_col].shift(-forecast_out) #创建名为 label 的新列，最后 5 行为 nan
    X = np.array(df [[forecast_col]]) #创建特征数组
    X = preprocessing.scale(X) #处理特征数组
    X_lately = X[-forecast_out:] #创建我想稍后在预测方法中使用的列
    X = X[:-forecast_out] # X 将包含训练和测试
    label.dropna(inplace=True) #删除na值
    y = np.array(label) # 分配 Y
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=test_size, random_state=0) #交叉验证

    响应 = [X_train,X_test,Y_train,Y_test,X_lately]
    返回响应

df = pd.read_csv(“GOOG.csv”)
df = df[df.symbol == &quot;GOOG&quot;]- &quot;错误信息出现的位置&quot;
Forecast_col = “关闭”
预测输出 = 5
   

测试大小 = 0,2

X_train、X_test、Y_train、Y_test、X_lately = 准备数据（df、forecast_col、forecast_out、test_size）
学习者 = 线性回归()
learner.fit (X_train,Y_train )
Score=learner.score(X_test,Y_test)#测试线性回归模型
Forecast= learner.predict(X_lately) #将包含预测数据的集合
响应={}#creting json 对象
响应[&#39;test_score&#39;]=分数
响应[&#39;forecast_set&#39;]=预测

打印（响应）
]]></description>
      <guid>https://stackoverflow.com/questions/77755413/dataframe-object-has-no-attribute-symbol</guid>
      <pubDate>Thu, 04 Jan 2024 01:24:44 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递值的形状为 (8631, 28)，索引意味着 (8631, 17)</title>
      <link>https://stackoverflow.com/questions/77750389/valueerror-shape-of-passed-values-is-8631-28-indices-imply-8631-17</link>
      <description><![CDATA[
第 1 步：创建管道
第2步：将管道转换为数据帧
第3步：我正在尝试将管道转换为数据帧，但出现异常。如何解决这个问题
第 4 步：如何解决 ValueError：传递值的形状为 (8631, 28)，索引意味着 (8631, 17) 在管道转换为数据帧之上，

from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
从 sklearn.impute 导入 SimpleImputer
从 sklearn.pipeline 导入管道
从 sklearn.compose 导入 ColumnTransformer

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split

print(&quot;步骤1：导入lib&quot;)
print(&quot;第2步：加载原始数据&quot;)
df = pd.read_csv(“online_shoppers_intention.csv”)

print(&quot;第三步：数据准备&quot;)
X = df.drop([&#39;收入&#39;], axis = 1)
y = df[&#39;收入&#39;]

print(&quot;第四步：数据分割&quot;)
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = .3、random_state = 0)
名称 = X_train.columns.tolist()

numeric_transformer = SimpleImputer(策略 = &#39;常量&#39;)
categorical_transformer = OneHotEncoder(handle_unknown = &#39;忽略&#39;)

numeric_cols = X.select_dtypes(exclude = &quot;object&quot;).columns.values.tolist()
categorical_cols = X.select_dtypes(exclude = [&#39;int&#39;, &#39;float64&#39;, &#39;bool&#39;]).columns.values.tolist()
    
预处理器 = ColumnTransformer(
    变形金刚=[
    (&#39;num&#39;, numeric_transformer, numeric_cols)
    ,(&#39;猫&#39;, categorical_transformer, categorical_cols)
    ],
    余数 = &#39;直通&#39;)

pipeline_preprocessor = Pipeline(steps = [(“预处理器”, 预处理器), (“pandarizer”, FunctionTransformer(lambda x: pd.DataFrame(x, columns = 名称)))]).fit(X_train)
    
X_train_pipe = pipeline_preprocessor.transform(X_train)
X_test_pipe = pipeline_preprocessor.transform(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/77750389/valueerror-shape-of-passed-values-is-8631-28-indices-imply-8631-17</guid>
      <pubDate>Wed, 03 Jan 2024 07:51:34 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：使用 URL 'http://127.0.0.1:8237' 初始化剩余存储时出错：模块 'jwt' 没有属性 'encode'</title>
      <link>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</link>
      <description><![CDATA[第一个命令python run_deployment.py --config deploy成功运行并建议我运行下一个命令 - zenml up
zenml up 生成以下错误。







我正在 YouTube 上关注 Ayush 的 MLOPs 课程。感谢您提前提供的帮助。
我试过了
&lt;前&gt;&lt;代码&gt;1。 pip 安装 jwt
2.pip安装PyJWT
3. pip卸载jwt
4. pip安装jwt==1.3.0
5. pip install --upgrade --force-reinstall PyJWT
6. pip install --upgrade --force-reinstall jwt
]]></description>
      <guid>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</guid>
      <pubDate>Fri, 29 Dec 2023 07:34:39 GMT</pubDate>
    </item>
    <item>
      <title>llama.cpp llama_cublas 已启用，但运行 ./main 时仅使用 75mb/6gb 的 vram</title>
      <link>https://stackoverflow.com/questions/77354443/llama-cpp-llama-cublas-enabled-but-only-75mb-6gb-of-vram-used-when-running-ma</link>
      <description><![CDATA[我启用了 llama_cublas 来与 nvidia cuda 工具包一起使用
使 LLAMA_CUBLAS=1

编译得很好
但是当我运行模型并监控 nvidia-smi 内存消耗时，只使用了 75mb。见下文。
llm_load_tensors：使用CUDA进行GPU加速
llm_load_tensors：所需内存 = 13189.99 MB
llm_load_tensors：将 0 个重复层卸载到 GPU
llm_load_tensors：将 0/43 层卸载到 GPU
llm_load_tensors：使用的 VRAM：0.00 MB
...................................................... ......................................................
llama_new_context_with_model：n_ctx = 512
llama_new_context_with_model：freq_base = 10000.0
llama_new_context_with_model：freq_scale = 1
llama_new_context_with_model：kv 自身大小 = 400.00 MB
llama_new_context_with_model：计算缓冲区总大小 = 81.13 MB
llama_new_context_with_model：VRAM 暂存缓冲区：75.00 MB
llama_new_context_with_model：使用的总 VRAM：75.00 MB（模型：0.00 MB，上下文：75.00 MB）

nvidia smi 输出
2023 年 10 月 24 日星期二 10:53:17
+------------------------------------------------ --------------------------------------+
| NVIDIA-SMI 535.113.01 驱动程序版本：535.113.01 CUDA 版本：12.2 |
|------------------------------------------+----- ---------------+--------------------+
| GPU 名称持久性-M |总线 ID Disp.A |挥发性未校正。 ECC |
|风扇温度性能功率：使用/上限 |内存使用情况 | GPU-Util 计算 M。
| | |米格·M。
|============================================+======== ==============+======================|
| 0 NVIDIA GeForce RTX 4050 ...关闭| 00000000:01:00.0 关闭 |不适用 |
|不适用 42C P8 5W / 30W | 89MiB / 6141MiB | 0% 默认 |
| | |不适用 |
+----------------------------------------------------+----- ---------------+--------------------+
                                                                                         
+------------------------------------------------ --------------------------------------+
|流程：|
| GPU GI CI PID 类型 进程名称 GPU 内存 |
| ID ID 用途 |
|=================================================== ========================================|
| 0 不适用 不适用 1991 G /usr/lib/xorg/Xorg 4MiB |
+------------------------------------------------ --------------------------------------+
]]></description>
      <guid>https://stackoverflow.com/questions/77354443/llama-cpp-llama-cublas-enabled-but-only-75mb-6gb-of-vram-used-when-running-ma</guid>
      <pubDate>Tue, 24 Oct 2023 18:03:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的 AI 和 ML 库编写一个 python 程序来预测游戏的下一个结果（从两种颜色中选择一种颜色）</title>
      <link>https://stackoverflow.com/questions/76086477/how-to-make-a-python-program-to-predict-the-next-outcome-of-a-game-of-picking-a</link>
      <description><![CDATA[我想使用 LSTM 技术在 python 中编写一个程序，可以预测下一个结果，或者说从两种颜色中选择一种颜色的概率，该程序应该使用 AI 和 ML 库，来读取最后 40 个结果的模式从而预测下一个结果。
嗯，我为此制定了以下计划。
从 keras.models 导入顺序
从 keras.layers 导入 LSTM，密集
将 numpy 导入为 np


def Predict_next_color_lstm（结果）：
    如果 len(结果) &lt; 40：
        返回“错误：提供的结果数量小于 40。”

    # 将字符串输入转换为整数序列
    seq = [如果 x == &#39;r&#39; 则为 0，否则结果中的 x 为 1]

    # 创建 40 个结果的滚动窗口
    X = []
    y = []
    对于范围内的 i(len(seq) - 40)：
        X.append(seq[i:i + 40])
        y.append(seq[i + 40])
    X = np.array(X)
    y = np.array(y)

    # 重塑 X 以适应 LSTM 输入形状
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # 创建LSTM模型
    模型=顺序（）
    model.add(LSTM(50, input_shape=(40, 1)))
    model.add（密集（1，激活=&#39;sigmoid&#39;））

    # 编译模型
    model.compile(loss=&#39;binary_crossentropy&#39;, 优化器=&#39;adam&#39;)

    # 训练模型
    model.fit(X, y, epochs=50, batch_size=32)

    # 预测下一个结果
    最后_40 = seq[-40:]
    pred = model.predict(np.array([last_40]))
    如果 pred &lt;，则返回 &#39;r&#39; 0.5 其他“g”


def get_input():
    # 要求用户输入长度为40的球颜色序列
    ball_seq = input(&quot;输入长度为 40 的球颜色序列（例如 rrggrrgrrgggrgrgrgrggggrgrgrrgrgggrrgggg）：&quot;)
    返回 ball_seq


＃ _主要的_
ball_seq = get_input()
print(“预测：”,predict_next_color_lstm(ball_seq))

但我在执行时不断收到以下错误：
C:\Users\Ashish\miniconda3\python.exe C:\Users\Ashish\Desktop\pyt_pract\test_prob1.py
输入长度为 40 的球颜色序列（例如 rrggrrgrrgggrgrgrggrrggggrgrgrrgrgggrrgggg）： rgggrrgrgrggrrgrgrgrggggrrrrggrrggrgrg
回溯（最近一次调用最后一次）：
文件“C:\Users\Ashish\Desktop\pyt_prac\test_prob1.py”，第 50 行，位于
print(“预测：”,predict_next_color_lstm(ball_seq))
文件“C:\Users\Ashish\Desktop\pyt_prac\test_prob1.py”，第 23 行，位于 Predict_next_color_lstm 中
X = np.reshape(X, (X.shape[0], X.shape[1], 1))
IndexError：元组索引超出范围]]></description>
      <guid>https://stackoverflow.com/questions/76086477/how-to-make-a-python-program-to-predict-the-next-outcome-of-a-game-of-picking-a</guid>
      <pubDate>Sun, 23 Apr 2023 18:08:18 GMT</pubDate>
    </item>
    <item>
      <title>用于分类特征的 LabelEncoder？</title>
      <link>https://stackoverflow.com/questions/61217713/labelencoder-for-categorical-features</link>
      <description><![CDATA[这可能是一个初学者问题，但我见过很多人使用 LabelEncoder() 用序数替换分类变量。很多人通过一次传递多个列来使用此功能，但是我对我的某些功能中的序数错误以及它将如何影响我的模型有些怀疑。这是一个例子：
输入
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 LabelEncoder

a = pd.DataFrame([&#39;高&#39;,&#39;低&#39;,&#39;低&#39;,&#39;中&#39;])
le = 标签编码器()
le.fit_transform(a)

输出
数组([0, 1, 1, 2], dtype=int64)

如您所见，序数值未正确映射，因为我的 LabelEncoder 只关心列/数组中的顺序（应该是 High=1、Med=2、Low=3，反之亦然）。错误的映射会对模型产生多大的影响？除了 OrdinalEncoder() 之外，是否有一种简单的方法可以正确映射这些值？]]></description>
      <guid>https://stackoverflow.com/questions/61217713/labelencoder-for-categorical-features</guid>
      <pubDate>Tue, 14 Apr 2020 21:40:52 GMT</pubDate>
    </item>
    </channel>
</rss>