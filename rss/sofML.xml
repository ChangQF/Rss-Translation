<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Mar 2024 03:16:37 GMT</lastBuildDate>
    <item>
      <title>使用 opacus 尝试训练 dp 模型，但遇到 `TypeError: __init__() Missing 1 requiredpositional argument: 'module'`</title>
      <link>https://stackoverflow.com/questions/78138246/using-opacus-try-to-train-a-dp-model-but-meet-typeerror-init-missing-1</link>
      <description><![CDATA[我正在使用 opacus 通过以下代码进行 dp 训练模型。然而，在运行PrivacyEngine线路时出现了一个错误。
transformed_data = self.table.transform(dataframe)
        加载器= DataLoader（transformed_data，batch_size = self.batch_size，shuffle = True）
        self.optimizer = torch.optim.Adam(self.parameters())
        # self.privacy_engine = PrivacyEngine(accountant=“rdp”, secure_mode=True)
        # self.privacy_engine = PrivacyEngine()
        self.net, self.optim, self.train_data = PrivacyEngine(accountant=“rdp”, # 错误 secure_mode=True).make_private_with_epsilon( \
            模块= self.net，\
            优化器 = self.optim,\
            data_loader = 加载器,\
            target_epsilon=self.epsilon_target,\
            target_delta=self._delta,\
            epochs=self.epoch_target,\
            max_grad_norm=self.max_grad_norm,\
            poisson_sampling=真，\
        ）
        对于范围内的 epoch_idx（self.start_epoch，self.epochs）：
            开始时间 = 时间.time()
            损失= self._train_epoch（加载器，约束，epoch_idx，show_progress = show_progress，** kwargs）
            train_loss = sum(损失) if isinstance(损失, 元组) else 损失
            结束时间 = time.time()
            如果详细的话：
                print(“epoch %d: 训练损失 %.3f, 时间成本 %.3fs” % (epoch_idx, train_loss, end_time - start_time))

错误如下：
 文件“/home/ruc/xiaotong/OpenDataGen/log/20240310/open-data-gen/src/model/dpautoregressive/dpmade.py”，第 401 行，适合
    self.net, self.optim, self.train_data = PrivacyEngine(accountant=“rdp”, secure_mode=True).make_private_with_epsilon( \
类型错误：__init__() 缺少 1 个必需的位置参数：“模块”

我很困惑，我认为函数中存在模块，但它告诉我缺少“模块”。
我认为这段代码应该是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/78138246/using-opacus-try-to-train-a-dp-model-but-meet-typeerror-init-missing-1</guid>
      <pubDate>Mon, 11 Mar 2024 03:09:04 GMT</pubDate>
    </item>
    <item>
      <title>我可以将 MLflow 自动记录与 Vanilla PyTorch 一起使用吗？</title>
      <link>https://stackoverflow.com/questions/78138203/can-i-use-mlflow-autologging-with-vanilla-pytorch</link>
      <description><![CDATA[我在官方 MLflow 文档中发现了以下声明：
&lt;块引用&gt;
对普通 PyTorch（即仅子类 torch.nn.Module 的模型）的自动记录支持仅自动记录对 torch.utils.tensorboard.SummaryWriter 的 add_scalar 和 add_hparams 方法的调用到毫升流。

基于此，我假设即使使用普通 PyTorch，add_scalar 和 add_hparams 方法也会通过自动记录自动执行。
因此，我运行了以下示例代码，但 MLflow 中没有记录任何内容：
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入火炬视觉
导入 torchvision.transforms 作为变换
导入流量

mlflow.autolog()

变换 = 变换.Compose([变换.ToTensor(), 变换.Normalize((0.5,), (0.5,))])
trainset = torchvision.datasets.FashionMNIST(root=&#39;./data&#39;,train=True,download=True,transform=transform)
trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True)


类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)

    def 前向（自身，x）：
        x = x.view(-1, 28 * 28)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


净=净()

标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.01, 动量=0.9)

对于范围（2）中的纪元：
    运行损失 = 0.0
    对于 i，enumerate(trainloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        输出 = 净值（输入）
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()
        running_loss += loss.item()
        如果我% 2000 == 1999：
            print(f&#39;[{epoch + 1}, {i + 1}] 损失: {running_loss / 2000}&#39;)
            运行损失 = 0.0

我是不是做错了什么？
我尝试了以下版本：

mlflow==2.11.1（最新）
火炬==2.2.1
torchvision==2.2.1
张量板==2.16.2
]]></description>
      <guid>https://stackoverflow.com/questions/78138203/can-i-use-mlflow-autologging-with-vanilla-pytorch</guid>
      <pubDate>Mon, 11 Mar 2024 02:50:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 绘制梯度下降曲线</title>
      <link>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</link>
      <description><![CDATA[我在 Keras 中实现了以下代码，该代码使用加州住房数据集，试图绘制 theta 1 和 theta 2 的值，以及随机梯度下降、批量梯度或小批量的选择如何影响结果： 
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 keras.models 导入顺序
从 keras.layers 导入密集
从 keras.optimizers 导入 SGD
从 sklearn.datasets 导入 fetch_california_housing
从 sklearn.preprocessing 导入 StandardScaler

# 加载加州住房数据集
加州住房 = fetch_加州住房()
X, y = california_housing.data, california_housing.target

# 标准化特征
定标器=标准定标器()
X_归一化 = 缩放器.fit_transform(X)

def build_model():
    模型=顺序（[
    密集（64，激活=“relu”，input_shape=（8，）），
    密集(64，激活=“relu”)，
    密集(1)
    ]）
    #model.compile（优化器=“rmsprop”，损失=“mse”，指标=[“mae”]）
    返回模型


sgd_optimizer = SGD(lr=0.001) # 随机梯度下降
minibatch_sgd_optimizer = SGD(lr=0.001) # 小批量梯度下降
batch_sgd_optimizer = SGD(lr=0.001) # 批量梯度下降


# 编译模型
模型=build_model()
model.compile(loss=&#39;mse&#39;, 优化器=sgd_optimizer)

theta1_sgd、theta2_sgd = []、[]
theta1_minibatch_sgd、theta2_minibatch_sgd = []、[]
theta1_batch_sgd、theta2_batch_sgd = []、[]


# 执行梯度下降并存储 theta 值的函数
def Perform_gradient_descent（优化器，batch_size=None）：
    theta1_列表、theta2_列表 = []、[]
    损失历史记录 = []
    for _ in range(5): # 纪元数
        历史记录=model.fit(X_normalized, y, epochs=1,batch_size=batch_size, verbose=0)
        loss_history.append(history.history[&#39;loss&#39;][0])
        weights = model.layers[0].get_weights()[0].flatten() # 获取当前 theta 值
        theta1_list.append(权重[0])
        theta2_list.append(权重[1])
    打印（theta1_列表，“”，theta2_列表）
    返回loss_history，theta1_list，theta2_list

# 使用不同的优化器执行梯度下降
loss_sgd, theta1_sgd, theta2_sgd = Perform_gradient_descent(sgd_optimizer, batch_size=1) # 随机梯度下降
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = Perform_gradient_descent(minibatch_sgd_optimizer, batch_size=32) # 小批量梯度下降
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = Perform_gradient_descent(batch_sgd_optimizer, batch_size=len(X_normalized)) # 批量梯度下降

# 绘制损失与纪元数的关系图
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(loss_sgd) + 1), loss_sgd, label=&#39;随机梯度下降&#39;)
plt.plot(range(1, len(loss_minibatch_sgd) + 1), loss_minibatch_sgd, label=&#39;小批量梯度下降&#39;)
plt.plot(range(1, len(loss_batch_sgd) + 1), loss_batch_sgd, label=&#39;批量梯度下降&#39;)
plt.xlabel(&#39;历元数&#39;)
plt.ylabel(&#39;损失&#39;)
plt.title(&#39;损失与历元数&#39;)
plt.图例()
plt.网格（真）
plt.show()

# 绘制梯度下降轨迹
plt.figure(figsize=(10, 6))
plt.plot(theta1_sgd,theta2_sgd,label=&#39;随机梯度下降&#39;,marker=&#39;o&#39;)
plt.plot(theta1_minibatch_sgd, theta2_minibatch_sgd, label=&#39;小批量梯度下降&#39;,marker=&#39;s&#39;)
plt.plot(theta1_batch_sgd, theta2_batch_sgd, label=&#39;批量梯度下降&#39;,marker=&#39;x&#39;)
plt.xlabel(&#39;Theta 1&#39;)
plt.ylabel(&#39;Theta 2&#39;)
plt.title(&#39;梯度下降轨迹&#39;)
#plt.xlim(-0.08, -0.05) # 设置 Theta 1 的限制
#plt.ylim(0.02, 0.03) # 设置 Theta 2 的限制
plt.图例()
plt.网格（真）
plt.show()

但是，我发现的问题是，有时保存 theta 值的列表的值是 Nan，而在其他情况下是正常值。当 epoch 数量增加到 10 以上时，我注意到了这一点，这是为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</guid>
      <pubDate>Sun, 10 Mar 2024 22:46:54 GMT</pubDate>
    </item>
    <item>
      <title>每次运行时都会出现不同的 ValueError</title>
      <link>https://stackoverflow.com/questions/78137114/different-valueerror-each-time-i-run</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78137114/different-valueerror-each-time-i-run</guid>
      <pubDate>Sun, 10 Mar 2024 18:55:17 GMT</pubDate>
    </item>
    <item>
      <title>多值和多目标分类[关闭]</title>
      <link>https://stackoverflow.com/questions/78136790/multi-value-and-multi-target-classification</link>
      <description><![CDATA[首先，我想检查一下我对类、多类、多值和多目标的理解：
类别
如果我有3个属性（温度、天气和湿度），并且我正在预测天气条件是否有利于玩耍，并且Play是二元的（是、否），这是一个简单的分类，例如：

&lt;标题&gt;

温度
天气
湿度
播放


&lt;正文&gt;

...
...
...
是/否



多类别
类似的情况，但 Play 不是一个枚举列表，包含（互斥的）游戏方式。这将是一个“多类”分类：

&lt;标题&gt;

温度
天气
湿度
播放


&lt;正文&gt;

...
...
...
秋千踢球标签



多值
如果存在多个非互斥的结果，例如每种游戏方式都有其自己的属性，并且每个属性都是二元的（是/否），则这是“多值”：

&lt;标题&gt;

温度
天气
湿度
秋千
踢球
标签


&lt;正文&gt;

...
...
...
是/否
是/否
是/否



多目标
最后一个示例，如果每个可能的结果可能包含非二进制、互斥的值，则这将是多目标：

&lt;标题&gt;

温度
天气
湿度
雪
骑自行车
正在运行


&lt;正文&gt;

...
...
...
滑雪滑雪雪地摩托
山路碎石
跑步机步道跑道



这是正确的吗？
我正在使用 Weka 来预测多目标结果。我希望使用“多目标”分类器的版本(meka.classifiers.multitarget.CC)，但这些结果始终只是“1” （使用“double[][] Predictions = result.allPredictions();”）
|==== 预测 (N=3.0) =====&gt;
| 1 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
| 2 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
| 3 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
|================================&lt;
如果我使用“多标签”分类器（meka.classifiers.multilabel.CC）然后它给我我认为正确的结果，这些值是值列表中的正确索引：
|==== 预测 (N=3.0) =====&gt;
| 1 [ -1 -1 -1 -1 ] [ 2.000 1.000 0.000 ]
| 2 [ -1 -1 -1 -1 ] [ 0.000 1.000 2.000 ]
| 3 [ -1 -1 -1 -1 ] [ 1.000 0.000 2.000 ]
|================================&lt;
我当然可以使用多标签分类器，但它看起来并不是正确的使用方法。
我不明白什么？
我期待多目标能够给我多标签所提供的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78136790/multi-value-and-multi-target-classification</guid>
      <pubDate>Sun, 10 Mar 2024 17:23:21 GMT</pubDate>
    </item>
    <item>
      <title>当第 33 次迭代从头开始训练线性回归时，MSE 始终变为 0</title>
      <link>https://stackoverflow.com/questions/78136597/mse-always-becomes-0-when-training-linear-regression-from-scratch-at-33th-iterat</link>
      <description><![CDATA[我正在尝试在 Kaggle 上的 Spotify 2023 数据集上从头开始训练多线性回归模型。
def min_max_scalar(df, col):
df[col] = (df[col] - min(df[col])) / (max(df[col]) - min(df[col]))
返回 df[列]

defmean_squared_error（实际，预测）：
返回 np.mean((实际 - 预测) ** 2)

Spotify_2023_data[&#39;streams&#39;] = min_max_scalar(spotify_2023_data, &#39;streams&#39;)

X = spotify_2023_data[[&#39;in_spotify_playlists&#39;, &#39;in_apple_playlists&#39;]]
Y = spotify_2023_data[&#39;流&#39;]
X[&#39;ones&#39;] = np.ones(len(X),) #进行拦截

阈值 = 1e-6
步长大小 = 5e-9
theta, theta_prev = np.array(np.repeat(5,3)), np.ones(3,) #随机权重

迭代= 0
训练错误 = []
测试错误 = []
训练大小 = np.arange(1,len(Y)+1)
训练迭代= []
#TODO：33 次，算法失控
for i in Training_size: #意味着通过改变训练数据的数量来显示偏差-方差权衡
    如果 i % 2 == 0: 继续
    X_train = X.iloc[:i]
    y_train = Y.iloc[:i]
    X_test = X.iloc[i:]
    y_test = Y.iloc[i:]

    而 np.linalg.norm(theta - theta_prev) &gt;阈值：课本中使用的#threshold机制来确定何时停止。不幸的是，我必须使用这个。
        θ_prev = θ
        梯度 = mse_gradient(theta, X_train, y_train)
        theta = theta_prev - step_size * 梯度
        if (np.isnan(theta)): 打印(theta)
        迭代 += 1

    print(&quot;i={}, results={}, mse={}&quot;.format(i, f(X_test, theta),mean_squared_error(y_test, f(X_test, theta))))
    print(&quot;theta={}&quot;.format(theta))
    打印（“iter =”，iter）
    迭代= 0
    Training_error.append(mean_squared_error(y_train, f(X_train, theta)))
    test_error.append(mean_squared_error(y_test, f(X_test, theta)))
    训练迭代.append(i)
        
    theta, theta_prev = np.array(np.repeat(5,3)), np.ones(3,)

无论我使用什么 i 值，它似乎总是在第 33 次迭代时产生 NaN 值。我检查过，这可能是因为 theta 此时跳到无穷大。我检查了使用的数据，似乎没有任何丢失的数据。我应该做什么才能让培训继续进行？]]></description>
      <guid>https://stackoverflow.com/questions/78136597/mse-always-becomes-0-when-training-linear-regression-from-scratch-at-33th-iterat</guid>
      <pubDate>Sun, 10 Mar 2024 16:19:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求推进 SketchCode 项目的指导 - YOLOV8 培训已完成 [关闭]</title>
      <link>https://stackoverflow.com/questions/78136580/seeking-guidance-for-advancing-sketchcode-project-yolov8-training-completed</link>
      <description><![CDATA[我一直在致力于一个名为 SketchCode 的令人兴奋的项目，旨在利用深度学习将手绘网站模型转换为功能性 HTML/CSS 代码。我在手绘草图的自定义数据集上成功训练了 YOLOV8 模型，该项目达到了一个里程碑。
这是该项目的非常简化的细分：
图像理解：模型分析手绘草图，识别各个 UI 组件。
序列生成：HTML 代码生成被视为语言翻译任务。该模型将视觉元素转换为相应的 HTML 代码片段。
技术细分：
数据集：我使用了此处提供的手绘草图数据集。
YOLOV8 训练：我使用 Ultralytics 库对 YOLOV8 模型进行了 10 个 epoch 的训练。（使用 GPU 而不是 CPU，并对其进行了 50 个 epoch 的训练）
但是，我目前陷入困境并寻求有关后续步骤的指导。训练输出看起来很有希望，但我不知道接下来该何去何从。
如果您能提供有关我接下来应该关注的重点的见解、建议或建议，我将不胜感激。无论是改进模型、改进数据集还是整合其他技术，您的专业知识都是无价的。
我尝试在 SketchCode 项目的手绘草图数据集上训练 YOLOV8 模型。我希望模型能够学习并准确检测草图中的各种组件，例如按钮、文本框、标题等。完成 10 个 epoch 的训练后，输出结果显示出良好的结果，不同类别的 mAP 分数都不错。]]></description>
      <guid>https://stackoverflow.com/questions/78136580/seeking-guidance-for-advancing-sketchcode-project-yolov8-training-completed</guid>
      <pubDate>Sun, 10 Mar 2024 16:14:16 GMT</pubDate>
    </item>
    <item>
      <title>是否可以对随机森林回归的某些特征进行加权？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78136531/is-it-possible-to-weight-some-features-for-a-random-forest-regression</link>
      <description><![CDATA[我一直在尝试训练一个随机森林模型，该模型根据到其他点的距离以及这些点的气候和地理数据等特征来预测点的特征。问题出现在结果上，它们并不糟糕，但还有改进的余地。该模型预测最重要的特征是点之间的距离，但我们认为它应该更加重视气候数据特征，而不太重视距离。
有没有办法让模型降低距离的重要性？也许给它们加权？或者问题可能出在我们为模型提供的数据上？
我尝试过多种模型，如 Lasso、Gradient Boosting 和 Random Forest。还尝试了不同类型的交叉验证。最好的结果来自具有交叉验证的随机森林，以避免某些空间自相关。尽管如此，该模型预计会比现在更好。我们怀疑它非常重视点之间的距离，但我们不知道如何改变这一点并获得更好的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78136531/is-it-possible-to-weight-some-features-for-a-random-forest-regression</guid>
      <pubDate>Sun, 10 Mar 2024 15:58:48 GMT</pubDate>
    </item>
    <item>
      <title>如何以高精度（+ 90%）对面部特征嵌入进行分类。我可以在 svm 模型中进行哪些调整来对 20 多个类别进行分类</title>
      <link>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</link>
      <description><![CDATA[我使用facenet提取特征并使用svm进行分类。效果很好，但 20 堂课后，准确率下降到 75%。如何在利用 GPU 的同时优化 svm。
我使用了这个 svm 类模型
scikit learn 的 svm 模型不使用 GPU，所以我使用了这个
类 SVM(nn.Module):
    def __init__(自身):
        超级（SVM，自我）.__init__()
        self.fc = nn.Linear(X.shape[1], len(ClassList))

    def 前向（自身，x）：
        返回 self.fc(x)

但是对于 20 多个类别来说，这个准确率非常低
我也尝试过使用这个：
类 SoftmaxUsed(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.layers = nn.Sequential(nn.Linear(512, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.线性(1024, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1024, len(ClassList)),
                                 nn.LogSoftmax(dim=1))
    def 前向（自身，x）：
        返回 self.layers(x)

但准确率最高仍为 86%]]></description>
      <guid>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</guid>
      <pubDate>Sat, 09 Mar 2024 18:56:09 GMT</pubDate>
    </item>
    <item>
      <title>RMSE 值也很低，但验证损失图在我的 LSTM 模型中有很多尖峰。我怎样才能解决这个问题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78132521/rmse-values-is-also-low-but-validation-loss-graph-has-alot-of-spikes-in-my-lstm</link>
      <description><![CDATA[我创建了一个用于时间序列预测的 LSTM 模型。我的价值观如下，
测试集的 RMSE 分数：1.55

我的验证损失图中有很多峰值，如下所示。如何减少峰值并修复此图表？

我的代码如下，
从 keras.models 导入顺序

def lstm_model(trainX,trainY):
  #创建堆叠的 LSTM 模型
  模型=顺序（）
  model.add(LSTM(64,activation=&#39;relu&#39;,input_shape=(trainX.shape[1],trainX.shape[2]),return_sequences=False))
  模型.add(Dropout(0.2))
  model.add(密集(1))
  # 编译模型
  model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;)

  print(&quot;LSTM 模型摘要&quot;)
  打印（模型.摘要（））
  打印(“---------------------------------------------- ----------”）

  # 拟合模型
  历史= model.fit（trainX，trainY，epochs = 200，batch_size = 8，validation_split = 0.01，verbose = 1）
  打印（历史）
  打印(“---------------------------------------------- ----------”）

  #model.save(&#39;/content/gdrive/MyDrive/MScProject/Implementation/lstm.h5&#39;)
  位置 = &#39;/content/gdrive/MyDrive/MScProject/Implementation/&#39; + 银行名称 + &#39;/lstm.h5&#39;
  模型.保存（位置）

  rmse = evaluate_models(历史, 模型)
  返回均方根误差

rmse = lstm_model(trainX,trainY)
模型测试（rmse）
]]></description>
      <guid>https://stackoverflow.com/questions/78132521/rmse-values-is-also-low-but-validation-loss-graph-has-alot-of-spikes-in-my-lstm</guid>
      <pubDate>Sat, 09 Mar 2024 13:36:46 GMT</pubDate>
    </item>
    <item>
      <title>邻居索引错误：self._check_indexing_error(key) KeyError：8</title>
      <link>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-self-check-indexing-errorkey-keyerror-8</link>
      <description><![CDATA[我正在创建一个服装推荐系统，使用 NearestNeighbors，数据来自 2 个数据集，其中一个数据集包含 ratings.csv，在本例中 0 和 1&lt; /code&gt; 基于是否保存到愿望清单以及所有衣服的衣服.csv，我想传递服装的 ID 并获取推荐商品的列表，但我收到索引错误。
这是代码：
user_ ratings_df = pd.read_csv(“ ratings.csv”)

user_ ratings_df[&#39;IDGARMENT&#39;] = user_ ratings_df[&#39;IDGARMENT&#39;].astype(int)

# 读入数据；使用默认的 pd.RangeIndex，即 0、1、2 等作为列
Clothes_desc = pd.read_csv(“clothes.csv”, on_bad_lines=&#39;skip&#39;)
Clothing_metadata = Clothing_desc[[&#39;IDGARMENT&#39;, &#39;描述&#39;, &#39;类别&#39;, &#39;品牌&#39;, &#39;价格&#39;]]

衣服元数据[&#39;IDGARMENT&#39;] = 衣服元数据[&#39;IDGARMENT&#39;].astype(int)
Clothes_data = user_ ratings_df.merge(clothes_metadata, on=&#39;IDGARMENT&#39;)

user_item_matrix = user_ ratings_df.pivot(index=[&#39;USERID&#39;], columns=[&#39;IDGARMENT&#39;], value=&#39;RATING&#39;).fillna(0)
用户项矩阵

# 定义一个关于余弦相似度的 KNN 模型
cf_knn_model=NearestNeighbors(metric=&#39;cosine&#39;,algorithm=&#39;brute&#39;,n_neighbors=10,n_jobs=-1)
#lr.fit(x.reshape(-1, 1), y)

# 将模型拟合到我们的矩阵上
cf_knn_model.fit(user_item_matrix)


def dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs):
    # 在矩阵上拟合模型
    cf_knn_model.fit（矩阵）
    
    # 计算邻居距离
    距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
    Clothing_rec_ids = Sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]
    
    # 存储推荐的列表
    cf_recs = []
    对于我在 dress_rec_ids 中：
        cf_recs.append({&#39;Desc&#39;:clothes_desc[&#39;DESCRIPTION&#39;][i[0]],&#39;距离&#39;:i[1]})
    
    # 选择需要的最多推荐数量
    df = pd.DataFrame(cf_recs, 索引 = 范围(1,n_recs))
    返回df


n_recs = 10
dress_recommender_engine（54448，user_item_matrix，cf_knn_model，n_recs）

我得到的错误是：
&lt;前&gt;&lt;代码&gt;&gt; *keyError Traceback（最近一次调用最后）文件
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802,
&gt;在Index.get_loc（self，key，method，tolerance）3801中尝试：
&gt; -&gt; [第 3802 章] 第 3803 章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; pandas/_libs/hashtable_class_helper.pxi:2263，在
&gt; pandas._libs.hashtable.Int64HashTable.get_item() 文件
&gt; pandas/_libs/hashtable_class_helper.pxi:2273，位于
&gt; pandas._libs.hashtable.Int64HashTable.get_item() KeyError：54448
&gt;上述异常是以下异常的直接原因：
&gt; KeyError Traceback（最近调用
&gt;最后）单元格 In[4]，第 64 行
&gt; 59 返回 df
&gt; 63 n_recs = 10
&gt; ---&gt; 64 dress_recommender_engine(54448, user_item_matrix, cf_knn_model, n_recs) 单元格 In[4]，第 48 行，in
&gt; dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs)
&gt; 42 cf_knn_model.fit（矩阵）
&gt; 44 # 提取输入的电影ID
&gt;第45话
&gt; 46
&gt; 47 # 计算邻居距离
&gt; ---&gt; 48 个距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
&gt;第49章 衣服
&gt; x: x[1])[:0:-1]
&gt; 51 # 存储推荐的列表 File ~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807, in
&gt;第3805章1：
&gt;第3806章
&gt; -&gt;第3807章 第3808章 第3809章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804，
&gt;在Index.get_loc（self，key，method，tolerance）3802返回
&gt; self._engine.get_loc(casted_key) 3803 除了 KeyError 为错误：
&gt; -&gt;第3804章 3805，除了TypeError：3806，引发KeyError（key）
&gt;第3807章否则我们会失败并重新加注
&gt;第3808章第3809章
&gt;密钥错误：54448*

错误似乎在这一行：
距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
当传递matrix[garment_id]时，知道如何解决它吗？]]></description>
      <guid>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-self-check-indexing-errorkey-keyerror-8</guid>
      <pubDate>Mon, 04 Mar 2024 14:12:06 GMT</pubDate>
    </item>
    <item>
      <title>Pytroch 分割模型(.pt) 未转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</guid>
      <pubDate>Sat, 02 Mar 2024 01:26:48 GMT</pubDate>
    </item>
    <item>
      <title>如何重现带有modified_huber损失的SGDClassifier？</title>
      <link>https://stackoverflow.com/questions/78057268/how-do-i-reproduce-a-sgdclassifier-with-modified-huber-loss</link>
      <description><![CDATA[我有一个像这样定义的模型：
rng = 42
模型=管道（[
    (&#39;缩放器&#39;, RobustScaler()),
    (&#39;特征&#39;, SelectKBest(k=42)),
    (&#39;model&#39;, SGDClassifier(loss=&#39;modified_huber&#39;, shuffle=True, random_state=rng))
]）

当我使用完全相同的输入在两个单独的程序执行（一个是临时的，另一个是 cron 作业）中进行训练和预测时，我会得到不同的模型权重，从而得到预测结果。
我注意到“铰链”损失是唯一具有完全相同权重的可重现模型。其他损失函数是什么阻止了它们被重现？
我已经检查并仔细检查输入是否相同，并使用其他损失函数进行验证。]]></description>
      <guid>https://stackoverflow.com/questions/78057268/how-do-i-reproduce-a-sgdclassifier-with-modified-huber-loss</guid>
      <pubDate>Sun, 25 Feb 2024 18:57:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用数据加载器解决这个问题？</title>
      <link>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</link>
      <description><![CDATA[我正在构建一些数据加载器来训练和测试机器学习模型。
我有一个名为“array”的元组列表像这样：
(Data(x=[468, 2], edge_index=[2, 1322], y=0, edge_weight=[1322]), &#39;morphed_img027485_img054553.png&#39;)
（数据（x=[468, 2]，edge_index=[2, 1322]，y=0，edge_weight=[1322]），&#39;morphed_img031737_img054553.png&#39;）

我像这样创建数据加载器：
data_loader = create_dataloader(数组，batch_size=60)
save_dataloader(data_loader, &#39;DataLoader 名称&#39;)

输出不是我所期望的，但它将所有数据合并到一个 DataBatch 中，如下所示：
[DataBatch(x=[936, 2]，edge_index=[2, 2644]，y=[2]，edge_weight=[2644]，batch=[936]，ptr=[3])， (&#39;morphed_img031737_img054553.png&#39;, &#39;morphed_img027485_img054553.png&#39;)]

为什么？如何拥有一个数据加载器，将所有数据像在数组中一样分开？]]></description>
      <guid>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</guid>
      <pubDate>Fri, 09 Feb 2024 14:52:39 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.decomposition.PCA 特征向量的简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我正在尝试了解主成分分析的工作原理，并在sklearn.datasets.load_iris数据集上对其进行测试。我了解每个步骤的工作原理（例如标准化数据、协方差、特征分解、排序最高特征值、使用 K 选定的维度将原始数据转换为新轴）。
下一步是可视化这些特征向量在数据集上的投影位置（在PC1 vs. PC2 图上，对吗？）。
有人可以解释如何在降维数据集的 3D 图上绘制 [PC1、PC2、PC3] 特征向量吗？
另外，我是否正确绘制了这个 2D 版本？我不确定为什么我的第一个特征向量的长度较短。我应该乘以特征值吗？
&lt;小时/&gt;
以下是我为实现这一目标所做的一些研究：
我遵循的 PCA 方法来自：
https://plot.ly /ipython-notebooks/principal-component-analysis/#Shortcut---PCA-in-scikit-learn （虽然我不想使用 plotly。我想坚持使用pandas、numpy、sklearn、matplotlib、scipy 和 seaborn）
我一直在遵循本教程来绘制特征向量，它看起来非常简单：基本示例对于带有 matplotlib 的 PCA 但我似乎无法用我的数据复制结果。
我发现了这个，但对于我想要做的事情来说，它似乎过于复杂，而且我不想创建一个 FancyArrowPatch： 使用 matplotlib 和 np.linalg 绘制协方差矩阵的特征向量
&lt;小时/&gt;
我尝试使我的代码尽可能简单，以便遵循其他教程：
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 sklearn.datasets 导入 load_iris
从 sklearn.preprocessing 导入 StandardScaler
来自 sklearn 导入分解
将seaborn导入为sns； sns.set_style(“whitegrid”, {&#39;axes.grid&#39; : False})

%matplotlib 内联
np.随机.种子(0)

# 鸢尾花数据集
DF_data = pd.DataFrame(load_iris().data,
                       索引 = [“iris_%d”; % i for i in range(load_iris().data.shape[0])],
                       列= load_iris().feature_names)

Se_targets = pd.Series(load_iris().target,
                       索引 = [“iris_%d”; % i for i in range(load_iris().data.shape[0])],
                       名称=“物种”）

# 缩放均值 = 0，var = 1
DF_standard = pd.DataFrame(StandardScaler().fit_transform(DF_data),
                           索引 = DF_data.index,
                           列= DF_data.列）

# Sklearn 用于主成分分析

# 变暗
m = DF_standard.shape[1]
K = 2

# PCA（我倾向于如何设置它）
M_PCA = 分解.PCA(n_components=m)
DF_PCA = pd.DataFrame(M_PCA.fit_transform(DF_standard),
                列=[“PC%d” % k for k in range(1,m + 1)]).iloc[:,:K]


# 绘制特征向量
#https://stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

# 这就是事情变得奇怪的地方......
数据 = DF_标准

mu = data.mean(轴=0)
特征向量，特征值 = M_PCA.components_，M_PCA.explained_variance_ #特征向量，特征值，V = np.linalg.svd(data.T, full_matrices=False)
projected_data = DF_PCA #np.dot(数据，特征向量)

西格玛=projected_data.std(axis=0).mean()

图, ax = plt.subplots(figsize=(10,10))
ax.scatter(projected_data[“PC1”],projected_data[“PC2”])
对于轴，zip 中的颜色（特征向量[:K], [“红色”,“绿色”]）：
# start, end = mu, mu + sigma * axis ### 导致“ValueError：太多值无法解压（预期为 2）”

    # 所以我尝试了这个，但我认为它不正确
    开始，结束 = (mu)[:K], (mu + sigma * 轴)[:K]
    ax.annotate(&#39;&#39;, xy=结束,xytext=开始, arrowprops=dict(facecolor=颜色, width=1.0))
    
ax.set_aspect(&#39;等于&#39;)
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    </channel>
</rss>