<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 01 Feb 2024 15:13:49 GMT</lastBuildDate>
    <item>
      <title>如何使用 XGBRegressor 预测未来数据</title>
      <link>https://stackoverflow.com/questions/77920944/how-to-predict-future-data-with-xgbregressor</link>
      <description><![CDATA[我有一个数据集，分割为 80/20，准确率约为 95%。我想使用现有数据来预测下个月的数据并看看它的进展如何。我将数据集与适当的时间行连接起来，并将我想要预测的列保留为 0。结果每天都返回相同的数字，与我在原始测试中看到的完全不同。
所以它看起来像：
时间壮举1壮举2
20240201 10 3
20240202 5 4
20240203 7 2
...

高精度预测
时间壮举1壮举2
20240201 10 3
20240202 5 4
20240203 7 2
...
20240301 0 0
20240302 0 0
20240203 0 0

每行预测相同的数字]]></description>
      <guid>https://stackoverflow.com/questions/77920944/how-to-predict-future-data-with-xgbregressor</guid>
      <pubDate>Thu, 01 Feb 2024 14:23:54 GMT</pubDate>
    </item>
    <item>
      <title>如何实现医疗疾病聊天机器人？需要帮助开始[关闭]</title>
      <link>https://stackoverflow.com/questions/77920821/how-to-implement-a-medical-disease-chatbot-need-help-starting</link>
      <description><![CDATA[我正在尝试创建一个结合 NLP 和计算机视觉的医疗疾病检测聊天机器人。我完成了 CV 部分，但对如何开始 NLP 部分感到困惑。
我设想的是一个聊天机器人，可以了解您的症状并帮助检测您的问题。我还希望聊天机器人可以选择获取医学 X 射线图像来帮助对疾病进行分类（也许可以将图像的分类与用户症状的分类结合起来）。对于图像分类，我使用了 Kaggle 的 X 射线数据集，并成功创建了一个根据 X 射线对疾病进行分类的模型。
现在，我需要使用 NLP 启动聊天机器人部分，但我不知道从哪里开始..

首先，这个医疗聊天机器人是否可以作为一个副项目来实现？我找不到类似的教程。
在哪里可以找到训练该模型的数据？或者使用像 BERT 这样的东西？
如果这个项目看起来可行，那么第一步是什么？数据怎么样？型号？

我尝试寻找数据集来训练医疗聊天机器人，但找不到任何内容。我在哪里可以找到这样的数据集？我可以微调 BERT 之类的东西吗？]]></description>
      <guid>https://stackoverflow.com/questions/77920821/how-to-implement-a-medical-disease-chatbot-need-help-starting</guid>
      <pubDate>Thu, 01 Feb 2024 14:07:25 GMT</pubDate>
    </item>
    <item>
      <title>安装tensorflowjs（python）时出错</title>
      <link>https://stackoverflow.com/questions/77920474/error-while-installing-tensorflowjs-python</link>
      <description><![CDATA[我已经用 Python 构建了一个 ML 模型，我想在 Web 应用程序中使用它。鉴于这种情况，我正在尝试使用tensorflowjs将模型转换为JSON。我使用 pip install tensorflowjs 安装该软件包，但收到与 tensorflow_decision_forest 相关的错误：
错误：找不到满足tensorflow_decision_forests要求的版本（来自版本：无）
错误：找不到tensorflow_decision_forests的匹配分布
无法加载 inference.so 自定义 C++ 张量流操作。此错误很可能是由于 TensorFlow 和 TensorFlow Decision Forests 的版本不兼容造成的。
我的配置-
使用 conda 和 Python 3.8 版本创建了一个虚拟环境（尝试使用 3.10，但问题仍然存在）。
张量板==2.13.0
张量板数据服务器== 0.7.2
张量流==2.13.0
张量流估计器== 2.13.0
tensorflow-intel == 2.13.0
tensorflow-io-gcs-filesystem == 0.31.0
尝试安装tensorflowjs版本 - 最新（4.17）、4.15、4.2.0和一些非常旧的版本
我尝试寻找解决方案，但没有确切的解决方案，只是知道tensorflow决策森林 仅适用于 Linux。我还尝试了在网上找到的 TensorFlow 库的各种组合，但没有任何效果。 （引用了此）]]></description>
      <guid>https://stackoverflow.com/questions/77920474/error-while-installing-tensorflowjs-python</guid>
      <pubDate>Thu, 01 Feb 2024 13:14:24 GMT</pubDate>
    </item>
    <item>
      <title>如何控制头盔物体检测模型产生的相同警报？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77919736/how-to-control-the-same-alerts-being-produced-by-a-helmet-object-detection-model</link>
      <description><![CDATA[我们有一个头盔对象检测模型，它可以检测是否佩戴头盔的人的天气。如果一个人没有戴头盔，它会通过 MQTT 代理将坐标发送到服务器。
现在，如果一个人没有戴头盔站在一个地方，则会产生连续的相同警报，但不应保存所有警报，因为这是同一个人，否则如果一个人只是移动一英寸或几厘米，则应该保存所有警报亦未得救。我的要求是同一个人的警报不应在数据库中多次保存，从而导致冗余。
从模型中，我们仅获得场景的坐标。
如何处理这种情况或管理同一个人的警报？ （这里我们不知道人脸，因为它只有头盔检测模型）
寻找一种有效的方法来处理这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77919736/how-to-control-the-same-alerts-being-produced-by-a-helmet-object-detection-model</guid>
      <pubDate>Thu, 01 Feb 2024 11:20:27 GMT</pubDate>
    </item>
    <item>
      <title>培训师的表现就像是从头开始培训</title>
      <link>https://stackoverflow.com/questions/77919591/trainer-acts-as-if-its-training-from-scratch</link>
      <description><![CDATA[我正在使用 Huggingface 训练器训练模型，并为 resume_from_checkpoint 参数指定了检查点文件夹。
但是，当它继续训练时，它仍然会使用与第一个保存步骤相对应的名称保存检查点（例如 checkpoint-4，即使 resume_from_checkpoint 应从  开始检查点-4096）。进度条还显示所有 max_steps，尽管我不希望它从头开始。
这是一个常见问题吗？我该如何解决这个问题？
我将训练参数保存在 yaml 文件中：
training_args：
   学习率：!!float 1e-4
   do_train：正确
   每个设备训练批次大小：8
   每设备评估批量大小：8
   记录步骤：1024
   输出目录：/path/to/training_output/
   overwrite_output_dir：假
   删除未使用的列：假
   保存策略：步骤
   评估策略：步骤
   保存步骤：1024
   load_best_model_at_end：真
   热身步数：100
   最大步数：65536
   种子：22
   resume_from_checkpoint：/path/to/checkpoint-4096

然后通过将 TrainingArguments 对象初始化为 **kwargs 来训练模型。
但是终端显示：
将模型检查点保存到 /path/to/checkpoint-4

进度条显示了所有步骤，即使我需要它从步骤 4096 开始。]]></description>
      <guid>https://stackoverflow.com/questions/77919591/trainer-acts-as-if-its-training-from-scratch</guid>
      <pubDate>Thu, 01 Feb 2024 10:58:33 GMT</pubDate>
    </item>
    <item>
      <title>如何安全地将元数据添加到 mlflow 模型？</title>
      <link>https://stackoverflow.com/questions/77919524/how-to-add-metadata-to-a-mlflow-model-safely</link>
      <description><![CDATA[当我训练 ML 模型时，我会得到一些想要存储在模型附近的参数，以便在生产中加载它们。
有一个功能
mlflow.pyfunc.log_model 可以得到元数据作为参数。
但是这并不安全
&lt;块引用&gt;
实验性：此参数可能会在未来版本中更改或删除，恕不另行通知。

我的考虑：

标签。我尝试使用注册模型并将它们添加到标签中，但它似乎不适用于长字符串。如 key = &quot;abc&quot;, value = &quot;abc&quot;*1000
unwrap_python_model()。不，这是实验性的

我该如何解决我的任务？]]></description>
      <guid>https://stackoverflow.com/questions/77919524/how-to-add-metadata-to-a-mlflow-model-safely</guid>
      <pubDate>Thu, 01 Feb 2024 10:47:58 GMT</pubDate>
    </item>
    <item>
      <title>无法启动 Azure Ml studio，这会出现“加载 Azure Ml 工作区时出错”错误</title>
      <link>https://stackoverflow.com/questions/77919192/not-able-to-launch-the-azure-ml-studio-which-gives-me-error-loading-azure-ml-wo</link>
      <description><![CDATA[我有一个位于专用端点后面的 Azure ML 工作区，并且公共访问已被禁用，当我尝试启动工作室时，出现加载工作区错误错误。请参阅下图以供参考：
在此处输入图像描述
我不确定可能出了什么问题。有人可以指导我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77919192/not-able-to-launch-the-azure-ml-studio-which-gives-me-error-loading-azure-ml-wo</guid>
      <pubDate>Thu, 01 Feb 2024 09:59:57 GMT</pubDate>
    </item>
    <item>
      <title>损失在减少，验证准确率正常，但分类报告统计数据很低</title>
      <link>https://stackoverflow.com/questions/77918946/loss-is-decreasing-and-validation-accuracy-is-normal-but-classification-report</link>
      <description><![CDATA[我有一个多类分类任务，用于预测 21 个类的 ui 元素草图的类型。这是我的数据增强代码
&lt;前&gt;&lt;代码&gt;目标大小 = (224, 224)
target_dims = (224, 224, 3) # 添加 RGB 通道
n_类 = 21
val_frac = 0.1
批量大小 = 64

data_augmentor = ImageDataGenerator(samplewise_center=True,
                                    Samplewise_std_normalization=真，
                                    验证分割=val_frac)

train_generator = data_augmentor.flow_from_directory（data_dir，target_size=target_size，batch_size=batch_size，shuffle=True，subset=“训练”）
val_generator = data_augmentor.flow_from_directory（data_dir，target_size=target_size，batch_size=batch_size，subset=“验证”）

此外，这是我的模型的代码
from tensorflow.keras.callbacks import EarlyStopping

# 定义 EarlyStopping 回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

# 创建你的模型
my_model = 顺序()
my_model.add(Conv2D(64，kernel_size=4，strides=1，activation=&#39;relu&#39;，input_shape=target_dims))
my_model.add(Conv2D(64，kernel_size=4，strides=2，activation=&#39;relu&#39;))
my_model.add(Dropout(0.5))
my_model.add(Conv2D(128，kernel_size=4，strides=1，activation=&#39;relu&#39;))
my_model.add(Conv2D(128，kernel_size=4，strides=2，activation=&#39;relu&#39;))
my_model.add(Dropout(0.5))
my_model.add(Conv2D(256，kernel_size=4，strides=1，activation=&#39;relu&#39;))
my_model.add(Conv2D(256，kernel_size=4，strides=2，activation=&#39;relu&#39;))
my_model.add(Dropout(0.5))
my_model.add(Conv2D(512，kernel_size=4，strides=1，activation=&#39;relu&#39;))
my_model.add(Conv2D(512，kernel_size=4，strides=2，activation=&#39;relu&#39;))
my_model.add(压平())
my_model.add(Dropout(0.5))
my_model.add（密集（512，激活=&#39;relu&#39;））
my_model.add（密集（n_classes，激活=&#39;softmax&#39;））

# 编译模型
my_model.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标=[“准确性”]）


所以主要问题是训练期间验证准确率良好（平均约为 73%）。但如果我随后使用这个模型并创建分类报告，我得到的统计数据非常低，f1、精度和准确度约为 0.5%。造成这种行为的原因是什么？数据集或增强是否存在问题？或者这个模型对于这个集合来说太简单了？]]></description>
      <guid>https://stackoverflow.com/questions/77918946/loss-is-decreasing-and-validation-accuracy-is-normal-but-classification-report</guid>
      <pubDate>Thu, 01 Feb 2024 09:24:02 GMT</pubDate>
    </item>
    <item>
      <title>对交叉熵损失真的很困惑[关闭]</title>
      <link>https://stackoverflow.com/questions/77917816/really-confused-about-cross-entropy-loss</link>
      <description><![CDATA[我遇到了我的项目面临的关于交叉熵损失的三个详细实现。顺便说一下，needle 是我要构建的东西，所以你可以把它看作像 pytorch 一样的东西。
那么谁能解释一下为什么？
第一个版本：
def softmax_loss(Z, y_one_hot):
    ”“”返回softmax损失。请注意，出于本次作业的目的，
    你不需要担心“很好”缩放数值属性
    log-sum-exp 计算的一部分，但可以直接计算它。

    参数：
        Z (ndl.Tensor[np.float32]): 形状的 2D 张量
            (batch_size, num_classes)，包含 logit 预测
            每堂课。
        y (ndl.Tensor[np.int8]): 形状的 2D 张量 (batch_size, num_classes)
            每个示例的真实标签索引处包含 1，并且
            其他地方为零。

    返回：
        样本上的平均 softmax 损失。 (ndl.张量[np.float32])
    ”“”
    ### 开始你的解决方案
    # 假设二元选择，之前 y 表示真实标签，例如
    # [0, 1, 0, 1, 0]
    # 返回 np.mean(np.log(np.sum(np.exp(Z), axis=1)) - Z[np.arange(y.size), y])
    #
    # 然而，这个问题应用了 ndl 并使用 one-hot y 作为张量，例如
    # [[1, 0],
    # [0, 1],
    # [1, 0],
    # [0, 1],
    # [1, 0]]
    # 这种情况下，直接执行矩阵乘法。
    #
    # 顺便说一下，如果这里设置axes=1的话，是不会通过测试的。这是因为轴应该是
    # 这里是一个可迭代对象，如果没有优化， int 绝不是可迭代的，因此我们应用
    # 元组轴=(1,)。
    lhs = ndl.log(ndl.exp(Z).sum(axes=(1,))).sum() # 求和到 (B, 1)，然后是标量
    rhs = (y_one_hot * Z).sum() # EW (B, k) 然后是标量
    
    return (lhs - rhs) / Z.shape[0] # 除以批量大小
    ### 结束你的解决方案

第二个版本：
def softmax_loss(Z, y):
    ”“”返回softmax损失。请注意，出于本次作业的目的，
    你不需要担心“很好”缩放数值属性
    log-sum-exp 计算的一部分，但可以直接计算它。

    参数：
        Z (np.ndarray[np.float32]): 形状的 2D numpy 数组
            (batch_size, num_classes)，包含 logit 预测
            每堂课。
        y (np.ndarray[np.uint8]): 形状为 (batch_size, ) 的一维 numpy 数组
            包含每个示例的真实标签。

    返回：
        样本上的平均 softmax 损失。
    ”“”
    ### 开始你的代码
    # 例如，假设 exp_logits 和 y 是
    # [[0.3, 0.2, 0.5] [2, 1, 1, 1]
    # [0.1, 0.6, 0.3]
    # [0.4, 0.3, 0.3]
    # [0.1, 0.7, 0.2]]
    # 高级索引从行[0, 1, 2, 3]和列[2, 1, 1, 1]获取元素，
    # 表示真实分类的预测点。
    lhs = np.log(np.sum(np.exp(Z), axis=1)) # 求和为形状 (B, 1)
    rhs = Z[np.arange(y.size), y] # 形状 (B, 1)
    # 这是预测点相减然后求平均值；
    # 如果先求均值再减，就减一。
    avg_loss = np.mean(左轴 - 右轴)
    
    返回平均损失
    ### 结束你的代码

第三个版本：
&lt;前&gt;&lt;代码&gt;
/* 由于矩阵在 C++ 中表示为数组，因此它将指向每次迭代的开始。 */
const float *X_batch = &amp;X[iter * batch * n];

/* 要计算 exp_logits，首先初始化形状为 (B, k) = (B, n) * (n, k) 的数组。 */
float *exp_logits = new float[batch * k];
mat_mul(X_batch, theta, exp_logits, 批次, n, k); // 实际上在这里记录
/* 在数组的 from 中对 (B, k) exp_logits 进行一一指数运算。 */
for (size_t i = 0; i &lt; 批 * k; i++) exp_logits[i] = exp(exp_logits[i]);

/* 对 axis=1 的矩阵求和，即在 &#39;k&#39; 的维度上。 */
for (size_t i = 0; i &lt; 批次; i++) {
    浮点总和=0；
    for (size_t j = 0; j &lt; k; j++) sum += exp_logits[i * k + j];
    for (size_t j = 0; j &lt; k; j++) exp_logits[i * k + j] /= sum; // grad 实际上在这里
}
/* exp_logits 总是在迭代中更新，但是我们需要预先为 y 添加 iter*batch。
    * 例如，假设 exp_logits 和 y 是
    * [[0.3, 0.2, 0.5] [[2]
    * [0.1, 0.6, 0.3] [1]
    * [0.4, 0.3, 0.3] [1]
    * [0.1, 0.7, 0.2]] [1]]
    * 减去后，损失将如下所示
    * [[0.3, 0.2, -0.5]
    * [0.1,-0.4,0.3]
    * [0.4，-0.7，0.3]
    * [0.1,-0.3,0.2]]
    */
for (size_t i = 0; i &lt; 批次; i++) exp_logits[i * k + y[iter * 批次 + i]] -= 1;
]]></description>
      <guid>https://stackoverflow.com/questions/77917816/really-confused-about-cross-entropy-loss</guid>
      <pubDate>Thu, 01 Feb 2024 05:04:35 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：函数“NativeBatchNormBackward0”在其第 0 个输出中返回了 nan 值[关闭]</title>
      <link>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</link>
      <description><![CDATA[我尝试从一篇提供 Tensorflow 代码的论文中实现一个卷积神经网络，并将其转换为 Pytorch。
我尝试重现的是：
https://github.com/akensert/deep-learning-peak-检测/树/主
这个想法是将 8192 个元素数组交给神经网络，神经网络检测不同的峰值及其沿该数组的位置。存储库提供了两个示例数组，但我无法运行神经网络，因为第一层已经产生错误。我在下面的示例中包含了第一层，它重现了与以下层完全相同的错误行为。
使用 torch.autograd.detect_anomaly() 我收到错误：
RuntimeError：函数“NativeBatchNormBackward0”在第 0 个输出中返回了 nan 值。
第一次调用loss.backward()期间。
我无法找出此错误背后的原因，因为我在网络上找不到任何有关它的信息。
所讨论的架构是在 conv1d() 期间 -&gt; BatchNorm1d() -&gt;; ReLU() -&gt;; MaxPool1d() 顺序。如果我注释掉 BatchNorm1d() 层，则不会发生错误。
我运行一个自定义损失函数，根据三个特征的两个 BCELosses 和一个 MSELoss 计算加权损失。
损失本身以数字形式返回，并且表现符合预期。
这是一个使用第一个卷积层和自定义损失重现代码的示例：
导入火炬
将 torch.nn 导入为 nn
将 numpy 导入为 np


类 CustomLoss(torch.nn.Module):
    def __init__(自身):
        超级().__init__()

    def 前向（自我，y_pred，y_true，n_splits，weight_prob = 1.0，weight_loc = 1.0，weight_area = 1.0）：
        y_true = torch.Tensor(y_true)
        y_pred = torch.Tensor(y_pred)

        pred_prob、pred_loc、pred_area = torch.tensor_split(y_pred、n_splits、dim=1)
        true_prob、true_loc、true_area = torch.tensor_split(y_true、n_splits、dim=1)

        掩码 = true_prob.eq(1.)

        prob_loss = torch.nn.BCELoss()(true_prob, pred_prob)
        loc_loss = torch.nn.BCELoss()(
            torch.masked_select(true_loc, mask), torch.masked_select(pred_loc, mask))
        area_loss = torch.nn.MSELoss()(
            torch.masked_select(true_area, mask), torch.masked_select(pred_area, mask))

        返回 （
                概率损失 * 权重概率 +
                loc_loss * 权重_loc +
                面积损失 * 重量面积
        ）


类 PeakDetection(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.n_splits = 3
        self.conv_block1 = nn.Sequential(
            nn.Conv1d(in_channels=1,
                      输出通道=3，
                      内核大小=9，
                      步幅=2，
                      填充=4),
            nn.BatchNorm1d(3),
            ReLU(),
            nn.MaxPool1d（内核大小=16）
        ）

    def 前向（自身，x）：
        输出 = self.conv_block1(x)
        输出 = self.CustomActivation(输出)
        返回输出

    def CustomActivation（自身，输入）：
        pred, loc, 区域 = torch.tensor_split(输入, self.n_splits, dim=1)
        pred = torch.sigmoid(pred)
        loc = torch.sigmoid(loc)
        return torch.concat([pred,loc,area],dim=1)


torch.autograd.detect_anomaly(True)
# torch.manual_seed(42)
模型 = PeakDetection()

优化器 = torch.optim.Adam(params=model.parameters(), lr=0.01)

X = np.ones((32, 1, 8192))
y = np.ones((32, 3, 256))

使用 torch.autograd.detect_anomaly()：
    y_pred = 模型(火炬.张量(X))
    损失 = CustomLoss()(y_pred, torch.Tensor(y), n_splits=3)
    优化器.zero_grad()
    loss.backward()
    优化器.step()

manual_seed(42) 总是会产生相关的 NativeBatchNormBackward0 错误。]]></description>
      <guid>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</guid>
      <pubDate>Wed, 31 Jan 2024 14:33:58 GMT</pubDate>
    </item>
    <item>
      <title>从预测数组重建图像 - 填充相同显示重建图像中的网格图块</title>
      <link>https://stackoverflow.com/questions/77878901/image-reconstruction-from-predicted-array-padding-same-shows-grid-tiles-in-rec</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77878901/image-reconstruction-from-predicted-array-padding-same-shows-grid-tiles-in-rec</guid>
      <pubDate>Thu, 25 Jan 2024 09:43:55 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float” - hdbscanValidity_index</title>
      <link>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</link>
      <description><![CDATA[我使用hdbscan包中的有效性索引，它根据以下论文实现DBCV评分：
https://www.dbs.ifi.lmu.de /~zimek/publications/SDM2014/DBCV.pdf
我正在做一个人脸聚类项目，使用有效性索引后提示错误。
这是代码：
dbcv_score_output = hdbscan.validity.validity_index(feature_vectors, archive_labels)
dbcv_分数_输出

完整错误：
hdbscan/validity.py:30: Ru​​ntimeWarning: 电源遇到溢出
  距离矩阵[距离矩阵！= 0] = (1.0 / 距离矩阵[

-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:371，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第356章 继续
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
--&gt;第371章
    [第 372 章]
    [第 374 章]

文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:165，在internal_minimum_spanning_tree(mr_distances)中
    136 def 内部最小跨度树（mr_distances）：
    第137章
    第138章
    139 个可达距离。给定最小生成树“内部”
   （...）
...
    167 为索引，枚举中的行(min_span_tree[1:], 1)：

文件 hdbscan/_hdbscan_linkage.pyx:15，在 hdbscan._hdbscan_linkage.mst_linkage_core()

ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float”

快速浏览输入及其类型：

特点：
dtype=float32
形状：（70201、320）


档案/集群（它是标签编码的）：
形状：(70201，)


当我尝试将功能类型更改为 double/float64 时，它显示了不同类型的错误：
hdbscan/validity.py:33: RuntimeWarning: true_divide 中遇到无效值
  结果 /= distance_matrix.shape[0] - 1
-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:372，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
    第371章
--&gt; [第 372 章]
    [第 374 章]
    第376章

文件〜/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:40，在_amax（a，轴，out，keepdims，初始，其中）
     38 def _amax(a, axis=None, out=None, keepdims=False,
     39 初始=_NoValue，其中=True）：
---&gt; 40 return umr_maximum(a, axis, None, out, keepdims, initial, where)

ValueError：零大小数组到没有标识的缩减操作最大值

我检查了存储库中的所有相关问题和修复，但没有效果。有什么建议或修复吗？]]></description>
      <guid>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</guid>
      <pubDate>Sat, 13 May 2023 13:04:50 GMT</pubDate>
    </item>
    <item>
      <title>K-Fold交叉验证的应用和部署</title>
      <link>https://stackoverflow.com/questions/72319891/application-and-deployment-of-k-fold-cross-validation</link>
      <description><![CDATA[K 折叠交叉验证是一种用于将数据分割为 K 折叠以进行测试和训练的技术。目标是估计机器学习模型的通用性。该模型经过 K 次训练，每个训练折叠训练一次，然后在相应的测试折叠上进行测试。
假设我想在某个具有 10 折的任意数据集上比较决策树和逻辑回归模型。假设在对 10 个折叠中的每个折叠训练每个模型并获得相应的测试精度后，逻辑回归在测试折叠中具有更高的平均精度，表明它是数据集更好的模型。
现在，进行应用和部署。我是在所有数据上重新训练 Logistic 回归模型，还是从在 K-Fold 上训练的 10 个 Logistic 回归模型创建一个整体？]]></description>
      <guid>https://stackoverflow.com/questions/72319891/application-and-deployment-of-k-fold-cross-validation</guid>
      <pubDate>Fri, 20 May 2022 13:39:25 GMT</pubDate>
    </item>
    <item>
      <title>在 bert 上训练新数据集</title>
      <link>https://stackoverflow.com/questions/69423258/training-a-new-dataset-on-bert</link>
      <description><![CDATA[我有一个亚马逊评论数据集，我想根据评论预测星级
我知道我可以使用预训练的 bert 模型，如下所示此处
但我想在我自己的数据集上训练 bert 模型。这就是这里正在做的事情 ？我可以在任何数据集的预训练模型上应用这种类型的“微调”以获得更准确的结果，还是我必须做其他事情来从头开始训练模型
如果我确实想从头开始训练模型，我会从哪里开始]]></description>
      <guid>https://stackoverflow.com/questions/69423258/training-a-new-dataset-on-bert</guid>
      <pubDate>Sun, 03 Oct 2021 08:30:00 GMT</pubDate>
    </item>
    <item>
      <title>您可以使用特定于任务的架构从头开始训练 BERT 模型吗？</title>
      <link>https://stackoverflow.com/questions/61826824/can-you-train-a-bert-model-from-scratch-with-task-specific-architecture</link>
      <description><![CDATA[基础模型的 BERT 预训练是通过语言建模方法完成的，我们在句子中屏蔽了一定比例的标记，然后让模型学习那些缺失的屏蔽。然后，我认为为了完成下游任务，我们添加一个新初始化的层并对模型进行微调。
但是，假设我们有一个巨大的句子分类数据集。理论上，我们是否可以从头开始初始化 BERT 基础架构，仅用这个句子分类数据集从头开始训练附加的下游任务特定层 + 基础模型权重，并且仍然取得良好的结果？]]></description>
      <guid>https://stackoverflow.com/questions/61826824/can-you-train-a-bert-model-from-scratch-with-task-specific-architecture</guid>
      <pubDate>Fri, 15 May 2020 19:21:56 GMT</pubDate>
    </item>
    </channel>
</rss>