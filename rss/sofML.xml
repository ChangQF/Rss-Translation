<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 12 Dec 2024 09:19:15 GMT</lastBuildDate>
    <item>
      <title>使用 peft 进行微调时出错</title>
      <link>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</guid>
      <pubDate>Thu, 12 Dec 2024 07:31:09 GMT</pubDate>
    </item>
    <item>
      <title>根据线条粗细、文本大小和背景颜色对图像进行分类</title>
      <link>https://stackoverflow.com/questions/79273650/classify-images-based-on-line-thickness-text-size-and-background-color</link>
      <description><![CDATA[我正在尝试找到一种方法，根据图像是否适合印在衣服上来对图像进行分类，最好使用 Python 库。我主要想做的是找到文本大小和颜色以及一般的线条粗细和背景颜色 - 当背景颜色较暗时，需要大量墨水。如果图像的线条非常细，颜色较浅或为白色，墨水就会“侵入”这些线条，图像将无法识别。检测文本及其大小/线条粗细将是一个不错的开始。我有成千上万张“好”和“坏”图像，因此训练 AI/ML 模型是一种选择，但在我看来，图像分析库可能已经足够好了。欢迎所有选项。]]></description>
      <guid>https://stackoverflow.com/questions/79273650/classify-images-based-on-line-thickness-text-size-and-background-color</guid>
      <pubDate>Thu, 12 Dec 2024 02:14:00 GMT</pubDate>
    </item>
    <item>
      <title>Softmax 函数的哪种表示是正确的？[关闭]</title>
      <link>https://stackoverflow.com/questions/79272621/which-representation-of-the-softmax-function-is-correct</link>
      <description><![CDATA[我偶然发现了 Softmax 函数的公式：
Softmax 公式
但是，我发现了 Softmax 函数的两个非常不同的视觉表示，我不知道哪一个是正确的：

类似 S 形的曲线：随着输入的增长，一条曲线从 0 增加到 1。它看起来类似于 Sigmoid 函数。
类似 Sigmoid 的曲线

多类归一化输出：显示多个类的概率如何随不同输入而变化的图表，其中概率总和为 1。
多类归一化输出



第一张图（类似 Sigmoid 的曲线）是否正确表示了 Softmax 函数，还是仅描述特定条件下的一个组件？
Softmax 是否应始终表示为跨多个类的概率分布（如第二张图所示）图表）？

我正在寻找有关如何正确解释和表示 Softmax 函数的清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/79272621/which-representation-of-the-softmax-function-is-correct</guid>
      <pubDate>Wed, 11 Dec 2024 17:19:50 GMT</pubDate>
    </item>
    <item>
      <title>图像分类的核心 ML 预测创建 ml 模型对不同的图像预测相同的结果</title>
      <link>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</link>
      <description><![CDATA[我正在探索 Apple Core ML 框架。
我使用 Create ML 应用创建了一个训练模型。图像分类用于识别图像是猫还是狗。我使用的数据集来自
https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?resource=download
我通过提供训练数据从 create ml 创建了我的 .mlmodel 文件。
现在我在应用程序中进行预测，它给出了相同的目标和概率结果以及不同的图像（猫或狗，给出相同的结果 - [&quot;cats&quot;: 0.6281524444894766, &quot;dogs&quot;: 0.3718475555105234]
应用程序代码：
override func viewDidLoad() {
if let img = UIImage(named: &quot;dog_29.jpg&quot;) {
predictImage(image: img)
} else {
print(&quot;image not caught&quot;)
}
}

private func predictImage(image: UIImage) {

let inputImageSize: CGFloat = 299.0
let minLen = min(image.size.width, image.size.height)
let resizedImage = image.resize(to: CGSize(width: inputImageSize * image.size.width / minLen, height: inputImageSize * image.size.height / minLen))

guard let pixelBuffer = resizedImage.pixelBuffer() else {
fatalError()
}

do {
let config = MLModelConfiguration()
let model = try CatsAndDogs(configuration:config)
let result = try model.prediction(image: pixelBuffer)
print(result.target)
print(result.targetProbability)

print(result)
} catch {
print(&quot;image category error&quot;)
}

}

resize 和 getcvbuffer 的代码：
 func resize(to newSize: CGSize) -&gt; UIImage {
UIGraphicsBeginImageContextWithOptions(CGSize(width: newSize.width, height: newSize.height), true, 1.0)
self.draw(in: CGRect(x: 0, y: 0, width: newSize.width, height: newSize.height))
let resizedImage = UIGraphicsGetImageFromCurrentImageContext()!
UIGraphicsEndImageContext()

return resizedImage
}

func pixelBuffer() -&gt; CVPixelBuffer? {
let width = self.size.width
let height = self.size.height
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary
var pixelBuffer: CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault,
Int(width),
Int(height),
kCVPixelFormatType_32ARGB,
attrs,
&amp;pixelBuffer)

保护让 resultPixelBuffer = pixelBuffer, status == kCVReturnSuccess else {
return nil
}

CVPixelBufferLockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(resultPixelBuffer)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
保护让 context = CGContext(data: pixelData,
width: Int(width),
height: Int(height),
bitsPerComponent: 8,
bytesPerRow: CVPixelBufferGetBytesPerRow(resultPixelBuffer),
space: rgbColorSpace,
bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue) else {
return nil
}

context.translateBy(x: 0, y: height)
context.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context)
self.draw(in: CGRect(x: 0, y: 0, width: width, height: height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))

return resultPixelBuffer
}

我也尝试了网络上可用的各种调整大小和 getcvbuffer 的方法，但结果都一样。
我尝试了数据集链接中测试文件夹中的猫和狗的不同图像。结果仍然相同。
为什么预测不正确？]]></description>
      <guid>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</guid>
      <pubDate>Wed, 11 Dec 2024 11:16:12 GMT</pubDate>
    </item>
    <item>
      <title>我实现的 KMeans 的结果并不一致</title>
      <link>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</link>
      <description><![CDATA[我正在尝试实现一个简化版的 KMeans 聚类，但不知何故，结果有时不一致
import numpy as np
import random

import matplotlib.pyplot as plt

class KMeans:
def __init__(
self,
n_clusters,
max_iter
):
self.n_clusters = n_clusters
self.max_iter = max_iter

def _get_distance(self, x, cluster_location):
&quot;&quot;&quot;计算每个点到聚类中心的欧几里得距离
&quot;&quot;&quot;
return np.linalg.norm(x[:,np.newaxis,:]-cluster_location, axis = 2)

def fit(self, x:np.ndarray) -&gt;无：
“”k 均值聚类步骤
1. 启动聚类位置
2. 计算每个点到聚类点的距离
3. 将每个点分配给一个聚类
4. 使用相关点的平均值更新聚类位置
5. 重复 2-4 直到收敛或达到 max_iter

参数：
x (_type_)：_description_

返回：
_type_：_description_
“”
self.x = x
data_dim = x.shape[1]

# 1. 初始化簇位置
cluster_locations = np.random.uniform(x.min(), x.max(), size=(self.n_clusters,data_dim))
# print(&quot;initial:\n&quot;,cluster_locations)

for _ in range(self.max_iter):
# 2. 计算每个点到簇点的距离
distances = self._get_distance(x, cluster_locations)

# 3. 将每个点分配给一个簇
clusters = np.argmin(distances, axis=1)

# 4. 使用关联点的平均值更新簇位置
for cluster in range(self.n_clusters):
# 检查簇中是否有任何点
cluster_mask = clusters == cluster
if np.any(cluster_mask):
cluster_locations[cluster] = np.mean(x[cluster_mask], axis=0)
else:
# 如果簇为空，则用随机点重新初始化
cluster_locations[cluster] = x[np.random.randint(x.shape[0])] 

self.cluster_locations = cluster_locations
self.clusters = clusters
return None

def visualize(self, data, clusters):
_, ax = plt.subplots(1,1,figsize=(5,5))

cluster_color = [(random.random(),random.random(),random.random()) for _ in range(self.n_clusters)]

for cluster in range(self.n_clusters):
to_plot = data[np.where(clusters == cluster)[0]]
ax.scatter(to_plot[:,0], to_plot[:,1], color=cluster_color[cluster])
ax.scatter(self.cluster_locations[:,0], self.cluster_locations[:,1], marker=&quot;x&quot;, color=&#39;r&#39;, s=30)

plt.show()

这是我使用它的方式
from sklearn.datasets import make_blobs

random_state = 42
n_samples = 100

x, _ = make_blobs(n_samples=n_samples, random_state=random_state)
my_kmeans = KMeans(3, 50)
my_kmeans.fit(x)
my_kmeans.visualize(x, my_kmeans.clusters)

大多数时候，它都会给我一个合理的输出，像这样

但每运行几次，它就会给我类似这样的结果

我是否遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</guid>
      <pubDate>Wed, 11 Dec 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>激活函数：二元分类中的 tanh(z)</title>
      <link>https://stackoverflow.com/questions/79270936/activation-function-tanhz-in-binary-classification</link>
      <description><![CDATA[我在二元分类中使用 tanh 激活函数，但我不知道如何处理损失函数。这是我的代码
class TanhNeuron():
def __init__(self): # w : 增加，b：部分
self.w=None
self.b=None

def forpass(self,x): 
z=np.sum(x*self.w)+self.b
return z

def backprop(self,x,err): # 梯度
w_grad=x*err
b_grad=1*err
return w_grad,b_grad

defactivation(self,z): # 激活函数：tanh(z)
a=np.tanh(z)
return a

def fit(self,x,y,epochs=1000):
self.w=np.ones(x.shape[1]) 
self.b=0 
for i in range(epochs):
for x_i,y_i in zip(x,y):
z=self.forpass(x_i)
a=self.activation(z)
err=-(y_i/a+(y_i-1)/(1-a))*(1-a**2) #问题出在这里...
w_grad,b_grad=self.backprop(x_i,err)
self.w-=w_grad
self.b-=b_grad

def predict(self,x):
z=[self.forpass(x_i) for x_i in x]
a=self.activation(np.array(z))
return a&gt;0.5

我使用测试用例打印了准确率。这是我的代码。
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer=load_breast_cancer()

x=cancer.data
y=cancer.target
x_train,x_test,y_train,y_test=train_test_split(x,y,stratify=y,test_size=0.2,random_state=42)

tanhneuron=TanhNeuron()
tanhneuron.fit(x_train,y_train)
print(np.mean(tanhneuron.predict(x_test)==y_test))

输出准确率为 0.36。
这意味着没有发生学习。]]></description>
      <guid>https://stackoverflow.com/questions/79270936/activation-function-tanhz-in-binary-classification</guid>
      <pubDate>Wed, 11 Dec 2024 08:08:13 GMT</pubDate>
    </item>
    <item>
      <title>交替最小二乘训练与推理</title>
      <link>https://stackoverflow.com/questions/79270892/alternating-least-squares-training-vs-inference</link>
      <description><![CDATA[我使用隐式 Python 包中的 als.AlternatingLeastSquares。虽然对稀疏 CSR 矩阵进行训练非常高效，大约需要 10-15 分钟（使用默认的潜在参数和迭代次数），但推理需要一小时多一点的时间。这是正常的吗？如何优化推理？通常 ML 算法恰恰相反：训练比推理花费的时间长得多。训练和推理都针对几百万用户。
我尝试减少潜在参数的数量，但模型质量显然变差了。]]></description>
      <guid>https://stackoverflow.com/questions/79270892/alternating-least-squares-training-vs-inference</guid>
      <pubDate>Wed, 11 Dec 2024 07:53:33 GMT</pubDate>
    </item>
    <item>
      <title>如何对具有大量类别的商品特征进行编码以进行推荐</title>
      <link>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</link>
      <description><![CDATA[对于我正在研究的推荐问题，有大约 50000 个独特品牌和 3 级产品类别，level_1_cat（50 个类别）、level_2_cat（100 个类别）和 level_3_cat（1000 个类别）。所有这些项目特征仅由整数表示。到目前为止，我已经为我的 lightfm 模型尝试了二进制编码、标签编码和目标编码。使用二进制编码和标签编码，结果比不使用任何项目特征更差。使用目标编码，结果与不使用任何项目特征相似。我想知道我还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</guid>
      <pubDate>Wed, 11 Dec 2024 06:39:00 GMT</pubDate>
    </item>
    <item>
      <title>无法访问自由变量“fig”，因为它与封闭范围内的值没有关联</title>
      <link>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</guid>
      <pubDate>Wed, 11 Dec 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 PyTorch 模型（例如 GFPGAN）进行跟踪？</title>
      <link>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</link>
      <description><![CDATA[我在设置 GFPGAN PyTorch 模型进行跟踪时遇到了麻烦，因此我将其转换为 CoreML 模型。我的理解是，我还需要定义一个从 Torch.nn.Module 继承的类，该类定义了 GPFGAN 模型的结构，并且 .pth 文件包含权重。但我不知道在哪里可以找到定义从 Torch.nn.Module 继承的类的信息。我是不是漏掉了什么？
我在 GFPGAN python 库中找到了从 Torch.nn.Module 继承的一些类，但它们在尝试将状态字典加载到其中时都会导致错误。
这是我的代码：
import torch
from gfpgan.archs import gfpganv1_clean_arch

model_path = &#39;GFPGANv1.4.pth&#39;
traceable_model = gfpganv1_clean_arch.GFPGANv1Clean(out_size=512)
traceable_model.load_state_dict(torch.load(model_path)) # 此处发生错误。
traceable_model.eval()
trace = torch.jit.trace(traceable_model, input_batch)

我应该如何加载 GFPGANv1.4 模型进行跟踪？]]></description>
      <guid>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</guid>
      <pubDate>Wed, 11 Dec 2024 00:00:30 GMT</pubDate>
    </item>
    <item>
      <title>了解 XGboost 中的 nrounds [关闭]</title>
      <link>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</link>
      <description><![CDATA[我不明白参数 nrounds。我正在使用 R 并使用包 xgboost 创建一个“极端”梯度提升回归模型。
有一个名为 nrounds 的参数，我的理解是它设置了梯度提升模型（也称为提升迭代）中的树的数量。但是当我设置 nrounds = 0 时，我并没有发现所有预测都等于 base_score（梯度提升模型中的初始猜测）。
为什么会这样？
注意：
我并不是想要一个有 0 棵树的 xgboost 模型，但我尝试用它来检查我对 nrounds 的理解。
我有一个 data.frame，我将其转换为 xgb.DMatrix，然后训练模型：
params &lt;- list(objective = &quot;reg:squarederror&quot;, eval_metric = &quot;rmse&quot;, base_score = 0.5)

bst_model &lt;- xgb.train(params = params, data = dtrain, nrounds = 0, verbose = 1)

使用预测我得到：
pred &lt;- predict(bst_model, newdata = dtrain)
print(pred[1:10])

[1] 1052663 874001 1498940 1991579 2316396 1086949 874001 1432935 1086949 2351261

我预期的位置：
[1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
]]></description>
      <guid>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</guid>
      <pubDate>Mon, 02 Sep 2024 09:36:49 GMT</pubDate>
    </item>
    <item>
      <title>pytorch torchvision.datasets.ImageFolder FileNotFoundError：未找到类 .ipynb_checkpoints 的有效文件</title>
      <link>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</link>
      <description><![CDATA[尝试在 Colab 中使用 pytorch torch.datasets.ImageFolder 加载训练数据。
transform = transforms.Compose([transforms.Resize(400),
transforms.ToTensor()])
dataset_path = &#39;ss/&#39;
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

我遇到了以下错误：
-------------------------------------------------------------------------------
FileNotFoundError Traceback (most recent call last)
&lt;ipython-input-27-7abcc1f434b1&gt; in &lt;module&gt;()
2 transforms.ToTensor()])
3 dataset_path = &#39;ss/&#39;
----&gt; 4 dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
5 dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

3 帧
/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in make_dataset(directory, class_to_idx, extensions, is_valid_file)
100 if extensions 不为 None:
101 msg += f&quot;支持的扩展名是：{&#39;, &#39;.join(extensions)}&quot;
--&gt; 102 raise FileNotFoundError(msg)
103 
104 return entities

FileNotFoundError: 未找到类 .ipynb_checkpoints 的有效文件。支持的扩展名为：.jpg、.jpeg、.png、.ppm、.bmp、.pgm、.tif、.tiff、.webp

我的数据集文件夹包含一个子文件夹，其中包含许多 png 格式的训练图像，但 ImageFolder 仍然无法访问它们。]]></description>
      <guid>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</guid>
      <pubDate>Fri, 02 Jul 2021 17:31:32 GMT</pubDate>
    </item>
    <item>
      <title>对大量分类特征进行编码的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/67197522/what-is-the-best-way-to-encode-a-large-number-of-categorical-features</link>
      <description><![CDATA[我正在尝试制作一个小型数据科学工具（有点像 WEKA 的迷你版）。现在，我有这些具有大量特征（70-100+）的数据集，它们大多是分类的。我正在使用 Python sklearn 进行机器学习逻辑，我需要根据我得到的 sklearn 错误将这些类别转换为数值。
鉴于此，One Hot Encoding 不是一种选择，因为它会过度扩大维度。
我研究过其他可能有效的方法，如频率编码、标签编码等。但我真的不确定在我的情况下该选择什么。
此外，WEKA 实际上如何处理这些问题？我在 WEKA 中输入了我的数据集，它们运行良好，给了我很好的结果！]]></description>
      <guid>https://stackoverflow.com/questions/67197522/what-is-the-best-way-to-encode-a-large-number-of-categorical-features</guid>
      <pubDate>Wed, 21 Apr 2021 14:07:19 GMT</pubDate>
    </item>
    <item>
      <title>数据集特征编码和缩放</title>
      <link>https://stackoverflow.com/questions/65028379/dataset-features-encoding-and-scaling</link>
      <description><![CDATA[我有一个包含非序数分类特征的数据集。在训练机器学习模型（线性 SVC）之前，对它们进行转换（编码 + 缩放）的最佳方法是什么？
我尝试过的方法：

标签编码 - 这种方法有效。但缩放毫无意义，因为特征中的不同类别没有任何特定顺序。

独热编码 - 特征中有数千个唯一类别，这会通过创建数千列使 ML 模型变得复杂。

计数编码 - 我的训练测试拆分没有训练集中特征的所有唯一类别，当我对这些特征进行计数编码时，测试集中会引入 NaN。

]]></description>
      <guid>https://stackoverflow.com/questions/65028379/dataset-features-encoding-and-scaling</guid>
      <pubDate>Thu, 26 Nov 2020 19:45:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在具有分类和数字特征的 Pandas 数据框上应用独热编码？</title>
      <link>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</link>
      <description><![CDATA[某些特征是数值的，例如“学校毕业率”，而其他特征则是分类的，例如学校名称。我对分类特征使用了标签编码器，将它们转换为整数。
我现在有一个包含浮点数和整数的数据框，分别表示数值特征和分类特征（使用标签编码器转换）。
我不确定如何使用学习器，我需要使用独热编码吗？如果需要，我该怎么做？根据我目前的理解，由于数据框中有浮点数，因此我无法简单地将数据框传递给 sklearn OneHotEncoder。我是否只需将标签编码器应用于所有特征即可解决问题？
来自我的数据框的示例数据。 OPEID 和 opeid6 使用标签编码器进行转换]]></description>
      <guid>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</guid>
      <pubDate>Wed, 31 Aug 2016 20:06:49 GMT</pubDate>
    </item>
    </channel>
</rss>