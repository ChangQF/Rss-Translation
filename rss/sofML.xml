<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 12 Aug 2024 03:17:53 GMT</lastBuildDate>
    <item>
      <title>Optuna XGBoost 未使用 Mac 的所有 CPU</title>
      <link>https://stackoverflow.com/questions/78859768/optuna-xgboost-not-using-all-of-macs-cpu</link>
      <description><![CDATA[我正在将 Optuna 与 mySQL 一起运行，以尝试实现并行化并使用更多 Mac 的 CPU。例如，当我运行 GridSearchCV 时，我的用户 CPU 使用率将上升到 90%，并且风扇会启动。但是当我使用 Optuna 时，我得到大约 30% 并且没有风扇。这表明它没有被利用。
我尝试使用 mySQL 和分发在 VSCode 的 Jupyter Notebook 上运行两个处理。我这样做的方式是在不同的内核上制作我的笔记本的两个副本，然后通过加载研究在同一个 SQL 数据库上运行优化代码。也许这不是正确的做法？因为在示例中他们在两个终端上运行了 foo.py？
这是我的代码：
def objective(trial, X_train, y_train, X_test, y_test):
# 定义超参数搜索空间
params = {
&#39;n_estimators&#39;: trial.suggest_int(&#39;n_estimators&#39;, 100, 5000),
&#39;max_depth&#39;: trial.suggest_int(&#39;max_depth&#39;, 2, 20),
&#39;learning_rate&#39;: trial.suggest_float(&#39;learning_rate&#39;, 0.01, 0.2),
&#39;subsample&#39;: trial.suggest_float(&#39;subsample&#39;, 0.7, 1.0),
&#39;colsample_bytree&#39;: trial.suggest_float(&#39;colsample_bytree&#39;, 0.6, 1.0),
&#39;min_child_weight&#39;: trial.suggest_int(&#39;min_child_weight&#39;, 1, 15),
&#39;gamma&#39;: trial.suggest_float(&#39;gamma&#39;, 0.0, 0.4),
&#39;lambda&#39;: trial.suggest_float(&#39;lambda&#39;, 1e-8, 1.0, log=True),
&#39;alpha&#39;: trial.suggest_float(&#39;alpha&#39;, 1e-8, 1.0, log=True)
}

# 初始化并训练模型
xgb = XGBRegressor(**params)
xgb.fit(X_train, y_train)

# 预测并计算指标
y_pred = xgb.predict(X_test)
error = max_percent_error(y_test, y_pred)
return error # 返回要最小化的误差

if __name__ == &quot;__main__&quot;:
study = optuna.load_study(study_name=&quot;example&quot;, storage=&quot;mysql://root@localhost/example&quot;)

# 对每个类别和目标执行优化
for category in train_test_splits:
for target_name in train_test_splits[category]:
if target_name in best_params_dict:
continue
print(f&quot;Running Optuna Optimization for target: {target_name} in category: {category}&quot;)

X_train = train_test_splits[category][target_name][&#39;X_train&#39;]
y_train = train_test_splits[category][target_name][&#39;y_train&#39;]
X_test = train_test_splits[category][target_name][&#39;X_test&#39;]

y_test = train_test_splits[category][target_name][&#39;y_test&#39;]

# 使用分布式计算优化研究
study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), 
n_trials=1900, n_jobs=-1)

# 存储为此目标找到的最佳参数
best_params = study.best_params
best_params_dict[category][target_name] = best_params

# 使用最佳参数训练最终模型
xgb_best = XGBRegressor(**best_params)
xgb_best.fit(X_train, y_train)

# 使用测试集评估模型
y_pred = xgb_best.predict(X_test)
test_max_percent_error = max_percent_error(y_test, y_pred)
test_r2_score = r2_score(y_test, y_pred)

print(f&quot;{target_name} 的最佳结果：&quot;)
print(f&quot; 最佳参数：{best_params}&quot;)
print(f&quot; 测试最大百分比误差：{test_max_percent_error:.4f}%&quot;)
print(f&quot; 测试 R^2：{test_r2_score:.4f}\n&quot;)


谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78859768/optuna-xgboost-not-using-all-of-macs-cpu</guid>
      <pubDate>Mon, 12 Aug 2024 01:47:51 GMT</pubDate>
    </item>
    <item>
      <title>将图像叠加在叠加图像上[关闭]</title>
      <link>https://stackoverflow.com/questions/78859435/overlaying-an-image-on-an-overlaying-image</link>
      <description><![CDATA[假设我有来自相机的帧。有一张图像，我想把它放在这个帧上，这样它的视角就会根据帧的变化而变化。我能够使用单应性对象跟踪来实现这一点，但问题是 - 叠加图像会遮挡帧上的所有对象。因此，例如，如果该帧上该图像应该位于的位置有一个人，则该人将不可见。是否有任何算法或 ML 模型可以帮助显示图像所在区域中的障碍物？我尝试使用 CV2 工具（如 findContours）进行扩张和类似操作，但不幸的是，它没有产生预期的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78859435/overlaying-an-image-on-an-overlaying-image</guid>
      <pubDate>Sun, 11 Aug 2024 21:20:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在 HuggingFace 中从头开始重新初始化 GPT XL？</title>
      <link>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</link>
      <description><![CDATA[我试图确认我的 GPT-2 模型是从头开始训练的，而不是使用任何预先存在的预训练权重。这是我的方法：

加载预训练的 GPT-2 XL 模型：我使用 AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;) 加载预训练的 GPT-2 XL 模型，并计算此模型权重的总 L2 范数。
从头开始初始化新的 GPT-2 模型：然后我使用 GPT2Config 从头开始​​使用自定义配置初始化新的 GPT-2 模型。
比较 L2 范数：我计算预训练模型和新初始化模型的权重的 L2 范数。我的假设是，如果临时模型确实是从随机权重初始化的，那么临时模型的 L2 范数应该比预训练模型小得多。

这是代码片段：
import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# 步骤 1：加载预训练的 GPT-2 XL 模型
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# 步骤 2：计算预训练模型权重的 L2 范数
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2预训练模型权重的范数：{pretrained_weight_norm:.2f}&quot;)

# 步骤 3：使用自定义配置从头开始初始化新的 GPT-2 模型
config = GPT2Config(
vocab_size=52000, # 确保这与 tokenizer 的词汇量相匹配
n_ctx=1024, # 上下文窗口大小（模型一次可以看到的 token 数量）
bos_token_id=0, # 序列开始 token
eos_token_id=1, # 序列结束 token
)
model = GPT2LMHeadModel(config)

# 步骤 4：计算刚初始化的模型权重的 L2 范数
scratch_weight_norm = 0.0
for param in model.parameters():
scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;从头开始初始化的模型的总 L2 范数：{scratch_weight_norm:.2f}&quot;)

这种方法是否是确认模型是从头开始训练的有效方法？是否存在任何潜在问题或更好的方法来验证模型没有预先存在的学习权重？
看起来正确
~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.py​​
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 689/689 [00:00&lt;00:00，8.05MB/s]
model.safetensors：100%|██████████████████████████████████████████████████████████████████████████| 6.43G/6.43G [00:29&lt;00:00，221MB/s]
generation_config.json：100%|██████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00，1.03MB/s]
预训练模型权重的总 L2 范数：24542.74
从头初始化模型的总 L2 范数：1637.31
（beyond_scale_div_coeff）

cross: https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905
ref: https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18]]></description>
      <guid>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</guid>
      <pubDate>Sun, 11 Aug 2024 20:27:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习（GAN）生成图像</title>
      <link>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</guid>
      <pubDate>Sun, 11 Aug 2024 20:01:20 GMT</pubDate>
    </item>
    <item>
      <title>与 Google Colab（Tesla T4）相比，我的 GPU（RTX 3070）真的那么慢吗？</title>
      <link>https://stackoverflow.com/questions/78858972/is-my-gpu-rtx-3070-that-slow-when-compared-to-google-colab-tesla-t4</link>
      <description><![CDATA[当使用我的 jupyter 笔记本和 google colab（使用完全相同的笔记本）训练同一模型时，
它们都运行相同的代码，因此它们肯定都使用“cuda”作为设备。
我的本​​地机器总训练时间：72.489 秒
Google Colab 总训练时间：3.115 秒
这是一个巨大的差异。以下是来自我的机器和google colab的“nvidia-smi”输出。
我已重新安装并更新了完整的 pytorch 生态系统，更新了我的驱动程序和 CUDA 驱动程序。
我的机器上的 cuda 中一切都在运行，但不知何故速度很慢。在我的机器中，CPU 实际上比 GPU 更快。
BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count()

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)
train_loader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)
train_loader, test_loader

这就是我的加载器的设置方式。
batch, labels = next(iter(train_loader))
print(f&#39;{batch.size() = }&#39;)
print(f&#39;{labels.size() = }&#39;)

在我这边加载一个批次需要 2-3 秒。而在 google colab 中速度更快。这可能是瓶颈吗？
我应该做什么，还是我的 GPU 真的那么糟糕？]]></description>
      <guid>https://stackoverflow.com/questions/78858972/is-my-gpu-rtx-3070-that-slow-when-compared-to-google-colab-tesla-t4</guid>
      <pubDate>Sun, 11 Aug 2024 17:29:05 GMT</pubDate>
    </item>
    <item>
      <title>如何调试不工作的 Yolov8n 模型？</title>
      <link>https://stackoverflow.com/questions/78858811/how-do-i-debug-yolov8n-model-not-working</link>
      <description><![CDATA[我正在尝试使用预先训练的 yolov8 模型进行对象检测和跟踪。我能够成功加载它，但由于某种原因，当它检测到对象时，它会随机检测到许多与图像无关的不同对象，所有对象的置信度得分均为 1.0。
这是我的代码：
from ultralytics import YOLO
import cv2

#加载 yolov8 模型
model = YOLO(&quot;yolov8n.pt&quot;)

#加载 video()
video_path = &#39;./puppy.mp4&#39;
cap = cv2.VideoCapture(video_path)

ret = True
while ret:
ret,frame = cap.read()
#从视频中返回一个新帧；如果成功读取帧，则 ret 为真，否则为假
如果不是 ret:
break
#检测对象
#跟踪对象
results = model.track(frame,persist=True) #persist= True，因此 YOLO 会记住它之前见过的帧

#绘制结果
frame_ = results[0].plot() #创建用于检测的图像
#也可以使用 cv2.rectangle 和 cv2.putText
#可视化
cv2.imshow(&#39;frame&#39;,frame_)
if cv2.waitKey(25) &amp; 0xFF==ord(&#39;q&#39;):
break

输出：
0：384x640 3 辆汽车、33 辆摩托车、21 架飞机、29 列火车、52 艘船、9 个消防栓、7 个长凳、1 个手提箱、37 个滑雪板、6 个滑雪板、1 个运动球、5 根棒球棒、8 个瓶子、1 个叉子、20 把刀、64 把勺子、1 个马桶、1 个烤面包机、1 把牙刷、329.4 毫秒
速度：4.5 毫秒预处理、329.4 毫秒推理、51.4 毫秒后处理每个形状为 (1, 3, 384, 640) 的图像

0：384x640 1 辆自行车、16 辆汽车、4 辆摩托车、13 架飞机、4公共汽车、9 列火车、9 艘船、8 个交通信号灯、1 个消防栓、10 个停车标志、2 张长椅、3 只羊、2 头牛、1 把雨伞、2 个飞盘、2 根棒球棒、1 副棒球手套、5 支网球拍、2 把刀、2 把勺子、1 根胡萝卜、3 个热狗、5 个披萨、1 个甜甜圈、1 张沙发、1 张床、476.8 毫秒
速度：9.3 毫秒预处理、476.8 毫秒推理、0.0 毫秒后处理每个形状为 (1, 3, 384, 640) 的图像

0：384x640 1 个人、7 辆汽车、3 辆摩托车、20 架飞机、1 辆公共汽车、7 列火车、44 艘船、2 个消防栓、5 个停车标志、3 个停车计费表、2 只鸟、3 头牛、2大象、1 只熊、4 把雨伞、1 个手提包、1 个飞盘、1 把刀、4 把勺子、2 个苹果、1 个胡萝卜、1 个热狗、1 个披萨、1 个蛋糕、1 张餐桌、651.2 毫秒
速度：15.5 毫秒预处理、651.2 毫秒推理、15.8 毫秒后处理每个形状为 (1、3、384、640) 的图像

0：384x640 7 个人、3 辆汽车、1 辆摩托车、2 列火车、13 艘船、2 个交通信号灯、2 个消防栓、54 个停车标志、2 个停车计费表、2 只猫、4 只狗、1 头牛、18 只熊、1 匹斑马、15 只长颈鹿、11 把雨伞、14 个手提包、6 条领带、1 个飞盘、1 个滑雪板、1 个运动球、 5 个棒球手套、4 个酒杯、1 把叉子、1 把刀、1 把勺子、1 个苹果、1 个橙子、1 根胡萝卜、2 个披萨、1 张餐桌、2 台笔记本电脑、1 部手机、1 个水槽、1 个吹风机、656.2 毫秒
速度：0.0 毫秒预处理、656.2 毫秒推理、66.9 毫秒后处理每个形状为 (1, 3, 384, 640) 的图像

0：384x640 20 艘船、8 张长凳、5 只猫、4 个手提箱、48 个飞盘、3 个滑板、20 个碗、1 张沙发、26 张床、37 台笔记本电脑、1 个鼠标、15 个遥控器、30 部手机、27 本书、1057.5 毫秒
速度：15.7 毫秒预处理、1057.5 毫秒推理，形状为 (1, 3, 384, 640) 时每幅图像的后处理时间为 55.0ms

如您所见，当视频中只有一条狗时，它检测到了许多随机物体。
这是我第一次使用 yolo，我是计算机视觉领域的新手。
我也尝试使用终端使用 yolov8n 进行预测，但它仍然检测到许多不同的物体，置信度均为 1.0。
这是我在 CLI 中输入的内容：
 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;
输出：
Ultralytics YOLOv8.2.75 🚀 Python-3.12.3 torch-2.4.0+cpu CPU（第 11 代 Intel Core(TM) i5-1145G7 2.60GHz）
YOLOv8n 摘要（融合）：168 层、3,151,904 个参数、0 个梯度、8.7 GFLOP

将 https://ultralytics.com/images/bus.jpg 下载到&#39;bus.jpg&#39;...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134k/134k [00:00&lt;00:00，8.86MB/s]
图像 1/1 C:\Users\aksha\OneDrive\Documents\Computer_Vision\tutorial_detect\bus.jpg：640x480 79 人、3 辆自行车、46 辆汽车、46 辆摩托车、15 架飞机、11 辆公共汽车、5 列火车、28 辆卡车、49 艘船、4 个消防栓、2 个停车计费表、2 个运动球、6 个瓶子、4 把勺子、324.2ms
速度：15.6ms 预处理、324.2ms 推理、27.8ms 后处理，形状为 (1, 3, 640, 480)，每幅图像
结果保存到 runs\detect\predict
💡 了解更多信息https://docs.ultralytics.com/modes/predict

我的 ultralytics、torch 和 np 版本如下：
8.2.75 - ultralytics
2.4.0+cpu - torch
1.26.4 - numpy
我不确定问题是什么；我尝试卸载并重新安装 ultralytics 两次，但问题没有解决。]]></description>
      <guid>https://stackoverflow.com/questions/78858811/how-do-i-debug-yolov8n-model-not-working</guid>
      <pubDate>Sun, 11 Aug 2024 16:25:30 GMT</pubDate>
    </item>
    <item>
      <title>Epoch 1/3 ^C - model.fit() 以此行终止，且没有任何错误</title>
      <link>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</guid>
      <pubDate>Sun, 11 Aug 2024 13:47:36 GMT</pubDate>
    </item>
    <item>
      <title>获取 ValueError：所有数组的长度必须相同</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[我一直试图将字典转换为数据框，但每次我都收到 ValueError：所有数组的长度必须相同。我已经检查了每个数组的长度并确认它们相同，但我仍然收到相同的错误
def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# 合并来自训练集和测试集的指标
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# 排序列 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# 获取指标表
metrics_table = metrics_from_pipes(pipelines)

运行此代码会出现此错误
ValueError Traceback (most recent call last)
Cell In[45]，第 82 行
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # 获取指标表
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;表 1：基本模型指标&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

单元格 In[45]，第 50 行，位于 metrics_from_pipes(pipes_dict)
41 # 将性能指标列表聚合到单独的数据框中
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: 所有数组的长度必须相同

当我检查 train_metrics 和 val 指标的字典结果时，我得到了这个
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;线性回归&#39;,
&#39;随机森林回归器&#39;,
&#39;梯度提升回归&#39;,
&#39;额外树回归器&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>批量数据的 SGD 优化器设置</title>
      <link>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</link>
      <description><![CDATA[我正在学习 Joh Krohn 的数学入门课程。课程解释得很清楚，但有一件事让我很困惑。在这个任务中 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynb，我们使用了 torch.optim.SGD torch SGD，它运行了所有示例数据。
optimizer = torch.optim.SGD([m,b], lr = 0.01)
epochs = 999
for epoch in range(epochs): 

optimizer.zero_grad() # 将梯度重置为零；否则它们会累积

yhats = 回归（xs，m，b）# 步骤 1
C = mse（yhats，ys）# 步骤 2

C.backward() # 步骤 3

optimizer.step() # 步骤 4

在第二个练习中，我们进行了学习率调度 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/learning-rate-scheduling.ipynb
有 8.000.000 个数据点，因此数据被设置为批处理，并且代码在这些样本上轮流运行，而不是在所有数据上按时期运行。然而，这不是用 torch.optim.SGD 完成的，而是在代码上显示以查看数学是如何工作的。我正在努力用 torch.optim.SGD 运行它。如何编写代码来运行它，而不是像下面这样编写大型数学方程式，其中已经创建了所有方程式，例如梯度、theta：
n = 8000000
x = torch.linspace(0., 8., n)
y = -0.5*x + 2 + torch.normal(mean=torch.zeros(n), std=1)
indices = np.random.choice(n, size=2000, replace=False)
gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 
lr = 0.01
new_theta = theta - lr*gradient
C = mse(regression(x[batch_indices], m, b), y[batch_indices])
b.requires_grad_()
m.requires_grad_()

def return(my_x, my_m, my_b):
return my_m*my_x + my_b

m = torch.tensor([0.9]).requires_grad_()
b = torch.tensor([0.1]).requires_grad_()

batch_size = 32 # 模型超参数
batch_indices = np.random.choice(n, size=batch_size, replace=False)
yhat = return(x[batch_indices], m, b)

yhat = return(x[batch_indices], m, b)

def mse(my_yhat, my_y): 
sigma = torch.sum((my_yhat - my_y)**2)
return sigma/len(my_y)

C = mse(yhat, y[batch_indices])

C.backward()
m.grad
b.grad

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T

theta = torch.tensor([[b, m]]).T 

lr = 0.01
new_theta = theta - lr*gradient
new_theta

b = new_theta[0]
m = new_theta[1]

C = mse(regression(x[batch_indices], m, b), y[batch_indices])

rounds = 100 

for r in range(rounds): 

# 这个采样步骤很慢；稍后我们将介绍更快的批量采样： 
batch_indices = np.random.choice(n, size=batch_size, replace=False)

yhat = return(x[batch_indices], m, b) # 步骤 1
C = mse(yhat, y[batch_indices]) # 步骤 2

C.backward() # 步骤 3

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 

new_theta = theta - lr*gradient # 步骤 4

b = new_theta[0].requires_grad_()
m = new_theta[1].requires_grad_()
]]></description>
      <guid>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</guid>
      <pubDate>Sun, 11 Aug 2024 11:20:48 GMT</pubDate>
    </item>
    <item>
      <title>解决自动标记（优化）+分类问题</title>
      <link>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</link>
      <description><![CDATA[我知道这不太侧重于编程，但我不知道还有什么地方可以问这个问题。这更多的是关于方法而不是技术问题。
上下文
我有一个优化 + 分类任务。因此，本质上，我的数据具有以下列：
[&#39;Model ID&#39;, &#39;Q&#39;, &#39;refinement&#39;, &#39;avg_time&#39;, &#39;lattice&#39;, &#39;radius&#39;]（还有更多，但为了简洁起见，我们只保留这些）

Model ID：代表“设计”，每个 Model ID 将有多行

Q：这是目标变量

refinement：这是一个设置变量；它可以取 1-8 的值，这直接影响 Q，（模型 ID，细化）对是唯一的。因此，模型 ID 将具有多行，细化程度各不相同

avg_time：这是模拟完成所需的时间，仅受细化的影响，细化程度越高，所需的时间越长。此值与设计无关，它仅取决于细化，因此特定细化的所有设计都具有相同的时间。

lattice 和 radius：这些代表“设计”，本质上更改它们将更改 Q


数据集
我的数据集来自随机设计的模拟。对于每个设计，我们可以有以下行为：

持续增加（每次细化时的 Q 值高于上一个细化级别）
持续减少（每次细化时的 Q 值低于上一个细化级别）
之字形，其中 Q 值遵循此当前模式（高，低，高，低）或（低，高，低，高）
碗形，其中 Q 值遵循此当前模式：（高，低，低，高）
梯形，其中 Q 值遵循此当前模式：（低，高，高，低）
我有代码可以检测这些形状并返回布尔值：

def is_zigzag(q_values)

def is_bowl(q_values)

def is_trapezoid(q_values)

数据集中细化的值范围是 2-5，但细化可以取 1-8 的值。
任务
因此，我试图实现的是自动标记每个模型 ID（通过对行进行分组）和最佳细化值（范围为 1-8），以最大化 Q 的变化（增量越大越好）并最小化所花费的时间（越低越好）。问题是由于数据是在细化级别 5 处切割的，所以我想到使用概率方法（例如 MLE）来创建未来细化的预期变化和预期所花费的时间。但我似乎无法“调整”它，所以它很有用。获得预期值后，我需要一个成本函数来计算（优化）Q 的回报与完成该细化级别所花费的增加时间的比较。
在下一部分中，我将开发一个分类器，它将采用设计和最佳细化。从理论上讲，它应该可以预测未见过的设计的最佳细化级别
我很感激任何有关解决这个问题的指导/帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</guid>
      <pubDate>Sun, 11 Aug 2024 11:01:40 GMT</pubDate>
    </item>
    <item>
      <title>处理缺失数据并建立具有不完整信息的预测模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</link>
      <description><![CDATA[我正在为涉及 20 个影响点的供水网络开发一个预测模型。但是，我只有这 20 个点中的 10 个的历史数据。
我想知道如何在这个不完整的数据集下构建预测模型。具体来说：
我可以使用哪些方法来处理剩余 10 个点的缺失数据？在这种情况下，是否有任何标准技术或最佳实践来处理缺失数据？
我如何有效地将我拥有的 10 个点的数据合并到模型中？我可以采用哪些策略来确保有效利用可用数据进行准确预测？
是否有特定的技术或模型可以帮助在数据不完整的情况下进行预测？我对可以有效管理和利用不完整数据的方法感兴趣。
我还没有具体的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</guid>
      <pubDate>Sun, 11 Aug 2024 10:43:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 hub.KerasLayer 使用 tf.keras.sequential 制作深度学习模型时出错</title>
      <link>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</link>
      <description><![CDATA[# 创建一个构建 Keras 模型的函数

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
print(&quot;Building model with:&quot;, MODEL_URL)

model = tf.keras.Sequential([
hub.KerasLayer(MODEL_URL), # 第 1 层（输入层）
tf.keras.layers.Dense(units=OUTPUT_SHAPE, 
activation=&quot;softmax&quot;) # 第 2 层（输出层）
])

# 编译模型
model.compile(
loss=tf.keras.losses.CategoricalCrossentropy(), # 我们的模型想要减少这个（它的猜测有多错误）
optimizer=tf.keras.optimizers.Adam(), # 一个朋友告诉我们的模型如何改进它的猜测
metrics=[&quot;accuracy&quot;] # 我们希望这个值上升
)

# 构建模型
model.build(INPUT_SHAPE) # 让模型知道它将获得什么样的输入

返回模型

我有上面的函数，当我运行下面的其他程序时
model = create_model()
model.summary()

它会产生一些错误
TypeError：添加的层必须是 Layer 类的实例。
收到：layer=&lt;Dense name=dense_18,built=False&gt; 类型为 &lt;class &#39;keras.src.layers.core.dense.Dense&#39;&gt;。

我哪里做错了？]]></description>
      <guid>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</guid>
      <pubDate>Sun, 11 Aug 2024 08:05:45 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 CV-k 折叠预测“是”“否”的均方误差？[关闭]</title>
      <link>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</link>
      <description><![CDATA[为了根据满意度指数和参与度指数预测客户是否会购买产品，我们使用了 k 近邻法。
如何用 4 倍交叉验证过程评估预测均方误差？它不需要在 R 中。我只需要如何计算它的理论。
我知道如何用数字计算，但不知道如何用字符串计算]]></description>
      <guid>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</guid>
      <pubDate>Sun, 11 Aug 2024 05:16:27 GMT</pubDate>
    </item>
    <item>
      <title>有人可以指导我学习 Yolov10 的路线图吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78850160/can-anyone-guide-me-the-roadmap-for-learning-yolov10</link>
      <description><![CDATA[我对深入研究深度学习领域很感兴趣，尤其关注使用 YOLOv10 进行对象检测。
但是，我对该领域常用的许多工具和框架仍然很陌生，例如 TensorFlow、OpenCV 和 PyTorch。
鉴于 YOLOv10 是 YOLO 系列中的最新版本，并引入了几个高级功能，例如 NMS 无训练和提高效率，我有点不知所措，不知道从哪里开始。
我正在寻找一个全面的路线图，可以指导我完成有效使用 YOLOv10 所需的先决条件、必要技能和学习资源。
我希望获得一些特定领域的建议包括：
我应该首先掌握的深度学习中的关键基础主题。

考虑到我对 PyTorch 和 OpenCV 还不熟悉，请提供学习资源或教程。

如何理解对象检测概念，特别是在 YOLOv10 的背景下。

为训练和部署 YOLO 模型设置开发环境的最佳实践。

任何可以巩固我理解的推荐项目或练习。

]]></description>
      <guid>https://stackoverflow.com/questions/78850160/can-anyone-guide-me-the-roadmap-for-learning-yolov10</guid>
      <pubDate>Thu, 08 Aug 2024 19:38:10 GMT</pubDate>
    </item>
    <item>
      <title>我使用自定义增强和 TFRecord 管道在大型图像数据集上训练模型的方法是否有效？[关闭]</title>
      <link>https://stackoverflow.com/questions/78847703/is-my-approach-to-training-a-model-on-a-large-image-dataset-using-custom-augment</link>
      <description><![CDATA[我有一个存储在 TFRecord 文件中的大型图像数据集，我想在这个数据集上训练一个神经网络。我的目标是在将图像输入模型之前对图像应用自定义增强。但是，我找不到内置的 TensorFlow 函数（如 ImageDataGenerator）来在训练之前将增强直接应用于存储为张量的图像。
为了解决这个问题，我编写了一个自定义 ModelTrainer 类，其中我：
从 TFRecord 加载每个图像。
对图像应用一系列自定义变换（侵蚀、膨胀、剪切、旋转）。
创建一个由原始图像及其变换版本组成的批次。
在这个批次上训练模型，其中每个批次由单个图像及其变换版本组成。
这是我的代码片段：
class ModelTrainer:
def __init__(self, model):
self.model = model

def preprocess_image(self, image):
image = tf.cast(image, tf.float32) / 255.0
return image

def apply_erosion(self, image):
kernel = np.ones((5,5), np.uint8)
return cv2.erode(image, kernel, iterations=1)

def apply_dilation(self, image):
kernel = np.ones((5,5), np.uint8)
return cv2.dilate(image, kernel, iterations=1)

def apply_shear(self, image):
rows, cols = image.shape
M = np.float32([[1, 0.5, 0], [0.5, 1, 0]])
返回 cv2.warpAffine(image, M, (cols, rows))

def apply_rotation(self, image, angle=15):
rows, cols = image.shape
M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
返回 cv2.warpAffine(image, M, (cols, rows))

def transform_image(self, img, i):
if i == 0:
返回 img
elif i == 1:
返回 self.apply_erosion(img)
elif i == 2:
返回 self.apply_dilation(img)
elif i == 3:
返回 self.apply_shear(img)
elif i == 4:
返回 self.apply_rotation(img)

def train_on_tfrecord(self, tfrecord_path, dataset, batch_size=5):
dataset = dataset.map(lambda img, lbl: (self.preprocess_image(img), lbl))
dataset = dataset.batch(1)
dataset = iter(dataset)

对于 batch_images，数据集中的标签：
img_np = batch_images.numpy().squeeze()
lbl_np = labels.numpy().squeeze(axis=0)
image_batch = []
label_batch = []

对于 i in range(5):
perceived_image = self.transform_image(img_np, i)
image_batch.append(transformed_image)
label_batch.append(lbl_np)

image_batch_np = np.stack(image_batch, axis=0)
label_batch_np = np.stack(label_batch, axis=0)

image_batch_tensor = tf.convert_to_tensor(image_batch_np, dtype=tf.float32)
label_batch_tensor = tf.convert_to_tensor(label_batch_np, dtype=tf.float32)

loss = self.model.train_on_batch(image_batch_tensor, label_batch_tensor)

predictions = self.model.predict(image_batch_tensor)
predict_labels = np.argmax(predictions, axis=-1)
true_labels = np.argmax(label_batch_tensor, axis=-1)
accuracy = np.mean(predicted_labels == true_labels)

print(f&quot;Batch Loss = {loss}, Accuracy = {accuracy:.4f}&quot;)


我的问题是：

我一次在一个图像及其转换版本上训练模型的方法是否好且有效？
以这种方式训练网络是否可取，在每个批次中处理一个图像及其增强？
是否有更好的方法或优化我应该考虑处理大型数据集和应用自定义增强？
]]></description>
      <guid>https://stackoverflow.com/questions/78847703/is-my-approach-to-training-a-model-on-a-large-image-dataset-using-custom-augment</guid>
      <pubDate>Thu, 08 Aug 2024 09:51:24 GMT</pubDate>
    </item>
    </channel>
</rss>