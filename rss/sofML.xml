<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 12 Feb 2024 09:14:48 GMT</lastBuildDate>
    <item>
      <title>何时应用假设检验以及选择哪一个假设检验，以及是否对所有类型的数据集进行假设检验</title>
      <link>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</link>
      <description><![CDATA[所以最近我一直在研究 Kaggle 的太空泰坦尼克号数据集，并且我看到了一个关于类似类型问题的代码笔记本，所以我尽管使用笔记本中的相同步骤，但他已经完成了加载数据等所有步骤，变量描述和单变量等。我理解了大部分步骤，甚至将它们应用于太空泰坦尼克号，但我不理解在双变量部分进行的假设检验，我很困惑他是如何应用这些类型的测试的我该如何学习。
那么我可以跳过双变量分析的这一部分吗？请有人给我指点一些资源，或者我可以在哪里理解假设检验，它是否适用于所有类型的数据集，例如 nlp、cv 等
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</guid>
      <pubDate>Mon, 12 Feb 2024 06:53:48 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 训练在 CPU 和 GPU 上都很慢</title>
      <link>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</link>
      <description><![CDATA[我正在尝试使用 Tensorflow Keras 在我的机器上训练 CNN 模型。
这是我的机器规格：

CPU：Ryzen 9 3900x（12 核 24 线程）
GPU：GTX 970 4GB

模型 = models.Sequential([
    层.InputLayer(input_shape=input_shape),
    层.Conv2D（32，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Flatten(),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.Dense（n_classes，激活=&#39;softmax&#39;），
]）

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    火车发电机，
    每纪元的步数=153，
    批量大小=32，
    验证数据=验证生成器，
    验证步骤=23，
    详细=1，
    纪元=20，
）

我使用的是仅具有 CPU 访问权限的 Windows 11。使用 model.fit() 进行训练大约需要 1 小时（仅 CPU 训练）。我在同一台机器上双重启动 Linux Ubuntu，以获得更轻松的 GPU 支持。现在我已经确认了 Linux 环境中的 GPU 支持，但训练仍然需要大约 1 小时（仅 GPU 训练）。
CPU强而GPU弱，速度一样吗？还有其他可能出错的地方吗？
谢谢
我确保在训练模型时使用 GPU。速度还是挺慢的。]]></description>
      <guid>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</guid>
      <pubDate>Mon, 12 Feb 2024 06:51:29 GMT</pubDate>
    </item>
    <item>
      <title>分类迁移学习模型是否过度拟合或欠拟合？</title>
      <link>https://stackoverflow.com/questions/77979247/is-classification-transfer-learning-model-overfitting-or-underfitting</link>
      <description><![CDATA[[这是一个使用 EfficientNetv2b3 作为基础模型的迁移学习分类项目。训练数据包含 26k 张照片，验证数据包含约 3.3k 张照片。
我还应该补充一点，这个模型工作得“不错”，并且与 inceptionv3 作为基本模型不同，因此我试图找出差异并改进 effectivenetv2b3 模型。
我无法确定这是过度拟合还是欠拟合。 ](https://i.stack.imgur.com/W3tA0.png) 
我增加了 dropout，添加了数据增强，并删除了一些密集层以降低复杂性。我不知道该怎么办。 val 损失和 val 精度从开始到结束保持不变。我还认为我的输入数据可能是错误的类型，但我已确定它是 float32。
添加的层如下：
对于base_model.layers中的图层：
 可训练层 = False
x = 层.Flatten()(base_model.output)
x = 层.BatchNormalization()(x)
x = 层.Dense(32, 激活 = &#39;relu&#39;)(x)
x = 层数.Dropout(0.5)(x)

x = 层.BatchNormalization()(x)
x = 层.Dense(1, 激活 = &#39;sigmoid&#39;)(x)

模型= keras.Model（输入= base_model.输入，输出= x）

&lt;前&gt;&lt;代码&gt;纪元 1/12
835/835 [==============================] - 15313s 18s/步 - 损失：0.2608 - 二进制精度：0.6075 - val_loss ：0.6336 - val_binary_accuracy：0.7945
纪元 2/12
835/835 [================================] - 718s 860ms/步 - 损耗：0.2446 - 二进制精度：0.7119 - val_loss ：0.6400 - val_binary_accuracy：0.7945
纪元 3/12
835/835 [================================] - 759s 909ms/步 - 损失：0.2424 - 二进制精度：0.7755 - val_loss ：0.6318 - val_binary_accuracy：0.7945
纪元 4/12
835/835 [================================] - 761s 912ms/步 - 损失：0.2421 - 二进制精度：0.7880 - val_loss ：0.6370 - val_binary_accuracy：0.7945
纪元 5/12
835/835 [================================] - 828s 991ms/步 - 损耗：0.2421 - 二进制精度：0.7942 - val_loss ：0.6200 - val_binary_accuracy：0.7945
纪元 6/12
835/835 [================================] - 774s 927ms/步 - 损失：0.2420 - 二进制精度：0.7936 - val_loss ：0.6208 - val_binary_accuracy：0.7945
纪元 7/12
835/835 [================================] - 769s 922ms/步 - 损失：0.2419 - 二进制精度：0.7921 - val_loss ：0.6214 - val_binary_accuracy：0.7945

我最初认为我过度拟合，但有人提到我可能不拟合，所以我正在寻找关于该怎么做的更多意见和选项。 val 损失和 val 准确率在整个训练过程中似乎保持不变。]]></description>
      <guid>https://stackoverflow.com/questions/77979247/is-classification-transfer-learning-model-overfitting-or-underfitting</guid>
      <pubDate>Mon, 12 Feb 2024 03:53:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么重新运行 jupyter 笔记本单元会增加内存使用量？</title>
      <link>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</link>
      <description><![CDATA[我不确定这是 Pytorch 还是 jupyter 笔记本问题。
我正在使用 Pytorch 和 jupyter 笔记本在 CPU 上训练一些神经网络。我可以运行整个 jupyter 笔记本，但是当我重新运行某些单元（例如训练代码）时，我会遇到内存错误或崩溃。
这是为什么呢？我重新运行的单元格没有在任何地方积累对象。他们只是重新加载数据加载器并重新训练模型。
额外的内存从哪里来？]]></description>
      <guid>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</guid>
      <pubDate>Mon, 12 Feb 2024 03:09:53 GMT</pubDate>
    </item>
    <item>
      <title>如何为每个患者创建一个单独的图，显示其“订单金额”和“订单数量”与“订单时间”变量的关系？</title>
      <link>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</link>
      <description><![CDATA[我现在面临的问题是代码返回一个图，该图显示与用户输入日期匹配的最新数据。我想一起研究数据，因此我想要图中的趋势，而不是最新数据的单个水平线。然而，它不显示与相同“患者ID”相对应的过去数据和最新数据的合并数据。我想创建两个图，其中一个图是“订单金额”与“订单时间”的对比，另一个图是“订单数量”与“订单时间”的对比。

数据按以下方式分组：

代码想要解决四个主要部分：

将最新数据与输入日期进行匹配，我已经根据输入日期过滤了数据帧。

将之前数据的相同 ID 与最新数据进行匹配，并将数据串在一起：为了实现这一点，我们需要识别每个患者 ID 的最新数据，然后将其与该患者之前的数据合并。相同的患者 ID。

在共享相同“患者 ID”和“订单时间”的两行中选取“订单金额”和“订单数量”值最高的行：将最近的数据与先前的数据合并后，我们可以比较每个患者 ID 的订单金额和订单数量的值，并选择具有最高值的行。 - 我被困在这个阶段:)


例如，第 27 行和第 28 行显示第 28 行的“订单金额”和“订单数量”分别为 659.8 和 21，以及 671.677 和 13。在这种情况下，我们将选择第 28 行作为“订单数量”值似乎要低得多，而“订单金额”值似乎彼此接近。在不同的场景中，当第 29 行和第 30 行具有相同的“患者 ID”时，我们会选择第 30 行，因为其“订单金额”值要低得多，并且其“订单数量”值与第 30 行的值相差小于“订单”数量&#39;。第 32 行和第 33 行是替代示例。如果“订单金额”和“订单数量”的值相同，我们可以选择第 32 行作为两者之间的第一行。如果只有一个例外，我们将选择单行。第 31 行就是一个示例。


将每个患者 ID 的“订单金额”与“订单时间”和“订单数量”与“订单时间”的点绘制在一起。这意味着每个患者 ID 一个图。

导入 pandas 作为 pd
导入日期时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
导入urllib
将 matplotlib.dates 导入为 mdates

input_date = input(&quot;请输入日期（格式：yyyy/mm/dd）：&quot;)
input_date = datetime.datetime.strptime(input_date, &quot;%Y/%m/%d&quot;)

df[&#39;订单时间&#39;] = pd.to_datetime(df[&#39;订单时间&#39;])

df[&#39;time_diff&#39;] = (df[&#39;订单时间&#39;] - input_date).dt.days

df_filtered = df[(df[&#39;time_diff&#39;] &gt;= -30) &amp; (df[&#39;time_diff&#39;] &lt;= 30)]

centre_data = df_filtered.sort_values(by=&#39;订单时间&#39;).groupby(&#39;患者 ID&#39;).last()

merged_data = pd.merge（df，recent_data，on =&#39;患者ID&#39;，后缀=（&#39;_prev&#39;，&#39;&#39;））

## 到目前为止似乎还不错，尽管将后缀包含为“_prev”很奇怪。

idx_amount = merged_data.groupby(&#39;患者ID&#39;)[&#39;订单金额&#39;].idxmax()
idx_quantity = merged_data.groupby(&#39;患者 ID&#39;)[&#39;订单数量&#39;].idxmax()

idx_highest = idx_amount[idx_amount.isin(idx_quantity)]

Filtered_data = merged_data.loc[idx_highest]

defplot_filtered_data（数据）：

    grouped_data = data.groupby(&#39;患者 ID&#39;)
    
    对于 grouped_data 中的 Patient_id、group_data：
        plt.plot(group_data[&#39;订单时间&#39;], group_data[&#39;订单金额&#39;],marker=&#39;o&#39;, label=f&#39;患者{patent_id}&#39;)
    
    plt.xlabel(&#39;下单时间&#39;)
    plt.ylabel(&#39;订单金额&#39;)
    plt.title(&#39;患者的订单金额与订单时间&#39;)
    plt.图例()
    plt.xticks（旋转=45）
    plt.tight_layout()
    plt.show()

绘图过滤数据（过滤数据）


我知道代码中存在错误，但我的编辑经验很少。有人可以指出我的错误吗？我知道这听起来有点复杂。]]></description>
      <guid>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</guid>
      <pubDate>Mon, 12 Feb 2024 02:57:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 model.evaluate() 和 model.predict() 时的准确性不同</title>
      <link>https://stackoverflow.com/questions/77979085/different-accuracy-when-using-model-evaluate-and-model-predict</link>
      <description><![CDATA[我正在使用 4 个标签执行多类分类，使用 model.predict() 和 model.evaluate() 时有不同的结果，这里是我的代码：
test_loss, test_accuracy = model.evaluate(testing_generator)

这段代码的结果是
测试损失：0.0561
测试准确率：99.31%
y_true=testing_generator.classes
y_pred_probs = model.predict(testing_generator)
y_pred = np.argmax(y_pred_probs, 轴=1)

准确度=准确度_得分(y_true, y_pred)

这段代码的结果是
准确率：25.17%
我使用包含测试图像的相同testing_generator，为什么会发生这种情况？有谁知道怎么解决吗？
我陷入了这个问题，我希望准确性是相同的值，因为我使用相同的testing_generator]]></description>
      <guid>https://stackoverflow.com/questions/77979085/different-accuracy-when-using-model-evaluate-and-model-predict</guid>
      <pubDate>Mon, 12 Feb 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>在 qiskit.algorithms.optimizers.ADAM 优化过程中获取中间步骤的方法</title>
      <link>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</link>
      <description><![CDATA[使用 Qiskit 内部 ADAM 优化器时，有什么方法可以获取中间步骤（类似于 Tensorflow 或 Pytorch Adams 优化器中的回调函数）？
我正在实现变分量子电路 (VQC)，以使用 Qiskit 的 Adam 优化器训练量子机器学习模型。
我浏览了 ADAM 的文档 Qiskit 中的优化器，但我找不到合适的方法来获取它们。
我可以使用 Pytorch Adam 优化器获取中间训练信息，但我想知道是否可以使用 Qiskit 来做到这一点。
我正在使用qiskit v0.45.2。]]></description>
      <guid>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</guid>
      <pubDate>Mon, 12 Feb 2024 00:13:43 GMT</pubDate>
    </item>
    <item>
      <title>整洁的Python基因组的计算并没有真正相加</title>
      <link>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</link>
      <description><![CDATA[我为 XOR 制作了一个非常简单的整洁的 python。这是基因组的摘要。
最佳基因组：
钥匙：141
健身：2.993003425787766
节点：
    0 DefaultNodeGene（键= 0，偏差= -0.8080802310263379，响应= 1.0，激活= sigmoid，聚合=总和）
连接：
    DefaultConnectionGene(键=(-2, 0)，权重=1.5655941592328844，启用=True)
    DefaultConnectionGene(key=(-1, 0)，权重=1.2986799185483175，启用=True

但是当我尝试计算它时，它并没有真正意义。
以下是评估结果：
 输入 (0.0, 0.0)，预期输出 (0.0,)，得到 [0.017286340618090416]
  输入（0.0，1.0），预期输出（1.0，），得到[0.9778511014280682]
  输入（1.0，0.0），预期输出（1.0，），得到[0.920780444438713]
  输入（1.0，1.0），预期输出（0.0，），得到[0.9999657218870016]

这就是我计算输入 (1, 1) 的方法：
activation_func( sum( 输入 * 权重 ) + 偏差 )

对于这种情况：
sigmoid( 1 * 1.2986799185483175 + 1 * 1.5655941592328844 + (-0.8080802310263379) )
= 0.88657197794273698476
但是根据评估应该是0.9999657218870016
我错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</guid>
      <pubDate>Sun, 11 Feb 2024 21:04:52 GMT</pubDate>
    </item>
    <item>
      <title>针对受阻文本优化 OCR</title>
      <link>https://stackoverflow.com/questions/77978340/optimize-ocr-for-obstructed-text</link>
      <description><![CDATA[我正在尝试实现 OCR 来检测运动员号码，但是我遇到了一些断线问题，有时会阻碍板中的文本...我尝试了多种 OCR，例如 Tesseract、PaddleOCR、EasyOCR 和 TrOCR。到目前为止，当数字完全可见时，EasyOCR 和 TrOCR 对我来说表现最好，但是一旦数字前面有任何障碍物，它们的准确性就会开始下降，这是可以理解的。我想知道是否可以做些什么来改进模型的推理？

正如我所提到的，我尝试了多种 OCR，但如果文本不完全可见，那么它们都不能很好地处理...]]></description>
      <guid>https://stackoverflow.com/questions/77978340/optimize-ocr-for-obstructed-text</guid>
      <pubDate>Sun, 11 Feb 2024 20:36:02 GMT</pubDate>
    </item>
    <item>
      <title>字符串“loss”被传递给 metric_name() 而不是指标名称</title>
      <link>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</link>
      <description><![CDATA[我正在遵循教程，但收到错误：
ValueError：无法解释指标标识符：丢失
在：
\keras\src\metrics\__init__.py:205，在 get(identifier) 中
看起来在wrapper.py第532行metric_name(key)应该接收损失函数的名称，但它实际上接收字符串“loss”
以下是错误的相关代码：
def buildNetwork():
    分类器=顺序（）
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;，input_shape =（30，）））
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;））
    classificator.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
    优化器= keras.optimizers.Adam(learning_rate=0.001,weight_decay=0.000001)
    classificator.compile（优化器=优化器，损失=&#39;binary_crossentropy&#39;，指标=[&#39;binary_accuracy&#39;]）
    返回分类器


分类器 = KerasClassifier(model=buildNetwork,batch_size=10,epochs=100, loss=&#39;binary_crossentropy&#39;)

分数 = cross_val_score(估计器=分类器, X=数据, y=真相, cv=10, error_score=&#39;raise&#39;)

如果在keras_metric_get中我手动设置identifier =“binary_crossentropy”它工作正常
我不知道是否是兼容性问题，但我有 Keras 3.0.8、TF 2.15.0 和 SciKeras 0.12.0]]></description>
      <guid>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</guid>
      <pubDate>Sun, 11 Feb 2024 14:32:54 GMT</pubDate>
    </item>
    <item>
      <title>关于进化图卷积网络的机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</link>
      <description><![CDATA[如何使用 Jupyter Notebook 的 Evolve Graph 卷积网络代码，使用 Kaggle 上的 Elliptic_Bitcoin_Heist 数据。
我无法编码。谁能帮我？我已经阅读了该程序 https://github.com/IBM/EvolveGCN.git 但我不知道如何使用它。我尝试在 ubuntu 上运行，但总是出错，我无法安装它。我想用jupyter笔记本编码]]></description>
      <guid>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</guid>
      <pubDate>Sun, 11 Feb 2024 14:10:41 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 自定义转换器从底层模型中抛出 NotFittedError</title>
      <link>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</link>
      <description><![CDATA[我想创建自己的 scikit-learn 转换器，用于对包含分类的数字特征进行编码，例如邮政编码或行业代码（NAICS、MCC 等）。在这些类型的代码中有一个结构：例如MCC 3000-3999 是“旅行和娱乐”，它进一步细分为更细粒度的类别，例如“航空公司”、“汽车租赁”等。我们不能将它们用作序数特征，但如果我们将它们视为纯分类特征（例如，通过 One -Hot-Encoding）我们需要选择在代码结构的哪个级别应用特征编码。
为了解决这个问题，我创建了自己的 scikit-learn 变压器，它是 TargetEncoder 使用决策树。代码如下所示。重要的是要认识到，在模型训练期间，应使用样本外决策树回归分数来避免过度拟合。因此，我实现了自己的 fit_transform 函数来生成这些样本外分数：
从 sklearn.tree 导入 DecisionTreeRegressor

从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.model_selection 导入 cross_val_predict

类 TaxonomyEncoder（TransformerMixin，BaseEstimator）：

def __init__(自身, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    自我简历 = 简历

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    返回自我

def 变换（自身，X）：
    返回 self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    返回 cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)

转换器工作正常，除非在 ColumnTransformer 中使用：
从 sklearn.compose 导入 ColumnTransformer

变压器 = ColumnTransformer([(&#39;分类法&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])

然后我得到决策树尚未拟合的错误：
NotFittedError：此 DecisionTreeRegressor 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

显然，scikit-learn 在导致此错误的表面下进行了一些检查。请注意，实际上没有理由需要拟合决策树，因为决策树是在 cross_val_predict 函数中重新拟合的。我该如何解决这个问题？
下面显示了重现该错误的完整工作示例：
导入 pandas 作为 pd
df = pd.DataFrame({&#39;mcc&#39;:[3000,3500,7339], &#39;y&#39;:[0,0,1]})

te = TaxonomyEncoder().fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
te.transform(df[[&#39;mcc&#39;]])

给出：
数组([[0.],
       [0.],
       [1.]])

并且 fit_transform 也给出了预期的结果：
te.fit_transform(df[[&#39;mcc&#39;]], df[&#39;y&#39;])

数组([[0.],
       [0.],
       [0.]])

但是当包装在 ColumnTransformer 中时，事情就会出错：
transformer = ColumnTransformer([(&#39;taxonomy&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])
]]></description>
      <guid>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</guid>
      <pubDate>Sun, 11 Feb 2024 12:34:35 GMT</pubDate>
    </item>
    <item>
      <title>我在实现graycomatrix方法时收到“JpegImageFile”对象不可下标错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</link>
      <description><![CDATA[代码
错误
我试图获取 2 个图像的 GLCM，每个图像分为 4 个补丁，但发生了此错误。这2张图片都是jpg的。该方法还可以获得补丁之间的相异性和相关性。我从这里得到了代码 https://scikit-image.org/文档/stable/auto_examples/features_detection/plot_glcm.html]]></description>
      <guid>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</guid>
      <pubDate>Sun, 11 Feb 2024 12:14:45 GMT</pubDate>
    </item>
    <item>
      <title>分类任务问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77976439/classification-task-issues</link>
      <description><![CDATA[我有一个关于二元变量分类任务的概念性问题。我可用的数据集如下：

id，设备 ID
性别，目标变量
日
小时
内容、具有许多唯一值的分类变量

这些数据代表客户进行的网络搜索，并且出现了两个问题：

目标变量高度不平衡，女性较多，男性较少。
内容变量有许多唯一值。

考虑到我的时间有限，并且模型结果背后的推理比模型本身更重要，任何人都可以提出解决这两个问题的最佳方法吗？
我正在寻求这两个问题的解决方案，以下是我的考虑：

对内容变量应用诸如单热编码之类的技术会导致维度问题。我可以考虑设置阈值并考虑最常见的值，同时将其他值分类为“其他”。
应用目标编码会反映出类别不平衡的问题。
]]></description>
      <guid>https://stackoverflow.com/questions/77976439/classification-task-issues</guid>
      <pubDate>Sun, 11 Feb 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在苹果 M1 Pro 芯片组上的 XGBoost 中启用 GPU</title>
      <link>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</link>
      <description><![CDATA[我尝试在带有设备 = cuda 的 Windows 上使用 GPU 进行 XGBoost 训练，它有效并且训练时间大大减少，现在我想在我的 Mac M1 Pro 上进行此实验。
如何在 m1 pro 芯片组上启用 GPU 的 XGBoost。
我尝试查找无法找到信息的文档。]]></description>
      <guid>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</guid>
      <pubDate>Sun, 11 Feb 2024 05:51:44 GMT</pubDate>
    </item>
    </channel>
</rss>