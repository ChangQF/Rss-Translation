<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 30 Aug 2024 06:23:22 GMT</lastBuildDate>
    <item>
      <title>需要一个 AI 模型来生成自动化测试用例。输入是测试用例场景。在没有 openAI 的情况下，实现此目标的最佳 AI 模型是什么 [关闭]</title>
      <link>https://stackoverflow.com/questions/78930172/need-a-ai-model-to-generate-the-testcases-for-automation-input-is-testcase-scen</link>
      <description><![CDATA[我希望在 Node.js 或 npm 环境中开发或集成 AI/ML 模型，该模型可以自动生成详细的测试用例。该模型应将测试数据和预定义场景作为输入，并生成全面的测试用例作为输出]]></description>
      <guid>https://stackoverflow.com/questions/78930172/need-a-ai-model-to-generate-the-testcases-for-automation-input-is-testcase-scen</guid>
      <pubDate>Fri, 30 Aug 2024 01:43:19 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 自定义数据集 CPU OOM 问题</title>
      <link>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</link>
      <description><![CDATA[我的数据加载器中存在一个非常持久的内存问题，根据 num_workers 的不同，在任意数量的 epoch（5-6）后内存就会填满。
我有 85% 的把握认为问题出在数据集上，因为每次调用 getitem() 都会增加内存。
我的数据只是我在 getitem() 中加载和处理的目录列表。我没有收到任何 cuda 错误
def transformations(self,input):
i,j,h,w = self.crop.get_params(input[&#39;target_material&#39;][0], scale=(0.7, 1.0), ratio=(1.0, 1.0))
if self.use_modality1:
input[&quot;m1&quot;] = torch.cat([self.color_jitter(TF.resized_crop(sample, i, j, h, w, size=(256, 256))) for sample in input[&#39;m1&#39;]], dim=0)
if self.use_semantic:
input[&quot;m2&quot;] = torch.cat([TF.resized_crop(sample, i, j, h,w,size=(self.img_size, self.img_size), interpolation=TF.InterpolationMode.NEAREST) for sample in input[&#39;m2&#39;]], dim=0)
return input

def __getitem__(self, idx):
While True: 
source = self.data[idx]
for _ in 10: 
target = select_target(self.data) 
if (diff(source,target) &lt; .10):
continue
else:
sample = {}
if self.use_m1: 
sample[&#39;m1&#39;] = torch.stack([
to_tensor(normalize_images(np.transpose(cv2.resize(read_npz(m1_input), dsize=(256, 256), interpolation=cv2.INTER_AREA)[..., :3], (2, 0, 1)), max=255)) for m1_input 在 m1_dirs 中
], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.stack([
to_tensor(normalize_images(np.expand_dims(cv2.resize(sem_image, dsize=(256, 256), interpolation=cv2.INTER_NEAREST), axis=0), max=40)) for m2_input 在 m2_dirs 中
], dim=0)

if self.config[&quot;transform&quot;] and random.random()&lt;0.5 and self.train:
sample = self.transformations(sample)
else: 
if self.m1:
sample[&#39;m1&#39;] = torch.cat([m1_tensor for m1_tensor in sample[&#39;m1&#39;]], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.cat([m2_tensor for m2_tensor in sample[&#39;m2&#39;]], dim=0)

return sample


跟踪内存后，我发现即使读取所有数据（未应用转换）也会导致内存增加，并且这些内存不会释放，而且会不断累积。最终，我收到 OOM 错误，代码失败。]]></description>
      <guid>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</guid>
      <pubDate>Thu, 29 Aug 2024 22:59:52 GMT</pubDate>
    </item>
    <item>
      <title>多目标文本回归的最佳架构是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</link>
      <description><![CDATA[我正在使用 Google 的“Civil-Comments”数据集构建 AI 模型。它有 7 个不同的标签，每个标签都是浮点数，可以是 0 到 1 之间的任意值。
我读过的 Embedding Bags 表现不佳。我一直在研究 transformers 和循环系统（LSTM、GRU、RNN 等）。我该怎么办？
如果重要的话，使用 PyTorch。但我不是在寻找代码。]]></description>
      <guid>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</guid>
      <pubDate>Thu, 29 Aug 2024 21:09:48 GMT</pubDate>
    </item>
    <item>
      <title>建立基于多个时间序列的模型来预测新的独立时间序列</title>
      <link>https://stackoverflow.com/questions/78929174/building-a-model-based-on-multiple-time-series-to-make-a-prediction-on-a-new-ind</link>
      <description><![CDATA[我为物理实验模拟了时间序列数据。每个数据都有 n 个特征、一个目标和一个从 t = 0 到 10000 的时间列。我的目标是建立一个在上述多个时间序列上进行训练的模型，然后预测一个新的测试时间序列集（t = 0 到 10000）。从我的角度来看，这可能是一个时间序列问题，因为目标依赖于时间，但是，我的目标也不是时间序列，因为大多数时间序列模型都需要有目标的历史数据，而我的目标似乎不是对一个全新的时间序列进行预测。
如何正确解决这个问题？
我尝试使用 xgboost 和 randomforest 与 GroupKFold 进行交叉验证，并确保没有数据泄漏。但是，我的预测值无法与目标实际值的高频相匹配（n 个特征的波动较小）。]]></description>
      <guid>https://stackoverflow.com/questions/78929174/building-a-model-based-on-multiple-time-series-to-make-a-prediction-on-a-new-ind</guid>
      <pubDate>Thu, 29 Aug 2024 18:17:46 GMT</pubDate>
    </item>
    <item>
      <title>Collab Runtime 出现断开连接问题</title>
      <link>https://stackoverflow.com/questions/78928469/collab-runtime-gets-disconnect-issue</link>
      <description><![CDATA[我正在使用 Google Collab 来运行我的模型训练，由于训练集很大，我必须运行更长的时间，当我离开超过 15 分钟时，运行时实例就会断开连接，我必须重新开始。
是否有任何解决方法可以安排模型训练并运行更长时间，而无需定期连接？]]></description>
      <guid>https://stackoverflow.com/questions/78928469/collab-runtime-gets-disconnect-issue</guid>
      <pubDate>Thu, 29 Aug 2024 15:00:57 GMT</pubDate>
    </item>
    <item>
      <title>如何用机器学习模型处理实时图像（帧）？</title>
      <link>https://stackoverflow.com/questions/78928144/how-to-process-real-time-image-frame-by-ml-models</link>
      <description><![CDATA[有一些非常好的 ML 模型在处理图像和给出结果方面运行得非常好，比如 deep-anything 和最新的 meta 的 splitation-anything-2。
我可以很好地运行它们，但我的要求是通过摄像头在实时视频帧上运行这些模型。
我知道运行模型基本上是为了优化速度或准确性。我不介意准确性出错，但我真的想优化这些模型的速度。
我现在不介意利用云 GPU 来运行它。
我该怎么做？我应该建立自己的模型来满足速度要求吗？]]></description>
      <guid>https://stackoverflow.com/questions/78928144/how-to-process-real-time-image-frame-by-ml-models</guid>
      <pubDate>Thu, 29 Aug 2024 13:44:46 GMT</pubDate>
    </item>
    <item>
      <title>Google COLAB：单击编辑器中的任意位置即可重复和交换单词或字母</title>
      <link>https://stackoverflow.com/questions/78927737/google-colab-repeating-and-exchanging-words-or-letters-on-clicking-on-anywhere</link>
      <description><![CDATA[[在此处输入图片描述](https://i.sstatic.net/1vRZC63L.png)
如何解决此问题：Google COLAB：我编写了任何代码，然后在 colab 编辑器中单击该代码，它表现得很奇怪，并且一次又一次地重复单词。它会从点击处随机复制任何内容，然后粘贴到我单击的下一个位置。]]></description>
      <guid>https://stackoverflow.com/questions/78927737/google-colab-repeating-and-exchanging-words-or-letters-on-clicking-on-anywhere</guid>
      <pubDate>Thu, 29 Aug 2024 12:15:56 GMT</pubDate>
    </item>
    <item>
      <title>使用熊猫进行机器学习的基线预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78927682/baseline-prediction-using-pandas-for-machine-learning</link>
      <description><![CDATA[我在阅读 Kaggle 教程时看到了这个，但无法理解。
如果您需要更多详细信息，请点击此链接：https://www.kaggle.com/code/kashnitsky/topic-1-exploratory-data-analysis-with-pandas/notebook
[在此处输入图片说明](https://i.ss在此处输入图片说明tatic.net/TLeRqcJj.png)
我真的不明白这是怎么发生的。]]></description>
      <guid>https://stackoverflow.com/questions/78927682/baseline-prediction-using-pandas-for-machine-learning</guid>
      <pubDate>Thu, 29 Aug 2024 12:02:35 GMT</pubDate>
    </item>
    <item>
      <title>如何用 pytorch 处理文本和数字特征？</title>
      <link>https://stackoverflow.com/questions/78926855/how-to-handle-text-and-number-features-with-pytorch</link>
      <description><![CDATA[我的数据集包含一些数字列和一个文本列。文本是一个句子。
我想使用 pytorch 将此数据集用于分类。但不知道如何处理文本列。
以前我使用 Catboost 和 text_features 选项，它运行良好。我还尝试将文本转换为嵌入，并像 Catboost 中的任何其他数字特征一样使用它，它也能正常工作。这是我所做的：
model = AutoModel.from_pretrained(&quot;DeepPavlov/rubert-base-cased&quot;, num_labels = 3, output_attentions = False, output_hidden_​​states = False)

def getEmbedding(text):

coded_input = tokenizer(text, return_tensors=&#39;pt&#39;, truncation=True,
max_length=max_len, add_special_tokens=True)

with torch.no_grad():

output = model(**encoded_input)
output = output[1].detach().cpu().numpy()[0]
torch.cuda.empty_cache()

return output

data[&#39;embedding&#39;] = data[&#39;text&#39;].apply(lambda x: getEmbedding(x))
data= pd.concat([data, data[&#39;embedding&#39;].apply(pd.Series)], axis=1)

但是现在我很好奇我使用带有 Bert 层的 pytorch 模型来处理文本，然后将结果与数字特征连接起来并将其传递到 Linear 层。类似于：
class BaselineNN(nn.Module):
def __init__(self, input_size, hidden1, out_size, drop1):
super(BaselineNN, self).__init__()
self.l1 = BertModel.from_pretrained(&#39;bert-base-uncased&#39;) # 用于文本特征的 bert 层
self.fc2 = nn.Linear(input_size, hidden1) # 用于数字特征的线性层
self.fc3 = nn.Linear(???, out_size) # 输出层

def forward(self, input1, input2, mask, token_type_ids):
_, output_1= self.l1(input1,tention_mask = mask, token_type_ids = token_type_ids)
output_2 = self.fc2(input2)
combined = torch.cat((output_1.view(output_1.size(0), -1),
output_2.view(output_2.size(0), -1)), dim=1)
out= self.fc2(combined)
return out

def init_weights(self, m):
if isinstance(m, nn.Linear):
torch.nn.init.kaiming_uniformal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
m.bias.data.fill_(0.01)

值得吗？我的意思是我可以将文本转换为嵌入，将嵌入转换为数字并在模型中使用它。
如果值得 - 我如何正确连接 Bert 输出和线性层输出以将其传递到下一层？]]></description>
      <guid>https://stackoverflow.com/questions/78926855/how-to-handle-text-and-number-features-with-pytorch</guid>
      <pubDate>Thu, 29 Aug 2024 08:35:02 GMT</pubDate>
    </item>
    <item>
      <title>如何将 CLIP 向量转换为 LLM 文本标记嵌入？</title>
      <link>https://stackoverflow.com/questions/78926828/how-to-convert-clip-vectors-to-an-llm-text-token-embeddings</link>
      <description><![CDATA[我希望将多种模态嵌入到您传统的基于文本的 LLM 中。为此，我需要将任何模态转换为 CLIP 向量（我已经完成了），现在我需要将此向量转换为 LLM 文本标记嵌入。有人能帮我完成这个转换吗？]]></description>
      <guid>https://stackoverflow.com/questions/78926828/how-to-convert-clip-vectors-to-an-llm-text-token-embeddings</guid>
      <pubDate>Thu, 29 Aug 2024 08:27:14 GMT</pubDate>
    </item>
    <item>
      <title>有没有针对低 TOPs 边缘 AI 设备上的多目标跟踪的解决方案？[关闭]</title>
      <link>https://stackoverflow.com/questions/78926753/any-solutions-for-multi-object-tracking-on-low-tops-edge-ai-device</link>
      <description><![CDATA[我正在尝试在低 AI 功率的 PTZ 摄像机上运行 MOT 算法。场景是在会议室或教室中跟踪某人（可能是中间的人，可以使用遥控器切换到其他人）。
到目前为止，我已经尝试过：
Byte-Track：我重新训练了 crowdhuman 数据集以使用 YOLOV5s 检测人头。fps 足够好（大约 14fps）以允许 PTZ 摄像机进行跟踪。但问题是，当目标被其他人遮挡时，很容易发生 ID 切换。（它纯粹基于卡尔曼滤波器）
Bot-SORT：重新训练 Fast-ReID 模型以获取 Reid 特征，骨干仍然是 YOLOV5s。但不可能对所有人都这样做，每个用户需要 14 毫秒。
我需要一种方法来尽可能避免 ID-Switch，同时保持良好的帧速率。对我的应用程序有什么好的解决方案吗？或者有什么建议可以优化当前的算法？]]></description>
      <guid>https://stackoverflow.com/questions/78926753/any-solutions-for-multi-object-tracking-on-low-tops-edge-ai-device</guid>
      <pubDate>Thu, 29 Aug 2024 08:02:36 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 的数据训练标记策略</title>
      <link>https://stackoverflow.com/questions/78926680/data-training-labeling-policy-for-yolo</link>
      <description><![CDATA[我正在训练我的 YOLO 来检测飞机和无人机。在某些图片中，无法检测到该物体确实是一架飞机，它甚至看起来像一架无人机（图片是从很远的地方拍摄的），但我从上下文中知道它确实是一架。我还应该把它标记为飞机吗？]]></description>
      <guid>https://stackoverflow.com/questions/78926680/data-training-labeling-policy-for-yolo</guid>
      <pubDate>Thu, 29 Aug 2024 07:45:18 GMT</pubDate>
    </item>
    <item>
      <title>序列到序列 LSTM 用于分类</title>
      <link>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</link>
      <description><![CDATA[我有一个包含两列的数据集：
past_events，future_events。
past_events 是 53 个数字代码的序列，如下所示
&#39;198&#39;、&#39;2000&#39;、&#39;197&#39;、&#39;85903&#39;，...
而 future_events 是 52 个数字代码的序列，如下所示
&#39;345&#39;、&#39;200&#39;、&#39;8904&#39;、&#39;23765&#39;，...
每个代码代表一个事件（代码按时间顺序排列）。
这个数据集中有近 3000 行。
我想构建一个 LSTM，它采用以下序列输入为 past_events，输出为 52 个事件的序列，这些事件是 future_events 的预测。
由于每个代码代表一个事件，在 future_events 中可能会出现 past_events 中不存在的代码，并且某些代码可能只出现在几行中，因此只出现在几个序列中。
这是我第一次做这种问题。如果是序列到值，我可以做到，但是对于这种分类的序列到序列，我不知道如何构建这个模型。
你能给我一些例子或一些解释这种问题的网站吗？
这是我尝试构建 LSTM，但没有成功
input_1 = Input(shape=sequence_length)

lstm_out = LSTM(128, return_sequences=True)(input_1)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)

lstm_out = Lambda(lambda x: x[:, :52, :])(lstm_out)

# 注意机制
attention = Attention()([lstm_out, lstm_out])

dense_out = TimeDistributed(Dense(128,activation=&#39;relu&#39;))(attention)
dense_out = Dropout(0.2)(dense_out)

output_layer = TimeDistributed(Dense(num_classes,activation=&#39;softmax&#39;))(dense_out)

我能得到的最好结果是，预测都是相同的数字代码，例如 52 &#39;367&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</guid>
      <pubDate>Thu, 29 Aug 2024 00:33:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来追踪公司员工的资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-employees-in-a-company</link>
      <description><![CDATA[我的目标是计算离职员工的流失风险。我想到了两种方法：

根据已辞职员工的数据库创建 100 份档案，按风险最高（100%）到风险最低（0%）进行排序，然后确定与每位在职员工最接近的档案。在职员工的风险评分将基于所创建的 100 份档案的排名。

生成一份代表员工历史中最突出特征的档案，并计算此高风险档案与在职员工档案的相似度。


我该如何追踪流失风险最高的人的档案？]]></description>
      <guid>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-employees-in-a-company</guid>
      <pubDate>Wed, 28 Aug 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>Google Collab 显示运行时在运行数据集时断开连接</title>
      <link>https://stackoverflow.com/questions/75904671/google-collab-shows-runtime-disconnected-on-running-dataset</link>
      <description><![CDATA[我在大小为 (2700000x7) 的二进制数据上运行决策树的 ML 代码，但当我运行它时，Google Collab 崩溃了，并在大约 3 小时后显示运行时断开连接。RAM 利用率只有 3GB。
尝试将数据集大小减小到 (100000x7)，它可以正常工作，但超过这个大小就会崩溃。]]></description>
      <guid>https://stackoverflow.com/questions/75904671/google-collab-shows-runtime-disconnected-on-running-dataset</guid>
      <pubDate>Sat, 01 Apr 2023 06:49:04 GMT</pubDate>
    </item>
    </channel>
</rss>