<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Mar 2024 12:23:39 GMT</lastBuildDate>
    <item>
      <title>为新数据集创建梯度下降模型时出错</title>
      <link>https://stackoverflow.com/questions/78120856/error-while-creating-gradient-descent-model-for-a-new-dataset</link>
      <description><![CDATA[导入 pandas 作为 pd

文件 = pd.read_excel(&#39;slr06.xlsx&#39;)
＃ 数据
x = pd.DataFrame(文件, 列=[&#39;X&#39;])
y = pd.DataFrame(文件, 列=[&#39;Y&#39;])
＃ 参数
w = 0.0
b = 0.0
# 超参数
学习率 = 0.01

# 创建梯度下降

def Descend(x, y, w, b, 学习率):
    dl_dw = 0.0
    dl_db = 0.0
    n = x.形状[0]
    # 损失 = (y-yhat)**2
    对于 zip(x, y) 中的 xi, yi：
        dl_dw = -2*xi*(yi-(w*xi+b))
        dl_db = -2*(yi-(w*xi+b))
    # 进行更新
    w = w - 学习率*(1/n)*dl_dw
    b = b - 学习率*(1/n)*dl_db
    
    返回w,b
# 迭代更新
对于范围（500）内的纪元：
    w, b = 下降(x, y, w, b, 学习率)
    yhat = w * x + b
    损失 = np.divide(np.sum((y - yhat)**2, axis=0), x.shape[0])
    print(f&quot;{epoch} 损失是 {loss} 参数 w: {w} | b:{b}&quot;)


我是机器学习新手，在使用数据集进行练习时遇到了错误
这是数据集的图片 - 
这是错误图片 - 
我正在创建一个梯度下降模型，我之前已经使用不同的数据集练习过该模型。我尝试使用随机数据来练习并更好地理解梯度下降]]></description>
      <guid>https://stackoverflow.com/questions/78120856/error-while-creating-gradient-descent-model-for-a-new-dataset</guid>
      <pubDate>Thu, 07 Mar 2024 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>apt 安装的 cudnn 在终端之间不持久</title>
      <link>https://stackoverflow.com/questions/78120833/apt-installed-cudnn-not-persisting-between-terminaks</link>
      <description><![CDATA[安装 nvidia cudnn
sudo apt install nvidia-cudnn nvidia-cuda-toolkit

似乎没有在 shell 和重新启动 PC 之间保留某些变量：它在安装时工作正常，但在启动新 shell 时，如果程序需要 cudnn，则无法找到共享库。
操作系统是 Ubuntu 23.10，使用 Fish 作为我的主要 shell
重新安装可以解决此问题，但似乎不太理想]]></description>
      <guid>https://stackoverflow.com/questions/78120833/apt-installed-cudnn-not-persisting-between-terminaks</guid>
      <pubDate>Thu, 07 Mar 2024 11:05:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pyspark Pipelines 进行特征选择</title>
      <link>https://stackoverflow.com/questions/78120573/feature-selection-using-pyspark-pipelines</link>
      <description><![CDATA[我一直在查看 ML 模型的 Spark 文档，并在查看开箱即用的功能选择器时，我发现有一些选项。在我经常使用的数据集中，我混合了连续数据和分类数据以及用于分类的二进制目标。
最接近的似乎是单变量特征选择器，但文档表明（如下所示）它不能与连续特征和分类特征的混合一起使用，而应该是其中之一。

&lt;标题&gt;

特征类型
标签类型
评分函数


&lt;正文&gt;

分类
分类
卡方 (chi2)


连续
分类
方差分析测试（f_classif）


连续
连续
F值（f_regression）



我目前正在分析功能重要性并希望选择我的功能，但想知道 Spark 是否有任何功能可以满足此要求？]]></description>
      <guid>https://stackoverflow.com/questions/78120573/feature-selection-using-pyspark-pipelines</guid>
      <pubDate>Thu, 07 Mar 2024 10:30:30 GMT</pubDate>
    </item>
    <item>
      <title>生物医学信号分析（EEG）一般分析步骤</title>
      <link>https://stackoverflow.com/questions/78120287/biomedical-signal-analysiseeg-general-analysis-steps</link>
      <description><![CDATA[我有一个数据框，其中列作为电极位置（10-20 脑电图标准），并且有 10000 行数据，对应于 50 秒的脑电图数据（以 200/秒采样）。我想执行分类，但对生物医学数据不熟悉，我不知道如何处理数据。任何指导表示赞赏
我尝试使用 mne 进行过滤和计算功率谱密度，也尝试创建频谱图，但我得到的结果无效。]]></description>
      <guid>https://stackoverflow.com/questions/78120287/biomedical-signal-analysiseeg-general-analysis-steps</guid>
      <pubDate>Thu, 07 Mar 2024 09:47:17 GMT</pubDate>
    </item>
    <item>
      <title>创建一种算法来自动对缺陷描述进行分类</title>
      <link>https://stackoverflow.com/questions/78120268/creating-an-algorithm-to-automatically-categorize-defect-descriptions</link>
      <description><![CDATA[我有一个缺陷描述列表，我想编写一个算法，可以根据这些描述自动分配缺陷类别。目标是让每个缺陷描述匹配列表中的现有类别，或者在没有合适的匹配时创建一个新类别。我如何在 Python 中开发这样的算法？为了确保分类过程的准确性和效率，我应该考虑哪些因素？
我自己已经尝试过使用 TF-IDF 方法，但效果不佳。]]></description>
      <guid>https://stackoverflow.com/questions/78120268/creating-an-algorithm-to-automatically-categorize-defect-descriptions</guid>
      <pubDate>Thu, 07 Mar 2024 09:44:37 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？
&lt;小时/&gt;
---&gt; 13 clf.fit(X_train, y_train, **fit_params)
类型错误：OneVsRestClassifier.fit() 获得意外的关键字参数“eval_set”]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>使用目标列识别机器学习问题</title>
      <link>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</link>
      <description><![CDATA[我正在解决一个问题，该问题使用给定的输入数据和选择的目标列自动定义预测类型。
现在的问题是如何区分目标列是多类分类还是二类分类或回归？
我尝试阅读来自 DSML 平台的各种实现的博客，但需要一些清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:26 GMT</pubDate>
    </item>
    <item>
      <title>如何用奇数样本大小批量训练神经网络？</title>
      <link>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</link>
      <description><![CDATA[我是神经网络领域的新手，正在使用 pytorch 进行一些训练。
我决定做一个简单的普通神经网络。
我使用了一个包含 2377 个数字特征和 6277 个示例的个人数据集。
我的第一次尝试是让神经网络预测每个示例，因此伪代码如下所示
对于范围内的 i(...)：
    X = ... # 特征
    y = ... # 结果
    y_pred = 模型(X[i])
    损失=标准(y_pred, y)

    y_pred.size # [1,1]
    y.尺寸#[1,1]

每个时期大约需要 10 秒，我决定使用小批量来改进它。
所以我在开始时定义了批量大小，Pytorch 中的神经网络是这样定义的
&lt;前&gt;&lt;代码&gt;batch_size = 30
n_inputs = X.size[1] #2377

## 2 个隐藏层
模型 = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    ReLU(),
    nn.线性(1024, 512),
    ReLU(),
    nn.线性(512, 356),
    ReLU(),
    nn.Linear(356,batch_size),
    ReLU(),
）

然后我分批进行训练
对于范围（5）内的纪元：
    总损失 = 0
    排列 = torch.randperm(X.size()[0])
    对于范围内的 i（0，X.size（）[0]，batch_size）：
        优化器.zero_grad()
        索引 = 排列[i:i+batch_size]
        batch_x, batch_y = x[索引], y[索引]

        ypred = 模型(batch_x)
        损失=标准(ypred,batch_y)
        总损失 += loss.item()
        
        ## 更新权重
        loss.backward()
        优化器.step()

现在的问题是我的神经网络总是输出 100 个值但最后的批量大小可能会有所不同。
事实上，如果我选择 100 作为批量大小，最后一批将由 77 个示例组成 (6277%100)。
我确信有一种方法可以解决这个问题，并且我的结构中有一个错误，但我看不到它。
您能帮助我概括批量训练以处理任意数量的示例和批量大小吗？]]></description>
      <guid>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</guid>
      <pubDate>Thu, 07 Mar 2024 08:58:57 GMT</pubDate>
    </item>
    <item>
      <title>高级机器学习算法？</title>
      <link>https://stackoverflow.com/questions/78119578/advanced-ml-algorithms</link>
      <description><![CDATA[我学习了所有机器学习算法，包括回归、分类和聚类，例如线性回归、物流、决策树、随机森林、SVM、XGBoost、AdaBoost、LightBoost、KNN、KMeans、KMeans+、DBSCAN、分层聚类、Mini- Batch KMeans 等也。
和降维技术。
现在，我的问题是：我应该学习哪些更高级的技术？]]></description>
      <guid>https://stackoverflow.com/questions/78119578/advanced-ml-algorithms</guid>
      <pubDate>Thu, 07 Mar 2024 07:46:25 GMT</pubDate>
    </item>
    <item>
      <title>如何将 BERT 输出转换回 token ID？</title>
      <link>https://stackoverflow.com/questions/78119353/how-to-transform-bert-output-back-into-token-ids</link>
      <description><![CDATA[我试图通过加载一个简单的 BERT 模型、输入一些文本并获取输出来学习 .NET 中机器学习的基础知识。很简单，对吧？
不。
该模型似乎输出两列“onnx::Gather_1269”和“onnx::Gather_1269”。和“1272”。无论哪种情况，输出都是 FLOATS 数组？？？！？！ ...并且它们的负载有负值??????!!!!!
什么？我到底应该如何将这些数据恢复到要取消标记的 tokenID 的 int[] 中？
我觉得我在某个地方错过了整个步骤。救命！
使用 BERTTokenizers；
使用 Microsoft.ML；
使用 Microsoft.ML.Data；

命名空间 TFLibrary；

公共静态类沙箱
{
    公共静态无效DoThing（字符串提示）
    {
        MLContext mlContext = new();
        var pipeline = mlContext.Transforms
            .ApplyOnnxModel(modelFile: @&quot;Models\bert_Opset18.onnx&quot;,
                shapeDictionary: new Dictionary;
                {
                    { “input_ids”, new[] { 1, 128 } },
                    { “attention_mask”, new[] { 1, 128 } },
                    { “onnx::Gather_1269”, new[] { 1, 128, 768 } },
                    { “1272”, 新[] { 1, 768 } },
                },
                inputColumnNames: new[] { “input_ids”, “attention_mask” },
                outputColumnNames: new[] { “onnx::Gather_1269”, “1272” },
                GPU设备ID: 0,
                FallbackToCpu: true);

        var model = pipeline.Fit(mlContext.Data.LoadFromEnumerable(new List()));

        var tokenizer = new BertBaseTokenizer();
        var 编码 = tokenizer.Encode(128, 提示);

        var 输入 = 新的 ModelInput()
        {
            InputIds = 编码.Select(t =&gt; t.InputIds).ToArray(),
            AttentionMask = 编码.Select(t =&gt; t.AttentionMask).Select(m =&gt; m / (float)long.MaxValue).ToArray()
        };

        var dataView = mlContext.Data.LoadFromEnumerable(new[] { input });

        var 输出 = model.Transform(dataView);
        列表&lt;浮动&gt; tkFloats = output.GetColumn(“onnx::Gather_1269”).SelectMany(f =&gt; f).ToList();

        int[] tokenIds = { 0 }; //我如何填充这个？模型结果 (tkFloats) 均为浮点数，且它们的载荷均为负值
        //我究竟该如何将其恢复到令牌IDS？
        //为什么输出不是那些ID？

        Console.WriteLine(string.Join(&#39; &#39;, tokenizer.Untokenize(tokenIds.Select(i =&gt; tokenizer.IdToToken(i)).ToList())));
    }

    公共类模型输入
    {
        [矢量类型(1, 128)]
        [列名(“input_ids”)]
        公共长[] InputIds { 获取;放; }

        [矢量类型(1, 128)]
        [列名(“attention_mask”)]
        公共浮动[] AttentionMask { 得到;放; }
    }

    //无法弄清楚我在哪里需要这个，如果我需要的话
    /*公共类模型输出
    {
        [矢量类型(1, 128, 768)]
        [ColumnName(“onnx::Gather_1269”)]
        公共浮动[] Gather_1269 { 得到;放; }

        [矢量类型(1, 768)]
        [列名(“1272”)]
        公共浮动[] _1272 { 得到;放; }
    }*/
}
]]></description>
      <guid>https://stackoverflow.com/questions/78119353/how-to-transform-bert-output-back-into-token-ids</guid>
      <pubDate>Thu, 07 Mar 2024 07:06:18 GMT</pubDate>
    </item>
    <item>
      <title>神经语言模型：出现错误 - ValueError：无法将大小为 380 的数组重塑为形状 (1,1,10)</title>
      <link>https://stackoverflow.com/questions/78119223/neural-language-model-getting-error-valueerror-cannot-reshape-array-of-size</link>
      <description><![CDATA[我正在尝试遵循基于字符的神经语言模型的教程，该模型尝试预测“基于特定单词的序列中的单词”。
按照指示，我已将文本序列生成到文件中，定义语言模型并保存模型以及映射字符（如 *.pkl）。
接下来，为了生成文本（使用保存的映射），将文本编码为整数，并使用 predict_classes() 运行代码以按顺序预测字符。使用以下函数（下面的代码）使用种子文本来预测字符序列。
但是，当我运行脚本时遇到以下错误：
&lt;块引用&gt;
ValueError：无法将大小为 380 的数组重塑为形状 (1,1,10)

这是我收到错误的部分：
defgenerate_seq（模型，映射，seq_length，seed_text，n_chars）：
    in_text = 种子文本
    # 生成固定数量的字符
    对于 _ 在范围内（n_chars）：
        # 将字符编码为整数
        编码 = [in_text 中字符的映射 [字符]]
        # 将序列截断为固定长度
        编码 = pad_sequences([编码], maxlen=seq_length, 截断=&#39;pre&#39;)
        # 一种热编码
        编码= to_categorical（编码，num_classes = len（映射））
        编码 = 编码.reshape(1, 编码.shape[0], 编码.shape[1]) # &lt;--- 错误行
        # 预测字符
        yhat = model.predict_classes（编码，详细=0）
        # 反向映射整数到字符
        输出字符 = &#39;&#39;
        对于 char，mapping.items() 中的索引：
            如果索引 == yhat：
                输出字符 = 字符
                休息
        # 附加到输入
        输入文本 += 输出字符
    返回 in_text

该函数正在被调用：
print(generate_seq(model, mapping, 10, &#39;唱儿子&#39;, 20))

为什么我会收到此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78119223/neural-language-model-getting-error-valueerror-cannot-reshape-array-of-size</guid>
      <pubDate>Thu, 07 Mar 2024 06:41:41 GMT</pubDate>
    </item>
    <item>
      <title>训练有素的多智能体强化学习模型在增加智能体时崩溃</title>
      <link>https://stackoverflow.com/questions/78119078/trained-multi-agent-rl-model-crashes-when-increasing-agents</link>
      <description><![CDATA[我正在 Open AI Gym 中制作一个具有稳定基线的自定义 Boid 植绒环境 3。 
错误1：
我已经创建了环境并针对 3 个 boids 进行了测试，该模型是使用 3 个 boids 制作的，但是当我在不更改任何内容的情况下测试 10 个 boids 时，会出现错误：
错误 1
错误2：
我测试了 boids 的不同初始化位置，但获得了奖励 600200，并且它们按预期移动。
但是，当我为了一致性而使用模型进行重新训练和测试时，它的表现不佳，并且我的剧集奖励大多为负。虽然我什么也没改变。
我的模型是训练有素的模型，是侥幸还是过度拟合？因为它对于不同的职位都有足够的表现。
奖励]]></description>
      <guid>https://stackoverflow.com/questions/78119078/trained-multi-agent-rl-model-crashes-when-increasing-agents</guid>
      <pubDate>Thu, 07 Mar 2024 06:04:41 GMT</pubDate>
    </item>
    <item>
      <title>将数据帧写入功能存储时出现 ConcurrentAppendException</title>
      <link>https://stackoverflow.com/questions/78118939/concurrentappendexception-while-writing-dataframe-to-feature-store</link>
      <description><![CDATA[我正在尝试将 Spark(pyspark) 数据帧写入特征存储
fs = FeatureStoreClient()

    尝试：
        fs.get_table(fs_name)
    除了值错误：
        fs.create_table(
            名称=文件系统名称，
            Primary_keys=pri_key_cols,
            df=df,
            时间戳_键=时间戳_列，
            描述=表描述，
            标签=tags_dict，
        ）
    别的：
        fs.write_table(name=fs_name, mode=“覆盖”, df=df)

此 Databricks 笔记本附加到 ADF 管道，当它运行时有时会出现异常并且管道失败。
ConcurrentAppendException：文件已通过并发更新添加到表的根中。请重试该操作。
冲突的提交：
]]></description>
      <guid>https://stackoverflow.com/questions/78118939/concurrentappendexception-while-writing-dataframe-to-feature-store</guid>
      <pubDate>Thu, 07 Mar 2024 05:23:31 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 Tensorflow.Net 模型时，未将对象引用设置为对象的实例[重复]</title>
      <link>https://stackoverflow.com/questions/78107434/object-reference-not-set-to-an-instance-of-an-object-when-attempting-to-train-te</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107434/object-reference-not-set-to-an-instance-of-an-object-when-attempting-to-train-te</guid>
      <pubDate>Tue, 05 Mar 2024 11:37:44 GMT</pubDate>
    </item>
    <item>
      <title>无法创建 Spark 会话</title>
      <link>https://stackoverflow.com/questions/55971395/unable-to-create-spark-session</link>
      <description><![CDATA[&lt;块引用&gt;
  当我创建 Spark 会话时，它抛出错误


无法创建 Spark 会话
使用pyspark，代码片段：

ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-2262882856df&gt;在&lt;模块&gt;()中
     37 如果 __name__ == &quot;__main__&quot;:
     38conf = SparkConf（）
---&gt; 39 sc = SparkContext（conf=conf）
     40 # 打印（sc.版本）
     41 # sc = SparkContext(conf=conf)

〜/anaconda3/lib/python3.5/site-packages/pyspark/context.py 在 __init__(self、master、appName、sparkHome、pyFiles、环境、batchSize、序列化器、conf、网关、jsc、profiler_cls)
    131 “注意此选项将在 Spark 3.0 中删除”）
    132
--&gt; 133 SparkContext._ensure_initialized（自我，网关=网关，conf=conf）
    134 尝试：
    第135章

_ensure_initialized 中的 ~/anaconda3/lib/python3.5/site-packages/pyspark/context.py（cls、实例、网关、conf）
    330 “由 %s 在 %s:%s 创建”
    331 %（当前应用程序名称、当前主控、
--&gt;第332章
    第333章：
    第334章

ValueError：无法同时运行多个 SparkContext；由 __init__ 在 :33 创建的现有 SparkContext(app=pyspark-shell, master=local[*])



进口


&lt;前&gt;&lt;代码&gt;
从 pyspark 导入 SparkConf、SparkContext


我尝试了这种替代方法，但也失败了：

spark = SparkSession(sc).builder.appName(&quot;检测恶意 URL 应用程序&quot;).getOrCreate()

这会引发另一个错误，如下所示：
NameError：名称“SparkSession”未定义
]]></description>
      <guid>https://stackoverflow.com/questions/55971395/unable-to-create-spark-session</guid>
      <pubDate>Fri, 03 May 2019 14:02:42 GMT</pubDate>
    </item>
    </channel>
</rss>