<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Sep 2024 15:16:58 GMT</lastBuildDate>
    <item>
      <title>如果数据集较小，是否建议拟合线性回归模型并在不拆分数据集的情况下计算分数？[关闭]</title>
      <link>https://stackoverflow.com/questions/78953584/is-fitting-the-linear-regression-model-and-calculating-the-score-without-splitti</link>
      <description><![CDATA[我一直在阅读有关随机梯度下降的教程，网址为 https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32

在代码示例中，作者使用“feature_array”和“target array”拟合模型，而没有使用 train_test_split() 方法将其拆分为训练和测试数据集。

我知道这里的数据集很小，但真的推荐这样做吗？
Q1。最重要的是，在用于训练模型的同一数据集上计算 r2_score 即“r_squared”的原因是什么？
Q2。这个分数值与“预测（预测烹饪时间）”有何关系？
Q3。如果我想使用 &#39;sklearn.metrics&#39; 中定义的 r2_score() 方法计算 r2_score，那么我该怎么做？
以下是本教程中的代码：
import numpy as np
import time
from sklearn.linear_model import SGDRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

def stochastic_gd(feature_array,
target_array, 
to_predict, 
learn_rate_type=&quot;invscaling&quot;):

start_time = time.time()
lin_reg_pipeline = make_pipeline(StandardScaler(), 
SGDRegressor(learning_rate=learn_rate_type))
lin_reg_pipeline.fit(feature_array, target_array)
stop_time = time.time()
print(f&quot;总运行时间：{start_time - stop_time:.6f}s&quot;)
print(f&quot;学习率算法：{learn_rate_type}&quot;)

print(f&quot;模型系数：{lin_reg_pipeline[1].coef_}&quot;)
print(f&quot;迭代次数：{lin_reg_pipeline[1].n_iter_}&quot;)
return lin_reg_pipeline

feature_array = [[500, 80, 30, 10],
[550, 75, 25, 0],
[475, 90, 35, 20],
[450, 80, 20,25],
[465, 75, 30, 0],
[525, 65, 40, 15],
[400, 85, 33, 0],
[500, 60, 30, 30],
[435, 45, 25, 0]]

target_array = [17, 11, 21, 23, 22, 15, 25, 18, 16]
to_predict = [[510, 50, 35, 10]]
lin_reg = stochastic_gd(feature_array, target_array, to_predict)
prediction = lin_reg.predict(to_predict)
print(f&quot;预测烹饪时间：{np.round(prediction, 0)[0].astype(&#39;int&#39;)} min&quot;)
r_squared = lin_reg.score(feature_array, target_array).reshape(-1, 1)[0][0]

print(f&quot;R-squared: {np.round(r_squared, 2)}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78953584/is-fitting-the-linear-regression-model-and-calculating-the-score-without-splitti</guid>
      <pubDate>Thu, 05 Sep 2024 14:34:55 GMT</pubDate>
    </item>
    <item>
      <title>用于检测视频异常的机器学习模型</title>
      <link>https://stackoverflow.com/questions/78953457/ml-models-for-detecting-irregularities-in-videos</link>
      <description><![CDATA[我需要构建一个系统来检测视频中每一帧的以下违规行为。有人能建议我应该使用哪种类型的机器学习或计算机视觉模型来完成这些任务吗？
以下是我需要检测的具体违规行为：
图像中有人吗？
是否有多个人在场？
人应该可见并直接面对摄像头。
禁止使用手机、平板电脑、智能手表或未经授权的电子设备。
我正在尝试最大限度地提高准确性和速度，因此如果您能提供任何关于哪种模型或技术最适合解决此类问题的建议，我将不胜感激。提前致谢！
我尝试了 CLIP 模型，但它没有达到我的期望。]]></description>
      <guid>https://stackoverflow.com/questions/78953457/ml-models-for-detecting-irregularities-in-videos</guid>
      <pubDate>Thu, 05 Sep 2024 14:07:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit Learn 在 Vertex 上导入模型</title>
      <link>https://stackoverflow.com/questions/78953273/importing-a-model-with-scikit-learn-on-vertex</link>
      <description><![CDATA[伙计们，我正尝试从本地导入模型，但每次我都会从 gcp 日志中收到相同的错误。框架是 scikit-learn
AttributeError: 无法从 &#39;/usr/app/model_server.py&#39;&gt; 获取 &lt;module &#39;model_server&#39; 上的属性 &#39;preprocess_text&#39; 
存在此问题的代码片段是
complaints_clf_pipeline = Pipeline(
[
(&quot;preprocess&quot;, text.TfidfVectorizer(preprocessor=utils.preprocess_text, ngram_range=(1, 2))),
(&quot;clf&quot;, naive_bayes.MultinomialNB(alpha=0.3)),
]
)

这个
preprocess_text 

来自上面的单元格，但我一直收到此问题，model_server 不存在于我的代码中。
有人可以帮忙吗？
我尝试重构代码但得到了同样的错误，尝试撤消此管道结构，但在尝试通过 API 查阅模型时又收到另一个错误。]]></description>
      <guid>https://stackoverflow.com/questions/78953273/importing-a-model-with-scikit-learn-on-vertex</guid>
      <pubDate>Thu, 05 Sep 2024 13:24:34 GMT</pubDate>
    </item>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理</title>
      <link>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个 3D 重建管道，它使用多视图模型 (zero123)，可以从一个输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
我现在的问题是：InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？
这是运行管道的 InstantMesh 代码：https://github.com/TencentARC/InstantMesh/blob/main/run.py]]></description>
      <guid>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Thu, 05 Sep 2024 12:19:51 GMT</pubDate>
    </item>
    <item>
      <title>具有高基数分类列的聚类数据</title>
      <link>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</link>
      <description><![CDATA[我有一个数据 - 例如：电器商店中的商品数据集。
让我们考虑以下是我的列：
ITEM_NO（唯一标识商品的编号）
、品牌（制造品牌）
、类别
、材料
、工艺
、重量
、价格
、类别特定属性（假设您有某个类别的特定属性，而其他商品可能不存在这些属性）
让我们假设材料、工艺的基数非常高（大约 5k）
我的最终目标：我想将类似类型的商品分组，基本上就是聚类
注意：每个材料值和工艺值都具有独特的意义，因此不能组合在一起
我尝试通过采用每个类别对数据进行分组，然后进行聚类（在缩放、编码和 PCA 之后），但效果并不好。但由于材料和工艺的基数非常高，所以这没有奏效。 [我旋转了材料和工艺的列]。
下一个方法：我根据材料对项目进行分组，并选取构成初始研发项目数量最多的材料，并将这些项目创建为数据集。接下来，我选取上述项目数据集的工艺列（以逗号分隔的字符串，例如 P1、P2、P3 作为值）。因此，我使用 Jaccard 指数根据工艺成对查找每个项目之间的相似性，然后选取 Jaccard 指数值大于 0.7 的项目对，但这花了很长时间。此外，由于材料的基数非常高，我不得不对每一种材料重复此练习。
我想要一个可扩展的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</guid>
      <pubDate>Thu, 05 Sep 2024 12:15:35 GMT</pubDate>
    </item>
    <item>
      <title>在 LSTM 神经网络中可以使用什么类似于 SHAP 值？（特征重要性已经被使用，我们需要更多的指标）</title>
      <link>https://stackoverflow.com/questions/78952839/what-can-be-used-in-lstm-neural-network-similar-to-shap-values-feature-importa</link>
      <description><![CDATA[在使用长短期记忆 (LSTM) 神经网络执行顺序数据任务的情况下，了解不同输入特征的贡献或重要性对于模型的可解释性至关重要。虽然已经采用了特征重要性技术，但当前的挑战是探索其他指标或方法，以深入了解各个特征如何影响模型的预测。
解决这一挑战的一种潜在方法是应用 SHAP (SHapley Additive exPlanations) 值，这些值通常用于传统机器学习模型中的特征归因。SHAP 值通过将每个特征的贡献归因于预测来提供一种解释模型输出的一致方法。但是，由于 LSTM 网络涉及跨时间步骤的顺序依赖关系和复杂交互，因此实施 SHAP 或类似方法需要适应性或替代可解释性技术，这些技术适用于深度学习模型，尤其是处理时间序列的模型。
目标是识别和实施超越传统特征重要性的指标，重点关注可以解释输入数据的时间结构的方法，并提供更细致的见解，了解每个特征如何影响 LSTM 的预测。
尝试的内容：我们应用传统的特征重要性指标来了解不同的输入特征如何影响 LSTM 神经网络的预测。此外，我们考虑使用机器学习模型中常用的 SHAP 值，以更详细的方式归因特征重要性。
预期结果：我们预计 SHAP 值或类似的可解释性方法将有助于解释每个特征对 LSTM 预测的影响，从而清楚地了解时间特征如何随时间影响模型的输出。
实际结果：虽然特征重要性让我们大致了解哪些特征有影响，但由于 LSTM 网络的顺序性和时间依赖性，SHAP 值更难实现。我们发现为更简单的模型设计的 SHAP 方法没有完全捕捉到 LSTM 固有的时间依赖性的复杂性。因此，模型预测的可解释性仍然有限，需要替代方法。]]></description>
      <guid>https://stackoverflow.com/questions/78952839/what-can-be-used-in-lstm-neural-network-similar-to-shap-values-feature-importa</guid>
      <pubDate>Thu, 05 Sep 2024 11:41:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 分析日志文件[关闭]</title>
      <link>https://stackoverflow.com/questions/78952683/analyse-log-file-with-ml</link>
      <description><![CDATA[我有一些带时间戳的日志文件，这些日志文件来自设备的控制器、驱动程序等。我在日志文件中看到了一些错误代码，但原因不明。我想分析这些日志文件并确定模式，例如错误是否连续发生，或者一个错误是否依赖于另一个错误。错误也可能由其他错误触发，甚至可能是一天前或一秒钟前的错误。最好的和最简单的方法是什么？或者如果有类似问题的解决方案，请告诉我 :)
1)我读过关于 LSTM 和 RNN 的文章，但据我所知，它们用于错误预测，而我的情况是，连续错误的发生窗口可能会有所不同（1 天 -1 秒），提前 1 秒预测它不会那么有帮助。
2)我考虑绘制图表并将每个错误作为图表的一个节点 - 但我不太确定如何处理这个问题。
3)我读过一些关于聚类的文章 -&gt; 但它不会丢弃发生的顺序]]></description>
      <guid>https://stackoverflow.com/questions/78952683/analyse-log-file-with-ml</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:41 GMT</pubDate>
    </item>
    <item>
      <title>无法让 XGBRegressor 输出 0 到 1 之间的值</title>
      <link>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</link>
      <description><![CDATA[我们创建了一个应用程序，在该应用程序中，我们为客户提供贷款优惠，客户可以根据其用例更改金额。现在，我正在尝试制作一个 XBGRegressor 模型，该模型可以预测客户的接受率金额，这将在下一个过程中进一步使用。
我使用的特征在某些列中具有空值，因此我制作了 XGBoost Regressor，因为它可以轻松处理空值。将平均值代入这些列是不可能的。我的训练数据的接受率在 0 到 1 的范围内，但我的模型预测的值仍然大于 1 甚至为负数。
我正在使用 Baysian Optimiser 来改进模型。 R 平方值不错（约为 0.75），有什么方法可以进一步改进吗？
这是我目前正在使用的代码。
def get_forecast(just_train_df, metric, data_df):
print(&#39;\nMetrics are:&#39;,metric)
data = just_train_df.copy()
X, y, X_train, y_train, X_test, y_test, X_forecast, y_original = data_preprocessing(data, metric, [])

pbounds = {
&#39;max_depth&#39;: (3, 10),
&#39;learning_rate&#39;: (0.01, 0.3),
&#39;gamma&#39;: (0, 0.5),
&#39;min_child_weight&#39;: (1, 10),
&#39;subsample&#39;: (0.5, 1.0),
&#39;colsample_bytree&#39;: (0.5, 1.0),
&#39;reg_alpha&#39;: (0, 1.0),
&#39;reg_lambda&#39;: (0, 1.0)
}

# 贝叶斯优化
def xgb_cv(max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda):

xgb_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(max_depth),
learning_rate=learning_rate,
gamma=gamma,
min_child_weight=int(min_child_weight),
subsample=subsample,
colsample_bytree=colsample_bytree,
reg_alpha=reg_alpha,
reg_lambda=reg_lambda,
n_jobs=-1
)
r2_scorer = make_scorer(r2_score)
scores = cross_val_score(xgb_model, X_train, y_train,scoring=r2_scorer, cv=5, n_jobs=-1)
return scores.mean() # 返回负均方误差，因为 BayesianOptimization 最小化了目标函数

optimizer = BayesianOptimization(
f=xgb_cv,
pbounds=pbounds,
random_state=36,
verbose=2
)

print(&#39;Initiated Bayesian Optimizer...\n&#39;)
optimizer.maximize(init_points=5, n_iter=50)
print(&#39;\nCompleted Bayesian Optimizer\n&#39;)

print(&quot;Best hyperparameters: &quot;, optimizer.max[&#39;params&#39;], &#39;\n&#39;)

# 使用 Bayesian Optimizer 中的最佳参数训练模型
best_params = optimizer.max[&#39;params&#39;]
final_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(best_params[&#39;max_depth&#39;]),
learning_rate=best_params[&#39;learning_rate&#39;],
gamma=best_params[&#39;gamma&#39;],
min_child_weight=int(best_params[&#39;min_child_weight&#39;]),
subsample=best_params[&#39;subsample&#39;],
colsample_bytree=best_params[&#39;colsample_bytree&#39;],
reg_alpha=best_params[&#39;reg_alpha&#39;],
reg_lambda=best_params[&#39;reg_lambda&#39;],
n_jobs=-1,
random_state=0
)

final_model.fit(X_train, y_train)

# 数据集上的预测
y_pred_test = final_model.predict(X_test)
y_pred_train = final_model.predict(X_train)
y_pred_val = final_model.predict(X_forecast)

# 在训练数据上评估 best_model
mae = mean_absolute_error(y_train, y_pred_train)
rmse = mean_squared_error(y_train, y_pred_train, squared=False)
r2_train = r2_score(y_train, y_pred_train)
print(f&quot;训练平均绝对误差：{mae}&quot;)
print(f&quot;训练 R 平方：{r2_train}\n&quot;)

# 在测试数据上评估模型
mae = mean_absolute_error(y_test, y_pred_test)
rmse = mean_squared_error(y_test, y_pred_test, squared=False)
r2_test = r2_score(y_test, y_pred_test)
print(f&quot;测试平均绝对误差：{mae}&quot;)
print(f&quot;测试 R 平方：{r2_test}\n&quot;)

目前我面临的一个主要问题是，大约 65-70% 的数据采用 1 的比例。我应该更改模型还是对现有模型进行更改。我是否应该在 XGBoost 中将输入特征标准化为 0 和 1。我在这个领域还很新。]]></description>
      <guid>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</guid>
      <pubDate>Thu, 05 Sep 2024 10:48:03 GMT</pubDate>
    </item>
    <item>
      <title>分类模型仅预测单个类别</title>
      <link>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</link>
      <description><![CDATA[我有一个逻辑回归模型，我正在尝试在我的 NLP 项目中做出健康的预测。我做了一些事情，下面是我的分类报告：
 精确率 召回率 f1 分数 支持率

0 0.68 0.83 0.75 23
1 0.78 0.78 0.78 23
2 0.87 0.87 0.87 30
3 0.76 0.62 0.68 26

准确率 0.77 102
宏平均值 0.77 0.77 0.77 102
加权平均值 0.78 0.77 0.77 102

我认为它实际上没有看起来那么糟糕，但即使我以不同的方式设置参数，大多数时候它也会返回“3”，这是我的一个类。我的意思是，它看起来总是专注于一个单一的类别，即使我的分类报告不支持这种行为。
总之，我的模型看起来很平衡，但它的表现却不是那样。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</guid>
      <pubDate>Thu, 05 Sep 2024 08:30:57 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 Google Teachable 机器模型作为对象检测模型吗？</title>
      <link>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</link>
      <description><![CDATA[我正在为我的论文项目工作，这是一个带有移动对象检测功能的自动收银机应用程序。我面临的问题是，自定义项目的变体太多了（大约 250 个类别）。如果我要微调以前的模型（如 MobileNet 或 YOLO），这将花费太多时间，因为我仍然需要创建 POS、数据库和集成热敏打印机。
那么，除了微调以前的模型外，我是否可以只使用 Google Teachable Machine 来处理我的对象检测数据集（假设背景相同）？
应用程序的工作方式是，收银员会在白色背景上拍摄买家想要购买的商品的照片，然后应用程序会自动检测出哪些商品在框架内（使用 Google Teachable Machine .tflite 模型）。
Teachable Machine .tflite 模型是否可以替换下面这段代码中的 modelPath？提前致谢。
private fun runObjectDetection(bitmap: Bitmap) {
// 步骤 1：创建 TFLite 的 TensorImage 对象
val image = TensorImage.fromBitmap(bitmap)

// 步骤 2：初始化检测器对象
val options = ObjectDetector.ObjectDetectorOptions.builder()
.setMaxResults(5)
.setScoreThreshold(0.5f)
.build()
val detector = ObjectDetector.createFromFileAndOptions(
this, // 应用程序上下文
**&quot;model.tflite&quot;, **
options
)
// 步骤 3：将给定的图像提供给模型并打印检测结果
val results = detector.detect(image)

// 步骤 4：解析检测结果并显示
debugPrint(results)

val resultToDisplay = results.map {
// 获取 top-1 类别并制作显示文本
val category = it.categories.first()
val text = &quot;${category.label}, ${category.score.times(100).toInt()}%&quot;

// 创建数据对象，用于显示检测结果
DetectionResult(it.boundingBox, text)
}

// 将检测结果绘制到位图上并显示。
val imgWithResult = drawDetectionResult(bitmap, resultToDisplay)
runOnUiThread {
inputImageView.setImageBitmap(imgWithResult)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</guid>
      <pubDate>Thu, 05 Sep 2024 07:38:34 GMT</pubDate>
    </item>
    <item>
      <title>具有共享权重的嵌套模块是否应为 nn.Module 对象参数？</title>
      <link>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</link>
      <description><![CDATA[我希望两个 torch.nn.Module 类共享其部分架构和权重，如下例所示：
from torch import nn

class SharedBlock(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

self.block = nn.Sequential(
# 在此处定义一些块架构...
)

def forward(self, x):
return self.block(x)

class MyNestedModule(nn.Module):
def __init__(self, shared_block: nn.Module, *args, **kwargs):
super().__init__()

self.linear = nn.Linear(...)
self.shared_block = shared_block

def forward(self, x):
return self.shared_block(self.linear(x))

class MyModule(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

# 应该是：
shared_block = SharedBlock(*args, **kwargs)
# 或者：
self.shared_block = SharedBlock(*args, **kwargs) # 注意：self。
# ...如果有区别，区别是什么？

self.nested1 = MyNestedModule(shared_block, *args, **kwargs)
self.nested2 = MyNestedModule(shared_block, *args, **kwargs)

def forward(self, x):
x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
y_1 = self.nested1(x_1)
y_2 = self.nested2(y_2)
return y_1, y_2

我想知道 shared_block 是否应该是 MyModule 的对象参数。我认为不是，因为它在 MyNestedModule 类对象中都被设置为对象参数，所以它应该在 torch grad 中注册，但如果我确实在 MyModule 中将它创建为对象参数，会发生什么？]]></description>
      <guid>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</guid>
      <pubDate>Wed, 04 Sep 2024 20:06:25 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 Conv2D 层的尺寸？</title>
      <link>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</link>
      <description><![CDATA[我试图了解我的 GAN 生成器输出的尺寸。每层之后的结果尺寸如下：
开始：torch.Size([128, 74, 1, 1]) 
block1 之后：torch.Size([128, 256, 3, 3]) 
block2 之后：torch.Size([128, 128, 6, 6]) 
block3 之后：torch.Size([128, 64, 13, 13]) 
block4 之后：torch.Size([128, 1, 28, 28])

生成器代码如下。此处 z_dim 为 74，但最初为 64。它附加了 10 个类标签，如下所示。
fake_noise = get_noise(cur_batch_size, z_dim, device=device) 
noise_and_labels = Combine_vectors(fake_noise, one_hot_labels)
fake = gen(noise_and_labels)

class Generator(nn.Module):
&#39;&#39;&#39;
Generator Class
值：
z_dim：噪声向量的维度，标量
im_chan：输出图像的通道数，标量
（MNIST 是黑白的，因此 1 个通道是您的默认值）
hidden_​​dim：内部维度，标量
&#39;&#39;&#39;
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.block1 = self.make_gen_block(z_dim, hidden_​​dim * 4)
self.block2 = self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1)
self.block3 = self.make_gen_block(hidden_​​dim * 2, hidden_​​dim)
self.block4 = self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True)

def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=1, final_layer=False):
&#39;&#39;&#39;
函数返回对应于生成器块的操作序列DCGAN；
转置卷积、批处理规范（最后一层除外）和激活。
参数：
input_channels：输入特征表示有多少个通道
output_channels：输出特征表示应该有多少个通道
kernel_size：每个卷积滤波器的大小，相当于 (kernel_size, kernel_size)
stride：卷积的步幅
final_layer：布尔值，如果是最后一层则为 true，否则为 false
（影响激活和 batchnorm）
&#39;&#39;&#39;
如果不是 final_layer：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.BatchNorm2d(output_channels),
nn.LeakyReLU(0.2, inplace=True),
)
else：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.Tanh(),
)

def forward(self, noise):
&#39;&#39;&#39;
完成生成器前向传递的函数：给定一个噪声张量，
返回生成的图像。
参数：
noise：具有维度 (n_samples, input_dim) 的噪声张量
&#39;&#39;&#39;
x = noise.view(len(noise), self.z_dim, 1, 1)
print(f&#39;Gen: {x.shape}&#39;)
x = self.block1(x)
print(f&#39;After block1: {x.shape}&#39;)
x = self.block2(x)
print(f&#39;After block2: {x.shape}&#39;)
x = self.block3(x)
print(f&#39;After block3: {x.shape}&#39;)
x = self.block4(x)
print(f&#39;After block4: {x.shape}&#39;)
return x

def get_noise(n_samples, z_dim, device=&#39;cpu&#39;):
&#39;&#39;&#39;
用于创建噪声向量的函数：给定维度 (n_samples, z_dim)
创建该形状的张量，其中填充了来自正态分布的随机数。
参数：
n_samples：要生成的样本数，标量
z_dim：噪声向量的维度，标量
device：设备类型
&#39;&#39;&#39;
return torch.randn(n_samples, z_dim, device=device)

根据此处的公式，第一个块之后的结果将是(1 + 2x0 -1x(3-1) -1)/2 +1 = 0，但它显示 3x3。我在这里做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</guid>
      <pubDate>Wed, 04 Sep 2024 16:09:58 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow fit 不会采用自定义数据集，但会引发错误：ValueError：无法获取具有未知等级的形状的长度</title>
      <link>https://stackoverflow.com/questions/78902994/tensorflow-fit-wont-take-custom-dataset-but-throws-error-valueerror-cannot-t</link>
      <description><![CDATA[我正在尝试训练一个直到最近才正常工作的模型。 fit 函数抛出了以下错误：
 &lt;ipython-input-20-01755a6ded38&gt; in &lt;cell line: 1&gt;()
----&gt; 1 model.fit(
2 dataset,
3 epochs=100,
4 verbose=1,
5 batch_size=8)

1 frames /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py in squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1)
105 def squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1=True):
106 &quot;&quot;&quot;如果等级与预期相差恰好 1，则挤压/扩展最后一个 dim。&quot;&quot;&quot;
--&gt; 107 x1_rank = len(x1.shape)
108 x2_rank = len(x2.shape)
109 if x1_rank == x2_rank:

ValueError: 无法获取具有未知等级的形状的长度。

我尝试将生成器生成的 x 和 y 传递给 fit，并且运行良好，因此这不是形状问题。
以下是错误的重现。该模型只是一个简单的 Sequential 模型：
model = keras.models.Sequential(
[keras.layers.Dense(10,activation=&#39;relu&#39;),
keras.layers.Dense(1,activation=&#39;sigmoid&#39;)]
)
model.compile(loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

数据集由 tf.data.Dataset.from_generator() 生成，如下所示：
#随机数据集
a = tf.convert_to_tensor(np.random.randint(0,100,size=[10,10]))

#数据生成器类
class DataGenerator:
def __init__(self,data,ratio=3):
self._ratio = ratio
self._data = data

def __call__(self):
shape = tf.shape(self._data).numpy()
x = tf.convert_to_tensor(np.random.randint(1000,100000, size=[shape[0] * self._ratio, shape[1]]))
x = tf.concat([self._data, x], axis = 0)
y = tf.convert_to_tensor(np.random.random(shape[0]*(1 + self._ratio)))

产生 x, y

data_gen = DataGenerator(a, 3)
dataset = tf.data.Dataset.from_generator(
data_gen,
output_signature=(
tf.TensorSpec(shape=(None,10), dtype=tf.int32),
tf.TensorSpec(shape=(None), dtype=tf.float32)))

Model.fit() 产生了上述错误：
model.fit(
dataset,
epochs=100,
verbose=1,
batch_size=8)

以下是 Colab 中错误的重现：https://colab.research.google.com/drive/1f7I2St2U3LxaWZZSTxT2xWN14gCZMBSE?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78902994/tensorflow-fit-wont-take-custom-dataset-but-throws-error-valueerror-cannot-t</guid>
      <pubDate>Thu, 22 Aug 2024 18:25:54 GMT</pubDate>
    </item>
    <item>
      <title>google colab 上的 FIt-SNE</title>
      <link>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</link>
      <description><![CDATA[如何在我的 colab 笔记本上实现基于 FFT 加速插值的 t-SNE (FIt-SNE)？
我试图在 狗情绪 的 kaggle 数据集上计算 t-SNE，
我首先尝试获取第一个组件。
pc = 60
pca = decomposition.PCA(n_components=pc)
_ = pca.fit(images)
imgPCA = pca.transform(images)

tsne = TSNE(n_components=2)
Z = tsne.fit_transform(imgPCA)
plot_embedding(Z)

然后我尝试了 多核 t-SNE 来提高迭代次数，但我还是不喜欢它
!pip install git+https://github.com/DmitryUlyanov/Multicore-TSNE.git

from MulticoreTSNE import MulticoreTSNE

Z = MulticoreTSNE(n_jobs=4, n_iter=10000).fit_transform(imgPCA)
plot_embedding(Z, show_axis = True)

现在我想尝试使用 FIt-SNE，但我不知道如何使用它，你能帮我吗？
或者，如果您愿意，也许您可​​以帮助改进前面的代码片段。
这里有代码来理解数据集的格式：
import pandas as pd
import cv2

img_size = (192,192,3)
num_px = img_size[0] * img_size[1] * img_size[2]

directory = &#39;/content/drive/MyDrive/Colab Notebooks/ML/Dog_Emotion/&#39; 
images = []
labels = []
labels_df = pd.read_csv(directory + &quot;labels.csv&quot;)
n_images = 0

for image in tqdm(labels_df.iloc, desc = &quot;loading images&quot;, unit = &quot;images&quot;, total = 4000):
images.append(np.asarray(cv2.resize(cv2.imread(directory + image[2] + &#39;/&#39; + image[1], cv2.IMREAD_COLOR), img_size[0:2])[:, :, ::-1]))
labels.append(image[2])

images, labels = np.array(images).reshape(4000, num_px), np.array(labels)

print(f&#39;labels shape: {labels.shape}&#39;)
print(f&#39;images shape: {images.shape}&#39;)
print(f&#39;images size: {img_size}&#39;)

def plot_embedding(Z, show_axis=&quot;False&quot;):
plt.figure(figsize=(10, 8))
map = {label: i for i, label in枚举（np.unique（labels））}
color = np.array（[map[l] for l in labels])
plt.scatter（Z[:, 0], Z[:, 1], c = color, cmap = &quot;jet&quot;)
plt.colorbar()
plt.title（&#39;2d t-SNE 可视化&#39;）
if not show_axis:
plt.axis（&quot;off&quot;)
plt.axis（&quot;equal&quot;)
plt.show()

标签形状：（4000,）
图像形状：（4000, 110592）
图像大小：（192, 192, 3）]]></description>
      <guid>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</guid>
      <pubDate>Wed, 24 Apr 2024 17:53:14 GMT</pubDate>
    </item>
    <item>
      <title>所有中间步骤都应该是转换器，并实现拟合和转换</title>
      <link>https://stackoverflow.com/questions/48758383/all-intermediate-steps-should-be-transformers-and-implement-fit-and-transform</link>
      <description><![CDATA[我正在使用重要特征选择实现一个管道，然后使用相同的特征来训练我的随机森林分类器。以下是我的代码。
m = ExtraTreesClassifier(n_estimators = 10)
m.fit(train_cv_x,train_cv_y)
sel = SelectFromModel(m, prefit=True)
X_new = sel.transform(train_cv_x)
clf = RandomForestClassifier(5000)

model = Pipeline([(&#39;m&#39;, m),(&#39;sel&#39;, sel),(&#39;X_new&#39;, X_new),(&#39;clf&#39;, clf),])
params = {&#39;clf__max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;]}

gs = GridSearchCV(model, params)
gs.fit(train_cv_x,train_cv_y)

所以X_new 是通过 SelectFromModel 和 sel.transform 选择的新特征。然后我想使用所选的新特征训练我的 RF。
我收到以下错误：
所有中间步骤都应为转换器并实现拟合和转换，
ExtraTreesClassifier ...
]]></description>
      <guid>https://stackoverflow.com/questions/48758383/all-intermediate-steps-should-be-transformers-and-implement-fit-and-transform</guid>
      <pubDate>Tue, 13 Feb 2018 01:59:49 GMT</pubDate>
    </item>
    </channel>
</rss>