<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 06 Jul 2024 18:18:31 GMT</lastBuildDate>
    <item>
      <title>从 UE5 中的动作分类器模型获取输入</title>
      <link>https://stackoverflow.com/questions/78714836/taking-input-from-an-action-classifier-model-in-ue5</link>
      <description><![CDATA[我已经训练了一个动作分类器模型，该模型可以实时使用网络摄像头对玩家的动作进行分类。
它使用媒体管道检测人的姿势，并向模型提供 x、y、深度和可见性参数。该模型对人正在执行的动作进行分类。现在我想将此模型的结果输入为游戏中的按钮按下事件。此输入将用于控制我的游戏角色。例如：如果模型检测到拳击动作，则游戏角色将出拳。
解决此问题的最佳方法是什么，以便我在模型决策和游戏动作之间获得尽可能低的延迟？
我研究了虚幻引擎 5 的 NNI 插件，但它似乎无法解决我的问题。我想到使用的一种方法是运行一个 python 脚本，该脚本在检测到动作时按下相关按钮。它将与游戏并行运行。还有其他更好的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78714836/taking-input-from-an-action-classifier-model-in-ue5</guid>
      <pubDate>Sat, 06 Jul 2024 12:29:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 StandardScaler 转换 x_test 数据时出现错误</title>
      <link>https://stackoverflow.com/questions/78714642/getting-error-while-transforming-the-x-test-data-using-standardscaler</link>
      <description><![CDATA[import numpy as np
import pandas as pd
df = pd.read_csv(&#39;C:/Users/sayed/Downloads/placement.csv&#39;)
df = df.iloc[:, 1:]

X = df.iloc[:, 0:2]
Y = df.iloc[:,-1]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

执行 x_test = scaler.transform(x_test) 后，我得到了这个警告
UserWarning：X 没有有效的功能名称，但 StandardScaler 配备了功能名称 warnings.warn(
我确实向 ChatGPT 询问了这个问题，它给出了以下答案
您收到的警告是由于缩放器的安装方式与使用方式不匹配造成的
我仍然不清楚错误所在。]]></description>
      <guid>https://stackoverflow.com/questions/78714642/getting-error-while-transforming-the-x-test-data-using-standardscaler</guid>
      <pubDate>Sat, 06 Jul 2024 11:01:20 GMT</pubDate>
    </item>
    <item>
      <title>在自定义环境中，Actor-Critic 模型中的奖励没有增加</title>
      <link>https://stackoverflow.com/questions/78714452/rewards-not-increasing-in-actor-critic-model-in-a-custom-environment</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78714452/rewards-not-increasing-in-actor-critic-model-in-a-custom-environment</guid>
      <pubDate>Sat, 06 Jul 2024 09:38:19 GMT</pubDate>
    </item>
    <item>
      <title>此函数能找到 pytorch 张量中最后一个元素的内存地址吗？</title>
      <link>https://stackoverflow.com/questions/78713769/does-this-function-find-the-memory-address-of-the-last-element-in-a-pytorch-tens</link>
      <description><![CDATA[我想知道我编写的这个函数是否能找到 pytorch 张量中最后一个元素的内存地址。我运行它，它看起来不错，但有几次它返回了张量的“最后一个元素”，并且它显示它位于内存中比第一个元素更靠后的部分，我使用 .data_ptr() 函数获得第一个元素。张量是连续的，尺寸为 (3,1000,1000)。
我将张量作为参数，然后获取张量的最后一个索引，然后调用 data_ptr() 函数。以下是我的代码：
def get_last_mem(tensor):
last_element = tensor[-1,-1].data_ptr()
return last_element

这是我的输出
变换前的第一个元素内存地址：136496996679680，连续：True
变换前的最后一个元素内存地址：136497008675680]]></description>
      <guid>https://stackoverflow.com/questions/78713769/does-this-function-find-the-memory-address-of-the-last-element-in-a-pytorch-tens</guid>
      <pubDate>Sat, 06 Jul 2024 03:07:18 GMT</pubDate>
    </item>
    <item>
      <title>我加载一个 float32 Hugging Face 模型，将其转换为 float16，然后保存。我该如何将其加载为 float16？</title>
      <link>https://stackoverflow.com/questions/78713551/i-load-a-float32-hugging-face-model-cast-it-to-float16-and-save-it-how-can-i</link>
      <description><![CDATA[我加载一个 huggingface-transformers float32 模型，将其转换为 float16，然后保存。我如何将其加载为 float16？
示例：
# pip install tr​​ansformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# 加载模型
model_path = &#39;huawei-noah/TinyBERT_General_4L_312D&#39;
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 将模型转换为 FP16
model.half()

# 检查模型 dtype
def print_model_layer_dtype(model):
print(&#39;\nModel dtypes:&#39;)
for name, param in model.named_pa​​rameters():
print(f&quot;参数：{name}，数据类型：{param.dtype}&quot;)

print_model_layer_dtype(model)
save_directory = &#39;temp_model_SE&#39;
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print(&#39;\n\n##################&#39;)
print(model2)
print_model_layer_dtype(model2)

在此示例中，model2 加载为 float32 模型（如 print_model_layer_dtype(model2) 所示），即使 model2 已保存为 float16（如 config.json 中所示）。将其加载为 float16 的正确方法是什么？
在 Windows 10 上使用 transformers==4.36.2 和 Python 3.11.7 进行了测试。]]></description>
      <guid>https://stackoverflow.com/questions/78713551/i-load-a-float32-hugging-face-model-cast-it-to-float16-and-save-it-how-can-i</guid>
      <pubDate>Fri, 05 Jul 2024 23:58:06 GMT</pubDate>
    </item>
    <item>
      <title>AlexNet 的顶层和底层如何通信？</title>
      <link>https://stackoverflow.com/questions/78713337/how-do-the-top-and-bottom-layers-of-alexnet-communicate</link>
      <description><![CDATA[我正在尝试使用 PyTorch 重新实现 Krizhevsky et al. (2012)，并且我对 AlexNet 模型的第二和第三卷积层如何精确通信感到困惑（第五层到第六层以及第六层到第七层的输入也是如此，尽管我在这里的问题中省略了这一点）。
在下图中，有两个&quot;过滤器&quot;，它们将输出从上半部分传递到下一个上半部分，但也传递到下半部分。同样，下半部分也有两个&quot;过滤器&quot;将输出传递到下一个下半部分和上半部分。
我没有足够的声誉点来嵌入图像，所以这里是Krizhevsky et al. (2012) 的图 1 的部分屏幕截图。
第二层的输出如何传递到第三层？
我读了这篇论文，除非我错过了什么，否则作者似乎没有准确概述输出是如何从第二层传递到第三层的。我浏览了大量博客文章和 git 存储库，大多数描述都是高级的，大多数实现似乎没有将模型拆分到两个 GPU 之间。
我能找到的最相关的内容是来自 convnet2 readme 的以下句子：

这里，层 conv2a 和 conv2b 将 conv1a 和 conv1b 都作为输入。执行隐式复制操作，以便将 conv1a 的输出放入 conv2b 的输入中，以及将 conv1b 的输出放入 conv2a 的输入中。

我最好的猜测是第二层中的 out_channels 参数实际上应该是 64 而不是 128，然后顶层和底层的输出应该连接为 torch.cat([output_from_top_half, output_from_bottom_half], dim=1) 并传递给第三层的上半部分和下半部分。但我不确定我的理解是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/78713337/how-do-the-top-and-bottom-layers-of-alexnet-communicate</guid>
      <pubDate>Fri, 05 Jul 2024 21:44:17 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型中的损失函数没有减少？</title>
      <link>https://stackoverflow.com/questions/78712668/loss-function-not-decreasing-on-a-cnn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78712668/loss-function-not-decreasing-on-a-cnn-model</guid>
      <pubDate>Fri, 05 Jul 2024 17:20:29 GMT</pubDate>
    </item>
    <item>
      <title>寻找包含带有摘要分析注释的电子表格的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78712190/looking-for-dataset-containing-spreadsheets-with-summary-analytical-comments</link>
      <description><![CDATA[我正在开展一个机器学习项目，该项目需要一种独特类型的数据集。具体来说，我需要包含分析注释或描述的 CSV 或 XLSX 格式的电子表格。
我正在寻找这样的数据集：每个电子表格都有清晰的行和列标题，并在文件底部或注释部分包含注释，以像人类一样描述数据。这些注释应该解释数据、提供见解或描述数据所代表的内容，而不仅仅是元数据或文档。
例如，请参阅附件示例电子表格。此表显示了学生的每周时间表，下面有分析注释，根据下面注释中的数据描述了潜在问题和建议。
数据集可以来自任何领域，而不仅仅是学术领域。示例包括：

商业公司：公司的季度资产负债表，附有描述业绩和未来计划的评论。
政府报告：龙卷风发生统计数据，附有突出重要年份和因素的评论。
个人信息：财务数据，附有财富分配和促成因素的分析。

有谁知道这类电子表格的集合或我可以找到这类数据集的来源？或者，任何关于如何有效创建或注释这些数据集的建议都将不胜感激。
我也在考虑购买这类数据集。例如，Statista 提供带注释的数据，但我在付费之前不确定数据量。像这样的平台或其他可以让我与销售代表交谈的来源会很有帮助。
我正在考虑的另一种方法是下载长期上市公司的季度财务报告，并添加来自《华尔街日报》等稳定记者的评论。但是，找到预先存在的数据集会更有益。
我已经使用“带分析的电子表格”和“带注释的电子表格”等关键字在 Google 搜索、Statista、Kaggle、Google 数据集搜索和美国人口普查局等来源上搜索过，但仍然需要进一步的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78712190/looking-for-dataset-containing-spreadsheets-with-summary-analytical-comments</guid>
      <pubDate>Fri, 05 Jul 2024 15:16:23 GMT</pubDate>
    </item>
    <item>
      <title>对失败的测试进行分类，并提出解决方案 [关闭]</title>
      <link>https://stackoverflow.com/questions/78711965/classification-of-failed-tests-and-fix-them-with-solution-proposal</link>
      <description><![CDATA[我现在在一家运行测试台的组织工作，这些测试是用 C 和 C++ 开发的。
当执行结束时，我们通过了测试，并且测试失败了，每个失败的测试都会生成一个日志文件。
现在我们想使用 IA 对这些失败的测试进行分类（因为有些测试有相同的错误）。
我们想预测这些失败测试的解决方案。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78711965/classification-of-failed-tests-and-fix-them-with-solution-proposal</guid>
      <pubDate>Fri, 05 Jul 2024 14:21:48 GMT</pubDate>
    </item>
    <item>
      <title>Darknet Yolov4-tiny（灰度输入）到 Tensorflow 权重，转换</title>
      <link>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</link>
      <description><![CDATA[TL;DR:
1 通道 TF 模型的行为与 3 通道模型不同。两者都成功从 Darknet -&gt; TF 转换，但 1 通道模型的表现不如转换前。
手头的任务和声明：
我有两个经过训练的 yolov4-tiny darknet 权重文件 (.weights)，一个有灰度输入（1 通道），另一个有颜色输入（3 通道）。我正在将两个权重文件转换为 Tensorflow 检查点格式，使用一个通用存储库（用于此任务），该存储库位于：
https://github.com/hunglc007/tensorflow-yolov4-tflite.git
两种模型的性能都已通过 c++ opencv LoadFromDarknet() 和 Python 等效项进行了测试。这两个模型本质上都是用灰度图像进行训练的，并且对灰度图像进行操作。3 通道模型的输入只是缩放到 3 通道的灰度图像。
Python 版本：3.10.11
TF 版本：2.10.1
问题陈述：
使用 tf.keras.Models.load_model(X) 加载时，带有颜色输入的权重文件转换良好，之后运行良好，但是当转换灰度输入权重文件时，使用 Tensorflow 加载时模型的性能急剧下降，我的意思是在最明显的情况下，带有颜色输入的模型运行完美，检测效果很差或不存在。值得注意的是，框不会错位，这意味着当发现检测结果时，它们大约在正确的位置，但例如宽度和高度可能会偏离。
我知道这个存储库的常见问题（硬编码内容等），并相应地更改了每次转换/模型加载的参数，并且在转换或模型加载期间不会发生任何错误。
我已经确认了输入层：

灰度：（无，640,640,1）
颜色：（无，640,640,3）

测试图像（用于性能测试）使用 opencv-python 加载，并且它们的有效性也已审查，即使将错误维度的数据插入到输入层也会出现错误。
除输入层之外的架构相同，已使用 model.summary() 确认。
我注意到，几年前我用不同的 TF 版本转换的 3 通道模型由 model.summary() 生成的架构有些不同。一些图块层似乎缺失了。此外，一些 tf 操作的名称也不同，但这可能只是 TF 版本不同。
旧颜色模型：
 tf_op_layer_Sigmoid (TensorFlo (None, 40, 40, 3, 2 0 [&#39;tf_op_layer_split_3[0][0]&#39;]
wOpLayer) )

tf_op_layer_Tile/multiples (Te (5,) 0 [&#39;tf_op_layer_strided_slice[0][0]
nsorFlowOpLayer) &#39;]

tf_op_layer_Sigmoid_3 (TensorF (None, 20, 20, 3, 2 0 [&#39;tf_op_layer_split_4[0][0]&#39;]
lowOpLayer) ) )

新灰度模型：
 tf.math.sigmoid (TFOpLambda) (无，40，40，3，2 0 [&#39;tf.split_3[0][0]&#39;]
)

---此处缺少图块层---

tf.math.sigmoid_3 (TFOpLambda) (无，20，20，3，2 0 [&#39;tf.split_4[0][0]&#39;]
)

我现在很卡。有什么帮助吗？
一些反复试验：

使用 Yolov4-tiny Head 解码块 -&gt;即使在模型能够加载的情况下也没有变化（解码时错误的尺寸会引发错误）
之前提到的旧 3 通道模型（几年前已转换为 Darknet -&gt; TF），以与新模型相同的方式加载时可以完美运行
]]></description>
      <guid>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</guid>
      <pubDate>Fri, 05 Jul 2024 13:54:37 GMT</pubDate>
    </item>
    <item>
      <title>关于将深度学习存储库称为上游或下游的澄清</title>
      <link>https://stackoverflow.com/questions/78711787/clarification-on-referring-to-deep-learning-repositories-as-upstream-or-downstre</link>
      <description><![CDATA[我正在努力理解深度学习模型背景下不同存储库之间的关系。具体来说，我对要使用的适当术语感到好奇。
目前，存在许多深度学习模型存储库，其中上传了用于下游任务的模型，例如 Hugging Face 或 TensorFlow Hub。我相信还有一些存储库存储了用于训练这些模型的代码，通常在 GitHub 等平台上。
我的问题是：
我们可以将深度学习模型存储库（例如 Hugging Face）称为下游存储库，将代码存储库（例如 GitHub）称为上游存储库吗？
我想确保在讨论这些关系时使用正确的术语。任何有关此主题的澄清或资源都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78711787/clarification-on-referring-to-deep-learning-repositories-as-upstream-or-downstre</guid>
      <pubDate>Fri, 05 Jul 2024 13:40:14 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML-Agents --num-envs 获取环境 ID</title>
      <link>https://stackoverflow.com/questions/78598596/unity-ml-agents-num-envs-get-env-id</link>
      <description><![CDATA[我想训练一款需要用户登录的游戏。目前，我对登录名进行了硬编码以使用训练帐户。每个玩家只能登录一次，因此使用 --num-envs=x 参数进行训练仍将导致只有一个环境实际进行训练。有没有办法访问当前环境的 ID，以便我可以为每个单独的环境使用不同的登录名？我希望能够说
playerName = $&quot;player{envId};

有没有办法做到这一点？
我研究了 Academy 和 Communicator 实现，但没有发现任何有用的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78598596/unity-ml-agents-num-envs-get-env-id</guid>
      <pubDate>Sun, 09 Jun 2024 12:31:24 GMT</pubDate>
    </item>
    <item>
      <title>在具有标准化变量的模型中缩放样本外预测：恢复到原始比例</title>
      <link>https://stackoverflow.com/questions/78576306/scaling-out-of-sample-forecasts-in-a-model-with-normalized-variables-reverting</link>
      <description><![CDATA[我正在使用一个模型进行预测，其中变量按 $ x_i = \frac{{x_i - \text{mean}(x_i)}}{{\text{sd}(x_i)}} $ 缩放，并且我保存了平均值和标准差。现在，对于样本外预测，假设目标变量 $ ( x_i )$，基于缩放模型，我该如何缩小预测值？
我是否应该使用样本内 $ \text{Mean}(x_i) $ 和 $ \text{sd}(x_i) $ 来缩小样本外预测值，以便：
$ \text{重新缩放的样本外预测} = \text{缩放的预测} \times \text{sd}(x_i) + \text{mean}(x_i) $
这里的适当程序是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78576306/scaling-out-of-sample-forecasts-in-a-model-with-normalized-variables-reverting</guid>
      <pubDate>Tue, 04 Jun 2024 15:17:14 GMT</pubDate>
    </item>
    <item>
      <title>SHAP - 具有多个维度的实例</title>
      <link>https://stackoverflow.com/questions/76083485/shap-instances-that-have-more-than-one-dimension</link>
      <description><![CDATA[我对 SHAP 还很陌生，我想尝试一下，但遇到了一些困难。
该模型已经过训练，似乎表现良好。然后我使用训练数据来测试 SHAP。它看起来像这样：
 var_Braeburn var_Cripps Pink var_Dazzle var_Fuji var_Granny Smith \
0 1 0 0 0 0 
1 0 1 0 0 0 
2 0 1 0 0 0 
3 0 1 0 0 0 
4 0 1 0 0 0 

var_Other Variety var_Royal Gala (Tenroy) root_CG202 root_M793 \
0 0 0 0 0 
1 0 0 1 0 
2 0 0 1 0 
3 0 0 0 0 
4 0 0 0 0 

root_MM106 ... frt_BioRich Organic Compost_single \
0 1 ... 0 
1 0 ... 0
2 0 ... 0 
3 1 ... 0 
4 1 ... 0 

frt_Biomin Boron_single frt_Biomin Zinc_single \
0 0 1 
1 0 0 
2 0 0 
3 0 0 
4 0 0 

frt_Fertco Brimstone90 sulphur_single frt_Fertco Guano _single \
0 0 0 
1 0 0 
2 0 0 
3 0 0 
4 0 0 

frt_Gro Mn_multiple frt_Gro Mn_single frt_Organic Mag Super_multiple \
0 0 0 0 
1 1 0 1 
2 1 0 1 
3 1 0 1 
4 1 0 1

frt_Organic Mag Super_single frt_Other Fertiliser 
0 0 0 
1 0 0 
2 0 0 
3 0 0 
4 0 0 

然后我执行 explainer = shap.Explainer(model) 和 shap_values = explainer(X_train)
此操作运行无错误，shap_values 给出以下结果：
.values =
array([[[ 0.00775555, -0.00775555],
[-0.03221035, 0.03221035],
[-0.0027203 , 0.0027203 ],
...,
[ 0.00259787, -0.00259787],
[-0.00459262, 0.00459262],
[-0.0303394 , 0.0303394 ]],

[[-0.00068313, 0.00068313],
[-0.03006355, 0.03006355],
[-0.00245706, 0.00245706],
...,
[-0.00418809, 0.00418809],
[-0.00088372, 0.00088372],
[-0.00030019, 0.00030019]],

[[-0.00068313, 0.00068313],
[-0.03006355, 0.03006355],
[-0.00245706, 0.00245706],
...,
[-0.00418809, 0.00418809],
[-0.00088372, 0.00088372],
[-0.00030019, 0.00030019]],

...,

但是，当我运行 shap.plots.beeswarm(shap_values) 时，出现以下错误：
ValueError: beeswarm plot 不支持绘制具有多个维度的实例的解释！
我在这里做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/76083485/shap-instances-that-have-more-than-one-dimension</guid>
      <pubDate>Sun, 23 Apr 2023 06:49:07 GMT</pubDate>
    </item>
    <item>
      <title>YOLO v8 的默认网格大小</title>
      <link>https://stackoverflow.com/questions/75904407/default-grid-size-for-yolo-v8</link>
      <description><![CDATA[YOLO 使用一个网格，其中分配了检测到的对象的中心。在最初的论文中，网格是 7x7
Yolo v8 中的网格大小是多少？
我问这个问题的原因是因为无锚检测，因为它不再根据锚框的偏移量进行计算，而是使用中心。]]></description>
      <guid>https://stackoverflow.com/questions/75904407/default-grid-size-for-yolo-v8</guid>
      <pubDate>Sat, 01 Apr 2023 05:19:52 GMT</pubDate>
    </item>
    </channel>
</rss>