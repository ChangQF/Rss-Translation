<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 17 Dec 2024 01:22:18 GMT</lastBuildDate>
    <item>
      <title>我怎样才能提高决策树的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</link>
      <description><![CDATA[这是我的代码
# 准备目标和特征
target_column = &#39;resolution&#39;
X = data.drop(columns=[target_column])
y = data[target_column]

# 根据需要将分类数据转换为二进制/数字
X_encoded = pd.get_dummies(X) # 对分类特征进行独热编码
le = LabelEncoder()
y_encoded = le.fit_transform(y) # 编码目标变量

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)

# 执行网格搜索以调整 max_depth、min_samples_split、min_samples_leaf 和 class_weight
param_grid = {
&#39;max_depth&#39;: range(1, 11), # 要测试的 max_depth 范围
&#39;min_samples_split&#39;: [2, 5, 10], # min_samples_split 的选项
&#39;min_samples_leaf&#39;: [1, 2, 4],
&#39;class_weight&#39;: [&#39;balanced&#39;]# min_samples_leaf 的选项
}
model = DecisionTreeClassifier(random_state=42)

# 使用 5 倍交叉验证初始化 GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2,scoring=&#39;accuracy&#39;) # 使用准确度进行分类
grid.fit(X_train, y_train)

# 来自网格搜索的最佳参数
best_params = grid.best_params_
# print(&quot;来自网格搜索的最佳参数：&quot;, best_params)

# 评估测试集上的最佳模型
best_model = grid.best_estimator_
test_accuracy = best_model.score(X_test, y_test) * 100 # 准确率百分比
# print(f&quot;最佳模型的测试准确率：{test_accuracy:.2f}%&quot;)

# 显示决策树的结构
# tree_structure = tree.export_text(best_model, feature_names=list(X_encoded.columns))
# print(&quot;\n决策树结构：&quot;)
# print(tree_structure).

这是我的结果
分类报告：
准确率 召回率 f1 分数 支持率

结果 1 0.39 0.88 0.54 388
结果 2 0.95 0.61 0.75 1397

准确率 0.67 1785
宏平均值 0.67 0.75 0.64 1785
加权平均值 0.83 0.67 0.70 1785


混淆矩阵：
[[343 45]
[540 857]]

“结果 1”类的准确率：88.40%
“结果2&#39;：61.35%
决策树分类器的准确率：0.67
我希望整体准确率和“结果 2”更高。我认为问题在于结果 1 在这里是少数，出于某种原因，决策树的准确率因此降低，尽管它应该是平衡的。]]></description>
      <guid>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</guid>
      <pubDate>Mon, 16 Dec 2024 20:55:17 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能修复破损的轮廓？[关闭]</title>
      <link>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</link>
      <description><![CDATA[我正在研究图像分割，但轮廓不太正确。这是我经过一些图像清理和轮廓检测后得到的结果，但正如您所见，轮廓在某些地方被破坏了。有办法解决这个问题吗？我偶然发现了这篇论文，DiSTNet2D，其中您应该训练神经网络进行分割。我不太确定这是否有点过头了。有人有更好的建议吗？我已附上我的代码和结果
def gaussian_filter_multiscale_retinex(image: np.ndarray, sigmas: list, weights: list) -&gt; np.ndarray:
img32 = image.astype(&#39;float32&#39;) / 255
img32_log = np.log1p(img32)

msr = np.zeros(image.shape, np.float32)
for sigma, weight in zip(sigmas, weights):
blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
blur_log = np.log1p(blur)
msr += (img32_log - blur_log) * weight

msr /= sum(weights)
return cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)

def process_image(img_path):
img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)
rtnx = gaussian_filter_multiscale_retinex(img, sigmas=[15, 55, 185], weights=[10, 5, 1])
阈值 = cv.adaptiveThreshold(rtnx, 255, adaptableMethod=cv.ADAPTIVE_THRESH_GAUSSIAN_C,
阈值类型=cv.THRESH_BINARY, blockSize=7, C=-7)

nb_components, output, stats, _ = cv.connectedComponentsWithStats(thresholded, connections=8)
大小 = stats[1:, -1]
new_img = np.zeros_like(thresholded)
new_img[np.isin(output, np.where(sizes &gt;= 12)[0] + 1)] = 255

numLabels、labels、stats、centroids = cv.connectedComponentsWithStats(new_img)


]]></description>
      <guid>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</guid>
      <pubDate>Mon, 16 Dec 2024 17:08:30 GMT</pubDate>
    </item>
    <item>
      <title>二维的 VC 维 [关闭]</title>
      <link>https://stackoverflow.com/questions/79284939/vc-dimension-of-2-dimensional</link>
      <description><![CDATA[问题是：
给出二维（轴平行）矩形的 VC 维数的上限，其中我们可以选择内部或外部是 +。
我得到了 4 个解决方案，但寻求鼓励的朋友说答案应该是 5]]></description>
      <guid>https://stackoverflow.com/questions/79284939/vc-dimension-of-2-dimensional</guid>
      <pubDate>Mon, 16 Dec 2024 14:09:59 GMT</pubDate>
    </item>
    <item>
      <title>针对群体数据/种族数据的最自然类别的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79284779/most-natural-class-of-machine-learning-models-for-group-data-race-data</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 小时数 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 5 4 93 5 50
2 5 5 103 9 88
2 5 8 112 12 99
2 5 1 200 10 100
2 5 2 90 19 78
3 2 5 100 12 84
3 2 7 102 13 88

我想建立一个机器学习模型，尝试预测谁将成为班级第一名（即最高分数）给定 Class_ID，使用 IQ 和 Hours（学习小时数）作为特征。
换句话说，输入是班级中每个学生（例如 1 到 n）的 IQ 和 Hours，输出是概率向量 (p_1, ..., p_n)，其中每个 p_i 是学生 i 在班级中获得最高分数的概率。
这是我尝试过的：

由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。不幸的是，输出是 相关性得分 列表，而不是概率列表，概率列表在概率方面没有自然解释。

解决这个问题的一种方法是在 xgboost 中对相关性得分应用 softmax，但没有直接有意义的概率解释，如基于能量的模型，如 RBM 的能量函数。事实上，我试过这样做，概率变得非常极端（大多数概率质量集中在每个班级的一名学生身上，导致测试结果不佳且方差较大）

另一类学习模型是分类模型，如逻辑回归/决策树。然而，我遇到的问题是每个班级的学生人数不同，因此要训练这样的模型，我们必须首先“扁平化”特征矩阵：

Class_ID Class_size IQ_1 IQ_2 IQ_3 IQ_4 IQ_5 小时_1 小时_2 小时_3 小时_4 小时_5 Score_1 Score_2 Score_3 Score_4 Score_5
1 1 101 99 130 南 南 10 19 3 南 南 98 80 95 南 南
2 5 93 103 112 200 90 5 9 12 10 19 50 88 99 100 78
3 2 100 102 南 南 南 12 13 南 南 南 84 88 南 南 南

使得 1 行代表 1 个训练示例。但随后特征矩阵变得非常稀疏（因为不同班级的学生人数可能有很大差异）
因此，逻辑模型/普通前馈神经网络/基于树的模型似乎也不适用于这类群体数据。
所以我的问题是，是否有任何自然的机器学习模型可以处理这些“群体数据”，就像我上面的数据集一样？
此外，在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不重要）。]]></description>
      <guid>https://stackoverflow.com/questions/79284779/most-natural-class-of-machine-learning-models-for-group-data-race-data</guid>
      <pubDate>Mon, 16 Dec 2024 13:03:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 aiplatform.BatchPredictionJob.create() 在 Vertex AI 中配置模型监控？</title>
      <link>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</link>
      <description><![CDATA[我在使用 aiplatform SDK 设置 Vertex AI 模型监控时遇到了问题，特别是在配置 BatchPredictionJob.create() 时。文档不清楚，缺少定义 model_monitoring_objective_config 和 model_monitoring_alert_config 的示例。由于引用不完整和参数映射不清楚，这导致了兼容性问题。
主要混淆是因为这些配置在 SDK 中没有很好的记录，需要探索 SDK 源代码才能了解正确的结构。此外，SDK 需要来自 aiplatform.model_monitoring 的配置，这在官方指南中没有明确说明。
我尝试过的方法：
我最初参考了 SDK 文档，并根据看似合乎逻辑的内容尝试了各种配置，假设所有组件都可以直接使用 SDK 的类进行配置。但是，这会导致多个类型和参数不匹配错误。
我的预期：
我预期清晰明了的文档，展示如何在使用 aiplatform.BatchPredictionJob.create() 创建批量预测作业时定义和传递监控配置。
实际发生的情况：
我遇到了由于 SDK 和 GAPIC API 之间的参数不匹配而导致的错误。在探索源代码后，我意识到必须使用 aiplatform.model_monitoring 类定义所需的配置。这种跨库依赖关系没有记录。]]></description>
      <guid>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</guid>
      <pubDate>Mon, 16 Dec 2024 07:45:27 GMT</pubDate>
    </item>
    <item>
      <title>利用贝叶斯优化进行多类分类</title>
      <link>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</link>
      <description><![CDATA[无法让这部分代码运行，而且速度真的很慢。
我只想创建一个模型，将叶子图像分为 4 种类型（无，然后是 3 种疾病类型）
我想使用 F1 作为损失函数，然后使用贝叶斯优化来获取叶子类模型的最佳参数，但它没有运行。或者它运行得非常慢然后失败了...
我在 CPU 上运行，因为我没有 GPU
# Optuna 的目标函数
def objective(trial, train_dataloader, val_dataloader, device):
# 使用 Optuna 的建议函数定义超参数搜索空间
conv1_filters = trial.suggest_categorical(&quot;conv1_filters&quot;, [16, 32, 64])
conv2_filters = trial.suggest_categorical(&quot;conv2_filters&quot;, [64, 128, 256])
kernel_size = trial.suggest_categorical(&quot;kernel_size&quot;, [3, 5, 7])
hidden_​​units = trial.suggest_categorical(&quot;hidden_​​units&quot;, [256, 512, 1024])
dropout_rate = trial.suggest_float(&quot;dropout_rate&quot;, 0.1, 0.5)
learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 1e-5, 1e-2, log=True)
num_epochs = trial.suggest_int(&quot;num_epochs&quot;, 10, 50)

# 使用给定的参数初始化模型
model = LeafCNN(
conv1_filters=conv1_filters,
conv2_filters=conv2_filters,
kernel_size=kernel_size,
hidden_​​units=hidden_​​units,
dropout_rate=dropout_rate
).to(device)

# 设置优化器和损失函数
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)

# 提前停止参数
patient = 10 # 允许 10 个 epoch 不进行改进
delta = 0.001 # 算作改进的最小变化
best_val_loss = float(&quot;inf&quot;)
patience_counter = 0 # 从 0 开始计数器

# 训练循环
for epoch in range(num_epochs):
model.train()
for X_batch, y_batch in train_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
optimizer.zero_grad()
outputs = model(X_batch)
loss = loss_fn(outputs, y_batch)
loss.backward()
optimizer.step()

# 验证损失
model.eval()
val_loss = 0.0
使用 torch.no_grad():
对于 val_dataloader 中的 X_batch、y_batch：
X_batch、y_batch = X_batch.to(device)、y_batch.to(device)
输出 = 模型 (X_batch)
损失 = loss_fn(outputs、y_batch)
val_loss += loss.item()

val_loss /= len(val_dataloader)
打印 (f&quot;Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}&quot;)

如果 val_loss &lt; best_val_loss - delta:
best_val_loss = val_loss
waiting_counter = 0
else:
waiting_counter += 1

if waiting_counter &gt;= waiting:
print(f&quot;在 epoch {epoch + 1} 触发提前停止&quot;)
break

# 使用 F1 分数进行评估
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
for X_batch, y_batch in val_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
output = model(X_batch)
_, predict = torch.max(outputs, 1)
all_preds.extend(predicted.cpu().numpy())
all_labels.extend(y_batch.cpu().numpy())

f1 = f1_score(all_labels, all_preds, average=&quot;weighted&quot;)
return f1 # 我们的目标是最大化 F1 分数

# 运行 Optuna 优化的主要函数
def optuna_search(train_dataloader, val_dataloader, device, num_trials):
# 创建 Optuna 研究
study = optuna.create_study(direction=&quot;maximize&quot;) # 最大化 F1 分数
study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device), n_trials=num_trials)

# 打印最佳超参数
print(&quot;找到最佳超参数：&quot;, study.best_params)
print(&quot;最佳 F1 分数：&quot;, study.best_value)

return study.best_params, study.best_value

# 主程序
if __name__ == &quot;__main__&quot;:

# 创建 DataLoaders
train_dataset = TensorDataset(X_train_split, y_train_split)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

val_dataset = TensorDataset(X_val, y_val)
val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# 运行 Optuna 优化
best_params, best_f1 = optuna_search(train_dataloader, val_dataloader, device, num_trials=10)
print(&quot;最佳参数：&quot;, best_params)
print(&quot;最佳 F1 分数：&quot;, best_f1)
]]></description>
      <guid>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</guid>
      <pubDate>Sun, 15 Dec 2024 23:07:57 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 停留在图像生成上</title>
      <link>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</link>
      <description><![CDATA[我创建了一个 LSTM 来生成序列中的下一张图像（我知道 CNN 是用于图像生成的，但我需要整个图像，而不仅仅是提供给序列下一次迭代的过滤器）。所以我有一个数据集，它包含图像（电影中的帧），我创建了它的序列，就像 1 个场景包含例如。 n 个图像，我有 s 个序列长度，那么输入将是 image_1 到 image_s，输出是 image_s+1，下一个输入是 image_2 到 image_s+1，输出是 image_s+2，依此类推。
模型如下：
class LSTM(nn.Module):
def __init__(self, input_len, hidden_​​size, num_layers):
super(LSTM, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_len, hidden_​​size, num_layers, batch_first=True)
self.output_layer = nn.Linear(hidden_​​size, input_len)
self.dropout = nn.Dropout(.2)

def forward(self, X):
hidden_​​states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
out, _ = self.lstm(X, (hidden_​​states, cell_states))
out = self.dropout(out)
out = self.output_layer(out[:, -1, :])
return out

训练是：
def train(num_epochs, model, loss_func, optimizer):
total_steps = loader.getSizeWithBatch()

for epoch in range(num_epochs):
loader.reset()
for item in range(total_steps-1):
element = loader.next()[0]
x_images,y_image = element
x_images = x_images.reshape(-1,sequence_len,input_len)
output = model(x_images)
y_image = y_image.reshape(-1,input_len)
loss = loss_func(output, y_image)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if (item + 1) % 1 == 0:
print(f&#39;Epoch: {epoch + 1};批次：{item + 1} / {total_steps};损失：{loss.item():&gt;4f}&#39;)

if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;model_save_interval&#39;]) == 0:
if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;clean_save_interval&#39;]) == 0:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;] + str(epoch+1)))
else:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;]))

Loader 以张量的形式引导图像由于内存使用，从文件中预先排序。
我使用 MSE 损失和 Adam 作为优化器。
问题是，当我训练它时，错误达到 0.003，这是目标，因为我通过将它们除以 255 来规范化值，但当我预测它时，它会产生一种模糊的场景图像，并且无论输入如何，预测图像始终相同，即使输入来自其他场景，它也会创建相同的图像，当我减去不同输出图像的颜色值时，该值为 0，因此每个输出图像都完全相同
我尝试添加 Droput，增加隐藏大小的神经元（现在是 128），尝试增加层数，不同的时期会创建相同的图像，只是模糊程度略低，但是一样的，我将学习率从 .001 降低到 .0001，一切都一样]]></description>
      <guid>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</guid>
      <pubDate>Sun, 15 Dec 2024 20:28:51 GMT</pubDate>
    </item>
    <item>
      <title>针对 Lllama 3.2 1B 的分层学生-教师优化</title>
      <link>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</guid>
      <pubDate>Sun, 15 Dec 2024 20:21:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能有效地执行此循环而不增加内存使用量？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>自定义环境：接住沿抛物线轨迹飞行的球</title>
      <link>https://stackoverflow.com/questions/79282335/custom-env-catching-a-ball-flying-by-a-parabolic-trajectory</link>
      <description><![CDATA[我正在练习使用 Farama Gymnasium 和 PyGame 实现自己的环境，
源代码可在此处获取
我实现了课程中的物理和策略梯度方法：https://huggingface.co/learn/deep-rl-course/unit4/hands-on
相同的代码能够解决推车杆问题。
但是我正在努力制作一个代理来移动球拍来接球。
环境：

一个球以一定的初始速度从左向右抛出
在游戏场地的右侧有一个可以上下移动的球拍
当球击中场地的另一侧时会分配奖励。如果击中球拍，则 +10，如果未击中，则 -10

我使用的是具有一个隐藏层的网络。
训练后，平台要么上升，要么下降并停留在那里。
我的想法是让它学习重力和抛物线的概念，或者至少尝试跟随球的 Y 坐标，
但这并没有发生。
你对如何调整我的参数或奖励函数有什么提示吗？
任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/79282335/custom-env-catching-a-ball-flying-by-a-parabolic-trajectory</guid>
      <pubDate>Sun, 15 Dec 2024 12:40:38 GMT</pubDate>
    </item>
    <item>
      <title>纠正抽样偏差（寻找数据科学或统计方法）？</title>
      <link>https://stackoverflow.com/questions/79282031/correcting-sampling-bias-looking-for-data-science-or-statistical-approach</link>
      <description><![CDATA[假设我正在查看一组学生及其在 5 个科目中取得的成绩。90% 的样本偏向于表现最差的学生，10% 的样本是随机的。
我可以使用什么简单的方法来纠正样本中的偏差，以近似估计无偏差的成绩总体？]]></description>
      <guid>https://stackoverflow.com/questions/79282031/correcting-sampling-bias-looking-for-data-science-or-statistical-approach</guid>
      <pubDate>Sun, 15 Dec 2024 08:39:26 GMT</pubDate>
    </item>
    <item>
      <title>获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</link>
      <description><![CDATA[我正在做一个机器学习项目，在 Jupyter Notebook 上预测电动汽车的价格。
我运行这些单元：
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
x[col] = le.transform(x[col]) 
print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5，random_state = 0)

r2_score(y_test，lm.predict(x_test))

从 sklearn.tree 导入 DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train，y_train)
r2_score(y_test，regressor.predict(x_test))

r2_score(y_train，regressor.predict(x_train))

uv = np.nanpercentile(df2[&#39;Base MSRP&#39;]，[99])[0]*2

df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv

df2 = df2[df2[&#39;Model Year&#39;] != &#39;N/&#39;] # 过滤掉包含 &#39;Model Year&#39; 的行&#39;N/&#39;

for col in cols:
df2[col] = df2[col].replace(&#39;N/&#39;, -1)
le.fit(df2[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

le = preprocessing.LabelEncoder()

cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]

for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError回溯（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 返回 _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
以下是数据集的链接：
https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:19 GMT</pubDate>
    </item>
    <item>
      <title>为 GPR 创建自定义内核</title>
      <link>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</link>
      <description><![CDATA[我想编写一个仅在 X 轴特定范围内工作的 RBF 内核。我尝试编写一个包含 RBF 核的类来测试代码
class RangeLimitedRBFTest(Kernel):
def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), x_min = 0., x_max = 1.):
self.length_scale = length_scale
self.length_scale_bounds = length_scale_bounds
self.rbf_kernel = RBF(length_scale, length_scale_bounds)
self.x_min = x_min
self.x_max = x_max

def __call__(self, X, Y=None, eval_gradient=False):
if eval_gradient and Y is not None:
raise ValueError(&quot;Gradient can only be evaluating when Y is None.&quot;)

X = np.atleast_2d(X)
if Y is not None:
Y = np.atleast_2d(Y)

print(f&quot;X 形状：{X.shape}&quot;)
如果 Y 不为 None:
print(f&quot;Y 形状：{Y.shape}&quot;)
else:
print(&quot;Y 形状：None&quot;)

K_rbf = self.rbf_kernel(X, Y, eval_gradient=eval_gradient)

如果 eval_gradient:
K, K_grad = K_rbf
print(f&quot;核矩阵形状 (K): {K.shape}&quot;)
print(f&quot;核梯度矩阵形状 (K_grad): {K_grad.shape}&quot;)
return K, K_grad
else:
K = K_rbf
return K

def diag(self, X):
return self.rbf_kernel.diag(X)

def is_stationary(self):
return self.rbf_kernel.is_stationary()

实现和拟合如下
kernel = 1.0 * RangeLimitedRBFTest(length_scale=0.1, length_scale_bounds=(8e-2, 8e-1), x_min=0., x_max=2.5) + WhiteKernel(noise_level=0.5, noise_level_bounds=(1e-2, 1e1))
gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1, alpha=1e-5, optimizer=&#39;fmin_l_bfgs_b&#39;)
gaussian_process.optimizer_kwargs = {&quot;max_iter&quot;: 10000} 
gaussian_process.fit(X, T_PMT)

如果我运行代码，我会得到以下输出
X 形状：(6248, 1)
Y 形状：无
核矩阵形状 (K)：(6248, 6248)
核梯度矩阵形状 (K_grad)：(6248, 6248, 1)
ValueError：第 0 维必须固定为 2，但得到 3

上述异常是导致以下异常的直接原因：

回溯（最近一次调用）：
文件 &quot;/home/tdaq/cremonini/pt100_probe/read_temperatures.py&quot;，第 97 行，位于 &lt;module&gt;
gaussian_process.fit(X, T_PMT)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/base.py&quot;，第 1389 行，在包装器中
return fit_method(estimator, *args, **kwargs)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 308 行，在 fit 中
self._constrained_optimization(
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 653 行，在 _constrained_optimization 中
opt_res = scipy.optimize.minimize(
文件&quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_minimize.py&quot;，第 713 行，在 minimal
res = _minimize_lbfgsb(fun, x0, args, jac, bounds,
文件 &quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py&quot;，第 360 行，在_minimize_lbfgsb
_lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
ValueError: 无法将 _lbfgsb.setulb 的第 7 个参数“g”转换为 C/Fortran 数组

如果我尝试使用通常的 RBF 内核，代码可以正常工作。我还尝试禁用优化器 optimizer=None，代码可以正常工作，但会出现非常大的错误。]]></description>
      <guid>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</guid>
      <pubDate>Wed, 11 Dec 2024 11:00:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'_IncompatibleKeys' 对象不可调用</title>
      <link>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</link>
      <description><![CDATA[我正在训练 CNN 以解决多标签分类问题，并使用 torch.save(model.state_dict(), &quot;model.pt&quot;) 保存了我的 .pt 模型。出于某种原因，当我使用以图像数组为输入的自定义函数 predict(x) 测试模型时，我收到以下错误：TypeError: &#39;_IncompatibleKeys&#39; object is not callable。它指出了下面代码的最后一部分：y_test_pred = model(images_tensors)。您知道这里的问题可能是什么吗？ 
导入 numpy 作为 np
导入 cv2
导入 torch
从 torch 导入 nn
导入 torch.nn. functional 作为 F
导入 os

类 Net(nn.Module):
def __init__(self, classes_number):
super().__init__()
self.ConvLayer1 = nn.Sequential(
nn.Conv2d(1, 8, 5), # inp (1, 512, 512)
nn.MaxPool2d(2),
nn.ReLU() # op (8, 254, 254)
)
self.ConvLayer2 = nn.Sequential(
nn.Conv2d(8, 16, 3), # inp (8, 254, 254)
nn.MaxPool2d(2),
nn.ReLU(),
            nn.BatchNorm2d(16) # 操作 (16, 126, 126)
        ）
        self.ConvLayer3 = nn.Sequential(
            nn.Conv2d(16, 32, 3), # inp (16, 126, 126)
            nn.MaxPool2d(2),
            ReLU(),
            nn.BatchNorm2d(32) # 操作 (32, 62, 62)
        ）
        self.ConvLayer4 = nn.Sequential(
            nn.Conv2d(32, 64, 3), # inp (32, 62, 62)
            nn.MaxPool2d(2),
            nn.ReLU() # 运算 (64, 30, 30)
        ）
        self.Lin1 = nn.Linear(30 * 30 * 64, 1500)
self.drop = nn.Dropout(0.5)
self.Lin2 = nn.Linear(1500, 150)
self.drop = nn.Dropout(0.3)
self.Lin3 = nn.Linear(150, classes_number)

def forward(self, x):
x = self.ConvLayer1(x)
x = self.ConvLayer2(x)
x = self.ConvLayer3(x)
x = self.ConvLayer4(x)
x = x.view(x.size(0), -1)
x = F.relu(self.Lin1(x))
x = self.drop(x)
x = F.relu(self.Lin2(x))
x = self.drop(x)
x = self.Lin3(x)
out = torch.sigmoid(x)
return out

def predict(x):
# 在考试中，x 将是指向我们保留集图像的所有路径的列表
images = []
for img_path in x:
img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 转换为灰度
img = cv2.resize(img, (512, 512))
images.append(img)
images = np.array(images)
images = images.reshape(len(images), 1, images.shape[1], images.shape[1]) # converting(n,512,512)&gt;(n,1,512,512)
images_tensors = torch.FloatTensor(np.array(images))
images_tensors = images_tensors.to(device)
classes = [&quot;red blood细胞”、“困难”、“配子体”、“滋养体”、“环”、“裂殖体”、“白细胞”]
model = Net(len(classes))
model = model.load_state_dict(torch.load(&#39;model.pt&#39;))

y_test_pred = model(images_tensors)
y_test_pred[y_test_pred &gt; 0.49] = 1
y_test_pred[y_test_pred &lt; 0.5] = 0

return y_test_pred.cpu().detach()
]]></description>
      <guid>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</guid>
      <pubDate>Tue, 26 Nov 2019 00:16:13 GMT</pubDate>
    </item>
    </channel>
</rss>