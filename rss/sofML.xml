<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 07 Jun 2024 06:21:53 GMT</lastBuildDate>
    <item>
      <title>AUROC = (Sum(TP)+Sum(TN)) / P+N 是否正确？[关闭]</title>
      <link>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</link>
      <description><![CDATA[我正在阅读一篇论文“一种用于洪水敏感性评估的新型混合人工智能方法”。该论文提到 ROC 曲线下面积 (AUROC) 的方程是：

这个方程正确吗？我认为这是一种计算 AUC 的非常简单的方法。任何想法或想法，我都想在我的报告中使用该方程。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</guid>
      <pubDate>Fri, 07 Jun 2024 02:14:29 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 flax 0.6.1 加载 flax 0.5.3 中保存的检查点</title>
      <link>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</link>
      <description><![CDATA[我使用 flax 0.5.3 保存了程序中的检查点。当我尝试将其加载到使用 flax 0.6.1 的程序中时，我收到一条错误消息：“列表的大小和状态字典不匹配”。唯一版本发生变化的 Python 包是 flax 及其依赖项。以下是堆栈跟踪。是否可以在 flax 0.6.1 或更高版本中加载使用 flax 0.5.3 保存的检查点？有没有办法将检查点迁移到较新版本的 flax？
 app.run(main)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 308 行，在 run
_run_main(main, args)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 254 行，在 _run_main
sys.exit(main(argv))
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 739 行，在 main
run_alphageometry(
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 652 行，在 run_alphageometry
bqsearch_init()
文件&quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 529 行，在 bqsearch_init 中
model = get_lm(_CKPT_PATH.value, _VOCAB_PATH.value)
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 213 行，在 get_lm 中
return lm.LanguageModelInference(vocab_path, ckpt_init, mode=&#39;beam_search&#39;)
文件 &quot;/home/peng/ag4masses/alphageometry/lm_inference.py&quot;，第 62 行，在 __init__ 中
(tstate, _, imodel, prngs) = trainer.initialize_model()
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 394 行，在 initialize_model 中
tstate = self.restore_checkpoint(tstate)
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 439 行，位于 restore_checkpoint
loaded_train_state = checkpoints.restore_checkpoint(load_dir, train_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/training/checkpoints.py&quot;，第 752 行，位于 restore_checkpoint
return serialization.from_state_dict(target, state_dict)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，位于 from_state_dict
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第149，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件&quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 156 行，在 &lt;lambda&gt; 中
lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)))
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;, 第 110 行, _restore_list
引发 ValueError(&#39;列表的大小和状态字典不匹配,&#39;
ValueError: 列表的大小和状态字典不匹配, 得到 6 和 1。

尝试将 flac 从 0.5.3 升级到 0.6.1，并使用 flax 0.5.3 加载代码保存的检查点。但出现上述错误。]]></description>
      <guid>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</guid>
      <pubDate>Fri, 07 Jun 2024 02:06:11 GMT</pubDate>
    </item>
    <item>
      <title>pycharm tolist 用户问题</title>
      <link>https://stackoverflow.com/questions/78589617/issue-with-pycharm-tolist-for-user</link>
      <description><![CDATA[第一次使用 pycharm 编写代码来制作聊天分析器，但显示其他用户的聊天 wt 选择没有显示
这是代码
在此处输入图片说明
检查 blackbox ai 和 chatgpt 没有帮助在此处输入图片说明
当我尝试查看显示分析时，只有总体显示，而其他用户未显示]]></description>
      <guid>https://stackoverflow.com/questions/78589617/issue-with-pycharm-tolist-for-user</guid>
      <pubDate>Fri, 07 Jun 2024 01:28:25 GMT</pubDate>
    </item>
    <item>
      <title>可教机器使用哪个版本的 tensorFlow/Keras？</title>
      <link>https://stackoverflow.com/questions/78589351/what-version-of-tensorflow-keras-does-teachable-machine-use</link>
      <description><![CDATA[我尝试按照 YouTube 上的教程操作（https://www.youtube.com/watch?v=wa2ARoUUdU8&amp;t=2877s&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI）。当我尝试运行 tennsorflow 时，一直出现错误。视频中，该人使用 https://teachablemachine.withgoogle.com/train/tiny_image 来训练模型。我完成了所有步骤，但是在运行时（视频的 49:32。它没有运行）错误与使用支持组参数的 TensorFlow / Keras 版本训练的模型有关，但我正在使用的当前 TensorFlow / Keras 版本无法识别它。提醒一下，我使用的是 vs 而不是 py charms 和 dowloand tensflow，我的版本是 2.16.1
这是我的代码：
import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import tensorflow as tf
import math
import time
import tensorflow as tf
print(tf.__version__)

cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)#决定要检测多少只手
classifier = Classifier(&quot;Model/keras_model.h5&quot;,&quot;Model/labels.txt&quot;)

offset = 20 #用于裁剪图像以增加尺寸
imgSize = 300

folder = “数据/C” #数据存储位置
counter = 0#知道将保存多少图像

labels = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]

while True:
success, img = cap.read()
hands, img = detector.findHands(img)
#裁剪图像
if hands:#还要注意，当手很大/离相机太近时，程序将停止工作
hand = hands[0]#仅适用于 1 只手
x,y,w,h = hand[&#39;bbox&#39;]#粘合框并给出尺寸

imgWhite = np.ones((imgSize,imgSize,3),np.uint8)*255#创建白色背景用于裁剪图像
imgCrop = img[y-offset:y+h+offset,x-offset:x+w+offset]#给出边界框要求

imgCropShape = imgCrop.shape

aspectRatio = h/w

ifaspectRatio&gt;1:#图像高度
k = imgSize/h #拉伸高度
wCal=math.ceil(k*w)
imgResize=cv2.resize(imgCrop,(wCal,imgSize))
imgResizeShape = imgResize.shape
wGap = math.ceil((imgSize-wCal)/2)
#在白色图像上叠加图像
imgWhite[:,wGap:wCal+wGap] = imgResize #中心图像
#发送值
#predection,index = classifier.getPrediction(img)
#print(predection,index)
else:#图像宽度
k = imgSize/w #拉伸高度
hCal=math.ceil(k*h)
imgResize=cv2.resize(imgCrop,(imgSize,hCal))
imgResizeShape = imgResize.shape
hGap = math.ceil((imgSize-hCal)/2)
#在白色图像上叠加图像
imgWhite[hGap:hCal+hGap,:] = imgResize #居中图像

cv2.imshow(&quot;ImageCrop&quot;, imgCrop)#再裁剪一次
cv2.imshow(&quot;ImageWhite&quot;, imgWhite)

cv2.imshow(&quot;Image&quot;, img)
key =cv2.waitKey(1)#1 毫秒延迟

]]></description>
      <guid>https://stackoverflow.com/questions/78589351/what-version-of-tensorflow-keras-does-teachable-machine-use</guid>
      <pubDate>Thu, 06 Jun 2024 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>训练误差持续下降，但测试误差却没有下降，即使测试数据集是训练数据集的子集</title>
      <link>https://stackoverflow.com/questions/78589173/train-error-decreases-consistently-but-test-error-does-not-even-when-test-data</link>
      <description><![CDATA[我的数据包含来自传感器的 6 个特征。我正在用这些数据训练 LSTM 网络来预测三个值。
在训练过程中，我的训练损失随着每个时期而持续减少，但测试损失在几个时期后并没有减少多少。
当训练数据和测试数据之间没有重叠时就会出现这种情况。因此，我尝试使用训练数据的子集作为测试数据。
但是，仍然是相同的行为，测试损失仍然没有减少。
以下是 LSTM 模型和训练器的代码。
class LSTMModel(nn.Module):
def __init__(self, in_dim=6, hidden_​​size=200, num_layers=1, output_size=3):
super(LSTMModel, self).__init__()
self.lstm_1 = nn.LSTM(in_dim, hidden_​​size, num_layers, batch_first=True)
self.lstm_2 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_3 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_4 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
x, _ = self.lstm_1(x)
x, _ = self.lstm_2(x)
x, _ = self.lstm_3(x)
x, _ = self.lstm_4(x)
output = self.fc(x[:, -1, :])
return output

class SimpleModelTrainer:
def __init__(self, model, train_dataset, test_dataset, batch_size=1024, epochs=100, lr=0.005): # window_size=200, do_windowing=True, waiting=5, pad_testing_data = False

self.model = model
self.optimizer = AdamW(params=self.model.parameters(), lr=lr)

self.lr = lr
self.epochs = epochs
self.batch_size = batch_size
self.loss_fn = nn.L1Loss()
self.train_data = train_dataset
self.test_data = test_dataset

def train(self):
self.train_dataloader = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device))
self.test_dataloader = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device))
total_samples = 0

for epoch in tqdm(range(self.epochs), desc=&quot;epoch&quot;):
self.model.train()
total_loss = 0

for train_data in tqdm(self.train_dataloader, desc=&quot;train&quot;):

X = train_data[0]
Y = train_data[1]

if X.shape[0] != self.batch_size: continue # 以避免 RuntimeError: 形状 &#39;[16, 1, 256]&#39; 对于大小为 3328 的输入无效

total_samples += self.batch_size

y_hat = self.model(X)

loss = self.loss_fn(y_hat, Y)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
total_loss += loss.item()

avg_train_loss = total_loss / total_samples
val_loss = self.test(self.test_dataloader)
print(f&quot;Epoch {epoch} - Train loss:{avg_train_loss:.10f}, Val loss:{val_loss:.10f}&quot;)

def test(self, dataloader):
self.model.eval()
with torch.no_grad():
total_loss = 0
total_samples = 0
for test_data in tqdm(dataloader, desc=&quot;test&quot;):
X = test_data[0]
Y = test_data[1]

if X.shape[0] != self.batch_size: continue # 以避免 RuntimeError: shape &#39;[Y, 200, 6]&#39; 对于大小为 Z 的输入无效

total_samples += self.batch_size 

y_hat = self.model(X)

loss = self.loss_fn(y_hat, Y)
total_loss += loss.item()

val_loss = total_loss/total_samples
return val_loss

我尝试使用随机生成的虚拟变量数据集。它给出了与上述完全相同的行为！
您可以在此 colab 笔记本中查看它。
正如您在笔记本中看到的那样，自第一个时期以来，验证损失一直停留在 0.00048。但训练损失随着每个时期持续下降，从 0.00048 下降到第 28 个时期的 0.000016。
（当我写这个问题时，它仍在训练。）测试数据集是训练数据集的子集：
train_dataset = CustomDataset(windowed_input_data, windowed_target_data)
test_dataset = CustomDataset(windowed_input_data[:20000], windowed_target_data[:20000])

因此，我相信我应该得到类似的验证损失行为，验证损失也应该达到约 0.00001。我想我在代码中犯了一些愚蠢的错误（错误的 pytorch API 调用？）我的眼睛根本无法帮助我。有人可以帮帮我吗？我在概念上错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78589173/train-error-decreases-consistently-but-test-error-does-not-even-when-test-data</guid>
      <pubDate>Thu, 06 Jun 2024 21:42:12 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 目录结构与文档不同</title>
      <link>https://stackoverflow.com/questions/78588051/tensorflow-directory-structure-different-from-documentation</link>
      <description><![CDATA[文档中的内容
来自我的 .venv 的 TensorFlow 目录
我试图访问最新版本的 TensorFlow 中的某些函数，如 tf.strings 等。网站上的文档说要以某种方式执行此操作，但 python 无法识别它们。我想要使用的函数只是在“api_packages.txt”文件中列出，但没有办法真正调用它们。我该怎么做才能访问这些模块？
尝试从使用不同版本的在线资源中学习时也会非常令人困惑，因为很多目录已经迁移到不同的位置。
模块无法识别的示例
我尝试引用所示的模块，但无法识别。]]></description>
      <guid>https://stackoverflow.com/questions/78588051/tensorflow-directory-structure-different-from-documentation</guid>
      <pubDate>Thu, 06 Jun 2024 16:44:35 GMT</pubDate>
    </item>
    <item>
      <title>预先训练的模型将评论分类到类别中[关闭]</title>
      <link>https://stackoverflow.com/questions/78587853/pre-trained-model-to-classify-comment-into-category</link>
      <description><![CDATA[是否有任何预先训练好的模型可以将评论分类为投诉、赞赏/表扬、询问等？
如果没有，是否可以使用 ChatGPT 为每个类别创建合成数据并训练分类器来完成此任务？
欢迎提出所有建议！！]]></description>
      <guid>https://stackoverflow.com/questions/78587853/pre-trained-model-to-classify-comment-into-category</guid>
      <pubDate>Thu, 06 Jun 2024 16:04:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在多元回归中显示预测因子重要性+特征名称？</title>
      <link>https://stackoverflow.com/questions/78587419/how-can-i-display-predictor-importance-feature-name-in-multivariate-regression</link>
      <description><![CDATA[我正在探索一个数据集，目的是找到任何有趣的关系（有很多感兴趣的变量，我想看看哪些特征或特征组合可以预测它们）。
作为第一种方法，我成功地用套索计算了一个多变量（几个目标变量）回归。
pipeline = Pipeline([
(&#39;scaler&#39;, StandardScaler()),
(&#39;model&#39;, Lasso())])

search = GridSearchCV(pipeline,
{&#39;model__alpha&#39;:np.arange(0.1,10,0.1)},
cv = 5,scoring=&quot;neg_mean_squared_error&quot;,verbose=3
)
search.fit(X_train,y_train)
search.best_params_
coefficients = search.best_estimator_.named_steps[&#39;model&#39;].coef_
importance = np.abs(系数)

现在我想看看预测因子的重要性，包括它们的特征名称，因为 importance 只是一堆数字。
我考虑过创建一个包含特征和目标的列名的数组并打印名称 + 系数，但我的问题是我不完全确定如何确保对应关系（正确的名称与正确的系数一起显示）。
有人能帮我吗？
这里有一些额外的信息：

预测因子数量：26
目标数量：30
importance 的形状：（30, 26）

我也非常感谢关于使用哪些重要性指标的任何其他建议或有关可能分析的任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/78587419/how-can-i-display-predictor-importance-feature-name-in-multivariate-regression</guid>
      <pubDate>Thu, 06 Jun 2024 14:49:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 paddleocr 从图像文档中提取表格</title>
      <link>https://stackoverflow.com/questions/78586625/extract-tables-from-image-documents-using-paddleocr</link>
      <description><![CDATA[我是 Ocr 的新手。我正尝试从图像格式的表格中提取数据。

为此我想使用 paddleocr，paddleocr 是否适合这些类型的表格图像。如果可以，有人可以提供图像的资源或教程吗？或者是否有任何适用于表格图像的软件包]]></description>
      <guid>https://stackoverflow.com/questions/78586625/extract-tables-from-image-documents-using-paddleocr</guid>
      <pubDate>Thu, 06 Jun 2024 12:37:33 GMT</pubDate>
    </item>
    <item>
      <title>我可以实施 Google Lens API 来从给定图像中识别汽车的品牌和型号吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78586504/can-i-implement-google-lens-api-to-identify-brand-and-model-of-a-car-from-a-give</link>
      <description><![CDATA[我正在尝试寻找一个可以从给定的汽车图像中识别汽车品牌或名称的 API。

我尝试过 Google Cloud Vision API，但它只是将汽车检测为汽车，而不是它的具体信息。
我也尝试过 SERP API，但它对我的用例也没有用。

我正在寻找：
我可以获得一些可以集成来获取图像中汽车品牌的 Google API 吗？]]></description>
      <guid>https://stackoverflow.com/questions/78586504/can-i-implement-google-lens-api-to-identify-brand-and-model-of-a-car-from-a-give</guid>
      <pubDate>Thu, 06 Jun 2024 12:17:42 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义数据集训练 ViTPose (Vision Transformer) 时损失为零</title>
      <link>https://stackoverflow.com/questions/78586455/loss-is-zero-while-training-vitpose-vision-transformer-with-custom-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78586455/loss-is-zero-while-training-vitpose-vision-transformer-with-custom-dataset</guid>
      <pubDate>Thu, 06 Jun 2024 12:08:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 logistf 包时，Firth 的模型在 R 中卡住了（出现未收敛警告，CPU 使用率高达 99%）</title>
      <link>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</guid>
      <pubDate>Wed, 05 Jun 2024 07:34:40 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 管道可用模型</title>
      <link>https://stackoverflow.com/questions/78328539/huggingface-pipeline-available-models</link>
      <description><![CDATA[我正在使用 Python 中的 Huggingface 来对特定的 LLM 文本生成模型进行推理。到目前为止，我使用这样的管道来初始化模型，然后插入用户的输入并检索响应：
import torch
from transformers import pipeline
print(torch.cuda.is_available())

generator = pipeline(&#39;text-generation&#39;, model=&#39;gpt2&#39;, device=&quot;cuda&quot;)
#推理代码

但是，当我用 google/gemma-2b-it 或其他一些模型更改 gpt2 时，它可能会要求进行身份验证，或者直接出现错误，表明它无法从 pipeline() 获得。
我知道有些模型需要特定的标记器和依赖项，但是，有没有办法从 pipeline() 列出所有可用的模型？有什么方法可以在 pipeline() 中使用其他模型及其所有依赖项，而无需在脚本中导入或使用它们？]]></description>
      <guid>https://stackoverflow.com/questions/78328539/huggingface-pipeline-available-models</guid>
      <pubDate>Mon, 15 Apr 2024 12:54:43 GMT</pubDate>
    </item>
    <item>
      <title>从熊猫数据框中删除高度相关的列</title>
      <link>https://stackoverflow.com/questions/44889508/remove-highly-correlated-columns-from-a-pandas-dataframe</link>
      <description><![CDATA[我有一个名为 data 的数据框，我使用其计算了相关矩阵
corr = data.corr()

如果两列之间的相关性大于 0.75，我想从数据框 data 中删除其中一列。我尝试了一些选项
raw =corr[(corr.abs()&gt;0.75) &amp; (corr.abs() &lt; 1.0)]

但这没有帮助；我需要 raw 中值非零的列号。基本上是以下 R 命令的一些 Python 等效命令（使用函数 findCorrelation）。
{hc=findCorrelation(corr,cutoff = 0.75)

hc = sort(hc)

data &lt;- data[,-c(hc)]}

如果有人能帮助我在 Python pandas 中获取类似于上述 R 命令的命令，那将会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/44889508/remove-highly-correlated-columns-from-a-pandas-dataframe</guid>
      <pubDate>Mon, 03 Jul 2017 15:39:25 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 L1/L2 正则化</title>
      <link>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</link>
      <description><![CDATA[如何在 PyTorch 中添加 L1/L2 正则化而无需手动计算？]]></description>
      <guid>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</guid>
      <pubDate>Thu, 09 Mar 2017 19:54:19 GMT</pubDate>
    </item>
    </channel>
</rss>