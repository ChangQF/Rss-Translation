<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 28 Jul 2024 06:20:10 GMT</lastBuildDate>
    <item>
      <title>计算机视觉 yolo [关闭]</title>
      <link>https://stackoverflow.com/questions/78802407/computer-vision-yolo</link>
      <description><![CDATA[我们如何构建一个在移动设备上运行且消耗很少资源的对象识别模型？
我使用了 Moodle，但准确率不高
我尝试了 yolo 算法，但速度很慢
我也想在 Raspberry Pi 上运行这个模型，我希望它也能识别远处的物体，而不仅仅是附近的物体]]></description>
      <guid>https://stackoverflow.com/questions/78802407/computer-vision-yolo</guid>
      <pubDate>Sat, 27 Jul 2024 20:34:43 GMT</pubDate>
    </item>
    <item>
      <title>列表索引超出范围OpenCV Python</title>
      <link>https://stackoverflow.com/questions/78802174/list-index-out-of-range-opencv-python</link>
      <description><![CDATA[我目前正在学习机器学习，正在做物体检测项目。
我的问题是 cv2.put_text() 在我的 for 循环中无法正常工作。
以下是相关代码：

classIds, confs, bbox = net.detect(img, confThreshold=0.5) 

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):
cv2.rectangle(img, box, (255, 0, 0), thicken=2)
cv2.putText(img, classNames[classId-1].upper(), (box[0]+10, box[1]+30),
cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2)

这是上述代码产生的错误消息：

文件 &quot;C:\Users\User\projects\opencv\ObjectDetector.py&quot;, 第 31 行, 
在 &lt;module&gt;
cv2.putText(img, classNames[classId-1].upper(), (box[0]+10, box[1]+30),
~~~~~~~~~~^^^^^^^^^^^
IndexError: 列表索引超出范围

这是我的完整代码：
&#39;
import cv2

# img = cv2.imread(&#39;bob.png&#39;)

cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

classNames = []
classFile = &#39;coco.names&#39;
with open(classFile, &#39;rt&#39;) as f:
classNames = f.read().rstrip(&#39;\n&#39;).split(&#39;\n&#39;)

configPath = &#39;ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt&#39;
weightsPath = &#39;frozen_inference_graph.pb&#39;

net = cv2.dnn.DetectionModel(weightsPath, configPath)
net.setInputSize(320, 320)
net.setInputScale(1.0 / 127.5)
net.setInputMean((127.5, 127.5, 127.5))
net.setInputSwapRB(True)

当 True 时： 
succss, img = cap.read()
classIds, confs, bbox = net.detect(img, confThreshold=0.5) 
print(classIds, bbox)

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), 
confs.flatten(), bbox):
cv2.rectangle(img, box, (255, 0, 0), thicken=2)
cv2.putText(img, classNames[classId-1].upper(),
(box[0]+10, box[1]+30),
cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2)

cv2.imshow(&#39;Output&#39;, img)
cv2.waitKey(1)

&#39;
我原本希望它显示一个带有摄像头的窗口，周围有一个框，其中指定了对象的名称，但它只显示了错误。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78802174/list-index-out-of-range-opencv-python</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:08 GMT</pubDate>
    </item>
    <item>
      <title>在技​​术机器学习文章中，是否应该将输入层形状作为模型架构的一部分进行报告？</title>
      <link>https://stackoverflow.com/questions/78802155/in-technical-ml-articles-should-one-report-input-layer-shape-as-part-of-their-m</link>
      <description><![CDATA[我是一名自然科学专业的学生，​​正在研究一个使用机器学习的项目。我正在研究一个由另一个研究小组创建的神经网络模型，该模型使用haiku实现。
在他们的原始文章中，当谈到他们的模型超参数时，他们在描述他们的MLP形状时没有包括输入层的大小，而只包括隐藏层和输出层的大小。例如，如果输入为 N=2000，后续层为 256, 64, 4（输出形状为 (4,)），那么他们甚至在论文中也将 MLP 形状报告为 (256,64,4)。
对我来说，这很令人困惑，因为从 2000 到 256 的解析必须具有与输入大小成比例的权重矩阵。后来，我意识到在 haiku 模块中，输入层会根据输入的大小进行调整，并且您通常不会在定义 MLP 本身时传递输入的大小。这与我在 ML 课程中使用 tensorflow/keras 的经历不同。我很惊讶，甚至测试了这一点——不同的输入形状输出不同的可训练参数。如果我理解错了，请纠正我：但是，如果输入 N=1000 与输入 N=2000，这难道不会产生完全不同的模型吗？
我在此向知情的机器学习专家提出问题：在报告层时，应该写“四层形状 (2000, 256, 64, 4)”还是“三层形状 (256, 64, 4)”？

P.S.如果有人好奇，我已经测试过，使用不同的输入大小实际上会导致 haiku 中的权重参数不同。
作为 MWE 示例
import haiku as hk
import jax.numpy as jnp
import numpy as np

import jax

class MyLinear1(hk.Module):

def __init__(self, output_size, name=None):
super().__init__(name=name)
self.output_size = output_size

def __call__(self, x):
j, k = x.shape[-1], self.output_size
w_init = hk.initializers.TruncatedNormal(1. / np.sqrt(j))
w = hk.get_parameter(&quot;w&quot;, shape=[j, k], dtype=x.dtype, init=w_init)
b = hk.get_parameter(&quot;b&quot;, shape=[k], dtype=x.dtype, init=jnp.ones)
return jnp.dot(x, w) + b

def _forward_fn_linear1(x):
module = MyLinear1(output_size=2)
return module(x)

forward_linear1 = hk.transform(_forward_fn_linear1)

rng_key = jax.random.PRNGKey(42)
dummy_x = jnp.ones([8])

params = forward_linear1.init(rng=rng_key, x=dummy_x)
print(params)

如果您使用 dummy_x = jnp.ones([8]) 它给出
 [-0.4785112 , -0.38034892],
[-0.41137823, -0.22265594],
[-0.43343404, 0.21691099],
[-0.18514387, -0.10827615],
[ 0.3682926 , -0.1418969 ],
[ 0.10915945, 0.4389233 ],
[-0.07725035, 0.08247987]], dtype=float32), &#39;b&#39;: Array([1., 1.], dtype=float32)}}

while dummy_x = jnp.ones[1]) 得出
{&#39;my_linear1&#39;: {&#39;w&#39;: Array([[ 1.51595 , -0.23353337]], dtype=float32), &#39;b&#39;: Array([1., 1.], dtype=float32)}}
]]></description>
      <guid>https://stackoverflow.com/questions/78802155/in-technical-ml-articles-should-one-report-input-layer-shape-as-part-of-their-m</guid>
      <pubDate>Sat, 27 Jul 2024 18:24:55 GMT</pubDate>
    </item>
    <item>
      <title>如何训练机器学习模型以使用词汇分析评估代码质量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78801874/how-to-train-a-machine-learning-model-to-evaluate-code-quality-using-lexical-ana</link>
      <description><![CDATA[我正在做一个项目，想开发一个机器学习模型，该模型可以根据词汇分析来评估代码解决方案的质量。我的目标是让模型能够确定给定的代码片段是编码问题的“好”解决方案还是“坏”解决方案。
还有其他提示或建议可以构建一个强大的机器学习模型来评估代码质量吗？]]></description>
      <guid>https://stackoverflow.com/questions/78801874/how-to-train-a-machine-learning-model-to-evaluate-code-quality-using-lexical-ana</guid>
      <pubDate>Sat, 27 Jul 2024 16:13:10 GMT</pubDate>
    </item>
    <item>
      <title>训练期间验证和使用检查点进行推理时，模型输出不同</title>
      <link>https://stackoverflow.com/questions/78801833/different-model-output-on-validation-during-training-versus-inference-with-check</link>
      <description><![CDATA[我使用 pytorch lightning 训练一个模型，并保存带有参数的检查点 (ckpt) 文件以供以后使用。除了一个新模型之外，它运行良好。
因此，我调试了推理，以找出为什么这个新模型在使用检查点的推理中表现非常不同。在继续之前，有几点注意事项：

模型之间的主要区别在于输入（其余的，即超参数，是等效的或非常接近的）
该模型具有与之前的模型相同的结构，运行良好（相同的层，相同的优化器（即 AdamW），例如权重衰减率的细微差异或奖励/损失函数已经改变 -&gt; 理论上不应该影响推理）
我使用 pytorch lightning (2.0) &amp; torch (2.3)
在 Cuda、Cpu 或 Mps 运行时上的行为相同（均用于训练和推理）
我使用验证数据集，在每个训练周期后计算性能。 无论模型或输入如何，我在验证数据集上都获得了良好的结果
验证是在时间数据上进行的，模型给出的输出与一定数量的过去数据成比例

我在进行推断时发现了什么：

第一个推断（即图像下方的第一行）等于我在训练阶段（在推断和验证的同一时期）在验证的第一个结果中获得的结果
总体而言，推断是好的，但有一个因素导致推断与验证数据集振荡和交替
我不知道这种振荡来自哪里
我检查了 ckpts 文件和权重，它们看起来都很正常
一个区别是在训练期间，验证指标是在完整的验证数据集上计算的，而推断则在完整的验证数据集上逐个计算指标相同 时间序列（两者的输出相同，但验证由网络中的一次传递完成，推理是循环的 -&gt; 一次传递）
最让我困扰的是，在验证阶段（训练期间），一切都正常，但在加载检查点并手动执行推理后就不行了。到目前为止，它适用于我之前的所有模型。

具有相同模型/相同时期的输出示例（左侧推理，右侧训练验证）。一图胜千言：

我期望左边和右边是相同的（即几乎相同的颜色），但推理预测中出现了一些噪音。如您所见，第一行在推理上没问题，但随后立即开始振荡。
例如，另一次运行验证与推理（偏差很小但可以容忍，但这里没有振荡）

什么可以解释相同网络架构上的这些振荡？
ps：由于许可，无法共享直接代码]]></description>
      <guid>https://stackoverflow.com/questions/78801833/different-model-output-on-validation-during-training-versus-inference-with-check</guid>
      <pubDate>Sat, 27 Jul 2024 15:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何预处理包含数字代码列表的功能？</title>
      <link>https://stackoverflow.com/questions/78801772/how-can-i-preprocess-a-feature-that-contains-a-list-of-number-codes</link>
      <description><![CDATA[我必须预处理一个特征，该特征基本上是编码为字符串的数字代码列表，并且我想对其进行编码，以便输出是每个数字的频率数组。还应通过输入缺失值来预处理特征。
这是我所做的：
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
s_data = pd.Series([&#39;123 342 789&#39;, &#39;12 34 56&#39;, np.nan, &#39;1 2 3 123&#39;)
s_data = str_data.str.split(&quot; &quot;)
pipeline = Pipeline([
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;))
(&#39;mlb&#39;, MultiLabelBinarizer())
])
encoded_data = pipeline.fit_transform(s_data)
encoded_df = pd.DataFrame(encoded_data, columns=mlb.classes_)

我期望的输出是这样的：
 1 12 123 2 3 34 342 56 789
0 0 0 1 0 0 0 1 0 1
1 0 1 0 0 0 1 0 1 0
2 1 0 1 1 1 0 0 0 0

但是 SimpleImputer 不会接受输入，说输入包含列表。当我尝试将输入更改为 numpy 数组格式时，MultiLabelBinariZer 拒绝了它，说它只需要 2 个输入，但给出了 3 个。]]></description>
      <guid>https://stackoverflow.com/questions/78801772/how-can-i-preprocess-a-feature-that-contains-a-list-of-number-codes</guid>
      <pubDate>Sat, 27 Jul 2024 15:30:18 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch/Unet 机器学习模型在本地运行和在 Azure 上运行时会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/78801760/pytorch-unet-machine-learning-model-gives-different-results-when-running-locally</link>
      <description><![CDATA[我有一个用 Python 编写的用于卫星图像分割的 ML 模型。我们使用 ONNX，以便它可以作为 C# 应用程序的一部分与 .NET 交互。然后，此应用程序通过 DevOps 部署到 Azure，作为 Azure 函数用作 Web API，它根据一组坐标对任何图像进行分割。
在我最近的测试中，我已确认运行相同的输入数据在本地运行和部署到 Azure 时会产生不同的结果。
我有一个不使用 ML 的旧式分割算法，它在两种环境中输出相同的结果，因此我绝对确定它与 ML 模型特别相关。
我没有编写 ML 模型，这也不是我的特定专业领域，所以我在这方面的知识有限，但我会将任何建议传达给我的开发人员。有人遇到过这样的情况吗？您对我们如何解决这个问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78801760/pytorch-unet-machine-learning-model-gives-different-results-when-running-locally</guid>
      <pubDate>Sat, 27 Jul 2024 15:21:00 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的线性回归模型返回负 R^2 分数</title>
      <link>https://stackoverflow.com/questions/78800849/linear-regression-model-in-scikit-learn-returning-negative-r2-score</link>
      <description><![CDATA[因此我尝试使用 scikit 学习线性回归来处理这个奥运数据框。
运动员部分有效，但年龄部分没有显示线性预测（我知道年龄与奖牌的相关性不好）
reg=LinearRegression()
predictors=[&quot;athletes&quot;, &quot;age&quot;]
output=&quot;medals&quot;
reg.fit(train[predictors],train[output])

我也希望第二个图是一条直线。我知道这里给出的图预测了两个输入参数的数据，但我想知道 scikit 如何区分这两个输入，即运动员和年龄]]></description>
      <guid>https://stackoverflow.com/questions/78800849/linear-regression-model-in-scikit-learn-returning-negative-r2-score</guid>
      <pubDate>Sat, 27 Jul 2024 07:59:21 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用任何类型的基础模型来检测通常发生在眼睛中的疾病？[关闭]</title>
      <link>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</link>
      <description><![CDATA[我想建立一个模型，可以检测视网膜图像是否有微动脉瘤。
可以使用基础模型吗？如果可以，它会带来什么好处？
如果我想，我可以使用哪种基础模型，比如我最近读到关于 RetFound 的文章，我认为它适合我的范围。
（这就像一个理论问题）如果这些模型甚至没有接受过执行特定任务的训练，那么它们究竟如何适用于我们的特定用例数据，比如制作基础模型的人如何决定训练数据的范围？它只是一个对视网膜图像进行处理模型，还是一个范围更广的模型，除了视网膜图像外，它还与其他相关数据有关（我相信）。第三个问题是可选的，如果您能回答，将不胜感激。
我尝试使用一个基础模型，https://github.com/facebookresearch/deit/blob/main/README_deit.md，但我在这里学得并不多。]]></description>
      <guid>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</guid>
      <pubDate>Fri, 26 Jul 2024 18:19:53 GMT</pubDate>
    </item>
    <item>
      <title>决策树分类器给出错误结果</title>
      <link>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</link>
      <description><![CDATA[我正在学习一门机器学习课程，其中的作业是实现 DecisionTreeClassifier 的拟合方法。
这是我的代码：
import numpy as np
import pandas as pd

class MyTreeClf:
def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
self.max_depth = max_depth
self.min_samples_split = min_samples_split
self.max_leafs = max_leafs
self.tree = None
self.leafs_cnt = 0

def node_entropy(self, probs):
return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

def node_ig(self, x_col, y, split_value):
left_mask = x_col &lt;= split_value
right_mask = x_col &gt; split_value

如果 len(x_col[left_mask]) == 0 或 len(x_col[right_mask]) == 0:
返回 0

left_probs = np.bincount(y[left_mask]) / len(y[left_mask])
right_probs = np.bincount(y[right_mask]) / len(y[right_mask])

entropy_after = len(y[left_mask]) / len(y) * self.node_entropy(left_probs) + len(y[right_mask]) / len(y) * self.node_entropy(right_probs)
entropy_before = self.node_entropy(np.bincount(y) / len(y))

返回 entropy_before - entropy_after

def get_best_split(self, X: pd.DataFrame，y：pd.Series）：
best_col，best_split_value，best_ig = None，None，-np.inf

对于 X.columns 中的 col：
sorted_unique_values = np.sort(X[col].unique())

对于 range(1，len(sorted_unique_values)) 中的 i：
split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

ig = self.node_ig(X[col]，y，split_value)

如果 ig &gt; best_ig:
best_ig = ig
best_col = col
best_split_value = split_value

返回 best_col、best_split_value、best_ig

def fit(self, X: pd.DataFrame, y: pd.Series,depth=0):
如果depth == 0:
self.tree = {}

best_col、best_split_value、best_ig = self.get_best_split(X, y)

如果depth &lt; self.max_depth 和 len(y) &gt;= self.min_samples_split 和 self.leafs_cnt &lt; self.max_leafs 和 best_col 不为 None:
left_mask = X[best_col] &lt;= best_split_value
right_mask = X[best_col] &gt; best_split_value

self.tree[depth] = {&#39;col&#39;: best_col, &#39;split&#39;: best_split_value, &#39;left&#39;: {}, &#39;right&#39;: {}}

self.fit(X[left_mask], y[left_mask],depth + 1)
self.fit(X[right_mask], y[right_mask],depth + 1)
else:
class_label = y.mode()[0]
self.tree[depth] = {&#39;class&#39;: class_label}
self.leafs_cnt += 1
df = pd.read_csv(&#39;c:\\Users\\Nijat\\Downloads\\banknote+authentication.zip&#39;, header=None)
df.columns = [&#39;variance&#39;, &#39;skewness&#39;, &#39;curtosis&#39;, &#39;entropy&#39;, &#39;target&#39;]
X, y = df.iloc[:,:4], df[&#39;target&#39;]

model = MyTreeClf()
model.fit(X, y)

print(model.leafs_cnt)

在 fit 方法中，使用 X 和 y 来构建树，应该计算叶子的数量。
树的构建如下：
根节点：从根节点开始，遍历每个属性。
阈值选择过程：
对于每个属性，选择唯一值并对其进行排序。
形成阈值列表以拆分值。
对于每个阈值，将数据集拆分为两个子集（左和右）。
评估每次拆分的信息增益。
选择具有最高信息增益的属性和阈值，并将它们保存在分层结构中。
递归拆分：
将数据集拆分为两个子集。
如果子集可以进一步分割，则递归重复该过程。
如果不能，则将子集声明为叶子并保存第一个类的概率。
约束：
当满足以下条件之一时停止分割：
最大树深度
节点中的最小实例数
叶子的最大数量

即使达到约束，也要通过创建必要的叶子来完成树。
叶子的数量保存在 leafs_cnt 变量中。该方法不返回任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</guid>
      <pubDate>Fri, 26 Jul 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：参数详细程度的值 -1 超出界限 [0,3]</title>
      <link>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</link>
      <description><![CDATA[错误消息如标题所示。根据下面的代码，这对我来说毫无意义：
clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class &#39;xgboost.sklearn.XGBClassifier&#39;&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter[&#39;y&#39;]) # 错误在这里出现

值显然是 1，但不知何故却变成了 -1？我不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</guid>
      <pubDate>Wed, 17 Jul 2024 22:12:23 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 没有为 xgboost 选择最佳超参数</title>
      <link>https://stackoverflow.com/questions/69429691/gridsearchcv-not-choosing-the-best-hyperparameters-for-xgboost</link>
      <description><![CDATA[我正在使用 xgboost 开发回归模型。由于 xgboost 有多个超参数，我使用 GridSearchCV() 添加了交叉验证逻辑。作为试验，我设置了 max_depth: [2,3]。我的python代码如下。
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
​
xgb_reg = xgb.XGBRegressor()
​
# 获取最佳超参数
scorer=make_scorer(mean_squared_error, False)
params = {&#39;max_depth&#39;: [2,3], 
&#39;eta&#39;: [0.1], 
&#39;colsample_bytree&#39;: [1.0],
&#39;colsample_bylevel&#39;: [0.3],
&#39;subsample&#39;: [0.9],
&#39;gamma&#39;: [0],
&#39;lambda&#39;: [1],
&#39;alpha&#39;:[0],
&#39;min_child_weight&#39;:[1]
}
grid_xgb_reg=GridSearchCV(xgb_reg,
param_grid=params,
scoring=scorer,
cv=5,
n_jobs=-1)
​
grid_xgb_reg.fit(X_train, y_train)
y_pred = grid_xgb_reg.predict(X_test)
y_train_pred = grid_xgb_reg.predict(X_train)

## 评估模型
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
​
print(&#39;RMSE train: %.3f, test: %.3f&#39; %(np.sqrt(mean_squared_error(y_train, y_train_pred)),np.sqrt(mean_squared_error(y_test, y_pred))))
print(&#39;R^2训练：%.3f，测试：%.3f&#39; %(r2_score(y_train, y_train_pred),r2_score(y_test, y_pred)))

问题是 GridSearchCV 似乎没有选择最佳超参数。在我的例子中，当我将 max_depth 设置为 [2,3] 时，结果如下。在以下情况下，GridSearchCV 选择 max_depth:2 作为最佳超参数。
# max_depth 为 2 时的结果
RMSE 训练：11.861，测试：15.113
R^2 训练：0.817，测试：0.601

但是，如果我将 max_depth 更新为 [3]（通过删除 2），则测试分数会比之前的值更好，如下所示。
# max_depth 为 3 时的结果
RMSE 训练：9.951，测试：14.752
R^2 训练：0.871，测试：0.620

问题
我的理解是，即使我设置max_depth 设置为 [2,3]，GridSearchCV 方法应该选择 max_depth:3 作为最佳超参数，因为 max_depth:3 可以比 max_depth:2 返回更好的 RSME 或 R^2 分数。有人能告诉我为什么当我将 max_depth 设置为 [2,3] 时，我的代码无法选择最佳超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/69429691/gridsearchcv-not-choosing-the-best-hyperparameters-for-xgboost</guid>
      <pubDate>Sun, 03 Oct 2021 23:45:55 GMT</pubDate>
    </item>
    <item>
      <title>Py4JJavaError：调用 z:org.apache.spark.api.python.PythonRDD.runJob 时发生错误。ModuleNotFoundError：没有名为“numpy”的模块</title>
      <link>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</guid>
      <pubDate>Tue, 03 Dec 2019 08:28:51 GMT</pubDate>
    </item>
    </channel>
</rss>