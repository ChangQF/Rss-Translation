<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 12 Oct 2024 12:30:11 GMT</lastBuildDate>
    <item>
      <title>如何对 NPU 进行编程以卸载合适的非 AI 相关操作？[关闭]</title>
      <link>https://stackoverflow.com/questions/79080861/how-to-program-a-npu-to-offload-suitable-non-ai-related-operations</link>
      <description><![CDATA[作为 GPU 的替代品，主流 NPU 的硬件功能是否适合用于任何非 AI 工作负载？它们擅长什么？
如果它们可以用于矩阵乘法或数据缓冲区上的其他操作之类的任何事情，那么用 C 语言编写的示例会很好。（对于任何主流 NPU，尤其是运行 Linux 或 Windows 的 PC 台式机，带有内置于 CPU 的 Intel Meteor Lake 或 AMD Ryzen AI。）]]></description>
      <guid>https://stackoverflow.com/questions/79080861/how-to-program-a-npu-to-offload-suitable-non-ai-related-operations</guid>
      <pubDate>Sat, 12 Oct 2024 11:04:04 GMT</pubDate>
    </item>
    <item>
      <title>微批次大小如何影响每个 GPU 的吞吐量？</title>
      <link>https://stackoverflow.com/questions/79080507/how-is-micro-batch-size-influencing-the-throughput-per-gpu</link>
      <description><![CDATA[我正在测试在 Megatron-LM 上全局批次大小恒定的情况下，微批次大小如何影响每个 GPU 的吞吐量。
我已经在 2 个 A40 GPU 上使用基于 400M Transformer 的模型进行了几次测试，并且仅使用数据并行性。以下是一些训练参数

在不同的测试中，我仅更改微批次大小，使用 seq_len =1024 和全局批次大小 =24 进行 100 次迭代训练。以下是使用不同微批次大小的一些结果

我使用 megatron-LM 原生功能每 5 次迭代打印一次日志，并计算所有迭代中每个 GPU 的平均吞吐量。
对于每次迭代，总计算复杂度相同，但每个 GPU 的吞吐量随着微批次大小的增加而增加。我知道这可能与 GPU 缓存负载或算术强度有关，但不太清楚。有人可以提供一些深入的解释吗？]]></description>
      <guid>https://stackoverflow.com/questions/79080507/how-is-micro-batch-size-influencing-the-throughput-per-gpu</guid>
      <pubDate>Sat, 12 Oct 2024 08:20:15 GMT</pubDate>
    </item>
    <item>
      <title>混合 TensorFlow LSTM-SVM 预测在测试集上全为零</title>
      <link>https://stackoverflow.com/questions/79080166/hybrid-tensorflow-lstm-svm-predictions-are-all-zeros-on-test-set</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79080166/hybrid-tensorflow-lstm-svm-predictions-are-all-zeros-on-test-set</guid>
      <pubDate>Sat, 12 Oct 2024 04:07:58 GMT</pubDate>
    </item>
    <item>
      <title>如何仅知道输出的总和来训练 NN？</title>
      <link>https://stackoverflow.com/questions/79079993/how-to-train-a-nn-by-only-knowing-the-sum-of-the-outputs</link>
      <description><![CDATA[在我的 ML 项目中，我需要调整某个函数 f，其中在我的数据集中我只知道 f 个评估的总和，例如在这种情况下：
f(a1,b1,c1) + f(a2,b2,c2) + … = S
我只知道总和 S，所以我只能根据总和预测计算损失。我的数据集由 30 到 90 个项的不同总和组成，具有不同的输入组合。
如何在 PyTorch 中正确实现这一点？梯度会“通过总和转移”吗？
我尝试使用常规 MLP 并仅根据预测输出的总和计算损失，但效果不佳，我不知道梯度是否计算正确。]]></description>
      <guid>https://stackoverflow.com/questions/79079993/how-to-train-a-nn-by-only-knowing-the-sum-of-the-outputs</guid>
      <pubDate>Sat, 12 Oct 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>cuda 中如何处理 bf16 和 float32？[关闭]</title>
      <link>https://stackoverflow.com/questions/79078710/how-are-bf16-and-float32-handled-in-cuda</link>
      <description><![CDATA[不幸的是，下面的注释和代码对我来说毫无意义。grad_thresholds[] 被定义为浮点数组。我认为因为这是一个 .cu 文件，一个 CUDA 文件，所以这些本质上是 C 声明。
但是有一个预处理器指令检查 FP_32；在这种情况下，数组值会再次设置。
注释中还提到了 bf16，但我认为“大脑浮点”毫无用处16.
有人能解释一下这段代码与注释的关系吗？
它来自这个文件：
此外，我在学习 CUDA/机器学习时做了一些笔记。这些笔记位于代码下方。
 // 在 fp32 中一次性比较所有参数上的梯度
// 我通过检查每个张量的几个元素的梯度差异来手动设置公差。bf16 看起来还不错，但在这里并不惊人。
// 可能有潜在的错误，或者可能是 bf16。不是 100% 确定。
// 此外，如果代码发生变化，并且其中一些被触发，如果影响不是太大，那么可能没问题，
// 因为我们使用的随机舍入会增加一些非确定性的“胡椒噪声”。
// 在这种情况下，在人工审核后，可以稍微延长容差。
// 此外，不同的 GPU 可能使用不同的矩阵乘法算法，因此
// 实际错误可能与硬件有关。

float grad_thresholds[NUM_PARAMETER_TENSORS] = {5e-1f, 4e-3f, 1e-1f, 3.5e-2f, 2e-2f, 3e-2f, 5e-2f, 5e-2f, 5e-2f, 1.5e-2f, 5e-4f, 8e-3f, 1.5e-3f, 2.5e-3f, 1e-1f, 2e-2f};
#如果已定义 (ENABLE_FP32)
for (int i = 0; i &lt; NUM_PARAMETER_TENSORS; i++) {
grad_thresholds[i] = 1e-6f; // 在 FP32 中我们可以更加精确
}
#endif

注释

CUDA 同时使用 C 和 C++ 语法；C++ 是 C 的超集。CUDA 是 C++ 的超集。
全局阈值被分解为组以进行梯度比较。
更新建议

根据平均值和标准差确定阈值。
为噪声添加 10% 的缓冲区。
重构动态阈值的逻辑
记录任何故障
创建单元测试。测试测试？


类型
BF16 代表大脑浮点 16。BF16 有一个 8 位指数，与 FP32 相同，允许非常大和非常小的数字。它有一个 7 位尾数。
FP32 代表浮点 32。

试图理解此代码的用途。]]></description>
      <guid>https://stackoverflow.com/questions/79078710/how-are-bf16-and-float32-handled-in-cuda</guid>
      <pubDate>Fri, 11 Oct 2024 14:46:17 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM-Attention时的NaN损失</title>
      <link>https://stackoverflow.com/questions/79078535/nan-loss-when-training-lstm-attention</link>
      <description><![CDATA[在模型训练过程中，loss值突然变成Nan，虽然我修改了很多参数，但还是失败了。
我在训练时检查了错误，错误打印在输出中，而不是输入中，所以我认为错误出在参数或我的模型中。
&gt;&gt;&gt; model_name: atae_lstm
&gt;&gt;&gt; 数据集：comment
&gt;&gt;&gt; 优化器：&lt;class &#39;torch.optim.adam.Adam&#39;&gt;
&gt;&gt;&gt; 初始化器：&lt;function kaiming_normal_ at 0x0000022D0817E0C0&gt;
&gt;&gt;&gt; lr: 1e-08
&gt;&gt;&gt; dropout: 0.1
&gt;&gt;&gt; l2reg：0.01
&gt;&gt;&gt; num_epoch：10
&gt;&gt;&gt; batch_size：8
&gt;&gt;&gt; log_step：10
&gt;&gt;&gt; embed_dim：300
&gt;&gt;&gt; hidden_​​dim：300
&gt;&gt;&gt; bert_dim：768
&gt;&gt;&gt; pretrained_bert_name：bert-base-uncased
&gt;&gt;&gt; max_seq_len：85
&gt;&gt;&gt; polarities_dim：3
&gt;&gt;&gt; hops：3
&gt;&gt;&gt; 耐心：5
&gt;&gt;&gt; 设备：cuda
&gt;&gt;&gt;种子：1234
&gt;&gt;&gt; valset_ratio：0
&gt;&gt;&gt; local_context_focus：cdm
&gt;&gt;&gt; SRD：3
&gt;&gt;&gt; model_class：&lt;class &#39;models.atae_lstm.ATAE_LSTM&#39;&gt;
&gt;&gt;&gt; 数据集文件：{&#39;train&#39;：&#39;./datasets/comment/train.raw&#39;，&#39;test&#39;：&#39;./datasets/comment/test.raw&#39;}
&gt;&gt;&gt; 输入列：[&#39;text_indices&#39;， &#39;aspect_indices&#39;]
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
时期：0
损失：1.4378，acc：0.1375
损失：1.4600，acc：0.1125
损失：1.4595，acc：0.1167
损失：1.4558，acc：0.1187
损失：1.4623，acc：0.1125
损失：1.4695，acc：0.1062
损失：1.4511，acc：0.1232
损失：1.4533，acc：0.1203
损失：1.4610，acc：0.1139
损失：1.4598，acc：0.1150
损失：1.4659，acc：0.1102
**损失：nan，acc：0.1073
损失： nan，acc：0.1077
loss：nan，acc：0.1152
loss：nan，acc：0.1133**
[[ 74 0 0]
[ 90 0 0]
[367 0 0]]
&gt; val_acc：0.1394，val_f1：0.0815
&gt;&gt; saved: state_dict/atae_lstm_comment_val_acc_0.1394

模型输入
text_indices = tokenizer.text_to_sequence(text_left + &quot; &quot; + aspects + &quot; &quot; + text_right)
text_indices = text_indices[:tokenizer.max_seq_len] # 如果长度超过 max_seq_len，则截断
aspect_indices = tokenizer.text_to_sequence(aspect)
aspect_indices = aspects_indices[:tokenizer.max_seq_len]

input_colses = &#39;atae_lstm&#39;: [&#39;text_indices&#39;, &#39;aspect_indices&#39;],

这是 LSTM-ATTENTION 模型
来自layer.attention 导入 Attention、NoQueryAttention
来自 layer.dynamic_rnn 导入 DynamicLSTM
导入 torch
导入 torch.nn 作为 nn
来自 layer.squeeze_embedding 导入 SqueezeEmbedding

class ATAE_LSTM(nn.Module):
def __init__(self, embedding_matrix, opt):
super(ATAE_LSTM, self).__init__()
self.opt = opt
self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
self.squeeze_embedding = SqueezeEmbedding()
self.lstm = DynamicLSTM(opt.embed_dim * 2, opt.hidden_​​dim, num_layers=1, batch_first=True)
self.attention = NoQueryAttention(opt.hidden_​​dim + opt.embed_dim, score_function=&#39;bi_linear&#39;)
self.dense = nn.Linear(opt.hidden_​​dim, opt.polarities_dim)
#self.activation = nn.ReLU() 
self.activation = nn.Tanh() 

def forward(self, input):
text_indices, aspects_indices = 输入[0], 输入[1]
x_len = torch.sum(text_indices != 0, dim=-1)
x_len_max = torch.max(x_len)
aspects_len = torch.sum(aspect_indices != 0, dim=-1).float()

x = self.embed(text_indices)
x = self.squeeze_embedding(x, x_len)
aspects = self.embed(aspect_indices)
aspects_pool = torch.div(torch.sum(aspect, dim=1), aspects_len.unsqueeze(1))
aspects = aspects_pool.unsqueeze(1).expand(-1, x_len_max, -1)
x = torch.cat((aspect, x), dim=-1)

# 确保 x_len 在 CPU 上且类型为 int64
h, (_, _) = self.lstm(x, x_len.cpu().to(torch.int64))
ha = torch.cat((h, aspects), dim=-1)
_, score = self.attention(ha)
output = torch.squeeze(torch.bmm(score, h), dim=1)

output = self.activation(output) 

out = self.dense(output)
return out

这是我更改的参数
 if opt.model_name.lower() == &#39;atae_lstm&#39;:
opt.batch_size = 8
opt.num_epoch = 10
opt.lr = 1e-8
opt.initializer= &#39;kaiming_normal_&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79078535/nan-loss-when-training-lstm-attention</guid>
      <pubDate>Fri, 11 Oct 2024 13:59:15 GMT</pubDate>
    </item>
    <item>
      <title>将 TensorFlow 权重和模型转换为 PyTorch（修改后的 EffectiveNet）</title>
      <link>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</link>
      <description><![CDATA[我正尝试在 pytorch 中模拟一个经过修改的 efficientnet TF 模型。我在 pytorch 中对模型进行了架构更改，转储了 TF 模型权重，然后将其重新加载到新的 pytorch 模型中。使用以下代码在 TF 中转储权重：
model = tf.saved_model.load(model_path)
ws = []
for i in range(len(model.variables)):
ws.append((i, model.variables[i].name, model.variables[i].numpy()))

with open(&quot;manually_dumped_contentnet_weights.pkl&quot;, &quot;wb&quot;) as ofile:
pickle.dump(ws, ofile)

pytorch 中的权重形状似乎与架构和导入的权重相匹配（在 conv2d 和深度 conv2d 之间进行转换之后）。我可以毫无错误地运行模型。但输出结果与 TF 模型的输出结果大不相同。
我注意到在 TF 代码中，模型不是直接加载的，而是在 tf Session 中加载的：
with Session(graph=Graph(), config=ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:
saved_model.loader.load(sess, [saved_model.tag_constants.SERVING], model_path)
patch_feature, patch_label = sess.run(output_nodes,feed_dict={input_node: patch})

现在我想知道我最初转储模型权重的尝试是否不正确。或者如果我遗漏了其他内容。
我在加载数据时进行的转置是 conv2d 的 (3,2,0,1) 和深度 conv2d 的 (2,3,0,1)：
def reload_conv2d(layer, weights):
### weights 是一个元组，其中每个元素都由一个三元组组成：(1) 索引号，(2) TF 中权重转储的层的名称，以及 (3) 权重
count = 0
if (
&quot;/conv2d/kernel&quot; not in weights[0][1]
and &quot;/conv2d_1/kernel&quot; not in weights[0][1]
and &quot;depthwise_conv2d/depthwise_kernel&quot; not in weights[0][1]
and &quot;final_conv2d/final_conv2d&quot; 不在 weights[0][1] 中 :
raise ValueError(
f&quot;需要在第一个索引上有 conv2d/kernel，但得到了 {weights[0][1]}&quot;
)
transpose_shape = (2,3,0,1) if &quot;depthwise&quot;在 weights[0][1] 中否则（3、2、0、1）
transposed_weights = torch.from_numpy（weights[0][2].transpose（transpose_shape[0]、transpose_shape[1]、transpose_shape[2]、transpose_shape[3]））
layer.weight.data = transposed_weights
count += 1
如果 layer.bias 不是 None 或 layer.bias:
如果（
&quot;/conv2d/bias&quot; 不在 weights[1][1] 中
并且 &quot;/conv2d_1/bias&quot; 不在 weights[1][1] 中
）：
引发 ValueError（
f&quot;需要在第二个索引上有 conv2d/bias 但得到了 {weights[1][1]}&quot;
)
layer.bias.data = (
torch.from_numpy(weights[1][2])
如果type(weights[1][2]) == np.ndarray
else torch.from_numpy(weights[1][2])
)
count += 1
return layer, count

为什么 pytorch 和 TF 模型对相同输入给出完全不同的结果？是因为权重倾倒，还是权重加载……或者是模型架构变化？输入 TF 权重（在模型更改和转置之后）加载正常，我可以毫无问题地运行模型，但这对于调试它没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</guid>
      <pubDate>Thu, 10 Oct 2024 23:37:12 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 yolo 自定义模型时，data.yaml 文件中的相对路径出现问题</title>
      <link>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</link>
      <description><![CDATA[我正在尝试创建一个训练管道，以使用用户输入的带标签图像来训练自定义 yolov9 模型。
我遇到了一个问题，如果我使用相对路径来设置 data.yaml 文件，就会出现错误：
 RuntimeError：数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”错误
数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”图像未找到，缺少路径“C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val”

更奇怪的是，错误路径提及，
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

不是存在或正在任何地方请求的路径。实际路径是
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

由于某种原因，它重复了路径的第一部分 3 次。
这是 data.yaml 文件：
 path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

这是开始训练的代码：

def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
# 指定训练运行的保存目录
save_dir = &#39;OIT_model/customOIT/yolocustomtrainoutput&#39;
if os.path.exists(save_dir):
for file in os.listdir(save_dir):
file_path = os.path.join(save_dir, file)
if os.path.isfile(file_path) or os.path.islink(file_path):
os.unlink(file_path)
elif os.path.isdir(file_path):
shutter.rmtree(file_path)
os.makedirs(save_dir, exist_ok=True)
model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
return
train_custom_dataset_yolo(&#39;OIT_model/customOIT/customdatasetyolo/data.yaml&#39;, epochs=1,imgsz=64, verbose=True)

然而，非常奇怪的是，当我用绝对路径替换相对路径时，如下所示：
 path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

训练没有问题。对于我来说，使用绝对路径不是一个选择，因为这个应用程序需要在其他机器上可重现。]]></description>
      <guid>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</guid>
      <pubDate>Thu, 10 Oct 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>如何从 PDF 研究论文中准确提取标题、标题和副标题？</title>
      <link>https://stackoverflow.com/questions/79050147/how-to-accurately-extract-title-headings-and-subheadings-from-pdf-research-pap</link>
      <description><![CDATA[我试图从 PDF 格式的研究论文中提取标题、标题和副标题。我尝试了各种方法，但都无法获得准确的结果。以下是我采取的步骤：
1. 尝试使用 PyMuPDF (fitz)
我使用 PyMuPDF (fitz) 从 PDF 中提取文本。虽然我能够获取文本，但问题是格式丢失了（例如，标题和副标题不容易区分）。文档的其他部分（如引文和脚注）也存在额外的噪音。
2. 提示语言模型
我还尝试使用提示语言模型 (LLM) 来分析提取的文本。我使用 Ollama 进行离线处理，但结果不够准确。当我尝试 OpenAI 的 GPT 和 Gemini 时，它们提供了准确的输出，但我想要一个可以离线工作的解决方案。
我尝试过的：

PyMuPDF (fitz)
Ollama (llama3.1、gemma)
OpenAI GPT 和 Gemini 可实现准确提取，但它们需要在线使用。
PyPDF2 和类似的库，但它们也会返回非结构化文本。

我需要的：

从 PDF 研究论文中准确提取标题、标题和副标题。
离线解决方案。
来自引用、页码等额外内容的噪音最小。

是否有可靠的离线方法或我可以采取的一些额外步骤至：

识别并准确提取标题、标题和副标题。
尽量减少输出中的噪音和不相关内容。
]]></description>
      <guid>https://stackoverflow.com/questions/79050147/how-to-accurately-extract-title-headings-and-subheadings-from-pdf-research-pap</guid>
      <pubDate>Thu, 03 Oct 2024 10:37:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 transformer 和 tensorflow 时内核崩溃</title>
      <link>https://stackoverflow.com/questions/79042488/kernel-crashing-while-using-transformer-and-tensorflow</link>
      <description><![CDATA[我正在研究一个开源数据集，并决定使用一个文本摘要库，为此我安装了 transformers。后来我发现有一些必备库我应该安装，因此通过浏览 stackoverflow 中的一些论坛，我在 anaconda prompt 中使用 pip 命令安装了以下软件包，并提到了版本：

transformers --&gt; 4.45.1
tensorflow --&gt; 2.17.0
numpy --&gt; 1.26.4
keras --&gt; 3.5.0

我的代码中的导入库命令如下所示：
import pandas as pd
from transformers import pipeline
import os

os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
import tensorflow as tf

当我尝试运行以下代码时，我收到错误消息，如下所示
未提供默认为 google-t5/t-5-small 和修订版 df1b051 的模型（https://huggingface.co/google-t5/t5-small。不建议在生产中使用未指定模型名称和修订版的管道

然后内核崩溃并显示此错误消息
由于不同计算顺序的浮点舍入，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量&#39;TF_ENABLE_ONEDNN_OPTS&#39; = &#39;0&#39;** 如屏幕截图所示 --\&gt; [未提供模型错误](https://i.sstatic.net/p5L7e0fg.png)。[内核崩溃错误](https://i.sstatic.net/KPgtb84G.png)

#loading summarization pipeline

summarizer = pipeline(&quot;summarization&quot;)

text = &quot;Capcom 的最新挑战者来了！《街头霸王™ 6》将于 2023 年 6 月 2 日在全球推出，代表了《街头霸王™》系列的下一个发展！《街头霸王 6》涵盖三种不同的游戏模式，包括世界巡回赛、格斗场和战斗中心。&quot;

#summarize text

summary = summaryr(text , max_length = 30 , min_length = 10 , do_sample = False)

print(summary)

我使用下面的代码来解决环境变量问题，但即便如此，我仍然收到此错误消息：
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
import tensorflow as tf

问题是我收到了两种错误 - 一种表示没有可用于管道的模型，另一种与环境变量有关。
如何解决它们？]]></description>
      <guid>https://stackoverflow.com/questions/79042488/kernel-crashing-while-using-transformer-and-tensorflow</guid>
      <pubDate>Tue, 01 Oct 2024 09:27:45 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSequenceClassification 损失没有减少</title>
      <link>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</link>
      <description><![CDATA[从数据集导入 load_dataset
从 torch.utils.data 导入 DataLoader
从 transformers 导入 AutoTokenizer、AutoModelForSequenceClassification
导入 torch
从 tqdm 导入 tqdm

def train_one_epoch(model、dataloader、optimizer):
model.train()
loss_list = []
for batch in tqdm(dataloader):
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;],
&#39;labels&#39;: batch[&#39;labels&#39;]
}
loss = model(**batch_data).loss
loss.backward()
optimizer.step()
optimizer.zero_grad()

loss_list.append(loss.detach().item())
avg_loss = sum(loss_list) / len(loss_list)
print(&#39;avg loss在 epoch:&#39;, avg_loss)

def assess(model, dataloader):
model.eval()
all_labels = []
all_predictions = []
for batch in dataloader:
with torch.no_grad():
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;]
}
logits = model(**batch_data).logits
predictions = torch.argmax(logits, dim=-1)
labels = batch[&#39;labels&#39;]
all_labels.extend(labels)
all_predictions.extend(predictions)
accuracy = compute_accuracy(all_predictions, all_labels)
print(&quot;Accuracy&quot;, accuracy)
return accuracy

def compute_accuracy(predictions, labels):
correct = 0
for pred，zip(predictions, labels) 中的标签：
if pred == label:
correct += 1
返回正确 / len(labels)

def my_collat​​e_fn(batched_samples):
texts = [example[&#39;text&#39;] 例如 batched_samples]
labels = [example[&#39;label&#39;] 例如 batched_samples]
text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors=&#39;pt&#39;)
labels = torch.LongTensor(labels)
return {
&#39;input_ids&#39;: text_encoding[&#39;input_ids&#39;].cuda(),
&#39;attention_mask&#39;: text_encoding[&#39;attention_mask&#39;].cuda(),
&#39;labels&#39;: labels.cuda()
}

torch.manual_seed(64)
batch_size = 16
学习率 = 5e-5
训练次数 = 10
模型名称 = “roberta-base”

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
datasets[&#39;train&#39;],
batch_size=8,
shuffle=True,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

validation_dataloader = DataLoader(
datasets[&#39;validation&#39;],
batch_size=8,
shuffle=False,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

best_acc = 0.0
for周期范围（1，num_epochs + 1）：
train_one_epoch（模型，train_dataloader，优化器）
valid_acc = 评估（模型，validation_dataloader）


100%|██████████| 865/865 [01:27&lt;00:00，9.89it/s]

周期内平均损失：0.6746856869559068

准确率 0.4908256880733945

100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]

epoch 中的平均损失：0.6922555248516833

准确率 0.4908256880733945

100%|██████████| 865/865 [01:27&lt;00:00, 9.89it/s]

epoch 中的平均损失：0.6976809655310791

准确率 0.5091743119266054

更改学习率也不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</guid>
      <pubDate>Sat, 21 Sep 2024 16:24:50 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量</title>
      <link>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</link>
      <description><![CDATA[我无法加载我的模型，它一直显示错误
ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]
这是我的代码
image_generator = ImageDataGenerator(
rescale=1./255,
rotation_range=20,
zoom_range=0.2,
width_shift_range=0.2,
height_shift_range=0.2,
Horizo​​ntal_flip=True,
validation_split=0.2
)

train_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;training&#39;
)

validation_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;validation&#39;
)

# 加载数据集中子文件夹中的 (num_classes) 类
num_classes = len(train_dataset.class_indices)

from tensorflow.keras.applications.mobilenet import MobileNet

# 加载 MobileNet 模型
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
include_top=False,
weights=&#39;imagenet&#39;)

pre_trained_model.summary()

# 打印数据集信息以供调试
print(f&quot;训练数据集形状：{train_dataset.image_shape}&quot;)
print(f&quot;验证数据集形状：{validation_dataset.image_shape}&quot;)

pre_trained_model.trainable = False

# 为预训练模型添加自定义层
model = tf.keras.Sequential([
pre_trained_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(1024,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;) 
])

# 编译模型
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

batch=40
history = model.fit(train_dataset,
validation_data=validation_dataset,
epochs=20,
steps_per_epoch = train_dataset.samples//batch,
validation_steps =validation_dataset.samples//batch,
verbose = 1
)

# 加载模型
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;

# 加载模型，确保必要时已编译
loaded_model = tf.keras.models.load_model(model_save_path) 

# 现在您可以根据需要修改已加载的模型
# 例如，如果您想提取子模型：
input_layer_index = 0 # 替换为实际索引
dense_2_index = 3 # 替换为实际索引
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
outputs=loaded_model.layers[dense_2_index].output)

# 检查已加载模型的配置
for i, layer in enumerate(loaded_model.layers):
print(f&quot;Layer {i}: {layer.name} - 输入形状：{layer.input_shape} - 输出形状：{layer.output_shape}&quot;)

print(&quot;已成功加载修订模型。&quot;)

我尝试加载模型，并希望它能够加载以进行测试]]></description>
      <guid>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</guid>
      <pubDate>Thu, 08 Aug 2024 07:06:54 GMT</pubDate>
    </item>
    <item>
      <title>我如何在第二次运行中重试 Optuna 中的失败试验？</title>
      <link>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</link>
      <description><![CDATA[我正在使用 Optuna 进行网格搜索，但第二次运行中不会重复失败试验。相反，已经完成的试验会毫无意义地重复。
在这里我分别描述这两个问题：

当试验失败（例如缺乏计算资源）时，第二次启动网格搜索（Python 文件）时不会重复。这可以使用以下自包含代码进行测试，其中我通过启动异常来模拟问题。注释这些行并再次运行，以查看 x=2 和 y=2 的组合是否没有重复。

import time
import optuna
from optuna.storages import RetryFailedTrialCallback
import numpy as np

def objective(trial):
# 获取值
params = {
&#39;x&#39;: trial.suggest_categorical(&#39;x&#39;, [0, 1, 2, 3]),
&#39;y&#39;: trial.suggest_categorical(&#39;y&#39;, [0, 1, 2, 3])
}
# 打印
print(&#39;Testing with x=&#39; + str(params[&#39;x&#39;]), &#39;y=&#39; + str(params[&#39;y&#39;]))

########################################
# 在第一部分之后注释此部分运行 #
###########################################
如果 params[&#39;x&#39;] == 2 且 params[&#39;y&#39;] == 2:
引发 ValueError(&quot;x==2, y==2&quot;)
########################################

# 返回
返回 params[&#39;x&#39;] ** 2 - params[&#39;y&#39;]

def optuna_search_space():
# 定义搜索空间
返回 {
&#39;x&#39;: range(3),
&#39;y&#39;: range(3),
}

def optuna_grid():
# 定义 URL
URL = &#39;mysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;IP&gt;:&lt;PORT&gt;&#39;
# 获取搜索空间
search_space = optuna_search_space()
# 定义存储
storage = optuna.storages.RDBStorage(
url=f&quot;{URL}/prove_optuna&quot;,
failed_trial_callback=RetryFailedTrialCallback(max_retry=3),
)
# 定义研究
study = optuna.load_study(
study_name=&quot;test1&quot;,
sampler = optuna.samplers.GridSampler(search_space),
storage = storage,
)
# 运行
study.optimize(objective)
# 打印
print(study.best_trial)

如果 __name__ == &quot;__main__&quot;:
# 运行
optuna_grid()


当我重新运行代码时，它会重复已经执行过的一次（或更多次）试验。我不想这样，因为这是计算资源的损失。

在 Optuna 仪表板上，可以看到，经过多次重新运行组合 (x=2, y=2) 后，它再也不会重复（即使第一次失败），并且组合 (x=0, y=1) 已经测试了几次（无用）。

我该如何解决这些问题？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</guid>
      <pubDate>Mon, 04 Dec 2023 13:32:29 GMT</pubDate>
    </item>
    <item>
      <title>使用序列数据进行多类分类，LSTM Keras 不起作用</title>
      <link>https://stackoverflow.com/questions/58339718/multiclass-classification-using-sequence-data-with-lstm-keras-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/58339718/multiclass-classification-using-sequence-data-with-lstm-keras-not-working</guid>
      <pubDate>Fri, 11 Oct 2019 10:53:27 GMT</pubDate>
    </item>
    <item>
      <title>如何从 scikit-learn 决策树中提取决策规则？</title>
      <link>https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree</link>
      <description><![CDATA[我可以从决策树中经过训练的树中提取底层决策规则（或“决策路径”）作为文本列表吗？
类似于：
如果 A&gt;0.4，则如果 B&lt;0.2，则如果 C&gt;0.8，则 class=&#39;X&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree</guid>
      <pubDate>Tue, 26 Nov 2013 17:58:00 GMT</pubDate>
    </item>
    </channel>
</rss>