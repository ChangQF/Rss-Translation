<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 11 Aug 2024 15:16:20 GMT</lastBuildDate>
    <item>
      <title>无法在 colab 中更改 elasticsearch 目录的所有权</title>
      <link>https://stackoverflow.com/questions/78858626/cannot-change-ownership-of-elasticsearch-directory-in-colab</link>
      <description><![CDATA[我尝试在 colab 中安装 elastic search，但似乎要运行该服务，需要将所有权更改为非 root用户。我尝试使用 chown 命令，然后执行 ls -l，但没有看到所有权更改。
有趣的是，我使用 chown 的详细选项，看到显示更改的日志

但执行 ls -l 仍显示 root 为所有者和组。
]]></description>
      <guid>https://stackoverflow.com/questions/78858626/cannot-change-ownership-of-elasticsearch-directory-in-colab</guid>
      <pubDate>Sun, 11 Aug 2024 14:54:52 GMT</pubDate>
    </item>
    <item>
      <title>纪元 1/3 ^C（model.fit() 以此行终止且没有任何错误）</title>
      <link>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminate-with-this-line-and-without-any-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminate-with-this-line-and-without-any-error</guid>
      <pubDate>Sun, 11 Aug 2024 13:47:36 GMT</pubDate>
    </item>
    <item>
      <title>获取 ValueError：所有数组的长度必须相同</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[我一直试图将字典转换为数据框，但每次我都收到 ValueError：所有数组的长度必须相同。我已经检查了每个数组的长度并确认它们相同，但我仍然收到相同的错误

def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# 合并来自训练集和测试集的指标
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# 排序列 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# 获取指标表
metrics_table = metrics_from_pipes(pipelines)
#print(&#39;Table 1: Base Models Metrics&#39;)
#metrics_table.style.background_gradient(cmap = Blues)
metrics_table

运行此代码会出现此错误
ValueError Traceback (most recent call last)
Cell In[45], line 82
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # 获取指标表
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;表 1：基本模型指标&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

单元格 In[45]，第 50 行，位于 metrics_from_pipes(pipes_dict)
41 # 将性能指标列表聚合到单独的数据框中
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: 所有数组的长度必须相同

当我检查 train_metrics 和 val 指标的字典结果时，我得到了这个
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;线性回归&#39;,
&#39;随机森林回归器&#39;,
&#39;梯度提升回归&#39;,
&#39;额外树回归器&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>批量数据的 SGD 优化器设置</title>
      <link>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</link>
      <description><![CDATA[我是 ML 的新手，刚刚开始学习。我正在学习 Joh Krohn 的数学 ML 入门课程。课程解释得很清楚，但有一件事让我很困惑。在这个任务中 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynb 我们使用了 torch.optim.SGD torch SGD，它运行了所有示例数据。
optimizer = torch.optim.SGD([m,b], lr = 0.01)
epochs = 999
for epoch in range(epochs): 

optimizer.zero_grad() # 将梯度重置为零；否则它们会累积

yhats = 回归（xs，m，b）# 步骤 1
C = mse（yhats，ys）# 步骤 2

C.backward() # 步骤 3

optimizer.step() # 步骤 4

在第二个练习中，我们进行了学习率调度 https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/learning-rate-scheduling.ipynb
有 8.000.000 个数据点，因此数据被设置为批处理，并且代码在这些样本上轮流运行，而不是在所有数据上按时期运行。然而，这不是用 torch.optim.SGD 完成的，而是在代码上显示以查看数学是如何工作的。我正在努力用 torch.optim.SGD 运行它。如何编写代码来运行它，而不是像下面这样编写大型数学方程式，其中已经创建了所有方程式，例如梯度、theta：
n = 8000000
x = torch.linspace(0., 8., n)
y = -0.5*x + 2 + torch.normal(mean=torch.zeros(n), std=1)
indices = np.random.choice(n, size=2000, replace=False)
gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 
lr = 0.01
new_theta = theta - lr*gradient
C = mse(regression(x[batch_indices], m, b), y[batch_indices])
b.requires_grad_()
m.requires_grad_()

def return(my_x, my_m, my_b):
return my_m*my_x + my_b

m = torch.tensor([0.9]).requires_grad_()
b = torch.tensor([0.1]).requires_grad_()

batch_size = 32 # 模型超参数
batch_indices = np.random.choice(n, size=batch_size, replace=False)
yhat = return(x[batch_indices], m, b)

yhat = return(x[batch_indices], m, b)

def mse(my_yhat, my_y): 
sigma = torch.sum((my_yhat - my_y)**2)
return sigma/len(my_y)

C = mse(yhat, y[batch_indices])

C.backward()
m.grad
b.grad

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T

theta = torch.tensor([[b, m]]).T 

lr = 0.01
new_theta = theta - lr*gradient
new_theta

b = new_theta[0]
m = new_theta[1]

C = mse(regression(x[batch_indices], m, b), y[batch_indices])

rounds = 100 

for r in range(rounds): 

# 这个采样步骤很慢；稍后我们将介绍更快的批量采样： 
batch_indices = np.random.choice(n, size=batch_size, replace=False)

yhat = return(x[batch_indices], m, b) # 步骤 1
C = mse(yhat, y[batch_indices]) # 步骤 2

C.backward() # 步骤 3

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 

new_theta = theta - lr*gradient # 步骤 4

b = new_theta[0].requires_grad_()
m = new_theta[1].requires_grad_()
]]></description>
      <guid>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</guid>
      <pubDate>Sun, 11 Aug 2024 11:20:48 GMT</pubDate>
    </item>
    <item>
      <title>解决自动标记（优化）+分类问题</title>
      <link>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</link>
      <description><![CDATA[我知道这不太侧重于编程，但我不知道还有什么地方可以问这个问题。这更多的是关于方法而不是技术问题。
上下文
我有一个优化 + 分类任务。因此，本质上，我的数据具有以下列：
[&#39;Model ID&#39;, &#39;Q&#39;, &#39;refinement&#39;, &#39;avg_time&#39;, &#39;lattice&#39;, &#39;radius&#39;]（还有更多，但为了简洁起见，我们只保留这些）

Model ID：代表“设计”，每个 Model ID 将有多行

Q：这是目标变量

refinement：这是一个设置变量；它可以取 1-8 的值，这直接影响 Q，（模型 ID，细化）对是唯一的。因此，模型 ID 将具有多行，细化程度各不相同

avg_time：这是模拟完成所需的时间，仅受细化的影响，细化程度越高，所需的时间越长。此值与设计无关，它仅取决于细化，因此特定细化的所有设计都具有相同的时间。

lattice 和 radius：这些代表“设计”，本质上更改它们将更改 Q


数据集
我的数据集来自随机设计的模拟。对于每个设计，我们可以有以下行为：

持续增加（每次细化时的 Q 值高于上一个细化级别）
持续减少（每次细化时的 Q 值低于上一个细化级别）
之字形，其中 Q 值遵循此当前模式（高，低，高，低）或（低，高，低，高）
碗形，其中 Q 值遵循此当前模式：（高，低，低，高）
梯形，其中 Q 值遵循此当前模式：（低，高，高，低）
我有代码可以检测这些形状并返回布尔值：

def is_zigzag(q_values)

def is_bowl(q_values)

def is_trapezoid(q_values)

数据集中细化的值范围是 2-5，但细化可以取 1-8 的值。
任务
因此，我试图实现的是自动标记每个模型 ID（通过对行进行分组）和最佳细化值（范围为 1-8），以最大化 Q 的变化（增量越大越好）并最小化所花费的时间（越低越好）。问题是由于数据是在细化级别 5 处切割的，所以我想到使用概率方法（例如 MLE）来创建未来细化的预期变化和预期所花费的时间。但我似乎无法“调整”它，所以它很有用。获得预期值后，我需要一个成本函数来计算（优化）Q 的回报与完成该细化级别所花费的增加时间的比较。
在下一部分中，我将开发一个分类器，它将采用设计和最佳细化。从理论上讲，它应该可以预测未见过的设计的最佳细化级别
我很感激任何有关解决这个问题的指导/帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</guid>
      <pubDate>Sun, 11 Aug 2024 11:01:40 GMT</pubDate>
    </item>
    <item>
      <title>处理缺失数据并建立具有不完整信息的预测模型？</title>
      <link>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</link>
      <description><![CDATA[我正在为涉及 20 个影响点的供水网络开发一个预测模型。但是，我只有这 20 个点中的 10 个的历史数据。
我想知道如何在这个不完整的数据集下构建预测模型。具体来说：
我可以使用哪些方法来处理剩余 10 个点的缺失数据？在这种情况下，是否有任何标准技术或最佳实践来处理缺失数据？
我如何有效地将我拥有的 10 个点的数据合并到模型中？我可以采用哪些策略来确保有效利用可用数据进行准确预测？
是否有特定的技术或模型可以帮助在数据不完整的情况下进行预测？我对可以有效管理和利用不完整数据的方法感兴趣。
&quot;我还没有具体的方法。&quot;]]></description>
      <guid>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</guid>
      <pubDate>Sun, 11 Aug 2024 10:43:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 hub.KerasLayer 使用 tf.keras.sequential 制作深度学习模型时出错</title>
      <link>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</link>
      <description><![CDATA[我是刚开始使用 keras 的，这里我遇到了一些问题：
创建一个构建 Keras 模型的函数
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
print(&quot;Building model with:&quot;, MODEL_URL)

model = tf.keras.Sequential([
hub.KerasLayer(MODEL_URL), # 第 1 层（输入层）
tf.keras.layers.Dense(units=OUTPUT_SHAPE, 
activation=&quot;softmax&quot;) # 第 2 层（输出层）
])

# 编译模型
model.compile(
loss=tf.keras.losses.CategoricalCrossentropy(), # 我们的模型想要减少这个（它的猜测有多错误）
optimizer=tf.keras.optimizers.Adam()，# 一个朋友告诉我们的模型如何改进它的猜测
metrics=[&quot;accuracy&quot;] # 我们希望这个数字上升
)

# 构建模型
model.build(INPUT_SHAPE) # 让模型知道它将获得什么样的输入

返回模型

我有上面的函数，当我运行下面的其他程序时
model = create_model()
model.summary()

它会产生一些错误
TypeError：添加的层必须是 Layer 类的实例。收到：layer= 类型为 &lt;class &#39;keras.src.layers.core.dense.Dense&#39;&gt;。
所以我错了什么，我一直在搜索这个，但问题和我得到的不一样]]></description>
      <guid>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</guid>
      <pubDate>Sun, 11 Aug 2024 08:05:45 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 CV-k 折叠预测“是”“否”的均方误差？[关闭]</title>
      <link>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</link>
      <description><![CDATA[为了根据满意度指数和参与度指数预测客户是否会购买产品，我们使用了 k 近邻法。
如何用 4 倍交叉验证过程评估预测均方误差？它不需要在 R 中。我只需要如何计算它的理论。
我知道如何用数字计算，但不知道如何用字符串计算]]></description>
      <guid>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</guid>
      <pubDate>Sun, 11 Aug 2024 05:16:27 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 SGDRegressor 模型性能</title>
      <link>https://stackoverflow.com/questions/78857401/how-to-improve-sgdregressor-model-performance</link>
      <description><![CDATA[我正在做一个个人项目，比较 OLS 模型和 SGDRessor 模型之间的模型性能。OLS 模型并不完美，但运行良好。SGDR 模型预测偏差很大。我检查了迭代过程中的成本降低情况。结果表明学习率太高。然而，降低学习率似乎会使成本降低变得更糟。
我是机器学习的新手，我不知道如何改进 SGDR 模型。任何建议都将不胜感激。
这是我的代码：Google Colab 链接。您可以重点关注“梯度下降方法”部分和“排除 SGDR 故障”。
提前感谢您的时间和帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78857401/how-to-improve-sgdregressor-model-performance</guid>
      <pubDate>Sun, 11 Aug 2024 02:34:35 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard 中的 add_hparams() 函数无法正常工作</title>
      <link>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</link>
      <description><![CDATA[我试图向此 SummaryWriter 添加指标，但不起作用。
我正在使用 SummaryWriter 的 add_hparams() 函数，其详细信息可在此处找到：https://pytorch.org/docs/stable/tensorboard.html。
我这样做：
 writer = SummaryWriter(f&#39;runs/lstm_experiment_final&#39;)

for e in tqdm(range(num_epochs)):
tr_loss, tr_f1, tr_precision, tr_recall = training_loop(model, train_dataloader, loss_function, optimizer, e, writer)
val_loss, val_f1, val_precision, val_recall = validation_loop(model, test_dataloader, loss_function, e, writer)

metric_dict = {&#39;Loss/train&#39;: tr_loss, &#39;Loss/valid&#39;: val_loss,
&#39;F1/train&#39;: tr_f1, &#39;F1/valid&#39;: val_f1,
&#39;Precision/train&#39;: tr_precision, &#39;Precision/valid&#39;: val_precision,
&#39;Recall/train&#39;: tr_recall, &#39;Recall/valid&#39;: val_recall}
writer.add_hparams(best_params, metric_dict, global_step=num_epochs-1)
writer.close()

这就是正在发生的事情。
在此处输入图片描述
换句话说，超参数确实记录在 TensorBoard 上，但度量值却没有记录。
我希望有人已经看到我的问题并知道如何解决这个问题。
提前谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</guid>
      <pubDate>Sun, 11 Aug 2024 00:21:56 GMT</pubDate>
    </item>
    <item>
      <title>启用 GPU 2024-OSError：[WinError 127] 找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</link>
      <description><![CDATA[我正在尝试启用 GPU 进行机器学习，但遇到了这个问题：

OSError：[WinError 127] 找不到指定的程序。错误
加载
“C:\Users\name\anaconda3\Lib\site-packages\torch\lib\c10_cuda.dll”
或其依赖项之一。

目前，我正在使用配备 NVIDIA 3050 GPU 的 Windows 笔记本电脑，以 Python 作为我的主要开发语言。

注意到该文件存在，因为 torch 库是使用 pip 直接下载的。因此，从技术上讲，此错误不应该发生。

最初，我收到了类似的错误“[WinError 126] 找不到指定的过程。”但我已经更新并安装了 VisualStudio 2022 的 C/C++ 编译器，它解决了 [WinError 126}，但我得到的却是 [WinError 127]。
要安装和启用 GPU，我将按照此教程执行以下步骤。

安装 VisualStudio -&gt; 全部下载

安装 Pytorch (pip) -&gt; CUDA 12.4

安装 CUDA 工具包 (12.6)

下载 cuDNN“下载 cuDNN v8.9.7 (2023 年 12 月 5 日)，适用于 CUDA 12.x”

将 cuDNN 的内容按照各自的文件夹名称粘贴到“NVIDIA GPU 计算工具包”中。


已验证已安装的 CUDA 已添加到环境中

我也按照这个最近更新的视频的建议下载 C/C++ 编译器。然而，这并没有解决我的问题。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</guid>
      <pubDate>Sat, 10 Aug 2024 16:59:08 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    <item>
      <title>在扩展中访问 NetLogo 扩展</title>
      <link>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</link>
      <description><![CDATA[我正在尝试开发一个 NetLogo 扩展来与不同的 LLMS（在线、离线）进行通信。LLM 调用返回 JSON 格式的字符串。我想解析 JSON 并将其转换为嵌套的 TABLE 对象，以访问 NetLogo Table 扩展。
是否有办法让一个扩展访问和使用另一个扩展中的类？]]></description>
      <guid>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</guid>
      <pubDate>Fri, 09 Aug 2024 03:21:02 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量</title>
      <link>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</link>
      <description><![CDATA[我无法加载我的模型，它一直显示错误
ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]
这是我的代码
image_generator = ImageDataGenerator(
rescale=1./255,
rotation_range=20,
zoom_range=0.2,
width_shift_range=0.2,
height_shift_range=0.2,
Horizo​​ntal_flip=True,
validation_split=0.2
)

train_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;training&#39;
)

validation_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;validation&#39;
)

# 加载数据集中子文件夹中的 (num_classes) 类
num_classes = len(train_dataset.class_indices)

from tensorflow.keras.applications.mobilenet import MobileNet

# 加载 MobileNet 模型
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
include_top=False,
weights=&#39;imagenet&#39;)

pre_trained_model.summary()

# 打印数据集信息以供调试
print(f&quot;训练数据集形状：{train_dataset.image_shape}&quot;)
print(f&quot;验证数据集形状：{validation_dataset.image_shape}&quot;)

pre_trained_model.trainable = False

# 为预训练模型添加自定义层
model = tf.keras.Sequential([
pre_trained_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(1024,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;) 
])

# 编译模型
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

batch=40
history = model.fit(train_dataset,
validation_data=validation_dataset,
epochs=20,
steps_per_epoch = train_dataset.samples//batch,
validation_steps =validation_dataset.samples//batch,
verbose = 1
)

# 加载模型
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;

# 加载模型，确保必要时已编译
loaded_model = tf.keras.models.load_model(model_save_path) 

# 现在您可以根据需要修改已加载的模型
# 例如，如果您想提取子模型：
input_layer_index = 0 # 替换为实际索引
dense_2_index = 3 # 替换为实际索引
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
outputs=loaded_model.layers[dense_2_index].output)

# 检查已加载模型的配置
for i, layer in enumerate(loaded_model.layers):
print(f&quot;Layer {i}: {layer.name} - 输入形状：{layer.input_shape} - 输出形状：{layer.output_shape}&quot;)

print(&quot;已成功加载修订模型。&quot;)

我尝试加载模型，并希望它能够加载以进行测试]]></description>
      <guid>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</guid>
      <pubDate>Thu, 08 Aug 2024 07:06:54 GMT</pubDate>
    </item>
    <item>
      <title>将自定义模型和配置注册到 AutoModel 和 AutoConfig</title>
      <link>https://stackoverflow.com/questions/77428197/registering-custom-model-and-config-to-automodel-and-autoconfig</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77428197/registering-custom-model-and-config-to-automodel-and-autoconfig</guid>
      <pubDate>Mon, 06 Nov 2023 00:48:18 GMT</pubDate>
    </item>
    </channel>
</rss>