<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 17 Sep 2024 01:00:24 GMT</lastBuildDate>
    <item>
      <title>尝试改变 vae 模型的输出以满足损失函数，但有些不起作用</title>
      <link>https://stackoverflow.com/questions/78991901/tried-changing-the-output-of-vae-model-to-satisfy-loss-function-but-something-do</link>
      <description><![CDATA[我尝试创建一个 VAE 模型，该模型通常包含自定义损失，使用 GradientTape() 或类来实现。我不想使用这些方法，而是尝试了一种解决方法，但效果不如预期，我想知道原因。
我尝试的模型代码 - （编码器和解码器只是基本模型，其中编码器输出 z_mean、z_log_var、z，解码器输出一批图像） -
def build_vae(encoder,coder, input_shape):
input_layer = layer.Input(shape=input_shape)
z_mean, z_log_var, z =coder(input_layer)
generated_images =coder(z)

return models.Model(input_layer, [generated_images, z_mean, z_log_var])

自定义损失 -
def custom_vae_loss(y_true, y_pred):
generated_images = y_pred[0]
z_mean = y_pred[1]
z_log_var = y_pred[2]

r_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, generated_images))
kl_loss = -0.5 * (tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)))

return r_loss+kl_loss

训练代码 -
vae.train_on_batch(batch, batch)
错误是 target_shape = (8, 256, 256, 3) 与 output_shape = (256, 256, 3) 不匹配
我当时想的是，应该是 y_pred 的列表被挤压了，或者其他什么原因，以至于 z_mean 和z_log_var 已被删除，这意味着 y_pred（来自 custom_vae_loss）是一批生成的图像，因此 generated_images 只是该批次中第一个生成的图像。
在损失函数中添加打印语句后，
y_pred 为 Tensor(&quot;vae_1/decoder_1/conv2d_transpose_31_1/Sigmoid:0&quot;, shape=(8, 256, 256, 3), dtype=float32)
y_true 的形状：(8, 256, 256, 3)
generated_images 的形状：(256, 256, 3)
z_mean 的形状：(256, 256, 3)
z_log_var 的形状：(256, 256, 3)
我该如何修复此问题？最好不使用 GradientTape() 或类。（我想不使用那些方法）]]></description>
      <guid>https://stackoverflow.com/questions/78991901/tried-changing-the-output-of-vae-model-to-satisfy-loss-function-but-something-do</guid>
      <pubDate>Mon, 16 Sep 2024 21:39:39 GMT</pubDate>
    </item>
    <item>
      <title>如何使用移动窗口训练神经网络？</title>
      <link>https://stackoverflow.com/questions/78991549/how-to-train-a-neural-network-with-a-shifting-window</link>
      <description><![CDATA[我有一个个人数据集，它具有季节性并且随时间而变化。每个季节性都定义为一个过滤周期。
我首先从数据框中提取训练和测试数据，然后对其进行缩放：
#training data
Qp_values = my_df[&#39;Set UF Permeate Flowrate (GPM)&#39;].values 
TMP_values = my_df[&#39;UF-TMP (psi)&#39;].values
backwash_states = my_df[&#39;Backwash (0=Off, 1=On)&#39;].values
elapsed_time = my_df[&#39;Elapsed Time (hr)&#39;].values

scaler = MinMaxScaler()
Qp_values = scaler.fit_transform(Qp_values.reshape(-1, 1)).flatten()
TMP_values = scaler.fit_transform(TMP_values.reshape(-1, 1)).flatten()

#testing数据
Qp_values_test = my_df[&#39;设置 UF 渗透流量 (GPM)&#39;].values
TMP_values_test = my_df[&#39;UF-TMP (psi)&#39;].values
backwash_states_test = my_df[&#39;反冲洗 (0=关闭, 1=开启)&#39;].values

Qp_values_test = scaler.fit_transform(Qp_values_test.reshape(-1, 1)).flatten()
TMP_values_test = scaler.fit_transform(TMP_values_test.reshape(-1, 1)).flatten()

然后我确定了季节性周期的数量，在我的数据中也称为过滤周期。我检查了一下，这是正确的。
#根据反冲洗状态的变化识别过滤循环
def find_contamination_cycles(stats):
cycles = [] #存储的元组列表，包括过滤循环的开始和结束索引
start = 0 #过滤循环的起始索引，设置为 0，因为过滤循环从头开始
in_cycle = True #布尔标志，用于指示过滤阶段是否在循环中，True，因为它是

for i in range(1, len(stats)):
if in_cycle and stats[i] == 1 and stats[i-1] == 0:
cycles.append((start, i))
in_cycle = False
elif not in_cycle and stats[i] == 0 and stats[i-1] == 1:
start = i
in_cycle = True

#处理循环延伸到循环末尾的情况列表
if in_cycle:
cycles.append((start, len(stats)-1))

return cycles

filte_filte_cycles = find_filte_cycles(backwash_states)

然后我定义我的神经网络模型如下：
# 输入：Qp_values (k) 和 TMP(k)
# 输出：TMP(k+1)，其中 k 是数据点 
class TMP_Prediction_Model(nn.Module):
def __init__(self, in_features=2, h1=10, h2=8, h3=8, h4=8, out_features=1):
super().__init__()
self.fc1 = nn.Linear(in_features, h1)
self.fc2 = nn.Linear(h1, h2)
self.fc3 = nn.Linear(h2, h3)
self.fc4 = nn.Linear(h3, h4)
self.out = nn.Linear(h4, out_features)

def forward(self, x):
x = F.relu(self.fc1(x))
x = F.relu(self.fc2(x))
x = F.relu(self.fc3(x))
x = F.relu(self.fc4(x))
x = self.out(x)
return x

torch.manual_seed(41)

# 实例化模型、优化器和损失函数
model = TMP_Prediction_Model()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

现在，如何在 0 到 8 个过滤周期上训练我的神经网络并在下一个周期上进行测试9 到 17 个过滤循环？然后再次重复此步骤，我将训练和测试窗口移动 1？]]></description>
      <guid>https://stackoverflow.com/questions/78991549/how-to-train-a-neural-network-with-a-shifting-window</guid>
      <pubDate>Mon, 16 Sep 2024 19:36:00 GMT</pubDate>
    </item>
    <item>
      <title>将成分/特征分成标有“0”或“1”的单独列</title>
      <link>https://stackoverflow.com/questions/78991472/separate-a-ingredients-feature-into-separate-columns-that-is-marked-with-0-or</link>
      <description><![CDATA[我正在查看一些食物浪费数据，其中包括食物成分等大量数据。我正在尝试对数据进行一些机器学习，但在准备过程中遇到了一些麻烦。
示例
我能够将所有单个成分分离到各自的列中，但当该成分在成分列表/列中时，我很难将其标记为 1。
示例 2
我一直在尝试逐行查看列名是否在成分列中，然后将其更改为 1。我根本没有接近目标，但这是我目前尝试的灾难。还尝试了 get_dummies 和其他一些方法。
有没有更简单的方法？
def xs_os(df, Ingredients_Column):
df2 = df.drop(&quot;Ingredients&quot;, axis = 0) 
for z in df:
for x in list(df2.columns.values):
if x in str(Ingredients_Column):
df.at[z, df[x]] = 1
xs_os(df, df[&#39;Ingredients&#39;])
df.head()
]]></description>
      <guid>https://stackoverflow.com/questions/78991472/separate-a-ingredients-feature-into-separate-columns-that-is-marked-with-0-or</guid>
      <pubDate>Mon, 16 Sep 2024 19:07:49 GMT</pubDate>
    </item>
    <item>
      <title>根据周围站点数据预测某个站点温度的建议和想法[关闭]</title>
      <link>https://stackoverflow.com/questions/78990923/suggestions-ideas-for-predicting-a-certain-stations-temperature-based-on-surro</link>
      <description><![CDATA[我有来自 X 个气象站的数据，每个气象站都有其纬度、经度、海拔高度和跨越数年的每日温度测量值。我的任务是根据周围气象站的数据预测其中一个气象站的温度。
因此，模型将获得所有 X 个气象站的数据，并对其进行自我训练，以期了解地理空间相关性（ASL 和站点之间的距离），然后根据前 X 天周围气象站的测量值随机预测这 20 个气象站中某一天的温度。
此外，通过一次预测一个气象站来训练网络，然后让每个气象站都获得 X 个模型来预测单个“缺失”气象站，还是使用随机选择，这样更好？
关于如何完成这项任务，有什么建议或见解吗？
我研究过一些领域，例如回归模型和 RNN 技术，但没有什么大收获。]]></description>
      <guid>https://stackoverflow.com/questions/78990923/suggestions-ideas-for-predicting-a-certain-stations-temperature-based-on-surro</guid>
      <pubDate>Mon, 16 Sep 2024 15:57:01 GMT</pubDate>
    </item>
    <item>
      <title>保存的检查点中不存在策略目录</title>
      <link>https://stackoverflow.com/questions/78990798/policies-directory-not-present-in-saved-checkpoint</link>
      <description><![CDATA[我正在使用 RayRL Lib，在切换到新 API 版本后，检查点目录不再包含策略文件夹。为什么会发生这种情况？
目前，检查点包含以下目录和文件：
env_runner
learner_group
algorithm_state.pkl
class_and_ctor_args.pkl
rllib_checkpoint.json

如能提供任何帮助，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78990798/policies-directory-not-present-in-saved-checkpoint</guid>
      <pubDate>Mon, 16 Sep 2024 15:18:44 GMT</pubDate>
    </item>
    <item>
      <title>训练 YOLOv8m 姿态估计模型时，第 90 个 Epoch 的 train/kobj_loss 出现意外峰值 [关闭]</title>
      <link>https://stackoverflow.com/questions/78990077/unexpected-spike-in-train-kobj-loss-at-epoch-90-while-training-yolov8m-pose-esti</link>
      <description><![CDATA[我在训练期间观察到 train/kobj_loss（关键点对象损失）中的一种特殊行为。具体来说，在第 90 个时期左右，kobj_loss 突然飙升，从 0.149 增加到 0.196，然后在第 90 个时期后稳定下来并再次下降（请参阅附图以供参考）。
损失图：

我的问题是：

什么可能导致 kobj_loss 突然飙升？
我应该进行哪些典型的调整或调查来解决这个问题？
这种行为在任何情况下都是可以预料到的，例如学习率变化或数据相关因素？

我正在使用 Ultralytics 库来训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78990077/unexpected-spike-in-train-kobj-loss-at-epoch-90-while-training-yolov8m-pose-esti</guid>
      <pubDate>Mon, 16 Sep 2024 12:01:36 GMT</pubDate>
    </item>
    <item>
      <title>使用特定时间之前 30 分钟的历史数据来预测该事件是否以及在哪个时间戳将在接下来的 15 分钟内发生</title>
      <link>https://stackoverflow.com/questions/78989934/using-30-minutes-of-historical-data-before-a-specific-time-to-predict-whether-th</link>
      <description><![CDATA[我有一个如下所示的数据框：
 时间戳 X Y Z V D
90 2003-01-01 02:42:00 9999.99 9999.99 9999.99 99999.9 999.99
91 2003-01-01 02:43:00 9999.99 9999.99 9999.99 99999.9 999.99
92 2003-01-01 02:44:00 9999.99 9999.99 9999.99 99999.9 999.99
93 2003-01-01 02:45:00  9999.99 9999.99 9999.99 99999.9 999.99 94 2003-01-01 02:46:00 9999.99 9999.99 9999.99 99999.9 999.99 95 2003-01-01 02:47:0 0 9999.99 9999.99 9999.99 99999.9 999.99 96 2003-01-01 02:48:00 9999.99 9999.99 9999.99 99999.9 999.99 97 2003-01-01 02:49:00 9999.99 9999.99 9999.99 99999.9 999.99 98 2003-01-01 02:50:00 9999.99 9999.99 9999.99 99999.9 999.99 99 2003-01-0 1 02:51:00 9999.99 9999.99 9999.99 99999.9 999.99 100 2003-01-01 02:52:00 9999.99 9999.99 9999.99 99999.9 999.99 101 2003-01-01 02:53:00 9999.99 9999.99 9999.99 99999.9 999.99 102 2003-01-01 02:54:00 9999.99 9999.99 9999.99 99999.9 999.99 103 2003-01-01 02:55:00 9999.99 9999.99 9999.99 99999.9 999.99 104 2003-01-01 02:56:00 9999.99 9999.99 9999.99 99999.9   999.99 105 2003-01-01 02:57:00 9999.99 9999.99 9999.99 99999.9 999.99 106 2003-01-01 02:58:00 9999.99 9999.99 9999.99 9999 9.9 999.99 107 2003-01-01 02:59:00 9999.99 9999.99 9999.99 99999.9 999.99 108 2003-01-01 03:00:00 -5.25 2.63 -2.78 -405.2     4.62 109 2003-01-01 03:01:00 -4.89 3.54 -2.46 -402.7 4.97 110 2003-01-01 03:02:00 -4.50 4.04 -2.20 -402.7 4.97 111 2003-01-01 03 :03:00 -4.43 3.97 -2.50 -405.0 4.43 112 2003-01-01 03:04:00 -4.40 4.47 -2.01 -407.6 4.56 113 2003-01-01 03:05:00 -4.35 4.42 -2.08   -405.9 4.36 114 2003-01-01 03:06:00 -4.33 4.56 -1.82 -407.4 4.33 115 2003-01-01 03:07:00 -4.52 4.41 -2.05 -407.4 4.33 116 2003-01 -01 03:08:00 -4.48 4.36 -2.04 -402.8 4.04 117 2003-01-01 03:09:00 -4.11 4.63 -2.24 -403.2 4.35 118 2003-01-01 03:10:00 -4.09 4.60    -2.28 -405.3 4.43 119 2003-01-01 03:11:00 -3.91 4.67 -2.37 -404.0 4.46 120 2003-01-01 03:12:00 -2.89 5.67 -1.60 -406.4 4.68 121 20 03-01-01 03:13:00 -3.16 5.77 -1.17 -407.5 4.83 122 2003-01-01 03:14:00 -2.99 5.95 -1.03 -407.4 4.72 123 2003-01-01 03:15:00 -2.86     6.09 -0.82 -407.0 4.49 124 2003-01-01 03:16:00 -1.15 6.07 0.75 -411.3 5.17 125 2003-01-01 03:17:00 0.64 6.06 2.25 -411.3 5.17 1 26 2003-01-01 03:18:00 0.49 6.10 2.42 -421.8 6.41 127 2003-01-01 03:19:00 0.72 6.17 2.06 -422.5 6.42 128 2003-01-01 03:20:00     0.57 6.23 1.86 -425.3 6.46 129 2003-01-01 03:21:00 0.77 6.12 1.97 99999.9 999.99 130 2003-01-01 03:22:00 0.59 6.16 2.10 -424 .8 6.11 131 2003-01-01 03:23:00 0.69 6.22 1.83 -424.8 6.11 132 2003-01-01 03:24:00 0.43 6.32 1.65 99999.9 999.99 133 2003-01-01 03:25:00 0.22 6.12 2.01 -423.0 5.36
134 2003-01-01 03:26:00 0.14 6.10 2.04 -423.0 5.36

我知道 2003-01-01 03:12:00 发生了一些特定的事情，与历史指标中其他列的数据有关。我需要能够使用 03:12:00 之前 30 分钟的数据来判断 03:12:00 之类的事情是否会在 03:12:00 到 03:12:00 之后 15 分钟内发生。
一般来说，我有 100 个事件及其已知时间戳，其中我有 30 分钟前的数据和 15 分钟的数据，我知道其中第一分钟发生了一些事情。
我希望能够使用 ML 来识别在之前的分析集 30 分钟之后的预测集 15 分钟内何时会发生一分钟的独特事件。
我一直在阅读有关 sklearn 的文章，并且我有一个 ML 的初学者介绍。我知道我需要确定目标。我知道 03:11:00 之后 15 分钟区块的时间戳的其他列中的所有数据都可以归类为“是”。如果我们说这些是目标，我该怎么做？一般来说，30 分钟区块之后的所有 15 分钟区块都是“是”。我如何使用 Python 中的非线性 ML 模型来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78989934/using-30-minutes-of-historical-data-before-a-specific-time-to-predict-whether-th</guid>
      <pubDate>Mon, 16 Sep 2024 11:24:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 React 应用程序中离线运行 roboflow 模型？</title>
      <link>https://stackoverflow.com/questions/78989326/how-to-run-roboflow-models-offline-in-a-react-app</link>
      <description><![CDATA[我在 Mac 上本地运行了 Roboflow 推理服务器，但我不确定如何使用 inferencejs 包在本地推理服务器上进行推理，更具体地说是在 Vite 中。
我已经将弃用的（我认为）Roboflowjs 库与 window.roboflow.auth 等一起使用，但我不确定如何向本地推理服务器发出请求。]]></description>
      <guid>https://stackoverflow.com/questions/78989326/how-to-run-roboflow-models-offline-in-a-react-app</guid>
      <pubDate>Mon, 16 Sep 2024 08:08:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytesseract OCR 检测“数字单位”格式的文本，将数字转换为浮点数时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</guid>
      <pubDate>Mon, 16 Sep 2024 07:49:42 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中的符号并将其输出为文本的最佳方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78988645/whats-the-best-way-to-recognize-symbols-in-an-image-and-output-them-as-text</link>
      <description><![CDATA[假设我有一张这样的图像，我想将看到的符号转换为文字（星号、正方形、菱形等），保持正确的顺序，解决这个问题的最佳方法是什么？
我在网上看到过很多使用 OpenCV 识别文本或数字的方法和库，但关于一般符号的并不多]]></description>
      <guid>https://stackoverflow.com/questions/78988645/whats-the-best-way-to-recognize-symbols-in-an-image-and-output-them-as-text</guid>
      <pubDate>Mon, 16 Sep 2024 01:36:47 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试训练人工智能，但使用 rust 和 pytorch 的准确率很低 [关闭]</title>
      <link>https://stackoverflow.com/questions/78988616/im-trying-to-train-an-ai-but-i-have-low-accuracy-using-rust-and-pytorch</link>
      <description><![CDATA[我刚刚开始接触机器学习的世界，我非常喜欢 Rust。我一直在测试和学习更多。在此先感谢你们的支持。
我以迁移训练为例做了一些测试，但我不明白为什么使用相同的验证库，训练中的准确率很高，而测试中的准确率很低。有人能理解为什么吗？我研究过过度拟合，但似乎不是这样，因为我使用的是相同的验证库，没有新数据。
`
use std::env;
use std::error::Error;
use std::path::PathBuf;

use anyhow::{ bail, Result };
use tch::nn::{ self, ModuleT, OptimizerConfig, VarStore };
use tch::vision::{ imagenet, resnet };
use tch::{ Device, Kind, Tensor };
pub fn bee_test() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {
tch::manual_seed(123);
let manifest_dir = env::var(&quot;CARGO_MANIFEST_DIR&quot;)?;
let project_dir = PathBuf::from(manifest_dir);

let dataset_path = project_dir.join(&quot;data/hymenoptera_data&quot;);
let dataset = imagenet::load_from_dir(dataset_path)?;
println!(&quot;{dataset:?}&quot;);

let model_path = project_dir.join(&quot;data/bee.ot&quot;);
println!(&quot;模型路径：{:?}&quot;, model_path);

let device = Device::cuda_if_available();
让 mut vs = VarStore::new(device);
vs.load(model_path.as_path()).map_err(|op| {
format!(&quot;加载模型时出错：{:?}&quot;, op);
op
})?;

让 net = resnet::resnet34_no_final_layer(&amp;vs.root());
让 linear = nn::linear(vs.root(), 512, 2, Default::default());

让 net2: nn::Sequential = nn
::seq()
.add_fn(move |xs| net.forward_t(xs, false))
.add(linear);

让 predict = net2.forward_t(&amp;dataset.test_images, false);
让 probabilities = predict.softmax(-1, tch::Kind::Float);
probabilities.print();

let class = predict.argmax(-1, false);
class.print();

let test_accuracy = predict.accuracy_for_logits(&amp;dataset.test_labels);

println!(&quot;测试准确率：{:.2}%&quot;, 100.0 * f64::try_from(test_accuracy)?);

Ok(())
}

pub fn bee_train() -&gt; Result&lt;()&gt; {
tch::manual_seed(123);

let manifest_dir = env::var(&quot;CARGO_MANIFEST_DIR&quot;)?;
let project_dir = PathBuf::from(manifest_dir);

let dataset_path = project_dir.join(&quot;data/hymenoptera_data&quot;);
let model_path = project_dir.join(&quot;data/resnet34.ot&quot;);

// 加载数据集并将其大小调整为通常的 imagenet 尺寸 224x224。
let dataset = imagenet::load_from_dir(dataset_path)?;
println!(&quot;{dataset:?}&quot;);

// 创建模型并从文件中加载权重。
let mut vs = tch::nn::VarStore::new(tch::Device::Cpu);
let net = resnet::resnet34_no_final_layer(&amp;vs.root());
vs.load(model_path)?;

// 预先计算最终激活。
let train_images = tch::no_grad(|| dataset.train_images.apply_t(&amp;net, false));
let test_images = tch::no_grad(|| dataset.test_images.apply_t(&amp;net, false));
println!(&quot;训练图像形状：{:?}&quot;, train_images.size());
println!(&quot;测试图像形状：{:?}&quot;, test_images.size());

// 初始化线性层和优化器
let linear = nn::linear(vs.root(), 512, dataset.labels, Default::default());
let mut sgd = nn::Sgd::default().build(&amp;vs, 1e-3)?;

for epoch_idx in 1..6000 {
let predict = train_images.apply(&amp;linear);
let loss = predict.cross_entropy_for_logits(&amp;dataset.train_labels);
sgd.backward_step(&amp;loss);

let test_accuracy = test_images.apply(&amp;linear).accuracy_for_logits(&amp;dataset.test_labels);
println!(
&quot;Epoch {}: 训练损失 = {:.4}, 测试准确率 = {:.2}%&quot;,
epoch_idx,
f64::try_from(loss)?,
100.0 * f64::try_from(test_accuracy)?
);
}

let save_model_path = project_dir.join(&quot;data/bee.ot&quot;);
vs.save(save_model_path)?;
Ok(())
}

使用 tch-rs
训练中获得的结果
Epoch 5999：训练损失 = 0.0148，测试准确率 = 96.75%
测试中获得的结果
测试准确率：46.10%
我想了解准确率低的原因，如何改进，途径是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78988616/im-trying-to-train-an-ai-but-i-have-low-accuracy-using-rust-and-pytorch</guid>
      <pubDate>Mon, 16 Sep 2024 01:00:02 GMT</pubDate>
    </item>
    <item>
      <title>如何实现用于预测和逆设计问题的自动编码器？</title>
      <link>https://stackoverflow.com/questions/78565562/how-to-implement-autoencoders-for-prediction-and-inverse-design-problems</link>
      <description><![CDATA[我正在探索使用自动编码器解决两类问题：预测和逆向设计。具体来说，我想要：
a. 设置输入变量并使用自动编码器的训练解码器通过前向传播预测可能的结果。
b. 输入预定义的结果，让编码器建议可能的变量设置，然后使用解码器输出可能的结果。
以下是我的问题：

如何实现自动编码器？我应该在 GitHub 上寻找一个实用的库，还是需要从头开始编写一个？

对于我描述的问题，我应该考虑哪种类型的自动编码器？变分自编码器（VAE）还是条件变分自编码器（CVAE）更合适？


Dired 结果：
模型方案
Dired 正向训练结果
Dired 反向训练结果]]></description>
      <guid>https://stackoverflow.com/questions/78565562/how-to-implement-autoencoders-for-prediction-and-inverse-design-problems</guid>
      <pubDate>Sun, 02 Jun 2024 05:47:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在我的图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将它们的参数减少到嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（具有其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，并将训练好的 CNN 的参数输入到 VAE 模型的编码器输入中，最后将参数向量重建为 CNN 模型参数。
提前致谢！
这是 CNN 模型：
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
self.conv2_drop = nn.Dropout2d()
self.fc1 = nn.Linear(320, 50)
self.fc2 = nn.Linear(50, 10)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
x = x.view(-1, 320)
x = F.relu(self.fc1(x))
x = F.dropout(x, training=self.training)
x = self.fc2(x)
return F.log_softmax(x)

以下代码用于 VAE：
class VAE(nn.Module):
def __init__(self, image_channels=1, h_dim=1024, z_dim=32):
super(VAE, self).__init__()
self.encoder = nn.Sequential(
nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 128, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(128, 256, kernel_size=4, stride=2),
nn.ReLU(),
Flatten()
)

self.fc1 = nn.Linear(h_dim, z_dim)
self.fc2 = nn.Linear(h_dim, z_dim)
self.fc3 = nn.Linear(z_dim, h_dim)

self.decoder = nn.Sequential(
UnFlatten(),
nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
nn.Sigmoid(),
)

def reparameterize(self, mu, logvar):
std = logvar.mul(0.5).exp_()
# return torch.normal(mu, std)
esp = torch.randn(*mu.size())
z = mu + std * esp
返回 z

def bottleneck(self, h):
mu, logvar = self.fc1(h), self.fc2(h)
z = self.reparameterize(mu, logvar)
返回 z, mu, logvar

def encode(self, x):
h = self.encoder(x)
z, mu, logvar = self.bottleneck(h)
返回 z, mu, logvar

def decrypt(self, z):
z = self.fc3(z)
z = self.decoder(z)
返回 z

def forward(self, x):
z, mu, logvar = self.encode(x)
z = self.decode(z)
返回 z, mu, logvar
]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 3 与 pytorch 后端时出现 RuntimeError</title>
      <link>https://stackoverflow.com/questions/78354838/runtimeerror-when-using-keras-3-with-pytorch-backend</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78354838/runtimeerror-when-using-keras-3-with-pytorch-backend</guid>
      <pubDate>Fri, 19 Apr 2024 15:39:03 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中“Flatten”起什么作用？</title>
      <link>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</link>
      <description><![CDATA[我正在尝试了解 Keras 中 Flatten 函数的作用。下面是我的代码，这是一个简单的两层网络。它接收形状为 (3, 2) 的二维数据，并输出形状为 (1, 4) 的一维数据：
model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation(&#39;relu&#39;))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;SGD&#39;)

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape

这会打印出 y 具有形状 (1, 4)。但是，如果我删除 Flatten 行，则它会打印出 y 具有形状 (1, 3, 4)。
我不明白这一点。根据我对神经网络的理解，model.add(Dense(16, input_shape=(3, 2))) 函数正在创建一个隐藏的完全连接层，其中包含 16 个节点。这些节点中的每一个都连接到每个 3x2 输入元素。因此，第一层输出处的 16 个节点已经是“平坦的”。因此，第一层的输出形状应该是 (1, 16)。然后，第二层将其作为输入，并输出形状为 (1, 4) 的数据。
那么，如果第一层的输出已经是“平坦的”并且形状为 (1, 16)，为什么我需要进一步将其平坦化？]]></description>
      <guid>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</guid>
      <pubDate>Wed, 05 Apr 2017 16:48:24 GMT</pubDate>
    </item>
    </channel>
</rss>