<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 31 Jul 2024 18:19:21 GMT</lastBuildDate>
    <item>
      <title>将 MLX 生成的模型转换为 GGUF（通过 ollama 访问）之前和之后的行为不匹配</title>
      <link>https://stackoverflow.com/questions/78817275/the-behavior-missmatch-before-and-after-converting-a-mlx-generated-model-to-gguf</link>
      <description><![CDATA[我们使用 MLX 对模型进行了微调，并成功保存了该模型，请查看此链接了解更多详细信息。
到目前为止，生成的模型使用如下命令运行良好：mlx_lm.generate --model new_model --prompt &quot;tell me sth about sql&quot; --temp 0.01 --ignore-chat-template。
但是，将其转换为 gguf 格式并通过 Ollama 访问后，输出会有所不同，与预期不符。
将其转换为 gguf 的过程如下：
python llama.cpp/convert_hf_to_gguf.py path/new_model --outfile path/new_model.gguf


创建模型文件，内容如下：
FROM ./new_model.gguf
# 将温度设置为 1 [越高越有创意，越低越连贯]
PARAMETER 温度 0.01

使用 Ollama 创建最终工件：
ollama create new_model -f modelfile

使用以下命令启动 ollama ollama run new_model并对其进行评估。
欢迎任何评论，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78817275/the-behavior-missmatch-before-and-after-converting-a-mlx-generated-model-to-gguf</guid>
      <pubDate>Wed, 31 Jul 2024 15:49:03 GMT</pubDate>
    </item>
    <item>
      <title>YoloTinyNet 如何对特定图像进行推理？</title>
      <link>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</link>
      <description><![CDATA[我有一个图像weirdobject.jpg和一个通过带注释的图像训练的模型，以识别该图像中的对象（注释是通过roboflow完成的，并导出到.ckpt文件中）。基本上这就是我使用 YoloTinyNet 对输入图像进行推理的方式：
class_names = [&quot;aeroplane&quot;, &quot;bicycle&quot;, &quot;bird&quot;, &quot;boat&quot;, &quot;bottle&quot;, &quot;bus&quot;, &quot;car&quot;, &quot;cat&quot;, &quot;chair&quot;, &quot;cow&quot;, &quot;diningtable&quot;,
&quot;dog&quot;, &quot;horse&quot;, &quot;motorbike&quot;, &quot;person&quot;, &quot;pottedplant&quot;, &quot;sheep&quot;, &quot;sofa&quot;, &quot;train&quot;, &quot;tvmonitor&quot;,
&quot;small_ball&quot;]
_common_params = {&#39;image_size&#39;: 448, &#39;num_classes&#39;: len(classes_name),
&#39;batch_size&#39;: 16} # 批次大小 1
_net_params = {&#39;cell_size&#39;: 7, &#39;boxes_per_cell&#39;: 2, &#39;weight_decay&#39;: 0.0005}
image = tf.placeholder(tf.float32, (1, 448, 448, 3))
_net =YoloTinyNet(_common_params, _net_params, test=True)
# 图像不应该包含实际的 .jpg 文件吗？或者此时占位符就足够了？
_net .inference(image)
sess = tf.Session()
saver = tf.train.Saver(_net.trainable_collection)
saver.restore(sess, modelFile)
src_img = cv2.imread(&quot;./weirdobject.jpg&quot;)
resized_img = cv2.resize(src_img, (448, 448))
np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)
#我在此阶段仅包含图像数据（在此之前不应该包含吗？）
np_predict = sess.run(object_predicts, feed_dict={image: np_img})

np_predict 结果似乎与我的预期不一致，因为返回的结果包括许多不在图像中的对象类别，而真实对象也包含在结果中。
我的问题是，这是通过固定图像对训练模型进行预测以检测对象的正确方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</guid>
      <pubDate>Wed, 31 Jul 2024 14:15:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow Pipeline 中对大型数据集应用图像增强？</title>
      <link>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</link>
      <description><![CDATA[我有一个图像数据集，每个图像包含一个 1 到 5 个字母的单词。我想使用深度学习对每个图像中组成单词的字符进行分类。这些图像的标签格式如下：
totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar
我正尝试将这些图像加载到 TensorFlow 管道中，以降低由于内存限制而导致的复杂性。下面是我从目录加载和处理图像和标签的代码：
def process_img(file_path):
label = get_label(file_path)
image = tf.io.read_file(file_path)
image = tf.image.decode_png(image, channels=1) 
image = tf.image.convert_image_dtype(image, tf.float32) 
target_shape = [695, 1204]
image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])

# 对标签进行编码
coded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
coded_label.set_shape([5, len(urdu_alphabets)])

return image,coded_label

train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)

train_ds 如下所示：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;
现在，我想对图像应用简单的增强，例如旋转、剪切、侵蚀和扩张。我最初使用了以下函数：
def augment(image, label):
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode=&#39;nearest&#39;, cval=0.0, interpolation_order=1)
image = tf.image.random_zoom(image, [0.85, 0.85])
image = tf.image.random_shear(image, 0.3)
image = tf.image.random_shift(image, 0.1, 0.1)
return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

但是，tf.image 中的许多函数都已弃用。如何以有效的方式在 TensorFlow 管道中将这些增强应用于图像？
注意：我可以通过使用 NumPy 数组加载没有 TensorFlow 管道的图像来执行这些增强，但我的数据集非常大（110 万张图像），因此我需要一种有效的方法来执行此操作。
非常感谢您的帮助。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</guid>
      <pubDate>Wed, 31 Jul 2024 14:11:01 GMT</pubDate>
    </item>
    <item>
      <title>获取矩阵中的方向元素</title>
      <link>https://stackoverflow.com/questions/78816668/get-directional-elements-in-matrix</link>
      <description><![CDATA[假设我的矩阵中有一个 NxN 兴趣点。该点位于位置 ij。那么，给定索引 ij，是否有一种简单的方法可以让线元素通过 ij 到达原点（位于矩阵中间）？
我正在使用 torch，我认为使用 torch.diag 是第一步，但实际上这个函数并没有通过矩阵的中间。
def directionalK(kx,ky, indices):
&#39;&#39;&#39;函数提供由索引指定的给定方向的 K 值&#39;&#39;&#39;
kx_grid,ky_grid = torch.meshgrid(kx,kx, indexing=&#39;ij&#39;)
k_grid = torch.sqrt(kx_grid**2 + ky_grid**2) 
k_grid[...,:len(k_grid)//2] *=-1 
y,x = indices
diag = x - len(k_grid)//2
]]></description>
      <guid>https://stackoverflow.com/questions/78816668/get-directional-elements-in-matrix</guid>
      <pubDate>Wed, 31 Jul 2024 13:35:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用苹果 MLX 框架正确保存微调模型</title>
      <link>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</link>
      <description><![CDATA[我们使用 MLX 来微调从 hugging face 获取的模型。
from transformers import AutoModel
model = AutoModel.from_pretrained(&#39;deepseek-ai/deepseek-coder-6.7b-instruct&#39;)

我们使用 python -m mlx_lm.lora --config lora_config.yaml 等命令对模型进行了微调，配置文件如下所示：
# 本地模型目录或 Hugging Face repo 的路径。
model: &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;
# 训练过的适配器权重的保存/加载路径。
adapter_path: &quot;adapters&quot;

当微调后生成适配器文件时，我们通过类似脚本对模型进行评估
from mlx_lm.utils import *
model,tokenizer = load(path_or_hf_repo =&quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;,
adapter_path = &quot;adapters&quot; # path to new training adaptor
)
text = &quot;Tell sth about New York&quot;
response = generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100)

并且它按预期工作。
但是，在我们保存模型并使用 mlx_lm.generate 进行评估后，模型运行不佳。 （该行为与使用 generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100) 调用模型完全不同。
mlx_lm.fuse --model &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot; --adapter-path &quot;adapters&quot; --save-path new_model
mlx_lm.generate --model new_model --prompt &quot;Tell sth about New York&quot; --adapter-path &quot;adapters&quot; --temp 0.01
]]></description>
      <guid>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</guid>
      <pubDate>Wed, 31 Jul 2024 09:35:34 GMT</pubDate>
    </item>
    <item>
      <title>导入 pywrap_saved_model 时 DLL 加载失败：找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</link>
      <description><![CDATA[我在导入 tflite-model-maker 时遇到问题，
我已经使用 cmd 管理员安装了它，它完全完成了
问题是当我将其导入到我的代码中时
我尝试使用 pip install，完全没有问题，
我不知道，顺便说一下，我使用的是 py 版本 3.8
在此处输入图片说明
在此处输入图片说明
我尝试使用 tensorflow 的 tflite-model-maker 制作音频分类模型
并将其用于我的语音识别项目]]></description>
      <guid>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</guid>
      <pubDate>Wed, 31 Jul 2024 03:52:53 GMT</pubDate>
    </item>
    <item>
      <title>优化序列以最小化涉及求和与序列长度的自定义评分函数</title>
      <link>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</link>
      <description><![CDATA[问题定义
我试图找到一个序列 S，使得对于从 1 到 n 的每个数字 i，使用以下公式计算并最小化分数 f(i)：
f(i) = a * |S| + (1-a) * N(S, i)
其中：

|S| 是序列 S 的长度。
N(S, i) 是 S 中加起来达到目标​​ i 所需的最小元素数。序列 S 中的元素可以重复使用。
a 是介于 0 和 1 之间的参数，用于平衡 |S| 的贡献并将 N(S, i) 添加到分数中。

目标是找到这样一个序列 S，使得从 1 到 n 的每个 i 产生的最大分数 f(i) 尽可能低。
我尝试过的方法
我最初尝试使用蛮力方法来生成可能的序列并对其进行评估，但这种方法计算成本高，并且对于较大的数字（例如 n = 20 及以上）不可行。
问题

是否有更有效的算法或方法来解决这个问题，可能使用动态规划或其他优化技术？
是否有类似于此问题结构的已知问题或数学框架，可以指导解决方案的开发？
你们知道如何解决这个问题吗？

想法
我考虑过使用随机近似来近似解决方案，但我无法执行这个想法。
观察：

我不知道如何证明，但{1,2,3,…,N}的序列可以形成1和（1 + N）N / 2之间的任何数字。因此，如果 a=1，则最佳序列应为 {1,2,3,…,x}，其中 argminx (1+x)x/2 &gt;= target

如果 a=0，(1-a)=1，则 N(S,i) 始终为 1，因为我们可以形成一个序列 {1,2,3,…,T}，这样我们就可以仅使用单个数字本身来表示低于目标的任何数字

根据观察 1 和观察 2，我猜测最佳序列应介于 {1,2,…x} 和 {1,2,…,T} 的组合之间，并且总共会有 T-x+1 次这样的迭代。


]]></description>
      <guid>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</guid>
      <pubDate>Wed, 31 Jul 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在嘈杂的回归数据上产生过度拟合？</title>
      <link>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data</link>
      <description><![CDATA[如何使用 PyTorch 在嘈杂的回归数据上生成过度拟合？尽管进行了各种尝试，但 PyTorch 倾向于泛化数据，这使得记住（过度拟合）数据变得具有挑战性。我修改了权重初始化，使用了 batch_size=1，使学习率变得灵活，并增加了层数和参数数量，但它仍然具有泛化能力。
是否有任何参数需要更改？

def __init__(self):
super().__init__()
inSize = 1
layer = []
for h in hiddenSizes:
layer.append(nn.Linear(inSize, h))
layer.append(nn.ELU()) 
inSize = h 
layer.append(nn.Linear(inSize, 1)) 
self.rede = nn.Sequential(*layers)
#self.apply(self._init_weights)
def _init_weights(self, m):
if isinstance(m, nn.Linear):
#nn.init.uniform_(m.weight, a=-2.0, b=2.0)
#nn.init.kaiming_normal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
#nn.init.xavier_uniform_(m.weight)
nn.init.normal_(m.weight, mean=0.0, std=1.)
if m.bias is not None:
#nn.init.uniform_(m.bias, a=-2.0, b=2.0)
#nn.init.zeros_(m.bias)
nn.init.normal_(m.bias, mean=0.0, std=1.0)
def forward(self, x): 
return self.rede(x)

hiddenSizes = [500, 500 ,250 , 250, 125] # 层和参数

lossFn = nn.MSELoss()
dataloader = DataLoader(dataset, batch_size=1, shuffle=True) # 我尝试了其他值
optimizer = torch.optim.Adam(model.parameters(), lr = .01)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1300, gamma=.9)
# scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=.01, step_size_up=200, mode=&#39;triangular&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data</guid>
      <pubDate>Wed, 31 Jul 2024 02:16:21 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 预测需要很长时间</title>
      <link>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</link>
      <description><![CDATA[我在 sklearn 中的几种常见机器学习方法中遇到了严重的性能问题。我正在研究二元分类问题，数据集包含 500 万个观测值和 100 个特征，使用 sklearn 中的 LogisticRegression()、MLPClassifier()、RandomForestClassifier() 和 LinearSVC() 等模型。
例如，这是我用于 L2 逻辑回归的设置，使用交叉验证从网格 c_grid = [1e-15, 1e-10, 1e-5, 1e-1, 10] 中找到最佳正则化项 C：
lr = LogisticRegression(class_weight=class_weight,
solver=&#39;sag&#39;, # 我还尝试了 &#39;liblinear&#39;
max_iter=10000,
tol=0.1,
random_state=seed,
penalty=&#39;l2&#39;)

C = [1e-15, 1e-10, 1e-5, 1e-1, 10]
c_grid = {&quot;C&quot;: C}
c_grid = {k: v for k, v in c_grid.items() if v is not None}

...

cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True) 
clf = GridSearchCV(estimator=lr, 
param_grid=c_grid, 
scoring=&#39;roc_auc&#39;,
cv=cv, 
return_train_score=True).fit(X_train, Y_train) 
best_model = clf.best_estimator_
prob = clf.predict_proba(X_train)[:, 1]
pred = clf.predict(X_train)

但是，整个训练过程花费了近 20 个小时。对于这种规模的数据集，这是否正常，或者可能是由于参数或设置不正确？例如，我调整了 LogisticRegression 中的各种参数，但似乎都没有改善这种情况。
此外，当我尝试使用 best_model 来计算测试结果时
prob = clf.predict_proba(X_test)[:, 1]
pred = clf.predict(X_test)

这似乎需要很长时间才能完成。我尝试使用类似这样的方法并行化该过程
X_test_batches = np.array_split(X_test, N)
args = [(best_model, batch) for batch in X_test_batches]

with Pool(N) as pool:
prob_batches = pool.map(predict_batch, args)
prob = np.concatenate(prob_batches)
pred = (prob &gt;= 0.5)

但它也没有太大帮助，所以最终我不得不手动实现我自己的预测函数（显然它只适用于逻辑回归，但不适用于我想要测试的其他模型）：
z = np.dot(X_test, best_model.coef_.T) + best_model.intercept_
prob = 1 / (1 + np.exp(-z))

鉴于训练和测试都花费了不合理的长时间，我猜测问题可能出在 clf.predict_proba() 和 clf.predict() 上。不过，我希望 sklearn 能够有效处理包含数百万个观测值的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</guid>
      <pubDate>Wed, 31 Jul 2024 00:04:22 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Python 脚本在后台运行同时仍与前端交互？[关闭]</title>
      <link>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</link>
      <description><![CDATA[我有这个网站，它允许人们添加新的预测。首先，他们点击一个新的预测和广告数据（csvs），然后选择他们想要使用的机器学习模型，然后它应该运行python代码并将csv输出到具有相关预测参数的数据库。
我的问题是我应该如何运行python代码，因为这个代码可能需要几个小时才能运行，然后我必须考虑许多试图做出新预测的用户。我研究过任务调度和redis。我也听说我可以使用一些AWS服务，但我不确定这里最好的选择是什么，因为我想尽可能地防止内存错误和超时，同时确保代码在稳定的环境中运行。
顺便说一下，我使用flask作为后端并在前端做出反应]]></description>
      <guid>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</guid>
      <pubDate>Tue, 30 Jul 2024 21:37:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 opencv 检测该焊接带中的缺陷（例如孔洞）[关闭]</title>
      <link>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</link>
      <description><![CDATA[参考图：带孔的焊条：https://i.sstatic.net/VaVQX3th.jpg
正常焊条：https://i.sstatic.net/MBcyyIyp.jpg
我在使用 opencv 检测缺陷（如孔、不均匀性）时遇到问题。我是 opencv 新手，尝试过轮廓检测、边缘检测，但没有得到想要的结果。我想使用 opencv 构建一个算法，检测这些孔并标记它们，而不标记任何其他不必要的东西，这些东西不是缺陷。
这是我在代码中使用的方法
import cv2
import numpy as np
from matplotlib import pyplot as plt

# 加载图像
image_path = &quot;sample weld strip.jpg&quot;
image = cv2.imread(image_path)

# 将图像转换为 HSV 颜色空间
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# 定义焊缝条的 HSV 范围
lower_hsv = (1, 1, 1)
upper_hsv = (177, 255, 255)

# 应用 HSV 掩码
mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)
masked_image = cv2.bitwise_and(image, image, mask=mask)

# 转换为灰度
gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)

# 应用高斯模糊
blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)

# 使用 Canny 进行边缘检测
edges = cv2.Canny(blurred_image, 50, 150)

# 查找轮廓
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 在重要轮廓（孔洞）周围绘制边界框
holes_image = image.copy()
for contour in contours:
area = cv2.contourArea(contour)
if 10 &lt;area &lt; 25：# 根据您的需要调整此阈值
x, y, w, h = cv2.boundingRect(contour)
cv2.rectangle(holes_image, (x, y), (x+w, y+h), (0, 0, 255), 2)

我正在寻求有关此问题的帮助或指导。]]></description>
      <guid>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</guid>
      <pubDate>Tue, 30 Jul 2024 20:06:30 GMT</pubDate>
    </item>
    <item>
      <title>如何进行布尔分类处理？</title>
      <link>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler、OneHotEncoder、OrdinalEncoder
从 sklearn.pipeline 导入 Pipeline

data = pd.read_csv(&#39;Datasets/StudentScore.csv&#39;)

target = &#39;MathScore&#39;
x = data.drop(data[[target, &#39;Unnamed: 0&#39;]], axis=1)
y = data[target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# 数值处理
num_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.fit_transform(x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])
x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.transform(x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])

# 序数处理
education_levels = [&quot;high school&quot;, &quot;some high school&quot;, &quot;some college&quot;, &quot;associate&#39;s degree&quot;, &quot;bachelor&#39;s degree&quot;,
&quot;master&#39;s degree&quot;]

ord_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OrdinalEncoder(categories=[education_levels])),
])

x_train[[&#39;ParentEduc&#39;]] = ord_transformer.fit_transform(x_train[[&#39;ParentEduc&#39;]])
x_test[[&#39;ParentEduc&#39;]] = ord_transformer.transform(x_test[[&#39;ParentEduc&#39;]])

# 名义处理
nom_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder())
])

x_train[[&#39;EthnicGroup&#39;]] = nom_transformer.fit_transform(x_train[[&#39;EthnicGroup&#39;]])
x_test[[&#39;EthnicGroup&#39;]] = nom_transformer.transform(x_test[[&#39;EthnicGroup&#39;]])

# 布尔处理
bool_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder(sparse_output=False)),
])

x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.fit_transform(
x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])
x_test[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.transform(x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])

我在尝试创建管道来处理布尔分类特征时遇到错误。具体来说，在训练集和测试集中的特征的 fit_transform 步骤中，我在名义处理和布尔处理部分中收到了
ValueError：列的长度必须与键的长度相同

。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</guid>
      <pubDate>Tue, 30 Jul 2024 19:12:18 GMT</pubDate>
    </item>
    <item>
      <title>Catboost 特征重要性计算</title>
      <link>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</link>
      <description><![CDATA[我仅用 3 棵树拟合了一个简单二分类模型，并想检查特征重要性结果是否与 Catboost 文档 (PredictionValuesChange) 中的公式相似。
训练模型后，我按照CatBoost JSON 模型教程中的步骤操作，并得到了以下树结构：
{
&quot;leaf_values&quot;: [
-0.13915912880676032,
0.1097787155963716
],
&quot;leaf_weights&quot;: [
2143.0251545906067,
2252.974784851074
],
&quot;splits&quot;: [
{
&quot;border&quot;: 3.5,
&quot;float_feature_index&quot;: 13,
&quot;split_index&quot;: 0,
&quot;split_type&quot;: &quot;FloatFeature&quot;
}
]
} 

模型中的每棵树只有深度 = 1，并且只有一棵树（索引 = 1）具有感兴趣的特征。我决定根据上述公式手动计算特征重要性，并将结果与​​ .get_feature_importance 方法进行比较。结果大不相同：

特征重要性：28.2947825
手动计算：68.06248029261762

以下是用于特征重要性计算的代码：
tree_indx = 1
v_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][0]
v_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][1]

c_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][0]
c_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][1]

avr = (v_1*c_1 + v_2*c_2)/(c_1+c_2)

fi = ((v_1 - avr)**2)*c_1 + ((v_2 - avr)**2)*c_2
print(fi)

我犯了错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</guid>
      <pubDate>Mon, 29 Jul 2024 15:29:02 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。

问题是前 50/60 次推理的性能非常好（比如 20 毫秒），然后它们开始变差，直到时间稳定在每张图像 70/75 毫秒左右。我不明白为什么性能会以这种方式变差。

环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
CPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

GPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型而出现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器的 SHAP 解释中 expected_value 的计算</title>
      <link>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</link>
      <description><![CDATA[我们如何理解SHAP explainer.expected_value？为什么经过sigmoid变换后，它与y_train.mean()不一样？
下面是代码摘要，供快速参考。完整代码可在此笔记本中找到：https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)
explainer = shap.Explainer(model)
shap_test = explainer(X_test)
shap_df = pd.DataFrame(shap_test.values)

#对于每种情况，如果我们将所有特征的 shap 值加上预期值相加，我们就可以得到该情况的边际，然后可以将其转换为返回该情况的预测概率case:
np.isclose(model.predict(X_test, output_margin=True),explainer.expected_value + shap_df.sum(axis=1))
#True

但是为什么下面不成立？为什么经过 sigmoid 变换后，XGBoost 分类器的 explainer.expected_value 与 y_train.mean() 不一样？
expit(explainer.expected_value) == y_train.mean()
#False
]]></description>
      <guid>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</guid>
      <pubDate>Mon, 18 Sep 2023 09:28:55 GMT</pubDate>
    </item>
    </channel>
</rss>