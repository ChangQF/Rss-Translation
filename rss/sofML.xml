<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Apr 2024 03:16:00 GMT</lastBuildDate>
    <item>
      <title>用于大规模服务器管理的人工智能自动化</title>
      <link>https://stackoverflow.com/questions/78301638/artificial-intelligence-enabled-automation-for-large-scale-server-management</link>
      <description><![CDATA[简介：在当今世界，有效管理企业的服务器基础设施可能既复杂又耗时。跟踪和管理数千台服务器在不同团队中的分布情况以及负责人通常会给管理员带来一项具有挑战性的任务。然而，人工智能 (AI) 和机器学习 (ML) 技术的发展为克服这些挑战提供了新的创新解决方案。
人工智能服务器管理：人工智能服务器管理可以作为有效管理大规模服务器基础设施的强大工具。这种方法涉及利用人工智能模型来确定哪些团队使用服务器以及谁负责它们。例如，可以创建包含服务器的 IP 地址、位置、功能和其他相关详细信息等信息的数据集。
通过机器学习自动化服务器部署：机器学习算法可以利用此数据集来预测哪些团队使用服务器。例如，可以开发基于特定服务器特征的分类模型。利用该模型可以预测服务器所属的团队，为管理员优化服务器部署提供指导。
安全和数据隐私：在支持人工智能的服务器管理应用程序中，安全和数据隐私是至关重要的问题。因此，必须实施安全措施来保护用户身份和密码等敏感信息。例如，服务器访问信息应安全存储，并且只有授权用户才能访问。
结论和建议：支持人工智能的服务器管理可以成为有效管理大规模服务器基础设施的有效工具。机器学习模型的利用可以在确定使用服务器的团队和优化服务器部署方面发挥重要作用。但是，始终考虑安全和数据隐私问题并实施适当的安全措施至关重要。
此时，在人工智能服务器管理领域需要进一步的研究和开发。然而，利用现有技术和方法，高效、安全地管理大规模服务器基础设施是可以实现的。
**综上所述，考虑到上述几点，我如何通过人工智能确定我的服务器（Red Hat、Solaris、Ubuntu、Debian 等）分配给哪个团队？注意：所有服务器都有可用的 root 用户和密码信息，我可以通过 SSH 连接。 **]]></description>
      <guid>https://stackoverflow.com/questions/78301638/artificial-intelligence-enabled-automation-for-large-scale-server-management</guid>
      <pubDate>Wed, 10 Apr 2024 00:21:07 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：“模型”层的输入 0 与该层不兼容：预期形状=（无，64，32，1），发现形状=（32，32，1）</title>
      <link>https://stackoverflow.com/questions/78301623/valueerror-input-0-of-layer-model-is-incompatible-with-the-layer-expected-sh</link>
      <description><![CDATA[我尝试调试并添加了打印语句，令我惊讶的是它的形状是正确的，但程序说它的形状不正确
形状：（无、64、32、1）
型号：“型号”
&lt;小时/&gt;
层（类型）输出形状参数#
图像（输入层）[(无, 64, 32, 1)] 0
Conv1（Conv2D）（无、64、32、32）320
pool1 (MaxPooling2D)（无、32、16、32）0
batch_normalization (BatchN (无, 32, 16, 32) 128
正规化）
重塑（重塑）（无、32、512）0
dense2（密集）（无、32、16）8208
batch_normalization_1（Batc（无、32、16）64
h归一化）
双向（Bidirectiona（无、32、256）148480
l)
dense3（密集）（无、32、42）10794
================================================== =================
总参数：167,994
可训练参数：167,898
不可训练参数：96
&lt;小时/&gt;
无
输入形状：(64,32,1)
回溯（最近一次调用最后一次）：
文件“D:\Arabic-Handwriting-OCR\Arabic-Handwriting-OCR\inference.py”，第 156 行，位于
preds = Prediction_model.predict(batch_images)
文件“C:\Users\User\miniconda3\envs\tf\lib\site-packages\keras\utils\traceback_utils.py”，第 70 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件“C:\Users\User\AppData\Local\Temp_autograph_ generated_file42woagrz.py”，第 15 行，位于 tf__predict_function 中
retval = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
ValueError：在用户代码中：ValueError：层“模型”的输入 0与图层不兼容：预期形状=(无, 64, 32, 1)，发现形状=(32, 32, 1)
我尝试检查模型输入的形状，它的字面意思是 (64, 32, 1)，但回溯显示找到的形状=(32,32,1)，为什么呢？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78301623/valueerror-input-0-of-layer-model-is-incompatible-with-the-layer-expected-sh</guid>
      <pubDate>Wed, 10 Apr 2024 00:15:22 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow EMNIST 预测</title>
      <link>https://stackoverflow.com/questions/78300818/tensorflow-emnist-predictions</link>
      <description><![CDATA[我是张量流新手，目前已经制作了 eMNIST 字母数据集的模型，我想用它来预测笔迹。目前，我为模型指定字母“a”的小写版本。当我执行我提供的代码行时，我收到多个没有多大意义的输出数组。这些数组都有 27 个索引，而 emnist 字母应该只包含 26 个，对吗？
以下是图片：
我用来测试给定图像（jpg 文件）的代码
输出预测数组
我猜测这些索引与给定的字母相对应，但是我为模型指定的字母“a”应该位于第一个索引之一中。
不知道从这里去哪里。我很好奇这些指数的含义。
感谢您提前提供的任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78300818/tensorflow-emnist-predictions</guid>
      <pubDate>Tue, 09 Apr 2024 19:53:52 GMT</pubDate>
    </item>
    <item>
      <title>尽管已安装，脚本仍不断请求安装face_recognition_models</title>
      <link>https://stackoverflow.com/questions/78300706/script-continuously-requests-face-recognition-models-installation-despite-being</link>
      <description><![CDATA[我在使用 Face_recognition 库的 Python 脚本中遇到问题。尽管已经成功安装了face_recognition_models包，但当我尝试执行脚本时，脚本反复提示我安装它。这是我收到的命令和输出：
说明问题的图片
代码：
导入操作系统
导入人脸识别
导入人脸识别模型
导入CV2


image_path = “tes.png”;

# 检查图片文件是否存在
如果不是 os.path.isfile(image_path):
    print(&quot;图像文件不存在:&quot;, image_path)
    出口（）

# 获取网络摄像头 #0 的引用（默认摄像头）
video_capture = cv2.VideoCapture(0)

# 加载您的图像并学习如何识别它。
图像=face_recognition.load_image_file(image_path)
face_encoding =face_recognition.face_encodings(图像)[0]

# 创建已知面部编码及其名称的数组
已知人脸编码 = [
    面部编码，
]
已知面孔名称 = [
        “瓦利德”
]

而真实：
        # 抓取单帧视频
        ret, 帧 = video_capture.read()

        # 将图像从 BGR 颜色（OpenCV 使用）转换为 RGB 颜色（face_recognition 使用）
        rgb_frame = 帧[:, :, ::-1]

        # 查找当前帧视频中的所有人脸
        面部位置 = 面部识别.面部位置(rgb_frame)
        face_encodings =face_recognition.face_encodings（rgb_frame，face_locations）

        # 循环遍历该视频帧中的每张脸
        对于（上，右，下，左），zip中的face_encoding（face_locations，face_encodings）：
            # 查看该面孔是否与已知面孔匹配
            匹配=face_recognition.compare_faces（known_face_encodings，face_encoding）

            名称=“未知”

            如果匹配中为真：
                first_match_index = matches.index(True)
                名称 =known_face_names[first_match_index]

            # 在脸部周围画一个框
            cv2.rectangle(frame, (左, 上), (右, 下), (0, 0, 255), 2)

            # 在脸部下方画一个带有名字的标签
            cv2.rectangle(frame, (左, 下 - 35), (右, 下), (0, 0, 255), cv2.FILLED)
            字体= cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(框架, 名称, (左 + 6, 下 - 6), 字体, 1.0, (255, 255, 255), 1)

        # 显示结果图像
        cv2.imshow(&#39;视频&#39;, 帧)

        # 按键盘上的“q”退出！
        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

# 释放网络摄像头的句柄
video_capture.release()
cv2.destroyAllWindows()


我尝试利用face_recognition 库执行Python 脚本。尽管成功安装了face_recognition_models包，但该脚本在执行时不断提示我安装它。我希望脚本能够识别已安装的包并执行而不会出现错误，但它继续请求安装face_recognition_models。]]></description>
      <guid>https://stackoverflow.com/questions/78300706/script-continuously-requests-face-recognition-models-installation-despite-being</guid>
      <pubDate>Tue, 09 Apr 2024 19:29:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 keras 和张量流实现双任务分类实验</title>
      <link>https://stackoverflow.com/questions/78300510/how-do-i-implement-a-dual-task-classification-experiment-using-keras-and-tensor</link>
      <description><![CDATA[我正在谈论以下代码（为书籍 NLP 迁移学习).
input1_shape = (len(train_x[0]),)
input2_shape = (len(train_x2[0]),)
sent2vec_vectors1 = 输入（形状=input1_shape）
sent2vec_vectors2 = 输入（形状=input2_shape）
组合 = 连接（[sent2vec_vectors1，sent2vec_vectors2]）
密集1 = 密集（512，激活=&#39;relu&#39;）（组合）
密集1 = Dropout(0.3)(密集1)
输出1 = 密集（1，激活=&#39;sigmoid&#39;，名称=&#39;分类1&#39;）（密集1）
输出2 = 密集（1，激活=&#39;sigmoid&#39;，名称=&#39;分类2&#39;）（密集1）
模型 = 模型(输入=[sent2vec_vectors1,sent2vec_vectors2], 输出=[输出1,输出2])

model.compile(loss={&#39;classification1&#39;: &#39;binary_crossentropy&#39;,
                    &#39;classification2&#39;: &#39;binary_crossentropy&#39;},
              优化器=&#39;亚当&#39;，指标=[&#39;准确性&#39;]）
历史 = model.fit([train_x,train_x2],[train_y,train_y2],
                    验证数据=([test_x,test_x2],[test_y,test_y2]),
                                     batch_size = 32，nb_epoch = 10，shuffle = True）

上面的代码适用于旧版本的张量流：&lt; 2.0
我一直在尝试让它在 TensorFlow 版本：2.16.1 上运行，并进行了以下更改
input1_shape = (len(train_x[0]),)
input2_shape = (len(train_x2[0]),)
sent2vec_vectors1 = 输入（形状=input1_shape，名称=“向量1”）
sent2vec_vectors2 = 输入（形状=input2_shape，名称=“向量2”）

类 ConcatenateLayer（层）：
    def 调用（自身，输入，轴=0）：
        返回 tf.concat(输入，轴=轴)
    


组合 = ConcatenateLayer()([sent2vec_vectors1,sent2vec_vectors2],axis=0)
密集1 = 密集（512，激活=&#39;relu&#39;）（组合）
密集1 = Dropout(0.3)(密集1)
输出1 = 密集（1，激活=&#39;sigmoid&#39;，名称=&#39;分类1&#39;）（密集1）
输出2 = 密集（1，激活=&#39;sigmoid&#39;，名称=&#39;分类2&#39;）（密集1）

模型 = 模型(输入=[sent2vec_vectors1,sent2vec_vectors2], 输出=[输出1,输出2])
model.compile(loss={&#39;classification1&#39;: &#39;binary_crossentropy&#39;,
                    &#39;classification2&#39;: &#39;binary_crossentropy&#39;},
              优化器=&#39;亚当&#39;，指标=[&#39;准确性&#39;，&#39;准确性&#39;]）

历史 = model.fit([train_x, train_x2], [train_y, train_y2],
                    验证数据=([test_x, test_x2], [test_y, test_y2]),
                     纪元 = 10，洗牌 = True
                    ）

我不断收到不兼容的形状错误，例如：
不兼容的形状：[64,1] 与 [32,1]
     [[{{节点gradient_tape/compile_loss/binary_crossentropy_1/logistic_loss/mul/BroadcastGradientArgs}}]] [操作：__inference_one_step_on_iterator_8871]


我在拟合函数中尝试了不同的批量大小，并尝试重塑数据。我试图将错误跟踪到张量流代码中，但一切都无济于事。
数据的形状如下：

对象名称：train_x 形状：形状：(1400,600)
对象名称：train_x2 形状：形状：(1400,600)
对象名称：train_y 形状：形状：(1400,)
对象名称：train_y2 形状：形状：(1400,)
对象名称：test_x 形状：形状：(600,600)
对象名称：test_x2 形状：形状：(600,600)
对象名称：test_y 形状：形状：(600,)
对象名称：test_y2 形状：形状：(600,)

还有
输入形状：

sent2vec_vectors1：（无，600）
sent2vec_vectors2：（无，600）

输出形状：

输出1：（无，1）
输出2：（无，1）

我觉得我错过了一些东西。我正在努力寻找 2.16.1 中类似方法的良好参考示例。任何建议。
我尝试过重塑数据并尝试了一系列批量大小，但我对张量流非常陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78300510/how-do-i-implement-a-dual-task-classification-experiment-using-keras-and-tensor</guid>
      <pubDate>Tue, 09 Apr 2024 18:46:59 GMT</pubDate>
    </item>
    <item>
      <title>我可以将回归值和分类值合并到时间序列模型中吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78300450/can-i-incorporate-both-regression-and-classification-values-in-time-series-model</link>
      <description><![CDATA[我可以将回归值和分类值作为特征结合起来，使用 ARIMA、SARIMAX 和 Prophet 等算法开发风速预测模型，其中预测输出（风速）位于分类值中吗？]]></description>
      <guid>https://stackoverflow.com/questions/78300450/can-i-incorporate-both-regression-and-classification-values-in-time-series-model</guid>
      <pubDate>Tue, 09 Apr 2024 18:29:16 GMT</pubDate>
    </item>
    <item>
      <title>努力开发使用带有 ARFRegressor 算法的 River 库的在线学习代码</title>
      <link>https://stackoverflow.com/questions/78298486/struggling-with-developing-code-for-an-online-learning-using-river-library-with</link>
      <description><![CDATA[我已经分割了数据，但正在努力编写用于训练模型的代码，并使用 pickle 将模型保存在特定目录中以供将来使用。关于如何继续的任何建议？
# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

从河流进口评估
从河流进口森林
来自河流的进口指标
来自河流的进口预处理

型号=（
    预处理.StandardScaler() |
    森林.ARFRegressor(种子=42)
）
指标 = 指标.MAE()

# 在测试集上评估模型
mae = evaluate.progressive_val_score(X_test, y_test, 模型, 指标)
print(f&#39;测试集上的 MAE: {mae}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78298486/struggling-with-developing-code-for-an-online-learning-using-river-library-with</guid>
      <pubDate>Tue, 09 Apr 2024 12:35:37 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证了模型训练正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>标签未包含在我的张量数据集中</title>
      <link>https://stackoverflow.com/questions/78297824/label-not-included-inside-my-tensor-dataset</link>
      <description><![CDATA[我是机器学习新手，我想使用 BERT 模型中的预训练模型。
我面临以下问题：标签输出未插入张量类型数据集。
有没有人有解决办法？
from sklearn.model_selection import train_test_split
X = 特征[&#39;clean_text&#39;]
y = 特征[&#39;标签&#39;]
X_train、X_test、y_train、y_test=train_test_split(X、y、test_size = 0.3、random_state = 42)
X_train = tokenizer(X_train.tolist(), 填充 = True, 截断 = True)
X_test = tokenizer(X_test.tolist(), 填充 = True, 截断 = True)
X_train = 字典(X_train)
X_test = 字典(X_test)
y_train = y_train.tolist()
y_test = y_test.tolist()
df_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))
df_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))
输入，输出=下一个（iter（df_train））
打印出）

输出如下：tf.Tensor(0, shape=(), dtype=int32)]]></description>
      <guid>https://stackoverflow.com/questions/78297824/label-not-included-inside-my-tensor-dataset</guid>
      <pubDate>Tue, 09 Apr 2024 10:36:28 GMT</pubDate>
    </item>
    <item>
      <title>识别和清理数据集中有问题的三元组[关闭]</title>
      <link>https://stackoverflow.com/questions/78297420/identifying-and-cleaning-problematic-triplets-in-a-dataset</link>
      <description><![CDATA[我有三元组（嵌入、嵌入、相关标志）我有大约 5k 个这样的三元组。
有一些三元组（一些少量）实际上并不相关，但在我的数据中显示为相关。
嵌入维度为512。
什么是不相关我认为没有问题。
有什么想法可以找到那些被怀疑是错误的有问题的三元组，可以手动检查和清理。
我尝试构建分类器，但它们的性能不太好。]]></description>
      <guid>https://stackoverflow.com/questions/78297420/identifying-and-cleaning-problematic-triplets-in-a-dataset</guid>
      <pubDate>Tue, 09 Apr 2024 09:23:02 GMT</pubDate>
    </item>
    <item>
      <title>如何计算二元分类概率[关闭]</title>
      <link>https://stackoverflow.com/questions/78296900/how-to-calculate-binary-classification-probabilites</link>
      <description><![CDATA[我正在研究一些基于数值特征的二元分类问题，例如预测维护、信用卡欺诈、心脏病等。我通常喜欢使用随机森林，因为它用途广泛、稳健且可以获得高指标。
除了预测1或0之外，我还想预测获得1的概率（在0.00到1.00之间浮动）。如何在代码中实现这一点？
我使用了随机森林分类器的predict_proba()方法。然而，它主要产生极值（0.00 - 0.10 和 0.90 - 1.00）。 也许它没有很好地校准？另外，我使用了SVM分类器的decision_function()方法，但SVM似乎不是很通用。因此我正在寻找一种不同的方法。
我更喜欢与 RF 分类器相关的方法，但我对其他方法持开放态度。
这是我的代码的相关部分：
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

校准器 = CaliberatedClassifierCV(rf, cv=&#39;prefit&#39;)
模型 = calibrator.fit(X_train, y_train)

概率 = model.predict_proba(X_test)

y_pred = model.predict(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78296900/how-to-calculate-binary-classification-probabilites</guid>
      <pubDate>Tue, 09 Apr 2024 07:56:47 GMT</pubDate>
    </item>
    <item>
      <title>CNN 教程模型根本拒绝训练</title>
      <link>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</link>
      <description><![CDATA[我遇到的问题涉及在一些众所周知的数据集上创建和使用 CNN 模型。问题始于我的一项家庭作业，我们应该创建一个 CNN 并在 CIFAR10 数据集上运行它。然而这个问题似乎更加严重，我怀疑可能出了什么问题。
我注意到，在运行我自己的模型时，无论我制作什么模型或调整什么超参数，性能都与随机猜测一致。
为了更好地了解什么是好的模型，我在 CIFAR10 和 MNIST 上在线下载了一些教程。教程页面、网站等上的这些模型都报告了不错的准确度，范围在 60-80% 之间。
然而事情就变得奇怪了。当我使用相同的数据预处理等运行这些完全相同的模型（我下载了文件，因此没有复制/粘贴错误）时，我得到了与我自己的模型相同的结果，没有学习发生，并且准确性保持在 10%。这种行为与我在网上找到的至少四个不同的教程示例是一致的。
我尝试创建一个新的 conda 环境，以便我可以全新安装 tensorflow-gpu，并确保尽可能使用与教程相同的版本，但无论出于何种原因，模型似乎拒绝训练为我。下面是一个最小的可重现示例，我只是从 tensorflow.org 上的 CNN 教程示例和我的结果中复制/粘贴了该示例。
从tensorflow.keras导入数据集、图层、模型

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# 将像素值标准化为 0 到 1 之间
训练图像，测试图像 = 训练图像 / 255.0，测试图像 / 255.0


模型 = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), 激活=&#39;relu&#39;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))


model.add(layers.Flatten())
model.add(layers.Dense(64,activation=&#39;relu&#39;))
model.add(layers.Dense(10))

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    指标=[&#39;准确性&#39;])

历史= model.fit(train_images, train_labels, epochs=10,
                    验证数据=（测试图像，测试标签））


结果
1563/1563 [================================] - 250s 11ms/步 - 损耗：2.3027 - 准确度：0.0977 - val_loss：2.3026 - val_accuracy：0.1000
纪元 2/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 精度：0.0999 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 3/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.1011 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 4/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0993 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 5/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0957 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 6/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0979 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 7/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0997 - val_loss ：2.3026 - val_accuracy：0.1000
]]></description>
      <guid>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</guid>
      <pubDate>Mon, 08 Apr 2024 13:58:28 GMT</pubDate>
    </item>
    <item>
      <title>如何创建 CNN-LSTM 架构？</title>
      <link>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</link>
      <description><![CDATA[我尝试创建混合 CNN 和 LSTM 模型。我遇到了与架构形状相关的问题。这导致epoch无法跑完数据200次。
我的数据大小是（96,2）
错误：
纪元 1/200
    178/未知 9s 34ms/步 - 损耗：1.2366 - mse：5.4560
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
第 4 行 [40] 中的单元格
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs = 200，batch_size = 128）

无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[78,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_23678]

CNN-LSTM模型：
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn


编译模型
def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()

model_create.fit（train_dataset，epochs = 200，batch_size = 128）


我曾尝试在 flatten() 函数之前添加 reshape 来改变形状。我还减小了批量大小和纪元大小。这些都不起作用。如何将我的模型与 train_data 相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 16:34:57 GMT</pubDate>
    </item>
    <item>
      <title>我如何提取边界框的 x 和 y 坐标</title>
      <link>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</link>
      <description><![CDATA[我的代码定义了一个 ROS 节点，用于使用 YOLO 算法在图像中进行对象检测，特别是使用从指定文件路径加载的 YOLO 模型。该节点订阅了广播图像的 ROS 主题 (/camera/color/image_raw)。收到图像后，会触发回调函数，使用 CvBridge 将 ROS 图像消息转换为 OpenCV 图像格式。然后该图像由 YOLO 模型处理以检测对象。结果（包括检测到的对象框）将打印到控制台。为了防止连续处理和重新处理图像，在处理第一个图像后设置一个标志（image_processed）。此外，处理后的图像将保存到指定的文件夹（临时）。该节点保持活动状态，侦听新图像，直到手动关闭。
#!/usr/bin/python3
导入罗斯比
从sensor_msgs.msg导入图像
从 cv_bridge 导入 CvBridge
导入CV2
导入操作系统
从 ultralytics 导入 YOLO

类节点（对象）：
    def __init__(自身):
        self.yolo_model = YOLO(&#39;/home/user/catkin_ws/src/run_folder/content/runs/obb/train/weights/best.pt&#39;)
        self.br = CvBridge()
        self.image_processed = False # 指示图像是否已处理的标志

        # 订阅发布图像的ROS主题
        rospy.Subscriber(“/camera/color/image_raw”, Image, self.callback)

    def 回调（自身，消息）：
        if not self.image_processed: # 仅当尚未处理图像时才处理
            image = self.br.imgmsg_to_cv2(msg) # 将ROS图像消息转换为OpenCV图像

            # 使用 YOLO 进行物体检测
            结果= self.yolo_model.predict（图像，显示= True）
            对于结果 [0] 中的 r：
                打印（“---------------------------”）
                打印（r.boxes）

            # 第一次处理后保存图像
            self.save_image(图像, &#39;临时&#39;)
            self.image_processed = True # 将标志设置为 True 以避免重新处理

    def save_image(自身, 图像, 文件夹名称):
        如果不是 os.path.exists(folder_name):
            os.makedirs(文件夹名称)
        
        file_path = os.path.join(folder_name, &#39;image.jpg&#39;)
        cv2.imwrite（文件路径，图像）
        print(f“图像保存在{file_path}”)

如果 __name__ == &#39;__main__&#39;:
    rospy.init_node（“image_processor_with_yolo”，匿名= True）
    节点 = 节点()
    rospy.spin() # 保持节点运行直到关闭


当我尝试提取边界框时，我得到“无”在输出中。]]></description>
      <guid>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</guid>
      <pubDate>Wed, 03 Apr 2024 10:11:55 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow 和 keras：不支持 None 值</title>
      <link>https://stackoverflow.com/questions/59672402/tensorflow-and-keras-none-values-not-supported</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59672402/tensorflow-and-keras-none-values-not-supported</guid>
      <pubDate>Thu, 09 Jan 2020 21:31:09 GMT</pubDate>
    </item>
    </channel>
</rss>