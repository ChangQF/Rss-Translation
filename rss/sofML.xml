<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 01 Aug 2024 06:22:10 GMT</lastBuildDate>
    <item>
      <title>面部识别 - 机器学习</title>
      <link>https://stackoverflow.com/questions/78818666/facial-recognition-machine-learning</link>
      <description><![CDATA[我尝试在 Google Colab 上使用 Tensorflow 进行面部识别，但遇到了错误。之前运行正常，但现在却出现此错误。完整的 .ipynb 文件已链接（请注意，您需要一个包含 .jpg 文件的负、正和锚文件夹才能运行程序。）
使暹罗模型出错
文件链接：https://www.mediafire.com/file/a5azngcpmdrrxyd/facial_recognition.ipynb/file
我尝试从 4.3 函数中删除嵌入函数，但当它进入训练时会抛出另一个错误。训练错误
文本代码错误复制：
（删除嵌入后）
Epoch 1/100
ValueError Traceback（最近一次调用最后一次）
in &lt;cell line: 5&gt;()
3
4 # 使用提供的训练数据和指定的 epoch 数训练 Siamese 网络
----&gt; 5 train(siamese_model, train_data, EPOCHS)
7 帧
/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in binary_crossentropy(target, output, from_logits)
666
667 if len(target.shape) != len(output.shape):
--&gt; 668 引发 ValueError(
669 “参数 target 和 output 必须具有相同的等级”
670 “(ndim)。已收到：“
ValueError：在用户代码中：
文件“&lt;ipython-input-20-c4cff50f6a59&gt;”，第 18 行，在 train_step *
loss = binary_cross_loss(y, yhat)
文件“/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py”，第 43 行，在 __call__ **
loss = self.call(y_true, y_pred)
文件&quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py&quot;，第 27 行，在 call
return self.fn(y_true, y_pred, **self._fn_kwargs)
文件 &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py&quot;，第 1913 行，在 binary_crossentropy
ops.binary_crossentropy(y_true, y_pred, from_logits=from_logits),
文件 &quot;/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py&quot;，第 1398 行，在 binary_crossentropy
return backend.nn.binary_crossentropy(
文件“/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py”，第 668 行，在 binary_crossentropy 中
raise ValueError(

ValueError: 参数“target”和“output”必须具有相同的等级 (ndim)。已接收：target.shape=(16,)，output.shape=(16, 100, 100, 1)

（删除嵌入之前）
[&lt;KerasTensor shape=(None, 4096), dtype=float32, sparse=False, name=keras_tensor_87&gt;]
TypeError Traceback（最近一次调用最后一次）
in &lt;cell line: 24&gt;()
22
23 # 使用 make_siamese_model() 函数创建暹罗神经网络模型
---&gt; 24 siamese_model = make_siamese_model()
25
26 # 显示 Siamese 模型的架构和参数摘要
2 帧
in call(self, input_embedding, validation_embedding)
7 # 魔法在这里发生 - 相似度计算
8 def call(self, input_embedding, validation_embedding):
----&gt; 9 return tf.math.abs(input_embedding - validation_embedding)
TypeError：调用 L1Dist.call() 时遇到异常。
无法自动推断“l1_dist_12”（类型为 L1Dist）的输出形状/dtype。L1Dist.call() 方法不正确，或者您需要实现 L1Dist.compute_output_spec() / compute_output_shape() 方法。遇到错误：
不支持 - 的操作数类型：&#39;list&#39; 和 &#39;list&#39;
L1Dist.call() 收到的参数：
• args=([&#39;&lt;KerasTensor shape=(None, 4096), dtype=float32, sparse=False, name=keras_tensor_88&gt;&#39;], [&#39;&lt;KerasTensor shape=(None, 4096), dtype=float32, sparse=False, name=keras_tensor_89&gt;&#39;])
• kwargs=&lt;class &#39;inspect._empty&#39;&gt;]]></description>
      <guid>https://stackoverflow.com/questions/78818666/facial-recognition-machine-learning</guid>
      <pubDate>Wed, 31 Jul 2024 23:57:30 GMT</pubDate>
    </item>
    <item>
      <title>通过旋转和裁剪对齐 X 射线图像 [关闭]</title>
      <link>https://stackoverflow.com/questions/78818523/align-x-ray-images-by-rotating-and-croping-them</link>
      <description><![CDATA[我有一个 X 射线图片数据库，我想对其进行图像分类 - 我想要：

识别（不完美）矩形块的旋转
旋转图像使其垂直（肖像形式）
通过裁剪剩余的黑色空间来移除，以便只保留骨骼的实际照片

我不完全确定如何最好地解决这个问题，这似乎是有人已经解决的问题。
我尝试了几种方法，但似乎都没有完全奏效 - 因为有些图片旋转了，有些没有，有些有清晰的白色轮廓，有些没有，所以结果非常不一致。
示例图像附在下面：
示例 1
示例 2
更多示例]]></description>
      <guid>https://stackoverflow.com/questions/78818523/align-x-ray-images-by-rotating-and-croping-them</guid>
      <pubDate>Wed, 31 Jul 2024 22:36:26 GMT</pubDate>
    </item>
    <item>
      <title>OSError：openvoid/Prox-Phi-3-mini-128k 似乎没有名为 microsoft/Phi-3-mini-128k-instruct--modeling_phi3.py 的文件</title>
      <link>https://stackoverflow.com/questions/78818025/oserror-openvoid-prox-phi-3-mini-128k-does-not-appear-to-have-a-file-named-micr</link>
      <description><![CDATA[我正在尝试使用 huggingface 模型（https://huggingface.co/openvoid/Prox-Phi-3-mini-128k）。但遇到此错误：
OSError：openvoid/Prox-Phi-3-mini-128k 似乎没有名为 microsoft/Phi-3-mini-128k-instruct--modeling_phi3.py 的文件

可能是什么原因？
我尝试降级 transformer 版本，它解决了一个配置错误。现在我遇到了这个错误。
查看https://huggingface.co/openvoid/Prox-Phi-3-mini-128k/main以获取可用文件。我尝试打开它，但显示 404 错误。
我正在尝试在本地运行此模型。]]></description>
      <guid>https://stackoverflow.com/questions/78818025/oserror-openvoid-prox-phi-3-mini-128k-does-not-appear-to-have-a-file-named-micr</guid>
      <pubDate>Wed, 31 Jul 2024 19:06:05 GMT</pubDate>
    </item>
    <item>
      <title>将 MLX 生成的模型转换为 GGUF（通过 ollama 访问）之前和之后的行为不匹配</title>
      <link>https://stackoverflow.com/questions/78817275/the-behavior-missmatch-before-and-after-converting-a-mlx-generated-model-to-gguf</link>
      <description><![CDATA[编辑 1：
让我更具体一点。我们微调的模型是一个代码生成模型，将以代码完成样式（文本生成样式）输出。
示例输入：system.out
示例输出：system.out.println(xxx)
转换为 Ollama 识别的格式并由 Ollama 启动后，样式变为：
示例输入：system.out
示例输出：似乎您在试图胡说八道，并且似乎还丢失了一些由我们的微调提供的代码相关知识。
原始版本：
我们使用 MLX 对模型进行了微调并成功保存了模型，请查看此链接了解更多详情。
到目前为止，生成的模型使用类似 mlx_lm.generate --model new_model --prompt &quot;tell me sth about sql&quot; 的命令运行良好--temp 0.01 --ignore-chat-template。
但是，将其转换为 gguf 格式并通过 Ollama 访问后，输出会有所不同，与预期不符。
将其转换为 gguf 的过程如下：
python llama.cpp/convert_hf_to_gguf.py path/new_model --outfile path/new_model.gguf


创建模型文件，内容如下：
FROM ./new_model.gguf
# 将温度设置为 1 [越高越有创意，越低越连贯]
PARAMETER 温度 0.01

使用 Ollama 创建最终工件：
ollama create new_model -f modelfile

使用以下命令启动 ollama ollama run new_model并对其进行评估。]]></description>
      <guid>https://stackoverflow.com/questions/78817275/the-behavior-missmatch-before-and-after-converting-a-mlx-generated-model-to-gguf</guid>
      <pubDate>Wed, 31 Jul 2024 15:49:03 GMT</pubDate>
    </item>
    <item>
      <title>YoloTinyNet 如何对特定图像进行推理？</title>
      <link>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</link>
      <description><![CDATA[我有一个图像weirdobject.jpg和一个通过带注释的图像训练的模型，以识别该图像中的对象（注释是通过roboflow完成的，并导出到.ckpt文件中）。基本上这就是我使用 YoloTinyNet 对输入图像进行推理的方式：
class_names = [&quot;aeroplane&quot;, &quot;bicycle&quot;, &quot;bird&quot;, &quot;boat&quot;, &quot;bottle&quot;, &quot;bus&quot;, &quot;car&quot;, &quot;cat&quot;, &quot;chair&quot;, &quot;cow&quot;, &quot;diningtable&quot;,
&quot;dog&quot;, &quot;horse&quot;, &quot;motorbike&quot;, &quot;person&quot;, &quot;pottedplant&quot;, &quot;sheep&quot;, &quot;sofa&quot;, &quot;train&quot;, &quot;tvmonitor&quot;,
&quot;small_ball&quot;]
_common_params = {&#39;image_size&#39;: 448, &#39;num_classes&#39;: len(classes_name),
&#39;batch_size&#39;: 16} # 批次大小 1
_net_params = {&#39;cell_size&#39;: 7, &#39;boxes_per_cell&#39;: 2, &#39;weight_decay&#39;: 0.0005}
image = tf.placeholder(tf.float32, (1, 448, 448, 3))
_net =YoloTinyNet(_common_params, _net_params, test=True)
# 图像不应该包含实际的 .jpg 文件吗？或者此时占位符就足够了？
_net .inference(image)
sess = tf.Session()
saver = tf.train.Saver(_net.trainable_collection)
saver.restore(sess, modelFile)
src_img = cv2.imread(&quot;./weirdobject.jpg&quot;)
resized_img = cv2.resize(src_img, (448, 448))
np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)
#我在此阶段仅包含图像数据（在此之前不应该包含吗？）
np_predict = sess.run(object_predicts, feed_dict={image: np_img})

np_predict 结果似乎与我的预期不一致，因为返回的结果包括许多不在图像中的物体类别，而真实物体也包含在结果中。
这是通过固定图像对训练模型进行预测以检测物体的正确方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</guid>
      <pubDate>Wed, 31 Jul 2024 14:15:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow Pipeline 中对大型数据集应用图像增强？</title>
      <link>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</link>
      <description><![CDATA[我有一个图像数据集，每个图像包含一个 1 到 5 个字母的单词。我想使用深度学习对每个图像中组成单词的字符进行分类。这些图像的标签格式如下：
totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar
我正尝试将这些图像加载到 TensorFlow 管道中，以降低由于内存限制而导致的复杂性。下面是我从目录加载和处理图像和标签的代码：
def process_img(file_path):
label = get_label(file_path)
image = tf.io.read_file(file_path)
image = tf.image.decode_png(image, channels=1) 
image = tf.image.convert_image_dtype(image, tf.float32) 
target_shape = [695, 1204]
image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])

# 对标签进行编码
coded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
coded_label.set_shape([5, len(urdu_alphabets)])

return image,coded_label
input_dir = &#39;/kaggle/input/dataset/Data/*&#39;
images_ds = tf.data.Dataset.list_files(input_dir, shuffle=True)

train_count = int(tf.math.round(len(images_ds) * 0.8))
train_ds = images_ds.take(train_count)
test_ds = images_ds.skip(train_count)
train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)

train_ds 如下所示：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;
现在，我想对图像应用简单的增强，例如旋转、剪切、侵蚀和扩张。我最初使用了以下函数：
def augment(image, label):
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode=&#39;nearest&#39;, cval=0.0, interpolation_order=1)
image = tf.image.random_zoom(image, [0.85, 0.85])
image = tf.image.random_shear(image, 0.3)
image = tf.image.random_shift(image, 0.1, 0.1)
return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

但是，tf.image 中的许多函数都已弃用。如何以高效的方式在 TensorFlow 管道中将这些增强应用于图像？
注意：我可以通过不使用 TensorFlow 管道使用 NumPy 数组加载图像来执行这些增强，但我的数据集非常大（110 万张图像），因此我需要一种高效的方法来执行此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</guid>
      <pubDate>Wed, 31 Jul 2024 14:11:01 GMT</pubDate>
    </item>
    <item>
      <title>获取矩阵中的方向元素</title>
      <link>https://stackoverflow.com/questions/78816668/get-directional-elements-in-matrix</link>
      <description><![CDATA[假设我的矩阵中有一个 NxN 兴趣点。该点位于位置 ij。那么，给定索引 ij，是否有一种简单的方法可以让线元素通过 ij 到达原点（位于矩阵中间）？
我正在使用 torch，我认为使用 torch.diag 是第一步，但实际上这个函数并没有通过矩阵的中间。
def directionalK(kx,ky, indices):
&#39;&#39;&#39;函数提供由索引指定的给定方向的 K 值&#39;&#39;&#39;
kx_grid,ky_grid = torch.meshgrid(kx,kx, indexing=&#39;ij&#39;)
k_grid = torch.sqrt(kx_grid**2 + ky_grid**2) 
k_grid[...,:len(k_grid)//2] *=-1 
y,x = indices
diag = x - len(k_grid)//2
]]></description>
      <guid>https://stackoverflow.com/questions/78816668/get-directional-elements-in-matrix</guid>
      <pubDate>Wed, 31 Jul 2024 13:35:49 GMT</pubDate>
    </item>
    <item>
      <title>如何进行布尔分类处理？</title>
      <link>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler、OneHotEncoder、OrdinalEncoder
从 sklearn.pipeline 导入 Pipeline

data = pd.read_csv(&#39;Datasets/StudentScore.csv&#39;)

target = &#39;MathScore&#39;
x = data.drop(data[[target, &#39;Unnamed: 0&#39;]], axis=1)
y = data[target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# 数值处理
num_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.fit_transform(x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])
x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.transform(x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])

# 序数处理
education_levels = [&quot;high school&quot;, &quot;some high school&quot;, &quot;some college&quot;, &quot;associate&#39;s degree&quot;, &quot;bachelor&#39;s degree&quot;,
&quot;master&#39;s degree&quot;]

ord_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OrdinalEncoder(categories=[education_levels])),
])

x_train[[&#39;ParentEduc&#39;]] = ord_transformer.fit_transform(x_train[[&#39;ParentEduc&#39;]])
x_test[[&#39;ParentEduc&#39;]] = ord_transformer.transform(x_test[[&#39;ParentEduc&#39;]])

# 名义处理
nom_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder())
])

x_train[[&#39;EthnicGroup&#39;]] = nom_transformer.fit_transform(x_train[[&#39;EthnicGroup&#39;]])
x_test[[&#39;EthnicGroup&#39;]] = nom_transformer.transform(x_test[[&#39;EthnicGroup&#39;]])

# 布尔处理
bool_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder(sparse_output=False)),
])

x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.fit_transform(
x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])
x_test[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.transform(x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])

我在尝试创建管道来处理布尔分类特征时遇到错误。具体来说，在训练集和测试集中的特征的 fit_transform 步骤中，我在名义处理和布尔处理部分中收到了
ValueError：列的长度必须与键的长度相同

。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</guid>
      <pubDate>Tue, 30 Jul 2024 19:12:18 GMT</pubDate>
    </item>
    <item>
      <title>Catboost 特征重要性计算</title>
      <link>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</link>
      <description><![CDATA[我仅用 3 棵树拟合了一个简单二分类模型，并想检查特征重要性结果是否与 Catboost 文档 (PredictionValuesChange) 中的公式相似。
训练模型后，我按照CatBoost JSON 模型教程中的步骤操作，并得到了以下树结构：
{
&quot;leaf_values&quot;: [
-0.13915912880676032,
0.1097787155963716
],
&quot;leaf_weights&quot;: [
2143.0251545906067,
2252.974784851074
],
&quot;splits&quot;: [
{
&quot;border&quot;: 3.5,
&quot;float_feature_index&quot;: 13,
&quot;split_index&quot;: 0,
&quot;split_type&quot;: &quot;FloatFeature&quot;
}
]
} 

模型中的每棵树只有深度 = 1，并且只有一棵树（索引 = 1）具有感兴趣的特征。我决定根据上述公式手动计算特征重要性，并将结果与​​ .get_feature_importance 方法进行比较。结果大不相同：

特征重要性：28.2947825
手动计算：68.06248029261762

以下是用于特征重要性计算的代码：
tree_indx = 1
v_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][0]
v_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][1]

c_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][0]
c_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][1]

avr = (v_1*c_1 + v_2*c_2)/(c_1+c_2)

fi = ((v_1 - avr)**2)*c_1 + ((v_2 - avr)**2)*c_2
print(fi)

我犯了错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</guid>
      <pubDate>Mon, 29 Jul 2024 15:29:02 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。

问题是前 50/60 次推理的性能非常好（比如 20 毫秒），然后它们开始变差，直到时间稳定在每张图像 70/75 毫秒左右。我不明白为什么性能会以这种方式变差。

环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
CPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

GPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型而出现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在 TensorFlow 中使用 model.fit() 时会得到 ValueError：无法识别的数据类型：x=[...] (类型 <class 'list'>)？</title>
      <link>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</link>
      <description><![CDATA[我尝试运行以下代码，该代码取自 CS50 的 AI 课程：
import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split

# 从文件读取数据
with open(&quot;banknotes.csv&quot;) as f:
reader = csv.reader(f)
next(reader)

data = []
for row in reader:
data.append(
{
&quot;evidence&quot;: [float(cell) for cell in row[:4]],
&quot;label&quot;: 1 if row[4] == &quot;0&quot; else 0,
}
)

#将数据分为训练组和测试组
evidence = [row[&quot;evidence&quot;] for row in data]
labels = [row[&quot;label&quot;] for row in data]
X_training, X_testing, y_training, y_testing = train_test_split(
evidence, labels, test_size=0.4
)

# 创建神经网络
model = tf.keras.models.Sequential()

# 添加一个有 8 个单元的隐藏层，使用 ReLU 激活函数
model.add(tf.keras.layers.Dense(8, input_shape=(4,),activation=&quot;relu&quot;))

# 添加一个有 1 个单元的输出层，使用 sigmoid 激活函数
model.add(tf.keras.layers.Dense(1,activation=&quot;sigmoid&quot;))

# 训练神经网络
model.compile(
optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)
model.fit(X_training, y_training, epochs=20)

# 评估模型的表现
model.evaluate(X_testing, y_testing, verbose=2)

但是，我收到以下错误：
Traceback（最近一次调用最后一次）：
文件“C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py”，第 41 行，位于&lt;module&gt;
model.fit(X_training, y_training, epochs=20)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py&quot;，第 113 行，位于 get_data_adapter 中
引发 ValueError(f&quot;无法识别的数据类型：x={x}（类型为 {type(x)}）&quot;)
ValueError：无法识别的数据类型：x=[...]（类型为 &lt;class &#39;list&#39;&gt;)

其中“...”是训练数据。
知道哪里出错了吗？我在 Windows 计算机上使用 Python 版本 3.11.8 和 TensorFlow 版本 2.16.1。
我尝试在 Google Colab 笔记本中运行相同的代码，并且成功了：问题仅发生在我的本地机器上。这是我期望的输出：
Epoch 1/20
26/26 [==============================] - 1s 2ms/step - 损失：1.1008 - 准确度：0.5055
Epoch 2/20
26/26 [===============================] - 0s 2ms/step - 损失：0.8588 - 准确度：0.5334
Epoch 3/20
26/26 [================================] - 0s 2ms/step - 损失：0.6946 - 准确度：0.5917
Epoch 4/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5970 - 准确度：0.6683
纪元 5/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5265 - 准确度：0.7120
纪元 6/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4717 - 准确度：0.7655
纪元 7/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4258 - 准确度：0.8177
纪元 8/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.3861 - 准确度：0.8433
纪元 9/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3521 - 准确度：0.8615
纪元 10/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.3226 - 准确度：0.8870
纪元 11/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2960 - 准确度：0.9028
纪元 12/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2722 - 准确度：0.9125
纪元 13/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.2506 - 准确度：0.9283
纪元 14/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2306 - 准确度：0.9514
纪元 15/20
26/26 [================================] - 0s 3ms/步 - 损失：0.2124 - 准确度：0.9660
纪元 16/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1961 - 准确度：0.9769
纪元 17/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.1813 - 准确度：0.9781
纪元 18/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1681 - 准确度：0.9793
纪元 19/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1562 - 准确度：0.9793
Epoch 20/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1452 - 准确度：0.9830
18/18 - 0s - 损失：0.1407 - 准确度：0.9891 - 187ms/epoch - 10ms/步
[0.14066053926944733, 0.9890710115432739]
]]></description>
      <guid>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</guid>
      <pubDate>Thu, 04 Apr 2024 00:28:01 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器的 SHAP 解释中 expected_value 的计算</title>
      <link>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</link>
      <description><![CDATA[我们如何理解SHAP explainer.expected_value？为什么经过sigmoid变换后，它与y_train.mean()不一样？
下面是代码摘要，供快速参考。完整代码可在此笔记本中找到：https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)
explainer = shap.Explainer(model)
shap_test = explainer(X_test)
shap_df = pd.DataFrame(shap_test.values)

#对于每种情况，如果我们将所有特征的 shap 值加上预期值相加，我们就可以得到该情况的边际，然后可以将其转换为返回该情况的预测概率case:
np.isclose(model.predict(X_test, output_margin=True),explainer.expected_value + shap_df.sum(axis=1))
#True

但是为什么下面不成立？为什么经过 sigmoid 变换后，XGBoost 分类器的 explainer.expected_value 与 y_train.mean() 不一样？
expit(explainer.expected_value) == y_train.mean()
#False
]]></description>
      <guid>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</guid>
      <pubDate>Mon, 18 Sep 2023 09:28:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在线托管 TensorFlow 模型</title>
      <link>https://stackoverflow.com/questions/76187823/how-to-host-tensorflow-model-online</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Serving 将 ML 模型作为 REST API 提供服务。
我想知道是否有办法在线而不是本地托管模型？
提前谢谢您。
我需要托管一个 ML 模型，在进行预测时，该模型会与字符串 id 进行映射。
该模型是一个 .h5 文件。
该程序在笔记本中运行。但在开发移动应用程序时，我不知道如何进行托管。]]></description>
      <guid>https://stackoverflow.com/questions/76187823/how-to-host-tensorflow-model-online</guid>
      <pubDate>Sat, 06 May 2023 08:09:39 GMT</pubDate>
    </item>
    <item>
      <title>为连续特征、多个标签选择朴素贝叶斯模型</title>
      <link>https://stackoverflow.com/questions/67269495/choose-naive-bayes-model-for-continous-feature-multiple-labels</link>
      <description><![CDATA[假设我有一个数据集，其特征值是连续的，并且有两个以上的可能标签（例如：下雨、晴天、刮风等），我应该在 sklearn 中实现哪种朴素贝叶斯模型？
我正在考虑高斯或多项式。然而，多项式适用于离散特征，我尝试了高斯，但结果发现预测的准确性就像随机选择一样。]]></description>
      <guid>https://stackoverflow.com/questions/67269495/choose-naive-bayes-model-for-continous-feature-multiple-labels</guid>
      <pubDate>Mon, 26 Apr 2021 15:28:29 GMT</pubDate>
    </item>
    </channel>
</rss>