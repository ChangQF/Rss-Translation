<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 13 Apr 2024 21:11:10 GMT</lastBuildDate>
    <item>
      <title>使用推荐引擎为两个用户推荐一部电影</title>
      <link>https://stackoverflow.com/questions/78321965/using-recommendation-engine-to-recommend-a-movie-for-two-users</link>
      <description><![CDATA[我使用 torch 和 Fastai 来训练数据并得出用户权重与物品权重。有了经过训练的数据，使用两个用户权重的组合向一对用户推荐电影的最佳方式是什么？是否像取一对用户的 n 个参数权重的平均值然后使用这些权重和余弦相似度函数找到最佳项目一样简单？我是机器学习和数据科学的新手，所以如果这是一个愚蠢的问题，我深表歉意。
movie_factors = learn.model.i_weight.weight
user_factors = learn.model.u_weight.weight

#随机选择两个用户
user1_factors = user_factors[43].data.cpu().numpy()
user2_factors = user_factors[54].data.cpu().numpy()

# 计算user1_factors和user2_factors的平均值
avg_u_factors = torch.from_numpy(np.array((user1_factors + user2_factors) / 2)).to(movie_factors.device)

距离 = nn.CosineSimilarity(dim=1)(movie_factors, avg_u_factors)
idx = distances.argsort(降序=True)[1]
dls.classes[&#39;标题&#39;][idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78321965/using-recommendation-engine-to-recommend-a-movie-for-two-users</guid>
      <pubDate>Sat, 13 Apr 2024 20:41:42 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 pytorch 训练机器学习多项式回归模型，我是一个初学者</title>
      <link>https://stackoverflow.com/questions/78321929/well-im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch</link>
      <description><![CDATA[好吧，我想将我的数据绘制成 plt.scatter 形式，但是当我尝试填充它时，它只是说 x 和 y 的大小不同，而且我还将它们压缩到只有一维，这样绘制起来更容易，但仍然不起作用，如果有人可以提供帮助，那就太好了。
这就是剧情机制
#使用 matplotlib.pyplot 中的散点图 (x,y) 可视化数据
defplot_predictions(train_features=X_train,
                     train_labels=y_train,
                     test_features=X_test,
                     测试标签=y_测试，
                     预测=无）：
    plt.figure(figsize=(10,7))

    plt.scatter(X_train, y_train, c=“g”, label=“训练数据”)

    plt.scatter(X_test, y_test, c=“b”, label=“测试数据”)

    如果预测不是 None：
        plt.scatter（test_features，预测，c =“r”，标签=“预测”）

    plt.legend(prop={“大小”: 14})
绘图预测（）；

#这里尝试解决问题
Predictions_reshape=y_preds.squeeze(dim=1)
labels_reshape=y_train.squeeze(dim=1)
打印（len（y_train），len（y_preds））
打印（labels_reshape.shape，predictions_reshape.shape）

labels_reshape=y_train.detach().numpy()
Predictions_reshape=y_preds.detach().numpy()
图_预测（标签_重塑，预测=预测_重塑）

ValueError：x 和 y 的大小必须相同
如上所述，我尝试挤压张量，使它们只有一个暗淡，并且我还检查了 len 是否相同，确实如此。]]></description>
      <guid>https://stackoverflow.com/questions/78321929/well-im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch</guid>
      <pubDate>Sat, 13 Apr 2024 20:23:01 GMT</pubDate>
    </item>
    <item>
      <title>ML 查找四边形的角点</title>
      <link>https://stackoverflow.com/questions/78321889/ml-find-corners-of-quadrilateral</link>
      <description><![CDATA[伙计们！我的任务是使用 ML 模型找到四边形形状的 4 个角点。有时四边形的一个角度会丢失（例如页面的折叠角度）。
首先，我尝试使用 MobileNetV3Small 作为主干进行图像分割，因为模型应该小而快。效果很好，但找到角落仍然是一个问题。我尝试按照 官方 keras 关键点检测等示例查找图像的关键点， medium 教程，以及许多其他来源，但似乎没有什么对我有用。我已经尝试修改它们很多次了。测试和验证的损失函数都会下降，但输出甚至不接近所需的位置。也尝试过类似以下的方法：
def conv(模型, 大小, conv2d_kernel, dilation_rate=(1, 1), pooling_size=(2, 2)):
    model.add(Conv2D(大小, conv2d_kernel, dilation_rate=dilation_rate))
    model.add(激活(&#39;relu&#39;))
    model.add(MaxPooling2D(pool_size=max_pooling))
    模型.add(Dropout(0.1))

def 密集（模型，单位）：
    model.add(密集(单位))
    model.add(激活(&#39;relu&#39;))
    模型.add(Dropout(0.1))

模型=顺序（）
model.add(InputLayer(形状=(224, 224, 3)))

转换（模型，大小=32，conv2d_kernel=（2, 2））
转换（模型，大小=64，conv2d_kernel=（3, 3））
转换（模型，大小=128，conv2d_kernel=（3, 3））

模型.add(压平())
密集（模型，20）
密集（模型，20）

model.add(密集(8))
model.compile(优化器=RMSprop(),
              损失=损失.MeanSquaredLogarithmicError(),
              指标=[metrics.MeanAbsoluteError()])

还尝试了像 (8) 和 (4,2) 这样的输出形状，但似乎没有任何效果。任何帮助将不胜感激。
PS：还忘记添加数据集注释正确，或者至少这是我在绘图上看到的。还尝试将坐标标准化为 0 和 1 之间。我的输入是 (224,224,3)。]]></description>
      <guid>https://stackoverflow.com/questions/78321889/ml-find-corners-of-quadrilateral</guid>
      <pubDate>Sat, 13 Apr 2024 20:06:34 GMT</pubDate>
    </item>
    <item>
      <title>如何修剪unet模型</title>
      <link>https://stackoverflow.com/questions/78321877/how-to-pruning-an-unet-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78321877/how-to-pruning-an-unet-model</guid>
      <pubDate>Sat, 13 Apr 2024 20:03:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytest 和假设进行可视化</title>
      <link>https://stackoverflow.com/questions/78321735/using-pytest-and-hypothesis-for-visualization</link>
      <description><![CDATA[我正在使用albumentation库进行图像增强，我也在为每个类似的旋转编写测试用例应该在50 - 90度之内，Blur=blur_limit min：3 max：99，我如何可视化我的测试用例在哪里未能使用假设
带有假设可视化的 pytest]]></description>
      <guid>https://stackoverflow.com/questions/78321735/using-pytest-and-hypothesis-for-visualization</guid>
      <pubDate>Sat, 13 Apr 2024 19:08:00 GMT</pubDate>
    </item>
    <item>
      <title>脑肿瘤分类的深度学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78321435/deep-learning-model-for-brain-tumor-classification</link>
      <description><![CDATA[我想建立一个包含四个类别的脑肿瘤分类模型。
我希望模型有一个预训练模型（例如 VGG 或 Xception）作为其特征提取器。还有用于分类部分的机器学习分类器（例如随机森林或 KNN）。我正在使用 Pytorch。我不知道如何建立这样的模型。首先我需要 DNN 来提取特征。]]></description>
      <guid>https://stackoverflow.com/questions/78321435/deep-learning-model-for-brain-tumor-classification</guid>
      <pubDate>Sat, 13 Apr 2024 17:21:31 GMT</pubDate>
    </item>
    <item>
      <title>用于大数据分析的 PySpark，断言错误：面临使用哈希和 one-hot 编码转换字符串特征的问题</title>
      <link>https://stackoverflow.com/questions/78321117/pyspark-for-big-data-analytics-assertion-error-facing-issues-converting-string</link>
      <description><![CDATA[我是大数据分析新手，正在使用 PySpark 处理大数据机器学习任务，特别是信用卡欺诈检测。然而，我遇到了障碍。在我的数据集中，我有两个字符串特征，需要在构建模型之前将其转换为数值。我尝试过各种方法，例如one-hot编码和散列，但没有成功。
如何解决这个问题？或者有更好的方法来处理它吗？
这些是功能，我想将字符串值转换为数字，

!pip3 安装 pyspark

# 从 &#39;google.colab&#39; 库导入 &#39;drive&#39; 模块来挂载 Google Drive
从 google.colab 导入驱动器

# 将 Google Drive 挂载到 &#39;/content/drive&#39; 目录
驱动器.mount（&#39;/内容/驱动器&#39;）

# 从 pyspark.sql 模块导入 SparkSession 类
从 pyspark.sql 导入 SparkSession

# 创建一个名为“spark”的 SparkSession 来与 Spark 交互
# &#39;master&#39; 参数设置为“local[*]”，这意味着 Spark 将使用所有可用核心以本地模式运行
# 将“appName”参数设置为“FraudDetection”，为 Spark 应用程序命名
# &#39;getOrCreate()&#39; 方法确保如果现有的 SparkSession 可用，它将被重用；否则，将创建一个新的


火花 = SparkSession.builder \
        .master(“本地[*]”) \
        .appName(&#39;欺诈检测&#39;) \
        .config(“spark.driver.memory”,“8g”) \
        .config(“spark.kryoserializer.buffer.max”,“16g”) \
        .getOrCreate()

# 使用spark加载spark_df
Spark_df = Spark.read.csv(&#39;/content/drive/MyDrive/big_data/transactions_train.csv&#39;,inferSchema=True, header =True)

# 此 Spark_df 中的可用列
Spark_df.columns

从 pyspark.sql.functions 导入 isnan、when、count、col

# 检查每列中是否有缺失值
Missing_counts = Spark_df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in spark_df.columns])
缺少计数.show()

# 检查任意行中是否有缺失值
Total_missing_count = Spark_df.rdd.map(lambda row: sum([1 for x in row if x == None])).sum()
print(&quot;DataFrame 中缺失值总数：{}&quot;.format(total_missing_count))

从 pyspark.sql.functions 导入时
# 将列类型转换为数值
”“”
  兑现 : 0,
  提现 : 1,
  借方：2，
  付款方式： 3,
  转乘：4，

”“”
Spark_df=spark_df.withColumn(“类型”,
                             当（spark_df.type==“CASH_IN”，0）
                            .when(spark_df.type==“CASH_OUT”, 1)
                            .when(spark_df.type==“借记”, 2)
                            .when(spark_df.type==“付款”, 3)
                            .when(spark_df.type==“传输”, 4)
                            .否则(-1))

Spark_df.show()

从 pyspark.ml.feature 导入 HashingTF
从 pyspark.sql.functions 导入 pandas_udf，PandasUDFType
从 pyspark.sql.types 导入 *
从 pyspark.ml 导入管道

# 定义一个函数将管道应用到每个分区
@pandas_udf（架构，PandasUDFType.GROUPED_MAP）
def 哈希功能（pdf）：
    # 定义函数内管道的阶段
    hashTF = HashingTF(inputCol=“nameOrig”，outputCol=“nameOrig_hashed”，numFeatures=10)
    hashingTF_dest = HashingTF(inputCol=“nameDest”，outputCol=“nameDest_hashed”，numFeatures=10)

    # 创建管道
    管道=管道（阶段= [hashingTF，hashingTF_dest]）

    # 让管道适应数据
    pipeline_model = pipeline.fit(pdf)

    # 转换数据
    df_hashed = pipeline_model.transform(pdf)

    返回 df_hashed

# 对 DataFrame 应用哈希
df_hashed = Spark_df.groupby(“nameOrig”).apply(hashingFeatures)

df_hashed.show()

大多数时候我都会面临这个错误和缓冲区溢出！

几天来我尝试了不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78321117/pyspark-for-big-data-analytics-assertion-error-facing-issues-converting-string</guid>
      <pubDate>Sat, 13 Apr 2024 15:37:56 GMT</pubDate>
    </item>
    <item>
      <title>未经训练的 DNN 产生的泛化误差过小</title>
      <link>https://stackoverflow.com/questions/78320915/unreasonably-small-generalization-error-produced-by-untrained-dnn</link>
      <description><![CDATA[我目前正在尝试训练 DNN 来执行回归任务。目标是在给定 4 个输入特征的情况下预测 2 个 QoI。对于训练和验证，我使用 LHS 获得的 40 个训练模式，其中分为 80-20 个。训练模型后，我用它来预测从正态分布中随机获得的 300 个新的未见样本，并计算两个 QoI 的 rmse（预测与实际结果之间）。我使用 2 个标准 scaler() 实例，一个用于 X（输入），一个用于 y（输出）。相同的两个缩放器实例用于预测 300 个样本。第一个在将输入提供给模型之前缩放输入，第二个将模型预测（反向）缩放到相应的幅度。 （我只会针对一个 QoI 提及这个问题，因为第二个 QoI 也验证了完全相同的问题）。
到目前为止，我所看到的 300 的 rmse 值约为 2 到 4.5 10^(-4)。这里的悖论是，有一次，只是为了检查，我在训练之前使用了我编译的模型来预测 300 并查看 RMSE 会是什么样子。我的大脑告诉我，这个错误应该比我迄今为止看到的值非常遥远，因为模型完全未经训练。结果（对于不同的种子状态，也称为初始化）也在 510^(-4) 左右，只是比从训练模型获得的结果稍差一点。在我看来，这完全是无稽之谈，我试图理解我在 python 代码中一定犯了一个错误，可能会导致我来到这里。 PS1：问题是我得到了一个非常“小”的东西。回归中使用的完全未经训练的 DNN 产生的泛化误差。 PS2：在模型预测缩小到相应的幅度后，正在计算 rsme。]]></description>
      <guid>https://stackoverflow.com/questions/78320915/unreasonably-small-generalization-error-produced-by-untrained-dnn</guid>
      <pubDate>Sat, 13 Apr 2024 14:34:18 GMT</pubDate>
    </item>
    <item>
      <title>sklearn DummyClassifier 的预测不正确</title>
      <link>https://stackoverflow.com/questions/78320892/incorrect-prediction-from-sklearn-dummyclassifier</link>
      <description><![CDATA[我正在尝试对学校项目的数据集执行虚拟分类。这个想法是为了了解不同政党发表演讲的频率。我的想法是按以下方式编写此代码：
from sklearn.dummy import DummyClassifier
将 pandas 导入为 pd
导入bz2


以 bz2.open(“data/ch3/speeches-201718.json.bz2”) 作为源：
    Speechs_201718 = pd.read_json（来源）

以 bz2.open(“data/ch3/speeches-201819.json.bz2”) 作为源：
    Speechs_201819 = pd.read_json（来源）


训练数据、测试数据 = 演讲_201718、演讲_201819

train_partys_count = Training_data[&#39;party&#39;].value_counts()
test_partys_count = test_data[&#39;party&#39;].value_counts()
dummy_clf = DummyClassifier(策略=“most_frequent”)

X = train_party_count
y = train_party_count.index
dummy_clf.fit(X.值, y)
打印（X）
打印（y）

test_parties_count.index = pd.CategoricalIndex(test_parties_count.index,categories=train_parties_count.index,ordered=True)
X_test = test_partys_count.sort_index()
打印（X_测试）
pred_mfc = dummy_clf.predict(X_test.values)

print(&quot;Urval av prediktioner [0-4]: &quot;, pred_mfc[:5])


我得到以下输出：
在此处输入图像描述
正如您所看到的，预测应该是 S，但结果却是 C，什么可能是不正确的？
我尝试以多种方式定义训练和测试数据，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78320892/incorrect-prediction-from-sklearn-dummyclassifier</guid>
      <pubDate>Sat, 13 Apr 2024 14:26:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：操作数无法与形状一起广播 (10,1024) (1024,1) [关闭]</title>
      <link>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</guid>
      <pubDate>Fri, 12 Apr 2024 11:33:12 GMT</pubDate>
    </item>
    <item>
      <title>我在重塑图像数据集时遇到错误</title>
      <link>https://stackoverflow.com/questions/78312092/i-am-facing-error-in-reshaping-our-image-dataset</link>
      <description><![CDATA[我遇到此错误文件“C:\Users\Kanishka Patel\anaconda3\Lib\site-packages\keras\src\ saving\serialization_lib.py”，第 600 行，在 deserialize_keras_object return deserialize_keras_object( ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^ 文件“C:\Users\Kanishka Patel\anaconda3\Lib\site-packages\ker
我尝试将 217560 重塑为 [224,224,3]]]></description>
      <guid>https://stackoverflow.com/questions/78312092/i-am-facing-error-in-reshaping-our-image-dataset</guid>
      <pubDate>Thu, 11 Apr 2024 17:27:34 GMT</pubDate>
    </item>
    <item>
      <title>不平衡的糖尿病视网膜病变分割：过度拟合和损失函数选择</title>
      <link>https://stackoverflow.com/questions/78311946/imbalanced-diabetic-retinopathy-segmentation-overfitting-and-loss-function-choi</link>
      <description><![CDATA[我正在训练一个 U-net 模型，用于分割糖尿病视网膜病变图像中的微动脉瘤。数据集不平衡，背景类（健康组织）占主导地位，约为 96%，前景类（微动脉瘤）仅占 4%。
我遇到两个主要问题：
过度拟合：模型似乎过度拟合训练数据。训练损失显着下降，但验证损失趋于稳定甚至增加。
负损失函数：在使用二元交叉熵 (BCE) 损失时，我在训练期间观察到负损失值。
当前方法：
模型：具有 EfficientNetB0 主干的 U-net 和来自 ImageNet 的预训练权重。
数据增强：旋转和翻转用于数据增强。
损失函数：目前使用二元交叉熵（BCE）。
预期模型：
从训练数据中学习相关特征。
很好地推广到未见过的数据（验证集），训练和验证损失在整个训练过程中稳步下降。
实现稳定的 BCE 损失，反映模型区分前景（微动脉瘤）和背景（健康组织）的性能]]></description>
      <guid>https://stackoverflow.com/questions/78311946/imbalanced-diabetic-retinopathy-segmentation-overfitting-and-loss-function-choi</guid>
      <pubDate>Thu, 11 Apr 2024 16:57:37 GMT</pubDate>
    </item>
    <item>
      <title>Numpy reshape() 以编程方式以 3D 方式显示 2D 数组</title>
      <link>https://stackoverflow.com/questions/78308446/numpy-reshape-to-display-2d-array-in-3d-programmatically</link>
      <description><![CDATA[示例数据
我有一系列经纬度的天气数据，其形状如下：(1038240,4)（有关示例数据，请参阅照片）
我想将其重塑为形状 (4,721,1440)，这将是 721 x 1440 地球图像上的四个天气变量（&amp; lat/lon）。
我已经尝试过：
newarr = t_new.reshape(4,721,1440)

将其置于正确的形状，但与前两个纬度/经度坐标不匹配，如下所示：
对于上图中的 (6,4) 示例数据，此操作看起来像下面的 (2,3,2) 数组：
所需输出示例
newarr = t_new.reshape(4,721,1440)
]]></description>
      <guid>https://stackoverflow.com/questions/78308446/numpy-reshape-to-display-2d-array-in-3d-programmatically</guid>
      <pubDate>Thu, 11 Apr 2024 06:06:56 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV 新手，我如何安装/构建 opencv_traincascade</title>
      <link>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</link>
      <description><![CDATA[所以我一直在从事机器学习项目，并且需要使用 opencv_traincascade 训练自定义数据集。但每当我尝试安装它时，它就永远无法工作。我还有其他东西，比如 opencv_annotation 和其他东西可以工作，但是 traincascade 或 event createsamples 不起作用。我必须手动构建这些吗？
我下载了mingw-gcc、cmake，在网上找不到可行的解决方案。顺便说一句，我有 opencv 4.9.0，手动安装在 anaconda 和我的 C: 驱动器中。我也尝试过寻找一些第三方，他们安装了整个 opencv 并且可以复制，但没有运气。任何帮助将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</guid>
      <pubDate>Sun, 07 Apr 2024 03:46:34 GMT</pubDate>
    </item>
    <item>
      <title>我想要一个 python 脚本将洋葱图像的背景更改为黑色，该怎么做？</title>
      <link>https://stackoverflow.com/questions/75859536/i-want-a-python-script-to-change-the-background-to-black-of-a-image-of-onion-ho</link>
      <description><![CDATA[我正在尝试将洋葱图像的背景颜色更改为黑色
我尝试使用 opencv 和 Pixellib 在 python 中编写代码，但它不起作用，我希望得到一些帮助来改变这一点]]></description>
      <guid>https://stackoverflow.com/questions/75859536/i-want-a-python-script-to-change-the-background-to-black-of-a-image-of-onion-ho</guid>
      <pubDate>Mon, 27 Mar 2023 19:10:07 GMT</pubDate>
    </item>
    </channel>
</rss>