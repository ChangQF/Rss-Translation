<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 12 Jul 2024 09:17:18 GMT</lastBuildDate>
    <item>
      <title>如何使adapter_conditioning_scale在多个T2I_Adapter中可训练？</title>
      <link>https://stackoverflow.com/questions/78738957/how-to-make-adapter-conditioning-scale-trainable-in-multiple-t2i-adapter</link>
      <description><![CDATA[下面是使用 Multi T2I_Adapter 的代码。如以下代码所示，adapter_conditioning_scale=[0.8, 0.8]，是手动设置的。
 adapters = MultiAdapter(
[
T2IAdapter.from_pretrained(&quot;TencentARC/t2iadapter_keypose_sd14v1&quot;),
T2IAdapter.from_pretrained(&quot;TencentARC/t2iadapter_depth_sd14v1&quot;),
]
)
adapters = adapters.to(torch.float16)

pipe = StableDiffusionAdapterPipeline.from_pretrained(
&quot;CompVis/stable-diffusion-v1-4&quot;,
torch_dtype=torch.float16,
adapter=adapters,
).to(&quot;cuda&quot;)

image = pipe(prompt, cond, adapter_conditioning_scale=[0.8, 0.8]).images[0]
make_image_grid([cond_keypose, cond_depth, image], rows=1, cols=3)

Google 的 Colab
我的问题：
我们可以使用什么机器学习技术来找到 adapter_conditioning_scale 的最佳值？
参考文献：
huggingface.co
T2IAdapter 代码]]></description>
      <guid>https://stackoverflow.com/questions/78738957/how-to-make-adapter-conditioning-scale-trainable-in-multiple-t2i-adapter</guid>
      <pubDate>Fri, 12 Jul 2024 06:45:55 GMT</pubDate>
    </item>
    <item>
      <title>查明代码文件中的错误来源[关闭]</title>
      <link>https://stackoverflow.com/questions/78738937/pinpointing-the-source-of-error-in-a-code-file</link>
      <description><![CDATA[我正在尝试实现一个小工具，它可以自动识别一组代码文件（作为输入）中的哪一部分代码导致了执行期间显示的错误文本。
错误可能是语法错误，也可能是逻辑错误。我还在考虑利用 llms 的 api 调用来更正代码。
据我所知，RAG 是必要的，因为我不可能将所有代码文件的数据都放入提示中，因为它肯定会超出上下文窗口的大小。这就是我尝试探索信息检索技术的原因。我知道一些 RAG 技术，如 RAG-fusion，但我想得到一些反馈和想法，了解我可以探索哪些其他方法/工具/模型。
代码调试问题可能存在我没有意识到的某些方面。任何帮助都非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78738937/pinpointing-the-source-of-error-in-a-code-file</guid>
      <pubDate>Fri, 12 Jul 2024 06:41:48 GMT</pubDate>
    </item>
    <item>
      <title>当数据框共享一列时，pd.concat()</title>
      <link>https://stackoverflow.com/questions/78738692/pd-concat-when-dataframes-share-a-column</link>
      <description><![CDATA[我试图连接两个共享一列的数据框。一个数据框包含所有列，但第二个数据框只有单个更改的列。我想连接它们，以便旧值被新值覆盖。我尝试了所有能想到的方法来做到这一点，但没有取得任何进展。以下是我想要发生的事情：
A B
0 1 2
1 2 3

连接：
A
0 3
1 4

成为：
A B
0 3 2
1 4 3

pd.concat(frames, axis=1) 让我得到了最接近的结果。两行乱七八糟，但它们应该合并在一起。不正确的行显示如下：
 A B
0 1 NaN
1 NaN 1

应将行组合在一起才能获得正确的输出。]]></description>
      <guid>https://stackoverflow.com/questions/78738692/pd-concat-when-dataframes-share-a-column</guid>
      <pubDate>Fri, 12 Jul 2024 05:16:59 GMT</pubDate>
    </item>
    <item>
      <title>MLP 回归器工程数据 SKLearn</title>
      <link>https://stackoverflow.com/questions/78738380/mlp-regressor-engineering-data-sklearn</link>
      <description><![CDATA[我的飞机分析模型上分布有 10 个加速度计。从我的分析模型中，我有一组传感器加速度，包括 10 个加速度计 X 6 个自由度 X 6000（60 秒）数据，即 60 x 6000 阵列和 49 X 6000 阵列中飞机上 49 个位置的应力。我使用不同频率的不同时间历史力生成了一组 100 个 60 X 6000 传感器数据和相应的 21 X 6000 传感器数据。我正在尝试使用 MLPRegressor 构建一个 ML 模型，使用交叉验证/提前停止，这样如果我给模型一个 60 x 6000 传感器数据，我就可以预测相应的应力矩阵。由于我想计算应力时间历史的循环次数，因此必须很好地预测带有噪声的应力预测。
我在识别模型的超参数时遇到了麻烦。我大概应该使用多少层？如果我得到的预测不能很好地预测循环计数，是否意味着无法为这种工程数据构建回归模型？
当达到 0.73 交叉验证分数时，我尝试过的大多数参数都无法收敛。此外，scikit learn MLP Regressor 不支持 GPU 使用，运行时间很长。]]></description>
      <guid>https://stackoverflow.com/questions/78738380/mlp-regressor-engineering-data-sklearn</guid>
      <pubDate>Fri, 12 Jul 2024 02:52:04 GMT</pubDate>
    </item>
    <item>
      <title>解决具有不同数据类型分类特征的 Keras 函数模型的类型转换错误</title>
      <link>https://stackoverflow.com/questions/78738365/resolving-type-conversion-error-for-keras-functional-model-with-categorical-feat</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78738365/resolving-type-conversion-error-for-keras-functional-model-with-categorical-feat</guid>
      <pubDate>Fri, 12 Jul 2024 02:44:28 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络预测值[关闭]</title>
      <link>https://stackoverflow.com/questions/78737493/forecasting-values-with-a-neural-network</link>
      <description><![CDATA[我正在使用神经网络进行氧气时间序列回归（TensorFlow 和 Keras），从 2023-08-25 00:02:12 到 2024-06-18 15:38:36，每 5 分钟获取一次氧气、饱和度、温度和盐度数据。
我的问题是，一旦我的神经网络经过训练和验证....我该如何使用它来预测我拥有的数据集之外的值？如果我希望我的网络预测接下来的 24、48 或 72 小时，如果它们不在我的验证数据集中，我该怎么做？
我已经将我的数据分为训练集和测试集，并且我已经对测试集进行了预测，但我总是需要一个“过去的数据集”。现在我想尝试预测步骤 t+24、t+48，但我不知道如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/78737493/forecasting-values-with-a-neural-network</guid>
      <pubDate>Thu, 11 Jul 2024 20:04:45 GMT</pubDate>
    </item>
    <item>
      <title>如何针对简单的 ML 模型对来自 EE 的卫星数据进行标准化/预处理？</title>
      <link>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</link>
      <description><![CDATA[我对 Earth Engine/QGIS 还不太熟悉（没有 ArcGIS 许可证），我想使用一个简单的 ML 模型，利用卫星 VCD、NDVI 和气象数据估算地面 O3。
我对 GIS/地理空间数据处理的世界感到迷茫，所以我尽我所能，疯狂地谷歌搜索并阅读了一些文章，以解释我的理由。
我想使用的数据：
EE 数据集：

Daymet V4 每日气候变量（https://developers.google.com/earth-engine/datasets/catalog/NASA_ORNL_DAYMET_V4#bands)
MOD13A2 NDVI 产品 (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A2)
Sentinel-5P (TROPOMI) O3 VCD 数据 (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_O3#bands)

标签：- 来自 EPA 的地面 O3 数据（https://epa.maps.arcgis.com/apps/webappviewer/index.html?id=5f239fd3e72f424f98ef3d5def547eb5&amp;extent=-146.2334,13.1913,-46.3896,56.5319），转到右上角的“选择图层”图标并选择 O3 活动/非活动，然后将鼠标悬停在任意点上

似乎这个 EPA 数据可以直接导出为 CSV，包括经度、纬度和臭氧测量值。

在这里，我假设我需要从不同来源提取的数据具有相同的空间/时间分辨率，以便使用一些简单的机器学习算法（RF/线性回归）。如果有其他方法，请告诉我！
在我设想的数据集中，每“行”数据将是给定像素在给定日期的气象变量值、NDVI 和 VCD 值，我可以对其执行基本的 RF/回归（使用 EE 或 Python）。在我看来，要使它发挥作用，所有数据集都需要就“像素”是什么达成一致，并成为/成为每日时间分辨率（Daymet 和 TROPOMI 已经是每日的，我假设我可以取最接近的 16 天 NDVI 值）。
基于这个假设，我想让所有数据都具有相同的空间分辨率，所以我正在尝试弄清楚如何“重新投影”将 TROPOMI 数据（当前分辨率为 1111.3km）转换为 1km 分辨率（据我所知，这是所有 Daymet 数据和 MODIS 数据的分辨率）。 *我不知道如何使来自 EPA 数据的“最近像素”（来自点数据而非栅格数据）匹配，以便将其用作数据标签，但这似乎是一个更常见的问题，因此在整理完其余部分后，我将四处寻找如何修复该问题。
因此，我想在这里完成的主要操作是标准化空间分辨率：将 TROPOMI 数据转换为 1km 像素或将 Daymet/MODIS 数据转换为 1.113km 像素。
我尝试在 Earth Engine 中可视化所有三个输入数据集（为 Daymet 选择最高温度），像素似乎根本没有对齐。我已在此处附上每个图层的屏幕截图：ndvi 像素、daymet 像素和 tropomi 像素
TROPOMI 数据似乎给出了某种奇怪的模糊像素，Daymet 数据是规则的方形像素但倾斜，而 NDVI 数据是平行四边形。我隐约觉得这与不同的“投影”/“CRS”有关设置，但我对这两者都不太了解，并且不确定如何继续我认为我需要做的重新缩放。
脚本链接：https://code.earthengine.google.com/6fafaccf040e206e97a32e795611d7e4]]></description>
      <guid>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</guid>
      <pubDate>Thu, 11 Jul 2024 16:43:51 GMT</pubDate>
    </item>
    <item>
      <title>检测图像中的算术运算符[关闭]</title>
      <link>https://stackoverflow.com/questions/78736359/detect-arithmetic-operators-in-an-image</link>
      <description><![CDATA[我使用 keras_ocr 创建了一个 OCR 脚本。输入是流程图（灰度图）。我想提取流程图图像的文本和形状坐标。但是，它不会提取诸如“+、-、*、/”之类的算术运算符。有时它也无法检测数值。这是我的完整脚本。
# 导入必要的库
import os
import matplotlib.pyplot as plt
import keras_ocr
import cv2
import numpy as np
from google.colab import drive
from symspellpy.symspellpy import SymSpell, Verbosity
import pkg_resources

class OCRProcessor:
def __init__(self):
# 创建用于 OCR 处理的管道
self.pipeline = keras_ocr.pipeline.Pipeline()

def __get_bbox(self, image_path):
try:
# 使用 OpenCV 读取图像
image = cv2.imread(image_path)
if image is None:
raise ValueError(f&quot;Image at path {image_path} could not be read.&quot;)

# 将图像转换为 RGB（keras-ocr 需要 RGB 图像）
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 使用 OCR 管道检测文本
images = keras_ocr.tools.read(image_path)
self.image = images
prediction_groups = self.pipeline.recognize([images])

if not prediction_groups or not prediction_groups[0]:
return [], []

# 提取边界框和文本
texts = []
results = []
for text, box in prediction_groups[0]:
texts.append(text)
xs, ys = set(), set()
for x in box:
xs.add(x[0])
ys.add(x[1])
results.append(list(map(int, [min(xs), min(ys), max(xs), max(ys)]))) # ymin, xmin, ymax, xmax

return texts, results
except Exception as e:
print(f&quot;An error occurred in __get_bbox: {e}&quot;)
return [], []

def process_image(self, image_path):
return self.__get_bbox(image_path)

# 定义函数来更正文本
def correct_text(text_array):
# 初始化 SymSpell 对象
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# 加载字典
dictionary_path = pkg_resources.resource_filename(
&quot;symspellpy&quot;, &quot;frequency_dictionary_en_82_765.txt&quot;)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

corrected_text_array = []
for text in text_array:
suggestions = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)
if suggestions:
corrected_text_array.append(suggestions[0].term)
else:
corrected_text_array.append(text)
return corrected_text_array

# 主函数
def main(image_path):
ocr_processor = OCRProcessor()
ex_text, ex_co = ocr_processor.process_image(image_path)
if not ex_text:
print(f&quot;在路径 {image_path} 处的图像中未检测到文本。&quot;)
return [], [], []

# 更正提取的文本
cr_text = correct_text(ex_text)

# 打印结果
print(&quot;提取的文本：&quot;, ex_text)
print(&quot;更正的文本：&quot;, cr_text)
print(&quot;Extracted Coordinates:&quot;, ex_co)

return ex_text, cr_text, ex_co

# 示例用法（您可以根据需要更新图像路径）
image_path = &#39;/content/Test2.jpg&#39;
ex_text, cr_text, ex_co = main(image_path)

ex_shape, ex_coor = detect_shapes(image_path)

# 打印或使用结果
print(&quot;Detected Shapes:&quot;, ex_shape)
print(&quot;Coordinates for Shapes:&quot;, ex_coor)

当我输入如下所示的流程图图像时，

它会生成一个文本和坐标，如下：
{{&#39;start&#39;,&#39;input&#39;,&#39;as&#39;,&#39;a&#39;,&#39;product&#39;,&#39;axe&#39;,&#39;a&#39;,&#39;print&#39;,&#39;product&#39;,&#39;end&#39;}}

问题是它无法检测到某些算术运算符。有没有什么解决方案可以解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/78736359/detect-arithmetic-operators-in-an-image</guid>
      <pubDate>Thu, 11 Jul 2024 15:09:46 GMT</pubDate>
    </item>
    <item>
      <title>在图像去噪方面，验证中的 SSIM 高于训练中的 SSIM [关闭]</title>
      <link>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</link>
      <description><![CDATA[我正在使用 2D U-Net 对显微镜图像进行去噪。我正在使用在不同 z 级别拍摄的图像训练我的网络，这些图像具有基本事实，即 z 中图像的平均值。因此，一些图像具有相同的基本事实。我将图像分成 z 组，以将它们放在训练、验证或测试集中。我正在计算每个批次的 SSIM，并在每个时期平均结果，然后绘制它们。
我面临的问题是验证中的 SSIM 总是高于训练中的 SSIM。
我没有在我的网络中使用 dropout 或批量标准化，这会导致验证具有更高的结果。我确保训练和验证中的图像没有重复。
训练和验证中的 SSIM
为什么会发生这种情况以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</guid>
      <pubDate>Thu, 11 Jul 2024 13:18:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Catboost Golang</title>
      <link>https://stackoverflow.com/questions/78732455/use-catboost-golang</link>
      <description><![CDATA[如何在 golang 上使用 CatBoost？也许有解决方案？
我尝试使用 https://github.com/bourbaki/catboost-go 但我一直收到 CGO 错误。
当我尝试使用 go run 运行它时，它会在编译时出错
usr/local/go/pkg/tool/linux_amd64/link：运行 gcc 失败：退出状态 1
/usr/bin/ld：/tmp/go-link-23536551/000001.o：在函数“_cgo_e59e54336bq_Cfunc”中：
/tmp/go-build/cgo-gcc-prolog:69：对“my_function”的未定义引用
collect2：错误：ld 返回 1 退出状态
]]></description>
      <guid>https://stackoverflow.com/questions/78732455/use-catboost-golang</guid>
      <pubDate>Wed, 10 Jul 2024 19:40:07 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
# NN 振荡器系统的实现
def rocks_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

# 2nd 导数的嵌套循环
with tf.GradientTape() as outer_tape:
outer_tape.watch(t)

with tf.GradientTape() as inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 1st导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

# 边界损失
bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

# 可学习参数 mu 传递给 ODE
ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

# 超参数 lambda1 的数据损失
data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

# 带有数据丢失的训练程序
def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

waiting = 200
best = float(&#39;inf&#39;)

early_stop = 0

for itr in range(epochs):
with tf.GradientTape() as tape:
#tape.watch(mu)
train_loss, mu = rocksor_system_data_loss(train_t, NN，func，params，mu，bc，data_t，data_u_noised，lambda1)
train_loss_record.append(train_loss)

grad_w = tape.gradient(train_loss，NN.trainable_variables + [mu])
optm.apply_gradients(zip(grad_w，NN.trainable_variables + [mu]))

if itr in loss_tracker:
print(train_loss.numpy())
print(mu.numpy())
plot_epochs_with_noise(train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN)

mu_list.append(mu.numpy()) 

# 提前停止
# wait += 1
# if train_loss.numpy() &lt; best:
# best = train_loss.numpy()
# wait = 0
# if wait &gt;= waiting:
# print(f&quot;在迭代 {itr} 时停止，损失为 {train_loss_record[itr]}。&quot;)
# early_stop = itr
# break

return train_loss_record, mu_list, early_stop

# 用于 ODE 损失计算/最小化
NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Android Studio 中集成已训练的模型</title>
      <link>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio</guid>
      <pubDate>Wed, 10 Jul 2024 11:31:14 GMT</pubDate>
    </item>
    <item>
      <title>CNN 预测随机图像</title>
      <link>https://stackoverflow.com/questions/78723722/cnn-predicting-random-images</link>
      <description><![CDATA[我已经用 CNN 训练了模型。我已经用胸部 X 光片图像对模型进行了二元分类训练。即使在训练模型之后。模型正在预测汽车、动物等随机图像，并给出更高的置信度。
需要深度学习模型中随机图像预测的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78723722/cnn-predicting-random-images</guid>
      <pubDate>Tue, 09 Jul 2024 04:49:05 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“experimental”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整数据集的大小和比例，如下所示，但遇到了此错误：
AttributeError：模块“keras.layers”没有属性“experimental”

resize_and_rescale= tf.keras.Sequential([
layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),
layers.experimental.preprocessing.Rescaling(1.0/255)
])

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    </channel>
</rss>