<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 05 Nov 2024 09:17:47 GMT</lastBuildDate>
    <item>
      <title>拟合 lightgbm 回归模型时日期时间中的对象类型错误</title>
      <link>https://stackoverflow.com/questions/79158044/object-type-error-in-datetime-while-fitting-lightgbm-regressor-model</link>
      <description><![CDATA[我想进行库存预测，使用正则表达式提取后数据如下所示。在此处输入图片说明
更重要的是，我花了太长时间执行这个模型（数据集大小为 4132338x10）并得到两个响应
提高值错误...
在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/79158044/object-type-error-in-datetime-while-fitting-lightgbm-regressor-model</guid>
      <pubDate>Tue, 05 Nov 2024 07:31:26 GMT</pubDate>
    </item>
    <item>
      <title>Weka RandomForest m_Classifiers 为空且仅在构建 Spring Boot 项目后才出现 NullPointerException 和无法找到允许的类错误</title>
      <link>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</link>
      <description><![CDATA[我正在开发一个 Spring Boot 项目，我使用 Weka 的 RandomForest 模型并使用从 PostgreSQL 数据库检索的数据对其进行训练。当我正常启动项目（IDE 中的运行命令）时，项目运行完美，所有预测都正常工作。但是，当我构建项目然后尝试运行它时，我遇到了以下两个错误：
错误 1
错误 2
我想知道为什么这些错误只在构建项目后出现。可能是什么原因造成的？我该如何解决？
问题：
我通过调用 buildClassifier 来训练 RandomForest 模型，它在常规运行中训练成功。
但是，在构建项目后，我在 classifyInstance 方法调用期间收到 m_Classifiers 为空错误，这表明分类器数组未初始化。此外，在构建环境中找不到 RandomTree，尽管它在正常运行期间可用。
我尝试过的解决方案：
验证 trainingSet 和 testSet 是否包含实例（它们不为空）。
确认在常规运行中成功使用 buildClassifier 训练模型。
使用 values[2] = Double.NaN 设置一个空的（缺失的）acc 类值。
通过 Maven 在 pom.xml 中添加了 Weka 依赖项（weka-stable，版本 3.8.6）。
问题：
为什么这些错误只在构建项目后出现？如何解决 m_Classifiers 为空且从构建运行时找不到 RandomTree 的问题？
其他信息：
Weka 版本：3.8.6
Java 版本：1.8
使用 Spring Tool Suite (STS)，并通过 Maven 添加 Weka 依赖项。
如果您能提供任何有关这些问题发生原因以及如何解决的建议或见解，我们将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</guid>
      <pubDate>Tue, 05 Nov 2024 07:17:06 GMT</pubDate>
    </item>
    <item>
      <title>GAN 图像生成给出疯狂的输出</title>
      <link>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</link>
      <description><![CDATA[我正在尝试使用 GAN 创建一个生成器网络，该网络可以将 32x32 图像转换为 128x128 图像。我最初训练了一个 CNN，并取得了一些成功：

为了获得更准确的结果，我使用了一种 GAN 架构，该架构为生成器网络实现了更低的损失分数。然而，当我尝试进行预测时，我收到的图像如下所示：

似乎生成器网络正在寻找某种方法来打破鉴别器网络。我该如何解决这个问题，并找到一种让生成器真正生成高清图像的方法？
这是我的 GAN 的样子：
generator = UpsamplingCNN().to(device)
discriminator = Discriminator().to(device)

criterion = nn.BCELoss()

## 不同的学习率
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))

num_generator_updates = 5
lambda_gp = 10
num_epochs = 50 
## 标签平滑
real_label = 0.9
fake_label = 0.0

for epoch in range(num_epochs):
generator.train()
discriminator.train()

for i, (low_res, high_res) in enumerate(tqdm(train_loader)):
# 将数据移动到正确的设备
low_res = low_res.to(device)
high_res = high_res.to(device)

# 为真实图像和虚假图像添加噪声
noise_std_dev = 0.05
noisy_real_images = high_res + noise_std_dev * torch.randn_like(high_res).to(device)
noisy_real_images = torch.clamp(noisy_real_images, 0.0, 1.0)

fake_images = generator(low_res)
noisy_fake_images = fake_images + noise_std_dev * torch.randn_like(fake_images).to(device)
noisy_fake_images = torch.clamp(noisy_fake_images, 0.0, 1.0)

# 真标签和假标签
output_real = discriminator(noisy_real_images).view(-1)
labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)
loss_real = criterion(output_real, labels_real)

# 使用嘈杂的假图像进行训练
output_fake = discriminator(noisy_fake_images.detach()).view(-1)
labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)
loss_fake = criterion(output_fake, labels_fake)

# 计算梯度惩罚
gradient_penalty = compute_gradient_penalty(discriminator, high_res, fake_images, device)

# 带梯度惩罚的鉴别器总损失
loss_D = loss_real + loss_fake + lambda_gp * gradient_penalty
loss_D.backward()

optimizer_D.step()

## 多个生成器更新
for _ in range(num_generator_updates):
optimizer_G.zero_grad()

# 生成假图像并计算生成器的损失
fake_images = generator(low_res)
output_fake_for_G = discriminator(fake_images).view(-1)
labels_fake_for_G = torch.full((output_fake_for_G.size(0),),real_label, dtype=torch.float, device=device)
loss_G = criterion(output_fake_for_G, labels_fake_for_G)
loss_G.backward()

optimizer_G.step()

if i % 10 == 0:
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], &#39;
f&#39;Loss_D: {loss_real.item() + loss_fake.item():.4f}, Loss_G: {loss_G.item():.4f}&#39;)

我还对判别器使用了梯度惩罚，如下所示：
def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):
alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)
interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
d_interpolates = discriminator(interpolates)
fake = torch.ones(d_interpolates.size(), device=device, require_grad=False)

gradients = torch.autograd.grad(
output=d_interpolates,
input=interpolates,
grad_outputs=fake,
create_graph=True,
retain_graph=True,
only_inputs=True
)[0]

gradients = gradients.view(gradients.size(0), -1)
gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
return gradient_penalty
]]></description>
      <guid>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</guid>
      <pubDate>Tue, 05 Nov 2024 03:55:19 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 SAM2 来追踪类似的物体吗？</title>
      <link>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</link>
      <description><![CDATA[有没有什么方法（或资源）可以使用 SAM2 来跟踪视频中的统一对象。
我发现 SAM2 可以更快地进行分割，我想用它来检测/分割和跟踪我的数据集上只有 1 个类（自定义类：0，统一对象）的对象。
可用的 SOTA 跟踪器（如 sort、botsort、bytetrack 和 deepsort）无法跨帧跟踪对象（但检测表现良好）]]></description>
      <guid>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</guid>
      <pubDate>Tue, 05 Nov 2024 03:50:39 GMT</pubDate>
    </item>
    <item>
      <title>将位图转换为黑白以进行 Google MLKit 文本识别是否更好</title>
      <link>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</link>
      <description><![CDATA[我正在使用 Google MLKit 进行文本识别，效果很好，但似乎很难识别彩色图像中的文本。我想知道是否最好将图像转换为灰度，或者在使用 ML 阅读器扫描之前我可以进行任何其他优化。似乎 Google 的 Google 镜头比普通的 ML 套件阅读器效果更好，所以他们一定在做其他事情。]]></description>
      <guid>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</guid>
      <pubDate>Tue, 05 Nov 2024 02:58:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何处理重叠数据</title>
      <link>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</link>
      <description><![CDATA[
我正在创建一个机器学习模型来确定用户是否是机器人，我使用 seaborn 绘制了配对图并意识到大多数数据是重叠的。下面是我为标准化、拆分和部署模型编写的代码。该图显示了该模型在超过 40000 个样本的情况下的表现。如您所见，模型正在猜测，我正在尝试找出原因。


 X = new_df[[&#39;Retweet Count&#39;, &#39;Mention Count&#39;, &#39;Follower Count&#39;, &#39;Tweet&#39;, &#39;Hashtags&#39;, &#39;Verified&#39;, &#39;Created At&#39;]]
y = new_df[[&#39;Bot Label&#39;]].values

y = y.ravel() # 确保 y 是一维数组而不是二维数组

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

Scaler = StandardScaler()
X_train_scaled = Scaler.fit_transform(X_train)
X_test_scaled = Scaler.transform(X_test)

从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 Confusion_matrix

rfc = RandomForestClassifier(n_estimators = 1000)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

]]></description>
      <guid>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</guid>
      <pubDate>Tue, 05 Nov 2024 01:41:31 GMT</pubDate>
    </item>
    <item>
      <title>行业 RAG 技术 [关闭]</title>
      <link>https://stackoverflow.com/questions/79157397/industries-rag-technology</link>
      <description><![CDATA[RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？
RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？在临床环境中可以实施哪些具体用例来改善决策？]]></description>
      <guid>https://stackoverflow.com/questions/79157397/industries-rag-technology</guid>
      <pubDate>Tue, 05 Nov 2024 00:54:23 GMT</pubDate>
    </item>
    <item>
      <title>我的自定义 layernorm 函数有什么问题？</title>
      <link>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</link>
      <description><![CDATA[import numpy as np
import torch
import torch.nn. functional as F

def layer_norm(x, weight, bias, eps=1e-6):
# x 形状：[bs, h, w, c]
# 计算空间维度（高度、宽度）的平均值和方差
mean = np.mean(x, axis=(1, 2), keepdims=True) # 形状：(batch_size, 1, 1, channels)
var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0 表示有偏方差

# 标准化
x_normalized = (x - mean) / np.sqrt(var + eps)

# 应用权重和偏差
out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
return out

def test1(x):
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
weight = np.ones(channels)
bias = np.zeros(channels)

normalized_output = layer_norm(x, weight, bias)
return normalized_output

def test2(x):
global channels
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
x_tensor = torch.tensor(x, dtype=torch.float32)
weight = torch.ones(channels)
bias = torch.zeros(channels)

# 使用 PyTorch 的层规范，在最后一个维度（通道）上进行规范化
normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
return normalized_output.detach().numpy()

# 测试
batch, channels, height, width = 4, 3, 8, 8
# 生成随机输入
x = np.random.randint(-10, 10, (batch, channels, height, width))

# 计算两种实现的输出
layernorm1 = test1(x)
layernorm2 = test2(x)

# 检查输出是否接近
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close) # 如果它们足够接近，则应输出 True

var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0对于有偏方差
var = np.var(x, axis=(1, 2), keepdims=True)

我的期望是 are_close==True,，这意味着 layernorm1 和 layernorm2 的距离非常小。由于 layernorm1 和 layernorm2 的形状很大，所以我会显示 layernorm1 和 layernorm2 的部分结果。 layernorm1[0,0,0:3,0:4] 数组([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1 .84752351]]) layernorm2[0,0,0:3,0:4] 数组([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)
我尝试了带或不带 ddof=0 的 variacne 方法，打印语句中全部为 False。
我想知道如何实现类似于 Pytorch 内置 layernorm 函数的自定义 layernorm。
从代码角度看 layernorm 步骤是什么？
关于计算机视觉，layernorm 对特征图有什么作用？]]></description>
      <guid>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</guid>
      <pubDate>Mon, 04 Nov 2024 22:07:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Android Studio 制作一个仅扫描姓名和身份证号码的身份证扫描器？[关闭]</title>
      <link>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</link>
      <description><![CDATA[我是一名学生，这将是我论文的系统。我们将实施一个基于移动设备的 OCR 扫描仪系统，该系统只能读取残障人士和老年人的身份证。我对如何做到这一点有一些想法，但我认为我需要更多的输入和逻辑。我正在考虑使用 Google ML Kit 进行机器学习和文本识别，并使用 TensorFlow 进行身份证的对象检测。
我的问题是，我国每个城市的残障人士和老年人身份证的格式和外观都不同，但我们没有时间收集所有这些信息。除了在对象检测中添加关键字（例如“老年人”、“残疾”等）以启动扫描仪外，是否有可能创建一个可以识别残障人士和老年人身份证的系统，而无需收集我国每种类型的老年人和残障人士身份证来开发算法？]]></description>
      <guid>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</guid>
      <pubDate>Mon, 04 Nov 2024 14:34:52 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>layoutlmv3：尽管已完成推理，但 postprocess 方法仍未返回超过 512 个标记的数据</title>
      <link>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</guid>
      <pubDate>Mon, 14 Aug 2023 13:24:26 GMT</pubDate>
    </item>
    <item>
      <title>将 .ckpt 转换为 .h5</title>
      <link>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</link>
      <description><![CDATA[我已经使用 resnet18 训练模型进行 mask R-CNN 检测。对于每个 epoch，它只创建一个“.ckpt”文件。
现在我想使用该 .ckpt 文件作为检测图像的检测器。我有使用“.h5”文件进行检测的 Python 代码。
请帮助我如何使用“.ckpt”文件进行检测。或者我如何将其转换为“.h5”？
谢谢
我曾尝试在训练过程中生成“.h5”文件而不是“.ckpt”，但对我来说不起作用。
现在我需要一种方法来使用“.ckpt”文件来检测图像中的对象。]]></description>
      <guid>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</guid>
      <pubDate>Thu, 01 Dec 2022 10:50:58 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于回归的神经网络？</title>
      <link>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</link>
      <description><![CDATA[我正在尝试使用 Keras 来构建神经网络。我使用的数据是 https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics。我的代码如下：
import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r&quot;&quot;&quot;file location&quot;&quot;&quot;, delimiter=&#39;,&#39;)

model = Sequential()
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics =[&#39;accuracy&#39;])

Y = data[:,-1]
X = data[:,:-1]

从这里我尝试使用model.fit(X,Y)，但模型的准确性似乎保持在 0。我是 Keras 的新手，所以这可能是一个简单的解决方案，提前致歉。
我的问题是，向模型添加回归以提高准确性的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</guid>
      <pubDate>Tue, 27 Feb 2018 11:53:38 GMT</pubDate>
    </item>
    </channel>
</rss>