<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 19 Jul 2024 18:20:00 GMT</lastBuildDate>
    <item>
      <title>神经网络训练经过几个阶段后，准确率的提升变得非常缓慢</title>
      <link>https://stackoverflow.com/questions/78770508/accuracy-improving-gets-so-slow-after-some-epoches-in-neurel-network-training</link>
      <description><![CDATA[我有大约 7000 万个样本来训练神经网络模型，准确率提高得非常好，并且很快，直到 25-30 个 epoch 左右，25-30 个 epoch 之后它就变得很慢了。
例如

epoch 7：损失：5.1151 - 准确率：0.1055
epoch 18：损失：2.9058 - 准确率：0.1516
epoch 26：损失：2.9018 - 准确率：0.2466
epoch 30：损失：2.9091 - 准确率：0.2615
epoch 56：损失：2.7810 - 准确率：0.2732

是不是因为我的学习率，或者模型对于这种训练来说太简单了？
这是我的模型参数：
input_neurons = 65 
output_neurons = 4880
hidden_​​layers = 3
hidden_​​neurons = 256
epochs = 100
batch_size = 8132
learning_rate = 0.001

这是我的模型：
with strategies.scope():
# 模型
model = keras.Sequential([
keras.layers.Input(shape=(input_neurons,)),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(output_neurons,activation=&#39;softmax&#39;)])

optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.summary()

我用这个代码训练它：
def data_generator():
for x, y in dataset:
Yield x.numpy(), y.numpy()

steps_per_epoch = 69820098 // batch_size

history = model.fit(
data_generator(),
epochs=epochs,
steps_per_epoch=steps_per_epoch,
verbose=1)
]]></description>
      <guid>https://stackoverflow.com/questions/78770508/accuracy-improving-gets-so-slow-after-some-epoches-in-neurel-network-training</guid>
      <pubDate>Fri, 19 Jul 2024 17:05:47 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种机器学习技术来解决我的序列问题？</title>
      <link>https://stackoverflow.com/questions/78770366/which-machine-learning-technique-should-i-use-to-solve-my-sequence-problem</link>
      <description><![CDATA[我有一组如下数据：



列 A
列 B
列 C
列D




汤姆
星期一
蓝色
0,3,20,36,80,98,100


丹
星期五
红色
0,15,45,100


莎拉
星期日
R ed
0,6,31,91,100


Dan
星期一
黄色
0,21,86,100


Tom
星期四
红色
0,12,50,70,89,100



（注意D 列中的所有序列都从 0 开始，到 100 结束，序列只能向上，可以是任意长度）
我还有另一组数据，如下所示：



A 列
B 列
C 列
列D




莎拉
星期日
黄色
0,10,15,51,65


莎拉
星期一
蓝色
0,19,34,56


汤姆
星期日
红色
0,8,11,15,22,28,40,60,71


汤姆
星期六
红色
0,1,23,44,89


丹
星期二
绿色
0,27,81



（请注意，D 列中的所有序列都从 0 开始，以 &lt; 结尾100，序列只能向上，可以是任意长度）
我想尝试预测第二个数据集的剩余序列，以便每个序列达到 100。我希望从这个小样本中捕捉到 Dan 的序列可能比 Tom 短，并且在底部两行中 Dan 可能比 Tom 更快达到 100，即使他只有 81 而 Tom 有 89。D 列中的两个相同序列应该根据其他列中的值具有不同的预测。
这只是一个例子，实际数据有更多的列，每个人在每个集合中都有数百/数千行。
我对 ML 比较陌生，只是想指导哪种技术最适合/适合解决这个问题。我研究了一些序列学习技术，但没有找到任何试图准确解决这个问题的方法。
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78770366/which-machine-learning-technique-should-i-use-to-solve-my-sequence-problem</guid>
      <pubDate>Fri, 19 Jul 2024 16:24:19 GMT</pubDate>
    </item>
    <item>
      <title>我的 RandomForestRegressor 上的 MAE 和 MSE 非常高</title>
      <link>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</link>
      <description><![CDATA[我是机器学习的新手，我得到了一个航班预测数据集，我想试试我的技能。
我清理了数据，修复了一些新功能，删除了其他功能
我还得到了一些有价值的数据。但当我尝试进行预测并评估我的模型时
这就是我得到的答案！那是在我使用 SearchGridCV 调整模型之后
测试集上的回归指标
r2：82.10%
mean_absolute_error：1229.1407307097613
mean_squared_error：2933265.159841384
model = RandomForestRegressor(
max_depth=20,
max_features=&#39;sqrt&#39;,
min_samples_leaf=2,
min_samples_split=5,
n_estimators=200
)

X = df.drop(&#39;Price&#39;,axis=1)
y = df[&#39;Price&#39;]

X_train, X_test, y_train, y_test = train_test_split(
pd.get_dummies(X)
, y, test_size=0.2, random_state=42)

model.fit(X_train,y_train)
y_preds = model.predict(X_test)

我尝试修改超参数并删除一些异常值
def remove_outliers_iqr(df, column):
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
return df[(df[column] &gt;= lower_bound) &amp; (df[column] &lt;= upper_bound)]

numerical_columns = [&#39;Price&#39;, &#39;Dep_hours&#39;, &#39;Dep_min&#39;, &#39;Arrival_hours&#39;, &#39;Arrival_min&#39;, &#39;Duration_hours&#39;, &#39;Duration_min&#39;]
for column in numeric_columns:
df = remove_outliers_iqr(df, column)

但我仍然得到相同的结果
这是完整的笔记本，因为我不知道如何以笔记本的方式在此处附加整个代码
https://github.com/jamhus/ztm-course/blob/master/fligt%20prices%20analysis/flight%20prices.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</guid>
      <pubDate>Fri, 19 Jul 2024 15:47:43 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 Python 中进程以退出代码 -1073741819（0xC0000005）结束？[关闭]</title>
      <link>https://stackoverflow.com/questions/78770006/how-to-solve-process-finished-with-exit-code-1073741819-0xc0000005-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78770006/how-to-solve-process-finished-with-exit-code-1073741819-0xc0000005-in-python</guid>
      <pubDate>Fri, 19 Jul 2024 14:50:36 GMT</pubDate>
    </item>
    <item>
      <title>如果 sum(y_true)=1，应该使用哪个损失函数？</title>
      <link>https://stackoverflow.com/questions/78769858/which-loss-function-should-be-used-if-sumy-true-1</link>
      <description><![CDATA[我的 yTrue 基本上类似于 [.2,.8]，但从不为 [1,0] 或 [0,1]
sum(yTrue)=1 始终
我尝试了 CategoricalCrossentropy，但发生了 TypeError-

TypeError：预期为 float32，但在 0x7752c1300580 处获得了 &lt;keras.src.losses.losses.CategoricalCrossentropy 对象&gt; 类型为“CategoricalCrossentropy”。
]]></description>
      <guid>https://stackoverflow.com/questions/78769858/which-loss-function-should-be-used-if-sumy-true-1</guid>
      <pubDate>Fri, 19 Jul 2024 14:15:08 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法识别的关键字参数：['batch_shape'] [关闭]</title>
      <link>https://stackoverflow.com/questions/78769397/valueerror-unrecognized-keyword-arguments-batch-shape</link>
      <description><![CDATA[尝试加载我在 kaggle 上训练的模型，保存并下载模型后，我遇到了这个问题，我无法在本地系统上加载模型，但当我尝试在 kaggle 上执行此操作时，它就可以正常工作
帮我解决这个问题，我一直在尝试解决这个问题很长时间，但我做不到]]></description>
      <guid>https://stackoverflow.com/questions/78769397/valueerror-unrecognized-keyword-arguments-batch-shape</guid>
      <pubDate>Fri, 19 Jul 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>在 Airflow dag 中加载 Joblib 模型</title>
      <link>https://stackoverflow.com/questions/78769379/loading-joblib-model-in-airflow-dag</link>
      <description><![CDATA[我尝试在 airflow dag 中使用 joblib，加载模型文件时，我运行 joblib.load(model_name)，但出现错误
 文件 &quot;/usr/local/lib/python3.9/pickle.py&quot;，第 331 行，在 _getattribute 中
raise AttributeError(&quot;无法在 {!r} 上获取属性 {!r}&quot;
AttributeError: 无法从 &#39;/home/***/.local/bin/***&#39;&gt; 获取 &lt;module &#39;__main__&#39; 上的属性 &#39;MyCustomModel&#39;

我的自定义模型已导入 dag 中，我也尝试在任务函数中导入，但我总是收到此错误，即 joblib 库找不到我需要解开模型的自定义类。我认为这是因为 airflow 运行时没有像我预期的那样处理导入或 python 命名空间。其他人是否尝试过加载python dag 中的 pickles 或 joblib 模型？或者有什么方法可以强制运行时导入此自定义类？]]></description>
      <guid>https://stackoverflow.com/questions/78769379/loading-joblib-model-in-airflow-dag</guid>
      <pubDate>Fri, 19 Jul 2024 12:26:48 GMT</pubDate>
    </item>
    <item>
      <title>Python 代码与网站交互并收集数据的方式？[关闭]</title>
      <link>https://stackoverflow.com/questions/78768755/ways-for-python-code-to-interact-with-websites-and-gather-data</link>
      <description><![CDATA[我需要收集数据，然后将其输入到 NLT 过滤器中，而该项目要求我从文章中收集数据（URL 和文本）。有没有办法用 Python 或任何编程语言来实现这一点？
目前，我有两个想法 - 创建一个使用屏幕上的坐标来管理这些操作的系统，然后尝试在指定的坐标处输入/提取数据，以及我训练的用于定位所述输入/提取字段的算法（ai？）。我仍然需要进一步研究这些，我非常乐于接受建议。]]></description>
      <guid>https://stackoverflow.com/questions/78768755/ways-for-python-code-to-interact-with-websites-and-gather-data</guid>
      <pubDate>Fri, 19 Jul 2024 10:05:02 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 与 XGBoost 的领先一步预测：为什么此代码中的 LSTM 表现如此糟糕？</title>
      <link>https://stackoverflow.com/questions/78768566/lstm-vs-xgboost-for-one-step-ahead-predictions-why-does-lstm-in-this-code-perf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78768566/lstm-vs-xgboost-for-one-step-ahead-predictions-why-does-lstm-in-this-code-perf</guid>
      <pubDate>Fri, 19 Jul 2024 09:19:23 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器，网格搜索</title>
      <link>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</guid>
      <pubDate>Fri, 19 Jul 2024 09:07:04 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 sklearn 序数编码器完成的映射？</title>
      <link>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</link>
      <description><![CDATA[我使用 sklearn 对数据集的两列进行了序数编码
我想知道哪一列映射到哪一列
假设 0 映射到两列的什么位置
我想知道语法，尝试询问 gpt 但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</guid>
      <pubDate>Fri, 19 Jul 2024 08:02:11 GMT</pubDate>
    </item>
    <item>
      <title>从 TMT（螺纹铣刀测试）报告中提取文本 [关闭]</title>
      <link>https://stackoverflow.com/questions/78767922/extracting-text-from-an-tmt-thread-mill-test-report</link>
      <description><![CDATA[我有一项任务，需要从表格的 TMT PNG 图像中提取特定值。根据所需的输出，我需要从表格单元格中提取特定值或从图像中的报告中提取一些文本。
您能否建议现有的机器学习模型和任何能够从图像中的特定位置提取文本、数字或特殊符号并将其输出到 Excel 文件中的库？此外，请将过程分解为分步任务以实现所需的输出。
目前，我已经开始研究库、
Oytesseract、opencv-python-headless、Pandas、Pillow 和 Openpyxl。
!sudo apt-get install tesseract-ocr
我需要知道是否有任何方法可以探索，以及我应该学习什么来实现这些新方法，并解决这个问题，请向我解释，以便我可以对其进行研究。]]></description>
      <guid>https://stackoverflow.com/questions/78767922/extracting-text-from-an-tmt-thread-mill-test-report</guid>
      <pubDate>Fri, 19 Jul 2024 06:53:53 GMT</pubDate>
    </item>
    <item>
      <title>尽管数据类型为数字且形状正确，KNNImputer 仍会删除列</title>
      <link>https://stackoverflow.com/questions/78767192/knnimputer-drops-columns-despite-of-numeric-datatypes-and-right-shape</link>
      <description><![CDATA[我正在使用 KNNImputer 在几个 pd.DataFrame 中估算 np.nan 值。我检查了每个数据框的所有数据类型都是数字。但是，KNNImputer 在某些数据框中删除了一些列：
&gt;&gt;&gt;input_df.shape 
(816, 216) 

&gt;&gt;&gt; input_df.dtypes.value_count()
float64 216
dtype: int64

&gt;&gt;output_df.shape 
(816, 27)

我使用了以下 KNNImputer 配置
imputer = KNNImputer(n_neighbors=1, 
weights=&quot;uniform&quot;,
add_indicator=False)

output_df = imputer.fit_transform(input_df)

我想知道为什么会发生这种情况，因为每个数据框都有 np.nan 值。顺便说一句，参数 n_neighbors=1 不应该对结果产生任何影响，因为我正在用最近邻居的值替换缺失值。]]></description>
      <guid>https://stackoverflow.com/questions/78767192/knnimputer-drops-columns-despite-of-numeric-datatypes-and-right-shape</guid>
      <pubDate>Fri, 19 Jul 2024 01:17:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Intel Iris 在训练 Yolov8 模型方面表现优于 RTX 3050 笔记本电脑 GPU [关闭]</title>
      <link>https://stackoverflow.com/questions/78766852/why-the-intel-iris-performs-better-than-rtx-3050-laptop-gpu-in-training-a-yolov8</link>
      <description><![CDATA[我有一台戴尔 G-15 游戏笔记本电脑，配备 Core i5 12500h 和 Rtx 3050。我是深度学习和人工智能的新手。我正在使用 Yolo v8 训练一个模型，用于自动车牌检测，其自定义数据集包含 28k 张图像。
当我尝试训练模型时，我遇到了一个奇怪的问题。当我使用我的 集成 GPU 时，训练速度非常快，大约 5-6it/s，但当我切换到 Rtx 3050 时，性能下降到 3-4it/s。我在 Tensorflow 中也遇到了这个问题。




我不明白为什么集成 GPU 更快。专用 GPU 不是应该更快吗？
此外，当使用专用 GPU 时，其使用率为 100%，但当使用集成 GPU 时，其使用率仅为 30-40%。为什么？
我对这种行为一无所知。我的所有驱动程序都是最新的。


]]></description>
      <guid>https://stackoverflow.com/questions/78766852/why-the-intel-iris-performs-better-than-rtx-3050-laptop-gpu-in-training-a-yolov8</guid>
      <pubDate>Thu, 18 Jul 2024 22:10:21 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正 Reshape 函数中的错误</title>
      <link>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</link>
      <description><![CDATA[当我拟合各种人工神经网络时，TLNN 的代码显示不正确，尤其是包含重塑函数的行。这有什么问题吗？更改数据集是否意​​味着重塑函数不起作用并显示此错误
def Forecast_TLNN(model, time_lagged_points, last_sequence, Future_steps):
Forecasted_values = []
max_lag = max(time_lagged_points)
for i in range(future_steps):
input_sequence = [last_sequence[max_lag - p] for p in time_lagged_points]
Forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
Forecasted_values.append(forecasted_value[0][0])
last_sequence = last_sequence[1:] + [forecasted_value[0][0]]
return Forecasted_values

错误显示在以下行中：forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
我似乎无法在互联网上找到有关此代码的任何更正。
 ---------------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[83], line 13
10 # look_back, hidden_​​nodes, output_nodes, epochs, batch_size, future_steps
11 parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]
---&gt; 13 RMSE_info = compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)

单元格 In[79]，第 6 行，在 compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps) 中
3 information_FNN_df = get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters_FNN, scaler)
4 optimal_params_FNN = analyze_results(information_FNN_df, test_rainfall_data, &#39;FNN&#39;)
----&gt; 6 information_TLNN_df = get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters_TLNN, scaler)
7 optimal_params_TLNN = analyze_results(information_TLNN_df, test_rainfall_data, &#39;TLNN&#39;)
9 information_SANN_df = get_accuracies_SANN(rainfall_data, test_rainfall_data, parameters_SANN, scaler)

单元格 In[55]，第 21 行，在 get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters, scaler)
18 batch_size = param[4]
19 future_steps = param[5]
---&gt; 21 model_TLNN, Forecasted_values_TLNN = TLNN(rainfall_data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
23 y_true = test_rainfall_data.iloc[:future_steps].Precipitation
24 mse, mae, mape, rmse = calculate_performance(y_true, Forecasted_values_TLNN)

单元格 In[53]，第 9 行，在 TLNN(data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
6 model_TLNN = train_model(model_TLNN, X_train, y_train, epochs, batch_size)
8 max_lag = max(time_lagged_points)
----&gt; 9 预测值_TLNN = 预测值_TLNN(model_TLNN, time_lagged_points, 
10 列表(数据[-max_lag:]), 未来步骤=未来步骤)
11 预测值_TLNN = 列表(scaler.inverse_transform([预测值_TLNN])[0])
13 返回 model_TLNN, 预测值_TLNN

单元格 In[51]，第 6 行，在预测值_TLNN(模型, time_lagged_points, last_sequence, 未来步骤)
4 for i in range(future_steps):
5 输入序列 = [last_sequence[max_lag - p] for p in time_lagged_points]
----&gt; 6 Forecasted_value = model.predict((np.reshape(input_sequence, (1, len(input_sequence)))))
7 Forecasted_values.append(forecasted_value[0][0])
8 last_sequence = last_sequence[1:] + [forecasted_value[0][0]]

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:285，在 reshape(a, newshape, order) 中
200 @array_function_dispatch(_reshape_dispatcher)
201 def reshape(a, newshape, order=&#39;C&#39;):
202 &quot;&quot;&quot;
203 为数组赋予新形状而不更改其数据。
204 
(...)
283 [5, 6]])
284 &quot;&quot;&quot;
--&gt; 285 返回 _wrapfunc(a, &#39;reshape&#39;, newshape, order=order)

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:56，位于 _wrapfunc(obj, method, *args, **kwds)
54 bound = getattr(obj, method, None)
55 如果 bound 为 None:
---&gt; 56 return _wrapit(obj, method, *args, **kwds)
58 try:
59 return bound(*args, **kwds)

File ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:45, in _wrapit(obj, method, *args, **kwds)
43 except AttributeError:
44 wrap = None
---&gt; 45 result = getattr(asarray(obj), method)(*args, **kwds)
46 if wrap:
47 if not isinstance(result, mu.ndarray):

ValueError: 设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (5,) + 非均匀部分。
]]></description>
      <guid>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</guid>
      <pubDate>Mon, 15 Jul 2024 10:54:58 GMT</pubDate>
    </item>
    </channel>
</rss>