<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 07 Feb 2025 03:21:46 GMT</lastBuildDate>
    <item>
      <title>在拥有 IterableDataSet 和 DataLoader 的情况下，如何计算训练和验证的准确性和损失？</title>
      <link>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</link>
      <description><![CDATA[我正在定义自己的 train() 函数 - 用于训练和验证（也许函数的名称在这里不是最具描述性的）
由于我正在使用自定义可迭代数据集和数据加载器，因此我正在尝试弄清楚如何计算这些指标。可迭代数据集没有 length()，所以我真的想不出其他解决方法。
这是我目前的代码：
def train(model, trainingDataLoader, validationDataLoader, optimizer, criterion, device, epochs=10):
# 为 epochs 创建外部进度条
epoch_pbar = tqdm(range(epochs), desc=&#39;Training Progress&#39;, position=0)

for epoch in epoch_pbar:
# 训练阶段
model.train()
train_loss = 0
train_correct = 0
train_total = 0

# 为训练批次创建进度条
train_pbar = tqdm(trainingDataLoader, 
desc=f&#39;Training Epoch {epoch+1}/{epochs}&#39;,
position=1, 
leave=False)

# 对于每个批次
for set_features、access_features、cache_features、train_pbar 中的标签：
# 组合特征
cache_features_flat = cache_features.reshape(-1, 17*9)
combined_features = torch.cat([set_features、access_features、cache_features_flat], dim=1)
# 移至设备
combined_features = combined_features.to(device)
labels = labels.to(device)
# 零梯度
optimizer.zero_grad()
# 正向传递
outputs = model(combined_features)
loss = criterion(outputs, labels)
# 反向传递
loss.backward()
optimizer.step()
# 训练统计
train_loss += loss.item()
_, predicted = torch.max(outputs, 1)
train_total += labels.size(0)
train_correct += (predicted == torch.max(labels, 1)[1]).sum().item()

# 使用当前损失和准确率更新训练进度条
train_pbar.set_postfix({
&#39;loss&#39;: f&#39;{train_loss/train_total:.4f}&#39;,
&#39;acc&#39;: f&#39;{100 * train_correct/train_total:.2f}%&#39;
})

# 验证阶段
model.eval()
val_loss = 0
val_correct = 0
val_total = 0

# 为验证批次创建进度条
val_pbar = tqdm(validationDataLoader, 
desc=f&#39;Validation Epoch {epoch+1}/{epochs}&#39;,
position=1, 
leave=False)

with torch.no_grad():
for set_features, access_features, cache_features, labels在 val_pbar 中：
# 组合特征
cache_features_flat = cache_features.reshape(-1, 17*9)
combined_features = torch.cat([set_features, access_features, cache_features_flat], dim=1)
# 移至设备
combined_features = combined_features.to(device)
labels = labels.to(device)
# 前向传递
outputs = model(combined_features)
loss = criterion(outputs, labels)
# 验证统计
val_loss += loss.item()
_, predicted = torch.max(outputs, 1)
val_total += labels.size(0)
val_correct += (predicted == torch.max(labels, 1)[1]).sum().item()
# 使用当前损失和准确率更新验证进度条
val_pbar.set_postfix({
&#39;loss&#39;: f&#39;{val_loss/val_total:.4f}&#39;,
&#39;acc&#39;: f&#39;{100 * val_correct/val_total:.2f}%&#39;
})

# 使用最终指标更新 epoch 进度条
epoch_pbar.set_postfix({
&#39;train_loss&#39;: f&#39;{train_loss/(trainingDataLoader.__len__()):.4f}&#39;,
&#39;train_acc&#39;: f&#39;{100 * train_correct/train_total:.2f}%&#39;,
&#39;val_loss&#39;: f&#39;{val_loss/(validationDataLoader.__len__()):.4f}&#39;,
&#39;val_acc&#39;: f&#39;{100 * val_correct/val_total:.2f}%&#39;
})

# 打印 epoch 的最终统计数据
print(f&#39;\nEpoch {epoch+1}/{epochs}:&#39;)
print(f&#39;训练损失：{train_loss/(trainingDataLoader.__len__()):.4f}, &#39;
f&#39;训练准确率：{100 * train_correct/train_total:.2f}%&#39;)
print(f&#39;验证损失：{val_loss/(validationDataLoader.__len__()):.4f}, &#39;
f&#39;验证准确率：{100 * val_correct/val_total:.2f}%\n&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</guid>
      <pubDate>Fri, 07 Feb 2025 00:48:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习代理学习陷入困境</title>
      <link>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</link>
      <description><![CDATA[我正在尝试熟悉 MATLAB 的强化学习库。我正在努力创建一个可以学习正弦函数作为简单热身的代理，但我已经陷入困境。问题是，经过几次迭代后，代理达到一定水平，然后达到上限，无论我让学习过程运行多长时间，它都不会进一步学习。这是代码：
obs_info = rlNumericSpec([1 1], LowerLimit=-pi, UpperLimit=pi);
obs_info.Name = &quot;Sinus Value&quot;;

act_info = rlNumericSpec([1 1], LowerLimit=-1, UpperLimit=1);
act_info.Name = &quot;Predicted Value&quot;;

reset_fcn_handle = @()reset_train();
step_fcn_handle = @(action, portfolio)step_train( ...
action, portfolio);

sinus_train_env = rlFunctionEnv( ...
obs_info, act_info, step_fcn_handle, reset_fcn_handle);

function [initial_observation, portfolio] = reset_train()
initial_observation = 2*pi*rand(1)-pi;
portfolio = struct;
portfolio.LastValue = initial_observation;
end

function [next_observation, reward, is_done, portfolioOut] = step_train( ...
action, portfolio)
expected_prediction = sin(portfolio.LastValue);
reward = 1 / 100 / (0.01 + abs(action - expected_prediction));
next_observation = 2*pi*rand(1)-pi;
portfolioOut = portfolio;
portfolioOut.LastValue = next_observation;
is_done = false;
end

我使用强化学习设计器来构建代理。&quot;兼容算法&quot;设置为 TD3（默认选项），隐藏单元数为 32。超参数和探索模型设置：

训练时，最大片段长度 = 1000，平均窗口长度 = 5，停止标准 = AverageReward，停止值 = 900。
30 分钟后的结果：

30 分钟后的结果1 小时：

我尝试修改奖励函数并让它运行两个小时：
reward = 1 / (0.01 + abs(action - expected_prediction));

结果：

第二次尝试修改奖励函数：
if (abs(action-expected_prediction) &gt; 0.05)
reward = -1;
else 
reward = 1;
结束

结果：

我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</guid>
      <pubDate>Thu, 06 Feb 2025 19:03:32 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 最终检测头仍然输出 (1, 7, 8400)，而不是 (1, 8, 8400)，针对 3 个类别</title>
      <link>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</link>
      <description><![CDATA[我训练了一个包含 3 个类别的 YOLOv8 检测模型，但原始前向传递仍然显示最终检测输出为 (1, 7, 8400)，而不是 (1, 8, 8400)。
我已完成的操作：
检查了我的 data.yaml：
yaml
train：path/to/train/images
val：path/to/val/images
nc：3
names：[&#39;glioma&#39;, &#39;meningioma&#39;, &#39;pituitary&#39;]

确认 nc：3 正确。
使用以下命令从头开始训练：
bash
yolo detect train \
data=path/to/data.yaml \
model=yolov8x \
epochs=1000 \
imgsz=640 \
device=1 \
waiting=100

训练运行无错误并成功完成。
安装了最新的 Ultralytics 版本 (v8.3.72)，以确保没有版本问题：
bash
pip uninstall ultralytics
pip install ultralytics

直接加载新的 best.pt：
python
from ultralytics import YOLO
import torch

model = YOLO(r&quot;best.pt&quot;).model
model.eval()

dummy_input = torch.randn(1, 3, 640, 640)
with torch.no_grad():
outputs = model(dummy_input)

for out in output:
# 一些输出是列表；仔细检查每个元素
if isinstance(out, torch.Tensor):
print(out.shape)
else:
print(&quot;List output:&quot;, [o.shape for o in out if hasattr(o, &#39;shape&#39;)])

控制台显示 (1, 7, 8400) 作为检测输出。
已验证的模型元数据显示 nc=3 且 model.names 有 3 个类。但是，原始检测层输出仍为 7 个通道。
观察：
如果 YOLO 检测层确实针对 3 个类别，则每个锚点应输出 (5 + 3)=8 个通道，而不是 7 个。
不匹配 (1, 7, 8400) 通常表示尽管 nc=3，它仍设置为 2 个类别。
问题/求助：
为什么即使我从头开始训练了 3 个类别，原始检测头仍为 (1, 7, 8400)？
如何确保检测层完全重新初始化为 (5 + 3)=8 以进行 3 类检测？
我尝试删除旧的 .pt 文件，重新检查我的 data.yaml，重新安装 ultralytics，并确认 model.model.nc == 3。但最终的检测层继续产生 7 个通道而不是 8 个。
关于可能导致这种持续不匹配的原因，您有什么想法吗？
在此先感谢您的帮助或见解！]]></description>
      <guid>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</guid>
      <pubDate>Thu, 06 Feb 2025 18:39:21 GMT</pubDate>
    </item>
    <item>
      <title>哪种 AI 检测工具可以提供最准确的结果来识别 AI 生成的内容？[关闭]</title>
      <link>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</link>
      <description><![CDATA[我想确保我收到或编写的内容是人工生成的。有各种可用的 AI 检测工具，但我不确定哪一种最准确、最可靠。有些文章可能部分由 AI 生成，这使得检测变得困难。为此目的，有哪些最好的工具可用？&quot;
我尝试过的方法和预期结果：
&quot;我尝试过 GPTZero 和 Originality.ai 等工具，但我正在寻找更准确或被广泛接受的解决方案。理想情况下，我需要一种能够高精度检测 AI 编写内容并提供可靠见解的工具。
我测试过 GPTZero、Originality.ai 和 Copyleaks AI Detector 等工具。虽然它们提供了一些见解，但我注意到它们的结果不一致。一些 AI 生成的内容被正确标记，而在其他情况下，人工编写的文本被错误地识别为 AI 生成的。我期望一种工具能够提供更高的准确性、清晰地解释其检测过程并能够有效地分析短篇和长篇内容。]]></description>
      <guid>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</guid>
      <pubDate>Thu, 06 Feb 2025 17:34:58 GMT</pubDate>
    </item>
    <item>
      <title>CNN 二元分类任务中 f-1 分数的不同结果</title>
      <link>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</link>
      <description><![CDATA[我正在为二元分类任务制作一个 CNN 模型。

当我使用 binary_crossentropy 作为损失函数并在最后一层保留 1 个神经元时，我的准确率约为 94%，val_accuracy 约为 85%，但我的 f-1 分数停留在 69% 左右。
当我使用 categorical_crossentropy 作为损失函数时，结果有些相似，但这次 f-1 分数约为 85%。

model = Sequential([
Input(shape=(*input_shape, 1)),

Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(64, (3, 3), 激活=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
Conv2D(64, (3, 3), 激活=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(128, (3, 3), 激活=&#39;relu&#39;, padding=“相同”， kernel_regularizer=l2(0.001)),
Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(256, (3, 3),activation=&#39;relu&#39;, padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
Conv2D(256, (3, 3),activation=&#39;relu&#39;, padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Flatten(),
Dense(512,activation=&#39;relu&#39;),
Dropout(0.5),
Dense(256,activation=&#39;relu&#39;),
Dropout(0.2),
Dense(2,activation=&#39;softmax&#39;)
])

谁能告诉我为什么会发生这种情况，以及解决办法是什么。
此外，我想知道准确率和 val_accuracy 差距的原因，即使类别是平衡的。
我尝试过改变模型结构和损失函数，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</guid>
      <pubDate>Thu, 06 Feb 2025 15:24:42 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 图像分类过度拟合问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</link>
      <description><![CDATA[我正在尝试使用 tensorflow keras 最新 api 和函数创建一个图像分类模型。我的模型通过查看复杂的特征和设计（例如微缩印刷、全息图、透明图案等）将纸币分为真币和假币。我有一个包含大约 300-400 张高质量图像的小型数据集。无论我做什么，我的模型都会过度拟合。它的训练准确率高达 1.000，训练损失高达 0.012。但验证准确率保持在 0.60-0.75 之间，验证损失保持在 0.40-0.53 之间。
我尝试了以下方法：

增加数据集。 （但我知道这不会有太大帮助，因为钞票差别不大。它们都非常相似。所以它不会有助于推广模型）
使用 drop-out、l1/l2 正则化
使用迁移学习。我使用了 ResNet50 模型。我首先通过冻结基础模型训练了几个时期，然后解冻模型并重新训练了更多时期。
使用类权重来平衡权重。
使用计划学习率在训练过程中进行修改。
使用提前停止和回调等。
尝试使用预处理

此外，如果我在其中使用规范化层，我的模型性能会更差，而没有它，它的性能会更好。所以我排除了该层。
但是，没有什么能帮助我提高泛化能力。我不知道我错过了什么。
我的模型：

data_augmentation = tf.keras.Sequential([
tf.keras.layers.RandomRotation(0.1),
tf.keras.layers.RandomZoom(0.1),
tf.keras.layers.RandomBrightness(0.1),
tf.keras.layers.RandomContrast(0.1),
])

train_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

train_ds = (
train_ds
.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)
.cache()
.shuffle(1000)
.prefetch(buffer_size=AUTOTUNE)
)

base_model = tf.keras.applications.ResNet50(

input_shape=(img_height, img_width, 3),
include_top=False,
weights=&#39;imagenet&#39;
)

# 取消冻结特定层微调

base_model.trainable = True
for layer in base_model.layers[:-10]: # 保持第一层冻结
layer.trainable = False

l2_lambda=0.0001

#model
model = tf.keras.Sequential([
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense
(512,activation=&#39;relu&#39;,kernel_regularizer=regularizers.l2(l2_lambda)),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(256,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(1,activation = &quot;sigmoid&quot;)
])
]]></description>
      <guid>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</guid>
      <pubDate>Thu, 06 Feb 2025 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>NameError: 尝试运行 def __init__(self, width, height, inter=cv2.INTER_AREA) 时未定义名称“cv2”：[关闭]</title>
      <link>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</link>
      <description><![CDATA[我尝试使用 cv2 编写一些神经网络代码，但出现错误
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError：名称“cv2”未定义

knn.py --dataset ./datasets/animals
回溯（最近一次调用）：
文件“/Users/test/Desktop/CODE/knn.py”，第 6 行，位于&lt;module&gt;
来自 pyimagesearch.preprocessing 导入 SimplePreprocessor
文件“/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py”，第 4 行，位于&lt;module&gt;
类 SimplePreprocessor:
文件 &quot;/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py&quot;，第 5 行，在 SimplePreprocessor 中
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError: 名称 &#39;cv2&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</guid>
      <pubDate>Thu, 06 Feb 2025 08:53:50 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的部分轨迹[关闭]</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个表示手机移动轨迹的数据集，该轨迹由步行和驾车行驶的路段组成。数据包括经度、纬度、时间戳和 3 轴加速度计数据。我需要提取所有步行行驶的子轨迹。有没有现成的解决方案可以解决这个问题？如果没有，我该如何处理这个任务？
我试图在互联网上寻找现成的解决方案，但没有找到任何有价值的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
销毁（游戏对象）；
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中开发用于图像分类的预训练模型</title>
      <link>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</link>
      <description><![CDATA[我有一个问题，关于如何修改预训练模型以对 3 个类而不是 1000 个类进行分类。这是我目前想到的 2 种方法。我不确定哪种方法最好。
NASNetMobile_model = tf.keras.applications.NASNetMobile (
input_shape=(224,224,3),
include_top=False,
pooling=&#39;avg&#39;,
classes=3,
weights=&#39;imagenet&#39;
)
NASNetMobile_model.trainable=False
NASNetMobile_model.summary()type here

在方法 1 中，NASNetMobile 模型使用预训练的 ImageNet 权重初始化，排除顶层并使用平均池化。该模型设置为不可训练，以防止其权重在训练期间更新。然后构建一个新的 Sequential 模型，其中包括预先训练的 NASNetMobile 模型，后面跟着两个密集层：一个有 128 个单元和 ReLU 激活，另一个有 3 个单元和 softmax 激活，用于最终分类。Sequential 模型使用 Adam 优化器和稀疏分类交叉熵损失进行编译。最后，在数据集上对模型进行 20 个 epoch 的训练，批处理大小为 4，验证分割为 20%。
方法 1
new_pretrained_model = tf.keras.Sequential()

new_pretrained_model.add(NASNetMobile_model)
new_pretrained_model.add(tf.keras.layers.Dense(128,activation=&#39;relu&#39;))
new_pretrained_model.add(tf.keras.layers.Dense(3,activation=&#39;softmax&#39;))

new_pretrained_model.layers[0].trainable = False
new_pretrained_model.summary() 此处

new_pretrained_model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

new_pretrained_model.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)

方法 2
在方法 2 中，使用功能 API 创建新模型。预训练的 NASNetMobile 模型的输出被用作具有 128 个单元和 ReLU 激活的新密集层的输入，然后是具有 3 个单元和 softmax 激活的最终密集层。此方法明确将 NASNetMobile 模型的输入连接到新的输出层，形成一个新模型，其输入与原始 NASNetMobile 模型相同，但具有用于分类的附加密集层。然后使用 Adam 优化器和稀疏分类交叉熵损失编译新模型，并在数据集上训练 20 个时期，批处理大小为 4，验证分割为 20%。
NASNetMobile_model_out = NASNetMobile_model.output
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(NASNetMobile_model_out)
output = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(x)
model_2 = tf.keras.Model(inputs = NASNetMobile_model.input,outputs=output)

model_2.summary()

model_2.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

model_2.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)
]]></description>
      <guid>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</guid>
      <pubDate>Mon, 27 May 2024 16:22:12 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>Keras，内存错误 - data = data.astype("float") / 255.0。无法为形状为 (13165, 32, 32, 3) 的数组分配 309.MiB</title>
      <link>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</link>
      <description><![CDATA[我目前正在研究 Smiles 数据集，然后应用深度学习来检测微笑是正面的还是负面的。我使用的机器是 Raspberry Pi 3，用于执行此程序的 Python 版本是 3.7（不是 2.7）
我的训练集中总共有 13165 张图像。我想将其存储到一个数组中。但是，我遇到了一个问题，就是分配一个形状为（13165, 32, 32, 3）的数组。
下面是源代码（shallownet_smile.py）：
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classes_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv.shallownet import ShallowNet
from keras.optimizers import SGD
from imutils import routes
import matplotlib.pyplot as plt
import numpy as np
import argparse

ap = argparse.ArgumentParser()
ap.add_argument(&quot;-d&quot;, &quot;--dataset&quot;, required=True, help=&quot;path to input dataset&quot;)
args = vars(ap.parse_args())

# 获取我们将要描述的图像列表
print(&quot;[INFO] loading images...&quot;)

imagePaths = list(paths.list_images(args[&quot;dataset&quot;]))

sp = SimplePreprocessor(32, 32)
iap = ImageToArrayPreprocessor()

sdl = SimpleDatasetLoader(preprocessors=[sp, iap])
(data, labels) = sdl.load(imagePaths, verbose=1)
# 将值转换为 0-1 之间的值
data = data.astype(&quot;float&quot;) / 255.0

# 将数据划分为训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25,
random_state=42)

# 将标签从整数转换为向量
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)

# 初始化优化器和模型
print(“INFO] 编译模型...”)

# 初始化随机梯度下降，学习率为 0.005
opt = SGD(lr=0.005)

model = ShallowNet.build(width=32, height=32,depth=3,classes=2)
model.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=opt,
metrics=[&quot;accuracy&quot;])

# 训练网络
print(“INFO] 训练网络...”)

H = model.fit(trainX, trainY,validation_data=(testX, testY),batch_size=32,
epochs=100, verbose=1)

print(“[INFO] 评估网络...”)

predictions = model.predict(testX, batch_size=32)

print(classification_report(
testY.argmax(axis=1),
predictions.argmax(axis=1),
target_names=[&quot;positive&quot;, &quot;negative&quot;]
))

plt.style.use(&quot;ggplot&quot;)
plt.figure()
plt.plot(np.arange(0, 100), H.history[&quot;loss&quot;], label=&quot;train_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;acc&quot;], label=&quot;train_acc&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_acc&quot;], label=&quot;val_acc&quot;)
plt.title(&quot;训练损失和准确率&quot;)
plt.xlabel(&quot;Epoch #&quot;)
plt.ylabel(&quot;损失/准确率&quot;)
plt.legend()
plt.show()

假设数据集位于我当前的目录中。以下是我得到的错误：

python3 shallownet_smile.py -d=datasets/Smiles

错误消息
我仍然感到困惑，不知道哪里出了问题。我将非常感谢任何专家或有深度学习/机器学习经验的人向我解释和澄清我做错了什么。
感谢您的帮助和关注。]]></description>
      <guid>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</guid>
      <pubDate>Sun, 05 Apr 2020 17:25:26 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的内容：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;softmax&quot;)) 
model.compile(loss = &quot;categorical_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train, y_train, validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”或 58% 是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法表示：“使用分类器，当您对输出进行 softmax 时，您可以将值解释为属于每个特定类别的概率。您可以使用它们的分布作为粗略衡量您对观察结果属于该类别的信心的指标。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>ResNet50 模型未通过 keras 中的迁移学习进行学习</title>
      <link>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</link>
      <description><![CDATA[我正在尝试对 ResNet50 模型执行迁移学习，该模型已针对 PASCAL VOC 2012 数据集的 Imagenet 权重进行了预训练。由于它是一个多标签数据集，因此我在最后一层使用 sigmoid 激活函数和 binary_crossentropy 损失。指标包括 precision、recall 和 accuracy。下面是我用来为 20 个类（PASCAL VOC 有 20 个类）构建模型的代码。
img_height,img_width = 128,128
num_classes = 20
#如果正在加载 imagenet 权重，
#输入必须具有静态正方形（(128, 128)、(160, 160)、(192, 192) 或 (224, 224) 之一）
base_model = applications.resnet50.ResNet50(weights= &#39;imagenet&#39;, include_top=False, input_shape= (img_height,img_width,3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
#x = Dropout(0.7)(x)
predictions = Dense(num_classes,activation= &#39;sigmoid&#39;)(x)
model = Model(inputs = base_model.input, output = predictions)
for layer in model.layers[-2:]:
layer.trainable=True
for layer in model.layers[:-3]:
layer.trainable=False

adam = Adam(lr=0.0001)
model.compile(optimizer= adam, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;,precision_m,recall_m])
#print(model.summary())

X_train, X_test, Y_train, Y_test = train_test_split(x_train, y, random_state=42, test_size=0.2)
savingcheckpoint = ModelCheckpoint(&#39;ResnetTL.h5&#39;,monitor=&#39;val_loss&#39;,verbose=1,save_best_only=True,mode=&#39;min&#39;)
earlystopcheckpoint = EarlyStopping(monitor=&#39;val_loss&#39;,patience=10,verbose=1,mode=&#39;min&#39;,restore_best_weights=True)
model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test,Y_test), batch_size=batch_size,callbacks=[savingcheckpoint,earlystopcheckpoint],shuffle=True)
model.save_weights(&#39;ResnetTLweights.h5&#39;)

它运行了 35 个 epoch 直到 earlystopping，指标如下（没有 Dropout 层）：
loss: 0.1195 - accuracy: 0.9551 - precision_m: 0.8200 - recall_m: 0.5420 - val_loss: 0.3535 - val_accuracy: 0.8358 - val_precision_m: 0.0583 - val_recall_m: 0.0757

即使使用 Dropout 层，我也看不出有什么区别。
loss: 0.1584 - accuracy: 0.9428 - precision_m: 0.7212 - recall_m: 0.4333 - val_loss: 0.3508 - val_accuracy: 0.8783 - val_precision_m: 0.0595 - val_recall_m: 0.0403

使用 dropout，对于经过几个时期，模型的验证精度和准确率达到了 0.2，但并未超过该值。
我发现，与有和没有 dropout 层的训练集相比，验证集的精度和召回率相当低。我应该如何解释这一点？这是否意味着模型过度拟合。如果是这样，我该怎么办？截至目前，模型预测相当随机（完全不正确）。数据集大小为 11000 张图像。]]></description>
      <guid>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</guid>
      <pubDate>Tue, 15 Oct 2019 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>ALS（交替最小二乘）算法对用户的多个排名</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量研究，我们决定使用 Google Cloud 基础架构，并在我们的产品推荐系统中使用 ALS 算法（一种协同过滤方法 - https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine#Training-the-models ），详细说明如下：
我们有两种类型的客户。第一类是附近销售产品的公司，第二类是打算从这些公司购买产品的消费者

每个消费者都可以搜索附近的公司或按行业搜索公司（例如杂货店、干洗店、肉店等）
当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多项操作）
2.1. 仅查看公司简介
2.2. 将公司添加到收藏夹
2.3. 开始与公司聊天
2.4. 从公司下订单
2.5.给公司评分和评论

所以我不明白的是：上面描述的每件商品都被确定为我们数据库中的某些评分列，例如：
查看公司简介：10 分
从公司下订单：20 分
给公司打星或评论：20 分
因此，对于同一用户，每件商品都是单独的评分。
在我们的数据库中，对于用户-公司对，可能会有超过 1 行
例如：
第 1 行：user18-company18-10pts（查看过一次个人资料）
第 2 行：user18-company18-20pts（从公司下订单）
第 3 行：user18-company19-10pts
我不确定这个算法，它是计算该用户对同一家公司的所有评分的总和（我到底想要什么）还是只是寻找单个用户对单个公司的评分的单行？（我想要的是这个 ALS 算法来总结该用户-公司对的第 1 行和第 2 行）
有人知道吗？这对我们的推荐系统非常重要。因为我正在寻找的算法需要计算用户所有评分的总和，以便推荐另一家公司。因为我们的商业模式与电影评分系统不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>