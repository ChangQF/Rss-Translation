<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 22 Nov 2024 09:20:08 GMT</lastBuildDate>
    <item>
      <title>PointPillars 模型调试请求</title>
      <link>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</guid>
      <pubDate>Fri, 22 Nov 2024 07:44:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在我的案例中，逻辑回归在二值图像分割的准确度上优于 CNN？</title>
      <link>https://stackoverflow.com/questions/79213671/why-does-logistic-regression-outperform-cnn-in-accuracy-for-binary-image-segment</link>
      <description><![CDATA[我是机器学习的新手，我正在研究一个二值图像分割任务，我需要根据背景对物体（杯子）进行分类。我创建了一个彩色输入图像数据集（JPEG，在 0 和 1 之间归一化）和相应的二值蒙版（灰度图像，归一化为 0 和 1）。
我使用 TensorFlow 实现了一个 CNN 模型，并将其性能与逻辑回归模型 (scikit-learn) 进行了比较。令人惊讶的是，逻辑回归模型实现了比 CNN（83%）更高的准确率（97%）并产生了更好的蒙版。我很困惑，因为 CNN 通常更适合与图像相关的任务。
我已经使用 TensorFlow 构建了我的 CNN 模型，如下所示：
def create_cnn_model(learning_rate = 0.0001, filters=64, kernel_size=(3, 3),depth=3):
model = Sequential()
# 初始 Conv 和 Pool 层
model.add(Conv2D(filters, kernel_size=kernel_size,activation=&#39;relu&#39;,padding=&#39;same&#39;,input_shape=(Resolution, Resolution, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
# 根据深度添加 Conv-Pool 层
for i in range(1,depth):
model.add(Conv2D(filters * (2 ** i), kernel_size=kernel_size,activation=&#39;relu&#39;, padding=&#39;same&#39;))
# 仅当尺寸大于 1x1 时才添加 MaxPooling
model.add(MaxPooling2D(pool_size=(2, 2)))
# 检查空间维度是否达到 1x1，如果为真则停止
if (Resolution // (2 ** (i+1))) &lt; 2:
break
# UpSampling 和 Conv 层以恢复图像尺寸
for i in range(depth, 1, -1):
model.add(Conv2D(filters * (2 ** i), kernel_size=kernel_size,activation=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(UpSampling2D(size=(2, 2)))
if (Resolution // (2 ** (i+1))) &lt; 2：
break 
model.add(UpSampling2D(size=(2, 2)))
model.add(Conv2D(1, kernel_size=(1, 1),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)) # 具有 1 个通道的输出层
# 编译模型
model.compile(optimizer=Adam(learning_rate = learning_rate),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
return model

使用上述模型，我的准确率约为 83%
然后我尝试将我的模型与机器学习模型 Logistic 回归进行比较：
model = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)

使用此模型，我的准确率达到 97%（并且预测的掩码比 CNN 预测的掩码更好）
我使用的训练数据是定制的：
-输入图像（马克杯）为 jpeg 彩色，以 RGB 格式导入并进行预处理（在 0&amp;1 之间标准化），然后发送到模型
-掩码图像（马克杯）为黑白灰度（白色为物体，黑色为背景），以 RGB 格式导入并进行预处理（标准化为 0&amp; 1)，然后发送到模型。
-有 240 张图像和相同数量的掩码
-使用的图像是马克杯，几乎没有背景
预期：由于 CNN 能够捕捉空间特征，其表现优于逻辑回归。
结果：逻辑回归实现了更好的准确性和掩码质量，这似乎违反直觉。
我怀疑问题可能与数据集、模型架构或预处理步骤的简单性有关。
有人可以帮忙解释为什么逻辑回归在这种情况下可能优于 CNN，以及我如何改进我的 CNN 以完成这项任务？]]></description>
      <guid>https://stackoverflow.com/questions/79213671/why-does-logistic-regression-outperform-cnn-in-accuracy-for-binary-image-segment</guid>
      <pubDate>Fri, 22 Nov 2024 04:50:14 GMT</pubDate>
    </item>
    <item>
      <title>预测管道收购日期的模型</title>
      <link>https://stackoverflow.com/questions/79213087/model-to-predict-acquisition-date-for-pipeline</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79213087/model-to-predict-acquisition-date-for-pipeline</guid>
      <pubDate>Thu, 21 Nov 2024 22:25:24 GMT</pubDate>
    </item>
    <item>
      <title>如何修复尝试通过 Azure Ai Studio 项目访问 VS 代码时出现的访问问题</title>
      <link>https://stackoverflow.com/questions/79212964/how-to-fix-access-issues-when-trying-to-access-vs-code-via-azure-ai-studio-proje</link>
      <description><![CDATA[我们从 Windows 机器访问 Azure AI Studio Compute 时遇到一个问题。当计算准备就绪并尝试通过 VScode Desktop 访问它时，我们会看到以下错误消息：在门户上

这是我从 Vscode 获得的扩展：

此问题仅发生在 Windows 计算机上，从 Mac 计算机访问时不会发生此问题。 Compute 实例已启动并正在运行，但尝试打开 vscode 时出现错误。对此有任何解决方案吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79212964/how-to-fix-access-issues-when-trying-to-access-vs-code-via-azure-ai-studio-proje</guid>
      <pubDate>Thu, 21 Nov 2024 21:17:20 GMT</pubDate>
    </item>
    <item>
      <title>在使用 dataloader 测试数据集时，我们应该设置 shuffle=true 吗？或者这无所谓？</title>
      <link>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</link>
      <description><![CDATA[我有一个自定义数据集（披萨、寿司和牛排的图片）。
我正在使用 torch DataLoader 来处理它，现在在编写测试数据加载器自定义时，我们应该设置 shuffle=true 还是这无关紧要？？
我还没有看到区别，只是问一般情况。]]></description>
      <guid>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</guid>
      <pubDate>Thu, 21 Nov 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 ML 模型在不同的运行中获得不同的性能？</title>
      <link>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</link>
      <description><![CDATA[我正在使用 snowpark 训练 ml 模型（Xgboost 和 LightGbm），但每次运行后我都会得到不同的指标值（AUC、平均精度），因此永远不知道哪个是我最好的模型。
我尝试在笔记本的开头设置一个全局变量 random_seed = 42，并将其放在我的欠采样函数和模型的初始化中：
 if model_type == &#39;xgboost&#39;:
model = XGBClassifier(
random_state=random_seed,
input_cols=feature_cols,
label_cols=target_col,
output_cols=[&#39;PREDICTION&#39;],
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;, &#39;DATE_MONTH&#39;],
**hyperparameters
)

elif model_type == &#39;lightgbm&#39;:
model = LGBMClassifier（
random_state=random_seed，
input_cols=feature_cols，
label_cols=target_col，
output_cols=[&#39;PREDICTION&#39;]，
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;，&#39;DATE_MONTH&#39;]，
**超参数

)

def undersample_majority_class（df）：

df_with_seniority = df.with_column（“years_since”，（F.col（&#39;TIME_SINCE_FIRST_LEAD&#39;）/12）。cast（&#39;int&#39;））

df_with_random = df_with_seniority.with_column（&#39;random_order&#39;，F.random（seed=random_seed））
window_spec = Window.partition_by(&quot;INDIVIDUAL_SK&quot;).order_by(F.col(&#39;random_order&#39;).asc())
df_ranked = df_with_random.with_column(&quot;month_rank&quot;, F.row_number().over(window_spec)
)

df_majority = df_ranked.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 0)
df_majority_sampled = df_majority.filter(((F.col(&quot;years_since&quot;) &gt; 10) &amp; (F.col(&quot;month_rank&quot;) == 1)) |
((F.col(&quot;years_since&quot;) &lt;= 10) &amp; (F.col(&quot;month_rank&quot;) &lt;= 2))
)

df_majority_sampled = df_majority_sampled.drop(&#39;years_since&#39;,&#39;month_rank&#39;,&#39;random_order&#39; )
df_minority = df.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 1)
df_balanced = df_majority_sampled.union_all(df_minority)

return df_balanced

我不知道该怎么做才能解决这个问题。

]]></description>
      <guid>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</guid>
      <pubDate>Thu, 21 Nov 2024 18:16:23 GMT</pubDate>
    </item>
    <item>
      <title>ANN 模型的准确性 [关闭]</title>
      <link>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</link>
      <description><![CDATA[我曾尝试使用历史数据构建一个 ANN 模型来预测太阳辐射。
2016 - 2020 NSRDB 数据
以下是评估指标
R2 值 .999
MSE .690
MAE .450
损失 .690
以下是我的问题
ANN 模型的准确率达到 .999 是否正常
是否过度拟合？
我附上了训练损失与验证损失图]]></description>
      <guid>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</guid>
      <pubDate>Thu, 21 Nov 2024 16:58:23 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法可以使用纯数字信号处理概念提取并分类脑电图数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79211719/is-there-a-way-that-i-can-extract-and-then-classify-eeg-data-using-pure-digital</link>
      <description><![CDATA[我正在做一个使用脑电图信号数据检测驾驶员困倦的项目，我们可以将信号从噪音和其他伪影中分离出来，但后来我们如何识别困倦状态
我尝试研究它，但我发现机器学习几乎无处不在，我正在寻找使用数字图像处理从脑电图信号中识别困倦的方法（也是电子和通信工程课程的主题）]]></description>
      <guid>https://stackoverflow.com/questions/79211719/is-there-a-way-that-i-can-extract-and-then-classify-eeg-data-using-pure-digital</guid>
      <pubDate>Thu, 21 Nov 2024 14:53:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 最小化汉明距离</title>
      <link>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</link>
      <description><![CDATA[我有一个 mxn 矩阵，其中 m&gt;n 和一个向量 b。它们仅由 1 和 0 组成。此外，所有地方的和都是以 2 为模的。通过高斯消元法，我可以得到 Ax=b 的解 x_p。这是一个包含 1 和 0 的向量。这不是唯一的解决方案。存在其他包含较少 1 的解。我想找到包含最少 1 的解（汉明距离），即我想最小化汉明距离。
为了做到这一点，我想到了使用 pytorch 的梯度下降法。这是我的代码：
def optimal_solution(self, max_iter=100, lr=0.0054, lambda_param=10):

matrix = self.relative_boundary()
# 在 [0,1] 中初始化 x 并转换为 pytorch 张量
x = self.solve()

matrix = torch.tensor(matrix, dtype=torch.float32)
b = torch.tensor(self.boundary_vector, dtype=torch.float32)
x = torch.tensor(x, dtype=torch.float32, require_grad=True)

# Adam 是梯度下降的高级版本，可在优化过程中调整学习率。
optimizer = torch.optim.Adam([x], lr=lr)

# 跟踪 x 变化的标准
prev_x = x.clone()

# 循环执行梯度下降，最多迭代 max_iter 次：
for _ in range(max_iter):
optimizer.zero_grad()

# 定义损失函数：稀疏性 + 约束满足
loss = torch.sum(x) + lambda_param * torch.sum((matrix @ x - b) ** 2)

# 跟踪 x 的变化量
change_in_x = torch.norm(x - prev_x).item()

# 计算相对于 x 的损失梯度
loss.backward()
# 根据梯度更新 x 的值
optimizer.step()

# 应用软投影（例如 S 型）使 x 保持在 [0, 1] 中
x.data = torch.clamp(x.data, 0, 1)

# 可选地打印损失以进行调试
print(f&quot;迭代 {_} 时的损失：{loss.item()}, x 的变化：{change_in_x}&quot;)

# 从计算图中分离 PyTorch 张量并将其转换为 NumPy 数组；
# 应用 0.5 的阈值将值转换为 0 或 1。
# 然后函数返回二进制解决方案向量。
x_binary = (x.detach().numpy() &gt; 0.5).astype(int)

return x_binary

但是，通过尝试不同的 lambda_par 和学习率，我要么得到零向量，要么它只输出一个具有较少 1 的向量，但不满足方程 Ax=b。您对有效的代码有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</guid>
      <pubDate>Thu, 21 Nov 2024 14:43:15 GMT</pubDate>
    </item>
    <item>
      <title>VAE 损失减少，但重建效果并未改善</title>
      <link>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</link>
      <description><![CDATA[我遇到了重建图像根本不起作用的问题。下面是我执行单个更新步骤的方法。我知道常规 VAE 可以实现更简单的实现，但由于其他限制，我需要以这种方式实现它。因此，我想知道此实现中是否存在任何错误，而不是要求更高效的实现。zeros_like_batchstats 函数返回一个与输入具有相同结构但所有值都设置为零的对象。
@jax.jit
def train_step(
rng: jax.random.PRNGKey,
state_enc: TrainState,
state_dec: TrainState,
imgs: jax.Array,
) -&gt; Tuple[TrainState, TrainState, Dict]:

((mean, logvar), enc_mutated_vars), vjp_fn_enc = jax.vjp(
lambda params: state_enc.apply_fn(
{&quot;params&quot;: params, &quot;batch_stats&quot;: state_enc.batch_stats},
imgs, train=True, mutable=[&quot;batch_stats&quot;]
),
state_enc.params,
)
z, vjp_fn_latents = jax.vjp(
lambda mean, logvar: sample_z(rng, mean, logvar),
mean, logvar
)

def recon_loss_fn(dec_params, latent_features):
# 从解码器获取重建图像
recon, mutated_vars = state_dec.apply_fn(
{&#39;params&#39;: dec_params, &#39;batch_stats&#39;: state_dec.batch_stats},
latent_features, train=True, mutable=[&#39;batch_stats&#39;]
)
recon_loss = jnp.mean(jnp.sum(binary_cross_entropy_fn(recon, imgs), axis=(1, 2, 3), keepdims=True))
返回 recon_loss, mutated_vars

recon_loss_grads_fn = jax.value_and_grad(recon_loss_fn, argnums=(0, 1), has_aux=True)
(recon_loss, dec_mutated_vars), (grads_dec, graz_z) = recon_loss_grads_fn(state_dec.params, z)
    grads_enc_recon=vjp_fn_enc((vjp_fn_latents(graz_z), {“batch_stats”: Zeros_like_batchstats(state_enc.batch_stats)}))[0]

    # 计算kld_loss
    def kld_loss_fn(平均值, logvar):
        kld_loss = jnp.mean(jnp.sum(-0.5 * (1 + logvar - 平均值 ** 2 - jnp.exp(logvar)), axis=1))
        返回 kld_loss

    kld_loss_grads_fn = jax.value_and_grad(kld_loss_fn, argnums=(0, 1))
    kld_loss, grads_mean_and_logvar = kld_loss_grads_fn(平均值, logvar)
    grads_enc_kld = vjp_fn_enc((grads_mean_and_logvar, {&quot;batch_stats&quot;: zeros_like_batchstats(state_enc.batch_stats)}))[0]

# 计算编码器的梯度
grads_enc = jax.tree_util.tree_map(lambda x, y: x + y, grads_enc_recon, grads_enc_kld)

# 存储梯度和批次统计信息\
state_enc = state_enc.apply_gradients(grads=grads_enc, batch_stats=enc_mutated_vars[&quot;batch_stats&quot;])
state_dec = state_dec.apply_gradients(grads=grads_dec, batch_stats=dec_mutated_vars[&quot;batch_stats&quot;])

metrics = {
&quot;train/recon_loss&quot;: recon_loss,
&quot;train/kld_loss&quot;: kld_loss,
}
return state_enc, state_dec, metrics

我已确认形状没有问题，并且损失也在减少。]]></description>
      <guid>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</guid>
      <pubDate>Thu, 21 Nov 2024 13:23:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的损失来训练不同阶段的模型</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[我正在尝试以端到端的方式训练一个两阶段模型。但是，我想用不同的损失更新模型的不同阶段。例如，假设端到端模型由两个模型组成：model1 和 model2。输出是通过运行计算的
features = model1(inputs)
output = model2(features)

我想用 loss1 更新 model1 的参数，同时保持 model2 的参数不变。接下来，我想用 loss2 更新 model2 的参数，同时保持 model1 的参数不变。我的完整实现如下：
import torch
import torch.nn as nn

# 定义第一个模型
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# 定义第二个模型
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# 初始化模型
model1 = Net()
model2 = Net1()

# 初始化单独的每个模型的优化器
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# 样本输入和标签
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 

但是，这将返回
回溯（最近一次调用最后一次）：
文件，第 55 行，在 &lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, 第 521 行, 在反向传播中
torch.autograd.backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, 第 289 行, 在反向传播中
_engine_run_backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, 第 769 行, 在 _engine_run_backward 中
return Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传播
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]（AsStridedBackward0 的输出 0）处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

我有点明白为什么会发生这种情况，但有办法解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>EOFError：输入不足 - Pickle</title>
      <link>https://stackoverflow.com/questions/79198550/eoferror-ran-out-of-input-pickle</link>
      <description><![CDATA[当我尝试以 pickle 格式加载机器学习模型以便它在 Web 应用程序中运行时，我收到此错误，路径显然是正确的，我的所有文件都位于同一文件夹中。但是，错误始终存在。

df = pd.DataFrame.from_dict(user_values, orient=&quot;index&quot;).T

with open(&quot;classifier.pkl&quot;, &quot;rb&quot;) as file:
loaded_model = pickle.load(file)

prediction = loaded_model.predict(df)[0][0]

else:
prediction = None

return render_template(&quot;index.html&quot;, prediction=prediction)

if __name__ == &quot;__main__&quot;:
app.run(debug=True)

]]></description>
      <guid>https://stackoverflow.com/questions/79198550/eoferror-ran-out-of-input-pickle</guid>
      <pubDate>Mon, 18 Nov 2024 02:30:01 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow InvalidArgumentError：ConcatOp 中的连接维度不匹配 - 形状不匹配</title>
      <link>https://stackoverflow.com/questions/79141216/tensorflow-invalidargumenterror-concatenation-dimension-mismatch-in-concatop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79141216/tensorflow-invalidargumenterror-concatenation-dimension-mismatch-in-concatop</guid>
      <pubDate>Wed, 30 Oct 2024 13:00:13 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用线性回归模型预测股票价格时准确率能达到 100%？</title>
      <link>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</link>
      <description><![CDATA[我正在尝试使用线性回归模型在 Python 中预测股票价格。我使用 train_test_split 分割数据，因此据我所知，我的测试数据不应该在我的训练数据中，所以我不明白为什么模型的准确率是 100%。
这是我的代码：
X = RMV.drop(&#39;Close&#39;, axis=1)
y = RMV[&#39;Close&#39;]`

来自 sklearn.model_selection 导入 train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

来自 sklearn.linear_model 导入 LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

reg_preds = reg.predict(X_test)

当我使用此代码运行交叉验证以测试准确性时，我得到的值为1.00。
scores = model_selection.cross_val_score(reg, X_test, y_test, cv=10)
print (&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() / 2)) 

作为参考，下面是我使用的数据样本：
 收盘价 SMA EMA MACD 上轨 中轨 下轨 RSI
日期 
2010-02-18 60.900002 57.335715 57.419887 2.099073 64.842238 55.4075 45.972762   60.517959
2010-02-19 61.000000 57.967857 57.897236 2.215288 65.422290 55.9000 46.377710 60.672590
2010-02-22 62.099998 58.560714 58.457604 2.368843 66.047128 56.4675 46.887872 62.416318
2010-02-23 61.200001 59.117857 58.823257 2.390360 66.386746 57.0000 47.613254 60.069541
2010-02-24 60.900002 58.539286 59.100156 2.356046 66.504379 57.5425 48.580621 59.269579

我哪里错了？
更新：准确度似乎是错误的指标，因此我已按照回复的建议改用 MSE：
print(&#39;均方误差：&#39;, metrics.mean_squared_error(y_true=y_test, y_pred=lm_preds))
print(&#39;判定系数：%.2f&#39; % metrics.r2_score(y_true=y_test, y_pred=lm_preds))

根据运行情况，这给了我大约 MSE = 13-15，R2 = 0.999，这仍然非常高。由于平均股价在 600 左右，MSE 实际上并没有看起来那么高。该模型似乎仍然表现得太好了。
我使用的是 2010-2020 年的 Rightmove 股票数据。我刚刚切换到使用 2010-2020 年和 2019-2020 年波动性更大的股票 (PMO.L)，并且我还删除了我使用的 5/7 个指标。
对于 2010-2020 年，该模型给出的 MSE 为 69（与股价相比相对较低）和 0.999 R2。然而，对于 2019-2020 年，该模型确实似乎有点差，MSE 为 15.5，R2 为 0.82，明显低于以前。然而，考虑到这只是一年的数据，它的表现似乎仍然太好了。
以下是用于训练新股票模型的特征数据样本：
2010-2020：
 SMA EMA
日期
2010-02-18 266.214286 266.857731
2010-02-19 266.910714 268.110034
2010-02-22 267.303571 269.428696
2010-02-23 267.589286 269.838203
2010-02-24 264.660714 270.659776

2011-2020:
 SMA EMA
日期
2019-02-18 73.425000 73.791397
2019-02-19 73.632143 74.052544
2019-02-20 73.785715 74.325538
2019-02-21 73.953572 74.335466
2019-02-22 73.928572 74.330738
]]></description>
      <guid>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</guid>
      <pubDate>Thu, 11 Feb 2021 12:40:43 GMT</pubDate>
    </item>
    </channel>
</rss>