<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 01 Oct 2024 15:18:38 GMT</lastBuildDate>
    <item>
      <title>AWS SageMaker 上的 Flask 应用程序无法通过异步端点调用处理 S3 视频处理</title>
      <link>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</link>
      <description><![CDATA[我正在 AWS SageMaker 中部署一个 Flask 应用程序，该应用程序旨在处理异步调用。该应用程序应从 S3 获取视频文件，使用面部情绪预测模型对其进行处理，然后返回结果。但是，我的设置没有按预期工作。
以下是我的流程中步骤的摘要：
异步 SageMaker 调用：我使用 Python 脚本中的invoke_endpoint_async 方法触发 SageMaker 端点的调用，为视频文件提供 S3 URI。
response = Runtime.invoke_endpoint_async(
EndpointName=endpoint_name,
InputLocation=&quot;&lt;s3 URI&gt;&quot;, 
ContentType=&quot;application/json&quot;
)

Flask 应用程序（通过 ECR 部署）：Flask 应用程序在 /invocations 端点监听 POST 请求，从请求正文中提取 S3 URI，下载视频文件，处理它，然后返回预测。
import os
from flask import Flask, request, jsonify
import source.face_emotion_utils.predict as face_predict
import boto3
import logs

app = Flask(__name__)

# 设置 S3 客户端
s3 = boto3.client(&#39;s3&#39;)

@app.route(&#39;/ping&#39;, methods=[&#39;GET&#39;])
def ping():
return jsonify({&#39;status&#39;: 200})

@app.route(&#39;/invocations&#39;, methods=[&#39;POST&#39;])
def faceInvocations():
try:
# 提取 JSON 负载
payload = request.get_json()
s3_uri = payload.get(&#39;InputLocation&#39;)

if not s3_uri:
return jsonify({&#39;error&#39;: &#39;s3_uri is required&#39;}), 400

# 从 S3 URI 解析存储桶名称和密钥
bucket_name, key = s3_uri.replace(&quot;s3://&quot;, &quot;&quot;).split(&quot;/&quot;, 1)
filename = key.split(&quot;/&quot;)[-1]
file_path = os.path.join(&#39;/app/input_files&#39;, filename)

# 从 S3 下载文件
s3.download_file(bucket_name, key, file_path)

# 处理文件（预测情绪）
prediction = face_predict.predict(file_path, video_mode=True)

# 清理临时文件
if os.path.exists(file_path):
os.remove(file_path)

return jsonify({&#39;prediction&#39;: prediction})
except Exception as e:
logs.error(f&quot;Error: {str(e)}&quot;)
return jsonify({&#39;error&#39;: str(e)}), 500

if __name__ == &#39;__main__&#39;:
app.run(host=&#39;0.0.0.0&#39;, port=8080)

我的期望：

SageMaker 端点应该从异步调用接收 S3 URI。Flask 应用应该下载视频文件、运行情绪预测并返回结果。

问题：
调用失败，我在 Flask 日志中收到错误。似乎 S3 文件没有被正确下载或处理。以下是一些日志：
INFO:root:Entered faceInvocations function
INFO:root:S3 URI: &lt;s3 URI&gt;
ERROR:root:下载文件时出错：调用 HeadObject 操作时发生错误 (404)：未找到


我还可以看到到目前为止的日志 INFO:root:Created directory
我尝试过的方法：

确保 S3 URI 有效且文件存在。
验证附加到 SageMaker 端点的 IAM 角色是否具有 S3 存储桶的 s3:GetObject 权限。
检查 Flask 应用代码是否存在处理 S3 URI 的错误，并能够使用 docker 获取结果
在日志中打印 S3 存储桶名称和密钥，以确保它们被正确解析。

问题：

为什么我的 Flask 应用无法从中下载视频文件S3，即使 S3 URI 看起来正确？
我在 Flask 应用程序中处理 S3 URI 的方式是否存在问题？
在传递 S3 URI 时，我是否遗漏了 SageMaker 异步调用的特定内容？

如能提供任何帮助或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</guid>
      <pubDate>Tue, 01 Oct 2024 15:09:40 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow/Keras 中使用 tf.keras.utils.image_dataset_from_directory 时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/79042251/valueerror-when-using-tf-keras-utils-image-dataset-from-directory-in-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79042251/valueerror-when-using-tf-keras-utils-image-dataset-from-directory-in-tensorflow</guid>
      <pubDate>Tue, 01 Oct 2024 08:17:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 mmdet 推断自定义预训练模型？</title>
      <link>https://stackoverflow.com/questions/79041799/how-to-infer-a-custom-pretrained-model-using-mmdet</link>
      <description><![CDATA[我正尝试使用一些自定义模型（调整后的 yolov5xu fwiw）运行 masa，这些模型已在某些外部数据集上训练。我很难理解如何使用 mdmdet 框架运行它。文档对我来说似乎有点不清楚，因为它指出我需要某种配置文件，但这令人惊讶，因为

我不会训练一个，我只需要 model().predict()
配置文件包含很多我不太熟悉的细节。为简单运行制作这个巨大的配置文件看起来有点麻烦。

虽然使用本机 api 推断 yolo 非常简单
model = YOLO(&#39;model.pt&#39;)
results = model.predict(&#39;video.mp4&#39;,save=True)

到目前为止，使用 mmdet 执行此操作似乎过于复杂。
有人知道我是否真的可以轻松地将我的模型插入 mmdet 吗？
我尝试过制作类似的配置
model = dict(
type=&#39;YOLOX&#39;,
backbone=dict(
init_cfg=dict(type=&#39;Pretrained&#39;, checkpoint=&#39;/path/to/yolo.pt&#39;)
)
)
load_from=&#39;/path/to/yolo.pt&#39;

但似乎还不足以运行。它会在配置中询问一些额外的字段]]></description>
      <guid>https://stackoverflow.com/questions/79041799/how-to-infer-a-custom-pretrained-model-using-mmdet</guid>
      <pubDate>Tue, 01 Oct 2024 05:50:15 GMT</pubDate>
    </item>
    <item>
      <title>神经网络输入数据中的特征应该是行还是列？</title>
      <link>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</link>
      <description><![CDATA[我已经实现了一个神经网络，并且对处理输入矩阵的数据形状的正确方法有疑问。具体来说，我想知道输入数据 X 是否应该在行上有示例而在列上有特征，或者反过来（行上有特征而在列上有示例）。
目前，我已经实现了它以采用形状为 (num_features, num_examples) 的 X。但是，我发现相互矛盾的来源表明，在许多库和框架中，相反的做法是常态。例如，当我加载 MNIST 数据集时，它会以 (num_examples, num_features) 的形式出现，这要求我在将数据作为输入提供给神经网络之前对其进行转置
鉴于此情况，如果常见的做法确实是使用 (num_examples, num_features)：

我是否应该接受原始形状的 X 并在内部对其进行转置？
我是否应该更改实现以直接使用其他维度的 X？
我是否应该简单地在网络文档中记录预期形状？
]]></description>
      <guid>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</guid>
      <pubDate>Tue, 01 Oct 2024 04:07:49 GMT</pubDate>
    </item>
    <item>
      <title>使用随机搜索优化随机森林超参数：ranger 包的运行时间长且错误多</title>
      <link>https://stackoverflow.com/questions/79039614/optimizing-random-forest-hyperparameters-with-random-search-long-runtime-and-er</link>
      <description><![CDATA[我尝试使用“随机搜索”方法确定随机森林模型的参数 ntree、mtry 和 nodesize 的最优值。我还尝试使用交叉验证来评估每个组合的预测准确度并选择最佳超参数值。
我运行了模型，如下面的代码所示，但这个过程需要很长时间。我等了 4 个小时，但它仍然没有完成。我正在寻找专家建议，看看代码在当前状态下是否正确。我尝试使用 ranger 包，但遇到了错误“错误：调整参数网格应该有列 mtry、splitrule、min.node.size
&quot;。对于这个我想应用于连续数据的任务，您还有更好的解决方案吗？
library(caret)
library(randomForest) 

# 将数据拆分为训练集 (70%) 和验证集 (30%)
set.seed(123)
train_index3 &lt;- createDataPartition(all_data3$LST, p = 0.7, list = FALSE)
train_data3 &lt;- all_data3[train_index3, ]
validation_data3 &lt;- all_data3[-train_index3, ]

# 设置使用随机搜索的交叉验证
train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5, search = &quot;random&quot;)

# 创建要调整的超参数列表
ntree_values &lt;- seq(100, 500, by = 10)
nodesize_values &lt;- c(1:30)
mtry_values &lt;- c(1:30)

# 初始化数据框以存储结果
results &lt;- data.frame(ntree = numeric(), nodesize = numeric(), mtry = numeric(), 
R2 = numeric(), MAE = numeric())

# 手动执行随机搜索
for (ntree in ntree_values) {
for (nodesize in nodesize_values) {
for (mtry in mtry_values) {

# 为每个组合训练模型
set.seed(123)
rf_model &lt;- randomForest(
LST ~ .
data = train_data3,
method = &quot;rf&quot;,
mtry = mtry,
ntree = ntree,
nodesize = nodesize,
significance = TRUE
)

# 获取验证集上的预测
predictions &lt;- predict(rf_model, validation_data3)

# 计算性能指标 (R² 和 MAE)
R2 &lt;- caret::R2(predictions, validation_data3$LST)
MAE &lt;- caret::MAE(predictions, validation_data3$LST)

# 存储结果
results &lt;- rbind(results, data.frame(ntree = ntree, nodesize = nodesize, mtry = mtry, 
R2 = R2, MAE = MAE))
}
}
}

# 查看结果
print(results)

# 步骤 1：预测训练数据
train_predictions3 &lt;- predict(rf_model, newdata = train_data3[, -c(1:5)])

# 第 2 步：根据验证数据进行预测
validation_predictions3 &lt;- predict(rf_model, newdata = validation_data3[, -c(1:5)])
]]></description>
      <guid>https://stackoverflow.com/questions/79039614/optimizing-random-forest-hyperparameters-with-random-search-long-runtime-and-er</guid>
      <pubDate>Mon, 30 Sep 2024 13:37:08 GMT</pubDate>
    </item>
    <item>
      <title>用于压力检测的心电图分析[关闭]</title>
      <link>https://stackoverflow.com/questions/79039477/ecg-analysis-for-stress-detection</link>
      <description><![CDATA[有人使用过数据集 SWELL 进行压力检测吗？您能帮我识别其中的各种参数列表吗？有人能告诉我数据集中与压力高度相关的最佳匹配特征吗？我应该使用回归分类进行压力预测吗？
我期待对数据集的准确解释]]></description>
      <guid>https://stackoverflow.com/questions/79039477/ecg-analysis-for-stress-detection</guid>
      <pubDate>Mon, 30 Sep 2024 13:02:42 GMT</pubDate>
    </item>
    <item>
      <title>了解 K 折交叉验证、模型训练和 R² 分数</title>
      <link>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</link>
      <description><![CDATA[我正在使用网格搜索设置中的 K 折交叉验证来调整超参数。我对模型的训练和评估方式有几个问题：

当我使用 GridSearchCV 时，模型会在多个折（假设为 10 个）上进行评估。对于每个超参数组合，模型会在 (K-1) 个折上进行训练，并在剩余的折上进行验证。当我在网格搜索后获得 best_grid 模型时，该模型是在哪些特定训练数据（即哪些折）上进行训练的？

当我调用 best_grid.predict(X_test) 时，该模型在哪个数据集上进行预测？它是否在网格搜索后在整个数据集上进行了训练，还是仍然基于交叉验证期间使用的折叠？

如果 best_grid 模型尚未在整个数据集上进行训练，我是否需要在进行预测之前再次明确地将其拟合到完整数据集？

我想获得 R² 训练分数，但我对使用以下代码时收到的分数感到困惑：
param_grid = {f&#39;regressor__regressor__{param}&#39;: values for param, values in model_info[&#39;params&#39;].items()}
grid_search = GridSearchCV(full_pipeline, param_grid, cv=stratified_kf.split(X, y_binned),scoring=&quot;r2&quot;, n_jobs=4, return_train_score=True)

grid_search.fit(X,y)

if grid_search.best_score_ &gt; best_score:
best_score = grid_search.best_score_
best_model = model_name
best_grid = grid_search

mean_train_score = best_grid.cv_results_[&#39;mean_train_score&#39;][best_grid.best_index_] #
print(mean_train_score) # 这里就是这个东西


]]></description>
      <guid>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</guid>
      <pubDate>Mon, 30 Sep 2024 12:59:27 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在 Keras 模型中组合图像、掩码和 CSV 数据时输入形状无效</title>
      <link>https://stackoverflow.com/questions/79039003/valueerror-invalid-input-shape-when-combining-image-mask-and-csv-data-in-kera</link>
      <description><![CDATA[我正在使用 Keras 开发深度学习模型，尝试结合三种输入：图像、蒙版和 CSV 数据。我的模型的目标是根据医学图像（CT 扫描）和 CSV 数据预测脑出血的存在和类型。我使用 Attention U-Net 构建了模型，用于图像分割，并使用密集层进行 CSV 数据处理。
输入数据包括：

图像：灰度 CT 扫描，形状为 (256, 256, 1)。

蒙版：对应于形状 (256, 256, 1) 的分割的二进制蒙版。

CSV 数据：各种出血类型的数值数据，形状为 (6,)。


尝试使用 model.fit() 训练模型时遇到以下错误：
ValueError：输入 Tensor(&quot; functional_1/Cast:0&quot;, shape=(None, 6), dtype=float32) 的输入形状无效。预期形状为 (None, 256, 256, 1)，但输入具有不兼容的形状 (None, 6)
似乎具有形状 (None, 6) 的 CSV 数据被输入到模型中，其中预期图像数据（具有形状 (None, 256, 256, 1)）。我仔细检查了输入形状并使用 train_test_split 分割数据，但问题仍然存在。
这是我的代码的简化版本：
# 模型定义
inputs, output =tention_unet() # 图像的 UNet 模型

csv_input = layer.Input(shape=(6,), name=&#39;csv_input&#39;) # CSV 数据输入

mask_input = layer.Input(shape=(256, 256, 1), name=&#39;mask_input&#39;) # Mask 输入

# CSV 数据处理
csv_x = layer.Dense(64,activation=&#39;relu&#39;)(csv_input)csv_x = layer.Dense(32,activation=&#39;relu&#39;)(csv_x)

# 将 CSV 与 U-Net 输出相结合
flatten_outputs = layer.Flatten()(outputs)combined = layer.Concatenate()([flatten_outputs, csv_x])

# 最终输出
final_output = layer.Dense(1,activation=&#39;sigmoid&#39;,name=&#39;final_output&#39;)(combined)

# 模型编译
model = models.Model(inputs=[inputs, csv_input, mask_input],outputs=[outputs, final_output])model.compile(optimizer=Adam(),loss={&#39;final_output&#39;:&#39;binary_crossentropy&#39;,&#39;outputs&#39;:&#39;categorical_crossentropy&#39;},metrics=[&#39;accuracy&#39;,dice_coef,jaccard_index])

# 数据形状
print(f&quot;X_train 的形状：{X_train.shape}&quot;) # 图像的形状：(batch_size,256,256, 1)print(f&quot;csv_train 的形状：{csv_train.shape}&quot;) # CSV 的形状：(batch_size, 6)print(f&quot;y_train 的形状：{y_train.shape}&quot;) # mask 的形状：(batch_size, 256, 256, 1)

# 模型训练
model.fit({&#39;input_layer&#39;: X_train, &#39;csv_input&#39;: csv_train, &#39;mask_input&#39;: y_train},{&#39;final_output&#39;: csv_train, &#39;outputs&#39;: y_train},validation_data=({&#39;input_layer&#39;: X_val, &#39;csv_input&#39;: csv_val, &#39;mask_input&#39;: y_val},{&#39;final_output&#39;: csv_val, &#39;outputs&#39;: y_val})、epochs=50、batch_size=32、callbacks=callbacks)

我尝试过的步骤：

我打印了训练数据的形状，它们看起来都是正确的：

图像：(batch_size、256、256、1)
CSV：(batch_size、6)
掩码：(batch_size、256、256、1)


我检查了 model.fit() 中数据输入的顺序，以确保数据被传递到正确的层。

如何解决这个形状不匹配问题，并在模型中正确组合图像、掩码和 CSV 数据？]]></description>
      <guid>https://stackoverflow.com/questions/79039003/valueerror-invalid-input-shape-when-combining-image-mask-and-csv-data-in-kera</guid>
      <pubDate>Mon, 30 Sep 2024 10:44:55 GMT</pubDate>
    </item>
    <item>
      <title>如何减少过度拟合或者减少类别相似性的影响？</title>
      <link>https://stackoverflow.com/questions/79038878/how-to-reduce-overfitting-or-reduce-effect-from-class-similarity</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79038878/how-to-reduce-overfitting-or-reduce-effect-from-class-similarity</guid>
      <pubDate>Mon, 30 Sep 2024 10:10:00 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么输入来预测 rl 模型？它会被缩放还是被反转缩放？</title>
      <link>https://stackoverflow.com/questions/79038696/what-input-should-i-use-to-predict-rl-model-will-it-be-scaled-or-inv-scaled</link>
      <description><![CDATA[我正在使用 sb3 DQN 来训练股票数据，其中我的观察结果是最后 120 根蜡烛，具有 7 个特征，即开盘高低收盘时最小 rsi 等......。因此 obs 形状将是 (120,7)，输出将是离散的，具有 3 个 int 0、1、2（分别为持有、买入、卖出）。
我的问题是：

我只使用 minmaxscaler 缩放 obs，这是正确的吗？还是我需要缩放所有数据，在这种情况下，我有 200000 行 5 分钟蜡烛？


这是我自定义健身房环境中的 scale_data 乐趣
def scale_data(self,obs):
df1 = obs
df1[[&#39;OPEN&#39;, &#39;HIGH&#39;, &#39;LOW&#39;, &#39;CLOSE&#39;, &#39;rsi&#39;, &#39;TICKVOL&#39;]] = scaler.fit_transform(
obs[[&#39;OPEN&#39;, &#39;HIGH&#39;, &#39;LOW&#39;, &#39;CLOSE&#39;, &#39;rsi&#39;, &#39;TICKVOL&#39;]]) 

df1[[&#39;MINUTE&#39;,&#39;HOUR&#39;,&#39;DAY_OF_WEEK&#39;]] = obs[[&#39;MINUTE&#39;,&#39;HOUR&#39;,&#39;DAY_OF_WEEK&#39;]]

return df1.values



另一件事是，如果答案是缩放所有数据，那么在预测时要传递什么，因为在预测时我可能没有所有的数据，我想用最近的 120 个蜡烛数据进行预测。如果我们缩放所有数据并仅使用最后 120 个进行预测，那么缩放将完全不同！
]]></description>
      <guid>https://stackoverflow.com/questions/79038696/what-input-should-i-use-to-predict-rl-model-will-it-be-scaled-or-inv-scaled</guid>
      <pubDate>Mon, 30 Sep 2024 09:24:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 从头开始​​实现 AdaGrad 优化器的问题</title>
      <link>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</guid>
      <pubDate>Mon, 30 Sep 2024 03:42:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用每个数据集在每次训练迭代中训练一个 LSTM 自动编码器？</title>
      <link>https://stackoverflow.com/questions/79035730/how-can-i-train-an-lstm-autoencoder-for-each-iteration-of-training-with-each-dat</link>
      <description><![CDATA[我一直在尝试构建和训练 LSTM 自动编码器。虽然我使用的参考仅训练了一次模型，但我添加了一个函数，如果每个数据集的每次训练迭代都结束，则多次运行训练。
不过，我并不确定我是否走在正确的轨道上。感觉我的代码在每次迭代中都有可能覆盖训练好的模型。
下面的 Python 代码是否真的会使用每个数据集对模型进行每次迭代训练（有 75 个 CSV 文件可用于训练此模型）？
以下是我在单个函数（trainModel()）中构建和训练模型的 Python 代码。
from sklearn.preprocessing import StandardScaler 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping

# LSTM 网络以输入形状为（n_sample、n_timesteps、features）的等间隔子序列的形式接收输入。
# 我们将使用以下自定义函数来创建这些序列
def create_sequences(X, y, time_steps=1):
Xs, ys = [], []
for i in range(len(X) - time_steps):
v = X.iloc[i:(i + time_steps)].values
Xs.append(v)
ys.append(y.iloc[i + time_steps])
return np.array(Xs), np.array(ys)

def trainModel():
for i in range(75):
fileList = pd.read_csv(&quot;/content/drive/MyDrive/fileList.csv&quot;)
filename = fileList.iloc[i, 0]
temp = pd.read_csv(&quot;/content/drive/MyDrive/dataFolder/&quot;+filename+&quot;.csv&quot;)
train_size = int(len(temp[[&quot;time_abs(%Y-%m-%dT%H:%M:%S.%f)&quot;, &quot;velocity(m/s)&quot;]]))
train = df.iloc[0:train_size]

# 规范化数据
scalar = StandardScaler()
scalar = scalar.fit(train[[&#39;velocity(m/s)&#39;]])

train[&#39;velocity(m/s)&#39;] = scalar.transform(train[[&#39;velocity(m/s)&#39;]])

time_steps = 30

X_train, y_train = create_sequences(train[[&#39;velocity(m/s)&#39;]],train[&#39;velocity(m/s)&#39;],time_steps)

# 构建 LSTM 自动编码器

# 自动编码器是一种神经网络模型旨在学习输入的压缩表示。
# 它们使用监督学习方法进行训练，称为自监督。
# 在这种架构中，编码器 LSTM 模型逐步读取输入序列。
# 读取整个输入序列后，此模型的隐藏状态或输出表示
# 整个输入序列的内部学习表示为固定长度向量。
# 然后将此向量作为输入提供给解码器模型，解码器模型将其解释为输出序列中的每个步骤
# 生成。
# timesteps = X_train.shape[1]
num_features = X_train.shape[2]

model = Sequential()
model.add(LSTM(128,input_shape=(timesteps,num_features)))
model.add(Dropout(0.2))
model.add(RepeatVector(timesteps)) # 重复输入 n 次。
model.add(LSTM(128,return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(num_features))) # 将层应用于输入的每个时间片段。

model.compile(loss=&#39;mae&#39;,optimizer=&#39;adam&#39;)

# 训练自动编码器
early_stop = EarlyStopping(monitor=&#39;val_loss&#39;,patience=3,mode=&#39;min&#39;) # 如果监控指标相对于应用的 3 个时期的模式没有变化，则停止训练
history = model.fit(X_train,y_train,epochs=100,batch_size=32,validation_split=0.1,callbacks=[early_stop],shuffle=False)

model.save(&#39;anomaly_model.h5&#39;, overwrite=False)
model.save(&#39;anomaly_model_&#39;+ i +&#39;.h5&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79035730/how-can-i-train-an-lstm-autoencoder-for-each-iteration-of-training-with-each-dat</guid>
      <pubDate>Sun, 29 Sep 2024 06:28:59 GMT</pubDate>
    </item>
    <item>
      <title>BigQuery Arima Plus 预测高于预期</title>
      <link>https://stackoverflow.com/questions/79032297/bigquery-arima-plus-forecasts-are-higher-than-expected</link>
      <description><![CDATA[我试图以 15 分钟的分辨率预测来自许多不同设备（1000 个，出于性能原因在几百个设备上进行测试）的值。所有设备都记录：名称和值。我尝试过不同的时间范围进行训练，但到目前为止还没有成功。
问题的一些示例：
设备始终记录 40 -&gt; Arima 从第一次预测（30 天训练）开始预测 20-80 的范围
设备记录 60-64 -&gt; Arima 预测 65-120 个峰值，然后在 102 时趋于平稳（1 天训练）。我正在努力理解 Arima plus 如何得出比训练数据集中的所有值都高出数量级的值。
这是 arima plus 模型创建和预测的伪代码：
我使用摘要表作为数据源，数据分辨率为 15 分钟
训练：
CREATE OR REPLACE MODEL predictioning.model
OPTIONS(
MODEL_TYPE=&#39;ARIMA_PLUS&#39;
,TIME_SERIES_TIMESTAMP_COL=&#39;timestamp&#39;
,TIME_SERIES_DATA_COL=&#39;value&#39;
,TIME_SERIES_ID_COL=&#39;id&#39;
-- ,auto_arima = TRUE
,clean_spikes_and_dips = FALSE
,adjust_step_changes = FALSE
-- ,data_frequency = &#39;AUTO_FREQUENCY&#39;
-- ,auto_arima_max_order = 2 -- 默认值为 5
-- ,max_time_series_length = 96 -- 1 d?
) AS
(
SELECT timestamp
,id
,SUM(value) as value

FROM `forecasting.summary`
WHERE CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &lt; _CUTOFF_DATE
AND CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &gt;= DATE_ADD(CURRENT_DATE(), INTERVAL _PERIOD DAY)
GROUP BY 1,2

);

虽然我确实使用了 SUM(value)，但很确定它只有 1 行的总和。从其他尝试中得出。
SELECT 
id
,forecast_timestamp as timestamp
,forecast_value as value
,standard_error 
,confidence_level 
,prediction_interval_lower_bound 
,prediction_interval_upper_bound
FROM 
ML.FORECAST(MODEL `forecasting.forecast`
,STRUCT(672 as horizo​​n -- 192 is 2d; 672 is 7d 
,0.95 as confidence_level
)
)
;

过去对我有用的一种方法是将每 15m 预测为其自己的时间序列。问题是，在这种情况下，它会导致数万或数十万个时间序列。虽然 ArimaPlus 声称能够处理数百万个数据，但如果我进行 30 天训练并且不限制最大 pdq，它的训练性能就已经很慢了。
我在预测表现出每日和每周季节性的数据时也遇到了类似的问题。在当前设备数据的情况下，可能存在与气候和天气条件相关的每日和年度季节性。
我怎样才能让 Arima 发挥作用？如果不行，你会推荐什么方法？]]></description>
      <guid>https://stackoverflow.com/questions/79032297/bigquery-arima-plus-forecasts-are-higher-than-expected</guid>
      <pubDate>Fri, 27 Sep 2024 16:57:25 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 UNet 训练图像分割模型</title>
      <link>https://stackoverflow.com/questions/55671269/cannot-train-image-segmentation-model-with-unet</link>
      <description><![CDATA[我的环境

Ubuntu 18.04
Python 3.6.8
Tensorflow 1.12.0

问题
我用 Tensorflow 做了 UNet 来分割图像。我参考了原论文，实现了相同的结构，只是输出通道数不同（用了 3 而不是 2）。我的模型看起来运行良好，但是损失不收敛，就像心电图一样……
我使用了 MSE 作为损失函数，但有些网站说 MSE 在 UNet 中无效，所以我改用 dice loss。但还是不行。我怀疑是网络结构不好。
代码
import tensorflow as tf

tf.reset_default_graph()

with tf.name_scope(&#39;input&#39;):
X = tf.placeholder(tf.float32, shape=[None, 572, 572, 3], name=&#39;X&#39;)
y = tf.placeholder(tf.float32, shape=[None, 388, 388, 3], name=&#39;y&#39;)

# 编码
with tf.name_scope(&#39;layer1&#39;):
conv1 = tf.layers.conv2d(X, filters=64, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv1&#39;)
conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv2&#39;)

使用 tf.name_scope(&#39;layer2&#39;):
pool1 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool1&#39;)
conv3 = tf.layers.conv2d(pool1, filters=128, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv3&#39;)
conv4 = tf.layers.conv2d(conv3, filters=128, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv4&#39;)

使用tf.name_scope(&#39;layer3&#39;):
pool2 = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool2&#39;)
conv5 = tf.layers.conv2d(pool2, filters=256, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv5&#39;)
conv6 = tf.layers.conv2d(conv5, filters=256, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv6&#39;)

使用 tf.name_scope(&#39;layer4&#39;):
pool3 = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool3&#39;)
conv7 = tf.layers.conv2d(pool3, filters=512, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv7&#39;)
conv8 = tf.layers.conv2d(conv7, filters=512, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv8&#39;)

使用 tf.name_scope(&#39;layer5&#39;):
pool4 = tf.nn.max_pool(conv8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool4&#39;)
conv9 = tf.layers.conv2d(pool4, filters=1024, kernel_size=3, strides=1,激活=tf.nn.relu，名称=&#39;conv9&#39;)
conv10 = tf.layers.conv2d(conv9，filters=1024，kernel_size=3，strides=1，激活=tf.nn.relu，名称=&#39;conv10&#39;)

#解码
使用 tf.name_scope(&#39;layer6&#39;)：
up_conv1 = tf.layers.conv2d_transpose(conv10，filters=512，kernel_size=2，strides=2)
croped_conv8 = tf.image.central_crop(conv8，7/8)
concat1 = tf.concat([croped_conv8，up_conv1]，axis=-1)
conv11 = tf.layers.conv2d(concat1，filters=512，kernel_size=3，激活=tf.nn.relu， name=&#39;conv11&#39;)
conv12 = tf.layers.conv2d(conv11, filters=512, kernel_size=3,activation=tf.nn.relu, name=&#39;conv12&#39;)

使用 tf.name_scope(&#39;layer7&#39;)：
up_conv2 = tf.layers.conv2d_transpose(conv12, filters=256, kernel_size=2, strides=2)
croped_conv6 = tf.image.central_crop(conv6, 13/17)
concat2 = tf.concat([croped_conv6, up_conv2], axis=-1)
conv13 = tf.layers.conv2d(concat2, filters=256, kernel_size=3,activation=tf.nn.relu, name=&#39;conv13&#39;)
conv14 = tf.layers.conv2d(conv13, filters=256, kernel_size=3,activation=tf.nn.relu, name=&#39;conv14&#39;)

使用 tf.name_scope(&#39;layer8&#39;)：
up_conv3 = tf.layers.conv2d_transpose(conv14, filters=128, kernel_size=2, strides=2)
croped_conv4 = tf.image.central_crop(conv4, 5/7)
concat3 = tf.concat([croped_conv4, up_conv3], axis=-1)
conv15 = tf.layers.conv2d(concat3, filters=128, kernel_size=3,activation=tf.nn.relu, name=&#39;conv15&#39;)
conv16 = tf.layers.conv2d(conv15, filters=128，kernel_size=3，activation=tf.nn.relu，name=&#39;conv16&#39;)

使用 tf.name_scope(&#39;layer8&#39;)：
up_conv4 = tf.layers.conv2d_transpose(conv16，filters=64，kernel_size=2，strides=2)
croped_conv2 = tf.image.central_crop(conv2，49/71)
concat4 = tf.concat([croped_conv2，up_conv4]，axis=-1)
conv17 = tf.layers.conv2d(concat4，filters=64，kernel_size=3，activation=tf.nn.relu，name=&#39;conv17&#39;)
conv18 = tf.layers.conv2d(conv17，filters=64，kernel_size=3，激活=tf.nn.relu，名称=&#39;conv18&#39;)

输出=tf.layers.conv2d(conv18，过滤器=3，kernel_size=1，名称=&#39;output&#39;)

使用tf.name_scope(&#39;train&#39;)：
骰子=2 * tf.math.reduce_sum(output*y) / (tf.math.reduce_sum(output) + tf.math.reduce_sum(y) + 1)
损失=1 - 骰子
优化器=tf.train.AdamOptimizer()
训练操作=优化器.最小化(损失)

使用tf.name_scope(&#39;save&#39;)：
保存器=tf.train.Saver()
损失_sumary=tf.summary.scalar(&#39;ls&#39;，损失)
文件写入器=tf.summary.FileWriter(&#39;./&#39;， tf.get_default_graph())
]]></description>
      <guid>https://stackoverflow.com/questions/55671269/cannot-train-image-segmentation-model-with-unet</guid>
      <pubDate>Sun, 14 Apr 2019 01:32:07 GMT</pubDate>
    </item>
    <item>
      <title>从 K 折交叉验证中选择哪个模型</title>
      <link>https://stackoverflow.com/questions/45480894/which-model-to-pick-from-k-fold-cross-validation</link>
      <description><![CDATA[我读过关于交叉验证的文章，以及它如何用于选择最佳模型和估计参数，但我并不真正理解它的含义。
假设我建立一个线性回归模型并进行 10 倍交叉验证，我认为这 10 个模型中的每一个都会有不同的系数值，现在我应该从 10 个不同的模型中选择哪一个作为我的最终模型或估计参数。
或者我们使用交叉验证只是为了找到平均误差（在我们的例子中是 10 个模型的平均值）并与另一个模型进行比较？]]></description>
      <guid>https://stackoverflow.com/questions/45480894/which-model-to-pick-from-k-fold-cross-validation</guid>
      <pubDate>Thu, 03 Aug 2017 09:59:13 GMT</pubDate>
    </item>
    </channel>
</rss>