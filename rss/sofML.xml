<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 25 Aug 2024 12:27:33 GMT</lastBuildDate>
    <item>
      <title>使用对最终目标有贡献的特征子集来训练核岭回归</title>
      <link>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</link>
      <description><![CDATA[我正在使用 Python 中的核岭回归 (KRR) 和 scikit-learn 研究机器学习问题。我的目标是训练一个内核，其中特征子集有助于目标预测。
例如，我们知道在标准情况下，训练将针对以下内容进行：
特征数据 X ((n_samples, n_features)) -&gt; Y (n_samples)
我想要做的是，训练回归模型具有维度映射：(n_subset) -&gt; y_subset
因此，(n_features) 由大小为 (n_subset) 的子组组成，这些子组本质上具有类似的起源，它们的模式重复相似，但值并不严格相同。
假设 (n_features) = (n_subset)*(n_terms)。
作为一个过早的解决方案，我已经实现了一个模型，其中每个特征子集对最终预测的贡献相同。以下是示例代码：
import numpy as np
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import GridSearchCV

def train_kernel_with_weights(X, Y, n_subset, kernel_param_grid):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset # 内核预测项的数量，将加总为总数
X_sub_rearr = X.reshape(nset * n_kerterm, n_subset)

kr1 = GridSearchCV(KernelRidge(), param_grid=kernel_param_grid, cv=5)

# 此处，测试了“等权重”方案，任意假设每个 (n_subset)-&gt;y_subset 项
# 对总 Y 的贡献相等。
kr1.fit(X_sub_rearr, np.repeat(Y/n_kerterm, n_kerterm))

return kr1

# 给定一个具有映射 (n_subset -&gt; value) 的核，获取子集总和预测值
def predict_krrsum_1(n_subset, kr1, X):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset

# 使用 n_subset 分割每个数据点的特征（可能是三元组）
X_sub_rearr = X.reshape(nset*n_kerterm,n_subset)
Y_krrsub_flat = kr1.predict(X_sub_rearr) # 1d 数组 (nset*n_kerterm)
Y_krrsub = Y_krrsub_flat.reshape(-1,n_kerterm) # (nset, n_kerterm)
Y_krrsum_1 = np.sum(Y_krrsub,axis=1)
return Y_krrsum_1


那么我的问题是：对于核训练函数：train_kernel_with_weights，
我该如何优化子集的权重，而不是随意使用相等的权重？]]></description>
      <guid>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</guid>
      <pubDate>Sun, 25 Aug 2024 12:10:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 在低资源语言和葡萄牙语之间进行机器翻译的语言模型</title>
      <link>https://stackoverflow.com/questions/78911175/a-language-model-for-machine-translation-between-a-low-resource-language-and-por</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911175/a-language-model-for-machine-translation-between-a-low-resource-language-and-por</guid>
      <pubDate>Sun, 25 Aug 2024 12:06:55 GMT</pubDate>
    </item>
    <item>
      <title>Resnet 训练和评估模式不一致</title>
      <link>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</link>
      <description><![CDATA[我正在尝试在 torch 中实现 Resnet。但我发现前向传递的输出在训练和评估模式之间差异很大。由于训练和评估模式除了 batch norm 和 dropout 之外不影响任何东西，所以我不知道结果是否有意义。
下面是我的测试代码：
import torch
from torch import nn
from torchvision import models

class resnet_lstm(torch.nn.Module):
def __init__(self):
super(resnet_lstm, self).__init__()
resnet = models.resnet50(pretrained=True)
self.share = torch.nn.Sequential()
self.share.add_module(&quot;conv1&quot;, resnet.conv1)
self.share.add_module(&quot;bn1&quot;, resnet.bn1) # 使用 BatchNorm3d
self.share.add_module(&quot;relu&quot;, resnet.relu)
self.share.add_module(&quot;maxpool&quot;, resnet.maxpool)
self.share.add_module(&quot;layer1&quot;, resnet.layer1)
self.share.add_module(&quot;layer2&quot;, resnet.layer2)
self.share.add_module(&quot;layer3&quot;, resnet.layer3)
self.share.add_module(&quot;layer4&quot;, resnet.layer4)
self.share.add_module(&quot;avgpool&quot;, resnet.avgpool)
self.fc = nn.Sequential(nn.Linear(2048, 512),
nn.ReLU(),
nn.Linear(512, 7))

def forward(self, x):
x = x.view(-1, 3, 224, 224)
x = self.share(x)
返回x

model = resnet_lstm()

input_ = torch.randn(1, 3, 224, 224)
model.train()
print(&quot;训练模式输出&quot;, model(input_))
model.eval()
print(&quot;评估模式输出&quot;, model(input_))


终端输出：
训练模式输出 tensor([[[[0.3603]],

[[0.5518]],

[[0.4599]],

...,

[[0.3381]],

[[0.4445]],

[[0.3481]]]], grad_fn=&lt;MeanBackward1&gt;)
评估模式输出tensor([[[[0.1582]],

[[0.1822]],

[[0.0000]],

...,

[[0.0567]],

[[0.0054]],

[[0.3605]]]], grad_fn=&lt;MeanBackward1&gt;)

如您所见，训练和评估模式的输出彼此非常不同。这会损害性能吗？]]></description>
      <guid>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</guid>
      <pubDate>Sun, 25 Aug 2024 11:07:12 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：形状'[30, 167, 512]'对于大小为 7695360 的输入无效</title>
      <link>https://stackoverflow.com/questions/78910546/runtimeerror-shape-30-167-512-is-invalid-for-input-of-size-7695360</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78910546/runtimeerror-shape-30-167-512-is-invalid-for-input-of-size-7695360</guid>
      <pubDate>Sun, 25 Aug 2024 06:06:35 GMT</pubDate>
    </item>
    <item>
      <title>什么是可微分渲染？如何从网格渲染图像？[关闭]</title>
      <link>https://stackoverflow.com/questions/78910450/whats-differentiable-rendering-how-can-i-render-a-image-from-a-mesh</link>
      <description><![CDATA[我正在考虑编写模型训练代码，此代码的其中一步是从特定相机视图获取渲染图像，以便我可以得到原始图像和最终渲染图像之间的损失；但我对可微分渲染知之甚少，因此，输入是反照率、SG 照明、金属、粗糙度、相机矩阵和网格，输出是渲染图像，什么代码库可以进行可微分渲染？完整的过程应该是什么样的？
输入是反照率、SG 照明、金属、粗糙度、相机矩阵和网格，我想找到一种方法来获取渲染图像]]></description>
      <guid>https://stackoverflow.com/questions/78910450/whats-differentiable-rendering-how-can-i-render-a-image-from-a-mesh</guid>
      <pubDate>Sun, 25 Aug 2024 04:37:22 GMT</pubDate>
    </item>
    <item>
      <title>在拆分之前仅对 tf.data.Dataset 进行一次混洗</title>
      <link>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</link>
      <description><![CDATA[我正在使用 tf.data.Dataset 来为我的模型的 model.fit() 方法提供数据。
整个数据集无法放入我的 RAM 中，因此我需要按批次加载它，因此我考虑使用数据集生成器 (tf.data.Dataset) 在训练期间加载它并预取批次。
我用来获取训练、验证和测试数据集的函数如下所示：
def get_all_datasets(batch_size=64):
path = &#39;a/path&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path = os.path.join(path, img_folder)
mk_path = os.path.join(path, mask_folder)

path = &#39;another/path/&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path2 = os.path.join(path, img_folder)
mk_path2 = os.path.join(path, mask_folder)

# 创建数据集
ds1 = create_dataset(im_path, mk_path, 256, 256)
ds2 = create_dataset(im_path2, mk_path2, 256, 256)
ds = ds1.concatenate(ds2)

# 在拆分之前对整个数据集进行一次打乱
ds = ds.shuffle(buffer_size=tf.data.experimental.cardinality(ds).numpy(),
reshuffle_each_iteration=True)

# 拆分为训练集、验证集和测试集
total_size = tf.data.experimental.cardinality(ds).numpy()
train_size = int(total_size * 0.7)
val_size = int(total_size * 0.15)
test_size = total_size - train_size - val_size

# 创建单独的数据集
train_dataset = ds.take(train_size)
remaining = ds.skip(train_size)
val_dataset = remaining.take(val_size)
test_dataset = remaining.skip(val_size)

# 缓存数据集以防止重新洗牌/重新加载
train_dataset = train_dataset.cache().shuffle(batch_size).batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)

return train_dataset, val_dataset, test_dataset

第一次洗牌是为了使训练、验证和测试中的“场景”比例相等。这种洗牌只能进行一次，以避免污染训练、验证和测试集。
我不知道如何避免每次都重新洗牌整个集合。也许这个数据集方法也不合适。

在训练期间，整个数据集被洗牌两次，一次用于训练，另一次用于验证。我看到 shuffle 缓冲区已填满整个数据集基数。
我还考虑过在 shuffle 一次后创建三个文件夹（train、val 和 test）以获取拆分，然后仅使用 shuffle 方法对训练数据进行 shuffle，但如果这是解决问题的最简单方法，我会感到惊讶...
我尝试在 shuffle 之前设置一个 if 语句来检查 shuffle 是否已完成，但 shuffle 仍然发生。我的想法是执行图已经“安排好”，并且在惰性解释期间将全局变量设置为 False。]]></description>
      <guid>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</guid>
      <pubDate>Sun, 25 Aug 2024 03:01:31 GMT</pubDate>
    </item>
    <item>
      <title>模型vgg16的维度问题（图像分割）</title>
      <link>https://stackoverflow.com/questions/78910294/dimension-problem-with-model-vgg16-image-segmentation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78910294/dimension-problem-with-model-vgg16-image-segmentation</guid>
      <pubDate>Sun, 25 Aug 2024 01:39:57 GMT</pubDate>
    </item>
    <item>
      <title>解决 VAE 中的梯度爆炸问题</title>
      <link>https://stackoverflow.com/questions/55756020/resolve-exploding-gradient-in-vae</link>
      <description><![CDATA[如何解决深度生成模型 (VAE) 中的梯度爆炸问题？
注意：数据集的列中包含大量 NaN 值]]></description>
      <guid>https://stackoverflow.com/questions/55756020/resolve-exploding-gradient-in-vae</guid>
      <pubDate>Fri, 19 Apr 2019 02:56:08 GMT</pubDate>
    </item>
    <item>
      <title>GAN 中的损失函数</title>
      <link>https://stackoverflow.com/questions/49988496/loss-functions-in-gans</link>
      <description><![CDATA[我正在尝试构建一个简单的 mnist GAN，不用多说，它没有成功。我搜索了很多，修复了大部分代码。虽然我真的不明白损失函数是如何工作的。
这是我所做的：
loss_d = -tf.reduce_mean(tf.log(discriminator(real_data))) # 最大化
loss_g = -tf.reduce_mean(tf.log(discriminator(generator(noise_input), trainable = False))) # 最大化，因为 d(g) 而不是 1 - d(g)
loss = loss_d + loss_g

train_d = tf.train.AdamOptimizer(learning_rate).minimize(loss_d)
train_g = tf.train.AdamOptimizer(learning_rate).minimize(loss_g)

我的损失值为 -0.0。你能解释一下如何处理 GAN 中的损失函数吗？]]></description>
      <guid>https://stackoverflow.com/questions/49988496/loss-functions-in-gans</guid>
      <pubDate>Mon, 23 Apr 2018 19:21:53 GMT</pubDate>
    </item>
    <item>
      <title>（MNIST - GAN）鉴别器和生成器误差在第一次迭代后降至接近零</title>
      <link>https://stackoverflow.com/questions/46731089/mnist-gan-discriminator-and-generator-error-dropping-close-to-zero-after-fir</link>
      <description><![CDATA[为了深入了解生成对抗网络，我尝试基于此斯坦福大学作业使用 tensorflow 自己为 MNIST 数据集实现 GAN。
我仔细检查并研究了给定练习的解决方案，并通过了测试。但是，我的生成器只会产生噪音。
我很确定我正确使用了辅助函数，所有测试都通过了，并且我在网上找到了显示完全相同实现的参考资料。因此，可能出错的地方只是鉴别器和生成器架构：
def discriminator(x):
with tf.variable_scope(&quot;discriminator&quot;):
l_1 = leaky_relu(tf.layers.dense(x, 256,activation=None))
l_2 = leaky_relu(tf.layers.dense(l_1, 256,activation=None))
logits = tf.layers.dense(l_2, 1,activation=None)
return logits

def generator(z):
with tf.variable_scope(&quot;generator&quot;):
l_1 = tf.maximum(tf.layers.dense(z, 1024,activation=None), 0)
l_2 = tf.maximum(tf.layers.dense(l_1, 1024,activation=None), 0)
img = tf.tanh(tf.layers.dense(l_2, 784,activation=None))
return img

我发现生成器和鉴别器错误在第一次迭代中下降到接近零。
Iter: 0, D: 1.026, G:0.6514
Iter: 50, D: 2.721e-05, G:5.066e-06
Iter: 100, D: 1.099e-05, G:3.084e-06
Iter: 150, D: 7.546e-06, G:1.946e-06
Iter: 200, D: 3.386e-06, G：1.226e-06
...

如果学习率较低，例如 1e-7，鉴别器和生成器的错误率会缓慢衰减，但最终会降至零，只会产生噪音。
Iter：0，D：1.722，G：0.6772
Iter：50，D：1.704，G：0.665
Iter：100，D：1.698，G：0.661
Iter：150，D：1.663，G：0.6594
Iter：200，D：1.661，G：0.6574
...

我已启动并运行 TensorFlow 图进行实验，但到目前为止未能从中解读出任何有意义的东西。
如果您有任何建议或可以推荐一种调试技术，我将非常乐意听取。
根据要求，这是我的 GAN - Loss 代码：
def gan_loss(logits_real, logits_fake):
labels_real = tf.ones_like(logits_real)
labels_fake = tf.zeros_like(logits_fake)

d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real, labels=labels_real)
d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=labels_fake)
D_loss = tf.reduce_mean(d_loss_real + d_loss_fake)

G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=labels_fake))
返回 D_loss, G_loss
]]></description>
      <guid>https://stackoverflow.com/questions/46731089/mnist-gan-discriminator-and-generator-error-dropping-close-to-zero-after-fir</guid>
      <pubDate>Fri, 13 Oct 2017 13:26:59 GMT</pubDate>
    </item>
    <item>
      <title>在生成对抗网络中，如何使用鉴别器的输出来训练生成器</title>
      <link>https://stackoverflow.com/questions/44728913/how-the-generator-is-trained-with-the-output-of-discriminator-in-generative-adve</link>
      <description><![CDATA[最近我了解了生成对抗网络。
对于生成器的训练，我有点困惑它是如何学习的。 这里是 GAN 的实现：
`# 训练生成器
z = Variable(xp.random.uniform(-1, 1, (batchsize, nz), dtype=np.float32))
x = gen(z)
yl = dis(x)
L_gen = F.softmax_cross_entropy(yl, Variable(xp.zeros(batchsize, dtype=np.int32)))
L_dis = F.softmax_cross_entropy(yl, Variable(xp.ones(batchsize, dtype=np.int32)))

# 训练鉴别器

x2 = Variable(cuda.to_gpu(x2))
yl2 = dis(x2)
L_dis += F.softmax_cross_entropy(yl2, Variable(xp.zeros(batchsize, dtype=np.int32)))

#print &quot;forward done&quot;

o_gen.zero_grads()
L_gen.backward()
o_gen.update()

o_dis.zero_grads()
L_dis.backward()
o_dis.update()`

因此，正如论文中提到的那样，它会为生成器计算损失。
但是，它会根据判别器输出调用生成器后向函数。判别器输出只是一个数字（而不是数组）。 
但我们知道，一般来说，为了训练网络，我们会计算最后一层的损失函数（最后一层输出与实际输出之间的损失），然后计算梯度。例如，如果输出是 64*64，那么我们将其与 64*64 图像进行比较，然后计算损失并进行反向传播。
但是，在我看到的生成对抗网络中的代码中，我看到他们从鉴别器输出（只是一个数字）计算生成器的损失，然后他们调用生成器的反向传播。生成器的最后一层例如是 64*64 像素，但鉴别器损失是 1*1（这与通常的网络不同）所以我不明白它是如何导致生成器被学习和训练的？
我认为如果我们连接两个网络（连接生成器和鉴别器），然后调用反向传播但只更新生成器参数，这是有意义的，它应该可以工作。但我在代码中看到的内容完全不同。
所以我想问这怎么可能？
谢谢 ]]></description>
      <guid>https://stackoverflow.com/questions/44728913/how-the-generator-is-trained-with-the-output-of-discriminator-in-generative-adve</guid>
      <pubDate>Fri, 23 Jun 2017 19:45:46 GMT</pubDate>
    </item>
    <item>
      <title>使用训练有素的字符级 LSTM 模型生成文本</title>
      <link>https://stackoverflow.com/questions/43391452/generate-text-with-a-trained-character-level-lstm-model</link>
      <description><![CDATA[我训练了一个模型，目的是生成句子，如下所示：
我输入 2 个序列作为训练示例：x 是字符序列，y 是相同的移位 1 的序列。该模型基于 LSTM，使用 tensorflow 创建。
我的问题是：由于模型接受一定大小的输入序列（在我的情况下为 50），我如何仅给它单个字符作为种子进行预测？我在一些例子中看到，经过训练后，它们只需输入单个字符即可生成句子。 
这是我的代码：
 使用 tf.name_scope(&#39;input&#39;)：
x = tf.placeholder(tf.float32, [batch_size, truncated_backprop], name=&#39;x&#39;)
y = tf.placeholder(tf.int32, [batch_size, truncated_backprop], name=&#39;y&#39;)

使用 tf.name_scope(&#39;weights&#39;)：
W = tf.Variable(np.random.rand(n_hidden, num_classes), dtype=tf.float32)
b = tf.Variable(np.random.rand(1, num_classes), dtype=tf.float32)

input_series = tf.split(x, truncated_backprop, 1)
labels_series = tf.unstack(y, axis=1)

使用 tf.name_scope(&#39;LSTM&#39;)：
cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)
cell = tf.contrib.rnn.MultiRNNCell([cell] * n_layers)

states_series, current_state = tf.contrib.rnn.static_rnn(cell, input_series, \
dtype=tf.float32)

logits_series = [tf.matmul(state, W) + b for state in states_series]
prediction_series = [tf.nn.softmax(logits) for logits in logits_series]

损失 = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) \
for logits, labels, in zip(logits_series, labels_series)]
total_loss = tf.reduce_mean(losses)

train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)
]]></description>
      <guid>https://stackoverflow.com/questions/43391452/generate-text-with-a-trained-character-level-lstm-model</guid>
      <pubDate>Thu, 13 Apr 2017 11:45:57 GMT</pubDate>
    </item>
    <item>
      <title>使用生成模型还是判别模型进行分类？</title>
      <link>https://stackoverflow.com/questions/43197513/use-generative-or-discriminative-model-for-classification</link>
      <description><![CDATA[我是机器学习的初学者！只是想了解一下我应该如何处理分类问题。鉴于手头的问题是分类一个对象是属于 A 类还是 B 类，我想知道我应该使用生成模型还是判别模型。我有两个问题。

判别模型似乎在分类问题上做得更好，因为它只关心如何绘制决策边界，而不关心其他事情。

问：但是，如果数据集很小，只有大约 80 个 A 类对象和不到 10 个 B 类对象需要训练和测试，判别模型会过度拟合，因此生成模型会表现更好吗？

此外，由于 A 类对象和 B 类对象的数量差异非常大，训练的模型很可能只能识别 A 类对象。即使模型将所有对象归类为 A 类，这仍然会产生非常高的准确度得分。 

问：鉴于没有其他方法可以增加 B 类数据集的大小，有什么想法可以减少这种偏见？ ]]></description>
      <guid>https://stackoverflow.com/questions/43197513/use-generative-or-discriminative-model-for-classification</guid>
      <pubDate>Tue, 04 Apr 2017 02:58:41 GMT</pubDate>
    </item>
    <item>
      <title>简单来说，损失函数是什么？</title>
      <link>https://stackoverflow.com/questions/42877989/what-is-a-loss-function-in-simple-words</link>
      <description><![CDATA[有人能用简单的语言并举几个例子解释一下机器学习/神经网络领域中的损失函数是什么吗？
这是我在学习 Tensorflow 教程时想到的：
https://www.tensorflow.org/get_started/get_started]]></description>
      <guid>https://stackoverflow.com/questions/42877989/what-is-a-loss-function-in-simple-words</guid>
      <pubDate>Sat, 18 Mar 2017 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的前向传递和后向传递是什么？</title>
      <link>https://stackoverflow.com/questions/36740533/what-are-forward-and-backward-passes-in-neural-networks</link>
      <description><![CDATA[神经网络中的前向传递和后向传递是什么意思？
每个人在谈论反向传播和时期时都会提到这些表达方式。
我的理解是前向传递和后向传递共同构成一个时期。]]></description>
      <guid>https://stackoverflow.com/questions/36740533/what-are-forward-and-backward-passes-in-neural-networks</guid>
      <pubDate>Wed, 20 Apr 2016 10:10:19 GMT</pubDate>
    </item>
    </channel>
</rss>