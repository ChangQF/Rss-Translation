<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Dec 2023 09:14:04 GMT</lastBuildDate>
    <item>
      <title>Edge Impulse 物体检测中的多个标签检测，无需相应产品</title>
      <link>https://stackoverflow.com/questions/77710120/multiple-label-detections-in-edge-impulse-object-detection-without-corresponding</link>
      <description><![CDATA[我正在使用 Edge Impulse 开发一个对象检测项目。当我启动相机时，我遇到了检测到多个标签的情况，即使框架中没有相应的实际产品。这就提出了一个问题：为什么这些标签在没有产品的情况下出现？
我已尝试检查数据并优化训练参数，但未能解决此问题。这可能是由于图像中的噪声或其他一些因素造成的吗？ Edge Impulse 中是否存在可能导致此行为的特定方法或技术？
任何有关解决此问题的见解或指导将不胜感激。谢谢。


]]></description>
      <guid>https://stackoverflow.com/questions/77710120/multiple-label-detections-in-edge-impulse-object-detection-without-corresponding</guid>
      <pubDate>Sun, 24 Dec 2023 09:07:02 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习将地址文本拆分为多个组件</title>
      <link>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</link>
      <description><![CDATA[我对机器学习完全陌生，所以不知道应该使用哪个方向或方法来解决我的问题。
问题：
我有一个 CSV 文件，每一行代表地址的不同组成部分，例如城市、街道、门牌号等，然后一列在一行中包含组合地址，具有预定义的格式，例如街道房屋号码、邮政编码、城市。
我想要的是判断用户输入的地址文本的不同组成部分，例如我想知道用户是否输入了所有组件，或者只是输入了街道名称和城市等，以及这些组件的值是什么。
问题：
我可以通过机器学习技术来实现这一目标，以便我使用 CSV 文件教导模型，这就是地址文本如何拆分为不同的组件，然后期望它根据该训练为我提供不同的组件？
我尝试向自己介绍 ML.NET，因为我是一名 C# 开发人员，并尝试查看 ML.NET 中是否有规定可以通过使用 CSV 文件训练模型来从给定文本字符串中获取多个标签。由于我对机器学习概念非常陌生，因此无法理解机器学习的不同术语和技术。]]></description>
      <guid>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</guid>
      <pubDate>Sun, 24 Dec 2023 08:47:00 GMT</pubDate>
    </item>
    <item>
      <title>向 Transformer 架构添加生成/预测元素</title>
      <link>https://stackoverflow.com/questions/77709804/adding-a-generative-predictive-element-to-transformer-architecture</link>
      <description><![CDATA[我偶然发现了标准变压器架构的一种变体，称为“iTransformer”，它基本上采用时间序列的倒转视图，并将每个变量的整个时间序列独立地嵌入到（变量）令牌中。它通过自注意力捕获多变量相关性，并利用层归一化和前馈网络模块来学习更好的序列全局表示以进行时间序列预测。
我正在尝试并想知道如何修改代码，以便它可以在没有基本事实的情况下生成/预测未来的时间序列数据。
这是原始的 github 存储库：
https://github.com/thuml/iTransformer/blob/34d9aa016917dc8c8db59bc00f9 b805ceea4042d/ data_provider/data_factory.py
我假设我必须修改测试函数，以便它可以接受之前的预测和输入来创建新的预测，直到未来 x 天？我一直在寻找一些正确方向的指针。]]></description>
      <guid>https://stackoverflow.com/questions/77709804/adding-a-generative-predictive-element-to-transformer-architecture</guid>
      <pubDate>Sun, 24 Dec 2023 05:56:12 GMT</pubDate>
    </item>
    <item>
      <title>导入 defs 时 DLL 加载失败：找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/77709781/dll-load-failed-while-importing-defs-the-specified-procedure-could-not-be-found</link>
      <description><![CDATA[我正在使用 Conda env 和 jupyter Notebook 来运行我的 Python 程序。所以我只想保存语义分割的模型，如YouTube教程所示。
这些是我正在使用的软件包。
&lt;前&gt;&lt;代码&gt;absl-py：2.0.0
cv2：无法从提供的列表中确定版本
numpy：1.26.2
matplotlib：3.8.0
补丁：0.2.3
皮尔：10.0.1
张量流：2.15.0
h5py：3.10.0
喀拉拉邦：2.15.0

我想跑步
model.save(&#39;models/satellite_standard_unet_100epochs_24Dec2023.hdf5&#39;)

但是当我导入 h5py
&lt;前&gt;&lt;代码&gt;导入h5py

它抛出一个错误
单元格输入[89]，第 1 行
----&gt; 1 导入 h5py

文件 c:\Users\User\anaconda3\envs\road-extract\lib\site-packages\h5py\__init__.py:33
     30 其他：
     31 提高
---&gt; 33 从 .导入版本
     35 如果版本.hdf5_version_tuple！=版本.hdf5_built_version_tuple：
     36 _warn((“当 h5py 是针对 {1} 构建时，它正在针对 HDF5 {0} 运行，”
     37 &quot;这可能会导致问题&quot;).format(
     38 &#39;{0}.{1}.{2}&#39;.format(*version.hdf5_version_tuple),
     39 &#39;{0}.{1}.{2}&#39;.format(*version.hdf5_built_version_tuple)
     40））

文件 c:\Users\User\anaconda3\envs\road-extract\lib\site-packages\h5py\version.py:15
     10“”“”
     11 h5py 的版本控制模块。
     12“”“”
     14 从集合导入namedtuple
---&gt; 15 从 .将 h5 导入为 _h5
     16导入系统
     17 导入numpy

文件h5py\h5.pyx:1，在init h5py.h5()中

ImportError: 导入 defs 时 DLL 加载失败: 找不到指定的过程。

这可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/77709781/dll-load-failed-while-importing-defs-the-specified-procedure-could-not-be-found</guid>
      <pubDate>Sun, 24 Dec 2023 05:45:40 GMT</pubDate>
    </item>
    <item>
      <title>如何将 model.safetensor 转换为 pytorch_model.bin？</title>
      <link>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</link>
      <description><![CDATA[我正在微调预训练的 bert 模型，但遇到了一个奇怪的问题：
当我使用 CPU 进行微调时，代码会像这样保存模型：

使用“pytorch_model.bin”。但是当我使用 CUDA（我必须这样做）时，模型会像这样保存：

当我尝试加载这个“model.safetensors”时将来，它会引发错误“pytorch_model.bin”未找到。我使用两个不同的 venv 来测试 CPU 和 CUDA。
如何解决这个问题？是版本问题吗？
我正在使用sentence_transformers框架来微调模型。
这是我的训练代码：
检查点 = &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;

word_embedding_model = models.Transformer(checkpoint,cache_dir=f&#39;model/{checkpoint}&#39;)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=&#39;mean&#39;)
模型 = SentenceTransformer(模块=[word_embedding_model, pooling_model], device=&#39;cuda&#39;)


train_loss = 损失.CosineSimilarityLoss(模型)

evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name=&#39;sbert&#39;)

model.fit(train_objectives=[(train_dataloader, train_loss)]，epochs=5，evaluator=evaluator，show_progress_bar=True，output_path=f&#39;model_FT/{checkpoint}&#39;，save_best_model=True)

我确实在两个不同的环境中尝试了测试，我希望代码能够保存一个“pytorch_model.bin”文件。不是“model.safetensors”。
编辑：我真的还不知道，但似乎是新版本的 Transformer 库导致了这个问题。我看到使用拥抱脸可以加载安全张量，但使用句子转换器（我需要使用）则不能。]]></description>
      <guid>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</guid>
      <pubDate>Sat, 23 Dec 2023 20:43:20 GMT</pubDate>
    </item>
    <item>
      <title>什么类型的 ML 模型可以检测过渡对中图像的哪一部分发生了变化？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</link>
      <description><![CDATA[假设我有一个图像对的数据集，其中每一对代表给定问题中的一个序列
例如，下面的一对图像代表 8-MNIST 拼图问题中的一个步骤（其中代表空白的“0”图块与“3”图块一起移动）。

对于每一对，我还有一个表示给定动作的 one-hot 向量，例如对于上面的对，one-hot 将是 [0 0 0 ... 0 1 0] 表示“” 0”瓷砖被推倒了
是否有一个模型可以从给定的图像和动作的标签中学习，以预测图像的哪一部分将被修改？ （给定示例中图像的“0 3”部分）。
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</guid>
      <pubDate>Sat, 23 Dec 2023 19:04:39 GMT</pubDate>
    </item>
    <item>
      <title>从口腔内扫描中检测点[关闭]</title>
      <link>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</link>
      <description><![CDATA[我们需要从口腔内扫描中检测点，但我们不知道如何开始或如何注释图像以训练模型来检测看不见的图像上的点。有人可以指导我们吗？
我们尝试了姿势检测和不同的代码进行训练，但数据不准确。]]></description>
      <guid>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</guid>
      <pubDate>Sat, 23 Dec 2023 18:49:39 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默病 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每个图像预测相同的类别（“CN - 认知正常”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
TFLite 模型的代码：https://colab.research.google。 com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=sharing
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）


解释器 = tf.lite.Interpreter(model_path=&quot;updated_final_model.tflite&quot;)
解释器.allocate_tensors()


class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
图像 = image.astype(&#39;float32&#39;) / 255.0
图像 = cv2.resize(图像, (150, 150))
图像 = np.expand_dims(图像, 轴=0)
返回图像

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
尝试：
    文件 = request.files[&#39;文件&#39;]
    image_file = Image.open(io.BytesIO(file.read()))
    图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
    print(&quot;输入图像形状：&quot;, image.shape)

    预处理图像 = 预处理图像（图像）

    
    terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)

    
    解释器.invoke()

    
    output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])

    Predicted_class = np.argmax(output_tensor, axis=1)[0]

  
    print(“预测类别索引：”,predicted_class)

    结果 = {“预测”：class_names[预测类]}
    返回 jsonify(结果)
除了异常 e：
    返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

我已尝试以下步骤来解决该问题：

我在 Flask API 代码的不同位置添加了打印语句，以了解执行流程。我预计会看到针对不同输入图像的不同预测，但 API 始终预测相同的类别（“CN - 认知正常”）。

我查看了加载的 TensorFlow Lite 模型并检查了其输入和输出详细信息。该模型似乎加载正确，并且输入图像按预期进行了预处理。我希望预测与模型的训练性能保持一致。

考虑到我的训练代码和 Flask API 之间颜色空间处理的差异，我尝试了不同的颜色空间转换。然而，这并没有解决问题。

我仔细检查了训练和 API 代码中的图像预处理步骤，以确保一致性。 Flask API 中的预处理与训练过程保持一致。


我希望 Flask API 能够根据输入图像提供多种预测，与模型在训练期间的性能保持一致。每个输入图像都应该产生与其实际类别相对应的预测。
无论图像的实际内容如何，​​Flask API 都会一致地为每个输入图像预测相同的类别（“CN - 认知正常”）。这种行为与训练期间模型的准确性不一致。
TFLite 转换或模型准确性是否存在我可能忽略的潜在问题？]]></description>
      <guid>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Sat, 23 Dec 2023 18:35:50 GMT</pubDate>
    </item>
    <item>
      <title>Python Tensorflow.keras LSTM：类型错误：`generator` 产生了形状为 (36, 36, 147) 的元素，而预期形状为 (36, 147) 的元素</title>
      <link>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</guid>
      <pubDate>Sat, 23 Dec 2023 18:05:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tf.GradientTape 训练训练/有效/测试集？</title>
      <link>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</link>
      <description><![CDATA[我想问一下如何使用train/valid/test模型来训练模型。
我主要模仿这里的代码（https://www.tensorflow.org/guide/ keras/writing_a_training_loop_from_scratch）。
这是生成数据集和基本内容的代码。
导入keras
从 keras 导入层
将 numpy 导入为 np

输入= keras.Input（形状=（784，），名称=“数字”）
x1 = 层.Dense(64, 激活=“relu”)(输入)
x2 = 层.Dense(64, 激活=“relu”)(x1)
输出=layers.Dense(10,name=“预测”)(x2)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备训练数据集。
批量大小 = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# 保留 10,000 个样本用于验证。
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

# 准备训练数据集。
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

# 准备验证数据集。
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

# 获取模型
输入= keras.Input（形状=（784，），名称=“数字”）
x = 层.Dense(64, 激活=“relu”, 名称=“dense_1”)(输入)
x = 层.Dense(64, 激活=“relu”, 名称=“dense_2”)(x)
输出=layers.Dense(10,name=“预测”)(x)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器来训练模型。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备指标。
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

@tf.函数
def train_step(x, y):
    使用 tf.GradientTape() 作为磁带：
        logits = 模型(x, 训练=True)
        loss_value = loss_fn(y, logits)
    grads = Tape.gradient(loss_value, model.trainable_weights)
    优化器.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    回波损耗值

@tf.函数
def test_step(x, y):
    val_logits = 模型(x, 训练=False)
    val_acc_metric.update_state(y, val_logits)

这是训练部分。
导入时间

历元 = 2
对于范围内的纪元（纪元）：
    print(&quot;\n纪元%d开始&quot; % (纪元,))
    开始时间 = 时间.time()

    # 迭代数据集的批次。
    对于枚举（train_dataset）中的步骤（x_batch_train，y_batch_train）：
        loss_value = train_step(x_batch_train, y_batch_train)

        # 每 200 个批次记录一次。
        如果步骤 % 200 == 0:
            打印（
                “第 %d 步的训练损失（一批）：%.4f”
                %（步长，浮点数（损失值））
            ）
            print(“到目前为止已看到：%d 个样本” % ((step + 1) * batch_size))

    # 显示每个时期结束时的指标。
    train_acc = train_acc_metric.result()
    print(&quot;历元训练 acc: %.4f&quot; % (float(train_acc),))

    # 在每个时期结束时重置训练指标
    train_acc_metric.reset_states()

    # 在每个时期结束时运行验证循环。
    对于 val_dataset 中的 x_batch_val、y_batch_val：
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(&quot;验证acc: %.4f&quot; % (float(val_acc),))
    print(&quot;所用时间: %.2fs&quot; % (time.time() - start_time))


我的问题是，如果数据集分为 3 个类别：训练、测试、有效，那么每个类别的代码会有什么不同，尤其是测试和有效。
例如，
&lt;前&gt;&lt;代码&gt;x_val = x_train[-20000:]
y_val = y_train[-20000:]

x_test = x_train[-20000:-10000]
y_test = y_train[-20000:-10000]

x_train = x_train[:-10000]
y_train = y_train[:-10000]


valid 也应该使用 tf.gradienttape 吗？或训练=真？这让我很困惑。
感谢您的回复。
谢谢，
分钟]]></description>
      <guid>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</guid>
      <pubDate>Sat, 23 Dec 2023 16:01:22 GMT</pubDate>
    </item>
    <item>
      <title>我正在研究一个示例：简单线性回归薪资数据解释了为什么为 x 列创建新轴的步骤以及什么是 np.newaxis [关闭]</title>
      <link>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</link>
      <description><![CDATA[# 分割训练数据和测试数据

X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)

# 为 x 列创建新轴

X_train = X_train[:,np.newaxis]
X_test = X_test[:,np.newaxis]

使用 np.newaxis 添加新轴有助于重塑数据以实现兼容性]]></description>
      <guid>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</guid>
      <pubDate>Sat, 23 Dec 2023 15:05:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Label Studio 没有找到云源 - 本地存储（不存在）</title>
      <link>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</link>
      <description><![CDATA[我已经从 git 安装了 Label-Studio git 安装，
并且想要将存储更改为云存储 - 本地存储，路径为 C:\data\media\dataset 数据集图像路径
所以我从这一步开始 https://labelstud.io/guide/storage#Local -storage 和我的命令是：
 设置 LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true

 设置 LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=C:\\data\\media\\dataset

我添加了set，因为如果没有它，它会出错。
当我更改本地存储时，它不存在。它警告使用“必须以 LOCAL_FILES_DOCUMENT_ROOT 开头”
[ErrorDetail(string=&#39;路径 C:\\\\data\\\\media 必须以 LOCAL_FILES_DOCUMENT_ROOT=D:\\ 开头，并且必须是子项，例如：D:\\abc&#39;, code =&#39;无效&#39;）]

[ErrorDetail(string=&#39;路径 LOCAL_FILES_DOCUMENT_ROOT=C:\\\\data\\\\media 不存在&#39;, code=&#39;无效&#39;)]

屏幕截图
屏幕截图]]></description>
      <guid>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</guid>
      <pubDate>Sat, 23 Dec 2023 13:55:59 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制交叉验证的 AUROC 并找到最佳阈值？</title>
      <link>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</link>
      <description><![CDATA[在通过交叉验证评估我的机器学习模型时，我遇到了一个问题。我知道如何在交叉验证中绘制 AUROC 和每个折叠的相应阈值，但我不确定是否绘制所有折叠的平均 AUROC 及其相应阈值。
为此，我在Stack Overflow上探索了相关问题，并找到了相应的解决方案。您可以通过以下链接找到原始问题：[https://stackoverflow.com/questions/57708023/plotting-the-roc-curve-of-k-fold-cross-validation%5C]。尽管我成功生成了平均 ROC，但在准确绘制相应阈值方面遇到了挑战。为了解决这个问题，我根据自己的理解合并了额外的代码，但我不确定这种方法的正确性。
此外，我观察到使用 np.mean() 计算的平均 AUC 与使用 sklearn.metrics 计算的 AUC 值之间存在差异。因此，我正在寻求关于哪个值更准确以获得精确的 AUC 结果的指导。下面是我调整后的修改代码。
X，y = make_classification（n_samples = 1000，n_features = 20，n_classes = 2，random_state = 42）

cv = 分层KFold(n_splits=10)
分类器= SVC（内核=&#39;sigmoid&#39;，概率= True，random_state = 0）

tprs = []
曲线面积=[]
最佳阈值 = []
Mean_fpr = np.linspace(0, 1, 100)
plt.figure(figsize=(10,10))
我=0
对于火车，在 cv.split(X, y) 中测试：
    probas_ = classifier.fit(X[训练], y[训练]).predict_proba(X[测试])
    # 计算 ROC 曲线并计算曲线面积
    fpr, tpr, 阈值 = roc_curve(y[测试], probas_[:, 1])

    # 我添加的代码：
    最优阈值索引 = np.argmax(tpr-fpr)
    最优阈值 = 阈值[最优阈值索引]
    最佳阈值.append(最佳阈值)
    #

    tprs.append(np.interp(mean_fpr, fpr, tpr))
    tprs[-1][0] = 0.0
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    plt.plot(fpr, tpr, lw=1, alpha=0.3,
             label=&#39;ROC 折叠 %d (AUC = %0.4f)&#39; % (i, roc_auc))

    我 += 1



plt.plot([0, 1], [0, 1], 线型=&#39;--&#39;, lw=2, 颜色=&#39;r&#39;,
         标签=&#39;机会&#39;，alpha=.8)

mean_tpr = np.mean(tprs, 轴=0)
平均值_tpr[-1] = 1.0


mean_auc = auc(mean_fpr,mean_tpr)

# 我添加的代码：
np_mean_AUC = np.mean(aucs)
# print(f&quot;np_mean_AUC={np_mean_AUC},mean_auc={mean_auc}&quot;)
#

std_auc = np.std(aucs)

plt.plot(mean_fpr,mean_tpr,颜色=&#39;b&#39;,
         标签=r&#39;平均ROC (AUC = %0.4f $\pm$ %0.4f)&#39; % (np_mean_AUC, std_auc),
         lw=2，阿尔法=.8)

# 我添加的代码：
mean_optimal_threshold_index = np.argmax(mean_tpr-mean_fpr)
plt.annotate(f&#39;平均最佳阈值({np.mean(optimal_thresholds):.2f})&#39;,
                xy=(mean_fpr[平均最佳阈值索引],mean_tpr[平均最佳阈值索引]),
                xy 文本=(5, -5),
                textcoords=&#39;偏移点&#39;,
                arrowprops = dict（facecolor =&#39;红色&#39;，arrowstyle =&#39;楔形，tail_width = 0.7&#39;，shrinkA = 0，shrinkB = 10），
                颜色=&#39;红色&#39;）
#

std_tpr = np.std(tprs, 轴=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_ Between(mean_fpr, tprs_lower, tprs_upper, color=&#39;grey&#39;, alpha=.2,
                 标签=r&#39;$\pm$ 1 标准。开发。”）

plt.xlim([-0.01, 1.01])
plt.ylim([-0.01, 1.01])
plt.xlabel(&#39;误报率&#39;,fontsize=18)
plt.ylabel(&#39;真阳性率&#39;,fontsize=18)
plt.title(&#39;SVM的交叉验证ROC&#39;,fontsize=18)
plt.legend(loc=“右下”, prop={&#39;size&#39;: 15})
plt.show()

以下是输出：
在此处输入图像描述
请告诉我我在代码中所做的更改是否可以准确绘制用于交叉验证的 ROC 曲线以及相应的阈值，以及标记的 AUC 值是否有意义。]]></description>
      <guid>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</guid>
      <pubDate>Fri, 22 Dec 2023 07:42:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 R(spacy) 中的实体识别来匹配实体和个人</title>
      <link>https://stackoverflow.com/questions/77692435/matching-entities-individuals-using-entity-recognition-in-rspacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77692435/matching-entities-individuals-using-entity-recognition-in-rspacy</guid>
      <pubDate>Wed, 20 Dec 2023 14:47:19 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: JoypadSpace.reset() 有一个意外的关键字参数“seed”，当我运行以下代码时，我应该怎么解决这个问题？</title>
      <link>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</link>
      <description><![CDATA[当我运行此代码时：
from nes_py.wrappers import JoypadSpace
进口健身房
导入gym_super_mario_bros
从gym_super_mario_bros.actions导入SIMPLE_MOVMENT
从gym.wrappers导入GrayScaleObservation
从 stable_baselines3.common.vec_env 导入 VecFrameStack,DummyVecEnv
从 matplotlib 导入 pyplot 作为 plt

env =gym_super_mario_bros.make(&#39;SuperMarioBros-v0&#39;,apply_api_compatibility=True,render_mode=“人类”)
env = JoypadSpace(env, SIMPLE_MOVMENT)
env = GrayScaleObservation(env,keep_dim=True)
env = DummyVecEnv([lambda:env])
env = VecFrameStack(env,4,channels_order=&#39;最后&#39;)
状态 = env.reset()

我收到以下错误：

我应该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</guid>
      <pubDate>Mon, 19 Jun 2023 19:47:30 GMT</pubDate>
    </item>
    </channel>
</rss>