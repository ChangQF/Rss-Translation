<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 26 Dec 2024 01:15:42 GMT</lastBuildDate>
    <item>
      <title>将多个模型指标运行记录到 MLFlow 中的同一个图中</title>
      <link>https://stackoverflow.com/questions/79308237/logging-multiple-model-metrics-runs-to-the-same-plot-in-mlflow</link>
      <description><![CDATA[我正在对模型参数进行网格搜索优化，并使用将损失记录到 MLFlow
Mlflow.log_metric(f“{run_number}_Loss”, returns, iteration)

但对于每次新运行，我在 MLFlow UI 中都会得到不同的图。
有没有办法多次记录到同一个图，也许是不同的颜色，并添加图例以便能够轻松比较不同的运行？]]></description>
      <guid>https://stackoverflow.com/questions/79308237/logging-multiple-model-metrics-runs-to-the-same-plot-in-mlflow</guid>
      <pubDate>Wed, 25 Dec 2024 19:50:23 GMT</pubDate>
    </item>
    <item>
      <title>在 MNIST 数据集上使用 Gumbel softmax 进行 VAE</title>
      <link>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</link>
      <description><![CDATA[我很难看出 kl 损失趋近于 0 的问题，重建损失很小，但每幅图像都一样，不代表任何数字。
这是我使用的编码器/解码器架构，我认为该架构足够复杂，可以有效地捕获和创建新的数字。
import tensorflow as tf
from keras import layer
import gumbel_softmax

class VariationalAutoEncoder(tf.keras.Model):
def __init__(self, latent_dim, categorical_dim):
super(VariationalAutoEncoder, self).__init__()
self.latent_dim = latent_dim
self.categorical_dim = categorical_dim
self.z_dim = latent_dim * categorical_dim

self.encoder = tf.keras.Sequential([
图层。输入层（输入形状=（28，28，1）），
图层。Conv2D（16，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
图层。Conv2D（32，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
图层。Conv2D（64，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
图层。BatchNormalization（），
图层。Flatten（），
图层。Dense（128，激活=&#39;relu&#39;），
图层。Dense（self.z_dim），
图层。Reshape（（latent_dim，categorical_dim）），
]）

self.decoder = tf.keras.Sequential（[
图层。输入层（输入形状=（latent_dim，categorical_dim）），
图层。Flatten（），
图层。Dense（7 * 7 * 64，activation=&#39;relu&#39;），
图层。Reshape（（7，7，64）），
图层。Conv2DTranspose（64，（3，3），activation=&#39;relu&#39;，strides= 2，padding=&#39;same&#39;），
图层。Conv2DTranspose（32，（3，3），activation=&#39;relu&#39;，strides = 2，padding=&#39;same&#39;），
图层。Conv2DTranspose（1，（3，3），activation=&#39;sigmoid&#39;，padding=&#39;same&#39;），
]）

def call（self，x，temperature，hard）：
logits = self.encoder（x）
z = gumbel_softmax.gumbel_softmax（logits，temperature，hard=hard）
重建= self.decoder(z)
return tf.reshape(reconstructed,(reconstructed.shape[0],28,28)), tf.nn.softmax(logits, axis=-1)


import tensorflow as tf

def sample_gumbel(shape, eps=1e-15): 
&quot;&quot;&quot;从 Gumbel(0, 1) 分布中采样。&quot;&quot;&quot;
U = tf.random.uniform(shape, minval=0, maxval=1)
return -tf.math.log(-tf.math.log(U + eps) + eps)

def gumbel_softmax_sample(logits,temperature): 
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样。&quot;&quot;&quot;
y = logits + sample_gumbel(tf.shape(logits))
return tf.nn.softmax(y /temperature)

def gumbel_softmax(logits,temperature,hard=False):
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样并可选择离散化。

参数：
logits：[batch_size, n_class] 未归一化的对数概率
temperature：非负标量
hard：如果为 True，则取 argmax，但对软样本 y 进行区分

返回：
[batch_size, n_class] 来自 Gumbel-Softmax 分布的样本。
如果 hard=True，则返回的样本将是独热样本，否则它将是跨类别总和为 1 的概率分布。
&quot;&quot;&quot;
y = gumbel_softmax_sample(logits,temperature) 
if hard: 
y_hard = tf.one_hot(tf.argmax(y, axis=-1), tf.shape(logits)[-1])
y = tf.stop_gradient(y_hard - y) + y 

return y


这是我使用的参数，我尝试了很多次，但没有成功。
LR_RATE = 5e-3

BATCH_SIZE=64
NUM_ITERS=900
tau0 = 1
ANNEAL_RATE=5e-5
MIN_TEMP=0.1
EPOCHS = 30

CATEGORICAL_DIM = 10
LATENT_DIM = 32

我计算了 kl 损失和重建损失的总和batch_size
def compute_loss(model, x,temperature,hard,beta):
reconstructed, logits = model(x,temperature =temperature,hard = hard)

mse = keras.losses.BinaryCrossentropy(reduction=&quot;sum_over_batch_size&quot;)

rebuilding_loss = mse(x,reconstructed)
kl_loss = tf.reduce_mean(
tf.reduce_sum(
logits * (tf.math.log(logits + 1e-8) - tf.math.log(1.0 / config.CATEGORICAL_DIM)),
axis=[1, 2]
)
)
total_loss = rebuilding_loss + beta * kl_loss
return total_loss, rebuilding_loss, beta * kl_loss, reconstructed


任何帮助都欢迎非常感谢，因为我在这个问题上已经呆了很长时间了。]]></description>
      <guid>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</guid>
      <pubDate>Wed, 25 Dec 2024 16:28:11 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些“领域特定”功能来预测糖尿病？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</link>
      <description><![CDATA[我需要在 MLOps 周期中为经典糖尿病 ML 数据集添加“领域特定”特征和特征工程的 Python 代码实现？
我正在向高中生教授机器学习。因此，我们使用经典 ML 糖尿病数据集。我需要使用 pandas、numpy 和 scikit-learn 使其保持简单。
我目前所做的工作：
我的特征工程实践想法是对性别进行分类、根据出生日期计算年龄（我已将日期添加到原始数据集）并根据 年龄 * BMI 计算风险百分比，但这些想法更侧重于从现有特征和特征交互中得出新变量。我真的需要一个想法/代码来实际演示“创建特定于域的功能”：
我拥有的代码需要基于特定于域的示例进行构建：
import pandas as pd
#数据以 CSV 格式导入，学生对此进行了一些基本的处理
data_frame = pd.read_csv(&quot;2.2.1.wrangled_data.csv&quot;)
data_frame[&#39;SEX&#39;] = data_frame[&#39;SEX&#39;].apply(lambda gender: -1 if gender.lower() == &#39;male&#39; else 1 if gender.lower() == &#39;female&#39; else None)
data_frame[&#39;Age&#39;] = ((data_frame[&#39;DoTest&#39;] - data_frame[&#39;DoB&#39;]).dt.days / 365.25).round().astype(int)
data_frame[&#39;Risk&#39;] = data_frame[&#39;BMI&#39;] * data_frame[&#39;Age&#39;]
data_frame[&#39;RiskPercentage&#39;] = ((data_frame[&#39;Risk&#39;] / data_frame[&#39;Risk&#39;].max()) * 100).round(2)

任何带有特定领域示例的帮助想法都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</guid>
      <pubDate>Wed, 25 Dec 2024 10:36:46 GMT</pubDate>
    </item>
    <item>
      <title>分离图像内的盲文字符</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[我正在做一个将盲文转换为文本的项目。我已经编写了从图像中识别盲文点的代码，但我不知道如何将盲文分割成单元格。
这部分是识别图像中的斑点（较小的低质量图像目前不起作用）
import cv2
import numpy as np
from sklearn.cluster import KMeans

# 加载图像
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 设置 SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# 按区域过滤（斑点大小）
params.filterByArea = True
params.minArea = 100 # 根据点大小进行调整
params.maxArea = 1000

# 按圆度过滤
params.filterByCircularity = True
params.minCircularity = 0.9 # 调整点的形状

# 按凸度过滤
params.filterByConvexity = False
params.minConvexity = 0.7

# 按惯性过滤（圆度）
params.filterByInertia = True
params.minInertiaRatio = 0.95

# 使用参数创建检测器
detector = cv2.SimpleBlobDetector_create(params)

# 检测斑点
keypoints = detector.detect(image)

# 将检测到的斑点绘制为红色圆圈
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;输出图像&quot;)
cv2.imshow(&quot;输出图像&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;检测到的斑点数量：{len(keypoints)}&quot;)

以下代码将 blob 的坐标放在图形上（认为这种方式可能更容易操作）
#将图像转换为图形

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob 的坐标
rounded_coords = np.round(blob_coords).astype(int) #四舍五入的坐标

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# 基于邻近度的分组
# 如果 X 距离小于最小距离
# 如果 Y 距离小于最小距离
# 存储 X 和 Y 坐标

# 计算最小 x 和 y差异（尝试基于接近度）
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # 单元格宽度阈值
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # 单元格高度阈值
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# 绘图
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

尝试通过接近度将它们分开（位于我尝试将距离很近的物体分组到一起（我将距离很近的物体分组到一起），但我无法理解其中的逻辑。我也尝试了组聚类 (Kmeans)，但它不是很准确，并且不适用于具有不同字符数的图像，因为它需要不断知道要形成多少个簇。
# 尝试 kmeans 聚类方法
# kmeans 不起作用（无法从图像中找出簇的数量）
# 如果可以找出 nclusters，则可以工作

导入数学
从 sklearn.cluster 导入 KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # 提取 blob 的 (x, y) 位置
rounded_coords = np.round(blob_coords).astype(int) # 为简单起见，对坐标进行四舍五入

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点

ax.invert_yaxis() # 反转 Y 轴以获得类似图像的坐标
plt.title(&quot;盲文单元检测&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 将无界输入导出到 mlpackage/mlmodel 文件</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[我想创建一个 .mlpackage 或 .mlmodel 文件，可以将其导入 Xcode 进行图像分割。为此，我想使用 YOLO 中的分割包来检查它是否符合我的需求。
现在的问题是，此脚本创建的 .mlpackage 文件仅接受固定大小（640x640）的图像：
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

我想在这里进行一些更改，可能使用 coremltools，以处理无界范围（我想处理任意大小的图像）。这里有一些描述：https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges，但我不明白如何用我的脚本实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>比较图像并返回相似度百分比（针对徽标）[关闭]</title>
      <link>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</link>
      <description><![CDATA[假设我有 2 张图片


这有相同的徽标，因此结果应该超过 90%
这里是另外 2 个徽标在此处输入图片说明
现在我们又有 2 张照片了


这也是相同的图片，所以结果一定是正面的。
我遇到的问题是，在交换图片并比较“奥迪”和“奥运会”的标志时，尽管图像完全不同，相似度得分却超过 75%。我尝试过边缘检测等方法来解决这种差异，但这些方法都被证明是无效的。您能建议一种合适的方法来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</guid>
      <pubDate>Tue, 24 Dec 2024 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>同一样本的预测在训练和测试中有所不同</title>
      <link>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</guid>
      <pubDate>Mon, 23 Dec 2024 16:37:41 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 值在二元分类中被反转或解释不佳 [关闭]</title>
      <link>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</link>
      <description><![CDATA[我正在做一个项目研究，预测自行车骑行者和比赛数据集的 top_20 标签，当自行车骑行者进入前 20 名时为 1，否则为 0。数据集严重不平衡（92000 个 1 类实例和大约 400000 个 0 类实例）。我运行一个具有简单架构的基本神经网络。我也在使用类权重。我的 NN 表现不佳，但这不是重点，因为我从项目目的上知道数据不是很好。f1 分数对于 0 类（多数）来说很好，约为 0.87。对于 1 类（少数），约为 0.55。这很好，但是当尝试使用 SHAP 解释结果时会出现问题。
使用 SHAP，我为所有实例设置了基值 1（有时为 0.98）。摘要图似乎合理，但力和瀑布图不合理。如果模型过于自信地预测类 0，基值如何导致 1？ 我还认为标签在 shap 值中是反转的，实际上预期值 1 反映了类 0。但我对我的 shap 值对象的结构感到困惑，因为我无法选择一个类来分析，但它被实例划分。例如 shap_value[1] 指的是实例编号 1，而不是类 1。等等。代码如下：
def build_model(optimizer=&#39;adam&#39;, dropout_rate=0.5, num_units_1=128, num_units_2=64):
model = Sequential([
Dense(num_units_1,activation=&#39;relu&#39;,input_shape=(X_train.shape[1],)), # 输入层
BatchNormalization(),
Dropout(dropout_rate), # Dropout 层
Dense(num_units_2,activation=&#39;relu&#39;,kernel_regularizer=l2(0.01)), # 隐藏层
#Dropout(dropout_rate), # Dropout 层
Dense(1,activation=&#39;sigmoid&#39;) # 用于二分类的输出层
])

# 用于收敛
lr_schedule = ExponentialDecay(
initial_learning_rate=0.001,
decay_steps=10000,
decay_rate=0.9
)

# 编译模型
optimizer_instance = {
&#39;adam&#39;: Adam(learning_rate=lr_schedule),
&#39;rmsprop&#39;: RMSprop(learning_rate=lr_schedule),
&#39;sgd&#39;: SGD(learning_rate=lr_schedule)
}[optimizer]

model.compile(optimizer=optimizer_instance,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
return model

对于 SHAP：
background = X_train_df.sample(200) 
test_sample = X_test_df.sample(100) 

# 解释器
explainer = shap.Explainer(best_model.predict,背景)
shap_values = explainer(test_sample)

# 可视化每个预测的 SHAP 值
shap.summary_plot(shap_values, test_sample)

该图突出显示了在我看来合理的内容：较低的 delta 值可以反映出骑车人接近第一个位置的事实（delta 是相对于第一个位置的时间差），因此它推到 1
但例如瀑布/力图对我来说似乎完全不一致：
最后，我的 shap 值的结构如下：
shap_values[2]
.values =
array([-5.55111512e-17, 1.00000000e-02, 1.73472348e-17, 3.12250226e-17,
2.42861287e-17, -3.46944695e-18, 1.04083409e-17, 0.00000000e+00])

.base_values =
0.99

.data =
array([1.20000000e+02, 1.57000000e+05, 1.55100000e+03, 3.28000000e+02,
1.71790754e-02, 3.83373426e-06, 2.40317094e+01, 1.54471545e-01])
]]></description>
      <guid>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</guid>
      <pubDate>Mon, 23 Dec 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在第 0 步记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？类似于 eval_on_start 但 train_on_start？
交叉：

discuss.huggingface
github/huggingface
]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>节点特征对节点分类的 GNN 的影响</title>
      <link>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</link>
      <description><![CDATA[我正在探索各种节点特征对图神经网络 (GNN) 节点分类任务性能的影响。我遇到过 PageRank、HITS 和 基于社区的属性 等特征，它们似乎通过提供额外的上下文信息来提高分类准确性。
我很想听听您对以下问题的看法：

您如何将 PageRank 或 HITS 等特征集成到您的 GNN 模型中，您观察到它们对节点分类性能有何影响？
您是否推荐使用特定方法或框架来有效地将基于社区的特征整合到 GNN 中？
您是否遇到过任何提供关于此主题的见解或案例研究的研究论文或资源？
]]></description>
      <guid>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</guid>
      <pubDate>Wed, 16 Oct 2024 14:40:25 GMT</pubDate>
    </item>
    <item>
      <title>无法将标准 torchvision ResNet50 模型导出到 ONNX 文件</title>
      <link>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</link>
      <description><![CDATA[我制作了一个非常简单的 Python 脚本，它加载 torchvision ResNet50 模型并尝试以两种方式导出到 onnx 文件（torch.onnx.export 和 torch.onnx.dynamo_export）
import torch
import torch.onnx

import torchvision

torch_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights=&#39;DEFAULT&#39;)
torch_model.eval()
torch_input = torch.randn(1, 3, 32, 32)

is_dynamo_export = False

if (is_dynamo_export):
onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
onnx_program.save(&quot;onnx_dynamo_export_ResNET50.onnx&quot;) 
else:
torch.onnx.export(torch_model, # 正在运行的模型
torch_input, # 模型输入（或多个输入的元组）
&quot;onnx_export_ResNET50.onnx&quot;, # 模型的保存位置（可以是文件或类似文件的对象）
export_params=True, # 将训练后的参数权重存储在模型文件中
opset_version=10, # 将模型导出到的 ONNX 版本
do_constant_folding=True, # 是否执行常量折叠以进行优化
input_names = [&#39;input&#39;], # 模型的输入名称
output_names = [&#39;output&#39;], # 模型的输出名称
dynamic_axes={&#39;input&#39; : {0 : &#39;batch_size&#39;}, # 可变长度轴
&#39;output&#39; : {0 : &#39;batch_size&#39;}}) 

出现错误：
文件“C:\tools\Python311\Lib\site-packages\torch\onnx\_internal\exporter.py”，第 1439 行，位于 dynamo_export
raise OnnxExporterError(

torch.onnx.OnnxExporterError：无法将模型导出到 ONNX。在“report_dynamo_export.sarif”处生成 SARIF 报告。SARIF 是静态分析工具输出的标准格式。SARIF 日志可以在 VS Code SARIF 查看器扩展或 SARIF Web 查看器（https://microsoft.github.io/sarif-web-component/）中加载。请在 PyTorch Github 上报告错误：https://github.com/pytorch/pytorch/issues

torch.onnx.errors.SymbolicValueError：不支持：ONNX 导出 opset 9 中的 Pad。填充的大小必须是恒定的。请尝试 opset 版本 11。[由 (%535 : int[] = prim::ListConstruct(%405, %534, %405, %533, %405, %532) 中定义的值 &#39;535 引起，范围：torchvision.models.detection.faster_rcnn.FasterRCNN::

这两种方法都适用于极其简单的模型，例如
class MyModel(nn.Module):

def __init__(self):
super(MyModel, self).__init__()
self.conv1 = nn.Conv2d(1, 6, 5)
self.conv2 = nn.Conv2d(6, 16, 5)
self.fc1 = nn.Linear(16 * 5 * 5, 120)
self.fc2 = nn.Linear(120, 84)
self.fc3 = nn.Linear(84, 10)

def forward(self, x):
x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), 2)
x = torch.flatten(x, 1)
x = F.relu(self.fc1(x))
x = F.relu(self.fc2(x))
x = self.fc3(x)
return x
]]></description>
      <guid>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</guid>
      <pubDate>Fri, 09 Aug 2024 15:25:36 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络进行分类</title>
      <link>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</link>
      <description><![CDATA[我正在使用 GNN 开展欺诈检测项目。我的图表以银行代码（SWIFT BIC 代码）作为节点，边表示交易。
以下是我的张量的形状：

节点特征张量形状：torch.Size([210, 6])
边缘特征张量形状：torch.Size([200, 4])
邻接矩阵张量形状：torch.Size([210, 210])
标签张量形状：torch.Size([200, 1])

我尝试了很多次，但目前正在遵循本教程：https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html
以下是我的 GNN 代码：
class GCNLayer(nn.Module):

def __init__(self, c_in, c_out):
super().__init__()
self.projection = nn.Linear(c_in, c_out)

def forward(self, node_feats, adj_matrix):
# Num neighbours = 传入边的数量
num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
node_feats = self.projection(node_feats)
print(&quot;node_feats &quot;,node_feats)
node_feats = torch.bmm(adj_matrix, node_feats)
node_feats = node_feats / num_neighbours
返回node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

使用 torch.no_grad():
out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;邻接矩阵&quot;, adjacency_matrix_tensor)
print(&quot;输入特征&quot;, node_features_tensor)
print(&quot;输出特征&quot;, out_feats)

但无论我怎么尝试，乘法过程中总是出现维度错误：“RuntimeError：mat1 和 mat2 形状无法相乘（210x6 和 2x2）”。
我知道我们正在尝试将 node_Features_tensor (210,6) 乘以 adjacency_matrix_tensor (210,210)，但我已经为此困扰了好几天！
我尝试了 GNN/GCN 的多种实现。我希望能够训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</guid>
      <pubDate>Tue, 12 Mar 2024 09:08:05 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在仅具有边缘特征的图上使用 GNN 吗？</title>
      <link>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</link>
      <description><![CDATA[我正在尝试使用 GNN 对系统发育数据进行分类（完全二分、单向树）。我将 R 中的系统发育树格式转换为 PyTorch 数据集。以其中一棵树为例：

Data(x=[83, 1], edge_index=[2, 82], edge_attr=[82, 1], y=[1], num_nodes=83)

它有 83 个节点（内部节点 + 提示节点，x=[83, 1]），我为所有节点分配了 0，因此每个节点都有一个特征值 0。我构建了一个 82 X 1 矩阵，其中包含节点之间所有有向边的长度（edge_attr=[82, 1]），我打算使用 edge_attr 表示边长度并将其用作权重。每棵树都有一个用于分类的标签（y=[1]，值在 {0, 1, 2} 中）。
如您所见，节点特征在我的例子中并不重要，唯一重要的是边缘特征（边缘长度）。
以下是我用于建模和训练的代码实现：
tree_dataset = TreeData(root=None, data_list=all_graphs)

class GCN(torch.nn.Module):
def __init__(self, hidden_​​size=32):
super(GCN, self).__init__()
self.conv1 = GCNConv(tree_dataset.num_node_features, hidden_​​size)
self.conv2 = GCNConv(hidden_​​size, hidden_​​size)
self.linear = Linear(hidden_​​size, tree_dataset.num_classes)

def forward(self, x, edge_index, edge_attr, batch):
# 1. 获取节点嵌入
x = self.conv1(x, edge_index, edge_attr)
x = x.relu()
x = self.conv2(x, edge_index, edge_attr)

# 2. 读出层
x = global_mean_pool(x, batch) # [batch_size, hidden_​​channels]

# 3. 应用最终分类器
x = F.dropout(x, p=0.5, training=self.training)
x = self.linear(x)

return x

model = GCN(hidden_​​size=32)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
train_loader = DataLoader(tree_dataset, batch_size=64, shuffle=True)
print(model)

def train():
model.train()

lost_all = 0
for data in train_loader:
optimizer.zero_grad() # 清除梯度。
out = model(data.x, data.edge_index, data.edge_attr, data.batch) # 执行一次前向传递。
loss = criterion(out, data.y) # 计算损失。
loss.backward() # 得出梯度。
lost_all += loss.item() * data.num_graphs
optimizer.step() # 根据梯度更新参数。

return lost_all / len(train_loader.dataset)

def test(loader):
model.eval()

correct = 0
for data in loader: # 在训练/测试数据集上分批迭代。
out = model(data.x, data.edge_index, data.edge_attr, data.batch)
pred = out.argmax(dim=1) # 使用概率最高的类。
correct += int((pred == data.y).sum()) # 对照真实标签进行检查。
return correct / len(loader.dataset) # 得出正确预测的比例。

for epoch in range(1, 20):
loss = train()
train_acc = test(train_loader)
# test_acc = test(test_loader)
print(f&#39;Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Loss: {loss:.4f}&#39;)

看来我的代码根本不起作用：
......
Epoch: 015, Train Acc: 0.3333, Loss: 1.0988
Epoch: 016, Train Acc: 0.3333, Loss: 1.0979
Epoch: 017, Train Acc: 0.3333, Loss: 1.0938
Epoch: 018, Train Acc: 0.3333, Loss: 1.1044
Epoch: 019，训练精度：0.3333，损失：1.1012
......
Epoch：199，训练精度：0.3333，损失：1.0965

是不是因为没有有意义的节点特征就不能使用GNN？还是我的实现有问题？]]></description>
      <guid>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</guid>
      <pubDate>Mon, 09 Oct 2023 12:36:47 GMT</pubDate>
    </item>
    <item>
      <title>模型的敏感性和特异性</title>
      <link>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</link>
      <description><![CDATA[如果我有一个包含两个类别的图像数据集：正常和异常，除了准确度指标之外，我还想添加敏感度和特异性标准。
那么，我该如何引入这两个指标来计算我的模型的性能呢？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</guid>
      <pubDate>Wed, 23 Dec 2020 08:15:24 GMT</pubDate>
    </item>
    </channel>
</rss>