<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 12 Feb 2024 12:23:42 GMT</lastBuildDate>
    <item>
      <title>torch.Sequential 中尺寸不兼容</title>
      <link>https://stackoverflow.com/questions/77981094/incompatibility-of-dimensions-in-torch-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77981094/incompatibility-of-dimensions-in-torch-sequential</guid>
      <pubDate>Mon, 12 Feb 2024 11:24:32 GMT</pubDate>
    </item>
    <item>
      <title>何时应用假设检验以及选择哪一个假设检验，以及是否对所有类型的数据集进行假设检验</title>
      <link>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</link>
      <description><![CDATA[我一直在研究 Kaggle 的太空泰坦尼克号数据集，并且我看过一个关于类似类型问题的代码笔记本，所以我尽管使用笔记本中的相同步骤，但他已经完成了所有步骤，例如加载数据、变量描述和单变量等。我理解了大部分步骤，甚至将它们应用于太空泰坦尼克号，但我不理解在双变量部分进行的假设检验，我很困惑他是如何应用这些类型的测试以及如何进行的我可以学习吗？
那么我可以跳过双变量分析的这一部分吗？
有人可以给我指出一些资源或者我可以在哪里理解假设检验吗？它是否适用于所有类型的数据集，例如 nlp、cv 等？]]></description>
      <guid>https://stackoverflow.com/questions/77979725/when-to-apply-hypothesis-testing-and-which-one-to-choose-and-is-hypothesis-tes</guid>
      <pubDate>Mon, 12 Feb 2024 06:53:48 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 训练在 CPU 和 GPU 上都很慢</title>
      <link>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</link>
      <description><![CDATA[我正在尝试使用 Tensorflow Keras 在我的机器上训练 CNN 模型。
这是我的机器规格：

CPU：Ryzen 9 3900x（12 核 24 线程）
GPU：GTX 970 4GB

模型 = models.Sequential([
    层.InputLayer(input_shape=input_shape),
    层.Conv2D（32，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D（64，kernel_size =（3,3），激活=&#39;relu&#39;），
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    层.MaxPooling2D((2, 2)),
    层.Flatten(),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.Dense（n_classes，激活=&#39;softmax&#39;），
]）

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    火车发电机，
    每纪元的步数=153，
    批量大小=32，
    验证数据=验证生成器，
    验证步骤=23，
    详细=1，
    纪元=20，
）

我使用的是仅具有 CPU 访问权限的 Windows 11。使用 model.fit() 进行训练大约需要 1 小时（仅 CPU 训练）。我在同一台机器上双重启动 Linux Ubuntu，以获得更轻松的 GPU 支持。现在我已经确认了 Linux 环境中的 GPU 支持，但训练仍然需要大约 1 小时（仅 GPU 训练）。
CPU强而GPU弱，速度一样吗？还有其他可能出错的地方吗？
我确保在训练模型时使用 GPU。速度还是挺慢的。]]></description>
      <guid>https://stackoverflow.com/questions/77979720/tensorflow-training-is-slow-on-both-cpu-and-gpu</guid>
      <pubDate>Mon, 12 Feb 2024 06:51:29 GMT</pubDate>
    </item>
    <item>
      <title>分类迁移学习模型是否过度拟合或欠拟合？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77979247/is-classification-transfer-learning-model-overfitting-or-underfitting</link>
      <description><![CDATA[这是一个使用 EfficientNetv2b3 作为基础模型的迁移学习分类项目。训练数据包含 26k 张照片，验证数据包含约 3.3k 张照片。
我还应该补充一点，这个模型工作得“不错”，并且与 inceptionv3 作为基本模型不同，因此我试图找出差异并改进 effectivenetv2b3 模型。
我无法确定这是过度拟合还是欠拟合。 ](https://i.stack.imgur.com/W3tA0.png) 
我增加了 dropout，添加了数据增强，并删除了一些密集层以降低复杂性。我不知道该怎么办。 val 损失和 val 精度从开始到结束保持不变。我还认为我的输入数据可能是错误的类型，但我已确定它是 float32。
添加的层如下：
对于base_model.layers中的图层：
 可训练层 = False
x = 层.Flatten()(base_model.output)
x = 层.BatchNormalization()(x)
x = 层.Dense(32, 激活 = &#39;relu&#39;)(x)
x = 层数.Dropout(0.5)(x)

x = 层.BatchNormalization()(x)
x = 层.Dense(1, 激活 = &#39;sigmoid&#39;)(x)

模型= keras.Model（输入= base_model.输入，输出= x）

&lt;前&gt;&lt;代码&gt;纪元 1/12
835/835 [==============================] - 15313s 18s/步 - 损失：0.2608 - 二进制精度：0.6075 - val_loss ：0.6336 - val_binary_accuracy：0.7945
纪元 2/12
835/835 [================================] - 718s 860ms/步 - 损耗：0.2446 - 二进制精度：0.7119 - val_loss ：0.6400 - val_binary_accuracy：0.7945
纪元 3/12
835/835 [================================] - 759s 909ms/步 - 损失：0.2424 - 二进制精度：0.7755 - val_loss ：0.6318 - val_binary_accuracy：0.7945
纪元 4/12
835/835 [================================] - 761s 912ms/步 - 损失：0.2421 - 二进制精度：0.7880 - val_loss ：0.6370 - val_binary_accuracy：0.7945
纪元 5/12
835/835 [================================] - 828s 991ms/步 - 损耗：0.2421 - 二进制精度：0.7942 - val_loss ：0.6200 - val_binary_accuracy：0.7945
纪元 6/12
835/835 [================================] - 774s 927ms/步 - 损失：0.2420 - 二进制精度：0.7936 - val_loss ：0.6208 - val_binary_accuracy：0.7945
纪元 7/12
835/835 [================================] - 769s 922ms/步 - 损失：0.2419 - 二进制精度：0.7921 - val_loss ：0.6214 - val_binary_accuracy：0.7945

我最初认为我过度拟合，但有人提到我可能不拟合，所以我正在寻找关于该怎么做的更多意见和选项。 val 损失和 val 准确率在整个训练过程中似乎保持不变。]]></description>
      <guid>https://stackoverflow.com/questions/77979247/is-classification-transfer-learning-model-overfitting-or-underfitting</guid>
      <pubDate>Mon, 12 Feb 2024 03:53:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么重新运行 jupyter 笔记本单元会增加内存使用量？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</link>
      <description><![CDATA[我不确定这是 Pytorch 还是 jupyter 笔记本问题。
我正在使用 Pytorch 和 jupyter 笔记本在 CPU 上训练一些神经网络。我可以运行整个 jupyter 笔记本，但是当我重新运行某些单元（例如训练代码）时，我会遇到内存错误或崩溃。
这是为什么呢？我重新运行的单元格没有在任何地方积累对象。他们只是重新加载数据加载器并重新训练模型。
额外的内存从哪里来？]]></description>
      <guid>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</guid>
      <pubDate>Mon, 12 Feb 2024 03:09:53 GMT</pubDate>
    </item>
    <item>
      <title>如何为每个患者创建一个单独的图，显示其“订单金额”和“订单数量”与“订单时间”变量的关系？</title>
      <link>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</link>
      <description><![CDATA[我现在面临的问题是代码返回一个图，该图显示与用户输入日期匹配的最新数据。我想一起研究数据，因此我想要图中的趋势，而不是最新数据的单个水平线。然而，它不显示与相同“患者ID”相对应的过去数据和最新数据的合并数据。我想创建两个图，其中一个图是“订单金额”与“订单时间”的对比，另一个图是“订单数量”与“订单时间”的对比。

数据按以下方式分组：

代码想要解决四个主要部分：

将最新数据与输入日期进行匹配，我已经根据输入日期过滤了数据帧。

将之前数据的相同 ID 与最新数据进行匹配，并将数据串在一起：为了实现这一点，我们需要识别每个患者 ID 的最新数据，然后将其与该患者之前的数据合并。相同的患者 ID。

在共享相同“患者 ID”和“订单时间”的两行中选取“订单金额”和“订单数量”值最高的行：将最近的数据与先前的数据合并后，我们可以比较每个患者 ID 的订单金额和订单数量的值，并选择具有最高值的行。 - 我被困在这个阶段:)


例如，第 27 行和第 28 行显示第 28 行的“订单金额”和“订单数量”分别为 659.8 和 21，以及 671.677 和 13。在这种情况下，我们将选择第 28 行作为“订单数量”值似乎要低得多，而“订单金额”值似乎彼此接近。在不同的场景中，当第 29 行和第 30 行具有相同的“患者 ID”时，我们会选择第 30 行，因为其“订单金额”值要低得多，并且其“订单数量”值与第 30 行的值相差小于“订单”数量&#39;。第 32 行和第 33 行是替代示例。如果“订单金额”和“订单数量”的值相同，我们可以选择第 32 行作为两者之间的第一行。如果只有一个例外，我们将选择单行。第 31 行就是一个示例。


将每个患者 ID 的“订单金额”与“订单时间”和“订单数量”与“订单时间”的点绘制在一起。这意味着每个患者 ID 一个图。

导入 pandas 作为 pd
导入日期时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
导入urllib
将 matplotlib.dates 导入为 mdates

input_date = input(&quot;请输入日期（格式：yyyy/mm/dd）：&quot;)
input_date = datetime.datetime.strptime(input_date, &quot;%Y/%m/%d&quot;)

df[&#39;订单时间&#39;] = pd.to_datetime(df[&#39;订单时间&#39;])

df[&#39;time_diff&#39;] = (df[&#39;订单时间&#39;] - input_date).dt.days

df_filtered = df[(df[&#39;time_diff&#39;] &gt;= -30) &amp; (df[&#39;time_diff&#39;] &lt;= 30)]

centre_data = df_filtered.sort_values(by=&#39;订单时间&#39;).groupby(&#39;患者 ID&#39;).last()

merged_data = pd.merge（df，recent_data，on =&#39;患者ID&#39;，后缀=（&#39;_prev&#39;，&#39;&#39;））

## 到目前为止似乎还不错，尽管将后缀包含为“_prev”很奇怪。

idx_amount = merged_data.groupby(&#39;患者ID&#39;)[&#39;订单金额&#39;].idxmax()
idx_quantity = merged_data.groupby(&#39;患者 ID&#39;)[&#39;订单数量&#39;].idxmax()

idx_highest = idx_amount[idx_amount.isin(idx_quantity)]

Filtered_data = merged_data.loc[idx_highest]

defplot_filtered_data（数据）：

    grouped_data = data.groupby(&#39;患者 ID&#39;)
    
    对于 grouped_data 中的 Patient_id、group_data：
        plt.plot(group_data[&#39;订单时间&#39;], group_data[&#39;订单金额&#39;],marker=&#39;o&#39;, label=f&#39;患者{patent_id}&#39;)
    
    plt.xlabel(&#39;下单时间&#39;)
    plt.ylabel(&#39;订单金额&#39;)
    plt.title(&#39;患者的订单金额与订单时间&#39;)
    plt.图例()
    plt.xticks（旋转=45）
    plt.tight_layout()
    plt.show()

绘图过滤数据（过滤数据）


我知道代码中存在错误，但我的编辑经验很少。有人可以指出我的错误吗？我知道这听起来有点复杂。]]></description>
      <guid>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</guid>
      <pubDate>Mon, 12 Feb 2024 02:57:19 GMT</pubDate>
    </item>
    <item>
      <title>整洁的Python基因组的计算并没有真正相加</title>
      <link>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</link>
      <description><![CDATA[我为 XOR 制作了一个非常简单的整洁的 python。这是基因组的摘要。
最佳基因组：
钥匙：141
健身：2.993003425787766
节点：
    0 DefaultNodeGene（键= 0，偏差= -0.8080802310263379，响应= 1.0，激活= sigmoid，聚合=总和）
连接：
    DefaultConnectionGene(键=(-2, 0)，权重=1.5655941592328844，启用=True)
    DefaultConnectionGene(key=(-1, 0)，权重=1.2986799185483175，启用=True

但是当我尝试计算它时，它并没有真正意义。
以下是评估结果：
 输入 (0.0, 0.0)，预期输出 (0.0,)，得到 [0.017286340618090416]
  输入（0.0，1.0），预期输出（1.0，），得到[0.9778511014280682]
  输入（1.0，0.0），预期输出（1.0，），得到[0.920780444438713]
  输入（1.0，1.0），预期输出（0.0，），得到[0.9999657218870016]

这就是我计算输入 (1, 1) 的方法：
activation_func( sum( 输入 * 权重 ) + 偏差 )

对于这种情况：
sigmoid( 1 * 1.2986799185483175 + 1 * 1.5655941592328844 + (-0.8080802310263379) )
= 0.88657197794273698476
但是根据评估应该是0.9999657218870016
我错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</guid>
      <pubDate>Sun, 11 Feb 2024 21:04:52 GMT</pubDate>
    </item>
    <item>
      <title>针对受阻文本优化 OCR</title>
      <link>https://stackoverflow.com/questions/77978340/optimize-ocr-for-obstructed-text</link>
      <description><![CDATA[我正在尝试实现 OCR 来检测运动员号码，但是我遇到了一些断线问题，有时会阻碍板中的文本...我尝试了多种 OCR，例如 Tesseract、PaddleOCR、EasyOCR 和 TrOCR。到目前为止，当数字完全可见时，EasyOCR 和 TrOCR 对我来说表现最好，但是一旦数字前面有任何障碍物，它们的准确性就会开始下降，这是可以理解的。我想知道是否可以做些什么来改进模型的推理？

正如我所提到的，我尝试了多种 OCR，但如果文本不完全可见，那么它们都不能很好地处理...]]></description>
      <guid>https://stackoverflow.com/questions/77978340/optimize-ocr-for-obstructed-text</guid>
      <pubDate>Sun, 11 Feb 2024 20:36:02 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 得到了意外的关键字参数“units”</title>
      <link>https://stackoverflow.com/questions/77977628/lstm-got-an-unexpected-keyword-argument-units</link>
      <description><![CDATA[这段LSTM代码用于时间序列分析，在模型架构上遇到了错误。
def LSTM(X_train, X_test, y_train, y_test):
  缩放器 = MinMaxScaler()
  X_train_scaled = 缩放器.fit_transform(X_train)
  X_test_scaled = 缩放器.transform(X_test)
  y_train_scaled = 缩放器.fit_transform(y_train)
  y_test_scaled = 缩放器.transform(y_test)

  X_train_reshape = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1],1))
  X_test_reshape = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1],1))

  模型=顺序（）
  model.add(LSTM(单位=50, input_shape=(X_train_reshape.shape[1], X_train_reshape.shape[2])))
  model.add(密集(单位=1))
  model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

  model.fit（X_train_reshape，y_train_scaled，epochs = 50，batch_size = 32）

  损失 = model.evaluate(X_test_reshape, y_test_scaled)
  print(f“测试损失：{loss}”)

  Predictions_scaled = model.predict(X_test_reshape)
  预测=scaler.inverse_transform(predictions_scaled)

  rmse = np.sqrt(np.mean(np.square(预测 - y_test.values)))
  print(f“均方根误差 (RMSE): {rmse}”)


该函数导致以下错误 -
 12 模型 = Sequential()
---&gt; 13 model.add(LSTM(单位=50, input_shape=(X_train_reshape.shape[1], X_train_reshape.shape[2])))
     14 model.add(密集(单位=1))
     15 model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;）

TypeError：LSTM() 得到了意外的关键字参数“units”
]]></description>
      <guid>https://stackoverflow.com/questions/77977628/lstm-got-an-unexpected-keyword-argument-units</guid>
      <pubDate>Sun, 11 Feb 2024 17:05:10 GMT</pubDate>
    </item>
    <item>
      <title>如何对多变量函数使用拉格朗日插值多项式？</title>
      <link>https://stackoverflow.com/questions/77968655/how-to-use-lagrange-interpolation-polynomial-for-functions-of-multiple-variables</link>
      <description><![CDATA[在对单个变量的函数进行插值的情况下，事情相对简单：
def create_basic_polynomial(x_values, i):
    def basic_polynomial(x):
        分频器 = 1
        结果 = 1
        对于范围内的 j(len(x_values))：
            如果 j != i:
                结果 *= (x-x_values[j])
                除法器 *= (x_values[i]-x_values[j])
        返回结果/除法器
    返回基本多项式


def create_lagrange_polynomial(x_values, y_values):
    基本多项式 = []
    对于范围内的 i(len(x_values))：
        basic_polynomials.append(create_basic_polynomial(x_values, i))

    def lagrange_polynomial(x):
        结果 = 0
        对于范围内的 i(len(y_values))：
            结果+= y_values[i]*basic_polynomials[i](x)
        返回结果
    返回拉格朗日多项式


x_值 = [0, 2, 3, 5]
y 值 = [0, 1, 3, 2]

lag_pol = create_lagrange_polynomial(x_values, y_values)

对于 x_values 中的 x：
    print(&quot;x = {:.4f}\t y = {:4f}&quot;.format(x,lag_pol(x)))

x = 0.0000 y = 0.000000
x = 2.0000 y = 1.000000
x = 3.0000 y = 3.000000
x = 5.0000 y = 2.000000

但是我们如何实现处理多变量函数的逻辑呢？]]></description>
      <guid>https://stackoverflow.com/questions/77968655/how-to-use-lagrange-interpolation-polynomial-for-functions-of-multiple-variables</guid>
      <pubDate>Fri, 09 Feb 2024 13:59:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用迁移学习提高 ResNet50 的准确性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77920449/how-i-can-improve-my-accuracy-with-my-resnet50-using-transfer-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77920449/how-i-can-improve-my-accuracy-with-my-resnet50-using-transfer-learning</guid>
      <pubDate>Thu, 01 Feb 2024 13:10:17 GMT</pubDate>
    </item>
    <item>
      <title>如何改进数据集不平衡的情感分析roBERTa模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77844710/how-to-improve-sentiment-analysis-roberta-model-with-imbalanced-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77844710/how-to-improve-sentiment-analysis-roberta-model-with-imbalanced-dataset</guid>
      <pubDate>Fri, 19 Jan 2024 08:39:14 GMT</pubDate>
    </item>
    <item>
      <title>验证准确性和分类报告中的准确性指标之间的显着差异[关闭]</title>
      <link>https://stackoverflow.com/questions/77286670/the-significant-difference-between-the-validation-accuracy-and-the-accuracy-metr</link>
      <description><![CDATA[验证准确度和分类报告中的准确度指标之间存在显着差异，我的不平衡数据中的验证准确度等于 75，但报告中的准确度指标等于 0.22 我该如何处理这个问题
我使用了一些不平衡处理问题]]></description>
      <guid>https://stackoverflow.com/questions/77286670/the-significant-difference-between-the-validation-accuracy-and-the-accuracy-metr</guid>
      <pubDate>Fri, 13 Oct 2023 09:39:41 GMT</pubDate>
    </item>
    <item>
      <title>将迁移学习应用于不同的骨干网[关闭]</title>
      <link>https://stackoverflow.com/questions/77179249/apply-transfer-learning-with-different-backbones</link>
      <description><![CDATA[我有一个使用 R50-FPN、COCO 数据集和 Mask R-CNN 训练的预训练权重。原始数据集不可用。
我正在考虑使用不同的主干网，例如 X101-FPN，并使用新数据集（我必须处理的图像）对预训练的权重应用迁移学习。
这对你来说有意义吗？我怀疑它是否适用于不同的主干结构，权重应该有很大不同，这很可能会使这个想法无效。
顺便说一句，这似乎是我想象的深度学习的一个非常常见的场景。比如说，尝试重用不同主干网的预训练权重，并且没有可用的原始数据集。人们如何处理它？就放弃吗？]]></description>
      <guid>https://stackoverflow.com/questions/77179249/apply-transfer-learning-with-different-backbones</guid>
      <pubDate>Tue, 26 Sep 2023 10:43:12 GMT</pubDate>
    </item>
    <item>
      <title>如何提高图像分类模型的准确性？</title>
      <link>https://stackoverflow.com/questions/50010040/how-can-i-increase-the-accuracy-of-my-image-classification-model</link>
      <description><![CDATA[我试图将一组蜜蜂图像分为两类 - 大黄蜂和蜜蜂，结果格式为 CSV 文件，例如 -
&lt;块引用&gt;
id，bumble_bee，honey_bee
20000,0.75,0.25。

我有一个正在运行的模型，但准确性非常低，我尝试了很多不同的方法，例如添加 VGG16 或 InceptionV3 等基本模型、调整纪元、调整优化器类型......但我只是没有没有注意到有多大区别。我的所有更改仍然导致大约 70-79% 的准确率。
如何提高模型的准确性？
这是我的代码：
from keras.preprocessing.image import ImageDataGenerator
从 keras.models 导入顺序
从 keras.layers 导入 Conv2D、MaxPooling2D
从 keras.layers 导入激活、Dropout、Flatten、Dense
从 keras 导入后端为 K

# 我们图像的尺寸。
图像宽度、图像高度 = 200, 200

train_data_dir = &#39;数据/火车/&#39;
validation_data_dir = &#39;数据/验证/&#39;
nb_train_samples = 2978
nb_validation_samples = 991
纪元 = 50
批量大小 = 25

如果 K.image_data_format() == &#39;channels_first&#39;:
    输入形状=（3，img_宽度，img_高度）
别的：
    输入形状=（img_宽度，img_高度，3）

模型=顺序（）
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(激活(&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(激活(&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))

模型.add(Conv2D(64, (3, 3)))
model.add(激活(&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))

模型.add(压平())
model.add(密集(64))
model.add(激活(&#39;relu&#39;))
模型.add(Dropout(0.5))
model.add(密集(2))
model.add(激活(&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,
              优化器=优化器.SGD(lr=1e-4,动量=0.9),
              指标=[&#39;准确性&#39;])

# 这是我们将用于训练的增强配置
train_datagen = 图像数据生成器(
    重新缩放=1。 /255，
    剪切范围=0.2，
    缩放范围=0.2，
    水平翻转=真）

# 这是我们将用于测试的增强配置：
# 仅重新缩放
test_datagen = ImageDataGenerator（重新缩放=1。/ 255）

train_generator = train_datagen.flow_from_directory(
    训练数据目录，
    目标大小=（img_宽度，img_高度），
    批量大小=批量大小，
    class_mode=&#39;分类&#39;）

validation_generator = test_datagen.flow_from_directory(
    验证数据目录，
    目标大小=（img_宽度，img_高度），
    批量大小=批量大小，
    class_mode=&#39;分类&#39;）

模型.fit_generator(
    火车发电机，
    steps_per_epoch=nb_train_samples //批量大小，
    纪元=纪元，
    验证数据=验证生成器，
    validation_steps=nb_validation_samples //批量大小）

model.save_weights(&#39;thirdtry.h5&#39;)

pred = model.predict(pred_images)
np.savetxt(&#39;resultsfrom3no.csv&#39;,pred)

这是其输出的示例：
&lt;块引用&gt;
找到属于 2 个类别的 2978 个图像。
找到属于 2 个类别的 991 张图像。
纪元 1/50 119/119 [================================] -
238s 2s/步 - 损失：0.5570 - acc：0.7697 - val_loss：0.5275 -
val_acc：0.7908
纪元 2/50 119/119 [================================] -
237s 2s/步 - 损失：0.5337 - acc：0.7894 - val_loss：0.5270 -
val_acc：0.7908
纪元 3/50 119/119 [================================] -
282s 2s/步 - 损失：0.5299 - acc：0.7939 - val_loss：0.5215 -
val_acc：0.7908
]]></description>
      <guid>https://stackoverflow.com/questions/50010040/how-can-i-increase-the-accuracy-of-my-image-classification-model</guid>
      <pubDate>Tue, 24 Apr 2018 20:11:11 GMT</pubDate>
    </item>
    </channel>
</rss>