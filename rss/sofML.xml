<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Sep 2024 15:18:03 GMT</lastBuildDate>
    <item>
      <title>我应该如何为我的黑客马拉松构建 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78966011/how-should-i-build-an-ml-model-for-my-hackathon</link>
      <description><![CDATA[我和我的团队必须构建一个 ML 模型，以使用 OCR 从标签中读取详细信息。
我是 ML 领域的新手，我该如何学习以及在哪个平台上构建我的模型？
Azure AI、AWS 或 Vertex AI 是执行此操作的好平台吗？它们中的哪一个或其他东西可以构建我的模型？
我尝试访问 Vertex AI、Azure AI，所有内容都要求预先提供银行详细信息，我想在继续之前修复平台。如果有人能在这方面提供帮助，那将非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78966011/how-should-i-build-an-ml-model-for-my-hackathon</guid>
      <pubDate>Mon, 09 Sep 2024 14:41:08 GMT</pubDate>
    </item>
    <item>
      <title>为自定义 MLP Keras 模型实现域自适应网络 (DAN)</title>
      <link>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</link>
      <description><![CDATA[我正在尝试使用 Keras 为 MLP 模型实现 DAN（域自适应网络）。
def build_network():
source_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_1&#39;)
target_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_2&#39;)
source_x = layer.Flatten()(source_input)
target_x = layer.Flatten()(target_input)
source_y1 = density_block(num_nodes=128, l_name=&quot;fc1&quot;)(source_x)
source_y2 = 密集块（num_nodes=128，l_name=“fc2”）（source_y1）
source_y3 = 密集块（num_nodes=128，l_name=“fc3”）（source_y2）
source_y4 = 密集块（num_nodes=128，l_name=“fc4”）（source_y3）
y_source = 层。密集（units=config_dict[&#39;output_dim&#39;]，activation=“tanh”，name=“fc5”）（source_y4）
target_y1 = 密集块（num_nodes=128，l_name=“fc6”）（target_x）
target_y2 = 密集块（num_nodes=128，l_name=“fc7”）（target_y1）
target_y3 = density_block(num_nodes=128, l_name=&quot;fc8&quot;)(target_y2)
target_y4 = density_block(num_nodes=128, l_name=&quot;fc9&quot;)(target_y3)
y_target = layer.Dense(units=config_dict[&#39;output_dim&#39;],activation=&quot;tanh&quot;, name=&quot;fc10&quot;)(target_y4)
model = Model([source_input, target_input], y_source)

return model

model = build_network()
model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.huber, metrics[&#39;mean_absolute_error&#39;])

然后我从预先训练的模型中加载一些权重。
print(&#39;-------------加载model-----------------&#39;)
m = Custommetrics()
mlp = models.load_model(os.path.join(config_dict[&#39;checkpoint_dir&#39;],
weights_file_name), custom_objects={&quot;r2_metric&quot;: m.r2_metric, &quot;geodesic_metric&quot;: m.geodesic_metric})
for layer1 in mlp.layers[1:]:
for layer2 in model.layers[1:]:
if layer1.name == layer2.name:
layer2.set_weights(layer1.get_weights())
layer2.trainable = False

最后，我训练模型：
source_x = np.load(os.path.join(source_dir, &#39;x_train.npy&#39;))
target_x = np.load(os.path.join(target_dir, &#39;x_train.npy&#39;))
source_y_train = np.load(os.path.join(target_dir, &#39;y_train.npy&#39;))
history = model.fit([source_x, target_x],
source_y_train,
batch_size=32, epochs=10,
validation_data=([source_x, target_x], source_y_train),
callbacks=[cp_callback])

当我打印 model.summary() 时，我得到以下内容：

模型：“model”
层（类型）输出形状参数 # 连接到
input_1（输入层）[（无，40，40，40，1）] 0 [] 

flatten（Flatten）（无，64000）0 [&#39;input_1[0][0]&#39;] 

fc1（密集）（无，128）8192128 [&#39;flatten[0][0]&#39;] 

fc2（密集）（无，128）16512 [&#39;fc1[0][0]&#39;] 

fc3（密集）（无，128）16512 [&#39;fc2[0][0]&#39;] 

fc4（密集）（无，128）16512 [&#39;fc3[0][0]&#39;]

input_2 (InputLayer) [(None, 40, 40, 40, 1)] 0 [] 

fc5 (Dense) (None, 6) 774 [&#39;fc4[0][0]&#39;] 

总参数：8242438 (31.44 MB)
可训练参数：0 (0.00 字节)
不可训练参数：8242438 (31.44 MB)


这已成功训练。但我有两个问题：
为什么摘要不包含目标分支 (fc6-10)？我应该如何逐层添加 MMD（即 fc1 和 fc6、fc2 和 fc7、fc3 和 fc8、fc4 和 fc9）？
提前感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</guid>
      <pubDate>Mon, 09 Sep 2024 13:49:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 进行自然语言处理</title>
      <link>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</link>
      <description><![CDATA[目前我对自然语言处理感兴趣。我有一个路线图，比如拿一份韩语版的 PDF（任何语言，英语除外），然后在应用一些操作（如数据导入、清理、标记化、删除停用词）后，我想将该语言翻译成英语。

我在这里使用 R 编程，你能建议我路线图需要进行哪些更改以及哪些库很重要吗？谢谢
建议重新评估我的路线图和库。]]></description>
      <guid>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</guid>
      <pubDate>Mon, 09 Sep 2024 13:03:37 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中，如何避免 RGB 图像训练数据的内存泄漏？</title>
      <link>https://stackoverflow.com/questions/78965096/how-to-avoid-memory-leaks-in-training-data-for-rgb-images-in-reinforcement-learn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78965096/how-to-avoid-memory-leaks-in-training-data-for-rgb-images-in-reinforcement-learn</guid>
      <pubDate>Mon, 09 Sep 2024 10:51:59 GMT</pubDate>
    </item>
    <item>
      <title>图像分类中的错误预测</title>
      <link>https://stackoverflow.com/questions/78964898/incorrect-prediction-in-image-classification</link>
      <description><![CDATA[我正在使用 Kaggel 数据集 https://www.kaggle.com/datasets/praveengovi/coronahack-chest-xraydataset/data 来开发图像分类模型，但我没有获得所需的预测准确度，我的模型只学习一个类并只预测一个类，准确度不佳。
这是我的 CNN 类
import torch.nn as nn
import torch.nn. functional as F
import torch

class XrayCNN(nn.Module):
def __init__(self,num_classes=2):
super(XrayCNN,self).__init__()

#convolution layer
self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding=1)
self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)
self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, padding=1)
self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)
self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, padding=1)

#池化层
self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

#连接层
self.fc1 = nn.Linear(512*9*9,1024)
self.fc2 = nn.Linear(1024,512)
self.fc3 = nn.Linear(512,num_classes)

#dropout
self.dropout = nn.Dropout(0.25)

self.bn1 = nn.BatchNorm2d(32)
self.bn2 = nn.BatchNorm2d(64)
self.bn3 = nn.BatchNorm2d(128)
self.bn4 = nn.BatchNorm2d(256)
self.bn5 = nn.BatchNorm2d(512)

def forward(self,x):
x = self.pool(F.relu(self.bn1(self.conv1(x))))
x = self.pool(F.relu(self.bn2(self.conv2(x))))
x = self.pool(F.relu(self.bn3(self.conv3(x))))
x = self.pool(F.relu(self.bn4(self.conv4(x))))
x = self.pool(F.relu(self.bn5(self.conv5(x))))

# print(x.shape)

size = x.size()[1:]
num_features = 1
for s in size:
num_features *= s

x = x.view(-1,num_features)

x = F.relu(self.fc1(x))
x = self.dropout(x)
x = F.relu(self.fc2(x))
x = self.dropout(x)

x = self.fc3(x)

return x

num_classes = 2
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; )
model = XrayCNN(num_classes=num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

我试图预测 Normal 和 Pnemonia 图像，但它将所有图像都预测为 Pnemonia，在我的数据集中，我将 Normal 和 Pnemonoia 图像分别下采样为 400 张图像进行分类，但它不起作用，即使 val 准确率为 87%，它在测试数据上也惨不忍睹
## 数据增强
image_transform =变换。Compose（[
变换。调整大小（（300,300）），
变换。随机水平翻转（p=0.5），
变换。随机垂直翻转（p=0.5），
变换。随机旋转（度=35），
变换。灰度（num_output_channels=1），
变换。ToTensor（），
变换。随机Affine（度=0，平移=（0.2,0.2）），
变换。正则化（平均值=[0.485]，标准差=[0.229]）

])

class CustomXrayDataset（Dataset）：
def __init__（self，image_label_dict，train_dir，transforms=None）：
self.image_label_dict = image_label_dict
self.train_dir = train_dir
self.transforms = image_transform
self.image_list = list(image_label_dict.keys())
#self.label_list = list(image_label_dict.values())

def __len__(self):
return len(self.image_list)

def label_encoding(self, label):
label_dict = {
&#39;Normal&#39;:0,
# &#39;COVID-19&#39;:1,
&#39;Pnemonia&#39;:1

}
return label_dict[label]

def __getitem__(self,idx):
image_name = self.image_list[idx]

image = Image.open(os.path.join(self.train_dir,image_name))
label = self.image_label_dict[image_name]
label = self.label_encoding(label)
if self.transforms:
image = self.transforms(image)
return image,label

想知道哪里出了问题，我开始一个简单的模型，但添加了层以期解决分类问题。
我的 Kaggle Notebook 链接：https://www.kaggle.com/code/siddharthsehgal/notebooka8a9efe8fa
我在 Collab 中运行了它]]></description>
      <guid>https://stackoverflow.com/questions/78964898/incorrect-prediction-in-image-classification</guid>
      <pubDate>Mon, 09 Sep 2024 09:55:19 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的 d-dim 参数向量是否具有相等的平方和？[关闭]</title>
      <link>https://stackoverflow.com/questions/78964442/do-d-dim-vectors-of-parameters-in-large-language-models-have-equal-sums-of-squar</link>
      <description><![CDATA[我正在做大型语言模型中的力学分析研究，想解释注意力模块的参数和FF模块的参数。但我有一个疑问，大型语言模型中的d维参数向量（Wq Wk Wv Wo矩阵的列向量和K V矩阵的行向量）在预训练之后是否仍然具有相等的平方和，并且这些参数的初始化满足假设。我发现了一些类似的研究（训练自然语言生成模型中的表示退化问题、IsoScore：测量嵌入空间利用的均匀性、语境化的词表示有多语境化？比较 BERT、ELMo 和 GPT-2 嵌入的几何形状、通过频谱控制改进神经语言生成），但它们专注于词嵌入的各向同性，并指出嵌入不是各向同性的。
初始化时，这些参数满足假设，条目从均值为零、方差有限的分布 D 中独立同分布采样。该说法的证明很简单。但我不知道如何调查它们在预训练后是否仍然满足，是否有一些与此相关的研究？]]></description>
      <guid>https://stackoverflow.com/questions/78964442/do-d-dim-vectors-of-parameters-in-large-language-models-have-equal-sums-of-squar</guid>
      <pubDate>Mon, 09 Sep 2024 07:57:18 GMT</pubDate>
    </item>
    <item>
      <title>是否可以通过SVM选出距离超平面最远的那些错误分类的样本？</title>
      <link>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-that-are-farthest-fr</link>
      <description><![CDATA[我正在使用 R 中的 e1071 库中的 SVM 模型，我的问题是我是否可以识别模型错误分类且距离 SVM 超平面最远的样本。
我的代码如下：
best_model &lt;- list(
model = svm(x = omicDataReduced[, -which(colnames(omicDataReduced) == classVariable)],
y = omicDataReduced[[classVariable]],
kernel = best_kernel,
cost = best_C,
probability = TRUE,
decision.values = TRUE),
error = best_error,
cost = best_C,
kernel = best_kernel
)

我的数据集有 790 个样本和 18,710 个预测变量。
编辑：
我发现使用“decision.values”属性，您可以获得到超平面的距离。我现在的问题是：我如何检查样本是否超出了边界？
这是我目前拥有的代码：
pred &lt;- predict(best_model$model, omicDataReduced[, -which(colnames(omicDataReduced) == classVariable)], decision.value=T)

decision_values &lt;- attr(pred, &quot;decision.values&quot;)

misclassified_indices &lt;- which(as.numeric(pred) != as.numeric(omicDataReduced[[classVariable]]))

misclassified_decision_values &lt;- abs(decision_values[misclassified_indices])

selected_indices &lt;- misclassified_indices[order(misclassified_decision_values, increasing = TRUE)][1:round(diagnosticChangeProbability * length(misclassified_indices))]
]]></description>
      <guid>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-that-are-farthest-fr</guid>
      <pubDate>Mon, 09 Sep 2024 07:52:54 GMT</pubDate>
    </item>
    <item>
      <title>具有主导变量或仅 1 个变量的 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</link>
      <description><![CDATA[您能否推荐一些学术资源，例如研究论文或书籍章节，用于研究使用具有主导变量的机器学习模型的效果，这些模型可以解释目标中的大部分方差，或者仅使用一个变量构建的模型？具体来说，我有兴趣了解与这些方法相关的潜在缺点、偏见或局限性。]]></description>
      <guid>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</guid>
      <pubDate>Mon, 09 Sep 2024 06:46:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 k-fold 进行逻辑回归并找到最佳阈值</title>
      <link>https://stackoverflow.com/questions/78964032/using-k-fold-for-logistic-regression-and-finding-optimal-threshold</link>
      <description><![CDATA[我的数据集非常不平衡。
我正在尝试使用 k 折并尝试拟合。但我不明白这会带来什么不同，因为我们只会在原始数据集上测试它？
我已经对数据进行了拆分、独热编码和缩放。此外，我还使用 smote-enn 对其进行了重新采样。]]></description>
      <guid>https://stackoverflow.com/questions/78964032/using-k-fold-for-logistic-regression-and-finding-optimal-threshold</guid>
      <pubDate>Mon, 09 Sep 2024 05:40:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 nn.Linear(in_features, out_features) 在 PyTorch 中使用形状为 (out_features, in_features) 的权重矩阵？</title>
      <link>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</link>
      <description><![CDATA[我试图理解为什么 PyTorch 的 nn.Linear(in_features, out_features) 层的权重矩阵具有形状 (out_features, in_features) 而不是 (in_features, out_features)。
从基本矩阵乘法的角度来看，具有形状 (in_features, out_features) 似乎可以消除在乘法过程中转置权重矩阵的需要。例如，对于形状为 (batch_size, in_features) 的输入张量 x，与形状为 (in_features, out_features) 的权重矩阵相乘将直接产生形状为 (batch_size, out_features) 的输出，而无需转置操作。
但是，PyTorch 将权重矩阵定义为 (out_features, in_features)，这意味着它在前向传递过程中会被转置。这种设计有什么好处？它如何与线性代数和神经网络实现的更广泛原则保持一致？这种选择背后是否有任何效率或一致性考虑使其更可取？]]></description>
      <guid>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</guid>
      <pubDate>Mon, 09 Sep 2024 03:08:49 GMT</pubDate>
    </item>
    <item>
      <title>结合语音、面部表情和文本数据进行实时心理健康监测的挑战 [关闭]</title>
      <link>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</guid>
      <pubDate>Mon, 09 Sep 2024 01:01:00 GMT</pubDate>
    </item>
    <item>
      <title>安装 Snap ML 时出错 | 未找到与 snapml 匹配的发行版 [关闭]</title>
      <link>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</link>
      <description><![CDATA[我在 MacOS 机器上（2.6 GHz Intel Core i7）
我试图安装库“snapml”，但一直出错。
有人知道问题可能是什么吗？
我的终端中有几行：
danilovpavel@Pauls-MacBook-Pro ~ % pip install snapml
错误：找不到满足要求 snapml 的版本（来自版本：无）
错误：未找到与 snapml 匹配的发行版]]></description>
      <guid>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</guid>
      <pubDate>Sat, 07 Sep 2024 17:39:40 GMT</pubDate>
    </item>
    <item>
      <title>我的 pytorch 模型 FPS 低且延迟严重</title>
      <link>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-pytorch-model</link>
      <description><![CDATA[我有一个任务，使用 OpenCV 处理来自 Hikvision IP 摄像机的流媒体视频。最初，视频的 FPS 约为 20-25，延迟为 2-3 秒。然而，随着代码运行时间的延长，FPS 迅速下降，延迟增加。最终，FPS 下降到 2-3，视频冻结。
class VideoLoader:
@staticmethod
def load_video(video_path):
cap = cv2.VideoCapture(video_path)
cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;))
cap.set(cv2.CAP_PROP_FPS, 25)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) 
assert cap.isOpened(), &quot;Video dosyası okunurken hata oluştu&quot;
return cap

我尝试过使用 GPU、采用多线程和降低分辨率等方法，但这些方法效果都不太好。您认为问题是什么，我该如何解决它
def frame_reader(cap, frame_queue):
while cap.isOpened():
success, frame = cap.read()
if not success:
break
if not frame_queue.full(): # 检查队列是否有空间
frame_queue.put(frame)
cap.release()
frame_queue.put(None) # 流结束信号

def video_processor(frame_queue, output_queue, model, class_names, playing_sounds):
Threshold = 0.5 # 测试结果
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)

while True:
frame = frame_queue.get()
if frame is None: # 流结束信号
break
start_time = time.time()
img = cv2.resize(frame, (640, 480))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float().unsqueeze(0).to(device)
img_tensor /= 255.0 # 标准化

def video_detection(video_source):
cap = VideoLoader.load_video(video_source)
model = YOLO(&quot;YOLO-Weights/ppe.pt&quot;)
class_names = [&#39;safety-glasses&#39;, &#39;gloves&#39;, &#39;orange-vest&#39;, &#39;yellow-vest&#39;,]
playing_sounds = set()

frame_queue = Queue(maxsize=5)
output_queue =队列（最大大小=5）

reader_thread = threading.Thread（目标=frame_reader，参数=（cap，frame_queue），守护进程=True）
processor_thread = threading.Thread（目标=video_processor，参数=（frame_queue，output_queue，model，class_names，played_sounds），守护进程=True）

reader_thread.start()
processor_thread.start()
]]></description>
      <guid>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-pytorch-model</guid>
      <pubDate>Tue, 03 Sep 2024 10:50:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用预训练的 BERT 词嵌入向量来微调（初始化）其他网络？</title>
      <link>https://stackoverflow.com/questions/65802782/how-to-use-pretrained-bert-word-embedding-vector-to-finetune-initialize-other</link>
      <description><![CDATA[当我以前使用 textcnn 进行分类工作时，我曾使用 Word2Vec 和 fasttext 等预训练词嵌入对 textcnn 进行微调。我使用这个过程：

在 textcnn 中创建一个嵌入层
通过 Word2Vec 或 fasttext 加载这次使用的单词的嵌入矩阵
由于嵌入层的向量值在训练过程中会发生变化，因此网络正在进行微调。

最近我也想尝试 BERT 来做这件事。我想，“由于使用 BERT 预训练嵌入来初始化其他网络的嵌入层和微调应该没有什么不同，所以应该很容易！”但事实上昨天我试了一整天还是不行。
我发现的事实是，由于BERT的嵌入是上下文嵌入，特别是在提取词嵌入时，每个句子中每个词的向量都会有所不同，所以似乎没有办法像往常一样使用该嵌入来初始化另一个网络的嵌入层……
最后，我想到了一种“微调”的方法，步骤如下：

首先，不要在textcnn中定义嵌入层。
在网络训练部分，我没有使用嵌入层，而是首先将序列标记传递给预训练的BERT模型并获取每个句子的词嵌入。
将2.中的BERT词嵌入放入textcnn并训练textcnn网络。

通过这种方法我终于能够进行训练了，但认真想想，我觉得我根本没有进行微调……
因为如你所见，每次我开始新的训练循环时，从 BERT 生成的词嵌入始终是相同的向量，所以只需将这些不变的向量输入到 textcnn 中，就不会让 textcnn 进行微调，对吗？

更新：
我想到了一种新方法，可以使用 BERT 嵌入并一起“训练”BERT 和 textcnn。
我的部分代码如下：
 BERTmodel = AutoModel.from_pretrained(&#39;bert- 
base-uncased&#39;,output_hidden_​​states=True).to(device)
TextCNNmodel = TextCNN(EMBD_DIM, CLASS_NUM, KERNEL_NUM, 
KERNEL_SIZES).to(device)
optimizer = torch.optim.Adam(TextCNNmodel.parameters(), lr=LR)
loss_func = nn.CrossEntropyLoss()

 for epoch in range(EPOCH):
TextCNNmodel.train()
BERTmodel.train()
for step, (token_batch, seg_batch, y_batch) in enumerate(train_loader):
token_batch = token_batch.to(device)
y_batch = y_batch.to(device)

BERToutputs = BERTmodel(token_batch)
# 我想使用倒数第二个隐藏层作为嵌入，因此
x_batch = BERToutputs[2][-2]

output = TextCNNmodel(x_batch)
output = output.squeeze()
loss = loss_func(output, y_batch)

optimizer.zero_grad()
loss.backward()
optimizer.step()

我认为通过在获取嵌入时启用 BERTmodel.train() 并删除 torch.no_grad()，损失梯度可以反向到 BERTmodel， TextCNNmodel的训练过程也很顺利。
为了以后使用这个模型，我保存了TextCNNmodel和BERTmodel的参数。
然后为了实验BERTmodel是否真的被训练和改变，在另一个程序中我加载了BERTModel，并输入一句话来测试BERTModel是否真的被训练。
但是我发现原始的&#39;bert-base-uncased&#39;模型和我的&#39;BERTmodel&#39;的输出（嵌入）是一样的，这很令人失望......
我真的不知道为什么BERTmodel部分没有变化......]]></description>
      <guid>https://stackoverflow.com/questions/65802782/how-to-use-pretrained-bert-word-embedding-vector-to-finetune-initialize-other</guid>
      <pubDate>Wed, 20 Jan 2021 03:43:21 GMT</pubDate>
    </item>
    <item>
      <title>CNN 的面部表情识别数据准备</title>
      <link>https://stackoverflow.com/questions/37452073/facial-expression-recognition-data-preparation-for-cnn</link>
      <description><![CDATA[我正在研究通过深度学习，特别是 CNN 进行面部表情识别。我对准备和/或预处理数据有一些疑问。
我有正面面部表情的分段视频（例如，根据某人的注释，2-3 秒的视频表达了某人的快乐情绪）。
注意：我的参与者表现出的表情强度很低（不是夸张的表情/微表情）
一般问题：现在，我应该如何准备数据以使用 CNN 进行训练（我有点倾向于使用深度学习库 TensorFlow）？
问题 1：我读过一些基于深度学习的面部表情识别 (FER) 论文，这些论文建议取该表情的峰值（很可能是单个图像）并将该图像用作训练数据的一部分。我如何知道表情的峰值？我的依据是什么？如果我要拍摄单张图片，那么参与者所展现的微妙表情的一些重要帧会不会丢失？
问题 2：或者，在 OpenCV 中执行分段视频以检测（例如 Viola-Jones）、裁剪并保存每帧的面部，并将这些图像与相应的标签一起用作我的训练数据的一部分，这样是否也正确？我猜有些面部帧是多余的。但是，由于我们知道数据中的参与者表现出低强度的表情（微表情），因此面部的一些动作也可能很重要。]]></description>
      <guid>https://stackoverflow.com/questions/37452073/facial-expression-recognition-data-preparation-for-cnn</guid>
      <pubDate>Thu, 26 May 2016 05:11:23 GMT</pubDate>
    </item>
    </channel>
</rss>