<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 21 Jul 2024 09:14:43 GMT</lastBuildDate>
    <item>
      <title>如何在 8 个 GPU 上并行化 Transformer 模型以进行机器翻译？</title>
      <link>https://stackoverflow.com/questions/78774602/how-to-parallelize-transformer-model-for-machine-translation-on-8-gpus</link>
      <description><![CDATA[我正尝试使用 transformer 模型以与原始文章几乎相同的方式执行机器翻译。虽然该模型运行良好，但它需要更大的计算资源。为了解决这个问题，我在一台有 8 个 GPU 处理器的计算机上运行了该模型，但我缺乏这方面的经验。
我尝试对并行化进行必要的调整：
transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

然而，由于我缺乏经验，事情进展不顺利。具体来说，我长时间被以下错误消息困扰：

文件
&quot;C:\Projects\MT005.venv\Lib\site-packages\torch\nn\ functional.py&quot;，
第 5382 行，在 multi_head_attention_forward 中引发 RuntimeError(f&quot;2D attn_mask 的形状是 {attn_mask.shape}，但应该是
{correct_2d_size}。&quot;) RuntimeError：2D attn_mask 的形状是
torch.Size([8, 64])，但应该是 (4, 4)。

有人可以帮我解决这个问题并让模型在所有 8 个 GPU 上运行吗？]]></description>
      <guid>https://stackoverflow.com/questions/78774602/how-to-parallelize-transformer-model-for-machine-translation-on-8-gpus</guid>
      <pubDate>Sun, 21 Jul 2024 07:14:42 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证函数返回“未知标签类型：（array（[0.0, 1.0]，dtype = object），）”</title>
      <link>https://stackoverflow.com/questions/78773942/cross-validation-function-returns-unknown-label-type-array0-0-1-0-dtype</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78773942/cross-validation-function-returns-unknown-label-type-array0-0-1-0-dtype</guid>
      <pubDate>Sat, 20 Jul 2024 21:29:48 GMT</pubDate>
    </item>
    <item>
      <title>你知道 iOS 是否有用于模糊图像中人脸和裸露部分的库吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78773917/do-you-know-if-there-are-libraries-for-blurring-faces-and-nudity-in-images-for-i</link>
      <description><![CDATA[我正在寻找一个库或 ML 模型，可用于模糊图像中存在的面部和裸体。您对此类库或模型有什么建议吗？
或者您将如何为此目的训练 ML 模型？我对 ML 还很陌生，因此如果我必须创建自己的 ML 模型或库，我将非常感激示例和教程。
提前感谢您的任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/78773917/do-you-know-if-there-are-libraries-for-blurring-faces-and-nudity-in-images-for-i</guid>
      <pubDate>Sat, 20 Jul 2024 21:07:35 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习方法检测玩具车中的未知缺陷</title>
      <link>https://stackoverflow.com/questions/78773870/detecting-unknown-defects-in-toy-car-using-deep-learning-methods</link>
      <description><![CDATA[问题：
我们需要根据玩具车图片判断汽车是否有缺陷。我们没有缺陷汽车的图片，也无法提前知道这些汽车可能存在哪些缺陷。
我们可以使用各种技巧拍摄数千张完好无损的汽车照片。此外，我们还可以自动拍摄数千/数万张汽车 CAD 图像快照。
我尝试过的方法：

在传统方法中，我尝试过 OpenCV，但它对所有事物都过于敏感，而且只能应用于以完全相同方式拍摄的两张照片。 :(

Siamese Network 和 Sentence Transformers（余弦相似度等...）也不适合，因为它们对摄影产生的差异很敏感。 :(

Autoencoder 有两种不同的方法：

3.1. 我用基于 CAD 的快照训练了一个自动编码器，然后在真实的有缺陷和无缺陷的图像上对其进行了测试，根据这些重建误差的分布从两个方向切断异常值。这个想法来自这里：https://github.com/sohamk10/Image-reconstruction-and-Anomaly-detection。不是很好解决方案。

3.2. 我用原始照片（大约 10,000 张照片，仅从一个视角拍摄）创建了一个自动编码器模型，其中解码器部分是从 ResNet50 拍摄（并冻结）的，我只训练了编码器部分。我也尝试了其他模型（MobileNetV3、EfficientNet-B3）。结果非常令人满意：它根据重建误差独立于照片环境识别训练过的对象，但它也将那些有轻微缺陷的对象识别为好对象，而它不应该这样做。



YOLO v7：产生与上一点（3.2.）中提到的自动编码器类似的结果。因此，它识别了物体，但不幸的是，它也会识别有缺陷的物体。


如何使用人工智能甚至不使用人工智能来解决这个业余项目问题？
如何修改第 3.2 点中提到的自动编码器。以在训练期间提供较低的 loss 和 val_loss 值（目前约为 0.6，但在 MNIST 数据集上为 0.02...）？]]></description>
      <guid>https://stackoverflow.com/questions/78773870/detecting-unknown-defects-in-toy-car-using-deep-learning-methods</guid>
      <pubDate>Sat, 20 Jul 2024 20:36:56 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle 无缘无故地给我类型错误</title>
      <link>https://stackoverflow.com/questions/78773741/kaggle-giving-me-type-errors-for-no-reason</link>
      <description><![CDATA[Kaggle 一直给我一个 TypeError：&#39;NoneType&#39; 对象对于以下代码不可迭代：
dls = DataBlock(
blocks=(ImageBlock, CategoryBlock), 
get_items=get_image_files, 
splitter=RandomSplitter(valid_pct=0.2, seed=42),
get_y=parent_label,
item_tfms=[Resize(192, method=&#39;squish&#39;)]
).dataloaders(path_to_images, bs=32)

dls.show_batch(max_n=6)

这是完整的笔记本：

我尝试过更改笔记本的环境，但不确定如何修复它。代码在另一台笔记本上运行良好，但对于这台笔记本，它只是出问题了。]]></description>
      <guid>https://stackoverflow.com/questions/78773741/kaggle-giving-me-type-errors-for-no-reason</guid>
      <pubDate>Sat, 20 Jul 2024 19:33:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么我通过 Mediapipe Model Maker 训练制作的自定义模型会将人检测为我的对象？</title>
      <link>https://stackoverflow.com/questions/78773679/why-does-my-custom-model-made-by-training-through-mediapipe-model-maker-detect-p</link>
      <description><![CDATA[所以我有一个想要检测的软玩具。我使用 Mediapipe Model Maker 对大约 100 张图像进行了训练。结果还不错。该模型大多数时候都能从正面识别我的玩具。但问题是，它似乎认为任何人（人的侧面或正面）都是我的玩具，而且比例很高（比如 &gt;90%）。
我不明白为什么会发生这种情况，我该怎么做才能微调我的模型，以便它不会将人检测为我的玩具。我已将我的玩具和检测示例的图像附在下面。

]]></description>
      <guid>https://stackoverflow.com/questions/78773679/why-does-my-custom-model-made-by-training-through-mediapipe-model-maker-detect-p</guid>
      <pubDate>Sat, 20 Jul 2024 19:04:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在评估商业项目时实施 NLP 进行文本分析？</title>
      <link>https://stackoverflow.com/questions/78773575/how-to-implement-nlp-for-text-analysis-in-evaluating-business-projects</link>
      <description><![CDATA[我需要根据特定标准评估业务活动（项目）的资格。我们通过采访利益相关者来收集数据，获取项目名称、描述、不确定性和结果等详细信息。然后根据这些叙述评估每个项目的资格。
我正在考虑将数据科学融入该项目的几种方法，并希望得到有关最佳方法的建议。具体来说，我有兴趣实施 NLP 进行文本分析：

如何分析项目描述以确定共同主题和关键术语？

推荐使用哪些工具和库来使用主题建模来发现共同主题和文本分类来对项目进行分类？


我正在考虑的其他方法包括预测建模、聚类分析和情绪分析。如果您对这些方法有任何建议或资源，那也会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78773575/how-to-implement-nlp-for-text-analysis-in-evaluating-business-projects</guid>
      <pubDate>Sat, 20 Jul 2024 18:23:40 GMT</pubDate>
    </item>
    <item>
      <title>将三个经过训练的二分类模型组合成 Keras 中的单个多分类模型</title>
      <link>https://stackoverflow.com/questions/78773489/combine-three-trained-binary-classification-models-into-single-multiclassificati</link>
      <description><![CDATA[我有三个经过训练的二分类模型，它们在输出层使用 Sigmoid 激活函数进行训练。

第一个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 ZERO。
第二个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 ONE。
第三个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 TWO。


我知道我可以使用 softmax 训练它们，在输出层构建三个神经元的模型。但假设我遇到一种情况，由于模型复杂，训练它们的权重确实需要很长时间，我只有它们各自的二分类模型。或者，我想提取它们在隐藏层的隐藏表示特征，例如 model_0（二分类检查图像是否为零）。
那么，如何将它们连接/组合/合并为单个模型？
我的代码目前卡在了这一点：
model_0 = init_binary_classification_model((28,28))
model_0.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_zero.h5&#39;)

model_1 = init_binary_classification_model((28,28))
model_1.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_one.h5&#39;)

model_2 = init_binary_classification_model((28,28))
model_2.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_two.h5&#39;)

其中：
def init_binary_classification_model(input_shape=(28,28)):
input_layer = Input(shape=input_shape)
tensor = Flatten()(input_layer)
tensor = Dense(16,activation=&#39;relu&#39;)(tensor)
tensor = Dense(8,activation=&#39;relu&#39;)(tensor)
output_layer = Dense(1,activation=&#39;sigmoid&#39;)(tensor)

return Model(inputs=input_layer,outputs=output_layer)

我期望多分类模型具有相同的输入形状(28,28)和不同的输出形状(3)，并且我不需要重新训练模型（如果可能的话）。
完整代码可在https://colab.research.google.com/drive/1y1mvAzebIFU_cuEQo8Q60L1I6uT8i2Ce?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78773489/combine-three-trained-binary-classification-models-into-single-multiclassificati</guid>
      <pubDate>Sat, 20 Jul 2024 17:44:05 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习进行手语字母识别给出错误预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78772476/sign-language-alphabet-identification-using-machine-learning-giving-wrong-predic</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78772476/sign-language-alphabet-identification-using-machine-learning-giving-wrong-predic</guid>
      <pubDate>Sat, 20 Jul 2024 10:17:39 GMT</pubDate>
    </item>
    <item>
      <title>R 中的神经网络代码</title>
      <link>https://stackoverflow.com/questions/78771276/neural-network-codes-in-r</link>
      <description><![CDATA[我正在 R 中运行我的神经网络代码来处理我的生存数据。我可以运行这些代码，但我需要计算神经网络模型的指标（精度、准确度、灵敏度、特异性），并且我应该计算混淆矩阵并绘制 Roc 曲线。你能帮我写一下这部分代码吗？
library(survival)
library(nnet)
library(readxl)
df2 &lt;- read_excel(&quot;E:/SOLMAZ/BS DATA/data BS.xlsx&quot;)
df2
# 将数据分成训练集和测试集
train.index &lt;- sample(1:nrow(df2), round(0.8*nrow(df2)))
train.data &lt;- df2[train.index,]
test.data &lt;- df2[-train.index,]

# 定义一个隐藏层的神经网络模型
nn.model&lt;- nnet(Surv(time_15year,BS_death) ~ age +sex +edu +job +place + cvahis +mihis + bphis+ heartdis + diabhis +hlphis +smok +pastsmok +pasive + activity + waterpip + cvatype, data = train.data, size = 5, maxit = 1000)

# 在测试集上生成预测
test.data$pred &lt;- predict(nn.model, newdata =test.data)
test.data$pred &lt;- ifelse(test.data$pred&gt;median(test.data$pred, na.rm = TRUE),1,0)
print(head(test.data))
table(as.factor(test.data$BS_death),as.factor(test.data$pred[,&quot;status&quot;]))
]]></description>
      <guid>https://stackoverflow.com/questions/78771276/neural-network-codes-in-r</guid>
      <pubDate>Fri, 19 Jul 2024 21:29:13 GMT</pubDate>
    </item>
    <item>
      <title>我的 RandomForestRegressor 上的 MAE 和 MSE 非常高</title>
      <link>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</link>
      <description><![CDATA[我得到了一个航班预测数据集，我想试试我的机器学习技能。
我清理了数据，修复了一些新功能，删除了其他功能
我还得到了一些有价值的数据。但当我尝试进行预测并评估我的模型时
这就是我得到的答案！那是在我使用 SearchGridCV 调整模型之后
测试集上的回归指标
r2：82.10%
mean_absolute_error：1229.1407307097613
mean_squared_error：2933265.159841384

model = RandomForestRegressor(
max_depth=20,
max_features=&#39;sqrt&#39;,
min_samples_leaf=2,
min_samples_split=5,
n_estimators=200
)

X = df.drop(&#39;Price&#39;,axis=1)
y = df[&#39;Price&#39;]

X_train, X_test, y_train, y_test = train_test_split(
pd.get_dummies(X)
, y, test_size=0.2, random_state=42)

model.fit(X_train,y_train)
y_preds = model.predict(X_test)

我尝试修改超参数并删除一些异常值
def remove_outliers_iqr(df, column):
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
return df[(df[column] &gt;= lower_bound) &amp; (df[column] &lt;= upper_bound)]

numerical_columns = [&#39;Price&#39;, &#39;Dep_hours&#39;, &#39;Dep_min&#39;, &#39;Arrival_hours&#39;, &#39;Arrival_min&#39;, &#39;Duration_hours&#39;, &#39;Duration_min&#39;]
for column in numeric_columns:
df = remove_outliers_iqr(df, column)

但我仍然得到相同的结果
这是完整的笔记本，因为我不知道如何以笔记本的方式在此处附加整个代码
https://github.com/jamhus/ztm-course/blob/master/Test%20projects/fligt%20prices%20analysis/flight%20prices.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</guid>
      <pubDate>Fri, 19 Jul 2024 15:47:43 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CLIP 模型获取多模态嵌入？</title>
      <link>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</link>
      <description><![CDATA[我希望使用 CLIP 来获取多模态（图像和文本）数据行的单个嵌入。
假设我有以下模型：
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

def convert_image_data_to_tensor(image_data):
return torch.tensor(image_data)

dataset = df[[&#39;image_data&#39;, &#39;text_data&#39;]].to_dict(&#39;records&#39;)

embeddings = []
for data in dataset:
image_tensor = convert_image_data_to_tensor(data[&#39;image_data&#39;])
text = data[&#39;text_data&#39;]

input = processing(text=text, images=image_tensor, return_tensors=True)
with torch.no_grad():
output = model(**inputs)

我想获取 output 中计算的嵌入。我知道 output 具有附加属性 text_embeddings 和 image_embeddings，但我不确定它们以后如何交互。如果我想为每个记录获取单个嵌入，我应该将这些属性连接在一起吗？是否有其他属性以其他方式将两者结合起来？
这些是存储在输出中的属性：
print(dir(output))

[&#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__dataclass_fields__&#39;, &#39;__dataclass_params__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__post_init__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;image_embeds&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;logits_per_image&#39;, &#39;logits_per_text&#39;, &#39;loss&#39;, &#39;move_to_end&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;text_embeds&#39;, &#39;text_model_output&#39;, &#39;to_tuple&#39;, &#39;update&#39;, &#39;values&#39;, &#39;vision_model_output&#39;]

此外，有没有办法指定 CLIP 输出的嵌入的大小？类似于如何在 BERT 配置中指定嵌入大小？
在此先感谢您的帮助。如果我误解了这里任何关键内容，请随时纠正我。]]></description>
      <guid>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</guid>
      <pubDate>Mon, 15 Jul 2024 19:53:18 GMT</pubDate>
    </item>
    <item>
      <title>通过向 CNN 输入添加位置和字符信息来增强文档布局分析</title>
      <link>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</link>
      <description><![CDATA[我正在研究文档布局分析，并一直在探索 CNN 和基于 Transformer 的网络来完成这项任务。通常，图像作为 3 通道 RGB 输入传递给这些网络。但是，我的数据源是 PDF 格式，我可以直接从中提取准确的位置和字符信息。
我担心将这些 PDF 数据转换为图像进行分析会导致宝贵的位置和字符信息丢失。我的想法是将 CNN 的输入维度从标准的 3 RGB 通道修改为包含这些额外位置和字符信息的更高维度输入。
我了解 CNN 的工作原理，并高度怀疑这种方法可能行不通，但我很感谢社区的任何反馈或建议。有没有人尝试过以这种方式增强输入通道，或者有没有人对将位置和字符数据直接集成到 CNN 中有什么见解？]]></description>
      <guid>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</guid>
      <pubDate>Fri, 12 Jul 2024 10:17:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Anaconda 在 docker 中为 k8s 开发机器学习 Python 应用程序</title>
      <link>https://stackoverflow.com/questions/71767475/machine-learning-python-app-in-docker-with-anaconda-for-k8s</link>
      <description><![CDATA[开发人员在 docker 中运行了一个用于机器学习的 Python 应用程序。特别是 Azure 容器实例。他们使用 micromamba:0.15.3，并且在 dockerfile 中还为 Web 服务器安装了 nginx。
Docker 文件末尾将运行：CMD [&quot;./start.sh&quot;]
以及其中的脚本：
service nginx start 
streamlit run app.py --theme.base &quot;dark&quot; --server.address localhost --server.port 5000 --server.enableCORS=false

我还看到他们在本地使用 anaconda 来运行 Web 应用程序。这也会运行 streamlit
现在我将删除 dockerfile 中的 nginx 部分，因为将迁移到 k8s，并将使用 nginx ingress controller + ingres 作为 vhost，它将指向正在运行的 python 服务
我应该为此使用哪个 Docker 映像？使用 conda、miniconda 或 python 官方映像有什么区别？我是否只需要一个可以添加 streamlit 的 python 图像，例如这里？
https://hub.docker.com/r/mambaorg/micromamba
https://hub.docker.com/_/python]]></description>
      <guid>https://stackoverflow.com/questions/71767475/machine-learning-python-app-in-docker-with-anaconda-for-k8s</guid>
      <pubDate>Wed, 06 Apr 2022 13:16:33 GMT</pubDate>
    </item>
    <item>
      <title>在 RNN 中未找到 rnn_utils 模块</title>
      <link>https://stackoverflow.com/questions/61175064/module-not-found-rnn-utils-in-rnn</link>
      <description><![CDATA[我需要使用这个库来构建我的模型，但是我遇到了这个错误。
from rnn_utils import *

没有名为“rnn_utils”的模块]]></description>
      <guid>https://stackoverflow.com/questions/61175064/module-not-found-rnn-utils-in-rnn</guid>
      <pubDate>Sun, 12 Apr 2020 17:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>