<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 21 May 2024 09:16:39 GMT</lastBuildDate>
    <item>
      <title>为什么我的 YOLO-v8 TFLite 模型比 Pytorch 模型慢？</title>
      <link>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</link>
      <description><![CDATA[我最初有一个经过训练的 Pytorch YOLO-v8 nano 模型，用于视频中的多对象检测（10 个类别 -“自行车”、“椅子”、“盒子”、“桌子”、“塑料袋”） ;、“花盆”、“行李箱​​包”、“雨伞”、“购物车”、“人”）。
我使用 ultralytics 库的导出功能将其转换为 TFLite 模型。然而，当我在视频流上运行这两个模型时，我的 TFLite 模型的运行速度（FPS 约为 8）比我的 Pytorch 模型（FPS 约为 20）慢得多。为什么会这样？
TFLite 和 Pytorch 模型均位于：https://drive .google.com/drive/folders/1A2XUD5sV332nXv-Z756Di_QUIZV3ObFv?usp=sharing
从经过训练的 Pytorch 模型到 tflite 模型的转换。
从 ultralytics 导入 YOLO

模型 = YOLO(“yolov8n_trained.pt”)
路径 = model.export(format=&quot;tflite&quot;)

在模型上运行视频流：
从 ultralytics 导入 YOLO
导入CV2
从导入时间开始

# 启动网络摄像头
Stream_url = “视频流路径”

cap = cv2.VideoCapture(stream_url) # 使用 0 表示网络摄像头
上限设置(3, 640)
上限设置(4, 640)

# 加载重新训练的 YOLOv8 模型
model = YOLO(“yolov8n_trained.tflite”) # 测试 tflite 模型时使用它
# model = YOLO(&quot;yolov8n_trained.pt&quot;) # 测试 pytorch 模型时使用它

# 自定义类名
classNames = [“自行车”,“椅子”,“盒子”,“桌子”,“塑料袋”,“花盆”,
              “行李箱包”、“雨伞”、“购物车”、“人”]

# 初始化变量来计算帧速率
上一个时间 = 0
帧率 = 0

而真实：
    成功，img = cap.read()
    如果没有成功：
        休息

    #计算处理帧所花费的时间
    curr_time = 时间()
    fps = 1 / (当前时间 - 上一个时间)
    上一个时间 = 当前时间

    #在图像上显示帧速率
    cv2.putText(img, f&quot;FPS: {fps:.2f}&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # 在图像上运行 YOLO 模型
    结果=模型（img，流= True）

    # 处理检测结果
    对于结果中的 r：
        对于 r.boxes 中的框：
            # 提取边界框坐标和置信度
            x1, y1, x2, y2 = map(int, box.xyxy[0]) # 转换为整数
            confidence = box.conf[0] # 置信度得分
            class_id = box.cls[0] # 类 ID

            # 绘制边界框
            color = (0, 255, 0) # 边界框的绿色
            cv2.矩形（img，（x1，y1），（x2，y2），颜色，2）

            # 绘制带有类名和置信度的标签
            label = f“{classNames[int(class_id)]}：{置信度：.2f}”
            label_size, base_line = cv2.getTextSize(标签, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1 = max(y1, label_size[1])
            cv2.矩形(img, (x1, y1 - label_size[1]), (x1 + label_size[0], y1 + base_line), (0, 255, 0), cv2.FILLED)
            cv2.putText(img, 标签, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

    # 显示网络摄像头源
    cv2.imshow(&#39;网络摄像头&#39;, img)
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

# 释放资源
cap.release()
cv2.destroyAllWindows()

]]></description>
      <guid>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</guid>
      <pubDate>Tue, 21 May 2024 08:49:32 GMT</pubDate>
    </item>
    <item>
      <title>DecisionTreeRegressor 算法如何工作？</title>
      <link>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</link>
      <description><![CDATA[我是人工智能新手，老实说，我不明白决策树回归算法为何有效。
我尝试研究其背后的算法，但找不到令我满意的答案。
# 导入数组和东西的 numpy 包
将 numpy 导入为 np

# 导入 matplotlib.pyplot 来绘制我们的结果
将 matplotlib.pyplot 导入为 plt

# import pandas 用于导入 csv 文件
将 pandas 导入为 pd

# 导入数据集
# 数据集 = pd.read_csv(&#39;Data.csv&#39;)
# 或者打开.csv文件来读取数据

数据集 = np.array(
[[&#39;资产翻转&#39;, 100, 1000],
[&#39;基于文本&#39;, 500, 3000],
[&#39;视觉小说&#39;, 1500, 5000],
[&#39;2D 像素艺术&#39;, 3500, 8000],
[&#39;2D 矢量艺术&#39;, 5000, 6500],
[&#39;策略&#39;, 6000, 7000],
[&#39;第一人称射击游戏&#39;, 8000, 15000],
[&#39;模拟器&#39;, 9500, 20000],
[&#39;赛车&#39;, 12000, 21000],
[&#39;角色扮演&#39;, 14000, 25000],
[&#39;沙盒&#39;, 15500, 27000],
[&#39;开放世界&#39;, 16500, 30000],
[&#39;MMOFPS&#39;, 25000, 52000],
[&#39;MMORPG&#39;, 30000, 80000]
]）

# 打印数据集
打印（数据集）

# 按 : 选择所有行和第 1 列
# 按1:2表示特征
X = 数据集[:, 1:2].astype(int)

# 打印X
打印（X）

# 按 : 选择所有行和第 2 列
# 由2到Y代表标签
y = 数据集[:, 2].astype(int)

# 打印y
打印（y）

# 导入回归器
从 sklearn.tree 导入 DecisionTreeRegressor

# 创建一个回归器对象
回归器 = DecisionTreeRegressor(random_state = 0)

# 用 X 和 Y 数据拟合回归器
回归器.fit(X, y)

# 预测一个新值

# 通过更改值来测试输出，例如 3750
y_pred = regressor.predict([[3750]]) #输出：8000

# 打印预测价格
print(&quot;预测价格：% d\n&quot;% y_pred)

为什么输出是8000？
我的意思是，我们如何知道这些价格预测函数背后运行的是什么？
它背后的数学公式是什么，我想手动完成，而不使用任何内置函数。
我尝试研究其背后的算法，但找不到令我满意的答案。
我希望得到满意的答复。]]></description>
      <guid>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</guid>
      <pubDate>Tue, 21 May 2024 08:34:32 GMT</pubDate>
    </item>
    <item>
      <title>无法将自参数添加到 catboost 中的自定义指标。无法优化方法“evaluate”，因为使用了 self 参数</title>
      <link>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</link>
      <description><![CDATA[我使用此处的示例。
该示例工作正常，但是，当我尝试通过在其中使用 self 来使自定义指标类变得更加灵活时，我遇到了 UserWarning: Can&#39;t optimze method &quot;evaluate&quot;因为使用了 self 参数
复制问题的代码（如果您希望问题消失，请注释掉 LoglossMetric 的评估方法中的 self.foo = 5 行）
from catboost import CatBoostClassifier、CatBoostRegressor、MultiTargetCustomMetric、MultiTargetCustomObjective
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification、make_regression、make_multilabel_classification
从 sklearn.model_selection 导入 train_test_split
LoglossMetric 类（对象）：
    def get_final_error(自身, 错误, 权重):
        返回误差/(重量+1e-38)

    def is_max_optimal(自身):
        返回错误

    def评估（自我，近似值，目标，重量）：
        自我.foo = 5
        断言 len(大约) == 1
        断言 len(目标) == len(大约[0])

        大约 = 大约[0]

        错误总和 = 0.0
        权重总和 = 0.0

        对于范围内的 i（len（大约））：
            e = np.exp(大约[i])
            p = e / (1 + e)
            w = 1.0 如果权重为 None else 权重[i]
            权重总和 += w
            error_sum += -w * (目标[i] * np.log(p) + (1 - 目标[i]) * np.log(1 - p))

        返回error_sum、weight_sum
X, y = make_classification(n_classes=2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
model2 = CatBoostClassifier(迭代=10, loss_function=“Logloss”, eval_metric=LoglossMetric(),
                            Learning_rate=0.03，bootstrap_type=&#39;贝叶斯&#39;，boost_from_average=False，
                            leaf_estimation_iterations=1, leaf_estimation_method=&#39;梯度&#39;)
model2.fit(X_train, y_train, eval_set=(X_test, y_test))

如果我在评估方法中使用 self ，也会发生同样的事情

catboost 未能完成哪些优化？原因是什么？
]]></description>
      <guid>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</guid>
      <pubDate>Tue, 21 May 2024 08:17:53 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow PPO 模型未给出模型输出</title>
      <link>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</guid>
      <pubDate>Tue, 21 May 2024 06:49:17 GMT</pubDate>
    </item>
    <item>
      <title>如何修复“ValueError：模型没有从输入中返回损失”？</title>
      <link>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</guid>
      <pubDate>Tue, 21 May 2024 06:32:28 GMT</pubDate>
    </item>
    <item>
      <title>为什么大型语言模型无法执行基本算术运算的理论？</title>
      <link>https://stackoverflow.com/questions/78509871/theories-for-why-large-language-models-cannot-perform-basic-arithmetic-operation</link>
      <description><![CDATA[我试图理解为什么基于转换器的语言模型会错误地回答这个问题：
&lt;前&gt;&lt;代码&gt;2571
77130

171400
&lt;小时/&gt;
251601
|
这个数字不正确，正确答案是251101
该模型有足够的训练数据来正确排列所有数字（尽管我怀疑这是为了读者的利益）。该模型通过具有挑战性的乘法正确计算出所有这些数字。该模型甚至可以正确执行后续进位。
那为什么它会在该标记位置输出 6？
许多这样的例子都出现在一组随机采样的数字相乘或相加的情况下。为什么语言模型会这样做（当它们在完成其他语言任务时），流行的理论是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78509871/theories-for-why-large-language-models-cannot-perform-basic-arithmetic-operation</guid>
      <pubDate>Tue, 21 May 2024 05:57:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在Unity中使用Python训练的机器学习模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78509446/how-to-using-machine-learning-model-trained-in-python-within-unity</link>
      <description><![CDATA[我有一项任务需要在 Unity 中使用用 Python 训练的机器学习模型。我仍在研究其可行性，并发现可以使用 IronPython，但网上关于此类任务的信息很少，因此我预计我的方法是非主流的。
有人对主流方法有任何经验或其他建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78509446/how-to-using-machine-learning-model-trained-in-python-within-unity</guid>
      <pubDate>Tue, 21 May 2024 02:36:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 和 SARIMAX 或其他方法预测手机销量 [关闭]</title>
      <link>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python-and-sarimax-or-other</link>
      <description><![CDATA[我正在尝试预测手机的销量，有几个因素。

发布日的销售额可能达到数月至 6 个月的销售额

季节性起着巨大的作用，我有大约 5 年的数据

有时销售会因为手机缺货而停止（但除此之外还有需求）

特定模型的寿命通常较短，因此数据不多，主要使用 2-6 个月的数据


我一直在使用 SARIMAX 将 2 和 4 很好地结合在一起。但我无法解析我的数据的 1 和 3。
你有什么建议？
我尝试阅读 SARIMAX 文档和烦人的 ChatGPT 来寻求解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python-and-sarimax-or-other</guid>
      <pubDate>Tue, 21 May 2024 00:45:07 GMT</pubDate>
    </item>
    <item>
      <title>keras 指标和 sklearn 指标之间的差异</title>
      <link>https://stackoverflow.com/questions/78509105/discrepancy-between-keras-metrics-and-sklearn-metrics</link>
      <description><![CDATA[我使用 InceptionV3 训练了一个 cnn 网络模型，对图像进行分类，以检测胸部 X 光片中的肺结核。
问题在于，在训练时，指标似乎进展顺利。但在评估时却发现了很大的差异。
首先使用此方法使用验证数据生成器评估模型。
## 模型评估
损失、准确度、精确度、召回率、auc = model.evaluate(valid_generator)
print(f&#39;损失: {loss}, 准确度: {accuracy}, 精度: { precision}, 召回率: {recall}, AUC: {auc}&#39;)

它给了我以下结果：
38/38 [================================] - 5s 128ms/步 - 损耗：0.1684 - 准确度：0.9339 - 精确度：0.9978 - 召回率：0.8515 - auc：0.9952
损失：0.16840478777885437，准确度：0.9339389204978943，精度：0.9977973699569702，召回率：0.8515037298202515，AUC：0.9952080845832825

还可以通过以下代码使用 sklearn.metrics 评估模型：
从sklearn.metrics导入classification_report，confusion_matrix
将 numpy 导入为 np
导入 sklearn.metrics

##获取验证数据集的预测
valid_generator.reset()
Y_pred = model.predict（valid_generator，steps=len（valid_generator），verbose=1）
Y_pred = np.round(Y_pred)

##将真实标签转换为数组格式
Y_true = valid_generator.classes

print(&quot;分类报告：\n&quot;,classification_report(Y_true, Y_pred))

# 混淆矩阵
conf_mat = 混淆矩阵(Y_true, Y_pred)
print(&quot;混淆矩阵:\n&quot;, conf_mat)

它给了我这个结果：
38/38 [================================] - 5s 130ms/步
分类报告：
               精确召回率 f1-score 支持

           0 0.57 0.64 0.60 679
           1 0.46 0.39 0.42 532

    准确度 0.53 1211
   宏观平均 0.51 0.51 0.51 1211
加权平均值 0.52 0.53 0.52 1211

混淆矩阵：
 [[432247]
 [325207]]

显然这些不同的评估方法的结果存在差异，哪一个是正确的？
按如下方式编译和训练我的模型：
&lt;前&gt;&lt;代码&gt;#InceptionV3
base_model = InceptionV3(权重=&#39;imagenet&#39;, include_top=False, input_shape=(299, 299, 3))
x = 基础模型.输出
x = GlobalAveragePooling2D()(x)
x = 密集（1024，激活=&#39;relu&#39;）（x）
预测=密集（1，激活=&#39;sigmoid&#39;）（x）

模型 = 模型（输入=base_model.输入，输出=预测）

对于 base_model.layers 中的图层：
    可训练层 = False

model.compile(优化器=Adam(learning_rate=0.0001),
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确率&#39;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])

历史=模型.fit(
    火车发电机，
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    验证数据=有效生成器，
    valid_steps=valid_generator.samples // valid_generator.batch_size,
    纪元=50
）

应该记住，我只对 2 个类别进行分类。

希望您能帮我解决这个疑惑。提前致谢。
能够知道要使用哪些评估指标以及哪些是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/78509105/discrepancy-between-keras-metrics-and-sklearn-metrics</guid>
      <pubDate>Mon, 20 May 2024 23:24:26 GMT</pubDate>
    </item>
    <item>
      <title>sklearn PolynomialFeatures：如果 LinearRegression 生成 y 截距，是否需要偏差</title>
      <link>https://stackoverflow.com/questions/78507382/sklearn-polynomialfeatures-is-the-bias-required-if-linearregression-generates-a</link>
      <description><![CDATA[我是机器学习的新手，因此我一直在尝试一些模型，试图获得更好的理解。
当我创建特征矩阵时：
X_Poly3（X_Poly3 = 多项式特征（3））

其中 X 是 2 列矩阵，生成的 X_Poly3 包含 10 列：
X1、X2、X1^2、X1.X2、X2^2、X1^3、X1^2.X2、X2^2.X1、X2^3 加上“偏差” 1 列。
当我将 LinearRegression() 拟合到该矩阵时，我最终得到 10 个系数加上 y 截距变量。
我认为 1 的偏置列将充当乘数来创建 y 截距，但如果 LinearRegression 创建 y 截距作为标准，是否需要偏置列？
我创建了一个多项式线性回归模型，但最终得到了看起来与 y 截距相关的 2 个变量。
将 numpy 导入为 np
从 sklearn.preprocessing 导入多项式特征

X = np.arange(6).reshape(3, 2)

poly = 多项式特征(3)
X_Poly3 = poly.fit_transform(X)

从 sklearn. Linear_model 导入 LinearRegression
y_train = np.arange(3).reshape(3, 1)

回归器=线性回归()
regressor.fit(X_Poly3, y_train)

print(regressor.intercept_)
打印（回归器.coef_）
]]></description>
      <guid>https://stackoverflow.com/questions/78507382/sklearn-polynomialfeatures-is-the-bias-required-if-linearregression-generates-a</guid>
      <pubDate>Mon, 20 May 2024 15:10:48 GMT</pubDate>
    </item>
    <item>
      <title>k-最近分类概率估计问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</link>
      <description><![CDATA[已知邻居连接方法对噪声敏感。我们将考虑训练样本的一个属性和两个对象的二元分类模型问题：（x_1 = 0.2），（x_2 = 0.7）。第一个对象属于第一类，第二个对象属于第二类。
让我们向对象添加一个新的噪声特征，均匀分布在段 ([0, 1]) 上。现在每个对象都由两个侧面来描述。需要使用具有欧几里德度量的最近邻方法对该空间中的新对象（ u = (0, 0) ）进行分类。
添加第二个噪声对象后，它比第一个更接近对象（u）的概率是多少？
如果可能的话，我想了解使用哪种方法来求解以及应该使用哪些概率和机器学习公式]]></description>
      <guid>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</guid>
      <pubDate>Mon, 20 May 2024 10:54:59 GMT</pubDate>
    </item>
    <item>
      <title>SpaCy 变压器 NER 训练 – 变压器零损耗，未训练</title>
      <link>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</link>
      <description><![CDATA[我正在使用 [&#39;transformer&#39;, &#39;ner&#39;] 组件训练 SpaCy 管道，ner 训练得很好，但 Transformer 的损失为 0，并且我假设它没有进行训练。 
这是我的配置：
&lt;代码&gt;[路径]
矢量=“en_core_web_trf”
init_tok2vec = null
火车=“/home/sxdadmin/spacy/input/train.spacy”
dev =“/home/sxdadmin/spacy/input/dev.spacy”

[系统]
gpu_allocator =“pytorch”；
种子 = 0

[自然语言处理]
lang =“en”；
pipeline = [“变压器”, “ner”]
批量大小 = 512
禁用 = []
创建之前 = null
创建后=空
after_pipeline_creation = null
tokenizer = {“@tokenizers”：“spacy.Tokenizer.v1”}
向量 = {“@vectors”：“spacy.Vectors.v1”}

#################################################### ####################
[成分]
#################################################### ####################

[组件.变压器]
工厂=“变压器”
最大批次项 = 4096

[组件.变压器.模型]
@architectures = “spacy-transformers.TransformerModel.v1”
name = “bert-base-cased”；
tokenizer_config = {“use_fast”：true}

[组件.transformer.model.get_spans]
@span_getters = “spacy-transformers.doc_spans.v1”

[components.transformer.set_extra_annotations]
@annotation_setters = “spacy-transformers.null_annotation_setter.v1”

#################################################### ####################

[组件.ner]
工厂=“ner”
不正确的跨度键 = null
移动=空
计分器 = {“@scorers”：“spacy.ner_scorer.v1”}
update_with_oracle_cut_size = 100

[组件.ner.模型]
@architectures = “spacy.TransitionBasedParser.v2”
state_type =“ner”；
extra_state_tokens = false
隐藏宽度 = 64
最大输出件数 = 2
use_upper = true
nO = 空

#################################################### ####################
[语料库]
#################################################### ####################

[语料库.train]
@readers =“spacy.Corpus.v1”
路径 = ${paths.train}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

[语料库.dev]
@readers =“spacy.Corpus.v1”
路径 = ${paths.dev}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

#################################################### ####################
[训练]
#################################################### ####################

dev_corpus = “corpora.dev”;
train_corpus = “语料库.train”;
种子 = 0
gpu_allocator =“pytorch”；
辍学率 = 0.1
累积梯度= 1
耐心=1600
最大纪元 = 0
最大步数 = 20000
评估频率 = 200
冻结组件 = []
注释组件 = []
before_to_disk = null
更新前=空

#################################################### ####################

[训练.batcher]
@batchers = “spacy.batch_by_words.v1”
丢弃尺寸过大= false
公差 = 0.2
获取长度=空

[训练.batcher.大小]
@schedules =“compounding.v1”；
开始 = 64
停止= 512
化合物 = 1.001
t = 0.0

#################################################### ####################

[训练记录器]
@loggers = “spacy.ConsoleLogger.v1”
进度条=假

[训练.优化器]
@optimizers =“Adam.v1”；
贝塔1 = 0.9
贝塔2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
梯度剪辑 = 1.0
use_averages = false
每股收益 = 0.00000001
学习率 = 0.001

[训练.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

#################################################### ####################
[预训练]
#################################################### ####################

[初始化]
矢量=“en_core_web_lg”
init_tok2vec = null
词汇数据=空
查找=空
before_init = null
after_init = null

[初始化.组件]
[初始化.组件.变压器]
[初始化.tokenizer]

和输出：

所有警告都满足，著名的Bert的max_length 512个tokens就是通过文本分割实现的。数据之前已在 [tok2vec, ner] 设置上进行了测试。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</guid>
      <pubDate>Mon, 20 May 2024 10:39:58 GMT</pubDate>
    </item>
    <item>
      <title>文本到 Openpose 和奇怪的 RNN 错误</title>
      <link>https://stackoverflow.com/questions/78503423/text-to-openpose-and-weird-rnn-bugs</link>
      <description><![CDATA[我想创建从文本描述生成 openpose 的 AI，例如输入“一个男人在跑步”输出将像我提供的图像一样，有没有推荐给我的模型架构？
我的数据条件是

canvas_width: 900px
canvas_height: 300px
帧数：5（5 人）

预期输出
我尝试为这项任务训练 RNN，我使用句子转换器嵌入文本，然后传递给 RNN，损失如下图所示
from sentence_transformers import SentenceTransformer 
sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
text = &quot;a man running&quot;
text_input = torch.tensor(sentence_model.encode(text), dtype=torch.float)

loss image with num_layers=3
我的 RNN 设置
embedding_dim = 384
hidden_​​dim = 512
num_layers = 3
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_​​dim, num_layers, output_dim)

但问题是无论我输入什么，输出每次都是一样的！但是当我尝试将 num_layers 更改为 1 并保持其他设置相同时，如下所示
embedding_dim = 384
hidden_​​dim = 512
num_layers = 1
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_​​dim, num_layers, output_dim)

损失现在看起来像这样
num_layers=1 的损失图像
现在问题已经解决！！
我还尝试检查“每次输出都相同”的原因问题我检查了 dataloader 和其他代码，但没有发现问题只有 num_layers=3 导致问题 num_layers=1 修复了它
这是我的训练循环
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)

trainingEpoch_loss = []
validationEpoch_loss = []

for epoch in range(num_epochs):
    step_loss = []
    rnn_model.train()
    for idx, train_inputs in enumerate(train_dataloader):
        optimizer.zero_grad()
        output = rnn_model(torch.unsqueeze(train_inputs[&#39;text&#39;], dim=0))
       training_loss = 标准（输出，train_inputs [&#39;poses&#39;]）
        training_loss.backward（）
        optimizer.step（）
        step_loss.append（training_loss.item（））

        if（idx+1）％1 == 0：打印（f&#39;Epoch [{epoch+1}/{num_epochs}]，Step [{idx+1}/{len（train_dataloader）}]，损失：{training_loss.item（）：.4f}&#39;）
    trainingEpoch_loss.append（np.array（step_loss）。mean（））

    rnn_model.eval（）
    for idx，val_inputs in enumerate（val_dataloader）：
      validationStep_loss = []
      输出= rnn_model(torch.unsqueeze(val_inputs[&#39;text&#39;], dim=0))
      val_loss = criterion(outputs, val_inputs[&#39;poses&#39;])
      validationStep_loss.append(val_loss.item())
    validationEpoch_loss.append(np.array(validationStep_loss).mean())

这是我的推论
text = &quot;a man running&quot;
processing_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)
output_poses = rnn_model(processed_text.unsqueeze(0))
print(output_poses.shape) #shape=(1, 180) 1 个人是 36（1 个人的原始数据是 54，但我将其更改为 36，因为我只想要 x 和 y 而不是 z，所以剪掉 z 轴）并且有 5 个人，所以 5*36 = 180

我的问题是

除了 RNN 之外，还有其他模型架构推荐用于此任务吗？
为什么无论我输入什么，每次 num_layers=3 时输出都相同，我很困惑，因为如果模型给出相同的输出，损失就不会下降，对吗？这意味着它在推理阶段给出相同的输出

预期答案

最适合我的任务的模型架构，任何论文或 github repo 相关内容都将不胜感激
回答为什么无论我输入什么，当 num_layers=3 时输出都是相同的
]]></description>
      <guid>https://stackoverflow.com/questions/78503423/text-to-openpose-and-weird-rnn-bugs</guid>
      <pubDate>Sun, 19 May 2024 17:37:20 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker 不认可训练作业来启动推理</title>
      <link>https://stackoverflow.com/questions/78497836/sagemaker-does-not-recognize-training-job-to-launch-inference</link>
      <description><![CDATA[我成功在 sagemaker 中启动了培训工作。但是，当我尝试使用该模型进行推理时，sagemaker 无法找到该模型。
导入 sagemaker
从 sagemaker.transformer 导入 Transformer
从 sagemaker.model 导入模型

# 设置会话参数
sagemaker_session = sagemaker.Session()
角色 = sagemaker.get_execution_role()

# 输入路径
model_s3_path = &#39;s3://sagemaker-us-west-1-6584743930/pytorch-training-2024-05-16-15-18-34-042/source/sourcedir.tar.gz&#39;
input_s3_path = &#39;s3://some-bucket/inference/beauty_annotations_simple_transformer.csv&#39;
output_s3_path = &#39;s3://some-bucket/inference/ouput/&#39;

# 定义实例类型
instance_type = &#39;ml.m5.large&#39;

＃ 模型
模型 = 模型（
    model_data=model_s3_path,
    角色=角色，
    Framework_version=&#39;2.0&#39;,
    Entry_point=&#39;推理.py&#39;,
    source_dir=&#39;./source_dir&#39;
）

# 变压器
变压器 = 变压器（
    model_name=模型.name,
    实例计数=1，
    实例类型=实例类型，
    输出路径=输出_s3_路径，
    assemble_with=&#39;Line&#39;, # 输出组装的方法
    Accept=&#39;application/jsonlines&#39;, # 输出格式
）

# 启动批量转换作业
变压器.变换（
    数据=input_s3_path，
    content_type=&#39;text/csv&#39;, # 输入格式
    split_type=&#39;Line&#39;, # 输出分割方式
）

# 作业等待
变压器.wait()

错误
&lt;块引用&gt;
ValueError：无法获取模型信息
pytorch-training-2024-05-16-15-18-34-042。请确保型号
存在。本地实例类型需要本地创建的模型。
]]></description>
      <guid>https://stackoverflow.com/questions/78497836/sagemaker-does-not-recognize-training-job-to-launch-inference</guid>
      <pubDate>Fri, 17 May 2024 20:29:40 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以逼近特殊条件下的softmax概率吗？</title>
      <link>https://stackoverflow.com/questions/62190052/is-any-method-to-approximate-the-softmax-probability-under-special-conditions</link>
      <description><![CDATA[我正在尝试找到不使用 exp() 来计算 softmax 概率的方法。
假设：
目标：计算 f(x1, x2, x3) = exp(x1)/[exp(x1)+exp(x2)+exp(x3)]

状况：

    1.-64＜ x1,x2,x3＜ 64

    2.结果只保留3位小数。

有没有办法找到一个多项式来近似表示这种条件下的结果？]]></description>
      <guid>https://stackoverflow.com/questions/62190052/is-any-method-to-approximate-the-softmax-probability-under-special-conditions</guid>
      <pubDate>Thu, 04 Jun 2020 08:25:38 GMT</pubDate>
    </item>
    </channel>
</rss>