<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ çš„æ´»è·ƒé—®é¢˜ - å †æ ˆå†…å­˜æº¢å‡º</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>æ¥è‡ª stackoverflow.com çš„æœ€æ–° 30 æ¡</description>
    <lastBuildDate>Mon, 12 Aug 2024 03:17:53 GMT</lastBuildDate>
    <item>
      <title>Optuna XGBoost æœªä½¿ç”¨ Mac çš„æ‰€æœ‰ CPU</title>
      <link>https://stackoverflow.com/questions/78859768/optuna-xgboost-not-using-all-of-macs-cpu</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°† Optuna ä¸ mySQL ä¸€èµ·è¿è¡Œï¼Œä»¥å°è¯•å®ç°å¹¶è¡ŒåŒ–å¹¶ä½¿ç”¨æ›´å¤š Mac çš„ CPUã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘è¿è¡Œ GridSearchCV æ—¶ï¼Œæˆ‘çš„ç”¨æˆ· CPU ä½¿ç”¨ç‡å°†ä¸Šå‡åˆ° 90%ï¼Œå¹¶ä¸”é£æ‰‡ä¼šå¯åŠ¨ã€‚ä½†æ˜¯å½“æˆ‘ä½¿ç”¨ Optuna æ—¶ï¼Œæˆ‘å¾—åˆ°å¤§çº¦ 30% å¹¶ä¸”æ²¡æœ‰é£æ‰‡ã€‚è¿™è¡¨æ˜å®ƒæ²¡æœ‰è¢«åˆ©ç”¨ã€‚
æˆ‘å°è¯•ä½¿ç”¨ mySQL å’Œåˆ†å‘åœ¨ VSCode çš„ Jupyter Notebook ä¸Šè¿è¡Œä¸¤ä¸ªå¤„ç†ã€‚æˆ‘è¿™æ ·åšçš„æ–¹å¼æ˜¯åœ¨ä¸åŒçš„å†…æ ¸ä¸Šåˆ¶ä½œæˆ‘çš„ç¬”è®°æœ¬çš„ä¸¤ä¸ªå‰¯æœ¬ï¼Œç„¶åé€šè¿‡åŠ è½½ç ”ç©¶åœ¨åŒä¸€ä¸ª SQL æ•°æ®åº“ä¸Šè¿è¡Œä¼˜åŒ–ä»£ç ã€‚ä¹Ÿè®¸è¿™ä¸æ˜¯æ­£ç¡®çš„åšæ³•ï¼Ÿå› ä¸ºåœ¨ç¤ºä¾‹ä¸­ä»–ä»¬åœ¨ä¸¤ä¸ªç»ˆç«¯ä¸Šè¿è¡Œäº† foo.pyï¼Ÿ
è¿™æ˜¯æˆ‘çš„ä»£ç ï¼š
def objective(trial, X_train, y_train, X_test, y_test):
# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
params = {
&#39;n_estimators&#39;: trial.suggest_int(&#39;n_estimators&#39;, 100, 5000),
&#39;max_depth&#39;: trial.suggest_int(&#39;max_depth&#39;, 2, 20),
&#39;learning_rate&#39;: trial.suggest_float(&#39;learning_rate&#39;, 0.01, 0.2),
&#39;subsample&#39;: trial.suggest_float(&#39;subsample&#39;, 0.7, 1.0),
&#39;colsample_bytree&#39;: trial.suggest_float(&#39;colsample_bytree&#39;, 0.6, 1.0),
&#39;min_child_weight&#39;: trial.suggest_int(&#39;min_child_weight&#39;, 1, 15),
&#39;gamma&#39;: trial.suggest_float(&#39;gamma&#39;, 0.0, 0.4),
&#39;lambda&#39;: trial.suggest_float(&#39;lambda&#39;, 1e-8, 1.0, log=True),
&#39;alpha&#39;: trial.suggest_float(&#39;alpha&#39;, 1e-8, 1.0, log=True)
}

# åˆå§‹åŒ–å¹¶è®­ç»ƒæ¨¡å‹
xgb = XGBRegressor(**params)
xgb.fit(X_train, y_train)

# é¢„æµ‹å¹¶è®¡ç®—æŒ‡æ ‡
y_pred = xgb.predict(X_test)
error = max_percent_error(y_test, y_pred)
return error # è¿”å›è¦æœ€å°åŒ–çš„è¯¯å·®

if __name__ == &quot;__main__&quot;:
study = optuna.load_study(study_name=&quot;example&quot;, storage=&quot;mysql://root@localhost/example&quot;)

# å¯¹æ¯ä¸ªç±»åˆ«å’Œç›®æ ‡æ‰§è¡Œä¼˜åŒ–
for category in train_test_splits:
for target_name in train_test_splits[category]:
if target_name in best_params_dict:
continue
print(f&quot;Running Optuna Optimization for target: {target_name} in category: {category}&quot;)

X_train = train_test_splits[category][target_name][&#39;X_train&#39;]
y_train = train_test_splits[category][target_name][&#39;y_train&#39;]
X_test = train_test_splits[category][target_name][&#39;X_test&#39;]

y_test = train_test_splits[category][target_name][&#39;y_test&#39;]

# ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—ä¼˜åŒ–ç ”ç©¶
study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), 
n_trials=1900, n_jobs=-1)

# å­˜å‚¨ä¸ºæ­¤ç›®æ ‡æ‰¾åˆ°çš„æœ€ä½³å‚æ•°
best_params = study.best_params
best_params_dict[category][target_name] = best_params

# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
xgb_best = XGBRegressor(**best_params)
xgb_best.fit(X_train, y_train)

# ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹
y_pred = xgb_best.predict(X_test)
test_max_percent_error = max_percent_error(y_test, y_pred)
test_r2_score = r2_score(y_test, y_pred)

print(f&quot;{target_name} çš„æœ€ä½³ç»“æœï¼š&quot;)
print(f&quot; æœ€ä½³å‚æ•°ï¼š{best_params}&quot;)
print(f&quot; æµ‹è¯•æœ€å¤§ç™¾åˆ†æ¯”è¯¯å·®ï¼š{test_max_percent_error:.4f}%&quot;)
print(f&quot; æµ‹è¯• R^2ï¼š{test_r2_score:.4f}\n&quot;)


è°¢è°¢]]></description>
      <guid>https://stackoverflow.com/questions/78859768/optuna-xgboost-not-using-all-of-macs-cpu</guid>
      <pubDate>Mon, 12 Aug 2024 01:47:51 GMT</pubDate>
    </item>
    <item>
      <title>å°†å›¾åƒå åŠ åœ¨å åŠ å›¾åƒä¸Š[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/78859435/overlaying-an-image-on-an-overlaying-image</link>
      <description><![CDATA[å‡è®¾æˆ‘æœ‰æ¥è‡ªç›¸æœºçš„å¸§ã€‚æœ‰ä¸€å¼ å›¾åƒï¼Œæˆ‘æƒ³æŠŠå®ƒæ”¾åœ¨è¿™ä¸ªå¸§ä¸Šï¼Œè¿™æ ·å®ƒçš„è§†è§’å°±ä¼šæ ¹æ®å¸§çš„å˜åŒ–è€Œå˜åŒ–ã€‚æˆ‘èƒ½å¤Ÿä½¿ç”¨å•åº”æ€§å¯¹è±¡è·Ÿè¸ªæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½†é—®é¢˜æ˜¯ - å åŠ å›¾åƒä¼šé®æŒ¡å¸§ä¸Šçš„æ‰€æœ‰å¯¹è±¡ã€‚å› æ­¤ï¼Œä¾‹å¦‚ï¼Œå¦‚æœè¯¥å¸§ä¸Šè¯¥å›¾åƒåº”è¯¥ä½äºçš„ä½ç½®æœ‰ä¸€ä¸ªäººï¼Œåˆ™è¯¥äººå°†ä¸å¯è§ã€‚æ˜¯å¦æœ‰ä»»ä½•ç®—æ³•æˆ– ML æ¨¡å‹å¯ä»¥å¸®åŠ©æ˜¾ç¤ºå›¾åƒæ‰€åœ¨åŒºåŸŸä¸­çš„éšœç¢ç‰©ï¼Ÿæˆ‘å°è¯•ä½¿ç”¨ CV2 å·¥å…·ï¼ˆå¦‚ findContoursï¼‰è¿›è¡Œæ‰©å¼ å’Œç±»ä¼¼æ“ä½œï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒæ²¡æœ‰äº§ç”Ÿé¢„æœŸçš„ç»“æœã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78859435/overlaying-an-image-on-an-overlaying-image</guid>
      <pubDate>Sun, 11 Aug 2024 21:20:25 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ HuggingFace ä¸­ä»å¤´å¼€å§‹é‡æ–°åˆå§‹åŒ– GPT XLï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</link>
      <description><![CDATA[æˆ‘è¯•å›¾ç¡®è®¤æˆ‘çš„ GPT-2 æ¨¡å‹æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä»»ä½•é¢„å…ˆå­˜åœ¨çš„é¢„è®­ç»ƒæƒé‡ã€‚è¿™æ˜¯æˆ‘çš„æ–¹æ³•ï¼š

åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 XL æ¨¡å‹ï¼šæˆ‘ä½¿ç”¨ AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;) åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 XL æ¨¡å‹ï¼Œå¹¶è®¡ç®—æ­¤æ¨¡å‹æƒé‡çš„æ€» L2 èŒƒæ•°ã€‚
ä»å¤´å¼€å§‹åˆå§‹åŒ–æ–°çš„ GPT-2 æ¨¡å‹ï¼šç„¶åæˆ‘ä½¿ç”¨ GPT2Config ä»å¤´å¼€å§‹â€‹â€‹ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–æ–°çš„ GPT-2 æ¨¡å‹ã€‚
æ¯”è¾ƒ L2 èŒƒæ•°ï¼šæˆ‘è®¡ç®—é¢„è®­ç»ƒæ¨¡å‹å’Œæ–°åˆå§‹åŒ–æ¨¡å‹çš„æƒé‡çš„ L2 èŒƒæ•°ã€‚æˆ‘çš„å‡è®¾æ˜¯ï¼Œå¦‚æœä¸´æ—¶æ¨¡å‹ç¡®å®æ˜¯ä»éšæœºæƒé‡åˆå§‹åŒ–çš„ï¼Œé‚£ä¹ˆä¸´æ—¶æ¨¡å‹çš„ L2 èŒƒæ•°åº”è¯¥æ¯”é¢„è®­ç»ƒæ¨¡å‹å°å¾—å¤šã€‚

è¿™æ˜¯ä»£ç ç‰‡æ®µï¼š
import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# æ­¥éª¤ 1ï¼šåŠ è½½é¢„è®­ç»ƒçš„ GPT-2 XL æ¨¡å‹
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# æ­¥éª¤ 2ï¼šè®¡ç®—é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„ L2 èŒƒæ•°
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„èŒƒæ•°ï¼š{pretrained_weight_norm:.2f}&quot;)

# æ­¥éª¤ 3ï¼šä½¿ç”¨è‡ªå®šä¹‰é…ç½®ä»å¤´å¼€å§‹åˆå§‹åŒ–æ–°çš„ GPT-2 æ¨¡å‹
config = GPT2Config(
vocab_size=52000, # ç¡®ä¿è¿™ä¸ tokenizer çš„è¯æ±‡é‡ç›¸åŒ¹é…
n_ctx=1024, # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆæ¨¡å‹ä¸€æ¬¡å¯ä»¥çœ‹åˆ°çš„ token æ•°é‡ï¼‰
bos_token_id=0, # åºåˆ—å¼€å§‹ token
eos_token_id=1, # åºåˆ—ç»“æŸ token
)
model = GPT2LMHeadModel(config)

# æ­¥éª¤ 4ï¼šè®¡ç®—åˆšåˆå§‹åŒ–çš„æ¨¡å‹æƒé‡çš„ L2 èŒƒæ•°
scratch_weight_norm = 0.0
for param in model.parameters():
scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;ä»å¤´å¼€å§‹åˆå§‹åŒ–çš„æ¨¡å‹çš„æ€» L2 èŒƒæ•°ï¼š{scratch_weight_norm:.2f}&quot;)

è¿™ç§æ–¹æ³•æ˜¯å¦æ˜¯ç¡®è®¤æ¨¡å‹æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„æœ‰æ•ˆæ–¹æ³•ï¼Ÿæ˜¯å¦å­˜åœ¨ä»»ä½•æ½œåœ¨é—®é¢˜æˆ–æ›´å¥½çš„æ–¹æ³•æ¥éªŒè¯æ¨¡å‹æ²¡æœ‰é¢„å…ˆå­˜åœ¨çš„å­¦ä¹ æƒé‡ï¼Ÿ
çœ‹èµ·æ¥æ­£ç¡®
~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.pyâ€‹â€‹
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 689/689 [00:00&lt;00:00ï¼Œ8.05MB/s]
model.safetensorsï¼š100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.43G/6.43G [00:29&lt;00:00ï¼Œ221MB/s]
generation_config.jsonï¼š100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00&lt;00:00ï¼Œ1.03MB/s]
é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„æ€» L2 èŒƒæ•°ï¼š24542.74
ä»å¤´åˆå§‹åŒ–æ¨¡å‹çš„æ€» L2 èŒƒæ•°ï¼š1637.31
ï¼ˆbeyond_scale_div_coeffï¼‰

cross: https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905
ref: https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18]]></description>
      <guid>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</guid>
      <pubDate>Sun, 11 Aug 2024 20:27:07 GMT</pubDate>
    </item>
    <item>
      <title>æœºå™¨å­¦ä¹ ï¼ˆGANï¼‰ç”Ÿæˆå›¾åƒ</title>
      <link>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</guid>
      <pubDate>Sun, 11 Aug 2024 20:01:20 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ Google Colabï¼ˆTesla T4ï¼‰ç›¸æ¯”ï¼Œæˆ‘çš„ GPUï¼ˆRTX 3070ï¼‰çœŸçš„é‚£ä¹ˆæ…¢å—ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/78858972/is-my-gpu-rtx-3070-that-slow-when-compared-to-google-colab-tesla-t4</link>
      <description><![CDATA[å½“ä½¿ç”¨æˆ‘çš„ jupyter ç¬”è®°æœ¬å’Œ google colabï¼ˆä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç¬”è®°æœ¬ï¼‰è®­ç»ƒåŒä¸€æ¨¡å‹æ—¶ï¼Œ
å®ƒä»¬éƒ½è¿è¡Œç›¸åŒçš„ä»£ç ï¼Œå› æ­¤å®ƒä»¬è‚¯å®šéƒ½ä½¿ç”¨â€œcudaâ€ä½œä¸ºè®¾å¤‡ã€‚
æˆ‘çš„æœ¬â€‹â€‹åœ°æœºå™¨æ€»è®­ç»ƒæ—¶é—´ï¼š72.489 ç§’
Google Colab æ€»è®­ç»ƒæ—¶é—´ï¼š3.115 ç§’
è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å·®å¼‚ã€‚ä»¥ä¸‹æ˜¯æ¥è‡ªæˆ‘çš„æœºå™¨å’Œgoogle colabçš„â€œnvidia-smiâ€è¾“å‡ºã€‚
æˆ‘å·²é‡æ–°å®‰è£…å¹¶æ›´æ–°äº†å®Œæ•´çš„ pytorch ç”Ÿæ€ç³»ç»Ÿï¼Œæ›´æ–°äº†æˆ‘çš„é©±åŠ¨ç¨‹åºå’Œ CUDA é©±åŠ¨ç¨‹åºã€‚
æˆ‘çš„æœºå™¨ä¸Šçš„ cuda ä¸­ä¸€åˆ‡éƒ½åœ¨è¿è¡Œï¼Œä½†ä¸çŸ¥ä½•æ•…é€Ÿåº¦å¾ˆæ…¢ã€‚åœ¨æˆ‘çš„æœºå™¨ä¸­ï¼ŒCPU å®é™…ä¸Šæ¯” GPU æ›´å¿«ã€‚
BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count()

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)
train_loader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)
train_loader, test_loader

è¿™å°±æ˜¯æˆ‘çš„åŠ è½½å™¨çš„è®¾ç½®æ–¹å¼ã€‚
batch, labels = next(iter(train_loader))
print(f&#39;{batch.size() = }&#39;)
print(f&#39;{labels.size() = }&#39;)

åœ¨æˆ‘è¿™è¾¹åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡éœ€è¦ 2-3 ç§’ã€‚è€Œåœ¨ google colab ä¸­é€Ÿåº¦æ›´å¿«ã€‚è¿™å¯èƒ½æ˜¯ç“¶é¢ˆå—ï¼Ÿ
æˆ‘åº”è¯¥åšä»€ä¹ˆï¼Œè¿˜æ˜¯æˆ‘çš„ GPU çœŸçš„é‚£ä¹ˆç³Ÿç³•ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/78858972/is-my-gpu-rtx-3070-that-slow-when-compared-to-google-colab-tesla-t4</guid>
      <pubDate>Sun, 11 Aug 2024 17:29:05 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•è°ƒè¯•ä¸å·¥ä½œçš„ Yolov8n æ¨¡å‹ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/78858811/how-do-i-debug-yolov8n-model-not-working</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„ yolov8 æ¨¡å‹è¿›è¡Œå¯¹è±¡æ£€æµ‹å’Œè·Ÿè¸ªã€‚æˆ‘èƒ½å¤ŸæˆåŠŸåŠ è½½å®ƒï¼Œä½†ç”±äºæŸç§åŸå› ï¼Œå½“å®ƒæ£€æµ‹åˆ°å¯¹è±¡æ—¶ï¼Œå®ƒä¼šéšæœºæ£€æµ‹åˆ°è®¸å¤šä¸å›¾åƒæ— å…³çš„ä¸åŒå¯¹è±¡ï¼Œæ‰€æœ‰å¯¹è±¡çš„ç½®ä¿¡åº¦å¾—åˆ†å‡ä¸º 1.0ã€‚
è¿™æ˜¯æˆ‘çš„ä»£ç ï¼š
from ultralytics import YOLO
import cv2

#åŠ è½½ yolov8 æ¨¡å‹
model = YOLO(&quot;yolov8n.pt&quot;)

#åŠ è½½ video()
video_path = &#39;./puppy.mp4&#39;
cap = cv2.VideoCapture(video_path)

ret = True
while ret:
ret,frame = cap.read()
#ä»è§†é¢‘ä¸­è¿”å›ä¸€ä¸ªæ–°å¸§ï¼›å¦‚æœæˆåŠŸè¯»å–å¸§ï¼Œåˆ™ ret ä¸ºçœŸï¼Œå¦åˆ™ä¸ºå‡
å¦‚æœä¸æ˜¯ ret:
break
#æ£€æµ‹å¯¹è±¡
#è·Ÿè¸ªå¯¹è±¡
results = model.track(frame,persist=True) #persist= Trueï¼Œå› æ­¤ YOLO ä¼šè®°ä½å®ƒä¹‹å‰è§è¿‡çš„å¸§

#ç»˜åˆ¶ç»“æœ
frame_ = results[0].plot() #åˆ›å»ºç”¨äºæ£€æµ‹çš„å›¾åƒ
#ä¹Ÿå¯ä»¥ä½¿ç”¨ cv2.rectangle å’Œ cv2.putText
#å¯è§†åŒ–
cv2.imshow(&#39;frame&#39;,frame_)
if cv2.waitKey(25) &amp; 0xFF==ord(&#39;q&#39;):
break

è¾“å‡ºï¼š
0ï¼š384x640 3 è¾†æ±½è½¦ã€33 è¾†æ‘©æ‰˜è½¦ã€21 æ¶é£æœºã€29 åˆ—ç«è½¦ã€52 è‰˜èˆ¹ã€9 ä¸ªæ¶ˆé˜²æ “ã€7 ä¸ªé•¿å‡³ã€1 ä¸ªæ‰‹æç®±ã€37 ä¸ªæ»‘é›ªæ¿ã€6 ä¸ªæ»‘é›ªæ¿ã€1 ä¸ªè¿åŠ¨çƒã€5 æ ¹æ£’çƒæ£’ã€8 ä¸ªç“¶å­ã€1 ä¸ªå‰å­ã€20 æŠŠåˆ€ã€64 æŠŠå‹ºå­ã€1 ä¸ªé©¬æ¡¶ã€1 ä¸ªçƒ¤é¢åŒ…æœºã€1 æŠŠç‰™åˆ·ã€329.4 æ¯«ç§’
é€Ÿåº¦ï¼š4.5 æ¯«ç§’é¢„å¤„ç†ã€329.4 æ¯«ç§’æ¨ç†ã€51.4 æ¯«ç§’åå¤„ç†æ¯ä¸ªå½¢çŠ¶ä¸º (1, 3, 384, 640) çš„å›¾åƒ

0ï¼š384x640 1 è¾†è‡ªè¡Œè½¦ã€16 è¾†æ±½è½¦ã€4 è¾†æ‘©æ‰˜è½¦ã€13 æ¶é£æœºã€4å…¬å…±æ±½è½¦ã€9 åˆ—ç«è½¦ã€9 è‰˜èˆ¹ã€8 ä¸ªäº¤é€šä¿¡å·ç¯ã€1 ä¸ªæ¶ˆé˜²æ “ã€10 ä¸ªåœè½¦æ ‡å¿—ã€2 å¼ é•¿æ¤…ã€3 åªç¾Šã€2 å¤´ç‰›ã€1 æŠŠé›¨ä¼ã€2 ä¸ªé£ç›˜ã€2 æ ¹æ£’çƒæ£’ã€1 å‰¯æ£’çƒæ‰‹å¥—ã€5 æ”¯ç½‘çƒæ‹ã€2 æŠŠåˆ€ã€2 æŠŠå‹ºå­ã€1 æ ¹èƒ¡èåœã€3 ä¸ªçƒ­ç‹—ã€5 ä¸ªæŠ«è¨ã€1 ä¸ªç”œç”œåœˆã€1 å¼ æ²™å‘ã€1 å¼ åºŠã€476.8 æ¯«ç§’
é€Ÿåº¦ï¼š9.3 æ¯«ç§’é¢„å¤„ç†ã€476.8 æ¯«ç§’æ¨ç†ã€0.0 æ¯«ç§’åå¤„ç†æ¯ä¸ªå½¢çŠ¶ä¸º (1, 3, 384, 640) çš„å›¾åƒ

0ï¼š384x640 1 ä¸ªäººã€7 è¾†æ±½è½¦ã€3 è¾†æ‘©æ‰˜è½¦ã€20 æ¶é£æœºã€1 è¾†å…¬å…±æ±½è½¦ã€7 åˆ—ç«è½¦ã€44 è‰˜èˆ¹ã€2 ä¸ªæ¶ˆé˜²æ “ã€5 ä¸ªåœè½¦æ ‡å¿—ã€3 ä¸ªåœè½¦è®¡è´¹è¡¨ã€2 åªé¸Ÿã€3 å¤´ç‰›ã€2å¤§è±¡ã€1 åªç†Šã€4 æŠŠé›¨ä¼ã€1 ä¸ªæ‰‹æåŒ…ã€1 ä¸ªé£ç›˜ã€1 æŠŠåˆ€ã€4 æŠŠå‹ºå­ã€2 ä¸ªè‹¹æœã€1 ä¸ªèƒ¡èåœã€1 ä¸ªçƒ­ç‹—ã€1 ä¸ªæŠ«è¨ã€1 ä¸ªè›‹ç³•ã€1 å¼ é¤æ¡Œã€651.2 æ¯«ç§’
é€Ÿåº¦ï¼š15.5 æ¯«ç§’é¢„å¤„ç†ã€651.2 æ¯«ç§’æ¨ç†ã€15.8 æ¯«ç§’åå¤„ç†æ¯ä¸ªå½¢çŠ¶ä¸º (1ã€3ã€384ã€640) çš„å›¾åƒ

0ï¼š384x640 7 ä¸ªäººã€3 è¾†æ±½è½¦ã€1 è¾†æ‘©æ‰˜è½¦ã€2 åˆ—ç«è½¦ã€13 è‰˜èˆ¹ã€2 ä¸ªäº¤é€šä¿¡å·ç¯ã€2 ä¸ªæ¶ˆé˜²æ “ã€54 ä¸ªåœè½¦æ ‡å¿—ã€2 ä¸ªåœè½¦è®¡è´¹è¡¨ã€2 åªçŒ«ã€4 åªç‹—ã€1 å¤´ç‰›ã€18 åªç†Šã€1 åŒ¹æ–‘é©¬ã€15 åªé•¿é¢ˆé¹¿ã€11 æŠŠé›¨ä¼ã€14 ä¸ªæ‰‹æåŒ…ã€6 æ¡é¢†å¸¦ã€1 ä¸ªé£ç›˜ã€1 ä¸ªæ»‘é›ªæ¿ã€1 ä¸ªè¿åŠ¨çƒã€ 5 ä¸ªæ£’çƒæ‰‹å¥—ã€4 ä¸ªé…’æ¯ã€1 æŠŠå‰å­ã€1 æŠŠåˆ€ã€1 æŠŠå‹ºå­ã€1 ä¸ªè‹¹æœã€1 ä¸ªæ©™å­ã€1 æ ¹èƒ¡èåœã€2 ä¸ªæŠ«è¨ã€1 å¼ é¤æ¡Œã€2 å°ç¬”è®°æœ¬ç”µè„‘ã€1 éƒ¨æ‰‹æœºã€1 ä¸ªæ°´æ§½ã€1 ä¸ªå¹é£æœºã€656.2 æ¯«ç§’
é€Ÿåº¦ï¼š0.0 æ¯«ç§’é¢„å¤„ç†ã€656.2 æ¯«ç§’æ¨ç†ã€66.9 æ¯«ç§’åå¤„ç†æ¯ä¸ªå½¢çŠ¶ä¸º (1, 3, 384, 640) çš„å›¾åƒ

0ï¼š384x640 20 è‰˜èˆ¹ã€8 å¼ é•¿å‡³ã€5 åªçŒ«ã€4 ä¸ªæ‰‹æç®±ã€48 ä¸ªé£ç›˜ã€3 ä¸ªæ»‘æ¿ã€20 ä¸ªç¢—ã€1 å¼ æ²™å‘ã€26 å¼ åºŠã€37 å°ç¬”è®°æœ¬ç”µè„‘ã€1 ä¸ªé¼ æ ‡ã€15 ä¸ªé¥æ§å™¨ã€30 éƒ¨æ‰‹æœºã€27 æœ¬ä¹¦ã€1057.5 æ¯«ç§’
é€Ÿåº¦ï¼š15.7 æ¯«ç§’é¢„å¤„ç†ã€1057.5 æ¯«ç§’æ¨ç†ï¼Œå½¢çŠ¶ä¸º (1, 3, 384, 640) æ—¶æ¯å¹…å›¾åƒçš„åå¤„ç†æ—¶é—´ä¸º 55.0ms

å¦‚æ‚¨æ‰€è§ï¼Œå½“è§†é¢‘ä¸­åªæœ‰ä¸€æ¡ç‹—æ—¶ï¼Œå®ƒæ£€æµ‹åˆ°äº†è®¸å¤šéšæœºç‰©ä½“ã€‚
è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡ä½¿ç”¨ yoloï¼Œæˆ‘æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ–°æ‰‹ã€‚
æˆ‘ä¹Ÿå°è¯•ä½¿ç”¨ç»ˆç«¯ä½¿ç”¨ yolov8n è¿›è¡Œé¢„æµ‹ï¼Œä½†å®ƒä»ç„¶æ£€æµ‹åˆ°è®¸å¤šä¸åŒçš„ç‰©ä½“ï¼Œç½®ä¿¡åº¦å‡ä¸º 1.0ã€‚
è¿™æ˜¯æˆ‘åœ¨ CLI ä¸­è¾“å…¥çš„å†…å®¹ï¼š
 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;
è¾“å‡ºï¼š
Ultralytics YOLOv8.2.75 ğŸš€ Python-3.12.3 torch-2.4.0+cpu CPUï¼ˆç¬¬ 11 ä»£ Intel Core(TM) i5-1145G7 2.60GHzï¼‰
YOLOv8n æ‘˜è¦ï¼ˆèåˆï¼‰ï¼š168 å±‚ã€3,151,904 ä¸ªå‚æ•°ã€0 ä¸ªæ¢¯åº¦ã€8.7 GFLOP

å°† https://ultralytics.com/images/bus.jpg ä¸‹è½½åˆ°&#39;bus.jpg&#39;...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134k/134k [00:00&lt;00:00ï¼Œ8.86MB/s]
å›¾åƒ 1/1 C:\Users\aksha\OneDrive\Documents\Computer_Vision\tutorial_detect\bus.jpgï¼š640x480 79 äººã€3 è¾†è‡ªè¡Œè½¦ã€46 è¾†æ±½è½¦ã€46 è¾†æ‘©æ‰˜è½¦ã€15 æ¶é£æœºã€11 è¾†å…¬å…±æ±½è½¦ã€5 åˆ—ç«è½¦ã€28 è¾†å¡è½¦ã€49 è‰˜èˆ¹ã€4 ä¸ªæ¶ˆé˜²æ “ã€2 ä¸ªåœè½¦è®¡è´¹è¡¨ã€2 ä¸ªè¿åŠ¨çƒã€6 ä¸ªç“¶å­ã€4 æŠŠå‹ºå­ã€324.2ms
é€Ÿåº¦ï¼š15.6ms é¢„å¤„ç†ã€324.2ms æ¨ç†ã€27.8ms åå¤„ç†ï¼Œå½¢çŠ¶ä¸º (1, 3, 640, 480)ï¼Œæ¯å¹…å›¾åƒ
ç»“æœä¿å­˜åˆ° runs\detect\predict
ğŸ’¡ äº†è§£æ›´å¤šä¿¡æ¯https://docs.ultralytics.com/modes/predict

æˆ‘çš„ ultralyticsã€torch å’Œ np ç‰ˆæœ¬å¦‚ä¸‹ï¼š
8.2.75 - ultralytics
2.4.0+cpu - torch
1.26.4 - numpy
æˆ‘ä¸ç¡®å®šé—®é¢˜æ˜¯ä»€ä¹ˆï¼›æˆ‘å°è¯•å¸è½½å¹¶é‡æ–°å®‰è£… ultralytics ä¸¤æ¬¡ï¼Œä½†é—®é¢˜æ²¡æœ‰è§£å†³ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78858811/how-do-i-debug-yolov8n-model-not-working</guid>
      <pubDate>Sun, 11 Aug 2024 16:25:30 GMT</pubDate>
    </item>
    <item>
      <title>Epoch 1/3 ^C - model.fit() ä»¥æ­¤è¡Œç»ˆæ­¢ï¼Œä¸”æ²¡æœ‰ä»»ä½•é”™è¯¯</title>
      <link>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</guid>
      <pubDate>Sun, 11 Aug 2024 13:47:36 GMT</pubDate>
    </item>
    <item>
      <title>è·å– ValueErrorï¼šæ‰€æœ‰æ•°ç»„çš„é•¿åº¦å¿…é¡»ç›¸åŒ</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[æˆ‘ä¸€ç›´è¯•å›¾å°†å­—å…¸è½¬æ¢ä¸ºæ•°æ®æ¡†ï¼Œä½†æ¯æ¬¡æˆ‘éƒ½æ”¶åˆ° ValueErrorï¼šæ‰€æœ‰æ•°ç»„çš„é•¿åº¦å¿…é¡»ç›¸åŒã€‚æˆ‘å·²ç»æ£€æŸ¥äº†æ¯ä¸ªæ•°ç»„çš„é•¿åº¦å¹¶ç¡®è®¤å®ƒä»¬ç›¸åŒï¼Œä½†æˆ‘ä»ç„¶æ”¶åˆ°ç›¸åŒçš„é”™è¯¯
def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# åˆå¹¶æ¥è‡ªè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æŒ‡æ ‡
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# æ’åºåˆ— 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# è·å–æŒ‡æ ‡è¡¨
metrics_table = metrics_from_pipes(pipelines)

è¿è¡Œæ­¤ä»£ç ä¼šå‡ºç°æ­¤é”™è¯¯
ValueError Traceback (most recent call last)
Cell In[45]ï¼Œç¬¬ 82 è¡Œ
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # è·å–æŒ‡æ ‡è¡¨
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;è¡¨ 1ï¼šåŸºæœ¬æ¨¡å‹æŒ‡æ ‡&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

å•å…ƒæ ¼ In[45]ï¼Œç¬¬ 50 è¡Œï¼Œä½äº metrics_from_pipes(pipes_dict)
41 # å°†æ€§èƒ½æŒ‡æ ‡åˆ—è¡¨èšåˆåˆ°å•ç‹¬çš„æ•°æ®æ¡†ä¸­
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: æ‰€æœ‰æ•°ç»„çš„é•¿åº¦å¿…é¡»ç›¸åŒ

å½“æˆ‘æ£€æŸ¥ train_metrics å’Œ val æŒ‡æ ‡çš„å­—å…¸ç»“æœæ—¶ï¼Œæˆ‘å¾—åˆ°äº†è¿™ä¸ª
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;çº¿æ€§å›å½’&#39;,
&#39;éšæœºæ£®æ—å›å½’å™¨&#39;,
&#39;æ¢¯åº¦æå‡å›å½’&#39;,
&#39;é¢å¤–æ ‘å›å½’å™¨&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>æ‰¹é‡æ•°æ®çš„ SGD ä¼˜åŒ–å™¨è®¾ç½®</title>
      <link>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å­¦ä¹  Joh Krohn çš„æ•°å­¦å…¥é—¨è¯¾ç¨‹ã€‚è¯¾ç¨‹è§£é‡Šå¾—å¾ˆæ¸…æ¥šï¼Œä½†æœ‰ä¸€ä»¶äº‹è®©æˆ‘å¾ˆå›°æƒ‘ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynbï¼Œæˆ‘ä»¬ä½¿ç”¨äº† torch.optim.SGD torch SGDï¼Œå®ƒè¿è¡Œäº†æ‰€æœ‰ç¤ºä¾‹æ•°æ®ã€‚
optimizer = torch.optim.SGD([m,b], lr = 0.01)
epochs = 999
for epoch in range(epochs): 

optimizer.zero_grad() # å°†æ¢¯åº¦é‡ç½®ä¸ºé›¶ï¼›å¦åˆ™å®ƒä»¬ä¼šç´¯ç§¯

yhats = å›å½’ï¼ˆxsï¼Œmï¼Œbï¼‰# æ­¥éª¤ 1
C = mseï¼ˆyhatsï¼Œysï¼‰# æ­¥éª¤ 2

C.backward() # æ­¥éª¤ 3

optimizer.step() # æ­¥éª¤ 4

åœ¨ç¬¬äºŒä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å­¦ä¹ ç‡è°ƒåº¦ https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/learning-rate-scheduling.ipynb
æœ‰ 8.000.000 ä¸ªæ•°æ®ç‚¹ï¼Œå› æ­¤æ•°æ®è¢«è®¾ç½®ä¸ºæ‰¹å¤„ç†ï¼Œå¹¶ä¸”ä»£ç åœ¨è¿™äº›æ ·æœ¬ä¸Šè½®æµè¿è¡Œï¼Œè€Œä¸æ˜¯åœ¨æ‰€æœ‰æ•°æ®ä¸ŠæŒ‰æ—¶æœŸè¿è¡Œã€‚ç„¶è€Œï¼Œè¿™ä¸æ˜¯ç”¨ torch.optim.SGD å®Œæˆçš„ï¼Œè€Œæ˜¯åœ¨ä»£ç ä¸Šæ˜¾ç¤ºä»¥æŸ¥çœ‹æ•°å­¦æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘æ­£åœ¨åŠªåŠ›ç”¨ torch.optim.SGD è¿è¡Œå®ƒã€‚å¦‚ä½•ç¼–å†™ä»£ç æ¥è¿è¡Œå®ƒï¼Œè€Œä¸æ˜¯åƒä¸‹é¢è¿™æ ·ç¼–å†™å¤§å‹æ•°å­¦æ–¹ç¨‹å¼ï¼Œå…¶ä¸­å·²ç»åˆ›å»ºäº†æ‰€æœ‰æ–¹ç¨‹å¼ï¼Œä¾‹å¦‚æ¢¯åº¦ã€thetaï¼š
n = 8000000
x = torch.linspace(0., 8., n)
y = -0.5*x + 2 + torch.normal(mean=torch.zeros(n), std=1)
indices = np.random.choice(n, size=2000, replace=False)
gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 
lr = 0.01
new_theta = theta - lr*gradient
C = mse(regression(x[batch_indices], m, b), y[batch_indices])
b.requires_grad_()
m.requires_grad_()

def return(my_x, my_m, my_b):
return my_m*my_x + my_b

m = torch.tensor([0.9]).requires_grad_()
b = torch.tensor([0.1]).requires_grad_()

batch_size = 32 # æ¨¡å‹è¶…å‚æ•°
batch_indices = np.random.choice(n, size=batch_size, replace=False)
yhat = return(x[batch_indices], m, b)

yhat = return(x[batch_indices], m, b)

def mse(my_yhat, my_y): 
sigma = torch.sum((my_yhat - my_y)**2)
return sigma/len(my_y)

C = mse(yhat, y[batch_indices])

C.backward()
m.grad
b.grad

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T

theta = torch.tensor([[b, m]]).T 

lr = 0.01
new_theta = theta - lr*gradient
new_theta

b = new_theta[0]
m = new_theta[1]

C = mse(regression(x[batch_indices], m, b), y[batch_indices])

rounds = 100 

for r in range(rounds): 

# è¿™ä¸ªé‡‡æ ·æ­¥éª¤å¾ˆæ…¢ï¼›ç¨åæˆ‘ä»¬å°†ä»‹ç»æ›´å¿«çš„æ‰¹é‡é‡‡æ ·ï¼š 
batch_indices = np.random.choice(n, size=batch_size, replace=False)

yhat = return(x[batch_indices], m, b) # æ­¥éª¤ 1
C = mse(yhat, y[batch_indices]) # æ­¥éª¤ 2

C.backward() # æ­¥éª¤ 3

gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T
theta = torch.tensor([[b, m]]).T 

new_theta = theta - lr*gradient # æ­¥éª¤ 4

b = new_theta[0].requires_grad_()
m = new_theta[1].requires_grad_()
]]></description>
      <guid>https://stackoverflow.com/questions/78858189/sgd-optimizer-setting-for-batched-data</guid>
      <pubDate>Sun, 11 Aug 2024 11:20:48 GMT</pubDate>
    </item>
    <item>
      <title>è§£å†³è‡ªåŠ¨æ ‡è®°ï¼ˆä¼˜åŒ–ï¼‰+åˆ†ç±»é—®é¢˜</title>
      <link>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</link>
      <description><![CDATA[æˆ‘çŸ¥é“è¿™ä¸å¤ªä¾§é‡äºç¼–ç¨‹ï¼Œä½†æˆ‘ä¸çŸ¥é“è¿˜æœ‰ä»€ä¹ˆåœ°æ–¹å¯ä»¥é—®è¿™ä¸ªé—®é¢˜ã€‚è¿™æ›´å¤šçš„æ˜¯å…³äºæ–¹æ³•è€Œä¸æ˜¯æŠ€æœ¯é—®é¢˜ã€‚
ä¸Šä¸‹æ–‡
æˆ‘æœ‰ä¸€ä¸ªä¼˜åŒ– + åˆ†ç±»ä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬è´¨ä¸Šï¼Œæˆ‘çš„æ•°æ®å…·æœ‰ä»¥ä¸‹åˆ—ï¼š
[&#39;Model ID&#39;, &#39;Q&#39;, &#39;refinement&#39;, &#39;avg_time&#39;, &#39;lattice&#39;, &#39;radius&#39;]ï¼ˆè¿˜æœ‰æ›´å¤šï¼Œä½†ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬åªä¿ç•™è¿™äº›ï¼‰

Model IDï¼šä»£è¡¨â€œè®¾è®¡â€ï¼Œæ¯ä¸ª Model ID å°†æœ‰å¤šè¡Œ

Qï¼šè¿™æ˜¯ç›®æ ‡å˜é‡

refinementï¼šè¿™æ˜¯ä¸€ä¸ªè®¾ç½®å˜é‡ï¼›å®ƒå¯ä»¥å– 1-8 çš„å€¼ï¼Œè¿™ç›´æ¥å½±å“ Qï¼Œï¼ˆæ¨¡å‹ IDï¼Œç»†åŒ–ï¼‰å¯¹æ˜¯å”¯ä¸€çš„ã€‚å› æ­¤ï¼Œæ¨¡å‹ ID å°†å…·æœ‰å¤šè¡Œï¼Œç»†åŒ–ç¨‹åº¦å„ä¸ç›¸åŒ

avg_timeï¼šè¿™æ˜¯æ¨¡æ‹Ÿå®Œæˆæ‰€éœ€çš„æ—¶é—´ï¼Œä»…å—ç»†åŒ–çš„å½±å“ï¼Œç»†åŒ–ç¨‹åº¦è¶Šé«˜ï¼Œæ‰€éœ€çš„æ—¶é—´è¶Šé•¿ã€‚æ­¤å€¼ä¸è®¾è®¡æ— å…³ï¼Œå®ƒä»…å–å†³äºç»†åŒ–ï¼Œå› æ­¤ç‰¹å®šç»†åŒ–çš„æ‰€æœ‰è®¾è®¡éƒ½å…·æœ‰ç›¸åŒçš„æ—¶é—´ã€‚

lattice å’Œ radiusï¼šè¿™äº›ä»£è¡¨â€œè®¾è®¡â€ï¼Œæœ¬è´¨ä¸Šæ›´æ”¹å®ƒä»¬å°†æ›´æ”¹ Q


æ•°æ®é›†
æˆ‘çš„æ•°æ®é›†æ¥è‡ªéšæœºè®¾è®¡çš„æ¨¡æ‹Ÿã€‚å¯¹äºæ¯ä¸ªè®¾è®¡ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰ä»¥ä¸‹è¡Œä¸ºï¼š

æŒç»­å¢åŠ ï¼ˆæ¯æ¬¡ç»†åŒ–æ—¶çš„ Q å€¼é«˜äºä¸Šä¸€ä¸ªç»†åŒ–çº§åˆ«ï¼‰
æŒç»­å‡å°‘ï¼ˆæ¯æ¬¡ç»†åŒ–æ—¶çš„ Q å€¼ä½äºä¸Šä¸€ä¸ªç»†åŒ–çº§åˆ«ï¼‰
ä¹‹å­—å½¢ï¼Œå…¶ä¸­ Q å€¼éµå¾ªæ­¤å½“å‰æ¨¡å¼ï¼ˆé«˜ï¼Œä½ï¼Œé«˜ï¼Œä½ï¼‰æˆ–ï¼ˆä½ï¼Œé«˜ï¼Œä½ï¼Œé«˜ï¼‰
ç¢—å½¢ï¼Œå…¶ä¸­ Q å€¼éµå¾ªæ­¤å½“å‰æ¨¡å¼ï¼šï¼ˆé«˜ï¼Œä½ï¼Œä½ï¼Œé«˜ï¼‰
æ¢¯å½¢ï¼Œå…¶ä¸­ Q å€¼éµå¾ªæ­¤å½“å‰æ¨¡å¼ï¼šï¼ˆä½ï¼Œé«˜ï¼Œé«˜ï¼Œä½ï¼‰
æˆ‘æœ‰ä»£ç å¯ä»¥æ£€æµ‹è¿™äº›å½¢çŠ¶å¹¶è¿”å›å¸ƒå°”å€¼ï¼š

def is_zigzag(q_values)

def is_bowl(q_values)

def is_trapezoid(q_values)

æ•°æ®é›†ä¸­ç»†åŒ–çš„å€¼èŒƒå›´æ˜¯ 2-5ï¼Œä½†ç»†åŒ–å¯ä»¥å– 1-8 çš„å€¼ã€‚
ä»»åŠ¡
å› æ­¤ï¼Œæˆ‘è¯•å›¾å®ç°çš„æ˜¯è‡ªåŠ¨æ ‡è®°æ¯ä¸ªæ¨¡å‹ IDï¼ˆé€šè¿‡å¯¹è¡Œè¿›è¡Œåˆ†ç»„ï¼‰å’Œæœ€ä½³ç»†åŒ–å€¼ï¼ˆèŒƒå›´ä¸º 1-8ï¼‰ï¼Œä»¥æœ€å¤§åŒ– Q çš„å˜åŒ–ï¼ˆå¢é‡è¶Šå¤§è¶Šå¥½ï¼‰å¹¶æœ€å°åŒ–æ‰€èŠ±è´¹çš„æ—¶é—´ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚é—®é¢˜æ˜¯ç”±äºæ•°æ®æ˜¯åœ¨ç»†åŒ–çº§åˆ« 5 å¤„åˆ‡å‰²çš„ï¼Œæ‰€ä»¥æˆ‘æƒ³åˆ°ä½¿ç”¨æ¦‚ç‡æ–¹æ³•ï¼ˆä¾‹å¦‚ MLEï¼‰æ¥åˆ›å»ºæœªæ¥ç»†åŒ–çš„é¢„æœŸå˜åŒ–å’Œé¢„æœŸæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚ä½†æˆ‘ä¼¼ä¹æ— æ³•â€œè°ƒæ•´â€å®ƒï¼Œæ‰€ä»¥å®ƒå¾ˆæœ‰ç”¨ã€‚è·å¾—é¢„æœŸå€¼åï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæˆæœ¬å‡½æ•°æ¥è®¡ç®—ï¼ˆä¼˜åŒ–ï¼‰Q çš„å›æŠ¥ä¸å®Œæˆè¯¥ç»†åŒ–çº§åˆ«æ‰€èŠ±è´¹çš„å¢åŠ æ—¶é—´çš„æ¯”è¾ƒã€‚
åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†å¼€å‘ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå®ƒå°†é‡‡ç”¨è®¾è®¡å’Œæœ€ä½³ç»†åŒ–ã€‚ä»ç†è®ºä¸Šè®²ï¼Œå®ƒåº”è¯¥å¯ä»¥é¢„æµ‹æœªè§è¿‡çš„è®¾è®¡çš„æœ€ä½³ç»†åŒ–çº§åˆ«
æˆ‘å¾ˆæ„Ÿæ¿€ä»»ä½•æœ‰å…³è§£å†³è¿™ä¸ªé—®é¢˜çš„æŒ‡å¯¼/å¸®åŠ©ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78858155/tackling-an-automatic-labeling-optimization-classification-problem</guid>
      <pubDate>Sun, 11 Aug 2024 11:01:40 GMT</pubDate>
    </item>
    <item>
      <title>å¤„ç†ç¼ºå¤±æ•°æ®å¹¶å»ºç«‹å…·æœ‰ä¸å®Œæ•´ä¿¡æ¯çš„é¢„æµ‹æ¨¡å‹ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä¸ºæ¶‰åŠ 20 ä¸ªå½±å“ç‚¹çš„ä¾›æ°´ç½‘ç»œå¼€å‘ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œæˆ‘åªæœ‰è¿™ 20 ä¸ªç‚¹ä¸­çš„ 10 ä¸ªçš„å†å²æ•°æ®ã€‚
æˆ‘æƒ³çŸ¥é“å¦‚ä½•åœ¨è¿™ä¸ªä¸å®Œæ•´çš„æ•°æ®é›†ä¸‹æ„å»ºé¢„æµ‹æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼š
æˆ‘å¯ä»¥ä½¿ç”¨å“ªäº›æ–¹æ³•æ¥å¤„ç†å‰©ä½™ 10 ä¸ªç‚¹çš„ç¼ºå¤±æ•°æ®ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯å¦æœ‰ä»»ä½•æ ‡å‡†æŠ€æœ¯æˆ–æœ€ä½³å®è·µæ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Ÿ
æˆ‘å¦‚ä½•æœ‰æ•ˆåœ°å°†æˆ‘æ‹¥æœ‰çš„ 10 ä¸ªç‚¹çš„æ•°æ®åˆå¹¶åˆ°æ¨¡å‹ä¸­ï¼Ÿæˆ‘å¯ä»¥é‡‡ç”¨å“ªäº›ç­–ç•¥æ¥ç¡®ä¿æœ‰æ•ˆåˆ©ç”¨å¯ç”¨æ•°æ®è¿›è¡Œå‡†ç¡®é¢„æµ‹ï¼Ÿ
æ˜¯å¦æœ‰ç‰¹å®šçš„æŠ€æœ¯æˆ–æ¨¡å‹å¯ä»¥å¸®åŠ©åœ¨æ•°æ®ä¸å®Œæ•´çš„æƒ…å†µä¸‹è¿›è¡Œé¢„æµ‹ï¼Ÿæˆ‘å¯¹å¯ä»¥æœ‰æ•ˆç®¡ç†å’Œåˆ©ç”¨ä¸å®Œæ•´æ•°æ®çš„æ–¹æ³•æ„Ÿå…´è¶£ã€‚
æˆ‘è¿˜æ²¡æœ‰å…·ä½“çš„æ–¹æ³•ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78858124/handling-missing-data-and-building-a-predictive-model-with-incomplete-informatio</guid>
      <pubDate>Sun, 11 Aug 2024 10:43:28 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ hub.KerasLayer ä½¿ç”¨ tf.keras.sequential åˆ¶ä½œæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶å‡ºé”™</title>
      <link>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</link>
      <description><![CDATA[# åˆ›å»ºä¸€ä¸ªæ„å»º Keras æ¨¡å‹çš„å‡½æ•°

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
print(&quot;Building model with:&quot;, MODEL_URL)

model = tf.keras.Sequential([
hub.KerasLayer(MODEL_URL), # ç¬¬ 1 å±‚ï¼ˆè¾“å…¥å±‚ï¼‰
tf.keras.layers.Dense(units=OUTPUT_SHAPE, 
activation=&quot;softmax&quot;) # ç¬¬ 2 å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
loss=tf.keras.losses.CategoricalCrossentropy(), # æˆ‘ä»¬çš„æ¨¡å‹æƒ³è¦å‡å°‘è¿™ä¸ªï¼ˆå®ƒçš„çŒœæµ‹æœ‰å¤šé”™è¯¯ï¼‰
optimizer=tf.keras.optimizers.Adam(), # ä¸€ä¸ªæœ‹å‹å‘Šè¯‰æˆ‘ä»¬çš„æ¨¡å‹å¦‚ä½•æ”¹è¿›å®ƒçš„çŒœæµ‹
metrics=[&quot;accuracy&quot;] # æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå€¼ä¸Šå‡
)

# æ„å»ºæ¨¡å‹
model.build(INPUT_SHAPE) # è®©æ¨¡å‹çŸ¥é“å®ƒå°†è·å¾—ä»€ä¹ˆæ ·çš„è¾“å…¥

è¿”å›æ¨¡å‹

æˆ‘æœ‰ä¸Šé¢çš„å‡½æ•°ï¼Œå½“æˆ‘è¿è¡Œä¸‹é¢çš„å…¶ä»–ç¨‹åºæ—¶
model = create_model()
model.summary()

å®ƒä¼šäº§ç”Ÿä¸€äº›é”™è¯¯
TypeErrorï¼šæ·»åŠ çš„å±‚å¿…é¡»æ˜¯ Layer ç±»çš„å®ä¾‹ã€‚
æ”¶åˆ°ï¼šlayer=&lt;Dense name=dense_18,built=False&gt; ç±»å‹ä¸º &lt;class &#39;keras.src.layers.core.dense.Dense&#39;&gt;ã€‚

æˆ‘å“ªé‡Œåšé”™äº†ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/78857786/error-when-using-hub-keraslayer-using-tf-keras-sequential-to-make-deep-learning</guid>
      <pubDate>Sun, 11 Aug 2024 08:05:45 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•è®¡ç®— CV-k æŠ˜å é¢„æµ‹â€œæ˜¯â€â€œå¦â€çš„å‡æ–¹è¯¯å·®ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</link>
      <description><![CDATA[ä¸ºäº†æ ¹æ®æ»¡æ„åº¦æŒ‡æ•°å’Œå‚ä¸åº¦æŒ‡æ•°é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼šè´­ä¹°äº§å“ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† k è¿‘é‚»æ³•ã€‚
å¦‚ä½•ç”¨ 4 å€äº¤å‰éªŒè¯è¿‡ç¨‹è¯„ä¼°é¢„æµ‹å‡æ–¹è¯¯å·®ï¼Ÿå®ƒä¸éœ€è¦åœ¨ R ä¸­ã€‚æˆ‘åªéœ€è¦å¦‚ä½•è®¡ç®—å®ƒçš„ç†è®ºã€‚
æˆ‘çŸ¥é“å¦‚ä½•ç”¨æ•°å­—è®¡ç®—ï¼Œä½†ä¸çŸ¥é“å¦‚ä½•ç”¨å­—ç¬¦ä¸²è®¡ç®—]]></description>
      <guid>https://stackoverflow.com/questions/78857561/how-to-calculate-mean-square-error-for-predictions-yes-no-with-a-cv-k-fold</guid>
      <pubDate>Sun, 11 Aug 2024 05:16:27 GMT</pubDate>
    </item>
    <item>
      <title>æœ‰äººå¯ä»¥æŒ‡å¯¼æˆ‘å­¦ä¹  Yolov10 çš„è·¯çº¿å›¾å—ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/78850160/can-anyone-guide-me-the-roadmap-for-learning-yolov10</link>
      <description><![CDATA[æˆ‘å¯¹æ·±å…¥ç ”ç©¶æ·±åº¦å­¦ä¹ é¢†åŸŸå¾ˆæ„Ÿå…´è¶£ï¼Œå°¤å…¶å…³æ³¨ä½¿ç”¨ YOLOv10 è¿›è¡Œå¯¹è±¡æ£€æµ‹ã€‚
ä½†æ˜¯ï¼Œæˆ‘å¯¹è¯¥é¢†åŸŸå¸¸ç”¨çš„è®¸å¤šå·¥å…·å’Œæ¡†æ¶ä»ç„¶å¾ˆé™Œç”Ÿï¼Œä¾‹å¦‚ TensorFlowã€OpenCV å’Œ PyTorchã€‚
é‰´äº YOLOv10 æ˜¯ YOLO ç³»åˆ—ä¸­çš„æœ€æ–°ç‰ˆæœ¬ï¼Œå¹¶å¼•å…¥äº†å‡ ä¸ªé«˜çº§åŠŸèƒ½ï¼Œä¾‹å¦‚ NMS æ— è®­ç»ƒå’Œæé«˜æ•ˆç‡ï¼Œæˆ‘æœ‰ç‚¹ä¸çŸ¥æ‰€æªï¼Œä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ã€‚
æˆ‘æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªå…¨é¢çš„è·¯çº¿å›¾ï¼Œå¯ä»¥æŒ‡å¯¼æˆ‘å®Œæˆæœ‰æ•ˆä½¿ç”¨ YOLOv10 æ‰€éœ€çš„å…ˆå†³æ¡ä»¶ã€å¿…è¦æŠ€èƒ½å’Œå­¦ä¹ èµ„æºã€‚
æˆ‘å¸Œæœ›è·å¾—ä¸€äº›ç‰¹å®šé¢†åŸŸçš„å»ºè®®åŒ…æ‹¬ï¼š
æˆ‘åº”è¯¥é¦–å…ˆæŒæ¡çš„æ·±åº¦å­¦ä¹ ä¸­çš„å…³é”®åŸºç¡€ä¸»é¢˜ã€‚

è€ƒè™‘åˆ°æˆ‘å¯¹ PyTorch å’Œ OpenCV è¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·æä¾›å­¦ä¹ èµ„æºæˆ–æ•™ç¨‹ã€‚

å¦‚ä½•ç†è§£å¯¹è±¡æ£€æµ‹æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯åœ¨ YOLOv10 çš„èƒŒæ™¯ä¸‹ã€‚

ä¸ºè®­ç»ƒå’Œéƒ¨ç½² YOLO æ¨¡å‹è®¾ç½®å¼€å‘ç¯å¢ƒçš„æœ€ä½³å®è·µã€‚

ä»»ä½•å¯ä»¥å·©å›ºæˆ‘ç†è§£çš„æ¨èé¡¹ç›®æˆ–ç»ƒä¹ ã€‚

]]></description>
      <guid>https://stackoverflow.com/questions/78850160/can-anyone-guide-me-the-roadmap-for-learning-yolov10</guid>
      <pubDate>Thu, 08 Aug 2024 19:38:10 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘ä½¿ç”¨è‡ªå®šä¹‰å¢å¼ºå’Œ TFRecord ç®¡é“åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•æ˜¯å¦æœ‰æ•ˆï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/78847703/is-my-approach-to-training-a-model-on-a-large-image-dataset-using-custom-augment</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ªå­˜å‚¨åœ¨ TFRecord æ–‡ä»¶ä¸­çš„å¤§å‹å›¾åƒæ•°æ®é›†ï¼Œæˆ‘æƒ³åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘çš„ç›®æ ‡æ˜¯åœ¨å°†å›¾åƒè¾“å…¥æ¨¡å‹ä¹‹å‰å¯¹å›¾åƒåº”ç”¨è‡ªå®šä¹‰å¢å¼ºã€‚ä½†æ˜¯ï¼Œæˆ‘æ‰¾ä¸åˆ°å†…ç½®çš„ TensorFlow å‡½æ•°ï¼ˆå¦‚ ImageDataGeneratorï¼‰æ¥åœ¨è®­ç»ƒä¹‹å‰å°†å¢å¼ºç›´æ¥åº”ç”¨äºå­˜å‚¨ä¸ºå¼ é‡çš„å›¾åƒã€‚
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªè‡ªå®šä¹‰ ModelTrainer ç±»ï¼Œå…¶ä¸­æˆ‘ï¼š
ä» TFRecord åŠ è½½æ¯ä¸ªå›¾åƒã€‚
å¯¹å›¾åƒåº”ç”¨ä¸€ç³»åˆ—è‡ªå®šä¹‰å˜æ¢ï¼ˆä¾µèš€ã€è†¨èƒ€ã€å‰ªåˆ‡ã€æ—‹è½¬ï¼‰ã€‚
åˆ›å»ºä¸€ä¸ªç”±åŸå§‹å›¾åƒåŠå…¶å˜æ¢ç‰ˆæœ¬ç»„æˆçš„æ‰¹æ¬¡ã€‚
åœ¨è¿™ä¸ªæ‰¹æ¬¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªæ‰¹æ¬¡ç”±å•ä¸ªå›¾åƒåŠå…¶å˜æ¢ç‰ˆæœ¬ç»„æˆã€‚
è¿™æ˜¯æˆ‘çš„ä»£ç ç‰‡æ®µï¼š
class ModelTrainer:
def __init__(self, model):
self.model = model

def preprocess_image(self, image):
image = tf.cast(image, tf.float32) / 255.0
return image

def apply_erosion(self, image):
kernel = np.ones((5,5), np.uint8)
return cv2.erode(image, kernel, iterations=1)

def apply_dilation(self, image):
kernel = np.ones((5,5), np.uint8)
return cv2.dilate(image, kernel, iterations=1)

def apply_shear(self, image):
rows, cols = image.shape
M = np.float32([[1, 0.5, 0], [0.5, 1, 0]])
è¿”å› cv2.warpAffine(image, M, (cols, rows))

def apply_rotation(self, image, angle=15):
rows, cols = image.shape
M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
è¿”å› cv2.warpAffine(image, M, (cols, rows))

def transform_image(self, img, i):
if i == 0:
è¿”å› img
elif i == 1:
è¿”å› self.apply_erosion(img)
elif i == 2:
è¿”å› self.apply_dilation(img)
elif i == 3:
è¿”å› self.apply_shear(img)
elif i == 4:
è¿”å› self.apply_rotation(img)

def train_on_tfrecord(self, tfrecord_path, dataset, batch_size=5):
dataset = dataset.map(lambda img, lbl: (self.preprocess_image(img), lbl))
dataset = dataset.batch(1)
dataset = iter(dataset)

å¯¹äº batch_imagesï¼Œæ•°æ®é›†ä¸­çš„æ ‡ç­¾ï¼š
img_np = batch_images.numpy().squeeze()
lbl_np = labels.numpy().squeeze(axis=0)
image_batch = []
label_batch = []

å¯¹äº i in range(5):
perceived_image = self.transform_image(img_np, i)
image_batch.append(transformed_image)
label_batch.append(lbl_np)

image_batch_np = np.stack(image_batch, axis=0)
label_batch_np = np.stack(label_batch, axis=0)

image_batch_tensor = tf.convert_to_tensor(image_batch_np, dtype=tf.float32)
label_batch_tensor = tf.convert_to_tensor(label_batch_np, dtype=tf.float32)

loss = self.model.train_on_batch(image_batch_tensor, label_batch_tensor)

predictions = self.model.predict(image_batch_tensor)
predict_labels = np.argmax(predictions, axis=-1)
true_labels = np.argmax(label_batch_tensor, axis=-1)
accuracy = np.mean(predicted_labels == true_labels)

print(f&quot;Batch Loss = {loss}, Accuracy = {accuracy:.4f}&quot;)


æˆ‘çš„é—®é¢˜æ˜¯ï¼š

æˆ‘ä¸€æ¬¡åœ¨ä¸€ä¸ªå›¾åƒåŠå…¶è½¬æ¢ç‰ˆæœ¬ä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•æ˜¯å¦å¥½ä¸”æœ‰æ•ˆï¼Ÿ
ä»¥è¿™ç§æ–¹å¼è®­ç»ƒç½‘ç»œæ˜¯å¦å¯å–ï¼Œåœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­å¤„ç†ä¸€ä¸ªå›¾åƒåŠå…¶å¢å¼ºï¼Ÿ
æ˜¯å¦æœ‰æ›´å¥½çš„æ–¹æ³•æˆ–ä¼˜åŒ–æˆ‘åº”è¯¥è€ƒè™‘å¤„ç†å¤§å‹æ•°æ®é›†å’Œåº”ç”¨è‡ªå®šä¹‰å¢å¼ºï¼Ÿ
]]></description>
      <guid>https://stackoverflow.com/questions/78847703/is-my-approach-to-training-a-model-on-a-large-image-dataset-using-custom-augment</guid>
      <pubDate>Thu, 08 Aug 2024 09:51:24 GMT</pubDate>
    </item>
    </channel>
</rss>