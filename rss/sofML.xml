<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Apr 2024 09:14:50 GMT</lastBuildDate>
    <item>
      <title>Randforest，关于“对于每个节点，选择不替换的特征”的问题</title>
      <link>https://stackoverflow.com/questions/78383228/randforest-question-on-for-each-node-select-features-without-replacement</link>
      <description><![CDATA[从书籍（与chatGPT仔细检查）中，随机森林的步骤可以总结如下：
&lt;块引用&gt;

绘制大小为 n 的随机引导样本（随机选择 n 个示例
来自带有替换的训练数据集）。
从引导样本中生成决策树。在每个节点：

&lt;块引用&gt;
a)。随机选择d个特征，不进行替换。
b).使用提供最佳分割的功能来分割节点
例如，最大化信息增益的目标函数。
3. 重复步骤 1-2 k 次。
...


我的问题来自粗体字：既然特征是无替换选择的，如果选择了所有特征，树会停止生长吗（因为没有可供选择的特征）？ ——看来这不是真的。但这一步怎么理解呢？]]></description>
      <guid>https://stackoverflow.com/questions/78383228/randforest-question-on-for-each-node-select-features-without-replacement</guid>
      <pubDate>Thu, 25 Apr 2024 08:20:17 GMT</pubDate>
    </item>
    <item>
      <title>如何向 DBSCAN 添加自定义参数</title>
      <link>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</link>
      <description><![CDATA[我一直在努力创建一个自定义 DBSCAN，在其中我可以创建一个自定义参数来根据出租车 ID 过滤距离，以查看哪些出租车造成了交通堵塞。我添加了时间和距离矩阵，但我不知道如何创建一个新的 eps 来根据 ID 过滤出租车。如果 eps 值较高，则应对不同的 Taix ID 进行聚类，如果较低，则应对同一出租车进行聚类。有关如何进行的任何提示？谢谢
time_dist = pdist(X[:, 0].reshape(n, 1), metric=self.metric)
euc_dist = pdist(X[:, 1:], metric=self.metric)


# 使用 time_dist 过滤 euc_dist 矩阵
dist = np.where(time_dist &lt;= self.eps2, euc_dist, 2 * self.eps1)

db = DBSCAN(eps=self.eps1,
            min_samples=self.min_samples,
            指标=&#39;预先计算&#39;）
db.fit(正方形(距离))

self.labels = db.labels_
]]></description>
      <guid>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</guid>
      <pubDate>Thu, 25 Apr 2024 07:54:41 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：“dense”层的输入 0 与该层不兼容</title>
      <link>https://stackoverflow.com/questions/78383058/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer</link>
      <description><![CDATA[我正在尝试使用超模型编写一个分类器，有 4 个类。 X的尺寸为14935×2
这是我的代码的一部分：
def build_model(hp):
    模型=顺序（）
    activation_choice = hp.Choice(&#39;activation&#39;, value=[&#39;relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;, &#39;elu&#39;, &#39;selu&#39;])
    model.add(密集(单位=hp.Int(&#39;units_input&#39;,
                                   最小值=512，
                                   最大值=1024，
                                   步骤=32),
                    input_dim=784，
                    激活=activation_choice））
    model.add(密集(单位=hp.Int(&#39;units_hidden&#39;,
                                   最小值=128，
                                   最大值=600，
                                   步骤=32),
                    激活=activation_choice））
    model.add（密集（4，激活=&#39;softmax&#39;））
    模型.编译(
        优化器=hp.Choice(&#39;优化器&#39;, 值=[&#39;adam&#39;,&#39;rmsprop&#39;,&#39;SGD&#39;]),
        损失=&#39;分类交叉熵&#39;，
        指标=[&#39;准确性&#39;])
    返回模型
调谐器=随机搜索(
    构建模型，
    目标=&#39;val_accuracy&#39;,
                                 
    最大试验次数=80，
    目录=&#39;测试目录&#39;
    ）
x_train, x_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)
x_train = x_train.reshape(2987, 8)
x_test = x_test.reshape(2987, 2)
x_train = x_train / 255
x_测试 = x_测试 / 255
y_train = utils.to_categorical(y_train, 10)
y_test = utils.to_categorical(y_test, 10)
调谐器.搜索(x_train,
             y_火车，
             批量大小=256，
             纪元=20，
             验证分割=0.2，
             ）

“tunner.search”报错：
层“密集”的输入 0与图层不兼容：输入形状的预期轴 -1 的值为 784，但收到的输入形状为（无，8）

我试图找到问题的解决方案，但没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78383058/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer</guid>
      <pubDate>Thu, 25 Apr 2024 07:49:53 GMT</pubDate>
    </item>
    <item>
      <title>我可以做些什么来优化 C++ 中的 CatBoost（或其他）模型？</title>
      <link>https://stackoverflow.com/questions/78382887/what-can-i-do-to-optimizing-catboost-or-other-model-in-c</link>
      <description><![CDATA[我的 C++ 程序从 .cbm 文件加载 CatBoost 模型并进行预测。但该模型对我来说花费了太多时间，我想减少它的延迟。
我想知道我可以在 C++ 中做什么来优化此类模型的延迟。
#include “wrapped_calcer.h”

结构模型{

    静态内联 ModelCalcerWrapper 模型 = ModelCalcerWrapper(&quot;./model/model.cbm&quot;);

    std::向量 xdataArray；

    无效 updateXData() {
        // 更新xdataArray中的数据
        // 我将计算一些值并将其放入模型中
    }

    双预测（）{
        // 我认为这一步对我来说花费了太多时间
        返回 model.Calc(xdataArray, {});
    }

};
]]></description>
      <guid>https://stackoverflow.com/questions/78382887/what-can-i-do-to-optimizing-catboost-or-other-model-in-c</guid>
      <pubDate>Thu, 25 Apr 2024 07:17:43 GMT</pubDate>
    </item>
    <item>
      <title>加载保存的深度学习模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78382495/issues-while-loading-saved-deep-learning-model</link>
      <description><![CDATA[在 VS code 中，我有 2 个笔记本，分别为 Final.ipynb 和 main.ipynb。在 Final.ipynb 中，我定义了模型架构，使用 Adam 对其进行编译并加载权重，并将模型保存为 main.keras。在 main.ipynb 中加载 main.keras 时，我收到这样的错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
[20] 中的单元格，第 1 行
----&gt; 1 模型 = keras.models.load_model(&#39;main4.keras&#39;)

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_api.py：176，在load_model（文件路径，custom_objects，编译，安全模式）
    第173章
    第175章
--&gt; [第 176 章]
    177 文件路径，
    第178章
    179 编译=编译，
    180 安全模式=安全模式，
    181）
    182 if str(文件路径).endswith((“.h5”,“.hdf5”)):
    [第 183 章]
    184 文件路径，custom_objects=custom_objects，compile=编译
    185）

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：152，在load_model（文件路径，custom_objects，编译，safe_mode）中
    147 引发值错误（
    148 &#39;无效的文件名：需要 `.keras` 扩展名。 ”
    149 f“已接收：文件路径={文件路径}”
    150）
    151 用 open(filepath, “rb”) 作为 f：
--&gt; [第 152 章]
    153 f、custom_objects、编译、安全模式
    154）

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：207，在_load_model_from_fileobj（fileobj，custom_objects，compile，safe_mode）中
    204 asset_store.close（）
    206如果failed_trackables：
--&gt; 207 _raise_loading_failure（错误消息）
    208回归模型

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：295，在_raise_loading_failure（error_msgs， warn_only）
    293 警告.warn(msg)
    294 其他：
--&gt;第295章

ValueError：总共无法加载 2 个对象。对象  的示例错误消息：

层“lstm_cell”需要 3 个变量，但在加载期间收到 0 个变量。预期：[&#39;kernel&#39;, &#39;recurrent_kernel&#39;, &#39;bias&#39;]

无法加载的对象列表：
[,]

这是我的 Final.ipynb 代码
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv3D(128, 3, input_shape=(75,46,140,​​1), padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(256, 3, padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(75, 3, padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(TimeDistributed(Flatten()))

model.add(双向(LSTM(128, return_sequences=True)))
模型.add(Dropout(.5))

model.add(双向(LSTM(128, return_sequences=True)))
模型.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer=&#39;he_normal&#39;, 激活=&#39;softmax&#39;))

model.compile(优化器=Adam(learning_rate=0.0001), 损失=CTCLoss)
model.load_weights(&#39;model_weights.weights.h5&#39;)
model.save(&#39;main4.keras&#39;)

这是我的 main.ipynb 代码
&lt;前&gt;&lt;代码&gt;@register_keras_serialized()
def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype=“int64”)
    input_length = tf.cast(tf.shape(y_pred)[1], dtype=“int64”)
    label_length = tf.cast(tf.shape(y_true)[1], dtype=“int64”)

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=“int64”)
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=“int64”)

    损失 = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    回波损耗

模型 = keras.models.load_model(&#39;main4.keras&#39;)

我尝试阅读重新启动内核并从头开始运行的文档，删除模型并再次保存它，但对我来说没有任何作用。我希望我的模型能够加载到新笔记本中并给出预测]]></description>
      <guid>https://stackoverflow.com/questions/78382495/issues-while-loading-saved-deep-learning-model</guid>
      <pubDate>Thu, 25 Apr 2024 05:56:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 softmax 回归批量梯度下降 - 卡住</title>
      <link>https://stackoverflow.com/questions/78382301/batch-gradient-descent-using-softmax-regression-stuck</link>
      <description><![CDATA[我已经用 softmax 回归编写了批量梯度下降，但是，结果总是 0 或 6。
我想我错过了一些东西，但不知道在哪里！
将 numpy 导入为 np

def softmax(z):
    总计 = sum(np.exp(x) for x in z)
    返回 np.array([(np.exp(k)/total) for k in z])


def one_hot_encoding(ys):
    如果 len(ys) == 0:
        返回 np.array([])
    k = np.max(ys)
    数组=[]
    对于 y 中的 y：
        行 = np.zeros(k + 1, dtype=int)
        行[y] = 1
        数组.追加（行）
    返回 np.array(array).reshape(len(ys),k+1)
        

def softmax_regression(xs, ys, 学习率, 迭代次数):
   
    tau = one_hot_encoding(ys)
    # 特征数量 = xs.shape[1]，类数量 = tau.shape[1]
    theta = np.zeros((xs.shape[1], tau.shape[1]))
    偏差 = np.zeros(tau.shape[1])

    对于 _ 在范围内（num_iterations）：
        总平均误差 = 0
        总梯度 = 0

        对于范围内的 i(xs.shape[0])：
            z = np.dot(xs[i], theta) + 偏差
            o = softmax(z)
            错误 = o - tau[i]
            Total_mean_error += np.mean(误差)
            总梯度 += (误差 * xs[i])
            
        θ += (学习率 * 总梯度) + (θ * 学习率)
        偏差 += 学习率 * 总平均误差

    打印（θ，偏差）
    def 模型（xs，theta=theta，偏差=偏差）：
        z = (theta.T * xs) + 偏差
        软=softmax(z)
        返回 soft.argmax()
    
    返回模型

            

＃测试

训练数据 = np.array([
    (0.17, 0),
    (0.79, 0),
    (2.66, 2),
    (2.81, 2),
    (1.58, 1),
    (1.86, 1),
    (2.97, 2),
    (2.70, 2),
    (1.64, 1),
    (1.68, 1)
]）

xs = Training_data[:,0].reshape((-1, 1)) # 一个 2D n×1 数组
ys = Training_data[:,1].astype(int) # 长度为 n 的一维数组

h = softmax_regression(xs, ys, 0.05, 750)

测试输入 = [(1.30, 1), (2.25, 2), (0.97, 0), (1.07, 1), (1.51, 1)]
print(f&quot;{&#39;预测&#39;:^10}{&#39;true&#39;:^10}&quot;)
对于 test_inputs 中的 x、y：
    print(f&quot;{h(x):^10}{y:^10}&quot;)
# 预测正确
#1 1
#2 2
# 0 0
#1 1
#1 1

将 theta 和偏差更新从加法切换为减法会导致结果始终为 6，反之亦然。
我已经通读了课程笔记和所有其他文档，但真的看不出它在哪里偏离]]></description>
      <guid>https://stackoverflow.com/questions/78382301/batch-gradient-descent-using-softmax-regression-stuck</guid>
      <pubDate>Thu, 25 Apr 2024 04:48:29 GMT</pubDate>
    </item>
    <item>
      <title>关于用于道路分析的聚类算法的建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78381575/recommendations-on-clustering-algorithms-to-use-for-road-analysis</link>
      <description><![CDATA[我想询问有关使用哪种聚类算法来分析这条道路上任何潜在组/聚类的建议。这些点代表了许多电动汽车快速充电站，但是，我希望以尽可能少的主观性对它们进行分组。我正在使用 GIS 工具，因此，任何有关任何潜在的聚类技术使用的建议都是非常受欢迎的。预先感谢！
我考虑过 k 均值，但我不喜欢由我来决定有多少个簇的想法。这个想法是，聚类应该告诉我应该有多少个聚类。]]></description>
      <guid>https://stackoverflow.com/questions/78381575/recommendations-on-clustering-algorithms-to-use-for-road-analysis</guid>
      <pubDate>Wed, 24 Apr 2024 23:06:38 GMT</pubDate>
    </item>
    <item>
      <title>分类数据的加权 K 模式聚类[关闭]</title>
      <link>https://stackoverflow.com/questions/78380891/weighted-k-modes-clustering-for-categorical-data</link>
      <description><![CDATA[我正在对 5 列分类数据（让每一列称为 A、B、C、D、E）执行聚类，并因此选择了 k 模式。每列中的每个类别根本没有顺序。数据集不平衡，A、B、C 中的某一类占数据的 60-80%。 D 列和 E 列有大量不同类别 (&gt;50)。
我想为 A 列和 B 列赋予优先权重，因此 K-modes 会将这些列视为簇的优先级高于 C D E 列，因此簇是由 A 和 B 的不同值形成的。
我正在使用 K-Modes 库 (python) 并修改了汉明距离度量，以便如果 A 列和 B 列内的 2 个类别不匹配，它们将受到严重惩罚，以努力在周围形成簇这些列。我预计随着惩罚的增加，K 模式将保证至少 A 列和 B 列内的所有类别都至少有一个不同的簇质心。事实并非如此，某些类别没有表示为簇质心。
谁能帮忙解释一下原因：

这没有按预期发生
我将如何修改 K-Modes 库（或解释我将如何修改算法以赋予某些列权重/偏好）
kModes.fit（x，sample_weights）的sample_weights参数有什么作用？我无法使用 sklearn.kmeans 或 kmode.kmode 的文档来理解直觉
]]></description>
      <guid>https://stackoverflow.com/questions/78380891/weighted-k-modes-clustering-for-categorical-data</guid>
      <pubDate>Wed, 24 Apr 2024 19:46:55 GMT</pubDate>
    </item>
    <item>
      <title>是否有人在 Capsnet 中开发或增强挤压方程 [关闭]</title>
      <link>https://stackoverflow.com/questions/78380434/is-anyone-develop-or-enhance-squash-equation-in-capsnet</link>
      <description><![CDATA[def 挤压（向量，轴=-1）：
s_squared_norm = tf.reduce_sum(tf.square(向量), axis, keepdims=True)
尺度 = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())
返回比例 * 向量
CapsNet 中使用的 Standardd squash 函数，我如何增强它以提高准确性。
我尝试用不同的方式进行编辑，但结果并不好]]></description>
      <guid>https://stackoverflow.com/questions/78380434/is-anyone-develop-or-enhance-squash-equation-in-capsnet</guid>
      <pubDate>Wed, 24 Apr 2024 18:07:04 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 Colab 上的 FIT-SNE</title>
      <link>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</link>
      <description><![CDATA[如何在我的 Colab 笔记本上实现基于 FFT 加速插值的 t-SNE (FIt-SNE)？
我试图在狗情绪的kaggle数据集上计算t-SNE ,
我首先尝试获取第一个组件。
&lt;前&gt;&lt;代码&gt;电脑 = 60
pca = 分解.PCA(n_components=pc)
_ = pca.fit(图像)
imgPCA = pca.transform(图像)

tsne = TSNE(n_components=2)
Z = tsne.fit_transform(imgPCA)
绘图嵌入（Z）

然后我尝试了 多核 t-SNE 来提高迭代次数，但我仍然不喜欢
!pip install git+https://github.com/DmitryUlyanov/Multicore-TSNE.git

从 MulticoreTSNE 导入 MulticoreTSNE

Z = MulticoreTSNE(n_jobs=4, n_iter=10000).fit_transform(imgPCA)
图嵌入（Z，show_axis = True）

现在我想尝试使用 FIt-SNE，但我不会知道如何使用它，你能帮我吗？
或者，如果您愿意，也许您可​​以帮助改进之前的代码片段。
有代码可以理解数据集的格式：
导入 pandas 作为 pd
导入CV2

img_size = (192,192,3)
num_px = img_size[0] * img_size[1] * img_size[2]

目录 = &#39;/content/drive/MyDrive/Colab Notebooks/ML/Dog_Emotion/&#39;
图片 = []
标签=[]
labels_df = pd.read_csv(目录 + “labels.csv”)
n_图像 = 0

对于 tqdm 中的图像（labels_df.iloc，desc =“加载图像”，单位=“图像”，总计= 4000）：
  images.append(np.asarray(cv2.resize(cv2.imread(目录 + image[2] + &#39;/&#39; + image[1], cv2.IMREAD_COLOR), img_size[0:2])[:, :, : :-1]))
  labels.append(图像[2])

图像，标签 = np.array(images).reshape(4000, num_px), np.array(labels)

print(f&#39;标签形状：{labels.shape}&#39;)
print(f&#39;图像形状：{images.shape}&#39;)
print(f&#39;图像大小: {img_size}&#39;)

defplot_embedding(Z, show_axis=&quot;False&quot;):
  plt.figure(figsize=(10, 8))
  地图= {标签：i代表i，枚举中的标签（np.unique（标签））}
  color = np.array([map[l] for l in labels])
  plt.scatter(Z[:, 0], Z[:, 1], c = 颜色, cmap = &quot;jet&quot;)
  plt.colorbar()
  plt.title(&#39;2d t-SNE 可视化&#39;)
  如果没有显示轴：
    plt.axis(“关闭”)
  plt.axis(“等于”)
  plt.show()

标签形状：(4000,)
图像形状：(4000, 110592)
图片尺寸：(192, 192, 3)]]></description>
      <guid>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</guid>
      <pubDate>Wed, 24 Apr 2024 17:53:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么批量求和和单次运行计算的杰卡德分数不同？</title>
      <link>https://stackoverflow.com/questions/78378954/why-does-jaccard-score-differ-between-batch-summation-and-single-run-calculation</link>
      <description><![CDATA[我遇到了 jaccard_score 函数 (jaccard_score(ground_true, inference,average=“micro”, Zero_division=0)) 的问题。
我没有足够的内存来存储验证的所有 ground_truth 和推论。因此，我所做的是将每个批次的 Jaccard 分数求和到一个变量，并在验证结束时将该变量除以批次数。通过这种方法，我获得了 0.8084487056909495 的结果。
然而，当我使用容量更大的计算机来处理数据，并一次处理所有ground_truth和掩码时，我得到一个类似0.7716579100568796的值。有人能解释一下为什么会发生这种情况吗？]]></description>
      <guid>https://stackoverflow.com/questions/78378954/why-does-jaccard-score-differ-between-batch-summation-and-single-run-calculation</guid>
      <pubDate>Wed, 24 Apr 2024 13:49:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn 的 QDA 警告我“变量共线”——我该怎么办？</title>
      <link>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</link>
      <description><![CDATA[在训练中运行QuadraticDiscriminationAnalysis (QDA)和.fit，然后调用.predict来预测多项分类I我收到警告：
&lt;块引用&gt;
discriminant_analysis.py:935: UserWarning: 变量共线 warnings.warn(“变量共线”)

此后，程序不会崩溃，但会导致分类效果非常差，通常为 20%，而来自 scikit-learn 的许多其他分类器对同一数据集的准确率为 80% 到 90% ，包括LDA。
从本论坛之前对问题的回答中，我意识到，当它认为 X 矩阵是线性相关的时，即至少其中一个向量可以由其他向量的线性组合生成时，就会发生这样的警告。在这种情况下，算法的矩阵求逆所产生的误差很大 - 我假设这就是分类精度如此差的原因。
但是，我知道 X 矩阵不是线性相关的。所以我假设它失败可能是因为 X 矩阵接近线性相关，并且在这些条件下矩阵的求逆并不精确。
假设这就是问题所在，有没有更好的方法来得到X矩阵求逆，从而得到的误差更小，分类精度更高？
（而且由于 LDA 从相同的数据集（即 X 矩阵）中生成了良好的精度，因此此例程中的矩阵求逆很好 - 那么为什么它在 QDA 中失败？）
我正在使用 scikit-learn 版本 1.3.0、Python 3.8 和  Anaconda 包管理器。更改为 scikit-learn 1.2.0 会生成相同的警告。
我多次尝试运行该程序，但总是得到这个结果。]]></description>
      <guid>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</guid>
      <pubDate>Wed, 24 Apr 2024 13:04:56 GMT</pubDate>
    </item>
    <item>
      <title>在使用 LR finder 代码后，如何获得使用循环学习率方法找到的最佳 LR 的学习率 (LR) 范围（最小值和最大值）？</title>
      <link>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</link>
      <description><![CDATA[我使用以下代码来获取给定神经网络模型的最佳学习率 - https://github.com/beringresearch/lrfinder/blob/master/lrfinder/lrfinder.py - 最终通过 get_best_lr 函数。因此，在获得最佳学习率的值后，如何以编程方式找出使用循环学习率 (CLR) 方法找到的最佳 LR 的 LR 边界（最小值和最大值）值 (https://arxiv.org/abs/1506.01186)?
来自引用的 GitHub 存储库的代码：
导入数学

将 matplotlib.pyplot 导入为 plt
导入tensorflow.keras.backend为K
将 numpy 导入为 np

从tensorflow.keras.callbacks导入LambdaCallback


LRFinder 类：
    ”“”
    训练的循环学习率中详细介绍了学习率范围测试
    神经网络 作者：Leslie N. Smith。学习率范围测试是一个测试
    它提供了有关最佳学习率的有价值的信息。期间
    预训练运行时，学习率线性增加或
    两个边界之间呈指数关系。较低的初始学习率允许
    网络开始收敛，并且随着学习率的增加
    最终会太大并且网络会发散。
    ”“”

    def __init__(自我，模型)：
        self.model = 模型
        自我损失= []
        自我学习率 = []
        self.best_loss = 1e9

    def on_batch_end（自身，批次，日志）：
        lr = K.get_value(self.model.optimizer.lr)
        self.learning_rates.append（lr）

        损失=日志[&#39;损失&#39;]
        self.losses.append(损失)

        如果批次&gt; 5 且 (math.isnan(loss) 或 loss &gt; self.best_loss * 4)：
            self.model.stop_training = True
            返回

        如果损失&lt; self.best_loss：
            self.best_loss = 损失

        lr *= self.lr_mult
        K.set_value(self.model.optimizer.lr, lr)

    def find(自我, 数据集, start_lr, end_lr, epochs=1,
             steps_per_epoch=无，**kw_fit）：
        如果steps_per_epoch为None：
            引发异常（&#39;正确训练数据生成器，&#39;
                            “steps_per_epoch”不能为“None”。”
                            &#39;你可以将其计算为&#39;
                            &#39;`np.ceil(len(TRAINING_LIST) / BATCH)`&#39;)

        self.lr_mult = (浮点(end_lr) /
                        浮动（start_lr））**（浮动（1）/
                                             浮点数（纪元*steps_per_epoch））
        初始权重 = self.model.get_weights()

        Original_lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, start_lr)

        回调 = LambdaCallback(on_batch_end=lambda 批次,
                                  日志：self.on_batch_end（批次，日志））

        self.model.fit（数据集，
                       纪元=纪元，回调=[回调]，**kw_fit）
        self.model.set_weights(initial_weights)

        K.set_value(self.model.optimizer.lr,original_lr)

    def get_learning_rates(自我):
        返回（自我学习率）

    def get_losses(自身):
        返回（自我损失）

    def get_derivatives(self, sma):
        断言 sma &gt;= 1
        导数 = [0] * sma
        对于范围内的 i(sma, len(self.learning_rates))：
            衍生品.append((self.losses[i] - self.losses[i - sma]) / sma)
        回报衍生品

    def get_best_lr（自身，sma，n_skip_beginning = 10，n_skip_end = 5）：
        衍生品 = self.get_derivatives(sma)
        best_der_idx = np.argmin(导数[n_skip_beginning:-n_skip_end])
        返回 self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:17 GMT</pubDate>
    </item>
    <item>
      <title>如何防止 Keras 在训练期间计算指标</title>
      <link>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</link>
      <description><![CDATA[我正在使用 Tensorflow/Keras 2.4.1，并且我有一个（无监督的）自定义指标，它将我的多个模型输入作为参数，例如：
model = build_model() # 返回一个 tf.keras.Model 对象
my_metric = custom_metric(model.output, model.input[0], model.input[1])
模型.add_metric(my_metric)
[...]
model.fit([...]) # 使用 fit 进行训练

但是，custom_metric 非常昂贵，因此我希望仅在验证期间计算它。我找到了这个答案，但我几乎不明白如何使解决方案适应我的指标，该指标使用多个模型输入作为参数，因为update_state 方法似乎不太灵活。
在我的上下文中，除了编写自己的训练循环之外，是否有办法避免在训练期间计算我的指标？
另外，我很惊讶我们无法本机指定 Tensorflow 某些指标只能在验证时计算，这有什么原因吗？
此外，由于模型经过训练来优化损失，并且训练数据集不应用于评估模型，我什至不明白为什么默认情况下 Tensorflow 在训练期间计算指标。]]></description>
      <guid>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</guid>
      <pubDate>Wed, 09 Mar 2022 16:11:26 GMT</pubDate>
    </item>
    <item>
      <title>TimeDistributed 层在 Keras 中的作用是什么？</title>
      <link>https://stackoverflow.com/questions/47305618/what-is-the-role-of-timedistributed-layer-in-keras</link>
      <description><![CDATA[我试图了解 TimeDistributed 包装器在 Keras 中的作用。
我知道 TimeDistributed“将一个层应用于输入的每个时间切片。”
但是我做了一些实验并得到了我无法理解的结果。
简而言之，对于 LSTM 层，TimeDistributed 和 Dense 层具有相同的结果。

&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add(TimeDistributed(密集(1)))
打印（模型.output_shape）

模型=顺序（）
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add((密集(1)))
打印（模型.output_shape）

对于这两个模型，我得到的输出形状为(None, 10, 1)。
谁能解释一下 RNN 层之后的 TimeDistributed 层和 Dense 层之间的区别吗？]]></description>
      <guid>https://stackoverflow.com/questions/47305618/what-is-the-role-of-timedistributed-layer-in-keras</guid>
      <pubDate>Wed, 15 Nov 2017 10:57:45 GMT</pubDate>
    </item>
    </channel>
</rss>