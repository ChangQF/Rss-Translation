<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Jan 2024 12:25:41 GMT</lastBuildDate>
    <item>
      <title>IOPub 数据速率超过 Jupyter Notebook</title>
      <link>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</link>
      <description><![CDATA[IOPub 数据速率超出。
Jupyter服务器将暂时停止发送输出
给客户端以避免崩溃。
要更改此限制，请设置配置变量
--ServerApp.iopub_data_rate_limit。
当前值：
ServerApp.iopub_data_rate_limit=1000000.0（字节/秒）
ServerApp.rate_limit_window=3.0（秒）
我尝试更新整个 jupyter 笔记本，但仍然没有解决问题，我也尝试根据我的数据集更新值，但仍然显示相同的错误]]></description>
      <guid>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</guid>
      <pubDate>Fri, 05 Jan 2024 10:04:06 GMT</pubDate>
    </item>
    <item>
      <title>实时跟踪脚本中 BYTETracker 初始化的问题</title>
      <link>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</link>
      <description><![CDATA[尝试使用 ByteTrack 库初始化实时跟踪脚本中的 BYTETracker 时，我遇到了 TypeError。该错误具体发生在 BYTETracker 类的 __init__ 方法中。
这是我的代码的相关部分：
跟踪器 = [BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument)]

我遇到的错误消息是：
TypeError：+ 不支持的操作数类型：“type”和“float”

我尝试通过创建一个 ByteTrackArgument 实例来解决这个问题，如下所示：
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]

但是，问题仍然存在。值得注意的是，我在脚本中使用 OpenCV 中的 FaceDetectorYN 进行人脸检测。
对于可能导致此错误的原因（尤其是与使用 FaceDetectorYN 结合使用）以及如何解决该错误有任何见解吗？
其他上下文：

我正在使用 ByteTrack 库进行实时跟踪。 链接
错误发生在 BYTETracker 类的 __init__ 方法中。
我将 ByteTrackArgument 的实例传递给 BYTETracker 构造函数。
使用 OpenCV 中的 FaceDetectorYN 执行人脸检测。 链接

这是我的完整代码：
&lt;前&gt;&lt;代码&gt;导入cv2
从 bytetracker 导入 BYTETracker
将 numpy 导入为 np

print(&quot;OpenCV 版本&quot;, cv2.__version__)

类 ByteTrackArgument：
    轨迹阈值 = 0.5
    轨道缓冲区 = 50
    匹配阈值 = 0.8
    纵横比阈值 = 10.0
    最小框面积 = 1.0
    mot20 = 假

MIN_THRESHOLD = 0.5 # 根据需要调整此阈值

# 初始化 ByteTrackArgument
byte_track_argument = ByteTrackArgument()

# 初始化BYTETracker
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]
def start_webcam_tracking():
    cap = cv2.VideoCapture(0) # 使用 0 作为默认网络摄像头，或提供网络摄像头 URL

    如果不是 cap.isOpened():
        print(“错误：无法打开相机。”)
        返回

    而真实：
        ret, 框架 = cap.read()
        如果不转：
            print(“错误：无法从相机读取帧。”)
            休息

        # 人脸检测代码
        检测器 = cv2.FaceDetectorYN.create(r&quot;C:\Users\gratu\live tracker\face_detection_yunet_2023mar.onnx&quot;, &quot;&quot;, (2200, 1200), Score_threshold=MIN_THRESHOLD)
        img_W = int(frame.shape[1])
        img_H = int(frame.shape[0])
        detector.setInputSize((img_W, img_H))

        检测= detector.detect(frame)[1]

        如果检测不是无：
            用于检测中的检测：
                x, y, 宽度, 高度 = 地图(int, 检测[:4])
                cv2.矩形(框架, (x, y), (x + 宽度, y + 高度), (0, 255, 0), 2)

                # 使用面部边界框更新跟踪器
                tracker.update(np.array([[x, y, x + 宽度, y + 高度]]), [frame.shape[0],frame.shape[1]])

        # 从 BYTETracker 获取跟踪结果
        online_targets = tracker.get_online_targets()

        如果 online_targets 不是 None：
            对于 online_targets 中的目标：
                x, y, x2, y2 = target # 根据BYTETracker的输出格式修改这部分
                cv2.矩形(框架, (x, y), (x2, y2), (255, 0, 0), 2)

        cv2.imshow(&#39;具有人脸检测和跟踪功能的网络摄像头&#39;, frame)

        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

    cap.release()
    cv2.destroyAllWindows()

如果 __name__ == “__main__”：
    start_webcam_tracking()

这是我想要复制的教程，他们使用 yolox 进行人物检测，我尝试对人脸检测做同样的事情
链接
任何帮助或建议将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</guid>
      <pubDate>Fri, 05 Jan 2024 09:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 正则化 Alpha - 权重还是叶子？</title>
      <link>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</link>
      <description><![CDATA[我已经阅读了有关 XGBoost 和 LightGBM 的论文以及它们的大部分文档，但找不到明确的声明表明除了 GOSS &amp; EFB为了更快的学习，基本算法与XGBoost相同。具体来说，XGBoost 目标函数中唯一的 L1 式正则化是 gamma*T，其中 gamma 是超参数，T 是叶子数量。 LightGBM 有 reg_alpha 参数，根据他们的文档，该参数应用 L1 正则化，该参数是否会惩罚叶子的数量，或者，在更传统的意义上，惩罚每个叶子的贡献的绝对值？
附注参考回归案例，我从未使用过该模型进行分类，因此不知道哪些部分仍然有效。]]></description>
      <guid>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</guid>
      <pubDate>Fri, 05 Jan 2024 08:25:18 GMT</pubDate>
    </item>
    <item>
      <title>我的 tfidf 向量自动编码器对于不同的文本输入产生相同的输出</title>
      <link>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</link>
      <description><![CDATA[我一直在尝试为我的降维任务实现一个自动编码器，以输入到最近邻居模型中，我意识到所有邻居最终彼此之间的距离为零，我意识到问题源于自动编码器，这对于不同的文本产生了类似的输出。
我已经尝试更改层数、正则化器以及输出层的激活（我使用线性，因为这应该是文本数据的最佳选择）。
我的 desc 和 req 数据帧的形状分别为 (31080, 471494) (31080, 169214)，大部分是稀疏数据，这是我在初始原始文本数据上使用 tfidf 获得的。
类 SparseDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, X, 批量大小):
        自我.X = X
        self.batch_size = 批量大小
        self.n_samples = X.shape[0]

    def __len__(自身):
        返回 int(np.ceil(self.n_samples / self.batch_size))

    def __getitem__(self, idx):
        开始 = idx * self.batch_size
        结束=分钟（开始+ self.batch_size，self.n_samples）
        batch_X = self.X[开始:结束].toarray()
        return batch_X, batch_X # 自动编码器获取与输入和目标相同的数据


组合_vecs_sparse = hstack([df_desc.sparse.to_coo(), df_req.sparse.to_coo()])


# 定义提前停止回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 3，restore_best_weights = True）

# 分割数据
X_train, X_val = train_test_split(combined_vecs_sparse, test_size=0.2, random_state=42)
X_train = X_train.tocsr()
X_val = X_val.tocsr()
before_memory = psutil.Process().memory_info().rss
input_dim = X_train.shape[1] # 特征数量

# 定义自动编码器结构
input_layer = 输入（形状=（input_dim，））

编码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（input_layer）
编码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
编码 = Dense(32,activation=&#39;relu&#39;,kernel_regularizer=l2(0.001))(encoded) # 编码表示
解码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
解码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（解码）
解码=密集（input_dim，激活=&#39;sigmoid&#39;）（解码）

自动编码器=模型（输入层，解码）

# 编译并训练自动编码器
autoencoder.compile（优化器=&#39;adam&#39;，损失=&#39;mse&#39;）


非常感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</guid>
      <pubDate>Fri, 05 Jan 2024 06:41:35 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默氏症 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。
该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每张图像预测相同的类别（“MCI - 轻度认知障碍”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
使用不同的数据集训练模型两次，但问题仍然存在。我现在已经走进了死胡同，不知道如何解决它。
任何帮助将不胜感激。
TFLite 模型代码：
https://colab.research.google.com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=分享
FlaskAPI 代码：
` from Flask import Flask, request, jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io
app = Flask(__name__)

解释器 = tf.lite.Interpreter(model_path=“latest_model.tflite”)
解释器.allocate_tensors()

class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
    图像 = cv2.resize(图像, (150, 150))
    图像 = image.astype(&#39;float32&#39;) / 255.0
    图像 = np.expand_dims(图像, 轴=0)
    返回图像


@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    尝试：
        文件 = request.files[&#39;文件&#39;]
        image_file = Image.open(io.BytesIO(file.read()))
        图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
        如果不是（image.shape[0] &gt;= 150 且 image.shape[1] &gt;= 150 且 image.shape[2] == 3）：
        return jsonify({&quot;error&quot;: &quot;无效的图像形状&quot;})


        图像 = image.astype(&#39;float32&#39;) / 255.0

        预处理图像 = 预处理图像（图像）

        terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)
        解释器.invoke()

        output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])
        Predicted_class_index = np.argmax(output_tensor, axis=1)[0]
        预测类名称 = 类名称[预测类索引]

        结果 = {“预测”：预测类名称，“输出张量”：output_tensor.tolist()}
        返回 jsonify(结果)
    除了异常 e：
       返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

`
尝试记录输出张量，但这是我从 Flask API 获得的输出。知道输出张量表明偏向于 MCI 类，但为什么它在 colab 环境中完美运行，而不是在 Flask API 中运行良好（如果是这样的话）？
此外，您是否推荐我可以使用任何其他更好的方法来代替使用 Flask API 将模型与我的 React Native 应用程序集成？
` {
&lt;前&gt;&lt;代码&gt;“输出张量”：[

[

0.0004518234636634588,

0.0004140451201237738,

0.002781340153887868,

0.7277416586875916,

0.2686111330986023

]

],

“预测”：“MCI-轻度认知障碍”

}`
]]></description>
      <guid>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Fri, 05 Jan 2024 05:43:52 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率的 Transformer 模型：一个自定义 LR 调度程序，它复制在“注意是”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在大约 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到机器学习项目的 CSV 数据库？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77762128/where-to-find-csv-databases-for-machine-learning-projects</link>
      <description><![CDATA[我想创建一个项目，旨在根据时间序列数据集来预测健康食品或类似食品的消费趋势。我考虑在 Euromonitor 网站上搜索数据集，并找到了很多有关该主题的信息（下面提供了其中一个链接），但我找不到，也不知道是否可以使用 下载任何文件例如，CSV 数据库。有谁知道我在哪里以及如何下载涵盖这个食物主题的数据库，以便我可以构建一个机器学习项目，例如考虑时间序列？我尝试在 Kaggle 上搜索，但没有找到满意的数据库。
主题示例：
https://www.euromonitor.com /文章/正念饮食和新食物信念
https://www.euromonitor.com/article/consumers-想要更健康的包装食品]]></description>
      <guid>https://stackoverflow.com/questions/77762128/where-to-find-csv-databases-for-machine-learning-projects</guid>
      <pubDate>Fri, 05 Jan 2024 01:57:36 GMT</pubDate>
    </item>
    <item>
      <title>多元数据排序算法设计[关闭]</title>
      <link>https://stackoverflow.com/questions/77756234/multivariate-data-ranking-algorithm-design</link>
      <description><![CDATA[目前我已经收集了很多学校的相关数据。每个学校都有教师人数、学生人数、硬件设施等十个评价指标，这些指标的程度是用数字来表示的，比如教师数7.5，学生数10.5，我没有了解各个指标的权重。现在我们想用机器学习算法对高校的综合实力进行自动排名，而不参考任何国际知名排名，比如QS。有什么算法可以让计算机根据高校的综合实力自动排名基于多维度指标的实力？]]></description>
      <guid>https://stackoverflow.com/questions/77756234/multivariate-data-ranking-algorithm-design</guid>
      <pubDate>Thu, 04 Jan 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>当我在 jupyter 笔记本上运行简单的 cnn 模型时，CPU 使用率较低</title>
      <link>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</link>
      <description><![CDATA[我在 Jupyter 笔记本上运行了一个非常简单的 CNN 模型，但过程非常慢。我在我的旧笔记本电脑（核心 i7U 10gen）上运行相同的程序。只花了一分半钟，但在我的新笔记本电脑（酷睿 i9 13900hx 和 rtx4060）上花了 30 分钟！它们都是在 CPU 上运行的，但在我的旧电脑上，CPU 使用率为 100%，在我的新电脑上，大约为 20%。然后，我在 PyCharm 中运行相同的程序，一切正常！这让我很困惑，我尝试了很多方法但都不起作用。我想知道真正的问题出在哪里？我的 Jupyter 笔记本还是其他东西？
我尝试在不同的 PC、不同的 IDE 平台上运行相同的程序。我想知道真正的问题出在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</guid>
      <pubDate>Fri, 29 Dec 2023 07:23:45 GMT</pubDate>
    </item>
    <item>
      <title>具有 1D 数据和 Ghost Dimension 的 TF.MultiHeadAttention</title>
      <link>https://stackoverflow.com/questions/76520092/tf-multiheadattention-with-1d-data-and-ghost-dimension</link>
      <description><![CDATA[背景：
我的数据的形状为 (batch_size, data_length) 并且尺寸似乎与内部 MultiHeadAttention 操作不兼容，尤其是 softmax。有人善意地建议我应该使用尺寸为 1 的幽灵维度作为最后一个维度。
我收到的错误消息：
&lt;前&gt;&lt;代码&gt;(32, 512, 1)
纪元 1/200

-------------------------------------------------- ------------------------

ValueError Traceback（最近一次调用最后一次）

&lt;ipython-input-7-870abeaa4b93&gt;在&lt;细胞系：281&gt;()
    279 model.compile（损失=“均方误差”，优化器=“rmsprop”，指标=[“准确性”]）
    280
--&gt; [第 281 章]

1 帧

tf__train_function（迭代器）中的/usr/local/lib/python3.10/dist-packages/keras/engine/training.py
     13 尝试：
     14 do_return =真
---&gt; 15 retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(迭代器)), 无, fscope)
     16 除外：
     17 do_return = 假

ValueError：在用户代码中：

    文件“/usr/local/lib/python3.10/dist-packages/keras/engine/training.py”，第 1284 行，train_function *
        返回step_function（自身，迭代器）
    文件“/usr/local/lib/python3.10/dist-packages/keras/engine/training.py”，第 1268 行，在 step_function **
        输出 = model.distribute_strategy.run(run_step, args=(data,))
    文件“/usr/local/lib/python3.10/dist-packages/keras/engine/training.py”，第 1249 行，在 run_step **
        输出 = model.train_step(数据)
    文件“/usr/local/lib/python3.10/dist-packages/keras/engine/training.py”，第 1050 行，在 train_step 中
        y_pred = self(x, 训练=True)
    文件“/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py”，第 70 行，在 error_handler 中
        从 None 引发 e.with_traceback(filtered_tb)

    ValueError：调用层“查询”（类型 EinsumDense）时遇到异常。
    
    维度必须相等，但对于 &#39;{{node model/multi_head_attention/query/einsum/Einsum}} = Einsum[N=2, T=DT_HALF,equation=“abc,cde-&gt;abde”] 为 1 和 512 (model/Cast, model/multi_head_attention/query/einsum/Einsum/Cast)&#39;，输入形状：[32,512,1]，[512,2,512]。
    
    调用层“query”接收的参数（类型 EinsumDense）：
      • 输入=tf.Tensor(形状=(32, 512, 1), dtype=float16)
]]></description>
      <guid>https://stackoverflow.com/questions/76520092/tf-multiheadattention-with-1d-data-and-ghost-dimension</guid>
      <pubDate>Wed, 21 Jun 2023 05:04:04 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 自定义学习率调度程序给出意外的 EagerTensor 类型错误</title>
      <link>https://stackoverflow.com/questions/76511182/tensorflow-custom-learning-rate-scheduler-gives-unexpected-eagertensor-type-erro</link>
      <description><![CDATA[下面是我的自定义LR调度程序，它是tensorflow.keras.optimizers.schedules.LearningRateSchedule的子类，出现错误TypeError：无法将-0.5转换为dtype int64的EagerTensor。真的很困惑为什么 Eagertensor 与此自定义类的返回调用的简单平方反比计算相关。
class lr_schedule(tensorflow.keras.optimizers.schedules.LearningRateSchedule)：
    def __init__(自身、dim_embed、warmup_steps):
        self.dim_embed = dim_embed
        self.warmup_steps = 预热步骤
    def __call__(自我，步骤)：
        返回（self.dim_embed ** -0.5）* min（（步骤** -0.5），步骤*（self.warmup_steps ** -1.5））

与此错误没有特别相关，但这是一个自定义 LR 调度程序，它复制“Attention is All You Need”论文中使用的预热调度程序。]]></description>
      <guid>https://stackoverflow.com/questions/76511182/tensorflow-custom-learning-rate-scheduler-gives-unexpected-eagertensor-type-erro</guid>
      <pubDate>Tue, 20 Jun 2023 03:08:43 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 KerasTensor 传递给 TF API 时出现的错误？</title>
      <link>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</guid>
      <pubDate>Sat, 09 Apr 2022 13:03:40 GMT</pubDate>
    </item>
    <item>
      <title>NEAT 的类型错误</title>
      <link>https://stackoverflow.com/questions/68248850/typeerror-with-neat</link>
      <description><![CDATA[我在尝试制作蛇 AI 时遇到 NEAT 类型错误：
node_inputs.append(self.values[i] * w)
类型错误：无法将序列乘以“float”类型的非 int

代码
类 SnakeGame(对象):
    def __init__(自身、基因组、配置):
    self.genome = 基因组
        self.nets = []

        对于 id，self.genomes 中的 g：
            净=整洁.nn.FeedForwardNetwork.create（g，配置）
            self.nets.append(net)
            g.适应度 = 0
 

在同一个类的另一个函数中的代码
def 游戏（自身）：
    而真实：
        对于 pg.event.get() 中的事件：
            如果 event.type == pg.QUIT：
                pg.quit()
        数据 = self.nets[0].activate(self.getData())
        输出 = data.index(max(data))

函数 getData 是什么样的
def getData(self):
    数据 = [self.x_position, self.y_position, self.food_x, self.food_y, self.snakeLength]
    返回数据

config-feedforward.txt 的部分代码
&lt;前&gt;&lt;代码&gt;[整洁]
健身标准 = 最大值
健身阈值 = 1000
弹出大小 = 2
灭绝时重置=真
]]></description>
      <guid>https://stackoverflow.com/questions/68248850/typeerror-with-neat</guid>
      <pubDate>Sun, 04 Jul 2021 21:19:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 BigQuery 中使用外部回归器训练 Arima_PLUS 模型？</title>
      <link>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</link>
      <description><![CDATA[我在大查询上创建了一个模型，是否可以包含额外的列作为外部回归量？
例如，我想包含日期、用户、每个会话的页面、跳出率等以预测用户。
创建或替换模型 bqml_tutorial.create_model
选项
(model_type=&#39;ARIMA_PLUS&#39;,
time_series_timestamp_col=&#39;日期&#39;,
time_series_data_col=&#39;用户&#39;,
auto_arima=真，
数据频率 = &#39;自动频率&#39;,
decompose_time_series=真）
作为
从“bqml_tutorial.cvrate”中选择日期、简历作为用户 ORDER BY Date
]]></description>
      <guid>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</guid>
      <pubDate>Thu, 20 May 2021 16:07:01 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用 WeightedRandomSampler</title>
      <link>https://stackoverflow.com/questions/60812032/using-weightedrandomsampler-in-pytorch</link>
      <description><![CDATA[我需要在 PyTorch 中实现多标签图像分类模型。但是我的数据不平衡，因此我使用 PyTorch 中的 WeightedRandomSampler 来创建自定义数据加载器。但是当我迭代自定义数据加载器时，出现错误：IndexError：列表索引超出范围 
使用此链接实现了以下代码：https://discuss.pytorch.org/t/balanced-sampling- Between-classes-with-torchvision-dataloader/2703/3?u=surajsubramanian
def make_weights_for_balanced_classes(images, nclasses):
    计数 = [0] * n 类
    对于图像中的项目：
        计数[项目[1]] += 1
    每个类别的权重 = [0.] * n 个类别
    N = 浮点（总和（计数））
    对于范围内的 i (nclasses)：
        每类权重[i] = N/float(count[i])
    重量 = [0] * len(图像)
    对于 idx，枚举（图像）中的 val：
        权重[idx] =weight_per_class[val[1]]
    返回重量

权重 = make_weights_for_balanced_classes(train_dataset.imgs, len(full_dataset.classes))
权重 = torch.DoubleTensor(权重)
采样器 = WeightedRandomSampler(权重, len(权重))

train_loader = DataLoader（train_dataset，batch_size = 4，采样器=采样器，pin_memory = True）

根据https://stackoverflow.com/a/60813495/10077354中的答案，以下是我的更新的代码。但是当我创建数据加载器时：loader = DataLoader(full_dataset, batch_size=4, Sampler=sampler)，len(loader) 返回 1。
class_counts = [1691, 743, 2278, 1271]
num_samples = np.sum(class_counts)
labels = [_ 的标签，full_dataset.imgs 中的标签]

class_weights = [num_samples/class_counts[i] for i in range(len(class_counts)]
权重 = [class_weights[labels[i]] for i in range(num_samples)]
采样器 = WeightedRandomSampler(torch.DoubleTensor(权重), num_samples)

提前非常感谢！
我根据下面接受的答案添加了一个实用函数：
def Sampler_（数据集）：
    dataset_counts = imageCount(数据集)
    num_samples = sum(数据集计数)
    labels = [_的标签，数据集中的标签]

    class_weights = [num_samples/dataset_counts[i] for i in range(n_classes)]
    权重 = [class_weights[labels[i]] for i in range(num_samples)]
    采样器 = WeightedRandomSampler(torch.DoubleTensor(权重), int(num_samples))
    返回采样器

imageCount 函数查找数据集中每个类别的图像数量。数据集中的每一行都包含图像和类，因此我们考虑元组中的第二个元素。
def imageCount(数据集):
    image_count = [0]*(n_classes)
    对于数据集中的 img：
        图像计数[img[1]] += 1
    返回图像数量
]]></description>
      <guid>https://stackoverflow.com/questions/60812032/using-weightedrandomsampler-in-pytorch</guid>
      <pubDate>Mon, 23 Mar 2020 10:45:53 GMT</pubDate>
    </item>
    </channel>
</rss>