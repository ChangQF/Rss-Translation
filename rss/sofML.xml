<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Feb 2025 15:18:14 GMT</lastBuildDate>
    <item>
      <title>Tensorflow 图像分类过度拟合问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</link>
      <description><![CDATA[我正在尝试使用 tensorflow keras 最新 api 和函数创建一个图像分类模型。我的模型通过查看复杂的特征和设计（例如微缩印刷、全息图、透明图案等）将纸币分为真币和假币。我有一个包含大约 300-400 张高质量图像的小型数据集。无论我做什么，我的模型都会过度拟合。它的训练准确率高达 1.000，训练损失高达 0.012。但验证准确率保持在 0.60-0.75 之间，验证损失保持在 0.40-0.53 之间。
我尝试了以下方法：

增加数据集。 （但我知道这不会有太大帮助，因为钞票差别不大。它们都非常相似。所以它不会有助于推广模型）
使用 drop-out、l1/l2 正则化
使用迁移学习。我使用了 ResNet50 模型。我首先通过冻结基础模型训练了几个时期，然后解冻模型并重新训练了更多时期。
使用类权重来平衡权重。
使用计划学习率在训练过程中进行修改。
使用提前停止和回调等。
尝试使用预处理

此外，如果我在其中使用规范化层，我的模型性能会更差，而没有它，它的性能会更好。所以我排除了该层。
但是，没有什么能帮助我提高泛化能力。我不知道我错过了什么。
我的模型：

data_augmentation = tf.keras.Sequential([
tf.keras.layers.RandomRotation(0.1),
tf.keras.layers.RandomZoom(0.1),
tf.keras.layers.RandomBrightness(0.1),
tf.keras.layers.RandomContrast(0.1),
])

train_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

train_ds = (
train_ds
.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)
.cache()
.shuffle(1000)
.prefetch(buffer_size=AUTOTUNE)
)

base_model = tf.keras.applications.ResNet50(

input_shape=(img_height, img_width, 3),
include_top=False,
weights=&#39;imagenet&#39;
)

# 取消冻结特定层微调

base_model.trainable = True
for layer in base_model.layers[:-10]: # 保持第一层冻结
layer.trainable = False

l2_lambda=0.0001

#model
model = tf.keras.Sequential([
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense
(512,activation=&#39;relu&#39;,kernel_regularizer=regularizers.l2(l2_lambda)),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(256,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(1,activation = &quot;sigmoid&quot;)
])
]]></description>
      <guid>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</guid>
      <pubDate>Thu, 06 Feb 2025 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>NameError: 尝试运行 def __init__(self, width, height, inter=cv2.INTER_AREA) 时未定义名称“cv2”：[关闭]</title>
      <link>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</link>
      <description><![CDATA[我尝试使用 cv2 编写一些神经网络代码，但出现错误
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError：名称“cv2”未定义

knn.py --dataset ./datasets/animals
回溯（最近一次调用）：
文件“/Users/test/Desktop/CODE/knn.py”，第 6 行，位于&lt;module&gt;
来自 pyimagesearch.preprocessing 导入 SimplePreprocessor
文件“/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py”，第 4 行，位于&lt;module&gt;
类 SimplePreprocessor:
文件 &quot;/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py&quot;，第 5 行，在 SimplePreprocessor 中
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError: 名称 &#39;cv2&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</guid>
      <pubDate>Thu, 06 Feb 2025 08:53:50 GMT</pubDate>
    </item>
    <item>
      <title>XGboost 在不同的标记器上具有不同的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</link>
      <description><![CDATA[我有一个transformer bodomerka/Mil_class_exp_sber_balanssedclass，我在sberbank-ai/ruBert-base的基础上对其进行了训练。额外训练的本质是，该模型可以对用俄语写的文本进行分类，并分类是否是军事经验（0或1）。
而且我还想训练Xgboost模型。我用transformer中的tokenizer对文本进行了token化。
最初，当我使用sberbank-ai/ruBert-base tokenizer时，准确率为0.86。但是，当我将其更改为bodomerka/Mil_class_exp_sber_balanssedclass时，准确率上升到了0.96。这是为什么呢？]]></description>
      <guid>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</guid>
      <pubDate>Wed, 05 Feb 2025 20:12:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习数据集预处理问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</link>
      <description><![CDATA[我有这个数据集，它是一个多类分类问题，y_train 高度不平衡。我想对此应用 smote，但它会抛出 NaN 和无穷大值错误。我尝试了 smote-variants 库下可用的所有 smote 技术，但没有成功
我计划在基于 IoT 的生产环境中部署它，所以我不想有任何偏差
任何建议或潜在解决方案都很好
这是标签编码后 y_train 的值计数
当然！以下是相同格式的标签分布，但采用 markdown 格式：
平衡和编码后的标签分布：
4 617
12 432
11 391
6 357
10 353
7 336
19 299
9 290
18 235
17 180
20 86
0 77
22 72
21 63
5 44
27 31
23 30
2 23
13 22
15 13
24 9
25 5
16 4
3 3
8 3
26 2
28 1
1 1
14 1
名称：count，dtype：int64

这是使用 mix-max 缩放数据集后的结果，我已估算缺失值值。
在应用任何 smote 之前没有 NaN 或 Infinity 值，在应用时，它会抛出此错误。
我添加了例外以删除 NaN 和无限值，但现在 smote 技术不起作用，任何可能起作用的技术建议
或者我应该只创建较小值数据的副本
例如这里的 14 只是 1，我会多次复制它，那么它会更正确，我不知道这是否是一种正确的方法]]></description>
      <guid>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</guid>
      <pubDate>Wed, 05 Feb 2025 19:00:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow Lite 将 ML 模型实现到 Android 应用中</title>
      <link>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</guid>
      <pubDate>Wed, 05 Feb 2025 16:29:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Java 从 Android 中的扫描文档中检测实心圆圈（单选按钮）？</title>
      <link>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</link>
      <description><![CDATA[我正在开发一款使用 GmsDocumentScanner 扫描纸质文档的 Android 应用。我的目标是检测哪些圆圈被填充，类似于表单上的单选按钮。
圆圈预先印在纸上，用户填充其中一个圆圈来标记他们的选择。扫描文档后，我需要检测标记的圆圈以及与之相关的文本或图标。为了便于理解，我在这里分享了一个片段。
此片段清晰地显示了扫描的纸张，应进一步处理以检测圆圈中的颜色标记。
我尝试了这些步骤

使用 GmsDocumentScanning 扫描文档
尝试使用像素强度分析检测填充的圆圈

private void understandText(Bitmap bitmap) {
InputImage image = InputImage.fromBitmap(bitmap, 0);
TextRecognizer understander = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS);

识别器.处理（图像）
.addOnSuccessListener（结果 -&gt; {
for（Text.TextBlock block : result.getTextBlocks()) {
detectRadioButtons（block, bitmap);
}
})
.addOnFailureListener（e -&gt; Log.e（&quot;错误&quot;, &quot;文本识别失败&quot;, e));
}


private void detectRadioButtons（Text.TextBlock textBlock, Bitmap bitmap）{
for（Text.Line line : textBlock.getLines()) {
Rect boundingBox = line.getBoundingBox();
if (isRadioButtonChecked（boundingBox, bitmap)) {
Log.i（&quot;检测&quot;, &quot;选中的单选按钮：&quot; + line.getText());
}
}
}


private boolean isRadioButtonChecked(Rect rect, Bitmap bitmap) {
int checkedPixelThreshold = 100;
int checkedPixels = 0;
int totalPixels = rect.width() * rect.height();

for (int x = rect.left; x &lt; rect.right; x++) {
for (int y = rect.top; y &lt; rect.bottom; y++) {
int pixel = bitmap.getPixel(x, y);
if (Color.red(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.green(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.blue(pixel) &lt; checkedPixelThreshold) {
checkedPixels++;
}
}
}
return checkedPixels &gt; (totalPixels * 0.5); // 超过 50% 的像素已填充
}


预期结果是
检测扫描纸上的圆圈
识别哪个圆圈已填充
将填充的圆圈与正确的文本标签或图标关联]]></description>
      <guid>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</guid>
      <pubDate>Wed, 05 Feb 2025 12:44:42 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 Keras、Tensorflow 实现重现性？</title>
      <link>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</link>
      <description><![CDATA[每次运行以下代码时，我获得的准确率和损失都不一样。我按照之前帖子中的说明操作，但无法解决。问题可能出在哪里？
import os
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = &quot;1&quot;
os.environ[&quot;TF_CUDNN_DETERMINISTIC&quot;] = &quot;1&quot;
...
...

SEED=65
tf.keras.utils.set_random_seed(SEED) 
tf.config.experimental.enable_op_determinism()
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

训练，测试，训练目标，测试目标 = train_test_split((df.loc[:,&quot;input_alarm_1&quot;:&quot;input_alarm_&quot;+str(num_backtracking_events)]), df.loc[:,&quot;output_failure&quot;], test_size=0.25, random_state=SEED)

训练 = np.asarray(training).astype(&#39;float32&#39;)
训练目标 = np.asarray(trainingtarget).astype(&#39;float32&#39;)
test = np.asarray(test).astype(&#39;float32&#39;)
testtarget = np.asarray(testtarget).astype(&#39;float32&#39;)

initializer = tf.keras.initializers.GlorotUniform(seed=SEED)

model = keras.Sequential(
[
layer.Dense(600, 激活=&quot;relu&quot;, input_shape=(num_backtracking_events,), kernel_initializer=initializer),
layer.Dense(300, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(100, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(1, 激活=&quot;sigmoid&quot;, kernel_initializer=initializer),
#dropout?
]
)

#编译模型
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

#fit
history = model.fit(training, trainingtarget, batch_size=10, epochs=50, validation_split=0.1)

# 评估 keras 模型
test_loss, test_acc = model.evaluate(test, testtarget)
print(&#39;Accuracy: %.2f&#39; % (test_acc*100))
print(&#39;Loss: %.2f&#39; % (test_loss*100))
]]></description>
      <guid>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</guid>
      <pubDate>Wed, 05 Feb 2025 08:09:02 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型在二元分类中仅预测一个类（猫）[关闭]</title>
      <link>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</link>
      <description><![CDATA[我使用 TensorFlow/Keras 训练了一个二元分类 CNN，以区分猫和狗。然而，在测试数据集上进行评估时，该模型只为每张图片预测“猫”，尽管数据集包含这两个类别。
这是我的代码：
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import load_model

def normalizer(image, label):
aux = tf.cast(image, dtype=tf.float32)
image_norm = aux/255.0
return image_norm, label

train_data, valid_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/training&#39;,
validation_split=0.1, 
subset=&quot;both&quot;, 
seed=42, 
image_size=(150, 150), 
batch_size=32 
)

test_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/test&#39;, 
image_size=(150, 150), 
batch_size=32 
)

train = train_data.map(normalizer)
valid = valid_data.map(normalizer) 
test = test_data.map(normalizer)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),activation=&#39;relu&#39;, input_shape=(150,150,3)))
model.add(MaxPooling2D())

model.add(Conv2D(filters=64, kernel_size=(3,3),激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Flatten())

model.add(Dense(units=256, 激活=&#39;relu&#39;))
model.add(Dense(units=1, 激活=&#39;sigmoid&#39;))

model.compile(
optimizer=&#39;adam&#39;,
loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[&#39;accuracy&#39;],
)

print(model.summary())

hist = model.fit(
训练，
batch_size=32， 
epochs=20， 
shuffle=True，
validation_data=valid
)

plt.plot(hist.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_loss&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.title(&#39;训练和验证中的损失&#39;)
plt.show()

如果 hist.history 中有 &#39;accuracy&#39;: 
plt.plot(hist.history[&#39;accuracy&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_accuracy&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.legend()
plt.title(&#39;训练和验证的准确性&#39;)
plt.show()

model.save(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

new_model = load_model(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

loss, acc = new_model.evaluate(test, batch_size=32)

print(loss)
print(acc)

y_pred = new_model.predict(test) 

y_true = np.concatenate([y.numpy() for x, y in test], axis=0)

matrix = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(
matrix.numpy(), 
annot=True, 
fmt=&#39;d&#39;, 
cmap=&#39;Blues&#39;, 
xticklabels=[&#39;Cat&#39;, &#39;Dog&#39;], 
yticklabels=[&#39;Cat&#39;, &#39;Dog&#39;],
)

plt.ylabel(&#39;True Label&#39;)
plt.xlabel(&#39;Predicted Label&#39;)
plt.title(&#39;Confusion Matrix&#39;)
plt.show()

这是混淆矩阵
混淆矩阵
我怀疑的可能原因

类别不平衡 –&gt; 我的训练数据中猫和狗的数量大致相等，所以我不认为这是原因。
标签问题 –&gt;我检查并确认 y_true 既有 0 也有 1，所以标签应该没问题。
注：该模型在验证中的准确率达到了约 70%
]]></description>
      <guid>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</guid>
      <pubDate>Mon, 03 Feb 2025 20:03:37 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
销毁（游戏对象）；
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中开发用于图像分类的预训练模型</title>
      <link>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</link>
      <description><![CDATA[我有一个问题，关于如何修改预训练模型以对 3 个类而不是 1000 个类进行分类。这是我目前想到的 2 种方法。我不确定哪种方法最好。
NASNetMobile_model = tf.keras.applications.NASNetMobile (
input_shape=(224,224,3),
include_top=False,
pooling=&#39;avg&#39;,
classes=3,
weights=&#39;imagenet&#39;
)
NASNetMobile_model.trainable=False
NASNetMobile_model.summary()type here

在方法 1 中，NASNetMobile 模型使用预训练的 ImageNet 权重初始化，排除顶层并使用平均池化。该模型设置为不可训练，以防止其权重在训练期间更新。然后构建一个新的 Sequential 模型，其中包括预先训练的 NASNetMobile 模型，后面跟着两个密集层：一个有 128 个单元和 ReLU 激活，另一个有 3 个单元和 softmax 激活，用于最终分类。Sequential 模型使用 Adam 优化器和稀疏分类交叉熵损失进行编译。最后，在数据集上对模型进行 20 个 epoch 的训练，批处理大小为 4，验证分割为 20%。
方法 1
new_pretrained_model = tf.keras.Sequential()

new_pretrained_model.add(NASNetMobile_model)
new_pretrained_model.add(tf.keras.layers.Dense(128,activation=&#39;relu&#39;))
new_pretrained_model.add(tf.keras.layers.Dense(3,activation=&#39;softmax&#39;))

new_pretrained_model.layers[0].trainable = False
new_pretrained_model.summary() 此处

new_pretrained_model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

new_pretrained_model.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)

方法 2
在方法 2 中，使用功能 API 创建新模型。预训练的 NASNetMobile 模型的输出被用作具有 128 个单元和 ReLU 激活的新密集层的输入，然后是具有 3 个单元和 softmax 激活的最终密集层。此方法明确将 NASNetMobile 模型的输入连接到新的输出层，形成一个新模型，其输入与原始 NASNetMobile 模型相同，但具有用于分类的附加密集层。然后使用 Adam 优化器和稀疏分类交叉熵损失编译新模型，并在数据集上训练 20 个时期，批处理大小为 4，验证分割为 20%。
NASNetMobile_model_out = NASNetMobile_model.output
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(NASNetMobile_model_out)
output = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(x)
model_2 = tf.keras.Model(inputs = NASNetMobile_model.input,outputs=output)

model_2.summary()

model_2.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

model_2.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)
]]></description>
      <guid>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</guid>
      <pubDate>Mon, 27 May 2024 16:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Swift、UIkit 和 CoreML 在 iOS 应用中访问图像分类器 ML 模型的预测结果</title>
      <link>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</link>
      <description><![CDATA[我正在尝试开发一款应用，使用经过 Apple CoreML 训练的模型对从相机拍摄的图像或从图像库中选择的图像进行分类。该模型经过了适当的训练和测试。在将其添加到 xcode 项目后，我使用 Preview 对其进行测试时，它没有显示任何问题。但是当我尝试使用 Swift 获取预测时，结果是错误的，与 Preview 显示的完全不同。感觉就像模型未经训练一样。
这是我访问模型所做预测的代码：
let pixelImage = buffer(from: (image ?? UIImage(named: &quot;imagePlaceholder&quot;))!)
self.imageView.image = image

guard let result = try? imageClassifier!.prediction(image: pixelImage!) else {
fatalError(&quot;发生意外错误&quot;)
}

let className: String = result.classLabel
let confidence: Double = result.classLabelProbs[result.classLabel] ?? 1.0
classifier.text = &quot;\(className)\nWith Confidence:\n\(confidence)&quot;

print(&quot;分类结果为：\(className)\n置信度为：\(confidence)&quot;)

imageClassifier 是我在代码段之前使用此行代码创建的模型：
let imageClassifier = try? myImageClassifier(configuration: MLModelConfiguration())

myImageClassifier 是我使用 CoreML 创建的 ML 模型的名称。
图像是正确的，即使我输入相同的图像，它也会显示与预览不同的结果。但必须将其转换为 UIImage 到 CVPixelBuffer 类型，因为预测只允许输入 CVPixelBuffer 类型。上面代码段中的 pixelImage 是更改为 CVPixelBuffer 类型后的图像。我使用这个 stackoverflow 问题中的解决方案进行转换。代码在这里以防出现问题：
func buffer(from image: UIImage) -&gt; CVPixelBuffer? {
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] 作为 CFDictionary
var pixelBuffer : CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault, Int(image.size.width), Int(image.size.height), kCVPixelFormatType_32ARGB, attrs, &amp;pixelBuffer)
guard (status == kCVReturnSuccess) else {
return nil
}

CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
让 context = CGContext(data: pixelData, width: Int(image.size.width), height: Int(image.size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue)

context?.translateBy(x: 0, y: image.size.height)
context?.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context!)
image.draw(in: CGRect(x: 0, y: 0, width: image.size.width, height: image.size.height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))

return pixelBuffer
}

我认为模型本身没有任何问题，只是我将其实现到应用程序中的方式有​​问题。
编辑：
我已经从 Apple 的教程中下载了一个示例项目，并将其模型 MobileNet 实现到我的项目中。代码执行没有错误，结果是正确的。我创建的模型可能出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</guid>
      <pubDate>Tue, 09 Nov 2021 13:30:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras，内存错误 - data = data.astype("float") / 255.0。无法为形状为 (13165, 32, 32, 3) 的数组分配 309.MiB</title>
      <link>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</link>
      <description><![CDATA[我目前正在研究 Smiles 数据集，然后应用深度学习来检测微笑是正面的还是负面的。我使用的机器是 Raspberry Pi 3，用于执行此程序的 Python 版本是 3.7（不是 2.7）
我的训练集中总共有 13165 张图像。我想将其存储到一个数组中。但是，我遇到了一个问题，就是分配一个形状为（13165, 32, 32, 3）的数组。
下面是源代码（shallownet_smile.py）：
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classes_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv.shallownet import ShallowNet
from keras.optimizers import SGD
from imutils import routes
import matplotlib.pyplot as plt
import numpy as np
import argparse

ap = argparse.ArgumentParser()
ap.add_argument(&quot;-d&quot;, &quot;--dataset&quot;, required=True, help=&quot;path to input dataset&quot;)
args = vars(ap.parse_args())

# 获取我们将要描述的图像列表
print(&quot;[INFO] loading images...&quot;)

imagePaths = list(paths.list_images(args[&quot;dataset&quot;]))

sp = SimplePreprocessor(32, 32)
iap = ImageToArrayPreprocessor()

sdl = SimpleDatasetLoader(preprocessors=[sp, iap])
(data, labels) = sdl.load(imagePaths, verbose=1)
# 将值转换为 0-1 之间的值
data = data.astype(&quot;float&quot;) / 255.0

# 将数据划分为训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25,
random_state=42)

# 将标签从整数转换为向量
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)

# 初始化优化器和模型
print(“INFO] 编译模型...”)

# 初始化随机梯度下降，学习率为 0.005
opt = SGD(lr=0.005)

model = ShallowNet.build(width=32, height=32,depth=3,classes=2)
model.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=opt,
metrics=[&quot;accuracy&quot;])

# 训练网络
print(“INFO] 训练网络...”)

H = model.fit(trainX, trainY,validation_data=(testX, testY),batch_size=32,
epochs=100, verbose=1)

print(“[INFO] 评估网络...”)

predictions = model.predict(testX, batch_size=32)

print(classification_report(
testY.argmax(axis=1),
predictions.argmax(axis=1),
target_names=[&quot;positive&quot;, &quot;negative&quot;]
))

plt.style.use(&quot;ggplot&quot;)
plt.figure()
plt.plot(np.arange(0, 100), H.history[&quot;loss&quot;], label=&quot;train_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;acc&quot;], label=&quot;train_acc&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_acc&quot;], label=&quot;val_acc&quot;)
plt.title(&quot;训练损失和准确率&quot;)
plt.xlabel(&quot;Epoch #&quot;)
plt.ylabel(&quot;损失/准确率&quot;)
plt.legend()
plt.show()

假设数据集位于我当前的目录中。以下是我得到的错误：

python3 shallownet_smile.py -d=datasets/Smiles

错误消息
我仍然感到困惑，不知道哪里出了问题。我将非常感谢任何专家或有深度学习/机器学习经验的人向我解释和澄清我做错了什么。
感谢您的帮助和关注。]]></description>
      <guid>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</guid>
      <pubDate>Sun, 05 Apr 2020 17:25:26 GMT</pubDate>
    </item>
    <item>
      <title>ResNet50 模型未通过 keras 中的迁移学习进行学习</title>
      <link>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</link>
      <description><![CDATA[我正在尝试对 ResNet50 模型执行迁移学习，该模型已针对 PASCAL VOC 2012 数据集的 Imagenet 权重进行了预训练。由于它是一个多标签数据集，因此我在最后一层使用 sigmoid 激活函数和 binary_crossentropy 损失。指标包括 precision、recall 和 accuracy。下面是我用来为 20 个类（PASCAL VOC 有 20 个类）构建模型的代码。
img_height,img_width = 128,128
num_classes = 20
#如果正在加载 imagenet 权重，
#输入必须具有静态正方形（(128, 128)、(160, 160)、(192, 192) 或 (224, 224) 之一）
base_model = applications.resnet50.ResNet50(weights= &#39;imagenet&#39;, include_top=False, input_shape= (img_height,img_width,3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
#x = Dropout(0.7)(x)
predictions = Dense(num_classes,activation= &#39;sigmoid&#39;)(x)
model = Model(inputs = base_model.input, output = predictions)
for layer in model.layers[-2:]:
layer.trainable=True
for layer in model.layers[:-3]:
layer.trainable=False

adam = Adam(lr=0.0001)
model.compile(optimizer= adam, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;,precision_m,recall_m])
#print(model.summary())

X_train, X_test, Y_train, Y_test = train_test_split(x_train, y, random_state=42, test_size=0.2)
savingcheckpoint = ModelCheckpoint(&#39;ResnetTL.h5&#39;,monitor=&#39;val_loss&#39;,verbose=1,save_best_only=True,mode=&#39;min&#39;)
earlystopcheckpoint = EarlyStopping(monitor=&#39;val_loss&#39;,patience=10,verbose=1,mode=&#39;min&#39;,restore_best_weights=True)
model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test,Y_test), batch_size=batch_size,callbacks=[savingcheckpoint,earlystopcheckpoint],shuffle=True)
model.save_weights(&#39;ResnetTLweights.h5&#39;)

它运行了 35 个 epoch 直到 earlystopping，指标如下（没有 Dropout 层）：
loss: 0.1195 - accuracy: 0.9551 - precision_m: 0.8200 - recall_m: 0.5420 - val_loss: 0.3535 - val_accuracy: 0.8358 - val_precision_m: 0.0583 - val_recall_m: 0.0757

即使使用 Dropout 层，我也看不出有什么区别。
loss: 0.1584 - accuracy: 0.9428 - precision_m: 0.7212 - recall_m: 0.4333 - val_loss: 0.3508 - val_accuracy: 0.8783 - val_precision_m: 0.0595 - val_recall_m: 0.0403

使用 dropout，对于经过几个时期，模型的验证精度和准确率达到了 0.2，但并未超过该值。
我发现，与有和没有 dropout 层的训练集相比，验证集的精度和召回率相当低。我应该如何解释这一点？这是否意味着模型过度拟合。如果是这样，我该怎么办？截至目前，模型预测相当随机（完全不正确）。数据集大小为 11000 张图像。]]></description>
      <guid>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</guid>
      <pubDate>Tue, 15 Oct 2019 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>回归模型中成本函数用L1范数代替L2范数</title>
      <link>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</link>
      <description><![CDATA[我想知道 Python 中是否有一个函数可以完成与 scipy.linalg.lstsq 相同的工作，但使用“最小绝对偏差”回归而不是“最小二乘”回归 (OLS)。我想使用 L1 范数，而不是 L2 范数。
事实上，我有 3d 点，我想要它们的最佳拟合平面。常见的方法是通过最小二乘法，如 Github 链接。但众所周知，这并不总是能给出最佳拟合，尤其是当我们的数据集中有闯入者时。最好计算最小绝对偏差。 此处 更详细地解释了这两种方法之间的区别。
由于它是 Ax = b 矩阵方程，需要循环来最小化结果，因此无法通过 MAD 等函数解决。我想知道是否有人知道 Python 中的相关函数（可能是线性代数包中）可以计算“最小绝对偏差”回归？]]></description>
      <guid>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</guid>
      <pubDate>Thu, 16 Aug 2018 18:12:02 GMT</pubDate>
    </item>
    </channel>
</rss>