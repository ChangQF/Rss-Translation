<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 10 Feb 2024 06:16:25 GMT</lastBuildDate>
    <item>
      <title>是否可以训练神经网络对包含特定形状的像素进行分组？</title>
      <link>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</link>
      <description><![CDATA[我想训练一个模型，可以训练该模型对形状物体（例如图像中的牙齿）进行分组。如果它接收到图像作为输入，它应该输出一个包含子列表的列表，每个子列表包含检测到的牙齿的像素。因此，如果输入图像在处理前被展平，则输出列表应如下所示：
[（像素1，像素2，像素3，...），（像素4，像素5，像素6，...），...]，
其中pixel1、pixel2等表示属于牙齿的像素的索引（假设输入图像首先被展平）。
没有牙齿的图像应该有一个空列表作为输出。
每个训练数据样本都应该有一个图像和一个牙齿列表。
我不太确定如何构建一个神经网络来按照我希望的方式执行此任务（如果可能的话）。
我最初的方法是训练一个单独的模型，将一个小像素网格（如 24x24）分类为“牙齿”或“非牙齿”，然后扫描更大的输入图像（可能是 200x200）并对以下部分进行分组归类为牙齿。
有更好的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</guid>
      <pubDate>Sat, 10 Feb 2024 02:50:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 XGboost 进行时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</link>
      <description><![CDATA[我需要找到时间序列中的异常情况，对于这项任务我选择了机器学习方法，即 XGboost。
有一个数据集，包含 2参数：秒和系列值（分贝）。
问题：从PrepareData()函数获得的y_test由于某种原因为空，我不明白为什么会发生这种情况。您有解决问题的想法吗？
代码：
def code_mean(data, cat_feature, real_feature):
     ”“”
     返回一个字典，其中键是 cat_feature 的唯一类别，
     这些值是基于 real_feature 的平均值
     ”“”
    返回 dict(data.groupby(cat_feature)[real_feature].mean())

进行聚合
df1 = pd.read_csv(&quot;data.csv&quot;, sep=&#39;,&#39;)
df1.columns = [&#39;时间,秒&#39;,&#39;y&#39;]

df1[&#39;时间，秒&#39;] = df1[&#39;时间，秒&#39;]*10000
df1[&#39;时间，秒&#39;] = pd.to_datetime(df1[&#39;时间，秒&#39;],unit=&#39;s&#39;)
df1.set_index(&#39;时间,秒&#39;, inplace=True)
aggregate_df1 = df1.resample(&#39;H&#39;).mean()

数据 = pd.DataFrame(aggregate_df1)
data.index = pd.to_datetime(data.index)
数据[“小时”] = data.index.小时
数据[“工作日”] = data.index.工作日
数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1
数据.head()

该函数将立即返回分为训练和测试的数据集和目标变量。我猜这就是所有问题所在：
def prepareData(数据, lag_start=5, lag_end=20, test_size=0.15):

    数据 = pd.DataFrame(data.copy())
    数据.列 = [“y”]

    test_index = int(len(数据)*(1-test_size))

    对于范围内的 i（lag_start，lag_end）：
        data[“lag_{}”.format(i)] = data.y.shift(i)

    data.index = pd.to_datetime(data.index)
    数据[“小时”] = data.index.小时
    数据[“工作日”] = data.index.工作日
    数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1

    数据[&#39;weekday_average&#39;] = data[&#39;weekday&#39;].apply(lambda x: code_mean(data[:test_index], &#39;weekday&#39;, &quot;y&quot;).get(x))
    data[&quot;hour_average&quot;] = data[&#39;hour&#39;].apply(lambda x: code_mean(data[:test_index], &#39;hour&#39;, &quot;y&quot;).get(x))

    data.drop([“小时”,“工作日”], axis=1, inplace=True)

    数据 = data.dropna()
    数据 = data.reset_index(drop=True)

    # разбиваем весь датасет на тренировочную и тестовую выборку
    X_train = data.loc[:test_index].drop([“y”], axis=1)
    y_train = data.loc[:test_index][“y”]
    X_test = data.loc[test_index:].drop([“y”], axis=1)
    y_test = data.loc[test_index:][“y”]
    
    返回X_train，X_test，y_train，y_test

最后是预测的主要函数：
def XGB_forecast（数据，lag_start=5，lag_end=20，test_size=0.15，scale=1.96）：

    X_train、X_test、y_train、y_test = 准备数据（aggregate_df1、lag_start、lag_end、test_size）
    dtrain = xgb.DMatrix(X_train, 标签=y_train)
    dtest = xgb.DMatrix(X_test)

    参数 = {
        &#39;目标&#39;: &#39;reg:线性&#39;,
        &#39;助推器&#39;：&#39;gb线性&#39;
    }
    树 = 1000

    cv = xgb.cv(params, dtrain, 指标 = (&#39;rmse&#39;), verbose_eval=False, nfold=10, show_stdv=False, num_boost_round=trees)

    bst = xgb.train(params, dtrain, num_boost_round=cv[&#39;test-rmse-mean&#39;].argmin())

    #cv.plot(y=[&#39;test-mae-mean&#39;, &#39;train-mae-mean&#39;])

    偏差 = cv.loc[cv[&#39;test-rmse-mean&#39;].argmin()][&quot;test-rmse-mean&quot;]

    Prediction_train = bst.predict(dtrain)
    plt.figure(figsize=(15, 5))
    plt.plot(预测训练)
    plt.plot(y_train)
    plt.axis(&#39;紧&#39;)
    plt.网格（真）

    Prediction_test = bst.predict(dtest)
    下=预测测试规模*偏差
    上=预测_测试+尺度*偏差

    如果 len(y_test) &gt; 0:
        异常 = np.array([np.NaN]*len(y_test))
        异常[y_test&lt;下限] = y_test[y_test&lt;下限]
    别的：
        异常 = np.array([])

    plt.figure(figsize=(15, 5))
    plt.plot(prediction_test, label=&quot;预测&quot;)
    plt.plot(lower, &quot;r--&quot;, label=&quot;上键/下键&quot;)
    plt.plot(上，“r--”)
    plt.plot(列表(y_test), label=“y_test”)
    plt.plot（异常，“ro”，markersize=10）
    plt.legend(loc=&quot;最佳&quot;)
    plt.axis(&#39;紧&#39;)
    plt.title(“XGBoost 平均绝对误差{}用户”.format(round(mean_absolute_error(prediction_test, y_test))))
    plt.网格（真）
    plt.图例()

函数调用：XGB_forecast(aggreerated_df1, test_size=0.2, lag_start=5, lag_end=30)]]></description>
      <guid>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</guid>
      <pubDate>Fri, 09 Feb 2024 23:30:21 GMT</pubDate>
    </item>
    <item>
      <title>C# 中的机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</link>
      <description><![CDATA[我应该构建一个机器学习模型来充当文档过滤器。在手头的数据集中，我有 3 种不同类型的文档，它们不应通过过滤器。该模型总共将处理 20 种不同类型的文档，最终目标是减轻手动处理这三种类型的负担。
我刚刚开始这个项目，我想在我自己探索该领域的同时，向社区寻求潜在方法的建议。训练数据相当小，在标记类型之间分布为 500、430、80。
该项目最好用 C# 编写。
到目前为止，我只从数据库中收集了文档并验证了它们是否被正确标记，并探索了各种方法，例如支持向量机、朴素拜耳、无监督学习方法和文本数据的各种预处理步骤。
微调预训练模型似乎很流行（迁移学习），也是一种潜在的方法，尤其是在数据集有限的情况下。但有些文档相对广泛，三种类型中的一种包含大约 3000 个单词，据我所知，这对于可用的预训练模型可能会出现问题，并且在 .net 中实现的可能性有限。
因此，目前我正着手构建一个传统模型，并征求您的意见和意见。]]></description>
      <guid>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</guid>
      <pubDate>Fri, 09 Feb 2024 22:21:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用注意力机制的双向 RNN/LSTM/GRU</title>
      <link>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</guid>
      <pubDate>Fri, 09 Feb 2024 21:52:40 GMT</pubDate>
    </item>
    <item>
      <title>训练随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</link>
      <description><![CDATA[我正在尝试在一组图像上训练随机森林分类器，旨在对 10 种不同的颜色类别进行分类。我有一个带有子目录的 \dataset 目录，每个子目录都以一种颜色（黄色、红色、白色等）命名。我采用 HSV 直方图并将其用作我的特征（它比 RGB 更好吗？）
这是我的代码。我在代码中添加了 print(image_path) 进行调试。但运行时，它只打印第一个文件，不打印其他任何内容。
有什么问题吗？如何解决？
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.feature_extraction.image 导入 extract_patches_2d
从 sklearn.model_selection 导入 train_test_split
从 skimage.color 导入 rgb2hsv，rgb2gray
从 skimage.feature 导入graycomatrix，graycoprops
导入操作系统
将 numpy 导入为 np
从 PIL 导入图像
导入作业库


# 定义路径和参数
data_dir =“.\数据集” # 替换为你的实际目录路径
num_trees = 100 # 森林中树木的数量
patch_size = (32, 32) # 用于特征提取的图像块的大小

# 定义特征提取函数
def extract_color_histogram(图像):
    # 将 PIL 图像转换为 NumPy 数组
    img_array = np.array(图像)
    # 调整图像大小
    img_resized = Image.fromarray(img_array).resize((640, 640))
    # 转换为 HSV
    img_hsv = img_resized.convert(“HSV”)
    hist, _ = np.histogram(img_hsv, bins=(8, 8, 8), 范围=((0, 255), (0, 255), (0, 255)))
    hist = hist.flatten()
    返回历史记录

def extract_texture_features(图像):
    # 转换为灰度并提取纹理特征
    灰色 = rgb2gray(图像)
    gray_uint = (gray * 255).astype(np.uint8) # 将灰度转换为无符号整数
    glcm = Graycomatrix(gray_uint，距离=[5]，角度=[0]，级别=256，对称=True，normed=True)
    特征 = Graycoprops(glcm, &#39;对比度&#39;)[0]
    返回特征

# 加载数据并提取特征
X、y = []、[]
对于 os.listdir(data_dir) 中的颜色：
    对于 os.listdir(os.path.join(data_dir, color)) 中的 image_file：
        image_path = os.path.join(data_dir, 颜色, image_file)
        打印（图像路径）
        img = Image.open(图像路径)
        img_array = np.array(img)
        补丁 = extract_patches_2d(img_array, patch_size=patch_size)
        对于补丁中的补丁：
            # 从每个补丁中提取特征
            color_hist = extract_color_histogram(补丁)
            纹理特征=提取纹理特征（补丁）
            特征 = np.concatenate((color_hist,texture_features))
            X.append（功能）
            y.追加（颜色）

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林模型
print(&quot;现在训练...&quot;)
模型 = RandomForestClassifier(n_estimators=num_trees, random_state=42)
model.fit(X_train, y_train)

# 评估模型性能
准确度 = model.score(X_test, y_test)
print(f&quot;模型精度: {accuracy:.2f}&quot;)

# 保存模型以供将来使用
joblib.dump(模型,“color_model.pkl”)
]]></description>
      <guid>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</guid>
      <pubDate>Fri, 09 Feb 2024 21:33:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM 的数据访问已损坏。想法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</link>
      <description><![CDATA[为了构建真正有用的 GenAI 应用程序，法学硕士需要能够从结构化和非结构化数据源访问专有数据。法学硕士很难了解检索相关数据的可用性、位置和方法。
关键问题：

法学硕士无法确定是否有必要的数据可用于回答任何数据源中的用户查询。
如果数据可用，法学硕士无法找到要从哪个数据源检索。
如果来源已知，为众多检索协议编写检索管道就会变得复杂、重复且不确定。

您在项目中遇到过这些问题吗？您发现哪些有效的变通方法或解决方案？有什么新工具或实践可以简化法学硕士与不同数据源的集成吗？
一些人建议微调或 RAG 作为潜在的解决方案，但是：

使用特定数据微调模型的成本很高，而且会随着最新数据而过时，并且存在内置的访问控制问题。持续的再培训成本高昂，而且跟不上实时数据变化的步伐。
RAG 不适用于结构化数据。大多数有用的数据都是结构化的，可以分布在多个数据源中，每个数据源都有自己的查询机制。
编写单独的检索管道是有限且复杂的，并且不能确保确定性、安全性和访问控制。
]]></description>
      <guid>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</guid>
      <pubDate>Fri, 09 Feb 2024 21:01:59 GMT</pubDate>
    </item>
    <item>
      <title>R 机器学习中 Ranger 模型的错误</title>
      <link>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</link>
      <description><![CDATA[我正在运行生存模型的机器学习代码。我的 pred_prob 代码有错误。谁能帮我？先感谢您
我的错误是：
&lt;代码&gt;&gt; pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
h(simpleError(msg, call)) 中的错误：
  在为函数“rowMeans”选择方法时评估参数“x”时出错：$ 运算符对于原子向量无效

我的代码是：
图书馆（护林员）# RSF
Library(survival) # 包含生存示例，处理生存对象
Library(caret) # 用于分层交叉验证
库(dplyr) # 数据操作
图书馆（佩奇）
# 加载数据集，其中Time是事件发生的时间，Event是事件发生的时间

库（readxl）
df2 &lt;- read_excel(“E:/ME/BS DATA/数据 BS.xlsx”)

# 改变状态变量的标签
df2 &lt;- df2 %&gt;% mutate(status = time_15year-1) # 0 = 审查，1 = 死亡
# 一些数据包含缺失值，为了简单起见，我省略了对 NA 的观察
df2 &lt;- na.omit(df2)
# 缩放至月
df2$time_15year &lt;- 楼层(df2$time_15year)
par(“三月”)
par(mar=c(.01,.01,.01,.01))
pairs(df2 %&gt;% dplyr::select(time_15year,BS_death), main = “NCCTG 脑中风数据”)
# 交叉验证，对状态变量进行分层，以确保每个组（此处已审查，已死亡）
# 均匀分布在交叉验证折叠上
折叠 &lt;- 2 # 表示 交叉验证
cvIndex &lt;- createFolds(factor(df2$BS_death), Folds, returnTrain = T)
#2 模型，训练
# 创建一些容器来存储结果
# （对于大模型来说不合理，对于大模型你可能需要将中间结果存储在磁盘上）
容器模型&lt;-向量（“列表”，长度（cvIndex））
容器_pred &lt;- 容器_模型

# 定义训练/测试数据
for (i in 1:length(cvIndex)) {
  train_data &lt;- df2[cvIndex[[i]], ]
  # 其余代码
}

测试数据 &lt;- df2[-cvIndex[[i]],]

train_data &lt;- train_data[complete.cases(train_data), ]
测试数据 &lt;- 测试数据[完整.案例(测试数据), ]

rangermodel &lt;- ranger(Surv(time_15year, BS_death) ~ 年龄 + 性别 + edu + 地点 + cvahis+ mihis + bphis +heartdis + smok + Pastsmok+ 被动+活动 +waterpip +cvatype, 数据 = train_data)

情节（rangermodel$unique.death.times，rangermodel$survival[1，]）
ranger_predict &lt;- 预测（rangermodel，数据=测试数据）
str(ranger_预测)
ranger_predict &lt;- unlist(ranger_predict)
pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
pred_prob[pred_prob&gt;中位数(pred_prob)]=1
pred_prob[pred_prob&lt;=中位数(pred_prob)]=0
表（pred_prob）
fusionMatrix(as.factor(testing_data$BS_death), as.factor(pred_prob))
]]></description>
      <guid>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</guid>
      <pubDate>Fri, 09 Feb 2024 19:06:49 GMT</pubDate>
    </item>
    <item>
      <title>'{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT] 的 1 减 3 导致的负维度大小</title>
      <link>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</link>
      <description><![CDATA[尝试 model.fit 后：
hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我收到错误：
ValueError Traceback（最近一次调用最后一次）
单元格位于\[132\]，第 1 行
\----\&gt; 1 hist = model.fit（train，epochs = 15，validation_data = val，callbacks = \ [tensorboard_callback \]）#mudei epocas pra 3，antesera 20

文件 \~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\utils\traceback_utils.py :70，在filter_traceback中。\.error_handler(\*args, \*\*kwargs)
67 过滤_tb = \_process_traceback_frames(e.__traceback__)
68 # 要获取完整的堆栈跟踪，请调用：
69 # `tf.debugging.disable_traceback_filtering()`
\---\&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
71 最后：
72 删除filtered_tb

文件 \~\\AppData\\Local\\Temp\__autograph_ generated_filea6t43riq.py:15，位于outer_factory.\.inner_factory.\.tf__train_function(iterator)
13 尝试：
14 do_return =真
\---\&gt; 15 retval_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(迭代器)), 无, fscope)
16 除外：
17 do_return = 假

ValueError：在用户代码中：

    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1338 行，位于训练函数*
        返回step_function（自身，迭代器）
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1322 行，位于步骤函数 **
        输出 = model.distribute_strategy.run(run_step, args=(data,))
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1303 行，位于运行步骤**
        输出 = model.train_step(数据)
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1080 行，位于训练步骤
        y_pred = self(x, 训练=True)
    文件“C：\ Users \ Enenon \ AppData \ Local \ Packages \ PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0 \ LocalCache \ local-packages \ Python310 \ site-packages \ keras \ src \ utils \traceback_utils.py”，第70行，位于错误处理程序
        从 None 引发 e.with_traceback(filtered_tb)
    
    ValueError：调用层“conv2d_26”（类型 Conv2D）时遇到异常。
    
    &#39;{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=“NHWC”, dilations=[1, 1, 1, 1],explicit_paddings=[ 1 减 3 导致的负维度大小], padding=“VALID”, strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_15/Cast,equential_15/conv2d_26/Conv2D/ReadVariableOp)&#39; 输入形状：[64,64,1, 1]、[3,3,1,32]。
    
    调用层“conv2d_26”接收的参数（类型 Conv2D）：
      • 输入=tf.Tensor(形状=(64, 64, 1, 1), dtype=float32)`

我的代码：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv2D(32, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
模型.add(压平())
model.add（密集（64，激活=&#39;relu&#39;））
model.add（密集（3，激活=&#39;sigmoid&#39;））

model.compile(&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我正在使用 64,64,1 数据集。这是将张量从 64,64 修改为 64,64,1 并从张量创建数据集的代码：
x = x.reshape(-1, 64, 64, 1)
标签 = tf.constant(y)
特征 = tf.constant(x)
datax = tf.data.Dataset.from_tensor_slices(特征)
datay = tf.data.Dataset.from_tensor_slices(标签)
数据 = tf.data.Dataset.zip((datax,datay))
data_iterator = data.as_numpy_iterator()
批处理 = data_iterator.next()
]]></description>
      <guid>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</guid>
      <pubDate>Fri, 09 Feb 2024 17:51:53 GMT</pubDate>
    </item>
    <item>
      <title>如何创建目标变量[关闭]</title>
      <link>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</link>
      <description><![CDATA[我尝试预测 ncaa 篮球疯狂游行的每个结果。
我有过去 20 场左右锦标赛的历史数据，并且有 team1_score 和 team2_score 等列。我认为创建一个目标变量很容易，只需创建一个列 team1_win 并在 true 时返回 1，否则返回 0。问题是我的数据是经过组织的，因此 team1 始终是获​​胜团队。所以我的目标变量列将只包含 1。对于二元分类来说，这似乎是一个问题。我不确定如何创建目标变量。我是否需要以某种方式重新整理我的数据，以便 team1 并不总是获胜团队？我的目标变量全为1有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</guid>
      <pubDate>Fri, 09 Feb 2024 17:22:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 WSL GPU 支持下运行 Windows Python 代码？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</link>
      <description><![CDATA[我尝试在 Windows PS 上的 WSL 2 发行版中使用 TensorFlow。我的问题是，我只能在互联网上找到描述安装过程的页面，但没有人解释如何在 Windows 上使用 GPU 加速从我的 virtual-env 环境运行代码。那么有没有办法在 WSL Distrubition 中远程运行机器学习脚本呢？
我已经成功安装了WSL2，包括miniconda和tensorflow（带有CUDA）。 Tensorflow Feedback 线也发现 GPU 没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</guid>
      <pubDate>Fri, 09 Feb 2024 16:52:00 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4 [关闭]</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>对 Dataframe 中的某些列进行估算</title>
      <link>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</link>
      <description><![CDATA[我正在尝试在名为“年龄”的单个列上使用Imputer来替换缺失值。但是，我收到错误：“预期是二维数组，却得到了一维数组：”
以下是我的代码
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 Imputer

数据集 = pd.read_csv(“titanic_train.csv”)

dataset.drop(&#39;小屋&#39;, axis=1, inplace=True)
x = dataset.drop(&#39;幸存&#39;, axis=1)
y = 数据集[&#39;幸存&#39;]

imputer = Imputer（missing_values =“nan”，策略=“平均值”，轴= 1）
imputer = imputer.fit(x[&#39;年龄&#39;])
x[&#39;年龄&#39;] = imputer.transform(x[&#39;年龄&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</guid>
      <pubDate>Tue, 18 Sep 2018 10:44:49 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的边缘检测器</title>
      <link>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</link>
      <description><![CDATA[我已阅读以下内容关于使用机器学习进行边缘检测的博客。他们

&lt;块引用&gt;
  使用了基于现代机器学习的算法。该算法是在图像上进行训练的，其中人类注释了最重要的边缘和对象边界。给定这个标记数据集，训练机器学习模型来预测图像中每个像素属于对象边界的概率。

我想使用 opencv 来实现这项技术。
有人知道或知道如何使用 Opencv 实现/开发此方法吗？
我们如何注释最重要的边缘和对象边界以供机器学习算法使用？]]></description>
      <guid>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</guid>
      <pubDate>Sat, 08 Apr 2017 18:51:27 GMT</pubDate>
    </item>
    <item>
      <title>监督学习，(ii) 无监督学习，(iii) 强化学习</title>
      <link>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</link>
      <description><![CDATA[在阅读有关监督学习、无监督学习、强化学习的内容时，我遇到了以下问题并感到困惑。请帮助我识别以下三个中哪一个是监督学习、无监督学习、强化学习。
什么类型的学习（如果有）最能描述以下三种场景：
(i) 为自动售货机创建硬币分类系统。为此，
开发商从美国造币厂获取准确的硬币规格并得出
尺寸、重量和面额的统计模型，自动售货机
然后机器对其硬币进行分类。
(ii) 算法不是调用美国造币厂来获取硬币信息，而是
赠送一大套贴有标签的硬币。该算法使用该数据来
推断自动售货机随后用来对其进行分类的决策边界
硬币。
(iii) 计算机通过重复玩井字游戏制定策略
并通过惩罚最终导致失败的举动来调整策略。]]></description>
      <guid>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</guid>
      <pubDate>Wed, 03 Apr 2013 09:00:48 GMT</pubDate>
    </item>
    </channel>
</rss>