<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Jun 2024 01:03:31 GMT</lastBuildDate>
    <item>
      <title>使用同一模型问题预测时间序列中多个实例的未来 CPU 使用率</title>
      <link>https://stackoverflow.com/questions/78583575/predicting-future-cpu-usage-in-time-series-with-multiple-instances-with-the-same</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78583575/predicting-future-cpu-usage-in-time-series-with-multiple-instances-with-the-same</guid>
      <pubDate>Wed, 05 Jun 2024 21:27:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python tensorflow 图像回归模型中增加我的 MAE 损失函数[关闭]</title>
      <link>https://stackoverflow.com/questions/78583554/how-to-increase-my-mae-loss-function-in-python-tensorflow-image-regression-model</link>
      <description><![CDATA[我有 8000 张黑色背景上的圆圈图像。每个标签都是圆圈的 x 坐标。每张图片的尺寸为 (128,128,3)。我的训练损失从 2000 开始，到 10 结束，而验证损失保持在 200 左右。另外，我通过将每幅图像除以 255 来对其进行标准化。
images_array 的形状为：(8000,128,128,3)
y 的形状为：(8000,1)
这是我的代码：
from sklearn.model_selection import train_test_split
X_train, X_temp, y_train, y_temp = train_test_split(images_array, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

import tensorflow as tf
X_train = tf.convert_to_tensor(X_train)
y_train = tf.convert_to_tensor(y_train)

从 tensorflow 导入 keras
从 tensorflow.keras 导入层，模型
从 tensorflow.keras.models 导入顺序

输入 = keras.Input(shape=(128, 128, 3))

# 定义层

x = 层。Conv2D(8, 3, 激活=&#39;relu&#39;)(输入)
x = 层。MaxPooling2D(2)(x)
x = 层。Conv2D(16, 3, 激活=&#39;relu&#39;)(x)
x = 层。MaxPooling2D(2)(x)
x=层。Flatten()(x)
x = 层。Dense(128, 激活=&#39;relu&#39;)(x)
output_x = 层。Dense(1, 激活 = &#39;linear&#39;, 名称 = &quot;y1_output&quot;)(x)

model = Model(inputs = input, output = output_x)
model.summary()


我可以做些什么来显著改善我的损失？
我尝试了不同的激活函数和不同的优化器。]]></description>
      <guid>https://stackoverflow.com/questions/78583554/how-to-increase-my-mae-loss-function-in-python-tensorflow-image-regression-model</guid>
      <pubDate>Wed, 05 Jun 2024 21:21:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 模型或其他实践在大型数据集中识别名字和姓氏的最佳实践是什么？</title>
      <link>https://stackoverflow.com/questions/78583245/what-are-the-best-practices-to-identify-first-and-last-name-in-large-datasets-us</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78583245/what-are-the-best-practices-to-identify-first-and-last-name-in-large-datasets-us</guid>
      <pubDate>Wed, 05 Jun 2024 20:00:15 GMT</pubDate>
    </item>
    <item>
      <title>EasyOCR TypeError：Reader.__init__() 得到了一个意外的关键字参数“detection”</title>
      <link>https://stackoverflow.com/questions/78582788/easyocr-typeerror-reader-init-got-an-unexpected-keyword-argument-detecti</link>
      <description><![CDATA[我尝试使用 easyocr 包，但出现以下错误
reader = easyocr.Reader([&#39;en&#39;], detection=&#39;DB&#39;, identification = &#39;Transformer&#39;)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Reader.init() 获得了意外的关键字参数“detection”
我刚开始使用 ocr 包。有谁能帮我解决这个问题
在官方仓库中，他们也包含了这些参数，但现在出现了错误
, ]]></description>
      <guid>https://stackoverflow.com/questions/78582788/easyocr-typeerror-reader-init-got-an-unexpected-keyword-argument-detecti</guid>
      <pubDate>Wed, 05 Jun 2024 18:02:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么卡方与orange3和python不同？</title>
      <link>https://stackoverflow.com/questions/78582697/why-chi-square-is-differ-from-orange3-and-python</link>
      <description><![CDATA[*我计算了平方的种类，但是我从orang3程序和python得到了不同的结果，有什么原因吗？
*这是我从 python 得到的数字
0个(毫升) 18.932143 0.755599
价格指数（克） 22.009615 0.519671
2毫克（毫克）84.000000 0.448668
3 件（克） 49.275000 0.382270
4年价格(克) 19.560714 0.848676
5 由 Google 数字化 13.391551 0.643942
*这是我通过orange3得到的号码
橙色3卡方数
*这是我使用的表的所有信息
仅供参考
*这是我使用的python代码
导入 pandas 作为 pd
导入 scipy.stats
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从 matplotlib 导入 font_manager ， rc

font_path = “C:/Windows/Fonts/malgun.ttf” # Windows 屏幕“Malgun Gothic”屏幕
字体 = font_manager.FontProperties(fname=font_path).get_name()
rc(&#39;字体&#39;, 系列=字体)

数据 = pd.read_csv(&#39;C:/Users/tlotr/OneDrive/Entertainment/BYDIET/Calorcateglrized_calorcateglrized.csv&#39;, 编码=&#39;euc-kr&#39;);
数据.info()

Variables = [&#39;浓度(ml)&#39;, &#39;含量浓度(g)&#39;, &#39;浓度浓度(mg)&#39;, &#39;浓度浓度(g)&#39;, &#39;浓度浓度(g)&#39;, &#39;雪花浓度(g)&#39;]

chi2_结果 = []

对于变量中的 var：
contingency_table = pd.crosstab(data[&#39;languageC&#39;], data[var]);
chi2, p, _, _ = stats.chi2_contingency(contingency_table);
chi2_结果。追加（（var，chi2，p））

chi2_results_df = pd.DataFrame(chi2_results, columns=[&#39;变量&#39;, &#39;Chi2&#39;, &#39;p 值&#39;]);
打印（chi2_results_df）
plt. 图（图尺寸=(12, 8)）
sns.barplot(x=&#39;变量&#39;, y=&#39;Chi2&#39;, data=chi2_df_results);
plt.title(&#39;美国&#39;)
plt.xlabel(&#39;标签&#39;)
plt.ylabel(&#39;Chi2 값&#39;)
plt.xticks（旋转=45）
plt.tight_layout()
显示 ( )


*这是我用的Orange3
Orage3 GUI]]></description>
      <guid>https://stackoverflow.com/questions/78582697/why-chi-square-is-differ-from-orange3-and-python</guid>
      <pubDate>Wed, 05 Jun 2024 17:39:06 GMT</pubDate>
    </item>
    <item>
      <title>给定的梯度下降代码是按顺序还是同时更新参数？</title>
      <link>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-paraments-sequentially-or-si</link>
      <description><![CDATA[我是机器学习的新手，一直在学习梯度下降算法。我相信此代码使用同时更新，即使它看起来像是顺序更新。由于偏导数的值是在更新 w 或 b 之前计算的，即从原始 w 和 b 计算，因此应用于单个 w、b 的算法是从原始值应用的。我错了吗？

dj_dw=((w*x[i]+b-y[i])*x[i])/m
dj_db=(w*x[i]+b-y[i])/m
w=w-a*dj_dw
b=b-a*dj_db

语言是 python3。
x 和 y 是训练集。
w 和 b 是应用算法的参数。
我正在使用梯度下降算法进行线性回归。
dj_dw 是均方误差成本函数关于 w 的偏微分。 dj_db 也是如此。
如有错误，请见谅，我是新手。
我尝试使用 gemini 和 chatgpt 进行交叉检查，他们说这是连续的，因此才造成混淆]]></description>
      <guid>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-paraments-sequentially-or-si</guid>
      <pubDate>Wed, 05 Jun 2024 15:28:51 GMT</pubDate>
    </item>
    <item>
      <title>将图像置于中心并在导出时添加背景</title>
      <link>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</link>
      <description><![CDATA[我想自动完成所有这些操作：

选择图像中的对象
在此对象上裁剪我的图像
裁剪为 1:1 的宽高比，在此对象周围留出一点空隙
以 800x800px 的 JPG 格式导出我的图像，我的对象位于图像中心，背景为白色。

我在 win11 64 位上
我做了什么：

安装 Python 并创建环境
安装opencv-python-headless、pillow、numpy、Pytorch以用于 CUDA 11.8
克隆存储库 segment-anything.git 并使用 PIP 安装它
下载sam_vit_b_01ec64.pth

像这样对 py 文件进行编码：
import os
import cv2
import numpy as np
from PIL import Image
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
return cv2.imread(image_path)

def save_image(image, path):
cv2.imwrite(path + &#39;.jpg&#39;, image)

def select_object(image):
sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;sam_vit_b_01ec64.pth&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
mask = mask_generator.generate(image)
largest_mask = max(masks, key=lambda x: x[&#39;area&#39;])
返回 largest_mask[&#39;segmentation&#39;]

def crop_to_object(image, mask):
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
padding = 5
x = max(0, x - padding)
y = max(0, y - padding)
w = min(image.shape[1] - x, w + 2 * padding)
h = min(image.shape[0] - y, h + 2 * padding)

cropped_image = image[y:y+h, x:x+w]
返回 cropped_image

def resize_to_square(image, size=800):
h, w = image.shape[:2]
scale = size / max(h, w)
new_h, new_w = int(h * scale), int(w * scale)
resized_image = cv2.resize(image, (new_w, new_h), 插值=cv2.INTER_LANCZOS4)

new_image = np.ones((size, size, 3), dtype=np.uint8) * 255

top = (size - new_h) // 2
left = (size - new_w) // 2
bottom = top + new_h
right = left + new_w

new_image[top:top+new_h, left:left+new_w] = resized_image

return new_image

def process_image(image_path, output_path):

image = load_image(image_path)
mask = select_object(image)
cropped_image = crop_to_object(image, mask)
final_image = resize_to_square(cropped_image, 800)
save_image(final_image, output_path + &#39;.jpg&#39;)

def process_folder(input_folder, output_folder):

如果 os.path.exists(output_folder):
os.makedirs(output_folder)

对于 root, _, files in os.walk(input_folder):
对于 filename in files:
如果 filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;, &#39;.bmp&#39;, &#39;.tiff&#39;)):
input_path = os.path.join(root, filename)

relative_path = os.path.relpath(input_path, input_folder)
output_path = os.path.join(output_folder,relative_path)

output_dir = os.path.dirname(output_path)
如果 os.path.exists(output_dir):
os.makedirs(output_dir)

尝试:
process_image(input_path, output_path)
print(f&quot;已处理 {input_path}&quot;)
except Exception as e:
print(f&quot;无法处理 {input_path}: {e}&quot;)

if __name__ == &quot;__main__&quot;:
input_folder = &quot;&quot;
output_folder = &quot;&quot;
process_folder(input_folder, output_folder)

发生了什么：我有一张 800x800 像素的 jpg 图片。但背景是黑色的，根本不在中心。
有人能帮我理解我错过了什么吗？
提前谢谢，
Cyril]]></description>
      <guid>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</guid>
      <pubDate>Wed, 05 Jun 2024 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>llama-index、uncharted 和 llama2:7b 在本地运行以生成索引</title>
      <link>https://stackoverflow.com/questions/78581041/llama-index-uncharted-and-llama27b-run-locally-to-generate-index</link>
      <description><![CDATA[我想在本地使用 llama-index 和 ollama 以及 llama3:8b 来索引 utf-8 json 文件。我没有 gpu。我使用 uncharted 将文档转换为 json。现在，如果没有 GPU 就无法在本地使用 llama-index，我想使用 hugging face 推理 API。但我不确定它是否免费。有人能建议一种方法吗？
这是我的 python 代码：


from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.ollama import Ollama
import json
from llama_index.core import Settings

# 将 JSON 文档转换为 LlamaIndex Document 对象
with open(&#39;data/UBER_2019.json&#39;, &#39;r&#39;,encoding=&#39;utf-8&#39;) as f:
json_doc = json.load(f)
documents = [Document(text=str(doc)) for doc in json_doc]

# 使用本地 LLM 初始化 Ollama
ollama_llm = Ollama(model=&quot;llama3:8b&quot;)
Settings.llm = ollama_llm

# 使用本地 LLM 创建索引
index = VectorStoreIndex.from_documents(documents)#, llm=ollama_llm)


但我一直收到没有 OPENAI 密钥的错误。我想使用 llama2，这样就不需要 OPENAI 密钥了
有人能指出我做错了什么吗？我还可以免费使用 huggingfaceinference API 对本地 json 文件进行索引吗？]]></description>
      <guid>https://stackoverflow.com/questions/78581041/llama-index-uncharted-and-llama27b-run-locally-to-generate-index</guid>
      <pubDate>Wed, 05 Jun 2024 12:38:14 GMT</pubDate>
    </item>
    <item>
      <title>ViTHybrid 无法添加位置嵌入和嵌入</title>
      <link>https://stackoverflow.com/questions/78581025/vithybrid-cant-add-positional-embeddings-and-embeddings</link>
      <description><![CDATA[当我创建一个新模型并为其提供随机大小的数据作为输入 [1, 3, 224, 224] 时，我得到了 embeddings 和 positional_embeddings 维度错误
model = ViTHybridModel(ViTHybridConfig(backbone_config = {
&quot;depths&quot;: [3, 4, 16, 3],
&quot;hidden_​​sizes&quot;: [128, 256, 512, 1024],
&quot;layer_type&quot;: &quot;bottleneck&quot;
}, image_size=224)

torch.Size([1, 3, 224, 224])
Traceback（最近一次调用最后一次）：
文件 &quot;D:\sddif\itestingvit.py&quot;，第 17 行，位于&lt;module&gt;
输出 = 模型 (输入 [&quot;pixel_values&quot;])
文件 &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，位于 _wrapped_call_impl
返回 self._call_impl(*args, **kwargs)
文件 &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，位于 _call_impl
返回 forward_call(*args, **kwargs)
文件&quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\vit_hybrid\modeling_vit_hybrid.py&quot;，第 588 行，在 forward
embedding_output = self.embeddings(
文件 &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl
return self._call_impl(*args, **kwargs)
文件 &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl
return forward_call(*args, **kwargs)
文件“C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\vit_hybrid\modeling_vit_hybrid.py”，第 128 行，正向
embeddings = embeddings + self.position_embeddings
RuntimeError：张量 a (50) 的大小必须与非单例维度 1 上的张量 b (577) 的大小匹配

]]></description>
      <guid>https://stackoverflow.com/questions/78581025/vithybrid-cant-add-positional-embeddings-and-embeddings</guid>
      <pubDate>Wed, 05 Jun 2024 12:35:10 GMT</pubDate>
    </item>
    <item>
      <title>最新版本的 Mask-RCNN 和 TensorFlow 版本的错误</title>
      <link>https://stackoverflow.com/questions/78579595/newest-version-of-working-mask-rcnn-errors-on-tensorflow-version</link>
      <description><![CDATA[我一直在尝试在生物实验室的细胞图像上实现 Mask-RCNN。
我知道 matterport/Mask_RCNN 无法正常工作，因为它使用的是 TensorFlow 1，所以我尝试使用使用 TensorFlow 2 的 github repos。但我仍然觉得有些已经过时了，或者我的设置不匹配，它没有运行。我一直在使用这个：https://github.com/ahmedfgad/Mask-RCNN-TF2
是否有 2024 年或 2023 年的最新版本可以作为参考？我真的很想尝试在我的系统上实现，但是当我尝试从同一位置修复问题时，不断收到类似 ModuleNotFoundError: 没有名为“keras.engine”的模块 或 ERROR: 找不到满足要求 tensorflow==2.2.0 的版本的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78579595/newest-version-of-working-mask-rcnn-errors-on-tensorflow-version</guid>
      <pubDate>Wed, 05 Jun 2024 08:11:47 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Keras 创建小批量？</title>
      <link>https://stackoverflow.com/questions/78577993/how-to-create-minibatch-with-keras</link>
      <description><![CDATA[假设我有 20 个 5*5 张量。我该如何创建一个 batchsize = 20 的数据集？
在阅读了很多帖子后，我认为最有可能的解决方案是：
步骤 1.使用 tf.dataset.from_tensors 创建 20 个数据集，每个数据集包含一个 5*5 张量。
步骤 2.使用 tf.dataset.zips 将 20 个数据集压缩为一个大数据集。
步骤 3.不要在 model.fit 中声明 batch_size=20，因为从官方文档中可以看到，“如果您的数据是数据集、生成器或 keras.utils.PyDataset 实例的形式，请不要指定 batch_size（因为它们会生成批次）”
我看不出上面的步骤是如何生成批次的。是不是因为数据集的形状（20*5*5）意味着小批次大小应该是 20，这等于第一个参数？
如果以下陈述是正确的。假设我想将批次大小减少到 10。我需要做的就是在步骤 1 之后先将两个数据集压缩 10 次，然后将新的 10 个数据集压缩为最终数据集。这是正确的吗？
此外，我尝试使用 tf.dataset.batch(20) 在步骤 2 之后应用大数据集。在我 tf.print 之前和之后的这个批处理命令的最终数据集之后，我得到的输出是：
之前：压缩数据集 (array(shape(5*5), array(shape(5*5)), …)
之后：批处理数据集 (array(shape(none*5*5), array(shape(none*5*5)), …)
形状不同。输出值后，我注意到应用 tf.dataset.batch(20) 后的实际形状为每个数组变为 1*5*5。
这个命令 tf.dataset.batch(20) 在我的例子中是无用的还是我应该在步骤 2 之后使用它？
官方文档只使用了一个例子是无量纲数组。所以我不知道这个命令对高阶张量如何起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78577993/how-to-create-minibatch-with-keras</guid>
      <pubDate>Tue, 04 Jun 2024 22:21:52 GMT</pubDate>
    </item>
    <item>
      <title>请问如何改进我的混合 1D CNN 和 Bi-LSTM 模型以实现高精度</title>
      <link>https://stackoverflow.com/questions/78575294/please-how-can-improve-my-hybrid-1d-cnn-and-bi-lstm-model-for-high-accuracy</link>
      <description><![CDATA[我正在构建一个混合 1D CNN 和 Bi-LSTM 模型，用于预测心脏病。然而，该模型的准确率是 0.73，但我想将其提高到 0.80 及以上。请就如何改进此模型提供任何帮助。谢谢。我期望准确率能稍微提高一点。
我的输入形状如下所示 (70000,13)
import tensorflow as tf 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling2D, MaxPooling1D, LSTM, Bidirectional, Dense, Flatten, Dropout, Input, BatchNormalization, Reshape
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils.class_weight import compute_class_weight 

dataset = pd.read_csv(&#39;heart_disease.csv&#39;)
dataset.shape

#预处理数据集
X = dataset.drop(columns=[&#39;disease&#39;])
y = dataset[&#39;disease&#39;]

`#标准化特征
scaler = StandardScaler()
X = scaler.fit_transform(X)
# print(X)

#将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(&quot;训练集大小：&quot;, X_train.shape)
print(&quot;测试集大小：&quot;, X_test.shape)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#重塑数据1D CNN + Bi-LSTM 模型
X_train_dl = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_dl = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

#print(X_train_dl)
#print(X_test_dl)

# 构建混合模型

model = Sequential()
model.add(Input(shape=(X_train_dl.shape[1], X_train_dl.shape[2])))

model.add(Conv1D(filters=32, kernel_size=2,activation=&#39;relu&#39;))

model.add(MaxPooling1D(pool_size=2))
model.add(BatchNormalization(momentum=0.99))

model.add(Conv1D(filters=64, kernel_size=2,activation=&#39;relu&#39;))
model.add(MaxPooling1D(pool_size=2))
model.add(BatchNormalization(momentum=0.99))

model.add(Bidirectional(LSTM(50, return_sequences=True)))
model.add(Dropout(0.5))
model.add(Flatten())

`model.add(Dense(128,activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(Dropout(0.5))

model.add(Dense(1,activation=&#39;sigmoid&#39;))

#编译模型

model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = 0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 提前停止回调
early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, waiting=20, restore_best_weights=True)

# 训练模型
history = model.fit(X_train_dl, y_train, epochs=10, batch_size=32, validation_data=(X_test_dl, y_test), callbacks=[early_stopping])

# 保存模型
model.save(&#39;my_model.keras&#39;)

# 评估模型
loss accuracy = model.evaluate(X_test_dl, y_test)
print(f &quot;混合模型 (1D CNN + Bi-LSTM) 准确率： {准确度：.2f}&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/78575294/please-how-can-improve-my-hybrid-1d-cnn-and-bi-lstm-model-for-high-accuracy</guid>
      <pubDate>Tue, 04 Jun 2024 12:04:25 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 KL 散度损失进行知识蒸馏。损失太高了</title>
      <link>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</link>
      <description><![CDATA[KL 散度损失过高
我正在尝试进行知识提炼。对于我的学生损失，我使用了交叉熵损失，对于我的知识蒸馏损失，我尝试使用 KL 散度损失。
这是我用于 KL 散度损失的代码。
class KLDivLoss(nn.Module):
    def __init__(self,ignore_index=-1, reduction=&quot;batchmean&quot;, log_target=False):
        super(KLDivLoss, self).__init__()
        self.reduction = reduction
        self.log_target = log_target
        self.ignore_index = ignore_index

    def forward(self, preds_S, preds_T, T =1.0, alpha = 1.0):
        preds_T[0] = preds_T[0].detach()  # 分离教师预测
        pred_1 = torch.sigmoid(preds_T[0]/T) # 白色
        pred_0 = 1 - pred_1
        preds_teacher = torch.cat((pred_0, pred_1), dim=1)
        assert preds_S[0].shape == preds_teacher.shape, “输入和目标形状必须匹配 KLDivLoss”
       stu_prob = F.log_softmax(preds_S[0]/T, dim=1)
        kd_loss = F.kl_div(stu_prob, 
                           preds_teacher, 
                             reduction=&#39;batchmean&#39;,
                           ) * T * T
        return {&#39;loss&#39;: kd_loss}

我从中得到的值非常大。我只是从学生模型中添加了知识蒸馏损失和交叉熵损失。由于我的 CE 损失非常小，这全都来自 KLdiv 损失。你能告诉我如何减少损失吗？或者如果我做错了什么。
在此处输入图片说明
我尝试使用 KL div 损失，其中温度 =1
我的老师模型以张量 [8,1,224,224] 的形式给出输出，因为它用于像素的二进制预测，而我的学生模型以 [8,2,224,224] 的形式给出输出，其中 0 属于黑色类，1 属于白色。
因此，为了将它们与 KL div 损失相匹配，我使用 sigmoid 函数来获取白色类的概率和 1 - 黑色的白色概率。然后将它们连接起来形成一个大小为 [8,2,224,224] 的张量，这与学生张量相似。
然后我尝试执行 KL 发散。我遭受的损失非常大]]></description>
      <guid>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</guid>
      <pubDate>Thu, 09 May 2024 06:27:53 GMT</pubDate>
    </item>
    <item>
      <title>训练 DL 模型时，本地会合中止，状态为：OUT_OF_RANGE：序列结束</title>
      <link>https://stackoverflow.com/questions/78376338/while-training-dl-model-local-rendezvous-is-aborting-with-status-out-of-range</link>
      <description><![CDATA[我正在创建一个植物疾病识别模型。我有一个包含 38 种疾病的数据集，每种疾病大约有 2000 张图像。但在训练模型时，由于某些 OUT_OF_RANGE 错误，一些时期被跳过。有人能帮我解决这个问题吗？
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input

train_dir = &#39;dataset/train&#39;
valid_dir = &#39;dataset/valid&#39;
batch_size = 32

train_datagen = ImageDataGenerator(
rescale=1./255,
rotation_range=40,
width_shift_range=0.2,
height_shift_range=0.2,
sher_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,
fill_mode=&#39;nearest&#39;
)

valid_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

valid_generator = valid_datagen.flow_from_directory(
valid_dir,
target_size=(150, 150),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

model = Sequential([
输入(shape=(150, 150, 3)),
Conv2D(32, (3, 3), 激活=&#39;relu&#39;),
MaxPooling2D(2, 2),
Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
MaxPooling2D(2, 2),
Conv2D(128, (3, 3), 激活=&#39;relu&#39;),
MaxPooling2D(2, 2),
Flatten(),
Dense(512,activation=&#39;relu&#39;),
Dense(38,activation=&#39;softmax&#39;) # 根据疾病类别数量调整输出单元
])

model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

history = model.fit(
train_generator,
steps_per_epoch=train_generator.samples //batch_size,
epochs=10,
validation_data=valid_generator,
validation_steps=valid_generator.samples //batch_size
)

model.save(&#39;plant_disease_model.h5&#39;)

class_indices = train_generator.class_indices
disease_names = list(class_indices.keys())
print(&quot;类别索引到疾病名称的映射：&quot;, class_indices)

终端：
找到属于 38 个类别的 70295 张图像。
找到属于 38 个类别的 17572 张图像。
2024-04-23 19:50:32.085744：I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 FMA，在其他操作中，请使用适当的编译器标志重建 TensorFlow。
纪元 1/10
\.venv\Lib\site-packages\keras\src\trainers\data_adapters\py_dataset_adapter.p
y:120：UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_m
ultiprocessing`、`max_queue_size`。请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()
←[1m2196/2196←[0m ←[32m━━━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m905s←[0m 411ms/step - 准确度：0.4608 - 损失：1.8737 - val_accuracy：0.7432 - val_
loss：0.8556
Epoch 2/10
←[1m 1/2196←[0m ←[37m━━━━━━━━━━━━━━━━━━━━━←[0m ←[1m12:02←[0m 329ms/step - 准确度：0.6875 - 损失：0.78202024-04-23 20:05:37.996528：W tensorfl
ow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
C:\Users\Admin\AppData\Local\Programs\Python\Python311\Lib\contextlib.py:155：UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 个批次。您可能需要在构建数据集时使用 `.repeat()` 函数。
self.gen.throw(typ, value, traceback)
2024-04-23 20:05:38.068817: W tensorflow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
←[1m2196/2196←[0m ←[32m━━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 49us/step - 精度：0.6875 - 损失：0.7820 - val_accuracy： 0.7500 - val_los
s: 0.2462

如上所示，epoch 1 成功完成，但 epoch 2 因某些错误而终止。同样，epoch 3、5、7、9 成功完成，但 epoch 4、6、8、10 出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78376338/while-training-dl-model-local-rendezvous-is-aborting-with-status-out-of-range</guid>
      <pubDate>Wed, 24 Apr 2024 06:27:20 GMT</pubDate>
    </item>
    <item>
      <title>Statsmodels - 使用经过训练的 arima 模型通过明确提供 endog 值来进行手动点预测</title>
      <link>https://stackoverflow.com/questions/56971901/statsmodels-use-trained-arima-model-to-do-manual-point-prediction-by-explicitl</link>
      <description><![CDATA[我正在使用 statsmodels 库来提供 ARIMAX 模型，用于预测时间序列。我有一个相当奇怪的问题 - 如何通过明确提供用于预测的 endog 和 exog 变量来强制训练模型执行完全手动点预测？
为了给你一个想法，我使用 2000-2017 年的年度数据训练我的模型，其中我根据前几年的劳动力和一堆 exog 变量预测公司未来的劳动力。它效果很好。问题是，2018 年和 2019 年，公司大幅扩大了员工数量，这是一次性的商业决策，我们也知道，从商业角度来看，我们在 2000-2017 年训练的模型是“正确的”。
我想要做的是使用我在 2000-2017 年训练的模型，并提供 2020 年的预测，同时明确提供 2018 年和 2019 年的“实际值”。这样，我们就可以确保模型不会试图适应这种一次性的跳跃，从而降低其质量。但我该怎么做呢？请注意，我使用 AR(2) 模型 - 因此我需要提供前 2 年的数据。
我见过一些 statsmodels 方法，它们允许您：
1) 选择经过训练的 ARIMAX 模型
2) 明确提供前 2 年的 exog 变量值
3) 明确提供前 2 年的 endog 值
4) 仅提供单点预测
predict 和 forecast 方法仅允许您指定要提供样本外预测的步数，但不允许明确提供用于预测的新内源值]]></description>
      <guid>https://stackoverflow.com/questions/56971901/statsmodels-use-trained-arima-model-to-do-manual-point-prediction-by-explicitl</guid>
      <pubDate>Wed, 10 Jul 2019 13:34:27 GMT</pubDate>
    </item>
    </channel>
</rss>