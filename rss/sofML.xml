<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 02 Dec 2024 06:27:50 GMT</lastBuildDate>
    <item>
      <title>在 Kaggle 中使用 Thundersvm</title>
      <link>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</link>
      <description><![CDATA[当我想使用 &#39;!pip install thundersvm` 在 kaggle 中安装 thundersvm 时，我遇到了此错误：
---------------------------------------------------------------------------
OSError Traceback (most recent call last)
Cell In[6], line 3
1 get_ipython().run_line_magic(&#39;pip&#39;, &#39;install thundersvm&#39;)
2 get_ipython().run_line_magic(&#39;pip&#39;, &#39;install keras_tuner&#39;)
----&gt; 3 from thundersvm import SVC
4 from sklearn.preprocessing import StandardScaler
5 from sklearn.metrics import classes_report

File /opt/conda/lib/python3.10/site-packages/thundersvm/__init__.py:10
3 &quot;&quot;&quot;
4 * 名称 : __init__.py
5 * 作者 : Locke &lt;luojiahuan001@gmail.com&gt;
6 * 版本 : 0.0.1
7 * 说明 :
8 &quot;&quot;&quot;
9 name = &quot;thundersvm&quot;
---&gt; 10 from .thundersvm import *

File /opt/conda/lib/python3.10/site-packages/thundersvm/thundersvm.py:39
36 lib_path = path.join(dirname, shared_library_name)
38 if path.exists(lib_path):
---&gt; 39 thundersvm = CDLL(lib_path)
40 else:
41 # 尝试构建目录
42 if platform == &quot;linux&quot;或平台 == &quot;linux2&quot;:

文件 /opt/conda/lib/python3.10/ctypes/__init__.py:374，在 CDLL.__init__(self, name, mode, handle, use_errno, use_last_error, winmode) 中
371 self._FuncPtr = _FuncPtr
373 如果句柄为 None:
--&gt; 374 self._handle = _dlopen(self._name, mode)
375 else:
376 self._handle = handle

OSError: libcusparse.so.9.0: 无法打开共享对象文件：没有此文件或目录

为了修复此问题，我尝试了以下方法：
# 下载并安装 CUDA 9.0
wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run
sudo sh cuda_9.0.176_384.81_linux-run

此方法也不起作用。我该如何修复此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</guid>
      <pubDate>Mon, 02 Dec 2024 06:08:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中使用 ImageAI 加载自定义训练的 ResNet50 模型？</title>
      <link>https://stackoverflow.com/questions/79242938/how-to-load-a-custom-trained-resnet50-model-with-imageai-in-python</link>
      <description><![CDATA[我正在使用 ImageAI 的自定义训练 ResNet50 模型进行图像分类项目，但遇到了持续的加载错误，导致我无法使用训练好的模型进行预测。该错误表明在尝试将自定义训练的 PyTorch 模型加载到 ImageAI 框架时存在兼容性问题或缺少特定方法。
我尝试使用 ImageAI 的标准模型加载程序加载自定义训练的 ResNet50 模型：
from imageai.Classification import ImageClassification

classifier = ImageClassification()
classifier.setModelPath(&quot;path/to/custom/model.pt&quot;)
classifier.setModelTypeAsResNet50()
classifier.loadModel() # 预计会成功加载模型

我期望的是一个类似于使用预训练模型的简单模型加载过程。相反，我收到了一个令人沮丧的错误：
&quot;分类失败：模型尚未加载。您需要在执行图像分类之前调用​​&#39;.loadModel()&#39;&quot;

我的环境详情：

Python 3.12
ImageAI 3.0.3
PyTorch 2.5.1+cu118
Windows 11

我已确认：

模型路径正确
模型已保存为有效的 .pt 文件
模型使用 ResNet50 架构在自定义数据集上进行训练

具体问题：

在 ImageAI 中是否有加载自定义训练模型的特定方法？
自定义模型是否有任何特殊的导出或转换要求？
我需要修改模型的状态字典或使用特定的导出吗格式？
]]></description>
      <guid>https://stackoverflow.com/questions/79242938/how-to-load-a-custom-trained-resnet50-model-with-imageai-in-python</guid>
      <pubDate>Mon, 02 Dec 2024 04:07:18 GMT</pubDate>
    </item>
    <item>
      <title>如何处理图神经网络训练中不同的特征维度？</title>
      <link>https://stackoverflow.com/questions/79242479/how-to-handle-varying-feature-dimensions-in-graph-neural-networks-training</link>
      <description><![CDATA[我有一个问题，在训练图神经网络 (GNN) 时如何处理具有不同特征维度的数据集。例如，在一个数据集（数据集 A）中，节点特征的维度为 4，边特征的维度为 16。在另一个数据集（数据集 B）中，节点特征为 5 维，边特征为 25 维。我可能使用的其他数据集也具有不同的特征大小。
在训练 GNN 时，您通常如何处理这种特征维度的变化？]]></description>
      <guid>https://stackoverflow.com/questions/79242479/how-to-handle-varying-feature-dimensions-in-graph-neural-networks-training</guid>
      <pubDate>Sun, 01 Dec 2024 21:06:57 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 模型在 Google Colab 中占用大量 RAM</title>
      <link>https://stackoverflow.com/questions/79242350/tensorflow-model-takes-so-much-ram-in-google-colab</link>
      <description><![CDATA[当我尝试训练这个模型时，这个模型甚至没有很多可训练的参数，它在 google colab 中崩溃了，因为它使用了所有的内存，我有一个 mp3 文件的数据集，每个 mp3 文件都是 128 kbps 和 3-4 秒长，我尝试制作一个音频分类器，但它不起作用，它需要太多的内存来进行训练，而且训练也很慢。有人能帮帮我吗？我使用 tensorflow 2.10.0 和 tensorflow-io 0.27.0，因为较新的版本存在其他问题。
import tensorflow as tf
import tensorflow_io as tfio
import os
import random
import joblib

BATCH_SIZE = 4
def load_sound(filename):
res = tfio.audio.AudioIOTensor(filename, dtype=tf.float32)
tensor = res.to_tensor()
tensor = tf.math.reduce_sum(tensor,axis = 1) / 2

sample_rate = res.rate

sample_rate = tf.cast(sample_rate, dtype=tf.int64)

wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)

返回 wav

base_dir = &#39;/content/drive/MyDrive/house_sounds/sound_data&#39;
folders = [&#39;door&#39;, &#39;voice&#39;, &#39;glass&#39;, &#39;footsteps&#39;]
files = []

对于文件夹中的文件夹：
folder_path = os.path.join(base_dir, folder)
file_paths = tf.data.Dataset.list_files(os.path.join(folder_path, &#39;*.mp3&#39;))
files.append(file_paths)

door = tf.data.Dataset.zip((files[0], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[0])],0))))
voice = tf.data.Dataset.zip((files[1], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[1])],1))))
glass = tf.data.Dataset.zip((files[2], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[2])],2))))
footsteps = tf.data.Dataset.zip((files[3], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[3])],3))))

data = door.concatenate(voice)
data = data.concatenate(glass)
data = data.concatenate(footsteps)

def create_spectrogram(file_path, label):
wav = load_sound(file_path)
wav = wav[:48000]
zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)
wav = tf.concat([zero_padding, wav], 0)

频谱图 = tf.signal.stft(wav, frame_length=320, frame_step=32)
频谱图 = tf.abs(频谱图)
频谱图 = tf.expand_dims(频谱图, axis=2)
返回频谱图, 标签

数据 = data.map(create_spectrogram)
数据 = data.cache()
数据 = data.shuffle(buffer_size = 1000)
数据 = data.batch(4)
数据 = data.prefetch(8)
print(&#39;Len: &#39;,len(data))
train = data.take(1600)
测试 = data.skip(1600).take(247)

从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Conv2D、Dense、Flatten、MaxPooling2D

model = Sequential([
Conv2D(16, (2,2), 激活=&quot;relu&quot;, input_shape=(1491,257,1)),
MaxPooling2D(pool_size=(5, 5)),
Conv2D(32, (2, 2), 激活=&#39;relu&#39;),
MaxPooling2D(pool_size=(5, 5)),
Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
MaxPooling2D(pool_size=(5, 5)),
Flatten(),
Dense(32, 激活=&#39;relu&#39;),
Dense(4,激活=&quot;softmax&quot;)
])

model.compile(
优化器=&#39;Adam&#39;,
损失=tf.keras.losses.SparseCategoricalCrossentropy(),
指标=[&#39;准确度&#39;]
)
model.summary()
hist = model.fit(训练，epochs=5，validation_data=测试)
]]></description>
      <guid>https://stackoverflow.com/questions/79242350/tensorflow-model-takes-so-much-ram-in-google-colab</guid>
      <pubDate>Sun, 01 Dec 2024 19:35:18 GMT</pubDate>
    </item>
    <item>
      <title>Catboost RAM 在训练结束时崩溃</title>
      <link>https://stackoverflow.com/questions/79241814/catboost-ram-crash-in-the-end-of-training</link>
      <description><![CDATA[我在各种数据集上训练 Catboost 模型，并面临一个重复的问题：一旦最后一棵树被拟合，RAM 消耗就会开始比训练期间增长得更多，这通常会导致 jupyter 内核崩溃。是否有任何类型的最终化作业可以禁用以防止内存溢出？
我尝试传递 used_ram_limit 参数，但在拟合最后一棵树后，它似乎对 RAM 消耗没有影响]]></description>
      <guid>https://stackoverflow.com/questions/79241814/catboost-ram-crash-in-the-end-of-training</guid>
      <pubDate>Sun, 01 Dec 2024 14:46:53 GMT</pubDate>
    </item>
    <item>
      <title>set_transform 或 with_transform 之后 transformer 的数据集结构出现意外</title>
      <link>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</guid>
      <pubDate>Sun, 01 Dec 2024 14:07:14 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 不接受数据集生成器的列表类型</title>
      <link>https://stackoverflow.com/questions/79241634/tensorflow-does-not-accept-list-type-for-dataset-generator</link>
      <description><![CDATA[我正在构建一个神经网络。我无法一次性将所有训练数据加载到内存中，因此我使用 TensorFlow 的 tf.data.Dataset.from_generator 函数逐步加载数据。但是，它会抛出一个错误，指出它不接受张量列表作为类型。
TypeError：`output_signature` 必须包含属于 
`tf.TypeSpec` 子类的对象，但发现 &lt;class &#39;list&#39;&gt; 不是。

我的神经网络的输入是 151 个独立张量的列表。我如何在生成器中表示它？我的代码如下：
def generator(file_paths, batch_size, files_per_batch, tam, value):
return tf.data.Dataset.from_generator(
lambda: data_generator(file_paths, batch_size, files_per_batch, tam, value),
output_signature=(
[tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for _ in range(tam+1)], # 151 个张量列表
tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) # 数组
)
)

inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]

train_dataset = generator(file_paths, batch_size, files_per_batch, tam, False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

model.fit(train_dataset, epochs=1000, validation_split=0.2, verbose=1)

我尝试使用 tf.data.Dataset.from_generator 将数据批量输入到我的神经网络中，因为我无法一次将所有数据加载到内存中。
但是，我遇到了一个错误：
TypeError：output_signature 必须包含属于 tf.TypeSpec 子类的对象，但发现 &lt;class &#39;list&#39;&gt; 不是。
]]></description>
      <guid>https://stackoverflow.com/questions/79241634/tensorflow-does-not-accept-list-type-for-dataset-generator</guid>
      <pubDate>Sun, 01 Dec 2024 13:13:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中实现 Softmax，其中输入是有符号的 8 个整数</title>
      <link>https://stackoverflow.com/questions/79239232/how-to-implement-softmax-in-python-whereby-the-input-are-signed-8-integers</link>
      <description><![CDATA[我正在尝试实现一个softmax 函数，该函数接受有符号的 int8 输入并返回有符号的 int8 输出数组。
我目前正在进行的实现是这样的，
 import numpy as np

def softmax_int8(inputs):
input = np.array(inputs, dtype=np.int8)

x = input.astype(np.int32)
x_max = np.max(x)
x_shifted = x - x_max
scale_factor = 2 ** 14
exp_limit = 16
exp_x = np.clip(x_shifted + exp_limit, 0, None)
exp_x = (1 &lt;&lt; exp_x)
sum_exp_x = np.sum(exp_x)

如果 sum_exp_x == 0:
sum_exp_x = 1

softmax_probs = (exp_x * scale_factor) // sum_exp_x
max_prob = np.max(softmax_probs)
min_prob = np.min(softmax_probs)
range_prob = max_prob - min_prob 如果 max_prob != min_prob 否则 1

scaled_probs = ((softmax_probs - min_prob) * 255) // range_prob - 128
output = scaled_probs.astype(np.int8)

返回输出

我使用此输入进行测试，Input = [101, 49, 6, -34, -75, -79, -38, 120, -55, 115]
但我得到此输出 array([-128, -128, -128, -128, -128, -128, -128, 127, -128, -121],dtype=int8)。
我的预期输出是 array([-57, -70, -79, -86, -92, -94, -88, -54, -91, -56], dtype=int8)。
我在这里做错了什么，我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/79239232/how-to-implement-softmax-in-python-whereby-the-input-are-signed-8-integers</guid>
      <pubDate>Sat, 30 Nov 2024 09:37:57 GMT</pubDate>
    </item>
    <item>
      <title>正在访问非叶张量的张量的 .grad 属性</title>
      <link>https://stackoverflow.com/questions/79238710/grad-attribute-of-a-tensor-that-is-not-a-leaf-tensor-is-being-accessed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79238710/grad-attribute-of-a-tensor-that-is-not-a-leaf-tensor-is-being-accessed</guid>
      <pubDate>Sat, 30 Nov 2024 02:10:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在拆分后合并 tarin 和测试数据集但每行回到其原始位置或索引</title>
      <link>https://stackoverflow.com/questions/79238631/how-to-merge-tarin-and-test-dataset-after-splitting-but-each-row-back-to-its-ori</link>
      <description><![CDATA[from sklearn.datasets import load_diabetes
import pandas as pd
import numpy as np
from sklearn.datasets import fetch_openml
# 加载糖尿病数据集 
diabetes = fetch_openml(&quot;diabetes&quot;, version=1, as_frame=True)
diabetes_df = diabetes.data
diabetes_df[&#39;target&#39;] = diabetes.target # 添加目标列

from sklearn.model_selection import train_test_split
# 拆分数据
train, test = train_test_split(diabetes_df, test_size=0.3, random_state=42)

我想合并训练和测试数据集，但与原始数据索引相同。
注意：但在出现一些失误后，我将数据集拆分为训练和测试数据集，并分别估算训练和测试数据集]]></description>
      <guid>https://stackoverflow.com/questions/79238631/how-to-merge-tarin-and-test-dataset-after-splitting-but-each-row-back-to-its-ori</guid>
      <pubDate>Sat, 30 Nov 2024 00:21:23 GMT</pubDate>
    </item>
    <item>
      <title>如何比较不同年份的集群？</title>
      <link>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</link>
      <description><![CDATA[我有多个数据集，所有数据集的组织方式都类似（相同的变量、值等）。我使用 KModes 独立分析了数据集，但是，我试图寻找多年来可能出现的趋势。我该如何比较不同年份的集群？]]></description>
      <guid>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</guid>
      <pubDate>Thu, 28 Nov 2024 15:14:27 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的 GaussianProcessRegressor 估计器可以在多核上并行化吗？</title>
      <link>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</link>
      <description><![CDATA[在具有 8 个内核（16 个线程）的机器上使用 GaussianProcessRegressor 时，我没有注意到任何性能改进，尽管我只使用物理内核。所以我想知道，sklearn.gaussian_process 中的 GaussianProcessRegressor 类是否能够利用多个处理器/内核/线程？
当前场景
4 个内核情况下的时间：0.57
8 个内核情况下的时间：0.56

加速不明显。这次只是将 fit_transform 的范围限定在数据块上。因此没有开销计时。]]></description>
      <guid>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</guid>
      <pubDate>Thu, 28 Nov 2024 03:36:56 GMT</pubDate>
    </item>
    <item>
      <title>在 AdaBoostClassifier 中使用 scikit-learn 的 MLPClassifier</title>
      <link>https://stackoverflow.com/questions/55632010/using-scikit-learns-mlpclassifier-in-adaboostclassifier</link>
      <description><![CDATA[对于二元分类问题，我想使用 MLPClassifier 作为 AdaBoostClassifier 中的基本估计器。但是，这不起作用，因为 MLPClassifier 未实现 AdaBoostClassifier 所需的 sample_weight（请参阅 此处）。在此之前，我尝试在 AdaBoostClassifier 中使用 Keras 模型和 KerasClassifier，但这也不起作用，如 此处 所述。 
用户 V1nc3nt 提出的一种方法是在 TensorFlow 中构建自己的 MLPclassifier 并考虑 sample_weight。
用户 V1nc3nt 分享了他的大部分代码，但由于我对 Tensorflow 的经验有限，我无法填补缺失的部分。因此，我想知道是否有人找到了从 MLP 构建 Adaboost 集合的有效解决方案，或者可以帮助我完成 V1nc3nt 提出的解决方案。
提前非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/55632010/using-scikit-learns-mlpclassifier-in-adaboostclassifier</guid>
      <pubDate>Thu, 11 Apr 2019 12:00:49 GMT</pubDate>
    </item>
    <item>
      <title>NotFittedError：估计器不适合，在利用模型之前调用“fit”</title>
      <link>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</link>
      <description><![CDATA[我在 Macbook OSX 10.2.1 (Sierra) 上运行 Python 3.5.2。
尝试运行 Kaggle 的 Titanic 数据集的一些代码时，我不断收到以下错误：


NotFittedError Traceback (most recent call
last) in ()
6 
7 # 使用测试集进行预测并打印它们。
----&gt; 8 my_prediction = my_tree_one.predict(test_features)
9 print(my_prediction)
10 
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input) 中
429 &quot;&quot;&quot;
430 
--&gt; 431 X = self._validate_X_predict(X, check_input)
432 proba = self.tree_.predict(X)
433 n_samples = X.shape[0]
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input)
386 “”“每当有人试图预测、应用、predict_proba 时验证 X”””
387 if self.tree_ is None:
--&gt; 388 raise NotFittedError(“估算器未安装，&quot;
389 “在利用模型之前调用 fit。”)
390 
NotFittedError：估算器未安装，在利用
模型之前调用 fit。

有问题的代码似乎是这样的：
# 用中位数估算缺失值
test.Fare[152] = test.Fare.median()

# 从测试集中提取特征：Pclass、Sex、Age 和 Fare。
test_features = test[[&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;]].values

# 使用测试集进行预测并打印。
my_prediction = my_tree_one.predict(test_features)
print(my_prediction)

# 创建一个包含两列的数据框：PassengerId 和 Survived。 Survived 包含您的预测
PassengerId =np.array(test[&quot;PassengerId&quot;]).astype(int)
my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [&quot;Survived&quot;])
print(my_solution)

# 检查您的数据框是否有 418 个条目
print(my_solution.shape)

# 将您的解决方案写入名为 my_solution.csv 的 csv 文件
my_solution.to_csv(&quot;my_solution_one.csv&quot;, index_label = [&quot;PassengerId&quot;])

以下是其余部分的链接 代码。
由于我已经调用了“fit”函数，我无法理解此错误消息。我哪里做错了？感谢您的时间。
编辑：
结果发现该问题继承自上一个代码块。
# 拟合您的第一个决策树：my_tree_one
my_tree_one = tree.DecisionTreeClassifier()
my_tree_one = my_tree_one.fit(features_one, target)

# 查看所包含特征的重要性和分数
print(my_tree_one.feature_importances_)
print(my_tree_one.score(features_one, target))

使用以下行：
my_tree_one = my_tree_one.fit(features_one, target)
生成错误：

ValueError：输入包含 NaN、无穷大或对于
dtype(&#39;float32&#39;)。
]]></description>
      <guid>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</guid>
      <pubDate>Fri, 02 Dec 2016 17:10:22 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习奖励函数？</title>
      <link>https://stackoverflow.com/questions/18758615/how-can-i-learn-a-reward-function</link>
      <description><![CDATA[我正在尝试突破常规，为类似奥赛罗的游戏开发 AI。
我正在研究多种不同的技术和算法来确定最佳动作，例如 Negascout 和 MTD(f)。但是它们都需要一个好的评估函数。
我想出了一些可用于函数的可能指标 {A_0...A_n}
G(state) = p_0*A_0 + p_1*A_1 + ... +p_n*A_n

我想以某种方式找到 p_0 到 p_n 的良好值
一个建议是使用机器学习来生成函数的参数，但在阅读中，我发现诸如 Q 学习之类的算法都要求我已经有一个奖励函数。
此外，在阅读有关 Td(lambda) 的内容时，我注意到它甚至不需要手动编码指标。它会使用什么样的奖励函数来学习？
我的理解中缺少什么？]]></description>
      <guid>https://stackoverflow.com/questions/18758615/how-can-i-learn-a-reward-function</guid>
      <pubDate>Thu, 12 Sep 2013 07:56:20 GMT</pubDate>
    </item>
    </channel>
</rss>