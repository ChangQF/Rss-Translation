<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 25 Aug 2024 18:19:06 GMT</lastBuildDate>
    <item>
      <title>如何在决策树中使用直方图实现分箱条件？</title>
      <link>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</guid>
      <pubDate>Sun, 25 Aug 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么每次运行程序都会得到不同的准确度</title>
      <link>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</link>
      <description><![CDATA[我使用 Python 中的 keras tensorflow 训练模型。
另外，正如您在下面的代码中看到的，我使用了种子参数，但每次我用相同的数据运行相同的代码时，我都会面临不同的准确率百分比。
我的代码：
#Seed
tf.random.set_seed(42)
np.random.seed(42)
set_random_seed(42)
random.seed(42)

data = (&#39;data.csv&#39;)

data = pd.get_dummies(data, columns=[&#39;cp&#39;, &#39;restecg&#39;], drop_first=True)

X = data.drop(&#39;num&#39;, axis=1)
y = data[&#39;num&#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def create_model(optimizer=&#39;adam&#39;, init=&#39;glorot_uniform&#39;, neurons=[16, 8], dropout_rate=0.3):
model = Sequential()
model.add(Input(shape=(X_train.shape[1],)))
model.add(Dense(neurons[0],activation=&#39;relu&#39;, kernel_initializer=init))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(neurons[1],activation=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(1,activation=&#39;sigmoid&#39;))
model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
返回模型

param_grid = {
&#39;optimizer&#39;: [&#39;adam&#39;, &#39;rmsprop&#39;],
&#39;model__neurons&#39;: [[16, 8]],
&#39;model__init&#39;: [&#39;glorot_uniform&#39;, &#39;normal&#39;],
&#39;model__dropout_rate&#39;: [0.3],
&#39;epochs&#39;: [50], 
&#39;batch_size&#39;: [10],
}

model = KerasClassifier(model=create_model, verbose=0)

kfold = KFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, n_jobs=-1)
grid_search_result = grid_search.fit(X_train, y_train)

print(f&quot;最佳参数：{grid_search_result.best_params_}&quot;)

print(f&quot;最佳准确度：{grid_search_result.best_score_}&quot;)

best_model = grid_search_result.best_estimator_

keras_model = best_model.model
keras_model.trainable = False 

y_pred_prob = best_model.predict(X_test).flatten()
y_pred = np.where(y_pred_prob &gt; 0.5, 1, 0)

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f&#39;Accuracy (手动计算): {accuracy:.2f}&#39;)
print(f&#39;ROC AUC: {roc_auc:.2f}&#39;)


我需要每次都获得相同的准确度。
我该如何解决这个问题？
谢谢帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</guid>
      <pubDate>Sun, 25 Aug 2024 17:25:44 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 项目模板创建失败</title>
      <link>https://stackoverflow.com/questions/78911757/aws-sagemaker-project-template-creation-failing</link>
      <description><![CDATA[我正在尝试使用 AWS Sagemaker Studio 中已提供的模板“模型构建、训练和部署”创建一个项目。但是，我遇到了以下错误：https://i.sstatic.net/e8MZgiMv.png
我执行了以下步骤：

在 SageMaker 中创建一个域。
域

向附加到域的 IAM 角色添加了其他策略（AWSCodeCommitFullAccess、AmazonS3FullAccess、AWSCloudFormationFullAccess）。
iam


我仍然收到上述错误。有什么线索可以说明我遗漏了什么吗？
PS：
AWSCodeCommit 的配额如下：codecommitquota
测试代码提交存储库创建：codecommitrepo]]></description>
      <guid>https://stackoverflow.com/questions/78911757/aws-sagemaker-project-template-creation-failing</guid>
      <pubDate>Sun, 25 Aug 2024 16:50:25 GMT</pubDate>
    </item>
    <item>
      <title>接收 Tensorflow Federated 错误：“无法运行计算：WhereOp：未处理的输入维度：0”</title>
      <link>https://stackoverflow.com/questions/78911566/receiving-tensorflow-federated-error-failed-to-run-computation-whereop-unha</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911566/receiving-tensorflow-federated-error-failed-to-run-computation-whereop-unha</guid>
      <pubDate>Sun, 25 Aug 2024 15:34:25 GMT</pubDate>
    </item>
    <item>
      <title>检测硬币图像的旋转角度</title>
      <link>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</link>
      <description><![CDATA[确定硬币图像角度的最佳方法是什么？
图像的分辨率是固定的（400x400），但可能在任何方向上偏离中心几个像素。
图像可以有不同的颜色、光照、旋转等。


我尝试过各种 CNN 想法，但都没有成功。看来，旋转后的图像之间的相似性不足以增强效果。
我无法找到用于此目的的现有数据集
请给我一些已被证明可行的想法，而不是理论]]></description>
      <guid>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</guid>
      <pubDate>Sun, 25 Aug 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>如何存储 MediaPipe GestureRecognizer 中的 understand_async 函数的结果</title>
      <link>https://stackoverflow.com/questions/78911213/how-to-store-the-result-of-the-recognize-async-function-from-mediapipe-gesturere</link>
      <description><![CDATA[我正在使用 MediaPipe 手势识别器任务来识别手势。我正在实时流模式下运行手势识别器任务，该任务使用recognize_async() 函数。
手势识别器会为每次识别运行生成一个手势检测结果对象。我如何将recognize_async() 函数的结果存储在变量中，以便在其余代码中使用它？
或者，是否可以在实时流模式下运行手势识别器任务并直接访问结果？这将允许我在代码中使用手势信息并对其执行其他逻辑或处理。
这是我的代码
import time
import mediapipe as mp
import cv2
from datetime import datetime

BaseOptions = mp.tasks.BaseOptions
GestureRecognizer = mp.tasks.vision.GestureRecognizer
GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions
GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult
VisionRunningMode = mp.tasks.vision.RunningMode

targets = {
&quot;Thumb_Down&quot;: 3,
&quot;Thumb_Up&quot;: 4,
}

last_gesture = 0

def save_gesture(result: GestureRecognizerResult, output_image: mp.Image，timestamp_ms：int)：
全局last_gesture

如果result.gestures：
gesture = result.gestures[0][0].category_name

如果gesture和gesture在目标中：
如果last_gesture !=gesture或last_gesture == 0：
last_gesture =gesture
print(gesture)

options = GestureRecognizerOptions(
base_options=BaseOptions(model_asset_path=&#39;gesture_recognizer.task&#39;)，
running_mode=VisionRunningMode.LIVE_STREAM，
result_callback=save_gesture)

recognizer = GestureRecognizer.create_from_options(options)

prev_timestamp_ms = 0

def detect_gesture()：

# 对于网络摄像头输入：
cap = cv2.VideoCapture(0)

while cap.isOpened()：
# 从摄像头捕获一帧
ret, frame = cap.read()

# 检查帧是否已成功捕获
if not ret:
print(&quot;Failed to capture a frame&quot;)
break

# 获取当前时间戳（以毫秒为单位）
current_timestamp_ms = int(time.time() * 1000)

# 检查时间戳是否单调递增
if current_timestamp_ms &gt; prev_timestamp_ms:

# 将从 OpenCV 接收到的帧转换为 MediaPipe 的图像对象。
mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
understander.recognize_async(mp_image, current_timestamp_ms)

# 显示捕获的帧
cv2.imshow(&#39;Camera Feed&#39;, frame)

# 按“q”退出循环
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

# 释放相机并关闭所有窗口
cap.release()
cv2.destroyAllWindows()

detect_gesture()

]]></description>
      <guid>https://stackoverflow.com/questions/78911213/how-to-store-the-result-of-the-recognize-async-function-from-mediapipe-gesturere</guid>
      <pubDate>Sun, 25 Aug 2024 12:26:37 GMT</pubDate>
    </item>
    <item>
      <title>使用对最终目标有贡献的特征子集来训练核岭回归</title>
      <link>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</link>
      <description><![CDATA[我正在使用 Python 中的核岭回归 (KRR) 和 scikit-learn 研究机器学习问题。我的目标是训练一个内核，其中特征子集有助于目标预测。
例如，我们知道在标准情况下，训练将针对以下内容进行：
特征数据 X ((n_samples, n_features)) -&gt; Y (n_samples)
我想要做的是，训练回归模型具有维度映射：(n_subset) -&gt; y_subset
因此，(n_features) 由大小为 (n_subset) 的子组组成，这些子组本质上具有类似的起源，它们的模式重复相似，但值并不严格相同。
假设 (n_features) = (n_subset)*(n_terms)。
作为一个过早的解决方案，我已经实现了一个模型，其中每个特征子集对最终预测的贡献相同。以下是示例代码：
import numpy as np
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import GridSearchCV

def train_kernel_with_weights(X, Y, n_subset, kernel_param_grid):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset # 内核预测项的数量，将加总为总数
X_sub_rearr = X.reshape(nset * n_kerterm, n_subset)

kr1 = GridSearchCV(KernelRidge(), param_grid=kernel_param_grid, cv=5)

# 此处，测试了“等权重”方案，任意假设每个 (n_subset)-&gt;y_subset 项
# 对总 Y 的贡献相等。
kr1.fit(X_sub_rearr, np.repeat(Y/n_kerterm, n_kerterm))

return kr1

# 给定一个具有映射 (n_subset -&gt; value) 的核，获取子集总和预测值
def predict_krrsum_1(n_subset, kr1, X):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset

# 使用 n_subset 分割每个数据点的特征（可能是三元组）
X_sub_rearr = X.reshape(nset*n_kerterm,n_subset)
Y_krrsub_flat = kr1.predict(X_sub_rearr) # 1d 数组 (nset*n_kerterm)
Y_krrsub = Y_krrsub_flat.reshape(-1,n_kerterm) # (nset, n_kerterm)
Y_krrsum_1 = np.sum(Y_krrsub,axis=1)
return Y_krrsum_1

那么我的问题是：对于核训练函数：train_kernel_with_weights，
我该如何优化子集的权重，而不是任意使用相等的权重？]]></description>
      <guid>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</guid>
      <pubDate>Sun, 25 Aug 2024 12:10:29 GMT</pubDate>
    </item>
    <item>
      <title>Resnet 训练和评估模式不一致</title>
      <link>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</link>
      <description><![CDATA[我正在尝试在 torch 中实现 Resnet。但我发现前向传递的输出在训练和评估模式之间差异很大。由于训练和评估模式除了 batch norm 和 dropout 之外不影响任何东西，所以我不知道结果是否有意义。
下面是我的测试代码：
import torch
from torch import nn
from torchvision import models

class resnet_lstm(torch.nn.Module):
def __init__(self):
super(resnet_lstm, self).__init__()
resnet = models.resnet50(pretrained=True)
self.share = torch.nn.Sequential()
self.share.add_module(&quot;conv1&quot;, resnet.conv1)
self.share.add_module(&quot;bn1&quot;, resnet.bn1) # 使用 BatchNorm3d
self.share.add_module(&quot;relu&quot;, resnet.relu)
self.share.add_module(&quot;maxpool&quot;, resnet.maxpool)
self.share.add_module(&quot;layer1&quot;, resnet.layer1)
self.share.add_module(&quot;layer2&quot;, resnet.layer2)
self.share.add_module(&quot;layer3&quot;, resnet.layer3)
self.share.add_module(&quot;layer4&quot;, resnet.layer4)
self.share.add_module(&quot;avgpool&quot;, resnet.avgpool)
self.fc = nn.Sequential(nn.Linear(2048, 512),
nn.ReLU(),
nn.Linear(512, 7))

def forward(self, x):
x = x.view(-1, 3, 224, 224)
x = self.share(x)
返回x

model = resnet_lstm()

input_ = torch.randn(1, 3, 224, 224)
model.train()
print(&quot;训练模式输出&quot;, model(input_))
model.eval()
print(&quot;评估模式输出&quot;, model(input_))


终端输出：
训练模式输出 tensor([[[[0.3603]],

[[0.5518]],

[[0.4599]],

...,

[[0.3381]],

[[0.4445]],

[[0.3481]]]], grad_fn=&lt;MeanBackward1&gt;)
评估模式输出tensor([[[[0.1582]],

[[0.1822]],

[[0.0000]],

...,

[[0.0567]],

[[0.0054]],

[[0.3605]]]], grad_fn=&lt;MeanBackward1&gt;)

可以看到，两种模式的输出差别很大，会不会影响性能呢？]]></description>
      <guid>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</guid>
      <pubDate>Sun, 25 Aug 2024 11:07:12 GMT</pubDate>
    </item>
    <item>
      <title>关键点/地标检测</title>
      <link>https://stackoverflow.com/questions/78906351/keypoints-landmarks-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78906351/keypoints-landmarks-detection</guid>
      <pubDate>Fri, 23 Aug 2024 14:19:21 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10：如何解读训练进度信息？</title>
      <link>https://stackoverflow.com/questions/78904715/yolov10-how-to-interpret-training-progress-info</link>
      <description><![CDATA[我正在用这个代码训练 YOLOv10：
model.train(
.........
epochs=250,
batch=16,
verbose=True,
save=True,
save_period=1,
time=4,
.........
)

我正在尝试各种 VM/GPU 选项来计算最具成本效益的训练选项，因此我多次运行它，同时我更改 batch 以填充接近 90% 的 GPU RAM。
在训练期间，除了其他输出外，我还获得了类似屏幕截图的信息。

问题如下。我允许自己在一个主题中提出几个问题，因为我希望其中两个问题或多或少像 yes/no/single_word 一样简单，另一个只是一个链接。

为什么计划的 epoch 数量 (1) 被更改 (2)？我相信这可能是因为分配给训练的时间有限，所以训练过程会计算在此期间可以运行多少个 epoch，但我不确定。
我相信 (3) 是每个 epoch 的批次数，这个数字会根据批次大小和 GPU RAM 使用情况而变化。它正确吗？
我如何估计在特定 GPU/批处理设置上训练过程需要多长时间（假设没有配置时间限制）？我尝试将 (4) 和 (5) 相加并乘以更改后的 epoch 数 (2)，但在所有实验中，我得到的时间总是接近配置的限制。
是否有文档解释屏幕截图上的所有其他数据以及其他训练输出？有些值是不言自明的，有些则不是，所以我宁愿避免猜测。我检查了 https://docs.ultralytics.com/modes/train/，但没有找到解释。
]]></description>
      <guid>https://stackoverflow.com/questions/78904715/yolov10-how-to-interpret-training-progress-info</guid>
      <pubDate>Fri, 23 Aug 2024 07:12:32 GMT</pubDate>
    </item>
    <item>
      <title>批量、随机和小批量梯度下降的混淆</title>
      <link>https://stackoverflow.com/questions/42881516/confusion-with-batch-stochastic-and-mini-batch-gradient-descent</link>
      <description><![CDATA[我正在研究一些卷积神经网络的东西，我一直在研究这三者之间的区别，但遇到了一些问题。我正在查看这个网站http://sebastianruder.com/optimizing-gradient-descent/。
在其中，作者说它计算了成本函数相对于整个数据集权重的梯度。我对整个训练数据集是如何应用的感到困惑。对我来说，随机性是直观的，因为我将单个图像放入模型中，获得成本函数的预测，然后进行优化。对于小批量和批量梯度下降，多个值如何应用于成本函数？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/42881516/confusion-with-batch-stochastic-and-mini-batch-gradient-descent</guid>
      <pubDate>Sun, 19 Mar 2017 00:30:19 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中的步骤和时期有什么区别？</title>
      <link>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</link>
      <description><![CDATA[在大多数模型中，都有一个 steps 参数，表示在数据上运行的步骤数。但我在大多数实际使用中看到，我们还会执行拟合函数 N epochs。
用 1 个 epoch 运行 1000 步和用 10 个 epoch 运行 100 步有什么区别？在实践中哪一个更好？连续 epoch 之间有任何逻辑变化吗？数据混洗？]]></description>
      <guid>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</guid>
      <pubDate>Tue, 12 Jul 2016 23:20:22 GMT</pubDate>
    </item>
    <item>
      <title>神经网络训练中的 Epoch 是什么</title>
      <link>https://stackoverflow.com/questions/31155388/what-is-an-epoch-in-neural-networks-training</link>
      <description><![CDATA[当我阅读如何在 pybrain 中构建 ANN 时，他们说：

对网络进行一些时期的训练。通常你会在这里设置 5 之类的值，
trainer.trainEpochs( 1 )


我查找了这是什么意思，然后我得出结论，我们使用一个 epoch 的数据来更新权重，如果我选择按照 pybrain 的建议使用 5 个 epoch 来训练数据，数据集将被分成 5 个子集，权重最多会更新 5 次。
我熟悉在线训练，其中权重在每个样本数据或特征向量之后更新，我的问题是如何确保 5 个 epoch 足以构建模型并设置权重？这种方式在在线训练中有什么优势？此外，术语“epoch”用于在线训练，它是指一个特征向量吗？]]></description>
      <guid>https://stackoverflow.com/questions/31155388/what-is-an-epoch-in-neural-networks-training</guid>
      <pubDate>Wed, 01 Jul 2015 07:44:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 sklearn GMM 混合模型中处理分类数据</title>
      <link>https://stackoverflow.com/questions/30984019/how-handle-categorical-data-in-sklearn-gmm-mixture-model</link>
      <description><![CDATA[有没有办法在 sklearn GMM 模块中输入分类观察值？
我的数据看起来有点像：
User,Siet_category,user_segment

UserA,Sports:News,efk-457
UserB,Music:Entertainment,asl-567
UserC,Sports:News,asl-567
UserD,Sports:News,efk-457

user_segment 是我的数据集中的类（大约有 10 个类）。
我认为这是 10 种不同分布的混合。
我想要做的是给出一个测试用户和站点类别，我想知道该测试用例属于哪个类/分布。
我知道我可以选择判别模型，但我想看看生成模型在这种情况下的表现。]]></description>
      <guid>https://stackoverflow.com/questions/30984019/how-handle-categorical-data-in-sklearn-gmm-mixture-model</guid>
      <pubDate>Mon, 22 Jun 2015 15:31:32 GMT</pubDate>
    </item>
    <item>
      <title>生成、判别和参数、非参数算法/模型之间的区别</title>
      <link>https://stackoverflow.com/questions/23821521/difference-between-generative-discriminating-and-parametric-nonparametric-algo</link>
      <description><![CDATA[在SO中，我找到了以下关于生成算法和判别算法的解释：
“生成算法模拟数据的生成方式，以便对信号进行分类。它提出了一个问题：根据我的生成假设，哪个类别最有可能生成此信号？
判别算法不关心数据是如何生成的，它只是对给定信号进行分类。”
并且这里是参数和非参数的定义算法
“参数化：数据来自特定形式的概率分布，直至未知参数。
非参数化：数据来自某个未指定的概率分布。
”
那么从本质上讲，我们可以说生成算法和参数化算法假设了底层模型，而判别算法和非参数算法不假设任何模型吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/23821521/difference-between-generative-discriminating-and-parametric-nonparametric-algo</guid>
      <pubDate>Fri, 23 May 2014 05:25:57 GMT</pubDate>
    </item>
    </channel>
</rss>