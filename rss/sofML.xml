<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Apr 2024 15:15:01 GMT</lastBuildDate>
    <item>
      <title>在 MacOS 上编译 llama-cpp-python 时找不到 ggml-common.h</title>
      <link>https://stackoverflow.com/questions/78305294/ggml-common-h-not-found-compiling-llama-cpp-python-on-macos</link>
      <description><![CDATA[我正在学习 LLM 和 RAG。
目前，我在尝试使用带有 M2 的 MAC PRO 实例化 llm 时遇到问题。
# 回调支持 token-wise 流式传输
callback_manager2 = CallbackManager([StreamingStdOutCallbackHandler()])
 
n_gpu_layers = 1 # 根据您的模型和 GPU VRAM 池更改此值。
n_batch = 1 # 应介于 1 和 n_ctx 之间，考虑 GPU 中的 VRAM 量。

llmGPU = LlamaCpp(
 model_path=“路径/llama-2-13b-chat.Q2_K.gguf”,
 输入={“温度”：0.75，“max_length”：2000，“top_p”：1}，
 n_gpu_layers=n_gpu_layers,
 n_batch=n_batch,
 回调管理器=回调管理器2,
 详细=真，
）

当我尝试这样做时，它会出现此错误：
“program_source：3：10：致命错误：找不到“ggml-common.h”文件#include“ggml-common.h” ^~~~~~~~~~~~~~~” UserInfo={NSLocalizedDescription=program_source:3:10: 致命错误：找不到“ggml-common.h”文件 #include “ggml-common.h” ^~~~~~~~~~~~~~~ } llama_new_context_with_model：无法初始化 Metal 后端

我在互联网上发现使用此命令安装 llama 可以解决这个问题：
!CMAKE_ARGS=“-DLLAMA_METAL_EMBED_LIBRARY=ON -DLLAMA_METAL=on” pip install -U llama-cpp-python --no-cache-dir

但这不适合我。 （我是在 jupyter 笔记本上完成所有这些操作。
请注意，如果我避免使用 GPU 层和批处理，它会起作用，但需要 3 分钟才能解决 3+3，所以太慢了。我尝试过使用其他 LLama 版本，但仍然存在同样的问题。
我将不胜感激一些帮助！谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78305294/ggml-common-h-not-found-compiling-llama-cpp-python-on-macos</guid>
      <pubDate>Wed, 10 Apr 2024 14:52:54 GMT</pubDate>
    </item>
    <item>
      <title>如何删除具有特定条件的矩阵中的行</title>
      <link>https://stackoverflow.com/questions/78304850/how-to-remove-rows-in-a-matrix-with-a-certain-criteria</link>
      <description><![CDATA[我有一个由 7,219 列和 6,817 行组成的基因表达矩阵。我希望删除变异性较低的行。例如，该行中的 max\min 值 &lt;= 5 的行，以及 max - min &lt;= 500 的行。我不知道一行最大值的索引，只知道整个矩阵的索引。我应该使用 for 还是 if 循环？或者有更简单的方法吗？谢谢
我在网上寻找过任何解决方案，但只找到了当元素为特定值时排除行的方法，而不是在存在涉及多个元素作为标准的不等式的情况下。]]></description>
      <guid>https://stackoverflow.com/questions/78304850/how-to-remove-rows-in-a-matrix-with-a-certain-criteria</guid>
      <pubDate>Wed, 10 Apr 2024 13:38:43 GMT</pubDate>
    </item>
    <item>
      <title>具有额外指标的软件漏洞数据集的 ML/DL 算法</title>
      <link>https://stackoverflow.com/questions/78304043/ml-dl-algorithm-for-software-vulnerability-dataset-with-extra-metrics</link>
      <description><![CDATA[我有一个包含八列的数据集。第一列表示 C/C++ 中函数的漏洞/中立状态，而其余七列包含进程/代码指标。我正在寻找一种能够处理这个组合数据集的机器学习/深度学习算法。您能否建议一种有效结合这些方面的算法或方法？
虽然我观察到许多只使用代码或指标的方法，但我还没有遇到任何将两者集成的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78304043/ml-dl-algorithm-for-software-vulnerability-dataset-with-extra-metrics</guid>
      <pubDate>Wed, 10 Apr 2024 11:18:44 GMT</pubDate>
    </item>
    <item>
      <title>Deeplearning4j - 为字符串员工、工人、同事等创建模型</title>
      <link>https://stackoverflow.com/questions/78303940/deeplearning4j-creating-a-model-for-strings-employee-worker-associate-etc</link>
      <description><![CDATA[我想使用 deeplearning4j 构建模型/训练模型。
该模型应该理解所有与员工相关的术语，例如，它应该将“员工”、“工人”、“同事”等词理解为相同的词。
我使用下面的代码片段来创建数据集并拥有迭代器
 数据集 allData;
    尝试 (RecordReader recordReader = new CSVRecordReader(0, &#39;,&#39;)) {
        recordReader.initialize(new StringSplit(&quot;员工 ID,0&quot;));

        DataSetIterator 迭代器 = new RecordReaderDataSetIterator(recordReader, 150, FEATURES_COUNT, CLASSES_COUNT);
        allData = 迭代器.next();
    }

    allData.shuffle(42);

并最终出现以下错误：
线程“main”中出现异常java.lang.NumberFormatException：对于输入字符串：“员工 ID”
    在 java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)
    在 java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
    在 java.base/java.lang.Double.parseDouble(Double.java:651)
    在 org.datavec.api.writable.Text.toDouble(Text.java:590)
    在 org.datavec.api.util.ndarray.RecordConverter.toMinibatchArray(RecordConverter.java:207)
    在 org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next（RecordReaderMultiDataSetIterator.java:153）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:346）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:421）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:53)
    在 com.test.dl4jtest.dl4jtest.TrainModel.main(TrainModel.java:46)

请建议我如何创建一个模型，该模型应在此处考虑工人、同事、雇员等术语。]]></description>
      <guid>https://stackoverflow.com/questions/78303940/deeplearning4j-creating-a-model-for-strings-employee-worker-associate-etc</guid>
      <pubDate>Wed, 10 Apr 2024 11:01:36 GMT</pubDate>
    </item>
    <item>
      <title>计算参考摘要和预测摘要之间的 ROGUE 分数</title>
      <link>https://stackoverflow.com/questions/78303659/calculating-rogue-score-between-reference-summary-and-prediction-summary</link>
      <description><![CDATA[我正在尝试计算 AI 生成的摘要和人类编写的摘要之间的 ROGUE 分数，如下所述。我对两者的摘要长度将超过 50 个标记。这是我的 python 代码。
pip install rouge_score
从 rouge_score 导入 rouge_scorer

预测=“我是ABC。我已经在 XYZ 大学完成了计算机应用学士学位，目前正在通过远程教育攻读计算机应用硕士学位。”

引用=“我是ABC。我已经在 XYZ 完成了为期四年的 PC 应用认证，目前正在通过远程培训攻读 PC 应用研究生学位。”


记分器 = rouge_scorer.RougeScorer([&#39;rouge1&#39;, &#39;rouge2&#39;, &#39;rougeL&#39;, &#39;rougeLsum&#39;])
分数 = Scorer.score(参考, 预测)
# 打印分数
对于分数的关键：
    print(f&#39;{key}: {scores[key]}&#39;)

输出：
rouge1：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

rouge2：分数（精度=0.3333333333333333，召回率=0.35714285714285715，fmeasure=0.3448275862068965）

rougeL：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

rougeLsum：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

我是这个领域的新手。因此，我不确定在我的用例中应考虑哪个 Rogue 分数及其重要性。]]></description>
      <guid>https://stackoverflow.com/questions/78303659/calculating-rogue-score-between-reference-summary-and-prediction-summary</guid>
      <pubDate>Wed, 10 Apr 2024 10:10:01 GMT</pubDate>
    </item>
    <item>
      <title>解决这个问题的最佳方法是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78302669/what-would-be-the-best-approach-to-this-problem</link>
      <description><![CDATA[我有一些财务数据。它是很长一段时间内记录的人们账户和其他财务详细信息的列表。目前有一条规定，两年后，如果账户为空，加上一些其他条件，债务就会被消除。
为了节省系统负载，我们正在尝试找到一个最佳时间来缩短该时间段，同时将损失降至最低。
例如：
- 这家伙欠债了
- 两年过去了，那家伙几乎什么也没归还
- 将 Guy 从系统中删除以节省处理成本

我正在尝试找出最佳取消时间的方法，同时不损失潜在的资金（因为我们假设在 X 时间后无论如何都不会偿还债务）
我考虑了不同的机器学习算法，但似乎我最终总是进行特征提取而不是某种分析预测。谁能帮忙推荐一个适合这个问题的机器学习算法？
我尝试了随机森林算法，xgboost。
这更多的是特征提取和数据分析的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78302669/what-would-be-the-best-approach-to-this-problem</guid>
      <pubDate>Wed, 10 Apr 2024 07:10:42 GMT</pubDate>
    </item>
    <item>
      <title>在Python中计算候选句子和参考句子之间的BLEU分数</title>
      <link>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</link>
      <description><![CDATA[我正在计算 2 个句子之间的 BLEU 分数，这两个句子看起来与我非常相似，但我得到的 BLEU 分数非常低。这应该发生吗？
预测=“我是ABC。”
参考=“我是ABC。”

从nltk.translate.bleu_score导入sent_bleu，corpus_bleu
从 nltk.translate.bleu_score 导入 SmoothingFunction
# 对句子进行标记
Prediction_tokens = Prediction.split()
Reference_tokens = Reference.split()
   
# 计算 BLEU 分数
bleu_score=sentence_bleu([reference_tokens],prediction_tokens,smoothing_function=SmoothingFunction().method4)

# 打印 BLEU 分数
print(f&quot;BLEU 分数: {bleu_score:.4f}&quot;)

输出为 0.0725
]]></description>
      <guid>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</guid>
      <pubDate>Wed, 10 Apr 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在具有连续标签的数据集上评估聚类算法的性能并优化超参数？</title>
      <link>https://stackoverflow.com/questions/78302382/how-to-evaluate-performance-and-optimize-hyperparameters-for-clustering-algorith</link>
      <description><![CDATA[我正在研究一个聚类问题，其中我的数据集的标签是连续的数值，而不是离散的类别。我使用 t-SNE 和 UMAP 将数据集特征的维度降低为二维，然后在散点图上可视化点，并根据其标签值对点进行着色。
我的目标是查看特征表示是否准确反映标签值的变化，即具有相似标签值的点是否在 2D 空间中聚集在一起。或者，特定的数据点不需要聚集在一起。只需要观察某些趋势，即数据点的分布随着标签值的增加或减少而按规则模式移动。
我正在调整 t-SNE 和 UMAP 的各种超参数来实现这一目标。到目前为止，我一直通过目视检查散点图是否存在任何模式或趋势来评估结果。这是我生成的绘图示例：
UMAP聚类结果图示例，供参考
有人可以提出方法或指标来定量评估 t-SNE 和 UMAP 执行的聚类反映标签连续性的程度吗？任何有关如何客观地确定这些方法的最佳超参数设置的建议将不胜感激。
如果您需要更多信息，请告诉我，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78302382/how-to-evaluate-performance-and-optimize-hyperparameters-for-clustering-algorith</guid>
      <pubDate>Wed, 10 Apr 2024 06:03:29 GMT</pubDate>
    </item>
    <item>
      <title>聚类解释</title>
      <link>https://stackoverflow.com/questions/78302316/clustering-explaination</link>
      <description><![CDATA[我想以描述的形式总结我创建的集群，根据每个集群与其他集群的不同之处来定义每个集群。
输出应该是摘要的形式]]></description>
      <guid>https://stackoverflow.com/questions/78302316/clustering-explaination</guid>
      <pubDate>Wed, 10 Apr 2024 05:45:21 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证了模型训练正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在各个图表上绘制多个线性回归特征与预测结果</title>
      <link>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</link>
      <description><![CDATA[我正在研究一个电视广告数据集，该数据集具有 3 个特征（TV、radio、newspaper）和 1 个因变量 (销售）。通过使用多元线性回归，我估算了销售额并将其与 y_test 数据集上的实际值进行了比较。我已经确认我的模型获得了很高的 r2 分数。
我知道在多元线性回归中，多个特征可以在更高维度的图上可视化。但是，我希望在单独的图表上查看每个单独的特征与预测结果。
导入 pandas
导入numpy
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score
将 statsmodels.api 导入为 sm
将 matplotlib.pyplot 导入为 plt

#导入数据集并分配给X和y
df = pandas.read_csv(“广告.csv”)
df = df.drop(df.columns[[0]], 轴=1)
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].值

#将数据集分割成X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#初始化回归器并使用 X_train 和 y_train 对其进行训练
回归器=线性回归()
回归器.fit(X_train, y_train)

#预测测试结果并用实际值绘制它们
y_pred = 回归器.预测(X_test)
print(&quot;预测值/实际值&quot;)
print(numpy.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))

#评估预测结果的模型性能
r_squared_线性 = r2_score(y_test, y_pred)
print(f“多元线性回归模型性能为{r_squared_线性}”)

#查看OLS回归结果
mod = sm.OLS(y.reshape(-1, 1), X)
res = mod.fit()
打印（res.summary（））

#在各个图表上绘制数据集的每个特征与预测结果

这是我的代码，它可以正常工作，没有任何问题。我只想用 matplotlib 完成绘图部分。我想绘制每个单独的特征（TV、radio、newspaper）与我的模型具有的 y_pred 值预测，包括每个图上的线性回归线。
如果有人能向我展示如何使用 for 循环（以确保它适用于不同数据集上的任何特征计数），我将不胜感激。
另外，为了获得额外的知识，如何在同一图中绘制所有特征以及 y_pred 结果？]]></description>
      <guid>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</guid>
      <pubDate>Tue, 09 Apr 2024 07:56:36 GMT</pubDate>
    </item>
    <item>
      <title>人工智能揭示图像识别中的具体细节</title>
      <link>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition</link>
      <description><![CDATA[我正在尝试执行以下操作：
我想使用某种人工智能 API 来执行自动图像识别（其中“图像识别”是指将图像提供给人工智能，人工智能将根据我想要识别的图像细节生成文本输出），例如另一位用户在 stackoverflow 上发布了 ColaCola Can 帖子，但不同之处在于我的详细信息可能因两个选项而异。
我的问题是基于鸟类的性别识别，根据其身体的细节（例如，如果鸟的胸部是黄色的，那么它是雄性，否则它是雌性）。
我可以假设模型没有提供任何棘手的图像，细节的照片是故意拍摄的，非常清晰，目的是促进该实验的模型，（我的意思是，重点关注该细节恒定光线充足的环境，变焦，聚焦等），并且输出必须类似于“男性”或“女性”。基于这种认识。
基本上，我需要以最简单、最不棘手的方式训练图像识别人工智能。
有没有简单的方法来执行它？]]></description>
      <guid>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition</guid>
      <pubDate>Mon, 08 Apr 2024 16:12:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么在启用 from_logits 的情况下使用 BinaryCrossEntropy 生成器损失？</title>
      <link>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</link>
      <description><![CDATA[从简单的普通 GAN 代码中，我查看  GitHub
我看到这个生成器模型具有激活sigmoid：
&lt;前&gt;&lt;代码&gt;# 生成器
G = tf.keras.models.Sequential([
  tf.keras.layers.Dense(28*28 // 2, input_shape = (z_dim,), 激活=&#39;relu&#39;),
  tf.keras.layers.Dense(28*28, 激活=&#39;sigmoid&#39;),
  tf.keras.layers.Reshape((28, 28))])

在启用 from_logits 的情况下，G 的损失定义如下：
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)
def G_loss(D, x_fake):
  返回 cross_entropy(tf.ones_like(D(x_fake)), D(x_fake))

据我所知，from_logits=True旨在使损失函数接受范围在-infinity到&lt;之间的y_pred值代码&gt;无穷大。与 from_logits=False 相反，损失函数假设值的范围在 0 到 1 之间。
如您所见，G 模型的输出层已经具有 sigmoid 激活，其范围在 0 到 1.
但是，为什么作者仍然使用 from_logits=True？]]></description>
      <guid>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</guid>
      <pubDate>Thu, 04 Apr 2024 18:03:32 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用 `bitsandbytes` 8 位量化需要 Accelerate：`pip install Accelerate` 和最新版本的 Bitsandbytes：`pip install</title>
      <link>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Mon, 01 Apr 2024 08:15:53 GMT</pubDate>
    </item>
    <item>
      <title>对于负数与正数比率非常高的多标签分类，应使用哪些损失函数和指标？</title>
      <link>https://stackoverflow.com/questions/59336899/which-loss-function-and-metrics-to-use-for-multi-label-classification-with-very</link>
      <description><![CDATA[我正在训练一个多标签分类模型来检测衣服的属性。我在 Keras 中使用迁移学习，重新训练 vgg-19 模型的最后几层。
属性总数为 1000 个，其中约 99% 为 0。准确性、精确度、召回率等指标都失败了，因为模型可以预测全零，但仍然获得非常高的分数。二元交叉熵、汉明损失等，在损失函数的情况下都不起作用。
我正在使用深度时尚数据集。 
那么，我可以使用哪些指标和损失函数来正确衡量我的模型？]]></description>
      <guid>https://stackoverflow.com/questions/59336899/which-loss-function-and-metrics-to-use-for-multi-label-classification-with-very</guid>
      <pubDate>Sat, 14 Dec 2019 16:15:51 GMT</pubDate>
    </item>
    </channel>
</rss>