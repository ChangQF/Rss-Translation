<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Mar 2024 09:13:20 GMT</lastBuildDate>
    <item>
      <title>我做了什么来纠正属性错误。请帮助我[关闭]</title>
      <link>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;batch_size = 100
对于范围 (25) 内的 i：
    num_batches = int(mnist.train.num_examples/batch_size)
    总成本 = 0
    对于范围内的 j（num_batches）：
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        c, _ = sess.run([成本,优化], feed_dict={x:batch_x, y:batch_y, keep_prob:0.8})
        总成本 += c
    打印（总成本）

属性错误
                            回溯（最近一次调用最后一次）
单元格 In[65]，第 3 行
      1 批量大小 = 100
      2 对于范围 (25) 内的 i：
----&gt; 3 num_batches = int(mnist.train.num_examples/batch_size)
      4 总成本 = 0
      5 对于 j 在范围内（num_batches）：

AttributeError：模块“keras.datasets.mnist”没有属性“train”
]]></description>
      <guid>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</guid>
      <pubDate>Sat, 23 Mar 2024 07:25:50 GMT</pubDate>
    </item>
    <item>
      <title>用户警告：RNN 模块权重不是单个连续内存块的一部分</title>
      <link>https://stackoverflow.com/questions/78209777/userwarning-rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209777/userwarning-rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memor</guid>
      <pubDate>Sat, 23 Mar 2024 04:48:10 GMT</pubDate>
    </item>
    <item>
      <title>形状值与模型预测之间的差异</title>
      <link>https://stackoverflow.com/questions/78209765/discrepency-between-shap-value-versus-model-prediction</link>
      <description><![CDATA[我使用 shap.Explainer 使用 XgboostRegressor 模型，
我在 pyspark 中应用了 udf，以便在数据很大时可以进行扩展。
我正在使用最新的 shap 库和 Xgboost 以及 Spark 版本 3.2。
下面的示例代码
feature_list = model.feature_names_in_
对于迭代器中的 X：
    解释器 = shap.TreeExplainer(模型, X.sample(20), feature_perturbation = &#39;干预&#39;)
    shap_values = 解释器(X)
    yield pd.concat(pd.Dataframe( shap_values.values, column = [feature + &#39;CONTRIBUTION&#39; for col in feature_list]),
                    pd.Dataframe( shap_values.base_values, 列 = [&#39;BASE_VALUE&#39;]))

model_prediction和shap值之间的关系是
model_prediction = feature1_contribution + feature2_contribution +... + featuren_contribution + base_value
假设对于一种组合，我的 model_prediction = 20
但我的一项功能贡献约为 50，但其他功能贡献在 -1 &lt;= feature_contribution &lt;= 1 之间。
这是什么意思？我已经检查过我使用的是与形状和模型预测相同的模型。
我做错了什么？任何建议将不胜感激。
谢谢。
我尝试过的方法
而不是解释器(X)
我尝试过 explainer.shap_values(X) 但这不起作用。
我尝试过 Explainer，而不是 TreeExplainer]]></description>
      <guid>https://stackoverflow.com/questions/78209765/discrepency-between-shap-value-versus-model-prediction</guid>
      <pubDate>Sat, 23 Mar 2024 04:41:50 GMT</pubDate>
    </item>
    <item>
      <title>本地机器上的图像分类，但偶数纪元明显快于奇数纪元</title>
      <link>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</link>
      <description><![CDATA[我正在使用 Visual Studio 代码在本地计算机上进行图像分类训练，但每个偶数纪元都比奇数纪元快得多，以前有人遇到过这个问题吗？我在谷歌上到处找，但似乎没有人提到这种不规则现象。
因此，我无法正确集成早期停止回调，因为偶数纪元的 val_loss 被搞乱了。我尝试在 google collab 中运行，效果很好，但免费版本有限，有什么想法可以解决这个问题吗？ 我的基线模型的训练结果图像
对于我更复杂的模型，这个问题仍然存在。
# 定义一个 EarlyStopping 回调
Early_stopping = 回调.EarlyStopping(
    Monitor=&#39;val_loss&#39;, # 监控提前停止的指标（验证损失）
    Patient=5, # 停止前没有改善的 epoch 数
    min_delta=1e-7, # 被视为改进的监控指标的最小变化
    verbose=1, # 详细级别（1 表示更新）
    Restore_best_weights=True, # 停止时将模型权重恢复到最佳状态
）

# 定义一个ReduceLROnPlateau回调
高原 = 回调.ReduceLROnPlateau(
    Monitor=&#39;val_loss&#39;, # 监控学习率降低的指标（验证损失）
    Factor=0.5, # 学习率降低的因子（例如，0.2 表示 lr *= 0.2）
    Patient=2, # 降低学习率之前没有改善的 epoch 数
    min_delta=1e-6, # 触发减少的监控指标的最小变化
    Cooldown=0, # 减少后恢复正常操作之前等待的纪元数
    verbose=1 # 详细级别（1 表示更新）
）

def 基线CNN模型():
  # 输入层
  输入=输入（形状=（图像H，图像W，3））

  # 卷积层
  x = Conv2D(32, (3, 3), 激活=&#39;relu&#39;)(输入)
  x = MaxPooling2D((2, 2))(x)
  x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;)(x)
  x = MaxPooling2D((2, 2))(x)

  # 压平层
  x = 展平()(x)

  # 全连接层
  x = 密集(64, 激活=&#39;relu&#39;)(x)
  输出=密集（1，激活=&#39;sigmoid&#39;）（x）

  # 创建模型
  模型=模型（输入=[输入]，输出=输出）

  返回模型

# 清除Keras会话以释放资源
keras.backend.clear_session()

# 使用定义的 &#39;baseline_model&#39; 函数创建 CNN 模型
基线CNN = 基线CNN模型()

# 使用指定的损失、优化器和指标编译 CNN 模型
基线CNN.编译（
    loss=&#39;binary_crossentropy&#39;, # 二元分类的二元交叉熵损失
    Optimizer=keras.optimizers.Adam(), # 具有自定义学习率的 Adam 优化器
    metrics=[&#39;binary_accuracy&#39;] # 训练期间监控的指标（二进制精度）
）

# 显示模型架构的摘要
基线CNN.summary()

# 训练模型
bCNNHist = 基线CNN.fit(trainDS,
          验证数据=valDS，
          纪元=10，
          批量大小=批量大小，
          callbacks=[plateau], #early_stopping, 提前停止和降低学习率的回调
          steps_per_epoch=int(len(trainDF)/batchSize), # 每个训练周期的步数
          validation_steps=int(len(valDF) / batchSize) # 每个验证时期的步骤数)
）
]]></description>
      <guid>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</guid>
      <pubDate>Sat, 23 Mar 2024 02:41:21 GMT</pubDate>
    </item>
    <item>
      <title>TPU 连接问题 训练 TF 模型 Google Colab</title>
      <link>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</guid>
      <pubDate>Fri, 22 Mar 2024 23:57:10 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用训练集还是验证集来进行参数优化？</title>
      <link>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</link>
      <description><![CDATA[我正在使用决策树和参数优化来训练模型。
我了解到验证集的目标是评估训练期间的模型性能并帮助调整参数。
考虑到这一点，我不应该使用 grid_search.fit 上的验证集而不是训练集吗？
param_grid = {
    &#39;最大深度&#39;: [3, 5, 7, 10],
    &#39;min_samples_split&#39;: [2, 5, 10],
    &#39;min_samples_leaf&#39;: [1, 2, 4]
}

clf = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, 评分=&#39;准确度&#39;)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(&quot;最佳参数：&quot;, best_params)
打印(“\n”)

＃验证
best_clf = grid_search.best_estimator_
val_accuracy = best_clf.score(X_val, y_val)
print(“最佳模型的验证准确度：”, val_accuracy)
打印(“\n”)

＃测试
y_test_pred = best_clf.predict(X_test)
test_accuracy = precision_score(y_test, y_test_pred)
test_ precision = precision_score(y_test, y_test_pred)
test_recall=recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
print(“具有最佳模型的测试集上的决策树测量：”)
print(&quot;准确度：&quot;, test_accuracy)
print(&quot;精度：&quot;, test_ precision)
print(&quot;召回：&quot;, test_recall)
print(&quot;F1 分数：&quot;, test_f1)
打印(“---------------------------------------------- ---------”）
]]></description>
      <guid>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</guid>
      <pubDate>Fri, 22 Mar 2024 23:27:32 GMT</pubDate>
    </item>
    <item>
      <title>连体网络反向传播</title>
      <link>https://stackoverflow.com/questions/78208649/siamese-network-backpropagation</link>
      <description><![CDATA[我正在尝试使用 Triplet-loss 构建一个跨视图本地化网络。
我想要查询照片（锚点），通过“model1”，以及负片和“model1”。积极通过“model2” （因为他们来自不同的领域，我不想分享他们的权重）。
我想知道当我计算 2 个不同模型的特征的损失时，如何正确反向传播。
这段代码正确吗？：
criterion_triplet = nn.TripletMarginLoss（margin=args.margin，p=2，reduction=“sum”）
模型 1 = MyNet(参数)
模型2 = MyNet(参数)
优化器1 = torch.optim.Adam(...)
优化器2 = torch.optim.Adam(...)

...训练循环...
   features1 = model1(images_anchor.to(args.device))
   features2 = model2(images_satelite.to(args.device))
   loss_triplet += criteria_triplet(features1,
                                     特点2,
                                     特点2)
   优化器1.zero_grad()
   优化器2.zero_grad()
   loss_triplet.backward()
   优化器1.step()
   优化器2.step()
]]></description>
      <guid>https://stackoverflow.com/questions/78208649/siamese-network-backpropagation</guid>
      <pubDate>Fri, 22 Mar 2024 20:15:29 GMT</pubDate>
    </item>
    <item>
      <title>协议错误 - 连接中止</title>
      <link>https://stackoverflow.com/questions/78208623/protocolerror-connection-aborted</link>
      <description><![CDATA[所以我正在制作一个 NLP 项目，其中我有效地进行了数据验证和数据转换，但是当涉及到模型训练时，它抛出了这个错误。我正在尝试从 Hugging-face 获取 cnn pegasus 模型，但它抛出此错误我使用相同的 api 进行数据转换并且它有效，但它抛出此错误，而模型火车请帮助我被困在这里。
我尝试了从禁用防火墙到良好的互联网连接的所有方法。还尝试从 Huggingface 下载模型，然后获取它，但我收到了有关不同权重的警告。]]></description>
      <guid>https://stackoverflow.com/questions/78208623/protocolerror-connection-aborted</guid>
      <pubDate>Fri, 22 Mar 2024 20:07:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么 torch.nn.function.linear 中重量的维度是 (out,in) 而不是 (in,out)</title>
      <link>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</link>
      <description><![CDATA[在 torch.nn.function.linear 的文档中（https://pytorch.org/docs/stable/ generated/torch.nn.function.linear.html），权重输入的维度为（out_features，in_features），然后计算时对权重矩阵进行转置输出：y=xA^T+b。为什么他们这样做而不是采用维度矩阵 W（in_features、out_features）并执行 y=xW+b？
通过执行 y=xW+b 尺寸将匹配，因此我找不到上述的明确原因。]]></description>
      <guid>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</guid>
      <pubDate>Fri, 22 Mar 2024 20:01:42 GMT</pubDate>
    </item>
    <item>
      <title>用于API结构转换的AI/ML模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78207358/ai-ml-model-for-api-structure-conversion</link>
      <description><![CDATA[是否有一个 AI 模型可以将不同 API 的响应转换为单一 JSON 格式？
所有 API 响应都有不同的结构。我正在寻找一种将响应转换为通用格式的模型。所有API都与数据集相关。
我正在尝试将不同的结构化 API 响应转换为单一格式，以便更容易使用。目前我创建了一个程序来做到这一点，但它需要针对不同的 API 进行少量的手动工作。]]></description>
      <guid>https://stackoverflow.com/questions/78207358/ai-ml-model-for-api-structure-conversion</guid>
      <pubDate>Fri, 22 Mar 2024 15:43:09 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML Studio Web 服务始终返回相同的预测</title>
      <link>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</link>
      <description><![CDATA[我目前正在为我的课程开发一个小型项目，但遇到了障碍，希望能得到一些帮助。在 Azure 机器学习工作室中训练 SVM 模型并将其部署为 Web 服务后，我遇到了一个特殊问题 - 无论输入数据如何，该服务都会返回相同的预测。
可能出了什么问题？
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier -SVM-Web
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM
这是我到目前为止所做的事情：

确保所有预处理步骤与模型训练阶段使用的步骤相同（包括数据标准化和缺失值处理）。
仔细检查模型是否使用输入数据进行评分。
验证了网络服务的配置，特别是在构建响应方面，以确保不返回任何静态值。
确保输入数据的格式与预期架构匹配。该模型在 ML Studio 环境中表现良好，并根据测试数据进行准确预测。但是，部署的 Web 服务似乎并未反映此行为并输出恒定值。
]]></description>
      <guid>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</guid>
      <pubDate>Fri, 22 Mar 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素[关闭]</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因子。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

确实需要得到整理，Stack坚持关闭我的帖子，写下gmail中navarrodan007的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>给定标签集之外的短 2-3 个标记文本或用户搜索查询的序列标签 [关闭]</title>
      <link>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</link>
      <description><![CDATA[我正在使用FLAIR模块解决序列标签问题。
我有包含 3 种不同类型实体的虚拟电子商务数据，每个实体都有大约 1K 个子实体。训练数据（大小约为 200K）是通过 3K 标签的组合综合创建的。
我尝试使用查询分类模型（带有 3K 标签）验证 FLAIR 序列标签。 FLAIR 模型（F1-score：60%）严重低于分类模型（F1-score：80%） &gt;).
我不愿意开发序列标签模块，因为我希望序列标签器也能够检测并提出新实体。
你能帮我了解哪里可能出错以及我可以尝试哪些其他模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</guid>
      <pubDate>Fri, 22 Mar 2024 05:30:33 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8 超参数调优</title>
      <link>https://stackoverflow.com/questions/78196468/yolov8-hyperparameter-tunning</link>
      <description><![CDATA[我尝试改进模型中的 mAP 结果。你能帮我调整yolov8中的超参数吗
如何调整 Yolov8 中的超参数？有什么方法可以在 Colab 中用 Python 代码对其进行调优吗？或者我应该与默认一起工作。 yaml 文件，我应该如何使用这个文件？
另外，我想提高准确性。我怎样才能通过调整来做到这一点？
这是我的问题]]></description>
      <guid>https://stackoverflow.com/questions/78196468/yolov8-hyperparameter-tunning</guid>
      <pubDate>Wed, 20 Mar 2024 22:05:12 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的forward函数到底输出什么？</title>
      <link>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</link>
      <description><![CDATA[此示例逐字取自 PyTorch 文档。现在我确实对深度学习有了一些总体背景，并且知道很明显，forward 调用表示前向传递，穿过不同的层并最终到达终点，在本例中有 10 个输出，然后获取前向传递的输出并使用定义的损失函数计算损失。现在，我忘记了在这种情况下 forward() 传递的输出到底给我带来了什么。
我认为神经网络的最后一层应该是某种激活函数，例如 sigmoid() 或 softmax() ，但我没有看到这些是定义在任何地方，而且，当我现在做项目时，我发现后来调用了 softmax() 。所以我只是想澄清 outputs = net(inputs) 给我的到底是什么 link，在我看来，默认情况下 PyTorch 模型前向传递的输出是 logits？
transform = Transforms.Compose(
    [transforms.ToTensor(),
     变换.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,
                                        下载=真，变换=变换）
trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,
                                          洗牌=真，num_workers=2）

将 torch.nn 导入为 nn
导入 torch.nn.function 作为 F


类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def 前向（自身，x）：
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


净=净()

导入 torch.optim 作为 optim

标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.001, 动量=0.9)

for epoch in range(2): # 多次循环数据集

    运行损失 = 0.0
    对于 i，enumerate(trainloader, 0) 中的数据：
        # 获取输入；数据是[输入，标签]的列表
        输入，标签=数据

        # 将参数梯度归零
        优化器.zero_grad()

        # 前向+后向+优化
        输出 = 净值（输入）
        打印（输出）
        休息
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        # 打印统计数据
        running_loss += loss.item()
        if i % 2000 == 1999: # 每 2000 个小批量打印一次
            print(&#39;[%d, %5d] 损失: %.3f&#39; %
                  (epoch + 1, i + 1, running_loss / 2000))
            运行损失 = 0.0

print(&#39;训练完成&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</guid>
      <pubDate>Tue, 24 Nov 2020 13:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>