<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 09 Feb 2024 09:14:20 GMT</lastBuildDate>
    <item>
      <title>使用 lcmm 在 R 中进行网格搜索。我可以使用 GPU 吗？</title>
      <link>https://stackoverflow.com/questions/77966884/gridsearch-in-r-using-lcmm-can-i-utilize-gpu</link>
      <description><![CDATA[我正在使用 lcmm 包在 R 中执行基于组的轨迹建模。这是我的代码：
fitGbtmql = 函数(k) {
  开始 = 系统时间()
  模型 = do.call(gridsearch, 列表(m = makeGbtmCallql(k), 代表 = 5, maxiter = 5, 迷你 = gbtm_modelsql[[&#39;1&#39;]], cl = 10))
  model$runTime = Sys.time() - 开始
  返回（型号）
}

哪里
makeGbtmCallql(k)

是一个具有 k 个簇的 hlme 函数。
我的数据集非常大，大约有 430 万行。网格搜索当然非常耗时，而且 10 核并行处理似乎也没有多大帮助。我可以将核心数提高到更高，甚至达到 20 个，但我怀疑这会产生很大的影响。
如果有任何帮助，我相信并行处理的 cl 参数基于此代码
gridsearch.parallel &lt;- 函数(m,rep,maxiter,minit,cl=NULL)
{
  if(!is.null(cl)){
    clusterSetRNGStream(cl)
    mc &lt;- match.call()$m
    mc$maxiter &lt;- maxiter
    分配（“迷你”，评估（迷你））
    
    clusterCall(cl, function () require(lcmm))
    clusterExport(cl, list(“mc”, “maxiter”, “minit”, as.character(as.list(mc[-1])$data)), envir = 环境())
    
    cat(“请耐心等待，网格搜索正在运行...\n”)
    
    模型 &lt;- parLapply(cl, 1:rep, function(X)
    {
      mc$B &lt;- 替代(随机(minit),parent.frame(n=2))
      return(do.call(as.character(mc[[1]]),as.list(mc[-1])))
    }
    ）
    
    llmodels &lt;- sapply(模型,函数(x){return(x$loglik)})
    
    kmax &lt;- which.max(llmodels)
    mc$B &lt;- 型号[[kmax]]$best
    mc$maxiter &lt;- NULL
    
    cat(&quot;搜索完成，进行最终估计\n&quot;)
    
    return(do.call(as.character(mc[[1]]),as.list(mc[-1])))
  }
  return(do.call(gridsearch, as.list(match.call()[2:5])))
}

（来源：https://github.com/CecileProust-Lima/lcmm/问题/39）
我想知道是否有一种方法可以在另一个使用 GPU 的包中运行它，或者使用其他方法来通过 CPU 加速它（我读了一些有关超线程的内容，也许还可以将其关闭，或者其他）。我在 R 和编码方面都是新手，并且绝对是机器学习方面的大菜鸟，因此任何建议都会非常有帮助。
我的硬件是：
I9-13900k
RTX A4000
64GB DDR5]]></description>
      <guid>https://stackoverflow.com/questions/77966884/gridsearch-in-r-using-lcmm-can-i-utilize-gpu</guid>
      <pubDate>Fri, 09 Feb 2024 08:42:38 GMT</pubDate>
    </item>
    <item>
      <title>nengoDL 安装问题 nengo 版本 4</title>
      <link>https://stackoverflow.com/questions/77966701/nengodl-installation-issue-with-nengo-version-4</link>
      <description><![CDATA[我尝试运行尖峰神经网络的代码，该模型使用以下代码将 CNN 转换为尖峰：
导入nengo
导入nengo_dl

将张量流导入为 tf
#print(cnn_model1.summary())

平方英尺= 20
转换器 = nengo_dl.Converter(
    函数模型，
    交换激活={
        tf.keras.activations.relu: nengo.SpikingRectifiedLinear()},
    scale_firing_rates=sfr,
    突触=0.005，
    inference_only=假）

但是我收到如下错误消息
用户警告：此版本的 NengoDL 尚未使用您的 Nengo 版本 (4.0.0) 进行测试。完全支持的最新版本是 3.2.0。
      警告.warn(warnstr)
 

和
ModuleNotFoundError：没有名为“keras.engine”的模块

然后尝试使用此代码安装nengo旧版本
pip install nengo==3.2.0 nengo-dl==3.4.0 nengo-gui==0.4.7

出现同样的错误
谁能帮我解决这个错误]]></description>
      <guid>https://stackoverflow.com/questions/77966701/nengodl-installation-issue-with-nengo-version-4</guid>
      <pubDate>Fri, 09 Feb 2024 07:54:02 GMT</pubDate>
    </item>
    <item>
      <title>2048游戏的AI</title>
      <link>https://stackoverflow.com/questions/77966419/ai-for-2048-game</link>
      <description><![CDATA[我有用 python 编写的 2048 游戏。
我给自己定义了一个项目，创建一个人工智能来玩这个游戏，不输给2048，并一直成为赢家。
重点是，我不想使用任何特殊的算法来做到这一点，只想创建一个神经网络并向其提供数据以进行学习并每次都变得更好。
我只知道我需要一个好游戏的数据集和一个具有 16 个输入的神经网络，用于显示每次移动的游戏板状态，以及最后的 4 个输出用于显示方向。
我说了很多才达到这个地步 -&gt;问题是我不知道从哪里开始，因为我是人工智能的新手
我首先需要一个数据集，我没有找到好的东西，如果有人有好的数据集并将其发送给我，我表示感谢。
另外，如果您知道启动此项目的好资源或类似的从头开始执行此操作的资源，请告诉我。
非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/77966419/ai-for-2048-game</guid>
      <pubDate>Fri, 09 Feb 2024 06:47:10 GMT</pubDate>
    </item>
    <item>
      <title>即使将其设置为可训练，机器学习模型也不会训练权重</title>
      <link>https://stackoverflow.com/questions/77965679/machine-learning-model-does-not-train-weights-even-after-setting-it-trainable</link>
      <description><![CDATA[以下是我一直在研究的模型。我一直在尝试使用生成器使用数据集 UCF-101 来训练它，但由于某种原因，即使将 VGG19 预训练的层设置为可训练后，它们仍然不会更新其权重值。我尝试了很多替代方案，但到目前为止没有任何效果。
我看到所有其他图层权重都在更新，但那些没有。
我的问题是：
什么可能导致某些可训练层在反向传播期间无法训练？
这是我的模型。
导入tensorflow为tf
从tensorflow.keras.applications导入VGG19
从tensorflow.keras.layers导入图层
从tensorflow.keras.layers导入乘法、Conv2D、注意力
从tensorflow.keras.layers导入（TimeDistributed、LSTM、Dense、Dropout、Flatten、GlobalAveragePooling2D、
                                     批量归一化）
从tensorflow.keras.models导入顺序


SpatialAttentionLayer 类（图层）：
    def __init__(self, **kwargs):
        super(SpatialAttentionLayer, self).__init__(**kwargs)

    def 构建（自身，input_shape）：
        self.conv2d = Conv2D(filters=512, kernel_size=(7, 7), 激活=&#39;softmax&#39;, padding=&#39;same&#39;)

    def 调用（自身，输入）：
        注意力 = self.conv2d(输入)
        return Multiply()([输入，注意力])


类 TemporalAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(TemporalAttentionLayer, self).__init__(**kwargs)

    def 构建（自身，input_shape）：
        self.kernel = self.add_weight(name=&#39;kernel&#39;,
                                      形状=(输入形状[-1], 1),
                                      初始值设定项=&#39;glorot_uniform&#39;,
                                      可训练=真）

    def 调用（自身，输入）：
        Attention_scores = tf.keras.backend.dot(输入，self.kernel)
        注意分数= tf.keras.backend.squeeze（注意分数，-1）
        注意分数 = tf.keras.backend.softmax(注意分数)
        Attention_sequence = 输入 * Attention_scores[..., None]
        attend_sequence = tf.reduce_sum(attended_sequence, axis=1)
        返回参加序列


# todo：为文档创建模型图
def ActionDetectionModel（num_frames，frame_width，frame_height，通道，num_classes，lstm_units = 256，
                         dend_units=1024，dropout_rate=0.5，fine_tune_until=None）：
    # 加载 VGG19 模型，不包括顶层
    base_model = VGG19（include_top = False，权重=&#39;imagenet&#39;，input_shape =（frame_width，frame_height，通道））

    对于 base_model.layers 中的图层：
        可训练层 = False
    如果fine_tune_until：
        对于 base_model.layers[-fine_tune_until:] 中的图层：
            层.可训练= True

    # 定义顺序模型
    模型=顺序（[
        # 添加TimeDistributed层通过VGG19处理每一帧
        TimeDistributed(base_model, input_shape=(num_frames,frame_width,frame_height,channels)),

        #TimeDistributed(SpatialAttentionLayer()),
        时间分布式(GlobalAveragePooling2D()),

        时间分布(Flatten()),

        # 用于时间处理的 LSTM 层
        LSTM（lstm_units，return_sequences = True），

        # 添加时间注意力机制
        时间注意力层（），

        # 用于正则化的批量归一化
        批量归一化(),

        # 用于进一步处理的密集层
        密集（dense_units，激活=&#39;relu&#39;），
        辍学率（辍学率），

        # 最终预测层
        密集（num_classes，激活=&#39;softmax&#39;）
    ]）

    返回模型



有一次，我在模型的开头添加了一个简化的 SpatialAttentionLayer；那时，该模型尚未接受训练。
我根据 VGG19 更改了模型，认为反向传播期间可能会发生一些事情。
我已经改变了学习率几次，但这些权重没有改变。有一次，我将整个 VGG19 设置为可训练，但什么也没有。]]></description>
      <guid>https://stackoverflow.com/questions/77965679/machine-learning-model-does-not-train-weights-even-after-setting-it-trainable</guid>
      <pubDate>Fri, 09 Feb 2024 02:00:44 GMT</pubDate>
    </item>
    <item>
      <title>如何查明两个相似图像之间的差异[关闭]</title>
      <link>https://stackoverflow.com/questions/77965431/how-to-pinpoint-differences-between-two-similar-images</link>
      <description><![CDATA[有关我的问题的一些背景信息。我有一个想要自动化的任务。我收到了两张图片

显示某些事物/部件应位于何处的模板
我必须将图像与模板进行比较。任何缺失的部件我都必须手动放置。

我想编写一个程序，可以采用模板并检查第 2 项是否缺少任何部分。
我已阅读此内容。然而，我的模板是电脑pdf文件，图像是用手机拍摄的照片。因此 stackoverflow 中的建议不太适合我的场景。
有没有办法在丢失的东西上放置一个边界框？
一个例子可以是

我收到了一张数字棋盘的屏幕截图，棋子随机分布在方格上

我收到一张真实棋盘的图片，必须检查它是否与屏幕截图相符。


我想在缺少棋子的棋子上放置一个边界框
我尝试使用结构相似性，但不断获得相似性分数。我更希望获得显示差异的输出图像。同样，我尝试了边缘检测和减法。但由于图像的大小或比例不同，因此它永远不会抵消]]></description>
      <guid>https://stackoverflow.com/questions/77965431/how-to-pinpoint-differences-between-two-similar-images</guid>
      <pubDate>Fri, 09 Feb 2024 00:16:37 GMT</pubDate>
    </item>
    <item>
      <title>如何修复错误：AttributeError：“VotingRegressor”对象没有属性“_model_meta”？</title>
      <link>https://stackoverflow.com/questions/77965050/how-to-fix-the-error-attributeerror-votingregressor-object-has-no-attribute</link>
      <description><![CDATA[我正在从事预测分析并使用 XGBoost 和支持向量回归器模型。
我集成了这两个模型，现在我需要在进行推理时验证这个模型。
集成是通过 VotingRegressor 完成的：
ensemble_model = VotingRegressor([
        （&#39;XGBoost&#39;，model_prod_xgb），
        (&#39;SVR&#39;, model_prod_svr)])
        
ensemble_model.fit(X_train, y_train)

产生错误的部分是这样的：
run_id = ensemble_model._model_meta.run_id

AttributeError：“VotingRegressor”对象没有属性“_model_meta”

如何修复此错误？]]></description>
      <guid>https://stackoverflow.com/questions/77965050/how-to-fix-the-error-attributeerror-votingregressor-object-has-no-attribute</guid>
      <pubDate>Thu, 08 Feb 2024 22:11:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在 mlx 中进行蒙版填充？</title>
      <link>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</link>
      <description><![CDATA[我想在 mlx 中实现 masked_fill，但它与 float(&#39;-inf&#39;) 配合效果不佳
https://pytorch.org/docs/stable/generate /torch.Tensor.masked_fill.html
我正在尝试使用 mlx.core.where 来实现此目的
masked_tensor = mlx.core.where(mask, mlx.core.array(float(&#39;-inf&#39;)), mlx.core.array(0))

但是对于面具
数组([[假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]]，dtype = bool）

这会返回
数组([[nan, nan, -inf, -inf],
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]]，dtype = float32）

这不是我想要的。理想情况下它会返回
数组([[0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf]], dtype=float32)

帮助]]></description>
      <guid>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</guid>
      <pubDate>Thu, 08 Feb 2024 16:55:27 GMT</pubDate>
    </item>
    <item>
      <title>就地修剪 nn.Linear 权重会导致意外错误，需要稍微奇怪的解决方法。需要解释</title>
      <link>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</link>
      <description><![CDATA[失败
导入火炬

def 测试1():
  层 = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试1()

有错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-bb36a010bd86&gt;在&lt;细胞系：10&gt;()
      8 x = 5 - torch.sum(layer(torch.ones(90)))
      9 x.backward()
---&gt; 10 测试1()
     11 # 这也有效
     12

2帧
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
    249 # 一些 Python 版本打印多行函数的第一行
    [第 250 章]
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError: 函数 TBackward0 在索引 0 返回无效渐变 - 得到 [10, 90] 但预期形状与 [10, 100] 兼容

这有效
导入火炬

def test2():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x #主要变化
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试2()

这也有效
导入火炬
def test3():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight) #主要变化
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试3()

我在尝试实现一篇关于模型剪枝（Temporal Neuron Variance Pruning）的论文时遇到了这个问题。我相信这与 autograd 图有关，但我不确定到底发生了什么。我已经看到了有关修剪的链接，并使用第三个片段让我的代码正常工作。我现在正在尝试找出为什么 1 和 2 不起作用。是否有一些解释为什么这些几乎相同的代码片段有效或失败？
我想弄清楚的要点 -

什么是TBackward0
在哪里定义的
哪里引发了运行时错误
为什么需要与旧形状兼容 - 尤其是当梯度已正确修改时（我假设我已正确编辑张量，因为情况 2、3 有效）
我可以更改其他内容（除了 2 个工作案例之外）来实现此功能吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</guid>
      <pubDate>Thu, 08 Feb 2024 05:17:32 GMT</pubDate>
    </item>
    <item>
      <title>在 esp32-cam 中使用人脸识别时出现错误 cam_hal：EV-VSYNC-OVF</title>
      <link>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</link>
      <description><![CDATA[我正在我的 esp32-cam 板上使用示例“CameraWebServer”。上传设置如下：
开发板：AI Thinker ESP32-CAM；
CPU频率：240MHz；
闪光频率：80 Mhz；
闪光模式：QIO。
Arduino 集成开发环境 2.0.0
esp32 乐鑫 版本 2.0.14

通过这些设置，我可以上传我的代码，但粪便识别功能不起作用。当我单击“注册面部”时，没有任何反应，并且我的串行监视器显示消息 EV-VSYNC-OVF。如何解决这个问题？
此外，我已经尝试修改上传设置并更改文件“CameraWebServer.ino”中的参数 config.frame_size 和 config.xclk_freq_hz，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</guid>
      <pubDate>Wed, 07 Feb 2024 22:12:55 GMT</pubDate>
    </item>
    <item>
      <title>如何让这个专家混合模型在张量流中工作？</title>
      <link>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</link>
      <description><![CDATA[我有两个张量。
张量 1 的形状为 (10, None, 16, 16, 64)
张量 2 的形状为 (None, 10)
“无”是c的批量大小
第一个张量表示来自 10 个不同模型的 logits 集合 (10)，其形状是每组 logits，(None) 是批量大小，(16, 16, 64) 是相应模型的输出。
第二个张量表示来自 1 个较小模型的一组 logits（无），即批处理大小，(10) 是 10 个值，表示第一个张量中每组 10 个 logits 的权重应如何。
我想将第一个张量乘以第二个张量，以便输出形状为 (10, None, 16, 16, 64)，并且第一个轴上的每组 logit 由第二个张量的相应 logit 进行加权
然后，我将对第一个轴上的相乘张量求和，以获得 MoE 模型的一个块的输出
以下是所有这些的实施方式（顺便说一下，MOPE 代表预训练专家的混合）：
def CreateMOPEBlock(x, 块, blockNum):
    专家日志 = []

    对于范围内的 i（num_classes）：
        块[i].trainable = False
        ExpertLogits.append(块[i](x))

    门控输入 = x
    GatingConv1 = tf.keras.layers.Conv2D(16, (3, 3), padding=&#39;相同&#39;)(GatingInput)
    GatingLayerNorm1 = tf.keras.layers.LayerNormalization()(GatingConv1)
    GatingLeakyReLU1 = tf.keras.layers.LeakyReLU()(GatingLayerNorm1)
    GatingConv2 = tf.keras.layers.Conv2D(32, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU1)
    GatingLayerNorm2 = tf.keras.layers.LayerNormalization()(GatingConv2)
    GatingLeakyReLU2 = tf.keras.layers.LeakyReLU()(GatingLayerNorm2)
    GatingConv3 = tf.keras.layers.Conv2D(64, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU2)
    GatingLayerNorm3 = tf.keras.layers.LayerNormalization()(GatingConv3)
    GatingLeakyReLU3 = tf.keras.layers.LeakyReLU()(GatingLayerNorm3)
    GatingFlatten = tf.keras.layers.Flatten()(GatingLeakyReLU3)
    GatingLogits = tf.keras.layers.Dense（num_classes，激活=&#39;softmax&#39;）（GatingFlatten）

    logits1 = ExpertLogits # 形状：(10, 无, 16, 16, 64)
    logits2 = GatingLogits # 形状：（无，10）

    # 在这里做一些奇特的数学计算
    多重逻辑 = ?

    返回 tf.keras.layers.add(multiple_logits)

MOPEInput = tf.keras.layers.Input(形状=(32, 32, 3))

# Block1、2和3只是10个keras顺序模型的数组
MOPEBlock1 = CreateMOPEBlock(MOPEInput, 块1)
MOPEBlock2 = CreateMOPEBlock(MOPEBlock1, 块2)
MOPEBlock3 = CreateMOPEBlock(MOPEBlock2, 块3)

MOPEFlatten = tf.keras.layers.Flatten()(MOPEBlock3)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEFlatten)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEX)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEOutput = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;)(MOPEX)

MOPEModel = tf.keras.Model(MOPEInput, MOPEOutput)

我已经尝试自己解决这个问题了！多次！
我还尝试询问多种大型语言模型，从 Mixtral-8x7b（以我的名字命名）到 GPT4。
结果看起来像这样：
将张量流导入为 tf

# 假设这些是你的张量
张量1 = tf.placeholder(tf.float32, shape=(10, 无, 16, 16, 64))
张量2 = tf.placeholder(tf.float32, shape=(无, 10))

# 重塑张量2（无，10）-&gt; （无、10、1、1、1）
tensor2_expanded = tf.expand_dims(tf.expand_dims(tf.expand_dims(tensor2, axis=-1), axis=-1), axis=-1)

# 排列张量1的轴 (10, None, 16, 16, 64) -&gt; （无、10、16、16、64）
tensor1_permuted = tf.transpose(tensor1, perm=[1, 0, 2, 3, 4])

# 张量相乘
结果= tf.multiply（tensor1_permuted，tensor2_expanded）

# 最后，将结果的轴排列回来 (None, 10, 16, 16, 64) -&gt; （10、无、16、16、64）
结果 = tf.transpose(结果, perm=[1, 0, 2, 3, 4])

即使对每个模型进行了广泛的调试，这些模型的解决方案也不起作用。
我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</guid>
      <pubDate>Wed, 07 Feb 2024 21:13:53 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformers 的推荐系统获取具有项目名称和类型的数据集的有效方法[关闭]</title>
      <link>https://stackoverflow.com/questions/77852593/efficient-methods-for-obtaining-dataset-with-item-names-and-types-for-transforme</link>
      <description><![CDATA[我找不到一种简单的方法（最好是 API）来获取大量项目名称及其类型的数据。例如，电影有其类型（恐怖、爱情、喜剧），游戏有其类型（恐怖、单人），音乐有其类型（古典、钢琴）等。
我能找到的最好办法就是访问 IMDB 等网站，并尝试从网站本身中获取数据（这在大多数地方甚至可能违反 TOS？）。
您是否找到了任何可以获取服务的完整项目列表及其类别等的 API，您能否分享它或者建议一种比抓取网络更简单的方法，因为它已经花费了我更多的时间应该，但在承诺之前我很想听听也许有人有更好的想法..
我试图在 Spotfiy、Google、Steam 等上查找 API，这将使我能够访问所有可能的游戏名称及其类别，但在其 API 中找不到任何此类端点。]]></description>
      <guid>https://stackoverflow.com/questions/77852593/efficient-methods-for-obtaining-dataset-with-item-names-and-types-for-transforme</guid>
      <pubDate>Sat, 20 Jan 2024 20:18:55 GMT</pubDate>
    </item>
    <item>
      <title>如何将极坐标数据框与 scikit-learn 一起使用？</title>
      <link>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</link>
      <description><![CDATA[我无法将极坐标数据帧与 scikitlearn 一起使用进行机器学习训练。
目前，我正在极坐标中进行所有数据帧预处理，在模型训练期间，我将其转换为 pandas 数据帧以使其正常工作。
是否有任何方法可以直接使用 Polars 数据帧进行 ML 训练而不将其更改为 pandas？]]></description>
      <guid>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</guid>
      <pubDate>Fri, 11 Nov 2022 05:59:55 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 变换。在分割任务中组合图像对的使用</title>
      <link>https://stackoverflow.com/questions/66284850/pytorch-transforms-compose-usage-for-pair-of-images-in-segmentation-tasks</link>
      <description><![CDATA[我正在尝试在分段任务中使用 transforms.Compose() 。但我不确定如何对图像和蒙版使用相同的（几乎）随机变换。
所以在我的分割任务中，我有原始图片和相应的掩模，我想生成更多随机变换的图像对来训练 popurse。这意味着如果我对原始图片进行一些变换，这种变换也应该发生在我的蒙版图片上，然后这对图像就可以进入我的 CNN。我的变压器是这样的：
train_transform = Transforms.Compose([
            Transforms.Resize(512), # 调整大小，较小的边缘将被匹配。
            变换.RandomHorizo​​ntalFlip(p=0.5),
            变换.RandomVerticalFlip(p=0.5),
            变换.RandomRotation(90),
            变换.RandomResizedCrop(320,scale=(0.3, 1.0)),
            添加高斯噪声(0., 1.),
            Transforms.ToTensor(), # 将 PIL 图像或 ndarray 转换为张量。
            Transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # 标准化为 Imagenet 均值和标准差
]）

mask_transform = 变换.Compose([
            Transforms.Resize(512), # 调整大小，较小的边缘将被匹配。
            变换.RandomHorizo​​ntalFlip(p=0.5),
            变换.RandomVerticalFlip(p=0.5),
            变换.RandomRotation(90),
            变换.RandomResizedCrop(320,scale=(0.3, 1.0)),
            ##---------------------！------------------
            Transforms.ToTensor(), # 将 PIL 图像或 ndarray 转换为张量。
            Transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # 标准化为 Imagenet 均值和标准差
]）


请注意，在代码块中，我添加了一个可以向原始图像转换添加随机噪声的类，该类不在 mask_transformation 中，我希望我的蒙版图像遵循原始图像转换，但忽略随机噪声。那么这两个转变如何成对发生（具有相同的随机行为）？]]></description>
      <guid>https://stackoverflow.com/questions/66284850/pytorch-transforms-compose-usage-for-pair-of-images-in-segmentation-tasks</guid>
      <pubDate>Fri, 19 Feb 2021 20:52:30 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络只预测一件事</title>
      <link>https://stackoverflow.com/questions/62712282/my-neural-network-only-predicts-one-thing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/62712282/my-neural-network-only-predicts-one-thing</guid>
      <pubDate>Fri, 03 Jul 2020 09:16:53 GMT</pubDate>
    </item>
    <item>
      <title>Mahout 基于项目的推荐引擎，没有偏好值</title>
      <link>https://stackoverflow.com/questions/17712903/mahout-item-based-recommendation-engine-with-no-preference-values</link>
      <description><![CDATA[我正在尝试使用 Mahout 构建一个推荐引擎，该引擎仅根据项目之间的相似性提供推荐，而不考虑用户偏好（即评分）。项目相似度由 mahout 外部的一些其他进程计算并保存到文件中。到目前为止，我已经确定我可以使用该类：
GenericBooleanPrefItemBasedRecommender

...选择项目，文档称其“适合在数据中不存在偏好值概念时使用”。但是，该类仍将其作为输入：
(DataModel dataModel, ItemSimilarity 相似度)

我知道我可以使用 ItemSimilarity 类来提供项目到项目的相似度值，但是在这种情况下我的数据模型是什么？我没有偏好，这似乎正是数据模型所代表的东西。我该如何解决这个问题，或者我在这里看错了东西？]]></description>
      <guid>https://stackoverflow.com/questions/17712903/mahout-item-based-recommendation-engine-with-no-preference-values</guid>
      <pubDate>Thu, 18 Jul 2013 01:13:28 GMT</pubDate>
    </item>
    </channel>
</rss>