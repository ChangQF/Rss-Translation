<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 23 Oct 2024 01:15:38 GMT</lastBuildDate>
    <item>
      <title>在单线段（触觉图）上进行图像相似性比较的理想方法是什么？</title>
      <link>https://stackoverflow.com/questions/79116100/ideal-way-to-do-image-similarity-comparisons-on-single-line-segments-tactile-ma</link>
      <description><![CDATA[大家好，
我正在尝试寻找一种理想且最准确的方法来对手绘触觉地图和其模板进行图像相似性比较。目标是提供一个值来显示手绘与模板的相似程度。图像要么是：完整的触觉地图绘图，要么是相同地图绘图的最短路径。这些图像中没有轮廓，因此这是分析中最困难的部分，因为它们都是单线段。

手绘地图
触觉地图模板

我不知道该怎么做了。请提供您的想法和可能的解决方案来完成此图像相似性任务！请随时向我提出任何可能帮助您回答的问题。
我尝试过的方法是：

使用自定义 Matlab 脚本对手绘到模板执行不同的仿射变换（缩放、旋转、剪切、拉伸、translateX、translateY），该模板根据不同的值提供从 0 到 1 的分数。这没有按预期工作，因为有时，转换不知道手绘的垂直线段应该与模板中的正确垂直线段对齐，特别是如果绘图未完成，具有模板图的大多数特征的完整绘图就可以了。将图像绘图分成象限会有帮助吗？

使用 Gemini 模型：1.5-Flash、1.5-Pro 和 1.0-Pro-vision 来比较两个输入图像，这两个图像也提供了不合适的分数（例如，一幅绘图，人类会给它 2 分（满分 10 分），但 Gemini 模型给了它 7 分（满分 10 分）。我还尝试使用 Gemini API 密钥对 Gemini-1.5-Flash-001 模型进行微调（我提供了 963 个相似度分数应该输出的示例（人类评分），但当我在 183 张验证图像上运行它时，它仍然给出了不合适的分数，比如给一幅糟糕的绘图更高的分数，而同一张地图的好的绘图的分数要低得多）。我曾尝试让模型给出分数推理的口头描述，但它产生了幻觉，并给出了完全错误的分割方向和方向的空间分析！例如：绘图将是垂直的线段，然后 90 度角右转水平线段，但它会表示完全不同的方向。我给 Gemini 的提示：
重要提示：完全忽略线条质量和样式的所有方面，包括平滑度、曲线、宽度和小的不规则性。将所有线条视为连接关键点的完美直线。

比较这两幅图像并提供 0 到 10 之间的相似度分数，其中 0 表示完全不同，高分表示所有局部特征都处于正确的位置，基于以下评分指南：

相似度评分指南标准：
- 整体结构布局
- 线段长度成比例
- 线段之间的角度允许 ±15° 变化
- 每次角度变化时纠正连续方向线段左转或右转变化

回答：
- 来自总评分标准的单个数字分数（0 到 10）
- 用 2-3 句话解释，不多说，说明分配的评分标准，突出第一幅和第二幅图像之间的关键相似点或差异


]]></description>
      <guid>https://stackoverflow.com/questions/79116100/ideal-way-to-do-image-similarity-comparisons-on-single-line-segments-tactile-ma</guid>
      <pubDate>Wed, 23 Oct 2024 00:36:59 GMT</pubDate>
    </item>
    <item>
      <title>使用计算机视觉查找图像中的异常/缺失部分</title>
      <link>https://stackoverflow.com/questions/79115794/finding-anomalies-missing-parts-in-image-using-computer-vision</link>
      <description><![CDATA[我正在尝试创建一个程序以便在工业环境中验证组装的组件。基本上，我希望能够检测到组件是否缺少任何部分，例如螺钉或支架等。我的第一个想法是简单地拍摄正确组件的参考图，然后使用基于特征的相似度得分将所有其他图像与该图像进行比较（就像在对这个问题的回答中一样。即使图像非常相似，只有一个明显的区别（参见发布的图像，忽略蓝色标记），这也不会产生任何有用的结果。我也尝试过其他技术，比如结构相似性指数，但根本不起作用。
第一张图片是一个有缺陷的组件的示例，缺少右下角的支架和螺丝，而第二张图片是一个正确的组件。这只是一个例子，其他东西，如左上角的螺丝或 4 个橡胶片也可能缺失。那么我的问题是，首先，为什么通过特征相似性检测效果如此糟糕，即使图片除了缺少支架外基本上是相同的？其次，解决这个问题的合理方法是什么？我的下一个想法是，要么在正确组件的图像上训练分类器，要么在错误组件的变体上训练分类器，但可能存在许多不同的变体，因此这可能会耗费大量资源。另一个想法是在正确组件的图像上训练异常检测模型，希望它能够检测到错误组件图像中的异常。到目前为止，我还不确定任何方法，因此我非常感谢任何有关此事的指导！
此外，我们可以做出以下假设：

组件通常在图像中具有一致的方向。
图像中的光照条件将保持相当均匀。
]]></description>
      <guid>https://stackoverflow.com/questions/79115794/finding-anomalies-missing-parts-in-image-using-computer-vision</guid>
      <pubDate>Tue, 22 Oct 2024 21:32:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么现代对象检测模型的 Github 存储库会显示在那个奇怪的命令行界面中？[关闭]</title>
      <link>https://stackoverflow.com/questions/79115741/why-github-repos-of-modern-object-detection-model-be-presented-in-that-weird-com</link>
      <description><![CDATA[我是深度学习新手，目前正在研究许多神圣的 Github 对象检测存储库。然而，无论我在哪里看到，这些存储库中的训练/评估代码部分总是使用参数解析器在命令行界面中显示。这是我正在研究的一个 repo 中的一个例子：
parser.add_argument(&quot;--dataset_type&quot;, default=&quot;voc&quot;, type=str,
help=&#39;指定数据集类型。目前支持 voc 和 open_images。&#39;)

parser.add_argument(&#39;--datasets&#39;, nargs=&#39;+&#39;, help=&#39;数据集目录路径&#39;)
parser.add_argument(&#39;--validation_dataset&#39;, help=&#39;数据集目录路径&#39;)
parser.add_argument(&#39;--balance_data&#39;, action=&#39;store_true&#39;,
help=&quot;通过对更频繁的标签进行下采样来平衡训练数据。&quot;)

parser.add_argument(&#39;--net&#39;, default=&quot;vgg16-ssd&quot;,
help=&quot;网络架构，它可以是mb1-ssd、mb1-lite-ssd、mb2-ssd-lite 或 vgg16-ssd。&quot;)
parser.add_argument(&#39;--freeze_base_net&#39;, action=&#39;store_true&#39;,
help=&quot;冻结基础网络层。&quot;)
parser.add_argument(&#39;--freeze_net&#39;, action=&#39;store_true&#39;,
help=&quot;冻结除预测头之外的所有层。&quot;)

parser.add_argument(&#39;--mb2_width_mult&#39;, default=1.0, type=float,
help=&#39;MobilenetV2 的宽度乘数&#39;)

我的意思是，为什么？参数解析器使得阅读代码和查看变量去向变得非常非常困难。上面的代码中，参数解析器代码有将近百行，看清参数是什么真的让我眼花缭乱。而且 Python 语法难道还不够方便吗，只需将所有训练参数转储到配置文件中即可？为什么每次训练时都要输入 3 行终端命令，而且为什么要将一个方便的 Python 程序变成看起来像命令行 ffmpeg 的东西？请有人向我解释一下，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79115741/why-github-repos-of-modern-object-detection-model-be-presented-in-that-weird-com</guid>
      <pubDate>Tue, 22 Oct 2024 21:10:51 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据框中的单个列进行单列编码？</title>
      <link>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</link>
      <description><![CDATA[我有一个名为“vehicles”的数据框，它有 8 列。其中 7 列是数字，但名为“Car_name”的列在数据框中是索引 1，是分类的。我需要对其进行编码
我试过这个代码，但不起作用
ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() 获得了一个意外的关键字参数“categorical_features”

然而，这在我使用的 YouTube 视频中运行良好。]]></description>
      <guid>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</guid>
      <pubDate>Tue, 22 Oct 2024 15:12:04 GMT</pubDate>
    </item>
    <item>
      <title>如何将 *.mil 加载到 coremltools 中？</title>
      <link>https://stackoverflow.com/questions/79114613/how-to-load-a-mil-into-coremltools</link>
      <description><![CDATA[在 Apple 的文档中，模型中间语言 (MIL) 被描述为一种中间语言。我在 Apple 的系统中发现了许多 .mil 文件。您可以使用以下命令轻松找到它们：
find /System/Library -name &quot;*.mil&quot;

我正在尝试研究这些 .mil 文件并将它们转换为 Core ML 文件以运行它们。
apple 的示例向我们展示了如何生成 mil 程序。但是我找不到从 *.mil 文件加载 mil 程序的方法。
以下是 Apple 的示例。
# import builder
from coremltools.converters.mil import Builder as mb

# MIL 程序的输入是张量列表。这里我们有一个输入，其
# 形状为 (1, 100, 100, 3)，隐式 dtype == fp32
@mb.program(input_specs=[mb.TensorSpec(shape=(1, 100, 100, 3)),])
def prog(x):
# MIL 操作采用命名输入（而不是位置输入）。
# 此处 `name` 参数是可选的。
x = mb.relu(x=x, name=&#39;relu&#39;)
x = mb.transpose(x=x, perm=[0, 3, 1, 2], name=&#39;transpose&#39;)
x = mb.reduce_mean(x=x, axis=[2, 3], keep_dims=False, name=&#39;reduce&#39;)
x = mb.log(x=x, name=&#39;log&#39;)
return x

print(prog)

main(%x: (1, 100, 100, 3, fp32)(Tensor)) {
block0() {
%relu: (1, 100, 100, 3, fp32)(Tensor) = relu(x=%x, name=&quot;relu&quot;)
%transpose_perm_0: (4,i32)*(Tensor) = const(val=[0, 3, 1, 2], name=&quot;transpose_perm_0&quot;)
%transpose: (1, 3, 100, 100, fp32)(Tensor) = transpose(x=%relu, perm=%transpose_perm_0, name=&quot;transpose&quot;)
%reduce_axes_0: (2,i32)*(Tensor) = const(val=[2, 3], name=&quot;reduce_axes_0&quot;)
%reduce_keep_dims_0: (bool)*(Scalar) = const(val=False, name=&quot;reduce_keep_dims_0&quot;)
%reduce: (1, 3, fp32)(Tensor) = reduce_mean(x=%transpose, axis=%reduce_axes_0, keep_dims=%reduce_keep_dims_0，name=&quot;reduce&quot;)
%log_epsilon_0: (fp32)*(Scalar) = const(val=1e-45，name=&quot;log_epsilon_0&quot;)
%log: (1, 3, fp32)(Tensor) = log(x=%reduce，epsilon=%log_epsilon_0，name=&quot;log&quot;)
} -&gt; (%log)
}

以下是来自 macOS 的 *.mil 文件
/System/Library/AssetsV2/com_apple_MobileAsset_UAF_Siri_Understanding/purpose_auto/86aa08fa2cf25cbea1f9f8dee8c0bdad3a3802b9.asset/AssetData/sr/Hermes/int8_conformer_matrix_split.mlmodelc/model.bnns.mil
program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{&quot;coremlc-component-MIL&quot;, &quot;5.33.5&quot;}, {&quot;coremlc-version&quot;, &quot;1877.0.10.0.2&quot;}, {&quot;coremltools-component-torch&quot;, &quot;2.0.1&quot;}, {&quot;coremltools-source-dialect&quot;, &quot;TorchScript&quot;}, {&quot;coremltools-version&quot;, &quot;7.1.2&quot;}})]
{
func main&lt;ios17&gt;(tensor&lt;fp16, [256, 14]&gt; conformer_cnn_cache_0_shared_in, tensor&lt;fp16, [256, 14]&gt; conformer_cnn_cache_1_shared_in, tensor&lt;fp16, [256, 14]&gt; constrainer_cnn_cache_2_shared_in，张量
所以我的最后一个问题是

有没有办法将 .mil 文件加载到 coremltools.converters.mil.mil.program.Program？

上面 Apple 网站上的 mil 和 /System/Library 中的 mil 一样吗？

]]></description>
      <guid>https://stackoverflow.com/questions/79114613/how-to-load-a-mil-into-coremltools</guid>
      <pubDate>Tue, 22 Oct 2024 14:38:22 GMT</pubDate>
    </item>
    <item>
      <title>决策树的修剪函数</title>
      <link>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</link>
      <description><![CDATA[我正在从头开始创建决策树并实施修剪。目前，我认为我的代码中的问题是，当我修剪一棵树时，我创建的新叶节点不会放入原始树中，因此当我计算新的准确度时，它是有效的，并且我的所有分支都会被修剪。附件是我的树类、叶类和修剪函数的代码。
我的问题是，当我尝试计算新的准确度时，我为尝试修剪而创建的新叶节点似乎没有反映在原始树中。
我如何修复/重构我的代码，以便修剪更新当前树，这样当我计算新的准确度时，就可以确定修剪是否成功？
class TreeNode:
def __init__(self, feature, split,depth, left = None, right = None):
&quot;&quot;&quot;
self.feature = 节点分裂的特征
self.split = 节点分裂的特征的值
self.left = 左子节点
self.right = 右子节点
self.depth = 此时树的深度
&quot;&quot;&quot;
self.feature = 特征
self.split = 分裂
self.left = 左
self.right = 右
self.depth = 深度

def getLeft(self):
return self.left

def getRight(self):
return self.right

def getFeature(self):
return self.feature

def getSplit(self):
return self.split

def getDepth(self):
return self.depth

def eval(self, sample):
if sample[self.feature] &lt; self.split:
return self.left.eval(sample)
else:
return self.right.eval(sample)

class LeafNode:
def __init__(self, roomNumber, users,depth):
&quot;&quot;&quot;
self.roomNumer = 分配给叶子的房间号
self.depth = 树中叶子的深度
&quot;&quot;&quot;
self.roomNumber = roomNumber
self.depth =depth
self.users = users

def getRoomNumber(self):
返回 self.roomNumber

def getDepth(self):
返回 self.depth

def getUsers(self):
返回 self.users

def eval(self, sample):
self.users += 1
返回 self.getRoomNumber()

def pruneTree(original_tree, validation, node):
如果节点为 None:
返回 None
如果 isinstance(node, LeafNode):
返回节点
node.left = pruneTree(original_tree, validation, node.left)
node.right = pruneTree(original_tree, validation, node.right)
如果 isinstance(node.left, LeafNode) 和 isinstance(node.right, LeafNode):

current_accuracy = assess(validation, original_tree)

leftRoom, leftPopulation = node.left.getRoomNumber(), node.left.getUsers()

rightRoom, rightPopulation = node.right.getRoomNumber(), node.right.getUsers()

previous_feature, previous_split, previous_depth, previous_left, previous_right = node.getFeature(), node.getSplit(), node.getDepth(), node.getLeft(), node.getRight()

newRoom = -1

newPopulation = leftPopulation + rightPopulation

如果 rightPopulation &gt;= leftPopulation:
newRoom = rightRoom
否则:
newRoom = leftRoom

node = LeafNode(roomNumber = newRoom, users=newPopulation,depth = previous_depth)

new_accuracy = assess(validation, original_tree)

如果 new_accuracy &lt; current_accuracy:
node = TreeNode(split = previous_split, feature=previous_feature,depth=previous_depth)
node.left = previous_left
node.right = previous_right
返回节点

def assess(test_db, trained_tree):
num_correct = 0
对于 test_db 中的数据：
sample = data[:-1]
prediction = trained_tree.eval(sample)

如果 prediction == data[-1]:
num_correct += 1
返回 num_correct/len(test_db)

pruned_tree = pruneTree(tree, validation, tree)
]]></description>
      <guid>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</guid>
      <pubDate>Tue, 22 Oct 2024 11:59:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 中加载 *.mil 文件？[关闭]</title>
      <link>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</link>
      <description><![CDATA[在 Apple 的文档中，模型中间语言 (MIL) 被描述为一种中间语言。我在 Apple 的系统中发现了许多 .mil 文件。您可以使用以下命令轻松找到它们：
find /System/Library -name &quot;*.mil&quot;

我正在尝试研究这些 .mil 文件并将它们转换为 Core ML 文件以运行它们。但是，我在 coremltools 中找不到允许我这样做的任何功能。您知道有什么方法可以实现这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</guid>
      <pubDate>Tue, 22 Oct 2024 09:45:03 GMT</pubDate>
    </item>
    <item>
      <title>过滤相关图像的自动化方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</link>
      <description><![CDATA[我有一组遥感图像，如下图所示。总共有大约 100,000 张图像，但只有少数与我的任务相关。这些图像中有很多都是我不需要的海洋之类的东西，或者只是损坏的照片。我感兴趣的是下图中第 2 和第 4 幅图像（它们包括森林区域、道路、村庄等）。我想知道是否有某种自动方法来选择这些图像。绘制颜色直方图时肯定存在模式，但我很难想出一种方法来从不相关的图像中过滤出相关的图像。这只是为了数据清理步骤。
我刚刚研究了深度聚类技术，但想知道是否有更简单的解决方案，因为它只是为了数据清理。
任何有关此任务的信息都非常感谢！
]]></description>
      <guid>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</guid>
      <pubDate>Tue, 22 Oct 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将 MS COCO 2017 测试数据集中的图像分成包含小、中、大物体的图像？</title>
      <link>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</link>
      <description><![CDATA[我是计算机视觉领域的新手，正在从事一项小任务，即在 MS COCO 测试数据集中分离包含小物体的图像。由于测试集没有注释，有什么方法可以完成此任务吗？如果您能提供任何帮助，我将不胜感激。
我尝试使用图像的宽度和高度参数，但在小图像部分只得到了 360 张图像。如何获取测试集的注释信息？]]></description>
      <guid>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</guid>
      <pubDate>Tue, 22 Oct 2024 08:33:45 GMT</pubDate>
    </item>
    <item>
      <title>SRGAN Android Tensorflow Lite 推理输出与 Python 版本不匹配</title>
      <link>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</link>
      <description><![CDATA[我尝试使用此模型的 tflite 版本实现 Android Kotlin 应用，https://github.com/krasserm/super-resolution/blob/master
我有一个 gan_generator.tflite 模型。我可以推断它并在 Python 中使用它获得正确的输出。但是即使我进行了相同的后处理操作，我也无法在 Android Kotlin 中获得与位图相同的结果。我遗漏了什么？
from model.srgan import generator
from utils import load_image, plot_sample
from model import resolve_single
import tensorflow as tf

model = generator()
model.load_weights(&#39;weights/srgan/gan_generator.h5&#39;)

# 从 Keras 模型创建 TFLite 转换器
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 设置转换参数（可选）
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.target_spec.supported_types = [tf.float32]

# 转换模型
tflite_model = converter.convert()

# 将 TFLite 模型保存到文件
with open(&#39;gan_generator.tflite&#39;, &#39;wb&#39;) as f:
f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=&#39;gan_generator.tflite&#39;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()

output_details = interpretationer.get_output_details()

input_shape = input_details[0][&#39;shape&#39;]

input_data = np.asarray(load_image(&#39;demo/0869x4-crop.png&#39;), dtype=np.float32)
input_data = np.expand_dims(input_data, axis=0)
input_data = np.reshape(input_data, (1, input_data.shape[1], input_data.shape[2], 3))

# 设置输入张量
interpreter.resize_tensor_input(input_details[0][&#39;index&#39;], (1, input_data.shape[1], input_data.shape[2], 3),strict=True)
interpreter.allocate_tensors()

interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data)

interpreter.invoke()

# 获取输出
output_data = interpretation.get_tensor(output_details[0][&#39;index&#39;])

output_data = np.squeeze(output_data)
output_data = np.clip(output_data, 0, 255).astype(np.uint8)

# 从 NumPy 数组创建 PIL 图像
image = Image.fromarray(output_data)
image #此处创建的输出图像成功

###----------Android 端-------------------------
 val tensorImage = TensorImage(INPUT_IMAGE_TYPE).also { it.load(inputBitmap) }
val processingImage = imageProcessor.process(tensorImage)

//[1,4X,4X,3] 输出
val outputBuffer = TensorBuffer.createFixedSize(
intArrayOf(
1,
processingImage.width * outputShapeForScaling,
processingImage.height * outputShapeForScaling,
3
),
OUTPUT_IMAGE_TYPE
)

// [1, None, None, 3] 动态输入
explainer!!.resizeInput(
explainer!!.getInputTensor(0).index(),
intArrayOf(1, processingImage.width, processingImage.height, 3)
)

解释器！！.allocateTensors()

解释器！！.run(processedImage.buffer, outputBuffer.buffer)

val processingOutput = processOutput(
outputBuffer,
width = processingImage.width * 4,
height = processingImage.height * 4
)


##---------后期处理---------------------------
private fun processOutput(outputBuffer: TensorBuffer, width: Int, height: Int): Bitmap {
val data = outputBuffer.floatArray

// 检查浮点数组是否具有正确数量的元素
if (data.size != width * height * 3) {
throw IllegalArgumentException(&quot;数据大小与预期图像大小不匹配。&quot;)
}

// 创建一个空的 Bitmap
val bitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)

//遍历浮点数组并设置位图中的像素
var index: Int
for (y in 0 till height) {
for (x in 0 till width) {
index = (y * width + x) * 3
// 从浮点数组中提取 RGB 值
val r = data[index].coerceIn(0f, 255f).toInt()
val g = data[index + 1].coerceIn(0f, 255f).toInt()
val b = data[index + 2].coerceIn(0f, 255f).toInt()

// 设置像素颜色（我们将 alpha 设置为 255 以实现完全不透明度）
bitmap.setPixel(x, y, Color.rgb(r, g, b))
}
}

return bitmap
}

结果：
]]></description>
      <guid>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</guid>
      <pubDate>Mon, 21 Oct 2024 19:24:29 GMT</pubDate>
    </item>
    <item>
      <title>如何通过flask API调用逻辑回归模型？</title>
      <link>https://stackoverflow.com/questions/79111190/how-to-call-logistic-regression-model-through-flask-api</link>
      <description><![CDATA[创建了一个逻辑回归模型，该模型运行良好，在本地运行时（通过 PyCharm）预测的值到目前为止都是正确的。
尝试通过 flask API 调用模型，从而创建一个 pickle 文件。
不确定如何在 API 中调用 classifier.predict 方法（即下面代码中的第 7 行）？因为我没有可用的实例（我遗漏了什么？或者做错了什么？）
API 代码

从 flask 导入 Flask、request、render_template、jsonify
导入 pickle

app = Flask(__name__)

-- 加载 pickle 文件
使用 open(&#39;sc.pkl&#39;, &#39;rb&#39;) 作为文件：
model = pickle.load(file) 

@app.route(&quot;/test&quot;, methods=[&quot;GET&quot;])
def test():
trans = model.transform([[1, 0, 12, 30]])

-- 想法是先通过硬编码值进行测试，然后使其参数化。
这是我遇到困难的地方，因为我没有分类器的实例来调用预测方法（下面代码中的第 7 行）
这是实际的代码我已成功从 pycharm(.py 文件) 运行 -
1. 读取 CSV，
为 x、y 赋值，
运行 train_test_split 函数
2. sc = StandardScaler()
3. x_train = sc.fit_transform(x_train)
4. x_test = sc.transform(x_test)

5. classifier = LogisticRegression(random_state=0)
6. classifier.fit(x_train,y_train)

7. print(classifier.predict(sc.transform([[1, 0, 12,30]]))) -- 预测
8. pickle.dump(sc, open(&#39;sc.pkl&#39;,&#39;wb&#39;)) -- 此处正在创建 Pickle 文件
]]></description>
      <guid>https://stackoverflow.com/questions/79111190/how-to-call-logistic-regression-model-through-flask-api</guid>
      <pubDate>Mon, 21 Oct 2024 17:46:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow/Keras CNN 图像分类中，预测准确度低于训练/测试准确度，为什么？</title>
      <link>https://stackoverflow.com/questions/73714379/in-tensorflow-keras-cnn-image-classification-predictions-are-less-accurate-tha</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/73714379/in-tensorflow-keras-cnn-image-classification-predictions-are-less-accurate-tha</guid>
      <pubDate>Wed, 14 Sep 2022 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>用于检测猫和狗的 Keras 神经网络预测不正确</title>
      <link>https://stackoverflow.com/questions/72221474/keras-neural-network-for-detecting-cats-vs-dogs-dont-predicting-correctly</link>
      <description><![CDATA[我一直在关注 YouTube 教程，让神经网络预测猫与狗的图像。教程结束时并未展示如何使用任何图像进行预测，我一直在努力理清它。
这是网络的代码：
import numpy as np
import cv2
import os
import random
import matplotlib.pyplot as plt
import pickle

DIRECTORY = r&#39;/content/drive/MyDrive/neural&#39;
CATEGORIES = [&#39;cats&#39;, &#39;dogs&#39;]

IMG_SIZE = 100
data = []

for category in CATEGORIES:
folder = os.path.join(DIRECTORY, category)
label = CATEGORIES.index(category)
for img in os.listdir(folder):
img_path = os.path.join(folder, img)
img_arr = cv2.imread(img_path)
img_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))
data.append([img_arr, label])

random.shuffle(data)

X = []
y=[]
对于数据中的特征、标签：
X.append(features)
y.append(labels)

X = np.array(X)
y = np.array(y)

X = X/255

从 keras.models 导入 Sequential
从 keras.layers 导入 Conv2D、MaxPooling2D、Flatten、Dense

model = Sequential()
model.add(Conv2D(64, (3,3), 激活 = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(64, (3,3), 激活 = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())

model.add(Dense(128, input_shape = X.shape[1:],activation = &#39;relu&#39;))

model.add(Dense(2,activation = &#39;softmax&#39;))

model.compile(optimizer = &#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics = [&#39;accuracy&#39;])

model.fit(X, y, epochs = 10,validation_split = 0.1)


以下是训练结果：
Epoch 10/10
647/647 [=================================] - 58s 90ms/step - 损失： 0.0262 - 准确率：0.9917 - val_loss：1.4013 - val_accuracy：0.7630
现在我尝试用模型进行预测。
import numpy as np
import cv2
import keras
CATEGORIES = [&#39;Cat&#39;, &#39;Dog&#39;]

def image(path):
img = cv2.imread(path)
new_arr = cv2.resize(img, (100, 100))
new_arr = np.array(new_arr)
new_arr = new_arr.reshape(-1, 100, 100, 3)
new_arr = new_arr/255
return new_arr

prediction = model.predict([image(&#39;/content/drive/MyDrive/neural/test/photo-1609779361684-8196b3a0abf1.jpg&#39;)])
print(CATEGORIES[prediction.argmax()])

我得到的结果完全是随机的。我认为问题在于调整我想要预测的图像的大小，但我尝试了不同的东西，却无法解决。]]></description>
      <guid>https://stackoverflow.com/questions/72221474/keras-neural-network-for-detecting-cats-vs-dogs-dont-predicting-correctly</guid>
      <pubDate>Thu, 12 May 2022 19:59:40 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 CNN 模型的准确性</title>
      <link>https://stackoverflow.com/questions/71083471/how-can-i-increase-my-cnn-models-accuracy</link>
      <description><![CDATA[我建立了一个 CNN 模型，将面部情绪分为快乐、悲伤、精力充沛和中性面部。我使用了 Vgg16 预训练模型并冻结了所有层。经过 50 个训练周期后，我的模型的测试准确率为 0.65，验证损失约为 0.8。
我的训练数据文件夹有 16000（4x4000），验证数据文件夹有 2000（4x500），测试数据文件夹有 4000（4x1000）个 RGB 图像。

你有什么建议可以提高模型准确率？

我尝试用我的模型做一些预测，预测的类别总是相同的。什么会导致问题？


到目前为止我尝试过的方法：

添加 dropout 层 (0.5)
在最后一层之前添加 Dense (256, relu)
对训练和验证数据进行混洗。
将学习率降低到 1e-5

但我无法提高验证和测试准确率。
我的代码
train_src = &quot;/content/drive/MyDrive/Affectnet/train_class/&quot;
val_src = &quot;/content/drive/MyDrive/Affectnet/val_class/&quot;
test_src=&quot;/content/drive/MyDrive/Affectnet/test_classs/&quot;

train_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
rescale=1./255, 
sher_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,

)

train_generator = train_datagen.flow_from_directory(
train_src,
target_size=(224,224 ),
batch_size=32,
class_mode=&#39;categorical&#39;,
shuffle=True
)

validation_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
rescale=1./255
)

validation_generator = validation_datagen.flow_from_directory(
val_src,
target_size=(224, 224),
batch_size=32,
class_mode=&#39;categorical&#39;,
shuffle=True
)
conv_base = tensorflow.keras.applications.VGG16(weights=&#39;imagenet&#39;,
include_top=False,
input_shape=(224, 224, 3)
)
for layer in conv_base.layers:
layer.trainable = False

model = tensorflow.keras.models.Sequential()

# VGG16 添加为卷积层。
model.add(conv_base)

# 层从矩阵转换为向量。
model.add(tensorflow.keras.layers.Flatten())

# 我们的神经层已添加。
model.add(tensorflow.keras.layers.Dropout(0.5))
model.add(tensorflow.keras.layers.Dense(256, 激活=&#39;relu&#39;))

model.add(tensorflow.keras.layers.Dense(4, 激活=&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,
optimizer=tensorflow.keras.optimizers.Adam(lr=1e-5),
metrics=[&#39;acc&#39;])
history = model.fit_generator(
train_generator,
epochs=50,
steps_per_epoch=100,
validation_data=validation_generator,
validation_steps=5,
worker=8
)

损失和准确性]]></description>
      <guid>https://stackoverflow.com/questions/71083471/how-can-i-increase-my-cnn-models-accuracy</guid>
      <pubDate>Fri, 11 Feb 2022 16:33:02 GMT</pubDate>
    </item>
    <item>
      <title>将 onnx 模型转换为 keras</title>
      <link>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</link>
      <description><![CDATA[我尝试将 ONNX 模型转换为 Keras，但当我调用转换函数时，我收到以下错误消息 “TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;”
ONNX 模型输入：input_1
您可以在此处查看 ONNX 模型：https://ibb.co/sKnbxWY
import onnx2keras
from onnx2keras import onnx_to_keras
import keras
import onnx

onnx_model = onnx.load(&#39;onnxModel.onnx&#39;)
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])

keras.models.save_model(k_model,&#39;kerasModel.h5&#39;,overwrite=True,include_optimizer=True)


 文件“C:/../onnx2Keras.py”，第 7 行，位于 &lt;module&gt;
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])
文件“..\site-packages\onnx2keras\converter.py”，第 80 行，位于 onnx_to_keras
weights[onnx_extracted_weights_name] = numpy_helper.to_array(onnx_w)
TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</guid>
      <pubDate>Tue, 15 Oct 2019 13:15:06 GMT</pubDate>
    </item>
    </channel>
</rss>