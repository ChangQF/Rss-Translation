<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 27 Feb 2025 06:27:04 GMT</lastBuildDate>
    <item>
      <title>BigQuery ML时间序列模型评估保持返回零</title>
      <link>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</link>
      <description><![CDATA[我正在使用BigQuery ML来训练ARIMA_PLUS模型，以预测CPU使用情况。该模型成功训练，但是当我运行ml。评估时，所有结果值均为null。
 模型训练查询 
 创建或替换模型`project.dataset.arima_model`
选项（
  model_type =&#39;arima_plus&#39;，
  time_series_timestamp_col =&#39;timestamp_column&#39;，
  time_series_id_col = [&#39;id_column_1&#39;，&#39;id_column_2&#39;]，
  time_series_data_col =&#39;data_column&#39;，
  forecast_limit_lower_bound = 0，
  forecast_limit_upper_bound = 100
） 作为
选择data_column，id_column_1，id_column_2，timestamp_column
来自`project.dataset.source_table`
在“ 2025-02-5”和&#39;2025-02-12&#39;之间的日期（timestamp_column）;
 
 评估查询 
 选择 * 
来自ml.evaluate（
  模型`project.dataset.arima_model`，
  （（
    选择data_column，id_column_1，id_column_2，timestamp_column
    来自`project.dataset.source_table`
    在“ 2025-02-13&#39;和&#39;2025-02-20&#39;之间的日期（timestamp_column）
  ），
  结构（
    true作为persim_gregation， 
    10作为地平线， 
    0.9作为信心_level
  ）
）；
 
 ml.Evaluate成功运行，但返回所有ID的null
查询结果 ]]></description>
      <guid>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</guid>
      <pubDate>Wed, 26 Feb 2025 23:29:23 GMT</pubDate>
    </item>
    <item>
      <title>NN回归训练损失初始增加[关闭]</title>
      <link>https://stackoverflow.com/questions/79471142/nn-regression-training-loss-initial-increase</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79471142/nn-regression-training-loss-initial-increase</guid>
      <pubDate>Wed, 26 Feb 2025 22:16:00 GMT</pubDate>
    </item>
    <item>
      <title>如何为AI模型培训构建服务器？ （GPU，硬件和远程访问）[关闭]</title>
      <link>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</link>
      <description><![CDATA[我想构建用于培训AI模型的计算机（服务器），包括：

微调聊天机器人LLMS 
机器学习和深度学习模型

我有几个问题：

 我应该购买哪个GPU？我知道GPU对于AI培训很重要，但我不确定哪一个是满足我需求的最佳选择。

 我还需要什么其他硬件？除GPU外，推荐的CPU，RAM，存储和其他组件是什么？

 如何远程控制此服务器？我希望能够从另一个位置访问和管理此服务器。我应该使用什么工具或方法？

 多人可以同时使用该服务器吗？如果我想与朋友或队友共享此服务器，我们如何一起训练模型？

]]></description>
      <guid>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</guid>
      <pubDate>Wed, 26 Feb 2025 18:16:44 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络层中使用二进制（{0,1}）权重</title>
      <link>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</guid>
      <pubDate>Wed, 26 Feb 2025 13:02:20 GMT</pubDate>
    </item>
    <item>
      <title>如何准备使用LSTM进行分类的不规则间隔时间序列数据？</title>
      <link>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</link>
      <description><![CDATA[ i具有这样的可变的可变价值的数据，如下所示： processed_data 是一个大小为215×1的单元格数组，保存单元格，每个单元都包含给定一天的数据。每个单元（天）的观测值数量不同（平均约为12,000行）。每行代表一个观察值，其中：第一列包含自上一行以来经过的秒数（未归一化），第二列包含指定安全性的价格（使用z得分进行了归一化），第三列是目标变量，信号传递该时刻的价格是否会高0.01％（表示为1）60秒（表示为1）60秒或不表示为0（表示为0）。我将前两列用作预测指标。我将日子分开，因为小时在 processed_data {i，1} 的最后一行之间以及day processed_data {i+1，1} 的第一行。以下是任意日的数据示例：
  2.57500000000437 0.502515050312692 0
1.036000000006 0.469361050915526 1
1.05899999999383 0.386501335237771 1
0.838000000003376 0.436219680495852 0
1.1299999999738 0.469361050915526 0
0.824000000000524 0.369924327252462 1
 
我只是ML的初学者，而且我很难想象应该如何格式化LSTM层的数据。如果我正确，它需要3维数据，其中一个维度代表 channel ，另一个维度为时间步长，而另一个 batch 。我现在确定我已经完全误解了这些概念，并写了以下代码：
  %%分区数据。
train_data_length = round（长度（processed_data） * 0.9）;
train_data = processed_data（1：train_data_length）;
test_data = processed_data（train_data_length+1：end）;

%%培训设置
％将数据转换为dlarray的单元格数组。
train_x =单元格（size（train_data））;
train_y =单元格（size（train_data））;

对于一天= 1：长度（train_data）
    ％添加批处理尺寸（C×B×T，其中B = 1）。
    data = permute（train_data {day}（：，1：2）&#39;，[1 3 2]）; ％[2×1×T]
    train_x {day} = dlarray（data，＆quot; cbt; quot;）;
    
    ％将标签转换为单速编码的CBT格式[2×1×T]。
    labels = train_data {day}（：，3）&#39;; ％[1×T]
    One_hot_labels = OneHotEncode（标签，1，&#39;classNames&#39;，[0 1]）; ％[2×t]
    One_hot_labels = reshape（One_hot_labels，2，1，[]）; ％[2×1×T]
    train_y {day} = dlarray（single（One_hot_labels），“ CBT”）;
结尾

ds = combine（...
    arraydatastore（train_x，&#39;outputType&#39;，&#39;same&#39;），...
    arraydatastore（train_y，&#39;outputType&#39;，&#39;same&#39;）...
）；

％clearvars -Efcect ds test_data ml_method

num_features = 2;
num_hidden_​​units = 128;
num_classes = 2;
mini_batch_size = 32;

层= [
    sequenceInputlayer（num_features，“名称”，“输入”）
    lstmlayer（num_hidden_​​units，&#39;outputmode&#39;，&#39;sequence&#39;）
    完整连接的layerer（num_classes）
    SoftMaxlayer
];

net = dlnetwork（层）;

选项=训练（&#39;Adam&#39;，...
    “ Maxepochs”，30，...
    &#39;minibatchsize&#39;，mini_batch_size，...
    “序列长度”，“最长”，...
    “洗牌”，“每个段”，...
    “情节”，“训练过程”，...
    “ inputdataformats&#39;，&#39;cbt&#39;，...
    “冗长”，错误，...
    “执行环境”，“ gpu&#39;）;

net = trainnet（ds，net，&#39;crossentropy&#39;，选项）;
 
在上面的代码中，我试图将通道定义为预测变量的数量（在我的情况下为2，可能是我正确定义的唯一维度）。我将批次设置为1，因为我认为这意味着网络将使用一个观察结果来做出预测。我将时间步骤设置为一天的数据价值的第一列（自上次观察以来的秒数），因为我认为这实际上意味着及时的步骤。现在我知道我完全错了。我还必须将Mini_batch_size从128中将其更改为32，但我发现这太低了，但是否则，我会用尽内存。我想这是因为我的格式格式不正确（我不确定这是否是一个重要的细节，但是我将包括我的GPU，它是带有8GB内存的RTX2070 SUPER）。我的问题是：我应该如何根据目标格式化LSTM层的数据？否则我的目标是不现实的，我正在使用错误？
我想象这个网络能够对数据中的每个观察结果进行预测。]]></description>
      <guid>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</guid>
      <pubDate>Wed, 26 Feb 2025 12:49:01 GMT</pubDate>
    </item>
    <item>
      <title>无法优化我的CNN模型来预测Watch Brands [关闭]</title>
      <link>https://stackoverflow.com/questions/79469709/cant-optimize-my-cnn-model-to-predict-watch-brands</link>
      <description><![CDATA[我想创建一个手表品牌标识符，该标识符获得图像并在其4个品牌之一或其他品牌中返回。
 rn我拥有的是4个品牌中每一个的3126张图像
和8000张来自其他品牌的手表图像（Chrono中每个品牌的均匀图像）。
 im使用VGG19作为基本模型，并在其中添加了一些图层。
问题在于，我训练的模型具有78％的准确性，更重要的是，从其他品牌中的一个品牌之一的手表中，很多时候都可以预测。。
我真正关心的是手表是否来自四个品牌，而不是哪个品牌，我该怎么做才能改善？
这是代码 link   
我认为可能仅仅是仅仅是4个中的二进制文件，但从最初的培训中，我看到了更糟糕的结果，也许我没有正确地做到这一点...
另外，我将优化器更改为ADAMW并增加了班级权重，我将重新训练该模型以查看是否有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79469709/cant-optimize-my-cnn-model-to-predict-watch-brands</guid>
      <pubDate>Wed, 26 Feb 2025 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>可以使用Python比较多个ROC曲线与Delong的测试进行比较吗？</title>
      <link>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</link>
      <description><![CDATA[我有3条ROC曲线，该曲线使用了3种不同测试的数据计算出来，该测试旨在将患者归类为患病或健康。我已经计算了所有3个测试的AUC，我想比较此曲线，以查看使用DeLong测试之间是否存在统计差异。我发现了非常简单的实现，例如：
 来自mlstatkit.stats导入delong_test

z_score，p_value = delong_test（labels，scores_model1，scores_model2）

打印（f＆quot; Model 1 AUC：{roc_auc_score（标签，scores_model1）：。4f}＆quort;）
打印（F＆quot;模型2 AUC：{roc_auc_score（标签，scores_model2）：。4f}＆quort;）
print（f＆quot; z得分：{z_score：.4f}，p-value：{p_value：.4f}＆quot;）
 
但是，我发现的所有实现都是用于比较两条ROC曲线。有人知道该测试是否可以进行两条以上的曲线？我的想法正在进行3种不同的配对测试并比较P值，但我不知道。
有什么方法可以用3个ROC曲线执行Delong的测试，有人可以帮助我进行编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</guid>
      <pubDate>Wed, 26 Feb 2025 12:07:00 GMT</pubDate>
    </item>
    <item>
      <title>改进职位描述，使用AI [封闭]</title>
      <link>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</link>
      <description><![CDATA[我正在研究AI驱动的人才获取解决方案，在该解决方案中，我们将恢复并将候选成就和职责转换为结构化属性。将主数据管理（MDM）用于作业角色和类别，我们生成这些属性。目标是将职位描述（JD）属性与候选属性匹配，以过滤给定作业的最相关的候选人。
这是我到目前为止尝试的：

  语义匹配：我使用语义相似性技术将职位描述属性与候选属性进行比较。

  向量嵌入：我尝试了嵌入技术（例如Word2Vec，句子变形金刚）以捕获属性的上下文含义。


但是，结果并不令人满意。传统方法（例如确切的关键字匹配或基于规则的方法）有局限性：

 他们无法捕获属性之间的语义相似性（例如Google AdWords和绩效营销）。

 他们不考虑同义词或相关技术（例如AWS和Amazon Web服务，或CI/CD Pipelines以及连续集成＆amp; exployment）。

 他们缺乏上下文感知的匹配，其中某些属性与特定的工作角色更相关。

 他们很难确定相关概念之间的语义相似性（例如，积极支持和客户健康监测）。

 他们没有有效地绘制同义词或相关术语（例如，预防和保留策略，或客户提高和扩张收入）。


我正在寻找改善匹配过程的建议或替代方法。具体：

 是否有更好的技术或模型来捕获语义相似性和上下文感知匹配？

 我如何有效地处理该域中的同义词和相关概念？

 是否有任何可以帮助解决此问题的预培训模型或数据集？

]]></description>
      <guid>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</guid>
      <pubDate>Wed, 26 Feb 2025 07:43:45 GMT</pubDate>
    </item>
    <item>
      <title>yolov9e-seg无法为整个图像进行细分</title>
      <link>https://stackoverflow.com/questions/79468647/yolov9e-seg-not-able-to-do-segmentation-for-the-entire-image</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79468647/yolov9e-seg-not-able-to-do-segmentation-for-the-entire-image</guid>
      <pubDate>Wed, 26 Feb 2025 05:46:59 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地运行DeepSeek模型</title>
      <link>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</link>
      <description><![CDATA[我试图根据他们的说明在本地运行DeepSeek，但它不能带来一些愚蠢的错误（我将稍后显示）。
这就是我正在做的。

从此处下载最小型号（3.5GB） noreferrer“&gt; https://huggingface.co/deepseek-ai/deepseek-r1-distill-qwen-1.5b  
按照此处的步骤操作： https://github.com/deepseek-ai/deepseek-v3?tab=readMe-Readme-ov-file#6-how-to-to-to-run-locally  

 2.1获取这个项目
 https://github.com/deepseek-ai/deepseek-ai/deepseek-ai/deepseek-v3.git 
 2.2运行码头容器类似于预先创建的卷以放置模型
  docker run  -  gpus all -it -it -name deepSeek01 -rm -mount source = deepSeekv3，target =/root/deepSeekv3 python：3.10 -Slim bash
 
我正在使用python：3.10-slim，因为这里（ https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-how-to-run-locally ）
＆quot&#39; linux只有python 3.10。 Mac和Windows不支持。
 2.3安装最新更新
apt-get Update 
 2.4获取此文件 https://github.com/deepseek-ai/deepseek-v3/blob/main/main/inference/requirements.txt 并安装要求
  pip install -r sumpliont.txt
 
 2.5将模型复制到安装在Docker容器上的音量。这5个文件来自此处 https：//hugging.co/deepseek-aiek-ai/deepseek-ai/deepseek-ai/deepseek-ai/deepseek/deepseek-ipseek-r1-r1-r1-r1-pp&gt;   config.json
generation_config.json
模型。系统
tokenizer.json
tokenizer_config.json
 
 2.6在此处编写的模型转换 https://github.com/deepseek-ai/deepseek-v3?tab=readme-readme-ov-file#model-weights-conversion 通过此命令
  python convert.py-hf-ckpt-path/root/deepSeekv3/source_model -save-path/root/deepSeekv3/converted_model -n-experts 256-model-parelally 16
 
在此步骤中（转换模型）我得到了此错误
  trackback（最近的最新通话）：
  file＆quort＆quort＆quot deepseekv3/inference/convert.py&quot;，第96行，in＆lt; module＆gt;
    main（args.hf_ckpt_path，args.save_path，args.n_experts，args.model_parallel）
  file＆quot＆quot&#39;deepseekv3/inference/convert.py&quot;，第63行，在main中
    主张映射中的密钥
断言
 
因此，基本上，下一步没有意义，因为这是必不可少的步骤。
我的问题：

我做错了什么？
 YouTube上有一些视频，其中DeepSeek与Ollama一起安装了。真的需要吗？我是否应该像他们在这里描述的那样能够运行它， https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-to-run-locally ？

更新1 
为了调试一点，我添加了这2行。
  print（＆quot;丢失键：＆quot;键）
打印（可用键：＆quot; list（mapping.keys（）））
 
缺少键是以下内容：
  embed_tokens
input_layernorm
down_proj
gate_proj
UP_PROJ
post_attention_layernorm
k_proj
 
虽然所有这些都确实存在于模型中。
另外，@hans Kilian在评论中提到，我可能会放一些文件，而这些文件不需要到source_model文件夹中。
我在convert.py中检查了第11行，其中一些键在模型中不存在。]]></description>
      <guid>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</guid>
      <pubDate>Tue, 25 Feb 2025 22:14:20 GMT</pubDate>
    </item>
    <item>
      <title>LSTM培训是否在恢复学习后重置？</title>
      <link>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</guid>
      <pubDate>Sun, 23 Feb 2025 21:11:38 GMT</pubDate>
    </item>
    <item>
      <title>使用射线调节器进行超参数调整的序列化误差</title>
      <link>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</guid>
      <pubDate>Fri, 21 Feb 2025 15:18:38 GMT</pubDate>
    </item>
    <item>
      <title>在编译时，我该如何解决问题？（depthwise_conv.cc）</title>
      <link>https://stackoverflow.com/questions/78125296/how-can-i-solve-the-problem-during-mbed-compiledepthwise-conv-cc</link>
      <description><![CDATA[我正在研究使用Tinyml Book的机器学习。
我正在尝试编译，但它不起作用。
问题的情况如下：
本书提出了以下过程。
  make -f tensorflow/lite/micro/tools/make/makefile \
target = mbed tags =; cmsis-nn disco_f746ng; generate_micro_speech_mbed_project
 
目录。
  CD TensorFlow/Lite/Micro/Tools/Make/gen/Mbed_cortex-M4/prj/micro_speech/mbed
 
配置MBED盗贼根。
  mbed config root。
 
 mbed部署
  MBED部署
 
修改MBED配置文件以使用C ++11。
  python3 -c&#39;导入fileInput，glob;
for glob.glob中的文件名（“ mbed-os/tools/profiles/*。json＆quot”）：
    对于fileInput.input中的行（文件名，Inplace = true）：
        print(line.replace(&quot;\&quot;-std=gnu++98\&quot;&quot;,&quot;\&quot;-std=c++11\&quot;, \&quot;-fpermissive\&quot;&quot;))&#39;

 
和编译
  mbed compile -m disco_f746ng -t gcc_arm
 
但是，部署过程中存在一些问题。在部署过程中，发生了问题。在寻找解决方案时，我找到了一个建议，以修改make命令如下。
  make -f tensorflow/lite/micro/tools/make/makefile \
target = mbed tags =; cmsis-nn disco_f746ng; generate_micro_speech_mbed_project
 
进行修改后，我以相同的方式进行了编译过程，但遇到了以下错误。
 编译[82.7％]：depthwise_conv.cc
[错误] depthwise_conv.cc@178,9：从&#39;int&#39;到&#39;const cmsis_nn_dims*&#39;[-fpermissive]
[错误] depthwise_conv.cc@178,22：从&#39;int&#39;到&#39;const cmsis_nn_dims*&#39;[-fpermissive]
[error] depthwise_conv.cc@178,49：太多的参数无法函数&#39;int32_t arm_depthwise_conv_s8_opt_get_get_buffer_size（const cmsis_nn_dims*，const cmsis_nnn_dims*）&#39;
[错误] depthwise_conv.cc@184,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[错误] depthwise_conv.cc@195,9：在此范围中未声明&#39;arm_math_success&#39;；您的意思是&#39;ARM_MATH_DSP&#39;吗？
[错误] depthwise_conv.cc@184,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@200,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@212,9：在此范围中未声明&#39;arm_math_success&#39;；您的意思是&#39;ARM_MATH_DSP&#39;吗？
[error] depthwise_conv.cc@200,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@272,5：&#39;arm_depthwise_conv_u8_basic_ver1&#39;在此范围中未声明；您的意思是&#39;ARM_DEPTHWIES_CONV_FAST_S16&#39;吗？
[错误]&#39;_queue.simplequeue&#39;对象没有属性&#39;队列&#39;
[mbed]错误：/usr/bin/python3＆quot返回的错误。
       代码：1
       路径：＆quot/home/ghjeon/tensorflow-lite/tensorflow/lite/micro/tools/make/gen/gen/mbed_cortex-m4/prj/prj/micro_speech/mbed;
       Command: &quot;/usr/bin/python3 -u /home/ghjeon/tensorflow-lite/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . -build ./build/disco_f746ng/gcc_arm&quot;
       提示：您可以用“ -v”重试最后一个命令。详细的详细输出
---

 
我无法解决这个问题。我一直无法解决这个问题2天。我要提前感谢任何可以提供帮助的人。
  [错误]&#39;_Queue.simplequeue&#39;对象没有属性&#39;queue&#39;
 
我已经看到了信息，表明可以使用Python 2.7解决上述错误。但是，我不确定这是否允许使用CLI1。因为ARM建议Cli1需要Python 3.7.x版本。]]></description>
      <guid>https://stackoverflow.com/questions/78125296/how-can-i-solve-the-problem-during-mbed-compiledepthwise-conv-cc</guid>
      <pubDate>Fri, 08 Mar 2024 02:44:08 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机过度适合我的数据[关闭]</title>
      <link>https://stackoverflow.com/questions/44939725/support-vector-machine-overfitting-my-data</link>
      <description><![CDATA[我正在尝试对虹膜数据集做出预测。我决定将SVM用于此目的。但是，它给了我准确的1.0。是过度拟合的情况还是因为模型很好？这是我的代码。
  x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.2，andury_state = 0）
svm_model = svm.svc（kernel =&#39;linear&#39;，c = 1，gamma =&#39;auto&#39;）
svm_model.fit（x_train，y_train）
预测= svm_model.predict（x_test）
准确_score（预测，y_test）
 
在这里，准确_score返回1。请帮助我。我是机器学习的初学者。]]></description>
      <guid>https://stackoverflow.com/questions/44939725/support-vector-machine-overfitting-my-data</guid>
      <pubDate>Thu, 06 Jul 2017 04:17:53 GMT</pubDate>
    </item>
    <item>
      <title>SVM在Scikit学习</title>
      <link>https://stackoverflow.com/questions/28154839/svm-overfitting-in-scikit-learn</link>
      <description><![CDATA[我正在使用SVM构建数字识别分类。我有10000个数据，然后将它们分为训练和测试数据，比率7：3。我使用线性内核。
结果证明，当更改训练示例编号时，训练精度总是1，但是测试精度仅为0.9（我预计准确性至少为0.95）。我认为结果表明过度拟合。但是，我从事参数，例如C，伽玛，...它们不会太多更改结果。
如何处理SVM中的过度拟合？
以下是我的代码：
 来自Sklearn Import SVM，Cross_validation
svc = svm.svc（kernel =&#39;linear&#39;，c = 10000，伽马= 0.0，冗长= true）.fit（sample_x，sample_y_1num）

Clf = SVC

preditive_y_train = clf.predict（sample_x）
preditive_y_test = clf.predict（test_x）    
准确性= clf.score（sample_x，sample_y_1num） 
efceracy_test = clf.score（test_x，test_y_1num）  
    
#CODDUCT交叉验证 

cv = cross_validation.shufflesplit（sample_y_1num.size，n_iter = 10，test_size = 0.2，andural_state = none）
scores = cross_validation.cross_val_score（clf，sample_x，sample_y_1num，cv = cv）
score_mean =平均值（得分） 
 ]]></description>
      <guid>https://stackoverflow.com/questions/28154839/svm-overfitting-in-scikit-learn</guid>
      <pubDate>Mon, 26 Jan 2015 16:54:04 GMT</pubDate>
    </item>
    </channel>
</rss>