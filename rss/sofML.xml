<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 04 Dec 2023 12:26:00 GMT</lastBuildDate>
    <item>
      <title>如何在colab中查找数据集的某一列中有多少个不同的数据</title>
      <link>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</link>
      <description><![CDATA[我有一个大约由 400000 行和 8 列组成的数据集，我只想知道一列中有多少种不同类型的数据，我该怎么做？列中的数据是字符串的形式，我需要给它们分配数字，所以我需要找出该列中有多少个不同的单词。我不知道我应该做什么]]></description>
      <guid>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</guid>
      <pubDate>Mon, 04 Dec 2023 12:22:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM 微调和推理所需的廉价云计算平台 [关闭]</title>
      <link>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</link>
      <description><![CDATA[我是一名刚毕业的人工智能毕业生，现在在一家非常小的初创公司工作，探索（并尝试实施）人工智能可以在公司软件中使用的地方。公司里没有其他人做人工智能，这就是为什么我想在这里问一个问题（也是因为我在谷歌上找不到具体的答案）。
基本上，我正在尝试使用 HuggingFace 来尝试一些法学硕士，以便我可以找到适合我的想法的法学硕士。问题是我的笔记本电脑不够强大，无法在 LLM 上运行推理，因为我只有 GTX 1650。我尝试使用 Google Colab，但只成功运行了一个小型 3B 参数模型，该模型表现不佳。
我的问题是：在哪里可以找到最便宜的云计算平台，该平台仍然强大到足以运行推理并可能对中小型法学硕士进行微调？如果有帮助的话，我目前正在尝试找到一个可以进行自定义命名实体识别的模型，因此该模型可能不需要太大，我也不需要进行训练。
问题是，由于我工作的公司是一家小型初创公司，他们无法为一个人提供像 AWS 或 Azure 这样的东西（我尝试研究了这方面的成本，我认为每月大约 2500 美元） .
我非常感谢您对此的帮助！感谢您的宝贵时间:)]]></description>
      <guid>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</guid>
      <pubDate>Mon, 04 Dec 2023 11:17:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测思路</title>
      <link>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</link>
      <description><![CDATA[虚拟数据
我必须预测此数据未来 3 个月的故事点，我该如何开始？
我必须预测未来 3 个月的故事点，我可以使用哪种 ML 算法
如何分析数据以及数据的趋势]]></description>
      <guid>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</guid>
      <pubDate>Mon, 04 Dec 2023 11:07:11 GMT</pubDate>
    </item>
    <item>
      <title>高分辨率数据集和 Nvidia MX 150 GPU 的 YOLOv8 自定义模型性能问题</title>
      <link>https://stackoverflow.com/questions/77598734/yolov8-custom-model-performance-issue-with-high-resolution-dataset-and-nvidia-mx</link>
      <description><![CDATA[我最近开发了一个定制的 YOLOv8 模型来检测家庭环境中的眼镜。为此，我创建了一个包含 1000 张图像的数据集，这些图像是使用 Galaxy S22 Ultra 相机以 3000x4000 的分辨率拍摄的。该数据集中的每个图像都已进行相应注释。我在此数据集上训练了 YOLOv8 模型 100 个 epoch。
但是，当使用相同分辨率的视频测试模型时，我注意到检测性能存在明显的滞后。我正在尝试查明此问题的原因，并考虑两种可能性：
&lt;强&gt;1。训练数据集的高分辨率：
训练图像的高分辨率 (3000x4000) 是否会导致检测滞后？如果是这样，在训练之前将图像大小调整为较低的分辨率是否会提高性能而不显着影响检测精度？
&lt;强&gt;2。 GPU能力：
我使用 Nvidia MX 150 GPU 进行训练和推理。在这种情况下，GPU 的功能是否会成为限制因素？如果是，有效训练和运行 YOLOv8 模型的推荐 GPU 规格是什么？
此外，如果您能提供有关在自定义数据集上训练 YOLOv8 模型的任何见解或最佳实践，尤其是在数据集准备和硬件要求方面，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77598734/yolov8-custom-model-performance-issue-with-high-resolution-dataset-and-nvidia-mx</guid>
      <pubDate>Mon, 04 Dec 2023 10:33:46 GMT</pubDate>
    </item>
    <item>
      <title>实施文本分类注意力机制的问题</title>
      <link>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</link>
      <description><![CDATA[我第一次尝试注意力机制
我无法理解 keras 中 Attention 和 MultiHeadAttention 的使用。 （实现上混乱，概念上很清楚）
我的一些疑问。

想知道如何准确使用它们吗？

我发现像这样的线条
model.add（MultiHeadAttention（num_heads = 8，key_dim = 16，attention_axes =（1, 1）））
或者
model.add(Attention(use_scale=True))

以类似的方式，我从头开始找到代码，但它们都不能直接工作，所以可以使用哪些代码？

我在 ML/DL 中创建文本分类模型的一些工作背景
我当前的模型有
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(嵌入(max_features, 128))
model.add(双向(LSTM(64)))
model.add（密集（1，激活=&#39;sigmoid&#39;））

现在我想添加注意力机制来改进它
任何参考资料或帮助都可能非常有帮助]]></description>
      <guid>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</guid>
      <pubDate>Mon, 04 Dec 2023 08:47:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 生成头像图像的良好路线图/工作流程是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像并输出专业头像]]></description>
      <guid>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</guid>
      <pubDate>Mon, 04 Dec 2023 05:47:06 GMT</pubDate>
    </item>
    <item>
      <title>在本地设备中使用 MMOCR 进行文本识别推理期间出现“FileNotFoundError”</title>
      <link>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</link>
      <description><![CDATA[SAR 文本识别模型用于自定义训练车牌数据集以识别尼泊尔字符。我使用 Google Drive 训练模型来识别文本。现在我想在本地设备上使用权重（以 .pth 扩展名结尾的文件）进行推理。 text_rec_model包含.pth文件的路径
infer = TextRecInferencer(weights=text_rec_model)

我在 PyCharm 终端中抛出此错误。
FileNotFoundError: [Errno 2] 没有这样的文件或目录: &#39;/content/drive/MyDrive/mmocr_tut/mmocr/configs/textrecog/sar/../../../dicts/english_digits_symbols.txt &#39;

配置文件是否保存在.pth 文件中？如果是这样，那么我如何编辑以便可以使用最新的检查点（在本例中为 epoch_85.pth 文件）运行推理]]></description>
      <guid>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</guid>
      <pubDate>Mon, 04 Dec 2023 04:35:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 上强制在 GPU 上训练模型？</title>
      <link>https://stackoverflow.com/questions/77597016/how-to-force-train-a-model-on-gpu-on-google-colab</link>
      <description><![CDATA[我正在尝试使用 Tensorflow 在 Google Colab 上训练 GAN 模型。然而，每个时期需要非常非常长的时间。我正在使用 Google Colab Pro，尽管我选择了 T4 GPU 作为运行时类型选择了 T4 GPU，我注意到它根本不使用 GPU。Google Colab 不使用 GPU 
我想知道如何通过强制使用 GPU 来改善运行时间。
我尝试使用
 与 tf.device(&#39;/device:GPU:0&#39;):

环绕我的火车功能，但它仍然没有任何区别。 Google 似乎只提供了有关如何在 Colab 中将运行时更改为 GPU 的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77597016/how-to-force-train-a-model-on-gpu-on-google-colab</guid>
      <pubDate>Mon, 04 Dec 2023 03:06:05 GMT</pubDate>
    </item>
    <item>
      <title>minMax TicTacToe 代码返回错误的最佳可能移动</title>
      <link>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</guid>
      <pubDate>Mon, 04 Dec 2023 02:07:13 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是一个特殊的全包“引理” spaCy 用于任何代词，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
[功能] https://i.stack.imgur.com/vwEYe.png&lt; /a&gt;
[测试] https://i.stack.imgur.com/37mTh.png&lt; /a&gt;
这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>MLPClassifier 适合二元分类吗？</title>
      <link>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</link>
      <description><![CDATA[我编写了一个使用 MLPClassifier 来解决二元分类问题的程序。它有点有效，但我不相信这是正确的模型。
我有 1300 个整数的十六进制数要放入两个类之一：类 0 和类 1。一个潜在的问题是，在我的训练数据中，98% 属于类 0，因此我将从 &quot; 获得 98% 的准确率“预测函数”总是返回“class 0”与输入无关。
是否有专为此类问题设计的机器学习模型？
================================================== ===============
TLDR？
我的数据如下：
X = 数组([[ 0, 11, 51, 13, 0, 9],
       [51,13,0,9,0,11],
       [ 0, 8, 0, 10, 0, 13],
       ...,
       [ 0, 11, 61, 12, 0, 8],
       [ 0, 8, 0, 0, 60, 11],
       [30, 11, 0, 6, 0, 9]])

目标是 y，一个包含 1300 个 0 和 1 的列表。我使用 MLPClassifier 并获得了 98% 的预测准确率。这时我突然想到，98% 的元组恰好属于 0 类，因此，如果我不费心进行任何机器学习，而是猜测类始终为 0，那么我将获得 98% 的准确率。
我检查了拟合度，看看它在 1 类元组上的表现如何，发现其中 82% 的预测正确，因此准确度为 98% 的 82%，即大约 80%，我想改进，但是怎么办？
除了盲目增加层的大小/数量之外，我不知道如何更改 MLPClassifier 的参数，但我突然想到，我很可能使用完全错误的模型来解决带有 Yes/ 的学习问题没有分类。另外，六元组中的整数不是任意的，我想到这也可能与模型的选择有关。特别是，六个输入中的三个始终在 0 - 15 范围内，另外三个是两位数代码，第一位数字有三种可能，第二位数字有两种可能。
感谢您的任何想法。
代码：
m = MLPClassifier(hidden_​​layer_sizes = (256, 128, 64), max_iter=10000) # 从我在网上找到的示例粘贴:(
_ = m.fit(X, y)
yhat = m.predict(X)
cm = 混淆矩阵(y, yhat)
print( &#39;准确率 = &#39;, np.mean( y == yhat ) )
打印（厘米）

输出：
准确度 = 0.9816653934300993
[[1267 13]
 [11 18]]

(pdb) class1 = [ i for i in range(len(y)) if y[i] == 1]
(pdb) z = m.predict(np.row_stack((X[q] for q in class1)))
(pdb) z
数组([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 0, 1])
(pdb) len(z), 总和(z)
29, 24
]]></description>
      <guid>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</guid>
      <pubDate>Sun, 03 Dec 2023 23:53:22 GMT</pubDate>
    </item>
    <item>
      <title>如何预测回归模型的健康百分比值</title>
      <link>https://stackoverflow.com/questions/77596584/how-to-predict-health-percentage-values-for-regression-model</link>
      <description><![CDATA[我目前正在研究预测维护数据集，其中包含来自传感器的数据、任何错误的发生以及每台机器的一些特征。在进行特征工程之后，我创建了一个列，其中包含引擎剩余使用寿命的％百分比。
y_column 是一系列从 0 到 1 均匀递增的变量，当达到 1（表示发生错误）时返回到 0。我正在使用回归模型，我的结果如下图所示。由于我是机器学习新手，解决此类问题的方法是什么？我比较习惯分类。有哪些方法可以改善此类结果？
我添加了滚动平均值/标准差/最小值/最大值，我添加了滞后特征，我复制了该数据集分类方法中使用的一些技术，但即使我得到 20% 的 RMSE 和 MAE，它也没有捕获问题的形象正确。到目前为止，我正在使用 GradientBoostingRegressor KNeighborsRegressor RandomForestRegressor。改善结果的一般技巧有哪些？顺便说一句，模型无需任何超参数调整即可拟合。对这种类型的结果有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77596584/how-to-predict-health-percentage-values-for-regression-model</guid>
      <pubDate>Sun, 03 Dec 2023 23:49:03 GMT</pubDate>
    </item>
    <item>
      <title>在TF中实现FedAvg算法的问题</title>
      <link>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</link>
      <description><![CDATA[我在 google colab 中使用 TF 实现 FL（不使用 TFF），我使用 Bot-IoT 数据集，并将数据划分为 10 个客户端，以便每个客户端学习所有类型的课程，以便可以在所有课程上进行训练。 
我面临的问题是聚合从每个客户端接收到的模型权重，以获得全局准确率和召回率，在我的代码中为 0%。
这是聚合函数：
&lt;块引用&gt;
def federated_averaging(client_weights):
新权重 = []
# 模型层数

# 权重求和
forweights_list_tuple in zip(*client_weights): # 迭代每一层
    layer_mean = np.mean(np.array([np.array(weights) 用于weights_list_tuple中的权重]), axis=0)
    
    new_weights.append(layer_mean)

返回新的权重

我的代码是收集每轮的模型权重，然后对权重进行平均，得到用于计算全局准确率的平均权重。]]></description>
      <guid>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</guid>
      <pubDate>Sun, 03 Dec 2023 23:31:31 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类，XGBClassifier 与 xgb.train 的 AUC 分数不同，即使在舍入预测概率后也是如此</title>
      <link>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</link>
      <description><![CDATA[看起来 XGBClassifier 是 xgb.train 的包装器。我试图使用 xgb.train 和 &quot;objective&quot;: &quot;binary:logistic&quot; 训练二元分类器。
似乎使用 xgb.train 后跟 .predict() 返回预测概率，我们可以将其舍入为 0 或 1 以进行分类（基于我在另一个叠加问题中看到的答案，xgb.train 结果概率，我们需要对其进行四舍五入以获得实际的分类预测）
因此，如果我们有二元分类，我们需要四舍五入 &lt;0.5 ==&gt; 0级且&gt;0.5==&gt; 1 级？
但是如果它是多类怎么办？那么我们假设 为均匀分布并均匀地分割它？例如3级==&gt; 0~1/3、1/3~2/3、2/3~1？
使用xgb.train作为分类器的正确方法是什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb

data = np.random.rand(50,10) # 50个实体，每个实体包含10个特征
label = np.random.randint(2, size=50) # 二进制目标

dtrain = xgb.DMatrix(数据, 标签=标签)
param = {&#39;max_depth&#39;:3, &#39;eta&#39;:0.1, &#39;silent&#39;:1, &#39;tree_method&#39;:&#39;hist&#39;,&#39;objective&#39;:&#39;binary:logistic&#39;, &#39;seed&#39;:42}
num_round = 100 # 与估计器的数量相同
bst = xgb.train( 参数, dtrain, num_round)
trainres = bst.predict(dtrain)

模型= xgb.XGBClassifier（n_estimators = 100，objective =&#39;binary：logistic&#39;，tree_method =&#39;hist&#39;，eta = 0.1，max_深度= 3，enable_categorical = True，seed = 42）
模型 = model.fit(数据,标签)
fitres = model.predict(数据)

# 比较分类
print(all([trainres 中的 x 的 round(x)] == fitres))


# 比较概率。预测概率给出 0 类和 1 类的概率，因此取 x[1]
print(all([x[1] for x in model.predict_proba(data)] == trainres))

输出：
&lt;前&gt;&lt;代码&gt;正确

真的
]]></description>
      <guid>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</guid>
      <pubDate>Fri, 01 Dec 2023 08:32:40 GMT</pubDate>
    </item>
    <item>
      <title>如何确定sklearn中MLPClassifier的“损失函数”？</title>
      <link>https://stackoverflow.com/questions/53369866/how-can-i-determine-loss-function-for-mlpclassifier-in-skilearn</link>
      <description><![CDATA[我想使用sklearn的MLPClassifier
mlp = MLPClassifier(hidden_​​layer_sizes=(50,), max_iter=10, alpha=1e-4,
                求解器=&#39;sgd&#39;，详细=10，tol=1e-4，random_state=1，
                Learning_rate_init=.1)

我没有找到损失函数的任何参数，我希望它是mean_squared_error。是否可以根据模型确定它？]]></description>
      <guid>https://stackoverflow.com/questions/53369866/how-can-i-determine-loss-function-for-mlpclassifier-in-skilearn</guid>
      <pubDate>Mon, 19 Nov 2018 07:12:03 GMT</pubDate>
    </item>
    </channel>
</rss>