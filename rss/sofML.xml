<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 13 May 2024 03:15:50 GMT</lastBuildDate>
    <item>
      <title>我想在输入问题时预测标签</title>
      <link>https://stackoverflow.com/questions/78469857/i-want-to-predict-tags-when-input-the-questions</link>
      <description><![CDATA[`问题=“facetgrid数据标签seaborn”
标签 = “python pandas seaborn”
我在小数据集中使用 MLPClassifier，但实际数据形状是 262529。为此使用哪种算法。我提供我的代码
从 sklearn.neural_network 导入 MLPClassifier
来自 sklearn.feature_extraction.text
导入CountVectorizer
来自 sklearn.model_selection
导入train_test_split
来自 sklearn.metrics
导入准确度_分数
df_half = df.iloc[:int(len(df)*0.1)]q = df_half[&#39;Question&#39;]t = [tags.split() for df_half[&#39;Tags&#39;]]向量化器中的标签= countvectorizer（）x_vectorized1 = vectorizer.fit_transform（q）label_binarizer = multiLabelBinarizer（）y_binarized1 = label_binarizer.fit_transform（t） 2，Random_State = 42）分类器= mlpClassifier (hidden_​​layer_sizes=(100,), max_iter=100, alpha=0.0001,solver=&#39;adam&#39;, verbose=10, random_state=42, tol=0.0001)classifier.fit(X_train1, y_train1)y_pred1 = classifier.predict(X_train1) 
准确度=准确度_得分（y_train1，y_pred1）
print(“准确度：”, 准确度)
精度：0.9964`
我想预测问题​​明智的标签以及算法使用的大数据集中的问题。我正在尝试
TF-IDF
纳维贝叶斯
线性支持向量机
随机森林（RF）`]]></description>
      <guid>https://stackoverflow.com/questions/78469857/i-want-to-predict-tags-when-input-the-questions</guid>
      <pubDate>Mon, 13 May 2024 02:42:40 GMT</pubDate>
    </item>
    <item>
      <title>我如何进一步推进这个 AI/ML 项目？</title>
      <link>https://stackoverflow.com/questions/78469835/how-can-i-proceed-further-in-this-ai-ml-project</link>
      <description><![CDATA[我有 10 个数据集（.csv），每个数据集大约有 100,000 行，每行包含 5 个输入（-4.0f 到 +4.0f）和一个输出列（0/1）。现在我想用它来训练一个神经网络并预测给定的测试数据集（也有 100,000 行，但没有填充输出列）。我想创建一个 5--(reLU)--&gt;32- -(reLU)--&gt;32--(sigmoid)--&gt;1 神经网络并用这样的奖励系统对其进行训练 [if (expec.op ==0)reward=1- o/pfromNN; if (expec.op ==1) 奖励= o/pfromNN] 。我的问题是如何使用它调整 NN 的权重或者如何进一步进行？我是 NN 的新手，所以请帮助我解决这个问题
我想过像体育馆的月球着陆器模块一样这样做，但由于这里没有涉及任何州，我很困惑]]></description>
      <guid>https://stackoverflow.com/questions/78469835/how-can-i-proceed-further-in-this-ai-ml-project</guid>
      <pubDate>Mon, 13 May 2024 02:25:15 GMT</pubDate>
    </item>
    <item>
      <title>如何将 standardscaler() 用于具有多列的单行的 Predict() 函数？</title>
      <link>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</link>
      <description><![CDATA[很抱歉，如果我的描述含糊不清。
我正在尝试建立一个房价预测系统。数据有异常值并且是非高斯的，对于目标特征 y，使用对数变换。在进行一项热编码之前，我已使用 StandardScaler() 来适合我的模型。代码如下所示：
numerical_features = df4[[&#39;bhk&#39;, &#39;面积&#39;, &#39;price_lakhs&#39;, &#39;price_per_sqft&#39;]]
categorical_features = df4.select_dtypes(include=[&#39;object&#39;])

定标器=标准定标器()
Standardized_features = scaler.fit_transform(numerical_features)

std_df4 = pd.DataFrame(standardized_features, columns=numerical_features.columns)
std_df4.head()

现在为了预测新值，我使用了这个 Predict_price() 函数。我很难理解如何像上面的代码块一样做到这一点。我将数值和分类值分开。我不能在下面做同样的事情。这段代码工作错误，我认为列 x[3:] 中的一个热编码值也可能已被缩放，这不是上层代码的工作方式。我使用的任何回归模型[下面代码中的 clf.predict()] 对于下面的 Predict() 输入的不同值给出相同的答案。
def Predict_price（bhk，面积，price_per_sqft，类型，区域）：
    
    house_type_loc_index = np.where(X.columns == &#39;type_&#39; + type)[0][0]
    打印（房屋类型位置索引）
    
    Region_loc_index = np.where(X.columns == &#39;region_&#39; + 区域)[0][0]
    打印（region_loc_index）

    x = np.zeros(len(X.columns))
    x[0] = bhk
    x[1] = 面积
    x[2] = 每平方英尺价格
    
    如果 house_type_loc_index &gt;= 0：
        x[房屋类型位置索引] = 1
        
    如果region_loc_index &gt;= 0：
        x[区域位置索引] = 1
    
    列 = X.列
    x = x.reshape(1, -len(列))

    定标器=标准定标器()
    标准化特征 = 缩放器.fit_transform(x)
    数据 = pd.DataFrame(standardized_features, columns = columns)
    
    打印（数据）
    
    ans = clf.predict(数据)[0]
    返回exp(ans)

我期望模型能够根据我给预测函数的输入来预测值。预测函数的调用如下。
predict_price(bhk = 2，面积 = 2000，price_per_sqft = 35，类型 = &#39;公寓&#39;，区域 = &#39;Airoli&#39;)

我得到的答案是：42.6103853222858455
predict_price(bhk = 3，面积 = 600，price_per_sqft = 70，类型 = &#39;别墅&#39;，区域 = &#39;Vashi&#39;)

我得到的答案是：42.6103853222858455
我进一步检查，对于上面的 Predict_price() 行，它收到的每个值都是 0。这就是为什么我强烈认为我在 Predict() 中错误地使用了 standardScaler()
bhk面积价格_每平方尺户型_公寓户型_独立屋 \
0 0.0 0.0 0.0 0.0 0.0

   类型_顶层公寓类型_单间公寓类型_别墅区_阿格里帕达 \
0 0.0 0.0 0.0 0.0

   地区_艾罗利 ... 地区_瓦赛 地区_瓦希 地区_维赫罗利 \
0 0.0 ... 0.0 0.0 0.0

   地区_Ville Parle East 地区_Ville Parle West 地区_Virar \
0 0.0 0.0 0.0

   地区_Virar West 地区_Wadala 地区_Worli 地区_other
0 0.0 0.0 0.0 0.0
]]></description>
      <guid>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</guid>
      <pubDate>Mon, 13 May 2024 01:25:36 GMT</pubDate>
    </item>
    <item>
      <title>随机森林机器学习</title>
      <link>https://stackoverflow.com/questions/78469722/random-forest-machine-learning</link>
      <description><![CDATA[我做了一个模型预测，准确率达到 80%。您可以在此处.
现在，我想要进行案例实现，所以我制作随机数据帧，其中包含 100 行数据，其与训练数据具有完全相同的特征，但是当我运行它时，它会抛出这样的错误...
`ValueError：调用输入形状与训练期间提供的输入形状不同的模型：训练模型时输入单个数组 Tensor(“inputs:0”, shape=(None, 27), dtype=float32)在{&#39;SeniorCitizen&#39;：，&#39;合作伙伴&#39;：，&#39;家属&#39;：，&#39;PhoneService&#39;：、&#39;MultipleLines&#39;: 、&#39;OnlineSecurity&#39;: 、&#39;OnlineBackup&#39;: 、&#39;DeviceProtection&#39;: 、&#39;TechSupport&#39;: 、&#39;StreamingTV&#39;: 、&#39;StreamingMovies&#39;: 、&#39;无纸化账单&#39;: &lt; ;Semantic.NUMERICAL: 1&gt;, &#39;gender_Female&#39;: , &#39;gender_Male&#39;: , &#39;InternetService_DSL&#39;: , &#39;InternetService_Fiber_optic&#39; : 、&#39;InternetService_No&#39;: 、&#39;Contract_Month-to-month&#39;: 、&#39;Contract_One_year&#39;: , &#39;Contract_Two_year&#39;: , &#39;PaymentMethod_Bank_transfer_(automatic)&#39;: , &#39;PaymentMethod_Credit_card_(automatic)&#39;: , &#39;PaymentMethod_Electronic_check &#39;: , &#39;PaymentMethod_Mailed_check&#39;: , &#39;tenure&#39;: , &#39;MonthlyCharges&#39;: , “TotalCharges”：&lt;语义.数值：1&gt;}。
调用层“random_forest_model”接收的参数（类型 RandomForestModel）：
  输入=tf.Tensor（形状=（无，27），dtype=float32）
  • 训练=False`

我很确定train和随机df的列数是相同的，列的名称是相同的，dtype是相同的。我不知道该怎么办了，请帮助我。
您可以在此处查看所有错误消息
我用来制作随机 100 行数据框的代码此处
解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78469722/random-forest-machine-learning</guid>
      <pubDate>Mon, 13 May 2024 01:17:18 GMT</pubDate>
    </item>
    <item>
      <title>在 Streamlit.io 上部署 Python 应用程序时出错：“sklearn”的 ModuleNotFoundError</title>
      <link>https://stackoverflow.com/questions/78469534/error-deploying-python-app-on-streamlit-io-modulenotfounderror-for-sklearn</link>
      <description><![CDATA[在此处输入图片说明
标题：在 Streamlit.io 上部署 Python 应用程序时出错：“sklearn”的 ModuleNotFoundError
描述：
我在 Streamlit.io 上部署 Python 应用程序时遇到错误。尽管在我的 requests.txt 文件中列出了“scikit-learn”，但我在部署过程中遇到了 ModuleNotFoundError。]]></description>
      <guid>https://stackoverflow.com/questions/78469534/error-deploying-python-app-on-streamlit-io-modulenotfounderror-for-sklearn</guid>
      <pubDate>Sun, 12 May 2024 23:18:51 GMT</pubDate>
    </item>
    <item>
      <title>确定某些公司名称是否相同的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78469183/model-to-determine-if-certain-company-names-are-the-same</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78469183/model-to-determine-if-certain-company-names-are-the-same</guid>
      <pubDate>Sun, 12 May 2024 20:10:11 GMT</pubDate>
    </item>
    <item>
      <title>准确的时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/78469052/accurate-time-series-anomaly-detection</link>
      <description><![CDATA[尝试以最高精度对时间序列数据执行异常检测。您最近遇到的任何框架或 python 库。或者请告诉我您遇到过或使用过的最好的图书馆。
问候，
K
尝试过像 PyCaret 这样的库，但错误率很高。]]></description>
      <guid>https://stackoverflow.com/questions/78469052/accurate-time-series-anomaly-detection</guid>
      <pubDate>Sun, 12 May 2024 19:18:40 GMT</pubDate>
    </item>
    <item>
      <title>最终迭代中的 Tensorflow 错误：重塑的输入是具有 28 个值的张量，但请求的形状具有 128 个值</title>
      <link>https://stackoverflow.com/questions/78468947/tensorflow-error-in-final-iteration-input-to-reshape-is-a-tensor-with-28-values</link>
      <description><![CDATA[def DMRL(n_users, n_items, embed_dim, n_factors):
    断言 embed_dim % n_factors == 0，“embed_dim 必须能被 n_factors 整除”
    
    user_input = 输入(形状=(1,), dtype=&#39;int32&#39;, name=&#39;UserInput&#39;)
    user_embedding = 嵌入(n_users, embed_dim, name=&#39;UserEmbedding&#39;)(user_input)
    用户=扁平化（名称=&#39;UserFlatten&#39;）（user_embedding）
    
    item_input = 输入(形状=(1,), dtype=&#39;int32&#39;, name=&#39;ItemInput&#39;)
    item_embedding = 嵌入(n_items, embed_dim, name=&#39;ItemEmbedding&#39;)(item_input)
    items = Flatten(name=&#39;ItemFlatten&#39;)(item_embedding)

    textual_input = 输入（形状=（768，），名称=&#39;TextualInput&#39;）
    textual_mlp = Modal_MLP(embed_dim, 4)(textual_input)

    Visual_input = 输入（形状=（4096，），名称=&#39;VisualInput&#39;）
    Visual_mlp = Modal_MLP(embed_dim, 4)(visual_input)
    
    user_factor_embedding = tf.split(用户, n_factors, 1)
    item_factor_embedding = tf.split(items, n_factors, 1)
    textual_factor_embedding = tf.split(textual_mlp, n_factors, 1)
    视觉因子嵌入 = tf.split(视觉_mlp, n_factors, 1)

    #解缠结表示学习
    cor_loss = CorrelationLossLayer(n_factors)([视觉因子_嵌入、文本_因子_嵌入、用户_因子_嵌入、项目_因子_嵌入])

    # 因素交互作用
    评级 = FactorInteractionLayer(n_factors)([user_factor_embedding, item_factor_embedding, textual_factor_embedding, Visual_factor_embedding])
    评级 = Flatten()(评级)

    输出=密集（1，激活=&#39;线性&#39;）（评级）
    
    模型=模型（输入= [用户输入，项目输入，文本输入，视觉输入]，输出= [输出，cor_loss]）

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01),
                  损失=[mse_loss, cor_loss_dummy],
                  损失权重=[1.0,0.2])
    返回模型

def mse_loss(y_true, y_pred):
    返回 tf.keras.losses.MeanSquaredError()(y_true, y_pred)

def cor_loss_dummy(y_true, y_pred):
    返回 y_pred

emebd_dim = 128
n_因子 = 2
batch_size = emebd_dim //n_factors

    model.fit([train_user, train_item, train_text, train_image],
                    [train_y，tf.zeros_like(train_y)]，
                    批量大小=批量大小，
                    纪元=50，
                    回调=[es],
                    验证分割=0.1）


train_user 形状：(275232,)
train_item 形状：(275232,)
train_text 形状：(275232, 768)
训练图像形状：(275232, 4096)
train_y.shape: (275232,)
最后一次迭代中总是出现错误并显示以下消息：
reshape 的输入是一个具有 28 个值的张量，但请求的形状有 128 个
我预计错误发生在 FactorInteractionLayer 类中。
类 FactorInteractionLayer(层):
    def __init__(self, n_factors):
        super(FactorInteractionLayer, self).__init__()
        self.n_factors = n_factors
        self.h = 密集(3, 激活=&#39;tanh&#39;)
        self.user_sig = Dense(1, 激活=&#39;sigmoid&#39;)
        self.text_sig = Dense(1, 激活=&#39;sigmoid&#39;)
        self.visual_sig = Dense(1, 激活=&#39;sigmoid&#39;)
        self.attention_layer = Dense(3, 激活=&#39;softmax&#39;)

    def 调用（自身，输入）：
        用户嵌入、项目嵌入、文本嵌入、视觉嵌入 = 输入[0]、输入[1]、输入[2]、输入[3]

        输出=0
        对于范围内的 i(self.n_factors)：
            user_emb、item_emb、text_emb、visual_emb = user_embedding[i]、item_embedding[i]、text_embedding[i]、visual_embedding[i]

            user_item_text_visual = Concatenate()([user_emb, item_emb, text_emb, Visual_emb])
            h = self.h(user_item_text_visual)
            注意力权重 = self.attention_layer(h)

            user_item = tf.matmul(user_emb, item_emb)
            user_item = self.user_sig(user_item)
            user_text = tf.matmul(user_emb, text_emb)
            user_text = self.text_sig(user_text)
            user_visual = tf.matmul(user_emb, Visual_emb)
            用户视觉 = self.视觉_sig(用户视觉)

            用户项目重要性=注意力权重[:, 0]
            用户文本重要性=注意力权重[:, 1]
            用户视觉重要性=注意力权重[:, 2]

            user_item_imp = tf.tensordot（user_item，user_item_importance，轴= 0）
            user_text_imp = tf.tensordot(user_text, user_text_importance, 轴=0)
            user_visual_imp = tf.tensordot（user_visual，user_visual_importance，轴= 0）

            sum_concat = 连接()([user_item_imp, user_text_imp, user_visual_imp])

            输出+= tf.reduce_sum(sum_concat)
        输出 = tf.expand_dims(输出, -1)
        返回输出

但是错误消息上方还有一些附加消息：
节点：&#39;gradient_tape/model/ItemEmbedding/embedding_lookup/Reshape_1&#39;。
所以，我无法预测问题出在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/78468947/tensorflow-error-in-final-iteration-input-to-reshape-is-a-tensor-with-28-values</guid>
      <pubDate>Sun, 12 May 2024 18:44:27 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow ImportError：未定义符号：_ZTIN6snappy4SinkE</title>
      <link>https://stackoverflow.com/questions/78468933/tensorflow-importerror-undefined-symbol-ztin6snappy4sinke</link>
      <description><![CDATA[我尝试在 conda Python 环境中导入 TensorFlow，但遇到以下错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ImportError Traceback（最近一次调用最后一次）
第 1 行 [2] 中的单元格
----&gt; 1 将张量流导入为tf
2 设备名称 = tf.test.gpu_设备名称()
4 如果 device_name != &quot;/device:GPU:0&quot;:
文件〜/anaconda3/envs/tf-env/lib/python3.11/site-packages/tensorflow/__init__.py:40
37导入打字为_typing
39 # 不要删除这一行；请参阅https://github.com/tensorflow/tensorflow/issues/42596
---&gt; 40 from tensorflow.python import pywrap_tensorflow # pylint:disable=unused-import
41 从tensorflow.python.tools导入module_util作为_module_util
42 从tensorflow.python.util.lazy_loader导入LazyLoader as _LazyLoader
文件〜/anaconda3/envs/tf-env/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py:34
29 从tensorflow.python.platform导入self_check
31 # TODO(mdan)：清理反模式：导入以消除副作用。
32
33 # 执行预加载健全性检查，以产生更具可操作性的错误。
---&gt; 34 self_check.preload_check()
36 # pylint: 禁用=通配符导入，g-导入不在顶部，未使用的导入，行太长
38 尝试：
39 # 如果存在显式共享对象，则此导入预计会失败
40 # 依赖项（with_framework_lib=true），因为我们不需要 RTLD_GLOBAL。
文件 ~/anaconda3/envs/tf-env/lib/python3.11/site-packages/tensorflow/python/platform/self_check.py:63，在 preload_check() 中
50 引发导入错误（
51 “找不到 DLL %r。 TensorFlow 要求这些 DLL“
52 “安装在您的 %%PATH%% 中指定的目录中”
（...）
56“https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads”
57%”或“.join(缺失))
58 其他：
59 # 加载执行CPU功能保护检查的库。在这里做这个
60 # 作为预加载检查使我们更有可能检测到任何 CPU 功能
61 # 在我们触发它们之前不兼容（这通常会导致
62# 信号）。
---&gt; 63 从tensorflow.python.platform导入_pywrap_cpu_feature_guard
64 _pywrap_cpu_feature_guard.InfoAboutUnusedCPUFeatures()
导入错误：/home/gimhara/anaconda3/envs/tf-env/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2：未定义符号：_ZTIN6snappy4SinkE

我使用的是安装在 Ubuntu 22.04 上的 Anaconda 环境中的 Python 3.11 和 TensorFlow v2.15.0。
根据我的研究，此错误似乎与 TensorFlow 使用的 Snappy 压缩库版本缺失或不兼容有关。
我已尝试以下步骤来解决该问题：

使用 pip 和 conda 重新安装 TensorFlow。

在 Ubuntu 上安装 libsnappy-dev 软件包。


但是，到目前为止，这些步骤都没有解决问题。
任何人都可以提供有关如何正确解决此 ImportError 并使 TensorFlow 正确运行的指导吗？
如果您需要任何其他信息，或者我是否应该提供有关我的设置或迄今为止已采取的步骤的更多详细信息，请告诉我。
预先感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78468933/tensorflow-importerror-undefined-symbol-ztin6snappy4sinke</guid>
      <pubDate>Sun, 12 May 2024 18:40:19 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：clean() 得到了意外的关键字参数“fix_unicode”</title>
      <link>https://stackoverflow.com/questions/78468882/typeerror-clean-got-an-unexpected-keyword-argument-fix-unicode</link>
      <description><![CDATA[我已经编写了一个Python程序来实现BERT算法，但是我遇到了下面的错误，下面是cleantext()函数。我更新了cleantext包，但是它不起作用。我已经编写了一个Python程序来实现BERT算法，但是我遇到了下面的错误错误和 cleantext() 函数如下。我更新了 cleantext 包，但它不起作用
代码：
def cleanhtml(raw_html):
    clean = re.compile(&#39;&lt;.*?&gt;&#39;)
    cleantext = re.sub(cleanr, &#39;&#39;, raw_html)
    返回纯文本

定义清洁（文本）：
    文本 = 文本.strip()
    
    #定期清洁
    文本=干净（文本，
        fix_unicode=真，
        to_ascii=假,
        较低=真，
        no_line_breaks=真，
        no_urls=真，
        no_emails=正确，
        no_phone_numbers=正确，
        no_numbers=假，
        no_digits=假，
        no_currency_symbols=真，
        no_punct=假,
        Replace_with_url=&quot;&quot;,
        Replace_with_email=“”，
        Replace_with_phone_number=“”，
        Replace_with_number=“”，
        Replace_with_digit=“0”，
        Replace_with_currency_symbol=“”，
    ）

    # 清理 html
    文本 = cleanhtml(文本)
    
    # 标准化
    标准化器 = hazm.Normalizer()
    文本=规范化器.规范化（文本）
    # 删除奇怪的模式
    wierd_pattern = re.compile(“[”;
        u“\U0001F600-\U0001F64F” # 表情符号
        u“\U0001F300-\U0001F5FF” # 符号 &amp;象形文字
        u“\U0001F680-\U0001F6FF” # 交通 &amp;地图符号
        u“\U0001F1E0-\U0001F1FF” # 标志 (iOS)
        u“\U00002702-\U000027B0”
        u“\U000024C2-\U0001F251”
        u“\U0001f926-\U0001f937”
        你&#39;\U00010000-\U0010ffff&#39;
        你“\u200d”
        u“\u2640-\u2642”
        u“\u2600-\u2B55”
        你“\u23cf”
        你“\u23e9”
        你“\u231a”
        你“\u3030”
        u“\ufe0f”
        你“\u2069”
        你“\u2066”
        # 你“\u200c”
        你“\u2068”
        你“\u2067”
        “]+”，flags=re.UNICODE)
    
    文本 = wierd_pattern.sub(r&#39;&#39;, 文本)
    
    # 删除多余的空格、主题标签
    文本 = re.sub(“#”, “”, 文本)
    文本 = re.sub(“\s+”, “”, 文本)
    
    返回文本

 data[&#39;cleaned_comment&#39;] = data[2].apply(cleaning)

# 根据评论的字词计算评论的长度
数据[&#39;cleaned_comment_len_by_words&#39;] = 数据[&#39;cleaned_comment&#39;].apply(lambda t: len(hazm.word_tokenize(t)))

# 删除长度少于三个字的评论
data[&#39;cleaned_comment_len_by_words&#39;] = data[&#39;cleaned_comment_len_by_words&#39;].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else len_t)
数据 = data.dropna(subset=[&#39;cleaned_comment_len_by_words&#39;])
数据 = data.reset_index(drop=True)

数据.head()

错误：
TypeError Traceback（最近一次调用最后一次）

&lt;ipython-input-17-46d1839b8806&gt;在&lt;细胞系：1&gt;()
----&gt; 1 数据[&#39;cleaned_comment&#39;] = 数据[2].apply(cleaning)
      2
      3 # 根据评论的字数计算评论的长度
      4 数据[&#39;cleaned_comment_len_by_words&#39;] = 数据[&#39;cleaned_comment&#39;].apply(lambda t: len(hazm.word_tokenize(t)))
      5

4帧

&lt;ipython-input-12-1e73a60c1c12&gt;在清洁中（文本）
      8
      9#定期清洗
---&gt; 10 文本 = 干净（文本，
     11fix_unicode=真，
     12 to_ascii=假,

类型错误：clean() 得到了意外的关键字参数“fix_unicode”

请帮我解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/78468882/typeerror-clean-got-an-unexpected-keyword-argument-fix-unicode</guid>
      <pubDate>Sun, 12 May 2024 18:21:16 GMT</pubDate>
    </item>
    <item>
      <title>使用Torchaudio库创建数据集时出错</title>
      <link>https://stackoverflow.com/questions/78466420/error-when-using-torchaudio-library-to-create-a-data-set</link>
      <description><![CDATA[我正在学习 YT 课程，研究使用 Torchaudio 的城市 8k 数据集。作者编写了完全相同的代码，但在我收到此错误时能够获得输出：
&lt;块引用&gt;
运行时错误：找不到适当的后端来处理 uri C:\Users\hbhavnag\Documents\Hussain\ASU\collision detector\urban sound\UrbanSound8K\audio\5\100263-2-0-121.wav 和格式无。

以下是我的代码：
from torch.utils.data 导入数据集
将 pandas 导入为 pd
导入火炬音频
导入操作系统

UrbanSoundDataset 类（数据集）：

    def __init__(自身,annotation_file,audio_dir):
        self.annotations = pd.read_csv(annotation_file)
        self.audio_dir = 音频_dir

    def __len__(自身):
        返回 len(self.annotations)

    def __getitem__(自身，索引)：
        audio_sample_path = self._get_audio_sample_path(索引)
        标签 = self._get_audio_sample_label(索引)
        信号，sr = torchaudio.load（audio_sample_path）
        返回信号、标签
    
    def _get_audio_sample_path（自身，索引）：
        Fold = f“fold{self.annotations.iloc[index,5]}”
        路径 = os.path.join(self.audio_dir, 折叠, self.annotations.iloc[index,0])
        返回路径
    
    def _get_audio_sample_label（自身，索引）：
        返回 self.annotations.iloc[index,6]
    
    如果 __name__ == “__main__”：
        注释_文件 = r“C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\metadata\UrbanSound8K.csv”
        audio_dir = r&quot;C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\audio&quot;
        usd = UrbanSoundDataset（注释文件，音频目录）
        print (f“数据集中有 {len(usd)} 个样本”)

    信号，标签 = 美元[2]

我尝试查找 Torchaudio 的文档，但不确定是否有任何内容可以直接帮助我。我假设存在一些版本兼容性问题。
我使用的是 Windows。]]></description>
      <guid>https://stackoverflow.com/questions/78466420/error-when-using-torchaudio-library-to-create-a-data-set</guid>
      <pubDate>Sun, 12 May 2024 00:34:24 GMT</pubDate>
    </item>
    <item>
      <title>我如何在 django web 上显示终端输出和 matplotlib 图形</title>
      <link>https://stackoverflow.com/questions/78465419/how-can-i-show-the-terminal-output-and-matplotlib-graphic-on-django-web</link>
      <description><![CDATA[我不知道该怎么做。这是我的 py 代码。 #
&lt;前&gt;&lt;代码&gt;
虹膜 = load_iris()
X = 虹膜数据
y = 虹膜.目标

plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=&#39;viridis&#39;)
plt.xlabel(&#39;萼片长度&#39;)
plt.ylabel(&#39;萼片宽度&#39;)
plt.title(&#39;Iris Veri Seti&#39;)
plt.colorbar(标签=&#39;类&#39;)
plt.show()
Veri setini eğitim ve test setlerine ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_test_scaled = 缩放器.transform(X_test)

分类器={
    “决策树”：DecisionTreeClassifier()，
    “随机森林”：RandomForestClassifier(),
    “梯度提升”：GradientBoostingClassifier()，
    “AdaBoost”：AdaBoostClassifier()
}

结果={}
对于名称，clf in classifiers.items()：
    clf.fit(X_train_scaled, y_train)
    y_pred = clf.predict(X_test_scaled)
    准确度=准确度_分数（y_test，y_pred）
    精度 = precision_score(y_test, y_pred, 平均值=&#39;加权&#39;)
    召回率=召回率（y_test，y_pred，平均值=&#39;加权&#39;）
    f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)
    results[name] = {“Accuracy”：准确率，“Precision”：精确率，“Recall”：召回率，“F1 Score”：f1}

print(&quot;Sınıflandırma Algoritması\tAccuracy\tPrecision\tRecall\tF1 Score&quot;)
对于名称，results.items() 中的指标：
    print(f&quot;{name}\t{metrics[&#39;Accuracy&#39;]:.4f}\t{metrics[&#39;Precision&#39;]:.4f}\t{metrics[&#39;Recall&#39;]:.4f}\t{metrics [&#39;F1 分数&#39;]:.4f}&quot;)


有没有简单的方法可以在网页上拍摄它？可以是 django 或其他东西，但它必须在网页上。
这是 matplotlib 输出
这是终端的输出]]></description>
      <guid>https://stackoverflow.com/questions/78465419/how-can-i-show-the-terminal-output-and-matplotlib-graphic-on-django-web</guid>
      <pubDate>Sat, 11 May 2024 16:56:57 GMT</pubDate>
    </item>
    <item>
      <title>我可以在我的 Flutter 应用程序中集成将执行语音命令的自定义 AI 模型吗？</title>
      <link>https://stackoverflow.com/questions/78458925/can-i-integrate-custom-ai-model-which-will-do-on-voice-commands-in-my-flutter-ap</link>
      <description><![CDATA[我想知道，因为我与人工智能并没有密切相关并集成它，也没有尝试过，是否有可能集成某种定制的人工智能模型来完成下一步的事情，例如：“嘿，你可以转到我的个人资料设置吗”。对于结果，我希望该人工智能模型能够自动响应我的导航到个人资料屏幕。我不知道这对于 Flutter 是否可行。
我做了一些研究，建议使用语音转文本，反之亦然，Tflite、Pytorch 等。通过他们自己的文本到语音转换功能，我可以从语音中获取文本，并基于它创建执行特定任务的函数（例如导航到我的应用程序中的配置文件设置）。但我不太确定是否要使用自定义 AI。
这只是一个研究问题，如果有人对此有更多了解，并且我需要随意加入对话以帮助我更多地了解这一点..提前致谢！ ：D
没什么——只是一项研究。]]></description>
      <guid>https://stackoverflow.com/questions/78458925/can-i-integrate-custom-ai-model-which-will-do-on-voice-commands-in-my-flutter-ap</guid>
      <pubDate>Fri, 10 May 2024 08:36:43 GMT</pubDate>
    </item>
    <item>
      <title>SageMaker 实验跟踪重复</title>
      <link>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</link>
      <description><![CDATA[我正在尝试通过 AWS SageMaker 使用脚本模式训练模型。
我想使用 AWS SageMaker Experiments 以及训练作业中的一些计算指标来跟踪此训练作业。当我开始训练作业时，会成功创建一个新的实验运行，该实验运行跟踪所有提供的超参数（例如，nesimators）。
但是，如前所述，此外，我还想跟踪自定义脚本中的其他指标（例如准确性）。在这里，我在拟合模型之前使用 load_run()，然后使用 run.log_metric() 记录指标。但是，当我这样做时，SageMaker 会在 UI 中创建一个新的单独实验条目，这意味着我的超参数和指标单独存储在两个单独的实验运行中：

我希望在一次实验运行中看到所有指标和超参数的组合。我做错了什么？
这是我用来启动训练过程的缩写代码：
&lt;前&gt;&lt;代码&gt;
exp_name = “sklearn-脚本模式-实验”

与运行（
    实验名称=实验名称，
    sagemaker_session=sess,
）运行时：

    sklearn_estimator = SKLearn(&#39;train.py&#39;,
                                    instance_type=&#39;ml.m5.large&#39;,
                                    Framework_version=&#39;1.0-1&#39;,
                                    role=“arn:aws:iam:::role/service-role/AmazonSageMaker-ExecutionRole-”,
                                    超参数={&#39;nestimators&#39;: 100},
                                    环境={“区域”：区域})

    sklearn_estimator.fit({&#39;train&#39;: f&#39;s3://{BUCKET}/{S3_INPUT_PATH}&#39;})

这是缩写的train.py：
 #在这里解析参数...等等...


    模型 = RandomForestClassifier(n_estimators=args.nesimators,
                                   最大深度=5，
                                   随机状态=1）

    使用 load_run(sagemaker_session=sagemaker_session) 运行：

        模型.fit(X, y)

        run.log_metric(name = &quot;最终测试损失&quot;, value = 0.9)
]]></description>
      <guid>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</guid>
      <pubDate>Wed, 02 Aug 2023 15:20:27 GMT</pubDate>
    </item>
    <item>
      <title>每个示例使用多个类别对分类特征进行编码</title>
      <link>https://stackoverflow.com/questions/57752264/encode-a-categorical-feature-with-multiple-categories-per-example</link>
      <description><![CDATA[我正在处理一个数据集，该数据集的一个特征是单个示例具有多个类别。
该功能如下所示：- 

&lt;前&gt;&lt;代码&gt;功能
0 [类别 1、类别 2、类别 2、类别 4、类别 5]
1 [类别 11、类别 20、类别 133]
2 [类别2、类别9]
3 [类别 1000、类别 1200、类别 2000]
4 [类别12]

该问题与发布的问题类似：- 每个示例使用多个类别对分类特征进行编码 - sklearn
现在，我想向量化这个特征。一种解决方案是按照上述类似问题的答案中的建议使用 MultiLabelBinarizer。但是，大约有 2000 个类别，这导致编码数据稀疏且维数非常高。
还有其他可以使用的编码吗？或者这个问题的任何可能的解决方案。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/57752264/encode-a-categorical-feature-with-multiple-categories-per-example</guid>
      <pubDate>Mon, 02 Sep 2019 06:25:01 GMT</pubDate>
    </item>
    </channel>
</rss>