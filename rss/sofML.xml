<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 05 Nov 2024 01:14:53 GMT</lastBuildDate>
    <item>
      <title>行业 RAG 技术</title>
      <link>https://stackoverflow.com/questions/79157397/industries-rag-technology</link>
      <description><![CDATA[RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？
RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？在临床环境中可以实施哪些具体用例来改善决策？]]></description>
      <guid>https://stackoverflow.com/questions/79157397/industries-rag-technology</guid>
      <pubDate>Tue, 05 Nov 2024 00:54:23 GMT</pubDate>
    </item>
    <item>
      <title>我的自定义 layernorm 函数有什么问题？</title>
      <link>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</link>
      <description><![CDATA[import numpy as np
import torch
import torch.nn. functional as F

def layer_norm(x, weight, bias, eps=1e-6):
# x 形状：[bs, h, w, c]
# 计算空间维度（高度、宽度）的平均值和方差
mean = np.mean(x, axis=(1, 2), keepdims=True) # 形状：(batch_size, 1, 1, channels)
var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0 表示有偏方差

# 标准化
x_normalized = (x - mean) / np.sqrt(var + eps)

# 应用权重和偏差
out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
return out

def test1(x):
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
weight = np.ones(channels)
bias = np.zeros(channels)

normalized_output = layer_norm(x, weight, bias)
return normalized_output

def test2(x):
global channels
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
x_tensor = torch.tensor(x, dtype=torch.float32)
weight = torch.ones(channels)
bias = torch.zeros(channels)

# 使用 PyTorch 的层规范，在最后一个维度（通道）上进行规范化
normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
return normalized_output.detach().numpy()

# 测试
batch, channels, height, width = 4, 3, 8, 8
# 生成随机输入
x = np.random.randint(-10, 10, (batch, channels, height, width))

# 计算两种实现的输出
layernorm1 = test1(x)
layernorm2 = test2(x)

# 检查输出是否接近
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close) # 如果它们足够接近，则应输出 True

var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0对于有偏方差
var = np.var(x, axis=(1, 2), keepdims=True)

我的期望是 are_close==True,，这意味着 layernorm1 和 layernorm2 的距离非常小。由于 layernorm1 和 layernorm2 的形状很大，所以我会显示 layernorm1 和 layernorm2 的部分结果。 layernorm1[0,0,0:3,0:4] array([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1.84752351]]) layernorm2[0,0,0:3,0:4] array([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)
我尝试了带或不带 ddof=0 的 variacne 方法，打印语句中全部为 False。
我想知道如何实现类似于 Pytorch 内置 layernorm 函数的自定义 layernorm。
从代码角度看 layernorm 步骤是什么？
关于计算机视觉，layernorm 对特征图有什么作用？]]></description>
      <guid>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</guid>
      <pubDate>Mon, 04 Nov 2024 22:07:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Android Studio 制作一个仅扫描姓名和身份证号码的身份证扫描器？</title>
      <link>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</link>
      <description><![CDATA[我是一名学生，这将是我论文的系统。我们将实施一个基于移动设备的 OCR 扫描仪系统，该系统只能读取残障人士和老年人的身份证。我对如何做到这一点有一些想法，但我认为我需要更多的输入和逻辑。我正在考虑使用 Google ML Kit 进行机器学习和文本识别，并使用 TensorFlow 进行身份证的对象检测。
我的问题是，我国每个城市的残障人士和老年人身份证的格式和外观都不同，但我们没有时间收集所有这些信息。除了在对象检测中添加关键字（例如“老年人”、“残疾”等）以启动扫描仪外，是否有可能创建一个可以识别残障人士和老年人身份证的系统，而无需收集我国每种类型的老年人和残障人士身份证来开发算法？]]></description>
      <guid>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</guid>
      <pubDate>Mon, 04 Nov 2024 14:34:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 优化多玩家对数线性学习的实现</title>
      <link>https://stackoverflow.com/questions/79155493/optimising-log-linear-learning-implementation-for-multiple-players-in-python</link>
      <description><![CDATA[我正在为潜在游戏类别测试对数线性学习，我想用两个以上的玩家来测试它。我在优化代码时遇到了问题。
目前，我正在利用对数线性学习会引发马尔可夫链这一事实，并且转换可以用转换矩阵来解释。然而，随着玩家数量的增加，转换矩阵的维度迅速增加，使得无法使用密集的 numpy 数组来存储转换矩阵。我尝试过使用 scipy.sparse 矩阵，但计算速度非常慢 - 矩阵的使用方式是将平稳分布与转换矩阵相乘。
我想使测试多个玩家成为可能，并优化代码的计算时间。
这是使用 numpy 的转换矩阵的公式。
self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players))))

self.potential = np.zeros((self.no_action_profiles, 1))

P = np.zeros([self.no_action_profiles, self.no_action_profiles]) 

for idx, profile in self.action_profiles:

self.potential[idx] = self.potential_function(profile)

for player_id in range(self.no_players):

mask = np.arange(len(profile)) != player_id
opposites_actions = profile[mask] # 从动作配置文件中提取对手的动作

utility = np.array([self.utility_functions[player_id](i, opposites_actions) for i in range(self.no_actions)])
exp_values = np.exp(beta * utility)

p = exp_values/np.sum(exp_values)

i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id)
stride = self.no_actions ** (self.no_players - 1 - player_id)

P[idx, i: i + self.no_actions**(self.no_players - player_id) : stride] += 1/self.no_players*p

self.P = P

并使用 scipy。
self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players))))

self.potential = lil_matrix((self.no_action_profiles, 1))

# P = np.zeros([self.no_action_profiles, self.no_action_profiles]) 

P_row, P_col, P_data = [], [], []

for idx, profile in self.action_profiles:

self.potential[idx] = self.potential_function(profile)

for player_id in range(self.no_players):

mask = np.arange(len(profile)) != player_id
opposites_actions = profile[mask] # 从动作配置文件中提取对手的动作

utility = np.array([self.utility_functions[player_id](i, opposites_actions) for i in range(self.no_actions)])
exp_values = np.exp(beta * utility)

p = exp_values/np.sum(exp_values)

i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id)
stride = self.no_actions ** (self.no_players - 1 - player_id)

for j, prob in enumerate(p):
P_row.append(idx)
P_col.append(i + j * stride)
P_data.append(prob / self.no_players) 

P = coo_matrix((P_data, (P_row, P_col)), shape=(self.no_action_profiles, self.no_action_profiles))

self.P = P.tocsr()

return self.P

它们的用法如下。
P = self.gameSetup.formulate_transition_matrix(beta)
mu0 = self.mu_matrix.copy()

self.expected_value = np.zeros((int(self.max_iter), 1))

P = np.linalg.matrix_power(P, scale_factor)

for i in range(self.max_iter):

mu = mu0 @ P

mu0 = mu

self.expected_value[i] = mu @ self.gameSetup.potential

self.expected_value = self.expected_value
self.stationary = mu
]]></description>
      <guid>https://stackoverflow.com/questions/79155493/optimising-log-linear-learning-implementation-for-multiple-players-in-python</guid>
      <pubDate>Mon, 04 Nov 2024 12:37:29 GMT</pubDate>
    </item>
    <item>
      <title>如何从背景噪音和其他非吉他声音中识别乐器声音？[关闭]</title>
      <link>https://stackoverflow.com/questions/79155281/how-to-identify-instrument-sound-from-background-noise-and-other-non-guitar-soun</link>
      <description><![CDATA[我准备开发一款吉他学习应用，用户可以在其中弹奏自己选择的歌曲。该应用可以判断他们弹奏的音符是否正确，以及是否按时弹奏。弹奏后，应用将根据用户的表现为其打分。一个问题是输入的音频可能很嘈杂，并且可能包含非吉他声音（例如，当你正在练习时，你的妈妈突然对你说“晚餐准备好了”，或者同一个房间里有一个人在弹钢琴）。这可能会欺骗音高检测库，使其认为用户弹奏了一个音符，但实际上他们并没有弹奏。
我在 Google 上搜索了音高检测，找到了一些基于算法的库，如 YIN，但我找不到可以从其他推断中识别乐器声音的库。这就是我创建这篇文章的原因。]]></description>
      <guid>https://stackoverflow.com/questions/79155281/how-to-identify-instrument-sound-from-background-noise-and-other-non-guitar-soun</guid>
      <pubDate>Mon, 04 Nov 2024 11:32:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们有时在机器学习中将数据加倍？[关闭]</title>
      <link>https://stackoverflow.com/questions/79154883/why-we-sometimes-double-data-in-machine-learning</link>
      <description><![CDATA[有人能解释一下为什么我需要像这样复制 train_x 和 train_y 吗？当我仅应用一次 pd.concat（例如 train_x = train_x）时，我的模型的性能似乎显著下降。我试图理解为什么与单独使用原始数据集相比，将数据加倍（使用 train_x 和 train_y 两次）可以改善我的结果。
train_x = pd.concat(\[train_x, train_x\])
train_y = pd.concat(\[train_y, train_y\])

我尝试保持 train_x 和 train_y 原样，但这种方法并没有产生好的结果。您能否解释一下为什么加倍数据有帮助？]]></description>
      <guid>https://stackoverflow.com/questions/79154883/why-we-sometimes-double-data-in-machine-learning</guid>
      <pubDate>Mon, 04 Nov 2024 09:32:56 GMT</pubDate>
    </item>
    <item>
      <title>替代已弃用的 tensorflow.keras.preprocessing.image.ImageDataGenerator 的新库是什么？</title>
      <link>https://stackoverflow.com/questions/79154443/what-is-the-new-library-in-place-of-deprecated-tensorflow-keras-preprocessing-im</link>
      <description><![CDATA[ImageDataGenerator 库已弃用。
dataGen= ImageDataGenerator(width_shift_range=0.1, # 0.1 = 10% 如果大于 1 例如 10 那么它指的是像素数 例如 10 个像素
height_shift_range=0.1,
zoom_range=0.2, # 0.2 意味着可以从 0.8 到 1.2
sher_range=0.1, # 剪切角的大小
rotation_range=10) # 度
dataGen.fit(X_train)
batches= dataGen.flow(X_train,y_train,batch_size=20) # 请求数据生成器生成图像批次大小 = 否。每次调用时创建的图像数量
X_batch,y_batch = next(batches)

我遇到了这段代码，必须根据新库重写。有人能告诉我正确的库吗？如果可能的话，可以用新库重写这段代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79154443/what-is-the-new-library-in-place-of-deprecated-tensorflow-keras-preprocessing-im</guid>
      <pubDate>Mon, 04 Nov 2024 06:45:20 GMT</pubDate>
    </item>
    <item>
      <title>训练LLM时出现问题，3D attn_mask的形状错误</title>
      <link>https://stackoverflow.com/questions/79154375/problem-when-training-llm-shape-of-3d-attn-mask-is-wrong</link>
      <description><![CDATA[我目前正在尝试使用 PyTorch 库训练 LLM，但我遇到了一个无法解决的问题。我不知道如何修复此错误。也许有人可以帮助我。在帖子中，我将包含错误的屏幕截图。
这是我的训练单元的代码：
for epoch in range(1): 
for batch in train_loader:

input_ids = batch[&#39;input_ids&#39;]
tention_mask = batch[&#39;attention_mask&#39;]

target = input_ids[:, 1:]
input_ids = input_ids[:, :-1]
input_attention_mask =tention_mask[:, :-1].to(torch.float).transpose(0, 1)
target_attention_mask =tention_mask[:, 1:].to(torch.float).transpose(0, 1)

input_ids, target, target_attention_mask, input_attention_mask = input_ids.to(device), target.to(device), target_attention_mask.to(device), input_attention_mask.to(device)

logits = model(input_ids, target, input_attention_mask, target_attention_mask)

loss = criterion(logits.view(-1, vocal_size), target.view(-1))

optimizer.zero_grad()
loss.backward()
optimizer.step()

if epoch % 25 == 0:
loss =estimate_loss()
print(f&quot;Epoch {epoch}, Loss Train: {losses[&#39;train_loader&#39;]:.3f}, Loss Validation: {losses[&#39;validation_loader&#39;]:.3f}&quot;)
if epoch % 250 == 0:
torch.save(model.state_dict(), &quot;model.pth&quot;)
#再次加载model.load_state_dict(torch.load(&quot;model.pth&quot;))

这是我的前向函数：
import torch.nn as nn
import torch

class TransformerLanguageModel(nn.Module):
def __init__(self, vocab_size, embedding_dim=512, num_heads=8, num_layers=8, hidden_​​dim=2048):
super().__init__()

self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
self.position_embedding = nn.Embedding(1023, embedding_dim)

self.encoder_layers = nn.ModuleList([
nn.TransformerEncoderLayer(
d_model=embedding_dim,
nhead=num_heads, 
dim_feedforward=hidden_​​dim
) for _ in range(num_layers)
])

self.decoder_layers = nn.ModuleList([
nn.TransformerDecoderLayer(
d_model=embedding_dim, 
nhead=num_heads, 
dim_feedforward=hidden_​​dim
) for _ in range(num_layers)
])

self.fc_out = nn.Linear(embedding_dim, vocab_size)

def forward(self, input_ids, target, input_attention_mask=None, target_attention_mask=None):
input_token_embeddings = self.token_embedding(input_ids)
input_positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0)
input_position_embeddings = self.position_embedding(input_positions)
coder_embeddings = input_token_embeddings + input_position_embeddings

for layer in self.encoder_layers:
coder_embeddings = layer(encoder_embeddings, src_key_padding_mask=input_attention_mask)

target_token_embeddings = self.token_embedding(targets)
target_positions = torch.arange(0, target.size(1), device=targets.device).unsqueeze(0)
target_position_embeddings = self.position_embedding(target_positions)
coder_embeddings = target_token_embeddings + target_position_embeddings

seq_len = target.size(1)
causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=targets.device)).unsqueeze(0)

for layer in self.decoder_layers:
coder_embeddings = layer(decoder_embeddings,coder_embeddings, tgt_key_padding_mask=target_attention_mask,memory_key_padding_mask=input_attention_mask, tgt_mask=causal_mask)

logits = self.fc_out(decoder_embeddings)
return logits

这是错误
3D attn_mask 的形状是torch.Size([1, 1023, 1023])，但应该是 (8184, 32, 32)。)

我尝试重塑注意力掩码以适应尺寸，我也尝试运行没有注意力掩码的训练代码，因为我在前向函数中定义，它也应该在没有注意力掩码的情况下工作。我对这两个选项都没有运气。重塑没有起作用，因为我得到了不同的错误，我无法将其重塑为正确的张量尺寸。此外，当只是尝试在没有参数 input_attention_mask 和 target_attention_mask 的情况下拟合模型时，发生了与屏幕截图中相同的错误。我有点不知道还能尝试什么。我也有点困惑，因为 attn_mask 从未被定义过，只有注意力掩码、input_attention_mask 和 target_attention_mask。]]></description>
      <guid>https://stackoverflow.com/questions/79154375/problem-when-training-llm-shape-of-3d-attn-mask-is-wrong</guid>
      <pubDate>Mon, 04 Nov 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>选择处理缺失值的最佳技术[关闭]</title>
      <link>https://stackoverflow.com/questions/79153897/choosing-the-best-techniques-for-handling-missing-values</link>
      <description><![CDATA[有很多处理缺失值的技术。例如，平均值/中位数/众数插补、随机样本插补、分布末端插补等。现在我很困惑：我应该何时使用哪种技术？
也许随机样本插补可能是最好的，但并非在所有方面都是如此。]]></description>
      <guid>https://stackoverflow.com/questions/79153897/choosing-the-best-techniques-for-handling-missing-values</guid>
      <pubDate>Mon, 04 Nov 2024 00:09:55 GMT</pubDate>
    </item>
    <item>
      <title>coremltools 错误：ValueError：perm 的长度应与 rank(x) 相同：3 != 2</title>
      <link>https://stackoverflow.com/questions/79153512/coremltools-error-valueerror-perm-should-have-the-same-length-as-rankx-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79153512/coremltools-error-valueerror-perm-should-have-the-same-length-as-rankx-3</guid>
      <pubDate>Sun, 03 Nov 2024 19:49:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用函数 shap.Explainer 会根据输入的不同顺序获得不同的 shap 值？</title>
      <link>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</link>
      <description><![CDATA[我训练了一个二分类模型，并想使用 shap.Explainer 来分析特征贡献。
代码如下：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])

shap_values.values 的结果如下：




Feature 1
...




sample 0
-0.009703
...


样本 1
-0.009297
...


样本 2
-0.007699
...


样本 3
0.032624
...



但是当输入顺序改变时：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])

样本 0 和样本 1 的结果已更改：




特征 1
...




样本 1
-0.010012
...


样本0
-0.008277
...


样本 2
-0.007699
...


样本 3
0.032624
...



我不知道有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</guid>
      <pubDate>Sun, 03 Nov 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 中的眼睛标志模型无法跟踪看不见的数据</title>
      <link>https://stackoverflow.com/questions/79149930/eye-landmark-model-in-tensorflow-not-tracking-with-unseen-data</link>
      <description><![CDATA[我一直在使用 UTKFace 数据集创建一个模型，该模型将绘制图像中眼睛周围的点。我使用的训练集大小约为 40000 张图像，包含缩放、旋转和平移的增强图像。它与训练集和验证集配合得很好。但当我用该集中的新增强图像测试它时，它不起作用。我尝试过稍微调整一些参数和训练时间，但结果并没有发生很好的变化。
我的层目前看起来像这样：


inputs = tf.keras.layers.Input(shape=(200, 200, 1))
x = tf.keras.layers.Conv2D(32, kernel_size=(5, 5))(inputs)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(256, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(512, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

# 线性层。
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(units=256, kernel_regularizer=tf.keras.regularizers.l2(0.1))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.Dense(units=24)(x)




这是训练后验证集的样例输出，这就是我想要的结果。

这是来自验证集的图像。这也是正确的。

这是同一幅图像，但略有增强，但眼睛没有被跟踪。

如果您能提供任何关于如何改善结果的建议，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79149930/eye-landmark-model-in-tensorflow-not-tracking-with-unseen-data</guid>
      <pubDate>Sat, 02 Nov 2024 03:49:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>将 .ckpt 转换为 .h5</title>
      <link>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</link>
      <description><![CDATA[我已经使用 resnet18 训练模型进行 mask R-CNN 检测。对于每个 epoch，它只创建一个“.ckpt”文件。
现在我想使用该 .ckpt 文件作为检测图像的检测器。我有使用“.h5”文件进行检测的 Python 代码。
请帮助我如何使用“.ckpt”文件进行检测。或者我如何将其转换为“.h5”？
谢谢
我曾尝试在训练过程中生成“.h5”文件而不是“.ckpt”，但对我来说不起作用。
现在我需要一种方法来使用“.ckpt”文件来检测图像中的对象。]]></description>
      <guid>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</guid>
      <pubDate>Thu, 01 Dec 2022 10:50:58 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于回归的神经网络？</title>
      <link>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</link>
      <description><![CDATA[我正在尝试使用 Keras 来构建神经网络。我使用的数据是 https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics。我的代码如下：
import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r&quot;&quot;&quot;file location&quot;&quot;&quot;, delimiter=&#39;,&#39;)

model = Sequential()
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics =[&#39;accuracy&#39;])

Y = data[:,-1]
X = data[:,:-1]

从这里我尝试使用model.fit(X,Y)，但模型的准确性似乎保持在 0。我是 Keras 的新手，所以这可能是一个简单的解决方案，提前致歉。
我的问题是，向模型添加回归以提高准确性的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</guid>
      <pubDate>Tue, 27 Feb 2018 11:53:38 GMT</pubDate>
    </item>
    </channel>
</rss>