<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Feb 2025 21:15:33 GMT</lastBuildDate>
    <item>
      <title>函数内部或外部的白点和偏差会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/79412793/diferent-results-for-wheits-and-biases-in-or-out-the-function</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412793/diferent-results-for-wheits-and-biases-in-or-out-the-function</guid>
      <pubDate>Tue, 04 Feb 2025 19:13:39 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 和 scikit-learn 为特定设备创建机器学习和线性回归模型</title>
      <link>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device-with-py</link>
      <description><![CDATA[我有以下数据源模式：
在此处输入图片说明
我将“日期”列编码为三个单独的列：
年、月、日期。
我的未来是
设备 ID；消耗能量；年；月；日
在此处输入图片说明
我想预测给定未来的消耗能量（设备 ID 和日期）。能耗是一个日益增长的设备计数器
我创建了相关矩阵来查看属性之间的相关性，我发现消耗_能耗和设备之间的相关性非常低 (-0.15)。
以下是模型训练的模型指标：
&#39;rmse&#39;: np.float64(0.7648236497453013)、&#39;r2_score&#39;: 0.41662485203361777、&#39;coefficients&#39;: array([[-0.59067425, 0.61791617, 0.04103179, 0.00336239]])、&#39;intercept&#39;: array([0.00086466]
当我仅使用一个 device_id 加载数据时，结果会更好：
&#39;rmse&#39;: np.float64(0.1045133437744489), &#39;r2_score&#39;: 0.9894746517207146, &#39;coefficients&#39;: array([[0. , 0.99656364, 0.06202053, 0.00616694]]), &#39;intercept&#39;: array([-0.00046585])
我理解，基于多个设备的数据构建的模型将产生不同的结果...
我在一个文件中接收了所有设备的数据。当给定的未来是 device_id 时，如何正确构建此模型？
我应该创建预测方法吗？该方法首先为给定的 device_id 选择数据，然后为该特定设备创建新的数据框，然后构建模型并进行预测？这会耗费时间，我需要根据需求对每个预测进行此计算。如果我想，该如何处理这个问题保存/选择模型并使用它来提供数据？（我的数据有很多设备）
我不知道是否可以根据具有多个设备的数据构建此模型，并驱动线回归算法将设备 ID 未来视为主要/重要的系数因子。我应该使用不同的 ml 模型吗？
谢谢
我尝试使用 python 和 scikit-learn 为特定设备创建机器学习和线回归模型]]></description>
      <guid>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device-with-py</guid>
      <pubDate>Tue, 04 Feb 2025 17:46:48 GMT</pubDate>
    </item>
    <item>
      <title>实现扩散生成模型进行数据增强，但训练损失值太高</title>
      <link>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</guid>
      <pubDate>Tue, 04 Feb 2025 16:56:25 GMT</pubDate>
    </item>
    <item>
      <title>通过计算检查梯度是否会爆炸或消失</title>
      <link>https://stackoverflow.com/questions/79412199/check-through-calculations-whether-the-gradients-will-explode-or-vanish</link>
      <description><![CDATA[我正在复习旧考试题目，偶然发现了这个：

考虑一个常规的 MLP（多层感知器）架构，该架构具有 10 个完全连接的层和 ReLU 激活函数。网络的输入是一个大小为 100 的向量，其中每个维度的均值为零，标准差在整个数据集中等于 1。
每个隐藏层有 10000 个神经元，权重从均值为零、方差为 0.01 的正态分布初始化。


以下哪个选项最有可能？
a) 梯度将爆炸 (y)
b) 梯度将消失 (n)
c) 都不是。 (n)

我如何证明这个答案？我想我需要计算一些东西，但我不知道从哪里开始。
输入中的预期值都是零，权重也是如此，所以当一切都为零时很难计算任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/79412199/check-through-calculations-whether-the-gradients-will-explode-or-vanish</guid>
      <pubDate>Tue, 04 Feb 2025 15:26:12 GMT</pubDate>
    </item>
    <item>
      <title>寻找专业培训项目评论数据集</title>
      <link>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</link>
      <description><![CDATA[我正在开展一个项目，该项目涉及分析专业培训计划的评论并根据这些评论创建推荐系统。具体来说，我有兴趣找到一个包含各种专业发展课程、培训计划和认证的用户评论的数据集。
我搜索过 Kaggle 和政府开放数据门户等常见来源，但没有找到我正在寻找的内容。如果有人知道我可以在哪里找到这样的数据集，或者知道我如何使用其他方法来利用现有数据（Coursera / udemy 课程评论），我将非常感谢您的帮助！
提前谢谢您。]]></description>
      <guid>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</guid>
      <pubDate>Tue, 04 Feb 2025 12:51:33 GMT</pubDate>
    </item>
    <item>
      <title>如何计算用户绘制形状识别的最小二乘误差和特征面积误差？</title>
      <link>https://stackoverflow.com/questions/79411692/how-do-you-calculate-the-least-squares-error-and-feature-area-error-for-user-dra</link>
      <description><![CDATA[我想创建一个简单的系统来识别用户在我的大学课程游戏中绘制的预定义形状，以施展不同的法术。每个法术形状只能用一个笔画来绘制。经过大量研究，我发现了 MergeCF 算法（Aaron Wolin，2009）。
它看起来很简单，我能够获得一个初始的“角”数组来分割笔画段。但是，尝试将假阳性角合并到正确角需要使用计算误差拟合值。
这些值是“最小二乘误差”和“特征面积误差”，这些值的计算据称在另一篇关于 Paulson 识别器的论文中（Brandon Paulson，2008）。
阅读这篇论文，它详细说明了使用另一篇论文中的“正交距离平方”方法找到最小二乘误差（Tevfik Metin Sezgin，2002），并且使用另一篇论文中的计算找到特征面积误差（Bo Yu，2003）。
通过找到从一对用户绘制的点到最佳拟合线的每个四边形的面积来找到特征面积。但是，要找到最佳拟合线，您需要使用最小二乘法将线拟合到笔划段。
正交距离平方法要求已经找到最佳拟合线。
特征区域论文没有详细说明如何为直线或圆弧找到最佳拟合线，而正交距离平方论文提到使用“混合拟合”方法，由于缺乏更好的词汇，我无法理解。
我甚至尝试使用 ChatGPT 来了解发生了什么，我并不经常这样做，但经过几次提示后，它只是循环说最小二乘线性回归线是通过计算最小二乘误差找到的，而最小二乘误差是通过计算最小二乘回归线找到的。
如何计算最小二乘误差和特征面积误差？请帮忙。
有关更多信息（不完全相关），我正在使用虚幻引擎 5 蓝图，并且我决定不使用任何神经网络 OCR，例如 Tesseract，因为游戏应该在 Pico Neo 3 Pro VR 耳机上打包和使用，而无需外部命令行程序，这是客户的要求。

Aaron Wolin (2009)：https://www.researchgate.net/publication/220772382_Sort_Merge_Repeat_An_Algorithm_for_Effectively_Finding_Corners_in_Hand-sketched_Strokes
Brandon Paulson (2008)：https://www.researchgate.net/publication/221607733_PaleoSketch_Accurate_primitive_sketch_recognition_and_beautification
Tevfik Metin Sezgin (2002)：https://www.researchgate.net/publication/2496082_Sketch_Based_Interfaces_Early_Processing_for_Sketch_Understanding
Bo Yu (2003): https://dl.acm.org/doi/10.1145/604471.604499
]]></description>
      <guid>https://stackoverflow.com/questions/79411692/how-do-you-calculate-the-least-squares-error-and-feature-area-error-for-user-dra</guid>
      <pubDate>Tue, 04 Feb 2025 12:28:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 2048 游戏的训练对我来说效果不佳？[关闭]</title>
      <link>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</link>
      <description><![CDATA[因此我开始为游戏 2048 训练神经网络，这里是我的仓库

我尝试使用 DQN 算法，即我的仓库中的 traindqn.py，但分数在 200 到 400 之间随机。

然后我尝试使用进化算法，即我的仓库中的 trainevo.py，分数确实上升了，但似乎稳定在 500 左右。
在尝试了几次手动超参数调整后，我放弃了 DQN。

至于 evo，我增加了种群规模，以下是两个训练图
popsize 64，512 episodes evo
popsize 128，512 episodes evo
它们差别不大，上升到 500 左右，没有继续前进。
我预计它至少会超过 600（我测试了一个随机代理，它的平均分数在 500 左右），如果可能的话，还有办法让它更高，比如超参数调整或更改网络结构。]]></description>
      <guid>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</guid>
      <pubDate>Tue, 04 Feb 2025 10:28:14 GMT</pubDate>
    </item>
    <item>
      <title>huggingface 的图像分割 ONNX 在 ML.Net 中使用时会产生截然不同的结果</title>
      <link>https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use</guid>
      <pubDate>Tue, 04 Feb 2025 09:38:57 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 Llava-v1.5-7b 来检测图像，并从 LM Studio 中提取它的 API。当我要求模型命名时，它将自己称为 Vicuna</title>
      <link>https://stackoverflow.com/questions/79410924/im-trying-to-use-llava-v1-5-7b-to-detect-images-and-im-pulling-its-api-from-l</link>
      <description><![CDATA[为了降低成本，我决定在本地提供 LlaVA 图像。我无法在 google collab 中加载它，因为免费版本提供 12GB 内存，而加载 LlaVA 需要 16GB。
我决定下载 LM studio 并从那里获取 LLaVA，托管服务器托管在我的本地主机端口上。
然后，我将图像转换为 base64 并在我的 VS 代码中调用 api。
# 构建有效载荷。
# 许多实现模仿 OpenAI Chat Completions API，因此我们发送“模型”和“消息”列表。
# 在这里，我们假设您的 Llava 模型接受“图像”字段以及用户的文本。
payload = {
&quot;model&quot;: &quot;llava-v1.5-7b&quot;, # 在此处指定您的模型名称
&quot;messages&quot;: [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: query_text,
&quot;image&quot;:coded_image # 此字段名称可能因您的集成而异
}
]
}

headers = {&quot;Content-Type&quot;: &quot;application/json&quot;}

try:
response = request.post(url, headers=headers, json=payload)
response.raise_for_status()
except request.RequestException as e:
print(f&quot;与 Llava API 通信时出错：{e}&quot;)
return None

try:
return response.json()
except json.JSONDecodeError:
print(&quot;Response is not valid JSON.&quot;)
返回 response.text

当我输入罗纳尔多的图像时，它给出的结果不准确。当我要求它自己命名时，它回答说，

我是一个名为 Vicuna 的语言模型，我接受了大型模型系统组织 (LMSYS) 研究人员的培训。

我构建有效载荷的方式有问题吗？还是问题出在其他地方？
我首先尝试在 LM 工作室界面上传图像，在那里我可以上传罗纳尔多的图像，模型成功识别了人物、活动等。
当我使用端口和 API 标识符在 vs 代码中执行相同操作时，结果与描述一致。]]></description>
      <guid>https://stackoverflow.com/questions/79410924/im-trying-to-use-llava-v1-5-7b-to-detect-images-and-im-pulling-its-api-from-l</guid>
      <pubDate>Tue, 04 Feb 2025 07:49:43 GMT</pubDate>
    </item>
    <item>
      <title>以多模态方式训练潜在扩散模型</title>
      <link>https://stackoverflow.com/questions/79410889/training-latent-diffusion-models-in-a-multi-modal-manner</link>
      <description><![CDATA[LDM 论文提到调节块可以包括语义图、文本、图像和表示。是否可以使用图像内容作为模型输入，并通过调节块提供对象类和相应的边界框位置作为训练数据？
这旨在通过提供调节类和边界框，使模型能够在推理过程中在相关区域中生成指定对象。
以上所有内容均基于数据集包含违禁品且具有正确标记的类和边界框的假设。
我正在尝试通过扩散模型生成异常 X 射线包裹图像。]]></description>
      <guid>https://stackoverflow.com/questions/79410889/training-latent-diffusion-models-in-a-multi-modal-manner</guid>
      <pubDate>Tue, 04 Feb 2025 07:33:59 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 随机森林的不同结果（带种子）</title>
      <link>https://stackoverflow.com/questions/79410458/different-results-with-seed-for-sklearn-random-forest</link>
      <description><![CDATA[我正在使用 sklearn 运行随机森林。我正在为随机森林设置种子，以及拆分数据以进行交叉验证。当我连续多次重新运行代码时，它给出了相同的结果。但是，一个月后重新运行相同的代码，我得到了略有不同的特征重要性。在其他一些类似的分析中，准确度指标也不同。数据没有改变。我在 Google Colab 上运行。
这是我的代码：
# 配置
file_path = &#39;/content/drive/My Drive/dataset.csv&#39;
columns_to_keep = [
&#39;target_column&#39;, &#39;feature_a&#39;, &#39;feature_b&#39;, &#39;feature_c&#39;, &#39;feature_d&#39;, &#39;feature_e&#39;,
&#39;feature_f&#39;, &#39;feature_g&#39;, &#39;feature_h&#39;, &#39;feature_i&#39;, &#39;feature_j&#39;, &#39;feature_k&#39;, &#39;feature_l&#39;,
&#39;feature_m&#39;, &#39;feature_n&#39;, &#39;feature_o&#39;, &#39;feature_p&#39;, &#39;feature_q&#39;, &#39;feature_r&#39;, &#39;feature_s&#39;,
&#39;feature_t&#39;, &#39;feature_u&#39;, &#39;feature_v&#39;, &#39;feature_w&#39;, &#39;feature_x&#39;, &#39;feature_y&#39;, &#39;feature_z&#39;,
&#39;feature_aa&#39;, &#39;feature_ab&#39;, &#39;feature_ac&#39;, &#39;feature_ad&#39;, &#39;feature_ae&#39;, &#39;feature_af&#39;, &#39;feature_ag&#39;,
&#39;feature_ah&#39;, &#39;feature_ai&#39;, &#39;feature_aj&#39;, &#39;feature_ak&#39;, &#39;feature_al&#39;, &#39;feature_am&#39;, &#39;feature_an&#39;
]

df = pd.read_csv(file_path, usecols=columns_to_keep)

categorical_columns = [&#39;feature_ak&#39;, &#39;feature_al&#39;, &#39;feature_am&#39;, &#39;feature_an&#39;, &#39;feature_ao&#39;]
one_hot_columns = [&#39;feature_al&#39;, &#39;feature_ak&#39;]

df = df.dropna()

# 对指定列进行独热编码
le = LabelEncoder()
for col in one_hot_columns:
df[col] = le.fit_transform(df[col])

# 将指定列转换为分类
for col in categorical_columns:
df[col] = df[col].astype(&#39;category&#39;)

# 拆分为特征和目标
X = df.drop(columns=[&#39;target_column&#39;])
y = df[&#39;target_column&#39;]

# 初始化 RandomForestClassifier 模型
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# 初始化 k 倍交叉验证
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# 存储结果
feature_importances_list = []
all_y_true = []
all_y_pred = []

# 执行 k 倍交叉验证
for fold_num, (train_index, test_index) in enumerate(kf.split(X), start=1):
# 拆分数据
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# 训练随机森林模型
rf_model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = rf_model.predict(X_test)

# 收集所有真实和预测标签
all_y_true.extend(y_test)
all_y_pred.extend(y_pred)

# 获取此折叠的特征重要性
feature_importances_list.append(rf_model.feature_importances_)

# 计算并打印此折叠的准确度
accuracy_fold = accuracy_score(y_test, y_pred)
print(f&quot;Fold {fold_num} Accuracy: {accuracy_fold:.4f}&quot;)

# 计算所有预测的准确度
accuracy_cv = accuracy_score(all_y_true, all_y_pred)

# 生成分类报告
final_report = classes_report(all_y_true, all_y_pred, digits=3)

# 折叠的平均特征重要性
average_importance = sum(feature_importances_list) / len(feature_importances_list)

# 创建带有特征名称的 DataFrame及其相应的平均重要性
feature_names = X.columns
importance_df = pd.DataFrame({
&#39;Feature&#39;: feature_names,
&#39;Importance&#39;: average_importance
}).sort_values(by=&#39;Importance&#39;, accending=False)

# 打印结果
print(f&quot;具有 k 倍 CV 的随机森林模型的总体准确率：{accuracy_cv:.4f}&quot;)

print(&quot;\n最终分类报告：&quot;)
print(final_report)

print(&quot;\n随机森林特征重要性（跨倍平均）：&quot;)
print(importance_df.head(20))
]]></description>
      <guid>https://stackoverflow.com/questions/79410458/different-results-with-seed-for-sklearn-random-forest</guid>
      <pubDate>Tue, 04 Feb 2025 02:34:12 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型在二元分类中仅预测一个类（猫）</title>
      <link>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</link>
      <description><![CDATA[我是 AI 和深度学习的新手，我使用 TensorFlow/Keras 训练了一个二元分类 CNN 来区分猫和狗。然而，在测试数据集上进行评估时，该模型只预测每张图片都是“猫”，尽管数据集包含这两个类别。
这是我的代码：
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import load_model

def normalizer(image, label):
aux = tf.cast(image, dtype=tf.float32)
image_norm = aux/255.0
return image_norm, label

train_data, valid_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/training&#39;,
validation_split=0.1, 
subset=&quot;both&quot;, 
seed=42, 
image_size=(150, 150), 
batch_size=32 
)

test_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/test&#39;, 
image_size=(150, 150), 
batch_size=32 
)

train = train_data.map(normalizer)
valid = valid_data.map(normalizer) 
test = test_data.map(normalizer)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),activation=&#39;relu&#39;, input_shape=(150,150,3)))
model.add(MaxPooling2D())

model.add(Conv2D(filters=64, kernel_size=(3,3),激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Flatten())

model.add(Dense(units=256, 激活=&#39;relu&#39;))
model.add(Dense(units=1, 激活=&#39;sigmoid&#39;))

model.compile(
optimizer=&#39;adam&#39;,
loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[&#39;accuracy&#39;],
)

print(model.summary())

hist = model.fit(
训练，
batch_size=32， 
epochs=20， 
shuffle=True，
validation_data=valid
)

plt.plot(hist.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_loss&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.title(&#39;训练和验证中的损失&#39;)
plt.show()

如果 hist.history 中有 &#39;accuracy&#39;: 
plt.plot(hist.history[&#39;accuracy&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_accuracy&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.legend()
plt.title(&#39;训练和验证的准确性&#39;)
plt.show()

model.save(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

new_model = load_model(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

loss, acc = new_model.evaluate(test, batch_size=32)

print(loss)
print(acc)

y_pred = new_model.predict(test) 

y_true = np.concatenate([y.numpy() for x, y in test], axis=0)

matrix = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(
matrix.numpy(), 
annot=True, 
fmt=&#39;d&#39;, 
cmap=&#39;Blues&#39;, 
xticklabels=[&#39;Cat&#39;, &#39;Dog&#39;], 
yticklabels=[&#39;Cat&#39;, &#39;Dog&#39;],
)

plt.ylabel(&#39;True Label&#39;)
plt.xlabel(&#39;Predicted Label&#39;)
plt.title(&#39;Confusion Matrix&#39;)
plt.show()

这是混淆矩阵
混淆矩阵
我怀疑的可能原因

类别不平衡 –&gt; 我的训练数据中猫和狗的数量大致相等，所以我不认为这是原因。
标签问题 –&gt;我检查并确认 y_true 既有 0 也有 1，所以标签应该没问题。
注：该模型在验证中的准确率达到了约 70%
]]></description>
      <guid>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</guid>
      <pubDate>Mon, 03 Feb 2025 20:03:37 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的部分</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个表示手机移动轨迹的数据集，该轨迹由步行和驾车行驶的路段组成。数据包括经度、纬度和时间戳。我需要提取所有步行行驶的子轨迹。有没有现成的解决方案可以解决这个问题？如果没有，我该如何处理这个任务？
我试图在互联网上寻找现成的解决方案，但没有找到任何有价值的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的 GaussianProcessRegressor 对象：选择固定超参数，无法重现优化内核</title>
      <link>https://stackoverflow.com/questions/79407078/gaussianprocessregressor-object-in-scikit-learn-select-fixed-hyperparameters-c</link>
      <description><![CDATA[我试图理解 scikit-learn 中的 GaussianProcessRegressor 对象，可惜没有成功。
考虑文档中的示例 带有噪声目标的示例，我将其复制到下面以方便使用（略作更改，使用 ConstantKernel 而不是在内核定义中乘以常数）
import numpy as np

X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)
y = np.squeeze(X * np.sin(X))

#############
#############
noise_std = 0.75

将 matplotlib.pyplot 导入为 plt

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;真正的生成过程&quot;)
rng = np.random.RandomState(1)

training_indices = rng.choice(np.arange(y.size), size=6, replace=False)
X_train, y_train = X[training_indices], y[training_indices]

noise_std = 0.75
y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)
从 sklearn.gaussian_process 导入 GaussianProcessRegressor
从 sklearn.gaussian_process.kernels 导入 RBF、WhiteKernel、ConstantKernel

# kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()
kernel = ConstantKernel(constant_value=1)*RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) 
gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9)

gaussian_process.fit(X_train, y_train_noisy)
gaussian_process.kernel_
mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)
plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
X_train,
y_train_noisy,
noise_std,
linestyle=&quot;None&quot;,
color=&quot;tab:blue&quot;,
marker=&quot;.&quot;,
markersize=10,
label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
X.ravel(),
mean_prediction - 1.96 * std_prediction,
mean_prediction + 1.96 * std_prediction,
color=&quot;tab:orange&quot;,
alpha=0.5,
label=r&quot;95% 置信区间&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;高斯过程回归在嘈杂的数据集上&quot;)

我得到了这些结果

现在，我想使用具有“固定”参数的内核可获得相同的结果（用于与问题无关的其他目的）。
因此，我得到了上述内核的优化超参数，
gaussian_process.kernel_.get_params()
输出
{&#39;k1&#39;: 4.28**2,
&#39;k2&#39;: RBF(length_scale=1.1),
&#39;k1__constant_value&#39;: 18.30421069841903,
&#39;k1__constant_value_bounds&#39;: (1e-05, 100000.0),
&#39;k2__length_scale&#39;: 1.1043558649730463,
&#39;k2__length_scale_bounds&#39;: (0.01, 100.0)}

因此，我修改了之前的内核 &amp;高斯过程定义
kernel_fixed = ConstantKernel(constant_value=18.30, constant_value_bounds=&#39;fixed&#39;) *RBF(length_scale=1.1043, length_scale_bounds=&#39;fixed&#39;) 
gaussian_process_fixed = GaussianProcessRegressor(kernel=kernel_fixed, alpha=noise_std**2, n_restarts_optimizer=9)
gaussian_process_fixed.fit(X,y)
mean_prediction, std_prediction = gaussian_process_fixed.predict(X, return_std=True)

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
X_train,
y_train_noisy,
noise_std,
linestyle=&quot;None&quot;,
color=&quot;tab:blue&quot;,
marker=&quot;.&quot;,
markersize=10,
label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
X.ravel(),
mean_prediction - 1.96 * std_prediction,
mean_prediction + 1.96 * std_prediction,
color=&quot;tab:orange&quot;,
alpha=0.5,
label=r&quot;95% 置信度interval&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;Gaussian process return on a noisy dataset&quot;)


但结果非常不同，而内核的参数与我修复的参数非常接近优化的参数
{&#39;k1&#39;: 4.28**2,
&#39;k2&#39;: RBF(length_scale=1.1),
&#39;k1__constant_value&#39;: 18.3,
&#39;k1__constant_value_bounds&#39;: &#39;fixed&#39;,
&#39;k2__length_scale&#39;: 1.1043,
&#39;k2__length_scale_bounds&#39;: &#39;已修复&#39;}

我遗漏了什么？？
]]></description>
      <guid>https://stackoverflow.com/questions/79407078/gaussianprocessregressor-object-in-scikit-learn-select-fixed-hyperparameters-c</guid>
      <pubDate>Sun, 02 Feb 2025 18:17:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn 在 KDE 之前正确缩放数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</link>
      <description><![CDATA[我在地理空间数据上使用 sklearn.neighbors.KernelDensity。我注意到带宽估计方法没有考虑数据的空间范围，只考虑样本和特征的数量。 sklearn 文档或 sklearn 教程似乎没有提到应该缩放数据。
因此我的问题是：在拟合 KDE 之前缩放地理空间数据的适当方法是什么？
这是一个最小示例：
import numpy as np
from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=2)

X_uniform = np.random.rand(100, 2)
X_upscaled = X_uniform * 100

for i,X in enumerate([X_uniform, X_upscaled]):
kde = KernelDensity(kernel=&#39;gaussian&#39;, broadband=&quot;scott&quot;).fit(X)
print(f&quot;bw: {kde.bandwidth_}&quot;)

# 创建网格
x_grid = np.linspace(X[:,0].min(), X[:,0].max(), 50)
y_grid = np.linspace(X[:,1].min(), X[:,1].max(), 50)
X,Y = np.meshgrid(x_grid, y_grid)
xy = np.vstack([X.ravel(), Y.ravel()]).T

z = np.exp(kde.score_samples(xy)).reshape(X.shape)
fig.add_trace(go.Contour(z=z, x=x_grid, y=y_grid), row=1, col=i+1)

fig.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</guid>
      <pubDate>Wed, 29 Jan 2025 15:33:57 GMT</pubDate>
    </item>
    </channel>
</rss>