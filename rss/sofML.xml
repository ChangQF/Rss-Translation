<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 22 Dec 2024 15:14:37 GMT</lastBuildDate>
    <item>
      <title>单个卷积滤波器能否组合来自多个通道的输入值</title>
      <link>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</link>
      <description><![CDATA[关于卷积神经网络的问题。
第 1 部分：假设输入是 RGB 图像，我们将其放入卷积层。人们是否曾经使用同时对输入的多个通道（例如 R 和 G）进行操作的过滤器，并在计算中结合两个通道的值？换句话说，过滤器矩阵是 3D（或更多）还是 2D？
第 2 部分：如果我错了，请纠正我，但有时在同一层中可以有多个过滤器。我的意思是，也许输入图像有 1 个通道（灰度），但层的输出有 2 个通道。这相当于有 2 个独立的过滤器，即 2 个不同的 3x3 矩阵，每个矩阵产生一个通道。
我理解过滤器是一个单一（例如 3x3）矩阵，我们在输入图像周围移动它并计算它“覆盖”的区域的某个加权平均值。]]></description>
      <guid>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</guid>
      <pubDate>Sun, 22 Dec 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 MacbookM4 上使用 CUDA 吗？[重复]</title>
      <link>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</link>
      <description><![CDATA[我正在尝试为我的本科论文创建一个动作检测系统。
我现在正尝试将 MMSkeleton 集成到我的项目管道中。https://github.com/open-mmlab/mmskeleton
要使用 MMSkeleton，我需要安装 PyTorch 和 torchvision（需要 CUDA），但据我所知，Mac 无法做到这一点。我收到此错误 OSError：编译 MMSkeleton 需要 CUDA！
有人知道我该如何克服这个障碍吗？]]></description>
      <guid>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</guid>
      <pubDate>Sun, 22 Dec 2024 10:49:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么当 refit=True 时，在 RandomizedSearchCV 之后会进行额外的拟合？</title>
      <link>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</link>
      <description><![CDATA[我正在使用 scikit-learn 中的 RandomizedSearchCV 进行超参数调整，并注意到即使 refit 参数设置为 True，在找到最佳参数后也会执行额外的拟合步骤。
这是一个最小的可重现示例：
来自 sklearn.datasets 导入 make_classification
来自 sklearn.model_selection 导入 RandomizedSearchCV
来自 sklearn.ensemble 导入 RandomForestClassifier
来自 scipy.stats 导入 randint

# 生成合成数据集
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# 定义模型和参数分布
model = RandomForestClassifier(random_state=42)
param_dist = {
&#39;n_estimators&#39;: randint(10, 100),
&#39;max_depth&#39;: randint(3, 20),
}

# 执行 RandomizedSearchCV
search = RandomizedSearchCV(
model, param_dist, n_iter=10, cv=3, random_state=42, refit=True
)
search.fit(X, y)

# 访问最佳估计器
best_model = search.best_estimator_

print(best_model)

我理解 refit=True 标志表示在超参数调整后，应在整个数据集上重新拟合最佳模型。但是，为什么交叉验证中的“最佳模型”不直接作为 search.best_estimator_ 返回？对整个数据集执行另一个拟合步骤的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</guid>
      <pubDate>Sat, 21 Dec 2024 21:49:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 MaxViT 进行迁移学习时我的分类器应该是什么？</title>
      <link>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</link>
      <description><![CDATA[我正在尝试使用自定义数据集在 Pytorch 预训练模型上进行迁移学习。我已经能够使用 SqueezeNet 成功执行迁移学习。
对于 Squeezenet，我的分类器是，layers source
model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1)))

对于 Efficientnet，我的分类器是，layers source
model.classifier = torch.nn.Sequential(
torch.nn.Dropout(p=0.2, inplace=True),
torch.nn.Linear(in_features=1280,
out_features=output_shape,
bias=True))

我也一直在尝试为 MaxViT 做类似的事情，我查看了源代码，发现参数中有 block_channels[-1]。我最近开始用这个，不知道它们是什么，layers source
self.classifier = nn.Sequential(
nn.AdaptiveAvgPool2d(1),
nn.Flatten(),
nn.LayerNorm(block_channels[-1]),
nn.Linear(block_channels[-1], block_channels[-1]),
nn.Tanh(),
nn.Linear(block_channels[-1], num_classes, bias=False),
)

如果需要，以下是我使用 squeezenet 执行迁移学习的完整代码，仅供参考。
weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=d1,
test_dir=d2,
transform=auto_transforms,
batch_size=32)
for param in model.features.parameters():
param.requires_grad = False

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape = len(class_names)

model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1))).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
results = engine.train(model=model,
train_dataloader=train_dataloader,
test_dataloader=test_dataloader,
optimizer=optimizer,
loss_fn=loss_fn,
epochs=15,
device=device)

我的 MaxViT 分类器应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</guid>
      <pubDate>Sat, 21 Dec 2024 20:24:49 GMT</pubDate>
    </item>
    <item>
      <title>在 Swin transformer 中出现此运行时错误：mat1 和 mat2 形状无法相乘（32x7 和 1024x5）</title>
      <link>https://stackoverflow.com/questions/79299672/in-swin-transformer-getting-this-runtime-error-mat1-and-mat2-shapes-cannot-be-m</link>
      <description><![CDATA[如标题所述。我得到了那里显示的错误。错误发生在显示 outputs = model(images) 的行上。我正在尝试使用 swin transformer 对 5 个类别进行图像分类，并且只对最后几层进行微调。以下是代码：
# 加载预训练的 Swin Transformer 模型
base_model = timm.create_model(&#39;swin_base_patch4_window7_224&#39;, pretrained=True)

# 修改模型，在分类头之前包含全局平均池化
class SwinClassifier(nn.Module):
def __init__(self, base_model, num_classes):
super(SwinClassifier, self).__init__()
self.base_model = base_model
self.global_pool = nn.AdaptiveAvgPool2d(1) # 全局平均池化，将空间维度降低到 1x1
self.fc = nn.Linear(base_model.num_features, num_classes) # 全连接层

def forward(self, x):
x = self.base_model.forward_features(x) # 提取特征
print(f&quot;shape after forward_features: {x.shape}&quot;) # 调试行
x = self.global_pool(x) # 应用全局平均池化 (输出形状：[batch_size, num_features, 1, 1])
print(f&quot;shape after global pooling: {x.shape}&quot;) # 调试行
x = x.view(x.size(0), -1) # 将张量展平为形状 [batch_size, num_features]
print(f&quot;shape after flattening: {x.shape}&quot;) # 调试行
x = self.fc(x) # 分类头以获取最终输出
return x

# 创建修改后的模型的实例
model = SwinClassifier(base_model, num_classes=5)

# 解冻最后 4 层 (blocks 9 到 12)
for name, param in model.base_model.named_pa​​rameters():
如果名称中包含“layers.3.blocks.9”或“layers.3.blocks.10”或“layers.3.blocks.11”或“layers.3.blocks.12” in name:
param.requires_grad = True # 取消冻结最后 4 层
else:
param.requires_grad = False # 冻结其余部分

# 确保新的头部参数可训练
for param in model.fc.parameters():
param.requires_grad = True

# 将模型移动到设备
model.to(device)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

# 提前停止参数
patience = 5 # 停止前没有改进的 epoch 数量
best_val_loss = float(&#39;inf&#39;) # 将最佳验证损失初始化为无穷大
epochs_without_improvement = 0 # 用于跟踪没有改进的 epoch 的计数器改进
best_model_wts = None # 存储最佳模型的权重

# 日志设置
logging.basicConfig(filename=&#39;train_log_MessiSwinFineTune3LyrAUCF1Kappa.txt&#39;, level=logging.INFO, format=&#39;%(asctime)s - %(message)s&#39;)

# a

epochs = 30 # 设置最大 epoch 数

for epoch in range(epochs):
model.train()
running_loss = 0.0
correct = 0
total = 0
all_preds = [] # 用于存储混淆矩阵的所有预测的列表
all_labels = [] # 用于存储混淆矩阵的所有真实标签的列表

# 训练循环
for images, labels in train_loader:

images, labels = images.to(device), labels.to(device)

# 将参数梯度归零
optimizer.zero_grad()

# 正向传递修改后的 Swin 模型
输出 = 模型 (图像)

# 计算损失
损失 = 标准 (输出、标签)

# 反向传播和优化
损失.backward()
优化器.step()

# 统计
运行损失 +=损失.item()
_, 预测 = torch.max(输出, 1)
总 += 标签.size(0)
正确 += (预测 == 标签).sum().item()

all_preds.extend(预测.cpu().numpy())
all_labels.extend(标签.cpu().numpy())

train_acc = 100 * 正确 / 总
avg_train_loss = 运行损失 / len(train_loader)


forward_features 之后的形状：torch.Size([32, 7, 7, 1024])
全局之后的形状池化：torch.Size([32, 7, 1, 1])
打印扁平化后的形状：torch.Size([32, 7])，并显示运行时错误，提示 mat1 和 mat2 形状无法相乘（32x7 和 1024x5）
以上内容与错误一起打印。有人能告诉我到底哪里出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79299672/in-swin-transformer-getting-this-runtime-error-mat1-and-mat2-shapes-cannot-be-m</guid>
      <pubDate>Sat, 21 Dec 2024 15:44:53 GMT</pubDate>
    </item>
    <item>
      <title>岭回归的 Scratch 实现</title>
      <link>https://stackoverflow.com/questions/79299410/scratch-implementation-of-ridge-regression</link>
      <description><![CDATA[我正在尝试实现 Ridge 回归，但我觉得 Python 运算符缺少了一些东西。这是我的代码：
import numpy as np

x = np.random.rand(10, 2) 
y = np.random.rand(10, 1) 
lambda_reg = 0.1 
alpha = 0.1 
num_iterations = 100000 

X_train = np.hstack((np.ones((x.shape[0], 1)), x))

def ridge_regression_gradient_descent(X, y, lambda_reg, alpha, num_iterations):
n, p = X.shape 
B = np.zeros(p) 
# 梯度下降循环
for _ in range(num_iterations):
y_pred = X.dot(B).reshape(-1, 1) 
gradient_B0 = - (1/n) * np.sum(y - y_pred) 
gradient_B = - (1/n) * (X[:, 1:].T @ (y - y_pred)) + 
lambda_reg * B[1:].reshape(-1, 1) # B1 到 Bp 的梯度 

B[0] -= alpha * gradient_B0 
B[1:] -= alpha * gradient_B.reshape(-1) 
return B

B = ridge_regression_gradient_descent(X_train, y, lambda_reg, alpha, num_iterations)
print(B)

有人能看出我做错了什么吗？
我尝试对矩阵乘法的代码进行多次更改 + 还将所有内容重塑为正确的格式。我实际上得到了 3 个 Beta，所以这没问题，但我没有得到任何接近使用公式得到的结果：B = (X.T X + lambda * I)^-1 * X.T Y]]></description>
      <guid>https://stackoverflow.com/questions/79299410/scratch-implementation-of-ridge-regression</guid>
      <pubDate>Sat, 21 Dec 2024 12:31:54 GMT</pubDate>
    </item>
    <item>
      <title>图像分类：从单一模式到多模式数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79299213/image-classification-from-single-to-multi-modal-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79299213/image-classification-from-single-to-multi-modal-data</guid>
      <pubDate>Sat, 21 Dec 2024 10:03:41 GMT</pubDate>
    </item>
    <item>
      <title>如何显示自动编码器生成的图像？</title>
      <link>https://stackoverflow.com/questions/79297188/how-do-i-display-the-images-generated-by-an-autoencoder</link>
      <description><![CDATA[我使用 python 创建了一个自动编码器，没有任何错误。但是，我不知道如何显示自动编码器生成的图像的代码。自动编码器的代码如下所示：
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layer, models, datasets, callbacks
import tensorflow.keras.backend as K

#由于某些奇怪的原因，文本中没有包含
from keras.models import Model

#将数据导入训练集和测试集
from tensorflow.keras import datasets
(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()

#缩放图像
def preprocess(imgs):
imgs = imgs.astype(&quot;float32&quot;) / 255.0
imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)
imgs = np.expand_dims(imgs, -1)
return imgs
x_train = preprocess(x_train)
x_test = preprocess(x_test)

#编码器
encoder_input = layer.Input(
shape=(32, 32, 1), name = &quot;encoder_input&quot;
)
x = layer.Conv2D(32, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(
encoder_input
)
x = layer.Conv2D(64, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(x)
x = layer.Conv2D(128, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(x)
shape_before_flattening = K.int_shape(x)[1:]
x = layer.Flatten()(x)
encoder_output = layer.Dense(2, name=&quot;encoder_output&quot;)(x)
encoder = models.Model(encoder_input,coder_output)

#解码器
decoder_input = layer.Input(shape=(2,), name=&quot;decoder_input&quot;)
x = layer.Dense(np.prod(shape_before_flattening))(decoder_input)
x = layer.Reshape(shape_before_flattening)(x)
x = layer.Conv2DTranspose(
128, (3, 3), strides=2, 激活 = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
x = layer.Conv2DTranspose(
64, (3, 3), strides=2,activation = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
x = layer.Conv2DTranspose(
32, (3, 3), strides=2,activation = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
decoder_output = layer.Conv2D(
1,
(3, 3),
strides = 1,
activation=&quot;sigmoid&quot;,
padding=&quot;same&quot;,
name=&quot;decoder_output&quot;
)(x)
decoder = models.Model(decoder_input,coder_output)

# 将编码器与解码器连接起来
autoencoder = Model(encoder_input,coder(encoder_output))

# 编译自动编码器
autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;)
# 通过将输入图像作为输入和输出传入来训练自动编码器
# 使用一个 epoch

autoencoder.fit(
x_train,
x_train,
epochs=1,
batch_size=100,
shuffle=True,
validation_data=(x_test, x_test),
)

# 重建图像
example_images = x_test[:50]
predictions = autoencoder.predict(example_images)

我尝试使用 plt.imshow，如下所示。我期望看到自动编码器生成的 10 张图像。但是它不起作用。我真的不知道如何使用它：
for i in range(10):
plt.figure(figsize=(20,3))
plt.imshow(predictions[i].astype(&quot;float32&quot;), cmap=&quot;gray_r&quot;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79297188/how-do-i-display-the-images-generated-by-an-autoencoder</guid>
      <pubDate>Fri, 20 Dec 2024 12:10:50 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用具有动态尺寸输入的 Dense 层？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</link>
      <description><![CDATA[我有一个模型，其输入（具有形状（高度、宽度、时间）的图像批次）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Dense 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度 (None) 的输入
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用 Dense 层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：应定义 Dense 层输入的最后一个维度。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？本质上，我正在寻找一种方法来创建一个具有原始像素值的 1D 数组，但与 Dense 层兼容。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 对缺失标签的支持是什么</title>
      <link>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</link>
      <description><![CDATA[我们有一个数据集，其中某些标签缺失。我们最近才知道这一点，并且删除了这些行。这让我开始思考这到底是怎么回事？给 GBM 举一个没有标签的例子似乎没有意义。
有人能解释一下（双关语）LightGBM 如何处理缺少标签的行吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</guid>
      <pubDate>Thu, 03 Oct 2019 18:07:18 GMT</pubDate>
    </item>
    <item>
      <title>glove 和 word2vec 的主要区别是什么？</title>
      <link>https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec</link>
      <description><![CDATA[word2vec 和 glove 有什么区别？
这两种方法都是训练词向量的方法吗？如果是，那么我们该如何使用这两种方法？]]></description>
      <guid>https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec</guid>
      <pubDate>Fri, 10 May 2019 06:10:19 GMT</pubDate>
    </item>
    <item>
      <title>如何制作自然语言处理的人工智能机器人？[关闭]</title>
      <link>https://stackoverflow.com/questions/52799472/how-to-make-an-ai-bot-of-natural-language-processing</link>
      <description><![CDATA[我想制作一个只能理解四个单词“上”、“下”、“左”、“右”的人工智能机器人。
我的朋友制作了一个python脚本，可以通过语音执行一些任务，比如打开youtube，只需说“Youtube”，Chrome浏览器就会打开youtube.com URL。但是系统很慢，因为他们使用谷歌助手/人工智能来处理语音，这让我感到不耐烦。
然后我想到了一个想法，如果一个离线的人工智能系统只理解几个单词，我们可以得到一些想要的结果，而且速度会非常快。
例如：- 我有一辆遥控车，我想让它通过语音激活，当我说“上”时，车应该向前行驶，同样，“下”-&gt;“后退”，“左”-&gt;“左”和“右”也是如此-&gt; 正确 &amp; &quot;{任何其他声音}&quot; -&gt; 指示灯闪烁，表示系统无法理解
所以：
我应该如何开始？
我应该如何训练 AI Bot？
我的要求是什么？
以及我应该知道的其他事情。]]></description>
      <guid>https://stackoverflow.com/questions/52799472/how-to-make-an-ai-bot-of-natural-language-processing</guid>
      <pubDate>Sun, 14 Oct 2018 04:30:01 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“tensorflow.python.pywrap_tensorflow”没有属性“TFE_Py_RegisterExceptionClass”</title>
      <link>https://stackoverflow.com/questions/46010571/attributeerror-module-tensorflow-python-pywrap-tensorflow-has-no-attribute-t</link>
      <description><![CDATA[我正在尝试使用最新的可用资源开发一些时间序列序列预测。为此，我确实检查了 TensorFlow 时间序列的示例代码，但我收到了此错误：
AttributeError：模块“tensorflow.python.pywrap_tensorflow”没有属性“TFE_Py_RegisterExceptionClass”

我正在使用 Anaconda。当前环境是 Python 3.5 和 TensorFlow 1.2.1。还尝试了 TensorFlow 1.3，但没有任何变化。
这是我尝试运行的代码。我在 Google 上没有找到与该问题相关的任何有用信息。关于如何解决这个问题您有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/46010571/attributeerror-module-tensorflow-python-pywrap-tensorflow-has-no-attribute-t</guid>
      <pubDate>Sat, 02 Sep 2017 04:48:51 GMT</pubDate>
    </item>
    </channel>
</rss>