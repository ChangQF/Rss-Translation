<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 18 Oct 2024 21:16:34 GMT</lastBuildDate>
    <item>
      <title>我们如何知道Ranker（Weka）要设置什么参数？</title>
      <link>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</link>
      <description><![CDATA[我是 ML 的新手，目前正在使用 Weka。所以我有几个问题无法通过手册弄清楚：

我们如何知道 Ranker 要设置哪些参数？我的意思是，阈值和 numToSelect。对此有什么解释吗？
当我通过资源管理器选择属性并保存修改后的数据集时，它始终是 N+1 属性（N 个选定的属性 + 类/标签）。为什么？标签/类不也是属性吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</guid>
      <pubDate>Fri, 18 Oct 2024 20:54:50 GMT</pubDate>
    </item>
    <item>
      <title>高效 Net V2 M ONNX 模型在小输入上的推理速度明显较慢</title>
      <link>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</link>
      <description><![CDATA[当我将 Efficient net v2 m 模型从 Pytorch 转换为 Onnx 以适应不同大小的输入时，我注意到一种奇怪且无法解释的行为。我希望从这个社区找到对我的观察的解释。
在我的 RTX 4090 上，1280X1280 大小图像上的 ONNX 模型在 35 毫秒内推断出批处理大小为 1。当我将图像大小缩小到大约 192X192（批处理大小相同为 1）时，运行时间几乎保持不变。这是可以理解的，因为固定开销占主导地位，例如初始化时间、线程池预热、与 GPU 之间的低效数据传输，最重要的是，计算库针对 GPU 上的矢量化和 SIMD 指令进行了优化。
然而，令人困惑的是，一旦我开始将输入图像大小减小到 192X192 以下，运行时间就会急剧增加。对于 64X64 图像，批处理大小为 1 时运行时间为 &gt;100ms。我完全理解为什么在较小的图像上推理不应该更快，但我不明白为什么它会更慢（而且慢得多）。
当我增加较小图像的批处理大小时，每批的运行时间会大幅改善（不仅仅是每幅图像的运行时间）。对于批处理大小为 16 的图像，推理 192X192 图像每批需要 25 毫秒（每幅图像不到 2 毫秒），而批处理大小为 1 时则需要 &gt;100ms。同样，我对此没有任何解释。固定开销和优化的 SIMD 矢量化将决定每幅图像的摊销运行时间应该随着批处理大小的增加而改善。但是，我观察到整个批次的运行时间也得到了改善。
对于较大的图像（例如 1280X1280），增加批次大小会增加每个批次的运行时间（尽管是亚线性的，这是完全可以预料的 - 随着批次大小的增加，每个图像的运行时间仍然会缩短到一定限度，之后，对于几乎无法放入 GPU 内存的更高批次大小，每个图像的运行时间也会增加约 10%）。
但是在 CPU 上运行时，处理时间会随着输入大小的增加而单调增加，正如预期的那样。
当我要求它对所有输入进行处理时，我已经验证了 ONNX 模型在 GPU 上成功运行。事实上，对于小输入，CPU 推理时间比 GPU 更快（这是可以理解的，因为有固定的 I/O 和其他开销）
注意：由于我在整个实验过程中将动态轴设置为 None，因此我为具有不同输入大小的同一 torch 模型保存了多个版本的 ONNX 模型。使用或不使用 onnx-sim 几乎不会对运行时间产生影响（处理速度差异小于 10-15%）。我在 C++ 中以 OrtCUDAProviderOptions 作为执行提供程序运行 onnx 模型，使用或不使用 GraphOptimizationLevel 几乎没有区别。
神经网络的输出对于所有输入都符合预期，因此我不希望我的代码中出现任何错误。
TL;DR
我的 ONNX 模型对于中等大小图像的运行速度比 GPU 上的小图像更快。对于较小的图像，增加批次大小会大幅减少每批次的处理时间（而不仅仅是 SIMD 并行化所预期的每张图像的摊销时间）。]]></description>
      <guid>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</guid>
      <pubDate>Fri, 18 Oct 2024 20:32:42 GMT</pubDate>
    </item>
    <item>
      <title>多类别分类中的准确率和 F1 分数 = 1.0 的问题</title>
      <link>https://stackoverflow.com/questions/79103501/issues-with-accuracy-and-f1-score-1-0-in-multi-class-classification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79103501/issues-with-accuracy-and-f1-score-1-0-in-multi-class-classification</guid>
      <pubDate>Fri, 18 Oct 2024 19:59:23 GMT</pubDate>
    </item>
    <item>
      <title>我应该在正弦位置编码中交错正弦和余弦吗？</title>
      <link>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</link>
      <description><![CDATA[我正在尝试实现正弦位置编码。我发现了两个给出不同编码的解决方案。我想知道其中一个是错误的还是两个都是正确的。我展示了两个选项的最终编码的视觉图。谢谢 :)
class SinusoidalPosEmb(nn.Module):
def __init__(self, dim):
super().__init__()
self.dim = dim
def forward(self, x):
device = x.device
half_dim = self.dim // 2
emb = math.log(10000) / (half_dim - 1)
emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
emb = x[:, None] * emb[None, :]
emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
return emb


2)
class TransformerPositionalEmbedding(nn.Module):
&quot;&quot;&quot;
摘自论文《Attention Is All You Need》第 3.5 节
&quot;&quot;&quot;
def __init__(self, dimension, max_timesteps=1000):
super(TransformerPositionalEmbedding, self).__init__()
assert dimension % 2 == 0, &quot;Embedding 维度必须是偶数&quot;
self.dimension = dimension
self.pe_matrix = torch.zeros(max_timesteps, dimension)
# 收集嵌入向量中的所有偶数维度
even_indices = torch.arange(0, self.dimension, 2)
# 使用对数变换计算项以加快计算速度
# (https://stackoverflow.com/questions/17891595/pow-vs-exp-performance)
log_term = torch.log(torch.tensor(10000.0)) / self.dimension
div_term = torch.exp(even_indices * -log_term)
# 根据奇数/偶数时间步长预先计算位置编码矩阵
timesteps = torch.arange(max_timesteps).unsqueeze(1)
self.pe_matrix[:, 0::2] = torch.sin(timesteps * div_term)
self.pe_matrix[:, 1::2] = torch.cos(timesteps * div_term)
def forward(self, timestep):
# [bs, d_model]
return self.pe_matrix[timestep]

]]></description>
      <guid>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</guid>
      <pubDate>Fri, 18 Oct 2024 19:35:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 3D UNet 在进行二元分割时总是预测黑色？</title>
      <link>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</link>
      <description><![CDATA[我的输入图像是灰度图像（uint8），目标图像是二值图像（bool）。我将输入图像转换为 0~1 并对其进行归一化，然后将其输入到 3D UNet 中。
这是我用于 3D UNet 模型的 代码。
这是我用于训练的代码：
unet = UNet3D(in_channels=1, num_classes=1).to(device)

bce_loss = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(unet.parameters(), lr=0.01, motivation=0.9, weight_decay=0.0005)

for epoch in range(args.epochs):
unet.train()
train_loss = 0.0
for step, (x, y_true, _) in enumerate(train_loader): 
x, y_true = x.to(device=device, dtype=torch.float), y_true.to(device=device, dtype=torch.float)
optimizer.zero_grad()
y_pred = unet(x) # y_pred - (batch_size,1,depth,width,height)

loss = bce_loss(y_pred, y_true) # y_true - (batch_size,1,depth,width,height)
train_loss += loss.item()
loss.backward()
optimizer.step()

print(&quot;Epoch %d, 平均训练交叉熵损失 %9.6f&quot; % (epoch + 1, train_loss / len(train_loader)))

我的二元交叉熵损失没有收敛到一个较小的数字。预测的最大值不能超过 0.5，所以我无法预测掩码。
我尝试使用不同的学习率，将损失函数改为 bce 损失 + dice 损失，但仍然不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</guid>
      <pubDate>Fri, 18 Oct 2024 18:00:36 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 预测模型中更新未来预测的傅里叶项和滞后</title>
      <link>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</link>
      <description><![CDATA[我构建了一个 XGBoost 预测模型，该模型结合了滞后特征、日期时间特征和傅立叶项。我的目标是对每个新的工作日进行预测。该模型使用 5 倍交叉验证进行训练，我从调整过程中保存了最佳超参数。之后，我使用这些最佳参数重新训练整个模型。
我面临的挑战是使用预测日的正确特征更新未来数据框：
对于日期时间特征，我可以轻松更新它们。
但是，更新傅立叶项 (FT) 和滞后是我遇到的难题。
这是我尝试过的方法：
我将历史数据框与新的预测数据框结合起来，将 Y 列重命名为 pred。
对于第一个预测日，我使用最后已知的实际值。
对于后续的预测日，我需要使用历史数据和新预测值的组合来更新傅立叶项和滞后。
尽管尝试了几次，我还是无法让更新过程正常工作。傅立叶项没有按预期对齐，我很难根据历史数据和预测数据的组合调整滞后。
问题：如何使用历史预测和新预测的组合正确更新未来几天的傅立叶项和滞后？以下是数据框。 pred 列是实际历史数据的 Y 变量（国家计数）的副本。

# 使用 NaN 初始化预测列
future_df_all_countries[&#39;pred&#39;] = np.nan

# 使用已经训练过的 `best_model` 对未来数据进行预测
# 循环遍历 future_df_all_countries 中的每一行
for i in range(len(future_df_all_countries)):
if not future_df_all_countries[&#39;is_actual&#39;].iloc[i]: # 仅预测是否为未来数据
if i == 0:
# 对于第一个预测日，使用最后一个实际计数
last_actual_count = future_df_all_countries.loc[future_df_all_countries[&#39;is_actual&#39;] == True, &#39;Country count&#39;].iloc[-1]
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = last_actual_count
else:
# 使用已经训练好的模型对当天进行预测
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = best_model.predict(future_df_all_countries[FEATURES].iloc[[i]])[0]

# 计算傅里叶变换的函数
def calculate_fourier_transform(group, components):
data_FT = group[[&#39;pred&#39;]] # 使用 &#39;pred&#39; 列进行 FT 计算
if data_FT[&#39;pred&#39;].isnull().all():
return pd.DataFrame(index=group.index) # 如果全部为 NaN，则返回空 DF
country_count_fft = np.fft.fft(np.asarray(data_FT[&#39;pred&#39;].tolist()))
ifft_results = pd.DataFrame(index=group.index)

# 使用指定的列名进行更新
for i, num_ in enumerate(components):
fft_list = np.copy(country_count_fft)
fft_list[num_:-num_] = 0
ifft_results[f&#39;ifft_{num_}_components&#39;] = np.fft.ifft(fft_list).real

return ifft_results

# 用于傅里叶变换的组件列表
component_list = [70, 80, 90] # 使用指定的组件

#使用“pred”列计算所有行的 FT
fourier_results = calculate_fourier_transform(future_df_all_countries, component_list)

# 使用 FT 结果更新 future_df_all_countries
for col in fourier_results.columns:
future_df_all_countries[col] = fourier_results[col]

# 根据“pred”创建滞后特征
def create_lag_features(df, target_col=&#39;pred&#39;, group_col=&#39;SHIP_TO_COUNTRY&#39;, lags=[5, 10, 15, 30]):
for lag in lags:
df[f&#39;lag_{lag}_day&#39;] = df.groupby(group_col)[target_col].shift(lag)
return df

# 应用滞后特征创建
future_df_all_countries = create_lag_features(future_df_all_countries)
]]></description>
      <guid>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</guid>
      <pubDate>Fri, 18 Oct 2024 13:07:36 GMT</pubDate>
    </item>
    <item>
      <title>如何提高大型不平衡数据集的随机森林模型性能？[关闭]</title>
      <link>https://stackoverflow.com/questions/79100257/how-to-improve-random-forest-model-performance-for-large-imbalanced-datasets</link>
      <description><![CDATA[我一直在研究随机森林模型来预测员工流失。我的数据集非常不平衡，大约有 80% 的非流失案例和 20% 的流失案例。虽然我尝试使用 SMOTE 来平衡类别，但我的模型的准确率已经提高，但精确度和召回率仍未达到我想要的水平。
我目前所做的工作：

使用 SMOTE 对少数类进行过采样
使用 GridSearchCV 调整超参数，如 n_estimators、max_depth 和 min_samples_split
使用 class_weight=‘balanced’ 进行测试
使用混淆矩阵、ROC 曲线和精确度-召回率曲线进行评估

该模型目前的准确率达到 86%，但流失类的精确度和召回率仍然很低。我想在不牺牲整体性能的情况下提高模型正确预测减员情况的能力。]]></description>
      <guid>https://stackoverflow.com/questions/79100257/how-to-improve-random-forest-model-performance-for-large-imbalanced-datasets</guid>
      <pubDate>Fri, 18 Oct 2024 01:19:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在创建模型时考虑不同的列</title>
      <link>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</link>
      <description><![CDATA[我有一个发送警报的监控服务。我正在创建一个模型，如果警报在过去 1 个月内发生超过 3 次，该模型将标记警报。
我可以使用 IsolationForest 实现这一点，并指定模型中每个警报要考虑的字段。
但是，我面临的问题是警报的字段可能会有所不同。
考虑以下 2 个警报
AlertName Date FQDN DBName
磁盘使用率 90% 10/17/2024 00:00:000 test.com 
DB 已重新启动。10/17/2024 01:00:000 db1

在上面的例子中，如果它是磁盘使用率 90% 警报，那么我应该使用 FQDN 字段
如果是 DB 已重新启动，我应该使用 DBName 字段。
对于每个警报，用于确定其是否为重复警报的字段会有所不同，而我无法控制这些字段。
是否可以开发一个模型，该模型会动态考虑不同警报的不同列，而我无需指定要为每种警报类型考虑哪一列？]]></description>
      <guid>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</guid>
      <pubDate>Thu, 17 Oct 2024 23:57:47 GMT</pubDate>
    </item>
    <item>
      <title>像素映射到网络输入</title>
      <link>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</link>
      <description><![CDATA[我正在嵌入式设备中使用对象检测模型，需要运行测试来比较嵌入式平台和 PC 上的性能。为了测试的完整性，我需要确保在两种情况下像素的映射方式相同。我了解 Pytorch 如何将图像转换为形状为 CxHxW 的张量，但我要问的是这些像素究竟是如何映射到输入的。那么，例如，图像左上角的像素（所有 3 个通道中的值）是分配给前向传递中的第一个输入，还是最后一个输入？]]></description>
      <guid>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</guid>
      <pubDate>Thu, 17 Oct 2024 06:59:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的数据集不重复地分成测试和训练？</title>
      <link>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</link>
      <description><![CDATA[我正在开发一个 Python 脚本来测试一个算法。我有一个数据集，需要将其分成 80% 用于训练，20% 用于测试。但是，我想保存测试集以供进一步分析，确保与之前的测试集不重叠。
虽然我的代码总体运行良好，但我遇到了一个问题：由于随机选择过程，测试数据集有时包含之前测试运行中已经选择的记录。
在流程结束时，所有 100% 的记录都应在其中一次运行中进行测试
举个例子说明：

在第一次运行中，我的数据集 {0,1,2,3,4,5,6,7,8,9 被拆分为训练集 {0,1,2,4,5,7,8,9 和测试集 {3,6。
在第二次运行中，训练集为 {0,1,2,3,4,5,7,9，测试集为{6,8。

如您所见，记录 {6 被选中两次进行测试，我想避免这种情况。
我如何修改代码以确保每次随机选择 20% 的测试集，但排除任何之前选择的记录？
这是当前代码：
df = pd.read_csv(&quot;CustomersInfo.csv&quot;)
y = df[&#39;CustomerRank&#39;]
X = df.drop(&#39;CustomerRank&#39;, axis=1, errors=&#39;ignore&#39;)

#----------------------------------------------------------------------------------
#这是需要修复的部分
for RandStat in [11, 22, 33, 44, 55]:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RandStat)
#-------------------------------------------------------------------

clf = XGBClassifier(random_state=RandStat)
clf.fit(X_train, y_train)
fnStoreAnalyse(y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</guid>
      <pubDate>Thu, 17 Oct 2024 03:50:34 GMT</pubDate>
    </item>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程二元分类：为什么 GPy 的方差比 scikit-learn 小得多？</title>
      <link>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</link>
      <description><![CDATA[我正在学习使用高斯过程进行二元分类，并且正在将 GPy 与 scikit-learn 进行比较，该问题涉及一个玩具一维问题，灵感来自 Martin Krasser 的博客文章。两种实现（GPy 和 scikit-learn）似乎都使用带有 RBF 内核的类似设置。优化内核超参数后，长度尺度相似，但方差相差很大。GPy 内核方差似乎太小了。
我如何修改我的 GPy 实现并获得与 scikit-learn 类似的结果？我怀疑这与每个算法的内部实现有关，但我不知道是什么导致了这种巨大的差异。下面我将进一步解释为什么我认为我的 GPy 实现需要修复。
实现细节：Python 3.9 搭配 GPy 1.13.2 和 scikit-learn 1.5.1。可重现的示例：
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# 第 1 部分：玩具数据集创建
#################################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2 # 潜在函数
t = bernoulli.rvs(sigmoid(a)) # Bernoulli 训练数据（0 和1s)

#####################################
# 第 2 部分：scikit-learn 实现
#####################################

从 sklearn.gaussian_process 导入 GaussianProcessClassifier
从 sklearn.gaussian_process.kernels 导入 ConstantKernel，RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
* RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
kernel=rbf,
optimizer=&#39;fmin_l_bfgs_b&#39;,
n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# 第 3 部分：GPy 实现
############################

导入 GPy

kern = GPy.kern.RBF(
input_dim=1,
variance=1.,
lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
X=X,Y=t, kernel=kern, 
inference_method=GPy.inference.latent_function_inference.laplace.Laplace(), 
可能性=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
num_restarts=10, optimizer=&#39;lbfgs&#39;,
verbose=True, robust=True)

print(m.kern)
# rbf。| 值 | 约束 | 先验
# 方差 | 0.8067562453940487 | 0.001,10.0 | 
# lengthscale | 0.8365668826459536 | 0.001,10.0 |

lenghtscale 值大致相似 (0.858 vs 0.836)，但方差值非常不同 (scikit-learn 为 1.5**2 = 2.25，GPy 仅为 0.806)。
我认为我的 GPy 实现需要调整的原因是，即使有 +/- 2 个标准偏差界限，真正的潜在函数 (参见上面代码第 1 部分中的“a”) 也与预测函数不紧密匹配。另一方面，scikit-learn 实现与之相当匹配（可以使用 scikit-learn 检索潜在函数平均值和标准差如此处所示）。

 左：两个模型的预测概率相似（这是有道理的，因为它们共享相似的长度尺度值）。右：GPy 的预测潜在函数与真实潜在函数的拟合度不如 scikit-learn 模型。 
到目前为止，我尝试过的方法，结果没有显著变化：

输入特征 (X) 归一化
使用 GPy.inference.latent_function_inference.expectation_propagation.EP() 作为 GPy 推理方法，而不是拉普拉斯方法
向 scikit-learn 实现中添加 WhiteKernel 组件，如此处所建议的那样
]]></description>
      <guid>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</guid>
      <pubDate>Mon, 14 Oct 2024 13:10:16 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以获得包含世界上几乎所有国家护照的护照图像数据集？</title>
      <link>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</link>
      <description><![CDATA[我正在训练一个 OCR 模型，用于从护照中识别 MRZ。为了训练我的模型以获得更高的准确性，我需要用尽可能多的图片来训练它。我试图在 KAGGLE 上找到护照的数据集，但找不到。
有人能告诉我从哪里可以获得包含几乎所有国家或北美和南美护照的护照图像数据集吗？
非常感谢您的帮助。
祝好，
Asma]]></description>
      <guid>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</guid>
      <pubDate>Mon, 03 Feb 2020 13:11:08 GMT</pubDate>
    </item>
    <item>
      <title>DNNCLassifier Tensorflow 上的 label_keys 类型错误</title>
      <link>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</link>
      <description><![CDATA[我想将标签嵌入到 Tensorflow 中的 DNNClassifier 模型中。
与文档示例此处不同，我收到以下错误消息：
label_keys_values = [&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;] 
m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
feature_columns=deep_columns,
n_classes=4,
hidden_​​units=[12, 4],
label_keys=label_keys_values)
m.fit(input_fn=train_input_fn, steps=200)

文件 &quot;embedding_model_probe.py&quot;，第 118 行，位于 &lt;module&gt;
m.fit(input_fn=train_input_fn, steps=200)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py&quot;，第 281 行，位于 new_func
return func(*args, **kwargs)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 430 行，位于 fit
loss = self._train_model(input_fn=input_fn, hooks=hooks)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 927 行，位于 _train_model
model_fn_ops = self._get_train_ops(features, labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1132 行，在 _get_train_ops 中
return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1103 行，在 _call_model_fn 中
model_fn_results = self._model_fn(features, labels, **kwargs)
文件&quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 180 行，在 _dnn_model_fn
logits=logits)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1004 行，在 create_model_fn_ops
labels = self._transform_labels(mode=mode, labels=labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1033 行，在 _transform_labels
&quot;label_ids&quot;: table.lookup(labels_tensor),
文件 &lt;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lookup/lookup_ops.py&gt;，第 179 行，在查找中
(self._key_dtype, keys.dtype))
TypeError: 签名不匹配。密钥必须是 dtype 

&lt; dtype: &#39;string&#39;&gt;，得到 &lt; dtype: &#39;int64&#39;&gt;

另一方面，如果我将 label_key_values 列设为 numpy.array，则会出现以下错误：
label_keys_values = np.array([&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;], dtype=&#39;string&#39;)

回溯（最近一次调用最后一次）：
文件 &quot;embedding_model_probe.py&quot;，第 116 行，位于 &lt;module&gt;
label_keys=label_keys_values)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 337 行，位于 __init__
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 331 行，位于 multi_class_head
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 986 行，位于 __init__
如果 label_keys 和 len(label_keys) != n_classes:
ValueError:具有多个元素的数组的真值不明确。请使用 a.any() 或 a.all()
]]></description>
      <guid>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</guid>
      <pubDate>Sat, 27 May 2017 16:16:58 GMT</pubDate>
    </item>
    </channel>
</rss>