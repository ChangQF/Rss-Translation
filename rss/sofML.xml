<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Dec 2024 01:15:33 GMT</lastBuildDate>
    <item>
      <title>使用 YOLO 将无界输入导出到 mlpackage/mlmodel 文件</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[我想创建一个 .mlpackage 或 .mlmodel 文件，可以将其导入 Xcode 进行图像分割。为此，我想使用 YOLO 中的分割包来检查它是否符合我的需求。
现在的问题是，此脚本创建的 .mlpackage 文件仅接受固定大小（640x640）的图像：
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

我想在这里进行一些更改，可能使用 coremltools，以处理无界范围（我想处理任意大小的图像）。这里有一些描述：https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges，但我不明白如何用我的脚本实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>比较图像并返回相似度百分比（针对徽标）[关闭]</title>
      <link>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</link>
      <description><![CDATA[假设我有 2 张图片


这有相同的徽标，因此结果应该超过 90%
这里是另外 2 个徽标在此处输入图片说明
现在我们又有 2 张照片了


这也是相同的图片，所以结果一定是正面的。
我遇到的问题是，在交换图片并比较“奥迪”和“奥运会”的标志时，尽管图像完全不同，相似度得分却超过 75%。我尝试过边缘检测等方法来解决这种差异，但这些方法都被证明是无效的。您能建议一种合适的方法来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</guid>
      <pubDate>Tue, 24 Dec 2024 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>同一样本的预测在训练和测试中有所不同</title>
      <link>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</guid>
      <pubDate>Mon, 23 Dec 2024 16:37:41 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 值在二元分类中被反转或解释不正确 [关闭]</title>
      <link>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</link>
      <description><![CDATA[我正在做一个项目研究，预测自行车骑行者和比赛数据集的 top_20 标签，当自行车骑行者进入前 20 名时为 1，否则为 0。数据集严重不平衡（92000 个 1 类实例和大约 400000 个 0 类实例）。我运行一个具有简单架构的基本神经网络。我也在使用类权重。我的 NN 表现不佳，但这不是重点，因为我从项目目的上知道数据不是很好。f1 分数对于 0 类（多数）来说很好，约为 0.87。对于 1 类（少数），约为 0.55。这很好，但是当尝试使用 SHAP 解释结果时会出现问题。
使用 SHAP，我为所有实例设置了基值 1（有时为 0.98）。摘要图似乎合理，但力和瀑布图不合理。如果模型过于自信地预测类 0，基值如何导致 1？ 我还认为标签在 shap 值中是反转的，实际上预期值 1 反映了类 0。但我对我的 shap 值对象的结构感到困惑，因为我无法选择一个类来分析，但它被实例划分。例如 shap_value[1] 指的是实例编号 1，而不是类 1。等等。代码如下：
def build_model(optimizer=&#39;adam&#39;, dropout_rate=0.5, num_units_1=128, num_units_2=64):
model = Sequential([
Dense(num_units_1,activation=&#39;relu&#39;,input_shape=(X_train.shape[1],)), # 输入层
BatchNormalization(),
Dropout(dropout_rate), # Dropout 层
Dense(num_units_2,activation=&#39;relu&#39;,kernel_regularizer=l2(0.01)), # 隐藏层
#Dropout(dropout_rate), # Dropout 层
Dense(1,activation=&#39;sigmoid&#39;) # 用于二分类的输出层
])

# 用于收敛
lr_schedule = ExponentialDecay(
initial_learning_rate=0.001,
decay_steps=10000,
decay_rate=0.9
)

# 编译模型
optimizer_instance = {
&#39;adam&#39;: Adam(learning_rate=lr_schedule),
&#39;rmsprop&#39;: RMSprop(learning_rate=lr_schedule),
&#39;sgd&#39;: SGD(learning_rate=lr_schedule)
}[optimizer]

model.compile(optimizer=optimizer_instance,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
return model

对于 SHAP：
background = X_train_df.sample(200) 
test_sample = X_test_df.sample(100) 

# 解释器
explainer = shap.Explainer(best_model.predict,背景)
shap_values = explainer(test_sample)

# 可视化每个预测的 SHAP 值
shap.summary_plot(shap_values, test_sample)

该图突出显示了在我看来合理的内容：较低的 delta 值可以反映出骑车人接近第一个位置的事实（delta 是相对于第一个位置的时间差），因此它推到 1
但例如瀑布/力图对我来说似乎完全不一致：
最后，我的 shap 值的结构如下：
shap_values[2]
.values =
array([-5.55111512e-17, 1.00000000e-02, 1.73472348e-17, 3.12250226e-17,
2.42861287e-17, -3.46944695e-18, 1.04083409e-17, 0.00000000e+00])

.base_values =
0.99

.data =
array([1.20000000e+02, 1.57000000e+05, 1.55100000e+03, 3.28000000e+02,
1.71790754e-02, 3.83373426e-06, 2.40317094e+01, 1.54471545e-01])
]]></description>
      <guid>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</guid>
      <pubDate>Mon, 23 Dec 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>训练 transformer 模型时出现 OOM 错误</title>
      <link>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</link>
      <description><![CDATA[我正在研究 HMER（手写数学表达式识别）问题，并尝试使用 CNN-Transformer 架构。但是，当我尝试训练我的模型时，我遇到了此错误：
DefaultCPUAllocator：内存不足：您尝试分配 69271363584 字节。
我认为我的位置编码存在一些问题，但我真的不知道如何调试它或这里真正的问题是什么（我在这方面还只是初学者）
我当前的模型如下所示
class PositionalEncoding(nn.Module):
def __init__(self, d_model, max_len=5000):
super(PositionalEncoding, self).__init__()
self.encoding = torch.zeros(max_len, d_model)
position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
self.encoding[:, 0::2] = torch.sin(position * div_term)
self.encoding[:, 1::2] = torch.cos(position * div_term)
self.encoding = self.encoding.unsqueeze(0) # 添加批次维度

def forward(self, x):
seq_len = x.size(1)
return x + self.encoding[:, :seq_len, :].to(x.device)

class TransformerDecoder(nn.Module):
def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_length):
super(TransformerDecoder, self).__init__()
self.embedding = nn.Embedding(vocab_size, d_model)
self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
decrypt_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads)
self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
self.fc_out = nn.Linear(d_model, vocab_size)

def forward(self, target_seqs, memory, target_mask):
# 嵌入目标序列
embedded = self.embedding(target_seqs) # (batch, seq_len, d_model)
embedded = checkpoint(self.positional_encoding, embedded) # 添加位置编码
# 转置以兼容 nn.TransformerDecoder (seq_len, batch, d_model)
embedded = embedded.permute(1, 0, 2)
memory = memory.permute(1, 0, 2)
# 解码
coded = checkpoint(self.transformer_decoder, embedded, memory, target_mask) # (seq_len, batch, d_model)
# 转置回 (batch, seq_len, d_model)
coded = decrypted.permute(1, 0, 2)
# 最终输出层
logits = checkpoint(self.fc_out,coded) # (batch, seq_len, vocab_size)
return logits


我的代码有什么问题吗？如果没有，是什么原因导致我遇到这个问题的]]></description>
      <guid>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</guid>
      <pubDate>Mon, 23 Dec 2024 09:44:33 GMT</pubDate>
    </item>
    <item>
      <title>并集和交集的 VC 维数的上限[关闭]</title>
      <link>https://stackoverflow.com/questions/79302585/upper-bound-on-vc-dimension-of-union-and-intersection</link>
      <description><![CDATA[问题：类 C 的 VC 维度为 d。类 C’ 包括由 C 中 s 个对象的交集和并集（以任何顺序）形成的所有对象。给出 C’ 的 VC 维度的上限。
尝试
我知道，如果 H 是大小为 s 且 VC 维度为 d 的假设类。以下为真：
如果 H&#39; 是由来自 H 的 s 个假设的所有并集形成的类，并且 s ≥ 1。则 VC 维度 (𝐻&#39;) ≤ $2𝑑𝑠log{_2}⁡(3𝑠)$。
如果 H&#39; 是由所有交集形成的类，则同样为真。
从这里开始是我的尝试：
为了形成 C’，我们允许交集和并集的嵌套组合。这意味着 $⋃{{i=1}}⋂{{j=1}}c{_{ij}}$，其中 c∈C。
如果 𝐶${_⋃}$ 和 𝐶${_⋂}$ 是两个假设类，则它们在并集或交集中的组合类的 VC 维数最多是它们的 VC 维数之和。
每个额外的并集或交集层最多对子集的组合增长贡献 $log{_2}⁡(3𝑠)$。因此，C’ 的 VC 维数受以下限制：VC(C’) ≤ $4𝑑𝑠log{_2}⁡(3𝑠)$。
我很想听听你对这是否正确的看法。]]></description>
      <guid>https://stackoverflow.com/questions/79302585/upper-bound-on-vc-dimension-of-union-and-intersection</guid>
      <pubDate>Mon, 23 Dec 2024 08:40:39 GMT</pubDate>
    </item>
    <item>
      <title>二进制交叉熵的实现给出不正常的结果</title>
      <link>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</link>
      <description><![CDATA[我正在尝试构建一个 NN，但在训练阶段，我得到的损失函数值就异常了。这是为什么呢？
哦，我只能使用 NumPy。
这就是数据看起来相似的方式：
print(X_train.shape) #(784,800)
print(X_test.shape) #(784,200)
print(Y_train.shape) #(800,1)
print(Y_test.shape) #(200,1)

损失函数 - BCE
def log_loss(y_hat, y):
m = y.shape[0]
epsilon = 1e-15 
y_hat = np.clip(y_hat, epsilon, 1 - epsilon) 

# loss = -1/m * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))
损失 = -1/m * (np.dot(y.T,np.log(y_hat)) + np.dot((1-y).T, np.log(1-y_hat)))
回报损失

训练阶段 - 前向传播 + 反向传播
input_layer = X_train.shape[0]
hidden_​​layer = 128
learning_rate = 0.01
epochs = 10

W1 = np.random.randn(hidden_​​layer, input_layer)
b1 = np.zeros((hidden_​​layer, 1))
W2 = np.random.randn(1, hidden_​​layer)
b2 = np.zeros((1, 1))

X = X_train
Y = Y_train
loss_list = []
epoch_list = []
num_of_examples = X.shape[1]

for i in range(epochs):
avg_epoch_loss = 0
for j in range(num_of_examples):

Z1 = np.matmul(W1,X[:,j].reshape(-1,1)) + b1 # 不要忘记添加偏差
A1 = sigmoid(Z1)
Z2 = np.matmul(W2,A1) + b2
A2 = sigmoid(Z2)
Yout = Y[j]

loss = log_loss( A2, Yout)
avg_epoch_loss = avg_epoch_loss + loss

dZ2 = (A2-Yout)
dW2 = np.matmul(dZ2,A1.T)
db2 = dZ2

dA1 = np.matmul(W2.T,dZ2)
dZ1 = dA1 * A1 * (1 - A1)
dW1 = np.matmul(dZ1,X[:,j].reshape(-1,1).T)
db1 = dZ1

W2 = W2 - 学习率 * dW2
b2 = b2 - 学习率 * db2
W1 = W1 - 学习率 * dW1
b1 = b1 - 学习率 * db1

avg_epoch_loss = avg_epoch_loss/num_of_examples
loss_list.append(avg_epoch_loss)
epoch_list.append(i)
print(&quot;Epoch&quot;, i,&quot;损失：&quot;，avg_epoch_loss)

这些是不正常的值 - 当然是寻找 0-1 之间的值：
Epoch 0 损失：[-18.37821485]
Epoch 1 损失：[-18.82406892]
Epoch 2 损失：[-18.82406892]
Epoch 3 损失：[-19.99316345]
Epoch 4 损失：[-29.91647793]
Epoch 5 损失：[-32.32075724]
Epoch 6 损失：[-32.89639034]
Epoch 7 损失：[-32.34691639]
Epoch 8 损失： [-32.749394]
第 9 阶段损失：[-33.61631871]
]]></description>
      <guid>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:53 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在第 0 步记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？类似于 eval_on_start 但 train_on_start？
交叉：

discuss.huggingface
github/huggingface
]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>节点特征对节点分类的 GNN 的影响</title>
      <link>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</link>
      <description><![CDATA[我正在探索各种节点特征对图神经网络 (GNN) 节点分类任务性能的影响。我遇到过 PageRank、HITS 和 基于社区的属性 等特征，它们似乎通过提供额外的上下文信息来提高分类准确性。
我很想听听您对以下问题的看法：

您如何将 PageRank 或 HITS 等特征集成到您的 GNN 模型中，您观察到它们对节点分类性能有何影响？
您是否推荐使用特定方法或框架来有效地将基于社区的特征整合到 GNN 中？
您是否遇到过任何提供关于此主题的见解或案例研究的研究论文或资源？
]]></description>
      <guid>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</guid>
      <pubDate>Wed, 16 Oct 2024 14:40:25 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络进行分类</title>
      <link>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</link>
      <description><![CDATA[我正在使用 GNN 开展欺诈检测项目。我的图表以银行代码（SWIFT BIC 代码）作为节点，边表示交易。
以下是我的张量的形状：

节点特征张量形状：torch.Size([210, 6])
边缘特征张量形状：torch.Size([200, 4])
邻接矩阵张量形状：torch.Size([210, 210])
标签张量形状：torch.Size([200, 1])

我尝试了很多次，但目前正在遵循本教程：https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html
以下是我的 GNN 代码：
class GCNLayer(nn.Module):

def __init__(self, c_in, c_out):
super().__init__()
self.projection = nn.Linear(c_in, c_out)

def forward(self, node_feats, adj_matrix):
# Num neighbours = 传入边的数量
num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
node_feats = self.projection(node_feats)
print(&quot;node_feats &quot;,node_feats)
node_feats = torch.bmm(adj_matrix, node_feats)
node_feats = node_feats / num_neighbours
返回node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

使用 torch.no_grad():
out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;邻接矩阵&quot;, adjacency_matrix_tensor)
print(&quot;输入特征&quot;, node_features_tensor)
print(&quot;输出特征&quot;, out_feats)

但无论我怎么尝试，乘法过程中总是出现维度错误：“RuntimeError：mat1 和 mat2 形状无法相乘（210x6 和 2x2）”。
我知道我们正在尝试将 node_Features_tensor (210,6) 乘以 adjacency_matrix_tensor (210,210)，但我已经卡在这个问题上好几天了！
我尝试了 GNN/GCN 的多种实现。我希望能够训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</guid>
      <pubDate>Tue, 12 Mar 2024 09:08:05 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在仅具有边缘特征的图上使用 GNN 吗？</title>
      <link>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</link>
      <description><![CDATA[我正在尝试使用 GNN 对系统发育数据进行分类（完全二分、单向树）。我将 R 中的系统发育树格式转换为 PyTorch 数据集。以其中一棵树为例：

Data(x=[83, 1], edge_index=[2, 82], edge_attr=[82, 1], y=[1], num_nodes=83)

它有 83 个节点（内部节点 + 提示节点，x=[83, 1]），我为所有节点分配了 0，因此每个节点都有一个特征值 0。我构建了一个 82 X 1 矩阵，其中包含节点之间所有有向边的长度（edge_attr=[82, 1]），我打算使用 edge_attr 表示边长度并将其用作权重。每棵树都有一个用于分类的标签（y=[1]，值在 {0, 1, 2} 中）。
如您所见，节点特征在我的例子中并不重要，唯一重要的是边缘特征（边缘长度）。
以下是我用于建模和训练的代码实现：
tree_dataset = TreeData(root=None, data_list=all_graphs)

class GCN(torch.nn.Module):
def __init__(self, hidden_​​size=32):
super(GCN, self).__init__()
self.conv1 = GCNConv(tree_dataset.num_node_features, hidden_​​size)
self.conv2 = GCNConv(hidden_​​size, hidden_​​size)
self.linear = Linear(hidden_​​size, tree_dataset.num_classes)

def forward(self, x, edge_index, edge_attr, batch):
# 1. 获取节点嵌入
x = self.conv1(x, edge_index, edge_attr)
x = x.relu()
x = self.conv2(x, edge_index, edge_attr)

# 2. 读出层
x = global_mean_pool(x, batch) # [batch_size, hidden_​​channels]

# 3. 应用最终分类器
x = F.dropout(x, p=0.5, training=self.training)
x = self.linear(x)

return x

model = GCN(hidden_​​size=32)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
train_loader = DataLoader(tree_dataset, batch_size=64, shuffle=True)
print(model)

def train():
model.train()

lost_all = 0
for data in train_loader:
optimizer.zero_grad() # 清除梯度。
out = model(data.x, data.edge_index, data.edge_attr, data.batch) # 执行一次前向传递。
loss = criterion(out, data.y) # 计算损失。
loss.backward() # 得出梯度。
lost_all += loss.item() * data.num_graphs
optimizer.step() # 根据梯度更新参数。

return lost_all / len(train_loader.dataset)

def test(loader):
model.eval()

correct = 0
for data in loader: # 在训练/测试数据集上分批迭代。
out = model(data.x, data.edge_index, data.edge_attr, data.batch)
pred = out.argmax(dim=1) # 使用概率最高的类。
correct += int((pred == data.y).sum()) # 对照真实标签进行检查。
return correct / len(loader.dataset) # 得出正确预测的比例。

for epoch in range(1, 20):
loss = train()
train_acc = test(train_loader)
# test_acc = test(test_loader)
print(f&#39;Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Loss: {loss:.4f}&#39;)

看来我的代码根本不起作用：
......
Epoch: 015, Train Acc: 0.3333, Loss: 1.0988
Epoch: 016, Train Acc: 0.3333, Loss: 1.0979
Epoch: 017, Train Acc: 0.3333, Loss: 1.0938
Epoch: 018, Train Acc: 0.3333, Loss: 1.1044
Epoch: 019，训练精度：0.3333，损失：1.1012
......
Epoch：199，训练精度：0.3333，损失：1.0965

是不是因为没有有意义的节点特征就不能使用GNN？还是我的实现有问题？]]></description>
      <guid>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</guid>
      <pubDate>Mon, 09 Oct 2023 12:36:47 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/61845701/neural-network-in-python</link>
      <description><![CDATA[我最近开始尝试在不使用任何 NW 模块（如 Tensor Flow）的情况下创建自己的神经网络，但我无法将已定义的变量放入函数中，因此我将数据放入文本文件中，然后对其进行读写。虽然它不允许我将权重重新转换为 int，以便我可以将它们乘以它们的学习率。我收到一条错误消息，提示 int 不适用于基数。

你对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/61845701/neural-network-in-python</guid>
      <pubDate>Sun, 17 May 2020 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中初始化权重？</title>
      <link>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</link>
      <description><![CDATA[如何初始化网络的权重和偏差（例如通过 He 或 Xavier 初始化）？]]></description>
      <guid>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</guid>
      <pubDate>Thu, 22 Mar 2018 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>神经网络（简单）</title>
      <link>https://stackoverflow.com/questions/46079541/neural-network-simple</link>
      <description><![CDATA[我很好奇为什么我没有打印任何输出，因为代码没有错误。
import numpy as np

class NN():
def _init_(self):
# 种子随机数生成器，因此每次程序运行时都会生成相同的数字
# np.random.seed(1)

# 模型单个神经元，具有 3 个输入连接和 1 个输出连接
# 将随机权重分配给 3x1 矩阵，值范围为 -1 到 1
# 平均值为 0
self.synaptic_weights = 2 * np.random.random((3, 1)) - 1

# 描述 s 形曲线我们传递输入的加权和
# 通过此函数将它们标准化为 0 和 1 之间
def __sigmoid(self, x):
return 1 / (1 + np.exp(-x))

# 梯度sigmoid 曲线
def __sigmoid_derivative(self, x):
return x * (1 - x)

def train(self, training_set_input, training_set_output, number_of_training_iterations):
for iteration in np.xrange(number_of_training_iterations):
# 将训练集通过神经网络
output = self.predict(training_set_input)

error = training_set_output - output

# 将误差乘以输入，再乘以 sigmoid 曲线的梯度
adjustment = np.dot(training_set_input.T, error * self.__sigmoid_derivative(output))

# 调整权重
self.synaptic_weights += adjustment

def predict(self, input):
# 将输入通过神经网络（单个神经元）
return self.__sigmoid(np.dot(inputs, self.synaptic_weights))

if __name__ == &quot;__NN__&quot;:
# 初始化单神经元神经网络
nn = NN()
weightz = nn.synaptic_weights
new_predict = nn.predict(np.array[1, 0, 0])

print(&quot;随机起始突触权重&quot;)
print(weightz)

# T 垂直翻转矩阵
training_set_input = np.array([0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1])
training_set_output = np.array([0, 1, 0, 0]).T

# 使用训练集训练网络
# 执行 10,000 次，每次进行小幅调整
nn.train(training_set_input, training_set_output, 10000)

print(&quot;新的起始突触权重&quot;)
print(weightz)

# 测试
print(&quot;预测&quot;)
print(new_predict)

将文件保存为 NN.py]]></description>
      <guid>https://stackoverflow.com/questions/46079541/neural-network-simple</guid>
      <pubDate>Wed, 06 Sep 2017 15:51:06 GMT</pubDate>
    </item>
    </channel>
</rss>