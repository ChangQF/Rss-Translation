<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 02 Dec 2024 12:36:51 GMT</lastBuildDate>
    <item>
      <title>YOLOV11 缓存中未找到标签</title>
      <link>https://stackoverflow.com/questions/79243452/yolov11-no-labels-found-in-cache</link>
      <description><![CDATA[我正在尝试训练一个对坦克进行分类的模型。我有 9 个标签。我正在 Google Collab 上工作：
这是文件夹结构：

现在，我从 Roboflow 获得了这个坦克数据集。我最初加载数据集的方式是使用网站上提供给我的下载代码。我的初始代码如下所示
!mkdir {HOME}/datasets
%cd {HOME}/datasets

from google.colab import userdata
from roboflow import Roboflow

ROBOFLOW_API_KEY = userdata.get(&#39;ROBOFLOW_API_KEY&#39;)
rf = Roboflow(api_key=ROBOFLOW_API_KEY)

workspace = rf.workspace(&quot;liangdianzhong&quot;)
project = rf.workspace(&quot;capstoneproject&quot;).project(&quot;russian-military-annotated&quot;)
version = project.version(4)
dataset = version.download(&quot;yolov11&quot;)

下一行代码显示我初始化训练模型
%cd {HOME}

!yolo task=detect mode=train model=yolo11s.pt data={dataset.location}/data.yaml epochs=1 batch= 60 imgsz=640 plots=True

我的错误在这里抛出：
从预训练权重中转移了 493/499 个项目
TensorBoard：以“tensorboard --logdir runs/detect/train7”开始，在 http://localhost:6006/ 查看
冻结层“model.23.dfl.conv.weight”
AMP：正在运行自动混合精度 (AMP) 检查...
AMP：检查已通过✅
train：正在扫描 /content/datasets/Russian-military-annotated-4/train/labels... 1026 张图片，33 个背景，0 个损坏：100% 1026/1026 [00:00&lt;00:00， 1995.16it/s]
train：已创建新缓存：/content/datasets/Russian-military-annotated-4/train/labels.cache
albumentations：Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method=&#39;weighted_average&#39;), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))
val：正在扫描 /content/datasets/Russian-military-annotated-4/valid/labels... 9 张图片，9 个背景，0 个损坏：100% 9/9 [00:00&lt;00:00， 1585.35it/s]
val：已创建新缓存：/content/datasets/Russian-military-annotated-4/valid/labels.cache
警告⚠️在/content/datasets/Russian-military-annotated-4/valid/labels.cache 中未找到标签，训练可能无法正常工作。请参阅 https://docs.ultralytics.com/datasets 获取数据集格式指南。
将标签绘制到 runs/detect/train7/labels.jpg... 
优化器：发现“optimizer=auto”，忽略“lr0=0.01”和“momentum=0.937”，并自动确定最佳“optimizer”、“lr0”和“momentum”... 
优化器：AdamW(lr=0.000714, motivation=0.9)，参数组为 81 weight(decay=0.0)、88 weight(decay=0.00046875)、87 bias(decay=0.0)
TensorBoard：添加了模型图可视化✅
图像大小 640 train，640 val
使用 2 个数据加载器工作者
将结果记录到 runs/detect/train7
开始训练 1 个时期...

时期 GPU_mem box_loss cls_loss dfl_loss 实例大小
1/1 15.2G 1.125 5.288 1.594 18 640: 100% 18/18 [00:22&lt;00:00, 1.24s/it]
类别图像实例框（P R mAP50 mAP50-95）：100% 1/1 [00:00&lt;00:00, 1.50it/s]
全部 9 0 0 0 0 0
警告 ⚠️ 在检测集中未找到标签，无法在没有标签的情况下计算指标

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79243452/yolov11-no-labels-found-in-cache</guid>
      <pubDate>Mon, 02 Dec 2024 09:03:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 MNIST 训练模型会在 Python 中对自定义图像进行错误分类？</title>
      <link>https://stackoverflow.com/questions/79243340/why-is-my-mnist-trained-model-misclassifying-a-custom-image-in-python</link>
      <description><![CDATA[我使用 MNIST 数据集训练了一个神经网络模型来识别手写数字。该模型在 MNIST 测试集上的准确率达到 97%，但无法正确预测自定义图像文件中的数字。例如，下图包含数字 8，但模型的预测始终不正确。
我在预处理步骤中做错了什么，如何正确准备自定义图像以匹配 MNIST 数据格式？
import cv2
import numpy as np
import os
from keras.api.datasets import mnist
from keras.api.models import Sequential
from keras.api.layers import Dense, Flatten
from keras.api.utils import to_categorical
from PIL import Image

# 加载 MNIST 数据集
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# 将 mnist 数据集从 uint8 转换为 float32，因为大多数深度学习框架都希望输入数据为浮点格式。
train_images = train_images.astype(&#39;float32&#39;) / 255
test_images = test_images.astype(&#39;float32&#39;) / 255

# 添加新的通道维度，得到形状 (num_samples, 28, 28, 1)
train_images = np.expand_dims(train_images, axis=-1)
test_images = np.expand_dims(test_images, axis=-1)

# 对标签进行独热编码
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# 构建模型
model = Sequential([
Flatten(input_shape=(28, 28, 1)),
Dense(128,activation=&#39;relu&#39;),
Dense(10,activation=&#39;softmax&#39;)
])

#编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

# 训练模型
print(&quot;训练模型...&quot;)
model.fit(train_images, train_labels, epochs=5, batch_size=128)

# 评估模型
loss, accuracy = model.evaluate(test_images, test_labels, verbose=0)
print(f&quot;测试准确率：{accuracy * 100:.2f}%&quot;)

# 加载图像进行预测
image_path = &#39;digit.png&#39; # 替换为您的图像路径
print(f&quot;加载并预测 {image_path}...&quot;)

try:
# 以灰度读取图像
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

if image is None:
raise IOError(f&quot;Error loading image at {image_path}&quot;)

# 将图像大小调整为 28x28
image = cv2.resize(image, (28, 28))

# 反转颜色（如果需要）
image = cv2.bitwise_not(image)

# 标准化图像
image_normalized = image.astype(&#39;float32&#39;) / 255

# 转换为可以保存为 PNG 的格式（值 0 到 255）
image_for_saving = (image_normalized * 255).astype(np.uint8)

# 定义保存图像的路径
preprocessed_image_path = &quot;preprocessed_digit.png&quot;

# 确保目录存在（当前目录）
output_directory = os.path.dirname(preprocessed_image_path)
if not os.path.exists(output_directory) and output_directory != &#39;&#39;:
os.makedirs(output_directory)

# 使用 PIL 保存图像
pil_image = Image.fromarray(image_for_saving)
pil_image.save(preprocessed_image_path)
print(f&quot;已将预处理图像保存到 {preprocessed_image_path}&quot;)

# 使用模型预测数字（假设模型已加载）
# 必要时将图像重塑为模型输入格式
image_input = np.expand_dims(image_normalized, axis=0)
image_input = np.expand_dims(image_input, axis=-1)
prediction = np.argmax(model.predict(image_input))
print(&quot;预测数字：&quot;, prediction)

except Exception as e:
print(f&quot;处理图像时出错：{e}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79243340/why-is-my-mnist-trained-model-misclassifying-a-custom-image-in-python</guid>
      <pubDate>Mon, 02 Dec 2024 08:03:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练过程中测试集损失逐渐增加？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79243289/why-does-the-test-set-loss-increase-gradually-during-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79243289/why-does-the-test-set-loss-increase-gradually-during-training</guid>
      <pubDate>Mon, 02 Dec 2024 07:42:20 GMT</pubDate>
    </item>
    <item>
      <title>由 AI ML 提供支持的聊天机器人 [关闭]</title>
      <link>https://stackoverflow.com/questions/79243253/ai-ml-powered-chat-bot</link>
      <description><![CDATA[我想创建一个聊天机器人或 slack bot/app 类型的应用程序，用于解决用户问题。例如，如果用户发布他们遇到的一些构建问题或错误，机器人应该能够读取 slack 频道中以前的聊天记录以查找类似问题，并能够为用户提供一些答案。是否可以开发类似的东西。基本上，我是一名 Java 开发人员，我是 Python 的新手。它可能涉及与 AI/ML 相关的东西，我不知道。有人可以指导我应该学习什么才能开发上述应用程序/设计吗？]]></description>
      <guid>https://stackoverflow.com/questions/79243253/ai-ml-powered-chat-bot</guid>
      <pubDate>Mon, 02 Dec 2024 07:25:49 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle 中使用 Thundersvm</title>
      <link>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</link>
      <description><![CDATA[当我想使用 !pip install thundersvm 在 kaggle 中安装 thundersvm 时，我遇到了此错误：
---------------------------------------------------------------------------
OSError Traceback (most recent call last)
Cell In[6], line 3
1 get_ipython().run_line_magic(&#39;pip&#39;, &#39;install thundersvm&#39;)
2 get_ipython().run_line_magic(&#39;pip&#39;, &#39;install keras_tuner&#39;)
----&gt; 3 from thundersvm import SVC
4 from sklearn.preprocessing import StandardScaler
5 from sklearn.metrics import classes_report

File /opt/conda/lib/python3.10/site-packages/thundersvm/__init__.py:10
3 &quot;&quot;&quot;
4 * 名称 : __init__.py
5 * 作者 : Locke &lt;luojiahuan001@gmail.com&gt;
6 * 版本 : 0.0.1
7 * 说明 :
8 &quot;&quot;&quot;
9 name = &quot;thundersvm&quot;
---&gt; 10 from .thundersvm import *

File /opt/conda/lib/python3.10/site-packages/thundersvm/thundersvm.py:39
36 lib_path = path.join(dirname, shared_library_name)
38 if path.exists(lib_path):
---&gt; 39 thundersvm = CDLL(lib_path)
40 else:
41 # 尝试构建目录
42 if platform == &quot;linux&quot;或平台 == &quot;linux2&quot;:

文件 /opt/conda/lib/python3.10/ctypes/__init__.py:374，在 CDLL.__init__(self, name, mode, handle, use_errno, use_last_error, winmode) 中
371 self._FuncPtr = _FuncPtr
373 如果句柄为 None:
--&gt; 374 self._handle = _dlopen(self._name, mode)
375 else:
376 self._handle = handle

OSError: libcusparse.so.9.0: 无法打开共享对象文件：没有此文件或目录

为了修复此问题，我尝试了以下方法：
# 下载并安装 CUDA 9.0
wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run
sudo sh cuda_9.0.176_384.81_linux-run

此方法也不起作用。我该如何修复此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</guid>
      <pubDate>Mon, 02 Dec 2024 06:08:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中使用 ImageAI 加载自定义训练的 ResNet50 模型？</title>
      <link>https://stackoverflow.com/questions/79242938/how-to-load-a-custom-trained-resnet50-model-with-imageai-in-python</link>
      <description><![CDATA[我正在使用 ImageAI 的自定义训练 ResNet50 模型进行图像分类项目，但遇到了持续的加载错误，导致我无法使用训练好的模型进行预测。该错误表明在尝试将自定义训练的 PyTorch 模型加载到 ImageAI 框架时存在兼容性问题或缺少特定方法。
我尝试使用 ImageAI 的标准模型加载程序加载自定义训练的 ResNet50 模型：
from imageai.Classification import ImageClassification

classifier = ImageClassification()
classifier.setModelPath(&quot;path/to/custom/model.pt&quot;)
classifier.setModelTypeAsResNet50()
classifier.loadModel() # 预计会成功加载模型

我期望的是一个类似于使用预训练模型的简单模型加载过程。相反，我收到了一个令人沮丧的错误：
分类失败：模型尚未加载。执行图像分类之前，您需要调用 &#39;.loadModel()&#39;


我的环境详情：

Python 3.12
ImageAI 3.0.3
PyTorch 2.5.1+cu118
Windows 11

我已确认：

模型路径正确
模型已保存为有效的 .pt 文件
该模型使用 ResNet50 架构在自定义数据集上进行训练

具体问题：

ImageAI 中是否有加载自定义训练模型的特定方法？
自定义模型是否有任何特殊的导出或转换要求？
我需要修改我的模型的状态字典或使用特定的导出格式？
]]></description>
      <guid>https://stackoverflow.com/questions/79242938/how-to-load-a-custom-trained-resnet50-model-with-imageai-in-python</guid>
      <pubDate>Mon, 02 Dec 2024 04:07:18 GMT</pubDate>
    </item>
    <item>
      <title>通过 Streamlit 上的 YFinance 获取数据</title>
      <link>https://stackoverflow.com/questions/79242889/fetching-data-via-yfinance-on-streamlit</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79242889/fetching-data-via-yfinance-on-streamlit</guid>
      <pubDate>Mon, 02 Dec 2024 03:31:17 GMT</pubDate>
    </item>
    <item>
      <title>如何处理图神经网络训练中不同的特征维度？</title>
      <link>https://stackoverflow.com/questions/79242479/how-to-handle-varying-feature-dimensions-in-graph-neural-networks-training</link>
      <description><![CDATA[我有一个问题，在训练图神经网络 (GNN) 时如何处理具有不同特征维度的数据集。例如，在一个数据集（数据集 A）中，节点特征的维度为 4，边特征的维度为 16。在另一个数据集（数据集 B）中，节点特征为 5 维，边特征为 25 维。我可能使用的其他数据集也具有不同的特征大小。
在训练 GNN 时，您通常如何处理这种特征维度的变化？]]></description>
      <guid>https://stackoverflow.com/questions/79242479/how-to-handle-varying-feature-dimensions-in-graph-neural-networks-training</guid>
      <pubDate>Sun, 01 Dec 2024 21:06:57 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 模型在 Google Colab 中占用大量 RAM</title>
      <link>https://stackoverflow.com/questions/79242350/tensorflow-model-takes-so-much-ram-in-google-colab</link>
      <description><![CDATA[当我尝试训练这个模型时，这个模型甚至没有很多可训练的参数，它在 google colab 中崩溃了，因为它使用了所有的内存，我有一个 mp3 文件的数据集，每个 mp3 文件都是 128 kbps 和 3-4 秒长，我尝试制作一个音频分类器，但它不起作用，它需要太多的内存来进行训练，而且训练也很慢。有人能帮帮我吗？我使用 tensorflow 2.10.0 和 tensorflow-io 0.27.0，因为较新的版本存在其他问题。
import tensorflow as tf
import tensorflow_io as tfio
import os
import random
import joblib

BATCH_SIZE = 4
def load_sound(filename):
res = tfio.audio.AudioIOTensor(filename, dtype=tf.float32)
tensor = res.to_tensor()
tensor = tf.math.reduce_sum(tensor,axis = 1) / 2

sample_rate = res.rate

sample_rate = tf.cast(sample_rate, dtype=tf.int64)

wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)

返回 wav

base_dir = &#39;/content/drive/MyDrive/house_sounds/sound_data&#39;
folders = [&#39;door&#39;, &#39;voice&#39;, &#39;glass&#39;, &#39;footsteps&#39;]
files = []

对于文件夹中的文件夹：
folder_path = os.path.join(base_dir, folder)
file_paths = tf.data.Dataset.list_files(os.path.join(folder_path, &#39;*.mp3&#39;))
files.append(file_paths)

door = tf.data.Dataset.zip((files[0], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[0])],0))))
voice = tf.data.Dataset.zip((files[1], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[1])],1))))
glass = tf.data.Dataset.zip((files[2], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[2])],2))))
footsteps = tf.data.Dataset.zip((files[3], tf.data.Dataset.from_tensor_slices(tf.fill([len(files[3])],3))))

data = door.concatenate(voice)
data = data.concatenate(glass)
data = data.concatenate(footsteps)

def create_spectrogram(file_path, label):
wav = load_sound(file_path)
wav = wav[:48000]
zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)
wav = tf.concat([zero_padding, wav], 0)

频谱图 = tf.signal.stft(wav, frame_length=320, frame_step=32)
频谱图 = tf.abs(频谱图)
频谱图 = tf.expand_dims(频谱图, axis=2)
返回频谱图, 标签

数据 = data.map(create_spectrogram)
数据 = data.cache()
数据 = data.shuffle(buffer_size = 1000)
数据 = data.batch(4)
数据 = data.prefetch(8)
print(&#39;Len: &#39;,len(data))
train = data.take(1600)
测试 = data.skip(1600).take(247)

从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Conv2D、Dense、Flatten、MaxPooling2D

model = Sequential([
Conv2D(16, (2,2), 激活=&quot;relu&quot;, input_shape=(1491,257,1)),
MaxPooling2D(pool_size=(5, 5)),
Conv2D(32, (2, 2), 激活=&#39;relu&#39;),
MaxPooling2D(pool_size=(5, 5)),
Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
MaxPooling2D(pool_size=(5, 5)),
Flatten(),
Dense(32, 激活=&#39;relu&#39;),
Dense(4,激活=&quot;softmax&quot;)
])

model.compile(
优化器=&#39;Adam&#39;,
损失=tf.keras.losses.SparseCategoricalCrossentropy(),
指标=[&#39;准确度&#39;]
)
model.summary()
hist = model.fit(训练，epochs=5，validation_data=测试)
]]></description>
      <guid>https://stackoverflow.com/questions/79242350/tensorflow-model-takes-so-much-ram-in-google-colab</guid>
      <pubDate>Sun, 01 Dec 2024 19:35:18 GMT</pubDate>
    </item>
    <item>
      <title>在简单数据集上使用分类器。使用分数时出错</title>
      <link>https://stackoverflow.com/questions/79241877/use-of-classifiers-on-a-simple-dataset-error-using-score</link>
      <description><![CDATA[我是 ML 新手，我正在尝试在一个简单的数据集上使用分类器

labels = [1, 0, 1, 0, 1, 0] 
data = pd.DataFrame({
&#39;Delta&#39;: delta_energy,
&#39;Theta&#39;: theta_energy,
&#39;Alpha&#39;: alpha_energy,
&#39;Beta&#39;: beta_energy,
&#39;Gamma&#39;: gamma_energy,
&#39;Label&#39;: labels
})

但是我在尝试运行此代码后遇到了错误 - AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;split&#39;
clf_index = 5
feature_indexes = [0,3]
names = [
&quot;Nearest Neighbors&quot;,
&quot;Linear SVM&quot;,
&quot;RBF SVM&quot;,
&quot;高斯过程&quot;,
&quot;决策树&quot;,
&quot;随机森林&quot;,
&quot;神经网络&quot;,
&quot;AdaBoost&quot;,
&quot;朴素贝叶斯&quot;,
&quot;QDA&quot;,
]
分类器 = [
KNeighborsClassifier(3),
SVC(kernel=&quot;linear&quot;, C=0.025, random_state=42),
SVC(gamma=2, C=1, random_state=42),
GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),
DecisionTreeClassifier(max_depth=5, random_state=42),
RandomForestClassifier(
max_depth=5, n_estimators=10, max_features=1, random_state=42
),
MLPClassifier(max_iter=1000, random_state=42),
AdaBoostClassifier(algorithm=&quot;SAMME&quot;, random_state=42),
GaussianNB(),
QuadraticDiscriminantAnalysis(),
]
table_data = (data.values[:,feature_indexes], data[data.columns[clf_index]])
datasets = [table_data]
figure = plt.figure(figsize=(27, 3))
i = 1
for ds_cnt, ds in enumerate(datasets):
X, y = ds
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42
)

x_range = abs(X[:, 0].max()) - abs(X[:, 0].min())
y_range = abs(X[:, 1].max()) - abs(X[:, 1].min())

x_min, x_max = X[:, 0].min() - 0.05*x_range, X[:, 0].max() + 0.05*x_range
y_min, y_max = X[:, 1].min() - 0.05*y_range, X[:, 1].max() + 0.05*y_range

# 先绘制数据集，红色和蓝色 = 0000FF 颜色。红色 = 0。蓝色 = 1 状态
cm = plt.cm.RdBu
cm_bright = ListedColormap([&quot;#FF0000&quot;, &quot;#0000FF&quot;])
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
if ds_cnt == 0:
ax.set_title(&quot;Input data&quot;)
# 绘制训练点
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=&quot;k&quot;)
# 绘制测试点
ax.scatter(
X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=&quot;k&quot;
)
ax.set_xlim(x_min, x_max)
ax.set_ylim(y_min, y_max)
ax.set_xlabel(data.columns[feature_indexes[0]])
ax.set_ylabel(data.columns[feature_indexes[1]])
i += 1
for name, clf in zip(names, classifiers):
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
clf = make_pipeline(StandardScaler(), clf)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
DecisionBoundaryDisplay.from_estimator(
clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5
)
# 绘制训练点
ax.scatter(
X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=&quot;k&quot;
)
# 绘制测试点
ax.scatter(
X_test[:, 0],
X_test[:, 1],
c=y_test,
cmap=cm_bright,
edgecolors=&quot;grey&quot;,
alpha=0.4,
)
ax.set_xlim(x_min, x_max)
ax.set_ylim(y_min, y_max)
ax.set_xticks(())
ax.set_yticks(())
if ds_cnt == 0:
ax.set_title(name)
ax.text(
x_max - x_range*0.1,
y_min + y_range*0.1,
(&quot;%.2f&quot; % score).lstrip(&quot;0&quot;),
size=25,
Horizo​​ntalalignment=&quot;right&quot;,
)
i += 1

plt.tight_layout()
plt.show()

在这个特定位置：clf = make_pipeline(StandardScaler(), clf)
clf.fit(X_train, y_train) 我该如何修复它？我尝试更改数据集，但仍然不起作用]]></description>
      <guid>https://stackoverflow.com/questions/79241877/use-of-classifiers-on-a-simple-dataset-error-using-score</guid>
      <pubDate>Sun, 01 Dec 2024 15:20:56 GMT</pubDate>
    </item>
    <item>
      <title>Catboost RAM 在训练结束时崩溃</title>
      <link>https://stackoverflow.com/questions/79241814/catboost-ram-crash-in-the-end-of-training</link>
      <description><![CDATA[我在各种数据集上训练 Catboost 模型，并面临一个重复的问题：一旦最后一棵树被拟合，RAM 消耗就会开始比训练期间增长得更多，这通常会导致 jupyter 内核崩溃。是否有任何类型的最终化作业可以禁用以防止内存溢出？
我尝试传递 used_ram_limit 参数，但在拟合最后一棵树后，它似乎对 RAM 消耗没有影响]]></description>
      <guid>https://stackoverflow.com/questions/79241814/catboost-ram-crash-in-the-end-of-training</guid>
      <pubDate>Sun, 01 Dec 2024 14:46:53 GMT</pubDate>
    </item>
    <item>
      <title>set_transform 或 with_transform 之后 transformer 的数据集结构出现意外</title>
      <link>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</guid>
      <pubDate>Sun, 01 Dec 2024 14:07:14 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 不接受数据集生成器的列表类型</title>
      <link>https://stackoverflow.com/questions/79241634/tensorflow-does-not-accept-list-type-for-dataset-generator</link>
      <description><![CDATA[我正在构建一个神经网络。我无法一次性将所有训练数据加载到内存中，因此我使用 TensorFlow 的 tf.data.Dataset.from_generator 函数逐步加载数据。但是，它会抛出一个错误，指出它不接受张量列表作为类型。
TypeError：`output_signature` 必须包含属于 
`tf.TypeSpec` 子类的对象，但发现 &lt;class &#39;list&#39;&gt; 不是。

我的神经网络的输入是 151 个独立张量的列表。我如何在生成器中表示它？我的代码如下：
def generator(file_paths, batch_size, files_per_batch, tam, value):
return tf.data.Dataset.from_generator(
lambda: data_generator(file_paths, batch_size, files_per_batch, tam, value),
output_signature=(
[tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for _ in range(tam+1)], # 151 个张量列表
tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) # 数组
)
)

inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]

train_dataset = generator(file_paths, batch_size, files_per_batch, tam, False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

model.fit(train_dataset, epochs=1000, validation_split=0.2, verbose=1)

我尝试使用 tf.data.Dataset.from_generator 将数据批量输入到我的神经网络中，因为我无法一次将所有数据加载到内存中。
但是，我遇到了一个错误：
TypeError：output_signature 必须包含属于 tf.TypeSpec 子类的对象，但发现 &lt;class &#39;list&#39;&gt; 不是。
]]></description>
      <guid>https://stackoverflow.com/questions/79241634/tensorflow-does-not-accept-list-type-for-dataset-generator</guid>
      <pubDate>Sun, 01 Dec 2024 13:13:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在 scikit-learn 训练期间显示损失值？</title>
      <link>https://stackoverflow.com/questions/44443479/how-to-show-loss-values-during-training-in-scikit-learn</link>
      <description><![CDATA[我想在训练期间检查我的损失值，这样我就可以观察每次迭代的损失。到目前为止，我还没有找到一种简单的方法让 scikit learn 给我一个损失值的历史记录，也没有找到 scikit 中已有的为我绘制损失的功能。
如果没有办法绘制这个，如果我能在 classifier.fit 的末尾简单地获取最终的损失值就太好了。
注意：我知道有些解决方案是封闭形式的。我正在使用几个没有分析解决方案的分类器，例如逻辑回归和 svm。
有人有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/44443479/how-to-show-loss-values-during-training-in-scikit-learn</guid>
      <pubDate>Thu, 08 Jun 2017 18:50:55 GMT</pubDate>
    </item>
    <item>
      <title>NotFittedError：估计器不适合，在利用模型之前调用“fit”</title>
      <link>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</link>
      <description><![CDATA[我在 Macbook OSX 10.2.1 (Sierra) 上运行 Python 3.5.2。
尝试运行 Kaggle 的 Titanic 数据集的一些代码时，我不断收到以下错误：


NotFittedError Traceback (most recent call
last) in ()
6 
7 # 使用测试集进行预测并打印它们。
----&gt; 8 my_prediction = my_tree_one.predict(test_features)
9 print(my_prediction)
10 
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input) 中
429 &quot;&quot;&quot;
430 
--&gt; 431 X = self._validate_X_predict(X, check_input)
432 proba = self.tree_.predict(X)
433 n_samples = X.shape[0]
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input)
386 “”“每当有人试图预测、应用、predict_proba 时验证 X”””
387 if self.tree_ is None:
--&gt; 388 raise NotFittedError(“估算器未安装，”
389 “在利用模型之前调用 fit。”)
390 
NotFittedError：估算器未安装，在利用
模型之前调用 fit。

有问题的代码似乎是这样的：
# 用中位数估算缺失值
test.Fare[152] = test.Fare.median()

# 从测试集中提取特征：Pclass、Sex、Age 和 Fare。
test_features = test[[&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;]].values

# 使用测试集进行预测并打印。
my_prediction = my_tree_one.predict(test_features)
print(my_prediction)

# 创建一个包含两列的数据框：PassengerId 和 Survived。 Survived 包含您的预测
PassengerId =np.array(test[&quot;PassengerId&quot;]).astype(int)
my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [&quot;Survived&quot;])
print(my_solution)

# 检查您的数据框是否有 418 个条目
print(my_solution.shape)

# 将您的解决方案写入名为 my_solution.csv 的 csv 文件
my_solution.to_csv(&quot;my_solution_one.csv&quot;, index_label = [&quot;PassengerId&quot;])

以下是其余部分的链接 代码。
由于我已经调用了“fit”函数，我无法理解此错误消息。我哪里做错了？感谢您的时间。
编辑：
结果发现该问题继承自上一个代码块。
# 拟合您的第一个决策树：my_tree_one
my_tree_one = tree.DecisionTreeClassifier()
my_tree_one = my_tree_one.fit(features_one, target)

# 查看所包含特征的重要性和分数
print(my_tree_one.feature_importances_)
print(my_tree_one.score(features_one, target))

使用以下行：
my_tree_one = my_tree_one.fit(features_one, target)
生成错误：

ValueError：输入包含 NaN、无穷大或对于
dtype(&#39;float32&#39;)。
]]></description>
      <guid>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</guid>
      <pubDate>Fri, 02 Dec 2016 17:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>