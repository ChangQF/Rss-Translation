<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 03 Nov 2024 21:15:54 GMT</lastBuildDate>
    <item>
      <title>如何评估SRGAN模型</title>
      <link>https://stackoverflow.com/questions/79153606/how-to-evaluate-the-srgan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79153606/how-to-evaluate-the-srgan-model</guid>
      <pubDate>Sun, 03 Nov 2024 20:34:58 GMT</pubDate>
    </item>
    <item>
      <title>表现出训练不稳定性最小的神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/79152950/smallest-neural-network-exhibiting-training-instability</link>
      <description><![CDATA[我遇到过训练发散（例如，损失函数的发散）导致我使用过的一些深度神经网络训练不稳定的情况。这似乎是深度网络的一个常见特征，即使不是普遍特征，也有很多实用的方法来处理它。
我的问题是：哪一个最小的神经网络已被证明表现出训练发散导致不稳定？我想尝试在最简单的测试用例中了解根本原因。
我浏览了已发表的文献，发现了很多关于这种现象的论文（例如，https://www.sciencedirect.com/science/article/abs/pii/S1568494624001091 和 https://arxiv.org/abs/2110.04369 等），但我不清楚表现出这种现象的最小可能网络是什么。]]></description>
      <guid>https://stackoverflow.com/questions/79152950/smallest-neural-network-exhibiting-training-instability</guid>
      <pubDate>Sun, 03 Nov 2024 14:44:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用函数 shap.Explainer 会根据输入的不同顺序获得不同的 shap 值？</title>
      <link>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</link>
      <description><![CDATA[我训练了一个二分类模型，并想使用 shap.Explainer 来分析特征贡献。
代码如下：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])

shap_values.values 的结果如下：




Feature 1
...




sample 0
-0.009703
...


样本 1
-0.009297
...


样本 2
-0.007699
...


样本 3
0.032624
...



但是当输入顺序改变时：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])

样本 0 和样本 1 的结果已更改：




特征 1
...




样本 1
-0.010012
...


样本0
-0.008277
...


样本 2
-0.007699
...


样本 3
0.032624
...



我不知道有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</guid>
      <pubDate>Sun, 03 Nov 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 ml 的人脸识别项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</link>
      <description><![CDATA[做什么 代码显示错误
此代码使用 OpenCV 和 LBPH 算法实现了人脸识别系统。它由三个主要组件组成。
数据收集（生成数据集）：
初始化 harr Cascade 分类器以检测人脸。
定义辅助函数 face cropped，将帧转换为灰度，检测人脸并裁剪它们。
从网络摄像头捕获视频并连续读取帧。
如果检测到人脸，它会调整大小并将裁剪的人脸转换为灰度，使用唯一 ID 保存它，然后显示图像。
在 200 张图像后或按下 Enter 键时，该过程停止。
训练分类器（训练分类器）：
从指定目录读取图像并准备进行训练。
将图像转换为灰度并提取其 ID。
初始化 LBPH 人脸识别器，使用人脸数据对其进行训练，并将模型保存为 classifier.xml。
实时人脸识别：
从网络摄像头捕获视频帧并使用 haar Cascade 检测人脸。
调用绘制边界函数在检测到的人脸周围绘制矩形，并使用训练有素的分类器预测其身份。
根据置信度显示识别的名称或“未知”，实现实时人脸检测和识别。]]></description>
      <guid>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</guid>
      <pubDate>Sun, 03 Nov 2024 07:21:47 GMT</pubDate>
    </item>
    <item>
      <title>LCD 7 段数字无法正确识别</title>
      <link>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly</link>
      <description><![CDATA[我选择读取我的热系统 LCD 7 段显示屏作为学习 CNN 的首要任务。
我能够正确读取大多数数字，但数字 6 大多数情况下被检测为 5。
有人可以建议我使用 MNIST 数据集执行该任务的方法是否正确，我应该找到更好的超参数以使其按预期工作吗？
这是我的代码，包含更多上下文：https://github.com/tkdcpl/cnn-lcd-digits]]></description>
      <guid>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly</guid>
      <pubDate>Sat, 02 Nov 2024 18:48:18 GMT</pubDate>
    </item>
    <item>
      <title>ImageDataGenerator 在预处理函数中发送图像数组，而不是文件路径</title>
      <link>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</guid>
      <pubDate>Sat, 02 Nov 2024 16:29:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么在Apple gpu上训练的模型比在Apple cpu（M2）上训练的性能更差？</title>
      <link>https://stackoverflow.com/questions/79150206/why-does-the-model-trained-on-apple-gpu-performs-worse-than-when-it-is-trained-o</link>
      <description><![CDATA[在苹果CPU和苹果GPU上训练了一个简单的CNN模型，并在测试数据集上评估了两个模型的性能，在苹果CPU上训练的模型准确率为98%，而在GPU上训练的模型准确率为10%。
两个模型是一样的，下面是用于训练和测试模型的代码。使用 PyTorch 创建模型。
CPU
for epoch in trange(3): 
for images, labels in tqdm(train_loader):

optimizer.zero_grad()

x = images 
y = model(x)
loss = criterion(y, labels)

loss.backward()
optimizer.step() #更新权重

GPU
for epoch in trange(3): 
for images, labels in tqdm(train_loader):
images, labels = images.to(device).float(), labels.to(device).float() 
optimizer.zero_grad()

x = images 
y = model_gpu(x).to(device)
loss = criterion(y, labels)

loss.backward()
optimizer.step()

模型测试
GPU
m = model_gpu.to(&quot;cpu&quot;) #将模型传输到 cpu

correct = 0
total = len(mnist_test)

with torch.no_grad():
for images, labels in tqdm(test_loader):
images, labels = images, labels

x = images 
y = m(x)

predictions = torch.argmax(y, dim=1)
correct += torch.sum((predictions == labels).float())

print(&#39;测试准确率：{}&#39;.format(correct/total)) 

也尝试在 GPU 上测试，但结果相同。
CPU
correct = 0
total = len(mnist_test)

使用 torch.no_grad():
for images, labels in tqdm(test_loader):
# 前向传递
x = images 
y = model(x)

predictions = torch.argmax(y, dim=1)
correct += torch.sum((predictions == labels).float())

print(f&#39;测试准确率：&#39;,{correct/total})
]]></description>
      <guid>https://stackoverflow.com/questions/79150206/why-does-the-model-trained-on-apple-gpu-performs-worse-than-when-it-is-trained-o</guid>
      <pubDate>Sat, 02 Nov 2024 07:59:33 GMT</pubDate>
    </item>
    <item>
      <title>尝试构建步骤但遇到一些问题 [重复]</title>
      <link>https://stackoverflow.com/questions/79149862/trying-to-build-the-step-but-meet-some-problems</link>
      <description><![CDATA[我正在尝试构建简单的管道：
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
import torch
from torch.utils.data import DataLoader, TensorDataset

class CGANDataAugmenter(BaseEstimator, TransformerMixin):
def __init__(self, device, opt):
self.generator = Generator(opt).to(device)
self.discriminator = Discriminator(opt).to(device)
self.device = device
self.opt = opt
self.n_samples=opt.n_samples
self.sampler=None

def fit_transform(self, X, y):
feature_name = X.columns
label_name = y.name
self.sampler = train_CGAN(X,y,self.generator,self.discriminator,self.device,self.opt)
generated_data = sample(self.sampler,self.n_samples, feature_name, label_name,self.opt)
original_data=pd.concat([X,y],axis=1)

Combine_data=pd.concat([original_data,generated_data],axis=0)

X_combined = combined_data.drop(columns=[label_name])
Y_combined = combined_data[label_name]

return X_combined, Y_combined

来自 imblearn.pipeline 导入 Pipeline
来自 sklearn.tree 导入 DecisionTreeClassifier

steps = [
(&#39;sampler&#39;, CGANDataAugmenter(device,opt)),
(&#39;model&#39;,DecisionTreeClassifier())
]

我得到了错误：

TypeError：Pipeline 的最后一步应该实现 fit 或为字符串“passthrough”。&#39;CGANDataAugmenter(device=device(type=&#39;cpu&#39;),
opt=Namespace(lr=0.0002, b1=0.5, b2=0.999, num_classes=2, latent_dim=8, n_epochs=100, batch_size=64, n_samples=100, origin_size=1))&#39; (type ) 不存在

哪里出了问题？我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/79149862/trying-to-build-the-step-but-meet-some-problems</guid>
      <pubDate>Sat, 02 Nov 2024 02:46:19 GMT</pubDate>
    </item>
    <item>
      <title>实时 resnet 预测</title>
      <link>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</link>
      <description><![CDATA[我训练了一个 resnet50 模型，使用 0 到 5 的数字手势，并尝试通过笔记本电脑的网络摄像头部署它来预测实时类别。
虽然该模型的准确率为 98%，而且我很确定错误不是因为模型训练不当而发生的，但实时值被困在 5 个类别中的 1 个或 2 个类别中，它们总是预测数字 0 和数字 2。
这是代码：
import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image # 导入 PIL 进行图像转换

# 定义模型架构并加载权重
class ResNet50Modified(nn.Module):
def __init__(self, num_classes=6):
super(ResNet50Modified, self).__init__()
self.model = models.resnet50(pretrained=True) # 使用 pretrained=True 以获得更好的性能
self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

def forward(self, X):
return self.model(X)

# 加载训练好的模型
model = ResNet50Modified(num_classes=6)
# 为 CPU 加载模型的 state_dict
model.load_state_dict(torch.load(&quot;resnet50_modified1.pth&quot;, map_location=torch.device(&#39;cpu&#39;)))
model.eval()

# 定义转换以匹配训练预处理
preprocess = transforms.Compose([
transforms.Resize((64, 64)), # 调整为模型的输入大小
transforms.ToTensor(), # 转换为张量
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 根据 ResNet 标准进行标准化
])

# 标志的标签
class_names = [&#39;Class_0&#39;, &#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;] # 替换为实际标志名称

# 打开网络摄像头进行实时预测
cap = cv2.VideoCapture(0)

if not cap.isOpened():
print(&quot;Error: Could not open webcam.&quot;)
exit()

while True:
ret, frame = cap.read()
if not ret:
print(&quot;Error: Could not read frame.&quot;)
break

# 将帧从 BGR（OpenCV）转换为RGB (PIL)
frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

# 将 NumPy 数组转换为 PIL 图像
pil_image = Image.fromarray(frame_rgb)

# 预处理帧
input_image = preprocess(pil_image) # 使用 PIL 图像进行预处理
input_image = input_image.unsqueeze(0) # 添加批次维度

# 使用模型进行预测
with torch.no_grad():
output = model(input_image)

# 应用 softmax 获取概率
probabilities = torch.softmax(outputs, dim=1)

# 获取预测的类别和置信度
_, predict = torch.max(probabilities, 1)
confidence = probabilities[0][predicted].item() * 100 # 转换为百分比
label = class_names[predicted.item()]

# 显示结果和置信度
cv2.putText(frame, f&quot;Predicted: {label}, Confidence: {confidence:.2f}%&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
cv2.imshow(&quot;Sign Detection&quot;, frame)

# 按“q”退出
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

cap.release()
cv2.destroyAllWindows()

置信度始终很高，但由于标签错误，即使我在网络摄像头上显示 5 个手指，它仍然停留在零。
我觉得问题出在帧处理上，有人对此有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</guid>
      <pubDate>Fri, 01 Nov 2024 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>需要从图像中分别分割出每个数字</title>
      <link>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</link>
      <description><![CDATA[我使用 MNIST 数据集创建了一个 CNN 模型。我想对图像中存在的数字序列进行预测。该技术涉及分割每张图像并将其输入到模型中，但我在从图像中分割数字时遇到了困难，因为存在两种不同类型的图像。我需要一种强大的技术来消除图像中存在的所有噪音和阴影并分别分割每个数字。
我也在这里分享这些图片。
我正在寻找强大的技术和代码。


我尝试了此代码和技术，但它对附加的图像不起作用
def fragment_and_display_digits(image_path):# Read imageimg = cv2.imread(image_path)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 获取图像尺寸
height, width = gray.shape
total_area = height * width

# 应用自适应阈值
thresh = cv2.adaptiveThreshold(
gray,
255,
cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
cv2.THRESH_BINARY_INV,
21, # 块大小
10 # C 常数
)

# 查找轮廓
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 根据面积过滤轮廓
valid_contours = []
min_area = total_area * 0.001 # 图像面积的最小 0.1%
max_area = total_area * 0.5 # 图像面积的最大 50%

for cnt in contours:
area = cv2.contourArea(cnt)
if min_area &lt; area &lt; max_area:
x, y, w, h = cv2.boundingRect(cnt)
aspects_ratio = w / float(h)
# 检查数字的纵横比是否合理（不要太宽或太高）
if 0.2 &lt; aspects_ratio &lt; 2：
valid_contours.append(cnt)

# 从左到右对轮廓进行排序
valid_contours = sorted(valid_contours, key=lambda x: cv2.boundingRect(x)[0])

# 提取并显示数字
digits = []
padding = int(min(height, width) * 0.02) # 根据图像大小进行自适应填充

for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
# 添加填充，同时保持在图像范围内
x1 = max(0, x - padding)
y1 = max(0, y - padding)
x2 = min(width, x + w + padding)
y2 = min(height, y + h + padding)
digit = img[y1:y2, x1:x2]
digits.append(digit)

# 显示结果
if digits:
# 创建带有检测到的数字的原始图像的可视化
img_with_boxes = img.copy()
for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
cv2.rectangle(img_with_boxes, (x, y), (x+w, y+h), (0, 255, 0), 2)

# 绘制带有方框和分割数字的原始图像
plt.figure(figsize=(15, 5))

# 带有方框的原始图像
plt.subplot(2, 1, 1)
plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))
plt.title(&#39;Detected Digits&#39;)
plt.axis(&#39;off&#39;)

#分割的数字
plt.subplot(2, 1, 2)
for i, digit in enumerate(digits):
plt.subplot(2, len(digits), len(digits) + i + 1)
plt.imshow(cv2.cvtColor(digit, cv2.COLOR_BGR2RGB))
plt.axis(&#39;off&#39;)
plt.title(f&#39;Digit {i+1}&#39;)

plt.tight_layout()
plt.show()
else:
print(&quot;图像中未找到数字&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</guid>
      <pubDate>Fri, 01 Nov 2024 06:27:46 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据帧进行分组以获取代表更大集合的全部范围的子集</title>
      <link>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</link>
      <description><![CDATA[这是我拥有的一组数据框的两个示例：



天
p1
p2
p3




4
2.1
3.4
4.5


15
2.2
3.6
2.8


39
2.5
2.1
0.4



和这个：



天
p1
p2
p3




4
2.1
3.4
4.5


18
8.2&lt; /td&gt;
2.2
5.8


22
6.4
3.6
1.4


29
2.4
4.1
2.3



我有大约 100 万个这样的数据框（列相同，长度不同），我想输出大约 50000 个子集，以公平地代表所有存在的不同数据框。基本上，数据框应该是一个有效的表示，因此在完整的 100 万或 50k 子集上训练 ML 模型应该会给 ML 模型带来几乎相同的行为。
天数很重要，因为 2 个具有相同参数 (p) 值但天数列差异很大的数据框是不相等的
我的方法是通过每个级别的变量将数据框分组在一起。然后从底层的每个组中取出 1 个数据框。
组级别 1 (GL1)：按行数对数据框进行分组。
组级别 2 (GL2)：对于 GL1 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 对具有相似天数列的数据框进行分组
组级别 3 (GL3)：对于 GL2 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 将具有相似参数值的数据框分组在一起
从每个 GL3 组中取出 1 个数据框来表示该组数据框。
它可能无法获得每个参数的完整最大值和最小值，但这种方法似乎相当全面。这是一个好主意还是您有更好的想法？]]></description>
      <guid>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</guid>
      <pubDate>Thu, 31 Oct 2024 16:36:13 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能波动</title>
      <link>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</link>
      <description><![CDATA[我使用 DDQN 和经验重放，就像本教程中一样 https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
除了我通过模糊 x_dot 和 theta_t（推车速度和杆的角速度）使问题变得更难一些。然后，我根据当前状态计算前一个 x_dot、theta_dot、x_dot_dot 和 theta_dot_dot，并使用此状态空间进行学习过程：(x, prev_x_dot, prev_prev_x_dot_dot, theta, prev_theta_dot, prev_prev_theta_dot_dot)。
无论如何，我的主要问题是，通过使用上面链接的教程中描述的 DQN 算法，算法不会收敛。如果最后 100 集的平均长度 &gt; 450，我认为学习成功。执行时，我可能会看到 50-60 个连续的 500 集长集，但随后集长随机波动并下降到甚至 20!?!? 我需要从某个范围内（对于每个 x、theta）的任意起始位置开始，进一步推动问题的发展，但到目前为止，结果并不乐观。
对于像 DQN 这样的算法来说，这是一种正常行为吗？我知道，作为基于先前执行计算的策略，损失函数可能存在一些收敛问题，但这是否包括性能的严重波动？
我使用的网络有 3 个非线性层，线性层尺寸为 256x256。]]></description>
      <guid>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</guid>
      <pubDate>Wed, 30 Oct 2024 14:36:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML.NET 中使用 CenterFace？模型预期形状为 (10, 3, 32, 32)</title>
      <link>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</link>
      <description><![CDATA[我尝试在 ML.NET 中使用 CenterFace ONNX，但一直出现各种错误，主要是关于输入大小的错误。
CenterFace 元数据指出，它应该有一个 10, 3, 32, 32 的输入，这对于图像检测来说已经毫无意义了 - 为算法提供 10 个批次（每个批次 32x32 像素）有什么意义？
这是我的主要代码：
 string modelPath = &quot;centerface.onnx&quot;;
var mlContext = new MLContext();

string imagePath = &quot;photo1.jpg&quot;;

var img = Image.FromFile(imagePath);
var DH = (int)(Math.Ceiling((float)img.Height / 32) * 32);
var DW = (int)(Math.Ceiling((float)img.Width / 32) * 32);

var inputData = new[] { new ModelInput { ImagePath = imagePath } };
IDataView imageData = mlContext.Data.LoadFromEnumerable(inputData);

var pipeline = mlContext.Transforms.LoadImages(outputColumnName: &quot;input.1&quot;, imageFolder: &quot;&quot;, inputColumnName: nameof(ModelInput.ImagePath))
.Append(mlContext.Transforms.ResizeImages(outputColumnName: &quot;input.1&quot;, imageWidth: DW, imageHeight: DH))
.Append(mlContext.Transforms.ExtractPixels(outputColumnName: &quot;input.1&quot;))
.Append(mlContext.Transforms.ApplyOnnxModel(
outputColumnNames: [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;],
inputColumnNames: [&quot;input.1&quot;],
modelFile: modelPath
));

var model = pipeline.Fit(imageData);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput, ModelOutput&gt;(mo​​del);
var prediction = predictionEngine.Predict(new ModelInput { ImagePath = imagePath });

使用我的 2 个模型类：
 public class ModelInput
{
public string ImagePath { get; set; }
}

public class ModelOutput
{
[ColumnName(&quot;537&quot;)] 
public float[] HeatMap { get; set; }

[ColumnName(&quot;538&quot;)]
public float[] Scale { get; set; }

[ColumnName(&quot;539&quot;)]
public float[] Offset { get; set; }

[ColumnName(&quot;540&quot;)]
public float[] Landmarks { get; set; }
}

但我确实一直收到有关输入大小的错误：

System.ArgumentException：“内存长度（3686400）必须与尺寸乘积（30720）匹配。”

30720 显然是 10x3x32x32。但同样，这有什么意义呢？
我认为我的 ONNX 坏了，但我确实有一个使用 OpenCVSharp 的工作实现：
// 这是计算机 DW 和 DH，与 ML.NET 示例中的方式相同
CenterFaceParams p = new(image, resizedSize.Width, resizedSize.Height, scoreThreshold, nmsThreshold);
Size size = new(p.DW, p.DH);

使用 Mat input = new();
Cv2.Resize(image, input, size);

使用 Mat blobInput = CvDnn.BlobFromImage(input, 1.0, size, new Scalar(0, 0, 0), true, false);
_net.SetInput(blobInput, &quot;input.1&quot;);

使用 (Mat heatMap = new())
使用 (Mat scale = new())
使用 (Mat offset = new())
使用 (Mat skylines = new())
{
_net.Forward([heatMap, scale, offset, skylines], [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;]);

CenterFaceDecodercoder = new(heatMap, scale, offset, skylines, p);
returncoder.GetOutput();
}

这个实现给了我所有 4 个层，建模后我得到了我想要的值。]]></description>
      <guid>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</guid>
      <pubDate>Thu, 24 Oct 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>为机器学习模型创建标记图像数据集</title>
      <link>https://stackoverflow.com/questions/52848947/create-labeled-image-dataset-for-machine-learning-models</link>
      <description><![CDATA[我的问题是如何为机器学习创建带标签的图像数据集？
我一直使用已有的数据集，因此我面临着如何标记图像数据集的困难（就像我们在猫与狗分类中所做的那样）。
我必须进行标记以及图像分割，在网上搜索后，我找到了一些手动标记工具，例如LabelMe和LabelBox。LabelMe 很好，但它以 XML 文件的形式返回输出。
现在我再次担心如何将 XML 文件输入神经网络？我根本不擅长图像处理任务，所以我需要一个替代建议。
编辑：我有学位证书和普通文件的扫描件，我必须制作一个分类器，将学位证书分类为 1，将非学位证书分类为 0。所以我的标签将是这样的：
Degree_certificate -&gt; y(1)
Non_degree_cert -&gt; y(0)]]></description>
      <guid>https://stackoverflow.com/questions/52848947/create-labeled-image-dataset-for-machine-learning-models</guid>
      <pubDate>Wed, 17 Oct 2018 06:56:31 GMT</pubDate>
    </item>
    </channel>
</rss>