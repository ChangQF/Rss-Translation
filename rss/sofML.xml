<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 29 Nov 2024 15:18:48 GMT</lastBuildDate>
    <item>
      <title>这些 `[0]` 在创建变量时是否有意义</title>
      <link>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</link>
      <description><![CDATA[使用 HuggingFace 工具集微调 Gemma 的指南位于：https://huggingface.co/blog/gemma-peft
以下行的链接：https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A
数据输入格式化函数是：
def formatting_func(example):
text = f&quot;Quote: {example[&#39;quote&#39;][0]}\nAuthor: {example[&#39;author&#39;][0]}&lt;eos&gt;&quot;
return [text]

这些 [0] 有意义吗？它们看起来不对，因为当打印出 text 变量时，我可以看到它们只是字符而不是字符串。]]></description>
      <guid>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</guid>
      <pubDate>Fri, 29 Nov 2024 10:14:43 GMT</pubDate>
    </item>
    <item>
      <title>运行 BayesSearchCV 查找 ANN 回归的最佳超参数时出错</title>
      <link>https://stackoverflow.com/questions/79236534/error-while-running-bayessearchcv-for-finding-best-hyperparameter-of-ann-regress</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79236534/error-while-running-bayessearchcv-for-finding-best-hyperparameter-of-ann-regress</guid>
      <pubDate>Fri, 29 Nov 2024 09:40:18 GMT</pubDate>
    </item>
    <item>
      <title>发生推理时间延迟</title>
      <link>https://stackoverflow.com/questions/79236337/inference-time-delay-occured</link>
      <description><![CDATA[我目前正在研究 GFPGAN 模型。我想使用 Python 的多处理功能同时实现多帧处理功能。
当我运行一个进程时，我会遇到大约 0.25 秒的延迟。当我同时运行两个进程时，延迟会增加到大约 0.45 秒。当三个进程同时运行时，延迟会增加到大约 0.65 秒。
我确信并行处理正在运行，但在同时运行多个进程时，我会遇到额外的延迟。
您能否告诉我这是什么原因，以及在使用多个进程时如何减少额外的延迟？
谢谢！
我为每个进程实现了加载模型文件 (*.pth) 和推理函数，并同时调用这些进程。我为每个进程创建了时间戳；因此，推理函数的开始时间相同，但结束时间略有不同。]]></description>
      <guid>https://stackoverflow.com/questions/79236337/inference-time-delay-occured</guid>
      <pubDate>Fri, 29 Nov 2024 08:32:20 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中 BayesianFPN 的张量维度不匹配</title>
      <link>https://stackoverflow.com/questions/79236267/mismatch-in-tensor-dimensions-in-bayesianfpn-with-reinforcement-learning</link>
      <description><![CDATA[我正在实施一个计算机视觉项目。我在这个项目中使用了 FPN（带有 ResNet50 主干）和 BayesianFPN。该网络位于强化学习代理下。实施后，它抛出一个RuntimeError：张量 a（64）的大小必须与非单例维度 3 上的张量 b（256）的大小匹配。

是什么导致代码抛出此错误？
是因为训练图像（RGB 图像）和验证图像（二进制掩码）不匹配吗？
我是否遗漏了一些内部维度更改？

以下是代码。这是我想要实现的 BayesianFPNwithRL 类。
PS：我尝试重塑权重张量。但没有成功。即使将其解压或扩展也无济于事。
供参考：RGB 图像尺寸 - 1280x720px；二进制掩码尺寸：1280x720px

# 具有 RL 的贝叶斯 FPN
class BayesianFPNWithRL(nn.Module):
def __init__(self, backbone_with_fpn, rl_agent, dropout_p=0.2):
super(BayesianFPNWithRL, self).__init__()
self.backbone_with_fpn = backbone_with_fpn
self.dropout = nn.Dropout(p=dropout_p)
self.rl_agent = rl_agent

def forward(self, x, mc_samples=10, train_rl=False):

fpn_outputs = self.backbone_with_fpn(x)
keys = list(fpn_outputs.keys())
features = [fpn_outputs[key] for key in keys]

common_size = features[0].shape[2:] 
features = [F.interpolate(f, size=common_size, mode=&quot;nearest&quot;) for f in features]

if not self.training:
sampled_features = []
for _ in range(mc_samples):
sampled_features.append([self.dropout(f) for f in features])
features = [
torch.mean(torch.stack([sample[i] for sample in sampled_features]), dim=0)
for i in range(len(features))
]

global_features = [f.mean(dim=(2, 3)) for f in features] 
rl_input = torch.cat(global_features, dim=1) 

action, log_prob = self.rl_agent.select_action(rl_input)

weights = torch.zeros(len(features), device=x.device)
weights[action] = 1.0 

selected_features = sum(w * f for w, f in zip(weights, features))

if train_rl:
return selected_features, log_prob
return selected_features

if __name__ == &quot;__main__&quot;:

resnet = resnet50(weights = ResNet50_Weights.DEFAULT)
return_layers = {
&#39;layer1&#39;: &#39;0&#39;,
&#39;layer2&#39;: &#39;1&#39;,
&#39;layer3&#39;: &#39;2&#39;,
&#39;layer4&#39;: &#39;3&#39;
}
in_channels_list = [256, 512, 1024, 2048]
out_channels = 256
backbone_with_fpn = BackboneWithFPN(resnet、return_layers、in_channels_list、out_channels)

rl_agent = RLAgent(input_dim=1280、hidden_​​dim=512、action_space=4)

bayesian_fpn_rl = BayesianFPNWithRL(backbone_with_fpn、rl_agent).to(&#39;cuda&#39;)

optimizer = torch.optim.Adam(bayesian_fpn_rl.parameters()、lr=1e-4)

for epoch in range(10): 
for images, ground_truth_masks in dataloader:
images, ground_truth_masks = images.to(&#39;cuda&#39;), ground_truth_masks.to(&#39;cuda&#39;)

model_output, log_prob = bayesian_fpn_rl(images, train_rl=True)

predicted_mask = (model_output &gt; 0.5).int()

奖励 = compute_reward(predicted_mask, ground_truth_masks)

损失 = rl_loss(log_prob, 奖励)

optimizer.zero_grad()
损失.backward()
optimizer.step()

print(f&quot;Epoch [{epoch + 1}], 损失: {loss.item():.4f}, 奖励: {reward:.4f}&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/79236267/mismatch-in-tensor-dimensions-in-bayesianfpn-with-reinforcement-learning</guid>
      <pubDate>Fri, 29 Nov 2024 08:06:22 GMT</pubDate>
    </item>
    <item>
      <title>“TypeError：类型为‘numpy.float32’的对象没有 len()” - DeepSORT 与 YOLO 集成</title>
      <link>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</link>
      <description><![CDATA[我正在将 YOLOv8 与 DeepSORT 集成以进行多对象跟踪，但在将检测数据传递给 DeepSORT update_tracks() 函数时遇到了 TypeError。
错误消息：

速度：4.5ms 预处理，332.2ms 推理，0.6ms 后处理每个形状为 (1, 3, 480, 640) 的图像 DeepSORT 检测：[[ 107.22
186.92 639.26 479.49 0.83611]] 回溯（最近一次调用）：文件 &quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 127 行，在 detect_customers() 文件
&quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 84 行，在 detect_customers tracks = tracker.update_tracks(deep_sort_detections,
frame=frame) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件
&quot;/home/roy/environments/001-opencv/lib/python3.12/site-packages/deep_sort_realtime/deepsort_tracker.py&quot;，
第 195 行，在 update_tracks 中断言 len(raw_detections[0][0])==4
^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError：类型为“numpy.float32”的对象
没有 len()

代码部分（检测和跟踪）：
def detect_customers():
# 初始化 YOLO 和 DeepSORT
model = YOLO(&quot;yolov8s.pt&quot;) # 加载 YOLO 模型
tracker = DeepSort(max_age=30, n_init=3)

cap = cv2.VideoCapture(0) # 将 0 替换为视频源
active_customers = {}

while True:
ret, frame = cap.read()
if not ret:
break

results = model(frame) # 执行 YOLO 推理
detections = []

for result in results:
for box in result.boxes:
# 提取边界框和置信度得分
x1, y1, x2, y2 = box.xyxy[0].tolist() # 将边界框转换为列表
confidence = float(box.conf[0]) # 置信度得分

# 以所需格式附加检测
detection = [float(x1), float(y1), float(x2), float(y2), float(confidence)]
detections.append(detection)

# 处理空检测
if len(detections) == 0:
deep_sort_detections = np.empty((0, 5)) # 空数组表示没有检测
else:
deep_sort_detections = np.array(detections, dtype=np.float32) # 转换为具有适当结构的 NumPy 数组

# 调试：打印传递给 DeepSORT 的检测
print(&quot;Detections for DeepSORT:&quot;, deep_sort_detections)

# 更新跟踪器
tracks = tracker.update_tracks(deep_sort_detections, frame=frame)
for track in tracks:
if not track.is_confirmed():
continue

track_id = track.track_id
ltrb = track.to_ltrb() # 转换为 (left, top, right, bottom)
cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)
cv2.putText(frame, f&quot;ID: {track_id}&quot;, (int(ltrb[0]), int(ltrb[1]) - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 显示框架
cv2.imshow(&quot;客户检测&quot;, frame)
if cv2.waitKey(1) &amp; 0xFF == 27：# 按 ESC 退出
break

cap.release()
cv2.destroyAllWindows()

问题摘要：

错误描述：将 deep_sort_detections 传递给 tracker.update_tracks() 时，我收到 TypeError：类型为 &#39;numpy.float32&#39; 的对象没有 len()。
检测格式：我将检测格式化为 [[x1, y1, x2, y2,
confidence], ...] 并将其转换为 NumPy 数组，其中 dtype=np.float32。但是，DeepSORT 似乎需要不同的格式，或者存在某种类型问题。

问题：

如何正确格式化检测数据以与 DeepSORT 的 update_tracks() 方法兼容？
我是否遗漏了数据的结构或传递给 DeepSORT 的方式？
]]></description>
      <guid>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</guid>
      <pubDate>Thu, 28 Nov 2024 21:25:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Arm CMSIS-NN Softmax 函数进行嵌入式机器学习</title>
      <link>https://stackoverflow.com/questions/79235253/how-to-use-arm-cmsis-nn-softmax-function-for-embedded-ml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79235253/how-to-use-arm-cmsis-nn-softmax-function-for-embedded-ml</guid>
      <pubDate>Thu, 28 Nov 2024 20:33:38 GMT</pubDate>
    </item>
    <item>
      <title>如何比较不同年份的集群？</title>
      <link>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</link>
      <description><![CDATA[我有多个数据集，所有数据集的组织方式都类似（相同的变量、值等）。我使用 KModes 独立分析了数据集，但是，我试图寻找多年来可能出现的趋势。我该如何比较不同年份的集群？]]></description>
      <guid>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</guid>
      <pubDate>Thu, 28 Nov 2024 15:14:27 GMT</pubDate>
    </item>
    <item>
      <title>无法从 xgboost 导入名称 XGBRegressor（未知位置）</title>
      <link>https://stackoverflow.com/questions/79234191/cannot-import-name-xgbregressor-from-xgboost-unknown-location</link>
      <description><![CDATA[xgboost 错误
无法导入 XGBRegressor
我在 vscode 上创建了一个环境，用于为机器学习项目实现端到端管道。我的大部分代码都保存在 github 中。我使用 requirements.txt 文件安装了所有 python 包。除了 xgboost 之外，其他所有包都可以使用]]></description>
      <guid>https://stackoverflow.com/questions/79234191/cannot-import-name-xgbregressor-from-xgboost-unknown-location</guid>
      <pubDate>Thu, 28 Nov 2024 13:56:19 GMT</pubDate>
    </item>
    <item>
      <title>我如何才能以某种方式融合嵌入以提高效率和分数？</title>
      <link>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</guid>
      <pubDate>Thu, 28 Nov 2024 13:01:00 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建多尺度 CNN，但遇到此错误：RuntimeError：mat1 和 mat2 形状无法相乘（32x4095 和 4096x4096）</title>
      <link>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</guid>
      <pubDate>Tue, 26 Nov 2024 23:06:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 RMSprop 优化器的动态学习率进行 Q 学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。以下是我正在使用的 Q-learning 更新函数：
def update_q_table(self, state, action, reward, next_state):
best_next_action = np.argmax(self.q_table[next_state, :])
td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
td_error = td_target - self.q_table[state, action]

# 更新 RMSprop 的平方梯度移动平均值 E[g^2]
self.gradient_Q[state, action] = (
self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2)
)

self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon)
self.learning_rate_history.append(self.learning_rate)

# 使用固定学习率和 TD 误差更新 Q 值
self.q_table[state, action] += self.learning_rate * td_error 

# 存储 E[g^2] 值用于跟踪
self.gradient_history.append(self.gradient_Q[state, action])
]]></description>
      <guid>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</guid>
      <pubDate>Mon, 11 Nov 2024 10:33:43 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用 DataLoaders 进行 k 折交叉验证</title>
      <link>https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch</link>
      <description><![CDATA[我已将训练数据集拆分为 80% 训练数据和 20% 验证数据，并创建了如下所示的 DataLoaders。但是我不想限制我的模型的训练。所以我想到将我的数据拆分为 K（可能是 5）个部分并执行交叉验证。但是我不知道如何在拆分数据集后将它们合并到我的数据加载器中。
train_size = int(0.8 * len(full_dataset))
validation_size = len(full_dataset) - train_size
train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])

full_loader = DataLoader(full_dataset, batch_size=4,sampler = sampler_(full_dataset), pin_memory=True) 
train_loader = DataLoader(train_dataset, batch_size=4, sampler = sampler_(train_dataset))
val_loader = DataLoader(validation_dataset, batch_size=1, sampler = sampler_(validation_dataset))
]]></description>
      <guid>https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch</guid>
      <pubDate>Fri, 27 Mar 2020 09:59:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AWS SageMaker 要运行 Web 服务器进行批量转换？</title>
      <link>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</link>
      <description><![CDATA[我正在创建自己的 Docker 容器以用于 SageMaker，我想知道当我想要执行批量转换作业时，serve 命令为什么会创建一个 Flask 应用程序来提供数据预测。只需解开模型并在我想要预测的数据集上运行模型的预测方法，不是更简单吗？我不需要 Web API/端点。我只需要每天自动生成一次预测。]]></description>
      <guid>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</guid>
      <pubDate>Thu, 21 Nov 2019 23:11:19 GMT</pubDate>
    </item>
    <item>
      <title>具有状态-动作-状态奖励结构的 Q 学习和以状态为行、以动作为列的 Q 矩阵[关闭]</title>
      <link>https://stackoverflow.com/questions/45382763/q-learning-with-a-state-action-state-reward-structure-and-a-q-matrix-with-states</link>
      <description><![CDATA[我在 R 中设置了一个 Q 学习问题，希望有人能帮助我确定我的方法在构建问题时的理论正确性。
问题结构
对于这个问题，环境由 10 个可能的状态组成。在每个状态中，代理有 11 个潜在动作可供选择（无论代理处于什么状态，这些动作都是相同的）。根据代理所处的特定状态以及代理随后采取的后续动作，转换到下一个状态有一个唯一的分布，即转换到任何下一个状态的概率仅取决于前一个状态以及随后采取的动作。
每个情节有 9 次迭代，即代理可以在新情节开始之前采取 9 个动作并进行 9 次转换。在每一集中，代理都将从状态 1 开始。
在每一集中，在代理的 9 个动作中的每一个之后，代理都将获得奖励，该奖励取决于代理的（紧接的）前一个状态和他们（紧接的）前一个动作以及他们所处的状态，即代理的奖励结构取决于状态-动作-状态三元组（一集中将有 9 个）。
代理的转移概率矩阵是静态的，奖励矩阵也是如此。
我已经设置了两种学习算法。在第一个算法中，q 矩阵更新发生在每个情节中的每个动作之后。在第二个算法中，q 矩阵在每集之后更新。该算法使用 epsilon 贪婪学习公式。
最大的问题是，在我的 Q 学习中，我的代理没有学习。随着时间的推移，它获得的奖励越来越少。我已经研究过其他潜在问题，例如简单的计算错误或代码中的错误，但我认为问题在于我的 q 学习问题的概念结构。
问题

我已将 Q 矩阵设置为 10 行 11 列的矩阵，即所有 10 个状态都是行，11 个动作都是列。这是最好的方法吗？这意味着代理正在学习一种策略，该策略规定“只要您处于状态 x，就执行动作 y”
鉴于我的问题的这种独特结构，标准 Q 更新是否仍然适用？即 Q[cs,act]&lt;&lt;-Q[cs,act]+alpha*(Reward+gamma*max(Q[ns,])-Q[cs,act])
其中 cs 是当前状态；act 是选择的动作；奖励是根据您的当前状态、您选择的操作以及您将转换到的下一个状态而获得的奖励；ns 是根据您的上一个状态和上一个操作而转换到的下一个状态（请注意，您是随机转换到此状态的）。
R 中是否有开放的 AI 健身房？是否有针对这种结构问题的 Q 学习包？
]]></description>
      <guid>https://stackoverflow.com/questions/45382763/q-learning-with-a-state-action-state-reward-structure-and-a-q-matrix-with-states</guid>
      <pubDate>Fri, 28 Jul 2017 21:36:22 GMT</pubDate>
    </item>
    <item>
      <title>文本分类器</title>
      <link>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</link>
      <description><![CDATA[有人知道好的开源文本分类模型吗？我知道斯坦福分类器、Weka、Mallet 等，但它们都需要训练。
我需要将新闻文章分类为体育/政治/健康/游戏/等。有没有预先训练过的模型？
Alchemy、OpenCalais 等不是选择。我需要开源工具（最好是 Java 语言的）。]]></description>
      <guid>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</guid>
      <pubDate>Thu, 07 Mar 2013 15:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>