<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 20 Aug 2024 21:16:40 GMT</lastBuildDate>
    <item>
      <title>‘charmap’ 编解码器无法对位置 18-37 中的字符进行编码：字符映射到 <undefined> 错误</title>
      <link>https://stackoverflow.com/questions/78894108/charmap-codec-cant-encode-characters-in-position-18-37-character-maps-to-un</link>
      <description><![CDATA[我正在尝试使用 django 连接 ml 模型。在这里我加载了模型和必要的编码器。
import joblib
import os
#from keras.model import load_model
from keras.src.saving.saving_api import load_model
from django.conf import settings
import numpy as np

def load_keras_model():
# 定义模型文件的路径
model_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, &#39;football_prediction_model.h5&#39;)
print(&quot;Keras model path:&quot;, model_path)

try:
# 加载模型
model1 = load_model(model_path)
# 通过打印其摘要来验证模型加载
print(&quot;模型已成功加载。&quot;)
print(&quot;模型摘要：&quot;)
model1.summary()
return model1

except Exception as e:
# 处理异常并打印错误消息
print(f&quot;加载模型时出错：{str(e)}&quot;)
return None

def load_encoder(filename):
coder_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, filename)
print(f&quot;{filename} path:&quot;,coder_path) # 调试路径
return joblib.load(encoder_path)

# 加载所有必要的模型和编码器
model = load_keras_model()
team_label_encoder = load_encoder(&#39;team_label_encoder.pkl&#39;)
outcome_label_encoder = load_encoder(&#39;outcome_label_encoder.pkl&#39;)
scaler = load_encoder(&#39;scaler.pkl&#39;)

def predict_outcome(home_team,away_team,year,month,day,temperature):
try:
print(f&quot;主队： {home_team}&quot;)
print(f&quot;客队：{away_team}&quot;)
print(f&quot;年份：{year}, 月份：{month}, 日：{day}, 温度：{temperature}&quot;)
# 对输入数据进行编码和缩放
home_team_encoded = team_label_encoder.transform([home_team])[0]
away_team_encoded = team_label_encoder.transform([away_team])[0]
temperature_scaled = scaler.transform([[temperature]])[0][0]

print(f&quot;编码的主队：{home_team_encoded}&quot;)
print(f&quot;编码的客队：{away_team_encoded}&quot;)
print(f&quot;缩放的温度：{temperature_scaled}&quot;)

# 为模型准备输入
input_data = np.array([[home_team_encoded, away_team_encoded, year, month, day,temperature_scaled]])
print(f&quot;输入日期：{input_data}&quot;)
input_data = input_data.reshape((1, 1, 6))
print(f&quot;输入更新日期：{input_data}&quot;)

# 进行预测
prediction = model.predict(input_data)
print(f&quot;预测：{prediction}&quot;)
consequence_index = np.argmax(prediction)
print(f&quot;结果索引：{outcome_index}&quot;)

# 将预测映射回原始结果标签
consequence_label = consequence_label_encoder.inverse_transform([outcome_index])
print(f&quot;输出标签：{outcome_label}&quot;)

return consequence_label[0]

except ValueError as e:
return f&quot;Error: {str(e)}&quot;

home_team = &#39;Scotland&#39;
away_team = &#39;England&#39;
year = 2024
month = 8
day = 20
temperature = 25

predicted_outcome = predict_outcome(home_team, away_team, year, month, day,temperature)
print(f&quot;Predicted Outcome: {predicted_outcome}&quot;)

对于上述代码，以下是输出。请注意，我在控制台中包含了部分输出。
主队：苏格兰
客队：英格兰
年份：2024，月份：8，日期：20，温度：25
D:\My Projects\FootBall-Match-Win-Prediction\BackEnd\venv\Lib\site-packages\sklearn\base.py:465：UserWarning：X 没有有效的特征名称，但 MinMaxScaler 配备了特征名称
warnings.warn(
编码的主队：3
编码的客队：1
缩放温度：0.75
输入日期：[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]
输入更新日期：[[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]]

预测结果：错误：“charmap”编解码器无法对位置 18-37 的字符进行编码：
字符映射到 &lt;undefined&gt;

系统检查未发现任何问题（0 静音）。
2024 年 8 月 21 日 - 00:30:52
Django 版本 5.1，使用设置“BackEnd.settings”
在 http://localhost:8000/ 启动开发服务器
使用 CTRL-BREAK 退出服务器。

对于它打印的 predicted_outcome 变量
错误：“charmap”编解码器无法对位置的字符进行编码18-37：
字符映射到 &lt;undefined&gt;。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78894108/charmap-codec-cant-encode-characters-in-position-18-37-character-maps-to-un</guid>
      <pubDate>Tue, 20 Aug 2024 19:16:51 GMT</pubDate>
    </item>
    <item>
      <title>利用分割模型对直肠内超声图像中的肿瘤分期进行分类</title>
      <link>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</guid>
      <pubDate>Tue, 20 Aug 2024 18:28:10 GMT</pubDate>
    </item>
    <item>
      <title>用于训练和测试分割的标准缩放器[关闭]</title>
      <link>https://stackoverflow.com/questions/78893752/standard-scaler-for-train-and-test-split</link>
      <description><![CDATA[在阅读了互联网上的几篇文章后，我了解到最好在将数据分成训练和测试后再进行规范化步骤。然后我就有疑问了。
数据被分成X_train、X_test、y_train、y_test。
那么我应该将训练和测试拟合到相同的缩放器对象还是不同的缩放器对象呢
选项 1：
scaler = StandardScaler()
Train_norm = scaler.fit_transform(X_train)
Test_norm = scaler.fit_transform(X_test)
y_train_norm = scaler.fit_transform(y_train)
y_test_norm = scaler.fit_transform(y_test)

选项 2：
scaler_x = StandardScaler()
scaler_y = StandardScaler()
Train_norm = scaler_x.fit_transform(X_train)
Test_norm = scaler_x.fit_transform(X_test)
y_train_norm = scaler_y.fit_transform(y_train)
y_test_norm = scaler_y.fit_transform(y_test)

或者有更好的方法吗？
有人能对此提供见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78893752/standard-scaler-for-train-and-test-split</guid>
      <pubDate>Tue, 20 Aug 2024 17:32:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 的注意力机制 Unet</title>
      <link>https://stackoverflow.com/questions/78893443/attention-unet-using-tensorflow</link>
      <description><![CDATA[我正在尝试使用 tf/keras 实现注意力 unet，但遇到了问题。我可能在数据集和将其传递给模型时遇到了问题。有人可以尝试帮助我吗（我可能会有一些非常菜鸟的后续问题）？我的目标是突出显示 CT 扫描中的癌症部位并将其显示给用户。
我遇到了形状问题（输入图像为 256x256，但输入形状需要额外的维度）
我最终如何在新图像上显示分割结果？
https://colab.research.google.com/drive/1sNgsP-SAzLdFj7zNe7UBX06Hjg0JV9V1?usp=sharing 是我正在使用的笔记本。]]></description>
      <guid>https://stackoverflow.com/questions/78893443/attention-unet-using-tensorflow</guid>
      <pubDate>Tue, 20 Aug 2024 16:08:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 TFIDF 的 SKL 管道中的数据形状问题</title>
      <link>https://stackoverflow.com/questions/78893376/data-shape-issues-in-skl-pipeline-using-tfidf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78893376/data-shape-issues-in-skl-pipeline-using-tfidf</guid>
      <pubDate>Tue, 20 Aug 2024 15:52:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多层感知器对 FFT 数据进行预处理以进行二元分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/78892290/how-can-fft-data-be-pre-processed-for-binary-classification-using-multi-layer-pe</link>
      <description><![CDATA[我有 410 个 FFT 样本，我打算将它们输入到 MLP 中，以将数据分为两个（二进制）类别。我使用 PyTorch 在 Python 中构建了一个神经网络，该网络尝试使用这些数据进行训练。但是，我获得的验证准确率仅达到 70%。
更改 MLP 的模型参数对最终准确率几乎没有影响，这让我相信问题出在初始数据处理上，尽管我可能是错的。
进一步研究这个问题后，我知道需要对数据进行预处理，以使其更易于训练，但我不明白如何预处理 FFT 数据。到目前为止，我已经根据最小值/最大值进行了标准化，并将数据设为零中心。
我查看了此处的帖子，但我的问题仍然存在。
编辑：
添加了 FFT 图像和代码片段
FFT 图像
model = nn.Sequential(OrderedDict([
(&#39;drop1&#39;, nn.Dropout(0.1)),
(&#39;dense1&#39;, nn.Linear(410, 100)),
(&#39;act1&#39;, nn.LeakyReLU()),
# (&#39;drop2&#39;, nn.Dropout(0.1)),
(&#39;drop4&#39;, nn.Dropout(0.2)),
(&#39;dense2&#39;, nn.Linear(100,25)),
(&#39;act2&#39;, nn.Sigmoid()),
(&#39;dense4&#39;, nn.Linear(25, 5)),
(&#39;act4&#39;, nn.LeakyReLU()),
(&#39;dense8&#39;, nn.Linear(5, 1)),
(&#39;act5&#39;, nn.Tanh()),
(&#39;dense5&#39;, nn.Linear(1, 1)),
(&#39;act8&#39;, nn.Sigmoid()),
]))
]]></description>
      <guid>https://stackoverflow.com/questions/78892290/how-can-fft-data-be-pre-processed-for-binary-classification-using-multi-layer-pe</guid>
      <pubDate>Tue, 20 Aug 2024 11:53:33 GMT</pubDate>
    </item>
    <item>
      <title>适合发票提取的ML模型</title>
      <link>https://stackoverflow.com/questions/78892147/suitable-ml-model-for-invoice-extraction</link>
      <description><![CDATA[我必须构建一个 ml 模型，用于从不同布局的 pdf 文件中提取发票详细信息，例如客户姓名、发票日期、发票号码等。发票 pdf 文件具有不同的格式，例如，在一个 pdf 文件中，发票日期位于右上角，而在另一个 pdf 文件中，则位于左上角。我有大约 252 个样本。我正在使用 202 个样本进行训练，其中 28 个样本用于验证。我目前正在使用每个标签/类的边界框坐标训练张量流更快的 RCNN resnet101 对象检测模型（例如：发票日期，数字是标签），其中 num_train_steps = 2000，批量大小 = 4 和 num_eval_steps = 250。当我在测试图像上测试训练后的模型时，其中一些能够提取发票详细信息，但其中一些给出错误的输出，甚至有时它们为相同的发票详细信息提供多个输出（例如，它们为一个发票日期预测两个值）。我应该如何调整参数（批次大小、num_train_steps、num_eval_steps）？为了更好地理解，请参阅faster rcnn 的配置文件]]></description>
      <guid>https://stackoverflow.com/questions/78892147/suitable-ml-model-for-invoice-extraction</guid>
      <pubDate>Tue, 20 Aug 2024 11:18:03 GMT</pubDate>
    </item>
    <item>
      <title>Yolop 输出在 SNPE android 上给出 NaN 值</title>
      <link>https://stackoverflow.com/questions/78892125/yolop-output-giving-nan-values-on-snpe-android</link>
      <description><![CDATA[我使用了 YOLOP 模型，并使用 SNPE SDK 将其转换为 dlc 文件。创建输入张量并构建网络。获取输出张量值给出 NaN 值列表。下面给出了我如何配置网络、传递输入张量。但我的变量 laneLineSeg 和 DetOutValues 是 NaN 值。有人能帮我吗？
这是我配置网络、传递输入张量的方法。但我的变量 laneLineSeg、DrivAreaSegValues 和 DetOutValues 是 NaN 值。
private val modelName = &quot;yolop_seg.dlc&quot;
private fun configureNetwork(): NeuralNetwork? {
返回尝试 {

val assetInputStream = applicationContext.assets.open(modelName)
val network = SNPE.NeuralNetworkBuilder(application)
.setDebugEnabled(false)
.setRuntimeOrder(NeuralNetwork.Runtime.GPU_FLOAT16)
.setModel(assetInputStream, assetInputStream.available())
.setCpuFallbackEnabled(true)
.setUseUserSuppliedBuffers(false)
.setUnsignedPD(false)
.setCpuFixedPointMode(false)
.setOutputLayers(&quot;Sigmoid_1671&quot;,&quot;Concat_1534&quot;, &quot;Sigmoid_1808&quot; )

.build()
assetInputStream.close()
network
} catch (e: Exception) {
Log.e(&lt;NETWORK&gt;, e.message.toString())
null
}
}
private fun getClassificationResult(network: NeuralNetwork, bitmap: Bitmap): FloatArray{
val image = Bitmap.createScaledBitmap(bitmap, 640, 640, true)

val inputMap: MutableMap&lt;String, FloatTensor&gt; = HashMap()
val inputNames: Set&lt;String&gt; = network.inputTensorsNames
val outputNames: Set&lt;String&gt; = network.outputTensorsNames
var mInputLayer = &quot;&quot;
val mOutputLayer = &quot;&quot;
mInputLayer = inputNames.iterator().next()

val tensor = network.createFloatTensor(1, 640, 640,3)
val dimension = tensor.shape
val isGrayScale = (dimension[dimension.size - 1] == 1)

val input: FloatArray = if (!isGrayScale) {
loadRgbBitmapAsFloat(image)
} else {
loadGrayScaleBitmapAsFloat(image)
}
tensor.write(input, 0, input.size)
输入映射[mInputLayer] = tensor
val 输出映射 = network.execute(inputsMap)

val driveAreaSeg = outputMap[&quot;drive_area_seg&quot;]
val DrivAreaSegValues = FloatArray(driveAreaSeg!!.size)
driveAreaSeg.read(DrivAreaSegValues, 0, DrivAreaSegValues.size)
val i = 0

val laneLineSeg = outputMap[&quot;lane_line_seg&quot;]
val LanLinSegValues = FloatArray(laneLineSeg!!.size)
laneLineSeg.read(LanLinSegValues, 0, LanLinSegValues.size)

val detOut = outputMap.get(&quot;det_out&quot;)
val DetOutValues = FloatArray(detOut!!.size)
detOut.read(DetOutValues, 0, DetOutValues.size)

return laneLineSeg

]]></description>
      <guid>https://stackoverflow.com/questions/78892125/yolop-output-giving-nan-values-on-snpe-android</guid>
      <pubDate>Tue, 20 Aug 2024 11:13:13 GMT</pubDate>
    </item>
    <item>
      <title>Val_accuracy 正在改变，有时它在补码之间交替（100％-val_acc）</title>
      <link>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</link>
      <description><![CDATA[我被分配根据我读过的一篇论文来实现一个机器学习模型。
这篇论文实现了一个用于属性分类的多任务学习模型（带标签的图像是模型输入，带标签的意思是属性注释，每幅图像有 40 个）。
它是一个多任务学习模型，因为在模型输入层和 40 个属性分支之后有一个共享的密集层，每个分支都有自己的损失函数（所有分支的二元交叉熵）和自己的 S 型激活函数（在最后一层，用于预测 40 个属性中的每一个是否存在于图像中）。
经过大量艰苦的努力，它终于开始在所有分支上返回所有 S 型函数的概率，但只有 val_accuracy 的概率是错误的：val_loss 和损失（训练损失）越来越小，acc（训练准确度）也在正常的概率值范围内，除了 val_accuracy 总是相同的值或它的补码。
例如（仅举 5 个时期为例）：
40 个分支之一的一个属性预测的准确度：
5_o_Clock_Shadow_Accuracy
0 0.823665
1 0.891178
2 0.891178
3 0.891178

同一属性的损失：
 5_o_Clock_Shadow_loss
0 0.921046
1 0.701494
2 0.913597
3 0.765397
4 0.894950

val_loss：
val_5_o_Clock_Shadow_loss
0 730232.750000
1 300412.500000
2 376215.843750
3 0.747685
4 1.607191

最后是 val_Accuracy：
val_5_o_Clock_Shadow_Accuracy
0 0.882382
1 0.117618
2 0.882382
3   0.882382 4 0.882382  我的模型： def subnet(shared_layers_output, i): att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_1&#39;)(shared_layers_output) att_branch = ReLU()(att_branch) att_branch = BatchNormal ization()(att_branch) att_branch = Dropout(0.5)(att_branch) att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_2&#39;)(att_branch) att_branch = ReLU()(att_branch) att_branch = BatchNormalization()(att_branch) att_branch = Dropout(0.5)(att_branch)

branch_output = Dense(1, name=att_list[i],activation=&#39;sigmoid&#39;)(att_branch)

return branch_output

def multi_task_model():

#输入
input_layer = Input(shape=(512,), name=&#39;input_layer&#39;)

#共享网络（1 个网络）
shared_x = Dense(512, name=&#39;shared_dense_layer&#39;)(input_layer)
shared_x = ReLU()(shared_x)
shared_x = BatchNormalization()(shared_x)
shared_x = Dropout(0.5)(shared_x)

branch_outputs = list()
for i in range(40):
branch_outputs.append(subnet(shared_x, i))

model = Model(input_layer, branch_outputs, name=&#39;model&#39;)

返回模型


训练和测试输入形状：(n_samples, 512)
训练和测试标签输入形状：(40, n_samples)
学习率：1e-03

5_o_Clock_Shadow 损失、val_loss、acc 和 val_acc 超过 5 个时期
 损失 val_loss acc val_acc
0 0.422385 1.949578 0.864272 0.8873
1 0.354094 151.987991 0.888797 0.1127
2 0.354356 58.867992 0.888797 0.1127
3 0.352891 94.257980 0.888797 0.1127
4 0.353390 10.997763 0.888797 0.1127
]]></description>
      <guid>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</guid>
      <pubDate>Sun, 18 Aug 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv10 和 RTSP 流的车牌识别系统中的高 RAM 和存储使用率 [关闭]</title>
      <link>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</link>
      <description><![CDATA[我正在开发一个车牌识别系统，使用来自安全摄像头的 RTSP 流来识别带有阿拉伯字母/数字的埃及车牌。我的设置包括：

YOLOv10 模型 1：检测和跟踪汽车。

YOLOv10 模型 2：检测车内的车牌。

YOLOv10 模型 3：对车牌上的字符和数字进行 OCR。


我正在使用 Python 推理库，将模型导出为 .engine 格式以实现 GPU 加速。
问题：

RAM 使用情况：系统每路摄像头信号消耗高达 8 GB 的 RAM。

存储使用情况：需要 15-20 GB 的存储空间来管理软件包。

性能：尽管使用了 GPU，但系统仍然资源密集。


我预计 GPU 加速会显著降低 RAM 和存储需求，但我没有看到预期的效率。类似产品 Plate Recognizer 的运行资源要少得多（0.5 GB RAM，无 GPU）（参考链接）。
这是车牌识别器使用的软件包列表，也许有人可以帮助我了解它们的工作原理，高效：
certifi==2024.6.2
cffi==1.16.0
charset-normalizer==3.3.2
configobj==5.0.8
cryptography==41.0.1
idna==3.7
Levenshtein==0.21.1
ntplib==0.4.0
numpy==1.24.4
opencv-python-headless==4.7.0.72
openvino==2023.3.0
openvino -telemetry==2024.1.0
persist-queue==0.8.1
psutil==5.9.5
pycparser==2.22
python-dateutil==2.8.2
python-Levenshtein==0.21.1
rapidfuzz==3.9.3
requests==2.32.3
rollbar==0.16.3
scipy==1.10.1
six==1.16.0
urllib3==2.2.1

什么我尝试过：

模型优化：导出到 .engine 进行 GPU 加速。

流管理：使用 Python 推理库来处理 RTSP 流。


问题：
如何在此设置中减少 RAM 和存储使用量？是否有可能更有效的替代模型或方法？]]></description>
      <guid>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</guid>
      <pubDate>Sat, 17 Aug 2024 19:53:15 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow 联合错误模块‘tensorflow_federated.python.learning’没有属性‘build_federated_averaging_process’</title>
      <link>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</link>
      <description><![CDATA[我有一个 ML 代码
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
import tensorflow_federated as tff

iris = load_iris()
df = pd.DataFrame(iris.data,columns=iris.feature_names)
df[&#39;Species&#39;]=iris.target

# 将数据框拆分为输入特征和目标变量

x = df.drop(&#39;Species&#39;,axis=1)
y = df[&#39;Species&#39;]
# 创建客户端数据集的函数（假设数据已预先分区）
def create_tf_dataset(client_data):
&quot;&quot;&quot;根据提供的客户端数据（特征、标签）创建 tf.data.Dataset。&quot;&quot;&quot;
features, labels = client_data
return tf.data.Dataset.from_tensor_slices((features, labels))

# 将数据拆分为客户端数据集（模拟数据分区）
client_datasets = []
num_clients = 5
for i in range(num_clients):
start_index = int(i * (len(x) / num_clients))
end_index = int((i + 1) * (len(x) / num_clients))
client_features = x[start_index:end_index]
client_labels = y[start_index:end_index]
client_datasets.append(create_tf_dataset((client_features, client_labels)))
# 定义模型架构（替换为所需的模型复杂度）
def model_fn(inputs):
features, _ = 输入 # 我们仅使用特征进行分类
density1 = tf.keras.layers.Dense(10,activation=&#39;relu&#39;)(features)
dense2 = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(dense1) # 3 个鸢尾花类的 3 个单元
return tf.keras.Model(inputs=features,outputs=dense2)

# 定义客户端优化器
client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# 定义服务器优化器（用于服务器端聚合）
server_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
fed_learning_model = tff.learning.build_federated_averaging_process(
model_fn,
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer)

但是我一直收到此错误。
---------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-e5966e29fc79&gt; in &lt;cell line: 1&gt;()
----&gt; 1 fed_learning_model = tff.learning.build_federated_averaging_process(
2 model_fn,
3 client_optimizer_fn=client_optimizer,
4 server_optimizer_fn=server_optimizer)

AttributeError：模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”

我不知道我的版本是否适合。我的 tensorflow federated 版本是 0.76.0
我的 tensorflow 版本是 2.14.1，python 版本是 3.10.12
当我通过互联网搜索时，我发现此代码不支持 tensorflow 版本 0.21.0 及以上版本。但我不知道在最新版本中该使用什么]]></description>
      <guid>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</guid>
      <pubDate>Tue, 23 Apr 2024 05:30:55 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn LOOCV 与手动执行产生不同的结果，为什么？</title>
      <link>https://stackoverflow.com/questions/75918536/scikit-learn-loocv-vs-doing-it-manually-give-different-results-why</link>
      <description><![CDATA[因此，我为一个小数据集建立了一个模型，由于它是一个小数据集，我做了一个留一交叉验证 (LOOCV) 检查来确保它的准确性。简而言之，我会手动移除一个样本，训练模型，预测遗漏的样本并保存预测，然后对所有样本重复该过程。然后，我会使用预测列表和实际值来获得 RMSE 和 R2。
今天我发现有一个 Scikit-Learn 实现 sklearn.model_selection.LeaveOneOut，但是，当我尝试它时，它给出了不同的 RMSE 结果，并且拒绝在 LOOCV 方法中使用 R 平方作为准确度（它似乎计算每个样本的准确度，这与 R2 不起作用）。
以下是代码的简要示例：
from numpy import mean
from numpy import std
from sklearn.datasets import make_blobs
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

cv = LeaveOneOut()
model = RandomForestRegressor(n_estimators=200, max_depth=6,n_jobs=40, random_state=0)

scores = cross_val_score(model, data2SN, labelCL,scoring=&#39;neg_root_mean_squared_error&#39;, cv=cv, n_jobs=-1)
# report performance
print(&#39;Accuracy: %.3f (%.3f)&#39; % (mean(scores), std(scores)))

我猜我是在计算整个数据集的 RMSE，而 LOOCV 是按样本计算的，最终我会取平均值，这就是导致两个代码输出不一致的原因，但是，当我尝试计算每个样本的 RMSE 时失败了（引用此 TypeError：Singleton array 3021.0 不能被视为有效集合）。所以我不确定 LOOCV 中 RMSE 是如何计算的。我不确定是否应该相信我的代码，或者只是盲目地使用 scikit-learn 实现。
我不知道该怎么做，chatGPT 简直令人困惑不已，所以我的人类同胞请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/75918536/scikit-learn-loocv-vs-doing-it-manually-give-different-results-why</guid>
      <pubDate>Mon, 03 Apr 2023 10:38:08 GMT</pubDate>
    </item>
    <item>
      <title>在新类中，我收到一个 AttributeError: 无法在 <module '__main__'> 上获取属性 'ResNet1D'</title>
      <link>https://stackoverflow.com/questions/69030379/torch-loadml-model-in-new-class-i-receive-an-attributeerror-cant-get-attribu</link>
      <description><![CDATA[我已使用 Google Colab 在名为 model_prep.py 的文件中成功训练了卷积神经网络模型。该模型的准确率为 92%。现在我对该模型很满意，我已使用 pyTorch 保存了我的模型。
torch.save(model, &#39;/content/drive/MyDrive/myModel.pt&#39;)

我对此的理解是，一旦模型经过完全训练，我就可以使用 pyTorch 保存训练后的模型，然后将其加载到未来的项目中以对新数据进行预测。因此，我创建了一个单独的 test.py 文件，并在其中加载了经过训练的模型，如下所示：
model = torch.load(&#39;/content/drive/MyDrive/myModel.pt&#39;)
model.eval()

但在新的 test.py 文件中，我收到一条错误消息
AttributeError: 无法在 &lt;module &#39;__main__&#39;&gt; 上获取属性 &#39;ResNet1D&#39;

虽然在与创建经过训练的模型相同的笔记本 (model_prep.py) 中加载模型时不会发生此错误。此错误仅在将模型加载到没有模型架构的单独笔记本中时发生。我该如何解决这个问题？我想将经过训练的模型加载到一个新的单独文件中以对新数据执行。有人能提出解决方案吗？
将来，我想使用 tkinter 创建一个 GUI，并部署经过训练的模型，使用 tkinter 文件中的新数据检查预测。这可能吗？]]></description>
      <guid>https://stackoverflow.com/questions/69030379/torch-loadml-model-in-new-class-i-receive-an-attributeerror-cant-get-attribu</guid>
      <pubDate>Thu, 02 Sep 2021 12:33:05 GMT</pubDate>
    </item>
    <item>
      <title>进行预测时如何处理未包含在训练集中的标签</title>
      <link>https://stackoverflow.com/questions/58627102/how-to-deal-with-label-that-not-included-in-training-set-when-doing-prediction</link>
      <description><![CDATA[例如，使用监督学习对 5 个不同的人脸进行分类。
但是当对训练集中没有出现的第 6 个人脸进行测试时，模型仍然会将其预测在 5 个人之内。
当模型之前没有训练过第 6 个人脸以及之后的人脸时，如何让模型将其预测为未知？]]></description>
      <guid>https://stackoverflow.com/questions/58627102/how-to-deal-with-label-that-not-included-in-training-set-when-doing-prediction</guid>
      <pubDate>Wed, 30 Oct 2019 14:01:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将边界框 (x1, y1, x2, y2) 转换为 YOLO 样式 (X, Y, W, H)</title>
      <link>https://stackoverflow.com/questions/56115874/how-to-convert-bounding-box-x1-y1-x2-y2-to-yolo-style-x-y-w-h</link>
      <description><![CDATA[我正在训练 YOLO 模型，我有以下格式的边界框：-
x1, y1, x2, y2 =&gt; ex (100, 100, 200, 200)

我需要将其转换为 YOLO 格式，如下所示：-
X, Y, W, H =&gt; 0.436262 0.474010 0.383663 0.178218

我已经计算了中心点 X、Y、高度 H 和重量 W。
但仍然需要一种方法将它们转换为上述浮点数。]]></description>
      <guid>https://stackoverflow.com/questions/56115874/how-to-convert-bounding-box-x1-y1-x2-y2-to-yolo-style-x-y-w-h</guid>
      <pubDate>Mon, 13 May 2019 15:52:12 GMT</pubDate>
    </item>
    </channel>
</rss>