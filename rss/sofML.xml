<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Dec 2023 03:14:22 GMT</lastBuildDate>
    <item>
      <title>如何在 Pytorch 中正确释放 GPU 内存</title>
      <link>https://stackoverflow.com/questions/77682343/how-to-properly-free-gpu-memory-in-pytorch</link>
      <description><![CDATA[我正在使用 Pytorch 和 Pytorch-Lightning 来训练和验证模型。但在测试期间（test_step），我遇到了 CUDA 内存错误，我不知道如何解决。该模型训练和验证良好，这不是批量大小、数据大小或模型大小的问题。因此，有关此主题的大多数其他答案不适用于我。
我需要在测试期间保持某种状态。数据属于某些 id，我收集多个时期的张量，如果它们属于同一 id，则将它们连接起来。然后，我计算对该级联张量的一些兴趣度量，并继续收集下一个张量，直到 id 发生变化。我尝试将批处理大小减少到 1，但这只会使流程进展得更远，然后会引发类似的错误。
我尝试了一些技巧，例如 gc.collect、torch.cuda.empty_cache() 和 del var 其中 var&lt; /code&gt; 是我收集张量以便稍后连接的变量。
连接本身不是问题，因为当我检查日志时，该过程会在多个时期内正常进行，直到在结束前的几个 id 处抛出以下错误。在我看来，我用来维护状态的张量并没有被释放，并且在每个纪元之后占用越来越多的空间。
torch.cuda.OutOfMemoryError：CUDA 内存不足。
尝试分配 12.48 GiB（GPU 0；39.56 GiB 总容量；
已分配 25.35 GiB；
9.62 GiB 免费；
PyTorch 总共预留了 29.44 GiB）
如果保留内存是&gt;&gt;分配的内存尝试设置 max_split_size_mb 以避免碎片。
请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

我已经不知道该做什么了。我已经尝试了这个问题中提到的几乎所有技巧，包括这个评论 https://stackoverflow.com/a/75779114/228177.
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77682343/how-to-properly-free-gpu-memory-in-pytorch</guid>
      <pubDate>Tue, 19 Dec 2023 00:11:12 GMT</pubDate>
    </item>
    <item>
      <title>LSTM多类蛋白质分类模型（keras）模型疯狂低精度</title>
      <link>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</link>
      <description><![CDATA[我正在做一项蛋白质分类作业。我得到了一个包含 5 个类别的不平衡数据集。每个条目都有一个总科名称和相应的序列。
我一直在遵循我的大学教学，了解使用 keras 和 sklearn 模块的正确方法和编码实践。然而，经过多次参数调整、添加和删除图层。我的模型始终预测精度较低（val_accuracy 0.2）。我依次使用 2 个双向 LSTM，然后是 relu 密集层、dropout 和带有 softmax 激活的最终密集层。
请参阅下面的代码：
导入 pandas 作为 pd
从 sklearn 导入预处理，model_selection
导入keras
将 numpy 导入为 np
将张量流导入为 tf
将tensorflow_addons导入为tfa
从 sklearn.utils 导入 class_weight
从 keras.layers 导入嵌入、Conv1D、MaxPooling1D
从 keras.layers 导入 Dropout、Flatten、Dense
从 keras.layers 导入 LSTM、双向、GRU、SimpleRNN、MaxPooling1D、SpatialDropout1D
从 scikeras.wrappers 导入 KerasClassifier
从 sklearn.model_selection 导入 GridSearchCV、StratifiedKFold
从 imblearn.over_sampling 导入 RandomOverSampler

种子 = 123

# 读入数据并删除索引列
数据= pd.read_csv(“./results/processedData.csv”,index_col=False)

# 对 seq 进行标记化，以便 CNN 可以使用
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)

打印（数据.描述（））

y = 数据.名称

# 将标记化值设置为 x
x = tokenizer.texts_to_sequences(data.seq)
最大长度 = 580
x = keras.preprocessing.sequence.pad_sequences(
    x，MAX_LENGTH，截断=“前”）

# 设置模型参数
令牌 = len(tokenizer.word_index) + 1
课程 = 5
单位 = 64
丢包率 = 0.2


def buildLSTM():
    模型 = keras.Sequential([
        嵌入（TOKENS，32，input_length = MAX_LENGTH，mask_zero = True），

        双向（LSTM（单位，return_sequences = True）），
        双向（LSTM（UNITS // 2）），


        密集（UNITS // 2，激活=“relu”），
        辍学率（DROPOUT_RATE），

        密集（类，激活=“softmax”）
    ]）
    model.compile(loss=“categorical_crossentropy”,
                  优化器=keras.optimizers.Adam(1e-4)，指标=[tf.keras.metrics.CategoricalAccuracy()])
    返回模型


# # 过采样
过采样 = RandomOverSampler(sampling_strategy=&#39;少数&#39;)

skf = StratifiedKFold(n_splits=5, shuffle=True)

对于 skf.split(x, y) 中的 trainI、testI：
    训练X，训练Y = x [训练I]，y [训练I]
    测试X，测试Y = x [测试I]，y [测试I]

    # 将输出设置为分类器
    labelBinarize = 预处理.LabelBinarizer()
    labelBinarize.fit(trainY)
    trainY = labelBinarize.transform(trainY)
    testY = labelBinarize.transform(testY)

    # 过采样训练集
    trainX, trainY = oversample.fit_resample(trainX, trainY)

    # 构建模型并拟合数据
    模型 = buildLSTM()
    model.fit(trainX,trainY,validation_data=(
        测试X，测试Y），纪元= 5，batch_size = 64）

    # 评估分数
    分数 = model.evaluate(testX, testY, verbose=0)
    准确度 = 分数[1] * 100
    print(f&#39;准确度：{准确度}&#39;)

# 打印摘要
打印（模型.摘要）


请参阅下面的纪元示例：
39/39 [================================] - 48s 944ms/步 - 损耗：1.6088 - 分类准确度：0.2923 - 验证损失：1.6090 - 验证分类准确度：0.0761
纪元 2/5
39/39 [================================] - 32s 825ms/步 - 损失：1.6075 - categorical_accuracy：0.2987 - val_loss ：1.6085 - val_categorical_accuracy：0.0761
纪元 3/5
39/39 [================================] - 33s 838ms/步 - 损失：1.6062 - categorical_accuracy：0.2987 - val_loss ：1.6081 - val_categorical_accuracy：0.0761
纪元 4/5
39/39 [==============================] - 34s 877ms/步 - 损失：1.6050 - categorical_accuracy：0.2987 - val_loss ：1.6077 - val_categorical_accuracy：0.0761
纪元 5/5
39/39 [================================] - 33s 845ms/步 - 损失：1.6037 - categorical_accuracy：0.2987 - val_loss ：1.6073 - val_categorical_accuracy：0.0761

如上面的代码所示，我尝试将过采样与 stratifiedkfold 一起使用来尝试纠正这些数据不平衡。此外，我尝试了不同范围的时期、批量大小和学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</guid>
      <pubDate>Mon, 18 Dec 2023 23:56:24 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 管道中的标准缩放器有缺陷吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77682306/standard-scaler-in-sklearn-pipeline-has-a-flaw</link>
      <description><![CDATA[当管道在 pipeline.fit() 上训练管道时，标准缩放器将根据训练数据适合 std/meanDev，对吧？现在，如果测试数据与训练数据有很大不同...在 pipeline.predict() 上，相同的 std/meanDev 将应用于由标准缩放器在训练数据上提取的测试数据！这是 sklearn 管道中的一个巨大缺陷吗？
我一直在研究这个问题，我有两个不同的数据集来解决同一问题，当我在一个数据集上训练管道并在另一个数据集上测试它时，它给我所有肯定或所有否定的答案（二元分类），当我分别缩放管道外部的两个数据集，它开始给出正确的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77682306/standard-scaler-in-sklearn-pipeline-has-a-flaw</guid>
      <pubDate>Mon, 18 Dec 2023 23:54:16 GMT</pubDate>
    </item>
    <item>
      <title>对点击流进行分类并了解特定的点击流如何导致特定的用户行为操作？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77682089/categorize-stream-of-clicks-and-understand-how-a-specific-stream-of-clicks-resul</link>
      <description><![CDATA[我有来自汽车拍卖网站的点击流数据，现在我想对点击流进行分类，并了解特定的点击流如何导致特定的用户行为操作，例如查看特定汽车、搜索特定汽车、将汽车添加到监视列表，竞标各种汽车？]]></description>
      <guid>https://stackoverflow.com/questions/77682089/categorize-stream-of-clicks-and-understand-how-a-specific-stream-of-clicks-resul</guid>
      <pubDate>Mon, 18 Dec 2023 22:35:23 GMT</pubDate>
    </item>
    <item>
      <title>查找文本的所有标记概率[关闭]</title>
      <link>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</link>
      <description><![CDATA[我正在玩 ROBERTA，试图找到输入文本中不可能的部分。例如，我想潜在地检测奇怪的单词选择、不正确的语法等。
我当前的方法是循环序列中的所有标记，将它们交换为掩码标记，然后运行模型。然后我可以获得 logits 的 softmax，并查找实际标记的概率：
导入火炬
从变压器导入 RobertaTokenizer、RobertaForMaskedLM

def get_token_probabilities(句子、模型、分词器):

    token_ids = tokenizer.encode(sentence, return_tensors=&#39;pt&#39;)
    结果=[]

    for i in range(1, token_ids.size(1) - 1): # 排除开始和结束标记
        masked_token_ids = token_ids.clone()
        masked_token_ids[0, i] = tokenizer.mask_token_id

        使用 torch.no_grad()：
            输出=模型（masked_token_ids）

        softmax = torch.softmax(outputs.logits[0, i], dim=0)

        原始令牌 ID = 令牌 ID[0, i]
        token_prob = softmax[original_token_id].item()

        结果.追加({
            “令牌”：tokenizer.decode([original_token_id]),
            “概率”：token_prob，
        })

    返回结果


def main():
    model_name = &#39;罗伯塔基地&#39;
    tokenizer = RobertaTokenizer.from_pretrained(model_name)
    模型 = RobertaForMaskedLM.from_pretrained(model_name)

    例子= [
        “安和保罗正在准备考试。”,
        “安和保罗正在准备考试。”,
        “安和保罗正在为考试而学习。”,
    ]

    对于示例中的句子：
        结果 = get_token_probabilities(句子、模型、分词器)
        对于结果中的条目：
            print(f&#39;{entry[&quot;token&quot;]}: {entry[&quot;probability&quot;]:.6f}&#39;)
        打印（）

如果 __name__ == &#39;__main__&#39;:
    主要的（）

结果：
&lt;前&gt;&lt;代码&gt;安：0.007430
 和：0.982790
 保罗：0.012045
 研究：0.002692
 为：0.973533
 他们的：0.077359
 考试：0.008554
.: 0.691542

安：0.005499
 和：0.871503
 保罗：0.006089
 研究：0.006135
 为：0.954329
 那里：0.000004
 考试：0.008875
.: 0.545155

安：0.003608
 和：0.965940
 保罗：0.004815
 研究：0.000095
 为：0.009223
 他们：0.013043
是：0.000293
 考试：0.000020
.: 0.220154

问题是这种方法效率不高，因为它必须针对输入中的每个标记运行。有没有更好的方法来做到这一点，也许只运行模型一次？]]></description>
      <guid>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</guid>
      <pubDate>Mon, 18 Dec 2023 22:30:18 GMT</pubDate>
    </item>
    <item>
      <title>KeyError：“[Index(['names_of_categorical_columns'], dtype='object', name='name_of_index_column')] 都不在 [index] 中”</title>
      <link>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</link>
      <description><![CDATA[我编写这段代码是为了获取一个变量中的所有分类列，并使用 OrdinalEncoder() 对它们进行编码。
我的代码：
s = (X_train.dtypes == &#39;对象&#39;)
object_cols = 列表(s[s].index)

ordinal_encoder = OrdinalEncoder(handle_unknown = &#39;use_encoded_value&#39;,unknown_value = 999)

# 进行复制以避免更改原始数据
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# 将序数编码器应用于具有分类数据的每一列
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

我收到此错误（在问题标题中）并且无法处理它。您能告诉我如何解决这个问题吗？
我尝试使用 OrdinalEncoder() 对分类数据进行编码，然后再使用它来训练 XGBRegressor() 模型。]]></description>
      <guid>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</guid>
      <pubDate>Mon, 18 Dec 2023 19:59:08 GMT</pubDate>
    </item>
    <item>
      <title>precisionAtRecall 从 0 开始</title>
      <link>https://stackoverflow.com/questions/77681250/precisionatrecall-starts-with-0</link>
      <description><![CDATA[在使用张量流进行模型训练期间，我目睹了以下指标更新：
在此处输入图像描述
两个 precisionAtRecall 值分别对应召回值 0.8 和 0.7。我不明白为什么 1) precisionAtRecall_0.8 = 0 而 precisionAtRecall_0.7 &gt; 0？ 2) 为什么 precisionAtRecall_0.8 = 0 在 0 和看似合理的数字之间振荡？
我期望 precisionAtRecall_0.8 &lt; precisionAtRecall_0.7 但如果 precisionAtRecall_0.7 &gt; 非零0]]></description>
      <guid>https://stackoverflow.com/questions/77681250/precisionatrecall-starts-with-0</guid>
      <pubDate>Mon, 18 Dec 2023 19:09:02 GMT</pubDate>
    </item>
    <item>
      <title>微调朴素贝叶斯模型以进行文本分类（多类别结果）</title>
      <link>https://stackoverflow.com/questions/77681240/fine-tuning-naive-bayesian-model-for-text-classification-multi-categorical-outc</link>
      <description><![CDATA[我有一个包含数千个有关美国经济问题的 Reddit 帖子的数据集。
我们随机抽取了总帖子数 40% 的样本，其中每个帖子都包含“责备”内容。具有三个可能值“共和党”、“民主党”的指示符。或“美联储”它捕获了给定帖子中每个事件中谁受到指责。
我的目标是使用 40% 的人工注释帖子来预测“责备”的价值。我的数据集中剩余 60% 的帖子中存在变量。我看过一些研究论文使用朴素贝叶斯模型来预测分类变量，但我的案例的结果并不令人印象深刻。因此，我想知道这里使用的 BERT 模型是否更合适，或者我是否错误地运行了贝叶斯模型。
这是我的代码：
#加载包
图书馆（tidyverse）
需要（读xl）
需要（writexl）
图书馆（量子）
库（quanteda.textmodels）
库（插入符号）

#数据示例：
dput(reddit_corpus[1:5,c(1,2,3,4,8)])


输出：
结构(列表(id = c(1933, 7161, 4661, 2885, 5102), 用户名 = c(“the_dog”,
&quot;Empyrean Cobalt&quot;、&quot;Engineer&quot;、&quot;AuraKUPO&quot;、&quot;kyo_465&quot;), post = c(&quot;认为 jhk AT Pinoy 等都应该感谢 cecas 为他们减轻了压力&quot;,
“ScamDetector说我希望你们都意识到UOB实际上没有回答Clement的问题点击展开UOB只是使用相同的模板像机器人模式一样回复”，
“netzach 说这个 Daniels 的家伙点击展开我认为人们并没有完全理解 WP 和 Daniel Goh”，
“TS仍然是学生，公司必须以现金支付FT CPF，只有sinkies需要从工资中支付一部分CPF”，
“ponpokku 说至少他们不会像 CECA 那样降级，对当地人不会构成威胁 CECA ish 你必须加倍工作才能 kio 他们的 sai 单击展开我不想雇用那些告诉我有比他们更糟糕的人”
), date = c(&quot;2021-07-13 00:00:00 UTC&quot;, &quot;2020-09-22 00:00:00 UTC&quot;,
“2021-08-07 00:00:00 UTC”、“2021-04-07 00:00:00 UTC”、“2021-07-23 00:00:00 UTC”
), Collective_action = c(0, 0, 0, 0, 0)), row.names = c(NA, -5L
), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))

构建模型：reddit_corpus
#生成 1109 个数字（2913 的 38%），无需替换
设置种子(300)
id_train &lt;- 样本(1:2913, 1109, 替换 = FALSE)
头（id_train，10）

#获取训练集
dfmat_training &lt;- dfm_subset(dfmt_sing, id_numeric %in% id_train)

#获取测试集（id_train中没有的文档）
dfmat_test &lt;- dfm_subset(dfmt_sing, !id_numeric %in% id_train)

#使用textmodel_nb()训练朴素贝叶斯分类器
tmod_nb &lt;- textmodel_nb(dfmat_training, dfmat_training$responsibility_attribution)
摘要(tmod_nb)

dfmat_matched &lt;- dfm_match(dfmat_test, features = featnames(dfmat_training))

结果：
#检查分类器的工作情况
实际类 &lt;- dfmat_matched$responsibility_attribution
Predicted_class &lt;- 预测(tmod_nb, newdata = dfmat_matched)
tab_class &lt;- 表（实际类，预测类）
选项卡类

输出：
 预测类
实际类 1 2 3 4
           1 240 6 5 45
           2 67 2 1 7
           3 79 0 10 15
           4 128 6 5 75

混淆矩阵
#我们可以使用caret包中的函数confusionMatrix()来评估分类的性能
fusionMatrix(tab_class, mode = “一切”, Positive = “pos”)

总体统计
                                          
               准确度：0.4732
                 95% 置信区间：(0.4355, 0.5112)

                     类别：1 类别：2 类别：3 类别：4
灵敏度 0.4669 0.142857 0.47619 0.5282
特异性 0.6836 0.889217 0.85970 0.7468
预测值 0.8108 0.025974 0.09615 0.3505
负预测值 0.3063 0.980456 0.98126 0.8595
精度 0.8108 0.025974 0.09615 0.3505
召回率 0.4669 0.142857 0.47619 0.5282
F1 0.5926 0.043956 0.16000 0.4213
患病率 0.7438 0.020260 0.03039 0.2055
检出率 0.3473 0.002894 0.01447 0.1085
检测率 0.4284 0.111433 0.15051 0.3097
平衡精度 0.5753 0.516037 0.66795 0.6375
]]></description>
      <guid>https://stackoverflow.com/questions/77681240/fine-tuning-naive-bayesian-model-for-text-classification-multi-categorical-outc</guid>
      <pubDate>Mon, 18 Dec 2023 19:07:27 GMT</pubDate>
    </item>
    <item>
      <title>解释 AutoGluon 表格分类器的文本特征 [关闭]</title>
      <link>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</link>
      <description><![CDATA[我正在训练 AutoGluon 表格分类器。
我的所有数据都是表格格式，并且大多数特征都是数字。然而，训练中使用的一些特征是文本特征。是否可以解释文本特征如何影响预测？例如，我感兴趣的是是否有任何特定的单词会导致预测发生这样或那样的变化。我尝试使用 SHAP 的 KernelExplainer，但我不确定这是否是正确的方法或如何在这种情况下使用它。
我最初使用 AutoGluon 的 TabularPredictor.feature_importance，但这只能告诉我哪些特征在最终预测中权重最大。然而，权重最高的特征是文本特征。我无法找出 AutoGluon 在该功能中具体查看什么内容来进行预测，无论是特定单词还是其他单词。]]></description>
      <guid>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</guid>
      <pubDate>Mon, 18 Dec 2023 17:28:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>如何将 OHLCV 交易数据加载到 TensorflowJS 卷积层中？</title>
      <link>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</link>
      <description><![CDATA[我的目标
获取本周前 5 个股票交易日的高位数据，看看我是否可以预测周末价格上涨或下跌。
数据
在交易中，我们使用 OHLCV 数据来描述交易日的情况：
（O=当日开盘价，H=当日最高价，L=当日最低价，C=当日收盘价，V=当日成交量）。
//1天的数据
[o、h、l、c、v]

// 例如
[10、12、9、11、100]

因此 5 天的数据如下：
//5天的数据
[[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[ o、h、l、c、v]]

// 例如
[[10, 12, 9, 11, 100], [11, 13, 11, 12, 200], [12, 12, 11, 11, 150], [11, 14, 11, 14, 300], [ 14, 14, 11, 12, 200]]

我读到，处理这些数据的最佳方法是使用卷积层，特别是 Tensorflow 中的 conv2d 层。据我了解，该层需要 4dTensor 作为输入。因此，我使用tensor4d()从包含4周训练数据（4倍5天数据）的数据数组创建这样的张量
// 4 次 5 天的 javascript 数组
常量数据 = [10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200]

// 转换为 4d 张量
const input_tensor = tf.tensor4d(data, [data.length / (5 * 5), 5, 5, 1]);

目标由每周 1 个结果组成（周末交易的结果）。
const target = [-1, 2, -3, 1] // % 盈利或亏损

// 转换为一维张量
const target_tensor = tf.tensor1d(目标);

模型
我的模型看起来像这样，是根据此处找到的示例构建的：https:// blog.quantinsti.com/卷积神经网络/
const model = tf.sequential();
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：32，
    inputShape: [5, 5, 1], // 5 个 ohlcv，共 5 个值，每个值 1
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：64，
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.flatten());
model.add(tf.layers.dense({

    激活：&#39;relu&#39;，
    单位：64，
}));
model.add(tf.layers.dense({

    单位：1，
    激活：&#39;relu&#39;，
}));

我使用之前创建的张量训练模型
//训练模型
等待 model.fit(input_tensor, target_tensor, {
                    
    批量大小：32，
    纪元：50，
    随机播放：正确，
});

错误
在此步骤中，我收到以下错误
unhandledRejection: TypeError: 无法将 undefined 或 null 转换为对象

最后一步
model.predict([10, 12, 9, 11, 100], [5, 1])

问题

我使用卷积层是因为我想保留 1 天的 OHLCV 数据之间的关系，我的模型是否属于这种情况？有必要吗？
我正确创建了 4d 和 1d 张量吗？
内核大小为 [1, 1] 的卷积层是否有意义？似乎违背了卷积层的目的？
如何修复该错误？
我是走在正确的道路上还是完全看错了方向？在这种情况下，您能否为我指明如何构建模型的正确方向？
]]></description>
      <guid>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</guid>
      <pubDate>Mon, 18 Dec 2023 16:22:48 GMT</pubDate>
    </item>
    <item>
      <title>在手动交叉验证和 cross_val_score 之间获取不同的分数值</title>
      <link>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</link>
      <description><![CDATA[我创建了一个 python for 循环，将训练数据集分割成分层的 KFold，并在循环内使用分类器来训练它。然后使用经过训练的模型通过验证数据进行预测。使用此过程实现的指标与使用 cross_val_score 函数实现的指标完全不同。我期望使用这两种方法得到相同的结果。
此代码用于文本分类，我使用 TF-IDF 对文本进行矢量化
这是代码：
手动实现交叉验证的代码：
#导入指标函数来衡量模型的性能
从 sklearn.metrics 导入 f1_score、accuracy_score、 precision_score、recall_score
从 sklearn.model_selection 导入 StratifiedKFold
data_validation = [] # 用于存储使用交叉验证的模型验证结果的列表
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
准确度值 = []
f1_val = []

# 使用ravel函数将多维数组展平为一维
对于 (skf.split(X_train, y_train)) 中的 train_index、val_index：
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index]
    y_tr, y_val = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # 对训练折叠进行向量化
    X_val_vec_tfidf = tfidf.transform(X_val) # 对验证折叠进行向量化
    #实例化模型
    模型= MultinomialNB(alpha=0.5, fit_prior=False)
    #用我们的训练数据集训练空模型
    model.fit(X_tr_vec_tfidf, y_tr)
    Predictions_val = model.predict(X_val_vec_tfidf) # 使用验证数据集进行预测
    acc_val = 准确度_分数（y_val，预测_val）
    Accuracy_val.append(acc_val)
    f_val = f1_score（y_val，预测值）
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# 存储指标的临时列表
temp = [&#39;NaiveBayes&#39;]
temp.append(avg_accuracy_val) #验证准确率分数
temp.append(avg_f1_val) #验证f1分数
data_validation.append(临时)
#使用数据帧创建一个表，其中包含所有经过训练和测试的 ML 模型的指标
result = pd.DataFrame(data_validation, columns = [&#39;算法&#39;,&#39;准确度分数：验证&#39;,&#39;F1-分数：验证&#39;])
result.reset_index(drop=True, inplace=True)
结果

输出：
 算法准确度分数：验证 F1-Score：验证
0 天真的贝叶斯 0.77012 0.733994

现在使用 cross_val_score 函数的代码：
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.feature_extraction.text 导入 CountVectorizer、TfidfVectorizer
分数 = [&#39;准确度&#39;, &#39;f1&#39;]
#使用NLP技术TF-IDF对训练和测试数据集进行文本向量化
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=多项式NB(alpha=0.5, fit_prior=False)
对于 [“accuracy”, “f1”] 中的分数：
    print (f&#39;{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} &#39;)

输出：
准确度：0.7341283583255231
f1：0.7062017090972422

可以看出，使用这两种方法的准确性和 f1 指标有很大不同。当我使用 KNeighborsClassfier 时，指标的差异更加严重。]]></description>
      <guid>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</guid>
      <pubDate>Mon, 18 Dec 2023 16:05:36 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 二元分类的分布外样本和异常</title>
      <link>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</link>
      <description><![CDATA[我对机器学习领域比较陌生，并且对创建基本自定义模型有基本的了解。
分配给我的任务
我的经理要求我开发一种机器学习模型，能够在攻击性图像发送到服务器之前识别它们。这些图像可以分为不同的类别，例如武器或成人内容。
问题
但是，我在模型检测异常或分布外样本的能力方面遇到了问题。
我尝试过的
我尝试了不同的方法，利用&#39;binary_crossentropy&#39;作为我的损失函数，但遇到了同样的问题。我还构建了一个包含两个类的模型：

12,278 与武器相关的图片 class_name = 武器
3,000 张食物 商品图片 class_name = 其他

但是，我不确定 “其他” 类别中应包含哪些内容。使用 MobileNet 作为基础模型构建模型后，它成功地准确预测了武器，并提供了 0.4 到 0.6 范围内的食品预测，这似乎是可以接受的。然而，当我引入大象或汽车的图像时，模型倾向于将它们预测为武器，显示的置信度接近 1。
我不知道如何解决这个问题。我尝试研究这个问题，并发现了一些关于分布外检测的线索以及与异常或离群值相关的概念。当模型的输入包含不属于训练数据的图像时，如何获得不确定性值？
如果您能提供任何指导、建议，甚至参考能够有效解决此问题的视频或资源，我将不胜感激。感谢您的帮助。
数据加载：
train_ds, test_ds = keras.utils.image_dataset_from_directory(
    目录=“/内容/武器”，
    标签=“推断”，
    label_mode =“二进制”，
    批量大小=32，
    子集=“两者”，
    图像大小=(224,224),
    验证分割=0.2，
    随机播放=真，
    种子=1337
）

现在标准化输入图像数据：
def process（图像，标签）：
  图像=tf.cast(图像/255, tf.float32)
  返回图像、标签
train_ds = train_ds.map(进程)
test_ds = test_ds.map(进程)

创建 CNN 模型：
input_shape = (224,224,3)
mobilenet = MobileNet(input_shape,weights=&#39;imagenet&#39;,include_top=False)
模型=顺序（）
 
model.add（移动网络）
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（1，激活=&#39;sigmoid&#39;））
模型.summary()



sgd = SGD(学习率=0.0001，动量=0.9，nesterov=True)
model.compile（损失=&#39;binary_crossentropy&#39;，优化器=sgd，指标=[&#39;准确性&#39;]）
历史= model.fit（train_ds，validation_data = test_ds，batch_size = 4，epochs = 6）


纪元 6/6
382/382 [==============================] - 58s 150ms/步 - 损失：0.0042 - 精度：0.9984 - val_loss ：0.0063 - val_accuracy：0.997
]]></description>
      <guid>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</guid>
      <pubDate>Mon, 18 Dec 2023 14:34:38 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 1 个特征，但 LinearRegression 期望有 2 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</link>
      <description><![CDATA[我正在使用 pywebio 为我的机器学习程序创建一个小型脚本运行用户界面。当不使用小型 UI 时，运行线性回归 predict() 函数时不会出现任何错误。
UI 正在从用户处检索两个数字：&#39;age&#39; 和 &#39;salary&#39;。这两个数字被输入到一个 numpy 数组中，并且当我收到有关 numpy 数组形状的错误时，该 numpy 数组已从一维数组重新调整为二维数组。
现在，当 sklearn 文档指出线性回归 predict() 时，我收到一条有关 predict() 方法仅接收 1 个特征而不是 2 个特征的错误消息&gt; 方法总是获取“self”和另一个特征。我该如何修复这个错误？
这是我的用户界面代码：
age = int(input(“请输入您的年龄：”, type=NUMBER))
工资 = int(input(&quot;请输入您的工资：&quot;, type=NUMBER))

条目 = np.array([年龄, 薪水])
reshape_entry = Entry.reshape(-1, 1)

估计 = regr.predict(reshape_entry)

这是错误消息：
ValueError Traceback（最近一次调用最后一次）
输入 In [21], in ()

输入[21]，在retirement_ui()中

文件 ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:362，在 LinearModel.predict(self, X) 中
    348 def 预测（自身，X）：
    第349章
    350 使用线性模型进行预测。
    第351章
   （...）
    360 返回预测值。
    第361章
--&gt;第362章

文件〜\ anaconda3 \ lib \ site-packages \ sklearn \ Linear_model \ _base.py：345，在LinearModel._decision_function（self，X）中
    第342章
    第343章
--&gt; [第 345 章]
    [第 346 章]

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:585，在 BaseEstimator._validate_data(self, X, y, Reset, validate_separately, **check_params)
    第582章
    第584章
--&gt;第585章
    第587章 回来

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:400，在 BaseEstimator._check_n_features(self, X, Reset) 中
    第397章 回归
    第399章
--&gt;正文 正文_第400章
    [401] 第401话
    [402] f“期待{self.n_features_in_}特征作为输入。”
    第403章）

ValueError：X 有 1 个特征，但 LinearRegression 期望有 2 个特征作为输入。
]]></description>
      <guid>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</guid>
      <pubDate>Wed, 27 Jul 2022 04:29:07 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何最佳实践来为基于文本的分类准备特征？</title>
      <link>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</link>
      <description><![CDATA[我们收到了许多来自客户的反馈和问题报告。它们是纯文本。我们正在尝试为这些文档构建一个自动分类器，以便未来反馈/问题可以自动路由到正确的支持团队。除了文本本身之外，我认为我们应该将客户资料、案例提交区域等内容纳入分类器中。我认为这可以为分类器做出更好的预测提供更多线索。
目前，所有选择用于训练的特征都是基于文本内容的。如何包含上述元特征？
添加1
我目前的做法是首先对原始文本（包括标题和正文）进行一些典型的预处理，例如删除停用词、词性标记和提取重要词。然后，我将标题和正文转换为单词列表，并以某种稀疏格式存储它们，如下所示：
&lt;块引用&gt;
实例 1：单词 1：单词 1 计数，单词 2：单词 2 计数，...
实例 2：wordX:word1 计数，wordY:word2 计数，...

对于其他非文本功能，我计划将它们添加为单词“columns”之后的新列。所以最终的实例将如下所示：
&lt;块引用&gt;
实例 1：word1:word1 计数，...，特征 X:值，特征 Y:值
]]></description>
      <guid>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</guid>
      <pubDate>Fri, 28 Feb 2014 05:57:21 GMT</pubDate>
    </item>
    </channel>
</rss>