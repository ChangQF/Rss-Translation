<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Dec 2023 18:17:49 GMT</lastBuildDate>
    <item>
      <title>在实现正则化值的梯度下降代码时出现 TypeError</title>
      <link>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</link>
      <description><![CDATA[“我的代码（来自 coursera）”
defgradient_desc(X, Y, w_in, b_in, cost_f, grad_f, alp, num, lambda_):
m = len(X)
# 一个数组，用于存储每次迭代的成本 J 和 w，主要用于稍后绘图
J_历史=[]
w_历史= []
对于范围内的 i（num）：
# 计算梯度并更新参数
dj_db, dj_dw = grad_f(X, Y, w_in, b_in, lambda_)
# 使用 w、b、alpha 和梯度更新参数
w_in = w_in - 阿尔法 * dj_dw
b_in = b_in - alpha * dj_db
# 每次迭代时保存成本 J
if i&lt;100000: # 防止资源耗尽
J_history.append(cost_f(X, Y, w_in, b_in, lambda_))
# 每隔 10 次或多次迭代打印成本 if &lt; 10
如果 i% math.ceil(num/10) == 0:
w_history.append(w_in)
print(f&quot;迭代 {i:4}: 成本 {float(J_history[-1]):0.2e} &quot;)
return w_in, b_in, J_history, w_history #return w 和 J,w 历史记录用于绘图

“错误”
类型错误：float() 参数必须是字符串或实数，而不是“NoneType”
这是整个代码中唯一有 bug 的部分（至少到目前为止是有 bug 的）。我想要解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</guid>
      <pubDate>Thu, 07 Dec 2023 18:03:29 GMT</pubDate>
    </item>
    <item>
      <title>如何获得分割图像的具体顺序？</title>
      <link>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</link>
      <description><![CDATA[我正在开展一个项目，旨在预测电阻值。为了进行预测，我需要知道电阻器的色带。使用图像分割来检测条带。我可以做到 98% 的准确率。但是，要知道电阻的值。我需要颜色的具体顺序。我怎样才能获得这些数据？
我尝试使用多类逻辑回归。结果很糟糕。人工神经网络能够找到数组中的第一个色带 (row_0)，成功率为 40%。比随机更好，因为有 11 种可能的颜色。
用于 ANN 和回归模型的数据包括像素值 (x,y)、波段的颜色和第一个波段的标签。我还对数据进行了标准化。标签和类别值的范围为 1-11。因为这是电阻值的颜色范围。
训练数据的第一行
图像如何分割
如有任何帮助，我们将不胜感激。谢谢:)]]></description>
      <guid>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</guid>
      <pubDate>Thu, 07 Dec 2023 17:55:10 GMT</pubDate>
    </item>
    <item>
      <title>实施跟踪器以在视频帧中保留汽车的 OCR 结果 [关闭]</title>
      <link>https://stackoverflow.com/questions/77621885/implementing-tracker-to-persist-ocr-results-for-cars-in-video-frames</link>
      <description><![CDATA[我有一个 Python 函数 detect_and_draw_cars 在循环中使用并给定视频帧，它利用 YOLO 来识别并绘制视频每一帧中汽车周围的边界框。此外，该函数还会对这些边界框中的感兴趣区域（如果找到）执行 OCR，以提取车牌（函数 box4）。
为了优化 OCR 处理，我希望实现一个 OpenCV 跟踪器，它可以在多个帧上跟踪汽车的中心。我们的目标是每辆车每 60 帧左右更新一次 OCR 结果，因为汽车的形状和方向随着时间的推移逐渐变化，我正在考虑通过它们的中心跟踪图像。
这是现有代码的片段：
# [现有代码]
def detector_and_draw_cars(图像,frame_count):
    汽车图像，汽车坐标= yolo_predire（图像）
    对于 i，(x1，y1，x2，y2) 在 enumerate(car_coordinates) 中：
        结果=[]

        cv2.矩形(图像, (x1, y1), (x2, y2), (0, 255, 0), 2)
        car_roi = 图像[y1:y2, x1:x2]

        结果 = box4(car_roi)
        如果结果：
          打印（结果）
          ocr_text = f&quot;{结果[0][-2]} - {(结果[0][-1]*100):.2f}%&quot;
          text_size, _ = cv2.getTextSize(ocr_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 1)
          cv2.矩形(图像, (x1, y1 - 文本大小[1] - 10), (x1 + 文本大小[0] + 10, y1), (255, 255, 255), -1)
          cv2.putText(图像, ocr_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)

    返回图像
]]></description>
      <guid>https://stackoverflow.com/questions/77621885/implementing-tracker-to-persist-ocr-results-for-cars-in-video-frames</guid>
      <pubDate>Thu, 07 Dec 2023 17:09:04 GMT</pubDate>
    </item>
    <item>
      <title>在这种情况下如何计算标准差（或置信区间）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77620366/how-to-calculate-standard-deviation-or-confidence-interval-in-this-case</link>
      <description><![CDATA[对于同一分类（二元）任务，我现在有五个独立的分类结果。我可以计算每个预测的 AUC（受试者工作特征曲线下面积），并且可以通过 Delong 方法获得置信区间 [1]。现在我需要通过平均这五个 AUC（例如，使用箱线图）来显示我的结果，如何计算平均 AUC 的标准差（或置信区间）。
[1] DeLong, E. R.、D. M. DeLong 和 D. L. Clarke-Pearson (1988)。 “比较两个或多个相关接收器工作特性曲线下的面积：非参数
我有两种计算方法，但不知道是否正确。 (1)使用5个AUC值的样本标准差； （2）使用每个AUC的标准差的均方根（最后需要除以5）。我更喜欢第二种选择，因为我可以将每个 AUC 视为随机变量并使用 Var((X+Y)/2)=(Var(X)+Var(Y))/4 等公式。]]></description>
      <guid>https://stackoverflow.com/questions/77620366/how-to-calculate-standard-deviation-or-confidence-interval-in-this-case</guid>
      <pubDate>Thu, 07 Dec 2023 13:23:54 GMT</pubDate>
    </item>
    <item>
      <title>Spacy 从上次训练的模型中重新训练（在线学习）</title>
      <link>https://stackoverflow.com/questions/77620043/spacy-retrain-from-last-trained-models-online-learning</link>
      <description><![CDATA[我正在使用 SpaCy 的 TextCatCNN 来解决文本分类问题。我刚刚准备好训练、测试和验证数据，并开始使用训练命令生成的默认配置文件训练模型：
python -m spacy init config --pipeline textcat ;

&lt;前&gt;&lt;代码&gt;[路径]
火车=空
开发=空
向量=空
init_tok2vec = null

[系统]
gpu_分配器=空
种子 = 0

[自然语言处理]
lang =“en”；
管道 = [“textcat”]
批量大小 = 1000
禁用 = []
创建之前 = null
创建后=空
after_pipeline_creation = null
tokenizer = {“@tokenizers”：“spacy.Tokenizer.v1”}
向量 = {“@vectors”：“spacy.Vectors.v1”}

[成分]

[组件.textcat]
工厂=“textcat”
记分器 = {“@scorers”：“spacy.textcat_scorer.v2”}
阈值 = 0.0

[组件.textcat.模型]
@architectures = “spacy.TextCatBOW.v2”
独占类 = true
ngram_size = 1
无输出层 = false
nO = 空

[语料库]

[语料库.dev]
@readers =“spacy.Corpus.v1”
路径 = ${paths.dev}
最大长度=0
gold_preproc = false
限制 = 0
增强器 = null

[语料库.train]
@readers =“spacy.Corpus.v1”
路径 = ${paths.train}
最大长度=0
gold_preproc = false
限制 = 0
增强器 = null

[训练]
dev_corpus = “corpora.dev”;
train_corpus = “语料库.train”;
种子 = ${系统.种子}
gpu_allocator = ${system.gpu_allocator}
辍学率 = 0.1
累积梯度= 1
耐心=1600
最大纪元 = 0
最大步数 = 20000
评估频率 = 200
冻结组件 = []
注释组件 = []
before_to_disk = null
更新前=空

[训练.batcher]
@batchers = “spacy.batch_by_words.v1”
丢弃尺寸过大= false
公差 = 0.2
获取长度=空

[训练.batcher.大小]
@schedules =“compounding.v1”；
开始 = 100
停止=1000
化合物 = 1.001
t = 0.0

[训练记录器]
@loggers = “spacy.ConsoleLogger.v1”
进度条=假

[训练.优化器]
@optimizers =“Adam.v1”；
贝塔1 = 0.9
贝塔2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
梯度剪辑 = 1.0
使用平均值 = false
每股收益 = 0.00000001
学习率 = 0.001

[训练.score_weights]
猫得分 = 1.0
cats_score_desc = null
cats_micro_p = null
cats_micro_r = null
cats_micro_f = null
cats_macro_p = null
cats_macro_r = null
cats_macro_f = null
cats_macro_auc = null
cats_f_per_type = null

[预训练]

[初始化]
向量 = ${paths.向量}
init_tok2vec = ${paths.init_tok2vec}
词汇数据=空
查找=空
before_init = null
after_init = null

[初始化.组件]

[初始化.tokenizer]

评估后，我得到了一些指标，一切正常。当时间流逝并生成新的数据样本时，问题就出现了（我通过生成两个训练集来模拟它）。我只想“更新权重”我的模型，以便包含新数据的方差。这称为在线学习。
我尝试通过在 [components.textcat] 中将 factory = &quot;textcat&quot; 更改为 source = &quot;/model-best&quot;  部分，但是当使用第二组数据进行训练时，损失函数值等于第一个训练过程，因此它没有执行在线学习。此外，当使用第二组数据进行预测时，指标相同或更差。
如果可以的话，我如何使用 SpaCy 的 TextCatCNN 进行在线学习？]]></description>
      <guid>https://stackoverflow.com/questions/77620043/spacy-retrain-from-last-trained-models-online-learning</guid>
      <pubDate>Thu, 07 Dec 2023 12:35:14 GMT</pubDate>
    </item>
    <item>
      <title>如何提高噪声数据集的分类精度（模型的鲁棒性）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77619251/how-to-improve-classification-accuracy-robustness-of-model-on-a-noisy-dataset</link>
      <description><![CDATA[我有一个分类问题，我为此开发了以下模型。 1. kNN 2. SVM 3. MLP 和 4. 逻辑回归。现在我应该研究当数据集被随机高斯噪声破坏时算法的鲁棒性。
我在数据样本中添加了噪声，所有算法的性能在存在噪声的情况下都会下降，4 种算法之间存在一些细微的差异。现在，我应该通过一些方法来提高这个嘈杂数据集的分类准确性，因为在现实世界中，数据会很嘈杂。我对提高模型对噪声的鲁棒性的技术完全一无所知。]]></description>
      <guid>https://stackoverflow.com/questions/77619251/how-to-improve-classification-accuracy-robustness-of-model-on-a-noisy-dataset</guid>
      <pubDate>Thu, 07 Dec 2023 10:24:29 GMT</pubDate>
    </item>
    <item>
      <title>自定义 haar 级联无法正确检测对象</title>
      <link>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</link>
      <description><![CDATA[我想创建一个 Haar 级联用于舌头检测。我已尝试使用 Cascade Trainer GUI（版本 3.3.1）构建 Haar 级联 40 多次，但它无法正确检测。我怎样才能做到这一点？
我创建了一个用于存放舌头图像（正样本“p”）的文件夹和另一个用于存放没有舌头的常见图像（负样本“n”）的文件夹，每个文件夹包含 150 个样本图像。一切都运行完美，没有任何错误，但测试图像没有检测到舌头。]]></description>
      <guid>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</guid>
      <pubDate>Thu, 07 Dec 2023 06:00:31 GMT</pubDate>
    </item>
    <item>
      <title>多输出模块的 Keras 精度不会改变</title>
      <link>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</link>
      <description><![CDATA[我想预测对欺诈案件的处罚/处罚，输入格式为（损害金额（$），如果是累犯），目标格式为（罚款（$），监狱（月），社区服务（小时），缓刑（月）） - 以下是我的代码：
fname = “文件路径.tsv”
全部输入 = []
所有输出 = []

将 open(fname) 作为 f：
    对于 i，enumerate(f) 中的行：
        如果我&lt; 3:#第一行
            print(&quot;标题:&quot;, line.strip().split(&#39;\t&#39;))
            继续
        fields = line.strip().split(&#39;\t&#39;)
        all_in.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[5:7]])
        all_out.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[7:11]])

case_in = np.array(all_in, dtype = “uint64”)
target_out = np.array(all_out, dtype = “uint64”)

Normalize_layer = tf.keras.layers.Normalization(axis=-1, name = “normalize_in”)
Normalize_layer.adapt(all_in)
Normalize_out = tf.keras.layers.Normalization(axis=-1, name = “normalize_out”)
Normalize_out.adapt(all_out)
denormalize_out = tf.keras.layers.Normalization(axis=-1, invert = True, name = “denormalize_out”)
denormalize_out.adapt(all_out)
缩放输出 = 标准化输出（全部输出）

输入= Normalize_layer（输入（形状= 2））
x = 密集（6，input_dim = 2，激活=“sigmoid”，use_bias = True）（输入）
x = 密集(4, 激活 = “sigmoid”)(x)
y_4 = 密集(1, 激活 = “sigmoid”, 名称 = “y_4”)(x)
惩罚 = Dense(3, 激活 = “sigmoid”, 名称 = “惩罚”)(x)
y_1 = 密集（1，激活=“sigmoid”，名称=“y_1”）（惩罚）
y_2 = 密集（1，激活=“sigmoid”，名称=“y_2”）（惩罚）
y_3 = 密集（1，激活=“sigmoid”，名称=“y_3”）（惩罚）

模型 = 模型（输入 = 输入，输出 = [y_1，y_2，y_3，y_4]）

模型.编译(
    优化器= SGD(learning_rate=0.01,weight_decay=1e-6,momentum=0.9,nesterov=True),
    损失={
        “y_1” ：“均方误差”，
        “y_2” ：“均方误差”，
        “y_3” ：“均方误差”，
        “y_4” ：“均方误差”
    },
    指标=[&#39;准确性&#39;]
）

模型.拟合(
    案例输入，
    横向扩展，
    batch_size = 10,##数据增加后，增加
    纪元 = 300，
    详细 = 2,
    验证分割= 0.8
）

对于 model.layers 中的图层：
    print(&quot;=====图层:&quot;, 图层名称,&quot;=====&quot;)
    if layer.get_weights() != []:
        权重=layer.get_weights()[0]
        偏差=layer.get_weights()[1]
        print(&quot;权重：&quot;)
        打印（权重）
        print(“偏差：”)
        打印（偏差）
    别的：
        print(&quot;权重：&quot;, [])

而且，一旦开始训练模型，准确性就根本不会改变。另外，当我试图解决这个问题时，我搞砸了一些东西，使大约 50 个数据集的数据只是 .fit 操作的一批，尽管我的批量大小为 10。我只是尝试了太多的事情来修复我的代码互联网上到处都是，这让一切变得更加混乱。
最初，我尝试对数据进行标准化，包括我的来龙去脉，这解冻了我的损失函数，但它并没有真正对准确性函数产生任何影响。我还将 ReLU 函数也更改为 sigmoid 函数，因为在某些时候，死 ReLU 似乎可能存在问题，因为我的层上的偏差没有从最初的 0 更新。之后，我不断地研究优化器、损失函数和纪元，但都无济于事。
如何让模块真正发挥作用？您对代码有一般反馈吗？]]></description>
      <guid>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</guid>
      <pubDate>Thu, 07 Dec 2023 05:51:59 GMT</pubDate>
    </item>
    <item>
      <title>从 ampligraph 导入复杂数据时如何修复此错误 ImportError</title>
      <link>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</link>
      <description><![CDATA[ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-41-449fec1eb93c&gt;在&lt;细胞系：28&gt;()
     26 从 imblearn.over_sampling 导入 RandomOverSampler、SMOTE、ADASYN
     27 从 imblearn.under_sampling 导入 ClusterCentroids、RandomUnderSampler、NearMiss、TomekLinks
---&gt; 28 从 ampligraph.latent_features 导入 ComplEx
     29

ImportError：无法从“ampligraph.latent_features”导入名称“ComplEx”（/usr/local/lib/python3.10/dist-packages/ampligraph/latent_features/__init__.py）

我尝试降级放大器来修复它，但它不起作用。我也尝试阅读新版本的文档，但我不知道，因为我是新手。]]></description>
      <guid>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</guid>
      <pubDate>Thu, 07 Dec 2023 04:28:22 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“Flags”对象没有属性“c_contigious”</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在阅读 Aurélien Géron 编写的《机器学习实践》一书，但遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(“int”) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

＃模型
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
&lt;前&gt;&lt;代码&gt;{
AttributeError: &#39;Flags&#39; 对象没有属性 &#39;c_contigious&#39;”
}

由于我正在关注这本书，所以我希望这段代码能够工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>如何将从 ListFromFiles 创建的 Tensorflow 数据集划分为训练集、验证集和测试集？</title>
      <link>https://stackoverflow.com/questions/77615270/how-can-i-divide-a-tensorflow-dataset-created-from-listfromfiles-into-train-val</link>
      <description><![CDATA[我有一个目录，其中包含我想要分成训练集、验证集和测试集的文件。我采取的方法是：
# 定义正负路径
POS = os.path.join(“*.txt”)

# 创建正数据集
pos = tf.data.Dataset.list_files(POS)

如果我尝试：
&lt;前&gt;&lt;代码&gt;位置[3]

我得到：
TypeError：“_ShuffleDataset”对象不可下标

我曾想过使用 take &amp; 进行拆分跳过 - 例如
train = pos.take(10)
有效 = pos.skip(10).take(5)
测试 = pos.skip(15).take(5)

但是，当我在包含 10 个文本文件 (a-j) 的示例目录中尝试以下测试时：
导入操作系统
将张量流导入为 tf

# 定义正向和负向路径
POS = os.path.join(“*.txt”)

# 创建数据集
pos = tf.data.Dataset.list_files(POS)#.shuffle(len(POS))

对于范围（20）内的 _：
    对于 pos.take(1) 中的 x：
       打印（x）

我得到这个输出：
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./b.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./h.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./e.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./f.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./f.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./h.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./i.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./b.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./j.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./d.txt&#39;, shape=(), dtype=string)

由于目录中只有 10 个 txt 文件，我预计会看到相同的输出 20x、10 个不同文件的列表（后跟崩溃）或 2 个 10 个不同文件的列表。相反，take 似乎每次都在处理一个新的列表。因此，我使用 skip 和 take 的想法很可能最终会在我的集合之间出现重叠。
我唯一能想到做的就是在创建数据集之前拆分文件列表。但是，必须有某种方法从数据集中获取切片。 切片 TF 数据 中的方法对我不起作用。我在 take 的文档中没有看到任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/77615270/how-can-i-divide-a-tensorflow-dataset-created-from-listfromfiles-into-train-val</guid>
      <pubDate>Wed, 06 Dec 2023 17:47:52 GMT</pubDate>
    </item>
    <item>
      <title>基于相同输入数据的并行或共享回归网络会更好吗？为什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</link>
      <description><![CDATA[我将多个并行回归网络组合成一个模型，其中组合输出以创建单个损失函数。这些网络正在寻找相同数据的不同方面，并同时进行训练。这可以被认为是一个基于物理的神经网络。
本能地，我想说，分割网络允许每个网络拥有自己的权重，而不受其他方面的干扰，这将加快训练速度和/或提高性能。 ChatGPT 似乎证实了我的怀疑，但无法给我任何来源。
有人有任何论文/证明或知道这两种方法的更具体术语吗？我只是真的不知道如何提出这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</guid>
      <pubDate>Wed, 06 Dec 2023 17:29:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tidymodels 框架中的 Youden 索引获取 ML 性能指标？</title>
      <link>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</link>
      <description><![CDATA[如何使用 tidymodels 框架中的 Youden 索引获取机器学习性能指标？
我在网上搜索过示例，但没有找到。]]></description>
      <guid>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</guid>
      <pubDate>Wed, 06 Dec 2023 16:12:03 GMT</pubDate>
    </item>
    <item>
      <title>考虑到之前的输出，是否可以在 keras 中自定义回归损失函数</title>
      <link>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</link>
      <description><![CDATA[我正在构建 CNN 图像回归模型。我想在损失函数中添加一个惩罚项，以惩罚当前预测是否大于先前的期望。
有没有办法使用 Keras 编写代码而不是使用 tf.GradientTape()？
（这个帖子看起来和我想问的很相似
使用 Gradient Tape 的自定义损失函数，TF2.6)&lt; /p&gt;
这是数据描述。
我的数据是视频图像集，我想预测异常事件发生之前还剩多少次。
因此，图像标签应该尽可能地减少到异常标签。
（例如，异常的标签为0，前一张图像为1，前两张图像，该图像标签为2。）
因此，我的预测也会减少，如果当前预测高于之前的预测，我想添加惩罚项。
这是模型架构代码。我使用预训练的 VGG16 模型并将其改编为回归模型。
initial_model = tf.keras.applications.VGG16(weights = &#39;imagenet&#39;,include_top = False)
初始模型.trainable = False

func_model_p = keras.Sequential()
输入 = keras.Input(形状 = (700,100,3))
func_model_p.add(输入)

func_model_p.add(keras.layers.GlobalAveragePooling2D())
func_model_p.add(keras.layers.Dense(1,激活=“线性”))

# 准备指标。
train_mse_metric_p = keras.metrics.MeanSquaredError()
val_mse_metric_p = keras.metrics.MeanSquaredError()

此代码适用于 tf.GradientTape。我想为 Keras 更改此代码。
&lt;前&gt;&lt;代码&gt;
对于范围内的 e（纪元）：
    对于范围内的 i(len(y_train_d_noshuffle))：
        使用 tf.GradientTape() 作为磁带：

            图像 = np.expand_dims(X_train_d_noshuffle[i], axis=0)
            y_hat = func_model_p(图像, 训练=True)

            先前值 = 先前列表[-1]
            Violation_term = tf.constant(max( (y_hat - previous_value) , 0), dtype=tf.float32)

            尝试：
                如果 y_train_d_noshuffle[i] &lt; y_train_d_noshuffle[i+1]：
                    违规项 = 0
            除外：通过

            y_train_c = tf.constant(y_train_d_noshuffle[i])

            mse = tf.keras.losses.MeanSquaredError()(y_train_c, y_hat)
            # 计算损失
            损失值 = mse + 违规项
            previous_list.append(y_hat)

            train_mse_sum = train_mse_sum + mse
            train_penalty_sum = train_penalty_sum + Violation_term
        
        grads = Tape.gradient(loss_value, func_model_p.trainable_weights)
        优化器.apply_gradients(zip(grads, func_model_p.trainable_weights))

        # 更新训练指标。
        train_mse_metric_p.update_state(y_train_d_noshuffle[i], y_hat)

        # 显示每个时期结束时的指标。
        train_mse = train_mse_metric_p.result().numpy()

    print(f&#39;epochs: {e}, train_mse_sum: {train_mse_sum}, train_penalty_sum: {float(train_penalty_sum)}&#39;) ```



如果您知道路请告诉我！

先感谢您！
]]></description>
      <guid>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</guid>
      <pubDate>Mon, 04 Dec 2023 22:05:42 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 S3 中保存的 recordio protobuf 数据训练 sagemaker 模型</title>
      <link>https://stackoverflow.com/questions/77518065/cant-train-sagemaker-model-using-recordio-protobuf-data-saved-in-s3</link>
      <description><![CDATA[我想使用 PySpark 在 Amazon EMR 中预处理数据，并使用管道模式在 SageMaker 中训练机器学习模型。我现在遇到的问题是将数据保存在 S3 中并将其输入模型。
SageMaker 模型接受 application/x-recordio-protobuf 类型。因此，我将数据保存为：
output_path = f“s3://my_path/output_processed”
df_transformed.write.format(“sagemaker”).mode(“覆盖”).save(output_path)

其中df_transformed是一个pyspark数据帧。
当我尝试将数据输入模型时：
records = RecordSet(s3_data=train_path, s3_data_type=&#39;S3Prefix&#39;, num_records=-1, feature_dim=50) rcf​​.fit(records)

我收到此错误：
&lt;前&gt;&lt;代码&gt;失败。原因：客户端错误：无法读取数据通道“train”。请求的内容类型是“application/x-recordio-protobuf”。请验证数据是否与请求的内容类型匹配。 （由 MXNetError 引起）

你知道我做错了什么吗？是否有必要在 EMR 中单独预处理数据并在 SageMaker 中进行训练，还是我可以在 SageMaker 中完成所有操作？ （考虑到成本）。
我遵循的教程：https://aws.amazon.com/blogs/machine-learning/using-pipe-in​​put-mode-for-amazon-sagemaker-algorithms/]]></description>
      <guid>https://stackoverflow.com/questions/77518065/cant-train-sagemaker-model-using-recordio-protobuf-data-saved-in-s3</guid>
      <pubDate>Mon, 20 Nov 2023 18:10:59 GMT</pubDate>
    </item>
    </channel>
</rss>