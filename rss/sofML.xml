<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 28 Jul 2024 12:26:32 GMT</lastBuildDate>
    <item>
      <title>寻找潜变量模型的水平曲线</title>
      <link>https://stackoverflow.com/questions/78803562/finding-level-curves-for-a-latent-variable-model</link>
      <description><![CDATA[我有一个潜在变量模型，它使用两个高斯分布 p(z) 和 p(x|z) 表示 p(x)。p(x|z) 的均值和协方差由使用神经网络的 z 的一些函数表示。
注意：x 和 z 都是二维的。因此函数 p(x) 定义在平面上，其水平曲线将是平面上的曲线。
问题：
我想找到对应于 p(x) = 1 的 p(x) 的水平曲线（轮廓）。
有哪些（有效的）方法可以做到这一点？
到目前为止，我的思考过程：

以某种方式使用 matlab 等内置包中的轮廓查找函数。
从 p(x) 进行某种形式的采样以获得水平曲线。可能的方向：从分布 1 / (1 - p(x)) 中抽样，这将生成接近所需轮廓的高概率样本，然后使用某种方法从采样点近似曲线本身。
]]></description>
      <guid>https://stackoverflow.com/questions/78803562/finding-level-curves-for-a-latent-variable-model</guid>
      <pubDate>Sun, 28 Jul 2024 10:54:25 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：PdfReader.getNumPages（）缺少 1 个必需的位置参数：“self”[重复]</title>
      <link>https://stackoverflow.com/questions/78803533/typeerror-pdfreader-getnumpages-missing-1-required-positional-argument-self</link>
      <description><![CDATA[从 os 导入 listdir
从 os.path 导入 isfile，join
导入 PyPDF2
从 PyPDF2 导入 PdfReader
path=&#39;C:/Users/fc/Supreme_court.pdf/&#39;
files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and join(path, f).endswith(&#39;.pdf&#39;)]
for f in files:
with open(f, &#39;rb&#39;) as file_handle:
pdf_reader = PyPDF2.PdfReader(file_handle, strict=False)
page_text = &#39;&#39;
for page_num in range(0,PdfReader.getNumPages()):
page = pdf_reader.getPage(page_num)
page_text += page.extract_text()
# 将纯文本字符串写入同名文件
with open(f.replace(&#39;.pdf&#39;, &#39;.txt&#39;), &#39;a+&#39;) as text_file_handle:
text_file_handle.writelines(page_text)
TypeError: PdfReader.getNumPages() 缺少 1 个必需的位置参数：&#39;self&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78803533/typeerror-pdfreader-getnumpages-missing-1-required-positional-argument-self</guid>
      <pubDate>Sun, 28 Jul 2024 10:39:10 GMT</pubDate>
    </item>
    <item>
      <title>EMA 衰减和 LR 衰减之间的实际差异</title>
      <link>https://stackoverflow.com/questions/78803473/practical-difference-between-ema-decay-and-lr-decay</link>
      <description><![CDATA[我无法理解 EMA 衰减和 LR 衰减在实践中的差异。
我觉得它们都以不同的方式完成了相同的事情（以下内容可能是错误的，所以我提前道歉，如果我的理解完全错误，请纠正我）：

使用 EMA，在训练期间保留模型的单独副本，并且每 N 步使用原始模型权重的平均值更新模型。
使用 LR 衰减，原始模型的权重在训练期间总是更新较少，但只有一个模型得到有效训练。

现在，给定一个包含 32 个样本的数据集，我可以想象这是两次训练的方式：
训练 A（无 EMA）
给定以下超参数：

LR 1e-5
批次大小为 4
线性调度程序

经过 4 个步骤后，模型将看到 16 个样本，LR 将下降到最终 LR 的一半。
实际上，模型已更新 4 次。
训练 B (EMA)
给定以下超参数：

LR 为 1e-5
批次大小为 1
恒定调度程序
EMA 衰减为 0.9999
EMA 更新步骤为 4

经过 16 个步骤后，模型将看到 16 个样本，EMA 衰减将上升到最终 EMA 的一半衰减。
实际上，原始模型已更新 16 次，EMA 模型已更新 4 次。
问题
最终，两个模型都更新了 4 次，我能看到的唯一区别是训练 A 直接更新了权重，而训练 B 更新了原始模型和 EMA 模型的权重。
为什么人们决定选择训练 B 而不是训练 A？]]></description>
      <guid>https://stackoverflow.com/questions/78803473/practical-difference-between-ema-decay-and-lr-decay</guid>
      <pubDate>Sun, 28 Jul 2024 10:00:34 GMT</pubDate>
    </item>
    <item>
      <title>Runge-Kutta 求解器无法取得进展（RuntimeWarning：迭代没有取得良好进展）</title>
      <link>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</link>
      <description><![CDATA[我收到此错误：RuntimeWarning：迭代没有取得良好进展，以过去十次迭代的改进来衡量。
我目前正在尝试从工业制冷系统的数字孪生构建自定义 RL 环境。我正在模拟特定时间步骤内的过程，并在一段时间内（即 24 小时）循环执行该步骤。
我尝试更改 RK 求解器的 time_step_methods 数量和 configure 方法中的 time_step_minutes。可能是我的动作空间/观察空间或奖励函数定义不明确？
我的代码片段：
class RefrigerationEnv(gym.Env):
def __init__(self, num_evaporators=7, time_step_seconds=10, time_step_minutes=15):
super(RefrigerationEnv, self).__init__()

self.num_evaporators = num_evaporators
self.time_step_seconds = time_step_seconds
self.time_step_minutes = time_step_minutes

tz = dateutil.tz.gettz(&quot;America/Los_Angeles&quot;)
self.start_datetime = datetime(2024, 5, 12, 10, 0, 0, tzinfo=tz)

self.end_datetime_episode = self.start_datetime + timedelta(hours=12)
self.simulation_time = self.start_datetime # 初始化模拟时间

# 动作空间
self.action_space = space.Box(
low=np.array([100, -1, 20] + [-15] * num_evaporators), # 每个设定点的下限
high=np.array([180, 12, 80] + [10] * num_evaporators), # 每个设定点的上限
dtype=np.float32
)

# 观察空间
self.observation_space = space.Dict(
{
&quot;refrigerator_temp__c&quot;: space.Box(low=-30, high=10, shape=(1,), dtype=np.float64),
&quot;energy_consumption&quot;: space.Box(low=0.0, high=400.0, shape=(1,), dtype=np.float64),
&quot;ambient_temp&quot;: space.Box(low=-10, high=50, shape=(1,), dtype=np.float64),
&quot;time_of_day&quot;: space.Box(low=0, high=24, shape=(1,), dtype=np.float64),
}
)

# 初始化模拟器
self.simulator = configure_example_simulation_from_path(
process_config_path=Path(&quot;simulation_payload/simple_flowsheet_config.json&quot;),
control_config_path=Path(&quot;simulation_payload/simple_control_config.yaml&quot;),
disorders=self.create_disturbances(),
settings=self.create_settings(),
start_datetime=self.start_datetime,
end_datetime=self.start_datetime + timedelta(minutes=self.time_step_minutes), # 模拟 1 小时
solver=RK12(max_step__s=self.time_step_seconds,relative_tolerance=0.00001),
)
]]></description>
      <guid>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</guid>
      <pubDate>Sun, 28 Jul 2024 08:21:22 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 文档解释</title>
      <link>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</link>
      <description><![CDATA[我是 ML 新手，正在阅读 https://xgboost.readthedocs.io/en/stable/tutorials/model.html 上的 XGBoost 信息
我无法弄清楚下面屏幕截图中的方程式如何进入下一步。
在此处输入图片描述
感谢大家的帮助！
我试图扩展方程式来思考为什么事情会这样发展，但我仍然不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</guid>
      <pubDate>Sun, 28 Jul 2024 08:17:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Azure Vision AI 的预训练模型分析货架图像，同时检测货架中的物体和间隙</title>
      <link>https://stackoverflow.com/questions/78802566/detect-objects-gaps-in-a-shelf-while-analyzing-shelf-images-using-pretrained-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78802566/detect-objects-gaps-in-a-shelf-while-analyzing-shelf-images-using-pretrained-m</guid>
      <pubDate>Sat, 27 Jul 2024 22:14:06 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 yolo [关闭]</title>
      <link>https://stackoverflow.com/questions/78802407/computer-vision-yolo</link>
      <description><![CDATA[我们如何构建一个在移动设备上运行且消耗很少资源的对象识别模型？
我使用了 Moodle，但准确率不高
我尝试了 yolo 算法，但速度很慢
我也想在 Raspberry Pi 上运行这个模型，我希望它也能识别远处的物体，而不仅仅是附近的物体]]></description>
      <guid>https://stackoverflow.com/questions/78802407/computer-vision-yolo</guid>
      <pubDate>Sat, 27 Jul 2024 20:34:43 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。
环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
GPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

CPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型自行出现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>列表索引超出范围OpenCV Python</title>
      <link>https://stackoverflow.com/questions/78802174/list-index-out-of-range-opencv-python</link>
      <description><![CDATA[我目前正在学习机器学习，正在做物体检测项目。
我的问题是 cv2.put_text() 在我的 for 循环中无法正常工作。
以下是相关代码：

classIds, confs, bbox = net.detect(img, confThreshold=0.5) 

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):
cv2.rectangle(img, box, (255, 0, 0), thicken=2)
cv2.putText(img, classNames[classId-1].upper(), (box[0]+10, box[1]+30),
cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2)

这是上述代码产生的错误消息：

文件 &quot;C:\Users\User\projects\opencv\ObjectDetector.py&quot;, 第 31 行, 
在 &lt;module&gt;
cv2.putText(img, classNames[classId-1].upper(), (box[0]+10, box[1]+30),
~~~~~~~~~~^^^^^^^^^^^
IndexError: 列表索引超出范围

这是我的完整代码：
&#39;
import cv2

# img = cv2.imread(&#39;bob.png&#39;)

cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

classNames = []
classFile = &#39;coco.names&#39;
with open(classFile, &#39;rt&#39;) as f:
classNames = f.read().rstrip(&#39;\n&#39;).split(&#39;\n&#39;)

configPath = &#39;ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt&#39;
weightsPath = &#39;frozen_inference_graph.pb&#39;

net = cv2.dnn.DetectionModel(weightsPath, configPath)
net.setInputSize(320, 320)
net.setInputScale(1.0 / 127.5)
net.setInputMean((127.5, 127.5, 127.5))
net.setInputSwapRB(True)

当 True 时： 
succss, img = cap.read()
classIds, confs, bbox = net.detect(img, confThreshold=0.5) 
print(classIds, bbox)

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), 
confs.flatten(), bbox):
cv2.rectangle(img, box, (255, 0, 0), thicken=2)
cv2.putText(img, classNames[classId-1].upper(),
(box[0]+10, box[1]+30),
cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2)

cv2.imshow(&#39;Output&#39;, img)
cv2.waitKey(1)

&#39;
我原本希望它显示一个带有摄像头的窗口，周围有一个框，其中指定了对象的名称，但它只显示了错误。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78802174/list-index-out-of-range-opencv-python</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:08 GMT</pubDate>
    </item>
    <item>
      <title>在技​​术机器学习文章中，是否应该将输入层形状作为模型架构的一部分进行报告？</title>
      <link>https://stackoverflow.com/questions/78802155/in-technical-ml-articles-should-one-report-input-layer-shape-as-part-of-their-m</link>
      <description><![CDATA[我是一名自然科学专业的学生，​​正在研究一个使用机器学习的项目。我正在研究一个由另一个研究小组创建的神经网络模型，该模型使用haiku实现。
在他们的原始文章中，当谈到他们的模型超参数时，他们在描述他们的MLP形状时没有包括输入层的大小，而只包括隐藏层和输出层的大小。例如，如果输入为 N=2000，后续层为 256, 64, 4（输出形状为 (4,)），那么他们甚至在论文中也将 MLP 形状报告为 (256,64,4)。
对我来说，这很令人困惑，因为从 2000 到 256 的解析必须具有与输入大小成比例的权重矩阵。后来，我意识到在 haiku 模块中，输入层会根据输入的大小进行调整，并且您通常不会在定义 MLP 本身时传递输入的大小。这与我在 ML 课程中使用 tensorflow/keras 的经历不同。我很惊讶，甚至测试了这一点——不同的输入形状输出不同的可训练参数。如果我理解错了，请纠正我：但是，如果输入 N=1000 与输入 N=2000，这难道不会产生完全不同的模型吗？
我在此向知情的机器学习专家提出问题：在报告层时，应该写“四层形状 (2000, 256, 64, 4)”还是“三层形状 (256, 64, 4)”？

P.S.如果有人好奇，我已经测试过，使用不同的输入大小实际上会导致 haiku 中的权重参数不同。
作为 MWE 示例
import haiku as hk
import jax.numpy as jnp
import numpy as np

import jax

class MyLinear1(hk.Module):

def __init__(self, output_size, name=None):
super().__init__(name=name)
self.output_size = output_size

def __call__(self, x):
j, k = x.shape[-1], self.output_size
w_init = hk.initializers.TruncatedNormal(1. / np.sqrt(j))
w = hk.get_parameter(&quot;w&quot;, shape=[j, k], dtype=x.dtype, init=w_init)
b = hk.get_parameter(&quot;b&quot;, shape=[k], dtype=x.dtype, init=jnp.ones)
return jnp.dot(x, w) + b

def _forward_fn_linear1(x):
module = MyLinear1(output_size=2)
return module(x)

forward_linear1 = hk.transform(_forward_fn_linear1)

rng_key = jax.random.PRNGKey(42)
dummy_x = jnp.ones([8])

params = forward_linear1.init(rng=rng_key, x=dummy_x)
print(params)

如果您使用 dummy_x = jnp.ones([8]) 它给出
 [-0.4785112 , -0.38034892],
[-0.41137823, -0.22265594],
[-0.43343404, 0.21691099],
[-0.18514387, -0.10827615],
[ 0.3682926 , -0.1418969 ],
[ 0.10915945, 0.4389233 ],
[-0.07725035, 0.08247987]], dtype=float32), &#39;b&#39;: Array([1., 1.], dtype=float32)}}

while dummy_x = jnp.ones[1]) 得出
{&#39;my_linear1&#39;: {&#39;w&#39;: Array([[ 1.51595 , -0.23353337]], dtype=float32), &#39;b&#39;: Array([1., 1.], dtype=float32)}}
]]></description>
      <guid>https://stackoverflow.com/questions/78802155/in-technical-ml-articles-should-one-report-input-layer-shape-as-part-of-their-m</guid>
      <pubDate>Sat, 27 Jul 2024 18:24:55 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的线性回归模型返回负 R^2 分数</title>
      <link>https://stackoverflow.com/questions/78800849/linear-regression-model-in-scikit-learn-returning-negative-r2-score</link>
      <description><![CDATA[因此我尝试使用 scikit 学习线性回归来处理这个奥运数据框。
运动员部分有效，但年龄部分没有显示线性预测（我知道年龄与奖牌的相关性不好）
reg=LinearRegression()
predictors=[&quot;athletes&quot;, &quot;age&quot;]
output=&quot;medals&quot;
reg.fit(train[predictors],train[output])

我也希望第二个图是一条直线。我知道这里给出的图预测了两个输入参数的数据，但我想知道 scikit 如何区分这两个输入，即运动员和年龄]]></description>
      <guid>https://stackoverflow.com/questions/78800849/linear-regression-model-in-scikit-learn-returning-negative-r2-score</guid>
      <pubDate>Sat, 27 Jul 2024 07:59:21 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用任何类型的基础模型来检测通常发生在眼睛中的疾病？[关闭]</title>
      <link>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</link>
      <description><![CDATA[我想建立一个模型，可以检测视网膜图像是否有微动脉瘤。
可以使用基础模型吗？如果可以，它会带来什么好处？
如果我想，我可以使用哪种基础模型，比如我最近读到关于 RetFound 的文章，我认为它适合我的范围。
（这就像一个理论问题）如果这些模型甚至没有接受过执行特定任务的训练，那么它们究竟如何适用于我们的特定用例数据，比如制作基础模型的人如何决定训练数据的范围？它只是一个对视网膜图像进行处理模型，还是一个范围更广的模型，除了视网膜图像外，它还与其他相关数据有关（我相信）。第三个问题是可选的，如果您能回答，我将不胜感激。
我尝试使用一个基础模型，https://github.com/facebookresearch/deit/blob/main/README_deit.md，但我在这里学得并不多。]]></description>
      <guid>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</guid>
      <pubDate>Fri, 26 Jul 2024 18:19:53 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：参数详细程度的值 -1 超出界限 [0,3]</title>
      <link>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</link>
      <description><![CDATA[错误消息如标题所示。根据下面的代码，这对我来说毫无意义：
clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class &#39;xgboost.sklearn.XGBClassifier&#39;&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter[&#39;y&#39;]) # 错误在这里出现

值显然是 1，但不知何故却变成了 -1？我不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</guid>
      <pubDate>Wed, 17 Jul 2024 22:12:23 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 没有为 xgboost 选择最佳超参数</title>
      <link>https://stackoverflow.com/questions/69429691/gridsearchcv-not-choosing-the-best-hyperparameters-for-xgboost</link>
      <description><![CDATA[我正在使用 xgboost 开发回归模型。由于 xgboost 有多个超参数，我使用 GridSearchCV() 添加了交叉验证逻辑。作为试验，我设置了 max_depth: [2,3]。我的python代码如下。
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
​
xgb_reg = xgb.XGBRegressor()
​
# 获取最佳超参数
scorer=make_scorer(mean_squared_error, False)
params = {&#39;max_depth&#39;: [2,3], 
&#39;eta&#39;: [0.1], 
&#39;colsample_bytree&#39;: [1.0],
&#39;colsample_bylevel&#39;: [0.3],
&#39;subsample&#39;: [0.9],
&#39;gamma&#39;: [0],
&#39;lambda&#39;: [1],
&#39;alpha&#39;:[0],
&#39;min_child_weight&#39;:[1]
}
grid_xgb_reg=GridSearchCV(xgb_reg,
param_grid=params,
scoring=scorer,
cv=5,
n_jobs=-1)
​
grid_xgb_reg.fit(X_train, y_train)
y_pred = grid_xgb_reg.predict(X_test)
y_train_pred = grid_xgb_reg.predict(X_train)

## 评估模型
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
​
print(&#39;RMSE train: %.3f, test: %.3f&#39; %(np.sqrt(mean_squared_error(y_train, y_train_pred)),np.sqrt(mean_squared_error(y_test, y_pred))))
print(&#39;R^2训练：%.3f，测试：%.3f&#39; %(r2_score(y_train, y_train_pred),r2_score(y_test, y_pred)))

问题是 GridSearchCV 似乎没有选择最佳超参数。在我的例子中，当我将 max_depth 设置为 [2,3] 时，结果如下。在以下情况下，GridSearchCV 选择 max_depth:2 作为最佳超参数。
# max_depth 为 2 时的结果
RMSE 训练：11.861，测试：15.113
R^2 训练：0.817，测试：0.601

但是，如果我将 max_depth 更新为 [3]（通过删除 2），则测试分数会比之前的值更好，如下所示。
# max_depth 为 3 时的结果
RMSE 训练：9.951，测试：14.752
R^2 训练：0.871，测试：0.620

问题
我的理解是，即使我设置max_depth 设置为 [2,3]，GridSearchCV 方法应该选择 max_depth:3 作为最佳超参数，因为 max_depth:3 可以比 max_depth:2 返回更好的 RSME 或 R^2 分数。有人能告诉我为什么当我将 max_depth 设置为 [2,3] 时，我的代码无法选择最佳超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/69429691/gridsearchcv-not-choosing-the-best-hyperparameters-for-xgboost</guid>
      <pubDate>Sun, 03 Oct 2021 23:45:55 GMT</pubDate>
    </item>
    </channel>
</rss>