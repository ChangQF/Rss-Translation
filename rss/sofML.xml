<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 26 Aug 2024 01:09:22 GMT</lastBuildDate>
    <item>
      <title>Dask 在 GridSearchCV 和 RandomizedSearchCV 上犯了错误</title>
      <link>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</link>
      <description><![CDATA[我正在尝试使用 dask 训练 xgboost 模型。我已经转换了数据并准备了如下数据：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

我可以像这样对数据进行简单的模型拟合：
model = dxgb.DaskXGBRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)

但是当我尝试任何更复杂的事情时。就像这样使用 dask 的 gridsearchcv：
param_grid = {
&#39;max_depth&#39;: [3, 8],
&#39;learning_rate&#39;: [0.01, 0.1]}

grid_search = GridSearchCV(
model, param_grid}

grid_search.fit(X_train, y_train)

我收到以下警告：
警告：dask_ml.model_selection._search:(&#39;daskxgbregressor-fit-score-f6830e79aeb606b3eed291ac24184a8c&#39;, 1, 1) 失败...正在重试

任务图中的所有内容都因错误而变黑。
有人知道如何解决吗这个？]]></description>
      <guid>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</guid>
      <pubDate>Mon, 26 Aug 2024 01:04:55 GMT</pubDate>
    </item>
    <item>
      <title>Matryoshka 适配器实现：谷歌关于嵌入模型的精彩新论文</title>
      <link>https://stackoverflow.com/questions/78912400/matryoshka-adaptor-implementation-exciting-new-paper-on-embedding-models-from-g</link>
      <description><![CDATA[Google 最近在 arxiv 上发布了一篇关于嵌入模型的激动人心的新论文]1。本质上，该论文提出了一种简单 MLP 形式的适配器，它可以高效地将任何嵌入模型的输出适配到 Matryoshka 嵌入，根据需要降低尺寸，同时保持性能；嵌入模型本身不需要微调或进行迁移学习。这是一篇令人兴奋的论文，具有一些非常令人兴奋的含义。
我一直在尝试自己复制结果，但不幸的是，我没有达到承诺的准确度 - 正如您所见，我获得的嵌入并不比裁剪原始模型的嵌入更好：

未获得论文图 4 中的结果 - 无监督 Matryoshka Adaptor 并不比具有截断嵌入的 BASE 模型更好；它应该比具有降维的 PCA 降级更少。
我认为我误解了并因此错误地实现了所提出的模型。我认为它是以下之一：

错误的训练过程 - 我一直在 NFCorpus 上训练它，但我也尝试过在其他 BEIR 数据集上尝试。对于应该更易于实现的无监督方法，我一直在为每个 BEIR 数据集在语料库的“文本”上训练它。训练期间损失在减少，但如您所见，

性能并不好。

其中一个损失函数的实现不正确。

我还想澄清如何计算对的数量（训练对和测试对），假设它们是查询语料库对，将

仅用于监督方法。


有人可以看看我在 GitHub 上的实现或提供任何调试建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78912400/matryoshka-adaptor-implementation-exciting-new-paper-on-embedding-models-from-g</guid>
      <pubDate>Sun, 25 Aug 2024 22:39:46 GMT</pubDate>
    </item>
    <item>
      <title>拟合模型时无法取未知等级形状的长度</title>
      <link>https://stackoverflow.com/questions/78911992/cannot-take-the-length-of-shape-with-unknown-rank-when-fitting-model</link>
      <description><![CDATA[我正在尝试训练深度音频分类模型。检查测试和训练数据的形状时，结果是 (16, 6245, 257, 1)，其中 16 是批次中的项目数，6245 是宽度，257 是高度，1 是通道数。然后我在神经网络中创建了此输入层：
model.add(Input(shape=(6245, 257, 1)))

我能够创建模型，甚至获得模型摘要。但是当我使用以下命令训练模型时：
hist = model.fit(train, epochs=6, validation_data=test)

我收到此错误：
无法获取具有未知等级的形状的长度。

我在 anaconda 环境中的 jupyter 笔记本中使用 tensorflow 2.17。
注意：我正在将音频转换为频谱图，并将此函数映射到整个数据集上。
def preprocess(file_path, label): 
def load_and_preprocess(path):
wav = load_wav_16k_mono(path)
wav = wav[:200134]
zero_padding = tf.zeros([200134] - tf.shape(wav), dtype=tf.float32)
wav = tf.concat([zero_padding, wav], 0)
spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)
spectrogram = tf.abs(spectrogram)
spectrogram = tf.expand_dims(spectrogram, axis=-1)
return spectrogram

# 使用 tf.py_function 包装文件加载和预处理
file_path = tf.convert_to_tensor(file_path, dtype=tf.string)
spectrogram = load_and_preprocess(path)

return spectrogram, label

尽管如此，检查频谱图的形状也会返回 TensorShape([6245, 257])，因此它与传递到模型中的样本相同。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78911992/cannot-take-the-length-of-shape-with-unknown-rank-when-fitting-model</guid>
      <pubDate>Sun, 25 Aug 2024 18:46:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在决策树中使用直方图实现分箱条件？</title>
      <link>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</guid>
      <pubDate>Sun, 25 Aug 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么每次运行程序都会得到不同的准确度</title>
      <link>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</link>
      <description><![CDATA[我使用 Python 中的 keras tensorflow 训练模型。
另外，正如您在下面的代码中看到的，我使用了种子参数，但每次我用相同的数据运行相同的代码时，我都会面临不同的准确率百分比。
我的代码：
#Seed
tf.random.set_seed(42)
np.random.seed(42)
set_random_seed(42)
random.seed(42)

data = (&#39;data.csv&#39;)

data = pd.get_dummies(data, columns=[&#39;cp&#39;, &#39;restecg&#39;], drop_first=True)

X = data.drop(&#39;num&#39;, axis=1)
y = data[&#39;num&#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def create_model(optimizer=&#39;adam&#39;, init=&#39;glorot_uniform&#39;, neurons=[16, 8], dropout_rate=0.3):
model = Sequential()
model.add(Input(shape=(X_train.shape[1],)))
model.add(Dense(neurons[0],activation=&#39;relu&#39;, kernel_initializer=init))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(neurons[1],activation=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(1,activation=&#39;sigmoid&#39;))
model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
返回模型

param_grid = {
&#39;optimizer&#39;: [&#39;adam&#39;, &#39;rmsprop&#39;],
&#39;model__neurons&#39;: [[16, 8]],
&#39;model__init&#39;: [&#39;glorot_uniform&#39;, &#39;normal&#39;],
&#39;model__dropout_rate&#39;: [0.3],
&#39;epochs&#39;: [50], 
&#39;batch_size&#39;: [10],
}

model = KerasClassifier(model=create_model, verbose=0)

kfold = KFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, n_jobs=-1)
grid_search_result = grid_search.fit(X_train, y_train)

print(f&quot;最佳参数：{grid_search_result.best_params_}&quot;)

print(f&quot;最佳准确度：{grid_search_result.best_score_}&quot;)

best_model = grid_search_result.best_estimator_

keras_model = best_model.model
keras_model.trainable = False 

y_pred_prob = best_model.predict(X_test).flatten()
y_pred = np.where(y_pred_prob &gt; 0.5, 1, 0)

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f&#39;Accuracy (手动计算): {accuracy:.2f}&#39;)
print(f&#39;ROC AUC: {roc_auc:.2f}&#39;)

我需要每次都获得相同的准确度。
我应该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</guid>
      <pubDate>Sun, 25 Aug 2024 17:25:44 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 项目模板创建失败</title>
      <link>https://stackoverflow.com/questions/78911757/aws-sagemaker-project-template-creation-failing</link>
      <description><![CDATA[我正在尝试使用 AWS Sagemaker Studio 中已提供的模板“模型构建、训练和部署”创建一个项目。但是，我遇到了以下错误：https://i.sstatic.net/e8MZgiMv.png
我执行了以下步骤：

在 SageMaker 中创建一个域。
域

向附加到域的 IAM 角色添加了其他策略（AWSCodeCommitFullAccess、AmazonS3FullAccess、AWSCloudFormationFullAccess）。
iam


我仍然收到上述错误。有什么线索可以说明我遗漏了什么吗？
PS：
AWSCodeCommit 的配额如下：codecommitquota
测试代码提交存储库创建：codecommitrepo]]></description>
      <guid>https://stackoverflow.com/questions/78911757/aws-sagemaker-project-template-creation-failing</guid>
      <pubDate>Sun, 25 Aug 2024 16:50:25 GMT</pubDate>
    </item>
    <item>
      <title>检测硬币图像的旋转角度</title>
      <link>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</link>
      <description><![CDATA[确定硬币图像角度的最佳方法是什么？
图像的分辨率是固定的（400x400），但可能在任何方向上偏离中心几个像素。
图像可以有不同的颜色、光照、旋转等。


我尝试过各种 CNN 想法，但都没有成功。看来，旋转后的图像之间的相似性不足以增强效果。
我无法找到用于此目的的现有数据集
请给我一些已被证明可行的想法，而不是理论]]></description>
      <guid>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</guid>
      <pubDate>Sun, 25 Aug 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>关键点/地标检测</title>
      <link>https://stackoverflow.com/questions/78906351/keypoints-landmarks-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78906351/keypoints-landmarks-detection</guid>
      <pubDate>Fri, 23 Aug 2024 14:19:21 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10：如何解读训练进度信息？</title>
      <link>https://stackoverflow.com/questions/78904715/yolov10-how-to-interpret-training-progress-info</link>
      <description><![CDATA[我正在用这个代码训练 YOLOv10：
model.train(
.........
epochs=250,
batch=16,
verbose=True,
save=True,
save_period=1,
time=4,
.........
)

我正在尝试各种 VM/GPU 选项来计算最具成本效益的训练选项，所以我运行它很多次，同时我更改 batch 以填充接近 90% 的 GPU RAM。
在训练期间，除了其他输出外，我还获得了类似屏幕截图的信息。

问题如下。我允许自己在一个主题中提出几个问题，因为我希望其中两个问题或多或少像 yes/no/single_word 一样简单，另一个只是一个链接。

为什么计划的 epoch 数量 (1) 被更改 (2)？我相信这可能是因为分配给训练的时间有限，所以训练过程会计算在此期间可以运行多少个 epoch，但我不确定。
我相信 (3) 是每个 epoch 的批次数，这个数字会根据批次大小和 GPU RAM 使用情况而变化。它正确吗？
我如何估计在特定 GPU/批处理设置上训练过程需要多长时间（假设没有配置时间限制）？我尝试将 (4) 和 (5) 相加并乘以更改后的 epoch 数 (2)，但在所有实验中，我得到的时间总是接近配置的限制。
是否有文档解释屏幕截图上的所有其他数据以及其他训练输出？有些值是不言自明的，有些则不是，所以我宁愿避免猜测。我检查了 https://docs.ultralytics.com/modes/train/，但没有找到解释。
]]></description>
      <guid>https://stackoverflow.com/questions/78904715/yolov10-how-to-interpret-training-progress-info</guid>
      <pubDate>Fri, 23 Aug 2024 07:12:32 GMT</pubDate>
    </item>
    <item>
      <title>从 ECL 中的数据集中提取实际 y 以进行随机森林分类</title>
      <link>https://stackoverflow.com/questions/78808039/extracting-actual-y-from-dataset-in-ecl-for-random-forest-classification</link>
      <description><![CDATA[我正在使用 ECL 开发随机森林分类模型。我有一个分为训练集和测试集的数据集，我正在尝试从这两个数据集中提取目标变量 𝑦（表示患者是否患有糖尿病）。但是，当前使用“ML_Core.Discretize.ByRounding”的方法无法产生预期的结果。
我尝试使用以下 ECL 代码片段从“TrainNF”和“TestNF”数据集中提取目标变量 𝑦（代表糖尿病指标）：
independent_cols := 8;

X_train := TrainNF(number &lt; independent_cols + 1);
y_train := ML_Core.Discretize.ByRounding(TrainNF(number = independent_cols + 1));

X_test := TestNF(number &lt; independent_cols + 1);
y_test := ML_Core.Discretize.ByRounding(TestNF(number = independent_cols + 1));

“y_train”和“y_test”变量不包含任何值；它们实际上是空白的，这表明提取没有按预期进行。
我预计“y_train”和“y_test”分别包含来自“TrainNF”和“TestNF”数据集的值。这些值对于训练随机森林分类器和评估其性能至关重要。]]></description>
      <guid>https://stackoverflow.com/questions/78808039/extracting-actual-y-from-dataset-in-ecl-for-random-forest-classification</guid>
      <pubDate>Mon, 29 Jul 2024 15:54:43 GMT</pubDate>
    </item>
    <item>
      <title>rpart() 决策树无法生成分割（只有一个节点（根节点）的决策树）</title>
      <link>https://stackoverflow.com/questions/78804884/rpart-decision-tree-fails-to-generate-splits-decision-tree-with-only-one-node</link>
      <description><![CDATA[我正在尝试创建决策树来预测特定贷款申请人是否会违约或偿还债务。
我正在使用以下数据集
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)

贷款 &lt;- read_csv(&#39;https://assets.datacamp.com/production/repositories/718/datasets/7805fceacfb205470c0e8800d4ffc37c6944b30c/loans.csv&#39;)

由于响应变量 default 被编码为 dbl，我首先将其转换为 chr，然后将其转换为 fct 类型变量以在我的分类中使用它模型。
loans &lt;- loans %&gt;% mutate(default = factor(as.character(default), levels = c(0, 1), labels = c(&#39;repaid&#39;, &#39;defaulted&#39;)))

现在，我开始构建递归分区 (rpart()) 对象 loans_model：响应变量为 default，解释变量为 loan_amount + credit_score + deal_to_income。
loans_model &lt;- rpart(default ~ loan_amount + credit_score + deal_to_income, data = loans, method = &#39;class&#39;)

当我使用此模型进行预测时，所有预测值都得到相同的值， 已偿还。
loans$pred_default &lt;- predict(loans_model, newdata = loans, type = &quot;class&quot;)

unique(unique(loans$pred_default)


输出：
[1] 已偿还
级别：已偿还 已违约

此外，当我尝试可视化决策树时，我只得到一个节点（根节点）。
rpart.plot(loan_model)


为什么我建立的模型没有做出适当的预测？]]></description>
      <guid>https://stackoverflow.com/questions/78804884/rpart-decision-tree-fails-to-generate-splits-decision-tree-with-only-one-node</guid>
      <pubDate>Sun, 28 Jul 2024 21:42:53 GMT</pubDate>
    </item>
    <item>
      <title>利用外推训练的随机森林模型来使用不同的卫星创建时间序列（谷歌地球引擎）</title>
      <link>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</link>
      <description><![CDATA[我正在尝试创建 1990 年至 2023 年的 LULC 时间序列。为了生成我的时间序列，我选择了四个使用不同卫星（Landsat 5、7、8 和 Sentinel-1 和 -2）的日期。我只有 2023 年的训练数据。目前我只是使用历史数据中使用的相同 2023 年点重新训练模型。我觉得这会导致我的分类出现一些不一致。我希望能够在所有日期运行我训练过的分类器，但存在一些严重的波段不一致。有没有办法让模型适应这些波段变化？
不一致的原因不仅在于卫星之间的波段差异，还在于我对不同年份的预处理步骤，即使用全色波段对 Landsat 数据进行全色锐化，以及对整个 sentinel-1（不收集 RBG）进行锐化。
我正在使用 Google Earth Engine API（JavaScript）
在删除几个波段后，我尝试对 sentinel 和 landsat8 使用相同的模型（在 2023 年 sentinel 数据上进行训练），但是我的 landsat8 数据的 LULC 地图完全错误（在很大程度上，基于我的视觉评估和地面真实数据）。]]></description>
      <guid>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</guid>
      <pubDate>Thu, 18 Jul 2024 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>为什么在使用 ranger 包进行预测时不使用变量重要性设置的原因是什么？</title>
      <link>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</link>
      <description><![CDATA[我正在使用 R 中的 ranger 包来构建随机森林模型，当我预测新数据时，会弹出此警告：
警告消息：
在 predict.ranger(object, ...) 中：
森林是使用“impurity_corrected”变量重要性构建的。对于预测，建议构建另一个不带此重要性设置的森林。

我想知道是否有人可以解释背后的原因，是否因为预测会出错，只是需要更多的计算时间等。
我在网上搜索答案，但找不到原因。]]></description>
      <guid>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</guid>
      <pubDate>Wed, 26 Jun 2024 20:05:02 GMT</pubDate>
    </item>
    <item>
      <title>非线性模型 GAM、MARS 假设</title>
      <link>https://stackoverflow.com/questions/67024201/non-linear-models-gam-mars-assumptions</link>
      <description><![CDATA[MARS 和 GAM 等模型是否假设异方差和 IID 误差？文献中似乎对某些假设存在分歧。看起来 MARS 比 GAM 更稳健，但原始论文中没有明确说明。
如果正态性是一个问题，是否应该使用变换数据（Box-Cox 或 Yeo-Johnson）进行回归？]]></description>
      <guid>https://stackoverflow.com/questions/67024201/non-linear-models-gam-mars-assumptions</guid>
      <pubDate>Fri, 09 Apr 2021 15:42:32 GMT</pubDate>
    </item>
    <item>
      <title>GRU 和 LSTM 哪个更快</title>
      <link>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</link>
      <description><![CDATA[我尝试在 keras 上用 GRU 和 LSTM 实现一个模型。两种实现的模型架构相同。正如我在许多博客文章中看到的，GRU 的推理时间比 LSTM 更快。但就我而言，GRU 并不更快，而且实际上比 LSTM 更慢。有人能找到原因吗？这与 Keras 中的 GRU 有什么关系吗？还是我哪里做错了。
非常感谢您的帮助...
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</guid>
      <pubDate>Mon, 27 Jan 2020 14:21:12 GMT</pubDate>
    </item>
    </channel>
</rss>