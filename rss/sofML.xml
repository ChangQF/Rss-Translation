<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 06 Oct 2024 03:24:40 GMT</lastBuildDate>
    <item>
      <title>将预训练模型应用于具有修剪机制的新 Transformer 架构中，并以特定的 RoI 为目标</title>
      <link>https://stackoverflow.com/questions/79058245/using-pretrained-models-into-a-new-transformer-architecture-with-pruning-mechani</link>
      <description><![CDATA[我发现由 Strudel 等人创建的模型称为 Segmenter 架构，用于使用视觉转换器 (ViT) 分割图像，我尝试使用它进行推理并得到这个
Transformers 很重，处理图像时更重，所以我尝试找到方法让它“更轻”，至少对于仅使用 CPU 的机器训练来说是这样。
假设我只想取树的一部分或河流的一部分，我发现我可以更改参数使其偏向应该关注的东西，就像传统的掩蔽一样，或者我可以使用像 SPViT 这样的方法来进行模型修剪。
但是 SPViT 有自己的 Transformer 训练迭代，所以基本上它会“擦除”或“压缩”不必要的像素，直到它达到每个 Transformer 训练迭代的一定商定准确度（因此有多次训练，但每次迭代的负载越来越少，直到它可以专注于某个 RoI）。有什么方法可以将模型“注入”到整个 SPViT 架构中 ViT 训练的第一次迭代中，并将其用作“修剪决定因素”变量，这样它就不必像被告知从头开始训练那样使用那么多计算？我在这里寻找什么方法？
抱歉，我还是新手，每个“名称”都是我试图找到的名称，但无法确定它是什么。但我想我可以得出结论，您不能随意分离嵌入表？]]></description>
      <guid>https://stackoverflow.com/questions/79058245/using-pretrained-models-into-a-new-transformer-architecture-with-pruning-mechani</guid>
      <pubDate>Sun, 06 Oct 2024 02:18:09 GMT</pubDate>
    </item>
    <item>
      <title>分类神经网络不收敛</title>
      <link>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</link>
      <description><![CDATA[我为 MNIST 开发了一个基本的分类网络，但在训练期间，验证准确率在 10% 左右。我尝试过各种优化器（SGD、Adam、Nadam）以及不同的学习率（0.1、1e-3、1e-4、1e-5），但验证准确率在每个时期都保持在 10% 左右。
这是我的代码：
import tensorflow as tf
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train, X_val = X_train[5000:]/255.0, X_train[:5000]/255.0
y_train, y_val = y_train[5000:]/255.0, y_train[:5000]/255.0

class_NN = keras.models.Sequential([
keras.layers.Input(shape = [28, 28]),
keras.layers.Flatten(),
keras.layers.Dense(300, 激活 = &quot;relu&quot;),
keras.layers.Dense(100, 激活 = &quot;relu&quot;),
keras.layers.Dense(10, 激活 = &quot;softmax&quot;)
])

class_NN.compile(loss = &quot;sparse_categorical_crossentropy&quot;,
optimizer = keras.optimizers.SGD(learning_rate = 1e-3), 
metrics = [&quot;accuracy&quot;])
class_NN.fit(X_train, y_train, epochs = 15,
validation_data = (X_val, y_val))

我在写这道题的时候，在开始训练之前从代码中去掉了/255.0。目前，验证的准确率随着训练的进行而提高。我的验证准确率达到了 96% 左右。是什么原因导致在训练过程中去掉/255.0后验证准确率有所提高？
X_train, X_val = X_train[5000:], X_train[:5000]
y_train, y_val = y_train[5000:], y_train[:5000]
]]></description>
      <guid>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</guid>
      <pubDate>Sun, 06 Oct 2024 00:55:12 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于算法交易的人工智能机器人？[关闭]</title>
      <link>https://stackoverflow.com/questions/79058024/how-to-create-an-ai-bot-for-algorithmic-trading</link>
      <description><![CDATA[我和其他开发人员有一个项目（更像是一种爱好），我们想创建一个基于人工智能的机器人来预测场外市场（二元期权）的下一个一分钟蜡烛图。我们还没有开始做任何事情，这就是为什么我们要向社区寻求有关解决这个问题的最佳方法的建议。
在开始之前，我们还有一些疑问，例如：
该机器人是基于人工智能的，但什么是最合适的方法？例如，我们应该使用神经网络吗？如果是，哪种类型？（LSTM 等）。我们应该考虑 SVM 吗？我也看过应用模式识别技术的文档。哪种技术最适合这种类型的问题？
由于我们使用的是一分钟蜡烛图，我们知道训练将非常耗时，并且需要大量的计算能力。我们可以在哪里训练模型？ （我考虑的是 AWS 之类的云服务或类似的东西）。
我们不是在寻找一个具体的答案，而是要了解哪些条件和解决方案最适合以最佳方式实现这一目标。
如果需要有关该问题的更多背景信息，我将检查并编辑该帖子。]]></description>
      <guid>https://stackoverflow.com/questions/79058024/how-to-create-an-ai-bot-for-algorithmic-trading</guid>
      <pubDate>Sat, 05 Oct 2024 22:13:56 GMT</pubDate>
    </item>
    <item>
      <title>在二元分类数据上拟合神经网络模型的问题</title>
      <link>https://stackoverflow.com/questions/79057502/problem-with-fitting-a-neural-network-model-on-binary-classification-data</link>
      <description><![CDATA[我有一个包含 280 个样本的数据集，其中有 20 个特征和 0.1 个结果。我缩放了它们。
此外，还有三个神经网络模型来训练它们的数据。
但是在循环模型时，我收到了拟合错误。
如何解决？
import numpy as np
import math

import tensorflow as tf

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import密集

model1 = Sequential([
密集(单位=25, 激活=&#39;relu&#39;),
密集(单位=15, 激活=&#39;relu&#39;),
密集(单位=1, 激活=&#39;线性&#39;)
])
model2 = Sequential([
密集(单位=20, 激活=&#39;relu&#39;),
密集(单位=12, 激活=&#39;relu&#39;),
密集(单位=12, 激活=&#39;relu&#39;),
密集(单位=20, 激活=&#39;relu&#39;),
密集(单位=1, 激活=&#39;线性&#39;)
])
model3 = Sequential([
密集(单位=32, 激活=&#39;relu&#39;),
密集(单位=16, 激活=&#39;relu&#39;),
密集(单位=8, 激活=&#39;relu&#39;),
密集(单位=4, 激活=&#39;relu&#39;),
Dense(units=12,activation=&#39;relu&#39;),
Dense(units=1,activation=&#39;linear&#39;)
])

nn_train_error = []
nn_cv_error = []

models_bc = [model1, model2, model3]
for model in models_bc:

# 设置损失和优化器
model.compile(
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
)

print(f&quot;Training {model.name}...&quot;)
# 训练模型
model.fit(x_bc_train_scaled, y_bc_train, epochs=100, verbose=0)
print(&quot;Done!\n&quot;)

# 设置阈值用于分类
threshold = 0.5

# 记录训练集错误分类示例的比例
yhat = model.predict(x_bc_train_scaled)
yhat = tf.math.sigmoid(yhat)
yhat = np.where(yhat &gt;= Threshold, 1, 0)# np 中的 where：条件函数
train_error = np.mean(yhat != y_bc_train)# np 中的 mean：显示错误分类的百分比

nn_train_error.append(train_error)

# 记录交叉验证集错误分类示例的比例
yhat = model.predict(x_bc_cv_scaled)
yhat = tf.math.sigmoid(yhat)
yhat = np.where(yhat &gt;= Threshold, 1, 0)
cv_error = np.mean(yhat != y_bc_cv)

nn_cv_error.append(cv_error)

print(nn_train_error)
print(nn_cv_error)

输出：
 ValueError Traceback（最近一次调用
最后）单元格 In\[109\]，第 15 行 13 print(f&quot;Training
{model.name}...&quot;) 14 # 训练模型 ---\&gt; 15
model.fit(x_bc_train_scaled, y_bc_train, epochs=100, verbose=0) 
16 print(&quot;Done!\\n&quot;)

Sequential.call() 接收的参数：
• input=tf.Tensor(shape=(None, 20), dtype=float32)
• training=True 
• mask=None
]]></description>
      <guid>https://stackoverflow.com/questions/79057502/problem-with-fitting-a-neural-network-model-on-binary-classification-data</guid>
      <pubDate>Sat, 05 Oct 2024 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 csr_matrix）</title>
      <link>https://stackoverflow.com/questions/79057484/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</link>
      <description><![CDATA[我尝试运行：
import numpy as np
import pandas as pd
import tensorflow as tf
import numpy as np

从 tensorflow.keras 导入 Sequential
从 tensorflow.keras.layers 导入 Dense、Embedding、GlobalAveragePooling1D
从 tensorflow.keras.layers 导入 TextVectorization
从 sklearn.model_selection 导入 train_test_split
从 tensorflow 导入 keras

从 nltk.tokenize.treebank 导入 TreebankWordTokenizer、TreebankWordDetokenizer
从 sklearn.feature_extraction.text 导入CountVectorizer

dataf=pd.read_csv(&#39;D:/datafile.csv&#39;)
data=pd.read_csv(&quot;D:/dataset1c2f4b7/dataset/train.csv&quot;,encoding=&#39;latin-1&#39;)
l=[]
对于 dataf[&#39;text&#39;] 中的 a:
l.append(a)
m=[]
对于 dataf[&#39;target&#39;] 中的 a:
m.append(a)

X_train, X_test, y_train, y_test = train_test_split(l, m, test_size=0.2, random_state=42)

vectorizer = CountVectorizer()
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)
X_train=np.array(X_train)
X_test=np.array(X_test)
y_train=np.array(y_train)
y_test=np.array(y_test)
print(X_train)
model = keras.models.Sequential() 
model.add(keras.layers.Embedding(10000, 128)) 
model.add(keras.layers.SimpleRNN(64, return_sequences=True)) 
model.add(keras.layers.SimpleRNN(64)) 
model.add(keras.layers.Dense(128,activation=&quot;relu&quot;)) 
model.add(keras.layers.Dropout(0.4)) 
model.add(keras.layers.Dense(1,激活=“sigmoid”）） 
model.summary() 

model.compile(“rmsprop”， 
“binary_crossentropy”， 
metrics=[“accuracy”])
model.fit(X_train, y_train,epochs=5,verbose=False,validation_data=(X_test, y_test),batch_size=10)
model.save(&#39;gfgModel.h5&#39;) 
tf.saved_model.save(model, &#39;one_step 05&#39;)

这显示
ValueError：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 csr_matrix）

我正在尝试创建一个文本分类器。
我只是期待要训​​练的模型，因为所有内容都是数组形式。]]></description>
      <guid>https://stackoverflow.com/questions/79057484/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</guid>
      <pubDate>Sat, 05 Oct 2024 16:43:01 GMT</pubDate>
    </item>
    <item>
      <title>如何将有限列表中的元素作为输入传递？</title>
      <link>https://stackoverflow.com/questions/79057233/how-to-pass-an-element-from-a-limited-list-as-input</link>
      <description><![CDATA[我拥有“石头剪刀布”游戏中手臂不同状态的汇编。我的目的是以类似的方式对这些类别进行编程。
[1, 0, 0] - 石头 
[0, 1, 0] - 布 
[0, 0, 1] - 剪刀

有没有方便的自动方法？
我使用了嵌入层，但我不确定它是否合适。]]></description>
      <guid>https://stackoverflow.com/questions/79057233/how-to-pass-an-element-from-a-limited-list-as-input</guid>
      <pubDate>Sat, 05 Oct 2024 14:23:29 GMT</pubDate>
    </item>
    <item>
      <title>如果两个基因组没有匹配的连接，如何获得 NEAT 算法中兼容距离的平均权重差异[关闭]</title>
      <link>https://stackoverflow.com/questions/79055601/how-do-i-get-the-average-weight-difference-for-the-compatibility-distance-in-a-n</link>
      <description><![CDATA[由于计算两个基因组的兼容性距离的公式包括平均权重差异，如果它们没有一个匹配的连接，就会出现问题。通常，你会通过将总权重差异除以共享权重的总量来计算平均权重差异。但由于你不能用 0 除以某个数，所以这是一个问题。
我可以将平均权重差异设置为无穷大或一个非常大的数字吗？或者有更好的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79055601/how-do-i-get-the-average-weight-difference-for-the-compatibility-distance-in-a-n</guid>
      <pubDate>Fri, 04 Oct 2024 19:28:30 GMT</pubDate>
    </item>
    <item>
      <title>我的梯度下降实现有什么问题（带铰链损失的 SVM 分类器）</title>
      <link>https://stackoverflow.com/questions/79055573/what-is-wrong-with-my-gradient-descent-implementation-svm-classifier-with-hinge</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79055573/what-is-wrong-with-my-gradient-descent-implementation-svm-classifier-with-hinge</guid>
      <pubDate>Fri, 04 Oct 2024 19:19:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Google Cloud Services 设计实时产品建议管道？[关闭]</title>
      <link>https://stackoverflow.com/questions/79055177/how-to-design-a-real-time-product-suggesting-pipeline-using-google-cloud-service</link>
      <description><![CDATA[我正在考虑一个用例，我需要使用谷歌云功能进行设计。
案例是：
假设用户正在点击一个产品。
该产品 ID 将被发送到实时流式数据流管道，
它将选择与该产品相关的更多项目并
将该产品 ID 发送回该特定用户。
然后该 ID 将用于获取产品信息，并将在页面上的某个位置呈现，例如 -&gt; 您可能喜欢的部分。
信息将存储在 Bigtable 中，这样我们就可以非常快速地获取我们的产品 ID，因为 Bigtable 具有非常高的吞吐量。
现在的问题是，如何将 ID 发送回同一个用户，以便该产品在该特定用户窗口中呈现，而不是其他用户？
如果有人有更好的方法，他们也可以提到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/79055177/how-to-design-a-real-time-product-suggesting-pipeline-using-google-cloud-service</guid>
      <pubDate>Fri, 04 Oct 2024 16:39:19 GMT</pubDate>
    </item>
    <item>
      <title>分类器的数据集极度不平衡[关闭]</title>
      <link>https://stackoverflow.com/questions/79055091/extremely-imbalanced-dataset-for-a-classifier</link>
      <description><![CDATA[我正在研究一种二分类器。我面临的挑战是我的数据集是多么的不平衡。只有 2% 的行属于 A 类（正）。剩下的 98% 属于 B 类（负）。
在这种情况下，获得很高的准确率并不意味着什么。你可以想象我追求的是真正的阳性。
我曾尝试在 Azure 机器学习设计器上使用 SMOTE，但得到的结果很差。为了防止发生任何数据泄露，我在拆分数据后应用了 SMOTE。
我曾尝试过自动化机器学习，希望它能以某种方式处理不平衡的数据集，但事实并非如此。它只是向我发送了一条关于 Imalabace 类的警告消息。结果也很糟糕。
我想知道我是否也可以应用 SMOTE 和可能的 Tomek Links，但仅限于 Auto ML 的训练数据集。
我想我需要去笔记本，复制排名最高的自动化模型的代码，并对其进行调整，以便我可以将这些转换仅应用于训练部分（如果可能的话）。此外，通过这样做，我不会使用自动化 ML 的功能，但我只使用自动化 ml 生成的模型之一。
有什么想法和指导吗？
在 Azure ML Designer 上应用了 SMOTE，希望更平衡的训练数据集能够帮助模型学习。但是我得到的结果并不好。
我尝试使用 AutomatedML，因为我读到它有一些内置功能可以帮助解决不平衡的数据集。但是，我没有看到任何进展。
在这两种情况下，我选择的指标都是精度。我也尝试了 Designer AUC 加权，但没有任何改善。]]></description>
      <guid>https://stackoverflow.com/questions/79055091/extremely-imbalanced-dataset-for-a-classifier</guid>
      <pubDate>Fri, 04 Oct 2024 16:10:32 GMT</pubDate>
    </item>
    <item>
      <title>nnUNetv2_plan_and_preprocess：未找到命令</title>
      <link>https://stackoverflow.com/questions/79054923/nnunetv2-plan-and-preprocess-command-not-found</link>
      <description><![CDATA[在程序中，U-Mamba，
当我运行
nnUNetv2_plan_and_preprocess -d 701 --verify_dataset_integrity

它显示
nnUNetv2_plan_and_preprocess：未找到命令

我的运行环境：
Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-101-generic x86_64)
RTX 2080ti，RTX 3090
这些事情已经完成：

conda activate env-...
torch with cuda
causal-conv1d
mamba-ssm
umamba pip install -e .
nnunetv2 pip install -e .
nnU-Net 数据集格式
导出 nnUNet_raw、nnUNet_preprocessed、nnUNet_results
]]></description>
      <guid>https://stackoverflow.com/questions/79054923/nnunetv2-plan-and-preprocess-command-not-found</guid>
      <pubDate>Fri, 04 Oct 2024 15:22:50 GMT</pubDate>
    </item>
    <item>
      <title>多模态命名实体识别</title>
      <link>https://stackoverflow.com/questions/79054866/multimodal-named-entity-recognition</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79054866/multimodal-named-entity-recognition</guid>
      <pubDate>Fri, 04 Oct 2024 15:08:57 GMT</pubDate>
    </item>
    <item>
      <title>MLFlow 在提供模型时抛出错误：配置的跟踪 uri 方案：“文件”对于代理 mlflow-artifact 方案的使用无效</title>
      <link>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</link>
      <description><![CDATA[我尝试使用 MLFlow CLI 在本地将模型作为 REST 端点提供服务。MLflow 版本为 2.11.3。以下是我使用的参数。
mlflow models serve -m &quot;runs:/7782c4a700cc49c1a04f9ce608d90358/knnmodel&quot; --env-manager local --port 5000

以下是例外情况：
回溯（最近一次调用最后一次）：
文件 &quot;/home/vagrant/.local/bin/mlflow&quot;，第 8 行，位于 &lt;module&gt;
sys.exit(cli())
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1157 行，在 __call__ 中
return self.main(*args, **kwargs)
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1078 行，在 main 中
rv = self.invoke(ctx)
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1688 行，在invoke 中
return _process_result(sub_ctx.command.invoke(sub_ctx))
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第1688，在invoke中
返回_process_result（sub_ctx.command.invoke（sub_ctx））
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第1434行，在invoke中
返回ctx.invoke（self.callback，**ctx.params）
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第783行，在invoke中
返回__callback（*args，**kwargs）
文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/cli.py”，第104行，在serve中
返回get_flavor_backend（
文件&quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/flavor_backend_registry.py&quot;，第 44 行，在 get_flavor_backend
local_path = _download_artifact_from_uri(
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py&quot;，第 105 行，在 _download_artifact_from_uri
返回 get_artifact_repository(artifact_uri=root_uri).download_artifacts(
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 124 行，在 get_artifact_repository
返回_artifact_repository_registry.get_artifact_repository(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 77 行，位于 get_artifact_repository
return storage(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py&quot;，第 27 行，位于 __init__
self.repo = get_artifact_repository(uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 124 行，在 get_artifact_repository 中
返回 _artifact_repository_registry.get_artifact_repository(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 77 行，在 get_artifact_repository 中
返回 storage(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 45 行，在 __init__ 中
super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))
文件&quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 59 行，在 resolve_uri
_validate_uri_scheme(track_parse.scheme)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 35 行，在 _validate_uri_scheme
raise MlflowException(
mlflow.exceptions.MlflowException: 配置的跟踪 uri 方案：&#39;file&#39; 无法与代理 mlflow-artifact 方案一起使用。允许的跟踪方案为：{&#39;https&#39;, &#39;http&#39;}

似乎 MLflow 无法从 MLflow Web 服务器访问工件。有解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</guid>
      <pubDate>Wed, 03 Apr 2024 09:02:38 GMT</pubDate>
    </item>
    <item>
      <title>在 FastAPI 中使用机器学习时出现“TypeError：float（）参数必须是字符串或数字，而不是‘PatientAttendance’”[关闭]</title>
      <link>https://stackoverflow.com/questions/71606629/getting-typeerror-float-argument-must-be-a-string-or-a-number-not-patienta</link>
      <description><![CDATA[我目前正在使用 FastAPI 构建 API 来部署我的逻辑回归模型。由于某种原因，我在测试模型时在服务器文档中收到上述错误。
我的代码如下：
app = FastAPI()

class PatientAttendance(BaseModel):
apptslotduration: int
patientage: int
log_distance: float
pct_appts_missed: float
doc_no_show_rate: float
zip_no_show_rate: float
note_no_show_rate: float
type_no_show_rate: float
spec_type_no_show_rate: float
monthly_no_show_rate: float
seasonal_no_show_rate: float
dow_no_show_rate: float
clinic_no_show_rate: float
lead_time_in_days: int
groupedstarttime: int
priminsurance_no_show_rate:浮点数
secondinsurance_no_show_rate：浮点数

@app.post(&#39;/predict/&#39;)
def predict(features：PatientAttendance)：
data = features
prediction = model.predict([[data]])
if prediction[0] == 0:
result = &quot;Patient Show&quot;
else:
result = &quot;No-Show&quot;
probability = model.predict_proba([[data]])

return {
&#39;prediction&#39;: prediction,
&#39;probability&#39;: probability
}

if __name__ == &#39;__main__&#39;:
uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)

错误：
TypeError: float() 参数必须是字符串或数字，而不是 &#39;PatientAttendance&#39;

我正在使用 Pydantic BaseModel，但我不知道为什么会收到此错误。我相信我的应用程序相对于服务器指向了正确的方向。我尝试使用 GET 和 POST。features 是我标准化并转换为字典的数据集中的特征数组。所有功能都已矢量化。每当我在服务器文档中测试 API 时，我似乎总是会遇到某种类型的错误。]]></description>
      <guid>https://stackoverflow.com/questions/71606629/getting-typeerror-float-argument-must-be-a-string-or-a-number-not-patienta</guid>
      <pubDate>Thu, 24 Mar 2022 17:08:12 GMT</pubDate>
    </item>
    <item>
      <title>错误：尝试在自定义 HF 数据集上使用 trainer.train() 时，vars() 参数必须具有 __dict__ 属性？</title>
      <link>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</link>
      <description><![CDATA[我有以下正在尝试微调的模型（CLIP_ViT + 分类头）。这是我的模型定义：
class CLIPNN(nn.Module):

def __init__(self, num_labels, pretrained_name=&quot;openai/clip-vit-base-patch32&quot;, dropout=0.1):
super().__init__()
self.num_labels = num_labels
# 加载预训练的转换器 &amp;处理器
self.transformer = CLIPVisionModel.from_pretrained(pretrained_name)
self.processor = CLIPProcessor.from_pretrained(pretrained_name)
# 初始化其他层（transformer 主体之后的头部）
self.classifier = nn.Sequential(
nn.Linear(512, 128, bias=True),
nn.ReLU(inplace=True),
nn.Dropout(p=dropout, inplace=False),
nn.Linear(128, self.num_labels, bias=True))

def forward(self, input, labels=None, **kwargs):
logits = self.classifier(inputs)
loss = None
if labels 不是 None:
loss_fct = nn.CrossEntropyLoss()
loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

return SequenceClassifierOutput(
loss=loss,
logits=logits,
)

我还对数据集进行了以下定义：
class CLIPDataset(nn.utils.data.Dataset):
def __init__(self, embeddings, labels):
self.embeddings = embeddings
self.labels = labels

def __getitem__(self, idx):
item = {&quot;embeddings&quot;: nn.Tensor(self.embeddings[idx])}
item[&#39;labels&#39;] = nn.LongTensor([self.labels[idx]])
return item

def __len__(self):
return len(self.labels)


注意：这里我假设模型输入的是预先计算的嵌入，而不是计算嵌入，我知道这是如果我想微调 CLIP 基础模型，那么这不是正确的逻辑，我只是​​想让我的代码工作。
类似这样的事情会引发错误：
model = CLIPNN(num_labels=2)
train_data = CLIPDataset(train_data, y_train)
test_data = CLIPDataset(test_data, y_test)

trainer = Trainer(
model=model, args=training_args, train_dataset=train_data, eval_dataset=test_data
)
trainer.train()


TypeError Traceback (most recent call last) in
----&gt; 1 trainer.train()
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py
in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,
**kwargs) 1256 self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)
1257 → 1258 for step, input in enumerate(epoch_iterator): 1259 1260 #
如果恢复训练，则跳过任何已经训练过的步骤
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py in next(self) 515 if self._sampler_iter为 None: 516 self._reset() →
517 data = self._next_data() 518 self._num_yielded += 1 519 if
self._dataset_kind == _DatasetKind.Iterable and \
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self) 555 def _next_data(self): 556 index =
self._next_index() # 可能引发 StopIteration → 557 data =
self._dataset_fetcher.fetch(index) # 可能引发 StopIteration 558 if
self._pin_memory: 559 data = _utils.pin_memory.pin_memory(数据)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py​​
在 fetch(self, perhaps_batched_index) 45 else: 46 data =
self.dataset[possibly_batched_index] —&gt; 47 return
self.collat​​e_fn(数据)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 default_data_collat​​or(features, return_tensors) 64 65 if
return_tensors == “pt”: —&gt; 66 返回
torch_default_data_collat​​or(features) 67 elif return_tensors == “tf”:
68 返回 tf_default_data_collat​​or(features)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 torch_default_data_collat​​or(features) 中 80 81 如果不是
isinstance(features[0], (dict, BatchEncoding)): —&gt; 82 features =
[vars(f) for f in features] 83 first = features[0] 84 batch = {
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
in (.0) 80 81 if not isinstance(features[0], (dict, BatchEncoding)):
—&gt; 82 features = [vars(f) for f in features] 83 first = features[0] 84
batch = {
TypeError: vars() 参数必须具有 dict 属性

知道我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</guid>
      <pubDate>Tue, 12 Oct 2021 11:12:03 GMT</pubDate>
    </item>
    </channel>
</rss>