<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 02 Aug 2024 18:20:56 GMT</lastBuildDate>
    <item>
      <title>为什么在时间序列机器学习模型中以一阶差分而不是实际变量为目标？</title>
      <link>https://stackoverflow.com/questions/78826341/why-target-the-first-difference-instead-of-the-actual-variable-in-time-series-ma</link>
      <description><![CDATA[我正在使用 LSTM 和 ANN 模型进行时间序列预测项目，我遇到的建议是，我应该以时间序列数据的一阶差分而不是实际变量为目标。
我熟悉差分以实现平稳性的概念，但我不确定为什么在使用具有滞后特征的 LSTM 和 ANN 等机器学习模型时特别推荐此步骤。
在我的实验中，当我使用实际变量而不是一阶差分作为目标时，指标似乎更好。
实际变量作为目标变量
一阶差分作为目标变量]]></description>
      <guid>https://stackoverflow.com/questions/78826341/why-target-the-first-difference-instead-of-the-actual-variable-in-time-series-ma</guid>
      <pubDate>Fri, 02 Aug 2024 16:02:27 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 ApplePersistenceIgnoreState 错误？</title>
      <link>https://stackoverflow.com/questions/78826248/how-to-fix-applepersistenceignorestate-error</link>
      <description><![CDATA[我正在学习一个关于如何使用神经网络进行图像分类的 Neuralnine 教程。
我正在使用 Imac。
下面是代码：
import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layer, models
#准备数据
(training_images, training_labels), (testing_images, testing_labels) = datasets.cifar10.load_data()
training_images, testing_images = training_images / 255, testing_images / 255

class_names = [&#39;Plane&#39;, &#39;Car&#39;, &#39;Bird&#39;, &#39;Cat&#39;, &#39;Deer&#39;, &#39;Dog&#39;, &#39;Frog&#39;, &#39;Horse&#39;, &#39;Ship&#39;, &#39;Truck&#39;]

for i in range(16):
plt.subplot(4,4,i+1)
plt.xticks([])
plt.yticks([])
plt.imshow(training_images[i], cmap=plt.cm.binary)
plt.xlabel(class_names[training_labels[i][0]])

plt.show()

training_images = training_images[:5000] #节省时间
training_labels = training_labels[:5000]
testing_images = testing_images[:4000]
testing_labels = testing_labels[:4000]

model = models.Sequential()
model.add(layers.Conv2D(32, (3,3),activation=&#39;relu&#39;, input_shape=(32,32,3)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3),激活=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3), 激活=&#39;relu&#39;))
model.add(layers.Flatten())
model.add(layers.Dense(64, 激活=&#39;relu&#39;))
model.add(layers.Dense(10, 激活=&#39;softmax&#39;))

model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

model.fit(training_images, training_labels, epochs=10, validation_data=(testing_images, testing_labels))

我在终端中收到以下消息：
ApplePersistenceIgnoreState：现有状态将不会已触及。新状态将写入 /var/folders/2g/.../T/org.python.python.savedState
2024-08-02 16:29:19.788 Python[48146:6869495] 警告：未启用可恢复状态的安全编码！通过实现 NSApplicationDelegate.applicationSupportsSecureRestorableState: 并返回 YES 来启用安全编码。

完全不知道该怎么做]]></description>
      <guid>https://stackoverflow.com/questions/78826248/how-to-fix-applepersistenceignorestate-error</guid>
      <pubDate>Fri, 02 Aug 2024 15:36:34 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 SageMaker/Hugging Face 端点以生成 Cypher 查询？</title>
      <link>https://stackoverflow.com/questions/78826111/how-to-optimize-sagemaker-hugging-face-endpoint-for-cypher-query-generation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78826111/how-to-optimize-sagemaker-hugging-face-endpoint-for-cypher-query-generation</guid>
      <pubDate>Fri, 02 Aug 2024 15:00:50 GMT</pubDate>
    </item>
    <item>
      <title>分类器 ML 基于特征和连续变量值预测类别的算法</title>
      <link>https://stackoverflow.com/questions/78825453/classifier-ml-algorithm-for-predicting-class-based-on-features-and-the-value-of</link>
      <description><![CDATA[我正在尝试编写一个分类器，我可以训练它来查看问题实例，并根据其特征和特定变量的值预测问题属于哪个类。我不是在寻找问题的答案，而只是寻找一些关于我应该专注于研究哪种 ML 算法的指导。
这是一个示例问题。假设我们有一些蛋糕食谱。它们具有以下连续变量特征：

面粉量
牛奶量
鸡蛋数量
糖的量

我们还有一个二进制变量：

蛋糕是用燃气还是电烤箱烤的？ （假设燃气为 1，电为 0）
每个食谱都烹制了两次，一次使用燃气，一次使用电

还通过让一些人吃蛋糕来测试蛋糕，每个人都给每个蛋糕一个“美味”指数。这也是一个连续变量，值越高，人们就越喜欢这个蛋糕。
我多次运行这个程序，最终得到一个数据集，每个食谱有两个条目（一个燃气，一个电），每个食谱都有各自的“美味指数”。我们会发现，对于每道菜谱，人们倾向于用燃气或电烹饪，这反映在口味指数中。
现在，我知道这是不现实的，但这是一种说明我想做什么的简单方法。
在训练系统执行此操作后，我现在想让系统采用一种新食谱，并根据具有相似（不一定相同）特征的过去食谱以及燃气或电是否提供最高的口味指数来预测应该使用燃气还是电烹饪。
任何关于哪种机器学习算法最适合此任务的建议都将不胜感激。如前所述，我只是想缩小我的研究范围，而不一定是被灌输答案。
到目前为止，我已经使用 Scikit-learn 在 Python 中尝试了几种 ML 算法。事实证明，KNN 在基于特征预测类别方面最准确，但我不确定如何将“口味指数”反映到其中。此外，这似乎是监督机器学习的一个例子，因为数据被标记（天然气或电力）。]]></description>
      <guid>https://stackoverflow.com/questions/78825453/classifier-ml-algorithm-for-predicting-class-based-on-features-and-the-value-of</guid>
      <pubDate>Fri, 02 Aug 2024 12:17:07 GMT</pubDate>
    </item>
    <item>
      <title>如何通过调整函数的一个输入参数来训练函数输出正确的值（基于训练数据）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78824490/how-to-train-a-function-to-output-a-correct-value-based-on-training-data-by-ad</link>
      <description><![CDATA[我是机器学习的新手。我有一个项目，需要帮助选择合适的机器学习算法。
我有一个包含三个输入（x、y 和 N）的函数：
F(x, y, N) → R
我希望软件自动调整并给出 N 的值（基于输入值 x 和 y），以便函数给出正确的值 R。
我有一个如下所示的训练数据集：



x
y
R




7
2
20


8
5
6


15
6
13


...
...



你对我应该寻找哪种合适的机器学习算法？如果神经网络合适，您对从多少层/节点开始有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78824490/how-to-train-a-function-to-output-a-correct-value-based-on-training-data-by-ad</guid>
      <pubDate>Fri, 02 Aug 2024 08:29:10 GMT</pubDate>
    </item>
    <item>
      <title>关于训练模型过程中遇到的问题</title>
      <link>https://stackoverflow.com/questions/78824487/questions-about-problems-encountered-during-training-model</link>
      <description><![CDATA[我目前正在训练一个 AI 模型，在训练过程中遇到了一些问题，如附图所示。有人能帮我找出问题并提出解决方案吗？问题情况和设置如下：

设置

模型的参数每 64 次迭代更新一次。

模型每个 epoch 需要 30 步，并且有 5 个 warmup epoch。

我正在使用名为 CosineAnnealingWarmup 的学习率调度程序，最大和最小学习率分别为 2e-4 和 8e-7。



问题情况

损失在一段时间内会减少，但在某个点之后开始振荡。

模型的性能指标在某个时间点之后停滞或下降点。




我将非常感激您提供的任何帮助。以下是我的损失、指标和 lr 调度程序。


我尝试了以下方法，但没有奏效。

我正在使用 AsymmetricLoss，并尝试将 gamma_neg 值设置为 2、将 gamma_pos 设置为 0、将 clip 设置为 0.1。

我还将批次大小从 256 增加到 512，但没有任何效果。

我目前正在尝试增加模型的容量，但我似乎得到了类似的结果。


对于 AsymmetricLoss 代码如下：
class AsymmetricLoss(nn.Module):
def __init__(
self, gamma_neg=3, gamma_pos=0, clip=.05, eps=1e-8,
disable_torch_grad_focal_loss=True, cls_cnt_list=None, smoothing=.1
):
super().__init__()

self.gamma_neg = gamma_neg
self.gamma_pos = gamma_pos
self.clip = clip
self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss
self.eps = eps
self.smoothing = smoothing

如果 cls_cnt_list != None:
n_samples = sum(cls_cnt_list)
self.weights = torch.tensor([n_samples / cnt for cnt in cls_cnt_list]) / n_samples

如果 torch.cuda.is_available():
self.weights = self.weights.cuda()
else:
self.weights = None

def forward(self, x, y):
n_pos = y.sum(dim=1, keepdim=True)
y = (1 - self.smoothing) * y + (self.smoothing / (y.shape[1] - n_pos))

# 计算概率
x_sigmoid = torch.sigmoid(x)
xs_pos = x_sigmoid
xs_neg = 1 - x_sigmoid

# 不对称裁剪
if self.clip is not None and self.clip &gt; 0:
xs_neg = (xs_neg + self.clip).clamp(max=1)

# 基本 CE 计算
los_pos = y * torch.log(xs_pos.clamp(min=self.eps))
los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))
loss = los_pos + los_neg

# 非对称聚焦
if self.gamma_neg &gt; 0 or self.gamma_pos &gt; 0:
if self.disable_torch_grad_focal_loss:
torch.set_grad_enabled(False)
pt0 = xs_pos * y
pt1 = xs_neg * (1 - y) # pt = p if t &gt; 0 else 1-p
pt = pt0 + pt1
one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)
one_sided_w = torch.pow(1 - pt, one_sided_gamma)
if self.disable_torch_grad_focal_loss:
torch.set_grad_enabled(True)
loss *= one_sided_w

# 类权重
if self.weights != None:
loss *= self.weights.unsqueeze(0)

return -loss.sum() * 1024
]]></description>
      <guid>https://stackoverflow.com/questions/78824487/questions-about-problems-encountered-during-training-model</guid>
      <pubDate>Fri, 02 Aug 2024 08:28:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么我用 Python 实现的 Skip-Gram 产生了错误的结果？</title>
      <link>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-in-python-producing-incorrect-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-in-python-producing-incorrect-results</guid>
      <pubDate>Fri, 02 Aug 2024 07:15:07 GMT</pubDate>
    </item>
    <item>
      <title>多类图像分类的准确率未提高超过 50%</title>
      <link>https://stackoverflow.com/questions/78823757/accuracy-for-multi-class-image-classification-not-improving-past-50</link>
      <description><![CDATA[我正在开展一个项目，根据牛仔裤背面的图案预测牛仔裤的品牌。
为此，我在网上收集了 3 个不同牛仔裤品牌（arizona、levi、lucky brand）的数据，并为每个品牌获取了大约 100 张图片。然后我做了一些数据增强，虽然非常基础
def generate_more_images():
for dir in os.listdir(&#39;jean_img&#39;):
if dir != &#39;.DS_Store&#39;:
n = 0
for image in os.listdir(f&#39;jean_img/{dir}&#39;):

if image != &#39;.DS_Store&#39;:
img = Image.open(f&#39;jean_img/{dir}/{image}&#39;)
img = img.resize((500,500))
for i in range(10):
img = img.rotate(np.random.uniform(-35,35))
bright_factor = np.random.uniform(0.6, 1.4)
augmenter = ImageEnhance.Brightness(img) 
img =增强器.增强（亮度因子）
如果 n &lt; 200：
img.save（f&#39;jean_img_expand_test/{dir}_expand/{n}_{dir}_{image}&#39;）
elif n &lt; 300:
img.save(f&#39;jean_img_expand_valid/{dir}_expand/{n}_{dir}_{image}&#39;)
else:
img.save(f&#39;jean_img_expand_train/{dir}_expand/{n}_{dir}_{image}&#39;)
n += 1

然后利用这些图像，我训练并测试了多个模型，由于这些图像的复杂性，我认为这些模型的表现会很好，但令我惊讶的是，我的模型表现最好的时候也只能达到 60 多分到 50 多分。
optimizer = Adam(learning_rate=0.001)
reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.2, waiting=5, min_lr=0.00001)

callback = callups.EarlyStopping(monitor=&#39;val_accuracy&#39;, waiting=20, restore_best_weights=True)

model = Sequential([
layer.Input(shape=(500, 500, 3)), 

layer.Conv2D(32, (3, 3), padding=&#39;same&#39;),
layer.LeakyReLU(alpha=0.3),
layer.MaxPooling2D((2, 2)),
layer.Dropout(0.25),

layer.Flatten(),

layer.Dense(64),
layer.LeakyReLU(alpha=0.3),
layer.Dropout(0.5),

layer.Dense(3,activation=&#39;softmax&#39;)
])
model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

model.fit(
train_images,
train_labels,
epochs=100,
validation_data=(val_images, val_labels),
callbacks=[callback, reduce_lr]
)

以下是一些训练图像：



我对计算机视觉还很陌生，但我认为这个分类问题并不难。我已经解决了更多类别的问题，算法运行得很好。如果有人能指出我工作中的缺陷，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78823757/accuracy-for-multi-class-image-classification-not-improving-past-50</guid>
      <pubDate>Fri, 02 Aug 2024 04:11:08 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 Pytorch 的 CrossEntropyLoss 应用类权重来解决多类多输出问题的不平衡数据分类问题</title>
      <link>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</link>
      <description><![CDATA[我正在尝试使用加权损失函数来处理数据中的类别不平衡问题。我的问题是多类别和多输出问题。例如（我的数据有五个输出/目标列（output_1、output_2、output_3），每个目标列有三个类（class_0、class_1 和 class_2）。我目前正在使用 pytorch 的交叉熵损失函数https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html，我看到它有一个权重参数，但我的理解是，这个相同的权重将统一应用于每个输出/目标，但我想在每个输出/目标中为每个类应用单独的权重。
具体来说，我可以获得如下所示的数据



A
B
C
D
E
OUTPUT_1
OUTPUT_2
OUTPUT_3




5.65
3.56
0.94
9.23
6.43
0
2
1


7.43
3.95
1.24
7.22
&lt; td&gt;2.66
0
0
0


9.31
2.42
2.91
2.64
6.28
2
0
2


8.19
5.12
1.32
3.12
8.41
0
2
0


9.35
1.92
3.12
4.13
3.14
0
1
1


8.43
9.72
7.23
8.29
9.18
1
0
2


4.32
2.12
3.84
9.42
8.19
0
1
0


3.92
3.91
2.90
8.1 9
8.41
2
0
2


7.89
1.92
4.12
8.19
7.28
0
1
2
&lt; /tr&gt;

5.21
2.42
3.10
0.31
1.31
2
0
0



因此，
输出 1 中的比例为：0 = 0.6、1 = 0.1、2 = 0.3
输出 2 中的比例为：0 = 0.4、1 = 0.3、2 = 0.3
输出 3 中的比例为：0 = 0.4、1 = 0.2、2 = 0.4

我想根据每个输出列中的类分布应用类权重，以便它重新规范化（或重新平衡？不确定这里要使用的术语是什么）第 1 类为 0.15，第 0 类和第 2 类各为 0.425（因此对于 output_1，权重将是 [0.425/0.6, 0.15/0.1, 0.425/0.3]，对于输出 2，它将是 [0.425/0.4, 0.15/0.3, 0.425/0.3] 等）。相反，我理解 pytorch 的 crossentropy 损失函数中的权重参数目前正在执行的操作是将单个类权重应用于每个输出列。我想知道我是否遗漏了什么，是否有办法使用 pytorch 的 crossentropyloss 函数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</guid>
      <pubDate>Fri, 02 Aug 2024 03:34:55 GMT</pubDate>
    </item>
    <item>
      <title>更清晰的分割 SAM (Segment Anything)</title>
      <link>https://stackoverflow.com/questions/78822914/sharper-segmentation-sam-segment-anything</link>
      <description><![CDATA[我需要像这样分割图像上的对象：
图像 1
图像 2
我选择使用 Meta 的 AI SAM（Segment Anything）来裁剪这些对象。
我的代码如下所示：
import cv2
import numpy as np
import sys, os
from pathlib import Path
import torch
import surveillance as sv
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

# 获取命令行参数
num = sys.argv[2]
img_path = sys.argv[1]
folder_path = sys.argv[3]

# 从给定路径加载图像
img = cv2.imread(img_path)

# 用于预处理图像以进行裁剪的函数
def preprocess_image_cut(img):
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
return gray

# 用于获取干净图像的函数
def get_clean_image(img, filter_level=1):
gray = preprocess_image_cut(img)
blurred_image = cv2.GaussianBlur(gray, (3, 3), 0)
_, binary_image = cv2.threshold(blurred_image, 210, 255, cv2.THRESH_BINARY)

kernel = np.ones((1, 1), np.uint8)
result_image = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel)
result_image = cv2.medianBlur(result_image, filter_level)

output_image = cv2.bitwise_or(gray, result_image)
return output_image

# 设置管道的函数
def setup_pipeline():
HOME = &#39;C:/&#39;
CHECKPOINT_PATH = os.path.join(HOME, &#39;weights&#39;, &#39;sam_vit_h_4b8939.pth&#39;)
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
MODEL_TYPE = &quot;vit_h&quot;

sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)
mask_generator = SamAutomaticMaskGenerator(sam)
return mask_generator

# 运行 SAM 模型的函数
def run_sam(mask_generator, clean_image):
image_rgb = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)
image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
sam_result = mask_generator.generate(image_rgb)
mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)
detections = sv.Detections.from_sam(sam_result=sam_result)

annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)

masks = [mask[&#39;segmentation&#39;] for mask in sorted(sam_result, key=lambda x: x[&#39;area&#39;, reverse=True])]
return mask

# 设置掩码生成器
mask_generator = setup_pipeline()

if __name__ == &quot;__main__&quot;:
if img_path.endswith(&quot;.png&quot;):
save_path = folder_path
os.makedirs(save_path, exist_ok=True)
if os.path.isdir(f&#39;{folder_path}/PROCESSED&#39;):
os.makedirs(f&#39;{folder_path}/PROCESSED&#39;, exist_ok=True)
img = cv2.imread(img_path)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
clean_image = get_clean_image(img, 3)

print(&quot;开始生成掩码&quot;)
mask = run_sam(mask_generator, clean_image)

for i, mask in enumerate(masks):
image_rgb = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)
image_bgra = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGRA)
extract_region = np.zeros_like(image_bgra)
extract_region[mask] = image_bgra[mask]
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
extract_region = extract_region[y:y+h, x:x+w]
extracted_region[:, :, 3] = (mask[y:y+h, x:x+w] &gt; 0) * 255
save_path2 = f&#39;{folder_path}/PROCESSED/img_{x}_{y}_{w}_{h}.png&#39;
cv2.imwrite(save_path2, extracted_region)

print(&quot;Finished&quot;)

但我在一些问题上遇到了困难，比如我不想让 Sam 分割数字，也不想分割将对象与数字联系起来的线条。
有人能就这个问题提出一些想法吗？也接受其他方法来分割图像。]]></description>
      <guid>https://stackoverflow.com/questions/78822914/sharper-segmentation-sam-segment-anything</guid>
      <pubDate>Thu, 01 Aug 2024 20:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Pyannote：离线加载并应用说话人区分</title>
      <link>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</link>
      <description><![CDATA[我尝试离线使用 Pyannotes 模型。
我是这样加载和应用模型的：
from pyannote.audio import Pipeline

access_token = &#39;xxxxxxxxxxx&#39;

model = Pipeline.from_pretrained(
&quot;pyannote/speaker-diarization-3.1&quot;,
use_auth_token=access_token)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

这样就没问题了。
但是现在我按照离线使用的说明操作：https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/applying_a_pipeline.ipynb
我的目录结构如下：
src-
     |-pyannote_offline_config.yaml
     |-pyannote_pytorch_model.bin
---- YAML ----
version: 3.1.0

pipeline:
name: pyannote.audio.pipelines.SpeakerDiarization
params:
clustering: AgglomerativeClustering
embedding: pyannote/wespeaker-voxceleb-resnet34-LM
embedding_batch_size: 32
embedding_exclude_overlap: true
分段：src/pyannote_pytorch_model.bin
分段批处理大小：32

参数：
聚类：
方法：质心
min_cluster_size：12
阈值：0.7045654963945799
分段：
min_duration_off：0.0

---- 正在加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但结果却是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
好的，下次尝试：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

params = {&#39;clustering&#39;:
{&#39;method&#39;: &#39;centroid&#39;,
&#39;min_cluster_size&#39;: 12,
&#39;threshold&#39;: 0.7045654963945799},
&#39;segmentation&#39;:
{&#39;min_duration_off&#39;: 0.0}}

pipeline = model.instantiate(params)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

pipeline(path_in,
num_speakers=num_speakers).labels()

但结果是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
我不明白问题所在。
如果我这样做，它就会起作用：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;, path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但上传到 gitlab 后，测试管道显示：“无法下载‘pyannote/speaker-diarization-3.1’管道。
这可能是因为管道是私有的或封闭的，因此请确保进行身份验证。访问 https://hf.co/settings/tokens
创建您的访问令牌并重试：
Pipeline.from_pretrained(&#39;pyannote/speaker-diarization-3.1&#39;,
... use_auth_token=YOUR_AUTH_TOKEN)&quot;
因此，似乎我的本地计算机上有一些东西没有通过 pip 安装下载。例如，如果我不使用 yaml 加载它，而只使用 model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;)，它也会起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</guid>
      <pubDate>Thu, 01 Aug 2024 12:28:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 llama 3 8b 训练 LLaVA-NeXT</title>
      <link>https://stackoverflow.com/questions/78820834/how-to-train-llava-next-with-llama-3-8b</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78820834/how-to-train-llava-next-with-llama-3-8b</guid>
      <pubDate>Thu, 01 Aug 2024 11:56:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在人工神经网络中进行样本外预测？</title>
      <link>https://stackoverflow.com/questions/78820463/how-to-make-out-of-sample-forecast-in-artificial-neural-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78820463/how-to-make-out-of-sample-forecast-in-artificial-neural-network</guid>
      <pubDate>Thu, 01 Aug 2024 10:36:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行单类物体检测，结果为假阳性 [关闭]</title>
      <link>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</link>
      <description><![CDATA[在这里，我尝试使用 cnn 构建一个 Manhole 物体检测，在这个模型中，经过训练我得到了 95% 的准确率。我得到的是假阳性，例如，如果我用人孔（训练对象）测试图像进​​行检测，它将绘制边界框，而我测试没有训练对象的随机图像，则会出现一个随机边界框，这就是问题所在，在实时网络摄像头测试中也是如此，但在这里，如果对象甚至没有被检测到，则会在框架中获取一些随机边界框。这里我提供我的代码，请帮助
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization``, Flatten, MaxPool2D, Dense
from pathlib import Path

train_img = Path(&quot;DATASET/train/Manhole&quot;)
val_img = Path(&quot;DATASET\valid\Manhole&quot;)

train_csv = pd.read_csv(&#39;DATASET/train/Manhole/_annotations.csv&#39;) 
val_csv = pd.read_csv(&#39;DATASET/valid/_annotations.csv&#39;)
#print(train_csv)
train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]].fillna(0)
train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]].astype(int)
train_csv.drop_duplicates(subset=&#39;filename&#39;,inplace=True, ignore_index=True)
val_csv.drop_duplicates(subset=&#39;filename&#39;, inplace=True, ignore_index=True)

def datagenerator(df ,batch_size ,path):
while True:
images = np.zeros((batch_size,640,640,3))
bounding_box_coords = np.zeros((batch_size, 4))

for i in range(batch_size):
rand_index = np.random.randint(0, train_csv.shape[0])
row = df.loc[rand_index, :]
images[i] = cv2.imread(str(path/row.filename)) / 255.
bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

产生 {&#39;filename&#39;: images}, {&#39;coords&#39;: bounding_box_coords}

# example, label = next(datagenerator(batch_size=16))
# img = example[&#39;filename&#39;][0]
# bbox_coords = label[&#39;coords&#39;][0] 

# x1, y1, x2, y2 = map(int, bbox_coords)
# print(&#39;bbox cords&#39;,x1,y1,x2,y2)
# cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
# cv2.putText(img, &#39;&#39;, (x1,y1-10),cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 2)
# # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# plt.imshow(img)
# plt.show()

input_ = 输入(shape=[640, 640, 3], name=&#39;filename&#39;)

x = input_
x = Conv2D(16, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(32, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(64, (3,3),激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(128，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(256，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(312，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(500，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(580，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(680，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Flatten()(x)
x = Dense(256，激活=&#39;relu&#39;)(x)
x = Dense(32, 激活=&#39;relu&#39;)(x)
输出坐标 = Dense(4, 名称=&#39;coords&#39;)(x)

模型 = tf.keras.models.Model(input_,output_coords)

模型摘要()

模型编译(loss={&#39;coords&#39;: &#39;mse&#39;},
优化器=tf.keras.optimizers.Adam(5e-5), 
指标={&#39;coords&#39;: &#39;accuracy&#39;})

检查点回调 = ModelCheckpoint(&#39;model_Checkpoint.h5&#39;, 监视器=&#39;val_loss&#39;, save_best_only=True, 模式=&#39;min&#39;)

模型拟合(数据生成器(df=train_csv,batch_size=6,path=train_img), 
epochs=80, steps_per_epoch=150,
validation_data=datagenerator(df=val_csv,batch_size=6,path=val_img), 
validation_steps=240, 
callbacks=[checkpoint_callback])

model.save(&#39;model2.h5&#39;)

我需要代码来在实时网络摄像头中正确检测训练过的对象，而不会出现任何边界框，并从 cnn 接收置信度值，这样我就可以设置检测的阈值]]></description>
      <guid>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</guid>
      <pubDate>Thu, 25 Jul 2024 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>NotImplementedError：无法将符号张量（up_sampling2d_4_target：0）转换为 numpy 数组</title>
      <link>https://stackoverflow.com/questions/60430561/notimplementederror-cannot-convert-a-symbolic-tensor-up-sampling2d-4-target0</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/60430561/notimplementederror-cannot-convert-a-symbolic-tensor-up-sampling2d-4-target0</guid>
      <pubDate>Thu, 27 Feb 2020 09:59:10 GMT</pubDate>
    </item>
    </channel>
</rss>