<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 10 Feb 2024 12:22:31 GMT</lastBuildDate>
    <item>
      <title>Autogluon 在训练 bagged 模型时不使用 GPU</title>
      <link>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</link>
      <description><![CDATA[我正在使用 Autogluon 0.8.2。我已经在 fit 方法中给出了相应的 GPU 参数，但我意识到在使用 GPU 在堆栈的第一层训练模型之后，当模型的袋装版本为下一个堆栈层进行训练时，GPU 不会在第一次堆栈训练后完全使用。我使用 nvidia-smi 检查了 GPU 利用率，结果为 0% 并且没有使用内存。
Autogluon 或袋装模型是否存在这样的训练问题？]]></description>
      <guid>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</guid>
      <pubDate>Sat, 10 Feb 2024 12:13:47 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列数据集训练模型，如何使用数据和时间作为输入特征？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</link>
      <description><![CDATA[我正在使用 房间占用估计数据集，包含 10129 个实例和 18 个特征。对于所有三个模型，预测准确率为 97-99%，我认为这是因为我删除了日期和时间列，因为它们是对象。
但是，我想使用日期和时间来查看准确性是否有变化。但是，我不知道该怎么做，但我最初想到从时间中提取小时和分钟并将其用作输入特征。有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</guid>
      <pubDate>Sat, 10 Feb 2024 09:46:38 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型显示 X 有 6 个特征，但 MinMaxScaler 期望有 7 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</link>
      <description><![CDATA[我正在构建一个 LSTM 模型，该模型可以随时间分析六个变量。但是，我的代码抛出错误。您能指导我在哪里更改我的代码吗？我还有一个采用这种格式的 ARIMA 模型。这种格式对于分析风速是否正确？我的任务是用 LSTM 模型组装 ARIMA 模型。但是，我无法完成这个 LSTM 模型。
随机导入
进口警告

将 numpy 导入为 np
将 pandas 导入为 pd
从 keras.layers 导入 LSTM，密集
从 keras.models 导入顺序
从 keras.optimizers 导入 Adam
从 sklearn.preprocessing 导入 MinMaxScaler

# 从 CSV 文件加载数据集
file_path = &#39;孟加拉国天气数据 (1948 - 2013).csv&#39;
df = pd.read_csv(文件路径)

# 如果有日期列，请确保 DataFrame 按日期排序
如果 df.columns 中有“日期”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期&#39;])
    df.sort_values(&#39;日期&#39;, inplace=True)

# 预定义的特征和目标变量
selected_features = [&#39;最高温度&#39;、&#39;最低温度&#39;、&#39;降雨量&#39;、&#39;相对湿度&#39;、&#39;云量覆盖&#39;、&#39;明亮阳光&#39;]
目标=&#39;风速&#39;

# 要求用户输入所选特征的值
用户输入 = {}
对于 selected_features 中的功能：
    value = float(input(f&quot;输入 {feature} 值：&quot;))
    用户输入[特征] = 值

# 要求用户输入年、月、站的值
user_input[&#39;YEAR&#39;] = int(input(&quot;请输入年份：&quot;))
user_input[&#39;月份&#39;] = int(input(&quot;请输入月份：&quot;))
user_input[&#39;车站名称&#39;] = input(&quot;输入车站：&quot;)

# 使用占位符值将“Wind_Speed”列添加到 user_input_df
user_input_df = pd.DataFrame({**user_input, &#39;Wind_Speed&#39;: [0]})

# 禁用特定警告
warnings.simplefilter(“忽略”, UserWarning)
warnings.simplefilter(“忽略”, FutureWarning)

尝试：
    # 确保 df[target] 有适当的索引
    如果不是 isinstance(df.index, pd.RangeIndex):
        df.reset_index(drop=True, inplace=True)

    # 对每个特征使用 MinMaxScaler 进行特征缩放
    缩放器 = MinMaxScaler()

    scaled_data = scaler.fit_transform(df[selected_features + [目标]])


    # 创建用于 LSTM 训练的序列
    序列长度 = 10
    x_train, y_train = [], []

    对于范围内的 i（sequence_length，len（scaled_data））：
        x_train.append(scaled_data[i - 序列长度:i, :-1])
        y_train.append(scaled_data[i, -1])

    x_train, y_train = np.array(x_train), np.array(y_train)

    # 重塑 LSTM 的输入数据
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))

    # 构建 LSTM 模型
    模型=顺序（）
    model.add(LSTM(单位=100, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(LSTM(单位=100，return_sequences=True))
    model.add(LSTM(单位=50, return_sequences=False))
    model.add(密集(单位=1))

    # 编译模型
    model.compile(优化器=Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

    # 训练模型
    model.fit(x_train、y_train、epochs=5、batch_size=16、validation_split=0.1)

    # 准备用于预测的输入数据
    输入=scaled_data[-sequence_length:, :-1]
    输入 = 缩放器.transform(输入)
    输入=输入.reshape(1,sequence_length,len(selected_features))


    ＃ 作出预测
    预测 = model.predict(输入)
    预测 =scaler.inverse_transform(预测.reshape(-1, 1))


    # 引入额外的随机性
    random_perturbation = random.uniform(-2, 0.5) # 根据需要调整范围
    预测+=随机扰动

    print(f&#39;预测下一周期的{目标}：{预测[0, 0]}&#39;)

    #MAPE计算
    random_mape = random.uniform(12, 14)
    print(f&#39;估计平均绝对百分比误差: {random_mape:.2f}%&#39;)

    # 将用户输入和预测输出合并到一个新的 DataFrame 中
    output_data = pd.concat([user_input_df, pd.DataFrame({目标: [预测[0, 0]]})], axis=1)

    # 将输入数据和预测输出保存到新的 CSV 文件中
    输出文件路径 = &#39;LSTM_Predictions_Output.csv&#39;
    输出数据.to_csv（输出文件路径，索引=False）

    print(f&#39;输入数据和预测输出保存到{output_file_path}&#39;)


除了 ValueError 为 e：
    打印（f&#39;错误：{e}&#39;）

最后：
    warnings.resetwarnings()
]]></description>
      <guid>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</guid>
      <pubDate>Sat, 10 Feb 2024 07:45:08 GMT</pubDate>
    </item>
    <item>
      <title>理想的step_per_epoch和纪元数是多少[关闭]</title>
      <link>https://stackoverflow.com/questions/77972223/what-is-the-ideal-step-per-epoch-and-no-of-epoch</link>
      <description><![CDATA[我的总数据点为 1000000 个观察值，分别分为 80000 个和 20000 个训练/测试观察值。理想的step_per_epoch是多少，epoch数和Batch_size]]></description>
      <guid>https://stackoverflow.com/questions/77972223/what-is-the-ideal-step-per-epoch-and-no-of-epoch</guid>
      <pubDate>Sat, 10 Feb 2024 07:29:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练过程中随机裁剪图像并协调标签？</title>
      <link>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</link>
      <description><![CDATA[我的任务是头影测量地标定位。
我在此数据框中显示坐标 X1,Y1 的图像路径。

&lt;标题&gt;

文件名
X1
Y1


&lt;正文&gt;

/Images_data/binary0006.png
89
80


/Images_data/binary0008.png
37
70


/Images_data/binary0007.png
50
76


/Images_data/binary0003.png
55
92


/Images_data/binary0005.png
91
64


/Images_data/binary0004.png
100
76



训练时如何裁剪图像并坐标X1,Y1？]]></description>
      <guid>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</guid>
      <pubDate>Sat, 10 Feb 2024 07:13:06 GMT</pubDate>
    </item>
    <item>
      <title>是否可以训练神经网络对包含特定形状的像素进行分组？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</link>
      <description><![CDATA[我想训练一个模型，可以训练该模型对形状物体（例如图像中的牙齿）进行分组。如果它接收到图像作为输入，它应该输出一个包含子列表的列表，每个子列表包含检测到的牙齿的像素。因此，如果输入图像在处理之前被展平，则输出列表应如下所示：
[（像素1，像素2，像素3，...），（像素4，像素5，像素6，...），...]，
其中pixel1、pixel2等表示属于牙齿的像素的索引（假设输入图像首先被展平）。
没有牙齿的图像应该有一个空列表作为输出。
每个训练数据样本都应该有一个图像和一个牙齿列表。
我不太确定如何构建一个神经网络来按照我希望的方式执行此任务（如果可能的话）。
我最初的方法是训练一个单独的模型，将一个小像素网格（如 24x24）分类为“牙齿”或“非牙齿”，然后扫描更大的输入图像（可能是 200x200）并对以下部分进行分组归类为牙齿。
有更好的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</guid>
      <pubDate>Sat, 10 Feb 2024 02:50:50 GMT</pubDate>
    </item>
    <item>
      <title>用Python训练分类器</title>
      <link>https://stackoverflow.com/questions/77970572/training-classifier-in-python</link>
      <description><![CDATA[我用 Python (PyCharm) 编写了一个分类器。它不显示模型的训练阶段。如何在我的代码中解决这个问题？我想查看分类器的训练阶段。在 Jupiter 笔记本中工作时，所有内容都会显示（您可以在图片中看到它）。
第二个问题是无休止的编译。介绍性行“进程已完成，退出代码为 0”没有出现在最后。我的代码中的行 &#39;print(&#39;\nTest set precision: {accuracy:0.3f}\n&#39;.format(**train_result))&#39; 也不起作用。
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从sklearn导入数据集
从 sklearn.preprocessing 导入 LabelEncoder

CSV_COLUMN_NAMES = [&#39;SEPAL_LENGTH&#39;、&#39;SEPAL_WIDTH&#39;、&#39;PETAL_LENGTH&#39;、&#39;PETAL_WIDTH&#39;、&#39;SPECIES&#39;]

数据= pd.read_csv（&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39;，名称= CSV_COLUMN_NAMES，标题= 0）

编码器 = LabelEncoder()
数据[&#39;SPECIES&#39;] =编码器.fit_transform(data[&#39;SPECIES&#39;])
打印（数据.head（））
X = pd.DataFrame(data, columns=data.columns.drop(&#39;SPECIES&#39;))

y = data.pop(&#39;物种&#39;)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

def input_fn（特征，标签，训练= True，batch_size = 256）：
    数据集 = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    数据集 = 数据集.batch(10)
    如果训练：
        数据集 = dataset.shuffle(1000).repeat()

    返回数据集.shuffle(1000).repeat()

my_feature_columns = [] # 记录特征

对于 X_train.keys() 中的密钥：
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

分类器= tf.estimator.DNNClassifier（feature_columns = my_feature_columns，hidden_​​units =。[30,10]，n_classes = 3）


 classifier.train（input_fn = lambda：input_fn（X_train，y_train，训练= True），步骤= 5）

 classifier.evaluate(input_fn=lambda: input_fn(X_test, y_test, Training=False))
 print(&#39;\n测试集精度: {accuracy:0.3f}\n&#39;.format(**train_result))
]]></description>
      <guid>https://stackoverflow.com/questions/77970572/training-classifier-in-python</guid>
      <pubDate>Fri, 09 Feb 2024 20:04:15 GMT</pubDate>
    </item>
    <item>
      <title>WEKA凯姆包</title>
      <link>https://stackoverflow.com/questions/77934889/weka-caim-package</link>
      <description><![CDATA[在网络搜索中找不到任何用于 CAIM 离散化的 WEKA 包。我需要 WEKA v3 的软件包。
在 google 上搜索 WEKA 软件包，但没有找到任何内容，但一些文档 说它存在。
谁能提供 WEKA 的 CAIM 包的工作链接吗？]]></description>
      <guid>https://stackoverflow.com/questions/77934889/weka-caim-package</guid>
      <pubDate>Sun, 04 Feb 2024 07:13:41 GMT</pubDate>
    </item>
    <item>
      <title>keras多类分类欠拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</guid>
      <pubDate>Mon, 29 Jan 2024 06:41:23 GMT</pubDate>
    </item>
    <item>
      <title>CNN 图像分类奇怪的 Sigmoid 预测问题 [已关闭]</title>
      <link>https://stackoverflow.com/questions/77893518/cnn-image-classification-weird-sigmoid-predictions-issue</link>
      <description><![CDATA[我正在使用 Tensorflow 开发机器学习神经网络模型，该模型可以识别一个人是否戴着口罩或未给出图像。请先阅读其余部分，然后再进入 github 链接（其中包含整个笔记本，以防万一模型本身以外的问题）。
我的问题是，模型训练后（准确度为 96%，损失为 15%），预测输出一个奇怪的 sigmoid 值，远低于阈值 0.5，并且在使用图像的测试图像之间无关紧要戴口罩的人和不戴口罩的人。
这是使用 Tensorflow 函数的神经网络模型：
模型 = 顺序([
    Conv2D(16, (3,3), 1, 激活=&#39;relu&#39;, input_shape = (256,256,3)),
    最大池化2D(),
    辍学率（0.25），

    Conv2D(32, (3,3), 1, 激活=&#39;relu&#39;),
    最大池化2D(),
    辍学率（0.25），

    Conv2D(16, (3,3), 1, 激活=&#39;relu&#39;),
    最大池化2D(),
    辍学率（0.25），

    展平（），

    密集（256，激活=&#39;relu&#39;），
    辍学（0.5），

    密集（1，激活=&#39;sigmoid&#39;）
]）

到目前为止我做了什么：
我首先认为这是过度拟合（仍然可能是），其中我的第一直觉是降低学习率。在这次修复之前，预测的 sigmoid 值在负数中使用了科学计数法，但后来他们冷静下来，恢复了不使用科学计数法。在其他人帮助我一点之后，我还添加了 Dropout 层，这也让 sigmoid 值平静了一点。然而，它们仍然不是正确的预测，并且它们小于 0.1，这不是 sigmoid 函数所期望的。]]></description>
      <guid>https://stackoverflow.com/questions/77893518/cnn-image-classification-weird-sigmoid-predictions-issue</guid>
      <pubDate>Sun, 28 Jan 2024 02:58:25 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>如何提高这个回归问题的准确率？</title>
      <link>https://stackoverflow.com/questions/63036212/how-to-improve-accuracy-score-for-this-regression-problem</link>
      <description><![CDATA[我正在使用 UCI 的学生表现数据集。我想根据给定的特征预测学生的最终结果。
我首先尝试使用两个主要且高度相关的特征 G1 和 G2，它们是两次考试的成绩。我使用 LinearRegression 算法，得到的准确度为 0.4 或更低。
然后我尝试对数据框中对象的所有特征进行特征工程，但准确性仍然相同。
如何提高准确度分数？
我的代码作为 Python 笔记本&lt; /p&gt;
from matplotlib import pyplot as plt
将seaborn导入为sns
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split

从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.linear_model 导入 ElasticNet
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.ensemble 导入 ExtraTreesRegressor
从 sklearn.ensemble 导入 GradientBoostingRegressor
从 sklearn.svm 导入 SVR

从 sklearn.metrics 导入mean_squared_error、mean_absolute_error、median_absolute_error、accuracy_score

df = pd.read_csv(&#39;student-mat.csv&#39;,sep=&#39;;&#39;)
df2 = pd.read_csv(&#39;student-por.csv&#39;,sep=&#39;;&#39;)

df = [df,df2]
df = pd.concat(df)
df = pd.get_dummies(df)

X = df.drop(&#39;G3&#39;,轴=1)
y = df[&#39;G3&#39;]

X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.1，random_state = 42）

模型=线性回归()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)
y_pred = [int(round(i)) for i in y_pred]

准确度分数（y_test，y_pred）
]]></description>
      <guid>https://stackoverflow.com/questions/63036212/how-to-improve-accuracy-score-for-this-regression-problem</guid>
      <pubDate>Wed, 22 Jul 2020 14:09:00 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的边缘检测器</title>
      <link>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</link>
      <description><![CDATA[我已阅读以下内容关于使用机器学习进行边缘检测的博客。他们

&lt;块引用&gt;
  使用了基于现代机器学习的算法。该算法是在图像上进行训练的，其中人类注释了最重要的边缘和对象边界。给定这个标记数据集，训练机器学习模型来预测图像中每个像素属于对象边界的概率。

我想使用 opencv 来实现这项技术。
有人知道或知道如何使用 Opencv 实现/开发此方法吗？
我们如何注释最重要的边缘和对象边界以供机器学习算法使用？]]></description>
      <guid>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</guid>
      <pubDate>Sat, 08 Apr 2017 18:51:27 GMT</pubDate>
    </item>
    <item>
      <title>r 神经网络包——多输出</title>
      <link>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</link>
      <description><![CDATA[我目前使用神经网络的方式是它从许多输入点预测一个输出点。更具体地说，我运行以下命令。
nn &lt;- 神经网络(
as.公式(a ~ c + d),
数据 = Z，隐藏 = c（3,2），err.fct =“sse”，act.fct = 自定义，
线性.输出=真，重复= 5）

这里，如果 Z 是一个由名称为 a、b、c 的列组成的矩阵，它将根据 c 行和 d 行中的对应点预测 a 列中某一行的一个点。 （以垂直维度作为训练样本。）
假设还有一列 b。我想知道是否有办法从 c 和 d 预测 a 和 b？我已经尝试过
as.formula(a+b ~ c+d)

但这似乎不起作用。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</guid>
      <pubDate>Thu, 07 Jan 2016 19:29:54 GMT</pubDate>
    </item>
    <item>
      <title>监督学习，(ii) 无监督学习，(iii) 强化学习</title>
      <link>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</link>
      <description><![CDATA[在阅读有关监督学习、无监督学习、强化学习的内容时，我遇到了以下问题并感到困惑。请帮助我识别以下三个中哪一个是监督学习、无监督学习、强化学习。
什么类型的学习（如果有）最能描述以下三种场景：
(i) 为自动售货机创建硬币分类系统。为此，
开发商从美国造币厂获取准确的硬币规格并得出
尺寸、重量和面额的统计模型，自动售货机
然后机器对其硬币进行分类。
(ii) 算法不是调用美国造币厂来获取硬币信息，而是
赠送一大套贴有标签的硬币。该算法使用该数据来
推断自动售货机随后用来对其进行分类的决策边界
硬币。
(iii) 计算机通过重复玩井字游戏制定策略
并通过惩罚最终导致失败的举动来调整策略。]]></description>
      <guid>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</guid>
      <pubDate>Wed, 03 Apr 2013 09:00:48 GMT</pubDate>
    </item>
    </channel>
</rss>