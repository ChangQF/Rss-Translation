<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Sep 2024 12:33:22 GMT</lastBuildDate>
    <item>
      <title>我正在实现 qwen_2-vl +byaldi，用于基于视觉的 ocr，并将其托管为基于 stremlit 的 Web 应用程序，但它无法正常工作，一直崩溃</title>
      <link>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</link>
      <description><![CDATA[我正在开发一个 Streamlit 应用程序，该应用程序利用多模态模型进行图像搜索和文本提取。然而，在尝试下载提取的文本时，该应用程序经常在文本提取过程中崩溃。以下是相关代码，以及重现问题的步骤。
requirement.txt
pdf2image

git+https://github.com/huggingface/transformers.git

qwen-vl-utils

#flash-attn

byaldi

qwen_vl_utils

transformers

重现步骤

使用提供的代码运行 Streamlit 应用程序。
上传有效的图像文件（JPG、JPEG、PNG）。
在提供的输入框中输入文本查询。
单击“搜索并提取文本”按钮。

import streamlit as st
import base64
from huggingface_hub import notebook_login
from byaldi import RAGMultiModalModel
from transformers import Qwen2VLForConditionalGeneration、AutoTokenizer、AutoProcessor
从 PIL 导入图像
从 io 导入 BytesIO
导入 torch
导入 re

@st.cache_resource
def load_models():
RAG = RAGMultiModalModel.from_pretrained(&quot;vidore/colpali&quot;, verbose=10)
model = Qwen2VLForConditionalGeneration.from_pretrained(
&quot;Qwen/Qwen2-VL-2B-Instruct&quot;,
torch_dtype=torch.float16,
device_map=&quot;auto&quot;,
)
processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen2-VL-2B-Instruct&quot;)
return RAG、model、processor

RAG、model、processor = load_models()

st.title(&quot;多模态图像搜索和文本提取App”)

uploaded_file = st.file_uploader(“选择图片”，type=[“jpg”，“jpeg”，“png”])

如果 uploaded_file 不为 None:
image = Image.open(uploaded_file)
st.image(image, caption=&#39;Uploaded Image&#39;, use_column_width=True)

temp_image_path = “uploaded_image.jpeg”
image.save(temp_image_path)

@st.cache_data
def create_rag_index(image_path):
RAG.index(
input_path=image_path,
index_name=&quot;image_index&quot;,
store_collection_with_index=True,
overwrite=True,
)

create_rag_index(temp_image_path)

text_query = st.text_input(&quot;输入您的文本查询&quot;)

if st.button(&quot;搜索并提取文本&quot;):
if text_query:
results = RAG.search(text_query, k=1, return_base64_results=True)

image_data = base64.b64decode(results[0].base64)
image = Image.open(BytesIO(image_data))
st.image(image, caption=&quot;结果图像&quot;, use_column_width=True)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{&quot;type&quot;: &quot;image&quot;},
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;extract text&quot;}
]
}
]

text_prompt = processing.apply_chat_template(messages, add_generation_prompt=True)

input = processing(
text=[text_prompt],
images=[image],
padding=True,
return_tensors=&quot;pt&quot;
)

输入 = 输入.to(model.device)

使用 torch.no_grad():
输出_ids = 模型.generate(**输入, max_new_tokens=1024)

生成_ids = 输出_ids[:, 输入.输入_ids.shape[1]:]

输出_text = 处理器.batch_decode(
生成_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
)[0]

# 突出显示查询的文本
def 突出显示_text(text, 查询):
突出显示_text = 文本
for word in query.split():
pattern = re.compile(re.escape(word), re.IGNORECASE)
突出显示_text = 模式.sub(lambda m: f&#39;&lt;span style=&quot;background-color: yellow;&quot;&gt;{m.group()}&lt;/span&gt;&#39;, highlight_text)
return highlight_text

highlight_output = highlight_text(output_text, text_query)

st.subheader(&quot;提取的文本（查询突出显示）：&quot;)
st.markdown(highlighted_output, unsafe_allow_html=True)
else:
st.warning(&quot;请输入查询。&quot;)
else:
st.info(&quot;上传图片以开始。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</guid>
      <pubDate>Wed, 25 Sep 2024 10:56:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 AI / OCR 检测和验证文档中的复选框[关闭]</title>
      <link>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</link>
      <description><![CDATA[我需要开发可以自动验证称为“W 合同”的文档的软件。这些合同包含许多需要填写的复选框和字段，我的目标是准确识别哪些框被选中并根据特定规则和参数验证值。
我曾尝试使用 OpenAI 的 API 来检测这些元素，但我发现它在处理文档中的大量复选框和选项时遇到了困难。它经常会错误识别结构或无法正确识别复选框。
我正在寻找可以帮助我更准确地执行此任务的技术、库或 API 的建议，最好使用 OCR 和计算机视觉。理想的解决方案应该能够：
检测 PDF 或图像文档中的复选框和输入字段。
准确确定复选框是否被选中。
读取字段并根据某些规则验证数据。
任何关于可以促进这项任务的工具或方法的建议都将不胜感激。我对基于 AI 的解决方案持开放态度，但也对传统的 OCR 方法感兴趣。
]]></description>
      <guid>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</guid>
      <pubDate>Wed, 25 Sep 2024 10:22:52 GMT</pubDate>
    </item>
    <item>
      <title>将双线性采样特征映射回体素</title>
      <link>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</link>
      <description><![CDATA[我正在开发一个专门用于双线性采样的类。我的目标是将从双线性采样过程中提取的特征映射到创建的体素网格中的适当位置。
实施步骤：
B = 批次

C = 通道

S = 视图数

D = 深度

H = 高度

W = 宽度

3 = x,y,z

2 = x,y

我创建了一个具有此形状 [B,D,H,W,3] 的体素
并且我有 360 个图像视图样本，尺寸为 [B,S,C,H,W]
第一步，我使用外部和内部将体素点（点云）投影到每个图像。然后我过滤了图像之外的点。
结果我得到了 valid_points = [B,S,H,W,2]
我从 valid_points 中对我的点进行了归一化，并创建了大小为 [B,S,H,W,2] 的网格，请注意，W 是有效点的数量，H = 1
然后我所做的是应用双线性采样，如代码所示：
valid_points = cur_coords[:, on_img[1]]

######### 将有效点归一化在 [-1, 1] 之间 ########
normalized_points = torch.zeros_like(valid_points)
normalized_points[:,:, 0] = 2.0 * (valid_points[:, :, 0] / (H_img - 1)) - 1.0 # 归一化y 坐标
normalized_points[:,:, 1] = 2.0 * (valid_points[:, :, 1] / (W_img - 1)) - 1.0 # [N, M&#39;,2]

grid = normalized_points.unsqueeze(1).cuda() # 形状 [S, H_out, W_out, 2]
sampled_features_with_location_list = []
for i in range(0,N):
img_s =camera_view_tensor[i].unsqueeze(0).permute(0, 3, 1, 2) #[B, C, H_in, W_in]
grid_s = grid[i].unsqueeze(0)
sampled_points = F.grid_sample(img_s, grid_s,mode=&#39;bilinear&#39;,
align_corners=None) # (B,N,C,H_out,W_out)
sampled_features_list.append(sampled_pointson)
sampled_points = torch.stack(sampled_features_list, dim=1) #[B = 1,S = 6,C= 3,H= 1,W =22965]

现在我想应用逆映射来提取 bev 特征。怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</guid>
      <pubDate>Wed, 25 Sep 2024 10:14:39 GMT</pubDate>
    </item>
    <item>
      <title>在不断演变的特征空间中采用反馈进行自适应预测的技术</title>
      <link>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</link>
      <description><![CDATA[我正在研究一个预测问题，其中目标变量 𝑦 来自正态分布，连续特征空间 𝑋 和 𝑦 之间的关系随时间保持稳定。但是，目标值（例如平均值和标准差）会随着系统的变化而随时间变化。我事先并不了解真正的目标值，因此我利用在线回归和强化学习 (RL) 等技术根据反馈迭代调整我的预测。
反馈机制仅指示我的预测是高估还是低估，并且此反馈是在延迟后提供的。误差大小未知，并且仅提供方向性反馈（高估/低估）。我目前正在使用增量方法，根据反馈向上或向下调整预测以更新预测。
关于改进此方法，我有两个主要问题：
(1) 改进自适应调整技术：

我目前正在使用在线回归和 RL，反馈会告诉我是否增加或减少先前的预测。除了简单的增量/减量之外，我是否应该探索更先进的技术来进行更智能的调整？具体来说，当反馈仅限于过度/不足指示时，是否有方法可以进行更细微的调整，并且随着时间的推移，这可能导致更快的收敛或更好的预测？

(2) 自适应更新值的有效管理：

为了增强我目前的方法，我考虑保持预测的动态上限和下限，并根据反馈调整这些界限（即缩小过度/不足估计之间的范围）。我探索的另一种策略是使用先前调整的指数加权移动平均线 (EWMA)，其中反复的低估会导致逐渐增大的校正。但是，在大型特征空间中管理这些调整在计算上是昂贵的。
我最初使用字典将特征 𝑋 映射到这些更新值（例如界限或 EWMA 调整）的方法随着特征空间的增长变得不切实际。我也尝试将这些值映射到回归模型，但效果不佳，可能是由于更新的非平稳性质。
鉴于这些调整值不是静态目标，而是基于反馈动态变化的，在高维特征空间中有效管理或建模它们的最佳方法是什么？在这样的设置中，是否有更合适的自适应更新策略，可能涉及函数逼近技术或内存高效的数据结构？
]]></description>
      <guid>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</guid>
      <pubDate>Wed, 25 Sep 2024 09:24:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 opencv 清理（去噪）图像？</title>
      <link>https://stackoverflow.com/questions/79021532/how-to-clean-denoise-an-image-using-opencv</link>
      <description><![CDATA[我试图只检测较大的字母并尝试将它们保存在一个平面中（想象它是一块黑板，我试图在其中打印较大的字母）。
我到目前为止执行的步骤

将图像转换为 hsv。

使用 inRange 功能我创建了一个蒙版。

添加了形态学关闭，以便我消除较小的噪音。

终于开始绘制大字母供我参考（如下所述，一些字母未被检测到）。

当我打印黑板时，会省略一个或两个字母。


我有下面的代码
#include &lt;iostream&gt;
#include &lt;fstream&gt;

#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;opencv2/dnn.hpp&gt;

using namespace cv;
using namespace cv::dnn;
using namespace std;

int main()
{
// 加载图像
cv::Mat image = cv::imread(&quot;/run/media/cams/B4267D4B267D0F9A/Downloads/2.png&quot;);
// cv::Mat image = cv::imread(&quot;/run/media/cams/B4267D4B267D0F9A/Downloads/np7.jpg&quot;);
// cv::Mat image = cv::imread(&quot;/run/media/cams/B4267D4B267D0F9A/Downloads/license-plate.png&quot;);

if (image.empty()) {
std::cerr &lt;&lt; &quot;错误：无法加载图像。&quot; &lt;&lt; std::endl;
return -1;
}

cv::Mat resizedImage;
cv::Size newSize(513, 134);

cv::resize(image,resizedImage,newSize);

int rows = resizedImage.rows;
int cols = resizedImage.cols;

cv::Mat result(rows, cols ,resizedImage.type(), cv::Scalar(0, 0, 0));

// 转换为灰度
cv::Mat gray;
cv::cvtColor(resizedImage, gray, cv::COLOR_BGR2GRAY);
imshow(&quot;Gray-img&quot;, gray);

// 转换为 hsv
cv::Mat hsv;
cv::cvtColor(resizedImage, hsv, cv::COLOR_BGR2HSV);
imshow(&quot;HSV&quot;, hsv);

cv:: Mat others;
cv::cvtColor(resizedImage, others, cv::COLOR_BGR2HSV_FULL);
imshow(&quot;HSV_FULL&quot;, others);

cv::Mat mask;
cv::Scalar lower(0, 0, 0); // 下限
cv::Scalar upper(179, 100, 130); // 上界

cv::inRange (others, lower, upper, mask);
imshow(&quot;Mask&quot;, mask);

// 创建一个矩形结构元素
cv::Mat kernel = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(3, 3));

// 执行形态学闭运算
cv::Mat close;
cv::morphologyEx(mask, close, cv::MORPH_CLOSE, kernel, cv::Point(-1, -1), 1);

// 将单通道闭图像合并为 3 通道图像
cv::Mat extract;
std::vector&lt;cv::Mat&gt; channels(3, close); // 创建一个包含三个“close”副本的向量
cv::merge(channels, extract); // 将它们合并为单个 3 通道图像

cv::imshow(&quot;Closed Mask&quot;, close);
// cv::imshow(&quot;Merged Image&quot;, extract);

// 查找轮廓
std::vector&lt;std::vector&lt;cv::Point&gt;&gt; contours;
// std::vector&lt;cv::Vec4i&gt; 层次结构;
cv::findContours(close, contours, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE);

for (const auto&amp; c : contours) {
// 获取边界矩形
cv::Rect boundingBox = cv::boundingRect(c);
int area = boundingBox.width * boundingBox.height;

// 根据面积进行过滤
if ( area&gt;2500 &amp;&amp; area&lt;5000) {
// 在原始图像上绘制矩形
cv::rectangle(resizedImage, boundingBox, cv::Scalar(36, 255, 12), 3);

// 从 &#39;extract&#39; 中提取区域并将其复制到 &#39;result&#39;
cv::imshow(&quot;contourssss&quot;, resizedImage);
waitKey(0);

// 从 &#39;extract&#39; 中提取区域并将其复制到 &#39;result&#39;
cv::Mat roi = extract(boundingBox); // 从 extract 中提取感兴趣的区域
roi.copyTo(result(boundingBox)); // 复制到 result 中的相应区域
}
}

// 显示结果
cv::imshow(&quot;Contours&quot;, result);
cv::waitKey(0);
cv::destroyAllWindows();

return 0;

}

输出]]></description>
      <guid>https://stackoverflow.com/questions/79021532/how-to-clean-denoise-an-image-using-opencv</guid>
      <pubDate>Wed, 25 Sep 2024 07:12:25 GMT</pubDate>
    </item>
    <item>
      <title>SBERT 微调总是在完成所有 epoch 之前停止</title>
      <link>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</link>
      <description><![CDATA[我正在使用 SBERT 预训练模型（特别是 MiniLM）进行一个包含 995 个分类的文本分类项目。我大部分时间都在按照此处列出的步骤进行操作，一切似乎都运行正常。
我的问题出现在实际训练模型时。无论我在训练参数中设置什么值，训练似乎总是提前结束，并且永远不会完成所有批次。例如，我设置了 num_train_epochs=1，但它最多只能达到 0.49 个 epoch。如果 num_train_epochs=4，它总是在 3.49 个 epoch 处结束。
这是我的代码：
from datasets import load_dataset
from sentence_transformers import (
SentenceTransformer,
SentenceTransformerTrainer,
SentenceTransformerTrainingArguments,
SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
&quot;nreimers/MiniLM-L6-H384-uncased&quot;,
model_card_data=SentenceTransformerModelCardData(
language=&quot;en&quot;,
license=&quot;apache-2.0&quot;,
model_name=&quot;all-MiniLM-L6-v2&quot;,
)
)

loss = BatchAllTripletLoss(model)
# 损失概述：https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# 此特定损失方法：https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss

# 训练参数：https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
# 必需参数：
output_dir=&quot;finetune/model20240924&quot;,
# 可选训练参数：
num_train_epochs=1,
max_steps = -1,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
learning_rate=1e-5,
warmup_ratio=0.1,
fp16=True, # 如果您收到 GPU 无法在 FP16 上运行的错误，请设置为 False
bf16=False, # 如果您拥有支持 BF16 的 GPU，请设置为 True
batch_sampler=BatchSamplers.GROUP_BY_LABEL, # 
# 可选的跟踪/调试参数：
eval_strategy=&quot;no&quot;,
eval_steps=100,
save_strategy=&quot;epoch&quot;,
# save_steps=100,
save_total_limit=2,
logs_steps=100,
run_name=&quot;miniLm-triplet&quot;, # 如果在 W&amp;B 中使用`wandb` 已安装
)

trainer = SentenceTransformerTrainer(
model=model,
args=args,
train_dataset=trainDataset,
eval_dataset=devDataset,
loss=loss,
#evaluator=dev_evaluator,
)
trainer.train()

请注意，我没有使用评估器，因为我们正在创建模型，并在事后使用专用的测试值集对其进行测试。我的数据集结构如下：
Dataset({
features: [&#39;Title&#39;, &#39;Body&#39;, &#39;label&#39;],
num_rows: 23961
})

与 dev 数据集具有相同的结构，只是行数较少。这将提供以下输出：
 [1473/2996 57:06 &lt; 59:07，0.43 it/s，Epoch 0/1]
步骤训练损失
100 1.265600
200 0.702700
300 0.633900
400 0.505200
500 0.481900
600 0.306800
700 0.535600
800 0.369800
900 0.265400
1000 0.345300
1100 0.516700
1200 0.372600
1300 0.392300
1400 0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={&#39;train_runtime&#39;: 3427.9198, &#39;train_samples_per_second&#39;: 6.99, &#39;train_steps_per_second&#39;: 0.874, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5003972503496366, &#39;epoch&#39;: 0.4916555407209613})

无论我如何调整值，我都无法让它完成所有批次。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</guid>
      <pubDate>Wed, 25 Sep 2024 03:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何将 CIFAR10 模型的准确率提高到 80% 以上？</title>
      <link>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</link>
      <description><![CDATA[有人能帮助我吗？我使用来自 tensorflow 数据集的 CIFAR10 数据集训练我的机器学习模型，但我无法将模型准确率提高到 80% 以上...
有人能给我一个建议吗？
import tensorflow as tf
import time
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

def normalize(train_images, test_images):
normalized_train_dataset = tf.cast(train_images, tf.float32) / 255.0
normalized_test_dataset = tf.cast(test_images, tf.float32) / 255.0
返回 normalized_train_dataset, normalized_test_dataset

# Normalisasi Dataset
train_dataset, test_dataset = normalize(train_images, test_images)

def visual(image, image_sample=2):
for i in range (image_sample):

print(f&quot;弯曲图像：{np.shape(image)}&quot;)
print(f&quot;弯曲数据：{image[i].dtype}&quot;)
print(f&quot;Nilai 最大图像：{np.max(image[i])}&quot;)
print(f&quot;Nilai 最小图像：{np.min(image[i])}&quot;)

plt.figure(figsize=(6,6))
plt.imshow(image[i])
plt.axis(&#39;off&#39;)
plt.colorbar()
plt.title(&quot;Gambar CIFAR-10&quot;)
plt.grid(False)
plt.show()

visualization(train_dataset)

train_labels = np.squeeze(train_labels)
test_labels = np.squeeze(test_labels)

print(f&quot;Shape Of Train Label : {train_labels.shape}&quot;)

print(f&quot;Shape Of Test_Label : {test_labels.shape}&quot;)

train_labels= to_categorical(train_labels, num_classes=10)
test_labels = to_categorical(test_labels, num_classes=10)

从 tensorflow.keras.preprocessing.image 导入 ImageDataGenerator

datagen = ImageDataGenerator(
rotation_range=20,
width_shift_range=0.2,
height_shift_range=0.2,
sheath_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,
fill_mode=&#39;nearest&#39;
)

model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(32, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu, input_shape=(32, 32, 3)),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(64, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(512, (3,3)，padding=&#39;same&#39;，activation=tf.nn.relu，kernel_regularizer=tf.keras.regularizers.l2(0.01))，
tf.keras.layers.BatchNormalization()，
tf.keras.layers.MaxPool2D((2,2)，strides=2)，

tf.keras.layers.Flatten()，
tf.keras.layers.Dense(512，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.3)，

tf.keras.layers.Dense(128，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.5)，

tf.keras.layers.Dense(10，激活=tf.nn.softmax)
])

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

early_stopping = tf.keras.callbacks.EarlyStopping(
monitor=&#39;val_loss&#39;,
patience=5,
restore_best_weights=True
)

reducer_lr = tf.keras.callbacks.ReduceLROnPlateau(
monitor=&#39;val_loss&#39;,
factor=0.2,
patience=3,
verbose=1,
min_lr=0.00001
)

callbacks = [early_stopping, reducer_lr]

start_time = time.time()

history = model.fit(datagen.flow(
train_dataset,
train_labels,
batch_size=64),
epochs=30,
validation_data=(test_dataset, test_labels),
callbacks=callbacks,
verbose=1
)

end_time = time.time()
training_time = end_time - start_time
print(f&quot;训练时间：{training_time/60:.2f} 分钟&quot;)

model.save(&#39;hand_gesture_detect.keras&#39;)

# 评估模型
loss_val, accuracy_val = model.evaluate(test_dataset, test_labels)
print(f&quot;损失：{loss_val}&quot;)
print(f&quot;准确率：{accuracy_val}&quot;)

来自 tensorflow.keras.applications 导入 ResNet50

base_model = ResNet50(weights=&#39;ImageNet&#39;, include_top=False, input_tensor=(32, 32, 3))

我已经使我的模型复杂化，但准确率仍然只有 77-80%，我不知道如何提高我的模型准确率]]></description>
      <guid>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</guid>
      <pubDate>Wed, 25 Sep 2024 01:55:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在 GPU 上运行 gridSearchCV 或 randonizedSerchCV</title>
      <link>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</link>
      <description><![CDATA[我想运行 gridSearchCV 或 randonizedSerchCV 来使用 GPU 在 Colab 环境中调整超参数。
但我找不到这些函数与 GPU 兼容的实现。
在这种情况下，我该如何调整超参数？
因此，由于我找不到在 GPU 上调整超参数的函数，我尝试实现 randonizedSerchCV。但我认为一定有一种方法可以做到这一点，而无需手动实现该函数。]]></description>
      <guid>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</guid>
      <pubDate>Wed, 25 Sep 2024 01:53:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型会拍摄两张图像并验证它们是否相同。
我在网上搜索过，但只能找到有关识别（一对多）的答案。
如何在文本或代码中创建这样的模型？
为了澄清起见，我说的相同是指眼睛相同，即它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>如何为排名模型生成数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</link>
      <description><![CDATA[我正在尝试创建两阶段推荐系统：使用矩阵分解生成候选对象，并使用 Lambdarank 排名模型对其进行排名。我有两个选项来生成数据集：

取 128 个项目
（相关项目 + 随机项目填充），使用第一个模型对其进行评分和排序，然后使用此序列训练第二个模型（因此我们始终具有相对值。
对所有项目进行评分，排序并取前 128 个项目，然后进行训练（我们可能没有相关项目，但我认为这更自然，因为在生产中我们将以这种方式进行预测）。

那么，哪个更好？此外，在训练中使用小权重（类似于隐式 ALS）填充项目是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</guid>
      <pubDate>Tue, 24 Sep 2024 16:14:00 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</guid>
      <pubDate>Tue, 24 Sep 2024 15:17:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将预测值合并回数据集？</title>
      <link>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</link>
      <description><![CDATA[我已经在 Python 中训练了一个 XGboost 模型，并将概率列表作为输出。我如何将这些概率带到原始数据集，以便在一个 DF 中拥有数据 + 预测值？假设我的原始原始测试 df 称为 df_raw。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
model = XGBClassifier(n_estimators=1500, max_depth=5, n_jobs=-1, min_child_weight=2, 
early_stopping_rounds=25)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
test_outputs = model.predict_proba(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</guid>
      <pubDate>Tue, 24 Sep 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>提取哪些特征来聚类文本？</title>
      <link>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</link>
      <description><![CDATA[我想为文本制作一个分类器，进一步用于为给定的文本推荐最相似的文本。
应用程序的流程如下：

使用 llm 从文本中提取 10 个主要主题（它可以从 150 个词池中选择）
我将词向量设为二进制向量，基本上在 150 维空间中工作，其中每个文本都有一个坐标，例如 [1, 0, 1, ..., 0]
然后我使用 cosine 距离找到最近的邻居（我想扩展到 3-5，但为了简单起见，我们假设只有一个）
我收到了最接近的文本

问题是文本非常不同，并且 llm 可以很好地提供主题，但是建议的文本并不完全符合我的预期。我尝试根据重要性对主题进行排序，并使向量非二进制（[10, 0, 0, 9, ..., 1]），但这似乎没有太大帮助。
我想知道这种方法是否不适合我的问题，或者我是否应该使用其他参数或其他任何东西来对我的文本进行分组。]]></description>
      <guid>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</guid>
      <pubDate>Wed, 11 Sep 2024 14:56:49 GMT</pubDate>
    </item>
    <item>
      <title>什么是 Killed:9 以及如何在 macOS 终端中修复？</title>
      <link>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</link>
      <description><![CDATA[我有一段用于机器学习项目的简单 Python 代码。我有一个相对较大的自发语音数据库。我开始训练我的语音模型。由于这是一个庞大的数据库，我让它连夜工作。早上我醒来时看到终端中出现一个神秘的
Killed: 9
行。没有其他内容。没有其他错误消息或需要处理的内容。代码运行良好约 6 小时，占整个过程的 75%，所以我真的不明白哪里出了问题。
什么是 Killed:9 以及如何修复它？浪费数小时的计算时间非常令人沮丧……
如果这很重要，我正在使用 macOS Mojave 测试版。提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</guid>
      <pubDate>Tue, 14 Aug 2018 03:28:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使 FeatureUnion 返回 Dataframe</title>
      <link>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</link>
      <description><![CDATA[所以我目前有一个包含大量客户转换器的管道：
p = Pipeline([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #添加 [&quot;time&quot;] 列的自定义转换器
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #添加 [&quot;zip&quot;] 列的自定义转换器
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #添加 onehot 列的自定义转换器
])

每个转换器都接收一个 pandas 数据框并返回包含一个或多个新列的相同数据框。它实际上运行得很好，但我如何并行运行“GetTimeFromDate”和“GetZipFromAddress”步骤？
我想使用 FeatureUnion：
f = FeatureUnion([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #添加 [&quot;time&quot;] 列的自定义转换器
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #添加 [&quot;zip&quot;] 列的自定义转换器])
])

p = Pipeline([
(&quot;FeatureUnionStep&quot;,f),
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #添加 onehot 列的自定义转换器
])

但问题是 FeatureUnion 返回的是 numpy.ndarray，而“GroupByTimeandZip”步骤需要一个数据框。
有没有办法让 FeatureUnion 返回 pandas 数据框？]]></description>
      <guid>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</guid>
      <pubDate>Fri, 15 Apr 2016 16:18:14 GMT</pubDate>
    </item>
    </channel>
</rss>