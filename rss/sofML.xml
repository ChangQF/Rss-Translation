<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 04 Dec 2023 09:15:06 GMT</lastBuildDate>
    <item>
      <title>实施文本分类注意力机制的问题</title>
      <link>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</link>
      <description><![CDATA[我第一次尝试注意力机制
我无法理解 keras 中 Attention 和 MultiHeadAttention 的使用。 （实现上混乱，概念上很清楚）
我的一些疑问。

想知道如何准确使用它们吗？

我发现像这样的线条
model.add（MultiHeadAttention（num_heads = 8，key_dim = 16，attention_axes =（1, 1）））
或者
model.add(Attention(use_scale=True))

以类似的方式，我从头开始找到代码，但它们都不能直接工作，所以可以使用哪些代码？

我在 ML/DL 中创建文本分类模型的一些工作背景
我当前的模型有
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(嵌入(max_features, 128))
model.add(双向(LSTM(64)))
model.add（密集（1，激活=&#39;sigmoid&#39;））

现在我想添加注意力机制来改进它
任何参考资料或帮助都可能非常有帮助]]></description>
      <guid>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</guid>
      <pubDate>Mon, 04 Dec 2023 08:47:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 生成头像图像的良好路线图/工作流程是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像并输出专业头像]]></description>
      <guid>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</guid>
      <pubDate>Mon, 04 Dec 2023 05:47:06 GMT</pubDate>
    </item>
    <item>
      <title>在本地设备中使用 MMOCR 进行文本识别推理期间出现“FileNotFoundError”</title>
      <link>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</link>
      <description><![CDATA[SAR 文本识别模型用于自定义训练车牌数据集以识别尼泊尔字符。我使用 Google Drive 训练模型来识别文本。现在我想在本地设备上使用权重（以 .pth 扩展名结尾的文件）进行推理。 text_rec_model包含.pth文件的路径
infer = TextRecInferencer(weights=text_rec_model)

我在 PyCharm 终端中抛出此错误。
FileNotFoundError: [Errno 2] 没有这样的文件或目录: &#39;/content/drive/MyDrive/mmocr_tut/mmocr/configs/textrecog/sar/../../../dicts/english_digits_symbols.txt &#39;

配置文件是否保存在.pth 文件中？如果是这样，那么我如何编辑以便可以使用最新的检查点（在本例中为 epoch_85.pth 文件）运行推理]]></description>
      <guid>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</guid>
      <pubDate>Mon, 04 Dec 2023 04:35:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 上强制在 GPU 上训练模型？</title>
      <link>https://stackoverflow.com/questions/77597016/how-to-force-train-a-model-on-gpu-on-google-colab</link>
      <description><![CDATA[我正在尝试使用 Tensorflow 在 Google Colab 上训练 GAN 模型。然而，每个时期需要非常非常长的时间。我正在使用 Google Colab Pro，尽管我选择了 T4 GPU 作为运行时类型选择了 T4 GPU，我注意到它根本不使用 GPU。Google Colab 不使用 GPU 
我想知道如何通过强制使用 GPU 来改善运行时间。
我尝试使用
 与 tf.device(&#39;/device:GPU:0&#39;):

环绕我的火车功能，但它仍然没有任何区别。 Google 似乎只提供了有关如何在 Colab 中将运行时更改为 GPU 的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77597016/how-to-force-train-a-model-on-gpu-on-google-colab</guid>
      <pubDate>Mon, 04 Dec 2023 03:06:05 GMT</pubDate>
    </item>
    <item>
      <title>minMax TicTacToe 代码返回错误的最佳可能移动</title>
      <link>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</guid>
      <pubDate>Mon, 04 Dec 2023 02:07:13 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是一个特殊的全包“引理” spaCy 用于任何代词，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
[功能] https://i.stack.imgur.com/vwEYe.png&lt; /a&gt;
[测试] https://i.stack.imgur.com/37mTh.png&lt; /a&gt;
这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>MLPClassifier 适合二元分类吗？</title>
      <link>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</link>
      <description><![CDATA[我编写了一个使用 MLPClassifier 来解决二元分类问题的程序。它有点有效，但我不相信这是正确的模型。
我有 1300 个整数的十六进制数要放入两个类之一：类 0 和类 1。一个潜在的问题是，在我的训练数据中，98% 属于类 0，因此我将从 &quot; 获得 98% 的准确率“预测函数”总是返回“class 0”与输入无关。
是否有专为此类问题设计的机器学习模型？
================================================== ===============
TLDR？
我的数据如下：
X = 数组([[ 0, 11, 51, 13, 0, 9],
       [51,13,0,9,0,11],
       [ 0, 8, 0, 10, 0, 13],
       ...,
       [ 0, 11, 61, 12, 0, 8],
       [ 0, 8, 0, 0, 60, 11],
       [30, 11, 0, 6, 0, 9]])

目标是 y，一个包含 1300 个 0 和 1 的列表。我使用 MLPClassifier 并获得了 98% 的预测准确率。这时我突然想到，98% 的元组恰好属于 0 类，因此，如果我不费心进行任何机器学习，而是猜测类始终为 0，那么我将获得 98% 的准确率。
我检查了拟合度，看看它在 1 类元组上的表现如何，发现其中 82% 的预测正确，因此准确度为 98% 的 82%，即大约 80%，我想改进，但是怎么办？
除了盲目增加层的大小/数量之外，我不知道如何更改 MLPClassifier 的参数，但我突然想到，我很可能使用完全错误的模型来解决带有 Yes/ 的学习问题没有分类。另外，六元组中的整数不是任意的，我想到这也可能与模型的选择有关。特别是，六个输入中的三个始终在 0 - 15 范围内，另外三个是两位数代码，第一位数字有三种可能，第二位数字有两种可能。
感谢您的任何想法。
代码：
m = MLPClassifier(hidden_​​layer_sizes = (256, 128, 64), max_iter=10000) # 从我在网上找到的示例粘贴:(
_ = m.fit(X, y)
yhat = m.predict(X)
cm = 混淆矩阵(y, yhat)
print( &#39;准确率 = &#39;, np.mean( y == yhat ) )
打印（厘米）

输出：
准确度 = 0.9816653934300993
[[1267 13]
 [11 18]]

(pdb) class1 = [ i for i in range(len(y)) if y[i] == 1]
(pdb) z = m.predict(np.row_stack((X[q] for q in class1)))
(pdb) z
数组([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 0, 1])
(pdb) len(z), 总和(z)
29, 24
]]></description>
      <guid>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</guid>
      <pubDate>Sun, 03 Dec 2023 23:53:22 GMT</pubDate>
    </item>
    <item>
      <title>如何预测回归模型的健康百分比值</title>
      <link>https://stackoverflow.com/questions/77596584/how-to-predict-health-percentage-values-for-regression-model</link>
      <description><![CDATA[我目前正在研究预测维护数据集，其中包含来自传感器的数据、任何错误的发生以及每台机器的一些特征。在进行特征工程之后，我创建了一个列，其中包含引擎剩余使用寿命的％百分比。
y_column 是一系列从 0 到 1 均匀递增的变量，当达到 1（表示发生错误）时返回到 0。我正在使用回归模型，我的结果如下图所示。由于我是机器学习新手，解决此类问题的方法是什么？我比较习惯分类。有哪些方法可以改善此类结果？
我添加了滚动平均值/标准差/最小值/最大值，我添加了滞后特征，我复制了该数据集分类方法中使用的一些技术，但即使我得到 20% 的 RMSE 和 MAE，它也没有捕获问题的形象正确。到目前为止，我正在使用 GradientBoostingRegressor KNeighborsRegressor RandomForestRegressor。改善结果的一般技巧有哪些？顺便说一句，模型无需任何超参数调整即可拟合。对这种类型的结果有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77596584/how-to-predict-health-percentage-values-for-regression-model</guid>
      <pubDate>Sun, 03 Dec 2023 23:49:03 GMT</pubDate>
    </item>
    <item>
      <title>在TF中实现FedAvg算法的问题</title>
      <link>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</link>
      <description><![CDATA[我在 google colab 中使用 TF 实现 FL（不使用 TFF），我使用 Bot-IoT 数据集，并将数据划分为 10 个客户端，以便每个客户端学习所有类型的课程，以便可以在所有课程上进行训练。 
我面临的问题是聚合从每个客户端接收到的模型权重，以获得全局准确率和召回率，在我的代码中为 0%。
这是聚合函数：
&lt;块引用&gt;
def federated_averaging(client_weights):
新权重 = []
# 模型层数

# 权重求和
forweights_list_tuple in zip(*client_weights): # 迭代每一层
    layer_mean = np.mean(np.array([np.array(weights) 用于weights_list_tuple中的权重]), axis=0)
    
    new_weights.append(layer_mean)

返回新的权重

我的代码是收集每轮的模型权重，然后对权重进行平均，得到用于计算全局准确率的平均权重。]]></description>
      <guid>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</guid>
      <pubDate>Sun, 03 Dec 2023 23:31:31 GMT</pubDate>
    </item>
    <item>
      <title>如何运行以下 Python 和 Boto3 代码？</title>
      <link>https://stackoverflow.com/questions/77596323/how-do-i-run-the-following-python-and-boto3-code</link>
      <description><![CDATA[我正在尝试探索 AWS 的机器学习模型。 AWS 最近发布了 Bedrock 服务，我想设置一个环境来执行此操作。我还需要与 S3、Langchain 和 Vector 数据库（例如 Pinecone）进行交互。这里有一些关于我可能会玩的东西的想法，我会尝试遵循这个：示例
我的 PC 上有 VS Code，并且我看过一份指南，其中详细介绍了使用 Pythin 和 Boto3 进行设置的一些步骤：指南
我从示例链接中看到的是，我猜测使用了 Jupyter notbeook。另外（请原谅我的天真），如果我将 S3 文件放入 Vector 数据库，这些服务将需要启动（或设置）肯定，并且由我支付。否则这个 Vector 数据库将位于哪里以及 Langchain 将在哪里运行？
例如运行：
%pip install --no-build-isolation --force-reinstall \
    “boto3＞=1.28.57” \
    “awscli&gt;=1.29.57” \
    “botocore”=1.31.57”

%pip install --quiet langchain==0.0.309

您将如何以及在哪里键入此命令来运行？
之后，Vector 数据库将仅通过 AWS Bedrock（它处理许多底层内容）进行查询。所以我想我的其他问题是最初将数据导入矢量数据库：
我需要 AWS Sagemaker 吗？我在设置 SageMaker 时看到，您可以指定 EC2 或某些计算能力。
有没有一种简单的方法可以在没有 SageMaker 的情况下使用此示例，或者 SageMaker 是最容易设置和使用的？
或者此处链接的指南是否足够？
本质上，要使用我链接的示例，我的设置应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77596323/how-do-i-run-the-following-python-and-boto3-code</guid>
      <pubDate>Sun, 03 Dec 2023 22:03:30 GMT</pubDate>
    </item>
    <item>
      <title>二元分类的最佳机器学习模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77596028/best-machine-learning-model-for-binary-classification</link>
      <description><![CDATA[我有 1300 个整数的十六进制数要放入两个类之一：类 0 和类 1。一个潜在的问题是，在我的训练数据中，98% 属于类 0，因此我将从 &quot; 获得 98% 的准确率“预测函数”总是返回“class 0”与输入无关。
是否有专为此类问题设计的机器学习模型？
================================================== ===============
TLDR？
我的数据如下：
X = 数组([[ 0, 11, 51, 13, 0, 9],
       [51,13,0,9,0,11],
       [ 0, 8, 0, 10, 0, 13],
       ...,
       [ 0, 11, 61, 12, 0, 8],
       [ 0, 8, 0, 0, 60, 11],
       [30, 11, 0, 6, 0, 9]])

目标是 y，一个包含 1300 个 0 和 1 的列表。我使用 MLPClassifier 并获得了 98% 的预测准确率。这时我突然想到，98% 的元组恰好属于 0 类，因此，如果我不费心进行任何机器学习，而是猜测类始终为 0，那么我将获得 98% 的准确率。
我检查了拟合度，看看它在 1 类元组上的表现如何，发现其中 82% 的预测正确，因此准确度为 98% 的 82%，即大约 80%，我想改进，但是怎么办？
除了盲目增加层的大小/数量之外，我不知道如何更改 MLPClassifier 的参数，但我突然想到，我很可能使用完全错误的模型来解决带有 Yes/ 的学习问题没有分类。另外，六元组中的整数不是任意的，我想到这也可能与模型的选择有关。特别是，六个输入中的三个始终在 0 - 15 范围内，另外三个是两位数代码，第一位数字有三种可能，第二位数字有两种可能。
感谢您的任何想法。
代码：
m = MLPClassifier(hidden_​​layer_sizes = (256, 128, 64), max_iter=10000) # 从我在网上找到的示例粘贴:(
_ = m.fit(X, y)
yhat = m.predict(X)
cm = 混淆矩阵(y, yhat)
print( &#39;准确率 = &#39;, np.mean( y == yhat ) )
打印（厘米）

输出：
准确度 = 0.9816653934300993
[[1267 13]
 [11 18]]

(pdb) class1 = [ i for i in range(len(y)) if y[i] == 1]
(pdb) z = m.predict(np.row_stack((X[q] for q in class1)))
(pdb) z
数组([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 0, 1])
(pdb) len(z), 总和(z)
29, 24
]]></description>
      <guid>https://stackoverflow.com/questions/77596028/best-machine-learning-model-for-binary-classification</guid>
      <pubDate>Sun, 03 Dec 2023 20:16:04 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML 模型给出的准确度、F1 分数、精确度和召回率均为 1.0，但似乎过度拟合 [关闭]</title>
      <link>https://stackoverflow.com/questions/77595988/my-ml-model-gives-accuracy-f1-score-precision-and-recall-as-1-0-but-it-seems-o</link>
      <description><![CDATA[
我有 Spotify 音乐数据集。
playlist_genre 是目标变量，它有 6 个类别 - 摇滚、拉丁、R&amp;B、说唱、流行、器乐
如果我使用标签编码对目标变量进行编码，那么我得到的准确度和 F1 分数为 1.0
如果我使用 getDummies 或 one-hot 编码，那么我的准确度和 f1 分数将分别降至 0.29 和 0.19。

使用的分类算法：随机森林
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/77595988/my-ml-model-gives-accuracy-f1-score-precision-and-recall-as-1-0-but-it-seems-o</guid>
      <pubDate>Sun, 03 Dec 2023 20:05:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 svm 进行高光谱图像分类</title>
      <link>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</link>
      <description><![CDATA[将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
从 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler



# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.1, random_state=42)

   # 将 SMOTE 应用于训练数据
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # 使用 RandomForestClassifier （如您的示例中所示）
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train_resampled, y_train_resampled)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度=准确度_得分(y_test, y_pred)
    print(&#39;准确度：&#39;, 准确度)

对于此代码，我遇到错误，没有足够独特的样本用于训练测试分割。如何解决这个问题？
我尝试过欠采样、不同的 ckassifiers，如 svm、knn 和随机森林分类器（对数据 imabalance 不太敏感）。仍然无法解决该错误。]]></description>
      <guid>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</guid>
      <pubDate>Sun, 03 Dec 2023 13:04:03 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类，XGBClassifier 与 xgb.train 的 AUC 分数不同，即使在舍入预测概率后也是如此</title>
      <link>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</link>
      <description><![CDATA[看起来 XGBClassifier 是 xgb.train 的包装器。我试图使用 xgb.train 和 &quot;objective&quot;: &quot;binary:logistic&quot; 训练二元分类器。
似乎使用 xgb.train 后跟 .predict() 返回预测概率，我们可以将其舍入为 0 或 1 以进行分类（基于我在另一个叠加问题中看到的答案，xgb.train 结果概率，我们需要对其进行四舍五入以获得实际的分类预测）
因此，如果我们有二元分类，我们需要四舍五入 &lt;0.5 ==&gt; 0级且&gt;0.5==&gt; 1 级？
但是如果它是多类怎么办？那么我们假设 为均匀分布并均匀地分割它？例如3级==&gt; 0~1/3、1/3~2/3、2/3~1？
使用xgb.train作为分类器的正确方法是什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb

data = np.random.rand(50,10) # 50个实体，每个实体包含10个特征
label = np.random.randint(2, size=50) # 二进制目标

dtrain = xgb.DMatrix(数据, 标签=标签)
param = {&#39;max_depth&#39;:3, &#39;eta&#39;:0.1, &#39;silent&#39;:1, &#39;tree_method&#39;:&#39;hist&#39;,&#39;objective&#39;:&#39;binary:logistic&#39;, &#39;seed&#39;:42}
num_round = 100 # 与估计器的数量相同
bst = xgb.train( 参数, dtrain, num_round)
trainres = bst.predict(dtrain)

模型= xgb.XGBClassifier（n_estimators = 100，objective =&#39;binary：logistic&#39;，tree_method =&#39;hist&#39;，eta = 0.1，max_深度= 3，enable_categorical = True，seed = 42）
模型 = model.fit(数据,标签)
fitres = model.predict(数据)

# 比较分类
print(all([trainres 中的 x 的 round(x)] == fitres))


# 比较概率。预测概率给出 0 类和 1 类的概率，因此取 x[1]
print(all([x[1] for x in model.predict_proba(data)] == trainres))

输出：
&lt;前&gt;&lt;代码&gt;正确

真的
]]></description>
      <guid>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</guid>
      <pubDate>Fri, 01 Dec 2023 08:32:40 GMT</pubDate>
    </item>
    <item>
      <title>如何确定sklearn中MLPClassifier的“损失函数”？</title>
      <link>https://stackoverflow.com/questions/53369866/how-can-i-determine-loss-function-for-mlpclassifier-in-skilearn</link>
      <description><![CDATA[我想使用sklearn的MLPClassifier
mlp = MLPClassifier(hidden_​​layer_sizes=(50,), max_iter=10, alpha=1e-4,
                求解器=&#39;sgd&#39;，详细=10，tol=1e-4，random_state=1，
                Learning_rate_init=.1)

我没有找到损失函数的任何参数，我希望它是mean_squared_error。是否可以根据模型确定它？]]></description>
      <guid>https://stackoverflow.com/questions/53369866/how-can-i-determine-loss-function-for-mlpclassifier-in-skilearn</guid>
      <pubDate>Mon, 19 Nov 2018 07:12:03 GMT</pubDate>
    </item>
    </channel>
</rss>