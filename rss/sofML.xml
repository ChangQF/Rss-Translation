<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 17 Jun 2024 15:16:56 GMT</lastBuildDate>
    <item>
      <title>Huggingface 自动求导</title>
      <link>https://stackoverflow.com/questions/78633325/huggingface-autograd</link>
      <description><![CDATA[我正在尝试微调（LoRA 微调）预训练语言模型。我遇到了梯度没有反向传播的情况。起初，我以为是因为我使用了 Huggingface 的 .generate() 方法，该方法具有 @no_grad 装饰器。但是，即使我在自定义自回归调用中使用 .forward() 方法来生成输出，我仍然无法让梯度反向传播。以下是我的反向传播测试代码。
def _test_gradient_prop():
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model_id = &quot;meta-llama/Meta-Llama-3-8B&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
device_map=&quot;auto&quot;
)
print(model)

managed_model = AutoregressiveGenerator(model, tokenizer, 64, 0.7)
optimizer = torch.optim.Adam(model.parameters())

question = &quot;吉萨金字塔位于意大利。&quot;
answer = &quot;这是真的。&quot;
test_template = f&quot;考虑以下答案中的真实性数量：问题：{question} 答案：{answer} 答案中的真实性数量是&quot;
input_ids = tokenizer.encode(test_template, return_tensors=&#39;pt&#39;).to(device)
# input_ids.requires_grad = True # 仅适用于浮点数
input_ids = torch.autograd.Variable(input_ids)

torch.manual_seed(0)
output = managed_model.managed_forward(input_ids)

optimizer.zero_grad()
dummy = torch.rand_like(output[0].float()) * 100
loss = torch.nn. functional.mse_loss(output[0].to(torch.float), dummy)
loss.requires_grad = True

print(loss)
loss.backward()

for name, param in model.named_pa​​rameters():
if &#39;mlp&#39; in name:
# print(f&quot;Parameter: {name}, require_grad: {param.requires_grad}&quot;)
if param.grad 为 None:
print(f&quot;{name} 的梯度为 None&quot;)
else:
print(f&quot;{name} 的梯度：{param.grad.sum()}&quot;)

 def __init__(self, model, tokenizer, max_new_tokens,temperature):
super().__init__()
self.model = model
self.tokenizer = tokenizer
self.max_new_tokens = max_new_tokens
self.temperature =temperature
self.last_token = None
self.token_counter = 0

def managed_forward(self, input_ids):
generated_ids = input_ids
for i in range(self.max_new_tokens):
generated_ids = self.forward(generated_ids)

if self.last_token == self.tokenizer.eos_token_id:
break
return generated_ids

def forward(self, input_ids):
generated_tokens = input_ids

output = self.model(generated_tokens)
logits = output.logits

next_logit = logits[:, -1, :] # 下一个 logit
next_logit = next_logit / self.temperature # 温度缩放
next_probs = torch.nn. functional.softmax(next_logit, dim=-1) # 获取概率

# 采样
next_token_id = torch.multinomial(next_probs, num_samples=1) # 从概率分布中绘制
# next_token_id = torch.argmax(next_logit, dim=-1).unsqueeze(-1) # 最大可能 token
self.last_token = next_token_id.item() # 用于管理终止

#附加到序列
generated_tokens = torch.cat((generated_tokens, next_token_id), dim=1)

return generated_tokens

def reset(self):
self.token_counter = 0

def __forward__(self, x):
pass

我还怀疑这可能是因为 input_ids 是一个长张量。是不是因为输入是一个长张量，所以没有为下游计算梯度？&lt;​​/p&gt;
提前谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78633325/huggingface-autograd</guid>
      <pubDate>Mon, 17 Jun 2024 14:53:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LSTM 模型中输入多个特征来预测单个特征输出？</title>
      <link>https://stackoverflow.com/questions/78633065/how-to-input-multiple-features-to-predict-a-single-feature-output-in-lstm-model</link>
      <description><![CDATA[我可以使用一个特征来预测输出。我想检查如果我再使用一个特征，我的准确率是否会更高。我想知道如何在 LSTM 模型中添加更多特征。我使用了 https://www.geeksforgeeks.org/multivariate-time-series-forecasting-with-lstms-in-keras/ 提供的代码，它使用两个特征来预测两个输出，我想使用两个特征在同一时间戳来生成一个输出。有人可以给我一个演示代码吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78633065/how-to-input-multiple-features-to-predict-a-single-feature-output-in-lstm-model</guid>
      <pubDate>Mon, 17 Jun 2024 14:03:06 GMT</pubDate>
    </item>
    <item>
      <title>带有测试集和带有校准的验证集的 ROC 曲线</title>
      <link>https://stackoverflow.com/questions/78631896/roc-curve-with-test-set-and-validation-set-with-calibration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78631896/roc-curve-with-test-set-and-validation-set-with-calibration</guid>
      <pubDate>Mon, 17 Jun 2024 09:40:10 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有非奇异输出的鉴别器（针对修改后的 WGAN 架构）可能比传统鉴别器表现更好？</title>
      <link>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</link>
      <description><![CDATA[以下是我提出的（修改后的）WGAN 模型的生成器和鉴别器模型架构，用于将降雨数据下采样 4 倍（仅作为测试）。数据集的输入形状是 (8030, 14, 21)，我想要的输出形状是 (8030, 28, 42)。
def build_generator(input_shape):
inputs = Input(shape=input_shape, name=&#39;generator_input&#39;)

# 下采样
downsample_3 = conv_block(inputs, 128, (3, 3), &#39;downsample_3&#39;)
downsample_2 = conv_block(downsample_3, 64, (3, 3), &#39;downsample_2&#39;)
downsample_1 = conv_block(downsample_2, 32, (3, 3), &#39;downsample_1&#39;)

# 瓶颈
bottleneck_0 = conv_block(downsample_1, 16, (3, 3), &#39;bottleneck_0&#39;)
bottleneck_00 = conv_block(bottleneck_0, 16, (3, 3), &#39;bottleneck_00&#39;)

# 上采样
upsample_1 = deconv_block(bottleneck_00, 32, (3, 3), &#39;upsample_1&#39;)
upsample_2 = deconv_block(upsample_1, 64, (3, 3), &#39;upsample_2&#39;)
upsample_3 = deconv_block(upsample_2, 128, (3, 3), &#39;upsample_3&#39;)

# 最终上采样至所需形状
output = Conv2DTranspose(filters=1, kernel_size=(3, 3), kernel_initializer=he_normal(), padding=&#39;same&#39;, 
strides=(2, 2), activated=&#39;relu&#39;, name=&#39;final_upsample_conv&#39;)(upsample_3)

model =模型（输入=输入，输出=输出，名称=&#39;generator&#39;）

返回模型

def build_discriminator（输入形状）：
输入层 = 输入（形状=输入形状，名称=&#39;discriminator_input&#39;）

x = conv_block（输入层，过滤器=16，内核大小=（3，3），名称=&#39;conv1&#39;）#，使用批处理规范=False）
x = conv_block（x，过滤器=32，内核大小=（3，3），名称=&#39;conv2&#39;）#使用批处理规范=False）
x = conv_block（x，过滤器=128，内核大小=（3，3），名称=&#39;conv3&#39;）#使用批处理规范=False）

x = MaxPooling2D（）（x）
x = Dropout（0.25）（x）
输出层 = Dense（1，激活=&#39;线性&#39;）（x）

模型= Model(inputs=input_layer, output=output_layer, name=&#39;discriminator&#39;)

返回模型

根据文献中的 WGAN 实现，我使用 RMSprop 优化器代替 ADAM，学习率为 5e-5，动量为 0.5。此外，我将鉴别器权重剪裁为 -1e-2 和 1e-2 之间。传统上（据我所知），鉴别器模型输出单个值损失，表示它是否能够区分真实输出和虚假输出。在我的情况下，鉴别器输出的形状为 (None, 14, 21, 1)，它似乎效果更好，但我不明白为什么。有人知道为什么会发生这种情况吗？提前致谢！
编辑：这些是 conv_block 和 deconv_block 的架构
def conv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2D(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_conv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
x = Dropout(0.25, name=name+&#39;_dropout&#39;)(x)
return x

def deconv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2DTranspose(filters=filters, kernel_size=kernel_size，padding=&#39;same&#39;，kernel_initializer=he_normal()，name=name+&#39;_deconv&#39;)(x)
x = LeakyReLU(alpha=0.2，name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
return x
]]></description>
      <guid>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</guid>
      <pubDate>Mon, 17 Jun 2024 09:29:09 GMT</pubDate>
    </item>
    <item>
      <title>Yolo v8 检测 20 秒后自动终止</title>
      <link>https://stackoverflow.com/questions/78631575/yolo-v8-detection-auto-terminate-after-20-seconds</link>
      <description><![CDATA[from ultralytics import YOLO

model = YOLO(&#39;best.pt&#39;)
results = model(source = 0, show=True, conf=0.4, save=
True)

此代码的问题是此代码持续运行，可以通过按 control + c 终止。但我尝试过许多方法在 20 秒后自动终止代码，但这些方法都不起作用。有办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78631575/yolo-v8-detection-auto-terminate-after-20-seconds</guid>
      <pubDate>Mon, 17 Jun 2024 08:26:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
使用 pdfplumber.open(pdf_path) 作为 pdf:
对于 pdf.pages 中的 page:
page_text = page.extract_text()
如果 page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

因此，当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复它？对此有什么建议吗？
提前致谢
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>点或叉（关于使用链式法则计算矩阵的梯度）</title>
      <link>https://stackoverflow.com/questions/78630853/dot-or-crossregarding-calculation-of-gradients-of-matrices-using-chain-rule</link>
      <description><![CDATA[在学习如何训练神经网络时遇到了一个问题。在此代码中，令人困惑的是如何理解何时使用“*”以及何时“使用np.dot()”进行乘法
def loss_gradients(forward_info: Dict[str, ndarray], 
weights: Dict[str, ndarray]) -&gt; Dict[str, ndarray]:
&#39;&#39;&#39;
计算损失对神经网络中每个参数的偏导数。
&#39;&#39;&#39; 
dLdP = -(forward_info[&#39;y&#39;] - forward_info[&#39;P&#39;])

dPdM2 = np.ones_like(forward_info[&#39;M2&#39;])

dLdM2 = dLdP * dPdM2

dPdB2 = np.ones_like(weights[&#39;B2&#39;])

dLdB2 = (dLdP * dPdB2).sum(axis=0)

dM2dW2 = np.transpose(forward_info[&#39;O1&#39;], (1, 0))

dLdW2 = np.dot(dM2dW2, dLdP)

dM2dO1 = np.transpose(weights[&#39;W2&#39;], (1, 0)) 

dLdO1 = np.dot(dLdM2, dM2dO1)

dO1dN1 = sigmoid(forward_info[&#39;N1&#39;]) * (1- sigmoid(forward_info[&#39;N1&#39;]))

dLdN1 = dLdO1 * dO1dN1

dN1dB1 = np.ones_like(weights[&#39;B1&#39;])

dN1dM1 = np.ones_like(forward_info[&#39;M1&#39;])

dLdB1 = (dLdN1 * dN1dB1).sum(axis=0)

dLdM1 = dLdN1 * dN1dM1

dM1dW1 = np.transpose(forward_info[&#39;X&#39;], (1, 0)) 

dLdW1 = np.dot(dM1dW1, dLdM1)

loss_gradients: Dict[str, ndarray] = {}
loss_gradients[&#39;W2&#39;] = dLdW2
loss_gradients[&#39;B2&#39;] = dLdB2.sum(axis=0)
loss_gradients[&#39;W1&#39;] = dLdW1
loss_gradients[&#39;B1&#39;] = dLdB1.sum(axis=0)

return loss_gradients

我们的想法是，知道何时使用哪个是至关重要的，因为输出不同]]></description>
      <guid>https://stackoverflow.com/questions/78630853/dot-or-crossregarding-calculation-of-gradients-of-matrices-using-chain-rule</guid>
      <pubDate>Mon, 17 Jun 2024 04:06:38 GMT</pubDate>
    </item>
    <item>
      <title>留一交叉验证来进行模型评估</title>
      <link>https://stackoverflow.com/questions/78630776/leave-one-out-cross-validation-for-model-evaluation</link>
      <description><![CDATA[# RF 
rf_optimal = RandomForestRegressor(**best_params, random_state=42)

# 留一交叉验证
loo = LeaveOneOut()
r2_train_scores = []
rmse_train_scores = []
y_test_true = []
y_test_pred = []

for train_index, test_index in loo.split(X_train):
X_train_fold, X_test_fold = X_train.values[train_index], X_train.values[test_index]
y_train_fold, y_test_fold = y_train.values[train_index], y_train.values[test_index]

rf_optimal1 = RandomForestRegressor(**best_params, random_state=42)
rf_optimal1.fit(X_train_fold, y_train_fold)
y_train_pred_fold = rf_optimal1.predict(X_train_fold)
y_test_pred_fold = rf_optimal1.predict(X_test_fold)

r2_train = r2_score(y_train_fold, y_train_pred_fold)
rmse_train = np.sqrt(mean_squared_error(y_train_fold, y_train_pred_fold))

r2_train_scores.append(r2_train)
rmse_train_scores.append(rmse_train)
y_test_true.append(y_test_fold[0])
y_test_pred.append(y_test_pred_fold[0])

r2_cal = np.mean(r2_train_scores)
rmse_cal = np.mean(rmse_train_scores)
r2_cv = r2_score(y_test_true,y_test_pred)
rmse_cv = np.sqrt(mean_squared_error(y_test_true,y_test_pred))

print(f&quot;Rc²: {r2_cal}, RMSEc: {rmse_cal}&quot;)
print(f&quot;Rcv²: {r2_cv}, RMSEcv: {rmse_cv}&quot;)

# 在整个训练数据集上进行训练，并在测试数据集上进行测试
rf_optimal.fit(X_train.values, y_train.values)
y_test_pred = rf_optimal.predict(X_test.values)

我正在尝试使用 LeaveOneOut() 评估 rf 模型。因此，我的令人困惑的是，我是否应该在新的 loo.split 中定义一个新的 RandomForestRegressor 并与整个训练数据集进行拟合，然后使用在 loo.split 之外定义的回归器预测测试数据集？我想要获得 Rc2、Rcv2 和 Rp2。]]></description>
      <guid>https://stackoverflow.com/questions/78630776/leave-one-out-cross-validation-for-model-evaluation</guid>
      <pubDate>Mon, 17 Jun 2024 03:11:20 GMT</pubDate>
    </item>
    <item>
      <title>我尝试手动应用梯度下降，但遇到了一个问题</title>
      <link>https://stackoverflow.com/questions/78630596/i-was-trying-to-apply-gradient-descent-manually-but-facing-a-problem</link>
      <description><![CDATA[import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
#创建随机 X 和 Y 值
np.random.seed(405)
X = 6*np.random.rand(100,1)-3
Y = 0.8*(X**2) + 0.9*X + 2 + np.random.randn(100,1)
Y = Y.reshape(-1,1)
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=2)
poly = PolynomialFeatures(degree=2,include_bias=False)
X_train_trans = poly.fit_transform(X_train)
X_test_trans = poly.transform(X_test)
model = LinearRegression()
model.fit(X_train_trans,y_train)
y_pred = model.predict(X_test_trans)
print(model.intercept_)
print(model.coef_)
n = len(X_train_trans)
β0 = 0
β1 = 1
β2 = 2
learning_rate = 0.0001 # 调整学习率
num_iterations = 50000 # 增加迭代次数
n = len(X_train_trans)
for iteration in range(num_iterations):
# 计算预测
y_prediction = β0 + β1 * X_train_trans[:, 0] + β2 * X_train_trans[:, 1]

# 计算损失（均方误差）
loss = (1/(2*n)) * np.sum((y_train - y_prediction)**2)

# 计算梯度
d_β0 = -(1 / n) * np.sum(y_train - y_prediction)
d_β1 = -(1 / n) * np.sum((y_train - y_prediction) * X_train_trans[:, 0])
d_β2 = -(1 / n) * np.sum((y_train - y_prediction) * X_train_trans[:, 1])

# 更新参数
β0 -= learning_rate * d_β0
β1 -= learning_rate * d_β1
β2 -= learning_rate * d_β2

# 定期打印损失和参数
if iteration % 1000 == 0:
print(f&quot;Iteration {iteration}: Loss = {loss}&quot;)

print(f&quot;最终参数（梯度下降）：β0 = {β0}, β1 = {β1}, β2 = {β2}&quot;)

问题是，每次我从线性回归模型和手动应用的模型中获得不同的系数值时，我都无法理解原因。我也尝试过改变学习率和迭代次数，但我无法获得相同的值。你能帮我吗？
我有一个关于多项式回归梯度下降的问题。如果可能的话，我想要答案。]]></description>
      <guid>https://stackoverflow.com/questions/78630596/i-was-trying-to-apply-gradient-descent-manually-but-facing-a-problem</guid>
      <pubDate>Mon, 17 Jun 2024 01:09:36 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 CNN 回归问题中每个预测值的置信度分数？</title>
      <link>https://stackoverflow.com/questions/78630404/how-to-calculate-confidence-score-for-each-prediction-value-in-cnn-regression-pr</link>
      <description><![CDATA[在回归中，假设我正在使用 CNN 进行预测，如果我必须回答每个预测的置信度值，那么在回归问题中，应该如何计算每个预测值的置信度？与分类一样，您可以抛出概率值，但对于回归，应该如何做到这一点？任何帮助都值得感激]]></description>
      <guid>https://stackoverflow.com/questions/78630404/how-to-calculate-confidence-score-for-each-prediction-value-in-cnn-regression-pr</guid>
      <pubDate>Sun, 16 Jun 2024 22:34:16 GMT</pubDate>
    </item>
    <item>
      <title>不切实际的完美测试成绩：诊断和解决 100% 准确率问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/78629141/unrealistically-perfect-testing-scores-diagnosing-and-addressing-100-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78629141/unrealistically-perfect-testing-scores-diagnosing-and-addressing-100-accuracy</guid>
      <pubDate>Sun, 16 Jun 2024 12:39:05 GMT</pubDate>
    </item>
    <item>
      <title>在 AWS Sagemaker 中训练线性模型时出现 UnexpectedStatusException？</title>
      <link>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</guid>
      <pubDate>Sun, 16 Jun 2024 05:27:48 GMT</pubDate>
    </item>
    <item>
      <title>如何为输入形状为 (n, 3) 和 (n, 2) 的数据集实现回归模型？</title>
      <link>https://stackoverflow.com/questions/78625334/how-can-i-implement-a-regression-model-for-a-dataset-with-inputs-in-the-shape-n</link>
      <description><![CDATA[我正在尝试为这种形式的数据集实现回归模型：

我遇到问题的部分是：对于数据集中的每个条目，输入 1 的形状为 (n, 3)，输入 2 的形状为 (n, 2) - 我不知道如何将模型塑造成可以使用回归模型的方式，因为我试图使每个条目的每个输入大小相同。我对这些数据完全没有背景信息，只是给出了数据并需要构建回归模型。
我尝试过以这种方式对所有输入求平均值：
取输入 1 并按索引沿每个向量求平均值，如下所示，[[2, 3, 4], [1, 2, 3]] -&gt; [1.5, 2.5, 3.5]
取输入 2 并按索引沿每个向量求平均值，如下所示，[[2, 45], [1, 45]] -&gt; [1.5, 45]
然后合并以获得长度为 5 的数组作为数据集中每个条目的输入，因此 [[2, 3, 4], [1, 2, 3]] 作为输入 1 和 [[2, 45], [1, 45]] 作为输入 2 将转换为 [1.5, 2.5, 3.5, 1.5, 45]。
然后我缩放了数据并使用了 Scikit 中的 MLPRegressor，但数字相差甚远。我尝试查看相关矩阵，如上所示，在平均输入得到的 5 个维度中都没有线性相关性。
我现在认为像我所做的那样平均输入并不是正确的处理方式。如何处理这样的数据集？]]></description>
      <guid>https://stackoverflow.com/questions/78625334/how-can-i-implement-a-regression-model-for-a-dataset-with-inputs-in-the-shape-n</guid>
      <pubDate>Sat, 15 Jun 2024 00:07:48 GMT</pubDate>
    </item>
    <item>
      <title>搜索具有相似文本的文档</title>
      <link>https://stackoverflow.com/questions/78599128/search-for-documents-with-similar-texts</link>
      <description><![CDATA[我有一个包含三个属性的文档：标签、位置和文本。
目前，我正在使用 LangChain/pgvector/embeddings 对它们全部进行索引。
我得到了满意的结果，但我想知道是否有更好的方法，因为我想查找一个或多个具有特定标签和位置的文档，但文本可能会有很大差异，但含义仍然相同。出于这个原因，我考虑使用嵌入/向量数据库。
这是否也是使用 RAG（检索增强生成）来“教”的一个例子LLM 不知道的一些常见缩写？
import pandas as pd

from langchain_core.documents import Document
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import PGVector
from langchain_openai.embeddings import OpenAIEmbeddings

connection = &quot;postgresql+psycopg://langchain:langchain@localhost:5432/langchain&quot;
embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)
collection_name = &quot;notas_v0&quot;

vectorstore = PGVector(
embeddings=embeddings,
collection_name=collection_name,
connection=connection,
use_jsonb=True,
)

# 开始索引

# df = pd.read_csv(&quot;notes.csv&quot;)
# df = df.dropna() # .head(10000)
# df[&quot;tags&quot;] = df[&quot;tags&quot;].apply(
# lambda x: [tag.strip() for tag in x.split(&quot;,&quot;) if tag.strip()]
# )

# long_texts = df[&quot;Texto Longo&quot;].tolist()
# wc = df[&quot;Centro Trabalho Responsável&quot;].tolist()
# notes = df[&quot;Nota&quot;].tolist()
# tags = df[&quot;tags&quot;].tolist()

# documents = list(
# map(
# lambda x: Document(
# page_content=x[0], metadata={&quot;wc&quot;: x[1], &quot;note&quot;: x[2], &quot;tags&quot;: x[3]}
# ),
# zip(long_texts, wc, notes, tags),
# )
# )

# print(
# [
# vectorstore.add_documents(documents=documents[i : i + 100])
# for i in range(0, len(documents), 100)
# ]
# )
# print(&quot;Done.&quot;)

### END INDEX

### BEGIN QUERY

result = vectorstore.similarity_search_with_relevance_scores(
&quot;EVTD202301222707&quot;,
filter={&quot;note&quot;: {&quot;$in&quot;: [&quot;15310116&quot;]}, &quot;tags&quot;: {&quot;$in&quot;: [&quot;abcd&quot;, &quot;xyz&quot;]}},
k=10, # 结果限制
)

### END QUERY
]]></description>
      <guid>https://stackoverflow.com/questions/78599128/search-for-documents-with-similar-texts</guid>
      <pubDate>Sun, 09 Jun 2024 16:40:32 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
解决方案：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
MSE = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); % Luko 等人的 Eq. 9。
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号
MSE(i) = (1/T)*sum((y_i&#39;-y_i_target).^2); % 均方误差
end
[minval, minidx] = min(MSE);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。
正如 BillBokeey 所强调的那样，所需的方程只是 Luko 等人提出的方程 9 的迭代版本。要进行训练，必须将方程 9 应用于训练数据集中的每个目标信号，并选择最小化均方误差 (MSE) 的结果 W_out。
最终更新：
我仍在寻找我的第二个解决方案的验证。我特别担心的是，我正在挑选出最小化 MSE 的最佳 Wout_i N x 1 向量，并有效地忽略所有其他 Wout_i 向量。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    </channel>
</rss>