<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 29 Dec 2023 09:14:08 GMT</lastBuildDate>
    <item>
      <title>请帮我修复[关闭]</title>
      <link>https://stackoverflow.com/questions/77730925/please-help-me-to-fix</link>
      <description><![CDATA[我正在尝试使用贝叶斯网络进行一些简单的概率计算。当我尝试计算“clusterIndex”的值时对于属于我的数据集的示例，我没有问题，否则它会给我以下错误。我不明白这意味着什么以及如何解决它。
这是我的代码：
def bNetCreation(dataSet):
    # Crea archi in modo tale che ogni feature dipenda da clusterIndex
    边缘=[]
    对于 dataSet.columns 中的列：
        如果列！= &#39;clusterIndex&#39;：
            Edges.append((&#39;clusterIndex&#39;, 列))
    Edges.append((&#39;节奏&#39;,&#39;舞蹈性&#39;))
    Edges.append((&#39;能量&#39;,&#39;舞蹈能力&#39;))
    Edges.append((&#39;响度&#39;,&#39;能量&#39;))
    Edges.append((&#39;节奏&#39;,&#39;能量&#39;))
    Edges.append((&#39;响度&#39;,&#39;言语性&#39;))
    Edges.append((&#39;活性&#39;,&#39;声学性&#39;))
    Edges.append((&#39;言语性&#39;,&#39;活跃度&#39;))
    Edges.append((&#39;响度&#39;,&#39;活跃度&#39;))
    Edges.append((&#39;danceability&#39;,&#39;valence&#39;))

    模型=贝叶斯网络（边缘）
    model.fit（数据集，估计器=MaximumLikelihoodEstimator，n_jobs=-1）
    以 open(&#39;modello.pkl&#39;, &#39;wb&#39;) 作为输出：
        pickle.dump（模型，输出）
    可视化贝叶斯网络（模型）
    返回模型
def prediciCluster(bayesianNetwork: BayesianNetwork, 示例, DifferentialColumn):
    推理=变量消除（贝叶斯网络）
    结果= inference.query(变量=[differentialColumn],evidence=example)
    打印（结果）

这是来自 inference.query 的错误：
返回 self.name_to_no[var][state_name]
       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^KeyError：0.6167826357877246
]]></description>
      <guid>https://stackoverflow.com/questions/77730925/please-help-me-to-fix</guid>
      <pubDate>Fri, 29 Dec 2023 08:21:41 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：使用 URL 'http://127.0.0.1:8237' 初始化剩余存储时出错：模块 'jwt' 没有属性 'encode'</title>
      <link>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</link>
      <description><![CDATA[第一个命令“python run_deployment.py --config deploy”运行成功并建议我运行下一个命令 - zenml up
“zenml up”生成以下错误。图片 1图片 2图片 3图片 4图片 5图片 6图片 7
我正在 YouTube 上关注 Ayush 的 MLOPs 课程。感谢您提前提供的帮助。
我尝试过 1. pip install jwt
2.pip安装PyJWT
3. pip卸载jwt
4. pip安装jwt==1.3.0
5. pip install --upgrade --force-reinstall PyJWT
6. pip install --upgrade --force-reinstall jwt]]></description>
      <guid>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</guid>
      <pubDate>Fri, 29 Dec 2023 07:34:39 GMT</pubDate>
    </item>
    <item>
      <title>即使训练和测试数据集具有良好的准确性，模型也无法正确分类[关闭]</title>
      <link>https://stackoverflow.com/questions/77730582/model-not-classifying-correctly-even-with-good-accuracy-on-training-and-test-dat</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77730582/model-not-classifying-correctly-even-with-good-accuracy-on-training-and-test-dat</guid>
      <pubDate>Fri, 29 Dec 2023 06:36:45 GMT</pubDate>
    </item>
    <item>
      <title>当所有列都是 float64 和 int64 时，为什么 dtype: object</title>
      <link>https://stackoverflow.com/questions/77730058/why-is-the-dtype-object-when-all-columns-are-float64-and-int64</link>
      <description><![CDATA[打印（cleaned_train.dtypes）
打印(“--”)
打印（cleaned_test.dtypes）
观察年份 int64
Insured_Period float64
住宅 int64
Building_Painted float64
Building_Fenced float64
建筑类型 float64
声明 float64
建筑尺寸 float64
地理代码 float64
数据类型：对象
--
观察年份 int64
Insured_Period float64
住宅 int64
Building_Painted float64
Building_Fenced float64
建筑类型 float64
声明 float64
建筑尺寸 float64
地理代码 float64
数据类型：对象
尝试获取 dtype:numeric 但得到了对象]]></description>
      <guid>https://stackoverflow.com/questions/77730058/why-is-the-dtype-object-when-all-columns-are-float64-and-int64</guid>
      <pubDate>Fri, 29 Dec 2023 03:03:31 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。
]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>将回归问题中的预测值转换为实际值[重复]</title>
      <link>https://stackoverflow.com/questions/77726639/covert-the-predicted-values-into-the-actual-ones-in-regression-problem</link>
      <description><![CDATA[我有一个回归代码，它获取数据并使用 one-hot 编码器将其转换为数值，.. 用于数值和分类值并预测温度（目标类别）。
我想显示代码的实际预测值而不是编码的值。
这是代码：
# 特征工程步骤（缩放和编码）
numeric_cols = data.select_dtypes(include=[&#39;number&#39;]).columns
categorical_cols = data.select_dtypes(exclude=[&#39;number&#39;]).columns

缩放器 = MinMaxScaler()
数据[数值列] = scaler.fit_transform(数据[数值列])

数据 = pd.get_dummies(数据, columns=categorical_cols, drop_first=True)

# 反转温度缩放以获得原始值
温度缩放器 = MinMaxScaler()
temp = data[&#39;家里设置的温度&#39;].values.reshape(-1, 1)
data[&#39;家里设置的温度&#39;] = temp_scaler.fit_transform(温度)

# 将数据拆分为特征 (X) 和目标变量 (y)
X = data.drop(columns=[&#39;家里设置的温度&#39;])
y = data[&#39;家中设置的温度&#39;]

# 将数据拆分为训练集（X_train）和测试集（X_test，y_train，y_test）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

如何显示预测的实际值而不是编码的预测值？]]></description>
      <guid>https://stackoverflow.com/questions/77726639/covert-the-predicted-values-into-the-actual-ones-in-regression-problem</guid>
      <pubDate>Thu, 28 Dec 2023 11:38:30 GMT</pubDate>
    </item>
    <item>
      <title>脑肿瘤生存模型的预测结果是二元的——如何解释？</title>
      <link>https://stackoverflow.com/questions/77725853/predicted-results-from-brain-tumor-survival-model-are-binary-how-to-interpret</link>
      <description><![CDATA[我正在使用 BraTS20 数据集进行“脑肿瘤总体生存预测”。
模型的输出是一个二进制数组，但是如何预测患者的生存时间？
为什么预测结果是二进制的？
如何在脑肿瘤生存时间的背景下解释这些二元结果？
我需要对模型或后处理步骤进行任何具体调整吗？
我想在向模型提供 MRI 输入时预测生存时间，我必须为此设计一个 Web 应用程序，用户在其中提供 MRI 以获得总体生存时间。
这里是数据转换成数组的地方
def getListAgeDays(id_list): # 仅创建年龄：类别数据

    x_val = [] # 初始化一个空列表来存储特征数据
    y_val = [] # 初始化一个空列表来存储类别标签
    对于 id_list 中的 i：
        if (i not in age_dict): # 检查 &#39;i&#39; 是否存在于 &#39;age_dict&#39; 字典中
            继续
        mask = getMaskSizesForVolume(nib.load(TRAIN_DATASET_PATH + f&#39;\\BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_seg.nii&#39;).get_fdata())
        # 加载分割掩码并提取其大小信息
        Brain_vol = getBrainSizeForVolume(nib.load(TRAIN_DATASET_PATH + f&#39;\\BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_t1.nii&#39;).get_fdata())
        # 加载大脑体积图像并提取其大小信息
        mask[1] = mask[1]/brain_vol # 标准化 mask 的大小
        掩码[2] = 掩码[2]/brain_vol
        掩码[3] = 掩码[3]/brain_vol
        合并 = [age_dict[i]、掩码[1]、掩码[2]、掩码[3]]
        # 通过将“age_dict[i]”与掩码大小值相结合来创建特征向量
        radiomics_values = df_radiomics.loc[df_radiomics[&#39;目标&#39;] == str(i)]
        # 查询 DataFrame &#39;df_radiomics&#39; 中与 &#39;i&#39; 相关的放射组学值
        如果 radiomics_values.empty：
            continue # 如果没有放射组学值则跳过本次迭代
        merged.extend(radiomics_values.values.tolist()[0][:-1])
        # 将放射组学值添加到“合并”特征向量（不包括最后一个值）
        x_val.append(merged) # 将特征向量附加到 &#39;x_val&#39;
        如果（days_dict[i] &lt; 250）：
            y_val.append([1, 0, 0]) # 根据 &#39;days_dict&#39; 条件附加类别标签
        elif (days_dict[i] &gt;= 250 且 days_dict[i] &lt; 450):
            y_val.append([0, 1, 0])
        别的：
            y_val.append([0, 0, 1])
    return np.array(x_val), np.array(y_val) # 以 NumPy 数组形式返回特征数据和类别标签

X_all, y_all = getListAgeDays(brats_ids) # 使用 &#39;train_and_test_ids&#39; 调用该函数
print(f&#39;X_test: {X_all.shape}&#39;) # 打印特征数据的形状

这是进入模型的X_test：
数组([[0.424, 0.385, 0.725, 0.262, 0.304, 0.352, 0.555, 0.368, 0.276,
        0.531、0.054、0.286、0.337、0.447、0.464、0.334、0.235、0.607、
        0.789, 0.569, 0.849],
       [0.691, 0.066, 0.026, 0.165, 0.019, 0.063, 0.209, 0.81 , 0. ,
        0.419、0.、1.、0.383、0.335、0.36、0.392、0.335、0.429、
        0.381、0.255、0.257]……

将模型保存到Joblib中
joblib.dump(模型, &#39;model_joblib&#39;)
mj = joblib.load(&#39;model_joblib&#39;)

这是预测
mj.predict(X_test)

输出为：
数组([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 , 0, 0, 0])
]]></description>
      <guid>https://stackoverflow.com/questions/77725853/predicted-results-from-brain-tumor-survival-model-are-binary-how-to-interpret</guid>
      <pubDate>Thu, 28 Dec 2023 08:51:33 GMT</pubDate>
    </item>
    <item>
      <title>在训练神经网络模型时，如何在 matlab 中编写 Garson 算法来查找参数的相对重要性？</title>
      <link>https://stackoverflow.com/questions/77725247/how-do-you-code-garsons-algorithm-in-matlab-to-find-the-relative-importance-of</link>
      <description><![CDATA[我正在进行一项预测分析研究，并遇到了加森算法，但我在为其编写公式时遇到了麻烦。这是我的代码：
%%% 数据输入
输入=数据(:, 1:4)&#39;;
目标=数据(:, 5)&#39;;

%%% 创建前馈神经网络
净 = 前馈网络([10, 15, 20]);

%%%% 设置每个隐藏层的神经元数量
net.layers{1}.size = 10;
net.layers{2}.size = 15;
net.layers{3}.size = 20;

%%%% 更改激活函数（例如，将输出层的“tansig”更改为“purelin”）
net.layers{1}.transferFcn = &#39;tansig&#39;; % 第一个隐藏层的激活函数
net.layers{2}.transferFcn = &#39;tansig&#39;; % 第二隐藏层的激活函数
net.layers{3}.transferFcn = &#39;tansig&#39;; % 第三隐藏层的激活函数
net.layers{4}.transferFcn = &#39;purelin&#39;; % 输出层的激活函数

%%%% 划分数据进行训练、验证和测试
net.divideParam.trainRatio = 0.7;
net.divideParam.valRatio = 0.15;
net.divideParam.testRatio = 0.15;

%%%% 训练神经网络
[net, tr] = train(net, 输入, 目标);

%%%%计算Garson的重要性
重要性= garsonsAlgorithm(net);

%%%% 显示特征重要性
disp(&#39;特征重要性（Garson 算法）:&#39;);
对于 i = 1：长度（重要性）
    disp([&#39;x 的重要性&#39; num2str(i) &#39;: &#39; num2str(importance(i))]);
结尾

这是我写的garson算法函数：
函数重要性 = garsonsAlgorithm(net)
    %%% 从神经网络中提取连接权重
    权重 = cell2mat(net.IW);
    
    % 检查权重是否存在偏差
    如果 isfield(net, &#39;b&#39;)
        bias_weights = cell2mat(net.b);
        权重 = [权重;偏差权重]；
    结尾
    
    %%%计算权重的绝对值
    绝对权重=abs(权重);
    
    %%%计算每个输入特征的重要性
    重要性 = sum(absolute_weights, 1);
    
    %%% 标准化重要性值
    重要性=重要性/总和（重要性）；
结尾

这是否正确？另外，当改变加森算法中的输入数量时，输出会出现任何变化（相对重要性）吗？
我附上了Garson算法的公式供参考：
Garson 算法公式]]></description>
      <guid>https://stackoverflow.com/questions/77725247/how-do-you-code-garsons-algorithm-in-matlab-to-find-the-relative-importance-of</guid>
      <pubDate>Thu, 28 Dec 2023 06:04:03 GMT</pubDate>
    </item>
    <item>
      <title>我来解决以下错误，尝试微调 NER 的 roberta 模型</title>
      <link>https://stackoverflow.com/questions/77725013/me-to-resolve-below-error-trying-to-fine-tune-roberta-model-for-ner</link>
      <description><![CDATA[帮助我解决以下错误，尝试对 NER 的 roberta 模型进行精细化处理。我使用基本模型并尝试微调我的数据。
进口火炬
导入 json
从 torch.utils.data 导入数据集
从转换器导入 RobertaForTokenClassification、RobertaTokenizer
从 Transformers 导入 Trainer、TrainingArguments
从 torch.nn 导入 CrossEntropyLoss
 model_name = “roberta-base”
            tokenizer = RobertaTokenizer.from_pretrained(model_name)
            模型 = RobertaForTokenClassification.from_pretrained(model_name)

            NERDataset 类（数据集）：
                def __init__(自身、文本、实体、分词器):
                    self.texts = 文本
                    self.entities = 实体
                    self.tokenizer = 分词器

                def __len__(自身):
                    返回 len(self.texts)

                def __getitem__(self, idx):
                    文本 = self.texts[idx]
                    实体 = self.entities[idx]

                    # 直接从实体中提取标签
                    #labels = [标签[“标签”] 实体中的实体 实体中的标签]
                    labels = [label[“label”] 对于实体​​中的实体 对于实体​​中的标签，如果 isinstance(label, dict) 和“label”是在标签中]

                    

                    # 对输入文本进行标记
                    编码 = self.tokenizer(
                        文本，
                        return_tensors =“pt”，
                        截断=真，
                        填充=“最大长度”，
                        最大长度=512，
                        标签=标签
                    ）

                    返回 {
                        “input_ids”：编码[“input_ids”].squeeze()，
                        “attention_mask”：编码[“attention_mask”].squeeze()，
                        “标签”：编码[“标签”].squeeze() if labels else None
                    }

            # 从 JSONL 文件加载数据
            def load_data_from_jsonl(file_path):
                数据 = []
                以 open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) 作为文件：
                    对于文件中的行：
                        尝试：
                            示例 = json.loads(行)
                            数据.追加（示例）
                        除了 json.JSONDecodeError 为 e：
                            print(f&quot;在线解码 JSON 时出错：{line}&quot;)
                            打印(e)
                返回数据

            # 用法示例：
            jsonl_file_path = “./admin.jsonl”
            数据 = load_data_from_jsonl(jsonl_file_path)

            # 从数据中提取文本和实体
            texts = [“John 是个好孩子”,“Goole 是软件”]
            实体=[
                [
                    {“标签”：“人”，“姓名”：“约翰”}，
                    {“标签”：“位置”，“城市”：“纽约”}
                ],
                [
                    {“标签”:“组织”,“名称”:“Google”},
                    {“标签”：“日期”，“年份”：“2023”}
                ]
            ]

            # 创建自定义数据集的实例
            数据集= NERDataset（文本=文本，实体=实体，标记器=标记器）

            # 使用训练器中的数据集
            训练参数 = 训练参数（
                输出目录=“./输出”,
                num_train_epochs=3,
                per_device_train_batch_size=16，
                evaluation_strategy=“纪元”，
            ）

            教练=教练（
                型号=型号，
                参数=训练参数，
                train_dataset=数据集，
                # ...其他培训师参数...
            ）

            训练师.train()
]]></description>
      <guid>https://stackoverflow.com/questions/77725013/me-to-resolve-below-error-trying-to-fine-tune-roberta-model-for-ner</guid>
      <pubDate>Thu, 28 Dec 2023 04:28:04 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它的准确度应该达到 30 左右%。
但是，它根本没有学到任何东西：准确度与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，效果会好一些，但使用高斯应该可以达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>llama.cpp 抱歉，您的 MOSTLY_Q4_1_SOME_F16 类型的 GGJTv1 文件不符合转换条件</title>
      <link>https://stackoverflow.com/questions/77337548/llama-cpp-sorry-your-ggjtv1-file-of-type-mostly-q4-1-some-f16-is-not-eligible-f</link>
      <description><![CDATA[这是我用来转换的存储库
https://github.com/ggerganov/llama.cpp
python3.10 ~/llama.cpp/convert-llama-ggml-to-gguf.py --input ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g /ggml-model-q4_1.bin --输出 ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.gguf

ValueError：GGJTv2 中的量化已更改。只能转换早于 GGJTv2 的未量化 GGML 文件。抱歉，您的 MOSTLY_Q4_1_SOME_F16 类型的 GGJTv1 文件不符合转换条件。
如您所见，我无法将 bin 文件转换为 gguf 文件。
我从这里得到这个文件
https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml -q4_1-from-gptq-4bit-128g
它给了我
ggml-model-q4_1.bin
这个问题源于我只是尝试直接从 llama.cpp/main.py 运行模型，这就是发生的情况
&lt;预&gt;&lt;代码&gt;./main -m ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin -t 4 -c 2048 -n 2048 - -颜色-i--指示

失败了
&lt;块引用&gt;
日志启动 main: build = 1407 (465219b) main: 使用 cc 构建 (Ubuntu
9.4.0-1ubuntu1~20.04.2) 9.4.0 对于 x86_64-linux-gnu main：seed = 1697867527 gguf_init_from_file：无效的魔术字符 tjgg。错误
加载模型：llama_model_loader：无法加载模型
模型/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin
llama_load_model_from_file：加载模型失败
llama_init_from_gpt_params：错误：无法加载模型
&#39;models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin&#39; 主要：
错误：无法加载模型

如有任何帮助，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77337548/llama-cpp-sorry-your-ggjtv1-file-of-type-mostly-q4-1-some-f16-is-not-eligible-f</guid>
      <pubDate>Sat, 21 Oct 2023 19:48:00 GMT</pubDate>
    </item>
    <item>
      <title>我需要清楚地预测 X 测试、X 训练、y 测试、y 训练</title>
      <link>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</link>
      <description><![CDATA[在线性回归模型中，假设我们有 3 个自变量（年龄、身高、性别）和 1 个因变量（糖尿病），然后我们将模型拆分为 X 训练，即（例如 70%）自变量数据对于训练，X测试-&gt;即30%的自变量数据用于测试
y 火车-&gt;即（例如70%）用于训练的因变量数据，y检验-&gt;即30%的因变量数据用于测试
因此，当我们预测 X 检验或预测 X 检验时，我们是在预测自变量的值还是在预测因变量（糖尿病？）]]></description>
      <guid>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</guid>
      <pubDate>Sat, 15 Jan 2022 12:37:46 GMT</pubDate>
    </item>
    <item>
      <title>鉴于您有多个虚拟列，如何预测值？</title>
      <link>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</link>
      <description><![CDATA[我有一个类似于以下内容的数据框：
 薪资 职务 Raiting Company_Name 地点 资历
0 100 SE 5 苹果 SF 副总裁
1 120 DS 4 三星 la Jr
2 230 QA 5 谷歌 sd Sr


（我的 df 具有比这更多的分类特征）
通常，当从模型进行预测时，它会像这样
in[1]: inModel_name.predict(catagory_1, catagory_2,..etc)
输出[2]：预测变量

然而，在使用pd.get_dummies之后，根据您创建的分类特征的数量，您会获得更多的列，这使得我之前提到的方法在尝试预测数据时变得不切实际。如何引用多列而不是手动输入 0。]]></description>
      <guid>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</guid>
      <pubDate>Sun, 16 Aug 2020 03:43:39 GMT</pubDate>
    </item>
    <item>
      <title>SVM 二元分类器为所有测试数据预测一类</title>
      <link>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</link>
      <description><![CDATA[我有一个包含 10 个特征的分类问题，我必须预测 1 或 0。当我训练 SVC 模型时，通过训练测试分割，数据测试部分的所有预测值均为 0。数据具有以下 0-1 计数：

0：1875
1：1463

训练模型的代码如下：
从 sklearn.svm 导入 SVC
模型 = SVC()
model.fit(X_train, y_train)
pred= model.predict(X_test)
从 sklearn.metrics 导入 precision_score
准确度分数（y_测试，预测）

为什么它在所有情况下都预测 0？]]></description>
      <guid>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</guid>
      <pubDate>Wed, 18 Sep 2019 11:10:06 GMT</pubDate>
    </item>
    <item>
      <title>如何解释回归中的MSE？</title>
      <link>https://stackoverflow.com/questions/48973140/how-to-interpret-mse-in-regression</link>
      <description><![CDATA[我正在尝试建立一个模型来预测房价。
我有一些功能 X（浴室数量等）和目标 Y（大约 300,000 美元到 800,000 美元）
在将 Y 拟合到模型之前，我使用 sklearn 的标准缩放器对 Y 进行标准化。
这是我的 Keras 模型：
def build_model():
    模型=顺序（）
    model.add（密集（36，input_dim = 36，激活=&#39;relu&#39;））
    model.add（密集（18，input_dim = 36，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;sigmoid&#39;））
    model.compile（损失=&#39;mse&#39;，优化器=&#39;sgd&#39;，指标=[&#39;mae&#39;，&#39;mse&#39;]）
    返回模型

我在尝试解释结果时遇到困难 - 0.617454319755 的 MSE 意味着什么？
我是否必须对这个数字进行逆变换，并对结果求平方根，得到 741.55 美元的错误率？
math.sqrt(sc.inverse_transform([mse]))

我很抱歉在刚开始时听起来很愚蠢！]]></description>
      <guid>https://stackoverflow.com/questions/48973140/how-to-interpret-mse-in-regression</guid>
      <pubDate>Sun, 25 Feb 2018 11:57:03 GMT</pubDate>
    </item>
    </channel>
</rss>