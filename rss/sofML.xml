<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 26 Jul 2024 01:06:03 GMT</lastBuildDate>
    <item>
      <title>优化不平衡分类问题中两个特定类别的准确率和召回率</title>
      <link>https://stackoverflow.com/questions/78795688/optimize-precision-and-recall-for-two-specific-classes-in-an-imbalanced-classifi</link>
      <description><![CDATA[我有三个类别 {-1, 0, 1}。相应类别的数据平均比例为 1:20:1。我想要实现
类别 -1 和 1 的高精度 (&gt;70%) 和平均召回率 (30%-40%)。
类别 0 的召回率高 (&gt;90%)。类别 0 的精度无关紧要。

对 -1 和 1 的错误分类代价高昂。
我目前有两个模型。我们将第一个模型称为 m1（集成，在所有三个类别上进行训练），将第二个模型称为 m2。m1 使用 EasyEnsembleClassifier（来自 imblearn，使用 XGBoost 作为基础模型）进行拟合，以对所有三个类别进行分类。如果 m1 的预测为 -1 或 1，则将数据输入到 m2（使用 XGBoost 在 -1 和 1 上训练为二元分类器），然后将 m2 的预测用作最终预测。如果 m1 的预测为 0，则 0 为最终预测。
m2 也已校准。因此，如果 m1 中有许多 0 被错误分类为 1 和 -1，我可以使用 m2 设置概率阈值以过滤掉错误的预测。
从混淆矩阵和下面的分类报告来看，我的方法似乎根本不起作用。我已经尝试过采样（SMOTE、ADASYN）、欠采样和设置类别权重（scale_pos_weight）。我可以做些什么来提高模型的性能并提高 -1 和 1 的预测质量。欢迎提出任何建议。
混淆矩阵
分类报告]]></description>
      <guid>https://stackoverflow.com/questions/78795688/optimize-precision-and-recall-for-two-specific-classes-in-an-imbalanced-classifi</guid>
      <pubDate>Thu, 25 Jul 2024 22:58:52 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是一个初学者，我还在学习稀疏矩阵以及它们如何与其他东西一起工作。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数 sparse_output=True 对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用 transformed_X 和目标 y 拟合 RandomForestClassifier 时，它显示了此错误。
#seed
np.random.seed(42)

#one hot encoding imports
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#Data splitting
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#select columns
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器 
enc = OneHotEncoder()

#使用 ct 拟合编码器
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;压缩稀疏行格式存储了 10360 个元素&gt;
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

#model score
blsc = model.score(X_test,Y_test)
print(f&#39;Baseline Model Score is : {blsc}&#39;)

-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[416], line 13
11 #建模
12 模型 = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。
373 # 仅需标准即可确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但出现了样本数量不一致的错误。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[444]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError：发现输入变量的样本数量不一致：[1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>修剪错误无法将“torch.cuda.FloatTensor”分配给参数</title>
      <link>https://stackoverflow.com/questions/78795181/pruning-error-cannot-assign-torch-cuda-floattensor-to-parameter</link>
      <description><![CDATA[我很难弄清楚为什么在尝试将彩票假设应用于我的模型时会出现此错误。显然，这是在修剪回调期间发生的，它似乎试图将之前的权重保存为参数，但保存的是 FloatTensor 的值。我不知道该怎么办。
 文件 &quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/pytorch_lightning/callbacks/pruning.py&quot;, 第 340 行, 在 apply_pruning 中 
self._apply_local_pruning(amount)
文件 &quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/pytorch_lightning/callbacks/pruning.py&quot;, 第 311 行, 在 _apply_local_pruning 中 
self.pruning_fn(module, name=name, amount=amount)
文件&quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/torch/nn/utils/prune.py&quot;，第 909 行，在 l1_unstructured 中 
L1Unstructured.apply(
文件 &quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/torch/nn/utils/prune.py&quot;，第 545 行，在 apply 中 
return super().apply(
文件 &quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/torch/nn/utils/prune.py&quot;，第 163 行，在 apply 中 
module.register_parameter(name + &quot;_orig&quot;, orig)
文件 &quot;/uufs/chpc.utah.edu/common/home/u0977428/.micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;，第 583 行，在 register_parameter 中 
raise TypeError(f&quot;无法将 &#39;{torch.typename(param)}&#39; 对象分配给参数 &#39;{name}&#39; &quot;
TypeError: 无法将 &#39;torch.cuda.FloatTensor&#39; 对象分配给参数 &#39;weight_orig&#39;（需要 torch.nn.Parameter 或 None）

def main(config, ...):
model = UNetLightning(
...
)

trainer_args = {
&quot;max_epochs&quot;: config.epochs,
&quot;precision&quot;: config.precision,
&quot;accelerator&quot;: config.accelerator,
&quot;callbacks&quot;: [],
&quot;gradient_clip_val&quot;: 1.0,
&quot;accumulate_grad_batches&quot;: 3,
}
initial_state = model.state_dict() # 保存初始状态以进行修剪
pruning_passes = 3 # 要进行的修剪次数

for i in range(pruning_passes):
print(f&quot;Pruning iteration {i + 1}/{pruning_passes}&quot;)

# 使用修剪回调初始化训练器
pruning_callback = ModelPruning(
&quot;l1_unstructured&quot;,
amount=0.2,
verbose=True,
use_global_unstructured=False
)

# 在将模型移至 GPU 之前应用修剪
model.cpu()
pruning_callback.on_fit_start(trainer=None, pl_module=model)

trainer_args[&quot;callbacks&quot;].append(pruning_callback)
trainer = Trainer(**trainer_args)

# 训练模型
trainer.fit(model, data_module)

# 删除下一次迭代的修剪回调
trainer_args[&quot;callbacks&quot;].remove(pruning_callback)

# 将模型重置为初始权重
model.load_state_dict(initial_state)

# 不进行修剪的最终训练传递以微调模型
trainer_args[&quot;callbacks&quot;].append(checkpoint_cb) # 微调后保存最佳模型
trainer = Trainer(**trainer_args)
trainer.fit(model, data_module)

# 保存最终模型
torch.save(model.state_dict(), &#39;final_pruned_model.pth&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78795181/pruning-error-cannot-assign-torch-cuda-floattensor-to-parameter</guid>
      <pubDate>Thu, 25 Jul 2024 19:42:41 GMT</pubDate>
    </item>
    <item>
      <title>我在训练随机森林回归器时不断遇到这个问题</title>
      <link>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</link>
      <description><![CDATA[/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning：X 有特征名称，但 RandomForestRegressor 在没有特征名称的情况下安装
warnings.warn(
我尝试添加 .values，但它仍然标记错误。]]></description>
      <guid>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</guid>
      <pubDate>Thu, 25 Jul 2024 19:18:50 GMT</pubDate>
    </item>
    <item>
      <title>XGboost 的特征重要性仅返回 1000 个特征中的 1 个特征</title>
      <link>https://stackoverflow.com/questions/78794708/feature-importance-with-xgboost-returns-only-1-feature-out-of-1000-feature</link>
      <description><![CDATA[我的 XGClassifier 或 XGboost 在打印 get_fscore() 时总是只返回 1000 个特征中的一个特征。
xgbc.get_booster().get_fscore()

output- {feature_m:98&gt;
xgbc= XGBClassifier() 
model = xgbc.fit( X_train_scaled, Y_train)

我尝试更新不同的参数，但结果总是相同的。任何线索都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78794708/feature-importance-with-xgboost-returns-only-1-feature-out-of-1000-feature</guid>
      <pubDate>Thu, 25 Jul 2024 17:27:27 GMT</pubDate>
    </item>
    <item>
      <title>需要具有 10k 行独特上下文的合成 PII 数据集</title>
      <link>https://stackoverflow.com/questions/78794440/need-synthetic-pii-dataset-with-unique-contexts-for-10k-lines</link>
      <description><![CDATA[我正在寻找一个包含 10,000 行数据的合成数据集，其中包含各种类型的个人身份信息 (PII)，用于分类问题。数据应按段落格式化，并且每个段落应具有唯一的上下文。
我需要涵盖不同类型的 PII 数据，例如
[&quot;地址&quot;,
&quot;银行 • 帐号&quot;
&quot;信用卡 • 卡号&quot;
&quot;电子邮件地址&quot;
&quot;政府 - 身份证号码&quot;
&quot;个人姓名&quot;
&quot;密码&quot;
&quot;电话号码&quot;
&quot;密钥•（又称私钥）&quot;
121]
&quot;用户 ID&quot;,
&quot;出生日期&quot;,&quot;性别&quot;]

此外，每个段落在上下文中都是不同的，这一点至关重要。我尝试过使用 Faker，但它依赖于占位符模板，例如：
templates = [
&quot;{intro} {name} 出生于 {dob}，住在 {address}。您可以通过电子邮件 {email} 或电话 {phone} 联系他们。{closing}&quot;,
&quot;{intro} {name} 的社会安全号码是 {ssn}，护照号码是 {passport}。他们的信用卡号是 {ccn}。{closing}&quot;,
&quot;{intro} {name} 在 {license_year} 年获得了驾照号码 {dl}。 {closing}&quot;,
&quot;{intro} {name} 的电子邮件地址是 {email}，家庭住址是 {address}。他们出生于 {dob}，电话号码是 {phone}。{closing}&quot;,
&quot;{intro} {name} 的全名是 {name}，出生于 {dob}。他们的联系信息包括电话号码 {phone} 和电子邮件 {email}。他们居住在 {address}。{closing}&quot;
]

问题是这些句子在模板中重复出现，导致上下文变化有限。
我也尝试过使用 Kaggel，但它的结果没有涵盖所有必需的 PII 数据类型。还检查了 Github 存储库，但没有找到任何可靠的解决方案。
我正在寻找一种生成完全随机且唯一段落的方法。有人可以建议一种方法或工具来创建这样的数据集，或者提供有关生成具有多样化和独特背景的合成 PII 数据的指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78794440/need-synthetic-pii-dataset-with-unique-contexts-for-10k-lines</guid>
      <pubDate>Thu, 25 Jul 2024 16:19:35 GMT</pubDate>
    </item>
    <item>
      <title>Flask 岭回归模型预测</title>
      <link>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</link>
      <description><![CDATA[当我运行 application.py 时，它将运行，当我将预测的数据点提供给模型，然后提交模型。然后将显示如下错误
内部服务器错误
服务器遇到内部错误，无法完成您的请求。
服务器超载或应用程序中出现错误。

我如何运行此应用程序并在给出值时给出预测点。我正在提供代码，请给我解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</guid>
      <pubDate>Thu, 25 Jul 2024 13:48:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 load_model 函数加载 HDF5 模型不起作用</title>
      <link>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</guid>
      <pubDate>Thu, 25 Jul 2024 12:55:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行单类物体检测获得假阳性</title>
      <link>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</link>
      <description><![CDATA[在这里，我尝试使用 cnn 构建一个 Manhole 物体检测，在这个模型中，经过训练我得到了 95% 的准确率。我得到的是假阳性，例如，如果我用人孔（训练对象）测试图像进​​行检测，它将绘制边界框，而我测试没有训练对象的随机图像，则会出现一个随机边界框，这就是问题所在，在实时网络摄像头测试中也是如此，但在这里，如果对象甚至没有被检测到，则会在框架中获取一些随机边界框。这里我提供我的代码，请帮助
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization``, Flatten, MaxPool2D, Dense
from pathlib import Path

train_img = Path(&quot;DATASET/train/Manhole&quot;)
val_img = Path(&quot;DATASET\valid\Manhole&quot;)

train_csv = pd.read_csv(&#39;DATASET/train/Manhole/_annotations.csv&#39;) 
val_csv = pd.read_csv(&#39;DATASET/valid/_annotations.csv&#39;)
#print(train_csv)
train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]].fillna(0)
train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]].astype(int)
train_csv.drop_duplicates(subset=&#39;filename&#39;,inplace=True, ignore_index=True)
val_csv.drop_duplicates(subset=&#39;filename&#39;, inplace=True, ignore_index=True)

def datagenerator(df ,batch_size ,path):
while True:
images = np.zeros((batch_size,640,640,3))
bounding_box_coords = np.zeros((batch_size, 4))

for i in range(batch_size):
rand_index = np.random.randint(0, train_csv.shape[0])
row = df.loc[rand_index, :]
images[i] = cv2.imread(str(path/row.filename)) / 255.
bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

产生 {&#39;filename&#39;: images}, {&#39;coords&#39;: bounding_box_coords}

# example, label = next(datagenerator(batch_size=16))
# img = example[&#39;filename&#39;][0]
# bbox_coords = label[&#39;coords&#39;][0] 

# x1, y1, x2, y2 = map(int, bbox_coords)
# print(&#39;bbox cords&#39;,x1,y1,x2,y2)
# cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
# cv2.putText(img, &#39;&#39;, (x1,y1-10),cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 2)
# # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# plt.imshow(img)
# plt.show()

input_ = 输入(shape=[640, 640, 3], name=&#39;filename&#39;)

x = input_
x = Conv2D(16, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(32, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(64, (3,3),激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(128，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(256，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(312，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(500，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(580，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(680，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Flatten()(x)
x = Dense(256，激活=&#39;relu&#39;)(x)
x = Dense(32, 激活=&#39;relu&#39;)(x)
输出坐标 = Dense(4, 名称=&#39;coords&#39;)(x)

模型 = tf.keras.models.Model(input_,output_coords)

模型摘要()

模型编译(loss={&#39;coords&#39;: &#39;mse&#39;},
优化器=tf.keras.optimizers.Adam(5e-5), 
指标={&#39;coords&#39;: &#39;accuracy&#39;})

检查点回调 = ModelCheckpoint(&#39;model_Checkpoint.h5&#39;, 监视器=&#39;val_loss&#39;, save_best_only=True, 模式=&#39;min&#39;)

模型拟合(数据生成器(df=train_csv,batch_size=6,path=train_img), 
epochs=80, steps_per_epoch=150,
validation_data=datagenerator(df=val_csv,batch_size=6,path=val_img), 
validation_steps=240, 
callbacks=[checkpoint_callback])

model.save(&#39;model2.h5&#39;)

我需要代码来在实时网络摄像头中正确检测训练过的对象，而不会出现任何边界框，并从 cnn 接收置信度值，这样我就可以设置检测的阈值]]></description>
      <guid>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</guid>
      <pubDate>Thu, 25 Jul 2024 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>物体检测和识别 Apple vision Pro</title>
      <link>https://stackoverflow.com/questions/78792448/object-detection-and-recognition-apple-vision-pro</link>
      <description><![CDATA[我的目标是将此功能添加到我的应用中，以便使用 Apple Vision Pro 区分男性和女性。但是，我在文档中找不到有关如何将模型添加到我的应用中的示例，基本上我想在启动应用时进行实时检测，然后在人脸上添加网格？但首先是否可以将其集成到 Apple Vision Pro 中]]></description>
      <guid>https://stackoverflow.com/questions/78792448/object-detection-and-recognition-apple-vision-pro</guid>
      <pubDate>Thu, 25 Jul 2024 09:13:43 GMT</pubDate>
    </item>
    <item>
      <title>hmmlearn 中的隐马尔可夫模型不收敛</title>
      <link>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</guid>
      <pubDate>Thu, 25 Jul 2024 00:56:12 GMT</pubDate>
    </item>
    <item>
      <title>如何根据物体之间的间距分割图像？</title>
      <link>https://stackoverflow.com/questions/78767970/how-to-segment-an-image-based-on-the-spacing-between-objects</link>
      <description><![CDATA[我正在做一个项目，对工程设计图进行分割，并将这些部分导出为 PNG 文件。
对于这幅图，我需要在三个对象周围创建三个边界框，并裁剪出这些部分。是否有任何算法或模型可以自动执行此过程？
]]></description>
      <guid>https://stackoverflow.com/questions/78767970/how-to-segment-an-image-based-on-the-spacing-between-objects</guid>
      <pubDate>Fri, 19 Jul 2024 07:04:09 GMT</pubDate>
    </item>
    <item>
      <title>karateclub MUSAE 嵌入产生奇怪的列数</title>
      <link>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</link>
      <description><![CDATA[我正在试验属性节点嵌入和结构嵌入，但 karateclub 实现返回的矩阵具有奇怪的列数。
MUSAE 给出 128 个“特征”，而不是请求的 32 个。当我请求 32 个时，GLEE 给出了 33 个。我遗漏了什么吗？
import random
import numpy as np
import networkx as nx
from scipy.sparse import coo_matrix

from karateclub.node_embedding.attributed import MUSAE
from karateclub.node_embedding.neighbourhood import GLEE

g = nx.newman_watts_strogatz_graph(50, 10, 0.2)

X = {i: random.sample(range(150),50) for i in range(50)}

row = np.array([k for k, v in X.items() for val in v])
col = np.array([val for k, v in X.items() for val in v])
data = np.ones(50*50)
shape = (50, 150)

X = coo_matrix((data, (row, col)), shape=shape)

model = MUSAE(dimensions=32)
model.fit(g, X)
emb = model.get_embedding()
print(emb.shape)

model = GLEE(dimensions=32)
model.fit(g)
emb = model.get_embedding()
print(emb.shape)

输出：
(50, 128)
(50, 33)
]]></description>
      <guid>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</guid>
      <pubDate>Fri, 14 Jun 2024 15:02:08 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 示例与 SequenceExample</title>
      <link>https://stackoverflow.com/questions/46857596/tensorflow-example-vs-sequenceexample</link>
      <description><![CDATA[TensorFlow 文档中没有提供太多信息：
https://www.tensorflow.org/api_docs/python/tf/train/Example
https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample
它只是向您推荐：
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto
每个都有简短的描述以及示例；但是，我仍然不太明白为什么你会选择其中一个而不是另一个？有人可以帮忙解释一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/46857596/tensorflow-example-vs-sequenceexample</guid>
      <pubDate>Fri, 20 Oct 2017 21:28:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 通过最近邻算法对数据进行分类？</title>
      <link>https://stackoverflow.com/questions/7326958/how-can-i-classify-data-with-the-nearest-neighbor-algorithm-using-python</link>
      <description><![CDATA[我需要使用（我希望）最近邻算法对一些数据进行分类。我在 Google 上搜索了这个问题，找到了很多库（包括 PyML、mlPy 和 Orange），但我不知道从哪里开始。
我应该如何使用 Python 实现 k-NN？]]></description>
      <guid>https://stackoverflow.com/questions/7326958/how-can-i-classify-data-with-the-nearest-neighbor-algorithm-using-python</guid>
      <pubDate>Tue, 06 Sep 2011 22:35:58 GMT</pubDate>
    </item>
    </channel>
</rss>