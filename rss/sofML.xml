<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ çš„æ´»è·ƒé—®é¢˜ - å †æ ˆå†…å­˜æº¢å‡º</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>æ¥è‡ª stackoverflow.com çš„æœ€æ–° 30 æ¡</description>
    <lastBuildDate>Wed, 25 Sep 2024 18:22:21 GMT</lastBuildDate>
    <item>
      <title>å°½ç®¡ np.where(np.isnan(X)) æ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ï¼Œsklearn ä»ä¼šå¼•å‘é”™è¯¯â€œè¾“å…¥ X åŒ…å« NaNâ€</title>
      <link>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä¸ºè‘—åçš„æ³°å¦å°¼å…‹å·æ•°æ®é›†åˆ¶ä½œä¸€ä¸ªç®€å•çš„ Knearestneighbor ç®—æ³•ã€‚å°½ç®¡æˆ‘çš„æ•°æ®æ¡†ä¼¼ä¹æ²¡æœ‰ä»»ä½• NaN å€¼ï¼Œä½†æˆ‘è¿˜æ˜¯ä¸æ–­æ”¶åˆ°æ ‡é¢˜ä¸­çš„é”™è¯¯ã€‚æˆ‘è¿·è·¯äº†ï¼Œå¸Œæœ›èƒ½å¾—åˆ°ä¸€äº›å¸®åŠ©ã€‚
y = train_data[&quot;Survived&quot;]
features = [&quot;Fare&quot;, &quot;Sex&quot;]

X = train_data[features]
X.loc[:, &#39;Sex&#39;] = X.loc[:, &#39;Sex&#39;].apply(weigh_sex)
X = ct.fit_transform(X)

X_test = test_data[features]
X_test.loc[:, &quot;Sex&quot;] = X_test.loc[:, &quot;Sex&quot;].apply(weigh_sex)
X_test = ct.fit_transform(X_test)

print(np.where(np.isnan(X))) #å³ä½¿æ”¾åœ¨ clf.fit() ä¹‹åï¼Œä¹Ÿä¸ä¼šè¿”å›ä»»ä½•å†…å®¹

clf.fit(X, y)

predictions = clf.predict(X_test) #æ­¤è¡Œå¼•å‘é”™è¯¯
output = pd.DataFrame({&#39;PassengerId&#39;: test_data.PassengerId, &#39;Survived&#39;: predictions})
output.to_csv(&#39;submission1.csv&#39;, index=False)

ç¼–è¾‘å®Œæ•´å›æº¯
å›æº¯ï¼ˆæœ€è¿‘ä¸€æ¬¡è°ƒç”¨æœ€åä¸€æ¬¡ï¼‰ï¼š
æ–‡ä»¶â€œC:\pystuff\StatsSandbox\Titanic.pyâ€ï¼Œç¬¬ 65 è¡Œï¼Œä½äº&lt;module&gt;
predictions = clf.predict(X_test) #æ­¤è¡Œå¼•å‘é”™è¯¯
æ–‡ä»¶â€œC:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_classification.pyâ€ï¼Œç¬¬ 266 è¡Œï¼Œåœ¨ predict ä¸­
neigh_ind = self.kneighbors(X, return_distance=False)
æ–‡ä»¶â€œC:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_base.pyâ€ï¼Œç¬¬ 804 è¡Œï¼Œåœ¨ kneighbors ä¸­
X = self._validate_data(X, accept_sparse=&quot;csr&quot;, reset=False, order=&quot;C&quot;)
æ–‡ä»¶â€œC:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\base.pyâ€ï¼Œç¬¬ 605 è¡Œï¼Œåœ¨ _validate_data ä¸­
out = check_array(X, input_name=&quot;X&quot;, **check_params)
æ–‡ä»¶ &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, ç¬¬ 957 è¡Œ, åœ¨ check_array ä¸­
_assert_all_finite(
æ–‡ä»¶ &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, ç¬¬ 122 è¡Œ, åœ¨ _assert_all_finite
_assert_all_finite_element_wise(
æ–‡ä»¶ &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, ç¬¬ 171 è¡Œ, åœ¨ _assert_all_finite_element_wise
raise ValueError(msg_err)
ValueError: è¾“å…¥ X åŒ…å« NaNã€‚
 ]]></description>
      <guid>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</guid>
      <pubDate>Wed, 25 Sep 2024 17:26:01 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚æœè®­ç»ƒé›†å’Œæµ‹è¯•é›†å®Œå…¨ä¸ç›¸äº¤ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨å“ªäº› ML/DL æ¨¡å‹æ¥ç© Hangman æ¸¸æˆï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</link>
      <description><![CDATA[Hangman æ˜¯ä¸€æ¬¾çŒœå­—æ¸¸æˆï¼Œç©å®¶éœ€è¦é€ä¸ªå­—æ¯çŒœå‡ºéšè—çš„å•è¯ã€‚æ¯æ¬¡çŒœå¯¹éƒ½ä¼šæ˜¾ç¤ºå•è¯ä¸­è¯¥å­—æ¯çš„æ‰€æœ‰å®ä¾‹ï¼Œè€ŒçŒœé”™åˆ™ä¼šå‡å°‘ç©å®¶å‰©ä½™çš„å°è¯•æ¬¡æ•°ã€‚ç›®æ ‡æ˜¯åœ¨å°è¯•æ¬¡æ•°ç”¨å®Œä¹‹å‰çŒœå‡ºå•è¯ã€‚éšè—å•è¯çš„é•¿åº¦æ˜¯å¯å˜çš„ï¼Œæœ€å¤§å°è¯•æ¬¡æ•°ä¸º 5 æ¬¡ã€‚
æˆ‘å¿…é¡»æƒ³å‡ºä¸€ä¸ªæ¨¡å‹/ç®—æ³•æ¥çŒœæµ‹éšè—å­—ç¬¦ä¸²çš„å­—æ¯ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœéšè—å•è¯æ˜¯â€œmathematicsâ€ï¼Œè¾“å…¥æ˜¯ _ a t h e _ a t i _ sï¼Œåˆ™æ¨¡å‹ç†æƒ³æƒ…å†µä¸‹åº”è¯¥çŒœæµ‹â€œmâ€æˆ–â€œcâ€ï¼‰ã€‚æµ‹è¯•å°†é€šè¿‡æ¨¡æ‹Ÿä»å®Œå…¨éšè—çš„å•è¯ï¼ˆä»…ä¸‹åˆ’çº¿ï¼‰å¼€å§‹çš„ç»åˆ‘æ¸¸æˆæ¥å®Œæˆã€‚
è®­ç»ƒæ•°æ®æ˜¯æ¥è‡ªè‹±è¯­çš„å•è¯åˆ—è¡¨ï¼Œæµ‹è¯•æ•°æ®å°†ä¸åŒ…å«ä»»ä½•è¿™äº›ç»™å®šçš„å•è¯ã€‚
æœºå™¨å­¦ä¹ æ–¹æ³•æ˜¯é¦–é€‰ï¼Œä½†ä¸æ˜¯å¼ºåˆ¶æ€§çš„ã€‚
æˆ‘å°è¯•åœ¨é€šè¿‡éšæœºåˆ é™¤ç»™å®šå•è¯ä¸­çš„å­—æ¯å®ä¾‹ç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒ BiLSTM æ¨¡å‹ï¼Œç„¶åä¸ºè¾“å…¥å­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªä½ç½®ï¼ˆåŒ…æ‹¬å·²ç»æ˜¾ç¤ºçš„å•è¯ï¼‰è¾“å‡º 27 ä¸ªç±»åˆ«çš„ softmaxï¼ˆæ¯ä¸ªå­—æ¯ 1 ä¸ªï¼Œå¡«å……å­—ç¬¦ 1 ä¸ªï¼‰ã€‚ç”±äºè®­ç»ƒé›†ä¸­çš„æœ€å¤§å•è¯æ•°ä¸º 32ï¼Œå› æ­¤æˆ‘å°†è¾“å…¥å­—ç¬¦ä¸²å¡«å……ä¸º 32 ä¸ªå­—ç¬¦ã€‚è¿™å¹¶æ²¡æœ‰ç»™å‡ºä»¤äººæ»¡æ„çš„ç»“æœã€‚
è¿˜å°è¯•äº†å­—èŠ‚å¯¹ç¼–ç  (BPE) æ¥æå–è®­ç»ƒè¯æ±‡è¡¨ä¸­çš„ç›¸å…³å­è¯ï¼Œç„¶åå°è¯•ä»æµ‹è¯•é›†ä¸­çŒœæµ‹å•è¯æ˜¯ BPE è¯æ±‡è¡¨ä¸­ä¸‰ä¸ªå•è¯çš„ç»„åˆã€‚
æˆ‘è¿˜èƒ½å°è¯•ä»€ä¹ˆï¼Ÿè¿™ä¼šå±äºå“ªç§é—®é¢˜ï¼Ÿæˆ‘åœ¨å“ªé‡Œå¯ä»¥é˜…è¯»æœ‰å…³æ­¤ç±»é—®é¢˜çš„æ›´å¤šä¿¡æ¯ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</guid>
      <pubDate>Wed, 25 Sep 2024 14:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter Notebook å’Œ Python è„šæœ¬è¿”å›ä¸åŒçš„ YOLOV8 å®ä¾‹åˆ†å‰²æ©ç </title>
      <link>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</link>
      <description><![CDATA[æˆ‘ä¸€ç›´åœ¨è¿è¡Œé¢„å…ˆè®­ç»ƒçš„ YOLOV8 æ¨¡å‹æ¥è¿›è¡Œå®ä¾‹åˆ†å‰²ã€‚ä¸ºäº†è¿›è¡Œæµ‹è¯•å’Œå¼€å‘ï¼Œæˆ‘ä¸€ç›´åœ¨ä½¿ç”¨ Anaconda Jupyter Notebookï¼Œåœ¨éƒ¨ç½²ä¹‹å‰æˆ‘å°†å…¶è½¬æ¢ä¸º Python è„šæœ¬ã€‚
è¿™æ˜¯æ¥è‡ª Jupyter Notebook çš„ï¼ˆæ­£ç¡®ï¼‰æ©ç ï¼š

è¿™æ˜¯æ¥è‡ª Python è„šæœ¬çš„ç»“æœï¼š

æ‚¨å¯ä»¥çœ‹åˆ° Python ç»“æœåœ¨è¾¹ç¼˜å¤„æœ‰å¥‡æ€ªçš„å—æˆ‘çš„å¤´å’Œæ‰‹æœºã€‚

ä¸¤ä¸ªç‰ˆæœ¬çš„ä»£ç å®Œå…¨ç›¸åŒ
Python è™šæ‹Ÿç¯å¢ƒç›´æ¥ä» Jupyter ç¯å¢ƒå¯¼å‡ºåˆ›å»ºï¼ˆæˆ‘ä»”ç»†æ£€æŸ¥äº†ï¼šPython ç‰ˆæœ¬ç›¸åŒï¼Œæ‰€æœ‰åº“çš„ç‰ˆæœ¬ä¹Ÿç›¸åŒï¼‰
CUDA ç‰ˆæœ¬ç›¸åŒ
ç¡¬ä»¶ç›¸åŒï¼ˆä¸¤ä¸ªç¤ºä¾‹éƒ½åœ¨åŒä¸€å°æœºå™¨ä¸Šæ‰§è¡Œï¼‰

æœ‰äººçŸ¥é“ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µæˆ–æˆ‘è¯¥å¦‚ä½•ä¿®å¤å®ƒå—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</guid>
      <pubDate>Wed, 25 Sep 2024 13:53:07 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘æ­£åœ¨å®ç° qwen_2-vl +byaldiï¼Œç”¨äºåŸºäºè§†è§‰çš„ ocrï¼Œå¹¶å°†å…¶æ‰˜ç®¡ä¸ºåŸºäº stremlit çš„ Web åº”ç”¨ç¨‹åºï¼Œä½†å®ƒæ— æ³•æ­£å¸¸å·¥ä½œï¼Œä¸€ç›´å´©æºƒ</title>
      <link>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ª Streamlit åº”ç”¨ç¨‹åºï¼Œè¯¥åº”ç”¨ç¨‹åºåˆ©ç”¨å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒæœç´¢å’Œæ–‡æœ¬æå–ã€‚ä½†æ˜¯ï¼Œåœ¨å°è¯•ä¸‹è½½æå–çš„æ–‡æœ¬æ—¶ï¼Œè¯¥åº”ç”¨ç¨‹åºç»å¸¸åœ¨æ–‡æœ¬æå–è¿‡ç¨‹ä¸­å´©æºƒã€‚ä»¥ä¸‹æ˜¯ç›¸å…³ä»£ç ï¼Œä»¥åŠé‡ç°é—®é¢˜çš„æ­¥éª¤ã€‚
requirement.txt
pdf2image

git+https://github.com/huggingface/transformers.git

qwen-vl-utils

#flash-attn

byaldi

qwen_vl_utils

transformers

é‡ç°æ­¥éª¤

ä½¿ç”¨æä¾›çš„ä»£ç è¿è¡Œ Streamlit åº”ç”¨ã€‚
ä¸Šä¼ æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶ (JPGã€JPEGã€PNG)ã€‚
åœ¨æä¾›çš„è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡æœ¬æŸ¥è¯¢ã€‚
å•å‡»â€œæœç´¢å¹¶æå–æ–‡æœ¬â€æŒ‰é’®ã€‚

import streamlit as st
import base64
from huggingface_hub import notebook_login
from byaldi import RAGMultiModalModel
from transformers import Qwen2VLForConditionalGenerationã€AutoTokenizerã€AutoProcessor
ä» PIL å¯¼å…¥å›¾åƒ
ä» io å¯¼å…¥ BytesIO
å¯¼å…¥ torch
å¯¼å…¥ re

@st.cache_resource
def load_models():
RAG = RAGMultiModalModel.from_pretrained(&quot;vidore/colpali&quot;, verbose=10)
model = Qwen2VLForConditionalGeneration.from_pretrained(
&quot;Qwen/Qwen2-VL-2B-Instruct&quot;,
torch_dtype=torch.float16,
device_map=&quot;auto&quot;,
)
processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen2-VL-2B-Instruct&quot;)
return RAGã€modelã€processor

RAGã€modelã€processor = load_models()

st.title(&quot;å¤šæ¨¡æ€å›¾åƒæœç´¢å’Œæ–‡æœ¬æå–App&quot;)

uploaded_file = st.file_uploader(&quot;é€‰æ‹©å›¾ç‰‡&quot;, type=[&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;])

å¦‚æœ uploaded_file ä¸ä¸º None:
image = Image.open(uploaded_file)
st.image(image, caption=&#39;Uploaded Image&#39;, use_column_width=True)

temp_image_path = &quot;uploaded_image.jpeg&quot;
image.save(temp_image_path)

@st.cache_data
def create_rag_index(image_path):
RAG.index(
input_path=image_path,
index_name=&quot;image_index&quot;,
store_collection_with_index=True,
overwrite=True,
)

create_rag_index(temp_image_path)

text_query = st.text_input(&quot;è¾“å…¥æ‚¨çš„æ–‡æœ¬æŸ¥è¯¢&quot;)

if st.button(&quot;æœç´¢å¹¶æå–æ–‡æœ¬&quot;):
if text_query:
results = RAG.search(text_query, k=1, return_base64_results=True)

image_data = base64.b64decode(results[0].base64)
image = Image.open(BytesIO(image_data))
st.image(image, caption=&quot;ç»“æœå›¾åƒ&quot;, use_column_width=True)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{&quot;type&quot;: &quot;image&quot;},
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;extract text&quot;}
]
}
]

text_prompt = processing.apply_chat_template(messages, add_generation_prompt=True)

input = processing(
text=[text_prompt],
images=[image],
padding=True,
return_tensors=&quot;pt&quot;
)

è¾“å…¥ = è¾“å…¥.to(model.device)

ä½¿ç”¨ torch.no_grad():
è¾“å‡º_ids = æ¨¡å‹.generate(**è¾“å…¥, max_new_tokens=1024)

ç”Ÿæˆ_ids = è¾“å‡º_ids[:, è¾“å…¥.è¾“å…¥_ids.shape[1]:]

è¾“å‡º_æ–‡æœ¬ = å¤„ç†å™¨.batch_decode(
ç”Ÿæˆ_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
)[0]

# çªå‡ºæ˜¾ç¤ºæŸ¥è¯¢çš„æ–‡æœ¬
def çªå‡ºæ˜¾ç¤º_æ–‡æœ¬(æ–‡æœ¬, æŸ¥è¯¢):
çªå‡ºæ˜¾ç¤º_æ–‡æœ¬ = æ–‡æœ¬
for word in query.split():
æ¨¡å¼ = re.compile(re.escape(word), re.IGNORECASE)
çªå‡ºæ˜¾ç¤º_æ–‡æœ¬ = æ¨¡å¼.sub(lambda m: f&#39;&lt;span style=&quot;background-color: yellow;&quot;&gt;{m.group()}&lt;/span&gt;&#39;, highlight_text)
return highlight_text

highlight_output = highlight_text(output_text, text_query)

st.subheader(&quot;æå–çš„æ–‡æœ¬ï¼ˆæŸ¥è¯¢çªå‡ºæ˜¾ç¤ºï¼‰ï¼š&quot;)
st.markdown(highlighted_output, unsafe_allow_html=True)
else:
st.warning(&quot;è¯·è¾“å…¥æŸ¥è¯¢ã€‚&quot;)
else:
st.info(&quot;ä¸Šä¼ å›¾ç‰‡ä»¥å¼€å§‹ã€‚&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</guid>
      <pubDate>Wed, 25 Sep 2024 10:56:35 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•ä½¿ç”¨ AI / OCR æ£€æµ‹å’ŒéªŒè¯æ–‡æ¡£ä¸­çš„å¤é€‰æ¡†[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</link>
      <description><![CDATA[æˆ‘éœ€è¦å¼€å‘å¯ä»¥è‡ªåŠ¨éªŒè¯ç§°ä¸ºâ€œW åˆåŒâ€çš„æ–‡æ¡£çš„è½¯ä»¶ã€‚è¿™äº›åˆåŒåŒ…å«è®¸å¤šéœ€è¦å¡«å†™çš„å¤é€‰æ¡†å’Œå­—æ®µï¼Œæˆ‘çš„ç›®æ ‡æ˜¯å‡†ç¡®è¯†åˆ«å“ªäº›æ¡†è¢«é€‰ä¸­å¹¶æ ¹æ®ç‰¹å®šè§„åˆ™å’Œå‚æ•°éªŒè¯å€¼ã€‚
æˆ‘å°è¯•ä½¿ç”¨ OpenAI çš„ API æ¥æ£€æµ‹è¿™äº›å…ƒç´ ï¼Œä½†æˆ‘å‘ç°å®ƒåœ¨å¤„ç†æ–‡æ¡£ä¸­çš„å¤§é‡å¤é€‰æ¡†å’Œé€‰é¡¹æ—¶é‡åˆ°äº†å›°éš¾ã€‚å®ƒç»å¸¸é”™è¯¯è¯†åˆ«ç»“æ„æˆ–æ— æ³•æ­£ç¡®è¯†åˆ«å¤é€‰æ¡†ã€‚
æˆ‘æ­£åœ¨å¯»æ‰¾å¯ä»¥å¸®åŠ©æˆ‘æ›´å‡†ç¡®åœ°æ‰§è¡Œæ­¤ä»»åŠ¡çš„æŠ€æœ¯ã€åº“æˆ– API çš„å»ºè®®ï¼Œæœ€å¥½ä½¿ç”¨ OCR å’Œè®¡ç®—æœºè§†è§‰ã€‚ç†æƒ³çš„è§£å†³æ–¹æ¡ˆåº”è¯¥èƒ½å¤Ÿï¼š
æ£€æµ‹ PDF æˆ–å›¾åƒæ–‡æ¡£ä¸­çš„å¤é€‰æ¡†å’Œè¾“å…¥å­—æ®µã€‚
å‡†ç¡®ç¡®å®šå¤é€‰æ¡†æ˜¯å¦è¢«é€‰ä¸­ã€‚
è¯»å–å­—æ®µå¹¶æ ¹æ®æŸäº›è§„åˆ™éªŒè¯æ•°æ®ã€‚
ä»»ä½•å…³äºå¯ä»¥ä¿ƒè¿›è¿™é¡¹ä»»åŠ¡çš„å·¥å…·æˆ–æ–¹æ³•çš„å»ºè®®éƒ½å°†ä¸èƒœæ„Ÿæ¿€ã€‚æˆ‘å¯¹åŸºäº AI çš„è§£å†³æ–¹æ¡ˆæŒå¼€æ”¾æ€åº¦ï¼Œä½†ä¹Ÿå¯¹ä¼ ç»Ÿçš„ OCR æ–¹æ³•æ„Ÿå…´è¶£ã€‚
]]></description>
      <guid>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</guid>
      <pubDate>Wed, 25 Sep 2024 10:22:52 GMT</pubDate>
    </item>
    <item>
      <title>å°†åŒçº¿æ€§é‡‡æ ·ç‰¹å¾æ˜ å°„å›ä½“ç´ </title>
      <link>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªä¸“é—¨ç”¨äºåŒçº¿æ€§é‡‡æ ·çš„ç±»ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å°†ä»åŒçº¿æ€§é‡‡æ ·è¿‡ç¨‹ä¸­æå–çš„ç‰¹å¾æ˜ å°„åˆ°åˆ›å»ºçš„ä½“ç´ ç½‘æ ¼ä¸­çš„é€‚å½“ä½ç½®ã€‚
å®æ–½æ­¥éª¤ï¼š
B = æ‰¹æ¬¡

C = é€šé“

S = è§†å›¾æ•°

D = æ·±åº¦

H = é«˜åº¦

W = å®½åº¦

3 = x,y,z

2 = x,y

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰æ­¤å½¢çŠ¶ [B,D,H,W,3] çš„ä½“ç´ 
å¹¶ä¸”æˆ‘æœ‰ 360 ä¸ªå›¾åƒè§†å›¾æ ·æœ¬ï¼Œå°ºå¯¸ä¸º [B,S,C,H,W]
ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä½¿ç”¨å¤–éƒ¨å’Œå†…éƒ¨å°†ä½“ç´ ç‚¹ï¼ˆç‚¹äº‘ï¼‰æŠ•å½±åˆ°æ¯ä¸ªå›¾åƒã€‚ç„¶åæˆ‘è¿‡æ»¤äº†å›¾åƒä¹‹å¤–çš„ç‚¹ã€‚
ç»“æœæˆ‘å¾—åˆ°äº† valid_points = [B,S,H,W,2]
æˆ‘ä» valid_points ä¸­å¯¹æˆ‘çš„ç‚¹è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œå¹¶åˆ›å»ºäº†å¤§å°ä¸º [B,S,H,W,2] çš„ç½‘æ ¼ï¼Œè¯·æ³¨æ„ï¼ŒW æ˜¯æœ‰æ•ˆç‚¹çš„æ•°é‡ï¼ŒH = 1
ç„¶åæˆ‘æ‰€åšçš„æ˜¯åº”ç”¨åŒçº¿æ€§é‡‡æ ·ï¼Œå¦‚ä»£ç æ‰€ç¤ºï¼š
valid_points = cur_coords[:, on_img[1]]

######### å°†æœ‰æ•ˆç‚¹å½’ä¸€åŒ–åœ¨ [-1, 1] ä¹‹é—´ ########
normalized_points = torch.zeros_like(valid_points)
normalized_points[:,:, 0] = 2.0 * (valid_points[:, :, 0] / (H_img - 1)) - 1.0 # å½’ä¸€åŒ–y åæ ‡
normalized_points[:,:, 1] = 2.0 * (valid_points[:, :, 1] / (W_img - 1)) - 1.0 # [N, M&#39;,2]

grid = normalized_points.unsqueeze(1).cuda() # å½¢çŠ¶ [S, H_out, W_out, 2]
sampled_features_with_location_list = []
for i in range(0,N):
img_s =camera_view_tensor[i].unsqueeze(0).permute(0, 3, 1, 2) #[B, C, H_in, W_in]
grid_s = grid[i].unsqueeze(0)
sampled_points = F.grid_sample(img_s, grid_s,mode=&#39;bilinear&#39;,
align_corners=None) # (B,N,C,H_out,W_out)
sampled_features_list.append(sampled_pointson)
sampled_points = torch.stack(sampled_features_list, dim=1) #[B = 1,S = 6,C= 3,H= 1,W =22965]

ç°åœ¨æˆ‘æƒ³åº”ç”¨é€†æ˜ å°„æ¥æå– bev ç‰¹å¾ã€‚æ€ä¹ˆåšï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</guid>
      <pubDate>Wed, 25 Sep 2024 10:14:39 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ä¸æ–­æ¼”å˜çš„ç‰¹å¾ç©ºé—´ä¸­é‡‡ç”¨åé¦ˆè¿›è¡Œè‡ªé€‚åº”é¢„æµ‹çš„æŠ€æœ¯</title>
      <link>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç ”ç©¶ä¸€ä¸ªé¢„æµ‹é—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡å˜é‡ ğ‘¦ æ¥è‡ªæ­£æ€åˆ†å¸ƒï¼Œè¿ç»­ç‰¹å¾ç©ºé—´ ğ‘‹ å’Œ ğ‘¦ ä¹‹é—´çš„å…³ç³»éšæ—¶é—´ä¿æŒç¨³å®šã€‚ä½†æ˜¯ï¼Œç›®æ ‡å€¼ï¼ˆä¾‹å¦‚å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼‰ä¼šéšç€ç³»ç»Ÿçš„å˜åŒ–è€Œéšæ—¶é—´å˜åŒ–ã€‚æˆ‘äº‹å…ˆå¹¶ä¸äº†è§£çœŸæ­£çš„ç›®æ ‡å€¼ï¼Œå› æ­¤æˆ‘åˆ©ç”¨åœ¨çº¿å›å½’å’Œå¼ºåŒ–å­¦ä¹  (RL) ç­‰æŠ€æœ¯æ ¹æ®åé¦ˆè¿­ä»£è°ƒæ•´æˆ‘çš„é¢„æµ‹ã€‚
åé¦ˆæœºåˆ¶ä»…æŒ‡ç¤ºæˆ‘çš„é¢„æµ‹æ˜¯é«˜ä¼°è¿˜æ˜¯ä½ä¼°ï¼Œå¹¶ä¸”æ­¤åé¦ˆæ˜¯åœ¨å»¶è¿Ÿåæä¾›çš„ã€‚è¯¯å·®å¤§å°æœªçŸ¥ï¼Œå¹¶ä¸”ä»…æä¾›æ–¹å‘æ€§åé¦ˆï¼ˆé«˜ä¼°/ä½ä¼°ï¼‰ã€‚æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨å¢é‡æ–¹æ³•ï¼Œæ ¹æ®åé¦ˆå‘ä¸Šæˆ–å‘ä¸‹è°ƒæ•´é¢„æµ‹ä»¥æ›´æ–°é¢„æµ‹ã€‚
å…³äºæ”¹è¿›æ­¤æ–¹æ³•ï¼Œæˆ‘æœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
(1) æ”¹è¿›è‡ªé€‚åº”è°ƒæ•´æŠ€æœ¯ï¼š

æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨åœ¨çº¿å›å½’å’Œ RLï¼Œåé¦ˆä¼šå‘Šè¯‰æˆ‘æ˜¯å¦å¢åŠ æˆ–å‡å°‘å…ˆå‰çš„é¢„æµ‹ã€‚é™¤äº†ç®€å•çš„å¢é‡/å‡é‡ä¹‹å¤–ï¼Œæˆ‘æ˜¯å¦åº”è¯¥æ¢ç´¢æ›´å…ˆè¿›çš„æŠ€æœ¯æ¥è¿›è¡Œæ›´æ™ºèƒ½çš„è°ƒæ•´ï¼Ÿå…·ä½“æ¥è¯´ï¼Œå½“åé¦ˆä»…é™äºè¿‡åº¦/ä¸è¶³æŒ‡ç¤ºæ—¶ï¼Œæ˜¯å¦æœ‰æ–¹æ³•å¯ä»¥è¿›è¡Œæ›´ç»†å¾®çš„è°ƒæ•´ï¼Œå¹¶ä¸”éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´å¿«çš„æ”¶æ•›æˆ–æ›´å¥½çš„é¢„æµ‹ï¼Ÿ

(2) è‡ªé€‚åº”æ›´æ–°å€¼çš„æœ‰æ•ˆç®¡ç†ï¼š

ä¸ºäº†å¢å¼ºæˆ‘ç›®å‰çš„æ–¹æ³•ï¼Œæˆ‘è€ƒè™‘ä¿æŒé¢„æµ‹çš„åŠ¨æ€ä¸Šé™å’Œä¸‹é™ï¼Œå¹¶æ ¹æ®åé¦ˆè°ƒæ•´è¿™äº›ç•Œé™ï¼ˆå³ç¼©å°è¿‡åº¦/ä¸è¶³ä¼°è®¡ä¹‹é—´çš„èŒƒå›´ï¼‰ã€‚æˆ‘æ¢ç´¢çš„å¦ä¸€ç§ç­–ç•¥æ˜¯ä½¿ç”¨å…ˆå‰è°ƒæ•´çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡çº¿ (EWMA)ï¼Œå…¶ä¸­åå¤çš„ä½ä¼°ä¼šå¯¼è‡´é€æ¸å¢å¤§çš„æ ¡æ­£ã€‚ä½†æ˜¯ï¼Œåœ¨å¤§å‹ç‰¹å¾ç©ºé—´ä¸­ç®¡ç†è¿™äº›è°ƒæ•´åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ã€‚
æˆ‘æœ€åˆä½¿ç”¨å­—å…¸å°†ç‰¹å¾ ğ‘‹ æ˜ å°„åˆ°è¿™äº›æ›´æ–°å€¼ï¼ˆä¾‹å¦‚ç•Œé™æˆ– EWMA è°ƒæ•´ï¼‰çš„æ–¹æ³•éšç€ç‰¹å¾ç©ºé—´çš„å¢é•¿å˜å¾—ä¸åˆ‡å®é™…ã€‚æˆ‘ä¹Ÿå°è¯•å°†è¿™äº›å€¼æ˜ å°„åˆ°å›å½’æ¨¡å‹ï¼Œä½†æ•ˆæœä¸ä½³ï¼Œå¯èƒ½æ˜¯ç”±äºæ›´æ–°çš„éå¹³ç¨³æ€§è´¨ã€‚
é‰´äºè¿™äº›è°ƒæ•´å€¼ä¸æ˜¯é™æ€ç›®æ ‡ï¼Œè€Œæ˜¯åŸºäºåé¦ˆåŠ¨æ€å˜åŒ–çš„ï¼Œåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­æœ‰æ•ˆç®¡ç†æˆ–å»ºæ¨¡å®ƒä»¬çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿåœ¨è¿™æ ·çš„è®¾ç½®ä¸­ï¼Œæ˜¯å¦æœ‰æ›´åˆé€‚çš„è‡ªé€‚åº”æ›´æ–°ç­–ç•¥ï¼Œå¯èƒ½æ¶‰åŠå‡½æ•°é€¼è¿‘æŠ€æœ¯æˆ–å†…å­˜é«˜æ•ˆçš„æ•°æ®ç»“æ„ï¼Ÿ
]]></description>
      <guid>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</guid>
      <pubDate>Wed, 25 Sep 2024 09:24:03 GMT</pubDate>
    </item>
    <item>
      <title>SBERT å¾®è°ƒæ€»æ˜¯åœ¨å®Œæˆæ‰€æœ‰ epoch ä¹‹å‰åœæ­¢</title>
      <link>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä½¿ç”¨ SBERT é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ MiniLMï¼‰è¿›è¡Œä¸€ä¸ªåŒ…å« 995 ä¸ªåˆ†ç±»çš„æ–‡æœ¬åˆ†ç±»é¡¹ç›®ã€‚æˆ‘å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨æŒ‰ç…§æ­¤å¤„åˆ—å‡ºçš„æ­¥éª¤è¿›è¡Œæ“ä½œï¼Œä¸€åˆ‡ä¼¼ä¹éƒ½è¿è¡Œæ­£å¸¸ã€‚
æˆ‘çš„é—®é¢˜å‡ºç°åœ¨å®é™…è®­ç»ƒæ¨¡å‹æ—¶ã€‚æ— è®ºæˆ‘åœ¨è®­ç»ƒå‚æ•°ä¸­è®¾ç½®ä»€ä¹ˆå€¼ï¼Œè®­ç»ƒä¼¼ä¹æ€»æ˜¯æå‰ç»“æŸï¼Œå¹¶ä¸”æ°¸è¿œä¸ä¼šå®Œæˆæ‰€æœ‰æ‰¹æ¬¡ã€‚ä¾‹å¦‚ï¼Œæˆ‘è®¾ç½®äº† num_train_epochs=1ï¼Œä½†å®ƒæœ€å¤šåªèƒ½è¾¾åˆ° 0.49 ä¸ª epochã€‚å¦‚æœ num_train_epochs=4ï¼Œå®ƒæ€»æ˜¯åœ¨ 3.49 ä¸ª epoch å¤„ç»“æŸã€‚
è¿™æ˜¯æˆ‘çš„ä»£ç ï¼š
from datasets import load_dataset
from sentence_transformers import (
SentenceTransformer,
SentenceTransformerTrainer,
SentenceTransformerTrainingArguments,
SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
&quot;nreimers/MiniLM-L6-H384-uncased&quot;,
model_card_data=SentenceTransformerModelCardData(
language=&quot;en&quot;,
license=&quot;apache-2.0&quot;,
model_name=&quot;all-MiniLM-L6-v2&quot;,
)
)

loss = BatchAllTripletLoss(model)
# æŸå¤±æ¦‚è¿°ï¼šhttps://www.sbert.net/docs/sentence_transformer/loss_overview.html
# æ­¤ç‰¹å®šæŸå¤±æ–¹æ³•ï¼šhttps://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss

# è®­ç»ƒå‚æ•°ï¼šhttps://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
# å¿…éœ€å‚æ•°ï¼š
output_dir=&quot;finetune/model20240924&quot;,
# å¯é€‰è®­ç»ƒå‚æ•°ï¼š
num_train_epochs=1,
max_steps = -1,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
learning_rate=1e-5,
warmup_ratio=0.1,
fp16=True, # å¦‚æœæ‚¨æ”¶åˆ° GPU æ— æ³•åœ¨ FP16 ä¸Šè¿è¡Œçš„é”™è¯¯ï¼Œè¯·è®¾ç½®ä¸º False
bf16=False, # å¦‚æœæ‚¨æ‹¥æœ‰æ”¯æŒ BF16 çš„ GPUï¼Œè¯·è®¾ç½®ä¸º True
batch_sampler=BatchSamplers.GROUP_BY_LABEL, # 
# å¯é€‰çš„è·Ÿè¸ª/è°ƒè¯•å‚æ•°ï¼š
eval_strategy=&quot;no&quot;,
eval_steps=100,
save_strategy=&quot;epoch&quot;,
# save_steps=100,
save_total_limit=2,
logs_steps=100,
run_name=&quot;miniLm-triplet&quot;, # å¦‚æœåœ¨ W&amp;B ä¸­ä½¿ç”¨`wandb` å·²å®‰è£…
)

trainer = SentenceTransformerTrainer(
model=model,
args=args,
train_dataset=trainDataset,
eval_dataset=devDataset,
loss=loss,
#evaluator=dev_evaluator,
)
trainer.train()

è¯·æ³¨æ„ï¼Œæˆ‘æ²¡æœ‰ä½¿ç”¨è¯„ä¼°å™¨ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ›å»ºæ¨¡å‹ï¼Œå¹¶åœ¨äº‹åä½¿ç”¨ä¸“ç”¨çš„æµ‹è¯•å€¼é›†å¯¹å…¶è¿›è¡Œæµ‹è¯•ã€‚æˆ‘çš„æ•°æ®é›†ç»“æ„å¦‚ä¸‹ï¼š
Dataset({
features: [&#39;Title&#39;, &#39;Body&#39;, &#39;label&#39;],
num_rows: 23961
})

ä¸ dev æ•°æ®é›†å…·æœ‰ç›¸åŒçš„ç»“æ„ï¼Œåªæ˜¯è¡Œæ•°è¾ƒå°‘ã€‚è¿™å°†æä¾›ä»¥ä¸‹è¾“å‡ºï¼š
 [1473/2996 57:06 &lt; 59:07ï¼Œ0.43 it/sï¼ŒEpoch 0/1]
æ­¥éª¤è®­ç»ƒæŸå¤±
100 1.265600
200 0.702700
300 0.633900
400 0.505200
500 0.481900
600 0.306800
700 0.535600
800 0.369800
900 0.265400
1000 0.345300
1100 0.516700
1200 0.372600
1300 0.392300
1400 0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={&#39;train_runtime&#39;: 3427.9198, &#39;train_samples_per_second&#39;: 6.99, &#39;train_steps_per_second&#39;: 0.874, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5003972503496366, &#39;epoch&#39;: 0.4916555407209613})

æ— è®ºæˆ‘å¦‚ä½•è°ƒæ•´å€¼ï¼Œæˆ‘éƒ½æ— æ³•è®©å®ƒå®Œæˆæ‰€æœ‰æ‰¹æ¬¡ã€‚å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</guid>
      <pubDate>Wed, 25 Sep 2024 03:55:44 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å°† CIFAR10 æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜åˆ° 80ï¼… ä»¥ä¸Šï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</link>
      <description><![CDATA[æœ‰äººèƒ½å¸®åŠ©æˆ‘å—ï¼Ÿæˆ‘ä½¿ç”¨æ¥è‡ª tensorflow æ•°æ®é›†çš„ CIFAR10 æ•°æ®é›†è®­ç»ƒæˆ‘çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½†æˆ‘æ— æ³•å°†æ¨¡å‹å‡†ç¡®ç‡æé«˜åˆ° 80% ä»¥ä¸Š...
æœ‰äººèƒ½ç»™æˆ‘ä¸€ä¸ªå»ºè®®å—ï¼Ÿ
import tensorflow as tf
import time
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

def normalize(train_images, test_images):
normalized_train_dataset = tf.cast(train_images, tf.float32) / 255.0
normalized_test_dataset = tf.cast(test_images, tf.float32) / 255.0
è¿”å› normalized_train_dataset, normalized_test_dataset

# Normalisasi Dataset
train_dataset, test_dataset = normalize(train_images, test_images)

def visualization(image, image_sample=2):
for i in range (image_sample):

print(f&quot;å¼¯æ›²å›¾åƒï¼š{np.shape(image)}&quot;)
print(f&quot;å¼¯æ›²æ•°æ®ï¼š{image[i].dtype}&quot;)
print(f&quot;Nilai æœ€å¤§å›¾åƒï¼š{np.max(image[i])}&quot;)
print(f&quot;Nilai æœ€å°å›¾åƒï¼š{np.min(image[i])}&quot;)

plt.figure(figsize=(6,6))
plt.imshow(image[i])
plt.axis(&#39;off&#39;)
plt.colorbar()
plt.title(&quot;Gambar CIFAR-10&quot;)
plt.grid(False)
plt.show()

visualization(train_dataset)

train_labels = np.squeeze(train_labels)
test_labels = np.squeeze(test_labels)

print(f&quot;Shape Of Train Label : {train_labels.shape}&quot;)

print(f&quot;Shape Of Test_Label : {test_labels.shape}&quot;)

train_labels= to_categorical(train_labels, num_classes=10)
test_labels = to_categorical(test_labels, num_classes=10)

ä» tensorflow.keras.preprocessing.image å¯¼å…¥ ImageDataGenerator

datagen = ImageDataGenerator(
rotation_range=20,
width_shift_range=0.2,
height_shift_range=0.2,
sheath_range=0.2,
zoom_range=0.2,
Horizoâ€‹â€‹ntal_flip=True,
fill_mode=&#39;nearest&#39;
)

model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(32, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu, input_shape=(32, 32, 3)),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(64, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, æ¿€æ´»=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, æ¿€æ´»=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(512, (3,3)ï¼Œpadding=&#39;same&#39;ï¼Œactivation=tf.nn.reluï¼Œkernel_regularizer=tf.keras.regularizers.l2(0.01))ï¼Œ
tf.keras.layers.BatchNormalization()ï¼Œ
tf.keras.layers.MaxPool2D((2,2)ï¼Œstrides=2)ï¼Œ

tf.keras.layers.Flatten()ï¼Œ
tf.keras.layers.Dense(512ï¼Œactivation=tf.nn.relu)ï¼Œ
tf.keras.layers.Dropout(0.3)ï¼Œ

tf.keras.layers.Dense(128ï¼Œactivation=tf.nn.relu)ï¼Œ
tf.keras.layers.Dropout(0.5)ï¼Œ

tf.keras.layers.Dense(10ï¼Œæ¿€æ´»=tf.nn.softmax)
])

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

early_stopping = tf.keras.callbacks.EarlyStopping(
monitor=&#39;val_loss&#39;,
patience=5,
restore_best_weights=True
)

reducer_lr = tf.keras.callbacks.ReduceLROnPlateau(
monitor=&#39;val_loss&#39;,
factor=0.2,
patience=3,
verbose=1,
min_lr=0.00001
)

callbacks = [early_stopping, reducer_lr]

start_time = time.time()

history = model.fit(datagen.flow(
train_dataset,
train_labels,
batch_size=64),
epochs=30,
validation_data=(test_dataset, test_labels),
callbacks=callbacks,
verbose=1
)

end_time = time.time()
training_time = end_time - start_time
print(f&quot;è®­ç»ƒæ—¶é—´ï¼š{training_time/60:.2f} åˆ†é’Ÿ&quot;)

model.save(&#39;hand_gesture_detect.keras&#39;)

# è¯„ä¼°æ¨¡å‹
loss_val, accuracy_val = model.evaluate(test_dataset, test_labels)
print(f&quot;æŸå¤±ï¼š{loss_val}&quot;)
print(f&quot;å‡†ç¡®ç‡ï¼š{accuracy_val}&quot;)

æ¥è‡ª tensorflow.keras.applications å¯¼å…¥ ResNet50

base_model = ResNet50(weights=&#39;ImageNet&#39;, include_top=False, input_tensor=(32, 32, 3))

æˆ‘å·²ç»ä½¿æˆ‘çš„æ¨¡å‹å¤æ‚åŒ–ï¼Œä½†å‡†ç¡®ç‡ä»ç„¶åªæœ‰ 77-80%ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•æé«˜æˆ‘çš„æ¨¡å‹å‡†ç¡®ç‡]]></description>
      <guid>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</guid>
      <pubDate>Wed, 25 Sep 2024 01:55:02 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ GPU ä¸Šè¿è¡Œ gridSearchCV æˆ– randonizedSerchCV</title>
      <link>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</link>
      <description><![CDATA[æˆ‘æƒ³è¿è¡Œ gridSearchCV æˆ– randonizedSerchCV æ¥ä½¿ç”¨ GPU åœ¨ Colab ç¯å¢ƒä¸­è°ƒæ•´è¶…å‚æ•°ã€‚
ä½†æˆ‘æ‰¾ä¸åˆ°è¿™äº›å‡½æ•°ä¸ GPU å…¼å®¹çš„å®ç°ã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘è¯¥å¦‚ä½•è°ƒæ•´è¶…å‚æ•°ï¼Ÿ
å› æ­¤ï¼Œç”±äºæˆ‘æ‰¾ä¸åˆ°åœ¨ GPU ä¸Šè°ƒæ•´è¶…å‚æ•°çš„å‡½æ•°ï¼Œæˆ‘å°è¯•å®ç° randonizedSerchCVã€‚ä½†æˆ‘è®¤ä¸ºä¸€å®šæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€Œæ— éœ€æ‰‹åŠ¨å®ç°è¯¥å‡½æ•°ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</guid>
      <pubDate>Wed, 25 Sep 2024 01:53:20 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å°†é¢„æµ‹å€¼åˆå¹¶å›æ•°æ®é›†ï¼Ÿ[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</link>
      <description><![CDATA[æˆ‘å·²ç»åœ¨ Python ä¸­è®­ç»ƒäº†ä¸€ä¸ª XGboost æ¨¡å‹ï¼Œå¹¶å°†æ¦‚ç‡åˆ—è¡¨ä½œä¸ºè¾“å‡ºã€‚æˆ‘å¦‚ä½•å°†è¿™äº›æ¦‚ç‡å¸¦åˆ°åŸå§‹æ•°æ®é›†ï¼Œä»¥ä¾¿åœ¨ä¸€ä¸ª DF ä¸­æ‹¥æœ‰æ•°æ® + é¢„æµ‹å€¼ï¼Ÿå‡è®¾æˆ‘çš„åŸå§‹åŸå§‹æµ‹è¯• df ç§°ä¸º df_rawã€‚
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
model = XGBClassifier(n_estimators=1500, max_depth=5, n_jobs=-1, min_child_weight=2, 
early_stopping_rounds=25)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
test_outputs = model.predict_proba(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</guid>
      <pubDate>Tue, 24 Sep 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>æå–å“ªäº›ç‰¹å¾æ¥èšç±»æ–‡æœ¬ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</link>
      <description><![CDATA[æˆ‘æƒ³ä¸ºæ–‡æœ¬åˆ¶ä½œä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¿›ä¸€æ­¥ç”¨äºä¸ºç»™å®šçš„æ–‡æœ¬æ¨èæœ€ç›¸ä¼¼çš„æ–‡æœ¬ã€‚
åº”ç”¨ç¨‹åºçš„æµç¨‹å¦‚ä¸‹ï¼š

ä½¿ç”¨ llm ä»æ–‡æœ¬ä¸­æå– 10 ä¸ªä¸»è¦ä¸»é¢˜ï¼ˆå®ƒå¯ä»¥ä» 150 ä¸ªè¯æ± ä¸­é€‰æ‹©ï¼‰
æˆ‘å°†è¯å‘é‡è®¾ä¸ºäºŒè¿›åˆ¶å‘é‡ï¼ŒåŸºæœ¬ä¸Šåœ¨ 150 ç»´ç©ºé—´ä¸­å·¥ä½œï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬éƒ½æœ‰ä¸€ä¸ªåæ ‡ï¼Œä¾‹å¦‚ [1, 0, 1, ..., 0]
ç„¶åæˆ‘ä½¿ç”¨ cosine è·ç¦»æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…ï¼ˆæˆ‘æƒ³æ‰©å±•åˆ° 3-5ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾åªæœ‰ä¸€ä¸ªï¼‰
æˆ‘æ”¶åˆ°äº†æœ€æ¥è¿‘çš„æ–‡æœ¬

é—®é¢˜æ˜¯æ–‡æœ¬éå¸¸ä¸åŒï¼Œå¹¶ä¸” llm å¯ä»¥å¾ˆå¥½åœ°æä¾›ä¸»é¢˜ï¼Œä½†æ˜¯å»ºè®®çš„æ–‡æœ¬å¹¶ä¸å®Œå…¨ç¬¦åˆæˆ‘çš„é¢„æœŸã€‚æˆ‘å°è¯•æ ¹æ®é‡è¦æ€§å¯¹ä¸»é¢˜è¿›è¡Œæ’åºï¼Œå¹¶ä½¿å‘é‡éäºŒè¿›åˆ¶ï¼ˆ[10, 0, 0, 9, ..., 1]ï¼‰ï¼Œä½†è¿™ä¼¼ä¹æ²¡æœ‰å¤ªå¤§å¸®åŠ©ã€‚
æˆ‘æƒ³çŸ¥é“è¿™ç§æ–¹æ³•æ˜¯å¦ä¸é€‚åˆæˆ‘çš„é—®é¢˜ï¼Œæˆ–è€…æˆ‘æ˜¯å¦åº”è¯¥ä½¿ç”¨å…¶ä»–å‚æ•°æˆ–å…¶ä»–ä»»ä½•ä¸œè¥¿æ¥å¯¹æˆ‘çš„æ–‡æœ¬è¿›è¡Œåˆ†ç»„ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</guid>
      <pubDate>Wed, 11 Sep 2024 14:56:49 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ AWS å…è´¹å¥—é¤ä¸Šåˆ†é… GPUï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</link>
      <description><![CDATA[æ˜¯å¦å¯ä»¥åœ¨ AWS å…è´¹å¥—é¤ä¸Šåˆ†é… GPUï¼Ÿå¦‚æœå¯ä»¥ï¼Œæœ‰äººå¯ä»¥è§£é‡Šä¸€ä¸‹æ­¥éª¤å—ï¼Ÿæˆ‘å°è¯•åœ¨ Amazon EC2 ä¸Šåˆ†é… GPUã€‚]]></description>
      <guid>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</guid>
      <pubDate>Fri, 13 Mar 2020 10:33:16 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•ä½¿ FeatureUnion è¿”å› Dataframe</title>
      <link>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</link>
      <description><![CDATA[æ‰€ä»¥æˆ‘ç›®å‰æœ‰ä¸€ä¸ªåŒ…å«å¤§é‡å®¢æˆ·è½¬æ¢å™¨çš„ç®¡é“ï¼š
p = Pipeline([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #æ·»åŠ  [&quot;time&quot;] åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #æ·»åŠ  [&quot;zip&quot;] åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #æ·»åŠ  onehot åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨
])

æ¯ä¸ªè½¬æ¢å™¨éƒ½æ¥æ”¶ä¸€ä¸ª pandas æ•°æ®æ¡†å¹¶è¿”å›åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæ–°åˆ—çš„ç›¸åŒæ•°æ®æ¡†ã€‚å®ƒå®é™…ä¸Šè¿è¡Œå¾—å¾ˆå¥½ï¼Œä½†æˆ‘å¦‚ä½•å¹¶è¡Œè¿è¡Œâ€œGetTimeFromDateâ€å’Œâ€œGetZipFromAddressâ€æ­¥éª¤ï¼Ÿ
æˆ‘æƒ³ä½¿ç”¨ FeatureUnionï¼š
f = FeatureUnion([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #æ·»åŠ  [&quot;time&quot;] åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #æ·»åŠ  [&quot;zip&quot;] åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨])
])

p = Pipeline([
(&quot;FeatureUnionStep&quot;,f),
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #æ·»åŠ  onehot åˆ—çš„è‡ªå®šä¹‰è½¬æ¢å™¨
])

ä½†é—®é¢˜æ˜¯ FeatureUnion è¿”å›çš„æ˜¯ numpy.ndarrayï¼Œè€Œâ€œGroupByTimeandZipâ€æ­¥éª¤éœ€è¦æ•°æ®æ¡†ã€‚
æœ‰æ²¡æœ‰åŠæ³•è®© FeatureUnion è¿”å› pandas æ•°æ®æ¡†ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</guid>
      <pubDate>Fri, 15 Apr 2016 16:18:14 GMT</pubDate>
    </item>
    <item>
      <title>å¼€æºç¥ç»ç½‘ç»œåº“ [å…³é—­]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªå¼€æºç¥ç»ç½‘ç»œåº“ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å·²ç»ç ”ç©¶è¿‡ FANNã€WEKA å’Œ OpenNNã€‚æˆ‘è¿˜åº”è¯¥çœ‹çœ‹å…¶ä»–çš„å—ï¼Ÿå½“ç„¶ï¼Œæ ‡å‡†æ˜¯æ–‡æ¡£ã€ç¤ºä¾‹å’Œæ˜“ç”¨æ€§ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>