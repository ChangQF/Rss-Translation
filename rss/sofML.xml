<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 31 Dec 2024 03:20:50 GMT</lastBuildDate>
    <item>
      <title>将骨干网络 SAM1 更改为 SAM2 [关闭]</title>
      <link>https://stackoverflow.com/questions/79319026/change-backbone-sam1-sam2</link>
      <description><![CDATA[我将 Hi-SAM（文本分割）模型的主干从 SAM1 更改为 SAM2
但模型训练效果不佳
是否可以用 SAM2 替换 SAM1 的主干？
dice：0.9284，dice_hr：0.9073，focal：0.2233，focal_hr：0.2115，iou_mse：0.0007，iou_mse_hr：0.0004████████████████████████████████████████████████████████████████████████████▊ | 296/314 [07:30&lt;00:39，2.19 秒/时]
骰子：0.9400，骰子时间：0.9326，焦距：0.1165，焦距时间：0.1104，iou_mse：0.0036，iou_mse_hr：0.0000██████████████████████████████████████████████████████████████████████████████▎ | 297/314 [07:30&lt;00:59，3.51 秒/时]
骰子：0.9576，骰子时间：0.9385，焦距：0.1073，焦距时间：0.1062，iou_mse：0.0000，iou_mse_hr：0.0004██████████████████████████████████████████████████████████████████████████████▊ | 298/314 [07:33&lt;00:41，2.58 秒/时]
骰子：0.9656，骰子时间：0.9489，焦距：0.1200，焦距时间：0.0978，iou_mse：0.0000，iou_mse_hr：0.0000████████████████████████████████████████████████████████████████████████████▎ | 299/314 [07:34&lt;00:39，2.67 秒/时]
列车：[001] 损失：2.1323：96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 300/314 [07:34&lt;00:27, 1.97s/it]libpng 警告：iCCP：已知不正确的 sRGB 配置文件
dice：0.9616，dice_hr：0.9379，focal：0.0929，focal_hr：0.0776，iou_mse：0.0003，iou_mse_hr：0.0002██████████████████████████████████████████████████████████████████████████████▊ | 300/314 [07:35&lt;00:27，1.97 秒/时]
骰子：0.9287，骰子时间：0.9306，焦距：0.5704，焦距时间：0.7822，iou_mse：0.0001，iou_mse_hr：0.0002████████████████████████████████████████████████████████████████████████████████▎ | 301/314 [07:35&lt;00:23，1.80 秒/时]
骰子：0.9257，骰子时间：0.8996，焦距：0.1794，焦距时间：0.1899，iou_mse：0.0001，iou_mse_hr：0.0002████████████████████████████████████████████████████████████████████████████████▊ | 302/314 [07:36&lt;00:16，1.38 秒/时]
骰子：0.9361，骰子时间：0.9250，焦距：0.2103，焦距时间：0.1946，iou_mse：0.0009，iou_mse_hr：0.0001████████████████████████████████████████████████████████████████████████████████▎ | 303/314 [07:37&lt;00:13，1.20 秒/时]
骰子：0.9332，骰子时间：0.9257，焦距：0.3758，焦距时间：0.3885，iou_mse：0.0015，iou_mse_hr：0.0000██████████████████████████████████████████████████████████████████████████████▊ | 304/314 [07:38&lt;00:10, 1.08s/it]
dice: 0.9594, dice_hr: 0.9600, focal: 0.2075, focal_hr: 0.2865, iou_mse: 0.0000, iou_mse_hr: 0.0003████████████████████████████████████████████████████████████████████████████▍ | 305/314 [07:38&lt;00:08, 1.03it/s]
loss 没有减少
我验证了输入数据没有问题。发生了什么？
输出掩码为黑色，什么都没有显示。]]></description>
      <guid>https://stackoverflow.com/questions/79319026/change-backbone-sam1-sam2</guid>
      <pubDate>Tue, 31 Dec 2024 02:16:18 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归的梯度下降</title>
      <link>https://stackoverflow.com/questions/79319005/gardient-descent-for-multiple-linear-regression</link>
      <description><![CDATA[所以这可能是一个愚蠢的问题，但我正在检查多元线性回归的梯度下降 (GD)，并注意到每个权重 (w_j) 的偏导数或梯度仅在其对应的特征值 (x_ij) 上有所不同。

现在，我总是试图直观地理解为什么某些事物是这样的，而且由于我的物理学背景，我经常试图从物理上理解。不过，对于这个问题，我似乎想不出除了将其理解为采用线性组合的 PD 的直接结果之外的任何东西。
有人对这个算法还有其他直观的看法吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79319005/gardient-descent-for-multiple-linear-regression</guid>
      <pubDate>Tue, 31 Dec 2024 01:56:08 GMT</pubDate>
    </item>
    <item>
      <title>加载的 Keras 模型在预测时出现错误（可能与掩蔽有关）</title>
      <link>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</link>
      <description><![CDATA[我目前正在开发和测试一个依赖大量数据进行训练的 RNN，因此尝试将训练文件和测试文件分开。我有一个文件，用于创建、训练和保存 tensorflow.keras 模型到文件 &#39;model.keras&#39;。然后，我将这个模型加载到另一个文件中并预测一些值，但出现以下错误：
无法将元素 {&#39;class_name&#39;: &#39;__tensor__&#39;, &#39;config&#39;: {&#39;dtype&#39;: &#39;float64&#39;, &#39;value&#39;: [0.0, 0.0, 0.0, 0.0]}} 转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF dtypes
顺便说一句，我曾尝试使用训练模型的文件中完全相同的数据运行 model.predict，并且运行顺利。模型加载肯定是问题所在，而不是用于预测的数据。
这个神秘的 float64 张量是我传递到掩码层的值。我不明白为什么 keras 无法将这个 JSON 对象识别为张量并以此方式应用掩码操作。我在下面附上了我的代码片段，为清晰和简洁起见进行了编辑：
model_generation.py：
# 创建模型

model = tf.keras.Sequential([
tf.keras.layers.Input((352, 4)),
tf.keras.layers.Masking(mask_value=tf.convert_to_tensor(np.array([0.0, 0.0, 0.0, 0.0]))),
tf.keras.layers.GRU(50, return_sequences=True,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.GRU(50,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.Dense(units=1,activation=&#39;sigmoid&#39;)])

# 编译模型...
# 训练模型...
model.save(&#39;model.keras&#39;)

model.predict(data) # 此行在此起作用

model_testing.py
model = tf.keras.models.load_model(&#39;model.keras&#39;)

model.predict(data) # 此行生成错误
]]></description>
      <guid>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</guid>
      <pubDate>Tue, 31 Dec 2024 00:53:11 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn StackingClassifier：Stacking 模型的手动 Python 实现与 sklearn.ensemble.StackingClassifier 之间的区别</title>
      <link>https://stackoverflow.com/questions/79318736/sklearn-stackingclassifier-differences-between-manual-python-implementation-of</link>
      <description><![CDATA[我尝试构建一个 Python 类 CustomStackingClassifier()，以实现集成机器学习中的 Stacking 方法。在此实现中，基础分类器的输出设置为预测概率，并使用 StratifiedKFold 进行模型训练。元分类器的输入矩阵具有维度 (样本、模型 * 类)。
此代码本质上手动复制了 sklearn.ensemble.StackingClassifier() 的功能。但是，在使用葡萄酒数据集进行测试并比较两种方法的结果后，我发现了差异。尽管花了很多时间，但我还是无法确定问题所在。我将非常感谢社区的任何帮助或见解。非常感谢！代码如下：
class CustomStackingClassifier(BaseEstimator, ClassifierMixin):

def init(self, base_classifiers, meta_classifier, n_splits=5):
&quot;&quot;&quot;
param base_classifiers: 估计器列表
param meta_classifier: final_estimator
param n_splits: cv
&quot;&quot;&quot;

self.base_classifiers = base_classifiersself
meta_classifier = meta_classifierself
n_splits = n_splits

def fit(self, X, y):
&quot;&quot;&quot;
:param X: 训练数据
:param y: 训练标签
&quot;&quot;&quot;

n_samples = X.shape[0]
n_classifiers = len(self.base_classifiers)
n_classes = len(np.unique(y)) # 获取类别数量

base_probabilities_1 = np.zeros((n_samples, n_classifiers * n_classes)) # 用于存储基分类器的预测概率

# 通过 StratifiedKFold 设置交叉验证，与 StackingClassifier 一致
kf = StratifiedKFold(n_splits=self.n_splits, shuffle=False, random_state=None)

# 重置数据的索引
X_re_index = X.reset_index(drop=True)
y_re_index = y.reset_index(drop=True)

# 训练每个基分类器并生成预测概率
for i, clf in enumerate(self.base_classifiers):
fold_probabilities = np.zeros((n_samples, n_classes)) 

# 训练并预测每个折叠
for train_index, val_index in kf.split(X_re_index,y_re_index):
X_train, X_val = X_re_index.iloc[train_index], X_re_index.iloc[val_index]
y_train, y_val = y_re_index.iloc[train_index], y_re_index.iloc[val_index]

# 训练基础分类器
clf.fit(X_train, y_train)

# 预测验证集上的概率
fold_probabilities[val_index] = clf.predict_proba(X_val)

# 将每个基础分类器的预测概率保存到 base_probabilities 中
base_probabilities_1[:, i * n_classes: (i + 1) * n_classes] = fold_probabilities

# 使用基础分类器的预测概率训练元分类器
self.meta_classifier.fit(base_probabilities_1, y_re_index)

return self

def predict(self, X):
&quot;&quot;&quot;
:param X: 测试数据
&quot;&quot;&quot;
# 获取每个基础分类器的预测概率
base_probabilities = np.column_stack([clf.predict_proba(X) for clf in self.base_classifiers])

# 使用元分类器预测最终标签
return self.meta_classifier.predict(base_probabilities)

def predict_proba(self, X):
&quot;&quot;&quot;
:param X: 测试数据
&quot;&quot;&quot;
# 获取每个基分类器的预测概率
base_probabilities = np.column_stack([clf.predict_proba(X) for clf in self.base_classifiers])

# 使用元分类器获取预测概率
return self.meta_classifier.predict_proba(base_probabilities)

希望澄清一下CustomStackingClassifier()是否存在逻辑问题，如果有问题，希望得到指导和修改建议。如果实现正确，为什么与sklearn.ensemble.StackingClassifier()相比结果有差异？
需要注意的是，基分类器和元分类器的hyperparameters和random_state都已修复，因此可以排除这些因素是原因。]]></description>
      <guid>https://stackoverflow.com/questions/79318736/sklearn-stackingclassifier-differences-between-manual-python-implementation-of</guid>
      <pubDate>Mon, 30 Dec 2024 22:14:19 GMT</pubDate>
    </item>
    <item>
      <title>SVM 调优过程：由 `vectbl_recycle_rhs_rows()` 中的错误引起：</title>
      <link>https://stackoverflow.com/questions/79318610/svm-tuning-process-caused-by-error-in-vectbl-recycle-rhs-rows</link>
      <description><![CDATA[我不明白SVM模型调优过程的输出。
这是我的代码。
basic_recipe &lt;-
recipe(target ~ 
loan_type + New_versus_Repeat + 
Total_Amount + Total_Amount_to_Repay +
disbursement_date + due_date + duration +
Amount_Funded_By_Lender + Lender_portion_Funded + Lender_portion_to_be_repaid, 
data = train) |&gt;
step_string2factor(loan_type, levels = all_loan_type) |&gt;
step_string2factor(New_versus_Repeat, levels = all_new_versus_repeat) |&gt;
step_other(loan_type, Threshold = 0.1 / 100) |&gt;
step_date(all_date()) |&gt;
step_log(duration, Total_Amount, Total_Amount_to_repay, Amount_Funded_By_Lender, Lender_portion_to_be_repaid, base = 10, offset = 0.001) |&gt;
step_normalize(Total_Amount, Total_Amount_to_repay, Amount_Funded_By_Lender, Lender_portion_to_be_repaid, Lender_portion_to_be_repaid, na_rm = TRUE) |&gt;
step_cut(duration, breaks = ggplot_build(p)$data[[1]]$x) |&gt;
step_dummy(loan_type, New_versus_Repeat, duration, one_hot = TRUE)

svm_model &lt;-
svm_rbf(mode = &quot;classification&quot;, cost = tune(), rbf_sigma = tune()) |&gt;
set_engine(&quot;kernlab&quot;)

control_gd &lt;- control_grid(
verbose = TRUE,
save_pred = FALSE,
parallel_over = &quot;everything&quot;
)

set.seed(1)

svm_wf &lt;-
working() |&gt;
add_model(svm_model) |&gt;
add_recipe(basic_recipe)

svm_grid &lt;-
grid_regular(
cost(),
rbf_sigma(),
levels = c(3, 3)
)

svm_tune &lt;- 
svm_wf |&gt;
tune_grid(
resamples = folds,
grid = svm_grid,
metrics = metrics,
control = control_gd
)

这是第一次重新采样的输出 Resample01
Resample01：预处理器 1/1
✓ Resample01：预处理器 1/1
i Resample01：预处理器 1/1，模型 1/9
! Resample01：预处理器 1/1，模型 1/9：变量 `&#39; 常量。无法缩放数据。
✓ Resample01：预处理器 1/1，模型 1/9
i Resample01：预处理器 1/1，模型 1/9（提取）
i Resample01：预处理器 1/1，模型 1/9（预测）
x Resample01：预处理器 1/1，模型 1/9（预测）：
`$&lt;-` 中出现错误：
！分配的数据 `orig_rows` 必须与现有数据兼容。
✖ 现有数据有 6456 行。
✖ 分配的数据有 6492 行。
ℹ 仅回收大小为 1 的向量。
由 `vectbl_recycle_rhs_rows()` 中的错误导致：
！无法将大小为 6492 的输入回收到大小为 6456 的输入。
i Resample01：预处理器 1/1
✓ Resample01：预处理器 1/1
i Resample01：预处理器 1/1，模型 2/9
! Resample01：预处理器 1/1，模型 2/9：变量 `&#39; 常量。无法缩放数据。
✓ Resample01：预处理器 1/1，模型 2/9
i Resample01：预处理器 1/1，模型 2/9（提取）
i Resample01：预处理器 1/1，模型 2/9（预测）
x Resample01：预处理器 1/1，模型 2/9（预测）：
`$&lt;-` 中出现错误：
! 分配的数据 `orig_rows` 必须与现有数据兼容。
✖ 现有数据有 6456 行。
✖ 分配的数据有 6492 行。
ℹ 仅回收大小为 1 的向量。
由 `vectbl_recycle_rhs_rows()` 中的错误引起：
！无法将大小为 6492 的输入回收为大小为 6456。


我不明白这些错误。
有人能帮我理解吗？我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79318610/svm-tuning-process-caused-by-error-in-vectbl-recycle-rhs-rows</guid>
      <pubDate>Mon, 30 Dec 2024 20:59:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在基础数据集上提高机器学习的能力？</title>
      <link>https://stackoverflow.com/questions/79318363/how-to-improve-machine-learning-on-basic-data-set</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79318363/how-to-improve-machine-learning-on-basic-data-set</guid>
      <pubDate>Mon, 30 Dec 2024 18:46:42 GMT</pubDate>
    </item>
    <item>
      <title>测试图像是否符合指南[关闭]</title>
      <link>https://stackoverflow.com/questions/79317677/test-if-image-is-compliant-to-guideline</link>
      <description><![CDATA[我正在尝试建立一个模型来确定产品图像是否合规（例如，图像应该有背景，图像不应放置在图像的边缘）。指南相当长，图像不合规的原因可能太多，因此不可能逐一解决不合规的原因。我正在寻找某种整体解决方案。
鉴于图像和文本的进步，我想有一些现成的工具可以进行微调。如果有人能在这里提供一些见解，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79317677/test-if-image-is-compliant-to-guideline</guid>
      <pubDate>Mon, 30 Dec 2024 13:02:30 GMT</pubDate>
    </item>
    <item>
      <title>SGDRegressor 图与 LinearRegressor 有很大不同</title>
      <link>https://stackoverflow.com/questions/79317350/sgdregressor-graph-looking-so-different-from-linearregression</link>
      <description><![CDATA[使用 ggplot2 的“经济学”数据集研究线性回归。
为什么我的 SGDRegression 图表看起来像这样？
x = df.loc[:,&#39;pce&#39;].values.reshape(-1,1)
y = df.loc[:,&#39;psavert&#39;].values.reshape(-1,1)
来自 sklearn.model_selection 导入 train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)
来自 sklearn.linear_model 导入 SGDRegressor
sr = SGDRegressor(eta0 = 0.001, verbose = 1)
sr.fit(x_train,y_train.flatten())
plt.scatter(x_train,y_train, s = 5, alpha = 0.3, c = &#39;blue&#39;)
plt.plot(x_train,sr.predict(x_train), c = &#39;green&#39;)
plt.xlabel(&#39;pce(billions)&#39;)
plt.ylabel(&#39;saving rate&#39;)
plt.show()

SGD 图表
线性回归图表
尝试调整 eta0 或 max_iter，但不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79317350/sgdregressor-graph-looking-so-different-from-linearregression</guid>
      <pubDate>Mon, 30 Dec 2024 10:35:23 GMT</pubDate>
    </item>
    <item>
      <title>由于模拟程度高，蒙特卡洛树搜索在 1 步内失误</title>
      <link>https://stackoverflow.com/questions/79316664/monte-carlo-tree-search-blundering-mate-in-1-due-to-high-simulations</link>
      <description><![CDATA[我很难理解在 MCTS 中选择终端节点时会发生什么。我看到这里、这里和这里有几篇标题类似的帖子，但它们似乎没有解释我的难点。
假设是白棋先走，W 步虽然在下一步 B 步时失误失利，但仍被扩展，并恰好在出局时获胜。经过反向传播后，W 得分较高，因此选择一个子节点进行扩展，即 B。B 是终端节点，因此我们反向传播黑棋获胜。现在 B 得分较高。现在我看到两种可能性：

允许再次选择 B（尽管它没有子节点），在这种情况下，B 会不断积累越来越多的得分，因为它每次都赢得黑棋并继续被选中。然后 W 将成为根节点模拟程度最高的子节点，因此尽管犯了错误，但仍被选中进行游戏（我根据维基百科文章选择模拟程度最高的节点）。

我不允许再次选择 B。但我认为这会带来另一个问题。如果当前状态是在 W 步之后，并且是黑棋先行，那么黑棋只会选择 B 一次，尽管 B 赢得了比赛，但黑棋不会获胜，因为 B 的模拟次数会比其他步少。


这似乎是算法的一个非常基本的部分，所以我确信我只是错过了一些关于它如何工作的显而易见的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79316664/monte-carlo-tree-search-blundering-mate-in-1-due-to-high-simulations</guid>
      <pubDate>Mon, 30 Dec 2024 02:07:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中为 Nvidia GeForce RTX 3050 Ti 启用 CUDA？</title>
      <link>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</link>
      <description><![CDATA[我想在我的显卡（Nvidia GeForce RTX 3050 Ti）上运行 PyTorch 库（我在 PyCharm 的虚拟环境中运行该库）。但是，它在 CPU 上运行，每当我使用命令 import torch 和 print(&quot;cuda is available:&quot;, torch.cuda.is_available()) 时，它总是返回 False。
我安装了 CUDA 版本 12.6。我还安装了 PyTorch for CUDA 版本 12.4，因为它是 PyTorch 网站上可用的最新版本。考虑到我的显卡类型，我应该安装什么？]]></description>
      <guid>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</guid>
      <pubDate>Thu, 07 Nov 2024 04:30:29 GMT</pubDate>
    </item>
    <item>
      <title>需要一些帮助来调试这个 java</title>
      <link>https://stackoverflow.com/questions/53719504/need-some-help-debugging-this-java</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/53719504/need-some-help-debugging-this-java</guid>
      <pubDate>Tue, 11 Dec 2018 07:44:58 GMT</pubDate>
    </item>
    <item>
      <title>随机森林多分类不会提高准确性</title>
      <link>https://stackoverflow.com/questions/52703577/random-forest-multi-class-does-not-improve-accuracy</link>
      <description><![CDATA[我正在制作一个随机森林多分类器模型。基本上有数百个家庭具有 200 多个特征，并且基于这些特征我必须将它们归类到其中一个类别 {1,2,3,4,5,6}。
我面临的问题是无论我怎么努力都无法提高模型的准确性。我使用过 RandomSearchCV 和 GridSearchCV，但我只能达到 68% 左右的准确率。
一些注意事项

样本点不平衡。这是按降序排列的类别顺序 {1,4,2,7,6,3}。我使用了 class_weight = &quot;balanced&quot;但它确实提高了准确度。
我尝试了 50-450 个估算器
我还计算了 f1 分数，而不仅仅是通过准确度来比较模型

你们还建议如何提高准确度/f1 分数？]]></description>
      <guid>https://stackoverflow.com/questions/52703577/random-forest-multi-class-does-not-improve-accuracy</guid>
      <pubDate>Mon, 08 Oct 2018 13:37:56 GMT</pubDate>
    </item>
    <item>
      <title>线性回归的梯度下降不收敛</title>
      <link>https://stackoverflow.com/questions/42869949/gradient-descent-on-linear-regression-not-converging</link>
      <description><![CDATA[我已经在 J​​avaScript 中实现了一个非常简单的梯度下降线性回归算法，但在查阅了多个来源并尝试了几种方法后，我无法让它收敛。
数据是绝对线性的，它只是数字 0 到 30 作为输入，x*3 作为其正确的输出以供学习。
这是梯度下降背后的逻辑：
train(input, output) {
const predictOutput = this.predict(input);
const delta = output - predictOutput;

this.m += this.learningRate * delta * input;
this.b += this.learningRate * delta;
}

predict(x) {
return x * this.m + this.b;
}

我从不同的地方获取了公式，包括：

Udacity 深度学习基础纳米学位的练习
Andrew Ng 的线性回归梯度下降课程（也在这里）
斯坦福的 CS229 讲义
我从卡内基梅隆大学找到的其他 PDF 幻灯片

我已经尝试过：

将输入和输出值标准化为 [-1, 1] 范围
将输入和输出值标准化为 [0, 1] 范围
将输入和输出值标准化为平均值 = 0 和标准差 = 1
降低学习率（1e-7 是我使用的最低值）
拥有一个完全没有偏差的线性数据集（y = x * 3)
具有非零偏差的线性数据集 (y = x * 3 + 2)
使用 -1 和 1 之间的随机非零值初始化权重

尽管如此，权重 (this.b 和 this.m) 仍未接近任何数据值，并且它们发散到无穷大。
我显然做错了什么，但我不知道是什么。

更新：以下是一些背景信息，可能有助于弄清楚我的问题到底是什么：
我正在尝试对线性函数的简单近似进行建模，并通过线性回归伪神经元进行在线学习。这样，我的参数就是：

权重：[this.m，this.b]
输入：[x，1]
激活函数：恒等函数 z(x) = x

因此，我的网络将表示为 y = this.m * x + this.b * 1，模拟我想要近似的数据驱动函数 (y = 3 * x)。
我希望我的网络“学习”参数 this.m = 3 和 this.b = 0，但似乎我陷入了局部最小值。
我的误差函数是均方误差： 
error(allInputs, allOutputs) {
let error = 0;
for (let i = 0; i &lt; allInputs.length; i++) {
const x = allInputs[i];
const y = allOutputs[i];
const predictOutput = this.predict(x);
const delta = y - predictOutput;

error += delta * delta;
}

return error / allInputs.length;
}

我更新权重的逻辑将是（根据我迄今为止检查过的来源）wi -= alpha * dError/dwi
为了简单起见，我将我的权重称为 this.m 和 this.b，这样我们就可以将其与我的 JavaScript 代码联系起来。我还将 y^ 称为预测值。
从这里开始：
error = y - y^
= y - this.m * x + this.b

dError/dm = -x
dError/db = 1

因此，将其应用于权重校正逻辑：
this.m += alpha * x
this.b -= alpha * 1

但这似乎根本不正确。]]></description>
      <guid>https://stackoverflow.com/questions/42869949/gradient-descent-on-linear-regression-not-converging</guid>
      <pubDate>Sat, 18 Mar 2017 02:53:48 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机糟糕的结果-Python</title>
      <link>https://stackoverflow.com/questions/38685875/support-vector-machine-bad-results-python</link>
      <description><![CDATA[我正在研究 SVM 并实现了此代码，它太基础、太原始并且花费太多时间，但我只是想看看它实际上是如何工作的。不幸的是，它给了我糟糕的结果。我错过了什么？一些编码错误或数学错误？如果您想查看数据集，请在此处链接。我从 UCI 机器学习存储库中获取了它。感谢您的支持。
def hypo(x,q):
return 1/(1+np.exp(-x.dot(q)))

data=np.loadtxt(&#39;LSVTVoice&#39;,delimiter=&#39;\t&#39;);

x=np.ones(data.shape)
x[:,1:]=data[:,0:data.shape[1]-1]
y=data[:,data.shape[1]-1]

q=np.zeros(data.shape[1])
C=0.002

##均值归一化
for i in range(q.size-1):
x[:,i+1]=(x[:,i+1]-x[:,i+1].mean())/(x[:,i+1].max()-x[:,i+1].min());

对于范围（2000）内的 i：
h=x.dot(q)
对于范围（q.size）内的 j：
q[j]=q[j]-(C*np.sum( -y*np.log(hypo(x,q))-(1-y)*np.log(1-hypo(x,q))) ) + (0.5*np.sum(q**2))

对于范围（y.size）内的 i：
如果 h[i]&gt;=0：
打印 y[i],&#39;1&#39; 
否则：
打印 y[i],&#39;0&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/38685875/support-vector-machine-bad-results-python</guid>
      <pubDate>Sun, 31 Jul 2016 16:04:48 GMT</pubDate>
    </item>
    <item>
      <title>什么是logits？softmax和softmax_cross_entropy_with_logits有什么区别？</title>
      <link>https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop</link>
      <description><![CDATA[在 tensorflow API 文档 中，他们使用一个名为 logits 的关键字。它是什么？很多方法都是这样写的：
tf.nn.softmax(logits, name=None)

如果logits只是一个通用的Tensor输入，为什么它被命名为logits？

其次，以下两种方法有什么区别？
tf.nn.softmax(logits, name=None)
tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)

我知道tf.nn.softmax的作用，但不知道另一个。举个例子会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop</guid>
      <pubDate>Sat, 12 Dec 2015 14:03:27 GMT</pubDate>
    </item>
    </channel>
</rss>