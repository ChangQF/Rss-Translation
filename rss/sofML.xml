<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 30 Jun 2024 06:19:35 GMT</lastBuildDate>
    <item>
      <title>调整超参数的步骤是什么？</title>
      <link>https://stackoverflow.com/questions/78687774/what-are-the-steps-to-adjusting-my-hyperparameters</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78687774/what-are-the-steps-to-adjusting-my-hyperparameters</guid>
      <pubDate>Sun, 30 Jun 2024 06:03:01 GMT</pubDate>
    </item>
    <item>
      <title>ai文档布局文本提取[关闭]</title>
      <link>https://stackoverflow.com/questions/78687256/ai-document-layout-text-extraction</link>
      <description><![CDATA[我计划基于静态布局创建一个自动文档文本提取器，我不知道如何使用人工智能实现此功能。
文档布局是静态的，
即使文档方向不直，我也想从中提取文本，文档图像是使用手机摄像头提供的，这使得很难使图像的方向与布局相匹配。
这两个例子
示例 1 的文档方向不直
我成功地使用布局提取了文本，但当文档方向不直时，它会提取错误的文本，因为如果字段的坐标不同，则它们会从文档更改为其他文档]]></description>
      <guid>https://stackoverflow.com/questions/78687256/ai-document-layout-text-extraction</guid>
      <pubDate>Sat, 29 Jun 2024 22:54:20 GMT</pubDate>
    </item>
    <item>
      <title>R 中的神经网络（未生成响应）</title>
      <link>https://stackoverflow.com/questions/78687170/neural-network-in-r-response-not-generating</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78687170/neural-network-in-r-response-not-generating</guid>
      <pubDate>Sat, 29 Jun 2024 22:04:05 GMT</pubDate>
    </item>
    <item>
      <title>使用类似数据集 (slack) 的消息回复对 llama3 进行微调</title>
      <link>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</link>
      <description><![CDATA[我想在一个数据集上微调 llama3，其中数据结构是考虑以下规则的消息列表：

有频道。
每个频道都有来自各种用户的消息。
每条消息可能都有与其上下文相对应的回复。

我已经有了抓取所有数据的逻辑，但我对数据集结构应该是什么样子有点困惑。
我读过 llama3 文档，看起来应该应用下面的模板（示例取自：https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/):
&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

您是一位乐于助人的 AI 助手，可提供旅行提示和建议&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

您能帮我什么忙？&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

假设每个单独的消息/重放如下所示：
&lt;timestamp&gt; - &lt;user&gt;: &lt;message&gt;

数据的最终结果应该是什么样的？
它是一个字典列表吗？如果是，那么回复应该如何放置？
我敢于问 GPT4o，它给了我以下示例：
prompt_example_1 = [
{&quot;role&quot;: &quot;system&quot;, &quot;message&quot;: &quot;Channel: general&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;message&quot;: &quot;U12345678 [2023-06-01 12:00:00]: 频道中的主要消息&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;message&quot;: &quot;U87654321 [2023-06-01 12:05:00]: 回复主要消息消息”},
{“role”: “assistant”, “message”: “U23456789 [2023-06-01 12:10:00]: 对主消息的另一条回复”},
{“role”: “user”, “message”: “U23456789 [2023-06-01 12:15:00]: 另一条主消息”},
{“role”: “assistant”, “message”: “U34567890 [2023-06-01 12:20:00]: 对第二条主消息的回复”}
]

出于某种原因我觉得不对劲。
如果我打乱数据集会发生什么？所有回复都将失去与其父消息的关联。]]></description>
      <guid>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</guid>
      <pubDate>Sat, 29 Jun 2024 20:35:49 GMT</pubDate>
    </item>
    <item>
      <title>根据数据序列标记评分标准的正确机器学习方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78686788/whats-the-right-machine-learning-approach-to-mark-rubrics-based-on-sequences-of</link>
      <description><![CDATA[我是一名教师，我正在做一个业余项目，帮助简化一些针对学生的评估工作流程。其中一个工作流程是以如下所示的评分标准的形式收集学生进度数据：

行是我们正在涵盖的特定结果（在本例中，读时钟和使用时间单位），列是学生能够完成的问题/任务（简单、中等和具有挑战性的任务）。在特定单元中，我会记录学生每次尝试问题/任务以及他们的表现（如果他们答对了就打勾，如果他们和小组一起答对了就打 G，如果他们需要帮助就打 A，如果他们答错了就打 X，等等）。在单元结束时，我会查看所有行并选择他们能够回答的最高级别的问题，这将转化为他们的成绩。换句话说，对于每一行，我根据每个单元格中的数据选择一列。序列中后面的数据具有更高的优先级，因此早期的一堆错误答案不一定比单元后面的正确答案更重要。
我想使用某种 ML 模型来根据每列中存在的数据预测每行将选择哪一列。行是相互独立评估的。我正在使用 Swift 在 iOS 和 macOS 上开发此应用程序，但我对 ML 世界还很陌生。我无法找到让 Create ML 做我想做的事情的方法，但任何能为我指明正确方向的想法都将不胜感激！我还没有完全掌握 Swift，所以如果我需要使用 python 或其他语言来创建模型，那也没问题，只要它可以集成到 swift 应用程序中即可。我的训练数据是来自许多评分标准的一组行，其中行中的每个单元格都有一个字母与我在评分标准上使用的符号相关联，并且有一列表示基于数据的正确单元格。]]></description>
      <guid>https://stackoverflow.com/questions/78686788/whats-the-right-machine-learning-approach-to-mark-rubrics-based-on-sequences-of</guid>
      <pubDate>Sat, 29 Jun 2024 18:33:17 GMT</pubDate>
    </item>
    <item>
      <title>如何调整 keras 模型的输出：ValueError：维度必须相等</title>
      <link>https://stackoverflow.com/questions/78686718/how-to-adapt-output-of-a-keras-model-valueerror-dimensions-must-be-equal</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78686718/how-to-adapt-output-of-a-keras-model-valueerror-dimensions-must-be-equal</guid>
      <pubDate>Sat, 29 Jun 2024 17:56:20 GMT</pubDate>
    </item>
    <item>
      <title>如何缩小不同机器学习模型的训练和测试分数之间的差距？[关闭]</title>
      <link>https://stackoverflow.com/questions/78686637/how-to-reduce-gap-between-train-and-test-scores-for-different-machine-learning-m</link>
      <description><![CDATA[我正在使用多个机器学习模型进行 AQI 预测。数据为每日格式，共有 1850 条记录。我的训练 R2 得分约为 99，测试得分约为 91。这个差距可以接受吗？如果没有，我该如何提高我的测试分数？
X = data[[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;, &#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;]]
y = data[&#39;AQI&#39;]

# 使用时间序列分割将数据拆分为训练集和测试集
tscv = TimeSeriesSplit(n_splits=2) 

for train_index, test_index in tscv.split(X):
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义参数每个模型的网格
param_grids = {
“决策树”：{&#39;max_depth&#39;：[3, 5, 7, 10]},
“随机森林”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;max_depth&#39;：[3, 5, 7, 10]},
“梯度提升”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;learning_rate&#39;：[0.01, 0.1, 0.2], &#39;max_depth&#39;：[3, 5, 7]},
“AdaBoost”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;learning_rate&#39;：[0.01, 0.1, 0.5]},
“XGBoost”: {&#39;n_estimators&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.01, 0.1, 0.2], &#39;max_depth&#39;: [3, 5, 7]},
“CatBoost”: {&#39;iterations&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.01, 0.1, 0.2], &#39;depth&#39;: [3, 5, 7]}, 0.7]},
}

# 要评估的模型列表
models = [
(“决策树”, DecisionTreeRegressor(random_state=42)),
(“随机森林”, RandomForestRegressor(random_state=42)),
(&quot;Gradient Boosting&quot;, GradientBoostingRegressor(random_state=42)),
(&quot;AdaBoost&quot;, AdaBoostRegressor(random_state=42)),
(&quot;XGBoost&quot;, XGBRegressor(random_state=42)),
(&quot;CatBoost&quot;, CatBoostRegressor(verbose=0)),
]

#用于存储模型性能和特征重要性的字典
model_performance = {}
feature_importance_dict = {}
predictions = {}

for name, model in models:
param_grid = param_grids[name]

if param_grid:
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3,scoring=&#39;neg_mean_squared_error&#39;)
grid_search.fit(X_train_scaled, y_train)
best_model = grid_search.best_estimator_
else:
best_model = model
best_model.fit(X_train_scaled, y_train)

# 计算预测
y_train_pred = best_model.predict(X_train_scaled)
y_test_pred = best_model.predict(X_test_scaled)

# 存储预测
predictions[name] = {&#39;model_name&#39;: name, &#39;y_test_pred&#39;: y_test_pred}

# 计算训练集的评估指标
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)

# 计算测试集的评估指标
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# 存储模型性能指标
model_performance[name] = {
&quot;Train_RMSE&quot;: train_rmse, 
&quot;Train_R2&quot;: train_r2, 
&quot;Train_MAE&quot;: train_mae,
&quot;Test_RMSE&quot;: test_rmse,
&quot;Test_R2&quot;: test_r2,
&quot;Test_MAE&quot;: test_mae
}

# 对于所有模型，尝试提取特征重要性
if hasattr(best_model, &#39;feature_importances_&#39;) or hasattr(best_model, &#39;coef_&#39;):
feature_importances = best_model.feature_importances_ if hasattr(best_model, &#39;feature_importances_&#39;) else best_model.coef_

# 获取特征名称
if isinstance(best_model, (LinearRegression, Ridge, Lasso)): # 对于线性模型
feature_names = [&#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;]
else: # 对于其他模型，从原始 DataFrame 获取特征名称
feature_names = [&#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;] # 将其替换为实际特征名称

# 使用特征名称存储特征重要性
feature_importance_dict[name] = {feature_names[i]: feature_importances[i] for i in range(min(len(feature_importances), len(feature_names)))}

# 将模型性能字典转换为 DataFrame
model_performance_df = pd.DataFrame.from_dict(model_performance, orient=&#39;index&#39;)

# 打印模型性能
print(model_performance_df)

我在这里减少了分割 (tscv = TimeSeriesSplit(n_splits=2) )，我的测试分数从 91 提高到了 94。我还能做什么？]]></description>
      <guid>https://stackoverflow.com/questions/78686637/how-to-reduce-gap-between-train-and-test-scores-for-different-machine-learning-m</guid>
      <pubDate>Sat, 29 Jun 2024 17:10:14 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用免费资源在本地机器上的大数据集上训练 gpt2 [关闭]</title>
      <link>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</link>
      <description><![CDATA[是否可以在 colab、jupyter 或 kaggle 上对 1.5m 个数据点进行 gpt2 训练？
到目前为止，我尝试在 colab 中进行此操作，但在标记化过程中会耗尽存储空间，这是可以理解的。我也尝试了批处理技术。后来我尝试在 kaggle 上运行相同的算法，但目前它在加载转换器时显示错误。仍在尝试运行它。我只是想知道是否可以做到这一点！]]></description>
      <guid>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</guid>
      <pubDate>Sat, 29 Jun 2024 17:07:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么循环中classification_report和precision_recall_fscore_support之间的敏感度（召回率）值不同？</title>
      <link>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</link>
      <description><![CDATA[我正在使用 sklearn.datasets 中的 make_classification 生成的合成数据集，其中包含 5 个类别。我已经在此数据上训练了一个 RandomForestClassifier，并使用两种不同的方法评估其性能。但是，我观察到这两种方法的敏感度（召回率）值存在差异。
这是我使用的代码：
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classes_report, precision_recall_fscore_support
import numpy as np
import pandas as pd

# 生成包含 5 个类别的合成数据集
X, y = make_classification(n_samples=1000, n_classes=5, n_informative=10, n_clusters_per_class=1, random_state=42)

# 分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练分类器
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = clf.predict(X_test)

# 方法 1：classification_report
print(&quot;分类报告&quot;)
print(classification_report(y_test, y_pred))

# 方法 2：使用 precision_recall_fscore_support 循环
res = []

for l in range(5):
prec, recall, _, _ = precision_recall_fscore_support(np.array(y_test) == l,
np.array(y_pred) == l,
pos_label=True, average=None)
res.append([l, recall[0], recall[1]])

df = pd.DataFrame(res, columns=[&#39;class&#39;, &#39;sensitivity&#39;, &#39;specificity&#39;])
print(&quot;\nSensitivity and Specificity&quot;)
print(df)

输出：
分类报告
准确率 召回率 f1 分数 支持率

0 0.76 0.71 0.74 35
1 0.72 0.93 0.81 30
2 0.72 0.81 0.76 32
3 0.85 0.86 0.86 59
4 0.88 0.64 0.74 44

准确率 0.79 200
宏平均值 0.78 0.79 0.78 200
加权平均值 0.80 0.79 0.79 200

敏感度和特异性
类别敏感度特异性
0 0 0.951515 0.714286
1 1 0.935294 0.933333
2 2 0.940476 0.812500
3 3 0.936170 0.864407
4 4 0.974359 0.636364

问题：
为什么 classification_report 和使用 precision_recall_fscore_support 的循环之间的敏感度（召回率）值不同？具体来说，为什么 classification_report 报告的召回率值与循环方法中计算出的敏感度值之间存在差异？如果可能，您能否用一个简单的示例（手动解决）来展示它
您尝试了什么，您期望什么？
我使用了两种方法来评估我的 RandomForestClassifier 的性能。首先，我使用 classification_report 来获取每个类别的精确度、召回率和 F1 分数。然后，我使用带有 precision_recall_fscore_support 的循环计算每个类别的敏感度和特异性。
我期望循环方法中计算出的敏感度值与 classification_report 中的召回率值相匹配，因为敏感度和召回率在分类任务中通常被视为同义词。但是，我发现两组值之间存在差异。
实际上结果如何？
classification_report 中的召回率值与循环方法中计算的敏感度值不同。classification_report 在多类上下文中为每个类提供召回率值，而循环方法将每个类视为二元分类问题，从而导致不同的敏感度和特异性值。]]></description>
      <guid>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</guid>
      <pubDate>Sat, 29 Jun 2024 14:57:35 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Android Studio 上使用可教机器分析机器学习模型（使用 Kotlin）</title>
      <link>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</link>
      <description><![CDATA[我在分析照片时遇到了问题。我有 3 种模型，分别是癌症、非癌症和未知。当我尝试分析时，结果始终是“未知” 有人知道如何解决吗？我尝试更改我的代码并询问人工智能，但仍然卡住了。
这是我的代码
ImageClassifierHelper

class ImageClassifierHelper(
private var Threshold: Float = 0.5f, // Tingkatkan Threshold
private var maxResult: Int = 3,
private val modelName: String = &quot;model_unquant.tflite&quot;,
val context: Context,
val classifierListener: ClassifierListener?
) {
interface ClassifierListener {
fun onError(error: String)
fun onResults(result: MutableList&lt;Category&gt;?)
}

private var imageClassifier: ModelUnquant? = null

init {
setupImageClassifier()
}

private fun setupImageClassifier() {
try {
imageClassifier = ModelUnquant.newInstance(context)
} catch (e: IOException) {
classifierListener?.onError(context.getString(R.string.failed))
Log.e(TAG, e.message.toString())
}
}

伴随对象 {
private const val TAG = &quot;ImageClassifierHelper&quot;
}

// 按照您的模型添加标签列表
private val labels = listOf(&quot;Non-Cancer&quot;, &quot;Cancer&quot;, &quot;Unknown&quot;)

fun classifyStaticImage(imageUri: Uri) {
if (imageClassifier == null) {
setupImageClassifier()
}

// 将 Uri 转换为 Bitmap
val bitmap = uriToBitmap(context.contentResolver, imageUri)
?: run {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}
if (bitmap == null) {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}

val resizedBitmap = Bitmap.createScaledBitmap(bitmap, 224, 224, true)

//将 Bitmap 转换为 TensorImage
val tensorImage = TensorImage(DataType.FLOAT32)
tensorImage.load(resizedBitmap)

// 为输入创建 TensorBuffer
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(tensorImage.buffer)

// 调用推理模型并计算结果
val output = imageClassifier?.process(inputFeature0)
val outputFeature0 = output?.outputFeature0AsTensorBuffer

// 将 Bitmap 转换为类别列表
val probability = outputFeature0?.let { tensorBuffer -&gt;
tensorBuffer.floatArray.mapIndexed { 索引，值 -&gt;
Category(labels.getOrElse(index) { &quot;Unknown&quot; }, value)
}.filter { it.score &gt;= Threshold } // 根据阈值过滤
.toMutableList()
}

classifierListener?.onResults(probability)
}

private fun uriToBitmap(contentResolver: ContentResolver, uri: Uri): Bitmap? {
return try {
val inputStream = contentResolver.openInputStream(uri)
BitmapFactory.decodeStream(inputStream)
} catch (e: IOException) {
e.printStackTrace()
null
}
}
}

]]></description>
      <guid>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</guid>
      <pubDate>Sat, 29 Jun 2024 12:13:59 GMT</pubDate>
    </item>
    <item>
      <title>q/kdb+ 中的机器学习</title>
      <link>https://stackoverflow.com/questions/78684235/machine-learning-in-q-kdb</link>
      <description><![CDATA[如果我在端口 5012 的 hdb 进程中存储了一个表。
我已经安装了 PyKX 并将其成功导入到终端中的 python 提示符中。
然后我连接到我的 host=‘localhost’, port=5012  并运行一个简单的查询以从 hdb 返回我的数据 q(‘{select name,price,volume,vwap from tab where date&gt;2024.01.01}’)
然后如何在 python 机器学习算法之一中使用这些数据。您如何将表数据转换为可用的 python 数据点，然后输入到您选择的模型中？您是否必须提取每列数据并保存为某种类型的变量，例如在 q 进程中运行 exec  语句？]]></description>
      <guid>https://stackoverflow.com/questions/78684235/machine-learning-in-q-kdb</guid>
      <pubDate>Fri, 28 Jun 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>绘制预测掩码的问题</title>
      <link>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</link>
      <description><![CDATA[我目前正在进行一个深度学习项目“叶病分割”。我已经训练了一个模型超过 50 个时期，并获得了以下准确度和损失指标：
训练损失：19.4736，训练准确度：0.9395
验证损失：19.6197，验证准确度：0.9100
测试损失：19.6148，测试准确度：0.9123
但是，当我绘制预测的蒙版时，它们看起来不准确。我的绘图代码有问题吗？
def plot_predictions(model, images, mask, num_samples=5):
predictions = model.predict(images[:num_samples])
for i in range(num_samples):
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.title(&#39;真实图像&#39;)
plt.imshow(images[i])
plt.subplot(1, 3, 2)
plt.title(&#39;地面真相面具&#39;)
plt.imshow(masks[i], cmap=&#39;gray&#39;) # 假设面具已经是二进制的
plt.subplot(1, 3, 3)
plt.title(&#39;预测面具&#39;)
plt.imshow(predictions[i][:, :, 0], cmap=&#39;gray&#39;) # 转换预测面具转换为二进制
plt.show()

plot_predictions(model, test_images.numpy(), test_masks_L, num_samples=5)

原始图像-蒙版-预测蒙版
请检查我的代码并帮助找出可能导致此问题的任何错误？]]></description>
      <guid>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</guid>
      <pubDate>Tue, 25 Jun 2024 21:18:52 GMT</pubDate>
    </item>
    <item>
      <title>mlflow 在记录图像时不会自动记录工件</title>
      <link>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</link>
      <description><![CDATA[我对 mlflow 还很陌生。我偶然发现了一些奇怪的行为。当我运行一个简单的 Keras 模型时，使用 MLFlow Autolog 进行拟合，如下所示：
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test,y_test))
mlflow.end_run()

这会产生预期的行为。我可以在工件和指标中看到模型。

但是，当我使用包含训练准确率和损失的自定义图形进行自动记录时，工件仅包含自定义图像。好像自动记录根本不起作用。
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))

##_________ 有问题的部分
fig, ax = plt.subplots(1,2,figsize=(10,4))
ax[0].plot(history.history[&#39;accuracy&#39;], label=&#39;Accuracy&#39; )
ax[0].plot(history.history[&#39;val_accuracy&#39;], label=&#39;Val Accuracy&#39; )
ax[0].set_title(&#39;Accuracy&#39;)
ax[0].legend(loc=&#39;best&#39;)
ax[1].plot(history.history[&#39;loss&#39;], label=&#39;Loss&#39; )
ax[1].plot(history.history[&#39;val_loss&#39;], label=&#39;Val Loss&#39; )
ax[1].set_title(&#39;Loss&#39;)
ax[1].legend(loc=&#39;best&#39;)
mlflow.log_figure(fig,&#39;training_history.png&#39;)
# _________

mlflow.end_run()

模型工件不存在。指标也没有记录。
我是否遗漏了一些简单的东西？

请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</guid>
      <pubDate>Mon, 24 Jun 2024 17:12:13 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PyTorch 的英特尔扩展指定 bfloat16 混合精度？</title>
      <link>https://stackoverflow.com/questions/74105061/how-do-you-specify-the-bfloat16-mixed-precision-with-the-intel-extension-for-pyt</link>
      <description><![CDATA[我想知道如何在 PyTorch 和英特尔 PyTorch 扩展中使用混合精度。
我尝试查看其 GitHub 上的文档，但找不到任何指定如何从 fp32 转到 blfoat16 的内容。]]></description>
      <guid>https://stackoverflow.com/questions/74105061/how-do-you-specify-the-bfloat16-mixed-precision-with-the-intel-extension-for-pyt</guid>
      <pubDate>Tue, 18 Oct 2022 02:31:55 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 tensorflow 2 中使用 bfloat16 时缺少节点的第 0 个输出...</title>
      <link>https://stackoverflow.com/questions/70004080/missing-0th-output-from-node-when-trying-to-use-bfloat16-in-tensorflow-2</link>
      <description><![CDATA[因此，我尝试将现有项目转换为使用 bfloat16，因为这可以让代码在张量核心上运行。我使用的是 mixed_precision.set_global_policy(&#39;mixed_bfloat16&#39;)，根据 keras 文档，这已经足够了，应该可以正常工作。但是，使用此行时，我收到以下错误：
发现 2 个根错误。
(0) 内部：缺少来自节点 model/sequential/conv2d_transpose/conv2d_transpose 的第 0 个输出
（定义在 C:\devlibs\Python\lib\site-packages\keras\backend.py:5530）

[[model/sequential/conv2d_transpose_4/BiasAdd/_76]]
(1) 内部：缺少来自节点 model/sequential/conv2d_transpose/conv2d_transpose 的第 0 个输出
（定义在 C:\devlibs\Python\lib\site-packages\keras\backend.py:5530）

0 个成功操作。
忽略 0 个派生错误。[Op:__inference_predict_function_1627]

错误可能源自输入操作。
连接到节点 model/sequential/conv2d_transpose/conv2d_transpose 的输入源操作：
In[0] model/sequential/conv2d_transpose/stack（定义在 C:\devlibs\Python\lib\site-packages\keras\layers\convolutional.py:1333）

In[1] model/sequential/conv2d_transpose/conv2d_transpose/Cast（定义在 C:\devlibs\Python\lib\site-packages\keras\mixed_precision\autocast_variable.py:146）

In[2] model/sequential/reshape/Reshape（定义在 C:\devlibs\Python\lib\site-packages\keras\layers\core\reshape.py:126）

此错误在训练期间在第一层抛出，但如果我强制该层使用 float32，则错误会传递到下一层（基本上所有层都是与 bfloat16 不兼容）。可能值得指出的是，我的输入数据是 uint8。]]></description>
      <guid>https://stackoverflow.com/questions/70004080/missing-0th-output-from-node-when-trying-to-use-bfloat16-in-tensorflow-2</guid>
      <pubDate>Wed, 17 Nov 2021 12:04:49 GMT</pubDate>
    </item>
    </channel>
</rss>