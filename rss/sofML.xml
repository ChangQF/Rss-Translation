<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 10 Mar 2025 06:24:00 GMT</lastBuildDate>
    <item>
      <title>错误分类的样本是单独的类[关闭]</title>
      <link>https://stackoverflow.com/questions/79496368/misclassified-samples-as-a-separate-class</link>
      <description><![CDATA[我一直在尝试将一组数据分类用于二进制分类。我看到数据的一个子集被错误分类，我删除了该子集，分类器的准确性从88％提高到96％。
我正在考虑为这些样本创建一个单独的类，然后与多类分类进行分类。这是合适的吗？
还有其他处理这些样品的方法吗？这些样本现在由二进制分类器随机分配给两个类。至少几次，我将决策树重现。]]></description>
      <guid>https://stackoverflow.com/questions/79496368/misclassified-samples-as-a-separate-class</guid>
      <pubDate>Sun, 09 Mar 2025 18:06:59 GMT</pubDate>
    </item>
    <item>
      <title>我在形状对齐中遇到了错误，该如何解决？</title>
      <link>https://stackoverflow.com/questions/79496192/i-am-having-a-error-with-shape-alignment-how-can-i-solve-this</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79496192/i-am-having-a-error-with-shape-alignment-how-can-i-solve-this</guid>
      <pubDate>Sun, 09 Mar 2025 16:02:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在虚幻引擎5中加入libtorch？</title>
      <link>https://stackoverflow.com/questions/79496110/how-to-include-libtorch-in-unreal-engine-5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79496110/how-to-include-libtorch-in-unreal-engine-5</guid>
      <pubDate>Sun, 09 Mar 2025 15:03:36 GMT</pubDate>
    </item>
    <item>
      <title>TPU上的RESNET50计算NAN的准确性和损失，在Google Colab中的CPU上正常工作</title>
      <link>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</link>
      <description><![CDATA[我使用Google Colab在V2-8 TPU加速器上培训了RESNET50，我已经用5000张形状（224、224、3）喂了它。我已经对它们进行了规范，没有NAN，没有INF，没有阶级失衡，一切都还好：
  input_shape =（224，224，3）

使用Strategy.scope（）：
    base_model = resnet50（weights =&#39;imagenet&#39;，include_top = false，input_shape = input_shape）
    base_model.trainable = false
    型号= tf.keras.models.sequeential（[[
        base_model，
        tf.keras.layers.globalaveragepooling2d（），
        tf.keras.layers.dense（1024，activation =&#39;relu&#39;），
        tf.keras.layers.dense（6，activation =&#39;sigmoid&#39;） 
    ）））
    model.compile（优化器=&#39;adam&#39;， 
                  损失=&#39;binary_crossentropy&#39;，
                  指标= [&#39;准确性&#39;]）
    
型号
    x_train， 
    y_train， 
    时代= 10， 
    验证_data =（x_val，y_val），
    batch_size = 32
  ）
 
当我对TPU进行训练时，训练期间的准确性和损失成为NAN。当我切换到CPU时，一切正常。
为什么会发生这种情况以及如何解决？
我尝试在Google Colab中对TPU和CPU进行培训。我希望该模型在没有任何问题的情况下进行培训，包括NAN值损失或准确性，尤其是因为培训在CPU上效果很好。但是，当使用TPU时，我遇到了NAN值的准确性和损失。我还验证了数据很干净，没有NAN，无限或失衡问题，并确保模型编译和培训设置是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</guid>
      <pubDate>Sun, 09 Mar 2025 07:07:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用训练有素的AI模型进行预测？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79495434/how-do-i-use-my-trained-ai-model-to-make-predictions</link>
      <description><![CDATA[我制作了用于多类分类的AI模型，它运行良好，但我不知道如何使用训练有素的模型做出单个预测或更多的预测。
我知道它可能在下面的代码中，但我无法弄清楚。
  n_epochs = 200
batch_size = 5
batches_per_epoch = len（x_train）// batch_size

best_acc = 0
best_weights =无
train_loss_hist = []
train_acc_hist = []
test_loss_hist = []
test_acc_hist = []

＃训练循环
对于范围（n_epochs）的时期：

    epoch_loss = []
    epoch_acc = []
    model.train（）
    使用tqdm.trange（batches_per_epoch，unit =; batch; mininterval = 0）作为bar：
        bar.set_description（f＆quot; epoch {epoch}＆quot;）
        因为我在酒吧：
        ＃批量
           start = i * batch_size
           x_batch = x_train [start：start+batch_size]
           y_batch = y_train [start：start+batch_size]
           ＃正向通行证
           y_pred =模型（x_batch）
           损失= loss_fn（y_pred，y_batch）
           ＃向后传球
           优化器.zero_grad（）
           loss.backward（）
           ＃更新权重
           优化器.step（）
           ＃计算和存储指标
           acc =（torch.argmax（y_pred，1）== torch.argmax（y_batch，1））。float（）。平均（）。
           epoch_loss.append（float（loss））
           epoch_acc.append（float（acc））
           bar.set_postfix（
            损失= float（损失），
            ACC = float（ACC）
           ）
＃在评估模式下设置模型并通过测试集运行
model.eval（）
y_pred =模型（x_test）
ce = loss_fn（y_pred，y_test）
acc =（torch.argmax（y_pred，1）== torch.argmax（y_test，1））。float（）。平均（）。
ce = float（CE）
ACC = float（ACC）
＃节省最佳模型
train_loss_hist.append（np.mean（epoch_loss）））
train_acc_hist.append（np.mean（epoch_acc）））
test_loss_hist.append（CE）
test_acc_hist.append（ACC）
如果acc＆gt; best_acc：
    best_acc = acc
    best_weights = copy.deepcopy（model.state_dict（））
print（f＆quot; epoch {epoch}验证：cross-entropy = {ce：.2f}，准确度= {acc*100：.1f}％＆quort;）
 
我使用pytorch构建模型。这是我的模型：
 类多类（nn.module）：
    def __init __（自我）：
        super（）.__ init __（）
        self.hidden = nn.linear（4，8）
        self.act = nn.relu（）
        self.output = nn.linear（8，3）

    def向前（self，x）：
        x = self.act（self.hidden（x））
        x = self.Output（x）
        返回x
 ]]></description>
      <guid>https://stackoverflow.com/questions/79495434/how-do-i-use-my-trained-ai-model-to-make-predictions</guid>
      <pubDate>Sun, 09 Mar 2025 05:02:34 GMT</pubDate>
    </item>
    <item>
      <title>导入pytorch时：“错误加载torch_python.dll或其依赖项之一”</title>
      <link>https://stackoverflow.com/questions/79495211/oserror-when-importing-pytorch-error-loading-torch-python-dll-or-one-of-its-de</link>
      <description><![CDATA[我使用 pip安装火炬安装了pytorch，但是当我尝试导入它时，我会收到以下错误：
  Oserror Trackback（最近的最新通话）  
[1]中的单元，第1行  
----＆gt; 1进口火炬  
      2印刷（Torch .__版本__）  
      3印刷（Torch.version.cuda） 

file＆quort&#39;\。venv \ lib \ site-packages \ torch \ __ init __. py＆quort&#39;,，第274行  
    270增加错误  
    272 kernel32.seterrormode（prev_error_mode）  
 - ＆gt; 274 _load_dll_libraries（）  
    275 del _load_dll_libraries


file＆quot&#39;\。venv \ lib \ site-packages \ torch \ __ init __. py＆quort; py＆quort in _load_dll_libraries in _load_dll_libraries  
    253 err = ctypes.winerror（last_error）  
    254 err.Strerror +=（  
    255 f&#39;错误加载“ {dll}”＆quot;或它的依赖之一。  
    256）  
 - ＆gt; 257增加错误  
    258 Elif Res并非没有：  
    259 is_loaded = true 

Oserror：[Winerror 127]找不到指定的程序。错误加载; \。venv \ lib \ site-packages \ torch \ lib \ torch_python.dll;或它的依赖之一。
 
 我的系统配置：&lt; /strong&gt; 
OS：Windows 11 
Python版本：3.11（pyenv）
Pytorch版本：2.6.0 
CUDA工具包已安装：11.8 
使用的安装命令：PIP安装火炬
虚拟环境：是（VENV）
 尝试了不同的pytorch安装 
使用官方Pytorch的推荐命令安装
网站：
  pip安装火炬火炬火炬 -  index-url https://download.pytorch.org/whl/cu118
 
  oserror：[winerror 127 winerror 127]在那儿无法找到 conda conda conda，但无法在那里找到pera。]]></description>
      <guid>https://stackoverflow.com/questions/79495211/oserror-when-importing-pytorch-error-loading-torch-python-dll-or-one-of-its-de</guid>
      <pubDate>Sun, 09 Mar 2025 00:03:32 GMT</pubDate>
    </item>
    <item>
      <title>二进制分类：为什么使用+1/0作为标签， +1/-1甚至 +100/-100之间有什么区别</title>
      <link>https://stackoverflow.com/questions/74752325/binary-classification-why-use-1-0-as-label-whats-the-difference-between-1-1</link>
      <description><![CDATA[在二进制分类问题中，我们通常将+1用于正标，为负标签0。  这是为什么？特别是为什么使用0而不是-1作为负面标签？
使用-1用于负标签，甚至更普遍，我们可以将+100用于正标签，而-100可以用于负标签？？]]></description>
      <guid>https://stackoverflow.com/questions/74752325/binary-classification-why-use-1-0-as-label-whats-the-difference-between-1-1</guid>
      <pubDate>Sat, 10 Dec 2022 11:05:01 GMT</pubDate>
    </item>
    <item>
      <title>重新启动运行时，模型负载获得不同的结果</title>
      <link>https://stackoverflow.com/questions/68116676/model-load-get-different-result-after-restart-runtime</link>
      <description><![CDATA[我已经使用Google Colab编写了Resnet50型号。训练我的模型后，然后在不重新启动运行时间的情况下保存模型后，将获得相同的结果，但是在重新启动运行时Google Colab并运行 Xtrain，Ytest，Ytest，X_Val，Y_VAL，Y_VAL 然后再次加载模型，我将获得不同的结果。
这是我的代码：
  #hyperParameter和回调
batch_size = 128
num_epochs = 120
input_shape =（48、48、1）
num_classes = 7

#compile模型。
从keras.optimizer进口亚当，sgd
model = resnet50（input_shape =（48、48、1），类= 7）
优化器= SGD（Learning_rate = 0.0005）
model.compile（优化器=优化器，lose =&#39;Sparse_categorical_crossentropy&#39;，metrics = [&#39;准确性&#39;]）

model.summary（）
历史= model.fit（
    data_generator.flow（Xtrain，ytrain，），
    step_per_epoch = len（xtrain） / batch_size，
    epochs = num_epochs， 
    详细= 1，
    验证_data =（x_val，y_val））

导入matplotlib.pyplot作为PLT 
model.save（&#39;fix_model_resnet50editsgd5st.h5&#39;）

#plot图
准确性=历史[&#39;精确度&#39;]
val_accuracy =历史[&#39;val_accuracy&#39;]
损失=历史[&#39;损失&#39;]
val_loss =历史[&#39;val_loss&#39;]
num_epochs = range（len（准确性））
plt.plot（num_epochs，准确性，&#39;r&#39;，label =&#39;triending acc&#39;）
plt.plot（num_epochs，val_accuracy，&#39;b&#39;，label =&#39;验证acc&#39;）
plt.title（“训练和验证精度”）
plt.ylabel（“准确性”）  
plt.xlabel（&#39;epoch&#39;）
plt.legend（）
plt. -figure（）
plt.plot（num_epochs，损失，&#39;r&#39;，label =“训练损失”）
plt.plot（num_epochs，val_loss，&#39;b&#39;，label =&#39;验证损失&#39;）
plt.title（“培训和验证损失”）
plt.ylabel（“损失”）  
plt.xlabel（&#39;epoch&#39;）
plt.legend（）
plt.show（）

#load模型
来自keras.models import load_model
model_load = load_model（&#39;fix_model_resnet50editsgd5st.h5&#39;）

model_load.summary（）


testdatamodel = model_load.evaluate（Xtest，ytest） 
打印（“测试损失” + str（testdatamodel [0]））
打印（“测试ACC：＆quot” + str（testdatamodel [1]））

traindata = model_load.evaluate（xtrain，ytrain） 
打印（“测试损失” + str（traindata [0]））
打印（“测试acc：＆quot” + str（traindata [1]））

valdata = model_load.evaluate（x_val，y_val） 
打印（“测试损失” + str（valdata [0]））
打印（“测试acc：＆quot” + str（valdata [1]））
 
  - 训练和保存模型后，然后运行加载模型，而无需重新启动运行时Google Colab：
如您所见
测试损失：0.9411-精度：0.6514 
火车损失：0.7796-准确性：0.7091 
  modelevaluateTestest＆amp;火车 
在重新启动运行时Colab之后再次运行加载模型：
测试损失：0.7928-准确性：0.6999 
火车损失：0.8189-准确性：0.6965 
 重新启动运行时评估测试和训练 ]]></description>
      <guid>https://stackoverflow.com/questions/68116676/model-load-get-different-result-after-restart-runtime</guid>
      <pubDate>Thu, 24 Jun 2021 13:25:12 GMT</pubDate>
    </item>
    <item>
      <title>无法获得我的线性回归的准确分数</title>
      <link>https://stackoverflow.com/questions/45627784/unable-to-obtain-accuracy-score-for-my-linear-regression</link>
      <description><![CDATA[我正在基于IMDB数据进行回归模型，以预测IMDB值。在线性回归上，我无法获得准确的得分。
我的代码行：
 量表
 
错误：
  valueerror：不支持连续
 
如果我要更改该线以获得R2分数，
  Metrics.R2_Score（test_y，linear_predicated_rating）
 
我能够获得R2而没有任何错误。
我为什么看到这个？
注意： test_y 是pandas dataframe]]></description>
      <guid>https://stackoverflow.com/questions/45627784/unable-to-obtain-accuracy-score-for-my-linear-regression</guid>
      <pubDate>Fri, 11 Aug 2017 05:46:30 GMT</pubDate>
    </item>
    <item>
      <title>决策树与逻辑回归相结合[关闭]</title>
      <link>https://stackoverflow.com/questions/41692017/decision-trees-combined-with-logistic-regression</link>
      <description><![CDATA[基本上我的问题与以下论文有关（仅读取节 1. Introwuction ，的开始 3.预测模型结构   3.1决策树特征变换，其他一切都可以跳过））
  https://pdfs.semanticscholar.org/daf9/ed5dc6c6bad5367d7d7fd8561527da30e9b8dd.pdf.pdf  
本文表明，与仅使用决策树或线性分类（不是两者）相比，二进制分类可以表现出更好的性能（例如，逻辑回归）。
简单地说，诀窍是我们有几个决策树（假设2棵树以简单起见，第一棵树带有3个叶子节点和带有2个叶子节点的第二棵树）和一些实现的特征矢量 x ，这是所有决策树的输入 
 so，

如果第一棵树的决定是叶节点1 ，第二个树的决定是叶节点2 ，那么线性分类器将接收二进制字符串 [1 0 0 0 0 0 1]  
如果第一棵树的决定是叶节点2 ，第二个树的决定是叶节点1 ，则线性分类器将接收二进制字符串 [0 1 0 1 0 1 0]  

等等
If we used only decision trees (without linear classification), clearly we would have either class 100/ class 010/class 001 for 1st tree and class 10/ class 01 for 2nd tree, but in this scheme the outputs of trees are combined into binary string which is fed to linear classifier.因此，尚不清楚如何训练这些决策树吗？我们拥有的是上述向量 x ，然后单击/无单击，这是线性分类的输出，而不是树
有什么想法？]]></description>
      <guid>https://stackoverflow.com/questions/41692017/decision-trees-combined-with-logistic-regression</guid>
      <pubDate>Tue, 17 Jan 2017 08:16:17 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn.decomposition.pca的特征向量的简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我正在尝试了解主组件分析的工作方式，并且我正在 sklearn.datasets.load_iris  dataset上对其进行测试。  我了解每个步骤的工作原理（例如，使用 k 选定的尺寸，将数据，协方差，特征分类，对最高特征值进行排序，将原始数据转换为新轴）。）。
下一步是可视化这些 eigenVectors 在数据集中投射到（在 pc1 vs. pc2 vs. pc2 plot 上，对吗？）。。
如何在还原尺寸数据集的3D图上绘制[PC1，PC2，PC3]特征向量？
另外，我是否正确绘制了此2D版本？我不确定为什么我的第一个特征向量的长度较短。  我应该乘以特征值吗？

 这是我为实现这一目标所做的一些研究： 
我关注的PCA方法来自：
 https://plot.ly/ipython-notebooks/principal-component-analysis/#shortcut--pca-in-scikit-learn （尽管我不想使用 Plotly 我想坚持使用。
我一直在关注本教程，以绘制特征向量，这似乎很简单： pca for matplotlib 基本示例
我找到了这个，但是我试图做的事情似乎过于复杂，我不想创建一个 fancyarrowpatch ： 的协方差矩阵

 我试图使我的代码尽可能直接地遵循其他教程： 
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT
来自sklearn.datasets import load_iris
从sklearn.prepercorsing进口标准标准
来自Sklearn进口分解
进口海洋为SNS； sns.set_style（&#39;whitegrid＆quort; {&#39;axes.grid&#39;：false}）

％matplotlib内联
np.random.seed（0）

＃虹膜数据集
df_data = pd.dataframe（load_iris（）。数据， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]，
                       列= load_iris（）。feature_names）

se_targets = pd.Series（load_iris（）。目标， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]， 
                       名称=“物种”

＃缩放平均= 0，var = 1
df_standard = pd.dataframe（standardscaler（）。fit_transform（df_data）， 
                           index = df_data.index，
                           列= df_data.columns）

＃用于主要组合分析的Sklearn

＃昏暗
m = df_standard.shape [1]
k = 2

＃PCA（我倾向于设置它）
m_pca = demomposition.pca（n_components = m）
df_pca = pd.dataframe（m_pca.fit_transform（df_standard）， 
                列= [＆quot; pc％d＆quot;范围内K的％k（1，m + 1）]）。iloc [：，：k]


＃绘制特征向量
#https：//stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

＃这是东西很奇怪的地方...
data = df_standard

mu = data.mean（轴= 0）
eigenVector，eigenvalues = m_pca.components_，m_pca.explained_variance_ #eigenVectors，eigenvalues，v = np.linalg.svd（data.t，fult_matrices = false）
Projected_data = df_pca＃np.dot（数据，特征向量）

sigma = Projected_data.std（axis = 0）.mean（）

图，ax = plt.subplots（figsize =（10,10））
ax.Scatter（Projected_data [＆quot; pc1;]，Projected_data [＆quot; pc2＆quot;]）
对于轴，zip中的颜色（eigenVectors [：k]，[red&#39;&#39;
＃开始，end = mu，mu + sigma *轴###导致“ valueerror：太多值无法打开（预期2）”

    ＃所以我尝试过，但我认为这是不对的
    start，end =（mu）[：k]，（Mu + Sigma * Axis）[：K] 
    ax.annotate（&#39;&#39;，xy = end，xytext = start，arrowprops = dict（faceColor = color，width = 1.0））
    
ax.set_aspect（&#39;quare&#39;）
plt.show（）
 
  &lt;img alt =“在此处输入图像描述” src =“ https://i.sstatic.net.net/t9st0.png”]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归和软磁回归之间的差异</title>
      <link>https://stackoverflow.com/questions/36051506/difference-between-logistic-regression-and-softmax-regression</link>
      <description><![CDATA[我知道逻辑回归是用于二进制分类和用于多类问题的软磁回归。如果我使用相同的数据训练多个逻辑回归模型并将其结果归一化以获取多级分类器而不是使用一个SoftMax模型，将会有任何差异。我认为结果是相同的。我可以说：“所有多级分类器都是二进制分类器的级联结果”。 （神经元网络除外）]]></description>
      <guid>https://stackoverflow.com/questions/36051506/difference-between-logistic-regression-and-softmax-regression</guid>
      <pubDate>Thu, 17 Mar 2016 04:08:48 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn Lasso回归是否比山脊回归差？</title>
      <link>https://stackoverflow.com/questions/35714772/sklearn-lasso-regression-is-orders-of-magnitude-worse-than-ridge-regression</link>
      <description><![CDATA[我目前使用 sklearn.linear_model 模块实现了脊和拉索回归。
然而，套索回归似乎在同一数据集上差3个数量级！
我不确定怎么了，因为从数学上讲，这不应该发生。这是我的代码：
  def ridge_regression（x_train，y_train，x_test，y_test，model_alpha）：
    clf = linear_model.ridge（model_alpha）
    clf.fit（x_train，y_train）
    预测= clf.predict（x_test）
    损失= np.sum（（预测-y_test）** 2）
    回报损失

def lasso_regression（x_train，y_train，x_test，y_test，model_alpha）：
    clf = linear_model.lasso（model_alpha）
    clf.fit（x_train，y_train）
    预测= clf.predict（x_test）
    损失= np.sum（（预测-y_test）** 2）
    回报损失


x_train，x_test，y_train，y_test = cross_validation.train_test_split（x，x，y，test_size = 0.1，randy_state = 0）
对于alpha，在[0，0.01，0.1，0.5，1，2，5，10，100，1000，10000]中：
    打印（alpha =&#39; + str（alpha） +;
    
对于[1、1.25、1.5、1.75、5、5、10、100、1000、10000、100000、1000000]中的alpha。
    打印（alpha =&#39; + str（alpha） +;
 
这是我的输出：
  alpha = 0：20575.7121727的拉索损失
alpha的套索损失= 0.01：19762.8763969
alpha的套索损失= 0.1：17656.9926418
alpha的拉索损失= 0.5：15699.2014387
alpha的拉索损失= 1：15619.9772649
alpha的拉索损失= 2：15490.0433166
alpha的拉索损失= 5：15328.4303197
alpha的拉索损失= 10：15328.4303197
alpha的拉索损失= 100：15328.4303197
alpha的套索损失= 1000：15328.4303197
alpha的拉索损失= 10000：15328.4303197
Alpha的脊损失= 1：61.6235890425
alpha = 1.25：61.6360790934
alpha的脊损失= 1.5：61.6496312133
alpha = 1.75：61.6636076713
alpha的脊损失= 2：61.6776331539
alpha的脊损失= 5：61.8206621527
Alpha的脊损失= 10：61.9883144732
alpha的脊损失= 100：63.9106882674
alpha = 1000：69.3266510866
alpha = 10000：82.0056669678
alpha的脊损失= 100000：88.4479064159
alpha = 1000000：91.7235727543
 
任何想法为什么？]]></description>
      <guid>https://stackoverflow.com/questions/35714772/sklearn-lasso-regression-is-orders-of-magnitude-worse-than-ridge-regression</guid>
      <pubDate>Tue, 01 Mar 2016 04:36:23 GMT</pubDate>
    </item>
    <item>
      <title>用Scikit-Learn进行象征性文字</title>
      <link>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</link>
      <description><![CDATA[我有以下代码从一组文件（文件夹名称是类别名称）中提取文本分类的代码。
 导入sklearn.datasets
来自sklearn.feature_extraction.text导入tfidfvectorizer

train = sklearn.datasets.datasets.load_files（&#39;./ train&#39;，description = none，类别= none，load_content = true，shuffle = true，encoding = none，decode_error =&#39;strict&#39;
打印Len（train.data）
打印火车。target_names

vectorizer = tfidfvectorizer（）
x_train = vectorizer.fit_transform（train.data）
 
它抛出以下堆栈跟踪：
  trackback（最近的最新通话）：
  文件“ C：\ eclipseworkspace \ textClassifier \ main.py”，第16行，in＆lt; module＆gt;
    x_train = vectorizer.fit_transform（train.data）
  文件“ c：\ python27 \ lib \ lib \ site-packages \ sklearn \ feature_extraction \ text.py”，第1285行，在fit_transform中
    x = super（tfidfvectorizer，self）.fit_transform（raw_documents）
  文件“ c：\ python27 \ lib \ lib \ site-packages \ sklearn \ feature_extraction \ text.py”，第804行，在fit_transform中
    self.fixed_vocabulary_）
  文件“ c：\ python27 \ lib \ site-packages \ sklearn \ sklearn \ feature_extraction \ text.py”，第739行，in _count_vocab
    对于分析中的功能（DOC）：
  文件“ c：\ python27 \ lib \ site-packages \ sklearn \ feature_extraction \ text.py”，第236行，in＆lt; lambda＆gt;
    tokenize（preprocess（self.decode（doc））），stop_words）
  文件“ C：\ Python27 \ lib \ site-packages \ sklearn \ sklearn \ feature_extraction \ text.py”，第113行，在解码中
    doc = doc.decode（self.soding，self.decode_error）
  文件“ C：\ Python27 \ lib \ cododings \ utf_8.py”，第16行，在解码中
    返回codecs.utf_8_decode（输入，错误，true）
UNICODEDEDECODEERROR：&#39;utf8&#39;编解码器无法在位置32054中解码字节0xff：无效启动字节
 
我运行Python 2.7。我该如何工作？
 编辑：
我刚刚发现，这对于具有 utf-8 编码的文件非常有效（我的文件是 ansi 编码）。有什么办法可以获得 sklearn.datasets.load_files（）与 ansi 编码？]]></description>
      <guid>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</guid>
      <pubDate>Fri, 01 May 2015 00:39:09 GMT</pubDate>
    </item>
    <item>
      <title>Python中的文本分类 - （基于NLTK句子）</title>
      <link>https://stackoverflow.com/questions/23178275/text-classification-in-python-nltk-sentence-based</link>
      <description><![CDATA[我需要对文本进行分类，并且我正在使用文本blob python模块来实现它。我可以使用幼稚的贝叶斯分类器/决策树。我担心以下提到的要点。

 我需要分类句子为参数/而不是参数。我正在使用两个分类器，并使用APT数据集​​训练模型。我的问题是我只需要用关键字训练模型吗？或者我可以用所有可能的参数训练数据集和非参数示例句子？从文本分类准确性和检索时间方面，哪种方法是最好的方法？

 由于分类要么是参数/而不是参数，因此哪个分类器将获得确切的结果？这是天真的贝叶斯/决策树/正幼稚的贝叶斯？

]]></description>
      <guid>https://stackoverflow.com/questions/23178275/text-classification-in-python-nltk-sentence-based</guid>
      <pubDate>Sun, 20 Apr 2014 04:01:27 GMT</pubDate>
    </item>
    </channel>
</rss>