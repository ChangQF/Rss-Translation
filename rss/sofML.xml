<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 12:25:19 GMT</lastBuildDate>
    <item>
      <title>将自定义损失函数移至 GPU</title>
      <link>https://stackoverflow.com/questions/77946691/moving-a-custom-loss-function-to-gpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77946691/moving-a-custom-loss-function-to-gpu</guid>
      <pubDate>Tue, 06 Feb 2024 09:45:05 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-coverts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-coverts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>关联矩阵热图中的问题[重复]</title>
      <link>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</link>
      <description><![CDATA[
我在热图中面临这个问题，它只显示热图第一行中的值..
我写了下面的代码
导入seaborn作为sns
    将 matplotlib.pyplot 导入为 plt
    将 pandas 导入为 pd

    # 选择数字列
    numeric_columns = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns

    # 计算数字列的相关矩阵
    相关矩阵 = df[numeric_columns].corr()

    # 添加列名作为相关矩阵的第二行
    相关矩阵.列 = 相关矩阵.列.值
    相关矩阵.索引 = 相关矩阵.列.值

    # 设置 matplotlib 图形
    plt.figure(figsize=(12, 10))

    # 绘制正确显示值的热图
    sns.heatmap（correlation_matrix，annot=True，cmap=&#39;coolwarm&#39;，fmt=“.2f”，linewidths=.5）

    # 调整布局以获得更好的可视化效果
    plt.title(“相关矩阵”)
    plt.show()

我需要有人能帮我解决这个问题..]]></description>
      <guid>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</guid>
      <pubDate>Tue, 06 Feb 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv5/SparseML - 无法在给定配方中找到任何修饰符</title>
      <link>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</link>
      <description><![CDATA[我正在尝试使用 SparseML 训练 YOLOv5s 模型。 （我不知道这是否重要，但我正在 Google Colab 中进行培训）。当我运行 train.py 时，出现以下错误：
ValueError：无法在给定配方中找到任何修饰符。修饰符必须在 yaml 键下以列表形式列出，名称中包含“修饰符”。这些键和列表也可以嵌套在用于分阶段食谱的额外键下。

这是我的recipe.yaml：
&lt;前&gt;&lt;代码&gt;---
布局：空
标题：ETS2 车辆检测数据集配方
---

修饰符：
    - !EpochRangeModifier
        开始纪元：0.0
        结束纪元：250.0

    - !SetLearningRateModifier
        开始纪元：5.0
        学习率：0.1

    - !LearningRateModifier
        开始纪元：0.0
        纪元结束：25.0
        lr_class：多步LR
        lr_kwargs：
            伽玛：0.9
            里程碑：[2.0、5.5、10.0]
        初始化lr：0.1

    - !GMPruningModifier
        开始纪元：50.0
        结束纪元：100.0
        更新频率：1.0
        初始化稀疏度：0.05
        最终稀疏度：0.65
        参数：[&#39;blocks.1.conv&#39;]
    
    - !QuantizationModifier
        开始纪元：100.0

    - !TrainableParamsModifier
        参数：[&#39;blocks.1.conv&#39;]

    - !SetWeightDecayModifier
        开始纪元：5.0
        重量衰减：0.0
    
    - !ConstantPruningModifier
        参数：[&#39;blocks.1.conv]

我做错了什么，如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</guid>
      <pubDate>Tue, 06 Feb 2024 01:41:58 GMT</pubDate>
    </item>
    <item>
      <title>如何塑造二元分类器模型以适应我的输入数据？</title>
      <link>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</link>
      <description><![CDATA[这是我的代码。
from pandas import read_csv
从 keras.preprocessing.image 导入 ImageDataGenerator
将张量流导入为 tf
从tensorflow.keras.models导入模型，顺序
从tensorflow.keras.layers导入Flatten、Dense、Conv2D


train_df = read_csv(“输出/train.csv”)
valid_df = read_csv(“输出/validate.csv”)

train_images = ImageDataGenerator(重新缩放=1./255)
train_generator = train_images.flow_from_dataframe(train_df, x_col=“file_path”, y_col=“on_off_str”,
                                                   class_mode=&#39;二进制&#39;，batch_size=8)

validate_images = ImageDataGenerator（重新缩放=1./255）
validate_generator = validate_images.flow_from_dataframe(valid_df, x_col=“file_path”, y_col=“on_off_str”,
                                                          class_mode=&#39;二进制&#39;，batch_size=8)


模型=顺序（[压平（input_shape =（32,32,3）），
                   密集（128，激活=tf.nn.relu），
                   密集（1，激活=tf.nn.sigmoid）]）

模型.summary()

model.compile(optimizer=tf.optimizers.Adam(),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

历史= model.fit（train_generator，steps_per_epoch = 8，epochs = 15，verbose = 1，
                    验证数据=验证生成器、验证步骤=8）

我正在尝试使用 32x32x3 图像的数据集创建一个简单的二元分类器。
我收到错误：
矩阵大小不兼容：In[0]: [8,196608]，In[1]: [24576,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]

解释器挂起。
如果我将批量大小更改为 1 以使大小匹配
我收到类似的错误，但解释器不再挂起。
矩阵大小不兼容：In[0]: [1,196608]，In[1]: [3072,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]
2024-02-05 19:54:54.132353: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。
         [[{{节点 PyFunc}}]]

如何更改输入或模型的形状？我的图像是 32x32x3。]]></description>
      <guid>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</guid>
      <pubDate>Tue, 06 Feb 2024 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>我尝试安装Installing TensorFlow, CUDA, cuDNN with Anaconda for GeForce GTX 1050 [关闭]</title>
      <link>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</link>
      <description><![CDATA[我尝试为 GeForce GTX 1050 安装“使用 Anaconda 安装 TensorFlow、CUDA、cuDNN”。
我使用了以下提到的文章：https://medium.com/@shaikhmuhammad/installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce-gtx-1050-ti-79c1eb94eb7a
我与文章中遵循的步骤的唯一区别是：

我有 nvidia 1050 而不是 1050ti
我在 c/users/lokes 中设置了我的环境，而不是在桌面上

我收到了这个错误，我已将其附加在屏幕截图中，但我无法找到解决该问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</guid>
      <pubDate>Tue, 06 Feb 2024 00:08:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LASSO 模型中获得 coef_ 中各个列的稀疏性？</title>
      <link>https://stackoverflow.com/questions/77944525/how-to-get-sparsity-in-individual-columns-in-coef-in-a-lasso-model</link>
      <description><![CDATA[所以我想在训练 LASSO 模型后获得稀疏系数，但我希望在每列系数上保持稀疏性。
假设在使用 5 个输入维度和 3 个输出维度训练 sklearn.linear_model.Lasso 后，我得到以下系数 _ ：
数组([[ 1.87666825 , 0. , -1.37516633, 0.57491936, -0.65120491],
       [0.，-0.5901364，-0.85214947，0.03498916，-0.81805396]，
       [-0.89515717、-0.04372684、-0.22986802、0.、-0.56785277]])

是否可以在特定列上获得稀疏性？例如：
&lt;前&gt;&lt;代码&gt;
数组([[ 0., 1.87666825, -1.37516633, 0.57491936, -0.65120491],
       [0.，-0.5901364，-0.85214947，0.03498916，-0.81805396]，
       [ 0.、-0.04372684、-0.22986802、-0.89515717、-0.56785277]])
]]></description>
      <guid>https://stackoverflow.com/questions/77944525/how-to-get-sparsity-in-individual-columns-in-coef-in-a-lasso-model</guid>
      <pubDate>Mon, 05 Feb 2024 23:23:46 GMT</pubDate>
    </item>
    <item>
      <title>如果在多类分类中删除相关嵌入，f1 分数会降低而 AUC 会增加吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</link>
      <description><![CDATA[我在 2 个场景中使用手套嵌入构建了一个神经网络模型。
场景一：
我使用手套嵌入在具有 3725 个唯一标记的 3 个类别的平衡数据集上构建了一个神经网络模型。我正在执行多类分类。
我得到的训练和测试样本 F1 分数分别为 93.2% 和 74.78%。
AUC得分分别为0.815和0.782。
我有 3725 个令牌，其中 25 个令牌在嵌入之间具有相关性 &gt; 0.95。
在场景 B 中，我删除了这些高度相关的标记。
场景 - B：
我再次使用手套嵌入在 3 个类的平衡数据集上构建了一个神经网络模型，这一次只有 3705 个唯一标记。
我得到的训练和测试样本 F1 分数分别为 74.2% 和 60.8%。
AUC得分分别为0.905和0.794。
即从场景 A 到场景 B，我的 f1 分数正在下降，但 AUC 正在增加。
问题：
当减小嵌入矩阵的大小时，文本分类模型中的 F1 分数是否会降低，但 AUC 会增加？
可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</guid>
      <pubDate>Mon, 05 Feb 2024 12:32:57 GMT</pubDate>
    </item>
    <item>
      <title>根据我的口味对图片进行分类</title>
      <link>https://stackoverflow.com/questions/77930655/classification-of-pictures-according-to-my-taste</link>
      <description><![CDATA[我正在尝试创建一个模型，将图片（艺术、照片）分为 3 类：“我喜欢”和“我喜欢”。 “正常”和“没兴趣” (2,1,0)，但我无法通过 53% 的猜测限制
我有一个包含 29240 张图像的数据集。我完全使用了它，每个类别有 4500 张图片，我使用了论证，没有任何东西有助于增加猜测的百分比。还尝试了调整大小和重新缩放。
如何修改模型以提高结果质量？
我找到了这个用于对象分类的模型：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()

model.add(Conv2D(16, (5, 5), padding=&#39;相同&#39;,
                 input_shape=(200, 200, 3), 激活=&#39;relu&#39;))

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(32, (5, 5), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (5, 5), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (5, 5), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))

模型.add(压平())
model.add（密集（512，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))
model.add（密集（512，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))

model.add（密集（3，激活=&#39;softmax&#39;））

model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/77930655/classification-of-pictures-according-to-my-taste</guid>
      <pubDate>Sat, 03 Feb 2024 02:15:34 GMT</pubDate>
    </item>
    <item>
      <title>优化不平衡测试集二元分类的决策阈值</title>
      <link>https://stackoverflow.com/questions/77901993/optimizing-decision-threshold-for-binary-classification-on-unbalanced-test-set</link>
      <description><![CDATA[我正在研究一个二元分类问题，使用多层感知器 (MLP) 模型来区分假拟音和非假拟音音效。该模型输出一个概率分数（在输出层中使用 sigmoid 激活函数），然后我根据决策阈值将其分类为假或非假。目前，决策阈值设置为0.5。
我的数据集本质上是不平衡的，两个类之间的分布存在显着差异。然而，出于训练目的，我设法通过过采样来平衡数据集，从而在训练集中实现 50/50 的类分布。另一方面，评估测试集反映了原始的不平衡分布，大约包含 80% 的 0 类（假）和 20% 的 1 类（非假）。
我在评估集上取得了很好的精度 (88%)，但我担心仅基于该集优化决策阈值可能无法很好地推广到其他数据集，尤其是那些具有更平衡或不同分布集的数据集类。
我正在寻求有关如何在这种情况下实现决策阈值优化的建议。
注意：我在处理/训练步骤中使用 pytorch。]]></description>
      <guid>https://stackoverflow.com/questions/77901993/optimizing-decision-threshold-for-binary-classification-on-unbalanced-test-set</guid>
      <pubDate>Mon, 29 Jan 2024 18:54:38 GMT</pubDate>
    </item>
    <item>
      <title>即使在专辑中分配标签字段，“label_fields”也无效</title>
      <link>https://stackoverflow.com/questions/76788751/label-fields-are-not-valid-even-when-assigning-label-fields-in-albumentations</link>
      <description><![CDATA[我正在使用带有以下代码的专辑：
 增强器 = alb.Compose([alb.RandomCrop(width=450, height=450),
                             alb.Horizo​​ntalFlip(p=0.5),
                             alb.RandomBrightnessContrast(p=0.2),
                             alb.RandomGamma(p=0.2),
                             alb.RGBShift(p=0.2),
                             alb.VerticalFlip(p=.5)],
                             bbox_params=alb.BboxParams(format=&#39;albumentations&#39;, label_fields=[&#39;person&#39;]))

“中间图像加载代码”

img = pyplot.imread(“路径”)
坐标 = [1,2,3,4]
尝试：
    增强=增强（图像=img，bboxes=[坐标]，class_labels=[&#39;人&#39;]）

除了异常 e：
    打印(e)


我收到异常：您的“label_fields”无效 - 它们必须与 dict 中的参数具有相同的名称
我上网查了一下，发现其他人也有同样的问题，但从未找到具体的解决方案。我还查看了文档，我无法解读 data 与我的问题有何关联。任何对此的帮助将不胜感激！
此外，由于某种原因，按“tab”键会导致在此网站上不起作用，因此缩进可能已关闭。]]></description>
      <guid>https://stackoverflow.com/questions/76788751/label-fields-are-not-valid-even-when-assigning-label-fields-in-albumentations</guid>
      <pubDate>Fri, 28 Jul 2023 14:32:09 GMT</pubDate>
    </item>
    <item>
      <title>预测测试图像时出现错误 - 无法重塑大小数组</title>
      <link>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</guid>
      <pubDate>Wed, 12 May 2021 17:25:10 GMT</pubDate>
    </item>
    <item>
      <title>错误 conda.core.link:_execute(698): 安装包“defaults::icu-58.2-ha925a31_3”时发生错误</title>
      <link>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</link>
      <description><![CDATA[我使用 anaconda 提示创建了环境 conda create -n Talkingbot python=3.5 然后安装 pip installtensorflow==1.0.0 （遵循与一个 udemy 中使用的相同命令当然）但是当我尝试安装spyder时
使用 conda install spyder 然后它给了我这个错误：
准备交易：完成
验证交易：完成
执行交易：完成
错误 conda.core.link:_execute(698)：安装包“defaults::icu-58.2-ha925a31_3”时发生错误。
回滚事务：完成

[Errno 13] 权限被拒绝：&#39;C:\\Users\\Lenovo\\anaconda3\\envs\\talkingbot\\Library\\bin\\icudt58.dll&#39;
()

然后我尝试使用 anaconda navigator 安装间谍程序，但从那里也没有安装间谍程序。
帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</guid>
      <pubDate>Sun, 13 Sep 2020 13:42:24 GMT</pubDate>
    </item>
    <item>
      <title>测试精度大于训练精度怎么办？</title>
      <link>https://stackoverflow.com/questions/51464591/test-accuracy-is-greater-than-train-accuracy-what-to-do</link>
      <description><![CDATA[我正在使用随机森林。我的测试准确度是 70%，而训练准确度是 34% ？该怎么办 ？我该如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/51464591/test-accuracy-is-greater-than-train-accuracy-what-to-do</guid>
      <pubDate>Sun, 22 Jul 2018 11:24:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn 进行加权线性回归</title>
      <link>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</link>
      <description><![CDATA[我的数据：
状态 N Var1 Var2
阿拉巴马州 23 54 42
阿拉斯加 4 53 53
亚利桑那州 53 75 65

Var1 和 Var2 是州级别的聚合百分比值。 N 是每个状态的参与者数量。我想在 Python 2.7 中使用 sklearn 考虑 N 作为权重，在 Var1 和 Var2 之间运行线性回归。
总的思路是：
fit(X, y[, 样本权重])

假设使用 Pandas 将数据加载到 df 中，并且 N 变为 df[&quot;N&quot;]，我是否只需拟合数据或者我是否需要在命令中将 N 用作 sample_weight 之前以某种方式对其进行处理？
fit(df[&quot;Var1&quot;], df[&quot;Var2&quot;],sample_weight=df[&quot;N&quot;])
]]></description>
      <guid>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</guid>
      <pubDate>Sat, 06 Feb 2016 02:58:20 GMT</pubDate>
    </item>
    </channel>
</rss>