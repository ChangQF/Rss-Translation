<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 04 Apr 2024 15:13:52 GMT</lastBuildDate>
    <item>
      <title>我正在使用 Function Transformer 对目标列进行特征转换，但我无法了解如何将其传递给函数？</title>
      <link>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</link>
      <description><![CDATA[代码是采用我的 Target 列，即 Time_taken(min)，其值为 (min) 36、(min) 54、(min) 65 ... 等等。所以我想创建一个新列“所用时间”其值为 36,54,65....并使用函数转换器删除 Time_taken(min)。
从 sklearn.base 导入 BaseEstimator、TransformerMixin
从 sklearn.pipeline 导入管道
类 TimeTakenTransformer(BaseEstimator, TransformerMixin):
def init(self, input_column):
self.input_column = input_column
def fit(self, X, y=None):
返回自我
def 变换（自身，X）：
    X = X.copy()
    操作 = []
    对于 X[self.input_column] 中的 i：
        a = i.split()
        op.append(int(a[1]))
    X[&#39;Time_taken&#39;] = op
    X.drop([self.input_column], axis=1, inplace=True)
    返回X

Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])
Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])
TARGET_COLUMN_NAME = “所用时间（分钟）” # 假设这是正确的列名称
假设 df 是您的 DataFrame，应用转换
df_transformed = Target_column.fit_transform(df[[TARGET_COLUMN_NAME]])
用转换后的列替换原始列
df[“Time_Taken”] = df_transformed
我无法识别传递输入的正确方法。所以我请求你修改我的代码并告诉我如何解决这个问题。感谢您的理解。]]></description>
      <guid>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</guid>
      <pubDate>Thu, 04 Apr 2024 13:19:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 train_on_batch() 时，张量流的形状与预期不同</title>
      <link>https://stackoverflow.com/questions/78273628/tensor-flow-gets-a-different-shape-than-expected-when-using-train-on-batch</link>
      <description><![CDATA[将数据传递给 train_on_batch() 时出现错误，即使之前检查时数据的形状正确。
def train（X_train，y_train，latent_dim，epochs = 20，batch_size = 128）：
    #初始化GAN
    生成器=build_generator（latent_dim）
    判别器=build_discriminator()
    gan=gan_net(生成器,鉴别器)
    
    batch_count = X_train.shape[0] //batch_size
    half_batch = 批量大小 // 2

    对于范围（1，纪元+1）中的纪元：
        print(&quot;###### @ Epoch&quot;,epoch)
        for _ in tqdm(范围(batch_count)):
            x = randint(0, X_train.shape[0], half_batch)
            X，标签 = X_train[x]，y_train[x]
            y = np.ones((half_batch, 1))
            print(f&quot;X: {X.shape}, 标签: {labels.shape}, y: {y.shape}&quot;)
            # 这会打印出正确的形状：
            # X: (64, 28, 28, 1), 标签: (64, 6), y: (64, 1)
            d_loss = discriminator.train_on_batch([X, 标签], y)
            # 这行会抛出错误

&lt;块引用&gt;
回溯（最近一次调用最后一次）：
文件“/Users/.../Downloads/Project_newest_new.py”，第 248 行，位于
训练（X_train，y_train，latent_dim，epochs = 20，batch_size = 128）
文件“/Users/.../Downloads/Project_newest_new.py”，第 198 行，列车中
d_loss = discriminator.train_on_batch([X, 标签], y)
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 551 行，在 train_on_batch 中
日志= self.train_function(data())
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py”，第 153 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 118 行，在 one_step_on_iterator 中
输出= self.distribute_strategy.run(
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 106 行，one_step_on_data
返回 self.train_step(数据)
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 57 行，在 train_step 中
y_pred = self(x, 训练=True)
文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
ValueError：调用 Reshape.call() 时遇到异常。
无法将具有 301056 个元素的张量重塑为形状 [64,28,28,1]（50176 个元素），其中 &#39;{{node function_3_1/reshape_2_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functioning_3_1 /embedding_1_1/GatherV2, function_3_1/reshape_2_1/Reshape/shape)&#39;，输入形状：[64,6,784], [4]，输入张量计算为部分形状：input[1] = [64,28,28,1] .
Reshape.call() 收到的参数：
输入=tf.Tensor（形状=（64,6,784），dtype=float32）

我很困惑，因为手动打印时 X、标签和 y 似乎具有正确的形状 X: (64, 28, 28, 1), labels: (64, 6), y: (64, 1) 但 train_on_batch() 似乎得到了不同的形状 shape=(64, 6, 784)。]]></description>
      <guid>https://stackoverflow.com/questions/78273628/tensor-flow-gets-a-different-shape-than-expected-when-using-train-on-batch</guid>
      <pubDate>Thu, 04 Apr 2024 11:46:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn SVC 的表现很差，甚至比 Logistic Regression 还差？</title>
      <link>https://stackoverflow.com/questions/78273085/why-does-the-scikit-learn-svc-performs-bad-and-even-worse-than-logistic-regressi</link>
      <description><![CDATA[我目前面临 SVC 的挑战性问题在多标签分类上下文中实现 scikit-learn。尽管严格尝试对模型进行微调，但性能仍然低得令人困惑。
一个数据实例由 98 个数字特征和 70 个单独的二进制标签（使用 MultiLabelBinarizer 编码的分类标签）（多标签数据）组成。
问题：

性能结果：SVC 在验证和测试数据集上的性能均低于标准，F1 分数（平均值 =“宏观”）为 0.3735，F1 分数（平均值 =验证数据的“加权”)为0.5677。形成鲜明对比的是，随机森林/决策树模型的 F1 分数高于 93%，甚至 Logistic 回归模型的 F1 分数也达到 77% 和 82%（宏观/加权），这对我来说毫无意义理论上的观点，因为 svc 实际上应该找到最好的类边界，因此比 LR 更好，因此也应该更好。

数据不平衡：我的数据集在标签分布方面存在严重的类别不平衡。这种不平衡是否会导致 SVC 表现不佳？我认为支持向量机对异常值更稳健，但令我困惑的是其他模型处理数据的效果很好。

get_config(&#39;random_state&#39;) 设置全局随机状态值


这是我到目前为止所做的事情：

数据缩放：在训练之前，我使用 scikit-learn 的 MinMaxScaler 缩放数据，确保所有特征都在 0 到 1 之间标准化。

数据拆分：数据集已拆分为训练集、验证集和测试集，以确保对模型性能进行稳健评估，例如


# 分为训练数据 (70%) 和剩余数据 (30%)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=get_config(&#39;random_state&#39;), stratify=y)

# 将剩余部分细分为验证数据 (20%) 和测试数据 (10%)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=get_config(&#39;random_state&#39;), stratify=y_temp)



模型和超参数优化：我利用了 scikit-learn 的 SVC，并通过权重和网格搜索使用随机和网格搜索来执行超参数优化。偏差（WandB）以找到最佳可能的组合。以下是我尝试过的超参数：


C：[0.01、0.1、1、10、50、100、500]
内核：[线性、poly、rbf、sigmoid]
学位：[2,3,4,5]
伽玛：[0.0001, 0.001, 0.01, 0.1, 1, &#39;缩放&#39;, &#39;自动&#39;]
Class_weight：[无，“平衡”]
Max_iter：[100、200、300、500、700、1000、2500、5000、8000、-1]
概率：[对，错]


分类器包装：考虑到任务的多标签性质，我将 SVC 包装在 scikit-learn 的两个不同的集成包装模型中（SVC 和 LR 本身不支持多标签分类），即 ClassifierChain 和 MultiOutputClassifier。以下是用于包装的代码片段：

# 创建 scikit-learn SVC 的实例
从 sklearn.svm 导入 SVC
从 sklearn.multioutput 导入 MultiOutputClassifier
从 sklearn.multioutput 导入 ClassifierChain

base_svc = SVC(C=C, kernel=kernel, Degree= Degree, gamma=gamma, coef0=coef0, 收缩=收缩,probability=概率, tol=tol, cache_size=cache_size, max_iter=max_iter,
Decision_function_shape=decision_function_shape，break_ties=break_ties，
random_state=random_state, class_weight=class_weight)

# 用 ClassifierChain 包装 SVC
self.clf = ClassifierChain(base_estimator=base_svc, random_state=get_config(&#39;random_state&#39;))

# 用 MultiOutputClassifier 包装 SVC
self.clf = MultiOutputClassifier(估计器=base_svc, n_jobs=-1)


我创建了两个相同的 SVC 模型，每个模型都有不同的包装器用于测试。令人惊讶的是，这两种模型的表现同样不尽如人意。
我将非常感谢社区的任何见解或帮助。 SVC 是否存在本质上不适合此特定任务的地方，或者我的实现或（hp）配置中是否存在潜在的疏忽，可能导致这种意外的糟糕性能？
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78273085/why-does-the-scikit-learn-svc-performs-bad-and-even-worse-than-logistic-regressi</guid>
      <pubDate>Thu, 04 Apr 2024 10:03:24 GMT</pubDate>
    </item>
    <item>
      <title>优化 Python 中的手动 QDA 实现：数值稳定性和性能问题</title>
      <link>https://stackoverflow.com/questions/78272861/optimizing-manual-qda-implementation-in-python-numerical-stability-and-performa</link>
      <description><![CDATA[我正在努力在 Python 中手动实现二次判别分析 (QDA)，以进行图像分类，处理高维数据。为了确保涉及协方差矩阵、逆矩阵和行列式的计算中的数值稳定性，我将数据集从 int8 转换为 float64：
X_train = train[&#39;X&#39;].transpose(2, 0, 1).reshape(-1, 28*28).astype(np.float64)

鉴于 float64 的内存使用量是 float32 的两倍，我担心对性能的影响，尤其是对于大型数据集。我的问题是：

转换为 float64 是确保此类计算中数值稳定性的最佳实践，还是有资源占用较少的替代方案？
在协方差矩阵计算至关重要的 QDA 中，使用 float32 是否会显着增加数值错误的风险？
是否存在特定场景或条件，在这些场景或条件下，float32 可能足以满足 QDA 模型的准确性和稳定性？

该数据集涉及数千张图像，每张图像都重新整形为包含 784 个特征（28x28 像素）的一维数组。每个类的协方差矩阵的手动 QDA 计算引起了对数值精度的担忧。
此外，我的自定义 qda_predict 函数遇到了严重的性能问题，该函数使用 scipy.stats.multivariate_normal.logpdf 来计算可能性。尽管将我的数据集减少到 100 个样本的子集，但该函数的运行时间过长或看似无穷无尽，这使得它对于较大的数据集来说不切实际：
def qda_predict(X, params):
    mu = 参数[&#39;mu&#39;]
    cov = 参数[&#39;cov&#39;]
    pi = 参数[&#39;pi&#39;]
    y_pred = []
    对于 X 中的 x：
        分数 = []
        对于范围内的 c(len(mu))：
            分数 = multivariate_normal.logpdf(x,mean=mu[c],cov=cov[c],allow_singular=True) + np.log(pi[c])
            分数.append(分数)
        y_pred.append(np.argmax(分数))
    返回 np.array(y_pred)

当使用 sklearn 的内置 QuadraticDiscriminantAnalysis 函数时，性能和运行时间显着提高，在大约 20 秒内对同一子集进行拟合和预测。这种明显的差异表明我的实施可能效率低下。
我正在寻找有关在这种情况下管理数值稳定性与性能权衡的见解或建议，以及在不影响模型准确性的情况下减少运行时间的具体优化。任何关于 sklearn 如何实现其效率以及这些策略是否可以应用于我的手动实现的建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78272861/optimizing-manual-qda-implementation-in-python-numerical-stability-and-performa</guid>
      <pubDate>Thu, 04 Apr 2024 09:21:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么 LSTM 对非序列时间序列数据集的准确率达到 90%</title>
      <link>https://stackoverflow.com/questions/78272446/why-lstm-is-giving-90-accuracy-for-non-sequential-time-series-dataset</link>
      <description><![CDATA[我目前正在研究时间序列分类。
为此我使用了 LSTM 模型。我以非顺序时间序列数据集的形式提供输入，这给了我 90% 的准确度。然而这是错误的方法，所以我给出了顺序时间序列数据集，结果准确率为 64%。
实际上，我预计准确度为 64-67%，因为存在一些重叠的时间序列数据点。但我的疑问是为什么非顺序时间序列的准确率达到 90%。
预计准确度为 64-67%]]></description>
      <guid>https://stackoverflow.com/questions/78272446/why-lstm-is-giving-90-accuracy-for-non-sequential-time-series-dataset</guid>
      <pubDate>Thu, 04 Apr 2024 07:50:02 GMT</pubDate>
    </item>
    <item>
      <title>我以 h5 格式单独保存了年龄、性别和情绪预测模型。有谁知道如何将它们组合在一起以供实时使用？</title>
      <link>https://stackoverflow.com/questions/78271808/i-have-separate-saved-models-for-age-gender-and-emotion-prediction-in-h5-format</link>
      <description><![CDATA[我创建了年龄、性别和情绪预测模型，并将它们保存为 h5 格式。
我将它们组合在一起来预测输出，但用于自定义输入。我无法将它们组合起来进行实时预测。]]></description>
      <guid>https://stackoverflow.com/questions/78271808/i-have-separate-saved-models-for-age-gender-and-emotion-prediction-in-h5-format</guid>
      <pubDate>Thu, 04 Apr 2024 05:17:49 GMT</pubDate>
    </item>
    <item>
      <title>CNN ValueError：由于下采样，输出中的维度之一 <= 0</title>
      <link>https://stackoverflow.com/questions/78271160/cnn-valueerror-one-of-the-dimensions-in-the-output-is-0-due-to-downsampling</link>
      <description><![CDATA[我正在制作一个深度学习模型来预测一个值。声明如下：我们有一个
时间 vs A vs B vs C -- xls 文件（多个，存在于 Data 命名文件夹中）
我想使用 A、B、C 的先前值来预测 A。这 ofc 意味着当它是第一个预测时，我们将得到 N/A，因为我们没有旧的 A 给用户。
目前我有这个：（我真的是新手，到处都在网上搜索）：
def load_data(folder_path):
    数据帧 = []
    对于 os.listdir(folder_path) 中的 file_name：
        if file_name.endswith(&#39;.xls&#39;):
            file_path = os.path.join(文件夹路径, 文件名)
            df = pd.read_excel(file_path) # 假设数据是Excel格式
            data_frames.append(df)
    返回 pd.concat(data_frames,ignore_index=True)

def preprocess_data(数据):
    X = 数据[[&#39;S&#39;, &#39;A&#39;, &#39;T&#39;]]
    y = 数据[&#39;F&#39;]
    定标器=标准定标器()
    X_scaled = 缩放器.fit_transform(X)
    X_reshape = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1) # CNN 重塑
    返回 X_reshape, y, 缩放器


def build_cnn_model(input_shape):
    模型=顺序（）
    model.add(Conv1D(filters=1, kernel_size=10, 激活=&#39;relu&#39;, input_shape=(1, 1,3),))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;))
    model.add(MaxPooling1D(pool_size=2))
    模型.add(压平())
    model.add（密集（64，激活=&#39;relu&#39;））
    model.add(密集(1))
    返回模型



def train_and_predict_next_Frequency(folder_path, current_scheduled, current_actual):
    数据=加载数据（文件夹路径）

    X、y、缩放器 = preprocess_data(数据)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    输入形状 = (X_train.shape[1], 1)
    X_train_reshape = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_reshape = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    打印（输入形状）
    模型 = build_cnn_model(input_shape)

    model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

    model.fit（X_train_reshape，y_train，epochs = 50，batch_size = 32，verbose = 1）

    mse = model.evaluate(X_test_reshape, y_test, verbose=0)
    print(&quot;均方误差：&quot;, mse)

    scaled_input = scaler.transform([[current_scheduled, current_actual]])
    input_reshape =scaled_input.reshape(1, 2, 1) # CNN 重塑
    next_Frequency_scaled = model.predict(input_reshape)
    next_Frequency =scaler.inverse_transform(next_Frequency_scaled.reshape(-1, 1))
    返回下一个频率[0][0]



文件夹路径=“数据” # 包含 Excel 文件的文件夹
当前计划 = 10
当前实际 = 8
预测频率 = train_and_predict_next_Frequency(folder_path, current_scheduled, current_actual)
print(“预测的下一个频率：”,predicted_Frequency)

我在这里收到此错误：
ValueError：由于 conv1d_10 中的下采样，输出中的维度之一 &lt;= 0。
考虑增加输入大小。收到的输入形状 [None, 1, 1, 3] 会产生
维度中具有零值或负值的输出形状。

使用 A、B、C 的先前值来预测A。 （上一个是什么？，我们有时间来告诉它）。
我尝试更改图层功能，但没有成功。我想实现上述目标]]></description>
      <guid>https://stackoverflow.com/questions/78271160/cnn-valueerror-one-of-the-dimensions-in-the-output-is-0-due-to-downsampling</guid>
      <pubDate>Thu, 04 Apr 2024 01:02:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 TensorFlow 中使用 model.fit() 会出现 ValueError: Unrecognized data type: x=[...] (of type <class 'list'>) 错误？</title>
      <link>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</link>
      <description><![CDATA[我尝试运行下面的代码，取自  CS50的AI课程：
导入 csv
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split

# 从文件中读取数据
open(“banknotes.csv”) 作为 f：
    读者 = csv.reader(f)
    下一个（读者）

    数据 = []
    对于读卡器中的行：
        数据.追加（
            {
                “证据”：[行[:4]中单元格的浮点（单元格）]，
                “标签”：如果 row[4] == “0”，则为 1否则 0,
            }
        ）

# 将数据分为训练组和测试组
证据 = [行[“证据”] 数据中的行]
labels = [行[“标签”] 数据中的行]
X_训练，X_测试，y_训练，y_测试=train_test_split（
    证据、标签、test_size=0.4
）

# 创建一个神经网络
模型 = tf.keras.models.Sequential()

# 添加一个包含 8 个单元的隐藏层，并使用 ReLU 激活
model.add(tf.keras.layers.Dense(8, input_shape=(4,), 激活=“relu”))

# 添加 1 个单元的输出层，使用 sigmoid 激活
model.add(tf.keras.layers.Dense(1,激活=“sigmoid”))

# 训练神经网络
模型.编译(
    优化器=“adam”，损失=“binary_crossentropy”，指标=[“准确性”]
）
model.fit(X_training, y_training, epochs=20)

# 评估模型的表现
model.evaluate(X_testing, y_testing, verbose=2)

但是，我收到以下错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py”，第 41 行，在  中
    model.fit(X_training, y_training, epochs=20)
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py”，第 113 行，在 get_data_adapter 中
    raise ValueError(f“无法识别的数据类型：x={x}（类型为 {type(x)})”)
ValueError：无法识别的数据类型：x=[...]（类型）

其中“...”是是训练数据。
知道出了什么问题吗？我在 Windows 计算机上使用 Python 版本 3.11.8 和 TensorFlow 版本 2.16.1。
我尝试在 Google Colab 笔记本中运行相同的代码，并且它有效：问题仅发生在我的本地计算机上。这是我期望的输出：
纪元 1/20
26/26 [================================] - 1s 2ms/步 - 损失：1.1008 - 准确度：0.5055
纪元 2/20
26/26 [================================] - 0s 2ms/步 - 损失：0.8588 - 准确度：0.5334
纪元 3/20
26/26 [================================] - 0s 2ms/步 - 损失：0.6946 - 准确度：0.5917
纪元 4/20
26/26 [================================] - 0s 2ms/步 - 损失：0.5970 - 准确度：0.6683
纪元 5/20
26/26 [================================] - 0s 2ms/步 - 损失：0.5265 - 准确度：0.7120
纪元 6/20
26/26 [================================] - 0s 2ms/步 - 损失：0.4717 - 准确度：0.7655
纪元 7/20
26/26 [================================] - 0s 2ms/步 - 损失：0.4258 - 准确度：0.8177
纪元 8/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3861 - 准确度：0.8433
纪元 9/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3521 - 准确度：0.8615
纪元 10/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3226 - 准确度：0.8870
纪元 11/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2960 - 准确度：0.9028
纪元 12/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2722 - 准确度：0.9125
纪元 13/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2506 - 准确度：0.9283
纪元 14/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2306 - 准确度：0.9514
纪元 15/20
26/26 [================================] - 0s 3ms/步 - 损失：0.2124 - 准确度：0.9660
纪元 16/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1961 - 准确度：0.9769
纪元 17/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1813 - 准确度：0.9781
18/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1681 - 准确度：0.9793
19/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1562 - 准确度：0.9793
20/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1452 - 准确度：0.9830
18/18 - 0s - 损失：0.1407 - 准确度：0.9891 - 187ms/epoch - 10ms/step
[0.14066053926944733，0.9890710115432739]
]]></description>
      <guid>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</guid>
      <pubDate>Thu, 04 Apr 2024 00:28:01 GMT</pubDate>
    </item>
    <item>
      <title>在 ARMA 模型中使用梯度下降进行系数估计</title>
      <link>https://stackoverflow.com/questions/78270443/using-gradient-descent-for-coefficient-estimation-in-arma-model</link>
      <description><![CDATA[我正在尝试使用梯度下降来估计其系数，从头开始实现 ARMA 模型。我知道这可能不是理想的解决方案。但我最担心的是，如果这不是一个正确的解决方案。
这是我的 ARMA 模型方程。为此，假设数据集已经静止。

为此，我采用了残差平方和，如下所示，
 
以及衍生品如下：

然后使用梯度下降算法，我可以获得所有系数的值。这是一个好方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78270443/using-gradient-descent-for-coefficient-estimation-in-arma-model</guid>
      <pubDate>Wed, 03 Apr 2024 20:50:17 GMT</pubDate>
    </item>
    <item>
      <title>m=365 时 PM10 浓度预测的 SARIMA 模型存在问题</title>
      <link>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</link>
      <description><![CDATA[我正在尝试构建 SARIMA（季节性自回归综合移动平均线）模型，用于根据五年的数据预测 PM10 浓度。但是，当我将季节性参数 m 设置为 365 时，我的代码似乎无法运行。
有人可以解释一下为什么我的代码没有在 m=365 下运行并提出潜在的解决方案吗？
提前致谢！
# 这是我的代码片段：
## 将数据集拆分为训练集和测试集

    `train_size = int(len(Alipur_df) * 0.8) # 80% 训练，20% 测试`
    `训练，测试 = Alipur_df[:train_size], Alipur_df[train_size:]`

## 将训练 DataFrame 转换为 numpy 数组

    `train_values = train[&#39;Alipur&#39;].values`
    `test_values = test[&#39;Alipur&#39;].values`

## 使用 auto_arima 找到 SARIMA 的最佳参数

    `auto_model = auto_arima(train[&#39;Alipur&#39;],seasonal=True,stationary=True,m=365,trac
]]></description>
      <guid>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</guid>
      <pubDate>Wed, 03 Apr 2024 06:23:04 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“llama_index.llms”（未知位置）导入名称“HuggingFaceInferenceAPI”</title>
      <link>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</link>
      <description><![CDATA[想要导入 HuggingFaceInferenceAPI。
从 llama_index.llms 导入 HugggingFaceInferenceAPI

llama_index.llms 文档没有 HuggingFaceInferenceAPI 模块。有人有这方面的更新吗？]]></description>
      <guid>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</guid>
      <pubDate>Sun, 31 Mar 2024 14:21:19 GMT</pubDate>
    </item>
    <item>
      <title>微调 GPT2 - 注意掩码和 pad token id 错误</title>
      <link>https://stackoverflow.com/questions/74682597/fine-tuning-gpt2-attention-mask-and-pad-token-id-errors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/74682597/fine-tuning-gpt2-attention-mask-and-pad-token-id-errors</guid>
      <pubDate>Mon, 05 Dec 2022 01:57:10 GMT</pubDate>
    </item>
    <item>
      <title>在应用 varImp 函数时使用带插入符号的 xgbTree 方法和目标变量的权重时出现非树模型错误</title>
      <link>https://stackoverflow.com/questions/63731960/non-tree-model-error-when-using-xgbtree-method-with-caret-and-weights-to-target</link>
      <description><![CDATA[当我使用 Caret 包中的“train”函数创建模型以使用权重进行梯度提升时，在使用“varImp”函数时出现错误，表示未检测到树模型。但是当我去掉重量时它就起作用了。
下面的代码会产生错误：
set.seed(123)

model_weights &lt;- ifelse(modelo_df_sseg$FATALIDADES == 1,
                        是 = (1/表(modelo_df_sseg$FATALIDADES)[2]) * 0.5,
                        否 = (1/表(modelo_df_sseg$FATALIDADES)[1]) * 0.5
                        ）

模型 &lt;- 火车（
  as.factor(FATALIDADES) ~.,
  数据= modelo_df_sseg，
  方法=“xgbTree”，
  trControl = trainControl(“cv”, 数量 = 10),
  权重 = 模型权重
  ）

varImp(模型)

但是如果我不施加权重，它就会起作用。
为什么 varImp 无法识别我的树？
编辑 2020 年 9 月 4 日
评论部分的missuse建议使用wts而不是权重。现在我收到以下错误：
nominalTrainWorkflow 中的错误（x = x，y = y，wts = 权重，info = trainInfo，：形式参数 &#39;wts&#39; 与多个实际参数匹配

我用 R 内置数据集编写了一个小代码，以便您可以自己测试：
set.seed(123)

Basex &lt;- 逮捕

model_weights &lt;- ifelse(basex$released == 2,
                        是 = (1/表(basex$released)[2]) * 0.5,
                        否 = (1/表(basex$released)[1]) * 0.5
                        ）

y = basex$已发布
x = 基x
tc = trainControl(“cv”，数量 = 10)

mtd =“xgbTree”；
模型 &lt;- 火车（
  X，
  是，
  方法=MTD，
  trControl = tc,
  wts = 模型权重，
  详细 = TRUE
  ）

也许我创建的权重向量是错误的。但我找不到任何有关“wts”参数的文档。]]></description>
      <guid>https://stackoverflow.com/questions/63731960/non-tree-model-error-when-using-xgbtree-method-with-caret-and-weights-to-target</guid>
      <pubDate>Thu, 03 Sep 2020 21:35:31 GMT</pubDate>
    </item>
    <item>
      <title>Keras AttributeError：“列表”对象没有属性“ndim”</title>
      <link>https://stackoverflow.com/questions/48493755/keras-attributeerror-list-object-has-no-attribute-ndim</link>
      <description><![CDATA[我正在 Jupyter Notebook (Python 3.6) 中运行 Keras 神经网络模型
我收到以下错误

&lt;块引用&gt;
  属性错误：“列表”对象没有属性“ndim”

从 Keras.model 调用 .fit() 方法后

&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Dense(5, input_dim=len(X_data[0]), 激活=&#39;sigmoid&#39; ))
model.add(Dense(1, 激活 = &#39;sigmoid&#39;))
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;acc&#39;])
model.fit（X_data，y_data，epochs = 20，batch_size = 10）

我检查了 Keras 的requirements.txt 文件（在 Anaconda3 中），numpy、scipy 和六个模块版本都是最新的。
如何解释这个 AttributeError？
完整的错误消息如下（似乎与Numpy有些相关）：

&lt;块引用&gt;
  ------------------------------------------------------------ ---------------------------- AttributeError Traceback（最近调用
  最后）在（）
        3 model.add(密集(1,激活=&#39;sigmoid&#39;))
        4 model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;acc&#39;])
  ----&gt; 5 model.fit(X_data, y_data, epochs=20, batch_size=10)
~\Anaconda3\lib\site-packages\keras\models.py 中的 fit(self, x, y,
  批量大小、纪元、详细、回调、validation_split、
  验证数据、洗牌、类权重、样本权重、初始纪元、
  steps_per_epoch、validation_steps、**kwargs）
      第963章
      第964章
  --&gt; 第965章 验证步骤=验证步骤）
      966
      第967章
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py 中的 fit(self, x,
  y、batch_size、纪元、详细、回调、validation_split、
  验证数据、洗牌、类权重、样本权重、初始纪元、
  steps_per_epoch、validation_steps、**kwargs) 1591
  第1592章 1592
  -&gt; 1593 batch_size=batch_size) 1594 # 准备验证数据。第1595章
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py 中
  _standardize_user_data(self、x、y、sample_weight、class_weight、check_batch_axis、batch_size) 1424
  self._feed_input_shapes，1425
  check_batch_axis=False,
  第1426章 1427、第1427章
  第1428章
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py 中
  _standardize_input_data（数据、名称、形状、check_batch_axis、exception_prefix）
       68 elif isinstance（数据，列表）：
       69 data = [x.values if x.class.name == &#39;DataFrame&#39; else x for x in data]
  ---&gt; 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71 其他：
       72 data = data.values if data.class.name == &#39;DataFrame&#39; else data
~\Anaconda3\lib\site-packages\keras\engine\training.py 中
  (.0)
       68 elif isinstance（数据，列表）：
       69 data = [x.values if x.class.name == &#39;DataFrame&#39; else x for x in data]
  ---&gt; 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71 其他：
       72 data = data.values if data.class.name == &#39;DataFrame&#39; else data
属性错误：“列表”对象没有属性“ndim”
]]></description>
      <guid>https://stackoverflow.com/questions/48493755/keras-attributeerror-list-object-has-no-attribute-ndim</guid>
      <pubDate>Mon, 29 Jan 2018 02:55:27 GMT</pubDate>
    </item>
    <item>
      <title>NotFittedError：估计器未安装，在利用模型之前调用“fit”</title>
      <link>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</link>
      <description><![CDATA[我在 Macbook OSX 10.2.1 (Sierra) 上运行 Python 3.5.2。
在尝试从 Kaggle 运行泰坦尼克号数据集的一些代码时，我不断收到以下错误：

&lt;块引用&gt;
  &lt;小时/&gt;
  
  NotFittedError Traceback（最近调用
  最后）在（）
        6
        7 # 使用测试集进行预测并打印它们。
  ----&gt; 8 my_prediction = my_tree_one.predict(test_features)
        9 打印（我的预测）
       10 
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
  在预测（自我，X，check_input）中
      第429章
      第430章
  --&gt; 431 X = self._validate_X_predict(X, check_input)
      第432章
      [第 433 章]
  
  /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
  在 _validate_X_predict(self, X, check_input) 中
      第386章
      第387章
  --&gt; 388 raise NotFittedError(&quot;估计器未安装，&quot;
      第389章
      390 
NotFittedError：估计器未安装，请在利用之前调用 fit
  型号。

有问题的代码似乎是这样的：
# 用中位数估算缺失值
test.Fare[152] = test.Fare.median()

# 从测试集中提取特征：Pclass、Sex、Age 和 Fare。
test_features = test[[&quot;Pclass&quot;, &quot;性别&quot;, &quot;年龄&quot;, &quot;票价&quot;]].values

# 使用测试集进行预测并打印它们。
my_prediction = my_tree_one.predict(test_features)
打印（我的预测）

# 创建一个包含两列的数据框：PassengerId &amp;幸存下来了。幸存包含你的预测
PassengerId =np.array(test[&quot;PassengerId&quot;]).astype(int)
my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [&quot;幸存&quot;])
打印（我的解决方案）

# 检查您的数据框是否有 418 个条目
打印（my_solution.shape）

# 将解决方案写入名为 my_solution.csv 的 csv 文件
my_solution.to_csv(&quot;my_solution_one.csv&quot;,index_label = [&quot;PassengerId&quot;])

这里是其余代码的链接。
由于我已经调用了“fit”函数，因此我无法理解此错误消息。我哪里出错了？感谢您抽出时间。
编辑：
结果发现问题是从上一个代码块继承而来的。
# 拟合你的第一个决策树：my_tree_one
my_tree_one = 树.DecisionTreeClassifier()
my_tree_one = my_tree_one.fit(features_one, 目标)

# 查看包含的功能的重要性和得分
打印（my_tree_one.feature_importances_）
打印（my_tree_one.score（features_one，目标））

用行：
my_tree_one = my_tree_one.fit(features_one, target)
生成错误：

&lt;块引用&gt;
  ValueError：输入包含 NaN、无穷大或太大的值
  dtype(&#39;float32&#39;)。
]]></description>
      <guid>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</guid>
      <pubDate>Fri, 02 Dec 2016 17:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>