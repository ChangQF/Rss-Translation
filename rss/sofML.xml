<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 12 Jun 2024 03:17:47 GMT</lastBuildDate>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>更改 Keras/Tensorflow“可见设备”会降低模型的准确性</title>
      <link>https://stackoverflow.com/questions/78610219/changing-keras-tensorflow-visible-devices-makes-the-model-less-accurate</link>
      <description><![CDATA[我正在运行一个用 Keras/Tensorflow 构建的 CNN。因为我在学校的远程服务器上运行，其他人也使用该服务器，所以我通常会设置“可见设备”来限制使用的 GPU 数量，而不是使用所有 GPU。以下是我用来执行此操作的代码：
def set_gpus(num_gpus=4):
print(&quot;Num GPUs Available: &quot;, len(tf.config.list_physical_devices(&#39;GPU&#39;)))
gpus = tf.config.list_physical_devices(&#39;GPU&#39;)
if gpus:
# 限制 TensorFlow 仅使用第一个 GPU
try:
tf.config.set_visible_devices(gpus[:num_gpus], &#39;GPU&#39;)
logical_gpus = tf.config.list_logical_devices(&#39;GPU&#39;)
print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPU&quot;)
except RuntimeError as e:
# 必须在初始化 GPU 之前设置可见设备
print(e)

我发现，当我没有运行 set_gpus 时，我的模型表现比运行 set_gpus 时好得多。当仅使用 4 个 GPU 时，训练准确率在 100 个 epoch 后达到 88%，但验证和测试准确率都保持在 50% 左右，两个类别的 F1 分数显示一个类别的准确率约为 66%，另一个类别的准确率约为 3%。相比之下，当我在未先运行 set_gpus() 的情况下运行代码时，验证和测试准确率约为 70%，类别特定分数分别为 56% 和 77%。
我尝试在运行 set_gpus 时更改 num_gpus，但即便如此，我得到的结果与仅使用 4 个 GPU 时的结果相同。即使我将 GPU 数量设置为 8（即服务器上的总数），结果也不会改变。有人知道是什么原因导致了这个问题吗？
我的代码在 Jupyter Notebook 上运行，其中安装了 Python 3.12.2、Keras 版本 3.0.5 和 Tensorflow 版本 2.16.1。以下是与我的 CNN 模型相关的附加代码，希望对您有所帮助：
def convolution_encoder(inputs, kernels, kernel_size=[3,3], stride=[1,1], data_format=&#39;channels_last&#39;):
# 规范化和注意力
x = layer.BatchNormalization(axis=-1)(inputs)
x = layer.Conv2D(filters=kernels,activation=&#39;relu&#39;, kernel_size=kernel_size, strides=stride, data_format=data_format)(x)
x = layer.MaxPooling2D(data_format=data_format)(x)
return x

def build_model_predict(
input_shape,
output_dim=2,
mode=&#39;single-channel&#39;,
data_format=&#39;channels_last&#39;
):
input = keras.Input(shape=input_shape)
x = 输入
x= convolution_encoder(x, 16，[5,5]，[2,2]，data_format=data_format) 
x= convolution_encoder(x，32，[3,3]，[1,1]，data_format=data_format)
x= convolution_encoder(x，64，[3,3]，[1,1]，data_format=data_format)
x= layer.Flatten()(x)
x= layer.Dropout(0.5)(x)
x= layer.Dense(units=256，activation=&#39;sigmoid&#39;)(x)
x= layer.Dropout(0.5)(x)
output = layer.Dense(output_dim，activation=&quot;softmax&quot;)(x)
return keras.Model(inputs, output)

def run_CNN(x, y, seeded=False, seed=42, epoch_num=50):
if seeded:
x_train, x_test, y_train，y_test= train_test_split(x，y，test_size= 0.20，random_state=seed，stratify=y)
a，x_val，b，y_val= train_test_split(x_train，y_train，test_size= 0.10，random_state=seed，stratify=y_train)
else:
x_train，x_test，y_train，y_test= train_test_split(x，y，test_size= 0.20，stratify=y)
a，x_val，b，y_val= train_test_split(x_train，y_train，test_size= 0.10，stratify=y_train)
keras.backend.clear_session()
model = build_model_predict(
x_train.shape[1:],
output_dim=y.shape[-1])
model.compile(
optimizer=keras.optimizers.Adam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08), # 优化器
# 最小化损失函数
loss=keras.losses.CategoricalFocalCrossentropy(),
# 要监控的指标列表
metrics=[keras.metrics.CategoricalAccuracy()],
)
print(&quot;Fit model on training data&quot;)
history = model.fit(
x_train,
y_train,
batch_size=64,
epochs=epoch_num,
# 我们通过一些验证来
# 监控验证损失和指标
# 在每个 epoch 结束时
validation_data=(x_val, y_val),
initial_epoch=0
#callbacks=callbacks_list
)
results = model.evaluate(x_test, y_test, batch_size=128)
print(&#39;按类别划分的训练准确率：&#39;)
y_train_predict = model.predict(x_train)
score= f1_score(np.argmax(y_train, axis=1), np.argmax(y_train_predict, axis=1), average=None)
print(score)
print(&#39;按类别划分的测试准确率：&#39;)
y_predict = model.predict(x_test)
score= f1_score(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1), average=None)
print(score)
return model

下面是运行模型的实际代码块：
set_gpus()
model=run_CNN(x_stft, y_short, seeded=True, epoch_num=100)
]]></description>
      <guid>https://stackoverflow.com/questions/78610219/changing-keras-tensorflow-visible-devices-makes-the-model-less-accurate</guid>
      <pubDate>Wed, 12 Jun 2024 00:45:35 GMT</pubDate>
    </item>
    <item>
      <title>CNN 网络中的卷积层可以在池化层或全连接层之后继续进行吗？</title>
      <link>https://stackoverflow.com/questions/78610065/can-convolutional-layers-in-cnn-networks-proceed-after-pooling-or-fully-connecte</link>
      <description><![CDATA[CNN 网络中的卷积层可以在池化或全连接层之后继续吗？我知道具有这种架构的网络是不寻常的，但我很好奇，因为我正在尝试了解不同 CNN 设计中的可变性。
我预计答案是肯定的？]]></description>
      <guid>https://stackoverflow.com/questions/78610065/can-convolutional-layers-in-cnn-networks-proceed-after-pooling-or-fully-connecte</guid>
      <pubDate>Tue, 11 Jun 2024 23:16:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么 FastAPI 框架使用起来很昂贵？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78609577/why-fastapi-framework-is-expensive-to-use</link>
      <description><![CDATA[有一篇文章比较了基于 Python 的后端框架 FastAPI 和 Flask 在 ML 项目中的应用。文章中关于 FastAPI 的评论如下：

FastAPI 框架的主要缺点是价格昂贵。具体价格会根据您使用它的国家/地区以及您每月进行的 API 调用次数而有所不同。但总体而言，成本很高。

我不太明白的是，如果 FastAPI 擅长创建 API，但调用开发的 API 却很昂贵，那么为什么它在 ML 中仍然越来越受欢迎？我知道 FastAPI 中集成了一些付费库。在使用 FastAPI 开发的 ML 项目的 API 调用中，这些付费库一定会被调用吗？如何估算使用 FastAPI 开发的 ML 项目需要多少成本？]]></description>
      <guid>https://stackoverflow.com/questions/78609577/why-fastapi-framework-is-expensive-to-use</guid>
      <pubDate>Tue, 11 Jun 2024 20:12:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Cord-V2 数据集微调 LayoutLmv3</title>
      <link>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</link>
      <description><![CDATA[我正在使用 CORD-v2 数据集对 LayoutLMv3 进行微调。我在数据预处理部分遇到了困难，特别是如何从图像中正确提取总量 (TTC)。我在网上找到的示例似乎使用了较旧的 CORD 数据集，该数据集的格式不同。新的 CORD-v2 数据集仅包含图像和地面实况标签。
如何解决这个问题？
我尝试过 YouTube 和 Hugging Face 中的示例，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</guid>
      <pubDate>Tue, 11 Jun 2024 09:22:25 GMT</pubDate>
    </item>
    <item>
      <title>每个时间戳有多个条目的数据集-解决方案？</title>
      <link>https://stackoverflow.com/questions/78605877/dataset-with-multiple-entries-per-timestamp-solutions</link>
      <description><![CDATA[我想预测某些公司每月股价的百分比变化。我有一个包含月度数据的数据集，因此这些公司的股价百分比变化也是按月计算的。我目前每个时间戳（月）有多个条目，范围从每月 185 到 238。
我听说和读到过，每个时间戳的多个条目会使模型难以学习，因此难以做出准确的预测。当我将条目数量限制为每月一个时，我会丢失非常重要的信息，因此这对我不利。
有人可以解释一下，并给出一些我可以遵循的建议，或者我可以尝试的模型吗？
我已经尝试过 XGBoost。当使用不考虑训练的先前股价数据时，XGBoost 确实很难学习。]]></description>
      <guid>https://stackoverflow.com/questions/78605877/dataset-with-multiple-entries-per-timestamp-solutions</guid>
      <pubDate>Tue, 11 Jun 2024 07:05:41 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：从“y”的唯一值推断出的类无效。预期：[0 1 2]，结果为 ['Dropout' 'Enrolled' 'Graduate']</title>
      <link>https://stackoverflow.com/questions/78605622/valueerror-invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2</link>
      <description><![CDATA[我正在使用 XGBoost 分类器模型进行分类任务。我的数据集包含分类变量，我的目标类别（“辍学”、“入学”、“毕业”）。
from xgboost import XGBClassifier

xgb = XGBClassifier(
n_estimators=200,
max_depth=6, 
learning_rate=0.1, 
subsample=0.8,
colsample_bytree=0.8,
eval_metric=&#39;mlogloss&#39; 
)

xgb.fit(X_train, y_train)

我收到以下错误
ValueError: 从“y”的唯一值推断出的无效类别。预期：[0 1 2]，
得到 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;]

之后，我使用标签编码器技术；它工作正常。但我需要 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;] 这个生产部分的分类。在训练 XGBClassifier 之后，我如何将这个 [0 1 2] 更改为 [&#39;Dropout&#39; &#39;Enrolled&#39; &#39;Graduate&#39;]。]]></description>
      <guid>https://stackoverflow.com/questions/78605622/valueerror-invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2</guid>
      <pubDate>Tue, 11 Jun 2024 05:56:08 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法的分类[关闭]</title>
      <link>https://stackoverflow.com/questions/78605421/classification-on-machine-learning-algorithms</link>
      <description><![CDATA[过去三个月我一直在研究机器学习，偶然发现了这句话

机器学习模型可以分为生成式或描述式、概率式与非概率式以及参数式与非参数式。

我试图根据我的理解对所有模型进行分类，如表所示。
表格
有人可以确认我是否正确完成了此分类吗？我理解内核是非参数的，但其余两个类别呢？]]></description>
      <guid>https://stackoverflow.com/questions/78605421/classification-on-machine-learning-algorithms</guid>
      <pubDate>Tue, 11 Jun 2024 04:38:15 GMT</pubDate>
    </item>
    <item>
      <title>我创建的模型损失很大</title>
      <link>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</link>
      <description><![CDATA[我无法分享堆栈上的完整数据，因此我只会分享下面的代码。
过去几天我一直在尝试这些数据，但似乎损失在 18000 或 15000 左右，这太高了。我的同事告诉我要使用神经网络来处理这些数据。
model = Sequential([
layer.Dense(128,activation=&#39;relu&#39;),
layer.Dense(64,activation=&#39;relu&#39;),
layer.Dense(32,activation=&#39;relu&#39;),
layer.Dense(1,activation=&#39;relu&#39;)
])

model.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;mae&#39;])

model.fit(x_train, y_train,epochs=100,batch_size=80)

Epoch 1/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step -损失：242343.5312 - mae：465.7755
纪元 2/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：243483.7969 - mae：468.0649
纪元 3/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━━ 0s 29ms/步 - 损失：246071.8281 - mae： 468.4903
纪元 4/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：250888.7188 - mae：476.0695
纪元 5/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 0s 15ms/步 - 损失：242264.2188 - mae：465.4283
纪元 6/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 242441.9062 - mae: 464.7845

如何让这个loss尽可能低？
这里是我的数据集的一个示例。]]></description>
      <guid>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</guid>
      <pubDate>Mon, 10 Jun 2024 19:23:13 GMT</pubDate>
    </item>
    <item>
      <title>还有其他方法可以用来更好地识别图像中的所有细胞吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</link>
      <description><![CDATA[我有兴趣找到每个细胞的 COM 和速度。为此，我需要我的程序能够识别至少 90% 的细胞。我尝试使用 opencv 的查找轮廓函数，但结果并不令人满意。我附上了得到的结果和真实图像。你们对我可以尝试什么还有其他建议吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</guid>
      <pubDate>Mon, 10 Jun 2024 18:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Yolov4如何在指定置信度下获取map@0.50</title>
      <link>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</link>
      <description><![CDATA[我尝试在指定的conf_thresh下获取map@0.50。
我设置了不同的conf_thresh，但我得到了平均IoU和相同的map@0.50
我尝试了此代码来获取map@0.50
darknet detector map .data .cfg .weights -thresh 0.2

我得到了对于conf_thresh = 0.2 TP FP FN，平均IoU = 73.15％和map@0.50 = 96.30％。
我尝试了其他conf_thresh从0.1到0.9，我得到了相同的map@0.50 = 96.30％。
我不知道这个map@0.50 = 96.30％在以下情况下的意思conf_thresh0.1到0.9都是96.30或者使用了Yolov4默认的conf_thresh。]]></description>
      <guid>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</guid>
      <pubDate>Mon, 10 Jun 2024 15:38:42 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种 ML 模型进行销售预测？</title>
      <link>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</link>
      <description><![CDATA[给定披萨店的训练数据，其中包含历史销售数据在此处输入图片描述。
| X 形状 | Y 形状 |
| -------- | -------- |
|(1582, 13)| (1582,) |

我正在使用顺序 NN，其中有 6 个密集层，带有“relu”和 1 个输出层，带有“线性”激活函数 (128,64,32,16,8,4,1)。
我尝试添加 l2 正则化，但仍然不起作用。
Epoch 999/1000
20/20 [================================] - 0s 6ms/step - loss: 40284.5391 - mae: 163.4370 - val_loss: 96699.7188 - val_mae: 260.4841
Epoch 1000/1000
20/20 [===============================] - 0s 19ms/步 - loss: 40112.4141 - mae: 163.2846 - val_loss: 98381.9688 - val_mae: 262.3724
]]></description>
      <guid>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</guid>
      <pubDate>Mon, 10 Jun 2024 14:05:42 GMT</pubDate>
    </item>
    <item>
      <title>时间序列中具有小数据集的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78599891/machine-learning-models-with-small-datasets-in-time-series</link>
      <description><![CDATA[我正在开展一个预测项目，其中我的数据集包含 24 个月的历史销售数据。我使用 20 个月进行训练和验证，其余 4 个月进行测试。对于验证，我使用滚动预测起源（类似于 TimeSeriesSplit）。
我尝试使用几种模型，包括 SVR、GBR、随机森林、Facebook Prophet、ARIMA 和线性回归。但是，我很难找到一个正常工作的模型。我所有的预测图看起来都不一致，并且与数据不太吻合。
您能否解释为什么机器学习模型可能无法在像我这样的小数据集上表现良好，并建议任何可能更适合小数据集的技术或模型？
我所有的预测结果看起来都像图像一样，有一条直线。
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78599891/machine-learning-models-with-small-datasets-in-time-series</guid>
      <pubDate>Sun, 09 Jun 2024 22:25:42 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林对心电图数据进行验证和测试的准确率较低</title>
      <link>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</guid>
      <pubDate>Sun, 09 Jun 2024 21:43:40 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着无法遵循 Voss 提出的方法（参见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了 Wout 运算的双重求和和 arg min。有什么方法可以验证我的答案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    </channel>
</rss>