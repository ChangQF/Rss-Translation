<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 28 Aug 2024 06:23:08 GMT</lastBuildDate>
    <item>
      <title>应用模型预测时的属性错误</title>
      <link>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</link>
      <description><![CDATA[代码
arr=[[90,42,43,20.879744,82.002744,6.502985,202.935536]]
y_predict=app.predict(arr)
y_predict

第 2 行代码出错
y_predict=app.predict(arr)
AttributeError: &#39;tuple&#39; 对象没有属性 &#39;predict&#39;

第 2 行代码出错了什么]]></description>
      <guid>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</guid>
      <pubDate>Wed, 28 Aug 2024 04:31:39 GMT</pubDate>
    </item>
    <item>
      <title>预言机预测未反映外部回归量的预期行为</title>
      <link>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</guid>
      <pubDate>Wed, 28 Aug 2024 04:13:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 训练 LSTM 时，稍微增加批量大小会导致 CPU 使用率异常</title>
      <link>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</link>
      <description><![CDATA[我正在尝试使用 GPU 通过 pytorch 训练一个简单的 LSTM 模型。输入大小约为 11KB（与目标大小相同）。当我以 32 的批处理大小运行训练循环时，一切似乎都运行良好。CPU 使用率低于 10%（尽管感觉有点过高），GPU 使用率约为 30%……
当我将批处理大小增加到 48 时，CPU 使用率上升到 100%，一切都变慢了（训练时间增加了一倍）。GPU 使用率几乎没有变化，约为 30%。我检查了批处理大小为 40 的情况，这似乎没问题。在 48 时，似乎发生了一些变化并触发了持续的 CPU 使用率。其他一切似乎都很好（剩余大量 RAM，几乎没有任何磁盘活动等）。
关于如何解决此问题的任何建议或想法？在批处理大小为 48 时，总输入大小应约为 528KB。不确定这是否会在某个地方触发某种阈值。任何帮助都将不胜感激。
这是在 Windows 11 上使用 NVIDIA GPU 进行的。我希望训练能够顺利进行，直到批次大小比我看到的要大得多。]]></description>
      <guid>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</guid>
      <pubDate>Tue, 27 Aug 2024 23:03:51 GMT</pubDate>
    </item>
    <item>
      <title>学习LSTM神经网络的问题</title>
      <link>https://stackoverflow.com/questions/78920308/the-problem-with-learning-the-lstm-neural-network</link>
      <description><![CDATA[我需要创建一个模型来预测某些信号随时间变化的误差，即解决时间序列回归问题。我为此使用 LSTM：
class MyLSTM(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers, seq_length):
super(MyLSTM, self).__init__()
self.num_classes = num_classes 
self.num_layers = num_layers 
self.input_size = input_size 
self.hidden_​​size = hidden_​​size 
self.seq_length = seq_length 

self.lstm = nn.LSTM(input_size = input_size, hidden_​​size = hidden_​​size, num_layers = num_layers, batch_first = True) 
self.fc_1 = nn.Linear(hidden_​​size, 128) 
self.fc_2 = nn.Linear(128, num_classes) 
self.activatin_func = nn.Tanh()

def forward(self, x):
h_0 = 变量(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size)) 
c_0 = 变量(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size)) 
out, (hn, cn) = self.lstm(x, (h_0, c_0)) 
hn = hn.view(-1, self.hidden_​​size) 
out = self.activatin_func(hn)
out = self.fc_1(out) 
out = self.fc_2(out) 
out = self.activatin_func(out)
return out

超参数：
num_epochs = 1000 
learning_rate = 0.01 

input_size = X.shape[1] 
num_classes = y.shape[1] 

hidden_​​size = 16 
num_layers = 1 


结果，我在训练阶段得到了模型估计的相当准确的近似值，而在测试阶段得到了差异。我觉得这很奇怪，因为训练非常成功，并且评估函数在预测阶段的值没有急剧跳跃，评估曲线应该足够接近真实值，但在实践中我得到了急剧跳跃，我不明白其性质。
可能是什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78920308/the-problem-with-learning-the-lstm-neural-network</guid>
      <pubDate>Tue, 27 Aug 2024 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>我在创建聊天机器人时收到错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78920227/i-received-an-error-while-creating-the-chatbot</link>
      <description><![CDATA[如果有人可以提供帮助，我无法发布直接代码。我正在使用 Keras、TensorFlow 和 Python。我的项目涉及两个主要部分：聊天机器人的创建和聊天机器人前端的开发。提前谢谢您。
以下是图片的链接
https://i.sstatic.net/TpgppnbJ.png]]></description>
      <guid>https://stackoverflow.com/questions/78920227/i-received-an-error-while-creating-the-chatbot</guid>
      <pubDate>Tue, 27 Aug 2024 18:00:12 GMT</pubDate>
    </item>
    <item>
      <title>CNN 优化器未更新权重</title>
      <link>https://stackoverflow.com/questions/78920202/cnn-optimizer-not-updating-weights</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78920202/cnn-optimizer-not-updating-weights</guid>
      <pubDate>Tue, 27 Aug 2024 17:55:02 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型_需要建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78920194/machine-learning-model-need-advice</link>
      <description><![CDATA[`CAD_df = pd.read_csv(&#39;/kaggle/input/canadian-salary-data-from-stack-overflow-survey/CanadaData.csv&#39;, index_col = &#39;Title&#39;)

sorted_CAD=CAD_df.sort_values(by = &#39;Year&#39;, accending=True)
sorted_CAD

# 导入必要的库
从 xgboost 导入 XGBRegressor
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入 mean_absolute_error
将 seaborn 导入为 sns
将 matplotlib.pyplot 导入为 plt
%matplotlib inline
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.pipeline 导入 Pipeline

类型这里


sorted_CAD.nunique()

# 防止数据泄露
CAD_dfn = sorted_CAD.drop([&#39;Year&#39;, &#39;Country&#39;], axis=1)
CAD_dfn

# 设置训练数据和目标变量
y = CAD_dfn[&#39;Salary (USD)&#39;]
X = CAD_dfn.drop(&#39;Salary (USD)&#39;, axis = 1)

# 构建模型训练管道 &amp; fittitng
my_pipeline = Pipeline(steps = [(&#39;preprocessor&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)),
(&#39;model&#39;,XGBRegressor(n_estimators=900,learning_rate=0.05,random_state=0))])

#训练模型
my_pipeline.fit(X, y)

#由于没有测试数据，因此进行交叉验证
val_score = -1 * cross_val_score(my_pipeline, X, y, cv = 7,scoring = &#39;neg_mean_absolute_error&#39;)

val_score.mean()

&#39;&#39;&#39;该模型使用 [公司规模、行业、经验、国家、城市] 等特征预测普通加拿大人的基本工资 &#39;&#39;&#39;

&#39;该模型使用 [公司规模] 等特征预测普通加拿大人的基本工资,行业,经验,国家,城市] &#39;
val_score.mean()
5474.365167467971
&#39;&#39;&#39;这意味着模型

模型在每个预测值上的准确度为 +/- $5,474，请随意发表评论或更多想法！
&#39;&#39;&#39;
&#39;这意味着模型在每个预测值上的准确度为 +/- $5,474，请随意发表评论或更多想法！\n
我是机器学习的新手，我建立了这个模型，一直在寻求听取关于需要改进什么或需要做更多事情的第二意见？非常感谢任何反馈，谢谢。`your text``]]></description>
      <guid>https://stackoverflow.com/questions/78920194/machine-learning-model-need-advice</guid>
      <pubDate>Tue, 27 Aug 2024 17:53:58 GMT</pubDate>
    </item>
    <item>
      <title>为 AWS SageMaker 配置 IAM 角色</title>
      <link>https://stackoverflow.com/questions/78919257/configuring-an-iam-role-for-aws-sagemaker</link>
      <description><![CDATA[我正在 Udacity 上学习一门课程。对于最终项目，我必须微调 LLM。为此，他们给予 AWS 云学分。但我无法按照指示在 sagemaker 中创建 IAM 角色。我收到以下错误
未选择用于推理托管端点的实例类型。默认为 ml.g5.12xlarge。
ClientError：调用 CreateEndpointConfig 操作时发生错误 (AccessDeniedException)：
用户：arn：aws：sts::730656687357：assumed-role/SageMaker-ProjectSageMakerRole/SageMaker
无权对资源执行：sagemaker：CreateEndpointConfig：
arn：aws：sagemaker：us-west-2：730656687357：endpoint-config/meta-textgeneration-llama-2-7b-2024-08-27-13-31-54-591
在基于身份的策略中明确拒绝

在为 AWS SageMaker 配置 IAM 角色时，没有管理端点来选择它在此输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78919257/configuring-an-iam-role-for-aws-sagemaker</guid>
      <pubDate>Tue, 27 Aug 2024 14:00:06 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 迁移学习：冻结层后预训练权重的损失</title>
      <link>https://stackoverflow.com/questions/78918374/yolov8-transfer-learning-loss-of-pretrained-weights-after-freezing-layers</link>
      <description><![CDATA[经过训练后，模型可以准确预测纸板箱，但无法检测到训练前用于识别的任何其他物体。似乎预训练的权重已被覆盖或丢失。我的目标是保留检测其他物体的能力，同时专门针对纸板箱对模型进行微调。
我从 Roboflow 获得了正确标记的数据集。
https://universe.roboflow.com/dataset-t7hz7/cardboard-eupc8
为什么模型在训练后失去了检测其他物体的能力，即使我已经冻结了最初的 10 层？如何保留 YoloV8 原有的物体检测功能，同时专注于识别纸板箱？
代码：
from ultralytics import YOLO

model = YOLO(&#39;yolov8n.pt&#39;)

model.train(
data=&#39;/Users/shubhamb/IdeaProjects/transfer-learning/data/data.yaml&#39;,
epochs=25,
imgsz=416,
freeze=10,
plots=True,
#device=&quot;mps&quot;,
name=&quot;custom_yolov8_model&quot;
)
]]></description>
      <guid>https://stackoverflow.com/questions/78918374/yolov8-transfer-learning-loss-of-pretrained-weights-after-freezing-layers</guid>
      <pubDate>Tue, 27 Aug 2024 10:20:25 GMT</pubDate>
    </item>
    <item>
      <title>Fairmot 重新实现 reid 过度拟合？</title>
      <link>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</link>
      <description><![CDATA[我正在以非常简化的方式重新实现 Fairmot，我很难理解为什么在 mot17、prw、cuhksysu 和部分 caltech 行人上进行训练时，reid 分支似乎立即过度拟合，而训练损失似乎以正常方式减少。
我想到了一些原因：

我的数据比原始项目少，所以模型在 reid 上过度拟合

我正在对他们的工作进行简化的增强，没有旋转，没有裁剪，所以这意味着我的数据更少，或者至少我的数据集中的变化更少

我搞砸了 id 损失，但我几乎复制粘贴了你的函数，所以

我使用的模型有点不同，可能对于任务来说太简单了（一直被教导更简单-&gt;更少过度拟合，无论如何我使用双线性插值进行上采样而不是逆卷积，因为我无法消除棋盘效应）

验证代码实现得很糟糕，但我试图在训练集上进行验证，结果与训练损失一致

我搞砸了一些我无法理解的东西


我在这里发布了我正在重新实现模型的 colab 笔记本。
此外，如果我的代码是正确的，我正在模型部分加载在 crowdhuman 上的预训练，并且我已经修改了数据集以具有不重叠的 id。
这是我的 colab，Google 限制了数据集的下载次数，无论如何它应该可以正常运行。
https://colab.research.google.com/drive/1EVR3S6Qd0sFeRbhCYiZo5xtPZXYHsOvA?usp=sharing
这是我使用的 id 损失：
class IdLoss(nn.Module):
def init(self,n_id):
super(IdLoss, self).init()
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
self.id_classifier = nn.Linear(id_vect_size, int(n_id+1),device=device)
self.cross_entr_loss=nn.CrossEntropyLoss(ignore_index=-1)
self.dropout = nn.Dropout(0.3)

def forward(self, id_output, ind_target, mask_target, id_targets):
id_vectors = gather_feat(id_output, ind_target)
id_vectors = id_vectors[mask_target &gt; 0].contiguous()
id_vectors = F.normalize(id_vectors)
id_targets = id_targets[mask_target &gt; 0].contiguous()
id_logits = self.id_classifier(id_vectors)

return self.cross_entr_loss(id_logits, id_targets)

我尝试了空间 dropout、限制 reid 向量大小、添加增强、不同的数据集大小（但最多 45000 张图像）我添加了 50% 的对原始图像执行增强的可能性，我简化了模型，改变了学习率。无法使 reid 损失表现良好。]]></description>
      <guid>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</guid>
      <pubDate>Mon, 26 Aug 2024 21:44:44 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow-federated 0.86.0.. AttributeError: 模块‘tensorflow_federated.python.learning’没有属性‘build_federated_averaging_process [关闭]</title>
      <link>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</link>
      <description><![CDATA[--------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-21-ab950bcb167c&gt; 在 &lt;cell line: 1&gt;()
----&gt; 1 trainer = tff.learning.build_federated_averaging_process(
2 model_fn,
3 client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),
4 server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01
5 )

AttributeError: 模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”

我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</guid>
      <pubDate>Mon, 26 Aug 2024 20:32:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>Javascript 或 ML 中的自定义对象检测？</title>
      <link>https://stackoverflow.com/questions/66060711/custom-object-detection-in-javascript-or-ml</link>
      <description><![CDATA[我是对象检测领域的新手。所以不要介意我的基本问题。
我有一张身份证图像。该图像包含便携式身份证和与持卡人相关的其他信息。
目前，我只想裁剪掉身份证。
以前，我曾经以编程方式裁剪它，因为我从未想过可以更改身份证的位置。
图像如下所示：

我想裁剪底部的身份证区域。
正如我之前所说，身份证的位置、宽度和高度不是固定的。
现在我的主要问题是，我是否应该使用 ML 来实现这一点，这会不会有点过头了？
如果那么，我可以按照哪些步骤从此处的切口开始一直到底部来检测底部的 IDCard。我现在没有任何 ML 知识。
如果这确实是一种过度杀伤，那么检测卡的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/66060711/custom-object-detection-in-javascript-or-ml</guid>
      <pubDate>Fri, 05 Feb 2021 09:26:52 GMT</pubDate>
    </item>
    </channel>
</rss>