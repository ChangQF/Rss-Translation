<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 26 Nov 2024 15:19:45 GMT</lastBuildDate>
    <item>
      <title>如何在机器学习中有效处理不平衡数据集？</title>
      <link>https://stackoverflow.com/questions/79227104/how-can-i-handle-imbalanced-datasets-effectively-in-machine-learning</link>
      <description><![CDATA[我正在研究一个二元分类问题，数据集严重不平衡（90:10 类分布）。使用标准准确度指标无法提供有意义的模型性能见解。
我尝试过采样少数类，欠采样多数类，但这种方法要么导致过度拟合，要么导致信息丢失。我也尝试过成本敏感型学习，但结果并没有显著改善。
有哪些有效的技术或策略，如高级采样方法、集成学习或度量优化，可以帮助解决这个问题？您还可以推荐一些专门用于解决不平衡问题的 Python 库吗？
我正在寻找处理不平衡数据集的高级策略，包括集成方法、度量优化或预处理技术。此外，如果能就相关 Python 库或具体实现提出建议，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79227104/how-can-i-handle-imbalanced-datasets-effectively-in-machine-learning</guid>
      <pubDate>Tue, 26 Nov 2024 14:25:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用数据科学技术来识别机器学习模型中的偏见，以及这些偏见的伦理影响是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79227101/how-can-data-science-techniques-be-used-to-identify-biases-in-machine-learning-m</link>
      <description><![CDATA[数据科学方法如何发现和解决由于数据集不平衡、算法有缺陷或人类偏见而可能存在于机器学习模型中的偏见。它还深入探讨了当这些偏见影响招聘、医疗保健或刑事司法等领域的关键决策时出现的道德问题。通过解决这个问题，讨论强调了数据科学家开发公平、透明和平等的人工智能系统的责任。
一个关于数据科学在识别和解决机器学习偏见方面的作用的深思熟虑的问题。我的目的是鼓励对该领域的技术和道德层面进行批判性思考。我希望它能引发有意义的讨论或澄清数据科学的一个细微方面。]]></description>
      <guid>https://stackoverflow.com/questions/79227101/how-can-data-science-techniques-be-used-to-identify-biases-in-machine-learning-m</guid>
      <pubDate>Tue, 26 Nov 2024 14:23:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>构建 Python ML 项目目录的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-a-python-ml-project</link>
      <description><![CDATA[我想知道为 ML Python 项目构建目录的适当方法；以使项目易于阅读、维护并使包的导入正常工作。（即，从命令行和 IDE 运行时导入都能顺利运行，而无需在 PYTHONPATH 或类似的东西中硬编码更改）
我对此很陌生，我已经搜索过但还没有找到标准方法。
例如，我将以这种方式构建一个项目：（注意：这只是一个示例，用于提供想法，文件夹可能包含更多 .py 文件、具有类定义的文件......）
+---configuration_files
+---data
+---models
+---README.md
+---report.md
+---requirements.txt
+---shell_scripts
\---src
|数据清理.py
| 数据预处理.py
| 预处理数据分析.py
| 原始数据分析.py
| 测试.py
| 训练.py
|
\---模块
| __init__.py
|
+---数据分析
| 数据分析实用程序.py
| __init__.py
|
+---数据处理
| 数据加载器.py
| __init__.py
|
+---数据预处理
| 数据预处理实用程序.py
| __init__.py
|
+---通用实用程序
| 通用实用程序.py
| __init__.py
|
+---测试
| 测试实用程序.py
| __init__.py
| \---train
trainer.py.py
train_functions_1.py
train_functions_2.py
__init__.py

您对此有何看法？
例如，将必须直接运行的脚本放在 src 目录中，使导入更容易；
但是我觉得将它们放在 data_preprocessing、data_analysis、train、test 等文件夹中，每个文件夹都有各自的包和模块，这样会更简洁。但是，这会在从应该由所有人共享的 general_utils 包导入时引入问题。
此外，src、modules、utils 等使用名称的约定是什么？我应该有一个 main.py  脚本吗？ （即使我有不同的阶段）
一般来说，在行业中，构建此类项目的最有效和最广泛使用的方式是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-a-python-ml-project</guid>
      <pubDate>Tue, 26 Nov 2024 11:44:32 GMT</pubDate>
    </item>
    <item>
      <title>对患者同时发生的医学症状进行聚类[关闭]</title>
      <link>https://stackoverflow.com/questions/79226363/clustering-co-occuring-medical-symptoms-for-patients</link>
      <description><![CDATA[我获得了一个数据集，其中跟踪了不同患者的 48 种不同医学症状。这些症状每天都会记录下来，每天的严重程度评级为 0-4。
我需要将一年中不同时间点同时出现的症状聚集在一起。
哪个无监督模型能够按时间拆分数据并提供输出来解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79226363/clustering-co-occuring-medical-symptoms-for-patients</guid>
      <pubDate>Tue, 26 Nov 2024 10:50:08 GMT</pubDate>
    </item>
    <item>
      <title>如何为 LSTM 模型使用分类数据和数值数据对数据集进行窗口化？</title>
      <link>https://stackoverflow.com/questions/79225304/how-to-window-a-dataset-with-categorical-and-numerical-data-for-lstm-model</link>
      <description><![CDATA[我有一个包含 3 列的数据集：时间戳 (字符串)、Inverter_Key (字符串)（我的情况是 21 个逆变器）、能量 (整数)、温度 (整数)。我尝试创建 LSTM 模型来预测每个逆变器的能量和温度。为了提高模型的速度，请改为获取输入形状 (96,66)。我将训练和测试的数据集合并起来，使数据集为 (96,4)。但是，测试时预测与实际数据不匹配。我相信这是由于窗口化造成的，因为我已经检查了异常值和相关关系。关系显示能量和温度具有线性关系 0.78，非线性关系为 0.8。我还缩放了我的能量数据以使其变小。因此，这一定是我窗口化数据的方式。
对于数据，能量和温度将每 15 分钟记录一次。例如
时间戳、逆变器密钥、能量、温度
12/12/2022 10:15，KU，2，22
12/12/2022 10:15，KS，4，22
12/12/2022 10:30，KU，4，21
12/12/2022 10:30，KS，5，21

我使用标记技术来标记时间，编码热代码以将逆变器密钥也标记为整数。
这是我的窗口化方式：
def windowed_dataset(data, window_size=96, batch_size=128, shuffle_buffer=1000):

数据集 = tf.data.Dataset.from_tensor_slices(data) 
dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
dataset = dataset.shuffle(shuffle_buffer)
dataset = dataset.map(lambda window: (window[:-1], window[-1]))
dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(tf.data.AUTOTUNE)

返回数据集

如何创建模型并拟合模型：
def create_M_LSTM_model():
model_m_lstm = tf.keras.models.Sequential([

tf.keras.layers.LSTM(64,return_sequences=True,input_shape = input_data_shape),
#tf.keras.layers.Dropout(0.2),
tf.keras.layers.LSTM(32,return_sequences=True),
tf.keras.layers.LSTM(16,return_sequences=True),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(6),
])
return model_m_lstm

def create_model_M_LSTM():
tf.random.set_seed(51) # 设置可重复性的种子

model_create = create_M_LSTM_model() # 创建模型
model_create.compile(
loss=tf.keras.losses.Huber(), # 损失函数
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # 优化器
metrics=[&quot;mae&quot;] # 度量
)
return model_create

# 创建并总结模型
model_create_m_lstm = create_model_M_LSTM()

# 打印模型总结
model_create_m_lstm.summary()

is_train = True
if is_train:
model_create_m_lstm.fit(train_dataset, epochs=20)

如何对包含分类数据和数值数据的数据集进行窗口训练？预期数据集是，当我从预测中提取 KU 逆变器的数据时，它将与测试数据集的 KU 逆变器数据相匹配。]]></description>
      <guid>https://stackoverflow.com/questions/79225304/how-to-window-a-dataset-with-categorical-and-numerical-data-for-lstm-model</guid>
      <pubDate>Tue, 26 Nov 2024 04:13:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 opencv 和 tensorflow 解决数字验证码</title>
      <link>https://stackoverflow.com/questions/79225295/solve-a-number-captcha-with-opencv-and-tensorflow</link>
      <description><![CDATA[我正在开展一个使用 OpenCV 进行图像预处理的项目。下面是我对图像进行预处理的代码：
def preprocess_with_opencv(img):
img = img.numpy()

# 调整大小
img = cv2.resize(img, (img_width, img_height), interpolation=cv2.INTER_AREA)

# 高斯模糊
img = cv2.GaussianBlur(img, (3, 3), 0)

# 自适应阈值
img = cv2.adaptiveThreshold(
img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
)

# 更改
img = img.astype(&quot;float32&quot;) / 255.0
img = np.expand_dims(img, axis=-1)

return img

预处理后，我使用以下函数对图像进行编码以便进一步处理：
def encode_single_sample(img_path, label):
# 1. 解码
img = tf.io.read_file(img_path)

# 2. 解码图像（灰度）
img = tf.io.decode_png(img, channels=1)

# 3. 使用 OpenCV 进行转换
img = tf.py_function(preprocess_with_opencv, [img], Tout=tf.float32)

# 4. 转置问题解决
img = tf.transpose(img, perm=[1, 0, 2])

# 5. 转换 dictionary (image, label)
return {&quot;image&quot;: img, &quot;label&quot;: label}

原始图像如下所示：

但是，使用 encode_single_sample 函数处理图像后，数据似乎不正确，输出图像不清晰。如何解决此问题并确保处理后的图像准确清晰？]]></description>
      <guid>https://stackoverflow.com/questions/79225295/solve-a-number-captcha-with-opencv-and-tensorflow</guid>
      <pubDate>Tue, 26 Nov 2024 04:08:47 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks 中使用 MLflow 记录模型时排除私有包依赖关系</title>
      <link>https://stackoverflow.com/questions/79224603/exclude-private-package-dependency-when-logging-model-with-mlflow-in-databricks</link>
      <description><![CDATA[我正在使用 Databricks Asset Bundles (DAB) 部署 Databricks 工作流。我的集群上安装了一个自定义包 (mypackage)，其中包含用于不同工作流的多个 CLI 命令。此包包含我的工作流所需的所有依赖项。请注意，mypackage 是一个私有 PyPI 存储库，无法从外部访问。
当我使用 mlflow.pyfunc.log_model() 记录我的模型时，MLflow 会自动检测并将 mypackage.submodule 作为模型的依赖项。这会导致问题，因为：

mypackage 是私有的，无法从外部环境安装。
该模型实际上不需要 mypackage 或其依赖项进行推理 - 它只需要 PyTorch 和 Pandas 等标准库。

这是我的代码的简化版本：
import mlflow.pyfunc
from mypackage.submodule import MyModelClass

model = MyModelClass()

mlflow.pyfunc.log_model(
python_model=model,
artifact_path=&quot;my_model&quot;,
# 其他参数
)

问题：

如何使用 mlflow.pyfunc.log_model() 记录我的模型，而不包括mypackage 作为依赖项？

是否有 MLflow 原生方法可以防止某些包包含在模型环境中？

是否有最佳实践来处理训练环境包含不应成为序列化模型依赖项的包的情况？


其他上下文：

我正在使用 Databricks 和 Databricks Asset Bundles 进行部署。
该模型使用推理所需的标准库（例如 PyTorch、Pandas）。
mypackage 仅在训练和工作流程编排期间使用，但不需要用于模型推理。


我尝试过的：
作为解决方法是，我尝试了一种破解方法，在 __main__ 模块中重新定义我的模型类，并使用 cloudpickle 对其进行序列化，本质上是删除对 mypackage 的引用。这是我所做的片段：
import inspect
import cloudpickle

# 提取模型类的源代码
model_source = inspect.getsource(MyModelClass)

# 创建主命名空间并在 __main__ 中重新定义类
main_namespace = {
&#39;__name__&#39;: &#39;__main__&#39;,
# 包含其他必要的导入
}
exec(model_source, main_namespace)

# 访问重新定义的类
MyModelClassMain = main_namespace[&#39;MyModelClass&#39;]
model = MyModelClassMain()

# 序列化模型
with open(&quot;model.pkl&quot;, &quot;wb&quot;) as f:
cloudpickle.dump(model, f)

# 记录模型而不包括“mypackage”
mlflow.pyfunc.log_model(
python_model=model,
articulate_path=&quot;my_model&quot;,
artifacts={&quot;model.pkl&quot;: &quot;model.pkl&quot;},
pip_requirements=[&quot;torch&quot;, &quot;pandas&quot;, &quot;cloudpickle&quot;],
# 其他参数
)

虽然这种方法有效，但感觉像是一种黑客解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79224603/exclude-private-package-dependency-when-logging-model-with-mlflow-in-databricks</guid>
      <pubDate>Mon, 25 Nov 2024 21:09:57 GMT</pubDate>
    </item>
    <item>
      <title>弹性净惩罚分位数回归性能不佳的原因及潜在解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/79224486/reasons-and-potential-solutions-for-poor-performance-of-elastic-net-penalized-qu</link>
      <description><![CDATA[我正在对我的数据集执行弹性净惩罚分位数回归 (EN-QR)，该数据集有 6,782 行和 227 列（即预测因子）。其中 195 个预测因子是代谢物，我的目标是找出哪些代谢物与接触空气污染物 PM2.5 有关。我选择执行 QR 而不是线性回归，因为 PM2.5 数据存在偏差且不呈正态分布，即使在执行对数转换和标准化之后也是如此。我将数据集按 70:30 的比例分为训练集 (n = 4,747) 和测试集 (n = 2,035)。
我遇到的问题是，当我将训练集中的 EN-QR 对象应用到测试集以预测 PM2.5 值（下图）时，我得到了一个奇怪的多峰分布，它与测试集中的实际 PM2.5 值不符（上图）。我已将这些分布的屏幕截图附在下面。

我不一定在寻找可以解决这个问题的人，但我想知道是否有人对此有解决问题的建议。模型会分裂成多峰分布而不是（近似）正态分布的一些潜在原因是什么？有没有我没有考虑包含在训练集模型中的东西？我是否应该研究其他策略来实现我的变量选择目标？理想情况下，我希望 EN-QR 模型能够发挥作用，因为它为我的研究问题提供了最多的信息。
我尝试根据数据属于多峰分布的哪个模式将数据分成四个集群。我发现美国人口普查区域与预测的 PM2.5 值的模式之间存在关系。例如，几乎所有 z 得分最低的模式中的人都来自南部，而几乎所有 z 得分最高的模式中的人都来自中西部。我觉得这很奇怪，因为美国人口普查区域被纳入为协变量，因此模型不应该按地区分层。我在下面附上了这张图片和表格。

这是我用来在训练集上执行 EN-QR 的 R 聊天。 Behat 是我用来预测测试集中 PM2.5 值的代码。
trainingset.ENQR &lt;- rq.pen.cv(
x = as.matrix(trainingset[, c(4:5, 27, 29:251)]),
y = trainingset[, 18],
tau = c(0.1, 0.5, 0.9),
penalty = &quot;ENet&quot;,
a = c(0.50, 0.75, 0.90),
nfolds = 10,
printProgress = TRUE,
penalty.factor = c(rep(0, 32), rep(1, 194))
)

lambda.min.training &lt;- trainingset.ENQR$gtr$lambda1se[2]

testingset.ENQR.50 &lt;- rqPen:::predict.rq.pen.seq.cv(object = trainingset.ENQR,
newx = as.matrix(testingset[, c(4:5, 27, 29:251)]),
tau = 0.50,
a = 0.50,
lambda = lambda.min.training,
cvmin = FALSE)
]]></description>
      <guid>https://stackoverflow.com/questions/79224486/reasons-and-potential-solutions-for-poor-performance-of-elastic-net-penalized-qu</guid>
      <pubDate>Mon, 25 Nov 2024 20:14:44 GMT</pubDate>
    </item>
    <item>
      <title>统计学习混淆表变量</title>
      <link>https://stackoverflow.com/questions/79224395/statistical-learning-confusion-table-variable</link>
      <description><![CDATA[我的混淆表中出现了一个额外的变量，不知道它从何而来。
数据集“默认”具有以下列：默认、学生、收入、余额
变量“默认”具有两个值：“是”和“否”
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
summary,
poly)
from ISLP import chaos_table

Default = load_data(&#39;Default&#39;)
vars = Default.columns.drop([&#39;default&#39;])
y = Default[&#39;default&#39;] == &#39;是&#39;
design = MS(vars)
X = design.fit_transform(Default)
glm = sm.GLM(y,
X,
family = sm.families.Binomial())
results = glm.fit()
summarize(results)
probs = results.predict()
labels = np.array([&#39;No&#39;]*10000)
labels[probs&gt;0.5] = &#39;Yes&#39;
confusion_table(labels,Default.default)

在输出中，我得到一个 3x3 表，其中包含变量“No”、“Yes”和“Ye”
我希望混淆表值仅为“Yes”和“No”。不知何故，numpy.array“labels”设置为“Ye”而不是“Yes”。]]></description>
      <guid>https://stackoverflow.com/questions/79224395/statistical-learning-confusion-table-variable</guid>
      <pubDate>Mon, 25 Nov 2024 19:35:27 GMT</pubDate>
    </item>
    <item>
      <title>当我 pickle ML 模型并将其注释掉时出现 FileNotFoundError</title>
      <link>https://stackoverflow.com/questions/79224304/filenotfounderror-when-i-pickle-my-ml-model-and-comment-it-out</link>
      <description><![CDATA[尝试为数据项目加载 pickle ML 模型时，我收到一条错误消息。如何解决？
我有正确的文件路径，并且已经确认了路径“/home/jovyan/work”，并且之前加载了 pickle。
我不知道这是否是云问题，但是当我单独注释掉 pickle 的写入时，或者如果我注释掉 pickle 的写入和模型的拟合...出于某种原因，我无法简单地加载 pickle。这就是我每次都要对模型进行 pickle 处理，而不必拟合模型的原因。
这些是我正在使用的函数：
def write_pickle(path, model_object, save_as:str): 

with open(path + save_as + &quot;.pickle&quot;, &quot;wb&quot;) as to_write: 

pickle.dump(model_object, to_write)

def read_pickle(path, saved_model_name:str):

with open(path + saved_model_name + &#39;.pickle&#39;, &#39;rb&#39;) as to_read:

model = pickle.load(to_read)

return model

这些函数适用于文件路径和所有内容，但当我注释掉 pickle 的写入或注释掉 ML 模型的拟合时……我收到了 FileNotFoundError。我不知道问题可能出在哪里。如果我不能弄清楚，那么我甚至连 pickle 模型都无用了。]]></description>
      <guid>https://stackoverflow.com/questions/79224304/filenotfounderror-when-i-pickle-my-ml-model-and-comment-it-out</guid>
      <pubDate>Mon, 25 Nov 2024 19:00:17 GMT</pubDate>
    </item>
    <item>
      <title>错误：TypeError：无法将 cuda:0 设备类型张量转换为 numpy。首先使用 Tensor.cpu() 将张量复制到主机内存</title>
      <link>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor-c</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor-c</guid>
      <pubDate>Sat, 23 Nov 2024 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    </channel>
</rss>