<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 21:15:41 GMT</lastBuildDate>
    <item>
      <title>如何使用 llm 对象通过单个脚本使用 vLLM 在多个 gpu 上加载多个模型？</title>
      <link>https://stackoverflow.com/questions/79016077/how-does-one-load-multiple-models-on-multiple-gpus-with-vllm-with-a-single-scrip</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79016077/how-does-one-load-multiple-models-on-multiple-gpus-with-vllm-with-a-single-scrip</guid>
      <pubDate>Mon, 23 Sep 2024 19:45:51 GMT</pubDate>
    </item>
    <item>
      <title>如何实现多元 N 节拍模型</title>
      <link>https://stackoverflow.com/questions/79015943/how-to-implement-multivariate-n-beats-model</link>
      <description><![CDATA[对于基于日期和一个特征的单变量，我们可以创建，但我有 6 个特征，如何将它们实现到单个 N 节拍模型中
我有库存特征，6 个特征和一个目标变量。我应该怎么做，如何训练？]]></description>
      <guid>https://stackoverflow.com/questions/79015943/how-to-implement-multivariate-n-beats-model</guid>
      <pubDate>Mon, 23 Sep 2024 18:52:34 GMT</pubDate>
    </item>
    <item>
      <title>我们如何在 R 中计算一个数据集中的一个记录与第二个数据集中的所有记录之间的 Gower 距离？</title>
      <link>https://stackoverflow.com/questions/79015729/how-do-we-calculate-the-gower-distance-between-one-record-in-one-dataset-and-all</link>
      <description><![CDATA[我想计算数据集 1 的一条记录与数据集 2 的所有记录之间的 Gower 距离。第一种方法如下
library(gower)

data(iris)
dat1 &lt;- iris[1:10,]
dat2 &lt;- iris[11:30,]

# 第一种方法
gower::gower_dist(dat1[1,], dat2)

这给了我长度为 20 的结果。
0.09079365 0.09873016 0.16142857 0.29476190 0.20920635 0.33079365 
0.21936508 0.05000000 0.23952381 0.11507937 0.12095238 0.15079365 
0.16984127 0.24523810 0.16539683 0.12920635 0.17206349 0.03555556 
0.02761905 0.14063492

我可以将第一个值解释为 dat1[1,] 和 dat2[1,] 之间的 gower 距离，将第二个值解释为 dat1[1,] 和 dat2[2,] 之间的 gower 距离，依此类推吗？
让我感到困惑的是，如果我计算
gower::gower_dist(dat1[1,],dat2[1,])

这给了我
0.75

这与 0.09079365 不同。最终，我想计算 dat1 中每个观测值与 dat2 中每个观测值的 Gower 距离。如果我使用第二种方法，我将需要在 dat2 中的所有观测值上添加一个 for 循环，如下所示。
# 第二种方法
for(i in 1:nrow(dat2)) {
print(gower::gower_dist(dat1[1,],dat2[i,]))
}

由于这两种方法给出的结果不同，我应该使用哪一种方法来实现目的？]]></description>
      <guid>https://stackoverflow.com/questions/79015729/how-do-we-calculate-the-gower-distance-between-one-record-in-one-dataset-and-all</guid>
      <pubDate>Mon, 23 Sep 2024 17:37:36 GMT</pubDate>
    </item>
    <item>
      <title>有人知道如何修复库错误吗？</title>
      <link>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</link>
      <description><![CDATA[早上好，我遇到了这个错误，尝试了所有方法，但还是无法解决，如果有人知道 crewapi 库是什么，那将非常有帮助。
我正在尝试最优解决方案以继续
这是代码和错误
ModuleNotFoundError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-6c7180fd4822&gt; in &lt;cell line: 1&gt;()
----&gt; 1 from crewapi_module import CrewAPI # 用正确的导入路径替换
2 
3 crew_api = CrewAPI(api_key=&quot;your_crewai_api_key&quot;)
4 
5 # 现在您可以与 API 交互，例如：

ModuleNotFoundError：没有名为“crewapi_module”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</guid>
      <pubDate>Mon, 23 Sep 2024 15:52:13 GMT</pubDate>
    </item>
    <item>
      <title>受不同聚类大小约束的 KMeans</title>
      <link>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</link>
      <description><![CDATA[我有一个包含商店坐标的数据框，我想根据供应商应该访问该商店的日期将它们划分为簇。例如，假设供应商应该访问 180 家商店。他应该在周一到周五访问 30-34 家商店，周六，他应该访问其他日子的 60%。
你们知道我该怎么做吗？使用 kmeans-constrained，我只能将它们划分为大小相等的簇。也许我需要使用某种解算器或在集群之间移动点以达到我想要的数字，但我不知道如何做到这一点。
以下是将它们均等划分的代码：
# 循环遍历供应商集群
for vendor in df[&quot;vendor&quot;].unique():

# 每个供应商的商店数量
n_shops = df.loc[df[&quot;vendor&quot;] == vendor][&quot;cod_shop&quot;].count()

# 索引
idx = df.loc[df[&quot;vendor&quot;] == vendor].index

# 一周中各天的集群数量
num_clusters = 5 # 星期一至星期五

# 集群的平均大小
avg_size = n_shops / (num_clusters + 0.6)

# 定义限制
min_shops = round(avg_size - n_shops * pct, 0)
max_shops = math.ceil(avg_size + n_shops * pct)

# 模型
kmeans = KMeansConstrained(n_clusters=num_clusters, size_min=min_shops, size_max=max_shops, random_state=42)
labels = kmeans.fit_predict(df.loc[df[&quot;vendor&quot;] == vendor][[&quot;latitude&quot;, &quot;longitude&quot;]])

# 向数据框添加标签
df.loc[idx, &quot;visit_day&quot;] = labels
]]></description>
      <guid>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</guid>
      <pubDate>Mon, 23 Sep 2024 14:42:09 GMT</pubDate>
    </item>
    <item>
      <title>图像拼接中的泊松混合导致图像模糊、鬼影重重</title>
      <link>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</guid>
      <pubDate>Mon, 23 Sep 2024 14:11:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 coremltools 将 TensorFlow ConcreteFunction 或 AutoTrackable 对象转换为 Core ML？</title>
      <link>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</link>
      <description><![CDATA[我正在尝试使用 coremltools 将 TensorFlow 对象检测模型 (ssd_mobilenet_v1_coco) 转换为 Core ML 格式。该模型采用 SavedModel 格式，但我在尝试转换时遇到了各种问题。以下是我到目前为止采取的步骤以及我得到的错误：
我到目前为止所做的：

加载了 TensorFlow SavedModel：
import tensorflow as tf

model = tf.saved_model.load(&quot;ssd_mobilenet_v1_coco_2017_11_17/saved_model&quot;)
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]`



尝试使用 coremltools 将模型转换为 Core ML：


mlmodel = ct.convert(concrete_func, source=&quot;tensorflow&quot;)
mlmodel.save(&quot;ssd_mobilenet_v1_coco.mlmodel&quot;) 

尝试转换 ConcreteFunction 时，我不断收到以下错误：
NotImplementedError：预期模型格式：[SavedModel | concrete_function | tf.keras.Model | .h5 | GraphDef]，得到了 ConcreteFunction

我尝试将模型导出为 SavedModel 并将该路径直接传递给 coremltools：
tf.saved_model.save(model, &quot;exported_saved_model&quot;)
mlmodel = ct.convert(&quot;exported_saved_model&quot;, source=&quot;tensorflow&quot;)

如何使用 coremltools 成功将此 TensorFlow 模型（或 ConcreteFunction）转换为 Core ML？是否有特定的方法来处理 AutoTrackable 或 ConcreteFunction 对象？在此转换过程中我遗漏了什么或做错了什么？
其他信息：
• TensorFlow 版本：X.X.X（例如 2.10.0）
• coremltools 版本：X.X.X（例如 5.0b3）
• Python 版本：X.X.X（例如 3.10）
]]></description>
      <guid>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</guid>
      <pubDate>Mon, 23 Sep 2024 13:36:52 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何方法可以扩展我们的数据集，从具有 200 行的初始数据集开始。我希望从中至少获得 2000 行来应用 ML 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</link>
      <description><![CDATA[我的项目是利用 ML 技术预测抑郁程度或向孩子的父母提出一些预防措施建议。
我想应用 ML 模型根据我们的数据集预测抑郁程度或其症状。因此，我需要至少 2000 个训练数据元组来训练它们。我怎样才能在不改变属性之间的关系（相关性）的情况下实现这一点。
我的数据集包含许多属性，例如屏幕时间、抑郁程度。如何使用一些代码来解决这个问题。
我尝试使用 CTGAN，但它给了我很多错误。]]></description>
      <guid>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</guid>
      <pubDate>Mon, 23 Sep 2024 12:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 实现基本神经网络的问题</title>
      <link>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</guid>
      <pubDate>Mon, 23 Sep 2024 09:49:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个人脸 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)


有人能帮我理解为什么即使 ROI 已被裁剪，该错误仍然持续存在吗？解决该错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 tch-rs 在 rust 中实现残差神经网络</title>
      <link>https://stackoverflow.com/questions/79006068/residual-neural-network-in-rust-with-tch-rs</link>
      <description><![CDATA[我正在尝试使用 tch-rs (Torch) 在 rust 中实现前馈残差神经网络。
到目前为止，这是我的代码：（这是一个最小的可重现示例）
use tch::{nn::{self, batch_norm1d, layer_norm, BatchNormConfig, ConvConfigND, LayerNormConfig, Module, ModuleT}, Tensor};
const NUM_HIDDEN: i64 = 10;

fn res_block(vs: &amp;nn::Path) -&gt; impl ModuleT {
let mut default = ConvConfigND::default();
default.padding = 1;
let conv1 = nn::conv1d(vs, NUM_HIDDEN, NUM_HIDDEN, 3, default);
让 bn1 = batch_norm1d(vs, NUM_HIDDEN, BatchNormConfig::default());
让 conv2 = nn::conv1d(vs, NUM_HIDDEN, NUM_HIDDEN, 3, default);
让 bn2 = batch_norm1d(vs, NUM_HIDDEN, BatchNormConfig::default());
nn::func_t(|x,train| {
let mut residual = Tensor::new();
x.clone(&amp;residual);
let x = bn1.forward_t(&amp;conv1.forward(x),train).relu();
let x = bn2.forward_t(&amp;conv2.forward(&amp;x),train);
let x = x + residual;
return x.relu();
})
}

当我编译此代码时，出现此错误：
`*mut torch_sys::C_tensor` 无法在线程之间安全地共享
在 `BatchNorm` 中，`*mut torch_sys::C_tensor` 未实现特征 `Sync`，而这是 `{closure@src\nn.rs:11:16: 所要求的11:25}：`&amp;BatchNorm` 实现 `Send` 所需的 Send

当我将 forward_t 行放入 func_t 中时，会发生此问题。
我该如何让它工作？
我也尝试使用顺序网络，但它们无法进一步传递残差变量。有没有办法让它工作？还是我需要做其他事情？]]></description>
      <guid>https://stackoverflow.com/questions/79006068/residual-neural-network-in-rust-with-tch-rs</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:36 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>gym_super_mario_bros 的 DummyVecEnv 构造函数存在问题</title>
      <link>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</link>
      <description><![CDATA[摘要：我想要做的就是使用 DummyVecEnv 构造函数来包装我的 gym 环境，即
env = DummyVecEnv([lambda: env])，但执行此操作时我不断收到错误。当前使用 https://pypi.org/project/gym-super-mario-bros/ 作为 Super Mario 的 gym 包装器。我感觉我误解了我应该为构造函数提供哪些参数，但我是 Python 新手，很难理解我遗漏了什么。
我在从 gym_super_mario_bros.make() 返回的环境中使用 DummyVecEnv 构造函数时遇到困难。我一直收到错误“您尝试创建多个环境，但创建它们的函数返回了相同的实例，而不是创建不同的对象”。
我最初尝试将我的环境包装在 DummyVecEnv 中，就像我在许多论坛上看到的那样：
env = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
env = JoypadSpace(env, SIMPLE_MOVEMENT) 
env = GrayScaleObservation(env, keep_dim=True) 
env = DummyVecEnv([lambda: env]) #This line 错误

但我收到此错误：
文件c:\Users\truem\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:30，在 DummyVecEnv.init(self, env_fns) 中
29 def init(self, env_fns: List[Callable[[], gym.Env]]):
---&gt; 30 self.envs = [_patch_env(fn()) for fn in env_fns]
31 if len(set([id(env.unwrapped) for env in self.envs])) != len(self.envs):
32 raise ValueError(
33 &quot;您尝试创建多个环境，但创建它们的函数返回了同一个实例&quot;
34 &quot;而不是创建不同的对象。&quot;(...)
39 &quot;请阅读 https://github.com/DLR-RM/stable-baselines3/issues/1151 了解更多信息。&quot;
40 )

因此，我尝试按照 github 链接中列出的修复程序以及文档中提供的示例进行操作
https://stable-baselines.readthedocs.io/en/master/guide/examples.html
我继续创建了一个新的辅助函数 create_default_environment，它应该创建新的 env 实例并返回它们，但尽管如此，它仍然不起作用。
这是我的代码现在的样子：
#将我的 env 创建包装在函数中
def create_default_environment():
newEnv = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
newEnv = JoypadSpace(newEnv, SIMPLE_MOVEMENT) 
return GrayScaleObservation(newEnv, keep_dim=True)
#在调用辅助函数的列表中创建 lambda
env = DummyVecEnv([lambda: create_default_environment()]) #仍然失败

我继续将我的初始 env 构造包装在辅助函数 create_default_environment() 中，我已经验证该函数每次调用都会返回一个新的 env 实例。然后我使用该包装器插入 DummyVecEnv 构造函数：
env = DummyVecEnv([lambda: create_default_environment()])
但编译器仍然抱怨我传入的函数列表没有返回新实例。
我一直在尝试模拟我在 baselines 提供的示例代码中看到的内容：
from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines import PPO2

env = DummyVecEnv(\[lambda: gym.make(&quot;HalfCheetahBulletEnv-v0&quot;)]) #为什么这个可以工作而我的不行？

但我不确定我做错了什么。我是 python 新手，所以如果我遗漏了什么明显的东西，请原谅我。]]></description>
      <guid>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</guid>
      <pubDate>Fri, 01 Mar 2024 05:53:20 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：多标签指示器不支持混淆矩阵</title>
      <link>https://stackoverflow.com/questions/76635503/valueerror-multilabel-indicator-is-not-supported-confusion-matrix</link>
      <description><![CDATA[我尝试运行时收到的错误消息是“multilabel-indicator 不受支持”：
您能给我任何解决方案或提示吗？
import seaborn as sns
sns.heatmap(confusion_matrix(y_test, y_pred), annot = True)

ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-21-ee6823d584e9&gt; in &lt;cell line: 1&gt;()
----&gt; 1 sns.heatmap(confusion_matrix(y_test, y_pred), annot = True)

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py infusion_matrix(y_true, y_pred, labels, sample_weight, normalize)
317 y_type, y_true, y_pred = _check_targets(y_true, y_pred)
318 if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):
--&gt; 319 raise ValueError(&quot;%s is not supports&quot; % y_type)
320 
321 if labels is None:

ValueError: multilabel-indicator is not supports
]]></description>
      <guid>https://stackoverflow.com/questions/76635503/valueerror-multilabel-indicator-is-not-supported-confusion-matrix</guid>
      <pubDate>Fri, 07 Jul 2023 09:06:01 GMT</pubDate>
    </item>
    <item>
      <title>开源神经网络库 [关闭]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[我正在寻找一个开源神经网络库。到目前为止，我已经研究过 FANN、WEKA 和 OpenNN。我还应该看看其他的吗？当然，标准是文档、示例和易用性。]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>