<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 12 Sep 2024 15:16:50 GMT</lastBuildDate>
    <item>
      <title>使用电子邮件 ID 识别垃圾邮件用户和真正用户 [关闭]</title>
      <link>https://stackoverflow.com/questions/78976869/identifying-spam-users-and-genuine-users-using-email-ids</link>
      <description><![CDATA[我正在开发一种机器学习算法，用于检测电子邮件 ID 是真是假。我已经探索了一些可以帮助我区分两者的功能。我在网上搜索了相关的研究论文，以便了解真实用户邮件 ID 中的字符估计数量、预期的字母与数字的比率等，但找不到。有人能帮我吗？
我探索了一些可以使用的功能。我在下面列出了这些功能-
1.用户名的长度：用户名部分（&#39;@&#39; 符号之前）的字符数。真正的电子邮件 ID 可能遵循一定的长度模式，可能与假冒电子邮件 ID 不同。
2.特殊字符数：电子邮件 ID 中特殊字符的数量，例如点、下划线和连字符。总计数和用户名或域内的计数都可以提供深刻的见解。
3.域名：电子邮件的域名（&#39;@&#39;符号后）可以是一个强有力的指标，特别是如果您可以识别通常与临时或一次性电子邮件服务相关的域名。
4.顶级域名（TLD）：顶级域名（.com、.org、.net 等）也可能提供线索，因为某些 TLD 可能在真实或虚假的电子邮件 ID 中更为普遍。
5.数字与字母的比率：用户名部分的数字与字母的比率。假冒电子邮件 ID 的比率可能高于或低于真实电子邮件 ID。
6.连续字符的存在：连续相同字符的存在和数量，这可能在随机生成的（假）电子邮件 ID 中更常见。
7.域名使用频率：如果您有电子邮件 ID 数据集，每个域名出现的频率可能有助于识别不太常见、可能可疑的域名。对于这些，您是否对真实用户和虚假用户进行了有效的研究，或者您可以给我一篇文章的链接。]]></description>
      <guid>https://stackoverflow.com/questions/78976869/identifying-spam-users-and-genuine-users-using-email-ids</guid>
      <pubDate>Thu, 12 Sep 2024 07:05:16 GMT</pubDate>
    </item>
    <item>
      <title>建议一些可微调的预训练对象检测模型以使用自定义数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78976857/suggest-some-fine-tunable-pretrained-object-detection-model-for-using-custom-dat</link>
      <description><![CDATA[我需要找到一个用于识别家庭内物体的物体检测模型。该模型应该经过预先训练，并且应该可以针对家居物品进行微调。您能推荐一些好的模型吗？
我尝试了 Yolov10、EfficientDET 和 RetinaNet。对它们的结果不满意。]]></description>
      <guid>https://stackoverflow.com/questions/78976857/suggest-some-fine-tunable-pretrained-object-detection-model-for-using-custom-dat</guid>
      <pubDate>Thu, 12 Sep 2024 07:02:43 GMT</pubDate>
    </item>
    <item>
      <title>可以对研究论文进行情绪分析吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78976695/can-a-sentiment-analysis-on-research-paper-be-done</link>
      <description><![CDATA[我开始开发一个情感分析工具，它将分析研究论文中表达的情感，尽管研究论文中写的语言非常微妙和温和。
我正在使用各种在线资源，但仍然很难达到我想要的输出。
我正在使用预先训练的 BERT 模型，因为我已经决定将其规模非常小，这是我大学最后一年的项目。
请分享任何有用的见解、经验和知识。
请分享任何有用的见解、经验和知识。]]></description>
      <guid>https://stackoverflow.com/questions/78976695/can-a-sentiment-analysis-on-research-paper-be-done</guid>
      <pubDate>Thu, 12 Sep 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>在 ML Studio 中使用本地计算机作为 Azure AutoML 的计算目标</title>
      <link>https://stackoverflow.com/questions/78976497/using-local-computer-as-a-compute-target-for-azure-automl-in-ml-studio</link>
      <description><![CDATA[是否可以将本地计算机作为计算目标连接到 Azure 机器学习工作区，并将其用作 Azure ML Studio 中 AutoML 的计算目标，而不是使用计算群集？我尝试浏览文档，但找不到任何有用的东西。有人可以指导我完成这个过程或分享代码片段/任何相关文档吗？]]></description>
      <guid>https://stackoverflow.com/questions/78976497/using-local-computer-as-a-compute-target-for-azure-automl-in-ml-studio</guid>
      <pubDate>Thu, 12 Sep 2024 04:44:02 GMT</pubDate>
    </item>
    <item>
      <title>测量肩角[关闭]</title>
      <link>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</link>
      <description><![CDATA[https://i.sstatic.net/tC8S69ry.jpg
我必须用 Python 测量每个肩膀的角度。什么是以最高精度测量的最佳方法？我对此很陌生，所以欢迎提出任何建议。
我尝试了 Media pipe，但它只提供骨架点。我需要测量肩部边缘的角度。（如图所示）]]></description>
      <guid>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</guid>
      <pubDate>Thu, 12 Sep 2024 03:05:15 GMT</pubDate>
    </item>
    <item>
      <title>尽管分类报告很好，但模型无法正确预测</title>
      <link>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</link>
      <description><![CDATA[我尝试从链接运行此模型：
https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook
当我在 colab 上使用类似数据集（但较小，2100 张图片到 6 个类）执行此操作时，效果很好。但是当我添加此代码来预测输入图像时：
from google.colab import files
from PIL import Image

def process_uploaded_image(image_path, target_size=(224, 224)):
img = Image.open(image_path)
img = img.resize(target_size) 
img_array = np.array(img) 

if img_array.shape[-1] == 4: 
img_array = img_array[..., :3]

img_array = img_array / 255.0 
img_array = np.expand_dims(img_array, axis=0) 
img_array = mobilenetv2.preprocess_input(img_array) 

return img_array

uploaded = files.upload()

for fn in uploaded.keys(): 
processed_image = process_uploaded_image(fn, target_size=IMAGE_SIZE) 
preds = model.predict(processed_image)
pred_class = np.argmax(preds, axis=1)

plt.imshow(Image.open(fn)) # 显示上传的图片
plt.title(f&#39;预测的类别：{categories[pred_class[0]]}&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(f&#39;文件 {fn} 被预测为：{categories[pred_class[0]]}&#39;)

结果是错误的预测。例如，模型总是将我的输入预测为“垃圾”类。当我停止运行时，它会更改为另一个类，但它仍然处于错误的预测中。
我还添加了此代码来检查预测概率：
preds = model.predict(processed_image)
pred_probs = preds[0] # 获取第一个（也是唯一一个）批次的预测概率
print(&quot;Prediction probabilities:&quot;, pred_probs)
pred_class = np.argmax(pred_probs)
print(&quot;Predicted class:&quot;, categories[pred_class])

输出：
**1/1** ━━━━━━━━━━━━━━━━━━━━━━ **0s** 24ms/步 预测概率：\[0.31027108 0.12315894 0.47848797 0.00863316 0.07789086 0.00155797\] 
预测类别：金属 

为什么会发生这种情况，我的模型如何正确预测结果？]]></description>
      <guid>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</guid>
      <pubDate>Thu, 12 Sep 2024 02:58:40 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure 中创建数据资产但出现此错误</title>
      <link>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</link>
      <description><![CDATA[我正在尝试创建 ML 数据资产，我正在关注此链接：https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html
以完成此操作，但我不断收到此错误

我知道我的文件与示例中的文件不完全相同，但即使我使用这些文件，我也会收到相同的错误。所以我不确定问题是什么，因为这是我第一次在 Azure 中使用 ML。]]></description>
      <guid>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</guid>
      <pubDate>Thu, 12 Sep 2024 00:30:32 GMT</pubDate>
    </item>
    <item>
      <title>排列检验的准确率非常高[关闭]</title>
      <link>https://stackoverflow.com/questions/78975982/accuracy-for-permutation-test-is-very-high</link>
      <description><![CDATA[我对我的分类器发生了什么有点困惑。
我有一个包含约 220 个特征和大约 4000 次试验的数据集。类别是完美平衡的，我正在使用具有 L1 范数的 SVC 执行一个简单的二元分类任务。
当我执行 LOOCV 时，我获得了不错的准确度，但是当我想检查置换数据时，我也会获得相同的 CV 准确度 +/- 2%。
这感觉很奇怪，因为当我使用随机数据但保留基本结构（220 X 4000）运行模拟时，我的准确度并没有明显高于 50%。对我来说，这意味着过度拟合不会自动发生，但它让我很困惑为什么我会得到这些奇怪的结果。
我遗漏了什么可以解释这一点吗？
初始测试代码：
n_samples = len(df)

reg_list = [.1]
for c in reg_list:
# 创建合成组标签（每个样本都是自己的组）
groups = df[&#39;run_value&#39;]
cv = LeaveOneOut()
svc = LinearSVC(penalty=&quot;l1&quot;, C=c, dual=False) # 使用 LinearSVC 的“l1”惩罚时 dual=False

decoder = Decoder(
estimator=svc,
mask=ffa_mask,
standardize=&quot;zscore_sample&quot;,
cv=cv,
评分=“accuracy”，
)

# 拟合解码器
decoder.fit(df[0].values, df[&#39;0_y&#39;].values)

# 输出结果
print(f“C={c}”)
# 访问每个类的交叉验证分数
cv_scores_class_0 = 解码器.cv_scores_[0]
cv_scores_class_1 = 解码器.cv_scores_[1]

# 计算两个类的平均交叉验证准确率
mean_score_class_0 = np.mean(cv_scores_class_0)
mean_score_class_1 = np.mean(cv_scores_class_1)
mean_score = np.mean([mean_score_class_0, mean_score_class_1])
print(f“类的平均 CV 分数0: {mean_score_class_0}&quot;)
print(f&quot;类别 1 的平均 CV 得分：{mean_score_class_1}&quot;)
print(f&quot;平均 CV 得分：{mean_score}&quot;)
# print(f&quot;C={c} 的权重：{svc.coef_}&quot;)

和排列：
n_samples = len(df)

reg_list = [.1]
for c in reg_list:
# 创建合成组标签（每个样本都是自己的组）
groups = df[&#39;run_value&#39;]
cv = LeaveOneOut()
svc = LinearSVC(penalty=&quot;l1&quot;, C=c, dual=False) # 使用 LinearSVC 的“l1”惩罚时 dual=False

decoder =解码器（
estimator=svc,
mask=ffa_mask,
standardize=&quot;zscore_sample&quot;,
cv=cv,
scoring=&quot;accuracy&quot;,
)

# 拟合解码器
decoder.fit(df[0].values, shuffle(df[&#39;0_y&#39;].values))

# 输出结果
print(f&quot;C={c}&quot;)
# 访问每个类的交叉验证分数
cv_scores_class_0 = 解码器.cv_scores_[0]
cv_scores_class_1 = 解码器.cv_scores_[1]

# 计算两个类的平均交叉验证准确率
mean_score_class_0 = np.mean(cv_scores_class_0)
mean_score_class_1 = np.mean(cv_scores_class_1)
mean_score = np.mean([mean_score_class_0, mean_score_class_1])
print(f&quot;类别 0 的平均 CV 得分：{mean_score_class_0}&quot;)
print(f&quot;类别 1 的平均 CV 得分：{mean_score_class_1}&quot;)
print(f&quot;平均 CV 得分：{mean_score}&quot;)
# print(f&quot;C={c} 的权重：{svc.coef_}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78975982/accuracy-for-permutation-test-is-very-high</guid>
      <pubDate>Wed, 11 Sep 2024 23:33:01 GMT</pubDate>
    </item>
    <item>
      <title>从测量位置数据集中分离系统误差和随机性</title>
      <link>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</link>
      <description><![CDATA[我正在尝试找出一种方法，将系统误差从一组数据集中分离出来，这些数据集表示机械平台的位置和位置误差。数据在 pandas 数据框中。
背景：平台在 2D 平面中移动。测量报告平台位置的 X 和 Y 坐标以及 X 和 Y 方向的位置误差。2D 平面上有一个相距 1nm 的标记网格，平台平稳移动到这些标记，并且仅在这些标记处进行测量。通过算法减去这些标记的真实位置和测量的平台位置来计算误差。
数据格式：这里的倾斜表示误差。



StageCoords_X
StageCoords_Y
Skew_X
Skew_Y




118760606
112836409
-29
-45


118760622
112836426
-18
5



**数据预处理：**我正在使用 nm 尺度进行工作，每次我扫描相同的“标记”位置时，测量的舞台位置都会在这些“标记”位置周围略有不同。这就是为什么我想在 X 和 Y 方向上对数据进行分类，以便每个矩形箱（结合 X 和 Y 轴箱宽度）将覆盖用于特定标记的数据点。我可以计算每个箱的平均和峰峰值误差并绘制它们。
# 根据箱宽度定义箱边界
bin_width = 1000000
x_bins = np.arange(df[&#39;StageCoordsNM.X&#39;].min(), df[&#39;StageCoordsNM.X&#39;].max() + bin_width, bin_width)
y_bins = np.arange(df[&#39;StageCoordsNM.Y&#39;].min(), df[&#39;StageCoordsNM.Y&#39;].max() + bin_width, bin_width)

df[&#39;X_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.X&#39;], bins=x_bins, labels=False)
df[&#39;Y_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.Y&#39;], bins=y_bins, labels=False)

我想要做什么：现在假设，我已经将舞台扫过同一区域 25 次。在 x 和 y 方向上进行分箱后，每个箱将有 25 个数据点，意味着每个箱有 25 个测量舞台位置误差。现在对于每个箱，我想提取误差的系统或可重复部分。每个箱 25 个数据点可能不足以得出任何可靠的结论。因此使用机器学习很困难。我想找出一种更具统计性的方法来做到这一点。
我到目前为止所做的：计算了“归一化加权调整重复性指数”。这应该表明我对特定分箱误差是否可重复的信心。忽略 &#39;_before&#39; 下标。
#*__Pk-to-Pk X 和 Y__
df[&#39;Error_pk2pk_X&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.max() - x.min())
df[&#39;Error_pk2pk_Y&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.max() - x.min())

df[&#39;Mean_SkewNM_X_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.mean())
df[&#39;Mean_SkewNM_Y_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.mean())

def compute_confidence_X_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_x_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) + epsilon) )
group[&#39;NWARI_x_before&#39;] = (group[&#39;WARI_x_before&#39;]) / (1+group[&#39;WARI_x_before&#39;])

group[&#39;Confidence_X_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) ) / np.abs(group[&#39;SkewNM.X&#39;].mean())
group[&#39;Confidence_X_before&#39;] = group[&#39;Confidence_X_before&#39;].apply(lambda x: max(x, -1))
return group

def compute_confidence_Y_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_y_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) + epsilon) )
group[&#39;NWARI_y_before&#39;] = (group[&#39;WARI_y_before&#39;]) / (1+group[&#39;WARI_y_before&#39;])

group[&#39;Confidence_Y_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) ) / np.abs(group[&#39;SkewNM.Y&#39;].mean())
group[&#39;Confidence_Y_before&#39;] = group[&#39;Confidence_Y_before&#39;].apply(lambda x: max(x, -1))
返回组

df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_X_before)
df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_Y_before)
df[&#39;NWARI_X_before&#39;] = df[&#39;NWARI_x_before&#39;]
df[&#39;NWARI_Y_before&#39;] = df[&#39;NWARI_y_before&#39;]

但我不确定这是否是正确的方法。或者是否有其他方法可以验证这一点。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</guid>
      <pubDate>Wed, 11 Sep 2024 22:23:55 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林机器学习模型整合连续数据和静态数据？</title>
      <link>https://stackoverflow.com/questions/78975841/how-can-i-integrate-continuous-data-and-static-data-using-random-forest-machine</link>
      <description><![CDATA[我正在使用随机森林回归模型来预测地下水位变化。我使用连续输入（时间序列数据），例如 GRACE、降水量、最高温度、最低温度、NDVI，以及静态数据，例如陆地海拔、水力传导率、坡度、沙粒百分比。当我将静态输入添加到连续输入时，模型高度重视静态输入并忽略连续输入。我该如何解决这个问题。
我得到了一个预测，但在特征重要性方面存在问题，模型高度重视静态输入，而不重视连续输入，这是错误的。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78975841/how-can-i-integrate-continuous-data-and-static-data-using-random-forest-machine</guid>
      <pubDate>Wed, 11 Sep 2024 22:07:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用并行 torch cuda 流而不导致 oom？（包括示例）</title>
      <link>https://stackoverflow.com/questions/78975818/how-to-use-parallel-torch-cuda-streams-without-causing-oom-example-included</link>
      <description><![CDATA[我将大量张量数据存储在 CPU 内存中，预期的工作流程是使用 GPU 来处理它们。在处理一个块时，同时将前一个块的结果传输回 CPU。并且同时将下一个块传输到 GPU，以便它准备好进行处理，而 GPU 不必等待同步传输。
为此，我使用默认的 cuda 流进行处理，并使用两个额外的流进行并行异步传输。但是，在实际应用中，每当我使用 s2 流将张量复制到 GPU（而不是默认流）时，它确实会提高速度！但总是导致 GPU 内存快速稳定地上升，直到溢出。
我确实尝试重现这种行为，但无法，我的示例似乎不会导致内存问题。但是，我相信我可能仍然以某种方式错误地使用了流。所以我希望，如果以前有人使用过流，就能发现这个错误？在当前示例中，s2 等待 s1，但我在实际应用中尝试的任何组合都会失败。
import torch
from time import perf_counter
import sys
from threading import Thread

cpu = torch.device(&#39;cpu&#39;)
gpu = torch.device(&#39;cuda&#39;)

_range = range(10)
tensors = [torch.rand(100000000, device=cpu) for i in _range]

s1 = torch.cuda.Stream(device=gpu)
s2 = torch.cuda.Stream(device=gpu)

for i in range(10000000000):
time_start = perf_counter()

for j in range(-1,11):
def PROCESS():
if j in _range:
k = tensors[j]
for l in range(10):
k.mul_(1.01)
k.add_(1.01)
k.pow_(0.5)

def CPU():
if j-1 in _range:
with torch.cuda.stream(s1):
p = tensors[j-1]
p.record_stream(s1)
p.data = p.data.to(device=cpu, memory_format=torch.preserve_format, non_blocking=True)

def GPU():
if j+1 in _range:
# 在实际应用中使用第二个流会导致 oom
with torch.cuda.stream(s2): 
g = tensors[j+1]
g.data = g.data.to(device=gpu, memory_format=torch.preserve_format, non_blocking=True)
g.record_stream(s2)
# 注意：对于 s2，记录流在分配，
# 因为它最初在 CPU 上，而我无法在 CPU 张量上记录

t2 = Thread(target=CPU)
t2.start()
t3 = Thread(target=GPU)
t3.start()
t1 = Thread(target=PROCESS)
t1.start()

s1.wait_stream(torch.cuda.default_stream(gpu))
s2.wait_stream(s1)
s1.synchronize()

t2.join()
t3.join()
t1.join()

lapsed = perf_counter() - time_start
time_duration = &quot;%.5f sec/it&quot; % lapsed

print(f&quot;\rspeed: {time_duration}&quot;,end=&quot;\r&quot;)

您可以直接运行示例并亲自查看结果，使用流 2 时，它确实运行得更快，如果注释掉该部分，它会运行得更慢。在实际应用中，它实际上要复杂得多，因为它使用了数千个张量，并且循环并不直接，但是使用 s2 流总是会导致 oom。]]></description>
      <guid>https://stackoverflow.com/questions/78975818/how-to-use-parallel-torch-cuda-streams-without-causing-oom-example-included</guid>
      <pubDate>Wed, 11 Sep 2024 21:55:42 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确结果</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>FFN 模型在预测总和方面实现了 100% 的准确率</title>
      <link>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</link>
      <description><![CDATA[我有一个模型，可以对 -10 到正 10 之间的数字进行加法运算，但使用神经网络通过两个数字相加的数据集来预测结果。然而，在获得训练准确度时，它只是打印出很多 100% 的准确度。我不确定模型是否只是快速训练，或者是否存在问题并且没有正确学习。有人能提供一些见解吗？
这是我的代码
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)

data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
x = np.random.randint(-10, 10)
y = np.random.randint(-10,10)
bothNumber = [x,y]
data.append(bothNumber)
labels.append(x+y)

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

def createModel():
class myModel(nn.Module):
def __init__(self):
super().__init__()

self.input = nn.Linear(2,8)
self.fc1 = nn.Linear(8,8)
self.output = nn.Linear(8,1)

def forward(self,x):
x = F.relu( self.input(x) )
x = F.relu( self.fc1(x) )
return self.output(x)

net = myModel()
lossfun = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(),lr=.001)

return net,lossfun,optimizer

def trainModel():

numepochs = 100
net,lossfun,optimizer = createModel()
loss = torch.zeros(numepochs)
trainacc = []
testacc = []

for epochi in range(numepochs):
batchLoss = []

for X,y in train_loader:
X = X.float()
y = y.float()
yHat = net(X)

loss = lossfun(yHat,y)
batchLoss.append(loss.item())

optimizer.zero_grad()
loss.backward()
optimizer.step()

loss[epochi] = np.mean(batchLoss)

with torch.no_grad():
train_predictions = []
train_labels = []
for x_train, y_train in train_loader:
x_train = x_train.float()
y_train = y_train.float()
train_pred = net(x_train)
train_predictions.append(train_pred)
train_labels.append(y_train)

train_predictions = torch.cat(train_predictions)
train_labels = torch.cat(train_labels)

train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
trainacc.append(train_acc.item())

X,y = next(iter(test_data))
X = X.float() # 将 X 转换为浮点数用于测试数据
y = y.float() # 将 y 转换为浮点数用于测试数据
with torch.no_grad():
yHat = net(X)

testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

return trainacc,testacc,losses,net

trainAcc, testAcc, loss , net = trainModel()


模型有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</guid>
      <pubDate>Wed, 11 Sep 2024 18:43:48 GMT</pubDate>
    </item>
    <item>
      <title>按特定日期（而非观察结果）进行训练和测试</title>
      <link>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</link>
      <description><![CDATA[我正在尝试使用 R 中的随机森林训练一个模型。我有一个时间序列，其中包含每个日期的多只股票的信息，并创建了一个非常简化的版本：
日期 &lt;- rep(seq(as.Date(&quot;2009/01/01&quot;), by = &quot;day&quot;, length.out = 100), 10)
名称 &lt;- c(rep(&quot;Stock A&quot;, 100), rep(&quot;Stock B&quot;,100), rep(&quot;Stock C&quot;, 100), rep(&quot;Stock D&quot;, 100), rep(&quot;Stock E&quot;,100), rep(&quot;Stock F&quot;,100), rep(&quot;Stock G&quot;,100), rep(&quot;Stock H&quot;,100), rep(&quot;Stock I&quot;, 100), rep(&quot;Stock J&quot;, 100))
类别 &lt;- sample(1:10, 1000, replace=TRUE)

DF &lt;- data.frame(Date, Name, Class)
DF &lt;- DF %&gt;% 排列(Date, Name)

看起来像这样：
 日期 名称 类
1 2009-01-01 股票 A 5
2 2009-01-01 股票 B 2
3 2009-01-01 股票 C 4
4 2009-01-01 股票 D 10
5 2009-01-01 股票 E 7
6 2009-01-01 股票 F 3
...
11 2009-01-02 股票 A 10
12 2009-01-02 股票 B 8
13 2009-01-02 股票 C 9


使用时trainControl 用于将数据拆分为训练和测试期，拆分是基于每个观察进行的，但我希望基于特定日期进行。到目前为止，我所做的是：
timecontrol &lt;- DF %&gt;% group_by(Date) %&gt;% trainControl(
method = &#39;timeslice&#39;,
initialWindow = 10,
horizo​​n = 5,
skip = 4,
fixedWindow = TRUE,
returnData = TRUE, 
classProbs = TRUE
)

fitRF &lt;- train(Class ~ ., 
data = DF,
method = &quot;ranger&quot;,
tuneGrid = tunegrid,
na.action = na.omit,
trControl = timecontrol)

这给了我一个包含 10 个观察的训练集，后面是 5 个测试观察。
但是，我希望有一个训练集（和测试集......）包含 10 个不同日期的所有观测值，这样，一个训练集将是 10 天乘以每天的观测值数量，并且在各个时间段之间跳跃，以便每个测试时间段都基于全新的数据（因此 skip=4）。
第一个训练/测试拆分应该是训练=10 数据集的第一个不同日期，测试=接下来的 5 个不同日期，然后第二个训练/测试拆分应该是测试集 2 是第一个测试集之后的 5 天。
与我上面显示的数据集不同，我的数据集每天包含不同数量的观测值。我的数据集包含 417497 个观测值，但只有 2482 个不同日期，因此能够根据“分组”日期进行训练/测试拆分会产生很大的不同。 
我能否使用 trainControl 来获得所需的分割，还是必须手动分割所有数据？]]></description>
      <guid>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</guid>
      <pubDate>Wed, 08 Apr 2020 08:33:55 GMT</pubDate>
    </item>
    <item>
      <title>MS Azure autoML 准备中出现错误 - 文件格式/编码错误？</title>
      <link>https://stackoverflow.com/questions/57096415/error-in-the-ms-azure-automl-preparation-wrong-file-format-encoding</link>
      <description><![CDATA[我正尝试按照以下 Github 示例部署 MS Azure 自动机器学习：
https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/classification-bank-marketing
我更改了那里的代码以向其提供我的数据，但在执行 autoML 运行时出现以下错误：
automl.client.core.common.exceptions.DataprepException：无法执行指定的转换。

来自：
文件“/azureml-envs/azureml_e9e27206cd19de471f4e5c7a1171037e/lib/python3.6/site-packages/azureml/automl/core/dataprep_utilities.py”，第 50 行，在 try_retrieve_pandas_dataframe_adb 中

现在，我认为我的数据有问题，但随后我使用原始 csv 文件执行了以下实验：
第一次执行与 Github 示例相同，直接基于 http 链接构建数据流
第二次执行基于相同的 csv 构建数据流，但下载到我的共享。
在第二种情况下，我得到了与我的数据相同的错误。这意味着 Azure autoML 运行/数据流/准备过程仅接受特定文件格式，该格式在保存到我的驱动器时发生了更改。
我不确定这是否与编码或其他任何内容有关。
您能提供建议吗？
###########################################
#Case 1，返回错误

data= &quot;\\\dwdf219\\...\\bankmarketing_train.csv&quot;
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=[&#39;y&#39;])
y_train = dflow.keep_columns(columns=[&#39;y&#39;],validate_column_exists=True)
dflow.head()

# 训练
automl_settings = {
&quot;iteration_timeout_minutes&quot;: 10,
&quot;iterations&quot;: 5,
&quot;n_cross_validations&quot;: 2,
&quot;primary_metric&quot;: &#39;AUC_weighted&#39;,
&quot;preprocess&quot;: True,
&quot;max_concurrent_iterations&quot;: 5,
&quot;verbosity&quot;:logging.INFO,
}

automl_config = AutoMLConfig(task = &#39;classification&#39;,
debug_log = &#39;automl_errors.log&#39;,
path = project_folder,
run_configuration=conda_run_config,
X = X_train,
y = y_train,
**automl_settings
) 

remote_run = experiment.submit(automl_config, show_output = True)

###########################################
#Case 2，一切正常

data = &quot;https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv&quot;
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=[&#39;y&#39;])
y_train = dflow.keep_columns(columns=[&#39;y&#39;],validate_column_exists=True)
dflow.head()

# 训练...
##################################### 
]]></description>
      <guid>https://stackoverflow.com/questions/57096415/error-in-the-ms-azure-automl-preparation-wrong-file-format-encoding</guid>
      <pubDate>Thu, 18 Jul 2019 14:04:38 GMT</pubDate>
    </item>
    </channel>
</rss>