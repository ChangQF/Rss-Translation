<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Jan 2024 21:12:47 GMT</lastBuildDate>
    <item>
      <title>为什么变压器编码器/源掩码具有形状（序列长度、序列长度）？</title>
      <link>https://stackoverflow.com/questions/77869184/why-does-the-transformer-encoder-source-mask-have-shape-sequence-length-sequen</link>
      <description><![CDATA[转换器源/编码器掩码的大小为 (S,S)，其中 S 是输入序列的长度。为什么是方阵？它不应该是一维形状吗？
例如，如果我有一个输入 [x,y,z,0,0,0]，其中 x,y,z 是大小等于隐藏维度的向量，则掩码应为 [1, 1,1,0,0,0] 表示 x,y,z 是 0,0,0 处的输入，是填充]]></description>
      <guid>https://stackoverflow.com/questions/77869184/why-does-the-transformer-encoder-source-mask-have-shape-sequence-length-sequen</guid>
      <pubDate>Tue, 23 Jan 2024 20:46:24 GMT</pubDate>
    </item>
    <item>
      <title>TF 损失：nan - 精度：0.0000e+00</title>
      <link>https://stackoverflow.com/questions/77868774/tf-loss-nan-accuracy-0-0000e00</link>
      <description><![CDATA[我在训练过程中遇到一个问题，损失变为 NaN，而准确度仍为 0.0。我已尝试解决该问题，但正在寻求有关潜在原因和解决方案的建议。
load_images_and_labels 方法从给定文件夹和 JSON 文件读取图像和标签数据。对于每个图像，它从 JSON 注释中提取灰度表示、关联的类别标签和边界框坐标，将它们组织到 NumPy 数组中，以便在机器学习任务中进一步使用。
def load_images_and_labels(folder_path, json_path):
    使用 open(json_path, &#39;r&#39;) 作为 json_file：
        label_data = json.load(json_file)

    图像数据 = []
    标签=[]
    盒子=[]

    对于 label_data[“images”] 中的 image_info：
        image_id = image_info[“id”]

        # 构造文件的完整路径
        文件名=图像信息[“文件名”]
        image_path = os.path.join(文件夹路径, 文件名)

        img = cv2.imread(图像路径)

        img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # 根据 JSON 数据中的 image_id 提取所有标签和边界框
        image_annotations = [label_data[“annotations”中的项目的项目] if item[“image_id”] == image_id]

        # 检查图像是否有注释
        如果图像_注释：
            对于 image_annotations 中的注释：
                # 提取标签和边界框信息
                image_label = 注释[“category_id”]
                bbox = 注释[“bbox”]

                # 将图像、标签和边界框添加到数组中
                image_data.append(img_grey)
                labels.append(image_label)
                盒子.append(bbox)

    # 将列表转换为 NumPy 数组
    image_array = np.array(image_data).astype(&#39;float32&#39;)
    label_array = np.array(标签)
    box_array = np.array(boxes).astype(&#39;float32&#39;)

    返回图像_数组、标签_数组、盒子_数组

方法的形状符合我的期望。
下面你可以看到我训练模型的方法。但我的输出如下所示：
&lt;前&gt;&lt;代码&gt;纪元 1/10
112/112 [================================] - 4s 15ms/步 - 损耗：nan - 精度：2.8035e- 04
纪元 2/10
112/112 [==============================] - 2s 15ms/步 - 损耗：nan - 精度：0.0000e+ 00
纪元 3/10
112/112 [==============================] - 2s 15ms/步 - 损耗：nan - 精度：0.0000e+ 00
纪元 4/10
112/112 [==============================] - 2s 15ms/步 - 损耗：nan - 精度：0.0000e+ 00

input_image = 输入(形状=(400, 400), name=&#39;image_input&#39;)
input_bbox = 输入（形状=（4，），名称=&#39;bounding_box_input&#39;）

# Flachklopfen des Bildes
展平图像 = 展平（）（输入图像）

merged_input = Concatenate()([flatten_image, input_bbox])

x = 密集（64，激活=&#39;relu&#39;）（merged_input）
输出层=密集（28，激活=&#39;softmax&#39;）（x）

模型 = tf.keras.Model(输入=[input_image, input_bbox], 输出=output_layer)
model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

＃ 训练
model.fit({&#39;image_input&#39;: train_images, &#39;bounding_box_input&#39;: train_boxes}, train_labels, epochs=10)

我已经考虑过以下事项：

检查数据和标签的正确性。
可以在此处找到证据
标准化图像中的像素值。例如：train_images = train_images / 255.0
]]></description>
      <guid>https://stackoverflow.com/questions/77868774/tf-loss-nan-accuracy-0-0000e00</guid>
      <pubDate>Tue, 23 Jan 2024 19:20:45 GMT</pubDate>
    </item>
    <item>
      <title>优化 CT 扫描分割的 U-Net 训练：专门强调具有感兴趣区域 (ROI) 的切片</title>
      <link>https://stackoverflow.com/questions/77868697/optimizing-u-net-training-for-ct-scan-segmentation-exclusive-emphasis-on-slices</link>
      <description><![CDATA[专门在包含感兴趣区域 (ROI) 的 CT 扫描切片上训练 U-Net 模型，同时排除没有 ROI 的切片，这是一种合理的方法吗？在医学图像分析中采用这种策略进行分割任务有哪些潜在的优点和缺点？
在我进行的测试中，与仅使用 ROI 切片相比，使用所有切片产生了更好的结果。然而，我认为，当将分析限制在仅包含 ROI 的切片时，可以获得最准确的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77868697/optimizing-u-net-training-for-ct-scan-segmentation-exclusive-emphasis-on-slices</guid>
      <pubDate>Tue, 23 Jan 2024 19:04:12 GMT</pubDate>
    </item>
    <item>
      <title>每个用户具有多行的回归模型来预测死亡</title>
      <link>https://stackoverflow.com/questions/77868519/regression-model-with-multiple-rows-per-user-to-predict-death</link>
      <description><![CDATA[我正在尝试建立一个回归模型，用于根据用户的实验室报告预测死亡率，问题是在我的数据集中，即使对于同一用户，每一行也是不同的实验室，例如：
值开始类别单位类型显示record.death种子report_Frequency
 3 25.83 1291715668918 生命体征 kg/m2 39156-5 体重指数 (BMI) [比率] 1561456468918 -4803776664509238228 4
25 26.47 1318326868918 生命体征 kg/m2 39156-5 体重指数 (BMI) [比率] 1561456468918 -4803776664509238228 4
37 27.42 1386669268918 生命体征 kg/m2 39156-5 体重指数 (BMI) [比率] 1561456468918 -4803776664509238228 4
49 29.19 1481622868918 生命体征 kg/m2 39156-5 体重指数 (BMI) [比率] 1561456468918 -4803776664509238228 4
74 19.35 1196939625807 生命体征 kg/m2 39156-5 体重指数 (BMI) [比率] 1490267625807 401572787436335446 10

我不太确定使用什么作为我的特征，我使用值、开始（这是时间戳中的实验室日期）和类型（这是指标类型的代码），目标变量是record.death，事情正如您在表中看到的那样，根据每个用户访问实验室的次数，我可以有多个行，我使用线性回归，但我的均方误差太高，可能出了什么问题？也许我需要为这种情况使用另一个模型？]]></description>
      <guid>https://stackoverflow.com/questions/77868519/regression-model-with-multiple-rows-per-user-to-predict-death</guid>
      <pubDate>Tue, 23 Jan 2024 18:25:58 GMT</pubDate>
    </item>
    <item>
      <title>将 xml 注释与图像链接以创建数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/77868119/link-xml-annotations-with-images-to-create-dataset</link>
      <description><![CDATA[我在一个文件夹中有一个数据集，该文件夹包含 2 个名为 train 的子文件夹，test 火车内部有两个子文件夹注释（它们是 XML 文件）和图像，它们是没有标签或边界框的数据集的图像，所以我想要将它们链接在一起以获得可以操作和训练模型的数据集
我想要数据集，我可以操作和训练我的模型，注意数据集在我的本地计算机中，并且我计划使用 YOLOv5]]></description>
      <guid>https://stackoverflow.com/questions/77868119/link-xml-annotations-with-images-to-create-dataset</guid>
      <pubDate>Tue, 23 Jan 2024 17:15:41 GMT</pubDate>
    </item>
    <item>
      <title>提取 STA 库中的集群成员资格</title>
      <link>https://stackoverflow.com/questions/77867534/extracting-cluster-membership-in-sta-library</link>
      <description><![CDATA[我一直在 R 中使用拓扑数据分析库进行聚类，称为半监督拓扑分析“STA”。
但是，与我之前用于聚类的许多库不同，没有调用来提取聚类，例如x$cluster。那么，我想知道如何提取诸如聚类分配和轮廓值之类的内容？
下面的代码和链接。
https://tianshufeng.github.io/STA/articles/STA.html 
https://rdrr.io/github/TianshuFeng/SemiMapper /man/mapper.sta.html
#安装和加载

devtools::install_github(“TianshuFeng/STA”)
图书馆（STA）

#虚拟数据

x1 = 重复次数(1:3, 次数 = 100)
x2 = 重复(1:3, 次数 = 100)
x3 = 重复次数(1:3, 次数 = 100)
x4 = 重复(1:3, 次数 = 100)
x5 = 重复(1:3, 次数 = 100)

DAT &lt;- data.frame(x1, x2,x3,x4,x5)
DAT &lt;- data.frame(lapply(DAT, function(x) as.numeric(as.character(x))))

#STA代码

MAP &lt;-mapper.sta(DAT,
                  过滤器值 = DAT$x1,
                  间隔数 = 5,
                  重叠百分比 = 40,
                  dist_method = “曼哈顿”,
                  cluster_method = “分层”,
                  NbClust_cluster_method = &quot;单个&quot;,
                  num_bins_when_clustering = 10,
                  cluster_index = “轮廓”;
）

simple_visNet(MAP, 过滤器 = DAT$x1, color_filter = TRUE)

＃＃＃＃边注
#如果存在带有依赖项的错误消息，请仅运行最后一段代码

cluster_cutoff_at_first_empty_bin &lt;- 函数（高度，直径，num_bins_when_clustering）{
  
  # 如果只有两个点（一个高度值），那么我们就有一个簇
  if (长度(高度) == 1) {
    if (高度 == 直径) {
      截止值 &lt;- Inf
      返回（截止）
    }
  }
  
  bin_breaks &lt;- seq(from=min(heights), to=diam,
                    by=(直径 - 最小(高度))/num_bins_when_clustering)
  if (长度(bin_breaks) == 1) { bin_breaks &lt;- 1 }
  
  myhist &lt;- hist(c(高度，直径)，breaks=bin_breaks，plot=FALSE)
  z &lt;- (myhist$counts == 0)
  如果（总和（z）== 0）{
    截止值 &lt;- Inf
    返回（截止）
  } 别的 {
    # 返回逻辑向量的索引 (z == TRUE)，min 给出最小索引
    截止 &lt;- myhist$mids[ min(which(z == TRUE)) ]
    返回（截止）
  }
  
}
]]></description>
      <guid>https://stackoverflow.com/questions/77867534/extracting-cluster-membership-in-sta-library</guid>
      <pubDate>Tue, 23 Jan 2024 15:41:35 GMT</pubDate>
    </item>
    <item>
      <title>如何更改微调技能中的学习率[关闭]</title>
      <link>https://stackoverflow.com/questions/77867150/how-to-change-the-learn-rate-in-the-fine-tune-skill</link>
      <description><![CDATA[查询：

在特定层将学习率提高十倍的目的：我想了解实现选择背后的基本原理，如果param_group为 True 时，全连接层 (net .fc）设置为默认学习率（learning_rate * 10）的十倍。这种方法有哪些好处和潜在影响？

在此处输入图像描述

动态学习率调整的可行性：是否可以动态调整学习率，例如，通过使用正弦函数在一定范围（例如，从 0 到pi/2）？如何在这段代码的上下文中实现这一点，特别是对于不同的层，例如全连接层（接近 pi/2）和卷积层（接近 0） ？
]]></description>
      <guid>https://stackoverflow.com/questions/77867150/how-to-change-the-learn-rate-in-the-fine-tune-skill</guid>
      <pubDate>Tue, 23 Jan 2024 14:48:15 GMT</pubDate>
    </item>
    <item>
      <title>此 MinMaxScaler 实例尚未安装。在使用此估计器之前，使用适当的参数调用“fit”。NotFittedError [关闭]</title>
      <link>https://stackoverflow.com/questions/77866770/this-minmaxscaler-instance-is-not-fitted-yet-call-fit-with-appropriate-argume</link>
      <description><![CDATA[xgb = XGBRegressor(n_estimators=1000，learning_rate=0.01)
xgb.fit(X_train3,y_train1,eval_set=[(X_train3,y_train1),(X_valid3,y_valid1)],early_stopping_rounds=100,verbose=True)
Predicted_results_v = xgb.predict(X_train3)
Predicted_results_t = xgb.predict(X_train3)
Predicted_results_v = Predicted_results_v.reshape(-1,1)
Predicted_results_t = Predicted_results_t.reshape(-1,1)
Predicted_reults_v = scaler1.inverse_transform(predicted_results_v)
Predicted_results_t = 缩放器.inverse_transform(predicted_results_t)

我正在使用 xgboost 库作为股票交易的机器学习模型。为了缩放数据，使用了 MinMaxScaler，但它无法适应数据，因为它引发了错误
NotFittedError：此 MinMaxScaler 实例尚未安装。
在使用此估计器之前，请使用适当的参数调用“fit”。
]]></description>
      <guid>https://stackoverflow.com/questions/77866770/this-minmaxscaler-instance-is-not-fitted-yet-call-fit-with-appropriate-argume</guid>
      <pubDate>Tue, 23 Jan 2024 13:45:59 GMT</pubDate>
    </item>
    <item>
      <title>如何清理存在拼写错误的数据以进行文本分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77866039/how-to-clean-my-data-which-has-spelling-errors-for-text-classification</link>
      <description><![CDATA[我的文本数据包含金融句子，其中有很多拼写错误和无意义的单词。我已经做了很多清理工作，但仍然留下一些文字。有什么方法可以帮助纠正我的数据的这种不稳定现象吗？
我已经尝试了所有的清理方法和缩写的处理。但这些就像手动输入]]></description>
      <guid>https://stackoverflow.com/questions/77866039/how-to-clean-my-data-which-has-spelling-errors-for-text-classification</guid>
      <pubDate>Tue, 23 Jan 2024 11:44:15 GMT</pubDate>
    </item>
    <item>
      <title>该模型不断预测同一类[关闭]</title>
      <link>https://stackoverflow.com/questions/77865558/this-model-keeps-predicting-the-same-class</link>
      <description><![CDATA[我编写了一个用于预测药用植物的代码，但它一直预测相同类型的植物（印楝）
即使我的输入不同。
https://colab.research.google.com/drive/1dDmFyach90W7wjWu8mOW1xbOvctRLB3Y ?usp=共享
这是我的代码的链接。
我的数据集
请尽快完成，我有一个项目要到期。
我尝试最小化数据集，就像我在堆栈溢出时看到的那样。
但它一直给出相同的输出。
我希望它能够正确预测叶子的类型。]]></description>
      <guid>https://stackoverflow.com/questions/77865558/this-model-keeps-predicting-the-same-class</guid>
      <pubDate>Tue, 23 Jan 2024 10:25:14 GMT</pubDate>
    </item>
    <item>
      <title>AssertionError LLama-cpp-python 模型加载失败</title>
      <link>https://stackoverflow.com/questions/77864368/assertionerror-llama-cpp-python-model-failed-to-load</link>
      <description><![CDATA[即使我使用 GGUF 格式，Llama-cpp-python 也会出现断言错误。
我正在尝试使用 llama-cpp-python 0.1.85 在 python 3.7.2 中运行 AI 模型，每次运行我的代码时都会收到此错误：
加载模型时出错：MapViewOfFile 失败：没有足够的内存资源来处理此命令。

llama_load_model_from_file：加载模型失败
回溯（最近一次调用最后一次）：
  文件“server.py”，第 26 行，在  中。
    n_ctx=N_CTX,
  文件“D:\AI 2\Venv\lib\site-packages\llama_cpp\llama.py”，第 323 行，位于 __init__ 中
    断言 self.model 不是 None
断言错误

我使用的是 GGUF 格式，所以我不知道问题是什么，它在第二台计算机上运行良好，但在我的主机上运行不佳，有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77864368/assertionerror-llama-cpp-python-model-failed-to-load</guid>
      <pubDate>Tue, 23 Jan 2024 06:30:48 GMT</pubDate>
    </item>
    <item>
      <title>部署时ValueError：无法确定Excel文件格式[关闭]</title>
      <link>https://stackoverflow.com/questions/77863886/when-deploy-valueerror-excel-file-format-cannot-be-determined</link>
      <description><![CDATA[我的本​​地计算机上有一个 Excel 工作表，它正在为我的 Streamlit 应用程序提供数据。该应用程序和每个组件在我的本地系统上运行良好。但我在部署时解决了这个问题
ValueError：无法确定 Excel 文件格式，您必须手动指定引擎。

我添加了引擎openpyxl
df = pd.read_excel(f, engine=“openpyxl”).reindex(columns = customer_id).dropna(how=&#39;all&#39;, axis=1)

现在我得到了一个不同的错误：
BadZipFile：文件不是 zip 文件
]]></description>
      <guid>https://stackoverflow.com/questions/77863886/when-deploy-valueerror-excel-file-format-cannot-be-determined</guid>
      <pubDate>Tue, 23 Jan 2024 03:45:05 GMT</pubDate>
    </item>
    <item>
      <title>添加 2 个模型作为另一个模型的输入（图表已断开连接）</title>
      <link>https://stackoverflow.com/questions/77859877/addition-of-2-models-as-input-to-another-graph-disconnected</link>
      <description><![CDATA[我有两个模型：model_A 和 model_B。我想对这两个模型进行元素明智加法，并将结果用作 model_C 的输入。所以，我有这个代码：
从tensorflow.keras.layers导入Conv2D，BatchNormalization，\
    激活、输入、添加
从tensorflow.keras.models导入模型
将 numpy 导入为 np
将张量流导入为 tf

def model_A（输入）：
    x1 = Conv2D(32, 3, padding=&#39;相同&#39;)(输入)
    x1 = BatchNormalization()(x1)
    x1 = 激活(&#39;relu&#39;)(x1)
    
    x2 = Conv2D(32, 3, 填充=&#39;相同&#39;)(x1)
    模型=模型（输入=输入，输出=x​​2，名称=&#39;model_A&#39;）
    返回模型
    

def model_B（输入）：
    f1 = Conv2D(32, 3, 填充=&#39;相同&#39;)(输入)
    f1 = BatchNormalization()(f1)
    f1 = 激活(&#39;relu&#39;)(f1)
    
    f2 = Conv2D(32, 3, 填充=&#39;相同&#39;)(f1)

    模型=模型（输入=输入，输出=f2，名称=&#39;model_B&#39;）
    返回模型

def model_C（输入）：
    f1 = Conv2D(32, 3, 填充=&#39;相同&#39;)(输入)
    f1 = BatchNormalization()(f1)
    f1 = 激活(&#39;relu&#39;)(f1)
    
    f2 = Conv2D(16, 3, 填充=&#39;相同&#39;)(f1)
    f2 = BatchNormalization()(f2)
    f2 = 激活(&#39;relu&#39;)(f2)

    f3 = Conv2D(1, 3, 填充=&#39;相同&#39;)(f2)

    模型=模型（输入=输入，输出=f3，名称=&#39;model_C&#39;）
    返回模型
    
def model_final(高度、宽度、通道):
    输入=输入（（高度，宽度，通道））
    
    modelA = model_A(输入)
    modelB = model_B(输入)
    
    加法 = Add()([modelA.output, modelB.output])
    
    modelC = model_C（加法）
    
    返回模型（输入，modelC.输出）
    
a = np.random.uniform(0, 1, (100, 32, 32, 3))
b = np.random.uniform(0, 1, (100, 32, 32, 3))
c = np.random.uniform(0, 1, (100, 32, 32, 3))
    
模型 = model_final(32, 32, 3)

优化器 = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(优化器=优化器,
              损失=&#39;mae&#39;,
              指标=[&#39;mae&#39;])
    

如果我运行代码，我会在 Model(inputs=inputs,outputs=f3, name=&#39;model_C&#39;) 处收到Graph Disconnected。所以，为了解决这个问题，我正在做：
def model_final(高度、宽度、通道):
    输入=输入（（高度，宽度，通道））
    
    modelA = model_A(输入)
    modelB = model_B(输入)
    
    加法 = Add()([modelA.output, modelB.output])
    
    input_C = 输入((高度,宽度,32))
    modelC = model_C(输入_C)
    modelC = modelC(加法)
    
    模型=模型（输入，模型C）
    返回模型

编译得很好。但是，我不确定这是否正确。如果这样做的逻辑是正确的！]]></description>
      <guid>https://stackoverflow.com/questions/77859877/addition-of-2-models-as-input-to-another-graph-disconnected</guid>
      <pubDate>Mon, 22 Jan 2024 12:34:03 GMT</pubDate>
    </item>
    <item>
      <title>Docker for Lambda (FAST API) 中的{“无法导入模块‘main’：没有名为‘main’的模块”，“errorType”：“Runtime.ImportModuleError”}</title>
      <link>https://stackoverflow.com/questions/71305887/unable-to-import-module-main-no-module-named-main-errortype-runtime</link>
      <description><![CDATA[我已经创建了一个 Fastapi，现在尝试使用 Docker 容器将其部署到 AWS lambda。但有一个错误：
{“errorMessage”：“无法导入模块“main”：没有名为“main”的模块”，“errorType”：“Runtime.ImportModuleError”，“stackTrace”：[] }

我已经尽力了。
这是我的 main.py 文件：
from fastapi 导入 FastAPI
从 starlette.status 导入 HTTP_302_FOUND,HTTP_303_SEE_OTHER
导入spacy
从字符串导入标点符号
从曼古姆进口曼古姆
进口uvicorn
应用程序 = FastAPI()

@app.get(&#39;/&#39;)
def home():
    返回{“答案”：“你好世界”}

@app.get(&#39;/tags&#39;)
def prep_data(文本):
    标签=标记（文本，nlp）
    标签 = getdict(标签)
    返回 {
        ‘标签’：标签
    }

处理程序 = Mangum(应用程序)
如果 __name__ == “__main__”：
    # 处理程序 = Mangum(应用程序)
    uvicorn.run(&#39;main:app&#39;, host=&#39;0.0.0.0&#39;, port=8000, reload=False, root_path=”/”)

该错误表明 main.py 文件没有 main.py 文件，正如您所看到的 dockerfile：
&lt;前&gt;&lt;代码&gt;来自 public.ecr.aws/lambda/python:3.8

复制./应用程序/应用程序

复制 ./requirements.txt /app/requirements.txt

工作目录/应用程序

运行 pip install -rrequirements.txt

CMD [“main.handler”]

我的目录结构是这样的：
&lt;前&gt;&lt;代码&gt;/应用程序
    主要.py
Dockerfile
要求.txt
]]></description>
      <guid>https://stackoverflow.com/questions/71305887/unable-to-import-module-main-no-module-named-main-errortype-runtime</guid>
      <pubDate>Tue, 01 Mar 2022 08:52:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn 确定 RF 模型中每个类的特征重要性</title>
      <link>https://stackoverflow.com/questions/50201913/using-scikit-learn-to-determine-feature-importances-per-class-in-a-rf-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/50201913/using-scikit-learn-to-determine-feature-importances-per-class-in-a-rf-model</guid>
      <pubDate>Sun, 06 May 2018 16:20:44 GMT</pubDate>
    </item>
    </channel>
</rss>