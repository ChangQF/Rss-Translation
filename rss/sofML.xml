<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 24 Aug 2024 21:13:35 GMT</lastBuildDate>
    <item>
      <title>LogisticRegression 模型 predict_proba 产生了意外结果。尽管数据偏向一侧，但所有输出均为 ~0.5</title>
      <link>https://stackoverflow.com/questions/78909975/the-logisticregression-model-predict-proba-is-producing-unexpected-results-all</link>
      <description><![CDATA[我正在使用 Google Colab，它有时会在没有任何警告的情况下更新库，并且我的代码会毫无解释地中断。
对于我的项目，我正在训练一个单独的 ML 模型进行异常检测。在这种情况下，我有被视为正常的良性数据和被视为异常的恶性数据。我从一批良性和恶性数据中生成重建错误列表。我使用 LogisticRegression 模型来查找划分正常和异常数据点的决策边界。
X = np.concatenate((np.array(benign_recon_error).reshape(-1, 1), np.array(malignant_recon_error).reshape(-1, 1)))
Y = np.concatenate((np.zeros(len(benign_recon_error)), np.ones(len(malignant_recon_error))))

# 训练逻辑回归模型
lr_clf = LogisticRegression().fit(X, Y)

x_values = np.linspace(min(X), max(X), 100)
y_values = lr_clf.predict_proba(x_values)[:, 1]
print(y_values)

decision_boundary = x_values[np.abs(y_values - 0.5).argmin()]

问题是 y_values 都在 0.5 左右徘徊。
[0.49999572 0.49999602 0.49999631 0.49999661 0.49999691 0.49999721
0.49999751 0.49999781 0.4999981 0.4999984 0.4999987 0.499999
0.4999993 0.4999996 0.49999989 0.50000019 0.50000049 0.50000079 0.50000109 0.50000139 0.50000169 0.50000198 0.50000228 0.50000258 0.50000288 0.50000318 0.50000348 0 .50000377 0.50000407 0.50000437 0.50000467 0.50000497 0.50000527 0.50000556 0.50000586 0.50000616 0.50000646 0.50000676 0.50000706 0.50000736 0.50000765 0.50000795 0.50000825 0.50000855 0.50000885 0.50000915 0.50000944 0.50000974 0.50001004 0 .50001034 0.50001064 0.50001094 0.50001123 0.50001153 0.50001183 0.50001213 0.50001243 0.50001273 0.50001303 0.50001332
 0.50001362 0.50001392 0.50001422 0.50001452 0.50001482 0.50001511 0.50001541 0.50001571 0.50001601 0.50001631 0.50001661 0 .5000169 0.5000172 0.5000175 0.5000178 0.5000181 0.5000184 0.5000187 0.50001899 0.50001929 0.50001959 0.50001989 0.50002019 0.50002049
0.50002078 0.50002108 0.50002138 0.50002168 0.50002198 0.50002228
0.50002258 0.50002287 0.50002317 0.50002347 0.50002377 0.50002407
0.50002437 0.50002466 0.50002496 0.50002526]

以下是数据的可视化。请记住，y 轴上的熵标签是针对数据点本身的，仅用于将数据分散到 y 轴上以防止它们聚集在一起。
异常概率图
异常预测的准确率为 0.66。
决策边界仍然有效，但回归线无效。预期行为是逻辑回归线倾斜，表明异常数据的概率随着侦察误差的增加而增加。这表明 y_value 在某种程度上是不正确的。
以下是代码按预期工作的示例。
异常概率图（工作）
虽然这似乎表明两组之间的方差有显著改善，但这些异常预测的准确率仅为 0.68。
我只是想弄清楚如何让我的代码像以前一样工作。]]></description>
      <guid>https://stackoverflow.com/questions/78909975/the-logisticregression-model-predict-proba-is-producing-unexpected-results-all</guid>
      <pubDate>Sat, 24 Aug 2024 20:55:24 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用轮廓分析来评估在 k 均值图像分割中选择的聚类数量是否最优？[关闭]</title>
      <link>https://stackoverflow.com/questions/78909544/is-it-possible-to-use-silhouette-analysis-to-assess-how-optimal-the-number-of-cl</link>
      <description><![CDATA[正如标题所述，我正在寻找方法来评估所选簇数的最优性，自动肘形法是主观的并且容易出错。我已经决定下次尝试使用轮廓分析，但根据我浏览过的教程/论文，似乎没有人使用过它。在这种情况下使用轮廓分析是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/78909544/is-it-possible-to-use-silhouette-analysis-to-assess-how-optimal-the-number-of-cl</guid>
      <pubDate>Sat, 24 Aug 2024 17:00:34 GMT</pubDate>
    </item>
    <item>
      <title>对于时间序列分类，当 shuffle = False 时，如何获得分层训练 - 测试分割？</title>
      <link>https://stackoverflow.com/questions/78909358/how-to-get-stratified-train-test-split-while-shuffle-false-for-time-series-cla</link>
      <description><![CDATA[我正在尝试使用深度学习模型构建时间序列分类器。我有一个数据集，其中有 5 种不同活动各 10 个样本（总共 50 个样本）。就好像前 10 个样本属于第 1 种活动，接下来的 10 个样本属于第 1 种活动，最后 10 个样本属于第 5 种活动。我需要将数据分成训练集和测试集，以便每个活动也应该在测试集中。问题是我无法使用 scikit-learn 中的 train_test_split，因为它不允许分层和 shuffle = False，而这正是我的情况。
我试过了：
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, 
test_size=0.3, 
stratify=y_train_val, 
random_state=42, 
shuffle = False)

我得到了错误：
Traceback（最近一次调用）：
文件 &quot;d:\Thesis\wearable_device_based_human_activity_recognition\MARS\miscellaneous\garmin_data_analysis.py&quot;，第 130 行，位于 &lt;module&gt;
data_train, data_valid = train_test_split(data_train, test_size=0.2, random_state=42, stratify = df[&#39;Class&#39;], shuffle=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;D:\DevSetupFiles\anaconda3\envs\mars\Lib\site-packages\sklearn\utils\_param_validation.py&quot;，第 213 行，在包装器中
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“D:\DevSetupFiles\anaconda3\envs\mars\Lib\site-packages\sklearn\model_selection\_split.py”，第 2784 行，位于 train_test_split
raise ValueError(
ValueError: 分层训练/测试拆分未针对 shuffle=False 实施

然后我尝试将数据集按顺序拆分为训练、验证和测试，同时对测试集进行分层。最小可重现代码如下：
train, valid, test = 0.6, 0.2, 0.2
data_points = np.arange(1, 101).reshape(100, 1) 
classes = np.repeat(np.arange(10), 10) 
class_set = np.unique(classes)

df = pd.DataFrame({
&#39;DataPoint&#39;: data_points.flatten(),
&#39;Class&#39;: classes
})

print(df.head(20))

data_train, data_valid, data_test = [], [], []

for class_i in class_set:
data_inds = np.where(classes == class_i)[0]
data_i = data_points[data_inds, ...]

N_i = len(data_inds)
N_i_train = int(N_i * train)
N_i_valid = int(N_i * valid)

data_train.append(data_i[:N_i_train])
data_valid.append(data_i[N_i_train:N_i_train + N_i_valid])
data_test.append(data_i[N_i_train + N_i_valid:])

data_train = np.concatenate(data_train, axis=0)
data_valid = np.concatenate(data_valid, axis=0)
data_test = np.concatenate(data_test, axis=0)

我的方法分割数据集正确吗？我需要在保持测试集每个活动的同时保持时间顺序。在这种情况下我应该如何解决我的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78909358/how-to-get-stratified-train-test-split-while-shuffle-false-for-time-series-cla</guid>
      <pubDate>Sat, 24 Aug 2024 15:28:57 GMT</pubDate>
    </item>
    <item>
      <title>24 步时间序列预测的难题：特征相关性和时间依赖性问题</title>
      <link>https://stackoverflow.com/questions/78909318/struggling-with-24-step-ahead-time-series-forecasting-issues-with-feature-corre</link>
      <description><![CDATA[我正在努力预测 24 小时后的负载数据，尽管尝试了各种算法（统计、机器学习、深度学习），但仍然面临一些挑战。以下是主要问题：
相关性低：特征（包括天气数据）与目标变量没有很强的相关性，这使得模型很难有效学习。我尝试使用滑动窗口技术，以 48 个过去值作为输入，但性能并没有太大提高。
数据中的峰值：存在明显的峰值，它们不是异常值，但具有随机性，没有特定的幅度或时间，因此很难预测。时间特征（如日、时、月和季节）也与目标显示出较弱的相关性。
时间依赖性：我的模型（ARIMA、LSTM、CNN-LSTM-Attention）难以有效捕捉与时间相关的模式。时间序列的残差部分代表最重要的部分，模型无法准确预测。
模型性能：错误率很高，尤其是在高峰时段。我尝试过大量的预处理步骤，如差分、应用对数变换、降噪和处理自相关，但结果仍然不令人满意。
ADF 统计量：-20.032363997484328
p 值：0.0
临界值：{&#39;1%&#39;：-3.4307346775420773，&#39;5%&#39;：-2.8617100123025554，&#39;10%&#39;：-2.5668604931899694}
KPSS 统计量：1.5772813072959655
p 值：0.01
临界值：{&#39;10%&#39;：0.347，&#39;5%&#39;：0.463，&#39;2.5%&#39;： 0.574, &#39;1%&#39;: 0.739}

您发现附件中显示了执行的不同分析步骤的图像，例如季节性分解、相关矩阵、ACF、PACF、目标与其滞后特征之间的相关性测试。
寻求有关改进模型性能的建议，特别是在处理这些随机峰值方面。有没有关于特征工程、替代模型或应对这些挑战的策略的提示？
[在此处输入图片描述](https://i.sstatic.net/9QIyxLbK.png)[[[在此处输入图片描述](https://i.sstatic.net/0k5QaM6C.png)](https://i.sstatic.net/MBzsxmpB.png)](https://i.sstatic.net/pBxGQQrf.png)]]></description>
      <guid>https://stackoverflow.com/questions/78909318/struggling-with-24-step-ahead-time-series-forecasting-issues-with-feature-corre</guid>
      <pubDate>Sat, 24 Aug 2024 15:12:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 进行 ASL 识别</title>
      <link>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</link>
      <description><![CDATA[实时模型是否可以转换为预先录制的视频上传？
例如，我将训练一个用于手语的 LSTM 模型，以实现实时识别。现在我想将其集成到 Android Studio 中的移动应用程序中，并使其预先录制。
我想将其他模型迁移到我的项目中。
我尝试了一些技术，例如 CNN 识别，但没有奏效。]]></description>
      <guid>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</guid>
      <pubDate>Sat, 24 Aug 2024 13:02:58 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和机器学习中“学习率高、大、低、小”是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</link>
      <description><![CDATA[在深度学习和机器学习中，经常会说学习率高、大、低或小，但我不知道这是什么意思。 *问题是关于如何用英语表达学习率。
例如，我在 PyTorch 中将学习率 0.000001 和 100.0 设置为 SGD() 的 lr 参数，如下所示：
示例 A：
 # ↓ 这里 ↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=0.000001)

示例 B：
 # ↓ 这里↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=100.0)

现在，示例A和示例B的学习率是高、大、低还是小？而常见的学习率是0.1 ~ 0.001。我不知道如何用英语表达学习率。]]></description>
      <guid>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</guid>
      <pubDate>Sat, 24 Aug 2024 12:39:22 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Tessaract 进行 OCR</title>
      <link>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</link>
      <description><![CDATA[我正在研究牙膏卷边的 OCR。所以我用手机相机拍了这些照片。我使用了 opencv 和 tessaract。这是来自 chatgpt 的基本代码。它根本不起作用，关于此的任何建议或帮助。这是图片。

有人请向我解释我该怎么做或建议我一些容易做的事情。]]></description>
      <guid>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</guid>
      <pubDate>Sat, 24 Aug 2024 12:00:40 GMT</pubDate>
    </item>
    <item>
      <title>Resnet 内存不足：torch.OutOfMemoryError：CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</guid>
      <pubDate>Sat, 24 Aug 2024 08:53:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建具有点数据检索的交互式聚类可视化？</title>
      <link>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</link>
      <description><![CDATA[我需要可视化数据点集群，并允许用户单击这些点来检索有关它们的详细信息。
我已经实现了 K-means（或任何聚类算法，如 dbscan）聚类并使用 Matplotlib 可视化了集群，但我不确定如何设置交互性，以便单击某个点显示其相关数据。
但我的主要问题是，我是否应该以特殊方式存储和构造数据以再次检索它？例如，我想识别靠近集群边界的点并检索它们的信息（可以通过单击或工具提示或任何方法）。]]></description>
      <guid>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</guid>
      <pubDate>Sat, 24 Aug 2024 08:29:56 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在包含其他业务逻辑的主 Web 应用程序中部署我的 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</link>
      <description><![CDATA[我正在为我的学期做一个小项目。我正在使用 flask。

我在这个应用程序中有一些业务逻辑，其中收集了一些数据。
我想将这些数据发送到我的 ML 模型。
那么我应该在不同的服务器上部署我的 ML 模型并使用其端点吗？
这不会增加响应时间吗？
此外，由于我将其部署在 render.com 或任何其他免费服务上的低内存免费服务器上。
我的主要业务逻辑也将占用一些内存。
此外，我应该如何分离我的 ML 模型，比如将它部署到整个其他服务器上并对其进行配置，然后将我的主要逻辑部署到其他服务器上？
那么这里应该做什么？
你的想法会对我这个项目有所帮助。
感谢您的宝贵回复
我正在创建一个名为 mainapp 的主 flask 应用程序。
在其中，我在单独的文件中编写了一些业务逻辑，并将它们作为包导入到我的 routes.py 文件中。
现在我也对 ML 模型做了同样的事情。这是一个简单的情感分析模型，我将传递句子并接收其分数。]]></description>
      <guid>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</guid>
      <pubDate>Sat, 24 Aug 2024 07:15:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</guid>
      <pubDate>Sat, 24 Aug 2024 03:11:26 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10 自定义训练——导入 YOLO 还是 YOLOv10？</title>
      <link>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</link>
      <description><![CDATA[我希望对我拥有的数据集使用 YoloV10n 进行自定义训练（到目前为止我一直在使用 YoloV8），但我不确定是否要使用/导入 YOLO 或 YOLOv10。
我最初尝试使用 YOLOv10，并能够成功完成自定义训练和推理。但是，由于断言不一致，我无法将模型导出到 tflite。然后我切换回 YOLO（从预先训练的 yolov10.pt 模型开始），并能够将其导出到 tflite。此外，Ultralytics 的官方文档确实指导如何使用 YOLO（而不是 YOLOv10）进行 YoloV10 训练。另一方面，RoboFlow 教程确实指导如何使用 YOLOv10... 🤔
我应该使用 from ultralytics import YOLOv10 还是 from ultralytics import YOLO？这有关系吗？对于训练、推理和导出（tflite 等），答案是否相同？]]></description>
      <guid>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</guid>
      <pubDate>Fri, 23 Aug 2024 21:07:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 包“敏感性”执行 Sobol 敏感性分析？</title>
      <link>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</link>
      <description><![CDATA[我想使用 R 包“sensitivity”执行 Sobol 敏感性分析。但是，我不确定如何在 sobol 函数中创建第一和第二个随机样本 (X1, X2)。我假设 X1 和 X2 都是输入数据的子集，但最终结果似乎不正确。
library(tidymodels)
library(sensitivity)

# 示例数据
set.seed(123)
x1 = runif(100)
x2 = runif(100)
x3 = runif(100)
y = 3 * x1 + 2 * x2 + x3 + rnorm(100)
data &lt;- data.frame(x1, x2, x3, y)

# 将数据拆分为训练集和测试集
set.seed(234)
data_split &lt;- initial_split(data, prop = 0.8)
train_data &lt;- training(data_split)
test_data &lt;- testing(data_split)

# 使用创建线性回归模型tidymodels
lm_spec &lt;- linear_reg() %&gt;%
set_engine(&quot;lm&quot;) %&gt;%
set_mode(&quot;regression&quot;)

lm_fit &lt;- lm_spec %&gt;%
fit(y ~ x1 + x2 + x3, data = train_data)

# 定义模型函数
model_function &lt;- function(x) {
new_data &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])
predict(lm_fit, new_data)$`.pred`
}

# 执行 Sobol 敏感性分析
set.seed(345)
X_index1 = sample(x=1:100, size = 50, replace = FALSE)
X_index2 = c(1:length(data$x1))[-X_index1]

sobol_results &lt;- sobol(model = model_function, 
X1 = data[X_index1, -4], 
X2 = data[X_index2, -4], 
nboot = 1000, order = 2)

sobol_results

sobol_results 显示敏感度顺序为：x2&gt;x1&gt;x3。根据函数 y = 3 * x1 + 2 * x2 + x3 + rnorm(100)，“x1”应该具有更高的敏感度，因为它是 3 * x1。我应该如何修正我的代码？谢谢。
]]></description>
      <guid>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</guid>
      <pubDate>Sat, 10 Jun 2023 01:54:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中检查模型处于训练模式还是评估模式？</title>
      <link>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</link>
      <description><![CDATA[如何从模型内部检查它当前处于训练模式还是评估模式？]]></description>
      <guid>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</guid>
      <pubDate>Thu, 17 Dec 2020 16:27:34 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow model.evaluate 给出的结果与训练得到的结果不同</title>
      <link>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</link>
      <description><![CDATA[我正在使用 tensorflow 进行多类分类
我以以下方式加载训练数据集和验证数据集
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

然后当我使用 model.fit() 训练模型
history = model.fit(
train_ds,
validation_data=val_ds,
epochs=epochs,
shuffle=True
)

我的验证准确率约为 95%。
但是当我加载相同的验证集并使用 model.evaluate() 时
model.evaluate(val_ds)

我的准确率非常低（约为 10%）。
为什么我得到的结果如此不同？我是否错误地使用了 model.evaluate 函数？
注意：在 model.compile() 中，我指定了以下内容，
优化器 - Adam，
损失 - SparseCategoricalCrossentropy，
指标 - 准确度
Model.evaluate() 输出
41/41 [================================] - 5s 118ms/step - 损失：0.3037 - 准确度：0.1032
测试损失 - 0.3036555051803589
测试准确度 - 0.10315627604722977

最后三个时期的 Model.fit() 输出
时期8/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.6094 - 准确度：0.8861 - val_loss：0.4489 - val_accuracy：0.9483
Epoch 9/10
41/41 [=============================] - 3s 80ms/步 - 损失：0.5377 - 准确度：0.8953 - val_loss：0.3868 - val_accuracy：0.9554
Epoch 10/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.4663 - 准确度：0.9092 - val_loss：0.3404 - val_accuracy：0.9590
]]></description>
      <guid>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</guid>
      <pubDate>Thu, 24 Sep 2020 15:26:53 GMT</pubDate>
    </item>
    </channel>
</rss>