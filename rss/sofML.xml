<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 30 Jan 2024 18:16:47 GMT</lastBuildDate>
    <item>
      <title>如何从聊天消息中发现主题[关闭]</title>
      <link>https://stackoverflow.com/questions/77908086/how-to-discover-topics-from-chat-messages</link>
      <description><![CDATA[我尝试使用不同的技术和模型处理数据（消息），但它不起作用。我无法从聊天消息中识别连贯的主题，我得到的主题包括“get”、“know”、“go”。
我应该使用其他模型吗？处理数据时是否需要添加更多步骤？
到目前为止我的代码（包括数据输出和绘图）： 这里
使用的数据集：此测试数据集是公开的
下面我提供了有关我所遵循的步骤及其迄今为止的结果的更多详细信息。
处理数据时使用的技术：

删除 nan 值。
删除重复项。
删除特殊字符或非字母字符。
删除常用短语，例如“你好吗”、“谢谢”。
从文本中提取每个单词。
删除停用词 (nlkt)。
词形还原 (en_core_web_sm)。
矢量化 (TfidfVectorizer)。

模型用于识别消息中的主题：

LDA（潜在狄利克雷分配）。
umap。
KMeans。

我希望从聊天消息中识别出连贯的主题。]]></description>
      <guid>https://stackoverflow.com/questions/77908086/how-to-discover-topics-from-chat-messages</guid>
      <pubDate>Tue, 30 Jan 2024 16:53:56 GMT</pubDate>
    </item>
    <item>
      <title>keras 继续训练模型与训练模型一次[关闭]</title>
      <link>https://stackoverflow.com/questions/77908073/keras-continue-training-model-vs-training-a-model-once</link>
      <description><![CDATA[使用完整数据集（比方说 10000 个数据元素）训练模型与使用 8000 个数据元素训练模型，然后使用剩余的 2000 个元素训练模型相比，有哪些优点/缺点/差异？
另外，如果我（第一次）用 8000 个元素训练它......然后第二次用所有 10000 个元素（重复 8000 和 2000 个新元素）......这有什么优点/缺点？
我现在也只有 model.save()...但为什么我要 save_weights() ？
同样，为什么要保存每个纪元迭代（检查点）？
还有纪元数、学习率等......有没有办法以受过教育的方式找到最佳值（而不是猜测/测试试验/错误）？
还有，有没有一种非 miniconda 的方式让我可以让 TensorFlow 使用我的 mbp M2 GPU？我的 mbp 变得很热...]]></description>
      <guid>https://stackoverflow.com/questions/77908073/keras-continue-training-model-vs-training-a-model-once</guid>
      <pubDate>Tue, 30 Jan 2024 16:51:59 GMT</pubDate>
    </item>
    <item>
      <title>在 RaspberryPI 3B 上集成对象检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/77907863/integrating-object-detection-on-raspberrypi-3b</link>
      <description><![CDATA[我正在使用 Raspberry Pi 3B 进行对象检测项目。训练完对象检测模型后，我计划在 Raspberry Pi 3B 上运行它。它是在自定义数据集上训练的 YOLOv8 模型。我担心潜在的延误和准确性问题。听说这个版本的RaspberryPI计算能力有限。在 Raspberry Pi 3B 上部署和运行用于实时应用程序的对象检测模型时，是否有我应该注意的具体注意事项、优化或潜在挑战？关于优化性能和确保准确结果有什么建议吗？
我已经使用 YOLOv8 训练了模型。现在想在Raspberry PI 3B上运行它来获取实时环境信息。]]></description>
      <guid>https://stackoverflow.com/questions/77907863/integrating-object-detection-on-raspberrypi-3b</guid>
      <pubDate>Tue, 30 Jan 2024 16:21:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中对图像进行聚类</title>
      <link>https://stackoverflow.com/questions/77906886/how-can-i-cluster-images-in-python</link>
      <description><![CDATA[我是机器学习和 scikit-learn 的新手。但明天我必须向老师提交任务，请帮忙。我需要从指定目录中的所有图像中提取特征，然后对该图像应用PCA方法并创建散点图，以显示图像分布。
我应该使用像 scikit-learn 这样的库来提取这些特征吗？如何绘制二维散点图？
我已经看过这篇文章https:// /scikit-learn.org/stable/modules/ generated/sklearn.decomposition.PCA.html但仍然没有清晰的理解。]]></description>
      <guid>https://stackoverflow.com/questions/77906886/how-can-i-cluster-images-in-python</guid>
      <pubDate>Tue, 30 Jan 2024 13:55:36 GMT</pubDate>
    </item>
    <item>
      <title>Java Weka API：获取 ROC 面积值</title>
      <link>https://stackoverflow.com/questions/77895012/java-weka-api-getting-roc-area-values</link>
      <description><![CDATA[我正在尝试在 java 类中使用 Weka API。我执行了 10 倍交叉验证，然后使用不同的阈值对数据进行二值化。
但是我是使用 Weka API 的新手，所以不确定我所做的是否正确。我正在获取 ROC 值，但请注意确保它们是正确的。下面是我的代码：
for(int i = 0; i &lt; 阈值.length; i++)
{
     // learnSet 的深拷贝
     ArrayList&gt; learnSetCopy = new ArrayList&lt;&gt;();
     for (ArrayList insideList : learnSet)
     {
         ArrayList; innerCopy = new ArrayList&lt;&gt;();
         for (int[] 数组：innerList)
         {
             int[] arrayCopy = Arrays.copyOf(array, array.length);
             innerCopy.add(arrayCopy);
         }
         learnSetCopy.add(innerCopy);
     }

     // validSet 的深拷贝
     ArrayList&gt; validSetCopy = new ArrayList&lt;&gt;();
     for (ArrayList insideList : validSet)
     {
         ArrayList; innerCopy = new ArrayList&lt;&gt;();
         for (int[] 数组：innerList)
         {
             int[] arrayCopy = Arrays.copyOf(array, array.length);
             innerCopy.add(arrayCopy);
         }
         validSetCopy.add(innerCopy);
     }

     //二值化化学蛋白质相互作用值
     binarizeCpiAttributes(learnSetCopy, 阈值[i]);
     //生成要通过Weka运行的Arff文件
     generateARFF(文件名 + “LearningThreshold” + 阈值[i] + “折叠” + j + “.arff”, attributeNames, learnSetCopy);

     binarizeCpiAttributes(validSetCopy, 阈值[i]);
     generateARFF(文件名 + “ValidThreshold” + 阈值[i] + “Fold” + j + “.arff”, attributeNames, validSetCopy);

     //创建学习和有效arff文件的实例
     实例learningInstances = DataSource.read(fileName + &quot;LearningThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);
     实例 validInstances = DataSource.read(fileName + &quot;ValidThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);

     //设置学习和有效集的类标签
     if(learningInstances.classIndex() == -1)
     {
         LearningInstances.setClassIndex(learningInstances.numAttributes()-1);
     }

     if(validInstances.classIndex() == -1)
     {
         validInstances.setClassIndex(validInstances.numAttributes()-1);
     }

     RandomForest cls = new RandomForest();
     字符串[] 选项 = {
         “-P”、“100”、
         “-I”、“100”、
         “-num-slots”、“1”、
         “-K”、“0”、
         “-M”、“1.0”、
         “-V”、“0.001”、
         “-S”、“1”
     };
     cls.setOptions(选项);
     cls.buildClassifier(learningInstances);

     评估 eval = 新评估(learningInstances);
     eval.evaluateModel(cls​​, validInstances);

     System.out.println(&quot;ROC曲线下面积：&quot; + eval.areaUnderROC(1));

     // 根据需要打印或使用 rocAuc 值
     System.out.println(“处理阈值：”+threshold[i]);
}

这是我得到的输出的示例。我确实认为它们应该更高，这让我怀疑我所做的是否正确：
ROC 曲线下面积：0.6000602772754672
处理阈值：0.4
ROC 曲线下面积：0.5848854731766124
处理阈值：0.5
ROC 曲线下面积：0.594831223628692
处理阈值：0.6
ROC曲线下面积：0.560051235684147
处理阈值：0.7

我在这里所做的是否正确，这是获取 ROC 面积值的正确方法还是我从 Weka API 获取其他值？
我尝试通读所提供的 Weka 文档，但对某些部分感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/77895012/java-weka-api-getting-roc-area-values</guid>
      <pubDate>Sun, 28 Jan 2024 13:56:04 GMT</pubDate>
    </item>
    <item>
      <title>将拥抱脸部音频分类器转换为 TFLite 格式</title>
      <link>https://stackoverflow.com/questions/77892732/convert-hugging-face-audio-classifier-to-tflite-format</link>
      <description><![CDATA[我正在制作一个在 Android 手机上运行的模型，该模型将能够识别一组特定的音频命令。有 6 个命令，所以我需要一个包含 7 个类的分类器，每个命令一个，加上一个用于任何无法识别的类。
为此，我首先使用 facebook/wav2vec2-base，并在每个命令类包含 1000 个示例的数据集上对其进行训练，另外还有 2000 个包含“无法识别”单词的示例。分类器表现出色。
TFLite 似乎是将模型移植到 Android 上的最佳方式，因此使用 optimization 将其导出为 TFLite 格式（optimum-cli export tflite --task audio-classification ...）。这并不容易，因为一开始它失败并出现错误 KeyError: “tflite 后端尚不支持 wav2vec2 (tf_wav2_vec2_for_sequence_classification)。仅支持 [&#39;onnx&#39;]。如果您想支持 tflite，请提出 PR 或提出问题。”。
最终我将其导出到 onnx，然后导出到 tf，最后导出到 TFLite。该模型太大，约 500MB，因此我使用动态量化将其缩小到约 100MB。整数量化确实弄乱了模型，所以我将其保留为动态量化。
我想将该模型与更简单的模型进行比较，但是 wav2vec2 没有任何“小”或“微小”变体。相反，我使用了 Whisper，它旨在用于 ASR（而非分类），使用 openai/whisper-tiny 变体。使用 transformers.WhisperForAudioClassification 加载它，因为分类是我的目标，并在同一数据集上对其进行了微调。尽管体积小得多（30MB），但它的性能优于第一个模型 - 太棒了。
尝试将此模型（用于分类的微调 Whisper 模型）导出到 TFLite 时出现问题：

直接导出到 TFLite（optimum-cli export tflite --task audio-classification ...）不起作用，因为任务 audio-classification 只能识别 Wav2Vec2。因此，将它用于 Whisper 模型会引发 ValueError: Unrecognized configuration class 对于这种 AutoModel：TFAutoModelForAudioClassification。模型类型应为 Wav2Vec2Config 之一。推测的解释：Whisper 本质上是 ASR，而不是分类器。
导出到 ONNX（optimum-cli export onnx --task audio-classification ...）不起作用，因为 ValueError：要求导出任务音频的耳语模型 -分类，但 Optimum ONNX 导出器仅支持特征提取、过去特征提取、自动语音识别、耳语自动语音识别等任务。请使用支持的任务。如果您希望 ONNX 导出耳语支持任务音频分类，请在 https://github.com/huggingface/optimum/issues 上提出问题。

如何让这个模型在 Android 上运行？]]></description>
      <guid>https://stackoverflow.com/questions/77892732/convert-hugging-face-audio-classifier-to-tflite-format</guid>
      <pubDate>Sat, 27 Jan 2024 20:36:25 GMT</pubDate>
    </item>
    <item>
      <title>线性回归均方根误差</title>
      <link>https://stackoverflow.com/questions/77892345/linear-regression-rmse</link>
      <description><![CDATA[尝试比较不同多项式次数的均方根误差，但最终得到相同的 RMSE。
train_rmse_errors=[]
test_rmse_errors=[]

对于范围 (1,20) 中的 d：
    
    poly_converter = 多项式特征（度=d，include_bias=False）
    poly_fearures = poly_converter.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)
    模型=线性回归()
    model.fit(X_train,y_train)
    
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_rmse = np.sqrt(mean_squared_error(y_train,train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test,test_pred))
    
    train_rmse_errors.append(train_rmse)
    test_rmse_errors.append(test_rmse)
]]></description>
      <guid>https://stackoverflow.com/questions/77892345/linear-regression-rmse</guid>
      <pubDate>Sat, 27 Jan 2024 18:38:33 GMT</pubDate>
    </item>
    <item>
      <title>我无法运行二元期权机器人</title>
      <link>https://stackoverflow.com/questions/77892067/i-can-not-run-a-binary-option-bot</link>
      <description><![CDATA[我正在使用这个 git hub 存储库
https://github.com/ItamarRocha/binary-bot,
当我运行 pip install -rrequirements.txt
我收到此错误
E:\binary-bot&gt;pip install -rrequirements.txt
收集 iqoptionapi@ git+git://github.com/Lu-Yi-Hsun/iqoptionapi.git@e96ba2c5
b905a139a4765167b08c5df48cf57773 来自 git+git://github.com/Lu-Yi-Hsun/iqoptionap
i.git@e96ba2c5b905a139a4765167b08c5df48cf57773（来自 -rrequirements.txt（第 1 行
））
  克隆 git://github.com/Lu-Yi-Hsun/iqoptionapi.git （修订版 e96ba2c5b905a
139a4765167b08c5df48cf57773) 到 c:\users\pars\appdata\local\temp\pip-install-fq4
i3vho\iqoptionapi
  运行命令 git clone -q git://github.com/Lu-Yi-Hsun/iqoptionapi.git &#39;C:\U
sers\pars\AppData\Local\Temp\pip-install-fq4i3vho\iqoptionapi&#39;
  致命：无法从远程存储库读取。

  请确保您拥有正确的访问权限
  并且存储库存在。
错误：命令出错，退出状态为 128：git clone -q git://github.com/L
u-Yi-Hsun/iqoptionapi.git &#39;C:\Users\pars\AppData\Local\Temp\pip-install-fq4i3vho
\iqoptionapi&#39; 检查日志以获取完整的命令输出。
警告：您正在使用 pip 版本 19.2.3，但版本 23.3.2 可用。
您应该考虑通过“python -m pip install --upgrade pip”通讯进行升级
和。

E:\binary-bot&gt;

如何解决？
我尝试使用 pip install -rrequirements.txt 安装该要求
但它给出了错误]]></description>
      <guid>https://stackoverflow.com/questions/77892067/i-can-not-run-a-binary-option-bot</guid>
      <pubDate>Sat, 27 Jan 2024 17:11:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 Torchio 对两个图像应用完全相同的变换</title>
      <link>https://stackoverflow.com/questions/77892019/apply-the-exact-same-transformation-to-two-images-using-torchio</link>
      <description><![CDATA[我想使用 torchio 对两个图像（图像和分割数据）应用完全相同的转换。这两个图像都存储在名为 image_data 和 segmentation_data 的 numpy 数组中。
到目前为止，我添加了一些增强功能：
self.augmentations = tio.Compose([
            仿射变换，
            弹性变换，
            翻转变换，
            交换变换
        ]）

例如， elastic_transform = tio.RandomElasticDeformation 并尝试通过以下方式将它们应用到图像：
 subject_image = tio.Subject(image=tio.ScalarImage(tensor=image_data))
        subject_segmentation = tio.Subject(
            图像=tio.ScalarImage(张量=segmentation_data))
        数据集 = tio.SubjectsDataset([subject_image, subject_segmentation])
        数据集 = self.augmentations(数据集)
        image_data = 数据集[0][&#39;image&#39;].data
        分段数据 = 数据集[1][&#39;图像&#39;].data

不幸的是，这是不正确的（因为 Compose 不适用于主题数据集）。如何正确地做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/77892019/apply-the-exact-same-transformation-to-two-images-using-torchio</guid>
      <pubDate>Sat, 27 Jan 2024 16:57:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 sympy 时消除格式字符串冲突</title>
      <link>https://stackoverflow.com/questions/77891386/get-rid-of-format-string-conflict-when-using-sympy</link>
      <description><![CDATA[当我尝试使用 sympy 求损失函数的导数时，它与格式字符串发生冲突。
将 numpy 导入为 np
将 sympy 导入为 sp

def 预测（X，w，b）：
    返回 np.dot(X, w) + b

def 损失(X, w, b, Y):
    返回 np.mean((预测(X, w, b) - Y) ** 2)

X, Y = np.loadtxt(“code/02_first/pizza.txt”，unpack=True，skiprows=1)

# 将 X 和 Y 转换为 sympy 符号
X, w, b, Y = sp.symbols(“X w b Y”)

def 梯度(X, w, b, Y):
    loss_expr = 损失(X, w, b, Y)
    dw_dX = sp.diff(loss_expr, w)
    db_dX = sp.diff(loss_expr, b)
    返回 dw_dX, db_dX

def train(X, Y, 迭代, lr):
    w = sp.symbols(&#39;w&#39;)
    b = sp.symbols(&#39;b&#39;)
    
    对于范围内的 i（迭代）：
        损失值 = 损失(X, w, b, Y)
        print(f&quot;迭代: {i:4d}, 损失: {loss_value:.10f}&quot;)
        dw_dX, db_dX = 梯度(X, w, b, Y)
        w -= dw_dX * lr
        b -= db_dX * lr
    返回w,b

w, b = 训练(X, Y, 迭代=20000, lr=0.001)

print(f&quot;\nw = {w:.10f}, b = {b:.10f}&quot;)
print(f&quot;预测: x = 20 ==&gt; y = {predict(20, w, b):.2f}&quot;)

TypeError：传递给 Pow.__format__ 的格式字符串不受支持

数据以 txt 形式存在（或通过链接此处&lt; /a&gt;):
预订披萨
13 33
2 16
14 32
23 51
13 27
1 16
18 34
10 17
26 29
3 15
3 15
21 32
7月22日
22 37
2 13
27 44
6 16
10 21
18 37
15 30
9 26
26 34
8月23日
15 39
10 27
21 37
5 17
6 18
13 25
13 23

我可以只使用numpy，但这样做我需要自己计算损失函数，这不是有效的（并且很容易用括号引发错误）。
为什么会出现错误以及为什么 sympy 与格式字符串不兼容？另外，如何使用 sympy 生成正确的脚本？]]></description>
      <guid>https://stackoverflow.com/questions/77891386/get-rid-of-format-string-conflict-when-using-sympy</guid>
      <pubDate>Sat, 27 Jan 2024 13:35:43 GMT</pubDate>
    </item>
    <item>
      <title>两阶段推荐系统中的top-K推荐[关闭]</title>
      <link>https://stackoverflow.com/questions/77890505/top-k-recommendations-in-two-stage-recommendation-system</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77890505/top-k-recommendations-in-two-stage-recommendation-system</guid>
      <pubDate>Sat, 27 Jan 2024 08:22:43 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 部署到 sagemaker</title>
      <link>https://stackoverflow.com/questions/77889951/pytorch-deployment-to-sagemaker</link>
      <description><![CDATA[我已压缩并保存到 s3，如下所示：
导入 tarfile
使用 tarfile.open(&#39;model.tar.gz&#39;, mode=&#39;w:gz&#39;) 作为存档：
        archive.add(&#39;模型&#39;, recursive=True)

进口圣人

sagemaker_session = sagemaker.Session()
输入= sagemaker_session.upload_data(path=&#39;model.tar.gz&#39;, key_prefix=&#39;model&#39;)


尝试像这样部署：

从 sagemaker 导入 get_execution_role
从 sagemaker.pytorch.model 导入 PyTorchModel
角色 = get_execution_role()

pytorch_model = PyTorchModel(model_data= &#39;s3://&#39; + sagemaker_session.default_bucket() + &#39;/model/model.tar.gz&#39;, role=role,
                             Entry_point=&#39;inference.py&#39;,framework_version =&#39;2.1&#39;,py_version=&#39;py310&#39;)

预测器= pytorch_model.deploy（instance_type =&#39;ml.p3.2xlarge&#39;，initial_instance_count = 1）

但我收到错误：
FileNotFoundError: [Errno 2] 没有这样的文件或目录: &#39;inference.py&#39;

我尝试了上面解释的所有内容]]></description>
      <guid>https://stackoverflow.com/questions/77889951/pytorch-deployment-to-sagemaker</guid>
      <pubDate>Sat, 27 Jan 2024 03:20:13 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker实例中的CUDA路径解决NameError：名称'_C'未使用GroundingDINO定义</title>
      <link>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</link>
      <description><![CDATA[我正在尝试在 Sagemaker 实例中安装和使用 grounding dino（使用 GPU ）但我收到错误：
NameError：名称“_C”未定义

我发现原因是因为变量CUDA_HOME没有配置所以要解决它我需要设置变量，但是在搜索答案后（我已经检查了公共路径/usr/local/cuda）我找不到sagemaker实例中cuda的安装路径。
cuda 安装在 sagemaker 实例中的什么位置以便我可以设置 CUDA_HOME？]]></description>
      <guid>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</guid>
      <pubDate>Fri, 26 Jan 2024 18:45:06 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Gemini-pro 模型构建带提示模板的会话缓冲存储器</title>
      <link>https://stackoverflow.com/questions/77671882/how-to-build-a-conversational-buffer-memory-with-prompt-template-for-gemini-pro</link>
      <description><![CDATA[嗨，我正在尝试使用 Gemini-pro LLM 模型构建一个具有内存支持的对话聊天机器人
我收到此错误消息：
ChatGoogleGenerativeAIError：Gemini 不支持“系统”类型的消息。请仅向其提供人类或人工智能（用户/助理）消息。
这是我的代码：
from langchain.chains import LLMChain
从 langchain.prompts 导入 (
    聊天提示模板，
    HumanMessagePrompt模板，
    消息占位符，
    系统消息提示模板，
）
从 langchain.chains 导入 ConversationChain
从 langchain.memory 导入 ConversationBufferMemory



＃ 迅速的
提示 = 聊天提示模板(
    消息=[
        SystemMessagePromptTemplate.from_template(
            “你是一名课程推荐者，你向候选人询问几个问题，以了解他的个人兴趣、最终目标和当前的技能水平，并向他提供一份精选的课程列表，并注明其难度级别为初级、中级和高级。”
        ),
        # 这里的`variable_name`必须与内存对齐
        MessagesPlaceholder(variable_name=&quot;chat_history&quot;),
        HumanMessagePromptTemplate.from_template(“{问题}”),
    ]
）

# 请注意，我们使用 `return_messages=True` 来适应 MessagesPlaceholder
# 请注意，“chat_history”与 MessagesPlaceholder 名称一致
内存 = ConversationBufferMemory(memory_key=“chat_history”, return_messages=True)
对话= LLMChain（llm = llm，提示=提示，详细=真，内存=内存）

# 请注意，我们只是传入 `question` 变量 - `chat_history` 由内存填充
对话({“问题”:“嗨”})

我试图给出系统提示，但它说 Gemini 型号不支持系统提示]]></description>
      <guid>https://stackoverflow.com/questions/77671882/how-to-build-a-conversational-buffer-memory-with-prompt-template-for-gemini-pro</guid>
      <pubDate>Sat, 16 Dec 2023 17:40:34 GMT</pubDate>
    </item>
    <item>
      <title>Killed:9 是什么以及如何在 macOS 终端中修复？</title>
      <link>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</link>
      <description><![CDATA[我有一个用于机器学习项目的简单 Python 代码。我有一个比较大的自发演讲数据库。我开始训练我的语音模型。由于它是一个巨大的数据库，我让它在一夜之间工作。早上醒来，我看到了一个神秘的东西
死亡人数：9
我的终端中的行。没有其他的。没有其他错误消息或可处理的内容。代码运行良好大约 6 个小时，占整个过程的 75%，所以我真的不明白出了什么问题。
什么是 Killed:9 以及如何修复它？损失数小时的计算时间是非常令人沮丧的......
如果有必要的话，我正在使用 macOS Mojave beta。预先感谢您！]]></description>
      <guid>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</guid>
      <pubDate>Tue, 14 Aug 2018 03:28:58 GMT</pubDate>
    </item>
    </channel>
</rss>