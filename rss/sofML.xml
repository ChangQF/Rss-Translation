<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Jan 2024 12:27:59 GMT</lastBuildDate>
    <item>
      <title>在本地部署 Azure autoML 模型</title>
      <link>https://stackoverflow.com/questions/77859735/deploying-azure-automl-model-locally</link>
      <description><![CDATA[我对 Azure 还很陌生，目前面临着一个似乎无法解决的问题。我已经使用 Azure AutoML 成功训练了一些非常有前途的模型，现在我想将它们部署在本地。
我使用简单的 CSV 文件作为数据集（使用 Azure ML v1 API）。当我尝试下载相应的模型并将其插入Conda环境（按照Azure指定设置）时，我总是收到错误消息：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ModuleNotFoundError Traceback（最近一次调用最后一次）
单元格 In[1]，第 12 行
      9 将 pandas 导入为 pd
     10 导入joblib
---&gt; 12 导入 azureml.automl.core
     13 从 azureml.automl.core.shared 导入logging_utilities、log_server
     14 从 azureml.telemetry 导入 INSTRUMENTATION_KEY

ModuleNotFoundError：没有名为“azureml”的模块

但是，我不认为它与软件包有关。当我尝试通过 Azure 中的实时端点部署模型并使用“测试”测试它们时，成功执行后，我收到以下错误消息：
测试端点失败
{“message”：“评分脚本中发生意外错误。检查日志以获取更多信息。”}

我是否忽略了什么？感谢任何帮助。
谢谢，伊曼纽尔]]></description>
      <guid>https://stackoverflow.com/questions/77859735/deploying-azure-automl-model-locally</guid>
      <pubDate>Mon, 22 Jan 2024 12:07:02 GMT</pubDate>
    </item>
    <item>
      <title>将图像处理与 CNN 结合使用，利用历史数据和非线性预测模型对时尚品牌进行销售预测</title>
      <link>https://stackoverflow.com/questions/77859348/using-image-processing-with-cnn-for-sales-forecasting-using-historical-data-and</link>
      <description><![CDATA[我想开发一种算法，使用图像处理将具有详尽历史数据的旧产品图像与没有历史数据的新产品图像进行匹配，并预测这些新图像的销量。我想了解我应该使用什么模型，图像处理是否可行，图像处理和随后的 cnn 聚类会导致大量数据丢失，是否有其他方法执行此过程，因为我需要考虑面料的坠落、服装的颜色、廓形等
我想尝试使用像 Inception v3 这样的机器学习模型进行图像处理，但是准确度水平低于标准，我需要更高的准确度才能做出更好的预测，这些预测直接影响库存计划]]></description>
      <guid>https://stackoverflow.com/questions/77859348/using-image-processing-with-cnn-for-sales-forecasting-using-historical-data-and</guid>
      <pubDate>Mon, 22 Jan 2024 11:00:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么感知器没有按预期进行训练？</title>
      <link>https://stackoverflow.com/questions/77858926/why-is-the-perceptron-not-training-as-expected</link>
      <description><![CDATA[我试图学习神经网络，并从感知器开始。
我看了一些教程并完全按照它们进行操作，但它对我不起作用。


var canvas = document.querySelector(&#39;canvas&#39;)
画布宽度=内部宽度；
画布高度=内部高度；
var c = canvas.getContext(&#39;2d&#39;)



函数符号（val）{
  如果（值&gt;= 0）{
    返回1
  } 别的 {
    返回-1
  }
}

类感知器{
  构造函数（数字）{
    this.权重 = []
    这个.lr = 0.1

    for (var i = 0; i &lt; num; i++) {
      this.weights.push(Math.random() * 2 - 1)
    }
  }

  猜测（输入）{
    变量总和 = 0
    for (var i = 0; i &lt; this.weights.length; i++) {
      sum += this.weights[i] * 输入[i]
    }
    var 输出 = 符号（总和）
    返回输出
  }

  训练（输入，目标）{
    var 猜测 = this.guess(输入)
    var 错误 = 目标 - 猜测

    for (var i = 0; i &lt; this.weights.length; i++) {
      this.weights[i] += 错误 * 输入[i] * this.lr
    }
  }
}

var Brain = 新感知器(2)

变量点 = []


类点{
  构造函数（x，y）{
    这个.x = x
    这个.y = y
    这个.标签 = 0

    c.线宽=2

    if (this.y &lt; canvas.height / 2) {
      这个.标签 = 1
    } else if (this.y &gt; canvas.height / 2) {
      这个.标签 = -1
    }
  }
  画（） {
    c.beginPath()
    c.arc(this.x, this.y, 10, 0, Math.PI * 2, false)
    c.fill()
    if (this.label == 1) {
      c.中风()
    }
  }
  更新（） {



    this.draw()
  }
}

for (var i = 0; i &lt; 100; i++) {
  point.push(new Point(Math.random() * canvas.width, Math.random() * canvas.height))
}


函数动画（）{
  请求动画帧（动画）
  c.clearRect(0, 0, canvas.width, canvas.height)
  点.forEach(点=&gt;{


    Brain.train([point.x, point.y], point.label)
    var猜测 = Brain.guess([point.x, point.y])
    if (猜测==点.标签) {
      c.fillStyle = &quot;绿色&quot;
    } 别的 {
      c.fillStyle = &quot;红色&quot;
    }
    点.update()
  })
}

动画（）
&lt;代码&gt;* {
  保证金：0；
  填充：0；
}

帆布 {
  位置：绝对；
}




这是我的代码
谁能告诉我这是怎么回事吗？
我的目标是让感知器对点进行分类，其中屏幕一半以上的点应具有 1 的值，而屏幕一半以下的点应具有 -1 的值]]></description>
      <guid>https://stackoverflow.com/questions/77858926/why-is-the-perceptron-not-training-as-expected</guid>
      <pubDate>Mon, 22 Jan 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么通过 Conda 安装不平衡学习库可以工作，但通过 Pip3 则不行？</title>
      <link>https://stackoverflow.com/questions/77858897/why-does-installing-imbalanced-learn-library-work-if-its-done-via-conda-but-not</link>
      <description><![CDATA[通过Pip3成功安装imbalanced-learn库后，
pip 安装 imblearn
当我尝试从 Python 笔记本中的 imblearn.over_sampling 模块导入 SMOTE 算法时，
从 imblearn.over_sampling 导入 SMOTE
我意外地遇到了导入错误，指出：
&lt;块引用&gt;
导入错误：“无法从“sklearn.utils._param_validation”导入名称“_MissingValues”。

幸运的是，随后卸载了不平衡学习，然后再次安装它，这次使用Conda，从那时起就没有错误了：

pip uninstall imblearn --yes

conda install -c conda-forge不平衡学习


所以，在遇到并解决这个问题之后，我首先尝试找出发生这种情况的原因。不幸的是，我没有发现这种差异背后的任何具体内容。]]></description>
      <guid>https://stackoverflow.com/questions/77858897/why-does-installing-imbalanced-learn-library-work-if-its-done-via-conda-but-not</guid>
      <pubDate>Mon, 22 Jan 2024 09:43:54 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Spacy Model .pkl 文件转换为 .pt/.pth pytorch 支持的格式</title>
      <link>https://stackoverflow.com/questions/77858297/how-to-convert-spacy-model-pkl-file-into-pt-pth-pytorch-supported-format</link>
      <description><![CDATA[我有 spacy 模型，用于以 .pkl 格式进行推理。 .pkl 文件的数据类型是。我想让推理脚本在 GPU 上运行。我尝试使用 spacy gpu、numba 等不同的方法。
导入spacy
spacy.prefer_gpu() # 或 spacy.require_gpu()
nlp = spacy.load(“内容/路径”)

它不起作用，我认为通过将 .pkl 转换为 .pt 将通过将 pt 文件加载到“cuda”设备来工作。请提出处理这种情况的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77858297/how-to-convert-spacy-model-pkl-file-into-pt-pth-pytorch-supported-format</guid>
      <pubDate>Mon, 22 Jan 2024 07:43:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 上安装和运行 Docker Compose？</title>
      <link>https://stackoverflow.com/questions/77858250/how-to-install-and-run-docker-compose-on-google-colab</link>
      <description><![CDATA[有人可以告诉我如何在 Google Colab 上运行 docker compose。
我已经尝试过很多次了。大多数时候我在 google colab 中都会遇到错误。
$ docker-compose up -d

我正在尝试使用 docker-compose 部署我的 Spring Boot Web 服务。在我的 docker-compose.yml 中
https://github.com/milvus-io /bootcamp/tree/e69ee26188cf24c4994dfd9eecf00ef3950fcd44/applications/image/reverse_image_search
在 Google Colab 上运行 Docker]]></description>
      <guid>https://stackoverflow.com/questions/77858250/how-to-install-and-run-docker-compose-on-google-colab</guid>
      <pubDate>Mon, 22 Jan 2024 07:30:12 GMT</pubDate>
    </item>
    <item>
      <title>多种产品的预测模型</title>
      <link>https://stackoverflow.com/questions/77857775/forecasting-model-for-multiple-products</link>
      <description><![CDATA[嗨，我是数据科学的一个相对较新的人，我有一个时间序列问题，我必须预测 100 多种产品的销售，并且所有产品都有不同的模式，而且新产品会不断添加，这很困难要单独建模它们，我还必须设置再训练流程，如何简化这个过程，有没有什么方法可以概括模型选择、验证和再训练，而不必每次都单独建模？
到目前为止，我已经尝试对它们进行单独建模，使用 SARIMA，但它并不适用于我的所有产品。我的数据由日期和体积组成，而且在大多数情况下，我在 acf 和 pcf 图中也没有观察到任何重要点。]]></description>
      <guid>https://stackoverflow.com/questions/77857775/forecasting-model-for-multiple-products</guid>
      <pubDate>Mon, 22 Jan 2024 05:19:57 GMT</pubDate>
    </item>
    <item>
      <title>如何提高YOLOv8自定义模型的准确率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</link>
      <description><![CDATA[我最近使用 YOLOv8 训练了一个用于动物物种检测的自定义数据集，但得到的结果非常糟糕。我有 70 多个类，每个类平均约有 80 张图像。我对模型进行了 200 个时期的训练，大约花费了 1.5 天。
我使用 Roboflow 来注释数据集。
我首先在 100 个 epoch 上训练模型，这给了我不好的结果，所以我再次尝试在 200 个 epoch 上训练模型，这比之前的模型得到了更好的结果。模型的准确率很低，没有达到我的预期。所以我需要一些帮助来提高模型的准确性。 我打算将类别增加到 110-115，但从当前的训练结果来看，我首先尝试使模型的准确率至少达到 75%。]]></description>
      <guid>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</guid>
      <pubDate>Sun, 21 Jan 2024 17:57:19 GMT</pubDate>
    </item>
    <item>
      <title>Flower - 联邦学习的每一轮模型精度都是相同的</title>
      <link>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</guid>
      <pubDate>Sun, 21 Jan 2024 14:33:27 GMT</pubDate>
    </item>
    <item>
      <title>在微调LLM模型时如何给数据集赋予权重或排名？</title>
      <link>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</link>
      <description><![CDATA[我目前正在使用 Llama 配方和 LoRA 技术对 meta-llama/Llama-2-7b-chat-hf 模型进行微调。我的方法包括采用即时工程来改进模型的性能，利用以 Alpaca 格式呈现的数据：
&lt;前&gt;&lt;代码&gt;[
    {
        &quot;instruction&quot;: &quot;什么是 CubeOS？&quot;,
        “输入”：“”，
        “输出”：“CubeOS 是专门的操作系统，包含操作 Cube 所需的所有软件和驱动程序。”
    },
    {
        &quot;instruction&quot;: &quot;Myst 是什么？&quot;,
        “输入”：“”，
        “output”：“Myst 作为 Cube 的控制台界面，也是随附应用程序的指定名称。”
    },
    。
    。
    。
]

这个过程使我能够有效地微调模型并将其应用于回答与机密文档相关的问题。
我尝试过为问答对分配分数，然后按照这些分数的顺序排列它们以进行微调。然而，我遇到了挑战，因为该模型似乎没有根据分数较高的数据的重要性给出结果。
我发现了一篇标题为 https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92，作者似乎采用了类似的方法。&lt; /p&gt;
此外，我还探索了其他资源，提出了一种方法，涉及确定各个特征的相关性分数，然后对它们进行排序以进行微调。以下链接 https://pyvideo.org /pydata-warsaw-2019/learning-to-rank-with-the-transformer.html 提供了对此技术的见解。
我还尝试使用 llama-2 所需的提示结构来训练模型：
[INST] &lt;&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ 用户消息 }} [/INST]

但是，这种方法并没有对答案产生令人满意的强调。
鉴于我拥有 PDF 和 DOC 格式的文档，我的目标是为特定文档分配更大的权重，并确保它们优先出现在最佳答案中。
我非常感谢您指导如何通过合并权重或分数来强调某些文档的重要性来微调模型。]]></description>
      <guid>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</guid>
      <pubDate>Wed, 23 Aug 2023 05:01:40 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSeq2SeqLM 和 AutoModelForCausalLM 之间的区别</title>
      <link>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</link>
      <description><![CDATA[根据标题，Huggingface 上的这两个自动类有何不同？我尝试阅读文档，但没有找到区分信息]]></description>
      <guid>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</guid>
      <pubDate>Thu, 23 Feb 2023 19:45:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在海量数据上训练机器学习模型？</title>
      <link>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</link>
      <description><![CDATA[关键点：数据集太大了，我几乎无法将其存储在硬件中。 （拍字节）
假设我的数据集中有数万亿行。该数据集太大，无法存储在内存中。我想在这个数据集上训练一个机器学习模型，比如逻辑回归。我该怎么办？
现在，我知道亚马逊/谷歌在大量数据上进行机器学习。他们怎样做呢？例如点击数据集，全局每个智能设备的输入都存储在一个数据集中。
拼命寻找新想法并乐于接受修正。
我的思路：

加载部分数据到内存
执行梯度下降

这样优化就是小批量下降。
现在的问题是，在优化中，无论是SGD还是mini Batch，在最坏的情况下，当它遍历完所有数据时就会停止。遍历整个数据集是不可能的。
所以我有了提前停止的想法。提前停止保留验证集，并在错误停止下降/收敛于验证集时停止优化。但由于数据集的大小，这可能不可行。
现在我正在考虑简单地随机采样训练集和测试集，并使用可行的大小来训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</guid>
      <pubDate>Thu, 22 Dec 2022 09:16:33 GMT</pubDate>
    </item>
    <item>
      <title>Google Colaboratory：有关其 GPU 的误导性信息（仅 5% RAM 可供某些用户使用）</title>
      <link>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</link>
      <description><![CDATA[更新：这个问题与Google Colab的“笔记本设置：硬件加速器：GPU”有关。这个问题是在添加“TPU”选项之前写的。
阅读了多个关于 Google Colaboratory 提供免费 Tesla K80 GPU 的激动人心的公告，我尝试运行 fast.ai 课程让它永远无法完成 - 内存很快就耗尽了。我开始调查原因。
最重要的是，“免费 Tesla K80”并不是对所有人来说都是“免费”的——对于某些人来说，只有一小部分是“免费”的。 
我从加拿大西海岸连接到 Google Colab，但本应是 24GB GPU RAM 的却只有 0.5GB。其他用户可以使用 11GB GPU RAM。
显然 0.5GB GPU RAM 不足以满足大多数 ML/DL 工作的需要。
如果您不确定自己得到什么，这里是我整理的一些调试功能（仅适用于笔记本电脑的 GPU 设置）：
# 内存占用支持库/代码
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip 安装 gputil
!pip 安装 psutil
!pip 安装人性化
导入 psutil
导入人性化
导入操作系统
导入 GPUtil 作为 GPU
GPU = GPU.getGPUs()
# XXX：Colab 上只有一个 GPU，且无法保证
GPU = GPU[0]
def printm():
 进程 = psutil.Process(os.getpid())
 print(&quot;Gen RAM 空闲：&quot; + humanize.naturalsize( psutil.virtual_memory().available ), &quot; | Proc 大小：&quot; + humanize.naturalsize( process.memory_info().rss))
 print(&quot;GPU RAM 可用：{0:.0f}MB | 已用：{1:.0f}MB | Util {2:3.0f}% | 总计 {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed、gpu.memoryUtil*100、gpu.memoryTotal))
打印（）

在运行任何其他代码之前在 jupyter 笔记本中执行它会给我：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：566MB |已用：10873MB |利用率 95% |总计 11439MB

获得完整卡的幸运用户将看到：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：11439MB |已用：0MB |利用率 0% |总计 11439MB

您是否发现我从 GPUtil 借用的 GPU RAM 可用性计算有任何缺陷？
您能否确认，如果您在 Google Colab 笔记本上运行此代码，您会得到类似的结果吗？
如果我的计算正确，有什么办法可以在免费盒子上获得更多 GPU RAM 吗？
更新：我不确定为什么我们中的一些人得到的只是其他用户的 1/20。例如帮助我调试这个的人来自印度，他掌握了全部内容！
注意：请不要再发送任何有关如何消除可能消耗 GPU 部分的潜在卡住/失控/并行笔记本的建议。不管你如何划分它，如果你和我在同一条船上并运行调试代码，你会发现你仍然获得总共 5% 的 GPU RAM（截至本次更新仍然如此）。]]></description>
      <guid>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</guid>
      <pubDate>Mon, 12 Feb 2018 15:44:14 GMT</pubDate>
    </item>
    <item>
      <title>Keras LSTM - 验证损失从 Epoch #1 开始增加</title>
      <link>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</guid>
      <pubDate>Wed, 31 Jan 2018 12:40:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在keras中将词嵌入和softmax权重结合起来？</title>
      <link>https://stackoverflow.com/questions/47095673/how-to-tie-word-embedding-and-softmax-weights-in-keras</link>
      <description><![CDATA[对于 NLP 和视觉语言问题中的各种神经网络架构来说，将初始词嵌入层的权重与输出 softmax 的权重联系起来是很常见的。通常这会提高句子生成的质量。 （参见示例此处）
在 Keras 中，通常使用 Embedding 类嵌入单词嵌入层，但是似乎没有简单的方法将该层的权重与输出 softmax 联系起来。有人知道如何实现这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/47095673/how-to-tie-word-embedding-and-softmax-weights-in-keras</guid>
      <pubDate>Fri, 03 Nov 2017 12:20:38 GMT</pubDate>
    </item>
    </channel>
</rss>