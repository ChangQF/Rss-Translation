<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 21:12:56 GMT</lastBuildDate>
    <item>
      <title>在 Rust 的“ort”包中使用 ONNX CLIP ViT-B-32，收到有关无效输入维度的错误</title>
      <link>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</guid>
      <pubDate>Tue, 06 Feb 2024 20:43:57 GMT</pubDate>
    </item>
    <item>
      <title>图像的梯度批处理</title>
      <link>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</link>
      <description><![CDATA[目前我有点迷失了，我正在尝试使用 gradio.File() 处理 gradio 中的图像，但它需要一个 Zip 文件，然后我必须解压缩它并复制路径，然后打开图像。我的算法适用于 gr.image() 打开的图像，但是当我尝试对多个图像执行相同操作时，它无法正常工作。我试图找出答案，但没有成功。我想批量打开图像，它应该与 gr.Image() 相同，但多个图像。
我尝试了选择 zip 文件的方法，但它无法正确处理图像。但对于像 gr.Image() 这样的单个图像，它工作得很好。]]></description>
      <guid>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</guid>
      <pubDate>Tue, 06 Feb 2024 20:09:38 GMT</pubDate>
    </item>
    <item>
      <title>从头开始反向传播方法出错</title>
      <link>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-from-scratch</link>
      <description><![CDATA[我正在尝试创建一个预测加密价格的人工智能，现在我在反向传播方法中遇到了这个持续存在的错误（特别是关于 np.dot 语句中用于计算新权重的数组规模） ），我认为这可能是由于某个功能造成的，但我不知道如何纠正它。
类 Layer_Dense：
    def __init__(self, n_neurons, 权重, 偏差):
        self.n_neurons = np.array(n_neurons)
        self.weights = np.array(权重)
        self.biases = np.array(偏差)
    def 前向（自身，输入）：
        self.inputs = np.array(输入)
        self.output = np.dot(self.weights, 输入) + self.biases
    def 反向传播（自身，梯度，学习率 = 0.01）：
        梯度 = np.array(梯度)
        # 计算相对于权重和偏差的梯度
        weights_gradient = np.dot(梯度, self.inputs.T) / len(self.inputs)
        biases_gradient = np.sum(梯度) / len(self.inputs)

        # 使用一些优化算法（例如梯度下降）更新权重和偏差
        self.weights -= 学习率 * 权重梯度
        self.biases -= 学习率 *biases_gradient
        
        self.weights = np.array(self.weights)

        # 返回相对于下一层输入的梯度
        返回 np.dot(self.weights.T, 梯度)



[...]



神经网络层 = [
    Layer_Dense（neural_network_layers_and_its_neurons[1]，weights_list_of_matrises_for_continuation[0]，biases_matrix_continuation[0]），
    Layer_Dense（neural_network_layers_and_its_neurons[2]，weights_list_of_matrises_for_continuation[1]，biases_matrix_continuation[1]），
    Layer_Dense（neural_network_layers_and_its_neurons[3]，weights_list_of_matrises_for_continuation[2]，biases_matrix_continuation[2]），
    Layer_Dense（neural_network_layers_and_its_neurons[4]，weights_list_of_matrises_for_continuation[3]，biases_matrix_continuation[3]），
    Layer_Dense(neural_network_layers_and_its_neurons[5]、weights_list_of_matrises_for_continuation[4]、biases_matrix_continuation[4])
    ]




[...]




对于范围内的 i（0，Batch_size）：

        真实输出 = []

        对于 X 中的 i：
            
            当前输入=我

            # 前向传递各层
            对于 enumerate(neural_network_layers) 中的 a 层：
                层.forward（当前输入）
                如果 a != (len(neural_network_layers)-1):
                    当前输入 = 向前（层.输出）
                如果 a == (len(neural_network_layers)-1):
                    当前输入 = 层.输出
                    当前输入 = 当前输入[0]

            real_outputs.append（当前输入）

        loss_a、accuracy_a = F.accuracy_and_or_loss_in_one_output_NN(expected_outputs, real_outputs, 0)

        准确度.append(accuracy_a)
        损失.追加（loss_a）
        
#这是我之前讨论过的函数

        loss_gradient = F.gradient_of_loss(expected_outputs, real_outputs)
        
        如果 a12 == 1：
            损失_b = 损失_a + 1e10
        
        当前梯度 = 损失梯度
        对于反向层（neural_network_layers）：
            current_gradient = layer.backpropagation(current_gradient,learning_rate)


def梯度损失（真实值，预测值）：
    true_values = np.array(true_values_)
    预测值 = np.array(预测值_)


    # 计算真实值和预测值之间的差异
    梯度 = 预测值 - 真实值

    返回梯度

我尝试过重塑数组，我尝试过转置它们，我已经改变了损失函数的梯度大约10次，我已经观看了3个有关偏导数的数学youtube视频，但没有任何效果]]></description>
      <guid>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-from-scratch</guid>
      <pubDate>Tue, 06 Feb 2024 18:02:18 GMT</pubDate>
    </item>
    <item>
      <title>Faiss GPU索引传递给拥抱面部训练器时无法序列化</title>
      <link>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</guid>
      <pubDate>Tue, 06 Feb 2024 16:45:32 GMT</pubDate>
    </item>
    <item>
      <title>Prophet 1.1.5 模型在通过 model_to_json 和 model_from_json 保存和加载后损坏</title>
      <link>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</link>
      <description><![CDATA[当模型拟合然后通过 model_to_json 和 model_from_json 保存和加载时，它不会提供准确的预测。如果跳过保存和加载过程并在拟合后立即使用模型进行预测，则不会出现问题。
加载 Prophet 模型后，显示 1970 年的所有数据相隔 1 秒，而不是 2023/24 相隔 15 分钟/1 小时。预测数据最终也以 1970 年代相隔 1 秒 (make_future_dataframe)。此外，预测数据往往不太准确。
当拟合和预测结合在一起时，绕过保存和加载阶段，Prophet 的行为符合预期。仅当我将模型保存到云存储桶或文件系统中时，才会出现此问题。
我比较了plot命令的输出。在保存和加载之前，其输出是准确的并且符合预期。
我怎样才能保存和保存？加载模型而不损坏模型？建议采取哪些故障排除步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</guid>
      <pubDate>Tue, 06 Feb 2024 16:06:33 GMT</pubDate>
    </item>
    <item>
      <title>对来自简单多元高斯的数据进行 VAE 训练会导致崩溃的重构分布</title>
      <link>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</guid>
      <pubDate>Tue, 06 Feb 2024 16:00:22 GMT</pubDate>
    </item>
    <item>
      <title>梯度与损失不匹配</title>
      <link>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</link>
      <description><![CDATA[我有一个综合实验，我在数据上拟合了一个模型，分布如下：

类别 0 - ~N(-1, 1)
1 级 - ~N(+1, 1)

该模型基本上将每个正数分类为 1，将负数分类为 0。到目前为止一切顺利。
在测试集中，我通过添加 b=3 更改了这两个类，所以现在：

类别 0 - ~ N(2, 1)
1 级 - ~ N(4, 1)

这是片段：
pbar = tqdm(cs)
对于 pbar 中的 c：
    偏差=变量(torch.tensor([c]).type(dtype),requires_grad=True)
    样本 = np.array(test_df.sample(frac=0.05).x)
    张量_x = torch.张量（样本，requires_grad=False）

    probs = calc_p(tensor_x, 偏差)
    ents = calc_ents(概率)
    
    使用 torch.no_grad()：
        protected_ents, 保护信息 = 保护器.protect(ents)

    损失 = mse_loss(ents, protected_ents.cpu())
    loss.backward()
    bias.grad.data.zero_()

现在我绘制了 X 数据分布的梯度和损失：

由于某种原因，损失看起来是正确的，但梯度却不然，当损失最大化或最小化时，它们应该交叉 0。
如果我改变线路：
loss = mse_loss(ents, protected_ents.cpu())

至：
loss = mse_loss(ents, torch.zeros_like(ents).type(torch.float64).cpu())

然后它按预期工作：

有谁知道是什么原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</guid>
      <pubDate>Tue, 06 Feb 2024 15:50:28 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 60 万条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 对大多数字符串数据集进行分类或聚类的解决方案</title>
      <link>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</link>
      <description><![CDATA[我想使用机器学习对主要字符串数据集进行分类/聚类。我不确定实现此目的的最佳方法是什么。我想使用 Tensorflow 来完成这个任务。也许有人有开发这样的解决方案的经验并且可以推荐一些东西。
至于我的数据集主要包含字符串值的问题，据我所知，我可以使用 pandas 类别将此数据转换为算法可以理解的内容。如果有更好的解决方案请务必推荐。
到目前为止，我一直在学习 Tensorflow 并分析我拥有的数据集（清理数据、理解结构）。]]></description>
      <guid>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</guid>
      <pubDate>Tue, 06 Feb 2024 15:03:49 GMT</pubDate>
    </item>
    <item>
      <title>在嘈杂的视频流中识别对象的更好方法是什么？</title>
      <link>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</link>
      <description><![CDATA[据我所知，即使我们使用视频流，我们也需要将其分割成帧来运行模型来检测对象。我们正在处理的视频可能非常嘈杂，以至于当一个人看着屏幕时，只有存在一系列图像（小物体正在移动或物体是静态的，但每一帧的噪声都有些不同）。因此，我担心单帧可能不足以以足够的准确度对对象进行分类（我刚刚开始讨论这个主题，所以到目前为止还没有 PoC）
如何提高准确度（也许使用图像序列，但任何其他方式也可以接受）？]]></description>
      <guid>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</guid>
      <pubDate>Tue, 06 Feb 2024 14:27:55 GMT</pubDate>
    </item>
    <item>
      <title>模型的预测始终为 0</title>
      <link>https://stackoverflow.com/questions/77947679/models-predictions-always-0</link>
      <description><![CDATA[我有一个形状为 (1280, 100, 20, 4096) 的训练集，我将其提供给基于变压器的模型进行二元分类（标签为 0 或 1）。这导致我很难处理大量的特征（我尝试将其批量提供给模型，但我不确定最好的方法。现在我只是将其减少到（450， 100, 20, 4096），但任何建议都值得赞赏），但我目前的问题是，无论我训练模型多少个时期，准确率始终为 67.5%（即 0 的百分比） -测试集中的标记特征），测试集上的精度和召回率将始终为 0%。我尝试在将数据输入模型之前对其进行标准化：
 缩放器 = StandardScaler()
    train_data = scaler.fit_transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)
    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)

但这并没有带来任何改进。我使用的模型基于仅编码器变压器：
层（类型）输出形状参数#
=================================================== ===============
input_1 (输入层) [(无, 100, 20, 4096)] 0
_________________________________________________________________
框架位置嵌入（Po（无、100、20、4096）8192000
_________________________________________________________________
Transformer_layer（编码器）（无、100、20、4096）134299652
_________________________________________________________________
global_max_pooling（GlobalMa（无，4096）0
_________________________________________________________________
辍学（Dropout）（无，4096）0
_________________________________________________________________
输出（密集）（无，1）4097
=================================================== ===============
总参数：142,495,749
可训练参数：142,495,749
不可训练参数：0
_________________________________________________________________

在训练期间，我可以看到损失、准确度、精确度和召回率达到了不错的水平，但是当我在测试集上评估模型时，所有这些值都如我之前所述：
纪元 100/100
29/29 [================================] - 90s 3s/step - 损失：0.0839 - 准确度：0.9610 - 召回率：0.9316 - 精度：0.9589
2024-02-06 12:38:38.815759: W tensorflow/core/framework/cpu_allocator_impl.cc:80] 9175040000 的分配超过可用系统内存的 10%。
9/9 [================================] - 21s 2s/步 - 损失：9.4117 - 准确度：0.6750 - 召回率：0.0000e+00 - 精度：0.0000e+00
测试准确率：67.5%
测试召回率：0.0%
测试精度：0.0%

模型优化器是adam，损失是二元交叉熵。激活是S形的。
我正在努力寻找模型的适当调整，甚至理解其当前的行为。此外，我不清楚将批量数据集输入缩放器和拟合函数是否会改变模型的实际训练。]]></description>
      <guid>https://stackoverflow.com/questions/77947679/models-predictions-always-0</guid>
      <pubDate>Tue, 06 Feb 2024 12:26:30 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>关联矩阵热图中的问题[重复]</title>
      <link>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</link>
      <description><![CDATA[
我在我的热图中遇到了这个问题，它只显示热图第一行中的值..
我编写了以下代码：
&lt;前&gt;&lt;代码&gt;
    将seaborn导入为sns
    
    将 matplotlib.pyplot 导入为 plt
    将 pandas 导入为 pd
    
    # 选择数字列
    numeric_columns = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
    
    # 计算数字列的相关矩阵
    相关矩阵 = df[numeric_columns].corr()
    
    # 添加列名作为相关矩阵的第二行
    相关矩阵.列 = 相关矩阵.列.值
    相关矩阵.索引 = 相关矩阵.列.值
    
    # 设置 matplotlib 图形
    plt.figure(figsize=(12, 10))
    
    # 绘制正确显示值的热图
    sns.heatmap（correlation_matrix，annot=True，cmap=&#39;coolwarm&#39;，fmt=“.2f”，linewidths=.5）
    
    # 调整布局以获得更好的可视化效果
    plt.title(“相关矩阵”)
    plt.show()


我需要有人能帮我解决这个问题..]]></description>
      <guid>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</guid>
      <pubDate>Tue, 06 Feb 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>比较不同模型的 F1 分数的概率阈值图 [关闭]</title>
      <link>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</link>
      <description><![CDATA[下面是两个并排的图，针对不平衡的数据集。

我们有一个非常大的不平衡数据集，我们正在以不同的方式处理/转换。每次转换后，我们都会对其运行 xgboost 估计器。
左侧是三个 xgboost 模型在三个不同转换数据集上的 PR 曲线。从左图可以看出，3条PR曲线全部重叠；事实上，其中两个（红色和绿色）曲线下的面积是相同的。
右侧是来自相同三个模型但在不同概率阈值下的 F1 分数（根据测试数据计算）的图。左右图中模型的颜色匹配。红色和绿色模型在不同概率阈值下的峰值 F1 分数大致相同。蓝色模型的峰值 F1 分数略低于其他两个模型的峰值 F1 分数。我的问题是：
A。我可以说，绿色模型比红色模型“远”好，因为它的 F1 分数在很大的概率阈值范围内相当稳定，而红色模型的 F1 分数随着概率阈值的微小变化而迅速下降。概率阈值。
b.红色和蓝色这两种型号中，哪一种更好，为什么？
如果您能给出合理的答复，我将不胜感激，因为它可能对我的工作有所帮助。顺便说一句，我已经进行了大量关于 F1 分数、AUC 和 PR 曲线的讨论，包括这个。
简单地说，这个问题涉及如何解释不同模型的 F1 分数阈值图，因为 PR 曲线没有得出结论。]]></description>
      <guid>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</guid>
      <pubDate>Sun, 04 Feb 2024 11:59:48 GMT</pubDate>
    </item>
    <item>
      <title>BUFFER_SIZE 在 Tensorflow 数据集改组中起什么作用？</title>
      <link>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</link>
      <description><![CDATA[所以我一直在玩这个代码：https://www.tensorflow。 org/tutorials/generative/dcgan 并几乎对其功能有了一个很好的了解。但是，我不太清楚 BUFFER_SIZE 变量的用途是什么。我怀疑它可能用于创建大小为 BUFFER_SIZE 的数据库子集，然后从该子集中获取批次，但我没有看到这一点，也找不到人解释它。
所以，如果有人能解释一下 BUFFER_SIZE 的作用，我将不胜感激❤]]></description>
      <guid>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</guid>
      <pubDate>Thu, 15 Oct 2020 13:14:54 GMT</pubDate>
    </item>
    </channel>
</rss>