<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Apr 2024 06:19:04 GMT</lastBuildDate>
    <item>
      <title>Python 机器学习代码故障排除：Fit 函数问题</title>
      <link>https://stackoverflow.com/questions/78364124/troubleshooting-python-machine-learning-code-issues-with-fit-function</link>
      <description><![CDATA[我正在尝试构建一个机器学习模型，但效果不是很好。有人可以帮我吗？这是我的代码。我创建这段代码是为了我的练习和理解。如果可能的话，有人可以帮助我使用此代码吗？
&lt;前&gt;&lt;代码&gt;在此输入

我用来训练代码的数据是这个google
将 numpy 导入为 np
将 pandas 导入为 pd
数据准备
data = pd.read_csv(&#39;调查肺癌.csv&#39;)
测试数据 = data.head(300)
取消注释以使用特定列
x1 = 测试数据[&#39;性别&#39;]
x2 = 测试数据[&#39;AGE&#39;]
使用除“LUNG_CANCER”之外的所有列
x = 测试数据.列
Xc = np.array([testdata[i] for i in x if i != &#39;LUNG_CANCER&#39;])
X = np.column_stack((np.ones(300), Xc.T))
将“GENDER”编码为二进制
对于范围内的 i(len(X.T[1]))：
如果 X.T[1][i] == &#39;M&#39;，则 X.T[1][i] = 1，否则 0

将“LUNG_CANCER”编码为二进制
y = testdata[&#39;LUNG_CANCER&#39;].apply(lambda x: 1 if x == &#39;yes&#39; else 0)
逻辑回归实现
逻辑回归类：
def __init__(self, X, y, lr=0.01, n=1000):

    自我.X = X

    自我.y = y

    self.weight = np.zeros(X.shape[1])

    self.lr = lr

    self.n_iter = n



@静态方法

定义 sigmoid(z):

    返回 1 / (1 + np.exp(-z))



def 适合（自我）：

    对于 _ 在范围内（self.n_iter）：

        y_pred = np.dot(self.X, self.weight)

        pred = self.sigmoid(y_pred)

        dw = (1 / self.X.shape[0]) * np.dot(self.X.T, pred - self.y)

        self.weight -= self.lr * dw

    返回自重



def 预测（自我，信息）：

    z = np.dot(信息, self.weight)

    prob = self.sigmoid(z)

    如果 prob &gt; 则返回 1 0.5 否则 0

测试模型
测试 = LogisticRegression(X, y)
打印(测试.fit())
info = np.array([1, 1, 64, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2])
预测 = test.predict(info)
打印(预测
n)]]></description>
      <guid>https://stackoverflow.com/questions/78364124/troubleshooting-python-machine-learning-code-issues-with-fit-function</guid>
      <pubDate>Mon, 22 Apr 2024 05:31:10 GMT</pubDate>
    </item>
    <item>
      <title>我对我创建的 PINNs 模型有疑问</title>
      <link>https://stackoverflow.com/questions/78363881/i-have-doubts-about-the-pinns-model-that-i-created</link>
      <description><![CDATA[我尝试为一维表面波高程创建 PINN，输入为 (x,t)。经过长时间的尝试和错误，我意识到我的模型仍然欠拟合。我对之前创建的代码产生了怀疑，但是我找不到错误在哪里，因为训练过程一直运行得很顺利。如果有人能告诉我我的代码哪里出了问题，我会发现它非常有帮助。这是我的代码
导入tensorflow为tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 转换为 TensorFlow 张量
x_train = tf.convert_to_tensor(x, dtype=tf.float32)
t_train = tf.convert_to_tensor(t, dtype=tf.float32)
eta_train = tf.convert_to_tensor(eta_X, dtype=tf.float32)

# 组合 x 和 t 作为训练输入
输入 = tf.concat([x_train, t_train], axis=1)

# 定义物理信息神经网络 (PINN) 模型
PINN 类（tf.keras.Model）：
    def __init__(自身):
        超级（PINN，自我）.__init__()
        self.dense1 = tf.keras.layers.Dense(1000，激活=&#39;tanh&#39;，input_dim=2)
        self.dense2 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense3 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense4 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense5 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense6 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.output_layer = tf.keras.layers.Dense(1, 激活=无)
        
    def 调用（自身，输入）：
        x = 输入[:, 0:1]
        t = 输入[:, 1:2]
        concat_input = tf.concat([x,t], 轴=1)
        hidden_​​1 = self.dense1(concat_input)
        隐藏_2 = self.dense2(隐藏_1)
        隐藏_3 = self.dense3(隐藏_2)
        隐藏_4 = self.dense4(隐藏_3)
        隐藏_5 = self.dense5(隐藏_4)
        隐藏_6 = self.dense6(隐藏_5)
        输出 = self.output_layer(hidden_​​6)
        返回输出
    
def物理损失（模型，x，t）：
    使用 tf.GradientTape(persistent=True) 作为磁带：
        磁带.watch(x)
        磁带.watch(t)
        u_pred = 模型(tf.concat([x,t], axis=1))
        u_x = Tape.gradient(u_pred, x)
        u_t = Tape.gradient(u_pred, t)
        删除磁带
    
    克=9.81
    h = 1
    
    损失 = u_t + np.sqrt(g*h) * u_x
    
    返回 tf.reduce_mean(tf.square(loss))

# 创建并编译PINN模型
模型 = PINN()
优化器 = tf.keras.optimizers.Adam(learning_rate=0.0001)

# 训练循环
纪元 = 200000
对于范围内的纪元（纪元）：
    使用 tf.GradientTape() 作为磁带：
        物理损失值=物理损失（模型，x_train，t_train）
        data_loss_value = tf.reduce_mean(tf.square(模型(输入) - eta_train))
        总损失 = 物理损失值 + 数据损失值
        
    梯度 = Tape.gradient(total_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(梯度, model.trainable_variables))
    
    如果纪元% 1000 == 0:
        print(f&quot;纪元 {epoch}/{epochs}，总损失：{total_loss.numpy()}，物理损失：{physicals_loss_value.numpy()}，数据损失：{data_loss_value.numpy()}&quot;)

我的代码中的一个错误导致我的模型欠拟合]]></description>
      <guid>https://stackoverflow.com/questions/78363881/i-have-doubts-about-the-pinns-model-that-i-created</guid>
      <pubDate>Mon, 22 Apr 2024 03:47:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 ONNX 模型进行量化。推理步骤</title>
      <link>https://stackoverflow.com/questions/78363618/quantization-using-onnx-model-inference-step</link>
      <description><![CDATA[我在 ONNX 量化方面遇到一些问题。
我将 ResNet18 转换为 ONNX 量化模型，并尝试实现一些专用硬件来复制操作。在量化步骤中，ONNX 参考文献中解释了量化的公式为：
y = 饱和 ((x / y_scale) + y_zero_point)。
对于去量化，公式为：
y = (x - x_zero_point) * x_scale
我使用对称量化（zero_point=0），所以它变成：
量化：
y = 饱和 (x / y_scale) 。
去量化：
y = x * x_scale
您可以在此处找到相关信息
https://onnxruntime.ai/docs/performance/model-optimizations/quantization。 html
问题是，如果我查看 ONNX 训练后在校准步骤中计算的缩放因子，缩放因子均小于 1。例如，它们的值介于 0 和 1 之间。如附图所示（使用 Netron 生成）。
用于量化的 ONNX 比例因子示例 
如果我的缩放因子小于一，那么量化和反量化操作就不再有意义。因为假设我想以 int8 进行量化，并且从 fp32 数字开始，我想要这样的东西：
xq = x_fp /s
这样我就可以减少用 int8 表示的 x_fp 的动态范围。但如果我的缩放因子小于 1，我实际上正在扩大范围！
因此，对于量化，我得到的范围更大而不是更小，而对于去量化，我得到的范围更小，而不是更大。
我在想我可能必须将缩放因子乘以我想要的量化位宽度，例如 uint8 的 2^8=256。
由于缩放因子的计算方式为 s= (2^N-1) / fp_range，因此他们可能只提供以下部分：
s_onnx = 1/fp_range
并让您精确地乘以所需的位。
有人可以帮我解决这个问题吗？
谢谢！
考虑到缩放因子在 0 和 1 之间，那么我期望这样的公式：
量化
x_q = x_fp * 比例
去量化
x_fp = x_q / 比例
而不是相反。]]></description>
      <guid>https://stackoverflow.com/questions/78363618/quantization-using-onnx-model-inference-step</guid>
      <pubDate>Mon, 22 Apr 2024 01:31:14 GMT</pubDate>
    </item>
    <item>
      <title>在 keras 调谐器中使用 F1 分数作为指标时遇到问题</title>
      <link>https://stackoverflow.com/questions/78363511/having-trouble-using-the-f1-score-as-a-metric-in-keras-tuner</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78363511/having-trouble-using-the-f1-score-as-a-metric-in-keras-tuner</guid>
      <pubDate>Mon, 22 Apr 2024 00:18:59 GMT</pubDate>
    </item>
    <item>
      <title>用于近似四象限图上 x,y 坐标的机器学习模型</title>
      <link>https://stackoverflow.com/questions/78363501/machine-learning-model-for-approximating-x-y-coordinates-on-four-quadrant-graph</link>
      <description><![CDATA[该项目适用于大学课程。我从 政治指南针 收集了数据，我想用它来近似 x 坐标和 y 坐标值使用 Twitter 数据的政治指南针。这只是一个片段：

&lt;标题&gt;

键
twitter_user_id
politician_name
twitter_handle
x_坐标
y_坐标
政党
选举年
国家
twitter_active_during_election


&lt;正文&gt;

8132862008
813286
巴拉克·奥巴马
巴拉克奥巴马
3
2
民主
2008
美国
正确


9390912008
939091
乔·拜登
乔拜登
3
3
民主
2008
美国
正确


150226332008
15022633
丹尼斯·库西尼奇
丹尼斯_库西尼奇
-2
-2
民主
2008
美国
正确


314286852008
31428685
比尔·理查森
理查森政府
4
4
民主
2008
美国
错误


154165052008
15416505
迈克·哈克比
GovMikeHuckabee
6
6
共和党
2008
美国
正确



我的教授告诉我使用两种逻辑回归模型 - 一种用于近似 x 值，另一种用于近似 y。我想确保这是一个可行的方法。根据我在网上阅读的内容，逻辑回归似乎是一个二元模型。我找不到提供除 yes/no 或 1/0 之外的输出的示例。
逻辑回归可以近似这些值吗？如果不是，什么模型是该项目的正确方法？]]></description>
      <guid>https://stackoverflow.com/questions/78363501/machine-learning-model-for-approximating-x-y-coordinates-on-four-quadrant-graph</guid>
      <pubDate>Mon, 22 Apr 2024 00:13:56 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络的多类别分类问题</title>
      <link>https://stackoverflow.com/questions/78363467/classification-problem-with-multi-categories-using-neural-networks</link>
      <description><![CDATA[在我的设置中，我遇到一个问题，其中每个实体（假设用户）都有一组分类属性。为了简单起见，我们可以将它们表示为数字，例如：

u1, [1,2,3]
u2 [0, 4]
u3 [0,1,2,3,4]

假设我们有多个类别MAX_CATEGORIES。因此，在我的玩具设置中，生成了标签：
def likes_movies(row):
    如果 (MAX_CATEGORY - 1) 在行中则返回 1，否则返回 0

我尝试了一个模型，其中每个类别都表示为嵌入，然后聚合它们：
类 SimpleMultiCategoricalClassifier(torch.nn.Module):
    def __init__(self, num_embeddings, embedding_dim):
        超级().__init__()

        self.embeddings = nn.EmbeddingBag(
                num_embeddings=num_embeddings,
                embedding_dim=embedding_dim, mode=“平均值”）

        self.net = nn.Sequential(
            nn.Linear(embedding_dim, 2),
            ReLU() 函数
        ）
        
    def 前向（自身，输入，偏移量）：
        x = self.embeddings(输入, 偏移量)
        返回 self.net(x)

但是我连最简单的功能都学不会。我的分类器损失只是在一个固定点上振荡，基本上没有学到任何东西。我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78363467/classification-problem-with-multi-categories-using-neural-networks</guid>
      <pubDate>Sun, 21 Apr 2024 23:45:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在多项式回归中找到未处理的输入数据的多项式方程？</title>
      <link>https://stackoverflow.com/questions/78363183/how-to-find-the-polynomial-equation-for-unprocessed-input-data-in-polynomial-reg</link>
      <description><![CDATA[这里我的 .csv 文件包含大小和时间。大小（唯一的自变量）是指程序的大小，时间（目标/预测）是指完成程序所需的时间。请告诉我找到从程序中获取方程的方法，以便我可以直接根据原始未处理的输入大小预测时间，而无需进一步处理。
导入 pandas 作为 pd
将 numpy 导入为 np
df = pd.read_csv(“/content/drive/MyDrive/Project/combined2.csv”)
X = df[[&#39;尺寸&#39;]]
打印（X）
y = df[&#39;时间&#39;]
从 sklearn.model_selection 导入 train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)
从 sklearn.preprocessing 导入 StandardScaler
定标器=标准定标器()
打印（X_train）
X_train_scaler = 缩放器.fit_transform(X_train)
X_test_scaler = 缩放器.transform(X_test)
从 sklearn. Linear_model 导入 LinearRegression
lin = 线性回归()
def 答案(arr,x):
  总和=0
  j=0
  对于我在arr：
    打印（j）
    总和=总和+(pow(x,j))*i
    j=j+1
  返回总和

我=1
从 sklearn.preprocessing 导入多项式特征
从 sklearn.metrics 导入mean_squared_error
当 i&lt;=50 时：
  poly = 多项式特征(度=i)
  X_poly_train = poly.fit_transform(X_train_scaler)
  X_test_poly = poly.transform(X_test_scaler)
  poly.fit(X_poly_train, y_train)
  lin.fit(X_poly_train, y_train)
  y_pred = lin.predict(X_test_poly)
  均方误差（y_test，y_pred）
  y_pred_train = lin.predict(X_poly_train)
  均方误差（y_train，y_pred_train）
  如果我==1：
    mnum=abs(均方误差(y_test, y_pred)-均方误差(y_train, y_pred_train))
  别的：
    如果 mnum&gt;abs(mean_squared_error(y_test, y_pred)-mean_squared_error(y_train, y_pred_train))：
      达达=我
      mnum=abs(均方误差(y_test, y_pred)-均方误差(y_train, y_pred_train))
  我=我+1
print(&#39;mse(testpred)-mse(trainpredtrain)=&#39;+str(mnum))
print(&#39;度=&#39;+str(达达))
打印（均方误差）
poly = 多项式特征(度=daada)
X_poly_train = poly.fit_transform(X_train_scaler)
X_test_poly = poly.transform(X_test_scaler)
poly.fit(X_poly_train, y_train)
lin.fit(X_poly_train, y_train)
y_pred = lin.predict(X_test_poly)
倾角=mean_squared_error(y_test, y_pred)
print(&#39;均方误差(test&amp;pred)=&#39;+str(dip))
y_pred_train = lin.predict(X_poly_train)
罗伊=mean_squared_error(y_train, y_pred_train)
print(&#39;均方误差(train,predtrain)=&#39;+str(roy))
print(&#39;mse(testpred)-mse(trainpredtrain)=&#39;+str(abs(dip-roy)))
print(&#39;截距=&#39;+str(lin.intercept_))
print(&#39;系数=&#39;)
打印（lin.coef_）
打印（缩放器.scale_）
&quot;&quot;&quot;&quot;coefficients_unscaled = lin.coef_ / scaler.scale_ # 除以缩放因子
Intercept_unscaled = lin.intercept_ - np.sum(coefficients_unscaled * scaler.mean_) # 调整均值缩放
print(&quot;coefficients_unscaled=&quot;)
打印（系数_未缩放）
打印（拦截_未缩放）
新大小 = 5128192
print(int(answer(coefficients_unscaled,new_size)-intercept_unscaled))“”“”
Coefficients_unscaled = lin.coef_ / scaler.scale_ # 除以缩放因子
Intercept_unscaled = lin.intercept_ - np.sum(coefficients_unscaled * scaler.mean_) # 调整均值缩放

打印（“coefficients_unscaled =”，coefficients_unscaled）
打印（“intercept_unscaled =”，intercept_unscaled）

# 计算多项式的函数
def 答案(arr, x):
    返回 sum(arr[i] * (x ** i) for i in range(len(arr)))

# x 的值
新大小 = 5128192

# 计算new_size处多项式的值
结果=答案（coefficients_unscaled，new_size）-拦截_unscaled
print(&quot;大小 5128192 的预测时间：&quot;, result)

即将到来的结果太大了。请帮我修复它。
我的问题是我无法获得原始输入的方程，它可以直接给出预测的输出。我尝试对其进行缩放，但得到的答案比预期值太大。]]></description>
      <guid>https://stackoverflow.com/questions/78363183/how-to-find-the-polynomial-equation-for-unprocessed-input-data-in-polynomial-reg</guid>
      <pubDate>Sun, 21 Apr 2024 21:20:09 GMT</pubDate>
    </item>
    <item>
      <title>如何正确标记停车位编号并据此监控停车位盒？</title>
      <link>https://stackoverflow.com/questions/78363064/how-can-i-properly-label-the-parking-slots-number-and-monitor-the-slot-boxes-acc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78363064/how-can-i-properly-label-the-parking-slots-number-and-monitor-the-slot-boxes-acc</guid>
      <pubDate>Sun, 21 Apr 2024 20:21:06 GMT</pubDate>
    </item>
    <item>
      <title>我收到错误 - ValueError: 在使用模型 ss3 进行投票分类器时，估计器 SS3 应该是分类器</title>
      <link>https://stackoverflow.com/questions/78362782/i-am-getting-an-error-valueerror-the-estimator-ss3-should-be-a-classifier-whi</link>
      <description><![CDATA[我正在研究一个使用投票的集成模型，用于 ss3 模型、svm 模型和 RoBERTa 模型，但我遇到了很多错误。
我尝试包含拟合函数、预测函数，并尝试在 ss3 类的 init 中添加 _estimator_type = &#39;classifier&#39; 行，但随后出现错误，表明这是不必要的。请帮助我消除这个错误并指导我如何制作集成模型。
这是我面临最多问题的代码部分：
SS3 级：
def __init__(self):
    _estimator_type = &#39;分类器&#39;
    # self.train_df = 无
    self.cf = 无
    self. precision = 无，
    自我回忆=无，
    self.f1_score = 无，
    self.accuracy = 无，
    self.local_values = 无

def get_params(self, deep=True):
    返回 {
        &#39;_estimator_type&#39;: self._estimator_type,
    }

这是错误：
C:\Users\hp\PycharmProjects\EnsembleModelVoting\.venv\Scripts\python.exe C:\Users\hp\PycharmProjects\EnsembleModelVoting\Base.py
&lt;类&#39;pandas.core.frame.DataFrame&#39;&gt;
索引：20 个条目，4119 至 1902
数据列（共2列）：
 # 列非空计数 Dtype
--- ------ -------------- -----
 0 文本 20 非空对象
 1 标签 20 非空 int32
数据类型：int32(1)、对象(1)
内存使用：400.0+字节
                                                   文字标签
第4119章.....鸡皮疙瘩的梦想0
第3477章
第4322章 我感到兴奋。天哪，有点颤抖 0
第3349章(∣´à¸´´´)... 0
第469章 就让这焦急安息吧。请这个1
第538章 第一次紧张，凌龙……1
6636 rabi o rapapa 但等我毕业... 0
5820 为什么使用限制事件？ 0
第1047章 占有权，还怕不行老大？ 0
第1219章 博伊101 0
6542 [BOT] 范妮在这里！那儿是谁&#39;3&#39;）/ 0
6580 黎明后真的随机梦 0
5110 欢呼比埃尔姐姐！！ 0
第1783章 我不明白这个世界..旧T恤... 0
960 改掉7年的习惯……0
分段阅读_第 4810 章
4539 已经感觉最接近了。尽管有... 0
6929 快到开斋节了，一定要打扫卫生... 0
第1858章 卖礼篮的人肯定很幸福……0
1902 他的父亲再次恐吓..BUTTEREADY ON BBMAS!!!..... 0
SS3 呼叫
RobertaForSequenceClassification 的一些权重未从 roberta-base 的模型检查点初始化，而是新初始化的：[&#39;classifier.dense.bias&#39;、&#39;classifier.dense.weight&#39;、&#39;classifier.out_proj.bias&#39;、&#39;classifier.out_proj。重量&#39;]
您可能应该在下游任务上训练该模型，以便能够将其用于预测和推理。
罗伯塔打电话
支持向量机调用
回溯（最近一次调用最后一次）：
  文件“C:\Users\hp\PycharmProjects\EnsembleModelVoting\Base.py”，第 166 行，在  中
    vote_classifier.fit(X_train, y_train) # 直接传递DataFrame对象X_train和y_train
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\hp\PycharmProjects\EnsembleModelVoting\.venv\Lib\site-packages\sklearn\base.py”，第 1474 行，包装器中
    返回 fit_method(估计器, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\hp\PycharmProjects\EnsembleModelVoting\.venv\Lib\site-packages\sklearn\ensemble\_voting.py”，第 366 行，适合
    返回 super().fit(X,transformed_y,sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\hp\PycharmProjects\EnsembleModelVoting\.venv\Lib\site-packages\sklearn\ensemble\_voting.py”，第 81 行，适合
    名称，clfs = self._validate_estimators()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\hp\PycharmProjects\EnsembleModelVoting\.venv\Lib\site-packages\sklearn\ensemble\_base.py”，第 236 行，位于 _validate_estimators 中
    引发值错误（
ValueError：估计器 SS3 应该是一个分类器。

进程已完成，退出代码为 1
]]></description>
      <guid>https://stackoverflow.com/questions/78362782/i-am-getting-an-error-valueerror-the-estimator-ss3-should-be-a-classifier-whi</guid>
      <pubDate>Sun, 21 Apr 2024 18:48:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：形状（无，1）和（无，13）不兼容</title>
      <link>https://stackoverflow.com/questions/78362771/valueerror-shapes-none-1-and-none-13-are-incompatible</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78362771/valueerror-shapes-none-1-and-none-13-are-incompatible</guid>
      <pubDate>Sun, 21 Apr 2024 18:45:10 GMT</pubDate>
    </item>
    <item>
      <title>Baseline3 TD3，reset() 方法值太多，无法解包错误</title>
      <link>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</link>
      <description><![CDATA[环境是python 3.10，stable-baseline3 2.3.0，我正在尝试TD3算法。
无论我做什么，我都会遇到同样的错误。
据我所知，重置方法的返回值与定义的观察空间相同
我制作的环境有如下重置方法
def重置（自身，种子=0）：
    self.current_index = 0
    self.current_cash = self.start_cash
    self.done = False
    self.当前时间 = self.开始时间

    # 초기 관찰 상태 계산
    初始状态 = self.get_state() # 字典
    返回初始状态

它从来都不复杂，定义环境，模型也很好
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
从 stable_baselines3 导入 TD3

类 CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(自我, 观察空间, features_dim=5):
        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)
        self.model_alpha = ModelAlpha()
    
    defforward（自我，观察）：
        价格 = 观察结果[&#39;价格&#39;]
        位置 = 观测值[&#39;位置&#39;]
        数量 = 观察结果[&#39;数量&#39;]
        pnr = 观测值[&#39;pnr&#39;]
        
        return self.model_alpha(价格, torch.cat([位置, 数量, pnr]))
        
        
# 환경과 모델 설정
env = MarketEnvironment(蜡烛, &#39;2020-07-01 00:00:00&#39;, &#39;2023-12-31 23:59:00&#39;) # 여러분의 환경 설정
策略_kwargs = 字典（
    features_extractor_class=自定义特征提取器，
    features_extractor_kwargs=dict（features_dim=5）
）

模型 = TD3(“MultiInputPolicy”，env，policy_kwargs=policy_kwargs，batch_size=128，verbose=1)

Jupyter 提示符表示
使用CPU设备
使用 Monitor 包装器包装环境
将环境包装在 DummyVecEnv 中。
它运行良好，直到
model.learn（total_timesteps=1，log_interval=10，progress_bar=True）

这段代码。
无论我做了什么，它都会一遍又一遍地说
文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\off_policy_algorithm.py:297，在 OffPolicyAlgorithm._setup_learn(self、total_timesteps、callback、reset_num_timesteps、tb_log_name ， 进度条）
    第290章
    第291章
    292 和 self.env.num_envs &gt; 1
    293 而不是 isinstance(self.action_noise, VectorizedActionNoise)
    第294章）：
    第295章
--&gt;第297章
    298 总时间步数，
    299回调，
    300 重置_num_timesteps，
    第301章
    第302章
    第303章）

文件~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\base_class.py:425，在BaseAlgorithm._setup_learn(self,total_timesteps,callback,reset_num_timesteps,tb_log_name,progress_bar)中
    第423章
    第424章 断言self.env不是None
--&gt;第425章
    第426章
    第427章

文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:77，在 DummyVecEnv.reset(self) 中
     范围内的 env_idx 为 75(self.num_envs)：
     76 Maybe_options = {“选项”: self._options[env_idx]} if self._options[env_idx] else {}
---&gt; 77 obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
     78 self._save_obs（env_idx，obs）
     79 # 种子和选项仅使用一次

ValueError：需要解压的值太多（预期为 2）

我知道这个错误的reset()方法是在一个名为VecEnv的抽象类中
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 21 Apr 2024 12:58:27 GMT</pubDate>
    </item>
    <item>
      <title>快速衡量一个值与许多其他值相比的新颖性[关闭]</title>
      <link>https://stackoverflow.com/questions/78359547/fast-measuring-the-novelty-of-a-value-compared-to-many-others</link>
      <description><![CDATA[我正在训练 ki 模型在迷宫中移动。每个模型都会选择随后获得奖励的行动。最后，这会产生一个数值列表，我想将其与其他数值进行比较。
目的是衡量列表的相似程度。但不仅仅是与其中一个比较，而是与所有这些比较。
但是，必须确保距离在总计算中不会相互抵消。如果一个动作列表位于另外两个动作列表之间，那么这两个动作列表可能会相互抵消并且距离为零。这绝不能发生。另外，计算的数值应该与下一个列表有 2 的位置处的列表中是否有 0 或 1 无关。因此，Levensthein 距离不起作用。
或者Python中有一个库可以完成此类任务吗？
第二件事是速度；有人可以给我看一段代码片段，可以在 python 中有效地进行这种新颖的测量吗？
顺便说一句，对于我来说，是否为一系列操作列表计算排名列表并将其与已执行的操作进行比较，或者是否为每个操作列表计算单独的值，对我来说并不重要。]]></description>
      <guid>https://stackoverflow.com/questions/78359547/fast-measuring-the-novelty-of-a-value-compared-to-many-others</guid>
      <pubDate>Sat, 20 Apr 2024 19:53:17 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 mRMRe 包找到最佳特征数？</title>
      <link>https://stackoverflow.com/questions/71996789/how-to-find-the-optimal-feature-count-by-mrmre-package</link>
      <description><![CDATA[我正在尝试使用 R 中的 mRMRe 包对基因表达数据集进行特征选择。我有包含超过 10K 个基因的 RNA seq 数据，我想找到适合分类模型的最佳特征。我想知道如何找到最佳特征数。这是我的代码，
mrEnsemble &lt;- mRMR.ensemble(data = Xdata, target_indices = c(1) ,feature_count = 100 ,solution_count = 1)
mrEnsemble_genes &lt;- as.data.frame(apply(solutions(mrEnsemble)[[1]], 2, function(x, y) { return(y[x]) }, y=featureNames(Xdata)))
查看（mrEnsemble_genes）

我刚刚设置了feature_count = 100，但我想知道如何在不设置数量的情况下找到分类的最佳特征数量。
提取 mrEnsemble_genes 后的结果将是基因列表，例如，
&lt;前&gt;&lt;代码&gt;gene05
基因08
基因45
基因67

他们的排名是根据相互信息计算出的分数吗？我的意思是排名第一的基因获得最高的 MI，它可能是对样本类别（即癌症和正常）进行分类的良好基因，对吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/71996789/how-to-find-the-optimal-feature-count-by-mrmre-package</guid>
      <pubDate>Mon, 25 Apr 2022 08:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 Scikit-learn 中获取多类分类的特异性和阴性预测值？</title>
      <link>https://stackoverflow.com/questions/63526955/how-to-obtain-specificity-and-negative-predictive-value-from-scikit-learn-for-mu</link>
      <description><![CDATA[目前，scikit-learn 的默认分类报告 (sklearn.metrics.classification_report - 链接）不包括特异性和阴性预测值 (NPV)。
因此，我制作了自己的分类报告功能：
def custom_classification_report(y_true, y_pred):
    tp, fn, fp, tn = fusion_matrix(y_true, y_pred).ravel()
    acc = (tp+tn)/(tp+tn+fp+fn)
    森 = (tp)/(tp+fn)
    sp = (tn)/(tn+fp)
    ppv = (tp)/(tp+fp)
    净现值 = (tn)/(tn+fn)
    f1 = 2*(sen*ppv)/(sen+ppv)
    fpr = (fp)/(fp+tn)
    tpr = (tp)/(tp+fn)
    return ( &#39;2X2 混淆矩阵:&#39;, [&#39;TP&#39;, tp, &#39;FP&#39;, fp, &#39;FN&#39;, fn, &#39;TN&#39;, tn],
                &#39;准确度：&#39;, round(acc, 3),
                &#39;灵敏度/召回率：&#39;, round(sen, 3),
                &#39;特异性：&#39;，round(sp, 3),
                &#39;PPV/精度：&#39;, round(ppv, 3),
                &#39;净现值：&#39;，圆形（净现值，3），
                &#39;F1-分数：&#39;, round(f1, 3),
                &#39;误报率：&#39;, round(fpr, 3),
                &#39;真阳性率：&#39;, round(tpr, 3),
            ）

def auc_roc(y_true, y_pred_score):
    return (&#39;AUC-ROC:&#39;, round(roc_auc_score(y_true, y_pred_score), 3))

def avg_ precision(y_true, y_pred_score, target_name):
    return (&#39;平均精度:&#39;, round(average_ precision_score(y_true, y_pred_score, pos_label=target_name), 3))
    tpr = (tp)/(tp+fn)
    return ( &#39;2X2 混淆矩阵:&#39;, [&#39;TP&#39;, tp, &#39;FP&#39;, fp, &#39;FN&#39;, fn, &#39;TN&#39;, tn],
                &#39;准确度：&#39;, round(acc, 3),
                &#39;灵敏度/召回率：&#39;, round(sen, 3),
                &#39;特异性：&#39;，round(sp, 3),
                &#39;PPV/精度：&#39;, round(ppv, 3),
                &#39;净现值：&#39;，圆形（净现值，3），
                &#39;F1-分数：&#39;, round(f1, 3),
                &#39;误报率：&#39;, round(fpr, 3),
                &#39;真阳性率：&#39;, round(tpr, 3),
            ）

def auc_roc(self, y_true, y_pred_score):
    return (&#39;AUC-ROC:&#39;, round(roc_auc_score(y_true, y_pred_score), 3))

def avg_ precision(self, y_true, y_pred_score, target_name):
    return (&#39;平均精度:&#39;, round(average_ precision_score(y_true, y_pred_score, pos_label=target_name), 3))

当我使用它进行二元类分类时，它工作得很好 -
print(&#39;&gt;&gt; 自定义分类报告:\n&#39;, custom_classification_report(y_test, Predicted_labels), &#39;\n&#39;)

当我使用同一行代码 print(&#39;&gt;&gt; 自定义分类报告:\n&#39;, custom_classification_report(y_test, Predicted_labels), &#39;\n&#39;) 进行多类分类时，它给出错误 ValueError: 需要解包的值太多（预期为 4）。这是为什么，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/63526955/how-to-obtain-specificity-and-negative-predictive-value-from-scikit-learn-for-mu</guid>
      <pubDate>Fri, 21 Aug 2020 16:48:48 GMT</pubDate>
    </item>
    <item>
      <title>R 错误中的 K 均值聚类</title>
      <link>https://stackoverflow.com/questions/46002289/k-means-clustering-in-r-error</link>
      <description><![CDATA[我有一个在 R 中创建的数据集。它的结构如下：

&lt;前&gt;&lt;代码&gt;&gt;头（btc_data）
           日期 btc_close eth_close vix_close gold_close DEXCHUS 更改
1647 2010-07-18 0.09 无 无 无 无 0
1648 2010-07-19 0.08 不适用 25.97 115.730 不适用 -1
1649 2010-07-20 0.07 不适用 23.93 116.650 不适用 -1
1650 2010-07-21 0.08 不适用 25.64 115.850 不适用 1
1651 2010-07-22 0.05 不适用 24.63 116.863 不适用 -1
1652 2010-07-23 0.06 不适用 23.47 116.090 不适用 1

我正在尝试使用 k 均值对观察结果进行聚类。但是，我收到以下错误消息：

&lt;前&gt;&lt;代码&gt;&gt; km &lt;- kmeans(trainingDS, 3)
do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)
另外：警告消息：
在 storage.mode(x) &lt;- &quot;double&quot; 中：通过强制引入的 NA

这是什么意思？我对数据的预处理是否错误？我能做什么来修复它？我不能删除 NA，因为在 4500 个初始观察中，如果我运行完整案例，我只剩下 100 个观察。
基本上，我希望基于值为 -1,0,1 的 change 列形成 3 个簇。然后，我希望分析每个集群的组成部分，以找到最强的变化预测因素。还有哪些其他算法对此最有用？ 
我还尝试使用以下代码删除所有 NA 值，但仍然收到相同的错误消息：

&lt;前&gt;&lt;代码&gt;&gt; Complete_cases &lt;- btc_data[complete.cases(btc_data), ]
&gt; km &lt;- kmeans(complete_cases, 3, nstart = 20)
do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)
另外：警告消息：
在 storage.mode(x) &lt;- &quot;double&quot; 中：通过强制引入的 NA

&gt; sum(!sapply(btc_data, is.finite))
[1]8008
&gt;总和（sapply（btc_data，is.nan））
[1] 0
&gt;
&gt; sum(!sapply(complete_cases, is.finite))
[1] 0
&gt;总和（sapply（complete_cases，is.nan））
[1] 0

这是数据的格式：

&lt;前&gt;&lt;代码&gt;&gt; sapply（btc_data，类）
      日期 btc_close eth_close vix_close gold_close DEXCHUS 更改
    “日期”“数字”“数字”“数字”“数字”“数字”“系数”
]]></description>
      <guid>https://stackoverflow.com/questions/46002289/k-means-clustering-in-r-error</guid>
      <pubDate>Fri, 01 Sep 2017 14:19:42 GMT</pubDate>
    </item>
    </channel>
</rss>