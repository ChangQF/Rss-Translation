<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 16 Jul 2024 09:16:56 GMT</lastBuildDate>
    <item>
      <title>使用 PyTorch 计算信息熵</title>
      <link>https://stackoverflow.com/questions/78753537/compute-information-entropy-with-pytorch</link>
      <description><![CDATA[问题：
如何利用当前模型计算信息熵。
当我尝试实现这篇论文提出的方法时。但我不知道如何让它发挥作用。
假设有一个矩阵 M，它是 m x n。
这个矩阵 M 缺少一些条目。 
该模型的任务是完成矩阵M，使用的方法类似于矩阵分解，但采用自动编码器的架构。
以下是该论文的陈述，未经修改。

在该算法中，我们使用二项分布概率来衡量每个未知延迟的不确定性。具体而言，对于每个未知延迟，我们利用当前模型计算两个潜在延迟的概率。这两个概率构成二项分布。


因此，我们将延迟值的概率设为P(x)=p，P(x_bar)=1-p，其中p等于1表示我们知道x处的延迟，p等于0表示我们不知道x_bar处的延迟。


同时，我们使用信息熵来衡量分布的不确定性。 
对于二项分布，信息熵可以计算为：H(p)=-plog(p)-(1-p)log(1-p)

这是否类似于使用 sigmoid 或 softmax 将模型的输出转换为概率
然后使用概率获取信息熵？
任何帮助都值得感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78753537/compute-information-entropy-with-pytorch</guid>
      <pubDate>Tue, 16 Jul 2024 08:42:59 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False),
Dense(64,activation=&#39;relu&#39;),
Dense(1)
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差

# 显示模型摘要
model.summary()

# 训练模型
model.fit(X_train, Y_train, epochs=40, batch_size=32, validation_data=(X_test, Y_test))

# 用新数据进行预测的示例
prediction = model.predict(X_test)
print(prediction)

结果：
模型：“顺序”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 的数量而改善。 这就是问题所在，我不明白发生了什么。
我是深度学习的初学者，所以也许我犯了一个非常简单的错误。 但是我仔细检查了我的数据，X_train 已标准化，我尝试在我的模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。 我真的想尝试找到一种方法来使用深度学习来找到它。 我 d]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    <item>
      <title>即使经过数百个时期，pytorch AdamW 的 LR 仍未衰减</title>
      <link>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</link>
      <description><![CDATA[我有以下使用 Pytorch 中的 AdamW 优化器的代码：
optimizer = AdamW(params=self.model.parameters(), lr=0.00005)

我尝试使用 wandb 进行登录，如下所示：
lrs = {f&#39;lr_group_{i}&#39;: param_group[&#39;lr&#39;]
for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})

请注意 weight_decay 参数的默认值为 0.01（对于 AdamW）。
当我检查 wandb 仪表板时，它显示 AdamW 的 LR 即使在 200 个 epoch 之后也相同，并且根本没有衰减。我尝试了几次。

为什么 LR 衰减没有发生？
此外，它仅显示一个参数组的 LR。为什么会这样？似乎我在这里错过了一些基本的东西。有人可以指出吗？]]></description>
      <guid>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</guid>
      <pubDate>Tue, 16 Jul 2024 06:09:44 GMT</pubDate>
    </item>
    <item>
      <title>视频对象分割准确率</title>
      <link>https://stackoverflow.com/questions/78752737/video-object-segmentation-accuracy</link>
      <description><![CDATA[我有一个关于测量视频分割模型准确度的问题。我发现这些模型通常使用 2 个成功衡量标准：Jaccard 指数和轮廓准确度。但是，我从未真正看到过这些方法的实现代码。有人知道我可以在哪里找到这些代码吗？谢谢。
我在网上查过，但找不到这些代码。]]></description>
      <guid>https://stackoverflow.com/questions/78752737/video-object-segmentation-accuracy</guid>
      <pubDate>Tue, 16 Jul 2024 04:51:49 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch DataLoader 中用于 LSTM 模型的 WeightedRandomSampler 的 IndexError</title>
      <link>https://stackoverflow.com/questions/78752550/indexerror-with-weightedrandomsampler-in-pytorch-dataloader-for-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78752550/indexerror-with-weightedrandomsampler-in-pytorch-dataloader-for-lstm-model</guid>
      <pubDate>Tue, 16 Jul 2024 03:05:49 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>在带有 Bullseye 的 Raspberry Pi 3B 上安装 Ultralytics 以运行 yolov5 时出错，“错误：无法为使用 PEP 517 的 opencv-python 构建轮子...”</title>
      <link>https://stackoverflow.com/questions/78752286/error-installing-ultralytics-to-run-yolov5-on-raspberry-pi-3b-with-bullseye-er</link>
      <description><![CDATA[我尝试使用 pip 在装有 Bullseye OS 的 Raspberry Pi 3B 上安装 Ultralytics。虽然我能够安装所有依赖项以成功运行 yolov5，但在尝试安装 Ultralytics 时，我收到错误，
&quot;CMake 安装出现问题，中止构建。CMake 可执行文件是 /tmp/pip-build-env-3didr32b/overlay/lib/python3.9/site-packages/cmake/data/bin/cmake

错误：无法为 opencv-python 构建 wheel
无法构建 opencv-python
错误：无法为使用 PEP 517 且无法直接安装的 opencv-python 构建 wheel&quot;

虽然有多种方法可以成功安装 opencv，并且对我来说很有效，但立即安装 Ultralytics 会导致再次安装 opencv，从而导致同样的失败。有人知道如何修复这个问题吗？
我尝试过单独安装 open-cv，然后安装 ultralytics，但出现了同样的错误。有人建议在安装 ultralytics 之前先安装 opencv-python-headless，但我无法成功安装此包。]]></description>
      <guid>https://stackoverflow.com/questions/78752286/error-installing-ultralytics-to-run-yolov5-on-raspberry-pi-3b-with-bullseye-er</guid>
      <pubDate>Tue, 16 Jul 2024 00:06:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入参数连接到 CNN_M_LSTM 模型？</title>
      <link>https://stackoverflow.com/questions/78752243/how-to-concatenate-inputs-parameters-to-the-cnn-m-lstm-model</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow 和 Keras API）中。
带有时间戳的能耗数据集的大小为 (730, 2)
Covid 数据集的大小为 (730, 1)
我想对 covid 数据集和带有时间戳的能耗进行切片和窗口化（批处理大小 = 128，窗口大小为 96）。最后将两个数据集压缩在一起。
这是我的代码片段和窗口，包括能耗和时间戳数据集，即 covid 数据集：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000


def windowed_dataset(series, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
数据集 = tf.data.Dataset.from_tensor_slices(series) 
数据集 = 数据集.window(window_size + 1, shift=1) 
数据集 = 数据集.flat_map(lambda window: window.batch(window_size + 1)) # 顺序数据集保持不变
数据集 = 数据集.shuffle(1000)
数据集 = 数据集.map(lambda window: (window[:-1], window[-1][0])) 
数据集 = 数据集.padded_batch(128,drop_remainder=True).cache()

返回数据集


下面是我如何压缩数据集：

train_energy_dataset = windowed_dataset(TRAIN.PRE_PROCESSED_SERIES)
train_covid_dataset = windowed_dataset(TRAIN.SERIES_COVID)

train_dataset = tf.data.Dataset.zip((train_covid_dataset,train_energy_dataset))

train_dataset = train_dataset.shuffle(buffer_size=1000)
train_dataset = train_dataset.map(lambda data1, data2: ((data1[0], data2[0]), data1[1])) # 根据需要调整映射
train_dataset = train_dataset.batch(128, drop_remainder=True).cache()

我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(128,1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(128,2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2)

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate(axis=1)([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2], output=output)
# 定义最终模型
return model_final


model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

尝试窗口化和压缩数据集时，我收到错误信息
ValueError: Exception调用 Functional.call() 时遇到。

输入 Tensor(&quot; functional_100_1/Cast_1:0&quot;, shape=(128, 128, None, 2), dtype=float32) 的输入形状无效。预期形状 (None, 128, 2)，但输入具有不兼容的形状 (128, 128, None, 2)

Functional.call() 接收的参数：
• 输入=(&#39;tf.Tensor(shape=(128, 128, None, 1), dtype=float64)&#39;, &#39;tf.Tensor(shape=(128, 128, None, 2), dtype=float64)&#39;)
• 训练=True
• 掩码=(&#39;None&#39;, &#39;None&#39;)

我还通过将模型的输入形状更改为来修复了此问题
 输入1 = tf.keras.layers.Input(shape=(128,2), name=&quot;input1&quot;)
输入2 = tf.keras.layers.Input(shape=(128), name=&quot;input2&quot;)

错误按预期出现 2 个维度，但收到 3 个维度。

我的期望是将 zip 数据集输入到我的 CNN_M_LSTM 模型中。]]></description>
      <guid>https://stackoverflow.com/questions/78752243/how-to-concatenate-inputs-parameters-to-the-cnn-m-lstm-model</guid>
      <pubDate>Mon, 15 Jul 2024 23:38:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TensorFlow 在 python 中用大型 txt 文件训练神经网络模型？</title>
      <link>https://stackoverflow.com/questions/78752044/how-to-train-neural-network-model-with-large-txt-files-in-python-using-tensorflo</link>
      <description><![CDATA[我构建了一个具有 65 个输入神经元和 4880 个输出神经元的模型。我的训练数据存储在两个大型文本文件中：“X_train.txt”包含每行代表 65 个数字的列表，而“Y_train.txt”包含每行代表一个索引号的行。我需要对“Y_train”执行独热编码，以在指定索引处创建一个包含 4880 个零和一个“1”的列表。
由于这些文件的大小，我想分批训练我的模型。如何使用 TensorFlow 在 Python 中有效地使用这些 txt 文件训练我的模型？
因此我尝试获取这些文件并尝试将这些文件转换为变量
with open(xPath, &#39;r&#39;) as file:
line = file.readline()
while line:
x_train.append(eval(line.strip())) 
line = file.readline()
x_train = np.array(x_train)

但由于文件太大，需要花费太多时间，这不是我可以等待的主要问题，但主要问题是它使用了太多内存……]]></description>
      <guid>https://stackoverflow.com/questions/78752044/how-to-train-neural-network-model-with-large-txt-files-in-python-using-tensorflo</guid>
      <pubDate>Mon, 15 Jul 2024 22:05:16 GMT</pubDate>
    </item>
    <item>
      <title>如何解决引导式利润计算中的零损失风险：机器学习</title>
      <link>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</link>
      <description><![CDATA[我刚刚为我的训练营完成了一个项目。然而，无论我如何进行利润计算或引导，我仍然面临零损失的风险。我不知道该如何解决这个问题。
\### 利润计算的关键值

BUDGET = 100 \* 10\*\*6 # 1 亿美元

REVENUE_PER_BARREL = 4.5 # 每桶 4.5 美元

WELLS_SELECTED = 200 # 选定用于开发的油井数量

COST_PER_WELL = BUDGET / WELLS_SELECTED # 每口井的成本

sufficient_volume = COST_PER_WELL / REVENUE_PER_BARREL

### 利润计算函数
def calculate_profit(predictions, target, n_wells, revenue_per_barrel, cost_per_well):
selected_indices = predictions.sort_values(ascending=False).head(n_wells).index
selected_reserves = target.loc[selected_indices].sum()
收入 = selected_reserves * 每桶收入 * 1000
利润 = 收入 - (每井成本 * n_wells)
返回利润

### 引导技术
def bootstrap_profit(predictions, target, n_wells, 每桶收入, 每井成本, n_samples=1000):
state = np.random.RandomState(42)
利润 = []
for _ in range(n_samples):
sample_indices = state.choice(predictions.index, size=n_wells, replace=True)
sample_predictions = predictions.loc[sample_indices]
sample_target = target.loc[sample_indices]
利润 = calculate_profit(sample_predictions, sample_target, n_wells,每桶收益，每井成本)
利润。附加(利润)
利润 = pd.Series(利润)
平均利润 = 利润。平均值()
下限 = 利润。分位数(0.025)
上限 = 利润。分位数(0.975)
损失风险 = (利润 &lt; 0).平均值() * 100
返回平均利润，(下限，上限)，损失风险
]]></description>
      <guid>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</guid>
      <pubDate>Mon, 15 Jul 2024 20:51:56 GMT</pubDate>
    </item>
    <item>
      <title>在 JAX 中对多个输入求导</title>
      <link>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</link>
      <description><![CDATA[我试图在 JAX 中求函数的一阶和二阶导数，但是我这样做的方式给出了错误的数字或零。我有一个数组，每个变量有两列，每个输入有两行
import jax.numpy as jnp
import jax

rng = rng = jax.random.PRNGKey(1234)
array = jax.random.normal(rng, (2,2))

两个测试函数
def F1(arr):
return 1/arr

def F2(arr):
return jnp.array([arr[0]**2 + arr[1]**3])

以及两种取一阶和二阶导数的方法，其中一种方法使用 jax.grad()
def dF_m1(arr, F):
return jax.grad(lambda arr: F(arr)[0])(arr)

def ddF_m1(arr, F, dF):
return jax.grad(lambda arr: dF(arr, F)[0])(arr)

另一个使用 jax.jacobian()
def dF_m2(arr, F):
jac = jax.jacobian(lambda arr: F(arr))(arr)
return jnp.diag(jac)

def ddF_m2(arr, F, dF):
hess = jax.jacobian(lambda arr: dF(arr, F))(arr)
return jnp.diag(hess)

使用这两种方法计算每个函数的一阶和二阶导数（和误差）可得出以下结果
exact_dF1 = (-1/array**2)
exact_ddF1 = (2/array**3)

print(&quot;函数 1 使用所有 grad()&quot;)
dF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)
ddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)
print(dF1_m1 - exact_dF1,&quot;\n&quot;)
print(ddF1_m1 - exact_ddF1,&quot;\n&quot;)

print(&quot;函数 1 使用所有 jacobian()&quot;)
dF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)
ddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)
print(dF1_m2 - exact_dF1,&quot;\n&quot;)
print(ddF1_m2 - exact_ddF1,&quot;\n&quot;)

输出
函数 1 使用所有 grad()
[[ 0. 48.43877 ]
[ 0. 0.62903005]] 

[[ 0. 674.248 ]
[ 0. 0.9977852]] 

函数 1 使用所有 jacobian()
[[0. 0.]
[0. 0.]] 

[[0. 0.]
[0. 0.]] 

和
exact_dF2 = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))
exact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))

print(&quot;函数 2 使用所有 grad()&quot;)
dF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)
ddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)
print(dF2_m1 - exact_dF2,&quot;\n&quot;)
print(ddF2_m1 - exact_ddF2,&quot;\n&quot;)

print(&quot;函数 2 使用所有 jacobian()&quot;)
dF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)
ddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)
print(dF2_m2 - exact_dF2,&quot;\n&quot;)
print(ddF2_m2 - exact_ddF2,&quot;\n&quot;)

输出
函数 2 使用所有 grad()
[[0. 0.]
[0. 0.]] 

[[0. 0.86209416]
[0. 7.5651155 ]] 

使用所有 jacobian() 的函数 2
[[ 0. -0.10149619]
[ 0. -6.925739 ]] 

[[0. 2.8620942]
[0. 9.565115 ]] 

我更愿意只对 F1 之类的东西使用 jax.grad()，但现在似乎只有 jax.jacobian 有效。这完全是因为我需要计算神经网络相对于其输入的高阶导数。感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</guid>
      <pubDate>Mon, 15 Jul 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 的 Beam RunInference 和句子转换器</title>
      <link>https://stackoverflow.com/questions/78750157/beam-runinference-and-sentence-transformers-from-huggingface</link>
      <description><![CDATA[我正在尝试将 RunInference beam Transform 与 stsb-xlm-r-multilingual 模型一起使用。
像这样：
 inferences = (
formatted_examples
| &quot;Run Inference&quot; &gt;&gt; RunInference(model_handler)
| &#39;ProcessOutput&#39; &gt;&gt; beam.ParDo(PostProcessor())
)

而 model_handler 是：
model_handler = HuggingFacePipelineModelHandler(
task=PipelineTask.FeatureExtraction,
model = &quot;sentence-transformers/stsb-xlm-r-multilingual&quot;,
load_pipeline_args={&#39;framework&#39;: &#39;pt&#39;},
inference_args={&#39;max_length&#39;: 200}
)

但是，转换返回的 predictionResult 对象包含一个嵌套数组，其中包含多个嵌入。
我可以像这样访问嵌入：
result.inference[0][0][0]。
这为我提供了一个嵌入。
我不确定所有其他嵌入是什么。我只对其中一个感兴趣，我不需要所有数据。
虽然我总是可以通过使用上面显示的内部索引来获取嵌入，但我想知道我是否做错了什么，或者是否有办法将一些参数传递给处理程序，以指示它只返回单个嵌入？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78750157/beam-runinference-and-sentence-transformers-from-huggingface</guid>
      <pubDate>Mon, 15 Jul 2024 13:36:33 GMT</pubDate>
    </item>
    <item>
      <title>无法将 (Dimension(None)、Dimension(80)) 的元素转换为张量</title>
      <link>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</link>
      <description><![CDATA[我正在尝试阅读 LibRecommender 中有关模型训练过程的教程：https://librecommender.readthedocs.io/en/latest/tutorial.html
我停在了训练模型阶段，代码如下：
model = WideDeep(
task=&quot;ranking&quot;,
data_info=data_info,
embed_size=16,
n_epochs=2,
loss_type=&quot;cross_entropy&quot;,
lr={&quot;wide&quot;: 0.05, &quot;deep&quot;: 7e-4},
batch_size=2048,
use_bn=True,
hidden_​​units=(128, 64, 32),
)

model.fit(
train_data,
neg_sampling=True, # 对训练和评估数据执行负抽样
verbose=2,
shuffle=True,
eval_data=eval_data,
metrics=[&quot;loss&quot;, &quot;roc_auc&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;ndcg&quot;],
)

我收到错误：
TypeError：调用 Flatten.call() 时遇到异常。

无法将 (Dimension(None)、Dimension(80)) 的元素转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF 数据类型。

Flatten.call() 接收的参数：
• 输入=tf.Tensor(shape=(?, 5, 16), dtype=float32)

我不知道为什么会收到此错误？我假设本教程中没有错误，我按照所示按 1:1 执行。
我在 PyCharm 环境中工作并使用 Jupyter 笔记本。]]></description>
      <guid>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</guid>
      <pubDate>Sun, 14 Jul 2024 14:37:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>多元时间序列数据的准备</title>
      <link>https://stackoverflow.com/questions/78467998/preparation-of-multivariate-time-series-data</link>
      <description><![CDATA[我正在做一个关于指数/股票价格预测的大学项目。我计划使用组合的 cnn-lstm 模型，我有几种不同类型的数据：开盘价最高价最低价收盘价成交量、价值、基本面数据（如失业率和各种利率）、技术指标（如 RSI、MACD 等）以及移动平均线（如 SMA、EMA、WMA 等）。为网络准备数据的最佳方法是什么？
目前，我正在使用以下转换
对于 OHLC - 简单微分
对于基本数据 - 对数化
对于移动平均线 - 从此移动平均线的值中减去蜡烛开盘值
指标值不变
然后我对所有数据集使用 StandardizeNormalizer。我还尝试分别对每个序列进行规范化（稳健缩放、标准化、最小最大缩放），并对所有数据进行微分，但效果不佳]]></description>
      <guid>https://stackoverflow.com/questions/78467998/preparation-of-multivariate-time-series-data</guid>
      <pubDate>Sun, 12 May 2024 13:20:10 GMT</pubDate>
    </item>
    </channel>
</rss>