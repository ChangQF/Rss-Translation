<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 23 Feb 2025 15:16:26 GMT</lastBuildDate>
    <item>
      <title>有效的网络验证损失Stagnan不会降低，验证精度会提高，但Stagnan [封闭]</title>
      <link>https://stackoverflow.com/questions/79459417/efficientnetb0-validation-loss-stagnan-not-decreases-validation-accuracy-increa</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79459417/efficientnetb0-validation-loss-stagnan-not-decreases-validation-accuracy-increa</guid>
      <pubDate>Sat, 22 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>如何设计AI系统来预测Gherkin用户故事的自动测试？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79458762/how-to-design-an-ai-system-to-predict-automated-tests-from-gherkin-user-stories</link>
      <description><![CDATA[我正在构建一个AI驱动系统，以从用户故事中生成自动测试用例。我对AI没有太多知识，所以我需要一些指导
 这是完整的描述： 
来自AI 驱动的用户故事的自动测试的智能预测
 任务：
•分析用户故事并预测测试用例
•过程并归一化测试数据
•探索和使用不同的机器/深度学习模型
•将开发的AI模型集成到HMI中，以预测所需的自动测试
  Technologies 烧瓶，Django，CI/CD（MLOPS），Gherkin，Python，ML/DL，Crisp-DM方法，NLP 
我期待一些指导，并就该项目提供建议，也许是拟议的阶段]]></description>
      <guid>https://stackoverflow.com/questions/79458762/how-to-design-an-ai-system-to-predict-automated-tests-from-gherkin-user-stories</guid>
      <pubDate>Fri, 21 Feb 2025 22:33:13 GMT</pubDate>
    </item>
    <item>
      <title>使用射线调节器进行超参数调整的序列化误差</title>
      <link>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</guid>
      <pubDate>Fri, 21 Feb 2025 15:18:38 GMT</pubDate>
    </item>
    <item>
      <title>加载CNN模型后的Numpy急切执行问题</title>
      <link>https://stackoverflow.com/questions/79457437/numpy-eager-execution-problem-after-loading-a-cnn-model</link>
      <description><![CDATA[我想保存和加载CNN模型以进一步培训。我已经开发了一个模型，并将其保存为 .h5 文件。首次运行时创建，培训和节省时没有问题。
加载现有 .h5 模型并尝试训练它时，问题存在。以下代码描述了实现和问题。
  import os.path
导入TensorFlow作为TF

＃启用急切的执行
tf.compat.v1.enable_eager_execution（）

...＃删除以供可读性

def train_model（model_name：str，model_handler：tensormodel，Visualiser：Visualiser，logger：logger，x_train，y_train，y_train，x_test，y_test，y_test，batch_size） - ＆gt;元组：
    logger.info（f＆quort启用启用：{tf.executing_eagerly（）};）

    ＃检查模型是否已经受过培训
    如果use_existing_models and os.path..exists（f＆quot; models/{model_name} .h5＆quort;）：
        模型=模型= tf.keras.models.models.load_model（f＆quot; model/{model_name} .h5＆quort;）
        （x_train，y_train），（x_test，y_test）= model_handler.load_data（）
    别的：
        如果model_name ==＆quot; base_model＆quot;：
            model = model_handler.create_cnn（）
        别的：
            model = model_handler.create_cnn（batch_normalisation = true）

    历史= model.fit（x_train，y_train，epochs = num_epochs，batch_size = batch_size，validation_data =（x_test，y_test））
    test_loss，test_acc = model.evaluate（x_test，y_test）
    model.save（f＆quot; model/{model_name} .h5;）
    
    logger.info（f＆quot“模型精度：{test_acc * 100：.2f}％＆quort”）
    Visualiser.plot_training_history（历史记录，model_name）
    返回（model，model_name），（test_acc）
 
以下是产生的错误。
  file＆quot＆quot＆quot＆quod&gt;&#39;
    历史= model.fit（x_train，y_train，epochs = num_epochs，batch_size = batch_size，validation_data =（x_test，y_test））
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^因为^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^因为^^^
  file＆quot＆quot＆quot＆quot＆quotement/machine_learning/machine-learning-real-time-objectification/.venv/lib/python3.12/site-packages/keras/keras/keras/src/src/src/src/utils/traceback_utils.py，1002222 ，在error_handler中
    从无
  file＆quot＆quot＆quot＆quot＆quotevelovermation/machine_learning/machine-learning-real-time-objectification/.venv/lib/python3.12/site-packages/keras/keras/keras/src/backend/backend/backend/tensorflow/core.core.pycore.py&quot;第155行，convert_to_numpy
    返回NP.Array（X）
           ^^^^^^^^^^^^
notimplementedError：numpy（）仅在启用急切执行时可用。
 
日志记录显示急切执行已启用以下内容：
  2025-02-21 13：46：42,505-信息 - 启用：true：true
 
我缺少什么？]]></description>
      <guid>https://stackoverflow.com/questions/79457437/numpy-eager-execution-problem-after-loading-a-cnn-model</guid>
      <pubDate>Fri, 21 Feb 2025 13:00:54 GMT</pubDate>
    </item>
    <item>
      <title>拟合功能在时期1之后停止</title>
      <link>https://stackoverflow.com/questions/79457237/fit-function-stops-after-epoch-1</link>
      <description><![CDATA[我已经实现了此功能以适合模型
  def fit_model（型号，x_train_sequence_tensor，y_train_sequence_tensor，epochs，val_set，time_windows，sualer）：
    
    x_column_list = [val_set.columns.to_list（）中的项目中的项目，如果不在[&#39;date&#39;，&#39;deres&#39;&#39;&#39;，&#39;rank&#39;，&#39;rank_group&#39;，&#39;counts&#39;，&#39;counts&#39;，&#39;target&#39;]]中
    x_val_set = val_set [x_column_list] .Round（2）
                    
    X_VAL_SET [X_VAL_SET.COLUMNS] = SCALER.TRANSFORM（X_VAL_SET [X_VAL_SET.COLUMNS]）
    x_val_sequence = get_feature_array（x_val_set，x_column_list，time_windows）
    X_VAL_SECONCE_TENSOR = TF.CONVERT_TO_TENSOR（X_VAL_SECERESE，dtype = tf.float32）
    
    y_column_list = [&#39;target&#39;]                
    y_val_set = val_set [y_column_list] .Round（2）
    y_val_sequence = get_feature_array（y_val_set，y_column_list，time_windows）
    y_val_sequence_tensor = tf.convert_to_tensor（y_val_sequence，dtype = tf.float32）

                    
    历史= model.fit（x_train_sequence_tensor，y_train_sequence_tensor，epochs， 
                        验证_data =（x_val_secorence_tensor，y_val_sepence_tensor））
    返回模型，历史记录

 
但是当我称其为时
  fitted_model，history = fit_model（模型，x_train_secorence_tensor，y_train_secorce_tensor， 
                    epochs = 100，val_set = val_set，time_windows = 90，scaleer = scaleer）
 
它在第一个时期后停止。它不能按要求所有100个运行。
我试图在函数调用之外打电话给它。
 `＃步骤3.2：安装模型 +我们通过一些验证
                                                ＃监视验证损失和指标
                                                ＃在每个时代的末尾
                    x_val_set = val_set [x_column_list] .Round（2）
                    
                    ＃x_val_set.values = scaler.transform（x_val_set.values）
                    
                    X_VAL_SET [X_VAL_SET.COLUMNS] = SCALER.TRANSFORM（X_VAL_SET [X_VAL_SET.COLUMNS]）
                    x_val_sequence = get_feature_array（x_val_set，x_column_list，90）
                    X_VAL_SECONCE_TENSOR = TF.CONVERT_TO_TENSOR（X_VAL_SECERESE，dtype = tf.float32）
                    
                    y_val_set = val_set [y_column_list] .Round（2）
                    y_val_sequence = get_feature_array（y_val_set，y_column_list，90）
                    y_val_sequence_tensor = tf.convert_to_tensor（y_val_sequence，dtype = tf.float32）

                    
                    triench_history = cnn1d_bilstm_model.fit（x_train_sequence_tensor，y_train_sequence_tensor，epochs = 200， 
                                                            ＃我们通过一些验证
                                                            ＃监视验证损失和指标
                                                            ＃在每个时代的末尾
                                                            验证_data =（x_val_secorence_tensor，y_val_sepence_tensor））
 
我在做什么错？]]></description>
      <guid>https://stackoverflow.com/questions/79457237/fit-function-stops-after-epoch-1</guid>
      <pubDate>Fri, 21 Feb 2025 11:26:50 GMT</pubDate>
    </item>
    <item>
      <title>我面临的问题是阅读Python中的MT数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79456454/i-am-facing-problem-to-read-mt-data-in-python</link>
      <description><![CDATA[错误是
  parserError                               
Trackback（最近的电话最后一次）
[9]中的单元，第1行
----＆gt; 1 data = pd.read_csv（r＆quot; c：\ users \ rosha \ oneDrive \ desktop \ vbox data \ vbox.csv＆quot;

File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace， skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, FoqueChar，引用，双语，EscapeChar，评论，编码，encoding_errors，言语，on_bad_lines，delim_whitespace，low_memory，memory_map，float_map，float_precision，storage_options，storage_options，dtype_backend），dtype_backend）
    899 kwds_defaults = _refine_defaults_read（
    900方言，
    901定界符，
   （...）
    908 dtype_backend = dtype_backend，
    909）
    910 kwds.update（kwds_defaults）
 - ＆gt; 912返回_read（filepath_or_buffer，kwds）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ io \ parsers \ parsers \ readers.py：583，in _read（filepath_or_buffer，kwds）
    580返回解析器
    582与解析器：
 - ＆gt; 583返回parser.Read（nrows）

file〜 \ anaconda3 \ lib \ site-packages \ pandas \ io \ parsers \ parsers \ readers.py：1704，in textfileReader.read.read（self，nrows）
   1697 nrows = validate_integer（&#39;nrows＆quot; nrows）
   1698尝试：
   1699＃错误：“ parserbase”没有“读”属性。
   1700（
   1701索引，
   1702列，
   1703 col_dict，
 - ＆gt; 1704）= self._engine.Read（＃类型：忽略[attr-defined]
   1705 nrows
   1706）
   1707除例外：
   1708 self.close（）

file〜 \ anaconda3 \ lib \ lib \ site-packages \ pandas \ io \ parsers \ c_parser_wrapper.py：234，在cparserwrapper.read（self，nrows）中
    232尝试：
    233如果self.low_memory：
 - ＆gt; 234块= self._reader.read_low_memory（nrows）
    235＃破坏了大块
    236数据= _concatenate_chunks（块）

文件〜\ anaconda3 \ lib \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：814，in pandas._libs.parsers.parsers.textreader.read_low_memory（）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：875，在pandas._libs.parsers.parsers.textreader._read_rows（）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：850，在pandas._libs.parsers.parsers.textreader._tokenize_rows（）

文件〜\ anaconda3 \ lib \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：861，in pandas._libs.parsers.parsers.textreader._check_tokenize_tokenize_status（）

file〜 \ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：2029，in pandas._libs.parsers.raise_parser_parser_error（）

ParserError：错误令牌数据。 C错误：第6行中的预期2个字段，Saw 20
 
如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/79456454/i-am-facing-problem-to-read-mt-data-in-python</guid>
      <pubDate>Fri, 21 Feb 2025 06:01:03 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow模型训练，列表到Numpy阵列转换不均会改变数据形状</title>
      <link>https://stackoverflow.com/questions/79455291/tensorflow-model-training-list-to-numpy-array-conversion-unevenly-changes-the-s</link>
      <description><![CDATA[我正在尝试从MRI图像中预测LSDC。对于每个研究_id，都有多个图像。每个研究_id代表每个患者。我想在5个级别上预测5个条件下的3级严重程度。
我正在尝试使用 sequence 类从TensorFlow创建数据集。这是我的DataGenerator类：
 类CustomDatagenerator（序列）：
    def __init __（self，image_dict，num_img，labels_dict = none，batch_size = 8，image_size =（224，224），shuffle = true）：
       self.image_dict = image_dict
       self.labels_dict = labels_dict
       self.batch_size = batch_size
       self.image_size = image_size
       self.shuffle =洗牌
       self.ids = list（image_dict.keys（））
       self.num_img = num_img
       self.on_epoch_end（）

    def __len __（自我）：
       返回int（np.floor（len（self.ids） / self.batch_size））

    def __getItem __（自我，索引）：
       start = index * self.batch_size
       end = min（（索引 + 1） * self.batch_size，len（self.ids））
       batch_ids = self.ids [start：end]
       batch_images = []
       batch_labels = []

       对于batch_ids中的ID_：
           图像= []

           对于self.image_dict.get（id_，[]）中的image_path：
               dicomdata = pydicom.dcmread（image_path）
               图像= dicomdata.pixel_array
               图像= cv2.resize（图像，self.image_size）
               image = np.expand_dims（图像，axis = -1）
               image = image.astype（&#39;float32&#39;） / np.max（图像）
               图像= np.Repeat（图像，3，轴= -1）
               images.append（图像）

           图像= np.Array（图像）

           如果Len（Images）＆lt; self.num_img：
               pad_amount = self.num_img- len（图像）
               padding = [（0，pad_amount）] + [（0，0）] *（len（images.shape） -  1）
               图像= np.pad（图像，填充，模式=&#39;常数&#39;）

           batch_images.append（图像）

           如果self.labels_dict：
               label = np.array（self.labels_dict.get（id_），dtype = np.float32）
               batch_labels.append（标签）

       batch_images = np.stack（batch_images）
       如果self.labels_dict：
           batch_labels = np.array（batch_labels，dtype = np.float32）
           返回batch_images，batch_labels

       返回batch_images

    def on_epoch_end（self）：
       如果self.shuffle：
           np.random.shuffle（self.ids）
 
我的标签字典如下：
  i，sid在枚举中（train_df [&#39;study_id&#39;]）：
        labels_dict [str（sid）] = []
        对于条件下的骗局：
            如果train_df.loc [i，con] ==&#39;normal_mild&#39;：
                labels_dict [str（sid）]。附加（[1，0，0]）
            elif train_df.loc [i，con] ==&#39;严重&#39;：
                labels_dict [str（sid）]。附加（[0，0，1]）
            别的：
                labels_dict [str（sid）]。附加（[0，1，0]）

       labels_dict [str（sid）] = np.array（labels_dict [str（sid）]，dtype = np.float32）
 
我尝试了多种方法将 labels_dict 转换为numpy数组。但是，要么在训练时显示形状错过错误。或试图查看数据时显示错误。
它显示的错误如下：
  -----＆gt; 1 Model.Fit（train_generator，epochs = 2）＃，step_per_epoch = len（train_generator）// 8）

/USR/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in Error_handler（*args，** kwargs）
    120＃要获取完整的堆栈跟踪，请致电：
    121＃`keras.config.disable_traceback_filtering（）`
 - ＆gt; 122从无
    123最后：
    124 del filtered_tb

＆lt; ipython-Input-12-Cf42609bddda＆gt;在__getItem __（自我，索引）中
     47 batch_images = np.stack（batch_images）
     48如果self.labels_dict：
---＆gt; 49 batch_labels = np.array（batch_labels，dtype = np.float32）
     50返回batch_images，batch_labels
     51 

ValueError：设置具有序列的数组元素。 1个维度后，请求的阵列具有不均匀的形状。检测到的形状为（8，） +不均匀部分。
 
我尝试使用 np.stack 或 batch_labels = batch_labels.reshape（（（batch_labels.shape.shape.shape [0]），len（presition），3），3））），但它显示不同的错误。我的数据没有任何 nan ，所有 labels_dict 均为Shape （batch_size，num_of_condition，severity_class）。即使我尝试从发电机打印数据。生成器数据形状来自 data_x，data_y = next（iter（train_generator））显示模型输入和输出的数据形状。我无法弄清楚这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79455291/tensorflow-model-training-list-to-numpy-array-conversion-unevenly-changes-the-s</guid>
      <pubDate>Thu, 20 Feb 2025 17:02:54 GMT</pubDate>
    </item>
    <item>
      <title>将Python ML模型与Flutter Client进行集成[关闭]</title>
      <link>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</link>
      <description><![CDATA[我在工作中面临挑战，我需要在我的客户端应用程序上运行许多Python ML模型，因为使某些模型在服务器上运行。。
除了项目资产中的张量流光模型和实施Python模型的同事告诉我，他不能以Tflite模型导出某些模型。，我没有实验。
有一个软件包（OnnxRuntime）使用ONNX型号，并在我的flutter代码中使用了其中的功能，例如Dart FFI，我以前使用此软件包在我的颤音代码中运行C ++功能，并且可以很好地运行。我的牛头人说，我有同样的问题，他不能将所有模型提取到ONNX模型中，但这让我认为有一种方法可以在我的Flutter应用程序中使用Python代码，例如Dart FFI，我知道它不会一样，因为Python是一种解释语言，我无法从中脱离共享对象，所以我的问题是：是否有一种方法可以在我的客户端应用程序或Python ML模型中使用Python代码，而无需使用TFLITE或OnnxRuntime？
 ps：这是一个移动应用程序，因此像运行本地Pyhton服务器这样的解决方案无法与我合作！]]></description>
      <guid>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</guid>
      <pubDate>Wed, 19 Feb 2025 16:13:44 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM的时间序列不同</title>
      <link>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</link>
      <description><![CDATA[我正在训练LSTM进行时间序列预测，其中数据来自不规则间隔的传感器。我正在使用最后5分钟的数据来预测下一个值，但是某些序列比其他序列大。
我的输入阵列的形状是（611,1200,15），其中（示例，时间段，功能）。每个样本的第二维度均未完成，因此我用NP.NAN值填充了丢失的数据。例如，示例（1，：，：）有1000个时间段和200 np.nan。
训练时，损失等于Nan。
我在做什么错？我该如何训练？
这是我尝试训练LSTM的尝试：
  def lstmfit（y，x，n_hidden = 1，n_neurons = 30，Learning_rate = 1E-2）：   
    lstm = sequention（）
    lstm.add（basking（mask_value = np.nan，input_shape =（none，x. shape [2]）））））））
        
    对于范围（n_hidden）的图层：
        lstm.add（lstm（n_neurons，， 
                      激活=“ tanh”
                      recurrent_activation =＆quot; sigmoid＆quot;
                      return_sequences = true））
        
    lstm.add（密集（1））
    
    lstm.compile（loss =; mse; optimizer =; adam＆quot;）
    
    早期_STOPPING =早期踩踏（Monitor =&#39;损失&#39;，耐心= 10，详细= 1，restore_best_weights = true）
  
    
    lstm.-fit（x，y.Reshape（-1），epochs = 100，callbacks = [arfore_stopping]）
    
    y_train_fit = lstm.predict（x）
    
    返回lstm，y_train_fit
 
模型的摘要：
  lstm.summary（）
型号：sequential_9＆quot
__________________________________________________________________________
 图层（类型）输出形状参数＃   
=============================================== ===============
 masking_7（掩模）（无，无，15）0         
                                                                 
 LSTM_6（LSTM）（无，无，30）5520      
                                                                 
 密集_10（密集）（无，无，1）31        
                                                                 
=============================================== ===============
总参数：5551（21.68 kb）
可训练的参数：5551（21.68 kb）
不可训练的参数：0（0.00字节）
__________________________________________________________________________
 
和训练的第一个时期：
 时期1/100
18/18 [=======================================
时代2/100
18/18 [========================================
时期3/100
18/18 [====================================
 ]]></description>
      <guid>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</guid>
      <pubDate>Tue, 18 Feb 2025 20:47:34 GMT</pubDate>
    </item>
    <item>
      <title>增加自我注意力后，CNN网络的非确定性行为</title>
      <link>https://stackoverflow.com/questions/79439790/non-deterministic-behavior-of-a-cnn-network-after-adding-self-attention</link>
      <description><![CDATA[当我添加nlbloclos时，在我的网络（简单CNN）中添加了一个自发层时，网络的结果不再可重现，当我再次训练它时，结果是不同的。但是，当我删除网络中的nlblocks时，它是确定性的。
这是代码：
  os.environ [＆quot&#39;cuda_visible_devices;
os.environ [&#39;tf_cpp_min_log_level&#39;] =&#39;3&#39;
种子= 42
os.environ [&#39;tf_deterministic_ops&#39;] =&#39;1&#39;
tf.config.experiment.enable_op_determinism（）

os.environ [&#39;pythonhashseed&#39;] = str（seed）
os.environ [&#39;tf_cudnn_deterministic&#39;] =&#39;1&#39; 

随机种子（种子）
np.random.seed（种子）
tf.random.set_seed（种子）

tf.keras.backend.set_floatx（&#39;float64&#39;）



nlblock类（层）：
    def __init __（self，num_channels，** kwargs）：
        super（nlblock，self）.__ init __（** kwargs）
        self.num_channels = num_channels
        self.theta = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =＆quort; same＆quot;）
        self.phi = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =＆quort; same＆quot;）
        self.g = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =; same＆quort;）
        self.attention_layer =注意（）＃keras注意层

    def呼叫（self，输入）：
        ＃变换功能图
        query = self.theta（输入）＃query（q）
        key = self.phi（输入）＃key（k）
        value = self.g（输入）＃value（v）

        ＃应用注意力层
        activation_output = self.attention_layer（[查询，键，值]）

        ＃残差连接
        返回输入 +注意_Output
        
＃定义NL注意的模型
def build_model（）：
    优化器= ADAM（Learning_rate = 0.002，beta_1 = 0.89，beta_2 = 0.995）

    输入= tf.keras.input（shape =（num_time_steps，1））＃输入层
    
    ＃Conv Block 1
    x = conv1d（filters = 64，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（输入）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 64）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）

    ＃Conv 2 2
    x = conv1d（filters = 32，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（x）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 32）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）
    x =辍学（0.1）（x）

    ＃Conv Block 3
    x = conv1d（filters = 16，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（x）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 16）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）
    x =辍学（0.35）（x）

    ＃完全连接的图层
    x = flatten（）（x）
    x =密集（40，激活=&#39;relu&#39;）（x）
    x =辍学（0.35）（x）
    输出=致密（20）（x）＃回归的输出层
    
    ＃编译模型
    型号= tf.keras.model（输入，输出）
    model.compile（优化器=优化器，lose = root_mean_squared_error，metrics = [&#39;mean_absolute_error&#39;]）
    
    返回模型

model = build_model（）

redy_lr = reducelronplateau（monitor =&#39;val_loss&#39;，因子= 0.6，耐心= 25，min_lr = 1e-6）
早期_Stopping =早期踩踏（Monitor =&#39;Val_loss&#39;，Patience = 30，Restore_best_weights = true）
＃步骤5：训练模型
历史= model.fit（x_train_scaled，y_train，validation_data =（x_val_scaled，y_val），
                epochs = 180，batch_size = 90，callbacks = [redion_lr，ropand_stopping]，冗长= 0）

test_loss，test_mae = model.evaluate（x_test_scaled，y_test，batch_size = len（x_test_scaled），词= 0）
 
我还使用TF.Matmul（）和SoftMax使用了服装注意块，但没有任何变化。]]></description>
      <guid>https://stackoverflow.com/questions/79439790/non-deterministic-behavior-of-a-cnn-network-after-adding-self-attention</guid>
      <pubDate>Fri, 14 Feb 2025 15:08:45 GMT</pubDate>
    </item>
    <item>
      <title>比较通过转移学习训练的几种不同Yolov8S模型的重量和偏见</title>
      <link>https://stackoverflow.com/questions/76220101/comparing-the-weights-and-biases-of-several-different-yolov8s-models-trained-thr</link>
      <description><![CDATA[我有3种不同的Yolov8s模型，我想评估：

  yolov8s接受了普通模型训练。Train（）命令

  yolo8vs模型，训练有冷冻骨干

  yolov8s模型，训练有所有层冰冻


我正在使用回调功能冻结重量，请参见下文：
  def freeze_layer（训练器）：
    型号= Trainer.Model
    num_freeze = 10
    打印（f＆quot {num_freeze}层；）
    冻结= [f&#39;model。{x}。&#39;&#39;对于x范围（num_freeze）]＃冻结层
    对于k，v in Model.Named_pa​​rameters（）：
        v.requires_grad = true＃火车所有层
        如果有（x中的x in for x in freeze）：
            打印（f&#39;Freezing {k}&#39;）
            v.requires_grad = false
    打印（f＆quot {num_freeze}层被冷冻。＆quot;）

如果__name__ ==＆quot __ Main __＆quot;：
    模型= Yolo（Yolov8s.pt）
    model.add_callback（＆quot; on_train_start＆quort＆quot; freeze_layer）
    模型。
        data =&#39;coco128.yaml＆quort;
        时代= 300，
        IMGSZ = 640
        ）
 
我希望能够评估这三个神经网络之间的权重和偏见的差异。我想看看哪些神经元在转移学习后死亡，并评估所有三个神经网络之间的主要差异。是否有任何标准解决方案可用于获得对神经网络的这种见解？
我已经比较了Yolo提供的标准培训指标，例如地图和损失，一切都是预期的。冻结的层越多，表现较差。当我冻结整个网络时，性能比其他两个实例要差得多。
我几乎不知道在比较同一网络架构时从哪里开始，但训练有不同的训练。
任何帮助都非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/76220101/comparing-the-weights-and-biases-of-several-different-yolov8s-models-trained-thr</guid>
      <pubDate>Wed, 10 May 2023 15:13:36 GMT</pubDate>
    </item>
    <item>
      <title>如何标记卫星图像以进行图像分割？</title>
      <link>https://stackoverflow.com/questions/64553426/how-to-label-satellite-images-for-image-segmentation</link>
      <description><![CDATA[我想在卫星图像中检测地雷。最初，我构建了一个模型，每个图像具有多个标签并训练它以对图像进行分类。
但是，我想使用以下提到的图像分割技术： https://towardsdataScience.com/dstl-satellite-imagery-contest-on-kaggle-2f3ef7b8ac40  
我通过AWS S3存储桶下载了所需的图像。我想标记我从频段文件生成的多光谱图像的每个像素。
但是，我在如何标记方面面临困难。
是否有任何开源或其他工具可以执行相同的操作？
图像是12个频段多光谱卫星图像。]]></description>
      <guid>https://stackoverflow.com/questions/64553426/how-to-label-satellite-images-for-image-segmentation</guid>
      <pubDate>Tue, 27 Oct 2020 11:25:11 GMT</pubDate>
    </item>
    <item>
      <title>SSD的预测图像ID和框</title>
      <link>https://stackoverflow.com/questions/60398211/predicted-image-id-and-box-from-ssd</link>
      <description><![CDATA[如何从SSD中找到预测的图像ID和盒子，我正在使用此 github链接这是我要保存图像ID和的测试功能盒子
  def Test（加载器，NET，CRITERION，设备）：
    net.eval（）
    Running_loss = 0.0
    Running_Regression_loss = 0.0
    Running_Classification_loss = 0.0
    num = 0
    对于_，枚举（加载程序）中的数据：
        图像，盒子，标签=数据
        图像=图像TO（设备）
        box = box.to（设备）
        标签= labels.to（设备）
        num += 1
    
        使用Torch.no_grad（）：
            信心，位置= NET（图像）
            recression_loss，分类_loss = Criterion（置信度，位置，标签，盒子）
            损失= recression_loss + classification_loss
    
        running_loss += loss.item（）
        Running_Regression_loss += regression_loss.item（）
        Running_classification_loss += classification_loss.item（）
    返回Runnun_loss / num，Runnun_Regression_loss / num，Run
 ]]></description>
      <guid>https://stackoverflow.com/questions/60398211/predicted-image-id-and-box-from-ssd</guid>
      <pubDate>Tue, 25 Feb 2020 15:36:01 GMT</pubDate>
    </item>
    <item>
      <title>训练芯片和目标图像格式在TensorFlow中</title>
      <link>https://stackoverflow.com/questions/58306578/training-chip-and-target-image-format-in-tensorflow</link>
      <description><![CDATA[我正在尝试为前哨图像构建土地覆盖分类模型。我正在使用的图像通道（频段）是32位浮点。
我需要了解如何最好地格式化图像数据，包括用于培训的芯片/补丁和用于分类的目标图像。我有几个问题：

我需要将原始图像和训练芯片从32位转换为其他深度吗？
我是否需要确保训练芯片/补丁和目标都具有相同的深度（32位，16位或其他）？
我需要转售我的数据吗？我看到了一些论文，其中数据在0-1或0-255之间重新缩放？
数据深度是否影响学习和预测的表现？
]]></description>
      <guid>https://stackoverflow.com/questions/58306578/training-chip-and-target-image-format-in-tensorflow</guid>
      <pubDate>Wed, 09 Oct 2019 14:35:06 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中的L1/L2正则化</title>
      <link>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</link>
      <description><![CDATA[如何在不手动计算的情况下在Pytorch中添加L1/L2正则化？]]></description>
      <guid>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</guid>
      <pubDate>Thu, 09 Mar 2017 19:54:19 GMT</pubDate>
    </item>
    </channel>
</rss>