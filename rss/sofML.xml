<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 29 Nov 2024 06:25:34 GMT</lastBuildDate>
    <item>
      <title>如何使用 Arm CMSIS-NN Softmax 函数进行嵌入式机器学习</title>
      <link>https://stackoverflow.com/questions/79235253/how-to-use-arm-cmsis-nn-softmax-function-for-embedded-ml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79235253/how-to-use-arm-cmsis-nn-softmax-function-for-embedded-ml</guid>
      <pubDate>Thu, 28 Nov 2024 20:33:38 GMT</pubDate>
    </item>
    <item>
      <title>如何比较不同年份的集群？</title>
      <link>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</link>
      <description><![CDATA[我有多个数据集，所有数据集的组织方式都类似（相同的变量、值等）。我使用 KModes 独立分析了数据集，但是，我试图寻找多年来可能出现的趋势。我该如何比较不同年份的集群？]]></description>
      <guid>https://stackoverflow.com/questions/79234461/how-to-compare-clusters-from-different-years</guid>
      <pubDate>Thu, 28 Nov 2024 15:14:27 GMT</pubDate>
    </item>
    <item>
      <title>无法从 xgboost 导入名称 XGBRegressor（未知位置）</title>
      <link>https://stackoverflow.com/questions/79234191/cannot-import-name-xgbregressor-from-xgboost-unknown-location</link>
      <description><![CDATA[xgboost 错误
无法导入 XGBRegressor
我在 vscode 上创建了一个环境，用于为机器学习项目实现端到端管道。我的大部分代码都保存在 github 中。我使用 requirements.txt 文件安装了所有 python 包。除了 xgboost 之外，其他所有包都可以使用]]></description>
      <guid>https://stackoverflow.com/questions/79234191/cannot-import-name-xgbregressor-from-xgboost-unknown-location</guid>
      <pubDate>Thu, 28 Nov 2024 13:56:19 GMT</pubDate>
    </item>
    <item>
      <title>我如何才能以某种方式融合嵌入以提高效率和分数？</title>
      <link>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</guid>
      <pubDate>Thu, 28 Nov 2024 13:01:00 GMT</pubDate>
    </item>
    <item>
      <title>利用贝叶斯优化进行黑箱优化</title>
      <link>https://stackoverflow.com/questions/79233932/blackbox-opimization-with-bayesian-optimization</link>
      <description><![CDATA[我想使用贝叶斯优化来识别物理实验中新的有趣参数。
更具体地说，我想用已知的有界实验参数初始化算法，并使用贝叶斯优化进行黑盒优化。我目前正在使用一个名为“bayes_op”的库在 Python 上工作，但是，我发现的所有示例都使用了数学定义的函数。
我想首先给出函数在某些实验点上的得分，以开始算法，然后获得下一个有趣的实验参数的建议。
到目前为止，我已经成功地从随机参数开始优化已知函数。但是，我想知道是否有可能在不提供显式函数的情况下使用此算法，同时利用已有的实验数据。我想补充一点，我没有数据集来训练任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/79233932/blackbox-opimization-with-bayesian-optimization</guid>
      <pubDate>Thu, 28 Nov 2024 12:40:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习。如何让神经网络记住上下文和数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/79233875/ml-how-to-make-a-neural-network-remember-the-context-and-data</link>
      <description><![CDATA[我希望神经网络能够记忆，但感知器只能在训练期间记住一些东西，但我希望神经网络能够适应新情况而无需重新训练，例如，如果我说我的名字是尼古拉，它就会记住，或者如果绿色蘑菇以前在游戏中有用，但后来变得有毒，它就会停止食用，然后，如果它们恢复正常，它就会再次开始食用。我考虑过 LSTM，但这是一个循环神经网络，人们似乎正在放弃它们而选择 transformer？]]></description>
      <guid>https://stackoverflow.com/questions/79233875/ml-how-to-make-a-neural-network-remember-the-context-and-data</guid>
      <pubDate>Thu, 28 Nov 2024 12:20:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在步骤 0 记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？与 eval_on_start 类似，但 train_on_start？
交叉：https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>如何解决矢量化器不匹配问题</title>
      <link>https://stackoverflow.com/questions/79231510/how-do-i-resolve-vectorizer-mismatch</link>
      <description><![CDATA[我正在使用 TfidfVectorizer 作为文本矢量化器，但当我尝试获取 cosine_similarity 时，我遇到了维度不匹配的问题。
我的情况如下：
首先，
def clean_text(text):
return re.sub(r&#39;[^a-zA-Z0-9 ]&#39;, &quot;&quot;, text)

movies[&#39;title&#39;] = movies[&#39;title&#39;].apply(clean_text)

vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words =&#39;english&#39;)

title_vec = vectorizer.fit_transform(movies[&#39;title&#39;])

title = &quot;Toy Story&quot;

title = clean_text(title)

word_vec = vectorizer.transform([title])

similarity = cosine_similarity(word_vec, title_vec)

这会导致错误消息：
ValueError: X 和 Y 矩阵的维度不兼容：X.shape[1] == 172412 而 Y.shape[1] == 156967

PS：我检查了 word_vec 和 title_vec 的 len，它们显示的长度不同。
我在 vectorizer 中设置了 ngram_range=(1,1)，但没有得到肯定的结果。
我使用了 countvectorizer()，但问题仍然存在
我没有其他选择，而 chatGPT 提供的解决方案也无法解决问题：
from scipy.sparse import hstack

用零填充较小的矩阵
if word_vec.shape[1] &gt; title_vec.shape[1]:
diff = word_vec.shape[1] - title_vec.shape[1]
title_vec = hstack([title_vec, np.zeros((title_vec.shape[0], diff))])
elif title_vec.shape[1] &gt; word_vec.shape[1]:
diff = title_vec.shape[1] - word_vec.shape[1]
word_vec = hstack([word_vec, np.zeros((word_vec.shape[0], diff))])

所以我不能使用上面的代码，但我把它放在这里以显示这个问题的严重程度。]]></description>
      <guid>https://stackoverflow.com/questions/79231510/how-do-i-resolve-vectorizer-mismatch</guid>
      <pubDate>Wed, 27 Nov 2024 18:38:44 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建多尺度 CNN，但遇到此错误：RuntimeError：mat1 和 mat2 形状无法相乘（32x4095 和 4096x4096）</title>
      <link>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</guid>
      <pubDate>Tue, 26 Nov 2024 23:06:22 GMT</pubDate>
    </item>
    <item>
      <title>如何从本地向量数据库中删除特定的问答对？</title>
      <link>https://stackoverflow.com/questions/79226011/how-to-delete-a-specific-question-response-pair-from-a-local-vector-database</link>
      <description><![CDATA[我已经实现了一个基于反馈的聊天机器人，它将问题-响应对存储在本地向量数据库中。我使用以下设置来访问向量数据库：
import os

VECTOR_DB_PATH = os.path.join(os.getcwd(), &quot;vectordb&quot;)

向量数据库用于存储嵌入和响应，以实现高效的相似性搜索。现在，我想从向量数据库中删除特定的问题-响应对。我知道我想删除的对（例如，问题：“你好吗？”，回答：“我很好，谢谢！”），但我不确定如何以编程方式从数据库中删除它。
以下是有关我的设置的一些详细信息：
我使用的向量数据库库是 FAISS。
问题的嵌入存储在数据库中以供检索。
我可以成功访问和查询向量数据库。
我正在使用的库是
import json
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import boto3
import botocore
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

从向量数据库中查找和删除特定对的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79226011/how-to-delete-a-specific-question-response-pair-from-a-local-vector-database</guid>
      <pubDate>Tue, 26 Nov 2024 09:12:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 RMSprop 优化器的动态学习率进行 Q 学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。以下是我正在使用的 Q-learning 更新函数：
def update_q_table(self, state, action, reward, next_state):
best_next_action = np.argmax(self.q_table[next_state, :])
td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
td_error = td_target - self.q_table[state, action]

# 更新 RMSprop 的平方梯度移动平均值 E[g^2]
self.gradient_Q[state, action] = (
self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2)
)

self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon)
self.learning_rate_history.append(self.learning_rate)

# 使用固定学习率和 TD 误差更新 Q 值
self.q_table[state, action] += self.learning_rate * td_error 

# 存储 E[g^2] 值用于跟踪
self.gradient_history.append(self.gradient_Q[state, action])
]]></description>
      <guid>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</guid>
      <pubDate>Mon, 11 Nov 2024 10:33:43 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用 DataLoaders 进行 k 折交叉验证</title>
      <link>https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch</link>
      <description><![CDATA[我已将训练数据集拆分为 80% 训练数据和 20% 验证数据，并创建了如下所示的 DataLoaders。但是我不想限制我的模型的训练。所以我想到将我的数据拆分为 K（可能是 5）个部分并执行交叉验证。但是我不知道如何在拆分数据集后将它们合并到我的数据加载器中。
train_size = int(0.8 * len(full_dataset))
validation_size = len(full_dataset) - train_size
train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])

full_loader = DataLoader(full_dataset, batch_size=4,sampler = sampler_(full_dataset), pin_memory=True) 
train_loader = DataLoader(train_dataset, batch_size=4, sampler = sampler_(train_dataset))
val_loader = DataLoader(validation_dataset, batch_size=1, sampler = sampler_(validation_dataset))
]]></description>
      <guid>https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch</guid>
      <pubDate>Fri, 27 Mar 2020 09:59:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AWS SageMaker 要运行 Web 服务器进行批量转换？</title>
      <link>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</link>
      <description><![CDATA[我正在创建自己的 Docker 容器以用于 SageMaker，我想知道当我想要执行批量转换作业时，serve 命令为什么会创建一个 Flask 应用程序来提供数据预测。只需解开模型并在我想要预测的数据集上运行模型的预测方法，不是更简单吗？我不需要 Web API/端点。我只需要每天自动生成一次预测。]]></description>
      <guid>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</guid>
      <pubDate>Thu, 21 Nov 2019 23:11:19 GMT</pubDate>
    </item>
    <item>
      <title>具有状态-动作-状态奖励结构的 Q 学习，以及以状态为行、以动作为列的 Q 矩阵</title>
      <link>https://stackoverflow.com/questions/45382763/q-learning-with-a-state-action-state-reward-structure-and-a-q-matrix-with-states</link>
      <description><![CDATA[我在 R 中设置了一个 Q 学习问题，希望有人能帮助我确定我的方法在构建问题时的理论正确性。
问题结构
对于这个问题，环境由 10 个可能的状态组成。在每个状态中，代理有 11 个潜在动作可供选择（无论代理处于什么状态，这些动作都是相同的）。根据代理所处的特定状态以及代理随后采取的后续动作，转换到下一个状态有一个唯一的分布，即转换到任何下一个状态的概率仅取决于前一个状态以及随后采取的动作。
每个情节有 9 次迭代，即代理可以在新情节开始之前采取 9 个动作并进行 9 次转换。在每一集中，代理都将从状态 1 开始。
在每一集中，在代理的 9 个动作中的每一个之后，代理都将获得奖励，该奖励取决于代理的（紧接的）前一个状态和他们（紧接的）前一个动作以及他们所处的状态，即代理的奖励结构取决于状态-动作-状态三元组（一集中将有 9 个）。
代理的转移概率矩阵是静态的，奖励矩阵也是如此。
我已经设置了两种学习算法。在第一个算法中，q 矩阵更新发生在每个情节中的每个动作之后。在第二个算法中，q 矩阵在每集之后更新。该算法使用 epsilon 贪婪学习公式。
最大的问题是，在我的 Q 学习中，我的代理没有学习。随着时间的推移，它获得的奖励越来越少。我已经研究过其他潜在问题，例如简单的计算错误或代码中的错误，但我认为问题在于我的 q 学习问题的概念结构。
问题

我已将 Q 矩阵设置为 10 行 11 列的矩阵，即所有 10 个状态都是行，11 个动作都是列。这是最好的方法吗？这意味着代理正在学习一种策略，该策略规定“只要您处于状态 x，就执行动作 y”
鉴于我的问题的这种独特结构，标准 Q 更新是否仍然适用？即 Q[cs,act]&lt;&lt;-Q[cs,act]+alpha*(Reward+gamma*max(Q[ns,])-Q[cs,act])
其中 cs 是当前状态；act 是选择的动作；奖励是根据您的当前状态、您选择的操作以及您将转换到的下一个状态而获得的奖励；ns 是根据您的上一个状态和上一个操作而转换到的下一个状态（请注意，您是随机转换到此状态的）。
R 中是否有开放的 AI 健身房？是否有针对这种结构问题的 Q 学习包？
]]></description>
      <guid>https://stackoverflow.com/questions/45382763/q-learning-with-a-state-action-state-reward-structure-and-a-q-matrix-with-states</guid>
      <pubDate>Fri, 28 Jul 2017 21:36:22 GMT</pubDate>
    </item>
    <item>
      <title>文本分类器</title>
      <link>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</link>
      <description><![CDATA[有人知道好的开源文本分类模型吗？我知道斯坦福分类器、Weka、Mallet 等，但它们都需要训练。
我需要将新闻文章分类为体育/政治/健康/游戏/等。有没有预先训练过的模型？
Alchemy、OpenCalais 等不是选择。我需要开源工具（最好是 Java 语言的）。]]></description>
      <guid>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</guid>
      <pubDate>Thu, 07 Mar 2013 15:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>