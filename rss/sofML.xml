<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 28 Dec 2023 21:12:25 GMT</lastBuildDate>
    <item>
      <title>ValueError：无法对块大小未知的对象调用 len()</title>
      <link>https://stackoverflow.com/questions/77729114/valueerror-cannot-call-len-on-object-with-unknown-chunk-size</link>
      <description><![CDATA[https://colab.research.google.com/drive /1wj_nA6J2FcgRIst-OU96usf3c1TA2Ubo?usp=共享
我正在尝试使用二分图构建一个 GCN 来进行攻击预测。我不确定我还需要做什么。谁能帮我解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/77729114/valueerror-cannot-call-len-on-object-with-unknown-chunk-size</guid>
      <pubDate>Thu, 28 Dec 2023 20:53:02 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。
]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>将文本转换为十进制数量的问题（处理“3个半”场景）</title>
      <link>https://stackoverflow.com/questions/77728048/issue-with-converting-text-to-decimal-quantities-handling-3-and-a-half-scenar</link>
      <description><![CDATA[此代码应将文本转换为小数
如果我写三个，它会将其转换为 3，但是当我写三个半时会出现问题
这种情况有什么解决办法吗？
示例：
“请给我三个半番茄和一瓶牛奶，三公斤胡萝卜和半公斤豆子最后一瓶水，谢谢”
结果：
购物清单：
1瓶牛奶
3.0公斤胡萝卜
0.5公斤番茄
1.0瓶水
0.5公斤豆子

结果应该是：
购物清单：
1瓶牛奶
3.0公斤胡萝卜
3.5公斤番茄
1.0瓶水
0.5公斤豆子

导入重新
从集合导入 OrderedDict

产品={
    “牛奶”：[“牛奶”]，
    “胡萝卜”：[“胡萝卜”，“胡萝卜”]，
    “番茄”：[“番茄”、“西红柿”]，
    “水”：[“水”]，
    “豆子”：[“豆子”]
    # 以类似的方式添加更多产品
}

def Convert_to_decimal(quantity_text):
    数量文本 = 数量文本.lower()
    如果数量文本在 [&#39;half&#39;, &#39;quarter&#39;] 中：
        返回 {&#39;half&#39;: 0.5, &#39;quarter&#39;: 0.25}[quantity_text]
    elif数量_文本.isdigit():
        返回浮点数（数量文本）
    别的：
        单词到数字 = {
            “一”：1、“二”：2、“三”：3、“四”：4、“五”：5、
            “六”：6、“七”：7、“八”：8、“九”：9、“十”：10
        }
        返回words_to_numbers.get(quantity_text, 0.0)

def create_shopping_list(文本):
    项目 = OrderedDict()
    对于产品，products.items() 中的关键字：
        对于关键字中的关键字：
            模式 = fr&#39;(\d+(?:\.\d+)?|\b(?:一半|四分之一|一|二|三|四|五|六|七|八|九|十)\b)\ s*(?:bottle|kilo)?s?\s*(?:of\s*)?{关键字}&#39;
            匹配= re.findall（模式，文本，re.IGNORECASE）
            对于比赛中的比赛：
                数量 = 转换为小数（匹配）
                items.setdefault(product, set()).add(quantity) # 在集合中存储数量以删除重复项

    结果=[]
    对于产品，items.items() 中的数量：
        对于数量的数量：
            # 根据产品类型添加单位
            如果产品==“水”：
                单位=&#39;瓶&#39;
            别的：
                如果产品为 [&#39;牛奶&#39;]，则单位 = &#39;瓶子&#39;，否则为 &#39;公斤&#39;
            result.append(f&quot;{数量} {单位} {产品}&quot;)

    # 将结果转换回列表以保留顺序并删除重复项
    unique_shopping_list = list(dict.fromkeys(结果))
    返回unique_shopping_list

# 用法示例
user_input = “请给我两颗四分之一的番茄和一瓶牛奶，3公斤胡萝卜和半公斤豆子，最后一瓶水，谢谢”
购物列表 = 创建购物列表（用户输入）
如果购物清单：
    print(&quot;购物清单：&quot;)
    对于 Shopping_list 中的商品：
        打印（项目）
别的：
    print(“输入中未找到任何项目。”)

&lt;前&gt;&lt;代码&gt;]]></description>
      <guid>https://stackoverflow.com/questions/77728048/issue-with-converting-text-to-decimal-quantities-handling-3-and-a-half-scenar</guid>
      <pubDate>Thu, 28 Dec 2023 16:30:50 GMT</pubDate>
    </item>
    <item>
      <title>将回归问题中的预测值转换为实际值[重复]</title>
      <link>https://stackoverflow.com/questions/77726639/covert-the-predicted-values-into-the-actual-ones-in-regression-problem</link>
      <description><![CDATA[我有一个回归代码，它获取数据并使用 one-hot 编码器将其转换为数值，.. 用于数值和分类值并预测温度（目标类别）。
我想显示代码的实际预测值而不是编码的值。
这是代码：
# 特征工程步骤（缩放和编码）
numeric_cols = data.select_dtypes(include=[&#39;number&#39;]).columns
categorical_cols = data.select_dtypes(exclude=[&#39;number&#39;]).columns

缩放器 = MinMaxScaler()
数据[数值列] = scaler.fit_transform(数据[数值列])

数据 = pd.get_dummies(数据, columns=categorical_cols, drop_first=True)

# 反转温度缩放以获得原始值
温度缩放器 = MinMaxScaler()
temp = data[&#39;家里设置的温度&#39;].values.reshape(-1, 1)
data[&#39;家里设置的温度&#39;] = temp_scaler.fit_transform(温度)

# 将数据拆分为特征 (X) 和目标变量 (y)
X = data.drop(columns=[&#39;家里设置的温度&#39;])
y = data[&#39;家中设置的温度&#39;]

# 将数据拆分为训练集（X_train）和测试集（X_test，y_train，y_test）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

如何显示预测的实际值而不是编码的预测值？]]></description>
      <guid>https://stackoverflow.com/questions/77726639/covert-the-predicted-values-into-the-actual-ones-in-regression-problem</guid>
      <pubDate>Thu, 28 Dec 2023 11:38:30 GMT</pubDate>
    </item>
    <item>
      <title>model.fit() 不使用 GPU，在 google colab 中训练深度学习模型</title>
      <link>https://stackoverflow.com/questions/77726390/model-fit-is-not-using-gpu-training-deep-learning-model-in-google-colab</link>
      <description><![CDATA[我正在 google colab pro 上训练我的模型，我看到我已连接到 V100 GPU 运行时，但训练并未使用它，仅使用 RAM。这就是我加载数据的方式，也许有帮助：
def data_generator(nifti_files, mask_files):
    对于 zip 中的 nifti_file、mask_file(nifti_files, mask_files)：
        nifti_image = np.load（nifti_file）
        nifti_mask = np.load（掩码文件）

        裁剪图像 = nifti_image[56:184, 56:184, 13:141]
        裁剪后的掩码 = nifti_mask[56:184, 56:184, 13:141]

        产量（cropped_image，cropped_mask）

# 创建数据集
数据集 = tf.data.Dataset.from_generator(
    lambda：data_generator（train_volumes，train_masks），
    输出签名=(
        tf.TensorSpec(形状=(128, 128, 128, 1), dtype=tf.float32),
        tf.TensorSpec(形状=(128, 128, 128, 4), dtype=tf.float32)
    ）
）

dataset_val = tf.data.Dataset.from_generator(
    lambda: data_generator(val_volumes, val_masks),
    输出签名=(
        tf.TensorSpec(形状=(128, 128, 128, 1), dtype=tf.float32),
        tf.TensorSpec(形状=(128, 128, 128, 4), dtype=tf.float32)
    ）
）

这是资源，在这里您可以看到没有使用 GPU：
图片]]></description>
      <guid>https://stackoverflow.com/questions/77726390/model-fit-is-not-using-gpu-training-deep-learning-model-in-google-colab</guid>
      <pubDate>Thu, 28 Dec 2023 10:43:45 GMT</pubDate>
    </item>
    <item>
      <title>脑肿瘤生存模型的预测结果是二元的——如何解释？</title>
      <link>https://stackoverflow.com/questions/77725853/predicted-results-from-brain-tumor-survival-model-are-binary-how-to-interpret</link>
      <description><![CDATA[我正在使用 BraTS20 数据集进行“脑肿瘤总体生存预测”。
模型的输出是一个二进制数组，但是如何预测患者的生存时间？
为什么预测结果是二进制的？
如何在脑肿瘤生存时间的背景下解释这些二元结果？
我需要对模型或后处理步骤进行任何具体调整吗？
我想在向模型提供 MRI 输入时预测生存时间，我必须为此设计一个 Web 应用程序，用户在其中提供 MRI 以获得总体生存时间。
这里是数据转换成数组的地方
def getListAgeDays(id_list): # 仅创建年龄：类别数据

    x_val = [] # 初始化一个空列表来存储特征数据
    y_val = [] # 初始化一个空列表来存储类别标签
    对于 id_list 中的 i：
        if (i not in age_dict): # 检查 &#39;i&#39; 是否存在于 &#39;age_dict&#39; 字典中
            继续
        mask = getMaskSizesForVolume(nib.load(TRAIN_DATASET_PATH + f&#39;\\BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_seg.nii&#39;).get_fdata())
        # 加载分割掩码并提取其大小信息
        Brain_vol = getBrainSizeForVolume(nib.load(TRAIN_DATASET_PATH + f&#39;\\BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_t1.nii&#39;).get_fdata())
        # 加载大脑体积图像并提取其大小信息
        mask[1] = mask[1]/brain_vol # 标准化 mask 的大小
        掩码[2] = 掩码[2]/brain_vol
        掩码[3] = 掩码[3]/brain_vol
        合并 = [age_dict[i]、掩码[1]、掩码[2]、掩码[3]]
        # 通过将“age_dict[i]”与掩码大小值相结合来创建特征向量
        radiomics_values = df_radiomics.loc[df_radiomics[&#39;目标&#39;] == str(i)]
        # 查询 DataFrame &#39;df_radiomics&#39; 中与 &#39;i&#39; 相关的放射组学值
        如果 radiomics_values.empty：
            continue # 如果没有放射组学值则跳过本次迭代
        merged.extend(radiomics_values.values.tolist()[0][:-1])
        # 将放射组学值添加到“合并”特征向量（不包括最后一个值）
        x_val.append(merged) # 将特征向量附加到 &#39;x_val&#39;
        如果（days_dict[i] &lt; 250）：
            y_val.append([1, 0, 0]) # 根据 &#39;days_dict&#39; 条件附加类别标签
        elif (days_dict[i] &gt;= 250 且 days_dict[i] &lt; 450):
            y_val.append([0, 1, 0])
        别的：
            y_val.append([0, 0, 1])
    return np.array(x_val), np.array(y_val) # 以 NumPy 数组形式返回特征数据和类别标签

X_all, y_all = getListAgeDays(brats_ids) # 使用 &#39;train_and_test_ids&#39; 调用该函数
print(f&#39;X_test: {X_all.shape}&#39;) # 打印特征数据的形状

这是进入模型的X_test：
数组([[0.424, 0.385, 0.725, 0.262, 0.304, 0.352, 0.555, 0.368, 0.276,
        0.531、0.054、0.286、0.337、0.447、0.464、0.334、0.235、0.607、
        0.789, 0.569, 0.849],
       [0.691, 0.066, 0.026, 0.165, 0.019, 0.063, 0.209, 0.81 , 0. ,
        0.419、0.、1.、0.383、0.335、0.36、0.392、0.335、0.429、
        0.381、0.255、0.257]……

将模型保存到Joblib中
joblib.dump(模型, &#39;model_joblib&#39;)
mj = joblib.load(&#39;model_joblib&#39;)

这是预测
mj.predict(X_test)

输出为：
数组([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 , 0, 0, 0])
]]></description>
      <guid>https://stackoverflow.com/questions/77725853/predicted-results-from-brain-tumor-survival-model-are-binary-how-to-interpret</guid>
      <pubDate>Thu, 28 Dec 2023 08:51:33 GMT</pubDate>
    </item>
    <item>
      <title>在训练神经网络模型时，如何在 matlab 中编写 Garson 算法来查找参数的相对重要性？</title>
      <link>https://stackoverflow.com/questions/77725247/how-do-you-code-garsons-algorithm-in-matlab-to-find-the-relative-importance-of</link>
      <description><![CDATA[我正在进行一项预测分析研究，并遇到了加森算法，但我在为其编写公式时遇到了麻烦。这是我的代码：
%%% 数据输入
输入=数据(:, 1:4)&#39;;
目标=数据(:, 5)&#39;;

%%% 创建前馈神经网络
净 = 前馈网络([10, 15, 20]);

%%%% 设置每个隐藏层的神经元数量
net.layers{1}.size = 10;
net.layers{2}.size = 15;
net.layers{3}.size = 20;

%%%% 更改激活函数（例如，将输出层的“tansig”更改为“purelin”）
net.layers{1}.transferFcn = &#39;tansig&#39;; % 第一个隐藏层的激活函数
net.layers{2}.transferFcn = &#39;tansig&#39;; % 第二隐藏层的激活函数
net.layers{3}.transferFcn = &#39;tansig&#39;; % 第三隐藏层的激活函数
net.layers{4}.transferFcn = &#39;purelin&#39;; % 输出层的激活函数

%%%% 划分数据进行训练、验证和测试
net.divideParam.trainRatio = 0.7;
net.divideParam.valRatio = 0.15;
net.divideParam.testRatio = 0.15;

%%%% 训练神经网络
[net, tr] = train(net, 输入, 目标);

%%%%计算Garson的重要性
重要性= garsonsAlgorithm(net);

%%%% 显示特征重要性
disp(&#39;特征重要性（Garson 算法）:&#39;);
对于 i = 1：长度（重要性）
    disp([&#39;x 的重要性&#39; num2str(i) &#39;: &#39; num2str(importance(i))]);
结尾

这是我写的garson算法函数：
函数重要性 = garsonsAlgorithm(net)
    %%% 从神经网络中提取连接权重
    权重 = cell2mat(net.IW);
    
    % 检查权重是否存在偏差
    如果 isfield(net, &#39;b&#39;)
        bias_weights = cell2mat(net.b);
        权重 = [权重;偏差权重]；
    结尾
    
    %%%计算权重的绝对值
    绝对权重=abs(权重);
    
    %%%计算每个输入特征的重要性
    重要性 = sum(absolute_weights, 1);
    
    %%% 标准化重要性值
    重要性=重要性/总和（重要性）；
结尾

这是否正确？另外，当改变加森算法中的输入数量时，输出会出现任何变化（相对重要性）吗？
我附上了Garson算法的公式供参考：
Garson 算法公式]]></description>
      <guid>https://stackoverflow.com/questions/77725247/how-do-you-code-garsons-algorithm-in-matlab-to-find-the-relative-importance-of</guid>
      <pubDate>Thu, 28 Dec 2023 06:04:03 GMT</pubDate>
    </item>
    <item>
      <title>在制作 yolov5 对象检测 iOS 应用程序时遇到问题</title>
      <link>https://stackoverflow.com/questions/77718415/having-trouble-making-a-yolov5-object-detection-ios-app</link>
      <description><![CDATA[我正在关注这个教程，但我的 Mac 硅 M2 芯片遇到一些问题（不确定这是否是问题），我已经克隆了 GitHub 存储库并手动安装了依赖项。但每次我在终端中执行下一步时：
python export-nms.py --include coreml --weights yolov5n.pt

我得到同样的错误：
&lt;块引用&gt;
CoreML：导出失败：无法仅从模型规范对象加载 mlProgram 类型的 MLModel。它还需要权重文件的路径。请同时使用“weights_dir”参数提供该信息。

我尝试将我的 Python 版本更改为教程使用的版本，但这不起作用，我也尝试过
python export-nms.py --weights /Users/oscarcanning-thompson/Documents/app/yolov5/yolov5n.pt --include coreml

ChatGPT 给了我（最后的手段），但这也不起作用
也许我把权重文件位置放错了。我不知道这是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77718415/having-trouble-making-a-yolov5-object-detection-ios-app</guid>
      <pubDate>Tue, 26 Dec 2023 17:29:52 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的Python REST API aiohttp 加载时间错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</link>
      <description><![CDATA[我在Python中使用aiohttp。我有一个机器学习项目，我想通过restapi 提供该项目。我对这些函数进行了计时，它们执行起来并不需要很长时间。他们不会等待结果，而是检查进度并开始任务。问题是等待时间相当长。我试图找到问题所在，它不是函数本身，而是 aiohttp 来运行函数或发送结果。函数本身不是问题。（等待时间10s，函数执行2e-05）。我只是创建一个包含该项目的类。有人对如何确保快速响应时间有建议吗？有没有办法为 API 预先分配资源或在单独的线程中生成机器学习项目。]]></description>
      <guid>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</guid>
      <pubDate>Mon, 25 Dec 2023 19:30:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用kaggle中的两个GPU在pytorch中进行训练？</title>
      <link>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</link>
      <description><![CDATA[我正在 Kaggle GPU 中训练模型。
但正如我所看到的，只有一个 GPU 正在工作。

我使用普通方法进行训练，例如
device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)
模型 = model.to(设备)

如何同时使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</guid>
      <pubDate>Wed, 13 Sep 2023 04:56:24 GMT</pubDate>
    </item>
    <item>
      <title>我遇到错误“不可散列类型：'numpy.ndarray'”</title>
      <link>https://stackoverflow.com/questions/70868316/i-am-facing-an-error-unhashable-type-numpy-ndarray</link>
      <description><![CDATA[这是我的代码
我尝试了很多方法，但都行不通
将 tkinter 导入为 tk
从 tkinter 导入文件对话框
从 tkinter 导入 *
从 PIL 导入 ImageTk，图像

导入numpy
#加载训练好的模型对符号进行分类
从 keras.models 导入 load_model
模型 = load_model(&#39;my_model.h5&#39;)

#dictionary 标记所有交通标志类别。
classes = { 1:&#39;限速（20公里/小时）&#39;,
            2:&#39;限速（30公里/小时）&#39;,
            3:&#39;限速（50公里/小时）&#39;,
            4:&#39;限速（60公里/小时）&#39;,
            5:&#39;限速（70公里/小时）&#39;,
            6:&#39;限速（80公里/小时）&#39;,
            7:&#39;限速结束（80公里/小时）&#39;,
            8:&#39;限速（100公里/小时）&#39;,
            9:&#39;限速（120公里/小时）&#39;,
            10:&#39;禁止通过&#39;,
            11:&#39;禁止超过3.5吨的车辆通过&#39;,
            12:&#39;十字路口的通行权&#39;,
            13:&#39;优先道路&#39;,
            14：&#39;产量&#39;，
            15：&#39;停止&#39;，
            16：“没有车辆”，
            17：&#39;Veh&gt;禁止3.5吨&#39;,
            18:&#39;禁止进入&#39;,
            19：“一般警告”，
            20:&#39;危险曲线左转&#39;,
            21:&#39;右危险曲线&#39;,
            22:&#39;双曲线&#39;,
            23：“崎岖不平的道路”，
            24：“路滑”，
            25:&#39;道路右侧变窄&#39;,
            26：“道路施工”，
            27：&#39;交通信号&#39;，
            28：“行人”，
            29：“儿童过马路”，
            30:&#39;自行车过路&#39;,
            31：“小心冰/雪”，
            32：“野生动物穿越”，
            33:&#39;结束速度+通过限制&#39;,
            34：“向前右转”，
            35：“前方左转”，
            36：&#39;仅向前&#39;，
            37：“直走或右转”，
            38：&#39;直走或左转&#39;,
            39：“靠右行驶”，
            40：“靠左行驶”，
            41：“强制迂回”，
            42：“禁止通过的结束”，
            43：&#39;结束禁止超车&gt; 3.5吨&#39;}

#初始化图形用户界面
顶部=tk.Tk()
顶部.几何(&#39;800x600&#39;)
top.title(&#39;交通标志分类&#39;)
顶部.configure(背景=&#39;#CDCDCD&#39;)

标签=标签(顶部,背景=&#39;#CDCCDD&#39;,字体=(&#39;arial&#39;,15,&#39;bold&#39;))
标志图像 = 标签（顶部）

def 分类（文件路径）：
    全局标签打包
    图像 = Image.open(文件路径)
    图像 = image.resize((30,30))
    图像 = numpy.expand_dims(图像, 轴=0)
    图像 = numpy.array(图像)
    Predict_x=model.predict(图像)
    pred=numpy.argmax(predict_x,axis=1)
    符号=类[pred+1]
    打印（签名）
    label.configure(foreground=&#39;#011638&#39;, text=sign)

def show_classify_button(文件路径):
    分类_b =按钮（顶部，文本=“分类图像”，命令= lambda：分类（文件路径），padx = 10，pady = 5）
    classify_b.configure(背景=&#39;#364156&#39;,前景=&#39;白色&#39;,字体=(&#39;arial&#39;,10,&#39;bold&#39;))
    分类b.place(relx=0.79,rely=0.46)

def upload_image():
    尝试：
        file_path=filedialog.askopenfilename()
        上传=Image.open(文件路径)
        上传的缩略图(((top.winfo_width()/2.25),(top.winfo_height()/2.25)))
        im=ImageTk.PhotoImage(已上传)

        sign_image.configure(图像=im)
        标志图像.image=im
        标签.configure(text=&#39;&#39;)
        显示分类按钮（文件路径）
    除了：
        经过

upload=Button(top,text=“上传图片”,command=upload_image,padx=10,pady=5)
upload.configure(background=&#39;#364156&#39;, foreground=&#39;white&#39;,font=(&#39;arial&#39;,10,&#39;bold&#39;))

upload.pack(side=BOTTOM,pady=50)
sign_image.pack(side=BOTTOM,expand=True)
label.pack(side=BOTTOM,expand=True)
header = Label(top, text=“了解你的交通标志”,pady=20, font=(&#39;arial&#39;,20,&#39;bold&#39;))
header.configure(背景=&#39;#CDCDCD&#39;,前景=&#39;#364156&#39;)
标题.pack()
顶部.mainloop()

我遇到的错误：
Tkinter 回调中出现异常
回溯（最近一次调用最后一次）：
  文件“C:\Program Files\Python310\lib\tkinter\__init__.py”，第 1921 行，在 __call__ 中
    返回 self.func(*args)
  文件“C:\Users\GOWTHAM\AppData\Local\Temp\ipykernel_10936\430013951.py”，第 78 行，位于 &lt;lambda&gt; 中。
    分类_b =按钮（顶部，文本=“分类图像”，命令= lambda：分类（文件路径），padx = 10，pady = 5）
  文件“C:\Users\GOWTHAM\AppData\Local\Temp\ipykernel_10936\430013951.py”，第 73 行，分类
    符号=类[pred+1]


类型错误：不可散列的类型：&#39;numpy.ndarray&#39;

任何人都可以帮我解决这个问题吗？提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/70868316/i-am-facing-an-error-unhashable-type-numpy-ndarray</guid>
      <pubDate>Wed, 26 Jan 2022 18:12:47 GMT</pubDate>
    </item>
    <item>
      <title>我需要清楚地预测 X 测试、X 训练、y 测试、y 训练</title>
      <link>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</link>
      <description><![CDATA[在线性回归模型中，假设我们有 3 个自变量（年龄、身高、性别）和 1 个因变量（糖尿病），然后我们将模型拆分为 X 训练，即（例如 70%）自变量数据对于训练，X测试-&gt;即30%的自变量数据用于测试
y 火车-&gt;即（例如70%）用于训练的因变量数据，y检验-&gt;即30%的因变量数据用于测试
因此，当我们预测 X 检验或预测 X 检验时，我们是在预测自变量的值还是在预测因变量（糖尿病？）]]></description>
      <guid>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</guid>
      <pubDate>Sat, 15 Jan 2022 12:37:46 GMT</pubDate>
    </item>
    <item>
      <title>鉴于您有多个虚拟列，如何预测值？</title>
      <link>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</link>
      <description><![CDATA[我有一个类似于以下内容的数据框：
 薪资 职务 Raiting Company_Name 地点 资历
0 100 SE 5 苹果 SF 副总裁
1 120 DS 4 三星 la Jr
2 230 QA 5 谷歌 sd Sr


（我的 df 具有比这更多的分类特征）
通常，当从模型进行预测时，它会像这样
in[1]: inModel_name.predict(catagory_1, catagory_2,..etc)
输出[2]：预测变量

然而，在使用pd.get_dummies之后，根据您创建的分类特征的数量，您会获得更多的列，这使得我之前提到的方法在尝试预测数据时变得不切实际。如何引用多列而不是手动输入 0。]]></description>
      <guid>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</guid>
      <pubDate>Sun, 16 Aug 2020 03:43:39 GMT</pubDate>
    </item>
    <item>
      <title>SVM 二元分类器为所有测试数据预测一类</title>
      <link>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</link>
      <description><![CDATA[我有一个包含 10 个特征的分类问题，我必须预测 1 或 0。当我训练 SVC 模型时，通过训练测试分割，数据测试部分的所有预测值均为 0。数据具有以下 0-1 计数：

0：1875
1：1463

训练模型的代码如下：
从 sklearn.svm 导入 SVC
模型 = SVC()
model.fit(X_train, y_train)
pred= model.predict(X_test)
从 sklearn.metrics 导入 precision_score
准确度分数（y_测试，预测）

为什么它在所有情况下都预测 0？]]></description>
      <guid>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</guid>
      <pubDate>Wed, 18 Sep 2019 11:10:06 GMT</pubDate>
    </item>
    <item>
      <title>如何解释回归中的MSE？</title>
      <link>https://stackoverflow.com/questions/48973140/how-to-interpret-mse-in-regression</link>
      <description><![CDATA[我正在尝试建立一个模型来预测房价。
我有一些功能 X（浴室数量等）和目标 Y（大约 300,000 美元到 800,000 美元）
在将 Y 拟合到模型之前，我使用 sklearn 的标准缩放器对 Y 进行标准化。
这是我的 Keras 模型：
def build_model():
    模型=顺序（）
    model.add（密集（36，input_dim = 36，激活=&#39;relu&#39;））
    model.add（密集（18，input_dim = 36，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;sigmoid&#39;））
    model.compile（损失=&#39;mse&#39;，优化器=&#39;sgd&#39;，指标=[&#39;mae&#39;，&#39;mse&#39;]）
    返回模型

我在尝试解释结果时遇到困难 - 0.617454319755 的 MSE 意味着什么？
我是否必须对这个数字进行逆变换，并对结果求平方根，得到 741.55 美元的错误率？
math.sqrt(sc.inverse_transform([mse]))

我很抱歉在刚开始时听起来很愚蠢！]]></description>
      <guid>https://stackoverflow.com/questions/48973140/how-to-interpret-mse-in-regression</guid>
      <pubDate>Sun, 25 Feb 2018 11:57:03 GMT</pubDate>
    </item>
    </channel>
</rss>