<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Mar 2024 12:23:13 GMT</lastBuildDate>
    <item>
      <title>模型精度低-Kaggle Titanic数据集</title>
      <link>https://stackoverflow.com/questions/78214016/low-model-accuracy-kaggle-titanic-dataset</link>
      <description><![CDATA[对于 Kaggle 上的泰坦尼克号挑战，我应用了几个预处理步骤和特征工程技术：
1.将工单列拆分为数字和字母，将它们视为分类特征：
X[“Ticket”] = X[“Ticket”].apply(lambda x: 1 if str(x).isdigit() else 0)

2.删除名称列：
X = X.drop([“名称”,“票”], axis=1)

3.将 Cabin 列中的缺失值视为单独的类别：
X[“Cabin”] = X[“Cabin”].apply(lambda x: 1 if pd.isna(x) else 0)

4.使用SimpleImputer使用均值策略填充缺失值，并删除Embarked列中包含缺失值的行。
`X_reshape = X[:, 2].reshape(-1, 1) # 输入应该是二维数组，但 X[:, 3] 是一维数组。
imputer = SimpleImputer（missing_values=np.nan，strategy=“mean”）
imputer.fit(X_reshape)

X[:, 2] = imputer.transform(X_reshape).flatten()`

5.使用ColumnTransformer进行标准缩放和数据分割。
from sklearn.preprocessing import StandardScaler
sc = 标准缩放器()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

尽管做出了这些努力，模型的准确度仍保持在 75%，而期望的准确度为 81%，标准差为 4%。我尝试过不同的模型，但尚未达到所需的性能。任何改进建议将不胜感激。
整个预处理步骤：
`# 分离票。如果有字母表则为 0，如果没有则为 1。

X[“票”] = X[“票”].apply(lambda x: 1 if str(x).isdigit() else 0)

# 删除名称列
X = X.drop([“名称”], axis=1)

从 sklearn.impute 导入 SimpleImputer

# 在 Cabin 中引入 NaN 值作为新类别。
X[“Cabin”] = X[“Cabin”].apply(lambda x: 1 if pd.isna(x) else 0)
X = X.值

＃ 年龄
X_reshape = X[:, 2].reshape(-1, 1) # 输入应该是二维数组，但 X[:, 3] 是一维数组。

imputer = SimpleImputer(missing_values=np.nan,策略=“平均值”)
imputer.fit(X_reshape)

X[:, 2] = imputer.transform(X_reshape).flatten()

# 上船了。因为只有两个样本的 Embarked 列为 NaN，所以我们可以删除这两行。
# train_df[train_df[&#39;Embarked&#39;].isnull()]
train_df.drop(索引=61，轴=0，就地=True)
train_df.drop(索引=829，轴=0，就地=True)

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.preprocessing 导入 OneHotEncoder

ct = ColumnTransformer([(“编码”, OneHotEncoder(), [1, 6, 7])], 余数=“直通”)
X = np.array(ct.fit_transform(X))

从 sklearn.preprocessing 导入 StandardScaler
sc = 标准缩放器()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)`
]]></description>
      <guid>https://stackoverflow.com/questions/78214016/low-model-accuracy-kaggle-titanic-dataset</guid>
      <pubDate>Sun, 24 Mar 2024 09:36:05 GMT</pubDate>
    </item>
    <item>
      <title>CCA 相关性分析显示所有 1</title>
      <link>https://stackoverflow.com/questions/78212742/cca-correlation-analysis-showing-all-1</link>
      <description><![CDATA[当我进行相关性规范分析时，我得到的输出全部等于 1，这正常吗？
#install.package(“CCA”)

setwd &lt;-“~/下载”

# 加载CCA包
图书馆（CCA）
matrice_goals&lt;- as.data.frame(matrice_goals)
Ind_demo-std &lt;- as.data.frame(Ind_demo_std)

# 计算相关矩阵
risultati &lt;- cancor(matrice_goals, Ind_demo_std)

打印（结果$cor）

print(risultati$cor)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

我不明白问题是代码还是数据，即使我已经标准化了。]]></description>
      <guid>https://stackoverflow.com/questions/78212742/cca-correlation-analysis-showing-all-1</guid>
      <pubDate>Sat, 23 Mar 2024 22:32:24 GMT</pubDate>
    </item>
    <item>
      <title>错误消息['OneHotEncoder'对象没有属性'_drop_idx_after_grouping']</title>
      <link>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</guid>
      <pubDate>Sat, 23 Mar 2024 21:35:23 GMT</pubDate>
    </item>
    <item>
      <title>统一构建 APK [关闭]</title>
      <link>https://stackoverflow.com/questions/78212208/building-apk-in-unity</link>
      <description><![CDATA[我做了一个统一项目，它采用机器学习模型来预测图像的类别。它在统一中完美运行。但是当我构建 apk 文件并在我的手机中运行它时。没有什么是可见的。即预测是不可见的。为什么 ？我直接在unity中集成了keras模型，是不是因为我的手机没有安装python？
Android 手机上的结果Unity 上的结果
当我提取 apk 时，我没有在 apk 中看到我的模型或其他详细信息。怎么解决这个问题。如何在我的 apk 中添加 Unity 中使用的所有文件夹和文件]]></description>
      <guid>https://stackoverflow.com/questions/78212208/building-apk-in-unity</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:12 GMT</pubDate>
    </item>
    <item>
      <title>深度学习模型训练精度高，但在二进制文本分类中的测试数据上表现不佳[关闭]</title>
      <link>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</link>
      <description><![CDATA[我在处理二进制文本分类任务时遇到了一个令人困惑的问题。尽管尝试了多种深度学习模型，包括各种架构和超参数，但我始终观察到很高的训练准确度，通常在 97% 到 99% 之间。然而，当我根据看不见的测试数据评估这些模型时，它们的性能显着恶化。
为了解决这个问题，我决定探索机器学习模型作为替代方法。令人惊讶的是，随机森林等模型的性能与深度学习模型相当甚至更好，训练和测试数据的准确率均达到 97% 左右。随后，我尝试了其他几种机器学习算法，逻辑回归成为最适合我的特定用例的选择。
尽管有这些发现，我仍然感到困惑，为什么深度学习模型尽管表现出令人印象深刻的训练准确性，却无法很好地泛化到未见过的数据。有人可以阐明这种差异背后的潜在原因吗？是否存在我可能忽略的深度学习特有的常见陷阱或注意事项？任何见解或建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</guid>
      <pubDate>Sat, 23 Mar 2024 19:05:28 GMT</pubDate>
    </item>
    <item>
      <title>无法从“jax”导入名称“linear_util”</title>
      <link>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</link>
      <description><![CDATA[我正在尝试重现S5模型的实验，https://github.com/lindermanlab/ S5，但是在解决环境的时候遇到了一些问题。当我运行 shell 脚本./run_lra_cifar.sh时，出现以下错误
回溯（最近一次调用最后一次）：
  文件“/Path/S5/run_train.py”，第3行，在&lt;module&gt;中。
    从 s5.train 导入火车
  文件“/Path/S5/s5/train.py”，第7行，在&lt;module&gt;中。
    从.train_helpers导入create_train_state，reduce_lr_on_plateau，\
  文件“/Path/train_helpers.py”，第 6 行，在  中。
    从 flax.training 导入 train_state
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py”，第 19 行，在  中
    从 。导入核心
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py”，第 15 行，在  中
    从 .axes_scan 导入广播
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py”，第 22 行，在  中
    从 jax 导入 Linear_util 作为 lu
ImportError：无法从“jax”导入名称“linear_util”（/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py）

我在 RTX4090 上运行它，我的 CUDA 版本是 11.8。我的jax版本是0.4.25，jaxlib版本是0.4.25+cuda11.cudnn86
我首先尝试使用作者的安装依赖项
pip install -rrequirements_gpu.txt

但是，这似乎不适用于我的情况，因为我什至无法导入 jax。所以我根据 https://jax.readthedocs.io/en 上的说明安装了 jax /latest/installation.html
通过输入
pip install --upgrade pip
pip install --upgrade “jax[cuda11_pip]” -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

到目前为止我已经尝试过：

使用较旧的 GPU（3060 和 2070）
将 python 降级到 3.9

有谁知道可能出了什么问题吗？感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</guid>
      <pubDate>Sat, 23 Mar 2024 09:57:12 GMT</pubDate>
    </item>
    <item>
      <title>如何将预训练的拥抱脸模型转换为.pt并在本地完全运行？</title>
      <link>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</link>
      <description><![CDATA[我正在尝试将此模型转换为.pt格式。它对我来说工作得很好，所以我不想对其进行微调。如何将其导出为.pt并运行界面？
我尝试使用它转换为 .pt：
从变压器导入 AutoConfig、AutoProcessor、AutoModelForCTC、AutoTokenizer、Wav2Vec2Processor
导入库
进口火炬



# 定义模型名称
model_name = “UrukHan/wav2vec2-俄罗斯”

# 加载模型和分词器
config = AutoConfig.from_pretrained(model_name)
模型 = AutoModelForCTC.from_pretrained(model_name, config=config)
处理器 = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 将模型保存为.pt 文件
torch.save(model.state_dict(), &quot;model.pt&quot;)

# 如果需要的话也保存分词器
tokenizer.save_pretrained(“模型标记器”)

但不幸的是它没有运行界面：
model = AutoModelForCTC.from_pretrained(“model.pt”)
处理器 = AutoProcessor.from_pretrained(“model.pt”)


# 使用模型进行推理
FILE = &#39;这里是 wav.wav&#39;
音频，_ = librosa.load（文件，sr = 16000）
音频=列表（音频）
def map_to_result(batch):
  使用 torch.no_grad()：
    input_values = torch.tensor(batch, device=“cpu”).unsqueeze(0) #, device=“cuda”
    logits = 模型(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  批处理=处理器.batch_decode(pred_ids)[0]
  退货批次
映射到结果（音频）
打印（映射到结果（音频））


模型.eval()

并遇到错误：
`model.pt 不是本地文件夹，也不是“https://huggingface.co/models”上列出的有效模型标识符
`]]></description>
      <guid>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</guid>
      <pubDate>Sat, 23 Mar 2024 09:18:49 GMT</pubDate>
    </item>
    <item>
      <title>本地机器上的图像分类，但偶数纪元明显快于奇数纪元</title>
      <link>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</guid>
      <pubDate>Sat, 23 Mar 2024 02:41:21 GMT</pubDate>
    </item>
    <item>
      <title>Python：运行保存的 SVM 模型时出错：ValueError：X 有 2943 个功能，但 SVC 期望 330320 个功能作为输入</title>
      <link>https://stackoverflow.com/questions/78207432/python-error-while-running-saved-svm-model-valueerror-x-has-2943-features-bu</link>
      <description><![CDATA[我使用 Sickit-learn 创建了一个 SVM 模型：
导入 pandas 作为 pd
df = pd.read_csv(r&quot;C:\Users\aaa\Documents\bbb\svm_.csv&quot;, 编码=&#39;latin1&#39;, sep=&#39;;&#39;)

从 imblearn.over_sampling 导入 RandomOverSampler
过采样器 = RandomOverSampler(sampling_strategy=&#39;auto&#39;, random_state=42)

X_resampled, y_resampled = oversampler.fit_resample(df.drop(columns=[&#39;alvo&#39;]), df[&#39;alvo&#39;])

df_resampled = pd.concat([X_resampled, pd.DataFrame({&#39;alvo&#39;: y_resampled})], axis=1)

打印（df_重采样）

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC
从sklearn.metrics导入accuracy_score，classification_report
从 sklearn.preprocessing 导入 OneHotEncoder

X = df_resampled.drop(columns=[&#39;alvo&#39;])
y = df_resampled[&#39;alvo&#39;]

编码器 = OneHotEncoder()
X_encoded = 编码器.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=2)

svm_classifier = SVC(kernel=&#39;poly&#39;, random_state=42)

svm_classifier.fit(X_train, y_train)

y_pred = svm_classifier.predict(X_test)

准确度=准确度_得分(y_test, y_pred)
分类报告结果 = 分类报告(y_test, y_pred)

print(f&quot;准确度: {accuracy}&quot;)
print(&quot;分类报告：\n&quot;,classification_report_result)

进口泡菜

model_file_path = “C:\\Users\\aaa\\Documents\\bbb\\svm_modelo_sent_simnao2.pkl”

打开（model_file_path，&#39;wb&#39;）作为f：
    pickle.dump(svm_classifier, f)

print(&quot;模型保存成功！&quot;)


支持向量机的简单实现来预测分类变量是正确的，工作得很好。
但是，当加载模型并运行以下代码时：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 OneHotEncoder
导入作业库

model_file_path = &quot;C:\\Users\\AAA\\Documents\\BBB\\svm_modelo_sent_simnao.pkl&quot;
svm_classifier = joblib.load(model_file_path)

new_df = pd.read_csv(r&quot;C:\Users\AAA\Documents\BBB\svm_simnao_rodar.csv&quot;, 编码=&#39;latin1&#39;, sep=&#39;;&#39;)

X_new = new_df.drop(columns=[&#39;alvo&#39;, &#39;ID_ASSUNTO&#39;], axis=1) # 删除 &#39;alvo&#39; 和 &#39;ID_ASSUNTO&#39; 列

categorical_columns = [&#39;UF&#39;, &#39;TIPO_ACAO&#39;, &#39;AREA_JURIDICA&#39;, &#39;VARA_CAMARA&#39;, &#39;CLIENTE_NOME&#39;] # 分类列列表
编码器= OneHotEncoder（类别=&#39;自动&#39;，稀疏=假）
X_new_encoded = 编码器.fit_transform(X_new[categorical_columns])

X_new_processed = pd.concat([pd.DataFrame(X_new_encoded), X_new.drop(columns=categorical_columns)], axis=1)

y_pred_new = svm_classifier.predict(X_new_processed)

new_df[&#39;alvo&#39;] = y_pred_new

预测 = new_df[[&#39;ID_ASSUNTO&#39;, &#39;alvo&#39;]]

打印（预测）

什么会导致以下错误：
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-522717c33138&gt;在&lt;模块&gt;中
     22
     23 # 做出预测
---&gt; 24 y_pred_new = svm_classifier.predict(X_new_processed)
     25
     26 # 将预测（&#39;alvo&#39;）添加到新数据中

〜\ AppData \ Roaming \ Python \ Python39 \ site-packages \ sklearn \ svm \ _base.py 在预测（自我，X）
    第818章
    第819章：
--&gt;第820章
    第821章
    第822章

〜\ AppData \ Roaming \ Python \ Python39 \ site-packages \ sklearn \ svm \ _base.py 在预测（自我，X）
    第431章 预测值。
    第432章
--&gt;第433章
    第434章
    第435章 回归预测（X）

_validate_for_predict(self, X) 中的 ~\AppData\Roaming\Python\Python39\site-packages\sklearn\svm\_base.py
    611
...
--&gt;第389章
    [第 390 章]
    [391] f“期待{self.n_features_in_}特征作为输入。”

ValueError: X 有 2943 个特征，但 SVC 预计有 330320 个特征作为输入。

&lt;小时/&gt;
我在部署代码中用于预测 alvo 的数据与我训练模型的数据具有完全相同的结构，并且我在训练和部署代码中都使用 OneHotEncoding...所以我有点迷失在这个之中。
知道如何解决这个问题吗？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78207432/python-error-while-running-saved-svm-model-valueerror-x-has-2943-features-bu</guid>
      <pubDate>Fri, 22 Mar 2024 15:57:18 GMT</pubDate>
    </item>
    <item>
      <title>测试精度大于 1，并且一开始就非常高</title>
      <link>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</link>
      <description><![CDATA[问题在于，在测试循环中打印时，正确的样本多于样本总数。训练函数正常计算精度，但测试函数始终以 1.1-1.3 的精度开始。此外，这两种准确性一开始都非常高，然后就会下降。
我使用的是 sst2 数据集，批量大小 = 16，总批量为 55。
这是我的数据准备
def preprocess_function（示例）：
        返回分词器（示例[“句子”]，
                         填充=“最大长度”，
                         截断=真，
                         最大长度=MODEL_MAX_LENGTH）
    
    tokenized_datasets = dataset.map(preprocess_function,batched=True)
    tokenized_datasets = tokenized_datasets.remove_columns([“sentence”, “idx”, “attention_mask”])
    tokenized_datasets = tokenized_datasets.rename_column(“标签”, “标签”)
    tokenized_datasets.set_format(“火炬”)

    train_dataset = tokenized_datasets[“train”].shuffle(seed=SEED)
    valid_dataset = tokenized_datasets[“验证”].shuffle(seed=SEED)

    data_collat​​or = DataCollat​​orForLanguageModeling（
        分词器=分词器，
        传销=真实，
        MLM_概率=0.15
    ）

    class_count = [sum(train_dataset[&#39;labels&#39;] == label) 范围内的标签 (NUM_LABELS)]
    class_weights = 1. / torch.tensor(class_count, dtype=torch.float)

    class_weights_all = class_weights[train_dataset[&#39;labels&#39;]]

    加权采样器 = 加权随机采样器（
        权重=class_weights_all，
        num_samples= len(class_weights_all),
        替换=假
    ）

    train_dataloader = 数据加载器(
        训练数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        采样器=加权采样器，
        pin_memory=真
    ）

    valid_dataloader = 数据加载器(
        有效数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        pin_memory=真
    ）

这是我的训练和测试循环：
def train_loop(模型,
               训练数据加载器，
               优化器，
               lr_调度程序，
               设备）：
    模型.train()
    总损失= 0
    总正确率 = 0
    样本总数 = 0
    计数器 = 0
    对于步骤，批量枚举（tqdm（train_dataloader））：
        如果计数器 &gt;= 100：
            休息

        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        输出=模型（**批次）
        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)
        # print(预测, 预测.size(), len(预测))

        标签=批次[&#39;标签&#39;]
        # print(标签, labels.size(), len(标签))
        正确 = (预测 == 标签).sum().item()
        总正确率 += 正确率
        样本总数 += labels.size(0)

        损失 = 输出.损失
        总损失 += loss.detach()

        # loss.requires_grad = True
        loss.backward()
        优化器.step()
        lr_scheduler.step()
        优化器.zero_grad()

        计数器 += 1

    如果total_samples &gt; 则准确度=total_ Correct /total_samples 0 否则 0
    返回总损失、准确率

def test_loop(模型,
              有效数据加载器，
              设备）：
    模型.eval()
    评估损失 = 0
    总正确率 = 0
    样本总数 = 0
    对于步骤，批量枚举（tqdm（valid_dataloader））：
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        使用 torch.no_grad()：
            输出=模型（**批次）

        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)

        标签=批次[&#39;标签&#39;]
        正确 = (预测 == 标签).sum().item() # 错误行
        总正确率 += 正确率
        样本总数 += labels.size(0)
        print(&quot;正确总数：&quot; + str(total_ Correct) + &quot; ||| 样本总数：&quot; + str(total_samples))

        损失 = 输出.损失
        eval_loss += loss.detach()

    如果total_samples &gt; eval_accuracy =total_ Correct /total_samples 0 否则 0
    返回 eval_loss、eval_accuracy

我想我现在只是没有看到一些东西。我是否使用了相同的变量？]]></description>
      <guid>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</guid>
      <pubDate>Thu, 21 Mar 2024 02:12:15 GMT</pubDate>
    </item>
    <item>
      <title>libmagic 不可用，但有助于对类文件对象进行文件类型检测</title>
      <link>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</guid>
      <pubDate>Tue, 19 Mar 2024 12:21:50 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有七个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数字特征是 [8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9] （我有该图所有点的坐标对 (x, y) 以两列 CSV 文件的形式存在，我也可以使用它们）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
例如，我编写了以下代码：
X = []
y = []
目录=“数据”；
对于 os.listdir（目录）中的 csv_file：
    data = pd.read_csv(f&quot;{目录}/{csv_file}&quot;)
    X.append(data.iloc[1:, :2].astype(float).values)
    y.append(data.iloc[0, 2:].astype(float).values)
X = np.array(X, dtype=np.float64) # X.shape: (50000, 253, 2)
y = np.array(y, dtype=np.float64) # y.shape: (50000, 7)

X_train = X[:40000,:,:]
X_val = X[40000:, :, :]
y_train = y[:40000,:]
y_val = y[40000:, :]

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_val_scaled = 缩放器.fit_transform(X_val)

输入 = keras.layers.Input(shape=(X.shape[1], X.shape[2]))
lstm_out = keras.layers.LSTM(32)(输入)
输出 = keras.layers.Dense(7)(lstm_out)

模型= keras.Model（输入=输入，输出=输出）
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=“mse”)
模型.summary()

历史=模型.fit(
    x=X_train,
    y = y_train，
    纪元=10，
）

损失非常高，而且一点也不好。
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？我明白
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- --------------------------
---&gt; 13 clf.fit(X_train, y_train, **fit_params)

TypeError：OneVsRestClassifier.fit() 得到意外的关键字参数“eval_set”
]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>将自定义 ML 模型与 Service Now 预测智能结合使用</title>
      <link>https://stackoverflow.com/questions/75694868/using-custom-ml-model-with-service-now-predictive-intelligence</link>
      <description><![CDATA[我有一个自定义 ML 模型，希望将其与 ServiceNow 的预测智能集成。
我一直在阅读他们的文档，但只能找到一些 ML API 和类的用法，但没有看到是否有办法将其与自定义 ML 模型集成。
SNOW 提供了一个 ClassificationSolution 类来使用内置模型，但我无法继续了解如何在 Predictive Intelligence 中创建我们自己的自定义模型。
文档
ServiceNow 版本 - 圣地亚哥]]></description>
      <guid>https://stackoverflow.com/questions/75694868/using-custom-ml-model-with-service-now-predictive-intelligence</guid>
      <pubDate>Fri, 10 Mar 2023 10:20:26 GMT</pubDate>
    </item>
    </channel>
</rss>