<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 14 Dec 2024 15:17:11 GMT</lastBuildDate>
    <item>
      <title>如何将 Flatten 层与具有动态尺寸的输入一起使用？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-the-flatten-layer-with-an-input-that-has-a-dynamically-sized-dimensio</link>
      <description><![CDATA[我有一个模型，其输入（一批具有形状（高度、宽度、时间）的图像）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Flatten 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度的输入（无）
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用密集层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：密集层输入的最后一个维度应已定义。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？我需要保留所有像素级信息。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-the-flatten-layer-with-an-input-that-has-a-dynamically-sized-dimensio</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林预测 FPL 球员总得分</title>
      <link>https://stackoverflow.com/questions/79280539/predicting-fpl-player-total-points-using-random-forest</link>
      <description><![CDATA[我有一个数据集，其中包含英超联赛（2016-2023 年）大约 100k 个比赛周统计数据。我的目标是预测一名球员在某个比赛周/比赛中将获得多少总分。
我将数据分为训练/测试集，其中训练集包含赛季 &lt; 2022 的统计数据，测试集包含赛季 &gt; 的统计数据2022.
为了说明某位球员的当前状态，我计算了过去 3 个比赛周以下变量的滚动平均值：
进球数、助攻数、零封数、失球数、分钟数、自进球数、扑救数、错失点球数、黄牌数、红牌数、扑救数、影响力、创造力、威胁和 ict_index
然后，我使用这些变量和一些其他变量运行随机森林：
was_home、player_team、opponent_team、opponent_strength、element_type（后卫/中场等）
模型如下所示：
rf &lt;- randomForest(
as.formula(paste(target, &quot;~&quot;, paste(predictors, collapse = &quot; + &quot;))),
data = train,
ntree = 500,
mtry = 7,
nodesize = 10,
significance = TRUE)

这样做我只得到 R^2 约为 57%。所以我的问题是这是否正常，或者我的方法是否出错？我想知道我可以在哪里改进模型，机器学习是否是预测总分的好方法？]]></description>
      <guid>https://stackoverflow.com/questions/79280539/predicting-fpl-player-total-points-using-random-forest</guid>
      <pubDate>Sat, 14 Dec 2024 11:25:11 GMT</pubDate>
    </item>
    <item>
      <title>文本分类中的挑战：使用 ML.NET 与基于 Python 的模型平衡速度、上下文感知和误报 [关闭]</title>
      <link>https://stackoverflow.com/questions/79280374/challenges-in-text-classification-balancing-speed-context-awareness-and-false</link>
      <description><![CDATA[我需要一些有关机器学习的建议。我目前正在进行一项文本分类任务，其主要目标是训练多个模型来检测特定类型的有害行为（例如，威胁、贬损语言、仇恨言论等）。
为了提供一些背景信息，我之前没有训练 ML 模型的经验。在我的项目中，一切都是基于 .NET 构建的，因此我被建议尝试使用 ML.NET 进行模型训练。它为我带来了几个优势：首先，它与 .NET 架构无缝集成，因为它是同一生态系统的一部分，我不需要做太多更改。其次，使用 ML.NET 训练的模型在推理过程中速度极快。
然而，我很快就遇到了重大挑战：

ML.NET 训练器是浅显的模型，无法理解句子的上下文。例如，我创建了一个包含 200,000 行的数据集来检测威胁，并期望获得不错的结果。不幸的是，在测试过程中，我遇到了许多本不应该出现的误报。

主要问题源于常用的单词和模式。例如，在威胁的情况下，像“我会……”或“我会……”这样的模式经常被标记为威胁。这意味着任何以这种模式开头的句子都会被错误分类，即使它根本没有威胁性。

为了解决这个问题，我开始更严格地平衡数据集。在实践中，这涉及测试模型、识别有问题的误报、了解它们被错误分类的原因，以及创建额外的示例来向模型表明这种模式不一定是威胁。


过了一段时间，我意识到 ML.NET 可能不是这类任务最可靠的解决方案。我开始探索基于 Python 的解决方案，并遇到了基于 BERT 的模型。我使用 RoBERTa-large 微调了自己的模型，但遇到了以下挑战：
不出所料，该模型非常大。
该模型的推理速度明显慢于 ML.NET 模型。
然后我尝试使用 DistilBERT-base，这是一个更小、更快的模型。虽然它确实更快，但我仍然面临很多误报。我怀疑这可能是由于微调设置不正确（因为我缺乏经验）或数据集本身的潜在问题。话虽如此，数据集似乎准备充分，所以我期待更好的结果。
现在，我想征求您的建议：
如果有人有使用 ML.NET 进行文本分类的经验，您是如何获得令人满意的结果的？
如果我决定完全切换到 Python，我最好的选择是什么？我需要一些快速且能够理解句子中上下文的东西，而不是仅仅依赖数据集中经常出现的单词，否则会导致大量的误报。]]></description>
      <guid>https://stackoverflow.com/questions/79280374/challenges-in-text-classification-balancing-speed-context-awareness-and-false</guid>
      <pubDate>Sat, 14 Dec 2024 09:18:34 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 警告未找到可见的 GPU，正在将设备设置为 CPU</title>
      <link>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</link>
      <description><![CDATA[系统信息

XGBoost 版本：2.1.3
NVIDIA 驱动程序版本：565.57.01
CUDA 版本：12.6（来自 nvcc）和 12.7（来自 nvidia-smi）
GPU：Tesla T4
操作系统：Ubuntu 24.04
Python 版本：3.11.10
torch.cuda.is_available()：True

尽管系统显示 CUDA 和 GPU 可用，但我遇到了来自 XGBoost 的以下警告：

XGBoost 警告：/workspace/src/context.cc:43：未找到可见的 GPU，将设备设置为CPU。

请帮我解决这个问题。谢谢！如果您需要任何其他信息，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</guid>
      <pubDate>Sat, 14 Dec 2024 09:12:14 GMT</pubDate>
    </item>
    <item>
      <title>结合 RNN 和 FFN</title>
      <link>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</link>
      <description><![CDATA[在 FFN 中，我们有一些输入和一些输出，并以此为基础训练模型。在 RNN 中，输入是序列的一段，输出是同一序列的下一个时间步。但是，在我的场景中，我将关节旋转作为输入，将顶点位置作为随时间变化的输出。我不知道如何在 RNN 中处理两个不同的序列（关节旋转和顶点位置）。
我有时间依赖性，并且这两个序列也是相互依赖的。我应该结合使用 FFN 和 RNN 来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</guid>
      <pubDate>Sat, 14 Dec 2024 07:42:00 GMT</pubDate>
    </item>
    <item>
      <title>拟合非线性混合模型 [迁移]</title>
      <link>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</link>
      <description><![CDATA[我试图拟合一个非线性混合模型 (nLMM)，以测试某些生物的丰度是否受到导致丰度显著增加的事件后的采样期的影响。
数据显示了一条重要的曲线，这些生物的丰度在事件发生后激增（事件发生在采样期：-1 和 1 之间），但随后下降。
我试图构建一个非线性混合模型，但我发现理解如何构建模型非常具有挑战性（例如，model &lt;- lmer(abundance ~ samples_period + (1 | rep), data = data）。我非常感谢任何帮助来确定丰度是否受到采样期的影响。
data &lt;- data.frame(
abundant = c(79, 72, 58, 61, 88, 123, 119, 96, 67, 78, 143, 75, 105, 46, 58, 
127, 173, 181, 67, 120, 64, 30, 49, 47, 104, 83, 146, 118, 53, 
98, 223, 257, 255, 292, 354, 133, 129, 140, 27, 55, 68, 148, 
122, 132, 77, 121, 108, 109),
rep = c(&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, &quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, “T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T3”、“T1”、“T2”、“T3”、“
“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“ “T3”，“T1”，“T2”，“T3”，
“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”），
sampling_period_consecutive = c(1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 
6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 
10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 14, 
14, 14, 15, 15, 15, 16, 16, 16),
采样周期 = c(-5, -5, -5, -4, -4, -4, -3, -3, -3, -2, -2, -2, -1, -1, 
-1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 
6, 11, 11, 11, 22, 22, 22, 34, 34, 34, 46, 46, 58, 
58, 58)
)

]]></description>
      <guid>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</guid>
      <pubDate>Fri, 13 Dec 2024 19:32:50 GMT</pubDate>
    </item>
    <item>
      <title>pytorch CNN 是否关心图像大小？</title>
      <link>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</link>
      <description><![CDATA[我最近在玩 CNN，我有如下粘贴的代码。我的问题是，这适用于任何图像大小吗？我不清楚哪个参数或通道（如果有的话）关心图像大小？如果是这样的话，模型如何知道它需要多少个神经元，这不是图像大小的函数吗？
关于预训练模型的相关点 - 如果我使用预训练模型，我是否需要重新格式化我的图像以使其与模型最初训练的图像相同，或者它如何工作？
class CNN(nn.Module):
def __init__(self, num_classes, num_channels=1):
super(CNN, self).__init__()
self.num_classes = num_classes
self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
self.relu1 = nn.ReLU()
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
self.relu2 = nn.ReLU()
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
self.fc = nn.Linear(64*7*7, num_classes)
]]></description>
      <guid>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</guid>
      <pubDate>Fri, 13 Dec 2024 17:24:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 CPU 上使用 llama_cpp 运行 LLaMA 13B 模型会花费过多时间并且产生较差的输出？</title>
      <link>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</guid>
      <pubDate>Fri, 13 Dec 2024 16:45:38 GMT</pubDate>
    </item>
    <item>
      <title>将加速光线行进应用于 NeRF 实现</title>
      <link>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</guid>
      <pubDate>Fri, 13 Dec 2024 15:46:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练损失和测试损失之间有这么大的差异？[关闭]</title>
      <link>https://stackoverflow.com/questions/79278629/why-is-there-such-a-big-difference-between-train-and-test-loss</link>
      <description><![CDATA[当前设备：cuda
100%|██████████| 1/1 [03:08&lt;00:00, 188.25s/it]
Epoch：1 | train_loss：333.7638 | train_acc：0.1961 | test_loss：2.1727 | test_acc：0.2039 | 

我正在训练图像分类模型 (AlexNet)。我该怎么做才能改变这种情况，或者这是正常的吗？
我尝试更改转换中的某些内容，但根本没有用]]></description>
      <guid>https://stackoverflow.com/questions/79278629/why-is-there-such-a-big-difference-between-train-and-test-loss</guid>
      <pubDate>Fri, 13 Dec 2024 14:21:16 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>“发现输入变量的样本数量不一致”我在 train_test_split 过程中做错了什么吗？</title>
      <link>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</link>
      <description><![CDATA[我正在尝试逻辑回归模型，并运行一些测试，但我一直收到此错误。不太确定我和其他人做了什么不同的事情
from sklearn import preprocessing
X = df.iloc[:,:len(df.columns)-1]
y = df.iloc[:,len(df.columns)-1]ere

这是我分离列的方式
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

TTS
logReg = LogisticRegression(n_jobs=-1)
logReg.fit(X_train, y_train)

y_pred = logReg.predict(X_train)

mae = mean_absolute_error(y_test, y_pred)
print(&quot;MAE:&quot; , mae)

ValueError Traceback（最近一次调用最后一次）
Cell In [112]，第 1 行
----&gt; 1 mae = mean_absolute_error(y_test, y_pred)
2 print(&quot;MAE:&quot; , mae)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:196, in mean_absolute_error(y_true, y_pred, sample_weight, multioutput)
141 def mean_absolute_error(
142 y_true, y_pred, *, sample_weight=None, multioutput=&quot;uniform_average&quot;
143 ):
144 &quot;&quot;&quot;平均绝对误差回归损失。
145 
146 更多信息请阅读 :ref:`用户指南 &lt;mean_absolute_error&gt;`。
(...)
194 0.85...
195 “” “”
--&gt; 196 y_type, y_true, y_pred, multioutput = _check_reg_targets(
197 y_true, y_pred, multioutput
198 )
199 check_consistent_length(y_true, y_pred, sample_weight)
200 output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:100，位于 _check_reg_targets(y_true, y_pred, multioutput, dtype)
66 def _check_reg_targets(y_true, y_pred, multioutput, dtype=&quot;numeric&quot;):
67 &quot;&quot;&quot;检查 y_true 和 y_pred 是否属于同一回归任务。
68 
69 参数
(...)
98 正确的关键字。
99 &quot;&quot;&quot;
--&gt; 100 check_consistent_length(y_true, y_pred)
101 y_true = check_array(y_true, Ensure_2d=False, dtype=dtype)
102 y_pred = check_array(y_pred, Ensure_2d=False, dtype=dtype)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py:387，在 check_consistent_length(*arrays) 中
385 uniques = np.unique(lengths)
386 if len(uniques) &gt; 1:
--&gt; 387 raise ValueError(
388 &quot;找到样本数量不一致的输入变量：%r&quot;
389 % [int(l) for l in lengths]
390 )

ValueError: 找到样本数量不一致的输入变量：[25404, 101612]

我以为是我拆分列的方式不对，但这似乎不是问题所在
当测试规模为 50/50 时，它可以工作，但其他测试规模都不起作用]]></description>
      <guid>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</guid>
      <pubDate>Wed, 11 Jan 2023 15:15:49 GMT</pubDate>
    </item>
    <item>
      <title>从头开始训练的 Keras Xception 在历史记录中给出了约 100% 的准确率，但在评估时仅预测 1，准确率为 50%</title>
      <link>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</link>
      <description><![CDATA[我在 keras 上训练 Xception 模型，没有使用预训练权重来解决二元分类问题，结果出现了非常奇怪的行为。历史图显示训练准确率不断增加，直到达到 100%，而验证准确率始终在 50% 左右，因此看起来是过度拟合，但事实并非如此，因为我检查过，即使在训练集上，它也总是预测（接近）1。
这种行为的原因可能是什么？
这是我用来训练的代码。 x_train_xception 已由 keras.applications.xception.preprocess_input 函数预处理。
我使用相同的代码（除了模型创建之外）来训练预训练的 Xception 模型，效果很好
inLayerX = Input((512, 512, 4))
xceptionModel = keras.applications.Xception(include_top = True, weights=None, input_tensor=inLayerX, classes=1, classifier_activation= &#39;sigmoid&#39;)

xceptionModel.compile(loss= &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

history = xceptionModel.fit(x_train_xception, y_train, batch_size= batch_size, epochs= epochs, validation_data=(x_val_xception, y_val))

_, accTest = xceptionModel.evaluate(x_test_xception, y_test)
_, accVal = xceptionModel.evaluate(x_val_xception, y_val)
_, accTrain = xceptionModel.evaluate(x_train_xception, y_train)
print(&quot;训练准确率 {:.2%}&quot;.format(accTrain))
print(&quot;验证准确率 {:.2%}&quot;.format(accVal))
print(&quot;测试准确率 {:.2%}&quot;.format(accTest))

输出：
2/2 [================================] - 6s 2s/step - 损失： 1.2063 - 准确率：0.5000
1/1 [==============================] - 4s 4s/步 - 损失：1.2960 - 准确率：0.4667
4/4 [================================] - 5s 1s/步 - 损失：1.2025 - 准确率：0.5083
训练准确率 50.83%
验证准确率 46.67%
测试准确率 50.00%

验证和测试准确率在预期之内，但真正困扰我的是训练准确率，从历史记录来看，我预计训练准确率接近 100%。
模型准确率
模型损失]]></description>
      <guid>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</guid>
      <pubDate>Sun, 10 Jul 2022 17:58:52 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 中的特征选择</title>
      <link>https://stackoverflow.com/questions/53528481/feature-selection-in-pyspark</link>
      <description><![CDATA[我正在研究一个形状为 1,456,354 X 53 的机器学习模型。我想对我的数据集进行特征选择。我知道如何使用以下代码在 python 中进行特征选择。
from sklearn.feature_selection import RFECV,RFE

logreg = LogisticRegression()
rfe = RFE(logreg, step=1, n_features_to_select=28)
rfe = rfe.fit(df.values,arrythmia.values)
features_bool = np.array(rfe.support_)
features = np.array(df.columns)
result = features[features_bool]
print(result)

但是，我找不到任何文章可以展示如何在 pyspark 中执行递归特征选择。 
我尝试在 pyspark 中导入 sklearn 库，但它给出了一个错误 sklearn 模块未找到。我在 google dataproc 集群上运行 pyspark。
有人能帮我在 pyspark 中实现这个吗]]></description>
      <guid>https://stackoverflow.com/questions/53528481/feature-selection-in-pyspark</guid>
      <pubDate>Wed, 28 Nov 2018 21:36:15 GMT</pubDate>
    </item>
    <item>
      <title>根据传感器数据进行步态/行走分析</title>
      <link>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</link>
      <description><![CDATA[我组装了一块地毯，里面有 8 个压力传感器。您可以在图片中看到传感器的排列。整个地毯为 80x80 厘米。每个传感器在被按下时都会输出数字信号（0 或 1）。微控制器每 100 毫秒读取一次所有传感器，并输出一个有效载荷字节，其中每个位包含一个三角形的信息。我将所有这些字节存储在一个 100 字节长的数组中。
我需要从这个数组计算步态（方向，用户前进的角度）。用户只是在原地行进，双脚交替抬起和放下。你知道我可以用来做这种分析的任何算法吗？我应该使用机器学习/神经网络吗？语言并不重要，我只需要找出分析这个字节数组的正确方法。谢谢！
]]></description>
      <guid>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</guid>
      <pubDate>Tue, 27 Sep 2016 19:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>