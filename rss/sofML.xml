<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 21 Mar 2024 21:13:49 GMT</lastBuildDate>
    <item>
      <title>轨迹和位置算法[关闭]</title>
      <link>https://stackoverflow.com/questions/78202619/algorithm-for-trajectory-and-position</link>
      <description><![CDATA[我正在尝试创建一个项目，计划使用某种类型的相机拍摄地面的高 fps (100-200) 照片，然后将这些照片相互比较，以提取运动信息，例如x,y 坐标、速度和距离。
我正在考虑光流，但是还有其他方法可以解决这个问题吗？
速度永远不会超过 5m/s，但精度要求很高。]]></description>
      <guid>https://stackoverflow.com/questions/78202619/algorithm-for-trajectory-and-position</guid>
      <pubDate>Thu, 21 Mar 2024 20:26:00 GMT</pubDate>
    </item>
    <item>
      <title>输入形状如何改变模型架构？</title>
      <link>https://stackoverflow.com/questions/78202453/how-does-the-input-shape-change-model-architecture</link>
      <description><![CDATA[在以下两种情况下我得到不同的结果：
示例 1：训练数据具有形状（batch_size，n_steps），模型为：
model_dense = tf.keras.Sequential([
   tf.keras.layers.Dense(1)
]）

示例 2：我的训练数据具有形状 (batch_size, n_steps, 1)，模型具有形状
model_dense = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[n_steps, 1]),
    tf.keras.layers.Dense(1)
]）


示例 2 的训练效果要好得多。两个模型都有 n_steps+1 个可训练参数，我认为示例 2 的展平层只会展平通道维度，因此使其等同于示例 1。我认为我错过了一些简单的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78202453/how-does-the-input-shape-change-model-architecture</guid>
      <pubDate>Thu, 21 Mar 2024 19:43:55 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Ridge 和 Lasso 回归处理数据集中潜在的多重共线性？</title>
      <link>https://stackoverflow.com/questions/78202221/how-to-handle-potential-multicollinearity-in-a-dataset-using-ridge-and-lasso-reg</link>
      <description><![CDATA[包含各种房屋信息的数据集，包括其大小、卧室数量、浴室数量、年龄和相应的销售价格。目标是建立一个线性回归模型，可以根据这些自变量准确预测房屋的销售价格，同时考虑数据中潜在的多重共线性。
数据集采用以下格式：
&lt;前&gt;&lt;代码&gt;house_data.csv
面积、卧室、浴室、年龄、价格
2500,4,3,25,550000
3000,3,2,15,625000
...

导入 pandas 作为 pd
从 sklearn. Linear_model 导入 LinearRegression、Ridge、Lasso
从 sklearn.model_selection 导入 train_test_split

# 加载数据集
house_data = pd.read_csv(&#39;house_data.csv&#39;)
X = house_data[[&#39;尺寸&#39;, &#39;卧室&#39;, &#39;浴室&#39;, &#39;年龄&#39;]]
y = house_data[&#39;价格&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准线性回归
Linear_reg = 线性回归()
Linear_reg.fit(X_train, y_train)
Linear_score = Linear_reg.score(X_test, y_test)
print(f&#39;标准线性回归分数：{linear_score}&#39;)

这就是我陷入困境的地方：如何确定岭回归中正则化参数（alpha）的最佳值？
如何在处理潜在的多重共线性的同时有效地实现 Lasso 回归？
虽然上面的代码适用于标准线性回归，但我正在努力解决以下问题：

如何确定岭回归中正则化参数 (alpha) 的最佳值？
如何在处理数据集中的多重共线性的同时有效实施 Lasso 回归？
]]></description>
      <guid>https://stackoverflow.com/questions/78202221/how-to-handle-potential-multicollinearity-in-a-dataset-using-ridge-and-lasso-reg</guid>
      <pubDate>Thu, 21 Mar 2024 18:51:40 GMT</pubDate>
    </item>
    <item>
      <title>Seq-to-seq LSTM 无法正确学习</title>
      <link>https://stackoverflow.com/questions/78201576/seq-to-seq-lstm-not-learning-properly</link>
      <description><![CDATA[我正在尝试使用 Pytorch 中的 LSTM 解决 seq-to-seq 问题。具体来说，我采用 5 个元素的序列来预测接下来的 5 个元素。我关心的是数据转换。我有大小为 [bs, seq_length, features] 的张量，其中 seq_length = 5 和 features = 1。每个特征都是一个介于 0 和 3 之间的整数（两者都包含在内）。
我认为输入数据必须使用 MinMaxScaler 转换为浮点范围 [0, 1]，以便使 LSTM 学习过程更容易。之后，我应用一个线性层，它将隐藏状态转换为相应的输出，其大小为特征。我在 Pytorch 中对 LSTM 网络的定义：
类 LSTM(nn.Module):
    def __init__(自身、input_dim、hidden_​​dim、output_dim、num_layers、dropout_prob):
        super(LSTM, self).__init__()
        self.lstm_layer = nn.LSTM(input_dim,hidden_​​dim,num_layers,dropout=dropout_prob)
        self.output_layer = nn.Linear（hidden_​​dim，output_dim）

    ...

    def 向前（自身，X）：
        out, (隐藏, 单元格) = self.lstm_layer(X)
        输出 = self.output_layer(输出)
        返回

我用来进行训练循环的代码如下：
def train_loop(t, checkpoint_epoch, dataloader, model, loss_fn, optimizationr):
    大小 = len(dataloader.dataset)
    对于批处理，枚举中的 X（数据加载器）：
        X = X[0].type(torch.float).to(设备)

        # X = torch.Size([batch_size, 10, input_dim])
        # 将序列拆分为输入和目标
        输入 = 变换(X[:, :5, :]) # 输入 = [batch_size, 5, input_dim]
        目标 = 变换(X[:, 5:, :]) # 目标 = [batch_size, 5, input_dim]

        # 预测（前向传递）
        使用自动转换（）：
            pred = 模型(输入) # pred = [batch_size, 5, input_dim]
            损失 = loss_fn(pred, 目标)

        # 反向传播
        优化器.zero_grad()
        scaler.scale(loss).backward()
        缩放器.step（优化器）
        定标器.update()

        如果批次 % 100 == 0:
            损失，当前 = loss.item(), 批次 * len(X)
            #print(f&quot;当前损耗:{loss:&gt;7f},[{current:&gt;5d}/{size:&gt;5d}]&quot;)

        # 删除变量并清空缓存
        del X、输入、目标、预测
        torch.cuda.empty_cache()

    回波损耗

我用于预处理数据的代码：
def main():
    代理数量 = 2
    # 打开HDF5文件
    将 h5py.File(&#39;dataset_&#39; + str(num_agents) + &#39;UAV.hdf5&#39;, &#39;r&#39;) 作为 f：
        # 访问数据集
        数据 = f[&#39;数据&#39;][:]
        # 转换为 PyTorch 张量
        data_tensor = torch.tensor(数据)

        大小 = data_tensor.size()
        序列长度 = 10
        重塑 = data_tensor.view(-1, 大小[2], 大小[3])

        r_size = reshape.size()
        重塑 = 重塑[:, :, 1:]
        reshape_v2 = reshape.view(r_size[0], -1)

        数据集 = create_dataset(reshape_v2.numpy(), seq_length)

        f.close()

    数据集 = TensorDataset(数据集)

    # 将数据集分为训练集和验证集
    train_size = int(0.8 * len(dataset)) # 80% 用于训练
    val_size = len(dataset) - train_size # 20% 用于验证
    train_dataset, val_dataset = random_split(数据集, [train_size, val_size])

    train_dataloader = DataLoader（train_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = True，pin_memory = True）
    val_dataloader = DataLoader（val_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = False，pin_memory = True）

尝试这个，模型没有正确学习，所以我想也许可以直接计算 targets （范围 [0, 1] 内的浮点值）和 pred 之间的损失code&gt; （我认为由于 LSTM 层的 tanh 激活函数，浮点值在 [-1, 1] 范围内），具有不同的尺度可能是错误的。然后，我尝试在前向传递中的线性层之后应用 sigmoid 激活函数，但也没有正确学习。我尝试了许多超参数组合的执行，但没有一个产生“正常”的结果。训练曲线。我还附上了 5000 epoch 的屏幕截图来说明训练过程：

我的问题是：

我的训练过程中似乎存在什么问题？
我所说的内容是否被认为是错误的？
]]></description>
      <guid>https://stackoverflow.com/questions/78201576/seq-to-seq-lstm-not-learning-properly</guid>
      <pubDate>Thu, 21 Mar 2024 16:43:58 GMT</pubDate>
    </item>
    <item>
      <title>如何以我的 k 折叠分割方式适应 lgb.cv？</title>
      <link>https://stackoverflow.com/questions/78201471/how-to-adapt-lgb-cv-in-my-k-folds-splitting-way</link>
      <description><![CDATA[我设计了一个方法，将数据分成5折，然后我想用它来执行5折交叉验证。
来自 load_data 导入 load_data
折叠、test_samples、input_shape = load_data()

folds[0].keys()
# dict_keys([&#39;训练&#39;, &#39;val&#39;, &#39;测试&#39;])

要使用特定的 5 倍来优化 GBM 模型，对于任何优化方法（例如 RandomSearch、GridSearch...），我需要为每个超参数配置训练 5 个模型，然后评估模型性能。
一种方法是，我使用迭代每次折叠来训练模型
early_stopping = lgb.early_stopping(stopping_rounds=10)
模型 = lgb.LGBMClassifier()
model.fit(X, y, 回调=[early_stopping],...)

我发现它的另一种方式是 lgb.cv，它不允许我的折叠 适合。
有人知道如何在不使用分割的情况下实现 lgb.cv 吗？
这是 1 个配置的片段代码
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.metrics 导入 precision_score
from timeit import default_timer 作为计时器

对于 i，折叠枚举（折叠）：
    print(&#39;折叠&#39;, i+1)
    训练，验证，测试=折叠[折叠].values()
    Early_stopping = lgb.early_stopping(stopping_rounds=10)
    模型 = lgb.LGBMClassifier()
    
    开始=定时器()
    model.fit(训练[&#39;x&#39;], 训练[&#39;y&#39;],
              回调=[early_stopping],
              评估集=[
                  (火车[&#39;x&#39;], 火车[&#39;y&#39;]),
                  (val[&#39;x&#39;], val[&#39;y&#39;]),
                  （测试[&#39;x&#39;]，测试[&#39;y&#39;]）]，
              eval_names=[&#39;训练&#39;, &#39;val&#39;, &#39;测试&#39;],
              eval_metric=[&#39;auc&#39;, &#39;binary_logloss&#39;],
              功能名称=功能名称）
    train_time = 计时器() - 开始

    ＃ 作出预测
    预测 = model.predict_proba(val[&#39;x&#39;])
    auc = roc_auc_score(val[&#39;y&#39;], 预测[:, 1])
    acc = precision_score(val[&#39;y&#39;], np.argmax(预测, axis=1))
    
    print(&#39;验证集上的验证准确度为{:.4f}.&#39;.format(acc))
    print(&#39;验证集上的验证auc为{:.4f}.&#39;.format(auc))
    print(&#39;训练时间为{:.4f}秒&#39;.format(train_time))

我如何以优化的方式调整它（例如，RandomSearch）？]]></description>
      <guid>https://stackoverflow.com/questions/78201471/how-to-adapt-lgb-cv-in-my-k-folds-splitting-way</guid>
      <pubDate>Thu, 21 Mar 2024 16:26:34 GMT</pubDate>
    </item>
    <item>
      <title>找到每个类别的图像原型？</title>
      <link>https://stackoverflow.com/questions/78200700/find-the-image-prototypes-for-each-class</link>
      <description><![CDATA[我尝试实现一种方法，该方法采用所有图像特征和相应的标签来为每个类生成图像原型。我在网上搜索过，但找不到任何教程或可用代码来验证我所做的是否正确。
此外，我尝试计算加权图像原型，其中每个图像都有自己的权重。但是，我仍然不确定这种方法是否准确。
labels = torch.arange(num_classes)
weights = torch.tesnor([......]) ## 大小 N 的权重 = num_images
类均值 = []
对于可用标签中的 i：
    idx = (伪标签 == i)
    样本计数 = idx.float().sum().item()
    如果样本数&gt; 0.0：
        壮举=特征[idx]
        class_emebdding = torch.sum(feat*weights[idx],dim=0)
        class_emebdding /= class_emebdding.norm()
原型 = torch.stack(class_means, 0)
]]></description>
      <guid>https://stackoverflow.com/questions/78200700/find-the-image-prototypes-for-each-class</guid>
      <pubDate>Thu, 21 Mar 2024 14:31:36 GMT</pubDate>
    </item>
    <item>
      <title>在基于品种的作物产量预测模型中查找每个品种的准确性</title>
      <link>https://stackoverflow.com/questions/78199996/finding-the-accuracy-for-each-variety-in-a-variety-based-crop-yield-prediction-m</link>
      <description><![CDATA[我一直在使用回归研究田间作物的产量预测模型。我的输入特征包括 30 多个特定于作物的变量，这些变量是我使用 Google Earth Engine 针对每个由单个多边形标记的田地得出的。我还通过调查了解了每块田地种植的农作物的品种（具体是两种类型）。我想了解每个品种的模型准确性如何。我们以后如何确定模型对每个品种的准确性？
品种 1 - 有 100 个样品
品种 2 - 有 60 个样品
我正在考虑做这样的事情：

如果我遵循 70-30% 的分割，我就有 48 个测试样本。根据每个样本绘制预测产量。
根据多样性将样本分为几类。
通过找出误差差异来计算每个类别的 RMSE/MAE。

我不太确定这种方法。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78199996/finding-the-accuracy-for-each-variety-in-a-variety-based-crop-yield-prediction-m</guid>
      <pubDate>Thu, 21 Mar 2024 12:40:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 Tensorflow 时 Python 产生的结果比 kotlin 更准确？</title>
      <link>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</link>
      <description><![CDATA[我正在制作一个应用程序，它将检测不同数字系统中不同的手写数学表达式。截至目前，阻碍任何进展的主要因素是 kotlin 在使用 Tensorflow lite 时产生的不准确性 - 大约 10% 正确。我的 Python 代码非常相似，但它使用常规张量流，并且更加准确 - 大约 70% 正确。
我的想法是图像从 OpenCV Mat 转换为 Tensorbbuffer 的方式导致了一些问题，或者预处理的处理方式导致了差异。
我的代码片段如下：

提取边界矩形后，进行预处理和标准化。

val image_roi = Mat(tmp,boundRect)
Imgproc.cvtColor(image_roi, image_roi, Imgproc.COLOR_RGB2GRAY) Imgproc.GaussianBlur(image_roi, image_roi, Size(3.0,3.0), 0.0)
Imgproc.dilate(image_roi, image_roi, Imgproc.getStructuringElement(Imgproc.MORPH_RECT, Size(4.0, 4.0)))
Imgproc.threshold(image_roi, image_roi, 90.0, 255.0, Imgproc.THRESH_BINARY);
Imgproc.resize(image_roi, image_roi, 大小(28.0,28.0))
Core.normalize(image_roi, image_roi, 0.0, 255.0, Core.NORM_MINMAX);
image_roi.convertTo(image_roi, CvType.CV_8UC1)
提取.add(image_roi)


运行预测，将 OpenCV Mat 转换为 Tensorbuffer（第 3 步）

for（提取的img）{
       val 张量缓冲区 = extractBytes(img)
       val 输出 = model.process(tensorBuffer)
       valoutputFeature0=outputs.outputFeature0AsTensorBuffer
       valconf=outputFeature0.floatArray
       out += getLanguageText(conf, 数字)
 
}


将 Mat 转换为 Tensorbbuffer

私有乐趣 extractBytes(img: Mat): TensorBuffer{
        val inputFeature = TensorBuffer.createFixedSize(intArrayOf(1, 28, 28, 1), DataType.FLOAT32)
        val byteBuffer = ByteBuffer.allocateDirect(28 * 28 * 4) // 每个浮点数 4 个字节
        byteBuffer.order(ByteOrder.nativeOrder())
        byteBuffer.rewind()
 
        for (i 从 0 到 28) {
            for (j in 0 到 28) {
                val temp = img.get(i, j)[0].toFloat() // 假设单通道（灰色）
                byteBuffer.putFloat(临时)
            }
        }
 
        inputFeature.loadBuffer(byteBuffer)
        返回输入特征
    }

在下面的粘贴箱中，我也包含了我的 pythin 代码。我需要一些帮助来弄清楚为什么我的模型无法通过 Kotlin 准确预测，但可以通过 Python 准确预测。
https://pastebin.com/BACzTkq6
以下是在 Python 和 Kotlin 中使用相同图像的差异示例：
通过 Kotlin 显示预测的图像
通过 python 显示预测的图像
我尝试了将 Matrix 转换为 Tensorbuffer 的不同方法，我尝试删除大部分（如果不是全部）图像预处理，我尝试让 python 在 Android studio 中工作（但这并没有成功。）]]></description>
      <guid>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</guid>
      <pubDate>Thu, 21 Mar 2024 11:18:54 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 矩阵乘法形状错误：“RuntimeError：mat1 和 mat2 形状无法相乘”</title>
      <link>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</link>
      <description><![CDATA[我是 PyTorch 的新手，正在创建一个多输出线性回归模型，根据字母为单词着色。 （这将帮助有字素颜色联觉的人更轻松地阅读。）它接收单词并输出 RGB 值。每个单词都表示为 45 个浮点数 [0,1] 的向量，其中 (0, 1] 代表字母，0 代表该位置不存在字母。每个样本的输出应该是一个向量 [r-value, g -值，b-值]。
我懂了
&lt;块引用&gt;
运行时错误：mat1 和 mat2 形状无法相乘（90x1 和 45x3）

当我尝试在训练循环中运行我的模型时。
查看现有的 Stack Overflow 帖子，我认为这意味着我需要重塑我的数据，但我不知道如何/在哪里以解决此问题的方式进行此操作。特别是考虑到我不知道那个 90x1 矩阵来自哪里。
我的模型
我一开始很简单；在我可以让单个层发挥作用之后，可以出现多个层。
类 ColorPredictor(torch.nn.Module):
    #构造函数
    def __init__(自身):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #编码词向量的长度 &amp; r,g,b 向量的大小
        
    ＃ 预言
    defforward(self, x: torch.Tensor) -&gt;;火炬.张量：
        y_pred = self.线性(x)
        返回 y_pred

我如何加载数据
# 数据集类
数据类（数据集）：
    # 构造函数
    def __init__(自身，输入，输出)：
        self.x = input # 编码词向量列表
        self.y = 输出 # 将 r、g、b 值转换为火炬张量的 Pandas 数据帧
        self.len = len(输入)
    
    # 吸气剂
    def __getitem__(自身，索引)：
        返回 self.x[索引], self.y[索引]
    
    # 获取样本数
    def __len__(自身):
        返回 self.len

# 创建训练/测试分割
train_size = int(0.8 * len(数据))
train_data = 数据(输入[:train_size], 输出[:train_size])
test_data = 数据(输入[train_size:], 输出[train_size:])

# 为训练和测试集创建 DataLoaders
train_loader = DataLoader（数据集= train_data，batch_size = 2）
test_loader = DataLoader（数据集= test_data，batch_size = 2）

发生错误的测试循环
对于范围内的纪元（纪元）：
    ＃ 火车
    model.train() #训练模式
    对于 train_loader 中的 x,y：
        y_pred = model(x) #此处错误
        损失=标准(y_pred, y)
        优化器.zero_grad()
        loss.backward()
        优化器.step()
      

错误回溯


新尝试：
将 45x1 输入张量更改为 2x45 输入张量，第二列全为零。这适用于第一次运行 train_loader 循环，但在第二次运行 train_loader 循环期间，我得到另一个矩阵乘法错误，这次是大小为 90x2 和 45x3 的矩阵。]]></description>
      <guid>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</guid>
      <pubDate>Thu, 21 Mar 2024 01:00:23 GMT</pubDate>
    </item>
    <item>
      <title>层顺序从未被调用，因此没有定义的输入</title>
      <link>https://stackoverflow.com/questions/78196623/the-layer-sequential-has-never-been-called-and-thus-has-no-defined-input</link>
      <description><![CDATA[我正在 Anaconda 虚拟环境中运行一个简单的脚本
从 deepface 导入 DeepFace

face_analysis = DeepFace.analyze(img_path = “face3.jpeg”)
打印（面部分析）

但我不断收到此错误。
行动：年龄：25%|██████████████████████████▊ | 1/4 [00:02&lt;00:06, 2.08s/it]
回溯（最近一次调用最后一次）：
  文件“C:\Users\Ctrend.pk\Cheer-Check\test2.py”，第 9 行，在  中
    分析 = DeepFace.analyze(img_path)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\DeepFace.py”，第 222 行，在分析中
    返回人口统计分析（
           ^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\demography.py”，第 157 行，位于分析
    表观年龄 = modeling.build_model(“年龄”).predict(img_content)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\modeling.py”，第 57 行，位于构建模型
    model_obj[模型名称] = model()
                            ^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py”，第 32 行，位于__在里面__
    self.model = load_model()
                 ^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py”，第 61 行，位于加载模型
    年龄模型=模型（输入=模型.输入，输出=基本模型输出）
                             ^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py”，第 228 行，在输入中
    返回 self._get_node_attribute_at_index(0, “input_tensors”, “input”)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py”，第 259 行，在 _get_node_attribute_at_index 中
    引发值错误（
ValueError：层equential_1从未被调用，因此没有定义的输入。

Deepface版本：0.0.87
张量流
版本：2.16.1
我认为它获取了年龄，但随后没有继续。我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78196623/the-layer-sequential-has-never-been-called-and-thus-has-no-defined-input</guid>
      <pubDate>Wed, 20 Mar 2024 22:50:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入（形状=（max_seq_length，），dtype=tf.int32，名称=“attention_mask”）
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别GPU。
该代码几周前就可以工作，有人有解决方案吗？
编辑：Kaggle 上也出现同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>我的 python DBSCAN 工作流程是否可以正确识别具有相似用户评分和流派概况的用户？生成水平状图</title>
      <link>https://stackoverflow.com/questions/77527379/is-my-python-dbscan-workflow-correct-for-identifying-users-that-have-similar-use</link>
      <description><![CDATA[水平图
导入请求
将 pandas 导入为 pd
从 sklearn.cluster 导入 DBSCAN
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.pipeline 导入管道
从 sklearn.decomposition 导入 PCA
将 matplotlib.pyplot 导入为 plt

# 数据帧结构

|标题 |用户评分 |类型 |

# One-Hot 编码 Genre 列（有很多流派）

Anime_dataframe_encoded = pd.get_dummies(anime_dataframe_separated, columns=[&#39;流派&#39;], prefix=&#39;流派&#39;)

Anime_dataframe_encoded = Anime_dataframe_encoded.groupby([&#39;标题&#39;,&#39;分数&#39;]).sum().reset_index()
Anime_dataframe_features = Anime_dataframe_encoded.drop(&#39;标题&#39;, axis=1)


定标器=标准定标器()
Anime_dataframe_scaled = 缩放器.fit_transform(anime_dataframe_features)

pca = PCA(n_components=1)
reduce_features = pca.fit_transform(anime_dataframe_scaled)

dbscan = DBSCAN(eps=0.5, min_samples=5)
标签 = dbscan.fit_predict(reduced_features)

# 可视化结果
plt.figure(figsize=(8, 8))
plt.scatter(reduced_features[:, 0], 标签, c=标签, cmap=&#39;viridis&#39;, s=50)
plt.title(&#39;DBSCAN 聚类结果&#39;)
plt.xlabel(&#39;主成分 1&#39;)
plt.ylabel(&#39;簇标签&#39;)
plt.show()



我只有 1 个用户的列表。但这是正确的前进道路吗？我在 DBSCAN 中看到的图像是甜甜圈形状，这很可能是由于大量数据（我需要添加更多用户列表）。但是，我不确定我所做的是否正确，因为我是初学者。]]></description>
      <guid>https://stackoverflow.com/questions/77527379/is-my-python-dbscan-workflow-correct-for-identifying-users-that-have-similar-use</guid>
      <pubDate>Wed, 22 Nov 2023 04:45:38 GMT</pubDate>
    </item>
    <item>
      <title>一种计算给定数据集中给定属性的循环数的方法</title>
      <link>https://stackoverflow.com/questions/77524730/a-way-to-count-the-number-of-cycles-of-an-given-attribute-in-a-given-dataset</link>
      <description><![CDATA[我想找到给定的电机故障电流数据集中的周期数，它是由在不同时间测量的电机电流给出的，我想知道它是否可以被视为一个信号或者它只是一个信号模式？
电机故障电流
我尝试了一些库，但它们适用于不同的条件，例如它们对所使用的信号有零交叉点，并且我找不到可以为此变成零交叉的点，因为电机电流始终为+ve并且因此它只是随着时间的推移而稍微波动，因此我得到了一个情节
data = pd.read_csv(“healthy.csv”)
y = np.array(data.Current_A)
x = 数据.索引
date_array = pd.array(data.TimeStamp)
plt.plot(日期数组,y)

我在这里使用了 3ph 健康电机电流数据集，有一件事，该数据集有空格作为“”当前-A” ，应固定为“Current_A”]]></description>
      <guid>https://stackoverflow.com/questions/77524730/a-way-to-count-the-number-of-cycles-of-an-given-attribute-in-a-given-dataset</guid>
      <pubDate>Tue, 21 Nov 2023 17:18:31 GMT</pubDate>
    </item>
    <item>
      <title>Altair 中具有自定义置信区间的折线图</title>
      <link>https://stackoverflow.com/questions/60649486/line-chart-with-custom-confidence-interval-in-altair</link>
      <description><![CDATA[假设我有下面的数据框：

我检查了文档，但它仅基于单个列。 
可重现的代码：
x = np.random.normal(100,5,100)
数据 = pd.DataFrame(x)
ε = 10
数据.列 = [&#39;x&#39;]
数据[&#39;下&#39;] = x - epsilon
数据[&#39;上&#39;] = x + epsilon
数据


我实际上很想使用 Altair，因为我喜欢它的交互性。]]></description>
      <guid>https://stackoverflow.com/questions/60649486/line-chart-with-custom-confidence-interval-in-altair</guid>
      <pubDate>Thu, 12 Mar 2020 07:37:09 GMT</pubDate>
    </item>
    <item>
      <title>Keras：打印出预测的类标签</title>
      <link>https://stackoverflow.com/questions/59910151/keras-printing-out-the-predicted-class-label</link>
      <description><![CDATA[我正在测试基于 cifar10 keras 数据集的 CNN 模型。所以，基本上
预测 = mymodel.predict(x)
打印（预测）

打印出预测本身：

&lt;预&gt;&lt;代码&gt;[[3.3675440e-04 5.7650192e-07 2.5850117e-02 5.4446888e-01 7.0444457e-02
  1.7459875e-01 3.5874096e-03 1.8062484e-01 1.2066155e-06 8.6996079e-05]]

给定 cifar10 类标签，正确的语法是什么
classes = [&#39;飞机&#39;, &#39;汽车&#39;, &#39;鸟&#39;, &#39;猫&#39;, &#39;鹿&#39;, &#39;狗&#39;, &#39;青蛙&#39;, &#39;马&#39;, &#39;船&#39;, &#39;卡车&#39;]

打印预测标签？]]></description>
      <guid>https://stackoverflow.com/questions/59910151/keras-printing-out-the-predicted-class-label</guid>
      <pubDate>Sat, 25 Jan 2020 14:28:07 GMT</pubDate>
    </item>
    </channel>
</rss>