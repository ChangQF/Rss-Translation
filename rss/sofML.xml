<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 30 Apr 2024 01:00:17 GMT</lastBuildDate>
    <item>
      <title>RFE 与 GBM 集成，用于特征选择和超参数调整</title>
      <link>https://stackoverflow.com/questions/78405164/integration-of-rfe-with-gbm-for-feature-selection-and-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78405164/integration-of-rfe-with-gbm-for-feature-selection-and-hyperparameter-tuning</guid>
      <pubDate>Mon, 29 Apr 2024 21:00:14 GMT</pubDate>
    </item>
    <item>
      <title>如何提高汽车价格估算中的 RMSE？</title>
      <link>https://stackoverflow.com/questions/78405026/how-can-i-improve-the-rmse-in-my-car-price-estimate</link>
      <description><![CDATA[如何提高汽车价格估算中的 RMSE？

首先，我将根据行驶公里数估算缺失的条件值。

&lt;前&gt;&lt;代码&gt;`
new_condition_df = df[df[&#39;condition&#39;].map(condition_mapping) == 2]
top_1000_highest_mileage = new_condition_df.nlargest(1000, &#39;里程&#39;)[&#39;里程&#39;]
average_top_1000_highest_mileage = top_1000_highest_mileage.mean()

# 过滤 DataFrame 中条件为 null 或未指定的行
null_condition_df = df[df[&#39;条件&#39;].isnull() | (df[&#39;条件&#39;] == &#39;&#39;)]

# 根据里程条件更新“条件”
null_condition_df.loc[null_condition_df[&#39;mileage&#39;] &gt;=average_top_1000_highest_mileage, &#39;condition&#39;] = &#39;CONDITION_USED&#39;
null_condition_df.loc[null_condition_df[&#39;里程&#39;] &lt; average_top_1000_highest_mileage, &#39;条件&#39;] = &#39;CONDITION_NEW&#39;

# 使用修改后的行更新原始 DataFrame
df.update(null_condition_df)
`


删除一些空行

columns_with_null = [&#39;color&#39;, &#39;vat_reclaimable&#39;, &#39;cubic_capacity&#39;, &#39;seller_country&#39;, &#39;feature&#39;]
df.dropna（子集=columns_with_null，inplace=True）

df[&#39;air_conditioning&#39;].fillna(&#39;AIRCONDITIONING_NONE&#39;, inplace=True)
df[&#39;parking_camera&#39;].fillna(&#39;PARKINGCAMERA_NONE&#39;, inplace=True)
df[&#39;parking_sensors&#39;].fillna(&#39;PARKINGSENZOR_NONE&#39;, inplace=True)


这里我试图估计drive列的缺失值，其中包含汽车是4x4还是4x2的信息，drive包含大量空值，这就是为什么我用如此复杂的方式估计它方式

features = [&#39;里程&#39;, &#39;立方容量&#39;, &#39;功率&#39;, &#39;年份&#39;] + list(df.columns[df.columns.str.startswith(&#39;car_style_&#39;)]) + list(df .columns[df.columns.str.startswith(&#39;transmission_&#39;)]) + list(df.columns[df.columns.str.startswith(&#39;fuel_type_&#39;)])

train_data = df.dropna(subset=[&#39;drive&#39;]) # Odstranění řádků s chybějícími hodnotami sloupce &#39;drive&#39;
X_train, X_test, y_train, y_test = train_test_split(train_data[features], pd.get_dummies(train_data[&#39;drive&#39;]), test_size=0.2, random_state=42)

模型=线性回归()
model.fit(X_train, y_train)

Missing_data = df[df[&#39;drive&#39;].isnull()]
X_missing = Missing_data[特征]
预测值 = model.predict(X_missing)


df_imput = df.copy()
Predicted_df = pd.DataFrame(predicted_values, columns=y_train.columns, index=missing_data.index)
df_impulated.loc[df_impulated[&#39;drive&#39;].isnull(), y_train.columns] = Predicted_df.values

Predicted_df_encoded = pd.DataFrame(predicted_values, columns=y_train.columns, index=missing_data.index)
Predicted_df_encoded = (predicted_df_encoded &gt; 0.5).astype(int)

对于 Predicted_df_encoded.columns 中的列：
    df_impulated[column] = 0 # Přidání sloupce se všemi hodnotami 0
    df_impulated.loc[predicted_df_encoded.index, 列] = Predicted_df_encoded[列].values

unique_values_impulated_encoded = df_impulated[&#39;drive&#39;].unique()
df = df_估算
df.drop(列=[&#39;drive&#39;], inplace=True)


这里我对特征字段进行编码

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
df = df.join(pd.DataFrame(mlb.fit_transform(df[&#39;feature&#39;]),columns=mlb.classes_))
df.fillna(0,就地=True)

df = df.drop(列=[&#39;特征&#39;])


培训本身

df_encoded = pd.get_dummies(df)

X_train, X_test, y_train, y_test = train_test_split(df_encoded.drop(columns=[&#39;price_with_vat_czk&#39;]), df_encoded[&#39;price_with_vat_czk&#39;], test_size=0.25, random_state=42)

模型=线性回归()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)


所有程序：https://onecompiler.com/python/42brp9a4r
数据集https://filetransfer.io/data-package/a0mFEfg4#link
我的 RMSE 约为 64k]]></description>
      <guid>https://stackoverflow.com/questions/78405026/how-can-i-improve-the-rmse-in-my-car-price-estimate</guid>
      <pubDate>Mon, 29 Apr 2024 20:23:39 GMT</pubDate>
    </item>
    <item>
      <title>如何可视化 yolo best.pt 模型架构</title>
      <link>https://stackoverflow.com/questions/78404883/how-to-visualize-yolo-best-pt-model-architecture</link>
      <description><![CDATA[我想以图形方式可视化 yolo v9 模型的架构
我尝试将其转换为 keras 但不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78404883/how-to-visualize-yolo-best-pt-model-architecture</guid>
      <pubDate>Mon, 29 Apr 2024 19:49:00 GMT</pubDate>
    </item>
    <item>
      <title>NefTune 在 Transformers 上获得 0 训练损失</title>
      <link>https://stackoverflow.com/questions/78404768/neftune-receiving-0-training-loss-on-transformers</link>
      <description><![CDATA[我基本上是在尝试使用 Neftune 微调我的模型。模型基于土耳其语言。但在那里我的训练损失为零。我尝试过另一种模型，例如 Turkish-GPT2 没有问题一切都好。我认为模型可能有问题。我不知道如何处理这个问题。
加载模型：
从变压器导入 AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“asafaya/kanarya-750m”)
模型 = AutoModelForCausalLM.from_pretrained(“asafaya/kanarya-750m”)

v3_prompt =“”“Aşağıda，daha fazla bağlam sağlayan bir girdiyle eşleştirilmiş，bir görevi açıklayan bir talimat bulunmaktadır。请注意，请确保您的设备正常工作。

＃＃＃ 输入：
{}

＃＃＃ 指示：
{}

＃＃＃ 回复：
{}
”“”

更改格式提示：
EOS_TOKEN = tokenizer.eos_token # 必须添加EOS_TOKEN
defformatting_prompts_func（示例）：
    输入=示例[“输入”]
    说明 = 示例[&#39;说明&#39;]
    输出=示例[“响应”]
    文本=[]
    对于 zip 中的输入、指令、输出（输入、指令、输出）：
        # 必须添加EOS_TOKEN，否则你的一代将永远延续下去！
        text = v3_prompt.format(输入、指令、输出) + EOS_TOKEN
        文本.append(文本)
    返回 {“文本”； ：文本，}
经过

从数据集导入数据集

数据集 = Dataset.from_pandas(数据[:40000])
数据集= dataset.map(formatting_prompts_func,batched=True)

Neftune：
from trl import SFTTrainer
从 Transformers 导入 TrainingArguments

trainer_2 = SFTTrainer(
    型号=型号，
    train_dataset=数据集，
    dataset_text_field=&quot;文本&quot;,
    最大序列长度=512，
    neftune_noise_alpha=5,
    包装=假，
    args = 训练参数(
        per_device_train_batch_size = 1, # 批量大小
        gradient_accumulation_steps = 2, # 梯度累积步数
        热身步骤 = 5,
        最大步数 = 80,
        学习率 = 2e-4,
        fp16 = 假，
        bf16 = torch.cuda.is_bf16_supported(),
        记录步骤 = 1,
        优化=“adamw_8bit”，
        权重衰减 = 0.01,
        lr_scheduler_type =“线性”，
        种子=3407，
        输出目录=“输出”，
    ),
）
trainer_2.train()

输出：
&lt;前&gt;&lt;代码&gt; [80/80 00:25，纪元 0/1]
步数训练损失
1 0.000000
2 0.000000
3 0.000000
4 0.000000
5 0.000000
6 0.000000
7 0.000000
8 0.000000
9 0.000000
10 0.000000
11 0.000000
12 0.000000
13 0.000000
14 0.000000
15 0.000000
16 0.000000
17 0.000000
18 0.000000
19 0.000000
20 0.000000
21 0.000000
22 0.000000
23 0.000000
24 0.000000
25 0.000000
26 0.000000
27 0.000000
28 0.000000
29 0.000000
30 0.000000
31 0.000000
32 0.000000
33 0.000000
34 0.000000
35 0.000000
36 0.000000
37 0.000000
38 0.000000
39 0.000000
40 0.000000
41 0.000000
42 0.000000
43 0.000000
44 0.000000
45 0.000000
46 0.000000
47 0.000000
48 0.000000
49 0.000000
50 0.000000
51 0.000000
52 0.000000
53 0.000000
54 0.000000
55 0.000000
56 0.000000
57 0.000000
58 0.000000
59 0.000000
60 0.000000
61 0.000000
62 0.000000
63 0.000000
64 0.000000
65 0.000000
66 0.000000
67 0.000000
68 0.000000
69 0.000000
70 0.000000
71 0.000000
72 0.000000
73 0.000000
74 0.000000
75 0.000000
76 0.000000
77 0.000000
78 0.000000
79 0.000000
80 0.000000
TrainOutput（global_step = 80，training_loss = 0.0，metrics = {&#39;train_runtime&#39;：25.3789，&#39;train_samples_per_second&#39;：6.304，&#39;train_steps_per_second&#39;：3.152，&#39;total_flos&#39;：117937578909696.0，&#39;train_loss&#39;：0.0，&#39;epoch&#39;： 0.004})
]]></description>
      <guid>https://stackoverflow.com/questions/78404768/neftune-receiving-0-training-loss-on-transformers</guid>
      <pubDate>Mon, 29 Apr 2024 19:23:04 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[我有以下输入矩阵
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
        [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])

和零元素的索引
mask_indices = torch.tensor(
[[7, 2],
[2, 6]])

如何从与以下矩阵的乘法中排除非零元素：
my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317]])

也就是说，不要将其相乘（包括零）：
a = torch.mm(inp_tensor, my_tensor)
打印（一）
张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我想排除零个元素（以及 my_tensor 的相应行）：
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.6524, 0.6057, 0.3725, 0.7980]]) # 删除零个元素

my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696]]) # 删除对应的零元素行

b = torch.mm(inp_tensor, my_tensor)
打印(b)
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330]])

inp_tensor = torch.tensor([[1.0000, 0.1115, 0.6524, 0.6057, 0.3725, 1.0000]]) # 删除零个元素

my_tensor = torch.tensor(
        [
        [0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.4414, 0.2969, 0.8317]]) # 删除对应的零元素行

c = torch.mm(inp_tensor, my_tensor)
打印（三）
&gt;&gt;&gt;&gt;&gt;张量([[2.2041, 2.5388, 2.3315]])
打印（火炬.cat（[b，c]））
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我需要它是高效的（即，没有for循环），因为我的张量非常大，并且还需要保持梯度（即，如果我调用optimizer.backward( ）更新计算图中的相关参数）]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在 Designer 中使用在 Azure AutoML 中创建的 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78403537/is-there-a-way-to-use-a-ml-model-created-in-azure-automl-within-designer</link>
      <description><![CDATA[我知道我可以在 Azure Designer 中创建自定义代码模块，但是有没有办法连接我在 AutoML 中本机创建的 ML 模型？
AutoML 模型正在使用 XGBoost，这似乎不是 Designer 的 ML 组件功能下的选项。我的目标是创建一个低代码解决方案，因此我不想使用自定义 Python 代码组件。
有什么想法吗？
使用 AutoML 构建模型，需要连接到 Designer 中的现有数据管道]]></description>
      <guid>https://stackoverflow.com/questions/78403537/is-there-a-way-to-use-a-ml-model-created-in-azure-automl-within-designer</guid>
      <pubDate>Mon, 29 Apr 2024 14:52:21 GMT</pubDate>
    </item>
    <item>
      <title>随机森林 - 针对 AUC 或 F1 分数进行优化</title>
      <link>https://stackoverflow.com/questions/78402507/random-forest-optimize-for-auc-or-f1-score</link>
      <description><![CDATA[我在 sklearn 中使用随机森林，我的数据集相当不平衡（20% 为正类，80% 为其他类）。有没有办法让它针对某些考虑到这一点的指标进行训练（优化），比如 AUC 分数或 F1 分数？有什么技巧可以用来推动它朝这个方向发展？
到目前为止，我想到/尝试过的唯一方法是使用不同的类权重。
或者，是否有其他实现（或另一个模型，例如 xgboost）可以允许我使用这种自定义指标？]]></description>
      <guid>https://stackoverflow.com/questions/78402507/random-forest-optimize-for-auc-or-f1-score</guid>
      <pubDate>Mon, 29 Apr 2024 11:49:58 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能寻找好的视频质量增强器[关闭]</title>
      <link>https://stackoverflow.com/questions/78402402/searching-for-a-good-video-quality-enhancer-using-ai</link>
      <description><![CDATA[我有一些 15 年前制作的旧视频。质量很差，它确实被压缩了，而且我有一些小故障，因为此时视频导入非常有问题。
我想尝试一些能够使用人工智能增强这些视频的程序。
如果只有可用的模型，我什至可以编写此代码。
我搜索 MacOS 或 Linux 工具。我更喜欢它是开源的。
你知道我在哪里可以找到一个好的工具吗？我尝试了一些，但质量不太好。
如果没有程序可以做到这一点，我该如何继续训练我的模型？我不知道这是否是一个好方法，因为它需要大量视频来完成......
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78402402/searching-for-a-good-video-quality-enhancer-using-ai</guid>
      <pubDate>Mon, 29 Apr 2024 11:32:19 GMT</pubDate>
    </item>
    <item>
      <title>关于 ML 中预测时间序列的分析和模型的建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78402245/suggestion-about-analysis-and-model-for-forecasting-time-series-in-ml</link>
      <description><![CDATA[我必须根据 csv 数据对给定日期的网络单元格进行流量预测，其中包含 5 列：区域、站点、单元格、日期、流量
例如：
&lt;上一页&gt;&lt;代码&gt;AAN,AAN001,AAN001A,2021-01-01,2.56

AAN,AAN001,AAN001B,2021-01-01,5.6

ANM,ANM448,ANM448B,2021-04-19,1.2

ANM,ANM448,ANM448C,2021-04-19,3.6

有人可以帮助我一点，我仍然是机器和深度学习的初学者
我已经尝试过 randomforst 但没有成功。
我做了一些研究，结果被告知我们可以使用 LSTM 模型？]]></description>
      <guid>https://stackoverflow.com/questions/78402245/suggestion-about-analysis-and-model-for-forecasting-time-series-in-ml</guid>
      <pubDate>Mon, 29 Apr 2024 11:01:17 GMT</pubDate>
    </item>
    <item>
      <title>Python SkLearn 线性回归的准确度分数给出了无意义的结果</title>
      <link>https://stackoverflow.com/questions/78402227/python-sklearn-accuracy-scores-for-linear-regression-give-nonsensical-results</link>
      <description><![CDATA[我正在参加 Kaggle 房价 竞赛练习我的机器学习技能。我对数据进行预处理，然后使用交叉验证来测试几个不同的模型，看看哪个模型表现最好。不幸的是，尽管我收到了大多数模型的正常结果，但我得到的线性回归结果毫无意义。我发布了我的代码和结果图片，您可以在 Kaggle 竞赛的链接中找到数据文件。您能否向我解释一下为什么我会收到这些结果以及我可以采取哪些措施来解决这些问题？
准确率得分结果
train = pd.read_csv(&#39;train.csv&#39;)

train.dropna（轴= 1，阈值= 1200，就地= True）
train[&#39;SalePrice&#39;] = np.log(train[&#39;SalePrice&#39;])

num_data = list(train.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).drop([&#39;Id&#39;, &#39;SalePrice&#39;], axis=1))
cat_data = list(train.select_dtypes(include=&#39;object&#39;))

cat_pipeline = Pipeline([(&#39;cat_imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)), (&#39;编码器&#39;, OneHotEncoder())])
num_pipeline = Pipeline([(&#39;num_imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())])
                         
col_transformer = ColumnTransformer([(&#39;cat_pipeline&#39;, cat_pipeline, cat_data),
                                     (&#39;num_pipeline&#39;, num_pipeline, num_data)], 余数=&#39;drop&#39;)

y = 火车[&#39;销售价格&#39;]
X = train.drop(columns=[&#39;Id&#39;, &#39;SalePrice&#39;])
X = col_transformer.fit_transform(X).toarray()

X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)

lin_reg = 线性回归()
sgd = SGDRegressor()
贝叶斯 = BayesianRidge()
树 = DecisionTreeRegressor()
森林 = RandomForestRegressor()
xgb = XGBRegressor()

结果={}
对于 [lin_reg、sgd、bayes、tree、forest、xgb] 中的模型：
    分数 = cross_validate(模型, X_train, y_train, 评分=[&#39;neg_mean_squared_error&#39;, &#39;neg_mean_absolute_error&#39;,
                                                              &#39;explained_variance&#39;, &#39;r2&#39;], cv=10)
    结果[str(model).split(&#39;(&#39;)[0]] = [分数[&#39;test_neg_mean_squared_error&#39;].mean(), 分数[&#39;test_neg_mean_absolute_error&#39;].mean(),
                           分数[&#39;test_explained_variance&#39;].mean(), 分数[&#39;test_r2&#39;].mean()]

pd.options.display.float_format = &#39;{:,.4f}&#39;.format
results_df = pd.DataFrame(数据=结果,
                          index=[&#39;负均方误差&#39;, &#39;负平均绝对误差&#39;, &#39;解释方差得分&#39;, &#39;R2 得分&#39;])
结果_df


我上网查了一下，但没有找到遇到同样问题的人。]]></description>
      <guid>https://stackoverflow.com/questions/78402227/python-sklearn-accuracy-scores-for-linear-regression-give-nonsensical-results</guid>
      <pubDate>Mon, 29 Apr 2024 10:55:30 GMT</pubDate>
    </item>
    <item>
      <title>Keras TextVectorization 似乎区分大小写，即使词汇量没有反映这一点？</title>
      <link>https://stackoverflow.com/questions/78400462/keras-textvectorization-seems-to-be-case-sensitive-even-though-vocabulary-doesn</link>
      <description><![CDATA[我有以下代码
导入 keras

v = {
    “甲板”：[&#39;a&#39;，&#39;B&#39;，&#39;C&#39;，&#39;D&#39;，&#39;E&#39;，&#39;F&#39;，&#39;G&#39;，&#39;H&#39;，&#39;I&#39;，&#39;J&#39;，&#39;K&#39;， &#39;L&#39;]
}

打印（len（v[“甲板”]））

l = keras.layers.TextVectorization(
    max_tokens=len(v[“甲板”])+2,
    词汇=v[“甲板”],
    输出模式=&#39;计数&#39;,
    名称=“甲板”）

打印（l.vocabulary_size（））
打印（l.get_vocabulary（））

print(l(&#39;a A b B&#39;))

输出是：
&lt;前&gt;&lt;代码&gt;12
13
[&#39;[UNK]&#39;、&#39;a&#39;、&#39;B&#39;、&#39;C&#39;、&#39;D&#39;、&#39;E&#39;、&#39;F&#39;、&#39;G&#39;、&#39;H&#39;、&#39;I&#39;、&#39;J&#39;、&#39;K&#39; ，&#39;L&#39;]
tf.Tensor([2.2.0.0.0.0.0.0.0.0.0.0.0.],形状=(13,),dtype=float32)

我希望至少有一个 b 能够被计算在内。
如果我使用l.adapt(v[“deck”])，事情似乎会相应地工作，但词汇都是小写的。
像这样：
导入 keras

v = {
    “甲板”：[&#39;a&#39;，&#39;B&#39;，&#39;C&#39;，&#39;D&#39;，&#39;E&#39;，&#39;F&#39;，&#39;G&#39;，&#39;H&#39;，&#39;I&#39;，&#39;J&#39;，&#39;K&#39;， &#39;L&#39;]
}

打印（len（v[“甲板”]））

l = keras.layers.TextVectorization(
    max_tokens=len(v[“甲板”])+2,
    # 词汇=v[“甲板”],
    输出模式=&#39;计数&#39;,
    名称=“甲板”）

l.adapt(v[&#39;甲板&#39;])

打印（l.vocabulary_size（））
打印（l.get_vocabulary（））

print(l(&#39;a A b B&#39;))

和输出：
&lt;前&gt;&lt;代码&gt;12
13
[&#39;[UNK]&#39;、&#39;l&#39;、&#39;k&#39;、&#39;j&#39;、&#39;i&#39;、&#39;h&#39;、&#39;g&#39;、&#39;f&#39;、&#39;e&#39;、&#39;d&#39;、&#39;c&#39;、&#39;b&#39; ， &#39;A&#39;]
tf.Tensor([0.0.0.0.0.0.0.0.0.0.0.2.2.]，形状=(13,)，dtype=float32)

如何正确使用词汇参数？]]></description>
      <guid>https://stackoverflow.com/questions/78400462/keras-textvectorization-seems-to-be-case-sensitive-even-though-vocabulary-doesn</guid>
      <pubDate>Mon, 29 Apr 2024 04:20:18 GMT</pubDate>
    </item>
    <item>
      <title>GKE 上的 GPU 时间共享</title>
      <link>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</link>
      <description><![CDATA[我正在尝试使用 说明中的 GPU 时间共享此处，但是我的工作负载不会在启用分时的节点上运行。
我有一个具有 GPU 配置的节点池，启用了策略分时的 GPU 共享以及“每个 GPU 的最大共享客户端数”。如 48 所示。节点运行良好，但我无法使用记录的 nodeSelector 配置为我的工作负载运行工作负载，例如
节点选择器：
  cloud.google.com/gke-accelerator：“nvidia-tesla-t4”
  cloud.google.com/gke-max-shared-clients-per-gpu：“48”
  cloud.google.com/gke-gpu-sharing-strategy：分时

这样，我的 Pod 就会陷入挂起状态，并显示消息xnodes did not match Pod&#39;s nodeaffinity/selector。如果我删除 gke-max-shared-clients-per-gpu 和 gke-gpu-sharing-strategy 密钥对，pod 会正常调度并运行。
当我检查 GPU 分时节点池中节点上的 kubernetes 标签时，它们不包含这些标签，并且我无法手动添加它们，因为 GCP 阻止了它。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</guid>
      <pubDate>Mon, 29 Apr 2024 02:20:41 GMT</pubDate>
    </item>
    <item>
      <title>呼吸信号的对数功率谱</title>
      <link>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</link>
      <description><![CDATA[我的数据集是噪声频谱的LPS，我的标签是干净频谱的LPS，每个图片大小是（1025*1292）。我使用unet作为我的模型。
型号：
导入火炬
将 torch.nn 导入为 nn

解码器类（nn.Module）：
    def __init__(self, in_channels,out_features, kernel_size, maxpoolindex, apply_dropout,stride):
        super(解码器, self).__init__()
        self.conv = nn.Conv2d(in_channels=in_channels、out_channels=out_features、kernel_size=kernel_size、stride=stride、padding=0、bias=True)
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.LeakyReLU(负斜率=0.2)
        self.dropout = nn.Dropout(p=0.1)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) 如果 maxpoolindex == 1 否则无

    def 前向（自身，x）：
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        如果 self.maxpool 不是 None：
            x = self.maxpool(x)
        返回x

编码器类（nn.Module）：
    def __init__(self,in_channels, out_features, kernel_size, apply_dropout):
        super(编码器, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d（in_channels = in_channels，out_channels = out_features，kernel_size = kernel_size，stride = 2，padding = 0，bias = True）
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)

    def 前向（自身，x）：
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        返回x

class DenoiseUnet(nn.Module):#除到第一个奇数停止的设计
    def __init__(自身):
        超级（DenoiseUnet，自我）.__init__()
        self.down_procedure = nn.ModuleList([
            解码器(1,8,2,0,0,2),
            解码器(8,16,2,0,0,2),
            解码器(16,32,2,0,0,2),
            解码器(32,128,2,0,0,2),
            解码器(128,128,1,0,0,1)
        ]）
        self.up_procedure = nn.ModuleList([
            编码器(2​​56,32,2,0),
            编码器(64,16,2,0),
            编码器(32,8,2,0),
            编码器(16,4,2,0),
        ]）
        self.convert = nn.ConvTranspose2d(4, 1, kernel_size=1, stride=1, padding=0)


    def 前向（自身，x）：
        连接=[]
        对于 self.down_procedure 中的 down：
            x = 向下(x)
            连接.append(x)

        连接=列表（反转（连接[：-1]））
        对于 up，在 zip(self.up_procedure, connection) 中连接：


            如果 x.shape[2] &lt;连接.形状[2]：
              连接 = 连接[:, :, :x.shape[2], :]
            别的：
              x = x[:, :, :connect.shape[2], :]

            如果 x.shape[3] &lt;连接.形状[3]：
              连接 = 连接[:, :, :, :x.shape[3]]
            别的：
              x = x[:, :, :, :connect.shape[3]]



            x = torch.cat([x, 连接], 暗淡=1)
            x = 上(x)

        y = self.convert(x)
        返回y


模型 = DenoiseUnet()
打印（模型）

但是经过 10 轮训练后，我得到这样的结果：
https://i.sstatic.net/b8DxXHUr.png
https://i.sstatic.net/UDhZNKHE.png
一个是预测结果，一个是测试，有人能帮我找出问题所在吗？
数据集数量不同，看起来是一样的。]]></description>
      <guid>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:28 GMT</pubDate>
    </item>
    <item>
      <title>我尝试使用 (pip install pickle5) 安装 Python 的 pickle 包，但安装包失败</title>
      <link>https://stackoverflow.com/questions/77890171/i-tried-installing-the-pickle-package-for-python-using-pip-install-pickle5-and</link>
      <description><![CDATA[这是我尝试过的：
pip install pickle5

这是带有错误消息的快照。
这也是我收到的错误：
错误：无法为 pickle5 构建 wheel，这是安装基于 pyproject.toml 的项目所必需的
我尝试按照其他一些帖子中的建议安装并重新安装 Microsoft Visual C++，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77890171/i-tried-installing-the-pickle-package-for-python-using-pip-install-pickle5-and</guid>
      <pubDate>Sat, 27 Jan 2024 05:36:19 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker 实例中的 CUDA 路径解决 NameError：名称“_C”未使用 GroundingDINO 定义</title>
      <link>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</link>
      <description><![CDATA[我正在尝试在 Sagemaker 实例中安装和使用 grounding dino（使用 GPU ）但我收到错误：
NameError：名称“_C”未定义

我发现原因是因为变量CUDA_HOME没有配置所以要解决它我需要设置变量，但是在搜索答案后（我已经检查了公共路径/usr/local/cuda）我找不到sagemaker实例中cuda的安装路径。
cuda 安装在 sagemaker 实例中的什么位置以便我可以设置 CUDA_HOME？]]></description>
      <guid>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</guid>
      <pubDate>Fri, 26 Jan 2024 18:45:06 GMT</pubDate>
    </item>
    </channel>
</rss>