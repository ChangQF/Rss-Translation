<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Jan 2024 06:18:43 GMT</lastBuildDate>
    <item>
      <title>只能在 mycode 中将大小为 1 的数组转换为 Python 标量</title>
      <link>https://stackoverflow.com/questions/77804678/can-only-convert-an-array-of-size-1-to-a-python-scalar-in-mycode</link>
      <description><![CDATA[def time_lag(数据, 滞后):
”“”
将数据集转换为网格信息的时间序列并吐出时间滞后的时间序列
data - csv 文件的全名
”“”
time_orig = pd.to_datetime(&#39;1900-01-01&#39;)
df = pd.read_csv(数据)
df.columns = [&#39;时间&#39;, &#39;wind_u10&#39;, &#39;wind_v10&#39;, &#39;slp&#39;, &#39;体重&#39;, &#39;浪涌&#39;]

# 重新组织矩阵
df_new = df.loc[df[&#39;权重&#39;] == df[&#39;权重&#39;].unique()[0]]
df_new.drop([&#39;weight&#39;], axis = 1, inplace=True) #, &#39;surge&#39;

对于范围 (1,10) 内的 i：
    df_sub = df.loc[df[&#39;weight&#39;] == df[&#39;weight&#39;].unique()[i]]
    df_sub.drop([&#39;weight&#39;, &#39;surge&#39;], axis = 1, inplace=True)
    df_new = pd.merge(df_new, df_sub, on=&#39;时间&#39;)


# 滞后时间序列数据
lagged_df = df_new.copy() # 防止修改原始矩阵
对于范围内的 j（滞后）：
    #lagged.drop(j, 轴 = 0, 就地 = True)
    lagged_df[&#39;时间&#39;] = lagged_df[&#39;时间&#39;]+4
    
    # 删除最后一行，因为 df_new 中没有匹配的行
    lagged_df.drop（lagged_df.tail（1）.index.item（），轴= 0，就地= True）
    
    # 从 df_new 中删除最上面的行以匹配滞后
    df_new.drop（df_new.head（1）.index.item（），轴= 0，就地= True）
    
    # 将滞后数据与 df_new 合并
    df_new = pd.merge(df_new, lagged_df, on = &#39;时间&#39;, how = &#39;外部&#39;, \
                   后缀 = (&#39;_left&#39;, &#39;_right&#39;))
df_new = df_new.T.reset_index(drop=True).T
ind = df_new.loc[pd.isna(df_new[df_new.shape[1]-1]), :].index
df_new.drop(ind, inplace=True)

# 风暴潮时间序列数据
Surge_ts = pd.DataFrame(df.loc[df[&#39;体重&#39;] == \
                            df[&#39;体重&#39;].unique()[0]][[&#39;时间&#39;, &#39;激增&#39;]])
# 删除缺失值/NaN 值
Surge_ts.reset_index(inplace=True) # 重置子集 isnans 的索引
Surge_ts.drop([&#39;index&#39;], axis = 1, inplace=True)
indx = Surge_ts.loc[pd.isna(surge_ts[“surge”]), :].index
df_new.drop(indx, inplace=True)
Surge_ts.drop(indx, inplace=True)

# 根据 df_new 过滤浪涌
lagged_time = 列表(df_new[0])
time_df_new = [float(x) for x in df_new[0]]
time_surge_ts = [float(x) for x in rush_ts[&#39;time&#39;]]
时间_两者 = []
对于 lagged_time 中的 k：
    if ((time_df_new 中的 k) &amp; (time_surge_ts 中的 k)):
        time_both.append(int(k))
        
Surge_ts = Surge_ts[surge_ts[&#39;time&#39;].isin(time_both)]

dt = pd.DataFrame(columns = [&#39;日期&#39;]);
对于 Surge_ts.index 中的 i：
    dt.loc[i, &#39;日期&#39;] = time_orig + \
        datetime.timedelta(小时 = int(surge_ts.loc[i, &#39;时间&#39;]))
        
Surge_ts[&#39;日期&#39;] = dt
df_new = df_new[df_new[0].isin([x*1.0 for x in time_both])]
df_new.drop(4, axis = 1, inplace = True) # 移除无滞后的浪涌数据
返回 df_new、surge_ts

数据 = &#39;stormdata.csv&#39;
x, 浪涌 = time_lag(数据,3)]]></description>
      <guid>https://stackoverflow.com/questions/77804678/can-only-convert-an-array-of-size-1-to-a-python-scalar-in-mycode</guid>
      <pubDate>Fri, 12 Jan 2024 06:15:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用人工智能识别文本和字典映射</title>
      <link>https://stackoverflow.com/questions/77804632/how-to-identify-text-and-dictionary-mapping-using-ai</link>
      <description><![CDATA[我正在从事一个医疗保健项目，我将收到一份包含有关患者疾病、药物等详细信息的文本。我有一个特定于领域的标准词典，其中包含疾病的标准术语和唯一的 ID。我需要创建一个人工智能模型来处理文本，然后识别字典中的正确映射。
示例：
在文字中 - 患者提到我从过去三天开始就胃痛，医生给我开了 xxx 药。
输出 - 腹痛 [1032713]（它产生了该症状的唯一 ID 和相应的标准术语）
有人可以指导我应该采取什么方法来解决这个问题吗？
我探索并发现我应该为文本和字典单词创建单词嵌入。但字典里可能有数百万个单词，这实际上是不可能的。]]></description>
      <guid>https://stackoverflow.com/questions/77804632/how-to-identify-text-and-dictionary-mapping-using-ai</guid>
      <pubDate>Fri, 12 Jan 2024 06:03:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 LCOOV 进行 K 最近邻的问题</title>
      <link>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-lcoov</link>
      <description><![CDATA[我有一个示例表，我想对其进行 KKNN 分类。变量 V4 是响应，我希望分类器查看新数据点是否会分类为 0 或 1 （实际数据有12 列，第 12 列是响应，但我仍然会简化示例
库 (kknn)

数据&lt;- data.frame(
  V1 = c(1.2, 2.5, 3.1, 4.8, 5.2),
  V2 = c(0.7, 1.8, 2.3, 3.9, 4.1),
  V3 = c(2.3, 3.7, 1.8, 4.2, 5.5),
  V4= c(0, 1, 0, 1, 0))

现在，我想使用 for 循环通过 LOOCV 构建 kknn 分类。假设 kknn=3
for (i in 1:nrow(data)) {

  train_data&lt;-data[-i,1:3]
  train_data_response&lt;-data.frame( 数据[-i,4])
  colnames(train_data_response) &lt;- “响应”
  test_set&lt;-data[i,3]
  模型&lt;-kknn(公式= train_data_response~.,data.frame(train_data),data.frame(test_set),k = 3,scale = TRUE)
}

现在我收到以下错误：
&lt;块引用&gt;
model.frame.default(公式，数据=训练)中的错误：
变量“train_data_response”的类型（列表）无效

有什么办法可以解决这个错误吗？我认为 kknn 接受矩阵或数据帧。我的训练和测试数据确实是数据框，那么什么给出了？
此外，我是否正确执行了 LCOOV？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-lcoov</guid>
      <pubDate>Fri, 12 Jan 2024 04:15:37 GMT</pubDate>
    </item>
    <item>
      <title>构建聊天机器人以建议代码片段和 sharePoint 文件链接</title>
      <link>https://stackoverflow.com/questions/77804224/build-chatbot-to-suggest-code-snippets-and-sharepoint-file-links</link>
      <description><![CDATA[我正在尝试构建一个聊天机器人来训练我的数据并输出响应（代码片段、语法、网络链接、SharePoint 文件链接、文本响应等）。考虑到多种类型的输出响应，我不确定生成式 AI 和描述性 ML 模型中的哪一种适合我。有人可以让我了解从哪里开始吗？
我过去曾尝试过使用 TensorFlow 和 Keras 进行文本处理，但在这种情况下，我认为我无法使用它。]]></description>
      <guid>https://stackoverflow.com/questions/77804224/build-chatbot-to-suggest-code-snippets-and-sharepoint-file-links</guid>
      <pubDate>Fri, 12 Jan 2024 03:43:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 kubernetes 中构建机器学习平台 [关闭]</title>
      <link>https://stackoverflow.com/questions/77804053/how-to-build-a-machine-learning-platform-in-kubernetes</link>
      <description><![CDATA[我需要在 Kubernetes 上构建一个自助机器学习平台。该平台适用于数据科学团队，无论他们的用例是机器学习、深度学习还是法学硕士。我想了解如何从 Kubernetes 设计转向 kubeflow、mlflow、spark、kafka 等组件，甚至 GPU/CPU 基线。任何有关此事的线索或阅读都将受到高度赞赏。 kubernetes 将是本地安装，而不是云托管服务。]]></description>
      <guid>https://stackoverflow.com/questions/77804053/how-to-build-a-machine-learning-platform-in-kubernetes</guid>
      <pubDate>Fri, 12 Jan 2024 02:30:35 GMT</pubDate>
    </item>
    <item>
      <title>初始化 VAE 权重</title>
      <link>https://stackoverflow.com/questions/77804014/initializing-vae-weights</link>
      <description><![CDATA[我正在训练遵循以下整体架构的 VAE：

变压器编码器
Mu/Logvar -&gt;重新参数化-&gt;潜在z
变压器解码器

根据典型的 VAE 设置，Mu 和 Logvar 只是两个前馈网络。然而，当我用标准值（例如权重为 0.5，偏差为 0）初始化它们时，我发现模型的初始 KL 损失巨大 - 例如5,000-20,000+。
当然，这个下降得相当快，但模型仍然花费数百个时期将 KL 损失从 300 降至 &lt;50。
一个“解决方法”我发现将权重初始化为低得多的值，并使用学习率预热。但初始化权重非常小：
def init_weights(self, initrange=0.0001) -&gt; &gt;没有任何：
        self.embedding_layer.weight.data.uniform_(-0.5, 0.5)
        
        nn.init.uniform_(self.fc_mu.weight, -initrange, initrange)
        nn.init.uniform_(self.fc_logvar.weight, -initrange, initrange)
        nn.init.zeros_(self.fc_mu.bias)
        nn.init.zeros_(self.fc_logvar.bias)

这样做的结果是一个更加稳定的 KL 损失（开始时约为 1.5），但我担心它会阻止我的解码器学习有意义的表示。实际的重建损失因此受到巨大影响。
所以我的问题是：这是 VAE 的已知问题吗？有什么我可以尝试的特定初始化技巧吗？或者也许我应该从正常值开始，让模型训练的时间明显更长？
提前非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/77804014/initializing-vae-weights</guid>
      <pubDate>Fri, 12 Jan 2024 02:14:48 GMT</pubDate>
    </item>
    <item>
      <title>如何预测非线性关系[关闭]</title>
      <link>https://stackoverflow.com/questions/77803595/how-to-predict-nonlinear-relationship</link>
      <description><![CDATA[我有亚马逊评论数据，想要制作根据评论预测星星数量的模型，因此我使用情感分析来获取四列neg neu pos compund，之后我尝试了这样许多算法来预测评级 (1-5)，但我的准确度仍然较差 (40-60%)
这就是原始数据
(https://i.stack.imgur.com/ispqP.png)&lt; /p&gt;
继维德斯之后
(https://i.stack.imgur.com/oWV2H.png)&lt; /p&gt;
我使用了 SVR Dessicon Tree KNN 等，但到目前为止我仍然没有得到最好的结果]]></description>
      <guid>https://stackoverflow.com/questions/77803595/how-to-predict-nonlinear-relationship</guid>
      <pubDate>Thu, 11 Jan 2024 23:34:16 GMT</pubDate>
    </item>
    <item>
      <title>解释这个学习曲线[关闭]</title>
      <link>https://stackoverflow.com/questions/77803095/interpreting-this-learning-curve</link>
      <description><![CDATA[我一直在为我的简历开发一个基本的情感分析项目，并在这里为我的神经网络模型绘制了一条学习曲线。我是否正确绘制了学习曲线？这个具体的曲线告诉了我什么？
就上下文而言，我的训练准确度是 0.758
测试精度：.731
CV 准确度：.715

我无法判断这是否表明过度拟合，或者更多的训练数据将是有益的。]]></description>
      <guid>https://stackoverflow.com/questions/77803095/interpreting-this-learning-curve</guid>
      <pubDate>Thu, 11 Jan 2024 21:20:04 GMT</pubDate>
    </item>
    <item>
      <title>随机森林模型精度低[已迁移]</title>
      <link>https://stackoverflow.com/questions/77802745/low-accuracy-in-random-forest-model</link>
      <description><![CDATA[HCV.Egy.Data &lt;- read.csv(“~/Stat/Metaheuristic ML/hepatitis+c+virus+hcv+for+egyptian+患者/HCV-Egy-Data.csv”)
表（HCV.Egy.Data$Baselinehistological.staging）
HCV.Egy.Data$Baselinehistological.staging &lt;- as.factor(HCV.Egy.Data$Baselinehistological.staging)
库（“随机森林”）
库（caTools）
split &lt;-sample.split(HCV.Egy.Data, SplitRatio = 0.7)
分裂
训练 &lt;- 子集(HCV.Egy.Data, split == &quot;TRUE&quot;)
测试 &lt;- 子集(HCV.Egy.Data, split == &quot;FALSE&quot;)
set.seed(120) # 设置种子
classifier_RF = randomForest(x = train[,-c(28,29)],
                             y = train$Baselinehistological.staging,
                             ntree = 500，类型=“分类”）
classifier_RF$类
y_pred = 预测(classifier_RF, newdata = 测试[,-c(28,29)])
fusion_mtx = 表(测试[, 29], y_pred)
混淆_mtx

表(train[,29], classifier_RF$预测)

我的 ML 模型使用许多不同的 ML 方法（随机森林、XGboost、adaboost、LR 等），该模型的准确度较低 (20-30%)。这是运行上述代码的混淆矩阵：
&lt;前&gt;&lt;代码&gt; y_pred
     1 2 3 4
  1 13 22 36 38
  2 21 17 39 33
  3 15 20 38 27
  4 13 16 37 45

我们甚至应用了 SMOTE 和离散化步骤，将准确率提高到只有 33%。有谁知道为什么使用该数据集的论文的准确率在 70-95% 范围内？是不是明显缺少了什么？
例如，这是一篇论文：https://www.researchgate.net/profile/Md-Satu/publication/341987762_Predicting_Infectious_State_of_Hepatitis_​​C_Virus_Affected_Patient%27s_Applying_Machine_Learning_Methods/链接/ 5edcab8a45851529453fc609/预测丙型肝炎感染状态患者-应用机器学习方法.pdf
我们还应用了 8 次 SMOTE，但准确率仅提高到 33%。
这是数据集： https://archive.ics.uci.edu/dataset/503/hepatitis+c+virus+hcv+for+egyptian+患者如果您知道任何修复方法，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77802745/low-accuracy-in-random-forest-model</guid>
      <pubDate>Thu, 11 Jan 2024 20:05:57 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 和 SHAP 中使用子采样数据的 SHAP 解释器错误</title>
      <link>https://stackoverflow.com/questions/77800921/shap-explainer-error-using-subsampled-data-in-xgboost-and-shap</link>
      <description><![CDATA[我正在尝试对数据子集进行随机采样，以从 XGBoost 模型创建 SHAP TreeExplainer 对象。我使用数据子集，因为从完整数据集（200K+）行创建 TreeExplainer 对象需要几天的时间才能运行。但是，当我运行代码时，我收到 TreeExplainer 错误。
我的代码：
&lt;前&gt;&lt;代码&gt;导入xgboost
将 numpy 导入为 np
将 pandas 导入为 pd
导入形状

数据= np.load（&#39;pandas_df.pkl&#39;，allow_pickle = True）
X = 数据[[&#39;feat_1&#39;, &#39;feat_2&#39;, ...., &#39;feat_n&#39;]]
y = 数据[[&#39;响应&#39;]]
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 0）

模型 = xgboost.XGBRFClassifier()
model.fit(X_train, y_train)

shap_sample_x_test = X_test.sample(n = 1000, random_state = 0)

解释器 = shap.Explainer(模型)
shap_values = 解释器(shap_sample_x_test)

shap.plots.beeswarm(shap_values)

这会导致此错误：
ExplainerError：TreeExplainer 中的可加性检查失败！请确保您传递给解释器的数据矩阵与模型训练时的形状相同。如果您的数据形状正确，请在 GitHub 上报告此情况。考虑使用 feature_perturbation=&#39;interventional&#39; 选项重试。此检查失败，因为其中一个样本的 SHAP 值总和为 -1.947115，而模型输出为 -1.936696。如果这种差异可以接受，您可以设置 check_additivity=False 来禁用此检查。
]]></description>
      <guid>https://stackoverflow.com/questions/77800921/shap-explainer-error-using-subsampled-data-in-xgboost-and-shap</guid>
      <pubDate>Thu, 11 Jan 2024 14:49:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 3.12.1 上安装 PyTorch</title>
      <link>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</link>
      <description><![CDATA[我正在安装 DARTS TimeSeries 库 (https: //github.com/unit8co/darts/blob/master/INSTALL.md#enabling-Optional-dependencies），但我遇到了依赖项安装问题。在 DARTS 安装指南中，它说如果我们遇到这个问题，我们必须参考 PyTorch 的官方安装指南，然后尝试再次安装 Darts。然后，当我尝试在 python 3.12.1 上安装 torch 时，我遇到了这个错误：
&lt;块引用&gt;
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版。

如何解决？
我使用 PyCharm 作为 Python 代码编辑器。
我尝试了pip install darts，但它没有安装所有软件包并遇到此错误错误：subprocess-exited-with-error
 用于安装构建依赖项的 pip 子进程未成功运行。
  │ 退出代码：1
  ╰─&gt; 【136行输出】
      正在收集setuptools&gt;=64.0
        从 https://files.pythonhosted.org/packages 获取 setuptools&gt;=64.0 的依赖信息

然后，我尝试使用 pip install torch 安装 torch 并遇到此错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版]]></description>
      <guid>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</guid>
      <pubDate>Wed, 10 Jan 2024 10:16:06 GMT</pubDate>
    </item>
    <item>
      <title>scikit 的 RFECV 类如何计算 cv_results_？</title>
      <link>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</link>
      <description><![CDATA[我对递归特征消除交叉验证的理解： (sklearn.feature_selection.RFECV) 您提供一种算法，该算法在整个数据集上进行训练并创建特征重要性排名使用属性 coef_ 或 feature_importances_。现在包含了所有功能，该算法通过交叉验证进行评估。然后，删除排名底部的特征，并在数据集上重新训练模型并创建新的排名，再次通过交叉验证进行评估。此过程一直持续到除了一个特征之外的所有特征都保留下来（或由 min_features_to_select 指定），并且最终选择的特征数量取决于产生最高 CV 分数的特征。 （来源)
问题：每个特征数量的 CV 分数存储在 rfecv.cv_results_[“mean_test_score”] 中，我在尝试复制时遇到了麻烦这些分数无需使用 scikit 的内置方法。
这是我试图获得 n-1 个特征的分数，其中 n 是特征总数。
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_validate
从 sklearn.feature_selection 导入 RFECV

alg = DecisionTreeClassifier(random_state = 0)
cv_split = 分层KFold(5)
# train 是 pandas 数据框，x_var 和 y_var 都是包含变量字符串的列表
X = 火车[x_var]
y = np.ravel(train[y_var])

alg.fit(X, y)
最低排名特征 = np.argmin(alg.feature_importances_)
x_var.pop(最低排名特征)

one_removed_feature = 火车[x_var]
alg.fit(one_removed_feature, y)
cv_score = cross_validate(alg, one_removed_feature, y, cv=cv_split, 评分=“准确度”)
np.mean(cv_score[“test_score”])

这是提供不同分数的内置方法：
&lt;前&gt;&lt;代码&gt;rfecv = RFECV(
    估计量=alg,
    步骤=1，
    CV=CV_分裂，
    评分=“准确度”，
）

rfecv.fit(X, y)
rfecv.cv_results_[“mean_test_score”][-2]

如何获得内置方法中计算出的准确分数？
我还想提一下，我确实首先尝试了所有 n 个功能，并且我的方法与
rfecv.cv_results_[“mean_test_score”][-1]。]]></description>
      <guid>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</guid>
      <pubDate>Tue, 09 Jan 2024 16:47:46 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于 Transformer 模型，该模型处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率：一个自定义 LR 调度程序，它复制在“Attention is”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>如何正确缩放、训练和拟合分类器管道中的数据</title>
      <link>https://stackoverflow.com/questions/74258306/how-to-scale-train-and-fit-data-in-classifier-pipeline-correctly</link>
      <description><![CDATA[我正在尝试扩展我的数据并训练分类器。我当前的数据框如下所示：
col1 col2 col3 类别
---- ---- ---- --------
....

我对分类器管道中的 StandardScaler 如何影响我的数据感到困惑。这是我的主要问题：

Scaler 也会缩放 Y_train 吗？这在机器学习的背景下真的很重要吗？
缩放器会在预测期间自动缩放 X_test 吗？如果没有，我该如何使用之前计算的指标来做到这一点？
我是否遗漏了缩放和拆分方面的一些基本内容？

这些文档有点含糊，所以希望有人能澄清这一点。非常感谢！
目前，我的管道如下所示：
分类器 = Pipeline(steps=[(“scaler”, StandardScaler()), (&#39;svc&#39;, SVC(kernel=“线性”, C=c))])

features = data.loc[:, data.columns != &#39;类别&#39;]
类别 = 数据[&#39;类别&#39;]

X_train, X_test, Y_train, Y_test = train_test_split(特征, 类别, train_size=0.7)

分类器.fit(X_train, Y_train)

分类器.预测(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/74258306/how-to-scale-train-and-fit-data-in-classifier-pipeline-correctly</guid>
      <pubDate>Mon, 31 Oct 2022 02:49:27 GMT</pubDate>
    </item>
    <item>
      <title>在计算测试数据的准确性时遇到此错误</title>
      <link>https://stackoverflow.com/questions/73498671/encountered-this-error-when-calculating-the-accuracy-on-a-test-data</link>
      <description><![CDATA[我是机器学习新手，在计算和返回测试数据的准确性时遇到此错误
def NBAaccuracy(features_train, labels_train, features_test, labels_test):
    ”“”计算朴素贝叶斯分类器“”的准确性
    ### 导入 GaussianNB 的 sklearn 模块
    从 sklearn.naive_bayes 导入 GaussianNB

    ### 创建分类器
    clf = GaussianNB() #TODO

    ### 将分类器拟合到训练特征和标签上
    clf.fit(features_train, labels_train, features_test, labels_test) #TODO

    ### 使用经过训练的分类器来预测测试特征的标签
    pred = clf.predict(features_test, labels_test) #TODO

    ### 计算并返回测试数据的准确性
    ### 这与示例略有不同，
    ### 我们只打印准确性
    ### 你可能需要导入 sklearn 模块
    从 sklearn.metrics 导入 precision_score
    准确度=准确度_分数（features_test，labels_test，normalize = False）#TODO
    返回精度
    返回NBA准确率

我收到此错误：
&lt;块引用&gt;
类型错误：fit() 最多接受 4 个参数（给定 5 个）
]]></description>
      <guid>https://stackoverflow.com/questions/73498671/encountered-this-error-when-calculating-the-accuracy-on-a-test-data</guid>
      <pubDate>Fri, 26 Aug 2022 09:10:51 GMT</pubDate>
    </item>
    </channel>
</rss>