<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 13 Feb 2024 03:15:34 GMT</lastBuildDate>
    <item>
      <title>如何正确准备数据集，设置 EfficientNetV2B0 模型以使用 Tensorflow 在自定义数据集上进行训练</title>
      <link>https://stackoverflow.com/questions/77985276/how-to-properly-prepare-dataset-setting-up-efficientnetv2b0-model-for-training</link>
      <description><![CDATA[我对机器学习还是新手。我想问我的代码有什么问题，结果模型没有正确分类苹果叶病。我在 4 个类的 7k 图像数据集上进行了训练，每个类都有大约 1.8k 的图像。每个类的总图像不相等，这会影响训练结果吗？或者我下面的代码有问题吗？
从 google.colab 导入驱动器
驱动器.mount(&#39;/content/gdrive&#39;)


导入压缩文件
zip_ref = zipfile.ZipFile(&#39;/content/gdrive/MyDrive/dataset/data9k.zip&#39;, &#39;r&#39;)
zip_ref.extractall(“/内容/数据集”)
zip_ref.close()


将张量流导入为 tf
从tensorflow.keras.applications.imagenet_utils导入preprocess_input
将 matplotlib.pyplot 导入为 plt


train_dataset = tf.keras.utils.image_dataset_from_directory(
&#39;/内容/数据集/数据集/火车&#39;,
批量大小=10，
图像大小=(224, 224),
标签=&#39;推断&#39;,
label_mode=&#39;分类&#39;
）


validation_dataset = tf.keras.utils.image_dataset_from_directory(
&#39;/内容/数据集/数据集/测试&#39;,
批量大小=10，
图像大小=(224, 224),
标签=&#39;推断&#39;,
label_mode=&#39;分类&#39;
）


val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = valid_dataset.take(val_batches // 5)
validation_dataset =validation_dataset.skip(val_batches // 5)


自动调谐 = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset =validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)


data_augmentation = tf.keras.Sequential(
[tf.keras.layers.RandomFlip(&#39;水平&#39;),
tf.keras.layers.RandomRotation(0.2)]
）


模型 = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
include_top=假，
权重=无，
输入张量=无，
输入形状=(224, 224, 3),
池=&#39;平均&#39;，
include_preprocessing=True
）


#model.trainable=False


Prediction_layer = tf.keras.layers.Dense(4, 激活=&#39;softmax&#39;)


输入 = tf.keras.Input(形状=(224, 224, 3))
x = 数据增强（输入）
#x = 预处理输入(x)
x = 模型(x)
x = tf.keras.layers.Dropout(0.2)(x)
输出=预测层(x)
模型= tf.keras.Model（输入，输出）


模型.编译(
优化器=tf.keras.optimizers.Adam(learning_rate=1e-4),
损失=tf.keras.losses.CategoricalCrossentropy(),
指标=[&#39;准确性&#39;]
）


模型.拟合(
训练数据集，
验证数据=验证数据集，
纪元=10
）


将 numpy 导入为 np
从tensorflow.keras.preprocessing导入图像


img_path = &#39;gdrive/MyDrive/dataset/rust.jpg&#39;
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, 轴=0)


预测 = model.predict(img_array)


class_names = [&#39;apple_scab&#39;, &#39;black_rot&#39;, &#39;cedar_apple_rust&#39;, &#39;healthy&#39;]


Predicted_index = np.argmax(预测[0])
标签=类名[预测索引]
print(&quot;预测标签：&quot;, label)
]]></description>
      <guid>https://stackoverflow.com/questions/77985276/how-to-properly-prepare-dataset-setting-up-efficientnetv2b0-model-for-training</guid>
      <pubDate>Tue, 13 Feb 2024 02:23:46 GMT</pubDate>
    </item>
    <item>
      <title>电话前缀的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/77984818/machine-learning-model-for-phone-prefixes</link>
      <description><![CDATA[我的手机有一个带有 LDAP 接口的 PHP 应用程序。该应用程序会在我的地址簿中搜索来电的电话号码并显示来电者的姓名。
地址簿条目组属于同一家公司，因此关联的电话号码共享相同的前几位数字，但分机号不同。如果一个不在我的地址簿中的人打电话来，我希望确定他或她最有可能来自哪家公司。
当然，我可以手动提取所有客户的基本号码并检查它们是否有未知的分机号，但是随着地址簿条目的不断添加，基本号码列表的更新会很繁琐。我正在考虑一种机器学习模型，可以使用所有可用条目进行训练，例如每天一次，并确定主叫号码最有可能的客户。
由于在机器学习方面经验不足，我很难选择合适的学习模型。决策树或随机森林一开始似乎是一个好的开始，但后来我意识到它们没有利用数字的序列。
那么，基于电话号码进行客户识别的良好机器学习方法是什么？目前，我正在查看约 1500 个地址簿条目，其中约 95% 来自 8 个不同的客户。一位客户有 1 到 6 个基数。该应用程序一半是为了结果，一半是为了 MLing 的乐趣。]]></description>
      <guid>https://stackoverflow.com/questions/77984818/machine-learning-model-for-phone-prefixes</guid>
      <pubDate>Mon, 12 Feb 2024 23:22:32 GMT</pubDate>
    </item>
    <item>
      <title>R² 与解释方差 - 它们的数学公式完全相同，结果为何会不同？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77984814/r%c2%b2-vs-explained-variance-their-mathematical-formulas-are-exactly-the-same-how</link>
      <description><![CDATA[在机器学习 (ML) 和数据科学中的预测值评估中，有两种测量方法可以评估预测变量 R² 和解释值。我注意到R²的数学公式和解释的值是完全一样的：
在此处输入图像描述
在此处输入图像描述
但是基于Python中的Scikit-learn库以及我在网上查到的几篇参考文献，它们的结果值可能不同，为什么计算这些值的数学公式相同但结果可能不同？
我实际上在带有 Scikit 库的 Python 上尝试了这两个，结果可能真的不同！太奇怪了！]]></description>
      <guid>https://stackoverflow.com/questions/77984814/r%c2%b2-vs-explained-variance-their-mathematical-formulas-are-exactly-the-same-how</guid>
      <pubDate>Mon, 12 Feb 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：形状 (7998,18) 和 (1498,18) 未对齐：18（暗淡 1）！= 1498（暗淡 0）</title>
      <link>https://stackoverflow.com/questions/77984676/valueerror-shapes-7998-18-and-1498-18-not-aligned-18-dim-1-1498-dim</link>
      <description><![CDATA[我已将数据框拆分为 80-5-15 进行数据训练，并且还对其运行序数 Logistic 回归。训练数据和OLR都很好，但准确率测试失败。我的代码是：
X_train_val_a1，X_test_a1，y_train_val_a1，y_test_a1 = train_test_split（X，y，test_size = 0.15，random_state = 42）X_train_a1，X_val_a1，y_train_a1，y_val_a1 = train_test_split（X_train_val_a1，y_train_val_a1，测试大小=0.059，随机状态=42）

model_a1 = OrderedModel(y_train_a1, X_train_a1, distr=&#39;logit&#39;) # 直接使用X_trainresult = model_a1.fit()print(result.summary())

pred_a1 = model_a1.predict(X_test_a1)prediction_a1 = list(map(round, pred_a1))print(&#39;\n测试精度 = &#39;, precision_score(y_test_a1, Prediction_a1))

错误出现在
 pred_a1 = model_a1.predict(X_test_a1)

错误信息是：
ValueError：形状 (7998,18) 和 (1498,18) 未对齐：18 (dim 1) != 1498 (dim 0)

如何解决这个问题？
谢谢，我希望我能说清楚:)]]></description>
      <guid>https://stackoverflow.com/questions/77984676/valueerror-shapes-7998-18-and-1498-18-not-aligned-18-dim-1-1498-dim</guid>
      <pubDate>Mon, 12 Feb 2024 22:36:17 GMT</pubDate>
    </item>
    <item>
      <title>语音识别 - 避免过滤填充词[关闭]</title>
      <link>https://stackoverflow.com/questions/77984412/speech-recognition-avoid-filtering-of-filler-words</link>
      <description><![CDATA[我想使用基本的语音识别并测量文本中填充词的数量，例如“啊”、“嗯”等。
但是，我遇到的问题是 Whisper 和 SpeechRecognition 等可用工具似乎会自动过滤掉所有填充词（或者接受了忽略这些填充词的数据训练）。
我该如何解决这个问题？或者我必须拿出我的计票器吗？
到目前为止，我只尝试了 Whisper 和 Python 的语音识别，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77984412/speech-recognition-avoid-filtering-of-filler-words</guid>
      <pubDate>Mon, 12 Feb 2024 21:26:55 GMT</pubDate>
    </item>
    <item>
      <title>使用哪个数据库来存储机器学习数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77984309/which-database-to-use-for-storing-data-for-machine-learning</link>
      <description><![CDATA[我在任何地方都找不到太多答案，所以我希望有人能给我一些有关这个主题的信息。
我想知道什么数据库适合存储机器学习数据集（大型 JSON 或文本文件）。据我所知，选择非关系数据库比关系数据库更好。但仍然有各种各样的工具，例如 HBase、MongoDB、Cassandra 或 HDF5 等。
我找不到这些机器学习工具的任何基准或比较。]]></description>
      <guid>https://stackoverflow.com/questions/77984309/which-database-to-use-for-storing-data-for-machine-learning</guid>
      <pubDate>Mon, 12 Feb 2024 21:03:45 GMT</pubDate>
    </item>
    <item>
      <title>CUML RandomForestClassifier TypeError 需要整数</title>
      <link>https://stackoverflow.com/questions/77984286/cuml-randomforestclassifier-typeerror-an-integer-is-required</link>
      <description><![CDATA[我正在尝试使用 CUML 的集成 RandomForestClassifier 来拟合我的数据。当我尝试安装时，出现类型错误，提示需要整数。
我的 X_train 是一个数据帧，我将其转换为 numpy 数组并确保类型为 float32。原始数据帧只有 int64 和 float32 类型，因为我使用 labelencoder 对字符串列进行编码，然后将其删除。
我的 Y_train 是一个系列，我将其转换为 numpy 数组，并尝试将其转换为 Int32 和 Int64（因为我相信这可能是错误所在）。
# 将 X_train DataFrame 转换为 float32 dtype 的 numpy 数组
X_train_np = X_train.values.astype(np.float32)

# 将 y_train 系列转换为 float32 dtype 的 numpy 数组
y_train_np = y_train.值


导入累积量
从 cuml.ensemble 导入 RandomForestClassifier
从 cuml.model_selection 导入 GridSearchCV

# 创建 cuML 随机森林分类器
cuml_classifier = RandomForestClassifier(random_state=42,n_streams=1)

# 定义网格搜索的参数grid
参数网格 = {
    &#39;最大深度&#39;：[5]，
    &#39;n_估计器&#39;：[5]，
    &#39;split_criterion&#39;: [&#39;熵&#39;]
}

# 创建 GridSearchCV 对象
网格搜索 = 网格搜索CV(
    cuml_分类器，
    参数网格=参数网格，
    评分=&#39;roc_auc&#39;,
    简历=3，
    return_train_score=真，
    改装=“AUC”，
    详细=1
）

# 将网格搜索拟合到训练数据中
grid_search.fit(X_train_np, y_train_np)

值错误：
3次配合全部失败。
您的模型很可能配置错误。
您可以尝试通过设置 error_score=&#39;raise&#39; 来调试错误。

以下是有关失败的更多详细信息：
-------------------------------------------------- ------------------------------------------
3 次拟合失败，出现以下错误：
回溯（最近一次调用最后一次）：
  文件“/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/sklearn/model_selection/_validation.py”，第 890 行，在 _fit_and_score 中
    estimator.fit(X_train, y_train, **fit_params)
  文件“/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/nvtx/nvtx.py”，第 115 行，在内部
    结果 = func(*args, **kwargs)
  文件“/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/cuml/internals/api_decorators.py”，第 188 行，包装器中
    ret = func(*args, **kwargs)
  文件“randomforestclassifier.pyx”，第 463 行，位于 cuml.ensemble.randomforestclassifier.RandomForestClassifier.fit
类型错误：需要一个整数

如何解决这个问题？
我已经检查了 y_train，所有值都按预期具有二进制 1 或 0，没有 NaN、None 或 null 值。
y_train 和 X_train 的长度相同。
我还尝试将 X_train 作为数据帧传递，将 y_train 作为序列传递，而不是转换为 numpy 数组。出现同样的错误。
我尝试了上述方法并保留了 astype(“float32”) 和 astype(“int32”)，但这给出了相同的错误。
# 创建 cuML 随机森林分类器
cuml_classifier = RandomForestClassifier(max_深度=5, n_estimators=5,random_state=42)

训练后的RF = cuml_classifier.fit(X_train_np, y_train_np)

预测 = cuml_classifier.predict(X_train)

cu_score = cuml.metrics.accuracy_score(y_train,预测)
sk_score = precision_score(y_train.values,预测)

print(&quot;累积准确度:&quot;, cu_score )
print(&quot;sklearn 准确度：&quot;, sk_score )

我尝试不使用 gridsearch，直接直接使用，结果没有错误。不过我仍然需要网格搜索。]]></description>
      <guid>https://stackoverflow.com/questions/77984286/cuml-randomforestclassifier-typeerror-an-integer-is-required</guid>
      <pubDate>Mon, 12 Feb 2024 20:56:17 GMT</pubDate>
    </item>
    <item>
      <title>MLFlow 可以在没有“with mlflow.start_run()”块的情况下使用吗？</title>
      <link>https://stackoverflow.com/questions/77983736/can-mlflow-be-used-without-the-with-mlflow-start-run-block</link>
      <description><![CDATA[我想跟踪整个笔记本并记录训练模型之前发生的清洁步骤的参数。我想使用 mlflow 来执行此操作，但在所有文档中，您似乎必须使用此格式跟踪模型：
与 mlflow.start_run():
    ...

有没有办法使用 mlflow 跟踪整个笔记本而不使用 with 块？]]></description>
      <guid>https://stackoverflow.com/questions/77983736/can-mlflow-be-used-without-the-with-mlflow-start-run-block</guid>
      <pubDate>Mon, 12 Feb 2024 19:05:13 GMT</pubDate>
    </item>
    <item>
      <title>二元预测的预测结果是否定的</title>
      <link>https://stackoverflow.com/questions/77982419/the-predictions-of-a-binary-prediction-are-negative</link>
      <description><![CDATA[我正在努力创建一个二进制模型。我以为一切正常，但当我发现模型关闭的频率很奇怪时，但当我尝试调整阈值时，我发现没有任何变化，所以就在那时我开始调查。
我检查了二元分类的预测值，发现其中大多数都是负值。
这是我的模型：
 public ITransformer TrainCategorialModel(IEnumerabletrainingData)
    {

        var 列名称 = typeof(TrainingCategorial)
            .GetProperties()
            .Where(property =&gt; property.DeclaringType != typeof(TrainingCategorial))
            .Select(属性 =&gt; 属性.名称)
            .ToArray();

        // 检查训练数据中的空值
        
        if (trainingData.Any(item =&gt; item == null))
        {
            throw new ArgumentException(“训练数据包含空值。”);
        }

        var pipeline = mLContext.Transforms.Concatenate(“特征”, columnNames)
            .Append(mLContext.BinaryClassification.Trainers.SdcaNonCalibrate(labelColumnName: &quot;CHPlabels&quot;, featureColumnName: &quot;Features&quot;));

        var data = mLContext.Data.LoadFromEnumerable(trainingData);

        var model = pipeline.Fit(data);

        返回模型；
    }

我的特征基于另一个类中的参数模型。
我的预测如下：
 公共列表; PredictCategorialModel（ITransformer 模型，IEnumerable 输入）
    {
        // 4. 转换数据
        IDataView 测试数据 = mLContext.Data.LoadFromEnumerable(input);

        // 5. 根据特征预测新值。
        列表&lt;浮动&gt; PredictedValues = mLContext.Data.CreateEnumerable(
            model.Transform(testingData),reuseRowObject: false)
            .Select(row =&gt; row.LabelPrediction)
            .ToList();

        // 应用阈值（例如 0.5）将分数转换为布尔预测
        变量阈值 = 0.3；
            列表&lt;布尔&gt; PredictedLabels = PredictedValues.Select(LabelPrediction =&gt; LabelPrediction &gt; 阈值).ToList();

        返回预测标签；
    }

我检查了我的数据，看起来没问题。如何解决这个问题？
更新：我认为问题出在模型中，我尝试了其他方法来创建预测布尔值，但遇到了相同的错误。我尝试过 LightGBM，因为我知道该模型不应该是线性的，但这产生了全新的问题（我对此也有一个未解答的问题）。有谁知道检查模型是否有效的好方法？]]></description>
      <guid>https://stackoverflow.com/questions/77982419/the-predictions-of-a-binary-prediction-are-negative</guid>
      <pubDate>Mon, 12 Feb 2024 15:09:31 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks MLFlow 中部署 ML 模型</title>
      <link>https://stackoverflow.com/questions/77982112/deploy-ml-model-in-databricks-mlflow</link>
      <description><![CDATA[我有一个在开发环境中训练的 ML 模型，现在我想将其部署在具有不同 URL 的 PROD databricks 中。我可以下载开发模型，然后手动将 pkl 文件上传到 PROD DBFS。下载模型后，如何在产品 MLFlow 中部署？
要下载我使用以下代码的模型
导入操作系统
导入流量
从 mlflow.store.artifact.models_artifact_repo 导入 ModelsArtifactRepository
mlflow.set_tracking_uri(“databricks”)
model_name = “测试模型”
model_stage = “无”
os.makedirs(“model2”,exist_ok=True)
local_path = ModelsArtifactRepository(f&#39;models:/{model_name}/{model_stage}&#39;).download_artifacts(“”, dst_path=&#39;model2&#39;)
print(f&#39;{model_stage} 模型 {model_name} 已在 {local_path} 下载&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77982112/deploy-ml-model-in-databricks-mlflow</guid>
      <pubDate>Mon, 12 Feb 2024 14:21:56 GMT</pubDate>
    </item>
    <item>
      <title>如何包装 keras 模型以供 scikit-learn 堆叠集成使用</title>
      <link>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</link>
      <description><![CDATA[我有一个已经训练过的 keras 模型列表。我想在 scikit learn 中将它们与 StackingClassifier 一起使用。由于 keras 没有 Predict_proba 方法，我创建了一个包装器。
如果使用我的为 VotingClassifier 包装的模型以及软方法和硬方法，它就可以工作。
但是当我使用堆叠模型时，第一次运行后，它会显示此错误。我没有找到任何相关信息。
类 KerasWrapperWithEncoder(BaseEstimator, ClassifierMixin):
    def __init__(自身，keras_model，classes_)：
        self.keras_model = keras_model
        self.encoder = OneHotEncoder(sparse_output=False)
        # L&#39;encoder OneHotEncoder 已经过去了
        self.classes_ = classes_ # 定义可分配类

    def fit(自身, X, y):
        # 模型已安装，不再适合
        y_reshape = y.reshape(-1, 1)
        self.encoder.fit(y_reshape)
        返回自我

    def 预测（自身，X）：
        # 利用 les modèles entraînés pour faire des predictions
        预测 = self.keras_model.predict(X)
        np_argmax = np.argmax(预测，轴=1)
        打印（预测）
        打印（np_argmax）
        返回 np_argmax

    def Predict_proba(自身, X):
        # 返回分类模型的类别概率
        概率 = self.keras_model.predict(X)
        print(&quot;概率形状：&quot;, probabilities.shape) # 调试
        返回概率


keras_wrapped_models_with_encoder = [
    (name.replace(&#39; &#39;, &#39;_&#39;).replace(&#39;__&#39;, &#39;_&#39;), KerasWrapperWithEncoder(model, _target_classes_))
    对于 keras_models.items() 中的名称、模型
]

vote_clf = 投票分类器(
         估计器=all_估计器，
         投票=&#39;软&#39;，
         n_职位=3，
         详细=真）
vote_clf .fit(X_train, y_train) # 完美运行

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
keras_stacking_models_current_year = StackingClassifier(
    估计器=all_估计器，
    Final_estimator=LogisticRegression(),
    简历=简历，
    详细=3，
    # n_jobs=2
）


keras_stacking_models_current_year.fit(X_train, y_train) # 抛出错误


&lt;前&gt;&lt;代码&gt;========================================stacking_models_all_models===== =======================
12105/12105 [================================] - 25s 2ms/步
概率形状：(387348, 3)
训练折叠 (1) 中的类数与类总数 (3) 不匹配。结果可能不适合您的用例。要解决此问题，请使用交叉验证技术来产生正确分层的折叠
_enforce_prediction_order（类、预测、n_classes、方法）
   第1457章
   第1458章）
-&gt;第1459章
   第1460章 1460
   第1461章 回归预测

ValueError：形状不匹配：形状（387348,3）的值数组无法广播到形状（387348,1,3）的索引结果
]]></description>
      <guid>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</guid>
      <pubDate>Mon, 12 Feb 2024 14:12:27 GMT</pubDate>
    </item>
    <item>
      <title>排名之间的注释者间协议[关闭]</title>
      <link>https://stackoverflow.com/questions/77977932/inter-annotator-agreement-between-ranking</link>
      <description><![CDATA[作为实验的一部分，我使用注释器对一组列表进行排名。每个列表包含不等、数量相对较少的项目。注释器根据每个项目的重要性对列表进行了重新排序。也就是说，重要的项目应该排名更高。这项任务是由多个注释者执行的，我想衡量他们之间的一致性。
我应该使用什么指标或如何制定数据，以便我可以使用常见的一致性指标，例如 Cohen 的 Kappa / Krippendorff Alpha？]]></description>
      <guid>https://stackoverflow.com/questions/77977932/inter-annotator-agreement-between-ranking</guid>
      <pubDate>Sun, 11 Feb 2024 18:28:44 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 自定义转换器从底层模型中抛出 NotFittedError</title>
      <link>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</link>
      <description><![CDATA[我想创建自己的 scikit-learn 转换器，用于对包含分类的数字特征进行编码，例如邮政编码或行业代码（NAICS、MCC 等）。在这些类型的代码中有一个结构：例如MCC 3000-3999 是“旅行和娱乐”，它进一步细分为更细粒度的类别，例如“航空公司”、“汽车租赁”等。我们不能将它们用作序数特征，但如果我们将它们视为纯分类特征（例如，通过 One -Hot-Encoding）我们需要选择在代码结构的哪个级别应用特征编码。
为了解决这个问题，我创建了自己的 scikit-learn 变压器，它是 TargetEncoder 使用决策树。代码如下所示。重要的是要认识到，在模型训练期间，应使用样本外决策树回归分数来避免过度拟合。因此，我实现了自己的 fit_transform 函数来生成这些样本外分数：
从 sklearn.tree 导入 DecisionTreeRegressor

从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.model_selection 导入 cross_val_predict

类 TaxonomyEncoder（TransformerMixin，BaseEstimator）：

def __init__(自身, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    自我简历 = 简历

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    返回自我

def 变换（自身，X）：
    返回 self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    返回 cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)

转换器工作正常，除非在 ColumnTransformer 中使用：
从 sklearn.compose 导入 ColumnTransformer

变压器 = ColumnTransformer([(&#39;分类法&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])

然后我得到决策树尚未拟合的错误：
NotFittedError：此 DecisionTreeRegressor 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

显然，scikit-learn 在导致此错误的表面下进行了一些检查。请注意，实际上没有理由需要拟合决策树，因为决策树是在 cross_val_predict 函数中重新拟合的。我该如何解决这个问题？
下面显示了重现该错误的完整工作示例：
导入 pandas 作为 pd
df = pd.DataFrame({&#39;mcc&#39;:[3000,3500,7339], &#39;y&#39;:[0,0,1]})

te = TaxonomyEncoder().fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
te.transform(df[[&#39;mcc&#39;]])

给出：
数组([[0.],
       [0.],
       [1.]])

并且 fit_transform 也给出了预期的结果：
te.fit_transform(df[[&#39;mcc&#39;]], df[&#39;y&#39;])

数组([[0.],
       [0.],
       [0.]])

但是当包装在 ColumnTransformer 中时，事情就会出错：
transformer = ColumnTransformer([(&#39;taxonomy&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])
]]></description>
      <guid>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</guid>
      <pubDate>Sun, 11 Feb 2024 12:34:35 GMT</pubDate>
    </item>
    <item>
      <title>RNN 的训练循环在每个 epoch 后返回相同的损失</title>
      <link>https://stackoverflow.com/questions/77938129/training-loop-of-rnn-returning-the-same-loss-after-each-epoch</link>
      <description><![CDATA[我正在尝试借助此存储库从头开始构建 RNN (https: //github.com/nicklashansen/rnn_lstm_from_scratch/tree/master），但每个时期后的训练损失保持不变。训练循环的代码如下：
# 超参数
纪元数 = 1000

# 初始化一个新网络
参数 = init_rnn(hidden_​​size=hidden_​​size, vocab_size=vocab_size)

# 将隐藏状态初始化为零
隐藏状态 = np.zeros((隐藏大小, 1))

# 轨迹丢失
训练损失、验证损失 = []、[]

def check_if_params_updated(old_params, new_params):
    # 该函数检查两组参数是否不同
    对于 zip 中的 old_param、new_param(old_params, new_params)：
        如果不是 np.array_equal(old_param, new_param):
            return True # 参数已更新
    return False # 参数尚未更新


# 对于每个纪元
对于范围内的 i（num_epochs）：
    
    # 轨迹丢失
    epoch_training_loss = 0
    epoch_validation_loss = 0
    
     # 对于验证集中的每个句子
    对于输入，val_loader 中的目标：
        
        # One-hot 编码输入和目标序列
        input_one_hot = one_hot_encode_sequence（输入，vocab_size）
        target_one_hot = one_hot_encode_sequence（目标，vocab_size）
        
        # 重新初始化隐藏状态
        隐藏状态 = np.zeros_like(隐藏状态)

        # 前向传递
        输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）

        # 向后传递
        损失，_ =向后传递（inputs_one_hot，输出，hidden_​​states，targets_one_hot，参数）
        
        # 更新损失
        epoch_validation_loss += 损失
    
    # 对于训练集中的每个句子
    对于输入，train_loader 中的目标：
        
        # One-hot 编码输入和目标序列
        input_one_hot = one_hot_encode_sequence（输入，vocab_size）
        target_one_hot = one_hot_encode_sequence（目标，vocab_size）
        
        # 重新初始化隐藏状态
        隐藏状态 = np.zeros_like(隐藏状态)

        # 前向传递
        输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）

        # 向后传递
        损失，梯度=backward_pass（inputs_one_hot，输出，hidden_​​states，targets_one_hot，参数）
        打印（inputs_one_hot.shape）
        
        如果 np.isnan(损失):
            raise ValueError(&#39;梯度消失/爆炸！&#39;)
        
        # 更新参数
        params = update_parameters(params, grads, lr=1e-3)
        
        # 更新损失
        epoch_training_loss += 损失
        
    # 保存绘图损失
    Training_loss.append(epoch_training_loss/len(training_set))
    validation_loss.append(epoch_validation_loss/len(validation_set))

    # 每 100 个 epoch 打印损失
    如果我％100==0：
        print(f&#39;Epoch {i}, 训练损失: {training_loss[-1]}, 验证损失: {validation_loss[-1]}&#39;)


# 获取测试集中的第一个句子
输入，目标 = test_set[1]

# One-hot 编码输入和目标序列
input_one_hot = one_hot_encode_sequence（输入，vocab_size）
target_one_hot = one_hot_encode_sequence（目标，vocab_size）

# 将隐藏状态初始化为零
隐藏状态 = np.zeros((隐藏大小, 1))

# 前向传递
输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）
output_sentence = [idx_to_word[np.argmax(output)] 用于输出中的输出]
print(&#39;输入句子：&#39;)
打印（输入）

print(&#39;\n目标序列:&#39;)
打印（目标）

print(&#39;\n预测序列:&#39;)
print([idx_to_word[np.argmax(output)] 用于输出中的输出])

# 绘制训练和验证损失图
纪元 = np.arange(len(training_loss))
plt.figure()
plt.plot(epoch, Training_loss, &#39;r&#39;, label=&#39;训练损失&#39;,)
plt.plot(epoch,validation_loss,&#39;b&#39;,label=&#39;验证损失&#39;)
plt.图例()
plt.xlabel(&#39;Epoch&#39;), plt.ylabel(&#39;NLL&#39;)
plt.show()

我尝试检查我的参数是否正在更新，它们确实更新了，还尝试检查梯度，它们并不是指数小。每次迭代后损失都会减少，但总 epoch 的损失会增加。您可以在存储库中找到完整的代码，其中包括前向和后向传递(https://github.com/危险dude237/RNN_From_Scratch）。
编辑：因此，在每个时期修复后损失保持不变的问题，我需要更改损失函数。但现在每个 epoch 后的损失都会增加。]]></description>
      <guid>https://stackoverflow.com/questions/77938129/training-loop-of-rnn-returning-the-same-loss-after-each-epoch</guid>
      <pubDate>Mon, 05 Feb 2024 00:28:27 GMT</pubDate>
    </item>
    <item>
      <title>Hyperopt 在执行期间设置超时并修改空间</title>
      <link>https://stackoverflow.com/questions/24673739/hyperopt-set-timeouts-and-modify-space-during-execution</link>
      <description><![CDATA[如果有人可以提供帮助：

如何为每个单独的测试设置超时？整个实验超时？
如何设置渐进策略，在实验的不同阶段（同时使用当前的优化算法）消除/修剪搜索空间中得分最差的分支的百分比？ IE。在最大总实验的 30% 时，它可以删除 50% 得分最差的分类器及其所有超参数分支，以将其从即将进行的测试中删除。然后，同样的过程在 60% 时...

非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/24673739/hyperopt-set-timeouts-and-modify-space-during-execution</guid>
      <pubDate>Thu, 10 Jul 2014 10:05:09 GMT</pubDate>
    </item>
    </channel>
</rss>