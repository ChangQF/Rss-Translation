<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 22 Sep 2024 21:14:36 GMT</lastBuildDate>
    <item>
      <title>预处理 COCO2017 数据集时出错</title>
      <link>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</link>
      <description><![CDATA[我正在使用 COCO2017 训练 mobilenetV2 来检测人。我在预处理数据集以将其更改为 Tensorflow 数据集时遇到了困难。当我设法进行更改时，它无法正确解析，导致我在执行 model.fit() 时出错。如何解决这个问题？
# 加载 COCO 数据集
(ds_train, ds_val), ds_info = tfds.load(
&#39;coco/2017&#39;,
split=[&#39;train[:80000]&#39;, &#39;validation&#39;],
with_info=True
)
IMG_SIZE = 224
NUM_CLASSES = 80 # 有效类别的数量，对于无效样本，另加一个

def preprocess(sample):
image = sample[&#39;image&#39;]

# 检查 &#39;objects&#39; 和 &#39;label&#39; 是否存在且有效
if &#39;objects&#39; in sample and &#39;label&#39; in sample[&#39;objects&#39;] and len(sample[&#39;objects&#39;][&#39;label&#39;]) &gt; 0:
label = tf.cast(sample[&#39;objects&#39;][&#39;label&#39;][0], tf.int64)
else:
# 为无效标签分配默认类（例如，额外的类 NUM_CLASSES）
label = tf.cast(NUM_CLASSES, tf.int64)

# 调整图像大小并进行预处理
image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
image = tf.keras.applications.mobilenet_v2.preprocess_input(image)

return image, label

# 应用预处理
ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE)

我得到：
TypeError Traceback (most recent call last) Cell在[48]中，第 20 行 17 返回图像，标签 19 # 应用预处理 ---&gt; 20 ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) 21 ds_val = ds_val.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) TypeError: 在用户代码中：

TypeError: outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.&lt;lambda&gt;() 需要 1 个位置参数，但给出了 2 个

我尝试了不同的预处理方法。基本上，我需要将输入图像设置为 224x224，以用于 mobilenetv2 模型。我尝试获取一个模型，以便可以在 Himax WE-I 上使用它。]]></description>
      <guid>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</guid>
      <pubDate>Sun, 22 Sep 2024 21:09:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用不同的深度神经网络机器学习算法利用水稻作物图像预测甲烷排放量和用水量[关闭]</title>
      <link>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</link>
      <description><![CDATA[我有水稻作物的图像，但我不知道如何使用深度神经网络的图像来预测甲烷排放量和用水量，而无需数据集的数值。我只有带土壤的水稻作物图像。
我尝试使用图像进行预测。我找不到预测甲烷排放量和用水量的正确代码。]]></description>
      <guid>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</guid>
      <pubDate>Sun, 22 Sep 2024 16:42:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么即使我每次都初始化模型，训练损失也会不断减少？</title>
      <link>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</link>
      <description><![CDATA[我试图重现 Neuromatch DL 课程中的这段摘录，在这里，我在 MNIST 图像数据集上训练一个多层感知器（在本例中没有隐藏层）。我注意到，每次运行整个代码时，初始损失和最终损失都在减少，在三次运行后从 (2.488, 0.0965) 减少到 (2.38,0.092)。
这是怎么发生的？由于 MLP 一次又一次地被初始化，权重不应该恢复为默认值吗？为什么模型拟合得更好？
我已将代码附在下面：
#加载数据集：
train_set, test_set = load_mnist_data(change_tensors=True) 

# 随机抽取 500 个索引的子集
subset_index = np.random.choice(len(train_set.data), 500)

# 我们将使用这些符号来表示训练数据和标签，以尽可能接近数学表达式。
X, y = train_set.data[subset_index, :], train_set.targets[subset_index]

loss_fn = F.nll_loss

cell_verbose = True # 仅用于切换是否打印损失

# 我相信，这是实际模型训练和优化开始的地方
partial_trained_model = MLP(in_dim=784, out_dim=10, hidden_​​dims=[])

if cell_verbose:
print(&#39;Init loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这与 np.log(10 = # of classes) 匹配

# 使用自适应梯度和动量调用优化器（有关此内容的更多信息，请参见第 7 节）
optimizer = optim.Adam(partial_trained_model.parameters(), lr=7e-4)
for i in range(200):
loss = loss_fn(partial_trained_model(X), y)
optimizer.zero_grad()
loss.backward()
optimizer.step()

if cell_verbose:
print(&#39;End loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这应该小于 1e-2

我添加了一个隐藏层，并且相同的特征仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</guid>
      <pubDate>Sun, 22 Sep 2024 10:27:57 GMT</pubDate>
    </item>
    <item>
      <title>训练过程因“RuntimeError：没有用于已保存叶子的梯度累加器！”而崩溃。[关闭]</title>
      <link>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</link>
      <description><![CDATA[在训练简单模型时，进程崩溃，在 loss.backward() 中出现 RuntimeError: No grad accumulator for a saved leaf!，但我确保所有需要计算梯度的数据都在 GPU 上。
def train_epoch(args, epoch, model, loss_fn, optim, dataloader, lr_scheduler=None, warmup_scheduler=None):
model.train()
dataloader.sampler.set_epoch(epoch)

mae_m, loss_m = AverageMeter(), AverageMeter()
calc_m, read_m = AverageMeter(), AverageMeter()
timer = Timer()
log_step = len(dataloader) // 11
if args.local_rank == 0:
args.writer.add_scalar(&#39;lr&#39;, optim.param_groups[0][&#39;lr&#39;], epoch)

mae_list, pred_list = [], []

for step, sample in enumerate(dataloader):
data, label = sample[&#39;data&#39;].cuda().requires_grad_(), sample[&#39;label&#39;].cuda()
read_m.add(timer.tiktok())

optim.zero_grad()
# (output, deep_output), attn = model(data)
output = model(data)
output = output.reshape(label.shape)
# loss = loss_fn(output, label) + loss_fn(deep_output, label)
loss = loss_fn(output, label)
loss.backward(retain_graph=True)

训练的代码如上。其中一个过程的错误如下所示：

我向 ChatGPT 寻求帮助，按照它的建议为 data 添加 .requires_grad_()（我认为这是不必要的）以确保它将计算梯度，并为 loss.backward() 添加 retain_graph=True。但仍然不起作用。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</guid>
      <pubDate>Sun, 22 Sep 2024 08:08:01 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</link>
      <description><![CDATA[它复杂吗？结果会是什么样子？从概念上讲，是否可以使用自动编码器进行特征提取并将其作为二元分类任务中的集成模型的输入？
6.定义自动编码器模型
def build_autoencoder(input_shape):
inputs = Input(shape=input_shape)
# Encoder
x = Conv2D(32, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(inputs)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Flatten()(x)
encoded = Dense(128,activation=&#39;relu&#39;)(x)

# 解码器
x = Dense(32 * 32 * 128,activation=&#39;relu&#39;)(encoded)
x = tf.keras.layers.Reshape((32, 32, 128))(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32,(3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
# 调整此处的上采样以返回原始大小
x = UpSampling2D((2, 2))(x) # 从 (7,7) 更改为 (2,2)
decoded = Conv2D(3, (3, 3),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)(x)

autoencoder = Model(inputs,decoded)
encoder = Model(inputs,coded)

autoencoder.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;)

return autoencoder,encoder

/**ValueError Traceback (most recent call last)
&lt;ipython-input-17-3da45b43efb2&gt; in &lt;cell line: 37&gt;()
35 
36 # 训练自动编码器
---&gt; 37 autoencoder.fit(train_gen, epochs=10, validation_data=valid_gen)

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in binary_crossentropy(target, output, from_logits)
673 for e1, e2 in zip(target.shape, output.shape):
674 如果 e1 不是 None 且 e2 不是 None 且 e1 != e2:
--&gt; 675 raise ValueError(
676 “参数 `target` 和 `output` 必须具有相同的形状。”
677 “收到：”

ValueError：参数 `target` 和 `output` 必须具有相同的形状。收到：target.shape=(None, 224, 224, 3)，output.shape=(None, 256, 256, 3)**/`在此处输入代码`
]]></description>
      <guid>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</guid>
      <pubDate>Sun, 22 Sep 2024 05:54:57 GMT</pubDate>
    </item>
    <item>
      <title>如何实时有效地检测图表和行为模式？[关闭]</title>
      <link>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</link>
      <description><![CDATA[如何使用 Python 或任何其他脚本语言实时有效地检测图表模式？
我想检测类似这种逻辑的各种模式
 1. 多个拒绝区域（多次拒绝区域）
2. 摆动高低区域
3. 拒绝预测（蜡烛的灯芯区域）
4. 流动性狩猎（通过突破高/低来获取流动性）

这种类型的模式以图表形式出现，但方式并不统一，这就是为什么我很难用手动条件进行编码。我该怎么做并创建一个模型来寻找进入机会？是否可以使用 DL/Generative AI 来实现，或者我如何实现这些参数来寻找机会？

这里，S&amp;D 是多重拒绝区；R 表示拒绝预测；两条红色水平线是两个摆动点
注意：由于我的代码很长，我分享了整个文件。我的尝试在这里，5 分钟的数据集在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</guid>
      <pubDate>Sun, 22 Sep 2024 05:33:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CycleGAN 实现用于图像转换的 Web 应用程序？</title>
      <link>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</link>
      <description><![CDATA[我正在尝试创建一个使用 CycleGAN 模型进行图像转换的 Web 应用程序。该应用程序应该有一个用户友好的前端，用户可以在其中上传图像，以及一个后端，使用预先训练的 CycleGAN 模型处理图像，并将转换后的图像返回给用户。
一个有效的 Web 应用程序，用户可以通过前端上传图像。
后端应该接收图像，通过 CycleGAN 模型进行处理，并返回转换后的图像。
前端应该向用户显示转换后的图像。]]></description>
      <guid>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</guid>
      <pubDate>Sun, 22 Sep 2024 04:22:01 GMT</pubDate>
    </item>
    <item>
      <title>运行随机森林生存模型 (rfsrc) 时 R 会话中止 [关闭]</title>
      <link>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc</link>
      <description><![CDATA[我正在使用 R 中的“randomForestSRC”包进行生存分析项目。不幸的是，每次我尝试运行随机森林生存模型 (rfsrc) 时，我的 R 会话都会崩溃并显示以下消息：
R 遇到致命错误。会话已终止。

这是我到目前为止所做的：
在此处输入图像描述

数据：数据集已清理，没有缺失值，由生存时间 (time_month) 和事件状态 (死亡) 组成。我已经成功地在这个数据集上运行了其他生存模型（例如，bnnsurvival、coxph），没有任何问题。

模型设置：

我为随机森林模型创建了公式，如下所示：



 formula_rfsrc &lt;- as.formula(paste(&quot;Surv(time_month, death) ~&quot;, paste(pred_vars, collapse = &quot; + &quot;)))


然后我尝试拟合模型：

 fit &lt;- rfsrc(formula_rfsrc, data = df_train, ntree = 50)


尝试过修复：

检查公式：我已验证公式正确，对生存对象使用 Surv()。
减少数据大小：我尝试在较小的数据子集上运行模型。
限制树深度：我使用了 nodesize、nodedepth 等参数，并减少了树的数量。
内存管理：我确保在模型拟合之前调用垃圾收集 (gc())，认为问题可能与内存有关。



尽管付出了这些努力，但每当我运行随机森林模型时，R 会话都会崩溃。
其他详细信息：
数据集有大约 30 个预测变量，我根据需要将其转换为因子或数字。
我正在运行它R 4.4.1.
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc</guid>
      <pubDate>Sat, 21 Sep 2024 21:33:20 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>如果训练损失始终低于验证损失，是否表明过度拟合？</title>
      <link>https://stackoverflow.com/questions/77998300/if-the-training-loss-is-always-lower-than-the-validation-loss-does-that-indicat</link>
      <description><![CDATA[
我正在 MNIST 数据上训练 CNN 模型，并设置验证分割 = 0.2，而批处理大小 = 128，epochs = 5。训练模型后，我在训练和验证中获得的准确率超过 99%。当我绘制学习曲线时，我得到的图表的训练和验证损失非常低，但训练损失始终高于验证损失。我想知道这是过度拟合的情况吗？]]></description>
      <guid>https://stackoverflow.com/questions/77998300/if-the-training-loss-is-always-lower-than-the-validation-loss-does-that-indicat</guid>
      <pubDate>Thu, 15 Feb 2024 03:30:34 GMT</pubDate>
    </item>
    <item>
      <title>希望使用 Google ML 套件识别面部类型</title>
      <link>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</link>
      <description><![CDATA[我想使用 Google ML 找出一个人的脸型，比如方形、椭圆形或圆形。我知道我需要从不同角度测量脸的长度和宽度来确定脸型。但是，API 并没有给我提供这些。任何输入都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</guid>
      <pubDate>Sun, 12 Mar 2023 17:03:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 迭代训练两个模型</title>
      <link>https://stackoverflow.com/questions/73618331/train-two-model-iteratively-with-pytorch</link>
      <description><![CDATA[我希望训练两个级联网络，例如 X-&gt;Z-&gt;Y，Z=net1(X)，Y=net2(Z)。
我希望迭代地优化这两个网络的参数，即对于 net1 的某个参数固定，首先使用 MSE(predY,Y) 损失函数训练 net2 的参数，直到收敛；然后使用收敛后的 MSE 损失函数训练 net1 的迭代，以此类推。
因此，我为每个网络分别定义了两个优化器。我的训练代码如下：
net1 = SimpleLinearF()
opt1 = torch.optim.Adam(net1.parameters(), lr=0.01)
loss_func = nn.MSELoss()

for itera1 in range(num_iters1 + 1):
predZ = net1(X)

net2 = SimpleLinearF()
opt2 = torch.optim.Adam(net2.parameters(), lr=0.01)
for itera2 in range(num_iters2 + 1):
predY = net2(predZ)
loss = loss_func(predY,Y)
if itera2 % (num_iters2 // 2) == 0:
print(&#39;iteration: {:d}, loss: {:.7f}&#39;.format(int(itera2), float(loss)))
loss.backward(retain_graph=True)
opt2.step()
opt2.zero_grad()

loss.backward()
opt1.step()
opt1.zero_grad()

但是，我遇到了以下错误：
RuntimeError：梯度计算所需的变量之一已被 
inplace 操作修改：[torch.FloatTensor [1, 1]]，即 AsStridedBackward0 的输出 0，处于 
版本 502；预期版本为 501。提示：启用异常检测以查找 
无法计算其梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

为什么会出现此错误以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/73618331/train-two-model-iteratively-with-pytorch</guid>
      <pubDate>Tue, 06 Sep 2022 07:36:45 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 - 提取阈值区域特征以进行物体检测</title>
      <link>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</guid>
      <pubDate>Thu, 18 Jun 2020 03:47:25 GMT</pubDate>
    </item>
    <item>
      <title>随机森林比线性回归差。这是正常的吗？原因是什么？</title>
      <link>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</link>
      <description><![CDATA[我正在尝试使用机器学习来预测数据集。这是一个具有 180 个输入特征和 1 个连续值输出的回归问题。我尝试比较神经网络、随机森林回归和线性回归。
正如我所料，3 隐藏层神经网络的表现优于其他两种方法，均方根误差 (RMSE) 为 0.1。然而，我意外地发现随机森林的表现甚至比线性回归更差（RMSE 0.29 vs. 0.27）。正如我所料，随机森林可以发现特征之间更复杂的依赖关系以减少错误。我尝试调整随机森林的参数（树数、最大特征、max_depth 等）。我也尝试了不同的 K 交叉验证，但性能仍然不如线性回归。
我在网上搜索了一下，一个答案说，如果特征对协变量具有平滑、近乎线性的依赖性，线性回归可能会表现得更好。我没有完全理解这一点，因为如果是这样的话，深度神经网络不应该带来很大的性能提升吗？
我很难给出解释。在什么情况下，随机森林比线性回归更差，但深度神经网络可以表现得更好？]]></description>
      <guid>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</guid>
      <pubDate>Thu, 04 Jan 2018 01:58:35 GMT</pubDate>
    </item>
    </channel>
</rss>