<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 01 Jun 2024 18:18:15 GMT</lastBuildDate>
    <item>
      <title>为什么使用相同的 Q 值表会得到不同的测试结果</title>
      <link>https://stackoverflow.com/questions/78564460/why-do-i-get-different-testing-result-using-the-same-q-value-table</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78564460/why-do-i-get-different-testing-result-using-the-same-q-value-table</guid>
      <pubDate>Sat, 01 Jun 2024 17:17:18 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch（pip 或 conda）</title>
      <link>https://stackoverflow.com/questions/78564366/pytorch-pip-or-conda</link>
      <description><![CDATA[我最近安装了 Pytorch（pip 包），后来发现我已经有一个 Pytorch（conda 包）。现在我不确定要保留哪一个，或者是否应该保留两者。另外，我是一个初学者，所以我不知道如何从这里继续。
我刚开始学习 ML，我听说 conda 更适合它，但我不确定。请帮助我]]></description>
      <guid>https://stackoverflow.com/questions/78564366/pytorch-pip-or-conda</guid>
      <pubDate>Sat, 01 Jun 2024 16:36:57 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 自定义对象检测希望在 Windows 11 原生 GPU 支持下运行</title>
      <link>https://stackoverflow.com/questions/78564211/tensorflow-custom-object-detection-want-work-in-windows-11-native-with-gpu-suppo</link>
      <description><![CDATA[我最近尝试在 Windows 11 本地机器上使用 Tensorflow 2.10 创建自定义对象检测模型。但 Tensorflow 对象检测 API model_builder_tf2_test.py 出现错误。
ImportError：无法从“tensorflow.python.framework”导入名称“tensor”
我参考了许多文章和论文，其中大多数需要使用 TensorFlow 版本 2.13 来解决这个问题 Tensorflow InceptionV4 ImportError：无法从“tensorflow.python.framework”导入名称“tensor”。它解决了这个问题，但 Tensorflow 2.13 不支持 Windows 原生中的 GPU 访问。如何在 Windows 11 中执行此操作？
我参考了许多文章和论文，其中大多数需要使用 TensorFlow 版本 2.13 来解决这个问题 Tensorflow InceptionV4 ImportError：无法从“tensorflow.python.framework”导入名称“tensor”。它解决了这个问题，但 Tensorflow 2.13 不支持 Windows 原生的 GPU 附件。
我想知道在 Windows 11 中有没有可能做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78564211/tensorflow-custom-object-detection-want-work-in-windows-11-native-with-gpu-suppo</guid>
      <pubDate>Sat, 01 Jun 2024 15:36:56 GMT</pubDate>
    </item>
    <item>
      <title>在 sagemaker 中部署 llama-3 8B 时出错：标记器不匹配？</title>
      <link>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</link>
      <description><![CDATA[尝试部署 meta/llama-3-8B-Instruct 时，我在 sagemaker 代码编辑器中收到以下错误：
“您从此检查点加载的 tokenizer 类与调用此函数的类的类型不同。这可能会导致意外的标记化。
您从此检查点加载的 tokenizer 类是“PreTrainedTokenizerFast”。
调用此函数的类是“LlamaTokenizer”。&quot;
我正在使用 Huggingface API 将模型从 HF Hub 直接加载到 sagemaker 代码编辑器，使用 AWS 中的 ml.g5xlarge 实例类型。使用 HF API 不需要或公开 tokenizer。]]></description>
      <guid>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</guid>
      <pubDate>Sat, 01 Jun 2024 09:35:46 GMT</pubDate>
    </item>
    <item>
      <title>Seq2Seq LSTM 模型是否适用于具有不变参数的多元时间序列预测</title>
      <link>https://stackoverflow.com/questions/78562876/should-the-seq2seq-lstm-model-be-used-for-multivariate-time-series-forecasting-w</link>
      <description><![CDATA[我正在使用 Seq2Seq LSTM 模型来预测两个时间序列变量（Value1 和 Value2）。我的数据集包含 2000 组实验，每组有多个时间步骤。每组包含 11 个与 value1 和 value2 相关的参数，请注意，与其他数据不同，每组中的 11 个参数不会随时间变化，但每组之间的 11 个参数并不相同。这是我的数据的示例图：
数据解释
您可以清楚地看到，我的数据集中的第 1 到第 11 个参数不会随时间变化，但每组中的 11 个参数不同，并决定了 value1 和 value2 的最终时间序列
我的问题是，选择 seq2seq，即编码器-解码器 LSTM 模型是最佳选择吗？或者这更像是回归分类问题？任何建议都将不胜感激
此外，我基于 Keras 构建了一个 seq2seq 模型，并尝试了两个输入，第一个输入仅使用 11 个参数时间序列（尽管随时间不变）作为 x_train，使用 value1&amp;2 时间序列作为 y_train，验证集的损失几乎没有下降。第二个输入是 11 个参数，value1&amp;2 的时间序列向前移动一个时间步，即 teacher-forcing 方法。但是在我的预测中我还需要y_test的前向时间步长的序列，我对此感到困惑，这样做是否意味着我正在使用y_test来预测y_test？
以下是第二个输入的代码示例：
#编码器
encoder_inputs = Input(shape=(X_train.shape[1], X_train.shape[2])) #基于X_train的输入形状
encoder_lstm1 = LSTM(32, return_sequences=True, return_state=True)
encoder_outputs1, state_h1, state_c1 =coder_lstm1(encoder_inputs)
encoder_lstm2 = LSTM(32, return_state=True)
encoder_outputs2, state_h2, state_c2 =coder_lstm2(encoder_outputs1)

#解码器
decoder_inputs =输入（形状=（X_train.shape[1]，y_train.shape[2]））# 两个目标
decoder_lstm = LSTM（32，return_sequences=True，return_state=True）
decoder_outputs，_，_ =coder_lstm（decoder_inputs，initial_state=[state_h2，state_c2]）
decoder_dense = Dense（y_train.shape[2]）
decoder_outputs =coder_dense（decoder_outputs）

# 模型
model = Model（[encoder_inputs，decoder_inputs]，decoder_outputs）

# 训练
decoder_input_data = np.zeros_like（y_train）
decoder_input_data[:，1:] = y_train[:，:-1] # 将 y_test 值向前移动一步

LSTM_model = model.fit（[X_train，解码器输入数据]，y_train，validation_split=0.2，epochs=100，batch_size=64，verbose=1)

# 测试
解码器输入测试 = np.zeros_like(y_test)
解码器输入测试[:, 1:] = y_test[:, :-1] # 这里我使用 y_test 作为测试的输入
y_pred = model.predict([X_test, 解码器输入测试])
]]></description>
      <guid>https://stackoverflow.com/questions/78562876/should-the-seq2seq-lstm-model-be-used-for-multivariate-time-series-forecasting-w</guid>
      <pubDate>Sat, 01 Jun 2024 05:26:17 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习进行文本分类</title>
      <link>https://stackoverflow.com/questions/78562439/text-classification-with-ml</link>
      <description><![CDATA[我目前正在新工作中开展我的第一个 ML 项目。任务是通过 ML 模型将收到的电子邮件分类到 4 个单独的邮箱以降低成本。我成功训练的模型在不平衡数据集上的概率几乎达到 90%。
现在的问题是，如何强制我的模型仅在预测准确率至少为 95% 时才对此类电子邮件进行分类。其余的将转到第 5 个邮箱进行人工分类。
有什么想法吗？非常感谢。
Michal
我希望我的模型只会对可以至少 95% 准确率预测的电子邮件进行分类。]]></description>
      <guid>https://stackoverflow.com/questions/78562439/text-classification-with-ml</guid>
      <pubDate>Fri, 31 May 2024 23:32:55 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 模型返回一个元组而不是张量[关闭]</title>
      <link>https://stackoverflow.com/questions/78562431/pytorch-model-returns-a-tuple-instead-of-tensor</link>
      <description><![CDATA[我使用 pytorch 在 python 中编写了一个程序。我的问题是：
我的 模型返回的是元组而不是张量。我如何让/强制我的模型返回张量？
这是我的代码：
import torch
from torch import nn
import numpy as np
from torch.optim import Adam

x = np.linspace(0, 1, 50).reshape((-1, 1)).astype(&#39;float32&#39;)
y = np.power(x, 2).reshape((-1, 1)).astype(&#39;float32&#39;)

x = torch.tensor(x)
y = torch.tensor(y)

model = nn.Sequential(
nn.Linear(1, 1),
nn.ReLU()
)

loss = nn.MSELoss()
opt = Adam(model.parameters(), lr=0.001)
model.train()
n_batch = 4
n_epoch = 100
for i in range(n_epoch):
for b in range(0, len(x), n_batch):
inp = x[b:b+n_batch]
out = y[b:b+n_batch]
opt.zero_grad()
pred = model(inp),
ls = loss(pred, out)
ls.backward()
opt.step()

这是我得到的错误：

文件 &quot;...\torch\nn\ functional.py&quot;，第 3355 行，在 mse_loss 中
if not (target.size() == input.size()): AttributeError: &#39;tuple&#39; 对象没有属性 &#39;size&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78562431/pytorch-model-returns-a-tuple-instead-of-tensor</guid>
      <pubDate>Fri, 31 May 2024 23:27:37 GMT</pubDate>
    </item>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我了解，predict_proba(xtest) 将给我以下结果：
投票给分类器的树数/树数

我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1： 
--- 5, 0 类 A（已选择） 
10 
--- 2, 3 类 B（未选择） 

树 2： 
--- 3, 2 类 A（已选择） 
10 
--- 5, 0 类 B（未选择）

predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0

期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8

有人知道如何做到这一点，或者他们正在使用什么实现？
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    <item>
      <title>多输出分类器低分</title>
      <link>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</guid>
      <pubDate>Fri, 31 May 2024 09:01:14 GMT</pubDate>
    </item>
    <item>
      <title>即使指定了某些列，Pandas 也会获取数据框的所有列</title>
      <link>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</guid>
      <pubDate>Fri, 31 May 2024 08:59:38 GMT</pubDate>
    </item>
    <item>
      <title>成本函数最小值太低</title>
      <link>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</link>
      <description><![CDATA[下面是我正在尝试编写的神经网络的一段 Matlab 代码。这是我第一次尝试与机器学习相关的任何事情。我在这里跟随 Michael Nielson 的书：http://neuralnetworksanddeeplearning.com/chap2.html
我正在加载一组 60000 张 28x28 灰度手写数字图像，并尝试训练这个神经网络来识别它们。还有一个包含 10000 张图像的测试数据集。该网络有 784 个输入神经元（28^2），两个隐藏层，每个隐藏层有 16 个神经元，输出层有 10 个神经元。我将权重和偏差初始化为 -0.5 和 0.5 之间的随机值。
我将成本函数评估为 C = 0.5*(a-y).^2。它似乎取得了一定成功，因为它从 C=1.35 开始，在 C=0.46 结束，然后基本趋于平稳（大约 75 个时期）。但是，错误仍然很高，只有 12% 的时间能猜出正确的数字，这几乎是随机的。我反复检查了数学，但找不到错误。我想一定有一个我没有看到的。下面的代码是主训练循环中的所有内容，因此任何错误都应该在那里。我没有将图像分成更小的批次，而是在每个时期一次处理整个 60k 图像。由于每张图像只有 28x28 像素，因此无需将其分开就足够快了。输入神经元 a_0 是一个 784x60000 的双精度数组，值介于 0 和 1 之间。我获取了原始图像，其中每个像素都是一个 uint8，然后将其转换为双精度，然后除以 255 得到 a_0。我在代码中对层进行编号，其中第 0 层是输入层，第 1 层和第 2 层是隐藏层，第 3 层是输出层。
a_0 = training_images;
epoch = 0;
while epoch &lt; 5 || C(epoch - 1) - C(epoch) &gt; 0.001 
epoch = epoch + 1;

%向前传播
z_1 = weights_1*a_0 + biases_1;
a_1 = sigmoid(z_1);
z_2 = weights_2*a_1 + biases_2;
a_2 = sigmoid(z_2);
z_3 = weights_3*a_2 + biases_3;
a_3 = sigmoid(z_3);

%评估成本函数
C(epoch) = 0.5*mean(sum((a_3-y).^2, 1));

%向后传播
sigmoid_d1 = a_1 .* (1-a_1); %Sigmoid 导数
sigmoid_d2 = a_2 .* (1-a_2);
sigmoid_d3 = a_3 .* (1-a_3);
delta_3 = (a_3-y).*sigmoid_d3;
delta_2 = weights_3.&#39;*delta_3 .* sigmoid_d2;
delta_1 = weights_2.&#39;*delta_2 .* sigmoid_d1;

%计算梯度
for image_index = 1:num_images
dC_dw3(:, :, image_index) = delta_3(:, image_index) * a_2(:, image_index).&#39;;
dC_dw2(:, :, image_index) = delta_2(:, image_index) * a_1(:, image_index).&#39;;
dC_dw1(:, :, image_index) = delta_1(:, image_index) * a_0(:, image_index).&#39;;
end

%计算调整
training_rate = 0.1;
adjust_biases_1 = -training_rate * mean(delta_1, 2);
adjust_biases_2 = -training_rate * mean(delta_2, 2);
adjust_biases_3 = -training_rate * mean(delta_3, 2);
调整权重1 = -训练速率 * 平均值(dC_dw1, 3);
调整权重2 = -训练速率 * 平均值(dC_dw2, 3);
调整权重3 = -训练速率 * 平均值(dC_dw3, 3);
偏差1 = 偏差1 + 调整偏差1;
偏差2 = 偏差2 + 调整偏差2;
偏差3 = 偏差3 + 调整偏差3;
权重1 = 权重1 + 调整权重1;
权重2 = 权重2 + 调整权重2;
权重3 = 权重3 + 调整权重3;
]]></description>
      <guid>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</guid>
      <pubDate>Fri, 31 May 2024 08:39:40 GMT</pubDate>
    </item>
    <item>
      <title>MNIST - mnist.train_images() 的问题 - HTTPError：禁止访问</title>
      <link>https://stackoverflow.com/questions/78460997/mnist-problem-with-mnist-train-images-httperror-forbidden</link>
      <description><![CDATA[我目前正在学习神经网络，我想使用 train_images() 函数，但我无法这样做。如果我运行以下代码：
import mnist

images = mnist.train_images()

，我将得到：
runfile(&#39;C:/Users/deriv/untitled0.py&#39;, wdir=&#39;C:/Users/deriv&#39;)
回溯（最近一次调用）：

文件 ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
exec(code, globals, locals)

文件 c:\users\deriv\untitled0.py:3
images = mnist.train_images()

文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:161 in train_images
return download_and_parse_mnist_file(&#39;train-images-idx3-ubyte.gz&#39;)

文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:143 在 download_and_parse_mnist_file 中
fname = download_file(fname, target_dir=target_dir, force=force)

文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:59 在 download_file 中
urlretrieve(url, target_fname)

文件 ~\anaconda3\Lib\urllib\request.py:241 在 urlretrieve 中
使用 contextlib.closing(urlopen(url, data)) 作为 fp:

文件 ~\anaconda3\Lib\urllib\request.py:216 在 urlopen 中
return opener.open(url, data, timeout)

文件~\anaconda3\Lib\urllib\request.py:525 in open
response = meth(req, response)

文件 ~\anaconda3\Lib\urllib\request.py:634 in http_response
response = self.parent.error(

文件 ~\anaconda3\Lib\urllib\request.py:563 in error
return self._call_chain(*args)

文件 ~\anaconda3\Lib\urllib\request.py:496 in _call_chain
result = func(*args)

文件 ~\anaconda3\Lib\urllib\request.py:643 in http_error_default
raise HTTPError(req.full_url, code, msg, hdrs, fp)

HTTPError: Forbidden

我使用 pip install 正确安装了 mnist，但我不知道为什么** mnist.train_images()** 导致错误。抱歉，这是一个简单的问题，但它对我有很大帮助。
我不知道是否应该直接从 http://yann.lecun.com/exdb/mnist/ 下载文件。但是我无法这样做，因为我没有访问这些资源的权限。]]></description>
      <guid>https://stackoverflow.com/questions/78460997/mnist-problem-with-mnist-train-images-httperror-forbidden</guid>
      <pubDate>Fri, 10 May 2024 15:02:52 GMT</pubDate>
    </item>
    <item>
      <title>esp32-cam 使用人脸识别时出现错误 cam_hal: EV-VSYNC-OVF</title>
      <link>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</link>
      <description><![CDATA[我在 esp32-cam 板上使用示例“CameraWebServer”。上传设置如上所列：
主板：AI Thinker ESP32-CAM；
CPU 频率：240 MHZ；
闪存频率：80 Mhz；
闪存模式：QIO。
Arduino IDE 2.0.0
esp32 by Espressif 版本 2.0.14

使用这些设置，我可以上传我的代码，但面部识别功能不起作用。当我点击“注册面部”时，什么也没发生，我的串行监视器显示消息 EV-VSYNC-OVF。如何解决这个问题？
此外，我已经尝试修改上传设置并更改文件“CameraWebServer.ino”中的参数 config.frame_size 和 config.xclk_freq_hz，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</guid>
      <pubDate>Wed, 07 Feb 2024 22:12:55 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时出现列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（例如每分钟大约 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数低于一个恒定的条时，查询将运行，并且响应将正常接收。但是，一旦查询数量增加到上述条以上，就会出现以下错误：

IndexError - 列表索引超出范围

请注意，发生此错误的查询数“条”取决于每个查询的长度，并且如果查询的长度在程序执行过程中保持不变，则该查询数是一致的。例如，在尝试将查询长度增加大约 20% 后，查询数量从大约 330 个下降到大约 60 个。

文件
&quot;/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py&quot;,
line 1315, in text
return self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError: 列表索引超出范围

是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，其配额应仅为 300 个查询/分钟。由于我连续进行 API 调用，但低于 60 个查询/分钟的速率，因此我认为我的使用情况尚可。我目前正在使用免费的 VertexAI 试用帐户（免费赠送 300 美元信用额度）。
我编写的 Gemini Pro API 调用函数是：
def gemini_response(message: str) -&gt; str:
# 初始化 Vertex AI
vertexai.init(project=&quot;project-id-0123&quot;, location=&quot;us-central1&quot;)

# 加载模型
model = GenerativeModel(&quot;gemini-pro&quot;)

# 查询模型
response = model.generate_content(message)
return response.text

在调试 candidates 变量出了什么问题时，变量检查结果如下所示：
&gt; self 
&gt; prompt_feedback {block_reason: OTHER} 
&gt; usage_metadata {prompt_token_count: 505 total_token_count: 505 } 

&gt; self.candidates 
&gt; []

&gt; self._raw_response 
&gt; prompt_feedback {block_reason: OTHER}
&gt; usage_metadata {prompt_token_count: 505 total_token_count: 505 }
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道使用 SelectKBest 选择了哪些功能？</title>
      <link>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</link>
      <description><![CDATA[运行 SelectKBest 后会选择一些特征，结果以数组形式返回，因此我不知道它们是什么特征，因为我的训练集有数千个特征。
我想在测试集中找到并挑选出这些特征，然后删除其余特征。有什么方便的方法吗？谢谢！
代码如下：
from sklearn.feature_selection import SelectKBest, f_regression
X_opt=SelectKBest(f_regression,k=2000)
X_new=X_opt.fit_transform(df_train_X_mm, train_y)
X_new`

结果如下：
array([[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
[0. , 0. , 0.00688335, ..., 0. , 0. ,
0. ],
[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
...,
[0. , 0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0.06257587，...，0. ，0. ，
0. ]])
]]></description>
      <guid>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</guid>
      <pubDate>Wed, 20 Jun 2018 07:25:42 GMT</pubDate>
    </item>
    </channel>
</rss>