<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 15 Jul 2024 15:18:01 GMT</lastBuildDate>
    <item>
      <title>如何在 Python 环境中结束 While 循环</title>
      <link>https://stackoverflow.com/questions/78750597/how-to-end-while-loop-in-python-environments</link>
      <description><![CDATA[我使用 Python 3.7.16，在以下 Python 环境中，我遇到了使用 IF 语句 (if (keyboard.is_pressed(&#39;k&#39;)) 停止简单 while 循环的问题：
import ...
import keyboard

lst = retro.data.list_games()

def main():
env = retro.make(game=&quot;...&quot;)

obs = env.reset() 
done = False
for game in range(1): 
rw = 0 
while (not done):
if (keyboard.is_pressed(&#39;k&#39;)):
print(&quot;Loop Terminated&quot;)
break 
if done:
obs = env.reset()
env.render()
obs, reward, done, info = env.step(env.action_space.sample())
time.sleep(0.07) 
print(reward)
rw += reward 
env.close()
print(&quot;Reward total:&quot;, rw)
print(info)

if __name__ == &quot;__main__&quot;:
main()

尝试次数：
我读到按下 CTRL+C、CTRL+D 或 CTRK+Z 键即可退出循环。就我而言：不，它不起作用。
我发现了一个易于实现的 IF 语句 (if (keyboard.is_pressed(&#39;k&#39;))。我实现了它（见上文）。那也不起作用。为什么它不起作用？如果我按下键“k”，什么也不会发生。while 循环继续。
我期望什么？
每次我按下一个键（“k”）时，程序都应该停止并向我显示我到目前为止获得的奖励。当我再次按下该键时，程序应该继续。]]></description>
      <guid>https://stackoverflow.com/questions/78750597/how-to-end-while-loop-in-python-environments</guid>
      <pubDate>Mon, 15 Jul 2024 15:06:30 GMT</pubDate>
    </item>
    <item>
      <title>执行机器学习/时间序列任务时如何处理状态特征</title>
      <link>https://stackoverflow.com/questions/78750232/how-to-deal-with-state-feture-while-performaing-ml-time-series-task</link>
      <description><![CDATA[我正在研究一个收入预测模型，其中状态是关键因素之一。我最初对状态变量应用了单热编码，但没有获得所需的准确度，我的模型失败了。
以下是我到目前为止尝试过的方法：
对状态变量应用了单热编码，得到了一个高维稀疏矩阵。
尝试了不同的机器学习模型，但准确度仍然不是最优的。
我怀疑单热编码不是最好的方法，因为状态变量的基数很高。我如何更好地处理这个问题以提高我的模型的性能？
任何建议或替代编码技术都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78750232/how-to-deal-with-state-feture-while-performaing-ml-time-series-task</guid>
      <pubDate>Mon, 15 Jul 2024 13:52:47 GMT</pubDate>
    </item>
    <item>
      <title>如何向 CNN_M_LSTM 模型添加 3 个输入参数？</title>
      <link>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow）中。
能耗和时间戳的大小为 (70082, 2)
Covid 数据集的大小为 (744, 1)
我曾使用 tensorslice 和 zip 将数据打包在一起并对数据集进行窗口化：
这是我打包和窗口化能耗和时间戳数据集以及 covid 数据集的代码：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000

def windowed_dataset(series_energy,series_covid, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
dataset_energy = tf.data.Dataset.from_tensor_slices(series_energy) 
dataset_covid = tf.data.Dataset.from_tensor_slices(series_covid) 
dataset = tf.data.Dataset.zip(dataset_energy,dataset_covid)
dataset = dataset.window(96 + 1, shift=1) #
dataset = dataset.flat_map(lambda window_covid, window_series: tf.data.Dataset.zip((window_covid, window_series)).batch(96 + 1))
dataset = dataset.shuffle(1000)
dataset = dataset.map(lambda window_covid, window_series: (window_covid[:-1], window_series[-1][0])) 
dataset = dataset.padded_batch(128,drop_remainder=True).cache()

返回数据集

对于模型 CNN_M_LSTM，我创建了 2 个输入。这是我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(96, 1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(96, 2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2[:, -1, :])

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate()([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2],outputs=output)
# 定义最终模型
return model_final


我如何拟合我的模型：
model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

错误是模型需要 2 个输入，但收到 1 个输入张量。我曾尝试将 covid 数据集、能量列和时间戳压缩到一个数据集。
我的期望是将 3 个输入输入到我的 CNN_M_LSTM 模型中。我还尝试单独输入数据，我收到数据形状错误，我尝试重新塑造它，但它也不起作用。有没有办法在我的 CNN_M_LSTM 模型中输入 3 个参数？]]></description>
      <guid>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</guid>
      <pubDate>Mon, 15 Jul 2024 11:49:20 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证和 MICE 归因 [关闭]</title>
      <link>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</link>
      <description><![CDATA[我正在研究一个二元分类问题，其中有一些缺失数据。我最初的想法是使用 MiceForest。我还使用了分层 k 折技术（数据不平衡）。我还想尽量减少数据泄漏。

我应该何时使用 MiceForest 填补缺失值？针对每个折？还是一开始就填补整个数据集？
我应该使用 SMOTE 来解决每个折中的类别不平衡问题吗？因为我得到了很多误报（当仅使用分层 k 折而没有过度采样时）。

当我对整个数据集进行 mice 填补，用 smote 解决类别不平衡问题，然后进行交叉验证时，我获得了非常好的性能。我觉得这是过度拟合？这是因为数据泄漏吗？（我对这个领域有点陌生）]]></description>
      <guid>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</guid>
      <pubDate>Mon, 15 Jul 2024 06:37:02 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace：Llama-3-8B 合作检查点碎片加载进度在 25% 处停止</title>
      <link>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</link>
      <description><![CDATA[我尝试使用 huggingface 在我的 Colab 笔记本中本地运行 Llama-3-8B 模型。加载模型时，检查点分片在 25% 处停止加载。我不明白问题可能是什么。
from transformers import AutoModelForCausalLM, AutoTokenizer

# 定义模型名称（这是一个占位符，请替换为实际模型名称）
model_name = &quot;meta-llama/Meta-Llama-3-8B&quot;

!huggingface-cli login --token $HF_TOKEN
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 如果模型很大，将其移动到 GPU 可能会有所帮助
model.to(&#39;cuda&#39;)

HF_Token 已定义，出于隐私原因，此处未提及。
提示以下错误：
您的 token 已保存到 /root/.cache/huggingface/token
登录成功
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: 
您的 Colab secrets 中不存在 secret `HF_TOKEN`。
要使用 Hugging Face Hub 进行身份验证，请在设置选项卡 (https://huggingface.co/settings/tokens) 中创建一个令牌，将其设置为 Google Colab 中的机密，然后重新启动会话。
您将能够在所有笔记本中重复使用此机密。
请注意，建议进行身份验证，但仍然可以选择访问公共模型或数据集。
warnings.warn(
词汇表中已添加特殊令牌，请确保对相关的词嵌入进行了微调或训练。
正在加载 检查点 分片：  25%
 1/4 [00:22&lt;01:07, 22.37s/it]
]]></description>
      <guid>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</guid>
      <pubDate>Mon, 15 Jul 2024 05:44:13 GMT</pubDate>
    </item>
    <item>
      <title>如何对视频中对象执行的具体动作进行分类。（不是仅使用一帧，而是使用一组帧）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</link>
      <description><![CDATA[我想创建一个人工智能模型，通过查看帧集合来确定对象行为的结果，而不是通过在家打高尔夫球时查看单个帧来确定高尔夫球是进入还是离开。
我尝试使用 ultralytics，但 ultralytics 按帧对对象进行分类，因此它不符合我的目的。我想知道如何创建一个区分视频中行为分类的模型。
如何制作一个分析帧而不是帧的人工智能模型。]]></description>
      <guid>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</guid>
      <pubDate>Sun, 14 Jul 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：目标 32 超出范围。运行时损失 = 标准（y_pred，y_train）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</link>
      <description><![CDATA[我正在运行一个简单的神经网络，其中包含一些大约 1112 行、23 个输入、2 个隐藏层和 31 个可能输出的 csv 数据。在前向训练之后，在以下代码执行过程中，我收到以下错误消息
在行 loss = criterion(y_pred, y_train)
错误：
-----------------------------------------------------------------------------
IndexError Traceback（最近一次调用最后一次）
&lt;ipython-input-64-47488b841fa2&gt; 在 &lt;cell line: 5&gt;()
8 
9 
---&gt; 10 loss = criterion(y_pred, y_train)
11 
12 #loss.append(loss.detach().numpy())

3 帧
/usr/local/lib/python3.10/dist-packages/torch/nn/ functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
3084 如果 size_average 不为 None 或 reduce 不为 None:
3085 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3086 返回 torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
3087 
3088 

IndexError：目标 32 超出范围。

有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</guid>
      <pubDate>Sun, 14 Jul 2024 19:10:09 GMT</pubDate>
    </item>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我以为你可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>二元分类中的 SHAP 值解释</title>
      <link>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</link>
      <description><![CDATA[我尝试使用每个特征的 SHAP 值来解释我的二元分类模型。我想知道：
正的 SHAP 值是否意味着该特征对预测“1”类的贡献更大，而负的 SHAP 值是否意味着该特征对预测“0”类的贡献更大？
如果我使用绝对 SHAP 值差异来描述特征贡献变化，这个想法是否合理？]]></description>
      <guid>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</guid>
      <pubDate>Fri, 12 Jul 2024 14:25:43 GMT</pubDate>
    </item>
    <item>
      <title>无法检测/删除图像数据集中两个位置不同的水印</title>
      <link>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</link>
      <description><![CDATA[我在从一组图片中删除水印时遇到了问题。这些水印彼此靠近，但又有所不同（见下文）。其中一个水印是红色方块，里面有白色文字。另一个是半透明的灰色句子。目的是处理图像以用于机器学习。
图像
解决问题的尝试：
由于水印在图像数据集中的位置各不相同，我尝试了以下操作：

复制图像并将其转换为 HSV 颜色空间
为感兴趣的区域选择一系列下限值和上限值（在分割图像并为每个通道构建直方图后选择这些值）
使用 cv2.inRange() 函数构建蒙版
使用蒙版在原始图像中修复水印

对于红色方块，前三个步骤完美无缺。但第三步只是有点奏效。水印比以前明显少了，但仍然很明显。对于文本，我无法在第 3 步中获得足够好的蒙版 - 它的颜色/像素强度与周围区域和文本本身太接近了。
这看起来更像是一个机器学习问题，这很好，但我想事先用尽其他选择。关于如何使用机器学习或算法方法解决此问题，您有什么想法吗？
解决方案：
image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
lower_limit = 89 # 255 的 35%
upper_limit = 255
mask = cv2.inRange(gray, lower_limit, upper_limit)

mask_inv = cv2.bitwise_not(mask)
result = cv2.bitwise_and(image, image, mask=mask_inv)

# 反转 mask_inv 以获得白色背景上的黑色文本
inverted_mask_inv = ~mask_inv

plt.subplot(1,2,1); plt.imshow(image); plt.title(&quot;Original&quot;)
plt.subplot(1,2,2); plt.imshow(inverted_mask_inv, cmap=&quot;gray&quot;); plt.title(&quot;Just the text&quot;)````

]]></description>
      <guid>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</guid>
      <pubDate>Thu, 11 Jul 2024 16:54:13 GMT</pubDate>
    </item>
    <item>
      <title>python 中某些函数的贬值：数据框的真值不明确</title>
      <link>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</guid>
      <pubDate>Thu, 11 Jul 2024 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>如何准确计算Doctr ocr中检测到的文本的绝对bbox坐标？</title>
      <link>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</link>
      <description><![CDATA[我一直试图在文档的图片上绘制 bbox，并尝试使用 mindee-doctr 进行 ocr 以查看检测到的文本行。我面临的问题是，我通过乘以相对坐标和页面尺寸计算出的 bbox 的绝对坐标，在原始图像上绘制时都向右上角偏移。有没有办法纠正这个问题？
这是我计算 bbox 的代码：
from doctr.models import ocr_predictor
from doctr.io import DocumentFile

# 使用 docTR 分析图像并获取结果
line_boundaries = []
model = ocr_predictor(pretrained=True) #设置preserve_aspect_ratio=False 或symmetric_pad=False 没有区别。
doc = DocumentFile.from_images(img_path)
result = model(doc)

# 提取每行的边界框坐标
for page in result.pages:
for block in page.blocks:
for line in block.lines:
# 将相对坐标与页面尺寸相乘，得到绝对坐标
x_min, y_min, x_max, y_max = round(line.geometry[0][0] * page.dimensions[0]), round(line.geometry[0][1] * page.dimensions[1]), round(line.geometry[1][0] * page.dimensions[0]), round(line.geometry[1][1] * page.dimensions[1])
line_boundaries.append((x_min, y_min, x_max, y_max))

这是 line_boundaries 的值：
[(531, 148, 1321, 184), (2725, 148, 3061, 177), (526, 254, 3071, 295), (526, 288, 3071, 332), (535, 324, 3071, 363), ... ]
这是我用来绘制方框的函数：
import cv2
from google.colab.patches import cv2_imshow # 代替 cv2.imshow 使用，因为它会导致 collab 崩溃

def draw_rectangles(image_path, line_boundaries):
&quot;&quot;&quot;
使用提供的线边界在图像上绘制矩形。

参数：
image_path：图像文件的路径。
line_boundaries：线边界列表，其中每个边界都是四个点的列表。

返回：
无
&quot;&quot;&quot;

# 加载图像
image = cv2.imread(img_path)

# 遍历线边界并绘制矩形
for bounding in line_boundaries:
#x1, y1, x2, y2 = int(boundary[0][0]), int(boundary[0][1]), int(boundary[2][0]), int(boundary[2][1])
x1, y1, x2, y2 = map(int, bounding) # 将坐标转换为整数
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# 用矩形显示图像
cv2_imshow(image) # 仅使用 cv2_imshow 代替 cv2.imshow 进行协作
cv2.waitKey(0)
cv2.destroyAllWindows()

这是带有在其上绘制的 bboxes。

我尝试过不使用舍入，但没有任何区别，也尝试过只使用预测器，但无济于事。在 ocr_predictor 中设置preserve_aspect_ratio=False 或symmetric_pad=False 也没有区别。]]></description>
      <guid>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</guid>
      <pubDate>Thu, 11 Jul 2024 05:38:22 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>带有 gpu 的 Lightgbm 分类器</title>
      <link>https://stackoverflow.com/questions/60360750/lightgbm-classifier-with-gpu</link>
      <description><![CDATA[model = lgbm.LGBMClassifier(
n_estimators=1250,
num_leaves=128,
learning_rate=0.009,
verbose=1
)

使用 LGBM 分类器，
现在有没有办法将其与 GPU 一起使用？]]></description>
      <guid>https://stackoverflow.com/questions/60360750/lightgbm-classifier-with-gpu</guid>
      <pubDate>Sun, 23 Feb 2020 09:20:03 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 聚类：预测（X）与 fit_predict（X）</title>
      <link>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</link>
      <description><![CDATA[在 scikit-learn 中，一些聚类算法同时具有 predict(X) 和 fit_predict(X) 方法，例如 KMeans 和 MeanShift，而其他算法仅具有后者，例如 SpectralClustering。根据文档：
fit_predict(X[, y]): 对 X 执行聚类并返回聚类标签。
predict(X): 预测 X 中每个样本所属的最接近聚类。

我不太明白这两者之间的区别，在我看来它们似乎是等价的。]]></description>
      <guid>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</guid>
      <pubDate>Mon, 09 May 2016 02:25:29 GMT</pubDate>
    </item>
    </channel>
</rss>