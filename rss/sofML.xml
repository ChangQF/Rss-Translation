<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 10 May 2024 01:01:05 GMT</lastBuildDate>
    <item>
      <title>如何基于另一个张量创建张量 - 在实践中学习 PyTorch？</title>
      <link>https://stackoverflow.com/questions/78457370/how-to-create-a-tensor-based-on-another-one-studying-pytorch-in-practice</link>
      <description><![CDATA[我正在使用 PyTorch 学习 IA 并实现一些玩具示例。
首先，我创建了一个一维张量 (X) 和一个从第一个张量导出的第二个张量 (y)：
X = torch.arange(0, 100, 1.0).unsqueeze(dim=1)
y = X * 2

所以我有类似的东西
X = 张量([[0.], [1.], [2.], [3.], [4.], [5.], ...
y = 张量([[ 0.], [ 2.], [ 4.], [ 6.], [ 8.], [10.], ...

然后，我训练了一个模型来预测 y，它运行良好。
现在，我想要一些不同的东西。 X 为 2D，y 为 1D。 y 通过对 X 的元素进行运算来计算：
如果 x[0] + x[1] &gt; 0？ y = 10: y -10 
X = 张量([[ 55.5348, -97.7608],
            [29.0493，-52.1908]，
            [47.1722，-43.1151]，
            [11.1242，-62.8652]，
            [ 44.8067, 80.8335],...
y = 张量([[-10.], [-10.], [ 10.], [-10.], [ 10.],...

第一个问题，这对于机器学习有意义吗？
第二个...
我正在使用 numpy 生成张量。我可以用更聪明的方式来做吗？
# Criar X valores de entrada para testes
X_numpy = np.random.uniform(低=-100, 高=100, 大小=(1000,2))
打印（“X”，X_numpy）

#y_numpy = np.array([[ (n[0]+n[1]) &gt;= 0 ? 10:-10] for n in X_numpy])
y_numpy = np.empty(形状=[0, 1])
对于 X_numpy 中的 n：
    如果 n[0] + n[1] &gt;= 0：
        y_numpy = np.append(y_numpy, [[10.]], 轴=0)
    elif n[0] + n[1] &lt; 0：
        y_numpy = np.append(y_numpy, [[-10.]], 轴=0)
]]></description>
      <guid>https://stackoverflow.com/questions/78457370/how-to-create-a-tensor-based-on-another-one-studying-pytorch-in-practice</guid>
      <pubDate>Thu, 09 May 2024 23:28:50 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的模型以匹配 U-Net 图</title>
      <link>https://stackoverflow.com/questions/78457326/how-do-i-fix-my-model-up-to-match-the-u-net-diagram</link>
      <description><![CDATA[运行时错误：张量的大小必须匹配（维度 1 除外）。预期大小为 1024，但列表中第 1 个张量的大小为 512。
U-Net
类 UNet(torch.nn.Module):
  def __init__(自身):
    超级().__init__()
    self.conv1 = torch.nn.Conv2d(1, 64, kernel_size=3, padding=&#39;相同&#39;)
    self.conv2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=&#39;相同&#39;)
    self.pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)

    self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=&#39;相同&#39;)
    self.conv4 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=&#39;相同&#39;)
    self.pool2 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)

    self.conv5 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=&#39;相同&#39;)
    self.conv6 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=&#39;相同&#39;)
    self.pool3 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)

    self.conv7 = torch.nn.Conv2d(256, 512, kernel_size=3, padding=&#39;相同&#39;)
    self.conv8 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=&#39;相同&#39;)
    self.pool4 = torch.nn.MaxPool2d(kernel_size=2, 步长 = 2)

    self.conv9 = torch.nn.Conv2d(512, 1024, kernel_size=3, padding=&#39;相同&#39;)
    self.conv10 = torch.nn.Conv2d(1024, 1024, kernel_size=3, padding=&#39;相同&#39;)
    self.Upsample = torch.nn.Upsample(scale_factor=2)
    #ConCat
    self.conv11 = torch.nn.Conv2d(1536, 512, kernel_size=3, padding=&#39;相同&#39;)#1024 + 512 = 1536
    self.conv12 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=&#39;相同&#39;)
    self.conv13 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=&#39;相同&#39;)
    #self.Upsample = torch.nn.Upsample(scale_factor=2)
    #ConCat
    self.conv14 = torch.nn.Conv2d(768, 256, kernel_size=3, padding=&#39;相同&#39;) #512 + 256 = 768
    self.conv15 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=&#39;相同&#39;)
    #self.Upsample = torch.nn.Upsample(scale_factor=2)
    #ConCat
    self.conv16 = torch.nn.Conv2d(384, 128, kernel_size=3, padding=&#39;相同&#39;) #256 + 128 = 384
    self.conv17 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=&#39;相同&#39;)
    #self.Upsample = torch.nn.Upsample(scale_factor=2)
    #ConCat
    self.conv18 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=&#39;相同&#39;)
    self.conv19 = torch.nn.Conv2d(192, 64, kernel_size=3, padding=&#39;相同&#39;) #128 + 64 = 192
    self.conv20 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=&#39;相同&#39;)
    self.conv21 = torch.nn.Conv2d(64, 1, kernel_size=1, padding=&#39;相同&#39;)
    self.relu = torch.nn.ReLU()

  def 前向（自身，x）：
    x = self.conv1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x1 = self.relu(x)
    x = self.pool1(x1)

&lt;前&gt;&lt;代码&gt; x = self.conv3(x)
    x = self.relu(x)
    x = self.conv4(x)
    x2 = self.relu(x)
    x = self.pool2(x)

    x = self.conv5(x)
    x = self.relu(x)
    x = self.conv6(x)
    x3 = self.relu(x)
    x = self.pool3(x)

    x = self.conv7(x)
    x = self.relu(x)
    x = self.conv8(x)
    x4 = self.relu(x)
    x = self.pool4(x)

    x = self.conv9(x)
    x = self.relu(x)
    x = self.conv10(x)
    x = self.relu(x)
    x = self.Upsample(x)

    x = torch.cat((x, x4), 暗淡=1)

    x = self.conv11(x)
    x = self.relu(x)`

x = self.conv12(x)
    x = self.relu(x)
    x = self.conv13(x)
    x = self.relu(x)
    x = self.Upsample(x)

    x = torch.cat((x, x3), 暗淡=1)
    x = self.conv14(x)
    x = self.relu(x)
    x = self.conv15(x)
    x = self.relu(x)
    x = self.Upsample(x)

    x = torch.cat((x, x2), 暗淡=1)

    x = self.conv16(x)
    x = self.relu(x)
    x = self.conv17(x)

    x = self.relu(x)
    x = self.conv18(x)
    x = self.relu(x)
    x = self.Upsample(x)

    x = torch.cat((x, x1), 暗淡=1)

    x = self.conv19(x)
    x = self.relu(x)
    x = self.conv20(x)
    x = self.relu(x)
    x = self.conv21(x)
    x = self.relu(x)

    返回x

我尝试更改我的连接输出通道以获得与 1024 相同的输出。我看到错误就在我第一次开始连接的第 10 层之后出现。我想知道我是否连接错误。]]></description>
      <guid>https://stackoverflow.com/questions/78457326/how-do-i-fix-my-model-up-to-match-the-u-net-diagram</guid>
      <pubDate>Thu, 09 May 2024 23:07:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将其参数减少为嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（及其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，也不知道如何将参数向量重建回 CNN 模型参数。
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>如何查找变量值以获得特定于因变量范围的值？</title>
      <link>https://stackoverflow.com/questions/78457218/how-do-i-find-the-values-of-variables-to-obtain-a-value-specific-to-a-range-of-t</link>
      <description><![CDATA[我有一个数据库，使用随机森林算法执行回归来预测响应变量，该变量取决于 8 个解释变量。我的随机森林算法具有良好的性能。我想找到解释变量的值，以便我的响应变量只取某个区间内的值。我尝试实现 slsqp 算法，但我无法让我的代码工作：
from scipy.optimize import minimize
将 numpy 导入为 np

def Objective_function(x, X, best_rf):
    预测 = best_rf.predict(np.array(x).reshape(1, -1))
    回报预测


lower_bounds = [0, 0, 0, 34.5, 0.25, 0, 0, 0.07699] # 限制cada变量的下限
upper_bounds = [50, 50, 428.2, 93.44, 2, 58, 5, 3.70255] # 限制cada变量的上级


def 约束函数(x, X, best_rf):
    预测 = Objective_function(x, X, best_rf)
    返回 0.3 - 预测 # 对 igual a 0.3 的海洋预测的限制


initial_guess = [15.0, 0.0, 0.000, 93.44, 0.50, 0.0, 0.0, 0.674536] # 使用初始值


结果=最小化（
    目标函数，
    初始猜测，
    args=(X, best_rf),
    方法=&#39;SLSQP&#39;,
    边界=列表（zip（下限，上限）），
    约束={&#39;type&#39;：&#39;ineq&#39;，&#39;fun&#39;：constraint_function，&#39;args&#39;：（X，best_rf）}
）


优化值 = 结果.x
print(“变量解释的优化值：”,optimized_values)


优化预测 = 目标函数（优化值，X，best_rf）
print(&quot;预测优化值：&quot;, optimization_prediction)

我正在等待解决此问题的建议，或提供帮助，以便我的代码可以执行我想要的操作。我的代码现在只打印我给出的解释变量的值]]></description>
      <guid>https://stackoverflow.com/questions/78457218/how-do-i-find-the-values-of-variables-to-obtain-a-value-specific-to-a-range-of-t</guid>
      <pubDate>Thu, 09 May 2024 22:23:12 GMT</pubDate>
    </item>
    <item>
      <title>Kohonen 神经网络用于确定点平面的象限</title>
      <link>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</link>
      <description><![CDATA[我需要实现 Kohonen 神经网络（没有老师）来确定点的平面（从 1 到 8）的象限。坐标 [x, y, z] 的向量被馈送到神经网络的输入。在输出中，我们得到一个向量，通过该向量可以确定该点属于哪个象限。
这是我训练神经网络的代码：
def 火车(
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        学习率：浮动，
        纪元：int
）-&gt; npt.NDArray[npt.NDArray[浮点]]：

    W = np.random.randn(3, 8)

    对于范围（1，纪元 + 1）中的纪元：
        random_indexes = np.random.choice(len(数据集)，size=len(数据集)，replace=False)
        对于 random_indexes 中的索引：
            x = 数据集[索引][0] # [[x, y, z]]
            y = 数据集[索引][1]

            x_normalized = x / np.sqrt(np.sum(x ** 2))

            z = np.dot(x_归一化，W)

            W[0][z.argmax()] += 学习率 * (x[0][0] - W[0][z.argmax()])
            W[1][z.argmax()] += 学习率 * (x[0][1] - W[1][z.argmax()])
            W[2][z.argmax()] += 学习率 * (x[0][2] - W[2][z.argmax()])

        学习率 = 学习率 / 时期

    返回W

def calc_accuracy(
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        W: npt.NDArray[npt.NDArray[float]]) -&gt;漂浮：
    正确 = 0
    对于数据集中的 x、y：
        z = np.dot(x, W)
        如果 z.argmax() == y.argmax():
            正确+=1
    返回（正确/len（数据集））* 100


def draw_accuracy_epoch_plots(
        数据集：list[list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]],
        epoch_number：int
）-&gt;没有任何：
    对于索引，枚举中的数据集（数据集）：
        准确度列表 = []
        准确度列表_2 = []
        对于范围（1，epoch_number + 1）中的历元：
            W = 训练（数据集，learning_rate=0.7，epochs=epochs）
            precision_list.append(calc_accuracy(数据集，W))
            precision_list_2.append(calc_accuracy(generate_dataset(15), W))

        plt.subplot(3, 2, (2 * 索引) + 1)
        plt.plot(accuracy_list)

        plt.subplot(3, 2, (2 * 索引) + 2)
        plt.plot(accuracy_list_2)


draw_accuracy_epoch_plots([generate_dataset(25),generate_dataset(50),generate_dataset(100)],100)

在代码中，我计算了不同时期神经网络预测的准确性。结果，我得到的准确度为 0 到 45%，这不适合我。同时，神经网络经常产生 0% 的匹配。
如果更改学习率和历元没有帮助，我该如何解决这个问题？我可以添加图层吗（我还没有找到这个问题的答案）？或者也许我错误地构建了学习算法？]]></description>
      <guid>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</guid>
      <pubDate>Thu, 09 May 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>尝试强制新预测器中的观察结果以在医疗补助支出数据集中执行逻辑函数</title>
      <link>https://stackoverflow.com/questions/78456621/attempting-to-coerce-observations-in-a-new-predictor-to-perform-logistic-functio</link>
      <description><![CDATA[我正在分析按药物数据划分的医疗补助支出数据字典数据集。具体来说，我想执行逻辑回归，其中 y 应该是 CAGR_Avg_Spnd_Per_Dsg_Unt_18_22。
不幸的是，根据我的代码，类和模式仍然是字符。
我对“Up”的灵感来自于和“向下”方法来自以下内容：
# 该库来自《统计学习简介：R 中的应用》

图书馆（ISLR）
附加(Smarket)
总结(Smarket)
# 期望的输出：
glm.fit=glm(方向~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 音量,
            家庭=二项式，数据=Smarket）
对比（方向）

通过使用 glm.fit，我可以执行预测、创建混淆矩阵等等。
但是，在检查时
摘要(drug.spending)

我的“向上”和“向下”是角色，而 ISLR 的“Up”的作者是角色。和“向下”看来是按数字算的。作者从未提供使用数据框的“向上”来执行此操作的代码。和“向下”观察！
这是我的代码：
库(dplyr)
图书馆（tidyr）
图书馆（心理学）
图书馆（跳跃）
设置.种子(1)

支出 &lt;- read.csv(“medicaid_spending_by_drug_data_dictionary.csv”)
drug.spending &lt;- 支出 %&gt;%
  na.omit(支出) %&gt;%
  filter(Mftr_Name == “总体”) %&gt;%
  安排(desc(Tot_Mftr)) %&gt;%
  过滤器（重复（Gnrc_Name））
drug.spending &lt;- drug.spending[!duplicate(drug.spending$Gnrc_Name),]
附加（药物.支出）



药物支出 &lt;- 药物支出 %&gt;%
  mutate(CAGR_Direction = ifelse(CAGR_Avg_Spnd_Per_Dsg_Unt_18_22 &gt; 0, &#39;向上&#39;, &#39;向下&#39;))
drug.spending$CAGR_Direction &lt;- factor(drug.spending$CAGR_Direction,levels = c(&#39;Down&#39;, &#39;Up&#39;)) # 更新 #1

摘要（药品.支出）
对比（CAGR_Direction）#给出错误

我使用了不同的强制转换，例如 as.numeric() 和 as.integer()。我不太确定我哪里出错了......]]></description>
      <guid>https://stackoverflow.com/questions/78456621/attempting-to-coerce-observations-in-a-new-predictor-to-perform-logistic-functio</guid>
      <pubDate>Thu, 09 May 2024 19:27:54 GMT</pubDate>
    </item>
    <item>
      <title>“适合训练数据的最短程序是最好的概括”这个定理是什么？</title>
      <link>https://stackoverflow.com/questions/78456286/shortest-program-that-fits-training-data-is-the-best-possible-generalisation-w</link>
      <description><![CDATA[在麻省理工学院的课堂上，一位演说家说“适合的最短程序”训练数据是最好的概括”这不仅仅是哲学 (https://en.wikipedia.org/wiki/Occam&#39;s_razor ），但实际上是一个由形式（简单）证明支持的数学事实。
该数学定理的名称是什么？这是“最小描述长度”吗？这就是当我问 ChatGPT 问题时它告诉我的，但对我来说，它看起来不像是一个经过证明的数学定理，而是类似的东西，尽管比奥卡姆剃刀稍微更正式。]]></description>
      <guid>https://stackoverflow.com/questions/78456286/shortest-program-that-fits-training-data-is-the-best-possible-generalisation-w</guid>
      <pubDate>Thu, 09 May 2024 18:19:17 GMT</pubDate>
    </item>
    <item>
      <title>书籍计数器 | 我需要一个可以计算图像/视频中书籍数量的模型</title>
      <link>https://stackoverflow.com/questions/78456209/book-counter-i-need-a-model-which-will-count-numbers-of-book-from-image-video</link>
      <description><![CDATA[我需要一个模型来计算图像/视频中的书籍数量。
原型机将能够拍摄图像/视频并继续计数书籍，就像我们在路灯上的车辆计数器一样。]]></description>
      <guid>https://stackoverflow.com/questions/78456209/book-counter-i-need-a-model-which-will-count-numbers-of-book-from-image-video</guid>
      <pubDate>Thu, 09 May 2024 18:01:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么无论使用什么数据集和模型，我的准确性都没有变化？</title>
      <link>https://stackoverflow.com/questions/78456058/why-doesnt-my-accuracy-vary-no-matter-the-dataset-and-the-model-used</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78456058/why-doesnt-my-accuracy-vary-no-matter-the-dataset-and-the-model-used</guid>
      <pubDate>Thu, 09 May 2024 17:26:57 GMT</pubDate>
    </item>
    <item>
      <title>test=model.predict([text]) 整个项目连续运行六次</title>
      <link>https://stackoverflow.com/questions/78455825/test-model-predicttext-entire-project-runs-six-times-consecutively</link>
      <description><![CDATA[似乎当执行 test=model.predict([text]) 行时，您的整个项目连续运行六次，并且当您关闭打开的窗口时，它会进行预测。
您可以看到下面的代码片段。
[主文件]
print(“test1”)
def 分类():
    打印（“测试2”）
    raw_text = str(entry.get(“1.0”, tk.END))
    tahmin_sonucu = 主要（raw_text）
    categories_label.config(text=“分类器：”+ tahmin_sonucu)

[主要功能]
from simpletransformers.classification import ClassificationModel

labels = [“Bilim Kurgu”、“Ekonomi”、“Islam”、“Polisiye”、“Romantik”、“Sağlık”、“Spor”]
模型 = ClassificationModel(&#39;bert&#39;, &#39;bert_model&#39;, num_labels=7, use_cuda=False,
                                    args={&#39;reprocess_input_data&#39;：True，&#39;overwrite_output_dir&#39;：True，&#39;num_train_epochs&#39;：3，
                                        “train_batch_size”：16，“fp16”：False，“output_dir”：“bert_model”})
打印（“测试11”）
def main(metin):
    塔明 = 无
    打印（“tes12”）
    tahmin=model.predict([metin])

    打印（标签[tahmin[0][0]]）
    
    返回标签[tahmin[0][0]]


输出]]></description>
      <guid>https://stackoverflow.com/questions/78455825/test-model-predicttext-entire-project-runs-six-times-consecutively</guid>
      <pubDate>Thu, 09 May 2024 16:31:34 GMT</pubDate>
    </item>
    <item>
      <title>Stylegan3汽车生成问题</title>
      <link>https://stackoverflow.com/questions/78454480/stylegan3-car-generation-issue</link>
      <description><![CDATA[我正在尝试训练stylegan3来生成汽车图像，特别是某个国家的救护车，我有一个小的数据集，经过长时间的训练过程后的输出不方便，我是否需要更长的训练或更大的数据集？
我试图使我的数据集更加统一，并连续训练我的网络 3 天，输出仍然看起来与原始数据完全相同，但分辨率较低，您对针对此用例的更好网络有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78454480/stylegan3-car-generation-issue</guid>
      <pubDate>Thu, 09 May 2024 12:37:13 GMT</pubDate>
    </item>
    <item>
      <title>feature_weights 参数没有影响 Xgboost</title>
      <link>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</link>
      <description><![CDATA[xgboost 有一个 parameter feature_weights 应该影响模型选择特征的概率，也就是说，我们可以给每个特征更多或更少的权重，但似乎该参数不起作用还是我做错了什么？
X &lt;- as.matrix(iris[,-5])
Y &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0)

库（xgboost）
dm1 &lt;- xgb.DMatrix(X, 标签 = Y)
#我为每个特征设置不同的概率
dm2 &lt;- xgb.DMatrix(X, 标签 = Y, feature_weights = c(1, 0, 0, 0.01))
params &lt;- list(objective = “binary:logistic”, eval_metric = “logloss”)

设置.种子(1)



xgb1 &lt;- xgboost（数据 = dm1，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148



xgb2 &lt;- xgboost（数据 = dm2，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148

但是模型的行为完全相同，似乎参数feature_weights被简单地忽略了]]></description>
      <guid>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</guid>
      <pubDate>Thu, 09 May 2024 11:10:59 GMT</pubDate>
    </item>
    <item>
      <title>输入列的架构不匹配，预期为字符串或字符串向量，得到 UInt32（参数‘inputSchema’）</title>
      <link>https://stackoverflow.com/questions/78453914/schema-mismatch-for-input-column-expected-string-or-vector-of-string-got-uint32</link>
      <description><![CDATA[未处理的异常。 System.ArgumentOutOfRangeException：输入架构不匹配
列“QuestionKey”：预期的字符串或字符串向量，得到 UInt32
（参数“输入模式”）

代码：
// 尝试不同的文本特征化技术
            var tokenizedText = mlContext.Transforms.Text.TokenizeIntoWords(&quot;Tokens&quot;, &quot;Question&quot;);
            var wordEmbeddings = mlContext.Transforms.Text.ApplyWordEmbedding(&quot;特征&quot;, &quot;令牌&quot;, WordEmbeddingEstimator.PretrainedModelKind.SentimentSpecificWordEmbedding);

            var concatenatedFeatures = mlContext.Transforms.Concatenate(“FeaturesConcatenate”, “特征”);

            // 将“问题”列转换为 KeyType
            var modelPipeline = mlContext.Transforms.Conversion.MapValueToKey(&quot;QuestionKey&quot;, &quot;Question&quot;)
                .Append(mlContext.Transforms.Text.TokenizeIntoWords(“Tokens”, “QuestionKey”))
                .Append(mlContext.Transforms.Text.ApplyWordEmbedding(“特征”, “令牌”, WordEmbeddingEstimator.PretrainedModelKind.SentimentSpecificWordEmbedding))
                .Append(mlContext.Transforms.Categorical.OneHotEncoding(“QuestionEncoded”, “QuestionKey”))
                .Append(mlContext.Transforms.Conversion.MapValueToKey(“标签”, “答案”))
                .Append(mlContext.Transforms.Concatenate(&quot;FeaturesConcatenated&quot;, &quot;Features&quot;));

            
            // 使用 AutoML 或手动调整进行超参数调整实验
            var trainer = mlContext.MulticlassClassification.Trainers.LightGbm(labelColumnName: &quot;答案&quot;, featureColumnName: &quot;问题&quot;);

            var TrainingPipeline = modelPipeline.Append(trainer);

            // 训练模型
            var TrainingModel = TrainingPipeline.Fit(dataView);

            // 将训练好的模型保存到文件中
            mlContext.Model.Save(trainedModel, dataView.Schema, modelPath);

            返回训练好的模型；
        }

我正在使用包含两列问题和答案的数据集构建一个常见问题解答聊天机器人。该数据集由大约 30000 个 QnA 组成。我被困在这里尝试不同的技术。]]></description>
      <guid>https://stackoverflow.com/questions/78453914/schema-mismatch-for-input-column-expected-string-or-vector-of-string-got-uint32</guid>
      <pubDate>Thu, 09 May 2024 10:49:39 GMT</pubDate>
    </item>
    <item>
      <title>如何配置作业yaml和Yolov8数据集ymal来访问Azure上的数据资产？</title>
      <link>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</link>
      <description><![CDATA[我目前正在使用 Azure ML CLI v2 在 Azure ML 工作室中使用 Yolov8 训练自定义模型。
问题：
当我在 Azure ML 上运行作业时，收到一条错误消息“权限被拒绝”
错误代码：ScriptExecution.StreamAccess.Authentication
本机错误：来自输入数据源的流式传输错误
    StreamError(PermissionDenied(Some(此请求无权使用此权限执行此操作。)))
=&gt;访问流时权限被拒绝。原因：一些（该请求无权使用该权限执行该操作。）
    PermissionDenied(Some(此请求无权使用此权限执行此操作。))
错误消息：尝试访问流时身份验证失败。确保您设置了正确的权限。好的（该请求无权使用该权限执行该操作。）| session_id=da7b713c-6cc8-4f6d-b24f-b54ab37e14ef

Yaml 文件：

job.yaml：

$schema：https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
实验名称：yolov8-实验

命令： |
  sed -i“s|路径：.*$|路径：${{inputs.training_data}}|”自定义数据集.yaml
  # 训练模型
  yolo 任务=检测训练数据=custom_dataset.yaml 模型=${{inputs.model_to_train }} epochs=10 项目=yolov8-实验名称=实验

输入：
  训练数据：
    类型：uri_文件夹
    模式：ro_mount
    path: azure:data_asset:1 #我已经从本地文件创建了数据资产。
  模型到训练：
    类型：自定义模型
    路径：azureml:yolov8l:1

code: /training-code/ #custom_dataset.yaml 存储在本地的路径
环境：azureml：yolov8-环境：1
计算：azureml：compute_cluster


custom_dataset.yaml 文件：

路径：../数据集
火车：/图像/火车/
测试：/图像/测试/
值：/图像/测试/

NC=2

# 类名
名称：[class1，class2]

我参考了以下文章：
中-文章-yolov8-training-azure-cli
在 Azure ML 上训练模型使用 CLI v2 - Microsoft 培训
我在确定继续进行 Azure 设置所需的权限时遇到问题。我已使用“az login”成功登录，并成功创建了各种组件，例如环境、计算集群和数据资产。
但是，我不确定进一步操作所需的具体权限。我的设置需要授予哪些权限才能正常运行？]]></description>
      <guid>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</guid>
      <pubDate>Thu, 09 May 2024 10:35:51 GMT</pubDate>
    </item>
    <item>
      <title>如何找到两幅图像之间的相关性</title>
      <link>https://stackoverflow.com/questions/59608470/how-to-find-correlation-between-two-images</link>
      <description><![CDATA[我需要使用 numpy 找到两幅图像之间的相关性，但只能使用基本数学。我遇到了问题：“*
IndexError：索引 5434 超出了大小为 5434 的轴 0 的范围*”。我有代码。请告诉我该怎么做。
img = PIL.Image.open(&quot;SR1.png&quot;).convert(&quot;L&quot;)
im = numpy.array(img)
img2 = PIL.Image.open(&quot;SR61.png&quot;).convert(&quot;L&quot;)
im2 = numpy.array(img2)
np.array(im,dtype=float)
np.array(im2,dtype=float)
import math
import cmath
def correlationCoefficient(X, Y, n) : 
sum_X = 0
sum_Y = 0
sum_XY = 0
squareSum_X = 0
squareSum_Y = 0

i = 0
for i in range(n) : 
sum_X = sum_X + X[i]
sum_Y = sum_Y + Y[i] 
sum_XY = sum_XY + X[i] * Y[i] 
squareSum_X = squareSum_X + X[i] * X[i] 
squareSum_Y = squareSum_Y + Y[i] * Y[i] 

i = i + 1

corr = (n * sum_XY - sum_X * sum_Y)/(cmath.sqrt((n * squareSum_X - sum_X * sum_X)* (n * squareSum_Y - sum_Y * sum_Y))) 
返回 corr

X = im.flatten()
Y = im2.flatten()

n = len(X) 

print (&#39;{0:.6f}&#39;.format(correlationCoefficient(X, Y, n))) 

]]></description>
      <guid>https://stackoverflow.com/questions/59608470/how-to-find-correlation-between-two-images</guid>
      <pubDate>Mon, 06 Jan 2020 07:53:13 GMT</pubDate>
    </item>
    </channel>
</rss>