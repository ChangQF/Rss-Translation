<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 07 Feb 2024 09:14:31 GMT</lastBuildDate>
    <item>
      <title>训练图不可散列类型：'numpy.ndarray'</title>
      <link>https://stackoverflow.com/questions/77953227/train-plot-unhashable-type-numpy-ndarray</link>
      <description><![CDATA[我正在尝试进行时间序列预测来预测一类课程全天发生的次数，现在我正处于测试数据的阶段，但在运行此代码时出现错误，我添加了完整的错误代码也是如此
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-19-2a33e5b15fbb&gt;在&lt;细胞系：4&gt;()
      2 测试 = out[out.index &gt; pd.to_datetime(&quot;2020-11-01&quot;, format=&#39;%Y-%m-%d&#39;)]
      3
----&gt; 4 plt.plot（火车，颜色=“黑色”）
      5 plt.plot（测试，颜色=“红色”）
      6 plt.ylabel(&#39;患者数量&#39;)

7帧
/usr/local/lib/python3.10/dist-packages/matplotlib/category.py 中的 update(self, data)
    第212章
    213 可兑换 = 真
--&gt; [第 214 章]
    215 # OrderedDict 只是迭代数据中的唯一值。
    216 _api.check_isinstance((str, 字节), 值=val)

类型错误：不可散列的类型：&#39;numpy.ndarray&#39;

我从此博客获取的代码
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
out.groupby(&#39;doctor_id&#39;).plot()
plt.plot(out.index, out[&#39;count&#39;], )

火车 = out[out.index &lt; pd.to_datetime(&quot;2020-11-01&quot;, format=&#39;%Y-%m-%d&#39;)]
测试 = out[out.index &gt; pd.to_datetime(&quot;2020-11-01&quot;, format=&#39;%Y-%m-%d&#39;)]


&lt;前&gt;&lt;代码&gt;
plt.plot（火车，颜色=“黑色”）
plt.plot（测试，颜色=“红色”）
plt.ylabel(&#39;类数&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）
plt.title(“类数据的训练/测试分割”)
plt.show()

我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23]]></description>
      <guid>https://stackoverflow.com/questions/77953227/train-plot-unhashable-type-numpy-ndarray</guid>
      <pubDate>Wed, 07 Feb 2024 08:27:05 GMT</pubDate>
    </item>
    <item>
      <title>如何提取音频文件的一个嵌入？</title>
      <link>https://stackoverflow.com/questions/77952851/how-to-extract-one-embedding-for-audiofile</link>
      <description><![CDATA[我想从视频中提取音频组件的嵌入，以便进一步将它们合并到推荐系统中。
在此之前，我使用 XClip 提取视频的嵌入。我每个视频都有一个嵌入。嵌入的大小为 512。
如何获得相同的音频结果（每个音频一个嵌入）？
我想使用 Vggish。但这个模型与其他模型一样，提供了类似 [N_sequence, EmbSize] 的输出。例如。 [62, 128]。
我试图获取每个视频嵌入的平均值，但它产生了非常复杂的嵌入。许多完全不同的视频具有非常相似的音频嵌入（通过点积或余弦相似度）。]]></description>
      <guid>https://stackoverflow.com/questions/77952851/how-to-extract-one-embedding-for-audiofile</guid>
      <pubDate>Wed, 07 Feb 2024 07:15:50 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：值的长度（####）与索引的长度（####）不匹配[重复]</title>
      <link>https://stackoverflow.com/questions/77952546/valueerror-length-of-values-does-not-match-length-of-index</link>
      <description><![CDATA[我必须添加一列，其中包含特定行的特定值。所以我将 df 分成 3 并尝试添加值。
当添加到第二个 split df 时，我想出了
&lt;块引用&gt;
ValueError：值的长度 (4744) 与索引的长度 (1897) 不匹配

即使在 .reset_index(drop=True) 之后，它仍然显示相同的错误。
random_values = np.random.randint(10,25, size=len1)
# 将随机值分配给特定列
列名 = &#39;公交车站距离&#39;
split1[列名] = 随机值

random_values = np.random.randint(5,15, size=len2)
# 将随机值分配给特定列
split2.reset_index(drop=True)
列名 = &#39;公交车站距离&#39;
split2[列名] = 随机值

random_values = np.random.randint(0,6, size=len3)
# 将随机值分配给特定列
列名 = &#39;公交车站距离&#39;
split2[列名] = 随机值

这里，
&lt;前&gt;&lt;代码&gt;len1=len(split1)
len2=len(分割2)
len3=len(分割3)

我希望它添加与第一个拆分 (split1) 类似的值。即使删除索引后，它也是一样的。
我将 len2 作为 len(split2)。所以我想它应该有效。但仍然不是。]]></description>
      <guid>https://stackoverflow.com/questions/77952546/valueerror-length-of-values-does-not-match-length-of-index</guid>
      <pubDate>Wed, 07 Feb 2024 06:08:38 GMT</pubDate>
    </item>
    <item>
      <title>当我的特征变量大部分为零时我该怎么办？</title>
      <link>https://stackoverflow.com/questions/77952098/what-should-i-do-when-my-feature-variables-are-mostly-zero</link>
      <description><![CDATA[我有一组商店销售数据，我想使用外部 POI 特征及其人口统计因素来预测其他商店的销售情况。然而，我的特征变量几乎 80% 为零，其余 20% 有不同的范围。导致所有特征都高度倾斜。
我得到了一个较低的 r 平方值，我已经尝试过随机森林、XGBOOST 以及 cat boost 回归。]]></description>
      <guid>https://stackoverflow.com/questions/77952098/what-should-i-do-when-my-feature-variables-are-mostly-zero</guid>
      <pubDate>Wed, 07 Feb 2024 03:42:49 GMT</pubDate>
    </item>
    <item>
      <title>我如何为 edx Data Science R Capstone 项目做好准备？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77952089/how-can-i-prepare-myserlf-for-edx-data-science-r-capstone-project</link>
      <description><![CDATA[我如何为 edx Data Science R Capstone 项目做好准备？
我已经完成了HarvardX 的机器学习系列课程，我想要一些建议来为这个项目做好准备。
我需要为此预留多少小时？ （我做八点到五点的生意，我有家人）
以及其他人可以给我的建议。
课程网址如下：
https://learning.edx.org/course/课程-v1：HarvardX+PH125.8x+1T2023/home]]></description>
      <guid>https://stackoverflow.com/questions/77952089/how-can-i-prepare-myserlf-for-edx-data-science-r-capstone-project</guid>
      <pubDate>Wed, 07 Feb 2024 03:39:58 GMT</pubDate>
    </item>
    <item>
      <title>推理后如何向 PyTorch 转换器添加编码器/解码器层？</title>
      <link>https://stackoverflow.com/questions/77952028/how-to-add-encoder-decoder-layers-to-pytorch-transformers-after-inference</link>
      <description><![CDATA[我有一个 PyTorch 编码解码器转换器：我想保存模型，然后加载权重，除非在放大模型中。例如，我想在 3 个编码器层和 3 个解码器层上训练模型，保存参数。然后，我想加载所有参数，但在最后添加另一个随机编码器和解码器层。然后，我想在推理过程中冻结除新层之外的所有参数。这些本质上是适配器层，但我想将它们作为块来实现。
类代码：
类 Seq2SeqTransformer(nn.Module):

  def __init__(自我,
              num_encoder_layers: int,
              num_decoder_layers: int,
              嵌入大小：整数，
              nhead：整数，
              src_vocab_size：整数，
              tgt_vocab_size：整数，
              暗淡前馈：int = 512，
              辍学：浮动= 0.1）：
    超级(Seq2SeqTransformer, self).__init__()
    self.transformer = 变压器(d_model=emb_size,
                                    n头=n头，
                                    num_encoder_layers=num_encoder_layers,
                                    num_decoder_layers=num_decoder_layers,
                                    暗淡前馈=暗淡前馈，
                                    辍学=辍学）
    self.generator = nn.Linear(emb_size, tgt_vocab_size)
    self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
    self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
    self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

  def 向前（自身，
              src：张量，
              trg：张量，
              src_mask：张量，
              tgt_mask：张量，
              src_padding_mask：张量，
              tgt_padding_mask：张量，
              memory_key_padding_mask：张量）：
      src_emb = self.positional_encoding(self.src_tok_emb(src))
      tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
      outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, 无,
                              src_padding_mask、tgt_padding_mask、内存_key_padding_mask）
      返回 self.generator(outs)

  def 编码（自身，src：张量，src_mask：张量）：
      返回 self.transformer.encoder(self.positional_encoding(
                          self.src_tok_emb(src)), src_mask)

  def解码（自身，tgt：张量，内存：张量，tgt_mask：张量）：
      返回 self.transformer.decoder(self.positional_encoding(
                        self.tgt_tok_emb(tgt))，内存，
                        tgt_掩码）

]]></description>
      <guid>https://stackoverflow.com/questions/77952028/how-to-add-encoder-decoder-layers-to-pytorch-transformers-after-inference</guid>
      <pubDate>Wed, 07 Feb 2024 03:17:33 GMT</pubDate>
    </item>
    <item>
      <title>我该如何进行模型预测？</title>
      <link>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</guid>
      <pubDate>Wed, 07 Feb 2024 02:51:25 GMT</pubDate>
    </item>
    <item>
      <title>在非线性回归模型中测量 r2_score</title>
      <link>https://stackoverflow.com/questions/77951236/measuring-the-r2-score-in-a-non-linear-regression-model</link>
      <description><![CDATA[我想测量 kaggle 中可用的 kc_house_data 数据中的 r2_score，但我不明白为什么它给我一个负面结果。你能解释一下吗？
df[“组合”] = 卧室_norm + 浴室_norm + sqftliving_norm + sqftabove_norm + long_norm + lat_norm + sqftliving15_norm +yrbuilt_norm + Grade_norm + Floors_norm + sqftbasement_norm+condition_norm
cdf = df[[“组合”,“价格”]]

def log(a, Beta_1, Beta_2, Beta_3):
    y = (Beta_1 * (np.power(Beta_2, a)) + Beta_3)
    返回y
beta_3 = 0.10
贝塔_2 = 1.5
贝塔_1 = 1
x_data, y_data = (df[“组合”].值, df[“价格”].值)
x_data_norm = x_data / 最大值(x_data)
y_data_norm = y_data / max(y_data)
y_pred = log(x_data_norm, beta_1, beta_2, beta_3)
从 scipy.optimize 导入 curve_fit
popt, pcov = curve_fit(log, x_data_norm, y_data_norm)
x = np.linspace(4, 12, 21613)
x = x / 最大值(x)
plt.figure(figsize=(8,5))
y = log(x, *popt)
plt.plot(x_data_norm, y_data_norm, &#39;ro&#39;, label=&#39;data&#39;)
plt.plot(x, y, 线宽=3.0, 标签=&#39;适合&#39;)
plt.legend(loc=&#39;最佳&#39;)
plt.ylabel(&#39;价格&#39;)
plt.xlabel(&#39;组合&#39;)
plt.show()

从 sklearn.metrics 导入 r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))


我尝试在 sklearn.metrics 中使用 r2_score，我期望得到一个 0 到 1 之间的数字，但我不明白为什么它计算出负数
from sklearn.metrics import r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))
R2 分数：-59.51
]]></description>
      <guid>https://stackoverflow.com/questions/77951236/measuring-the-r2-score-in-a-non-linear-regression-model</guid>
      <pubDate>Tue, 06 Feb 2024 22:40:53 GMT</pubDate>
    </item>
    <item>
      <title>二值图像分割训练期间类别概率下降</title>
      <link>https://stackoverflow.com/questions/77951083/class-probability-decreasing-during-binary-image-segmentation-training</link>
      <description><![CDATA[我正在使用 Keras 中的标准 UNet 架构训练用于裂纹检测的二值图像分割模型，其中裂纹是少数类别。对于损失函数，我使用 Focal Loss，其 alpha 为 0.8，gamma 为 2，因为我读到它在处理不平衡数据时很有用。
在训练期间，训练和验证损失会大幅下降。尽管如此，我还是有一个回调来预测一些样本测试图像，并注意到裂纹类别概率随着时间的推移而降低。最终，预测掩模完全被背景类覆盖。我不确定这里发生了什么，因为我认为焦点损失可以解决这个问题。我也尝试过使用 Dice Loss 来避免调整参数，但看到了类似的行为。
我有什么遗漏的吗？我的标签为 0 表示背景，1 表示裂纹，并且我在最终模型层中使用 sigmoid 函数，以及在卷积块中使用增强层（翻转/随机变换）和 Dropout。]]></description>
      <guid>https://stackoverflow.com/questions/77951083/class-probability-decreasing-during-binary-image-segmentation-training</guid>
      <pubDate>Tue, 06 Feb 2024 21:59:52 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust 的“ort”包中使用 ONNX CLIP ViT-B-32，收到有关无效输入维度的错误</title>
      <link>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</guid>
      <pubDate>Tue, 06 Feb 2024 20:43:57 GMT</pubDate>
    </item>
    <item>
      <title>图像的梯度批处理</title>
      <link>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</link>
      <description><![CDATA[我正在尝试使用 gradio.File() 处理 gradio 中的图像，但它需要一个 Zip 文件，然后我必须解压缩它并复制路径，然后打开图像。我的算法适用于 gr.image() 打开的图像，但是当我尝试对多个图像执行相同操作时，它无法正常工作。我试图找出答案，但没有成功。我想批量打开图像，它应该与 gr.Image() 相同，但多个图像。
我尝试了选择 zip 文件的方法，但它无法正确处理图像。但对于像 gr.Image() 这样的单个图像，它工作得很好。]]></description>
      <guid>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</guid>
      <pubDate>Tue, 06 Feb 2024 20:09:38 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以从 PyCaret 中的现有模型进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Roberta 计算最后 4 个隐藏层的加权和？</title>
      <link>https://stackoverflow.com/questions/77933640/how-to-calculate-the-weighted-sum-of-last-4-hidden-layers-using-roberta</link>
      <description><![CDATA[这篇论文中的表格解释了获得嵌入的各种方法，我认为这些方法也适用于 Roberta：

我正在尝试使用 Roberta 计算最后 4 个隐藏层的加权和来获得令牌嵌入，但我不知道这是否是正确的方法，这是我尝试过的代码：
从变压器导入 RobertaTokenizer, RobertaModel
进口火炬

tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
模型 = RobertaModel.from_pretrained(&#39;roberta-base&#39;)
Caption = [&#39;这是一只黄色的鸟&#39;, &#39;示例标题&#39;]

tokens = tokenizer(标题, return_tensors=&#39;pt&#39;, padding=True)

input_ids = 标记[&#39;input_ids&#39;]
注意掩码 = 标记[&#39;注意掩码&#39;]

输出=模型（input_ids，attention_mask，output_hidden_​​states = True）

状态 = 输出.hidden_​​states
token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()
]]></description>
      <guid>https://stackoverflow.com/questions/77933640/how-to-calculate-the-weighted-sum-of-last-4-hidden-layers-using-roberta</guid>
      <pubDate>Sat, 03 Feb 2024 20:34:36 GMT</pubDate>
    </item>
    <item>
      <title>导入bertopic时导入UMAP的问题</title>
      <link>https://stackoverflow.com/questions/75158273/problem-in-importing-umap-while-importing-bertopic</link>
      <description><![CDATA[所以我的代码一切正常，然后突然 hdbscan 不再工作了，然后我重新安装了所有软件包，现在我遇到了 umap 问题。
我按照此处和其他论坛中的建议进行了操作，卸载并重新安装了 umap-learn 和 bertopic 。我可以将 umap 导入为 import umap 或 import umap.umap_ as UMAP ，问题是当我导入 bertopic 时。我尝试过：
导入bertopic

和
导入 umap.umap_ 作为 UMAP
导入bertopic

和
导入umap
导入bertopic

和
导入umap
从 bertopic 导入 BERTopic

最后：
导入 umap.umap_ 作为 UMAP
从 bertopic 导入 BERTopic

在所有情况下，当我导入 bertopic 时都会出现问题： ImportError: Cannot import name &#39;UMAP&#39; from &#39;umap&#39; (unknown location) 。我也重新启动机器几次。我不认为这个问题与环境有关，因为我之前在相同的代码运行时一直使用相同的环境： Python 3.10.7 和 Visual Code Studio 1.74.3 。 bertopic版本为0.13.0，umap-learn版本为0.5.3]]></description>
      <guid>https://stackoverflow.com/questions/75158273/problem-in-importing-umap-while-importing-bertopic</guid>
      <pubDate>Wed, 18 Jan 2023 11:06:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的决策树创建的分割实际上并未划分样本？</title>
      <link>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</link>
      <description><![CDATA[这是我对著名的 Iris 数据集进行二特征分类的基本代码： 
from sklearn.datasets import load_iris
从 sklearn.tree 导入 DecisionTreeClassifier，export_graphviz
从 graphviz 导入源

虹膜 = load_iris()
iris_limited = iris.data[:, [2, 3]] # 这仅获取花瓣长度 &amp;宽度。

# 我使用最大深度来避免过度拟合
# 并简化树，因为我将其用于教育目的
clf = DecisionTreeClassifier(criterion=&quot;基尼&quot;,
                             最大深度=3，
                             随机状态=42）

clf.fit(iris_limited, iris.target)

Visualization_raw = Export_graphviz(clf,
                                    out_file=无，
                                    特殊字符=真，
                                    feature_names=[“长度”,“宽度”],
                                    类名=iris.target_names,
                                    节点 ID=真）

可视化源 = 源（可视化_原始）
Visualization_png_bytes = Visualization_source.pipe(format=&#39;png&#39;)
将 open(&#39;my_file.png&#39;, &#39;wb&#39;) 作为 f：
    f.write（可视化_png_bytes）

当我检查树的可视化时，我发现了这一点：

乍一看这是一棵相当正常的树，但我注意到它有一些奇怪的地方。节点 #6 共有 46 个样本，其中只有一个是杂色的，因此该节点被标记为 virginica。这似乎是一个相当合理的停留地点。然而，由于某种我无法理解的原因，该算法决定进一步分为节点#7 和#8。但奇怪的是，仍然存在的 1 个 versicolor 仍然被错误分类，因为无论如何两个节点最终都具有 virginica 类。它为什么要这样做？它是否盲目地只关注基尼系数的下降，而不考虑它是否有任何影响——这对我来说似乎是奇怪的行为，而且我在任何地方都找不到它的记录。
是否可以禁用，或者这实际上是正确的吗？]]></description>
      <guid>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</guid>
      <pubDate>Sat, 28 Apr 2018 14:25:02 GMT</pubDate>
    </item>
    </channel>
</rss>