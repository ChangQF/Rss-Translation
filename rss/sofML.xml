<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 09 Feb 2024 12:23:05 GMT</lastBuildDate>
    <item>
      <title>我如何构建我的神经 Newtork 输出层以允许复杂的输出决策</title>
      <link>https://stackoverflow.com/questions/77967751/how-do-i-structure-my-neural-newtork-output-layer-to-allow-for-complex-output-de</link>
      <description><![CDATA[我目前正在尝试教 AI 玩纸牌/骰子游戏 DragonWood。我生成了一个 python 库来处理游戏规则，并且有一个简单的确定性算法来做出决策。我正在尝试用经过训练的模型替换该算法。
DragonWood 是一款最多 4 名玩家的游戏，涉及攻击生物/增强卡和玩家冒险家卡，并滚动骰子以查看该卡是否被捕获。生物卡在捕获时给予积分，而增强卡则对玩家进行修改，例如为骰子滚动添加修饰符或允许再次滚动骰子。游戏结束时得分最多的人获胜。
景观中始终有 5 张生物/增强卡（每包 42 张），玩家手上最多可拥有 9 张冒险家卡。冒险家牌有5种花色，编号为1至12，共64张牌。
每回合玩家可以执行以下操作：

抽一张牌
攻击

如果攻击玩家可以从手中选择以下其中一项：

顺子
多张同花色的牌
多张相同价值的卡片

然后他们会得到与他们选择用来攻击的卡牌数量相匹配的骰子数量。他们掷骰子，如果总数大于或等于卡牌的防御值，他们就会捕获该卡牌。
我希望使用 NEAT 算法来复制这个决定。我在设计神经网络的输出层时遇到困难。
输入层可能是：

当时景观中生物/增强卡的单热编码
玩家手中冒险家卡牌的一次性编码
游戏状态的一些其他详细信息。

所以输入层是 42 + 64 + 4 个节点。
我需要网络告诉我要做什么决定，即攻击/重投，以及是否攻击要攻击的卡牌以及要攻击的卡牌。我目前对输出层的想法是它需要

要攻击的卡的单热编码
用于攻击的卡片的单一热编码，
2 个决策节点

所以 42 + 64 + 2 个节点。
这感觉输出节点太多，而且网络需要学会只用手中的牌进行攻击。我假设我可以对玩家手牌的输出应用一个掩码来强制执行此操作。
此架构是否有效？是否有更好的方法在输出层中对决策进行编码？]]></description>
      <guid>https://stackoverflow.com/questions/77967751/how-do-i-structure-my-neural-newtork-output-layer-to-allow-for-complex-output-de</guid>
      <pubDate>Fri, 09 Feb 2024 11:22:06 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 - 如何根据相对大小对图像中的对象进行计数/分类</title>
      <link>https://stackoverflow.com/questions/77967671/computer-vision-how-to-count-categorize-objects-in-image-based-on-relative-siz</link>
      <description><![CDATA[我被难住了。经过一番谷歌搜索和修补后，我对如何解决我的问题一无所获。
我想训练一个用于计算机视觉的 ML 模型来扫描一堆不同尺寸管道的正面照片。该堆叠可以包含各种直径和各种壁厚。
理想情况下，我想要一个具有管道外径上限（例如：56“）和最小管道直径下限（例如：9”）的模型。可能的管道将基于可用尺寸的库存。
有没有办法训练模型来估计集合中最大或最小管道的尺寸，计算实例并对图像中每个不同的管道尺寸重复相同的过程？
我遇到的一切都是为了寻找清晰、可识别的物体或“哪个与另一个不同”的问题。有没有人尝试过进行排序&amp;根据相对大小计算模型？]]></description>
      <guid>https://stackoverflow.com/questions/77967671/computer-vision-how-to-count-categorize-objects-in-image-based-on-relative-siz</guid>
      <pubDate>Fri, 09 Feb 2024 11:07:32 GMT</pubDate>
    </item>
    <item>
      <title>预测机器学习中看不见的输入以获得最佳输出 python 我上周问了这个问题，但它被关闭了，没有人知道如何回答</title>
      <link>https://stackoverflow.com/questions/77967312/predict-unseen-inputs-for-optimum-output-in-machine-learning-python-i-asked-this</link>
      <description><![CDATA[我不知道为什么他们说我的问题不具体，尽管它很清楚：
我用随机森林算法（你可以使用任何算法）编写了一个机器学习模型，并用我的数据集成功地训练了它，它准确地预测了我的测试集输入 (x) 的输出 (y)。到这里为止它工作正常。现在我想添加一个部分，它还可以预测 x 的新未给定值，它认为 y 最大。所以我希望它基本上针对任何 x 值优化 y，而不需要给它输入 x。如果需要的话，这是我的代码：（其中没有关于我想添加的部分，它只是训练和测试）：
&lt;前&gt;&lt;代码&gt;
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 r2_score
将 matplotlib.pyplot 导入为 plt
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.ensemble 导入 RandomForestRegressor


excel_file_path = r&#39;文件位置&#39;
df = pd.read_excel(excel_file_path)
df.columns=[&#39;Chordwise_Portion&#39;,&#39;变形&#39;,&#39;TSR&#39;,&#39;CP/CP_baseline&#39;]
df[&#39;CP/CP_baseline&#39;] = pd.to_numeric(df[&#39;CP/CP_baseline&#39;], 错误=&#39;强制&#39;)

训练集 = df.iloc[0:350, 0:4]
test_set = df.iloc[350:649, 0:4]


defscale_dataset（数据框）：
  X = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values
  定标器=标准定标器()
  X = 缩放器.fit_transform(X)
  数据 = np.hstack((X, np.reshape(y, (-1, 1))))
  返回数据，X，y

训练，X_train，y_train =scale_dataset（训练集）
测试，X_测试，y_测试=scale_dataset（测试集）


#射频
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)


# 计算 R 平方
r2 = r2_score(y_test, y_pred)
r2 = r2*100
打印（）
print(f&#39;R平方: {r2} %&#39;)
打印（）


#最优预测
最大训练=最大（y_训练）
index_max_train = np.where(y_train == np.max(y_train)) ###
max_pred = max(y_pred)
index_max_pred = [np.where(y_pred == max_pred)[0][0]+350]
data_pred = df.iloc[index_max_pred,0:3] ###

######
max_value = max(max_train, max_pred)
如果 max_value == max_train：
  max_data = df.iloc[index_max_train]
别的：
  最大数据 = 数据预测

print(&quot;Cp/Cp_baseline 的最大总体值为：&quot;, max_value,&quot; 对于以下条件：&quot;)
打印（最大数据）
打印（）
print(&quot;Cp/Cp_baseline 的最大预测值为：&quot;, max_pred,&quot; 对于以下条件：&quot;)
打印（数据预测）

# 绘制结果
plt.scatter(X_test[:, 0], y_test, label=&#39;真实数据&#39;)
plt.scatter(X_test[:,0], y_pred, color=&#39;r&#39;, label=&#39;预测数据&#39;)
plt.xlabel(&#39;Chordwise_portion&#39;)
plt.ylabel(&#39;Cp/Cp_baseline&#39;)
title = &quot;随机森林算法 - R^2 = {:.4f} %&quot;.format(r2)

plt.标题（标题）
plt.图例()
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/77967312/predict-unseen-inputs-for-optimum-output-in-machine-learning-python-i-asked-this</guid>
      <pubDate>Fri, 09 Feb 2024 10:04:52 GMT</pubDate>
    </item>
    <item>
      <title>为多标签 ViTForImageClassification 准备数据集</title>
      <link>https://stackoverflow.com/questions/77967230/prepare-a-dataset-for-multilabel-vitforimageclassification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77967230/prepare-a-dataset-for-multilabel-vitforimageclassification</guid>
      <pubDate>Fri, 09 Feb 2024 09:52:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 lcmm 在 R 中进行网格搜索。我可以使用 GPU 吗？</title>
      <link>https://stackoverflow.com/questions/77966884/gridsearch-in-r-using-lcmm-can-i-utilize-gpu</link>
      <description><![CDATA[我正在使用 lcmm 包在 R 中执行基于组的轨迹建模。这是我的代码：
fitGbtmql = 函数(k) {
  开始 = 系统时间()
  模型 = do.call(gridsearch, 列表(m = makeGbtmCallql(k), 代表 = 5, maxiter = 5, 迷你 = gbtm_modelsql[[&#39;1&#39;]], cl = 10))
  model$runTime = Sys.time() - 开始
  返回（型号）
}

哪里
makeGbtmCallql(k)

是一个具有 k 个簇的 hlme 函数。
我的数据集非常大，大约有 430 万行。网格搜索当然非常耗时，而且 10 核并行处理似乎也没有多大帮助。我可以将核心数提高到更高，甚至达到 20 个，但我怀疑这会产生很大的影响。
如果有任何帮助，我相信并行处理的 cl 参数基于此代码
gridsearch.parallel &lt;- 函数(m,rep,maxiter,minit,cl=NULL)
{
  if(!is.null(cl)){
    clusterSetRNGStream(cl)
    mc &lt;- match.call()$m
    mc$maxiter &lt;- maxiter
    分配（“迷你”，评估（迷你））
    
    clusterCall(cl, function () require(lcmm))
    clusterExport(cl, list(“mc”, “maxiter”, “minit”, as.character(as.list(mc[-1])$data)), envir = 环境())
    
    cat(“请耐心等待，网格搜索正在运行...\n”)
    
    模型 &lt;- parLapply(cl, 1:rep, function(X)
    {
      mc$B &lt;- 替代(随机(minit),parent.frame(n=2))
      return(do.call(as.character(mc[[1]]),as.list(mc[-1])))
    }
    ）
    
    llmodels &lt;- sapply(模型,函数(x){return(x$loglik)})
    
    kmax &lt;- which.max(llmodels)
    mc$B &lt;- 型号[[kmax]]$best
    mc$maxiter &lt;- NULL
    
    cat(&quot;搜索完成，进行最终估计\n&quot;)
    
    return(do.call(as.character(mc[[1]]),as.list(mc[-1])))
  }
  return(do.call(gridsearch, as.list(match.call()[2:5])))
}

（来源：https://github.com/CecileProust-Lima/lcmm/问题/39）
我想知道是否有一种方法可以在另一个使用 GPU 的包中运行它，或者使用其他方式来通过 CPU 加速它（我读了一些关于超线程的内容，也许还可以将其关闭，或者其他）。我在 R 和编码方面都是新手，并且绝对是机器学习方面的大菜鸟，因此任何建议都会非常有帮助。
我的硬件是：
I9-13900k
RTX A4000
64GB DDR5]]></description>
      <guid>https://stackoverflow.com/questions/77966884/gridsearch-in-r-using-lcmm-can-i-utilize-gpu</guid>
      <pubDate>Fri, 09 Feb 2024 08:42:38 GMT</pubDate>
    </item>
    <item>
      <title>nengoDL 安装问题 nengo 版本 4</title>
      <link>https://stackoverflow.com/questions/77966701/nengodl-installation-issue-with-nengo-version-4</link>
      <description><![CDATA[我尝试运行尖峰神经网络的代码，该模型使用以下代码将 CNN 转换为尖峰：
导入nengo
导入nengo_dl

将张量流导入为 tf
#print(cnn_model1.summary())

平方英尺= 20
转换器 = nengo_dl.Converter(
    函数模型，
    交换激活={
        tf.keras.activations.relu: nengo.SpikingRectifiedLinear()},
    scale_firing_rates=sfr,
    突触=0.005，
    inference_only=假）

但是我收到如下错误消息
用户警告：此版本的 NengoDL 尚未使用您的 Nengo 版本 (4.0.0) 进行测试。完全支持的最新版本是 3.2.0。
      警告.warn(warnstr)
 

和
ModuleNotFoundError：没有名为“keras.engine”的模块

然后尝试使用此代码安装nengo旧版本
pip install nengo==3.2.0 nengo-dl==3.4.0 nengo-gui==0.4.7

出现同样的错误
谁能帮我解决这个错误]]></description>
      <guid>https://stackoverflow.com/questions/77966701/nengodl-installation-issue-with-nengo-version-4</guid>
      <pubDate>Fri, 09 Feb 2024 07:54:02 GMT</pubDate>
    </item>
    <item>
      <title>2048游戏的AI</title>
      <link>https://stackoverflow.com/questions/77966419/ai-for-2048-game</link>
      <description><![CDATA[我有用 python 编写的 2048 游戏。
我给自己定义了一个项目，创建一个人工智能来玩这个游戏，不输给2048，并一直成为赢家。
重点是，我不想使用任何特殊的算法来做到这一点，只想创建一个神经网络并向其提供数据以进行学习并每次都变得更好。
我只知道我需要一个好游戏的数据集和一个具有 16 个输入的神经网络，用于显示每次移动的游戏板状态，以及最后的 4 个输出用于显示方向。
我说了很多话才达到这个地步 -&gt;问题是我不知道从哪里开始，因为我是人工智能的新手
我首先需要一个数据集，我没有找到好的东西，如果有人有好的数据集并将其发送给我，我表示感谢。
另外，如果您知道启动此项目的好资源或类似的从头开始执行此操作的资源，请告诉我。
非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/77966419/ai-for-2048-game</guid>
      <pubDate>Fri, 09 Feb 2024 06:47:10 GMT</pubDate>
    </item>
    <item>
      <title>即使将其设置为可训练，机器学习模型也不会训练权重</title>
      <link>https://stackoverflow.com/questions/77965679/machine-learning-model-does-not-train-weights-even-after-setting-it-trainable</link>
      <description><![CDATA[以下是我一直在研究的模型。我一直在尝试使用生成器使用数据集 UCF-101 来训练它，但由于某种原因，即使将 VGG19 预训练的层设置为可训练后，它们仍然不会更新其权重值。我尝试了很多替代方案，但到目前为止没有任何效果。
我看到所有其他图层权重都在更新，但那些没有。
我的问题是：
什么可能导致某些可训练层在反向传播期间无法训练？
这是我的模型。
导入tensorflow为tf
从tensorflow.keras.applications导入VGG19
从tensorflow.keras.layers导入层
从tensorflow.keras.layers导入乘法、Conv2D、注意力
从tensorflow.keras.layers导入（TimeDistributed、LSTM、Dense、Dropout、Flatten、GlobalAveragePooling2D、
                                     批量归一化）
从tensorflow.keras.models导入顺序


SpatialAttentionLayer 类（图层）：
    def __init__(self, **kwargs):
        super(SpatialAttentionLayer, self).__init__(**kwargs)

    def 构建（自身，input_shape）：
        self.conv2d = Conv2D(filters=512, kernel_size=(7, 7), 激活=&#39;softmax&#39;, padding=&#39;same&#39;)

    def 调用（自身，输入）：
        注意力 = self.conv2d(输入)
        return Multiply()([输入，注意力])


类 TemporalAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(TemporalAttentionLayer, self).__init__(**kwargs)

    def 构建（自身，input_shape）：
        self.kernel = self.add_weight(name=&#39;kernel&#39;,
                                      形状=(输入形状[-1], 1),
                                      初始值设定项=&#39;glorot_uniform&#39;,
                                      可训练=真）

    def 调用（自身，输入）：
        Attention_scores = tf.keras.backend.dot(输入，self.kernel)
        注意分数= tf.keras.backend.squeeze（注意分数，-1）
        注意分数 = tf.keras.backend.softmax(注意分数)
        Attention_sequence = 输入 * Attention_scores[..., None]
        attend_sequence = tf.reduce_sum(attended_sequence, axis=1)
        返回参加序列


# todo：为文档创建模型图
def ActionDetectionModel（num_frames，frame_width，frame_height，通道，num_classes，lstm_units = 256，
                         dend_units=1024，dropout_rate=0.5，fine_tune_until=None）：
    # 加载 VGG19 模型，不包括顶层
    base_model = VGG19（include_top = False，权重=&#39;imagenet&#39;，input_shape =（frame_width，frame_height，通道））

    对于 base_model.layers 中的图层：
        可训练层 = False
    如果fine_tune_until：
        对于 base_model.layers[-fine_tune_until:] 中的图层：
            层.可训练= True

    # 定义顺序模型
    模型=顺序（[
        # 添加TimeDistributed层通过VGG19处理每一帧
        TimeDistributed(base_model, input_shape=(num_frames,frame_width,frame_height,channels)),

        #TimeDistributed(SpatialAttentionLayer()),
        时间分布式(GlobalAveragePooling2D()),

        时间分布(Flatten()),

        # 用于时间处理的 LSTM 层
        LSTM（lstm_units，return_sequences = True），

        # 添加时间注意力机制
        时间注意力层（），

        # 用于正则化的批量归一化
        批量归一化(),

        # 用于进一步处理的密集层
        密集（dense_units，激活=&#39;relu&#39;），
        辍学率（辍学率），

        # 最终预测层
        密集（num_classes，激活=&#39;softmax&#39;）
    ]）

    返回模型



有一次，我在模型的开头添加了一个简化的 SpatialAttentionLayer；那时，该模型尚未接受训练。
我根据 VGG19 更改了模型，认为反向传播期间可能会发生一些事情。
我已经改变了学习率几次，但这些权重没有改变。有一次，我将整个 VGG19 设置为可训练，但什么也没有。]]></description>
      <guid>https://stackoverflow.com/questions/77965679/machine-learning-model-does-not-train-weights-even-after-setting-it-trainable</guid>
      <pubDate>Fri, 09 Feb 2024 02:00:44 GMT</pubDate>
    </item>
    <item>
      <title>如何查明两个相似图像之间的差异[关闭]</title>
      <link>https://stackoverflow.com/questions/77965431/how-to-pinpoint-differences-between-two-similar-images</link>
      <description><![CDATA[有关我的问题的一些背景信息。我有一个想要自动化的任务。我收到了两张图片

显示某些事物/部件应位于何处的模板
我必须将图像与模板进行比较。任何缺失的部件我都必须手动放置。

我想编写一个程序，可以采用模板并检查第 2 项是否缺少任何部分。
我已阅读此内容。然而，我的模板是电脑pdf文件，图像是用手机拍摄的照片。因此 stackoverflow 中的建议不太适合我的场景。
有没有办法在丢失的东西上放置一个边界框？
一个例子可以是

我收到了一张数字棋盘的屏幕截图，棋子随机分布在方格上

我收到一张真实棋盘的图片，必须检查它是否与屏幕截图相符。


我想在缺少棋子的棋子上放置一个边界框
我尝试使用结构相似性，但不断获得相似性分数。我更希望获得显示差异的输出图像。同样，我尝试了边缘检测和减法。但由于图像的大小或比例不同，因此它永远不会抵消]]></description>
      <guid>https://stackoverflow.com/questions/77965431/how-to-pinpoint-differences-between-two-similar-images</guid>
      <pubDate>Fri, 09 Feb 2024 00:16:37 GMT</pubDate>
    </item>
    <item>
      <title>如何修复错误：AttributeError：“VotingRegressor”对象没有属性“_model_meta”？</title>
      <link>https://stackoverflow.com/questions/77965050/how-to-fix-the-error-attributeerror-votingregressor-object-has-no-attribute</link>
      <description><![CDATA[我正在从事预测分析并使用 XGBoost 和支持向量回归器模型。
我集成了这两个模型，现在我需要在进行推理时验证这个模型。
集成是通过 VotingRegressor 完成的：
ensemble_model = VotingRegressor([
        （&#39;XGBoost&#39;，model_prod_xgb），
        (&#39;SVR&#39;, model_prod_svr)])
        
ensemble_model.fit(X_train, y_train)

产生错误的部分是这样的：
run_id = ensemble_model._model_meta.run_id

AttributeError：“VotingRegressor”对象没有属性“_model_meta”

如何修复此错误？]]></description>
      <guid>https://stackoverflow.com/questions/77965050/how-to-fix-the-error-attributeerror-votingregressor-object-has-no-attribute</guid>
      <pubDate>Thu, 08 Feb 2024 22:11:35 GMT</pubDate>
    </item>
    <item>
      <title>就地修剪 nn.Linear 权重会导致意外错误，需要稍微奇怪的解决方法。需要解释</title>
      <link>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</link>
      <description><![CDATA[失败
导入火炬

def 测试1():
  层 = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试1()

有错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-bb36a010bd86&gt;在&lt;细胞系：10&gt;()
      8 x = 5 - torch.sum(layer(torch.ones(90)))
      9 x.backward()
---&gt; 10 测试1()
     11 # 这也有效
     12

2帧
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
    249 # 一些 Python 版本打印多行函数的第一行
    [第 250 章]
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError: 函数 TBackward0 在索引 0 返回无效渐变 - 得到 [10, 90] 但预期形状与 [10, 100] 兼容

这有效
导入火炬

def test2():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x #主要变化
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试2()

这也有效
导入火炬
def test3():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight) #主要变化
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试3()

我在尝试实现一篇关于模型剪枝（Temporal Neuron Variance Pruning）的论文时遇到了这个问题。我相信这与 autograd 图有关，但我不确定到底发生了什么。我已经看到了有关修剪的链接，并使用第三个片段让我的代码正常工作。我现在正在尝试找出为什么 1 和 2 不起作用。是否有一些解释为什么这些几乎相同的代码片段有效或失败？
我想弄清楚的要点 -

什么是TBackward0
在哪里定义的
哪里引发了运行时错误
为什么需要与旧形状兼容 - 特别是当梯度已正确修改时（我假设我已正确编辑张量，因为情况 2、3 有效）
我可以更改其他内容（除了 2 个工作案例之外）来实现此功能吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</guid>
      <pubDate>Thu, 08 Feb 2024 05:17:32 GMT</pubDate>
    </item>
    <item>
      <title>在 esp32-cam 中使用人脸识别时出现错误 cam_hal：EV-VSYNC-OVF</title>
      <link>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</link>
      <description><![CDATA[我正在我的 esp32-cam 板上使用示例“CameraWebServer”。上传设置如下：
开发板：AI Thinker ESP32-CAM；
CPU频率：240MHz；
闪光频率：80 Mhz；
闪光模式：QIO。
Arduino 集成开发环境 2.0.0
esp32 乐鑫 版本 2.0.14

通过这些设置，我可以上传我的代码，但粪便识别功能不起作用。当我单击“注册面部”时，没有任何反应，并且我的串行监视器显示消息 EV-VSYNC-OVF。如何解决这个问题？
此外，我已经尝试修改上传设置并更改文件“CameraWebServer.ino”中的参数 config.frame_size 和 config.xclk_freq_hz，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</guid>
      <pubDate>Wed, 07 Feb 2024 22:12:55 GMT</pubDate>
    </item>
    <item>
      <title>如何让这个专家混合模型在张量流中工作？</title>
      <link>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</link>
      <description><![CDATA[我有两个张量。
张量 1 的形状为 (10, None, 16, 16, 64)
张量 2 的形状为 (None, 10)
“无”是c的批量大小
第一个张量表示来自 10 个不同模型的 logits 集合 (10)，其形状是每组 logits，(None) 是批量大小，(16, 16, 64) 是相应模型的输出。
第二个张量表示来自 1 个较小模型的一组 logits（无），即批处理大小，(10) 是 10 个值，表示第一个张量中每组 10 个 logits 的权重应如何。
我想将第一个张量乘以第二个张量，以便输出形状为 (10, None, 16, 16, 64)，并且第一个轴上的每组 logit 由第二个张量的相应 logit 进行加权
然后，我将对第一个轴上的相乘张量求和，以获得 MoE 模型的一个块的输出
以下是所有这些的实施方式（顺便说一下，MOPE 代表预训练专家的混合）：
def CreateMOPEBlock(x, 块, blockNum):
    专家日志 = []

    对于范围内的 i（num_classes）：
        块[i].trainable = False
        ExpertLogits.append(块[i](x))

    门控输入 = x
    GatingConv1 = tf.keras.layers.Conv2D(16, (3, 3), padding=&#39;相同&#39;)(GatingInput)
    GatingLayerNorm1 = tf.keras.layers.LayerNormalization()(GatingConv1)
    GatingLeakyReLU1 = tf.keras.layers.LeakyReLU()(GatingLayerNorm1)
    GatingConv2 = tf.keras.layers.Conv2D(32, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU1)
    GatingLayerNorm2 = tf.keras.layers.LayerNormalization()(GatingConv2)
    GatingLeakyReLU2 = tf.keras.layers.LeakyReLU()(GatingLayerNorm2)
    GatingConv3 = tf.keras.layers.Conv2D(64, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU2)
    GatingLayerNorm3 = tf.keras.layers.LayerNormalization()(GatingConv3)
    GatingLeakyReLU3 = tf.keras.layers.LeakyReLU()(GatingLayerNorm3)
    GatingFlatten = tf.keras.layers.Flatten()(GatingLeakyReLU3)
    GatingLogits = tf.keras.layers.Dense（num_classes，激活=&#39;softmax&#39;）（GatingFlatten）

    logits1 = ExpertLogits # 形状：(10, 无, 16, 16, 64)
    logits2 = GatingLogits # 形状：（无，10）

    # 在这里做一些奇特的数学计算
    多重逻辑 = ?

    返回 tf.keras.layers.add(multiple_logits)

MOPEInput = tf.keras.layers.Input(形状=(32, 32, 3))

# Block1、2和3只是10个keras顺序模型的数组
MOPEBlock1 = CreateMOPEBlock(MOPEInput, 块1)
MOPEBlock2 = CreateMOPEBlock(MOPEBlock1, 块2)
MOPEBlock3 = CreateMOPEBlock(MOPEBlock2, 块3)

MOPEFlatten = tf.keras.layers.Flatten()(MOPEBlock3)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEFlatten)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEX)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEOutput = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;)(MOPEX)

MOPEModel = tf.keras.Model(MOPEInput, MOPEOutput)

我已经尝试自己解决这个问题了！多次！
我还尝试询问多种大型语言模型，从 Mixtral-8x7b（以我的名字命名）到 GPT4。
结果看起来像这样：
将张量流导入为 tf

# 假设这些是你的张量
张量1 = tf.placeholder(tf.float32, shape=(10, 无, 16, 16, 64))
张量2 = tf.placeholder(tf.float32, shape=(无, 10))

# 重塑张量2（无，10）-&gt; （无、10、1、1、1）
tensor2_expanded = tf.expand_dims(tf.expand_dims(tf.expand_dims(tensor2, axis=-1), axis=-1), axis=-1)

# 排列张量1的轴 (10, None, 16, 16, 64) -&gt; （无、10、16、16、64）
tensor1_permuted = tf.transpose(tensor1, perm=[1, 0, 2, 3, 4])

# 张量相乘
结果= tf.multiply（tensor1_permuted，tensor2_expanded）

# 最后，将结果的轴排列回来 (None, 10, 16, 16, 64) -&gt; （10、无、16、16、64）
结果 = tf.transpose(结果, perm=[1, 0, 2, 3, 4])

即使对每个模型进行了广泛的调试，这些模型的解决方案也不起作用。
我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</guid>
      <pubDate>Wed, 07 Feb 2024 21:13:53 GMT</pubDate>
    </item>
    <item>
      <title>如何将极坐标数据框与 scikit-learn 一起使用？</title>
      <link>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</link>
      <description><![CDATA[我无法将极坐标数据帧与 scikitlearn 一起使用进行机器学习训练。
目前，我正在极坐标中进行所有数据帧预处理，并且在模型训练期间，我将其转换为 pandas 数据帧以使其正常工作。
是否有任何方法可以直接使用 Polars 数据帧进行 ML 训练而不将其更改为 pandas？]]></description>
      <guid>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</guid>
      <pubDate>Fri, 11 Nov 2022 05:59:55 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络只预测一件事</title>
      <link>https://stackoverflow.com/questions/62712282/my-neural-network-only-predicts-one-thing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/62712282/my-neural-network-only-predicts-one-thing</guid>
      <pubDate>Fri, 03 Jul 2020 09:16:53 GMT</pubDate>
    </item>
    </channel>
</rss>