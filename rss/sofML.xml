<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 08 Aug 2024 06:22:56 GMT</lastBuildDate>
    <item>
      <title>我正在编写决策树修剪算法[关闭]</title>
      <link>https://stackoverflow.com/questions/78846680/i-am-writing-a-decision-tree-pruning-algorithm</link>
      <description><![CDATA[我正在尝试修剪决策树，这是我的代码，它没有按照我的预期工作，这里到底出了什么问题，
我试图实现的是，我试图根据 2 个标准（纯度阈值和人口阈值）修剪树，下面的算法没有给我我想要的结果，我这里缺少什么条件或检查，如何使其更强大？
# 使用前序遍历和标准修剪树的函数
def prune_preorder(inner_tree, index, purity_threshold=95,population_threshold=5):
# 处理当前节点
node_samples = inner_tree.n_node_samples[index]
# 修复修剪节点的特征确定逻辑
feature = feature_names[inner_tree.feature[index]] if inner_tree.feature[index] != -1 else &quot;Leaf节点”
阈值 = f” &lt;= {inner_tree.threshold[index]:.2f}”如果 inner_tree.feature[index] != -1 否则””
值 = inner_tree.value[index][0]
多数类别计数 = max(值)
纯度 = (多数类别计数 / 总和(值)) * 100 如果节点样本 &gt; 0 否则 0
人口百分比 = (节点样本 / 总样本) * 100 如果节点样本 &gt; 0 else 0

# 在修剪决策之前打印节点统计信息
print(f&quot;Node {feature} {index} : Samples={node_samples}, Purity={purity:.2f}%, Population={population_pct:.2f}%&quot;)

# 检查是否应修剪节点
if purity &gt;= purity_threshold orpopulation_pct &lt;人口阈值：
print(f&quot;修剪节点 {feature} {index} {threshold} (纯度： {purity:.2f}%, 人口： {population_pct:.2f}%)&quot;)
# 通过将子指针设置为 -1 将节点标记为叶子
inner_tree.children_left[index] = -1
inner_tree.children_right[index] = -1
else:
# 如果存在左子树，则遍历左子树
if inner_tree.children_left[index] != -1:
prune_preorder(inner_tree, inner_tree.children_left[index], purity_threshold, 人口阈值)

# 如果存在右子树，则遍历右子树
if inner_tree.children_right[index] != -1:
prune_preorder(inner_tree, inner_tree.children_right[index], purity_threshold,人口阈值)

# 使用前序遍历从根节点应用修剪
prune_preorder(clf.tree_, 0, purity_threshold=95,population_threshold=5)

# 为修剪后的树创建节点标签
node_labels_pruned = custom_node_labels(clf, feature_names, xmtrain, ymtrain)

理想情况下，树在满足修剪标准的任何地方都不应有子节点，但我看到一些标记为叶子的节点没有样本，还有更多子节点，理想情况下不应该这样。]]></description>
      <guid>https://stackoverflow.com/questions/78846680/i-am-writing-a-decision-tree-pruning-algorithm</guid>
      <pubDate>Thu, 08 Aug 2024 05:47:13 GMT</pubDate>
    </item>
    <item>
      <title>未找到与 torch==1.9.1 匹配的分布</title>
      <link>https://stackoverflow.com/questions/78846461/no-matching-distribution-found-for-torch-1-9-1</link>
      <description><![CDATA[我尝试使用 google colab 安装 torchmeta，但它依赖于 torch&lt;1.10.0 and &gt;=1.4.0，每当我尝试安装 torch 1.9.0 或任何版本的 torch&lt;1.10.0 and &gt;=1.4.0 时，都会出现以下错误：

错误：找不到满足要求 torch==1.9.1 的版本（来自版本：1.11.0、1.12.0、1.12.1、1.13.0、1.13.1、2.0.0、2.0.1、2.1.0、2.1.1、2.1.2、2.2.0、2.2.1， 2.2.2、2.3.0、2.3.1、2.4.0)错误：未找到与 torch==1.9.1 匹配的发行版

如何解决此问题？
我正在使用 Google colab，Python 版本 3.10.12。
如果我遇到的问题无法解决，请告诉我使用 colab 安装 torchmeta 的其他方法。
我正在尝试安装 torch&lt;1.10.0 和 &gt;1.4.0，然后安装依赖于我提到的 torch 版本 torch&lt;1.10.0 和 &gt;1.4.0 的 torchmeta。]]></description>
      <guid>https://stackoverflow.com/questions/78846461/no-matching-distribution-found-for-torch-1-9-1</guid>
      <pubDate>Thu, 08 Aug 2024 03:54:54 GMT</pubDate>
    </item>
    <item>
      <title>人工智能技术用于查找两个时间序列之间的相关性/模式/共同趋势[关闭]</title>
      <link>https://stackoverflow.com/questions/78846294/ai-techniques-to-find-correlation-pattern-common-trend-between-two-time-series</link>
      <description><![CDATA[我有一个想法，使用人工智能技术在两个连续的时间序列数据之间找到有用的信息。结果可以是相关值、共同模式或趋势等，输出结果如 TS1（时间序列 1）数据与 TS2 数据相关，反之亦然。
稍后我想特别指出这种关系究竟发生在哪里，以及它是什么类型的效果。

例如：
输入：过去 5 年的 TS1 和 TS2 数据。[浮点/双精度值]
过程：寻找相关性/模式/共同趋势。[这是我寻求指导的部分。]
输出：TS1 的变化每个月都会对 TS2 产生负面影响。 [输出文本可以由

过程部分的结果组成。]

加载数据、将其传递给模型/系统，并为非技术人员解释结果并不是一项艰巨的任务。对我来说，有趣的部分是如何找到某种关系。
到目前为止，我已经使用了 Person、Spearman 和 Kendall 相关性，并且根据我的要求，它工作得很好。但是，我想了解和使用多种技术，尤其是高级统计和机器学习模型。
由于我是时间序列数据的新手，我对选择正确的路径来实现上述目标的知识有限。所以，有人可以指导我哪些高级技术/模型（静态、机器学习等）适合找到两个连续时间序列数据之间的关系？
提前谢谢您。
祝您有美好的一天！ :)]]></description>
      <guid>https://stackoverflow.com/questions/78846294/ai-techniques-to-find-correlation-pattern-common-trend-between-two-time-series</guid>
      <pubDate>Thu, 08 Aug 2024 02:13:43 GMT</pubDate>
    </item>
    <item>
      <title>face_recognition 模块以某种方式严重干扰了 Speech_recognition 模块（python）</title>
      <link>https://stackoverflow.com/questions/78845929/face-recognition-module-somehow-badly-interfering-with-speech-recognition-module</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845929/face-recognition-module-somehow-badly-interfering-with-speech-recognition-module</guid>
      <pubDate>Wed, 07 Aug 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>数值数据中的异常值检测问题</title>
      <link>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</link>
      <description><![CDATA[我目前正在进行一个数据分析项目，其中我使用 Z 分数来检测数据集数值列中的异常值。但是，我遇到了一个问题，合法的数据点被标记为异常值，我不确定为什么会发生这种情况。
这是我正在做的事情：

缺失值的插补：我使用 sklearn.impute 中的 IterativeImputer 来填充数值列中的缺失值。

异常值检测：我计算每个数值列的 Z 分数，以使用阈值 3 来检测异常值。
例如，我有一条关于埃及古典式摔跤运动员 Yasser Abdel Rahman Sakr 的记录，其属性如下：

体重：120 公斤
身高：180 厘米



尽管这些是合理的测量值，但该记录被标记为我的代码中的异常值。其他记录也出现了此问题。
以下是我的代码的相关部分：
import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer

# 假设“数据”已定义并加载
numeric_cols = data.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
categorical_cols = data.select_dtypes(include=[&#39;object&#39;]).columns

# 在数字列中插入缺失值
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
df_numeric = pd.DataFrame(mice_imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)

# 将插入的数字列与原始分类列合并列
df_MICE = pd.concat([df_numeric, data[categorical_cols]], axis=1)

# 存储异常信息的字典
outliers_info = {}

for col in numeric_cols:
# 计算平均值和标准差
mean = df_MICE[col].mean()
std_dev = df_MICE[col].std()

# 如果 std_dev 为零，则避免除以零
if std_dev == 0:
print(f&quot;列 {col} 的标准差为零。跳过异常值检测。)
继续

# 计算 Z 分数
z_scores = (df_MICE[col] - mean) / std_dev

# 定义异常值阈值
阈值 = 3

# 查找异常值
outliers = df_MICE[np.abs(z_scores) &gt;阈值]

# 将异常值的数量和异常值样本存储在字典中
outliers_info[col] = {
&#39;count&#39;: len(outliers),
&#39;sample&#39;: outliers.head(1) # 一个异常值的样本
}

# 打印每个数值列的异常值数量和样本
for col, info in outliers_info.items():
print(f&#39;Column: {col}&#39;)
print(f&#39;Number of outliers: {info[&quot;count&quot;]}&#39;)
if info[&#39;count&#39;] &gt; 0：
print(&#39;样本异常值：&#39;)
print(info[&#39;sample&#39;])
else:
print(&#39;无异常值。&#39;)
print() # 打印空白行以提高可读性

问题：
尽管是真实且可信的记录，但 Yasser Abdel Rahman Sakr 的体重和身高被标记为异常值。其他记录也会出现此问题。
问题：

什么原因导致合法数据点被标记为异常值？
是否有任何改进或替代方法可以更好地处理此情况下的异常值检测？
我是否应该考虑其他因素或异常值检测方法？
]]></description>
      <guid>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</guid>
      <pubDate>Wed, 07 Aug 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Mediapipe 根据特定面部区域过滤面部标志坐标？</title>
      <link>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</guid>
      <pubDate>Wed, 07 Aug 2024 20:41:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中使用 knnsearch 设置 k 值</title>
      <link>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</link>
      <description><![CDATA[我有一个代码来对图像进行分类。
training1 = xlsread(&#39;Data Train&#39;);

% 提及训练数据矩阵在 excel 文件中的位置
training = [training1(:,1) training1(:,2) training1(:,3) training1(:,4) training1(:,5) training1(:,6) training1(:,7) training1(:,8) training1(:,9) training1(:,10) training1(:,11) training1(:,12) training1(:,13) training1(:,14) training1(:,15) training1(:,16) training1(:,17) training1(:,18) training1(:,19) training1(:,20) training1(:,21) training1(:,22) training1(:,23) training1(:,24)];

% 提及输入数据变量
Z=[MeanR MeanG MeanB MeanH MeanS MeanV VarRed VarGreen VarBlue VarH VarS VarV RangeR RangeG RangeB RangeH RangeS RangeV sdR sdG sdB sdH sdS sdV];

%执行 knn 分类
result = knnsearch(training,Z);

if (result&gt;=1 &amp;&amp; result&lt;=20)
set(handles.EditBox,&#39;string&#39;,&#39;Raw&#39;);
elseif (result&gt;=21 &amp;&amp; result&lt;=40)
set(handles.EditBox,&#39;string&#39;,&#39;Undercook&#39;);
elseif (result&gt;=41 &amp;&amp; result&lt;=60)
set(handles.EditBox,&#39;string&#39;,&#39;Cook&#39;);
elseif (result&gt;=61 &amp;&amp; result&lt;=80)
set(handles.EditBox,&#39;string&#39;,&#39;Rotten&#39;);
end

knnsearch 语法是否只默认 k 值为 1？
如何才能让 knnsearch 中的 k 值为 5？
当我尝试将其更改为
k = 5;
result = knnsearch(training,Z,&#39;K&#39;,k); 

系统不显示分类结果。]]></description>
      <guid>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</guid>
      <pubDate>Wed, 07 Aug 2024 17:01:04 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中的学习率</title>
      <link>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</link>
      <description><![CDATA[在梯度下降算法中，我根据它们的导数更新B和M值，然后将它们与学习率值相乘，但是当我对L使用相同的值，例如0.0001时，它不能正常工作。减小或增加L值不起作用。作为一种解决方法，我不得不为b和m值设置不同的L值。这是正常的还是有错误？
import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
veri_seti = &quot;study_score_decreasing.csv&quot; #study_score_decreasing.csv #study_score_increasing.csv 
data = pd.read_csv(veri_seti)

# 梯度下降 Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
m_gradient = 0
b_gradient = 0
n = len(points)

for i in range(n):
x = points.iloc[i].study_time
y = points.iloc[i].score

m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
b_gradient += -(2/n) * (y - (m_next * x + b_next))

m = m_next - m_gradient * 0.0001 #(L = 0.0001)
b = b_next - b_gradient * 0.1 #(L = 0.1)

return m, b

# 图形选项 图表
def show_graph(m, b):
plt.scatter(data.study_time, data.score, color=&quot;red&quot;)
x_range = range(int(data.study_time.min()), int(data.study_time.max()) + 1)
plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
plt.xlabel(&#39;学习时间&#39;)
plt.ylabel(&#39;分数&#39;)
plt.title(&#39;学习时间与分数&#39;)
plt.show()
time.sleep(0.001)
print(&quot;=&gt; F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Ana Fonksiyon
def main(m, b, L, epochs):
print(&quot;=&gt; F(X):&quot;, m, &quot;X&quot;, b)

for i in range(epochs):
m, b = gradient_descent(m, b, data, L)
show_graph(m, b)

# 基础说明
main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)

我逐个更新了L值，得到了合乎逻辑的结果，但是用一个共同的L值，为什么解看起来不合逻辑？]]></description>
      <guid>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</guid>
      <pubDate>Wed, 07 Aug 2024 16:59:10 GMT</pubDate>
    </item>
    <item>
      <title>构建模拟 SVM 模型的自定义分类器</title>
      <link>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</link>
      <description><![CDATA[我在代码中使用了以下 SVM：
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classes_report, confusion_matrix, f1_score

# 加载数据
data = pd.read_csv(&#39;data.csv&#39;)

# 分离特征 (X) 和目标变量 (y)
X = data.drop(columns=&#39;label&#39;)
y = data[&#39;label&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义小网格搜索的参数网格
param_grid = {
&#39;C&#39;: [0.1, 1, 10],
&#39;gamma&#39;: [&#39;scale&#39;, 0.01, 0.1]
}

# 执行带有交叉验证的网格搜索
grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;), param_grid, cv=3,scoring=&#39;f1_weighted&#39;, verbose=2, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# 来自网格搜索的最佳参数
best_params = grid_search.best_params_
print(f&#39;Best parameters: {best_params}\n&#39;)

# 训练 SVM 模型使用最佳参数
svm_best = SVC(kernel=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=best_params[&#39;gamma&#39;])
svm_best.fit(X_train_scaled, y_train)

# 对测试集进行预测
y_pred_best = svm_best.predict(X_test_scaled)

# 对改进模型的评估
print(&quot;改进的 SVM 模型评估&quot;)
print(confusion_matrix(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))
improved_f1 = f1_score(y_test, y_pred_best, average=&#39;weighted&#39;)
print(f&#39;改进的加权 F1 分数： {improved_f1}\n&#39;)


如您所见，我直接使用来自 sklearn 的 SVM 模型。我如何创建一个名为“分类器”的类，它将执行相同的操作并获得相同的结果？这可能吗？
我尝试创建类并使用每个函数的参数，但结果总是更糟。]]></description>
      <guid>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</guid>
      <pubDate>Wed, 07 Aug 2024 12:39:48 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow Pipeline 中对大型数据集应用图像增强？</title>
      <link>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</link>
      <description><![CDATA[我有一个图像数据集，每个图像包含一个 1 到 5 个字母的单词。我想使用深度学习对每个图像中组成单词的字符进行分类。这些图像的标签格式如下：
totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar
我正尝试将这些图像加载到 TensorFlow 管道中，以降低由于内存限制而导致的复杂性。下面是我从目录加载和处理图像和标签的代码：
def process_img(file_path):
label = get_label(file_path)
image = tf.io.read_file(file_path)
image = tf.image.decode_png(image, channels=1) 
image = tf.image.convert_image_dtype(image, tf.float32) 
target_shape = [695, 1204]
image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])

# 对标签进行编码
coded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
coded_label.set_shape([5, len(urdu_alphabets)])

return image,coded_label
input_dir = &#39;/kaggle/input/dataset/Data/*&#39;
images_ds = tf.data.Dataset.list_files(input_dir, shuffle=True)

train_count = int(tf.math.round(len(images_ds) * 0.8))
train_ds = images_ds.take(train_count)
test_ds = images_ds.skip(train_count)
train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)

train_ds 如下所示：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;
现在，我想对图像应用简单的增强，例如旋转、剪切、侵蚀和扩张。我最初使用了以下函数：
def augment(image, label):
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode=&#39;nearest&#39;, cval=0.0, interpolation_order=1)
image = tf.image.random_zoom(image, [0.85, 0.85])
image = tf.image.random_shear(image, 0.3)
image = tf.image.random_shift(image, 0.1, 0.1)
return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

但是，tf.image 中的许多函数都已弃用。如何以高效的方式在 TensorFlow 管道中将这些增强应用于图像？
注意：我可以通过不使用 TensorFlow 管道使用 NumPy 数组加载图像来执行这些增强，但我的数据集非常大（110 万张图像），因此我需要一种高效的方法来执行此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</guid>
      <pubDate>Wed, 31 Jul 2024 14:11:01 GMT</pubDate>
    </item>
    <item>
      <title>Python Darts 中的 RNN 训练指标</title>
      <link>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</link>
      <description><![CDATA[我目前正在使用 python darts 训练 RNNModel。为了比较不同的训练模型，我想从 fit 方法中提取 train_loss 和 val_loss。我该怎么做？我读过一些关于度量集合的内容，但不知道如何使用它。
这是我当前的代码
from darts.models import RNNModel
from darts import TimeSeries

train = # training data as TimeSeries
model = RNNModel(model=&quot;LSTM&quot;, input_chunk_length=self.past_samples)
model.fit(train)

训练期间，控制台中会显示损失，但我不知道如何访问它。
到目前为止，我尝试在网上查找任何文档，并向 bing chat 和 ChatGPT 寻求帮助。但是他们告诉我使用不存在的 model.history.history[&quot;loss&quot;]]]></description>
      <guid>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</guid>
      <pubDate>Tue, 12 Mar 2024 05:29:49 GMT</pubDate>
    </item>
    <item>
      <title>无法在 python 中安装 lap==0.4.0 库</title>
      <link>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</guid>
      <pubDate>Tue, 13 Jun 2023 09:55:26 GMT</pubDate>
    </item>
    <item>
      <title>Pyspark 中的过采样或 SMOTE</title>
      <link>https://stackoverflow.com/questions/53936850/oversampling-or-smote-in-pyspark</link>
      <description><![CDATA[我有 7 个类，总记录数为 115，我想对这些数据运行随机森林模型。但由于数据不足以获得高精度。所以我想对所有类进行过采样，使多数类本身获得更高的计数，然后少数类获得更高的计数。这在 PySpark 中可行吗？
+---------+-----+
| SubTribe|count|
+---------+-----+
| Chill| 10|
| Cool| 18|
|Adventure| 18|
| Quirk| 13|
| Mystery| 25|
| Party| 18|
|Glamorous| 13|
+---------+-----+
]]></description>
      <guid>https://stackoverflow.com/questions/53936850/oversampling-or-smote-in-pyspark</guid>
      <pubDate>Wed, 26 Dec 2018 20:31:36 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的神经元应该是异步的吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</link>
      <description><![CDATA[我正在设计一个神经网络，并试图确定我是否应该以这样一种方式编写它，即每个神经元都是 Erlang 中的自己的“进程”，或者我是否应该只使用 C++ 并在一个线程中运行一个网络（我仍然会通过在每个网络自己的线程中运行一个实例来使用我的所有核心）。
是否有充分的理由放弃 C++ 的速度而选择 Erlang 提供的异步神经元？]]></description>
      <guid>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</guid>
      <pubDate>Thu, 07 Jul 2016 16:17:43 GMT</pubDate>
    </item>
    </channel>
</rss>