<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 24 Nov 2024 18:21:42 GMT</lastBuildDate>
    <item>
      <title>加载的模型数据与测试数据不同吗？</title>
      <link>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</link>
      <description><![CDATA[我将一个经过训练的模型加载到 R studio 4 中，当我使用 xgboost 进行预测时，它总是说：
 存储在对象和新数据中的特征名称不同

我检查了训练和测试的数据，列名和顺序相同，但测试数据有一个额外的列作为响应/依赖变量，用于预测结果或将预测与其进行比较。
没有它，你怎么知道要预测什么？
loadedmodel$feature_names（打印出 6 列）

test &lt;- as.matrix(test2)

test$feature_names（它说原子向量，实际上是 6 列和 1 个依赖列与预测结果相同）

dtest&lt;- xgb.DMatrix(test2, missing=NaN)

pred&lt;- predict(loadedmodel, dtest)

什么是错了吗？
MADZ]]></description>
      <guid>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</guid>
      <pubDate>Sun, 24 Nov 2024 17:33:43 GMT</pubDate>
    </item>
    <item>
      <title>当模型训练完成时，演员-评论家的标准差是否会收敛到 0？</title>
      <link>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</link>
      <description><![CDATA[我正在使用 Advantage Actor-Critic 算法来解决我的问题。
actor NN 预测我采样动作的平均值和标准偏差。
这是否意味着预测的标准偏差会随着时间的推移而减少，直到模型收敛时变得太小？
或者即使 A&amp;C 模型收敛，我也“注定”总是具有随机性？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</guid>
      <pubDate>Sun, 24 Nov 2024 16:32:22 GMT</pubDate>
    </item>
    <item>
      <title>在验证期间计算 mIoU</title>
      <link>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</link>
      <description><![CDATA[我正在研究二元分割任务，并已实施以下训练和验证循环。
我需要两点帮助：
如何计算每个时期后每个类别的 IoU，并打印第 1 类 IoU、第 2 类 IoU 和总体 mIoU 分数？
根据最佳 mIoU 分数或最低验证损失保存模型更好吗？
任何指导都将不胜感激。谢谢！
这是我的代码：
# 初始化列表以存储损失值
train_losses = []
val_losses = []

# 训练和验证循环
for epoch in range(n_eps):
model.train()
train_loss = 0.0

# 训练循环
for images, mask in tqdm(train_loader):
images, mask = images.to(device), mask.to(device)

optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, mask)
loss.backward()
optimizer.step()

train_loss += loss.item()

avg_train_loss = train_loss / len(train_loader)
train_losses.append(avg_train_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}], Train Loss: {avg_train_loss:.4f}&quot;)

model.eval()
val_loss = 0.0

# 验证循环
with torch.no_grad():
for images, mask in val_loader:
images, mask = images.to(device), mask.to(device)
output = model(images)
val_loss += criterion(outputs, mask).item()

avg_val_loss = val_loss / len(val_loader)
val_losses.append(avg_val_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}], Val Loss: {avg_val_loss:.4f}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</guid>
      <pubDate>Sun, 24 Nov 2024 15:59:00 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 可加性检查失败，RandomForestClassifier 的 SHAP 值达到天文数字</title>
      <link>https://stackoverflow.com/questions/79220150/shap-additivity-check-fails-with-astronomical-shap-values-for-randomforestclassi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79220150/shap-additivity-check-fails-with-astronomical-shap-values-for-randomforestclassi</guid>
      <pubDate>Sun, 24 Nov 2024 13:20:28 GMT</pubDate>
    </item>
    <item>
      <title>缺失值和特征选择</title>
      <link>https://stackoverflow.com/questions/79219843/missing-values-and-feature-selection</link>
      <description><![CDATA[在将数据加载到 Vertex AI 表格数据集之前，我正在构建一个用于特征选择的 Kubeflow 管道（我们稍后想使用 AutoML 建模，以及一些 Python 自定义模型作为比较）。
我有一个 Pandas 数据框，其中包含：
100 个数字特征和 6 个字符串（无数组或任何结构）分类特征。所有特征都有一些缺失值。
在执行任何模型训练之前，我被要求使用 scikit-learn SelectKBest 对整个数据集执行特征选择，然后将其拆分为训练集和测试集。这是因为我必须将选定的特征加载到 Vertex AI 表格数据集中，然后我们在建模时将数据拆分为训练/验证/测试集。
我最初的计划是：

使用独热编码对分类特征进行编码
使用任何平均值/中位数/零插补来填补缺失值（这只是一个 PoC）
使用 SelectKBest 进行特征选择
使用选定的特征名称来检索仍包含 NaN 的特征
加载到 Vertex AI 表格数据集

然后我会拆分训练/测试集并分别填补缺失值，以避免稍后从 Vertex AI 表格数据集中泄露数据
这有意义吗？或者我应该：

独热编码分类特征
拆分训练/测试集
分别估算缺失值
使用 SelectKBest 进行特征选择
为训练/测试集分别创建 Vertex AI 表格数据集

如此处所建议的那样]]></description>
      <guid>https://stackoverflow.com/questions/79219843/missing-values-and-feature-selection</guid>
      <pubDate>Sun, 24 Nov 2024 10:25:26 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Visual Studio 代码中训练监督和机器学习模型</title>
      <link>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</link>
      <description><![CDATA[我在 Nvidia RTX 4070 8Gb Vram 上运行 Visual Studio 代码。
我花了很长时间训练一些模型，而我朋友的 4070 上花费的时间比我少得多。
有什么建议吗？
更新了 CUDA 驱动程序和工具包。确实将我的电池设置为高性能模式（但我认为这不是问题所在）
检查 Python 包和框架版本是否与我朋友的设置相同。
从 cmd 检查 Nvidia-Smi 中的使用情况，但似乎没有使用 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</guid>
      <pubDate>Sun, 24 Nov 2024 06:39:27 GMT</pubDate>
    </item>
    <item>
      <title>Python scorecardpy：UnboundLocalError：赋值前引用了局部变量“card_df”</title>
      <link>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</link>
      <description><![CDATA[我使用 scorecardpy 函数来获取模型：
import scorecardpy as ac
card=sc.scorecard(bins_adj, lr, X_train.columns)

然后我尝试使用以下代码保存此模型：
import numpy as np
np.save(&#39;card.npy&#39;,card)

之后我尝试重新加载此模型：
card=np.load(&#39;card.npy&#39;,allow_pickle=True)

然后我想使用该模型获取分数：
score=sc.scorecard_ply(data_train, card, print_step=0)

但它给出了错误：
UnboundLocalError Traceback（最近一次调用最后一次）
单元格在 [91]，第 1 行
score=sc.scorecard_ply(data_train, card, print_step=0)

文件 ~/.local/lib/python3.9/site-packages/scorecardpy/scorecard.py:330，在 scorecard_ply(dt, card, only_total_score, print_step, replace_blank_na, var_kp)
card_df=card.copy(deep=True)
# x 变量
xs=card_df.loc[card_df.variable != &#39;basepoints&#39;, &#39;variable&#39;].unique()
# x 变量的长度
xs_len=len(xs)

UnboundLocalError：局部变量“card_df”在赋值前被引用

我无法解决这个问题问题。请帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</guid>
      <pubDate>Sun, 24 Nov 2024 04:16:39 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮我把这个图弄得更整洁一点吗？</title>
      <link>https://stackoverflow.com/questions/79218937/can-someone-help-me-make-this-graph-look-a-bit-neater</link>
      <description><![CDATA[import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import datetime

# 加载特斯拉数据集
tesla_stock = pd.read_csv(&#39;C:/Users/Admin/Downloads/AI/Tesla.csv&#39;)

# 动态处理缺失或重命名的“收盘价”列
possible_target_columns = [&#39;Close&#39;, &#39;Last&#39;, &#39;Price&#39;, &#39;Adjusted Close&#39;, &#39;VWAP&#39;, 
&#39;Close/Last&#39;]
target_column = None

# 打印可用列以进行调试
print(&quot;数据集中可用的列：&quot;, tesla_stock.columns)

for col in possible_target_columns:
if col in tesla_stock.columns:
target_column = col
print(f&quot;使用 &#39;{col}&#39; 作为预测的目标列。&quot;)
break

# 处理未找到合适列的情况
if target_column is None:
print(&quot;未找到合适的预测列。请检查数据集。&quot;)
raise KeyError(&quot;确保数据集包含带有股票价格的列（例如，&#39;Close&#39;、
&#39;Close/Last&#39;）。&quot;)

# 清理数字列（删除 &#39;$&#39; 并转换为浮点数）
for column in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close/Last&#39;, &#39;Volume&#39;]:
if column in tesla_stock.columns:
tesla_stock[column] = tesla_stock[column].replace(&#39;[\$,]&#39;, &#39;&#39;, 
regex=True).astype(float)

# 将“日期”转换为日期时间并设置为索引
tesla_stock[&#39;Date&#39;] = pd.to_datetime(tesla_stock[&#39;Date&#39;])
tesla_stock.set_index(&#39;Date&#39;, inplace=True)

# 定义特征和目标变量
features = [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Volume&#39;]
X = tesla_stock[features]
y = tesla_stock[target_column]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 进行预测
predicted = model.predict(X_test)

# 评估模型
print(&quot;模型得分 (R²)：&quot;, model.score(X_test, y_test))
print(&quot;平均绝对误差：&quot;, metrics.mean_absolute_error(y_test, predicted))
print(&quot;均方误差：&quot;, metrics.mean_squared_error(y_test, predicted))
print(&quot;均方根误差：&quot;, math.sqrt(metrics.mean_squared_error(y_test, 
predicted)))

# 绘制测试集的实际价格与预测价格
dfr = pd.DataFrame({&#39;Actual&#39;: y_test, &#39;Predicted&#39;: predicted})
plt.figure(figsize=(14, 8))
dfr.head(25).plot(kind=&#39;bar&#39;, figsize=(14, 8))
plt.title(&quot;实际价格与预测价格&quot;)
plt.xlabel(&quot;样本&quot;)
plt.ylabel(&quot;价格 (USD)&quot;)
plt.xticks(rotation=45, ha=&quot;right&quot;)
plt.tight_layout()
plt.show()

# 预测未来 30 天
last_date = tesla_stock.index[-1] # 历史数据中的最后一个日期
last_price = tesla_stock[target_column].iloc[-1] # 历史数据中的最后一个价格
future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, 31)] # 
生成未来 30 天

# 创建占位符 DataFrame用于未来特征
future_features = pd.DataFrame(index=future_dates)
for feature in features:
if feature in tesla_stock.columns:
future_features[feature] = tesla_stock[feature].mean() # 使用每个特征的平均值

# 使用训练模型预测未来价格
future_predictions = model.predict(future_features)

# 结合历史和未来数据，绘制无缝图
all_dates = list(tesla_stock.index) + list(future_dates) # 结合历史和未来日期
all_prices = list(tesla_stock[target_column]) + list(future_predictions) # 结合历史和预测价格

# 绘制历史数据和未来预测
plt.figure(figsize=(14, 8))
plt.plot(tesla_stock.index, tesla_stock[target_column], label=&#39;历史价格&#39;, 
color=&#39;blue&#39;)
plt.plot(future_dates, future_predictions, label=&#39;30 天未来预测&#39;, 
color=&#39;red&#39;)
plt.title(&#39;特斯拉未来 30 天股价预测&#39;)
plt.xlabel(&#39;日期&#39;)
plt.ylabel(&#39;价格 (美元)&#39;)
plt.legend()
plt.xticks(rotation=45)
plt.show()

# 显示未来预测
future_features[&#39;Predicted_Close&#39;] = future_predictions
print(future_features[[&#39;Predicted_Close&#39;]])

输入图片描述在这里
我试图让红线连接到图表的末端，然后弯曲到它所在的位置。而不是一开始就只是一条随机的短红线，因为它看起来有点混乱。我一直在尝试这样做，但我真的很难做到。如果有人能帮助我，我将不胜感激，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79218937/can-someone-help-me-make-this-graph-look-a-bit-neater</guid>
      <pubDate>Sat, 23 Nov 2024 22:00:52 GMT</pubDate>
    </item>
    <item>
      <title>抱抱脸生成方法后内存增加</title>
      <link>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</link>
      <description><![CDATA[我想使用 huggingface 的 codegemma 模型进行推理，但当我使用 model.generate(**inputs) 方法时，无论 max_token_len 数量是多少，使用 torch profiler 时，GPU 内存成本在峰值使用量中都会从 39 GB 增加到 49 GB。我知道我们需要在推理和上下文中保存模型的激活，例如 4096 个输入标记，但我不敢相信它可以增加 10 GB 的推理内存使用量。有人可以解释一下这是怎么回事吗？提前谢谢您。]]></description>
      <guid>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</guid>
      <pubDate>Sat, 23 Nov 2024 19:21:47 GMT</pubDate>
    </item>
    <item>
      <title>错误：TypeError：无法将 cuda:0 设备类型张量转换为 numpy。首先使用 Tensor.cpu() 将张量复制到主机内存</title>
      <link>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</guid>
      <pubDate>Sat, 23 Nov 2024 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>具有动态约束的贷款人和借款人之间的多元化资金分配算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</guid>
      <pubDate>Sat, 23 Nov 2024 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>如何改进 CNN 的图像分类能力</title>
      <link>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</guid>
      <pubDate>Sat, 23 Nov 2024 15:26:06 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法使用 ML 从合同中提取信息，并将合同文件和目标字符串作为输入和输出？</title>
      <link>https://stackoverflow.com/questions/59409984/is-there-a-way-to-extract-information-from-contracts-using-ml-with-including-con</link>
      <description><![CDATA[我正在研究是否可以自动从合同中提取某些字段的信息（例如合同方、开始和结束日期）。
我想知道是否可以用机器学习提取这些信息，将整个合同作为输入，将信息作为输出，而无需标记或注释整个文本。
我理解应该针对每个目标字段分别运行提取。]]></description>
      <guid>https://stackoverflow.com/questions/59409984/is-there-a-way-to-extract-information-from-contracts-using-ml-with-including-con</guid>
      <pubDate>Thu, 19 Dec 2019 12:38:43 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 中的 KMeans 聚类</title>
      <link>https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark</link>
      <description><![CDATA[我有一个包含许多列的 spark 数据框“mydataframe”。我尝试仅在两列上运行 kmeans：lat 和 long（纬度和经度），使用它们作为简单值。我想仅基于这两列提取 7 个聚类，然后将聚类分配附加到我的原始数据框。我试过了：
从 numpy 导入数组
从 math 导入 sqrt
从 pyspark.mllib.clustering 导入 KMeans、KMeansModel

# 准备一个只有 2 列的数据框：
data = mydataframe.select(&#39;lat&#39;, &#39;long&#39;)
data_rdd = data.rdd # 需要是 RDD
data_rdd.cache()

# 构建模型（对数据进行聚类）
clusters = KMeans.train(data_rdd, 7, maxIterations=15, initializationMode=&quot;random&quot;)

但过了一会儿我收到错误：

org.apache.spark.SparkException：作业因阶段失败而中止：阶段 5191.0 中的任务 1 失败 4 次，最近一次失败：丢失任务1.3 阶段 5191.0（TID 260738，10.19.211.69，执行器 1）：org.apache.spark.api.python.PythonException：Traceback（最近一次调用最后一次）

我尝试分离并重新连接集群。结果相同。我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark</guid>
      <pubDate>Fri, 01 Dec 2017 02:22:01 GMT</pubDate>
    </item>
    <item>
      <title>从自然语言文本中提取数据[关闭]</title>
      <link>https://stackoverflow.com/questions/11962955/extracting-data-from-natural-language-text</link>
      <description><![CDATA[我有一组文本报纸广告，我想提取诸如所售商品及其价格等信息。这些广告不遵循任何结构化格式。我可以访问数千条此类广告。
我应该从哪里开始这个项目？有没有可以提供帮助的图书馆？]]></description>
      <guid>https://stackoverflow.com/questions/11962955/extracting-data-from-natural-language-text</guid>
      <pubDate>Wed, 15 Aug 2012 01:21:53 GMT</pubDate>
    </item>
    </channel>
</rss>