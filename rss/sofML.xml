<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 09:16:16 GMT</lastBuildDate>
    <item>
      <title>我如何重新训练 Xgboost 回归器？</title>
      <link>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</link>
      <description><![CDATA[我现在正在做的实习要求模型“定期更新新数据”。如何在 Xgboost 上做到这一点？我已将模型保存为 pickle 文件
在项目中，我将从后端获取新数据，并且我应该定期重新训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</guid>
      <pubDate>Tue, 18 Jun 2024 08:51:09 GMT</pubDate>
    </item>
    <item>
      <title>每次运行机器学习预测都错误</title>
      <link>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</link>
      <description><![CDATA[我训练了一个机器学习模型，测试集中的几行相同的数据（1800 行中的大约 100 行）在使用不同种子的每次运行（10 次）时都会给出错误的预测。我应该对此做些什么吗？例如将其放入训练集中以将一些数据从训练集换到测试集，还是保持原样？
使用不同的种子运行模型 10 次。有些行始终得到错误的预测。]]></description>
      <guid>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</guid>
      <pubDate>Tue, 18 Jun 2024 08:37:10 GMT</pubDate>
    </item>
    <item>
      <title>根据 HistGratientBoostingClassifier 绘制决策树</title>
      <link>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</link>
      <description><![CDATA[我有一个 HistGradientBoostingClassifier 模型，我想绘制一个或多个决策树，但我找不到原生函数来执行此操作，我可以访问 Tree 预测器对象及其节点，但为了将其绘制到 sklearn.tree.plot_tree 函数中，它需要是 DecisionTree 类型的对象
我试过这个：
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

出现此错误：

InvalidParameterError：plot_tree 的“decision_tree”参数必须
是“sklearn.tree._classes.DecisionTreeClassifier”的实例或
“sklearn.tree._classes.DecisionTreeRegressor”的实例。得到的是
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
对象位于 0x7f676ebf0310&gt;。

注意：RF_90 是 HistGradientBoostingClassifier 拟合模型]]></description>
      <guid>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</guid>
      <pubDate>Tue, 18 Jun 2024 07:26:34 GMT</pubDate>
    </item>
    <item>
      <title>用于特征节点值插值的图神经网络</title>
      <link>https://stackoverflow.com/questions/78635913/graph-neural-networks-for-features-nodes-value-interpolations</link>
      <description><![CDATA[我目前正在尝试了解是否存在基于 GNN 的深度学习技术，可以重建图的部分区域。例如，我有一个包含 5 个节点和 10 条边的图，我知道 3 个节点和所有 10 条边的特征的数值，我想构建一个能够预测其他 2 个缺失节点的特征中应该出现的假设值的网络。我试图想象这种架构，但我发现很难想象基于 GCN 或消息传递的网络如何与我没有数值但想找到最佳值的节点一起运行。
我目前尝试咨询图的主要 DL 方法，但问题似乎是这种类型的任务没有得到解决。因此，我质疑它的可行性。
（https://distill.pub/2021/gnn-intro/，以及一些教程https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html）]]></description>
      <guid>https://stackoverflow.com/questions/78635913/graph-neural-networks-for-features-nodes-value-interpolations</guid>
      <pubDate>Tue, 18 Jun 2024 06:59:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tfa.losses.TripletSemiHardLoss 训练具有三重损失的暹罗网络？</title>
      <link>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</guid>
      <pubDate>Tue, 18 Jun 2024 06:47:18 GMT</pubDate>
    </item>
    <item>
      <title>运行时错误预测张量大小为 64，与 y_train 张量大小不同，张量暗淡错误</title>
      <link>https://stackoverflow.com/questions/78635713/runtime-error-pred-tensor-size-is-64-which-is-different-to-y-train-tensor-size</link>
      <description><![CDATA[我正在尝试使用 Hippo 模型来预测外汇价格（数据集有日期时间列和开盘价列）。以下是完整代码的 GitHub 链接 GitHub Repo
原始 hippo 模型可在此链接中找到：GitHub Model
我有两个问题：

在更新状态 u 中，第二个 dim 应该是批处理大小，即 [64,1]，在我的情况下，我将其设置为 1，但它仍然显示 [1, 64]
运行时错误为 pred 张量大小与 output20 中的 y_train 大小不匹配。

以下是错误详细信息
Cell In[18]，第 17 行，在 train(X_train, y_train, model, loss_fn, optimizer, batch_size, device)
14 print(&#39;pred.shpae:&#39;,pred.shape)
15 print(&#39;y_train&#39;,y_train.shape)
---&gt; 17 loss = loss_fn(pred, torch.tensor(y_train))
18 optimizer.zero_grad()
19 loss_backward()

文件 ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1511，位于 Module._wrapped_call_impl(self, *args, **kwargs)
1509 return self._compiled_call_impl(*args, **kwargs) # 类型：ignore[misc]
1510 else:
-&gt; 1511 return self._call_impl(*args, **kwargs)

File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1520, in Module._call_impl(self, *args, **kwargs)
1515 # 如果我们没有任何钩子，我们希望跳过此函数中的其余逻辑
1516 # 并直接调用 forward。
1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
1518 or _global_backward_pre_hooks or _global_backward_hooks
1519 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1520 返回 forward_call(*args, **kwargs)
1522 尝试：
1523 结果 = None

文件 ~\anaconda3\lib\site-packages\torch\nn\modules\loss.py:535，位于 MSELoss.forward(self, input, target)
534 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 535 返回 F.mse_loss(input, target, reduction=self.reduction)

文件 ~\anaconda3\lib\site-packages\torch\nn\ functional.py:3338，位于 mse_loss(input, target, size_average, reduce, reduction)
3335 如果 size_average 不是 None 或 reduce 不是 None：
3336 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3338 expand_input, expand_target = torch.broadcast_tensors(input, target)
3339 return torch._C._nn.mse_loss(expanded_input, expand_target, _Reduction.get_enum(reduction))

文件 ~\anaconda3\lib\site-packages\torch\ functional.py:76，在 broadcast_tensors(*tensors) 中
74 if has_torch_function(tensors):
75 return handle_torch_function(broadcast_tensors, tensors, *tensors)
---&gt; 76 return _VF.broadcast_tensors(tensors)

RuntimeError: 在非单例维度 0 处，张量 a (64) 的大小必须与张量 b (4322) 的大小匹配

这是我的“训练”方法在 loss_fn 上抛出错误
def train(X_train, y_train, model, loss_fn, optimizer, batch_size = None, device = None):
size = len(X_train)
if device is None:
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model.to(device)
if batch_size is None:
# X_train_tensor= torch.tensor(X_train, dtype = torch.float32).to(device)
# y_train_tensor = torch.tensor(y_train, dtype = torch.float32).to(device)

model.train()
pred = model(X_train)[0] 
print(&#39;pred.shpae:&#39;,pred.shape)
print(&#39;y_train&#39;,y_train.shape)

loss = loss_fn(pred, torch.tensor(y_train))
optimizer.zero_grad()
loss_backward()
optimizer.step()
loss_value = loss.item()
print(f&#39;loss:{loss_value:&gt;7f}, [{size:&gt;5d}/{size:&gt;5d}]&#39;)
else:
dataset = TimeSeriesDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)

model.train()

for batch_idx, (X_batch, y_batch )in enumerate(dataloader):
X_batch, y_batch = X_batch.to(device,dtype = torch.float32), y_batch.to(device,dtype = torch.float32)

# 计算预测误差
pred = model(X_batch)
loss_ = loss_fn(pred, y_batch)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if batch%10==0:
loss_value = loss.item()
current = batch_idx * len(X_batch)
print(f&quot;loss: {loss_value:&gt;7f} [{current:&gt;5d}/{len(dataset):&gt;5d}]&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78635713/runtime-error-pred-tensor-size-is-64-which-is-different-to-y-train-tensor-size</guid>
      <pubDate>Tue, 18 Jun 2024 06:03:38 GMT</pubDate>
    </item>
    <item>
      <title>SHAP DeepExplainer 对以 1D CNN 作为第一层的模型给出错误</title>
      <link>https://stackoverflow.com/questions/78634609/shap-deepexplainer-gives-an-error-for-model-with-1d-cnn-as-a-first-layer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78634609/shap-deepexplainer-gives-an-error-for-model-with-1d-cnn-as-a-first-layer</guid>
      <pubDate>Mon, 17 Jun 2024 20:45:50 GMT</pubDate>
    </item>
    <item>
      <title>训练机器学习模型并部署到生产的工作流程</title>
      <link>https://stackoverflow.com/questions/78634511/workflow-for-training-a-machine-learning-model-and-deploying-to-production</link>
      <description><![CDATA[这是一个基本问题，但我对开发和部署机器学习模型的工作流程的理解包括以下高级步骤：

获取数据并对其进行预处理
提出一个模型和一组可能的超参数组合进行评估。
使用网格搜索和分层 K 折交叉验证来评估超参数组合
使用网格搜索的结果来确定最佳超参数组合。

我不确定在将机器学习模型部署到生产之前接下来会发生什么。
据我了解，您可以以 pickle 格式保存经过训练的模型及其权重，然后将其部署到生产中。您实际上为此保存了哪个经过训练的模型及其权重？我假设您将使用网格搜索中的最佳超参数创建一个模型。那么您将使用什么数据来生成经过训练的模型及其权重？您是否会简单地使用整个数据集来训练最佳模型并将其与其权重一起保存？]]></description>
      <guid>https://stackoverflow.com/questions/78634511/workflow-for-training-a-machine-learning-model-and-deploying-to-production</guid>
      <pubDate>Mon, 17 Jun 2024 20:14:41 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow 希望 CNN 的输出与输入的形状相同</title>
      <link>https://stackoverflow.com/questions/78634442/tensorflow-wants-cnns-output-to-be-the-same-shape-as-input</link>
      <description><![CDATA[我正在编写一个 CNN 模型，用于分析来自 16 个通道、每个通道 60000 个样本的时间序列记录。我希望输出是一些特定值的 1x4 向量。问题是 tensorflow 希望输出与输入具有相同的形状（它不能，因为 60000 个样本是单个记录）。
我想问你为什么它不起作用？代码如下。不要介意非常具体的内核形状 - 我正在试验，并且正在大力进行中。根据我的研究，我猜想这与第一层的输入形状有关，但我不知道如何更改它，因为我希望将此输入视为图像类型
 model = keras.Sequential([

keras.layers.Conv2D(32, (500, 8), strides = (10, 2), input_shape=(60000, 16, 1)), #conv1D - 过滤器、内核大小、激活、输入形状
keras.layers.BatchNormalization(), #批量标准化
keras.layers.Activation(&#39;relu&#39;), # 单独的激活层

keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 2)),
keras.layers.Flatten(),

keras.layers.Dense(640, activity = &#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(256,activation=&#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(32,activation=&#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(4,activation =&#39;sigmoid&#39;)

])

它产生：
ValueError：数据基数不明确。确保所有数组都包含相同数量的样本。&#39;x&#39; 大小：60000
&#39;y&#39; 大小：4
]]></description>
      <guid>https://stackoverflow.com/questions/78634442/tensorflow-wants-cnns-output-to-be-the-same-shape-as-input</guid>
      <pubDate>Mon, 17 Jun 2024 19:52:28 GMT</pubDate>
    </item>
    <item>
      <title>“快速”版 ZFNet？</title>
      <link>https://stackoverflow.com/questions/78633543/fast-version-zfnet</link>
      <description><![CDATA[我正在阅读旧论文：

SPPNet：链接
Faster R-CNN：链接

在这两种情况下，作者都提到了“Zeiler 和 Fergus (ZF) Net 的快速版本”；具体来说：

在 SPPNet 中：

ZF-5：该架构基于 Zeiler 和 Fergus (ZF) 的“快速”(较小) 模型 [4]。数字表示五个卷积层。


在 Faster R-CNN 中：

我们使用“快速”版 ZF net [32]，它有五个卷积层和三个全连接层。



其中 [4] 和 [32] 均引用 D. Zeiler 和 R. Fergus 撰写的同一篇著名论文 Visualizing and Understanding Convolutional Networks。
我的问题：什么是“快速”版 ZFNet？
搜索 ZFNet 并阅读论文，似乎只有一种架构具有五个卷积层和三个全连接层。 “快速”版本与后续论文中提到的任何其他 ZFNet 版本究竟有何区别？]]></description>
      <guid>https://stackoverflow.com/questions/78633543/fast-version-zfnet</guid>
      <pubDate>Mon, 17 Jun 2024 15:44:35 GMT</pubDate>
    </item>
    <item>
      <title>在规范化灰度图像后，我是否应该在 ImageDataGenerator 中再次重新缩放图像？</title>
      <link>https://stackoverflow.com/questions/78632053/should-i-rescale-images-again-in-imagedatagenerator-after-normalizing-grayscale</link>
      <description><![CDATA[我正在使用卷积神经网络 (CNN) 进行图像分类任务。在预处理期间，我将 RGB 图像转换为灰度图像，并通过将像素值除以 255 对其进行归一化。在此归一化步骤之后，我使用 ImageDataGenerator 进行数据增强。但是，我不确定在数据增强期间是否需要在 ImageDataGenerator 中包含重新缩放参数，或者是否应该将其删除，因为图像已经归一化。]]></description>
      <guid>https://stackoverflow.com/questions/78632053/should-i-rescale-images-again-in-imagedatagenerator-after-normalizing-grayscale</guid>
      <pubDate>Mon, 17 Jun 2024 10:16:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
使用 pdfplumber.open(pdf_path) 作为 pdf:
对于 pdf.pages 中的 page:
page_text = page.extract_text()
如果 page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

因此，当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复它？对此有什么建议吗？
提前致谢
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>删除 add_loss() 的解决方法</title>
      <link>https://stackoverflow.com/questions/78615219/workaround-for-removal-of-add-loss</link>
      <description><![CDATA[我正在学习 Keras/Tensorflow 课程，该课程使用 Keras 2 构建变分自动编码器，并尝试使其在 Keras 3 中工作。我已经设法克服了很多问题，但我被困在这部分，希望有人能帮助我前进。
为了计算损失，原始代码使用（我已经更改为 ops。）：
reconstruction_loss = ops.binary_crossentropy(inputs, output)
reconstruction_loss *= 784
kl_loss = 0.5 * (ops.exp(z_log_var) - (1 + z_log_var) + ops.square(z_mean))
kl_loss = ops.sum(kl_loss, axis=-1)

total_vae_loss = ops.mean(reconstruction_loss + kl_loss)

然后将损失添加到模型中，编译并拟合：
vae_model.add_loss(total_vae_loss)
vae_model.compile(optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;])

vae_model.fit(x_train_flat, epochs=epochs, batch_size=batch_size)

我可以在 Keras 文档 中看到 add_loss 功能已被删除：

Symbolic Layer.add_loss()：符号 add_loss() 已被删除（您仍然可以在 call() 方法中使用 add_loss() layer/model)。

但我真的不明白在调用方法中添加它。我觉得它是为了构建自己的层？
如果我删除 add_loss 行，我会得到错误：
ValueError：没有要计算的损失。在 `compile()` 中提供一个 `loss` 参数。

我尝试了各种方法将 total_vae_loss 作为损失添加到 compile() 中，但得到错误，主要是说它必须是可调用的。
从这里开始，我只是在黑暗中射击，我将损失计算放入一个函数中，并将其添加到编译中，以防止出现该错误，但当我拟合时，我得到了一个 ValueError：
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[52]，第 1 行
----&gt; 1 vae_model.fit(x_train_flat,
2 epochs=epochs,
3 batch_size=batch_size)

文件 ~/code/python/mls/mlsenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 ~/code/python/mls/mlsenv/lib/python3.10/site-packages/optree/ops.py:594，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
592 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
593 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 594 return treespec.unflatten(map(func, *flat_args))

ValueError: 不支持 None 值。

有没有办法让它工作？我可以包含完整的堆栈跟踪，但不确定包含如此多文本的最佳方法。]]></description>
      <guid>https://stackoverflow.com/questions/78615219/workaround-for-removal-of-add-loss</guid>
      <pubDate>Wed, 12 Jun 2024 22:56:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行反量化，并为下一层重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在做一个学校项目，需要我对模型的每一层进行手动量化。具体来说，我想手动实现：

量化激活，结合量化权重 A - 层 A -
量化输出 - 去量化输出 - 重新量化输出，结合量化权重 B - 层 B - ...

我知道 Pytorch 已经有一个量化函数，但该函数仅限于 int8。我想执行从 bit = 16 到 bit = 2 的量化，然后比较它们的准确性。
我遇到的问题是，量化后，层的输出大了几个量级（bit = 16），我不知道如何将其去量化。我正在使用激活和权重的相同最小值和最大值执行量化。因此，这里有一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期的非量化输出 = 70

使用位量化 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
使用最小值 = 1、最大值 = 8 反量化输出 = -102980

这个计算对我来说很有意义，因为输出涉及激活和权重的乘积，它们的幅度增加也相乘。如果我使用原始的最小值和最大值执行一次反量化，则输出会大得多，这是合理的。
Pytorch 如何处理反量化？我试图找到 Pytorch 的量化，但找不到它。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>基本 CNN 的输入类型和偏差类型给出错误</title>
      <link>https://stackoverflow.com/questions/75205745/input-type-and-bias-type-for-basic-cnn-giving-error</link>
      <description><![CDATA[我尝试按照使用 pytorch 制作 CNN 的指南进行操作（链接）。我没有使用 CIFAR-10 数据集，而是制作了自己的数据集。我认为问题就出在这里，但我不知道发生了什么。
这是我的错误：

听起来很傻，但我尝试按照指南进行操作，希望成功，但却遇到了这些错误。我尝试在网上研究一些可能的解决方案，并努力寻找可能对我有帮助的资源。
我还将与您分享我的 Dataset 类：
class ASLDataset(torch.utils.data.Dataset): # 从 Dataset 类继承
def __init__(self, csv_file, root_dir=&quot;&quot;, transform=None):
self.annotation_df = pd.read_csv(csv_file)
self.root_dir = root_dir # 图像的根目录，保留&quot;&quot;如果在 __getitem__ 方法中使用图像路径列
self.transform = transform

def __len__(self):
return len(self.annotation_df) # 返回数据框的长度（行数）

def __getitem__(self, idx):
image_path = os.path.join(self.root_dir, self.annotation_df.iloc[idx, 1]) # 在 csv 文件中使用图像路径列（索引 = 1）
image = cv2.imread(image_path) # 通过 cv2 读取图像
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 将 BGR 转换为 RGB 以供 matplotlib 使用
class_name = self.annotation_df.iloc[idx, 2] # 在 csv 文件中使用类名列（索引 = 2）
class_index = self.annotation_df.iloc[idx, 3] # 使用csv 文件中的类索引列 (index = 3)
if self.transform:
image = self.transform(image)
return image, class_index #, class_name

train_dataset = ASLDataset(&#39;./train.csv&#39;) #, train_transform)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)

val_dataset = ASLDataset(&#39;./test.csv&#39;) # val.csv
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

classes = (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;, &#39;N&#39;, &#39;nothing&#39;, &#39;O&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;R&#39;, &#39;S&#39;, &#39;space&#39;, &#39;T&#39;, &#39;U&#39;, &#39;V&#39;, &#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;)

以下是错误代码以及指南中的网络中出现的行：
class Network(nn.Module):
def __init__(self):
super(Network, self).__init__()

self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)
self.bn1 = nn.BatchNorm2d(12)
self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5,步长=1，填充=1)
self.bn2 = nn.BatchNorm2d(12)
self.pool = nn.MaxPool2d(2, 2)
self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, 步长=1，填充=1)
self.bn4 = nn.BatchNorm2d(24)
self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, 步长=1，填充=1)
self.bn5 = nn.BatchNorm2d(24)
self.fc1 = nn.Linear(24 * 10 * 10, 10)

def forward(self, input):
output = F.relu(self.bn1(self.conv1(input)))
output = F.relu(self.bn2(self.conv2(output)))
output = self.pool(output)
output = F.relu(self.bn4(self.conv4(output)))
output = F.relu(self.bn5(self.conv5(output)))
output = output.view(-1, 24 * 10 * 10)
output = self.fc1(output)

return output

def train(num_epochs):
best_accuracy = 0.0

# 定义您的执行设备
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(&quot;模型将在&quot;, device, &quot;device&quot;)
# 将模型参数和缓冲区转换为 CPU 或 Cuda
model.to(device)

for epoch in range(num_epochs): # 多次循环遍历数据集
running_loss = 0.0
running_acc = 0.0

for i, (images, labels) in enumerate(train_dataloader, 0):

# 获取输入
images = Variable(images.to(device))
print(type(labels))
labels = Variable(labels.to(device))

# 将参数梯度归零
optimizer.zero_grad()
# 使用训练集中的图像预测类别
output = model(images)
# 根据模型输出和实际标签计算损失
loss = loss_fn(outputs, labels)
# 反向传播损失
loss.backward()
# 根据在计算的梯度上
optimizer.step()

#代码从这里继续
]]></description>
      <guid>https://stackoverflow.com/questions/75205745/input-type-and-bias-type-for-basic-cnn-giving-error</guid>
      <pubDate>Mon, 23 Jan 2023 04:48:54 GMT</pubDate>
    </item>
    </channel>
</rss>