<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 01:04:46 GMT</lastBuildDate>
    <item>
      <title>TensorFlow InvalidArgumentError：预期为 uint8 张量，但得到的是浮点张量</title>
      <link>https://stackoverflow.com/questions/78634989/tensorflow-invalidargumenterror-expected-uint8-tensor-but-got-float-tensor</link>
      <description><![CDATA[**问题：**在神经网络训练过程中尝试执行乘法运算 (Mul) 时，我在 TensorFlow 中遇到了 InvalidArgumentError。错误消息为：
InvalidArgumentError：无法计算 Mul，因为输入 #1（基于零）应为 uint8 张量，但却是浮点张量 [Op:Mul]
代码片段图片
[准确度指标和运行训练模型](https://i.sstatic.net/Jff3wuL2.png) -&gt;准备 MNIST 数据的代码。
已尝试：我尝试使用 tf.cast 将输入张量转换为 uint8，但错误仍然存​​在。我还检查了张量形状和数据类型，但一切似乎都正确。
有人可以解释一下可能导致此错误的原因以及我该如何解决它吗？任何帮助或指导都将不胜感激。
其他详细信息
提供任何其他相关代码片段或错误回溯，以帮助其他人更好地理解问题。
提及您为解决问题而采取的任何具体步骤，例如检查张量形状、数据类型或查阅文档。]]></description>
      <guid>https://stackoverflow.com/questions/78634989/tensorflow-invalidargumenterror-expected-uint8-tensor-but-got-float-tensor</guid>
      <pubDate>Mon, 17 Jun 2024 23:25:40 GMT</pubDate>
    </item>
    <item>
      <title>SHAP DeepExplainer 对以 1D CNN 作为第一层的模型给出错误</title>
      <link>https://stackoverflow.com/questions/78634609/shap-deepexplainer-gives-an-error-for-model-with-1d-cnn-as-a-first-layer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78634609/shap-deepexplainer-gives-an-error-for-model-with-1d-cnn-as-a-first-layer</guid>
      <pubDate>Mon, 17 Jun 2024 20:45:50 GMT</pubDate>
    </item>
    <item>
      <title>训练机器学习模型并部署到生产的工作流程</title>
      <link>https://stackoverflow.com/questions/78634511/workflow-for-training-a-machine-learning-model-and-deploying-to-production</link>
      <description><![CDATA[这是一个基本问题，但我对开发和部署机器学习模型的工作流程的理解包括以下高级步骤：

获取数据并对其进行预处理
提出一个模型和一组可能的超参数组合进行评估。
使用网格搜索和分层 K 折交叉验证来评估超参数组合
使用网格搜索的结果来确定最佳超参数组合。

我不确定在将机器学习模型部署到生产之前接下来会发生什么。
据我了解，您可以以 pickle 格式保存经过训练的模型及其权重，然后将其部署到生产中。您实际上为此保存了哪个经过训练的模型及其权重？我假设您将使用网格搜索中的最佳超参数创建一个模型。那么您将使用什么数据来生成经过训练的模型及其权重？您是否会简单地使用整个数据集来训练最佳模型并将其与其权重一起保存？]]></description>
      <guid>https://stackoverflow.com/questions/78634511/workflow-for-training-a-machine-learning-model-and-deploying-to-production</guid>
      <pubDate>Mon, 17 Jun 2024 20:14:41 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow 希望 CNN 的输出与输入的形状相同</title>
      <link>https://stackoverflow.com/questions/78634442/tensorflow-wants-cnns-output-to-be-the-same-shape-as-input</link>
      <description><![CDATA[我正在编写一个 CNN 模型，用于分析来自 16 个通道、每个通道 60000 个样本的时间序列记录。我希望输出是一些特定值的 1x4 向量。问题是 tensorflow 希望输出与输入具有相同的形状（它不能，因为 60000 个样本是单个记录）。
我想问你为什么它不起作用？代码如下。不要介意非常具体的内核形状 - 我正在试验，并且正在大力进行中。根据我的研究，我猜想这与第一层的输入形状有关，但我不知道如何更改它，因为我希望将此输入视为图像类型
 model = keras.Sequential([

keras.layers.Conv2D(32, (500, 8), strides = (10, 2), input_shape=(60000, 16, 1)), #conv1D - 过滤器、内核大小、激活、输入形状
keras.layers.BatchNormalization(), #批量标准化
keras.layers.Activation(&#39;relu&#39;), # 单独的激活层

keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 2)),
keras.layers.Flatten(),

keras.layers.Dense(640, activity = &#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(256,activation=&#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(32,activation=&#39;sigmoid&#39;),
keras.layers.Dropout(0.5),

keras.layers.Dense(4,activation =&#39;sigmoid&#39;)

])

它产生：
ValueError：数据基数不明确。确保所有数组都包含相同数量的样本。&#39;x&#39; 大小：60000
&#39;y&#39; 大小：4
]]></description>
      <guid>https://stackoverflow.com/questions/78634442/tensorflow-wants-cnns-output-to-be-the-same-shape-as-input</guid>
      <pubDate>Mon, 17 Jun 2024 19:52:28 GMT</pubDate>
    </item>
    <item>
      <title>“快速”版 ZFNet？</title>
      <link>https://stackoverflow.com/questions/78633543/fast-version-zfnet</link>
      <description><![CDATA[我正在阅读旧论文：

SPPNet：链接
Faster R-CNN：链接

在这两种情况下，作者都提到了“Zeiler 和 Fergus (ZF) Net 的快速版本”；具体来说：

在 SPPNet 中：

ZF-5：该架构基于 Zeiler 和 Fergus (ZF) 的“快速”(较小) 模型 [4]。数字表示五个卷积层。


在 Faster R-CNN 中：

我们使用“快速”版 ZF net [32]，它有五个卷积层和三个全连接层。



其中 [4] 和 [32] 均引用 D. Zeiler 和 R. Fergus 撰写的同一篇著名论文 Visualizing and Understanding Convolutional Networks。
我的问题：什么是“快速”版 ZFNet？
搜索 ZFNet 并阅读论文，似乎只有一种架构具有五个卷积层和三个全连接层。 “快速”版本与后续论文中提到的任何其他 ZFNet 版本究竟有何区别？]]></description>
      <guid>https://stackoverflow.com/questions/78633543/fast-version-zfnet</guid>
      <pubDate>Mon, 17 Jun 2024 15:44:35 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 自动求导</title>
      <link>https://stackoverflow.com/questions/78633325/huggingface-autograd</link>
      <description><![CDATA[我正在尝试微调（LoRA 微调）预训练语言模型。我遇到了梯度没有反向传播的情况。起初，我以为是因为我使用了 Huggingface 的 .generate() 方法，该方法具有 @no_grad 装饰器。但是，即使我在自定义自回归调用中使用 .forward() 方法来生成输出，我仍然无法让梯度反向传播。以下是我的反向传播测试代码。
def _test_gradient_prop():
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model_id = &quot;meta-llama/Meta-Llama-3-8B&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
device_map=&quot;auto&quot;
)
print(model)

managed_model = AutoregressiveGenerator(model, tokenizer, 64, 0.7)
optimizer = torch.optim.Adam(model.parameters())

question = &quot;吉萨金字塔位于意大利。&quot;
answer = &quot;这是真的。&quot;
test_template = f&quot;考虑以下答案中的真实性数量：问题：{question} 答案：{answer} 答案中的真实性数量是&quot;
input_ids = tokenizer.encode(test_template, return_tensors=&#39;pt&#39;).to(device)
# input_ids.requires_grad = True # 仅适用于浮点数
input_ids = torch.autograd.Variable(input_ids)

torch.manual_seed(0)
output = managed_model.managed_forward(input_ids)

optimizer.zero_grad()
dummy = torch.rand_like(output[0].float()) * 100
loss = torch.nn. functional.mse_loss(output[0].to(torch.float), dummy)
loss.requires_grad = True

print(loss)
loss.backward()

for name, param in model.named_pa​​rameters():
if &#39;mlp&#39; in name:
# print(f&quot;Parameter: {name}, require_grad: {param.requires_grad}&quot;)
if param.grad 为 None:
print(f&quot;{name} 的梯度为 None&quot;)
else:
print(f&quot;{name} 的梯度：{param.grad.sum()}&quot;)

 def __init__(self, model, tokenizer, max_new_tokens,temperature):
super().__init__()
self.model = model
self.tokenizer = tokenizer
self.max_new_tokens = max_new_tokens
self.temperature =temperature
self.last_token = None
self.token_counter = 0

def managed_forward(self, input_ids):
generated_ids = input_ids
for i in range(self.max_new_tokens):
generated_ids = self.forward(generated_ids)

if self.last_token == self.tokenizer.eos_token_id:
break
return generated_ids

def forward(self, input_ids):
generated_tokens = input_ids

output = self.model(generated_tokens)
logits = output.logits

next_logit = logits[:, -1, :] # 下一个 logit
next_logit = next_logit / self.temperature # 温度缩放
next_probs = torch.nn. functional.softmax(next_logit, dim=-1) # 获取概率

# 采样
next_token_id = torch.multinomial(next_probs, num_samples=1) # 从概率分布中绘制
# next_token_id = torch.argmax(next_logit, dim=-1).unsqueeze(-1) # 最大可能 token
self.last_token = next_token_id.item() # 用于管理终止

#附加到序列
generated_tokens = torch.cat((generated_tokens, next_token_id), dim=1)

return generated_tokens

def reset(self):
self.token_counter = 0

def __forward__(self, x):
pass

我还怀疑这可能是因为 input_ids 是一个长张量。是不是因为输入是一个长张量，所以没有为下游计算梯度？&lt;​​/p&gt;
提前谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78633325/huggingface-autograd</guid>
      <pubDate>Mon, 17 Jun 2024 14:53:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LSTM 模型中输入多个特征来预测单个特征输出？</title>
      <link>https://stackoverflow.com/questions/78633065/how-to-input-multiple-features-to-predict-a-single-feature-output-in-lstm-model</link>
      <description><![CDATA[我可以使用一个特征来预测输出。我想检查如果我再使用一个特征，我的准确率是否会更高。我想知道如何在 LSTM 模型中添加更多特征。我使用了 https://www.geeksforgeeks.org/multivariate-time-series-forecasting-with-lstms-in-keras/ 提供的代码，它使用两个特征来预测两个输出，我想使用两个特征在同一时间戳来生成一个输出。有人可以给我一个演示代码吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78633065/how-to-input-multiple-features-to-predict-a-single-feature-output-in-lstm-model</guid>
      <pubDate>Mon, 17 Jun 2024 14:03:06 GMT</pubDate>
    </item>
    <item>
      <title>在规范化灰度图像后，我是否应该在 ImageDataGenerator 中再次重新缩放图像？</title>
      <link>https://stackoverflow.com/questions/78632053/should-i-rescale-images-again-in-imagedatagenerator-after-normalizing-grayscale</link>
      <description><![CDATA[我正在使用卷积神经网络 (CNN) 进行图像分类任务。在预处理期间，我将 RGB 图像转换为灰度图像，并通过将像素值除以 255 对其进行归一化。在此归一化步骤之后，我使用 ImageDataGenerator 进行数据增强。但是，我不确定在数据增强期间是否需要在 ImageDataGenerator 中包含重新缩放参数，或者是否应该将其删除，因为图像已经归一化。]]></description>
      <guid>https://stackoverflow.com/questions/78632053/should-i-rescale-images-again-in-imagedatagenerator-after-normalizing-grayscale</guid>
      <pubDate>Mon, 17 Jun 2024 10:16:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 S3 存储桶中的训练数据训练 YOLOv8？</title>
      <link>https://stackoverflow.com/questions/78619753/how-to-train-yolov8-with-traingin-data-in-s3-bucket</link>
      <description><![CDATA[似乎 model.train 需要 data.yml 文件的路径，并且该文件需要有训练和验证集的路径。s3 引用似乎不是实际路径，我看到人们使用数据生成器使用 S3 进行训练。有人知道如何使用 YOLOv8 做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78619753/how-to-train-yolov8-with-traingin-data-in-s3-bucket</guid>
      <pubDate>Thu, 13 Jun 2024 19:08:12 GMT</pubDate>
    </item>
    <item>
      <title>删除 add_loss() 的解决方法</title>
      <link>https://stackoverflow.com/questions/78615219/workaround-for-removal-of-add-loss</link>
      <description><![CDATA[我正在学习 Keras/Tensorflow 课程，该课程使用 Keras 2 构建变分自动编码器，并尝试使其在 Keras 3 中工作。我已经设法克服了很多问题，但我被困在这部分，希望有人能帮助我前进。
为了计算损失，原始代码使用（我已经更改为 ops。）：
reconstruction_loss = ops.binary_crossentropy(inputs, output)
reconstruction_loss *= 784
kl_loss = 0.5 * (ops.exp(z_log_var) - (1 + z_log_var) + ops.square(z_mean))
kl_loss = ops.sum(kl_loss, axis=-1)

total_vae_loss = ops.mean(reconstruction_loss + kl_loss)

然后将损失添加到模型中，编译并拟合：
vae_model.add_loss(total_vae_loss)
vae_model.compile(optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;])

vae_model.fit(x_train_flat, epochs=epochs, batch_size=batch_size)

我可以在 Keras 文档 中看到 add_loss 功能已被删除：

Symbolic Layer.add_loss()：符号 add_loss() 已被删除（您仍然可以在 call() 方法中使用 add_loss() layer/model)。

但我真的不明白在调用方法中添加它。我觉得它是为了构建自己的层？
如果我删除 add_loss 行，我会得到错误：
ValueError：没有要计算的损失。在 `compile()` 中提供一个 `loss` 参数。

我尝试了各种方法将 total_vae_loss 作为损失添加到 compile() 中，但得到错误，主要是说它必须是可调用的。
从这里开始，我只是在黑暗中射击，我将损失计算放入一个函数中，并将其添加到编译中，以防止出现该错误，但当我拟合时，我得到了一个 ValueError：
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[52]，第 1 行
----&gt; 1 vae_model.fit(x_train_flat,
2 epochs=epochs,
3 batch_size=batch_size)

文件 ~/code/python/mls/mlsenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 ~/code/python/mls/mlsenv/lib/python3.10/site-packages/optree/ops.py:594，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
592 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
593 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 594 return treespec.unflatten(map(func, *flat_args))

ValueError: 不支持 None 值。

有没有办法让它工作？我可以包含完整的堆栈跟踪，但不确定包含如此多文本的最佳方法。]]></description>
      <guid>https://stackoverflow.com/questions/78615219/workaround-for-removal-of-add-loss</guid>
      <pubDate>Wed, 12 Jun 2024 22:56:17 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
解决方案：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
MSE = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); % Luko 等人的 Eq. 9。
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号
MSE(i) = (1/T)*sum((y_i&#39;-y_i_target).^2); % 均方误差
end
[minval, minidx] = min(MSE);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。
正如 BillBokeey 所强调的那样，所需的方程只是 Luko 等人提出的方程 9 的迭代版本。要进行训练，必须将方程 9 应用于训练数据集中的每个目标信号，并选择最小化均方误差 (MSE) 的结果 W_out。
最终更新：
我仍在寻找我的第二个解决方案的验证。我特别担心的是，我正在挑选出最佳的 Wout_i N by 1 向量，该向量最小化 MSE 并有效地忽略所有其他 Wout_i 向量。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行反量化，并为下一层重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在做一个学校项目，需要我对模型的每一层进行手动量化。具体来说，我想手动实现：

量化激活，结合量化权重 A - 层 A -
量化输出 - 去量化输出 - 重新量化输出，结合量化权重 B - 层 B - ...

我知道 Pytorch 已经有一个量化函数，但该函数仅限于 int8。我想执行从 bit = 16 到 bit = 2 的量化，然后比较它们的准确性。
我遇到的问题是，量化后，层的输出大了几个量级（bit = 16），我不知道如何将其去量化。我正在使用激活和权重的相同最小值和最大值执行量化。因此，这里有一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期的非量化输出 = 70

使用位量化 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
使用最小值 = 1、最大值 = 8 反量化输出 = -102980

这个计算对我来说很有意义，因为输出涉及激活和权重的乘积，它们的幅度增加也相乘。如果我使用原始的最小值和最大值执行一次反量化，则输出会大得多，这是合理的。
Pytorch 如何处理反量化？我试图找到 Pytorch 的量化，但找不到它。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>无法从‘diffusers.utils’导入名称‘randn_tensor’</title>
      <link>https://stackoverflow.com/questions/77101192/cannot-import-name-randn-tensor-from-diffusers-utils</link>
      <description><![CDATA[
我正在使用这个自动训练协作，当我标记并将图片放入图片文件夹并尝试运行它时，它显示此错误，我该如何解决？
重现：

单击 ipynb 链接

创建一个名为 images 的新文件夹

添加一些图片并将提示替换为描述您的图片的内容

转到运行时并运行所有


ipynb 链接]]></description>
      <guid>https://stackoverflow.com/questions/77101192/cannot-import-name-randn-tensor-from-diffusers-utils</guid>
      <pubDate>Thu, 14 Sep 2023 00:51:19 GMT</pubDate>
    </item>
    <item>
      <title>基本 CNN 的输入类型和偏差类型给出错误</title>
      <link>https://stackoverflow.com/questions/75205745/input-type-and-bias-type-for-basic-cnn-giving-error</link>
      <description><![CDATA[我尝试按照使用 pytorch 制作 CNN 的指南进行操作（链接）。我没有使用 CIFAR-10 数据集，而是制作了自己的数据集。我认为问题就出在这里，但我不知道发生了什么。
这是我的错误：

听起来很傻，但我尝试按照指南进行操作，希望成功，但却遇到了这些错误。我尝试在网上研究一些可能的解决方案，并努力寻找可能对我有帮助的资源。
我还将与您分享我的 Dataset 类：
class ASLDataset(torch.utils.data.Dataset): # 从 Dataset 类继承
def __init__(self, csv_file, root_dir=&quot;&quot;, transform=None):
self.annotation_df = pd.read_csv(csv_file)
self.root_dir = root_dir # 图像的根目录，保留&quot;&quot;如果在 __getitem__ 方法中使用图像路径列
self.transform = transform

def __len__(self):
return len(self.annotation_df) # 返回数据框的长度（行数）

def __getitem__(self, idx):
image_path = os.path.join(self.root_dir, self.annotation_df.iloc[idx, 1]) # 在 csv 文件中使用图像路径列（索引 = 1）
image = cv2.imread(image_path) # 通过 cv2 读取图像
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 将 BGR 转换为 RGB 以供 matplotlib 使用
class_name = self.annotation_df.iloc[idx, 2] # 在 csv 文件中使用类名列（索引 = 2）
class_index = self.annotation_df.iloc[idx, 3] # 使用csv 文件中的类索引列 (index = 3)
if self.transform:
image = self.transform(image)
return image, class_index #, class_name

train_dataset = ASLDataset(&#39;./train.csv&#39;) #, train_transform)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)

val_dataset = ASLDataset(&#39;./test.csv&#39;) # val.csv
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

classes = (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;, &#39;N&#39;, &#39;nothing&#39;, &#39;O&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;R&#39;, &#39;S&#39;, &#39;space&#39;, &#39;T&#39;, &#39;U&#39;, &#39;V&#39;, &#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;)

以下是错误代码以及指南中的网络中出现的行：
class Network(nn.Module):
def __init__(self):
super(Network, self).__init__()

self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)
self.bn1 = nn.BatchNorm2d(12)
self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5,步长=1，填充=1)
self.bn2 = nn.BatchNorm2d(12)
self.pool = nn.MaxPool2d(2, 2)
self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, 步长=1，填充=1)
self.bn4 = nn.BatchNorm2d(24)
self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, 步长=1，填充=1)
self.bn5 = nn.BatchNorm2d(24)
self.fc1 = nn.Linear(24 * 10 * 10, 10)

def forward(self, input):
output = F.relu(self.bn1(self.conv1(input)))
output = F.relu(self.bn2(self.conv2(output)))
output = self.pool(output)
output = F.relu(self.bn4(self.conv4(output)))
output = F.relu(self.bn5(self.conv5(output)))
output = output.view(-1, 24 * 10 * 10)
output = self.fc1(output)

return output

def train(num_epochs):
best_accuracy = 0.0

# 定义您的执行设备
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(&quot;模型将在&quot;, device, &quot;device&quot;)
# 将模型参数和缓冲区转换为 CPU 或 Cuda
model.to(device)

for epoch in range(num_epochs): # 多次循环遍历数据集
running_loss = 0.0
running_acc = 0.0

for i, (images, labels) in enumerate(train_dataloader, 0):

# 获取输入
images = Variable(images.to(device))
print(type(labels))
labels = Variable(labels.to(device))

# 将参数梯度归零
optimizer.zero_grad()
# 使用训练集中的图像预测类别
output = model(images)
# 根据模型输出和实际标签计算损失
loss = loss_fn(outputs, labels)
# 反向传播损失
loss.backward()
# 根据在计算的梯度上
optimizer.step()

#代码从这里继续
]]></description>
      <guid>https://stackoverflow.com/questions/75205745/input-type-and-bias-type-for-basic-cnn-giving-error</guid>
      <pubDate>Mon, 23 Jan 2023 04:48:54 GMT</pubDate>
    </item>
    </channel>
</rss>