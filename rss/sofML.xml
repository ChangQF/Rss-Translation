<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 12 Jan 2025 06:22:21 GMT</lastBuildDate>
    <item>
      <title>我到底该怎么做才能区分机器学习模型中的两个变量？</title>
      <link>https://stackoverflow.com/questions/79348913/what-exactly-can-i-do-to-differentiate-the-two-variables-in-my-machine-learning</link>
      <description><![CDATA[我正在使用 Kaggle 并研究 Pokemon 类型数据集：[Pokemon Type Chart][1]，我的机器学习模型旨在获取用户输入，在本例中是他们选择的 Pokemon 类型，并显示与该特定输入的类型匹配的输出。（例如弱点、免疫力、超级有效性、抵抗力）但我明显的问题是数据集只具有三个值，其中弱点和超级有效性发生冲突。我的大脑已经完全耗尽了精力去尝试弄清楚之前的代码。
 # 加载数据集
typing_chart = pd.read_csv(&quot;/mnt/data/typing_chart.csv&quot;)

# 提取唯一类型
unique_types = typing_chart.columns[1:].tolist()

def type_effectiveness_checker(primary_type, secondary_type=&quot;None&quot;):
result = {
&quot;weak&quot;: set(), # 宝可梦较弱的类型
&quot;resistant&quot;: set(), # 宝可梦可以抵抗的类型
&quot;immune&quot;: set(), # 宝可梦可以免疫的类型
&quot;effective&quot;: set() # 宝可梦可以超级有效的类型
}

# 评估有效性的函数
def assess_effectiveness(attacking_type):
effectiveness_row = typing_chart.loc[typing_chart[typing_chart.columns[0]] == attacking_type].iloc[0]
for defending_type, value ineffectiveness_row.items():
if defending_type == typing_chart.columns[0]:
continue # 跳过第一列（攻击类型标签）

if value == 2.0:
result[&quot;effective&quot;].add(defending_type) # 对该类型超级有效
elif value == 0.5:
result[&quot;resistant&quot;].add(defending_type) # 对该类型有抵抗力
elif value == 0.0:
result[&quot;immune&quot;].add(defending_type) # 对该类型免疫
elif value == 2.0: # 弱点通过反向查找确定
reverse_effectiveness = typing_chart.loc[typing_chart[defending_type] == 2.0]
result[&quot;weak&quot;].update(reverse_effectiveness[typing_chart.columns[0]].tolist())

# 处理主要类型
if primary_type in unique_types:
assess_effectiveness(primary_type)

# 处理次要类型（如果提供）
if secondary_type != &quot;None&quot; and secondary_type in unique_types:
evaluate_effectiveness(secondary_type)

# 将结果格式化为排序列表
result = {key: sorted(values) for key, values in result.items()}
return result

# Gradio 函数显示有效性结果
def gradio_type_effectiveness(primary_type, secondary_type):
results = type_effectiveness_checker(primary_type, secondary_type)
return (
f&quot;Weak To: {&#39;, &#39;.join(results[&#39;weak&#39;]) if results[&#39;weak&#39;] else &#39;None&#39;}&quot;,
f&quot;Resistant To: {&#39;, &#39;.join(results[&#39;resistant&#39;]) if results[&#39;resistant&#39;] else &#39;None&#39;}&quot;,
f&quot;Immune To: {&#39;, &#39;.join(results[&#39;immune&#39;]) if results[&#39;immune&#39;] else &#39;无&#39;}&quot;,
f&quot;超级有效对抗：{&#39;, &#39;.join(results[&#39;effective&#39;]) if results[&#39;effective&#39;] else &#39;无&#39;}&quot;
)

# Gradio 接口定义
gradio_interface = gr.Interface(
fn=gradio_type_effectiveness,
输入=[
gr.Dropdown(choices=unique_types, label=&quot;主要类型&quot;),
gr.Dropdown(choices=[&quot;无&quot;] + unique_types, label=&quot;次要类型（可选）&quot;)
],
输出=[
gr.Textbox(label=&quot;弱&quot;),
gr.Textbox(label=&quot;抗性&quot;),
gr.Textbox(label=&quot;免疫&quot;),
gr.Textbox(label=&quot;超级有效反对”)
],
title=&quot;宝可梦类型有效性检查器&quot;,
description=&quot;选择宝可梦的主要和（可选）次要类型，查看其类型弱点、抵抗力、免疫力和有效性。&quot;
)

# 本地使用说明
if __name__ == &quot;__main__&quot;:
gradio_interface.launch()

[1]: https://www.kaggle.com/datasets/jadenbailey/pokemon-type-chart
]]></description>
      <guid>https://stackoverflow.com/questions/79348913/what-exactly-can-i-do-to-differentiate-the-two-variables-in-my-machine-learning</guid>
      <pubDate>Sat, 11 Jan 2025 21:15:19 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用属性进行自动分类？</title>
      <link>https://stackoverflow.com/questions/79348879/is-automatic-classification-possible-using-attributes</link>
      <description><![CDATA[我将训练我的模型对某些种类的作物和杂草进行分类。在此之前，我想自动注释一些包含以下图像的数据集：

农作物，包括

玉米，
大豆，
向日葵等；


杂草，包括

双子叶植物和
单子叶植物



起初，我直接这样做：

将 crops 类化，属性为 species，值为 corn、soybean 等；
将 weeds 类化，属性为 class，值为 dicotyledonous 和单子叶植物。

但后来我发现我既找不到支持属性的自动分类工具，也找不到属性的实际用途的描述，使用它们的最佳实践是什么？
我是否正确理解，对于自动分类，我应该为每种识别的对象类型创建一个类（例如，玉米、大豆、... 双子叶杂草、单子叶杂草等），而不是具有属性的类（Occam 的方式=）？]]></description>
      <guid>https://stackoverflow.com/questions/79348879/is-automatic-classification-possible-using-attributes</guid>
      <pubDate>Sat, 11 Jan 2025 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CLI 在 spaCy 训练管道中包含自定义组件？</title>
      <link>https://stackoverflow.com/questions/79348814/how-do-i-include-a-custom-component-in-a-spacy-training-pipeline-using-the-cli</link>
      <description><![CDATA[我目前正在尝试在我的 spaCy 训练管道中实现一个简单的自定义组件。我正在使用 spaCy CLI 进行训练，这意味着我正在通过 config.cfg 文件指导管道配置，尽管我确实有用于生成和注释训练和评估数据的脚本。我创建的自定义组件是无状态的，从各方面来看，这意味着它不需要工厂，只需使用 @Language.component() 装饰器即可。该组件只需获取 doc 对象并根据给定的一组命名实体中出现的次数为其添加分类。如果任何给定的实体类型有多个，则分类分数为 1.0；如果不是，则为 0.0。
以下是该函数的代码：
# custom_classifier.py

from spacy.language import Language

@Language.component(&quot;custom_classifier&quot;)
def custom_classifier(doc):
entity_types = [&quot;ENTITY1&quot;, &quot;ENTITY2&quot;, &quot;ENTITY3&quot;]
entity_counts = [sum(1 for ent in doc.ents if ent.label_ == entity_type) for entity_type in entity_types]

if any(count &gt; 1 for count in entity_counts):
doc.cats[&quot;MULTIPLE&quot;] = 1.0
else:
doc.cats[&quot;MULTIPLE&quot;] = 0.0

return doc

然后我尝试在我的训练管道中使用这个自定义组件，方法是将其添加到我的 config.cfg 定义中，如下所示：
[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;custom_classifier&quot;]
batch_size = 1000
...

[components]
...

[components.taper_classifier]
source = &quot;custom_classifier.custom_classifier&quot;
after = &quot;ner&quot;

无论我做什么，当我尝试运行 spacy train config.cfg 时，我最终都会收到此错误消息的某个版本：
OSError：[E050] 找不到模型“custom_classifier.custom_classifier”。它似乎不是 Python 包或数据目录的有效路径。

到目前为止，我尝试了各种解决方案，但都没有奏效。这些包括：

尝试我能想到的模块路径的所有可能的排列。
将组件函数转换为工厂并相应地修改装饰器和配置。
使用 @spacy.registry 装饰器注册函数。

我已经梳理了几个小时的文档，我就是想不出我做错了什么，虽然我承认文档似乎假设使用自定义组件的训练是在代码中而不是使用 CLI 进行的，并且没有详细说明如何使用 config.cfg 文件声明自定义组件。我觉得要么是我忽略了一些简单的东西，要么是我从根本上误解了这些自定义组件的工作原理。无论如何，任何帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79348814/how-do-i-include-a-custom-component-in-a-spacy-training-pipeline-using-the-cli</guid>
      <pubDate>Sat, 11 Jan 2025 20:14:52 GMT</pubDate>
    </item>
    <item>
      <title>使用优化算法寻找神经网络（LSTM）的最佳超参数</title>
      <link>https://stackoverflow.com/questions/79348547/finding-optimal-hyperparameter-for-neural-networklstm-using-optimization-algor</link>
      <description><![CDATA[我一直在尝试使用灰狼算法 (GWO) 和粒子群优化器 (PSO) 为 LSTM 模型找到最佳超参数（lstm 单元数、epoch 数、批量大小等）。这花费了很多时间。下面是我正在做的事情的描述。
我有一个 LSTM 模型，包含在一个必须优化的目标函数中。此函数执行弓箭任务：

此函数根据传递给它的参数构建模型。
然后它训练模型
然后它在测试数据上找到 MSE。

此 MSE 分数数据是根据哪个 GWO 优化器将计算适应度而返回的。分数越低，越适合将是解决方案。因为它是 MSE 的最小化问题。
这个过程需要几个小时。有没有其他方法可以实现这个过程，以便花费更少的时间？]]></description>
      <guid>https://stackoverflow.com/questions/79348547/finding-optimal-hyperparameter-for-neural-networklstm-using-optimization-algor</guid>
      <pubDate>Sat, 11 Jan 2025 17:18:38 GMT</pubDate>
    </item>
    <item>
      <title>Conda 环境未激活</title>
      <link>https://stackoverflow.com/questions/79348397/conda-environment-not-activating</link>
      <description><![CDATA[我一直试图激活我在 VS Code 中创建的 conda 环境“venv”，但无法激活。我尝试在删除它后重新创建它，但仍然无法激活Conda 激活未发生
我正在关注此Conda 已激活
我已经尝试了 15 多分钟来解决这个问题。我甚至尝试将整个路径设置为激活，但什么也没发生。请帮助我]]></description>
      <guid>https://stackoverflow.com/questions/79348397/conda-environment-not-activating</guid>
      <pubDate>Sat, 11 Jan 2025 15:38:28 GMT</pubDate>
    </item>
    <item>
      <title>Xavier 初始化在经过第一个隐藏层后不保持方差 - 需要解释</title>
      <link>https://stackoverflow.com/questions/79348329/xavier-initialization-doesnt-maintain-variance-after-pass-through-first-hidden</link>
      <description><![CDATA[一直在尝试初始化不同的权重。目前使用 MNIST 数据集，其中有 1 个隐藏层，每个层有 256 个神经元，并使用 tanh 激活。输入为零均值和单位方差。
因此，我假设在第一层之后的第一次传递之后，方差将大致相同或乘以我们提供的增益。
因此，如果输入方差为 1，则 fc1 之后的方差应为 1*5/3 或接近 1。但我得到的方差是 489。有人能更好地启发我吗？我感觉我直觉上缺少了一些东西。我尝试了
init.xavier_normal_(self.fc1.weight,gain=nn.init.calculate_gain(&#39;tanh&#39;)) 和
init.xavier_normal_(self.fc1.weight)

import torch
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
import torch.nn. functional as F
import random
import torch.nn.init as init

random.seed(42)
torch.manual_seed(42)

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
# 下载并加载训练和测试集
trainset = datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform)
testset = datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform)

# 创建 DataLoader 来批量处理数据
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)
class SimpleNN(nn.Module):
def __init__(self):
super(SimpleNN, self).__init__()
self.flatten = nn.Flatten()
self.fc1 = nn.Linear(28 * 28, 256)
self.fc2 = nn.Tanh() 
self.fc3 = nn.Linear(256, 10)
init.xavier_normal_(self.fc1.weight,gain=nn.init.calculate_gain(&#39;tanh&#39;))
init.xavier_normal_(self.fc3.weight)

def forward(self, x):
self.activations = {} 

x = self.flatten(x)
self.activations[&#39;input&#39;] = x

x = self.fc1(x)
self.activations[&#39;fc1&#39;] = x

x = self.fc2(x)
self.activations[&#39;fc2&#39;] = x

x = self.fc3(x)
self.activations[&#39;fc3&#39;] = x

return x
def analyze_layer_statistics(model):

stats = {}
for layer_name,activations in model.activations.items():
stats[layer_name] = {
&#39;mean&#39;: torch.mean(activations).item(),
&#39;var&#39;: torch.var(activations).item(),
&#39;min&#39;: torch.min(activations).item(),
&#39;max&#39;: torch.max(activations).item()
}

返回统计信息
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr=0.01)
epochs = 20
losses = []
stats = []
for epoch in range(epochs):
model.train()
total = 0
correct = 0
running_loss = 0.0
for images, labels in trainloader:
optimizer.zero_grad() 
output = model(images)
loss = criterion(output, labels)
loss.backward()
optimizer.step()
running_loss += loss
total += images.shape[0]
correct += (torch.argmax(output,dim=1)==labels).sum()
print(f&#39;Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}&#39;)
print(f&#39;Accuracy = {(correct/total)*100}&#39;)
loss.append(running_loss/len(trainloader)) 
stats.append(analyze_layer_statistics(model))
break

输出：
stats 输出为

[{&#39;input&#39;: {&#39;mean&#39;: 0.023540694266557693,
&#39;var&#39;: 1.056724190711975,
&#39;min&#39;: -0.4242129623889923,
&#39;max&#39;: 2.821486711502075},
&#39;fc1&#39;: {&#39;平均值&#39;: 0.15851731598377228,
&#39;变量&#39;: 489.0372009277344,
&#39;最小值&#39;: -79.83320617675781,
&#39;最大值&#39;: 87.52055358886719},
&#39;fc2&#39;: {&#39;平均值&#39;: -0.0067255799658596516,
&#39;变量&#39;: 0.9763329029083252,
&#39;最小值&#39;: -1.0,
&#39;最大值&#39;: 1.0},
&#39;fc3&#39;: {&#39;平均值&#39;: -0.31016167998313904,
&#39;var&#39;: 17.94700050354004,
&#39;min&#39;: -11.344281196594238,
&#39;max&#39;: 13.964859962463379}}]

当我一次性传递整个数据集时，即一次性将全部 60000 个条目传递到第一层 - 第一次传递后的方差为 4
[{&#39;input&#39;: {&#39;mean&#39;: -0.00012828917533624917,
&#39;var&#39;: 1.0000507831573486,
&#39;min&#39;: -0.4242129623889923,
   “最大”：2.821486711502075}，
  &#39;fc1&#39;：{&#39;平均值&#39;：-0.04259666055440903，
   “变量”：3.9302892684936523，
   “分钟”：-13.193024635314941，
   “最大”：11.443111419677734}，
  &#39;fc2&#39;：{&#39;平均值&#39;：-0.012426979839801788，
   “变量”：0.6235496401786804，
   “分钟”：-1.0，
   “最大”：1.0}，
&#39;fc3&#39;: {&#39;mean&#39;: -0.197908416390419,
&#39;var&#39;: 1.3438835144042969,
&#39;min&#39;: -4.865184783935547,
&#39;max&#39;: 5.251534938812256}}]

我觉得我在初始化权重时缺少一些方差逻辑背后的直觉]]></description>
      <guid>https://stackoverflow.com/questions/79348329/xavier-initialization-doesnt-maintain-variance-after-pass-through-first-hidden</guid>
      <pubDate>Sat, 11 Jan 2025 14:56:26 GMT</pubDate>
    </item>
    <item>
      <title>MedSAM 算法中的损失函数[关闭]</title>
      <link>https://stackoverflow.com/questions/79347947/loss-function-in-medsam-algorithm</link>
      <description><![CDATA[我正在研究 MedSAM（医学图像中的任意分割）算法。它是 Meta AI 针对医学图像的任意分割模型的微调版本。在论文中，其损失函数由二元交叉熵 (BCE) 损失和 Dice 损失的未加权和给出。

我想手动计算这个损失。
假设我有 $8 \times 8$ 灰度图像，下面显示了该图像的矩阵和我想要分割为阴影灰色的区域。这是我的真实情况。

N 是像素数，在本例中为 64。下面是此特定图像的分割结果：

因此，我可以使用以下代码从此处计算 BCE 损失和 Dice 损失：
import numpy as np

# 真实值（来自原始 8x8 灰度图像）
ground_truth = np.array([
[0.96, 0.16, 0.77, 0.00, 0.49, 0.25, 0.87, 0.31],
[0.18, 0.67, 0.44, 0.17, 0.12, 0.93, 0.30, 0.39],
    [0.25, 0.62, 0.57, 0.76, 1.00, 0.03, 0.58, 0.80],
    [0.46, 0.21, 0.45, 0.83, 0.34, 0.39, 0.66, 0.42],
    [0.91, 0.16, 0.10, 0.56, 0.78, 0.71, 0.70, 0.91],
    [0.40, 0.88, 0.35, 0.72, 0.87, 0.27, 0.59, 0.27],
    [0.70, 0.97, 0.89, 0.39, 0.48, 0.94, 0.84, 0.07],
[0.72, 0.51, 0.02, 0.16, 0.96, 0.70, 0.14, 0.23],
])

# 分割结果（来自 MedSAM 结果）
segmentation_result = np.array([
[0.79, 0.78, 0.38, 0.61, 0.63, 0.25, 0.40, 0.60],
[0.70, 0.57, 0.23, 0.63, 0.88, 0.03, 0.94, 0.01],
[0.32, 0.99, 0.39, 0.66, 0.58, 0.22, 0.03, 0.65],
    [0.86, 0.96, 0.38, 0.40, 0.90, 0.34, 0.70, 0.91],
    [0.56, 0.18, 0.08, 0.12, 0.14, 0.68, 0.36, 0.41],
    [0.31, 0.48, 0.68, 0.89, 0.92, 0.93, 0.53, 0.50],
    [0.87, 0.62, 0.30, 0.95, 0.71, 0.65, 0.75, 0.65],
    [0.91, 0.55, 0.24, 0.84, 0.91, 0.90, 0.72, 0.20],
])

# 二元交叉熵损失
epsilon = 1e-7 # 防止 log(0)
bce_loss = -np.mean(
ground_truth * np.log(segmentation_result + epsilon) +
(1 - ground_truth) * np.log(1 - fragmentation_result + epsilon)
)

# 骰子损失
numerator = 2 * np.sum(ground_truth * fragmentation_result)
denominator = np.sum(ground_truth**2) + np.sum(segmentation_result**2)
dice_loss = 1 - (numerator / (denominator + epsilon))

bce_loss, dice_loss

计算出的损失为：

二元交叉熵 (BCE) 损失：0.9590
Dice 损失：0.2083

据我所知，BCE 可确保准确预测每个体素，而 Dice 可确保分割区域的整体形状和重叠正确。这种组合充分利用了两种指标的优势，使算法在不同的医学图像分割任务中具有稳健性。
我的问题：

我的计算是否正确？

如何以及为什么在此处包含 BCE？由于这不是分类模型，而只是一个分割模型，Dice Loss 是否足够？

如果图像包含同一类别的另一个分割区域会怎样？或者如果它包含不同类别的另一个分割区域会怎样？BCE 损失仅适用于两类分类（(1) 肿瘤或 (2) 健康组织）。

]]></description>
      <guid>https://stackoverflow.com/questions/79347947/loss-function-in-medsam-algorithm</guid>
      <pubDate>Sat, 11 Jan 2025 11:14:59 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个机器学习程序可以将 3D 模型纹理与正确的模型匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/79347239/is-there-a-machine-learning-program-that-can-match-3d-model-textures-to-the-corr</link>
      <description><![CDATA[我有大量（约 20,000 张）图像，这些图像是纹理、法线贴图和其他用于纹理 3D 模型对象的贴图。我还有 3D 模型本身（OBJ 格式，但可以转换）。
我想使用模型 UV 坐标数据将纹理文件映射到其对应的 3D 模型。
问题是，所有图像都未分类且未标记，因此将它们与各自的 3D 模型匹配需要手动浏览它们。这非常繁琐。
是否存在这样的程序？我还没有找到。
如果没有，如果我尝试自己构建它，我应该记住哪些注意事项？我有编程经验，但没有机器学习经验。
想法：

先将图像映射在一起，然后将模型映射到该组图像可能会更容易
UV 坐标数据可以导出为图像。如果这很困难，那么使用它而不是从 3D 模型文件本身进行映射是有意义的。
]]></description>
      <guid>https://stackoverflow.com/questions/79347239/is-there-a-machine-learning-program-that-can-match-3d-model-textures-to-the-corr</guid>
      <pubDate>Fri, 10 Jan 2025 23:20:37 GMT</pubDate>
    </item>
    <item>
      <title>1000- torchrl：使用 SyncDataCollector 和自定义 pytorch dqn</title>
      <link>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</link>
      <description><![CDATA[我尝试将 torchrl 的 SyncDataCollector 与我自己在 torch 中实现的 DQN 一起使用。由于 DQN 使用 Conv2d 和线性层，我必须计算第一个线性层的输入的正确大小，即以下网络中的 size 参数
class PixelDQN(nn.Module):
def __init__(self, input_shape, n_actions) -&gt;无：
super().__init__()
self.conv = nn.Sequential(
nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 64, kernel_size=3, stride=1),
nn.ReLU(),
nn.Flatten(),
)
size = self.conv(torch.zeros(1, *input_shape)).size()[-1]
self.fc_adv = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, n_actions),
)
self.fc_val = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, 1)
)

def forward(self, x: torch.Tensor):
print(x.shape)
conv = self.conv(x)
print(conv.shape)
adv = self.fc_adv(conv)
val = self.fc_val(conv)
outp = val + (adv - adv.mean(dim=1, keepdim=True))
return outp

负责这个。如您所见，我期望批量输入，因为我将使用重放缓冲区并从中抽样一批。
我以以下方式包装该 DQN，然后使用 SyncDataCollector：
n_obs = [4,84,84]
n_act = 6

agent = QValueActor(
module=PixelDQN(n_obs, n_act), in_keys=[&quot;pixels&quot;], spec=env.action_spec
)
policy_explore = EGreedyModule(
env.action_spec, eps_end=EPS_END, annealing_num_steps=ANNEALING_STEPS
)
agent_explore = TensorDictSequential(
agent, policy_explore
)

collector = SyncDataCollector(
env,
agent_explore,
frames_per_batch=FRAMES_PER_BATCH,
init_random_frames=INIT_RND_STEPS,
postproc=MultiStep(gamma=GAMMA, n_steps=N_STEPS)
)

但是，这失败了，因为 SyncDataCollector 在将 obs 提供给 DQN 之前没有对来自环境的 obs 进行批处理，因此 size 计算出错，并且线性层获得错误的输入维度。
RuntimeError：mat1 和 mat2 形状无法相乘（64x49 和 3136x256）
我已经尝试在 SyncDataCollector 中设置 buffer=True。我也尝试使用
agent_explore = TensorDictSequential(
UnsqueezeTransform(0, allow_positive_dim=True), agent, policy_explore
)

因为这是 ChatGPT 建议的，但似乎没有任何效果。
我也在我的 env 创建中尝试了 UnsqueezeTransform，但这也没有用，我的 env 如下所示：
def make_env(env_name: str):
return TransformedEnv(
GymEnv(env_name, from_pixels=True),
Compose(
RewardSum(),
EndOfLifeTransform(),
NoopResetEnv(noops=30),
ToTensorImage(),
Resize(84, 84),
GrayScale(),
FrameSkipTransform(frame_skip=4),
CatFrames(N=4, dim=-3),
)
)

我可以将 size 计算拉入 PixelDQN 的前向传递中，并检查输入张量的大小以调整计算，但这似乎是一件很奇怪的事情，因为这意味着我需要在每次前向传递时运行大小计算。]]></description>
      <guid>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</guid>
      <pubDate>Fri, 10 Jan 2025 09:49:05 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm.cv：cvbooster.best_iteration 总是返回 -1</title>
      <link>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</link>
      <description><![CDATA[我正在从 XGBoost 迁移到 LightGBM（因为我需要它精确处理交互约束），并且我很难理解 LightGBM CV 的结果。在下面的示例中，在第 125 次迭代中实现了最小对数损失，但 model[&#39;cvbooster&#39;].best_iteration 返回 -1。我原本希望它也能返回 125 - 还是我在这里误解了什么？有没有更好的方法来获得最佳迭代，还是只需要手动检查？
我看过这个讨论，但即使我检查cvbooster中的boosters（例如，model[&#39;cvbooster&#39;].boosters[0].best_iteration），它们也都返回 -1...
import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(n_samples=10_000, n_features=5, n_informative=3, random_state=9)

data_train_lgb = lgb.Dataset(X, label=y)

param = {&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: [&#39;binary_logloss&#39;],
&#39;device_type&#39;: &#39;cuda&#39;}

model = lgb.cv(param,
data_train_lgb,
num_boost_round=1_000,
return_cvbooster=True)

opt_1 = np.argmin(model[&#39;valid binary_logloss-mean&#39;])
print(f&quot;index argmin: {opt_1}&quot;)
print(f&quot;logloss argmin: {model[&#39;valid binary_logloss-mean&#39;][opt_1]}&quot;)

opt_2 = model[&#39;cvbooster&#39;].best_iteration
print(f&quot;index best_iteration: {opt_2}&quot;)
print(f&quot;logloss best_iteration: {model[&#39;valid binary_logloss-mean&#39;][opt_2]}&quot;)

---

&gt;&gt;&gt; 索引参数最小值：125
&gt;&gt;&gt; 对数损失参数最小值：0.13245999867688793

&gt;&gt;&gt; 索引最佳迭代：-1
&gt;&gt;&gt; 对数损失最佳迭代：0.2661896445658779
]]></description>
      <guid>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</guid>
      <pubDate>Fri, 10 Jan 2025 03:40:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在平面图上检测北箭头？</title>
      <link>https://stackoverflow.com/questions/79344396/how-to-detect-north-arrow-on-a-floor-plan</link>
      <description><![CDATA[我正在从事一项任务，该任务涉及使用多模态 AI 模型（例如 Google Gemini）分析楼层平面图以提取结构化信息，例如入口点、卧室和其他主要特征的位置。
但是，检测楼层平面图中的方向性存在挑战。
这是一个示例楼层平面图图像：

在右下角，有一个方向指示器，显示北 (N)，箭头指向上方。作为人类，我们很容易理解北方指向上方，我们可以相应地调整对楼层平面图的解释。然而，当使用AI模型处理图像时，模型无法读取“N”标签或理解箭头方向，从而导致方向分析不正确。
为了解决这个问题，我尝试使用PaddleOCR检测图像中的文本并对其进行注释。我使用的代码如下
from paddleocr import PaddleOCR, draw_ocr
from PIL import Image
# 初始化 PaddleOCR
ocr = PaddleOCR(use_angle_cls=True, lang=&#39;en&#39;) # 下载并加载模型一次

# 提供图片路径
img_path = &#39;prop_1.png&#39;

# 执行 OCR
result = ocr.ocr(img_path, cls=True)

# 打印结果
for idx in range(len(result)):
res = result[idx]
for line in res:
print(line)


OCR 输出成功检测到“卧室”、“厨房”、“客厅”等文本标签，但未能检测到“N”标签和指向上方的箭头指示方向。]]></description>
      <guid>https://stackoverflow.com/questions/79344396/how-to-detect-north-arrow-on-a-floor-plan</guid>
      <pubDate>Fri, 10 Jan 2025 01:12:24 GMT</pubDate>
    </item>
    <item>
      <title>阿曼车牌的 OCR 预处理 - 字母识别问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</link>
      <description><![CDATA[我正在开发一个用于阿曼车牌的 OCR 系统，并努力提高字母识别的准确性。车牌上通常包含小而粗的字符，而我目前的预处理流程无法产生令人满意的结果。
到目前为止，我所做的是：
尽管进行了预处理和配置（--oem 3，--psm 6），但 PaddleOCR 和 Tesseract 仍然难以识别字母。
预处理步骤：
尝试使用 Sauvola 和 Wolf-Jolion 二值化、缩放图像（1.5 倍）并应用膨胀来增强文本。
问题：
字母仍然难以识别。
如何改进预处理以更好地对小而粗的字母进行 OCR 识别？
是否有任何 OCR 模型或自定义训练方法更适合像阿曼这样设计复杂的车牌？
样本车牌：
]]></description>
      <guid>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</guid>
      <pubDate>Thu, 09 Jan 2025 18:15:26 GMT</pubDate>
    </item>
    <item>
      <title>DMIR 的 XMorpher 模型 - 数据集问题</title>
      <link>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</link>
      <description><![CDATA[我正在尝试运行 GitHub 项目 XMorpher (https://github.com/Solemoon/XMorpher/tree/main)，但作者没有提供正确的数据集？在代码中他使用了：
 train_labeled_unlabeled_dir = &#39;data/train_labeled_unlabeled&#39;
train_unlabeled_unlabeled_dir = &#39;data/train_unlabeled_unlabeled&#39;
test_labeled_labeled_dir = &#39;data/test&#39;

但实际上数据文件夹仅包含 0_1.mat 文件，在 Matlab 中打开时，该文件包含 2 个变量：
fix_img # 大小：144x144x128 (int16)
mov_img # 大小：144x144x128 (int16)

由于一张图片包含 5308416 个字节，我无法看到变量的值。有人知道运行此模型的其他方法吗？或者我可以使用其他哪个数据集？
感谢您任何答案。]]></description>
      <guid>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</guid>
      <pubDate>Tue, 12 Nov 2024 08:25:50 GMT</pubDate>
    </item>
    <item>
      <title>从 Pytorch 中加载 EMNIST 数据集</title>
      <link>https://stackoverflow.com/questions/78215347/load-emnist-dataset-from-within-the-pytorch</link>
      <description><![CDATA[我正在研究 EMNIST 数据集并想从 PyTorch 加载它，但它返回一个奇怪的错误：

RuntimeError：文件未找到或已损坏。

以下是我尝试加载数据集的方法：
trainset = torchvision.datasets.EMNIST(root=&quot;emnist&quot;,
split=&quot;letters&quot;,
train=True,
download=True,
transform=transforms.ToTensor())

可能出了什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78215347/load-emnist-dataset-from-within-the-pytorch</guid>
      <pubDate>Sun, 24 Mar 2024 16:53:53 GMT</pubDate>
    </item>
    <item>
      <title>yolov5 模型的错误检测</title>
      <link>https://stackoverflow.com/questions/72659581/wrong-detection-from-yolov5-model</link>
      <description><![CDATA[我正在训练一个 yolov5 模型来对汽车的 4 个不同部件（底盘、前扰流板、轮毂盖和车轮）的图像进行分类，但它的猜测完全错误，无法区分底盘和前扰流板以及车轮和轮毂盖。经过 100 次和 1000 次训练，情况都是如此。有人能告诉我哪里出了问题吗？
错误的猜测：

示例：

示例：
]]></description>
      <guid>https://stackoverflow.com/questions/72659581/wrong-detection-from-yolov5-model</guid>
      <pubDate>Fri, 17 Jun 2022 12:42:53 GMT</pubDate>
    </item>
    </channel>
</rss>