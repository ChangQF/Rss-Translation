<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 04 May 2024 15:13:48 GMT</lastBuildDate>
    <item>
      <title>在 Transformer 中使用 LabelEncoding 的 ML 模型管道</title>
      <link>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</link>
      <description><![CDATA[我正在尝试将各种转换与 LightGBM 模型一起合并到 scikit-learn 管道中。该模型旨在预测二手车的价格。训练完成后，我计划将此模型集成到 HTML 页面中以供实际使用。
从 sklearn.preprocessing 导入 StandardScaler、LabelEncoder
从 sklearn.pipeline 导入管道
从 sklearn.compose 导入 ColumnTransformer
导入作业库

打印（数字特征）
`[&#39;car_year&#39;, &#39;km&#39;, &#39;horse_power&#39;, &#39;cyl_capacity&#39;]`
打印（分类特征）
`[&#39;品牌&#39;、&#39;型号&#39;、&#39;装饰级别&#39;、&#39;燃料类型&#39;、&#39;变速箱&#39;、&#39;车身类型&#39;、&#39;颜色&#39;]`

# 定义数字和分类特征的转换器
numeric_transformer = Pipeline(steps=[(&#39;scaler&#39;, StandardScaler())])
categorical_transformer = 管道(steps=[(&#39;labelencoder&#39;, LabelEncoder())])

# 使用 ColumnTransformer 组合变压器
预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, numeric_transformer, numeric_features_train),
        (&#39;猫&#39;, categorical_transformer, categorical_features)
    ]
）

# 将LightGBM模型附加到预处理管道
管道=管道（步骤=[
    （&#39;预处理器&#39;，预处理器），
    (&#39;模型&#39;, best_lgb_model)
]）

# 将管道拟合到训练数据
pipeline.fit(X_train, y_train)

训练时得到的输出是：
LabelEncoder.fit_transform() 需要 2 个位置参数，但给出了 3 个]]></description>
      <guid>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</guid>
      <pubDate>Sat, 04 May 2024 15:12:44 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法同步创建数据集（名称已存在）</title>
      <link>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</link>
      <description><![CDATA[当我尝试将模型另存为 h5 时
caption_model.save(“/kaggle/working/mymodel.h5”)

我发现了这个错误
ValueError Traceback（最近一次调用最后一次）
[19] 中的单元格，第 1 行
----&gt; 1 title_model.save(“/kaggle/working/mymodel.h5”)

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第 119 章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183，在 Group.create_dataset(self, name, shape, dtype, data, **kwds)
    第180章 180
    [第 181 回]
--&gt;第183章
    184 dset = 数据集. 数据集（dsid）
    185 返回数据集

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163，在 make_new_dset(parent、shape、dtype、数据、名称、块、压缩、shuffle、fletcher32、maxshape、compression_opts 中、 fillvalue、scaleoffset、track_times、external、track_order、dcpl、dapl、efile_prefix、virtual_prefix、allow_unknown_filter、rdcc_nslots、rdcc_nbytes、rdcc_w0)
    160 其他：
    161 sid = h5s.create_simple（形状，maxshape）
--&gt;第163章
    165 if (data is not None) and (not isinstance(data, Empty)):
    166 dset_id.write（h5s.ALL，h5s.ALL，数据）

文件 h5py/_objects.pyx:54，在 h5py._objects.with_phil.wrapper() 中

文件 h5py/_objects.pyx:55，在 h5py._objects.with_phil.wrapper() 中

文件h5py/h5d.pyx:137，在h5py.h5d.create()中

ValueError：无法同步创建数据集（名称已存在）````

你们中有人以前遇到过这个问题或者知道如何解决它吗？

谢谢

你们中有人以前遇到过这个问题或者知道如何解决它吗？

谢谢
]]></description>
      <guid>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</guid>
      <pubDate>Sat, 04 May 2024 14:51:33 GMT</pubDate>
    </item>
    <item>
      <title>努力解决卷积自动编码器的输入和输出形状差异</title>
      <link>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</link>
      <description><![CDATA[拟合模型时出现以下错误：
ValueError：层“sequential_15”的输入 0与图层不兼容：预期形状=(无, 27088, 64, 1)，发现形状=(无, 27086, 64, 1)
我认为 MaxPool2D 是下限舍入，而 UpSamling2D 是上限舍入。当我查看模型摘要时，我发现地板舍入导致了不同的输入和输出形状，但是，我正在努力寻找必要的参数来充分适应该模型。
这是模型的代码块：
input_shape=BUFFER_INPUT_SHAPE_WITHCHANNELS
# n_channels = input_shape[-1]
# 编码器
模型=顺序（）
model.add(Conv2D(256, (4,4), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;,
输入形状=输入形状)) # (27086, 64, 1)
model.add(MaxPool2D((2,2), padding=&#39;相同&#39;))
model.add(Conv2D(128, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPool2D((2,2), padding=&#39;相同&#39;))
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPool2D((2,2), padding=&quot;相同&quot;))
# 解码器
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(128, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(256, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(1, (4,4), 激活=&#39;sigmoid&#39;,
填充=&#39;相同&#39;））

model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;,
指标=[&#39;mse&#39;])
模型.summary()
      



_________________________________________________________________

层（类型）输出形状参数#
=

conv2d_135（Conv2D）（无、27086、64、256）4352

max_pooling2d_60（最大池化（无、13543、32、256）0
g2D)

conv2d_136（Conv2D）（无、13543、32、128）524416

max_pooling2d_61（最大池化（无、6772、16、128）0
g2D)

conv2d_137（Conv2D）（无、6772、16、64）131136

max_pooling2d_62（最大池化（无、3386、8、64）0
g2D)

conv2d_138（Conv2D）（无、3386、8、64）65600

up_sampling2d_58（上采样（无、6772、16、64）0
g2D)

conv2d_139（Conv2D）（无、6772、16、128）131200

up_sampling2d_59（上采样（无、13544、32、128）0
g2D)

conv2d_140（Conv2D）（无、13544、32、256）524544

up_sampling2d_60（上采样（无、27088、64、256）0
g2D)

conv2d_141（Conv2D）（无、27088、64、1）4097

如果您发现任何其他提示或改进，请告诉我。
我尝试更改池、内核和步幅参数。我还尝试通过将原始输入修剪为可多次整除的形状来更改原始输入。但是，我不确定这种方法是否通常采用。
编辑：
我将数据修剪为形状 (27072,64,1)。但这似乎不是处理我的问题的适当方法。我不想修剪我的数据]]></description>
      <guid>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</guid>
      <pubDate>Sat, 04 May 2024 14:40:11 GMT</pubDate>
    </item>
    <item>
      <title>错误：HuggingFaceInstructEmbeddings 初始化</title>
      <link>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</link>
      <description><![CDATA[`蟒蛇
从 langchain.embeddings 导入 HuggingFaceInstructEmbeddings
instructor_embeddings = HuggingFaceInstructEmbeddings()

`
我陷入了这个错误，似乎无法解决这个问题：
&#39;&#39;&#39;
TypeError：INSTRUCTOR._load_sbert_model() 得到意外的关键字参数“token”
&#39;&#39;&#39;
请帮忙！我该如何解决这个问题？
我想使用 HuggingFaceInstructEmbeddings 对我的数据集进行矢量化，但它不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</guid>
      <pubDate>Sat, 04 May 2024 11:32:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：不同的 DataLoader 批量大小会产生截然不同的损失</title>
      <link>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</link>
      <description><![CDATA[我正在使用此近似示例作为正弦波学习近似的基础。我是 PyTorch 新手……为什么不同的 BATCH_SIZE 值会显著改变结果？
批次大小 512：

批次大小 10000（所有数据点）：

我在下面发布了代码，但首先我将解释一下我所做的更改原文：

所有随机种子都是静态的，禁用了随机排序：

学习率改为 1e-4，X 大小改为 10**4，MAX_EPOCH 改为 20。

添加了一个图表来绘制 X 范围内所有值的近似值，以显示差异


完整代码如下……
谢谢！
import torch
import numpy as np
import matplotlib.pyplot as plt

from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
LR = 1e-4
MAX_EPOCH = 20
BATCH_SIZE = 10**4

class SineApproximator(nn.Module):
def __init__(self):
super(SineApproximator, self).__init__()
self.regressor = nn.Sequential(nn.Linear(1, 1024),
nn.ReLU(inplace=True),
nn.Linear(1024, 1024),
nn.ReLU(inplace=True),
nn.Linear(1024, 1))
def forward(self, x):
output = self.regressor(x)
return output

torch.manual_seed(41)
if torch.cuda.is_available():
torch.cuda.manual_seed_all(41)
np.random.seed(41)

X = np.random.rand(10**4) * 2 * np.pi
y = np.sin(X)

X_train, X_val, y_train, y_val = map(torch.tensor, train_test_split(X, y, test_size=0.2, shuffle=False, random_state=41))
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
pin_memory=True, shuffle=True)
val_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=BATCH_SIZE,
pin_memory=True, shuffle=True)

model = SineApproximator().to(device)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss(reduction=&quot;mean&quot;)

train_loss_list = list()
val_loss_list = list()
for epoch in range(MAX_EPOCH):
print(&quot;epoch %d / %d&quot; % (epoch + 1, MAX_EPOCH))
model.train()
# 训练循环
temp_loss_list = list()
for X_train, y_train in train_dataloader:
X_train = X_train.type(torch.float32).to(device)
y_train = y_train.type(torch.float32).to(device)

optimizer.zero_grad()

score = model(X_train)
loss = criterion(input=score, target=y_train)
loss.backward()

optimizer.step()

temp_loss_list.append(loss.detach().cpu().numpy())

temp_loss_list = list()
for X_train, y_train in train_dataloader:
X_train = X_train.type(torch.float32).to(device)
y_train = y_train.type(torch.float32).to(device)

score = model(X_train)
loss = criterion(input=score, target=y_train)

temp_loss_list.append(loss.detach().cpu().numpy())

avg_loss = np.average(temp_loss_list)
train_loss_list.append(avg_loss)
print(&quot;\ttrain loss: %.5f&quot; % train_loss_list[-1])

# 构建一个 np 数组，所有 X 值位于其最小值和最大值之间，间隔为 0.01，每个值位于自己的数组中：
model.eval()
X_all = torch.tensor(np.arange(X.min(), X.max(), 0.01)).type(torch.float32).unsqueeze(1).to(device)
y_prediction = model(X_all)
y_prediction = y_prediction.detach().cpu().numpy().flatten()
original = plt.scatter(X, y, s=1)
predicted = plt.scatter(X_all.detach().cpu().numpy(), y_prediction, s=1)
plt.legend((predicted, original), (&quot;Function&quot;, &quot;Samples&quot;))
plt.waitforbuttonpress()
]]></description>
      <guid>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</guid>
      <pubDate>Sat, 04 May 2024 10:10:58 GMT</pubDate>
    </item>
    <item>
      <title>如何从未分割的（正常）图像中提取放射组学特征？</title>
      <link>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</link>
      <description><![CDATA[我正在通过从医学图像中提取放射组学特征来利用机器学习进行肿瘤检测。我的问题是：
由于健康数据上没有肿瘤，因此图像中没有分割部分。如何从健康图像中提取放射学特征？
“ValueError：在此掩码中找不到标签（即没有任何内容被分段）！”]]></description>
      <guid>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</guid>
      <pubDate>Sat, 04 May 2024 09:57:03 GMT</pubDate>
    </item>
    <item>
      <title>饮食推荐系统的python代码[关闭]</title>
      <link>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</link>
      <description><![CDATA[我一直在尝试在 GitHub 上找到的这段代码：https://github .com/zakaria-narjis/Diet-Recommendation-System 用于饮食和食物建议。
它的效果很好，但食物建议（食谱中的总卡路里）通常比我每天必须摄入的饮食计划和卡路里（“计划”卡路里）更高，尤其是当它是五顿饭并且它提供的时候用正餐换零食，这没有意义，数据包含真正的零食。
我对完整代码做了一些调整，因为它一开始不起作用，但是这里是我认为有问题的函数：
defgenerate_recommendations(self,):
    总卡路里=self.weight_loss*self.calories_calculator()
    建议=[]
    self.meals_calories_perc 中的膳食：
        膳食卡路里=self.meals_calories_perc[膳食]*total_calories
        如果餐==&#39;早餐&#39;：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10) ),rnd(30,100)]
        elif 餐==&#39;发射&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        elif 餐==&#39;晚餐&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        别的：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10) ),rnd(30,100)]
        生成器=生成器（推荐_营养）
        Recommended_recipes=generator.generate().json()[&#39;输出&#39;]
        suggest.append(recommended_recipes)
    对于推荐中的推荐：
        推荐食谱：
            食谱[&#39;image_link&#39;]=find_image(食谱[&#39;名称&#39;])
    返回建议
]]></description>
      <guid>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</guid>
      <pubDate>Sat, 04 May 2024 07:57:23 GMT</pubDate>
    </item>
    <item>
      <title>在ReactJs中将ai模型集成到chart.js中</title>
      <link>https://stackoverflow.com/questions/78428075/integration-ai-model-in-chart-js-in-reactjs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78428075/integration-ai-model-in-chart-js-in-reactjs</guid>
      <pubDate>Sat, 04 May 2024 06:56:42 GMT</pubDate>
    </item>
    <item>
      <title>COCO分段json格式的合并和减去注释</title>
      <link>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</link>
      <description><![CDATA[我是 Python 和机器学习新手，遇到以下问题：我以 COCO .json 格式注释了数据。在这种情况下，水下照片上的珊瑚表面区域是活的，而珊瑚的部分区域是死的。有时，蒙版会重叠。
我想减去我注释为“死区”的区域来自我注释为“活着”的区域。在每张照片上，只有一个珊瑚被注释，有时我会分多个部分进行注释，因此我也想在减法之前合并每个类别的蒙版。注释类“死”了。仅出现在图像的子集中。
有人能指出我如何做到这一点的正确方向吗？
我添加了一张示例照片：黄色表示“活着”类别，绿色表示“死亡”类别。

非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</guid>
      <pubDate>Sat, 04 May 2024 04:11:41 GMT</pubDate>
    </item>
    <item>
      <title>如何解释基于情感分析数据训练的朴素贝叶斯模型？</title>
      <link>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</link>
      <description><![CDATA[我正在编写此代码来训练朴素贝叶斯模型：
# 加载必要的库
库（readxl）
库（插入符号）
图书馆(e1071)

# 加载预处理后的TF-IDF矩阵
tfidf_df &lt;- read_excel(&#39;~/Downloads/tfidf_r.xlsx&#39;)

# 加载带有标签的原始DataFrame
df &lt;- read_excel(&#39;~/Downloads/all-review_label.xlsx&#39;)

# 将 TF-IDF 矩阵与标签 DataFrame 合并
merged_df &lt;- 合并(tfidf_df, df, by=&#39;review_id&#39;)

# 将数据分为训练集和测试集
  设置.种子(42)
  train_indices &lt;- createDataPartition(merged_df$review_id, p = 0.8, list = FALSE)
  train_data &lt;- merged_df[train_indices, ]
  test_data &lt;- merged_df[-train_indices, ]
  
  # 初始化并训练朴素贝叶斯分类器
  naive_bayes_model &lt;- naiveBayes(标签 ~ ., data = train_data)
  
  # 对测试集进行预测
  y_pred &lt;- 预测（naive_bayes_model，newdata = test_data）
  打印（y_pred）
  
  # 将预测值和实际值转换为相同水平的因子
  级别 &lt;- 唯一（c（级别（y_pred），级别（test_data$review）））
  y_pred &lt;- 因子(y_pred, 级别 = 级别)
  test_data$label&lt;- 因子(test_data$label, 级别 = 级别)

我想解释使用 SHAP 模型获得的结果，我使用了以下代码：
库（kernelshap）
图书馆（shapviz）

xvars &lt;- setdiff(colnames(merged_df), &quot;label...2&quot;)

# 如果 length(xvars) 大于 10，则使用 kernelshap()。对 bg_X 进行子采样至 100-500 行
shap_values &lt;- kernelshap(naive_bayes_model,
                          X = 合并_df,
                          bg_X = 合并_df,
                          功能名称 = xvars)

shap_values &lt;- shapviz(shap_values)
sv_importance(shap_values, kind = “bar”)

但是 R 显示“停止”图标几个小时，但没有给出任何结果。
如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</guid>
      <pubDate>Fri, 03 May 2024 18:57:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么最终模型中的树木数量不是我指定的数量？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</link>
      <description><![CDATA[我使用 quantregForest 进行分位数回归森林模型，代码如下：
opt_mdl &lt;- quantregForest(x = train[, features],
                          y = 训练[，目标]，
                          节点大小 = 5,
                          尝试= 14，
                          n树= 500，
                          nthreads = 并行::DetectCores() - 1)

其中 Train 是一个包含 1909 个数据实例和特征列的数据框，features 是要使用的 24 个特征名称的列表。这里我将ntree指定为500。但是在opt_mdl中，值ntree (opt_mdl$ntree)是 34。为什么会发生这种情况以及如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</guid>
      <pubDate>Fri, 03 May 2024 18:51:19 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的多线程无法在 Raspberry Pi 上正常工作</title>
      <link>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</guid>
      <pubDate>Fri, 03 May 2024 12:10:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在短时间内建立准确的数据集？</title>
      <link>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</link>
      <description><![CDATA[我们正在开发一款 iOS 应用，让用户可以发送可定制的数字卡片。用户可以从各种卡片模板中进行选择，输入自己的文本，并根据自己的喜好对卡片进行编辑。我们还有一项功能，用户可以提供短信，例如“妈妈生日快乐”，并收到文本的扩展版本，例如“祝我特别的母亲生日快乐！”我爱你，希望你度过愉快的一天。”
我正在研究如何实现这一目标，并计划使用自然语言处理 (NLP) 和 CoreML 创建一个模型。然而，我在为这个特定任务寻找合适的数据集时遇到了问题。因此，我有兴趣构建专门为此目的而定制的准确数据集。但是，我不确定从哪里可以获得必要的数据，或者是否有其他数据源可供快速使用。
如果您有任何见解或替代方法来实现此功能，请分享。]]></description>
      <guid>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</guid>
      <pubDate>Thu, 02 May 2024 09:18:54 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 获取 One vs Rest SVC() 的模型参数？</title>
      <link>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</link>
      <description><![CDATA[我尝试使用decision_function_shape= ovr制作onvsrest分类模型，但是当我将其更改为decision_function_shape= ovo时，它给了我与ovr相同的结果。结果我读到 svc() 正在使用 ovo 作为基础，无论它是作为 ovr 还是 ovo 启动的。那么我怎样才能改变我的代码，以便它给我一个 ovr 结果呢？
model3 = SVC(kernel = &#39;rbf&#39;, Decision_function_shape=&#39;ovr&#39;)
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)

我尝试过使用 OneVsRestClassifier() 但不知道如何给出所有这些命令的输出，它总是出错并说 OneVsRestClassifier 没有这些命令。有没有办法用 OneVsRestClassifier 获取 cm、sm、sv、beta 和截距？
cm3 = fusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = 分类报告（Y_测试，model3_预测）
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)

希望有人能帮助我，先谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</guid>
      <pubDate>Sat, 27 Apr 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>Android 机器学习库</title>
      <link>https://stackoverflow.com/questions/43649359/machine-learning-libraries-for-android</link>
      <description><![CDATA[我正在尝试为我的 Android 应用程序构建一个小型文本挖掘工具。我正在检查一个机器学习库，它可以让我进行聚类、分类等。
有适用于 Android 的机器学习库吗？我遇到了 Tensorflow，但我需要更多地访问常见的 ML 函数。]]></description>
      <guid>https://stackoverflow.com/questions/43649359/machine-learning-libraries-for-android</guid>
      <pubDate>Thu, 27 Apr 2017 05:33:52 GMT</pubDate>
    </item>
    </channel>
</rss>