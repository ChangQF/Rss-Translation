<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 19 Jun 2024 03:17:32 GMT</lastBuildDate>
    <item>
      <title>机器学习——对数据进行反复训练会导致过度拟合吗？</title>
      <link>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</link>
      <description><![CDATA[所以我有一个已经训练好的机器学习模型，没有过拟合或欠拟合，这个模型是完美的，可以正确检测物体。但是，之后我尝试如果我再次反复训练这个模型，结果发生了过拟合，在一个已经完美的模型上反复训练模型会影响模型吗？所以原本正常的变成了过拟合。
我希望得到关于我的情况的解释]]></description>
      <guid>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</guid>
      <pubDate>Wed, 19 Jun 2024 02:24:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络近似代码中的值错误</title>
      <link>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</link>
      <description><![CDATA[import numpy as np
import matplotlib.pyplot as plt

# 定义目标函数 f(x) = sin(x)^2
def f(x):
return np.sin(x)**2

# 生成训练数据集
np.random.seed(0) # 为了可重复性
num_samples = 100
x_train = np.linspace(0, 2*np.pi, num_samples)
y_train = f(x_train)

# 初始化系数
n = len(x_train)
a = np.random.randn(n) # 线性组合系数
b = np.random.randn() # 常数项
m = 10 # 神经元数量
alphas = np.random.randn(m, n) # ReLU 神经元的权重
ts = np.random.randn(m) # ReLU 阈值神经元
cs = np.random.randn(m) # ReLU 神经元的系数

# 实现神经网络近似
def relu(x):
return np.maximum(0, x)

def f_N(x):
return np.dot(a, x) + b + np.sum(cs[:, np.newaxis] * relu(np.dot(alphas, x) - ts[:, np.newaxis]), axis=0)

# 定义损失函数
def loss_function(a, b, cs):
predictions = f_N(x_train)
return np.mean((predictions - y_train)**2)

# 梯度下降优化
def gradient_descent(learning_rate=0.01, num_iterations=1000):
loss = []
for i in range(num_iterations):
# 计算梯度
predictions = f_N(x_train)
误差 = 预测 - y_train

grad_a = np.outer(误差, x_train) / num_samples
grad_b = np.mean(误差)

relu_grad = np.dot(cs[:, np.newaxis] * (np.dot(alphas, x_train) - ts[:, np.newaxis] &gt; 0), alphas)
grad_cs = np.sum(error[:, np.newaxis] * relu_grad, axis=0) / num_samples

# 更新系数
a -= learning_rate * grad_a
b -= learning_rate * grad_b
cs -= learning_rate * grad_cs

# 裁剪或投影 cs 以确保它们保持在界限内
cs = np.clip(cs, -1, 1) # 在 -1 和 1 之间裁剪的示例

# 计算当前损失
current_loss = loss_function(a, b, cs)
loss.append(current_loss)

# 打印损失以进行监控
if i % 100 == 0:
print(f&quot;Iteration {i}, Loss: {current_loss}&quot;)

return loss

# 执行梯度下降
losses = gradient_descent()

# 绘制结果
plt.figure(figsize=(12, 6))

# 绘制目标函数
plt.plot(x_train, y_train, label=&#39;目标函数：$f(x) = \sin(x)^2$&#39;, color=&#39;blue&#39;)

# 绘制神经网络近似
y_pred = f_N(x_train)
plt.plot(x_train, y_pred, label=&#39;神经网络近似&#39;, linestyle=&#39;--&#39;, color=&#39;red&#39;)

plt.title(&#39;神经网络近似 vs 目标函数&#39;)
plt.xlabel(&#39;x&#39;)
plt.ylabel(&#39;y&#39;)
plt.legend()
plt.grid(True)
plt.show()

# 绘制损失曲线
plt.figure(figsize=(10, 5))
plt.plot(losses, label=&#39;训练Loss&#39;)
plt.title(&#39;Training Loss over Iterations&#39;)
plt.xlabel(&#39;Iteration&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.grid(True)
plt.show()


代码的目标如下：
定义目标函数 f(x) = sin(x)^2，并生成训练数据集 {(x_i, y_i)}，其中 y_i = f(x_i)，针对一系列 x_i 值。
用随机值初始化神经网络系数 a、b 和 c_i，并从标准高斯分布生成样本 (alpha_1,t_1),...,(alpha_m,t_m)。
实现神经网络近似 fN(x) = a^Tx+b+sum{i=1}^{m} ci\sigma(alpha{i}^T-t_i) 如图所示，使用初始化系数。
定义损失函数 L(a, b, c_1, ..., c_m)，用于测量神经网络近似值 f_N(x_i) 与训练数据点 (x_i, y_i) 的目标函数 f(x_i) 之间的差异。例如，您可以使用均方误差：L(a, b, c_1, ..., c_m) = Σ_i (f_N(x_i) - y_i)^2
使用线性回归最小化关于系数 a、b 和 c_i 的损失函数 L。这可以使用梯度下降或其他优化算法来完成。
在优化过程中，通过裁剪或投影系数到边界定义的可行区域，确保系数保持在提供的边界内。
我收到以下错误，无法调试它：

-----------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[30]，第 68 行
65 returnloss
67 # 执行梯度下降
---&gt; 68loss = gradient_descent()
70 # 绘制结果
71 plt.figure(figsize=(12, 6))

Cell In[30]，第 41 行
38 for i in range(num_iterations):
39 # 计算梯度
40 predictions = f_N(x_train)
---&gt; 41 错误 = 预测 - y_train
43 grad_a = np.outer(error, x_train) / num_samples
44 grad_b = np.mean(error)

ValueError：操作数不能与形状 (10,) (100,) 一起广播

如果有人能帮助我，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</guid>
      <pubDate>Wed, 19 Jun 2024 01:52:20 GMT</pubDate>
    </item>
    <item>
      <title>顺序模型拒绝构建</title>
      <link>https://stackoverflow.com/questions/78640126/sequential-model-refusing-to-build</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 在 Python 中构建一个用于训练数据的模型，但构建失败。
到目前为止，我已经尝试过这个：
def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories):
tf.random.set_seed(200)
model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),
layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) 

返回模型

model = create_model(NUM_WORDS, EMBEDDING_DIM, 32, 16, 5)

print(model)

---


每当我 print(model) 时，它都会显示 &lt;Sequential name=sequation,built=False&gt;。]]></description>
      <guid>https://stackoverflow.com/questions/78640126/sequential-model-refusing-to-build</guid>
      <pubDate>Wed, 19 Jun 2024 00:59:40 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络 (CNN) 在决策中使用黑色背景 - LIME</title>
      <link>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</link>
      <description><![CDATA[我是一名正在做学校项目的学生，需要帮助。
我的二元分类卷积神经网络在验证数据上具有非常高的准确率 (&gt;96%)，在测试数据集上的表现也同样出色。然而，当我使用 LIME 可视化图像中对其决策很重要的部分时，它往往会突出显示背景。所以我的问题是：
为什么会这样，以前有人见过吗？
当它在做决定时实际上是看着黑色面具时，它是如何达到 96% 的准确率的？
我在图像上应用黑色面具的原因是，我得到的整个数据集具有完全相同的背景，即白色滚轮，并且正如您从我上传的其中一张图片中看到的那样，该模型在决策过程中严重依赖滚轮，因此我将背景预处理为完全黑色 (0, 0, 0)RGB 像素，但现在模型似乎以某种方式使用了它。
我只是被难住了，非常感谢任何帮助！
模型架构[滚轮问题示例应用黑色蒙版的问题示例](https://i.sstatic.net/TM0FGHCJ.png)
我尝试过各种架构，有些是用 keras 层构建的，甚至尝试过预训练的 ResNet50。我还改变了大多数重要的超参数，但行为仍然存在。如果有帮助，我可以提供任何细节。
我很感激任何提前提供的帮助！:)]]></description>
      <guid>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</guid>
      <pubDate>Wed, 19 Jun 2024 00:05:44 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'float' 和 'list' 实例之间不支持 '<'</title>
      <link>https://stackoverflow.com/questions/78640019/typeerror-not-supported-between-instances-of-float-and-list</link>
      <description><![CDATA[我正在做家庭作业，但一直遇到这个错误。这些是我正在使用的函数，问题似乎出在预测函数中。预测和 TPR FPR 分数函数按预期工作，直到我在 roc_curve_computer 函数中调用它们。然后我收到 TypeError：&#39;&lt;&#39; not supports between &#39;float&#39; and &#39;list&#39; 消息。我尝试了多种不同的方法，但我遗漏了一些东西，但我仍然无法确定。问题出现在阈值变量从分配早期的单个浮点数变为分配结束时的浮点数组时。
def predict(probs, Threshold):
#创建数组来存储数据
preds = []

#for 循环遍历阈值上方和下方的数据排序。
for probs in probs:
if probs &lt;阈值：
preds.append(0)
else：
preds.append(1)
return preds

# 阈值
thresh = 0.5

# 预测值
preds = predict(probs, thresh)

def TPR_FPR_score(true_labels, preds):

# 将数组分类为 pos 和 neg 类别
actual_pos = np.array([True if x == 1 else False for x in true_labels])
actual_neg = np.array([True if x == 0 else False for x in true_labels])
pred_pos = np.array([True if x == 1 else False for x in preds])
pred_neg = np.array([True if x == 0 else False for x in preds])

# 设置准确度公式的变量
TP = (pred_pos) &amp; actual_pos).sum()
TN = (pred_neg &amp; actual_neg).sum()
FP = (pred_pos &amp; actual_neg).sum()
FN = (pred_neg &amp; actual_pos).sum()

#使用精度分数公式
TPR = TP / (TP + FN)

FPR = FP / (FP + TN)

return TPR, FPR

def roc_curve_computer(true_labels_check, pred_probs_check, Threshold):

TPR = []
FPR = []
preds_new = []
labels = 5

#拉动函数以获取预测值
for labels in range(labels):
preds_new = predict(pred_probs_check, Threshold)

#拉动函数以获取 TPR 和 FPR 值
for labels in range(labels):
TPR, FPR = TPR_FPR_score(labels, preds_new)

返回 TPR、FPR

true_labels_check = [1, 0, 1, 0, 0]
pred_probs_check = [0.875, 0.325, 0.6, 0.09, 0.4]
thresholds_check = [0.00, 0.25, 0.50, 0.75, 1.00]

print(roc_curve_computer(true_labels_check, pred_probs_check, Thresholds_check))

我尝试了另一种方式来执行预测函数，但遇到了类似的问题。任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/78640019/typeerror-not-supported-between-instances-of-float-and-list</guid>
      <pubDate>Tue, 18 Jun 2024 23:47:33 GMT</pubDate>
    </item>
    <item>
      <title>在图层中添加 CategoryEncoding 会导致形状不匹配</title>
      <link>https://stackoverflow.com/questions/78639963/adding-categoryencoding-to-the-layer-causes-mismatch-in-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78639963/adding-categoryencoding-to-the-layer-causes-mismatch-in-shape</guid>
      <pubDate>Tue, 18 Jun 2024 23:15:47 GMT</pubDate>
    </item>
    <item>
      <title>是否应将多个分类嵌入组合成条件 GAN（cGAN）？</title>
      <link>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</link>
      <description><![CDATA[我正在尝试制作一个条件 GAN (cGAN)，它可以根据标题和视频类别/流派生成 YouTube 缩略图。
它根本不起作用，甚至差得很远，所以我试图回到有关我的架构的基本问题。现在，我所做的是制作两个嵌入向量，一个用于标题，一个用于类别，然后我将它们组合起来并将它们都发送到生成器和鉴别器。
我想知道这样做可以吗？还是我应该分别传递它们？我尝试对其进行一些研究，但这是一个相当小众的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</guid>
      <pubDate>Tue, 18 Jun 2024 21:04:12 GMT</pubDate>
    </item>
    <item>
      <title>顺序模型未建立</title>
      <link>https://stackoverflow.com/questions/78639634/sequential-model-not-building</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 在 Python 中构建一个用于训练数据的模型，但构建失败。有人发现问题了吗？
我到目前为止已经尝试过了：
def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories):
tf.random.set_seed(200)
model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),
layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

返回模型

model = create_model(NUM_WORDS, EMBEDDING_DIM，32，16，5）

print（model）

每当我print（model）时，它都会显示&lt;Sequential name=sequation,built=False&gt;。]]></description>
      <guid>https://stackoverflow.com/questions/78639634/sequential-model-not-building</guid>
      <pubDate>Tue, 18 Jun 2024 20:58:02 GMT</pubDate>
    </item>
    <item>
      <title>理解 Transformers 的结果，通过梯度下降进行情境学习</title>
      <link>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</link>
      <description><![CDATA[我正在尝试实现这篇论文：
https://arxiv.org/pdf/2212.07677
（这是他们的代码）：
https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
我正在努力匹配他们的实验结果。具体来说，在他们最简单的 GD 模型（单层、单头、无 softmax）上，他们在测试数据上获得了大约 0.20 的恒定低损失。从概念上讲，我不太明白为什么会这样。
据我所知，这个模型只对数据进行了一次梯度下降迭代，那么为什么它会达到如此低的损失？为什么损失在训练步骤中会保持恒定/接近恒定？我们不是在 GD 模型中训练学习率吗？]]></description>
      <guid>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</guid>
      <pubDate>Tue, 18 Jun 2024 20:43:45 GMT</pubDate>
    </item>
    <item>
      <title>如何将设计元素集成到圆形的三个字母的标签中，以便机器学习系统有效地识别方向？</title>
      <link>https://stackoverflow.com/questions/78639183/how-to-integrate-design-elements-into-a-circular-three-letter-tag-for-effective</link>
      <description><![CDATA[我正在开发一个项目，涉及圆形标签的自动方向识别，每个标签都标有三个字母。这些标签可以从任何角度拍摄。我希望融入有助于确定其方向的设计元素，以便机器学习模型进行后续处理。识别系统可能包括图像转换，然后进行机器学习分析。
约束和目标：
标签是圆形的，具有三个字母的代码，并且需要非侵入性且体积小。
设计元素必须清楚地指示方向，才能被计算机视觉系统有效识别。
首选方法包括使用视觉元素、不同的颜色或纹理，因为这些必须易于被系统识别。
具体问题：
我可以将哪些设计元素融入这些三个字母的标签中，以清楚地指示它们的方向以供 ML 处理？
是否有经过验证的计算机视觉和机器学习方法来处理这种方向识别问题？
如果能提供任何实施示例或关于纹理、图案或其他设计特征类型的建议，这些设计特征在类似场景中可以很好地与机器学习应用程序配合使用，我将不胜感激。
感谢您的建议和指导。]]></description>
      <guid>https://stackoverflow.com/questions/78639183/how-to-integrate-design-elements-into-a-circular-three-letter-tag-for-effective</guid>
      <pubDate>Tue, 18 Jun 2024 18:58:45 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“keras.src.backend”没有属性“convert_to_numpy”</title>
      <link>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</link>
      <description><![CDATA[我尝试使用 utoencoder 和 rus 相应的代码，并在使用 tensorflow 和 keras 时遇到问题，在下面的代码中我展示了代码和相应的错误。当我拟合自动编码器模型时，它显示 AttributeError: module &#39;keras.src.backend&#39; 没有属性 &#39;convert_to_numpy&#39;。我无法理解这个错误和相应的解决方案。对于这种情况我该如何解决我的问题？我使用 anaconda3 运行此代码。我使用 tensorflow 版本 2.16.1 和 keras 版本 3.3.3，错误显示在模型、拟合线中。我尝试使用自动编码器消除噪音，在这种情况下我编写了代码。我尝试运行多次但没有成功。我在 genimi 中写入错误。它向我展示了两种方法 1. 升级 tensorflow 和 keras 2. 不要使用 backend.convert_to_numpy，而是使用推荐的方法在较新版本中将张量转换为 NumPy 数组。]]></description>
      <guid>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</guid>
      <pubDate>Tue, 18 Jun 2024 17:28:14 GMT</pubDate>
    </item>
    <item>
      <title>如何准确检测不同音轨中主节拍和配乐的开始？[关闭]</title>
      <link>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</link>
      <description><![CDATA[我正在做一个需要编辑配乐的项目。挑战在于检测任何给定配乐的主要节拍和旋律何时得到正确发展。我确信有更好的术语来描述我的目标，但理想情况下，我想跳过“构建”并立即让歌曲从“主要部分”开始。这需要适用于不同类型的各种歌曲，这些歌曲通常具有不同的结构和开始模式，这使得简化流程变得困难。
例如：
https://www.youtube.com/watch?v=P77CNtHrnmI -&gt;我希望我的代码能够识别 0:24 处的开始
https://www.youtube.com/watch?v=OOsPCR8SyRo -&gt; 0:12 处的开始检测
https://www.youtube.com/watch?v=XKiZBlelIzc -&gt; 0:19 处的起始检测
我尝试使用 librosa 分析起始强度并检测节拍，但当前的实现要么检测到歌曲的最开始，要么无法一致地识别节拍何时完全形成。
这是我的方法；
def analyze_and_edit_audio(input_file, output_file):
y, sr = librosa.load(input_file)
tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
beat_times = librosa.frames_to_time(beat_frames, sr=sr)
main_beat_start = beat_times[0]

我对 librosa/audio 编辑经验很少，因此如果您有任何建议，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</guid>
      <pubDate>Tue, 18 Jun 2024 10:35:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit-learn 在 Python 中对未标记数据实现层次聚类？</title>
      <link>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</link>
      <description><![CDATA[我正在学习聚类，在尝试查找带有标记数据的数据库时遇到了一些问题，这对我来说是一个限制，因为我发现了非常有趣的未标记数据集。我读过各种无监督聚类技术，并想实现层次聚类。
我将数据加载到 pandas DataFrame 中，对数据进行标准化并应用层次聚类。然后我可视化了树状图，但我不确定如何解释结果或我是否使用了正确的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</guid>
      <pubDate>Sat, 15 Jun 2024 04:03:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么单棵树的随机森林比决策树分类器好得多？</title>
      <link>https://stackoverflow.com/questions/48239242/why-is-random-forest-with-a-single-tree-much-better-than-a-decision-tree-classif</link>
      <description><![CDATA[我使用以下代码将决策树分类器和随机森林分类器应用于我的数据：
def decision_tree(train_X, train_Y, test_X, test_Y):

clf = tree.DecisionTreeClassifier()
clf.fit(train_X, train_Y)

return clf.score(test_X, test_Y)

def random_forest(train_X, train_Y, test_X, test_Y):
clf = RandomForestClassifier(n_estimators=1)
clf = clf.fit(X, Y)

return clf.score(test_X, test_Y)

为什么随机森林分类器的结果好得多（运行 100 次，随机抽取 2/3 的数据用于训练，1/3 用于测试）？
100%|██████████████████████████████████████| 100/100 [00:01&lt;00:00, 73.59it/s]
算法：决策树
最小值：0.3883495145631068
最大值：0.6476190476190476
平均值：0.4861783113770316
中位数：0.48868030937802126
标准差：0.047158171852401135
方差：0.0022238931724605985
100%|█████████████████████████████████████████| 100/100 [00:01&lt;00:00, 85.38it/s]
算法：随机森林
最小值：0.6846846846846847
最大值：0.8653846153846154
平均值：0.7894823428836184
中位数：0.7906101571063208
标准差：0.03231671150915106
方差：0.0010443698427656967

具有一个估计量的随机森林估计量不只是决策树吗？
我做错了什么或误解了这个概念吗？]]></description>
      <guid>https://stackoverflow.com/questions/48239242/why-is-random-forest-with-a-single-tree-much-better-than-a-decision-tree-classif</guid>
      <pubDate>Sat, 13 Jan 2018 11:04:39 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络中的旋转等方差？</title>
      <link>https://stackoverflow.com/questions/28201617/rotational-equivariance-in-convolutional-neural-network</link>
      <description><![CDATA[我想知道 CNN 的基本架构是否具有旋转等方差特性？我只知道平移等方差，但不确定旋转等方差。
根据我的搜索，可以通过旋转输入图像进行训练来实现旋转等方差。我真的需要这样做吗？旋转度数是多少？更具体地说，例如，我有一个可以在横向模式下检测/读取文本的 CNN。如果我将图像旋转 90 度/使其变为纵向，它会给出与原始图像相同的结果/执行相同的操作吗？]]></description>
      <guid>https://stackoverflow.com/questions/28201617/rotational-equivariance-in-convolutional-neural-network</guid>
      <pubDate>Wed, 28 Jan 2015 20:18:54 GMT</pubDate>
    </item>
    </channel>
</rss>