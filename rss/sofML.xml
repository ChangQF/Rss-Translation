<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 16 Sep 2024 18:21:57 GMT</lastBuildDate>
    <item>
      <title>根据周围站点数据预测某个站点温度的建议和想法[关闭]</title>
      <link>https://stackoverflow.com/questions/78990923/suggestions-ideas-for-predicting-a-certain-stations-temperature-based-on-surro</link>
      <description><![CDATA[我有来自 X 个气象站的数据，每个气象站都有其纬度、经度、海拔高度和跨越数年的每日温度测量值。我的任务是根据周围气象站的数据预测其中一个气象站的温度。
因此，模型将获得所有 X 个气象站的数据，并对其进行自我训练，以期了解地理空间相关性（ASL 和站点之间的距离），然后根据前 X 天周围气象站的测量值随机预测这 20 个气象站中某一天的温度。
此外，通过一次预测一个气象站来训练网络，然后让每个气象站都获得 X 个模型来预测单个“缺失”气象站，还是使用随机选择，这样更好？
关于如何完成这项任务，有什么建议或见解吗？
我研究过一些领域，例如回归模型和 RNN 技术，但没有什么大收获。]]></description>
      <guid>https://stackoverflow.com/questions/78990923/suggestions-ideas-for-predicting-a-certain-stations-temperature-based-on-surro</guid>
      <pubDate>Mon, 16 Sep 2024 15:57:01 GMT</pubDate>
    </item>
    <item>
      <title>保存的检查点中不存在策略目录</title>
      <link>https://stackoverflow.com/questions/78990798/policies-directory-not-present-in-saved-checkpoint</link>
      <description><![CDATA[我正在使用 RayRL Lib，在切换到新 API 版本后，检查点目录不再包含策略文件夹。为什么会发生这种情况？
目前，检查点包含以下目录和文件：
env_runner
learner_group
algorithm_state.pkl
class_and_ctor_args.pkl
rllib_checkpoint.json

如能提供任何帮助，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78990798/policies-directory-not-present-in-saved-checkpoint</guid>
      <pubDate>Mon, 16 Sep 2024 15:18:44 GMT</pubDate>
    </item>
    <item>
      <title>训练 YOLOv8m 姿态估计模型时，第 90 个 Epoch 的 train/kobj_loss 出现意外峰值 [关闭]</title>
      <link>https://stackoverflow.com/questions/78990077/unexpected-spike-in-train-kobj-loss-at-epoch-90-while-training-yolov8m-pose-esti</link>
      <description><![CDATA[我在训练期间观察到 train/kobj_loss（关键点对象损失）中的一种特殊行为。具体来说，在第 90 个时期左右，kobj_loss 突然飙升，从 0.149 增加到 0.196，然后在第 90 个时期后稳定下来并再次下降（请参阅附图以供参考）。
损失图：

我的问题是：

什么可能导致 kobj_loss 突然飙升？
我应该进行哪些典型的调整或调查来解决这个问题？
这种行为在任何情况下都是可以预料到的，例如学习率变化或数据相关因素？

我正在使用 Ultralytics 库来训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78990077/unexpected-spike-in-train-kobj-loss-at-epoch-90-while-training-yolov8m-pose-esti</guid>
      <pubDate>Mon, 16 Sep 2024 12:01:36 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何开发这个计算机视觉项目？狗消除检测器，通过 RTSP 摄像头、Web 应用程序发出警报 [关闭]</title>
      <link>https://stackoverflow.com/questions/78989939/how-should-i-approach-in-developing-this-computer-vision-project-dog-eliminatio</link>
      <description><![CDATA[我目前正在开发一个项目，旨在使用 YOLOv8 开发一个狗消除检测器，并使用声音警报来阻止在摄像机的视觉范围允许的指定地点的狗。此应用程序可通过带有登录门户的网页查看。
堆栈：Python、Django、YOLOv8、OpenCV、HTML、CSS、JS。
我遇到了许多问题，我不确定我是否为这个项目使用了正确的模型，我目前正在使用 yolov8s.pt 进行迁移学习，并在一个包含 1 个类别（大便狗）的数据集上进行训练。
它看起来是这样的：
results = model.train(data=&quot;config.yaml&quot;, epochs=100,patient=15, batch=4,workers=6,device=[0])

然而，经过多次尝试和整理数据集，我似乎总是得不到任何检测结果，而当我得到检测结果时，它们总是误报。难道物体检测是错误的选择，考虑到项目目标，姿势估计会更好吗？]]></description>
      <guid>https://stackoverflow.com/questions/78989939/how-should-i-approach-in-developing-this-computer-vision-project-dog-eliminatio</guid>
      <pubDate>Mon, 16 Sep 2024 11:25:56 GMT</pubDate>
    </item>
    <item>
      <title>使用特定时间之前 30 分钟的历史数据来预测该事件是否以及在哪个时间戳将在接下来的 15 分钟内发生</title>
      <link>https://stackoverflow.com/questions/78989934/using-30-minutes-of-historical-data-before-a-specific-time-to-predict-whether-th</link>
      <description><![CDATA[我有一个如下所示的数据框：
 时间戳 X Y Z V D
90 2003-01-01 02:42:00 9999.99 9999.99 9999.99 99999.9 999.99
91 2003-01-01 02:43:00 9999.99 9999.99 9999.99 99999.9 999.99
92 2003-01-01 02:44:00 9999.99 9999.99 9999.99 99999.9 999.99
93 2003-01-01 02:45:00  9999.99 9999.99 9999.99 99999.9 999.99 94 2003-01-01 02:46:00 9999.99 9999.99 9999.99 99999.9 999.99 95 2003-01-01 02:47:0 0 9999.99 9999.99 9999.99 99999.9 999.99 96 2003-01-01 02:48:00 9999.99 9999.99 9999.99 99999.9 999.99 97 2003-01-01 02:49:00 9999.99 9999.99 9999.99 99999.9 999.99 98 2003-01-01 02:50:00 9999.99 9999.99 9999.99 99999.9 999.99 99 2003-01-0 1 02:51:00 9999.99 9999.99 9999.99 99999.9 999.99 100 2003-01-01 02:52:00 9999.99 9999.99 9999.99 99999.9 999.99 101 2003-01-01 02:53:00 9999.99 9999.99 9999.99 99999.9 999.99 102 2003-01-01 02:54:00 9999.99 9999.99 9999.99 99999.9 999.99 103 2003-01-01 02:55:00 9999.99 9999.99 9999.99 99999.9 999.99 104 2003-01-01 02:56:00 9999.99 9999.99 9999.99 99999.9   999.99 105 2003-01-01 02:57:00 9999.99 9999.99 9999.99 99999.9 999.99 106 2003-01-01 02:58:00 9999.99 9999.99 9999.99 9999 9.9 999.99 107 2003-01-01 02:59:00 9999.99 9999.99 9999.99 99999.9 999.99 108 2003-01-01 03:00:00 -5.25 2.63 -2.78 -405.2     4.62 109 2003-01-01 03:01:00 -4.89 3.54 -2.46 -402.7 4.97 110 2003-01-01 03:02:00 -4.50 4.04 -2.20 -402.7 4.97 111 2003-01-01 03 :03:00 -4.43 3.97 -2.50 -405.0 4.43 112 2003-01-01 03:04:00 -4.40 4.47 -2.01 -407.6 4.56 113 2003-01-01 03:05:00 -4.35 4.42 -2.08   -405.9 4.36 114 2003-01-01 03:06:00 -4.33 4.56 -1.82 -407.4 4.33 115 2003-01-01 03:07:00 -4.52 4.41 -2.05 -407.4 4.33 116 2003-01 -01 03:08:00 -4.48 4.36 -2.04 -402.8 4.04 117 2003-01-01 03:09:00 -4.11 4.63 -2.24 -403.2 4.35 118 2003-01-01 03:10:00 -4.09 4.60    -2.28 -405.3 4.43 119 2003-01-01 03:11:00 -3.91 4.67 -2.37 -404.0 4.46 120 2003-01-01 03:12:00 -2.89 5.67 -1.60 -406.4 4.68 121 20 03-01-01 03:13:00 -3.16 5.77 -1.17 -407.5 4.83 122 2003-01-01 03:14:00 -2.99 5.95 -1.03 -407.4 4.72 123 2003-01-01 03:15:00 -2.86     6.09 -0.82 -407.0 4.49 124 2003-01-01 03:16:00 -1.15 6.07 0.75 -411.3 5.17 125 2003-01-01 03:17:00 0.64 6.06 2.25 -411.3 5.17 1 26 2003-01-01 03:18:00 0.49 6.10 2.42 -421.8 6.41 127 2003-01-01 03:19:00 0.72 6.17 2.06 -422.5 6.42 128 2003-01-01 03:20:00     0.57 6.23 1.86 -425.3 6.46 129 2003-01-01 03:21:00 0.77 6.12 1.97 99999.9 999.99 130 2003-01-01 03:22:00 0.59 6.16 2.10 -424 .8 6.11 131 2003-01-01 03:23:00 0.69 6.22 1.83 -424.8 6.11 132 2003-01-01 03:24:00 0.43 6.32 1.65 99999.9 999.99 133 2003-01-01 03:25:00 0.22 6.12 2.01 -423.0 5.36
134 2003-01-01 03:26:00 0.14 6.10 2.04 -423.0 5.36

我知道 2003-01-01 03:12:00 发生了一些特定的事情，与历史指标中其他列的数据有关。我需要能够使用 03:12:00 之前 30 分钟的数据来判断 03:12:00 之类的事情是否会在 03:12:00 到 03:12:00 之后 15 分钟内发生。
一般来说，我有 100 个事件及其已知时间戳，其中我有 30 分钟前的数据和 15 分钟的数据，我知道其中第一分钟发生了一些事情。
我希望能够使用 ML 来识别在之前的分析集 30 分钟之后的预测集 15 分钟内何时会发生一分钟的独特事件。
我一直在阅读有关 sklearn 的文章，并且我有一个 ML 的初学者介绍。我知道我需要确定目标。我知道 03:11:00 之后 15 分钟区块的时间戳的其他列中的所有数据都可以归类为“是”。如果我们说这些是目标，我该怎么做？一般来说，30 分钟区块之后的所有 15 分钟区块都是“是”。我如何使用 Python 中的非线性 ML 模型来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78989934/using-30-minutes-of-historical-data-before-a-specific-time-to-predict-whether-th</guid>
      <pubDate>Mon, 16 Sep 2024 11:24:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 React 应用程序中离线运行 roboflow 模型？</title>
      <link>https://stackoverflow.com/questions/78989326/how-to-run-roboflow-models-offline-in-a-react-app</link>
      <description><![CDATA[我在 Mac 上本地运行了 Roboflow 推理服务器，但我不确定如何使用 inferencejs 包在本地推理服务器上进行推理，更具体地说是在 Vite 中。
我已经将弃用的（我认为）Roboflowjs 库与 window.roboflow.auth 等一起使用，但我不确定如何向本地推理服务器发出请求。]]></description>
      <guid>https://stackoverflow.com/questions/78989326/how-to-run-roboflow-models-offline-in-a-react-app</guid>
      <pubDate>Mon, 16 Sep 2024 08:08:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytesseract OCR 检测“数字单位”格式的文本，将数字转换为浮点数时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</guid>
      <pubDate>Mon, 16 Sep 2024 07:49:42 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中的符号并将其输出为文本的最佳方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78988645/whats-the-best-way-to-recognize-symbols-in-an-image-and-output-them-as-text</link>
      <description><![CDATA[假设我有一张这样的图像，我想将看到的符号转换为文字（星号、正方形、菱形等），保持正确的顺序，解决这个问题的最佳方法是什么？
我在网上看到过很多使用 OpenCV 识别文本或数字的方法和库，但关于一般符号的并不多]]></description>
      <guid>https://stackoverflow.com/questions/78988645/whats-the-best-way-to-recognize-symbols-in-an-image-and-output-them-as-text</guid>
      <pubDate>Mon, 16 Sep 2024 01:36:47 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试训练人工智能，但使用 rust 和 pytorch 的准确率很低 [关闭]</title>
      <link>https://stackoverflow.com/questions/78988616/im-trying-to-train-an-ai-but-i-have-low-accuracy-using-rust-and-pytorch</link>
      <description><![CDATA[我刚刚开始接触机器学习的世界，我非常喜欢 Rust。我一直在测试和学习更多。在此先感谢你们的支持。
我以迁移训练为例做了一些测试，但我不明白为什么使用相同的验证库，训练中的准确率很高，而测试中的准确率很低。有人能理解为什么吗？我研究过过度拟合，但似乎不是这样，因为我使用的是相同的验证库，没有新数据。
`
use std::env;
use std::error::Error;
use std::path::PathBuf;

use anyhow::{ bail, Result };
use tch::nn::{ self, ModuleT, OptimizerConfig, VarStore };
use tch::vision::{ imagenet, resnet };
use tch::{ Device, Kind, Tensor };
pub fn bee_test() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {
tch::manual_seed(123);
let manifest_dir = env::var(&quot;CARGO_MANIFEST_DIR&quot;)?;
let project_dir = PathBuf::from(manifest_dir);

let dataset_path = project_dir.join(&quot;data/hymenoptera_data&quot;);
let dataset = imagenet::load_from_dir(dataset_path)?;
println!(&quot;{dataset:?}&quot;);

let model_path = project_dir.join(&quot;data/bee.ot&quot;);
println!(&quot;模型路径：{:?}&quot;, model_path);

let device = Device::cuda_if_available();
让 mut vs = VarStore::new(device);
vs.load(model_path.as_path()).map_err(|op| {
format!(&quot;加载模型时出错：{:?}&quot;, op);
op
})?;

让 net = resnet::resnet34_no_final_layer(&amp;vs.root());
让 linear = nn::linear(vs.root(), 512, 2, Default::default());

让 net2: nn::Sequential = nn
::seq()
.add_fn(move |xs| net.forward_t(xs, false))
.add(linear);

让 predict = net2.forward_t(&amp;dataset.test_images, false);
让 probabilities = predict.softmax(-1, tch::Kind::Float);
probabilities.print();

let class = predict.argmax(-1, false);
class.print();

let test_accuracy = predict.accuracy_for_logits(&amp;dataset.test_labels);

println!(&quot;测试准确率：{:.2}%&quot;, 100.0 * f64::try_from(test_accuracy)?);

Ok(())
}

pub fn bee_train() -&gt; Result&lt;()&gt; {
tch::manual_seed(123);

let manifest_dir = env::var(&quot;CARGO_MANIFEST_DIR&quot;)?;
let project_dir = PathBuf::from(manifest_dir);

let dataset_path = project_dir.join(&quot;data/hymenoptera_data&quot;);
let model_path = project_dir.join(&quot;data/resnet34.ot&quot;);

// 加载数据集并将其大小调整为通常的 imagenet 尺寸 224x224。
let dataset = imagenet::load_from_dir(dataset_path)?;
println!(&quot;{dataset:?}&quot;);

// 创建模型并从文件中加载权重。
let mut vs = tch::nn::VarStore::new(tch::Device::Cpu);
let net = resnet::resnet34_no_final_layer(&amp;vs.root());
vs.load(model_path)?;

// 预先计算最终激活。
let train_images = tch::no_grad(|| dataset.train_images.apply_t(&amp;net, false));
let test_images = tch::no_grad(|| dataset.test_images.apply_t(&amp;net, false));
println!(&quot;训练图像形状：{:?}&quot;, train_images.size());
println!(&quot;测试图像形状：{:?}&quot;, test_images.size());

// 初始化线性层和优化器
let linear = nn::linear(vs.root(), 512, dataset.labels, Default::default());
let mut sgd = nn::Sgd::default().build(&amp;vs, 1e-3)?;

for epoch_idx in 1..6000 {
let predict = train_images.apply(&amp;linear);
let loss = predict.cross_entropy_for_logits(&amp;dataset.train_labels);
sgd.backward_step(&amp;loss);

let test_accuracy = test_images.apply(&amp;linear).accuracy_for_logits(&amp;dataset.test_labels);
println!(
&quot;Epoch {}: 训练损失 = {:.4}, 测试准确率 = {:.2}%&quot;,
epoch_idx,
f64::try_from(loss)?,
100.0 * f64::try_from(test_accuracy)?
);
}

let save_model_path = project_dir.join(&quot;data/bee.ot&quot;);
vs.save(save_model_path)?;
Ok(())
}

使用 tch-rs
训练中获得的结果
Epoch 5999：训练损失 = 0.0148，测试准确率 = 96.75%
测试中获得的结果
测试准确率：46.10%
我想了解准确率低的原因，如何改进，途径是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78988616/im-trying-to-train-an-ai-but-i-have-low-accuracy-using-rust-and-pytorch</guid>
      <pubDate>Mon, 16 Sep 2024 01:00:02 GMT</pubDate>
    </item>
    <item>
      <title>带有随机森林的分类链：为什么即使 Base Estimator 可以处理 np.nan 却不支持它？</title>
      <link>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</link>
      <description><![CDATA[我正在研究一个多标签分类问题，使用ClassifierChain方法，以RandomForestClassifier作为基础估计器。我遇到了一个问题，我的输入矩阵X包含np.nan值。当单独使用RandomForestClassifier时，它可以毫无问题地处理NaN值，因为它通过其内部树分割机制原生支持缺失值。
这让我很困惑，因为基础估计器（RandomForestClassifier）确实可以正确处理NaN值。我不明白为什么 ClassifierChain（它只是一个包装器）会在底层分类器没有 NaN 问题的情况下引发此错误。
当我训练一个简单的 RandomClassifier 时，它确实会处理 np.nan：
from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
y_single_label = [0, 0, 1, 1]

tree = RandomForestClassifier(random_state=0)
tree.fit(X, y_single_label)
X_test = np.array([np.nan]).reshape(-1, 1)
tree.predict(X_test)

即使我使用 MultiOutputClassifier 而不是ClassifierChain（不模拟标签之间的依赖关系），训练进行时没有任何错误，即使输入中有 NaN - 正如预期的那样。
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain , MultiOutputClassifier

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)

# 用于多标签分类的两个标签列
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

# 基础分类器
base_clf = RandomForestClassifier()

# MultiOutputClassifier（二元相关性）与基础分类器
clf_BR = MultiOutputClassifier(base_clf)

# 拟合模型
clf_BR.fit(X, y)

但是，当我切换到 ClassifierChain 方法时：
# 带有基础分类器的分类器链
clf_chain = ClassifierChain(base_clf)

# 拟合模型
clf_chain.fit(X, y)

我在超参数调整期间收到以下错误：

试验 0 失败，参数为：{&#39;n_estimators&#39;: 30, &#39;max_depth&#39;: 16, &#39;max_samples&#39;: 0.4497444900238575, &#39;max_features&#39;: 550, &#39;order_type&#39;: &#39;random&#39;}，错误原因如下：ValueError(&#39;输入 X 包含 NaN.\nClassifierChain 不接受编码为 NaN 的缺失值原生。对于监督学习，您可能需要考虑 sklearn.ensemble.HistGradientBoostingClassifier 和 Regressor，它们原生接受编码为 NaN 的缺失值。或者，可以预处理数据，例如通过在管道中使用 imputer 转换器或删除具有缺失值的样本。请参阅https://scikit-learn.org/stable/modules/impute.html 您可以在以下页面找到处理 NaN 值的所有估算器的列表：https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values&#39;)

由于保持缺失值原样而不是对其进行插补或删除非常重要，我想知道是否有办法让 ClassifierChain 处理缺失值。是否有任何解决方法或我遗漏了什么？
以下是我的环境详细信息：

Python 版本：3.12.5（由 conda-forge 打包）
scikit-learn 版本：1.5.1
]]></description>
      <guid>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</guid>
      <pubDate>Fri, 13 Sep 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>Ray 自定义环境渲染</title>
      <link>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</link>
      <description><![CDATA[我正在创建自己的 gym 环境来测试 freeze-tag 问题。我正在尝试使用 Ray 来做 MAPPO。我有两个问题：
1：我的模拟没有渲染
2：它创建了多个 PyGame 窗口
我已将渲染方法和训练脚本的片段附加到附件中。
# 渲染函数
def render(self):
self.screen.fill((255, 255, 255))

for agent in self.all_agents:
if agent.status == 1:
pygame.draw.circle(self.screen, agent.color, (agent.x, agent.y), agent.size)

elif agent.status == 0:
pygame.draw.circle(self.screen, (0, 255, 255), (agent.x, agent.y),agent.size)

pygame.display.flip()

# Train_MAPPO_FTP.py
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
import gym_FTP as e
import pygame
import numpy as np

# 环境创建函数
def env_creator(config):
robots = 5
adversaries = 2
time_steps = 500

screen = pygame.display.set_mode([1000, 1000])
gym_ftp = e.gym_FTP(screen, robots, 0, adversaries, time_steps, 15)
return gym_ftp

def train_and_evaluate(time_steps):
# 初始化 Ray
ray.init(ignore_reinit_error=True)

# 注册环境
register_env(&quot;Env_FTP&quot;, env_creator)

# create_env_on_local_worker = True
# 配置算法
config = PPOConfig() \
.environment(&quot;Env_FTP&quot;) \
.rollouts(num_rollout_workers=1,
rollout_fragment_length=1,
create_env_on_local_worker=True) \
.training(
train_batch_size=1, # 每次训练更新前汇总经验
sgd_minibatch_size=1,
model={&quot;fcnet_hiddens&quot;: [64, 64]}
) \
.framework(&quot;torch&quot;) \
.evaluation(evaluation_num_workers=1) \
.resources(num_gpus=0) # 设置 GPU 数量

# 构建算法
algo = config.build()

# 参数
episodes = 5
iterations = time_steps / 10

for episode in range(episodes):
for i in range(int(iterations)):
results = algo.train()
print(f&quot;训练迭代 {i + 1} 已完成。mean_reward {results[&#39;episode_reward_mean&#39;]},&quot;
f&quot; 总损失 {results[&#39;info&#39;][&#39;learner&#39;][&#39;__all__&#39;][&#39;total_loss&#39;]}&quot;)

# 关闭 Ray
ray.shutdown()

def main():
time_steps = 500
train_and_evaluate(time_steps)

main()


我已进行多次检查，以测试我的代理的速度是否根据新操作进行更新，以及位置是否正在更新，因此我确定这不是问题所在。当我使用其他算法进行测试时，此环境也有效。我可以正确使用 gym 环境的其他功能，并让它渲染和做一些有趣的事情。这似乎完全是 RAY 的问题。我的目标是拥有 n 个机器人和 m 个对手。我想根据环境状态为 n 个代理获取新操作。我想每集训练 500 个时间步，收集 10 个批次。例如前 10 个时间步，然后再添加 10 个时间步作为经验，然后再添加 10 个。所以我们每集最多更新 50 次。我们将进行 100 集。]]></description>
      <guid>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</guid>
      <pubDate>Wed, 11 Sep 2024 20:54:21 GMT</pubDate>
    </item>
    <item>
      <title>级联分段-通道设置是否正确？</title>
      <link>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</link>
      <description><![CDATA[我想训练一个机器学习模型，用于处理 2D DICOM 图像中的精细蒙版细节。我有 500 张图像准备进行标记/注释。我可以使用这种技术吗？还是我理解错了？
鱼 + 脊椎的注释


我用 1 个类别注释了 500 张图像：鱼。然后我训练一个 model1.pth，将鱼从背景中分割出来。该模型有 2 个 out_channels：鱼和背景。

我再次注释相同的 500 幅图像，但现在有 2 个类别：鱼 + 脊椎。我加载 model1.pth 并创建一个具有 2 个 in_channels 和 3 个 out_channels 的模型：脊椎、鱼和背景，并将模型保存为 model2.pth

最后，我再次注释了 500 幅图像，但现在我包括了变形。如果我有 3 种类型的变形，则每种变形都会有自己的类别。我加载 model2.pth，创建一个具有 3 个 in_channels 和 6 个 out_channels 的模型：背景、鱼、脊柱、deform1、deform2、deform3，并将模型保存为 model3.pth。

该模型现在可以直接用于新图像。这是它的工作原理吗？


背景和细节。我尝试过的方法
目标是找到鱼脊柱中的变形。到目前为止，我已尝试通过使用 MONAI 的 UNet 模型 来分割 3 个类 + 背景。这些图像是转换为 NifTi 格式 (.dcm.nii.gz) 的 2D DICOM 图像，典型尺寸为 2000x900 像素。我使用 3Dslicer 进行注释。类别（到目前为止）：

背景

鱼

脊椎

变形


到目前为止，我已经在（仅）12 张训练图像上进行了测试，只是为了让它运行，我得到了所有 3 个类别的结果，但我猜模型训练过度了。此外，我猜这种技术使得在训练结束后进行微小更改变得更加困难。例如我想要多种不同类型的变形。
我的结果：红线左侧：来自 tensorboard，红线右侧：在新图像上测试模型

在开始注释 500 张图像之前，我想验证我是否走在正确的道路上。我希望通过使用级联技术，我可以获得一个可以轻松分割鱼和脊椎的模型，并且我可以随后尝试不同的变形注释。]]></description>
      <guid>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</guid>
      <pubDate>Thu, 05 Sep 2024 05:34:37 GMT</pubDate>
    </item>
    <item>
      <title>优化欺诈检测不平衡数据的指标</title>
      <link>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</link>
      <description><![CDATA[我需要您的帮助来提高我的模型性能。就像大多数欺诈检测一样，我有一个不平衡的数据集（0.1/0.9）。我想优化目标 1 和 0 的召回率，因为一方面我想避免欺诈检测，另一方面我想限制将非欺诈客户定位为欺诈的成本，因为 5% 的错误分类会使我的收入减少 3000 欧元（而定位正确的欺诈者会让我为检测到的每位客户节省 1000 欧元的损失）。
我的第一个问题是：您会根据这个问题考虑哪些指标？我更关注召回率，但我会阅读您的意见。
第二个问题：我如何提高模型性能？
到目前为止，我在不降低阈值的情况下获得的最佳结果是：
准确率：0.89
混淆矩阵：
[[3153 279]
[ 145 297]]
分类报告：
准确率 召回率 f1 分数 支持
 0 0.96 0.92 0.94 3432
1 0.52 0.67 0.58 442

准确率 0.89 3874

而如果我降低阈值以增加目标 1 的召回率：
准确率：0.61
混淆矩阵：
[[1959 1473]
[ 42 400]]
分类报告：
准确率 召回率 f1 分数 支持率
 0 0.98 0.57 0.72 3432
1 0.21 0.90 0.35 442

准确率 0.61 3874

我尝试了几种模型：
线性回归、XGBoost、随机森林和 SVM
此外，甚至过采样/反采样技术（仅在训练集上）
RandomOverSampling、RandomUnderSampling、SMOTE
您还有其他建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</guid>
      <pubDate>Wed, 08 Nov 2023 10:08:43 GMT</pubDate>
    </item>
    <item>
      <title>机器学习无法预测正确的结果</title>
      <link>https://stackoverflow.com/questions/69469083/machine-learning-not-predicting-correct-results</link>
      <description><![CDATA[我正在创建一个简单的 Python 机器学习脚本，该脚本将根据以下参数预测贷款是否会被批准
商业经验：应大于 7
成立年份：应在 2015 年之后
贷款：没有以前或当前的贷款

如果符合上述条件，则只会批准贷款。该数据集可从此链接下载：
https://drive.google.com/file/d/1QtJ3EED7KDqJDrSHxHB6g9kc5YAfTlmF/view?usp=sharing
对于上述数据，我有以下脚本
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np

data = pd.read_csv(&quot;test2.csv&quot;)
data.head()

X = data[[&quot;Business Exp&quot;, &quot;Year of Founded&quot;, &quot;上一个/当前贷款&quot;]]
Y = data[&quot;OUTPUT&quot;]

clf = LogisticRegression()
clf.fit(X, Y)

test_x2 = np.array([[9, 2017, 0]])
Y_pred = clf.predict(test_x2)
print(Y_pred)

我在 test_x2 中传递测试数据。测试数据是，如果业务 exp 为 9，成立年份为 2017 年，没有当前/以前的贷款，则意味着将提供贷款。因此它应该预测，结果应该是 1，但它显示为 0。代码或数据集是否存在问题。由于我是机器学习的初学者，并且仍在学习它，因此我创建了这个自定义数据集以供我自己理解。]]></description>
      <guid>https://stackoverflow.com/questions/69469083/machine-learning-not-predicting-correct-results</guid>
      <pubDate>Wed, 06 Oct 2021 16:07:20 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中“Flatten”起什么作用？</title>
      <link>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</link>
      <description><![CDATA[我正在尝试了解 Keras 中 Flatten 函数的作用。下面是我的代码，这是一个简单的两层网络。它接收形状为 (3, 2) 的二维数据，并输出形状为 (1, 4) 的一维数据：
model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation(&#39;relu&#39;))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;SGD&#39;)

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape

这会打印出 y 具有形状 (1, 4)。但是，如果我删除 Flatten 行，则它会打印出 y 具有形状 (1, 3, 4)。
我不明白这一点。根据我对神经网络的理解，model.add(Dense(16, input_shape=(3, 2))) 函数正在创建一个隐藏的完全连接层，其中包含 16 个节点。这些节点中的每一个都连接到每个 3x2 输入元素。因此，第一层输出处的 16 个节点已经是“平坦的”。因此，第一层的输出形状应该是 (1, 16)。然后，第二层将其作为输入，并输出形状为 (1, 4) 的数据。
那么，如果第一层的输出已经是“平坦的”并且形状为 (1, 16)，为什么我需要进一步将其平坦化？]]></description>
      <guid>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</guid>
      <pubDate>Wed, 05 Apr 2017 16:48:24 GMT</pubDate>
    </item>
    </channel>
</rss>