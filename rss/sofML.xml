<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Apr 2024 15:14:02 GMT</lastBuildDate>
    <item>
      <title>为什么会出现错误“TypeError：Grouper.__init__() 获得意外的关键字参数“base””？</title>
      <link>https://stackoverflow.com/questions/78373272/why-is-there-an-error-typeerror-grouper-init-got-an-unexpected-keyword-a</link>
      <description><![CDATA[需要了解的版本：

Jupyter 笔记本：6.5.4

Python：3.11.3

熊猫：2.2.2

xarray：2022.11.0


我正在使用链接中的降水数据集进行作业 (http:// Research.jisao.washington.edu/data_sets/widmann/）来执行传感器选择（您应该将传感器放置在哪里才能最好地预测降雨）。
首先，我使用以下代码打开文件并读取它：
将 xarray 导入为 xr
将 pandas 导入为 pd

nc_file_path = &#39;./pnwrain.50km.daily.4994.nc&#39;
数据集 = xr.open_dataset(nc_file_path,decode_times=False)
数据集

输出：
图片 1 
然后，我使用以下代码将时间变量转换为日期时间：
ref_date = pd.to_datetime(&#39;1949-01-01&#39;)
# 将“时间”变量转换为日期时间
数据集[&#39;时间&#39;] = ref_date + pd.to_timedelta(数据集[&#39;时间&#39;], 单位=&#39;D&#39;)

之后，我使用以下代码计算每月总计：
monthly_totals = dataset[&#39;data&#39;].resample(time=&#39;ME&#39;).sum()

print(&quot;每月总计：&quot;,monthly_totals)

输出：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
TypeError Traceback（最近一次调用最后一次）
第 1 行 [3] 中的单元格
----&gt; 1 Month_totals = dataset[&#39;data&#39;].resample(time=&#39;ME&#39;).sum()
      3 print(&quot;每月总计：&quot;,monthly_totals)

文件〜\ anaconda3 \ Lib \ site-packages \ xarray \ core \ dataarray.py：6636，在DataArray.resample（自我，索引器，skipna，关闭，标签，基础，keep_attrs，loffset，restore_coord_dims，**indexer_kwargs）
   6540 「」返回用于执行重采样操作的 Resample 对象。
   6541
   6542 处理下采样和上采样。重新采样的
   （...）
   6632 .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
   第6633章
   第6634章
-&gt;第6636章
   第6637章
   第6638章
   第6639章
   6640 关闭=关闭，
   第6641章
   第6642章
   第6643章
   第6644章
   第6645章
   第6646章
   6647）

文件〜\ anaconda3 \ Lib \ site-packages \ xarray \ core \ common.py：965，在DataWithCoords._resample（self，resample_cls，索引器，skina，关闭，标签，基础，keep_attrs，loffset，restore_coord_dims，**indexer_kwargs）
    第963章
    第964章：
--&gt;第965章
    第966章 freq=频率，close=关闭，label=标签，base=基数，loffset=loffset
    第967章）
    第968章
    第969章
    第970章
    第971章
    第972章 自己，
    第973章 组=组，
   （...）
    第977章
    第978章

文件 ~\anaconda3\Lib\site-packages\pandas\core\resample.py:2208，在 TimeGrouper.__init__(self, obj, freq, key, close, label, how, axis, fill_method, limit, kind, convention,原点、偏移量、group_keys、**kwargs）
   第2205章
   第2206章
-&gt;第2208章

类型错误：Grouper.__init__() 得到了意外的关键字参数“base”

有人知道为什么会遇到这个错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78373272/why-is-there-an-error-typeerror-grouper-init-got-an-unexpected-keyword-a</guid>
      <pubDate>Tue, 23 Apr 2024 14:47:45 GMT</pubDate>
    </item>
    <item>
      <title>商业提案生成器</title>
      <link>https://stackoverflow.com/questions/78373183/businuss-proposal-generator</link>
      <description><![CDATA[我想微调或者使用嵌入来训练 gpt 为各种客户制定业务提案。我希望将招标或合同以及来自企业的信息（如名称、描述等）作为输入，并让它以特定格式输出提案。我该怎么办？
不知道如何开始]]></description>
      <guid>https://stackoverflow.com/questions/78373183/businuss-proposal-generator</guid>
      <pubDate>Tue, 23 Apr 2024 14:34:55 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的机器学习分类算法</title>
      <link>https://stackoverflow.com/questions/78373040/machine-learning-classification-algorithms-for-multi-level-data</link>
      <description><![CDATA[我正在开发一个机器学习项目，我的数据集包含 1960 年到 2022 年 218 个国家/地区的社会、人口和经济方面的变量。目标变量是一个二元变量（是或否），表示如果该国在某一年至少发生过一次政变企图。
我的问题是：多级数据的最佳分类模型是什么？
通过咨询不同的来源，我写下了这些模型（没有特定的顺序）：

随机森林
XGBoost
物流分类
决策树

他们是不是错了？还有更多我不知道的型号吗？
如果没有，您知道我可以使用哪些资源在 R 中实现这些模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78373040/machine-learning-classification-algorithms-for-multi-level-data</guid>
      <pubDate>Tue, 23 Apr 2024 14:14:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 中的二元分类获得近 100% 的准确度，但电子邮件的预测级别却极其错误</title>
      <link>https://stackoverflow.com/questions/78372798/getting-nearly-100-accuracy-using-binary-classification-in-tensorflow-but-incre</link>
      <description><![CDATA[我正在创建一个 Chrome 扩展程序，通过 Gmail 的 API 读取用户电子邮件，然后将用户电子邮件传递给 Flask 中经过训练的 Keras 模型，以确定电子邮件是由人工智能还是人类编写的，数据集为 700双方的电子邮件，总共 1400 封。我的准确度评级显示 10 个时期的准确度不低于 92%，但是，当我逐字传递数据集中的电子邮件（例如人工编写的电子邮件）时，我得到的预测评级为 99%，表明该电子邮件是由 AI 编写的，反之亦然。我是 NLP 和 ML 的新手，所以我想知道我处理分类的方式是否是问题所在。
fromflask导入Flask，request，jsonify
导入操作系统
导入全局
导入html
将 numpy 导入为 np
从tensorflow.keras.models导入顺序，load_model
从tensorflow.keras.layers导入嵌入、SpatialDropout1D、LSTM、密集
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.preprocessing.sequence导入pad_sequences
从 sklearn.model_selection 导入 train_test_split


应用程序=烧瓶（__名称__）

def train_aiVS human_email():
    最大文件数 = 700
    root_dir = os.path.dirname(os.path.abspath(__file__))
    folder_path_ai = os.path.join(root_dir, &#39;数据/AI_Phising 电子邮件数据集过滤&#39;)
    folder_path_ human = os.path.join(root_dir, &#39;数据/安然数据集过滤&#39;)
 
    AI_files = glob.glob(os.path.join(folder_path_ai, &#39;*.txt&#39;))[:max_files]
    test_files = glob.glob(os.path.join(folder_path_ human, &#39;*.txt&#39;))[:max_files]

    文本=[]
    标签=[]

    对于 AI_files 中的文件：
        将 open(file, &#39;r&#39;) 作为 f：
            文本.append(f.read())
        labels.append(1) # 正类

    对于 Test_files 中的文件：
        将 open(file, &#39;r&#39;) 作为 f：
            文本.append(f.read())
        labels.append(0) # 负类

    max_words = 1000 # 标记化的最大单词数
    分词器 = 分词器(num_words=max_words)
    tokenizer.fit_on_texts(文本)
    序列 = tokenizer.texts_to_sequences(texts)
    padd_sequences = pad_sequences(序列)

    X = np.array（填充序列）
    y = np.array(标签)

    嵌入尺寸 = 100
    max_len = X.shape[1] # 基于填充序列的序列最大长度

    模型=顺序（）
    model.add（嵌入（input_dim = max_words，output_dim = embedding_dim））
    model.add(SpatialDropout1D(0.2))
    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model.add（密集（1，激活=&#39;sigmoid&#39;））

    model.compile（损失=&#39;binary_crossentropy&#39;，优化器=&#39;adam&#39;，指标=[&#39;准确性&#39;]）

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)

    model.fit（X_train，y_train，batch_size = 32，epochs = 10，validation_data =（X_test，y_test），详细= 2）

    # 保存模型
    model.save(&#39;Keras Models/aiVS human.keras&#39;)
    print(&quot;aiVSHuman模型保存成功&quot;)
    返回模型



#使用train_aiVS human_email函数训练模型，将其保存到keras。大约需要10分钟，所以不要每次都跑
#模型=train_aiVSHuman_email()

#获取保存的模型，并将其用于预测
模型 = load_model(&#39;Keras Models/aiVS human.keras&#39;)

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    #train_aiVSHuman_email()

    分词器 = 分词器(num_words=1000)
    数据 = 请求.json
    消息 = 数据[&#39;消息&#39;]
    #打印（消息）

    #去掉 HTML 数字并将其转换回字符
    消息 = html.unescape(消息)
    消息 = [消息]
    tokenizer.fit_on_texts（消息）


    # 标记并填充输入消息
    序列 = tokenizer.texts_to_sequences(消息)
    padd_sequence = pad_sequences(序列, maxlen=10000, padding=&#39;post&#39;)
    
    #sample_message = [“示例测试消息”]
    #tokenizer.fit_on_texts(sample_message)

    #sample_sequence = tokenizer.texts_to_sequences(sample_message)
    打印（填充序列）
    #print(“\n”)
    #sample_padded_sequence = pad_sequences(sample_sequence, maxlen=10000,padding=&#39;post&#39;)
    ##print（样本填充序列）
    

    ＃ 做出预测
    预测 = model.predict(padded_sequence)
    is_ai_message = 预测[0][0]&gt; 0.4 # 假设0.4为阈值

    #response = {&#39;is_ai_message&#39;: is_ai_message}
    如果（is_ai_message）：
        return jsonify({“status”: “收到消息”, “printed_message”: f“AI 是否用预测{prediction[0][0]} 为电子邮件编写了消息：{message}”})
    别的：
        return jsonify({“status”: “收到消息”, “printed_message”: f“是带有预测{预测[0][0]}的人类书面消息，用于电子邮件：{message}”})
    

]]></description>
      <guid>https://stackoverflow.com/questions/78372798/getting-nearly-100-accuracy-using-binary-classification-in-tensorflow-but-incre</guid>
      <pubDate>Tue, 23 Apr 2024 13:35:45 GMT</pubDate>
    </item>
    <item>
      <title>非常小的数据集的需求预测</title>
      <link>https://stackoverflow.com/questions/78372549/demand-forecasting-on-very-small-dataset</link>
      <description><![CDATA[我想使用非常有限的数据集创建需求预测模型。该数据集由仅八行的年度数据组成（8 年数据，每年 1 行）。它是一个多变量数据，其中需求取决于多个参数。
我的目标是预测第九年的需求。
数据集包括年份、总可用性、库存、到期需求、分布和需求等参数。
需求是使用以下公式手动计算的：
需求 = 总可用性 -（库存 + 到期需求 + 已分配）。
请任何人建议合适的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78372549/demand-forecasting-on-very-small-dataset</guid>
      <pubDate>Tue, 23 Apr 2024 12:58:28 GMT</pubDate>
    </item>
    <item>
      <title>在 Pytorch 中冻结现有模型的层</title>
      <link>https://stackoverflow.com/questions/78372187/freeze-layers-of-existing-model-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78372187/freeze-layers-of-existing-model-in-pytorch</guid>
      <pubDate>Tue, 23 Apr 2024 12:00:09 GMT</pubDate>
    </item>
    <item>
      <title>修复从头构建的 LSTM 模型的训练函数</title>
      <link>https://stackoverflow.com/questions/78371853/fixing-the-training-function-for-an-lstm-model-built-from-scratch</link>
      <description><![CDATA[所以，我目前正在尝试构建一个定制的 LSTM 模型。为此，我尝试首先从头开始构建自定义 LSTM，然后慢慢修改它。然而，现在我很难让模型学习。
这是我的 LSTM 函数的代码。
类 LSTM_Model(nn.Module):
    def __init__(自身、input_sz、hidden_​​sz、out_sz、bs)：
        超级().__init__()
        self.input_sz = input_sz
        self.hidden_​​size = 隐藏_sz
        self.W = nn.Parameter(torch.Tensor(input_sz,hidden_​​sz * 4))
        self.U = nn.Parameter(torch.Tensor(hidden_​​sz,hidden_​​sz * 4))
        self.bias = nn.Parameter(torch.Tensor(hidden_​​sz * 4))
        self.fc1 = nn.Linear(hidden_​​sz, out_sz)
        self.init_weights()

    def init_weights(自身):
        stdv = 1.0 / math.sqrt(self.hidden_​​size)
        self.parameters() 中的权重：
            重量.data.uniform_(-stdv, stdv)

    defforward(self, x, init_states=None):
        bs, seq_sz, _ = x.size()
        隐藏序列 = []
        如果 init_states 为 None：
            h_t, c_t = (torch.zeros(bs, self.hidden_​​size).to(x.device),
                        torch.zeros(bs, self.hidden_​​size).to(x.device))
        别的：
            h_t, c_t = 初始化状态

        对于范围内的 t(seq_sz)：
            x_t = x[:, t, :]
            如果 x_t.size()[0] != bs:
                fill_in = torch.zeros(bs-x_t[0], seq_sz)
                x_t - torch.cat((x_t, fill_in), 0)

            门 = torch.matmul(x_t, self.W) + torch.matmul(h_t, self.U) + self.bias
            i_t, f_t, g_t, o_t = Gates.chunk(4, 1)
            i_t, f_t, g_t, o_t = torch.sigmoid(i_t), torch.sigmoid(f_t), torch.tanh(g_t), torch.sigmoid(i_t)
            c_t = f_t * c_t + i_t * g_t
            h_t = o_t * torch.tanh(c_t)
            hide_seq.append(h_t)
        输出 = self.fc1(h_t)
        返回，(h_t, c_t)

这是我的训练函数。
def train_model(data_loader、模型、loss_function、优化器)：
    num_batches = len(data_loader)
    总损失= 0
    模型.train()
    隐藏 = 无
    #torch.autograd.set_detect_anomaly(True)
    对于 data_loader 中的 X、y：
        输出，隐藏 = 模型（X，隐藏）
        损失=损失函数（输出，y）

        优化器.zero_grad()
        loss.backward()
        #loss.backward(retain_graph=True)
        优化器.step()

        总损失 += loss.item()

    avg_loss = 总损失 / num_batches
    print(f&quot;列车损失：{avg_loss}&quot;)
    返回 avg_loss

运行它会收到一条初始错误消息，这要求我放置retain_graph(True)，但之后我收到更多错误。 （还要澄清数据是按这个顺序[批次、序列、特征]）
我还注意到，删除训练函数的隐藏变量，程序会给出输出，但模型根本不学习，可能是因为它抛弃了我认为的隐藏状态。
我尝试做一些研究，但一切似乎都进入了死胡同。从训练函数中删除隐藏状态部分会使程序运行，但它不再学习任何内容并像静态线一样输出（我正在根据股票市场数据训练我的模型）。然而，解决这些问题会让我遇到各种不同的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78371853/fixing-the-training-function-for-an-lstm-model-built-from-scratch</guid>
      <pubDate>Tue, 23 Apr 2024 11:07:26 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：L2.__init__() 得到意外的关键字参数“l”</title>
      <link>https://stackoverflow.com/questions/78371566/typeerror-l2-init-got-an-unexpected-keyword-argument-l</link>
      <description><![CDATA[img_size = (224, 224)

通道 = 3

img_shape = (img_size[0], img_size[1], 通道)

class_count = len(列表(train_gen.class_indices.keys()))

base_model = tf.keras.applications.efficientnet.EfficientNetB7(include_top=False,weights=“imagenet”,input_shape=img_shape,pooling=&#39;max&#39;)

base_model.trainable = False

模型=顺序（[
    基本模型，
    BatchNormalization（轴=-1，动量=0.99，epsilon=0.001），
    密集（128，kernel_regularizer=regularizers.l2（l=0.016），activity_regularizer=regularizers.l1（0.006），
          bias_regularizer=regularizers.l1(0.006), 激活=&#39;relu&#39;),
    Dropout（率=0.45，种子=123），
    密集（class_count，激活=&#39;softmax&#39;）
]）

&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
TypeError Traceback（最近一次调用最后一次）
[70] 中的单元格，第 16 行
      9 base_model = tf.keras.applications.efficientnet.EfficientNetB7(include_top=False,weights=“imagenet”,input_shape=img_shape,pooling=&#39;max&#39;)
     11 基础模型.trainable = False
     13 模型 = 顺序（[
     14 基本模型，
     15 BatchNormalization（轴=-1，动量=0.99，epsilon=0.001），
---&gt; 16 密集（128，kernel_regularizer=regularizers.l2（l=0.016），activity_regularizer=regularizers.l1（0.006），
     17bias_regularizer=regularizers.l1(0.006), 激活=&#39;relu&#39;),
     18 Dropout（比率=0.45，种子=123），
     19 密集（class_count，激活=&#39;softmax&#39;）
     20]）
     22 model.compile(Adamax(learning_rate=0.002),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
     25 模型.summary()

类型错误：L2.__init__() 得到意外的关键字参数“l”
]]></description>
      <guid>https://stackoverflow.com/questions/78371566/typeerror-l2-init-got-an-unexpected-keyword-argument-l</guid>
      <pubDate>Tue, 23 Apr 2024 10:17:26 GMT</pubDate>
    </item>
    <item>
      <title>使用Python绘制糖尿病视网膜病变检测图[关闭]</title>
      <link>https://stackoverflow.com/questions/78365786/mapping-in-a-diabetic-retinopathy-detection-using-python</link>
      <description><![CDATA[我目前正在开发一个使用 Inception V3 预训练模型进行糖尿病视网膜病变分类的 Python 程序，然后将其传递到 SVM 和随机森林分类器。快速分解该项目：

对 5 种类型（0、1、2、3、4）的眼睛图像进行分类，其中 0 表示没有患病，4 表示严重。
在这个特定的程序中，我有意将 0,1 图像映射到值 10，将 2,3,4 映射到值 11，将其变成二元分类器。
然后构建模型，然后正确训练和执行。

为了提供有关我的代码工作的更多详细信息，我想显示图像的特定映射。
我建了两个表：
表 1：它有三列，第一列是从 xero 开始的所有图像的编号，第二列是与图像对应的唯一标识符，第三列是类的原始映射 0、1、2 ,3,4。
表 2：前两列相同，但第三列现在显示映射的 ID（10 或 11）
我想从第二个表中随机获取行及其映射的 id，然后比较第一个表中的值以获得原始 id，这样我就可以看到实际和预测的混淆矩阵。
一些有帮助的图像是：
图片 1
图片_2 图片 3
图片 4
我尝试实现上述问题，但是 2,3,4 的映射没有出现，只有 0,1 的映射出现。这是我迄今为止正在使用的代码片段：
# 从训练数据中随机选择 916 行及其对应的 ID 代码
    selected_indices = np.random.choice(len(x_train), 916, 替换=False)
    x_train2 = x_train[选定的索引]
    y_train2_ids = [labels_df.iloc[i][&#39;id_code&#39;] for i in selected_indices]

# 查找测试数据中与x_train2中的ID代码匹配的索引
    匹配索引 = []
    对于 y_train2_ids 中的 id_code：
        索引 = labels_df[labels_df[&#39;id_code&#39;] == id_code].index
        如果 len(索引) &gt; 0:
            matching_indices.append(索引[0])
        别的：
            print(f&quot;未找到 ID 代码的匹配索引：{id_code}&quot;)

# 将对应的标签存储在y_a中
    y_a = y_train_encoded[匹配索引]
# 根据唯一类动态初始化每个组合的计数
    组合计数 = {}
    对于 unique_classes 中的 true_class：
        对于 unique_classes 中的 Predicted_class：
            key = f&#39;True Positive ({true_class} -&gt; {predicted_class})&#39;
            组合计数[键] = 0

# 迭代每对真实标签（y_a）和预测标签（predictions）
    对于 zip(y_a, Predictions.round()) 中的 true_label、predicted_label：
    # 将预测标签转换为整数
        预测标签 = int(预测标签[0])
        key = f&#39;True Positive ({true_label} -&gt; {predicted_label})&#39;
        组合计数[键] += 1

# 打印结果的表格结构
    print(“真实标签与预测标签的比较”)
    print(&quot;{:&lt;25} {:&lt;20}&quot;.format(&quot;组合&quot;, &quot;计数&quot;))
    打印（“-”* 45）
    对于组合，在combination_counts.items()中计数：
        print(&quot;{:&lt;25} {:&lt;20}&quot;.format(组合, 计数))

当前输出是：
真实标签和预测标签的比较
组合数
--------------------------------------------------------
真阳性 (0 -&gt; 0) 325
真阳性 (0 -&gt; 1) 229
真阳性 (1 -&gt; 0) 250
真阳性 (1 -&gt; 1) 112

我也想获取 0,1,2,3,4 的值]]></description>
      <guid>https://stackoverflow.com/questions/78365786/mapping-in-a-diabetic-retinopathy-detection-using-python</guid>
      <pubDate>Mon, 22 Apr 2024 11:05:03 GMT</pubDate>
    </item>
    <item>
      <title>我在使用 djangorestframework 创建 REST API 来部署图像分类模型时遇到错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78365482/i-encounter-an-error-when-creating-a-rest-api-with-djangorestframework-to-deploy</link>
      <description><![CDATA[HTTP 500 内部服务器错误
允许：发布、选项
内容类型：application/json
变化：接受
{
“错误”：“找不到文件：filepath=saved_models/model.keras。请确保该文件是可访问的 .keras zip 文件。”
}
视图页面的脚本是：
&#39;&#39;&#39;Python
从rest_framework.views导入APIView
从rest_framework.response导入响应
从rest_framework导入状态
将张量流导入为 tf
从tensorflow.keras.models导入load_model
从tensorflow.keras.preprocessing导入图像
从张量流导入keras
从 process.models 导入 ImageModel
from process.serializers import ImageSerializer #ImageModelSerializer, ImagePredictionSerializer
将 numpy 导入为 np
从 io 导入 BytesIO
从 PIL 导入图像
从tensorflow.keras.applications.imagenet_utils导入preprocess_input
从rest_framework.exceptions导入ParseError
类 ImagePredictionAPIView(APIView):
def post(self, request, *args, **kwargs):
序列化器 = ImageSerializer(data=request.data)
如果序列化器.is_valid():
image_file = serializer.validated_data.get(&#39;image&#39;)
如果不是图像文件：
raise ParseError(“没有提交文件。”)
&lt;前&gt;&lt;代码&gt;尝试：
            model = load_model(“saved_models/model.keras”)
            img = image.load_img(image_file, target_size=(img_width, img_height))
            img_array = image.img_to_array(img)
            img_array = np.expand_dims(img_array, 轴=0)
            img_array = 预处理_输入(img_array)
            预测 = model.predict(img_array)
            预测标签=“好”；如果预测[0][0]&gt; 0.5 否则“不好”
            返回响应（{&#39;预测&#39;：预测_标签}，状态=状态.HTTP_200_OK）
        除了文件未找到错误：
            return Response({&#39;error&#39;: &quot;Le modèle spécifié n&#39;a pas été trouvé.&quot;}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        除了异常 e：
            返回响应（{&#39;错误&#39;：str（e）}，状态= status.HTTP_500_INTERNAL_SERVER_ERROR）
    别的：
        返回响应（序列化器.错误，状态=状态.HTTP_400_BAD_REQUEST）

&#39;&#39;&#39;
序列化器页面脚本是：
&#39;&#39;&#39; 从rest_framework导入序列化器
导入uuid
导入base64
从 django.core.files.base 导入 ContentFile
从 process.models 导入 ImageProcess
类 Base64ImageField(serializers.ImageField):
def to_internal_value(自身, 数据):
if isinstance(data, str) 和 data.startswith(&#39;data:image&#39;):
# Base64 编码图像 - 解码
format, imgstr = data.split(&#39;;base64,&#39;) # 格式 ~= data:image/X,
ext = format.split(&#39;/&#39;)[-1] # 猜测文件扩展名
id = uuid.uuid4()
数据 = ContentFile(base64.b64decode(imgstr), name=id.urn[9:] + &#39;.&#39; + ext)
返回 super(Base64ImageField, self).to_internal_value(data)
类 ImageSerializer(serializers.ModelSerializer):
图像 = Base64ImageField()
类元：
模型 = 图像处理
字段=（&#39;pk&#39;，&#39;图像&#39;）
# read_only_fields = (&#39;照片&#39;,) &#39;
&#39;&#39;&#39;
网址脚本是：
&#39;&#39;&#39;
从 django.urls 导入路径
从 。导入视图
urlpatterns = [
路径(&#39;api/upload/&#39;,views.ImagePredictionAPIView.as_view(),name=&#39;image-upload&#39;),
]
&#39;&#39;&#39;
模型页面脚本是：
&#39;&#39;&#39;
从 django.db 导入模型
类 ImageModel(models.Model):
图像 = models.ImageField(upload_to=&#39;images/&#39;)
&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78365482/i-encounter-an-error-when-creating-a-rest-api-with-djangorestframework-to-deploy</guid>
      <pubDate>Mon, 22 Apr 2024 10:15:50 GMT</pubDate>
    </item>
    <item>
      <title>关于Keras历史回调损失与控制台输出损失不匹配的调查</title>
      <link>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</link>
      <description><![CDATA[请问，有谁知道为什么这个问题中描述了这个问题（Keras历史回调损失与损失的控制台输出不匹配）会发生吗？这个问题只有一个答案，它指的是可能的 TensorFlow 版本错误，但我不相信这一点，特别是因为 OP 没有对答案发表评论。我也遇到了这种情况，使用 Keras 指南中的 LossAndErrorPrintingCallback(keras.callbacks.Callback) 类和 def function on_epoch_end(self, epoch, logs=None) 函数 &lt; a href=&quot;https://keras.io/guides/writing_your_own_callbacks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://keras.io/guides/writing_your_own_callbacks/。我还测试了 CSVLogger Keras 回调的使用，并且得到的结果与 model.fit() 输出中显示的结果不同。我使用的是 TensorFlow 2.4.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</guid>
      <pubDate>Sun, 21 Apr 2024 02:09:48 GMT</pubDate>
    </item>
    <item>
      <title>当我一次读取 3 个或更多 rtsp 流时，OpenCV VideoCapture 出现灰屏</title>
      <link>https://stackoverflow.com/questions/76736531/opencv-videocapture-gives-me-a-gray-screen-when-im-reading-from-3-or-more-rtsp</link>
      <description><![CDATA[我正在 LAN 网络上读取 rtsp 流。当它是单个或 2 个流时，它运行平稳，但当我在 2 个以上的流上尝试它时，它开始给我一个灰色的屏幕，其中几乎没有可见的像素。
cap = cv2.VideoCapture(rtsp_uri)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)

fps = cap.get(cv2.CAP_PROP_FPS)
每帧后延迟 = 1/fps
打印（每个帧后延迟）

而真实：
    尝试：
        ret, 框架 = cap.read()
    除了：
        经过

    如果 self.stopper[camera_id]:
        del self.stopper[camera_id]
        返回

    如果不转：
        继续

    帧 = cv2.cvtColor(src=frame, 代码=cv2.COLOR_BGR2RGB)
    框架= cv2.调整大小（
        框架，
        (self.config.frame_width, self.config.frame_height)
    ）
    队列.入队（帧）
    time.sleep(delay_after_each_frame)

这里我使用队列来存储帧，并且该脚本在线程中运行。
有什么方法可以让我一次从多个摄像机读取流而没有那些灰色帧。
这是我得到的框架。

用于多个流和输出的代码。
self.config.get_camera_ids() 中的camera_id：
    如果camera_id不在self.threads中：
        self.threads[camera_id] = 线程(
            目标=self.process_rtsp_stream，
            args=(self.config.get_rtsp(camera_id),
                    self.queues[camera_id],camera_id)
        ）
        self.threads[camera_id].daemon = True
        self.threads[camera_id].start()

# 删除已删除相机的线程
对于 self.threads 中的camera_id：
    如果camera_id不在self.config.get_camera_ids()中：
        del self.threads[camera_id]
]]></description>
      <guid>https://stackoverflow.com/questions/76736531/opencv-videocapture-gives-me-a-gray-screen-when-im-reading-from-3-or-more-rtsp</guid>
      <pubDate>Fri, 21 Jul 2023 08:58:28 GMT</pubDate>
    </item>
    <item>
      <title>ConnectionAbortedError：W&B 超参数搜索期间 [WinError 10053]</title>
      <link>https://stackoverflow.com/questions/76143500/connectionabortederror-winerror-10053-during-wb-hyperparameter-search</link>
      <description><![CDATA[我正在尝试使用权重和参数进行超参数搜索。偏见。尝试从文档运行此代码 https://笔记本中的 /docs.wandb.ai/guides/sweeps/add-w-and-b-to-your-code：
导入wandb
将 numpy 导入为 np
随机导入

# 定义扫描配置
扫描配置= {
    &#39;方法&#39;：&#39;随机&#39;，
    &#39;名称&#39;: &#39;扫一扫&#39;,
    &#39;metric&#39;: {&#39;goal&#39;: &#39;最大化&#39;, &#39;name&#39;: &#39;val_acc&#39;},
    &#39;参数&#39;：
    {
        &#39;batch_size&#39;: {&#39;values&#39;: [16, 32, 64]},
        &#39;纪元&#39;: {&#39;值&#39;: [5, 10, 15]},
        &#39;lr&#39;: {&#39;最大&#39;: 0.1, &#39;最小&#39;: 0.0001}
    }
 }

 # 通过传入config来初始化sweep。
 # （可选）提供项目名称。
 scan_id = wandb.sweep(
 扫描=扫描配置，
     项目=&#39;我的第一次扫描&#39;
 ）

 # 定义接受超参数的训练函数
 # 来自 `wandb.config` 的值并使用它们来训练
 # 模型和返回指标
 def train_one_epoch(epoch, lr, bs):
     acc = 0.25 + ((epoch/30) + (random.random()/10))
     损失 = 0.2 + (1 - ((epoch-1)/10 + random.random()/5))
     返回 acc, 损失

 defvaluate_one_epoch（纪元）：
     acc = 0.1 + ((epoch/20) + (random.random()/10))
     损失 = 0.25 + (1 - ((epoch-1)/10 + random.random()/6))
     返回 acc, 损失

 def main():
     运行=wandb.init()

     # 请注意，我们从 `wandb.config` 定义值
     # 而不是定义硬值
     lr = wandb.config.lr
     bs = wandb.config.batch_size
     纪元 = wandb.config.epochs

     对于 np.arange(1, epochs) 中的纪元：
         train_acc, train_loss = train_one_epoch(epoch, lr, bs)
         val_acc, val_loss = evaluate_one_epoch(epoch)

     wandb.log({
         &#39;纪元&#39;：纪元，
         &#39;train_acc&#39;：train_acc，
         &#39;火车损失&#39;：火车损失，
         &#39;val_acc&#39;: val_acc,
         &#39;val_loss&#39;：val_loss
     })

# 开始清扫工作。
wandb.agent(sweep_id, function=main, count=4)

出现错误ConnectionAbortedError：[WinError 10053]已建立的连接被主机中的软件中止。如何修复？当我在 Colab 中运行它时，一切正常。]]></description>
      <guid>https://stackoverflow.com/questions/76143500/connectionabortederror-winerror-10053-during-wb-hyperparameter-search</guid>
      <pubDate>Sun, 30 Apr 2023 20:54:46 GMT</pubDate>
    </item>
    <item>
      <title>Flutter TFLite 错误：“metal_delegate.h”文件未找到</title>
      <link>https://stackoverflow.com/questions/63139273/flutter-tflite-error-metal-delegate-h-file-not-found</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/63139273/flutter-tflite-error-metal-delegate-h-file-not-found</guid>
      <pubDate>Tue, 28 Jul 2020 17:05:02 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB 中的自组织映射</title>
      <link>https://stackoverflow.com/questions/40830533/self-organizing-maps-in-matlab</link>
      <description><![CDATA[我正在尝试使用余弦而不是欧几里德距离对一些似乎可分离的数据进行聚类。如何使用 MATLAB 的 selforgmap 来实现此目的？我不相信这是通过“distanceFcn”选项实现的。 

&lt;块引用&gt;
  x = simplecluster_dataset；
  net = selforgmap([8 8],100,3,&#39;hextop&#39;,&#39;余弦&#39;);
  净=火车（净，x）；
  视图（净） y = 净（x）；
  类 = vec2ind(y);
]]></description>
      <guid>https://stackoverflow.com/questions/40830533/self-organizing-maps-in-matlab</guid>
      <pubDate>Sun, 27 Nov 2016 15:23:20 GMT</pubDate>
    </item>
    </channel>
</rss>