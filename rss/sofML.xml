<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 26 Aug 2024 06:23:02 GMT</lastBuildDate>
    <item>
      <title>如何平衡高度不平衡的多标签文本数据集以进行 BERT 训练？</title>
      <link>https://stackoverflow.com/questions/78912925/how-to-balance-a-highly-imbalanced-multi-label-text-dataset-for-bert-training</link>
      <description><![CDATA[我有一个包含两列和 11,000 行的 CSV 数据集。第 1 列是“FileContent”，其中包含使用 OCR 从 PDF 中提取的文本数据（通常每个条目超过 500 个单词）。第 2 列是“Category”，它有 70 个唯一类别，代表多标签数据。以下是类别：
[&#39;FED&#39;, &#39;IPD&#39;, &#39;ILRF&#39;, &#39;COMP&#39;, &#39;BSI&#39;, &#39;CPTR&#39;, &#39;MARK&#39;, &#39;BANKD&#39;, &#39;CFPB&#39;, &#39;BF&#39;, &#39;BUY&#39;,&#39;Health&#39;, &#39;ENV&#39;, &#39;PEN&#39;, &#39;WSBP&#39;, &#39;BENE&#39;, &#39;SAFE&#39;, &#39;ADLG&#39;, &#39;TELEM&#39;, &#39;FPLG&#39;, &#39;UDPA&#39;, &#39;BTR&#39;, &#39;GC&#39;, &#39;CACT&#39;, &#39;AVI&#39;, &#39;ELD&#39;, &#39;EPG&#39;, &#39;COS&#39;, &#39;SEC&#39;, &#39;TSA&#39;, &#39;ADD&#39;, &#39;FMLG&#39;, &#39;LOR&#39;, &#39;RICO&#39;, &#39;ENT&#39;, &#39;RITE&#39;, &#39;WH&#39;, &#39;PAY&#39;, &#39;PSAF&#39;, &#39;PRL&#39;, &#39;ERG&#39;, &#39;STO&#39;, &#39;Life Sciences&#39;, &#39;FRAN&#39;, &#39;PLIM&#39;, &#39;LEND&#39;, &#39;AUTO&#39;, &#39;TRA&#39;, &#39;BCY&#39;, &#39;ILRL&#39;, &#39;UFS&#39;, &#39;SBS&#39;, &#39;FUND&#39;, &#39;WSBS&#39;, &#39;TLHN&#39;, &#39;SS&#39;, &#39;CAR&#39;, &#39;FLR&#39;, &#39;WSBE&#39;, &#39;LIQ&#39;, &#39;ESGD&#39;, &#39;UI&#39;, &#39;CAM&#39;, &#39;X&#39;, &#39;ETR&#39;, &#39;IRA&#39;, &#39;NUC&#39;, &#39;COBRA&#39;, &#39;Correction&#39;, &#39;FINH&#39;]


类别以多标签格式组合，如下所示：
&#39;IPD,CPTR,MARK&#39;
&#39;BANKD,CFPB,BF,BUY,COMP,BSI&#39;

我面临的问题是我的数据集高度不平衡。以下是不同类别的一些示例计数：
WH,PAY,ELD,BANKD,COMP,BSI,BUY 1
COS,IPD,MARK,ERG 1
CPTR,PRL,PSAF,Life Sciences,UDPA 1
FPLG,Health,UDPA,ADLG 1
TSA,CPTR,IPD 1
...
BSI,COMP 203
COMP,BSI 205
EPG,ELD 323
ELD,EPG 351
ILRF 631

我的目标是训练一个 BERT 模型，根据 FileContent 预测正确的类别。但是，由于数据的不平衡性质，我担心模型的性能。
使用 BERT 进行训练时，平衡高度不平衡的多标签文本数据集的最佳实践是什么？我应该使用 SMOTE 之类的技术还是任何其他专门用于多标签数据集的方法？
我正在寻找一种明确的方法或程序来平衡这种高度不平衡的多标签文本数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78912925/how-to-balance-a-highly-imbalanced-multi-label-text-dataset-for-bert-training</guid>
      <pubDate>Mon, 26 Aug 2024 05:06:42 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以高效且经济地微调 7B 参数 LLM？[关闭]</title>
      <link>https://stackoverflow.com/questions/78912744/where-can-i-fine-tune-a-7b-parameter-llm-efficiently-and-affordably</link>
      <description><![CDATA[我需要在 4 个数据集上运行大约 5 种不同的微调方法，每个数据集有 1000-1500 个样本。
我需要微调的模型是 Mistral7B。
我有一个基本的 Google Colab 订阅，但它不允许访问 A100 GPU，并且可用的 T4 GPU 缺乏足够的内存来微调 7B 参数模型——它很快就会耗尽内存。即使进行了量化和混合精度等优化，我仍然会达到内存限制。
我正在寻找具有足够 GPU 内存的经济高效的平台或服务来高效处理这些任务。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78912744/where-can-i-fine-tune-a-7b-parameter-llm-efficiently-and-affordably</guid>
      <pubDate>Mon, 26 Aug 2024 03:01:28 GMT</pubDate>
    </item>
    <item>
      <title>一个 scikit ML 示例中的问题，“使用多项逻辑 + L1 进行 MNIST 分类”</title>
      <link>https://stackoverflow.com/questions/78912616/question-in-one-scikit-ml-example-mnist-classification-using-multinomial-logis</link>
      <description><![CDATA[https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html
这个例子我有两个问题：
Q1：理论上应该是数据越多，规则越少。但是“C=50.0 / train_samples”会导致数据越多，规则越强。是代码的缺陷，还是我的误解？
clf = LogisticRegression(C=50.0 / train_samples, penalty=&quot;l1&quot;,solver=&quot;saga&quot;, tol=0.1)
Q2：为什么只用 coef 矩阵就能画出最终的数字图像，这背后的想法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78912616/question-in-one-scikit-ml-example-mnist-classification-using-multinomial-logis</guid>
      <pubDate>Mon, 26 Aug 2024 01:44:53 GMT</pubDate>
    </item>
    <item>
      <title>Dask 在 GridSearchCV 和 RandomizedSearchCV 上犯了错误</title>
      <link>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</link>
      <description><![CDATA[我正在尝试使用 dask 训练 xgboost 模型。我已经转换了数据并准备了如下数据：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

我可以像这样对数据进行简单的模型拟合：
model = dxgb.DaskXGBRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)

但是当我尝试任何更复杂的事情时。就像这样使用 dask 的 gridsearchcv：
param_grid = {
&#39;max_depth&#39;: [3, 8],
&#39;learning_rate&#39;: [0.01, 0.1]}

grid_search = GridSearchCV(
model, param_grid}

grid_search.fit(X_train, y_train)

我收到以下警告：
警告：dask_ml.model_selection._search:(&#39;daskxgbregressor-fit-score-f6830e79aeb606b3eed291ac24184a8c&#39;, 1, 1) 失败...正在重试

任务图中的所有内容都因错误而变黑。
有人知道如何解决吗这个？]]></description>
      <guid>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</guid>
      <pubDate>Mon, 26 Aug 2024 01:04:55 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8 推理在 Mac 上运行良好，但在 Windows 上不运行</title>
      <link>https://stackoverflow.com/questions/78911914/yolov8-inference-working-on-mac-but-not-windows</link>
      <description><![CDATA[我在 pycharm 中使用 ultralytics 中的 yolo v8 对我训练的模型进行推理，当我在 macbook 上运行它时，它工作正常，但在我的 windows 笔记本电脑上，我得到大量边界框，置信度得分为 1，即使我使用的是相同的 best.pt 文件和相同的代码：
正在发生的事情的示例
从 ultralytics 导入 YOLO
从 ultralytics.utils.benchmarks 导入基准

model = YOLO(&quot;best.pt&quot;)
# model = YOLO(&quot;yolov8m.pt&quot;)

results = model(source=0, show=True, conf=0.6, save=True)
# results = model.track(source=0, show=True, conf=0.6, tracker=&quot;bytetrack.yaml&quot;)

我之前也遇到过类似问题（不完全一样），与下面链接的问题类似，pycharm 无法检测到 pytorch，每次导入它时都会出错：
import torch：如何修复 OSError WinError 126，加载 fbgemm.dll 或依赖项时出错
我使用用户提供的 libomp140.x86_64.dll 文件并将其放在 C:\Windows\System32 中修复了它，这可能是问题所在吗？
我已经尝试过的方法：
我尝试重新安装ultralytics 和 pytorch 库以及 pycharm 认为这是他们的安装问题，但什么都没有改变。]]></description>
      <guid>https://stackoverflow.com/questions/78911914/yolov8-inference-working-on-mac-but-not-windows</guid>
      <pubDate>Sun, 25 Aug 2024 18:16:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在决策树中使用直方图实现分箱条件？</title>
      <link>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</guid>
      <pubDate>Sun, 25 Aug 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>检测硬币图像的旋转角度</title>
      <link>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</link>
      <description><![CDATA[确定硬币图像角度的最佳方法是什么？
图像的分辨率是固定的（400x400），但可能在任何方向上偏离中心几个像素。
图像可以有不同的颜色、光照、旋转等。


我尝试过各种 CNN 想法，但都没有成功。看来，旋转后的图像之间的相似性不足以增强效果。
我无法找到用于此目的的现有数据集
请给我一些已被证明可行的想法，而不是理论]]></description>
      <guid>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</guid>
      <pubDate>Sun, 25 Aug 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>利用外推训练的随机森林模型来创建使用不同卫星的时间序列</title>
      <link>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</link>
      <description><![CDATA[我正在尝试创建 1990 年至 2023 年的 LULC 时间序列。为了生成我的时间序列，我选择了四个使用不同卫星（Landsat 5、7、8 和 Sentinel-1 和 -2）的日期。我只有 2023 年的训练数据。目前我只是使用历史数据中使用的相同 2023 年点重新训练模型。我觉得这会导致我的分类出现一些不一致。我希望能够在所有日期运行我训练过的分类器，但存在一些严重的波段不一致。有没有办法让模型适应这些波段变化？
不一致的原因不仅在于卫星之间的波段差异，还在于我对不同年份的预处理步骤，即使用全色波段对 Landsat 数据进行全色锐化，以及对整个 sentinel-1（不收集 RBG）进行锐化。
我正在使用 Google Earth Engine API（JavaScript）
在删除几个波段后，我尝试对 sentinel 和 landsat8 使用相同的模型（在 2023 年 sentinel 数据上进行训练），但是我的 landsat8 数据的 LULC 地图完全错误（在很大程度上，基于我的视觉评估和地面真实数据）。]]></description>
      <guid>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</guid>
      <pubDate>Thu, 18 Jul 2024 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 ranger 包进行预测时为什么不使用变量重要性设置的原因？[关闭]</title>
      <link>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</link>
      <description><![CDATA[我正在使用 R 中的 ranger 包来构建随机森林模型，当我预测新数据时，会弹出此警告：
警告消息：
在 predict.ranger(object, ...) 中：
森林是使用“impurity_corrected”变量重要性构建的。对于预测，建议构建另一个不带此重要性设置的森林。

我想知道是否有人可以解释背后的原因，是否因为预测会出错，只是需要更多的计算时间等。
我在网上搜索答案，但找不到原因。]]></description>
      <guid>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</guid>
      <pubDate>Wed, 26 Jun 2024 20:05:02 GMT</pubDate>
    </item>
    <item>
      <title>批量大小会影响模型的准确性吗？</title>
      <link>https://stackoverflow.com/questions/71211496/does-batch-size-affects-on-the-accuracy-of-a-model</link>
      <description><![CDATA[我使用 ResNet18 主干和研究建议的技术训练了 Cifar100 数据集，最终得到了一些令人惊讶的结果。我进行了两次尝试，第一次使用 640 批量大小，第二次使用 320 批量大小。其余所有超参数都保持相似。
我得到的 640 批量大小的准确度为：76.45%
我得到的 320 批量大小的准确度为：78.64%
你能告诉我为什么会发生这种情况吗？
在我看来，这只是因为协变量偏移。完成完整样本的每次迭代的分布都会影响准确性。我认为，与 640 批量大小相比，320 批量大小的分布彼此相似，这导致更高的准确性。
你能解释一下吗，有什么解决方案？]]></description>
      <guid>https://stackoverflow.com/questions/71211496/does-batch-size-affects-on-the-accuracy-of-a-model</guid>
      <pubDate>Mon, 21 Feb 2022 18:44:57 GMT</pubDate>
    </item>
    <item>
      <title>GRU 和 LSTM 哪个更快</title>
      <link>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</link>
      <description><![CDATA[我尝试在 keras 上用 GRU 和 LSTM 实现一个模型。两种实现的模型架构相同。正如我在许多博客文章中看到的，GRU 的推理时间比 LSTM 更快。但就我而言，GRU 并不更快，而且实际上比 LSTM 更慢。有人能找到原因吗？这与 Keras 中的 GRU 有什么关系吗？还是我哪里做错了。
非常感谢您的帮助...
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</guid>
      <pubDate>Mon, 27 Jan 2020 14:21:12 GMT</pubDate>
    </item>
    <item>
      <title>Adam 优化器与梯度下降</title>
      <link>https://stackoverflow.com/questions/52014332/adam-optimizer-vs-gradient-descent</link>
      <description><![CDATA[我想了解 Adam 优化器和梯度下降优化器之间的区别，以及在哪种情况下最适合使用哪一个。我正在查看 TF 网站，但如果你知道哪里可以以更好、更易于理解的方式解释这些内容，请告诉我？]]></description>
      <guid>https://stackoverflow.com/questions/52014332/adam-optimizer-vs-gradient-descent</guid>
      <pubDate>Sat, 25 Aug 2018 05:30:44 GMT</pubDate>
    </item>
    <item>
      <title>随机梯度下降（SGD）与小批量大小 1</title>
      <link>https://stackoverflow.com/questions/49570738/stochastic-gradient-descentsgd-vs-mini-batch-size-1</link>
      <description><![CDATA[随机梯度下降基本上是指批量大小 = 1 并选择随机训练行的小批量训练吗？也就是说，它与“正常”梯度下降相同，只是提供训练数据的方式有所不同？
有一件事让我感到困惑，我曾见过有人说，即使使用 SGD，您也可以提供超过 1 个数据点，并拥有更大的批量，那么这难道不就是“正常”的小批量梯度下降吗？]]></description>
      <guid>https://stackoverflow.com/questions/49570738/stochastic-gradient-descentsgd-vs-mini-batch-size-1</guid>
      <pubDate>Fri, 30 Mar 2018 08:04:43 GMT</pubDate>
    </item>
    <item>
      <title>GAN 仅需几个 epoch 即可收敛</title>
      <link>https://stackoverflow.com/questions/41874096/gan-converges-in-just-a-few-epochs</link>
      <description><![CDATA[我在 Keras 中实现了一个生成对抗网络。我的训练数据大小约为 16,000，其中每幅图像大小为 32*32。我的所有训练图像都是针对对象检测任务的 imagenet 数据集中图像的调整大小版本。我将图像矩阵直接输入网络，而没有进行中心裁剪。我使用了 AdamOptimizer，学习率为 1e-4，beta1 为 0.5，我还将 dropout 率设置为 0.1。我首先在 3000 张真实图像和 3000 张假图像上训练了判别器，它实现了 93% 的准确率。然后，我训练了 500 个 epoch，批处理大小为 32。但是，我的模型似乎只在几个 epoch（&lt;10）内收敛，并且它生成的图像很难看。 
损失函数图
生成器生成的随机样本
我想知道我的训练数据集是否太小（与 DCGAN 论文中的 30 多万相比）或我的模型配置不正确。此外，我是否应该按照 Ian Goodfellow 在原始论文中的建议，在 D 上训练 SGD 进行 k 次迭代（其中 k 很小，可能是 1），然后在 G 上使用 SGD 进行一次迭代训练？（我刚刚尝试一次训练它们）
以下是生成器的配置。
g_input = Input(shape=[100])
H = Dense(1024*4*4, init=&#39;glorot_normal&#39;)(g_input)
H = BatchNormalization(mode=2)(H)
H = Activation(&#39;relu&#39;)(H)
H = Reshape( [4, 4,1024] )(H)
H = UpSampling2D(size=( 2, 2))(H)
H = Convolution2D(512, 3, 3, border_mode=&#39;same&#39;, init=&#39;glorot_uniform&#39;)(H)
H = BatchNormalization(mode=2)(H)
H = Activation(&#39;relu&#39;)(H)
H = UpSampling2D(size=( 2, 2))(H)
H = Convolution2D(256, 3, 3, border_mode=&#39;same&#39;, init=&#39;glorot_uniform&#39;)(H)
H = BatchNormalization(mode=2)(H)
H = Activation(&#39;relu&#39;)(H)
H = UpSampling2D(size=( 2, 2))(H)
H = Convolution2D(3, 3, 3, border_mode=&#39;same&#39;, init=&#39;glorot_uniform&#39;)(H)
g_V = Activation(&#39;tanh&#39;)(H)
generator = Model(g_input,g_V)
generator.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt)
generator.summary()

下面是判别器的配置：
d_input = Input(shape=shp)
H = Convolution2D(64, 5, 5, subsample=(2, 2), border_mode = &#39;same&#39;, init=&#39;glorot_normal&#39;)(d_input)
H = LeakyReLU(0.2)(H)
#H = Dropout(dropout_rate)(H)
H = Convolution2D(128, 5, 5, subsample=(2, 2), border_mode = &#39;same&#39;, init=&#39;glorot_normal&#39;)(H)
H = BatchNormalization(mode=2)(H)
H = LeakyReLU(0.2)(H)
#H = Dropout(dropout_rate)(H)
H = Flatten()(H)
H = Dense(256, init=&#39;glorot_normal&#39;)(H)
H = LeakyReLU(0.2)(H)
d_V = Dense(2,activation=&#39;softmax&#39;)(H)
discriminator = Model(d_input,d_V)
discriminator.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=dopt)
discriminator.summary()

下面是 GAN 的整体配置：
gan_input = Input(shape=[100])
H = generator(gan_input)
gan_V = discriminator(H)
GAN = Model(gan_input, gan_V)
GAN.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt)
GAN.summary()
]]></description>
      <guid>https://stackoverflow.com/questions/41874096/gan-converges-in-just-a-few-epochs</guid>
      <pubDate>Thu, 26 Jan 2017 13:04:41 GMT</pubDate>
    </item>
    <item>
      <title>SGD（随机梯度下降）与反向传播</title>
      <link>https://stackoverflow.com/questions/37953585/sgdstochastic-gradient-descent-vs-backpropagation</link>
      <description><![CDATA[您能告诉我随机梯度下降（SGD）和反向传播之间的区别吗？]]></description>
      <guid>https://stackoverflow.com/questions/37953585/sgdstochastic-gradient-descent-vs-backpropagation</guid>
      <pubDate>Tue, 21 Jun 2016 20:02:39 GMT</pubDate>
    </item>
    </channel>
</rss>