<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 29 Oct 2024 15:18:48 GMT</lastBuildDate>
    <item>
      <title>问题：ValueError：分类指标无法处理多类多输出和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/79137738/problem-valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-mu</link>
      <description><![CDATA[ValueError：分类指标无法处理多类多输出和多标签指标目标的混合
这是我第一次训练模型。
我检查了预测和 y 的 dtype 和维度是否相同，但错误仍然存​​在。
这是我的代码。
希望有人能帮我解决问题。
def train(tr_set, dv_set, model, config, device):
&#39;&#39;&#39; 多标签分类的训练函数 &#39;&#39;&#39;

n_epochs = config[&#39;n_epochs&#39;]
optimizer = getattr(torch.optim, config[&#39;optimizer&#39;])(model.parameters(), **config[&#39;optim_hparas&#39;])
best_acc = 0.0
loss_record = {&#39;train&#39;: [], &#39;dev&#39;: []}
early_stop_cnt = 0
epoch = 0
criterion = nn.BCEWithLogitsLoss()

while epoch &lt; n_epochs:
model.train()
total_correct = 0
total_samples = 0
total_train_loss = 0.0
all_labels = []
all_preds = []

for x, y in tr_set:
optimizer.zero_grad()
x = x.to(device)
y = y.float().to(device)
pred = model(x)
loss = criterion(pred, y)
loss.backward()
total_train_loss += loss.detach().cpu().item()
optimizer.step()

# 记录损失和准确度指标
loss_record[&#39;train&#39;].append(loss.detach().cpu().item())
predict = (torch.sigmoid(pred) &gt; 0.5).float()
all_labels.append(y.cpu())
all_preds.append(predicted.cpu())

correct_preds = (predicted == y).float().mean().item()
total_correct += correct_preds
total_samples += 1
.......

print(f&#39;在 {epoch} 个 epoch 后完成训练&#39;)
return best_acc, loss_record

def dev(dv_set, model, device, criterion):
&#39;&#39;&#39; 在验证集上评估 &#39;&#39;&#39;
model.eval()
total_loss = 0.0
all_labels = []
all_preds = []

with torch.no_grad():
for x, y in dv_set:
x = x.to(device)
y = y.float().to(device)
pred = model(x)
loss = criterion(pred, y)
total_loss += loss.item()

# 附加标签和预测以进行指标计算
predicted = (torch.sigmoid(pred) &gt; 0.5).float()
all_labels.append(y.cpu())
all_preds.append(predicted.cpu())

all_labels = torch.cat(all_labels).numpy()
all_preds = torch.cat(all_preds).numpy()

# 计算指标
accuracy = accuracy_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)
recall = recall_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)
f1 = f1_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)

dev_acc = accuracy

]]></description>
      <guid>https://stackoverflow.com/questions/79137738/problem-valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-mu</guid>
      <pubDate>Tue, 29 Oct 2024 14:49:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 进行多轮训练时如何控制内存增长？</title>
      <link>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</link>
      <description><![CDATA[在使用 TensorFlow 进行多轮训练时，我遇到了与内存增长相关的问题。具体来说，我有一个模型训练循环，其中我在每个循环中生成训练和评估数据，并且我的内存使用量似乎不断增长，最终导致内存不足错误。我正在尝试了解如何在这些迭代过程中有效地管理或释放内存。
在此处输入图片描述
这是我的代码的简化版本
# 定义用于训练数据的 TensorFlow 变量
for num_round in range(1, 1 + total_num_round):

train_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, epochs_t + 1)
eval_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, eval_num_batch)

# 训练和评估过程

# 删除使用的数据
del train_data, eval_data
gc.collect()

我遇到的问题面临：

每轮生成的 train_data 和 eval_data 占用大量内存，我似乎无法有效地释放这些内存，导致内存不断增长。
函数 generate_all_batch_s_path_samples 没有使用 tf.function 修饰，因为它使用线程进行并行计算，这使其与 tf.function 不兼容。

问题：

除了使用 tf.keras.backend.clear_session() 之外，还有更有效的方法来在迭代之间释放内存吗？
是否有推荐的方法来管理这种多轮训练场景中的内存增长？

任何建议、建议或代码非常感谢您的示例！在此先谢谢大家的帮助。
上下文：

我使用的是 TensorFlow 2.16.0。
数据生成过程（generate_all_batch_s_path_samples）在每一轮中都会创建新的张量用于训练和评估。

再次感谢您的支持！

我尝试了几种方法来控制内存使用情况：

使用 assign() 而不是重复定义 train_data 和 eval_data。
使用 gc.collect() 和 del train_data, eval_data 释放内存，但这些方法不起作用。


]]></description>
      <guid>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</guid>
      <pubDate>Tue, 29 Oct 2024 14:36:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在 azureml 中对多个节点执行扫描</title>
      <link>https://stackoverflow.com/questions/79137274/how-to-perform-a-sweep-on-multiple-nodes-in-azureml</link>
      <description><![CDATA[我一直尝试在 azureml 上进行分布式训练，同时使用扫描功能，但每次我尝试将 instance_count 设置为超过 1 个节点时，当我创建作业时，它只会创建 1 个节点。无论我对管道进行什么更改，它总是只在 1 个节点上运行每个试验，而不是我请求的 8 个节点。我正在运行的集群有 30 个可用节点，当我不使用扫描步骤时，管道适用于分布式训练。 PytorchDistribution 已全部设置并存在于作业中，但 instance_count 始终显示 1，即使我在管道中将其设置为 8
from azure.ai.ml import MLClient, Input
from azure.ai.ml.dsl import pipeline
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml.sweep import Uniform, MedianStoppingPolicy

@pipeline(
name=&quot;training_pipeline&quot;,
display_name=&quot;Training Pipeline&quot;,
experiment_name=&quot;test-pipeline&quot;,
default_compute=&quot;my-cluster&quot;,
)
def training_pipeline(
development_data: str,
test_data: str,
name: str,
description: str
):
preprocess = components[&quot;model_preprocess&quot;]
preprocess_node =预处理（
development_data=development_data，
test_data=test_data，
label_col=&quot;label&quot;，
tokenizer_name=&quot;distilbert/distilbert-base-uncased&quot;
)

train = components[&quot;model_train&quot;]
train_node = train（
dataset=preprocess_node，
model_name=&quot;distilbert/distilbert-base-uncased&quot;，
task=&quot;token-classification&quot;，
learning_rate=Uniform（min_value=5e-07，max_value=5e-03），
weight_decay=Uniform（min_value=0.0，max_value=1.0）
)
train_node.set_resources（
instance_type=&quot;my-cluster&quot;，
instance_count=8
)

sweep_node = train_node.sweep（
采样算法=“bayesian”，
primary_metric=“eval_overall_f1”，
goal=“maximize”，
max_total_trials=30，
max_concurrent_trials=3，
early_termination_policy=MedianStoppingPolicy（delay_evaluation=5，evaluation_interval=2），
compute=“my-cluster”，
)

compress = components[“model_compress”]
compress_node = compress(
model=sweep_node，
tokenizer_name=“distilbert/distilbert-base-uncased”
)

quantize = components[&quot;model_quantize&quot;]
quantize_node = quantize(
compressed_model=compress_node
)

register = components[&quot;model_register&quot;]
register(
model=quantize_node,
name=name,
description=description
)

如果 __name__ == &quot;__main__&quot;:
dev_input = Input(
type=AssetTypes.URI_FILE, path=&quot;azureml:train_data:1&quot;
)
test_input = Input(
type=AssetTypes.URI_FILE, path=&quot;azureml:test_data:1&quot;
)
pipeline_job = training_pipeline(
development_data=dev_input,
test_data=test_input,
name=&quot;test-model&quot;,
description=&quot;用于测试的模型&quot;
)

mlclient.jobs.create_or_update(pipeline_job)

当我运行此代码时，它使管道和扫描步骤运行良好，但每次试验只有 1 个实例
只有 1 个 instance_count]]></description>
      <guid>https://stackoverflow.com/questions/79137274/how-to-perform-a-sweep-on-multiple-nodes-in-azureml</guid>
      <pubDate>Tue, 29 Oct 2024 12:44:26 GMT</pubDate>
    </item>
    <item>
      <title>对整个数据集进行标准化（MinMax）是否会对看不见的时间序列数据产生更好的结果？</title>
      <link>https://stackoverflow.com/questions/79136221/normalization-minmax-on-the-whole-dataset-produces-better-results-on-unseen-ti</link>
      <description><![CDATA[我已经训练了一个模型，用于预测 15 天内价格的最小最大值：1 最大值；0.5 无，0 最小值。问题是 Keras 中的 mse 回归，模型是无状态的：
RNN(PeepholeCell(units=2,activation=&#39;relu&#39;))
TimeDistributed(Dense(1))
Dense(1)
优化器是 Adam。形状：
test_X.shape = (69501, 1, 45); test_y.shape = (11508, 1, 1)
我进行了如下拟合：
history = model.fit(train_X, train_y,
epochs=num_epochs, batch_size=10080, validation_data=(test_X, test_y), verbose=2,
shuffle=False, callbacks=[
earlystopping,
reduce_lr
]
, class_weight=class_weights
)

我对不同的模型配置、学习率策略进行了不同的训练尝试，并使用了 2014 年至 2021 年的每小时汇总数据，并使用了 2019 年至 2021 年的数据进行测试。对于验证，我使用了当前未见的数据（最近 900 天），粒度为 1 天，（最近 900 * 24）粒度为 1 小时。要预测的 y 数据为 0（表示最小相对值），1（表示最大值），否则为 0.5。标签 (y) 具有振荡统计数据，但在不同批次（平均值、标准差、最大值、最小值、QR）中，其幅度随时间相同。除了一些大小反映价格幅度变化的特征外，这些特征还具有恒定范围（例如振荡器）。
我发现，通过回测对未见数据执行的最佳模型是仅将 MinMax 缩放器（scikit）应用于整个数据集的模型。我尝试在训练数据集上安装 RobustScaling 和 MinMax 标准化，并用它来转换测试数据。
我的理解是，仅应用 MinMax 缩放器，而不进行标准化或异常值移除，将保留数据的分布，即使不均匀，NN 也应该能够从中学习。
这样，模型可以专注于学习 x 数据幅度差异的两个阶段，同时不会将信息泄露给 y 变量，因为分布相似且幅度恒定。
为了避免过度拟合，我恰当地将 LSTM 的单位减少到最小值，与更高的单位值相比，该值表现最佳。
你觉得我做错了什么吗？
因此，对整个数据集进行规范化（MinMax）可以在看不见的时间序列数据上产生更好的结果？]]></description>
      <guid>https://stackoverflow.com/questions/79136221/normalization-minmax-on-the-whole-dataset-produces-better-results-on-unseen-ti</guid>
      <pubDate>Tue, 29 Oct 2024 07:40:35 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'KerasHistory'对象没有属性'layer'</title>
      <link>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</link>
      <description><![CDATA[我在使用 Keras 模型时遇到错误“AttributeError：&#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;”。
我尝试访问层信息，但似乎我引用了错误的对象。我尝试将名称层更改为操作，但没有成功。
我使用的是 TensorFlow v2.17.0。这是代码：
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense, Dropout

input_shape = (96, 96, 1)

# 输入张量形状
X_input = Input(input_shape)

# 零填充
X = ZeroPadding2D((3,3))(X_input)

# 1 - 阶段
X = Conv2D(64, (7,7), strides= (2,2), name = &#39;conv1&#39;, kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = &#39;bn_conv1&#39;)(X)
X = Activation(&#39;relu&#39;)(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - 阶段
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - 阶段
X = res_block(X, filter= [128,128,512], stage= 3)

# 平均池化
X = AveragePooling2D((2,2), name = &#39;Averagea_Pooling&#39;)(X)

# 最终层
X = Flatten()(X)
X = Dense(4096, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.2)(X)
X = Dense(2048, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.1)(X)
X = Dense(30, 激活 = &#39;relu&#39;)(X)

model_1_facialKeyPoints = Model(inputs= X_input, 输出 = X)
model_1_facialKeyPoints.summary()

这是回溯：
AttributeError Traceback (最近一次调用最后一次)
&lt;ipython-input-366-fd266d53d661&gt; 在 &lt;cell line: 34&gt;()
32 
33 
---&gt; 34 model_1_facialKeyPoints = Model( 输入= X_input，输出 = X)
35 model_1_facialKeyPoints.summary()

4 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/ functional.py in _validate_graph_inputs_and_outputs(self)
692 # 检查 x 是否为输入张量。
693 # pylint：disable=protected-access
--&gt; 694 
695 layer = x._keras_history.layer
696 if len(layer._inbound_nodes) &gt; 1 or (

AttributeError: &#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</guid>
      <pubDate>Tue, 29 Oct 2024 05:11:46 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 值错误：按要求创建数组时无法避免复制</title>
      <link>https://stackoverflow.com/questions/79135634/xgboost-value-error-unable-to-avoid-copy-while-creating-an-array-as-requested</link>
      <description><![CDATA[我正在尝试拟合 xgboost 模型，但它给出了一个错误：
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# 假设 y_train 和 y_val 是具有多列的 DataFrames
y_train_single = y_train[&#39;Fraction_Insertions&#39;] # 替换为您的特定目标
y_val_single = y_val[&#39;Fraction_Insertions&#39;] # 替换为您的特定目标

# 创建 DMatrix 对象
train = xgb.DMatrix(X_train, label=y_train_single)
test = xgb.DMatrix(X_val, label=y_val_single)

# 设置参数
params = {
&#39;objective&#39;: &#39;reg:squarederror&#39;,
&#39;eval_metric&#39;: &#39;rmse&#39;,
&#39;eta&#39;: 0.1,
&#39;max_depth&#39;: 3,
&#39;seed&#39;: 42
}

# 训练模型
model = xgb.train(params, train, num_boost_round=100)

# 进行预测
y_pred = model.predict(test)

# 评估
print(&quot;均方误差 (MSE)：&quot;, mean_squared_error(y_val_single, y_pred))
print(&quot;R^2 分数：&quot;, r2_score(y_val_single, y_pred))


{
&quot;name&quot;: &quot;ValueError&quot;,
&quot;message&quot;: &quot;按要求创建数组时无法避免复制。
如果使用 `np.array(obj, copy=False)`，请将其替换为 `np.asarray(obj)`，以便在需要时进行复制（NumPy 1.x 中没有行为变化）。
有关更多详细信息，请参阅 https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-i
]]></description>
      <guid>https://stackoverflow.com/questions/79135634/xgboost-value-error-unable-to-avoid-copy-while-creating-an-array-as-requested</guid>
      <pubDate>Tue, 29 Oct 2024 02:12:22 GMT</pubDate>
    </item>
    <item>
      <title>训练 CNN 模型时的损失和指标问题</title>
      <link>https://stackoverflow.com/questions/79135625/a-loss-and-metrics-problem-while-training-a-cnn-model</link>
      <description><![CDATA[我的自定义损失和指标出现了问题。我的目的是用图像训练一个 CNN 模型，并使用图像中物体的角度方向的切线，我有一列指示切线是正还是负。最后我有两个输出，一个是切线（回归），另一个是（分类）。现在，当我编写 model.evaluate 时，我把回归写为第一个出现的东西，但它并没有作为第一个出现。我不确定它们是否以某种方式被反转了。因为我找不到我得到的奇怪结果的解释。这是我的代码：
# 自定义损失和度量函数
@keras.utils.register_keras_serializable(package=&quot;Custom&quot;)

def angular_loss(y_true, y_pred):
angles_true = tf.math.atan(y_true) * 180.0 / np.pi
angles_pred = tf.math.atan(y_pred) * 180.0 / np.pi
return tf.abs(angles_true - angles_pred)

@keras.utils.register_keras_serializable(package=&quot;Custom&quot;)
def rmse_degrees(y_true, y_pred):
a = tf.constant(np.pi)
angles_true = tf.math.atan(y_true) * 180.0 / a
angles_pred = tf.math.atan(y_pred) * 180.0 / a
b = tf.square(angles_true - angles_pred)
return tf.reduce_mean(b)
# 定义模型
input_image = Input(shape=X_train_images.shape[1:], name=&#39;input_image&#39;)
x = layer.Conv2D(32, (3, 3),activation=&#39;relu&#39;)(input_image)
x = layer.MaxPooling2D((2, 2))(x)
x = layer.Dropout(0.3)(x)
x = layer.Conv2D(64, (3, 3),activation=&#39;relu&#39;)(x)
x = layer.MaxPooling2D((2, 2))(x)
x = layer.Dropout(0.3)(x)
x = layer.Flatten()(x)
x = layer.Dense(128,activation=&#39;relu&#39;)(x) # 中间密集层
x = layer.Dropout(0.3)(x)

output_regression = layer.Dense(1,activation=&#39;linear&#39;,name=&#39;reg_output&#39;)(x)
output_classification = layer.Dense(1,activation=&#39;sigmoid&#39;,name=&#39;cls_output&#39;)(x)
model = keras.Model(inputs=input_image,outputs=[output_regression,output_classification])
model.summary()
model.save(&quot;modelfinal3.keras&quot;)
# 编译模型

model.compile(
optimizer = RMSprop(learning_rate=0.0001),
loss={
&#39;reg_output&#39;: angular_loss,
&#39;cls_output&#39;:&#39;binary_crossentropy&#39;
},
metrics={
&#39;reg_output&#39;: [rmse_degrees],
&#39;cls_output&#39;: [&#39;accuracy&#39;]
}
)

# 定义 ModelCheckpoint 回调以保存最佳模型
callbacks = [
keras.callbacks.ModelCheckpoint(&quot;modelfinal3.keras&quot;, monitor=&quot;reg_output_loss&quot;, save_best_only=True , mode=&#39;min&#39;),
keras.callbacks.EarlyStopping(monitor=&#39;reg_output_loss&#39; , waiting = 8 ,mode=&#39;min&#39; )
]

# 在没有验证数据的情况下训练模型
history = model.fit(
X_train_images,{&#39;reg_output&#39; : Y1_regression ,&#39;cls_output&#39; : Y2_classification} ,
epochs= 10 ,
batch_size= 64,
回调=回调
)

test_model =keras.models.load_model(&quot;modelfinal3.keras&quot;, custom_objects ={&#39;angular_loss&#39;: angular_loss, &#39;rmse_degrees&#39;: rmse_degrees })
results = test_model.evaluate(X_test_images, {&#39;reg_output&#39; : Y1_regression_test ,&#39;cls_output&#39; : Y2_classification_test },return_dict=True )

print(results)

结果
30/30 ━━━━━━━━━━━━━━━━━━━━━━ 51s 2s/步 - cls_output_accuracy：0.6618 - cls_output_loss：9.0815 - 损失：14.4716 - reg_output_loss：5.3914 - reg_output_rmse_degrees：2806.2744
Epoch 7/10
30/30 ━━━━━━━━━━━━━━━━━━━━━━ 53s 2s/步 - cls_output_accuracy：0.6401 - cls_output_loss：8.9781 - 损失：14.7173 - reg_output_loss：5.7363 - reg_output_rmse_degrees： 2787.8420
时代 8/10
30/30 ━━━━━━━━━━━━━━━━━━━━━━ 51s 2s/步 - cls_output_accuracy：0.6524 - cls_output_loss：9.0007 - 损失：14.5403 - reg_output_loss：5.5401 - reg_output_rmse_degrees：2789.3442
时代 9/10
30/30 ━━━━━━━━━━━━━━━━━━━━━ 51 秒 2 秒/步 - cls_output_accuracy：0.6674 - cls_output_loss：9.4412 - 损失：14.7438 - reg_output_loss：5.3030 - reg_output_rmse_degrees：2844.9971
Epoch 10/10
30/30 ━━━━━━━━━━━━━━━━━━━━━━━ 52 秒 2 秒/步 - cls_output_accuracy：0.6610 - cls_output_loss：9.3189 - 损失：14.7248 - reg_output_loss：5.4059 - reg_output_rmse_degrees：2828.9368
11/11 ━━━━━━━━━━━━━━━━━━━━━━ 2s 142ms/步 - cls_output_accuracy：1.0000 - cls_output_loss：9.4424 - 损失：9.4928 - reg_output_loss：1.1921e-07 - reg_output_rmse_degrees： 2435.0107
{&#39;cls_output_accuracy&#39;: 1.0, &#39;cls_output_loss&#39;: 8.932900428771973, &#39;loss&#39;: 9.235373497009277, &#39;reg_output_loss&#39;: 1.1920930376163597e-07, &#39;reg_output_rmse_degrees&#39;: 2396.7568359375}

进程已完成，退出代码为 0 

我预计 cls 指标的正常值约为 90%，reg 指标的正常值约为 4。]]></description>
      <guid>https://stackoverflow.com/questions/79135625/a-loss-and-metrics-problem-while-training-a-cnn-model</guid>
      <pubDate>Tue, 29 Oct 2024 02:11:19 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Pandas 数据转换为对象的 numpy dtype。使用 np.asarray(data) 检查输入数据。解决它</title>
      <link>https://stackoverflow.com/questions/79135568/valueerror-pandas-data-cast-to-numpy-dtype-of-object-check-input-data-with-np</link>
      <description><![CDATA[https://colab.research.google.com/drive/1O98QhAdit3D8s61WCSGj35AkKAGkBtu8?usp=sharing
解决无错误代码
测试此部分使用的两个数据集来自加州大学欧文分校机器学习
存储库。您可以下载这些数据集并手动将其加载到您的程序中，也可以使用 ucimlrepo python 包导入它们，可以使用命令 pip install ucimlrepo 安装（每个存储库的网页都有使用 ucimlrepo 直接从 python 中加载数据所需的代码）。
说明：请在单个 Python 文件和 pdf 文件中提供您的代码，并附上您的书面回复。

使用混凝土抗压强度数据集
(https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)：
(a) (15 分) 使用其他 8 个变量执行线性回归以预测混凝土抗压强度。报告您获得的拟合系数以及 R2 值，并写出您的拟合方程。
(b) (5 分) 报告拟合的 F 统计量的 p 值。F 统计量的零假设可以拒绝吗？
(c) (5 分) 报告每个系数的 p 值，并说明是否可以拒绝零假设。
使用电离层数据集
(https://archive.ics.uci.edu/dataset/52/ionosphere)：
(a) (5 分) 拟合逻辑回归模型，使用其他 34 个变量作为输入预测因子，预测接收到的信号是“好”还是“坏”。
(b) (5 分) 根据拟合的 p 值，是否可以从模型中删除任何属性？提供答案的理由。
(c) (5 分) 使用 k 倍交叉验证优化此数据集上高斯朴素贝叶斯分类器的 var_smoothing 参数。提供图表并说明所选最优值的理由。
(d) (5 分) 使用 k 倍交叉验证优化此数据集上的 KNN 的 k。提供图表并说明所选最优值的理由。
(e) (5 分) 在三个拟合模型中，哪一个最准确？请务必为您的答案提供理由。
]]></description>
      <guid>https://stackoverflow.com/questions/79135568/valueerror-pandas-data-cast-to-numpy-dtype-of-object-check-input-data-with-np</guid>
      <pubDate>Tue, 29 Oct 2024 01:50:29 GMT</pubDate>
    </item>
    <item>
      <title>评估模型预测性能时，mase() 错误无法索引数据以外的行</title>
      <link>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</link>
      <description><![CDATA[我正在预测时间序列结果 Y，其预测因子滞后 t-12 天，以产生 12 天前的预测，样本外验证框架为 10 个不重叠的折叠。我想使用平均缩放误差作为性能指标。根据我对这篇文章的理解，我将朴素预测的时间步长设置为 12，以便朴素预测和我的模型预测在同一时间间隔。我理解这会将我的模型预测（使用滞后值预测）与 12 天前的 y 值进行比较。由于某种原因，mase() 函数似乎无法以 12 天的步长循环遍历数据，因为它会返回一条错误消息，指出它必须索引“负值行”，这是不可能的。我使用 yardstick 包 mase 函数 时也遇到了同样的情况。有谁知道如何修复该问题，或者可以指出我做错了什么吗？
查看带有模拟数据的示例
#load libraries#

library(tidiverse)
library(Metrics)
library(caret)

#create simulation data

set.seed(123)
y&lt;-sample(1:150,662,replace = T)
X1_L12&lt;-runif(662,-1,1)#假设 X1 滞后 12 天
X2_L12&lt;-runif(662,-1:1)#假设 X2 滞后 12 天
date&lt;-sample(1:662)

dat&lt;-data.frame(date,y,X1_L12,X2_L12)

#create the function to compute mase setting the t-12 时的简单预测

mase_lag12 &lt;- function(data, lev = NULL, model = NULL) {
data$pred &lt;- as.numeric(data$pred)
data$obs &lt;- as.numeric(data$obs)
masefunction = mase(data$obs,data$pred,12)
names(masefunction) &lt;- c(&#39;MASE&#39;)
masefunction
}

#OOS 验证框架
#创建时间片并定义性能指标
myTimeControlmase &lt;- trainControl(method = &quot;timeslice&quot;,
initialWindow = 53,
horizo​​n = 13,
skip=65,
fixedWindow = TRUE,
summaryFunction = mase_lag12)

#model
glmnetmod_lag12 = train(y~X1_L12+X2_L12,
method = &quot;glmnet&quot;,
family=&quot;poisson&quot;,
trControl = myTimeControlmase,maximize=FALSE,
preProc = c(&quot;range&quot;),
data=dat)

在代码的最后一部分拟合模型时，它会返回以下错误消息：
actual[1:naive_end] 中的错误：只有 0 可以与负下标混合
]]></description>
      <guid>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</guid>
      <pubDate>Mon, 28 Oct 2024 20:20:47 GMT</pubDate>
    </item>
    <item>
      <title>如何通过递归特征消除来选择最佳的特征数量和具有最高F1和ROC的最佳算法？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-select-the-best-number-of-feature-and-best-algorithm-with-highest-f1-and</link>
      <description><![CDATA[我想对我的数据的多个子集（即每个子集针对每个特定客户）运行递归特征消除，其中有 X 个特征（即 4-12）和 Y 个不同算法（即 DecisionTreeClassifier、GradientBoostingClassifier、LogisticRegression）。
如何创建一个代码，在每个子集上生成并显示具有最佳算法的最佳特征数量（最高精确度-召回率，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
下面是我尝试在逻辑回归中为不同数量的特征生成结果，但我仍然需要替换估算器中的算法并逐一检查结果。
for i in range(4, 13):
rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)
rfe.fit(X_resampled,Y_resampled)
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, 阈值 = roc_curve(Y_test, pred_test)
print(f&#39;特征数量：{i}，准确率得分：{accuracy_score(Y_test, pred_test_1)}&#39;)
print(f&#39;特征数量：{i}，ROC：&#39;，logit_roc_auc)
precision, recall, 阈值 = precision_recall_curve(Y_test, pred_test)
print(f&#39;特征数量：{i}，f1 得分：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;特征数量：{i}，PRC AUC：{auc(recall,precision)}&#39;)
precision = precision_score(Y_test, pred_test_1)
recall = recall_score(Y_test, pred_test_1)
print(f&#39;特征数量：{i},召回率：&#39;, recall)
print(f&#39;特征数量：{i},精确度：&#39;, precision)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-select-the-best-number-of-feature-and-best-algorithm-with-highest-f1-and</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>删除所有人口后，NEAT 给出错误</title>
      <link>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</guid>
      <pubDate>Sun, 27 Oct 2024 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 真的需要可学习的自注意力层吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79120787/do-the-transformers-really-need-the-learnable-self-attention-layer</link>
      <description><![CDATA[Transformer 的核心组件是自注意力机制，它负责通过计算注意力分数来捕获 token 之间的依赖关系。这些分数通常通过可学习的投影（查询、键、值）计算得出。
我想知道自注意力层是否需要可学习。是否可以用更简单或不可学习的东西（例如固定或动态生成的表示）替换可学习的查询、键和值投影？这是否仍允许模型捕获足够的上下文信息以用于语言建模等任务？
我尝试删除所有可学习的投影（查询、键、值）并直接将点积应用于 token 本身。但是，我正在寻找一种替代方案，其中可以在不使用可学习参数的情况下将 token 投影到更有意义的维度。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79120787/do-the-transformers-really-need-the-learnable-self-attention-layer</guid>
      <pubDate>Thu, 24 Oct 2024 07:21:36 GMT</pubDate>
    </item>
    <item>
      <title>如果不重新嵌入数据，更新 Qdrant vectordb 中的点是否安全？</title>
      <link>https://stackoverflow.com/questions/78776361/is-updating-points-in-qdrant-vectordb-without-re-embedding-the-data-safe</link>
      <description><![CDATA[我正在使用 Langchain 构建一个 RAG 聊天机器人，使用我存储在 Qdrant 矢量数据库中的数据。
我想更改我的 qdrant 矢量数据库中一些文档的元数据。
为此，我单独存储了这些文档（包括矢量），删除了旧数据，修改了元数据并将它们重新插入，而无需再次嵌入它们。我想知道这样做是否安全。
（注意：我只更改了元数据。页面内容相同。）
仅供参考，当我滚动点时，记录看起来像这样：
Record(id=&#39;001c7e73032a40158b0c629f163e3bcf&#39;, payload={&#39;page_content&#39;: &#39;猫（Felis catus），通常称为家猫或家猫，是一种小型家养食肉哺乳动物。它是唯一被驯化的物种...&#39;, &#39;metadata&#39;: {&#39;source&#39;: &#39;Wikipedia&#39;, &#39;url&#39;: &#39;https://en.wikipedia.org/wiki/Cat&#39;, &#39;count&#39;: 1}}, vector=[-0.013379335966146134, 0.007072219895927447, -0.0018562082621154511, -0.0036226000104401957, -0.005558645199573012, 0.012487823730867539, -0.023378909852710315,...

所以我的问题是：
我保留了向量、page_content 和 id。这是否消除了重新嵌入数据的需要？
PS：避免重新嵌入数据的原因是嵌入数据的成本很高，因为我有很多数据。]]></description>
      <guid>https://stackoverflow.com/questions/78776361/is-updating-points-in-qdrant-vectordb-without-re-embedding-the-data-safe</guid>
      <pubDate>Sun, 21 Jul 2024 21:14:04 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines 模块错误 -> ‘gym.logger’ 没有属性 ‘MIN_LEVEL’</title>
      <link>https://stackoverflow.com/questions/71449872/stable-baselines-module-error-gym-logger-has-no-attribute-min-level</link>
      <description><![CDATA[我尝试使用 stable_baselines，但我尝试使用的任何模型都给出了相同的错误：
模块“gym.logger”没有属性“MIN_LEVEL”
我已从他们的网站上附上了一个示例，该示例给出了相同的错误。我尝试在线查找，但没有成功。此外，我目前正在使用 Conda 创建具有以下设置的环境。
Tensorflow：1.15.0
Python：3.7.11
代码如下。
import gym

from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common import make_vec_env
from stable_baselines import PPO2

# 多进程环境
env = make_vec_env(&#39;CartPole-v1&#39;, n_envs=4)

model = PPO2(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
model.save(&quot;ppo2_cartpole&quot;)

del model # 删除以演示保存和加载

model = PPO2.load(&quot;ppo2_cartpole&quot;)

# 享受训练有素的代理
obs = env.reset()
while True:
action, _states = model.predict(obs)
obs, rewards, dones, info = env.step(action)
env.render()

完整错误：
-------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
/var/folders/2l/c0wfhk2x0qz3v_6x0ylvvdr00000gn/T/ipykernel_4323/1825670659.py in &lt;module&gt;
8 env = make_vec_env(&#39;CartPole-v1&#39;, n_envs=4)
9 
---&gt; 10 model = PPO2(MlpPolicy, env, verbose=1)
11 model.learn(total_timesteps=25000)
12 model.save(&quot;ppo2_cartpole&quot;)

~/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py in __init__(self, policy, env, gamma, n_steps, ent_coef, learning_rate, vf_coef, max_grad_norm, lam, nminibatches, noptepochs, cliprange, cliprange_vf, verbose, tensorboard_log, _init_setup_model, policy_kwargs, full_tensorboard_log, seed, n_cpu_tf_sess)
95 
96 if _init_setup_model:
---&gt; 97 self.setup_model()
98 
99 def _make_runner(self):

~/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py in setup_model(self)
108 
109 def setup_model(self):
--&gt; 110 with SetVerbosity(self.verbose):
111 
112 assert issubclass(self.policy, ActorCriticPolicy), &quot;Error: PPO2 模型的输入策略必须是 &quot; \

~/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines/common/base_class.py in __enter__(self)
1127 self.tf_level = os.environ.get(&#39;TF_CPP_MIN_LOG_LEVEL&#39;, &#39;0&#39;)
1128 self.log_level = logger.get_level()
-&gt; 1129 self.gym_level = gym.logger.MIN_LEVEL
1130 
1131 if self.verbose &lt;= 1:

AttributeError: 模块“gym.logger”没有属性“MIN_LEVEL”
]]></description>
      <guid>https://stackoverflow.com/questions/71449872/stable-baselines-module-error-gym-logger-has-no-attribute-min-level</guid>
      <pubDate>Sat, 12 Mar 2022 12:55:23 GMT</pubDate>
    </item>
    <item>
      <title>sklearn：获取点到最近聚类的距离</title>
      <link>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</link>
      <description><![CDATA[我正在使用 DBSCAN 之类的聚类算法。
它返回一个名为 -1 的“聚类”，这些点不属于任何聚类。对于这些点，我想确定它与最近聚类之间的距离，以获得类似于该点异常程度的指标。这可能吗？或者这种指标还有其他选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</guid>
      <pubDate>Thu, 18 May 2017 07:31:36 GMT</pubDate>
    </item>
    </channel>
</rss>