<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 09 Dec 2023 21:12:04 GMT</lastBuildDate>
    <item>
      <title>如何构建数据集来训练预测模型</title>
      <link>https://stackoverflow.com/questions/77632864/how-to-structure-dataset-to-train-forecasting-model</link>
      <description><![CDATA[我在不同股票和不同日期选择了 1000 个类似的股票走势，并且我为每个时间序列使用了一些功能。我预计这些 5 分钟 ohlc 数据在一定程度上有点相似，我想训练一个模型，根据我认为表现相似的股票的 X 5 分钟蜡烛来预测接下来的 6 根蜡烛会是什么样子或也许收盘价是多少。
我如何创建训练数据，以便模型知道这些是 1000 个不同的时间序列并寻找它们之间的模式？ （也许使用一种非常大的热门编码？）
有人可以给我一个正确的方向吗？谢谢:)]]></description>
      <guid>https://stackoverflow.com/questions/77632864/how-to-structure-dataset-to-train-forecasting-model</guid>
      <pubDate>Sat, 09 Dec 2023 20:56:40 GMT</pubDate>
    </item>
    <item>
      <title>我需要建立一个机器学习模型，根据公司电子邮件使用数据评估员工流失率</title>
      <link>https://stackoverflow.com/questions/77632626/i-need-to-make-an-ml-model-that-evaluates-employee-attrition-rate-based-on-his-c</link>
      <description><![CDATA[此模型输入是每个员工的电子邮件使用情况的元数据，例如发出的电子邮件数量、收到的电子邮件数量等以及每封电子邮件的内容。输出是每个员工的自然流失率。该评估基于比较不同时期之间的电子邮件使用数据。
主要问题是没有附加数据集来解决这个问题。您建议采用什么方法来生成数据集？我应该为此模型选择哪种算法？]]></description>
      <guid>https://stackoverflow.com/questions/77632626/i-need-to-make-an-ml-model-that-evaluates-employee-attrition-rate-based-on-his-c</guid>
      <pubDate>Sat, 09 Dec 2023 19:38:25 GMT</pubDate>
    </item>
    <item>
      <title>在张量流中的卷中执行串联时出现问题</title>
      <link>https://stackoverflow.com/questions/77632484/issue-performing-concatenation-in-a-volume-in-tensorflow</link>
      <description><![CDATA[我正在构建一个肿瘤分割深度学习模型，即将使用3D Unet，但我提出了这个问题，这是我的代码：
# 卷积块
def conv_block(输入, num_filters):
    x = Conv3D(num_filters, (3, 3, 3), padding = “相同”)(输入)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    x = Conv3D(num_filters, (3, 3, 3), 填充 = “相同”)(x)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    返回x

# 编码器块
def编码器_块（输入，num_filters）：
    x = conv_block(输入, num_filters)
    p = MaxPool3D((2, 2, 2))(x)
    返回 x, p

# 解码器块
def解码器_块（输入，跳过，num_filters）：
    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=“相同”)(输入)
    x = 连接()([x, 跳过])
    x = conv_block(x, num_filters)
    返回x

# 大学网络

def unet(输入形状):
    输入 = 输入（输入形状）

    “----编码器----”
    s1, p1 = 编码器_块(输入, 64)
    s2, p2 = 编码器_块(p1, 128)
    s3, p3 = 编码器_块(p2, 256)
    s4, p4 = 编码器_块(p3, 512)

    “----桥---”
    b1 = conv_block(p4, 1024)

    “----解码器----”
    d1 = 解码器_块(b1, s4, 512)
    d2 = 解码器_块(d1, s3, 256)
    d3 = 解码器块(d2, s2, 128)
    d4 = 解码器块(d3, s1, 64)

    输出 = Conv3D(1, 1, 填充 =“相同”, 激活 =“sigmoid”)(d4)

    模型=模型（输入，输出，名称=“UNET”）
    返回模型

输入形状 = (155, 255, 255, 3)

测试模型=unet(输入形状)

问题是这样的：
&lt;前&gt;&lt;代码&gt;在&lt;细胞系：3&gt;()
      1 输入形状 = (155, 255, 255, 3)
      2
----&gt; 3 测试模型=unet(输入形状)

3帧
&lt;ipython-input-14-03345eb7b1a8&gt;在unet（输入形状）中
     14
     15、“----解码器----”
---&gt; 16 d1 = 解码器_块(b1, s4, 512)
     17 d2 = 解码器_块（d1，s3，256）
     18 d3 = 解码器_块(d2, s2, 128)

&lt;ipython-input-12-44442d65f832&gt;在解码器块（输入，跳过，num_filters）
      2 def解码器_块（输入，跳过，num_filters）：
      3 x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=“相同”)(输入)
----&gt; 4 x = 连接()([x, 跳过])
      5 x = conv_block(x, num_filters)
      6 返回 x

error_handler 中的 /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py(*args, **kwargs)
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/concatenate.py 在 build(self, input_shape)
    129）
    [130] 第 130 章1：
--&gt; 131 引发值错误（err_msg）
    132
    133 def _merge_function（自我，输入）：

ValueError：“连接”层需要具有匹配形状（连接轴除外）的输入。收到：input_shape=[(无、18、30、30、512)、(无、19、31、31、512)]

我不知道为什么输出层对一个体素求和，这会产生错误，我不想消除该体素，因为它可能是有价值的信息]]></description>
      <guid>https://stackoverflow.com/questions/77632484/issue-performing-concatenation-in-a-volume-in-tensorflow</guid>
      <pubDate>Sat, 09 Dec 2023 18:48:43 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：期望有一个 cuda 设备，但得到：cpu</title>
      <link>https://stackoverflow.com/questions/77632426/valueerror-expected-a-cuda-device-but-got-cpu</link>
      <description><![CDATA[我正在尝试使用 DPO 执行强化学习，但是当我尝试执行 dpo_trainer.train() 时，即使我检查过，我仍然收到此错误
dpo_trainer.args.device

它返回
设备（类型=&#39;cuda&#39;，索引=0）

我看到很少的解决方案，例如将模型移动到 cuda 或设置
&#39;os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = &#39;0&#39;&#39;
但没有任何帮助，我无法找到问题所在。以下是我正在使用的训练参数
from Transformers import TrainingArguments
训练参数 = 训练参数（
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    最大步数=100，
    记录步骤=10，
    保存步骤=50，
    梯度累积步数=8，
    梯度检查点=真，
    学习率=2e-4,
    评价_策略=“步骤”，
    评估步骤=50，
    输出目录=&#39;./RL&#39;,
    lr_scheduler_type =“常量”，
    热身步数=0.03，
    optim=&quot;paged_adamw_32bit&quot;,
    fp16=正确，
    删除_未使用的列=假，
    run_name=“dpo_mistral7B”，
）
从 trl 导入 DPOTrainer

dpo_trainer = DPOTrainer(
        模型，
        模型参考，
        参数=训练参数，
        贝塔=0.1，
        train_dataset=数据集，
        分词器=分词器，
        pft_config=peft_config,
        最大提示长度=512，
        最大长度=1024，
        # 生成器=torch.Generator(device=&#39;cuda&#39;),

    ）
]]></description>
      <guid>https://stackoverflow.com/questions/77632426/valueerror-expected-a-cuda-device-but-got-cpu</guid>
      <pubDate>Sat, 09 Dec 2023 18:30:37 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 的自定义优化器</title>
      <link>https://stackoverflow.com/questions/77632195/custom-optimizer-for-tensorflow</link>
      <description><![CDATA[我正在尝试在 TensorFlow 上试验神经网络的自定义优化算法，但由于缺乏有关该主题的信息而陷入困境。我需要的是一些代码，这些代码将在每次迭代时为我提供向量 x （当前点）和向量 g （x 处的梯度），然后我将更新 x，然后使用一些代码来设置更新后的值。这是我目前所拥有的：
来自tensorflow.python.framework导入操作
从tensorflow.python.ops导入gen_training_ops
从tensorflow.python.ops导入math_ops
从tensorflow.python.training导入优化器
从tensorflow.python.util.tf_export导入tf_export
将张量流导入为 tf
将 numpy 导入为 np

类 TestGD(优化器.优化器):
  def __init__(自身, rad=0.01,
               use_locking=False, name=“TestGD”）：
    super(TestGD, self).__init__(use_locking, 名称)
    self._radius = rad

  def _create_slots(self, var_list):
    num_dims = len(var_list)
    self._beta = (num_dims - 1) / (num_dims + 1)
    self._B_matrix = np.identity(num_dims)

  def _prepare（自我）：
    self._radn_t = ops.convert_to_tensor(self._call_if_callable(self._radius), name=“beta”)
    self._beta_t = ops.convert_to_tensor(self._call_if_callable(self._beta), name=“beta”)
    self._B_matrix_t = ops.convert_to_tensor(self._call_if_callable(self._B_matrix), name=“B”)

  def _apply_dense（自身，梯度，变量）：
    返回 self._resource_apply_dense(grad, var)

  def _resource_apply_dense（自身，梯度，变量）：
    print(grad.shape, &quot;&lt;------------&quot;)
    #我计划在这里的某个地方实现我的算法
    var_update = tf.compat.v1.assign_sub(var, 0.01 * grad)
    返回 tf.group(var_update)

  def _apply_sparse(自我, grad, var):
    raise NotImplementedError(“不支持稀疏梯度更新。”)


# 构建LeNet模型
模型 = tf.keras.Sequential([
    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), 激活=&#39;relu&#39;, input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), 激活=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(120, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(84, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(10, 激活=&#39;softmax&#39;)
]）

# 使用您的自定义优化器
#custom_optimizer = SimpleGD(learning_rate=0.001)
自定义优化器 = TestGD()

# 使用自定义优化器编译模型
model.compile(优化器=custom_optimizer,
              损失=&#39;sparse_categorical_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

# 获取数据集
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 # 将像素值标准化为 0 到 1 之间

x_train = x_train[..., tf.newaxis].astype(“float32”)
x_test = x_test[..., tf.newaxis].astype(“float32”)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=60000).batch(64)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
测试数据集 = 测试数据集.batch(64)

＃ 训练
model.fit(train_dataset, epochs=5)

＃ 评估
test_loss, test_acc = model.evaluate(test_dataset)
print(f&quot;测试准确度：{test_acc}&quot;)

问题是，我得到的 grad 和 var 的形状非常奇怪，它们绝对不是向量。我应该怎么做才能将问题减少到 x 和 g 向量以及如何在最小化步骤后正确更新结果？]]></description>
      <guid>https://stackoverflow.com/questions/77632195/custom-optimizer-for-tensorflow</guid>
      <pubDate>Sat, 09 Dec 2023 17:23:42 GMT</pubDate>
    </item>
    <item>
      <title>协助在 Python 代码中集成预训练数据和测试 [关闭]</title>
      <link>https://stackoverflow.com/questions/77630814/assistance-with-integrating-pretrained-data-and-testing-in-python-code</link>
      <description><![CDATA[我正在开发一个 Python 项目，我需要将训练数据和测试数据集成到我的代码中全局算法已在 C++ 中实现。然而，我对是否将预训练数据放置在正确的位置感到有点迷失。我尝试添加数据，但不确定它是否位于正确的位置。
此外，我在使用自己的数据集测试代码并分析输出方面面临挑战。任何人都可以提供有关预训练数据的正确放置的指导或建议，并提供使用自定义数据有效测试代码的见解吗？
仓库：https://github.com/Lecanyu/JigsawNet]]></description>
      <guid>https://stackoverflow.com/questions/77630814/assistance-with-integrating-pretrained-data-and-testing-in-python-code</guid>
      <pubDate>Sat, 09 Dec 2023 09:39:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在wmt数据集中提取人类得分[关闭]</title>
      <link>https://stackoverflow.com/questions/77630433/how-to-extract-human-score-in-wmt-dataset</link>
      <description><![CDATA[我需要带有人工评分的 WMT 新闻数据。我查看了github链接 https://github.com/wmt-conference/wmt22-news -systems ，有一个 xml 文件夹，其中包含源、参考、翻译数据。还有另一个名为 humaneval 的文件夹，其中提供了人工评分。
我面临着连接这两个数据的挑战。在 humaneval 数据中提供了 SID 列，但其值与 xml 数据中提供的 source_id 不同。
如果有人可以帮助指导这两个数据文件夹之间的连接，那将会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77630433/how-to-extract-human-score-in-wmt-dataset</guid>
      <pubDate>Sat, 09 Dec 2023 07:03:35 GMT</pubDate>
    </item>
    <item>
      <title>RandomForest 函数在预测函数中应用时给出错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77629933/randomforest-function-giving-error-when-applied-in-predict-function</link>
      <description><![CDATA[当我使用 randomForest 训练模型，然后尝试按以下方式进行预测时，没有问题：
rf &lt;- randomForest(price_category~., data=train_set, ntree=300,importance=TRUE)

test_set$ClassPredicted &lt;- 预测(rf, newdata = test_set, &quot;class&quot;)

但是，当我尝试选择预测变量并按以下方式构建它时：
 rf1 &lt;- randomForest（价格类别 = 生活区域 + 年 + n_照片 + 能源标签，数据 = train_set，ntree=1000，重要性=TRUE）

test_set$ClassPredicted &lt;- 预测(rf1, newdata = test_set, &quot;class&quot;)

弹出此错误：
predict.randomForest(rf1, newdata = test_set, &quot;response&quot;) 中的错误没有森林
对象中的组件

为什么会发生这种情况以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77629933/randomforest-function-giving-error-when-applied-in-predict-function</guid>
      <pubDate>Sat, 09 Dec 2023 02:24:34 GMT</pubDate>
    </item>
    <item>
      <title>MMDetection3D 和 nuScenes：输出格式、转换和比较</title>
      <link>https://stackoverflow.com/questions/77629887/mmdetection3d-and-nuscenes-output-format-conversion-and-comparision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77629887/mmdetection3d-and-nuscenes-output-format-conversion-and-comparision</guid>
      <pubDate>Sat, 09 Dec 2023 01:53:34 GMT</pubDate>
    </item>
    <item>
      <title>实时多重检测 (RTMDet) CONFIG_PATH fileNotFoundError</title>
      <link>https://stackoverflow.com/questions/77629676/real-time-multi-detection-rtmdet-config-path-filenotfounderror</link>
      <description><![CDATA[我正在尝试遵循 Roboflow 指南&lt; /a&gt; 在自定义数据集上训练 RTMDet。我没有高端 GPU，因此我尝试使用 Colab 环境。
当我尝试使用 rtmdet_m 权重和配置文件初始化模型时，即使我 100% 确信驱动器目录中存在文件，也会收到 filenotfound 错误。我相信这个问题与下面的配置文件中的 Base 有关
_base_ = &#39;/content/drive/MyDrive/RTMDet_Models/rtmdet_l_syncbn_fast_8xb32-300e_coco.py&#39;



# ========================修改参数======================
加深因子 = 0.67
加宽因子 = 0.75

# =======================大多数情况下未修改==================
模型=字典（
主干=字典（深度因子=深度因子，加宽因子=加宽因子），
颈部=字典（深度因子=深度因子，加宽因子=加宽因子），
bbox_head=dict(head_module=dict(widen_factor=widen_factor)))

我还尝试将配置文件移动到本地 /content 目录，但它没有解决问题
这是我用于初始化的其余代码以及确切的错误消息
# 设置配置和权重文件的路径
WEIGHTS_PATH = &#39;/content/drive/MyDrive/RTMDet_Models/rtmdet_m_syncbn_fast_8xb32-
300e_coco_20230102_135952-40af4fe8.pth&#39;
CONFIG_PATH = &#39;/content/drive/MyDrive/RTMDet_Models/rtmdet_m_syncbn_fast_8xb32-300e_coco.py&#39;

# 初始化模型
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
模型= init_ detector（CONFIG_PATH，WEIGHTS_PATH，设备= DEVICE）

-------------------------------------------------- ------------------------
FileNotFoundError Traceback（最近一次调用最后一次）
&lt;ipython-input-61-1d88a7ed1feb&gt;在&lt;细胞系：3&gt;()
      1 # 初始化模型
      2 DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
----&gt; 3 模型= init_ detector（CONFIG_PATH，WEIGHTS_PATH，设备= DEVICE）

5帧
_is_lazy_import(文件名)中的/usr/local/lib/python3.10/dist-packages/mmengine/config/config.py
   第1653章
   第1654章
-&gt;第1655章
   第1656章
   第1657章

FileNotFoundError: [Errno 2] 没有这样的文件或目录:
&#39;/content/drive/MyDrive/RTMDet_Models/rtmdet_l_syncbn_fast_8xb32-300e_coco.py&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/77629676/real-time-multi-detection-rtmdet-config-path-filenotfounderror</guid>
      <pubDate>Fri, 08 Dec 2023 23:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何提取 SequentialFeatureSelector 的最佳估计器</title>
      <link>https://stackoverflow.com/questions/77629138/how-to-extract-best-estimator-of-a-sequentialfeatureselector</link>
      <description><![CDATA[我已经从 sklearn 训练了一个 SequentialFeatureSelector，现在对它生成的最佳模型（基于给定的评分方法）感兴趣。是否有可能提取参数并使用它们生成所使用的模型？
我已经看到 SequentialFeatureSelector 存在一个 get_params() 函数，但我不明白如何解释输出并检索最佳估计器。&lt; /p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/77629138/how-to-extract-best-estimator-of-a-sequentialfeatureselector</guid>
      <pubDate>Fri, 08 Dec 2023 20:58:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 WEKA 检测信用卡欺诈？需要帮助[关闭]</title>
      <link>https://stackoverflow.com/questions/77629043/how-to-detect-credit-card-fraud-using-weka-help-needed</link>
      <description><![CDATA[我的计算机科学模块有这项作业。我需要使用 WEKA 来检测哪些用户可能拖欠信用卡到期付款。
我是 WEKA 的新手，所以有点困难。我收到了一个包含 48 个属性的 CSV，其中包括一个类：defaultnmIndicator 下个月默认的指示器（1=是，0=否）。
我需要预测，所以我知道我需要一个分类器。其中很多都是灰色的，包括 j48 和后勤。
例如，当我对整个集合运行线性回归时，相关系数约为 0.48。
我认为我在这里遗漏了一些非常明显的步骤。我的讲师刚刚建议我们搜索 YouTube，所以您可以想象这有多大帮助？
尝试的步骤：

我对数据集进行了标准化
估算缺失数据
使用完整训练集运行 CorrelationAttributeEval
尝试添加或删除此列表底部和顶部的属性，但模型没有得到真正的改进。
前 3 名：
`排名属性：
0.32588 43 金融压力指数
0.26159 22 平均循环信用卡利用率
0.24705 23 平均活跃信用额度利用率

&lt;小时/&gt;
底部3
-0.2096 30 任期最旧信用额度
-0.22082 自上次错过付款后 29 天
-0.40432 2 信用评分
`
我期望相关系数会更高。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77629043/how-to-detect-credit-card-fraud-using-weka-help-needed</guid>
      <pubDate>Fri, 08 Dec 2023 20:33:44 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML 模型给出的准确度、F1 分数、精确度和召回率均为 1.0，但似乎过度拟合 [关闭]</title>
      <link>https://stackoverflow.com/questions/77595988/my-ml-model-gives-accuracy-f1-score-precision-and-recall-as-1-0-but-it-seems-o</link>
      <description><![CDATA[
我有 Spotify 音乐数据集。
playlist_genre 是目标变量，它有 6 个类别 - 摇滚、拉丁、R&amp;B、说唱、流行、器乐
如果我使用标签编码对目标变量进行编码，那么我得到的准确度和 F1 分数为 1.0
如果我使用 getDummies 或 one-hot 编码，那么我的准确度和 f1 分数将分别降至 0.29 和 0.19。

使用的分类算法：随机森林
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/77595988/my-ml-model-gives-accuracy-f1-score-precision-and-recall-as-1-0-but-it-seems-o</guid>
      <pubDate>Sun, 03 Dec 2023 20:05:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用LSTM模型进行多步预测？</title>
      <link>https://stackoverflow.com/questions/69785891/how-to-use-the-lstm-model-for-multi-step-forecasting</link>
      <description><![CDATA[我用 LSTM 开发了一个时间序列模型。我不能用它来预测未来几天的股价。我想用它来预测明年的股价并绘制它。如何用它来预测未来（明年）的股价？
df=pd.read_csv(&#39;foolad.csv&#39;)
df=df.set_index(pd.DatetimeIndex(df[&#39;Date&#39;].values))

data=df.filter([&#39;关闭&#39;])
数据集=数据.值

Training_data_len=math.ceil(len(数据集)*0.8)
缩放器=MinMaxScaler(feature_range=(0,1))
scaled_data=scaler.fit_transform(数据集)
缩放数据

训练数据=缩放数据[0:训练数据长度，:]

xtrain=[]
y火车=[]
人数 = 60

对于范围内的 i(n,len(training_data))：
    xtrain.append(training_data[i-n:i, 0])
    ytrain.append(training_data[i,0])

xtrain , ytrain = np.array(xtrain) , np.array(ytrain)
xtrain=np.reshape(xtrain , (xtrain.shape[0],xtrain.shape[1],1))
xtrain.shape

模型=顺序()
model.add(LSTM(50,return_sequences=True,input_shape=(xtrain.shape[1],1)))
model.add(LSTM(50,return_sequences=False))
model.add(密集(25))
model.add(密集(1))

model.compile(loss=&#39;mean_squared_error&#39;,optimizer=&#39;adam&#39;)

model.fit(xtrain,ytrain,epochs=1,batch_size=1)

test_data=scaled_data[training_data_len - n : , :]
x测试=[]
ytest=数据集[training_data_len:,:]
对于范围内的 i(n , len(test_data))：
    xtest.append(test_data[i-n : i , 0])

xtest=np.array(xtest)
xtest=np.reshape(xtest , (xtest.shape[0],xtest.shape[1],1))

预测=模型.预测(xtest)
预测=scaler.inverse_transform(预测)

#未来360天我能做什么？......

]]></description>
      <guid>https://stackoverflow.com/questions/69785891/how-to-use-the-lstm-model-for-multi-step-forecasting</guid>
      <pubDate>Sun, 31 Oct 2021 10:20:32 GMT</pubDate>
    </item>
    <item>
      <title>UnboundLocalError：赋值前引用的局部变量“batch_outputs”</title>
      <link>https://stackoverflow.com/questions/63364588/unboundlocalerror-local-variable-batch-outputs-referenced-before-assignment</link>
      <description><![CDATA[我正在使用 Keras 编写机器学习代码来对前列腺癌的严重程度进行分级。运行后出现如下错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
UnboundLocalError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-0e08590512ec&gt;在&lt;模块&gt;中
      8 表示列中的文件：
      9 数据=generate_tiles(文件)
---&gt; 10 预测 = model.predict(数据)
     11 max_score = 预测.max()
     12

_method_wrapper 中的 /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py(self, *args, **kwargs)
     86 raise ValueError(&#39;多工作模式下不支持{}。&#39;.format(
     87 方法.__名称__))
---&gt; 88 返回方法（self，*args，**kwargs）
     89
     90返回tf_decorator.make_decorator（

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py 中的预测（自我，x，batch_size，详细，步骤，回调，max_queue_size，工人，use_multiprocessing）
   第1283章
   第1284章
-&gt;第1285章
   第1286章
   第1287章

UnboundLocalError：赋值前引用的局部变量“batch_outputs”

有谁知道批量输出也会引用什么？我的代码中没有这样的变量。]]></description>
      <guid>https://stackoverflow.com/questions/63364588/unboundlocalerror-local-variable-batch-outputs-referenced-before-assignment</guid>
      <pubDate>Tue, 11 Aug 2020 18:50:56 GMT</pubDate>
    </item>
    </channel>
</rss>