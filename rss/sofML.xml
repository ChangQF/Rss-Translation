<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 24 Jan 2024 12:27:19 GMT</lastBuildDate>
    <item>
      <title>预测的差异</title>
      <link>https://stackoverflow.com/questions/77872971/discrepancy-in-predictions</link>
      <description><![CDATA[我对 TensorFlow 相当陌生，正在尝试弄清楚一些事情。
Q1：我有一个维度为 44 和 5 的矩阵，代表 5 个特征向量。我定义了两个模型：第一个是基本的神经网络结构，第二个使用卷积层。我对输入层和大小不太了解。因此，我需要一些反馈来确认代码是否正确。
&lt;前&gt;&lt;代码&gt; X_train = np.random.rand(44, 5)
    y_train = np.random.rand(44, 1)
    X_rem = np.random.rand(11, 5)
    X_train= np.asarray(X_train).astype(np.float32)
    X_rem= np.asarray(X_rem).astype(np.float32)
    y_train= np.asarray(y_train).astype(np.float32)
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
    批量大小 = 44
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
    test_dataset = tf.data.Dataset.from_tensor_slices((X_rem, y_rem))


 模型 = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(5,)),
        tf.keras.layers.Dense(10, 激活=&#39;relu&#39;),
        #tf.keras.layers.Dense(5, 激活=&#39;relu&#39;),
        tf.keras.layers.Dense(1)
    ]）

 模型 = tf.keras.Sequential([
        tf.keras.layers.Input(形状=(5,)),
        tf.keras.layers.Reshape((5, 1)),
        tf.keras.layers.Conv1D(1, kernel_size=1, 激活=&#39;relu&#39;),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(12, 激活=&#39;线性&#39;),
        tf.keras.layers.Dense(1)
    ]）

 model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.2),
                  损失=&#39;mean_absolute_error&#39;,
                  指标=[&#39;mean_absolute_error&#39;])

问题2：如果代码正确，我有一个维度为 (44, 5) 的矩阵。那么，为什么输入形状指定为(5,)呢？据我了解，该模型需要以下格式的输入形状（批量大小、高度、宽度）。不过，我以前没有重塑过它们。
Q3：使用 model1 时，我获得了 model.fit(X_train, y_train, epochs=100)&lt; 的不同结果 (model.predict(X_rem) )&lt; /code&gt; 和 model.fit(train_dataset, epochs=100)  。我从 model.fit(train_dataset, epochs=100) 获得的输出是所有预测的单个值。”
Q4：如果我使用 model2，我将获得所有预测的单一值。
我尝试在定义模型之前重塑输入数据。我从未完全理解其背后的概念。]]></description>
      <guid>https://stackoverflow.com/questions/77872971/discrepancy-in-predictions</guid>
      <pubDate>Wed, 24 Jan 2024 12:15:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Logistic 回归中分离出重叠变量 - Kernel Trick / SVM 方法有效吗？</title>
      <link>https://stackoverflow.com/questions/77872945/how-to-separate-out-overlapping-variables-in-logistic-regression-would-kernel</link>
      <description><![CDATA[KDE 问题图
大家好，
这里的数据科学学生 - 尝试运行逻辑回归模型，但在查看密度时注意到我的 y 变量（本例中的转换） - 显示与预测特征（年龄）显着重叠。
我想知道是否可以使用内核技巧来分离年龄变量以更具预测性？
目前 - 我已经尝试过在训练/测试划分中使用类权重和分层等方法来尝试帮助平衡，但基本上我的每个模型的分数都在 74/26 划分标记附近（或更糟） 每个分类都预测负面结果（占主导地位的 0 结果）
任何建议表示赞赏，如果这看起来像一个愚蠢的问题/死胡同，我深表歉意！
亲切的问候，
本
我尝试过一些设置，但没有效果...
from sklearn.model_selection import train_test_split

将 X 和 y 分成训练集和测试集
`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16, stratify=y)&#39;
实例化模型（使用默认参数）
`logreg = LogisticRegression(random_state=16, class_weight={0: 0.74, 1: 0.26})
#logreg = LogisticRegression(random_state=16, class_weight=&#39;balanced&#39;)&#39;
“从 sklearn.model_selection 导入 train_test_split”
将 X 和 y 分成训练集和测试集
&#39;X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 16，stratify = y）&#39;

实例化模型（使用默认参数）
&#39;logreg = LogisticRegression(random_state=16, class_weight={0: 0.74, 1: 0.2})&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/77872945/how-to-separate-out-overlapping-variables-in-logistic-regression-would-kernel</guid>
      <pubDate>Wed, 24 Jan 2024 12:12:03 GMT</pubDate>
    </item>
    <item>
      <title>RandomForestRegressor 跨时间序列日期生成相同的预测</title>
      <link>https://stackoverflow.com/questions/77872699/randomforestregressor-producing-identical-predictions-across-time-series-dates</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77872699/randomforestregressor-producing-identical-predictions-across-time-series-dates</guid>
      <pubDate>Wed, 24 Jan 2024 11:34:15 GMT</pubDate>
    </item>
    <item>
      <title>过滤系统中相机识别的合适解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/77872631/suitable-solution-for-a-camera-recognition-in-a-filter-system</link>
      <description><![CDATA[我们是正在开展一个项目的学生，我们将检测过滤系统的某些变化。我们最感兴趣的是看到过滤带或过滤系统其他部件的损坏或磨损。遗憾的是，我们无法提供有关该系统的更多详细信息。
过滤系统正在用摄像头进行监控，因此我们可以访问它的一些镜头。目前我们可以检索一个多月前的录像。该系统很快就会成为一个封闭的解决方案，几乎没有光，尽管我们可以稍后安装灯或获得能够在黑暗中进行良好记录的摄像机。
我们应该如何处理该项目以尝试找到正确的解决方案？为了决定解决方案，我们可以获得哪些其他信息可能有用？我们是否已经能够针对该项目研究任何解决方案？
我们之前曾被建议在这个项目中使用机器学习，尽管我们不知道这是否是最合适的解决方案。鉴于我们可以访问一些镜头，在这个项目中使用机器学习有什么优点吗？]]></description>
      <guid>https://stackoverflow.com/questions/77872631/suitable-solution-for-a-camera-recognition-in-a-filter-system</guid>
      <pubDate>Wed, 24 Jan 2024 11:24:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 训练 VGG16 模型进行图像分类</title>
      <link>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</link>
      <description><![CDATA[我正在使用 PyTorch 进行图像分类。
我编写了以下适用于简单线性模型的训练函数：
criterion = nn.CrossEntropyLoss()
def train（模型、数据加载器、纪元）：
模型.to（设备）
优化器 = torch.optim.Adam(model.parameters(), lr=1e-3)
运行损失，运行加速 = 0., 0.
损失历史记录 = []
precision_history = [](data_train):.2f}%&quot;)

对于范围内的 i(1, 纪元 + 1)：
  模型.train()
  对于输入，数据加载器中的目标：
      输入，目标 = 输入.to(设备), 目标.to(设备)
      输出 = 模型（输入）
      损失=标准（输出，目标）
      优化器.zero_grad()
      loss.backward()
      优化器.step()
      preds = torch.argmax(输出, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == Targets).item()

  print(f&quot;[TRAIN epoch {i}] 损失: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 

我有预训练的 VGG16 模型，我想更改其最后一层的权重：
model_vgg = models.vgg16(weights=&#39;DEFAULT&#39;)
model_vgg.classifier[6] = nn.Linear(4096, 2)

对于 model_vgg.parameters() 中的参数：
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

火车（model_vgg，train_loader，2）

但是，在训练时出现以下错误：
RuntimeError Traceback（最近一次调用最后一次）

&lt;定时评估&gt;在&lt;模块&gt;中

&lt;ipython-input-27-1f64686a5cfd&gt;火车中（模型、数据加载器、纪元）
     39 损失 = 标准（输出，目标）
     40 优化器.zero_grad()
---&gt; 41loss.backward()
     42 优化器.step()
     43 preds = torch.argmax(输出, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:50 GMT</pubDate>
    </item>
    <item>
      <title>如何处理数据集中一堆标题的拼写错误？</title>
      <link>https://stackoverflow.com/questions/77872287/how-to-deal-with-spelling-mistakes-in-a-bunch-of-captions-in-a-dataset</link>
      <description><![CDATA[我正在尝试将预训练的 Transformer 模型（例如 GPT-2 或 Roberta）应用于我的文本到图像任务。目标是将字幕转换为相应的图像。为了实现这一点，我首先需要对标题进行标记以获得嵌入，例如：
tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
tokens = tokenizer.tokenize(标题)

当我打印标记的输出时，大多数单词的输出都是准确的，但是有一些标记代表子单词而不是完整的单词，因为这些单词在一堆标题中存在拼写错误。数据集。我不确定解决这个问题的最佳方法。我应该删除有拼写错误的单词，还是删除包含这些单词的整个标题，或者手动更正每个单词？]]></description>
      <guid>https://stackoverflow.com/questions/77872287/how-to-deal-with-spelling-mistakes-in-a-bunch-of-captions-in-a-dataset</guid>
      <pubDate>Wed, 24 Jan 2024 10:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 flutterflow 中的自定义小部件通过 google colab 或 google 可教学机器训练的模型进行图像识别</title>
      <link>https://stackoverflow.com/questions/77871985/how-to-use-custom-widgets-in-flutterflow-to-image-recognition-with-model-trained</link>
      <description><![CDATA[我正在使用 TensorFlow Lite (tflite) 包开发图像识别应用程序。我在 Google Colab 上训练了该模型，并将其与 FlutterFlow 集成。下面是我的小部件代码。有人可以指导我定义这些小部件的参数，确保正确使用吗？我不确定我的小部件代码是否正确，并且在将其与在 Google Colab 上训练的模型一起使用时面临着挑战。
对于这种情况我应该采取什么步骤有什么建议吗？ 这是 flutterflow 平台，我正在为此小部件编写代码
// 自动 FlutterFlow 导入
导入&#39;/backend/backend.dart&#39;;
导入&#39;/backend/schema/structs/index.dart&#39;;
导入&#39;/flutter_flow/flutter_flow_theme.dart&#39;;
导入&#39;/flutter_flow/flutter_flow_util.dart&#39;;
导入“index.dart”； // 导入其他自定义小部件
导入“包：flutter/material.dart”；
// 开始自定义小部件代码
// 不要删除或修改上面的代码！

导入&#39;包：image_picker / image_picker.dart&#39;；
导入&#39;包：image_downloader / image_downloader.dart&#39;；
导入“包：tflite/tflite.dart”；
导入 &#39;dart:io&#39;;

类 ImageRecognitionWidget 扩展 StatefulWidget {
  常量 ImageRecognitionWidget({
    钥匙？钥匙，
    这个宽度，
    这个高度，
  }) : 超级(键: 键);

  最后双打？宽度;
  最后双打？高度;

  @覆盖
  _ImageRecognitionWidgetState createState() =&gt;; _ImageRecognitionWidgetState();
}

类 _ImageRecognitionWidgetState 扩展 State; {
  最终 ImagePicker _picker = ImagePicker();
  细绳？ _图像网址；
  列表？ _认可；

  @覆盖
  无效初始化状态（）{
    super.initState();
    _imageUrl = 空； // 设置为 null 或提供初始 URL
    加载模型（）；
    if (_imageUrl == null || _imageUrl!.isEmpty) {
      抓取图像（）；
    }
  }

  未来&lt;空&gt; loadModel() 异步 {
    尝试 {
      细绳？ res = 等待 Tflite.loadModel(
        模型：“资产/图像/mobilenet_v1_1.0_224.tflite”，
        标签：“assets/images/labels.txt”，
      ）；
      print(&quot;加载模型：$res&quot;);
    } 捕获 (e) {
      print(&#39;加载模型失败：$e&#39;);
    }
  }

  未来&lt;空&gt;异步抓取图像（）{
    细绳？ imageId = 等待 ImageDownloader.downloadImage(_imageUrl!);
    if (imageId == null) {
      返回;
    }
    细绳？路径=等待ImageDownloader.findPath(imageId);
    print(&#39;保存的新图像：$path&#39;);
    等待识别图像（文件（路径！））；
  }

  未来&lt;空&gt; recognizeImage(文件图像) async {
    var 识别 = 等待 Tflite.runModelOnImage(
      路径：图像.路径，
      结果数: 6,
      阈值：0.05，
      图像平均值：127.5，
      图像标准：127.5，
    ）；
    设置状态（（）{
      _recognitions = 认可；
    });
  }

  @覆盖
  小部件构建（BuildContext上下文）{
    列表&lt;小部件&gt; stackChildren = [];
    stackChildren.add（
      中心（
        子项：列（
          孩子们：_recognitions！= null
              ？ _recognitions!.map((res) {
                  返回文本（
                    &quot;${res[&quot;index&quot;]} ${res[&quot;label&quot;]}: ${res[&quot;confidence&quot;].toStringAsFixed(3)}&quot;,
                    样式：文本样式（
                      颜色: 颜色.黑色,
                      字体大小：20.0，
                      背景颜色：颜色.白色，
                    ),
                  ）；
                }).toList()
              ：[]，
        ),
      ),
    ）；
    返回列（
      孩子：stackChildren，
    ）；
  }
}


我浏览过许多 YouTube 教程，但不幸的是，没有一个教程免费提供这些方面的指导。这是启发我创建此应用程序的视频的链接。我的目标是使用我自己训练的模型开发应用程序。 文本]]></description>
      <guid>https://stackoverflow.com/questions/77871985/how-to-use-custom-widgets-in-flutterflow-to-image-recognition-with-model-trained</guid>
      <pubDate>Wed, 24 Jan 2024 09:53:22 GMT</pubDate>
    </item>
    <item>
      <title>识别 SMOTE 生成的合成样本</title>
      <link>https://stackoverflow.com/questions/77870847/identify-the-synthetic-samples-generated-by-smote</link>
      <description><![CDATA[我有一个带标签的数据集，X 形状为 7000 x 2400，y 形状为 7000。数据严重不平衡，因此我尝试使用 SMOTE 生成合成样本。不过，我想确定 SMOTE 实际生成的合成样本。
作为示例，下面是一个代码片段：
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 imblearn.over_sampling 导入 SMOTE

虹膜 = load_iris()

X = 虹膜[&#39;数据&#39;]
y = 虹膜[&#39;目标&#39;]

#数据是平衡的，所以我故意去掉了一些样本
X = X[:125,::]
y = y[:125]

过采样 = SMOTE()
X_smt, y_smt = oversample.fit_resample(X, y)

数组 X_smt 和 y_smt 既有原始样本又有合成样本。是否有一种简单的方法可以通过索引或其他机制来识别合成样本？]]></description>
      <guid>https://stackoverflow.com/questions/77870847/identify-the-synthetic-samples-generated-by-smote</guid>
      <pubDate>Wed, 24 Jan 2024 06:04:18 GMT</pubDate>
    </item>
    <item>
      <title>梯度消失会导致“没有为任何变量提供梯度”</title>
      <link>https://stackoverflow.com/questions/77870522/can-vanishing-gradients-cause-no-gradients-provided-for-any-variable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77870522/can-vanishing-gradients-cause-no-gradients-provided-for-any-variable</guid>
      <pubDate>Wed, 24 Jan 2024 04:01:23 GMT</pubDate>
    </item>
    <item>
      <title>如何查询嵌入以进行语义搜索？</title>
      <link>https://stackoverflow.com/questions/77870218/how-to-query-embeddings-for-semantic-search</link>
      <description><![CDATA[我对某些 SKU 商品有 1000 个描述，我想生成逆嵌入映射来进行语义搜索
例如，这就是我所拥有的
项目描述
项目1 [单词1，单词2，单词3，单词4......]
项目2 [字1、字2_2、字3_3、字4_4......]

如您所见，item1 和 item2 共享 word1，但 item1 和 item2 有两个不同的上下文，通过生成嵌入，我们应该能够捕获每个单词的上下文
这是我生成嵌入的方法
my_description = []
以 open(&#39;/content/gdrive/My Drive/my.csv&#39;, &#39;r&#39;) 作为数据：
    df = pd.read_csv(数据, 编码 = (&#39;utf-8&#39;),nrows=100)
    对于索引，df.iterrows() 中的行：
        my_str = 行[&#39;描述&#39;]
        my_description.append(my_str)



进口火炬
从 Transformer 导入 BertTokenizer、BertModel
%matplotlib 内联
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
模型 = BertModel.from_pretrained(&#39;bert-base-uncased&#39;,
output_hidden_​​states = True, # 模型是否返回所有隐藏状态。
）
模型.eval()


文本2 = 公司描述[0]

# 添加特殊标记。
标记文本2 =“[CLS]” + 文本2 + &quot; [九月]”

# 将句子分割成标记。
tokenized_text2 = tokenizer.tokenize(marked_text2)

# 将标记字符串映射到它们的词汇索引。
indexed_tokens2 = tokenizer.convert_tokens_to_ids(tokenized_text2)

snippets_ids2 = [1] * len(tokenized_text2)
tokens_tensor2 = torch.tensor([indexed_tokens2])
snippets_tensors2 = torch.tensor([segments_ids2])

使用 torch.no_grad()：
    输出2 =模型（tokens_tensor2，segments_tensors2）
    隐藏状态2 = 输出2[2]

token_embeddings2 = torch.stack(hidden_​​states2, 暗淡=0)
token_embeddings2.size()
token_embeddings2 = torch.squeeze(token_embeddings2, 暗淡=1)
token_embeddings2.size()
token_embeddings2 = token_embeddings2.permute(1,0,2)
token_embeddings2.size()

token_vecs_cat2 = []

对于 token_embeddings2 中的令牌：
     cat_vec = torch.cat((令牌[-1], 令牌[-2], 令牌[-3], 令牌[-4]), 暗淡= 0)
     token_vecs_cat2.append(cat_vec)
token_vecs_sum2 = []
将 numpy 导入为 np
x_token = np.empty((0, 768))

对于 token_embeddings2 中的令牌：
    sum_vec = torch.sum(token[-4:], dim=0)
    token_vecs_sum2.append(sum_vec)
    x_token = np.concatenate((x_token, sum_vec.numpy().reshape((1,-1))), axis=0)

x_token 将是我所有单词/令牌在一个描述中的嵌入
例如，item1 有 500 个 token，嵌入数为 700
x_token 的形状为 (500 x 700)
所以对于每个项目我都会有这样的东西
项目标记嵌入
项目 1 标记 1 [x1,x2,x3,.....]
项目 1 标记 2 [x1,x2,x3,.....]
....
项目 2 令牌 1_2 [x1,x2,x3,.....]
项目 2 标记 2_2 [x1,x2,x3,.....]
....
项目 n 标记 1_n [x1,x2,x3,.....]
项目 n 标记 2_n [x1,x2,x3,.....]

现在我的问题是如何执行搜索
如果我的搜索查询是一个句子
“word1 word2 word3.....wordn”
如果我为句子中的每个单词生成嵌入，并对每个标记的前 10 个最近邻执行 ANN
如果我的查询有 10 个令牌，我将得到 100 个项目描述（每个令牌 10 个）
在这种情况下，我如何入围前 10 名项目描述？我应该使用哪个令牌？
查询 = [token1, token2.......tokenN]

                   前10名的nearest_neighbor的物品，
query_token1 -&gt;; [项目x1_1、项目x1_2、项目x1_10]
query_token2 -&gt;; [itemx2_1、itemx2_2、itemx2_10]

我做语义搜索错了吗？]]></description>
      <guid>https://stackoverflow.com/questions/77870218/how-to-query-embeddings-for-semantic-search</guid>
      <pubDate>Wed, 24 Jan 2024 02:10:12 GMT</pubDate>
    </item>
    <item>
      <title>加州房价预测例外[关闭]</title>
      <link>https://stackoverflow.com/questions/77870091/california-housing-price-prediction-exception</link>
      <description><![CDATA[我编写了以下代码：

将 pandas 导入为 pd
housing_pd = pd.read_csv(“housing.csv”)
housing_pd
housing_pd[“ocean_proximity”].value_counts()
housing_pd_shuffled = housing_pd.sample(n=len(housing_pd), random_state=1)
housing_pd_shuffled
pd.get_dummies(housing_pd_shuffled[&#39;ocean_proximity&#39;]).head()

在执行最后一行代码（第 7 行）之前，一切似乎都工作得很好。我能否获得额外的帮助来了解为什么会发生这种情况，因为我跟随教程的讲师没有遇到此问题。
尝试多次执行，但我遇到了如下图所示的相同问题： ]]></description>
      <guid>https://stackoverflow.com/questions/77870091/california-housing-price-prediction-exception</guid>
      <pubDate>Wed, 24 Jan 2024 01:23:03 GMT</pubDate>
    </item>
    <item>
      <title>Aws SageMaker 实时随机砍伐森林</title>
      <link>https://stackoverflow.com/questions/77869995/aws-sagemaker-random-cut-forest-real-time</link>
      <description><![CDATA[我正在尝试了解 Sagemaker 的工作原理。我的 OpenSearch 中有一些数据需要识别异常情况。
我知道我可以执行以下逻辑：

将我的 OpenSearch 数据导入为 CSV 或使用 SDK；
使用随机森林砍伐 (RCF) 算法；
在 SageMaker 中生成端点；

但是，我的 OpenSearch 数据是实时的，我希望实时预测有一个（实时）仪表板，我们可以在其中观察异常行为并可能生成某种警报。
当 OpenSearch 收到新数据时，是否有可能在 SageMaker 中自动运行查询并将结果显示在像 Grafana 这样的仪表板上？]]></description>
      <guid>https://stackoverflow.com/questions/77869995/aws-sagemaker-random-cut-forest-real-time</guid>
      <pubDate>Wed, 24 Jan 2024 00:43:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 创建自定义模型类</title>
      <link>https://stackoverflow.com/questions/77869443/create-custom-model-class-with-python</link>
      <description><![CDATA[我有一个示例类如下：
类 MLP(nn.Module)：
    # 声明一个带有模型参数的层。在这里，我们完全声明两个
    # 连接层
    def __init__(自身):
        # 调用`MLP`父类`Module`的构造函数来执行
        # 必要的初始化。这样，其他函数参数
        # 也可以在类实例化时指定，比如模型
        # 参数，`params`（稍后描述）
        超级().__init__()
        self.hidden = nn.Linear(20, 256) # 隐藏层
        self.out = nn.Linear(256, 10) # 输出层

    # 定义模型的前向传播，即如何返回
    # 基于输入“X”所需的模型输出
    def 向前（自身，X）：
        # 注意这里我们使用 ReLU 中定义的函数版本
        # nn.功能模块。
        返回 self.out(torch.relu(self.hidden(X)))

调用类是这样的：
net = MLP() net(X)
现在，我需要为 4 层模型创建类似的类和函数：
图层配置激活功能
全连接输入大小 128，输出大小 64 ReLU
全连接输入大小 64，输出大小 32 ReLU
辍学概率 0.5 -
全连接输入大小 32，输出大小 1 Sigmoid
我需要传递以下断言：
&lt;前&gt;&lt;代码&gt;模型 = Net()

断言 model.fc1.in_features == 128
断言 model.fc1.out_features == 64
断言 model.fc2.in_features == 64
断言 model.fc2.out_features == 32
断言 model.fc3.in_features == 32
断言 model.fc3.out_features == 1

x = 火炬.rand(2, 128)
输出 = model.forward(x)
断言 output.shape == (2, 1), “Net() 错误！”

这是我到目前为止所拥有的：
类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
                
        
        self.fc1 = nn.Linear(128, 64)
        self.fc2 = nn.Linear(64, 32)
        self.dropout = nn.Dropout(p=0.5)
        self.fc3 = nn.Linear(32, 1)
        

    def 前向（自身，x）：
        返回 self.fc3(torch.sigmoid(self.dropout(self.fc2(torch.relu(self.fc1(torch.relu(X)))))))
       

但是我收到错误：
运行时错误：mat1 和 mat2 形状无法相乘（2x20 和 128x64）

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77869443/create-custom-model-class-with-python</guid>
      <pubDate>Tue, 23 Jan 2024 21:44:52 GMT</pubDate>
    </item>
    <item>
      <title>将 xml 注释与图像链接以创建数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/77868119/link-xml-annotations-with-images-to-create-dataset</link>
      <description><![CDATA[我在一个文件夹中有一个数据集，该文件夹包含 2 个名为 train 的子文件夹，test 火车内部有两个子文件夹注释（它们是 XML 文件）和图像，它们是没有标签或边界框的数据集的图像，所以我想要将它们链接在一起以获得可以操作和训练模型的数据集
请注意，数据集位于我的本地计算机中，我计划使用 YOLOv5 和 google colab
如何将 XML 文件与图像链接并准备好可供使用的数据集？]]></description>
      <guid>https://stackoverflow.com/questions/77868119/link-xml-annotations-with-images-to-create-dataset</guid>
      <pubDate>Tue, 23 Jan 2024 17:15:41 GMT</pubDate>
    </item>
    <item>
      <title>如何将保存的模型从sklearn转换为tensorflow/lite</title>
      <link>https://stackoverflow.com/questions/59723922/how-to-convert-saved-model-from-sklearn-into-tensorflow-lite</link>
      <description><![CDATA[如果我想使用 sklearn 库实现分类器。有没有办法保存模型或将文件转换为已保存的 tensorflow 文件，以便稍后将其转换为 tensorflow lite？]]></description>
      <guid>https://stackoverflow.com/questions/59723922/how-to-convert-saved-model-from-sklearn-into-tensorflow-lite</guid>
      <pubDate>Mon, 13 Jan 2020 20:47:25 GMT</pubDate>
    </item>
    </channel>
</rss>