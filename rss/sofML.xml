<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 14 Mar 2024 21:12:59 GMT</lastBuildDate>
    <item>
      <title>如何在普通电脑上实际运行大数据</title>
      <link>https://stackoverflow.com/questions/78163349/how-to-actually-run-big-data-on-normal-pc</link>
      <description><![CDATA[我有一个大约 100GB 的数据集，我一直在尝试处理它，但我厌倦的所有解决方案在我的案例中都失败了。
我的 csv 数据集位于连接到我的电脑的外部硬盘上。我的电脑有 jupyter，但我尝试运行它，甚至只运行了 33%，它就崩溃了。我已经尝试过 jupyter lab 但还是不行。我尝试使用 SPLUNK 中的 jupyter，但无法将一个连接到另一个。
我有数据科学经验，但从未经历过这么大的事情。我的问题基本上是如何/在哪里分析这么大的数据集。我以为我可以使用我习惯的东西，但一切都失败了。]]></description>
      <guid>https://stackoverflow.com/questions/78163349/how-to-actually-run-big-data-on-normal-pc</guid>
      <pubDate>Thu, 14 Mar 2024 20:39:08 GMT</pubDate>
    </item>
    <item>
      <title>由于 ValueError，autoencoder.fit 不起作用</title>
      <link>https://stackoverflow.com/questions/78163348/autoencoder-fit-doesnt-work-becaue-of-a-valueerror</link>
      <description><![CDATA[我不明白我的问题是什么。它应该可以工作，只是因为它是张量流文档中的标准自动编码器。
这是错误
第 64 行，通话中
解码 = self.decoder(编码)
ValueError：调用 Autoencoder.call() 时遇到异常。
无效的数据类型：&lt;0x7fb471cc1c60 处的属性对象&gt;
Autoencoder.call() 收到的参数：
x=tf.Tensor(shape=(32, 28, 28), dtype=float32)
这是我的代码
(x_train, _), (x_test, _) = Fashion_mnist.load_data()

x_train = x_train.astype(&#39;float32&#39;) / 255.
x_test = x_test.astype(&#39;float32&#39;) / 255.

打印（x_train.shape）
打印（x_test.shape）

类自动编码器（模型）：
  def __init__(自身，latent_dim，形状)：
    super(自动编码器, self).__init__()
    self.latent_dim = Latent_dim
    self.shape = 形状
    self.encoder = tf.keras.Sequential([
      层.Flatten(),
      层.Dense（latent_dim，激活=&#39;relu&#39;），
    ]）
    self.decoder = tf.keras.Sequential([
      层.Dense（tf.math.reduce_prod（形状），激活=&#39;sigmoid&#39;），
      图层.重塑（形状）
    ]）

  def 调用（自身，x）：
    编码 = self.encoder(x)
    打印（编码）
    解码 = self.decoder(编码)
    打印（解码）
    返回解码后的内容


形状 = x_test.shape[1:]
潜伏暗度 = 64
自动编码器 = 自动编码器（latent_dim，形状）

autoencoder.compile(optimizer=&#39;adam&#39;, loss=losses.MeanSquaredError())

自动编码器.fit(x_train, x_train,
                纪元=10，
                随机播放=真，
                验证数据=（x_test，x_test））

我尝试更改数据库，也尝试了不同的形状]]></description>
      <guid>https://stackoverflow.com/questions/78163348/autoencoder-fit-doesnt-work-becaue-of-a-valueerror</guid>
      <pubDate>Thu, 14 Mar 2024 20:39:06 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习预测未来股票价格</title>
      <link>https://stackoverflow.com/questions/78163298/predicting-future-stock-prices-using-machine-learning</link>
      <description><![CDATA[我对此很陌生，并尝试在 python 中训练机器学习模型（xgb）来预测股票价格。我首先获取股票价格，然后计算技术指标以用作特征。然后，我将数据拆分为训练集和测试集，其中特征（modeling_df）为 X，收盘价（closes）为 Y。
我的问题是，我不确定该模型实际上是根据过去的价格来预测价格，但该模型是通过查看当前时间的特征来预测价格。我还想确保它使用“滚动窗口”，因此，如果有 30 个值，则值 11-20 应基于 0-10，而 21-30 应基于 0- 20.
如果有人知道神经网络的解决方案是否不同，那么我们也将不胜感激！
scaled_features = scaler.fit_transform(modeling_df)

X = 缩放特征
Y = 关闭

x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.05,shuffle=False)

然后我使用：
final_model = xgb.XGBRegressor(**best_params)
Final_model.fit(x_train,y_train, )

预测 = Final_model.predict(x_test)

然后我使用 matplotlib 显示它。
我尝试将整个数据集移动 10（和其他值），以尝试预测未来的 10 个数据点，但我不确定如何验证它是否确实有效。
这是我的图表（数据集没有被移动）。
烛台 + 蓝线 = 实际价格
紫色线 = 预测价格
图表：
]]></description>
      <guid>https://stackoverflow.com/questions/78163298/predicting-future-stock-prices-using-machine-learning</guid>
      <pubDate>Thu, 14 Mar 2024 20:30:48 GMT</pubDate>
    </item>
    <item>
      <title>使用convert_marian_to_pytorch.py​​脚本转换opus mt模型后出现的问题</title>
      <link>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</link>
      <description><![CDATA[利用convert_marian_pytorch.py​​ 脚本将 OPUS 转换模型转换为与 Hugging Face Transformers 兼容的格式后，我将生成的模型上传到 Hugging Face 的模型中心 此处。然而，经过测试，模型的翻译输出似乎是一堆随机单词，缺乏连贯性。
我正在寻求帮助来解决此问题或探索替代方法以直接在 Python 中使用 Marian 模型，而不依赖 OPUS cat mt 引擎。任何见解或建议将不胜感激。
经测试，模型的翻译输出似乎是一堆随机单词，缺乏连贯性。]]></description>
      <guid>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</guid>
      <pubDate>Thu, 14 Mar 2024 20:30:25 GMT</pubDate>
    </item>
    <item>
      <title>CNN中的核运动与图像处理中的卷积运算相同吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78162534/kernel-movement-in-cnn-is-same-as-convolution-operation-in-image-processing</link>
      <description><![CDATA[我们都知道图像处理中的卷积运算：

翻转蒙版并进行关联。
一维掩模水平翻转，因为只有一行。
2D 蒙版垂直和水平翻转。
遮罩在图像矩阵上从左向右滑动。
当蒙版悬停在图像上时，蒙版和图像的相应元素会相乘并添加产品。

但是在 CNN 中，我们通常不会执行，因为我们仅从左到右水平滑动内核并移动到下一行，并简单地对具有相应内核值的图像像素执行点积，这里我们只执行相关运算而不是卷积。
请解答我的疑问]]></description>
      <guid>https://stackoverflow.com/questions/78162534/kernel-movement-in-cnn-is-same-as-convolution-operation-in-image-processing</guid>
      <pubDate>Thu, 14 Mar 2024 17:57:13 GMT</pubDate>
    </item>
    <item>
      <title>如何使用mediapipline多类分割模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78162416/how-to-use-mediapipline-multiclass-segmentation-model</link>
      <description><![CDATA[我正在使用媒体管道构建我的微调模型。我已经在节点上实现了，但是你知道如何在pyton中使用衣服标签：4用于上衣
我尝试了 meidapipe 的这款笔记本 https://colab.research .google.com/drive/1IFABSWb7-mz0pSXjFizyU0tDZD2RVv8W#scrollTo=Yl_Oiye4mUuo
如果您有任何解决方案或笔记本，可以在评论中留言吗？我会喜欢的]]></description>
      <guid>https://stackoverflow.com/questions/78162416/how-to-use-mediapipline-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Mar 2024 17:35:36 GMT</pubDate>
    </item>
    <item>
      <title>根据历史数据预测用户最常问的 5 个问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/78160731/predicting-top-5-frequently-asked-questions-by-users-based-on-historical-data</link>
      <description><![CDATA[问题陈述：
我有一个数据集，其中包含用户 ID 以及他们提出的问题。我的目标是根据每个用户的历史数据预测他们最有可能问的前 5 个问题。此任务涉及利用用户过去的问题历史记录来准确预测他们未来的查询。
要点：
数据集：数据集由两列组成：用户 ID 和每个用户提出的相应问题。
预测目标：目标是建立一个模型，可以分析每个用户的历史问题数据，并通过提供用户 ID 来预测他们将来最有可能问的前 5 个问题。
总体而言，我们的目标是开发一个有效的预测模型，该模型可以预测每个用户可能会问的前 5 个问题，从而根据用户的历史行为改善用户互动和满意度。
我尝试了各种聚类算法来根据用户的历史数据预测用户最有可能问的前 5 个问题。然而，这些方法的准确性并不令人满意。我正在寻找可以提高我的预测准确性的替代方法或方法。]]></description>
      <guid>https://stackoverflow.com/questions/78160731/predicting-top-5-frequently-asked-questions-by-users-based-on-historical-data</guid>
      <pubDate>Thu, 14 Mar 2024 13:04:26 GMT</pubDate>
    </item>
    <item>
      <title>使用遗传算法优化面部情绪识别模型超参数</title>
      <link>https://stackoverflow.com/questions/78157230/optimizing-facial-emotion-recognition-model-hyperparameters-using-genetic-algori</link>
      <description><![CDATA[我正在构建一个面部情绪识别系统，可以对快乐、悲伤、愤怒、惊讶等情绪进行分类。我已经使用 TensorFlow/Keras 训练了一个卷积神经网络模型，目前它的准确率达到了50%左右。然而，我相信微调超参数可能会进一步提高准确性。
现在，我有兴趣优化模型的超参数以实现更高的准确性。我听说过使用遗传算法进行超参数优化，但我不确定如何继续。有人可以指导我如何应用遗传算法来微调模型的超参数吗？具体来说，如何修改我的代码以纳入遗传算法以进行超参数优化？
这是我的代码摘要：
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras导入模型，层

# 数据增强
增强器 = ImageDataGenerator(
    重新缩放=1.0/255，
    剪切范围=0.2，
    缩放范围=0.2，
    水平翻转=真
）

# 加载数据并将图像大小调整为 48x48 像素
Augmented_trained_data = Augmentor.flow_from_directory(
    “面部识别数据集/训练”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

Augmented_validation_data = Augmentor.flow_from_directory(
    “面部识别数据集/验证”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

Augmented_testing_data = Augmentor.flow_from_directory(
    “面部识别数据集/测试”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

# 模型定义
模型 = models.Sequential([
    层.Conv2D(32, (2, 2), 激活=“relu”, input_shape=(48, 48, 1)),
    层.MaxPool2D((2, 2)),
    层.Conv2D(64, (2, 2), 激活=“relu”),
    层.MaxPool2D((2, 2)),
    层.Conv2D(128, (2, 2), 激活=“relu”),
    层.MaxPool2D((2, 2)),
    层.Flatten(),
    层.密集（128，激活=“relu”），
    层数.Dropout(0.25),
    层.密集（6，激活=“softmax”）
]）

# 模型编译
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
    指标=[“准确度”]
）

# 模型训练
模型.拟合(
    增强训练数据，
    验证数据=增强验证数据，
    纪元=10
）

# 模型评估
test_loss, test_accuracy = model.evaluate(augmented_testing_data)
print(f&quot;测试准确度: {test_accuracy * 100:.2f}%&quot;)&#39;&#39;&#39;


]]></description>
      <guid>https://stackoverflow.com/questions/78157230/optimizing-facial-emotion-recognition-model-hyperparameters-using-genetic-algori</guid>
      <pubDate>Wed, 13 Mar 2024 22:53:36 GMT</pubDate>
    </item>
    <item>
      <title>无法将保存的 keras 模型转换为 TFLite</title>
      <link>https://stackoverflow.com/questions/78156242/cant-convert-saved-keras-model-to-tflite</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78156242/cant-convert-saved-keras-model-to-tflite</guid>
      <pubDate>Wed, 13 Mar 2024 19:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在设置依赖项时如何修复此错误？</title>
      <link>https://stackoverflow.com/questions/78155493/how-can-i-fix-this-error-while-i-was-setting-up-dependencies</link>
      <description><![CDATA[我只是想为我的机器学习作业设置依赖关系
!apt-get install -y xvfb python-opengl &gt; /dev/null 2&gt;&amp;1
！pip installgym pyvirtualdisplay&gt; /dev/null 2&gt;&amp;1
！pip installgym pyvirtualdisplay&gt; /dev/null 2&gt;&amp;1
!apt-get install -y xvfb python-opengl ffmpeg &gt; /dev/null 2&gt;&amp;1
!pip 安装gym[classic_control]
!apt-get 更新 &gt; /dev/null 2&gt;&amp;1
!apt-get install cmake &gt; /dev/null 2&gt;&amp;1
!pip install --upgrade setuptools 2&gt;&amp;1
!pip install ez_setup &gt; /dev/null 2&gt;&amp;1

我收到此错误输出，显示某些安装正确，但有很多“系统找不到指定的路径。”
系统找不到指定的路径。

还有另一个错误
错误：子进程退出并出现错误
  
  python setup.py Egg_info 未成功运行。
  退出代码：1
  
  [77行输出]
  
  
  警告，没有“设置”文件存在，正在运行“buildconfig/config.py”
  使用WINDOWS配置...
  
 
  未找到 FREETYPE 的路径。
  ...在 prebuilt-x64 中发现包含目录但没有库目录。
  找不到 PNG 的路径。
  ...在 prebuilt-x64 中发现包含目录但没有库目录。
  未找到 JPEG 的路径。
  ...在 prebuilt-x64 中发现包含目录但没有库目录。
  freetype 的 DLL：prebuilt-x64/SDL2_ttf-2.0.15/lib/x64/libfreetype-6.dll
  
  ---
  如需编译帮助，请参阅：
      https://www.pygame.org/wiki/CompileWindows
  要为 pygame 开发做出贡献，请参阅：
      https://www.pygame.org/contribute.html
  ---
  
  [输出结束]
  
  注意：此错误源自子进程，并且可能不是 pip 的问题。
错误：元数据生成失败

生成包元数据时遇到错误。

输出见上文。

注意：这是上面提到的包的问题，​​而不是 pip 的问题。
提示：详细信息请参见上文。

我尝试在谷歌上搜索答案，也尝试过 chatgpt，但没有一个能给我答案。]]></description>
      <guid>https://stackoverflow.com/questions/78155493/how-can-i-fix-this-error-while-i-was-setting-up-dependencies</guid>
      <pubDate>Wed, 13 Mar 2024 16:44:29 GMT</pubDate>
    </item>
    <item>
      <title>使用文本特征的二元分类导致 AUC 分数非常低 [关闭]</title>
      <link>https://stackoverflow.com/questions/78154496/binary-classification-using-textual-features-results-in-very-low-auc-scores</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78154496/binary-classification-using-textual-features-results-in-very-low-auc-scores</guid>
      <pubDate>Wed, 13 Mar 2024 14:20:24 GMT</pubDate>
    </item>
    <item>
      <title>在python中创建线性回归模型的问题</title>
      <link>https://stackoverflow.com/questions/78152862/problem-with-creating-a-linear-regression-model-in-python</link>
      <description><![CDATA[我有一个数据库，其中包含一个城市的多个属性：
该数据库中保存了各种属性（约 19,000 个）。每个房产都有一些特征，例如：销售价格、房产面积、浴室数量、建造年份、上市天数......
我是机器学习算法编程的新手，希望首先编写一个简单的线性回归模型，该模型可以根据其他数据预测该房产的上市天数。
数据保存在Excel中。
这就是我所做的：
从 sklearn.linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.metrics 导入mean_squared_error, r2_score
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
导入请求


模型=线性回归()
data=pd.read_excel(r“C:\Users....”)

X=np.array(data.drop([“daysOnMarket”], axis=1))
Y=np.array(数据[“daysOnMarket”])
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)
model.fit(x_train, y_train)
y_pred=模型.预测(x_test)
print(model.score(x_test, y_test))
打印（均方误差（y_test，y_pred））

现在让我觉得我做错了的是，我得到的分数是 0.9999852324248868，均方误差是 0.07659752059595726
现在我不明白这是一个过度拟合问题还是我只是在编程中做错了什么。
谁能帮我找出问题出在哪里吗？
这是我的数据示例：
]]></description>
      <guid>https://stackoverflow.com/questions/78152862/problem-with-creating-a-linear-regression-model-in-python</guid>
      <pubDate>Wed, 13 Mar 2024 10:06:17 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试通过训练迭代计算余弦相似度时，如何选择模型权重[关闭]</title>
      <link>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</link>
      <description><![CDATA[我正在尝试计算“t”步骤的权重值和“t-1”步骤的权重值之间的余弦相似度。
我正在使用 LSTM 网络 &amp; 2FC层。
我想检查训练期间的体重差异。
但是如果我想检查这些事情，我想知道我是只选择 LSTM 层的权重还是全部（LSTM，2FC 权重）或最终层的权重（FC_2 权重）
我不知道如何在余弦相似条件下优化选择图层的最佳解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</guid>
      <pubDate>Wed, 13 Mar 2024 08:34:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Lime 需要训练数据来计算局部解释</title>
      <link>https://stackoverflow.com/questions/76370183/why-does-lime-need-training-data-to-compute-local-explanations</link>
      <description><![CDATA[我正在使用 Lime 来计算局部解释，但是我不明白为什么我必须在下面的代码行中传递训练数据 X_train
explainer = Lime_tabular.LimeTabularExplainer(X_train, mode=“回归”, feature_names= boston.feature_names)

下面是关于 Lime 如何运作的摘录，摘自这本名为  的伟大书籍可解释的机器学习，作者：Christoph Molnar，围绕 XAI -
&lt;块引用&gt;
训练本地代理模型的秘诀：

选择您感兴趣的实例，了解其黑盒预测的解释。
扰动您的数据集并获取这些新点的黑盒预测。
根据新样本与感兴趣实例的接近程度对新样本进行加权。
在具有变化的数据集上训练加权、可解释的模型。
通过解释本地模型来解释预测。


如果我理解正确的话，Lime 通过从其邻域采样点来为每个感兴趣的实例训练一个加权可解释模型。分配给该模型中的特征的权重充当该特定实例的局部解释。
这正是我们在下面的代码行中所做的 -
exp =explainer.explain_instance(X_test.values[3], model.predict, num_features=6)
我们传递一个实例，使用该实例将计算适合可解释模型的邻居。那么为什么我们要在第一行代码中传递X_train呢？我不明白 Lime 如何利用它。]]></description>
      <guid>https://stackoverflow.com/questions/76370183/why-does-lime-need-training-data-to-compute-local-explanations</guid>
      <pubDate>Wed, 31 May 2023 04:28:17 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中学习隐马尔可夫模型</title>
      <link>https://stackoverflow.com/questions/48341856/learning-hidden-markov-model-in-r</link>
      <description><![CDATA[隐马尔可夫模型 (HMM) 是一种观察一系列观察结果的模型，但不知道模型生成观察结果所经历的状态序列。隐马尔可夫模型的分析旨在从观察到的数据中恢复隐藏状态的序列。
我有包含观察值和隐藏状态的数据（观察值具有连续值），其中隐藏状态由专家标记。我想训练一个 HMM，它能够基于（以前未见过的）观察序列来恢复相应的隐藏状态。
有没有 R 包可以做到这一点？研究现有的包（depmixS4、HMM、seqHMM - 仅适用于分类数据）允许您仅指定多个隐藏状态。 
编辑：
示例：
data.tagged.by.expert = data.frame(
    hidden.state = c(&quot;唤醒&quot;, &quot;REM&quot;, &quot;REM&quot;, &quot;NonREM1&quot;, &quot;NonREM2&quot;, &quot;REM&quot;, &quot;REM&quot;, &quot;唤醒&quot;),
    传感器1 = c(1,1.2,1.2,1.3,4,2,1.78,0.65),
    传感器2 = c(7.2,5.3,5.1,1.2,2.3,7.5,7.8,2.1),
    传感器3 = c(0.01,0.02,0.08,0.8,0.03,0.01,0.15,0.45)
 ）

data.newly.measured = data.frame(
    传感器1 = c(2,3,4,5,2,1,2,4,5,8,4,6,1,2,5,3,2,1,4),
    传感器2 = c(2.1,2.3,2.2,4.2,4.2,2.2,2.2,5.3,2.4,1.0,2.5,2.4,1.2,8.4,5.2,5.5,5.2,4.3,7.8),
    传感器3 = c(0.23,0.25,0.23,0.54,0.36,0.85,0.01,0.52,0.09,0.12,0.85,0.45,0.26,0.08,0.01,0.55,0.67,0.82,0.35)
 ）

我想创建一个具有离散时间t的隐马尔可夫模型，其中随机变量x(t)代表时间t的隐藏状态， x(t)  {&quot;Wake&quot;, &quot; REM&quot;、&quot;NonREM1&quot;、&quot;NonREM2&quot;} 和 3 个连续随机变量 sensor1(t)、sensor2(t)、sensor3(t) 代表时间  的观测值t。 
model.hmm = learn.model(data.tagged.by.user)

然后我想使用创建的模型来估计负责新测量观测的隐藏状态
hidden.states =estimate.hidden.states(model.hmm, data.newly.measured)
]]></description>
      <guid>https://stackoverflow.com/questions/48341856/learning-hidden-markov-model-in-r</guid>
      <pubDate>Fri, 19 Jan 2018 13:07:57 GMT</pubDate>
    </item>
    </channel>
</rss>