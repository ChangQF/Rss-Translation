<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 07 May 2024 06:20:22 GMT</lastBuildDate>
    <item>
      <title>“管道”对象没有属性“_check_fit_params”</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[来自 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler
从 imblearn.pipeline 导入管道

# 定义特征和目标
X = df.drop(&#39;感染&#39;, axis=1)
y = df[&#39;感染&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样到多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 将多数类欠采样至其原始大小的 80%

管道 = 管道(步骤=[(&#39;o&#39;, 上), (&#39;u&#39;, 下)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(“重采样的类分布：”, pd.Series(y_resampled).value_counts())

这是我的代码
并且
这是我遇到的错误
AttributeError Traceback（最近一次调用最后一次）
单元格 In[7]，第 19 行
     16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
     18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
     21 # 显示新的班级分布
     22 print(&quot;重采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372，在 Pipeline.fit_resample(self, X, y, **fit_params)
    第342章
    第343章
    第344章 一个接一个地安装所有变压器/采样器并且
   （...）
    第369章 变形的目标。
    第370章
    第371章
--&gt;第372章
    第373章
    第374章

AttributeError：“管道”对象没有属性“_check_fit_params”

我已经尝试了一切。我的所有包都已更新。以及我尝试使用的所有方法，查看 SKLearn 和 IBLEARN 这两个网站。请有人帮忙..紧急]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>DeepAR LSTM 使用动态特征进行预测？</title>
      <link>https://stackoverflow.com/questions/78440282/deepar-lstm-using-dynamic-features-to-forecast</link>
      <description><![CDATA[我正在使用 DeepAR 的 gluonts.torch 实现来使用搜索量作为因变量/输入变量/动态变量来预测未来的销售。
我的数据如下所示：
日期销售搜索
2018年12月30日 205 13
2019-01-06 245 19
2019-01-13 207 20
2019-01-20 221 21
2019-01-27 179 17
…………
2023年11月26日 142 11
2023年12月3日 183 13
2023年12月10日 211 14
2023年12月17日 236 14
2023年12月24日 275 15

我做了以下事情：
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np

从 gluonts.dataset.pandas 导入 PandasDataset
从 gluonts.torch 导入 DeepAREstimator
从 gluonts.evaluation 导入 make_evaluation_predictions，评估器

#读入数据
sales = pd.read_csv(&#39;data/sales.csv&#39;,index_col=0, parse_dates=True)

#分割训练和测试
预测长度=52
to_pandasdataset = lambda 数据: PandasDataset(
    数据，目标=“销售”，
    feat_dynamic_real=[“搜索”]
）
train = to_pandasdataset(sales.iloc[:-prediction_length,:])
测试= to_pandasdataset（销售）

#训练和预测
estimator = DeepAREstimator(freq=“1W”, Prediction_length=prediction_length, num_layers=4, num_feat_dynamic_real=1[![在此处输入图像描述][1]][1],hidden_​​size=14,trainer_kwargs={&#39;accelerator&#39;: &#39; cpu&#39;, &#39;max_epochs&#39;:10})

预测器 = estimator.train(train,num_workers=8)
Forecast_it，ts_it = make_evaluation_predictions（数据集=测试，预测器=预测器）
预测 = 列表(forecast_it)
测试=列表（ts_it）
评估器 = 评估器(分位数=(np.arange(20) / 20.0)[1:])
agg_metrics, item_metrics = evaluator(测试, 预测, num_series=len(测试))

＃阴谋

图, ax = plt.subplots(1, 1, Figsize=(12, 6))
#ax.plot(测试[0][-10 *预测长度:].to_timestamp())
ax.plot(测试[0].to_timestamp())
plt.sca(ax)
预测[0].plot(间隔=(0.9,))

# 在训练期结束时添加一条垂直线

最后训练日期 = sales.index[-预测长度]
plt.axvline(x=last_training_date, color=&#39;red&#39;, linestyle=&#39;--&#39;)

plt.legend([“观测值”,“预测中值”,“90%预测区间”,“训练结束”])
plt.xlabel(“日期”)
plt.ylabel(“”)
plt.show()

预测看起来很糟糕：

我认为它不好的原因是销售和搜索之间的相关性非常高：

我在训练时是否遗漏了某些内容，或者模型在测试/预测时未采用动态特征值？怎么表现这么差？有了这种高相关性，简单的线性回归很容易胜过 DeepAR，对吗？]]></description>
      <guid>https://stackoverflow.com/questions/78440282/deepar-lstm-using-dynamic-features-to-forecast</guid>
      <pubDate>Tue, 07 May 2024 05:24:22 GMT</pubDate>
    </item>
    <item>
      <title>如何标记多列、多类别数据集并将其保存到 CSV 或 Parquet 并使用 SVM 对其进行训练</title>
      <link>https://stackoverflow.com/questions/78440187/how-to-label-a-multicolumn-multi-category-dataset-and-save-it-to-a-csv-or-parqu</link>
      <description><![CDATA[我正在使用 OPENSMILE 库进行音频分类。预处理音频数据后，我得到一个 800x25 形状的数据，该数据仅适用于一个文件（每个文件大约 15 秒长）
为了训练这个数据集和为了可移植性，我想转换为 CSV 文件，我总共有 5 个文件夹（data/category_0 ... data/category_4），每个类别在处理时都有大约 50 个文件（.wav）使用 Opensmile 处理后的文件 data/category_0/audio1.wav 之一，我得到了形状为 (800x25) 的 pd 数据框，当数据如下时，我应该如何训练多类分类
5个文件夹
每个文件夹 20 个文件
每个文件的输出形状为 (800x25)
导入操作系统
导入时间

将 numpy 导入为 np
将 pandas 导入为 pd

导入音频文件
导入opensmile

微笑= opensmile.Smile(
    feature_set=opensmile.FeatureSet.eGeMAPSv02,
    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,
    多处理=真，
    详细=真
）
k = smile.process_signal(
    信号，
    采样率
）


我们可以看到有多个子列开始和结束，我如何将此数据集正确存储到 csv 文件中，这是不可能的，我至少如何训练它们
我尝试重塑尺寸，但仍然使用并且它弄乱了数据集的形状]]></description>
      <guid>https://stackoverflow.com/questions/78440187/how-to-label-a-multicolumn-multi-category-dataset-and-save-it-to-a-csv-or-parqu</guid>
      <pubDate>Tue, 07 May 2024 04:53:37 GMT</pubDate>
    </item>
    <item>
      <title>Pandas 代码在 datacamp 实践中是错误的 [关闭]</title>
      <link>https://stackoverflow.com/questions/78439826/pandas-code-is-wrong-in-datacamp-practical</link>
      <description><![CDATA[&lt;块引用&gt;
实践考试：房屋销售
Real Agents 是一家专注于销售房屋的房地产公司。
Real Agents 在一个大都市区销售多种类型的房屋。
有些房屋销售缓慢，有时需要降低价格才能找到买家。
为了保持竞争力，Real Agents 希望优化其试图出售的房屋的挂牌价格。
他们希望通过根据房屋的特征预测其售价来实现这一目标。
如果他们能够提前预测销售价格，就可以缩短销售时间。
数据
数据集包含该地区以前出售的房屋的记录。

&lt;标题&gt;

列
姓名
标准


&lt;正文&gt;

house_id
标称。
房屋的唯一标识符。不可能缺少值。


城市
标称。
房屋所在的城市。 “Silvertown”、“Riverford”、“Teasdale”和“Poppleton”之一。将缺失值替换为“未知”。


促销价
离散。
房屋的售价（以美元计）。值可以是任何大于或等于零的正数。删除缺失的条目。


促销日期
离散。
最后一次出售房屋的日期。将缺失值替换为 2023-01-01。


列出的月份
连续。
房屋在最后一次销售之前在市场上挂牌的月数，四舍五入到小数点后一位。将缺失值替换为列出的平均月数，精确到小数点后一位。


卧室
离散。
房子里的卧室数量。任何大于或等于零的正值。将缺失值替换为平均卧室数，四舍五入到最接近的整数。


房屋类型
序数。
“梯田”之一（两堵共用墙）、“半独立式” （一堵共用墙），或“独立”。 （无共用墙）。用最常见的房屋类型替换缺失值。


区域
连续。
房屋面积，以平方米为单位，四舍五入到小数点后一位。将缺失值替换为平均值，精确到小数点后一位。



任务1
Real Agents 的团队知道房产所在的城市会对售价产生影响。
不幸的是，他们认为这并不总是记录在数据中。
计算城市缺失值的数量。
您应该使用文件“house_sales.csv”中的数据。
您的输出应该是一个对象missing_city，其中包含此列中缺失值的数量。
所有必需的数据都应该已创建，并具有所需的列，并识别和替换缺失的值。

将 pandas 导入为 pd

# 从 CSV 文件加载数据集
数据 = pd.read_csv(“house_sales.csv”)

# 检查“city”中是否有缺失值柱子
missing_city = data[“city”].isnull().sum()

# 替换“城市”中缺失的值带有“未知”的列
data[“城市”].fillna(“未知”, inplace=True)

在这段代码中发现错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78439826/pandas-code-is-wrong-in-datacamp-practical</guid>
      <pubDate>Tue, 07 May 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>将任意深度转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</link>
      <description><![CDATA[我正在尝试将现有的 深度-anything PyTorch 模型转换为 CoreML 格式。我决定使用 Google Colab 并采取了以下内容： 推理深度任意模型的注释。但是，我在尝试将其导入 iOS 端时遇到了一些异常。这是我的转换代码片段：
# 安装所有需要的扩展
!pip 安装 coremltools
# ...

将 coremltools 导入为 ct
进口火炬

# 将 PyTorch 模型转换为 TorchScript
追踪模型 = torch.jit.trace(深度_任何东西, torch.rand(1, 3, 518, 518))

# 将 TorchScript 模型转换为 CoreML
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”,形状=(1,3,518,518),比例=1/255.0)]
）

输出 = model_coreml._spec.description.output[0]
输出.type.imageType.colorSpace = ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
输出.类型.图像类型.宽度 = 518
输出.类型.图像类型.高度 = 518

# 保存修改后的CoreML模型
打印（model_coreml）
model_coreml.save(&#39;/content/drive/MyDrive/trained_models/深度9.mlpackage&#39;)

我尝试直接指定输入参数，就像我为输出参数所做的那样：
# 为输入模式创建字典
input_schema = {&#39;input_name&#39;: &#39;输入&#39;, &#39;input_type&#39;: ct.TensorType(shape=(1, 3, 518, 518))}

# 将输入模式添加到模型的元数据中
model_coreml.user_define_metadata[&#39;inputSchema&#39;] = str(input_schema)

或者使用convert_to选项设置neuralnetwork，如下所示：
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”,形状=(1,3,518,518),比例=1/255.0)],
    Convert_to=&#39;神经网络&#39;
）

或者使用BGR/GRAYSCALE设置ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
没有任何帮助。
如果我尝试使用neuralnetwork后端导入模型，我只会收到无限加载。如果我尝试使用 mlprogram 后端导入模型（默认，如果未指定），我会收到以下信息：

我期待任何建议和帮助，因为我需要的只是转换现有的 深度任意 模型，无需对 CoreML 格式进行调整或更改。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</guid>
      <pubDate>Tue, 07 May 2024 01:53:36 GMT</pubDate>
    </item>
    <item>
      <title>如何以编程方式设置 W&B 运行失败警报？</title>
      <link>https://stackoverflow.com/questions/78439701/how-to-programmatically-set-alerts-for-failure-in-a-wb-run</link>
      <description><![CDATA[如何以编程方式设置 W&amp;B 运行失败警报？
我正在尝试在 W&amp;B（权重和偏差）项目中设置警报，以便在运行失败时通知我。我一直在测试几个我认为根据我的研究可以工作的函数，但似乎没有一个在 W&amp;B API 中实现。这是我尝试设置这些通知的代码片段：
导入 wandb

模式 = &#39;空运行&#39;
运行名称 = &#39;我的运行&#39;
批次数量 = 50
路径=&#39;/数据&#39;
名称 = &#39;实验1&#39;
今天 = &#39;2023-08-01&#39;
概率 = [0.1, 0.9]
批量大小 = 32
data_mixture_name = &#39;mix1&#39;

调试 = 模式 == &#39;dryrun&#39;
run = wandb.init(mode=mode,project=“超越规模”,name=run_name,save_code=True)
wandb.config.更新（{
    “num_batches”：num_batches，
    “路径”：路径，
    “姓名”：姓名，
    “今天”：今天，
    “概率”：概率，
    &#39;batch_size&#39;：batch_size，
    “调试”：调试，
    &#39;data_mixture_name&#39;：data_mixture_name
})

# 尝试设置通知
run.notify_on_failure()
run.notify_on_crash()
run.notify_on_exit()
run.notify_on_heartbeat()
run.notify_on_abort()

每次尝试都会导致 AttributeError，表明“Run”对象没有此类属性。例如：
AttributeError：“Run”对象没有属性“notify_on_failure”

在 W&amp;B 中是否有正确的方法来设置故障或其他警报？如果是这样，我应该如何修改我的方法？
参考：https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891]]></description>
      <guid>https://stackoverflow.com/questions/78439701/how-to-programmatically-set-alerts-for-failure-in-a-wb-run</guid>
      <pubDate>Tue, 07 May 2024 01:17:24 GMT</pubDate>
    </item>
    <item>
      <title>为 Windows 11 和 AMD GPU 安装 Pytorch</title>
      <link>https://stackoverflow.com/questions/78439640/installing-pytorch-for-windows-11-and-amd-gpu</link>
      <description><![CDATA[有人能帮我安装 Pytorch 吗？我的设备目前使用 Windows 操作系统和 AMD GPU。但是，Pytorch 安装不支持 Windows 操作系统与 ROCm 组合。只有选择 Linux 操作系统时，ROCm 选项才可用。
我可以使用 CUDA 工具包代替 ROCm 吗？或者我以某种方式将操作系统更改为 Linux？有没有办法绕过所有这些，并且仍然能够使用 Pytorch？
任何建议都将不胜感激！
我曾尝试在 youtube 上寻找安装教程，但它们没有与我相同的操作系统和 GPU 组合。（即 Windows 操作系统和 AMD GPU）]]></description>
      <guid>https://stackoverflow.com/questions/78439640/installing-pytorch-for-windows-11-and-amd-gpu</guid>
      <pubDate>Tue, 07 May 2024 00:46:02 GMT</pubDate>
    </item>
    <item>
      <title>关于 ML + 计算机视觉项目的建议</title>
      <link>https://stackoverflow.com/questions/78439584/advice-regarding-a-ml-computer-vision-project</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我需要帮助来改进它和所有步骤，例如 -
CNN 在这方面有何帮助？
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。
任何帮助、意见或建议。
这是我的第一个研究项目，请详细回答所有内容，我仍在学习中。 0_0]]></description>
      <guid>https://stackoverflow.com/questions/78439584/advice-regarding-a-ml-computer-vision-project</guid>
      <pubDate>Tue, 07 May 2024 00:12:44 GMT</pubDate>
    </item>
    <item>
      <title>R 中的 PCA 时间序列索引：分数图与预期索引不匹配</title>
      <link>https://stackoverflow.com/questions/78439533/pca-time-series-index-in-r-score-plot-doesnt-match-expected-index</link>
      <description><![CDATA[我正在致力于在 R 中创建 PCA 索引以了解它的工作原理。为此，我使用了“弥补”数据。然而，当我绘制第一个组件的分数时，结果并不符合预期。具体来说，分数图显示该指数在第一年表现不佳，然后随着时间的推移迅速提高，这与我的预期相反。
pca &lt;- prcomp（数据，比例= TRUE，中心= TRUE）
分数 &lt;- 比例（数据）%*% pca$rotation[, 1]
情节（分数）

我试图找出我可能做错了什么，或者如何更好地解释结果。任何见解或建议将不胜感激。

数据在这里]]></description>
      <guid>https://stackoverflow.com/questions/78439533/pca-time-series-index-in-r-score-plot-doesnt-match-expected-index</guid>
      <pubDate>Mon, 06 May 2024 23:48:12 GMT</pubDate>
    </item>
    <item>
      <title>Transformers.js 扩展在关闭扩展选项卡后一次又一次地重新下载模型</title>
      <link>https://stackoverflow.com/questions/78439242/transformers-js-extension-is-redownloading-the-model-again-and-again-after-closi</link>
      <description><![CDATA[我正在尝试使用 Transformers.js 库构建一个 Chrome 扩展，以测试其限制并查看 onDevice ML 在某些情况下是否可以作为选择。
根据他们的官方仓库中提供的示例，我添加新功能。
当我添加翻译管道时，我看到了一些奇怪的事情，即每次打开和关闭浏览器后模型都会一次又一次下载。
class MyTranslationPipeline {
静态任务=“翻译”；
静态模型＝“Xenova/nllb-200-distilled-600M”；
静态实例= null；

静态异步 getInstance(progress_callback = null) {
    if (this.instance === null) {
        console.log(“正在加载翻译管道...”);
        this.instance = pipeline(this.task, this.model, {progress_callback});
    }

    返回这个实例；
 }
}

原始示例展示了 env.allowLocalModels = false; 但我将其设置回 true，但它不起作用，我希望当我重新打开浏览器时，将从中加载模型缓存它已经在的地方。
缓存条目]]></description>
      <guid>https://stackoverflow.com/questions/78439242/transformers-js-extension-is-redownloading-the-model-again-and-again-after-closi</guid>
      <pubDate>Mon, 06 May 2024 21:40:33 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中 2D 输入的集成梯度实现</title>
      <link>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</link>
      <description><![CDATA[我正在尝试为 GNN 实现积分梯度计算（在文章中描述）我正在与.具体来说，我使用论文中的Eq3
我的网络输入是一个 NxM 矩阵，表示 N 个顶点的图，每个顶点都有一个 M 维的特征向量。
考虑到他们是针对 N 维输入推导出该方法，并且在我的情况下，我想为每个节点获得一个分数，如何扩展论文中的方法？
在我当前的实现中，我获得了一个 NxM 矩阵，通过执行 torch.sum(dim=1) 来折叠该矩阵，但这感觉并不那么干净
这是我当前在 PyTorch 中的实现
graph_copy = input_graph.detach().clone()
# 公式中的k/m
k_m = torch.linspace(0, 1, n_steps)
# 存储样本的数组
input_features = [baseline.clone()] # 第一个样本是基线
# x-x&#39;
diff = 原始输入特征 - 基线

# 填充样本数组
对于范围内的 i(1, k_m.shape[0])：
    temp = 基线 + k_m[i] * diff
    input_features_path.append（临时）

梯度= []
对于范围内的 i(k_m.shape[0])：
    ### 将每个输入的向量梯度归零
    input_features[i].requires_grad = True
    model.zero_grad()
    任务.zero_grad()
    
    temp_model_out = 模型（图 = graph_copy，输入 = input_features[i]）[&#39;graph_feature&#39;][0]
    temp_mlp_output = 任务.mlp(temp_model_out)
    temp_prob = Softmax(temp_mlp_output, 暗淡 = 0)

    temp_gradient = grad(输出 = temp_prob[true_label_id], 输入 = input_features_path[i])
    梯度.append(temp_gradient[0])

    input_features_path[i].requires_grad = False

使用 torch.no_grad()：
    ig_scores = original_input_feature * torch.stack(梯度, 暗淡 = 2). 平均值(暗淡 = 2)
    Final_ig_scores = ig_scores.sum(dim = 1)
    排序，索引= torch.sort（final_ig_scores，降序= True）

]]></description>
      <guid>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</guid>
      <pubDate>Mon, 06 May 2024 18:08:43 GMT</pubDate>
    </item>
    <item>
      <title>关于用于食谱成分提取的 spaCy NER 模型注释的反馈</title>
      <link>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</link>
      <description><![CDATA[我正在训练一个 spaCy NER 模型来专门识别和分类食谱中的成分线。目标是从各种食谱中准确提取成分及其数量、单位和制备说明。下面是我如何注释数据的概述：
跨越标签

数量：与单位相关的数字（例如“2”、“3/4”）
成分：实际成分名称（例如“糖”、“牛奶”）
测量单位：测量单位（例如“杯”、“汤匙”）
说明：准备说明（例如“切细丁”、“分开”）

关系标签

quantity_of：将跨度标签数量与成分关联起来
action_to：将跨度标签说明与成分相关联
unit_of：将跨度标签测量单位与数量相关

我提供了两个示例：一个简单的成分系列和一个复杂的成分系列。
简单行
复杂线
我正在寻求有关以下几点的反馈：

注释过度：是否存在太多类别或过于详细的注释，无法有效学习和实际应用？是否应该合并或省略某些类别？
训练数据量：通常建议使用多少带注释的数据来在此类 NER 任务中实现稳健的模型？我想确保该模型在各种配方格式中都是可靠的。

我尝试过的：
我使用类似的标签设置训练了一个模型，减去关系标签和“测量单位”。跨越标签。我使用来自随机食谱的约 700 个样本对模型进行了训练。结果不佳；该模型很难识别某些方面，特别是在数量及其单位不相邻的情况下（例如 3 瓣大蒜）]]></description>
      <guid>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</guid>
      <pubDate>Mon, 06 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>如何根据掩蔽将矩阵相乘并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>通过定义和 ROC 方法在 Python 中计算准确率（基尼系数）</title>
      <link>https://stackoverflow.com/questions/73031395/accuracy-ratio-gini-coef-computation-in-python-by-definition-and-roc-method</link>
      <description><![CDATA[为什么以下计算准确率的方法会给出不同的结果？
方法 1：累积精度曲线 (CAP) 曲线
准确率的计算公式为：训练模型的 CAP 曲线下面积与随机模型的 CAP 曲线下面积之差，除以完美模型的 CAP 曲线下面积与随机模型的 CAP 曲线下面积之差。随机模型。
方法 2：接受者操作特征 (ROC) 曲线。
我们计算 ROC 曲线下的面积，并使用统计量
AR = 基尼 = 2*（ROC 曲线下面积）- 1
有关统计数据的推导，请参阅论文“衡量评级系统的判别力” (https:/ /www.bundesbank.de/resource/blob/704150/b9fa10a16dfff3c98842581253f6d141/mL/2003-10-01-dkp-01-data.pdf)
有关我正在使用的代码的来源和更多示例，请参阅文章“使用 ROC 进行机器学习分类器评估” (https://towardsdatascience.com/machine -学习分类器评估使用-roc-and-cap-curves-7db60fe6b716)
接下来，我们有一个实际测试标签的向量y_test，以及输出的正类（label=1）的预测概率的向量test_pred_probs来自一些预测模型。我们计算该预测的准确率。
from sklearn.metrics import roc_curve, auc, roc_auc_score
将 numpy 导入为 np
y_测试 = [0,1,1,0,1,1,1,0,0,1]
test_pred_probs = [0.2,0.4,0.1,0,0,1,0.9,0.3,0.2,0.8]
总计 = len(y_test)
one_count = np.sum(y_test)
零计数 = 总数 - 一个计数
lm = [y 为 _,y 排序(zip(test_pred_probs,y_test),reverse=True)]
x = np.arange(0,总计+1)
y = np.append([0],np.cumsum(lm))
a = auc([0,总计],[0,one_count])
aP = auc([0,one_count,总计],[0,one_count,one_count]) - a
aR = auc(x,y) - a
print(&quot;加速比：&quot;,aR/aP) #返回 0.5
print(&quot;ROC 曲线的 Acc 比率：&quot;,2*roc_auc_score(y_test,test_pred_probs)-1) #returns 0.458

在某些情况下，这两种方法会给出截然不同的结果 - 前者给出的 AR 约为 0.7，后者使用 ROC 方法给出的 AR 为 0.12。]]></description>
      <guid>https://stackoverflow.com/questions/73031395/accuracy-ratio-gini-coef-computation-in-python-by-definition-and-roc-method</guid>
      <pubDate>Tue, 19 Jul 2022 05:09:47 GMT</pubDate>
    </item>
    <item>
      <title>一次性将 pandas 数据帧随机分为几组，以进行 x 倍交叉验证</title>
      <link>https://stackoverflow.com/questions/52574923/randomly-divide-a-pandas-dataframe-into-groups-at-once-for-x-fold-crossvalidatio</link>
      <description><![CDATA[假设我有一个包含 500 行的数据框。我想执行 10 倍交叉验证。因此，我需要将这些数据分成 10 个集合，每个集合包含 50 行。我想一次性将整个数据分成 10 个组，而且随机。
有没有办法使用 pandas、numpy 等库来实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/52574923/randomly-divide-a-pandas-dataframe-into-groups-at-once-for-x-fold-crossvalidatio</guid>
      <pubDate>Sun, 30 Sep 2018 05:27:12 GMT</pubDate>
    </item>
    </channel>
</rss>