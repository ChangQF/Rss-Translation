<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 10 Mar 2024 06:16:16 GMT</lastBuildDate>
    <item>
      <title>如何在Python中处理人脸识别中的规格眩光？</title>
      <link>https://stackoverflow.com/questions/78134869/how-to-handle-specs-glare-in-face-recognition-in-python</link>
      <description><![CDATA[我想构建一个基于 python Flask api 的应用程序，我想发送 2 个人物图像（脸部图像）的路径（url 或本地图像），并且如果两张脸都匹配（两张图片都属于），我希望得到“是”的响应同一个人），如果图像是不同的人，则不会。我为此使用了face_recognition 库。在我介绍戴眼镜的人的照片之前，这个算法一直运行良好。该模型无法识别眼镜中是否有眩光。如何进行？有没有一种预处理方法可以提供帮助？或者我应该搬到另一个图书馆？
&lt;前&gt;&lt;代码&gt;
导入人脸识别
导入CV2
将 numpy 导入为 np
从日期时间导入日期时间
从烧瓶导入烧瓶，请求，jsonify
导入人脸识别
导入请求
从 io 导入 BytesIO
从日期时间导入日期时间
从 PIL 导入图像
应用程序=烧瓶（__名称__）

def preprocess_image(image_path,resize_height=400):
    if image_path.startswith((&#39;http://&#39;, &#39;https://&#39;)):
            响应 = requests.get(图像路径)
            图像 = Image.open(BytesIO(响应.内容))
    别的：
        图像 = Image.open(图像路径)
    exif_data = image.getexif()

    如果 exif_数据：
        如果 exif_data.get(274) == 6:
            图像 = 图像.旋转(270)
    宽度，长度=图像大小
    比例 = resize_height / 长度
    new_width = int(宽度 * 比例)
    打印（图像路径，图像大小，比例，新宽度）
    
    new_width = int(宽度 * 比例)
    resized_image = image.resize((new_width, resize_height))

    rgb_image = resized_image.convert(“RGB”)
    numpy_image = np.array(rgb_image)

    返回numpy_image


@app.route(&#39;/&#39;,methods = [&#39;GET&#39;,&#39;POST&#39;])
def home():
    返回“服务器正在运行”
@app.route(&#39;/compare&#39;,methods = [&#39;GET&#39;,&#39;POST&#39;])
defface_compare():
    st = 日期时间.now()
    数据 = request.get_json()
    image_path1 = 数据[&#39;img1&#39;]
    image_path2 = 数据[&#39;img2&#39;]

    已处理图像1 = 预处理图像(图像路径1)
    已处理图像2 = 预处理图像(图像路径2)
    
    面部位置1 = 面部识别.面部位置（处理后的图像1）
    面部位置2 = 面部识别.面部位置(processed_image2)
    #show_bounding_box(face_locations1,processed_image1)
    #show_bounding_box(face_locations2,processed_image2)
    如果不是face_locations1：
        return jsonify({&#39;result&#39;: &#39;未找到面 1&#39;, &#39;time&#39;:str(datetime.now()-st)})
    如果不是face_locations2：
        return jsonify({&#39;result&#39;: &#39;未找到人脸 2&#39;, &#39;time&#39;:str(datetime.now()-st)})

    face_encoding1=face_recognition.face_encodings(processed_image1,known_face_locations=face_locations1,model=&#39;小&#39;)[0]
    face_encoding2=face_recognition.face_encodings(processed_image2,known_face_locations=face_locations2,model=&#39;小&#39;)[0]
    
    结果=face_recognition.compare_faces([face_encoding1],face_encoding2,公差=0.47)
    return jsonify({&#39;结果&#39;: str(结果[0]), &#39;时间&#39;:str(datetime.now()-st)})

def show_bounding_box(face_loc, img):
    边界框 = []
    对于face_loc中的face_location：
        上、右、下、左 = 面部位置
        bounding_boxes.append((左、上、右、下))
    对于bounding_boxes中的（左，上，右，下）：
        cv2.矩形(img, (左, 上), (右, 下), (0, 255, 0), 2)
    cv2.imshow(“带有边界框的面孔”, img)
    cv2.waitKey(0)

如果 __name__ == &#39;__main__&#39;:
    app.run（调试=True，线程=True）
]]></description>
      <guid>https://stackoverflow.com/questions/78134869/how-to-handle-specs-glare-in-face-recognition-in-python</guid>
      <pubDate>Sun, 10 Mar 2024 06:01:06 GMT</pubDate>
    </item>
    <item>
      <title>我正在研究一个项目：使用深度学习进行 NER 提取。这是我第一次从事这方面的工作。如何访问访问数据集</title>
      <link>https://stackoverflow.com/questions/78134821/i-am-working-on-a-project-ner-extraction-using-my-deep-learning-this-is-my-fi</link>
      <description><![CDATA[我发现了某些遗留数据集，例如 CoNull 2003 和 OntoNotes。我如何访问这些数据集并在我的项目中使用它们？他们各自的网站显示了很多我不知道的信息。请帮忙
我正在尝试启动该项目。需要建议]]></description>
      <guid>https://stackoverflow.com/questions/78134821/i-am-working-on-a-project-ner-extraction-using-my-deep-learning-this-is-my-fi</guid>
      <pubDate>Sun, 10 Mar 2024 05:34:51 GMT</pubDate>
    </item>
    <item>
      <title>尝试将标签设置为张量时出现值错误</title>
      <link>https://stackoverflow.com/questions/78134521/value-error-when-trying-to-make-labels-to-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78134521/value-error-when-trying-to-make-labels-to-tensor</guid>
      <pubDate>Sun, 10 Mar 2024 02:21:05 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 的多类目标编码器</title>
      <link>https://stackoverflow.com/questions/78133596/scikit-learns-target-encoder-for-multi-class</link>
      <description><![CDATA[如何转换本教程中的代码 https://towardsdatascience .com/target-encoding-for-multi-class-classification-c9a7bcb1a53
导入category_encoders作为ce
def target_encode_multiclass(X,y): #X,y 是 pandas df 和系列
    y=y.astype(str) #转换为字符串进行onehot编码
    enc=ce.OneHotEncoder().fit(y)
    y_onehot=enc.transform(y)
    class_names=y_onehot.columns #onehot 编码列的名称
    X_obj=X.select_dtypes(&#39;object&#39;) #单独的分类列
    X=X.select_dtypes(排除=&#39;对象&#39;)
    对于 class_names 中的 class_：
  
        enc=ce.TargetEncoder()
        enc.fit(X_obj,y_onehot[class_]) #转换所有分类
        temp=enc.transform(X_obj) #class_ 的列
        temp.columns=[str(x)+&#39;_&#39;+str(class_) for x in temp.columns]
        X=pd.concat([X,temp],axis=1) #添加到原始数据集
  
    返回X

只使用sklearn而不使用category_encoders？
目前有
from sklearn.preprocessing import OneHotEncoder, TargetEncoder
def target_encode_multiclass(X,y):
    y=y.astype(str)
    enc=OneHotEncoder().fit(y)
    y_onehot=enc.transform(y)
    类名=y.列
    X_obj=X.select_dtypes(&#39;对象&#39;)
    X=X.select_dtypes(排除=&#39;对象&#39;)
    对于 class_names 中的 class_：
        enc=目标编码器()
        enc.fit(X_obj, y_onehot[class_])
        temp=enc.transform(X_obj)
        temp.columns=[str(x)+&#39;_&#39;+str(class_) for x in temp.columns]
        X=pd.concat([X,温度],轴=1)
    返回X

抛出错误“enc.fit(X_obj, y_onehot[class_])”行上的索引维度必须为 1 或 2]]></description>
      <guid>https://stackoverflow.com/questions/78133596/scikit-learns-target-encoder-for-multi-class</guid>
      <pubDate>Sat, 09 Mar 2024 19:12:44 GMT</pubDate>
    </item>
    <item>
      <title>Python 中 HIV/AIDS 治疗的预测建模 - 特征选择 [关闭]</title>
      <link>https://stackoverflow.com/questions/78133554/predictive-modeling-for-hiv-aids-treatment-in-python-feature-selection</link>
      <description><![CDATA[我正在研究健康分析领域的预测建模项目，特别关注使用 Python 优化 HIV/AIDS 治疗结果。我收集了具有各种特征的数据集，包括患者人口统计数据、治疗历史和实验室结果。
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.feature_selection 导入 SelectFromModel

# 加载数据集（用于说明目的的虚拟数据）
数据 = pd.read_csv(&#39;hiv_aids_dataset.csv&#39;)
# 将数据拆分为特征 (X) 和目标变量 (y)
X = 数据。 Drop(&#39;治疗结果&#39;, 轴=1)
y = 数据[&#39;治疗结果&#39;]

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化随机森林分类器
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# 拟合模型
clf.fit(X_train, y_train)

# 特征选择
模型 = SelectFromModel(clf, prefit=True)
selected_features = X.columns[model.get_support()]

print(&quot;选定的功能：&quot;, selected_features)

对于改进所选功能有什么建议或对此有任何替代方法吗？另外，有什么 EDA 建议可以提高我的模型的准确性吗？]]></description>
      <guid>https://stackoverflow.com/questions/78133554/predictive-modeling-for-hiv-aids-treatment-in-python-feature-selection</guid>
      <pubDate>Sat, 09 Mar 2024 19:01:33 GMT</pubDate>
    </item>
    <item>
      <title>如何以高精度（+ 90%）对面部特征嵌入进行分类。我可以在 svm 模型中进行哪些调整来对 20 多个类别进行分类</title>
      <link>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</link>
      <description><![CDATA[我使用facenet提取特征并使用svm（pytorch中的nn.svm模型）进行分类。效果很好，但 20 堂课后，准确率下降到 75%。如何在利用 GPU 的同时优化 svm。
我使用了这个 svm 类模型
sklearn 的 svm 模型不使用 GPU，所以使用了这个
类 SVM(nn.Module):
    def __init__(自身):
        超级（SVM，自我）.__init__()
        self.fc = nn.Linear(X.shape[1], len(ClassList))

    def 前向（自身，x）：
        返回 self.fc(x)

但是对于 20 多个类别来说，这个准确率非常低
我也尝试过使用这个：
类 SoftmaxUsed(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.layers = nn.Sequential(nn.Linear(512, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.线性(1024, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1024, len(ClassList)),
                                 nn.LogSoftmax(dim=1))
    def 前向（自身，x）：
        返回 self.layers(x)

但准确率最高仍为 86%]]></description>
      <guid>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</guid>
      <pubDate>Sat, 09 Mar 2024 18:56:09 GMT</pubDate>
    </item>
    <item>
      <title>如何计算微调稳定扩散模型的Inception Score</title>
      <link>https://stackoverflow.com/questions/78133289/how-to-calculate-inception-score-of-fine-tune-stable-diffusion-model</link>
      <description><![CDATA[我想计算稳定扩散模型的初始分数，我找到了分数代码，但不确定如何生成.npz
我们使用稳定扩散 1.5 进行微调
我想知道如何计算稳定扩散的起始分数以及 FID 等其他参数]]></description>
      <guid>https://stackoverflow.com/questions/78133289/how-to-calculate-inception-score-of-fine-tune-stable-diffusion-model</guid>
      <pubDate>Sat, 09 Mar 2024 17:40:54 GMT</pubDate>
    </item>
    <item>
      <title>构建新的推荐系统[关闭]</title>
      <link>https://stackoverflow.com/questions/78132563/building-new-recommender-system</link>
      <description><![CDATA[构建新闻推荐系统，使用什么样的推荐模型，基于什么进行推荐，如何收集新闻推荐系统的数据？此外，我需要相同的示例数据。

如何收集相关数据？
我需要示例数据。
]]></description>
      <guid>https://stackoverflow.com/questions/78132563/building-new-recommender-system</guid>
      <pubDate>Sat, 09 Mar 2024 13:48:55 GMT</pubDate>
    </item>
    <item>
      <title>使用mutual_info_classif进行多类目标和特征选择[关闭]</title>
      <link>https://stackoverflow.com/questions/78132544/multi-class-target-and-feature-selection-using-mutual-info-classif</link>
      <description><![CDATA[假设数据集有 1500 个特征、2500 个实例和 7 个类（类别/目标）。
在多类分类的情况下，目标列将有 7 个不同的值。我想使用mutual_info_classif提取重要特征。 mutual_info_classif 会按预期工作吗？
我已经完成了代码并得到了结果。我不太确定结果的有效性。]]></description>
      <guid>https://stackoverflow.com/questions/78132544/multi-class-target-and-feature-selection-using-mutual-info-classif</guid>
      <pubDate>Sat, 09 Mar 2024 13:43:09 GMT</pubDate>
    </item>
    <item>
      <title>准确度为 90%，RMSE 值也很低，但验证损失图在我的 LSTM 模型中有很多尖峰。我怎样才能解决这个问题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78132521/accuracy-is-90-rmse-values-is-also-low-but-validation-loss-graph-has-alot-of-s</link>
      <description><![CDATA[我创建了一个用于时间序列预测的 LSTM 模型。我的价值观如下，
测试集的 RMSE 分数：1.55
测试集的准确度：89.8838968601839

我的验证损失图中有很多峰值，如下所示。如何减少峰值并修复此图表？

我的代码如下，
从 keras.models 导入顺序

def lstm_model(trainX,trainY):
  #创建堆叠的 LSTM 模型
  模型=顺序（）
  model.add(LSTM(64,activation=&#39;relu&#39;,input_shape=(trainX.shape[1],trainX.shape[2]),return_sequences=False))
  模型.add(Dropout(0.2))
  model.add(密集(1))
  # 编译模型
  model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;)

  print(&quot;LSTM 模型摘要&quot;)
  打印（模型.摘要（））
  打印(“---------------------------------------------- ----------”）

  # 拟合模型
  历史= model.fit（trainX，trainY，epochs = 200，batch_size = 8，validation_split = 0.01，verbose = 1）
  打印（历史）
  打印(“---------------------------------------------- ----------”）

  #model.save(&#39;/content/gdrive/MyDrive/MScProject/Implementation/lstm.h5&#39;)
  位置 = &#39;/content/gdrive/MyDrive/MScProject/Implementation/&#39; + 银行名称 + &#39;/lstm.h5&#39;
  模型.保存（位置）

  rmse = evaluate_models(历史, 模型)
  返回均方根误差

rmse = lstm_model(trainX,trainY)
模型测试（rmse）
]]></description>
      <guid>https://stackoverflow.com/questions/78132521/accuracy-is-90-rmse-values-is-also-low-but-validation-loss-graph-has-alot-of-s</guid>
      <pubDate>Sat, 09 Mar 2024 13:36:46 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降权重不断变大</title>
      <link>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</link>
      <description><![CDATA[为了熟悉梯度下降算法，我尝试创建自己的线性回归模型。对于少数数据点来说它效果很好。但是当尝试使用更多数据来拟合它时，w0 和 w1 的大小总是增加。有人可以解释一下这种现象吗？
类线性回归：
    def __init__(自身, x_向量, y_向量):

        self.x_vector = np.array(x_vector, dtype=np.float64)
        self.y_向量 = np.array(y_向量, dtype=np.float64)
        自身.w0 = 0
        自身.w1 = 0

    def _get_predicted_values(self, x):
        公式 = lambda x: self.w0 + self.w1 * x
        返回公式(x)

    def_get_gradient_matrix（自身）：
        预测 = self._get_predicted_values(self.x_vector)
        w0_hat = sum((self.y_向量 - 预测))
        w1_hat = sum((self.y_向量 - 预测) * self.x_向量)

        梯度矩阵 = np.array([w0_hat, w1_hat])
        梯度矩阵 = -2 * 梯度矩阵

        返回梯度矩阵

    def fit(自我，step_size=0.001，num_iterations=500)：
        for _ in range(1, num_iterations):
            梯度矩阵 = self._get_gradient_matrix()
            self.w0 -= 步长大小 * (梯度矩阵[0])
            self.w1 -= 步长大小 * (梯度矩阵[1])

    def _show_coeffiecients（自身）：
        print(f&quot;w0: {self.w0}\tw1: {self.w1}\t&quot;)

    def 预测（自身，x）：
        y = 自身.w0 + 自身.w1 * x
        返回y

# 这工作正常
x = [x 表示 x 在范围 (-3, 3) 内]
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coeffiecients() #输出：w0：-6.99999999999994 w1：5.00000000000002

#虽然这不是
x = [x for x in range(-50, 50)] # 增加 x 值的数量
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coefficients()

最后一行产生警告：
运行时警告：乘法中遇到溢出
w1_hat = sum((self.y_向量 - 预测) * self.x_向量)
公式 = lambda x: self.w0 + self.w1 * x
]]></description>
      <guid>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</guid>
      <pubDate>Wed, 06 Mar 2024 14:22:16 GMT</pubDate>
    </item>
    <item>
      <title>边际对数似然 GPytorch - 最小化的初始条件</title>
      <link>https://stackoverflow.com/questions/78031211/marginal-log-likelihood-gpytorch-initial-condition-for-minimization</link>
      <description><![CDATA[这个问题是针对高斯过程训练的情况提出的，但我想它也适用于神经网络。
# 找到最优模型超参数
模型.train()
可能性.train()

# 使用 adam 优化器
Optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # 包含高斯似然参数

#“损失”对于 GP - 边际对数似然
mll = gpytorch.mlls.ExactMarginalLogLikelihood（可能性，模型）

对于范围内的 i（训练迭代）：
    优化器.zero_grad()
    输出=模型（train_x）
    损失 = -mll(输出, train_y)
    loss.backward()
    print(&#39;Iter %d/%d - 损失: %.3f&#39; % (i + 1,training_iterations,loss.item()))
    优化器.step()

在典型的训练过程中，我们试图最小化损失函数（在本例中为边际对数似然）。我的问题是，最小化的初始条件是什么？如何将其输入/输出到训练循环？
我只是想从数学角度理解训练阶段，发现 Pytorch “以一种无形的方式”做了很多事情。]]></description>
      <guid>https://stackoverflow.com/questions/78031211/marginal-log-likelihood-gpytorch-initial-condition-for-minimization</guid>
      <pubDate>Wed, 21 Feb 2024 02:05:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 mlx 中进行蒙版填充？</title>
      <link>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</link>
      <description><![CDATA[我想在 mlx 中实现 masked_fill，但它与 float(&#39;-inf&#39;) 配合效果不佳
https://pytorch.org/docs/stable/generate /torch.Tensor.masked_fill.html
我正在尝试使用 mlx.core.where 来实现此目的
masked_tensor = mlx.core.where(mask, mlx.core.array(float(&#39;-inf&#39;)), mlx.core.array(0))

但是对于面具
数组([[假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]]，dtype = bool）

这会返回
数组([[nan, nan, -inf, -inf],
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]]，dtype = float32）

这不是我想要的。理想情况下它会返回
数组([[0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf]], dtype=float32)

帮助]]></description>
      <guid>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</guid>
      <pubDate>Thu, 08 Feb 2024 16:55:27 GMT</pubDate>
    </item>
    <item>
      <title>使用迁移学习时模型不学习</title>
      <link>https://stackoverflow.com/questions/76521642/model-is-not-learning-when-using-transfer-learning</link>
      <description><![CDATA[我是机器学习的初学者，我正在尝试开发一个可以根据面部数据集预测年龄的模型。然而，我的模型没有学习，我正在努力找出原因。我正在导入 VGG16 架构，但它仍然无法学习。我已经搜索过论坛，甚至尝试仅在 2 个示例上训练模型，但它仍然无法过拟合。
在整个训练过程中，训练准确性几乎没有增加，验证准确性也没有增加。
path = “../input/agedetection/dataset/dataset”
文件= os.listdir(路径)
X = []
年龄_温度 = []

对于文件中的文件：
    img = cv2.imread(路径+&#39;/&#39;+文件)
    img = cv2.resize(img, dsize = target_size)
    X.追加（img）
    字段 = file.split(&#39;_&#39;)
    Age_temp.append(字段[0])

X = np.array(X).astype(&#39;float32&#39;)
X = X/255

#将年龄转换为不同的括号 - 0-20, 21-40, 41-60,61+
年龄 = np.zeros(len(age_temp))

对于范围内的 i(len(age_temp))：
    curr_age = int(age_temp[i])
    如果 curr_age &lt;= 20：
        值=0
    elif curr_age &lt;= 40：
        值=1
    elif curr_age &lt;= 60：
        值=2
    别的：
        值=3
    年龄[i] = val

年龄 = to_categorical(年龄, 班级数 = 4)
年龄 = 年龄.astype(&#39;float32&#39;)

base_model_age = tf.keras.applications.VGG16(input_shape=input_shape,include_top=False,weights=“imagenet”)
对于 base_model_age.layers[:-20] 中的图层：
    层.trainable=False
model_age = 顺序()
model_age.add(base_model_age)
model_age.add(压平())
model_age.add（密集（1024，激活=&#39;relu&#39;））
model_age.add（密集（4，激活=&#39;relu&#39;））

model_age.compile(优化器=Adam(亚当),
              损失=&#39;mse&#39;
              ,指标=[&#39;准确性&#39;])

hist_age = model_age.fit(X_train,age_train,
                         验证数据=（X_验证，年龄_验证），
                         epochs=10，steps_per_epoch=256，
                         回调=[lrd, mcp])
]]></description>
      <guid>https://stackoverflow.com/questions/76521642/model-is-not-learning-when-using-transfer-learning</guid>
      <pubDate>Wed, 21 Jun 2023 09:05:40 GMT</pubDate>
    </item>
    <item>
      <title>Python - 从 OLS 模型中获取排列重要性</title>
      <link>https://stackoverflow.com/questions/70623835/python-grabbing-permutation-importance-from-ols-model</link>
      <description><![CDATA[正如标题所述，我正在尝试获取 OLS 模型中特征的排列重要性，但得到的是：
TypeError：估计器应该是实现“fit”方法的估计器，已通过
这是我的代码：
导入 pandas 作为 pd
从 sklearn.inspection 导入 permutation_importance
从 sklearn.model_selection 导入 train_test_split
将 statsmodels.api 导入为 sm

df = pd.read_csv(r&#39;my_file&#39;)

X = df.drop（我的因变量）
y = df[我的因变量）

X_train, X_test, y_train, y_test = train_test_split(X, y)

模型 - sm.OLS(y_train, X_train).fit()
打印（模型.摘要（））

分数 = permuation_importance(模型, X_train, y_train, 评分=&#39;neg_root_mean_squared_error&#39;)

重要性 = 分数.importances_mean

对于枚举中的 i,v（重要性）：
   print(&#39;特征：%0d，得分：%.5f&#39; % (i,v))

我有一种感觉，因为我使用的模型不是来自 sklearn，所以想知道是否有办法从 OLS 模型中获取特征指标？谢谢！！]]></description>
      <guid>https://stackoverflow.com/questions/70623835/python-grabbing-permutation-importance-from-ols-model</guid>
      <pubDate>Fri, 07 Jan 2022 15:51:40 GMT</pubDate>
    </item>
    </channel>
</rss>