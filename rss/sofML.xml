<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 01 Jul 2024 06:23:22 GMT</lastBuildDate>
    <item>
      <title>Python 中的可解释 AI：在 shap.Explainer() 中，我应该输入 classifier 还是 classifier.predict？那么，如何获取 shap 值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78690391/explainable-ai-in-python-in-shap-explainer-should-i-input-classifier-or-clas</link>
      <description><![CDATA[我正在使用 SHAP（Shapley Additive Explanations）。我理解工作流程必须是：将我的数据拆分为训练和测试，训练我的模型，运行 SHAP 解释器，获取 shap 值。
当然，我见过使用不同方法做同样事情的代码（有点像 Perl 哲学，但很好）。我搞不懂它们之间的区别。我已阅读 SHAP 文档，但未能理解正确的方法。
我已看到两者：

explainer = shap.Explainer(clf)
explainer = shap.Explainer (clf.predict, X train)

后来，为了获取 shap 值，我看到了：

explainer(X)
explainer(X_test)
explainer.shap_values(X_test) - 这是我唯一理解差异的。它返回一个 numpy 数组，而不是 shap 解释对象。

下面，我复制了一些我见过的例子。
在Towards Data Science中：
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

explainer = shap.Explainer(clf.predict, X_test)
shap_values = explainer(X_test)

在 SHAP 官方GitHub 页面（不是他们的文档）
explainer = shap.Explainer(clf)
shap_values = explainer(X)

在geeks for geeks和datacamp
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

解释器 = shap.Explainer(clf)
shap_values = explainer(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78690391/explainable-ai-in-python-in-shap-explainer-should-i-input-classifier-or-clas</guid>
      <pubDate>Mon, 01 Jul 2024 03:49:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用我自己的数据集训练万寿菊模型进行单目深度图估计？</title>
      <link>https://stackoverflow.com/questions/78690038/how-can-i-train-the-marigold-model-for-monocular-depth-map-estimation-using-my-o</link>
      <description><![CDATA[我偶然发现了这篇论文“重新利用基于扩散的图像生成器进行单目深度估计”，并在 GitHub 上找到了它的代码。
我目前正在尝试使用自己的数据集训练模型，但感觉有点卡住，不确定需要进行哪些修改。
在 /dataset 目录中，有几个 .yaml 配置文件。我应该为我的数据集创建新的 train.yaml 和 val.yaml 文件吗，还是最好修改现有的 dataset_train.yaml 和 dataset_val.yaml 文件并保留其他文件？
如能提供任何有关如何正确设置这些配置文件的指导，我将不胜感激！
论文：https://arxiv.org/abs/2312.02145
GitHub：https://github.com/prs-eth/Marigold]]></description>
      <guid>https://stackoverflow.com/questions/78690038/how-can-i-train-the-marigold-model-for-monocular-depth-map-estimation-using-my-o</guid>
      <pubDate>Sun, 30 Jun 2024 23:25:07 GMT</pubDate>
    </item>
    <item>
      <title>无法找到股价预测模型中数据泄露的位置</title>
      <link>https://stackoverflow.com/questions/78689836/cannot-find-where-data-is-being-leaked-in-stock-price-prediction-model</link>
      <description><![CDATA[我使用 AAPL 从 2010 年初到 2021 年初的股票数据训练了一个模型。但是，我注意到，每当我要求它预测 2021 年初到 2021 年底的情况时，它似乎总能给出完美的答案，这可能是因为一些数据被泄露到了模型中。但是，我似乎找不到数据泄露的位置。任何帮助都将不胜感激，谢谢！
训练过程：
# 下载数据
data = yf.download(&quot;AAPL&quot;, start=&quot;2010-01-01&quot;, end=&quot;2021-01-01&quot;)
data = data.dropna()
data = data[cols]

# 在 0 和 1 之间缩放数据
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

window_size=60

# 创建用于训练和测试的序列
# 创建用于训练和测试的序列
def create_sequences(data, window_size):
X, y = [], []
for i in range(len(data) - window_size):
X.append(data[i:(i + window_size), :])
y.append(data[i + window_size, 0]) # 我们只预测收盘价
return np.array(X), np.array(y)

X_train, y_train = create_sequences(scaled_data, window_size)

from keras.callbacks import EarlyStopping

# 定义提前停止标准
early_stopping = EarlyStopping(monitor=&#39;loss&#39;, waiting=10)

# 构建 LSTM 模型
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(units=50))
model.add(Dense(units=1))
model.compile(loss=&quot;mae&quot;, optimizer=&quot;adam&quot;)

# 添加早期停止回调到拟合函数
model.fit(X_train, y_train, epochs=200, batch_size=64, callbacks=[early_stopping])

这是我测试模型的地方，它似乎以某种方式泄露了数据：
data_custom_range = yf.download(&quot;AAPL&quot;, start=&quot;2021-01-01&quot;, end=&quot;2021-12-28&quot;)
data_custom_range = data_custom_range.dropna()
data_custom_range = data_custom_range[cols] # 选择收盘价、成交量

window_size = 60

# 创建用于训练和测试的序列
def create_sequences(data, window_size):
X, y = [], []
for i in range(window_size, len(data)): # 从开始window_size
X.append(data[(i - window_size):i, :]) # 使用前 window_size 天的数据
y.append(data[i, 0]) # 预测第二天的价格
return np.array(X), np.array(y)

# 准备数据
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data_custom_range.values)

# 仅在收盘价上拟合单独的缩放器
price_scaler = MinMaxScaler()
price_scaler.fit(data_custom_range.iloc[:, 0:1].values) # 仅使用收盘价

# 创建序列
X_custom, y_custom = create_sequences(scaled_data, window_size)

# 进行预测
predictions = []
for i in range(window_size, len(X_custom)):
prediction = model.predict(X_custom[i:i+1])
predictions.append(prediction[0, 0])

# 使用 price_scaler 对预测进行逆变换
predictions = price_scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
actual = price_scaler.inverse_transform(y_custom.reshape(-1, 1))

这是输出的图表（向右移动 60 天）：
import matplotlib.pyplot as plt

# 将预测向右移动 80 个单位
shifted_predictions = np.append([None]*60, predictions)

# 绘制实际价格和预测价格
plt.plot(actual, label=&#39;Actual Price&#39;)
plt.plot(shifted_predictions, label=&#39;预测价格&#39;) # 使用 &#39;shifted_predictions&#39; 而不是 &#39;predictions&#39;
plt.xlabel(&#39;时间&#39;)
plt.ylabel(&#39;价格&#39;)
plt.title(&#39;实际价格与预测价格&#39;)
plt.legend()
plt.show()


如果需要任何其他信息，请随时告诉我，非常感谢！
注意：如果我不小心授予它访问 T+1 时间数据的权限，请告诉我，我试图查看我在哪里不小心这样做了，但找不到它 :/]]></description>
      <guid>https://stackoverflow.com/questions/78689836/cannot-find-where-data-is-being-leaked-in-stock-price-prediction-model</guid>
      <pubDate>Sun, 30 Jun 2024 21:20:54 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 作为双机器学习中的回归器</title>
      <link>https://stackoverflow.com/questions/78689719/lstm-as-regressor-in-double-machine-learning</link>
      <description><![CDATA[我想使用双重机器学习 (DML) 进行一些因果关系研究。为此，我使用了 doubleml 包中的 DoubleMLIRM。DML 使用两种 ML/DL/NN 方法；一种用于回归，另一种用于分类。对于分类，我使用 ExtraTreesClassifier，我想为回归器开发一个 LSTM。我有点困惑我该怎么做，因为我想将其输入为回归器。这是我目前使用 DML 和分类器的代码。我想在代码中定义 LSTM 回归器。df 是一个时间序列格式的数据框，它至少有 3 列，分别为 y_col、d_col 和 x_cols。请注意，d_cols 列是二进制的（这是处理变量）。
import doubleml as dml
from sklearn.ensemble import ExtraTreesClassifier

data_dml_base = dml.DoubleMLData(df,
y_col=y_col,
d_cols= d_cols,
x_cols=x_cols)

boost1 = ExtraTreesClassifier(n_jobs=1,
n_estimators=300, max_depth = 2)

boost2 = LSTM(...) # 回归器，我该如何写这个？

np.random.seed(1111)
dml_plr_boost = dml.DoubleMLIRM(data_dml_base,
ml_g = boost2,
ml_m = boost1,
n_folds = 10)
dml_plr_boost.fit(store_predictions=True)
dml_plr_boost.summary

我以前使用 XGBRegressor 作为回归器，但它对我的数据不太适用。代码看起来应该是这样的：
boost3 = XGBRegressor(n_jobs=1, objective = &#39;reg:squarederror&#39;,
eta=0.3, n_estimators=500, max_depth = 2)

dml_plr_boost_xgb = dml.DoubleMLIRM(data_dml_base,
ml_g = boost3,
ml_m = boost1,
n_folds = 10)
]]></description>
      <guid>https://stackoverflow.com/questions/78689719/lstm-as-regressor-in-double-machine-learning</guid>
      <pubDate>Sun, 30 Jun 2024 20:29:08 GMT</pubDate>
    </item>
    <item>
      <title>Minecraft mod 机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/78689322/minecraft-mod-machine-learning</link>
      <description><![CDATA[我正在和朋友们一起开发一个 Minecraft 模组，我们的想法之一是让怪物在了解玩家在地图上的行进路线后跟踪玩家，以使其变得可怕。我考虑过使用强化学习和训练，记录玩家在特定时间点后的位置，但我对机器学习还不熟悉，在大学里只上过 2 门关于这个主题的课程，如果能得到任何建议，我将不胜感激。
还没有写过代码，主要是想找个起点。我考虑过使用 TensorFlow，但我唯一的经验是使用基本语言模型，而不是训练实体来执行操作]]></description>
      <guid>https://stackoverflow.com/questions/78689322/minecraft-mod-machine-learning</guid>
      <pubDate>Sun, 30 Jun 2024 17:30:40 GMT</pubDate>
    </item>
    <item>
      <title>从 YOLO 格式标记文本文件生成模型/ckpt 文件？</title>
      <link>https://stackoverflow.com/questions/78689257/generate-model-ckpt-file-from-yolo-format-labeled-text-file</link>
      <description><![CDATA[我正在尝试使用注释工具从头开始创建自己的 ckpt 文件，首先注释两个被归类为两个不同类别的图像及其宽度、高度和位置信息，如下所示：
&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;

现在给定 .txt YOLO 格式的标记文本文件，我想生成一个模型/检查点文件，用于另一个程序中的图像分类。
怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78689257/generate-model-ckpt-file-from-yolo-format-labeled-text-file</guid>
      <pubDate>Sun, 30 Jun 2024 17:02:39 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 DB 扫描来聚类椭圆？[关闭]</title>
      <link>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</link>
      <description><![CDATA[我有一组椭圆形的轮廓。这些轮廓以特定角度倾斜，我想对这些轮廓进行聚类。如果两个相邻的细菌的质心相距小于距离 R，并且它们的运动方向相差小于角度 A，我们将这两个细菌定义为同一簇的成员。
我的轮廓如下所示：

我们可以针对这种情况使用 DB-scan 算法吗？据我了解，我们定义一个半径为 R 的小圆，如果这些点的总数符合我们之前设定的标准，则该圆中重叠的所有点都将成为核心点。从这些核心点开始，集群不断扩展，然后包含非核心点。
我不明白如何将角度的其他条件纳入其中。我是否应该首先使用 DB 扫描基于 R 查找集群，然后使用其他可以处理角度的算法过滤掉这个结果？或者有没有办法使用 DB 扫描本身来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</guid>
      <pubDate>Sun, 30 Jun 2024 16:41:42 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我目前正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 PyTorch 和 Ultralytics Yolo 代码以利用 GPU？</title>
      <link>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</link>
      <description><![CDATA[我正在做一个涉及对象检测和跟踪的项目。对于对象检测，我使用 yolov8，对于跟踪，我使用 SORT 跟踪器。运行以下代码后，我的 GPU 使用率始终低于 10%，而 CPU 使用率始终超过 40%。我安装了 cuda、cudnn，并使用 cuda 安装了 torch。我还编译了支持 cuda 的 opencv。我正在使用 RTX 4060 ti，但看起来它没有被使用。
有没有办法进一步优化下面的代码，以便所有工作都由 GPU 而不是 CPU 处理？
from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO(&#39;yolov8s.pt&#39;).to(device)

cap = cv2.VideoCapture(0)

while True:
ret, frame = cap.read() 
if not ret:
print(&quot;**未收到帧**&quot;)
继续

results = model(frame)
dets_to_sort = np.empty((0, 6))
for result in results:
for obj in result.boxes:
bbox = obj.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox

conf = obj.conf.item()
class_id = int(obj.cls.item())
dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))

tracked_dets = sort_tracker.update(dets_to_sort)
for det in tracked_dets:
x1, y1, x2, y2 = [int(i) for i in det[:4]]
track_id = int(det[8]) if det[8] 不为 None else 0
class_id = int(det[4])
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1)
如果 key == ord(&quot;q&quot;):
break
如果 key == ord(&quot;p&quot;):
cv2.waitKey(-1)

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</guid>
      <pubDate>Sun, 30 Jun 2024 07:43:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 FastAPI 中加载深度学习模型</title>
      <link>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</link>
      <description><![CDATA[我无法使用 Keras 和 FAstAPI 成功加载以 .keras 格式保存的机器学习模型。尽管按照标准程序使用 tensorflow.keras.models.load_model() 加载模型，但应用程序仍会抛出错误，指示未找到文件、与 FastAPI 和 TensorFlow 版本的兼容性问题或依赖项冲突。我已验证文件路径并确保安装了所有必要的依赖项，但问题仍然存在。详细的调试尝试包括检查 FastAPI、Keras 和 TensorFlow 之间的版本兼容性，以及确认模型文件可访问且格式正确。尽管做出了这些努力，但模型加载过程始终失败，阻碍了机器学习功能进一步集成到我的 FastAPI 应用程序中。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</guid>
      <pubDate>Sun, 30 Jun 2024 06:14:54 GMT</pubDate>
    </item>
    <item>
      <title>我该如何修复“缓存可能已过期”错误？</title>
      <link>https://stackoverflow.com/questions/78685493/how-do-i-the-fix-cache-may-be-out-of-date-error</link>
      <description><![CDATA[我使用 YOLOv5s 作为预训练模型，以十张图片作为数据集，成功创建了一个训练模型。
但是当我使用另外十张图片创建第二个模型时，我遇到了麻烦。
我的第一个和第二个代码相似，但保存的模型名称不同。
我使用 Spyder 3.11 作为我的 IDE
import torch
from PIL import Image
import os

# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤 3：根据训练数据拟合模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = os.path.join(dataset_path, &#39;yolov5s.pt&#39;)
torch.save(model.state_dict(), save_path)

我收到此错误：
异常：&#39;model&#39;。缓存可能已过期，请尝试“force_reload=True”或参阅 https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading 寻求帮助。
我尝试了 force_reload=True，但对我没有帮助。还阅读了所有错误链接网站的指南并尝试了所有步骤，但也要考虑它们的实用性。
我花了大约七天时间，目前我就像疯了一样。]]></description>
      <guid>https://stackoverflow.com/questions/78685493/how-do-i-the-fix-cache-may-be-out-of-date-error</guid>
      <pubDate>Sat, 29 Jun 2024 08:45:22 GMT</pubDate>
    </item>
    <item>
      <title>Flutter 中的 TensorFlowInferenceInterface</title>
      <link>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</link>
      <description><![CDATA[我在 Android 中有一个对象检测代码，我想将其转换为 Flutter 以便也用于 IOS
public static Classifier create(
AssetManager assetManager,
String modelFilename,
String[] labels,
int inputSize,
int imageMean,
float imageStd,
String inputName,
String outputName) {
final TensorFlowImageClassifier c = new TensorFlowImageClassifier();
c.inputName = inputName;
c.outputName = outputName;

// 将标签名称读入内存。
Collections.addAll(c.labels, labels);

c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);

// 输出的形状为 [N, NUM_CLASSES]，其中 N 是批次大小。
final Operation operation = c.inferenceInterface.graphOperation(outputName);
final int numClasses = (int) operation.output(0).shape().size(1);

// 理想情况下，可以从输入操作的形状中检索 inputSize。唉，
// 通常使用的 graphdef 中输入的占位符节点未指定形状，因此它
// 必须作为参数传入。
c.inputSize = inputSize;
c.imageMean = imageMean;
c.imageStd = imageStd;

// 预分配缓冲区。
c.outputNames = new String[]{outputName};
c.intValues = new int[inputSize * inputSize];
c.floatValues = new float[inputSize * inputSize * 3];
c.outputs = new float[numClasses];

return c;
}

调用：
create(
getAssets(),
&quot;file:///android_asset/xxx&quot;,
getResources().getStringArray(R.array.yyy),
INPUT_SIZE,
128,
128,
&quot;input&quot;,
&quot;InceptionV3/Predictions/Reshape_1&quot;)

org.tensorflow.contrib.android.TensorFlowInferenceInterface
在 Flutter 中找不到这个类，我应该用什么来替换它？]]></description>
      <guid>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</guid>
      <pubDate>Thu, 27 Jun 2024 12:06:57 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 在 Docker 容器中使用 layoutparser 预训练模型错误：未找到检查点</title>
      <link>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</link>
      <description><![CDATA[以下是我的 Dockerfile。
来自 python:3.9
运行 apt-get clean &amp;&amp; apt-get update
pip install --upgrade pip

运行 pip install layoutparser 

运行 pip install &quot;layoutparser[ocr]&quot; 

运行 pip install pytesseract 

运行 pip install pdf2image 

运行 pip install torch 

运行 pip install torchvision

运行 apt-get install -y poppler-utils #(pdf-image) 

运行 apt-get install -y tesseract-ocr 

运行 apt-get install git #(安装 detectron2) 

运行 pip install &quot;git+https://github.com/facebookresearch/detectron2.git&quot; #(detectron2 模型) 

运行 apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6 -y #(运行软件包所需)

workdir /home/jovyan/work/layout_parser

volume [&quot;/home/jovyan/work/layout_parser&quot;]

CMD [&quot;python&quot;, &quot;test_code.py&quot;]

python 代码 test_code.py:
import pdf2image
import layoutparser as lp
import pytesseract
import numpy as np
import cv2
import matplotlib.pyplot as plt

pdf_file= r&quot;/home/jovyan/work/layout_parser/test_pdf.pdf&quot;
image = np.asarray(pdf2image.convert_from_path(pdf_file)[0])

model = lp.Detectron2LayoutModel(&#39;lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config&#39;)


我收到以下错误：

我尝试了以下方法解决问题，但均未成功：

使用的 torch 和 torchvision 版本的变化
使用的 python 3.7、3.8、3.9 基础版本
预建模型的 config_path 的变化使用
手动下载配置文件中的模型 -&gt; layout_parser_modelzoo
尝试在扩展 pth 和 pkl 中手动下载模型。收到以下错误：


如何使用预先训练的模型？]]></description>
      <guid>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</guid>
      <pubDate>Tue, 25 Apr 2023 07:07:40 GMT</pubDate>
    </item>
    <item>
      <title>我的项目中没有安装 face_recognition，显示这些类型的错误？</title>
      <link>https://stackoverflow.com/questions/58379962/face-recognition-is-not-install-in-my-project-showing-these-types-of-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/58379962/face-recognition-is-not-install-in-my-project-showing-these-types-of-error</guid>
      <pubDate>Mon, 14 Oct 2019 15:36:16 GMT</pubDate>
    </item>
    </channel>
</rss>