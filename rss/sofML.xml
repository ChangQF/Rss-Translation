<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 09 May 2024 18:17:51 GMT</lastBuildDate>
    <item>
      <title>图书柜台|我需要一个模型来计算图像/视频中的书籍数量</title>
      <link>https://stackoverflow.com/questions/78456209/book-counter-i-need-a-model-which-will-count-numbers-of-book-from-image-video</link>
      <description><![CDATA[我需要一个模型来计算图像/视频中的书籍数量。
原型机将能够拍摄图像/视频并继续计数书籍，就像我们在路灯上的车辆计数器一样。]]></description>
      <guid>https://stackoverflow.com/questions/78456209/book-counter-i-need-a-model-which-will-count-numbers-of-book-from-image-video</guid>
      <pubDate>Thu, 09 May 2024 18:01:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么无论使用什么数据集和模型，我的准确性都没有变化？</title>
      <link>https://stackoverflow.com/questions/78456058/why-doesnt-my-accuracy-vary-no-matter-the-dataset-and-the-model-used</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78456058/why-doesnt-my-accuracy-vary-no-matter-the-dataset-and-the-model-used</guid>
      <pubDate>Thu, 09 May 2024 17:26:57 GMT</pubDate>
    </item>
    <item>
      <title>test=model.predict([text])整个项目连续运行六次</title>
      <link>https://stackoverflow.com/questions/78455825/test-model-predicttext-entire-project-runs-six-times-consecutively</link>
      <description><![CDATA[似乎当执行 test=model.predict([text]) 行时，您的整个项目连续运行六次，并且当您关闭打开的窗口时，它会进行预测。
您可以看到下面的代码片段。
[主文件]
print(“test1”)
def 分类():
    打印（“测试2”）
    raw_text = str(entry.get(“1.0”, tk.END))
    tahmin_sonucu = 主要（原始文本）
    categories_label.config(text=“分类器：”+ tahmin_sonucu)

[主要功能]
from simpletransformers.classification import ClassificationModel

labels = [“Bilim Kurgu”、“经济”、“伊斯兰”、“Polisiye”、“浪漫”、“Sağlık”、“体育”]
模型 = ClassificationModel(&#39;bert&#39;, &#39;bert_model&#39;, num_labels=7, use_cuda=False,
                                    args={&#39;reprocess_input_data&#39;：True，&#39;overwrite_output_dir&#39;：True，&#39;num_train_epochs&#39;：3，
                                        “train_batch_size”：16，“fp16”：False，“output_dir”：“bert_model”})
打印（“测试11”）
def main(metin):
    塔明 = 无
    打印（“tes12”）
    tahmin=model.predict([metin])

    打印（标签[tahmin[0][0]]）
    
    返回标签[tahmin[0][0]]


输出]]></description>
      <guid>https://stackoverflow.com/questions/78455825/test-model-predicttext-entire-project-runs-six-times-consecutively</guid>
      <pubDate>Thu, 09 May 2024 16:31:34 GMT</pubDate>
    </item>
    <item>
      <title>在数据框中，我想以行数加倍的方式组合 2 列[重复]</title>
      <link>https://stackoverflow.com/questions/78455730/in-a-dataframe-i-want-to-combine-2-columns-in-such-way-that-the-no-of-rows-gets</link>
      <description><![CDATA[我有一个熊猫数据框（df）说
&lt;前&gt;&lt;代码&gt; Col1 Col2 Col3
1 条 AAA CCC 垃圾邮件
2 bbb ddd 火腿
……

我想将 Col1 和 Col2 与 Col3 一起合并到一个新的数据帧中，例如“test_df”
我想要这个
&lt;前&gt;&lt;代码&gt; Col_new Col3
1 条 AAA 垃圾邮件
2个bbb火腿
3 抄送垃圾邮件
4 dd 火腿

是否有任何简单的代码行？]]></description>
      <guid>https://stackoverflow.com/questions/78455730/in-a-dataframe-i-want-to-combine-2-columns-in-such-way-that-the-no-of-rows-gets</guid>
      <pubDate>Thu, 09 May 2024 16:13:58 GMT</pubDate>
    </item>
    <item>
      <title>将分类加权损失函数集成到我的代码中后，准确率下降了</title>
      <link>https://stackoverflow.com/questions/78455283/the-accuracy-decreased-after-integrating-categorical-weighted-loss-function-to-m</link>
      <description><![CDATA[我想提高准确性，并且我有不平衡数据集：akiec：229，bcc：360，bkl：769，df：81，mel：779，vasc：99。为了解决这个问题，我选择将分类加权损失机制集成到模型中。然而，尽管进行了这样的调整，我还是注意到准确性随后下降了。这个意想不到的结果让我怀疑实施过程中出现了错误。您能否帮助我识别和解决任何潜在的错误以优化模型的性能？
# 定义目录
train_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Train&#39;
test_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Test&#39;
validation_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Validation&#39;

# 确定类的数量
numClasses = len(os.listdir(train_dir))

# 定义超参数网格
参数网格 = {
    &#39;学习率&#39;：[0.001]，
    &#39;批量大小&#39;：[16]，
}

最佳准确度 = 0
最佳参数=无

# 执行网格搜索
对于 ParameterGrid(param_grid) 中的参数：
    # 为每次网格搜索迭代加载预训练的 VGG19 模型
    base_model = VGG19(权重=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))
    对于 base_model.layers 中的图层：
        可训练层 = False

    # 定义函数从最后一个卷积层提取特征
    def extract_features(生成器, 模型):
        特征 = model.predict(生成器)
        返回 features.reshape((len(generator.filenames), -1))

    # 创建数据生成器
    train_datagen = 图像数据生成器(
        重新缩放=1./255，
        旋转范围=20，
        width_shift_range=0.2，
        height_shift_range=0.2，
        剪切范围=0.2，
        缩放范围=0.2，
        水平翻转=真，
        fill_mode=&#39;最近&#39;)

    validation_datagen = ImageDataGenerator（重新缩放=1./255）

    train_generator = train_datagen.flow_from_directory(
        火车目录，
        目标大小=(224, 224),
        批量大小=参数[&#39;批量大小&#39;],
        class_mode=&#39;分类&#39;
    ）

    validation_generator =validation_datagen.flow_from_directory(
        验证目录，
        目标大小=(224, 224),
        批量大小=参数[&#39;批量大小&#39;],
        class_mode=&#39;分类&#39;
    ）
&#39;&#39;&#39;

可能这里有一个错误

&#39;&#39;&#39;

    # 定义类索引
    类索引 = {
        &#39;基亚克&#39;: 0,
        “密件抄送”：1，
        &#39;bkl&#39;：2，
        “df”：3，
        “梅尔”：4，
        “血管”：5
    }

    ## 计算班级人数
    类计数 = {}
    对于 os.listdir(train_dir) 中的 class_name：
        class_counts[class_name] = len(os.listdir(os.path.join(train_dir, class_name)))

    # 计算类别权重
    类权重 = {}
    Total_samples = sum(class_counts.values())
    对于 class_name、class_count 在 class_counts.items() 中：
        class_weights[class_indices[class_name]] = 总样本数 / (class_count * len(class_counts))



    # 定义模型架构以接受提取的特征作为输入
    输入=输入(形状=(combined_data_train.shape[1],))
    x = 密集（256，激活=&#39;relu&#39;）（输入）
    预测=密集（numClasses，激活=&#39;softmax&#39;）（x）
    模型=模型（输入=输入，输出=预测）

    # 使用当前的超参数和类权重编译模型
    model.compile(optimizer=SGD(learning_rate=params[&#39;learning_rate&#39;]),loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;],sample_weight_mode=&#39;temporal&#39;)

    # 定义提前停止
    Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

    # 通过提前停止来训练模型
    num_epochs = 50 # 您可以在此处调整纪元数
    历史=模型.fit(
        x=组合数据训练，
        y=train_generator.labels,
        纪元=num_epochs，
        批量大小=参数[&#39;批量大小&#39;],
        validation_data=(combined_data_validation,validation_generator.labels),
        回调=[early_stopping],
        类权重=类权重，
        详细=1
    ）

    model.save(&#39;best_vgg19_model_with_age.h5&#39;)

    # 根据验证数据评估模型
    _，val_accuracy = model.evaluate（combined_data_validation，validation_generator.labels，详细= 0）

    # 如有必要，更新最佳精度和最佳参数
    如果 val_accuracy &gt;最佳准确度：
        最佳准确度 = 有效准确度
        最佳参数 = 参数

# 打印最佳参数和准确度
print(&#39;最佳参数：&#39;, best_params)
print(&#39;最佳验证准确度：&#39;, best_accuracy)


# 加载最佳模型
best_model = load_model(&#39;best_vgg19_model_with_age.h5&#39;)

]]></description>
      <guid>https://stackoverflow.com/questions/78455283/the-accuracy-decreased-after-integrating-categorical-weighted-loss-function-to-m</guid>
      <pubDate>Thu, 09 May 2024 14:52:54 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么工具来注释机器学习项目的图像，以便我可以将数据添加到每个边界框</title>
      <link>https://stackoverflow.com/questions/78455181/what-tool-should-i-use-to-annotate-images-for-a-machine-learning-project-so-that</link>
      <description><![CDATA[我和我的团队有大量水果图像需要注释来训练 YOLO 模型。注释过程需要包括每个水果周围的边界框和重量等附加数据。
根据 YOLO 文档，可以通过直接将补充信息附加到通常包含边界框数据的文件来包含补充信息。我在 GitHub 上发现了一条建议采用这种方法的回复（回复链接），但我还没有找到支持在定义边界框的同时添加额外数据的注释工具（例如 Roboflow 或 CVAT）。
虽然可以在创建边界框后手动将数据添加到文本文件中，但这将是一项艰巨的任务。我正在寻找一种更加自动化的解决方案，我可以在其中注释每个图像，同时方便地添加附加信息。
是否有任何工具或网站支持此功能？]]></description>
      <guid>https://stackoverflow.com/questions/78455181/what-tool-should-i-use-to-annotate-images-for-a-machine-learning-project-so-that</guid>
      <pubDate>Thu, 09 May 2024 14:36:09 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些有关如何正确注释数据集的说明</title>
      <link>https://stackoverflow.com/questions/78454868/i-need-some-clarification-on-how-to-annotate-a-dataset-correctly</link>
      <description><![CDATA[我正在做一个项目，我想做的是检测器和分类器。具体来说，我有两个类（袋子、纸板），我想使用像 YOLOv8 这样的检测器，然后使用分类器来提高它们的性能。我正在创建我的数据集，所以我正在拍摄视频，在某些地方扔掉这种浪费（别担心，我不会把它留在那里 xD），然后我注释这些视频中的帧。我想要这个数据集，这样我就可以对 YOLOv8 模型和分类器进行训练，训练它们识别这种浪费。我只有一个疑问。
当垃圾在地面上静止约 10 秒（30 fps 时为 300 帧）时，我应该用相同的边界框注释每个帧吗？或者对网络有危险吗？]]></description>
      <guid>https://stackoverflow.com/questions/78454868/i-need-some-clarification-on-how-to-annotate-a-dataset-correctly</guid>
      <pubDate>Thu, 09 May 2024 13:46:42 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正 r 中光栅堆栈的方向</title>
      <link>https://stackoverflow.com/questions/78454706/how-do-i-correct-the-orientation-of-a-raster-stack-in-r</link>
      <description><![CDATA[我有一个每日数据的 NetCDF 文件。我将其转换为光栅堆栈，但其方向不正确（我已附上图像）。我该如何纠正它。我还将我的代码附在本文中。 [raster_stack 图像和 r 代码](https://i.sstatic.net/UmI4kSNE.png)
另外，请告诉是否有人知道，我如何从这些栅格文件中提取年度数据到 Excel 格式（在 arcGIS 或 r 中）。我有 1955 年到 2023 年的栅格文件，其中包含每日降雨量数据，我想根据我拥有的管理形状文件提取年降雨量数据。
我在 r 中运行了代码，但没有取得任何进展。]]></description>
      <guid>https://stackoverflow.com/questions/78454706/how-do-i-correct-the-orientation-of-a-raster-stack-in-r</guid>
      <pubDate>Thu, 09 May 2024 13:18:03 GMT</pubDate>
    </item>
    <item>
      <title>如何为大型语言模型有效构建提示？</title>
      <link>https://stackoverflow.com/questions/78454565/how-to-structure-prompts-effectively-for-large-language-models</link>
      <description><![CDATA[我目前正在使用大型语言模型 (LLM)，并在构建提示以获得最准确和相关的响应方面面临挑战。我对提示工程有基本的了解，但正在寻找有关如何改进提示的最佳实践和建议。
这是我的具体问题：

LLM 的结构良好的提示的关键组成部分是什么？
提示的复杂性或简单性如何影响 LLM 的回答？
设计提示时是否需要避免常见的陷阱或错误？

我对确保模型理解并遵守提示中提供的上下文的技术特别感兴趣。任何可以指导我的示例或资源将不胜感激。
我希望指导法学硕士做什么和不做什么，包括与之相关的流程和链条，正如我所期望的那样
我尝试过的示例提示：
”“”作为您的专属助手，我在这里解答与我们公司产品和服务相关的任何疑问。请注意，我的专业知识仅限于内部主题，我不具备处理外部查询或执行数学计算的能力。“”“”
在某些情况下，此提示有效，但在其他情况下则无效。]]></description>
      <guid>https://stackoverflow.com/questions/78454565/how-to-structure-prompts-effectively-for-large-language-models</guid>
      <pubDate>Thu, 09 May 2024 12:52:49 GMT</pubDate>
    </item>
    <item>
      <title>Stylegan3汽车生成问题</title>
      <link>https://stackoverflow.com/questions/78454480/stylegan3-car-generation-issue</link>
      <description><![CDATA[你好堆栈溢出社区，
我正在尝试训练stylegan3来生成汽车图像，特别是某个国家的救护车，我有一个小的数据集，经过长时间的训练过程后的输出不方便，我是否需要更长的训练或更大的数据集？
我试图使我的数据集更加统一，并连续训练我的网络 3 天，输出仍然看起来与原始数据完全相同，但分辨率较低，您对针对此用例的更好网络有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78454480/stylegan3-car-generation-issue</guid>
      <pubDate>Thu, 09 May 2024 12:37:13 GMT</pubDate>
    </item>
    <item>
      <title>我的情绪分析给出了错误的预测</title>
      <link>https://stackoverflow.com/questions/78454399/my-sentiment-analysis-is-giving-wrong-predictions</link>
      <description><![CDATA[我有一个包含两列的数据框：推文和标签（攻击性语言、仇恨言论、无仇恨和攻击性）。
我清理了推文并创建了我的模型。
但在建模之后，我的所有测试文本都给出了相同的预测结果：“没有仇恨和攻击性”
# 清理我的文本
def clean_text(文本):
    文本 = str(文本).lower()
    文本 = re.sub(&#39;\[.*?\]&#39;, &#39;&#39;, 文本)
    text = re.sub(&#39;https?://\S+|www\.\S+&#39;, &#39;&#39;, text)
    文本 = re.sub(&#39;&lt;.*?&gt;+&#39;, &#39;&#39;, 文本)
    text = re.sub(&#39;[%s]&#39; % re.escape(string.punctuation), &#39;&#39;, text)
    文本 = re.sub(&#39;\n&#39;, &#39;&#39;, 文本)
    文本 = re.sub(&#39;\w*\d\w*&#39;, &#39;&#39;, 文本)
    text = [text.split(&#39; &#39;) 中的单词逐个单词，如果单词不在停用词中]
    文本 =“”.join(文本)
    text = [stemmer.stem(word) for word in text.split(&#39; &#39;)]
    文本 =“”.join(文本)
    
    返回文本

数据[“推文”] = 数据[“推文”].apply(clean_text)


#将我的列转换为数组
将 numpy 导入为 np
x = np.array(data[“tweet”])
y = np.array(数据[“标签”])


# 拟合推文列以进行建模
从 sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn.model_selection 导入 train_test_split
count_vec = CountVectorizer()
X = count_vec.fit_transform(x)

X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.33、random_state = 42)


从 sklearn.tree 导入 DecisionTreeClassifier

dc_tree = 决策树分类器()
dc_tree.fit(X_train, y_train)

##测试我的模型
test_text = &quot;杀掉所有的人并将其烧毁&quot;;
test_data = count_vec.transform([test_text]).toarray()

打印（dc_tree.预测（测试数据））
#Output：[&#39;没有仇恨和攻击性&#39;] #Expectation：[&#39;仇恨言论&#39;]


test_text = “实践爱和耐心，过上美好的生活！”
test_data2 = count_vec.transform([test_text]).toarray()
打印（dc_tree.预测（test_data2））
#Output：[&#39;没有仇恨和攻击性&#39;]

]]></description>
      <guid>https://stackoverflow.com/questions/78454399/my-sentiment-analysis-is-giving-wrong-predictions</guid>
      <pubDate>Thu, 09 May 2024 12:19:20 GMT</pubDate>
    </item>
    <item>
      <title>feature_weights 参数没有影响 Xgboost</title>
      <link>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</link>
      <description><![CDATA[xgboost 有一个 parameter feature_weights 应该影响模型选择特征的概率，也就是说，我们可以给每个特征更多或更少的权重，但似乎该参数不起作用还是我做错了什么？
X &lt;- as.matrix(iris[,-5])
Y &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0)

库（xgboost）
dm1 &lt;- xgb.DMatrix(X, 标签 = Y)
#我为每个特征设置不同的概率
dm2 &lt;- xgb.DMatrix(X, 标签 = Y, feature_weights = c(1, 0, 0, 0.01))
params &lt;- list(objective = “binary:logistic”, eval_metric = “logloss”)

设置.种子(1)



xgb1 &lt;- xgboost（数据 = dm1，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148



xgb2 &lt;- xgboost（数据 = dm2，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148

但是模型的行为完全相同，似乎参数feature_weights被简单地忽略了]]></description>
      <guid>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</guid>
      <pubDate>Thu, 09 May 2024 11:10:59 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError: 默认进程组尚未初始化，请确保调用 init_process_group</title>
      <link>https://stackoverflow.com/questions/78376085/runtimeerror-default-process-group-has-not-been-initialized-please-make-sure-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78376085/runtimeerror-default-process-group-has-not-been-initialized-please-make-sure-t</guid>
      <pubDate>Wed, 24 Apr 2024 05:05:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中添加L1标准化？</title>
      <link>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写逻辑回归代码。在这段代码中，我认为我的成本导数是我的正则化，但我的任务是添加 L1norm 正则化。你如何在Python中添加这个？是否应该在我定义成本导数的地方添加此内容？感谢任何正确方向的帮助。
def Sigmoid(z):
    返回 1/(1 + np.exp(-z))

def 假设(theta, X):
    返回 Sigmoid(X @ theta)

def Cost_Function(X,Y,theta,m):
    hi = 假设(theta, X)
    _y = Y.reshape(-1, 1)
    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))
    返回J

def Cost_Function_Derivative(X,Y,theta,m,alpha):
    hi = 假设(theta,X)
    _y = Y.reshape(-1, 1)
    J = alpha/float(m) * X.T @ (hi - _y)
    返回J

def Gradient_Descent(X,Y,θ,m,alpha):
    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)
    返回 new_theta

定义精度(theta):
    正确 = 0
    长度 = len(X_test)
    预测=（假设（theta，X_test）&gt; 0.5）
    _y = Y_test.reshape(-1, 1)
    正确=预测==_y
    my_accuracy = (np.sum(正确) / 长度)*100
    print (&#39;LR 精度:&#39;, my_accuracy, &quot;%&quot;)

def Logistic_Regression(X,Y,alpha,theta,num_iters):
    m = 长度（Y）
    对于范围内的 x（num_iters）：
        new_theta = Gradient_Descent(X,Y,theta,m,alpha)
        θ = 新_θ
        如果 x % 100 == 0：
            打印 #(&#39;θ: &#39;, θ)
            print #(&#39;成本：&#39;, Cost_Function(X,Y,theta,m))
    精度(θ)
ep = .012
初始_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep
阿尔法 = 0.5
迭代次数 = 10000
Logistic_Regression(X_train,Y_train,alpha,initial_theta,迭代)
]]></description>
      <guid>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</guid>
      <pubDate>Wed, 14 Feb 2018 08:34:48 GMT</pubDate>
    </item>
    <item>
      <title>NLTK 与距离度量的一致性</title>
      <link>https://stackoverflow.com/questions/32733510/nltk-agreement-with-distance-metric</link>
      <description><![CDATA[我的任务是计算 注释者间协议 “https://en.wikipedia.org/wiki/Multi-label_classification” rel=&quot;nofollow noreferrer&quot;&gt;多标签分类，其中每个示例可以分配多个标签。我发现 NLTK 可以根据距离度量来衡量一致性。
我正在寻找使用 MASI 距离计算 krippendorff alpha 的示例。
这就是我所拥有的。
&lt;前&gt;&lt;代码&gt;导入nltk
从 nltk.metrics 导入 masi_distance


玩具数据 = [[&#39;1&#39;, 5723, [1,2]],[&#39;2&#39;, 5723, [2,3]]]

任务= nltk.metrics.agreement.AnnotationTask（数据= toy_data，距离= masi_distance）
打印任务.alpha()

此代码失败并显示
类型错误：不可散列的类型：“列表”

以下方法也不起作用：
toy_data = [[&#39;1&#39;, 5723, set([1,2])],[&#39;2&#39;, 5723, set([2,3])]]

你有一个可行的例子吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/32733510/nltk-agreement-with-distance-metric</guid>
      <pubDate>Wed, 23 Sep 2015 07:28:09 GMT</pubDate>
    </item>
    </channel>
</rss>