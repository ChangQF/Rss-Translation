<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 08 Mar 2024 21:12:51 GMT</lastBuildDate>
    <item>
      <title>使用 Tensorflow API 进行对象检测 - 没有名为“tensorflow.python.keras.layers.preprocessing”的模块</title>
      <link>https://stackoverflow.com/questions/78129120/object-detection-with-tensorflow-api-no-module-named-tensorflow-python-keras</link>
      <description><![CDATA[我遵循了教程Tensorflow 2 对象检测 API 教程  并收到以下错误：
找不到模块“official.legacy”
我执行了以下步骤：

打开 PowerShell
cd C:\Users\Administrator\Desktop\TestTensorflow
conda create -n tensorflow pip python=3.11
conda 激活张量流
pip install --ignore-installed --upgrade tensorflow
使用验证脚本：
python -c &quot;将张量流导入为 tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;

结果：成功

为了降低复杂性，第一步跳过了 GPU 支持

下载并安装 TensorFlow Model Garden 的对象检测 API https://github.com/tensorflow/模型（主分支）

将 .zip 文件夹解压到 C:\Users\Administrator\Desktop\TestTensorflow\Tensorflow
将生成的文件夹“model-master”重命名为“models”

cd C:\Users\Administrator\Desktop\TestTensorflow\Tensorflow\models\research

protoc object_detection/protos/*.proto --python_out=.

pip 安装 cython

pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI

cp object_detection/packages/tf2/setup.py 。
结果：成功安装pycocotools-2.0

python -m pip install .

使用python object_detection/builders/model_builder_tf2_test.py测试安装”
结果 = 错误（参见屏幕截图）

]]></description>
      <guid>https://stackoverflow.com/questions/78129120/object-detection-with-tensorflow-api-no-module-named-tensorflow-python-keras</guid>
      <pubDate>Fri, 08 Mar 2024 16:47:46 GMT</pubDate>
    </item>
    <item>
      <title>在谷歌驱动器中使用yolov8训练自定义数据集</title>
      <link>https://stackoverflow.com/questions/78129048/training-custom-dataset-using-yolov8-in-google-drive</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78129048/training-custom-dataset-using-yolov8-in-google-drive</guid>
      <pubDate>Fri, 08 Mar 2024 16:34:20 GMT</pubDate>
    </item>
    <item>
      <title>如何优化非线性损失函数？</title>
      <link>https://stackoverflow.com/questions/78129032/how-to-optimize-a-non-linear-loss-function</link>
      <description><![CDATA[当可训练参数显示为线性系数时，优化损失函数很容易，但如果可训练参数是指数形式怎么办？
导入tensorflow为tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)
e = 2.718281828459045

类测试：
    def __init__(自身, A):
        自我.A = A
    def 函数（自身）：
        返回 e**( 参数 * self.A )

定义损失（）：
    测试=测试（1。）
    x = (test.function()-148.4131591025766)**2
    print(&#39;损失：&#39; + str(x.numpy()))
    打印（）
    返回x

参数 = tf.Variable(1.001, 可训练=False, dtype=np.float32)
参数_t = []

对于范围 (10) 内的 ii：
    print(&#39;参数：&#39; + str(param.numpy()))
    opt.minimize(loss, var_list=[param])
    param_t.append(param.numpy())

Fig, ax = plt.subplots()
ax.plot(param_t)
plt.show()在这里输入

优化器将可训练参数param带到-无穷大，同时损失函数增加！但是，正确的答案应该是 param=1 和 loss=0。
如何修改我的代码？]]></description>
      <guid>https://stackoverflow.com/questions/78129032/how-to-optimize-a-non-linear-loss-function</guid>
      <pubDate>Fri, 08 Mar 2024 16:30:33 GMT</pubDate>
    </item>
    <item>
      <title>使用predict和predict_proba计算ROC AUC有什么区别？</title>
      <link>https://stackoverflow.com/questions/78128804/what-is-the-difference-between-using-predict-and-predict-proba-to-calculate-roc</link>
      <description><![CDATA[此代码使用 .predict_proba 并采用类别 1 的值：
# 导入sklearn.metrics包
从 sklearn.metrics 导入 roc_auc_score
# 使用 .predict_proba 计算 roc 准确度分数
pred=sktree.predict_proba(x_test)[:,1]
roc=roc_auc_score(y_test,pred)
print(&quot;决策树分类器的 roc 准确度得分为 {0:1.3f}&quot;.format(roc))
]]></description>
      <guid>https://stackoverflow.com/questions/78128804/what-is-the-difference-between-using-predict-and-predict-proba-to-calculate-roc</guid>
      <pubDate>Fri, 08 Mar 2024 15:43:05 GMT</pubDate>
    </item>
    <item>
      <title>经典机器学习方法对初始变量的敏感性[关闭]</title>
      <link>https://stackoverflow.com/questions/78128617/sensitivity-to-initial-variables-of-classical-ml-methods</link>
      <description><![CDATA[以下哪种机器学习算法对优化算法中使用的初始变量不敏感？
A.隐马尔可夫模型
B.人工神经网络
C.随机森林
D.支持向量机
E. k-最近邻
我认为 A 和 B 很敏感。至于 C，由于引导，随机森林对初始变量不那么敏感。 D 并不那么敏感，因为当我们训练模型时，无论初始变量如何，软边际都会收敛到适当的值。 E 很敏感，因为参数 k 很重要。]]></description>
      <guid>https://stackoverflow.com/questions/78128617/sensitivity-to-initial-variables-of-classical-ml-methods</guid>
      <pubDate>Fri, 08 Mar 2024 15:10:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 进行时间序列预测的异常[关闭]</title>
      <link>https://stackoverflow.com/questions/78127888/anomaly-in-time-series-forecasting-using-lstm</link>
      <description><![CDATA[我正在使用 LSTM 进行时间序列预测，有些数据被视为异常值，但有这些值是正常的，我是否应该考虑删除或纠正这些值？它会影响我的 LSTM 模型吗？
我绘制了数据分布图，可以看出它不是正态分布的。
在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78127888/anomaly-in-time-series-forecasting-using-lstm</guid>
      <pubDate>Fri, 08 Mar 2024 13:08:25 GMT</pubDate>
    </item>
    <item>
      <title>设置和训练模型以根据过去的注册预测事件注册的最佳方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78127837/best-way-to-set-up-and-train-a-model-to-predict-event-registration-based-on-past</link>
      <description><![CDATA[我正在尝试找出根据之前的注册和其他人口统计因素（年龄、性别、收入等）来预测某人是否会注册参加活动的最佳方法
我有一个包含大约 4 万人的数据库可供使用。我的问题是，根据前几年的出勤率来训练/测试模型，然后使用相同的数据来预测明年的出勤率，这是一种不好的做法吗？我应该在这里采取什么方法？
我的利益相关者希望获得今年活动数据库中每个人的预测分数。所以我不能仅仅将某些人作为训练值而忽略。
我目前已经建立了一个逻辑回归模型，并根据去年的注册对其进行了训练]]></description>
      <guid>https://stackoverflow.com/questions/78127837/best-way-to-set-up-and-train-a-model-to-predict-event-registration-based-on-past</guid>
      <pubDate>Fri, 08 Mar 2024 12:59:14 GMT</pubDate>
    </item>
    <item>
      <title>无法进行网格搜索和训练模型</title>
      <link>https://stackoverflow.com/questions/78127612/not-able-to-do-grid-search-and-train-the-model</link>
      <description><![CDATA[我正在研究基本的文本分类问题，我想使用堆叠分类器以及对基本分类器的参数进行一些微调以获得高精度结果。
我的数据集有 8000 行和 2 列（文本和类）。下面的代码似乎被卡住了，我不熟悉该领域（初学者）来发现问题。
导入 pandas 作为 pd
从 sklearn.model_selection 导入 GridSearchCV，train_test_split
从 sklearn.ensemble 导入 StackingClassifier
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.svm 导入 NuSVC
从 sklearn.discriminant_analysis 导入 LinearDiscriminantAnalysis
从sklearn.metrics导入accuracy_score、log_loss、classification_report、confusion_matrix

# 定义分类器的参数网格
param_grid_nusvc = {
    “努”：[0.1，0.3，0.5，0.7，0.9]，
    &#39;内核&#39;：[&#39;线性&#39;，&#39;rbf&#39;]，
}

param_grid_logreg = {
    ‘C’: [0.1, 1, 10],
    &#39;惩罚&#39;: [&#39;l1&#39;, &#39;l2&#39;],
}

# 以更高的清晰度对分类器执行网格搜索
nusvc_grid_search = GridSearchCV(NuSVC(probability=True), param_grid_nusvc, cv=2, rating=&#39;accuracy&#39;) # 使用准确度评分
logreg_grid_search = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=2, 评分=&#39;准确度&#39;)

nusvc_grid_search.fit(X_train, y_train)
logreg_grid_search.fit(X_train, y_train)

# 获取最佳参数
best_params_nusvc = nusvc_grid_search.best_params_
best_params_logreg = logreg_grid_search.best_params_

# 设置具有最佳参数的基分类器
best_nusvc = NuSVC(概率=True, **best_params_nusvc)
best_logreg = LogisticRegression(**best_params_logreg)

# 设置堆叠分类器
sc = 堆叠分类器(
    估计量=[
        (&#39;NuSVC&#39;, best_nusvc),
        （&#39;LDA&#39;，线性判别分析（））
    ],
    最终估计器=best_logreg
）

sc.fit(X_train, y_train)

# 评估组合分类器
print(&#39;****结果****&#39;)
train_predictions = sc.predict(X_test)
acc = 准确度_分数(y_test, train_predictions)
print(&quot;准确度: {:.4%}&quot;.format(acc))

train_predictions_proba = sc.predict_proba(X_test)
ll = log_loss(y_test, train_predictions_proba)
print(&quot;对数丢失: {}&quot;.format(ll))

# 打印分类报告（可选）
print(&#39;\n分类报告:&#39;)
打印（分类报告（y_test，train_predictions））

# 打印混淆矩阵（可选）
print(&#39;\n混淆矩阵:&#39;)
打印（confusion_matrix（y_test，train_predictions））

上面的一些更改是根据 chatGPT 的建议进行的，以指导我如何使用网格搜索进行微调。代码似乎卡住了（大约 20 分钟）。如果没有网格搜索，它似乎可以轻松地在 2-3 分钟内运行。]]></description>
      <guid>https://stackoverflow.com/questions/78127612/not-able-to-do-grid-search-and-train-the-model</guid>
      <pubDate>Fri, 08 Mar 2024 12:17:54 GMT</pubDate>
    </item>
    <item>
      <title>在 k 折交叉验证后，如何为训练数据集选择人工神经网络的权重？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78127470/how-could-i-select-the-weights-of-an-artificial-neural-network-for-the-training</link>
      <description><![CDATA[我将数据集 80% 用于训练，20% 用于测试。我使用 5 倍交叉验证来评估我的模型。最后我得到了 5 个不同权重的神经网络。我应该选择哪一个？准确率最高的那个？]]></description>
      <guid>https://stackoverflow.com/questions/78127470/how-could-i-select-the-weights-of-an-artificial-neural-network-for-the-training</guid>
      <pubDate>Fri, 08 Mar 2024 11:46:16 GMT</pubDate>
    </item>
    <item>
      <title>添加到我的应用程序时，CreateML 模型无法按预期工作[关闭]</title>
      <link>https://stackoverflow.com/questions/78127234/createml-model-doesnt-work-as-expected-when-added-to-my-application</link>
      <description><![CDATA[我有一个训练有素的模型来识别深蹲（好的和坏的重复）。当我使用一些测试数据预览它时，它似乎在 CreateML 中完美运行，尽管将其添加到我的应用程序后，模型似乎不准确，并且大多数时候会混淆操作。有谁知道问题是否与代码相关，或者与模型本身及其如何分析实时数据有关？
下面我添加了“Good Squats”功能之一大多数时候甚至不会被调用（即使信心较低）。大多数时候，模型将所有事情都归为糟糕的深蹲，尽管事实显然并非如此。
问题可能是我的数据集没有足够的视频吗？
如果操作==“GoodForm” &amp;&amp;信心&gt; 0.80&amp;&amp; !squatDetected {
            打印（“好形式”）
            蹲检测=真
            
            DispatchQueue.main.asyncAfter(截止日期: .now() + 1.5) {
                self.squatDetected = false
            }
            DispatchQueue.main.async {
                self.showGoodFormAlert（带有：信心）
                音频服务PlayAlertSound（系统声音ID（1322））
            }
        }

我尝试过使用不同的 fps 设置和动作持续时间训练其他模型。目前我拥有的最好的设置是：120FPS 1.5s 动作持续时间。
更改了我的预测函数，使其仅分析每 10（以及每 20）帧而不是每帧。还是什么都没有]]></description>
      <guid>https://stackoverflow.com/questions/78127234/createml-model-doesnt-work-as-expected-when-added-to-my-application</guid>
      <pubDate>Fri, 08 Mar 2024 11:03:19 GMT</pubDate>
    </item>
    <item>
      <title>siann的解决方案有一个问题： ValueError: Variable <tf.Variable 'u/bias:0' shape=(1,) dtype=float64> has `None` forgradient</title>
      <link>https://stackoverflow.com/questions/78125337/there-is-a-problem-with-scianns-solution-valueerror-variable-tf-variable-u</link>
      <description><![CDATA[使用scinn求解偏微分方程时出现问题，结果显示：
&lt;块引用&gt;
ValueError：变量 渐变有“无”。请确保您的所有操作都定义了梯度（即可微分）。无梯度的常见操作：K.argmax、K.round、K.eval。

导入数学
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 siann 导入为 sn
从 siann.utils.math 导入 diff、sign、sin、cos、tan、exp、sqrt、pow


亩=0.20
罗 = 1000
xE = 21*1000000000
G = xE/(2*(1 + mu))
cp = math.sqrt(xE*(1 - mu)/(rho*(1 + mu)*(1 - 2*mu)))
r0 = 2.5
xdb = 100/1000
xdp = 2.5*100/1000
ρ0 = 1000
xD = 4000
SB = 4000/1000
r0 = 3.0
b = 2.0
阿尔法 = 2000

# 计算A0
定义 A0():
    term1 = xdb/(8*sb)*rho0*xD**2
    项 2 = (xdp/xdb)**2.2
    返回第 1 项/第 2 项

# 待解变量
r = sn.Variable(&#39;r&#39;, dtype=&#39;float64&#39;)
z = sn.Variable(&#39;z&#39;, dtype=&#39;float64&#39;)
t = sn.Variable(&#39;t&#39;, dtype=&#39;float64&#39;)
u = sn.Functional(&#39;u&#39;, [r, z, t], 4*[40], &#39;tanh&#39;)

# 偏微分方程
PDE1= diff(u,r,阶=2)+1/r*diff(u,r)+diff(u,z,阶=2)-1/cp*diff(u,t,阶=2)


＃边界条件
公差=0.0000001
BC1= (1-符号(t-TOL))*(1-符号(r-r0-TOL))*(diff(u,z))
BC2=(1+符号(z-TOL))*(1-符号(z-b-TOL))*(1-符号(r-r0-TOL))*(xE/(1+mu)*(mu/( 1-2*mu)*(diff(u,r,阶=2)+diff(u,r)/r+diff(u,z,阶=2))+diff(u,r,阶=2) )-A0()*exp(-1*alpha*t))

# 训练和验证模型
m = sn.SciModel([r,z,t], [PDE1, BC1,BC2])
r_data,z_data,t_data = np.meshgrid(
    np.linspace(r0, 10, 40),
    np.linspace(0, 5, 40),
    np.linspace(0, 0.001, 100)
）
 
# 这一步出错了
h = m.train([r_data,z_data,t_data], 3*[&#39;零&#39;],learning_rate=0.002, epochs=1000, verbose=0)

r_test,z_test ,t_test = np.meshgrid(
    np.linspace(0, 10, 40),
    np.linspace(0, 5, 40),
    np.linspace(0, 0.001, 80)
）
u_pred = u1.eval(m, [r_test,z_test ,t_test])

图 = plt.figure(figsize=(3, 4))
plt.pcolor(r_test, z_test, u_pred, cmap=&#39;地震&#39;)
plt.xlabel(&#39;r&#39;)
plt.ylabel(&#39;z&#39;)
plt.colorbar()

我试图解决它，然后在 h = m.train(...) 步骤出现了 ValueError。]]></description>
      <guid>https://stackoverflow.com/questions/78125337/there-is-a-problem-with-scianns-solution-valueerror-variable-tf-variable-u</guid>
      <pubDate>Fri, 08 Mar 2024 03:00:25 GMT</pubDate>
    </item>
    <item>
      <title>机器学习通过 SVC 症状预测疾病 [关闭]</title>
      <link>https://stackoverflow.com/questions/78123509/machine-learning-prediction-of-disease-by-symptoms-with-svc</link>
      <description><![CDATA[我正在尝试创建一种通过症状来预防疾病的功能，但我有什么想法吗？
所以我想创建 pred 函数来预防症状引起的疾病
它会像：
症状 = [“皮肤皮疹”、“连续打喷嚏”、“瘙痒”]
pred（症状，模型）

这是我当前的代码：
将 pandas 导入为 pd
从 sklearn.preprocessing 导入 LabelEncoder、StandardScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入准确度_分数、精度_分数、召回_分数、f1_分数

def load_and_preprocess_data(文件路径):
    df = pd.read_csv(文件路径)

    # 识别非数字列（不包括“疾病”）
    non_numeric_cols = df.select_dtypes(include=[&#39;object&#39;]).columns.difference([&#39;Disease&#39;])
    print(&quot;非数字列（不包括&#39;疾病&#39;）：&quot;, non_numeric_cols)

    df.fillna(0,就地=True)

    如果 df.columns 中有“疾病”：
        le = 标签编码器()
        df[&#39;疾病&#39;] = le.fit_transform(df[&#39;疾病&#39;])

    X = df.drop(&#39;疾病&#39;, axis=1)
    y = df[&#39;疾病&#39;]

    # 特征缩放（如果需要，考虑替代缩放方法）
    定标器=标准定标器()
    X = pd.DataFrame(scaler.fit_transform(X))

    返回 X, y

def build_and_train_model(X_train, y_train):
    # 使用不同的内核进行实验（线性、rbf 等）
    model = SVC(kernel=&#39;linear&#39;) # 替换为所选内核

    # 训练模型
    model.fit(X_train, y_train)

    返回模型

def Predict_and_evaluate(模型, X_test, y_test):
    y_pred = model.predict(X_test)
    准确度=准确度_得分(y_test, y_pred)
    精度 = precision_score(y_test, y_pred, 平均值=&#39;加权&#39;)
    召回率=召回率（y_test，y_pred，平均值=&#39;加权&#39;）
    f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)

    print(“准确度：”, 准确度)
    print(&quot;精度：&quot;, 精度)
    print(“回忆：”, 回忆)
    print(&quot;F1-分数：&quot;, f1)


def main():
    文件路径 = &#39;数据集/merged_dataset.csv&#39;

    X, y = load_and_preprocess_data(文件路径)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    模型 = build_and_train_model(X_train, y_train)

    预测和评估（模型，X_测试，y_测试）


如果 __name__ == “__main__”：
    主要的（）

这就是 csv 的样子：
疾病、瘙痒、皮疹、结节性皮肤疹、连续打喷嚏
真菌感染,1,3,4,0,0
真菌感染,1,3,0,0,6
过敏,1,3,0,0,0
... 和更多

是症状的严重程度，如 1,3,4,0,0（0 表示对该疾病不重要）
所以我必须用我训练的模型通过症状列表来猜测疾病我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78123509/machine-learning-prediction-of-disease-by-symptoms-with-svc</guid>
      <pubDate>Thu, 07 Mar 2024 18:09:35 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程数据错误</title>
      <link>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</link>
      <description><![CDATA[如何在ma​​tlab的fitrgp函数中插入误差数组来进行高斯过程回归？我有一个数组 x，其他数组 y 和标准差数组 &lt; code&gt;delta_y 与 y 关联。
只有 x 和 y，我可以使用 gprMdl = fitrgp(x, y)，但是如何添加  delta_y 作为y 的errorbar？]]></description>
      <guid>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</guid>
      <pubDate>Wed, 06 Mar 2024 15:02:10 GMT</pubDate>
    </item>
    <item>
      <title>跨多个模型的交叉验证的一致性[关闭]</title>
      <link>https://stackoverflow.com/questions/77778800/consistency-in-cross-validation-folds-across-multiple-models</link>
      <description><![CDATA[我目前正在做一个机器学习项目，其中使用三种不同的模型：随机森林、AdaBoost 和梯度提升。对于每个模型，我将它们应用于一组训练和测试数据。此外，我计划将五重交叉验证纳入我的实验中。
我的问题涉及这些模型之间交叉验证的实施。具体来说，我是否应该对所有三个模型（RF、ADA 和 GB）使用相同的五倍，以确保每个模型训练和测试的数据的一致性？或者，为每个模型生成不同的折叠集，从而独立地对 RF、ADA 和 GB 进行交叉验证过程是否更合适？
哪种方法更有利于实验的完整性？在这种情况下有什么标准做法或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77778800/consistency-in-cross-validation-folds-across-multiple-models</guid>
      <pubDate>Mon, 08 Jan 2024 10:17:44 GMT</pubDate>
    </item>
    <item>
      <title>Hard Voting 如何在 scikit-learn 的 VotingClassifier 中选择偶数个分类器的结果？</title>
      <link>https://stackoverflow.com/questions/48991245/how-does-hard-voting-select-a-result-with-an-even-number-of-classifiers-in-a-vot</link>
      <description><![CDATA[我在投票分类器中有两个分类器，它们都用于预测实例是 0 类还是 1 类。结果是使用硬投票（使用多数投票）进行聚合的，但是我不确定 VotingClassifier 是如何进行聚合的使用偶数个分类器做出决策。]]></description>
      <guid>https://stackoverflow.com/questions/48991245/how-does-hard-voting-select-a-result-with-an-even-number-of-classifiers-in-a-vot</guid>
      <pubDate>Mon, 26 Feb 2018 14:54:46 GMT</pubDate>
    </item>
    </channel>
</rss>