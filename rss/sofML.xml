<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 31 Aug 2024 03:16:47 GMT</lastBuildDate>
    <item>
      <title>transformers/LLM 与 pytorch 内存不足</title>
      <link>https://stackoverflow.com/questions/78934229/transformers-llm-with-pytorch-running-out-of-memory</link>
      <description><![CDATA[我改编了 https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858 中的转换器，用于将另一种语言翻译成我自己的两种形式语言（每种语言的词汇量为 23）。

最大输入长度约为 600，最大输出长度约为 3000，因此我选择了略大于 3000 的联合序列长度。
训练和验证的批量大小均为 1。
嵌入维度为 8，前馈维度为 32，头数为 8，块数为 6。
数据类型为 int64。

使用这些超参数，训练在我的 CPU 上使用 ~15 GiB RAM。一旦训练的批处理大小 &gt; 1，它就会尝试分配超过 40 GiB 并崩溃。因此，在 Colab 上进行免费训练是不可行的。

我做错了什么？我改编的转换器具有更大的超参数和词汇，网站声称可以在 Colab 上对其进行训练。也许其中一个原因是我的文本太长了？（我无法轻松地在测试较短的文本上进行训练，因为我的数据集不包含很多文本。）
如何减少内存占用？
]]></description>
      <guid>https://stackoverflow.com/questions/78934229/transformers-llm-with-pytorch-running-out-of-memory</guid>
      <pubDate>Sat, 31 Aug 2024 03:08:19 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：实现批量损失（RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn）</title>
      <link>https://stackoverflow.com/questions/78934120/pytorch-implementing-batch-loss-runtimeerror-element-0-of-tensors-does-not-re</link>
      <description><![CDATA[我目前正在训练一个自动编码器，针对我的具体用例，我在 PyTorch 中实现了基于 Wasserstein 度量（地球移动度量）的自定义损失。我使用这个的一般想法是，下面代码片段中的 pred 和 target 是一批向量，比如 batch_size x vector_length
def wasserstein_distance(pred, target, num_bins=200):
# 获取列数（特征）
num_columns = pred.size(1)

# 计算每列的 Wasserstein 距离
distances = []
for i in range(num_columns):
# 获取列的预测值和目标值
pred_col = pred[:, i]
target_col = target[:, i]

# 确定列的直方图范围
min_val = min(pred_col.min().item(), target_col.min().item())
max_val = max(pred_col.max().item(), target_col.max().item())

# 计算预测值和目标值的直方图
pred_hist = torch.histc(pred_col, bins=num_bins, min=float(min_val), max=float(max_val))
target_hist = torch.histc(target_col, bins=num_bins, min=float(min_val), max=float(max_val))

# 将直方图标准化以形成概率分布
pred_hist /= pred_hist.sum()
target_hist /= target_hist.sum()

# 计算累积分布函数 (CDF)
pred_cdf = torch.cumsum(pred_hist, dim=0)
target_cdf = torch.cumsum(target_hist, dim=0)

# 计算 Wasserstein 距离（地球移动距离）
wasserstein_dist = torch.sum(torch.abs(pred_cdf - target_cdf))

distances.append(wasserstein_dist)

# 计算所有列的 Wasserstein 距离的平均值
mean_wasserstein_distance = torch.mean(torch.stack(distances))

return mean_wasserstein_distance

我在以下训练函数中使用它：
def train_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, epoch_count: int, learning_rate: float) -&gt; np.ndarray:
print(&quot;##### 开始训练模型 #####&quot;)
model.train()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)
loss = []

for epoch in range(epoch_count):
total_loss = 0.0
total_samples = 0

for batch in data_loader:
x = batch[0]
x = x.to(device)
optimizer.zero_grad()
pred_ae = model(x)
loss_ae = wasserstein_distance(pred_ae, x[:, 6:])

loss_ae.backward()
optimizer.step()

total_loss += loss_ae.item()
total_samples += x.size(0)

avg_loss = total_loss / total_samples
loss.append(avg_loss)

print(f&#39;Epoch: {epoch} Loss per unit: {avg_loss}&#39;)

print(&quot;##### FINISHED TRAINING OF MODEL #####&quot;)
return model, np.array(losses)


但是，我收到以下错误：
File &quot;&quot;, line 432, in &lt;module&gt;
model,losses = train_model(model,data,50,0.0001)
文件&quot;&quot;，第 237 行，在 train_model 中
loss_ae.backward()
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py&quot;，第 522 行，在 Backward 中
torch.autograd.backward(
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py&quot;，第 266 行，在 Backward 中
Variable._execution_engine.run_backward(# 调用 C++ 引擎运行反向传递
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn


我确信每个 requires_grad 默认为 true（我没有手动更改任何内容）。我对 PyTorch 有点陌生，但我怀疑问题在于我为损失返回了一个值，而 Backward 期望每个张量都有一个值？？？但这不是我真正想要的训练方案，我希望每个更新都是特定于批次的（具体来说，学习每个批次中每列的分布）
如能提供帮助，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78934120/pytorch-implementing-batch-loss-runtimeerror-element-0-of-tensors-does-not-re</guid>
      <pubDate>Sat, 31 Aug 2024 01:05:47 GMT</pubDate>
    </item>
    <item>
      <title>在 colab 上安装 nvstrings</title>
      <link>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</link>
      <description><![CDATA[在此处输入图片描述
如何克服这个问题？
我正在使用托管在 colab 免费层 gpu 上的 spacy en_core_web_trf 开发文档解析器模型，它需要 gpu 设备上的输入字符串，即为什么我需要安装 nvstrings 库。我已经在笔记本上安装了 nvidia RAPIDS 库，但它似乎不包含 nvstrings 库]]></description>
      <guid>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</guid>
      <pubDate>Fri, 30 Aug 2024 17:30:48 GMT</pubDate>
    </item>
    <item>
      <title>在 Microsoft Ai Hub 上应使用哪种 RAG 架构和流程来从超大文档生成内容</title>
      <link>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</link>
      <description><![CDATA[我正在研究一个用例，旨在自动生成响应，该响应是对提案请求 (RFP) 的回复 - 该响应回答了请求中的问题，并且实际上包含了有关我们公司的详细信息。
外部企业首先向我们提供请求，目前人工需要花费很长时间来编写响应，响应可能长达 90 页。
在理想情况下，我会接受请求，然后自动回复 90 页，但目前没有一个 LLM 可以做到这一点。
Microsoft 建议在使用 RAG 之前将之前的响应拆分成小块，以便可以检索和生成所有信息。但我担心不同部分之间会丢失上下文和理解。
有人遇到过类似的问题吗？你是如何设计这个解决方案的？现在，我发现有必要给这些回复贴上标签，然后将它们分成小节，为每个小节生成内容并将它们缝合在一起（对请求也做类似的事情，但这些内容不太广泛，通常只有 3-4 页长）。
我使用 Microsoft Ai Hub 和 Prompt Flows 来完成大部分工作。]]></description>
      <guid>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</guid>
      <pubDate>Fri, 30 Aug 2024 15:02:23 GMT</pubDate>
    </item>
    <item>
      <title>Mask r-cnn 模型未收敛，训练和验证的准确度在 NaN 和 0.09 之间波动[关闭]</title>
      <link>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</guid>
      <pubDate>Fri, 30 Aug 2024 14:09:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 NEAT-Python 库重新利用训练的神经网络</title>
      <link>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</link>
      <description><![CDATA[我正在使用 NEAT-Python 库编写一个程序，以训练 AI 代理在 Python 中玩贪吃蛇游戏。训练后，我想导出网络，然后使用它来可视化它如何用另一种语言（例如 JavaScript）做出决策。是否可以导出网络，然后在 Python 之外使用它？我尝试查看 Uber Research 的 PyTorch-NEAT 库，但它并不是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</guid>
      <pubDate>Fri, 30 Aug 2024 13:57:57 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来找到模式客户资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</link>
      <description><![CDATA[我有一个数据集，其中包含从一家虚构公司购买产品的客户的个人特征。最初，我没有任何目标变量，只有他们的特征。我的目标是找到一种模式，该模式不一定是每列中最常见的特征。例如，是否可以使用 RandomForest 来做到这一点？或者我应该使用其他技术？
数据集具有类似于以下的结构。这些列都是 object 格式，并且有一些 NaN 值表示为 &#39;Blank&#39;:
日期姓名薪资职位年龄
&#39;05/10/2023&#39; &#39;Daniel&#39; &#39;10,000&#39; &#39;IT&#39; 32
&#39;05/12/2024&#39; &#39;John&#39; &#39;9,000&#39; &#39;Blank&#39; 27
&#39;03/01/2023&#39; &#39;Niel&#39; &#39;Blank&#39; &#39;数据科学家&#39; 21
&#39;03/01/2023&#39; &#39;Isa&#39; &#39;10,000&#39; &#39;工程师&#39; 51
&#39;05/10/2023&#39; &#39;Ana&#39; &#39;11,000&#39; &#39;数据科学家&#39; 52
&#39;05/12/2024&#39; &#39;Ian&#39; &#39;9,500&#39; &#39;Doctor&#39; 48
&#39;03/01/2023&#39; &#39;Fred&#39; &#39;Blank&#39; &#39;IT&#39; 21
&#39;03/01/2023&#39; &#39;Carol&#39; &#39;15,000&#39; &#39;Blank&#39; 30

我正在考虑返回输出，例如，说明构成最标准配置文件的特征，例如：
最标准的配置文件是：薪水 x、职位 y 和年龄 z。

我考虑过使用聚类，但我不认为这是最好的方法（例如，薪水的输出是一个简单的平均值）。我认为最好的方法是创建一个可能并不一定存在的配置文件，并且基于研究每个变量（薪水、职位和年龄）的模式。
# 编码分类变量
df[&#39;Position&#39;] = pd.Categorical(df[&#39;Position&#39;]).codes

# 执行聚类
kmeans = KMeans(n_clusters=1, random_state=42)
kmeans.fit(df[[&#39;Salary&#39;, &#39;Position&#39;, &#39;Age&#39;]])

# 获取聚类的质心
centroid = kmeans.cluster_centers_[0]

有没有更好的方法？NLP 或 RandomForest 是一种选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</guid>
      <pubDate>Fri, 30 Aug 2024 13:39:34 GMT</pubDate>
    </item>
    <item>
      <title>zero123 的更大分辨率输出</title>
      <link>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</link>
      <description><![CDATA[我在 InstantMesh 环境中使用 zero123，我想知道 zero123 是否有可能输出更大分辨率的图像？
目前的分辨率是 320x320，对于从 InstantMesh 的 3D 重建管道获得良好的输出纹理来说，这个分​​辨率有点低。]]></description>
      <guid>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</guid>
      <pubDate>Fri, 30 Aug 2024 13:30:55 GMT</pubDate>
    </item>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理 [关闭]</title>
      <link>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个使用多视图模型 (zero123) 的 3D 重建管道，该模型可从一张输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
我现在的问题是：InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？]]></description>
      <guid>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Fri, 30 Aug 2024 12:00:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来追踪公司客户的资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</link>
      <description><![CDATA[我的目标是计算客户离开公司的流失风险。我想到这个方法：

生成一份代表客户历史中最突出特征的资料，并计算新客户之间的相似度。

我该如何追踪流失风险最高的人的资料？]]></description>
      <guid>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</guid>
      <pubDate>Wed, 28 Aug 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>了解 Vits 对 HiFi-GAN 的使用</title>
      <link>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</link>
      <description><![CDATA[我正在（尝试）学习语音合成的 AI/ML，并尝试理解 Vits 如何使用 HiFi-GAN。
据我所知，Vits 会将文本输入转换为梅尔频谱图，然后由 HiFi-GAN 转换为音频波。
让我困惑的是为什么从 Vits 发送到 HiFi-GAN 的输入不是梅尔频谱图。
例如，当我测试其他模型并将以下代码添加到 HiFi-GAN 的正向方法时：
class Generator(torch.nn.Module):
...
def forward(self, x):
plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
...
...

它保存了正确的图像，看起来像梅尔频谱图图像，但是，当我对 vits 执行相同操作时，保存的图像是纯绿色图像，当然不是梅尔频谱图的表示。
但生成的音频文件当然是有效的音频文件。
有人能向我解释一下吗？
我正在评估一些神经 tts 模型，我想要做的是保存由模型创建的梅尔频谱图，以便稍后进行比较，并通过不同的声码器运行它们以进行比较。
我注意到 vits repo 中的 HiFi-GAN 代码与原始 repo 略有不同，但我无法理解为什么。
有什么方法可以将输入参数 x 转换为梅尔频谱图表示，而无需先将其转换为音频，然后再将音频转换为梅尔？]]></description>
      <guid>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</guid>
      <pubDate>Sat, 15 Jun 2024 02:17:36 GMT</pubDate>
    </item>
    <item>
      <title>残差图诊断以及如何改进回归模型</title>
      <link>https://stackoverflow.com/questions/62459677/residual-plot-diagnostic-and-how-to-improve-the-regression-model</link>
      <description><![CDATA[在为此住房数据集创建回归模型时，我们可以绘制实值函数中的残差。
from sklearn.linear_model import LinearRegression

X = housing[[&#39;lotsize&#39;]]
y = housing[[&#39;price&#39;]]

model = LinearRegression()
model.fit(X, y)

plt.scatter(y,model.predict(X)-y)


我们可以清楚地看到差异（预测 - 实际值）对于较低的价格主要为正，而对于较高的价格，差异为负。
对于线性回归来说也是如此，因为模型针对 RMSE 进行了优化（因此不考虑残差的符号）。
但是在执行 KNN 时
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors = 3)

我们可以找到类似的图。

在这种情况下，我们可以给出什么解释，我们如何改进模型。
编辑：我们可以使用所有其他预测因子，结果类似。
housing = housing.replace(to_replace=&#39;yes&#39;, value=1, regex=True)
housing = housing.replace(to_replace=&#39;no&#39;, value=0, regex=True)
X = housing[[&#39;lotsize&#39;,&#39;bedrooms&#39;,&#39;stories&#39;,&#39;bathrms&#39;,&#39;bathrms&#39;,&#39;driveway&#39;,&#39;recroom&#39;,
&#39;fullbase&#39;,&#39;gashw&#39;,&#39;airco&#39;,&#39;garagepl&#39;,&#39;prefarea&#39;]]

下图为具有 3 个邻居的 KNN。如果有 3 个邻居，人们会预期过度拟合，我不明白为什么会有这种趋势。
]]></description>
      <guid>https://stackoverflow.com/questions/62459677/residual-plot-diagnostic-and-how-to-improve-the-regression-model</guid>
      <pubDate>Thu, 18 Jun 2020 21:27:35 GMT</pubDate>
    </item>
    <item>
      <title>多项式回归度增加后训练得分降低</title>
      <link>https://stackoverflow.com/questions/47717818/train-score-diminishes-after-polynomial-regression-degree-increases</link>
      <description><![CDATA[我尝试使用线性回归将多项式拟合到一组添加了噪声的正弦信号中的点，使用 sklearn 中的 linear_model.LinearRegression。
正如预期的那样，训练和验证分数随着多项式度数的增加而增加，但在 20 度左右之后，事情开始变得奇怪，分数开始下降，模型返回的多项式看起来根本不像我用来训练它的数据。
下面是一些可以看到这种情况的图表，以及生成回归模型和图表的代码：
在度数 = 17 之前，它是如何正常工作的。原始数据 VS 预测：

之后情况变得更糟：

验证曲线，增加多项式的次数：

from sklearn.pipeline import make_pipeline
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.learning_curve import验证曲线

def make_data(N, err=0.1, rseed=1):
rng = np.random.RandomState(1)
x = 10 * rng.rand(N)
X = x[:, None]
y = np.sin(x) + 0.1 * rng.randn(N)
if err &gt; 0:
y += err * rng.randn(N)
return X, y

def PolynomialRegression(degree=4):
return make_pipeline(PolynomialFeatures(degree),
LinearRegression())

X, y = make_data(400)

X_test = np.linspace(0, 10, 500)[:, None]
degrees = np.arange(0, 40)

plt.figure(figsize=(16, 8))
plt.scatter(X.flatten(), y)
for degree in degrees:
y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)
plt.plot(X_test, y_test, label=&#39;degre={0}&#39;.format(degree))
plt.title(&#39;原始数据 VS 不同预测值度&#39;)
plt.legend(loc=&#39;best&#39;);

degree = np.arange(0, 40)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
&#39;polynomialfeatures__degree&#39;,
degree, cv=7)

plt.figure(figsize=(12, 6))
plt.plot(degree, np.median(train_score, 1), marker=&#39;o&#39;, 
color=&#39;blue&#39;, label=&#39;training score&#39;)
plt.plot(degree, np.median(val_score, 1), marker=&#39;o&#39;,
color=&#39;red&#39;, label=&#39;validation score&#39;)
plt.legend(loc=&#39;best&#39;)
plt.ylim(0, 1)
plt.title(&#39;学习曲线，增加多项式的次数&#39;)
plt.xlabel(&#39;degree&#39;)
plt.ylabel(&#39;score&#39;);

我知道预期的事情是，当模型的复杂性增加时，验证分数会下降，但为什么训练分数也会下降呢？我可能遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/47717818/train-score-diminishes-after-polynomial-regression-degree-increases</guid>
      <pubDate>Fri, 08 Dec 2017 15:53:36 GMT</pubDate>
    </item>
    </channel>
</rss>