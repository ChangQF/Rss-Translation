<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 24 Nov 2024 03:33:01 GMT</lastBuildDate>
    <item>
      <title>有人能帮我把这个图弄得更整洁一点吗？</title>
      <link>https://stackoverflow.com/questions/79218937/can-someone-help-me-make-this-graph-look-a-bit-neater</link>
      <description><![CDATA[import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import datetime

# 加载特斯拉数据集
tesla_stock = pd.read_csv(&#39;C:/Users/Admin/Downloads/AI/Tesla.csv&#39;)

# 动态处理缺失或重命名的“收盘价”列
possible_target_columns = [&#39;Close&#39;, &#39;Last&#39;, &#39;Price&#39;, &#39;Adjusted Close&#39;, &#39;VWAP&#39;, 
&#39;Close/Last&#39;]
target_column = None

# 打印可用列以进行调试
print(&quot;数据集中可用的列：&quot;, tesla_stock.columns)

for col in possible_target_columns:
if col in tesla_stock.columns:
target_column = col
print(f&quot;使用 &#39;{col}&#39; 作为预测的目标列。&quot;)
break

# 处理未找到合适列的情况
if target_column is None:
print(&quot;未找到合适的预测列。请检查数据集。&quot;)
raise KeyError(&quot;确保数据集包含带有股票价格的列（例如，&#39;Close&#39;、
&#39;Close/Last&#39;）。&quot;)

# 清理数字列（删除 &#39;$&#39; 并转换为浮点数）
for column in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close/Last&#39;, &#39;Volume&#39;]:
if column in tesla_stock.columns:
tesla_stock[column] = tesla_stock[column].replace(&#39;[\$,]&#39;, &#39;&#39;, 
regex=True).astype(float)

# 将“日期”转换为日期时间并设置为索引
tesla_stock[&#39;Date&#39;] = pd.to_datetime(tesla_stock[&#39;Date&#39;])
tesla_stock.set_index(&#39;Date&#39;, inplace=True)

# 定义特征和目标变量
features = [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Volume&#39;]
X = tesla_stock[features]
y = tesla_stock[target_column]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 进行预测
predicted = model.predict(X_test)

# 评估模型
print(&quot;模型得分 (R²)：&quot;, model.score(X_test, y_test))
print(&quot;平均绝对误差：&quot;, metrics.mean_absolute_error(y_test, predicted))
print(&quot;均方误差：&quot;, metrics.mean_squared_error(y_test, predicted))
print(&quot;均方根误差：&quot;, math.sqrt(metrics.mean_squared_error(y_test, 
predicted)))

# 绘制测试集的实际价格与预测价格
dfr = pd.DataFrame({&#39;Actual&#39;: y_test, &#39;Predicted&#39;: predicted})
plt.figure(figsize=(14, 8))
dfr.head(25).plot(kind=&#39;bar&#39;, figsize=(14, 8))
plt.title(&quot;实际价格与预测价格&quot;)
plt.xlabel(&quot;样本&quot;)
plt.ylabel(&quot;价格 (USD)&quot;)
plt.xticks(rotation=45, ha=&quot;right&quot;)
plt.tight_layout()
plt.show()

# 预测未来 30 天
last_date = tesla_stock.index[-1] # 历史数据中的最后一个日期
last_price = tesla_stock[target_column].iloc[-1] # 历史数据中的最后一个价格
future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, 31)] # 
生成未来 30 天

# 创建占位符 DataFrame用于未来特征
future_features = pd.DataFrame(index=future_dates)
for feature in features:
if feature in tesla_stock.columns:
future_features[feature] = tesla_stock[feature].mean() # 使用每个特征的平均值

# 使用训练模型预测未来价格
future_predictions = model.predict(future_features)

# 结合历史和未来数据，绘制无缝图
all_dates = list(tesla_stock.index) + list(future_dates) # 结合历史和未来日期
all_prices = list(tesla_stock[target_column]) + list(future_predictions) # 结合历史和预测价格

# 绘制历史数据和未来预测
plt.figure(figsize=(14, 8))
plt.plot(tesla_stock.index, tesla_stock[target_column], label=&#39;历史价格&#39;, 
color=&#39;blue&#39;)
plt.plot(future_dates, future_predictions, label=&#39;30 天未来预测&#39;, 
color=&#39;red&#39;)
plt.title(&#39;特斯拉未来 30 天股价预测&#39;)
plt.xlabel(&#39;日期&#39;)
plt.ylabel(&#39;价格 (美元)&#39;)
plt.legend()
plt.xticks(rotation=45)
plt.show()

# 显示未来预测
future_features[&#39;Predicted_Close&#39;] = future_predictions
print(future_features[[&#39;Predicted_Close&#39;]])

输入图片描述在这里
我试图让红线连接到图表的末端，然后弯曲到它所在的位置。而不是一开始就只是一条随机的短红线，因为它看起来有点混乱。我一直在尝试这样做，但我真的很难做到。如果有人能帮助我，我将不胜感激，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79218937/can-someone-help-me-make-this-graph-look-a-bit-neater</guid>
      <pubDate>Sat, 23 Nov 2024 22:00:52 GMT</pubDate>
    </item>
    <item>
      <title>抱抱脸生成方法后内存增加</title>
      <link>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</link>
      <description><![CDATA[我想使用 huggingface 的 codegemma 模型进行推理，但当我使用 model.generate(**inputs) 方法时，无论 max_token_len 数量是多少，使用 torch profiler 时，GPU 内存成本在峰值使用量中都会从 39 GB 增加到 49 GB。我知道我们需要在推理和上下文中保存模型的激活，例如 4096 个输入标记，但我不敢相信它可以增加 10 GB 的推理内存使用量。有人可以解释一下这是怎么回事吗？提前谢谢您。]]></description>
      <guid>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</guid>
      <pubDate>Sat, 23 Nov 2024 19:21:47 GMT</pubDate>
    </item>
    <item>
      <title>错误：TypeError：无法将 cuda:0 设备类型张量转换为 numpy。首先使用 Tensor.cpu() 将张量复制到主机内存</title>
      <link>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor</guid>
      <pubDate>Sat, 23 Nov 2024 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>具有动态约束的贷款人和借款人之间的多元化资金分配算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</guid>
      <pubDate>Sat, 23 Nov 2024 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>如何改进 CNN 的图像分类能力</title>
      <link>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218187/how-to-improve-cnn-for-classifying-images</guid>
      <pubDate>Sat, 23 Nov 2024 15:26:06 GMT</pubDate>
    </item>
    <item>
      <title>InvalidArgumentError：在使用单个图像输入进行预测时，TensorFlow 模型中只有一个输入大小可以为 -1，而不能同时为 0 和 1</title>
      <link>https://stackoverflow.com/questions/79215834/invalidargumenterror-only-one-input-size-may-be-1-not-both-0-and-1-in-tensorf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79215834/invalidargumenterror-only-one-input-size-may-be-1-not-both-0-and-1-in-tensorf</guid>
      <pubDate>Fri, 22 Nov 2024 16:34:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么以及如何在 ML 预处理中使用数据管道而不是仅仅使用 Pandas [关闭]</title>
      <link>https://stackoverflow.com/questions/79215565/why-and-how-to-use-data-pipelines-compared-to-just-pandas-in-preprocessing-for-m</link>
      <description><![CDATA[我们正在使用 SKlearn Pipelines。老实说，我还没有完全理解它们的意义。当我预处理数据时，我发现使用 pandas 更容易、更灵活。我可以随时在笔记本中直接检查 DataFrame，而使用 Pipelines 似乎并不那么简单。
此外，对于将字符串转换为整数等简单任务，使用自定义转换器和 ColumnTransformers（具有针对不同列的多个管道）通常感觉有点过度。
所以，我很好奇——更有经验的人如何处理这个问题？您是否坚持在大型项目中使用管道，还是将 pandas 混合用于更简单的任务，并仅使用管道进行扩展等操作？
从使用 pandas 进行探索和准备开始，然后在明确需要什么后将其全部形式化为管道是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/79215565/why-and-how-to-use-data-pipelines-compared-to-just-pandas-in-preprocessing-for-m</guid>
      <pubDate>Fri, 22 Nov 2024 15:15:51 GMT</pubDate>
    </item>
    <item>
      <title>PointPillars 模型调试请求</title>
      <link>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</guid>
      <pubDate>Fri, 22 Nov 2024 07:44:47 GMT</pubDate>
    </item>
    <item>
      <title>sklearn“transform_output”在Flask应用程序上下文和请求上下文中的设置不同</title>
      <link>https://stackoverflow.com/questions/77907033/sklearn-transform-output-setting-different-in-flask-application-context-v-requ</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77907033/sklearn-transform-output-setting-different-in-flask-application-context-v-requ</guid>
      <pubDate>Tue, 30 Jan 2024 14:17:46 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法使用 ML 从合同中提取信息，并将合同文件和目标字符串作为输入和输出？</title>
      <link>https://stackoverflow.com/questions/59409984/is-there-a-way-to-extract-information-from-contracts-using-ml-with-including-con</link>
      <description><![CDATA[我正在研究是否可以自动从合同中提取某些字段的信息（例如合同方、开始和结束日期）。
我想知道是否可以用机器学习提取这些信息，将整个合同作为输入，将信息作为输出，而无需标记或注释整个文本。
我理解应该针对每个目标字段分别运行提取。]]></description>
      <guid>https://stackoverflow.com/questions/59409984/is-there-a-way-to-extract-information-from-contracts-using-ml-with-including-con</guid>
      <pubDate>Thu, 19 Dec 2019 12:38:43 GMT</pubDate>
    </item>
    <item>
      <title>如何列出 OpenAI gym 中每个状态的可能后继状态？（严格针对普通 MDP）</title>
      <link>https://stackoverflow.com/questions/53690171/how-to-list-possible-successor-states-for-each-state-in-openai-gym-strictly-fo</link>
      <description><![CDATA[有没有办法遍历每个状态，强制环境进入该状态，然后采取一步，然后使用返回的“信息”字典来查看所有可能的后继状态？
或者有一种更简单的方法来恢复每个状态的所有可能的后继状态，也许隐藏在某个地方？
我在网上看到一个叫做 MuJoKo 或类似的东西有一个 set_state 函数，但我不想创建一个新的环境，我只想设置 openAi gym 已经提供的状态。
上下文：尝试实现拓扑顺序值迭代，这需要制作一个图表，其中每个状态都有一个指向任何动作可以转换到的任何状态的边。
我意识到显然在某些游戏中没有提供，但对于那些有它的游戏，有办法吗？
（除了运行游戏并采取我尚未采取的每一步的蛮力方法之外无论我降落在哪个状态，直到我到达所有状态并看到所有内容，这取决于游戏，可能需要很长时间）
这是我第一次使用 OpenAi gym，所以请尽可能详细地解释。例如，我不知道什么是 Wrappers。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/53690171/how-to-list-possible-successor-states-for-each-state-in-openai-gym-strictly-fo</guid>
      <pubDate>Sun, 09 Dec 2018 07:08:15 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 中的 KMeans 聚类</title>
      <link>https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark</link>
      <description><![CDATA[我有一个包含许多列的 spark 数据框“mydataframe”。我尝试仅在两列上运行 kmeans：lat 和 long（纬度和经度），使用它们作为简单值。我想仅基于这两列提取 7 个聚类，然后将聚类分配附加到我的原始数据框。我试过了：
从 numpy 导入数组
从 math 导入 sqrt
从 pyspark.mllib.clustering 导入 KMeans、KMeansModel

# 准备一个只有 2 列的数据框：
data = mydataframe.select(&#39;lat&#39;, &#39;long&#39;)
data_rdd = data.rdd # 需要是 RDD
data_rdd.cache()

# 构建模型（对数据进行聚类）
clusters = KMeans.train(data_rdd, 7, maxIterations=15, initializationMode=&quot;random&quot;)

但过了一会儿我收到错误：

org.apache.spark.SparkException：作业因阶段失败而中止：阶段 5191.0 中的任务 1 失败 4 次，最近一次失败：丢失任务1.3 阶段 5191.0（TID 260738，10.19.211.69，执行器 1）：org.apache.spark.api.python.PythonException：Traceback（最近一次调用最后一次）

我尝试分离并重新连接集群。结果相同。我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark</guid>
      <pubDate>Fri, 01 Dec 2017 02:22:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛至零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对一些数据进行预测。
我在一个小的数据集（大约 100 条记录）上尝试了它，它工作得很好。然后我插入了新的数据集，我发现 NN 收敛到 0 输出，误差大约收敛到正例数与总例数之比。
我的数据集由是/否特征（1.0/0.0）组成，基本事实也是是/否。
我的假设：

1) 存在输出为 0 的局部最小值（但我尝试了许多学习率和初始权重值，它似乎总是收敛到那里）

2) 我的权重更新是错误的（但在我看来很好）

3) 它只是一个输出缩放问题。我尝试缩放输出（即输出/最大值（输出）和输出/平均值（输出）），但结果并不好，如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？Softmax？ 
代码如下：
import pandas as pd
import numpy as np
import pickle
import random
from collections import defaultdict

alpha = 0.1
N_LAYERS = 10
N_ITER = 10
#N_FEATURES = 8
INIT_SCALE = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(float)
ytest = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
if(deriv==True):
return (1 - np.tanh(x)**2) * alpha
else:
return np.tanh(x)

def sigmoid(x,deriv=False):
if(deriv==True):
return x*(1-x)
else:
return 1/(1+np.exp(-x))

def relu(x,deriv=False):
if(deriv==True):
return 0.01 + 0.99*(x&gt;0)
else:
return 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

for i in range(N_LAYERS-1):
syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)

delta = defaultdict(np.array)

for j in xrange(N_ITER):
l[0] = X
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

error = (y - l[N_LAYERS])

e = np.mean(np.abs(error))
if (j% 1) == 0:
print &quot;\nIteration &quot; + str(j) + &quot; of &quot; + str(N_ITER)
print &quot;Error: &quot; + str(e)

delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
for i in range(N_LAYERS-1,0,-1):
error = delta[i+1].dot(syn[i].T)
delta[i] = error*relu(l[i],deriv=True) * alpha

for i in range(N_LAYERS):
syn[i] += l[i].T.dot(delta[i+1])

pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 准确率 = 真阳性 / (真阳性阳性 + 假阳性)

l[0] = Xtest
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

out = l[N_LAYERS]/max(l[N_LAYERS])

tp = float(0)
fp = float(0)
fn = float(0)
tn = float(0)

for i in l[N_LAYERS][:50]:
print i

for i in range(len(ytest)):
if out[i] &gt; 0.5 and ytest[i] == 1:
tp += 1
if out[i] &lt;= 0.5 and ytest[i] == 1:
fn += 1
if out[i] &gt; 0.5 且 ytest[i] == 0:
fp += 1
if out[i] &lt;= 0.5 且 ytest[i] == 0:
tn += 1

print &quot;tp: &quot; + str(tp)
print &quot;fp: &quot; + str(fp)
print &quot;tn: &quot; + str(tn)
print &quot;fn: &quot; + str(fn)

print &quot;\nprecision: &quot; + str(tp/(tp + fp))
print &quot;recall: &quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-measure:&quot; + str(f1)

输出结果如下：
第 0 次迭代（共 10 次）
错误： 0.222500767998

10 次迭代中的第 1 次
错误：0.222500771157

10 次迭代中的第 2 次
错误：0.222500774321

10 次迭代中的第 3 次
错误：0.22250077749

10 次迭代中的第 4 次
错误：0.222500780663

10 次迭代中的第 5 次
错误：0.222500783841

10 次迭代中的第 6 次
错误：0.222500787024

10 次迭代中的第 7 次
错误：0.222500790212

10 次迭代中的第 8 次
错误：0.222500793405

10 次迭代中的第 9 次10
错误：0.222500796602

[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 1.31432294e-05]

tp：28.0
fp：119.0
tn： 5537.0
fn：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-measure：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    <item>
      <title>从自然语言文本中提取数据[关闭]</title>
      <link>https://stackoverflow.com/questions/11962955/extracting-data-from-natural-language-text</link>
      <description><![CDATA[我有一组文本报纸广告，我想提取诸如所售商品及其价格等信息。这些广告不遵循任何结构化格式。我可以访问数千条此类广告。
我应该从哪里开始这个项目？有没有可以提供帮助的图书馆？]]></description>
      <guid>https://stackoverflow.com/questions/11962955/extracting-data-from-natural-language-text</guid>
      <pubDate>Wed, 15 Aug 2012 01:21:53 GMT</pubDate>
    </item>
    </channel>
</rss>