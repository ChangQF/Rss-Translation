<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 22 May 2024 21:16:05 GMT</lastBuildDate>
    <item>
      <title>matplotlib 无法在 macOS Sonoma VS code 上正确安装</title>
      <link>https://stackoverflow.com/questions/78519268/matplotlib-not-installing-correctly-on-macos-sonoma-vs-code</link>
      <description><![CDATA[在此处输入图片描述
我安装的所有其他模块导入正常，但 matplotlib 返回警告。我用了
pip3 install matplotlib 以便安装它，感谢您的帮助！
这是错误导入“matplotlib”无法从源代码解决]]></description>
      <guid>https://stackoverflow.com/questions/78519268/matplotlib-not-installing-correctly-on-macos-sonoma-vs-code</guid>
      <pubDate>Wed, 22 May 2024 17:58:55 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习如何在 React 和 JS 中创建搜索+机器学习推荐算法？</title>
      <link>https://stackoverflow.com/questions/78519224/how-can-i-learn-how-to-create-a-searching-machine-learning-recommender-algorit</link>
      <description><![CDATA[我目前正在为 A 级计算机科学开始我的 NEA 项目，并且基本上想在 React 中构建一个 supercook 的克隆，用户登录后，输入他们拥有的食材，应用程序将显示从网上找到的食谱。然后我想实现一个机器学习方面，用户可以根据他们对食谱的喜爱程度对食谱进行评分，算法将调整推荐给用户的食谱。这个项目要到四月才能完成（我仍然会一边学习其他科目一边做这个项目，所以不是无限的空闲时间）。
我该如何学习使用 React + JavaScript，我该如何学习如何创建显示食谱的算法以及机器学习方面？我对这个项目有所有的想法，也希望它能发挥作用，我只是希望有人能指导我去哪里学习。那么，您是否认为在 brain.js 或 tensorflow.js 之类的东西中从头开始进行所有机器学习是可行的？或者尝试将预先存在的推荐算法应用于我的应用程序是否更好？如果是这样，我该怎么做？请记住，我对 javascript + react 的知识非常有限，所以我从头开始学习这些知识，而且几乎没有机器学习知识。
我已经搜索了很多关于这些主题的内容，但由于我对这些类型的算法、推荐系统和机器学习的了解有限，我并不完全清楚自己在做什么，因此，如果能给出一个清晰的解释和一些关于去哪里和做什么的指导，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78519224/how-can-i-learn-how-to-create-a-searching-machine-learning-recommender-algorit</guid>
      <pubDate>Wed, 22 May 2024 17:47:47 GMT</pubDate>
    </item>
    <item>
      <title>建议用于 Stable_Diffuson webui 的服务器 GPU [关闭]</title>
      <link>https://stackoverflow.com/questions/78518811/suggest-server-gpu-for-stable-diffuson-webui</link>
      <description><![CDATA[我需要使用 controlnet 部署稳定的扩散 webui API，建议一些好的服务器 GPU 以便轻松部署，与 img2img 和 controlnet 配合良好，我需要最大速度 20 秒，在线服务器 GPUsSources
我在本地 GPU 上尝试过，但速度很低]]></description>
      <guid>https://stackoverflow.com/questions/78518811/suggest-server-gpu-for-stable-diffuson-webui</guid>
      <pubDate>Wed, 22 May 2024 16:12:37 GMT</pubDate>
    </item>
    <item>
      <title>在 Pytorch 中正确实现 F-Beta 分数作为损失</title>
      <link>https://stackoverflow.com/questions/78518778/correct-implementation-of-f-beta-score-as-a-loss-in-pytorch</link>
      <description><![CDATA[我正在尝试将 F-Beta 分数视为损失。据我所知，我不能对概率使用 &gt;= 或 argmax() 函数，因为它是不可微分的。如果我错了，请纠正我。
我能想到的最接近的一个是这个 Kaggle笔记本，但它适用于多标签和 Keras。我正在 PyTorch 中尝试进行二进制分类。
这是迄今为止我的代码：
导入火炬
导入 torch.nn.function 作为 F

ApproxFBetaLoss 类（torch.nn.Module）：
   
    def __init__(self, beta:float = 1, class_idx:int = 0, eps:float = 1e-12) -&gt;没有任何：
        ”“”
        想法来自：
        1. https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric
        2. https://arxiv.org/pdf/2104.01459（替代F Beta损失）
        
        参数：
            beta：Beta 控制损失。 0＜贝塔&lt; 1 有利于精度且 beta &gt; 1 有利于召回，beta == 1 是 F-1 分数
            class_idx：哪个类是好还是坏。默认情况下，我们称其为 0 类概率
        ”“”
        超级().__init__()
        自我.beta = beta
        self.class_idx = class_idx
        self.eps = eps # 除以零的问题


    defforward(self, logits: torch.Tensor, labels: torch.Tensor):
        ”“”
        logits：[批量，二维] Logits
        标签：[批次] 
        ”“”
        probabilities = F.softmax(logits, dim=1) # 我们也可以使用 1 类的 Sigmoid 来实现相同的效果

        # 预测 = torch.argmax(概率, dim=1).float() # argmax 可微吗？我不这么认为，因为它会给出 True/False 和中断
        预测 = 概率[:,self.class_idx] # 0 类概率 IS_BAD = CLASS_0

        # 我认为这个计算可能存在一些问题。这在多大程度上是正确的？
        true_positives = (预测 * labels).sum().to(device = logits.device, dtype = logits.dtype)
        false_positives = ((1 - 标签) * 预测).sum().to(device = logits.device, dtype = logits.dtype)
        false_negatives= (标签 * (1 - 预测)).sum().to(device = logits.device, dtype = logits.dtype)

        精度 = 真阳性 / (真阳性 + 假阳性 + self.eps)
        召回率=真阳性/（真阳性+假阴性+ self.eps）

        f_beta_score = (1 + self.beta**2) * (精度 * 召回率) / (self.beta**2 * 精度 + 召回率 + self.eps)

        return 1 - f_beta_score # 损失为 1 - F_BETA_SCORE
    


logits = torch.tensor([[2, 10], [7, 3], [1, 4], [6, 5], [10, 3], [3, 10], [10, 3], [ 3, 10]]).to(设备 = &quot;cuda:0&quot;, dtype = torch.bfloat16)
标签 = torch.tensor([0, 1, 1, 0, 1, 1, 0, 1]).to(device = &quot;cuda:0&quot;, dtype = torch.bfloat16)

ApproxFbetaLoss（beta = 1，class_idx = 1）（logits，标签）

这给我带来了 0.3594 的损失，并查看 sklearn 的 F-1 分数，Class-1 的 F-1 分数为  &gt;0.6666 这使得它的损失为 1-0.66 = 0.3333
看起来我可能很接近，但有人可以确认或更正吗？]]></description>
      <guid>https://stackoverflow.com/questions/78518778/correct-implementation-of-f-beta-score-as-a-loss-in-pytorch</guid>
      <pubDate>Wed, 22 May 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>如何确定FastText模型在文本分类中的准确性？</title>
      <link>https://stackoverflow.com/questions/78518695/how-to-find-accuracy-of-fasttext-model-in-text-classification</link>
      <description><![CDATA[在机器学习中，所有模型都有准确度方程，而在 FastText 模型中，我们没有请支持。]]></description>
      <guid>https://stackoverflow.com/questions/78518695/how-to-find-accuracy-of-fasttext-model-in-text-classification</guid>
      <pubDate>Wed, 22 May 2024 15:50:57 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 的表现比基线更差</title>
      <link>https://stackoverflow.com/questions/78518529/gridsearchcv-performs-worse-than-baseline</link>
      <description><![CDATA[我正在使用 scikit-learn 解决二元分类问题。我测试过的模型之一是 KNeighborsClassifier，我使用默认参数获得了 78% 的基线分数 (“recall_macro”)。此外
然后我运行一个 GridSearchCV 交叉验证器（使用包含默认值的参数网格，仅使用训练集，并针对我需要的评分指标），但我得到“优化的”得分71%。不仅如此，少数族裔的召回率也从 59% 下降到 43%。
据我了解，GridSearchCV 运行的数据与模型所拟合的数据并不完全相同，因此可能存在轻微差异，但这似乎过多。我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78518529/gridsearchcv-performs-worse-than-baseline</guid>
      <pubDate>Wed, 22 May 2024 15:16:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么 nni 运行多个 python 文件导致我的电脑崩溃？</title>
      <link>https://stackoverflow.com/questions/78518205/why-does-nni-run-multiple-python-files-resulting-in-crashing-of-my-pc</link>
      <description><![CDATA[我尝试过以下方法：
实验名称：超参数搜索
作者姓名：Raj
试用并发数：1
培训服务平台：本地
searchSpacePath：searching_space.json
多线程：真
use注解: false
调音器：
    内置调谐器名称：随机

审判：
    命令：python train.py
    代码目录: .
    GPU数量：1

本地配置：
    使用ActiveGpu：真
    最大TrialNumPerGpu：2
    GPU索引：0

上面的结果导致如此多的Python文件在后台运行并导致我的笔记本电脑崩溃。另外，当我尝试使用 useActiveGpu: false 时，问题仍然存在。即使我以某种方式防止我的笔记本电脑崩溃，链接也无法打开。
但是当我使用 CPU 时，代码如下：
作者姓名：默认
实验名称：超参数搜索
试用并发数：1
培训服务平台：本地
use注解: false
searchSpacePath：searching_space.json
调音器：
  内置调谐器名称：随机
  类参数：
    优化模式：最小化
审判：
  命令：python train.py
  代码目录: .
  GPU数量: 0

nni 链接有效，所有任务都按预期运行并成功。
我使用的是 Pycharm IDE 的虚拟环境，nni 是 3.0 版本，我的操作系统是 Windows 10。我将所有变量设置为 .to(device)，其中我的设备是 cuda。代码不在nni下运行时运行正常，但只有在nni中使用GPU时才会出现问题。
我尝试尝试 TrialConcurrency、useActiveGpu、maxTrialNumPerGpu、gpuIndices。但所有结果都是一样的。笔记本电脑崩溃或链接无法打开，即使打开，“正在运行”、“失败”或“成功”中也不会显示任何内容，并且服务器之后不会立即工作。]]></description>
      <guid>https://stackoverflow.com/questions/78518205/why-does-nni-run-multiple-python-files-resulting-in-crashing-of-my-pc</guid>
      <pubDate>Wed, 22 May 2024 14:20:29 GMT</pubDate>
    </item>
    <item>
      <title>广义 Jensen-Shannon 散度 - 多重分布 - 不等长向量 - R</title>
      <link>https://stackoverflow.com/questions/78518168/generalised-jensen-shannon-divergence-multiple-distributions-vectors-of-uneq</link>
      <description><![CDATA[我想计算 R 中长度不等的三个分布（dist1、dist2、dist3）之间的广义 Jensen-Shannon 散度 (GSJD)。
我想知道是否有人可以帮助我：
(1) 将我的原始数据转换为适合 GSJD 分析的概率矩阵，
(2) 以及之后如何在R中运行GSJD。
到目前为止，我一直在使用 Philentropy 库，但没有成功。
我的发行版如下所示。所有帮助将不胜感激
install.packages(“philentropy”)
图书馆（慈善事业）

#具有三个分布的数据

dist1 &lt;- 样本（seq（从 = 0，到 = 1，by = 0.005），大小 = 100，替换 = TRUE）
dist2 &lt;- 样本（seq（从 = 0，到 = 1，by = 0.005），大小 = 150，替换 = TRUE）
dist3 &lt;- 样本（seq（从 = 0，到 = 1，by = 0.005），大小 = 250，替换 = TRUE）
]]></description>
      <guid>https://stackoverflow.com/questions/78518168/generalised-jensen-shannon-divergence-multiple-distributions-vectors-of-uneq</guid>
      <pubDate>Wed, 22 May 2024 14:14:41 GMT</pubDate>
    </item>
    <item>
      <title>如何解决功能未定义错误？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78517764/how-do-i-resolve-features-not-defined-error</link>
      <description><![CDATA[传入 features 参数的函数
这是纽约市出租车乘坐问题的预测分析。
我一直在通过 Great Learning 学习数据科学/机器学习课程。我遇到了一个函数问题，我之前已经定义了参数，但它返回一个错误，指出未定义。非常感谢任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78517764/how-do-i-resolve-features-not-defined-error</guid>
      <pubDate>Wed, 22 May 2024 13:07:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 SHAP 解释学习到的潜在空间位置</title>
      <link>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</link>
      <description><![CDATA[我在 MNIST 数据集上的 pytorch 中实现了一个监督自动编码器。
我在潜在空间（大小 8）上使用分类层对其进行监督。在训练期间，我优化了 MSE 重建损失和分类损失 (BCE)。我在潜在空间中有单个实例，这些实例很有趣，我想找到它们不同位置的解释。
所以我的问题是，在潜在维度上使用 SHAP 值是否是一种有效的方法（它有效，我得到了值，但我不确定这是否有意义）。
更具体地说：我想比较例如实例 A 和实例 B。假设在潜在空间中它们相距很远，例如在潜在维度 3 of 8 中。现在我想找到输入中可以解释这种现象的像素。因此，我计算实例 A 和 B 的潜在表示的 SHAP 值，并比较两者的维度 3 的 SHAP 值。这是有效的吗？我认为它与解释多输出回归没有太大不同，对吧？但我还没有看到任何 SHAP 的应用来解释潜在位置
非常感谢您的任何评论！]]></description>
      <guid>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</guid>
      <pubDate>Wed, 22 May 2024 12:17:33 GMT</pubDate>
    </item>
    <item>
      <title>限制中途使用上传的图像来创建新图像</title>
      <link>https://stackoverflow.com/questions/78517370/restrict-mid-journey-to-use-the-uploaded-image-for-creating-new-image</link>
      <description><![CDATA[在中途有没有一种方法可以限制它使用我们自己上传的图像来创建新图像？例如，我有一个帽子的图像，并希望在旅途中创建一个戴着相同帽子的男孩的新图像。
如果不在旅途中，是否有任何图像生成工具可以执行相同的操作。]]></description>
      <guid>https://stackoverflow.com/questions/78517370/restrict-mid-journey-to-use-the-uploaded-image-for-creating-new-image</guid>
      <pubDate>Wed, 22 May 2024 11:55:32 GMT</pubDate>
    </item>
    <item>
      <title>ML AI 模型检测 SQL 数据库中的异常</title>
      <link>https://stackoverflow.com/questions/78516610/ml-ai-model-to-detect-anomalies-in-sql-database</link>
      <description><![CDATA[构建一个机器学习模型来检测 SQL 数据库中的异常，我应该采取什么方法
我使用了隔离森林模型，我希望当我将模型连接到数据库时，在命名表以检查异常后，模型返回所有​​可能的异常，并列出所有异常]]></description>
      <guid>https://stackoverflow.com/questions/78516610/ml-ai-model-to-detect-anomalies-in-sql-database</guid>
      <pubDate>Wed, 22 May 2024 09:42:44 GMT</pubDate>
    </item>
    <item>
      <title>阻止进程时内核崩溃</title>
      <link>https://stackoverflow.com/questions/78516548/kernel-crashed-while-stemming-process</link>
      <description><![CDATA[我使用这个函数来进行句子词干提取
从 nltk.stem 导入 WordNetLemmatizer、PorterStemmer
从 nltk.tokenize 导入 word_tokenize
导入字符串
从 nltk.corpus 导入停用词

标点符号 = set(字符串.标点符号)
english_stopwords = set(stopwords.words(&#39;english&#39;))
porter_stemmer = PorterStemmer()
def clean_text(文本):
    文本 = 文本.lower()
    标记 = word_tokenize(文本)
    clean_tokens = []
    clean_tokens = [如果令牌不在 english_stopwords 中则为令牌中的令牌的令牌]
    clean_tokens = [如果标记不在标点符号中，则标记为标记中的标记]
    clean_tokens = [token 中的 token if token.isalnum()]
    clean_tokens = [porter_stemmer.stem(token) for token in clean_tokens if len(token) &gt;; 0]

    返回 &#39;​​ &#39;.join(cleaned_tokens)


我只是在我的 csv 上运行它
导入 pandas 作为 pd
currData = pd.read_csv(f&#39;../Steam dataset/clean_steam_database(english)_133.csv&#39;)
currData[&#39;review&#39;] = [clean_text(word) for word in currData[&#39;review&#39;]]

但它说：
“在当前单元或前一个单元中执行代码时内核崩溃。
请检查单元格中的代码以确定失败的可能原因。
点击这里查看更多信息。
查看 Jupyter 日志以获取更多详细信息。”
和
给出这个错误：
＆quot;16：23：09.679 [错误]将会话处置为内核进程死亡 ExitCode：3221225725，原因：
16:23:09.706 [info] Cell 2 在 -1716369788.31 秒内完成（开始：1716369788310，结束：未定义）”
它可能是什么？
我实际上是从以下位置获取这个数据集的：
https://www.kaggle.com/datasets/najzeko/ steam-reviews-2021?resource=download
我在已分成 1000 个部分的数据集上运行此代码。
每次执行该函数时，我都会打印以查看哪个索引有问题，但是，当我直接在该索引处运行该函数时，它没有问题。
当我在 google colab 上运行这段代码时，它说
“RecursionError：比较中超出了最大递归深度”
Stem命令有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78516548/kernel-crashed-while-stemming-process</guid>
      <pubDate>Wed, 22 May 2024 09:32:06 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“utils.feature_extractor”的模块[关闭]</title>
      <link>https://stackoverflow.com/questions/78515681/modulenotfounderror-no-module-named-utils-feature-extractor</link>
      <description><![CDATA[
嗨，为什么我有这个错误，我的文件上已经有这个函数，但它仍然错误？
我正在创建一个网络钓鱼链接检测，需要在执行结果之前扫描所需的功能。
我已经添加了该功能，我检查了拼写，但什么也没发生。]]></description>
      <guid>https://stackoverflow.com/questions/78515681/modulenotfounderror-no-module-named-utils-feature-extractor</guid>
      <pubDate>Wed, 22 May 2024 06:36:46 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 返回的精度比默认值差</title>
      <link>https://stackoverflow.com/questions/67666417/gridsearchcv-returns-worse-accuracy-than-default</link>
      <description><![CDATA[我正在使用 Kaggle 的心脏病预测数据集，并发现了一些奇怪的东西，但我找不到答案。
使用带有“liblinear”求解器的默认 Logistic 回归 (C = 1)，我在训练集上获得的准确度为 87.26%，在测试集上的准确度为 86.81%。不错。然而，我尝试使用 GridSearchCV 调整 C，以防我找到更好的值，但我不断得到更差的结果（训练集上的准确度约为 85%，测试集上的准确度约为 82.5%）。
GridSearchCV 是否使用其他指标来比较这些 C 值？我只是不明白为什么它会返回一个更糟糕的解决方案。
我将代码的最后一部分留在这里。
默认 Logistic 回归
从 sklearn.linear_model 导入 LogisticRegression
    
lr = LogisticRegression(求解器 = &#39;lib线性&#39;, random_state = 2)
lr = lr.fit(X_train, y_train)

model_score(lr, X_train, y_train, X_test, y_test)

GridSearchCV
从 sklearn.model_selection 导入 GridSearchCV
# 为逻辑回归寻找更好的超参数
lr_params = [ {&#39;C&#39;: np.logspace(-1, 0.3, 30)} ]

lr = LogisticRegression(求解器 = &#39;lib线性&#39;, random_state = 2)
lr_cv = GridSearchCV(lr, lr_params, cv = 5, 评分 = &#39;准确度&#39;)
lr_cv.fit(X_train, y_train)

lr_best_params = lr_cv.best_params_
lr = 逻辑回归(**lr_best_params)
lr.fit(X_train, y_train)

model_score(lr, X_train, y_train, X_test, y_test)

编辑
此链接中的完整代码 （查看第 4-4.1 节）。]]></description>
      <guid>https://stackoverflow.com/questions/67666417/gridsearchcv-returns-worse-accuracy-than-default</guid>
      <pubDate>Mon, 24 May 2021 03:42:19 GMT</pubDate>
    </item>
    </channel>
</rss>