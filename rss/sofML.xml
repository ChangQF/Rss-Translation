<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 30 Jan 2024 00:57:34 GMT</lastBuildDate>
    <item>
      <title>LSTM 模型仅输出 1 个单值</title>
      <link>https://stackoverflow.com/questions/77903104/lstm-models-only-output-1-single-value</link>
      <description><![CDATA[我正在使用 LSTM 来研究时间序列，并尝试了一些变体，但我没有运气让模型为我提供与单个值不同的输出值。
这是我之前的数据：
临时订单检查：
总行数 = 10410 &amp;列车索引 = 7287 &amp;价值指数 = 8848
时间顺序正确：训练 &lt;验证&lt;测试
前向验证的数据分割完成：
训练集形状：(7287, 57) &amp;验证集形状：(1561, 57) &amp;测试集形状：(1562, 57)

重塑的数据形状对于训练、测试和验证集来说是正确的。
火车改造：(7280, 7, 38)
测试重塑：(1555,7,38)
瓦尔重塑：(1554, 7, 38)

修剪形状：
X_train 修剪：(7280, 7, 38)，y_train 修剪：(7280,)
X_测试修剪：（1554,7,38），y_测试修剪：（1554,）
X_val 修剪：(1554, 7, 38)，y_val 修剪：(1554,)

模型类型：lstm
模型类型的 DataFrame 运行状况检查：lstm

最后一次检查之前训练的变量： X_train: (7280, 7, 38) y_train: (7280,) X_test: (1555, 7, 38) y_test: (1554,) X_val: (1554, 7, 38）y_val：（1554，）

最终形状：X_train：（7280,7,38），y_train：（7280,），X_test：（1554,7,38），y_test：（1554,），X_val：（1554,7,38），y_val：（第1554章
数据已准备好用于模型训练。

我的模型结构如下：
def create_model(params):

    #-------------------------GPU设置-------------------- -----------------
    # 计算全局batch size并相应调整学习率
    #------------------------------------------------- ------------------------
    策略 = tf.distribute.MirroredStrategy()
    num_gpus = Strategy.num_replicas_in_sync # 可用 GPU 数量
    # global_batch_size = params[&#39;batch_size&#39;] * num_gpus
    # params[&#39;learning_rate&#39;] *= num_gpus # 缩放学习率
    
    input_shape = (params[&#39;timesteps&#39;], params[&#39;n_features&#39;])
    print(&#39;设备数量：{}&#39;.format(strategy.num_replicas_in_sync))
    
    使用strategy.scope()：
    #-------------------------GPU设置-------------------- -----------------
    # 计算全局batch size并相应调整学习率
    #------------------------------------------------- ------------------------
        
        # 正则化参数
        l1_reg = 0.0001 # L1正则化因子 0.005 0.001 0.01 0.02
        l2_reg = 0.0001 # L2正则化因子

        模型=顺序（）

        # LSTM层
        model.add(LSTM(64, input_shape=input_shape, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        模型.add(Dropout(0.2))

        # 用于特征学习的密集层
        model.add(密集(32, 激活=&#39;relu&#39;, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        模型.add(Dropout(0.2))
        
        model.add(密集(16, 激活=&#39;relu&#39;, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        # 最终输出层
        model.add（密集（1，激活=&#39;线性&#39;））`

训练后并尝试在这里预测我的测试：
警告：测试预测是恒定的。模型可能无法有效学习。
警告：验证预测是恒定的。模型可能无法有效学习。
-------------------------------------------------- -
样本测试预测：[0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942]
样本验证预测：[0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942]

在输出中具有单个密集值，我希望预测下一个值，但它始终给出一个值。]]></description>
      <guid>https://stackoverflow.com/questions/77903104/lstm-models-only-output-1-single-value</guid>
      <pubDate>Mon, 29 Jan 2024 23:06:11 GMT</pubDate>
    </item>
    <item>
      <title>从学习曲线解读，哪种模型表现更好？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77902992/interpretation-from-learning-curve-which-model-perform-better</link>
      <description><![CDATA[对于具有不同学习率的 XGBoost n_estimator，学习曲线可能看起来非常不同。对于所附的两张图片，哪个模型可能表现更好？
]]></description>
      <guid>https://stackoverflow.com/questions/77902992/interpretation-from-learning-curve-which-model-perform-better</guid>
      <pubDate>Mon, 29 Jan 2024 22:31:54 GMT</pubDate>
    </item>
    <item>
      <title>屏蔽每个神经元的输入，可能吗？</title>
      <link>https://stackoverflow.com/questions/77902803/masking-input-for-each-neuron-is-it-possible</link>
      <description><![CDATA[假设我有一个输入数据框，行是数据数量，列是 5 个特征：ft1、ft2、.....、ft5。
然后我有一个简单的二元分类神经网络，其隐藏层有 2 个神经元。像这样的东西：

我可以控制输入每个神经元的输入吗？就像只有 ft1, 3, 5 到神经元 1，只有 ft2, 4 到神经元 2？
我首先想到的是遮蔽，但似乎不太管用，也许我可以调整权重？有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77902803/masking-input-for-each-neuron-is-it-possible</guid>
      <pubDate>Mon, 29 Jan 2024 21:49:05 GMT</pubDate>
    </item>
    <item>
      <title>“平均距离”在无监督机器学习的数据预处理中有何重要性？</title>
      <link>https://stackoverflow.com/questions/77902657/what-is-the-importance-of-average-of-distances-in-data-pre-processing-for-unsu</link>
      <description><![CDATA[我是计算机科学专业的学士学位学生，一直在从事一个关于葡萄牙语文档自动摘要的学术项目。
该项目是用 Python 开发的，是我的第一个关于 ML 及其技术的项目。我已经完成了所有自然语言处理并开始了所谓的“矢量化”。
我使用了 word2vec，现在我有一个包含单词级向量的列表列表（每个子列表代表一个句子，子列表中的每个元素代表句子中向量表示中的一个单词）。我有这样的东西：
text_vectors = [
    [[1, 2, 3], [4, 5, 6]], # 句子1：单词的向量表示
    [[2, 3, 4], [5, 6, 7]], # 句子2：单词的向量表示
    # 根据需要添加更多句子
]

所以我开始计算平均距离，但我看不出所有这些预处理是如何相互关联的。
这是我正在遵循的工作流程
obs：相比 TF-IDF Average，我更喜欢 word2vec]]></description>
      <guid>https://stackoverflow.com/questions/77902657/what-is-the-importance-of-average-of-distances-in-data-pre-processing-for-unsu</guid>
      <pubDate>Mon, 29 Jan 2024 21:12:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么带有 Logistic 回归的递归特征消除 (RFE) 会在二元分类中选择意外的特征？</title>
      <link>https://stackoverflow.com/questions/77902343/why-is-recursive-feature-elimination-rfe-with-logistic-regression-selecting-un</link>
      <description><![CDATA[我正在处理一个包含 94 个样本的数据集，每个样本属于 5 个组中的一组，并且可以通过 610 个特征进行描述。对于这五个组中的每一组，我想确定能够最好地表征该组的最小特征数。为此，我为每个组设置了一个二元分类问题，运行逻辑回归，然后使用调整后的逻辑回归模型运行 RFE，以获得提供最大平衡精度的最小数量的特征。然而，当我检查 RFE 认为哪些功能对于特定组而言是重要的时，我发现 RFE 正在选择所有组共有的或感兴趣组中不存在的功能作为对该组来说重要的功能。我本以为 RFE 会挑选出该组感兴趣的最独特的功能，虽然每个组都有一些独特的功能，但 RFE 显然不会将它们选择为前 10-30 个功能。对选定数量的特征运行 RFE 时，准确率仍然很高 (97.8%)。我是否误解了 RFE 的要点，或者我的代码有问题？我在运行之前记录了数据转换。我使用此处提供的示例代码中的 RFE： https://machinelearningmastery.com/ rfe-feature-selection-in-python/
我尝试了提供的示例代码，但 RFE 提供了奇怪的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77902343/why-is-recursive-feature-elimination-rfe-with-logistic-regression-selecting-un</guid>
      <pubDate>Mon, 29 Jan 2024 20:05:10 GMT</pubDate>
    </item>
    <item>
      <title>在 librosa 中重新采样音频时遇到问题</title>
      <link>https://stackoverflow.com/questions/77901612/facing-problem-in-resampling-audio-in-librosa</link>
      <description><![CDATA[我正在尝试使用我的数据集微调 wav2vec2 模型。因此我加载了音频。现在想要将它们下采样到 16kHz。但是 librosa.reshape 函数给出了一个我无法解决的错误。错误信息是：
&lt;块引用&gt;
resample() 需要 1 个位置参数，但给出了 3 个

首先，我尝试使用采样率为 16kHz 的 librosa 加载它。但由于我在这方面的经验较少，因此我在项目的后期遇到了问题。我找到了一个应该重新采样音频信号的代码。我尝试使用它，但遇到了上述问题。
这部分工作正常：
数据库={}
音频 = []
PSR = []
对于 df[&#39;audio&#39;] 中的路径：
  Speech_array,sr = torchaudio.load(路径)
  audios.append(speech_array[0].numpy())
  psr.追加(sr)
数据库[&#39;音频&#39;] = 音频
数据库[&#39;psr&#39;] = psr

每个索引都会出现错误：
导入 librosa
将 numpy 导入为 np

# 假设“database”是包含“audio”和“psr”列的 DataFrame

# 存储新采样率的列表
新_sr = []

# 对每个音频信号重新采样并存储新的采样率
对于范围内的 i(len(database[&#39;psr&#39;]))：
    尝试：
        audio_signal = np.asarray(database[&#39;audio&#39;][i]) # 将音频转换为 numpy 数组
        Original_sr = database[&#39;psr&#39;][i] # 原始采样率

        # 检查音频信号是否为单声道（单声道）
        如果audio_signal.ndim == 1：
            # 对单声道音频信号重新采样
            resampled_audio = librosa.resample(audio_signal,original_sr,16000)
        别的：
            # 对多通道音频的每个通道分别重新采样
            重新采样通道 = []
            对于 audio_signal 中的通道：
                resampled_channel = librosa.resample（通道，original_sr，16000）
                resampled_channels.append(resampled_channel)
            resampled_audio = np.array(resampled_channels)

        # 将重新采样的音频存储回 DataFrame 中
        数据库[&#39;音频&#39;][i] = resampled_audio

        # 存储新的采样率（16000 Hz）
        new_sr.append(16000)
    除了异常 e：
        print(f“处理索引 {i} 处的音频时出错：{e}”)

# 向 DataFrame 添加新的采样率
数据库[&#39;newsr&#39;] = new_sr
]]></description>
      <guid>https://stackoverflow.com/questions/77901612/facing-problem-in-resampling-audio-in-librosa</guid>
      <pubDate>Mon, 29 Jan 2024 17:37:05 GMT</pubDate>
    </item>
    <item>
      <title>无法在 celery 工作线程中使用 YOLO 加载和预测图像</title>
      <link>https://stackoverflow.com/questions/77901605/not-able-to-load-and-predict-on-images-using-yolo-inside-celery-worker</link>
      <description><![CDATA[这是我的代码
导入系统
从 Question_Detection.inference_object_detection.check_model 导入 verify_model
从记录器导入记录器
从 ultralytics 导入 YOLO


类 ModelNotFoundError（异常）：
    经过


def load_model():
    尝试：
        成功，model_path = verify_model()
        logger.info(“模型加载开始”)
        如果没有成功：
            raise ModelNotFoundError(&quot;未找到模型！&quot;)
        logger.info(“123”)
        记录器.info（模型路径）
        _模型 = YOLO(模型路径)
        logger.info(“456”)
        如果 _model 为 None：
            raise ModelNotFoundError(“模型未加载！”)
        logger.info(“789”)
        返回 _model, _model.names
    除了 ModelNotFoundError 为 e：
        记录器.错误(e)
        sys.exit(1) # 以非零退出代码退出系统以指示错误
    除了异常 e：
        记录器.错误(e)
        返回无，无



def 推理（图像）：
    ”“”
    图像推理
    :param images: 可以是单个图像、图像列表或目录
    :return: 结果列表
    ”“”
    logger.info(“开始图像推理！”)
    模型，类别 = load_model()
    如果模型为无：
        raise ModelNotFoundError(“模型未加载！”)

    res = 模型（图像），类别
    logger.info(&quot;图像推理完成！&quot;)
    返回资源

如果我正常调用推理，它就像黄油一样工作
但是，当同一个函数被另一个 celery 工作函数调用时，这不起作用。
它甚至不会抛出任何错误。
调试时，model_path之后没有记录任何内容，基本上没有到达456行。
为什么我无法加载模型？]]></description>
      <guid>https://stackoverflow.com/questions/77901605/not-able-to-load-and-predict-on-images-using-yolo-inside-celery-worker</guid>
      <pubDate>Mon, 29 Jan 2024 17:35:56 GMT</pubDate>
    </item>
    <item>
      <title>部署在 Streamlit Coumminity Cloud 上的机器学习模型给出了错误的预测</title>
      <link>https://stackoverflow.com/questions/77899985/machine-learning-model-after-deployed-on-streamlit-coumminity-cloud-is-giving-wr</link>
      <description><![CDATA[我创建了一个垃圾邮件预测机器学习模型，为了创建一个界面，我去了 pycharm 并编写了 app.py 代码，还有另外两个文件 &lt; app.py 文件中使用了 code&gt;vectorizer.pkl 和 model.pkl，在终端上我编写了 streamlit run app.py，它把我带到了本地主机 8501 浏览器，现在一切都很顺利，做出了正确的预测。我想公开这个应用程序以在 GitHub 上分享链接。我在 streamlit 社区云上创建了一个帐户，将其与我的 GitHub 链接起来，并部署了该应用程序。 GitHub 存储库包含所有必要的文件，包括带有整个 ml 模型、app.py、model.pkl,requirements.txt 的笔记本， 向量化器.pkl。现在的问题是，使用公共链接部署的应用程序对于完全相同的输入给出了不同的预测（即说“不是垃圾邮件”而不是“垃圾邮件”）。我将输出附在此处，请告诉我该怎么做。
单击这些链接可查看附加的图像：


app.py 文件的 Github 链接：https:// github.com/ardra1111/Spam-Gaurd/blob/main/app.py
Streamlit 云应用程序的链接：https://spam-gaurd-ir5t9kbcsjpdkus4cveywj.streamlit.app/ 
当我第一次在 streamlit.io 上部署该应用程序时，它运行良好。几周前，但由于不活动，链接不起作用，所以我删除了该应用程序，并重新开始将其部署为新应用程序，我不明白下一步应该做什么来解决此问题？
我第一次尝试使用渲染将其部署在那里，并且遇到了同样的问题，请帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77899985/machine-learning-model-after-deployed-on-streamlit-coumminity-cloud-is-giving-wr</guid>
      <pubDate>Mon, 29 Jan 2024 13:14:02 GMT</pubDate>
    </item>
    <item>
      <title>通过 Docker 镜像使用时将 Faiss 索引写入文件的问题</title>
      <link>https://stackoverflow.com/questions/77899677/issue-on-writing-faiss-index-to-a-file-when-using-through-docker-image</link>
      <description><![CDATA[我有一个 FastAPI Docker 映像，在启动部分，我从 Redis 获取 FAISS 索引的二进制版本，使用 pickle.loads 对其进行解封，然后使用
file_path = os.path.join(folder_path, &#39;index.faiss&#39;)
faiss.write_index(faissModelFromRedis,文件路径)

将其写入文件。这在本地有效，但是当使用 Docker 部署到 Azure Web App 时，它会抛出
文件“/app/main.py”，第 38 行，在 loadModelAndSetRetriever 中
2024-01-27T14:03:27.547507225Z faiss.write_index(faiss_model,file_path)
2024-01-27T14:03:27.547510425Z 文件“/usr/local/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py”，第 10200 行，在 write_index 中
2024-01-27T14:03:27.547513825Z 返回 _swigfaiss_avx2.write_index(*args)
2024-01-27T14：03：27.547517125Z TypeError：重载函数“write_index”的参数数量或类型错误。
2024-01-27T14:03:27.547520225Z 可能的 C/C++ 原型是：
2024-01-27T14:03:27.547523325Z faiss::write_index(faiss::索引常量*，字符常量*)
2024-01-27T14:03:27.547526526Z faiss::write_index(faiss::索引 const *,FILE *)
2024-01-27T14:03:27.547529526Z faiss::write_index(faiss::Index const *,faiss::IOWriter *)

我们如何解决这个问题？
我尝试使用 pickle.dumps，但使用 Faiss，它无法读取已保存文件的索引。
还尝试在 Dockerfile 中添加 chmod 777 或新用户添加命令，认为这是 faiss 的写入访问问题。]]></description>
      <guid>https://stackoverflow.com/questions/77899677/issue-on-writing-faiss-index-to-a-file-when-using-through-docker-image</guid>
      <pubDate>Mon, 29 Jan 2024 12:19:42 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI Gym 步骤功能不起作用错误需要 5 个变量才能解压</title>
      <link>https://stackoverflow.com/questions/77898872/openai-gym-step-function-doesnt-work-error-needs-5-variables-to-unpack</link>
      <description><![CDATA[导入健身房
从 nes_py.wrappers 导入 JoypadSpace
从 Contra.actions 导入 SIMPLE_MOVMENT、COMPLEX_MOVMENT、RIGHT_ONLY
SIMPLE_MOVMENT #简化环境

环境=gym.make（&#39;Contra-v0&#39;）
envi = JoypadSpace(envi, SIMPLE_MOVMENT)

重新启动=真

对于范围（1000）内的帧：
    如果重新启动：
        状态 = envi.reset()
    action = envi.action_space.sample() # 生成随机动作
    state,reward,restart,truncated,inf=envi.step(action)#采取随机生成的动作
    envi.render() # 只显示游戏
环境.close()

运行代码后，出现以下错误：
&lt;前&gt;&lt;代码&gt;---&gt; 50 观察、奖励、终止、截断、info = self.env.step(action)
     51 self._elapsed_steps += 1
     53 如果 self._elapsed_steps &gt;= self._max_episode_steps：

ValueError：没有足够的值来解压（预期为 5，实际为 4）

我一直在努力解决这个问题，但我不知道如何解决它。我尝试使用较旧的健身房版本，但仍然不起作用]]></description>
      <guid>https://stackoverflow.com/questions/77898872/openai-gym-step-function-doesnt-work-error-needs-5-variables-to-unpack</guid>
      <pubDate>Mon, 29 Jan 2024 10:06:00 GMT</pubDate>
    </item>
    <item>
      <title>模型“利用”加权损失？</title>
      <link>https://stackoverflow.com/questions/77897842/model-exploiting-weighted-loss</link>
      <description><![CDATA[我正在创建一个进行多类分类的模型，并尝试使用加权损失来优化它。
我的数据集包含 7 个类。这是我拥有的每个数据点的数量：
梅尔：1113
内华达州：6705
密件抄送：514
AKIEC：327
吉隆坡：1099
DF：115
VASC：142
我认为我的模型正在“利用”通过仅返回“AKIEC”来减少该加权损失每次。 （不知道为什么它不是每次都返回 VASC，但 AKIEC 是我观察到的）
这是我的模型在“MEL”上给出的输出图片：
[1.9501211e-02、8.6555272e-02、7.0136093e-02、5.3331017e-02、7.6896048e-01、
1.2218184e-03、2.9415233e-04]
这是我的模型使用的加权损失权重：
{0：0.888866699950075、1：0.3305042436345482、2：0.9486769845232151、3：0.9673489765351972、4：0.8902646030953569、5：0.9885 172241637543, 6: 0.9858212680978532}
这导致分类交叉熵损失约为 0.33，二进制准确度约为 94%。
如果有必要，这是我的模型代码：
def classi(input_shape):
    输入=图层.输入（形状=输入形状）
    vgg19 = k.applications.VGG19（include_top=False，权重=“imagenet”，input_tensor=输入）
    x = vgg19（输入，训练=False）
    
    x = 层.Conv2D(64, 3, 填充=“相同”)(x)
    x = 层.Activation(“relu”)(x)
    x = 层.BatchNormalization()(x)
    #经典层
    对于 [96, 128, 256]:#, 320]:#, 512]:#, 1024, 2048] 中的过滤器：
        x = 层.Conv2D(过滤器, 3, 填充=“相同”)(x)
        x = 层.Activation(“relu”)(x)
        x = 层.BatchNormalization()(x)

        x = 层.Conv2D(过滤器, 3, 填充=“相同”)(x)
        x = 层.Activation(“relu”)(x)
        x = 层.BatchNormalization()(x)

        x = groups.MaxPool2D(3, strides=2, padding=“相同”)(x)

    ＃输出
    x = 层数.Dropout(rate=0.3)(x)
    x = 层.Flatten()(x)
    x = 层.Dense(128, 激活=“sigmoid”)(x)

    输出 = 层.Dense(7, 激活 =“softmax”)(x)

    model = k.Model(输入=输入，输出=输出，名称=“分类”)
    返回模型

我的模型接受过以下训练：
batch_size=8
steps_per_epoch = 10015 //batch_size
纪元 = 60

这里的问题是因为我的加权损失，还是我的模型只是不“好”？足够的？ （训练不够，层数太少......）或者我的指标和损失是否实施不正确？]]></description>
      <guid>https://stackoverflow.com/questions/77897842/model-exploiting-weighted-loss</guid>
      <pubDate>Mon, 29 Jan 2024 06:46:18 GMT</pubDate>
    </item>
    <item>
      <title>将 YOLOv8 集成到 Transformer 模型中</title>
      <link>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</link>
      <description><![CDATA[我需要帮助将 YOLOv8 模型 添加到下面的代码中，而不是使用 InceptionV3为我的项目提取图像特征。我需要传递检测到的对象并从 YOLOv8 模型中提取特征，以使用转换器生成标题。
def CNN_Encoder_Incep():
    inception_v3 = tf.keras.applications.InceptionV3(
        include_top=假，
        权重=&#39;imagenet&#39;
    ）
    inception_v3.trainable = False

    输出= inception_v3.output
    输出 = tf.keras.layers.Reshape(
        (-1, 输出.形状[-1]))(输出)

    cnn_model = tf.keras.models.Model(inception_v3.输入，输出)
    返回cnn_model

类 ImageCaptioningModel(tf.keras.Model):

    def __init__(self, cnn_model, 编码器, 解码器, image_aug=None):
        超级().__init__()
        self.cnn_model = cnn_model
        self.encoder = 编码器
        self.decoder = 解码器
        self.image_aug = image_aug
        self.loss_tracker = tf.keras.metrics.Mean(name=&quot;loss&quot;)
        self.acc_tracker = tf.keras.metrics.Mean(name=&quot;准确度&quot;)


    defcalculate_loss(self, y_true, y_pred, mask):
        损失 = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        损失*=掩模
        返回 tf.reduce_sum(loss) / tf.reduce_sum(mask)


    defcalculate_accuracy(self, y_true, y_pred, mask):
        精度 = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        准确度 = tf.math.logic_and(掩码, 准确度)
        准确度 = tf.cast(准确度, dtype=tf.float32)
        掩码 = tf.cast(掩码, dtype=tf.float32)
        返回 tf.reduce_sum(accuracy) / tf.reduce_sum(mask)


    defcompute_loss_and_acc(self,img_embed,captions,training=True):
        编码器输出 = self.encoder(img_embed, 训练=True)
        y_input = 标题[:, :-1]
        y_true = 标题[:, 1:]
        掩码=（y_true！= 0）
        y_pred = self.解码器（
            y_输入，编码器_输出，训练=真，掩码=掩码
        ）
        损失 = self.calculate_loss(y_true, y_pred, mask)
        acc = self.calculate_accuracy(y_true, y_pred, mask)
        回波损耗，ACC


    def train_step(自身, 批次):
        imgs、字幕 = 批处理

        如果 self.image_aug：
            imgs = self.image_aug(imgs)

        img_embed = self.cnn_model(imgs)

        使用 tf.GradientTape() 作为磁带：
            损失，acc = self.compute_loss_and_acc（
                img_embed、字幕
            ）

        训练变量 = (
            self.encoder.trainable_variables + self.decoder.trainable_variables
        ）
        grads = Tape.gradient(损失, train_vars)
        self.optimizer.apply_gradients(zip(grads, train_vars))
        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}


    def test_step（自身，批次）：
        imgs、字幕 = 批处理

        img_embed = self.cnn_model(imgs)

        损失，acc = self.compute_loss_and_acc(
            img_embed、字幕、训练=False
        ）

        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}

    @财产
    定义指标（自身）：
        返回 [self.loss_tracker, self.acc_tracker]

cnn_model = CNN_Encoder_Incep()
标题模型 = ImageCaptioningModel(
    cnn_model=cnn_model，编码器=编码器，解码器=解码器，image_aug=图像增强，
）

我尝试这样做，但当我尝试将其传递给 cnn_model 变量时，我不断收到多个错误。
def CNN_Encoder():
    yolov8_model = tf.keras.models.load_model(&#39;./content/yolov8n_objdet_oidv7_640x640.pt&#39;)
    yolov8_model.trainable = False
    输出=yolov8_model.output
    输出= tf.keras.layers.Reshape((-1,output.shape[-1]))(输出)
    cnn_model = tf.keras.models.Model(yolov8_model.输入，输出)
    cnn_model_onnx = cnn_model.export(format=&#39;onnx&#39;)
    返回cnn_model
]]></description>
      <guid>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</guid>
      <pubDate>Mon, 29 Jan 2024 05:22:01 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中使用 Elastic Net 运行回归不断得到 NAN</title>
      <link>https://stackoverflow.com/questions/77894355/running-a-regression-using-elastic-net-in-python-keep-getting-nan</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77894355/running-a-regression-using-elastic-net-in-python-keep-getting-nan</guid>
      <pubDate>Sun, 28 Jan 2024 10:03:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用更多 GPU RAM？</title>
      <link>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</link>
      <description><![CDATA[我正在 pytorch 中从事这个深度学习项目，其中我有 2 个完全连接的神经网络，我需要训练然后测试它们。但是当我在 google colab 中运行代码时，它并不比在我的 PC 上的 CPU 上运行快多少。顺便说一句，我有 colab pro。它还使用 A100 GPU 40GB GPU RAM 中的 0.6 个。
导入火炬
导入火炬视觉
导入 torchvision.transforms 作为变换
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim


设备 = torch.device(“cuda:0”)
# 定义变换
变换 = 变换.Compose([
    变换.ToTensor(),
    变换.Normalize((0.5,),(0.5,))
]）

# 加载 FashionMNIST 数据集
trainset = torchvision.datasets.FashionMNIST（&#39;./data&#39;，download=True，train=True，transform=transform）
测试集 = torchvision.datasets.FashionMNIST(&#39;./data&#39;, download=True, train=False, transform=transform)

# 创建数据加载器
trainloader = torch.utils.data.DataLoader(trainset,batch_size=1,shuffle=True,num_workers=2)
testloader = torch.utils.data.DataLoader(testset,batch_size=1,shuffle=False,num_workers=2)

# 为类定义常量
类 = (&#39;T 恤/上衣&#39;, &#39;裤子&#39;, &#39;套头衫&#39;, &#39;连衣裙&#39;, &#39;外套&#39;,
           “凉鞋”、“衬衫”、“运动鞋”、“包”、“踝靴”）




# 定义全连接神经网络
FCNN 类（nn.Module）：
    def __init__(自身, num_layers=1):
        超级（FCNN，自我）.__init__()
        self.num_layers = num_layers
        self.fc_layers = nn.ModuleList()
        如果 self.num_layers == 1:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
        elif self.num_layers == 2：
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
            self.fc_layers.append(nn.Linear(1024, 1024))
        self.output_layer = nn.Linear(1024, 10)

    def 前向（自身，x）：
        x = x.view(-1, 28 * 28)
        对于 self.fc_layers 中的层：
            x = nn.function.relu(层(x))
        x = self.output_layer(x)
        返回x

# 修改train函数以将输入和标签移动到GPU
def train(网络, 标准, 优化器, epochs=15):
    对于范围内的纪元（纪元）：
        运行损失 = 0.0
        对于 i，enumerate(trainloader, 0) 中的数据：
            输入，标签=数据[0].to（设备），数据[1].to（设备）
            优化器.zero_grad()

            输出 = 净值（输入）
            损失=标准（输出，标签）
            loss.backward()
            优化器.step()

            running_loss += loss.item()
            如果我% 2000 == 1999：
                print(&#39;[%d, %5d] 损失: %.2f&#39; %
                      (epoch + 1, i + 1, running_loss / 2000))
                运行损失 = 0.0

# 定义函数来测试准确性
定义测试（净）：
    正确 = 0
    总计 = 0
    使用 torch.no_grad()：
        对于测试加载器中的数据：
            图像、标签=数据
            输出=净（图像）
            _, 预测 = torch.max(outputs.data, 1)
            总计 += labels.size(0)
            正确+=（预测==标签）.sum().item()

    print(&#39;准确率：%d %%&#39; % (
            100 * 正确/总计))

＃ 主功能
如果 __name__ == “__main__”：
    # 定义网络
    net1 = FCNN(num_layers=1)
    net2 = FCNN(num_layers=2)
    net2.to（设备）

    # 定义损失函数和优化器
    标准 = nn.CrossEntropyLoss()
    优化器1 = optim.SGD(net1.parameters(), lr=0.001, 动量=0.0)
    optimer2 = optim.SGD(net2.parameters(), lr=0.001, 动量=0.0)

    # 使用 1 个 FC 层训练和测试网络
    #print(“训练网络1层...”)
    #train(net1, 标准, 优化器1)
    #测试（网络1）

    # 使用 2 个 FC 层训练和测试网络
    print(&quot;2层训练网络...&quot;)
    训练（net2、标准、优化器2）
    测试（网络2）

尝试在 Google Colab 中使用不同的 GPU
尝试添加此行以始终使用 CUDA 核心：
device = torch.device(“cuda:0”)

并让网络使用该设备：
device = torch.device(“cuda:0”)
]]></description>
      <guid>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</guid>
      <pubDate>Sun, 28 Jan 2024 07:11:15 GMT</pubDate>
    </item>
    <item>
      <title>langchain 中的自查询检索仅返回 4 个结果</title>
      <link>https://stackoverflow.com/questions/76603178/self-querying-retrieval-in-langchain-returning-only-4-results</link>
      <description><![CDATA[我想要文档中与特定标签示例可持续性相关的所有文章。但它只给我返回了四篇文章。 Vectorstore 中的 20 篇文章中有 7 篇与可持续发展相关。
这是我的代码：
进口松果
从 langchain.schema 导入文档
从 langchain.embeddings.openai 导入 OpenAIEmbeddings
从 langchain.vectorstores 导入 Pinecone
导入操作系统
从 langchain.llms 导入 OpenAI
从 langchain.retrievers.self_query.base 导入 SelfQueryRetriever
从 langchain.chains.query_constructor.base 导入 AttributeInfo


pinecone_api_key = “xxxxxxxx”；
pinecone_env = “xxxxxxxxxx”；
# pinecone.init(api_key=os.environ[&quot;PINECONE_API_KEY&quot;], 环境=os.environ[&quot;PINECONE_ENV&quot;])
pinecone.init(api_key=pinecone_api_key, 环境=pinecone_env)

嵌入 = OpenAIEmbeddings()

index_name=“langchain-self-retriever-ppo”
索引 = 松果.Index(index_name)
文本字段=“文本”；

# vectorstore = Pinecone.from_documents(
# docs, embeddings, index_name=“langchain-self-retriever-demo”
＃）

矢量存储 = 松果(
    索引、embeddings.embed_query、text_field
）

元数据字段信息=[
    属性信息(
        name=“标题”,
        description=&quot;新闻文章的标题&quot;,
        type=&quot;字符串或列表[字符串]&quot;,
    ),
    属性信息(
        名称=“日期”，
        description=&quot;新闻文章发布日期&quot;,
        类型=“整数”，
    ),
    属性信息(
        name=“出版物”，
        description=&quot;发表这篇新闻文章的出版物的名称&quot;,
        类型=“字符串”，
    ),
    属性信息(
        名称=“域”，
        description=&quot;新闻文章的领域&quot;,
        类型=“浮动”
    ),
]
document_content_description = “新闻文章的简要摘要”
llm = OpenAI(温度=0)
# 检索器 = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description,metadata_field_info, verbose=True)

检索器 = SelfQueryRetriever.from_llm(
    嗯，
    矢量商店，
    文档内容描述，
    元数据字段信息，
    启用_限制=真，
    详细=真
）

# 本例仅指定相关查询
retrieved_docs =retrieve.get_relevant_documents(“与可持续发展相关的文章”)
打印（检索文档）
打印（len（检索文档））

我已经进入了 get_relevant_documents 方法，它使用 self.vectorstore.search 调用 self.similarity_search 方法，默认情况下将限制设置为4 如果没有给出。
我尝试将限制设置为 7，它返回了 7 篇可持续性 文章。
但我不知道有多少文章与可持续性相关，因此我无法默认设置限制。]]></description>
      <guid>https://stackoverflow.com/questions/76603178/self-querying-retrieval-in-langchain-returning-only-4-results</guid>
      <pubDate>Mon, 03 Jul 2023 08:45:41 GMT</pubDate>
    </item>
    </channel>
</rss>