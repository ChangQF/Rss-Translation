<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 07 Jul 2024 18:18:23 GMT</lastBuildDate>
    <item>
      <title>评估无监督学习 - 播放列表生成器应用程序</title>
      <link>https://stackoverflow.com/questions/78718024/evaluate-unsupervised-learning-playlist-generator-app</link>
      <description><![CDATA[我正在为我的简历做一个项目，我想创建一个播放列表生成器应用程序。该应用程序将接收一个播放列表作为输入，并生成一个适合作为输出的播放列表。我的数据将包括从 Spotify for Developers API 中提取的歌曲名称、歌词和歌曲属性，数据集大小约为 150K 个条目。
我正在考虑使用 K-NN 来完成这个无监督任务（或另一种无监督算法），以便对于给定的播放列表输入，我可以将歌曲分类为标签，然后从同一标签中选择相似的歌曲。然而，我开始认为这可能会有问题，因为我不知道事后如何评估我的模型。我无法手动创建带标签的测试数据，因为这对我来说不可行，即使可以，给定的播放列表也没有基本事实。
那么当我完成后，我怎么知道我的模型是否在发挥作用？]]></description>
      <guid>https://stackoverflow.com/questions/78718024/evaluate-unsupervised-learning-playlist-generator-app</guid>
      <pubDate>Sun, 07 Jul 2024 17:53:45 GMT</pubDate>
    </item>
    <item>
      <title>从 UNETR 获取预测后，在 matplotlib 中显示多类标签分割</title>
      <link>https://stackoverflow.com/questions/78717927/multi-class-label-segmentation-display-in-matplotlib-after-getting-preds-from-un</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78717927/multi-class-label-segmentation-display-in-matplotlib-after-getting-preds-from-un</guid>
      <pubDate>Sun, 07 Jul 2024 17:10:00 GMT</pubDate>
    </item>
    <item>
      <title>ValuerError：发现输入变量的样本数量不一致</title>
      <link>https://stackoverflow.com/questions/78717924/valuererror-found-input-variables-with-inconsistent-numbers-of-samples</link>
      <description><![CDATA[我编写了以下代码来学习机器学习方法中的分数。但是我收到了以下错误。原因是什么？？
ValueError: 发现输入变量的样本数量不一致：[6396, 1599]

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv(&#39;Armenian Market Car Prices.csv&#39;)

df[&#39;Car Name&#39;] = df[&#39;Car Name&#39;].astype(&#39;category&#39;).cat.codes

df = df.join(pd.get_dummies(df.FuelType, dtype=int))
df = df.drop(&#39;FuelType&#39;, axis=1)

df[&#39;Region&#39;] = df[&#39;Region&#39;].astype(&#39;category&#39;).cat.codes

df[&#39;Price&#39;] = df.pop(&#39;Price&#39;)

X = df.drop(&#39;Price&#39;, axis=1)
y = df[&#39;Price&#39;]

来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.linear_model 导入 LinearRegression

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()

model.fit(X_train, y_train)

-------------------------------------------------------------------------------
ValueError Traceback (最近一次调用最后一次)
Cell In[358]，第 1 行
----&gt; 1 model.fit(X_train, y_train)

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1473，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1466 estimator._validate_params()
1468 使用 config_context(
1469 skip_parameter_validation=(
1470 prefer_skip_nested_validation 或 global_skip_validation
1471 )
1472 ):
-&gt; 1473 返回 fit_method(estimator, *args, **kwargs)

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_base.py:609，位于 LinearRegression.fit(self, X, y, sample_weight)
605 n_jobs_ = self.n_jobs
607 accept_sparse = False if self.positive else [&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;]
--&gt; 609 X, y = self._validate_data(
610 X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True
611 )
613 has_sw = sample_weight 不为 None
614 if has_sw:

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:650，位于 BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
648 y = check_array(y, input_name=&quot;y&quot;, **check_y_params)
649 else:
--&gt; 650 X, y = check_X_y(X, y, **check_params)
651 out = X, y
653 如果不是 no_val_X 且 check_params.get(&quot;ensure_2d&quot;, True):

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:1291，在 check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, Ensure_2d, allow_nd, multi_output, Ensure_min_samples, Ensure_min_features, y_numeric, estimator)
1273 X = check_array(
1274 X,
1275 accept_sparse=accept_sparse,
(...)
1286 input_name=&quot;X&quot;,
1287 )
1289 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-&gt; 1291 check_consistent_length(X, y)
1293 return X, y

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:460，位于 check_consistent_length(*arrays)
458 uniques = np.unique(lengths)
459 if len(uniques) &gt; 1:
-&gt; 460 raise ValueError(
461 &quot;找到样本数量不一致的输入变量：%r&quot;
462 % [int(l) for l in lengths]
463 )

ValueError：找到样本数量不一致的输入变量：[6396, 1599]

我尝试了所有方法，但都没有用，或者我不知道如何解决问题。
Jupyternaut：

您提供的错误消息表明输入数据存在问题。具体来说，似乎有两个不同版本的输入数据，一个有 6396 个样本，另一个有 1599 个样本。这可能会在尝试拟合模型或对数据执行其他操作时导致问题。
要解决此问题，您可能需要检查代码并确保对每个操作使用正确版本的输入数据。您可能还想尝试清理输入数据，删除任何重复或不一致的数据。
]]></description>
      <guid>https://stackoverflow.com/questions/78717924/valuererror-found-input-variables-with-inconsistent-numbers-of-samples</guid>
      <pubDate>Sun, 07 Jul 2024 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>我在 WLASL 视频数据集上训练的用于预测手语手势的 ConvLSTM 模型在 10 个周期的训练阶段没有提高准确率</title>
      <link>https://stackoverflow.com/questions/78717922/my-convlstm-model-for-predicting-sign-language-gestures-trained-on-wlasl-video-d</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78717922/my-convlstm-model-for-predicting-sign-language-gestures-trained-on-wlasl-video-d</guid>
      <pubDate>Sun, 07 Jul 2024 17:07:23 GMT</pubDate>
    </item>
    <item>
      <title>元特征分析：评估不同特征组合之间复杂性的中间结果</title>
      <link>https://stackoverflow.com/questions/78717886/meta-feature-analysis-intermediate-results-to-assess-complexity-among-different</link>
      <description><![CDATA[我正在使用 meta-feature 分析包。它提供了一种方便的方式来汇总元特征统计信息。
但是在这种情况下，我正在寻找一种通过成对特征组合来访问数据集中数据点复杂性分析的“中间”结果的方法。这是为了让我理解具有特征的数据集比其他数据集具有更高的类重叠（复杂性）。
例如，在只有 4 个特征（萼片长度、萼片宽度、花瓣长度、花瓣宽度）的鸢尾花数据集中：
pip install -U pymfe # package install

from sklearn.datasets import load_iris
from pymfe.mfe import MFE
data = load_iris()
X= data.data
y = data.target

而不是整体摘要：
extractor = MFE(features=[ &quot;f1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
meta_feat = extractor.extract()

为了获得平均值和标准差，我想分析由于萼片长度 v 萼片宽度、萼片长度 v 花瓣长度、萼片长度 v 花瓣宽度等造成的复杂性...因为f1沿垂直轴投射数据点以组合二元特征。
如何实现这一目标？
如果可能的话，一些可视化帮助也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78717886/meta-feature-analysis-intermediate-results-to-assess-complexity-among-different</guid>
      <pubDate>Sun, 07 Jul 2024 16:51:09 GMT</pubDate>
    </item>
    <item>
      <title>GNN 的损失没有减少</title>
      <link>https://stackoverflow.com/questions/78717784/the-loss-in-gnn-doesnt-decrease</link>
      <description><![CDATA[当我研究 GNN 时，我的训练代码无法训练模块，参数保持不变
def create_node_emb(num_node=34, embedding_dim=16):
# TODO：实现此函数，它将创建节点嵌入矩阵。
# 将返回一个 torch.nn.Embedding 层。您不需要更改 
# num_node 和 embedding_dim 的值。返回的 
# 层的权重矩阵应在均匀分布下初始化。

emb = None

############# 您的代码在此处 ############
emb = torch.nn.Embedding(num_embeddings=num_node,embedding_dim=embedding_dim)
shape = emb.weight.data.shape
emb.weight.data = torch.rand(shape)
############################################

return emb

这是正确的代码
def train(emb, loss_fn, sigmoid, train_label, train_edge):
# TODO：在此处训练嵌入层。您还可以更改 epoch 和 
# 学习率。一般来说，你需要实现：
#（1）获取 train_edge 中节点的嵌入
#（2）对每个节点对之间的嵌入进行点积
#（3）将点积结果输入 sigmoid
#（4）将 sigmoid 输出输入 loss_fn
#（5）打印每个 epoch 的损失和准确率
#（6）使用损失和优化器更新嵌入
#（作为健全性检查，损失应该在训练期间减少）
# epochs = 500
learning_rate = 0.1

optimizer = SGD(emb.parameters(), lr=learning_rate, motivation=0.9)

for i in range(epochs):
emb.train()
############# 您的代码在这里 ############
optimizer.zero_grad()

dot_pro = torch.sum(output[0]*output[1],dim=1)
pred = sigmoid(dot_pro)
loss = loss_fn(pred,train_label)
loss.backward()
optimizer.step()

acc = accuracy(pred,train_label)
if i%50 == 0:
print(&quot;acccracy ={},loss={}&quot;.format(acc,loss))
params = list(emb.parameters())
for i, param in enumerate(params):
print(&quot;Parameter {} shape: {}&quot;.format(i, param.shape))
print(param)

但是如果我像下面的代码一样更改训练代码，
def train(emb, loss_fn, sigmoid, train_label, train_edge):
# TODO：在此处训练嵌入层。您还可以更改时期和 
# 学习率。一般来说，你需要实现：
#（1）获取 train_edge 中节点的嵌入
#（2）对每个节点对之间的嵌入进行点积
#（3）将点积结果输入 sigmoid
#（4）将 sigmoid 输出输入 loss_fn
#（5）打印每个 epoch 的损失和准确率
#（6）使用损失和优化器更新嵌入
#（作为健全性检查，损失应该在训练期间减少）
# epochs = 500
learning_rate = 0.1

optimizer = SGD(emb.parameters(), lr=learning_rate, motivation=0.9)

for i in range(epochs):
emb.train()
############# 您的代码在这里 ############
optimizer.zero_grad()

node_list_fir,node_list_sec = (train_edge[0]),(train_edge[1])
emb_list_fir,emb_list_sec = emb(node_list_fir),emb(node_list_sec) 
emb_torch = torch.sum(emb_list_fir*emb_list_sec,dim=1)
emb_torch = sigmoid(emb_torch)

emb_res = torch.round(emb_torch)
loss = loss_fn(emb_res,train_label)

loss.backward()
optimizer.step()

acc = accuracy(emb_res,train_label)
if i%50 == 0:
print(&quot;acccracy ={},loss={}&quot;.format(acc,loss))
params = list(emb.parameters())
for i, param in enumerate(params):
print(&quot;参数 {} 形状: {}&quot;.format(i, param.shape))
print(param)


日志中的参数没有变化
这种现象的原因是什么，我是深度学习新手，为什么我将 train_edge 分成两部分然后输入到模型中，训练效果不好。我不知道背后的理论]]></description>
      <guid>https://stackoverflow.com/questions/78717784/the-loss-in-gnn-doesnt-decrease</guid>
      <pubDate>Sun, 07 Jul 2024 16:07:40 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该为移动应用程序训练 OCR 模型以进行手写文本识别，还是仅使用 API 服务？</title>
      <link>https://stackoverflow.com/questions/78717411/should-i-train-an-ocr-model-for-handwritten-text-recognition-for-a-mobile-app-or</link>
      <description><![CDATA[为移动应用程序构建手写文本识别模型是否值得（需要在下个月完成此项目）？准确率必须至少达到 75%。
我正在构建一个简单的 Android 移动应用程序（React Native 前端），该应用程序能够识别手写文本，这是我学校项目的一部分。
基本上，这个概念是，孩子会在纸上写 15 个短语，应用程序必须至少在 75% 的时间内正确识别它。这些短语很简单，例如：“你好”、“早上好”、“你好吗？”等等。
它不适用于一般的手写文本识别目的，而是只需要识别 15 个特定短语。写这些短语的人是 10-14 岁的孩子。
从成本和性能方面来看，这里最好的选择是什么？我应该只使用 Google Vision 等 API 还是训练模型（例如使用 PaddleOCR 或 TrOCR 的模型）并将其部署到本地？如果是后者，我可以在哪个平台上进行训练，我可以针对我的情况使用哪个数据集？
我尝试使用 Google Vision 和 Microsoft API 进行文本识别，它们确实表现良好，但 1000 个请求的成本约为 2 美元。我还尝试了 Tesseract 和 PaddleOCR 推理模型，但它们的表现不佳。]]></description>
      <guid>https://stackoverflow.com/questions/78717411/should-i-train-an-ocr-model-for-handwritten-text-recognition-for-a-mobile-app-or</guid>
      <pubDate>Sun, 07 Jul 2024 13:21:19 GMT</pubDate>
    </item>
    <item>
      <title>基于 BERT 的推荐系统返回不相关的结果</title>
      <link>https://stackoverflow.com/questions/78717310/bert-based-recommendation-system-returning-irrelevant-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78717310/bert-based-recommendation-system-returning-irrelevant-results</guid>
      <pubDate>Sun, 07 Jul 2024 12:31:41 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：只有整数标量数组才能用 numpy/shap 转换为标量索引[关闭]</title>
      <link>https://stackoverflow.com/questions/78716908/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-nu</link>
      <description><![CDATA[我正在写这段代码：
# 部分依赖图
fig, ax = plt.subplots(figsize=(12, 8))
features_to_plot = list(range(min(3, len(feature_names)))) # 绘制最多 3 个特征
PartialDependenceDisplay.from_estimator(best_rf_model, X_test_scaled, features=features_to_plot,
feature_names=feature_names, ax=ax)
plt.show()

# SHAP 值
explainer = shap.TreeExplainer(best_rf_model)
shap_values = explainer.shap_values(X_test_scaled)

# 处理不同的 SHAP 值形状
if isinstance(shap_values, list):
# 对于多类问题
print(&quot;多类 SHAP 值检测到&quot;)
for i, class_shap_values in enumerate(shap_values):
plt.figure(figsize=(12, 8))
shap.summary_plot(class_shap_values, X_test_scaled, plot_type=&quot;bar&quot;, feature_names=feature_names, show=False)
plt.title(f&quot;SHAP Feature Importance for Class {i}&quot;)

我一直收到我在标题中描述的错误。
我发现这真的很奇怪，因为我已经确保它是 1d 并且所有元素都是整数，所以我应该怎么做才能解决这个问题？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78716908/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-nu</guid>
      <pubDate>Sun, 07 Jul 2024 09:26:47 GMT</pubDate>
    </item>
    <item>
      <title>Darknet Yolov4-tiny（灰度输入）到 Tensorflow 权重，转换</title>
      <link>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</link>
      <description><![CDATA[TL;DR:
1 通道 TF 模型的行为与 3 通道模型不同。两者都成功从 Darknet -&gt; TF 转换，但 1 通道模型的表现不如转换前。
手头的任务和声明：
我有两个经过训练的 yolov4-tiny darknet 权重文件 (.weights)，一个有灰度输入（1 通道），另一个有颜色输入（3 通道）。我正在将两个权重文件转换为 Tensorflow 检查点格式，使用一个通用存储库（用于此任务），该存储库位于：
https://github.com/hunglc007/tensorflow-yolov4-tflite.git
两种模型的性能都已通过 c++ opencv readNetFromDarknet() 和 Python 等效项进行了测试。这两个模型本质上都是用灰度图像进行训练的，并且对灰度图像进行操作。3 通道模型的输入只是缩放到 3 通道的灰度图像。
Python 版本：3.10.11
TF 版本：2.10.1
问题陈述：
使用 tf.keras.Models.load_model(X) 加载时，带有颜色输入的权重文件转换良好，之后运行良好，但是当转换灰度输入权重文件时，使用 Tensorflow 加载时模型的性能急剧下降，我的意思是在最明显的情况下，带有颜色输入的模型运行完美，检测效果很差或不存在。值得注意的是，框不会错位，这意味着当发现检测结果时，它们大约在正确的位置，但例如宽度和高度可能会偏离。
我知道这个存储库的常见问题（硬编码内容等），并相应地更改了每次转换/模型加载的参数，并且在转换或模型加载期间不会发生任何错误。
我已经确认了输入层：

灰度：（无，640,640,1）
颜色：（无，640,640,3）

测试图像（用于性能测试）使用 opencv-python 加载，并且它们的有效性也已审查，即使将错误维度的数据插入到输入层也会出现错误。
除输入层之外的架构相同，已使用 model.summary() 确认。
我注意到，几年前我用不同的 TF 版本转换的 3 通道模型由 model.summary() 生成的架构有些不同。一些图块层似乎缺失了。此外，一些 tf 操作的名称也不同，但这可能只是 TF 版本不同。
旧颜色模型：
 tf_op_layer_Sigmoid (TensorFlo (None, 40, 40, 3, 2 0 [&#39;tf_op_layer_split_3[0][0]&#39;]
wOpLayer) )

tf_op_layer_Tile/multiples (Te (5,) 0 [&#39;tf_op_layer_strided_slice[0][0]
nsorFlowOpLayer) &#39;]

tf_op_layer_Sigmoid_3 (TensorF (None, 20, 20, 3, 2 0 [&#39;tf_op_layer_split_4[0][0]&#39;]
lowOpLayer) ) )

新灰度模型：
 tf.math.sigmoid (TFOpLambda) (无，40，40，3，2 0 [&#39;tf.split_3[0][0]&#39;]
)

---此处缺少图块层---

tf.math.sigmoid_3 (TFOpLambda) (无，20，20，3，2 0 [&#39;tf.split_4[0][0]&#39;]
)

我现在很困惑。有什么帮助吗？
一些反复试验：

使用 Yolov4-tiny Head 解码块 -&gt;即使在模型能够加载的情况下也没有变化（解码时错误的尺寸会引发错误）
之前提到的旧 3 通道模型（几年前已转换为 Darknet -&gt; TF），当以与新模型相同的方式加载时，可以完美运行
]]></description>
      <guid>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</guid>
      <pubDate>Fri, 05 Jul 2024 13:54:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 torch.where 对张量进行阈值处理是否会将其张量从计算图中分离出来？</title>
      <link>https://stackoverflow.com/questions/78704234/does-using-torch-where-to-threshold-a-tensor-detach-it-tensor-from-the-computati</link>
      <description><![CDATA[我正在 PyTorch 中为多类语义分割编写自定义损失函数。该函数的一部分是从张量中选择阈值通道，这些通道用 tracker_index 表示。
该函数的最后一部分是计算图的一部分，即 channel_tensor，如果我注释掉应用 torch.where 的行，一切都会顺利运行。我尝试将 1 和 0 设置为 float32 张量，并确保它们与 channel_tensor 位于同一设备上，这让我相信八分之一阈值是不可微的，因此不能成为损失函数的一部分，否则 torch.where 将始终将张量从计算图中分离出来。请指教。
channel_tensor =torch.select(
segmentation_output,
dim=-3,
index=tracker_index
)
channels[tracker_index]= torch.where(channel_tensor &gt; self.threshold, torch.tensor(1, device=channel_tensor.device, dtype=torch.float32), torch.tensor(0, device=channel_tensor.device, dtype=torch.float32))
]]></description>
      <guid>https://stackoverflow.com/questions/78704234/does-using-torch-where-to-threshold-a-tensor-detach-it-tensor-from-the-computati</guid>
      <pubDate>Wed, 03 Jul 2024 21:13:24 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用免费资源在本地机器上的大数据集上训练 gpt2 [关闭]</title>
      <link>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</link>
      <description><![CDATA[是否可以在 colab、jupyter 或 kaggle 上对 1.5m 个数据点进行 gpt2 训练？
到目前为止，我尝试在 colab 中进行此操作，但在标记化过程中会耗尽存储空间，这是可以理解的。我也尝试了批处理技术。后来我尝试在 kaggle 上运行相同的算法，但目前它在加载转换器时显示错误。仍在尝试运行它。我只是想知道是否可以做到这一点！]]></description>
      <guid>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</guid>
      <pubDate>Sat, 29 Jun 2024 17:07:55 GMT</pubDate>
    </item>
    <item>
      <title>我们应该在 train_test_split() 中为 random_state 使用什么值，以及在哪种情况下使用？[关闭]</title>
      <link>https://stackoverflow.com/questions/54264452/what-value-should-we-use-for-random-state-in-train-test-split-and-in-which-sce</link>
      <description><![CDATA[X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

在上面的代码中，random_state 使用的是 0。为什么我们不使用 1？]]></description>
      <guid>https://stackoverflow.com/questions/54264452/what-value-should-we-use-for-random-state-in-train-test-split-and-in-which-sce</guid>
      <pubDate>Sat, 19 Jan 2019 05:52:01 GMT</pubDate>
    </item>
    <item>
      <title>为 sklearn 算法选择 random_state</title>
      <link>https://stackoverflow.com/questions/26097921/choosing-random-state-for-sklearn-algorithms</link>
      <description><![CDATA[我理解 random_state 用于各种 sklearn 算法，以打破具有相同度量值的不同预测器（树）之间的平局（例如 GradientBoosting）。但文档并未对此进行澄清或详细说明。例如 
1）这些种子还用于哪些随机数生成？例如，对于 RandomForestClassifier，随机数可用于查找一组随机特征来构建预测器。使用子采样的算法可以使用随机数来获取不同的子样本。同一个种子（random_state）是否可以在多个随机数生成中发挥作用？
我主要关心的是 
2）这个 random_state 变量的影响有多大？该值能否对预测（分类或回归）产生重大影响。如果是，我应该更关注哪种数据集？还是更关注稳定性而不是结果质量？
3) 如果它能产生很大的影响，如何最好地选择 random_state？如果没有直觉，很难进行 GridSearch。特别是如果数据集是这样的，一个 CV 可能需要一个小时。
4) 如果动机只是让我的模型获得稳定的结果/评估，并在重复运行中获得交叉验证分数，那么如果我在使用任何算法之前设置 random.seed(X)（并使用 random_state 作为 None），是否具有相同的效果。
5) 假设我在 GradientBoosted 分类器上使用 random_state 值，并且我正在交叉验证以找到我的模型的优点（每次都在验证集上评分）。一旦满意，我将在整个训练集上训练我的模型，然后再将其应用于测试集。现在，完整训练集比交叉验证中的较小训练集具有更多实例。因此，与 cv 循环中发生的情况相比，random_state 值现在可能导致完全不同的行为（特征选择和单个预测变量）。类似地，现在设置与 CV 中的实例数有关，而实际实例数更多，诸如最小样本叶等也可能导致模型较差。这是正确的理解吗？有什么方法可以防止这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/26097921/choosing-random-state-for-sklearn-algorithms</guid>
      <pubDate>Mon, 29 Sep 2014 10:38:22 GMT</pubDate>
    </item>
    <item>
      <title>在逻辑回归中使用排名数据</title>
      <link>https://stackoverflow.com/questions/22117692/using-ranking-data-in-logistic-regression</link>
      <description><![CDATA[我正在尝试在逻辑回归中使用一些排名数据。我想使用机器学习来制作一个简单的分类器，以确定网页是否“好”。这只是一个学习练习，所以我不期望获得很好的结果；只是希望学习“过程”和编码技术。
我已将我的数据放入 .csv 中，如下所示：
URL WebsiteText AlexaRank GooglePageRank

在我的测试 CSV 中，我们有：
URL WebsiteText AlexaRank GooglePageRank 标签

标签是二进制分类，用 1 表示“好”，用 0 表示“坏”。
我目前仅使用网站文本运行 LR；我在其上运行了 TF-IDF。
我有两个问题需要帮助：

如何规范化我的 AlexaRank 排名数据？我有一组 10,000 个网页，我有所有网页的 Alexa 排名；但是它们的排名不是 1-10,000。它们在整个互联网中排名靠前，因此虽然 http://www.google.com 可能排名 #1，但 http://www.notasite.com 可能排名 #83904803289480。如何在 Scikit learn 中对此进行规范化，以便从我的数据中获得最佳结果？

我以这种方式运行我的逻辑回归；我几乎可以肯定我做错了。我试图对网站文本进行 TF-IDF，然后添加另外两个相关列并拟合逻辑回归。如果有人能快速验证我是否正确地采用了我想在 LR 中使用的三列，我将不胜感激。
 loadData = lambda f: np.genfromtxt(open(f,&#39;r&#39;), delimiter=&#39; &#39;)

print &quot;loading data..&quot;
traindata = list(np.array(p.read_table(&#39;train.tsv&#39;))[:,2])#Reading WebsiteText 列进行 TF-IDF。
testdata = list(np.array(p.read_table(&#39;test.tsv&#39;))[:,2])
y = np.array(p.read_table(&#39;train.tsv&#39;))[:,-1] #读取标签

tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents=&#39;unicode&#39;, analyzer=&#39;word&#39;,

token_pattern=r&#39;\w{1,}&#39;, ngram_range=(1, 2), use_idf=1, smooth_idf=1,sublinear_tf=1)

rd = lm.LogisticRegression(penalty=&#39;l2&#39;, dual=True, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=None)

X_all = traindata + testdata
lentrain = len(traindata)

print &quot;fitting pipeline&quot;
tfv.fit(X_all)
print &quot;transforming data&quot;
X_all = tfv.transform(X_all)
X = X_all[:lentrain]
X_test = X_all[lentrain:]

print &quot;20 Fold CV Score: &quot;, np.mean(cross_validation.cross_val_score(rd, X, y, cv=20,scoring=&#39;roc_auc&#39;))

#添加两个整数列
AlexaAndGoogleTrainData = list(np.array(p.read_table(&#39;train.tsv&#39;))[2:,3])#不确定我是否做对了。期望它包含 AlexaRank 和 GooglePageRank 列。
AlexaAndGoogleTestData = list(np.array(p.read_table(&#39;test.tsv&#39;))[2:,3])
AllAlexaAndGoogleInfo = AlexaAndGoogleTestData + AlexaAndGoogleTrainData

#向 X 添加两列。
X = np.append(X, AllAlexaAndGoogleInfo, 1) #我认为我做错了。

print &quot;training on full data&quot;
rd.fit(X,y)
pred = rd.predict_proba(X_test)[:,1]
testfile = p.read_csv(&#39;test.tsv&#39;, sep=&quot;\t&quot;, na_values=[&#39;?&#39;], index_col=1)
pred_df = p.DataFrame(pred, index=testfile.index, columns=[&#39;label&#39;])
pred_df.to_csv(&#39;benchmark.csv&#39;)
print &quot;提交文件已创建..&quot;`


]]></description>
      <guid>https://stackoverflow.com/questions/22117692/using-ranking-data-in-logistic-regression</guid>
      <pubDate>Sat, 01 Mar 2014 17:25:19 GMT</pubDate>
    </item>
    </channel>
</rss>