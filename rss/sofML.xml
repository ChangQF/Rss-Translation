<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Jan 2024 15:15:15 GMT</lastBuildDate>
    <item>
      <title>是否有一个值可以表达多种因素对结果的影响？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77846958/is-there-an-value-that-expresses-the-effect-of-multiple-factors-to-the-outcome</link>
      <description><![CDATA[我有多个变量，例如 X_1、X_2、...、X_n，它们不一定是独立的，但预计会影响 Y。
是否有一个值可以指示 X_1、X_2、...、X_n 对 Y 总共有多大影响？
例如，如果还有其他变量 Z_1、Z_2、..._、Z_m 也影响 Y，但与 X_1 ~X_n 无关，我们进行线性回归并得到 Y=w_1X_1 +w_2X_2+ ...+w_nX_n+ w_n +1 Z_1 +...+w_n+m Z_m ，“w_1X_1 +...+w_nX_n”可能是一个指标。]]></description>
      <guid>https://stackoverflow.com/questions/77846958/is-there-an-value-that-expresses-the-effect-of-multiple-factors-to-the-outcome</guid>
      <pubDate>Fri, 19 Jan 2024 14:53:49 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 VGGish 正确运行并查询音频分类的嵌入</title>
      <link>https://stackoverflow.com/questions/77846542/how-to-know-vggish-runs-correctly-and-queries-about-embeddings-for-audio-classif</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77846542/how-to-know-vggish-runs-correctly-and-queries-about-embeddings-for-audio-classif</guid>
      <pubDate>Fri, 19 Jan 2024 13:45:06 GMT</pubDate>
    </item>
    <item>
      <title>从头开始的 DQN 给出错误形状的输出</title>
      <link>https://stackoverflow.com/questions/77846372/dqn-from-scratch-giving-wrong-shaped-output</link>
      <description><![CDATA[我正在尝试从头开始构建一个 DQN 代理，目前当我将观察结果传递到网络时，它会生成一个 3D 数组，而它应该是 2D
网络代码：
类网络()：
    def __init__(self, 层=无):
        self.layers = 层数
        自我损失=无
        self.loss_derivative = 无
        print(&quot;模型启动...&quot;)

    def use(self, loss, loss_derivative):
        自我损失=损失
        self.loss_derivative = loss_derivative
        print(&quot;错误函数已初始化...&quot;)

    def 预测（自身，输入）：
        结果=[]
        对于范围内的 i（len（输入））：
            输出=输入[i]
            对于 self.layers 中的图层：
                输出=层.forward_propogation(输出)
            结果.追加（输出）
        返回结果
    
    def fit（自我，x_train，y_train，epochs = 1000，learning_rate = 0.1，batch_size = None）：
        print(&quot;模型开始训练...&quot;)
        对于范围内的 i（纪元）：
            如果可调用（学习率）：
                学习率 = 学习率(i)
            总损失= 0
            如果batch_size为None：
                对于范围内的 j(len(x_train))：
                    输出 = x_train[j]
                    对于 self.layers 中的图层：
                        输出=层.forward_propogation(输出)
                    Total_loss += self.loss(y_train[j], 输出)

                    output_error_gradient = self.loss_derivative(y_train[j], 输出)
                    对于反向层（self.layers）：
                        输出误差梯度=层.反向传播（输出误差梯度，学习率）
            elif batch_size 不是 None：
                对于 self.get_minibatches 中的 x_batch、y_batch（y_train、x_train、batch_size、shuffle=True）：
                    批量损失 = 0
                    输出错误梯度列表 = []
                    对于范围内的 j(len(x_batch))：
                        输出 = x_batch[j]
                        对于 self.layers 中的图层：
                            输出=层.forward_propogation(输出)
                        batch_loss += self.loss(y_batch[j], 输出)

                        list_of_output_error_gradients.append(self.loss_derivative(y_batch[j], 输出))

                    输出误差梯度 = np.mean(输出误差梯度列表，轴 = 1)
                    对于反向层（self.layers）：
                        输出误差梯度=层.反向传播（输出误差梯度，学习率）
                    总损失 += 批次损失
            总损失 /= len(x_train)
            
            print(f&quot;Epoch {i+1} / {epochs} Error={total_loss}, lr = {learning_rate}&quot;)

图层代码：
类密集（层）：
    def __init__(self, n_inputs, n_neurons):
        self.n_inputs = n_inputs
        self.n_神经元 = n_神经元
        self.weights = np.random.randn(n_neurons, n_inputs) * np.sqrt(2 / n_inputs)
        self.biases = np.zeros((n_neurons, 1))

    defforward_propogation（自身，输入）：
        self.input = 输入
        self.output = np.dot(self.weights, self.input) + self.biases
        返回自身输出
    
    def向后传播（自身，输出误差梯度，学习率）：
        输入误差梯度 = np.dot(self.weights.T, 输出误差梯度)
        
        weights_error_gradient = np.dot(output_error_gradient, self.input.T)
        baises_error_gradient = 输出误差梯度

        self.weights -= 学习率 * 权重误差梯度
        self.biases -= 学习率 * baises_error_gradient

        返回输入错误梯度

我正在运行的代码：
data = pd.read_csv(“RL/GOOGL.csv”)
data = data.set_index(“日期”)
data = data.drop([“调整关闭”], axis = 1)
data.index = pd.to_datetime(data.index)

env = StocksEnv(数据)
obsv = env.reset()

激活 = 激活（relu，relu_derivative）

代理 = DQNAgent(env)
obsv = np.reshape(obsv, (1, obsv.shape[0]))
预测 = agent.main_net.predict(obsv)
预测 = np.array(预测)
打印（预测.形状）

输出 (1, 3, 64)
Obsv 的形状为 (5, )，我尝试将其更改为 (1, 5)，得到了上面的输出。我期望输出形状为 (1, 3) 1 行，每个操作都有一个值。]]></description>
      <guid>https://stackoverflow.com/questions/77846372/dqn-from-scratch-giving-wrong-shaped-output</guid>
      <pubDate>Fri, 19 Jan 2024 13:21:14 GMT</pubDate>
    </item>
    <item>
      <title>Ultralytics 训练回调中的 gRPC 服务器流</title>
      <link>https://stackoverflow.com/questions/77846210/grpc-server-streaming-in-ultralytics-training-callback</link>
      <description><![CDATA[我正在开发一个使用 ultralytics 包、YOLOv8 模型和 gRPC 的 ML 训练服务器和客户端。服务器是用Python编写的，客户端是用C#编写的。我一切顺利，可以从客户端毫无问题地开始培训过程。
现在我想向客户报告培训过程，以直观地向用户更新培训的进展情况。
为此，我创建了这个小原型文件：
syntax = “proto3”;

包training_client；

服务 培训服务 {
  rpc StartTraining(StartTrainingRequest) 返回 (stream InTrainingResponse) {}
}

消息开始训练请求{}

消息指标
{
  字符串名称=1；
  浮点值 = 2；
}

消息训练响应
{
  int32 纪元 = 1；
  重复的 Metric 指标 = 2；
}

我的基本服务器实现如下所示：
来自多处理导入进程、队列

导入gpc
从 ultralytics.engine.trainer 导入 BaseTrainer

导入training_pb2_grpc
从 ultralytics 导入 YOLO
从并发.futures 导入 ThreadPoolExecutor

从 Training_pb2 导入 InTrainingResponse，指标

def on_train_epoch_end(训练器: BaseTrainer):
    print(&quot;将值放入队列&quot;)
    TrainingServicer.progress_queue.put((trainer.epoch, trainer.metrics))

def on_train_end(训练器: BaseTrainer):
    print(&quot;训练结束&quot;)
    TrainingServicer.progress_queue.put(None) # 训练完成，打破无限循环

类 TrainingServicer(training_pb2_grpc.TrainingServiceServicer):
    进度队列 = 队列()

    def __init__(自身):
        超级().__init__()
        self.model = YOLO(“yolov8m.pt”)
        self.model.add_callback(“on_train_epoch_end”, on_train_epoch_end)
        self.model.add_callback(“on_train_end”, on_train_end)

    def run_training（自我）：
        self.model.train（数据=“数据集/data.yaml”，纪元= 15，imgsz = 512，批次= 2，设备= 0）

    def StartTraining（自身，请求，上下文）：
        Training_thread = Process(目标=self.run_training)
        训练线程.start()

        而真实：
            尝试：
                项目 = TrainingServicer.progress_queue.get(timeout=1)
                如果项目为无：
                    休息
                纪元，指标 = 项目
                resp = InTrainingResponse(epoch=epoch)
                对于metrics.items()中的k、v：
                    resp.metrics.append(指标(名称=k,值=v))
                print(&quot;产量训练更新&quot;)
                产量响应
            除了异常 e：
                print(&quot;队列为空或没有新数据可用&quot;)
                继续


定义服务（）：
    服务器 = grpc.server(ThreadPoolExecutor(max_workers=10))
    Training_pb2_grpc.add_TrainingServiceServicer_to_server（TrainingServicer（），服务器）
    server.add_secure_port(&#39;[::]:30008&#39;, grpc.local_server_credentials())
    print(&quot;启动服务器&quot;)
    服务器.start()
    server.wait_for_termination()


如果 __name__ == &#39;__main__&#39;:
    服务（）

我的基本客户是这样的：
使用 Grpc.Net.Client;
使用 Grpc.Core；
使用 TrainingClient；

使用 var Channel = GrpcChannel.ForAddress(“http://localhost:30008”);
var client = new TrainingService.TrainingServiceClient(通道);

使用 var call = client.StartTraining(new StartTrainingRequest());
等待foreach（call.ResponseStream.ReadAllAsync（）中的var纪元）
{
     Console.WriteLine($&quot;已收到 Epoch {epoch.Epoch} 和 {epoch.Metrics}&quot;);
}

我现在的问题是回调已成功将项目放入队列中，但队列的使用者部分从未收到它们，这表明对我来说两个队列是不同的实例，我通过检查它们的内存地址很快确认了这一点。
对 model.train() 的调用是阻塞的，这就是为什么我尝试在不同的进程中运行它，以便能够将每个结果返回给 RPC。
我对multiprocessing.Queue的理解是，这个实现使用共享内存空间来允许不同的进程共享数据，但我似乎无法正确使用它。
上面解释了：
尝试使用多处理来使用生产者/消费者类型的模式来向客户端报告训练中期的训练指标。]]></description>
      <guid>https://stackoverflow.com/questions/77846210/grpc-server-streaming-in-ultralytics-training-callback</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 预测任何数字（没有类/标签）[关闭]</title>
      <link>https://stackoverflow.com/questions/77846035/using-keras-to-predict-any-number-without-classes-labels</link>
      <description><![CDATA[我是 keras 新手，我正在尝试制作一个可以预测人年龄的模型。
有没有办法让 .predict() 预测任何数字，而不是必须给它一个范围或桶（类）来从中选择（预测概率）？
我尝试了（许多变体）提供类/标签，并且预测概率与我的范围（如 0 岁到 100 岁）成线性比例。我也尝试了一整个星期来搜索类似的问题。]]></description>
      <guid>https://stackoverflow.com/questions/77846035/using-keras-to-predict-any-number-without-classes-labels</guid>
      <pubDate>Fri, 19 Jan 2024 12:21:08 GMT</pubDate>
    </item>
    <item>
      <title>基于类的文本生成的最新技术是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77845982/whats-the-state-of-the-art-for-class-based-text-generation</link>
      <description><![CDATA[我正在开发一个项目，我想训练（即微调）一个文本生成模块，类似于马尔可夫链用于文本预测的工作方式。特别是，我有一个由几个类组成的数据集，其中每个类都有一定数量的实例。例如，它可能类似于具有“解释”、“证明”、“QnA”等类别的论文集合。我希望模型能够同时在所有类别上进行微调，并分别预测每个类别的相似文本，而不是在每个类别上训练不同的模型。
我已经使用 LLaMA 搜索了一些解决方案，但我还没有找到任何可以执行此多类任务的解决方案（也就是说，除了聊天机器人之类的东西，但似乎直接微调是更好的方法）。有没有任何模型能够处理这项任务？或者，在一种元学习方法中微调模型是否是更好的做法，其中训练样本包含带有类名的前缀？]]></description>
      <guid>https://stackoverflow.com/questions/77845982/whats-the-state-of-the-art-for-class-based-text-generation</guid>
      <pubDate>Fri, 19 Jan 2024 12:12:50 GMT</pubDate>
    </item>
    <item>
      <title>在 kaggle 中运行 .ipynb 文件时查找错误</title>
      <link>https://stackoverflow.com/questions/77845629/lookup-error-while-running-the-ipynb-file-in-kaggle</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77845629/lookup-error-while-running-the-ipynb-file-in-kaggle</guid>
      <pubDate>Fri, 19 Jan 2024 11:11:35 GMT</pubDate>
    </item>
    <item>
      <title>NLP 的生命周期有哪些不同阶段？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77845526/what-are-different-lifecycle-stages-of-nlp</link>
      <description><![CDATA[NLP 的不同生命周期阶段是什么？
我想要简要解释 NLP 及其生命周期 NLP 及其不同的生命周期 Natural Language API 允许您轻松地将 NLU 应用到您的应用程序中。自然语言 API 使用机器学习来揭示含义和含义。 text.API的结构]]></description>
      <guid>https://stackoverflow.com/questions/77845526/what-are-different-lifecycle-stages-of-nlp</guid>
      <pubDate>Fri, 19 Jan 2024 10:54:21 GMT</pubDate>
    </item>
    <item>
      <title>我有一个由 12 列组成的数据集，现在我想训练我的模型，这样如果输入任何类型的提示，它就会从 1 列生成信息 [关闭]</title>
      <link>https://stackoverflow.com/questions/77845089/i-have-a-dataset-which-consists-of-12-columns-now-i-want-train-my-model-such-th</link>
      <description><![CDATA[我的列是：主题；描述;介绍; SOW（工作范围）；除外情况；标题;描述1；描述2；产品;描述3；描述4；描述5.现在，我想在这个数据集上训练我的模型，当我在 Prompt 中输入任何列的信息时，它会生成并给我 SOW，即使 Prompt 包含不属于训练数据集的内容，它也会生成 SOW。至少尝试产生某种类型的工作范围。
下面是数据集的示例，即它是数据集的一行：
例如
主题：空调改造
描述：不适用
简介：根据您的请求和我们的后续调查。我们现在很高兴提交报价，以便在上述地址进行机械服务修改
母猪：--&gt; 4个寄存器的搬迁--&gt;供应和安装 1 个收银机 --&gt; 供应和安装 3 个门传递格栅
排除情况：--&gt;除非另有说明，否则在正常工作时间之外工作。 --&gt;未列入上述范围的作品 --&gt;系统安装所需以外的建筑工程 --&gt;绘画或修补 --&gt;如有必要，对配电盘或电源进行电气升级 --&gt;现有管道系统和调节器的空气平衡（如有必要） --&gt;除非另有说明，否则保留、违约金和间接损失除外。 --&gt;除非另有说明，提供设备的安全访问仍然是建筑物业主及其代理人的责任。 --&gt;如果在执行报价工作时发现有缺陷，则对上述报价中未详细说明的任何设备进行维修或更换。
标题：St. Vincent&#39;s - 用于楼梯间增压风扇 3-2 的 VSD
描述1：St. Vincent&#39;s - 楼梯间增压风机3-2的VSD
描述2：不适用
产品：管道工程/格栅/Flexs
描述3：EC电源
描述4：大金FDYQ160LB-AV
描述5：每次测试成本 5.50 美元
我尝试通过引用其他列来训练模型，但生成的输出无法生成 SOW，而我只想要 SOW。]]></description>
      <guid>https://stackoverflow.com/questions/77845089/i-have-a-dataset-which-consists-of-12-columns-now-i-want-train-my-model-such-th</guid>
      <pubDate>Fri, 19 Jan 2024 09:42:32 GMT</pubDate>
    </item>
    <item>
      <title>OpenFL 连接/设置 [关闭]</title>
      <link>https://stackoverflow.com/questions/77845060/openfl-connection-setup</link>
      <description><![CDATA[我们对联合学习非常陌生，我们希望使用 OpenFL 框架设置 2 个不同的设备（笔记本电脑）作为聚合器和协作器。我们已经为同一任务安装了所有必需的库，但没有找到任何教程可以帮助我们解决同样的问题。有人可以帮助我们解决这个问题吗？
我们甚至不确定如何开始解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77845060/openfl-connection-setup</guid>
      <pubDate>Fri, 19 Jan 2024 09:37:39 GMT</pubDate>
    </item>
    <item>
      <title>Python dgl 库 API 更新</title>
      <link>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</link>
      <description><![CDATA[这是我的代码：
def 标准化（自我，logits）：
    self.\_logits_name = “\_logits”
    self.\_normalizer_name = “\_norm”
    self.g.edata\[self.\_logits_name\] = logits

    self.g.update_all(fn.copy_u(self._logits_name, self._logits_name),
                     fn.sum(self._logits_name, self._normalizer_name))
    返回 self.g.edata.pop(self._logits_name), self.g.ndata.pop(self._normalizer_name)

def edge_softmax(自身):

    如果 self.l0 == 0:
        分数 = self.softmax(self.g, self.g.edata.pop(&#39;a&#39;))
    别的：
        分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))
        self.g.ndata[&#39;z&#39;] = 标准化器[:,0,:].unsqueeze(1)

    self.g.edata[&#39;a&#39;] = 分数[:,0,:].unsqueeze(1)

这是堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“/datasets/\_deepnote_work/train.py”，第 211 行，位于 \ 中
主要（参数）

文件“/datasets/\_deepnote_work/train.py”，第 130 行，在 main 中
logits = 模型（特征）

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 209 行，向前
h，边缘 = self.gat_layers\[0\](h，边缘)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 105 行，向前
self.edge_softmax()

文件“/datasets/\_deepnote_work/gat.py”，第 166 行，edge_softmax
分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))

文件“/datasets/\_deepnote_work/gat.py”，第 157 行，标准化
self.g.update_all(fn.copy_u(self.\_logits_name, self.\_logits_name),

文件“/root/venv/lib/python3.9/site-packages/dgl/heterograph.py”，第 5110 行，位于 update_all
ndata = core.message_passing()
文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 398 行，message_passing
ndata = invoke_gspmm(g, mfunc, rfunc)

文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 361 行，invoke_gspmm
x = alldata\[mfunc.target\]\[mfunc.in_field\]

文件“/root/venv/lib/python3.9/site-packages/dgl/view.py”，第 80 行，在 _getitem_ 中
返回 self.\_graph.\_get_n_repr(self.\_ntid, self.\_nodes)\[key\]

文件“/root/venv/lib/python3.9/site-packages/dgl/frame.py”，第 688 行，在 _getitem_ 中
返回 self.\_columns\[name\].data
关键错误：&#39;\_logits&#39;

我查看了DGLEdgeBatch的文档，但没有找到任何解决方案
链接到文档：https://docs.dgl.ai/en/1.1.x/api/python/udf.html#edge-wise-user-defined-function
阅读 DGL 的文档并尝试了一些替代函数。但他们没有工作。
如何修复/更新代码？
编辑：
抱歉没说清楚。但上面的代码是旧的。我尝试运行它并收到很多运行时错误。我相信自那以后 API 发生了很多变化。您能否建议我进行一些更改以使代码正常工作？]]></description>
      <guid>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</guid>
      <pubDate>Thu, 18 Jan 2024 06:02:39 GMT</pubDate>
    </item>
    <item>
      <title>相关矩阵的累积 AOC 计算 [关闭]</title>
      <link>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</link>
      <description><![CDATA[我正在使用一个非常简单的数据集（胎儿健康分类）进行练习，使用支持向量机、相关指标和典型模型指标（没什么特别的）进行一些练习。我想做以下事情：

采用（与目标）最相关的变量并计算 SVM 模型；然后保留 AUC 结果。
采用第二个最相关的变量（与目标）并使用第一个和第二个变量，计算 SVM 模型；然后保留 AUC 结果。
依此类推......直到到达最后一个变量

之后，我需要创建一个显示以下信息的图表：

X轴：累计变量数
Y 轴：模型中包含的每个变量数量对应的 AUC

我有以下代码；我认为这是合理的。然而，它被卡住了。我不得不中断迭代，因为它们似乎没有结束。关于如何修复循环有什么建议吗？
**导入参考文件的一些行**

df = pd.read_csv(&quot;ASI_casoPractico.csv&quot;, sep = &quot;;&quot;)

# 导入库

将 pandas 导入为 pd
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

# 相关矩阵

corr_matrix = df.corr().abs()
排序校正 =
corr_matrix[&#39;目标&#39;].sort_values(升序=False)

# 创建按相关性排序的变量列表

Sorted_vars = Sorted_corr.index.tolist()

# 为结果创建空列表

结果=[]

# 使用 SVM 进行变量迭代和模型生成

对于范围内的 i(1, len(sorted_vars) + 1)：

  # 选择相关性最好的变量
  选定的变量 = 排序的变量[:i]

  # 训练和测试的数据分开

  X_train = df[selected_vars]
  y_train = df[&#39;目标&#39;]

  # 训练支持向量机

  svm = SVC(内核=&#39;线性&#39;, 概率=True)
  svm.fit(X_train, y_train)

  # 计算AUC
  y_pred = svm.predict_proba(X_train)[:, 1]
  auc = roc_auc_score(y_train, y_pred)

  # 将值添加到列表中
  结果.append([i, auc])

# 为结果创建数据框
results_df = pd.DataFrame(结果, columns=[&#39;变量&#39;, &#39;AUC&#39;])

# 图
results_df.plot(x=&#39;变量&#39;, y=&#39;AUC&#39;, kind=&#39;线&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 05:28:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 scikit-learn 中对线性回归模型使用交叉验证</title>
      <link>https://stackoverflow.com/questions/77829091/how-to-use-cross-validation-on-linear-regression-model-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中使用网格搜索交叉验证进行训练线性回归模型可以说是 10 倍，就像我分享的图像中一样。
但是当我这样做时，我得到：
spipe = 管道([
    （&#39;缩放&#39;，StandardScaler（）），
    (&#39;模型&#39;, 线性回归())
]）

网格 = GridSearchCV(
    估计器=管道，
    简历=4
）

网格.fit(X,Y)

类型错误：GridSearchCV.__init__() 缺少 1 个必需参数：&#39;param_grid&#39;

所以我的理解是它想要迭代 LinearRegression 模型的可能参数，我应该将它们放入 param_grid 中。
但我不想为每次折叠调整参数。相反，我想简单地按照照片所示进行操作：进行 10 次折叠并对其进行 10 次训练和验证，以便模型微调 1 个线性回归多项式（我想这就是模型内部发生的情况）。
我尝试使用cross_val_score，但它似乎在 10 次折叠上训练 10 次，因为它返回 10 个分数而不是 1 个分数（所以我猜测 10 个线性回归多项式，每个折叠 1 个）。 
总而言之，如何将折叠交叉验证方法与线性回归结合使用？
如果有人需要，这里是设置：
从 sklearn.linear_model 导入 LinearRegression
从 sklearn.datasets 导入 fetch_california_housing
将 pandas 导入为 pd
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.model_selection 导入 GridSearchCV

加利福尼亚州 = fetch_california_housing()

pd.set_option(&#39;显示.精度&#39;, 4)
pd.set_option(&#39;display.max_columns&#39;, 9)
pd.set_option(&#39;display.width&#39;, None)
california_df = pd.DataFrame(california.data,
                             列=加利福尼亚.feature_names）
california_df[&#39;MedHouseValue&#39;] = pd.Series(california.target)
X = 加利福尼亚州. 数据
Y = 加利福尼亚.目标
]]></description>
      <guid>https://stackoverflow.com/questions/77829091/how-to-use-cross-validation-on-linear-regression-model-in-scikit-learn</guid>
      <pubDate>Tue, 16 Jan 2024 22:44:50 GMT</pubDate>
    </item>
    <item>
      <title>子类化 keras.model 以创建具有多列输入的自定义自回归 LSTM 模型</title>
      <link>https://stackoverflow.com/questions/76957152/subclassing-keras-model-to-create-a-custom-autoregressive-lstm-model-with-multi</link>
      <description><![CDATA[我正在尝试创建一个模型来根据天气数据预测能源电网负载（电网消耗的净电量）。在生产中，我们没有负载数据来进行标准批量预测。我们正在尝试一种自回归方法，以便我们可以向其提供最后报告的负载读数和未来 24 小时的预测天气数据，以生成 24 小时的负载预测。
我正在使用本教程，建议对模型进行子类化类进行逐步预测。我相信 model.fit 文档也建议子类化。
上面的教程创建了一个名为 Feedback 的 keras.model 子类，并覆盖了 model.call() 方法，该方法在训练和预测期间调用。
def call(自我，输入，训练=无)：
  # 使用 TensorArray 捕获动态展开的输出。
  预测=[]
  # 初始化 LSTM 状态。
  预测，状态 = self.warmup(输入)

  # 插入第一个预测。
  预测.append(预测)

  # 运行其余的预测步骤。
  对于范围内的n（1，self.out_steps）：
    # 使用最后的预测作为输入。
    x = 预测
    # 执行一个lstm步骤。
    x，状态= self.lstm_cell（x，状态=状态，
                              训练=训练）
    # 将 lstm 输出转换为预测。
    预测 = self.dense(x)
    # 将预测添加到输出中。
    预测.append(预测)

  # 预测.shape =&gt; （时间、批次、特征）
  预测 = tf.stack(预测)
  # 预测.shape =&gt; （批次、时间、特征）
  预测 = tf.transpose(预测, [1, 0, 2])
  返回预测


调用 fit() 时，我传入数据集进行训练和验证。通过 keras.utils.timeseries_dataset_from_array() 创建的数据集。
history = model.fit(dataset_train, epochs=epochs,
                    验证数据=数据集_val，
                    回调=[es_callback, modelckpt_callback])

我的数据形状是每小时的时间序列数据、11列天气数据和1列目标。我使用的窗口大小为两个小时。
我的问题是，for 循环中的预测调用似乎仅使用先前的预测作为输入。我不明白他们如何访问训练或验证数据集。
我尝试在 Pycharm 调试器中查找访问数据集的方法，但没有找到任何内容。我也尝试寻找进行类似子类化的人，但本教程是我能找到的最好的教程。
如果需要运行示例，该教程将介绍数据集创建和子类实现。我希望有人可以解释如何正确地对 keras.model 进行子类化（以与该教程类似的方式）以获取多列输入并进行自回归预测。 call() 方法的重写是我最困惑的地方。]]></description>
      <guid>https://stackoverflow.com/questions/76957152/subclassing-keras-model-to-create-a-custom-autoregressive-lstm-model-with-multi</guid>
      <pubDate>Tue, 22 Aug 2023 22:18:42 GMT</pubDate>
    </item>
    <item>
      <title>简单线性 Sigmoid 神经网络不学习</title>
      <link>https://stackoverflow.com/questions/64269084/simple-linear-sigmoid-neural-network-not-learning</link>
      <description><![CDATA[我正在学习 pytorch 并尝试将网络训练为异或门。一切都进行得很顺利，但它就是不学习。它确实改变了它的权重，但它会收敛到每个输入的结果，这远远超出了预期结果。
我尝试过许多学习率和权重初始化。
因此输入是 A 门和 B 门，如果两者相等则应返回 1，否则应返回 0，如下所示：
&lt;前&gt;

    [0,0] =&gt; 1
    [0,1] =&gt; 0
    [1,0] =&gt; 0
    [1,1] =&gt; 1


这是我对模型进行建模和训练的尝试：
&lt;前&gt;

    导入火炬作为火炬
    将 torch.nn 导入为 nn
    
    网络类（nn.Module）：
        
        def __init__(自身):
            超级（网络，自我）.__init__()
            self.x1 = nn.Linear(2,4)
            self.s1 = nn.Sigmoid()
            self.x2 = nn.Linear(4,1)
            self.s2 = nn.Sigmoid()
        
        定义初始化（自身）：
            nn.init.uniform_(self.x1.weight)
            nn.init.uniform_(self.x2.weight)
    
        def前锋（自我，功绩）：
            f1 = torch.tensor(feats).float()
            xr1= 自身.x1(f1)
            xs1= self.s1(xr1)
            xr2= 自身.x2(xs1)
            输出 = self.s2(xr2)
            返回
    
        def 火车（自我，val_expected，feats_next）：
            val_expected_tensor = torch.tensor(val_expected)
            标准 = nn.MSELoss()
            优化器 = torch.optim.SGD(self.parameters(), lr=0.01)
            def 闭包():
                优化器.zero_grad()
                resp = self.forward(feats_next)
                误差 = 标准（分别，val_expected_tensor）
                error.backward()
                返回错误
            优化器.step(闭包)
    
    网络=网络（）
    .net.init()
    
    对于 ([0.,0.],[0.,1.],[1.,0.],[1.,1.]) 中的输入：
        响应=net.forward（输入）
        打印（响应）
    
    打印（“--火车开始-”）
    对于范围（1000）内的 i：
        net.train([1.],[0.,0.])
        net.train([0.],[1.,0.])
        net.train([0.],[0.,1.])
        net.train([1.],[1.,1.])
    print (&quot;---火车结束---&quot;)
    
    对于 ([0.,0.],[0.,1.],[1.,0.],[1.,1.]) 中的输入：
        响应=net.forward（输入）
        打印（响应）


这是一次以 0.001 学习率进行 100000 次迭代的运行：
&lt;前&gt;

    张量([0.7726], grad_fn=)
    张量([0.7954], grad_fn=)
    张量([0.8229], grad_fn=)
    张量([0.8410], grad_fn=)
    --列车启动-
    *.........*........*.........*.........*......... *.........*........*.........*.........*.........
    ---火车结束---
    张量([0.6311], grad_fn=)
    张量([0.6459], grad_fn=)
    张量([0.6770], grad_fn=)
    张量([0.6906], grad_fn=)


我真的迷路了。这不应该起作用吗？]]></description>
      <guid>https://stackoverflow.com/questions/64269084/simple-linear-sigmoid-neural-network-not-learning</guid>
      <pubDate>Thu, 08 Oct 2020 19:04:05 GMT</pubDate>
    </item>
    </channel>
</rss>