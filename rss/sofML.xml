<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Mar 2024 06:16:19 GMT</lastBuildDate>
    <item>
      <title>处理用于骨折检测的 Flask 应用程序中的不相关上传</title>
      <link>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</link>
      <description><![CDATA[我正在开发一个用于骨折检测的 Flask 应用程序，用户可以在其中上传 X 射线图像，该应用程序会预测是否存在骨折。但是，我在处理不相关的上传时遇到问题，例如肘部、手和肩膀以外的身体部位的图像。
以下是我的 Flask 应用程序代码的概述：
Flask 应用代码
导入操作系统
从烧瓶导入烧瓶，请求，jsonify
从 werkzeug.utils 导入 secure_filename
从预测导入预测
从flask_cors导入CORS

应用程序=烧瓶（__名称__）
CORS(app) # 为所有路由启用 CORS
app.config[&#39;UPLOAD_FOLDER&#39;] = &#39;上传&#39;
app.config[&#39;ALLOWED_EXTENSIONS&#39;] = {&#39;png&#39;, &#39;jpg&#39;, &#39;jpeg&#39;}

def allowed_file(文件名):
    返回 &#39;​​。&#39;在 filename 和 filename.rsplit(&#39;.&#39;, 1)[1].lower() 中 app.config[&#39;ALLOWED_EXTENSIONS&#39;]

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def Predict_bone_fracture():
    如果“文件”不在 request.files 中：
        return jsonify({&#39;error&#39;: &#39;没有文件部分&#39;})

    文件 = request.files[&#39;文件&#39;]

    if file.filename == &#39;&#39;:
        return jsonify({&#39;error&#39;: &#39;没有选择文件&#39;})

    如果文件和 allowed_file(file.filename):
        文件名 = secure_filename(文件.文件名)
        filepath = os.path.join(app.config[&#39;UPLOAD_FOLDER&#39;], 文件名)
        文件.保存（文件路径）

        # 使用 Predictions.py 中的预测函数执行预测
        骨骼类型结果 = 预测（文件路径）
        结果=预测（文件路径，bone_type_result）

        # 您可以根据您的要求自定义响应
        返回jsonify（{&#39;bone_type&#39;：bone_type_result，&#39;结果&#39;：结果}）

    return jsonify({&#39;error&#39;: &#39;文件格式无效&#39;})

如果 __name__ == “__main__”：
    应用程序.run()


predict_bone_fracture() 函数接收上传的图像，将其保存到指定文件夹，然后使用外部模块 (predictions.py) 中的 Predict() 函数执行预测。如果上传的文件不是图像或格式不受支持，则返回错误响应。
我主要关心的是如何处理用户上传与指定身体部位（即肘部、手部、肩膀）不对应的图像的情况。例如，如果用户上传眼睛图像而不是骨骼图像，则应用程序应拒绝上传并提供适当的错误消息。
我相信我需要采用一种机制来检测上传图像中的相关身体部位，并验证它们是否与预期的预测身体部位（即肘部、手部、肩膀）相匹配。但是，我不确定实现此目的的最佳方法。
您能否提供有关如何解决此问题的建议或想法？具体来说，我正在寻找以下方面的指导：
实施一种机制来检测上传图像中的相关身体部位。
检查检测到的身体部位是否与预期的预测身体部位相匹配。
为不相关的上传提供适当的错误处理和消息。
任何见解或代码示例将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</guid>
      <pubDate>Sat, 30 Mar 2024 05:41:52 GMT</pubDate>
    </item>
    <item>
      <title>在进行二值图像分类时，设置为二值的类模式错误地标记了图像，但它在分类上是否正确</title>
      <link>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</link>
      <description><![CDATA[我目前正在研究二值图像分类。我的问题是，当我使用数据增强时，当它设置为二进制时，它会错误地标记图像。
我尝试过的事情：

在扩充我的数据之前，在图像上查找错误标记的类。不过这并没有什么问题。

将课程模式更改为分类模式。这是可行的，但是我不从事多类分类。

尝试使用图像数据生成器。没有任何效果。

寻找分类不平衡的情况。也没什么问题。


我还可以尝试什么？我应该在生成器上使用分类类模式吗？]]></description>
      <guid>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</guid>
      <pubDate>Sat, 30 Mar 2024 03:44:32 GMT</pubDate>
    </item>
    <item>
      <title>计算explained_variance_score，手动方法和函数调用结果不同</title>
      <link>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</link>
      <description><![CDATA[根据官方页面的公式
https://scikit-learn.org/stable/modules/ model_evaluation.html#explained-variance-score，计算数据集的以下 EVS：
y_true = [1, 2, 3, 4, 5] y_pred = [6, 7, 8, 9, 10]
手动：evs = 1 - var(y_true - y_pred)/var(y_true) = -11.5
使用代码：evs = 1
从 sklearn.metrics 导入解释_方差_分数

y_true = [1, 2, 3, 4, 5]
y_pred = [6, 7, 8, 9, 10]

解释的方差 = 解释的方差_分数(y_true, y_pred)

为什么结果不同？]]></description>
      <guid>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</guid>
      <pubDate>Sat, 30 Mar 2024 03:26:49 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn 排列和更新 Polars DataFrame</title>
      <link>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</link>
      <description><![CDATA[我正在尝试重写scikit-learn 排列重要性要实现的：

与 Polar 的兼容性
与功能集群的兼容性

将极坐标导入为 pl
将 Polars.selectors 导入为 cs
将 numpy 导入为 np

从 sklearn.datasets 导入 make_classification
从 sklearn.model_selection 导入 train_test_split

X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
feature_names = [f&quot;feature_{i}&quot;;对于范围内的 i(X.shape[1])]

X_train_polars = pl.DataFrame(X_train, schema=feature_names)
X_test_polars = pl.DataFrame(X_test, schema=feature_names)
y_train_polars = pl.Series(y_train, schema=[“目标”])
y_test_polars = pl.Series(y_test, schema=[“目标”])

为了获得一组特征的未来重要性，我们需要同时排列一组特征，然后传递给评分器以与基线分数进行比较。
但是，在检查特征簇时，我正在努力替换多个极坐标数据框列：
from sklearn.utils import check_random_state
随机状态=检查随机状态(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

X_train_permuted = X_train_polars.clone()
shuffle_arr = np.array(X_train_permuted[:, [“feature_0”, “feature_1”]])

random_state.shuffle(shuffle_arr)
X_train_permuted.replace_column( # 这个操作到位
                0,
                pl.Series(name=“feature_0”,values=shuffle_arr))

通常，shuffle_arr 的形状为 (n_samples,)，可以使用 polars.DataFrame.replace_column() 轻松替换 Polars 数据帧中的相关列。在这种情况下，shuffle_arr 的多维形状为（簇中的 n_samples，n_features）。替换相关列的有效方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</guid>
      <pubDate>Sat, 30 Mar 2024 03:20:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络距离与性能之间的关系</title>
      <link>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</link>
      <description><![CDATA[我一直想知道“距离”与“距离”之间是否存在相关性。神经网络权重及其性能之间的关系。
为了详细说明，请考虑以下场景：
我们有三个模型：M1、M2 和 M3，它们都具有相同的结构，每个模型都在其各自的数据集 D1、D2 和 D3 上进行训练。训练后，让我们说一下“距离”。 M1 和 M2 之间的距离是 5，而 M1 和 M3 之间的距离是 20。本质上，M1 和 M2 “空间上”更接近。
我想说的是，如果我们在 D2 和 D3 上评估 M1，它在 D2 上的性能应该更高，因为 M1 更接近 M2，并且 M2 是在 D2 上训练的。然而，一些实验与这一假设相矛盾。
我故意将“距离”括起来用引号引起来，因为我不确定在这种情况下采用的适当指标。
我找到了一些关于该主题的论文，但它们似乎不能满足我的需求。
谁能帮助我更好地理解“距离”和“距离”之间是否存在关系？和性能？
非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</guid>
      <pubDate>Sat, 30 Mar 2024 01:27:55 GMT</pubDate>
    </item>
    <item>
      <title>jupyter笔记本中的tensorflow导入错误（ import tensorflow_io as tfio ）[关闭]</title>
      <link>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</link>
      <description><![CDATA[导入tensorflow_io as tfio 我在音频分类器深度学习项目的代码中遇到错误，有人可以帮助我吗？
来自nicholes renotte yt频道的项目，jupyter笔记本中的tensor_io代码存在问题。]]></description>
      <guid>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</guid>
      <pubDate>Fri, 29 Mar 2024 22:24:20 GMT</pubDate>
    </item>
    <item>
      <title>2类的组合</title>
      <link>https://stackoverflow.com/questions/78246119/combination-of-2-classes</link>
      <description><![CDATA[创建单独的数据生成器以进行训练和验证
train_data = data_generator.flow_from_directory(
火车路径，
目标大小=(img_size,img_size),
批量大小=批量大小_训练，
class_mode=&#39;分类&#39;,
类=[&#39;AKIEC et BCC&#39;,&#39;VASC et DF&#39;,&#39;MEL &amp; NV&amp; BKL&#39;]
）
找到属于 3 个类别的 0 张图片。]]></description>
      <guid>https://stackoverflow.com/questions/78246119/combination-of-2-classes</guid>
      <pubDate>Fri, 29 Mar 2024 22:04:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在Tensorflow中转换为对数梅尔谱图？</title>
      <link>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</link>
      <description><![CDATA[我尝试修改我的预处理函数来创建 log-mel-spectrogram，以便我的 CNN 模型可以在其上进行训练。
我尝试将我的频谱图转换为对数梅尔频谱图，但我无法做到这一点。但是，由于我使用的是tensorflow，所以这个转换过程需要使用tensorflow框架。
def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    
    返回频谱图、标签
]]></description>
      <guid>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 21:10:07 GMT</pubDate>
    </item>
    <item>
      <title>了解 PyTorch 模型中的批处理</title>
      <link>https://stackoverflow.com/questions/78245568/understanding-batching-in-pytorch-models</link>
      <description><![CDATA[我有以下模型，它构成了我的整个模型管道中的步骤之一：
导入火炬
将 torch.nn 导入为 nn

类 NPB(nn.Module):
    def __init__(self, d, nhead, num_layers, dropout=0.1):
        超级（NPB，自我）.__init__()
            
        self.te = nn.TransformerEncoder(
            nn.TransformerEncoderLayer（d_model = d，nhead = nhead，dropout = dropout，batch_first = True），
            层数=层数，
        ）

        self.t_emb = nn.Parameter(torch.randn(1, d))
        
        self.L = nn.Parameter(torch.randn(1, d))

        self.td = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            层数=层数，
        ）

        self.ffn = nn.Linear(d, 6)
    
    def 向前（自身，t_v，t_i）：
        打印（“--------------- t_v，t_i -----------------”）
        打印（&#39;t_v：&#39;，元组（t_v.shape））
        print(&#39;t_i: &#39;, 元组(t_i.shape))

        打印（“--------------- t_v + t_i + t_emb -----------------”）
        _x = t_v + t_i + self.t_emb
        打印（元组（_x.shape））

        print(&quot;---------------- 特 --------------&quot;)
        _x = self.te(_x)
        打印（元组（_x.shape））
        
        print(&quot;---------------- td ---------------&quot;)
        _x = self.td(self.L, _x)
        打印（元组（_x.shape））

        print(&quot;---------------- ffn --------------&quot;)
        _x = self.ffn(_x)
        打印（元组（_x.shape））

        返回_x

这里 t_v 和 t_i 是来自早期编码器块的输入。我将它们作为 (4,256) 的形状传递，其中 256 是特征数量，4 是批量大小。 t_emb 是时间嵌入。 L 表示学习矩阵，表示查询的嵌入。我使用以下代码测试了该模块块：
t_v = torch.randn((4,256))
t_i = torch.randn((4,256))
npb = NPB(d=256, nhead=8, num_layers=2)
npb（t_v，t_i）

输出：
&lt;前&gt;&lt;代码&gt;================ NPB ===============
--------------- t_v, t_i -----------------
电视: (4, 256)
t_i: (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- 特 ---------------
(4, 256)
--------------- TD ---------------
(1, 256)
--------------- ffn ---------------
(1, 6)

我期望输出的形状应为 (4,6)，大小为 6 的批次中每个样本有 6 个值。但输出的大小为(1,6)。经过大量调整后，我尝试将 t_emb 和 L 形状从 (1,d) 更改为 (4,d)&lt; /code&gt;，因为我不希望所有采样共享这些变量（通过广播：
self.t_emb = nn.Parameter(torch.randn(4, d)) # [n, d] = [4, 256]
self.L = nn.Parameter(torch.randn(4, d))

这给出了所需的形状输出（4,6：
&lt;前&gt;&lt;代码&gt;---------------------------- t_v, t_i -----------------
电视: (4, 256)
t_i: (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- 特 ---------------
(4, 256)
--------------- TD ---------------
(4, 256)
--------------- ffn ---------------
(4, 6)

我有以下疑问：
Q1. 到底为什么将 L 和 t_emb 形状从 (1,d) 更改为  (4,d) 有效吗？为什么它不能通过广播与(1,d)一起工作？
Q2.我是否以正确的方式进行批处理，或者输出是人为正确的，而在幕后它所做的事情与我预期的不同（预测大小为 4 的批次中每个样本的 6 个值）？&lt; /p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/78245568/understanding-batching-in-pytorch-models</guid>
      <pubDate>Fri, 29 Mar 2024 19:24:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在 lambda 函数中对时间戳和输出列表进行加法和减法？</title>
      <link>https://stackoverflow.com/questions/78245499/how-can-i-do-addition-and-substraction-of-timestamp-and-a-list-of-output-in-a-la</link>
      <description><![CDATA[RFM = sales_data.groupby([&#39;CLIENT_ID&#39;]).agg({
    &#39;CLIENT_ID&#39;: lambda x: (last_purchase_date - x.max()).days,
    &#39;Transaction_ID&#39;: &#39;计数&#39;,
    &#39;网络&#39;：&#39;总和&#39;
})

RFM.rename(columns={&#39;CLIENT_ID&#39;: &#39;Recency&#39;, &#39;Transaction_ID&#39;: &#39;Frequency&#39;, &#39;NET&#39;: &#39;MonetaryValue&#39;}, inplace= True)
显示(RFM)

我不知道如何修复我已经转换的这些东西，但它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78245499/how-can-i-do-addition-and-substraction-of-timestamp-and-a-list-of-output-in-a-la</guid>
      <pubDate>Fri, 29 Mar 2024 19:08:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中模拟 Microsoft Excel 的求解器功能（GRG 非线性）？</title>
      <link>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</link>
      <description><![CDATA[演示 Excel 求解器使用的屏幕截图：

我的任务是自动化某个 Excel 工作表。该工作表恰好使用名为 Solver 的 Excel 插件实现了逻辑。它使用单元格 $O$9 中的单个值 (-1.95624)（这是图中用红色和蓝色墨水突出显示的计算结果）作为输入值，然后使用名为的算法返回 C、B1 和 B2 的三个值“GRG非线性回归”。我的任务是用 Python 模拟这个逻辑。以下是我的尝试。主要问题是我没有得到与 Excel 的 Solver 插件计算出的 C、B1 和 B2 相同的值。
导入 numpy、scipy、matplotlib
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 scipy.optimize 导入 curve_fit
从 scipy.optimize 导入 Differential_evolution
进口警告

xData = numpy.array([-2.59772914040242,-2.28665528866907,-2.29176070881848,-2.31163972446061,-2.28369414349715,-2.27911303233721,-2.282 22332344644,-2.39089535619106,-2.32144325648778,-2.17235002006179,-2.22906032068685,-2.42044014499938,-2.71639505549322,-2.65 462061336346,- 2.47330475191616,-2.33132910807216,-2.33025978869114,-2.61175064230516,-2.92916553244925,-2.987503044973,-3.00367414706232, -1.45507812104723]) # 使用与参数相同的表名
yData = numpy.array([0.0692847120775066,0.0922342111029099,0.0918076382491768,0.0901635409944003,0.0924824386284127,0.092867647175396, 0.092605957740688,20.0838696111204451,0.0893625419994501,0.102261091024881,0.097171046758256,70.0816272542472914,0.0620128251 290935,0.0657047909578125,0.0777509345715382,0.088561321341585,0.088647672874835,90.0683859871424735,0.0507304952495273,0.047 9936476914665,0.0472601632188253,0.18922126828463 ]) # 使用与参数相同的表名

def func(x, a, b, Offset): # 带偏移量的 Sigmoid A 来自 zunzun.com
    返回 1.0 / (1.0 + numpy.exp(-a * (x-b))) + 偏移量


# 遗传算法最小化（误差平方和）的函数
def sumOfSquaredError(parameterTuple):
    warnings.filterwarnings(“ignore”) # 不通过遗传算法打印警告
    val = func(xData, *parameterTuple)
    返回 numpy.sum((yData - val) ** 2.0)


defgenerate_Initial_Parameters():
    # 用于边界的最小值和最大值
    maxX = max(x数据)
    minX = min(x数据)
    maxY = max(y数据)
    minY = min(yData)

    参数范围 = []
    parameterBounds.append([minX, maxX]) # 的搜索范围
    parameterBounds.append([minX, maxX]) # b 的搜索范围
    parameterBounds.append([0.0, maxY]) # Offset 的搜索范围

    #“种子”用于可重复结果的 numpy 随机数生成器
    结果 = Differential_evolution(sumOfSquaredError,parameterBounds,seed=3)
    返回结果.x

# 生成初始参数值
遗传参数=generate_Initial_Parameters()

# 曲线拟合测试数据
参数，协方差 = curve_fit（func，xData，yData，遗传参数，maxfev = 50000）

# 将参数转换为Python内置类型
params = [float(param) for param in params] # 将 numpy float64 转换为 Python float
C、B1、B2 = 参数
OutputDataSet = pd.DataFrame({“C”：[C]，“B1”：[B1]，“B2”：[B2]，“ProType”：[input_value_1]，“RegType”：[input_value_2 ]})


有什么想法会有帮助吗？提前致谢
这是我的尝试：
鉴于 xData 和 yData 的这些数据集，正确的输出应该是：
C= -2.35443383，B1 = -14.70820051，B2 = 0.0056217]]></description>
      <guid>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</guid>
      <pubDate>Fri, 29 Mar 2024 15:08:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中对多个类进行分类</title>
      <link>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</link>
      <description><![CDATA[我已经关注了 youtube 上的教程，该教程向我展示了如何对 2 个数据集进行分类（咳嗽，不是咳嗽），但现在我需要添加一个额外的类，即打喷嚏，因此需要训练 3 个类上（咳嗽，打喷嚏，其他），我不知道该怎么做。请帮忙！！！
在代码中，模型在 2 个类别（咳嗽、not_cough）上进行训练并且表现相当不错，但我无法让它在多个类别（例如咳嗽、打喷嚏、其他）上工作。
导入操作系统
从 matplotlib 导入 pyplot 作为 plt
将张量流导入为 tf
将tensorflow_io导入为tfio
从tensorflow.keras.models导入顺序，load_model
从tensorflow.keras.layers导入Conv2D、Dense、Flatten、MaxPool2D、Dropout、TimeDistributed、Reshape
从tensorflow.keras.optimizers.legacy导入Adam
从 keras 导入层
从 keras.utils 导入到_categorical

def load_wav_16k_mono(文件名):
    # 加载编码后的wav文件
    file_contents = tf.io.read_file(文件名)
    # 解码 wav（按通道的张量）
    wav，sample_rate = tf.audio.decode_wav（文件内容，desired_channels = 1）
    # 删除尾随轴
    wav = tf.squeeze(wav, 轴=-1)
    样本率 = tf.cast(样本率，dtype=tf.int64)
    # 从 44100Hz 到 16000Hz - 音频信号的幅度
    wav = tfio.audio.resample(wav,rate_in=sample_rate,rate_out=16000)
    返回波形

def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    返回频谱图、标签


def get_CNN(input_shape):
    模型=顺序（）
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=input_shape))
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;))
    model.add(MaxPool2D((2,2)))
    模型.add(压平())
    model.add（密集（128，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;softmax&#39;））
    
    model.compile(&#39;Adam&#39;, loss=&#39;BinaryCrossentropy&#39;, 指标=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),&#39;accuracy&#39;])
    model.summary() # 删除一些最大池层以减少参数
    返回模型
    

def main():
    POS_COUGH = “./data/咳嗽”
    NEG_COUGH =“./data/not_cough”
  
    #POS_SPEECH =“./数据/语音”

    pos_cough = tf.data.Dataset.list_files(POS_COUGH+&#39;\*.wav&#39;)
    neg_cough = tf.data.Dataset.list_files(NEG_COUGH+&#39;\*.wav&#39;)
    
    #pos_speech = tf.data.Dataset.list_files(POS_SPEECH +&#39;\*.wav&#39;)

    咳嗽标签 = tf.data.Dataset.from_tensor_slices(tf.ones(len(pos_cough)))
    
    not_cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(neg_cough)))
    
    # 添加标签并合并正负样本
    咳嗽 = tf.data.Dataset.zip((pos_cough, 咳嗽_标签))
    
    not_cough = tf.data.Dataset.zip((neg_cough, not_cough_labels))
   
    阴性= not_cough
    阳性=咳嗽
    # 连接两个相同的元素
    数据=正数.连接（负数）

    ### 2. 创建 Tensorflow 数据管道
    数据 = data.map(预处理)
    数据 = data.cache()
    数据 = data.shuffle(buffer_size=1000)
    数据 = 数据.batch(16)
    数据 = 数据.预取(8)
    
    ## 3. 将数据拆分为训练数据和测试数据
    火车 = data.take(int(len(数据) * 0.7))
    test = data.skip(int(len(data) * 0.7)).take(int(len(data) - len(data) * 0.7)) #test.as_numpy_iterator().next()

    输入形状频谱图 = (396, 65,1)
    模型 = get_CNN(input_shape_spectrogram)
    hist = model.fit(train, epochs=2,validation_data=test)
]]></description>
      <guid>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 11:24:58 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”井字棋电脑需要多少场比赛才能训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。它在对抗随机走棋时效果很好（我刚刚根据约 10,000 场比赛生成的数据再次运行了 5353 场比赛，它赢得了 5353 场比赛中的 4757 场），但它仍然经常输给我。
以下是完美答案应解决的一些问题：

需要玩多少场游戏才能让“火柴盒电脑”与 Michie 设计的电脑完全一样，才能完美地开始玩游戏？

带有实际火柴盒的原始计算机是否达到了完美状态
玩吗？

如果仅与计算机进行训练，计算机能否达到完美的发挥
随机移动？


编辑：
这个问题并不是寻求代码方面的帮助，但下面的评论表明包含代码可能会有所帮助。以下是我创建的 GitHub 存储库的链接，以便我可以在此处共享：
https://github.com/ACertainArchangel/ Recreation-Of-MENACE-Tic-Tac-Toe..git
抱歉，我知道这不太好并且不遵守约定；我只写了几个月的代码:)]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch安装</title>
      <link>https://stackoverflow.com/questions/77478747/pytorch-installation</link>
      <description><![CDATA[我正在尝试在 python 3.12.0 中安装 Pytorch 并在 Windows 11 中安装 cuda 12.1 ？但我收到错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版
我也安装了 nvidia cuda 12.1
我尝试使用 Pytorch 网站安装 Pytorch，但它不起作用并且给我错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版]]></description>
      <guid>https://stackoverflow.com/questions/77478747/pytorch-installation</guid>
      <pubDate>Tue, 14 Nov 2023 07:14:28 GMT</pubDate>
    </item>
    <item>
      <title>将 Detectron2 模型转换为 torchscript</title>
      <link>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</link>
      <description><![CDATA[我想将 detectorron2 &#39;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml 模型&#39; 转换为 torchscript。
我用过托克
我的代码如下。
&lt;前&gt;&lt;代码&gt;导入cv2

将 numpy 导入为 np

进口火炬
从 detector2 导入 model_zoo
从 detector2.config 导入 get_cfg
从 detectorron2.engine 导入 DefaultPredictor
从 detector2.modeling 导入 build_model
从 detectorron2.export.flatten 导入 TracingAdapter
导入操作系统

ModelPath=&#39;/home/jayasanka/working_files/create_torchsript/model.pt&#39;
将 open(&#39;savepic.npy&#39;, &#39;rb&#39;) 作为 f：
    图像 = np.load(f)

#------------------------------------------------- ------------------------------------------------

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(“COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml”))

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # 你的类数 + 1

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, ModelPath)

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.60 # 设置该模型的测试阈值

预测器 = DefaultPredictor(cfg)



我使用了 TracingAdapter 和跟踪函数。我不太了解其背后的概念是什么。
&lt;前&gt;&lt;代码&gt;# im = cv2.imread(图像)
im = torch.tensor(图像)

def inference_func（模型，图像）：
    输入= [{“图像”：图像}]
    返回 model.inference(inputs, do_postprocess=False)[0]

包装器= TracingAdapter（预测器，im，inference_func）
包装器.eval()
Traced_script_module= torch.jit.trace（包装器，（im，））
traced_script_module.save(“torchscript.pt”)

它给出了下面给出的错误。
回溯（最近一次调用最后一次）：
  文件“script.py”，第 49 行，位于  中。
    Traced_script_module= torch.jit.trace（包装器，（im，））
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 744 行，跟踪中
    _模块_类，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 959 行，在trace_module 中
    参数名称，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1051 行，在 _call_impl 中
    返回forward_call（*输入，**kwargs）
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1039 行，位于 _slow_forward
    结果 = self.forward(*输入, **kwargs)
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/detectron2/export/flatten.py”，第 294 行，向前
    输出 = self.inference_func(self.model, *inputs_orig_format)
  文件“script.py”，第 44 行，inference_func
    返回 model.inference(inputs, do_postprocess=False)[0]
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/yacs/config.py”，第 141 行，在 __getattr__ 中
    引发属性错误（名称）
属性错误：推理


你能帮我解决这个问题吗？
还有其他方法可以轻松做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</guid>
      <pubDate>Tue, 06 Sep 2022 08:50:15 GMT</pubDate>
    </item>
    </channel>
</rss>