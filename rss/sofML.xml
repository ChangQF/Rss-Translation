<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 03 Oct 2024 01:16:04 GMT</lastBuildDate>
    <item>
      <title>使用分层数据的无监督学习</title>
      <link>https://stackoverflow.com/questions/79048804/unsupervised-learning-with-hierarchical-data</link>
      <description><![CDATA[首先，请原谅我对这个话题的无知。但是，除了简单地输入数据（比如说一组元素 A）并对 A 中的元素进行聚类/分析之外，是否可以在一组集合之间比较和聚类元素。例如，输入集合 A 后，输入集合 B、集合 C、D 等，然后找出元素 B3 是否与元素 A2 相似，或者元素 C6 是否与 D7 相似。
让我用两个例子来说明。
示例 1：比较一群运动员的统计数据，看看在个人层面上哪些运动员彼此最相似。
示例 2：比较一群团队，看看这些团队中的哪些运动员在团队中扮演着类似的角色。就像运动员 A 与 X 团队一样，运动员 B 与 Y 团队一样。如果运动员 A 传球次数更多，并且与 X 队的其他球员相比覆盖范围更大，那么他很可能是一名组织核心。Z 队的 B 也是如此。
因此，您要根据这些元素对各自集合的“意义”对不同集合中的单个元素进行聚类。]]></description>
      <guid>https://stackoverflow.com/questions/79048804/unsupervised-learning-with-hierarchical-data</guid>
      <pubDate>Thu, 03 Oct 2024 00:22:03 GMT</pubDate>
    </item>
    <item>
      <title>C# 中的 LSTM 模型预测问题 - 警告：未编译的模型指标</title>
      <link>https://stackoverflow.com/questions/79048630/issue-with-lstm-model-prediction-in-c-sharp-warning-uncompiled-model-metrics</link>
      <description><![CDATA[我使用以下代码预测 LSTM 模型：
import os

os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39; 
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;1&#39; 

import sys
import numpy as np
import warnings
from tensorflow.keras.models import load_model
import joblib

warnings.filterwarnings(&quot;ignore&quot;, category=UserWarning, message=&quot;Compiled the loaded model, but the compilation metrics have yet to be build.&quot;)

current_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(current_dir, &#39;lstm_model.h5&#39;)
scaler_path = os.path.join(current_dir, &#39;minmax_scaler.pkl&#39;)

def mse(y_true, y_pred):
return np.mean(np.square(y_true - y_pred))

def predict_lstm(input_sequence):
# 加载经过训练的 LSTM 模型
model = load_model(model_path, custom_objects={&#39;mse&#39;: mse})

# 加载拟合的缩放器
scaler = joblib.load(scaler_path)

input_sequence_scaled = scaler.transform(np.array(input_sequence).reshape(-1, 1))

input_sequence_scaled = input_sequence_scaled.reshape(1, len(input_sequence_scaled), 1)

prediction_scaled = model.predict(input_sequence_scaled)

prediction = scaler.inverse_transform(prediction_scaled)

返回 prediction[0][0]

如果 __name__ == &quot;__main__&quot;:
input_sequence = [float(arg) for arg in sys.argv[1:]]

如果 len(input_sequence) &lt; 10:
print(&quot;错误：需要至少 10 个过去的时间戳才能进行 LSTM 预测。&quot;)
else:
try:
predict_value = predict_lstm(input_sequence)
print(f&quot;预测值：{predicted_value:.6f}&quot;)
except Exception as e:
print(f&quot;发生错误：{str(e)}&quot;)

我可以使用以下命令在终端中成功运行预测脚本（我得到 112）：
python predict_lstm.py 10 20 30 40 50 60 70 80 90 100

但是，当我尝试使用 Swagger 通过 C# 控制器调用此脚本时，我收到以下警告：
错误：WARNING:absl:Compiled已加载模型，但编译的指标尚未构建。在训练或评估模型之前，model.compile_metrics 将为空。

这是我的 C# 控制器：
[HttpGet(&quot;lstm&quot;)]
public IActionResult PredictLstm([FromQuery] LstmInputModel model)
{
if (model == null || model.Timestamps == null || model.Timestamps.Length == 0)
{
return BadRequest(&quot;Invalid input. An array of timestamps is required.&quot;);
}

string scriptPath = &quot;Solutions/predict_lstm.py&quot;;
string result = RunPythonScript(scriptPath, model.Timestamps);

if (!string.IsNullOrEmpty(result))
{
return Ok(result);
}
else
{
return BadRequest(&quot;执行预测脚本时出错。&quot;);
}
}

private string RunPythonScript(string scriptPath, double[] features)
{
try
{
string featuresString = string.Join(&quot; &quot;, features.Select(f =&gt; f.ToString()));
ProcessStartInfo psi = new ProcessStartInfo
{
FileName = &quot;python&quot;,
Arguments = $&quot;\&quot;{scriptPath}\&quot; {featuresString}&quot;,
RedirectStandardOutput = true,
RedirectStandardError = true,
UseShellExecute = false,
CreateNoWindow = true
};

using (Process process = Process.Start(psi))
{
process.WaitForExit();
string output = process.StandardOutput.ReadToEnd();
string errors = process.StandardError.ReadToEnd();
返回 !string.IsNullOrEmpty(errors) ? $&quot;Error: {errors}&quot; : output;
}
}
catch (System.Exception ex)
{
返回 $&quot;Exception occurred: {ex.Message}&quot;;
}
}

什么原因可能导致此问题？我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79048630/issue-with-lstm-model-prediction-in-c-sharp-warning-uncompiled-model-metrics</guid>
      <pubDate>Wed, 02 Oct 2024 22:25:42 GMT</pubDate>
    </item>
    <item>
      <title>R 中 cox 模型的 c 指数</title>
      <link>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</link>
      <description><![CDATA[我最近在机器学习中为 Cox 模型运行了我的 R 代码。我已经在模型中进行了测试和训练。我想运行 c 索引一次进行测试，一次进行训练。我有代码，但根据 Cox 的 AUC，值太小了。我该如何解决这个问题？非常感谢
这些是我的代码：
 train_indices1 &lt;- sample(1:nrow(df6), 0.8 * nrow(df6))

training_data1 &lt;- df6[train_indices1, ]

testing_data1 &lt;- df6[-train_indices1, ]

res.cox11 &lt;- coxph(Surv(times,eventHFF) ~ AgeCategori + Gender + shoghl+ education + sokonat +taahol + BMIcategori+Hypertension+ DiabetesMellitus+ CAD +

HyperLipidemia+Sm​​oking+CKDDialysis +AtrialFibrillation+ StrokeTIA+ CTD+ ChemotherapyRadiotherapy +恶性肿瘤+急性心衰类型+HF类型+SBP+DBP+脾气猫+

心率猫+SPO2+NYHA等级+AF需要治疗+急性透析超滤+WRF，数据 = training_data1)

summary(res.cox11)

predicted_status1 &lt;- predict(res.cox11, newdata = testing_data1, type = &quot;risk&quot;)

predicted_status_binary1&lt;- ifelse(predicted_status1 &gt; 0.5, 1, 0)

confusion1 &lt;-fusionMatrix(factor(predicted_status_binary1), factor(testing_data1$eventHFF))

print(confusion1)

COX_MODEL1 &lt;- roc(testing_data1$eventHFF ~ predict_status1, plot = TRUE，print.auc = TRUE，main = &quot;ROC - COX 模型&quot;)

train_cindex &lt;- concordance(Surv(training_data1$times, training_data1$eventHFF) ~ predict(res.cox11, newdata = training_data1))$concordance

test_cindex &lt;- concordance(Surv(testing_data1$times, testing_data1$eventHFF) ~ predict(res.cox11, newdata = testing_data1))$concordance

cat(&quot;C-index for training data:&quot;, train_cindex, &quot;\n&quot;)

cat(&quot;C-index for testing data:&quot;, test_cindex, &quot;\n&quot;)

我该如何解决这个问题？
&gt; ci1 &lt;- ci.auc(COX_MODEL1)
95% CI: 0.5169-0.7843 (DeLong)
&gt; cat(&quot;训练数据的 C 指数 (重新拟合)：&quot;, train_cindex_refit, &quot;\n&quot;)
训练数据的 C 指数 (重新拟合)：0.3102263 
&gt; cat(&quot;测试数据的 C 指数 (重新拟合)：&quot;, test_cindex_refit, &quot;\n&quot;)
测试数据的 C 指数 (重新拟合)：0.3152355 
]]></description>
      <guid>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</guid>
      <pubDate>Wed, 02 Oct 2024 21:06:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Tesseract OCR 读取俄罗斯车牌？</title>
      <link>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</link>
      <description><![CDATA[我正在研究读取俄罗斯车辆的车牌。
Tesseract OCR 无法正确检测车牌。
我尝试了很多设置，例如（噪声过滤器、内核更改）

尝试了不同的内核，拉普拉斯、自定义、高斯
尝试了旋转、降低噪音
应用了一组字母/数字，但算法上将 2 检测为 Z

有什么建议可以尝试吗？
import cv2
import pytesseract
import numpy as np
from google.colab.patches import cv2_imshow

# 可选：如果 Tesseract 可执行文件路径尚未位于系统 PATH 中，请设置该路径
# pytesseract.pytesseract.tesseract_cmd = r&#39;C:\Program Files\Tesseract-OCR\tesseract.exe&#39;

# 函数用于围绕图像中心旋转图像
def rotate_image(image, angle):
(h, w) = image.shape[:2]
center = (w // 2, h // 2)
M = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated = cv2.warpAffine(image, M, (w, h))
return rotated

# 函数用于根据图像矩计算倾斜角度
def compute_skew_angle(image):
coords = np.column_stack(np.where(image &gt; 0))
angle = cv2.minAreaRect(coords)[-1]
if angle &lt; -45:
angle = -(90 + angle)
else:
angle = -angle
return angle

# 加载 Haar 级联以检测车牌
plate_cascade = cv2.CascadeClassifier( &#39;/content/haarcascade_russian_plate_number.xml&#39;)

# 加载图像
img = cv2.imread(&#39;/content/vehicle1.png&#39;)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
cv2_imshow(gray)

# 检测车牌
plates = plate_cascade.detectMultiScale(gray, 1.1, 4)

for (x, y, w, h) in boards:
# 在检测到的车牌周围绘制一个矩形
cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)

# 提取车牌区域
plate_img = gray[y:y + h, x:x + w]
cv2_imshow(plate_img)

# 将车牌图像调整为标准尺寸（例如 400x100）
scaled_plate = cv2.resize(plate_img, (4000, 1000))
cv2_imshow(scaled_plate)
# 应用高斯模糊以降低噪音
denoised_plate = cv2.GaussianBlur(scaled_plate, (5, 5), 0)
cv2_imshow(denoised_plate)
# 应用自适应阈值以获得更好的 OCR 性能
#adaptive_thresh_plate = cv2.adaptiveThreshold(denoised_plate, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
# cv2.THRESH_BINARY, 11, 2)
#cv2_imshow(adaptive_thresh_plate)
# 计算车牌倾斜角度
#skew_angle = compute_skew_angle(adaptive_thresh_plate)
#skew_angle = compute_skew_angle(denoised_plate)
# 旋转图像以校正倾斜
#rotated_plate = rotate_image(denoised_plate, skew_angle)
#cv2_imshow(rotated_plate)

# 对旋转和处理后的车牌执行 OCR
#plate_text = pytesseract.image_to_string(denoised_plate, config=&#39;--psm 6&#39;)
#enchancedimage 
kernel = cv2.getGaussianKernel(ksize=5, sigma=1.5) 
kernel = kernel @ kernel.T

enchanced_img = cv2.filter2D(denoised_plate, -1, kernel)
cv2_imshow(enchanced_img)
rotated_plate = rotate_image(denoised_plate, -2.5)
cv2_imshow(rotated_plate)
for i in range(3,14):
print(f&#39;PSM: {i}&#39;)
#plate_text =pytesseract.image_to_string(enchanced_img, 
#config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)
plate_text =pytesseract.image_to_string(rotated_plate, 
config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)

#plate_text =pytesseract.image_to_string(enchanced_img,config = f&#39;--psm {i}&#39;)
print(&quot;检测到的车牌号：&quot;, plate_text)

# 显示带检测框的原始图像
#cv2_imshow(img)
cv2.waitKey(0)
cv2.destroyAllWindows()


]]></description>
      <guid>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</guid>
      <pubDate>Wed, 02 Oct 2024 20:30:40 GMT</pubDate>
    </item>
    <item>
      <title>在使用 TensorFlow 计算 SINR 时遇到问题，当涉及到复数时，TF 的数值稳定性如何？</title>
      <link>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerical-stable-is-tf</link>
      <description><![CDATA[因此，我有用于训练神经网络的代码，对于具有多组 (G) 和多用户 (K) 的系统，损失为负 SINR。然而，当我计算 sinr 时，发生了一些奇怪的事情。
所以SINR公式是这样的：

这是我的代码：
def find_sinr_over_group(H, W):

    西格玛2 = 1
    # 计算 W 的 Hermitian 转置（共轭转置）
    W_H = tf.transpose(tf.math.conj(W), perm=[0, 2, 1]) # 形状：(batch_size, G, M)

    # 计算每组中每个用户的信号功率
    信号功率 = []
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h_g = W_H[:, g, :] # 形状: (batch_size, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算信号功率
        s = tf.matmul(w_h_g[:, tf.newaxis, :], h_g) # 形状: (batch_size, 1, K)
        s = tf.squeeze(s, axis=1) # 形状: (batch_size, K)
        signal_power.append(s)

    signal_power = tf.stack(signal_power, axis=-1) # 形状：(batch_size, K, G)
    signal_power = tf.math.real(tf.math.multiply(signal_power, tf.math.conj(signal_power)))
    # signal_power = tf.math.abs(signal_power) ** 2 # 取绝对平方

    # 计算每组中每个用户的总功率
    总功率=[]
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h = W_H[:, :, :] # 形状：(batch_size, G, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算总功率
        t = tf.matmul(w_h, h_g) # 形状: (batch_size, G, K)
        t = tf.math.real(tf.math.multiply(t, tf.math.conj(t)))
        # t = tf.math.abs(t) ** 2 # 形状：(batch_size, G, K)
        Total_power.append(tf.reduce_sum(t, axis=1)) # 对 G 求和得到 (batch_size, K)

    Total_power = tf.stack(total_power, axis=-1) # 形状：(batch_size, K, G)

    # 通过从总功率中减去信号功率来隔离干扰功率
    干扰功率 = 总功率 - 信号功率 # 形状：(batch_size, K, G)

    #添加噪声功率
    Interference_plus_noise_power = Interference_power + sigma2 # 添加噪声

    # 计算SINR
    sinr = signal_power / Interference_plus_noise_power # 形状：(batch_size, K, G)

    返回正弦值

但问题是，在某些情况下干扰功率是负的。这在数字上是不可能的，因为信号功率只是总功率的一个特例。有人知道为什么会发生这种情况以及我应该如何解决它吗？
正如你所看到的，一开始我使用了 tf.abd 函数，然后以 2 的幂进行提升。我认为这可能是 abs 的问题，所以我尝试通过共轭进行 myltiping 来获取信号的功率。但我仍然有这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerical-stable-is-tf</guid>
      <pubDate>Wed, 02 Oct 2024 20:23:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 opencv.dnn.blobFromImage() 输出转换回 rgb 图像后包含 9 张灰度图像？</title>
      <link>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</link>
      <description><![CDATA[大家好，我想问你们一个问题。
据我所知，blobFromImage 将 img 形状：（宽度、高度、通道）转换为 4d 数组（n、通道、宽度、高度）。
因此，如果您传递 1/255 的 scale_factor。| 大小（640,640）据我所知，每个元素应计算为 RGB =&gt; R = R/255。| G = G/255。|...
值 = (U8 - 平均值) * scale_factor

基本上 minmax 在 0 到 1 之间标准化。
所以在 py 上。
我尝试将输出 blob/ndarray * 255 相乘。并重新整形为（640, 640, 3），看起来输出图像是一张包含 3 行 3 列灰度和略有不同的饱和度的 9 张图像的图像？这是我尝试过的，与上面的 255 示例输出相同。
 test = cv2.dnn.blobFromImage(img, 1.0/127.5, (640, 640), (127.5, 127.5, 127.5), swapRB=True)
t1 = test * 127.5
t2 = t1 + 127.5
cv2.imwrite(&quot;./test_output.jpg&quot;, t2.reshape((640, 640, 3)))

我一直在查看他们的 opencv repo
 subtract(images[i], mean, images[i]);
multiply(images[i], scalefactor, images[i]);

说实话，看起来在 opencv lib 中实现的方式相同，但想请教一下大家的意见。
另一个问题是，如果输入的是完整的 u8 rgb 值，为什么会变成灰度？
我尝试通过应用类似的公式转换 4d ndarray 以匹配 blobFromImage 的输出。但输出并不相同。
我希望将图像转换为 (1, 3, w, h) 的 ndarray，减去平均值并乘以比例因子，当您转换回 (width, height, channel) 时，与 blobFromImage 的输出相同。
编辑：感谢您对添加输入和输出图像的建议。
输入
输出]]></description>
      <guid>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</guid>
      <pubDate>Wed, 02 Oct 2024 18:53:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么 tf.keras.models.load_model 要重新构建模型？</title>
      <link>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</link>
      <description><![CDATA[ValueError: 顺序模型“sequential_2”已配置为使用
输入形状（None、224、224、3）。您无法使用 input_shape [None、224、224、3] 构建它

def load_model(model_path):
&quot;&quot;&quot;
从指定路径加载已保存的模型。
&quot;&quot;&quot;

tf.keras.config.enable_unsafe_deserialization()
print(f&quot;正在加载已保存的模型：{model_path}&quot;)
model = tf.keras.models.load_model(model_path)
return model

loaded_1000_image_model = load_model(&#39;/content/drive/MyDrive/Dog Vision/models/20241002-16491727887796-1000-images-mobilenetv2-Adam.h5&#39;)

我原本以为它会加载模型而不会出现任何错误，但由于某种原因，它给出了一个值错误，尽管我没有尝试重建或再次给出任何输入形状。]]></description>
      <guid>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</guid>
      <pubDate>Wed, 02 Oct 2024 17:18:38 GMT</pubDate>
    </item>
    <item>
      <title>我的模型目前存在多少过度拟合问题？[关闭]</title>
      <link>https://stackoverflow.com/questions/79047127/how-much-of-an-overfitting-issue-do-my-models-currently-have</link>
      <description><![CDATA[我正在使用三个机器学习模型 - 逻辑回归、梯度提升和深度神经网络 - 使用相对简单的数据集和使用“sklearn”的机器学习模型模板。我获得的性能指标似乎很强，但我的老师提到了过度拟合的可能性，因为测试准确率略低于训练准确率。此外，它是一种二元分类。
我进行了 5 倍交叉验证，交叉验证分数非常接近我的模型准确率。但是，我仍然不确定这是否表示过度拟合，或者我的模型是否按预期运行。
以下是每个模型的结果：
逻辑回归：
测试准确率：97.69%
训练准确率：98.15%
平均交叉验证得分：97.97%
梯度提升：
测试准确率：97.98%
训练准确率：98.79%
平均交叉验证得分：97.97%
深度神经网络：
测试准确率：97.49%
训练准确率： 98.23%
平均交叉验证得分：97.92%
我是否可以得出结论：尽管可能存在轻微的过度拟合，但模型对于分类仍然有效，并且所使用的特征可能对于对目标进行分类非常有用？]]></description>
      <guid>https://stackoverflow.com/questions/79047127/how-much-of-an-overfitting-issue-do-my-models-currently-have</guid>
      <pubDate>Wed, 02 Oct 2024 14:05:29 GMT</pubDate>
    </item>
    <item>
      <title>构建电子邮件解析、数据提取和数据验证的系统[关闭]</title>
      <link>https://stackoverflow.com/questions/79047083/build-a-system-for-email-parsing-and-data-extraction-and-data-validation</link>
      <description><![CDATA[我正在为 B2B 匹配平台开发 MVP。核心思想是根据客户的需求和能力自动将客户与服务提供商配对。以下是我想要实现的目标：

从收到的电子邮件中提取数据（客户要求和服务提供商详细信息）：非结构化数据（客户和服务提供商之间的沟通没有标准）
以可用的格式构造这些数据
将这些结构化数据用于智能匹配系统

目前，我正在使用 OpenAI 的 API（gpt4-* 模型）来处理电子邮件解析和数据提取。虽然它在大多数情况下都能正常工作，但我担心它在生产环境中的稳健性和可靠性：有时，我会出现幻觉。
我的问题是：对于这种任务，有没有比这种方法更强大的替代方案？我正在寻找能够实现以下功能的东西：

可靠地解析电子邮件并提取相关信息
处理大量电子邮件
提供一致且易于使用的结构化输出

我只接受开源解决方案。
我尝试过的方法：

使用 OpenAI API 模型从非结构化数据中提取关键信息，但我仍然有幻觉。

我还需要一种实时构造数据的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79047083/build-a-system-for-email-parsing-and-data-extraction-and-data-validation</guid>
      <pubDate>Wed, 02 Oct 2024 13:55:12 GMT</pubDate>
    </item>
    <item>
      <title>我可以采取下一步措施来制作一个好的图像重复和接近重复查找器[关闭]</title>
      <link>https://stackoverflow.com/questions/79046818/next-steps-i-can-take-to-make-a-good-image-duplicate-and-near-duplicate-finder</link>
      <description><![CDATA[我正在尝试制作一个可以查找图像重复和近似图像重复的应用程序，以便从您的图像库中查找相似的照片。我有一个 NAS 设置，里面有大约 30-40gb 的图像，其中很多图像都有我和我的家人拍摄的多张类似图像。
对于我当前的实现，我可以拍摄 2 幅图像，从中提取嵌入（使用 CLIP 库），比较它的余弦相似度（对于精确重复）和欧几里得距离（对于压缩重复，如从聊天中下载的图像或图像的屏幕截图）。
我如何进一步推进这个项目并制作更好的应用程序？我目前的实现如下所示：
main.py
从 PIL 导入图像
导入 torch
导入 numpy 作为 np
导入 cv2 作为 cv
导入 clip
从 sklearn.cluster 导入 KMeans

设备 = &quot;cpu&quot;
image_path = &quot;assets/photos/aayush.jpeg&quot;
image_path2 = &quot;assets/photos/aayushResized.png&quot;

# 加载 CLIP 模型和预处理函数
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)

# 预处理图像并添加批处理维度
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
image2 = preprocess(Image.open(image_path2)).unsqueeze(0).to(device)

print(&quot;处理后的图像数据类型为：&quot;, type(image))

def calculate_distance(embedding1, embedding2):
&quot;&quot;&quot; 计算两幅图像之间的欧几里得距离 &quot;&quot;&quot;
dist = torch.nn. functional.pairwise_distance(embedding1, embedding2)
return (dist)

def calculate_cosine_similarity(embedding1, embedding2):
&quot;&quot;&quot; 计算 2 个图像嵌入之间的余弦相似度 &quot;&quot;&quot;
return torch.nn. functional.cosine_similarity(embedding1, embedding2)

def resize_with_aspect_ratio(raw_image_path, new_width):
# 读取原始图像
original_image = cv.imread(raw_image_path)

# 计算纵横比
aspects_ratio = original_image.shape[1] / original_image.shape[0]

# 根据所需宽度确定新高度
determined_height = int(new_width / aspects_ratio)

# 调整图像大小
resized_image = cv.resize(original_image, (new_width, determined_height))
print(&quot;Newly Resized images shape: &quot;, resized_image.shape)
cv.imshow(&quot;Screen&quot;, resized_image)
cv.imwrite(&quot;aayushResized.png&quot;, resized_image)
cv.waitKey(0)
cv.destroyAllWindows()

return resized_image

def normalize(image):
# 计算每个通道的标准差
channel_stds = np.std(image, axis=(0, 1))
print(channel_stds)
# 通过将图像除以通道标准差来进行归一化
normalized_image = image / channel_stds
return (normalized_image)

def clutster(image):
kmeans = KMeans(n_clusters=2, random_state=0, n_init=&quot;auto&quot;).fit(X)

# 从模型中获取图像嵌入

with torch.no_grad():
emb1 = model.encode_image(image)
emb2 = model.encode_image(image2)

print(&quot;嵌入的数据类型为：&quot;, type(emb1), &quot;And &quot;, type(emb2))
# print(emb1, &quot;\n &quot;, emb2)

# 计算嵌入之间的余弦相似度
similarity = calculate_cosine_similarity(emb1, emb2)
distance = calculate_distance(emb1, emb2)

print(f&quot;余弦相似度：{similarity.item()} {type(similarity)}&quot;)
print(f&quot;欧几里得距离：{distance.item()} {type(similarity)}&quot;)


cluster.py
# 使用聚类算法 [k means] 将张量聚类在一起：
import os
import torch
import clip
from PIL import Image
import numpy as np
from sklearn.cluster import KMeans

device =“CPU”
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)
# labels = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;]
name_list = []
np_array = []

# 循环遍历图像文件夹
for photos in os.listdir(&quot;assets/photos&quot;):
image_path = os.path.join(&quot;assets/photos&quot;, photos)
# 预处理图像并获取其张量
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
name_list.append(photos)
# 将张量的高维展平为类似2dim
numpyImage = np.array(image).flatten()
np_array.append(numpyImage)

k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(np_array)

# 获取 int 类标签（由 KMeans 自动生成）
cluster_labels = kmeans.labels_
image_cluster_map = {}
# 循环遍历图像并为其赋予标签
for i, labels in enumerate(cluster_labels):
# print(f&quot;Image {i} 属于集群 {labels}&quot;)
image_cluster_map[f&#39;image_{name_list[i]}&#39;] = labels

# 查看我们的最终集群
for i, (key, value) in enumerate(image_cluster_map.items()):
print(key, &quot;标记为：“，值）

]]></description>
      <guid>https://stackoverflow.com/questions/79046818/next-steps-i-can-take-to-make-a-good-image-duplicate-and-near-duplicate-finder</guid>
      <pubDate>Wed, 02 Oct 2024 12:49:21 GMT</pubDate>
    </item>
    <item>
      <title>XFormersMetadata.__init__() 收到意外的关键字参数“is_prompt”</title>
      <link>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</guid>
      <pubDate>Sun, 29 Sep 2024 13:03:20 GMT</pubDate>
    </item>
    <item>
      <title>给定的梯度下降代码是按顺序还是同时更新参数？</title>
      <link>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-parameters-sequentially-or-s</link>
      <description><![CDATA[我是机器学习的新手，一直在学习梯度下降算法。我相信此代码使用同时更新，即使它看起来像是顺序更新。由于偏导数的值是在更新 w 或 b 之前计算的，即从原始 w 和 b 开始，因此应用于单个 w、b 的算法是从原始值开始应用的。我错了吗？

dj_dw=((w*x[i]+b-y[i])*x[i])/m
dj_db=(w*x[i]+b-y[i])/m
w=w-a*dj_dw
b=b-a*dj_db

语言是 python3。
x 和 y 是训练集。
w 和 b 是应用算法的参数。
我正在使用梯度下降算法进行线性回归。
dj_dw 是均方误差成本函数对 w 的偏微分。dj_db 也是如此。
如有错误，敬请原谅，我是新手。
我尝试使用 gemini 和 chatgpt 进行交叉检查，他们说这是连续的，因此造成混淆]]></description>
      <guid>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-parameters-sequentially-or-s</guid>
      <pubDate>Wed, 05 Jun 2024 15:28:51 GMT</pubDate>
    </item>
    <item>
      <title>如何增强自定义视频录制 React.js 网站上的虚拟背景质量和背景分割</title>
      <link>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</link>
      <description><![CDATA[我们目前正在开发一个视频录制平台。为了方便视频拍摄，我们已将 MediaRecorder API 集成到我们的系统中。
作为我们平台功能集的一部分，用户可以使用虚拟背景和背景模糊等功能增强他们的视频。
这些功能依靠 Google MediaPipe 的 API 进行分割，使我们能够准确地隔离框架内的主体。
然而，在将我们的输出与 Google Meet 和 Microsoft Teams 等成熟平台进行比较后，我们发现质量存在明显差距。尽管我们努力实施先进的分割技术，但结果仍未达到行业标准，特别是在视觉保真度和准确性方面。
我们探索了各种途径并尝试了不同的技术。
其中包括TensorFlow（一种开源机器学习框架）、ML Kit（由 Google 提供的机器学习 SDK）和Video-SDK（一种全面的视频处理工具包）。尽管我们付出了这些努力，但我们的输出质量和一致性仍未达到预期水平。
如果您有任何关于增强虚拟背景的建议或关于用于虚拟背景集成的替代开源或付费工具的建议，我们将不胜感激？]]></description>
      <guid>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</guid>
      <pubDate>Thu, 11 Apr 2024 05:10:10 GMT</pubDate>
    </item>
    <item>
      <title>虚拟背景渲染不正确：寻求有关 WebRTC 实施的指导</title>
      <link>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</link>
      <description><![CDATA[我正在使用 RobustVideoMatting 库 (GitHub 存储库) 将虚拟背景支持集成到视频通话应用程序中。前景分割成功，但在 WebRTC 流式传输期间，我面临着背景渲染的挑战，我无法将背景图像应用于分割的视频。
尝试的方法：
方法 1 - CSS 样式：
const bg = `url(&#39;${this.bg}&#39;) center center / cover`;
this.canvas.style.background = bg;

方法 2 - HTMLImageElement：
const backgroundImage = new Image();
backgroundImage.src = &quot;https://d..........................jpg&quot;;
backgroundImage.onload = () =&gt; {
canvasRef.current.getContext(&quot;2d&quot;).drawImage(backgroundImage, 0, 0, 640, 480);
}

请求协助：
我正在寻求解决此问题的指导。如果有人有虚拟背景和 WebRTC 方面的经验，或者在使用 RobustVideoMatting 库时有具体注意事项，我们将不胜感激。
GitHub 存储库： PeterL1n/RobustVideoMatting
CodePen 链接 CodePen 参考
需要将背景图像或视频应用于分段视频帧。]]></description>
      <guid>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</guid>
      <pubDate>Fri, 01 Mar 2024 11:28:35 GMT</pubDate>
    </item>
    <item>
      <title>文件名中的键值对是否有标准的文件命名约定？</title>
      <link>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</link>
      <description><![CDATA[我有多个以它们所包含的内容命名的数据文件。例如
machine-testM_pid-1234_key1-value1.log

键和值由 - 和 _ 分隔。有没有更好的语法？有没有自动读取这些文件/文件名的解析器？
这里的想法是文件名是人类和机器可读的。]]></description>
      <guid>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</guid>
      <pubDate>Tue, 10 Apr 2012 10:32:32 GMT</pubDate>
    </item>
    </channel>
</rss>