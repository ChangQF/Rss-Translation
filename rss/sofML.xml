<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 03 Jun 2024 18:20:47 GMT</lastBuildDate>
    <item>
      <title>使用什么方法对网络结构进行统计分析？</title>
      <link>https://stackoverflow.com/questions/78571525/what-methods-to-use-for-statistical-analysis-of-network-structures</link>
      <description><![CDATA[我有一个网络结构（邻接矩阵），想找到一种方法来告诉我网络中的哪些特征对于解释我的响应变量最重要。在这种情况下，它是一种感染模拟措施。直观地说，一个可以提取我看不到的图形特征并告诉我它很重要的神经网络会很棒，但我不知道该怎么做。理想情况下，我也想保持简单。任何建议都值得赞赏。
我一直在使用广泛的 GAM 进行建模，但总是回到这个问题上，我是提取我认为重要的特征的人。]]></description>
      <guid>https://stackoverflow.com/questions/78571525/what-methods-to-use-for-statistical-analysis-of-network-structures</guid>
      <pubDate>Mon, 03 Jun 2024 16:28:23 GMT</pubDate>
    </item>
    <item>
      <title>OverflowError：无法将无穷大转换为整数</title>
      <link>https://stackoverflow.com/questions/78571461/overflowerror-cannot-convert-infinity-to-integer</link>
      <description><![CDATA[我遇到了这个错误，如果有人有解决方案，请告诉我。
---------------------------------------------------------------------------
OverflowError Traceback (most recent call last)
Cell In[61], 
line 1 ----&gt; 1 loss, accuracies = federated_learning(clients)
Cell In[60], line 113, in federated_learning(clients)
111 # 服务器收集客户端的加密权重，然后执行权重聚合
112 with torch.no_grad():
--&gt; 113 aggregator()
115 # 日志记录
116 if (iteration + 1) % 2 == 0:
Cell In[60], line 96, in federated_learning.&lt;locals&gt;.aggregator()
93 global_model.linear.bias = nn.Parameter(torch.tensor(model_bias, dtype=torch.float32))
95 # 加密聚合的全局模型参数并保存到每个客户端的 enc_file
---&gt; 96 encrypt_weights(aggregated_pa​​rams_float32, clients[0].enc_file) # 使用一个客户端的 enc_file 进行加密
98 for client in clients:
99 with open(clients[0].enc_file, &#39;r&#39;) as f:
Cell In[50], line 53, in encrypt_weights(model_params, enc_file)
51 # 通过缩放将十进制转换为整数
52 scale_factor = Decimal(10**10) # 放大以保持精度
---&gt; 53 model_params_scaled = [int(Decimal(param) * scale_factor) for param in model_params_str]
55 crypto_params = [elgamal.encrypt(param) for param in model_params_scaled]
57 # 将加密参数和公钥 (p, g, h) 以及私钥 x 保存到文件
Cell In[50], line 53, in &lt;listcomp&gt;(.0)
51 # 通过缩放将十进制转换为整数
52 scale_factor = Decimal(10**10) # 放大以保持精度
---&gt; 53 model_params_scaled = [int(Decimal(param) * scale_factor) for param in model_params_str]
55 crypto_params = [elgamal.encrypt(param) for param in model_params_scaled]
57 # 将加密参数和公钥 (p, g, h) 以及私钥 x 保存到文件
**OverflowError: 无法将无穷大转换为整数**

我对模型参数进行了加密和解密，并尝试使用较大的 int 值。]]></description>
      <guid>https://stackoverflow.com/questions/78571461/overflowerror-cannot-convert-infinity-to-integer</guid>
      <pubDate>Mon, 03 Jun 2024 16:13:39 GMT</pubDate>
    </item>
    <item>
      <title>如何标记社交媒体中未标记的大量文本数据</title>
      <link>https://stackoverflow.com/questions/78571418/how-to-label-unlabeled-large-text-data-from-socia-media</link>
      <description><![CDATA[我收集了一些社交媒体评论，并计划执行分层深度学习模型。作为初始过程，我想将这些未标记的数据标记为一些预定义的主题，例如交通、食品等。然后必须定义不同的交通或食品类别。所以，我需要两列标签。我有 9.341.087 个无标签训练数据和 30.000 行无标签测试文本数据。由于我是机器学习的新手，不确定在没有标记数据的情况下是否可以继续标记。感谢您的帮助
我尝试了分层文本分类，需要拖曳特定的标签列，但行数太少，我无法手动完成。]]></description>
      <guid>https://stackoverflow.com/questions/78571418/how-to-label-unlabeled-large-text-data-from-socia-media</guid>
      <pubDate>Mon, 03 Jun 2024 16:04:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试为 knn 算法绘制不同的邻居</title>
      <link>https://stackoverflow.com/questions/78571281/trying-to-plot-different-neighbors-for-a-knn-algorithm</link>
      <description><![CDATA[它给出了一个值错误
ValueError：x 和 y 必须具有相同的第一维，但形状为 (10,) 和 (30,)
training_accuracy = []
test_accuracy = []
neighbors_settings = range(1, 11)
for n_neighbors in neighbours_settings:
# 构建模型
clf = KNeighborsClassifier(n_neighbors=n_neighbors)
clf.fit(X_train, y_train)
# 记录训练集准确率
training_accuracy.append(clf.score(X_train, y_train))
# 记录泛化准确率
test_accuracy.append(clf.score(X_test, y_test))

plt.plot(neighbors_settings, training_accuracy, label=&quot;training准确度”）
plt.plot（neighbors_settings，test_accuracy，label =“测试准确度”）
plt.ylabel（“准确度”）
plt.xlabel（“n_neighbors”）
plt.legend（）
]]></description>
      <guid>https://stackoverflow.com/questions/78571281/trying-to-plot-different-neighbors-for-a-knn-algorithm</guid>
      <pubDate>Mon, 03 Jun 2024 15:35:17 GMT</pubDate>
    </item>
    <item>
      <title>加载预先训练的 json 模型时，Python tensorflow keras 出现错误</title>
      <link>https://stackoverflow.com/questions/78570246/python-tensorflow-keras-error-when-load-a-pre-trained-json-model</link>
      <description><![CDATA[这是我在尝试执行 Python 代码时遇到的**错误**
[ERROR:0@5.030] global persistence.cpp:519 cv::FileStorage::Impl::open 无法以读取模式打开文件：“models/haarcascade_frontalface_default.xml”
回溯（最近一次调用）：
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\anti.py”，第 14 行，位于&lt;module&gt;
model = model_from_json(loaded_model_json)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\models\model.py&quot;，第 575 行，位于 model_from_json
return serialization_lib.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 694 行，位于 deserialize_keras_object
cls = _retrieve_class_or_fn(
^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 812 行，位于 _retrieve_class_or_fn
raise TypeError(
TypeError：无法找到类“Functional”。确保自定义类已用修饰`@keras.saving.register_keras_serializable()`。完整对象配置：{&#39;class_name&#39;: &#39;Functional&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;model&#39;, &#39;trainable&#39;: True,...(**json 文件的内容**........&#39;keras_version&#39;: &#39;2.15.0&#39;, &#39;backend&#39;: &#39;tensorflow&#39;)

以下是库版本
keras 3.3.3
opencv-python 4.9.0.80
tensorflow 2.16.1
python 3.12.3
以下是我正在尝试执行的**代码**
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array 
import os
import numpy as np

root_dir = os.getcwd()
# 加载人脸检测模型
face_cascade = cv2.CascadeClassifier(&quot;models/haarcascade_frontalface_default.xml&quot;)
# 加载反欺骗模型图
json_file = open(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/Antispoofing_model_mobilenet.json&#39;,&#39;r&#39;)
loaded_model_json = json_file.read()
json_file.close()
model = tf.keras.models.model_from_json(loaded_model_json)
# 加载反欺骗模型权重
model.load_weights(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/finalyearproject_antispoofing_model_99-0.978947.h5&#39;)
print(&quot;模型从磁盘加载&quot;)

video = cv2.VideoCapture(0)
while True:
try:
ret,frame = video.read()
gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = face_cascade.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces: 
face = frame[y-5:y+h+5,x-5:x+w+5]
resized_face = cv2.resize(face,(160,160))
resized_face = resized_face.astype(&quot;float&quot;) / 255.0
resized_face = np.expand_dims(resized_face, axis=0)
# 将人脸 ROI 传递到经过训练的活体检测器
# 模型确定人脸是“真”还是“假”
preds = model.predict(resized_face)[0]
print(preds)
if preds&gt; 0.5:
标签 = &#39;poof&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 0, 255), 2)
else:
标签 = &#39;eal&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 255, 0), 2)
cv2.imshow(&#39;frame&#39;, frame)
key = cv2.waitKey(1)
if key == ord(&#39;q&#39;):
break
except Exception as e: 
pass
video.release() 
cv2.destroyAllWindows()


请告诉我我应该怎么做（我尝试降级库，但也出现了错误）
我尝试降级库，但也出现了错误]]></description>
      <guid>https://stackoverflow.com/questions/78570246/python-tensorflow-keras-error-when-load-a-pre-trained-json-model</guid>
      <pubDate>Mon, 03 Jun 2024 12:21:33 GMT</pubDate>
    </item>
    <item>
      <title>梯度累积损失计算</title>
      <link>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</link>
      <description><![CDATA[假设我们有数据 [b,s,dim]，我最近注意到 CrossEntropyLoss 是 (1) 计算一批中所有 token (b * s) 的平均值，而不是 (2) 计算每个句子然后计算平均值。
以下是计算 hugging_face 转换器 BertLMHeadMode 损失的代码
sequence_output = output[0]
prediction_scores = self.cls(sequence_output)

lm_loss = None
if labels is not None:
# 我们正在进行下一个 token 预测；将预测分数和输入 ID 移位一格
shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()
labels = labels[:, 1:].contiguous()
loss_fct = CrossEntropyLoss()
lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

我知道 (1) 和 (2) 在这种情况下没有区别。但是当我们应用梯度累积时，我认为情况就不同了。
假设我的batch_size为4，4个句子的长度分别为100,200,300,400。
使用batch_size 4时，损失是根据总共1000个token的平均值计算的。
但是当batch_size = 1且梯度累积= 4时，我认为损失是不同的。我们首先分别计算每个句子的损失，然后计算平均值，这意味着对于100个token的句子，我们计算100个token的损失平均值，然后除以4并将其添加到总损失中，对于其他3个句子也是如此，我认为以这种方式计算的损失与使用batch_size = 4计算的损失不同。
我误解了什么吗？
想知道我是否正确或误解了什么]]></description>
      <guid>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</guid>
      <pubDate>Mon, 03 Jun 2024 11:19:53 GMT</pubDate>
    </item>
    <item>
      <title>用于聊天中问答识别的印地语 NLP 模型</title>
      <link>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</link>
      <description><![CDATA[我正在开展一个自然语言处理项目，涉及分析印地语聊天数据。具体来说，我需要在聊天记录中识别问题及其对应的答案。
是否有任何预先训练过的印地语 NLP 模型或库可以有效地处理此任务？理想情况下，我正在寻找一种解决方案，可以：
检测句子并将其分类为问题或答案。
识别上下文并将问题与各自的答案配对。
任何有关模型、库或方法的建议都将不胜感激。谢谢！
我尝试了各种模型，但没有一个适合。]]></description>
      <guid>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</guid>
      <pubDate>Mon, 03 Jun 2024 06:56:50 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 模型在攻击期间预测相同的 token，但在训练期间表现良好</title>
      <link>https://stackoverflow.com/questions/78568713/transformer-model-predicting-the-same-token-during-infrence-but-performing-well</link>
      <description><![CDATA[我的 Transformer 模型无法正常工作。
训练循环：
for epoch in range(40):
data_loader = tqdm(data_loader, desc=f&quot;Epoch {epoch + 1}/{20}&quot;, unit=&quot;batch&quot;)
for batch_idx, (en_batch, hi_batch) in enumerate(data_loader):
en_batch = en_batch.to(&#39;cuda&#39;).to(torch.long)
hi_batch = hi_batch.to(&#39;cuda&#39;).to(torch.long)
y_pred = model(en_batch, hi_batch)
loss = loss_fn(y_pred[:, 0:127, :].transpose(2,1), hi_batch[:, 1:128]).mean()
history.append(loss.item())
如果 batch_idx % 400 == 0:
clear_output(wait=False)
torch.save(history, &#39;history2.pth&#39;)
torch.save(losses, &#39;valLoss2.pth&#39;)
torch.save(model.state_dict(), &#39;model_weightsBPE2.pth&#39;)
model.eval()
val_loss = 0
使用 torch.no_grad():
对于 id , (en_batch, hi_batch) 在 enumerate(val_loader, 1):
en_batch, hi_batch = en_batch.to(&#39;cuda&#39;), hi_batch.to(&#39;cuda&#39;)
y_pred = model(en_batch.to(torch.long), hi_batch[:, :-1].to(torch.long))
val_loss += loss_fn(y_pred[:, 0:127, :].transpose(2,1), hi_batch[:, 1:128].to(torch.long)).mean()
如果 id % 5 == 0:
break
val_loss /= 5
model.train()
print(f&quot;验证损失：{val_loss}&quot;)
losses.append(val_loss.item())
如果 batch_idx % 100 == 0:
print(&quot;-&quot;20, batch_idx, &quot;-&quot;, epoch, &quot;-&quot;, loss.item(), &quot;-&quot;20)
print(&quot;en : &quot;, id_to_token(en_batch, &quot;en&quot;))
print(&quot;hi : &quot;, id_to_token(hi_batch, &quot;hi&quot;))
print(&quot;out: &quot;, id_to_token_M(y_pred, &quot;hi&quot;))
optimizer.zero_grad()
loss.backward()
optimizer.step()
scheduler.step()


训练输出：
out: संघ के प्रदेशाध्यक्ष बृज मो गाुप्ता ने महारे्ल इेंदिर क के艾特里公园韋प माया।


推理循环：
def inference_loop( input_seq, max_output_length=128):
input_seq = input_seq.unsqueeze(0) # 添加批次维度
current_token = torch.tensor([[1]], device=input_seq.device)
output_seq = []
for * in range(max*output_length):
predictions = model(input_seq, current_token)
next_token = predictions[:, -1, :].argmax(dim=-1)
current_token = torch.cat([current_token, next_token.unsqueeze(0)], dim=1)
if next_token.item() == 2:
break
output_seq.append(next_token.item())
return output_seq


推理输出：
&#39; क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क&#39;


我该如何解决这个问题，训练和验证损失都下降了，但在推理过程中它开始一次又一次地预测相同的标记。]]></description>
      <guid>https://stackoverflow.com/questions/78568713/transformer-model-predicting-the-same-token-during-infrence-but-performing-well</guid>
      <pubDate>Mon, 03 Jun 2024 06:31:29 GMT</pubDate>
    </item>
    <item>
      <title>稳定扩散图像中突变的检测方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78568259/detection-methods-for-mutations-in-stable-diffusion-images</link>
      <description><![CDATA[我正在使用 Python 中的稳定扩散来生成人体图像。我面临的一个挑战是自动检测生成的图像中的任何异常。这些异常包括手、脚等身体部位的不规则性，或身体部位过多或过少，以及介于两者之间的一切。
我测试了几种方法，但没有成功。例如，我尝试使用 OpenAI GPT4 视觉来识别这些异常；但是，它无法检测到异常，除非我说“你确定吗”几次，然后它似乎会发现它。
为了保持本地化，我尝试了视觉模型，如 llava，但这也失败了。我也尝试训练自己的卷积神经网络，但由于训练数据不足（手动标记），它最终过度拟合。其他潜在解决方案（如 ViT 和 EfficientNet）似乎也面临数据不足的相同问题。
我需要的是一种无需人工干预即可自动检测这些异常的方法，有什么建议吗？
我尝试了以下操作：



试用
问题




OpenAI GPT4 视觉
除非多次提示，否则无法检测到异常


本地视觉模型（如 llava）
未发现异常


训练自己的卷积神经网络
由于训练数据不足（手动标记）


潜在解决方案（如 ViT 和 EfficientNet）
面临数据不足的相同问题



不良照片示例：
1：图片
2：图片
3：图片]]></description>
      <guid>https://stackoverflow.com/questions/78568259/detection-methods-for-mutations-in-stable-diffusion-images</guid>
      <pubDate>Mon, 03 Jun 2024 03:28:04 GMT</pubDate>
    </item>
    <item>
      <title>编码结果以布尔值而不是数字形式输出</title>
      <link>https://stackoverflow.com/questions/78565774/encoded-result-gives-output-as-bolean-instead-of-numeric</link>
      <description><![CDATA[执行 OneHotEncoding 后，我的数据框将其列的值设为 True 和 False，而不是我应该得到的 1 和 0。我的代码如下所示，其中数据框为 ds，分类列名称为“type”：
pd.get_dummies(ds,columns = [&quot;type&quot;])

我试图获取数字结果。]]></description>
      <guid>https://stackoverflow.com/questions/78565774/encoded-result-gives-output-as-bolean-instead-of-numeric</guid>
      <pubDate>Sun, 02 Jun 2024 07:52:13 GMT</pubDate>
    </item>
    <item>
      <title>在 Feast + Cassandra 中存储特征之前是否需要一个中间持久存储？</title>
      <link>https://stackoverflow.com/questions/78544969/is-an-intermediary-persistent-store-needed-before-storing-features-in-feast-ca</link>
      <description><![CDATA[我目前正在为 MLOps 项目构建大数据管道，该管道用于批处理。
这是当前设置：

我将原始结构化数据存储在 Hive 中。
Spark 作业提取原始数据并对其进行处理。
我打算使用 feast 和 Apache Cassandra 作为离线存储，用于存储由我的 Spark 作业产生的计算和策划特征。

我想高效地将数据从 spark 作业传递到 feast 和 Cassandra，我不确定在将处理后的数据传递给 feast 以存储在离线存储中之前，是否需要中间数据持久性解决方案来保存处理后的数据，在我的情况下有必要吗？]]></description>
      <guid>https://stackoverflow.com/questions/78544969/is-an-intermediary-persistent-store-needed-before-storing-features-in-feast-ca</guid>
      <pubDate>Tue, 28 May 2024 15:00:31 GMT</pubDate>
    </item>
    <item>
      <title>预测多元时间序列时 VARIMA 模型的模型漂移</title>
      <link>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</link>
      <description><![CDATA[我目前正在尝试在多变量时间序列数据上训练 VARIMA 模型，该数据是关于冷却系统的 5 种不同类型的传感器测量的。数据具有周期性，因此完全相同的模式每 25 个数据点左右就会重复出现。我有一个包含 5 个不同组件的数据集，其中每分钟都有一个数据点。我使用了 2 周的数据来训练模型，然后让它合成数据。我使用的 VARIMA 模型是从 Darts 包导入的，我是这样定义模型的：model_VARIMA = VARIMA(p=12, d=0, q=0, trend=&quot;n&quot;)。该模型是在 2 周的训练数据上训练的。当预测未来 30 个点或更多时，预测显然开始显示模型漂移。所有组件都没有产生周期性的多变量时间序列数据，而是慢慢开始遵循一条不再改变值的水平直线。换句话说，所有成分的标准偏差都会慢慢变为零，所有成分都会慢慢开始向其平均值漂移并永远停留在那里。我想知道是否有人对此有解释并有解决问题的方法。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</guid>
      <pubDate>Tue, 28 May 2024 12:15:20 GMT</pubDate>
    </item>
    <item>
      <title>langchain RetrievalQA 错误：ValueError：缺少一些输入键：{'query'}</title>
      <link>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</link>
      <description><![CDATA[在 RAG 项目中，我使用的是 langchain。当我使用查询输入运行 QA 链时，此错误一直出现：
----&gt; result = qa_chain({&#39;query&#39;: question})
ValueError: 缺少一些输入键：{&#39;query&#39;}

这是我的代码：
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 构建提示
template = &quot;&quot;&quot;根据以下上下文回答问题。
上下文：
{context}
------------------
问题：{query}
答案：&quot;&quot;&quot;

# LLM 链
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
qa_chain = RetrievalQA.from_chain_type(
llm,
trieser=vectordb.as_retriever(),
return_source_documents=True,
chain_type_kwargs={&quot;prompt&quot;: QA_CHAIN_PROMPT}
)

question = &quot;这篇研究论文使用了什么方法？&quot;

result = qa_chain({&#39;query&#39;: question})

# 检查查询结果
result[&quot;result&quot;]
# 检查我们从中获取的源文档
result[&quot;source_documents&quot;][0]
]]></description>
      <guid>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</guid>
      <pubDate>Fri, 24 May 2024 21:23:49 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降：缩减的特征集比原始特征集的运行时间更长</title>
      <link>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</link>
      <description><![CDATA[我尝试用 Python 实现梯度下降算法来解决机器学习问题。我处理的数据集已经过预处理，当我使用奇异值分解 (SVD) 比较两个数据集（一个具有原始特征，另一个具有前者集合的缩减维度）时，我在运行时观察到了意外行为。我不断观察到，与缩减后的数据集相比，较大的原始数据集的梯度下降算法的运行时间较短，这与我的预期相反。考虑到缩减后的数据集较小，运行时间难道不应该更短吗？我试图理解为什么会发生这种情况。
以下是相关代码片段：
import time
import numpy as np
import matplotlib.pyplot as plt

def h_theta(X1, theta1):
# 假设函数的实现
return np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
# 成本函数的实现
return np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, theta):
# 梯度的计算
gradient = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
return gradient

def gradient_descent(X1, y1):
theta_initial = np.zeros(X1.shape[1]) # 用初始化 theta零

num_iterations = 1000
learning_rates = [0.1, 0.01, 0.001] 
cost_iterations = []
theta_values = []
start = time.time()
for alpha in learning_rates:
theta = theta_initial.copy()
cost_history = []
for i in range(num_iterations):
gradient = grad(X1, y1, theta)
theta = theta - np.dot(alpha, gradient)
cost = j_theta(X1, y1, theta)
cost_history.append(cost)
cost_iterations.append(cost_history)
theta_values.append(theta)
end = time.time()
print(f&quot;所用时间：{end - start} 秒&quot;)
fig, axs = plt.subplots(len(learning_rates), figsize=(8, 15)) 
for i, alpha in enumerate(learning_rates):
axs[i].plot(range(num_iterations), cost_iterations[i], label=f&#39;alpha = {alpha}&#39;) 
axs[i].set_title(f&#39;学习率：{alpha}&#39;)
axs[i].set_ylabel(&#39;成本 J&#39;)
axs[i].set_xlabel(&#39;迭代次数&#39;) 
axs[i].legend()
plt.tight_layout() 
plt.show()

# 使用 SVD 将 X 减少到 3 个特征（列）的代码：
# 对 X 执行奇异值分解并将其减少到 3 列
U, S, Vt = np.linalg.svd(X_normalized)
# 将 X 减少到 3 列
X_reduced = np.dot(X_normalized, Vt[:3].T)

# 打印 X_reduced 的前 5 行
print(&quot;X_reduced 的前 5 行：&quot;)
# 规范化 X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;X 经过缩减和规范化后的均值和标准差:\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# 打印简化后的 X 的形状，以确认它只有 3 个特征
print(&quot;Shape of X_reduced:&quot;, X_reduced.shape)

# 将截距列添加到 X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))

# 示例用法
# X_normalized_with_intercept 和 y_normalized 表示原始数据集
# X_reduced_with_intercept 和 y_normalized 表示简化后的数据集

# 对原始数据集执行梯度下降
gradient_descent(X_normalized_with_intercept, y_normalized)

# 对简化后的数据集执行梯度下降
gradient_descent(X_reduced_with_intercept, y_normalized)

在我的梯度下降实现中，什么原因导致精简数据集的运行时间始终比完整数据集长？任何有关故障排除的见解或建议都将不胜感激。
我尝试过重写和审查我的实现，但似乎对于大多数学习率以及增加的迭代次数，较大特征集的运行时间低于其 SVD 子集。]]></description>
      <guid>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</guid>
      <pubDate>Sun, 07 Apr 2024 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 LSTM 预测未来值</title>
      <link>https://stackoverflow.com/questions/69906416/forecast-future-values-with-lstm-in-python</link>
      <description><![CDATA[此代码可预测截至当前日期的指定股票价值，但无法预测训练数据集之外的日期价值。此代码来自我之前提出的一个问题，因此我对它的理解程度相当低。我认为解决方案是一个简单的变量更改以添加额外的时间，但我不知道需要操作哪个值。
import pandas as pd
import numpy as np
import yfinance as yf
import os
import matplotlib.pyplot as plt
from IPython.display import display
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;

pd.options.mode.chained_assignment = None

# 下载数据
df = yf.download(tickers=[&#39;AAPL&#39;], period=&#39;2y&#39;)

# 拆分数据
train_data = df[[&#39;Close&#39;]].iloc[: - 200, :]
valid_data = df[[&#39;Close&#39;]].iloc[- 200:, :]

# 缩放数据
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(train_data)

train_data = scaler.transform(train_data)
valid_data = scaler.transform(valid_data)

# 提取训练序列
x_train, y_train = [], []

for i in range(60, train_data.shape[0]):
x_train.append(train_data[i - 60: i, 0])
y_train.append(train_data[i, 0])

x_train = np.array(x_train)
y_train = np.array(y_train)

# 提取验证序列
x_valid = []

for i in range(60, valid_data.shape[0]):
x_valid.append(valid_data[i - 60: i, 0])

x_valid = np.array(x_valid)

# 重塑序列
x_train = x_train.reshape(x_train.shape[0], 
x_train.shape[1], 1)
x_valid = x_valid.reshape(x_valid.shape[0], 
x_valid.shape[1], 1)

# 训练模型
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, 
input_shape=x_train.shape[1:]))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
model.fit(x_train, y_train, epochs=50, batch_size=128, verbose=1)

# 生成模型预测
y_pred = model.predict(x_valid)
y_pred = scaler.inverse_transform(y_pred)
y_pred = y_pred.flatten()

# 绘制模型预测
df.rename(columns={&#39;Close&#39;: &#39;Actual&#39;}, inplace=True)
df[&#39;Predicted&#39;] = np.nan
df[&#39;Predicted&#39;].iloc[- y_pred.shape[0]:] = y_pred
df[[&#39;Actual&#39;, &#39;Predicted&#39;]].plot(title=&#39;AAPL&#39;)

display(df)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/69906416/forecast-future-values-with-lstm-in-python</guid>
      <pubDate>Tue, 09 Nov 2021 23:47:07 GMT</pubDate>
    </item>
    </channel>
</rss>