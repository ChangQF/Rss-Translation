<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 03 Apr 2024 09:14:34 GMT</lastBuildDate>
    <item>
      <title>客户与业务对话的短信分类</title>
      <link>https://stackoverflow.com/questions/78266176/text-message-classification-for-customer-business-conversations</link>
      <description><![CDATA[我想将客户的新消息分为 +-3 类。在某些情况下，企业是一家药房，消息通常是订单，但有时是一般性问题。我想将消息分为 A. 一般问题、B. 需要药剂师和 C. 不需要药剂师的订单。这将确保药剂师能够专注于需要的信息，从而提高业务效率。
我想知道我是否可以使用现有的预训练模型以及如何有效地做到这一点，或者我是否必须在旧消息上训练我自己的特定模型，如果可以的话我如何标记消息快速进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/78266176/text-message-classification-for-customer-business-conversations</guid>
      <pubDate>Wed, 03 Apr 2024 08:04:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PYMC 定义自定义似然函数？</title>
      <link>https://stackoverflow.com/questions/78266093/how-can-i-define-a-custom-likelihood-function-using-pymc</link>
      <description><![CDATA[使用 emcee 包执行 MCMC 时，我可以简单地定义自定义似然函数和后验函数，并将它们放入 emcee.EnsembleSampler 中。整个代码：
def log_likelihood(theta):
    teff_k、logg_k、feh_k、CFE、NFE、OFE、MGFE、ALFE、SIFE、CAFE、SCFE、TIFE、VFE、CRFE、COFE、NIFE = θ
    VMIC、HE_ABUN、LIFE、NAFE、SFE、KFE、MNFE、CUFE、ZNFE、YFE、SRFE、BAFE、NDFE、LAFE、EUFE、ZRFE、PRFE、CEFE、SMFE、AUFE、THFE、UFE = 其他参数
    input_para = [teff_k, logg_k, feh_k, VMIC, HE_ABUN, LIFE,
                  CFE、NFE、OFE、NAFE、MGFE、ALFE、
                  SIFE、SFE、KFE、CAFE、SCFE、TIFE、
                  VFE、CRFE、MNFE、COFE、NIFE、CUFE、
                  ZNFE、YFE、SRFE、BAFE、NDFE、LAFE、
                  EUFE、ZRFE、PRFE、CEFE、SMFE、AUFE、
                  THFE、UFE]
    input_para = np.array(input_para).T
    input_para = torch.tensor(input_para, dtype=torch.float64).view(1, -1)
    # 使用 Transformer 模型预测并计算 chi_square
    model1 = 变压器（num_layers、d_model、num_heads、d_ff、dropout、input_p_dim、output_s_dim）
    Predict_flux_7718 = 模型1(input_para)
    chi_squared = np.sum(residn ** 2 / yerrn ** 2 + np.log(yerrn ** 2))
    返回 -0.5 * 卡方

……
def log_probability(theta):
    lp = log_prior(theta)
    如果不是 np.isfinite(lp):
        返回-np.inf
    返回 lp + log_likelihood(theta)

……
 采样器 = emcee.EnsembleSampler(nwalkers, ndim, log_probability,
                                    池=池，
                                    移动=[(emcee.moves.DEMove(), 0.01),(emcee.moves.DESnookerMove(), 0.01)])

但是如果我想将 emcee 更改为 pymc 包，我该如何定义这个似然函数？例如  Likelihood = pm.Potential(&#39;likelihood&#39;, log_likelihood(theta))?]]></description>
      <guid>https://stackoverflow.com/questions/78266093/how-can-i-define-a-custom-likelihood-function-using-pymc</guid>
      <pubDate>Wed, 03 Apr 2024 07:50:36 GMT</pubDate>
    </item>
    <item>
      <title>图像超分辨率深度学习模型的指导</title>
      <link>https://stackoverflow.com/questions/78265945/guidance-with-image-super-resolution-deep-learning-models</link>
      <description><![CDATA[我正在寻找可以在嵌入式平台（Nvidia Jetson 类型）上实时运行的超分辨率模型，同时获得尽可能高的输出图像质量。
我一直在寻找单图像（SISR）和多帧（MFSR）选项。我主要看到四类模型：仅关注空间超分辨率（SISR）的模型、利用时间相关性（MFSR）的模型、尝试提取图像高频部分的模型以及基于视觉变换器的模型。然而，作为该领域的专家，我很快发现存在大量可能的模型，其中许多模型可能不适合嵌入式设备。我想就以下问题寻求一些指导：

首先，您会推荐一个特定的模型系列来实现我的目标吗？
您有特定的型号吗？

提前谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78265945/guidance-with-image-super-resolution-deep-learning-models</guid>
      <pubDate>Wed, 03 Apr 2024 07:26:45 GMT</pubDate>
    </item>
    <item>
      <title>使用非序列数据时，LSTM 给出更概括的结果，准确率为 89%，而使用序列数据时准确率为 64%</title>
      <link>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</link>
      <description><![CDATA[我正在研究时间序列分类。我使用了下面给出的两个预处理步骤

时间序列数据集 --&gt;切片时间序列 ---&gt;训练-验证-测试-分割 --&gt;模型训练
准确度——90%

时间序列数据集 --&gt;训练-验证-测试-分割 --&gt;独立切片训练/测试/验证 --&gt;模型训练
准确度 -- 64%


我目前正在努力为我的项目获取顺序概率分布，并已进入第二步。但是，我得到的准确度结果低于预期，甚至低于第一步。我已经调整了超参数并解决了类别不平衡的问题以消除偏差，但准确率没有提高到超过 64%。谁能提供一些关于为什么会发生这种情况的见解？
对于训练验证测试拆分，我使用 sklearn
x_main, x_test, y_main, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y,shuffle=True)
x_train, x_val, y_train, y_val = train_test_split(x_main, y_main, test_size= 0.1, random_state=42,stratify=y_main,shuffle=True)
至少我预计第二步的结果接近 80-85%]]></description>
      <guid>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</guid>
      <pubDate>Wed, 03 Apr 2024 05:05:10 GMT</pubDate>
    </item>
    <item>
      <title>当重新训练循环神经网络模型时，val_loss比loss大得多[关闭]</title>
      <link>https://stackoverflow.com/questions/78263413/when-retraining-a-recurrent-neural-network-model-val-loss-is-much-bigger-than-l</link>
      <description><![CDATA[我想使用循环神经网络来分析时间序列。我第一次使用常规正弦函数的时间序列。我想构建一个模型并在后续数据上对其进行测试。
将 numpy 导入为 np
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split

# Подготовка данных
x = np.arange(1, 100001)
y = np.sin(0.01*x)

x = x.reshape(-1, 1, 1)

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)
模型 = tf.keras.Sequential([
    tf.keras.layers.Embedding（input_dim = 200001，output_dim = 64），
    tf.keras.layers.Conv1D(filters=64，kernel_size=3，padding=&#39;same&#39;，activation=&#39;relu&#39;),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Dense(1)
]）
#model.compile(优化器=‘adam’,loss=‘mean_squared_error’)

# Компиляция модели
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)


model.fit（x_train，y_train，epochs = 60，batch_size = 1024，validation_data =（x_val，y_val））# Обучаем модель на данных x и y

结果，我的模型被重新训练。
纪元 12/60
79/79 [================================] - 1s 9ms/步 - 损耗：0.0140 - val_loss：0.5431
13/60 纪元
79/79 [================================] - 1s 9ms/步 - 损耗：0.0132 - val_loss：0.5430
14/60 纪元
79/79 [================================] - 1s 9ms/步 - 损耗：0.0125 - val_loss：0.5427

但是val_loss比loss大得多。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78263413/when-retraining-a-recurrent-neural-network-model-val-loss-is-much-bigger-than-l</guid>
      <pubDate>Tue, 02 Apr 2024 18:41:45 GMT</pubDate>
    </item>
    <item>
      <title>Autograd 返回无</title>
      <link>https://stackoverflow.com/questions/78263388/autograd-returning-none</link>
      <description><![CDATA[我正在尝试创建一个收缩自动编码器，我在几篇论文中读到，其主要思想是使用编码器输出相对于其输入的雅可比行列式的范数。
换句话说，我试图在使用原始输入的同时获取编码器输出的梯度。
到目前为止，我有这样的事情：
梯度 = torch.autograd.grad(输出 = Latent_X, 输入 = X, grad_outputs = torch.ones_like(latent_X),
                                create_graph = True，allow_unused = True）[0]

print(渐变) # 无！
frobenius_norm = torch.mean(torch.norm(梯度, p = &#39;fro&#39;, dim = (1,2)))

Contractive_penalty = self.args[&#39;lambda&#39;] * frobenius_norm
总损失 += 收缩惩罚

计算第一行中的梯度时会出现问题。由于某种原因，它返回 None，所以我首先尝试的是查看数据是什么样的。
输入数据X：(32 x 2866)
张量([[0.4663, 0.3859, 0.6573, ..., 0.7819, 0.0822, 0.3332],
        [0.4204, 0.8448, 0.6168, ..., 0.2698, 0.3503, 0.3372],
        [0.6329, 0.4084, 0.7437, ..., 0.3490, 0.4902, 0.8333],
        ...,
        [0.3004, 0.6908, 0.7698, ..., 0.8115, 0.9253, 0.1996],
        [0.6895, 0.6812, 0.4595, ..., 0.8959, 0.6600, 0.5660],
        [0.5647, 0.2448, 0.5046, ..., 0.6494, 0.4483, 0.5269]],
       device=&#39;cuda:0&#39;, grad_fn=)

编码器的输出：(32 x 32)
张量([[0.3837, 0.3975, 0.5000, ..., 0.6480, 0.9503, 0.8660],
        [0.4182, 0.5000, 0.8683, ..., 0.4916, 0.7044, 0.5293],
        [0.5000, 0.7034, 0.5588, ..., 0.3750, 0.5000, 0.5000],
        ...,
        [0.4598, 0.4478, 0.9167, ..., 0.8179, 0.7026, 0.5000],
        [0.5000, 0.5370, 0.4786, ..., 0.4529, 0.3132, 0.4245],
        [0.4134, 0.5000, 0.4898, ..., 0.4799, 0.5000, 0.7334]],
       device=&#39;cuda:0&#39;, grad_fn=)

所以这两个张量都有一个计算图...所有这些张量也将 requires_grad_ 设置为 True。
关于自动编码器架构，它只是一个又一个的线性层，我的前向函数如下所示：
defforward(self, x)：
    编码 = self.encoder(x)
    解码 = self.decoder(编码)
    返回解码后的、编码后的

至于训练部分：
x_batch = torch.tensor(tr_model.X_train[b]).to(self.device)

x_batch.requires_grad_(True)
x_batch.retain_grad()

# h 是编码器的输出
x_pred_batch, h = tr_model.model.forward(x_batch)

# 这里我们计算收缩损失
损失 = tr_model.compute_model_loss(x_pred_batch, x_batch, h)

如果有帮助，当我使用普通自动编码器执行 loss.backward() 时，一切正常。 （虽然我在那里不使用编码器的输出）
非常感谢您阅读本文！！]]></description>
      <guid>https://stackoverflow.com/questions/78263388/autograd-returning-none</guid>
      <pubDate>Tue, 02 Apr 2024 18:36:19 GMT</pubDate>
    </item>
    <item>
      <title>如何修复此错误没有名为“llama_index.llms.llama_cpp”的模块</title>
      <link>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</link>
      <description><![CDATA[我尝试将 mixtral-8x7b 与我自己的数据一起使用，但没有成功。这是我的代码
导入火炬
从 llama_index.llms.llama_cpp 导入 LlamaCPP
从 llama_index.llms.llama_cpp.llama_utils 导入 messages_to_prompt、completion_to_prompt
llm = 骆驼CPP(
    model_url=None, # 我们将在本地加载。
    model_path=&#39;./Models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#39;, # 4 位模型
    温度=0.1，
    max_new_tokens=1024, # 增加以支持更长的响应
    context_window=8192, # Mistral7B 有一个 8K 上下文窗口
    生成_kwargs={},
    # 至少设置为 1 才能使用 GPU
    model_kwargs={“n_gpu_layers”: 40}, # 40 对于 RTX 3090 来说是一个很好的层数，如果您的 VRAM 少于 24GB，您可能需要减少层数
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    详细=真
）

这给出了错误“没有名为“llama_index.llms.llama_cpp”的模块”。
我已经安装了 llama_index，使用了我的 MAC Mini 以及 Google Colab 的 GPU
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</guid>
      <pubDate>Tue, 02 Apr 2024 17:10:19 GMT</pubDate>
    </item>
    <item>
      <title>视觉变压器：运行时错误：mat1和mat2形状不能相乘（32x1000和768x32）[关闭]</title>
      <link>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</link>
      <description><![CDATA[我正在尝试对视觉变换器模型进行回归，但无法用回归层替换最后一层分类
类RegressionViT(nn.Module)：
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(预训练=预训练)
        # 从 vit_b_16 访问实际输出特征尺寸
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def 前向（自身，x）：
        x = self.vit_b_16(x)
        x = self.regressor(x)
        返回x


＃ 模型
模型 = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型.to（设备）

criteria = nn.MSELoss() # 使用适当的损失函数进行回归
优化器 = optim.Adam(model.parameters(), lr=0.0001)


当我尝试初始化并运行模型时收到此错误
运行时错误：mat1 和 mat2 形状无法相乘（32x1000 和 768x32）

问题是回归层和vit_b_16模型层之间不匹配，解决此问题的正确方法是什么]]></description>
      <guid>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</guid>
      <pubDate>Mon, 01 Apr 2024 06:33:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 数据加载器中的 Snuffle</title>
      <link>https://stackoverflow.com/questions/78248552/snuffle-in-pytorch-dataloader</link>
      <description><![CDATA[我在 PyTorch 中有一个返回文本和图像的自定义数据集。我有一个关于数据加载器中的鼻烟的问题。他能不能把文字和对应的图片混合起来，也就是让图片和文字不匹配？
我不知道我能用它做什么]]></description>
      <guid>https://stackoverflow.com/questions/78248552/snuffle-in-pytorch-dataloader</guid>
      <pubDate>Sat, 30 Mar 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用序列设置数组元素。尝试用 Python 制作星空图</title>
      <link>https://stackoverflow.com/questions/78245302/valueerror-setting-an-array-element-with-a-sequence-trying-to-make-a-skymap-in</link>
      <description><![CDATA[# 日期时间库
从日期时间导入日期时间
从 geopy.geocoders 导入 Nominatim
从 tzwhere 导入 tzwhere
从 pytz 导入时区，UTC

# matplotlib 帮助显示我们的星图
将 matplotlib.pyplot 导入为 plt
# 恒星数据的天空场
从 skyfield.api 导入 Star、负载、wgs84
从 skyfield.data 导入 hipparcos
从 skyfield.projections 导入 build_steregraphic_projection

# de421 显示地球和太阳在太空中的位置
eph = 负载(&#39;de421.bsp&#39;)
# hipparcos 数据集包含恒星位置数据
将 load.open(hipparcos.URL) 作为 f：
星星= hipparcos.load_dataframe(f)
位置 = &#39;纽约时代广场，纽约&#39;
当 = &#39;2023-01-01 00:00&#39;

定位器 = Nominatim(user_agent=&#39;myGeocoder&#39;)
位置 = locator.geocode(位置)
lat, long = location.latitude, location.longitude 代码在这里
 # 将日期字符串转换为日期时间对象
 dt = datetime.strptime(当, &#39;%Y-%m-%d %H:%M&#39;)
 导入pytz
 从 pytz 导入时区，UTC
 # 定义日期时间并根据我们的时区转换为 utc
 #def 时区（纬度、经度、DT）：
 tzw = tzwhere.tzwhere()
 timezone_str = tzw.tzNameAt(纬度, 经度)
 本地 = pytz.timezone(timezone_str)
 #tzw = tzwhere.tzwhere()
 #timezone_str = tzw.tzNameAt(纬度, 经度)
 #local = pytz.local(timezone_str)

# 从本地时区和日期时间获取UTC
#dt = datetime.strptime(when, &#39;%Y-%m-%d %H:%M&#39;)
local_dt = local.localize(dt, is_dst=None)
utc_dt = local_dt.astimezone(utc)
打印（utc_dt）&#39;

上面的代码给出了这个错误。
ValueError：使用序列设置数组元素。请求的数组在二维后具有不均匀的形状。检测到的形状为(1, 2)+不均匀部分。
错误位于 tzw = tzwhere.tzwhere() 行
我应该如何纠正这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78245302/valueerror-setting-an-array-element-with-a-sequence-trying-to-make-a-skymap-in</guid>
      <pubDate>Fri, 29 Mar 2024 18:14:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 3 中预先创建的批量图像数据训练分割模型</title>
      <link>https://stackoverflow.com/questions/78157765/training-a-segmentation-model-with-pre-created-batches-of-image-data-in-keras-3</link>
      <description><![CDATA[我有一个用例，我需要在将数据输入模型进行训练之前手动生成批量数据。
假设我有(256 x 100 x 100)图像，而我手动创建的批次的尺寸为(32 x 100 x 100)。如果我的训练集按 [batch_1, batch_2, ... batch_8] 的顺序排列，则得到原始形状 (256 x 100 x 100)。
当我将其提供给 model.fit() 时，将批量大小指定为 32，并指定 shuffle=False，将会模型正确地获取我的每个手动批次以按顺序进行训练，而不会混合每个批次的数据点并创建大小为 32 的新批次？
我正在使用Keras 3.0.1。]]></description>
      <guid>https://stackoverflow.com/questions/78157765/training-a-segmentation-model-with-pre-created-batches-of-image-data-in-keras-3</guid>
      <pubDate>Thu, 14 Mar 2024 02:23:09 GMT</pubDate>
    </item>
    <item>
      <title>检测两个相似图像之间差异的模型</title>
      <link>https://stackoverflow.com/questions/77442436/a-model-to-detect-the-differences-between-2-similar-images</link>
      <description><![CDATA[在我的工作中，我遇到了以下机器学习问题：
我的产品正在对半导体元件进行检查。产品将从组件经过的机器接收以下输入。机器将为我提供：

刚刚检查过的组件的图像。 （我们称之为检查图像）
处于理想形状和状态的组件的图像。 （我们称之为模板）
所检查图像的 3D 深度图。
模板的 3D 深度图。

请注意，即使检查的图像具有理想的形状和状态，检查的图像和模板也不可能相同。这是因为拍摄这些图像时相机移动非常常见且无法避免
我想训练一个模型，它可以将模板+检查的图像作为输入，并能够将这两个图像之间的差异绑定到检查的图像上。我想要检测的差异是特定的，并且仅与材料变化相关 - 而不是其他差异，例如颜色变化 - 在检查的图像上有额外的材料，例如组件中的额外焊料或裂纹。
什么样的模型/架构可以用于解决这个问题？以及如何选择此类问题的损失函数？
我尝试了 Siamese 网络，但在我的情况下它有 2 个缺陷：

当对明显差异和微小差异进行混合训练时，预测模式下的模型只能检测主要差异。如果模板和检查图像只有微小差异，则模型无法检测到这两个图像之间存在问题。
我希望我的模型也关注微小的差异，这就是为什么我提出了差异边界框的想法，而暹罗模型无法输出这一点。

另外，当我尝试一些图像处理库来检测差异时，结果很幼稚，因为在生产线上，由于相机的移动，模板和检查的图像不可能完全相同。]]></description>
      <guid>https://stackoverflow.com/questions/77442436/a-model-to-detect-the-differences-between-2-similar-images</guid>
      <pubDate>Wed, 08 Nov 2023 02:02:04 GMT</pubDate>
    </item>
    <item>
      <title>pytorch .stack .squeeze后的最终形状</title>
      <link>https://stackoverflow.com/questions/51851966/pytorch-stack-final-shape-after-squeeze</link>
      <description><![CDATA[我有一个 200 列 x 2500 行的 pandas 数据框，我将其转换为张量 
张量 = torch.tensor(df.values)
张量.size() =&gt; ([2500,200])

我将其分块并枚举

&lt;前&gt;&lt;代码&gt;列表=[]
对于 i，枚举中的块（tensor.chunk（100，dim = 0））
    chunk.size =&gt;([25,200])
    输出=隐藏层（块）
    输出.size() =&gt; ([25,1])
    列表+=输出

块被输入到一些层并作为 1 个特征张量输出。所以现在我有一个 100 个张量的列表，每个张量有 25 个 1、100x25x1 的块
所以我
stacked = torch.stack(list, 1).squeeze(2)
stacked.size()=([25,100])

我已经尝试过堆叠和挤压，但我似乎无法回到我想要的 ([2500,1]) 。我错过了什么吗？如果您能快速帮助我了解堆叠和挤压的作用以及为什么它对我不起作用，我将永远感激您！谢谢]]></description>
      <guid>https://stackoverflow.com/questions/51851966/pytorch-stack-final-shape-after-squeeze</guid>
      <pubDate>Wed, 15 Aug 2018 02:01:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：如何解决 RuntimeError：就地操作只能用于不与任何其他变量共享存储的变量</title>
      <link>https://stackoverflow.com/questions/45693586/pytorch-how-to-get-around-the-runtimeerror-in-place-operations-can-be-only-use</link>
      <description><![CDATA[使用 PyTorch，我在使用两个变量进行操作时遇到问题：
sub_patch : [torch.FloatTensor 大小 9x9x32]

pred_pa​​tch : [torch.FloatTensor 大小 5x5x32]

sub_patch是torch.zeros创建的变量
pred_pa​​tch 是一个变量，我使用嵌套 for 循环对其中的 25 个节点进行索引，并与其相应的大小为 [5,5,32] 的唯一过滤器 (sub_filt_patch) 相乘。结果被添加到 sub_patch 中的相应位置。
这是我的一段代码：
对于范围内的 i(filter_sz)：
    对于范围内的j（filter_sz）：

        # 从滤波器张量中索引正确的滤波器
        sub_filt_col = (patch_col + j) * filter_sz
        sub_filt_row = (patch_row + i) * filter_sz

        sub_filt_patch = sub_filt[sub_filt_row:(sub_filt_row + filter_sz), sub_filt_col:(sub_filt_col+filter_sz), :]

        # 将过滤器和 pred_pa​​tch 相乘并求和到子补丁上
        sub_patch[i:(i + filter_sz), j:(j + filter_sz), :] += (sub_filt_patch * pred_pa​​tch[i,j]).sum(dim=3)

我从这段代码的底行得到的错误是
运行时错误：就地操作只能用于不与任何其他变量共享存储的变量，但检测到有 2 个对象共享它

我明白为什么会发生这种情况，因为 sub_patch 是一个变量，而 pred_pa​​tch 也是一个变量，但是我怎样才能解决这个错误呢？任何帮助将不胜感激！
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/45693586/pytorch-how-to-get-around-the-runtimeerror-in-place-operations-can-be-only-use</guid>
      <pubDate>Tue, 15 Aug 2017 13:12:39 GMT</pubDate>
    </item>
    <item>
      <title>pytorch Network.parameters() 缺少 1 个必需的位置参数：'self'</title>
      <link>https://stackoverflow.com/questions/43779500/pytorch-network-parameters-missing-1-required-positional-argument-self</link>
      <description><![CDATA[当我在主函数的这一行中调用 pytorch 中的 Network.parameters() 时出现问题：
优化器= optim.SGD(Network.parameters(),lr=0.001,momentum=0.9)
我收到错误代码：
类型错误：parameters() 缺少 1 个必需的位置参数：&#39;self&#39;
我的网络在此类中定义
类网络（nn.Module）：
def __init__(自身):
    超级（网络，自我）.__init__()
    self.conv1 = nn.Conv2d(1, 32, 5)
    self.pool = nn.MaxPool2d(2, 2)
    self.conv2 = nn.Conv2d(32, 64, 5)
    self.pool2 = nn.MaxPool2d(2, 2)
    self.conv3 = nn.Conv2d(64, 64, 5)
    self.pool2 = nn.MaxPool2d(2, 2)
    self.fc1 = nn.Linear(64 * 5 * 5, 512)
    self.fc2 = nn.Linear(512, 640)
    self.fc3 = nn.Linear(640, 3756)

def 前向（自身，x）：
    x = self.pool(F.relu(self.conv(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    x = x.view(-1, 64 * 5 * 5)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    返回x

非常确定我正确导入了所有火炬模块。
你知道我在这里做错了什么吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/43779500/pytorch-network-parameters-missing-1-required-positional-argument-self</guid>
      <pubDate>Thu, 04 May 2017 09:50:43 GMT</pubDate>
    </item>
    </channel>
</rss>