<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 11 Oct 2024 06:25:06 GMT</lastBuildDate>
    <item>
      <title>网格搜索为带有 LOGO 或 LOO 的 best_score 给出 nan，而不是 k 倍 CV</title>
      <link>https://stackoverflow.com/questions/79076968/grid-search-gives-nan-for-best-score-with-logo-or-loo-not-k-fold-cv</link>
      <description><![CDATA[大家好
我有一个使用网格搜索的 nan R2 分数问题。
FDODB=pd.read_excel(&#39;Final Training Set for LOGO.xlsx&#39;)
array = FDODB.values
X = array[:,2:126]
Y = array[:,1]
Compd = array[:,0]

scaler = StandardScaler()
X = scaler.fit_transform(X)

params = {
&#39;max_depth&#39;: [5,7,9],
&#39;learning_rate&#39;: [0.03,0.05,0.07],
&#39;n_estimators&#39;: [200,300,400],
&#39;min_child_weight&#39;: [5,7,9],
&#39;subsample&#39;: [0.3, 0.5, 0.7],
&#39;base_score&#39;：[0.4, 0.5, 0.6]
}

xgb_reg = XGBRegressor(tree_method=&#39;hist&#39;, device=&#39;cuda&#39;)
logo = LeaveOneGroupOut()

grid_search = GridSearchCV(
estimator=xgb_reg,
param_grid=params,
scoring=&#39;r2&#39;,
error_score=&quot;raise&quot;,
cv=logo,
verbose=2,
n_jobs=-1,
return_train_score=True
)

grid_search.fit(X, Y, groups=Compd)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(&#39;最佳超参数：&#39;, best_params)
print(&#39;最佳 R2 分数：&#39;, best_score)

c:\Anaconda\envs\machinelearning\Lib\site-packages\sklearn\model_selection_search.py​​:1102: UserWarning: 一个或多个测试分数是非有限的：[nan]
warnings.warn(
最佳超参数：{&#39;base_score&#39;: 0.5, &#39;learning_rate&#39;: 0.03, &#39;max_depth&#39;: 8, &#39;min_child_weight&#39;: 6, &#39;n_estimators&#39;: 200, &#39;subsample&#39;: 0.3}
最佳 R2 分数：nan
这是我的代码。当我使用 10 倍 CV 进行网格搜索时，我无法获得 R2 分数。但每当我尝试 LOO 或 LOGO CV 时，所有 R2 分数都只是 nan。我检查了我的训练集是否有 nan 值，但结果没问题。你能帮我看看可能的原因是什么以及如何解决吗？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79076968/grid-search-gives-nan-for-best-score-with-logo-or-loo-not-k-fold-cv</guid>
      <pubDate>Fri, 11 Oct 2024 06:01:41 GMT</pubDate>
    </item>
    <item>
      <title>将 TensorFlow 权重和模型转换为 PyTorch（修改后的 EffectiveNet）</title>
      <link>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</link>
      <description><![CDATA[我正尝试在 pytorch 中模拟一个经过修改的 efficientnet TF 模型。我在 pytorch 中对模型进行了架构更改，转储了 TF 模型权重，然后将其重新加载到新的 pytorch 模型中。使用以下代码在 TF 中转储权重：
model = tf.saved_model.load(model_path)
ws = []
for i in range(len(model.variables)):
ws.append((i, model.variables[i].name, model.variables[i].numpy()))

with open(&quot;manually_dumped_contentnet_weights.pkl&quot;, &quot;wb&quot;) as ofile:
pickle.dump(ws, ofile)

pytorch 中的权重形状似乎与架构和导入的权重相匹配（在 conv2d 和深度 conv2d 之间进行转换之后）。我可以毫无错误地运行模型。但输出结果与 TF 模型的输出结果大不相同。
我注意到在 TF 代码中，模型不是直接加载的，而是在 tf Session 中加载的：
with Session(graph=Graph(), config=ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:
saved_model.loader.load(sess, [saved_model.tag_constants.SERVING], model_path)
patch_feature, patch_label = sess.run(output_nodes,feed_dict={input_node: patch})

现在我想知道我最初转储模型权重的尝试是否做得不正确。或者如果我遗漏了其他内容。
我在加载数据时进行的转置是 conv2d 的 (3,2,0,1) 和深度 conv2d 的 (2,3,0,1)：
def reload_conv2d(layer, weights):
### weights 是一个元组，其中每个元素都由一个三元组组成：(1) 索引号，(2) TF 中权重转储的层的名称，以及 (3) 权重
count = 0
if (
&quot;/conv2d/kernel&quot; not in weights[0][1]
and &quot;/conv2d_1/kernel&quot; not in weights[0][1]
and &quot;depthwise_conv2d/depthwise_kernel&quot; not in weights[0][1]
and &quot;final_conv2d/final_conv2d&quot; 不在 weights[0][1] 中 :
raise ValueError(
f&quot;需要在第一个索引上有 conv2d/kernel，但得到了 {weights[0][1]}&quot;
)
transpose_shape = (2,3,0,1) if &quot;depthwise&quot;在 weights[0][1] 中否则（3、2、0、1）
transposed_weights = torch.from_numpy（weights[0][2].transpose（transpose_shape[0]、transpose_shape[1]、transpose_shape[2]、transpose_shape[3]））
layer.weight.data = transposed_weights
count += 1
如果 layer.bias 不是 None 或 layer.bias:
如果（
&quot;/conv2d/bias&quot; 不在 weights[1][1] 中
并且 &quot;/conv2d_1/bias&quot; 不在 weights[1][1] 中
）：
引发 ValueError（
f&quot;需要在第二个索引上有 conv2d/bias 但得到了 {weights[1][1]}&quot;
)
layer.bias.data = (
torch.from_numpy(weights[1][2])
如果type(weights[1][2]) == np.ndarray
else torch.from_numpy(weights[1][2])
)
count += 1
return layer, count

我不知道为什么在相同的输入上，pytorch 和 TF 模型会给出完全不同的结果。无论是由于权重倾倒，还是权重加载……或者是模型架构变化？输入 TF 权重（在模型更改和转置之后）加载良好，我可以毫无问题地运行模型，这一事实对调试它没有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</guid>
      <pubDate>Thu, 10 Oct 2024 23:37:12 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 yolo 自定义模型时，data.yaml 文件中的相对路径出现问题</title>
      <link>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</link>
      <description><![CDATA[我正在尝试创建一个训练管道，以使用用户输入的带标签图像来训练自定义 yolov9 模型。
我遇到了一个问题，如果我使用相对路径来设置 data.yaml 文件，就会出现错误：
 RuntimeError：数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”错误
数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”图像未找到，缺少路径“C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val”

更奇怪的是，错误路径提及，
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

不是存在或正在任何地方请求的路径。实际路径是
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

由于某种原因，它重复了路径的第一部分 3 次。
这是 data.yaml 文件：
 path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

这是开始训练的代码：

def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
# 指定训练运行的保存目录
save_dir = &#39;OIT_model/customOIT/yolocustomtrainoutput&#39;
if os.path.exists(save_dir):
for file in os.listdir(save_dir):
file_path = os.path.join(save_dir, file)
if os.path.isfile(file_path) or os.path.islink(file_path):
os.unlink(file_path)
elif os.path.isdir(file_path):
shutter.rmtree(file_path)
os.makedirs(save_dir, exist_ok=True)
model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
return
train_custom_dataset_yolo(&#39;OIT_model/customOIT/customdatasetyolo/data.yaml&#39;, epochs=1,imgsz=64, verbose=True)

然而，非常奇怪的是，当我用绝对路径替换相对路径时，如下所示：
 path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

训练没有问题。对于我来说，使用绝对路径不是一个选择，因为这个应用程序需要在其他机器上可重现。]]></description>
      <guid>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</guid>
      <pubDate>Thu, 10 Oct 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>卷积-反卷积，同时保持原始图像大小</title>
      <link>https://stackoverflow.com/questions/79074519/convolution-deconvolution-while-maintaining-the-original-image-size</link>
      <description><![CDATA[我正在尝试在 tensorflow 中实现 pspnet。
它需要一个池化模块来接收输入，以及几个内核大小：

使用每个内核AveragePooling2D对输入进行平均池化

进行 1x1 卷积，之后使用 UpSampling2D


最后将所有不同的 conv-deconv 输出连接在一起并前馈
def pyramid_pooling_module(x, pool_sizes):
pool_outputs = []
for pool_size in pool_sizes:
pooled=layers.AveragePooling2D(pool_size)(x)
pooled=layers.Conv2D(512, (1,1), padding=&#39;same&#39;)(pooled)
pooled=layers.UpSampling2D(size=pool_size, interpolation=&#39;bilinear&#39;)(pooled)
print(pool_size)
pool_outputs.append(pooled)
return layer.Concatenate()(pool_outputs)

输入的维度为 68, 120
因此，所使用的内核 (1x1, 2x2, 3x3, 6x6) 会与 3x3, 6x6 的平均池化层产生舍入误差
因此这些层的最终池化输出为 (66, 120)
我不确定如何修复此问题，是否应该将输入调整为可以被 6x6 整除的大小？还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79074519/convolution-deconvolution-while-maintaining-the-original-image-size</guid>
      <pubDate>Thu, 10 Oct 2024 13:00:23 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能改善姿势分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79074518/how-can-i-improve-pose-classification</link>
      <description><![CDATA[我使用 mediapipe 进行姿势检测，我想对运动姿势进行分类，但大多数姿势都很相似，我使用的是 tensorflow 本身提供的姿势分类。
我面临的问题是，对每一帧都进行分类，这也会导致错误检测，能够正确摆姿势，但也会导致错误检测。
结果是连续的，就像所需的动作（如 crikerter 击中六分）恰好在第 23 帧，并且被正确识别，但我在第 25 和第 26 帧也得到了错误检测的动作，我想避免这种情况
我尝试纠正数据，提供增强数据，增加过滤置信度，获得更多嵌入（如角度、距离等）。]]></description>
      <guid>https://stackoverflow.com/questions/79074518/how-can-i-improve-pose-classification</guid>
      <pubDate>Thu, 10 Oct 2024 12:59:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Swin Transformer V2 Backbone 在 PyTorch 上定制 Faster R-CNN 模型</title>
      <link>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</link>
      <description><![CDATA[对于我的对象检测项目，我一直在使用 fasterrcnn_resnet50_fpn_v2 模型。我的输入图像是高分辨率的（大约 3000 x 4000 像素），我将它们拼接成 1200 x 1600 像素的图块以进行训练和推理。但是，我很难用这个模型有效地检测小物体（小到 10 x 10 像素）。
在寻找替代方案时，我读到了 SwinTransformer V2，我发现它很有前途，尤其是对于高分辨率图像的应用程序。由于我的数据集中的所有图像尺寸也是 1200 x 1600，我不想缩小它们的尺寸，所以我想自定义 Faster R-CNN 以使用 Swin V2 主干，并可能添加 FPN 并实现 Cascade R-CNN 头。但是，我面临的挑战是骨干、颈部和 RPN 头部之间的尺寸不匹配。
这是我目前想到的（我决定使用基础模型）；
import torch
from torch import nn
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.swin_transformer import swin_v2_b, Swin_V2_B_Weights
from torchvision.ops import MultiScaleRoIAlign
import torchvision.transforms as transforms
import request
from PIL import Image

NUM_CLASSES = 100 
trainable_layers = 2

class CustomSwin(nn.Module):
def __init__(self, backbone):
super().__init__()
self.backbone = backbone
self.out_channels = 1024

def forward(self, x):
return torch.permute(self.backbone(x), (0, 3, 1, 2))

backbone = swin_v2_b(weights=Swin_V2_B_Weights.DEFAULT)

# 删除分类头 
backbone.norm = nn.Identity()
backbone.permute = nn.Identity()
backbone.avgpool = nn.Identity()
backbone.flatten = nn.Identity()
backbone.head = nn.Identity()

# 冻结所有参数
for param in backbone.parameters():
param.requires_grad = False

# 取消冻结最后的 trainable_layers
for layer in list(backbone.features)[-trainable_layers:]:
for param in layer.parameters():
param.requires_grad = True

custom_backbone = CustomSwin(backbone)

# 为非常小的物体添加较小的尺寸
anchor_generator = AnchorGenerator(
sizes=((8, 16, 32, 64, 128, 256, 512),), aspects_ratios=((0.5, 1.0, 2.0),)
)

roi_pooler = MultiScaleRoIAlign(featmap_names=[&quot;0&quot;], output_size=7, samples_ratio=2)

model = FasterRCNN(
custom_backbone,
num_classes=NUM_CLASSES,
rpn_anchor_generator=anchor_generator,
box_roi_pool=roi_pooler,
min_size=1224, 
max_size=1632,
)

我不确定当前的实现是否是最佳的，或者添加 FPN（特征金字塔网络）或 Cascade R-CNN 等组件是否会增强模型的性能（我有一个相当大的数据集）。有人成功实施了这些修改吗？任何指导都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</guid>
      <pubDate>Thu, 10 Oct 2024 11:07:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PyTorch 调度程序似乎不能正常工作？</title>
      <link>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</link>
      <description><![CDATA[我正在尝试使用一个简单的 PyTorch Scheduler 来训练 mobileNetV3Large。
这是负责训练的代码部分：
bench_val_loss = 1000
bench_acc = 0.0
epochs = 15
optimizer = optim.Adam(embeddingNet.parameters(), lr=1e-3) 
loss_optimizer = torch.optim.Adam(loss_fn.parameters(), lr=1e-3)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, waiting=3, Threshold=0.02)

for epoch in range(1, epochs + 1):

print(f&#39;current lr: {scheduler.get_last_lr()}&#39;)
loss=train(embeddingNet, loss_fn, device, train_dataloader, optimizer, loss_optimizer, epoch)
val_loss，准确率 =test(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)
#val_loss = simpleTest(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)

torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceLAST.pth&#39;)

如果准确率 &gt;= bench_acc:
bench_val_loss = val_loss
torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceBEST.pth&#39;)

scheduler.step(accuracy)

writer.add_scalars(&#39;训练与验证损失&#39;，
{&#39;训练&#39;：损失， &#39;Validation&#39;: val_loss},
global_step=epoch+1)

在这里您可以找到前 7 个训练日志
测试集准确率 (Precision@1) = 0.17834772304046048
当前 lr：[0.001]
Epoch 3：Loss = 39.68284225463867
Epoch 3：valLoss = 39.9765007019043
100%|██████████| 962/962 [01:43&lt;00:00, 9.28it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.92it/s]
计算准确率
测试集准确率 (Precision@1) = 0.31242593533096324
当前 lr: [0.001]
Epoch 4: Loss = 39.4412841796875
Epoch 4: valLoss = 39.67761562450512
100%|██████████| 962/962 [01:45&lt;00:00, 9.11it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.86it/s]
计算准确率
测试集准确率 (Precision@1) = 0.3633824276282377
当前 lr: [0.001]
Epoch 5: Loss = 39.09823989868164
Epoch 5: valLoss = 39.54649614901156
100%|██████████| 962/962 [01:42&lt;00:00, 9.37it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.87it/s]
计算准确率
测试集准确率 (Precision@1) = 0.44244117149145085
当前 lr: [0.001]
Epoch 6: Loss = 38.70449447631836
Epoch 6: valLoss = 39.1865906792718
100%|██████████| 962/962 [01:45&lt;00:00, 9.15it/s]
100%|██████████| 370/370 [00:39&lt;00:00, 9.25it/s]
计算准确率
测试集准确率 (Precision@1) = 0.5167597765363129
当前 lr: [0.0001]

我不明白为什么调度程序决定降低学习率，即使准确率的增长速度比阈值更快。
错误在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</guid>
      <pubDate>Thu, 10 Oct 2024 08:51:32 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Keras 面部识别计算机视觉图像分类模型，准确率高 (95%)/验证率低 (0%)</title>
      <link>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</guid>
      <pubDate>Thu, 10 Oct 2024 08:13:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用深度学习来解决由合成数据组成的拼图游戏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</link>
      <description><![CDATA[我花了一些时间研究 Python 中的拼图生成器，该生成器接收图像、行数 (M) 和列数 (N)，并将原始图像分解为 M*N 个 png 图像块输出到文件夹中。这些图像是正方形，带有不规则形状的制表符和空格，因此每个块只能放在一个位置。
接下来，我想创建一个拼图解算器来接收这些 PNG 图像，提取关键特征并确定它们的位置。
这里是图像的示例。如果您对它的实现方式感到好奇，也可以查看生成器代码。
我遇到过制作拼图解算器的不同方法，但我对使用深度学习感兴趣。我的主要问题是我不知道从哪里开始。截至目前，这些碎片没有任何旋转，但我希望将来能够解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</guid>
      <pubDate>Wed, 09 Oct 2024 22:50:24 GMT</pubDate>
    </item>
    <item>
      <title>当批处理大小不等于 1 时，UNet 执行过程中会出现错误</title>
      <link>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</link>
      <description><![CDATA[我尝试使用 DDIM 反演教程中提供的代码运行稳定扩散模型。但是，当输入的批处理大小设置为大于 1 的值（例如 32）时，我遇到以下错误：
RuntimeError：张量 a (131072) 的大小必须与非单例维度 1 上的张量 
b (4096) 的大小匹配。

看来 131072 可能来自 32 x 4096，表明张量维度不匹配。发生错误的具体行是：
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample

这是我的代码中与反演过程相关的部分：
## 反演 (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
def invert_process(self, guide_scale, input, denoise_kwargs):

pred_images = []
pred_latents = []

decrypt_kwargs = {&#39;vae&#39;: self.vae}

# 反转时间步&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
timesteps = reversed(self.scheduler.timesteps)
num_inference_steps = len(self.scheduler.timesteps)

with torch.no_grad():
for i in tqdm(range(0, num_inference_steps)):

t = timesteps[i]
self.cur_t = t.item()

# 对于稳定扩散的文本条件
if &#39;encoder_hidden_​​states&#39; in denoise_kwargs.keys():
bs = denoise_kwargs[&#39;encoder_hidden_​​states&#39;].shape[0]
input = torch.cat([input] * bs)

# 预测噪声残差
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
noise_pred = noisy_residual

#对于稳定扩散的文本条件
if noisy_residual.shape[0] == 2:
# 执行指导
noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
noisy_residual = noise_pred_uncond + guide_scale * (noise_pred_text - noise_pred_uncond)
input, _ = input.chunk(2)

current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

latents = input

# 反转更新步骤（重新安排更新步骤以获得 x(t)（新潜伏）作为 x(t-1)（当前潜伏）的函数
# 向潜伏添加噪声

latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred

input = latents

pred_latents.append(latents)
pred_images.append(decode_latent(latents, **decode_kwargs))

return pred_images, pred_latents


可能导致当批量大小大于 1 时，张量大小不匹配？如何在模型中保持批量大小大于 1 的同时解决此问题？
我尝试将 t 的大小更改为形状为 (批量大小,) 的张量。
此外，我确认当批量大小为 1 时模型可以正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</guid>
      <pubDate>Wed, 09 Oct 2024 16:32:07 GMT</pubDate>
    </item>
    <item>
      <title>错误：所有估算器都应实现拟合和变换，或者在使用 make_column_transformer 时可以使用“drop”或“passthrough”说明符</title>
      <link>https://stackoverflow.com/questions/71566189/error-all-estimators-should-implement-fit-and-transform-or-can-be-drop-or</link>
      <description><![CDATA[我正在尝试实现一个使用 ColumnTransformer() 和 SVC() 的模型。
我的转换方法如下所示：
num_features = X_train_svm.select_dtypes(include=np.number).columns.to_list()
cat_features = X_train_svm.select_dtypes(include=[&#39;object&#39;]).columns.to_list()

transform1 = make_column_transformer([(StandardScaler(), num_features),
(OneHotEncoder(), cat_features)
])

后面跟着一个管道：
pipe = make_pipeline(transform1, svm.SVC())

然后当我尝试拟合训练和测试数据时:
pipe.fit(X_train, y_train)

我收到错误 :: TypeError：所有估算器都应实现 fit 和 transform，或者可以是“drop”或“passthrough”说明符。&#39;(StandardScaler(), [&#39;Height&#39;, &#39;Age&#39;, &#39;Weight&#39;, &#39;Quantity&#39;, &#39;Cost&#39;])&#39; (type &lt;class &#39;tuple&#39;&gt;) 不适用。
请帮我修复此错误。
我尝试将缩放器更改为 Ordinal Scaler，尝试使用 (-1,1) 重塑数据，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/71566189/error-all-estimators-should-implement-fit-and-transform-or-can-be-drop-or</guid>
      <pubDate>Tue, 22 Mar 2022 02:13:21 GMT</pubDate>
    </item>
    <item>
      <title>如何调整随机森林模型中的特征权重？</title>
      <link>https://stackoverflow.com/questions/67417292/how-to-adjust-feature-weights-in-random-forest-model</link>
      <description><![CDATA[我正在使用 Scikit-Learn 的随机森林库，我想知道是否可以更改特征权重，以便特定特征产生更大的影响。我查看了随机森林文档，但我只看到我不感兴趣的类别的权重变化。
除了重写代码本身之外，还有其他方法可以做到这一点吗？任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/67417292/how-to-adjust-feature-weights-in-random-forest-model</guid>
      <pubDate>Thu, 06 May 2021 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>添加我自己的密集层后，vgg16模型的可训练参数发生了变化</title>
      <link>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</link>
      <description><![CDATA[vgg16_model = tf.keras.applications.vgg16.VGG16()

model= Sequential()

for layer in vgg16_model.layers[:-1]:

model.add(layer)

model.summary() #最后一个密集层到现在为止已被移除 


for layer in model.layers:

layer.trainable=False #对于迁移学习，我已冻结了这些层

model.add(Dense(2,activation=&#39;softmax&#39;))

model.summary() #现在当我添加密集层时，模型的可训练参数会发生变化

]]></description>
      <guid>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</guid>
      <pubDate>Sun, 10 Jan 2021 07:39:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 迁移学习的边界框回归准确率为 0%。具有 Sigmoid 激活的输出层仅输出 0 或 1</title>
      <link>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</link>
      <description><![CDATA[我正在尝试创建一个对象定位模型来检测汽车图像中的车牌。我使用了 VGG16 模型并排除了顶层以添加我自己的密集层，最后一层有 4 个节点和 S 形激活以获得 (xmin、ymin、xmax、ymax)。
我使用 keras 提供的函数读取图像，并将其调整为 (224, 244, 3)，还使用 ​​preprocess_input() 函数来处理输入。我还尝试通过使用填充调整大小来手动处理图像以保持比例，并通过除以 255 对输入进行规范化。
当我训练时，似乎什么都不起作用。我的训练和测试准确率为 0%。下面是我为该模型编写的代码。
def get_custom(output_size, optimizer, loss):

vgg = VGG16(weights=&quot;imagenet&quot;, include_top=False, input_tensor=Input(shape=IMG_DIMS))

vgg.trainable = False

flatten = vgg.output
flatten = Flatten()(flatten)

bboxHead = Dense(128,activation=&quot;relu&quot;)(flatten)
bboxHead = Dense(32,activation=&quot;relu&quot;)(bboxHead)

bboxHead = Dense(output_size,activation=&quot;sigmoid&quot;)(bboxHead)

model = Model(inputs=vgg.input,outputs=bboxHead)
model.compile(loss=loss,optimizer=optimizer,metrics=[&#39;accuracy&#39;])

return模型

X 和 y 分别为 (616, 224, 224, 3) 和 (616, 4)。我将坐标除以相应边的长度，因此 y 中的每个值都在 (0,1) 范围内。
我将在下面链接我的 github 中的 python 笔记本，以便您可以看到完整的代码。我正在使用 google colab 来训练模型。
https://github.com/gauthamramesh3110/image_processing_scripts/blob/main/License_Plate_Detection.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</guid>
      <pubDate>Sat, 26 Dec 2020 18:24:41 GMT</pubDate>
    </item>
    <item>
      <title>Plotly：如何使用热图制作带注释的混淆矩阵？</title>
      <link>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</link>
      <description><![CDATA[我喜欢使用 Plotly 来可视化一切，我试图通过 Plotly 可视化混淆矩阵，这是我的代码：
def plot_confusion_matrix(y_true, y_pred, class_names):
fusion_matrix = metrics.confusion_matrix(y_true, y_pred)
confusion_matrix =fusion_matrix.astype(int)

layout = {
&quot;title&quot;: &quot;混淆矩阵&quot;,
&quot;xaxis&quot;: {&quot;title&quot;: &quot;预测值&quot;},
&quot;yaxis&quot;: {&quot;title&quot;: &quot;实际值&quot;}
}

fig = go.Figure(data=go.Heatmap(z=confusion_matrix,
x=class_names,
y=class_names,
hoverongaps=False),
layout=layout)
fig.show()

结果是

我怎样才能在相应的单元格内显示数字而不是悬停，像这样]]></description>
      <guid>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</guid>
      <pubDate>Thu, 26 Mar 2020 02:18:36 GMT</pubDate>
    </item>
    </channel>
</rss>