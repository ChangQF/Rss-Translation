<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 03 Feb 2024 15:12:45 GMT</lastBuildDate>
    <item>
      <title>在 Rust-linfa 中加载用于预测的线性回归模型</title>
      <link>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</link>
      <description><![CDATA[我对 Rust 比较陌生，一直在研究 linfa 在 Rust 中的机器学习，特别是线性回归模型。我希望能够保存和加载经过训练的线性回归模型，但我无法找到实现此目的的方法。
到目前为止，我的方法是获取训练中涉及的主要参数，这些参数可以从 linfa 的线性回归实现中获取，并将它们存储在一个可以存储为 JSON 文件的结构中（通过 serde_json 完成）。然而，在此之后我不知道如何将其加载回来进行训练。
以上内容详情如下：
存储训练参数的结构：
struct ModelJson {
    系数：Vec f64 ，
    拦截：f64，
}

存储过程：
let model = lin_reg.fit(&amp;dataset)?;
让 model_json = ModelJson {
    系数： model.params().to_vec(),
    拦截： model.intercept(),
};

存储的数据看起来如何：
{“系数”:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.00103006349345 99436]，“拦截”：50.525680622870084}

关于序列化和反序列化整个模型，我发现以下信息表明 linfa 中支持相同的操作。
加载和保存模型
这引出了我的第二种方法，其中我使用了 linfa-linear 的 serde 功能（包含 LinearRegression 模型），首先在我的 Cargo.toml 中包含以下内容：
linfa-clustering = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}
根据我对实现的理解，此功能为 LinearRegression 实现了以下功能：
Serde 序列化和反序列化实现 - 派生
上述实现：
&lt;前&gt;&lt;代码&gt;#[cfg_attr(
    特征=“serde”，
    派生（序列化，反序列化），
    serde(crate = “serde_crate”)
)]
/// 可用于进行预测的拟合线性回归模型。
pub struct FittedLinearRegression; {
    截距：F，
    参数：Array1,
}

发现于： linfa-线性导出实现
我的实现如下：
let model = lin_reg.fit(&amp;dataset)?;
让序列化 = serde_json::to_string(&amp;model).unwrap();

但是此方法出现以下错误：
不满足特征边界 `FittedLinearRegression: serde::ser::Serialize`
以下其他类型实现了特征 `serde::ser::Serialize`：
  布尔值
  字符
  大小
  i8
  i16
  i32
  i64
  i128
和其他 133 个rustcClick 以获取完整的编译器诊断
main.rs(82, 22)：此调用引入的绑定所需

是否有其他方法可以做到这一点，或者是否有某种方法可以使这些方法之一发挥作用？

阿莱霍
]]></description>
      <guid>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</guid>
      <pubDate>Sat, 03 Feb 2024 13:44:24 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的数据集，至少包含 100 列和 10,000 个原始数据</title>
      <link>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</link>
      <description><![CDATA[我想要一个至少包含 100 列和 10000 行的数据集。您不能使用任何编程语言来生成它，因为它将用于机器学习实践
提前感谢您的帮助🙏]]></description>
      <guid>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</guid>
      <pubDate>Sat, 03 Feb 2024 09:03:55 GMT</pubDate>
    </item>
    <item>
      <title>有人可以解释一下在其他层但不在第一层使用 BatchNorm2d 的目的吗</title>
      <link>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</link>
      <description><![CDATA[我正在设计Discriminator类，我在github上看到有人的实现，我无法向自己解释为什么batchnormalization被用在conv2，conv3中，但特别是在第一个卷积层中没有，我为您提供了代码类还有转换函数
类鉴别器（nn.Module）：
    def __init__(self, conv_dim = 32):
        super(鉴别器, self).__init__()

        self.conv_dim = conv_dim
        
        self.conv1 = conv(
            3、conv_dim、4、batch_norm = False
        ）
        self.conv2 = conv(
            转换亮度, 转换亮度 * 2, 4
        ）
        self.conv3 = conv(
            转换亮度 * 2, 转换亮度 * 4, 4
        ）
        self.fc = nn.Linear(
            卷积暗度 * 4 * 4 * 4, 1
        ）
    def 前向（自身，x）：
        leaky_relu = F.leaky_relu
        输出=leaky_relu（
            自转换1(x), 0.2
        ）
        输出=leaky_relu（
            self.conv2(输出), 0.2
        ）
        输出=leaky_relu（
            self.conv3(输出), 0.2
        ）
        输出 = out.view(-1, self.conv_dim * 4 * 4 * 4)
        输出 = self.fc(输出)
        返回

我尝试询问 ChatGPT，但它没有给出一致的答案]]></description>
      <guid>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</guid>
      <pubDate>Sat, 03 Feb 2024 08:35:38 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中使用神经网络进行动物检测</title>
      <link>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</link>
      <description><![CDATA[当我运行最后一部分来训练模型时，我无法检查图像是否有错误。正如你所看到的，我想检测我是否能够训练我的模型来检测动物，例如动物。猫和狗之间。 数据集。
将 pandas 导入为 pd
将 numpy 导入为 np
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D
从tensorflow.keras.layers导入MaxPooling2D
从tensorflow.keras.layers导入Flatten
从tensorflow.keras.layers导入Dense

# 初始化 CNN
分类器=顺序（）

＃卷积
classifier.add(Conv2D(32,(3,3), input_shape = (64, 64, 3), 激活 = &#39;relu&#39;))

# 池化
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 添加第二个卷积层
classifier.add(Conv2D(32, (3,3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 展平
分类器.add(Flatten())

# 全连接
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 1, 激活 = &#39;sigmoid&#39;))

# 编译 CNN
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标= [&#39;准确性&#39;]）

# 将 CNN 拟合到图像上

从 keras.preprocessing.image 导入 ImageDataGenerator
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

##### 这里我收到错误
分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)

错误：
]]></description>
      <guid>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</guid>
      <pubDate>Sat, 03 Feb 2024 05:55:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch CCN 勉强训练</title>
      <link>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</guid>
      <pubDate>Sat, 03 Feb 2024 05:53:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么变压器可以接受不同长度的输入？</title>
      <link>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</link>
      <description><![CDATA[我读了“你所需要的就是注意力”论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。并且使用屏蔽注意力来对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？
在我与 Chat-GPT 的第一次对话中，它告诉我我们使用 pad 令牌，而在第二次对话中，我们没有使用。我很困惑，我什至不知道该相信什么。]]></description>
      <guid>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</guid>
      <pubDate>Sat, 03 Feb 2024 04:28:27 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（大约每分钟 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量低于恒定条时，查询将运行并且可以很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
&lt;块引用&gt;
索引错误 - 列表索引超出范围

请注意，查询数量“bar”是发生此错误的时间取决于每个查询的长度，并且如果查询长度在程序执行期间保持相同，则该错误是一致的。例如，在尝试将查询长度增加大约 20% 后，查询长度从大约 330 个查询下降到大约 60 个查询。
&lt;块引用&gt;
文件
“/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，
第 1315 行，文本
返回 self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，该位置的配额应该为 300 个查询/分钟。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。
我写的Gemini Pro API调用函数是：
def gemini_response(message: str) -&gt; &gt;字符串：
    # 初始化顶点AI
    vertexai.init(project=“project-id-0123”, location=“us-central1”)

    # 加载模型
    模型 = GenerativeModel(“gemini-pro”)

    # 查询模型
    响应 = model.generate_content(消息)
    返回响应.文本
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CreateML 只检测到 JSON 文件中的一个类，而实际上我有 2 个类？</title>
      <link>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</link>
      <description><![CDATA[我正在开发一个对象检测机器学习模型，我决定使用 CreateML 来训练它。我希望我的模型能够区分苹果和橙子。
在开始训练之前，我必须提供我的“训练数据”到 CreateML，其中包括我的 JSON 文件（带有我的图像注释）。在我的 JSON 文件中，我希望模型能够检测到 2 个类（苹果和橙子）。
但由于某种原因，CreateML 仅检测类“apple”。 下面是我的 JSON 文件的一小部分的图像，其中包括“apple”的一个注释和“橙色”。
这是我的 JSON 注释的小图片，包括我的类“apple”和和“橙色”
此外，CreateML 表示我对标签“apple”的计数为 37。手动统计后，我发现这是不准确的；实际上，我对“apple”类的计数是 97。 下面是代表这一点的图像。
CreateML 向我展示了我的 JSON 文件计数。它只向我展示了“apple”类，但我也有一个“橙色”类。
最初，我的 JSON 文件中的一些注释具有不同的属性。例如，“坐标”可以是“坐标”。我的部分注释有时有整数值，有时有小数值。我意识到这可能会带来问题，所以我将它们全部更改为整数值。它最终没有成功。
此外，我还进行了一个小实验。因为我所有的“苹果”都是注释位于文件的最顶部，所有“橙色”都位于文件的最顶部。注释在底部，我翻转顺序看看结果。我把所有的“橙色”都放在了注释在顶部。令我惊讶的是，CreateML 仍然告诉我有一个类，但这次它说该类名为“orange！”为什么会出现这种情况？
有人听说过这个吗？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</guid>
      <pubDate>Sat, 03 Feb 2024 01:48:45 GMT</pubDate>
    </item>
    <item>
      <title>尝试在本地运行 falcon-40b 模型。需要帮助，模型没有给出任何输出，并且在 VS Code 上显示退出代码 1</title>
      <link>https://stackoverflow.com/questions/77930495/trying-to-run-falcon-40b-model-locally-need-help-the-model-is-not-giving-any-ou</link>
      <description><![CDATA[我有 Nvidia rtx 3060，并决定在我自己的项目中尝试一下。所以我已将所有模型文件下载到文件夹 ./Model 中，与 app.py 并行。我只是想在继续之前测试一下法学硕士。但是，当我尝试执行代码时，它不会产生任何输出，而且当我将鼠标悬停在 VScode 上执行的命令上时，它会显示 Command Executed now and failed (Exit Code 1)。
测试代码如下：
从变压器导入 AutoModelForCausalLM, AutoTokenizer
进口火炬

# 设置模型目录的路径
model_directory = &quot;./model&quot;;

# 从指定目录初始化分词器和模型
tokenizer = AutoTokenizer.from_pretrained(model_directory)
模型 = AutoModelForCausalLM.from_pretrained(model_directory)

# 确保模型正在使用 GPU
模型 = model.to(“cuda”)

# 定义提示
提示=“”“
&lt;人类&gt;：像我五岁一样解释一下LLMS
&lt;助理&gt;：
”“”

# 生成配置
生成配置={
    &quot;max_length&quot;: 200, # 调整生成token的最大长度
    “温度”: 0.7, # 温度控制随机性
    &quot;top_p&quot;: 0.7, # top_p控制核采样
    &quot;num_return_sequences&quot;: 1, # 要生成的序列数
    &quot;pad_token_id&quot;: tokenizer.eos_token_id, # 填充令牌
    &quot;eos_token_id&quot;: tokenizer.eos_token_id, # 序列结束标记
}

# 对提示进行编码
编码 = tokenizer(提示, return_tensors=“pt”).to(“cuda”)

# 使用编码的提示和生成配置生成响应
with torch.no_grad(): # 禁用梯度计算以节省内存并加快计算速度
    输出 = model.generate(
        input_ids=编码[“input_ids”],
        注意掩码=编码[“注意掩码”]，
        **生成_配置
    ）

# 将生成的 token 解码为文本
generated_text = tokenizer.decode(outputs[0],skip_special_tokens=True)

# 打印生成的文本
打印（生成的文本）

以下是 VScode 屏幕截图：
VScode 显示退出代码
我做错了什么以及如何解决这个问题？
我尝试执行上面给出的代码。我应该收到短信回复，但我什么也没收到。]]></description>
      <guid>https://stackoverflow.com/questions/77930495/trying-to-run-falcon-40b-model-locally-need-help-the-model-is-not-giving-any-ou</guid>
      <pubDate>Sat, 03 Feb 2024 00:38:50 GMT</pubDate>
    </item>
    <item>
      <title>如何修复一层的输出，使其与另一层兼容？</title>
      <link>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</link>
      <description><![CDATA[我的输入是由 21 个氨基酸标签编码的序列。它是一个包含 21 个元素 (1x21) 的数组。我想通过嵌入层然后通过卷积层（等等）将其提供给它。但它不允许我添加卷积层。我明白为什么（输出和输入的尺寸不匹配），但我不知道如何修复它。
这是我的代码：
&lt;前&gt;&lt;代码&gt;embeded_vector_size = 9
最大长度 = 21
vocab_size = len(标签)
模型=顺序（）
model.add（嵌入（vocab_size，embeded_vector_size，input_length = max_length，name =“嵌入”））
model.add(Conv2D(filters=64,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))

我收到此错误：
层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

完整回溯：
ValueError Traceback（最近一次调用最后一次）
[177] 中的单元格，第 7 行
      5 model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length, name=“embedding”))
      6 打印(模型.summary())
----&gt; 7 model.add(Conv2D(filters=3,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))
      8 # 打印(模型.summary())
      9 # 模型.add(MaxPooling2D())

文件 /opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py:253，在assert_input_compatibility(input_spec、inputs、layer_name)中
    第251章
    [252] 第252话规格.min_ndim:
--&gt;第253章
    254 f&#39;层“{layer_name}”的输入{input_index} &#39;
    255、“与层不兼容：”
    [256] 第256话
    257 f”发现 ndim={ndim}。 ”
    258 f“接收到完整形状：{tuple(shape)}”
    第259章）
    260 # 检查数据类型。
    第261章

ValueError：层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</guid>
      <pubDate>Fri, 02 Feb 2024 22:14:15 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 Gemini-Pro 制作 LLM 聊天机器人时出现类型错误</title>
      <link>https://stackoverflow.com/questions/77929619/type-error-when-trying-to-make-a-llm-chatbot-using-gemini-pro</link>
      <description><![CDATA[这是代码
导入操作系统

将streamlit导入为st
从 dotenv 导入 load_dotenv
将 google.generativeai 导入为 gen_ai

加载_dotenv()

st.set_page_config(
    page_title=&quot;与 Gemini Pro 聊天&quot;,
    page_icon=&quot;:大脑:&quot;,
    布局=“居中”
）

GOOGLE_API_KEY = os.getenv(“GOOGLE_API_KEY”)
gen_ai.configure(api_key=GOOGLE_API_KEY)
模型 = gen_ai.GenerativeModel(“gemini-pro”)

deftranslate_role_for_streamlit(user_role):
  如果 user_role == “模型”：
    返回“助手”
  别的：
    返回用户角色

如果“聊天会话”是不在 st.session_state 中：
  st.session_state.chat_session = model.start_chat(history=[])
st.title(“CAIE 机器人”)

对于 st.session_state.chat_session.history 中的消息：
  与 st.chat_message(translate_role_for_streamlit(message.role))：
    st.markdown(message.parts[0].text)

user_prompt = st.chat_input(“询问 CAIE 机器人...”)
如果用户提示：
  st.chat_message(“用户”).markdown(user_prompt)
  gemini_response = st.session_state.chat_session.send_message(user_prompt)
  与 st.chat_message(“助理”)：
    st.markdown(gemini_response.text)

!streamlit 运行 main.py

我在线路中遇到错误
如果“chat_session”是不在 st.session_state 中：

我想检查用户是否与机器人进行了活跃的聊天，如果是，那么机器人将保存之前的对话历史记录以供下一次对话使用。会话结束后，历史记录将重置]]></description>
      <guid>https://stackoverflow.com/questions/77929619/type-error-when-trying-to-make-a-llm-chatbot-using-gemini-pro</guid>
      <pubDate>Fri, 02 Feb 2024 20:09:24 GMT</pubDate>
    </item>
    <item>
      <title>epoch 中一批的梯度爆炸和 NaN 损失</title>
      <link>https://stackoverflow.com/questions/77929468/exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</link>
      <description><![CDATA[我正在尝试训练一个机器学习模型，在训练过程中，我在该纪元中的一个批次上不断收到 NaN（无限）损失，而所有其他批次都会产生合理的损失（通过跳过参数更新）该特定批次）。
&lt;前&gt;&lt;代码&gt;3027
losstensor(2.9372, device=&#39;cuda:0&#39;, grad_fn=)
3028
losstensor(2.9796, device=&#39;cuda:0&#39;, grad_fn=)
3029
losstensor(2.9189, device=&#39;cuda:0&#39;, grad_fn=)
3030
losstensor(2.9621, device=&#39;cuda:0&#39;, grad_fn=)
3031
losstensor(3.3539, device=&#39;cuda:0&#39;, grad_fn=)
3032
losstensor(2.8540, device=&#39;cuda:0&#39;, grad_fn=)
3033
losstensor(inf, device=&#39;cuda:0&#39;, grad_fn=) &lt;---------------------------------------- ----
3034
losstensor(3.0785, device=&#39;cuda:0&#39;, grad_fn=)
3035
losstensor(2.9671, device=&#39;cuda:0&#39;, grad_fn=)
3036
losstensor(2.9451，device=&#39;cuda:0&#39;，grad_fn=)
3037
losstensor(3.0042, device=&#39;cuda:0&#39;, grad_fn=)
3038


通过调整不同的学习率，产生 NaN 损失的批次索引保持不变

通过调整批次大小，仍然总会有一个批次产生 NaN 损失，尽管现在它是包含不同数据样本的不同批次索引


一些特定于我的模型的信息：

使用 AdamW 优化器和 CTC 损失训练的变压器

我的数据集样本在分组之前会被打乱


导致此问题的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77929468/exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</guid>
      <pubDate>Fri, 02 Feb 2024 19:36:21 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 CIDEr 评估指标来评估图像字幕模型。即使两个句子相同，它也会输出 0.0 [关闭]</title>
      <link>https://stackoverflow.com/questions/77927272/im-using-cider-evaluation-metric-for-image-captioning-model-it-outputs-0-0-eve</link>
      <description><![CDATA[从 pycocoevalcap.cider.cider 导入 Cider
导入 json

# 加载您的参考和候选标题
# 参考标题的格式应为：{image_id: [caption1, caption2, ...]}
# 候选标题的格式应为：[{image_id, Caption}]

Reference_file = &#39;/content/ref.json&#39;
候选文件 = &#39;/content/preds.json&#39;

将 open(reference_file, &#39;r&#39;) 作为 f：
    引用 = json.load(f)

以 open(candidate_file, &#39;r&#39;) 作为 f：
    候选人 = json.load(f)
打印（候选人）
# 创建 CIDEr 记分器
cider_scorer = 苹果酒()

# 计算 CIDEr 分数
cider_score, cider_scores = cider_scorer.compute_score(参考文献, 参考文献)

# 打印 CIDEr 分数
print(&quot;CIDEr 分数：&quot;, cider_score)


它给出输出 0.0。
ref.json 有以下数据
{&#39;1087539207_9f77ab3aaf.jpg&#39;: [&#39;三个人在背景有树木的田野里骑着全地形车&#39;]}&#39;

preds.json 具有以下数据
{&#39;1087539207_9f77ab3aaf.jpg&#39;: [&#39;三个人在背景有树木的田野里骑着全地形车&#39;]}&#39;

Pycocoevalcap 是一个 github 存储库，它实现了 CIDEr，即基于共识的图像描述评估]]></description>
      <guid>https://stackoverflow.com/questions/77927272/im-using-cider-evaluation-metric-for-image-captioning-model-it-outputs-0-0-eve</guid>
      <pubDate>Fri, 02 Feb 2024 13:18:02 GMT</pubDate>
    </item>
    <item>
      <title>MLflow 代理工件访问：无法找到凭据</title>
      <link>https://stackoverflow.com/questions/72886409/mlflow-proxied-artifact-access-unable-to-locate-credentials</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/72886409/mlflow-proxied-artifact-access-unable-to-locate-credentials</guid>
      <pubDate>Wed, 06 Jul 2022 15:40:30 GMT</pubDate>
    </item>
    <item>
      <title>重新训练 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</link>
      <description><![CDATA[我有一个使用对象检测 SSD 移动网络训练的张量流模型。
训练现已完成，我导出了模型推理以进行测试。我的问题是，如果我想稍后使用新的图像数据集重新训练模型，我现在应该在这个阶段做什么以使权重渗透到模型中，以便我可以从那时起重新训练它。我知道有一个冻结脚本，我必须使用它吗？ 
谢谢
阿亚德]]></description>
      <guid>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</guid>
      <pubDate>Thu, 11 Oct 2018 22:12:07 GMT</pubDate>
    </item>
    </channel>
</rss>