<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 25 Nov 2024 21:16:09 GMT</lastBuildDate>
    <item>
      <title>在 Databricks 中使用 MLflow 记录模型时排除私有包依赖项</title>
      <link>https://stackoverflow.com/questions/79224603/exclude-private-package-dependency-when-logging-model-with-mlflow-in-databricks</link>
      <description><![CDATA[我正在使用 Databricks Asset Bundles (DAB) 部署 Databricks 工作流。我的集群上安装了一个自定义包 (mypackage)，其中包含用于不同工作流的多个 CLI 命令。此包包含我的工作流所需的所有依赖项。请注意，mypackage 是一个私有 PyPI 存储库，无法从外部访问。
当我使用 mlflow.pyfunc.log_model() 记录我的模型时，MLflow 会自动检测并将 mypackage.submodule 作为模型的依赖项。这会导致问题，因为：

mypackage 是私有的，无法从外部环境安装。
该模型实际上不需要 mypackage 或其依赖项进行推理 - 它只需要 PyTorch 和 Pandas 等标准库。

这是我的代码的简化版本：
import mlflow.pyfunc
from mypackage.submodule import MyModelClass

model = MyModelClass()

mlflow.pyfunc.log_model(
python_model=model,
artifact_path=&quot;my_model&quot;,
# 其他参数
)

问题：

*如何使用 mlflow.pyfunc.log_model() 记录我的模型，而无需包括 mypackage 作为依赖项？*

是否有 MLflow 原生方法可以防止某些包被包含在模型环境中？

是否有最佳实践来处理训练环境包含不应成为序列化模型依赖项的包的情况？


其他上下文：

我正在使用 Databricks 和 Databricks Asset Bundles 进行部署。
该模型使用推理所需的标准库（例如 PyTorch、Pandas）。
mypackage 仅在训练和工作流程编排期间使用，但不需要用于模型推理。


有关如何正确排除的任何指导在使用 MLflow 进行日志记录时，如果能够从模型依赖项中删除 mypackage，将不胜感激。
我尝试过的：
作为一种解决方法，我尝试了一种破解方法，在 __main__ 模块中重新定义我的模型类，并使用 cloudpickle 对其进行序列化，本质上是删除了对 mypackage 的引用。这是我所做的片段：
import inspect
import cloudpickle

# 提取模型类的源代码
model_source = inspect.getsource(MyModelClass)

# 创建主命名空间并在 __main__ 中重新定义类
main_namespace = {
&#39;__name__&#39;: &#39;__main__&#39;,
# 包含其他必要的导入
}
exec(model_source, main_namespace)

# 访问重新定义的类
MyModelClassMain = main_namespace[&#39;MyModelClass&#39;]
model = MyModelClassMain()

# 序列化模型
with open(&quot;model.pkl&quot;, &quot;wb&quot;) as f:
cloudpickle.dump(model, f)

# 记录模型而不包括“mypackage”
mlflow.pyfunc.log_model(
python_model=model,
articulate_path=&quot;my_model&quot;,
artifacts={&quot;model.pkl&quot;: &quot;model.pkl&quot;},
pip_requirements=[&quot;torch&quot;, &quot;pandas&quot;, &quot;cloudpickle&quot;],
# 其他参数
)

虽然这种方法有效，但感觉像是一种黑客解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79224603/exclude-private-package-dependency-when-logging-model-with-mlflow-in-databricks</guid>
      <pubDate>Mon, 25 Nov 2024 21:09:57 GMT</pubDate>
    </item>
    <item>
      <title>弹性净惩罚分位数回归模型效果不佳的原因及解决方法</title>
      <link>https://stackoverflow.com/questions/79224486/reasons-and-potential-solutions-for-poor-performance-of-elastic-net-penalized-qu</link>
      <description><![CDATA[我正在对我的数据集执行弹性净惩罚分位数回归 (EN-QR)，该数据集有 6,782 行和 227 列（即预测因子）。其中 195 个预测因子是代谢物，我的目标是找出哪些代谢物与接触空气污染物 PM2.5 有关。我选择执行 QR 而不是线性回归，因为 PM2.5 数据存在偏差且不呈正态分布，即使在执行对数转换和标准化之后也是如此。我将数据集按 70:30 的比例分为训练集 (n = 4,747) 和测试集 (n = 2,035)。
我遇到的问题是，当我将训练集中的 EN-QR 对象应用到测试集以预测 PM2.5 值（下图）时，我得到了一个奇怪的多峰分布，它与测试集中的实际 PM2.5 值不符（上图）。我已将这些分布的屏幕截图附在下面。

我不一定在寻找可以解决这个问题的人，但我想知道是否有人对此有解决问题的建议。模型会分裂成多峰分布而不是（近似）正态分布的一些潜在原因是什么？有没有我没有考虑包含在训练集模型中的东西？是否有其他我应该研究的策略来实现我的变量选择目标？理想情况下，我希望 EN-QR 模型能够发挥作用，因为它为我的研究问题提供了最多的信息。
我尝试根据数据属于多峰分布的哪个模式将数据分成四个集群。我发现美国人口普查区域与预测的 PM2.5 值的模式之间存在关系。例如，几乎所有 z 得分最低的模式中的人都来自南部，而几乎所有 z 得分最高的模式中的人都来自中西部。我觉得这很奇怪，因为美国人口普查区域被纳入为协变量，因此模型不应该按地区分层。我在下面附上了这张图片和表格。

这是我用来在训练集上执行 EN-QR 的 R 聊天。 Behat 是我用来预测测试集中 PM2.5 值的代码。
trainingset.ENQR &lt;- rq.pen.cv(
x = as.matrix(trainingset[, c(4:5, 27, 29:251)]),
y = trainingset[, 18],
tau = c(0.1, 0.5, 0.9),
penalty = &quot;ENet&quot;,
a = c(0.50, 0.75, 0.90),
nfolds = 10,
printProgress = TRUE,
penalty.factor = c(rep(0, 32), rep(1, 194))
)

lambda.min.training &lt;- trainingset.ENQR$gtr$lambda1se[2]

testingset.ENQR.50 &lt;- rqPen:::predict.rq.pen.seq.cv(object = trainingset.ENQR,
newx = as.matrix(testingset[, c(4:5, 27, 29:251)]),
tau = 0.50,
a = 0.50,
lambda = lambda.min.training,
cvmin = FALSE)

非常感谢您的建议和帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79224486/reasons-and-potential-solutions-for-poor-performance-of-elastic-net-penalized-qu</guid>
      <pubDate>Mon, 25 Nov 2024 20:14:44 GMT</pubDate>
    </item>
    <item>
      <title>统计学习混淆表变量</title>
      <link>https://stackoverflow.com/questions/79224395/statistical-learning-confusion-table-variable</link>
      <description><![CDATA[我的混淆表中出现了一个额外的变量，不知道它从何而来。
数据集“默认”具有以下列：默认、学生、收入、余额
变量“默认”具有两个值：“是”和“否”
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
summary,
poly)
from ISLP import chaos_table

Default = load_data(&#39;Default&#39;)
vars = Default.columns.drop([&#39;default&#39;])
y = Default[&#39;default&#39;] == &#39;是&#39;
design = MS(vars)
X = design.fit_transform(Default)
glm = sm.GLM(y,
X,
family = sm.families.Binomial())
results = glm.fit()
summarize(results)
probs = results.predict()
labels = np.array([&#39;No&#39;]*10000)
labels[probs&gt;0.5] = &#39;Yes&#39;
confusion_table(labels,Default.default)

在输出中，我得到一个 3x3 表，其中包含变量“No”、“Yes”和“Ye”
我希望混淆表值仅为“Yes”和“No”。不知何故，numpy.array“labels”设置为“Ye”而不是“Yes”。]]></description>
      <guid>https://stackoverflow.com/questions/79224395/statistical-learning-confusion-table-variable</guid>
      <pubDate>Mon, 25 Nov 2024 19:35:27 GMT</pubDate>
    </item>
    <item>
      <title>FileNotFoundError。当我 pickle 我的 ML 模型并将其注释掉时，会出现此错误，这可能是什么问题？</title>
      <link>https://stackoverflow.com/questions/79224304/filenotfounderror-what-could-be-the-problem-with-why-i-am-getting-this-error-wh</link>
      <description><![CDATA[我在尝试为数据项目加载 pickle ML 模型时收到一条错误消息。如果有人能给我建议，我将不胜感激。
我有正确的文件路径，并且已经确认了路径“/home/jovyan/work”，并且之前加载了 pickle。
我不知道这是否是云问题，但是当我单独注释掉 pickle 的写入时，或者如果我注释掉 pickle 的写入和模型的拟合...出于某种原因，我无法简单地加载 pickle。这就是我每次都要对模型进行 pickle 处理，而不必拟合模型的原因。
这些是我正在使用的函数....
def write_pickle(path, model_object, save_as:str): 

with open(path + save_as + &quot;.pickle&quot;, &quot;wb&quot;) as to_write: 

pickle.dump(model_object, to_write)

def read_pickle(path, saved_model_name:str):

with open(path + saved_model_name + &#39;.pickle&#39;, &#39;rb&#39;) as to_read:

model = pickle.load(to_read)

return model

这些函数可以处理文件路径和所有内容，但是当我注释掉 pickle 的写入或注释掉 ML 模型的拟合时.... 我收到了 FileNotFoundError。我不知道问题可能出在哪里。如果我不能弄清楚，那么我甚至连 pickle 这个模型都毫无用处。]]></description>
      <guid>https://stackoverflow.com/questions/79224304/filenotfounderror-what-could-be-the-problem-with-why-i-am-getting-this-error-wh</guid>
      <pubDate>Mon, 25 Nov 2024 19:00:17 GMT</pubDate>
    </item>
    <item>
      <title>比较两个图像集分布？[关闭]</title>
      <link>https://stackoverflow.com/questions/79221947/compare-two-images-sets-distirbutions</link>
      <description><![CDATA[我有两组患有某种疾病的人体皮肤图像。第 1 组的图像肤色较浅，但由于光线不好，许多图像显得较暗。我通过修改第 1 组图像的肤色使它们更暗，制作了另一组，即第 2 组。现在，我想比较这两组的分布。我应该使用哪种测量方法，例如 Wasserstein 距离、Kullback-Leibler 距离等？在测量距离之前我应该​​做什么，例如我应该制作两个大小为 256 个箱的直方图，其中每个像素代表其在两组中出现的概率并进行比较吗？我的图像是 RGB，如果是 RGB 图像，我应该如何测量？]]></description>
      <guid>https://stackoverflow.com/questions/79221947/compare-two-images-sets-distirbutions</guid>
      <pubDate>Mon, 25 Nov 2024 06:52:08 GMT</pubDate>
    </item>
    <item>
      <title>加载的模型数据与测试数据不同吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</link>
      <description><![CDATA[我将经过训练的模型加载到 RStudio 4 中，当我使用 xgboost 进行预测时，它总是显示：
存储在对象和新数据中的特征名称不同

我检查了训练和测试的数据，列名和顺序相同，但测试数据有一个额外的列作为响应/依赖变量，用于预测结果或将预测与其进行比较。
没有它，你怎么知道要预测什么？
loadedmodel$feature_names（打印出 6 列）

test &lt;- as.matrix(test2)

test$feature_names（它显示原子向量，实际上 6 列和 1 个依赖列与预测结果相同）

dtest&lt;- xgb.DMatrix(test2, missing=NaN)

pred&lt;- predict(loadedmodel, dtest)

什么是错了吗？]]></description>
      <guid>https://stackoverflow.com/questions/79220707/loaded-model-data-differenct-from-test-data</guid>
      <pubDate>Sun, 24 Nov 2024 17:33:43 GMT</pubDate>
    </item>
    <item>
      <title>当模型训练时，actor-critic 中的标准差是否会收敛到 0？[关闭]</title>
      <link>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</link>
      <description><![CDATA[我正在使用 Advantage Actor-Critic 算法来解决我的问题。
actor NN 预测我从中采样动作的平均值和标准偏差。
这是否意味着预测的标准偏差会随着时间的推移而减少，直到模型收敛时变得太小？
或者即使 A&amp;C 模型收敛，我也“注定”总是具有随机性？]]></description>
      <guid>https://stackoverflow.com/questions/79220588/is-the-standard-deviation-in-actor-critic-meant-to-converge-to-0-when-the-model</guid>
      <pubDate>Sun, 24 Nov 2024 16:32:22 GMT</pubDate>
    </item>
    <item>
      <title>在验证期间计算 mIoU</title>
      <link>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</link>
      <description><![CDATA[我正在开展二元分割任务，并已实施以下训练和验证循环。
我需要两点帮助：
如何在每个 epoch 之后计算每个类的 IoU 并打印第 1 类 IoU、第 2 类 IoU 和总体 mIoU 分数？
根据最佳 mIoU 分数或最低验证损失保存模型更好吗？
这是我的代码：
# 初始化列表以存储损失值
train_losses = []
val_losses = []

# 训练和验证循环
for epoch in range(n_eps):
model.train()
train_loss = 0.0

# 训练循环
for images, mask in tqdm(train_loader):
images, mask = images.to(device), mask.to(device)

optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, mask)
loss.backward()
optimizer.step()

train_loss += loss.item()

avg_train_loss = train_loss / len(train_loader)
train_losses.append(avg_train_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}], Train Loss: {avg_train_loss:.4f}&quot;)

model.eval()
val_loss = 0.0

# 验证循环
with torch.no_grad():
for images, mask in val_loader:
images, mask = images.to(device), mask.to(device)
output = model(images)
val_loss += criterion(outputs, mask).item()

avg_val_loss = val_loss / len(val_loader)
val_losses.append(avg_val_loss)
print(f&quot;Epoch [{epoch+1}/{n_eps}]，Val 损失：{avg_val_loss:.4f}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79220513/computing-miou-during-validation</guid>
      <pubDate>Sun, 24 Nov 2024 15:59:00 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Visual Studio 代码中训练监督和机器学习模型</title>
      <link>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</link>
      <description><![CDATA[我在 Nvidia RTX 4070 8Gb Vram 上运行 Visual Studio 代码。
我花了很长时间训练一些模型，而我朋友的 4070 上花费的时间比我少得多。
有什么建议吗？
更新了 CUDA 驱动程序和工具包。确实将我的电池设置为高性能模式（但我认为这不是问题所在）
检查 Python 包和框架版本是否与我朋友的设置相同。
从 cmd 检查 Nvidia-Smi 中的使用情况，但似乎没有使用 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/79219489/not-able-to-train-supervised-and-machine-learning-model-in-visual-studio-code</guid>
      <pubDate>Sun, 24 Nov 2024 06:39:27 GMT</pubDate>
    </item>
    <item>
      <title>Python scorecardpy：UnboundLocalError：赋值前引用了局部变量“card_df”</title>
      <link>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</link>
      <description><![CDATA[我使用 scorecardpy 函数来获取模型：
import scorecardpy as ac
card=sc.scorecard(bins_adj, lr, X_train.columns)

然后我尝试使用以下代码保存此模型：
import numpy as np
np.save(&#39;card.npy&#39;,card)

之后我尝试重新加载此模型：
card=np.load(&#39;card.npy&#39;,allow_pickle=True)

然后我想使用该模型获取分数：
score=sc.scorecard_ply(data_train, card, print_step=0)

但它给出了错误：
UnboundLocalError Traceback（最近一次调用最后一次）
单元格在 [91]，第 1 行
score=sc.scorecard_ply(data_train, card, print_step=0)

文件 ~/.local/lib/python3.9/site-packages/scorecardpy/scorecard.py:330，在 scorecard_ply(dt, card, only_total_score, print_step, replace_blank_na, var_kp)
card_df=card.copy(deep=True)
# x 变量
xs=card_df.loc[card_df.variable != &#39;basepoints&#39;, &#39;variable&#39;].unique()
# x 变量的长度
xs_len=len(xs)

UnboundLocalError：局部变量“card_df”在赋值前被引用

如何解决此问题有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</guid>
      <pubDate>Sun, 24 Nov 2024 04:16:39 GMT</pubDate>
    </item>
    <item>
      <title>如何让这个图看起来更整洁一点？</title>
      <link>https://stackoverflow.com/questions/79218937/how-to-make-this-graph-look-a-bit-neater</link>
      <description><![CDATA[import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import datetime

# 加载特斯拉数据集
tesla_stock = pd.read_csv(&#39;C:/Users/Admin/Downloads/AI/Tesla.csv&#39;)

# 动态处理缺失或重命名的“收盘价”列
possible_target_columns = [&#39;Close&#39;, &#39;Last&#39;, &#39;Price&#39;, &#39;Adjusted Close&#39;, &#39;VWAP&#39;, 
&#39;Close/Last&#39;]
target_column = None

# 打印可用列以进行调试
print(&quot;数据集中可用的列：&quot;, tesla_stock.columns)

for col in possible_target_columns:
if col in tesla_stock.columns:
target_column = col
print(f&quot;使用 &#39;{col}&#39; 作为预测的目标列。&quot;)
break

# 处理未找到合适列的情况
if target_column is None:
print(&quot;未找到合适的预测列。请检查数据集。&quot;)
raise KeyError(&quot;确保数据集包含带有股票价格的列（例如，&#39;Close&#39;、
&#39;Close/Last&#39;）。&quot;)

# 清理数字列（删除 &#39;$&#39; 并转换为浮点数）
for column in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close/Last&#39;, &#39;Volume&#39;]:
if column in tesla_stock.columns:
tesla_stock[column] = tesla_stock[column].replace(&#39;[\$,]&#39;, &#39;&#39;, 
regex=True).astype(float)

# 将“日期”转换为日期时间并设置为索引
tesla_stock[&#39;Date&#39;] = pd.to_datetime(tesla_stock[&#39;Date&#39;])
tesla_stock.set_index(&#39;Date&#39;, inplace=True)

# 定义特征和目标变量
features = [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Volume&#39;]
X = tesla_stock[features]
y = tesla_stock[target_column]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 进行预测
predicted = model.predict(X_test)

# 评估模型
print(&quot;模型得分 (R²)：&quot;, model.score(X_test, y_test))
print(&quot;平均绝对误差：&quot;, metrics.mean_absolute_error(y_test, predicted))
print(&quot;均方误差：&quot;, metrics.mean_squared_error(y_test, predicted))
print(&quot;均方根误差：&quot;, math.sqrt(metrics.mean_squared_error(y_test, 
predicted)))

# 绘制测试集的实际价格与预测价格
dfr = pd.DataFrame({&#39;Actual&#39;: y_test, &#39;Predicted&#39;: predicted})
plt.figure(figsize=(14, 8))
dfr.head(25).plot(kind=&#39;bar&#39;, figsize=(14, 8))
plt.title(&quot;实际价格与预测价格&quot;)
plt.xlabel(&quot;样本&quot;)
plt.ylabel(&quot;价格 (USD)&quot;)
plt.xticks(rotation=45, ha=&quot;right&quot;)
plt.tight_layout()
plt.show()

# 预测未来 30 天
last_date = tesla_stock.index[-1] # 历史数据中的最后一个日期
last_price = tesla_stock[target_column].iloc[-1] # 历史数据中的最后一个价格
future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, 31)] # 
生成未来 30 天

# 创建占位符 DataFrame用于未来特征
future_features = pd.DataFrame(index=future_dates)
for feature in features:
if feature in tesla_stock.columns:
future_features[feature] = tesla_stock[feature].mean() # 使用每个特征的平均值

# 使用训练模型预测未来价格
future_predictions = model.predict(future_features)

# 结合历史和未来数据，绘制无缝图
all_dates = list(tesla_stock.index) + list(future_dates) # 结合历史和未来日期
all_prices = list(tesla_stock[target_column]) + list(future_predictions) # 结合历史和预测价格

# 绘制历史数据和未来预测
plt.figure(figsize=(14, 8))
plt.plot(tesla_stock.index, tesla_stock[target_column], label=&#39;历史价格&#39;, 
color=&#39;blue&#39;)
plt.plot(future_dates, future_predictions, label=&#39;30 天未来预测&#39;, 
color=&#39;red&#39;)
plt.title(&#39;特斯拉未来 30 天股价预测&#39;)
plt.xlabel(&#39;日期&#39;)
plt.ylabel(&#39;价格 (美元)&#39;)
plt.legend()
plt.xticks(rotation=45)
plt.show()

# 显示未来预测
future_features[&#39;Predicted_Close&#39;] = future_predictions
print(future_features[[&#39;Predicted_Close&#39;]])

输入图片描述在这里
我试图让红线连接到图表的末端，然后弯曲到它所在的位置。而不是一开始就只是一条随机的短红线，因为它看起来有点混乱。我一直在尝试这样做，但我真的很难做到。怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79218937/how-to-make-this-graph-look-a-bit-neater</guid>
      <pubDate>Sat, 23 Nov 2024 22:00:52 GMT</pubDate>
    </item>
    <item>
      <title>抱抱脸生成方法后内存增加</title>
      <link>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</link>
      <description><![CDATA[我想使用 huggingface 中的 codegemma 模型进行推理，但当我使用 model.generate(**inputs) 方法时，无论 max_token_len 数量是多少，使用 torch profiler 时，GPU 内存成本在峰值使用量中都会从 39 GB 增加到 49 GB。我理解我们需要在推理和上下文中保存模型的激活，例如 4096 个输入标记，但我不敢相信它可以增加 10 GB 的推理内存使用量。有人能解释一下这是怎么回事吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79218644/memory-increasing-after-hugging-face-generate-method</guid>
      <pubDate>Sat, 23 Nov 2024 19:21:47 GMT</pubDate>
    </item>
    <item>
      <title>错误：TypeError：无法将 cuda:0 设备类型张量转换为 numpy。首先使用 Tensor.cpu() 将张量复制到主机内存</title>
      <link>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor-c</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218508/error-typeerror-cant-convert-cuda0-device-type-tensor-to-numpy-use-tensor-c</guid>
      <pubDate>Sat, 23 Nov 2024 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>具有动态约束的贷款人和借款人之间的多元化资金分配算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79218415/algorithm-for-diversified-fund-allocation-between-lenders-and-borrowers-with-dyn</guid>
      <pubDate>Sat, 23 Nov 2024 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 RMSprop 优化器的动态学习率进行 Q 学习</title>
      <link>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。以下是我正在使用的 Q-learning 更新函数：
**
python 复制代码**
def update_q_table(self, state, action, reward, next_state):
&quot;&quot;&quot;使用 Q-learning 更新规则更新 Q-table。&quot;&quot;&quot;
def update_q_table(self, state, action, reward, next_state):
best_next_action = np.argmax(self.q_table[next_state, :])
td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
td_error = td_target - self.q_table[state, action]

# 更新 RMSprop 的平方梯度移动平均值 E[g^2]
self.gradient_Q[state, action] = (
self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2)
)

self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon)
self.learning_rate_history.append(self.learning_rate)

# 使用固定学习率和 TD 误差更新 Q 值
self.q_table[state, action] += self.learning_rate * td_error 

# 存储 E[g^2] 值以进行跟踪
self.gradient_history.append(self.gradient_Q[state, action])


我正在使用受 RMSprop 启发的动态学习率实现 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。
具体来说：
我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率从 0.001 开始，并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。这是我正在使用的 Q 学习更新函数：]]></description>
      <guid>https://stackoverflow.com/questions/79177301/q-learning-using-dynamic-learning-rate-with-rmsprop-optimizer</guid>
      <pubDate>Mon, 11 Nov 2024 10:33:43 GMT</pubDate>
    </item>
    </channel>
</rss>