<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 21 Mar 2024 12:23:58 GMT</lastBuildDate>
    <item>
      <title>尝试在自注释数据集上训练用于虹膜识别的神经网络</title>
      <link>https://stackoverflow.com/questions/78199835/trying-to-train-a-neural-network-for-iris-recognition-on-a-self-annotated-datase</link>
      <description><![CDATA[classloss = tf.keras.losses.BinaryCrossentropy()
回归损失 = 本地化损失
类损失（y\[0\]，类）

在此处输入图像描述
模型 = FaceTracker(facetracker)
model.compile(opt, classloss, regressloss)
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
hist = model.fit(train, epochs=15,validation_data=val,callbacks=\[tensorboard_callback\])


model.fit 抛出以下错误：
为什么会出现错误。在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78199835/trying-to-train-a-neural-network-for-iris-recognition-on-a-self-annotated-datase</guid>
      <pubDate>Thu, 21 Mar 2024 12:07:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 numpy 函数计算以下 hessian 矩阵以加快计算速度？</title>
      <link>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</link>
      <description><![CDATA[我必须实现一个等效函数来计算逻辑损失的 hessian，将其写为指数项对数之和。我在Python中实现了以下功能：
def hessian(self,w,hess_trick=0):
        赫斯 = 0
        对于 zip(self.data, self.labels) 中的 x_i,y_i：
            hess += np.exp(y_i * np.dot(w.T, x_i))/((1 + np.exp(y_i * np.dot(w.T,x_i)))**2) * np.outer(x_i, x_i.T)
        返回hess + lambda_reg * np.identity(w.shape[0]) + hess_trick * 10**(-12) * np.identity(w.shape[0])

我的问题是如何在不使用慢速 python 的情况下编写等效但更快的函数？
由于我对 numpy 不太有信心，我尝试编写以下函数：
 def new_hessian(self, w, hess_trick=0):
        exp_term = np.exp(self.labels * np.dot(self.data, w))
        sigmoid_term = 1 + exp_term
        inv_sigmoid_sq = 1 / sigmoid_term ** 2

        diag_elements = np.sum((exp_term * inv_sigmoid_sq)[:, np.newaxis] * self.data ** 2, axis=0)
        off_diag_elements = np.dot((exp_term * inv_sigmoid_sq) * self.data.T, self.data)
        hess = np.diag(diag_elements) + off_diag_elements
        正则化 = lambda_reg * np.identity(w.shape[0])

        hess += hess_trick * 1e-12 * np.identity(w.shape[0])

        返回 hess + 正则化

通过调试这个函数，我发现存在一个根本性的问题。对于特征数量较小的值（例如小于 200），hessian 的两种实现不相等。当我增加特征数量时，这两个函数似乎是相等的。问题是，当使用牛顿方法来优化对数损失来测试这些实现时，较快的实现会比第一个（但在运行时速度方面较慢）实现更多的迭代收敛。]]></description>
      <guid>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</guid>
      <pubDate>Thu, 21 Mar 2024 12:02:04 GMT</pubDate>
    </item>
    <item>
      <title>我尝试使用过去的数据进行预测。 jupyter 实验室和机器学习。我是这条路的新手</title>
      <link>https://stackoverflow.com/questions/78199588/i-try-to-get-predicion-using-past-data-jupyter-lab-and-maching-learning-i-am-n</link>
      <description><![CDATA[plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap=&quot;YlGnBu&quot;)
当我运行这个时，我收到以下错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
单元格 In[23]，第 2 行
      1 plt.figure(figsize=(15,8))
----&gt; 2 sns.heatmap(train_data.corr(), annot=True, cmap=“YlGnBu”)

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\frame.py:11036，在 DataFrame.corr(self, method, min_periods, numeric_only)
  11034 列 = 数据.列
  第11035章
&gt;第11036章
  第11038章
  第11039章

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\frame.py:1981，在 DataFrame.to_numpy(self, dtype, copy, na_value) 中
   1979 如果 dtype 不是 None：
   第1980章
-&gt;第1981章
   1982 如果 result.dtype 不是 dtype：
   第1983章

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\internals\managers.py:1692，在 BlockManager.as_array(self, dtype, copy, na_value) 中

ValueError：无法将字符串转换为浮点数：&#39;HOWARD AVENUE-PARK PLACE&#39;
&lt;图形尺寸 1500x800，0 轴&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/78199588/i-try-to-get-predicion-using-past-data-jupyter-lab-and-maching-learning-i-am-n</guid>
      <pubDate>Thu, 21 Mar 2024 11:30:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 Tensorflow 时 Python 产生的结果比 kotlin 更准确？</title>
      <link>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</link>
      <description><![CDATA[我目前正在制作一个应用程序，它将检测不同数字系统中不同的手写数学表达式。截至目前，阻碍任何进展的主要因素是 kotlin 在使用 Tensorflow lite 时产生的不准确性 - 大约 10% 正确。我的 Python 代码非常相似，但它使用常规张量流，并且更加准确 - 大约 70% 正确。
我的想法是图像从 OpenCV Mat 转换为 Tensorbuffer 的方式导致了一些问题，或者预处理的处理方式导致了差异。
我的代码片段如下：

提取边界矩形后，进行预处理和标准化。

val image_roi = Mat(tmp,boundRect)
Imgproc.cvtColor(image_roi, image_roi, Imgproc.COLOR_RGB2GRAY) Imgproc.GaussianBlur(image_roi, image_roi, Size(3.0,3.0), 0.0)
Imgproc.dilate(image_roi, image_roi, Imgproc.getStructuringElement(Imgproc.MORPH_RECT, Size(4.0, 4.0)))
Imgproc.threshold(image_roi, image_roi, 90.0, 255.0, Imgproc.THRESH_BINARY);
Imgproc.resize(image_roi, image_roi, 大小(28.0,28.0))
Core.normalize(image_roi, image_roi, 0.0, 255.0, Core.NORM_MINMAX);
image_roi.convertTo(image_roi, CvType.CV_8UC1)
提取.add(image_roi)


运行预测，将 OpenCV Mat 转换为 Tensorbuffer（第 3 步）

for（提取的img）{
       val 张量缓冲区 = extractBytes(img)
       val 输出 = model.process(tensorBuffer)
       valoutputFeature0=outputs.outputFeature0AsTensorBuffer
       valconf=outputFeature0.floatArray
       out += getLanguageText(conf, 数字)
 
}


将 Mat 转换为 Tensorbbuffer

私有乐趣 extractBytes(img: Mat): TensorBuffer{
        val inputFeature = TensorBuffer.createFixedSize(intArrayOf(1, 28, 28, 1), DataType.FLOAT32)
        val byteBuffer = ByteBuffer.allocateDirect(28 * 28 * 4) // 每个浮点数 4 个字节
        byteBuffer.order(ByteOrder.nativeOrder())
        byteBuffer.rewind()
 
        for (i 从 0 到 28) {
            for (j in 0 到 28) {
                val temp = img.get(i, j)[0].toFloat() // 假设单通道（灰色）
                byteBuffer.putFloat(临时)
            }
        }
 
        inputFeature.loadBuffer(byteBuffer)
        返回输入特征
    }

在下面的粘贴箱中，我也包含了我的 pythin 代码。我需要一些帮助来弄清楚为什么我的模型无法通过 Kotlin 准确预测，但可以通过 Python 准确预测。
https://pastebin.com/BACzTkq6
以下是在 Python 和 Kotlin 中使用相同图像的差异示例：
通过 Kotlin 显示预测的图像
通过 python 显示预测的图像
我尝试了将 Matrix 转换为 Tensorbuffer 的不同方法，我尝试删除大多数（如果不是全部）图像预处理，我尝试让 python 在 Android studio 中工作（但这并没有成功。）
此外，我也曾多次向其他地方寻求过帮助，但都没有得到任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</guid>
      <pubDate>Thu, 21 Mar 2024 11:18:54 GMT</pubDate>
    </item>
    <item>
      <title>在 llm_chain input_variables 中找不到 StuffDocumentsChain document_variable_name 上下文的验证错误：['input'] (type=value_error)</title>
      <link>https://stackoverflow.com/questions/78199098/validation-error-for-stuffdocumentschain-document-variable-name-context-was-not</link>
      <description><![CDATA[我正在尝试根据路由使用不同的rags在langchain中创建一个路由链，每个rags都有不同的提示模板，这是我的代码：
period_template = &quot;&quot;&quot;AAAText
多曼达：
{输入}“””


ref_template = &quot;&quot;&quot;BBBtext.
多曼达：
{输入}“””

list_template = &quot;&quot;&quot;CCCText.
多曼达：
{输入}“””

提示信息 = [
    {
        “名称”：“期间”，
        “描述”：“AAA”，
        “提示模板”：期间模板
    },
    {
        “名称”：“参考”，
        “描述”：“BBB”，
        “提示模板”：参考模板
    },
    {
        “名称”：“历史”，
        “描述”：“CCC”，
        “提示模板”：列表模板
    }
]

目的地链 = {}
对于提示信息中的 p_info：
    PROMPT = PromptTemplate(template=p_info[&quot;prompt_template&quot;], input_variables=[&quot;input&quot;])
    链 = RetrievalQA.from_chain_type(
        聊天模型，
        检索器=db.as_retriever(),
        return_source_documents=真，
        chain_type_kwargs={“提示”: 提示}
    ）
    destination_chains[p_info[“名称”]] = 链

我收到错误：
&lt;块引用&gt;
ValidationError：StuffDocumentsChain 出现 1 个验证错误
root document_variable_name 上下文在 llm_chain input_variables 中找不到：[&#39;input&#39;] (type=value_error)

出了什么问题，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78199098/validation-error-for-stuffdocumentschain-document-variable-name-context-was-not</guid>
      <pubDate>Thu, 21 Mar 2024 10:16:24 GMT</pubDate>
    </item>
    <item>
      <title>以多个预测值之和为目标构建核岭回归</title>
      <link>https://stackoverflow.com/questions/78198400/building-kernel-ridge-regression-with-targeting-sum-of-multiple-predicted-values</link>
      <description><![CDATA[我们可以实现一个代码来实现核岭回归，将一些基于子集的预测值求和为最终目标吗？
例如，我们知道标准情况将训练：
特征数据 X（样本数，特征数）-&gt; Y（样本数）
典型的 sklearn 代码训练 X -&gt; 的直接映射Y，立即接受所有功能。
我想要做的是训练回归模型，以便：
对于特征数量 m*n，训练一个映射：
small_x_i(样本数, 0..m) -&gt; small_y_i（样本数）。
（small_y_i 不受限制）
对于 X 的子集，回归也应用 n 次：0..m、m..2m、...、m * (n-1)..m * n
然后，small_y_i 的总和以最终值 Y(samples) 为目标。
我之所以尝试这样做，是因为我的问题有一个特定的限制，即我一次可以使用的功能数量是有限的。在整个特征中，有一些组具有相同的概念起源，我需要训练一个回归，以便每组特征的预测值之和最终得到目标值。
我知道用 sklearn 实现 KRR 的标准案例，但实际上不知道如何实现上面要求的任务。搜索“作为目标的预测值之和”不会给出可引用的线索。]]></description>
      <guid>https://stackoverflow.com/questions/78198400/building-kernel-ridge-regression-with-targeting-sum-of-multiple-predicted-values</guid>
      <pubDate>Thu, 21 Mar 2024 08:18:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么线性回归的纯 NumPy 实现的学习率优化如此无效？</title>
      <link>https://stackoverflow.com/questions/78196911/why-is-learning-rate-optimization-to-pure-numpy-implementation-of-linear-regress</link>
      <description><![CDATA[在 numpy 中创建简单的线性回归模型后，我发现改变步长/学习率并不能有效提高模型的准确性或收敛速度。请注意给出此标准模型：
%%time

x = np.arange(100)
y = 3 * x + 5
x = np.column_stack((np.ones(100), x))

w = np.zeros((100, 2))
步长=10**-4
迭代次数 = 10**5

对于范围内的 i（迭代）：
    损失 = 2 * (np.sum(w * x, axis=1) - y)
    w -= step_size * np.average(x * loss[:, None], axis=0)

打印（w[0]）

模型提供以下输出：
&lt;前&gt;&lt;代码&gt;[4.60820653 3.00590689]
CPU时间：用户5.04秒，系统：19.9毫秒，总计：5.06秒
挂壁时间：5.07 秒


更改step_size变量可以被视为超参数优化，但是当将其更改为大于10-4（例如10-3）时，模型无法收敛并爆炸：
&lt;前&gt;&lt;代码&gt;#step_size = 10**-3
[楠楠]
CPU时间：用户4.98秒，系统：38.2毫秒，总计：5.02秒
挂壁时间：5秒

这种行为在数学上是可以预料到的，但遇到这种情况却令人沮丧，并引出了一个问题：如何更有效地优化步长？
&lt;小时/&gt;
我尝试更改与梯度相关的系数（我将梯度变量标记为“损失”），而不是更改步长，因为这也会影响每个步骤的戏剧性（据我所知）给定更大或更小的损失，因此下降的梯度更陡。令人惊讶的是，将损失系数从 2 更改为 5 显着提高了性能（这正是我所做的，而不是优化步长，但我想这是一个单独的主题）。
# 将损耗系数从 2 更改为 5
[4.99998468 3.00000023]
CPU时间：用户5.03秒，系统：42.7毫秒，总计：5.07秒
挂壁时间：5.08 秒
]]></description>
      <guid>https://stackoverflow.com/questions/78196911/why-is-learning-rate-optimization-to-pure-numpy-implementation-of-linear-regress</guid>
      <pubDate>Thu, 21 Mar 2024 00:26:12 GMT</pubDate>
    </item>
    <item>
      <title>vercel：错误：上传文件的大小超过 300MB</title>
      <link>https://stackoverflow.com/questions/78196675/vercel-error-size-of-uploaded-file-exceeds-300mb</link>
      <description><![CDATA[我正在尝试部署一个使用 Deepface.analyze 函数的基本 python 应用程序。尝试在 Vercel 上部署应用程序时，出现此错误：
上传文件大小超过300MB

是因为像deepFace和tensorflow这样的大型库吗？还是因为我的代码结构？
我有一个具有以下结构的基本 python Flask 应用程序：
&lt;前&gt;&lt;代码&gt;静态
--样式.css
模板
--index.html
应用程序.py
要求.txt
]]></description>
      <guid>https://stackoverflow.com/questions/78196675/vercel-error-size-of-uploaded-file-exceeds-300mb</guid>
      <pubDate>Wed, 20 Mar 2024 23:05:34 GMT</pubDate>
    </item>
    <item>
      <title>启动 ML 项目指南 [关闭]</title>
      <link>https://stackoverflow.com/questions/78196528/guide-to-starting-a-ml-project</link>
      <description><![CDATA[我正在致力于创建机器学习模型，学习如何将传入电子邮件分类到文件夹中，主要重点是模型必须自主学习，而无需了解电子邮件习惯，并且随着时间的推移，可以更好地进行分类。
有关如何启动此项目的任何提示、视频、链接、知识以及如何创建此自主分类的策略？ （我正在使用安然语料库数据集）。
在启动项目时需要帮助]]></description>
      <guid>https://stackoverflow.com/questions/78196528/guide-to-starting-a-ml-project</guid>
      <pubDate>Wed, 20 Mar 2024 22:20:07 GMT</pubDate>
    </item>
    <item>
      <title>如何从 shap 值中仅获取重要的单词？</title>
      <link>https://stackoverflow.com/questions/78194233/how-can-i-get-only-important-word-from-shap-value</link>
      <description><![CDATA[我只想获取文字和值，而不获取图表。
shap_values[:,:,1].abs.mean(0) 根据重要性提供“单词”。然而，代码给出了一个仅由数字组成的数组。如果您使用 shap.plots.bar(shap_values[:,:,1].abs.mean(0))，您可以看到单词。在没有图表的情况下，如何获得考虑到其重要性的“单词”？
!pip 安装数据集
从数据集导入load_dataset

数据集 = load_dataset(“imdb”)
df = 数据集[&#39;测试&#39;].to_pandas()
Short_data = [v[:500] for v in df[“text”][:20]]

从转换器导入 AutoTokenizer、AutoModelForSequenceClassification、管道
t1okenizer = AutoTokenizer.from_pretrained(“lvwerra/distilbert-imdb”)
m1odel = AutoModelForSequenceClassification.from_pretrained(“lvwerra/distilbert-imdb”)
分类器 = pipeline(&#39;文本分类&#39;, device=0,return_all_scores=True, model=m1odel,tokenizer=t1okenizer)
分类器（短数据[：10]）

导入形状
解释器 = shap.Explainer(分类器)
shap_values = 解释器(short_data[:20])
shap.plots.bar(shap_values[:,:,1].abs.mean(0))
]]></description>
      <guid>https://stackoverflow.com/questions/78194233/how-can-i-get-only-important-word-from-shap-value</guid>
      <pubDate>Wed, 20 Mar 2024 14:44:04 GMT</pubDate>
    </item>
    <item>
      <title>尝试从我的网络摄像头访问实时检测时，y python 代码中出现 ocr 错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</link>
      <description><![CDATA[我正在尝试使用 opencv 和 easyocr 进行实时图像检测
但面对这个错误
这是我收到的错误：
&lt;小时/&gt;
错误回溯（最近一次调用最后一次）
单元格 In[79]，第 40 行
     37 除外：
     38 通
---&gt; 40 cv2.imshow(&#39;物体检测&#39;, cv2.resize(image_np_with_detections, (800, 600)))
     42 if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
     43 cap.release()

错误：OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\highgui\src\window.cpp:1272：错误：（-2：未指定错误）该功能未实现。使用 Windows、GTK+ 2.x 或 Cocoa 支持重建库。如果您使用的是 Ubuntu 或 Debian，请安装 libgtk2.0-dev 和 pkg-config，然后在函数“cvShowImage”中重新运行 cmake 或配置脚本

我尝试更换网络摄像头，但遇到同样的问题]]></description>
      <guid>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</guid>
      <pubDate>Tue, 19 Mar 2024 05:44:48 GMT</pubDate>
    </item>
    <item>
      <title>如何将 model.safetensor 转换为 pytorch_model.bin？</title>
      <link>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</link>
      <description><![CDATA[我正在微调预训练的 bert 模型，但遇到了一个奇怪的问题：
当我使用 CPU 进行微调时，代码会像这样保存模型：

使用“pytorch_model.bin”。但是当我使用 CUDA（我必须这样做）时，模型会像这样保存：

当我尝试加载这个“model.safetensors”时将来，它会引发错误“pytorch_model.bin”未找到。我使用两个不同的 venv 来测试 CPU 和 CUDA。
如何解决这个问题？是版本问题吗？
我正在使用sentence_transformers框架来微调模型。
这是我的训练代码：
检查点 = &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;

word_embedding_model = models.Transformer(checkpoint,cache_dir=f&#39;model/{checkpoint}&#39;)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=&#39;mean&#39;)
模型 = SentenceTransformer(模块=[word_embedding_model, pooling_model], device=&#39;cuda&#39;)


train_loss = 损失.CosineSimilarityLoss(模型)

evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name=&#39;sbert&#39;)

model.fit(train_objectives=[(train_dataloader, train_loss)]，epochs=5，evaluator=evaluator，show_progress_bar=True，output_path=f&#39;model_FT/{checkpoint}&#39;，save_best_model=True)

我确实在两个不同的环境中尝试了测试，我希望代码能够保存一个“pytorch_model.bin”文件。不是“model.safetensors”。
编辑：我真的还不知道，但似乎是新版本的 Transformer 库导致了这个问题。我发现使用拥抱脸可以加载安全张量，但使用句子转换器（我需要使用）则不能。]]></description>
      <guid>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</guid>
      <pubDate>Sat, 23 Dec 2023 20:43:20 GMT</pubDate>
    </item>
    <item>
      <title>MLJ：选择行和列进行评估训练</title>
      <link>https://stackoverflow.com/questions/65346104/mlj-selecting-rows-and-columns-for-training-in-evaluate</link>
      <description><![CDATA[我想实现一个也可以在 MLJ 中运行的内核岭回归。此外，我希望可以选择使用特征向量或预定义的内核矩阵，如 Python sklearn。
当我运行这段代码时
const MMI = MLJModelInterface
MMI.@mlj_model 可变结构 KRRModel &lt;: MLJModelInterface.Deterministic
        mu::Float64 = 1::(_ &gt; 0)
        内核::字符串=“线性”
结尾
函数 MMI.fit(m::KRRModel,详细程度::Int,K,y)
    K = MLJBase.矩阵(K)
    拟合结果 = inv(K+m.mu*I)*y
    缓存=无
    报告=无
    返回（拟合结果、缓存、报告）
结尾

   
数 = 10
K = 随机数 (N,N)
K = K*K
a = 随机数 (N)
y = K*a + 0.2*randn(N)
m = KRRModel()
kregressor = 机器(m,K,y)
cv = CV(; nfolds=6, shuffle=无, rng=无)
评估！（kregressor，重采样=cv，测量=rms，详细程度=1）

evaluate! 函数在 K 行的不同子集上评估机器。根据表示定理，核岭回归的非零系数数量等于样本数量。因此，可以使用尺寸减小的矩阵 K[train_rows,train_rows] 代替 K[train_rows,:]。
为了表示我正在使用内核矩阵，我设置了 m.kernel = &quot;&quot; 。当 m.kernel = &quot;&quot; 时，如何让 evaluate! 选择列和行以形成较小的矩阵？
这是我第一次使用 MLJ，我希望进行尽可能少的修改。]]></description>
      <guid>https://stackoverflow.com/questions/65346104/mlj-selecting-rows-and-columns-for-training-in-evaluate</guid>
      <pubDate>Thu, 17 Dec 2020 18:08:42 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 几何：张量大小存在问题</title>
      <link>https://stackoverflow.com/questions/63610626/pytorch-geometric-having-issues-with-tensor-sizes</link>
      <description><![CDATA[这是我第一次使用Pytorch和Pytorch几何。我正在尝试使用 Pytorch Geometric 创建一个简单的图神经网络。我正在通过遵循 Pytorch Geometric 文档并扩展 InMemoryDataset 创建自定义数据集。之后，我将数据集分为训练数据集、验证数据集和测试数据集，其大小分别为（3496、437、439）。这些是每个数据集中的图表数量。这是我的简单神经网络
类 Net(torch.nn.Module):
def __init__(自身):
    超级（网络，自我）.__init__()
    self.conv1 = GCNConv(dataset.num_node_features, 10)
    self.conv2 = GCNConv(10, dataset.num_classes)

def 转发（自身，数据）：
    x，edge_index，batch = data.x，data.edge_index，data.batch
    x = self.conv1(x, 边缘索引)
    x = F.relu(x)
    x = F.dropout(x, 训练=self.training)
    x = self.conv2(x, 边缘索引)

    返回 F.log_softmax(x, 暗淡=1)

我在训练模型时收到此错误，这表明我的输入尺寸存在一些问题。也许原因在于我的批量大小？
RuntimeError：TorchScript 解释器中的以下操作失败。
TorchScript 的回溯（最近一次调用最后一次）：
文件“E:\Users\abc\Anaconda3\lib\site-packages\torch_scatter\scatter.py”，第 22 行，位于 scatter_add 中
        大小[dim] = int(index.max()) + 1
    out = torch.zeros(大小, dtype=src.dtype, device=src.device)
    返回 out.scatter_add_(dim, 索引, src)
           ~~~~~~~~~~~~~~~~ &lt;--- 这里
别的：
    返回 out.scatter_add_(dim, 索引, src)
运行时错误：索引 13654 超出尺寸 678 的维度 0 的范围

该错误专门发生在神经网络中的这行代码上，
x = self.conv1(x, 边缘索引)

编辑：添加了有关edge_index的更多信息，并更详细地解释了我正在使用的数据。
这是我试图传递的变量的形状
x: torch.Size([678, 43])
边缘索引: torch.Size([2, 668])
torch.max(edge_index): 张量(541690)
torch.min(edge_index): 张量(1920)

我使用的数据列表包含 Data(x=node_features, edge_index=edge_index, y=labels) 对象。当我将数据集拆分为训练、验证和测试数据集时，我分别在每个数据集中获得 (3496, 437, 439) 图。最初，我尝试从数据集中创建一个图表，但我不确定它如何与 Dataloader 和小批量一起使用。
train_loader = DataLoader(train_dataset,batch_size=batch_size)
val_loader = DataLoader(val_dataset,batch_size=batch_size)
test_loader = DataLoader(test_dataset,batch_size=batch_size)

这是从数据帧生成图形的代码。我尝试创建一个简单的图，其中只有一些顶点和一些连接它们的边。我可能忽略了一些事情，这就是我遇到这个问题的原因。创建此图时，我尝试遵循 Pytorch 几何文档（Pytorch几何：创建您自己的数据集)
def 进程（自身）：
        数据列表 = []

        分组 = df.groupby(&#39;EntityId&#39;)
        对于 id，分组中的组：
            node_features = torch.tensor(group.drop([&#39;Labels&#39;], axis=1).values)
            source_nodes = group.index[1:].values
            target_nodes = group.index[:-1].values
            标签 = torch.tensor(group.Labels.values)
            edge_index = torch.tensor([源节点, 目标节点])

            数据=数据（x=节点特征，边缘索引=边缘索引，y=标签）
            data_list.append(数据)

        如果 self.pre_filter 不是 None：
            data_list = [data_list 中的数据 if self.pre_filter(data)]

        如果 self.pre_transform 不是 None：
            data_list = [self.pre_transform(data) for data_list中的数据]

        数据，切片= self.collat​​e（data_list）
        torch.save((数据, 切片), self.processed_pa​​ths[0])

如果有人可以帮助我在任何类型的数据上创建图表并将其与 GCNConv 一起使用，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/63610626/pytorch-geometric-having-issues-with-tensor-sizes</guid>
      <pubDate>Thu, 27 Aug 2020 06:52:23 GMT</pubDate>
    </item>
    <item>
      <title>MLP 回归器比 Kernel Ridge 回归最差</title>
      <link>https://stackoverflow.com/questions/53005365/mlp-regressor-worst-than-kernel-ridge-regression</link>
      <description><![CDATA[我正在研究回归问题。我有一个数据集，我在该数据集上完成了特征工程（估算缺失值、box cox 转换倾斜变量等）。
我使用 cross_val_score 和 5 个数据集分割来训练和测试多个模型。首先，我尝试了 Kernel Ridge Regression、Lasso、Elastic Net、Gradient Boosting 等模型。然后我尝试了 scikit learn 的 MLPRegressor。
然而，使用均方根误差，“简单”的结果要好得多。模型比 MLP 回归器（例如，Kernel Ridge 的平均得分为 0.1153，MLPRegressor (hidden_​​layer_sizes=(256,)*25) 的平均得分为 0.1461，这是我发现运行不同架构的最佳得分）。
示例代码：
KRR = KernelRidge(alpha=0.6, kernel=&#39;多项式&#39;, Degree=2, coef0=2.5)

mlpreg = MLPRegressor(hidden_​​layer_sizes=(256,)*25,activation=&#39;relu&#39;,solver=&#39;adam&#39;,
                      详细=0)

以及我用于评分的函数：
def rmsle_crossval(model, train: pd.DataFrame, y_train: List[float]):
    kf = KFold(n_folds, shuffle=True,
               random_state=42).get_n_splits(train.values)
    rmse= np.sqrt(-cross_val_score(模型, train.values, y_train,
                  评分=“neg_mean_squared_error”，cv = kf))
    返回（均方根误差）

即使我尝试使用单个隐藏层 1 和与 KRR 相同的参数的 MLPRegressor 以尽可能接近 KRR，我的得分仍为 0.4381。
你知道为什么会有这样的差异吗？
编辑：
数据形状：(1460, 81)]]></description>
      <guid>https://stackoverflow.com/questions/53005365/mlp-regressor-worst-than-kernel-ridge-regression</guid>
      <pubDate>Fri, 26 Oct 2018 09:14:38 GMT</pubDate>
    </item>
    </channel>
</rss>