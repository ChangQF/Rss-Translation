<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 05 Aug 2024 12:30:22 GMT</lastBuildDate>
    <item>
      <title>使用 GAN 生成医学图像</title>
      <link>https://stackoverflow.com/questions/78834126/medical-image-generation-using-gans</link>
      <description><![CDATA[我正在使用 gans 生成合成 ct 扫描图像以用于我的分割任务。我有一个包含 75 张 PNG 图像的数据集，我想生成合成图像。
这是我使用的代码，但我没有得到正确的输出。[在此处输入图像描述](https://i.sstatic.net/82rfdq8T.png)
是否有可用的在线代码或资源可以做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78834126/medical-image-generation-using-gans</guid>
      <pubDate>Mon, 05 Aug 2024 11:07:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在一个短语中组合两个函数机器学习sklearn</title>
      <link>https://stackoverflow.com/questions/78833739/how-to-combine-2-functions-in-one-phrase-machine-learning-sklearn</link>
      <description><![CDATA[我有一个 data_set，用于存储助手对我的问题/任务的回答
data_set = { &#39;whats new&#39;:&#39;passive 没什么特别的...&#39;, &#39;&#39;:&#39;&#39;, }

passive - 与机器人的简单对话中的存根。
助手可以同时执行 2 个功能吗？例如，打开浏览器和游戏的功能。
也就是说，我希望它看起来像这样：
data_set = { &#39;whats new&#39;:&#39;game browser 没什么特别的...&#39;, &#39;&#39;:&#39;&#39;, }

def game():
pass
def browser():
pass

我希望我的助手在一个短语中同时执行 2 个功能]]></description>
      <guid>https://stackoverflow.com/questions/78833739/how-to-combine-2-functions-in-one-phrase-machine-learning-sklearn</guid>
      <pubDate>Mon, 05 Aug 2024 09:28:05 GMT</pubDate>
    </item>
    <item>
      <title>如何保存 TensorFlow 模型并在不同的文件中使用它？</title>
      <link>https://stackoverflow.com/questions/78833727/how-to-save-a-tensorflow-model-and-use-it-in-a-different-file</link>
      <description><![CDATA[我想保存我训练过的模型并将其加载到另一个文件中。
我尝试使用
model.save(my_model.keras)
但当我将其加载到另一个文件中或在运行时断开连接后加载它时（Colab 笔记本），它不起作用。有人可以建议不同的方法或解释我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78833727/how-to-save-a-tensorflow-model-and-use-it-in-a-different-file</guid>
      <pubDate>Mon, 05 Aug 2024 09:24:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 检测二进制斑点中的文本</title>
      <link>https://stackoverflow.com/questions/78833072/detecting-text-in-binary-blobs-using-ml</link>
      <description><![CDATA[上下文 - 我是一名经验丰富的 SWE，但在 ML 方面经验不多，我想执行以下操作：
给定一个大小为 256 的二进制 blob（字节数组），由嵌入有意义文本的随机字节组成，我想用 Python 编写一个可以检测所述文本的 ML 操作。
到目前为止，我已经使用了以下功能/标签：

字节值/预期文本
字节是否为 utf8/blob 中文本的开始/结束位置
每个字节前/后 N 个字节中 utf8 的字节数/偏移量 + 文本长度
我还尝试了几个分类器，包括 SVR 和 NN，但结果非常令人失望。欢迎提出任何想法。

带有预期文本“翻译 NLP”的示例 blob python&quot;

DEC776ADF283C633EFD77472616E736C6174696F6E204E4C5020707974686F6E0B8B520125C98845BC3C4830190ABDE803578AAB59C17F5444887672B7F4405B9167D23EE9893D5415981DF3F6BF06663DDADDFD921F3F7F7EE6C36E0050ED6B8332FAD95CB88CD1AAA33216445FD28384E1E03CB3E3192A78C310CAD18 292D93B727BA73A31AB60A93A4651B7AD8F921CC0055BD4BCE531536D38019C5DA076B66E922FDC76954B32E04FF9F94B9150AC7A20472194DA4CEE348F9115707FEC6 CCD79AC4 6DA94FE489D521A093045BC1929CAD5B77D88C66B86249006FF2FF83358E8E112E1390B4D02461603612BC9EC772A34B63B7FDB5743FD951337AF8B

在十六进制工作室中它看起来像这样：
]]></description>
      <guid>https://stackoverflow.com/questions/78833072/detecting-text-in-binary-blobs-using-ml</guid>
      <pubDate>Mon, 05 Aug 2024 06:46:19 GMT</pubDate>
    </item>
    <item>
      <title>用于聚类的机器学习模型（与 K-means 类似，但功能不同）[关闭]</title>
      <link>https://stackoverflow.com/questions/78833002/machine-leraning-model-for-clusteringsimilar-with-k-means-but-different-functio</link>
      <description><![CDATA[当我研究几种机器学习模型时，
我看到了几种聚类算法，包括 K-Means。
据我所知，K-Means 使用欧几里得距离作为自己的计算方法，
我想要的不是使用欧几里得距离，而是数据的值。
例如，样本分布很广（如坐标），坐标有自己的值。
我想找到平均值较高的 N 个聚类。
有没有其他适合此图的算法，或者我是否只处理 K-Means 算法中的几个参数即可完成此操作。
谢谢
有没有其他适合此图的算法，或者我是否只处理 K-Means 算法中的几个参数即可完成此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78833002/machine-leraning-model-for-clusteringsimilar-with-k-means-but-different-functio</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>createDataPartition 给出异常不均匀的测试和训练集</title>
      <link>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</link>
      <description><![CDATA[我正在尝试使用 caret 包将我的数据拆分为测试集和训练集。我有 77 行，每列都有完整数据。函数“createDataPartition”导致训练数据为 4 行，测试数据为 73 行，这似乎不对。任何帮助都将不胜感激。这是我的代码：
&gt; # 将数据拆分为训练和测试
&gt; set.seed(123)
&gt; data.full &lt;- data.full %&gt;% select(fasting_status, a1c, glu, uc_ratio)
&gt; training.samples &lt;- data.full %&gt;% 
+ createDataPartition(p = 0.8, list = FALSE)
警告消息：
1：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类没有记录 ( )，这些将被忽略
2：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类只有一条记录 ( )，这些将被选为样本
&gt; train.data &lt;- data.full[training.samples, ]
&gt; test.data &lt;- data.full[-training.samples, ] ```

这是我的可重现数据：

```&gt; dput(data.full) 结构(列表(fasting_status = 结构(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L), 级别 = c(&quot;1&quot;, &quot;2&quot;), 类别 = &quot;因素&quot;), 
a1c = c(4.3, 4.5, 4.4, 2.9, 4.3, 4.4, 4.2, 4.5, 4.2, 4.2, 
4.5, 4.5, 4.8, 4.5, 5.2, 4.9, 4.6, 4.2, 4.4, 4.9, 4.6, 4.5, 
    4.4、4.8、4.5、4.1、3.8、3.1、4.3、4.6、4.7、4.9、4.6、4.4、3.1、4.6、4.4、4.2、4.4、5.2、4.4、5.1、4.6、4.7、5.2、4.7、4。 7、4.6、4.4、4.4、4.2、4.5、4.6、4.4、3.2、4.8、5.2、5.2、4.6、4.9、5.6、4.6、4.9、4.5、5.1、4.6、4.9、4.6、4.3、4.6、
4.6, 4.3, 4.6, 4.3, 4.6, 6.5, 4.8), glu = c(88.5, 98, 117.5, 
53, 108.5, 106, 105, 101, 91, 99.5, 128.5, 113, 114, 121.5, 
121, 131.5, 160.5, 96, 110, 140, 119.5, 115.3, 112, 143.5, 
116.5, 116.5, 111, 139.5, 123.5, 131, 113, 137, 114, 98.5, 
    124.5、123.5、111.5、111、127、123、137.5、119、107、130.5、142.5、115、133.5、119、148.3、125.5、138.5、106.5、153.5、 .5、179、145、143、124.5、134、146.5、127.5、124.5、123、129、145.3、125.5、146.5、153.5、115.5、128、110.5、131、 
139.5, 124, 154, 94, 76.3), uc_ratio = c(30.65603924, 15.32801962, 
60.59075991, 7.39973361, 57.84661317, 27.46781116, 16.0944206, 
6.131207848, 94.61568474, 19.50838861, 7.803355443, 19.41549152, 
7.464079119, 19.67095851, 29.50643777, 62.94706724, 80.472103、25.75107296、73.57449418、39.01677721、41.13018598、10.62933697、7.803355443、30.04291845、32.75355771、 9416、5.969860273、22.72153497、7.153075823、75.61823012、23.50296342、53.64806867、11.19611891、38.25340549、 88.36152487、51.50214592、9.196811772、41.98544505、6.35828962、9.196811772、94.87237407、12.87553648、6.035407725、7.3997 3361、10.72961373、11.70503316、9.035464197、16.34988759、11.68917269、35.11509949、61.85306741、11.36076748、 
    12.2624157、7.153075823、14.30615165、10.40447392、3.901677721、52.11526671、21.45922747、30.49469166、81.06819266、 38861、34.33476395、8.0472103、24.94635193、9.754194304、64.3776824、9.196811772、11.92179304、34.87124464、 74.39198856, 124.4635193, 
13.79521766, 5.722460658, 66.76204101, 69.9757432, 19.50838861
)), row.names = c(NA, -77L), class = &quot;data.frame&quot;)```
]]></description>
      <guid>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</guid>
      <pubDate>Mon, 05 Aug 2024 02:29:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 NLP 的 MLP 与 Transformer 架构</title>
      <link>https://stackoverflow.com/questions/78832485/mlp-vs-transformer-architecture-for-nlp</link>
      <description><![CDATA[我不太明白在 NLP 中使用经典 MLP 与自注意力转换器之间的区别。自注意力转换器能做什么而 MLP 不能？它与仅仅添加更多隐藏层有何不同？我理解发送键和查询然后创建注意力权重的要点，但对我来说，直觉上（我知道我错了），这似乎是一种额外的抽象，可以做 MLP 用更多隐藏层可以做的事情。转换器修复了 MLP 的 NLP 架构的哪些根本问题？]]></description>
      <guid>https://stackoverflow.com/questions/78832485/mlp-vs-transformer-architecture-for-nlp</guid>
      <pubDate>Mon, 05 Aug 2024 01:51:18 GMT</pubDate>
    </item>
    <item>
      <title>如何对 Keras 模型的计算使用情况进行基准测试</title>
      <link>https://stackoverflow.com/questions/78832141/how-to-benchmark-keras-model-computational-use</link>
      <description><![CDATA[我正在做一个涉及板载图像分类系统的项目。由于它的硬件非常有限，我想找到一种方法来“基准测试”模型预测如何影响我的 CPU 和内存。我正在使用 google colab 上的 Keras 训练 CNN 模型，并希望获取有关 CPU 使用率和内存使用率的信息，我只想要有关预测部分的信息，而不是训练，因为它不会在板载完成。
出于某种原因，我很难在互联网上找到对这些参数进行基准测试的项目，我找到的是 psutil 和 memory_profiler python 库。所以我想知道使用这些库是否是对这些参数的良好估计，或者在我的硬件上加载模型之前是否有其他更好的方法来测量它。]]></description>
      <guid>https://stackoverflow.com/questions/78832141/how-to-benchmark-keras-model-computational-use</guid>
      <pubDate>Sun, 04 Aug 2024 21:12:56 GMT</pubDate>
    </item>
    <item>
      <title>未找到检查点</title>
      <link>https://stackoverflow.com/questions/78831928/no-checkpoint-found</link>
      <description><![CDATA[# 训练模型并创建检查点以存储权重
import os

checkpoint_dir = &#39;./training_checkpoints&#39;

# 检查点文件的名称
checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt_{epoch}.weights.h5&quot;) # 将 &#39;.weights.h5&#39; 添加到文件名
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)

EPOCHS = 10
history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])

checkpoint_dir = &#39;/content/training_checkpoints&#39;
latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

if latest_checkpoint:
print(f&quot;最新检查点发现：{latest_checkpoint}&quot;)
model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)
model.load_weights(latest_checkpoint) 
model.build(tf.TensorShape([1, None]))
else:
print(&quot;未找到检查点。&quot;)

输出：
未找到检查点。

我原本期望在完成 10 个 epoch 后生成最新的检查点，但却没有得到任何检查点……似乎代码无法检测到最新的检查点。为什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78831928/no-checkpoint-found</guid>
      <pubDate>Sun, 04 Aug 2024 19:16:56 GMT</pubDate>
    </item>
    <item>
      <title>将来自 OneHotencoder 的稀疏矩阵转换为数组，然后转换为 DataFrame [关闭]</title>
      <link>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</link>
      <description><![CDATA[我在将 OneHotencoder 传来的稀疏矩阵转换为数组，然后再转换为 DataFrame 的过程中遇到了错误，即
ValueError: 必须传递 2-D 输入。shape=()

实际上，它之前运行良好。但在重新启动内核后重新运行 shell 后遇到了这个问题。
对于以下代码：
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown=&#39;ignore&#39;)
temp = ohe.fit_transform(df[[&#39;encoded_data&#39;]])

temp = pd.DataFrame(np.array(temp))
temp
]]></description>
      <guid>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</guid>
      <pubDate>Sun, 04 Aug 2024 17:09:05 GMT</pubDate>
    </item>
    <item>
      <title>生成 512x512 照片的模型</title>
      <link>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</link>
      <description><![CDATA[我如何让这个模型生成 512x512 像素或更大的图像？现在它生成 64x64 像素的图像。我尝试更改模型中的某些值，但没有成功。这些卷积层如何工作，尤其是 Conv2D 和 Conv2DTranspose？我不明白图像在这些层中是如何调整大小的。
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layer
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

cd /content/drive/MyDrive

dataset = keras.preprocessing.image_dataset_from_directory(
directory = &#39;Humans&#39;, label_mode = None, image_size = (64,64), batch_size = 32,
shuffle = True
).map(lambda x: x/255.0)

discriminator = keras.models.Sequential(
[
keras.Input(shape = (64,64,3)),
layer.Conv2D(64, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Flatten(),
layers.Dropout(0.2),
layers.Dense(1,activation = &#39;sigmoid&#39;)
]
)

latent_dim = 128
generator = keras.models.Sequential(
[
layers.Input(shape = (latent_dim,)),
layers.Dense(8*8*128),
layers.Reshape((8,8,128)),
layers.Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(256, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(512, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(3, kernel_size = 5,padding = &#39;same&#39;,activation = &#39;sigmoid&#39;)
]
)

opt_gen = keras.optimizers.Adam(1e-4)
opt_disc = keras.optimizers.Adam(1e-4)
loss_fn = keras.losses.BinaryCrossentropy()

for epoch 在 range(500) 中：
对于 idx，real 在 enumerate(tqdm(dataset)) 中：
batch_size = real.shape[0]
random_latent_vectors = tf.random.normal(shape = (batch_size,latent_dim))
fake = generator(random_latent_vectors)

如果 idx % 50 == 0：
img = keras.preprocessing.image.array_to_img(fake[0])
img.save(f&#39;gen_images/generated_img{epoch}_{idx}_.png&#39;)

使用 tf.GradientTape() 作为 disc_tape：
loss_disc_real = loss_fn(tf.ones((batch_size,1)), discriminator(real))
loss_disc_fake = loss_fn(tf.zeros(batch_size,1), discriminator(fake))
loss_disc = (loss_disc_real+loss_disc_fake)/2

grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)

opt_disc.apply_gradients(
zip(grads, discriminator.trainable_weights)
)

with tf.GradientTape() as gen_tape:
fake = generator(random_latent_vectors)
output = discriminator(fake)
loss_gen = loss_fn(tf.ones(batch_size,1),output)

grads = gen_tape.gradient(loss_gen, generator.trainable_weights)
opt_gen.apply_gradients(
zip(grads, generator.trainable_weights)
)

我尝试更改图像大小和卷积层中的某些值，但它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</guid>
      <pubDate>Sun, 04 Aug 2024 13:42:23 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更高的余弦相似度分数[关闭]</title>
      <link>https://stackoverflow.com/questions/78830868/how-to-get-higher-score-for-cosine-similarity</link>
      <description><![CDATA[我已经使用 nltk 清理了我的数据，我的数据是完全干净的，但我仍然无法获得更高的相似度得分，我正在制作一个食谱推荐系统，该系统需要配料并返回我们可以烹饪的食谱
https://colab.research.google.com/drive/1YnM0tUyWhhTQIXZipWdzIFfZGJHViFpi?usp=sharing
帮忙
我正在尝试获得更高的相似度得分，但我仍然停留在 0.62，数据集有超过 6000 行]]></description>
      <guid>https://stackoverflow.com/questions/78830868/how-to-get-higher-score-for-cosine-similarity</guid>
      <pubDate>Sun, 04 Aug 2024 10:55:14 GMT</pubDate>
    </item>
    <item>
      <title>通过 OpenCV 快速查找重复图像[关闭]</title>
      <link>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</link>
      <description><![CDATA[我正在尝试通过 C# 添加一种在 MongoDB 中存储和查找重复图片的方法。我有一个应用程序，允许用户创建图像并将其保存在我的网站上。我将它们存储在 MongoDB 中，现在想查找重复项，但由于图像数量众多，我无法只用其他图像检查新图像。我希望有一些索引和标志来快速找到可能的重复项，然后完全检查它们。我使用 OpenCV 查找图片的描述符并存储它们，以便之后我可以轻松地用新图片检查它们。我可以使用什么作为标志或索引来不搜索整个数据库？有没有快速的方法？这是我的代码示例，它允许我找到描述符：
Mat img1 = Cv2.ImRead(&quot;somepic&quot;, ImreadModes.Grayscale); 
var orb = ORB.Create(); 
KeyPoint[] keyPoints; 
Mat descriptors = new Mat(); 
orb.DetectAndCompute(img1, null, out keyPoints, descriptors);

有一个通过描述符比较两幅图像的示例（左侧原始图像和右侧裁剪后的图像）。所以这种方法确实有效。但是如何在数百万张图片中快速做到这一点？
比较图像
我听说我可以将描述符分成 4-8 个部分并将它们用作索引，但无法保证我会找到可能的重复项。我也听说过 k-means，但我也不了解如何将其与描述符一起使用。或者也许还有其他没有机器学习的方法？
附言：我尝试过 PHash，但它对裁剪后的图片效果很糟糕。]]></description>
      <guid>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</guid>
      <pubDate>Sat, 03 Aug 2024 19:14:23 GMT</pubDate>
    </item>
    <item>
      <title>在音频 AI 中实现迁移学习</title>
      <link>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</link>
      <description><![CDATA[虽然我的任务是从音频中检测疾病，但我能否通过迁移一些预训练网络来实现 CNN 神经网络（以 MFCC 为输入）来进行语音处理？
我的意思是，虽然这两项任务彼此不同，但原则上这样做可以吗？
因为一般来说，我看到的迁移学习是当模型经过预训练时检测人，然后你进行微调以检测你想要的人]]></description>
      <guid>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</guid>
      <pubDate>Sat, 03 Aug 2024 14:06:15 GMT</pubDate>
    </item>
    <item>
      <title>对 GAN 输出大小的困惑</title>
      <link>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</link>
      <description><![CDATA[我正在尝试理解代码，我对测试单元感到困惑。当我打印输出的形状时，它是 hidden_​​output.shape =(num_test, 20, 4, 4), test_hidden_​​block_stride(hidden_​​output).shape) == (num_test, 20, 10, 10) 和 Gen_output.shape=(num_test, 1,28,28)（对于 Mnist 数据集）。我试图理解这里的大小是如何计算的。任何帮助都将不胜感激！
class Generator(nn.Module):
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.gen = nn.Sequential(
self.make_gen_block(z_dim, hidden_​​dim * 4),
self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1),
self.make_gen_block(hidden_​​dim * 2, hidden_​​dim),
self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True),
)
def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0 ,final_layer=False):
# 构建神经块
layer = []
layer.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, output_padding=padding))
if not final_layer:
layer.append(nn.BatchNorm2d(output_channels))
layer.append(nn.ReLU(True))
else:
layer.append(nn.Tanh())

return nn.Sequential(*layers)
# 测试
gen = Generator()
num_test = 100
# 测试隐藏块
test_hidden_​​noise = get_noise(num_test, gen.z_dim)
test_hidden_​​block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)
test_uns_noise = gen.unsqueeze_noise(test_hidden_​​noise)
hidden_​​output = test_hidden_​​block(test_uns_noise)
# 检查它是否与其他 strides 兼容
test_hidden_​​block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)
test_final_noise = get_noise(num_test, gen.z_dim) * 20
test_final_block = gen.make_gen_block(10, 20, final_layer=True)
test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)
final_output = test_final_block(test_final_uns_noise)
# 测试整个过程：
test_gen_noise = get_noise(num_test, gen.z_dim)
test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)
gen_output = gen(test_uns_gen_noise)

我正在尝试手动计算公式中的大小。我只是看到不同的内核大小、步幅和填充。不确定要使用哪些值。]]></description>
      <guid>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</guid>
      <pubDate>Sun, 30 Jun 2024 00:41:37 GMT</pubDate>
    </item>
    </channel>
</rss>