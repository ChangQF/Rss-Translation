<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 14 Dec 2023 09:13:40 GMT</lastBuildDate>
    <item>
      <title>使用机器学习回归的图像相似度百分比</title>
      <link>https://stackoverflow.com/questions/77658841/similarity-percentage-on-images-using-machine-learning-regression</link>
      <description><![CDATA[我对此很陌生，但我正在开发一个项目，该项目对包含两种花的数据集使用分类和回归：雏菊和向日葵。我已经成功地应用随机森林分类来找到每个测试图像的类别，但我的问题是如何使用回归来显示图像与每个类别共享的相似度百分比？（例如雏菊图片为 87%雏菊和 13% 向日葵）。我知道回归不是最好的方法，但我将其作为一项作业来做，所以我必须这样做。谢谢！！
我尝试过随机森林回归器和 sci 套件预测概率，但我的程序完全冻结了，所以我猜我做错了什么。
这是与我的问题相关的代码片段：
# 将列表转换为 numpy 数组
test_images = np.array(test_images)
test_labels = np.array(test_labels)
test_probabilities = classifier.predict_proba(test_images)

# 为测试集创建交互式图像查看器
图像查看器类：
    def __init__(自身、图像、真实标签、预测概率):
        self.images = 图像
        self.true_labels = true_labels
        自我预测概率 = 预测概率
        自我索引 = 0

        self.fig, self.ax = plt.subplots()
        self.display_image()

        self.next_button = 按钮(plt.axes([0.7, 0.02, 0.1, 0.05]), &#39;下一个&#39;)
        self.next_button.on_clicked(self.next_image)

        plt.show()

    def display_image(自身):
        img = self.images[self.index].reshape(256, 256, 3)
        true_class = self.true_labels[self.index]
        预测概率=自我.预测概率[自我.索引]

        self.ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        self.ax.set_title(f&quot;真: {true_class}\n预测概率: {predicted_probs}&quot;)
        self.ax.axis(&#39;关闭&#39;)

    def next_image(自身, 事件):
        self.index = (self.index + 1) % len(self.images)
        self.display_image()
        plt.draw()

# 初始化测试集的图像查看器
test_image_viewer = ImageViewer（测试图像，测试标签，测试概率）

这段代码不显示任何内容，正如我所说，我也在同一个程序中使用分类，但它工作正常，直到我添加回归。另外，我显示图像的方式有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77658841/similarity-percentage-on-images-using-machine-learning-regression</guid>
      <pubDate>Thu, 14 Dec 2023 08:55:16 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“tensorflow_federated.python.simulation”没有属性“from_clients_and_fn”</title>
      <link>https://stackoverflow.com/questions/77657875/attributeerror-module-tensorflow-federated-python-simulation-has-no-attribute</link>
      <description><![CDATA[&lt;块引用&gt;
将 pandas 导入为 pd
将tensorflow_federated导入为tff
df = pd.DataFrame({
&#39;用户 ID&#39;: [1, 1, 2, 3],
&#39;产品 ID&#39;: [2, 3, 1, 2],
&#39;购买&#39;: [1, 0, 1, 0],
&#39;浏览&#39;: [0, 1, 0, 1]
})
数据 = {}
对于 df.groupby(“user_id”) 中的 user_id、user_data：
购买 = user_data[user_data[“购买”] == 1][“product_id”].tolist()
browsers = user_data[user_data[“browse”] == 1][“product_id”].tolist()
data[user_id] =（购买、浏览）
client_data = [data[user_id] for data中的user_id]
client_ids = [str(i) for i in range(len(client_data))]
tff_data = tff.simulation.datasets.TestClientData(client_data, client_ids)
数据集 = tff_data.create_tf_dataset_for_client(tff_data.client_ids[0])
我在最后一步中遇到问题，AttributeError: module &#39;tensorflow_federated.python.simulation&#39; has no attribute &#39;from_clients_and_fn&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/77657875/attributeerror-module-tensorflow-federated-python-simulation-has-no-attribute</guid>
      <pubDate>Thu, 14 Dec 2023 05:05:24 GMT</pubDate>
    </item>
    <item>
      <title>根据提示对 NLP 任务进行分类？</title>
      <link>https://stackoverflow.com/questions/77657539/classifying-nlp-tasks-based-on-prompts</link>
      <description><![CDATA[目前，我正在为公司创建一个聊天机器人。我被分配的任务是“根据提示对 NLP 任务进行分类”。例如确定用户请求是总结文本还是翻译段落。
我在 Google 上搜索过，但没有找到明确的答案。您能否建议如何完成这项任务或分享任何相关的研究资源？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77657539/classifying-nlp-tasks-based-on-prompts</guid>
      <pubDate>Thu, 14 Dec 2023 02:48:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Marqo 进行矢量搜索的单个或多个键值对数据结构？</title>
      <link>https://stackoverflow.com/questions/77657071/single-or-multiple-key-value-pair-data-structure-for-vector-search-with-marqo</link>
      <description><![CDATA[我正在使用 Marqo Cloud 为工作项目实施矢量搜索。我的文档（产品）有一些数据，其结构可以如下：
单个键值对，例如：标签：红色、斑点、尼龙、休闲
或者每个标签标题包含多个键值对，例如：
红色
设计：斑点
材质: 尼龙
风格：休闲
在矢量搜索中，这些数据结构中的一种会比另一种表现得更好吗？或者差异可能可以忽略不计？]]></description>
      <guid>https://stackoverflow.com/questions/77657071/single-or-multiple-key-value-pair-data-structure-for-vector-search-with-marqo</guid>
      <pubDate>Wed, 13 Dec 2023 23:34:17 GMT</pubDate>
    </item>
    <item>
      <title>8 位量化是否应该使 GPU 上的耳语推理速度更快？</title>
      <link>https://stackoverflow.com/questions/77656929/should-8bit-quantization-make-whisper-inference-faster-on-gpu</link>
      <description><![CDATA[我正在对拥抱脸变压器进行耳语推理。
load_in_8bit 量化由 bitsandbytes 提供。
如果在 NVIDIA T4 GPU 上以 8 位模式加载 Whisper-large-v3，则对示例文件的推理需要更长的时间 (5 倍)。 nvidia-smi 中的 GPU 利用率为 33%。
量化不应该提高 GPU 上的推理速度吗？
https://pytorch.org/docs/stable/quantization.html
类似问题：

https://discuss .huggingface.co/t/enabling-load-in-8bit-makes-inference-much-slower/38596

&lt;代码&gt;
进口火炬

从转换器导入 WhisperFeatureExtractor、WhisperTokenizerFast
从 Transformers.pipelines.audio_classification 导入 ffmpeg_read

MODEL_NAME =“openai/whisper-large-v3”

tokenizer = WhisperTokenizerFast.from_pretrained(MODEL_NAME)
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)

model_8bit = AutoModelForSpeechSeq2Seq.from_pretrained(
     “openai/whisper-large-v3”，
    device_map=&#39;自动&#39;,
    load_in_8bit=真）

样本=“样本.mp3”；第27章 长

使用 torch.inference_mode()：
    将 open(sample, &quot;rb&quot;) 作为 f：
        输入 = f.read()
        输入= ffmpeg_read（输入，feature_extractor.sampling_rate）

        input_features = feature_extractor（输入，sampling_rate = feature_extractor.sampling_rate，return_tensors =&#39;pt&#39;）[&#39;input_features&#39;]

        input_features = torch.tensor(input_features, dtype=torch.float16, device=&#39;cuda&#39;)

        forced_decoder_ids_output = model_8bit.generate(input_features=input_features, return_timestamps=False)

        out = tokenizer.decode(forced_decoder_ids_output.squeeze())
        打印出）
]]></description>
      <guid>https://stackoverflow.com/questions/77656929/should-8bit-quantization-make-whisper-inference-faster-on-gpu</guid>
      <pubDate>Wed, 13 Dec 2023 22:43:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在文本分类中得到一长串零？</title>
      <link>https://stackoverflow.com/questions/77656547/why-do-i-get-a-long-list-of-zeros-in-classification-of-text</link>
      <description><![CDATA[我有 500 条来自 YouTube 的俄语评论。我使用 youtokentome 库对它们进行标记。
df[&#39;textOriginal&#39;].to_csv(&#39;text.txt&#39;, index=False, header=False)

model_path = &#39;tokenizer.model&#39;
yttm.BPE.train(data=&#39;text.txt&#39;, model=model_path, vocab_size=5000)

tokenizer = yttm.BPE(模型=模型路径)

df[&#39;tokens&#39;] = df[&#39;textOriginal&#39;].apply(lambda x: tokenizer.encode(x, output_type=yttm.OutputType.ID))

文本标记示例
接下来，我给出张量中的标记列表。
tokens_tensor = df[&#39;tokens&#39;].apply(lambda x: torch.tensor(x)).tolist()
tokens_tensor = torch.nn.utils.rnn.pad_sequence（tokens_tensor，batch_first=True）

接下来，我想将文本分为 3 类。为此，我使用 nn.Embedding+nn.LIST+ nn.Linear。
但是模型的返回值我不清楚。我得到一长串零。
如何获得对象的分类？
我的模型代码：
&lt;前&gt;&lt;代码&gt;embedding_dim = 300
词汇大小 = 5000
隐藏大小 = 512
输出暗度 = 3

导入 torch.nn.function 作为 F

类 MyModel(nn.Module):
    def __init__(自身、vocab_size、embedding_dim、hidden_​​size、output_dim、dropout_rate=0.5):
        超级（MyModel，自我）.__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_​​size=hidden_​​size,batch_first=True)
        self.线性 = nn.Linear(hidden_​​size, output_dim)


    defforward(self, input_seq):
        嵌入 = self.embedding(input_seq)
        lstm_out, _ = self.lstm(嵌入)
        lstm_out = lstm_out[:, -1, :]
        x = self.线性(lstm_out)
        返回 F.log_softmax(x, 暗淡=1)
]]></description>
      <guid>https://stackoverflow.com/questions/77656547/why-do-i-get-a-long-list-of-zeros-in-classification-of-text</guid>
      <pubDate>Wed, 13 Dec 2023 21:08:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个用于线性回归的 cpp 代码不能很好地工作，而 python 对应的代码却可以？我做错了什么[关闭]</title>
      <link>https://stackoverflow.com/questions/77656109/why-does-this-cpp-code-for-linear-regression-not-work-well-whereas-the-python-co</link>
      <description><![CDATA[所以我正在学习 Andrew ng 的机器学习课程，在学习了单个特征的线性回归之后，我试图让它在 cpp 中工作，但与按我预期工作的 python 代码相比，代码提供了非常奇怪的结果它...
#include ;
使用命名空间 std；

结构梯度结果{
    浮动w_结果；
    浮动b_结果；
};

结构计算梯度{
    浮动 dj_dw_结果；
    浮动 dj_db_结果；
};

ComputeGradient 计算(float x[], float y[], float w, float b, float s)
{
    浮动 dj_dw = 0.0;
    浮动 dj_db = 0.0;

    for (int i = 0; i &lt; s; i++)
    {
        浮点数 f_wb = w * x[i] + b;
        浮点数 dj_dw_i = (f_wb - y[i]) * x[i];
        浮点数 dj_db_i = (f_wb - y[i]);
        dj_dw += dj_dw_i;
        dj_db += dj_db_i;
    }
    dj_dw = dj_dw / s;
    dj_db = dj_db / s;

    计算梯度cResult；
    cResult.dj_dw_result = dj_dw;
    cResult.dj_db_result = dj_db;

    返回结果；
}

GradientResult 梯度下降(float w, float b, float x[], float y[], float s, int 迭代器, float alpha)
{
    ComputeGradient cResult = 计算(x, y, w, b, s);

    浮点数 local_w = w;
    浮点数 local_b = b;

    for (int i = 0; i &lt; 迭代器; i++)
    {
        浮动 dj_dw = cResult.dj_dw_result;
        浮动 dj_db = cResult.dj_db_result;
        local_w = local_w - alpha * dj_dw;
        local_b = local_b - alpha * dj_db;
    }

    GradientResult结果；
    结果.w_result = local_w;
    结果.b_result = local_b;

    返回结果；
}

int main()
{

    浮点数 xtrain[] = {2.4, 5.3, 9.7, 6.2, 13.6, 29.8};
    浮点 ytrain[] = {6.9, 14.3, 23.6, 17.8, 29.2, 42.6};

    浮点 s = sizeof(xtrain) / sizeof(float);
    浮动w_init = 0.0;
    浮动b_init = 0.0;
    int迭代= 1000000；
    浮动学习率= 0.0001；

    GradientResult 结果 = 梯度下降（w_init，b_init，xtrain，ytrain，s，迭代次数，learning_rate）;
    浮动 w = 结果.w_结果;
    浮动 b = 结果.b_结果;

    计算&lt;&lt; w &lt;&lt;结束&lt;&lt; b &lt;&lt;结束；
    
    for(int i = 0; i &lt; s; i++)
    {
        浮动 ans = w * xtrain[i] + b;
        计算&lt;&lt;一个&lt;&lt;结束；
    }

}

这是 cpp 代码，这是 python 代码：
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 lab_utils_uni 导入 plt_intuition、plt_stationary、plt_update_onclick、soup_bowl
plt.style.use(&#39;./deeplearning.mplstyle&#39;)

xtrain = np.array([2.4, 5.3, 9.7, 6.2, 13.6, 29.8])
ytrain = np.array([6.9, 14.3, 23.6, 17.8, 29.2, 42.6])

def计算成本（x，y，w，b）：
    成本=0
    m = x.shape[0]
    
    对于范围 (m) 内的 i：
        f_wb = w * x[i] + b
        成本 = 成本 + (f_wb - y[i])**2
    总成本 = 1/(2 * m) * 成本
    
    返回总成本

def 计算梯度(x, y, w, b):
    m = x.shape[0]
    dj_dw = 0
    DJ_数据库 = 0
    
    对于范围 (m) 内的 i：
        f_wb = w * x[i] + b
        dj_dw_i = (f_wb - y[i]) * x[i]
        dj_db_i = (f_wb - y[i])
        dj_dw += dj_dw_i
        dj_db += dj_db_i
    dj_dw = dj_dw/m
    dj_db = dj_db/m
    
    返回 dj_dw、dj_db

def梯度下降（x，y，w_in，b_in，alpha，迭代，成本函数，梯度函数）：
    本地_w = w_in
    本地_b = b_in
    wb_历史记录 = []
    
    对于范围内的 i（迭代）：
        dj_dw, dj_db = 梯度函数(x, y, local_w, local_b)
        
        local_w = local_w - (alpha * dj_dw)
        local_b = local_b - (alpha * dj_db)
        
        wb_history.append([local_w, local_b])
        
    返回 local_w、local_b、wb_history

w_init = 0
b_init = 0

迭代器 = 1000000
阿尔法 = 0.0001
wb_历史记录 = []

Final_w，final_b，wb_history = 梯度下降（xtrain，ytrain，w_init，b_init，alpha，迭代器，compute_cost，computeGradient）

对于范围 (6) 内的 i：
    print(final_w * xtrain[i] + Final_b, &quot;\n&quot;)
打印（最终_w，最终_b）


你能帮我找出我做错的地方吗
功能的输入是这样的：
2.4、5.3、9.7、6.2、13.6、29.8
与输出配对：
6.9、14.3、23.6、17.8、29.2、42.6
这是 python 的结果：
&lt;前&gt;&lt;代码&gt;11.737688309201088
15.264764800070795
20.616191200011038
16.35937474551312
25.359500963594435
45.062479981556244

这是 cpp 的结果：
&lt;前&gt;&lt;代码&gt;86530.3
188406
342975
220022
479980
1.04908e+006
]]></description>
      <guid>https://stackoverflow.com/questions/77656109/why-does-this-cpp-code-for-linear-regression-not-work-well-whereas-the-python-co</guid>
      <pubDate>Wed, 13 Dec 2023 19:26:27 GMT</pubDate>
    </item>
    <item>
      <title>Hessian 矩阵最后一层的特征值比其他层小得多[关闭]</title>
      <link>https://stackoverflow.com/questions/77655218/eigenvalues-of-hessian-matrix-final-layer-much-smaller-than-others</link>
      <description><![CDATA[我在各种简单的神经网络模型（例如前馈网络、LeNet CNN 和单层注意力模型）中遇到了 Hessian 特征值的一致模式。具体来说，最终分类层中的 Hessian 特征值非常小（小于 1e-7），与前面层中的值要大得多。有趣的是，模型深处的特征值大小似乎有增加的趋势，但在分类层突然减小。
这一观察结果似乎违反直觉，特别是考虑到较大的 Hessian 特征值应对应于最不可概括且对数据最敏感的层（请参阅 https://arxiv.org/pdf/1611.01838.pdf)。
我排除了一些事情：

这些模型正在有效地融合并产生具有竞争力的结果
基准数据集上的性能。
各个层的权重大小相似，不是特别大
最后一层很小。
我尝试过各种初始化，甚至合并了
LogSoftmax 非线性进入最后一层。

这是我计算粗麻布和特征值的方法：
defcompute_hessian（参数，损失）：
    “”“”计算给定参数和损失的 Hessian 矩阵。
    # 确保计算损失相对于参数的梯度
    first_grad = torch.autograd.grad(loss, param, create_graph=True)[0]
    dummy_param = torch.ones_like(param)
    hessian = torch.autograd.grad(first_grad, param, grad_outputs=dummy_param, create_graph=True)[0]
    返回粗麻布

def 计算特征值（hessian）：
    ”“”获取特征值“”
    #svd
    特征值 = torch.linalg.svdvals(hessian)
    sum_eigenevalues = torch.sum(特征值)
返回特征值，sum_eigenevalues.item()

作为背景，hessian 矩阵是损失对参数的二阶导数。它通常用于理解损失函数的形状。获取 hessian 的特征值意味着要么获取 hessian 的 SVD（其形状与参数相同），要么构造 (hessian.T hessian) 的语法矩阵。特征值揭示了损失景观的主曲率（即最大下降或上升的方向）。正特征值表明您处于该方向的局部最小值，负值意味着您可能会进一步下降。较小的特征值通常对应于损失景观中较平坦的区域，这通常与神经网络背景下更好的泛化相关。]]></description>
      <guid>https://stackoverflow.com/questions/77655218/eigenvalues-of-hessian-matrix-final-layer-much-smaller-than-others</guid>
      <pubDate>Wed, 13 Dec 2023 16:40:25 GMT</pubDate>
    </item>
    <item>
      <title>最后一行的结果没有像我想象的那样出现[关闭]</title>
      <link>https://stackoverflow.com/questions/77654857/result-of-the-last-line-doesnt-appear-as-i-thought-it-would-be</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77654857/result-of-the-last-line-doesnt-appear-as-i-thought-it-would-be</guid>
      <pubDate>Wed, 13 Dec 2023 15:46:40 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器演示期间用户输入的问题</title>
      <link>https://stackoverflow.com/questions/77653884/a-problem-with-the-user-input-during-the-random-forest-classifier-demonstration</link>
      <description><![CDATA[我使用随机森林分类器获得了超过 90% 的准确率，但我担心其余算法给出的结果要低得多：
包含结果的表格
但这不是主要问题。问题是，当我使用用户输入时，预测是 100% 错误的。用户输入的列的顺序对应于训练数据集列的位置。
模型 = RandomForestClassifier()
model.fit(X_train, y_train)
预测 = model.predict(X_test)
acc = precision_score(y_test, 预测) # 输出：0.91

X_test_user = df_user_compounds_1.to_numpy()
user_input_predictions_1 = model.predict(X_test_user) #
user_input_predictions_1 # 输出： array([0, 0, 0, 0, 0], dtype=int64)，但应该是： array([1, 1, 1, 1, 1],dtype=int64)

有人知道为什么会发生这种情况吗？
数据集经过预处理 - 没有缺失值，没有重复，使用 RandomOverSampler 进行平衡，使用 MinMaxScaler 进行缩放，没有负值，并且包含 11 个特征/7K 行。]]></description>
      <guid>https://stackoverflow.com/questions/77653884/a-problem-with-the-user-input-during-the-random-forest-classifier-demonstration</guid>
      <pubDate>Wed, 13 Dec 2023 13:17:28 GMT</pubDate>
    </item>
    <item>
      <title>执行与自然语言处理相关的代码时出错[关闭]</title>
      <link>https://stackoverflow.com/questions/77653463/error-in-execution-of-code-related-to-natural-language-processing</link>
      <description><![CDATA[此代码显示了图像中给出的错误。我无法理解其中的原因。
导入系统
断言 sys.version_info[0]==3
断言 sys.version_info[1] &gt;= 5

从平台导入 python_version
assert int(python_version().split(&quot;.&quot;)[1]) &gt;= 5, &quot;请按照\中的说明升级您的Python版本
    在与此笔记本相同的目录中找到 README.txt 文件。您的 Python 版本是“ + python_版本()

从 gensim.models 导入 KeyedVectors
从 gensim.test.utils 导入数据路径
导入打印件
将 matplotlib.pyplot 导入为 plt
plt.rcParams[&#39;figure.figsize&#39;] = [10, 5]

导入nltk
nltk.download(&#39;reuters&#39;) #指定下载位置，可选添加参数：download_dir=&#39;/specify/desired/path/&#39;
从 nltk.corpus 导入路透社

将 numpy 导入为 np
随机导入
将 scipy 导入为 sp
从 sklearn.decomposition 导入 TruncatedSVD
从 sklearn.decomposition 导入 PCA

START_TOKEN = &#39;&#39;
END_TOKEN = &#39;&#39;

np.随机.种子(0)
随机种子(0)

错误消息：
&lt;块引用&gt;
[nltk data]加载路透社时出错：

我不知道如何在Python中使用导入命令。我尝试了所有可能的方法进行检查，包括删除带有其他新闻门户名称的“路透社”，但没有任何效果。现在，如果有人帮助我正确编写代码的“导入”部分，那就更好了。我认为其他部分没问题，因为没有显示其他消息。]]></description>
      <guid>https://stackoverflow.com/questions/77653463/error-in-execution-of-code-related-to-natural-language-processing</guid>
      <pubDate>Wed, 13 Dec 2023 12:07:23 GMT</pubDate>
    </item>
    <item>
      <title>具有不同输入形状的 3D 深度学习输入 [关闭]</title>
      <link>https://stackoverflow.com/questions/77602918/3d-deep-learning-input-with-varying-input-shapes</link>
      <description><![CDATA[如何将可变维度的数据集输入到深度学习模型中。
我正在使用可变切片进行 3D 医学成像，我使用了 PCA 和其他切片选择技术以及填充以使模型具有相同的形状
但我想知道是否有任何用于深度学习模型的可变输入形状的技术。
下面是代码：
来自tensorflow.keras导入层

#输入形状 = (200, 200, 60, 1)
输入形状=像素数组[1:]
输入 = keras.Input(shape=(pixel_arrays.shape[1:]))
x=layers.Conv3D(filters=16,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(输入)
x=layers.Conv3D(filters=32,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x=layers.Conv3D(filters=64,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
#x = 层.Conv3D(filters=32, kernel_size=3, 激活=&#39;relu&#39;, padding=&#39;same&#39;)(x)
]]></description>
      <guid>https://stackoverflow.com/questions/77602918/3d-deep-learning-input-with-varying-input-shapes</guid>
      <pubDate>Mon, 04 Dec 2023 22:33:53 GMT</pubDate>
    </item>
    <item>
      <title>如果我的模型在最后一层使用 sigmoid 和二元交叉熵进行训练，我可以输出类的概率而不是 0/1 吗？</title>
      <link>https://stackoverflow.com/questions/70159955/if-my-model-is-trained-using-sigmoid-at-the-final-layer-and-binary-crossentropy</link>
      <description><![CDATA[我使用 sigmoid 函数训练了一个最后带有密集层的 CNN 模型：
model.add(layers.Dense(1,activation=&#39;sigmoid&#39;))

我还使用二进制交叉熵进行了编译：
model.compile(loss=&#39;binary_crossentropy&#39;,
              优化器 = &#39;亚当&#39;,
              指标=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),&#39;准确性&#39;])

二值图像分类的 f1 分数较低，我的模型预测一个类别优于另一个类别。所以我决定根据我的 sigmoid 函数在最后一层的输出概率添加一个阈值：
c = load_img(&#39;/home/kenan/Desktop/COV19D/validation/covid/ct_scan_19/120.jpg&#39;,
             color_mode=&#39;灰度&#39;,
             目标大小 = (512,512))
c=img_to_array(c)
c= np.expand_dims(c, 轴=0)
pred = model.predict_proba(c)
预测
y_classes = ((model.predict(c)&gt; 0.99)+0).ravel()
y_类

我想在代码中使用“pred”作为该类的概率，但它始终为 0 或 1，如下所示：
Out[113]: array([[1.]], dtype=float32)

为什么它不给出预测 [0,1] 之间类别的概率而不是 1？有没有办法获得我的情况下的类概率而不是 0 或 1？]]></description>
      <guid>https://stackoverflow.com/questions/70159955/if-my-model-is-trained-using-sigmoid-at-the-final-layer-and-binary-crossentropy</guid>
      <pubDate>Mon, 29 Nov 2021 18:56:08 GMT</pubDate>
    </item>
    <item>
      <title>gTTS 中没有声音</title>
      <link>https://stackoverflow.com/questions/63464494/no-sound-in-gtts</link>
      <description><![CDATA[我正在尝试使用 gTTS 将文本转换为语音。
导入子流程
从 gtts 导入 gTTS

mytext = &#39;Rasa Bot 用户您好，我是机器人&#39;
语言=&#39;en&#39;
myobj = gTTS(文本 = mytext, lang=语言)
myobj.save(“welcome.mp3”)
subprocess.call([&#39;mpg321&#39;,&#39;welcome.mp3&#39;,&#39;--play-and-exit&#39;])


但是我好像听不到任何声音。我在 Ubuntu 中使用 PyCharm 执行此操作。
终端内容如下：
(venv) rome@rome-VirtualBox:~/Desktop/rasa/intr2$ python ttos.py
mpg321：无法识别的选项“--play-and-exit”
适用于第 1、2 和 3 层的高性能 MPEG 1.0/2.0/2.5 音频播放器。
版本0.3.2-1（2012/03/25）。乔·德鲁 (Joe Drew) 撰写并拥有版权，
现在由 Nanakos Chrysostomos 等人维护。
使用不同人的代码。请参阅“自述文件”了解更多信息！
本软件绝对不提供任何保证！使用风险自负！

正在播放welcome.mp3 中的MPEG 流...
MPEG 2.0 第三层，32 kbit/s，24000 Hz 单声道

请帮忙！！]]></description>
      <guid>https://stackoverflow.com/questions/63464494/no-sound-in-gtts</guid>
      <pubDate>Tue, 18 Aug 2020 08:15:35 GMT</pubDate>
    </item>
    <item>
      <title>设置分类器参数，无需拟合即可使用</title>
      <link>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</link>
      <description><![CDATA[我正在使用 python 和 scikit-learn 进行一些分类。
是否可以重用分类器学习到的参数？
例如：
从 sklearn.svm 导入 SVC

cl = SVC(...) # 使用一些超参数创建 svm 分类器
cl.fit(X_train, y_train)
参数 = cl.get_params()

让我们将这个 params 作为字符串字典存储在某处，甚至写入 json 文件。假设，我们稍后想要使用这个经过训练的分类器对某些数据进行一些预测。尝试恢复它：
params = ... # 检索以字典形式存储在某处的这些参数
data = ... # 我们要预测的数据
cl = SVC(...)
cl.set_params(**参数)
预测 = cl.predict(数据)

如果我这样做，我会得到 NonFittedError 和以下堆栈跟踪：
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 548 行，在预测中
    y = super(BaseSVC, self).predict(X)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 308 行，在预测中
    X = self._validate_for_predict(X)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 437 行，在 _validate_for_predict 中
    check_is_fitted(自我,&#39;support_&#39;)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\utils\validation.py”，第 768 行，在 check_is_fitted 中
    引发 NotFittedError(msg % {&#39;name&#39;: type(estimator).__name__})
sklearn.exceptions.NotFittedError：此 SVC 实例尚未安装。在使用此方法之前，请使用适当的参数调用“fit”。

是否可以为分类器设置参数并在不拟合的情况下进行预测？我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</guid>
      <pubDate>Sun, 14 Jan 2018 17:04:03 GMT</pubDate>
    </item>
    </channel>
</rss>