<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 03:14:41 GMT</lastBuildDate>
    <item>
      <title>推荐系统基于内容的过滤模型——结合相似度矩阵</title>
      <link>https://stackoverflow.com/questions/78170382/recommendation-system-content-based-filtering-model-combine-similarity-matrice</link>
      <description><![CDATA[我第一次使用 python ML 构建电影推荐器。我有一个具有各种属性的电影数据集，以及基于不同属性组的 5 个相似度矩阵。例如，1 个模型显示基于电影标题和情节的相似性向量，另一个模型基于演员阵容等。通过这些，我可以根据 5 组不同的特征找到与输入标题最相似的前 x 部电影。
我的问题是如何组合这 5 个矩阵或其输出来创建一个统一的模型？我读过有关 python 和随机森林中的集成的内容，但我不确定如何在这里应用它们。我有一个名为“likeage”的列，它以数字表示用户是否喜欢某部电影，那么我如何构建一个模型来预测这一点呢？
任何建议或相关文献将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78170382/recommendation-system-content-based-filtering-model-combine-similarity-matrice</guid>
      <pubDate>Sat, 16 Mar 2024 02:05:46 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型仅返回 0 分。我做错了什么？</title>
      <link>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</link>
      <description><![CDATA[在 Jupyter-Notebook 中，我创建了一个函数，可以对不同的 sklearn 机器学习模型进行拟合和评分。使用的数据集有超过 400000 行和 103 列，因此我分为两个不同的数据集：训练数据集和验证数据集。但是当我在函数中使用数据时，我想要测试的所有 4 个模型的得分均为 0。
这是我的代码：
# 分割数据集
df_val = df_tmp[df_tmp[&#39;年份&#39;] == 2012]
df_train = df_tmp[df_tmp[&#39;年份&#39;] != 2012]

# 将数据集分为训练和测试
X_train, y_train = df_train.drop(&#39;年&#39;, axis=1), df_train[&#39;年&#39;]
X_val, y_val = df_val.drop(&#39;年份&#39;, axis=1), df_val[&#39;年份&#39;]

# 将模型放入字典中
测试模型 = {
    “套索”：套索()，
    “ElasticNet”：ElasticNet()，
    “RandomForestRegressor”：RandomForestRegressor()，
    “山脊”：山脊()
}

# 创建函数来评估两个模型
def fit_and_score(test_models, X_train, X_val, y_train, y_val):
    
    # 记录模型分数的字典
    模型分数 = {}
    
    ＃ 环形
    for name, model in test_models.items(): # name, model = key, value
        # 拟合模型
        model.fit(X_train, y_train)
        # 评估模型并将其分数附加到 models_scores
        models_scores[名称] = model.score(X_val, y_val)
        
    返回模型分数

首先我想也许我没有正确编写函数，所以我单独测试了模型，它们仍然得分为 0。之后我决定测试是否我的数据有问题（我不认为这是它，bcz 我从一个旧的 Kaggle 竞赛中得到它，推土机竞赛），所以我用相同的数据训练并安装了一个模型，希望我的分数是 1，但我得到了 0.31。我真的不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</guid>
      <pubDate>Sat, 16 Mar 2024 01:04:49 GMT</pubDate>
    </item>
    <item>
      <title>当调用 sklearn Pipeline 对象上的方法时，Python 会引发 AttributeError</title>
      <link>https://stackoverflow.com/questions/78170066/python-raises-an-attributeerror-when-methods-on-the-sklearn-pipeline-object-are</link>
      <description><![CDATA[问题
我正在对 Pipeline 对象调用 fit_transform() 和 transform() 方法，但每当我尝试时，Python 都会引发 AttributeError这样做。这是我正在尝试运行的导入内容。 （注意：训练/测试分割已经完成）
from sklearn.impute import SimpleImputer
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.pipeline 导入管道

管道 = 管道([(&#39;mean_impute&#39;, SimpleImputer()),
                 (&#39;标准&#39;, StandardScaler()),
                 (&#39;sklearn_lm&#39;, LinearRegression())])

pipeline.fit_transform(x_train, y_train) #&lt;-- 此处错误

x_transform = pipeline.transform(x_test) #&lt;-- 如果上一行不存在

错误内容如下：
属性错误：此“管道”没有属性“fit_transform”
出了什么问题？我确信这很简单。
我尝试过的事情：

查看了 sci-kit learn 的文档，确认 sklearn 中的 Pipeline 对象存在这些方法
检查了 x_train 和 y_train 的大小，确保它们相同，并且都有标题
重新安装sci-kit learn
]]></description>
      <guid>https://stackoverflow.com/questions/78170066/python-raises-an-attributeerror-when-methods-on-the-sklearn-pipeline-object-are</guid>
      <pubDate>Fri, 15 Mar 2024 23:15:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBRegressor 树中叶子值的求和与预测不匹配</title>
      <link>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</link>
      <description><![CDATA[据我了解，XGBoost 模型（在本例中为 XGBRegressor）的最终预测是通过对预测叶子的值求和来获得的 [1] [2]。然而，我未能匹配对值求和的预测。这是 MRE：
导入json
从集合导入双端队列

将 numpy 导入为 np
从 sklearn.datasets 导入 load_diabetes
从 sklearn.model_selection 导入 train_test_split
将 xgboost 导入为 xgb


def leafs_vector(树):
    “”“”返回每棵树的节点向量，只有叶子有 0 个不同“”“”

    堆栈 = 双端队列([树])

    而堆栈：
        节点 = stack.popleft()
        如果“叶”是在节点中：
            产量节点[“叶子”]
        别的：
            产量 0
            对于节点 [“children”] 中的子节点：
                堆栈.追加（子）


# 加载糖尿病数据集
糖尿病 = load_diabetes()
X, y = 糖尿病.数据, 糖尿病.目标

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 XGBoost 回归模型
xg_reg = xgb.XGBRegressor(目标=&#39;reg:squarederror&#39;,
                          最大深度=5，
                          n_估计器=10)

# 训练模型
xg_reg.fit(X_train, y_train)

# 计算原始预测
y_pred = xg_reg.predict(X_test)

# 获取每个预测叶子的索引
Predicted_leafs_indices = xg_reg.get_booster().predict(xgb.DMatrix(X_test), pred_leaf=True).astype(np.int32)

# 获取树木
树 = xg_reg.get_booster().get_dump(dump_format=“json”)
trees = [json.loads(tree) 用于树中的树]

# 获取节点向量（按节点 ID 排序）
leafs = [树中树的列表(leafs_vector(tree))]

l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)))

断言 np.allclose(np.array(l_pred), y_pred, atol=0.5) # 失败

我还尝试添加 base_score 的默认值 (0.5)（如 这里）到总和，但它也不起作用。
&lt;前&gt;&lt;代码&gt;l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)) + 0.5)
]]></description>
      <guid>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</guid>
      <pubDate>Fri, 15 Mar 2024 21:07:25 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret 3.3.0 Compare_models() 显示所有模型 AUC 为零</title>
      <link>https://stackoverflow.com/questions/78169647/pycaret-3-3-0-compare-models-show-zeros-for-all-models-auc</link>
      <description><![CDATA[在使用compare_model()评估模型期间。所有 AUC 均为零。
Pycaret 3.3.0 的这个输出很奇怪。这是什么原因？
[1]: https://i.stack.imgur.com/qm2ZT.png]]></description>
      <guid>https://stackoverflow.com/questions/78169647/pycaret-3-3-0-compare-models-show-zeros-for-all-models-auc</guid>
      <pubDate>Fri, 15 Mar 2024 21:02:29 GMT</pubDate>
    </item>
    <item>
      <title>从 dvc 中提取当前运行阶段</title>
      <link>https://stackoverflow.com/questions/78169479/extract-current-running-stage-from-dvc</link>
      <description><![CDATA[我正在使用“dvc repro -f”进行实验，其中根据 dvc.yaml 配置执行多个阶段。例如：
&lt;前&gt;&lt;代码&gt;阶段：
 训练：
   foreach：
    -周期：0
    -周期：1
    -周期：2
   做：
    命令：
     蟒蛇火车.py
 选择：
   foreach：
    -周期：0
    -周期：1
    -周期：2
   做：
    命令：
     蟒蛇火车.py

在每个阶段的执行过程中，例如第0个周期的训练阶段，我的目标是在Python程序中提取其阶段名称，例如“Training_0”，而在选择阶段，它应该是Selection_0。我正在寻找一种方法来在阶段正在执行时或在其执行开始之前提取此信息。我尝试使用 dvc.api，但 api 不返回当前正在运行的阶段。我怎样才能实现这个目标？]]></description>
      <guid>https://stackoverflow.com/questions/78169479/extract-current-running-stage-from-dvc</guid>
      <pubDate>Fri, 15 Mar 2024 20:20:24 GMT</pubDate>
    </item>
    <item>
      <title>想要建立一个作物推荐系统[关闭]</title>
      <link>https://stackoverflow.com/questions/78169349/want-to-build-a-crop-recommendation-system</link>
      <description><![CDATA[我有一个数据集，其中有 4 个柱状土壤类型、州、季节、作物。这里土壤类型、州、季节是字符串数据类型的特征，标签是农作物，也是字符串数据类型。我想建立一个推荐系统，以土壤类型、季节、州为输入，并根据输入推荐作物。有人可以帮助我吗我面临困难，因为它们都是字符串数据类型
这是我的数据集的样本
、州、土壤类型、季节、农作物
0,Andaman_and_Nicobar_Island,Latterite,rabi,“黑豆，小麦”
1、Andaman_and_Nicobar_Island、冲积土、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
2、Andaman_and_Nicobar_Island、Red、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
3、Andaman_and_Nicobar_Island、Black、Kharif、“Bajra、黑克、Moong、豇豆、高粱”
4、Andaman_and_Nicobar_Island、Latterite、Kharif、“乌拉德、玉米”
5、Andaman_and_Nicobar_Island，Black，Kharif，“乌拉德，玉米”
6、Andaman_and_Nicobar_Island、Latterite、Kharif、“Bajra、黑豆、Moong、豇豆、高粱”
7、Andaman_and_Nicobar_Island、Latterite、Kharif、“丁香、咖啡、椰子、槟榔”
8,Andaman_and_Nicobar_Island,Alluvial,Kharif,“丁香、咖啡、椰子、槟榔”
9、Andaman_and_Nicobar_Island，Red，Kharif，“丁香、咖啡、椰子、槟榔”
10、Andaman_and_Nicobar_Island、Latterite、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
11、Andaman_and_Nicobar_Island，粘土，拉比，“香蕉，姜黄”
12、Andaman_and_Nicobar_Island，冲积物，拉比，“香蕉，姜黄”
13、Andaman_and_Nicobar_Island，粘土，冬天，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
14、Andaman_and_Nicobar_Island，红土，冬天，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
15、Andaman_and_Nicobar_Island，冲积土，冬季，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
16、Andaman_and_Nicobar_Island，Black，rabi，“黑革，小麦”
]]></description>
      <guid>https://stackoverflow.com/questions/78169349/want-to-build-a-crop-recommendation-system</guid>
      <pubDate>Fri, 15 Mar 2024 19:46:02 GMT</pubDate>
    </item>
    <item>
      <title>制作基于fpga的au加速器[关闭]</title>
      <link>https://stackoverflow.com/questions/78169187/making-a-fpga-based-au-accelerator</link>
      <description><![CDATA[我有一个新项目，是基于 FPGA 重新创建人工智能加速器。
我找到了此类事物的实现示例，但它们仅支持定点量化类型。不同的是我想支持bf16格式，这样ai加速器也可以用来训练。
问题不在于这是否可能，我知道它是可能的，但我认为我可能对此抱有不切实际的期望。
根据我在网上可以找到的信息，单个 bf16 乘法器最多可以占用 75 个逻辑元件（不使用 dsp 片进行 7 位乘法），如果我选择脉动阵列类型的架构，每个处理元件可能需要200-500 个逻辑元件。我能买得起的 FPGA 类型有 25k 到 100k 逻辑元件，因此这种方法的性能会非常差。
理想情况下，我希望达到 100GFLOPs（每秒 1000 亿 bf16 mult+add）。这意味着我至少需要 1000 个 500MHz 的处理元件。
有人对如何执行此操作有一些指示或建议吗？
对于任何想知道的人，我知道 FPGA 设计和软件开发是非常乏味的任务，但我并不害怕承担它们。
我还没有开始这个项目，只是研究了它的可行性。]]></description>
      <guid>https://stackoverflow.com/questions/78169187/making-a-fpga-based-au-accelerator</guid>
      <pubDate>Fri, 15 Mar 2024 19:11:09 GMT</pubDate>
    </item>
    <item>
      <title>WSL 2 对于机器学习训练 (PyTorch) 的性能有多好？</title>
      <link>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</link>
      <description><![CDATA[由于 Linux 的硬件兼容性问题，我目前无法在 Windows 上工作。由于我使用的大多数软件都是本机 Linux CLI 工具，因此我正在考虑使用 WSL 2 构建一个开发环境。我的问题是：如果我选择在 WSL 2 中工作，预计性能会下降多少？
更具体地说：我主要需要为机器学习应用程序编写和运行 python 代码，因此我预计 CPU 和 Nvidia GPU 都会承受繁重的本地工作负载。因为我需要进行认真的神经网络训练，所以性能对我来说是一件大事。最初，由于 WSL 2 是虚拟化的一种形式，我放弃了它作为一种选择，就像我放弃了使用 GPU 直通来启动 VM 的想法一样；但后来我了解到 WSL 2 应该是“类型 1 管理程序”1，这应该意味着更好的性能。
所以我想我的完整问题是：神经网络训练 (PyTorch) 的 WSL 2 性能与裸机上的 Windows 训练相比如何？
&lt;小时/&gt;
[1]：我认为我完全理解什么是“类型 1 虚拟机管理程序”。意味着...]]></description>
      <guid>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</guid>
      <pubDate>Fri, 15 Mar 2024 19:02:26 GMT</pubDate>
    </item>
    <item>
      <title>Generative Ai API 获取响应时出错</title>
      <link>https://stackoverflow.com/questions/78167229/generative-ai-apis-error-in-getting-response</link>
      <description><![CDATA[我在 Generatve Ai 、HuggingFace api 密钥 google/flan-base 模型中生成响应时遇到错误
这是我的代码和最后一个单元格错误
导入操作系统
导入 json
将 pandas 导入为 pd
导入回溯
从 langchain 导入 PromptTemplate、HuggingFaceHub、LLMChain
从 langchain.chains 导入 SequentialChain
从 dotenv 导入 load_dotenv

加载_dotenv()
key = os.getenv(“hugging_face_key”)
os.environ[&#39;HUGGINGFACEHUB_API_TOKEN&#39;] = key
llm = HuggingFaceHub(repo_id=&#39;google/flan-t5-base&#39;, model_kwargs={&#39;温度&#39;: 0.5})

RESPONSE_JSON = {
    “1”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
    “2”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
    “3”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
}

模板=“”“
文本：{文本}
您是 MCQ 专家。鉴于上述文字，您的工作是\
以 {tone} 语气为 {subject} 学生创建一个包含 {number} 个多项选择题的测验，
确保问题不重复，并检查所有问题是否与文本相符。
确保按照下面的 RESPONSE_JSON 格式设置您的响应并将其用作指南。 \
确保进行 {number} 个 MCQ
### RESPONSE_JSON
{响应_json}
”“”

quiz_ Generation_prompt = 提示模板(
    input_variables=[&quot;文本&quot;,&quot;数字&quot;,&quot;主题&quot;,&quot;语气&quot;,&quot;re​​sponse_json&quot;],
    模板=模板
）
quiz_chain = LLMChain(llm=llm，prompt=quiz_ Generation_prompt，output_key=“quiz”，verbose=True)

模板2 =“”“
您是一位专业的英语语法学家和作家。为 {subject} 学生提供多项选择测验。\
您需要评估问题的复杂性并对测验进行完整的分析。出于复杂性考虑，最多只能使用 50 个单词
如果测验不符合学生的认知和分析能力，\
更新需要更改的测验问题并更改语气，使其完全适合学生的能力。
测验_MCQ：
{测验}

上述测验的专家英语作家检查：
”“”
    
quiz_evaluation_prompt = PromptTemplate(input_variables=[“主题”,“测验”], template=TEMPLATE2)
review_chain = LLMChain(llm=llm，prompt=quiz_evaluation_prompt，output_key=“审阅”，verbose=True)

generate_evaluate_chain = SequentialChain(chains=[quiz_chain, review_chain], input_variables=[&quot;text&quot;,&quot;number&quot;,&quot;subject&quot;,&quot;tone&quot;,&quot;re​​sponse_json&quot;],
                                          output_variables=[“测验”,“评论”], verbose=True)

# 假设您之前已经定义了“file_path”
以 open(file_path, &#39;r&#39;) 作为文件：
    文本 = 文件.read()

json.dumps(RESPONSE_JSON)

数量 = 5
主题=“机器学习”
TONE =“简单”

# 执行generate_evaluate_chain并将结果存储在&#39;response&#39;中
响应=generate_evaluate_chain.run（文本=文本，数字=数字，主题=主题，音调=音调，response_json=json.dumps（RESPONSE_JSON））



我最后一个单元格中的错误
当不存在一个输出键时，不支持`run`。
得到\[&#39;测验&#39;、&#39;评论&#39;\]。

尝试了不同的功能并做了一些研究，但没有用]]></description>
      <guid>https://stackoverflow.com/questions/78167229/generative-ai-apis-error-in-getting-response</guid>
      <pubDate>Fri, 15 Mar 2024 13:21:49 GMT</pubDate>
    </item>
    <item>
      <title>将 fit_resamples 与自定义分割数据一起使用？</title>
      <link>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</link>
      <description><![CDATA[我有一个自定义函数，可以根据各种标准和规则将数据分成训练集和测试集。我想在 tidymodels 工作流程中与 fit_resamples 一起使用此函数。但是，当我可以使我的列表看起来像用 vfold_cv 制作的列表时，它似乎不起作用。我正在使用的示例代码：
data(ames, package = “modeldata”)

split_data &lt;- 函数(df, n) {
  set.seed(123) # 为了重现性
  df$id &lt;- seq.int(nrow(df))
  list_of_splits &lt;- list()
  
  for(i in 1:n) {
    train_index &lt;- 样本(df$id, size=ceiling(nrow(df)*.8))
    train_set &lt;- df[train_index,]
    test_set &lt;- df[-train_index,]
    list_of_splits[[i]] &lt;- list(train_set = train_set, test_set = test_set)
  }
  
  返回（分割列表）
}

分割 &lt;- split_data(ames, 5)

重新采样 &lt;- map(splits, ~rsample::make_splits(
  x = .$train_set |&gt;选择(colnames(.$test_set)),
  评估=.$test_set
））

名称（重新采样）&lt;-paste0（“折叠”，seq_along（重新采样））

重新采样 &lt;- tibble::tibble(splits = 重新采样,
                            id = 名称（重新采样））

lm_model &lt;-
  Linear_reg() %&gt;%
  set_engine(“lm”)

lm_wflow &lt;-
  工作流程() %&gt;%
  add_model(lm_model) %&gt;%
  add_formula(Sale_Price ~ 经度 + 纬度)

res &lt;- lm_wflow %&gt;%
  fit_resamples（重新采样=重新采样）

运行最后一行后返回的错误是：
`check_rset()` 中出现错误：
！ “resamples”参数应该是一个“rset”对象，例如由“vfold_cv()”或其他“rsample”函数生成的类型。

如果我尝试强制该类“rset” class(resamples) &lt;- “rset”，列表看起来不再正确，我得到了相同的错误。
使用自定义交叉折叠数据集的正确方法是什么？
注意 - 附加问题：在上面的示例代码中，测试集和训练集的大小在折叠中是一致的。在我的实际数据中，这会略有不同 - 这有关系吗？]]></description>
      <guid>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</guid>
      <pubDate>Fri, 15 Mar 2024 13:11:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在Android Studio中实现实时tflite模型？</title>
      <link>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</link>
      <description><![CDATA[我正在尝试在移动应用程序上进行实时模型实现。我在 Teachable Machine 中训练了模型并将其导出为 model_unquanted.tflite。当我将其导入 Android Studio 时，它会提供以下 Kotlin 代码来实现它：
val model = ModelUnquant.newInstance(context)

// 创建输入以供参考。
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

// 运行模型推理并获取结果。
val 输出 = model.process(inputFeature0)
valoutputFeature0=outputs.outputFeature0AsTensorBuffer

// 如果不再使用则释放模型资源。
模型.close()

以下是我对实时更新的实现：
 覆盖 fun onSurfaceTextureUpdated(p0: SurfaceTexture) {
            位图=textureView.bitmap！！
            val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)

            val byteBuffer: ByteBuffer = ByteBuffer.allocate(224* 224* 3)
            byteBuffer.rewind()

            inputFeature0.loadBuffer(byteBuffer)

            val 输出 = model.process(inputFeature0)
            valoutputFeature0=outputs.outputFeature0AsTensorBuffer

            var 可变 = bitmap.copy(Bitmap.Config.ARGB_8888, true)
            val canvas = android.graphics.Canvas(可变)

            val h = mutable.height
            val w = 可变的.width

            val xPosition = 10 // 根据需要调整该值
            val yPosition = 30 // 根据需要调整该值

            canvas.drawText（outputFeature0.toString（），xPosition.toFloat（），yPosition.toFloat（），绘画）

            imageView.setImageBitmap(可变)

        }

logcat 错误：
致命异常：main
进程：com.example.tflite_realtime，PID：14671
java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。
]]></description>
      <guid>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</guid>
      <pubDate>Fri, 15 Mar 2024 08:22:05 GMT</pubDate>
    </item>
    <item>
      <title>加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</link>
      <description><![CDATA[当我在终端中运行它时，如下所示，出现此错误
nizhar@nizhar-desktop:~/Documents$ g++ -std=c++11 tespredict.cpp -o Predictx -I/usr/local/include/onnxruntime/include -L/usr/local/lib -lonnx运行时
nizhar@nizhar-desktop:~/文档$ ./predictx
./predictx：加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录


我的 onnxrune-time 库路径位于
&lt;前&gt;&lt;代码&gt;/usr/local/lib
/usr/local/include/onnxruntime

我还在.bashrc中添加了
export ONNXRUNTIME_DIR=“/usr/local/include/onnxruntime/include”
导出 LD_LIBRARY_PATH=“/usr/local/lib:$LD_LIBRARY_PATH”

并且已经这样做了
源 ~/.bashrc

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</guid>
      <pubDate>Fri, 15 Mar 2024 08:04:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 提高蛋白质序列 SNP 分类的准确性</title>
      <link>https://stackoverflow.com/questions/78164309/improve-accuracy-on-protein-sequence-snp-classification-using-cnns</link>
      <description><![CDATA[我正在尝试使用卷积神经网络 (CNN) 对蛋白质序列单核苷酸多态性 (SNP) 进行分类。我的数据集由代表这些 SNP 序列的小波相干变换尺度图组成。然而，尽管尝试了各种 CNN 架构并利用了迁移学习技术，我仍然无法在训练集和验证集上实现超过 62% 的准确率。数据集中的一些图像
如何处理图像高度相似的数据集？]]></description>
      <guid>https://stackoverflow.com/questions/78164309/improve-accuracy-on-protein-sequence-snp-classification-using-cnns</guid>
      <pubDate>Fri, 15 Mar 2024 02:01:00 GMT</pubDate>
    </item>
    <item>
      <title>比较两个名字的相似度并使用神经网络识别重复项</title>
      <link>https://stackoverflow.com/questions/72914328/compare-similarity-of-two-names-and-identify-duplicates-with-neural-network</link>
      <description><![CDATA[我有一个包含成对名称的数据集，它看起来像这样：
&lt;前&gt;&lt;代码&gt;ID；姓名1；姓名2
1;迈克·米勒；迈克·米勒
2；约翰·多伊；皮特·麦吉伦
3；萨拉·约翰逊；伊迪塔·约翰逊
4；约翰·莱蒙德-李·彼得；约翰·LL.彼得
5；玛塔·桑兹；玛莎桑德
6；约翰·彼得；约翰娜·彼得拉
7；乔安娜·内姆齐克；乔安娜·尼姆齐克

我有一些案例，已贴上标签。所以我手动检查它们并确定它们是否重复。这些情况下的手动判断将是：
1：是重复的
2：不重复
3：不重复
4：是重复的
5：不重复
6：不重复
7：是重复的

（第七个案例是一个具体案例，因为这里语音也参与了游戏。但这不是主要问题，我可以忽略语音。）
第一种方法是计算每对的编辑距离并将其标记为重复项，其中编辑距离例如小于或等于 2。这将导致以下输出：
1：编辑距离：2 =&gt;复制
2：编辑距离：11 =&gt;不是重复的
3：编辑距离：4 =&gt;不是重复的
4：编辑距离：8 =&gt;不是重复的
5：编辑距离：2 =&gt;复制
6：编辑距离：4 =&gt;不是重复的
7：编辑距离：2 =&gt;复制

这将是一种使用“固定”的方法。基于Levinshtein距离的算法。
现在，我想使用神经网络/机器学习来完成此任务：
我不需要神经网络来检测语义相似性，例如“医院”和“临床”。然而，我想避免 Levenshtein 距离，因为我希望 ML 算法能够检测“John Lemond-Lee Peter”。和“约翰·LL。彼得”作为潜在的重复，也不是 100% 确定。在这种情况下，编辑距离会导致相对较高的数字 (8)，因为需要添加相当多的字符。在像“约翰·彼得”这样的情况下，和“约翰娜彼得拉”编辑距离会导致较小的数字 (4)，但这实际上不是重复的，对于这种情况，我希望 ML 算法能够检测到这可能不是重复的。所以我需要 ML 算法来“学习我需要检查重复项的方式”。通过我的标签，我将作为输入给出 ML 算法我想要的方向。
我实际上认为这对于机器学习算法/神经网络来说应该是一个简单的任务，但我不确定。
如何实现神经网络来比较名称对并识别重复项，而不使用显式距离度量（例如编辑距离、欧几里德距离等）？
我认为可以将字符串转换为数字，并且神经网络可以处理它并学习根据我的标签风格检测重复项。因此不必指定距离度量。 我想到了一个人：我会把这个任务交给一个人，这个人会判断并做出决定。此人对编辑距离或任何其他数学概念一无所知。所以我只是想训练神经网络学会做人类正在做的事情。当然，每个人都是不同的，这也取决于我的标签。
（编辑：到目前为止我见过的机器学习/神经网络解决方案（例如this) 使用像 levenshtein 这样的度量作为特征输入。但正如我所说，我认为应该可以教会神经网络“学习”。 “人类判断”而不使用这样的距离度量？关于我的具有名称对的具体情况：使用编辑距离作为特征的 ML 方法有什么好处？因为它只会将这些名称对检测为具有低编辑距离的重复。因此，如果两个名称之间的编辑距离小于 x，我可以使用一个简单的算法将一对标记为重复。为什么要使用 ML，额外的好处是什么？）]]></description>
      <guid>https://stackoverflow.com/questions/72914328/compare-similarity-of-two-names-and-identify-duplicates-with-neural-network</guid>
      <pubDate>Fri, 08 Jul 2022 16:25:04 GMT</pubDate>
    </item>
    </channel>
</rss>