<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Fri, 07 Feb 2025 09:17:49 GMT</lastBuildDate>
    <item>
      <title>在拥有IterabledataSet和DataLoader的同时，如何计算培训和验证的准确性和损失？</title>
      <link>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</link>
      <description><![CDATA[我正在定义自己的train（）函数 - 进行培训和验证（也许该功能的名称不是最描述性的）
由于我正在使用一个自定义估计数据集以及数据加载程序，因此我正在尝试弄清楚如何计算这些指标。 Itable数据集没有长度（），所以我无法真正弄清楚其他出路。
到目前为止，这是我的代码：
  def train（模型，训练dataloader，验证dataloader，优化器，标准，设备，时代= 10）：
    ＃为时代创建外部进度栏
    epoch_pbar = tqdm（range（epochs），desc =&#39;训练进度&#39;，位置= 0）
    
    对于epoch_pbar中的epoch：
        ＃训练阶段
        model.train（）
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        
        ＃创建进度栏进行培训批次
        train_pbar = tqdm（triendingdataloader， 
                         desc = f&#39;training epoch {epoch+1}/{epochs}&#39;，，
                         位置= 1， 
                         离开= false）
        
        ＃每批
        对于set_features，access_features，cache_features，train_pbar中的标签：
            ＃结合功能
            cache_features_flat = cache_features.reshape（-1，17*9）
            combined_features = torch.cat（[set_features，access_features，cache_features_flat]，dim = 1）
            
            ＃移至设备
            combined_features = combined_features.to（设备）
            标签= labels.to（设备）
            
            ＃零梯度
            优化器.zero_grad（）
            
            ＃正向通行证
            输出=型号（combined_features）
            损失=标准（输出，标签）
            
            ＃向后传球
            loss.backward（）
            优化器.step（）
            
            ＃培训统计
            train_loss += lose.item（）
            _，预测= torch.max（输出，1）
            train_total += labels.size（0）
            train_correct +=（预测== torch.max（labels，1）[1]）。sum（）。item（）。
            
            ＃更新培训进度栏，目前损失和准确性
            train_pbar.set_postfix（{
                &#39;损失&#39;：f&#39;{train_loss/train_total：.4f}&#39;，
                &#39;acc&#39;：f&#39;{100 * train_correct/train_total：.2f}％&#39;
            }））
        
        ＃验证阶段
        model.eval（）
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        ＃创建验证批处理的进度栏
        val_pbar = tqdm（验证dataloader， 
                       desc = f&#39;validation epoch {epoch+1}/{epochs}&#39;，
                       位置= 1， 
                       离开= false）
        
        使用Torch.no_grad（）：
            对于set_features，access_features，cache_features，val_pbar中的标签：
                ＃结合功能
                cache_features_flat = cache_features.reshape（-1，17*9）
                combined_features = torch.cat（[set_features，access_features，cache_features_flat]，dim = 1）
                
                ＃移至设备
                combined_features = combined_features.to（设备）
                标签= labels.to（设备）
                
                ＃正向通行证
                输出=型号（combined_features）
                损失=标准（输出，标签）
                
                ＃验证统计
                val_loss += lose.item（）
                _，预测= torch.max（输出，1）
                val_total += labels.size（0）
                val_correct +=（预测== torch.max（labels，1）[1]）。sum（）。item（）。
                
                ＃更新验证进度栏具有当前损失和准确性
                val_pbar.set_postfix（{
                    &#39;损失&#39;：f&#39;{val_loss/val_total：.4f}&#39;，
                    &#39;acc&#39;：f&#39;{100 * val_correct/val_total：.2f}％&#39;
                }））
        
        ＃最终指标更新时代进度栏
        epoch_pbar.set_postfix（{
            &#39;train_loss&#39;：f&#39;{train_loss/（triendingdataloader .__ len __（））：。4f}&#39;，
            &#39;train_acc&#39;：f&#39;{100 * train_correct/train_total：.2f}％&#39;，
            &#39;val_loss&#39;：f&#39;{val_loss/（验证dataloader .__ len __（））：。4f}&#39;，，
            &#39;val_acc&#39;：f&#39;{100 * val_correct/val_total：.2f}％&#39;
        }））
        
        ＃打印时代的最终统计数据
        print（f&#39;\ nepoch {epoch+1}/{epochs}：&#39;）
        打印（training损失：{train_loss/（triendingdataloader .__ len __（））：。4f}，&#39;
              F&#39;Training精度：{100 * train_correct/train_total：.2f}％&#39;）
        打印（f&#39;validation损失：{val_loss/（versedationdataloader .__ len __（））：。4f}，&#39;
              f&#39;Validation精度：{100 * val_correct/val_total：.2f}％\ n&#39;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</guid>
      <pubDate>Fri, 07 Feb 2025 00:48:54 GMT</pubDate>
    </item>
    <item>
      <title>RL代理学习卡住</title>
      <link>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</link>
      <description><![CDATA[我试图熟悉MATLAB的强化学习库。我正在努力创建一个可以将正弦功能学习为简单热身的代理，但我已经卡住了。问题在于，经过几次迭代后，代理人达到一定水平，然后撞到天花板，无论我让学习过程运行多长时间，都不会进一步学习。这是代码：
  obs_info = rlnumericspec（[1 1]，lowerlimit = -pi，upperlimit = pi）;
obs_info.name =＆quot“鼻窦价值＆quot”;

act_info = rlnumericspec（[1 1]，lowerlimit = -1，upperlimit = 1）;
act_info.name =“预测价值”;

reset_fcn_handle = @（）reset_train（）;
step_fcn_handle = @（action，portfolio）step_train（...
    行动，投资组合）；

sinus_train_env = rlfunctionenv（...
    obs_info，act_info，step_fcn_handle，reset_fcn_handle）;

函数[onirent_observation，portfolio] = reset_train（）
    initial_observation = 2*pi*rand（1）-pi;
    portfolio = struct;
    portfolio.lastvalue = initial_observation;
结尾

函数[next_observation，Reward，is_done，portfolioout] = step_train（...
    动作，投资组合）
    Expect_prediction = sin（portfolio.lastvalue）;
    奖励= 1 /100 /（0.01 + abs（Action -endured_prediction））;
    next_observation = 2*pi*rand（1）-pi;
    portfolioout = portfolio;
    portfolioout.lastvalue = next_observation;
    is_done = false;
结尾
 
我使用加固学习设计师来构建代理。 “兼容算法”已设置为TD3（默认选项），并且隐藏单元的数量为32。超参数和探索模型设置：
  
对于训练，最大剧集长度= 1000，平均窗口长度= 5，停止条件=平均值，停止值= 900。
30分钟后的结果：
  
1小时后的结果：
   
我试图修改奖励功能并让其运行两个小时：
 奖励= 1 /（0.01 + abs（Action -Expection_prediction））;
 
结果：
  
第二次尝试修改奖励功能：
  if（abs（Action-expected_prediction）＆gt; 0.05）
    奖励= -1;
别的 
    奖励= 1;
结尾
 
结果：
   
我缺少什么？]]></description>
      <guid>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</guid>
      <pubDate>Thu, 06 Feb 2025 19:03:32 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8最终检测头仍输出（1、7、8400），而不是（1、8、8400）</title>
      <link>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</link>
      <description><![CDATA[我训练了3个类的Yolov8检测模型，但是原始的正向通行证仍然显示（1、7、8400）而不是（1、8、8400）的最终检测输出。
我做了什么：
检查了我的data.yaml：
  yaml
火车：路径/到/火车/图像
Val：路径/到/Val/图像
NC：3
名称：[&#39;神经胶质瘤&#39;，&#39;脑膜瘤&#39;，&#39;垂体&#39;]
 
确认的NC：3是正确的。
通过命令从头开始训练：
  bash
Yolo检测火车\
    data =路径/到/data.yaml \
    模型= yolov8x \
    epochs = 1000 \
    imgsz = 640 \
    设备= 1 \
    耐心= 100
 
训练没有错误地进行，并成功完成。
安装了最新的Ultrytics版本（v8.3.72），以确保没有版本问题：
  bash
PIP卸载超级分析
PIP安装超级词
 
直接加载了新的best.pt：
  python
从超级物质进口YOLO
导入火炬

型号= yolo（r＆quot; best.pt;）。模型
model.eval（）

dummy_input = torch.randn（1，3，640，640）
使用Torch.no_grad（）：
    输出=模型（Dummy_input）

输出输出：
    ＃一些输出是列表；仔细检查每个元素
    如果Isinstance（Out，Torch.Tensor）：
        打印（OUT.SHAPE）
    别的：
        打印（“列表输出：＆quot” [o。
 
控制台显示检测输出（1、7、8400）。
经过验证的模型元数据说NC = 3和Model.Names有3个类。但是，原始检测层输出仍然是7个通道。
观察：
如果Yolo检测层是真正的3类，则应输出（5 + 3）=每个锚点8个通道，而不是7通道。
不匹配（1、7、8400）通常表明尽管NC = 3。
问题 /请求帮助：
为什么即使我从头开始训练了3堂课，为什么还要原始检测头（1、7、8400）？
如何确保将检测层完全重新定位为（5 + 3）= 8以进行3级检测？
我已经尝试删除旧的.pt文件，重新检查我的数据。yaml，重新安装超级图像和确认模型。model.nc== 3。但是最终检测层继续产生7个频道，而不是8个频道。
关于可能导致这种持续不匹配的什么想法？
事先感谢您的任何帮助或见解！]]></description>
      <guid>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</guid>
      <pubDate>Thu, 06 Feb 2025 18:39:21 GMT</pubDate>
    </item>
    <item>
      <title>哪种AI检测工具为识别AI生成的内容提供了最准确的结果？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</link>
      <description><![CDATA[我想确保我收到或写的内容是人类生成的。有各种AI检测工具可用，但我不确定哪种是最准确和最可靠的。某些文章可能部分是AI生成的，因此很难进行检测。用于此目的的最佳工具是什么？＆quort 
我尝试过的＆amp;预期结果：
“我尝试过诸如gptzero和simpality.ai之类的工具，但我正在寻找一种更准确或广泛接受的解决方案。理想情况下，我需要一个可以以高精度检测AI编写的内容并提供可靠见解的工具。
我已经测试了诸如gptzero，siginality.ai和copyleaks AI检测器等工具。尽管他们提供了一些见解，但我注意到其结果不一致。正确标记了一些AI生成的内容，而在其他情况下，人写的文本被错误地确定为AI生成的。我期望一种工具，可以提供更高准确性，清晰的检测过程解释，并有效地分析简短和长格式的能力。 ]]></description>
      <guid>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</guid>
      <pubDate>Thu, 06 Feb 2025 17:34:58 GMT</pubDate>
    </item>
    <item>
      <title>CNN中二进制分类任务中F-1分数的不同结果</title>
      <link>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</link>
      <description><![CDATA[我正在制作用于二进制分类任务的CNN模型。

当我使用binary_crossentropy作为损失功能并将1个神经元保持在最后一层时，我的准确性约为94％，val_accuracy的85％左右，但我的F-1得分被卡在69％左右。
当我使用cancorical_crossentropy作为损失函数时，结果有些相似，但是此时间F-1分数约为85％。

  model =顺序（[[
    输入（shape =（*input_shape，1）），

    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchnormitization（），

    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchnormitization（），
    
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchnormitization（），

    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchnormitization（），
    
    conv2d（256，（3，3），激活=&#39;relu&#39;，padding =＆quort; same; kernel_regularizer = l2（0.001）），
    conv2d（256，（3，3），激活=&#39;relu&#39;，padding =＆quort; same; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchnormitization（），

    flatten（），
    密集（512，激活=&#39;relu&#39;），
    辍学（0.5），
    密集（256，激活=&#39;relu&#39;），
    辍学（0.2），
    致密（2，激活=&#39;softmax&#39;）
）））
 
任何人都可以告诉我为什么会发生这种情况，以及解决方案是什么。
我也想知道准确性和val_accuracy差距的原因，即使班级平衡。 
我尝试更改模型结构和损失功能，但没有解决。]]></description>
      <guid>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</guid>
      <pubDate>Thu, 06 Feb 2025 15:24:42 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow图像分类过拟合问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</link>
      <description><![CDATA[我正在尝试使用TensorFlow Keras最新API和功能创建图像分类模型。我的模型通过查看复杂的功能和设计（例如微打印，全息图，透明图案等）将货币注释分类为真正的和假货币笔记。我总共有一个较小的高质量图像数据集。无论我做什么，我的模型都过高。它的训练准确性高达1.000，训练损失高达0.012。但是验证精度仍保持在0.60-0.75中，验证损失保持在0.40-0.53。的范围内。
我尝试了以下内容：

增加数据集。 （但是我知道这对货币笔记没有太大差异并没有太大的帮助。它们都非常相同。因此，它无助于概括模型）
使用辍学，L1/L2正则化
使用转移学习。我使用了Resnet50型号。我首先通过冷冻基本模型来训练了几个时期，然后再冻结模型并重新训练以进行更多时代。
使用类重量来平衡权重。
使用计划学习率在进行培训时修改。
使用早期打电话等。
尝试使用预处理

此外，如果我在其中使用归一化层，并且没有它，则我的模型会更糟。所以我排除了那一层。
但是，没有任何帮助我改善概括。我不知道我缺少什么。
我的模型：
 
data_augmentation = tf.keras.Sequeential（[[
    tf.keras.layers.randomrotation（0.1），，
    tf.keras.layers.randomzoom（0.1），
    tf.keras.layers.randombrightness（0.1），
    tf.keras.layers.randomcontrast（0.1），
）））


train_ds = tf.keras.utils.image_dataset_from_directory（
  data_dir，
  验证_split = 0.2，
  子集=“训练”
  种子= 123，
  image_size =（img_height，img_width），
  batch_size = batch_size）

val_ds = tf.keras.utils.image_dataset_from_directory（
  data_dir，
  验证_split = 0.2，
  子集=“验证”
  种子= 123，
  image_size =（img_height，img_width），
  batch_size = batch_size）

 
train_ds =（（
    train_ds
    .map（lambda x，y ：（ data_augmentation（x，triaght = true），y），num_parallel_calls = autotune）
    。缓存（）
    .shuffle（1000）
    .prefetch（buffer_size = autotune）
）


base_model = tf.keras.applications.Resnet50（

    input_shape =（img_height，img_width，3），
    include_top = false，
    重量=&#39;Imagenet&#39;
）



＃未冻结特定层用于微调


base_model.trainable = true
对于base_model.layers [： -  10]：＃保持第一层冻结
    layer.trainable = false

l2_lambda = 0.0001

＃模型
模型= tf.keras.Sequeential（[[
    base_model，
    tf.keras.layers.globalaveragepooling2d（），
tf.keras.layers.dens
（512，activation =&#39;relu&#39;，kernel_regularizer =正元器.l2（l2_lambda）），），
    tf.keras.layers.dropout（0.5），
    tf.keras.layers.dense（256，activation =&#39;relu&#39;），
    tf.keras.layers.dropout（0.4），
    tf.keras.layers.dense（1，激活=“ Sigmoid＆quort”）
）））
 ]]></description>
      <guid>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</guid>
      <pubDate>Thu, 06 Feb 2025 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>NameError：名称'CV2'在尝试运行DEF __INIT __（self，width，height，Inter = cv2.inter_area）时未定义。</title>
      <link>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</link>
      <description><![CDATA[我试图用CV2晒太阳并获得错误
  def __init __（自我，宽度，高度，inter = cv2.inter_area）：
                                            ^^^
名称：名称“ CV2”未定义

knn.py-dataset ./datasets/animals
Trackback（最近的最新电话）：
  file＆quot＆quort＆quot＆test/desktop/code/knn.py&quot;，第6行，in＆lt; module＆gt;
    来自Pyimagesearch.preprocestiond import SimplePreprocessor
  file＆quot＆quot＆quot test/desktop/code/pyimagesearch/preprocessing/simplepreprocessor.py＆quot;，4，第4行，in＆lt; module＆gt;
    类简单的Preprocessor：
  file＆quot＆quot＆quot test/desktop/code/pyimagesearch/preprocessing/simplepreprocessor.py&quot;，第5行，在SimplePreProcessor中
    def __init __（自我，宽度，高度，inter = cv2.inter_area）：
                                            ^^^
名称：未定义的名称“ CV2”
 ]]></description>
      <guid>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</guid>
      <pubDate>Thu, 06 Feb 2025 08:53:50 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的轨迹的一部分[封闭]</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个代表手机运动轨迹的数据集，并且该轨迹由步行行驶的部分和汽车旅行的轨迹组成。数据包括经度，纬度，时间戳和3轴加速度计数据。我需要提取步行旅行的所有子接头。这个问题有现成的解决方案吗？如果没有，我该如何处理此任务？
我试图在互联网上找到现成的解决方案，但没有找到任何值得的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>ML代理商不收集简单的弹丸到目标任务的统一任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在目标下发射弹丸。代理只有一个动作 - 射击的角度。发射力是恒定的。我也没有改变目标的位置。因此，这应该是微不足道的，因为该模型只需要学习正确的射击角度即可。但是经过300000个训练步骤，该模型仍在不稳定地拍摄。
代理：
 使用unity.mlagents;
使用unity.mlagents.actuators;
使用unity.mlagents.sensors;
使用UnityEngine；

公共班级Projectileagent：代理商
{
    公共转型目标；             //固定目标与2D对撞机＆amp; “目标”标签
    公共变换发射点；        //弹丸的产生的位置
    公共GameObject ProjectilePrefab;  //预先使用刚性2d＆amp; ProjectIlecollision脚本
    公共Float filexforce = 500F;      //恒定的力施加到弹丸上

    私人bool haslaunched = false;

    公共覆盖void onepisodebegin（）
    {
        haslaunched = false;
        requestDecision（）; //在每个情节开始时要求一个决定
    }

    公共覆盖void collectobsertations（vectorsensor传感器）
    {
        //观察从发射点到目标的相对位置（x，y）
        vector2 diff = target.position-启动点。位置;
        Sensor.Addobservation（diff.x）;
        Sensor.Addobservation（diff.y）;
    }

    公共覆盖空白onactionReceived（ActionBuffers Action）
    {
        如果（！已启动）
        {
            //一个连续动作（0..1）映射到[0..180]度
            float Angle01 = Mathf.Clamp01（Action.Contuunactions [0]）;
            float angleDegrees = mathf.lerp（0f，180f，angle01）;

            启动项目（AngleDegrees）;
            haslaunched = true;
        }
    }

    私人空白发射项目（浮动式Angledegrees）
    {
        gameObject projobj = instantiate（projectileprefab，launchpoint.position.position，quaternion.Identity）;
        ProjectIlecollision projscript = proJobj.getComponent＆lt; projectileCollision＆gt;（）;
        projscript.agent = this;

        arigidbody2d rb = projobj.getComponent＆lt; strigbody2d＆gt;（）;
        float rad = angleDegrees * mathf.deg2rad;
        vector2方向= new vector2（mathf.cos（rad），mathf.sin（rad））;
        rb.addforce（方向 *固定图）;
    }

    //弹丸呼叫以击中目标
    公共void onhittarget（）
    {
        addreward（1.0f）;
        entepisode（）;
    }

    //弹丸拨打的丢失目标
    公共空白（Vector2 ProjectilePosition）
    {
        float距离= vector2.distance（projectileposition，target.position）;
        float maxDistance = 10f; //根据需要进行调整
        float接近= 1F-（距离 / maxDistance）;
        接近= Mathf.Clamp01（接近）;

        //部分奖励接近
        addreward（接近 * 0.5F）;

        //小姐的小额罚款
        addreward（-0.1f）;
        entepisode（）;
    }

    //统一编辑器测试的启发式（随机角度）
    公众超越空白启发式（在ActionBuffers Action out）
    {
        ActionSout.Contulactions [0] = Random.Value;
    }
}
 
弹丸：
 使用UnityEngine；

公共课程项目计算：Monobehaviour
{
    公共Projectileagent代理；

    私人void start（）
    {
        //短时间破坏，以便我们可以注册一次
        销毁（gameObject，终生）；
    }

    私有void onCollisionEnter2d（Collision2D Collision）
    {
        if（collision.gameobject.comparetag（“ target”））
        {
            Agent.Onhittarget（）;
        }
        别的
        {
            Agent.Onmiss（transform.position）;
        }
        销毁（gameObject）;
    }
}
 
    
我尝试的

奖励成型：
+1击中目标，基于部分距离的奖励，而缺失的小负数。
我将热门奖励提高到+3，减少了罚款，等等。
培训步骤：
我使用ppo跑了300k+步骤。
碰撞检查：
日志在预期时间确认onhittarget（）和onmiss（）火。
固定力＆amp;重力：
已验证箭头可以通过硬编码角度手动到达目标。
重力是设置的，以便在物理上可以击中。
没有随机目标：
目前，目标已固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>在Tensorflow中开发用于图像分类的预处理模型</title>
      <link>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</link>
      <description><![CDATA[我有一个关于如何修改验证模型以对3类分类而不是1000的问题。这是我到目前为止提出的2种方法。.im不确定哪个是最好的。
  nasnetmobile_model = tf.keras.applications.nasnetmobile（
                                                input_shape =（224,224,3），
                                                include_top = false，
                                                池=&#39;avg&#39;，
                                                类= 3，
                                                重量=&#39;Imagenet&#39;
                                                ）
nasnetmobile_model.trainable = false
nasnetmobile_model.summary（）键入此处
 
 in方法1，Nasnetmobile模型以预先训练的成像网权重量初始化，不包括顶层和使用平均池。该模型设置为不可训练，以防止其权重在训练期间进行更新。然后构建了一个新的顺序模型，其中包括预先训练的NASNETMOBILE模型，然后是两个密集的层：一个具有128个单元和relu激活的层，另一个具有3个单元和SoftMax激活，用于最终分类。顺序模型与ADAM优化器和稀疏的分类横向渗透丢失一起编译。最后，该模型在数据集上进行了20个时期的培训，其批次大小为4，验证分配为20％。
方法1 
  new_pretratained_model = tf.keras.sequential（）

new_pretraining_model.add（nasnetmobile_model）
new_pretrained_model.add（tf.keras.layers.dense（128，activation =&#39;relu&#39;））
new_pretrained_model.add（tf.keras.layers.dense（3，activation =&#39;softmax&#39;））

new_pretained_model.layers [0] .trainable = false
new_pretained_model.summary（）此处


new_pretratained_model.compile（
            优化器=&#39;Adam&#39;，
            损失=&#39;Sparse_categorical_crossentropy&#39;，
            指标= [&#39;准确性&#39;]
            ）

new_pretratained_model.fit（
        Xtrain，
        Ytrain，
        时代= 20，
        batch_size = 4，
        验证_split = 0.2
        ）
 
方法2 
在方法2中，功能API用于创建新模型。预先训练的NASNETMOBILE模型的输出被视为具有128个单位和relu激活的新密集层的输入，然后是具有3个单元和SoftMax激活的最终致密层。这种方法明确将NasnetMobile模型的输入连接到新的输出层，形成了一种与原始NasnetMobile模型相同的输入的新模型，但具有额外的密集层进行分类。然后将新模型与Adam Optimizer和稀疏的分类横向渗透损失一起编译，并在数据集上进行20个时期的培训，其批量大小为4，验证分配为20％。。
  nasnetmobile_model_out = nasnetmobile_model.output
x = tf.keras.layers.dense（128，activation =&#39;relu&#39;）（nasnetmobile_model_out）
输出= tf.keras.layers.dense（3，激活=&#39;softmax&#39;）（x）
model_2 = tf.keras.model（inputs = nasnetmobile_model.input，outputs =输出）

model_2.summary（）


model_2.compile（
            优化器=&#39;Adam&#39;，
            损失=&#39;Sparse_categorical_crossentropy&#39;，
            指标= [&#39;准确性&#39;]
            ）

model_2.fit（fit（
        Xtrain，
        Ytrain，
        时代= 20，
        batch_size = 4，
        验证_split = 0.2
        ）
 ]]></description>
      <guid>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</guid>
      <pubDate>Mon, 27 May 2024 16:22:12 GMT</pubDate>
    </item>
    <item>
      <title>ALS的Pyspark实施如何处理每个用户项目组合的多个评级？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到，到ALS的输入数据不需要每个用户项目组合的唯一评分。
这是一个可再现的例子。
 ＃示例数据框架
df = spark.createDataframe（[（0，0，4.0），（0，1，2.0）， 
（1，1，3.0），（1，2，4.0）， 
（2，1，1.0），（2，2，5.0）]，[&#39;用户;

DF.Show（50,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 1 | 2.0 |
| 1 | 1 | 3.0 |
| 1 | 2 | 4.0 |
| 2 | 1 | 1.0 |
| 2 | 2 | 5.0 |
+-----+----+-------+
 
您可以看到，每个用户项目组合只有一个评分（理想的情况）。
如果我们将此数据框传递到ALS，它将为您提供如下的预测：
 ＃拟合ALS
来自pyspark.ml.Recmendation Import ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.9169915 |
| 0 | 1 | 2.031506 |
| 0 | 2 | 2.3546133 |
| 1 | 0 | 4.9588947 |
| 1 | 1 | 2.8347554 |
| 1 | 2 | 4.003007 |
| 2 | 0 | 0.9958025 |
| 2 | 1 | 1.0896711 |
| 2 | 2 | 4.895194 |
+----+----+------------+
 
到目前为止，一切都有意义。但是，如果我们有一个包含多个用户项目评级组合的数据框，如以下 -  
 ＃示例daataframe
df = spark.createDataFrame（[（（0，0，4.0），（0，0，3.5），
                            （0，0，4.1），（0，1，2.0），
                            （0，1，1.9），（0，1，2.1），
                            （1，1，3.0），（1，1，2.8），
                            （1，2，4.0），（1，2，3.6），
                            （2，1，1.0），（2，1，0.9），
                            （2，2，5.0），（2，2，4.9）]，
                           [用户“
DF.Show（100,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 0 | 3.5 |
| 0 | 0 | 4.1 |
| 0 | 1 | 2.0 |
| 0 | 1 | 1.9 |
| 0 | 1 | 2.1 |
| 1 | 1 | 3.0 |
| 1 | 1 | 2.8 |
| 1 | 2 | 4.0 |
| 1 | 2 | 3.6 |
| 2 | 1 | 1.0 |
| 2 | 1 | 0.9 |
| 2 | 2 | 5.0 |
| 2 | 2 | 4.9 |
+-----+----+-------+
 
您可以在上述数据框架中看到，有一个用户项目组合的多个记录。例如 - 用户&#39;0&#39;对项目&#39;0&#39;分别额定为4.0,3.5和4.1。。
如果我将此输入数据范围传递给ALS怎么办？这个可以吗？
我最初认为它不应该工作，因为ALS应该每个用户项目组合获得唯一的评分，但是当我运行此功能时，它可以奏效并使我感到惊讶！&gt; 
 ＃拟合ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.7877638 |
| 0 | 1 | 2.020348 |
| 0 | 2 | 2.4364853 |
| 1 | 0 | 4.9624424 |
| 1 | 1 | 2.7311888 |
| 1 | 2 | 3.8018093 |
| 2 | 0 | 1.2490809 |
| 2 | 1 | 1.0351425 |
| 2 | 2 | 4.8451777 |
+----+----+------------+
 
为什么起作用？我认为它会失败，但也没有给我预测。
我尝试查看研究论文，有限的ALS源代码以及Internet上的可用信息，但找不到任何有用的东西。
是否平均需要这些不同的评分，然后将其传递给ALS或其他任何东西？
以前有人遇到过类似的事情吗？还是关于ALS如何在内部处理此类数据的任何想法？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度评分</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深层神经网络模型（以 keras 实现）来进行预测。这样的东西：
  def make_model（）：
 型号=顺序（）       
 model.Add（conv2d（20，（5,5），激活=; relu; quot;））
 model.Add（maxpooling2d（pool_size =（2,2）））    
 模型add（Flatten（））
 型号add（密集（20，激活=; relu; quot;））
 model.Add（lambda（lambda x：tf.expand_dims（x，axis = 1））））））
 model.Add（Simplernn（50，Activation =; relu; quot;））
 model.Add（密集（1，激活=; softmax;））    
 model.compile（lose =; cancorical_crossentropy＆quot＆quot＆quort＆quotizer = adagrad，zerrics = [＆quciet; cocucet＆quot;]）

 返回模型

型号= make_model（）
model.fit（x_train，y_train，validation_data =（x_validation，y_validation），epochs = 25，batch_size = 25，冗长= 1）
   
## predicon：
预测= model.predict_classes（x）
概率=型号。
 
我的问题是分类（二进制）问题。我想计算这些预测的每个置信度得分即我想知道 - 我的型号99％确定它是“ 0”。还是58％是“ 0”。
我已经找到了有关如何做的观点，但不能实施它们。我希望遵循我应该如何通过上述模型进行预测，以使我对每个预测的信心？我会很感谢一些实用的例子（最好在Keras中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>Resnet50模型在Keras中没有通过转移学习学习</title>
      <link>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</link>
      <description><![CDATA[我正在尝试对Pascal VOC 2012数据集的ImageNet重量进行预处理的RESNET50模型进行转移学习。由于它是一个多标签数据集，因此我在最后一层中使用 sigmoid 激活功能， binary_crossentropy 损失。指标是精确度，召回和准确性。以下是我用来构建20个类模型的代码（Pascal VOC有20个类）。
  img_height，img_width = 128,128
num_classes = 20
＃如果正在加载Imagenet重量，
#input必须具有静态平方形（（128，128），（160，160），（192，192）或（224，224））
base_model = applications.Resnet50.Resnet50（weights =&#39;imagEnet&#39;，include_top = false，input_shape =（img_height，img_width，3））
x = base_model.output
x = globalaveragepooling2d（）（x）
#x =辍学（0.7）（x）
预测=密集（num_classes，activation =&#39;sigmoid&#39;）（x）
模型=模型（inputs = base_model.input，outputs =预测）
对于模型中的图层[-2：]：
        layer.trainable = true
对于模型中的图层[： -  3]：
        layer.trainable = false

Adam = Adam（LR = 0.0001）
model.compile（优化器= adam，lose =&#39;binary_crossentropy&#39;，量表= [&#39;cercuciony&#39;，precision_m，recke_m]）
#print（model.summary（））

x_train，x_test，y_train，y_test = train_test_split（x_train，y，andury_state = 42，test_size = 0.2）
SaveCheckPoint = modelCheckPoint（&#39;resnettl.h5&#39;，monitor =&#39;val_loss&#39;，verbose = 1，save_best_only = true，mode =&#39;min&#39;）
早期踩踏点=早期踩踏（Monitor =&#39;val_loss&#39;，耐心= 10，详细= 1，mode =&#39;min&#39;，restore_best_weights = true）
model.fit（x_train，y_train，epochs = epochs = epochs，validation_data =（x_test，y_test），batch_size = batch_size，callbacks = [savevecheckpoint，fordonstoppcheckpoint]，shuffle = true）
model.save_weights（&#39;resnettlweights.h5&#39;）
 
它的运行时间为35个时期，直到早期踩踏，指标如下（没有辍学层）：
loss: 0.1195 - accuracy: 0.9551 - precision_m: 0.8200 - recall_m: 0.5420 - val_loss: 0.3535 - val_accuracy: 0.8358 - val_precision_m: 0.0583 - val_recall_m: 0.0757
 
即使有辍学层，我也看不到太大的区别。
 损失：0.1584-准确性：0.9428 -precision_m：0.7212 -recome_m：0.4333 -val_loss：0.3508 -val_accuracy：0.87783 -val_precision_m：0.0595-
 
对于辍学，对于几个时期，该模型达到了验证精度和精度为0.2，但不超过该验证。
我看到验证集的精确度和回忆与带有和不脱落层的训练集相比相当低。我应该如何解释？这是否意味着该模型过于拟合。如果是这样，我该怎么办？截至目前，模型预测是非常随机的（完全不正确）。数据集大小为11000张图像。]]></description>
      <guid>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</guid>
      <pubDate>Tue, 15 Oct 2019 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>用户的多个排名中的ALS（交替平方）算法</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量的研究，我们决定使用Google云基础架构并使用ALS算法（一种协作过滤方法 -   https://cloud.google.com/solutions/solutions/recommendations-using-machine-learning-learning-compute-engine-engine-engine-wrinage--模式）在我们的产品推荐系统中，该系统在下面的详细信息中进行了解释：
我们有两种类型的客户。第一类是在附近出售产品的公司，第二类是将要从这些公司购买产品的消费者

每个消费者都有能力搜索附近的公司或通过其行业搜索公司（例如杂货，干洗，屠夫等））
 当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多个项目）
 2.1。仅查看公司资料
 2.2。将公司添加到收藏夹
 2.3。开始与公司聊天
 2.4。从公司订购
 2.5。给公司评级和评论 

所以我不理解的是：上面描述的每个项目都被确定为数据库中的一些评分列，例如：
查看公司资料：10 pts 
从公司订购：20 pts 
向公司发表明星或发表评论：20 pts 
因此，每个项目都是同一用户的单独评级。
在我们的数据库中，用户 - 公司对可能有超过1行
例如：
第1行：User18-Company18-10pts（一次查看配置文件）
第2行：User18-Company18-20pts（从公司订购）
第3行：User18-Company19-10pts 
我不确定该算法，它是计算该用户对同一公司的所有评级的总和（我确切想要的），还是只是为用户的评分寻找一行一家公司？ （我想要的是该ALS算法总结该用户 - 公司对的Row1和Row2）
有人知道吗？这对于我们的推荐系统非常重要。因为我要寻找的算法需要计算用户的所有评分总和，以便推荐另一家公司。因为我们的业务模型与电影评级系统有所不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    <item>
      <title>顺序分类软件包和算法</title>
      <link>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</link>
      <description><![CDATA[我正在尝试制作一个分类器，该分类器选择项目 i 的评分（1-5）。  对于每个项目I，我都有一个向量 x ，其中包含大约40个与 i 有关的数量。  我也对每个项目都有一个金标准评分。  根据 X 的某些功能，我想训练分类器，以给我1-5的评分，与金标准非常匹配。  
我在分类器上看到的大多数信息仅处理二进制决策，而我有一个评级决定。  是否有常见的技术或代码库来处理这种问题？ ]]></description>
      <guid>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</guid>
      <pubDate>Mon, 16 Aug 2010 16:25:53 GMT</pubDate>
    </item>
    </channel>
</rss>