<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 03 May 2024 18:17:37 GMT</lastBuildDate>
    <item>
      <title>不知道这可能意味着什么，有其他人遇到过这个错误或知道它是什么吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78426251/no-idea-what-this-could-mean-has-anyone-else-encountered-this-error-or-know-wha</link>
      <description><![CDATA[我在运行机器学习模拟时遇到了这个问题，它在我朋友的机器上可以运行，但在我的机器上不行！
我已经尽我所能尝试了一切并阅读了很多博客，但找不到解决方案，
任何人都可以帮助或纠正我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78426251/no-idea-what-this-could-mean-has-anyone-else-encountered-this-error-or-know-wha</guid>
      <pubDate>Fri, 03 May 2024 17:51:42 GMT</pubDate>
    </item>
    <item>
      <title>正确理解 LSTM 的 Keras 实现：单元如何工作？</title>
      <link>https://stackoverflow.com/questions/78425666/proper-understanding-of-keras-implementation-of-lstm-how-do-the-units-work</link>
      <description><![CDATA[我的问题是 N_u 单位的 LSTM 如何处理 N_x 长度的数据？我知道以前有很多类似的问题被问过，但答案却充满矛盾和困惑。因此，我试图通过提出具体问题来消除我的疑虑。我在这里关注简单的博客：
https://colah.github.io/posts/2015-08-Understanding -LSTM/
Q0) keras 实现与上面博客一致吗？
请考虑以下代码。
导入tensorflow为tf
N_u,N_x=1,1
模型 = tf.keras.Sequential([
    tf.keras.layers.LSTM(N_u, stateful=True, batch_input_shape=(32, 1, N_x))
]）
模型.summary()

为了简单起见，我在这里的输入数据只是一个标量，并且我有一个时间步长来使事情变得简单。输出形状为(32,1)。参数个数为12。
Q1) 我有一个 LSTM 单元或单元，对吧？下面代表一个单元格，对吗？

我从图片中了解到会有12个参数：忘记gate=2个权重+1个偏差； input_gate=2*(2个权重+1个偏置);输出门=（2个权重+1个偏置）。所以到目前为止一切都很好。
Q2) 现在让我们设置N_u,N_x=1,2。我希望相同的单元格将应用于 x 的两个元素。但我发现现在参数总数是16个！为什么？是否因为我获得了与 x_2 和 LSTM 单元之间的 LSTM 连接相对应的 4 个额外权重参数？
Q3) 现在让我们设置N_u,N_x=2,1。我现在有两个 LSTM 单元。我的理解是两个单元将并行操作相同的数据（在本例中为标量）。这两个单位是完全独立的还是相互影响的？我预计参数数量为 2*12=24，但实际上我得到了 32 个。为什么是 32？
Q4）如果我设置N_u,N_x=2,2，参数数量是40。我想如果我理解了以上两点就可以得到它。
Q5）最后，是否有 keras 实现所基于的文档/论文？
提前谢谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78425666/proper-understanding-of-keras-implementation-of-lstm-how-do-the-units-work</guid>
      <pubDate>Fri, 03 May 2024 15:46:19 GMT</pubDate>
    </item>
    <item>
      <title>如何提高决策树的准确性？</title>
      <link>https://stackoverflow.com/questions/78425640/how-can-i-improve-the-accuracy-of-my-decision-tree</link>
      <description><![CDATA[目前该项目使用 R。
现在我的决策树的准确率是 5%。如何改进？我已经尝试过一些事情，但没有运气！我的目标变量是 Daily_Max，它是每日臭氧水平读数。
&lt;前&gt;&lt;代码&gt;库(mlbench)
#data(包=“mlbench”)
数据（“臭氧”，包=“mlbench”）
臭氧名称
colnames(Ozone) = c(“月”、“日”、“周”、“Daily_Max”、“Pressure_Height”、“Wind_Speed”、“湿度”、“Sandburg_Temp”、“ElMonte_Temp” ;、“Inversion_Height”、“Pressure_Gradient”、“Inversion_Temp”、“Visibility”）

#（来源）Leo Breiman，加州大学伯克利分校统计系。 Leo Breiman 和 Jerome H. Friedman (1985)，《估计多元回归和相关性的最佳变换》中使用的数据，JASA，80，第 580-598 页。
#（资源）https://rdrr.io/cran/mlbench/man/Ozone.html


＃打扫  -  -  -  -  -  -  -  -  - 
sum(is.na(臭氧))

df_Ozone = na.omit(臭氧)
总和（is.na（df_Ozone））


#我的数据需要平衡吗？ ----------------
#班级分布
class_counts = 表(df_Ozone$Daily_Max)
打印（类计数）

# 绘制类别分布
barplot(class_counts, main = “类别分布”)

# 训练模型的示例（替换为您的实际模型）
w_model &lt;- rpart(Daily_Max ~ ., data = df_Ozone)

＃ 作出预测
w_predictions &lt;- 预测(w_model, newdata = df_Ozone)

# 计算混淆矩阵
fusion_matrix &lt;- 表(w_predictions, df_Ozone$Daily_Max)
打印（混淆矩阵）

#分箱
#低 0 - 12 ppb
#中 12 - 25 ppb
#高 26 - 38 ppb
休息时间 = c(1, 12, 25, 38)
df_Ozone$bin = cut(df_Ozone$Daily_Max, Breaks = Breaks, labels = c(“低”, “中”, “高”), include.lowest = TRUE)
打印（df_臭氧）


＃决策树  -  -  -  - -
设置.种子(123)

# 将数据分为训练集和测试集
train_indices = Sample(1:nrow(df_Ozone), 0.7 * nrow(df_Ozone)) # 70% 用于训练
dt_train = df_Ozone[train_indices, ]
dt_test = df_Ozone[-train_indices, ]

库（r部分）
库（rpart.plot）
dt_model = rpart(Daily_Max ~ ., 数据 = dt_train)

# 可视化决策树
rpart.plot(dt_model)

# 对测试集进行预测
dt_predictions = 预测(dt_model, dt_test)
dt_预测

dt_table = 表(dt_test$Daily_Max, dt_predictions)
数据表


# 计算准确率
dt_accuracy = sum(diag(dt_table)) / sum(dt_table)
dt_准确度

dt_准确度 = dt_准确度 * 100
dt_accuracy #accuracy 为 4.92%，表现不佳

# 决策树总结
摘要（dt_model）


我尝试删除“周”从数据中提取特征，它实际上使准确性变得更差！我不知道从这里该去哪里。]]></description>
      <guid>https://stackoverflow.com/questions/78425640/how-can-i-improve-the-accuracy-of-my-decision-tree</guid>
      <pubDate>Fri, 03 May 2024 15:39:50 GMT</pubDate>
    </item>
    <item>
      <title>运行网络时出错：输出排名无效</title>
      <link>https://stackoverflow.com/questions/78425599/error-running-network-invalid-rank-for-output</link>
      <description><![CDATA[我正在尝试使用 Go 的 onnxruntime 包装器来推断 ONNX 模型。
模型可视化：

我尝试将 ort.NewShape(int64(rows), 1) 更新为 ort.NewShape(int64(rows))
但这会导致一个新错误：
&lt;块引用&gt;
运行模型时出错：运行网络时出错：非零状态代码
运行 TreeEnsembleRegressor 节点时返回。名称：&#39;&#39;状态
信息：
/tmp/onnxruntime-20240227-5139-v94cng/onnxruntime/core/framework/execution_frame.cc:173
地位
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(const int,
int, const TensorShape *, OrtValue *&amp;, const Node &amp;) 形状 &amp;&amp;
张量.Shape() == *形状为假。 OrtValue形状验证
失败的。当前形状：{6}请求的形状：{6,1}
]]></description>
      <guid>https://stackoverflow.com/questions/78425599/error-running-network-invalid-rank-for-output</guid>
      <pubDate>Fri, 03 May 2024 15:30:37 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 联合混淆矩阵</title>
      <link>https://stackoverflow.com/questions/78425468/tensorflow-federated-confusion-matrix</link>
      <description><![CDATA[我想知道如何在 TensorFlow 联合学习中创建混淆矩阵。此外，我很好奇最后为每个客户生成混淆矩阵的可能性。
一个想法是使用本地预测和地面实况标签来计算每个客户端设备上的混淆矩阵，方法是创建一个接受这些输入并返回相应混淆矩阵的函数。之后可以将其汇总到总体中。我对不对？
有人可以帮我看看这在 Tensroflow-Federated Framework 中是如何实现的吗]]></description>
      <guid>https://stackoverflow.com/questions/78425468/tensorflow-federated-confusion-matrix</guid>
      <pubDate>Fri, 03 May 2024 15:05:41 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 使用的 VRAM 数量过多</title>
      <link>https://stackoverflow.com/questions/78425445/pytorch-is-using-an-ungodly-amount-of-vram</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78425445/pytorch-is-using-an-ungodly-amount-of-vram</guid>
      <pubDate>Fri, 03 May 2024 15:00:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 GAN 模型训练在 90 次迭代时停止></title>
      <link>https://stackoverflow.com/questions/78425198/why-does-my-gan-model-training-stops-at-90-iterations</link>
      <description><![CDATA[我正在训练一个cycleGAN模型，每个域40张图片。 epochs 应该是 50，batch_size=1，总迭代次数是 2000，batch_per_epoch 是 40，但是模型在迭代 90 次后停止训练，没有任何条件可以将 90 视为 true 并且模型停止.
def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, 数据集, epochs=1):
    # 定义训练运行的属性
    n_epochs, n_batch, = epochs, 1 #batch size固定为1，如论文中建议的
    # 确定鉴别器的输出正方形形状
    n_patch = d_model_A.output_shape[1]
    # 解压数据集
    trainA, trainB = 数据集
    # print(trainA.shape,trainB.shape)
    # 为假图像准备图像池
    池A，池B = 列表（），列表（）
    # 计算每个训练时期的批次数
    bat_per_epo = int(len(trainA) / n_batch)
    # 计算训练迭代次数
    n_steps = bat_per_epo * n_epochs
    
    打印（n_steps，bat_per_epo）
    # 手动枚举纪元
    对于范围内的 i（n_steps）：
        # 从每个域（A和B）中选择一批真实样本
        X_realA, y_realA =generate_real_samples(trainA, n_batch, n_patch)
        X_realB, y_realB =generate_real_samples(trainB, n_batch, n_patch)
        # 使用 B to A 和 A to B 生成器生成一批假样本。
        X_fakeA, y_fakeA =generate_fake_samples(g_model_BtoA, X_realB, n_patch)
        X_fakeB, y_fakeB =generate_fake_samples(g_model_AtoB, X_realA, n_patch)
        # 更新池中的假图像。请记住，论文建议使用 50 个图像的缓冲区
        X_fakeA = update_image_pool(poolA, X_fakeA)
        X_fakeB = update_image_pool(poolB, X_fakeB)
        # 通过复合模型更新生成器 B-&gt;A
        # print(类型(X_realA),类型(X_realB),类型(X_fakeA),类型(X_fakeB),类型(y_realA),类型(y_realB),类型(y_fakeA),类型(y_fakeB))
        
        g_loss2 = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])
        
        # 更新 A 的鉴别器 -&gt; [真/假]
        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)
        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)
        
        # 通过复合模型更新生成器 A-&gt;B
        g_loss1 = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])
        # 更新 B 的鉴别器 -&gt; [真/假]
        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)
        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)
        
        # 总结性能
        #由于我们的批量大小=1，迭代次数将与数据集的大小相同。
        #在一个纪元中，迭代次数等于图像数量。
        #如果你有 100 张图像，那么 1 epoch 就是 100 次迭代
        print(&#39;迭代&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1 ,dB_loss2, g_loss1,g_loss2))
        # 定期评估模型性能
        #如果批量大小（总图像）=100，则每 75 次迭代后将总结性能。
        如果 (i+1) % (bat_per_epo * 1) == 0:
            # 绘制 A-&gt;B 翻译
            总结性能（i，g_model_AtoB，trainA，&#39;AtoB&#39;）
            # 情节 B-&gt;A 翻译
            总结性能（i，g_model_BtoA，trainB，&#39;BtoA&#39;）
        如果 (i+1) % (bat_per_epo * 5) == 0:
            # 保存模型
            # #如果批量大小（总图像）=100，模型将在之后保存
            #每 75 次迭代 x 5 = 375 次迭代。
            save_models(i, g_model_AtoB, g_model_BtoA)

这是训练函数，历元传递为 50，最后一个条件：
如果 (i+1) % (bat_per_epo * 5) == 0:
    # 保存模型
    # #如果批量大小（总图像）=100，模型将在之后保存
    #每 75 次迭代 x 5 = 375 次迭代。
    save_models(i, g_model_AtoB, g_model_BtoA)

没有被执行。
随着生成器损失的减少，模型似乎正在训练和改进，生成的图像并没有那么糟糕，因为它只训练了 40 次迭代和 80 次迭代，在第 40 次和 80 次之后生成了图片并保存了模型迭代，然后停止。
这是我得到的结果：
40 次迭代后
80 次迭代后]]></description>
      <guid>https://stackoverflow.com/questions/78425198/why-does-my-gan-model-training-stops-at-90-iterations</guid>
      <pubDate>Fri, 03 May 2024 14:13:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在损失函数内使用未知的matlab函数训练pytorch模型？</title>
      <link>https://stackoverflow.com/questions/78425073/how-to-train-a-pytorch-model-with-an-unknown-matlab-function-inside-the-loss-fun</link>
      <description><![CDATA[我目前正在尝试实现自定义损失函数来使用 Python 和 Pytorch 训练神经网络。问题是我的函数调用 Matlab 中的另一个函数来计算一些值。我们将此 Matlab 函数称为 M1(y)。所以基本上我的损失函数是
损失(y,t) = M1(y) - M2(t)

其中 y 是模型的预测值，t 是实际目标值，M2(t) 是另一个 Matlab 函数。有没有办法在不知道 M1 和 M2 细节的情况下计算梯度并训练模型？
我相信应该可以使用类似的方法来近似梯度
(损失(Y(x,d+h),t) - 损失(Y(x,d-h),t))/(2h)

对于一个非常小的 h.其中 Y(x,d) 是输入 x 和用于计算 y 的参数 d。这将近似损失函数关于参数 d 的导数。
这是一个值得追求的解决方案，还是太不准确了？
我将如何使用 pytorch 来实现它？]]></description>
      <guid>https://stackoverflow.com/questions/78425073/how-to-train-a-pytorch-model-with-an-unknown-matlab-function-inside-the-loss-fun</guid>
      <pubDate>Fri, 03 May 2024 13:44:54 GMT</pubDate>
    </item>
    <item>
      <title>针对 BERT 模型进行微调 - 我需要什么数据集来调整模型</title>
      <link>https://stackoverflow.com/questions/78424935/fine-tuned-for-the-bert-model-what-dataset-do-i-need-to-tune-the-model</link>
      <description><![CDATA[我有一个与深度学习相关的问题。
如果您在一周中有时间可以向我解释一件事，我将不胜感激。
我目前正在创建一个与微调 BERT 相关的项目。我仍在考虑基本版本或大版本。
该项目将基于初步文件分析并提取从文件中提取的最重要信息。类似于信用申请的预检查文件。
根据 WordPiece 方法，我为 BERT 创建了大约 1500 个额外代币。
我对数据集有疑问。目前我有大约 25 万个句子（每个句子占一个新行）。
这些句子的质量各不相同。没有标签，因为我想基于MLM（屏蔽）进行微调。
数据集的质量是平均的。可能会有一些噪音和错误。
我的问题是，是否最好对较少数量的句子进行微调，例如从大约 30-40,000 个（适当选择的）+新令牌的数据集中进行选择，或者尝试消耗资源并启用完整数据集的训练？
为了时间+计算资源，我更愿意先向社区询问，而不是每次烧毁环境几次。
问候，]]></description>
      <guid>https://stackoverflow.com/questions/78424935/fine-tuned-for-the-bert-model-what-dataset-do-i-need-to-tune-the-model</guid>
      <pubDate>Fri, 03 May 2024 13:17:37 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的多线程无法在 Raspberry Pi 上正常工作</title>
      <link>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</guid>
      <pubDate>Fri, 03 May 2024 12:10:47 GMT</pubDate>
    </item>
    <item>
      <title>从核矩阵中删除特征</title>
      <link>https://stackoverflow.com/questions/78424564/remove-feature-from-kernel-matrix</link>
      <description><![CDATA[我对机器学习还很陌生，所以请耐心等待:)
我正在尝试使用带有预计算内核的 SVM 来执行二进制分类任务（在 python 中使用 sklearn）。
我创建了我的火车内核，但我包含了一个我不打算包含的功能，并且测试数据中不存在该功能。我对内核除了“距离”之外到底是什么没有非常透彻的了解。数据点之间，但是否可以从训练内核中删除此功能而无需生成新的功能？
我希望这是有道理的，请再次耐心等待，因为我对此很陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78424564/remove-feature-from-kernel-matrix</guid>
      <pubDate>Fri, 03 May 2024 11:59:00 GMT</pubDate>
    </item>
    <item>
      <title>自监督模型收敛到一个常数</title>
      <link>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</link>
      <description><![CDATA[我试图训练巴洛双胞胎模型进行图像分类。尽管如此，我在完成模型训练后遇到了一个问题。模型似乎已经成为一个常数，无论两个给出的图像有多么不同，它总是返回数字 2046，小数部分略有变化。
该模型尝试将互相关矩阵最小化为单位矩阵。
有没有办法解决这个问题。
def off_diagonal(x):
    # 返回方阵非对角线元素的展平视图

BarlowTwins 类（nn.Module）：
    def __init__(自身, 羔羊,batch_size):
        超级().__init__()
        self.batch_size = 批量大小
        self.lambd = 羔羊
        self.backbone = torchvision.models.resnet34(zero_init_residual=True,weights=&#39;DEFAULT&#39;)
        self.backbone.fc = nn.Identity()
        self.size = [512,2048,2048,2048]


        ＃ 投影仪
        _尺寸 = [512,4096,4096,4096]

        层=[]
        对于范围内的 i(len(self.sizes) - 2)：
            层.追加（nn.Linear（self.sizes [i]，self.sizes [i + 1]，偏差= False））
            Layers.append(nn.BatchNorm1d(self.sizes[i + 1]))
            层.append(nn.ReLU(inplace=True))
        层.追加（nn.Linear（self.sizes [-2]，self.sizes [-1]，偏差= False））
        self.projector = nn.Sequential(*层数)

        # 表示 z1 和 z2 的归一化层
        self.bn = nn.BatchNorm1d(self.sizes[-1], affine=False)

    def 前进（自身，y1，y2）：
        z1 = self.投影仪(self.backbone(y1))
        z2 = self.projector(self.backbone(y2))

        # 经验互相关矩阵
        c = self.bn(z1).T @ self.bn(z2)

        # 对所有 GPU 之间的互相关矩阵求和
        c.div_(self.batch_size)

        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()
        # 打印(&#39;c&#39;, c)
        val = torch.diagonal(c).sum()

        off_diag = off_diagonal(c).pow_(2).sum()
        # print(&#39;off_diag&#39;, off_diag)
        
        损失 = on_diag + self.lambd * off_diag
        回波损耗，值

来自 tqdm 导入 tqdm



def intiate_p（模型，epoch_n，加载器，print_freq，lr，动量，weight_decay）：
    epoch_tqdm = tqdm(范围(epoch_n))
    参数权重 = []
    参数偏差 = []
    r = 打印频率
    函数=模型


    优化器= optim.SGD(model.parameters(),lr=lr,动量=动量,weight_decay=weight_decay)
  

    调度程序= optim.lr_scheduler.PolynomialLR(优化器,total_iters=40,power=2.0)


    损失=[]

    # 目标 = torch.tensor(2048, dtype=torch.float32)
    开始时间 = 时间.time()
    stats_file = open(&#39;stats.txt&#39;, &#39;a&#39;, 缓冲=1)

    
    对于 epoch_tqdm 中的纪元：
        
        对于 enumerate(loader, start=epoch * len(loader)) 中的步骤 ((y1, y2), _)：

            优化器.zero_grad()
            损失 = func.forward(y1, y2)[0]
            损失.追加（损失）
            loss.backward()
            优化器.step()
            调度程序.step()
            epoch_tqdm.set_description(f“损失是：{abs(loss -2048)}”)

  
    回波损耗

这里的问题是我不确定我的模型评估方法是否正确。因为我随机将两个图像作为 y1 和 y2 输入到我的模型中进行 100 次迭代，但结果保持不变。
旁注：我尝试了许多不同的训练变量值，我能得到的最佳损失是 100。
md = BarlowTwins(batch_size=64,lambda=0.005)
t = intiate_p(model=md, epoch_n=20, loader=loader,lr=0.4,momentum=0.3 ,print_freq=10,weight_decay=0.0001)
# 从 2000 左右开始，损失收敛到 100 左右

这是不同获取值的直方图示例：
]]></description>
      <guid>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</guid>
      <pubDate>Thu, 02 May 2024 22:03:12 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 YOLOv8 进行训练后出现多处理错误</title>
      <link>https://stackoverflow.com/questions/78412108/multiprocessing-error-after-trying-to-train-with-yolov8</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78412108/multiprocessing-error-after-trying-to-train-with-yolov8</guid>
      <pubDate>Wed, 01 May 2024 05:10:52 GMT</pubDate>
    </item>
    <item>
      <title>处理机器学习中预测时间序列的分类特征的方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78402245/approach-to-deal-with-categorical-features-for-forecasting-time-series-in-ml</link>
      <description><![CDATA[我必须根据 csv 数据对给定日期的网络单元格进行流量预测，其中包含 5 列：区域、站点、单元格、日期、流量
例如：
&lt;上一页&gt;&lt;代码&gt;AAN,AAN001,AAN001A,2021-01-01,2.56

AAN,AAN001,AAN001B,2021-01-01,5.6

ANM,ANM448,ANM448B,2021-04-19,1.2

ANM,ANM448,ANM448C,2021-04-19,3.6

我对处理站点或单元格等分类特征感到困惑。我可以在这些列中进行热编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/78402245/approach-to-deal-with-categorical-features-for-forecasting-time-series-in-ml</guid>
      <pubDate>Mon, 29 Apr 2024 11:01:17 GMT</pubDate>
    </item>
    <item>
      <title>Akinator 游戏背后是什么样的算法？</title>
      <link>https://stackoverflow.com/questions/13649646/what-kind-of-algorithm-is-behind-the-akinator-game</link>
      <description><![CDATA[令我惊讶的是，Akinator 应用能够通过以下方式猜测字符：只问几个问题。所以我想知道什么样的算法或方法让它做到这一点？这类算法有名称吗？我在哪里可以阅读有关它们的更多信息？]]></description>
      <guid>https://stackoverflow.com/questions/13649646/what-kind-of-algorithm-is-behind-the-akinator-game</guid>
      <pubDate>Fri, 30 Nov 2012 16:59:38 GMT</pubDate>
    </item>
    </channel>
</rss>