<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 27 Jul 2024 03:17:59 GMT</lastBuildDate>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

 input = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用任何类型的基础模型来检测通常发生在眼睛中的疾病？</title>
      <link>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</link>
      <description><![CDATA[我刚来这里不久，如果可以的话，我希望你们能帮我。
我想建立一个模型，可以检测视网膜图像是否有微动脉瘤。
可以使用基础模型吗？如果可以，它会带来什么好处？
如果我想，我可以使用哪种基础模型，比如我最近读到关于 RetFound 的文章，我相信它适合我的范围。
（这就像一个理论问题）如果这些模型甚至没有接受过执行特定任务的训练，它们究竟如何适用于我们的特定用例数据，比如，制作基础模型的人如何决定训练数据的范围？它只是一个对视网膜图像进行处理模型，还是一个范围更广的模型，除了视网膜图像外，它还与其他相关数据有关（我相信）。第三个问题是可选的，如果你能回答，我将不胜感激。
我尝试使用一个基础模型，https://github.com/facebookresearch/deit/blob/main/README_deit.md，但我在这里学得并不多，任何帮助、任何指导都将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</guid>
      <pubDate>Fri, 26 Jul 2024 18:19:53 GMT</pubDate>
    </item>
    <item>
      <title>涅斯捷罗夫加速梯度的两种变体：它们是等效的吗？</title>
      <link>https://stackoverflow.com/questions/78799083/two-variants-of-nesterov-accelerated-gradient-are-they-equivalent</link>
      <description><![CDATA[我很困惑地发现 Paperswithcode 上对 Nesterov 加速梯度的描述，即：
v_t = beta * v_t-1 + eta * ∇ J(theta - beta * v_t-1)
theta_t = theta_t-1 + v_t

与原始 Sutskever 等人的论文第 133 页中的描述略有不同。 3：
v_t = beta * v_t-1 - eta * ∇ J(theta + beta * v_t-1)
theta_t = theta_t-1 + v_t

在软弱的时刻，我问过你知道是谁，他围绕前瞻和后瞻制定了一个答案，但我无法验证这一点。
这两个公式是否等价，如果等价，如何证明这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78799083/two-variants-of-nesterov-accelerated-gradient-are-they-equivalent</guid>
      <pubDate>Fri, 26 Jul 2024 16:21:10 GMT</pubDate>
    </item>
    <item>
      <title>Raspberry Pi 0 2W 上的深度音频分类模型的最佳 ML API 是什么？</title>
      <link>https://stackoverflow.com/questions/78798996/what-is-the-best-ml-api-for-a-deep-audio-classification-model-on-the-raspberry-p</link>
      <description><![CDATA[我想听听您的意见，哪种 ML API 最适合构建音频分类模型。此模型将部署在一台小型 Raspberry Pi（Raspberry Pi 0 2W；可能略有不同）计算机上，该计算机位于我正在构建的设备中，是我所从事的技术初创公司的一部分。欢迎您提出您的想法和建议。
我开始使用 tensorflow keras 进行深度频谱分析；但此时的首要任务是构建尽可能最好的模型，我知道 tf keras 有点业余。]]></description>
      <guid>https://stackoverflow.com/questions/78798996/what-is-the-best-ml-api-for-a-deep-audio-classification-model-on-the-raspberry-p</guid>
      <pubDate>Fri, 26 Jul 2024 15:58:11 GMT</pubDate>
    </item>
    <item>
      <title>来自segmentation_models_pytorch 的 Unet 在训练中停滞</title>
      <link>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</link>
      <description><![CDATA[我一直在遵循关于在自定义数据集上训练分割模型的教程，但它拒绝在训练模型方面取得任何进展。
这是我的模型设置
import fragmentation_models_pytorch as smp
import torch

ENCODER = &#39;efficientnet-b0&#39;
ENCODER_WEIGHTS = &#39;imagenet&#39;
CLASSES = [&#39;ship&#39;]
ACTIVATION = &#39;sigmoid&#39;
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model = smp.Unet(
coder_name=ENCODER, 
coder_weights=ENCODER_WEIGHTS, 
classes=len(CLASSES), 
activation=ACTIVATION,
).to(DEVICE)

from fragmentation_models_pytorch import utils as smp_utils

loss = smp_utils.losses.DiceLoss()
metrics = [
smp_utils.metrics.IoU(threshold=0.5),
]

optimizer = torch.optim.Adam([ 
dict(params=model.parameters(), lr=0.0001),
])


和 epochs 运行器
train_epoch = smp_utils.train.TrainEpoch(
model, 
loss=loss, 
metrics=metrics, 
optimizer=optimizer,
device=DEVICE,
verbose=True,
)

valid_epoch = smp_utils.train.ValidEpoch(
model, 
loss=loss, 
metrics=metrics, 
device=DEVICE,
verbose=True,
)

而且，当我运行训练时，模型只是停留在第一个 epoch 上，没有任何进展
max_score = 0

for i in range(0, 40):

print(&#39;\nEpoch: {}&#39;.format(i))
train_logs = train_epoch.run(train_loader)
valid_logs = valid_epoch.run(valid_loader)

# 执行某些操作（保存模型、更改 lr 等）
if max_score &lt; valid_logs[&#39;iou_score&#39;]:
max_score = valid_logs[&#39;iou_score&#39;]
torch.save(model, &#39;./best_model.pth&#39;)
print(&#39;模型已保存！&#39;)

if i == 25:
optimizer.param_groups[0][&#39;lr&#39;] = 1e-5
print(&#39;将解码器学习率降低至 1e-5！&#39;)

结果：
Epoch：0
train：0%| | 0/3851 [00:00&lt;?, ?it/s]

我这样把它放了 3 个小时，它一点变化都没有
我在 CPU (i7-10710U) 上运行（我知道它比 GPU 慢得多，但我的 GPU (GeForce 1650mq) 不支持 cuda），内存为 32 GB，我之前运行过类似的模型，没有任何问题。
有人能帮帮我吗？也许我漏掉了什么？也许有一个更轻的模型可以在我的系统上运行？
我已经尝试了一些其他设置和模型，YOLOv8 和 YOLOv3 也拒绝训练。]]></description>
      <guid>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</guid>
      <pubDate>Fri, 26 Jul 2024 15:14:45 GMT</pubDate>
    </item>
    <item>
      <title>GradientBoostedClassifier() 中的 min_samples_leaf 行为怪异</title>
      <link>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</link>
      <description><![CDATA[尝试在 GradientBoostedClassifer() 中调整 min_samples_leaf。我看到了偏差/方差权衡的预期结果。但是，为了测试边界，我让 min_samples_leaf &gt;训练数据集中有 n_samples，预计会出现错误或其他问题，但我仍然得到与模型调整时类似的结果：
df = df_a # 样本数 = 347
df=df.sample(frac=1) 
train_proportion = 0.8 
n = len(df)
t = int(train_proportion * n)

# 单独的训练和测试集
y = df[&#39;detected&#39;]
X = df.loc[:, ~df.columns.isin([&#39;detected&#39;])]

# 训练集中的样本
train_x = X.iloc[:t,:].reset_index().iloc[:,1:]
# 测试集中的样本
test_x = X.iloc[t:,:].reset_index().iloc[:,1:]
# 训练集中的目标
train_y = pd.Series(y[:t].reset_index().iloc[:,1:].iloc[:,0])
#测试集中的目标
test_y = pd.Series(y[t:].reset_index().iloc[:,1:].iloc[:,0])

clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 10, random_state= 0, min_samples_leaf=500)
clf.fit(train_x,train_y)
print(clf.score(train_x,train_y))
print(clf.score(test_x,test_y))

输出：
0.924187725631769
0.9142857142857143

为什么会这样？我预计会出现错误或不会进行拆分。文档中似乎没有说明如果 min_samples_leaf &gt; n_samples 会发生什么。对 int 的唯一要求是范围 [1,inf]。对此也没有其他说明。
我当时想也许它会将 min_samples_leaf 重置为某个可用值，但所有子树都没有深度，也没有进行拆分：subtree]]></description>
      <guid>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</guid>
      <pubDate>Fri, 26 Jul 2024 07:05:30 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是一个初学者，我还在学习稀疏矩阵以及它们如何与其他东西一起工作。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数 sparse_output=True 对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用 transformed_X 和目标 y 拟合 RandomForestClassifier 时，它显示了此错误。
#seed
np.random.seed(42)

#one hot encoding imports
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#Data splitting
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#select columns
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器 
enc = OneHotEncoder()

#使用 ct 拟合编码器
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;以压缩稀疏行格式存储 10360 个元素&gt;
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[416], line 13
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。
373 # 仅需标准即可确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但结果显示样本数量不一致。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[444]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError：发现输入变量的样本数量不一致：[1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>我在训练随机森林回归器时不断遇到这个问题</title>
      <link>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</link>
      <description><![CDATA[/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: 
UserWarning：X 有特征名称，但 RandomForestRegressor 拟合时没有特征名称
warnings.warn(

我尝试添加 .values，但仍然标记错误。]]></description>
      <guid>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</guid>
      <pubDate>Thu, 25 Jul 2024 19:18:50 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在我的图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将它们的参数减少到嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（具有其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，并将训练好的 CNN 的参数输入到 VAE 模型的编码器输入中，最后将参数向量重建为 CNN 模型参数。
提前致谢！
这是 CNN 模型：
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
self.conv2_drop = nn.Dropout2d()
self.fc1 = nn.Linear(320, 50)
self.fc2 = nn.Linear(50, 10)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
x = x.view(-1, 320)
x = F.relu(self.fc1(x))
x = F.dropout(x, training=self.training)
x = self.fc2(x)
return F.log_softmax(x)

以下代码用于 VAE：
class VAE(nn.Module):
def __init__(self, image_channels=1, h_dim=1024, z_dim=32):
super(VAE, self).__init__()
self.encoder = nn.Sequential(
nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 128, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(128, 256, kernel_size=4, stride=2),
nn.ReLU(),
Flatten()
)

self.fc1 = nn.Linear(h_dim, z_dim)
self.fc2 = nn.Linear(h_dim, z_dim)
self.fc3 = nn.Linear(z_dim, h_dim)

self.decoder = nn.Sequential(
UnFlatten(),
nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
nn.Sigmoid(),
)

def reparameterize(self, mu, logvar):
std = logvar.mul(0.5).exp_()
# return torch.normal(mu, std)
esp = torch.randn(*mu.size())
z = mu + std * esp
返回 z

def bottleneck(self, h):
mu, logvar = self.fc1(h), self.fc2(h)
z = self.reparameterize(mu, logvar)
返回 z, mu, logvar

def encode(self, x):
h = self.encoder(x)
z, mu, logvar = self.bottleneck(h)
返回 z, mu, logvar

def decrypt(self, z):
z = self.fc3(z)
z = self.decoder(z)
返回 z

def forward(self, x):
z, mu, logvar = self.encode(x)
z = self.decode(z)
返回 z, mu, logvar
]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Matrox Model Finder 在单个图像中多次查找同一模型？</title>
      <link>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</link>
      <description><![CDATA[我是 Matrox 的新手，所以这可能是一个初学者的问题。
我有一个托盘，上面有多个项目，它们都是同一型号。当我将 ModelFinder 步骤添加到程序中时，我添加了我正在寻找的模型，但它只显示我注册的模型，我猜是因为相机的失真。我如何让 Matrox 知道还有更多项目，并且它们也是同一型号？

我添加了一个必须找到它的搜索区域，我选择了查找所有出现的选项，但它只显示一个，而不是实际存在的 3/4。
]]></description>
      <guid>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</guid>
      <pubDate>Thu, 11 Apr 2024 16:02:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会，尝试利用 Sagemaker 的几个实用程序，而不是像我目前所做的那样在 Notebook 上运行所有内容。
问题是，在研讨会上，他们教你如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都在使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，所以我正在实例化一个像这样的估算器 此示例文件:
sklearn = SKLearn(entry_point=&quot;train.py&quot;, 
framework_version=&quot;1.2-1&quot;, 
instance_type=&quot;ml.m5.xlarge&quot;, 
role=role,
hyperparameters=fixed_hyperparameters
)

之后，我将使用刚刚创建的估算器实例化 HyperparameterTuner 作业，其中包含我想要的超参数范围测试。
hyperparameters_ranges = {
&quot;n_estimators&quot;: ContinuousParameter(100, 500),
&quot;learning_rate&quot;: ContinuousParameter(1e-2, 1e-1),
&quot;max_depth&quot;: IntegerParameter(2, 5),
&quot;subsample&quot;: ContinuousParameter(0.6, 1),
&quot;max_df&quot;: ContinuousParameter(0.4, 1),
&quot;max_features&quot;: IntegerParameter(5, 25),
&quot;use_idf&quot;: CategoricalParameter([True, False])
}

metric = &quot;validation:f1&quot;

tuner = HyperparameterTuner(
sklearn,
metric,
hyperparameters_ranges,
max_jobs=2,
max_parallel_jobs=2
)

我的问题是，我没有找到任何关于如何访问“train.py”文件中 SKLearn 估算器中传递的超参数的信息。我也没有找到最佳超参数存储在哪里，以便我可以将它们用于最终模型。有人能告诉我这是否可行，或者提供替代方案，看看是否有其他更简单的方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    <item>
      <title>视觉框架（iOS）：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest 有何不同？</title>
      <link>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</link>
      <description><![CDATA[您可以使用两种不同的请求通过 iOS Vision Framework 执行人脸检测任务：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest。它们都返回一个 VNFaceObservation 数组，每个检测到的人脸对应一个数组。VNFaceObservation 具有多种可选属性，包括 boundingBox 和 landmarks。landmarks 对象还包括可选属性，例如 nose、innerLips、leftEye 等。
这两种不同的 Vision 请求在执行人脸检测的方式上是否有所不同？
似乎 VNDetectFaceRectanglesRequest 只能找到一个边界框（可能还有其他一些属性），但找不到任何地标。另一方面，VNDetectFaceLandmarksRequest 似乎可以同时找到边界框和地标。
是否存在一种请求类型可以找到面部而另一种则找不到的情况？ VNDetectFaceLandmarksRequest 是否 优于 VNDetectFaceRectanglesRequest，或者后者在 性能 或 可靠性 方面可能更具优势？
以下是如何使用这两个 Vision 请求的示例代码：
let faceLandmarkRequest = VNDetectFaceLandmarksRequest()
let faceRectangleRequest = VNDetectFaceRectanglesRequest()
let requestHandler = VNImageRequestHandler(ciImage: image, options: [:])
try requestHandler.perform([faceRectangleRequest, faceLandmarkRequest])
if let rectangleResults = faceRectangleRequest.results as? [VNFaceObservation] {
let boundingBox1 = rectangleResults.first?.boundingBox //这是一个可选类型
}
if let landmarkResults = faceLandmarkRequest.results as? [VNFaceObservation] {
let boundingBox2 = landmarkResults.first?.boundingBox //这是一个可选类型
let landmarks = landmarks //这是一个可选类型
}
]]></description>
      <guid>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</guid>
      <pubDate>Thu, 25 Feb 2021 11:53:04 GMT</pubDate>
    </item>
    <item>
      <title>银行交易数据集</title>
      <link>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</link>
      <description><![CDATA[我想使用银行交易数据集制作信用卡和借记卡之间的图表。借记和贷记金额，但没有得到正确的数据集。
有人能给我提供同样的数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</guid>
      <pubDate>Sat, 06 Jul 2019 13:09:23 GMT</pubDate>
    </item>
    </channel>
</rss>