<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 25 Mar 2025 18:24:38 GMT</lastBuildDate>
    <item>
      <title>训练量：“包装”和“ group_by_length”相互抵消？</title>
      <link>https://stackoverflow.com/questions/79533473/trainingarguments-do-packing-and-group-by-length-counteract-each-other</link>
      <description><![CDATA[在huggingface的 sftConfig （从 trieb&gt; triagn&gt; triagn&gt; triebs triaght&gt; triaghtarguments ）中，有两个参与priaged ）


  group_by_length ：是否将训练数据集中大致相同长度的样本组合在一起（以最大程度地减少应用的填充并更有效）。仅在应用动态填充时有用。
 包装：是否将多个序列打包到固定长度格式中。使用 max_length 定义序列长度。


  config = sftConfig（...，，， 
                   group_by_length = true， 
                   包装= true，...）
 
这些论点的目的是减少填充桨的努力。但是，当 packing = true 时，使用 使用 group_by_length = true 。我们是否可以两者都提高训练表现？他们互相抵消吗？]]></description>
      <guid>https://stackoverflow.com/questions/79533473/trainingarguments-do-packing-and-group-by-length-counteract-each-other</guid>
      <pubDate>Tue, 25 Mar 2025 11:13:20 GMT</pubDate>
    </item>
    <item>
      <title>在HuggingFace Seq2Seqtrainer中使用培训，验证和测试集</title>
      <link>https://stackoverflow.com/questions/79532570/use-of-training-validation-and-test-set-in-huggingface-seq2seqtrainer</link>
      <description><![CDATA[ i具有以下数据集，该数据集具有3个拆分（ Train ，验证和 test ）。数据是2种语言的平行语料库。
  datasetDict（{{
    火车：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：109942
    }））
    验证：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：6545
    }））
    测试：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：13743
    }））
}））
 
对于我的 seq2seqtrainer ，我提供数据集如下：
  trainer = seq2seqtrainer（
    模型=模型，
    args =训练_args，
    train_dataset = tokenized_dataset [&#39;train&#39;]，
    eval_dataset = tokenized_dataset [&#39;验证&#39;]，
    tokenizer = tokenizer，
    data_collat​​or = data_collat​​or，
    compute_metrics = compute_metrics，
）
 
将验证拆分 ever_dataset 中的拆分是正确的吗？在 documentation&gt; documentation 

用于评估的数据集。如果是数据集，则自动删除 model.forward（）方法未接受的列。如果是字典，它将在每个数据集上进行评估。

或者我应该将测试分配在 eval_dataset 中？无论哪种方式，都没有使用其中一个分裂？]]></description>
      <guid>https://stackoverflow.com/questions/79532570/use-of-training-validation-and-test-set-in-huggingface-seq2seqtrainer</guid>
      <pubDate>Tue, 25 Mar 2025 02:22:21 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以在使用CV2保持文本的清晰度的同时删除此还原图像的背景噪声？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79531758/is-there-any-way-to-remove-the-background-noise-of-this-restored-image-while-mai</link>
      <description><![CDATA[原始图像：
这是原始图像 
部分还原图像：
这是部分修复的图像 
第一个图像是在通过一些恢复技术运行之前。第二张图像是在修复后。背景噪音很多，字符的一部分缺少。我想从此图像中提取文本。我们可以在保持文本的准确性的同时执行此操作吗？任何帮助都将受到赞赏！]]></description>
      <guid>https://stackoverflow.com/questions/79531758/is-there-any-way-to-remove-the-background-noise-of-this-restored-image-while-mai</guid>
      <pubDate>Mon, 24 Mar 2025 17:32:07 GMT</pubDate>
    </item>
    <item>
      <title>CATBOOST模型的串联TF-IDF数据和分类数据</title>
      <link>https://stackoverflow.com/questions/79531266/concatenating-tf-idf-data-and-categorical-data-for-catboost-model</link>
      <description><![CDATA[我一直在尝试将TF-IDF数据与分类数据相连。但是，当串联时，默认情况下，分类数据会自动转换为float。由于catboost不支持分类特征的浮动，因此由于不再被认为是分类数据而导致稀疏数据的错误。
有解决这个问题的解决方案吗？请在下面找到我的代码以供参考：
 导入numpy作为NP
导入大熊猫作为pd
来自Catboost Import CatboostClassifier
来自sklearn.feature_extraction.text导入tfidfvectorizer
从Sklearn.Preprocessing Import LabElenCoder
来自scipy.sparse导入hstack，csr_matrix

text_data = [
    “我喜欢机器学习和数据科学”
    “深度学习是机器学习的子集”
    “自然语言处理是惊人的”
    “ AI正在改变世界”
    “大数据和AI正在彻底改变行业”。
这是给出的

pecorical_data = {
    “ cantory”：“ tech; quot” tech&#39;tech&#39;nlp’s&#39;&#39;
    “地区”：“欧洲”，“亚洲”欧洲“欧洲”
}

y = np.Array（[0，1，0，1，1]）

df_cat = pd.dataframe（centorical_data）

vectorizer = tfidfvectorizer（）
x_tfidf = vectorizer.fit_transform（text_data）

df_cat_encoded = df_cat.apply（labelencoder（）。fit_transform）

x_categorical = csr_matrix（df_cat_encoded.values）

x_combind = hstack（[x_tfidf，x_categorical]）

model = catboostClassifier（迭代= 100，Learning_rate = 0.1，深度= 5，冗长= 0）

model.fit（x_combined，y，cat_features = [x_tfidf.shape [1]，x_tfidf.shape [1] + 1]）

预测= model.predict（x_combined）

打印（预测）
 
错误：
  catboostror：&#39;data&#39;是scipy.sparse.spmatrix floating Point数值类型， 
这意味着没有分类功能，但是“ cat_features”参数指定非零 
分类功能的数量
 ]]></description>
      <guid>https://stackoverflow.com/questions/79531266/concatenating-tf-idf-data-and-categorical-data-for-catboost-model</guid>
      <pubDate>Mon, 24 Mar 2025 13:53:36 GMT</pubDate>
    </item>
    <item>
      <title>Lora Fineted Llama 8b的自定义前往前方法 - 使用Unsploth</title>
      <link>https://stackoverflow.com/questions/79531231/custom-forward-method-for-lora-finetuned-llama-8b-using-unsloth</link>
      <description><![CDATA[我正在尝试对我的固定模型进行一些消融研究。试图超载前向方法
 类GROK_CUSTOMPEFTCAUSALLM（TORCH.NN.MODULE）：
def __init __（自我，模型）：
    super（）.__ init __（）
    self.model =型号＃原始fastlanguagemodel

    ＃修补内部模型的前进
    orig_forward = self.model.model.forward

    def new_forward（inned_self，input_ids，activation_mask = none，num_logits_to_keep = none，** kwargs）：
        打印（&#39;🔹自定义前进被调用！＆quot; flush = true）

        ＃嵌入
        hidden_​​states = inner_self.model.embed_tokens（input_ids）
        打印（嵌入后隐藏状态：{hidden_​​states.shape}＆quot; flush = true）

        ＃注意面具
        如果active_mask无：
            activation_mask = torch.ones（input_ids.shape，dtype = type = turch.bfloat16，device = input_ids.device）.bool（）
        别的：
            ##强制入bool
            activation_mask = activation_mask.bool（）

        ＃层
        past_key_values =无
        对于i，在枚举中层（inner_self.model.layers）：
            打印（f＆quot&#39;layer {i}＆quot＆quot＆clush = true）
            layer_output = layer（hidden_​​states，activation_mask = activation_mask）
            print（f&#39;layer {i} output：{type（layer_output）}，len：{len（layer_output）如果isInstance（layer_output，tuple，tuple）else 1}＆quot;，\ \
                                                                                              冲洗= true）
            hidden_​​states = layer_output [0]如果isInstance（layer_output，tuple）else layer_output
            如果len（layer_output）＆gt; 1：
                past_key_values = layer_output [1]＃更新kv缓存
            print（f＆quot&#39;efter layer {i}：{hidden_​​states.shape}＆quort＆quort＆quort; flush = true）

        ＃规范
        hidden_​​states = inner_self.model.norm（hidden_​​states）
        print（f＆quot&#39;norm：{hidden_​​states.shape}＆quort＆quot; flush = true）

        ＃logits
        logits = inner_self.lm_head（hidden_​​states）
        print（f＆quot“ logits：{logits.shape}＆quort”，flush = true）

        ＃返回完整输出
        返回causallMoutputwithpast（logits = logits，past_key_values = past_key_values）
    ＃覆盖内部模型的前进
    导入类型
    self.model.model.forward = types.methodtype（new_forward，self.model.model）

def生成（self， *args，** kwargs）：
    打印（&#39;🔹自定义生成调用！＆quot; flush = true）
    ＃过滤num_logits_to_keep

    if&#39;num_logits_to_keep＆quot在夸尔格斯：
        del Kwargs [＆quot; num_logits_to_keep;]
    返回self.model.generate（*args，** kwargs）
 
我像这样调用了
 模型，tokenizer = fastlanguagemodel.from_pretrateing（
        model_name = inf_model_，
        max_seq_length = 3072，
        dtype = none，
        load_in_4bit = true
）

eos_token = tokenizer.eos_token

模型= fastlanguagemodel.for_inference（模型）

＃补丁传递通行证
模型= grok_custompeftcausallm（模型）
print（type（model.model））＃确保它是您的“ SkippableModel”
输入= tokenizer（
[
    data_prompt.format（
        语境_，
        “”
    ）
]，return_tensors =; pt; quot。


print（data_prompt.format（上下文_，; quord;））
启动器_ = time.time（）
输出=型号。generate（**输入，max_new_tokens = 800，温度= 0.1）
打印（&#39;take out ::&#39;，time.time（） - 启动_）

答案= tokenizer.batch_decode（输出）
 
它只是生成gibberish（在没有此超载的情况下，它的性能绝对是预期的 - 良好的代码生成）。现在的问题是，我可以使用的层次结构的唯一部分是Model.Model，它指向基本Meta Llama上的Casualllama包装器。我有一种不好的感觉，这不是正确的前进，不塞在引擎盖下发生了其他事情。我在这里错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79531231/custom-forward-method-for-lora-finetuned-llama-8b-using-unsloth</guid>
      <pubDate>Mon, 24 Mar 2025 13:38:48 GMT</pubDate>
    </item>
    <item>
      <title>TABPFN功能选择提高了keyError（f“ [{key}]中的一个都不在[{axis_name}]中</title>
      <link>https://stackoverflow.com/questions/79529836/tabpfn-feature-selection-raises-keyerrorfnone-of-key-are-in-the-axis-nam</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79529836/tabpfn-feature-selection-raises-keyerrorfnone-of-key-are-in-the-axis-nam</guid>
      <pubDate>Sun, 23 Mar 2025 22:59:22 GMT</pubDate>
    </item>
    <item>
      <title>当训练模型使用KAFKA和RDL索引4096训练模型时的错误是否超出了尺寸4096的轴0</title>
      <link>https://stackoverflow.com/questions/79526992/error-when-training-the-model-for-sensor-data-using-kafka-and-rdl-index-4096-is</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79526992/error-when-training-the-model-for-sensor-data-using-kafka-and-rdl-index-4096-is</guid>
      <pubDate>Sat, 22 Mar 2025 05:30:38 GMT</pubDate>
    </item>
    <item>
      <title>DL4J自动编码器用于异常检测：意外结果</title>
      <link>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</guid>
      <pubDate>Thu, 20 Mar 2025 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>高效NETB3模型的准确性非常低，并且在识别脑肿瘤问题方面学习高原</title>
      <link>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</guid>
      <pubDate>Mon, 27 Jan 2025 11:58:17 GMT</pubDate>
    </item>
    <item>
      <title>神经网络精确度低</title>
      <link>https://stackoverflow.com/questions/73780627/neural-network-low-accuracy</link>
      <description><![CDATA[该模型的精度确实很低。这是我第一次写神经网络，所以我真的不知道如何使它变得更好
 将TensorFlow导入为TF
导入matplotlib.pyplot作为PLT
    
#data设置
data = tf.keras.datasets.cifar10


（x_train，y_train），（x_test，y_test）= data.load_data（）
plt.imshow（x_train [0]，cmap = plt.cm.binary）

#normalize数据
x_train = tf.keras.utils.normalize（x_train，axis = 1）
x_test = tf.keras.utils.normalize（x_test，axis = 1）

#building AI模型

型号= tf.keras.models.sequeential（）
Model.Add（tf.keras.layers.flatten（））
model.Add（tf.keras.layers.dense（128，激活= tf.nn.relu））
model.Add（tf.keras.layers.dense（128，激活= tf.nn.relu））
model.Add（tf.keras.layers.dense（10，激活= tf.nn.softmax）））



#compile模型
model.compile（优化器=&#39;adam&#39;，
             损失=&#39;Sparse_categorical_crossentropy&#39;，
             指标= [&#39;准确性&#39;]）

plt.show（）
#Train AI模型
model.fit（x_train，y_train，epochs = 3）
 ]]></description>
      <guid>https://stackoverflow.com/questions/73780627/neural-network-low-accuracy</guid>
      <pubDate>Tue, 20 Sep 2022 00:59:44 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的精度非常低</title>
      <link>https://stackoverflow.com/questions/65777704/getting-a-very-low-accuracy-with-neural-network</link>
      <description><![CDATA[我正在尝试使用keras在CIFAR-10数据集上实现ANN
但是由于某种原因，我不知道我只能获得10％的准确性？
我分别使用了5个隐藏层IWTH 8,16,32,64,128神经元。
 这是jupyter笔记本 的链接
  model = sequention（）
model.Add（密集（单位= 8，activation =&#39;sigmoid&#39;，input_dim = x.shape [1]）））））
model.Add（密集（单位= 16，activation =&#39;Sigmoid&#39;））））
model.Add（密集（单位= 32，activation =&#39;Sigmoid&#39;））
model.Add（密集（单位= 64，activation =&#39;sigmoid&#39;））
model.Add（密集（单位= 128，activation =&#39;Sigmoid&#39;））
model.Add（密度（单位= 10，activation =&#39;softmax&#39;））

model.compile（loss =&#39;accorical_crossentropy&#39;，importizer =&#39;adam&#39;，量表= [&#39;fecicy&#39;]）

model.fit（x_train，y_train，epochs = 1000，batch_size = 500）
 ]]></description>
      <guid>https://stackoverflow.com/questions/65777704/getting-a-very-low-accuracy-with-neural-network</guid>
      <pubDate>Mon, 18 Jan 2021 15:44:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络不确定的乳腺癌数据集</title>
      <link>https://stackoverflow.com/questions/52358505/neural-network-undefitting-breast-cancer-dataset</link>
      <description><![CDATA[我正在尝试在乳腺癌数据集上创建一个用于二进制分类的神经网络：
  https://wwwww.kaggle.com/uciml/uciml/uciml/breast-cancer-cancer-wisconsin-data-data-data-data-data-data 
我的神经网络由3层组成（不包括输入层）：

 第一层：6个带有Tanh激活的神经元。

 第二层：6个带有Tanh激活的神经元。

 最终层：1个神经元，带有Sigmoid激活。


不幸的是，我在训练示例中仅获得约44％的精度，在测试示例中的精度约为23％。
这是我的python代码：
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT

data = pd.read_csv（&#39;data.csv; quot;）
data = data.drop（[&#39;id&#39;]，轴= 1）
data = data.drop（data.columns [31]，轴= 1）
data = data.replace（{&#39;m&#39;：1，&#39;b&#39;：0}）

x =数据
x = x.drop（[&#39;诊断&#39;]，轴= 1）
x = np.array（x）

x_mean = np.mean（x，axis = 1，keepdims = true）
x_std = np.std（x，axis = 1，keepdims = true）
x_n =（x -x_mean） / x_std
y = np.array（数据[&#39;诊断&#39;]）
y = y.Reshape（569，1）
M = 378
y_train = y [：m，：]
y_test = y [m：，：]

x_train = x_n [：M，：]
x_test = x_n [m :，：]

Def Sigmoid（Z）：
  返回1 /（1 + np.exp（-z））

def dsigmoid（z）：
  返回np.multiply（z，（1 -z））

def tanh（z）：
  返回（np.exp（z）-np.exp（-z）） /（np.exp（z） + np.exp（-z））

def dtanh（z）：
  返回1 -np.square（tanh（z））

def成本（a，y）：
  m = y.形[0]
  返回 - （1.0/m） *np.sum（np.dot（y.t，np.log（a）） + np.dot（（（1 -y）.t，np，np.log（1 -a）））

def train（x，y，型号，epocs，a）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  费用= []
  
  对于我的范围（EPOC）：
    
    ＃前传播

    z1 = np.dot（x，w1） + b1
    a1 = tanh（z1）

    z2 = np.dot（a1，w2） + b2
    a2 = tanh（z2）

    z3 = np.dot（a2，w3） + b3
    A3 = Sigmoid（Z3）
    
    costs.append（成本（A3，y））

    #back繁殖
    
    dz3 = z3 -y
    d3 = np.multiply（dz3，dsigmoid（z3））
    dw3 = np.dot（a2.t，d3）
    db3 = np.sum（d3，axis = 0，keepdims = true）

    d2 = np.multiply（np.dot（d3，w3.t），dtanh（z2））
    dw2 = np.dot（a1.t，d2）
    db2 = np.sum（d2，轴= 0，keepdims = true）

    d1 = np.multiply（np.dot（d2，w2.t），dtanh（z1））
    dw1 = np.dot（x.T，d1）
    db1 = np.sum（d1，axis = 0，keepdims = true）

    W1  -  =（A / M） * DW1
    W2- =（A / M） * DW2
    w3- =（a / m） * dw3

    B1  -  =（A / M） * DB1
    b2  -  =（a / m） * db2
    B3  -  =（A / M） * DB3
    
  cache = {&#39;w1&#39;：w1，&#39;w2&#39;：w2，&#39;w3&#39;：w3，&#39;b1&#39;：b1&#39;：b1，&#39;b2&#39;：b2，&#39;b3&#39;：b3}
  返回缓存，成本

np.random.seed（0）

型号= {&#39;w1&#39;：np.random.rand（30，6） * 0.01，&#39;w2&#39;：np.random.rand（6，6） * 0.01，&#39;w3&#39;：np.random.rand（6，1） &#39;b3&#39;：np.random.rand（1，1）}

型号，成本=火车（x_train，y_train，型号，1000，0.1）

plt.plot（[i在范围内（1000）]，费用）
打印（费用[999]）
plt.show（）



def预测（x，y，模型）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  z1 = np.dot（x，w1） + b1
  a1 = tanh（z1）

  z2 = np.dot（a1，w2） + b2
  a2 = tanh（z2）

  z3 = np.dot（a2，w3） + b3
  A3 = Sigmoid（Z3）
  
  m = a3.形[0]
  y_predict = np.zeros（（M，1））
  
  对于我的范围（m）：
    y_predict = 1如果a3 [i，0]＆gt; 0.5其他0
  返回y_predict
 ]]></description>
      <guid>https://stackoverflow.com/questions/52358505/neural-network-undefitting-breast-cancer-dataset</guid>
      <pubDate>Sun, 16 Sep 2018 21:24:54 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的数据增强</title>
      <link>https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch</link>
      <description><![CDATA[我对Pytorch中执行的数据增加有些困惑。现在，据我所知，当我们执行数据增强时，我们将保留原始数据集，然后添加其他版本（翻转，裁剪等）。但这似乎并没有发生在Pytorch中。据我从参考文献中理解时，当我们在pytorch中使用 data.transforms 时，它将它们一一应用它们。因此：
  data_transforms = {
    &#39;train&#39;：transforms.compose（[
        transforms.randomresizedcrop（224），
        transforms.randomhorizo​​ntalflip（），
        transforms.totensor（），
        transforms.normize（[0.485，0.456，0.406]，[0.229，0.224，0.225]）
    ]），，
    &#39;val&#39;：transforms.compose（[
        转换式Resize（256），
        transforms.centercrop（224），
        transforms.totensor（），
        transforms.normize（[0.485，0.456，0.406]，[0.229，0.224，0.225]）
    ]），，
}
 
在这里，对于培训，我们首先是随机裁剪图像并将其调整为Shape （224,224）。然后，我们将这些（224,224）图像进行水平翻转。因此，我们的数据集现在仅包含水平翻转的图像，因此在这种情况下，我们的原始图像丢失了。
我对吗？这理解是正确的吗？如果不是，那么我们在上面的此代码中（取自官方文档）将Pytorch告诉原始图像并将其调整到预期形状（224,224）？]]></description>
      <guid>https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch</guid>
      <pubDate>Fri, 03 Aug 2018 17:51:49 GMT</pubDate>
    </item>
    <item>
      <title>在GridSearchCV中得分XGBoost</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我正在尝试使用XGBoost首次分析数据。我想使用GridSearchCV找到最佳参数。我想最大程度地减少均方根错误，为此，我使用了“ rmse”。作为eval_metric。但是，网格搜索的评分没有这样的度量。我在此网站上发现“ neg_mean_squared_error”这样做一样，但是我发现这给我与RMSE不同的结果。当我计算“ neg_mean_squared_error”的绝对值的根目录时，我的值约为8.9，而不同的功能使我的RMSE约为4.4。
我不知道怎么了，或者如何获得这两个功能同意/给出相同的值？
由于这个问题，我遇到了错误的值，因为 best_params _ ，它比我最初开始调音的某些值更高的RMSE。
如何在网格搜索中获得RMSE的分数以及为什么我的代码给出不同的值？
  def modelfit（alg，trainx，trainy，usetraincv = true，cv_folds = 10，armond_stopping_rounds = 50）：
    如果用UsetrainCV：
        xgb_param = alg.get_xgb_params（）
        xgtrain = xgb.dmatrix（trainx，label = Trainy）
        cvresult = xgb.cv（xgb_param，xgtrain，num_boost_round = alg.get_params（）[&#39;n_estimators&#39;]，nfold = cv_folds，
                          指标=&#39;rmse&#39;，ropard_stopping_rounds = armon_stopping_rounds）
        alg.set_params（n_estimators = cvresult.shape [0]）

    ＃将算法适合数据
    alg.fit（Trainx，Trainy，eval_metric =&#39;rmse&#39;）

    ＃预测训练集：
    dtrain_predictions = alg.predict（trainx）
    ＃dtrain_predprob = alg.predict_proba（Trainy）[：，1]
    打印（dtrain_predictions）
    print（np.sqrt（mean_squared_error（trainy，dtrain_predictions））））））））

    ＃打印模型报告：
    打印（“ \ nmodel报告”）
    打印（&#39;rmse：％.4G;％np.sqrt（量学

 param_test2 = {
 &#39;max_depth&#39;：[6,7,8]，
 &#39;min_child_weight&#39;：[2,3,4]
}

grid2 = gridSearchCV（估算= xgb.xgb.xgbregressor（Learning_rate = 0.1，n_estimators = 2000，max_depth = 5，
 min_child_weight = 2，gamma = 0，subsampe = 0.8，colsample_bytree = 0.8，
 objective =&#39;reg：linear&#39;，nthread = 4，scale_pos_weight = 1，andury_state = 4），
 param_grid = param_test2，评分=&#39;neg_mean_squared_error&#39;，n_jobs = 4，iid = false，cv = 10，冗长= 20）
grid2.fit（x_train，y_train）
＃best_estimator的平均交叉验证得分
print（grid2.best_params_，np.sqrt（np.abs（grid2.best_score_）））），print（np.sqrt（np.abs（grid2.score2.score（x_train，y_train，y_train））））
modelfit（grid2.best_estimator_，x_train，y_train）
print（np.sqrt（np.abs（grid2.score）（x_train，y_train）））））））））））
 ]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>对神经网络的输入类型很重要？ [关闭]</title>
      <link>https://stackoverflow.com/questions/37438078/does-type-of-input-to-the-neural-network-matter</link>
      <description><![CDATA[我正在做视频分类。
我有一个神经网络，我必须使用视频（图像组）训练。
我可以选择从几个选项更改网络输入的形状。
在所有情况下，我都认为网络体系结构（排列和层数）＆amp;学习参数（LR/Decay/正则化/等）是恒定的。
例如，我可以选择将网络输入作为以下内容之一。

  batch_size x（no_of_imgs*no_of_channels）x高度x宽度{3尺寸输入} 

  batch_size x no_of_imgs x no_of_channels x高度x宽度{4尺寸输入} 

  batch_size x no_of_channels x no_of_imgs x高度x宽度{4尺寸输入} 


输入形状将如何影响网络的准确性？]]></description>
      <guid>https://stackoverflow.com/questions/37438078/does-type-of-input-to-the-neural-network-matter</guid>
      <pubDate>Wed, 25 May 2016 13:04:48 GMT</pubDate>
    </item>
    </channel>
</rss>