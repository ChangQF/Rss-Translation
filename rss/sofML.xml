<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 18 May 2024 12:25:05 GMT</lastBuildDate>
    <item>
      <title>我该如何解决python中的属性错误？</title>
      <link>https://stackoverflow.com/questions/78499207/how-can-i-solve-attributeerror-in-python</link>
      <description><![CDATA[我正在从事一个数据科学项目，但遇到了瓶颈
我使用了 append 函数，但它不起作用。
这里给出了一个错误：
AttributeError: &#39;DataFrame&#39; 对象没有属性 &#39;concat&#39;
def recommend(name, cosine_similarities = cosine_similarities):
# 创建一个列表来放置顶级餐厅
recommend_restaurant = []

# 查找输入的酒店的索引
idx = indices[indices == name].index[0]

# 查找具有相似余弦相似值的餐厅，并从最大数字开始排序
score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)

# 提取具有相似余弦相似值的前 30 家餐厅索引
top30_indexes = list(score_series.iloc[0:31].index)

# 排名前 30 的餐厅名称
for each in top30_indexes:
recommend_restaurant.append(list(df_percent.index)[each])

# 创建新数据集以显示类似餐厅
df_new = pd.DataFrame(columns=[&#39;cuisines&#39;, &#39;Mean Rating&#39;, &#39;cost&#39;])

# 创建排名前 30 的类似餐厅及其部分列
for each in recommend_restaurant:
df_new = df_new.concat(pd.DataFrame(df_percent[[&#39;cuisines&#39;,&#39;Mean Rating&#39;, &#39;cost&#39;]][df_percent.index == each].sample()))

# 删除同名餐厅，仅按最高评分对排名前 10 的餐厅进行排序
df_new = df_new.drop_duplicates(subset=[&#39;cuisines&#39;,&#39;平均评分&#39;, &#39;cost&#39;], keep=False)
df_new = df_new.sort_values(by=&#39;平均评分&#39;, accending=False).head(10)

print(&#39;与 %s 相似且有类似评价的前 %s 家餐厅：&#39; % (str(len(df_new)), name))

return df_new
]]></description>
      <guid>https://stackoverflow.com/questions/78499207/how-can-i-solve-attributeerror-in-python</guid>
      <pubDate>Sat, 18 May 2024 09:14:08 GMT</pubDate>
    </item>
    <item>
      <title>计算错误的神经网络比正确计算的神经网络更好</title>
      <link>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</link>
      <description><![CDATA[我设计了自己的神经网络，发现了一个错误。在反向传播过程中，我没有将 Z 值插入激活函数的导数中，而是插入了 A 值。结果是，当我使用 A 值时，神经网络的学习速度比使用 Z 值的计算更快、更稳定。计算应该是错误的。那么为什么它效果更好，产生更好的结果和更稳定的结果呢？
Z=x×w+b A=activationfunction(Z)
计算错误，但结果更好：
dA/dZ=activationfunction_derivative(A)
delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
计算正确，但结果更差：
dA/dZ=activationfunction_derivative(Z)
代码错误，但结果更好：
def leaky_relu(self, x, alpha=0.01):
return np.where(x &gt; 0, x, alpha * x)

def leaky_relu_derivative(self, x, alpha=0.01):
返回 np.where(x &gt; 0, 1, alpha)
def linear(self, x):
返回 x

def linear_derivative(self):
返回 1

def forward_propagation(self, X):
self.activations = []
activation = X
self.activations.append(activation)
for weight, bias in zip(self.weights[:-1], self.biases[:-1]):
activation = self.leaky_relu(np.dot(activation, weight) + bias)
self.activations.append(activation)
activation = self.linear(np.dot(activation, self.weights[-1]) + self.biases[-1])
self.activations.append(activation)
返回激活

def msnq(self, y_true, y_pred):
返回 np.mean(np.square(y_true - y_pred))

def backpropagation(self, y_true, t):
deltas = [] 
error = y_true - self.activations[-1]
delta = error * self.linear_derivative() 
deltas.append(delta) 
for i in range(len(self.activations) - 2, 0, -1):
delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
deltas.append(delta) 
]]></description>
      <guid>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</guid>
      <pubDate>Fri, 17 May 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 如何将数据清理纳入训练模型中</title>
      <link>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</link>
      <description><![CDATA[我一直在尝试寻找这个问题的答案，但没有找到任何与之相关的内容。
问题是，如果我清理数据并将中值归入 NaN 值，我是否应该以某种方式将其合并到将用于测试数据的模型中。换句话说，我的测试数据是否也需要清理和估算，或者训练会解决这个问题吗？我想说它需要被合并，因为否则 NaN 值会破坏模型，而且任何偏度都不会得到解决。
特别是：
用中位数替换 NaN：
data = data.fillna(data.median())

使用分位数变换处理偏度，使每个特征遵循正态分布（以下仅举一例）。
qualtile_transformer = QuantileTransformer(output_distribution=&#39;正常&#39;, random_state=0&#39;)
数据[&#39;feat_0&#39;] = quantile_transformer.fit_transform(数据[&#39;feat_0&#39;].values.reshape(-1,1)).flatten()

型号：
from sklearn. Linear_model 导入 LinearRegression
Linear_regr = 线性回归()
Linear_regr.fit(Xtrain,Ytrain)

预测：
# 使用测试集进行预测
Ypred = Linear_regr.predict(Xtest)

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</guid>
      <pubDate>Fri, 17 May 2024 20:47:39 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中为我的标签应用程序处理数据库锁</title>
      <link>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</link>
      <description><![CDATA[我希望我的应用程序的用户检索一些要标记的数据。在第一个版本中，我没有实现锁定，因此多个用户可以同时访问相同的数据，因此第二个版本会覆盖第一个版本的标签。
我正在使用 python fastAPI sqlite 后端。
我最初想出了为标签添加“is-being-labelized”值的想法，以便下一个提议的数据不一样。我不喜欢它，因为我不知道如何处理用户在没有标记数据的情况下退出应用程序（或其他应用程序）的情况。目前，我最好的方法是添加一个包含检索时间时间戳的列，并实现一个逻辑，假设检索后 30 秒，我们检查标签是否不再是 None。如果它仍然是 None （意味着该人没有标记），我们删除时间戳的值。我也不完全高兴，因为它不能处理人们冥想然后回来标记数据的情况。
您有更好的建议吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</guid>
      <pubDate>Fri, 17 May 2024 20:19:31 GMT</pubDate>
    </item>
    <item>
      <title>我用自己的数据集训练yolo模型但没有测试结果</title>
      <link>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</link>
      <description><![CDATA[我正在使用 Yolov3 模型以及从 Kaggle 收到的数据集来训练模型。模型训练已完成，我将新权重添加到备份文件夹中。我运行了我训练过的一种水果进行测试，但没有发生对象检测。同一图像显示为 Prediction.jpg。训练看起来不错，但我不明白为什么它不能检测物体。请帮助我。
火车站代码：
./darknet探测器列车 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights

测试终端代码：
./darknet探测器测试 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out预测.jpg

我设置并编辑了 obj.data、obj.names 和 yolov3.cfg 文件。
我有 3 个类别：苹果、香蕉和橙子。我已经根据3个类在cfg文件中正确设置了filter和class值等值。
cfg 文件
[网]
# 测试
批次=64
细分=1
＃ 训练
细分=16
宽度= 608
高度=608
频道=3
动量=0.9
衰减=0.0005
角度=0
饱和度=1.5
曝光=1.5
色调=0.3

学习率=0.001
烧入=1000
max_batches = 6000 # 类数 * 2000
政策=步骤
步骤=3600,4800 # max_batches num %80, %90
尺度=.1,.1

数据集中除了.jpg图片外，还有yolo格式的同名.txt文件。
在此处输入图像描述文件图像
包含所有图像路径的 train.txt 和 test.txt 文件也已准备就绪。
当我在终端中运行测试命令时，它可以工作，但图片看起来相同，没有检测对象的边界框。我确定我已经安装了 Opencv。我正在使用 macOS。为什么它没有检测到它？请有人帮忙。我多次通过 make clean 清理暗网，并通过 make opencv = 1 运行它，但结果没有改变。
[yolo]参数：iou损失：mse（2），iou_norm：0.75，obj_norm：1.00，cls_norm：1.00，delta_norm：1.00，scale_x_y：1.00
总 BFLOPS 137.613
平均输出 = 1052318
正在从 /Users/melisabagcivan/darknet/backup/yolov3_final.weights 加载权重...
 看过 64 个，训练过：32013 个 K 图像（500 Kilo-batches_64）
完毕！从权重文件加载 107 层
输入图像路径：/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 检测层：82-类型=28
 检测层：94-型=28
 检测层：106-类型=28
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg：预测为 6738.129000 毫秒。

我尝试了很多图像，但它没有在任何图像中绘制方框。我不明白它是否无法检测到它或者我在测试时是否犯了错误。请帮忙，我快疯了。]]></description>
      <guid>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</guid>
      <pubDate>Fri, 17 May 2024 19:21:32 GMT</pubDate>
    </item>
    <item>
      <title>从 Orange 导出的模型在 Orange 中运行良好，但在 Python 中运行不佳</title>
      <link>https://stackoverflow.com/questions/78497427/model-exported-from-orange-works-well-in-orange-but-not-in-python</link>
      <description><![CDATA[我用 Orange 训练了一个机器学习模型，可以非常准确地对狗和猫进行分类。但是，当我将模型导出到 pickle 文件并在 Python 中加载时，无论输入数据如何，它都会一致预测“cat”。
这是我用 python 写的：
导入pickle
从 PIL 导入图像
将 numpy 导入为 np

modello = &#39;modelli/catDogsLogisticRegression.pkcls&#39;

def load_model_from_pickle(modello):
    尝试：
        使用 open(modello, &#39;rb&#39;) 作为 file_pickle：
            模型 = pickle.load(file_pickle)
            返回模型
    除了文件未找到错误：
        print(f“文件 {modello} 非 trovato。”)
        返回无

def preprocess_image(image_path):
    # 想象中的卡里卡
    img = Image.open(图像路径)
    # 在 scala di grigi 中进行想象和转换
    img = img.resize((32, 64)).convert(&#39;L&#39;)
    # 将 l&#39;immagine 转换为 un array numpy 并将 Ridimensiona 转换为 un unico vettare
    img_array = np.array(img).reshape(1, -1)
    返回img_array

# 使用的意义
加载模型 = load_model_from_pickle(modello)
如果加载模型：
    print(&quot;成功模型&quot;)
    # 模型用途
    # Carica e pre-elabora un&#39;immagine
    image_path = &#39;甘蔗.jpg&#39;
    新数据 = 预处理图像（图像路径）
    # Prevedere la classe del nuovo esempio
    Predicted_class = returned_model.predict(new_data)[0]
    print(“Prevista 类：”, &#39;Gatto&#39; if Predicted_class == 0 else &#39;Cane&#39;)
别的：
    print(“模型错误。”)

你知道模型在 Python 中表现不同的原因吗？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78497427/model-exported-from-orange-works-well-in-orange-but-not-in-python</guid>
      <pubDate>Fri, 17 May 2024 18:43:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用决策树算法和bert算法对文本进行分类</title>
      <link>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-algorithm-to-classify-a-text</link>
      <description><![CDATA[我想集成并使用 BERT 和决策树两种算法进行文本分类，因此我需要该领域的指导和帮助。
如果有人有这个领域的源代码或文章，请提供给我。或者即使朋友有更好的建议将 BERT 算法与任何其他算法结合起来]]></description>
      <guid>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-algorithm-to-classify-a-text</guid>
      <pubDate>Fri, 17 May 2024 17:44:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在流模式下分割拥抱脸部数据集而不将其加载到内存中？</title>
      <link>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</link>
      <description><![CDATA[我正在使用 Hugging Face 数据集，我需要将数据集拆分为训练集和验证集。我的主要要求是数据集应该以流模式处理，因为我不想将整个数据集加载到内存中。
从数据集导入load_dataset，DatasetDict

# 从 Hugging Face 加载数据集
数据集 = load_dataset(&#39;小队&#39;, split=&#39;训练&#39;)

# 将数据集分为训练集和验证集
# 指定测试集（验证集）的分数
train_val_split = dataset.train_test_split(test_size=0.1)

# 提取训练和验证数据集
train_dataset = train_val_split[&#39;train&#39;]
val_dataset = train_val_split[&#39;测试&#39;]

# 打印数据集的大小
print(f&quot;训练集大小: {len(train_dataset)}&quot;)
print(f&quot;验证集大小: {len(val_dataset)}&quot;)

# 如果需要的话保存数据集
# train_dataset.save_to_disk(&#39;路径/到/train_dataset&#39;)
# val_dataset.save_to_disk(&#39;路径/到/val_dataset&#39;)

是否有一种方法可以在流模式下分割 Hugging Face 数据集？对我的代码的任何建议或改进将不胜感激。
参考文献：

https ://discuss.huggingface.co/t/how-to-split-a-dataset-into-train-test-and-validation/1238
https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/21
https://discuss.huggingface .co/t/possible-to-stream-and-create-new-splits/67214
https://huggingface.co/docs/datasets/v1.11.0 /splits.html
https://discuss.huggingface.co/t/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-memory /87205
]]></description>
      <guid>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</guid>
      <pubDate>Fri, 17 May 2024 17:18:18 GMT</pubDate>
    </item>
    <item>
      <title>我的逻辑回归机器学习模型有问题</title>
      <link>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model</link>
      <description><![CDATA[我的模型精度很差。此数据取自 https://archive.ics.uci .edu/dataset/15/breast+cancer+wisconsin+original 显示逻辑回归模型的准确度为 96%，所以问题确实出在我的模型中。我在 R 中构建了以下模型。
# 导入数据集
tumor_study &lt;- read.csv(“breast-cancer-wisconsin.data”, header = FALSE, na.strings = “NA”)

# 添加列名
特征&lt;-c(“id_number”，“ClumpThickness”，“Uniformity_CellSize”，
              “Uniformity_CellShape”、“边缘粘附”、
              “SingleEpithelial_CellSize”、“BareNuclei”、“Bland_Chromatin”、
              “Normal_Nucleoli”、“Mitoses”、“Class”）

colnames(tumor_study) &lt;- 特征

# 清洗数据
# 删除第一列（id_number）
肿瘤研究 &lt;- 肿瘤研究[,-1]

# 转换“?” BareNuclei 列中为 NA，然后为数字
tumor_study$BareNuclei[tumor_study$BareNuclei == &quot;?&quot;] &lt;- NA
tumor_study$BareNuclei &lt;- as.numeric(tumor_study$BareNuclei)

# 删除 BareNuclei 中缺失值的行
tumor_study &lt;-tumor_study[!is.na(tumor_study$BareNuclei),]

# 将类转换为因子
tumor_study$Class &lt;- 因子(tumor_study$Class, level = c(2, 4), labels = c(“良性”, “恶性”))

# 将数据集分为训练集和测试集
库（caTools）
设置.种子(123)
split &lt;-sample.split(tumor_study$Class, SplitRatio = 0.8)
Training_set &lt;-tumor_study[split == TRUE,]
test_set &lt;-tumor_study[split == FALSE,]

# 应用特征缩放
训练集[, 1:9] &lt;- 比例(训练集[, 1:9])
test_set[, 1:9] &lt;- 比例(test_set[, 1:9])

# 构建逻辑回归模型
分类器 &lt;- glm(公式 = Class ~ ., family = 二项式, data = Training_set)

# 预测训练集的概率
prob_y_train &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = Training_set[,-10]）
Predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5,“良性”,“恶性”)

# 使用 test_set 进行预测
prob_y_test &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = test_set[,-10]）
Predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5,“良性”,“恶性”)

# 使用混淆矩阵检查准确性
cm_test &lt;- 表(test_set[,10], Predicted_y_test)
打印（厘米_测试）

## 如果你检查准确度...它接近 2%

如何找出模型中的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model</guid>
      <pubDate>Fri, 17 May 2024 06:43:30 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 GitHub 上分享我的自我项目吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</link>
      <description><![CDATA[我一直在考虑是否应该在 GitHub 上分享我自己的项目。这些项目主要涉及机器学习，重点关注回归和分类任务。虽然我已经付出了努力，但我不确定是否值得清理代码并上传它。展示这些小项目实际上是否有益，或者最终只是浪费时间？]]></description>
      <guid>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</guid>
      <pubDate>Wed, 15 May 2024 15:26:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将其参数减少为嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（及其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，并将经过训练的 CNN 参数输入到 VAE 模型的编码器输入中，最后将参数向量重建为 CNN 模型参数。
提前致谢！
这是 CNN 模型：
类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def 前向（自身，x）：
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, 训练=self.training)
        x = self.fc2(x)
        返回 F.log_softmax(x)

下面的代码用于VAE：
类 VAE(nn.Module):
    def __init__(self, image_channels=1, h_dim=1024, z_dim=32):
        超级（VAE，自我）.__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
            ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, 步长=2),
            ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, 步长=2),
            ReLU(),
            nn.Conv2d(128, 256, kernel_size=4, 步长=2),
            ReLU(),
            展平()
        ）
        
        self.fc1 = nn.Linear(h_dim, z_dim)
        self.fc2 = nn.Linear(h_dim, z_dim)
        self.fc3 = nn.Linear(z_dim, h_dim)
        
        self.decoder = nn.Sequential(
            展开（），
            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),
            ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=5, 步长=2),
            ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=6, 步长=2),
            ReLU(),
            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
            nn.Sigmoid(),
        ）
        
    def 重新参数化(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        # 返回 torch.normal(mu, std)
        esp = torch.randn(*mu.size())
        z = mu + std * esp
        返回 z
    
    def 瓶颈（自身，h）：
        mu, logvar = self.fc1(h), self.fc2(h)
        z = self.reparameterize(mu, logvar)
        返回 z、mu、logvar

    def 编码（自身，x）：
        h = self.encoder(x)
        z, mu, logvar = self.bottleneck(h)
        返回 z、mu、logvar

    def 解码（自身，z）：
        z = self.fc3(z)
        z = self.decoder(z)
        返回 z

    def 前向（自身，x）：
        z、mu、logvar = self.encode(x)
        z = self.decode(z)
        返回 z、mu、logvar
]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>在 CUDA GPU 上运行 Pytorch 量化模型</title>
      <link>https://stackoverflow.com/questions/69718379/running-pytorch-quantized-model-on-cuda-gpu</link>
      <description><![CDATA[我很困惑是否可以在 CUDA 上运行 int8 量化模型，或者只能使用 fakequantise 在 CUDA 上训练量化模型以部署在另一个后端（例如 CPU）上。
我想使用实际的 int8 指令而不是 FakeQuantized float32 指令在 CUDA 上运行模型，并享受效率提升。奇怪的是，Pytorch 文档对此没有具体说明。如果可以使用不同的框架（例如 TensorFlow）在 CUDA 上运行量化模型，我很想知道。
这是准备量化模型的代码（使用训练后量化）。该模型是带有 nn.Conv2d 和 nn.LeakyRelu 以及 nn.MaxPool 模块的普通 CNN：
model_fp = torch.load(models_dir+net_file)

model_to_quant = copy.deepcopy(model_fp)
model_to_quant.eval()
model_to_quant = quantize_fx.fuse_fx(model_to_quant)

qconfig_dict = {“”: torch.quantization.get_default_qconfig(&#39;qnnpack&#39;)}

model_prepped = quantize_fx.prepare_fx(model_to_quant, qconfig_dict)
model_prepped.eval()
model_prepped.to(device=&#39;cuda:0&#39;)

train_data = ImageDataset(img_dir, train_data_csv, &#39;cuda:0&#39;)
train_loader = DataLoader(train_data,batch_size=32,shuffle=True,pin_memory=True)

对于 i，枚举（train_loader）中的（输入，_）：
    如果我&gt; 1：中断
    print(&#39;batch&#39;, i+1, end=&#39;\r&#39;)
    输入 = input.to(&#39;cuda:0&#39;)
    model_prepped（输入）

这实际上量化了模型：
model_quantized = quantize_fx.convert_fx(model_prepped)
model_quantized.eval()

这是在 CUDA 上运行量化模型的尝试，并引发 NotImplementedError，当我在 CPU 上运行它时，它工作正常：
model_quantized = model_quantized.to(&#39;cuda:0&#39;)
对于 train_loader 中的 i、_：
    输入 = input.to(&#39;cuda:0&#39;)
    输出= model_quantized（输入）
    打印（输出，输出形状）
    休息

这是错误：
回溯（最近一次调用最后一次）：
  文件“/home/adam/Desktop/thesis/Ship Detector/quantization.py”，第54行，在&lt;模块&gt;中。
    输出= model_quantized（输入）
  文件“/home/adam/.local/lib/python3.9/site-packages/torch/fx/graph_module.py”，第 513 行，位于wrapped_call 中
    引发 e.with_traceback(无)
NotImplementedError：无法使用来自“QuantizedCUDA”后端的参数运行“quantized::conv2d.new”。
这可能是因为该后端不存在该运算符，或者在选择性/自定义构建过程中省略了该运算符（如果使用自定义构建）。
如果您是在移动设备上使用 PyTorch 的 Facebook 员工，请访问 https://fburl.com/ptmfixes 了解可能的解决方案。
“quantized::conv2d.new”仅适用于以下后端：[QuantizedCPU、BackendSelect、Named、ADInplaceOrView、AutogradOther、AutogradCPU、AutogradCUDA、AutogradXLA、UNKNOWN_TENSOR_TYPE_ID、AutogradMLC、Tracer、Autocast、Batched、VmapMode]。
]]></description>
      <guid>https://stackoverflow.com/questions/69718379/running-pytorch-quantized-model-on-cuda-gpu</guid>
      <pubDate>Tue, 26 Oct 2021 06:30:58 GMT</pubDate>
    </item>
    <item>
      <title>Kmeans 算法的特征缩放</title>
      <link>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</link>
      <description><![CDATA[我知道下定义的 KMeans 算法需要特征缩放
sklearn.cluster.KMeans
我的问题是，在使用 KMeans 之前是否需要手动完成，或者 KMeans 会自动执行特征缩放？如果是自动的，请告诉我它在 KMeans 算法中指定的位置，因为我无法在此处的文档中找到它：
https://scikit-learn.org/stable /modules/ generated/sklearn.cluster.KMeans.html
顺便说一句，人们说 Kmeans 本身负责特征缩放。]]></description>
      <guid>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</guid>
      <pubDate>Thu, 15 Aug 2019 09:27:46 GMT</pubDate>
    </item>
    <item>
      <title>F1 分数 vs ROC AUC</title>
      <link>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</link>
      <description><![CDATA[我有以下 2 个不同案例的 F1 和 AUC 分数

&lt;块引用&gt;
  模型 1：精度：85.11 召回率：99.04 F1：91.55 AUC：69.94
模型 2：精度：85.1 召回率：98.73 F1：91.41 AUC：71.69

我的问题的主要动机是正确预测正例，即减少假负例（FN）。我应该使用 F1 分数并选择模型 1 还是使用 AUC 并选择模型 2。谢谢 ]]></description>
      <guid>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</guid>
      <pubDate>Thu, 25 May 2017 04:14:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 NLP 和 python 从文档中提取特定内容，例如姓名或出生日期？ [关闭]</title>
      <link>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</link>
      <description><![CDATA[我想从文档（例如简历）中提取非常具体的内容，例如姓名、地址和出生日期。假设我有 1000 个此类文档，我想使用机器学习和自然语言处理将其自动化。最好是Python。
我怎样才能做到这一点？或者我从哪里开始？
更新：我知道 NER，但我希望从可以加载到 Excel 或其他内容的文档中提取非常具体的信息。
示例：我想从项目报告中提取项目的主题、团队成员姓名和任期。]]></description>
      <guid>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</guid>
      <pubDate>Wed, 22 Jun 2016 11:52:43 GMT</pubDate>
    </item>
    </channel>
</rss>