<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 03 Jan 2025 01:16:16 GMT</lastBuildDate>
    <item>
      <title>检测页面中的图像[关闭]</title>
      <link>https://stackoverflow.com/questions/79324425/detect-image-in-a-page</link>
      <description><![CDATA[我正在构建一个项目，我需要知道文档中哪些页面包含图像。我不想为此使用基于 llm 的视觉模型。请给我一些解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79324425/detect-image-in-a-page</guid>
      <pubDate>Thu, 02 Jan 2025 16:48:36 GMT</pubDate>
    </item>
    <item>
      <title>WCSS 不会持续下降</title>
      <link>https://stackoverflow.com/questions/79324000/wcss-not-decreasing-constantly</link>
      <description><![CDATA[您好，我的 k-means 算法在此数据集上存在问题：
https://www.kaggle.com/datasets/youssefaboelwafa/clustering-penguins-species/data
我尝试从头开始实现它。似乎一切都正常，但当使用肘部法找到最佳 k 时，我得到了错误的结果：
k = 1 到 10 的第一个示例

k = 1 到 10 的第二个示例

但从数学上讲，当聚类质心收敛时，WCSS 应该总是减少，对吗？如您所见，在我实现的版本中并没有发生这种情况。总是有小的“颠簸”。
我尝试将我的结果与 sklearn 的结果进行比较：
k = 1 到 10 的第一个示例 (SKLEARN)

k = 1 到 10 的第二个示例 (SKLEARN)

如您所见，WCSS 不断减少。不过，WCSS 值要高得多。我不知道为什么。
这是我的笔记本：
https://github.com/Orivex/Cluster-Penguin-Species/blob/master/KMeansClustering.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/79324000/wcss-not-decreasing-constantly</guid>
      <pubDate>Thu, 02 Jan 2025 14:23:01 GMT</pubDate>
    </item>
    <item>
      <title>请求合作伙伴/开发人员完成复杂的乐透预测算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79323951/request-for-partner-developer-to-finish-complex-lotto-prediction-algorithm</link>
      <description><![CDATA[各位开发者大家好，
我正在寻找合作伙伴或开发者同事，共同完成一项旨在对我国彩票系统进行逆向工程的算法。在发现彩票是伪随机生成的之后，我感到有必要开发一种算法来揭示其隐藏的模式。
我取得了重大进展，包括可视化数据、执行相关性分析、特征工程，甚至尝试机器学习技术。然而，我遇到了障碍，我相信，有了正确的合作，我们可以破解密码，完成我们已经开始的工作。
当前进展：
数据已经整理和清理。
我已经实现了 80% 的距离相关性，通过进一步调整，我将其提高到了 82%。
我的线性相关性也达到了 84%。
挑战：尽管取得了进步，但到目前为止，我应用的机器学习模型还没有提供我想要的结果，我正处于需要新见解或新方法的阶段。
我正在寻找：
在数据科学、统计分析和机器学习方面具有专业知识的合作伙伴。
愿意接受高潜在回报挑战的人。
具有逆向工程、预测模型或乐透系统是一个加分项，但不是必需的。
如果您愿意接受挑战并相信我们可以成功，那么让我们一起合作并共同完成这个项目。如果我们成功了，回报可能是巨大的！
在我的最终预测模型中，我试图获得小于 10 的 mae，但我得到的是超过 100 万的 mae]]></description>
      <guid>https://stackoverflow.com/questions/79323951/request-for-partner-developer-to-finish-complex-lotto-prediction-algorithm</guid>
      <pubDate>Thu, 02 Jan 2025 14:05:47 GMT</pubDate>
    </item>
    <item>
      <title>LSTM模型预测不会随着输入的不同而改变</title>
      <link>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</link>
      <description><![CDATA[我正在 PyTorch 中实现一个 LSTM 模型，以预测股票的收盘价在接下来的 5 分钟和 10 分钟内是上涨还是下跌。
具体来说，我使用了 24 年的 5 分钟数据，包含 19 个特征，每个预测分为一周的时间段（使用 7 种不同的股票）

我面临的问题是，无论如何，LSTM 模型似乎都会预测一个特定值附近的值，以始终将损失降至最低，但损失不会下降太多。
我预先在 torch.tensors 中准备了输入和目标，其中包含 [batch_size、sequence_len、features]（在我的情况下为 [32、2016、19]），将它们标准化为 0 到 1 之间，然后将它们输入到我的 LSTM 模型中，该模型的结构如下：
class MultiInputOutputLSTM(nn.Module):
def __init__(self, input_size, hidden_​​size, num_layers, output_size, dropout, lr, batch_size):
super(MultiInputOutputLSTM, self).__init__()
self.input_size = input_size
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.dropout = dropout
self.batch_size = batch_size
self.loss_list = []
self.accuracy = 0
self.predictions_list = [0]

self.lstm = nn.LSTM(input_size = self.input_size, hidden_​​size = self.hidden_​​size, num_layers = self.num_layers, dropout = self.dropout, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size, bias=True)
self.sigmoid = nn.Sigmoid()
self.criterion = nn.BCEWithLogitsLoss()
self.optimizer = torch.optim.RMSprop(self.parameters(), lr = lr, alpha=0.9, weight_decay=1e-4, influence=0.5)
self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, &#39;min&#39;)

def forward(self, x):

h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_​​size)
c0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_​​size)
lstm_out, _ = self.lstm(x, (h0, c0))
output = self.fc(lstm_out[:, -1, :])

return output

def train_step(self ,x, y):
self.train()
predictions = self.forward(x)

if abs(predictions.detach().cpu().numpy()[0][0] - y.detach().cpu().numpy()[0][0]) &lt; 1e-4 和 abs(predictions.detach().cpu().numpy()[0][1] - y.detach().cpu().numpy()[0][1]) &lt; 1e-4:
self.accuracy += 1
self.predictions_list.append(predictions.detach().cpu().numpy()[0][0])
penalty = torch.mean((predictions-0.5)**2)
loss = self.criterion(predictions, y) + penalty
self.scheduler.step(loss)
self.optimizer.zero_grad()
self.loss_list.append(loss.item())
mean_loss = sum(self.loss_list)/len(self.loss_list)
loss.backward()
torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=6)
self.optimizer.step()
return loss.item(), mean_loss, self.accuracy, predictions.detach().cpu().numpy()[0], y.detach().cpu().numpy()[0]

def test_step(self, input, target):
self.eval()
with torch.no_grad():
output = torch.round(self.forward(inputs))
output = output.detach().cpu().numpy()[0]
target = target.detach().cpu().numpy()[0]
if (outputs == target).all():
accuracy = 1
else:
accuracy = 0
return accuracy, output, target

如果价格上涨，则目标为 1，否则为 0。
超参数为：
input_size = 19
hidden_​​size = 3
num_layers = 5
output_size = 2
lr = 0.001
num_epochs = 5
batch_size = 32
dropout = 0.3

模型在开始时创建预测，然后它们会改变 +- 1e-3、1e-4，因此在训练过程中基本保持不变。
训练测试分割为 85-15%。
以下是我尝试过的列表：
降低或增加学习率（从 0.00001 到 0.1）、输出大小（使用更多和更少的预测）、batch_size（从 1 到 256）、num_layers（1-5）、input_size（使用 1、2、3... 19 个特征）、dropout（0 到 0.5）和 num_epochs（从 1 到 100）。
我尝试过 Adam 优化器，然后是 SDG，然后是 RMSProp 优化器，更改 alpha、动量和 weight_decay。
还尝试过 BCELoss（在 self.forward() 中使用 sigmoid 激活层）、L1Loss、MSELoss、CrossEntropyLoss，最后决定使用惩罚值太接近 0.5 的损失（这没有帮助）非常多，但此时我愿意尝试任何方法。
平均损失从 0.924 下降到 0.889。
正如您所想象的，准确度非常低（从 5.56% 下降到 11.67%）：准确度不为零的唯一原因是纯粹的运气。
我也试图预测数据框收盘列的最后一项与未来的下两个值之间的差异，但......有同样的问题。]]></description>
      <guid>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</guid>
      <pubDate>Thu, 02 Jan 2025 13:06:55 GMT</pubDate>
    </item>
    <item>
      <title>将 onnx 文件编译为 hailo 可执行文件 (.hef) 时出现问题</title>
      <link>https://stackoverflow.com/questions/79323698/trouble-compiling-onnx-file-to-hailo-executable-hef</link>
      <description><![CDATA[按照 Cytron 网站 上的转换教程并运行命令后
hailomz compile yolov5s --ckpt=best.onnx --hw-arch hailo8l --calib-path yolov5/train/images --classes 1 --performance
我收到错误消息：ModuleNotFoundError：没有名为“hailo_platform”的模块。
我已完成的操作：

我按照教程复制了每个命令（仅将 hailoDFC 版本号中的 28 更改为 29）
已安装 Hailo model zoo 和 hailo dataflow 编译器
我已检查安装是否正确更正并以不同的顺序多次重做
我运行了 hailomz 和 hailo -h 并获得了成功的结果
我已经在干净的 Ubuntu 22.04 WSL 上完成了安装
训练数据的路径是正确的 yolov5/train/images
仅运行不带参数的 hailomz compile 会产生相同的结果
我已经安装了 HailoRT（因为 hailo_platform 是其中的一部分）并运行了相同的命令，错误消息不同：
ImportError：libhailort.so.4.19.0：无法打开共享对象文件：没有这样的文件或目录。

WSL 上的 Ubuntu 22.04
Python 2012年10月3日
HailoDFC 3.29.0
HailoMZ v2.14]]></description>
      <guid>https://stackoverflow.com/questions/79323698/trouble-compiling-onnx-file-to-hailo-executable-hef</guid>
      <pubDate>Thu, 02 Jan 2025 12:27:26 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：运行 nextcord 时出现 smolagents，但包已安装并单独运行正常 [关闭]</title>
      <link>https://stackoverflow.com/questions/79323567/modulenotfounderror-smolagents-when-running-nextcord-but-the-package-is-instal</link>
      <description><![CDATA[我最近安装了 smolagents 包，我编写的一个小脚本在单独运行中运行良好，正如预期的那样。然而，当将该功能集成到我的 discord 机器人中时，我得到了一个 ModuleNotFoundError。我检查了三次，该包已完全安装到我的 venv 中，并且只有在我初始化机器人时才会发生错误，否则它在同一个项目中可以正常工作。
nextcord.ext.commands.errors.ExtensionFailed：扩展“ai_chat”引发错误：ModuleNotFoundError：没有名为“smolagents”的模块

但是当我单独运行 smolagents 脚本时，它没问题：
┌──────────────────────────────────────── 新运行 ────────────────────────────────────┐
│ │
│ 你无法逃离黑洞的边界是什么？ │
│ │
└─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────┘
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 步骤 0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌─ 执行此代码： ────────────────────────────────────────────────────┐
│ 1 event_horizo​​n_info = web_search(query=&quot;event horizo​​n black hole&quot;) │
│ 2 print(event_horizo​​n_info) │
└────────────────────────────────────────────────────────────────────────┘
┌─ 执行此代码：──────────────────────────────────────────────────────┐
│ 1 final_answer(&quot;event horizo​​n&quot;) │
└────────────────────────────────────────────────────────────────────┘

我尝试通过 pip 强制重新安装包，使用新令牌登录，附加包路径，并将脚本从我拥有的较大的 python 脚本中分离出来。]]></description>
      <guid>https://stackoverflow.com/questions/79323567/modulenotfounderror-smolagents-when-running-nextcord-but-the-package-is-instal</guid>
      <pubDate>Thu, 02 Jan 2025 11:30:57 GMT</pubDate>
    </item>
    <item>
      <title>混淆矩阵未在热图的每个部分显示数字</title>
      <link>https://stackoverflow.com/questions/79321937/confusion-matrix-not-showing-numbers-in-every-portion-of-heat-map</link>
      <description><![CDATA[该图像显示混淆矩阵，其中文本为数字，图像颜色与结果混淆矩阵中相同，但混淆矩阵第一行不显示数字

# 计算准确率
from sklearn import metrics

result_N = metrics.confusion_matrix(y_test, y_pred_N)
print(&quot;Confusion Matrix:&quot;)
print(result_N)

def plt1():
import seaborn as sns; sns.set()
plt.figure(figsize=(4,4))
c_mtrx_N = pd.crosstab(y_test, y_pred_N, rownames=[&#39;Actual&#39;], colnames=[&#39;Predicted&#39;])
sns.heatmap(c_mtrx_N, annot=True, fmt = &#39;.3g&#39;)

plt1()

如何在混淆矩阵中用颜色显示数字？我也尝试过更改混淆矩阵的颜色，但同样的问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79321937/confusion-matrix-not-showing-numbers-in-every-portion-of-heat-map</guid>
      <pubDate>Wed, 01 Jan 2025 16:22:09 GMT</pubDate>
    </item>
    <item>
      <title>训练视频异常检测模型时出现 torch.OutOfMemoryError: CUDA out of memory 错误</title>
      <link>https://stackoverflow.com/questions/79321152/error-of-torch-outofmemoryerror-cuda-out-of-memory-when-training-video-anomaly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79321152/error-of-torch-outofmemoryerror-cuda-out-of-memory-when-training-video-anomaly</guid>
      <pubDate>Wed, 01 Jan 2025 04:31:05 GMT</pubDate>
    </item>
    <item>
      <title>mlagents-learn --help 出现错误（python=3.11、3.10、3.9、3.8）</title>
      <link>https://stackoverflow.com/questions/79316958/mlagents-learn-help-is-giving-errors-python-3-11-3-10-3-9-3-8</link>
      <description><![CDATA[我正在尝试安装 mlagents。我进入了 python 部分，但在使用 pyenv 创建虚拟环境并将本地版本设置为 3.10、3.9 和 3.8 后，它们都不起作用。我升级了 pip，安装了 mlagents，然后安装了 torch、torchvision 和 torchaudio。然后我测试了 mlagents-learn --help，然后因为错误安装了 protobuf 3.20.3。然后我再次测试，得到以下错误
(venv) D:\Unity\AI Ecosystem&gt;mlagents-learn --help
回溯（最近一次调用）：
文件“&lt;frozen runpy&gt;”，第 198 行，在 _run_module_as_main
文件“&lt;frozen runpy&gt;”，第 88 行，在 _run_code
文件“D:\Unity\AI Ecosystem\venv\Scripts\mlagents-learn.exe\__main__.py”，第 4 行，在 &lt;module&gt;
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\learn.py”，第 2 行，在 &lt;module&gt;
从 mlagents 导入 torch_utils
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\__init__.py”，第 1 行，位于 &lt;module&gt;
从 mlagents.torch_utils.torch 导入 torch 作为 torch # noqa
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\torch.py​​”，第 6 行，位于 &lt;module&gt;
从 mlagents.trainers.settings 导入 TorchSettings
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py”，第 644 行，位于 &lt;module&gt;
class TrainerSettings(ExportableSettings):
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py”，第 667 行，位于 TrainerSettings
cattr.register_structure_hook(
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\converters.py”，第 207 行，位于 register_structure_hook
self._structure_func.register_cls_list([(cl, func)])
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\dispatch.py​​”，第 55 行，位于 register_cls_list
self._single_dispatch.register(cls, handler)
文件&quot;C:\Users\Ebrah\AppData\Local\Programs\Python\Python311\Lib\functools.py&quot;，第 864 行，在寄存器中
raise TypeError(
TypeError: `register()` 的第一个参数无效。 typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] 不是类或联合类型。

我尝试安装 cattrs 1.5.0，但错误仍然存​​在。正如我之前所说，我也尝试了 3.11、3.10、3.9 和 3.8，但在所有这些版本中都出现了相同的错误。我的 unity 版本是 2022.3.5f1，但我看不出这会有什么不同。我的 pyenv 版本是 3.1.1。我在 Windows 11 上并且正在使用 pyenv-win。]]></description>
      <guid>https://stackoverflow.com/questions/79316958/mlagents-learn-help-is-giving-errors-python-3-11-3-10-3-9-3-8</guid>
      <pubDate>Mon, 30 Dec 2024 06:36:09 GMT</pubDate>
    </item>
    <item>
      <title>线性回归模型勉强优化了截距b</title>
      <link>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</link>
      <description><![CDATA[我从头开始编写了一个线性回归模型。我使用“残差平方和”作为梯度下降的损失函数。为了进行测试，我使用线性数据 (y=x)
运行算法时，截距 b 几乎没有变化。因此斜率 m 计算不正确。
%matplotlib qt5 
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)

class LinearRegression():
def __init__(self):
self.X = None
self.y = None

def ssr(self, m, b):
sum = 0
for i in range(len(self.X)):
sum += (self.y[i] - (m * self.X[i] + b) ) ** 2

return sum

def ssr_gradient(self, m, b):
sum_m = 0
sum_b = 0
n = len(self.X)
for i in range(n):
error = self.y[i] - (m * self.X[i] + b)
derivative_m = -(2/n) * self.X[i] * error # 相对于 m 的导数
derivative_b = -(2/n) * error # 相对于 m 的导数b
sum_m += derived_m
sum_b += derived_b

return sum_m, sum_b

def fit(self, X, y, m, b): # 梯度下降
self.X = X
self.y = y

M, B = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1))
SSR = np.zeros_like(M)
for i in range(M.shape[0]):
for j in range(M.shape[1]):
SSR[i, j] = self.ssr(M[i, j], B[i, j])

fig, axis = plt.subplots(1, 2, figsize=(12, 6))
gd_model = fig.add_subplot(121,投影=“3d”，computed_zorder=False)
lin_reg_model = axis[1] 

current_pos = (m, b, self.ssr(m, b))
learning_rate = 0.001
min_step_size = 0.001
max_steps = 1000
current_steps = 0

while(current_steps &lt; max_steps):
M_derivative, B_derivative = self.ssr_gradient(current_pos[0], current_pos[1])
M_step_size, B_step_size = M_derivative * learning_rate, B_derivative * learning_rate

if abs(M_step_size) &lt; min_step_size 或 abs(B_step_size) &lt; min_step_size:
break

M_new, B_new = current_pos[0] - M_step_size, current_pos[1] - B_step_size

current_pos = (M_new, B_new, self.ssr(M_new, B_new))

print(f&quot;参数：m：{current_pos[0]}; b：{current_pos[1]}; SSR：{current_pos[2]}&quot;)

current_steps += 1

x = np.arange(0, 10, 1)
y = current_pos[0] * x + current_pos[1]
lin_reg_model.scatter(X_train, y_train, label=&quot;Train&quot;, s=75, c=&quot;#1f77b4&quot;)
lin_reg_model.plot(x, y)

gd_model.plot_surface(M, B, SSR, cmap=&quot;viridis&quot;, zorder=0)
gd_model.scatter(current_pos[0], current_pos[1], current_pos[2], c=&quot;red&quot;, zorder=1)
gd_model.set_xlabel(&quot;斜率 m&quot;)
gd_model.set_ylabel(&quot;截距 b&quot;)
gd_model.set_zlabel(&quot;残差平方和&quot;)

plt.tight_layout()
plt.pause(0.001)

gd_model.clear()
lin_reg_model.clear()

self.m = current_pos[0]
self.b = current_pos[1]

def predict(self, X_test):
return self.m * X_test + self.b

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train, 1, 10)


这是初始值 m=1 和 b=10 的结果：
参数：m：-0.45129949840919587；b：9.50972664859535；SSR：145.06534359577407

显然这不是最佳的，因为我的数据是线性的。因此最佳参数应该是 m=1 和 b=0
但我在代码中找不到问题。该算法根据初始值打印不同的结果，但只要 SSR 函数恰好有一个最小值，它就应该一遍又一遍地打印相同的结果。
我尝试使用不同的学习率，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</guid>
      <pubDate>Fri, 27 Dec 2024 19:40:21 GMT</pubDate>
    </item>
    <item>
      <title>如何对具有大量类别的商品特征进行编码以进行推荐</title>
      <link>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</link>
      <description><![CDATA[对于我正在研究的推荐问题，有大约 50000 个独特品牌和 3 级产品类别，level_1_cat（50 个类别）、level_2_cat（100 个类别）和 level_3_cat（1000 个类别）。所有这些项目特征仅由整数表示。到目前为止，我已经为我的 lightfm 模型尝试了二进制编码、标签编码和目标编码。使用二进制编码和标签编码，结果比不使用任何项目特征更差。使用目标编码，结果与不使用任何项目特征相似。我想知道我还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</guid>
      <pubDate>Wed, 11 Dec 2024 06:39:00 GMT</pubDate>
    </item>
    <item>
      <title>什么是冻结/解冻神经网络中的层？</title>
      <link>https://stackoverflow.com/questions/62228981/what-is-freezing-unfreezing-a-layer-in-neural-networks</link>
      <description><![CDATA[我已经研究神经网络有一段时间了，最​​近在阅读有关迁移学习的文章时，遇到了在训练神经网络之前冻结和解冻层这两个术语，并且正在努力理解它们的用法。

什么时候应该使用冻结/解冻？
哪些层需要冻结/解冻？例如，当我导入预先训练的模型并在我的数据上训练它时，我的整个神经网络（输出层除外）是否都被冻结了？
如何确定是否需要解冻？
如果需要，我如何确定要解冻和训练哪些层以提高模型性能？
]]></description>
      <guid>https://stackoverflow.com/questions/62228981/what-is-freezing-unfreezing-a-layer-in-neural-networks</guid>
      <pubDate>Sat, 06 Jun 2020 08:04:33 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的内容：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;sigmoid&quot;)) 
model.compile(loss = &quot;binary_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train,y_train,validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”，或者 58% 确定它是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法是：“使用分类器，当你输出时，你可以将值解释为属于每个特定类别的概率。你可以使用它们的分布作为你对观察结果属于该类别的信心的粗略衡量标准。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有灰度图像的预训练神经网络？</title>
      <link>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</link>
      <description><![CDATA[我有一个包含灰度图像的数据集，我想在这些图像上训练最先进的 CNN。我非常想微调一个预先训练好的模型（比如这里的模型）。
问题是，我能找到权重的几乎所有模型都是在包含 RGB 图像的 ImageNet 数据集上训练的。
我无法使用其中一个模型，因为它们的输入层需要一批形状为 (batch_size, height, width, 3) 或 (64, 224, 224, 3) 的模型，但我的图像批次是 (64, 224, 224)。
我有什么办法可以使用其中一个模型吗？我曾考虑在加载权重后删除输入层并添加自己的输入层（就像我们对顶层所做的那样）。这种方法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</guid>
      <pubDate>Fri, 24 Aug 2018 00:33:04 GMT</pubDate>
    </item>
    <item>
      <title>如何发现数据集中的哪些特征具有预测作用？</title>
      <link>https://stackoverflow.com/questions/21971709/how-can-you-discover-what-features-in-a-dataset-are-predictive</link>
      <description><![CDATA[我正在为此处提供的数据集开发机器学习算法。
有 26 列数据。其中大部分毫无意义。我如何才能有效快速地确定哪些特征是有趣的 - 哪些特征可以告诉我给定的 URL 是短暂的还是常青的（这是数据集中的因变量）？是否有智能、编程式的 Scikit 学习方法来做到这一点，或者它只是将每个特征与从属特征（“标签”，第 26 列）进行图形化并查看有什么影响的情况？
肯定有比这更好的方法吗？
编辑：我找到了一些分类器的代码 - 我如何打印出这里赋予每个特征的权重？
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics,preprocessing,cross_validation
from sklearn.feature_extraction.text import TfidfVectorizer
import sklearn.linear_model as lm
import pandas as p
loadData = lambda f: np.genfromtxt(open(f,&#39;r&#39;), delimiter=&#39; &#39;)

print &quot;loading data..&quot;
traindata = list(np.array(p.read_table(&#39;train.tsv&#39;))[:,2])
testdata = list(np.array(p.read_table(&#39;test.tsv&#39;))[:,2])
y = np.array(p.read_table(&#39;train.tsv&#39;))[:,-1]

tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents=&#39;unicode&#39;, 
analyzer=&#39;word&#39;,token_pattern=r&#39;\w{1,}&#39;,ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)

rd = lm.LogisticRegression(penalty=&#39;l2&#39;, dual=True, tol=0.0001, 
C=1, fit_intercept=True,intercept_scaling=1.0, 
class_weight=None,random_state=None)

X_all = traindata + testdata
lentrain = len(traindata)

打印“拟合管道”
tfv.fit(X_all)
打印“转换数据”
X_all = tfv.transform(X_all)

X = X_all[:lentrain]
X_test = X_all[lentrain:]

打印“20 倍 CV 得分：”，np.mean(cross_validation.cross_val_score(rd,X,y,cv=20,scoring=&#39;roc_auc&#39;))

打印“在完整数据上训练”
rd.fit(X,y)
pred = rd.predict_proba(X_test)[:,1]
testfile = p.read_csv(&#39;test.tsv&#39;, sep=&quot;\t&quot;, na_values=[&#39;?&#39;], index_col=1)
pred_df = p.DataFrame(pred, index=testfile.index, columns=[&#39;label&#39;])
pred_df.to_csv(&#39;benchmark.csv&#39;)
print &quot;提交文件已创建..&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/21971709/how-can-you-discover-what-features-in-a-dataset-are-predictive</guid>
      <pubDate>Sun, 23 Feb 2014 17:28:49 GMT</pubDate>
    </item>
    </channel>
</rss>