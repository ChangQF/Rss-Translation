<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 07 Aug 2024 12:30:02 GMT</lastBuildDate>
    <item>
      <title>应用 Pytorch 最小化函数时参数没有改变</title>
      <link>https://stackoverflow.com/questions/78843331/parameters-not-changing-while-applying-pytorch-minimization-fucntion</link>
      <description><![CDATA[获取数据的代码：
import pandas as pd
import torch

dataset = pd.read_csv(&#39;/kaggle/input/fish-bear/population_data.csv&#39;)
years = torch.tensor(dataset[&#39;year&#39;], dtype = torch.float64)
fish_pop = torch.tensor(dataset[&#39;fish_hundreds&#39;], dtype = torch.float64)
bears_pop = torch.tensor(dataset[&#39;bears_hundreds&#39;], dtype = torch.float64)
pop = torch.cat((fish_pop.reshape((51, 1)), bears_pop.reshape((51, 1))), 1)

常微分方程求解器
从 typing 导入 List、Callable、Sequence、NamedTuple、Union

class _Tableau(NamedTuple):

c: List[float]
b: List[float]
a: List[List[float]]

rk4_tableau = _Tableau(c=[0.0, 0.5, 0.5, 1.0],
b=[1 / 6., 1 / 3., 1 / 3., 1 / 6.],
a=[[0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.0],
[0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]])

def explicit_rk(tableau: _Tableau, fcn: Callable[..., torch.Tensor],
y0: torch.Tensor, t: torch.Tensor,
params: Sequence[torch.Tensor]):
c = tableau.c
a = tableau.a
b = tableau.b
s = len(c)
nt = len(t)

# 设置结果列表
yt_lst: List[torch.Tensor] = []
yt_lst.append(y0)
y = yt_lst[-1]
for i in range(nt - 1):
t0 = t[i]
t1 = t[i + 1]
h = t1 - t0
ks: List[torch.Tensor] = []
ksum: Union[float, torch.Tensor] = 0.0
for j in range(s):
if j == 0:
k = fcn(y, t0, params)
else:
ak: Union[float, torch.Tensor] = 0.0
aj = a[j]
for m in range(j):
ak = aj[m] * ks[m] + ak
k = fcn(h * ak + y, t0 + c[j] * h, params)
ks.append(k)
ksum = ksum + b[j] * k
y = h * ksum + y
yt_lst.append(y)
yt = torch.stack(yt_lst, dim=0)
return yt

def rk4_ivp(fcn: Callable[..., torch.Tensor], y0: torch.Tensor, t: torch.Tensor,
params: Sequence[torch.Tensor], **kwargs):
return explicit_rk(rk4_tableau, fcn, y0, t, params)

最小化代码：
import torch

def lotka_volterra(y, t, params):
y1, y2 = y
a, b, c, d = params

return torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2])

def loss_function(params):

y0 = torch.tensor([fish_pop[0], bears_pop[0]], dtype = torch.float64)

t = torch.linspace(years[0], years[-1], len(years), dtype = torch.float64)

output = rk4_ivp(lotka_volterra, y0, t, params)

loss = torch.sum((output - pop)**2)
loss.requires_grad = True
return loss

def minimal(loss_function, initial_parameters: torch.Tensor):
list_params = []
params = initial_parameters
params.requires_grad = True
optimizer = torch.optim.SGD([params], lr=0.5)

for i in range(5):
optimizer.zero_grad()
loss: torch.Tensor = loss_function(params)
loss.backward()
optimizer.step()
list_params.append(params.detach().clone())

return params, list_params

starting_point = torch.nn.Parameter(torch.tensor([1.1, .4, .1, .4], dtype = torch.float64))
minimized_pa​​rams, list_of_params =最小化（loss_function，starting_point）

loss_function（minimized_pa​​rams），minimized_pa​​rams

在迭代结束时，参数不会得到优化并按原样返回。
结果：
（tensor（118.6865，dtype=torch.float64，requires_grad=True），
参数包含：
tensor（[1.1000, 0.4000, 0.1000, 0.4000]，dtype=torch.float64，
require_grad=True））

Kaggle Notebook 链接：https://www.kaggle.com/code/rakshitsingh421/parameter-estimation/edit
我尝试更改 require_grad 属性，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78843331/parameters-not-changing-while-applying-pytorch-minimization-fucntion</guid>
      <pubDate>Wed, 07 Aug 2024 11:04:40 GMT</pubDate>
    </item>
    <item>
      <title>无法初始化类 org.deeplearning4j.nn.modelimport.keras.Hdf5Archive</title>
      <link>https://stackoverflow.com/questions/78843048/could-not-initialize-class-org-deeplearning4j-nn-modelimport-keras-hdf5archive</link>
      <description><![CDATA[当我尝试使用 scala 加载 .h5 模型时出现以下错误
KerasModelImport.importKerasModelAndWeights()。


线程“streaming-job-executor-1”中出现异常java.lang.NoClassDefFoundError: 无法初始化类 org.deeplearning4j.nn.modelimport.keras.Hdf5Archive
at org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.modelHdf5Filename(KerasModelBuilder.java:229)
at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasModelAndWeights(KerasModelImport.java:166)


代码中添加的依赖项 jar 文件
上图显示了我程序中添加的依赖项。同时，我安装了 libhdf5-dev
并为其设置了路径。但我遇到了同样的错误。
我的代码是用 Scala 语言编写的。这是我尝试加载模型的方式：
val model: ComputationGraph = KerasModelImport.importKerasModelAndWeights(modelPath)

我还在程序中导入了以下内容：
import org.deeplearning4j.nn.graph.ComputationGraph
import org.deeplearning4j.nn.conf.ComputationGraphConfiguration
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.util.ModelSerializer
import org.deeplearning4j.nn.modelimport.keras.KerasModelImport
import org.deeplearning4j.nn.modelimport.keras.Hdf5Archive

我不确定是不是版本不匹配。有人能指出这里出了什么问题吗？我是 deeplearning4j 库的新手。]]></description>
      <guid>https://stackoverflow.com/questions/78843048/could-not-initialize-class-org-deeplearning4j-nn-modelimport-keras-hdf5archive</guid>
      <pubDate>Wed, 07 Aug 2024 10:01:18 GMT</pubDate>
    </item>
    <item>
      <title>我在这次体重更新中到底做错了什么？</title>
      <link>https://stackoverflow.com/questions/78843005/what-am-i-exactly-doing-wrong-in-this-weight-updation</link>
      <description><![CDATA[我正在尝试从头开始实现反向传播。我正在尝试更新具有多个输入和多个输出但没有隐藏层的网络的权重。我实现的逻辑对我来说似乎很好。但权重并没有收敛，而是增加了。我无法调试为什么会发生这种情况。
def update():
global weights_n
for i in range(10000):
y_mult = p1(x_random)
print(&quot;Varying Output&quot;,y_mult)
print(weights_n)
error = calculate_errors(y_mult, y)
z=y_mult-y
print(&quot;Error&quot;,error)
grad_a1 = np.dot(x_random.T,z)
print(learning_rate * grad_a1)
weights_n = weights_n - learning_rate * grad_a1
print(weights_n)
if error.all()&lt;=0.5:
break
return weights_n

当我尝试将其与 numpy sum 和 matmul 一起使用时，它可以工作，但使用点积错误不但没有减少，反而增加了。]]></description>
      <guid>https://stackoverflow.com/questions/78843005/what-am-i-exactly-doing-wrong-in-this-weight-updation</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:16 GMT</pubDate>
    </item>
    <item>
      <title>如何继续创建视频描述模型？</title>
      <link>https://stackoverflow.com/questions/78842373/how-to-go-on-about-creating-a-video-description-model</link>
      <description><![CDATA[我最近一直在尝试制作一个可以实时将视频转换为文本的应用程序。这意味着您可以将实时视频转换为文本。
首先，我尝试只使用 gpt api，但当然速度很慢而且远没有达到实时，然后我想到使用对象检测模型和 gpt 来生成视频描述，但问题是它非常不准确，因为只检测到了对象，但错过了背景或动作，我尝试过跟踪以及 yolo 的姿势模型，但这似乎也不起作用。所以我想我会尝试在这里询问任何可能有用的建议？]]></description>
      <guid>https://stackoverflow.com/questions/78842373/how-to-go-on-about-creating-a-video-description-model</guid>
      <pubDate>Wed, 07 Aug 2024 07:34:13 GMT</pubDate>
    </item>
    <item>
      <title>根据面积或宽度提取并替换像素值</title>
      <link>https://stackoverflow.com/questions/78842299/extract-and-replace-pixel-values-based-on-their-area-or-width</link>
      <description><![CDATA[我有一张如下所示的分割图像
车辆边缘有一条非常细的不同颜色的线（人开车的红色区域）。我想通过根据相邻像素分配不同的标签 ID（红色或黑色）来消除这条细线。我知道如何根据所需的颜色甚至 ID 提取像素。但在这种情况下，颜色或 ID 不是固定的，它可能是不同图像中的不同颜色或 ID。我无法想出一种方法来提取这些像素。有人能帮我提取属于细线的像素吗？]]></description>
      <guid>https://stackoverflow.com/questions/78842299/extract-and-replace-pixel-values-based-on-their-area-or-width</guid>
      <pubDate>Wed, 07 Aug 2024 07:17:11 GMT</pubDate>
    </item>
    <item>
      <title>无需深度学习或 Tesseract 的文本图像二元分类器</title>
      <link>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</link>
      <description><![CDATA[我有 20k 张小标签图像，每张图像都有单词“Back”或“Front”。
图像分辨率为全部 (200px, 25px)

我可以使用 tesseract_OCR 对这些图像进行 100% 准确率的分类。
 txt = pytesseract.image_to_string(img, lang=&#39;eng&#39;)
if &quot;Front&quot; in txt:
return &quot;Front&quot;
if &quot;Back&quot; in txt:
return &quot;Back&quot;

问题是，它太慢了（20k 张图像需要 1 小时）并且需要安装 OCR 包。
我知道即使是 3 层的简单 CNN 也能很好地运行，但我认为这个问题似乎可以用简单的算法解决，而不需要复杂的技术。
你能给我推荐一种新方法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</guid>
      <pubDate>Wed, 07 Aug 2024 06:46:36 GMT</pubDate>
    </item>
    <item>
      <title>对于暴露的情况，孪生网络是否是正确的算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/78842114/is-a-siamese-network-the-right-algorithm-for-the-exposed-case</link>
      <description><![CDATA[我的数据库中存储了 20,000 多张艺术品图片（绘画、雕塑、罐子等）。实际作品分布在多个仓库中。理想情况下，实物应该贴上标签（带有其 ID、QR 码等），这些标签是纸质的，因此可能会受损、印刷不良、无法读取、完全丢失甚至放错位置。我的目标是创建一个模型，该模型接收输入（任何仓库的某人发送的图像），从可用数据中识别完全相同的艺术品并返回其 ID、详细信息等。
在我的例子中，样本是静态的、固定的（不会有“新”艺术品，除非客户购买更多），因此模型永远不会“看到”新图像，这让我认为过度拟合可能是模型最理想的结果（这意味着大量的数据增强和大量的 epoch）。
请注意，每个类（艺术作品）只有一个图像可用。这就是无法改变的情况。
所选的编程语言是 R，主要是 tensorflow 和 keras3 库。
话虽如此，我很难找到解决方案，因为每个文档都依赖于相同的 cat vs dogs 或 mnist 数据集。我的问题是：

暹罗网络是否是用于此目的的正确算法？
我可以采取哪些方法来提高准确性？

仅出于测试目的，我抽取了 10 个样本，从每个样本中生成了 9 个其他样本（数据增强、应用旋转、垂直/水平翻转、随机饱和度因子、随机亮度因子等）。后来，为每个类别创建了 5 个正对和 5 个负对。最后，我运行了一个暹罗网络，但准确率似乎停留在 49%。]]></description>
      <guid>https://stackoverflow.com/questions/78842114/is-a-siamese-network-the-right-algorithm-for-the-exposed-case</guid>
      <pubDate>Wed, 07 Aug 2024 06:29:33 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 张量与标量相乘后丢失requires_grad</title>
      <link>https://stackoverflow.com/questions/78841738/pytorch-tensor-losing-requires-grad-after-multiplication-with-a-scalar</link>
      <description><![CDATA[我正在开展一个深度学习项目，并试图实现一个投影网络。简而言之，我有一个根据某些损失函数输出值的神经网络。如果这些值不满足所有约束，我想将它们放入梯度下降网络（损失为 ReLU（需要 - 已实现））。在下面的代码中，我用 p 和 phi 参数（第一个网络输出的值）初始化投影网络并希望它们进行训练。但是，我遇到了 p 和 phi（theta）变量都失去梯度跟踪的问题，导致损失没有梯度跟踪。如果这很简单，我深表歉意，因为我对 Pytorch 还很陌生，在网上找不到任何东西。
投影 nn 的代码：
class Projection(nn.Module):
def __init__(self, p, phi):
super().__init__()
self.p = nn.Parameter(p.clone().detach().requires_grad_(True))
self.phi = nn.Parameter(phi.clone().detach().requires_grad_(True))

def forward(self, u, g, v):
print(self.p.requires_grad, self.phi.requires_grad) ## 均为 true
p = self.p * P_max
print(p.requires_grad) # 在此点之后为 false
p = p.view(batch_size, K, 1)
sig = torch.nn.Sigmoid()
theta = torch.diag_embed(torch.exp(sig(self.phi) * 2 * torch.pi * 1j))
print(theta.requires_grad) # 在此点之后为 false
gh = torch.conj(torch.transpose(g,-1,-2))
print(gh.shape, theta.shape, v.shape)
term = torch.matmul(torch.matmul(gh, theta), v)
u_tilde = torch.add(u, term)
print(u_tilde.shape, p.shape)
harvested = eta * torch.matmul(torch.square(torch.abs(torch.transpose(u_tilde, -1, -2))), p) * 1e6
ReLU = torch.nn.ReLU()
diff = e_min_gen - harvested
loss = torch.sum(ReLU(diff))
print(loss.requires_grad)
return loss

P_max 声明如下：
P_max = 0.5
我特别困惑，因为在同一个项目中我使用梯度下降来生成可行数据。模型如下：
class Descent(nn.Module):
def __init__(self):
super().__init__()
self.weight = nn.Parameter(torch.randn(N, require_grad=True))

def forward(self, u, g, phi, v, p):
phi = phi * self.weight
sig = torch.nn.Sigmoid()
theta = torch.diag_embed(torch.exp(sig(phi) * 2 * torch.pi * 1j))
gh = torch.conj(torch.transpose(g,-1,0))
term = torch.mm(torch.mm(gh, theta), v)
u_tilde = torch.add(u, term)
harvested = eta * torch.matmul(torch.square(torch.abs(torch.transpose(u_tilde, 0, 1))), p) * 1e6
ReLU = torch.nn.ReLU()
diff = e_min_gen - harvested
loss = torch.sum(ReLU(diff))
return loss

此模型保留 require_grad 直到最后。
任何解决方法都将不胜感激。感谢您的时间。
我尝试查找梯度跟踪丢失的原因。我已经准确地确定了跟踪停止的点。如果我不将 self.p 乘以 P_max，则 p 的 gradient_tracking=True。]]></description>
      <guid>https://stackoverflow.com/questions/78841738/pytorch-tensor-losing-requires-grad-after-multiplication-with-a-scalar</guid>
      <pubDate>Wed, 07 Aug 2024 03:39:37 GMT</pubDate>
    </item>
    <item>
      <title>如何最好地将 ML 模型集成到 Web 应用程序？</title>
      <link>https://stackoverflow.com/questions/78841736/how-to-best-integrate-ml-models-to-web-application</link>
      <description><![CDATA[我没有机器学习经验，是 Web 应用程序编程的新手。我目前有一个使用 audiocraft 和分类模型的 flask 应用程序。目前我已将它们本地存储在应用程序文件夹中。我已构建此应用程序的映像，结果显示它有 7GB。
有没有办法将这些模型/框架存储在其他地方，并且仅在需要时引用它们？
此外，当我在 docker 上运行容器时，从 audiocraft 生成 8 秒音频大约需要 10 分钟。您建议我做什么来加快这个过程？
music_generation\routes.py（片段）
def load_model():
model = MusicGen.get_pretrained(&#39;facebook/musicgen-small&#39;)
return model

def generate_music_tensors(description, duration: int):
model = load_model()
model.set_generation_params(
use_sampling=True,
top_k=250,
duration=duration
)
output = model.generate(
descriptions=[description],
progress=True,
return_tokens=True
)
return output[0]

@music_generation_bp.route(&#39;/&#39;, methods=[&#39;POST&#39;])
def generate_music():
data = request.json

description = data.get(&#39;description&#39;)
duration = data.get(&#39;duration&#39;, 8) # 如果未提供，则默认为 8 秒
print(&quot;Description:&quot;, description)
print(&quot;Duration:&quot;, duration)

如果没有 description:
return jsonify({&#39;error&#39;: &#39;Description is required&#39;}), 400
# 为用户生成唯一密钥
user_id = str(uuid.uuid4()) # 或使用来自身份验证系统的用户 ID
audio_key_prefix = f&quot;generated_music_{user_id}_{description}&quot;

# 生成音乐张量
music_tensors = generate_music_tensors(description, duration)
print(&quot;Music Tensors: &quot;, music_tensors)
...


image_classification\routes.py (代码片段)
# 加载预训练模型
model_path = os.path.join(MODELS_DIR, &#39;multi_output_model.h5&#39;)
model = tf.keras.models.load_model(model_path)

@image_classification_bp.route(&#39;/&#39;, methods=[&#39;POST&#39;])
@limiter.limit(&quot;1/minute&quot;)
def classify_image():
if &#39;file&#39; not in request.files:
return jsonify({&quot;error&quot;: &quot;No file part in the request&quot;}), 400

file = request.files[&#39;file&#39;]

if file.filename == &#39;&#39;:
return jsonify({&quot;error&quot;: &quot;No selected file&quot;}), 400

#file = os.path.join(TEST_IMG_DIR, &#39;blue-dress2.png&#39;)

if file:
# 读取图像文件
img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
img = cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = preprocess_input(img)
img = np.expand_dims(img, axis=0)

# 执行预测
predictions = model.predict(img)

]]></description>
      <guid>https://stackoverflow.com/questions/78841736/how-to-best-integrate-ml-models-to-web-application</guid>
      <pubDate>Wed, 07 Aug 2024 03:39:15 GMT</pubDate>
    </item>
    <item>
      <title>当我导入库时，为什么我的代码会出现错误“sklearn 未定义”？</title>
      <link>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</link>
      <description><![CDATA[我的代码：
import numpy as np
import pandas as pd
import pickle as pk
from sklearn import linear_model
from sklearn.utils import shuffle

data = pd.read_csv(&quot;student-mat.csv&quot;, sep=&quot;;&quot;)
data = data[[&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;studytime&quot;, &quot;failures&quot;, &quot;absences&quot;]]
print(data.head())
predict = &quot;G3&quot;
x = np.array(data.drop(predict, axis=1))
y = np.array(data[predict])
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
linear = linear_model.LinearRegression()
linear.fit(x_train, y_train)
acc = linear.score(x_test, y_test)
print(acc)
with open(&quot;studentmodel.pickle&quot;, &quot;wb&quot;) as f:
pickle.dump(linear,f)
pickle_in = open(&quot;studentmodel.pickle&quot;, &quot;rb&quot;)
linear = pickle.load(pickle_in)
print(&quot;coefficient:\n&quot;, linear.coef_)
print(&quot;intercept:\n&quot;, linear.intercept_)`

当我运行此代码时，它会抛出错误 name sklearn is not defined。但是，这很奇怪，因为我导入了正确的 sklearn 库。
完整错误：
NameError Traceback（最近一次调用最后一次）
Cell In[1]，第 14 行
12 x = np.array(data.drop(predict, axis=1))
13 y = np.array(data[predict])
---&gt; 14 x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
15 linear = linear_model.LinearRegression()
16 linear.fit(x_train, y_train)

NameError：名称“sklearn”未定义
]]></description>
      <guid>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</guid>
      <pubDate>Wed, 07 Aug 2024 02:46:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH, YAML_FILE_PATH)`

我在运行训练脚本时收到此未来错误，并且我正在努力解决该错误：FutureWarning：torch.cuda.amp.autocast(args...) 已弃用。请改用 torch.amp.autocast(&#39;cuda&#39;, args...)。使用 torch.cuda.amp.autocast(amp)]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Skip-Gram 实现产生了错误的结果？</title>
      <link>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</guid>
      <pubDate>Fri, 02 Aug 2024 07:15:07 GMT</pubDate>
    </item>
    <item>
      <title>更清晰的分割 SAM (Segment Anything)</title>
      <link>https://stackoverflow.com/questions/78822914/sharper-segmentation-sam-segment-anything</link>
      <description><![CDATA[我需要像这样分割图像上的对象：
图像 1
图像 2
我选择使用 Meta 的 AI SAM（Segment Anything）来裁剪这些对象。
我的代码如下所示：
import cv2
import numpy as np
import sys, os
from pathlib import Path
import torch
import surveillance as sv
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

# 获取命令行参数
num = sys.argv[2]
img_path = sys.argv[1]
folder_path = sys.argv[3]

# 从给定路径加载图像
img = cv2.imread(img_path)

# 用于预处理图像以进行裁剪的函数
def preprocess_image_cut(img):
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
return gray

# 用于获取干净图像的函数
def get_clean_image(img, filter_level=1):
gray = preprocess_image_cut(img)
blurred_image = cv2.GaussianBlur(gray, (3, 3), 0)
_, binary_image = cv2.threshold(blurred_image, 210, 255, cv2.THRESH_BINARY)

kernel = np.ones((1, 1), np.uint8)
result_image = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel)
result_image = cv2.medianBlur(result_image, filter_level)

output_image = cv2.bitwise_or(gray, result_image)
return output_image

# 设置管道的函数
def setup_pipeline():
HOME = &#39;C:/&#39;
CHECKPOINT_PATH = os.path.join(HOME, &#39;weights&#39;, &#39;sam_vit_h_4b8939.pth&#39;)
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
MODEL_TYPE = &quot;vit_h&quot;

sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)
mask_generator = SamAutomaticMaskGenerator(sam)
return mask_generator

# 运行 SAM 模型的函数
def run_sam(mask_generator, clean_image):
image_rgb = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)
image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
sam_result = mask_generator.generate(image_rgb)
mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)
detections = sv.Detections.from_sam(sam_result=sam_result)

annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)

masks = [mask[&#39;segmentation&#39;] for mask in sorted(sam_result, key=lambda x: x[&#39;area&#39;, reverse=True])]
return mask

# 设置掩码生成器
mask_generator = setup_pipeline()

if __name__ == &quot;__main__&quot;:
if img_path.endswith(&quot;.png&quot;):
save_path = folder_path
os.makedirs(save_path, exist_ok=True)
if os.path.isdir(f&#39;{folder_path}/PROCESSED&#39;):
os.makedirs(f&#39;{folder_path}/PROCESSED&#39;, exist_ok=True)
img = cv2.imread(img_path)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
clean_image = get_clean_image(img, 3)

print(&quot;开始生成掩码&quot;)
mask = run_sam(mask_generator, clean_image)

for i, mask in enumerate(masks):
image_rgb = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)
image_bgra = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGRA)
extract_region = np.zeros_like(image_bgra)
extract_region[mask] = image_bgra[mask]
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
extract_region = extract_region[y:y+h, x:x+w]
extracted_region[:, :, 3] = (mask[y:y+h, x:x+w] &gt; 0) * 255
save_path2 = f&#39;{folder_path}/PROCESSED/img_{x}_{y}_{w}_{h}.png&#39;
cv2.imwrite(save_path2, extracted_region)

print(&quot;Finished&quot;)

但我在一些问题上遇到了困难，比如我不想让 Sam 分割数字，也不想分割将对象与数字联系起来的线条。
有人能就这个问题提出一些想法吗？也接受其他方法来分割图像。]]></description>
      <guid>https://stackoverflow.com/questions/78822914/sharper-segmentation-sam-segment-anything</guid>
      <pubDate>Thu, 01 Aug 2024 20:21:17 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的激活函数[关闭]</title>
      <link>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</link>
      <description><![CDATA[机器学习中的激活函数是什么意思。我浏览了大多数文章和视频，每个人都提到或将其与神经网络进行比较。我是机器学习的新手，对深度学习和神经网络不太熟悉。所以，有人能给我解释一下激活函数到底是什么吗？而不是用神经网络来解释。我在学习逻辑回归的 Sigmoid 函数时就被这种模棱两可的感觉所困扰。]]></description>
      <guid>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</guid>
      <pubDate>Tue, 20 Mar 2018 18:21:08 GMT</pubDate>
    </item>
    </channel>
</rss>