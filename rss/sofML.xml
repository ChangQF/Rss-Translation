<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 29 Jan 2025 06:23:39 GMT</lastBuildDate>
    <item>
      <title>如何使用“tf.data.Dataset.save”将数据集保存在多个分片中</title>
      <link>https://stackoverflow.com/questions/79395477/how-to-save-a-dataset-in-multiple-shards-using-tf-data-dataset-save</link>
      <description><![CDATA[如何使用 tf.data.Dataset.save() 将 tf.data.Dataset 保存到多个分片中？我正在使用 tf.data.experimental.make_csv_dataset 从 CSV 读取我的数据集。
此处的 TF 文档 不是很有用。有一个 shard_func 参数，但给出的示例没有帮助，并且不清楚如何以确定性方式映射到 int。使用随机 int 似乎也不起作用。
类似问题 此处 中的解决方案为我生成了一个错误
TypeError：不支持 % 的操作数类型：&#39;collections.OrderedDict&#39; 和 &#39;int&#39;
单分片（有效）
以下成功保存到单个分片。
import pandas as pd
import numpy as np
import tensorflow as tf

# gen data
n=10000
pd.DataFrame(
{&#39;label&#39;: np.random.randint(low=0, high=2, size=n),
&#39;f1&#39;: np.random.random(n),
&#39;f2&#39;: np.random.random(n),
&#39;f3&#39;: np.random.random(n),
&#39;c1&#39;: np.random.randint(n),
&#39;c2&#39;: np.random.randint(n)}
).to_csv(&#39;tmp.csv&#39;)
# 将数据加载到 tf.data.Dataset
data_ts = tf.data.experimental.make_csv_dataset(
&#39;tmp.csv&#39;, 1, label_name=&#39;label&#39;, num_epochs=1)
data_ts.save(&#39;tmp.data&#39;) # 单个分片，有效！

使用 randint 的多个分片（保存单个分片）
尝试使用随机数保存到多个分片，仍然只保存到单个分片，尽管文件名中有一个随机整数。
# 尝试使用随机数进行分片。
def random_shard_function(features, label):
return np.int64(np.random.randint(10))
data_ts.save(&#39;tmp2.data&#39;, shard_func=random_shard_function)



Modulo shard（错误）
尝试这个解决方案问题。
def modulo_shard_function(features, label):
return x &amp; 10
data_ts.save(&#39;tmp2.data&#39;, shard_func=modulo_shard_function)

TypeError: &amp; 不支持的操作数类型：&#39;collections.OrderedDict&#39; 和 &#39;int&#39;
调试 - 不知道 shard_fun 如何工作。
如果我打印出输入，似乎分片函数只运行一次，张量是 SymbolicTensors
def debug_shard_function(features, label):
for val in features.items():
print(f&#39;{val=}&#39;)
print(f&#39;{label=}&#39;)
print(f&#39;{type(val[1])}&#39;)
return np.int64(10)
data_ts.save(&#39;tmp2.data&#39;, shard_func=debug_shard_function)

输出：
仍然保存到单个分片
val=(&#39;&#39;, &lt;tf.Tensor &#39;args_0:0&#39; shape=(None,) dtype=int32&gt;)
val=(&#39;f1&#39;, &lt;tf.Tensor &#39;args_3:0&#39; shape=(None,) dtype=float32&gt;)
val=(&#39;f2&#39;, &lt;tf.Tensor &#39;args_4:0&#39; shape=(None,) dtype=float32&gt;)
val=(&#39;f3&#39;, &lt;tf.Tensor &#39;args_5:0&#39; shape=(None,) dtype=float32&gt;)
val=(&#39;c1&#39;, &lt;tf.Tensor &#39;args_1:0&#39; shape=(None,) dtype=int32&gt;)
val=(&#39;c2&#39;, &lt;tf.Tensor &#39;args_2:0&#39; shape=(None,) dtype=int32&gt;)
label=&lt;tf.Tensor &#39;args_6:0&#39; shape=(None,) dtype=int32&gt;
&lt;class &#39;tensorflow.python.framework.ops.SymbolicTensor&#39;&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/79395477/how-to-save-a-dataset-in-multiple-shards-using-tf-data-dataset-save</guid>
      <pubDate>Wed, 29 Jan 2025 00:08:54 GMT</pubDate>
    </item>
    <item>
      <title>NumPy Stride 技巧：是否可以将窗口重新添加回原始数组大小的同一位置而无需 for 循环？</title>
      <link>https://stackoverflow.com/questions/79395423/numpy-stride-tricks-is-it-possible-to-add-the-windows-back-into-the-original-ar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79395423/numpy-stride-tricks-is-it-possible-to-add-the-windows-back-into-the-original-ar</guid>
      <pubDate>Tue, 28 Jan 2025 23:28:50 GMT</pubDate>
    </item>
    <item>
      <title>LSTM AE 预测恒定输出</title>
      <link>https://stackoverflow.com/questions/79395028/lstm-ae-predicts-constant-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79395028/lstm-ae-predicts-constant-output</guid>
      <pubDate>Tue, 28 Jan 2025 19:57:18 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型从物理应用题中提取向量</title>
      <link>https://stackoverflow.com/questions/79394932/machine-learning-model-to-extract-vectors-from-physics-word-problems</link>
      <description><![CDATA[我正在开展一个小项目，从物理应用题中提取向量及其属性，例如量级、方向和类型，以便进一步处理它们。这个项目将通过实施机器学习模型来取代我目前在 spaCy 中使用的基于规则的方法。例如，给定文本：
“Joe 向北走 2 米，然后向东走 3 米，”

我的目标是提取相应的向量属性：量级、方向和类型。在这种情况下，正确的输出将是：
2m，北，位移
3m，东，位移

但是，提取 3N，北，位移和 2m，东，位移是不正确的，因为 3m 与东方向相关，反之亦然。虽然我们目前的方法适用于更简单的句子（例如上面给出的句子），但句子结构的多变性带来了重大挑战，我相信机器学习可以更好地解决这些挑战。
我在网上看到这个模型可以通过简单的前馈结构和二元分类来完成。但我对训练数据和方法应该如何做感到非常困惑。ChatGPT 建议使用以下结构作为潜在的训练数据。这个结构合法吗？我也不确定如何使用 Keras 访问它。
这是 chatgpt 生成的 json 文件
[

{
&quot;context&quot;: [
&quot;一辆车以 50 公里的速度向北行驶，而另一辆车向东行驶 30 公里。&quot;
],
&quot;pairs&quot;: [
{&quot;magnitude&quot;: &quot;50 km&quot;, &quot;direction&quot;: &quot;north&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;30 km&quot;, &quot;direction&quot;: &quot;east&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;50 km&quot;, &quot;direction&quot;: &quot;east&quot;, &quot;label&quot;: 0},
{&quot;magnitude&quot;: &quot;30 km&quot;, &quot;direction&quot;: &quot;north&quot;, &quot;label&quot;: 0}
]
},
{
&quot;context&quot;: [
&quot;一辆汽车向南行进 40 公里。随后，它向东移动 20 公里。”
],
&quot;pairs&quot;: [
{&quot;magnitude&quot;: &quot;40 km&quot;, &quot;direction&quot;: &quot;south&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;20 km&quot;, &quot;direction&quot;: &quot;east&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;40 km&quot;, &quot;direction&quot;: &quot;east&quot;, &quot;label&quot;: 0},
{&quot;magnitude&quot;: &quot;20 km&quot;, &quot;direction&quot;: &quot;south&quot;, &quot;label&quot;: 0}
]
},
{
&quot;context&quot;: [
“一架飞机以 5 米/秒^2 的速度向上加速，然后以 3 米/秒^2 的速度向西减速。”
],
&quot;pairs&quot;: [
{&quot;magnitude&quot;: &quot;5 m/s^2&quot;, &quot;direction&quot;: &quot;upwards&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;3 m/s^2&quot;, &quot;direction&quot;: &quot;westward&quot;, &quot;label&quot;: 1},
{&quot;magnitude&quot;: &quot;5 m/s^2&quot;, &quot;direction&quot;: &quot;westward&quot;, &quot;label&quot;: 0},
{&quot;magnitude&quot;: &quot;3 m/s^2&quot;, &quot;direction&quot;: &quot;upwards&quot;, &quot;label&quot;: 0}
]
}
]

如何提取这些向量属性并相应地关联它们？]]></description>
      <guid>https://stackoverflow.com/questions/79394932/machine-learning-model-to-extract-vectors-from-physics-word-problems</guid>
      <pubDate>Tue, 28 Jan 2025 19:04:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ollama-python 包有效地对本地 Llama 3.1 模型进行并发调用？</title>
      <link>https://stackoverflow.com/questions/79394817/how-to-efficiently-make-concurrent-calls-to-a-local-llama-3-1-model-using-the-ol</link>
      <description><![CDATA[我正在使用本地 Llama 3.1:7b 模型。我正在使用 Python 和 ollama-python 包与模型交互。
我需要对模型进行 20,000 多次调用，但我目前是按顺序调用的，效率很低。我正在寻找提高效率的方法。具体来说，我试图找出是否有办法：

批量输入以一次发送多个请求。
使用线程、多处理或任何形式的并发处理来同时处理多个调用，而不是按顺序进行。
提高大量查询的整体速度和响应时间。

是否有人有使用 ollama-python 包实现此目的的建议或代码示例，或者在 Python 中处理并发 API 调用的一般技术？即使解决方案不专门涉及 ollama，我也愿意听取其他方法。
到目前为止，我已经尝试按顺序运行模型，目前正在尝试对调用进行线程化。]]></description>
      <guid>https://stackoverflow.com/questions/79394817/how-to-efficiently-make-concurrent-calls-to-a-local-llama-3-1-model-using-the-ol</guid>
      <pubDate>Tue, 28 Jan 2025 18:12:08 GMT</pubDate>
    </item>
    <item>
      <title>自定义字母表的文本识别</title>
      <link>https://stackoverflow.com/questions/79394460/text-recogniton-for-custom-alphabet</link>
      <description><![CDATA[我有一个虚构的字母表，由大约 20 种形状和字母组成，它们与希腊字母和西里尔字母相似。
它们是作为资产生成的，我为它们每个都制作了 30 x 30 的图像。我想创建一个特殊的图像处理工具来实时翻译它们。
它们总是打印在黑色多边形上。
所以我尝试使用通用 opencv2 方法使用黑色多边形进行检测，并且成功了。
为了扫描和检测字母，我尝试了 ORB 特征提取和匹配，但没有成功。由于相似的符文形状，它在整个字母中都发现了特征。
我曾尝试使用 Yolo11 训练物体检测模型，但由于数据量少（我没有已打印的示例，我尝试生成具有不同角度的模拟图像），它成本高且表现不佳。
我没有足够的数据来训练 HOG。
是否有一个简单的 Python 模式匹配算法，可以考虑现实生活中相机的小倾斜平移和滚动，因此仍然可以检测字母？]]></description>
      <guid>https://stackoverflow.com/questions/79394460/text-recogniton-for-custom-alphabet</guid>
      <pubDate>Tue, 28 Jan 2025 16:02:10 GMT</pubDate>
    </item>
    <item>
      <title>Keras 训练问题：“您的输入数据不足”警告和 0 损失/准确度</title>
      <link>https://stackoverflow.com/questions/79394023/keras-training-issue-your-input-ran-out-of-data-warning-and-0-loss-accuracy</link>
      <description><![CDATA[我正在使用 ImageDataGenerator 训练 CNN 模型，数据集包含 1500 张训练图像和 32 个批次大小。我计算每个时期的步数为
⌈1500/32⌉=47，最后一个批次不完整，我正在使用 flow_from_directory() 加载图像。但是，在第一个时期之后，模型显示某些时期的损失为 0，准确率为 0，并且训练中断并显示警告：

UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 steps_per_epoch * epochs 个批次。在构建数据集时，您可能需要使用 .repeat() 函数。
self.gen.throw(typ, value, traceback)

train_datagen = ImageDataGenerator(rescale=1./255)
valid_datagen = ImageDataGenerator(rescale=1./255)

# 从目录导入数据并将其转换为批次
train_data = train_datagen.flow_from_directory(train_path,
batch_size=32,
target_size=(224, 224),
class_mode=&quot;binary&quot;)

valid_data = valid_datagen.flow_from_directory(test_path,
batch_size=32,
target_size=(224, 224),
class_mode=&quot;binary&quot;)

tf.random.set_seed(42)

model_1 = tf.keras.models.Sequential([
输入（形状=（224, 224, 3）），
Conv2D（过滤器=10，
kernel_size=3，
strides=1，
padding=&#39;valid&#39;，
激活=&#39;relu&#39;），
Conv2D（10，3，激活=&#39;relu&#39;），
Conv2D（10，3，激活=&#39;relu&#39;），
Flatten（），
Dense（1，激活=&#39;sigmoid&#39;）
]）

model_1.compile（loss=“binary_crossentropy”，
optimizer=tf.keras.optimizers.Adam（），
metrics=[“accuracy”]）

history_1 = model_1.fit（train_data，
epochs=5，
steps_per_epoch=len（train_data），
validation_data=valid_data，
validation_steps=len(valid_data))

输出日志：
Epoch 1/5
/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122：UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()

47/47 ━━━━━━━━━━━━━━━━━━━━━ 767s 14s/step - 准确度：0.6488 - 损失：0.6197 - val_accuracy：0.7620 - val_loss：0.4793

Epoch 2/5
47/47 ━━━━━━━━━━━━━━━━━━━━━━ 0s 426us/step - 准确度：0.0000e+00 - 损失： 0.0000e+00

Epoch 3/5
/usr/lib/python3.11/contextlib.py:158: UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。您可能需要在构建数据集时使用 `.repeat()` 函数。
self.gen.throw(typ, value, traceback)

47/47 ━━━━━━━━━━━━━━━━━━━━━ 48s 224ms/step - 准确度：0.8120 - 损失：0.4131 - val_accuracy：0.8600 - val_loss：0.3441

Epoch 4/5
47/47 ━━━━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - 准确度：0.0000e+00 - 损失： 0.0000e+00

纪元 5/5
47/47 ━━━━━━━━━━━━━━━━━━━━━━ 11 秒 219 毫秒/步 - 准确度：0.8336 - 损失：0.3814 - val_accuracy：0.8880 - val_loss：0.3042
]]></description>
      <guid>https://stackoverflow.com/questions/79394023/keras-training-issue-your-input-ran-out-of-data-warning-and-0-loss-accuracy</guid>
      <pubDate>Tue, 28 Jan 2025 13:39:12 GMT</pubDate>
    </item>
    <item>
      <title>如何从接地恐龙的预测函数中提取边界框坐标？</title>
      <link>https://stackoverflow.com/questions/79393871/how-to-extract-bounding-box-coordinates-from-grounding-dinos-predict-function</link>
      <description><![CDATA[预测函数返回的框似乎不是规范化的形式，即使与图像宽度和高度相乘后，我也无法获得边界框的坐标。
import torch
from groundingdino.util.inference import load_model, load_image, predict, annotate
import cv2

# 加载模型
model = load_model(&quot;../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py&quot;,
&quot;../GroundingDINO/weights/groundingdino_swint_ogc.pth&quot;)
IMAGE_PATH = &quot;.asset/cat_dog.jpeg&quot;
TEXT_PROMPT = &quot;person . animal . bird . object&quot;
BOX_THRESHOLD = 0.35
TEXT_THRESHOLD = 0.25

# 加载图像
image_source, image = load_image(IMAGE_PATH)

# 执行预测
boxes, logits, phrases = predict(
model=model,
image=image,
caption=TEXT_PROMPT,
box_threshold=BOX_THRESHOLD,
text_threshold=TEXT_THRESHOLD
)

# 获取图像尺寸
ht, wd = image_source.shape[:2]
print(ht, wd, image_source.shape[:2])

# 将边界框转换为绝对坐标
abs_box = boxes * torch.tensor([wd, ht, wd, ht])
abs_box = [abs_bo.numpy().astype(&quot;int&quot;) for abs_bo in abs_box]

annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)

for abs_bo in abs_box:
cv2.rectangle(annotated_frame, (abs_bo[0], abs_bo[1]),[![enter image description here][1]][1] (abs_bo[2], abs_bo[3]), (255, 0, 0), 2)

cv2.imwrite(&quot;annotated_image.jpg&quot;, annotated_frame)



蓝色框由“绝对”坐标，任何关于如何操作返回的数据以获得绝对坐标的见解都将非常有帮助，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79393871/how-to-extract-bounding-box-coordinates-from-grounding-dinos-predict-function</guid>
      <pubDate>Tue, 28 Jan 2025 12:45:28 GMT</pubDate>
    </item>
    <item>
      <title>使用巨大且不同尺寸的图像训练 CNN</title>
      <link>https://stackoverflow.com/questions/79393407/training-a-cnn-with-huge-and-different-dimensions-of-images</link>
      <description><![CDATA[我有一个包含高维度图像（空表）的数据集，我正尝试将其用于分类目的。根据这些统计数据：

由于维度差异太大，因此无法统一维度，而且这可能会压缩表格的行，从而导致表格的结构发生变化。有什么想法可以处理这种情况吗？我相信这样的维度会消耗大量的计算资源，而我没有这些资源（我最多只有 8gb VRAM）。]]></description>
      <guid>https://stackoverflow.com/questions/79393407/training-a-cnn-with-huge-and-different-dimensions-of-images</guid>
      <pubDate>Tue, 28 Jan 2025 10:07:48 GMT</pubDate>
    </item>
    <item>
      <title>人工智能如何连续两次给我错误答案这个简单的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79393114/how-ai-give-me-the-wrong-answer-twice-in-a-row-of-this-simple-question</link>
      <description><![CDATA[早上好，程序员们，今天我出于好奇，尝试在 deep ai.org 上计算二进制到十六进制和八进制数。但每次都给我错误的答案。看：

另外：正确的答案：

看完这个，AI 没那么聪明XD]]></description>
      <guid>https://stackoverflow.com/questions/79393114/how-ai-give-me-the-wrong-answer-twice-in-a-row-of-this-simple-question</guid>
      <pubDate>Tue, 28 Jan 2025 08:17:46 GMT</pubDate>
    </item>
    <item>
      <title>使用“bitsandbytes”4 位量化需要最新版本的 bitsandbytes：“pip install -U bitsandbytes”</title>
      <link>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</link>
      <description><![CDATA[加载 tokenizer 时，我收到此错误：
ImportError：使用 bitsandbytes 4 位量化需要最新版本的 bitsandbytes：
pip install -U bitsandbytes。

我在 Macbook M2 pro 上使用 Jupyter 笔记本。
以下是源代码：
quant_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_type=&quot;nf4&quot;

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

base_model = AutoModelForCausalLM.from_pretrained(
BASE_MODEL,
quantization_config=quant_config,
device_map=&quot;auto&quot;,
)

base_model.generation_config.pad_token_id = tokenizer.pad_token_id

有人能帮忙吗？
我按照说明更新了 bitsandbytes，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</guid>
      <pubDate>Fri, 10 Jan 2025 03:52:41 GMT</pubDate>
    </item>
    <item>
      <title>optuna 试验可以被清除/删除吗？（处理随时间推移的政权更迭）</title>
      <link>https://stackoverflow.com/questions/75939676/can-optuna-trials-be-purged-deleted-dealing-with-regime-changes-over-time</link>
      <description><![CDATA[背景：我正在建模的 ML 问题会随着时间推移而随着不同的制度而变化 - 因此模型权重和最佳模型超参数会随着时间推移而变化。
问题：对于给定的 optuna.study，当我调用 study.best_trial.params 时，这可能会将最佳试验标识为过去的试验 - 这代表了对当前制度的最佳参数的过时观点。
次要问题：当 optuna 在优化过程中对超参数进行采样以进行评估时，它会使用所有试验的结果来概率地告知从何处进行采样。随着制度的变化，较旧的试验在此过程中的相关性降低。
问题：随着时间的推移，有没有办法清除、删除或忽略较旧的试验？这样上述两个问题就解决了吗？
反思：

也许我误解了 optuna 的工作原理。我认为获取 best_params 的最佳方法是调用 optuna.study.best_trial.params，它会查看所有历史试验并贪婪地找到具有最佳优化分数的试验并返回用于该运行的参数。
但也许有另一种方法可以获取过去 N 次试验中的 best_params？
或者也许我应该只关注从最后一次试验中获取最佳参数 - 这将很好地代表当前情况。我相信我可以使用 study.get_trials()[-1].params 来获得它。

感谢您的任何反馈/建议。]]></description>
      <guid>https://stackoverflow.com/questions/75939676/can-optuna-trials-be-purged-deleted-dealing-with-regime-changes-over-time</guid>
      <pubDate>Wed, 05 Apr 2023 12:53:15 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的 Transformers 库中 Trainer 使用的损失函数是什么？</title>
      <link>https://stackoverflow.com/questions/71581197/what-is-the-loss-function-used-in-trainer-from-the-transformers-library-of-huggi</link>
      <description><![CDATA[Hugging Face 的 Transformers 库中的 Trainer 中使用的损失函数是什么？
我正在尝试使用 Hugging Face 的 Transformers 库中的 Trainer 类 来微调 BERT 模型。
在他们的文档中，他们提到可以通过覆盖类中的 compute_loss 方法来指定自定义损失函数。但是，如果我不执行方法覆盖并使用 Trainer 直接微调 BERT 模型进行情绪分类，那么默认使用的损失函数是什么？是分类交叉熵吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/71581197/what-is-the-loss-function-used-in-trainer-from-the-transformers-library-of-huggi</guid>
      <pubDate>Wed, 23 Mar 2022 02:35:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 sotfmax_cross_entropy_with_logits 中将 logit 解释为“未缩放对数概率”？</title>
      <link>https://stackoverflow.com/questions/48483980/why-explain-logit-as-unscaled-log-probabililty-in-sotfmax-cross-entropy-with-l</link>
      <description><![CDATA[在 tensorflow 文档 (softmax_cross_entropy_with_logits) 中，他们说“logits：未缩放的对数概率”。什么是“对数概率”？
首先，我理解 &#39;logits&#39; 是 &#39;归一化之前的输出&#39; 或 &#39;类别的分数&#39;。
logits = tf.matmul(X,W) + b
hypothesis = tf.nn.softmax(logits)

如果我通过 tf.matmul(X,W) + b 得到 [1.5, 2.4, 0,7]，那么 [1.5, 2.4, 0,7] 就是 logits(score)，并且这是未缩放的。我能理解到这个阶段。但是，我不明白为什么 [1.5, 2.4, 0.7] 是 &#39;对数概率&#39;。]]></description>
      <guid>https://stackoverflow.com/questions/48483980/why-explain-logit-as-unscaled-log-probabililty-in-sotfmax-cross-entropy-with-l</guid>
      <pubDate>Sun, 28 Jan 2018 07:00:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我实现的深度神经网络在经过几次迭代后成本就会增加？</title>
      <link>https://stackoverflow.com/questions/47952812/why-does-the-cost-in-my-implementation-of-a-deep-neural-network-increase-after-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/47952812/why-does-the-cost-in-my-implementation-of-a-deep-neural-network-increase-after-a</guid>
      <pubDate>Sat, 23 Dec 2017 13:14:53 GMT</pubDate>
    </item>
    </channel>
</rss>