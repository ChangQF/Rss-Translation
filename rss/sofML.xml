<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Dec 2023 12:23:05 GMT</lastBuildDate>
    <item>
      <title>自定义梯度提升分类器实现。训练无进展</title>
      <link>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</link>
      <description><![CDATA[我正在尝试实现 GradientBoostingClassifier
我从 StatQuest 视频中获取了该算法（梯度提升第 4 部分（共 4 部分）：分类详细信息）并尝试使用 numpy + sklearn.DecisionTreeRegressor 作为基本模型来实现它
这是我的代码
从 sklearn.tree 导入 DecisionTreeRegressor

定义 sigmoid(x):
  如果x&gt; 0:
    z = np.exp(-x)
    返回 1/(1+z)
  别的：
    z = np.exp(x)
    返回 z/(1+z)

类 GradientBoostingClassifier()：
  def __init__(self, n_estimators=20, lr=0.1):
    self.n_estimators = n_estimators
    self.lr = lr
    self.training_history = {
        “log_loss”：[]，“roc_auc”：[]，“pr_auc”：[]
    }
    self.base_learners = []

  def fit(自身, X, y):
    数据 = X.copy()
    特征=数据.列
    # 1. 使用常量值初始化模型：
    p = y.sum() / len(y)
    赔率 = p / (1-p)
    log_odds = np.log(赔率)
    数据[&#39;cur_log_odds&#39;] = log_odds
    数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

    self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
    self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
    self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    # 2. 对于 m = 1 到 M：
    for _ in tqdm(range(self.n_estimators)):
      # 2.1 计算所谓的伪残差：
      数据[&#39;残差&#39;] = (数据[&#39;预测&#39;] - y)

      # 2.2 拟合基学习器回归器来预测伪残差：
      base_learner = DecisionTreeRegressor（max_深度 = 3，min_samples_split = 2，random_state = 42）
      base_learner.fit(数据[特征],数据[&#39;残差&#39;])
      self.base_learners.append(base_learner)

      # 2.3 对于每个叶子计算其输出对数几率
      # 从回归树中获取叶子数
      数据[&#39;叶子&#39;] = base_learner.apply(数据[特征])

      # 将输出对数奇数计算为 sum(residuals) / sum(old_prediction * (1 - old_prediction))
      leafs_output = data.groupby(&#39;leaf&#39;, as_index=False).apply(
          lambda d: d[&#39;残差&#39;].sum() / (0.00001+(d[&#39;预测&#39;] * (1-d[&#39;预测&#39;]) ).sum())
      ).rename(columns={无: &#39;lambda_odds&#39;})

      数据 = data.merge(leafs_output, on=&#39;leaf&#39;)
      # 2.4 更新 current_log_odds = current_log_odds + lr*predicted_log_odds
      数据[&#39;cur_log_odds&#39;] += self.lr*data[&#39;lambda_odds&#39;]
      数据 = data.drop(&#39;lambda_odds&#39;, axis=1)

      数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

      self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
      self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
      self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    返回数据

  def Predict_proba(自身, X):
    经过

问题是，即使经过 1000 次迭代（n_estimators = 1000），我的 roc_auc 和 pr_auc 分数也接近随机模型给出的分数（roc_auc=0.5，pr_auc=0.29，这是正类的比例）
你能告诉我我做错了什么吗？
即使 n_estimators = 10，sklearn GradientBoostingClassifier 实现在同一数据集上也能给出更高的分数]]></description>
      <guid>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</guid>
      <pubDate>Sun, 24 Dec 2023 12:11:08 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证期间出错，ValueError：发现未知类别</title>
      <link>https://stackoverflow.com/questions/77710473/error-during-cross-validation-valueerror-found-unknown-categories</link>
      <description><![CDATA[我的目标是预测电影的类型。我无法真正更改代码的主要结构，因为它是一个作业。
您可以在此处找到完整代码：https://github.com/pietrosaveri/ML_movies 
您可以在此处找到数据集：https://www.kaggle。 com/datasets/akshaypawar7/millions-of-movies
我正在尝试评估 ML 算法的不同组合，看看哪一种是最好的。
我相信此错误是由创建新列的 OnceHotEncoder 转换器造成的。还有一个错误：ConvergenceWarning：达到了 max_iter，这意味着 coef_ 没有收敛。
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350：ConvergenceWarning：达到了 max_iter，这意味着 coef_ 达到了不收敛
  警告.警告(
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:821：UserWarning：评分失败。这些参数的训练测试分区的分数将设置为 nan。细节：
回溯（最近一次调用最后一次）：
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py”，第 810 行，在 _score 中
    分数=记分器（估计器，X_测试，y_测试）
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 266 行，在 __call__ 中
    返回 self._score（部分（_cached_call，无），估计器，X，y_true，**_kwargs）
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 353 行，在 _score 中
    y_pred = method_caller(估计器, “预测”, X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 86 行，在 _cached_call 中
    结果，_ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_response.py”，第 194 行，在 _get_response_values 中
    y_pred = 预测方法(X)
             ^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py”，第 514 行，在预测中
    Xt = 变换.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py”，第 157 行，包装
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
...
    引发值错误（消息）
ValueError: 在转换过程中在第 0 列中发现未知类别 [&#39;fa&#39;, &#39;xx&#39;, &#39;he&#39;]

  警告.警告(
]]></description>
      <guid>https://stackoverflow.com/questions/77710473/error-during-cross-validation-valueerror-found-unknown-categories</guid>
      <pubDate>Sun, 24 Dec 2023 11:29:20 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试运行管道文件时，有关 zenml 中的 mlops 的错误出现 KeyError：“不存在名称为“local”且类型为“orchestrator”的风味。”</title>
      <link>https://stackoverflow.com/questions/77710193/error-regarding-mlops-in-zenml-when-i-try-to-run-pipeline-file-im-getting-keyer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77710193/error-regarding-mlops-in-zenml-when-i-try-to-run-pipeline-file-im-getting-keyer</guid>
      <pubDate>Sun, 24 Dec 2023 09:42:10 GMT</pubDate>
    </item>
    <item>
      <title>Edge Impulse 物体检测中的多个标签检测，无需相应产品</title>
      <link>https://stackoverflow.com/questions/77710120/multiple-label-detections-in-edge-impulse-object-detection-without-corresponding</link>
      <description><![CDATA[我正在使用 Edge Impulse 开发一个对象检测项目。当我启动相机时，我遇到了检测到多个标签的情况，即使框架中没有相应的实际产品。这就提出了一个问题：为什么这些标签在没有产品的情况下出现？
我已尝试检查数据并优化训练参数，但未能解决此问题。这可能是由于图像中的噪声或其他一些因素造成的吗？ Edge Impulse 中是否存在可能导致此行为的特定方法或技术？
任何有关解决此问题的见解或指导将不胜感激。谢谢。


]]></description>
      <guid>https://stackoverflow.com/questions/77710120/multiple-label-detections-in-edge-impulse-object-detection-without-corresponding</guid>
      <pubDate>Sun, 24 Dec 2023 09:07:02 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习将地址文本拆分为多个组件</title>
      <link>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</link>
      <description><![CDATA[我对机器学习完全陌生，所以不知道应该使用哪个方向或方法来解决我的问题。
问题：
我有一个 CSV 文件，每一行代表地址的不同组成部分，例如城市、街道、门牌号等，然后一列在一行中包含组合地址，具有预定义的格式，例如街道房屋号码、邮政编码、城市。
我想要的是判断用户输入的地址文本的不同组成部分，例如我想知道用户是否输入了所有组件，或者只是输入了街道名称和城市等，以及这些组件的值是什么。
问题：
我可以通过机器学习技术来实现这一目标，以便我使用 CSV 文件教导模型，这就是地址文本如何拆分为不同的组件，然后期望它根据该训练为我提供不同的组件？
我尝试向自己介绍 ML.NET，因为我是一名 C# 开发人员，并尝试查看 ML.NET 中是否有规定可以通过使用 CSV 文件训练模型来从给定文本字符串中获取多个标签。由于我对机器学习概念非常陌生，因此无法理解机器学习的不同术语和技术。]]></description>
      <guid>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</guid>
      <pubDate>Sun, 24 Dec 2023 08:47:00 GMT</pubDate>
    </item>
    <item>
      <title>向 Transformer 架构添加生成/预测元素</title>
      <link>https://stackoverflow.com/questions/77709804/adding-a-generative-predictive-element-to-transformer-architecture</link>
      <description><![CDATA[我偶然发现了标准变压器架构的一种变体，称为“iTransformer”，它基本上采用时间序列的倒转视图，并将每个变量的整个时间序列独立地嵌入到（变量）令牌中。它通过自注意力捕获多变量相关性，并利用层归一化和前馈网络模块来学习更好的序列全局表示以进行时间序列预测。
我正在尝试并想知道如何修改代码，以便它可以在没有基本事实的情况下生成/预测未来的时间序列数据。
这是原始的 github 存储库：
https://github.com/thuml/iTransformer/blob/34d9aa016917dc8c8db59bc00f9 b805ceea4042d/ data_provider/data_factory.py
我假设我必须修改测试函数，以便它可以接受之前的预测和输入来创建新的预测，直到未来 x 天？我一直在寻找一些正确方向的指针。]]></description>
      <guid>https://stackoverflow.com/questions/77709804/adding-a-generative-predictive-element-to-transformer-architecture</guid>
      <pubDate>Sun, 24 Dec 2023 05:56:12 GMT</pubDate>
    </item>
    <item>
      <title>如何将 model.safetensor 转换为 pytorch_model.bin？</title>
      <link>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</link>
      <description><![CDATA[我正在微调预训练的 bert 模型，但遇到了一个奇怪的问题：
当我使用 CPU 进行微调时，代码会像这样保存模型：

使用“pytorch_model.bin”。但是当我使用 CUDA（我必须这样做）时，模型会像这样保存：

当我尝试加载这个“model.safetensors”时将来，它会引发错误“pytorch_model.bin”未找到。我使用两个不同的 venv 来测试 CPU 和 CUDA。
如何解决这个问题？是版本问题吗？
我正在使用sentence_transformers框架来微调模型。
这是我的训练代码：
检查点 = &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;

word_embedding_model = models.Transformer(checkpoint,cache_dir=f&#39;model/{checkpoint}&#39;)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=&#39;mean&#39;)
模型 = SentenceTransformer(模块=[word_embedding_model, pooling_model], device=&#39;cuda&#39;)


train_loss = 损失.CosineSimilarityLoss(模型)

evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name=&#39;sbert&#39;)

model.fit(train_objectives=[(train_dataloader, train_loss)]，epochs=5，evaluator=evaluator，show_progress_bar=True，output_path=f&#39;model_FT/{checkpoint}&#39;，save_best_model=True)

我确实在两个不同的环境中尝试了测试，我希望代码能够保存一个“pytorch_model.bin”文件。不是“model.safetensors”。
编辑：我真的还不知道，但似乎是新版本的 Transformer 库导致了这个问题。我发现使用拥抱脸可以加载安全张量，但使用句子转换器（我需要使用）则不能。]]></description>
      <guid>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</guid>
      <pubDate>Sat, 23 Dec 2023 20:43:20 GMT</pubDate>
    </item>
    <item>
      <title>什么类型的 ML 模型可以检测过渡对中图像的哪一部分发生了变化？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</link>
      <description><![CDATA[假设我有一个图像对的数据集，其中每一对代表给定问题中的一个序列
例如，下面的一对图像代表 8-MNIST 拼图问题中的一个步骤（其中代表空白的“0”图块与“3”图块一起移动）。

对于每一对，我还有一个表示给定动作的 one-hot 向量，例如对于上面的对，one-hot 将为 [0 0 0 ... 0 1 0] 表示“” 0”瓷砖被推下
是否有一个模型可以从给定的图像和动作的标签中学习，以预测图像的哪一部分将被修改？ （给定示例中图像的“0 3”部分）。
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</guid>
      <pubDate>Sat, 23 Dec 2023 19:04:39 GMT</pubDate>
    </item>
    <item>
      <title>从口腔内扫描中检测点[关闭]</title>
      <link>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</link>
      <description><![CDATA[我们需要从口腔内扫描中检测点，但我们不知道如何开始或如何注释图像以训练模型来检测看不见的图像上的点。有人可以指导我们吗？
我们尝试了姿势检测和不同的代码进行训练，但数据不准确。]]></description>
      <guid>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</guid>
      <pubDate>Sat, 23 Dec 2023 18:49:39 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默病 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每个图像预测相同的类别（“CN - 认知正常”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
TFLite 模型的代码：https://colab.research.google。 com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=sharing
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）


解释器 = tf.lite.Interpreter(model_path=&quot;updated_final_model.tflite&quot;)
解释器.allocate_tensors()


class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
图像 = image.astype(&#39;float32&#39;) / 255.0
图像 = cv2.resize(图像, (150, 150))
图像 = np.expand_dims(图像, 轴=0)
返回图像

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
尝试：
    文件 = request.files[&#39;文件&#39;]
    image_file = Image.open(io.BytesIO(file.read()))
    图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
    print(&quot;输入图像形状：&quot;, image.shape)

    预处理图像 = 预处理图像（图像）

    
    terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)

    
    解释器.invoke()

    
    output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])

    Predicted_class = np.argmax(output_tensor, axis=1)[0]

  
    print(“预测类别索引：”,predicted_class)

    结果 = {“预测”：class_names[预测类]}
    返回 jsonify(结果)
除了异常 e：
    返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

我已尝试以下步骤来解决该问题：

我在 Flask API 代码的不同位置添加了打印语句，以了解执行流程。我预计会看到针对不同输入图像的不同预测，但 API 始终预测相同的类别（“CN - 认知正常”）。

我查看了加载的 TensorFlow Lite 模型并检查了其输入和输出详细信息。该模型似乎加载正确，并且输入图像按预期进行了预处理。我希望预测与模型的训练性能保持一致。

考虑到我的训练代码和 Flask API 之间颜色空间处理的差异，我尝试了不同的颜色空间转换。然而，这并没有解决问题。

我仔细检查了训练和 API 代码中的图像预处理步骤，以确保一致性。 Flask API 中的预处理与训练过程保持一致。


我希望 Flask API 能够根据输入图像提供多样化的预测，与训练期间模型的性能一致。每个输入图像都应该产生与其实际类别相对应的预测。
无论图像的实际内容如何，​​Flask API 都会一致地为每个输入图像预测相同的类别（“CN - 认知正常”）。这种行为与训练期间模型的准确性不一致。
TFLite 转换或模型准确性是否存在我可能忽略的潜在问题？]]></description>
      <guid>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Sat, 23 Dec 2023 18:35:50 GMT</pubDate>
    </item>
    <item>
      <title>Python Tensorflow.keras LSTM：类型错误：`generator` 产生了形状为 (36, 36, 147) 的元素，而预期形状为 (36, 147) 的元素</title>
      <link>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</guid>
      <pubDate>Sat, 23 Dec 2023 18:05:26 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制交叉验证的 AUROC 并找到最佳阈值？</title>
      <link>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</link>
      <description><![CDATA[在通过交叉验证评估我的机器学习模型时，我遇到了一个问题。我知道如何在交叉验证中绘制 AUROC 和每个折叠的相应阈值，但我不确定是否绘制所有折叠的平均 AUROC 及其相应阈值。
为此，我在Stack Overflow上探索了相关问题，并找到了相应的解决方案。您可以通过以下链接找到原始问题：[https://stackoverflow.com/questions/57708023/plotting-the-roc-curve-of-k-fold-cross-validation%5C]。尽管我成功生成了平均 ROC，但在准确绘制相应阈值方面遇到了挑战。为了解决这个问题，我根据自己的理解合并了额外的代码，但我不确定这种方法的正确性。
此外，我观察到使用 np.mean() 计算的平均 AUC 与使用 sklearn.metrics 计算的 AUC 值之间存在差异。因此，我正在寻求关于哪个值更准确以获得精确的 AUC 结果的指导。下面是我调整后的修改代码。
X，y = make_classification（n_samples = 1000，n_features = 20，n_classes = 2，random_state = 42）

cv = 分层KFold(n_splits=10)
分类器= SVC（内核=&#39;sigmoid&#39;，概率= True，random_state = 0）

tprs = []
曲线面积=[]
最佳阈值 = []
Mean_fpr = np.linspace(0, 1, 100)
plt.figure(figsize=(10,10))
我=0
对于火车，在 cv.split(X, y) 中测试：
    probas_ = classifier.fit(X[训练], y[训练]).predict_proba(X[测试])
    # 计算 ROC 曲线并计算曲线面积
    fpr, tpr, 阈值 = roc_curve(y[测试], probas_[:, 1])

    # 我添加的代码：
    最优阈值索引 = np.argmax(tpr-fpr)
    最优阈值 = 阈值[最优阈值索引]
    最佳阈值.append(最佳阈值)
    #

    tprs.append(np.interp(mean_fpr, fpr, tpr))
    tprs[-1][0] = 0.0
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    plt.plot(fpr, tpr, lw=1, alpha=0.3,
             label=&#39;ROC 折叠 %d (AUC = %0.4f)&#39; % (i, roc_auc))

    我 += 1



plt.plot([0, 1], [0, 1], 线型=&#39;--&#39;, lw=2, 颜色=&#39;r&#39;,
         标签=&#39;机会&#39;，alpha=.8)

mean_tpr = np.mean(tprs, 轴=0)
平均值_tpr[-1] = 1.0


mean_auc = auc(mean_fpr,mean_tpr)

# 我添加的代码：
np_mean_AUC = np.mean(aucs)
# print(f&quot;np_mean_AUC={np_mean_AUC},mean_auc={mean_auc}&quot;)
#

std_auc = np.std(aucs)

plt.plot(mean_fpr,mean_tpr,颜色=&#39;b&#39;,
         标签=r&#39;平均ROC (AUC = %0.4f $\pm$ %0.4f)&#39; % (np_mean_AUC, std_auc),
         lw=2，阿尔法=.8)

# 我添加的代码：
mean_optimal_threshold_index = np.argmax(mean_tpr-mean_fpr)
plt.annotate(f&#39;平均最佳阈值({np.mean(optimal_thresholds):.2f})&#39;,
                xy=(mean_fpr[平均最佳阈值索引],mean_tpr[平均最佳阈值索引]),
                xy 文本=(5, -5),
                textcoords=&#39;偏移点&#39;,
                arrowprops = dict（facecolor =&#39;红色&#39;，arrowstyle =&#39;楔形，tail_width = 0.7&#39;，shrinkA = 0，shrinkB = 10），
                颜色=&#39;红色&#39;）
#

std_tpr = np.std(tprs, 轴=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_ Between(mean_fpr, tprs_lower, tprs_upper, color=&#39;grey&#39;, alpha=.2,
                 标签=r&#39;$\pm$ 1 标准。开发。”）

plt.xlim([-0.01, 1.01])
plt.ylim([-0.01, 1.01])
plt.xlabel(&#39;误报率&#39;,fontsize=18)
plt.ylabel(&#39;真阳性率&#39;,fontsize=18)
plt.title(&#39;SVM的交叉验证ROC&#39;,fontsize=18)
plt.legend(loc=“右下”, prop={&#39;size&#39;: 15})
plt.show()

以下是输出：
在此处输入图像描述
请告诉我我在代码中所做的更改是否可以准确绘制用于交叉验证的 ROC 曲线以及相应的阈值，以及标记的 AUC 值是否有意义。]]></description>
      <guid>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</guid>
      <pubDate>Fri, 22 Dec 2023 07:42:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 R(spacy) 中的实体识别来匹配实体和个人</title>
      <link>https://stackoverflow.com/questions/77692435/matching-entities-individuals-using-entity-recognition-in-rspacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77692435/matching-entities-individuals-using-entity-recognition-in-rspacy</guid>
      <pubDate>Wed, 20 Dec 2023 14:47:19 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: JoypadSpace.reset() 有一个意外的关键字参数“seed”，当我运行以下代码时，我应该做什么来解决这个问题？</title>
      <link>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</link>
      <description><![CDATA[当我运行此代码时：
from nes_py.wrappers import JoypadSpace
进口健身房
导入gym_super_mario_bros
从gym_super_mario_bros.actions导入SIMPLE_MOVMENT
从gym.wrappers导入GrayScaleObservation
从 stable_baselines3.common.vec_env 导入 VecFrameStack,DummyVecEnv
从 matplotlib 导入 pyplot 作为 plt

env =gym_super_mario_bros.make(&#39;SuperMarioBros-v0&#39;,apply_api_compatibility=True,render_mode=“人类”)
env = JoypadSpace(env, SIMPLE_MOVMENT)
env = GrayScaleObservation(env,keep_dim=True)
env = DummyVecEnv([lambda:env])
env = VecFrameStack(env,4,channels_order=&#39;最后&#39;)
状态 = env.reset()

我收到以下错误：

我应该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</guid>
      <pubDate>Mon, 19 Jun 2023 19:47:30 GMT</pubDate>
    </item>
    <item>
      <title>来自 sklearn 的 GridSearchCV</title>
      <link>https://stackoverflow.com/questions/76011951/gridsearchcv-from-sklearn</link>
      <description><![CDATA[在使用 sklearn 的岭回归器和 GridSearchCV 构建线性回归时，我收到以下错误：
&#39;ValueError：估算器 Ridge() 的参数“ridge”无效。有效参数为：[&#39;alpha&#39;、&#39;copy_X&#39;、&#39;fit_intercept&#39;、&#39;max_iter&#39;、&#39;positive&#39;、&#39;random_state&#39;、&#39;solver&#39;、&#39;tol&#39;]。&#39;
我的代码如下：
X,y = Train_arr.iloc[:,:-1],Train_arr.iloc[:,-1]

从 sklearn.linear_model 导入 Ridge
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn.metrics 导入mean_squared_error

山脊 = 山脊()
param_grid = {&#39;ridge__aplha&#39;: np.logspace(-10,10,100)}

ridge_regressor = GridSearchCV(ridge, param_grid,scoring=&#39;neg_mean_squared_error&#39;,cv=5, n_jobs =-1)
ridge_regressor.fit(X,y)

如果有人能强调我哪里出错了，这导致了上述值错误，我将不胜感激。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/76011951/gridsearchcv-from-sklearn</guid>
      <pubDate>Fri, 14 Apr 2023 05:52:23 GMT</pubDate>
    </item>
    </channel>
</rss>