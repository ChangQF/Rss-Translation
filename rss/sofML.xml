<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Sep 2024 03:18:21 GMT</lastBuildDate>
    <item>
      <title>我应该做什么 Web 开发或 Ai/ML [关闭]</title>
      <link>https://stackoverflow.com/questions/78942412/what-should-i-do-web-development-or-ai-ml</link>
      <description><![CDATA[我是计算机科学专业的学生。我的第三年才刚刚开始，我已经学过 html、CSS 和 JavaScript。但我不确定我应该做 Ai/ML 还是 Web 开发。
我尝试过 Web 开发，但我不确定是否应该继续。]]></description>
      <guid>https://stackoverflow.com/questions/78942412/what-should-i-do-web-development-or-ai-ml</guid>
      <pubDate>Tue, 03 Sep 2024 01:40:05 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将 PyTorch CVAE 模型转换为 Java 程序？</title>
      <link>https://stackoverflow.com/questions/78942272/is-it-possible-to-convert-a-pytorch-cvae-model-into-a-java-program</link>
      <description><![CDATA[我编写了一个 Python 程序，可以使用 CVAE 模型架构生成图像。它使用 torch、torchvision、pillow、BertTokenizer 和 numpy 等库。它从 .pth 文件加载权重。我的代码如下所示：
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
from transformers import BertTokenizer
import numpy as np

LATENT_DIM = 128
HIDDEN_DIM = 256
model_path = &quot;./model.pth&quot;
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class TextEncoder(nn.Module):
def __init__(self, input_size, output_size):
super(TextEncoder, self).__init__()
self.fc = nn.Linear(input_size, output_size)

def forward(self, x):
return self.fc(x)

class CVAE(nn.Module):
def __init__(self, input_encoder):
super(CVAE, self).__init__()
self.input_encoder = input_encoder

self.encoder = nn.Sequential(
nn.Conv2d(4, 32, 3, stride=1, padding=1),
nn.ReLU(),
nn.Conv2d(32, 64, 3, stride=2, padding=1),
nn.ReLU(),
nn.Conv2d(64, 128, 3, stride=2, padding=1),
nn.ReLU(),
nn.Flatten(),
nn.Linear(128 * 4 * 4, HIDDEN_DIM),
)

self.fc_mu = nn.Linear(HIDDEN_DIM + HIDDEN_DIM, LATENT_DIM)
self.fc_logvar = nn.Linear(HIDDEN_DIM + HIDDEN_DIM, LATENT_DIM)

self.decoder_input = nn.Linear(LATENT_DIM + HIDDEN_DIM, 128 * 4 * 4)
self.decoder = nn.Sequential(
nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),
nn.ReLU(),
nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),
nn.ReLU(),
nn.Conv2d(32, 4, 3, stride=1, padding=1),
nn.Tanh(),
)

def encode(self, x, c):
x = self.encoder(x)
x = torch.cat([x, c], dim=1)
mu = self.fc_mu(x)
logvar = self.fc_logvar(x)
return mu, logvar

def decrypt(self, z, c):
z = torch.cat([z, c], dim=1)
x = self.decoder_input(z)
x = x.view(-1, 128, 4, 4)
return self.decoder(x)

def reparameterize(self, mu, logvar):
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
return mu + eps * std

def forward(self, x, c):
mu, logvar = self.encode(x, c)
z = self.reparameterize(mu, logvar)
return self.decode(z, c), mu, logvar

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

def generate_image(model, text_prompt, device):
coded_input = tokenizer(
text_prompt, padding=True, truncation=True, return_tensors=&quot;pt&quot;
)
input_ids =coded_input[&quot;input_ids&quot;].to(device)
tention_mask =coded_input[&quot;attention_mask&quot;].to(device)

使用 torch.no_grad():
text_encoding = model.text_encoder(input_ids,tention_mask)

z = torch.randn(1, LATENT_DIM).to(device)

使用 torch.no_grad():
generated_image = model.decode(z, text_encoding)

generated_image = generated_image.squeeze(0).cpu()
generated_image = (generated_image + 1) / 2
generated_image = generated_image.clamp(0, 1)
generated_image = transforms.ToPILImage()(generated_image)

return generated_image

def clean_image(image, Threshold=0.75):
np_image = np.array(image)
alpha_channel = np_image[:, :, 3]
alpha_channel[alpha_channel &lt;= int(threshold * 255)] = 0
alpha_channel[alpha_channel &gt; int(threshold * 255)] = 255
return Image.fromarray(np_image)

def gen(prompt):
text_encoder = TextEncoder(hidden_​​size=HIDDEN_DIM, output_size=HIDDEN_DIM)
model = CVAE(text_encoder).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
return clean_image(generate_image(model, prompt, device)).resize(
(16, 16), resample=Image.NEAREST
)

我需要将此生成器和模型嵌入到 jar 文件中，因此必须将脚本转换为 Java 代码。
我听说过 DL4J 之类的库，但我不知道如何使用它们，或者它们是否支持我想要实现的目标。对此事的任何见解都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78942272/is-it-possible-to-convert-a-pytorch-cvae-model-into-a-java-program</guid>
      <pubDate>Mon, 02 Sep 2024 23:50:39 GMT</pubDate>
    </item>
    <item>
      <title>在 Google colab 中克隆 GitHub 代码并解决库和依赖项版本不匹配错误</title>
      <link>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</link>
      <description><![CDATA[由于本地机器资源有限，我想使用 GitHub 中的代码在 Google colab 中使用 ML 进行面部表情识别。一个问题是 Google colab 的环境与其他环境不同，后者的代码结构很容易理解，比如 vs code。另一个问题是由于代码版本太旧而导致的库版本和依赖项错误。
以下是 GitHub 链接：
https://github.com/Talented-Q/POSTER_V2
我尝试在我的 Google colab 中从 GitHub 克隆代码，但执行文件非常复杂，因为文件太多，Google colab 环境与我们在本地（如 vs code）中使用的环境完全不同。而且我还收到了很多与库版本和依赖项相关的错误，解决起来非常有挑战性。]]></description>
      <guid>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</guid>
      <pubDate>Mon, 02 Sep 2024 18:29:56 GMT</pubDate>
    </item>
    <item>
      <title>RandomForest 模型未按预期工作</title>
      <link>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</link>
      <description><![CDATA[我想尝试预测股票价格。我知道股市基本上是一个随机过程，因此我不期望任何正回报。我编写了一个小脚本，使用随机森林回归器，该回归器已根据过去 20 年左右的 AAPL 股票数据（过去 100 天除外）进行训练。我使用过去 100 天作为验证。
根据开盘价/收盘价/最高价/最低价和交易量，我在数据框中创建了另外两列：收盘价的涨跌百分比和 days_since_start_column，因为如果我的判断正确，模型无法根据日期时间进行学习。
无论如何，这是代码的其余部分：
df = pd.read_csv(&#39;stock_data.csv&#39;)
df = df[::-1].reset_index()
df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;])
df[&#39;% Difference&#39;] = df[&#39;close&#39;].pct_change()

splits = [
{&#39;date&#39;: &#39;2020-08-31&#39;, &#39;ratio&#39;: 4},
{&#39;date&#39;: &#39;2014-06-09&#39;, &#39;ratio&#39;: 7},
{&#39;date&#39;: &#39;2005-02-28&#39;, &#39;ratio&#39;: 2},
{&#39;date&#39;: &#39;2000-06-21&#39;, &#39;ratio&#39;: 2}
]

用于 split 中的 split:
split[&#39;date&#39;] = pd.to_datetime(split[&#39;date&#39;])
split_date = split[&#39;date&#39;]
ratio = split[&#39;ratio&#39;]
df.loc[df[&#39;timestamp&#39;] &lt; split_date, &#39;close&#39;] /= 比率

df[&#39;days_since_start&#39;] = (df[&#39;timestamp&#39;] - df[&#39;timestamp&#39;].min()).dt.days
#data = r.json()
target = df.close
features = [&#39;days_since_start&#39;,&#39;open&#39;,&#39;high&#39;,&#39;low&#39;,&#39;volume&#39;]

X_train = (df[features][:-100])
X_validation = df[features][-100:]

y_train = df[&#39;close&#39;][:-100]
y_validation = df[&#39;close&#39;][-100:]

#X_train,X_validation,y_train,y_validation = train_test_split(df[features][:-100],target[:-100],random_state=0)

model = RandomForestRegressor()
model.fit(X_train,y_train)
predictions = model.predict(X_validation)

predictions_df = pd.DataFrame(columns=[&#39;days_since_start&#39;,&#39;close&#39;])
predictions_df[&#39;close&#39;] = predictions
predictions_df[&#39;days_since_start&#39;] = df[&#39;timestamp&#39;][-100:].values
plt.xlabel(&#39;日期&#39;)
#plt.scatter(df.loc[X_validation.index, &#39;timestamp&#39;], predictions, color=&#39;red&#39;, label=&#39;预测收盘价&#39;, alpha=0.6)
plt.plot(df.timestamp[:-100],df.close[:-100],color=&#39;black&#39;)
plt.plot(df.timestamp[-100:],df.close[-100:],color=&#39;green&#39;)
plt.plot(predictions_df.days_since_start,predictions_df.close,color=&#39;red&#39;)
plt.show()

我用黑色绘制了过去几年截至最近 100 天的收盘价，用绿色绘制了最近 100 天的收盘价，用红色绘制了最近 100 天的预测收盘价。这是结果（过去 100 天）：

为什么价格大幅上涨后模型保持平稳？我在训练过程中做错了什么吗？我的验证数据集太小了吗？还是这只是超参数调整的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</guid>
      <pubDate>Mon, 02 Sep 2024 17:53:54 GMT</pubDate>
    </item>
    <item>
      <title>huggingface_hub/file_download.py 收到 urllib3 ConnectTimeoutError</title>
      <link>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</link>
      <description><![CDATA[尝试复制代码：https://github.com/ximinng/DiffSketcher.Do 所有环境配置操作均成功完成，但在尝试运行代码启动模型时出现错误：在此处输入图像描述、在此处输入图像描述、在此处输入图像描述。如何处理？请帮忙！！！！！！我尝试用 huggingface 上发布的其他相同模型替换模型（runwayml/stable-diffusion-v1-5-ov）
替代模型：在此处输入图片说明。
但我只能更改代码中的这一行，无法解决。我不知道如何直接替换它：在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</guid>
      <pubDate>Mon, 02 Sep 2024 15:43:01 GMT</pubDate>
    </item>
    <item>
      <title>异常检测：如何找出数据集最后一天的800个混合样本？</title>
      <link>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</link>
      <description><![CDATA[有 8000 名患者。 4 年内共进行了 22666 组测试。
每位患者至少进行了两组在不同日期进行的测试。
每组测试有 11 个不同的实验室
（实验室：白蛋白、ALP、ALT、AST、BUN、肌酐、GGT、葡萄糖、LDH、总胆红素、总蛋白）
每 11 个测试彼此独立，参考范围各不相同。
医院 A 的患者临时涌入导致最后一天数据集中的标本溢出（4000 个测试集）
计算机错误导致 4000 个测试集中的 800 多个测试集的结果在患者高峰当天被逆转。
如何找到被逆转的 800 个标本？
我尝试使用非混合日进行训练（不包括最后一天的数据）使用 xgboost，预测最后一天测试的 11 个测试结果。我试图找出实际值和预测值之间的差异，以找出具有多个异常值的样本。但 F1 分数很差。我还尝试使用 min-max 或 z-score 进行规范化，但仍然没有改善。我想我必须研究不同的方法。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</guid>
      <pubDate>Mon, 02 Sep 2024 13:10:44 GMT</pubDate>
    </item>
    <item>
      <title>并行化原生单批次 PyTorch 模型</title>
      <link>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</link>
      <description><![CDATA[是否可以并行化（原生）单批模型？
通常，并行化是通过 torch.bmm（批处理矩阵乘法）而不是 torch.matmul 来完成的，并且专门为批处理固定一个维度。但是，例如对于 torch.tensordot 函数，这不可用。
因此，如果有这样的模型，是否可以并行计算批处理的每个梯度？理想情况下，并行化应该适用于训练和推理。
代码示例：
import torch
import torch.nn as nn

class LinearMultidimModel(nn.Module):
def __init__(self, input_dim, output_dim):
super(LinearMultidimModel, self).__init__()
self.weight = nn.Parameter(torch.randn(input_dim, hidden_​​dim, output_dim))
self.bias = nn.Parameter(torch.randn(output_dim))

def forward(self, x):
# 使用 torch.tensordot 执行线性变换
out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
return out

# 示例用法
input_dim = 3
hidden_​​dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# 虚拟输入
x = torch.randn(input_dim, hidden_​​dim)# 但是如果我想放入一个批次，torch.randn(batch_size, input_dim, hidden_​​dim) 怎么办？
output = model(x)
print(output)

请记住，如果没有 hidden_​​dim，它会原生地进行并行化，可以完全删除 hidden_​​dim 并使用 获得结果
x = torch.randn(5, input_dim)。
我尝试过使用 Einsum，但它适用于固定数量的隐藏维度...]]></description>
      <guid>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</guid>
      <pubDate>Mon, 02 Sep 2024 12:42:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Vast.Ai 托管机器学习 Flask 应用程序[关闭]</title>
      <link>https://stackoverflow.com/questions/78940345/how-to-host-mochine-learning-flask-application-with-vast-ai</link>
      <description><![CDATA[我想托管我的 Flask 应用程序，该应用程序包含其推理在 gpu 上进行的模型，这就是我决定使用 vast.ai 的原因，它提供便宜且优质的 gpu，但我不知道如何为我的模型托管设置 vast.ai
请问您能否提供一些关于如何在 vast.ai 上托管我的项目的教程或指南，它与其他实时服务器有何不同？]]></description>
      <guid>https://stackoverflow.com/questions/78940345/how-to-host-mochine-learning-flask-application-with-vast-ai</guid>
      <pubDate>Mon, 02 Sep 2024 11:58:30 GMT</pubDate>
    </item>
    <item>
      <title>我无法训练 vit_base_patch16_224 模型来为屏幕截图创建高质量的嵌入</title>
      <link>https://stackoverflow.com/questions/78939966/im-failing-to-train-a-vit-base-patch16-224-model-for-creating-high-quality-embe</link>
      <description><![CDATA[我创建了一个使用 vit_base_patch16_224 作为编码器的自动编码器。我使用大约 100_000 张屏幕截图进行了几个轮次的训练，但解码器似乎根本没有学会解码。损失似乎在减少。也许你们中的一些人对我可能做错的事情有一些有用的建议。
这是自动编码器类：
class ViTAutoEnc(nn.Module):
def __init__(self, vit_model=&#39;vit_base_patch16_224&#39;):
super(ViTAutoEnc, self).__init__()

# 使用预先训练的 Vision Transformer 作为编码器
self.encoder = create_model(vit_model, pretrained=True)
self.encoder.head = nn.Identity()
# 从 ViT 模型中提取特征维度
self.vit_embedding_dim = self.encoder.embed_dim

self.decoder = nn.Sequential(
nn.Linear(768, 768*7*7),
nn.Unflatten(1, torch.Size([768, 7, 7])),
nn.ConvTranspose2d(768, 512, kernel_size=4, stride=2, padding=1), # 7x7 -&gt; 14x14
nn.ReLU(inplace=True),
nn.Conv2d(512, 512, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), # 14x14 -&gt; 28x28
nn.ReLU(inplace=True),
nn.Conv2d(256, 256, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 28x28 -&gt; 56x56
nn.ReLU(inplace=True),
nn.Conv2d(128, 128, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # 56x56 -&gt; 112x112
nn.ReLU(inplace=True),
nn.Conv2d(64, 64, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # 112x112 -&gt; 224x224
nn.ReLU(inplace=True),
nn.Conv2d(32, 32, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.Conv2d(32, 3, kernel_size=1), # 调整通道
nn.Sigmoid() # 将输出标准化为 [0, 1] 
)

count = sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)
print(f&#39;解码器有 {count/10**6:.2f} 百万个可训练参数&#39;)

count = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)
print(f&#39;编码器有 {count/10**6:.2f} 百万个可训练参数&#39;)

def forward(self, x, decrypt=True):
# 编码器通过ViT

x = self.encoder.forward_features(x).mean(dim=1) # 从 ViT 获取特征 (batch_size, vit_embedding_dim)
x = F.normalize(x, p=2, dim=1) # 沿特征维度进行归一化

# 解码器传递
if decrypt:
x = self.decoder(x) # 解码
return x

这是项目 git。
损失看起来像这样

编码器有 8580 万个参数，解码器有 4116 万个参数。我使用的是 Adam 优化器 lr=0.00001。批量大小为 64。]]></description>
      <guid>https://stackoverflow.com/questions/78939966/im-failing-to-train-a-vit-base-patch16-224-model-for-creating-high-quality-embe</guid>
      <pubDate>Mon, 02 Sep 2024 10:21:45 GMT</pubDate>
    </item>
    <item>
      <title>水印灰度图像 PSNR 较好，但水印提取 BER 较高</title>
      <link>https://stackoverflow.com/questions/78939872/high-ber-in-watermark-extraction-despite-good-psnr-in-watermarking-grayscale-ima</link>
      <description><![CDATA[我正在开展一个为灰度图像添加水印的项目，尽管重建图像的峰值信噪比 (PSNR) 很高，但提取的水印中还是出现了高误码率 (BER) 的问题。下面是我使用的详细过程：

输入准备：


我从 128x128 灰度图像开始，我将其称为“原始”图像。

我将这些图像重塑为 126x128，然后将它们与 16x16 二进制水印图像（其中像素值为 0 或 1）连接起来。

此连接产生 128x128 的“连接”作为我的模型输入的图像。



模型架构：


第一个 CNN：该网络将 128x128 连接图像作为输入，并输出 128x128 灰度图像。目标是使此输出图像尽可能接近原始 128x128 灰度图像。

第一个 CNN 的损失函数：我使用 PSNR 的倒数（1/PSNR（原始，输出））作为此 CNN 的损失。 PSNR 通常在 40 dB 以上，这表明重建效果良好。

第二个 CNN：第二个网络获取第一个 CNN 的输出并尝试提取 16x16 水印图像。

第二个 CNN 的损失函数：我使用原始水印和提取的水印之间的误码率 (BER) 作为此 CNN 的损失。不幸的是，BER 仍然非常高，大约为 50%。


问题：
虽然重建图像的 PSNR 很好，但高 BER 导致总损失约为 50%，这对我的应用来说是不理想的。
这是损失计算的代码：
def psnr_loss(output, target):
mse_loss = nn.MSELoss()(output, target) # 均方误差
if mse_loss == 0:
return torch.tensor(float(&#39;inf&#39;)).to(output.device) # 处理零除的情况
L = 255.0 # 图像中的最大像素值
psnr_value = 10 * torch.log10(L**2 / mse_loss)
return psnr_value # 返回 PSNR（单位为 dB）

def ber_loss(output, target, Threshold=0.5):
# 对输出进行阈值处理，将其转换为二进制（0 或 1）
output_binary = (output &gt; Threshold).float()

# 计算误码数
bit_errors = torch.sum(torch.abs(output_binary - target))

# 计算总位数
total_bits = target.numel()

# 将 BER 计算为误码与总位数的比率
ber = bit_errors / total_bits

return ber

# 组合损失类
class CombinedLoss(nn.Module):
def __init__(self, ber_weight=1.0):
super(CombinedLoss, self).__init__()
self.ber_weight = ber_weight

def forward(self, watermark_output, watermark_target, image_output, image_target):
# 计算逆 PSNR 以最小化
psnr = 1 / psnr_loss(image_output, image_target)

# 计算 BER（误码率）
ber = ber_loss(watermark_output, watermark_target)

# 返回组合损失
return psnr + self.ber_weight * ber

这里我还附上了训练的输出。
带有损失、PSNR 和 BER 的模型输出]]></description>
      <guid>https://stackoverflow.com/questions/78939872/high-ber-in-watermark-extraction-despite-good-psnr-in-watermarking-grayscale-ima</guid>
      <pubDate>Mon, 02 Sep 2024 09:54:18 GMT</pubDate>
    </item>
    <item>
      <title>如果模型在训练集上表现正常但在验证集上表现异常，这意味着什么[关闭]</title>
      <link>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</link>
      <description><![CDATA[我尝试将堆叠在一起的 25x25 像素图像分类为 50x25 像素图像是相同 (1) 还是不同 (0)。我使用 keras 创建 NN 层。 Keras 顺序层如下所示：
layers.Input((2*imsize,imsize,3)), # 具有 3 个通道的输入形状
layers.Reshape((2,imsize,imsize,3)), # 将输入转换为两个 25x25 图像
layers.LayerNormalization(axis=[-1,-2,-3]), # 规范化图像
layers.Flatten(), # 展平数组
layers.Dense(16,activation=&#39;relu&#39;), # 16 个输出隐藏层
layers.Dense(2,activation=&#39;softmax&#39;) 

然后我使用 adam 优化器编译了这些层，损失和准确率如下：
ml.compile(optimizer=&#39;adam&#39;,
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
metrics=[&#39;accuracy&#39;])

之后，我使用 epoch=20 和 batch_size=100 训练模型。我根据 epoch 绘制了这些结果。
结果


当前评估：
我目前的观察是

模型过度拟合，因为它仅在训练集上表现正常？
模型是学习到了错误的东西，因为损失在验证集上不减反增

我的问题是：我对模型的评估正确吗？我应该如何理解这个结果以便改进它？
更新：
相同（1）的示例数据集：

不同（0）的示例数据集：
]]></description>
      <guid>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</guid>
      <pubDate>Mon, 02 Sep 2024 04:51:20 GMT</pubDate>
    </item>
    <item>
      <title>自定义链接矩阵、树状图标签不正确</title>
      <link>https://stackoverflow.com/questions/78938314/custom-linkage-matrix-dendrogram-labels-arent-correct</link>
      <description><![CDATA[我创建了自己的链接函数，它生成具有正确 scipy 格式（n-1x4）的链接矩阵。
当我尝试将树状图函数与用于链接的标签一起使用时，树状图显示在迭代中某些序列被合并，但链接矩阵显示其他序列在同一迭代中被合并。
添加图像：
树状图序列 ID
这些是实际合并的序列：“步骤 1：合并 80c8714e-1899-407e-960d-5391bfba9a75 和 b4a3e9f4-aae9-41d4-930a-9d2422e1b37f”
如您所见，这些是不同的序列。
我想问一下我是否遗漏了什么有一种方法可以使用链接矩阵来创建序列标签，因为它们是正确的。
（此外，树状图显示正确的输出，只有标签不正确）
这是树状图的代码
sequence_string_data = [code_from_before]
linkage_matrix = clustering(sequence_string_data)
seq_ids = list(sequences.keys())
seq_labels = [f&quot;{seq_id} (size={len(sequences[seq_id])})&quot; for seq_id in seq_ids]

plt.figure(figsize=(15, 10))
dendro = dendrogram(linkage_matrix, labels=seq_labels, leaf_rotation=90, leaf_font_size=8)

我尝试更改这两个函数以查看是否还有其他问题，但无济于事。]]></description>
      <guid>https://stackoverflow.com/questions/78938314/custom-linkage-matrix-dendrogram-labels-arent-correct</guid>
      <pubDate>Sun, 01 Sep 2024 20:25:26 GMT</pubDate>
    </item>
    <item>
      <title>数据分类不适用于 BERT 模型</title>
      <link>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</guid>
      <pubDate>Sun, 01 Sep 2024 00:59:26 GMT</pubDate>
    </item>
    <item>
      <title>Yolov9 C++ 推理输出的 x、y、宽度和高度不符合预期</title>
      <link>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</guid>
      <pubDate>Wed, 28 Aug 2024 12:54:32 GMT</pubDate>
    </item>
    <item>
      <title>无需深度学习或 Tesseract 的文本图像二元分类器</title>
      <link>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</link>
      <description><![CDATA[我有 20k 张小标签图像，每张图像都有单词“Back”或“Front”。
图像分辨率为全部 (200px, 25px)

我可以使用 tesseract_OCR 对这些图像进行 100% 准确率的分类。
 txt = pytesseract.image_to_string(img, lang=&#39;eng&#39;)
if &quot;Front&quot; in txt:
return &quot;Front&quot;
if &quot;Back&quot; in txt:
return &quot;Back&quot;

问题是，它太慢了（20k 张图像需要 1 小时）并且需要安装 OCR 包。
我知道即使是 3 层的简单 CNN 也能很好地运行，但我认为这个问题似乎可以用简单的算法解决，而不需要复杂的技术。
你能给我推荐一种新方法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</guid>
      <pubDate>Wed, 07 Aug 2024 06:46:36 GMT</pubDate>
    </item>
    </channel>
</rss>