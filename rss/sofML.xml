<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 15 Feb 2024 12:23:31 GMT</lastBuildDate>
    <item>
      <title>如何进行“次主题情感”分析？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</link>
      <description><![CDATA[我必须找到句子中的子主题以及它们背后的情感，我面临的问题是一个句子可以有多个子主题，而我不知道不明白如何对他们单独进行情感分析。我该如何解决这个问题？
看下面的例子：
一个轮胎丢失，因此安装两个轮胎的时间有所延迟。车库处理这个问题的方式非常棒。
如果我们看一下上述评论的次主题情绪，我们会更清楚地了解这些内容是什么
一般都是。
发送的轮胎不正确为负 车库服务为正 等待时间为负
数据集：

我能够弄清楚如何在句子中找到子主题，但我无法单独找到每个子主题背后的情感，而只能找到整个句子的情感。
我一直在尝试让它们一起工作，但我不知道该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</guid>
      <pubDate>Thu, 15 Feb 2024 09:03:04 GMT</pubDate>
    </item>
    <item>
      <title>与未知数据的实体匹配[关闭]</title>
      <link>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</link>
      <description><![CDATA[相似的训练目标数据集对执行效果不好吗？
我将在数据帧上完成实体​​匹配任务。我决定使用基于机器学习的方法并使其通用。在这里，主要问题是我没有任何关于我的特定目标的标记数据集（即我有关于具有属性的人的标记数据集，但我想在关于汽车的数据帧上进行实体匹配）。我应该考虑哪种方法？如果我使用与目标没有相似性的标记数据集来训练模型，它会运行良好吗？如果可能的话，最佳介质是什么？在Google Scholar上，主要使用DITTO和DeepMatcher方法，但这些模型的f1分数是通过类似的训练目标数据集对观察的。]]></description>
      <guid>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</guid>
      <pubDate>Thu, 15 Feb 2024 08:30:49 GMT</pubDate>
    </item>
    <item>
      <title>我有一个 1510X132 功率输入和输出数据的数据集，我必须在数据上应用 DNN 模型来查找错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</link>
      <description><![CDATA[我已经应用了该模型并得到了 mse、mae 错误。我已经预测了数据，现在我必须绘制实际数据与预测数据图，以检查预测数据和实际数据是否彼此相似。我无法编码
实际=pd.DataFrame()
#y=pd.DataFrame()
j=1
对于范围内的 i(0, len(df.columns)-4, 4)：
     #实际[j]=df.iloc[0:1, i+1:i+5]
     实际[j] = df.iloc[0:2, i+1]
     实际[j+1] = df.iloc[0:2, i+2]
     #y[[j, j+1]]=df.iloc[:, i+3:i+5]
    `j+4`

plt.figure(figsize=(10, 6))
plt.plot((实际), label=&#39;实际&#39;)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</guid>
      <pubDate>Thu, 15 Feb 2024 06:21:44 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中运行 train 方法时将 PNG 图像转换为 np.array</title>
      <link>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</guid>
      <pubDate>Thu, 15 Feb 2024 01:22:49 GMT</pubDate>
    </item>
    <item>
      <title>集合预测中要缩放什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77997900/what-to-scale-in-ensemble-predictions</link>
      <description><![CDATA[我正在使用堆叠集成方法进行股票预测，但不知道什么和什么不应该扩展。预测训练和测试都是在 x 训练和测试上进行缩放的。我也不知道验证集应该在训练集还是测试集上。
这是代码：
feature_scaler = MinMaxScaler(feature_range=(0, 1))
    Predictions_train_scaled = feature_scaler.fit_transform(predictions_train)
    Predictions_test_scaled = feature_scaler.transform(predictions_test)

    target_scaler = MinMaxScaler(feature_range=(0, 1))
    y_train_shape = y_train.values.reshape(-1, 1)
    y_train_scaled = target_scaler.fit_transform(y_train_shape)
    y_test_shape = y_test.values.reshape(-1, 1)
    y_test_scaled = target_scaler.transform(y_test_shape)


    X_train_meta, y_train_meta_un = train_test_split(predictions_train_scaled,
    y_train_scaled、test_size=0.2、random_state = 1）
    X_val_meta, y_val_meta_un = train_test_split(predictions_test_scaled, y_test_scaled,
    测试大小=0.3，随机状态=1）

    y_train_meta = y_train_meta_un.reshape(-1, 1)
    y_val_meta = y_val_meta_un.reshape(-1, 1)

    #meta_model = 线性回归()
    meta_model = MLPRegressor(hidden_​​layer_sizes=(100, 50)，激活=&#39;relu&#39;，求解器=&#39;adam&#39;，alpha=0.001，random_state=42，max_iter=5000)
    meta_model.fit(X_train_meta, y_train_meta.ravel())

    train_meta_score = meta_model.score(X_train_meta, y_train_meta)
    print(f&quot;元模型训练分数：{train_meta_score}&quot;)
    
    val_score = meta_model.score(X_val_meta, y_val_meta)
    print(f“元模型验证分数：{val_score}”)

    cv_scores = cross_val_score(meta_model, X_train_meta, y_train_meta.ravel(), 评分=&#39;neg_mean_squared_error&#39;, cv=5)
    cv_rmse = np.sqrt(-cv_scores)
    print(f&quot;交叉验证 RMSE: {cv_rmse.mean()}&quot;)

    #predictions_test_scaled = feature_scaler.transform(predictions_test)
    stacked_predictionss = meta_model.predict(predictions_test_scaled)

    stacked_predictions = target_scaler.inverse_transform(stacked_predictionss.reshape(-1, 1))

    返回 stacked_predictions, stacked_predictionss
]]></description>
      <guid>https://stackoverflow.com/questions/77997900/what-to-scale-in-ensemble-predictions</guid>
      <pubDate>Thu, 15 Feb 2024 00:32:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 LoRA 微调 ControlNet？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</link>
      <description><![CDATA[我想使用 LoRA 适配器微调 ControlNet。我的数据集由成对图像组成 - 分割图（没有家具的房间，只有墙壁、地板、窗户、门） - RGB 图像（有家具的房间）。我的目标是微调 ControlNet 以根据空房间的分割图生成带家具的房间（我使用空提示）
所以，我不太清楚如何训练 LoRA。我考虑了两个选择

训练 LoRA 进行稳定扩散（仅使用带家具的房间图像作为数据集），然后将此稳定扩散注入 ControlNet

例如，如果我使用这个管道
管道 = StableDiffusionControlNetPipeline.from_pretrained(
    “runwayml/stable-diffusion-v1-5”，controlnet=controlnet，torch_dtype=torch.float16
）

然后将“runwayml/stable-diffusion-v1-5”替换为与我精心调校的 LoRA 一起。这是正确的做法吗？如果我仅微调稳定扩散并且不显示我的分割图，ControlNet 是否能够生成房间？

在图像对上训练 LoRA for ControlNet。
然而，我读到 Huggingface 不支持为 ControlNet 训练 LoRA（所有教程仅展示如何训练稳定扩散）。但我找到了文章
https://fangchuan.github.io/ctrl-room.github.io/&lt; /a&gt;
作者对 ControlNet 进行了微调以生成房间全景图（但文章没有提供技术细节），因此我认为可以直接为 ControlNet 训练 LoRA

那么什么方法来微调 ControlNet 是普遍可以接受的呢？]]></description>
      <guid>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</guid>
      <pubDate>Wed, 14 Feb 2024 21:19:17 GMT</pubDate>
    </item>
    <item>
      <title>有没有方法可以识别 XGBoost 树适合哪个数据子样本？</title>
      <link>https://stackoverflow.com/questions/77997182/are-there-ways-to-identify-which-subsample-of-data-an-xgboost-tree-was-fitted-on</link>
      <description><![CDATA[我想确定为每棵树的训练随机选择哪些特定的样本行（根据子样本参数）。例如，训练集有 100 行，子采样率为 0.7，我想知道为每棵树的训练选择的确切 70 行。我一直在到处寻找解决方案，但没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/77997182/are-there-ways-to-identify-which-subsample-of-data-an-xgboost-tree-was-fitted-on</guid>
      <pubDate>Wed, 14 Feb 2024 20:58:25 GMT</pubDate>
    </item>
    <item>
      <title>UndefinedMetricWarning：精度定义不明确，由于没有预测样本而被设置为 0.0 [重复]</title>
      <link>https://stackoverflow.com/questions/77995496/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-due-to-no</link>
      <description><![CDATA[当我尝试拟合 X 和 Y 训练集时，它会向我发出警告消息：
UndefinedMetricWarning：精度定义不明确，由于没有预测样本而被设置为 0.0。使用“zero_division”参数来控制此行为。 _warn_prf(平均值，修饰符，f“{metric.capitalize()}是”，len(结果))
这导致我的精确率、召回率和 F1 分数为 0。但是添加后
average=&#39;weighted&#39; 和 labels，它产生了可用的分数：
f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;, labels=np.unique(y_pred))

但是我的问题是我的 Y_pred 保持不变，每当我制作 ConfusionMatrix 时，它都会显示有 0 个 True 或 False Positive。有没有办法改变我的 Y_pred，以反映我修正的分数？
如上所述，我修复了分数，但 Y_pred 仍然不变。]]></description>
      <guid>https://stackoverflow.com/questions/77995496/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-due-to-no</guid>
      <pubDate>Wed, 14 Feb 2024 15:37:39 GMT</pubDate>
    </item>
    <item>
      <title>一类 SVM - 测试集上的异常值相对于训练集非常低</title>
      <link>https://stackoverflow.com/questions/77995464/one-class-svm-outliers-on-test-set-very-low-relative-to-training-set</link>
      <description><![CDATA[我正在使用 scikit-learn 一类 SVM 进行异常值检测。但相对于训练集，测试集上检测到的异常值数量非常少。
单类 SVM 的每个输入都是三个浮点数 [float1、float2、float3] 的列表。
所有列都使用最小-最大缩放比例缩放为 0 到 1 之间的值。
我按如下方式初始化并拟合 SVM：
clf = OneClassSVM(kernel=&#39;线性&#39;, nu=0.01, gamma=&#39;auto&#39;).fit(training_and_testing_sets[:TRAINING_SET_SIZE])

因为我对 nu 使用了 0.01 的值。我预计测试集上的异常值数量为整个测试集的 1%。但它是 0.004%。测试集也相应地缩放。
造成这种差异的原因是什么以及如何解决该问题？]]></description>
      <guid>https://stackoverflow.com/questions/77995464/one-class-svm-outliers-on-test-set-very-low-relative-to-training-set</guid>
      <pubDate>Wed, 14 Feb 2024 15:33:33 GMT</pubDate>
    </item>
    <item>
      <title>NLP 中的搜索、NER 和关系提取用例</title>
      <link>https://stackoverflow.com/questions/77992256/search-ner-and-relation-extraction-use-case-in-nlp</link>
      <description><![CDATA[我正在尝试对维基百科的文本创建 NER 和摘要。我正在使用维基页面中的文本： https://en.wikipedia.org/wiki/Hyderabad&lt; /a&gt;
下面是我的代码：
from txtai.embeddings import Embeddings
导入系统
将渐变导入为 gr
导入spacy
从字符串导入标点符号


# 创建嵌入模型，由句子转换器和句子转换器支持变形金刚
嵌入=嵌入（{“路径”：“../Lib/site-packages/all-MiniLM-L6-v2”}）

f = open(&#39;海得拉巴.txt&#39;, &#39;r&#39;, 编码=&#39;ISO-8859-1&#39;)
data = f.read().split(“.”)

# 为文本列表建立索引
嵌入.index（数据）

def 搜索（查询）：
    uid = embeddings.search(查询, 1)[0][0]
    返回数据[uid]

def 响应（问题）：
    合并=搜索（问题）
    ans = 代词_coref(合并)
    返回答案

演示 = gr. 接口(
    fn=响应，
    输入=[“文本”]，
    输出=[“文本”]，
）
演示.launch(share=False)

搜索：什么是 Purana pul？
响应：两者之间由许多跨河桥梁连接，其中最古老的是 Purana Pul（“老桥”），建于公元 1578 年。
我想加强对以下内容的响应：旧城和新城由许多横跨穆西河的桥梁连接起来，其中最古老的是 Purana Pul（“旧桥”），建于公元 1578 年.
NER 可以通过 en_core_web_md 模型完成。
如何在响应中将代词替换为适当的名词？]]></description>
      <guid>https://stackoverflow.com/questions/77992256/search-ner-and-relation-extraction-use-case-in-nlp</guid>
      <pubDate>Wed, 14 Feb 2024 05:33:45 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测访问日期与客户类别图不准确</title>
      <link>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</link>
      <description><![CDATA[我正在尝试对一堆类和日期时间进行时间序列预测，但由于某种原因我的图表看起来像这样，我的完整代码如下：
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
plt.plot(out.index, out[&#39;count&#39;], )



而我从其中获取此时间序列代码的博客有这样的结果

所以我不确定是否应该继续XD
我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23
我尝试了下面的代码，但由于某种原因，第一个图是空的
plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
out.groupby(&#39;class_id&#39;).plot()
plt.plot(out.index, out[&#39;count&#39;], )

]]></description>
      <guid>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</guid>
      <pubDate>Wed, 31 Jan 2024 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>三重态损失时间序列</title>
      <link>https://stackoverflow.com/questions/77879087/triplet-loss-time-series</link>
      <description><![CDATA[我正在阅读这篇论文，我想知道三重态损失在一般情况下是如何工作的时间序列、正样本和上下文。
假设我们有一个包含 3 个样本、一个特征和序列长度为 6 的数据集，如下所示：
&lt;前&gt;&lt;代码&gt;y1 = [1,3,4,7,5,2]
y2 = [1,2,3,4,5,6]
y3 = [9,8,4,6,2,1]

如果我遵循给定论文的逻辑，三元组将生成如下：
s_i = 6 # yi 的长度
s_pos = 3 # 随机长度
s_ref = 3+2 # s_pos 和 之间随机s_i

x_ref = [1,2,3,4,5]
x_pos = [2,3,4]

...
x_1_neg = [3,4,7]

现在为了减少损失，我们希望找到给定锚点 x_ref 的 x_pos。
但是，编码器找到正确的对不是很容易吗？因为 x_pos 包含在 x_ref 中，而 x_1_neg 则不包含？仅向编码器提供 x_pos 周围的上下文（例如 [1,...,5]）不是更有意义吗？]]></description>
      <guid>https://stackoverflow.com/questions/77879087/triplet-loss-time-series</guid>
      <pubDate>Thu, 25 Jan 2024 10:13:24 GMT</pubDate>
    </item>
    <item>
      <title>不寻常的学习率查找曲线：最小学习率时损失最低</title>
      <link>https://stackoverflow.com/questions/77082609/unusual-learning-rate-finder-curve-loss-lowest-at-smallest-learning-rate</link>
      <description><![CDATA[我正在使用 PyTorch Lightning 的 LR Finder，但得到了一条非典型曲线。当学习率最小时，损失从最低点开始，逐渐增加直至稳定，然后呈现典型的 U 形曲线。无论我从什么学习率开始（我尝试过 1e-12、1e-6、1e-3 等），同样的事情会发生，在最低学习率下损失总是最小的：

示例模型类：
`class simple_model(pl.LightningModule):
def __init__(self, 编码器名称, lr, **kwargs):
    超级().__init__()
    self.model = timm.create_model(encoder_name, pretrained=True)
    self.loss_fn = nn.BCEWithLogitsLoss()
    self.lr = lr

defforward（自身，图像）：

    pred = self.model(图像)
    返回预测值

def共享步骤（自身，批次，阶段）：
    
    图像 = 批处理[“图像”]
    标签=批次[“标签”]

    logits = self.forward(图像)
    损失 = self.loss_fn(logits.squeeze(),labels.float())

    返回 {
        “损失” ： 损失，
        “预测”：logits.sigmoid().round().squeeze().cpu().detach().numpy(),
        “true”：labels.cpu().detach().numpy()
    }

def shared_epoch_end(自身, 输出, 阶段):

    loss = [x[“loss”].item() for x in 输出]
    Whole_dataset_loss = np.mean(损失)
    self.log_dict({f&quot;{stage}_loss&quot;: Whole_dataset_loss}, prog_bar=True)

def Training_step（自身，批次，batch_idx）：
    返回 self.shared_step(batch, &quot;train&quot;)

def Training_epoch_end(自我，输出)：
    return self.shared_epoch_end(输出,“火车”)

defvalidation_step(self,batch,batch_idx):
    返回 self.shared_step(batch, &quot;valid&quot;)

defvalidation_epoch_end（自我，输出）：
    返回 self.shared_epoch_end(输出,“有效”)

def test_step（自身，批次，batch_idx）：
    返回 self.shared_step(batch, “测试”)

def test_epoch_end(自身，输出)：
    return self.shared_epoch_end(输出,“测试”)

def 配置_优化器（自身）：
    返回 torch.optim.Adam(self.parameters(), self.lr)

LR Finder 的使用示例：
trainer = pl.Trainer(accelerator=&#39;gpu&#39;,devices=1,max_epochs=140, precision=16, log_every_n_steps=1) lr_finder = trainer.tuner.lr_find(model,dataset)
我已经在多个数据集和模型类型（完全监督和 ssl）中看到了这种行为。几乎所有的实现方式都是从 pytorch lighting (1.9.0) 中开箱即用的，并且我的模型训练和收敛得很好，所以我不太确定如何解决这个问题。无论我使用 ImageNet 权重还是随机权重（全部来自 timm 库），都会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/77082609/unusual-learning-rate-finder-curve-loss-lowest-at-smallest-learning-rate</guid>
      <pubDate>Mon, 11 Sep 2023 14:42:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 mediapipe 不在实时反馈上绘制地标</title>
      <link>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</link>
      <description><![CDATA[这是我从 mediapipe 文档中获得的代码。我尝试了很多方法来在实时反馈中展示地标图，但似乎没有任何效果。我确实需要一些帮助来了解我错过的事情。
导入 mediapipe 作为 mp
从 mediapipe.tasks 导入 python
从 mediapipe.tasks.python 导入视觉
导入CV2
导入时间
将 mediapipe 导入为 mp
将 numpy 导入为 np
从 mediapipe 导入解决方案
从 mediapipe.framework.formats 导入地标_pb2
将 numpy 导入为 np

边距 = 10 # 像素
字体大小 = 1
字体粗细 = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54) # 鲜艳的绿色

BaseOptions = mp.tasks.BaseOptions
HandLandmarker = mp.tasks.vision.HandLandmarker
HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions
HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult
VisionRunningMode = mp.tasks.vision.RunningMode

# 使用直播模式创建一个手部地标实例：
def print_result(结果：mp.tasks.vision.HandLandmarkerResult，output_image：mp.Image，timestamp_ms：int)：
    print(&#39;手部地标结果：{}&#39;.format(result))

选项 = HandLandmarkerOptions(
    base_options=BaseOptions(model_asset_path=&#39;hand_landmarker.task&#39;),
    running_mode=VisionRunningMode.LIVE_STREAM,
    结果回调=打印结果）
使用 HandLandmarker.create_from_options(options) 作为地标：
    上限 = cv2.VideoCapture(0)
    而真实：
        ret, 框架 = cap.read()
        如果不转：
            休息
        frame_np = np.array(帧)
        时间戳 = int(round(time.time()*1000))
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_np)
        帧 = mp_image.numpy_view()
        结果 =landmarker.detect_async(mp_image, 时间戳)
        如果类型（结果）不是类型（无）：
           hand_landmarks_list = 结果.hand_landmarks
           对于范围内的 idx(len(hand_landmarks_list))：
                hand_landmarks = hand_landmarks_list[idx]

                # 绘制手部标志。
                hand_landmarks_proto =地标_pb2.NormalizedLandmarkList()
                hand_landmarks_proto.landmark.extend([
                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) 用于 hand_landmarks 中的地标
                ]）
                解决方案.drawing_utils.draw_landmarks（
                    框架，
                    hand_landmarks_proto，
                    解决方案.hands.HAND_CONNECTIONS,
                    解决方案.drawing_styles.get_default_hand_landmarks_style(),
                    Solutions.drawing_styles.get_default_hand_connections_style())
        别的：
            打印（&#39;其他&#39;）
        cv2.imshow(&#39;框架&#39;, 框架)
        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息
    cap.release()
    cv2.destroyAllWindows()

每次传递“结果函数”时，我都会收到此 NoneType 错误。我也不知道如何处理。 mediapipe 文档没有提供有关如何在实时源中显示此内容的任何见解。]]></description>
      <guid>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</guid>
      <pubDate>Thu, 22 Jun 2023 15:33:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中累积大批量的梯度</title>
      <link>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</link>
      <description><![CDATA[我正在使用一个对内存要求很高的 CNN 模型来执行分类任务。
这对我在训练期间可以使用的批量大小造成了很大的限制。
一种解决方案是在训练期间累积梯度，这意味着模型的权重不会在每个批次后更新。相反，相同的权重用于多个批次，而每个批次的梯度会被累积，然后针对单个权重更新操作进行平均。
我正在使用 Tensorflow 后端 Keras，并且我非常确定 Keras 没有现成的函数/方法来实现此目的。
如何为 Keras/tensorflow 模型完成此操作？]]></description>
      <guid>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</guid>
      <pubDate>Wed, 20 Mar 2019 19:26:43 GMT</pubDate>
    </item>
    </channel>
</rss>