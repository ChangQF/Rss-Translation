<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 27 Mar 2024 15:15:13 GMT</lastBuildDate>
    <item>
      <title>我在尝试对心电图和脑电图数据执行二元分类时需要一些帮助来解决类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78232398/i-need-some-help-in-solving-a-class-imbalance-problem-while-trying-to-perform-bi</link>
      <description><![CDATA[我有一个数据集，其中包含 23 名患者的心电图和脑电图值，每个患者有 18 个视频。这些视频与我试图预测的目标情绪相关。现在根据数据集有 8 种目标情绪，但我已将它们重新分类为 0 - 非恐惧和 1 - 恐惧。这导致了比例为 1:7 的阶级不平衡（恐惧：不是恐惧）。因此，我的准确率在 90% 范围内出现错误。我真的很感激能帮助解决这个问题。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.neighbors 导入 KNeighborsClassifier
将 matplotlib.pyplot 导入为 plt
从sklearn.metrics导入confusion_matrix
从 sklearn 导入 svm
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入准确度_分数、精度_分数、召回_分数、f1_分数
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.linear_model 导入 LogisticRegression
从sklearn.metrics导入confusion_matrix、recall_score、 precision_score、f1_score、accuracy_score
从 sklearn.model_selection 导入 KFold
将seaborn导入为sns
从 sklearn.metrics 导入分类报告
从 imblearn.over_sampling 导入 SMOTE

defvaluate_cv_model（模型，数据，目标，kFolds）：
    a_score = cross_val_score(模型、数据、目标、cv=kFolds、评分=&#39;准确度&#39;)
    准确度 = a_score.mean()
​
    返回精度

defplot_confusionMatrix（clf，y_test，X_test）：
    
    y_pred = clf.predict(X_test)
    cm = 混淆矩阵(y_test, y_pred)
    cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm, annot=True, fmt=&#39;.2f&#39;, cmap=“蓝调”)
    plt.ylabel(&#39;真实标签&#39;)
    plt.xlabel(&#39;预测标签&#39;)
    报告=分类报告（y_test，y_pred）
    plt.show()

    返回报告

def KNN(X_train, y_train, X_test, y_test, num_neighbors):
    
    # 创建模型
    KNN = KNeighborsClassifier(n_neighbors = num_neighbors)
    
    # 拟合模型
    KNN.fit(X_train, y_train)
    
    # 获取准确率
    test_accuracy = KNN.score(X_test, y_test)
    train_accuracy = KNN.score(X_train, y_train)
    
    # 预测值
    预测 = KNN.predict(X_test)
    
    返回 test_accuracy、train_accuracy、预测、KNN

def SVM（X_train，y_train，X_test，y_test，内核）：

    # 创建多类分类模型
    SVM = svm.SVC(kernel=kernel, C=1, Decision_function_shape=&#39;ovo&#39;)
    
    # 拟合模型
    SVM.fit(X_train, y_train)
    
    # 获取准确率
    test_accuracy = SVM.score(X_test, y_test)
    train_accuracy = SVM.score(X_train, y_train)
    
    # 预测值
    预测 = SVM.predict(X_test)
    
    返回 test_accuracy、train_accuracy、预测、SVM
def Logistic_Regression (X_train, y_train, X_test, y_test):
    
    # 创建增加 max_iter 的模型
    log_reg = LogisticRegression(multi_class=&#39;多项式&#39;, 求解器=&#39;lbfgs&#39;, max_iter=1000)

    # 拟合模型
    log_reg.fit(X_train, y_train)

    # 获取准确率
    test_accuracy = log_reg.score(X_test, y_test)
    train_accuracy = log_reg.score(X_train, y_train)

    # 预测值
    预测 = log_reg.predict(X_test)

    返回 test_accuracy、train_accuracy、预测、log_reg

ECG_data = pd.read_csv(&#39;/kaggle/input/ecgdata/binary_ECG.csv&#39;)
ECG_data.drop([&#39;未命名: 0&#39;,&#39;video_name&#39;], axis=1, inplace=True)
y_ECG = ECG_data.target
X_ECG = ECG_data.drop(&#39;目标&#39;, 轴 = 1)

# 应用 SMOTE 来处理类别不平衡
smote = SMOTE(sampling_strategy=&#39;auto&#39;, random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ECG, y_train_ECG)

kf = KFold(n_splits=8, random_state=42 , shuffle = True)
X_train_ECG、X_test_ECG、y_train_ECG、y_test_ECG = train_test_split(X_ECG、y_ECG、test_size = 0.2、random_state = 42)
y_test_ECG = np.array(y_test_ECG)

def 评估（y_test，预测）：
    准确度=准确度_分数（y_测试，预测）
    精度 = precision_score(y_test, 预测, 平均值=&#39;加权&#39;)
    召回率=召回率（y_测试，预测，平均值=&#39;加权&#39;）
    f1 = f1_score(y_test, 预测, 平均值=&#39;加权&#39;)
    返回准确率、精确率、召回率、f1

我尝试过使用 SVM、KNN 和逻辑回归进行训练。在尝试实现 SMOTE 时，我遇到了逻辑回归的收敛错误，即使我将 max_iter 增加到 python 中的最大允许限制，简单似乎也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/78232398/i-need-some-help-in-solving-a-class-imbalance-problem-while-trying-to-perform-bi</guid>
      <pubDate>Wed, 27 Mar 2024 14:21:43 GMT</pubDate>
    </item>
    <item>
      <title>如何将 tfidfvectorizer 的功能从英语修改为西班牙语</title>
      <link>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</link>
      <description><![CDATA[我有一个 tfidfvectorizer，它适合英语文本数据来预测英语通话的情绪。任务是将其转换为西班牙语。我想使用此 tfidfvectorizers 的权重，并希望将功能从英语转换为西班牙语，例如“谢谢”变成“gracias”并使用旧的权重。所以本质上我想使用相同的 tfidf 矢量器，但修改了特征名称。有人可以建议一些方法在 Python 中做到这一点吗？
带有解决方案的代码。]]></description>
      <guid>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</guid>
      <pubDate>Wed, 27 Mar 2024 14:11:46 GMT</pubDate>
    </item>
    <item>
      <title>使用隔离森林进行异常检测[关闭]</title>
      <link>https://stackoverflow.com/questions/78232159/anomaly-detection-with-isolation-forest</link>
      <description><![CDATA[我有车辆数据。该数据是在会议中测量的。我在数据框中有一列显示测量会话 ID。在“时间”列中，时间每 200 毫秒累加一次。测量的块具有不同的长度。有些是 600000 毫秒长，有些是 400000 毫秒长。如果 id 发生变化，时间列会再次从 0 开始计数。我现在的问题是，我如何向隔离森林教授这一点，或者我如何准备数据和列，以便隔离森林考虑到这一点？我真的需要尽快得到一个好的答案。非常感谢
我没有任何想法。 Time 列也只是 float64 的类型，它不是日期时间对象。]]></description>
      <guid>https://stackoverflow.com/questions/78232159/anomaly-detection-with-isolation-forest</guid>
      <pubDate>Wed, 27 Mar 2024 13:45:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在 stylegan2 中提示文本</title>
      <link>https://stackoverflow.com/questions/78231191/how-to-prompt-a-text-in-stylegan2</link>
      <description><![CDATA[我想在stylegan2中输入文本提示。
到目前为止，我已经能够使用加载生成器 (G)
G,D,Gs = load_networks(&#39;gdrive:networks/stylegan2-ffhq-config-a.pkl&#39;)

它调用 pretrained_networks.py 文件中定义的函数。
现在，我不知道如何按照我想要的方式使用生成器。
具体来说，给定文本提示，我想我应该：

对 numpy 数组中的文本进行编码
使用此编码提供生成器

但是我该怎么做呢？]]></description>
      <guid>https://stackoverflow.com/questions/78231191/how-to-prompt-a-text-in-stylegan2</guid>
      <pubDate>Wed, 27 Mar 2024 11:04:39 GMT</pubDate>
    </item>
    <item>
      <title>尽管 PyTorch 的学习率降低，FISTA 优化并未减少损失</title>
      <link>https://stackoverflow.com/questions/78231057/fista-optimization-not-reducing-loss-despite-decreasing-learning-rate-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78231057/fista-optimization-not-reducing-loss-despite-decreasing-learning-rate-in-pytorch</guid>
      <pubDate>Wed, 27 Mar 2024 10:44:19 GMT</pubDate>
    </item>
    <item>
      <title>集成梯度下降和 ISTA 通过正则化最小化自定义损失函数</title>
      <link>https://stackoverflow.com/questions/78230490/integrating-gradient-descent-and-ista-for-minimizing-a-custom-loss-function-with</link>
      <description><![CDATA[我正在研究一个问题，目标是最小化损失函数，该损失函数是标准损失度量和正则化项的组合。具体来说，我的目标函数的形式为：L(y, m_t(x))+ l* P(t)
地点：

L 代表损失度量。
y 是实际结果。
x 表示输入数据。
t 是模型的参数向量。
m_t 是由 t 参数化的模型。
P(t) 是参数的惩罚函数，以鼓励某些属性（例如稀疏性）。
λ 是正则化系数。

我的问题围绕优化策略。我正在考虑首先使用梯度下降（GD）来最小化函数，直到它停止取得重大进展。在 GD 达到这个平台后，我正在考虑切换到迭代收缩阈值算法（ISTA）来进一步细化解决方案，特别是有效地合并正则化部分。
但是，我不确定结合这两种优化技术的最佳方法。具体来说：
我是否应该将 GD 专门应用于损失项 L(y,m_t(x))​，然后使用 ISTA 处理包括正则化项在内的整个成本函数？
优化过程中是否存在在 GD 和 ISTA 之间转换的推荐方法或最佳实践？
在针对此类问题集成这些优化方法时，我是否应该注意任何潜在的陷阱或注意事项？
任何见解、对类似实现的参考或有关如何解决此问题的指导将不胜感激。
预先感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78230490/integrating-gradient-descent-and-ista-for-minimizing-a-custom-loss-function-with</guid>
      <pubDate>Wed, 27 Mar 2024 09:11:49 GMT</pubDate>
    </item>
    <item>
      <title>内核重新​​启动 Untitled2.ipynb 的内核似乎已死亡。存储tflite模型时会自动重启</title>
      <link>https://stackoverflow.com/questions/78230023/kernel-restarting-the-kernel-for-untitled2-ipynb-appears-to-have-died-it-will-r</link>
      <description><![CDATA[我正在使用 ml.g4dn.xlarge 实例在 sagemaker 实例上运行笔记本。
我正在尝试创建如下所示的 tflite 模型
保存模型
model.save(os.path.join(model_dir, &#39;model.h5&#39;))
# 将模型转换并保存为 TensorFlow Lite
转换器 = tf.lite.TFLiteConverter.from_keras_model(模型)
converter.experimental_new_converter = True # 启用基于 MLIR 的转换流程
converter.debug_info = True # 启用调试.mlir文件的生成
tflite_model =转换器.convert()

打开（os.path.join（model_dir，&#39;model.tflite&#39;），&#39;wb&#39;）作为f：
    f.write(tflite_model)

我收到错误
内核重启
Untitled2.ipynb 的内核似乎已经死亡。它会自动重新启动。

我检查了一些要求升级实例类型的链接。我将其升级到 ml.g4dn.xlarge 但仍然给出相同的错误。有什么建议可能是什么原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/78230023/kernel-restarting-the-kernel-for-untitled2-ipynb-appears-to-have-died-it-will-r</guid>
      <pubDate>Wed, 27 Mar 2024 07:42:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我无法在函数调用中访问对象键</title>
      <link>https://stackoverflow.com/questions/78229600/why-cant-i-access-an-object-key-within-a-function-call</link>
      <description><![CDATA[我试图通过在此函数 graphics.drawPoint(ctx,pixelLoc, utils.样式[标签].color);
来自 Radu 的代码：无黑盒机器学习
utils.styles={
    汽车：{颜色：&#39;灰色&#39;，文本：&#39;🚓&#39;}，
    鱼：{颜色：&#39;红色&#39;，文本：&#39;🐠&#39;}，
    房子: { color:&#39;yellow&#39;, text:&#39;🏚&#39; },
    树：{颜色：&#39;绿色&#39;，文本：&#39;🌳&#39;}，
    自行车: { color:&#39;cyan&#39;, text:&#39;🚲&#39; },
    guiter: { color:&#39;blue&#39;, text:&#39;🎸&#39; },
    铅笔：{颜色：&#39;洋红色&#39;，文本：&#39;✏&#39;}，
    时钟：{ 颜色：&#39;lightgray&#39;，文本：&#39;⏲&#39; }
}

for（样本的常量样本）{
  const {点，标签}=样本；


sample 是以标签作为键的对象集合[&#39;car&#39;,&#39;fish&#39;,&#39;house...&#39;clock&#39;] 与 uitils.styles 相同&gt;
graphics.drawPoint(utils.styles[label].color);

这会产生一条错误消息：Uncaught TypeError:
&lt;块引用&gt;
无法读取未定义的属性（读取“颜色”）
]]></description>
      <guid>https://stackoverflow.com/questions/78229600/why-cant-i-access-an-object-key-within-a-function-call</guid>
      <pubDate>Wed, 27 Mar 2024 05:56:17 GMT</pubDate>
    </item>
    <item>
      <title>用于解析（决策树）绘制图表的法学硕士？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78227894/llm-for-parsing-drawn-diagrams-of-decision-trees</link>
      <description><![CDATA[
我正在寻找一种开源法学硕士，它能够接受与上述类似的决策树查询。这是否意味着找到最好的开源 ViT 模型？或者是否有更好的方法，例如以某种方式将树预处理为文本并向法学硕士询问？考虑到我仅对开源模型的限制，什么性能会更好？
我已经尝试过 ChatGPT4，它似乎表现得很好，但我希望在本地运行所有内容。]]></description>
      <guid>https://stackoverflow.com/questions/78227894/llm-for-parsing-drawn-diagrams-of-decision-trees</guid>
      <pubDate>Tue, 26 Mar 2024 20:03:23 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”井字棋电脑需要多少场比赛才能训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。它在对抗随机走棋时效果很好（我刚刚根据约 10,000 场比赛生成的数据再次运行了 5353 场比赛，它赢得了 5353 场比赛中的 4757 场），但它仍然经常输给我。
以下是完美答案应解决的一些问题：

需要玩多少场游戏才能让“火柴盒电脑”与 Michie 设计的电脑完全一样，才能完美地开始玩游戏？

带有实际火柴盒的原始计算机是否达到了完美状态
玩吗？

如果仅与计算机进行训练，计算机能否达到完美的发挥
随机移动？


编辑：
这个问题并不是寻求代码方面的帮助，但下面的评论表明包含代码可能会有所帮助。以下是我创建的 GitHub 存储库的链接，以便我可以在此处共享：
https://github.com/ACertainArchangel/ Recreation-Of-MENACE-Tic-Tac-Toe..git
抱歉，我知道这不太好并且不遵守约定；我只写了几个月的代码:)]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 MPI 优化 Optuna 参数</title>
      <link>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</link>
      <description><![CDATA[我有一些机器学习代码，它使用 SVM（来自 scikit-learn）和预计算内核，我想使用 optuna 对其进行优化，因此代码简单地看起来有点像这样
def 目标（试用：试用，fast_check=True，target_meter=0，return_info=False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #构建火车内核
     内核训练 = ...

     #构建测试内核
     内核测试 = ...

     #火车服务
     svc = SVC(内核=“预计算”, C=C, tol=tol)
     svc.fit(kernel_train, train_labels)
     test_predict = svc.predict(kernel_test)
     test_auc = roc_auc_score(test_labels,test_predict)

     返回测试_auc

Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
研究.优化（目标，n_Trials=40）


但是，由于我正在计算的内核的复杂性，我使用 mpi4py 来并行计算，但同​​时使用 optuna 和 MPI 时遇到一些问题。
显然，我想要多个处理器上的内核代码，但是当我创建研究并优化它时，我不想在处理器上创建多个不同的研究，我只想对根进行优化的一项研究（我假设？）。我已经尝试了下面的方法，它有效，但是当我不使用 MPI 时，它的优化效果不佳，我认为这正在创建多项研究并优化它们，这似乎效率不高。似乎更难以收敛到最佳参数。
从 mpi4py 导入 MPI

mpi_comm = MPI.COMM_WORLD
排名 = mpi_comm.Get_rank()
n_procs = mpi_comm.Get_size()
根=0

def目标（试验：试验，fast_check = True，target_meter = 0，return_info = False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #使用 MPI 构建训练内核
     内核训练 = ...

     #使用MPI构建测试内核
     内核测试 = ...

     #火车服务
     如果排名==根：
           svc = SVC(内核=“预计算”, C=C, tol=tol)
           svc.fit(kernel_train, train_labels)
           test_predict = svc.predict(kernel_test)
           test_auc = roc_auc_score(test_labels,test_predict)
     别的：
           测试_auc = 0
     test_auc = mpi_comm.bcast(test_auc, root=0)

如果排名==根：
     Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
别的：
     研究 = 0
 研究= mpi_comm.bcast（研究，根= 0）

研究.优化（目标，n_Trials=40）

这是一个非常小众的问题，但只是想知道是否有人对这些软件包有任何经验，并且可以帮助建议如何运行多处理代码，同时仅优化一个处理器上的参数。如果有任何术语不正确，我深表歉意，我是使用这两个软件包的新手，因此请耐心等待。 :)]]></description>
      <guid>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</guid>
      <pubDate>Mon, 25 Mar 2024 09:20:08 GMT</pubDate>
    </item>
    <item>
      <title>scikeras.wrappers.KerasClassifier 返回 ValueError：无法解释指标标识符：loss</title>
      <link>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</link>
      <description><![CDATA[我正在研究 KerasClassifier，因为我想将其插入 scikit-learn 管道中，但我收到了前面提到的 ValueError。
以下代码应该能够重现我遇到的错误：
从 sklearn.model_selection 导入 KFold，cross_val_score
从 sklearn.preprocessing 导入 StandardScaler
从 scikeras.wrappers 导入 KerasClassifier
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从 sklearn.datasets 导入 load_iris
将 numpy 导入为 np

数据 = load_iris()
X = 数据.数据
y = 数据.目标

def create_model():
    模型=顺序（）
    model.add（密集（8，input_dim = 4，激活=&#39;relu&#39;））
    model.add（密集（3，激活=&#39;softmax&#39;））
    model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
                  优化器=&#39;亚当&#39;,
                  指标=[&#39;准确性&#39;])
    返回模型

clf = KerasClassifier(build_fn=create_model,
                      纪元=100，
                      批量大小=10，
                      详细=1)

管道=管道([
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;clf&#39;，clf）
]）

kf = KFold(n_splits=5, shuffle=True, random_state=42)
结果= cross_val_score（管道，X，y，cv = kf）
print(&quot;交叉验证准确度：&quot;, np.mean(结果))

似乎我的模型正在随着纪元的运行而被编译。但是，之后我收到错误：
ValueError：无法解释指标标识符：丢失

tensorflow 和 scikeras 库的版本是：
scikeras==0.12.0
张量流==2.15.0

编辑：
最终我尝试了不同的库版本，以下内容让我成功运行了代码，看来问题是由 scikit-learn 的版本引起的：
scikeras==0.12.0
张量流==2.15.0
scikit学习==1.4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</guid>
      <pubDate>Fri, 01 Mar 2024 17:03:39 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层equential_40的输入0与该层不兼容：预期min_ndim = 3，发现ndim = 2。收到完整形状：（无，58）</title>
      <link>https://stackoverflow.com/questions/72743778/valueerror-input-0-of-layer-sequential-40-is-incompatible-with-the-layer-expec</link>
      <description><![CDATA[我正在研究有关学生在课程中表现的数据集，我想根据学生上一年的成绩来预测学生的水平（低、中、高）。我正在使用 CNN 来实现此目的，但是当我构建并拟合模型时，我收到此错误：
ValueError：层equential_40的输入0与层不兼容：：预期min_ndim = 3，发现ndim = 2。收到完整形状：（无，58）

这是代码：
#重塑数据
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]))

#检查重塑后的形状
打印（X_train.shape）
打印（X_test.shape）

#标准化像素值
X_train=X_train/255
X_测试=X_测试/255

#定义模型
模型=顺序()

#添加卷积层
model.add(Conv1D(32,3，激活=&#39;relu&#39;，input_shape=(28,1)))

#添加池化层
model.add(MaxPool1D(pool_size=2))

#添加全连接层
模型.add(压平())
model.add(密集(100,activation=&#39;relu&#39;))

#添加输出层
model.add(密集(10,activation=&#39;softmax&#39;))

#编译模型
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

模型.summary()

#拟合模型
model.fit(X_train,y_train,epochs=10)

这是输出：
型号：“sequential_40”
_________________________________________________________________
层（类型）输出形状参数#
=================================================== ===============
conv1d_23（Conv1D）（无、9、32）128
_________________________________________________________________
max_pooling1d_19（最大池化（无、4、32）0
_________________________________________________________________
flatten_15（压平）（无，128）0
_________________________________________________________________
密集_30（密集）（无，100）12900
_________________________________________________________________
密集_31（密集）（无，10）1010
=================================================== ===============
总参数：14,038
可训练参数：14,038
不可训练参数：0
]]></description>
      <guid>https://stackoverflow.com/questions/72743778/valueerror-input-0-of-layer-sequential-40-is-incompatible-with-the-layer-expec</guid>
      <pubDate>Fri, 24 Jun 2022 12:01:17 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛到零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对某些数据进行预测。
我在一个小数据集（大约 100 条记录）上进行了尝试，效果非常好。然后我插入新的数据集，发现神经网络收敛到 0 输出，并且误差近似收敛于正例数与示例总数之间的比率。
我的数据集由是/否特征 (1.0/0.0) 组成，基本事实也是是/否。
我的假设：
1）有一个输出为0的局部最小值（但我尝试了学习率和初始权重的许多值，它似乎总是收敛在那里）
2）我的体重更新是错误的（但对我来说看起来不错）
3）这只是一个输出缩放问题。我尝试缩放输出（即输出/最大（输出）和输出/平均值（输出）），但结果并不好，正如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？软最大？ 
这是代码：
导入 pandas 作为 pd
将 numpy 导入为 np
进口泡菜
随机导入
从集合导入defaultdict

阿尔法 = 0.1
N_层 = 10
N_ITER = 10
#N_功能 = 8
初始化规模 = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(浮点数)
y测试 = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
    如果（导数==真）：
        返回 (1 - np.tanh(x)**2) * alpha
    别的：
        返回 np.tanh(x)

def sigmoid(x,deriv=False):
    如果（导数==真）：
        返回 x*(1-x)
    别的：
        返回 1/(1+np.exp(-x))

def relu(x,deriv=False):
    如果（导数==真）：
        返回 0.01 + 0.99*(x&gt;0)
    别的：
        返回 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

对于范围内的 i (N_LAYERS-1)：
    syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)
delta = defaultdict(np.array)

对于 xrange(N_ITER) 中的 j：
    l[0] = X
    对于范围 (1,N_LAYERS+1) 内的 i：
        l[i] = relu(np.dot(l[i-1],syn[i-1]))

    错误 = (y - l[N_LAYERS])

    e = np.mean(np.abs(误差))
    如果（j％1）== 0：
        print &quot;\n迭代 &quot; + str(j) + &quot; of &quot; + str(N_ITER)
        打印“错误：” + str(e)

    delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
    对于范围内的 i(N_LAYERS-1,0,-1)：
        错误 = delta[i+1].dot(syn[i].T)
        delta[i] = error*relu(l[i],deriv=True) * alpha

    对于范围内的 i（N_LAYERS）：
        syn[i] += l[i].T.dot(delta[i+1])



pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 精度 = 真阳性 /（真阳性 + 假阳性）

l[0] = X测试
对于范围 (1,N_LAYERS+1) 内的 i：
    l[i] = relu(np.dot(l[i-1],syn[i-1]))

输出 = l[N_LAYERS]/max(l[N_LAYERS])

tp = 浮点数(0)
fp = 浮点数(0)
fn = 浮点数(0)
tn = 浮点数(0)

对于 l[N_LAYERS][:50] 中的 i：
    打印我

对于范围内的 i(len(ytest))：
    如果输出[i]&gt; 0.5 且 ytest[i] == 1：
        tp += 1
    如果 out[i] &lt;= 0.5 且 ytest[i] == 1：
        fn += 1
    如果输出[i]&gt; 0.5 且 ytest[i] == 0：
        焦距 += 1
    如果 out[i] &lt;= 0.5 且 ytest[i] == 0：
        tn += 1

打印“tp：” + str（tp）
打印“fp：” + str（fp）
打印“tn:”+str(tn)
打印“fn:”+str(fn)

print &quot;\n精度: &quot; + str(tp/(tp + fp))
print &quot;回忆：&quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-测量:&quot; + str(f1)

这是输出：
第 0 次迭代（共 10 次）
错误：0.222500767998

第 1 次迭代（共 10 次）
错误：0.222500771157

第 2 次迭代（共 10 次）
错误：0.222500774321

第 3 次迭代（共 10 次）
错误：0.22250077749

第 4 次迭代（共 10 次）
错误：0.222500780663

第 5 次迭代（共 10 次）
错误：0.222500783841

第 6 次迭代（共 10 次）
错误：0.222500787024

第 7 次迭代（共 10 次）
错误：0.222500790212

第 8 次迭代（共 10 次）
错误：0.222500793405

迭代第 9 次（共 10 次）
错误：0.222500796602


[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[0。]
[4.62182626e-06]
[0。]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[4.62182626e-06]
[0。]
[0。]
[5.04501079e-10]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[5.04501079e-10]
[0。]
[0。]
[4.62182626e-06]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[1.31432294e-05]

目标点：28.0
焦距：119.0
电话号码：5537.0
号码：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-测量：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    <item>
      <title>如何计算线性回归中的正则化参数</title>
      <link>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</link>
      <description><![CDATA[当我们有一个高次线性多项式用于拟合线性回归设置中的一组点时，为了防止过度拟合，我们使用正则化，并在成本函数中包含 lambda 参数。然后使用该 lambda 更新梯度下降算法中的 theta 参数。
我的问题是我们如何计算这个 lambda 正则化参数？]]></description>
      <guid>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</guid>
      <pubDate>Wed, 29 Aug 2012 16:04:04 GMT</pubDate>
    </item>
    </channel>
</rss>