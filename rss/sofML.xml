<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 21 Apr 2024 15:13:36 GMT</lastBuildDate>
    <item>
      <title>mat1 和 mat2 形状不能相乘（32x1000 和 512x128）</title>
      <link>https://stackoverflow.com/questions/78362003/mat1-and-mat2-shapes-cannot-be-multiplied-32x1000-and-512x128</link>
      <description><![CDATA[我正在使用 Google Colab 对水牛芒果进行炭疽病检测。我目前正在使用带有训练、验证和测试数据集的 ResNet18 模型。
类MangoDiseaseDetection(nn.Module)：
  def __init__(自身):
      super(MangoDiseaseDetection, self).__init__()
      self.base_model = models.resnet18(预训练 = True)
      self.base_model.requires_grade = False #这将冻结预训练层
      
      自我检测 = nn.Sequential(
          nn.线性(512, 128),
          nn.ReLU(inplace = True),
          nn.Dropout(p = 0.5),
          nn.线性(128, 1),
          nn.Sigmoid()
      ）

  def 前向（自身，x）：
      x = self.base_model(x)
      x = 自我检测(x)
      返回x

# 模型实例化
模型 = MangoDiseaseDetection()

criterion = nn.BCELoss() #二元交叉熵损失
优化器 = torch.optim.Adam(model.parameters(), lr =learning_rate)

def train(模型、标准、优化器、train_loader)：
  model.train() # 模型训练模式
  纪元损失 = 0.0

  对于图像，tqdm（train_loader，unit =“batch”）中的标签：
      优化器.zero_grad()

      # 前向传递和损失计算
      输出=模型（图像）
      损失 = 标准(输出, labels.float())

      # 后台传递和参数更新
      loss.backward()
      优化器.step()

      epoch_losee += loss.item() # epoch 的累积损失

  return epoch_loss / len(train_loader) # 每批次的平均损失。

# 验证函数

def 验证（模型、标准、val_loader）：
    model.eval() # 模型到评估模式
    纪元损失 = 0.0
    使用 torch.no_grad()：
        对于图像，val_loader 中的标签：
            输出=模型（图像）
            损失 = 标准(输出, labels.float())

# 训练并验证模型

训练损失 = []
val_losses = []

对于范围内的纪元（纪元）：
    print(f&quot;纪元 {纪元+1} / {纪元}&quot;)
    train_loss = train(模型、标准、优化器、train_loader)
    val_loss = 验证（模型、标准、val_loader）
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    print(f“训练损失：{train_loss:.4f}，验证损失{val_loss:.4f}”)

然后我遇到了这个错误：
运行时错误：mat1 和 mat2 形状无法相乘（32x1000 和 512x128）
几个小时以来我一直在试图解决这个问题，这就是为什么我想在这里寻求一些帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78362003/mat1-and-mat2-shapes-cannot-be-multiplied-32x1000-and-512x128</guid>
      <pubDate>Sun, 21 Apr 2024 14:52:50 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列预测中提前多天进行预测时出现问题</title>
      <link>https://stackoverflow.com/questions/78361646/problem-forecasting-multiple-days-ahead-in-time-series-forecasting</link>
      <description><![CDATA[我正在做一个项目，涉及使用各种机器学习和深度学习模型来预测股票价格。目标变量是股票每天到第二天收盘价的百分比变化。该数据集由 23 个变量组成，包括宏观经济指标（gdp、通货膨胀...）、一些技术指标（RSI、EMAF、EMAS...）、其他股票从前一天到当前一天的走势以及一些公司基本面数据（每季度收益）。总共有 23 行（不包括目标列），所有行的缩放比例都在 -1 和 1 之间。
对于单步预测，模型的表现大致符合我的预期，其中随机森林和 lstm 表现最好，在股票走势方向上达到了 80% 左右的准确度。然而，我遇到的问题是在尝试比较模型预测未来 5 天的能力时。我预计模型预测的天数越多，其预测精度就会越来越低，但实际上精度保持不变，甚至由于某种原因而增加。这让我相信存在某种数据泄漏问题，或者我错误地分割了数据。
到目前为止，我尝试解决此问题的方法是为我尝试训练的每一天训练一个新模型，每个模型都针对移动的 y 列。这意味着如果我想用随机森林预测 5 天，我将训练 5 个随机森林，1 个针对明天，1 个针对后天，依此类推......（全部来自同一个 x 数据集）。其代码如下：
def precision_trend(list1, list2):
计数器 = 0
对于范围内的 x(len(list1)-1)：
if (list1[x] &gt; 0) == (list2[x] &gt; 0):
计数器 = 计数器 + 1
返回计数器 / (len(list1) - 1)

def spliter(路径, days_ahead):
数据 = pd.read_csv(路径)
data.drop([&#39;公司&#39;], axis=1, inplace=True)
x = np.array(data.iloc[:-days_ahead, :-1]) # 使用除最后 days_ahead 之外的所有行作为特征
y = [np.array(data.iloc[i:-(days_ahead-i) if days_ahead-i &gt; 0 else None, -1]) for i in range(days_ahead)]
返回 x、y

train_x, train_ys = 分离器(train_path, 5)
test_x, test_ys = 分割器(test_path, 5)

train_ys = np.array(train_ys)

模型 = [RandomForestRegressor(n_estimators=100, random_state=4) for _ in range(5)]
预测=[]

对于 zip(models, train_ys, test_ys) 中的模型、train_y、test_y：
model.fit(train_x, train_y)
pred = model.predict(test_x)
预测.append(pred)
acc = 准确度趋势(test_y, pred) * 100
print(f&quot;天 {len(预测)} 准确度：{acc}%&quot;)
rmse =mean_absolute_error(test_y, pred)
print(f&quot;Day {len(预测)} MAE: {rmse}&quot;)


输出如下：
第 1 天准确率：73.85272145144077%
第 1 天 MAE：0.01674374965706874
第 2 天准确率：74.49306296691569%
第 2 天 MAE：0.016746951626310427
第 3 天准确度：75.02668089647813%
第 3 天 MAE：0.016743799915880105
第 4 天准确度：76.09391675560299%
第 4 天 MAE：0.016736960931285863
第 5 天准确度：75.02668089647813%
第 5 天 MAE：0.016843232056633316
由于我使用最小-最大缩放测试数据集的方式，存在少量数据泄漏，但我不相信这会解释这个问题，并且目标变量未缩放。
我知道就这个问题提供建议相当困难，但我相当有信心这一定是我没有看到的编码问题。任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78361646/problem-forecasting-multiple-days-ahead-in-time-series-forecasting</guid>
      <pubDate>Sun, 21 Apr 2024 13:05:28 GMT</pubDate>
    </item>
    <item>
      <title>Baseline3 TD3，reset() 方法值太多，无法解包错误</title>
      <link>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</link>
      <description><![CDATA[环境是python 3.10，stable-baseline3 2.3.0，我正在尝试TD3算法。
无论我做什么，我都会遇到同样的错误。
据我所知，重置方法的返回值与定义的观察空间相同
我制作的环境有如下重置方法
def重置（自身，种子=0）：
    self.current_index = 0
    self.current_cash = self.start_cash
    self.done = False
    self.当前时间 = self.开始时间

    # 초기 관찰 상태 계산
    初始状态 = self.get_state() # 字典
    返回初始状态

它从来都不复杂，定义环境，模型也很好
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
从 stable_baselines3 导入 TD3

类 CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(自我, 观察空间, features_dim=5):
        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)
        self.model_alpha = ModelAlpha()
    
    defforward（自我，观察）：
        价格 = 观察结果[&#39;价格&#39;]
        位置 = 观测值[&#39;位置&#39;]
        数量 = 观察结果[&#39;数量&#39;]
        pnr = 观测值[&#39;pnr&#39;]
        
        return self.model_alpha(价格, torch.cat([位置, 数量, pnr]))
        
        
# 환경과 모델 설정
env = MarketEnvironment(蜡烛, &#39;2020-07-01 00:00:00&#39;, &#39;2023-12-31 23:59:00&#39;) # 여러분의 환경 설정
策略_kwargs = 字典（
    features_extractor_class=自定义特征提取器，
    features_extractor_kwargs=dict（features_dim=5）
）

模型 = TD3(“MultiInputPolicy”，env，policy_kwargs=policy_kwargs，batch_size=128，verbose=1)

Jupyter 提示符表示
使用CPU设备
使用 Monitor 包装器包装环境
将环境包装在 DummyVecEnv 中。
它运行良好，直到
model.learn（total_timesteps=1，log_interval=10，progress_bar=True）

这段代码。
无论我做了什么，它都会一遍又一遍地说
文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\off_policy_algorithm.py:297，在 OffPolicyAlgorithm._setup_learn(self、total_timesteps、callback、reset_num_timesteps、tb_log_name ， 进度条）
    第290章
    第291章
    292 和 self.env.num_envs &gt; 1
    293 而不是 isinstance(self.action_noise, VectorizedActionNoise)
    第294章）：
    第295章
--&gt;第297章
    298 总时间步数，
    299回调，
    300 重置_num_timesteps，
    第301章
    第302章
    第303章）

文件~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\base_class.py:425，在BaseAlgorithm._setup_learn(self,total_timesteps,callback,reset_num_timesteps,tb_log_name,progress_bar)中
    第423章
    第424章 断言self.env不是None
--&gt;第425章
    第426章
    第427章

文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:77，在 DummyVecEnv.reset(self) 中
     范围内的 env_idx 为 75(self.num_envs)：
     76 Maybe_options = {“选项”: self._options[env_idx]} if self._options[env_idx] else {}
---&gt; 77 obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
     78 self._save_obs（env_idx，obs）
     79 # 种子和选项仅使用一次

ValueError：需要解压的值太多（预期为 2）

我知道这个错误的reset()方法是在一个名为VecEnv的抽象类中
无论如何，我显然无法正确地实现这个..东西。任何建议都会有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 21 Apr 2024 12:58:27 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在微控制器上以低延迟进行音频干分离？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78361504/is-it-possible-to-do-audio-stem-separation-on-a-microcontroller-with-low-latency</link>
      <description><![CDATA[是否可以在微控制器上以低延迟（即几秒）运行像 Demucs 这样的开源模型来执行音频干分离？
我尝试在我的电脑上运行它，3 分钟的曲目需要一分钟来处理。
像 Akai pro MPC one 或 live 这样的设备表示，它可以使用其 MPC 茎工具在几秒钟内处理一首曲目，而且它们似乎在四核 ARM 处理器上运行。
那么诀窍是什么呢？他们运行自己的超高效模型吗？
谢谢
尝试在 PC 上运行工具，以及 Lalal.ai 等在线服务，它们需要一段时间才能处理，因此不确定微控制器如何处理它。]]></description>
      <guid>https://stackoverflow.com/questions/78361504/is-it-possible-to-do-audio-stem-separation-on-a-microcontroller-with-low-latency</guid>
      <pubDate>Sun, 21 Apr 2024 12:13:31 GMT</pubDate>
    </item>
    <item>
      <title>valueerror: 层“model_2”的输入 0 与该层不兼容：预期形状=(none, 128, 128, 3)，发现形状=(1, 224, 224, 3)</title>
      <link>https://stackoverflow.com/questions/78361052/valueerror-input-0-of-layer-model-2-is-incompatible-with-the-layer-expected</link>
      <description><![CDATA[我正在尝试使用 cnn 模型和 Streamlit UI 进行疟疾检测。但它显示“valueerror：层“model_2”的输入0”的错误与层不兼容：预期形状=(无, 128, 128, 3)，发现形状=(1, 224, 224, 3)”用于预测。
错误消息：
ValueError：层“model_2”的输入 0与图层不兼容：预期形状=(无, 128, 128, 3)，发现形状=(1, 224, 224, 3)

回溯：
文件“C:\Users\HP.conda\envs\DiseasePredictionSystem\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
exec（代码，模块。字典）
文件“C:\Users\HP\Desktop\多种疾病预测系统\多种疾病 pred.py”，第 361 行，位于
结果 = malaria_model.predict(final_image)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\HP.conda\envs\DiseasePredictionSystem\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件“C:\Users\HP.conda\envs\DiseasePredictionSystem\Lib\site-packages\keras\src\layers\input_spec.py”，第 245 行，位于assert_input_compatibility
引发值错误（
由于我是机器学习新手，这是第一次为该项目学习 CNN，所以我不明白如何修复错误。如果有我需要学习的参考资料，请发给我。
这是我与 cnn 模型交互的 UI 
这是我用于该项目的 UI 文件。
malaria_model = tf.keras.models.load_model(r&#39;C:\Users\HP\Desktop\多种疾病预测系统\保存的模型\malaria.h5&#39;)
#疟疾预测页面
如果选择==“疟疾疾病预测”：
 #页面标题
 st.title(&#39;使用机器学习预测疟疾&#39;)
 
 IMG_大小 = 224
 def resize_rescale(图像):
      返回 tf.image.resize(图像, (IMG_SIZE, IMG_SIZE))/255.0

 uploaded_image = st.file_uploader(&quot;上传图片&quot;, type=[&quot;jpg&quot;, &quot;png&quot;, &quot;jpeg&quot;])

 如果 uploaded_image 不是 None：
# 显示上传的图片
     st.image（uploaded_image，caption =“上传的图像”，use_column_width = False）
     图像 = Image.open(上传的图像)

# 处理图像（例如，执行图像分析）

 if st.button(“PREDICT”)：
     image_array = tf.keras.preprocessing.image.img_to_array(图像)

# 将 NumPy 数组转换为 TensorFlow 张量
     张量图像 = tf.convert_to_tensor(image_array)
     张量图像 = tf.expand_dims(张量图像，轴=0)

     最终图像=调整大小重新缩放（张量图像）

     结果 = malaria_model.predict(final_image)

     如果结果[0][0] &lt; 0.5：
        st.header(“寄生虫”)
     别的：
        st.header(“未感染”)

[这是我用于该项目的参考代码文件]
(https://github.com/kanchitank /Medibuddy-Smart-Disease-Predictor/blob/main/notebooks/malaria.ipynb)]]></description>
      <guid>https://stackoverflow.com/questions/78361052/valueerror-input-0-of-layer-model-2-is-incompatible-with-the-layer-expected</guid>
      <pubDate>Sun, 21 Apr 2024 09:40:42 GMT</pubDate>
    </item>
    <item>
      <title>cross_validate 和 RocCurveDisplay 获得的 auc 值不同</title>
      <link>https://stackoverflow.com/questions/78361038/different-values-between-auc-obtained-from-cross-validate-and-roccurvedisplay</link>
      <description><![CDATA[在训练随机森林分类器后，我尝试了两种计算 AUC 分数的方法。第一个是从 cross_validate 函数获取指标：
numeric_transformer = make_pipeline(
    IterativeImputer（估计器=RandomForestRegressor（），random_state=0），
    标准定标器()
）
预处理器 = make_column_transformer(
    （数字转换器，数字列）
）


管道=管道（步骤=[
    （&#39;预处理器&#39;，预处理器），
    （&#39;clf&#39;，随机森林分类器（））
]）

得分 = {
    &#39;AUC&#39;: &#39;roc_auc&#39;,
    &#39;准确性&#39;：&#39;准确性&#39;，
    &#39;F1_SCORE&#39;: &#39;f1&#39;,
    &#39;精度&#39;：&#39;精度&#39;，
    &#39;召回&#39;：&#39;召回&#39;
}
打印（评分）

cv = 分层KFold(n_splits=5)
cv_scores_RF = cross_validate(管道, X, y, cv=cv, 评分=评分, return_estimator=True)

print(&quot;随机森林指标&quot;)
print(f&quot;AUC: {abs(cv_scores_RF[&#39;test_AUC&#39;]).mean()}&quot;)
print(f&quot;准确度: {cv_scores_RF[&#39;test_ACCURACY&#39;].mean()}&quot;)
print(f&quot;F1 SCORE: {abs(cv_scores_RF[&#39;test_F1_SCORE&#39;]).mean()}&quot;)
print(f&quot;精度: {abs(cv_scores_RF[&#39;test_PRECISION&#39;]).mean()}&quot;)
print(f&quot;RECALL: {abs(cv_scores_RF[&#39;test_RECALL&#39;]).mean()}&quot;)


通过上面的代码，我获得了 0.72 的 AUC。
然后我用函数 RocCurveDisplay 绘制了 ROC 曲线：
导入 matplotlib.pyplot 作为 plt
从 sklearn.metrics 导入 RocCurveDisplay


plt.figure(figsize=(8, 6))


tprs = []
曲线面积=[]


对于 i，枚举中的估计器（cv_scores_RF[&#39;estimator&#39;]）：
    viz = RocCurveDisplay.from_estimator(估计器, X, y, ax=plt.gca(), name=f&#39;ROC 折叠 {i+1}&#39;)
    
    roc_auc = auc(即.fpr, 即.tpr)
    aucs.append(roc_auc)

    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)

mean_tpr = np.mean(tprs, 轴=0)
mean_auc = np.mean(aucs)

plt.plot(mean_fpr,mean_tpr,color=&#39;b&#39;,linestyle=&#39;--&#39;,lw=2,label=f&#39;平均ROC (AUC = {mean_auc:.2f})&#39;)

plt.xlabel(&#39;误报率&#39;)
plt.ylabel(&#39;真阳性率&#39;)
plt.title(&#39;接收器工作特性 (ROC) - RF&#39;)
plt.legend(loc=&#39;右下&#39;)

plt.show()

print(f&quot;平均 AUC: {mean_auc:.2f}&quot;)


但是绘制曲线，我在图中得到的 AUC 为 0.97。为什么会出现这种情况？
我期望得到类似的值，例如我将分类器更改为 SVM，也得到了不同的值。从 cross_validate 获得的指标中，我得到了 0.53 AUC，但从 RocCurveDisplay 中得到了 0.71。还尝试使用朴素贝叶斯，在这种情况下，我得到了非常相似的值，分别为 0.66 和 0.68。]]></description>
      <guid>https://stackoverflow.com/questions/78361038/different-values-between-auc-obtained-from-cross-validate-and-roccurvedisplay</guid>
      <pubDate>Sun, 21 Apr 2024 09:35:49 GMT</pubDate>
    </item>
    <item>
      <title>只有输入张量可以作为位置参数传递</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 ML 模型从 Google Drive 集成到生产后端？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78360641/how-can-i-integrate-my-ml-model-from-google-drive-to-the-production-backend</link>
      <description><![CDATA[我遇到了一个问题，即我的模型尺寸很大，因此我使用 dvc 对其进行跟踪。现在我想直接从云平台使用它，所以我将模型推送到Google Drive。但如何不下载而直接从Google Drive使用它呢？如果没有，是否有其他免费平台可以存储模型并直接使用它们？
我尝试了 chatgpt 但失败了：
drive_link = &#39;https://drive.google.com/file/d/1ofu7smGB7D2rwce_-1Tiz4tk8Wfwjpqn/view?usp=sharing&#39;
model_path = get_file(&#39;model.h5&#39;,drive_link,cache_dir=&#39;./&#39;)
模型 = tf.keras.models.load_model(model_path)
]]></description>
      <guid>https://stackoverflow.com/questions/78360641/how-can-i-integrate-my-ml-model-from-google-drive-to-the-production-backend</guid>
      <pubDate>Sun, 21 Apr 2024 06:44:41 GMT</pubDate>
    </item>
    <item>
      <title>像宠物一样的健身功能[关闭]</title>
      <link>https://stackoverflow.com/questions/78360524/fitness-function-to-be-like-a-pet</link>
      <description><![CDATA[我想对一个应该像宠物一样行动的机器人使用强化学习。健身函数基本上是什么样子的（不需要精确的代码）？你很难衡量宠物对主人产生的感受。如果有关于这个主题的任何科学研究，如果有人能给我一个链接就好了，因为我还没有发现任何东西？]]></description>
      <guid>https://stackoverflow.com/questions/78360524/fitness-function-to-be-like-a-pet</guid>
      <pubDate>Sun, 21 Apr 2024 05:42:20 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：float() 参数必须是字符串或实数，而不是“方法”[关闭]</title>
      <link>https://stackoverflow.com/questions/78360484/typeerror-float-argument-must-be-a-string-or-a-real-number-not-method</link>
      <description><![CDATA[我正在对泰坦尼克号数据集进行决策树分类。
&lt;前&gt;&lt;代码&gt;代码：
`model.fit(X_train,y_train)`
输出 ：
TypeError Traceback（最近一次调用最后一次）
〜\ AppData \ Local \ Temp \ ipykernel_19572 \ 2721349307.py 在？（）
----&gt; 1 model.fit(X_train,y_train)

c:\Users\Dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py 中？（估计器，*args，**kwargs）
   第1470章
   第1471章
   第1472章
   第1473章
-&gt;第1474章

c:\Users\Dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py 中？(self、X、y、sample_weight、check_input)
   1005 self：决策树分类器
   1006 拟合估计器。
   第1007章
   1008
-&gt;第1009章
   1010X，
   1011 年，
   第1012章

c:\Users\Dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py 中？(self、X、y、sample_weight、check_input、missing_values_in_feature_mask)
    第248章
    249 dtype=DTYPE，accept_sparse=“csc”，force_all_finite=False
...
   第2154章
   第2155章
   第2156章

类型错误：float() 参数必须是字符串或实数，而不是“方法”
输出被截断。作为可滚动元素查看或在文本编辑器中打开。调整单元格输出设置...

我期待正确的输出。]]></description>
      <guid>https://stackoverflow.com/questions/78360484/typeerror-float-argument-must-be-a-string-or-a-real-number-not-method</guid>
      <pubDate>Sun, 21 Apr 2024 05:18:20 GMT</pubDate>
    </item>
    <item>
      <title>Python-KNN预测误差和数据标准化</title>
      <link>https://stackoverflow.com/questions/78360476/python-knn-predict-error-and-data-normalization</link>
      <description><![CDATA[我是 M,L 的初学者，正在做我的第一份作业。即使执行教程指示的所有操作，我也无法使预测正常工作。
从 sklearn.neighbors 导入 KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=3, metric=&#39;euclidean&#39;)
knn_model.fit(train_val_process, merge_labels.label)
y_pred_knn = knn.predict(test_data_process)
准确度=metrics.accuracy_score(y_test, y_pred_knn)
print(&#39;准确度报告：&#39;, 准确度)

错误如下：
类型错误：KNeighborsClassifier.predict() 缺少 1 个必需的位置参数：&#39;X&#39;
我应该将数据均值标准化为 1 std 0。这样好吗？
from sklearn.preprocessing import StandardScaler
定标器=标准定标器()
缩放器.fit(pd.concat([train_data, val_data]))
train_data_process = pd.DataFrame(scaler.transform(train_data), columns=train_data.columns)
val_data_process = pd.DataFrame(scaler.transform(val_data), columns=val_data.columns)
train_val_process = pd.concat([train_data, val_data])
test_data_process = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)
y_test = test_labels.label

我期待它能够预测并完成这项工作。]]></description>
      <guid>https://stackoverflow.com/questions/78360476/python-knn-predict-error-and-data-normalization</guid>
      <pubDate>Sun, 21 Apr 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>在weka中重新采样过滤器</title>
      <link>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</link>
      <description><![CDATA[我的数据集中的数据实例数量很少。所以，我尝试了“重新采样” Weka中的过滤器可以增加数据量，从而提高模型性能。样本量百分比设置为200可以吗？因为那时我在交叉验证测试中获得了良好的相关系数。
我想知道将样本大小百分比设置为 200 时，重新采样过滤器是否工作正常。
使用此过滤器后，我的模型会准确预测吗？
由于数据量较少，是否有其他增强方法可以增强模型的性能？]]></description>
      <guid>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</guid>
      <pubDate>Sat, 20 Apr 2024 04:29:50 GMT</pubDate>
    </item>
    <item>
      <title>我尝试运行遗传算法来优化 ANN keras 模型的超参数。但我遇到很多错误</title>
      <link>https://stackoverflow.com/questions/78340561/i-try-to-run-genetic-algorithm-for-optimize-hyperparameters-of-ann-keras-model</link>
      <description><![CDATA[我正在尝试使用遗传算法优化我的 ANN 模型。我写的代码是：
# 创建模型的函数，KerasClassifier 所需
def create_model():
 # 创建模型
 模型=顺序（）
 model.add(Dense( 6, input_shape = (29,) , 激活 = &#39;relu&#39;, kernel_initializer = &#39;uniform&#39;))
 model.add(Dense(6, 激活 = &#39;relu&#39;, kernel_initializer = &#39;uniform&#39;))
 model.add(Dense(1, 激活=&#39;sigmoid&#39;, kernel_initializer = &#39;uniform&#39;))
# 编译模型
 model.compile(loss=&#39;binary_crossentropy&#39;, 优化器=&#39;Adam&#39;, 指标=[&#39;accuracy&#39;])
 返回模型

# 创建模型
模型= KerasClassifier（模型= create_model，详细= 0）

param_grid = {&#39;activation&#39;: [&quot;logistic&quot;, &quot;relu&quot;, &quot;Tanh&quot;],
              &#39;hidden_​​layer_sizes&#39;: [(50,),(100,), (50,70), (100,70), (100,100)],
              &#39;learning_rate&#39;: [&#39;常量&#39;,&#39;invscaling&#39;,&#39;自适应&#39;],
              }

cv = StratifiedKFold(n_splits=3, shuffle=True)

evolved_estimator = GASearchCV(估计器=模型,
                               简历=简历，
                               评分=&#39;准确率&#39;,
                               人口规模=10，
                               世代=35，
                               锦标赛大小=3，
                               精英主义=正确的，
                               交叉概率=0.8，
                               突变概率=0.1，
                               参数网格=参数网格，
                               标准=&#39;最大&#39;，
                               算法=&#39;eaMuPlusLambda&#39;,
                               n_工作=-1，
                               详细=真，
                               keep_top_k=4)

但是当运行它时，会显示：
ValueError：激活必须是整数、分类或连续类的有效实例
我尝试修复此错误。
经过搜索，我发现可以这样写（虽然我必须更改hidden_​​layer_sizes参数）：
param_grid = {&#39;activation&#39;: Categorical([&quot;logistic&quot;, &quot;relu&quot;, &quot;Tanh&quot;]),
              &#39;hidden_​​layer_sizes&#39;：整数（50,100），
              &#39;learning_rate&#39;：分类（[&#39;常量&#39;，&#39;invscaling&#39;，&#39;自适应&#39;]），
              }

cv = StratifiedKFold(n_splits=3, shuffle=True)

evolved_estimator = GASearchCV(估计器=模型,
                               简历=简历，
                               评分=&#39;准确率&#39;,
                               人口规模=10，
                               世代=35，
                               锦标赛大小=3，
                               精英主义=正确的，
                               交叉概率=0.8，
                               突变概率=0.1，
                               参数网格=参数网格，
                               标准=&#39;最大&#39;，
                               算法=&#39;eaMuPlusLambda&#39;,
                               n_工作=-1，
                               详细=真，
                               keep_top_k=4)

这部分代码被执行了。但是当我尝试在训练数据上拟合优化模型时，它给了我这个错误：
GA_result =volved_estimator.fit(x_train, y_train)

ValueError：估计器 KerasClassifier 的参数激活无效。这个问题可以通过在 KerasClassifier 构造函数中设置此参数来解决：KerasClassifier(activation=Tanh) 使用 estimator.get_params().keys() 检查可用参数列表
我真的很困惑。谁能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78340561/i-try-to-run-genetic-algorithm-for-optimize-hyperparameters-of-ann-keras-model</guid>
      <pubDate>Wed, 17 Apr 2024 11:14:52 GMT</pubDate>
    </item>
    <item>
      <title>当我一次读取 3 个或更多 rtsp 流时，OpenCV VideoCapture 出现灰屏</title>
      <link>https://stackoverflow.com/questions/76736531/opencv-videocapture-gives-me-a-gray-screen-when-im-reading-from-3-or-more-rtsp</link>
      <description><![CDATA[我正在 LAN 网络上读取 rtsp 流。当它是单个或 2 个流时，它运行平稳，但当我在 2 个以上的流上尝试它时，它开始给我一个灰色的屏幕，其中几乎没有可见的像素。
cap = cv2.VideoCapture(rtsp_uri)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)

fps = cap.get(cv2.CAP_PROP_FPS)
每帧后延迟 = 1/fps
打印（每个帧后延迟）

而真实：
    尝试：
        ret, 框架 = cap.read()
    除了：
        经过

    如果 self.stopper[camera_id]:
        del self.stopper[camera_id]
        返回

    如果不转：
        继续

    帧 = cv2.cvtColor(src=frame, 代码=cv2.COLOR_BGR2RGB)
    框架= cv2.调整大小（
        框架，
        (self.config.frame_width, self.config.frame_height)
    ）
    队列.入队（帧）
    time.sleep(delay_after_each_frame)

这里我使用队列来存储帧，并且该脚本在线程中运行。
有什么方法可以让我一次从多个摄像机读取流而没有那些灰色帧。
这是我得到的框架。

用于多个流和输出的代码。
self.config.get_camera_ids() 中的camera_id：
    如果camera_id不在self.threads中：
        self.threads[camera_id] = 线程(
            目标=self.process_rtsp_stream，
            args=(self.config.get_rtsp(camera_id),
                    self.queues[camera_id],camera_id)
        ）
        self.threads[camera_id].daemon = True
        self.threads[camera_id].start()

# 删除已删除相机的线程
对于 self.threads 中的camera_id：
    如果camera_id不在self.config.get_camera_ids()中：
        del self.threads[camera_id]
]]></description>
      <guid>https://stackoverflow.com/questions/76736531/opencv-videocapture-gives-me-a-gray-screen-when-im-reading-from-3-or-more-rtsp</guid>
      <pubDate>Fri, 21 Jul 2023 08:58:28 GMT</pubDate>
    </item>
    <item>
      <title>在决策树 ID3 算法中选择分区背后的直觉</title>
      <link>https://stackoverflow.com/questions/36105633/intuition-behind-choosing-partition-in-decision-tree-id3-algorithm</link>
      <description><![CDATA[我试图理解机器学习中决策树分类器背后的直觉。我知道决策树中每个节点的目标是进一步划分可能标签的当前空间，以便根据该节点给定问题的答案消除尽可能多的候选标签。但这与根据最小化分区“熵”的属性选择分区有什么关系呢？其中“熵”定义如下：
H(S) = −p_1*log2(p_1) −... −p_n*log2(p_n)

和分区熵：

&lt;前&gt;&lt;代码&gt;H = q_1*H(S_1) +...+ q_m*H(S_m)

其中 H(S)：给定子集的熵
     H：分区熵
     p_i&#39;s：属于 i 类的数据比例
     q_i&#39;s：基于给定分区属于子集 i 的数据比例

此外，每个节点的“问题”是否必须是是/否问题，从而将当前标签空间分成 2 个？而不是 3 个或更多子集？任何清晰的例子将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/36105633/intuition-behind-choosing-partition-in-decision-tree-id3-algorithm</guid>
      <pubDate>Sat, 19 Mar 2016 18:41:37 GMT</pubDate>
    </item>
    </channel>
</rss>