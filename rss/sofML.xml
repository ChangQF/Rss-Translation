<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 18 Apr 2024 18:17:32 GMT</lastBuildDate>
    <item>
      <title>KITTI数据集绘制历史轨迹</title>
      <link>https://stackoverflow.com/questions/78349057/kitti-dataset-plotting-historical-trajectory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78349057/kitti-dataset-plotting-historical-trajectory</guid>
      <pubDate>Thu, 18 Apr 2024 16:37:02 GMT</pubDate>
    </item>
    <item>
      <title>通过 Keras 训练同时检查不同类型的数据</title>
      <link>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</link>
      <description><![CDATA[在回归任务中，我得到以下数据：

具有已知标签的输入向量。 MSE损失应该用在预测和标签之间。
没有标签的输入向量对，已知模型应给出相似的结果。应在两个预测之间使用 MSE 损失。

同时将这两种数据拟合 Keras 模型的正确方法是什么？
理想情况下，我希望火车循环以交错的方式迭代这两种类型 - 一个有监督的（1）批次，然后是一个自我监督的（2）批次，然后再次监督，等等。
如果重要的话，我正在使用 Jax 后端。]]></description>
      <guid>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</guid>
      <pubDate>Thu, 18 Apr 2024 16:05:11 GMT</pubDate>
    </item>
    <item>
      <title>Cypress 测试自动化：频繁 UI 更改的自我修复？</title>
      <link>https://stackoverflow.com/questions/78348693/cypress-test-automation-self-healing-for-frequent-ui-changes</link>
      <description><![CDATA[目前，测试自动化框架严重依赖 UI 元素选择器（ID、类等）来与应用程序交互。然而，频繁的 UI 更改通常会导致这些选择器无效，从而导致测试脚本中需要手动更新。
这种手动干预成为了瓶颈，我正在寻求解决方案来提高框架对 UI 更改的适应能力。理想情况下，我希望框架能够：

识别选择器在测试执行期间何时无效。
提出定位目标元素的替代方法（可能使用新的选择器）。

我有兴趣结合机器学习 (ML) 来增强框架在遇到意外 UI 更改时建议新定位器的能力。

我熟悉自我修复测试自动化框架的概念，但尚未实施。所以不知道如何才能实现这一目标？
我愿意探索不同的解决方案，包括那些可能涉及机器学习 (ML) 的解决方案。
我可能缺少任何其他有效的、基于 JS 的赛普拉斯自我修复测试自动化策略吗？
集成 Python 模型进行定位器验证是否可行，或者是否有更简单的基于 JS 的替代方案？
如果 Python 集成是一个可行的选择，您能否推荐在 Cypress 框架中实现此目标的资源或最佳实践？

我很欣赏有关在 Cypress 中构建自我修复框架的任何建议或见解，该框架可以适应 UI 更改并减少测试脚本的手动维护。]]></description>
      <guid>https://stackoverflow.com/questions/78348693/cypress-test-automation-self-healing-for-frequent-ui-changes</guid>
      <pubDate>Thu, 18 Apr 2024 15:34:33 GMT</pubDate>
    </item>
    <item>
      <title>重新创建研究论文的结果</title>
      <link>https://stackoverflow.com/questions/78348126/recreating-results-from-research-paper</link>
      <description><![CDATA[所以我一直在尝试重新创建这篇特定论文的结果（神经协同过滤）。 
我使用的数据集非常类似于这个.
我知道我应该将数据放入训练集和测试集。
我的问题是我是否应该自己创建 test. Negative 文件，或者是否由代码内部的负采样自动处理它（基本上包含基于缺乏数据的负反馈）。
我非常感谢您的反馈！
这里是这篇论文在 github 上的官方实现。]]></description>
      <guid>https://stackoverflow.com/questions/78348126/recreating-results-from-research-paper</guid>
      <pubDate>Thu, 18 Apr 2024 14:10:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 AdamW 算法的张量流模型中的 MeanAbsolutePercentageError() 错误</title>
      <link>https://stackoverflow.com/questions/78347984/error-with-meanabsolutepercentageerror-in-tensorflow-model-with-adamw-algorith</link>
      <description><![CDATA[我试图找到人工神经网络中 2 个隐藏层的最佳节点数。我想使用 MeanAbsolutePercentageError 来评估模型的准确性。这是有错误的代码小节：
这是我的代码：
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从tensorflow.keras.losses导入MeanAbsoluteError
从tensorflow.keras.utils导入set_random_seed
从tensorflow.keras.losses导入MeanAbsolutePercentageError
从tensorflow.keras.optimizers导入AdamW
设置随机种子（777）

结果 = []
对于范围 (1, 8) 内的 A：
  对于范围 (1, 8) 内的 B：

    设置随机种子（777）
    模型=顺序（层=[密集（单位=A，激活=“elu”，input_shape=[TRAIN_X.columns.size]），
                                        密集（单位=B，激活=“elu”），
                                        密集(单位=1)])
    MODEL.compile( 优化器=AdamW() ,
                  损失=MeanAbsolutePercentageError() )
    模型.fit( TRAIN_X/10 , TRAIN_Y/10 ,
                          validation_data=[ TEST_X / 10 , TEST_Y / 10 ] , epochs=300 )
    预测 = 模型.预测( TEST_X / 10 ) * 10
    C = MeanAbsolutePercentageError( 预测 , TEST_Y )
    结果.append([A, B, C])

我期望结果采用 [A,B,C] 格式的数组，这意味着 A = 第一个隐藏层中的 # 个节点，B = 第二个隐藏层中的 # 个节点，C = 平均绝对百分比误差。但是，我收到了这个错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-10-59489aa7b7de&gt;在&lt;细胞系：10&gt;()
     20 validation_data=[ TEST_X / 10 , TEST_Y / 10 ] , epochs=300 )
     21 预测 = 模型.预测( TEST_X / 10 ) * 10
---&gt; 22 C = MeanAbsolutePercentageError( 预测 , TEST_Y )
     23 结果.append([A,B,C])

3帧
/usr/local/lib/python3.10/dist-packages/keras/src/utils/losses_utils.py 中的 validate(cls, key)
     第85章
     86 def 验证（cls，密钥）：
---&gt; 87 如果 key 不在 cls.all() 中：
     88 引发值错误（
     89 f&#39;无效的还原键：{key}。预期的键为“{cls.all()}”

ValueError：具有多个元素的数组的真值不明确。使用 a.any() 或 a.all()
]]></description>
      <guid>https://stackoverflow.com/questions/78347984/error-with-meanabsolutepercentageerror-in-tensorflow-model-with-adamw-algorith</guid>
      <pubDate>Thu, 18 Apr 2024 13:53:29 GMT</pubDate>
    </item>
    <item>
      <title>如何利用RMS获取首次预测时间</title>
      <link>https://stackoverflow.com/questions/78347968/how-to-obtain-the-first-prediction-time-by-using-rms</link>
      <description><![CDATA[我正在尝试实现从论文中找到的这一点：
首先，选取轴承故障采样点中的前l个采样点，计算其均方根值的平均值μlRMS和标准差sigmaRMS，并建立3σ判据——
相应地基于判断区间[μlRMS − 3σlRMS, μlRMS +3σlRMS]。
2) 其次，计算第l+1个点FPTl+1的RMS指数，并与步骤1中的判定区间进行比较。如果其值不在该范围内，则使l=l+1后重新计算判定区间如果其值在此范围内，则触发一次判断。
3）最后，为了避免误触发，以连续3次触发作为最终FPT的识别依据，并使本次FPTl = FPT
论文标题：物理引导神经网络：滚动体剩余使用寿命预测
通过动态使用长短期记忆网络的轴承
降解过程的权重
我完全不明白他们是如何实现这一目标的。如果我计算 RMS，我只有一个值，那么如何从 RMS 获取平均值和标准差。另外，通过尝试我的代码，FPT 在轴承寿命开始时开始。
def find_fpt(数据):
    上= np.mean(数据，轴=1) + np.std(数据，轴=1)
    下= np.mean（数据，轴= 1） - np.std（数据，轴= 1）

    rms_values = np.sqrt(np.mean(样本 ** 2, 轴=1))
    间隔 = [下[0], 上[0]]
    触发 = 0
    对于范围内的 i(len(rms_values))：

        如果间隔[1]&gt;均方根值[i+1]&gt;间隔[0]：
            触发 += 1
            如果触发器== 3：
                返回我
        别的：
            间隔 = [下[i + 1], 上[i + 1]]

这是我尝试编写的代码，但没有成功实现 FPT]]></description>
      <guid>https://stackoverflow.com/questions/78347968/how-to-obtain-the-first-prediction-time-by-using-rms</guid>
      <pubDate>Thu, 18 Apr 2024 13:51:23 GMT</pubDate>
    </item>
    <item>
      <title>用于一维数据中的台阶/扭结识别的卷积神经网络</title>
      <link>https://stackoverflow.com/questions/78347795/convolutional-neural-network-for-step-kink-identification-in-1d-data</link>
      <description><![CDATA[我正在尝试实现一个能够检测一维数据中的步骤/扭结的卷积神经网络。
阶跃被定义为随时间变化然后变为恒定的信号，反之亦然。一个例子是机械测试中机器位移发生变化，然后保持恒定。
为了训练模型，我使用以下简单函数生成了一些合成数据：
将 numpy 导入为 np
从张量流导入keras
随机导入
导入数学

def createStepFunction(步骤: int, dy: float = 1.0, tHold: float = 3600.0, dt: float = 100.0):
    t = [0.0]
    y = [0.0]
    对于 _ 在范围内（0，步数）：
        保持时间 = round(tHold*random.random(),0)
        t1 = t[-1] + round(dt*random.random(),0)
        t2 = t1 + 保持时间
        t3 = t2 + round(dt*random.random(),0)
        t4 = t3 + 保持时间

        y1 = y[-1] + dy*random.random()
        y2 = y1
        y3 = y2 - dy*random.random()
        y4 = y3
        t.extend([t1,t2,t3,t4])
        y.extend([y1,y2,y3,y4])

    tInterp = np.arange(0.0,t[-1]+0.5,1.0)
    yInterp = np.interp(tInterp, t, y)
    flag = np.array([1 if (elem in t and elem != 0.0) else 0 for elem in tInterp])
    # flagBool = [True if elem == 1 else False for elem in flag]
    返回 tInterp、yInterp、标志

t、y、标志 = createStepFunction(200)
t = t/np.max(t)
gaussian_noise = np.random.normal(np.mean(y), 5e-4, len(t))
yn = y*高斯噪声
yn = (yn - np.min(yn))/(np.max(yn) - np.min(yn))

我还在数据中添加了一些高斯噪声。结果如下：
示例数据
我的特征是 one-hot 向量，除了台阶/扭结的位置之外，所有位置都为 0，它们在此处被分配了值 1。
我使用此函数创建批量数据：
def createBatches(windowSize: int, arr):
    位置 = 0
    批次 = []

    而 pos + windowSize &lt;长度（arr）：
        值 = arr[pos:pos+windowSize]
        批次.append(值)
        pos += 窗口大小
    
    返回 np.asarray(批次)

窗口大小 = 200
xBatch = createBatches(windowSize, yn)
yBatch = createBatches(窗口大小, 标志)
n_timesteps = xBatch.shape[1]
n_特征 = 1
n_outputs = yBatch.shape[1]

我的 Keras 模型如下所示：
模型 = keras.models.Sequential()
model.add(keras.layers.Conv1D(filters=24，kernel_size=2，activation=&#39;relu&#39;，input_shape=(n_timesteps，n_features)))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;, strides=1))
model.add(keras.layers.MaxPooling1D(pool_size=2))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Flatten())
model.add（keras.layers.Dense（200，激活=&#39;relu&#39;））
model.add(keras.layers.Dense(n_outputs, 激活=&#39;softmax&#39;))
model.compile（loss=&#39;mean_squared_error&#39;，optimizer=keras.optimizers.Adam（learning_rate=0.05），metrics=[&#39;accuracy&#39;]）
模型.summary()


# 拟合网络
历元 = 1000
批量大小 = 200
详细 = 1

历史= model.fit(xBatch, yBatch, epochs=epochs, batch_size=batch_size, verbose=verbose)

该架构类似于我在一本书中找到的示例。
然而，该模型似乎与所提供的数据不相符。损失并没有明显下降，而且准确率很差。我尝试了各种参数但没有成功。
我错过了什么？我生成的训练数据有问题吗？
是否有可能使用这种方法来达到我的目的？]]></description>
      <guid>https://stackoverflow.com/questions/78347795/convolutional-neural-network-for-step-kink-identification-in-1d-data</guid>
      <pubDate>Thu, 18 Apr 2024 13:24:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有类标签的情况下可视化实例预测？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78345121/how-to-visualize-instance-predictions-but-without-class-labels</link>
      <description><![CDATA[导入 matplotlib.pyplot 作为 plt
Predicted_images_path = os.path.abspath(“/content/predicted”)
dataset_dicts_validation = DatasetCatalog.get(&#39;void-detection-2-valid&#39;)
对于 dataset_dicts_validation 中的 d：
    im = cv2.imread(d[“文件名”])
    输出 = 预测器(im)
    v = 展示台(im[:, :, ::-1],
                   元数据=元数据，
                   比例=0.5，
                   instance_mode=ColorMode.IMAGE_BW
    ）
    out = v.draw_instance_predictions(outputs[“instances”].to(“cpu”))
    图 = plt.figure(frameon=False, dpi=1)
    图.set_size_英寸(1024,1024)
    ax = plt.Axes(图, [0., 0., 1., 1.])
    ax.set_axis_off()
    图.add_axes(ax)
    ax.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB),spect=&#39;auto&#39;)
    Fig.savefig(f&quot;{predicted_images_path}/{d[&#39;file_name&#39;].split(&#39;/&#39;)[-1]}&quot;)

分割图像：

我使用 detectorron2 来训练模型并对检测到的空洞进行预测。如何仅标记分段而无需文字。
使用 ROBOFLOW 进行图像注释
二值图像：
]]></description>
      <guid>https://stackoverflow.com/questions/78345121/how-to-visualize-instance-predictions-but-without-class-labels</guid>
      <pubDate>Thu, 18 Apr 2024 05:51:09 GMT</pubDate>
    </item>
    <item>
      <title>在 OCI OML 中使用关联规则训练模型时出现错误</title>
      <link>https://stackoverflow.com/questions/78345096/getting-an-error-while-training-a-model-with-association-rule-in-oci-oml</link>
      <description><![CDATA[我正在使用 OCI 的机器学习功能来对我的数据使用关联规则模型。
我有以下设置。
设置 = {&#39;ASSO_MIN_SUPPORT&#39;:&#39;0.04&#39;,
&#39;ASSO_MIN_CONFIDENCE&#39;:&#39;0.1&#39;,
&#39;ASSO_MAX_RULE_LENGTH&#39;: &#39;2&#39;,
“ODMS_ITEM_ID_COLUMN_NAME”:“PRODUCT_NAME”}
ar_mod = oml.ar(**设置)
ar_mod = ar_mod.fit(SALES_TRANS_CUST, case_id = &#39;CUST_ID&#39;)
我收到以下错误：
oracledb.thick_impl._raise_from_info oracledb.exceptions.DatabaseError：ORA-40104：模型构建的训练数据无效
我正在尝试使用 fit() 方法训练模型，但它给出错误。]]></description>
      <guid>https://stackoverflow.com/questions/78345096/getting-an-error-while-training-a-model-with-association-rule-in-oci-oml</guid>
      <pubDate>Thu, 18 Apr 2024 05:42:48 GMT</pubDate>
    </item>
    <item>
      <title>滴灌管的自动检测和测量[关闭]</title>
      <link>https://stackoverflow.com/questions/78344991/automating-detection-and-measurement-of-drip-irrigation-pipes</link>
      <description><![CDATA[我正在使用无人机图像来自动从中提取一些数据。我想自动检测和测量田野中布置的灌溉线的长度，如下所示。我可以采取什么方法来实现这一目标？图像处理中是否有合适的基于规则的技术，例如颜色、边缘检测等？或者我应该使用一些对象检测技术？如果使用这些，注释和训练模型的最佳方法是什么？
该图像如下所示，下面显示了放大版本。

在此图像（上图的放大部分）中，您可以看到滴水线（其颜色始终为黑色，但有时在更复杂的背景和环境中，它们也可能并不总是直线，也可能是弯曲的）
]]></description>
      <guid>https://stackoverflow.com/questions/78344991/automating-detection-and-measurement-of-drip-irrigation-pipes</guid>
      <pubDate>Thu, 18 Apr 2024 05:09:44 GMT</pubDate>
    </item>
    <item>
      <title>流式 LightGBM 数据集构建在训练中冻结</title>
      <link>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</link>
      <description><![CDATA[我一直在尝试使用参考数据集（称为 ref_dataset）以流方式在 Python 中构建 LightGBM 数据集。我不确定它是如何完成的，它涉及调用 Dataset 类中看似非公共的方法。
我已经尝试过：
label_column = “标签”
权重列=“权重”
ref_dataset = lightgbm.Dataset(
   Sample_df.drop(列=[标签列，权重列])
   标签=sample_df[标签_列],
   权重=sample_df[权重列],
   参数=配置，
   **（ref_dataset_kwargs 或 {}），
）
ref_dataset.construct()
temp_dataset = lightgbm.Dataset（无，参考= ref_dataset，params = ref_dataset.get_params（））
# train_filenames_and_part_infos 只是一个元组列表[filename,part_info_dict]
估计行数=总和（
    part_info[“num_rows”] for _，train_filenames_and_part_infos 中的part_info
）
temp_dataset._init_from_ref_dataset(estimated_num_rows, ref_dataset._handle)

权重列表 = []
标签列表=[]
# 这个循环实际上不是我的代码，它更复杂，但基本上是它的作用
对于文件名，train_filenames_and_part_infos 中的 _：
    tbl: pyarrow.Table = load_from_file(文件名)
    标签 = tbl[label_column].to_pandas().to_numpy()
    权重 = tbl[weight_column].to_pandas().to_numpy()

    labels_list.append(标签)
    weights_list.append(权重)
    tbl = tbl.drop_columns([label_column, Weight_column])
    np_array: np.ndarray = tbl.to_pandas().to_numpy()
    如果 temp_dataset._start_row + np_array.shape[0] &gt; temp_dataset.num_data():
        raise RuntimeError(“数据集太小，无法容纳数据”)
    temp_dataset._push_rows(np_array)

all_weights = np.concatenate(weights_list)
all_labels = np.concatenate(labels_list)
实际长度 = all_weights.shape[0]
# 不幸的是，由于各种原因，这个估计并不准确
extra_zeros_features = np.zeros(
     （估计行数 - 实际长度，temp_dataset.num_feature()），dtype=np.float32
）
temp_dataset._push_rows(extra_zeros_features)
_LIB.LGBM_DatasetMarkFinished(temp_dataset._handle)
extra_zeros = np.zeros(估计行数 - 实际长度, dtype=np.float32)
temp_dataset.set_weight(np.concatenate([all_weights, extra_zeros]))
temp_dataset.set_label(np.concatenate([all_labels, extra_zeros]))

lightgbm.train(
    params=config, # 包含分布式投票并行训练的网络参数
    train_set=temp_dataset，
    num_boost_round=100,
    valid_sets=valid_sets, # 在其他地方初始化
    valid_names=valid_names, # 在其他地方初始化
    init_model=starting_model, # 不是很有必要
    **lightgbm_train_kwargs, # 空
）

不幸的是，当我运行这段代码时，我得到了这个控制台输出（有些行可能是无序的，因为我实际上是在分布式上运行它，并且日志是聚合的；我已经做了一些简单的编辑删除干扰线）：
[LightGBM] [Info] 总 bin 137618
[LightGBM] [Info] 尝试绑定端口 50627...
[LightGBM] [Info] 绑定端口50627成功
[LightGBM] [信息] 聆听...
[LightGBM] [Info] 训练集中的数据点数量：3934363，使用的特征数量：1382
[LightGBM] [信息] 连接到等级 0
[LightGBM] [信息] 连接到排名 1
[LightGBM] [信息] 连接到等级 2
[LightGBM] [信息] 连接到等级 3
[LightGBM] [信息] 已连接至等级 4
[LightGBM] [信息] 已连接至排名 5
[LightGBM] [信息] 已连接至排名 6
[LightGBM] [信息] 已连接至排名 8
[LightGBM] [Info] 本地排名：7，机器总数：9
[LightGBM] [Info] 自动选择col-wise多线程，测试开销为5.318313秒。
[LightGBM] [Info] 从分数-0.000000开始训练

然后它就坐在那里，CPU 和网络都处于空闲状态。我没有看到它在几个小时内取得任何进展。我已经检查了所有的排名，是不是我做错了什么？我还如何使用给定的样本进行构建？
更多信息：
检查空闲 Python 进程的堆栈跟踪显示代码卡在：
更新（lightgbm/basic.py:3891）
火车（lightgbm/engine.py:276）
...我的代码...

对于我正在使用的 LightGBM 版本 (4.3.0)，这对应于代码：
_safe_call(_LIB.LGBM_BoosterUpdateOneIter(
                self._handle,
                ctypes.byref(is_finished)))

另一个更新：
不同工人的垃圾箱数量似乎不同；有些有 137608、137612、137616。这是一个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</guid>
      <pubDate>Thu, 18 Apr 2024 02:25:39 GMT</pubDate>
    </item>
    <item>
      <title>以向量作为自变量而不是多个单值变量进行回归？</title>
      <link>https://stackoverflow.com/questions/78337383/regression-with-vector-as-independent-variable-instead-of-multiple-single-value</link>
      <description><![CDATA[我正在做一个项目，试图通过绘制书中句子的情绪来预测用户对书籍的评分（评论）。
给您一个想法的图表：
红色是得分最高的 25% 书籍的平均情绪图，
蓝色最差。
正如你所看到的，这些书的开头相当中等，在结尾之前就下降了，并且最后都具有很高的情绪。您还可以看到，最好得分 25%（红色）在最后达到最高点。
我想做的是使用回归来根据包含书中每个句子的情感分数的向量来预测一本书的分数。
我尝试了一些方法，但没有效果。
我的想法是将所有书籍分成 100 个部分，对每本书取这 100 个部分的平均值，并在此数据上训练支持向量回归模型（带有多边形内核）。然而，它的表现并不比每次都预测平均分数更好。
所以：
1 自变量 = [avg_sentiment1,avg_sentiment2,...avg_sentiment100]
1 因变量 = 分数（1 到 5 之间的数字，或更具体地说，在我们的数据集中，介于 ~3.200 和 ~4.700 之间）
因此，虽然使用此设置拟合 sklearn SVR 模型不会给出任何错误，但它似乎并没有学习（它的性能比始终预测平均值的虚拟预测器更差）。我尝试了几种具有不同参数的不同回归模型（Ridge、具有不同内核的 SVR、添加 SplineTransformer）。没有比随机做得更好的了。
我可以在网上找到的所有回归示例似乎都使用奇异值预测奇异值（因此自变量年龄 = 12，因变量身高 = 160 厘米，类似的东西），或者最多使用多个变量（添加更多奇异值）值，例如体重 = 67（公斤），作为自变量）。
我的回归是否将 100 个数字向量解释为 100 个不相关的变量？这有关系吗？
救命，我已经超出了我的能力范围。哪种技术最适用于此？最好是我可以在 SKLearn 上找到的东西，我不是专家（正如您可能知道的那样）]]></description>
      <guid>https://stackoverflow.com/questions/78337383/regression-with-vector-as-independent-variable-instead-of-multiple-single-value</guid>
      <pubDate>Tue, 16 Apr 2024 21:22:19 GMT</pubDate>
    </item>
    <item>
      <title>根据单个树获取 XGBoost 预测</title>
      <link>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</link>
      <description><![CDATA[它可能与如何获取每棵树的重复xgboost 中的预测？ 但解决方案不再有效（可能是 XGBoost 库上的更改）。我的想法是以原始格式 model.get_booster().get_dump() 转储模型并在不同的平台中实现它（仅预测）。不过，我首先尝试用 python 来实现它。运行以下代码，使用所有单独的增强器进行预测并将它们组合起来，不会返回与 model.predict() 函数相同的结果。
有什么方法可以将 model.predict() 与助推器的组合相匹配吗？我错过了什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb
从sklearn导入数据集
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# 加载数据
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# 拟合模型
模型 = xgb.XGBClassifier(
    n_估计器=10，
    最大深度=10，
    use_label_encoder=False,
    目标=&#39;二进制：逻辑&#39;
）
模型.fit(X, y)
booster_ = model.get_booster()

# 提取个人预测
individual_preds = []
对于 booster_ 中的 tree_：
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    ）
individual_preds = np.vstack(individual_preds)

# 将个体预测汇总为最终预测
individual_logits = inverse_sigmoid(individual_preds)
Final_logits = individual_logits.sum(axis=0)
Final_preds = sigmoid(final_logits)

# 验证正确性
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)

&lt;块引用&gt;
断言错误：数组不几乎等于小数点后 7 位
不匹配的元素：150 / 150 (100%)
最大绝对差：0.90511334
最大相对差值：0.99744916
x: 数组([7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05、7.4847587e-05、7.4847587e-05、7.4847587e-05、
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,...
y: 数组([0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,...
]]></description>
      <guid>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</guid>
      <pubDate>Tue, 16 Apr 2024 12:49:27 GMT</pubDate>
    </item>
    <item>
      <title>如何找到 RBF 核 SVM 的准确性？</title>
      <link>https://stackoverflow.com/questions/78325995/how-can-i-find-accuracy-in-rbf-kernal-svm</link>
      <description><![CDATA[我正在尝试使用 SVM 实现人体检测。我正在使用 HOG 特征提取，然后对其应用 SVM。当我应用线性 SVM 时，我会得到图像的分数，但在 RBF kernal SVM 中我只能得到 0 和 1。无论如何我可以获得检测分数吗？我怎样才能像线性SVM一样看到RBF核的系数？
我尝试了Python提供的SVM库。]]></description>
      <guid>https://stackoverflow.com/questions/78325995/how-can-i-find-accuracy-in-rbf-kernal-svm</guid>
      <pubDate>Mon, 15 Apr 2024 03:45:27 GMT</pubDate>
    </item>
    <item>
      <title>将 MLflow 项目与 docker-compose 结合起来</title>
      <link>https://stackoverflow.com/questions/68753631/combine-mlflow-projects-with-docker-compose</link>
      <description><![CDATA[我面临以下情况：
我们在 docker 容器中训练模型，该容器是通过运行 docker-compose 文件构建的。我已经实现了 MLflow 来与 docker-compose 一起使用（通过执行类似于这篇文章的操作： https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039），再创建两个容器（一个用于服务器，一个用于 postgresql 后端）。
然而，故事并没有结束。我们的目标是实现完整的机器学习管道，其中包括数据创建、预处理步骤等。我知道，机器学习项目有助于创建这样的管道。我已经看到它被设计为与 docker 图像一起使用（https://www.mlflow .org/docs/latest/projects.html），但我不明白，如何将它与 docker-compose 一起使用。
您能否通过提供任何提示、指南、文档等来帮助我？
或者总的来说，有什么建议吗？如何使用 mlflow 实现完整的机器学习管道？
非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/68753631/combine-mlflow-projects-with-docker-compose</guid>
      <pubDate>Thu, 12 Aug 2021 07:58:26 GMT</pubDate>
    </item>
    </channel>
</rss>