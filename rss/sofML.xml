<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Sep 2024 06:24:24 GMT</lastBuildDate>
    <item>
      <title>SBERT 微调总是在完成所有 epoch 之前停止</title>
      <link>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</link>
      <description><![CDATA[我正在使用 SBERT 预训练模型（特别是 MiniLM）进行一个包含 995 个分类的文本分类项目。我大部分时间都在按照此处列出的步骤进行操作，一切似乎都运行正常。
我的问题出现在实际训练模型时。无论我在训练参数中设置什么值，训练似乎总是提前结束，并且永远不会完成所有批次。例如，我设置了 num_train_epochs=1，但它最多只能达到 0.49 个 epoch。如果 num_train_epochs=4，它总是在 3.49 个 epoch 处结束。
这是我的代码：
from datasets import load_dataset
from sentence_transformers import (
SentenceTransformer,
SentenceTransformerTrainer,
SentenceTransformerTrainingArguments,
SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
&quot;nreimers/MiniLM-L6-H384-uncased&quot;,
model_card_data=SentenceTransformerModelCardData(
language=&quot;en&quot;,
license=&quot;apache-2.0&quot;,
model_name=&quot;all-MiniLM-L6-v2&quot;,
)
)

loss = BatchAllTripletLoss(model)
# 损失概述：https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# 此特定损失方法：https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss

# 训练参数：https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
# 必需参数：
output_dir=&quot;finetune/model20240924&quot;,
# 可选训练参数：
num_train_epochs=1,
max_steps = -1,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
learning_rate=1e-5,
warmup_ratio=0.1,
fp16=True, # 如果您收到 GPU 无法在 FP16 上运行的错误，请设置为 False
bf16=False, # 如果您拥有支持 BF16 的 GPU，请设置为 True
batch_sampler=BatchSamplers.GROUP_BY_LABEL, # 
# 可选的跟踪/调试参数：
eval_strategy=&quot;no&quot;,
eval_steps=100,
save_strategy=&quot;epoch&quot;,
# save_steps=100,
save_total_limit=2,
logs_steps=100,
run_name=&quot;miniLm-triplet&quot;, # 如果在 W&amp;B 中使用`wandb` 已安装
)

trainer = SentenceTransformerTrainer(
model=model,
args=args,
train_dataset=trainDataset,
eval_dataset=devDataset,
loss=loss,
#evaluator=dev_evaluator,
)
trainer.train()

请注意，我没有使用评估器，因为我们正在创建模型，并在事后使用专用的测试值集对其进行测试。我的数据集结构如下：
Dataset({
features: [&#39;Title&#39;, &#39;Body&#39;, &#39;label&#39;],
num_rows: 23961
})

与 dev 数据集具有相同的结构，只是行数较少。这将提供以下输出：
 [1473/2996 57:06 &lt; 59:07，0.43 it/s，Epoch 0/1]
步骤训练损失
100 1.265600
200 0.702700
300 0.633900
400 0.505200
500 0.481900
600 0.306800
700 0.535600
800 0.369800
900 0.265400
1000 0.345300
1100 0.516700
1200 0.372600
1300 0.392300
1400 0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={&#39;train_runtime&#39;: 3427.9198, &#39;train_samples_per_second&#39;: 6.99, &#39;train_steps_per_second&#39;: 0.874, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5003972503496366, &#39;epoch&#39;: 0.4916555407209613})

尽管我调整了值，但还是无法完成所有批次。任何建议都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</guid>
      <pubDate>Wed, 25 Sep 2024 03:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何将 CIFAR10 模型的准确率提高到 80% 以上？</title>
      <link>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</link>
      <description><![CDATA[有人能帮助我吗？我使用来自 tensorflow 数据集的 CIFAR10 数据集训练我的机器学习模型，但我无法将模型准确率提高到 80% 以上...
有人能给我一个建议吗？
import tensorflow as tf
import time
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

def normalize(train_images, test_images):
normalized_train_dataset = tf.cast(train_images, tf.float32) / 255.0
normalized_test_dataset = tf.cast(test_images, tf.float32) / 255.0
返回 normalized_train_dataset, normalized_test_dataset

# Normalisasi Dataset
train_dataset, test_dataset = normalize(train_images, test_images)

def visual(image, image_sample=2):
for i in range (image_sample):

print(f&quot;弯曲图像：{np.shape(image)}&quot;)
print(f&quot;弯曲数据：{image[i].dtype}&quot;)
print(f&quot;Nilai 最大图像：{np.max(image[i])}&quot;)
print(f&quot;Nilai 最小图像：{np.min(image[i])}&quot;)

plt.figure(figsize=(6,6))
plt.imshow(image[i])
plt.axis(&#39;off&#39;)
plt.colorbar()
plt.title(&quot;Gambar CIFAR-10&quot;)
plt.grid(False)
plt.show()

visualization(train_dataset)

train_labels = np.squeeze(train_labels)
test_labels = np.squeeze(test_labels)

print(f&quot;Shape Of Train Label : {train_labels.shape}&quot;)

print(f&quot;Shape Of Test_Label : {test_labels.shape}&quot;)

train_labels= to_categorical(train_labels, num_classes=10)
test_labels = to_categorical(test_labels, num_classes=10)

从 tensorflow.keras.preprocessing.image 导入 ImageDataGenerator

datagen = ImageDataGenerator(
rotation_range=20,
width_shift_range=0.2,
height_shift_range=0.2,
sheath_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,
fill_mode=&#39;nearest&#39;
)

model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(32, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu, input_shape=(32, 32, 3)),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(64, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(512, (3,3)，padding=&#39;same&#39;，activation=tf.nn.relu，kernel_regularizer=tf.keras.regularizers.l2(0.01))，
tf.keras.layers.BatchNormalization()，
tf.keras.layers.MaxPool2D((2,2)，strides=2)，

tf.keras.layers.Flatten()，
tf.keras.layers.Dense(512，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.3)，

tf.keras.layers.Dense(128，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.5)，

tf.keras.layers.Dense(10，激活=tf.nn.softmax)
])

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

early_stopping = tf.keras.callbacks.EarlyStopping(
monitor=&#39;val_loss&#39;,
patience=5,
restore_best_weights=True
)

reducer_lr = tf.keras.callbacks.ReduceLROnPlateau(
monitor=&#39;val_loss&#39;,
factor=0.2,
patience=3,
verbose=1,
min_lr=0.00001
)

callbacks = [early_stopping, reducer_lr]

start_time = time.time()

history = model.fit(datagen.flow(
train_dataset,
train_labels,
batch_size=64),
epochs=30,
validation_data=(test_dataset, test_labels),
callbacks=callbacks,
verbose=1
)

end_time = time.time()
training_time = end_time - start_time
print(f&quot;训练时间：{training_time/60:.2f} 分钟&quot;)

model.save(&#39;hand_gesture_detect.keras&#39;)

# 评估模型
loss_val, accuracy_val = model.evaluate(test_dataset, test_labels)
print(f&quot;损失：{loss_val}&quot;)
print(f&quot;准确率：{accuracy_val}&quot;)

来自 tensorflow.keras.applications 导入 ResNet50

base_model = ResNet50(weights=&#39;ImageNet&#39;, include_top=False, input_tensor=(32, 32, 3))

我已经使我的模型复杂化，但准确率仍然只有 77-80%，我不知道如何提高我的模型准确率]]></description>
      <guid>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</guid>
      <pubDate>Wed, 25 Sep 2024 01:55:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在 GPU 上运行 gridSearchCV 或 randonizedSerchCV</title>
      <link>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</link>
      <description><![CDATA[我想运行 gridSearchCV 或 randonizedSerchCV 来使用 GPU 在 Colab 环境中调整超参数。
但我找不到这些函数与 GPU 兼容的实现。
在这种情况下，我该如何调整超参数？
因此，由于我找不到在 GPU 上调整超参数的函数，我尝试实现 randonizedSerchCV。但我认为一定有一种方法可以做到这一点，而无需手动实现该函数。]]></description>
      <guid>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</guid>
      <pubDate>Wed, 25 Sep 2024 01:53:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型会拍摄两张图像并验证它们是否相同。
我在网上搜索过，但只能找到有关识别（一对多）的答案。
如何在文本或代码中创建这样的模型？
为了澄清起见，我说的相同是指眼睛相同，即它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>如何为排名模型生成数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</link>
      <description><![CDATA[我正在尝试创建两阶段推荐系统：使用矩阵分解生成候选对象，并使用 Lambdarank 排名模型对其进行排名。我有两个选项来生成数据集：

取 128 个项目
（相关项目 + 随机项目填充），使用第一个模型对其进行评分和排序，然后使用此序列训练第二个模型（因此我们始终具有相对值。
对所有项目进行评分，排序并取前 128 个项目，然后进行训练（我们可能没有相关项目，但我认为这更自然，因为在生产中我们将以这种方式进行预测）。

那么，哪个更好？此外，在训练中使用小权重（类似于隐式 ALS）填充项目是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</guid>
      <pubDate>Tue, 24 Sep 2024 16:14:00 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</guid>
      <pubDate>Tue, 24 Sep 2024 15:17:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将预测值合并回数据集？</title>
      <link>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</link>
      <description><![CDATA[我已经在 Python 中训练了一个 XGboost 模型，并将概率列表作为输出。我如何将这些概率带到原始数据集，以便在一个 DF 中拥有数据 + 预测值？假设我的原始原始测试 df 称为 df_raw。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
model = XGBClassifier(n_estimators=1500, max_depth=5, n_jobs=-1, min_child_weight=2, 
early_stopping_rounds=25)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
test_outputs = model.predict_proba(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</guid>
      <pubDate>Tue, 24 Sep 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>cuDNN 错误：CUDNN_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/79018072/cudnn-error-cudnn-status-execution-failed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79018072/cudnn-error-cudnn-status-execution-failed</guid>
      <pubDate>Tue, 24 Sep 2024 10:04:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 smote 将过采样数据存储在单独的变量中？</title>
      <link>https://stackoverflow.com/questions/79016928/how-can-i-store-the-oversampled-data-using-smote-in-a-separate-variable</link>
      <description><![CDATA[应用 Smote 过采样技术后，我只想将新生成的值存储到 X2 和 y2。X2 的独立特征和 y2 的目标变量
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
dataset = pd.read_csv(&#39;https://archive.ics.uci.edu/static/public/17/data.csv&#39;)
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values
le = LabelEncoder()
y = le.fit_transform(y)
smt = SMOTE()
X1, y1 = smt.fit_resample(X, y)
#在单独的变量中使用 smote 对数据进行过采样
#X2 = ?
#y2 = ?

]]></description>
      <guid>https://stackoverflow.com/questions/79016928/how-can-i-store-the-oversampled-data-using-smote-in-a-separate-variable</guid>
      <pubDate>Tue, 24 Sep 2024 04:07:41 GMT</pubDate>
    </item>
    <item>
      <title>通过模型的大规模测试预测毒性测定[关闭]</title>
      <link>https://stackoverflow.com/questions/79016340/predicting-toxicity-assay-through-mass-testing-of-models</link>
      <description><![CDATA[我目前正在创建一个模型来预测污染对生物体的毒性测定。由于没有合适的数据集，我还没有尝试任何东西。我只是想问问我的代码是否合适。欢迎提出批评。此外，如果我遗漏了什么或应该包括什么，请告诉我。
此外，我正在考虑更多模型，例如 RandomForestRegressor、Boosting（AdaBoost、GradientBoost）。我应该考虑这些吗？此外，当我最终获得数据时，是否有任何模型我应该从测试中删除？
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(&#39;&#39;) # 包含数据的 csv 文件（浓度和死亡率）

# 基本图表
sns.scatterplot(data = df, x = &#39;Concentration&#39;, y = &#39;Mortality&#39;) 

# 训练与测试的基本划分
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=101) 

# 线性模型
from sklearn.linear_model import LinearRegression 
lr_model = LinearRegression()
lr_model.fit(X_train,y_train)
lr_preds = lr_model.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
mean_absolute_error(y_test, lr_preds)
np.sqrt(mean_absolute_error(y_test, lr_preds))
concentration_range = np.arange(0,100) # 根据最小/最大浓度调整
concentration_preds = lr_model.predict(concentration_range.reshape(-1,1))
plt.figure(figsize = (12,6),dpi = 200)
sns.scatterplot(data = df, x = &#39;Concentration&#39;, y = &#39;信号&#39;)
plt.plot(concentration_range,concentration_preds)

# 多项式模型
# 用于测试模型的函数
def run_model(model, X_train, y_train, X_test, y_test):
# 拟合模型
model.fit(X_train,y_train)

# 获取指标
preds = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,preds))
mae = mean_absolute_error(y_test, preds)
print(f&#39;MAE: {mae}&#39;)
print(f&#39;RMSE: {rmse}&#39;)

# 绘制结果模型信号范围
density_range = np.arange(0,100) # 再次调整
density_preds = model.predict(concentration_range.reshape(-1,1))

plt.figure(figsize = (12,8), dpi = 200)
sns.scatterplot(x = &#39;Concentration&#39;, y = &#39;Mortality&#39;, data = df, color = &#39;black&#39;)
plt.plot(concentration_range, density_preds)

来自 sklearn.pipeline 导入 make_pipeline
来自 sklearn.preprocessing 导入 PolynomialFeatures

pipe = make_pipeline(PolynomialFeatures(degree = 2),LinearRegression()) # degree 可调整
run_model(pipe, X_train, y_train, X_test, y_test)

# K-Nearest Neighbors 模型
来自 sklearn.neighbors 导入 KNeighborsRegressor
k_values = [1,2,3,4,5,6,7,8,9,10]
for k in k_values:

model = KNeighborsRegressor(n_neighbors=k)
run_model(model, X_train,y_train,X_test, y_test)

# 决策树模型
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
run_model(model, X_train, y_train, X_test, y_test)

# SVR 模型
from sklearn.svm import SVR # 支持向量回归
from sklearn.model_selection import GridSearchCV
svr = SVR()
param_grid = {&#39;C&#39;:[0.01,0.1,1,5,10,100,1000],
&#39;gamma&#39;:[&#39;auto&#39;,&#39;scale&#39;]}

grid = GridSearchCV(svr, param_grid)
run_model(grid, X_train,y_train,X_test, y_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79016340/predicting-toxicity-assay-through-mass-testing-of-models</guid>
      <pubDate>Mon, 23 Sep 2024 21:25:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个面部 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)

为什么即使 ROI 已被裁剪，该错误仍然如此持续，解决此错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用回归算法而不是分类算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms</link>
      <description><![CDATA[众所周知，在 ML 中，如果依赖特征本质上是连续的，则应用回归模型。但是，如果依赖特征本质上是分类的，则使用分类算法。
正如您在这张图（https://i.sstatic.net/9Q3wfudK.png）中看到的那样，最大值为。大量数据点重复出现，表明它们正在形成类别。
那么，为什么这里使用回归？
这是数据集：（https://drive.google.com/file/d/1vTIiQ0NZKgBI-EfpGzfPKHx1VaAdEYdH/view?usp=sharing）
我和同学、老师讨论了这个问题。他们都说回归是用来预测的，但没人能解释他们是如何得出应该用回归来代替分类的结论的。]]></description>
      <guid>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms</guid>
      <pubDate>Mon, 23 Sep 2024 07:10:30 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何 Python 库可以使用相机检测被检测物体的纬度和经度？[关闭]</title>
      <link>https://stackoverflow.com/questions/62105606/is-there-any-python-libraries-to-detect-latitude-and-longitude-of-detected-objec</link>
      <description><![CDATA[我想使用带有经度和纬度的卫星摄像机来检测物体。]]></description>
      <guid>https://stackoverflow.com/questions/62105606/is-there-any-python-libraries-to-detect-latitude-and-longitude-of-detected-objec</guid>
      <pubDate>Sat, 30 May 2020 16:40:13 GMT</pubDate>
    </item>
    <item>
      <title>什么是 Killed:9 以及如何在 macOS 终端中修复？</title>
      <link>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</link>
      <description><![CDATA[我有一段用于机器学习项目的简单 Python 代码。我有一个相对较大的自发语音数据库。我开始训练我的语音模型。由于这是一个庞大的数据库，我让它连夜工作。早上我醒来时看到终端中出现一个神秘的
Killed: 9
行。没有其他内容。没有其他错误消息或需要处理的内容。代码运行良好约 6 小时，占整个过程的 75%，所以我真的不明白哪里出了问题。
什么是 Killed:9 以及如何修复它？浪费数小时的计算时间非常令人沮丧……
如果这很重要，我正在使用 macOS Mojave 测试版。提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</guid>
      <pubDate>Tue, 14 Aug 2018 03:28:58 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中“Flatten”起什么作用？</title>
      <link>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</link>
      <description><![CDATA[我正在尝试了解 Keras 中 Flatten 函数的作用。下面是我的代码，这是一个简单的两层网络。它接收形状为 (3, 2) 的二维数据，并输出形状为 (1, 4) 的一维数据：
model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation(&#39;relu&#39;))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;SGD&#39;)

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape

这会打印出 y 具有形状 (1, 4)。但是，如果我删除 Flatten 行，则它会打印出 y 具有形状 (1, 3, 4)。
我不明白这一点。根据我对神经网络的理解，model.add(Dense(16, input_shape=(3, 2))) 函数正在创建一个隐藏的完全连接层，其中包含 16 个节点。这些节点中的每一个都连接到每个 3x2 输入元素。因此，第一层输出处的 16 个节点已经是“平坦的”。因此，第一层的输出形状应该是 (1, 16)。然后，第二层将其作为输入，并输出形状为 (1, 4) 的数据。
那么，如果第一层的输出已经是“平坦的”并且形状为 (1, 16)，为什么我需要进一步将其平坦化？]]></description>
      <guid>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</guid>
      <pubDate>Wed, 05 Apr 2017 16:48:24 GMT</pubDate>
    </item>
    </channel>
</rss>