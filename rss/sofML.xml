<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Mar 2024 09:14:10 GMT</lastBuildDate>
    <item>
      <title>此代码不适用于tensorflow 2.16.0+版本</title>
      <link>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</link>
      <description><![CDATA[检查点 = ModelCheckpoint(
    &#39;./base.model&#39;,
    监视器=&#39;val_accuracy&#39;,
    详细=1，
    save_best_only=真，
    模式=&#39;最大&#39;,
    save_weights_only=假,
    保存频率=1
）
提前停止=提前停止(
    监视器=&#39;val_loss&#39;,
    最小增量=0.001，
    耐心=30，
    详细=1，
    模式=&#39;自动&#39;
）

opt1 = tf.keras.optimizers.Adam()

回调= [检查点，提前停止]

这不适用于tensorflow 2.16.1
但是，正在 google colab 上开发 2.15.0
我如何修复我的代码或如何安装tensorflow 2.15.0？
我尝试了pip install tensorflow=2.15.0
但是，它显示错误]]></description>
      <guid>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</guid>
      <pubDate>Tue, 26 Mar 2024 08:41:28 GMT</pubDate>
    </item>
    <item>
      <title>无法在 LightGBM 中检索 best_iteration</title>
      <link>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</link>
      <description><![CDATA[这是我的代码：
def 目标（试验、train_set、valid_set、num_iterations）：
    
    参数 = {
        &#39;目标&#39;：&#39;二进制&#39;，
        &#39;任务&#39;:&#39;训练&#39;,
        &#39;提升&#39;：&#39;gbdt&#39;，
        &#39;度量&#39;：[&#39;auc&#39;]，
        &#39;n_estimators&#39;：num_iterations，
        “冗长”：-1，
        &#39;feature_pre_filter&#39;：假，
        &#39;学习率&#39;: Trial.suggest_float(&#39;学习率&#39;, 0.01, 0.3),
        &#39;num_leaves&#39;: Trial.suggest_int(&#39;num_leaves&#39;, 2, 256),
        &#39;最大深度&#39;: Trial.suggest_int(&#39;最大深度&#39;, 3, 12),
        &#39;min_data_in_leaf&#39;: Trial.suggest_int(&#39;min_data_in_leaf&#39;, 20, 10000),
        &#39;lambda_l1&#39;: Trial.suggest_float(&#39;lambda_l1&#39;, 1e-10, 10.0, log=True),
        &#39;lambda_l2&#39;: Trial.suggest_float(&#39;lambda_l2&#39;, 1e-10, 10.0, log=True),
        &#39;min_gain_to_split&#39;: Trial.suggest_float(&#39;min_gain_to_split&#39;, 0, 15),
        &#39;feature_fraction&#39;: Trial.suggest_float(&#39;feature_fraction&#39;, 0.2, 1.0),
        &#39;bagging_fraction&#39;: Trial.suggest_float(&#39;bagging_fraction&#39;, 0.2, 1.0),
        &#39;bagging_freq&#39;: Trial.suggest_int(&#39;bagging_freq&#39;, 1, 10),
    }

    
    pruning_callback = optuna.integration.LightGBMPruningCallback(Trial, &#39;auc&#39;, valid_name=&#39;valid_set&#39;)
    
    最佳评估结果 = {}
    模型 = lgb.train(
        参数，
        训练集=训练集，
        valid_sets=[训练集, valid_set],
        valid_names=[&#39;train_set&#39;, &#39;valid_set&#39;],
        回调=[修剪_回调，
                   记录评估（最佳评估结果），
                   提前停止(100)
                  ]
    ）
    Trial.set_user_attr(key=&#39;best_booster&#39;, value=模型)
    Trial.set_user_attr(key=&#39;best_eval_result&#39;, value=best_eval_result)

    prob_pred = model.predict(feature_test, num_iteration=model.best_iteration)
    返回 roc_auc_score(label_test, prob_pred, labels=[0,1])

func = lambda 试验：目标（试验=试验，train_set=train_set，valid_set=valid_set，num_iterations=num_iterations）


迭代次数 = 1000

研究 = optuna.create_study(
    修剪器=optuna.pruners.HyperbandPruner(),
    方向=&#39;最大化&#39;
）

optuna.logging.set_verbosity(optuna.logging.警告)

def save_best_booster（研究，试用）：
    如果 Study.best_Trial.number == Trial.number：
        Study.set_user_attr(key=&#39;best_booster&#39;, value=Trial.user_attrs[&#39;best_booster&#39;])
        Study.set_user_attr(key=&#39;best_eval_result&#39;, value=Trial.user_attrs[&#39;best_eval_result&#39;])

研究.优化(
    功能，
    n_试验=30，
    show_progress_bar=真，
    回调=[save_best_booster]
）

试验 = 研究.best_试验

best_model=study.user_attrs[&#39;best_booster&#39;]


训练输出日志确实有一些如下内容：
提前停止，最佳迭代是：
[30]train_set的auc：0.982083 valid_set的auc：0.874471
训练直到 100 轮验证分数没有提高

假设高于 valid_set 的 auc: 0.874471 的 auc 值确实是所有迭代中的最佳值，则 best_iteration 应为 [30]，如上所示。
但是，我通过调用 best_model.best_iteration 得到了“-1”，如下所示：
在： print(best_model.best_iteration)

输出：-1

感谢谁能解决我的问题！
期待您的回复:)]]></description>
      <guid>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</guid>
      <pubDate>Tue, 26 Mar 2024 08:12:26 GMT</pubDate>
    </item>
    <item>
      <title>有关正确/错误解决方案的 C 代码数据集</title>
      <link>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</link>
      <description><![CDATA[我正在寻找有关体育编程任务解决方案的公共数据集（例如来自 LeetCode、timus...）。问题是，为了微调我的模型，我需要正确和错误的解决方案。例如，在 Leetcode 上，人们可以找到通过每项任务的所有测试的解决方案（但不确定在未经许可的情况下使用它们是否公平），但根本没有公开开放的不正确解决方案那里。
所需的编程语言是 C。
有人可以帮我吗？
我检查了 HuggingFace 上的所有“代码”数据集，同样什么也没有。]]></description>
      <guid>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</guid>
      <pubDate>Tue, 26 Mar 2024 07:46:53 GMT</pubDate>
    </item>
    <item>
      <title>我们如何创建或者什么类型的机器学习或深度学习模型可用于创建用于安排比赛的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78223241/how-can-we-create-or-what-type-of-machine-learning-or-deeplearning-models-be-use</link>
      <description><![CDATA[想要创建一个网球锦标赛时间表
我还没有尝试过，但想了解启动和训练模型的基本知识。我有一组过去的比赛数据，其中有各种限制，所以我想创建一个可以进行预测的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78223241/how-can-we-create-or-what-type-of-machine-learning-or-deeplearning-models-be-use</guid>
      <pubDate>Tue, 26 Mar 2024 06:01:20 GMT</pubDate>
    </item>
    <item>
      <title>如何有效利用机器学习专业课程可选实验室（吴恩达先生）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78223041/how-to-effectively-use-optional-labs-in-machine-learning-specialization-course</link>
      <description><![CDATA[我应该复制实验室中给出的代码并在本地运行吗？另外，您如何使用可选实验室，请详细说明（提供分步路径）。
目前，我正在阅读代码并了解实验室中使用的库。
但我相信这不是理想的方式。我没有充分发挥实验室的优势。]]></description>
      <guid>https://stackoverflow.com/questions/78223041/how-to-effectively-use-optional-labs-in-machine-learning-specialization-course</guid>
      <pubDate>Tue, 26 Mar 2024 04:47:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flower 和 Tensorflow 来结束联邦学习中服务器的额外参数？</title>
      <link>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</link>
      <description><![CDATA[我想将带有模型更新的额外参数发送到服务器，然后将服务器中的这些额外参数用于其他目的。我在这个项目中使用 Flower 和 Tensorflow。在发送额外参数之前，我的模型运行良好。目前我有这些代码客户端模型 server.py。
如何在服务器中成功发送额外参数或值并接收它？
感谢您的帮助。
我尝试在 get_parameter 方法中发送附加参数，并使用 FedAvg 策略接收它。但我一次又一次地遇到这个错误。 错误]]></description>
      <guid>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</guid>
      <pubDate>Mon, 25 Mar 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>这段代码应该做什么？它继续执行并且不会停止[关闭]</title>
      <link>https://stackoverflow.com/questions/78221744/what-does-this-code-is-supposed-to-do-it-keeps-executing-and-does-not-stop</link>
      <description><![CDATA[我正在使用 Google Colab 上的 CelebA 数据集开发生成模型。一切都运行良好，但它要执行这个单元，它会继续执行。我应该在执行时做一些事情吗？为什么一直执行3、4个小时？
这是代码：
导入时间
迭代次数 = 15000
批量大小 = 16

RES_DIR = &#39;res2&#39;
FILE_PATH = &#39;%s/生成_%d.png&#39;
如果不是 os.path.isdir(RES_DIR):
    os.mkdir(RES_DIR)

CONTROL_SIZE_SQRT = 6
control_vectors = np.random.normal(size=(CONTROL_SIZE_SQRT**2, LATENT_DIM)) / 2

开始=0
d_损失= []
a_损失= []
图片已保存 = 0
对于范围内的步长（iters）：
    开始时间 = 时间.time()
    Latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))
    生成=生成器.预测（潜在向量）

    真实=图像[开始：开始+批量大小]
    组合图像 = np.concatenate([生成, 真实])

    标签 = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])
    标签 += .05 * np.random.random(labels.shape)

    d_loss = discriminator.train_on_batch(combined_images, labels)
    d_losses.append(d_loss)

    Latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))
    误导目标 = np.zeros((batch_size, 1))

    a_loss = gan.train_on_batch（潜在向量，误导目标）
    a_losses.append(a_loss)

    开始+=批量大小
    如果开始&gt; images.shape[0] - 批量大小：
        开始=0

    如果步骤 % 50 == 49：
        gan.save_weights(&#39;/gan.h5&#39;)

        print(&#39;%d/%d: d_loss: %.4f, a_loss: %.4f.(%.1f sec)&#39; % (step + 1, iters, d_loss, a_loss, time.time() - start_time))

        control_image = np.zeros((宽度 * CONTROL_SIZE_SQRT, 高度 * CONTROL_SIZE_SQRT, 通道))
        control_generate = 生成器.predict(control_vectors)

        对于范围内的 i（CONTROL_SIZE_SQRT ** 2）：
            x_off = i % CONTROL_SIZE_SQRT
            y_off = i // CONTROL_SIZE_SQRT
            control_image[x_off * 宽度:(x_off + 1) * 宽度, y_off * 高度:(y_off + 1) * 高度, :] = control_ generated[i, :, :, :]
        im = Img.fromarray(np.uint8(control_image * 255))#.save(StringIO(), &#39;jpeg&#39;)
        im.save(FILE_PATH % (RES_DIR, images_saved))
        图片已保存 += 1

什么时候执行完成？]]></description>
      <guid>https://stackoverflow.com/questions/78221744/what-does-this-code-is-supposed-to-do-it-keeps-executing-and-does-not-stop</guid>
      <pubDate>Mon, 25 Mar 2024 20:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何微调任何生成模型？自动列车</title>
      <link>https://stackoverflow.com/questions/78221298/how-can-i-fine-tune-the-any-generative-model-autotrain</link>
      <description><![CDATA[如何微调 Realistic_Vision_V6.0_B1_noVAE 模型并生成自己的图像？这是huggingface 链接。
我在稳定的扩散基础 xl 模型上使用了 autotrain。但在现实视觉模型上使用它似乎不太正确。模型生成不良图像。
&lt;块引用&gt;
自动训练 Dreambooth --型号 SG161222/Realistic_Vision_V6.0_B1_noVAE
--image-path input_images/ --prompt “摩诃人的照片” --分辨率 1024 --批量大小 1 --步数 500 --混合精度 fp16 --梯度累积 4 --lr 1e-4 --项目名称
现实愿景

这给了我 pytorch_lora_weights.safetensors 文件，我用它来生成我自己的图像。
pipeline.load_lora_weights（“模型/”，weight_name =“pytorch_lora_weights.safetensors”）
这种微调 Realistic_Vision_V6.0_B1_noVAE 模型的技术是否正确？或者任何像稳定扩散这样的模型？]]></description>
      <guid>https://stackoverflow.com/questions/78221298/how-can-i-fine-tune-the-any-generative-model-autotrain</guid>
      <pubDate>Mon, 25 Mar 2024 19:17:08 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的堆叠集成学习</title>
      <link>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</link>
      <description><![CDATA[我有两个 BERT 模型来实现代码中漏洞检测的多标签分类。一名接受过源代码培训，另一名接受过编译代码培训。他们实现的任务是多标签分类，因此两个模型的单个输出都是一个包含 6 个元素的数组，每个元素可以是 0 或 1，指示漏洞是否存在。
我想在这两个模型之上构建一个经典的 ML 分类器（如随机森林、SVM 或逻辑回归等），实现称为 Stacking 的集成技术。知道我正在处理多标签分类，我该如何实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</guid>
      <pubDate>Sun, 24 Mar 2024 13:28:23 GMT</pubDate>
    </item>
    <item>
      <title>我做了什么来纠正属性错误。请帮助我[关闭]</title>
      <link>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;batch_size = 100
对于范围 (25) 内的 i：
    num_batches = int(mnist.train.num_examples/batch_size)
    总成本 = 0
    对于范围内的 j（num_batches）：
        batch_x,batch_y = mnist.train.next_batch(batch_size)
        c, _ = sess.run([成本,优化], feed_dict={x:batch_x, y:batch_y, keep_prob:0.8})
        总成本 += c
    打印（总成本）

属性错误
                            回溯（最近一次调用最后一次）
单元格 In[65]，第 3 行
      1 批量大小 = 100
      2 对于范围 (25) 内的 i：
----&gt; 3 num_batches = int(mnist.train.num_examples/batch_size)
      4 总成本 = 0
      5 对于 j 在范围内（num_batches）：

AttributeError：模块“keras.datasets.mnist”没有属性“train”
]]></description>
      <guid>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</guid>
      <pubDate>Sat, 23 Mar 2024 07:25:50 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因素。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因素。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

确实需要得到整理，Stack坚持关闭我的帖子，写下gmail中navarrodan007的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 矩阵乘法形状错误：“RuntimeError：mat1 和 mat2 形状无法相乘”</title>
      <link>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</link>
      <description><![CDATA[我是 PyTorch 的新手，正在创建一个多输出线性回归模型，根据字母为单词着色。 （这将帮助有字素颜色联觉的人更轻松地阅读。）它接收单词并输出 RGB 值。每个单词都表示为 45 个浮点数 [0,1] 的向量，其中 (0, 1] 代表字母，0 代表该位置不存在字母。每个样本的输出应该是一个向量 [r-value, g -值，b-值]。
我懂了
&lt;块引用&gt;
运行时错误：mat1 和 mat2 形状无法相乘（90x1 和 45x3）

当我尝试在训练循环中运行我的模型时。
查看现有的 Stack Overflow 帖子，我认为这意味着我需要重塑我的数据，但我不知道如何/在哪里以解决此问题的方式进行此操作。特别是考虑到我不知道那个 90x1 矩阵来自哪里。
我的模型
我一开始很简单；在我可以让单个层发挥作用之后，可以出现多个层。
类 ColorPredictor(torch.nn.Module):
    #构造函数
    def __init__(自身):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #编码词向量的长度 &amp; r,g,b 向量的大小
        
    ＃ 预言
    defforward(self, x: torch.Tensor) -&gt;;火炬.张量：
        y_pred = self.线性(x)
        返回 y_pred

我如何加载数据
# 数据集类
数据类（数据集）：
    # 构造函数
    def __init__(自身，输入，输出)：
        self.x = input # 编码词向量列表
        self.y = 输出 # 将 r、g、b 值转换为火炬张量的 Pandas 数据帧
        self.len = len(输入)
    
    # 吸气剂
    def __getitem__(自身，索引)：
        返回 self.x[索引], self.y[索引]
    
    # 获取样本数
    def __len__(自身):
        返回 self.len

# 创建训练/测试分割
train_size = int(0.8 * len(数据))
train_data = 数据(输入[:train_size], 输出[:train_size])
test_data = 数据(输入[train_size:], 输出[train_size:])

# 为训练和测试集创建 DataLoaders
train_loader = DataLoader（数据集= train_data，batch_size = 2）
test_loader = DataLoader（数据集= test_data，batch_size = 2）

发生错误的测试循环
对于范围内的纪元（纪元）：
    ＃ 火车
    model.train() #训练模式
    对于 train_loader 中的 x,y：
        y_pred = model(x) #此处错误
        损失=标准(y_pred, y)
        优化器.zero_grad()
        loss.backward()
        优化器.step()
      

错误回溯


新尝试：
将 45x1 输入张量更改为 2x45 输入张量，第二列全为零。这适用于第一次运行 train_loader 循环，但在第二次运行 train_loader 循环期间，我得到另一个矩阵乘法错误，这次是大小为 90x2 和 45x3 的矩阵。]]></description>
      <guid>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</guid>
      <pubDate>Thu, 21 Mar 2024 01:00:23 GMT</pubDate>
    </item>
    <item>
      <title>从“y”的唯一值推断出的类无效。预期：[0 1 2 3 4 5]，得到[1 2 3 4 5 6]</title>
      <link>https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got</link>
      <description><![CDATA[我已经使用 XGB 分类器训练了数据集，但在本地出现了此错误。它在 Colab 上有效，而且我的朋友对相同的代码也没有任何问题。
我不知道这个错误意味着什么......
从 y 的唯一值推断出的类无效。预期：[0 1 2 3 4 5]，得到[1 2 3 4 5 6]
这是我的代码，但我想这不是原因。
start_time = time.time()
xgb = XGBClassifier（n_估计器 = 400，学习率 = 0.1，最大深度 = 3）
xgb.fit(X_train.values, y_train)
print(&#39;适合时间：&#39;, time.time() - start_time)
]]></description>
      <guid>https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got</guid>
      <pubDate>Mon, 25 Apr 2022 08:32:38 GMT</pubDate>
    </item>
    <item>
      <title>尽管验证准确度很高，为什么我的神经网络对属于某一类的测试图像预测出错误的类标签？</title>
      <link>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</guid>
      <pubDate>Tue, 12 Apr 2022 11:20:17 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 的 GridSearchCV 中评分</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我目前正在尝试使用 XGBoost 第一次分析数据。我想使用 GridsearchCV 找到最佳参数。我想最小化均方根误差，为此，我使用“rmse”作为 eval_metric。然而，网格搜索中的评分没有这样的指标。我在这个网站上发现“neg_mean_squared_error”的作用相同，但我发现这给出了与 RMSE 不同的结果。当我计算“neg_mean_squared_error”的绝对值的根时，我得到的值约为 8.9，而另一个函数给出的 RMSE 约为 4.4。
我不知道出了什么问题，也不知道如何让这两个函数达成一致/给出相同的值？
由于这个问题，我得到了错误的“best_params_”值，这给了我比我最初开始调整的一些值更高的 RMSE。
谁能解释一下如何在网格搜索中获得 RMSE 分数或者为什么我的代码给出不同的值？ 
提前致谢。
def modelfit（alg、trainx、trainy、useTrainCV=True、cv_folds=10、early_stopping_rounds=50）：
    如果使用TrainCV：
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(trainx, label=trainy)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;], nfold=cv_folds,
                          指标=&#39;rmse&#39;，early_stopping_rounds=early_stopping_rounds）
        alg.set_params(n_estimators=cvresult.shape[0])

    # 将算法拟合到数据上
    alg.fit(trainx, trainy, eval_metric=&#39;rmse&#39;)

    # 预测训练集：
    dtrain_predictions = alg.predict(trainx)
    # dtrain_predprob = alg.predict_proba(trainy)[:, 1]
    打印（dtrain_预测）
    打印（np.sqrt（mean_squared_error（trainy，dtrain_predictions）））

    # 打印模型报告：
    print(&quot;\n模型报告&quot;)
    print(&quot;RMSE : %.4g&quot; % np.sqrt(metrics.mean_squared_error(trainy, dtrain_predictions)))

 参数_test2 = {
 &#39;最大深度&#39;:[6,7,8],
 &#39;min_child_weight&#39;:[2,3,4]
}

grid2 = GridSearchCV(估计器 = xgb.XGBRegressor( 学习率 =0.1, n_estimators=2000, max_深度=5,
 min_child_weight=2，gamma=0，子样本=0.8，colsample_bytree=0.8，
 目标=&#39;reg：线性&#39;，nthread=4，scale_pos_weight=1，random_state=4），
 param_grid = param_test2，评分=&#39;neg_mean_squared_error&#39;，n_jobs=4，iid=False，cv=10，详细=20）
grid2.fit(X_train,y_train)
# best_estimator 的平均交叉验证分数
打印（grid2.best_params_，np.sqrt（np.abs（grid2.best_score_））），打印（np.sqrt（np.abs（grid2.score（X_train，y_train））））
modelfit(grid2.best_estimator_, X_train, y_train)
打印（np.sqrt（np.abs（grid2.score（X_train，y_train））））
]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    </channel>
</rss>