<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 16 May 2024 21:14:19 GMT</lastBuildDate>
    <item>
      <title>如何利用机器学习算法来提高各个领域的预测准确性和效率？</title>
      <link>https://stackoverflow.com/questions/78492345/how-can-machine-learning-algorithms-be-leveraged-to-improve-prediction-accuracy</link>
      <description><![CDATA[应用算法来改进预测
尝试过：制作食物热量预测模型
期待：一种可以提供更高准确性的算法，可以分析食物的适当比例以及其中存在的准确或精确的卡路里含量。]]></description>
      <guid>https://stackoverflow.com/questions/78492345/how-can-machine-learning-algorithms-be-leveraged-to-improve-prediction-accuracy</guid>
      <pubDate>Thu, 16 May 2024 20:33:26 GMT</pubDate>
    </item>
    <item>
      <title>基于剧情简介的多标签电影类型分类器</title>
      <link>https://stackoverflow.com/questions/78491754/multi-label-movie-genre-classifier-based-on-synopsis</link>
      <description><![CDATA[我必须创建一个机器学习模型，可以根据电影的概要将电影分类为类型。该数据集包含大量示例和标记数据。流派输出变量很复杂，因为有 18 种不同的流派，但所有这些流派都可以组合起来以获得电影的“官方”流派。所以实际上总共有 1000 种不同的流派。我尝试用多标签分类来解决它。然而，最后，我需要根据每部电影的概率对类型进行排名，这很奇怪，因为它是多标签的，对吧？
因此，性能也很差（12% 准确率），我需要达到 77%。我很好地处理了数据（删除了停用词、矢量化文本……），并且使用了逻辑回归和一对一分类器。您对如何改进这一点有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78491754/multi-label-movie-genre-classifier-based-on-synopsis</guid>
      <pubDate>Thu, 16 May 2024 18:10:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 BERT 对连续数据流进行分类</title>
      <link>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</link>
      <description><![CDATA[我正在研究构建分类系统，该系统基本上将通过各种管道实时摄取连续的数据流，例如 Twitter 帖子和这些帖子中的评论，以文章形式来自网站源的数据。
我希望系统将数据组织在主题和子主题中，例如：
主题——笔记本电脑
副主题——宏碁推出全新 XYZ 笔记本电脑
我无意使用生成式人工智能来编写子主题，我更期待从数据源中形成句子，例如，如果宏碁推出 XYZ 笔记本电脑，那么所有 Twitter 帖子和文章都会包含类似的内容“宏碁透露/推出了等等”。
我只需要一个从哪里开始的方向，因为我有点迷失如何实现它
我什至从来没有超越过构建数据流。更不用说设置模型来进行分类了]]></description>
      <guid>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</guid>
      <pubDate>Thu, 16 May 2024 17:44:49 GMT</pubDate>
    </item>
    <item>
      <title>具有 n 维卫星图像的多类 UNet</title>
      <link>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</link>
      <description><![CDATA[我正在尝试使用 Pytorch 中的 UNet 从多维（8 波段）卫星图像中提取预测掩模。我无法让预测蒙版看起来有些预期/连贯。我不确定问题是否在于我的训练数据的格式化方式、我的训练代码或我用于进行预测的代码。我怀疑这是我的训练数据输入模型的方式。我有 8 个波段卫星图像和单波段掩码，其值范围为 0-n 个类别，其中 0 是背景，1-n 是目标标签，如下所示：

在单通道示例的情况下，图像形状为 (8, 512, 512)，掩模形状为 (512, 512)，在 OHE 情况下为 (512, 512, 8)，而 (512, 512, 3) 在堆叠的情况下。
有些蒙版可能包含所有类标签，有些可能只有几个或仅是背景标签。我尝试过使用这些单通道掩码，我还将它们转换为 3 通道掩码，第一个通道是给定图像的所有标签，并且我还尝试了对它们进行热编码，以便每个掩码都是 0- n 个维度，每个通道都有一个不同的标签，其中背景/目标为二进制 0-1。
在每种情况下，无论我采用哪种方式格式化训练数据，训练结果最终要么是全黑、全白，要么是像这样的网格效果：

是否有一种理想的方式来格式化这些数据以进行训练/预测，或者我是否做错了什么导致了这些错误的预测掩码？
为了不发布数百行代码，以下是我用来尝试堆叠/OHE 掩码、训练和预测的一些通用片段：
蒙版操作：
将 numpy 导入为 np
从 PIL 导入图像
mask = np.array(Image.open(mask_path))
如果堆栈：
    Zeros = np.zeros((mask.shape[0], mask.shape[1]))
    掩码 = np.transpose(np.array([掩码, 零, 零]), (1, 2, 0)).astype(np.uint8)
如果一个热：
    one_hot_mask = np.zeros((mask.shape[0], mask.shape[1], self.num_classes))
    label_values = 列表(np.unique(mask))
    对于范围内的 i(0, self.num_classes):
        如果我不在 label_values 中：
            one_hot_mask[:, :, i] = 0
        别的：
            如果堆栈：
                one_hot_mask[:, :, i][掩码[:, :, 0] == i] = 1
            别的：
                one_hot_mask[:, :, i][掩码[:, :] == i] = 1

火车
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim

频段数 = 8
num_classes = 8 # 碰巧该数据集具有与输入频段/通道相同数量的类，但情况并非总是如此
纪元=5
学习率 = 0.001
权重衰减 = 0

模型 = UNet(n_channels=num_bands, n_classes=num_classes).to(device)
优化器= optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)
loss_fn = nn.CrossEntropyLoss() 如果 num_classes &gt; 1 否则 nn.BCEWithLogitsLoss()

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

对于范围内的纪元（纪元）：
    循环= tqdm（枚举（train_loader），总计= len（train_loader））
    对于batch_idx，（数据，目标）循环：
        数据 = data.float().to(设备)
        目标 = Targets.long().to(设备)
        预测=模型（数据）
        损失= loss_fn（预测，目标）
        优化器.zero_grad()
        循环.set_postfix(loss=loss.item())
    checkpoint_name = os.path.join(model_dir, f&quot;model_{epoch}.pt&quot;)
    torch.save(model.state_dict(), checkpoint_name)

预测
将 numpy 导入为 np
从 skimage 导入 io
图像 = np.array(io.imread(image_path))

张量 = ToTensor()(图像)
batch_t = torch.unsqueeze(张量, 0).to(设备)
preds = 模型(batch_t)
softmax = torch.nn.Softmax(dim=1)
preds = torch.argmax(softmax(model(preds)),axis=1).cpu()
preds = np.array(preds[0,:,:])
plt.imshow(preds, cmap=&#39;tab20&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</guid>
      <pubDate>Thu, 16 May 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>TypeError“NoneType”对象不可下标</title>
      <link>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable</guid>
      <pubDate>Thu, 16 May 2024 16:44:52 GMT</pubDate>
    </item>
    <item>
      <title>有 0 张图像属于 0 个类别，我被卡住了</title>
      <link>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</link>
      <description><![CDATA[我尝试使用 google Colab 让程序运行我的 google 驱动器中的图像，但它在 model.fit(train_generator, epochs=10,validation_data=test_generator) 处不断出现错误
我尝试将批次大小调整为 10，并将所有图像大小调整为 100，我认为它会起作用，但它仍然给了我
找到属于 0 个类别的 0 个图像。
找到属于 0 个类别的 0 张图片。

我将训练和测试图像放入两个单独的文件夹中，分别名为“training_data”和“testing_data”。
!pip 安装tensorflow
导入CV2
从tensorflow.keras.preprocessing.image导入img_to_array
将 numpy 导入为 np
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.applications导入EfficientNetB0
从tensorflow.keras.layers导入密集，GlobalAveragePooling2D
从tensorflow.keras.models导入模型
从sklearn.metrics导入confusion_matrix、accuracy_score、 precision_score、recall_score

#上传图片
def load_image(图像位置):
    # 使用OpenCV加载图像
    图像 = cv2.imread(图像位置)

   #检查图片是否上传
    如果图像为无：
        print(“错误：找不到图像”，image_location)
        返回无

    resized_image= cv2.resize(图像, (100, 100))
    image_array = img_to_array(调整大小的图像)

    图像数组 /= 255.0

    返回图像数组


example_image_location = &#39;/content/drive/MyDrive/机器学习文件夹/training_data/LightGreen_Crayon_Testing.jpg&#39;

# 加载并预处理示例图像
示例图像 = 加载图像（示例图像位置）


如果 example_image 不是 None：

    # 收集/处理数据
  train_dir = &#39;/content/drive/MyDrive/机器学习文件夹/training_data&#39;
  test_dir = &#39;/content/drive/MyDrive/机器学习文件夹/testing_data&#39;

  train_datagen = ImageDataGenerator（重新缩放=1。/ 255）
  test_datagen = ImageDataGenerator（重新缩放=1。/ 255）

  train_generator = train_datagen.flow_from_directory(train_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

  test_generator = test_datagen.flow_from_directory(test_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

    ＃ 建筑模型
  base_model = EfficientNetB0(weights=&#39;imagenet&#39;, include_top=False)
  x = 基础模型.输出
  x = GlobalAveragePooling2D()(x)
  x = 密集（1024，激活=&#39;relu&#39;）（x）
  预测=密集（1，激活=&#39;sigmoid&#39;）（x）
  模型 = 模型（输入=base_model.输入，输出=预测）

  对于 base_model.layers 中的图层：
    可训练层 = False
    model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    model.fit（train_generator，epochs = 10，validation_data = test_generator）
]]></description>
      <guid>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</guid>
      <pubDate>Thu, 16 May 2024 15:27:25 GMT</pubDate>
    </item>
    <item>
      <title>我如何使变压器输出相对于特定上下文的翻译</title>
      <link>https://stackoverflow.com/questions/78490608/how-can-i-make-a-transformer-output-a-translation-relative-to-a-specific-context</link>
      <description><![CDATA[我正在做一个类似机器翻译的项目，其中我有一个具有编码器-解码器结构的转换器，它应该从自然语言命令生成 SQL 查询，例如：
输入：计算 XYZ 公司生产的飞机数量
输出：从飞机中选择 COUNT(*) 个，其中制造商=&#39;XYZ&#39;；
现在我已经实现了我的目标，我的模型在大多数情况下都可以生成看起来不错的查询，但仍然有一个问题需要解决，那就是它可以生成的查询在语法上都是正确的，但它并没有解决问题属性/表名称，例如：
输入：计算 XYZ 公司生产的飞机数量
输出： SELECT COUNT(*) FROM 飞机，其中生产=&#39;XYZ&#39;;
或者：
输入：显示所有 40 岁以上的员工
输出： SELECT * FROM员工，其中国家/地区&gt; 40；
就像我说的，从语法上讲，查询没有错误，但它肯定无法在数据库本身上正确执行。
我的模型是在 json 数据集上进行训练的，该数据集包含遵循以下形式的样本列表：
&lt;前&gt;&lt;代码&gt;[
  [
    “计算 XYZ 公司生产的飞机数量”，
    “从制造商 = &#39;XYZ&#39; 的飞机中选择 COUNT(*)；”
  ],
  [
    “大西洋发现了多少海洋物种？”,
    “SELECT COUNT(*) FROM Marine_species WHERE location = &#39;Atlantic Ocean&#39;;”
  ]
]

尽管我发现一些数据集包含数据库架构作为一组 CREATE 查询，如下所示：
{“指令”：“CREATE TABLE table_72445 (
    “县”文本，
    “人口”真实的，
    “人均收入”文本，
    “家庭收入中位数”文本，
    “家庭收入中位数”文本
）
-- 列出河滨家庭收入中位数”，
“输出”：“选择\“家庭收入中位数” FROM table_72445 WHERE \“县\” =“河滨”}

那么我是否可以利用这种数据集，以便为我的转换器提供它应该工作的数据库上下文，或者是否有其他方法来实现此任务目标？
注意：我无法将之前的语料库作为一个整体作为我的输入，因为这会导致巨大的性能开销。想象一下，每次我想要执行时，都会定义包含数十个表的相同集合，因此这是别无选择。]]></description>
      <guid>https://stackoverflow.com/questions/78490608/how-can-i-make-a-transformer-output-a-translation-relative-to-a-specific-context</guid>
      <pubDate>Thu, 16 May 2024 14:25:29 GMT</pubDate>
    </item>
    <item>
      <title>具有数值列和数组列的机器学习输入，处理机器学习中的混合类型数据</title>
      <link>https://stackoverflow.com/questions/78490240/machine-learning-input-with-numerical-columns-and-array-columns-handling-mixed</link>
      <description><![CDATA[我正在开发一个机器学习项目，其中有一个数据集，其中包含数字列和包含数组的列的组合。数字列（例如平均值）包含单个值，而带有数组的列（例如梯度）每行可以包含可变数量的元素。
处理此类输入的最佳实践是什么？我可以在机器学习模型中同时使用数字列和带有数组的列吗？如果是这样，在模型的预处理和训练阶段处理这种数据异构性的最常见策略是什么？
如果有任何建议或资源可以帮助我更好地了解如何应对这一挑战，我将不胜感激。
示例：
模型输入：

&lt;标题&gt;

平均值
渐变


&lt;正文&gt;

0.5
[1,2,3,45,0.2]


1
[2,5,1.2,5,0]


]]></description>
      <guid>https://stackoverflow.com/questions/78490240/machine-learning-input-with-numerical-columns-and-array-columns-handling-mixed</guid>
      <pubDate>Thu, 16 May 2024 13:28:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 NER 和 XLM-RoBERTa 进行信息提取</title>
      <link>https://stackoverflow.com/questions/78488394/information-extraction-using-ner-with-xlm-roberta</link>
      <description><![CDATA[我想询问有关从 PDF 文档中提取信息的问题。
我将使用 XLM-RoBERTa 和 NER 来执行信息提取，并且我将使用来自以下来源的代码进行微调：https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a。从这个来源来看，它确实使用了 BERT，并且在我尝试之后，结果发现来自单词的预测实体与真实值不同。我很困惑是否需要从代码中更改某些内容，或者数据集是否有问题，因为我使用的数据集是自制的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78488394/information-extraction-using-ner-with-xlm-roberta</guid>
      <pubDate>Thu, 16 May 2024 08:03:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 DeepSeekMath 7b 模型在 vllm 中进行无说明的断言</title>
      <link>https://stackoverflow.com/questions/78487360/assertion-with-no-scription-in-vllm-with-deepseekmath-7b-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78487360/assertion-with-no-scription-in-vllm-with-deepseekmath-7b-model</guid>
      <pubDate>Thu, 16 May 2024 02:49:09 GMT</pubDate>
    </item>
    <item>
      <title>无法使用自定义算法运行训练</title>
      <link>https://stackoverflow.com/questions/78486992/unable-to-run-training-using-a-custom-algorithm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78486992/unable-to-run-training-using-a-custom-algorithm</guid>
      <pubDate>Wed, 15 May 2024 23:54:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么 JAX 编译时间会随着 vmap 批次大小而增加？</title>
      <link>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</link>
      <description><![CDATA[我正在使用 JAX 来评估批量损失梯度，其中涉及一些复杂的线性代数（包括 Cholesky 分解和解决方案等）。我的梯度损失的示意图形式是
jax.jit( jax.value_and_grad( jax.vmap(loss)(...).mean() ) )

我发现编译/首次评估时间在给定 vmap 的特定批量大小之前是恒定的（正如我通常所期望的那样），然后开始超线性增长。在 A100 上，nbatch &lt;= 64 需要 6 分钟，nbatch=128 需要 13 分钟，nbatch=256 需要 1 小时，这变得很笨拙。
这是一个可重现的示例。经过大量二分之后，我想我看到了简单的批量 cholesky 的相同行为：
# 这里没什么可看的，只是设置我们将要解决的线性系统
诺布斯, ngp = 256, 64
t = np.linspace(0, 1, 诺布)
f = np.arange(1, ngp + 1, dtype=np.float64)

fmat = np.zeros((nobs, 2*ngp), dtype=np.float64)
fmat[:, ::2] = np.sin(2.0 * jnp.pi * f * t[:,np.newaxis])
fmat[:, 1::2] = np.cos(2.0 * jnp.pi * f * t[:,np.newaxis])

一、ones = jax.numpy.identity(nobs, dtype=np.float64), jax.numpy.ones(nobs, dtype=np.float64)

def 函数（pars）：
  ftf = fmat @ jax.numpy.diag(pars**2) @ fmat.T + 1
  cf = jax.scipy.linalg.cho_factor(ftf)
  b = jax.scipy.linalg.cho_solve(cf, 个)
  返回 b.mean()

然后如果我获得转换并编译它
jvg = jax.value_and_grad(lambda pars: jax.vmap(func)(pars).mean())
pars = jax.random.normal(jax.random.PRNGKey(0), (nbatch,2*ngp,))
jjvg = jax.jit(jvg).lower(pars).compile()

我发现编译时间（以秒为单位）随着 nbatch 的增加而增长：
&lt;预&gt;&lt;代码&gt;[16,0.532]、[32,0.507]、[64,0.516]、[128,0.580]、[256,0.652]、[512,0.822]、[1024,1.7]、[2048 ,2.75]

如果我尝试 jax.make_jaxpr 而不是 jax.jit，我会看到每个批量大小的相同代码。
这里可能发生了什么？我使用的是 JAX 0.4.26 和带有 CUDA 12.2 和驱动程序 535.104.05 的 V100。]]></description>
      <guid>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</guid>
      <pubDate>Wed, 15 May 2024 19:17:27 GMT</pubDate>
    </item>
    <item>
      <title>在深度训练/验证循环期间使用分层 k 折叠时出现越界错误</title>
      <link>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</guid>
      <pubDate>Mon, 13 May 2024 14:43:39 GMT</pubDate>
    </item>
    <item>
      <title>需要使用 Rand_forest 和 h2o 进行预测的指导</title>
      <link>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</link>
      <description><![CDATA[我有一个随机森林模型，我正在尝试更好地理解它。
为了举例，假设我们有一片蓝莓灌木丛。我们感兴趣的是预测单个灌木丛中所有蓝莓的收获中特定灌木丛中腐烂蓝莓的产量。
每个灌木都有一个识别名称：bush_name，例如&#39;bush001&#39;，我们希望根据每个单独的灌木进行预测。例如，我想知道 Bush025 是否在 2/2/22 生产了腐烂的浆果。
为了本示例，输入位于具有以下虚拟结构的 df 中：
train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05&quot; -01”、“2022-11-01”、“2022-11-02”)、
                   Bush_name = c(“bush001”、“bush001”、“bush001”、“bush043”、“bush043”、“bush043”),
                   错误 = c(2, 0, 1, 0, 3, 1),
                   有腐烂的浆果 = c(1, 0, 0, 1, 1, 0),
                   浆果计数 = c(12, 1, 7, 100, 14, 4),
                   天气 = c(1, 0, 2, 0, 1, 1))

我已经建立了一个随机森林模型，并进行了以下高级设置：
库(agua)
图书馆（防风草）
图书馆（水）

h2o.init(n线程 = -1)

model_fit &lt;- rand_forest(mtry = 10, trees = 100) %&gt;%
  set_engine(“h2o”) %&gt;%
  set_mode(“分类”) %&gt;%
  适合（has_rotten_berry ~ .,
      数据 = train_data) %&gt;%
  step_dummy(灌木名称) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_normalize(all_predictors())

训练后我确实收到了这条消息：
警告消息：
在 .h2o.processResponseWarnings(res) 中：
  删除坏列和常量列：[bush_name]。

我想知道的是：
当我尝试预测训练模型中的新数据时，似乎我只能使用我已经训练过的灌木丛的 Bush_names 输入新的测试数据。 我假设该模型正在创建特定于灌木丛的预测是否正确？因此必须在训练中输入新的灌木丛信息才能输出这些新灌木丛的未来预测？
示例：我种植了一棵新灌木，bush700，它不存在于原始训练数据集中。如果我尝试使用新的灌木丛数据进行预测，但训练数据中不存在该数据，则会向我传达一条消息：数据中存在新的级别。所以我假设因为这些预测似乎是特定于灌木丛的，并且我们无法为新添加的灌木丛获得任何新的灌木丛预测。
这个假设正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</guid>
      <pubDate>Tue, 07 May 2024 16:58:00 GMT</pubDate>
    </item>
    <item>
      <title>错误消息 R - PrettyNum(.Internal(format(x,rim,digits,nsmall,width,3L,：‘digits’参数的值 0 无效”</title>
      <link>https://stackoverflow.com/questions/75961903/error-message-r-prettynum-internalformatx-trim-digits-nsmall-width-3l</link>
      <description><![CDATA[我想知道是否有人可以帮助我识别和解决使用 LCAvarsel 时遇到的错误术语。
就上下文而言，该库旨在用于潜在类分析中的变量选择。
install.packages(“LCAvarsel”)
install.packages(“poLCA”)
库（LCAvarsel）
图书馆（poLCA）

数据（癌）
sel1 &lt;- LCAvarsel（癌）

返回以下错误：
prettyNum(.Internal(format(x,rim,digits,nsmall,width,3L,:
“数字”参数的值 0 无效
https://www.rdocumentation.org/packages/LCAvarsel /versions/1.1/topics/LCAvarsel]]></description>
      <guid>https://stackoverflow.com/questions/75961903/error-message-r-prettynum-internalformatx-trim-digits-nsmall-width-3l</guid>
      <pubDate>Fri, 07 Apr 2023 20:59:28 GMT</pubDate>
    </item>
    </channel>
</rss>