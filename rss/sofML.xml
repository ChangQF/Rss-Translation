<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Jun 2024 09:17:14 GMT</lastBuildDate>
    <item>
      <title>模型校准</title>
      <link>https://stackoverflow.com/questions/78574055/model-calibration</link>
      <description><![CDATA[我正在研究概率校准，如果我有一个高度不平衡的数据集（例如来自 Kaggle 的欺诈检测数据集），我对如何正确评估校准有一些疑问。
我知道有很多校准技术，例如 Platt Scaling、Isotonic Regression、Beta Calibration、SplineCalib 和 Venn-ABERS。此外，还有多种方法可以衡量校准指标（例如 ECE、Brier Score、对数损失和校准曲线）的效果。
此外，我想在增强算法上测试它：XGBoost、LGBM
那么，

对于不平衡的数据集，我应该使用哪种校准模型？
哪种指标更适合这样的任务？
您能否提供一些资源（书籍、YouTube 视频、文章）以了解有关不同方法的更多信息以及何时应该使用它们？

我在 XGBoost 模型上尝试了 Platt Scaling 和 Isotonic Regression，并比较了校准前后的 ECE、Brier Score 和对数损失指标，它们的值更差。]]></description>
      <guid>https://stackoverflow.com/questions/78574055/model-calibration</guid>
      <pubDate>Tue, 04 Jun 2024 07:56:41 GMT</pubDate>
    </item>
    <item>
      <title>如何改进 CNN 二元分类模型</title>
      <link>https://stackoverflow.com/questions/78573842/how-to-improve-cnn-binary-classification-model</link>
      <description><![CDATA[我是 AI 和 ML 的新手，正在尝试开发一个模型来对一组脑肿瘤的 MRI 扫描图像进行分类。它有两个类别（“健康”和“肿瘤”）。我的数据集有 4600 张图像（2087 张为健康，2513 张为肿瘤）。我使用了 15 个 epoch，准确度在逐渐提高。但验证损失和验证准确度却时高时低。我觉得这是因为模型过度拟合。有什么建议可以提高验证准确度？我必须在模型中添加更多或更少的层吗？按升序添加 Conv2D 的过滤器数量是否正确？谢谢。
这是我如何分割训练和验证数据。
training_data, validation_data = tf.keras.utils.image_dataset_from_directory(data_dir,
batch_size=32,
image_size=(128, 128),
validation_split=.2,
subset=&#39;both&#39;,
seed=42)

输出：
找到属于 2 个类别的 4514 个文件。
使用 3612 个文件进行训练。
使用 902 个文件进行验证。
图像如下所示

在这里我重新缩放数据
training_data = training_data.map(lambda x,y: (x/255,y))
validation_data = validation_data.map(lambda x,y: (x/255,y))

我按如下方式进行了数据增强
data_augmentation = tf.keras.Sequential(
[
tf.keras.layers.RandomFlip(&quot;horizo​​ntal&quot;),
tf.keras.layers.RandomRotation(0.2),
tf.keras.layers.RandomZoom(0.2),
])

这是我的模型
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(shape=(128, 128, 3)))

model.add(data_augmentation)

model.add(tf.keras.layers.Conv2D(32, kernel_size=3,activation=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Conv2D(64, kernel_size=3,激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Conv2D(128, kernel_size=3, 激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Flatten()),

model.add(tf.keras.layers.Dense(128,激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.Dropout(0.3)),

model.add(tf.keras.layers.Dense(32, 激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.Dropout(0.3)),

model.add(tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;))
]]></description>
      <guid>https://stackoverflow.com/questions/78573842/how-to-improve-cnn-binary-classification-model</guid>
      <pubDate>Tue, 04 Jun 2024 07:08:06 GMT</pubDate>
    </item>
    <item>
      <title>人类与机器意识 [关闭]</title>
      <link>https://stackoverflow.com/questions/78572931/human-versus-machine-consciousness</link>
      <description><![CDATA[在评估人工智能系统时，我试图确定不同的系统在使用算法来确认处理人类提示时对真理的认识，而不是它们自己的内部迭代处理周期时，是否会区分不同的意识形式。人工智能系统是否区分人类意识和形而上学意识？
这是我第一次尝试理解人类意识与机器意识。]]></description>
      <guid>https://stackoverflow.com/questions/78572931/human-versus-machine-consciousness</guid>
      <pubDate>Tue, 04 Jun 2024 00:27:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林计算预测的置信区间？</title>
      <link>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</link>
      <description><![CDATA[我正在计算名为 &quot;spot&quot; 的变量的预测（数据的未来结果）。我正在使用随机森林和名为 &quot;DTCI&quot; 的独立变量来协助预测 &quot;spot&quot;。预测以每月频率进行，与数据频率相同。我想根据每个月的上限和下限获得每个预测月份的置信区间。它与附图中所做的类似，带有绿色限制。

我尝试使用 GradientBoostingRegressor 构建间隔，如下所示：
# 设置下限和上限分位数
inf = 0.1
sup = 0.9

# 每个模型必须独立
lower_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=inf)
upper_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=sup)

lower_model.fit(X_train, y_train)
upper_model.fit(X_train, y_train)

predictions = pd.DataFrame(y_hat_forecast_spot)

predictions[&quot;inf&quot;] = lower_model.predict(X_fore)
predictions[&quot;sup&quot;] = upper_model.predict(X_fore)

然而，结果并没有我预期的趋势。由于它是一个时间序列，我想象（如上图所示）限值应该以置信区域变大的方式增长。换句话说，日期越远，预测就越困难，因此与之相关的误差或间隔就越大。
我使用 GradientBoostingRegressor 得到的结果（如下所示）的间隔会随时间变化而不是增长。

GradientBoostingRegressor 适合时间序列吗？或者有其他函数可以更好地理解时间序列吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</guid>
      <pubDate>Mon, 03 Jun 2024 21:17:18 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中回归任务的批量归一化</title>
      <link>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</link>
      <description><![CDATA[我有一个想要适合回归的神经网络。当我在层之间不使用批量归一化时，它表现良好，但是当我添加批量归一化时，它表现非常糟糕。
例如，没有批量归一化的 MAE 是 17000，而使用批量归一化的 MAE 则高达 180000。这是为什么？
X = train_data.drop([&#39;SalePrice&#39;], axis=1)
y = train_data[&#39;SalePrice&#39;]

# 获取每种类型的列
numeric_cols = [col for col in X.columns if X[col].dtype in [&#39;float64&#39;, &#39;int64&#39;]]
categoric_cols = [col for col in X.columns if X[col].dtype == &#39;object&#39;]

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)

# 转换管道
numeric_transformer = Pipeline(steps=[
(&#39;impute_numeric&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scale&#39;, StandardScaler())
])

categoric_transformer = Pipeline(steps=[
(&#39;impute_categoric&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encode&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)),
])

preprocessing = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, numeric_cols),
(&#39;cat&#39;, categoric_transformer, categoric_cols)
])

# 拟合变换器
transformed_X_train = preprocessing.fit_transform(X_train)
transformed_X_val = preprocessing.transform(X_val)

# 模型 14392
input_shape = perceived_X_train.shape[1]
model = keras.Sequential([
layer.Dense(units=256,activation=&#39;swish&#39;,input_shape=[input_shape]),
layers.BatchNormalization(),
layers.Dense(units=256,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=1),
])

# 设置优化器和损失函数
model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;mae&#39;
)

early_stopping = callups.EarlyStopping(
min_delta=0.01, # 计为改进的最小变化量
patient=20, # 需要失败的时期数 min_delta 才能提前停止
restore_best_weights=True,
)

# 拟合神经网络
history = model.fit(
transformed_X_train, y_train,
epochs=200,
batch_size=128,
callbacks=[early_stopping],
validation_data=(transformed_X_val, y_val)
#validation_split=0.2,# 自动创建测试和验证集
)

history_df = pd.DataFrame(history.history)
history_df.loc[0:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot()

# 最佳 13962.4795
print(&quot;Evaluation模型的”，model.evaluate(transformed_X_val, y_val, batch_size=128))
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</guid>
      <pubDate>Mon, 03 Jun 2024 20:03:21 GMT</pubDate>
    </item>
    <item>
      <title>加载预先训练的 json 模型时出错</title>
      <link>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</link>
      <description><![CDATA[这是我收到的错误：
回溯（最近一次调用）：
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\anti.py”，第 14 行，位于 &lt;module&gt;
model = tf.keras.models.model_from_json(loaded_model_json)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\models\model.py&quot;，第 575 行，位于 model_from_json
return serialization_lib.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 694 行，位于 deserialize_keras_object
cls = _retrieve_class_or_fn(
^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 812 行，位于 _retrieve_class_or_fn
raise TypeError(
TypeError：无法找到类“Functional”。确保自定义类已用修饰`@keras.saving.register_keras_serializable()`。完整对象配置：{&#39;class_name&#39;: &#39;Functional&#39;, &#39;config&#39;:.....(**json 文件的内容**........&#39;keras_version&#39;: &#39;2.15.0&#39;, &#39;backend&#39;: &#39;tensorflow&#39;)

以下是库版本：
keras 3.3.3
opencv-python 4.9.0.80
tensorflow 2.16.1
python 3.12.3

以下是我的代码：
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array 
import os
import numpy as np

root_dir = os.getcwd()
# 加载人脸检测模型
trained_face_data = cv2.CascadeClassifier(cv2.data.haarcascades + &#39;haarcascade_frontalface_default.xml&#39;)
# 加载反欺骗模型图
json_file = open(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/Antispoofing_model_mobilenet.json&#39;,&#39;r&#39;)
loaded_model_json = json_file.read()
json_file.close()
model = tf.keras.models.model_from_json(loaded_model_json)
# 加载反欺骗模型权重
model.load_weights(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/project_antispoofing_model_97-0.957895.h5&#39;)
print(&quot;模型从中加载disk&quot;)

video = cv2.VideoCapture(0)
while True:
try:
ret,frame = video.read()
gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = training_face_data.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces: 
face = frame[y-5:y+h+5,x-5:x+w+5]
resized_face = cv2.resize(face,(160,160))
resized_face = resized_face.astype(&quot;float&quot;) / 255.0
resized_face = np.expand_dims(resized_face, axis=0)
# 将人脸 ROI 传递给训练过的活体检测器
# 模型以确定人脸是“真”还是“假”
preds = model.predict(resized_face)[0]
print(preds)
if preds&gt; 0.5:
标签 = &#39;poof&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 0, 255), 2)
else:
标签 = &#39;eal&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 255, 0), 2)
cv2.imshow(&#39;frame&#39;, frame)
key = cv2.waitKey(1)
if key == ord(&#39;q&#39;):
break
except Exception as e: 
pass
video.release() 
cv2.destroyAllWindows()

我该怎么办？我尝试降级库，但也遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</guid>
      <pubDate>Mon, 03 Jun 2024 12:21:33 GMT</pubDate>
    </item>
    <item>
      <title>梯度累积损失计算</title>
      <link>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</link>
      <description><![CDATA[假设我们有数据 [b,s,dim]，我最近注意到 CrossEntropyLoss 是 (1) 计算一批中所有 token (b * s) 的平均值，而不是 (2) 计算每个句子然后计算平均值。
以下是计算 hugging_face 转换器 BertLMHeadMode 损失的代码
sequence_output = output[0]
prediction_scores = self.cls(sequence_output)

lm_loss = None
if labels is not None:
# 我们正在进行下一个 token 预测；将预测分数和输入 ID 移位一格
shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()
labels = labels[:, 1:].contiguous()
loss_fct = CrossEntropyLoss()
lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

我知道 (1) 和 (2) 在这种情况下没有区别。但是当我们应用梯度累积时，我认为情况就不同了。
假设我的batch_size为4，4个句子的长度分别为100,200,300,400。
使用batch_size 4时，损失是根据总共1000个token的平均值计算的。
但是当batch_size = 1且梯度累积= 4时，我认为损失是不同的。我们首先分别计算每个句子的损失，然后计算平均值，这意味着对于100个token的句子，我们计算100个token的损失平均值，然后除以4并将其添加到总损失中，对于其他3个句子也是如此，我认为以这种方式计算的损失与使用batch_size = 4计算的损失不同。
我误解了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</guid>
      <pubDate>Mon, 03 Jun 2024 11:19:53 GMT</pubDate>
    </item>
    <item>
      <title>用于聊天中问答识别的印地语 NLP 模型</title>
      <link>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</link>
      <description><![CDATA[我正在开展一个自然语言处理项目，涉及分析印地语聊天数据。具体来说，我需要在聊天记录中识别问题及其对应的答案。
是否有任何预先训练过的印地语 NLP 模型或库可以有效地处理此任务？理想情况下，我正在寻找一种解决方案，可以：
检测句子并将其分类为问题或答案。
识别上下文并将问题与各自的答案配对。
我尝试了各种模型，但没有一个合适。]]></description>
      <guid>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</guid>
      <pubDate>Mon, 03 Jun 2024 06:56:50 GMT</pubDate>
    </item>
    <item>
      <title>矩阵的线性回归</title>
      <link>https://stackoverflow.com/questions/78568690/linear-reggresion-of-matrix</link>
      <description><![CDATA[令 ( \mathbf{X} \in \mathbb{R}^{n \times d} ) 为回归量矩阵，并令 ( \mathbf{Y} \in \mathbb{R}^n ) 为响应向量。考虑具有参数向量 ( \mathbf{b} \in \mathbb{R}^d ) 的线性回归模型。求最小二乘 (LS) 函数相对于参数向量 ( \mathbf{b} ) 的梯度和 Hessian 矩阵。
线性回归模型：
[
\mathbf{Y} = \mathbf{Xb} + \mathbf{\epsilon}
]
其中 ( \mathbf{Y} \in \mathbb{R}^n ) 是响应向量， ( \mathbf{X} \in \mathbb{R}^{n \times d} ) 是回归量矩阵，( \mathbf{b} \in \mathbb{R}^d ) 是参数向量，( \mathbf{\epsilon} ) 是误差向量。
我被困住了。也许
相对于 \( \mathbf{b}\) 的梯度为： \[ \mathbf{b}} J(\mathbf{b}) = \mathbf{b}} \left ( \frac{1} {2}(\mathbf{Y} - \mathbf{Xb})^\top(\mathbf{Y} - \mathbf{Xb})\right) \] 简化 \( J (\mathbf{b}) 的表达式) \): \[ J(\mathbf{b}) = \frac{1}{2} (\mathbf{Y}^\top \mathbf{Y} - 2 \mathbf{Y} ^\top \mathbf{ Xb} + \mathbf{b}^\top \mathbf{X}^\top \mathbf{Xb}) \] 则梯度为： \[ \nabla_{\mathbf{b}} J (\mathbf{b} ) = -\mathbf{X}^\top \mathbf{Y} + \mathbf{X}^\top \mathbf{Xb}\]]]></description>
      <guid>https://stackoverflow.com/questions/78568690/linear-reggresion-of-matrix</guid>
      <pubDate>Mon, 03 Jun 2024 06:25:46 GMT</pubDate>
    </item>
    <item>
      <title>我的训练被随机终止，没有错误日志</title>
      <link>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</link>
      <description><![CDATA[我一直在尝试在集群计算机上训练 trackformer 模型。它显示了一条 Killed 消息，并且没有任何日志。

将权重传递到模型中时发生错误
尝试 dmesg 时，我得到了以下输出
[Mon Jun 3 07:24:26 2024] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-1,oom_memcg=/system.slice/slurmstepd.scope/job_19894856,task_memcg=/system
.slice/slurmstepd.scope/job_19894856/step_batch/user/task_0,task=python,pid=702977,uid=1380211
[2024 年 6 月 3 日星期一 07:24:26] 内存 cgroup 内存不足：已终止进程 702977 (python) total-vm:16068556kB, anon-rss:2144556kB, file-rss:123156kB, shmem-rss:28256kB, UID:1380211 pgtab
les:10612kB oom_score_adj:0

我试图在自定义数据集上训练 trackformer 模型。但训练过程随机停止。]]></description>
      <guid>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</guid>
      <pubDate>Mon, 03 Jun 2024 05:38:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在可变形 DETR 中可视化注意力以及基于 DETR 的一系列后续工作？</title>
      <link>https://stackoverflow.com/questions/78568416/how-to-visualize-attentions-in-deformable-detr-and-a-series-of-follow-up-works-i</link>
      <description><![CDATA[许多论文，即使代码是开源的，也没有提供生成这些可视化的具体代码。是否有任何脚本或参考资料可用于指导这些可视化的重现？
具体来说，Conditional-DETR 中的框定位可视化或其他一些作品中的 Cross Attention 可视化。很多时候，即使我能理解作者想要通过这些图传达的意思，但理解用于可视化的实际视觉特征和后处理方法仍然完全难以捉摸。]]></description>
      <guid>https://stackoverflow.com/questions/78568416/how-to-visualize-attentions-in-deformable-detr-and-a-series-of-follow-up-works-i</guid>
      <pubDate>Mon, 03 Jun 2024 04:41:57 GMT</pubDate>
    </item>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我了解，predict_proba(xtest) 将给我以下结果：
投票给分类器的树数/树数

我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1： 
--- 5, 0 类 A（已选择） 
10 
--- 2, 3 类 B（未选择） 

树 2： 
--- 3, 2 类 A（已选择） 
10 
--- 5, 0 类 B（未选择）

predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0

期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8

有人知道如何做到这一点，或者他们正在使用什么实现？
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    <item>
      <title>Synthcity DECAF 生成人工智能中的形状误差</title>
      <link>https://stackoverflow.com/questions/78548544/synthcity-decaf-shape-error-in-generative-artificial-intelligence</link>
      <description><![CDATA[我正在尝试使用 DECAF 生成器生成新数据，但出现无法解决的错误。
我使用的代码与主 repo 文档中提到的代码完全相同（链接 Synthcity Docs）：
from sklearn.datasets import load_iris
from synthcity.plugins import Plugins

X, y = load_iris(as_frame = True, return_X_y = True)
X[&quot;target&quot;] = y

plugin = Plugins().get(&quot;decaf&quot;, n_iter = 100)
plugin.fit(X)

plugin.generate(50)

我不断得到
ValueError：传递值的形状为 (150, 1)，索引暗示 (150, 3)

无论我做什么。我有点惊讶错误竟然会发生，因为它实际上是作者的一个案例研究。
有人能解释或更重要的是解决这个错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78548544/synthcity-decaf-shape-error-in-generative-artificial-intelligence</guid>
      <pubDate>Wed, 29 May 2024 09:25:58 GMT</pubDate>
    </item>
    <item>
      <title>加载 json 模型时 Python tensorflow keras 出错：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</guid>
      <pubDate>Sat, 16 Mar 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>Facebook Prophet 错误：cmdstanpy - 错误 - 链 [1] 错误：由信号 11 终止 未知错误 -11</title>
      <link>https://stackoverflow.com/questions/75413943/facebook-prophet-error-cmdstanpy-error-chain-1-error-terminated-by-signa</link>
      <description><![CDATA[我正在尝试在时间序列数据上训练 Facebook 的预言机。
我使用的数据，一列名为 ds，另一列名为 y
但是在拟合过程中，我首先收到此错误：
第一个错误屏幕截图
接着
第二个错误屏幕截图
有人知道为什么会这样吗？谢谢！
我尝试在线搜索问题，但找不到解决方案。我尝试安装 cmdstanpy 版本 0.9.5 并重新安装 prophecy，但没有成功，所以我有点卡住了。我正在运行 python 3.7.12 版本。]]></description>
      <guid>https://stackoverflow.com/questions/75413943/facebook-prophet-error-cmdstanpy-error-chain-1-error-terminated-by-signa</guid>
      <pubDate>Fri, 10 Feb 2023 16:48:37 GMT</pubDate>
    </item>
    </channel>
</rss>