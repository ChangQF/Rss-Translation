<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 04 Jul 2024 01:05:20 GMT</lastBuildDate>
    <item>
      <title>将基于 Bert 的 PyTorch 模型导出到 CoreML。如何让 CoreML 模型适用于任何输入？</title>
      <link>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</link>
      <description><![CDATA[我使用以下代码将基于 Bert 的 PyTorch 模型导出到 CoreML。
由于我使用
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)

在 macOS 上测试时，CoreML 模型仅适用于该输入。如何让 CoreML 模型适用于任何输入（即任何文本）？

导出脚本：
# -*- coding: utf-8 -*-
&quot;&quot;&quot;Core ML Export
pip install tr​​ansformers torch coremltools nltk
&quot;&quot;&quot;
导入 os
从 transformers 导入 AutoModelForTokenClassification、AutoTokenizer
导入 torch
导入 torch.nn 作为 nn
导入 nltk
导入 coremltools 作为 ct
nltk.download(&#39;punkt&#39;)
# 加载模型和 tokenizer
model_path = os.path.join(&#39;model&#39;)
model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
# 修改模型的 forward 方法以返回元组
class ModifiedModel(nn.Module):
def __init__(self, model):
super(ModifiedModel, self).__init__()
self.model = model
self.device = model.device # 添加设备属性

def forward(self, input_ids,tention_mask, token_type_ids=None):
outputs = self.model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)
returnoutputs.logits

modified_model = ModifiedModel(model)

# 导出到 Core ML
def convert_to_coreml(model, tokenizer):
# 定义用于跟踪的虚拟输入
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
dummy_input = {k: v.to(model.device) for k, v in dummy_input.items()}

# 使用虚拟输入跟踪模型
traced_model = torch.jit.trace(model,(
dummy_input[&#39;input_ids&#39;],dummy_input[&#39;attention_mask&#39;], dummy_input.get(&#39;token_type_ids&#39;)))

# 转换为 Core ML
输入 = [
ct.TensorType(name=&quot;input_ids&quot;, shape=dummy_input[&#39;input_ids&#39;].shape),
ct.TensorType(name=&quot;attention_mask&quot;, shape=dummy_input[&#39;attention_mask&#39;].shape)
]
if &#39;token_type_ids&#39; in dummy_input:
输入.append(ct.TensorType(name=&quot;token_type_ids&quot;, shape=dummy_input[&#39;token_type_ids&#39;].shape))

mlmodel = ct.convert(traced_model, 输入=inputs)

# 保存 Core ML 模型
mlmodel.save(&quot;model.mlmodel&quot;)
print(&quot;模型导出到 Core ML成功&quot;)

convert_to_coreml(modified_model, tokenizer)

在 Ubuntu 20.04 上测试了 Python 3.10 和 torch 2.3.1（在 Windows 10 上不工作）。]]></description>
      <guid>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</guid>
      <pubDate>Wed, 03 Jul 2024 23:39:36 GMT</pubDate>
    </item>
    <item>
      <title>自定义线性回归的问题</title>
      <link>https://stackoverflow.com/questions/78704446/problem-with-customized-linear-regression</link>
      <description><![CDATA[我正在学习机器学习并研究学习模型。我最近的任务是编写一个具有 MSE 优化的多元线性回归模型（不使用 sklearn 中现有的模型），并用数据对其进行测试。
我的模型是：
 def linear_regression(x: np.array, y: np.array, learning_speed: float = 0.1, echoes: int = 1000):

n = len(y)
coef = np.zeros(len(x[0]) + 1)

for _ in range(echoes):

d = np.array([]) # 数组包含每个 coef 的步骤

for idx in range(len(coef)):
d_coef = 0
for row in range(len(x)):
if idx == len(coef) - 1:
d_coef += (1) * (np.sum(coef[:-1] * x[row]) + coef[-1] - y[row])
else:
d_coef += (x[row, idx]) * (np.sum(coef[:-1] * x[row]) - y[row] + coef[-1]) # (V1X1 + ... + VnXn + b - y) 与 coef idx 相关的导数之和
d = np.append(d, (2 / n) * d_coef)

coef -= learning_speed * d

print(f&#39;coef = {coef[:-1]}&#39;)
print(f&#39;b = {coef[-1]}&#39;)

我使用一些数据集测试了该模型。对于小型数据集，它运行完美。但是后来我尝试了更大的一个：
x1_rnd = np.array([])
x2_rnd = np.array([])
c1 = 5
c2 = 0.75
b = 15
for _ in range(500):
x1_rnd = np.append(x1_rnd, np.random.randint(1, 100))
x2_rnd = np.append(x2_rnd, np.random.randint(1, 100))

combo = zip(x1_rnd, x2_rnd)
x = np.array([list(elem) for elem in combo])
y_rnd = c1 * x1_rnd + c2 * x2_rnd + b

我的模型有显著的错误。即使经过 20000 次迭代（学习速度 = 0.0001），它也能或多或少地预测 X1 和 X2 的系数，但与“b”的预测相差甚远。我注意到较小的“b”会导致更好的预测，而较大的“b”会使预测变差（例如，如果它是 0.1，模型可以正确快速地运行；如果 b = 100，那就是一场噩梦）。
我找不到我的代码梯度下降中的任何错误。请帮忙解决它！]]></description>
      <guid>https://stackoverflow.com/questions/78704446/problem-with-customized-linear-regression</guid>
      <pubDate>Wed, 03 Jul 2024 22:44:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 torch.where 对张量进行阈值处理是否会将其张量从计算图中分离出来？</title>
      <link>https://stackoverflow.com/questions/78704234/does-using-torch-where-to-threshold-a-tensor-detach-it-tensor-from-the-computati</link>
      <description><![CDATA[我正在 PyTorch 中为多类语义分割编写自定义损失函数。该函数的一部分是从张量中选择阈值通道，这些通道用 tracker_index 表示。
该函数的最后一部分是计算图的一部分，即 channel_tensor，如果我注释掉应用 torch.where 的行，一切都会顺利运行。我尝试将 1 和 0 设置为 float32 张量，并确保它们与 channel_tensor 位于同一设备上，这让我相信八分之一阈值是不可微的，因此不能成为损失函数的一部分，否则 torch.where 将始终将张量从计算图中分离出来。请指教。
channel_tensor =torch.select(
segmentation_output,
dim=-3,
index=tracker_index
)
channels[tracker_index]= torch.where(channel_tensor &gt; self.threshold, torch.tensor(1, device=channel_tensor.device, dtype=torch.float32), torch.tensor(0, device=channel_tensor.device, dtype=torch.float32))
]]></description>
      <guid>https://stackoverflow.com/questions/78704234/does-using-torch-where-to-threshold-a-tensor-detach-it-tensor-from-the-computati</guid>
      <pubDate>Wed, 03 Jul 2024 21:13:24 GMT</pubDate>
    </item>
    <item>
      <title>受不同聚类大小约束的 KMeans</title>
      <link>https://stackoverflow.com/questions/78703770/kmeans-constrained-with-different-cluster-size</link>
      <description><![CDATA[我有一个包含商店坐标的数据框，我想根据供应商应该访问该商店的日期将它们划分为簇。例如，假设供应商应该访问 180 家商店。他应该在周一到周五访问 30-34 家商店，周六，他应该访问其他日子的一半，所以在 15 到 17 之间。
你们知道我该怎么做吗？使用 kmeans-constrained 我只能将它们划分为大小相等的簇。也许我需要使用某种解算器或在集群之间移动点来达到我想要的数字，但我不知道该怎么做。
这是我的代码的一部分：
n_clusters = 6
n_shops = len(df)

min_shops = int(n_shops / n_clusters - n_shops * 0.01)
max_shops = int(n_shops / n_clusters + n_shops * 0.01)

kmeans = KMeansConstrained(
n_clusters=n_clusters,
size_min=min_shops,
size_max=max_shops,
random_state=0
)

kmeans.fit(df[[&quot;latitude&quot;, &quot;longitude&quot;]])

labels = kmeans.labels_
centers = kmeans.cluster_centers_

cluster_counts = np.bincount(labels, minlength=n_clusters)
]]></description>
      <guid>https://stackoverflow.com/questions/78703770/kmeans-constrained-with-different-cluster-size</guid>
      <pubDate>Wed, 03 Jul 2024 18:46:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在 AWS DeepRacer 学生联赛中提高我的汽车速度？</title>
      <link>https://stackoverflow.com/questions/78703745/how-to-increase-speed-of-my-car-in-aws-deepracer-student-league</link>
      <description><![CDATA[如何在此处访问速度的输入参数？
 # 读取输入参数
track_width = params[&#39;track_width&#39;]
distance_from_center = params[&#39;distance_from_center&#39;]

我曾尝试使用此方法访问速度参数
speed = params[&#39;speed&#39;]

但这导致错误并且无法运行模型]]></description>
      <guid>https://stackoverflow.com/questions/78703745/how-to-increase-speed-of-my-car-in-aws-deepracer-student-league</guid>
      <pubDate>Wed, 03 Jul 2024 18:36:28 GMT</pubDate>
    </item>
    <item>
      <title>关于python：segment_anything导致numpy.uint8错误</title>
      <link>https://stackoverflow.com/questions/78703313/segment-anything-causing-error-with-numpy-uint8</link>
      <description><![CDATA[我尝试在装有 Sonoma 14.5 的 M2 MacBook 上本地运行 https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb。但是，我在第 11 步一直遇到以下错误：
-------------------------------------------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
Cell In[75]，第 1 行
----&gt; 1 mask = mask_generator.generate(image)

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115，在 context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
112 @functools.wraps(func)
113 def decorate_context(*args, **kwargs):
114 with ctx_factory():
--&gt; 115 return func(*args, **kwargs)

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:163，位于 SamAutomaticMaskGenerator.generate(self, image)
138 &quot;&quot;&quot;
139 为给定图像生成蒙版。
140 
(...)
159 以 XYWH 格式给出的蒙版。
160 &quot;&quot;&quot;
162 # 生成蒙版
--&gt; 163 mask_data = self._generate_masks(image)
165 # 过滤蒙版中的小断开区域和孔洞
166 if self.min_mask_region_area &gt; 0：

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:206，在 SamAutomaticMaskGenerator._generate_masks(self, image) 中
204 data = MaskData()
205 for crop_box, layer_idx in zip(crop_boxes, layer_idxs):
--&gt; 206 crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
207 data.cat(crop_data)
209 # 删除裁剪之间的重复蒙版

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:236，位于 SamAutomaticMaskGenerator._process_crop(self, image, crop_box, crop_layer_idx, orig_size)
234 cropped_im = image[y0:y1, x0:x1, :]
235 cropped_im_size = cropped_im.shape[:2]
--&gt; 236 self.predictor.set_image(cropped_im)
238 # 获取此裁剪的点
239 points_scale = np.array(cropped_im_size)[None, ::-1]

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/predictor.py:57，位于 SamPredictor.set_image(self, image, image_format)
55 # 将图像转换为模型所需的形式
56 input_image = self.transform.apply_image(image)
---&gt; 57 input_image_torch = torch.as_tensor(input_image, device=self.device)
58 input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
60 self.set_torch_image(input_image_torch, image.shape[:2])

RuntimeError: 无法推断 numpy.uint8 的 dtype

我使用的是 Python 3.9.19 的 conda 环境，也用 Python 3.11 进行了测试。根据网上的评论，我怀疑这是 numpy 版本的问题，但尝试了多个版本后，我找不到正确的组合。我目前正在尝试以下内容：
numpy==1.24.4
torch==1.9.0
torchvision==0.10.0
opencv-python==4.10.0.84

在 Google Colab 上运行同一个笔记本可以正常工作，并且指示的版本是：
import numpy as np
import torch
import cv2

print(np.__version__)
print(torch.__version__)
print(cv2.__version__)

1.25.2
2.3.0+cu121
4.8.0

这是使用 Python 3.10.12。这些版本在 Mac 上不可用，所以我陷入困境。
我如何找出 numpy.uint8 无法识别的原因，以及如何修复此错误？大多数在线评论都指向升级 numpy，但我尝试了几个 numpy 版本，但没有成功。任何帮助都非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78703313/segment-anything-causing-error-with-numpy-uint8</guid>
      <pubDate>Wed, 03 Jul 2024 16:43:17 GMT</pubDate>
    </item>
    <item>
      <title>机器学习回归任务：我应该如何解决这个问题？</title>
      <link>https://stackoverflow.com/questions/78702761/machine-learning-regression-task-how-should-i-approach-this-problem</link>
      <description><![CDATA[我正在研究一个问题，预测“明年”何时需要对道路进行处理。我的数据集如下所示：数据集
我对数据进行了如下预处理：使用独热编码（其中没有关系）和标签编码将分类数据转换为数字。我从线性回归开始，实现了 0.72 的 R²。
在我的数据集中，我使用 NEXT_TREATMENT_YEAR 作为目标变量。一个问题是我的数据集没有任何未来日期，所以我希望我的模型具有推断能力。我还使用了随机森林和 XGBoost，但这些都不能推断。我现在正在使用 PyTorch 建立一个神经网络。但是，我需要一些指导来确保我走在正确的轨道上。]]></description>
      <guid>https://stackoverflow.com/questions/78702761/machine-learning-regression-task-how-should-i-approach-this-problem</guid>
      <pubDate>Wed, 03 Jul 2024 14:47:52 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯优化收敛</title>
      <link>https://stackoverflow.com/questions/78702328/bayesian-optimization-convergence</link>
      <description><![CDATA[我正在尝试使用贝叶斯优化为 ML 模型找到最佳参数。优化似乎效果不错，但我对其收敛性有些怀疑。我附上了一张图，显示了不同迭代中的目标值。通常，我会将几乎恒定的值视为优化值。但是，在这种情况下，大多数迭代结束时的值在 30 到 35 之间变化，而对于一些迭代，该值下降到 0.5 左右。您认为这是收敛吗？如果是，还有其他方法可以检查收敛吗？如果没有，您建议如何实现收敛？我正在使用一个名为 bayes_opt 的库。

首先，它需要进行大量迭代。我尝试使用不同的效用函数并更改 kappa 和 xi 值以检查是否有任何变化。使用更高的 kappa，算法似乎收敛得更快一些。我还通过将不同参数的限制与目标值进行比较来更改它们。完成所有这些之后，该图看起来或多或少相同。]]></description>
      <guid>https://stackoverflow.com/questions/78702328/bayesian-optimization-convergence</guid>
      <pubDate>Wed, 03 Jul 2024 13:13:31 GMT</pubDate>
    </item>
    <item>
      <title>需要的建议：从能源效率和楼宇管理系统的初级数据科学家做起 [关闭]</title>
      <link>https://stackoverflow.com/questions/78701117/advice-needed-starting-as-a-junior-data-scientist-in-energy-efficiency-and-buil</link>
      <description><![CDATA[大家早上好，我是这个论坛的新人。我是一名初级数据科学家。9 月份，我将在一家从事商业建筑能源效率和建筑管理系统的公司开始实习。我将成为第一位加入办公室的数据科学家，但老实说，我不知道从哪里开始。我参加了几次面试，以下是我大致了解的情况：他们有现场传感器，可以记录各种建筑参数，如能耗、湿度、二氧化碳和其他特征，每 45 分钟上传一次到公司仪表板。他们的使命是减少能源消耗，从而为客户省钱。他们有一个数据分析师团队，负责执行一些基本的统计。他们希望我开始实施高级统计（我可以做到），对缺失数据进行 ML（我可以做到），对价格进行一些预测，并为我能识别的其他类型的问题实施机器学习。我有一些想法，并且正在阅读几篇关于 BMS 的论文，但我想请您就我能做什么以及您是否知道我可以在哪里找到数据库来运行一些模拟提出一些建议。只要我能带来成果，他们就给了我创作自由。谢谢。
我还没有开始实习，所以我还没有尝试任何具体的方法。但是，根据我对他们的需求和我的技能的理解，我有一个总体计划：
高级统计：我计划从分析从传感器收集的数据开始，以识别模式和相关性。
缺失数据处理：我将使用机器学习技术（如 KNN、MICE 或深度学习模型）来处理缺失数据。
能源消耗预测：我的目标是使用时间序列分析和回归技术开发预测模型来预测能源消耗和价格。（需要帮助）
机器学习应用：我打算探索 ML 在优化能源使用和改善建筑管理方面的其他潜在应用。 （需要帮助）
我希望能够明确从哪里开始，并得到关于具体技术或可能有益的项目的实用建议。我还希望找到实践的资源或数据库。因此，我正在寻求社区的指导，以改进我的方法并确保我走在正确的轨道上。]]></description>
      <guid>https://stackoverflow.com/questions/78701117/advice-needed-starting-as-a-junior-data-scientist-in-energy-efficiency-and-buil</guid>
      <pubDate>Wed, 03 Jul 2024 09:17:19 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用不同分辨率图像训练 DeepLabV3 的最佳实践</title>
      <link>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</link>
      <description><![CDATA[我正在尝试在 COCO 2017 数据集 上训练 PyTorch 的 DeepLabV3 进行语义分割，但我不确定如何处理不同分辨率的图像。我知道 DeepLab 的架构可以毫无问题地处理它们，但由于它们的分辨率，我无法将它们分批堆叠。处理此问题的最佳做法是什么？我是否将它们调整为固定大小？我是否随机裁剪固定大小？我知道有很多解决方案可以解决此问题，但我真的不知道在语义分割训练的背景下最佳做法是什么。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</guid>
      <pubDate>Tue, 02 Jul 2024 16:39:00 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret 设置独热编码</title>
      <link>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</link>
      <description><![CDATA[我陷入了 Pycaret 中分类变量独热编码的问题。问题是，即使设置了我的分类变量，管道也会对分类变量应用规范化，我不知道我做错了什么。
首先，使用下面的代码一切正常：
from pycaret.classification import *
from pycaret.datasets import get_data
import pandas as pd
import numpy as np
import seaborn as sns
dataset = get_data(&#39;income&#39;)
dataset.dtypes

直到我开始设置和
exp_clf01 = setup( data = dataset
, target = &#39;income &gt;50K&#39;
, session_id = 123
, numeric_features = [&#39;age&#39;,&#39;education-num&#39;,&#39;capital-gain&#39;,&#39;capital-loss&#39;,&#39;hours-per-week&#39;]
, categorical_features = [&#39;workclass&#39;,&#39;education&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;,&#39;native-country&#39;]
)
df_transformed = get_config(&quot;X_train&quot;)
df_transformed.head()

尝试查看数据框的头部后，它仅将独热编码应用于列 race，并将其他分类输入标准化，我不明白为什么。




age
workclass
education
education-num
marital-status
occupation
other列




46.0
0.303273
0.271186
11.0
0.101942
0.484643
...


27. 0
0.218620
0.412939
13.0
0.044165
0.484643
...


33.0
0.218557
0.568315
 14.0
0.448894
0.455449
...


60.0
0.218557
0.412673
13.0
0.448894
0.484286
&lt; td&gt;...


25.0
0.218620
0.063798
6.0
0.044165
0.229692
...




我该如何防止这种行为？]]></description>
      <guid>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</guid>
      <pubDate>Sun, 09 Oct 2022 00:59:48 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow：仅当 val_acc 可用时才可以保存最佳模型，跳过</title>
      <link>https://stackoverflow.com/questions/61505749/tensorflowcan-save-best-model-only-with-val-acc-available-skipping</link>
      <description><![CDATA[我遇到了 tf.callbacks.ModelChekpoint 问题。正如您在我的日志文件中看到的那样，警告总是出现在计算 val_acc 的最后一次迭代之前。因此，Modelcheckpoint 永远找不到 val_acc
Epoch 1/30
1/8 [==&gt;...........................] - ETA：19s - 损失：1.4174 - 准确度：0.3000
2/8 [======&gt;.......................] - ETA：8s - 损失：1.3363 - 准确度：0.3500 
3/8 [==========&gt;...................] - ETA：4s - 损失：1.3994 - 准确度：0.2667
4/8 [==============&gt;...............] - ETA：3s - 损失：1.3527 - 准确度：0.3250
6/8 [======================&gt;........] - ETA：1 秒 - 损失：1.3042 - 准确率：0.3333
警告：tensorflow：只有 val_acc 可用时才能保存最佳模型，跳过。
8/8 [================================] - 4s 482ms/step - 损失：1.2846 - 准确度：0.3375 - val_loss：1.3512 - val_accuracy：0.5000

Epoch 2/30
1/8 [==&gt;..............................] - ETA：0s - 损失：1.0098 - 准确度：0.5000
3/8 [==========&gt;...................] - ETA：0s - 损失：0.8916 - 准确度：0.5333
5/8 [==================&gt;............] - ETA：0s - 损失：0.9533 - 准确度： 0.5600
6/8 [=======================&gt;........] - ETA：0s - 损失：0.9523 - 准确度：0.5667
7/8 [==========================&gt;....] - ETA：0s - 损失：0.9377 - 准确度：0.5714
警告：tensorflow：只有 val_acc 可用时才能保存最佳模型，跳过。
8/8 [================================] - 1s 98ms/step - 损失：0.9229 - 准确度：0.5750 - val_loss：1.2507 - val_accuracy：0.5000

这是我用于训练 CNN 的代码。
callbacks = [
TensorBoard(log_dir=r&#39;C:\Users\reda\Desktop\logs\{}&#39;.format(Name),
histogram_freq=1),
ModelCheckpoint(filepath=r&quot;C:\Users\reda\Desktop\checkpoints\{}&quot;.format(Name), monitor=&#39;val_acc&#39;,
verbose=2, save_best_only=True, mode=&#39;max&#39;)]
history = model.fit_generator(
train_data_gen, 
steps_per_epoch=total_train // batch_size,
epochs=epochs,
validation_data=val_data_gen,
validation_steps=total_val // batch_size,
callbacks=callbacks)
]]></description>
      <guid>https://stackoverflow.com/questions/61505749/tensorflowcan-save-best-model-only-with-val-acc-available-skipping</guid>
      <pubDate>Wed, 29 Apr 2020 15:38:04 GMT</pubDate>
    </item>
    <item>
      <title>使用已保存的模型通过 Java 代码（Weka）测试数据</title>
      <link>https://stackoverflow.com/questions/47952227/using-a-saved-model-to-test-data-through-java-code-weka</link>
      <description><![CDATA[我对机器学习完全陌生，所以在某些情况下我的理解可能是错误的。我正尝试使用 weka 通过 Java 代码加载已保存的模型来测试数据。
 Instances testingData = readArffFile(testFile);
try
{
LibSVM cls = (LibSVM) weka.core.SerializationHelper.read(model);

Evaluation eval = new Evaluation(testingData);
eval.crossValidateModel(cls​​, testingData, 10, new Random(1));
//eval.evaluateModel(cls​​, testingData);
}

当我保存模型时，我使用了交叉验证。
现在我有点困惑，是使用 eval.crossValidateModel() 还是 eval.evaluateModel()。 
如果我使用 evaluateModel()，它会给出错误的准确率（比实际准确率高得多）。我认为这是因为 &lt;Evaluation eval = new Evaluation(testingData);&gt; 行在相同的数据上进行训练，然后在相同的数据上进行测试。这不是我想要的。我想在不进行训练的情况下在模型上测试数据（我认为模型在训练后会保存）
如果我使用 eval.crossValidateModel()，我认为它仍在再次训练模型，因为据我所知，交叉验证将数据集分成 k 倍，然后对 k-1 进行训练，然后对剩余的倍进行测试。
那么有没有办法只使用这个加载的模型进行测试？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/47952227/using-a-saved-model-to-test-data-through-java-code-weka</guid>
      <pubDate>Sat, 23 Dec 2017 11:46:38 GMT</pubDate>
    </item>
    <item>
      <title>前馈神经网络的图灵完备性？</title>
      <link>https://stackoverflow.com/questions/46696576/turing-completeness-of-feed-forward-neural-networks</link>
      <description><![CDATA[我读到 RNN 是图灵完备的，而前馈神经网络 (FFN) 不是。但基于通用近似定理，FFN 可以在给定足够多节点的情况下模拟任何函数，而且我们还知道 lambda Church 演算（基于无状态函数）等同于图灵机，为什么 FFN 不能通过在 Church 演算中模拟任意函数来实现图灵完备呢？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/46696576/turing-completeness-of-feed-forward-neural-networks</guid>
      <pubDate>Wed, 11 Oct 2017 20:01:46 GMT</pubDate>
    </item>
    <item>
      <title>Keras：内核和活动正则化器之间的区别</title>
      <link>https://stackoverflow.com/questions/44495698/keras-difference-between-kernel-and-activity-regularizers</link>
      <description><![CDATA[我注意到 Keras 中不再提供 weight_regularizer，取而代之的是 activity 和 kernel 正则化器。
我想知道：

kernel 和 activity 正则化器之间的主要区别是什么？
我可以使用 activity_regularizer 代替 weight_regularizer 吗？
]]></description>
      <guid>https://stackoverflow.com/questions/44495698/keras-difference-between-kernel-and-activity-regularizers</guid>
      <pubDate>Mon, 12 Jun 2017 09:16:34 GMT</pubDate>
    </item>
    </channel>
</rss>