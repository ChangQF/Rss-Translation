<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 17 Aug 2024 01:05:58 GMT</lastBuildDate>
    <item>
      <title>随机森林模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</link>
      <description><![CDATA[我正在尝试使用 Python 创建一些模型来预测城市与公司的合作，但我一直在数据解析部分遇到问题，我尝试制作一个随机森林模型，但在读取列时遇到了问题，即使我清理了数据并将列名更改为每个文件中的完全相同，我该如何解决这个问题？
这些是我正在使用的数据集：
text
import pandas as pd
import matplotlib.pyplot as plt

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
cities_responses_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Cities/Cities Responses/2020_Full_Cities_Dataset.csv&quot;)

corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)
corp_water_security_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Water Security/2020_Corporates_Disclosing_to_CDP_Water_Security.csv&quot;)

# 合并数据集
merged_2020 = pd.merge(cities_disclosing_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_2020 = pd.merge(merged_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

merged_responses_2020 = pd.merge(cities_responses_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_responses_2020 = pd.merge(merged_responses_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

# 计算协作率
merged_2020[&#39;climate_change_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_climate&#39;]).astype(int)
merged_2020[&#39;water_security_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_water&#39;]).astype(int)

# 计算影响
merged_responses_2020[&#39;impact&#39;] = merged_responses_2020[&#39;Response Answer_city&#39;].apply(lambda x: len(str(x)))

# 计算协作率和平均影响
climate_change_collab_rate = merged_2020[&#39;climate_change_collaboration&#39;].mean()
water_security_collab_rate = merged_2020[&#39;water_security_collaboration&#39;].mean()
average_impact_2020 = merged_responses_2020[&#39;impact&#39;].mean()

print(f&quot;2020 年气候变化协作率：{climate_change_collab_rate}&quot;)
print(f&quot;2020 年水安全协作率：{water_security_collab_rate}&quot;)
print(f&quot;2020 年对城市的平均影响：{average_impact_2020}&quot;)

# 协作率条形图
collab_rates = {
&#39;气候变化&#39;:climate_change_collab_rate,
&#39;水安全&#39;: water_security_collab_rate
}
plt.bar(collab_rates.keys(), collab_rates.values())
plt.title(&#39;2020 年的协作率&#39;)
plt.ylabel(&#39;Rate&#39;)
for i, rate in enumerate(collab_rates.values()):
plt.text(i, rate + 0.01, f&#39;{rate:.2f}&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)
plt.show()

# 影响分布的直方图
plt.hist(merged_responses_2020[&#39;impact&#39;], bins=20, edgecolor=&#39;black&#39;)
plt.title(&#39;2020 年的影响分布&#39;)
plt.xlabel(&#39;影响&#39;)
plt.ylabel(&#39;频率&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</guid>
      <pubDate>Fri, 16 Aug 2024 23:07:44 GMT</pubDate>
    </item>
    <item>
      <title>什么时候需要将数据居中？</title>
      <link>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</link>
      <description><![CDATA[我正在尝试找出不同的缩放方法，我有以下问题。StandardScaler、RobustScaler 和类似的缩放器将数据相对于平均值或中位数居中，但 MinMaxScaler 不会这样做，它只会将数据传输到 0 到 1 的范围。什么时候应该将数据居中，什么时候不需要？
我还读到，当数据已经呈正态分布时，最好使用 StandardScaler，但如果只有一部分数据具有此属性怎么办？我应该对一半数据使用 StadardScaler，对另一半数据使用 MinMaxScaler（听起来不合逻辑）还是优先使用其中一种方法？]]></description>
      <guid>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</guid>
      <pubDate>Fri, 16 Aug 2024 21:39:50 GMT</pubDate>
    </item>
    <item>
      <title>添加更多退化效果来动态训练 Real-ESRGAN</title>
      <link>https://stackoverflow.com/questions/78880440/adding-more-degradation-effects-to-train-real-esrgan-on-the-fly</link>
      <description><![CDATA[我计划为我的项目微调 real-ESRGAN，当我在其 github repo 中进行研究时，发现有两种情况可以微调模型：

动态生成退化图像（这意味着我只需要向模型提供高分辨率图像，它就会通过模糊、添加噪声、压缩等方式自动生成低质量图像）
使用我自己的配对数据

对于第一种情况，我想知道我是否可以添加更多退化效果，例如将颜色更改为灰度或复古黄色以使其看起来像旧照片？]]></description>
      <guid>https://stackoverflow.com/questions/78880440/adding-more-degradation-effects-to-train-real-esrgan-on-the-fly</guid>
      <pubDate>Fri, 16 Aug 2024 18:39:39 GMT</pubDate>
    </item>
    <item>
      <title>如何配置 Co-DETR 来预测四边形而不是边界框？</title>
      <link>https://stackoverflow.com/questions/78880373/how-do-i-configure-co-detr-to-predict-quadrilaterals-instead-of-bounding-boxes</link>
      <description><![CDATA[我已经使用 MMDetection 框架在 COCO 数据集上训练了一个 Co-DETR 模型，该模型用于手写人口普查记录，其中注释是四边形，由于透视偏移或旋转，它们接近矩形但不完全是矩形。
该模型训练得很好，但当我运行推理时，模型输出的是边界框而不是四边形。当人口普查记录旋转或透视偏移时，这会出现问题，如绿色列所示：

从这里可以看出，列的左上角和绿色边界框匹配，右下角和边界框非常接近，但列的其他角与预测边界框的角偏离，因为这里的图像略有旋转。
这个框架是否有任何设置，使模型可以输出四边形预测（基本上每个类有 4 个关键点），这些预测与它所训练的注释相匹配，而不是边界盒子？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78880373/how-do-i-configure-co-detr-to-predict-quadrilaterals-instead-of-bounding-boxes</guid>
      <pubDate>Fri, 16 Aug 2024 18:17:01 GMT</pubDate>
    </item>
    <item>
      <title>如何打包使用SAM语义分割模型的Python项目？</title>
      <link>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</link>
      <description><![CDATA[我有一个 Python 项目，其中我使用 Segment-Anything 模型的结果来找出通道内的气泡位置。现在，我需要打包应用程序，然后将其交给我的同事，但我不想在他们所有的电脑上安装和调试 pytorch 和 conda。
我目前正在尝试使用 Briefcase 和 PyOxidizer，一旦我尝试它们，我将尝试更新它。
我确实想支持 mac，因为他们主要使用 mac，但我可以强制他们使用 windows。
真的只是想知道是否有任何技巧或窍门可以做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</guid>
      <pubDate>Fri, 16 Aug 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>隐式分面搜索 [关闭]</title>
      <link>https://stackoverflow.com/questions/78879509/implicit-faceted-search</link>
      <description><![CDATA[我目前正在探索分面搜索的原理和工作原理，特别关注显式和隐式分面搜索之间的差异。
到目前为止，我发现显式分面搜索很简单，涉及用户明确选择方面来优化搜索结果。这可以使用 Elasticsearch 或 Apache Solr 等开源搜索引擎轻松实现。
但是，我对隐式分面搜索更感兴趣，它涉及一个可以解释自然语言查询并自动将其转换为显式搜索指令的 AI 系统。这需要更深层次的自然语言理解和可能的机器学习技术来识别和提取用户输入中的相关方面。
例如：我正在寻找一份新工作，特别是软件开发的远程工作。我住在纽约，想在城市及其周边工作。
因此，将获取的方面是，技能：软件开发；工作类型：远程；地点：纽约。
我的具体问题是：
可以使用哪些 NLP 技术来准确地从自然语言查询中提取方面？
如何将这些方面动态转换为 Elasticsearch 或 Solr 中的搜索过滤器？
是否有现有的工具或方法可以将这种方面提取与传统搜索引擎集成？
我已经探索了命名实体识别 (NER) 等基本 NLP 模型，但不确定如何将其扩展到成熟的隐式方面搜索系统。任何关于如何解决这个问题的指导都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78879509/implicit-faceted-search</guid>
      <pubDate>Fri, 16 Aug 2024 14:06:30 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 TensorFlow 中张量包含 Nan 或 inf 值错误？</title>
      <link>https://stackoverflow.com/questions/78879458/how-to-resolve-tensors-contains-nan-or-inf-values-error-in-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78879458/how-to-resolve-tensors-contains-nan-or-inf-values-error-in-tensorflow</guid>
      <pubDate>Fri, 16 Aug 2024 13:54:12 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中实现零层 Transformer</title>
      <link>https://stackoverflow.com/questions/78879457/implementing-a-zero-layer-transformer-in-pytorch</link>
      <description><![CDATA[我正在尝试实现零层变压器，如本文章或视频中所述。我想到了以下实现：
import torch
import torch.nn as nn
import torch.nn. functional as F

class ZeroLayerTransformers(nn.Module):
def __init__(self):
super().__init__()
self.W_e = nn.Embedding(vocab_size, embedding_size)
self.W_U = nn.Embedding(vocab_size, embedding_size)

def forward(self, tokens, target=None):
x = self.W_e(tokens)
logits = torch.matmul(x, self.W_U.weight.T)

if goals is None:
loss = None
else:
batch_size, time, _ = logits.shape
logits = logits.view(batch_size * time, vocab_size)
target = target.view(batch_size * time)
loss = F.cross_entropy(logits, target)
return logits, loss

这是零层变换器的正确实现吗？
我按照图示实现了 ZeroLayerTransformer，并在一个小型文本语料库上对其进行了训练。我预期：

W_e 和 W_U 的乘积近似二元语法对数概率。
W_e * W_U 的可视化显示清晰的单词关联模式。
在下一个单词预测方面的表现类似于简单的 n-gram 模型。

模型训练（损失减少），但我不确定它是否真正按预期捕获了二元语法统计数据。我也不确定我的实现是否准确地代表了文章中描述的概念。]]></description>
      <guid>https://stackoverflow.com/questions/78879457/implementing-a-zero-layer-transformer-in-pytorch</guid>
      <pubDate>Fri, 16 Aug 2024 13:53:45 GMT</pubDate>
    </item>
    <item>
      <title>Reshape RuntimeError：形状'[1, 2]'对于大小为 100 的输入无效</title>
      <link>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</link>
      <description><![CDATA[我正在 Pytorch 中构建一个简单的足球比分预测模型。
我的步骤：

清理和处理数据
使用球队名称作为输入，将分数作为输出
对球队名称进行独热编码
将两个球队名称传递给模型
添加非线性
（问题就在这里）接收两个分数作为输出

但是我的模型多次返回两个分数
tensor([[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4845], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4 844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844]], grad_fn=&lt;SigmoidBackward0&gt;)

这是我的 NN 的输出，但我不确定为什么当我的输出层大小为 2 个神经元时，它会返回 50x2。
我的模型：
class EstimatorModel(nn.Module):
def __init__(self,
input_neurons:int,
hidden_​​neurons:int,
output_neurons:int):

super().__init__()
self.input_neurons= nn.Linear(in_features=input_neurons, out_features=hidden_​​neurons)
self.hidden_​​neurons= nn.Linear(in_features=hidden_​​neurons, out_features=hidden_​​neurons)
self.output_neurons= nn.Linear(in_features=hidden_​​neurons, out_features=output_neurons)

# 激活函数
self.sigmoid = nn.Sigmoid()

def forward(self, x):
x = self.input_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.output_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

return x

使用带有 Adam 优化器的 MSEloss。
训练循环：
 for e in range(epochs):
x=0
for input, output in zip(input_data, output_data):
input = stack(inputs, dim=1)
output = stack(outputs, dim=1)
#outputs = tensor(outputs

print(outputs.shape)
print(outputs)

prediction:Tensor = model(inputs)
prediction_processed = model.process_model_output(prediction)

print(prediction.shape)
print(prediction)

loss = loss_func(prediction, output.float())

optim.zero_grad()
loss.backward()
optim.step()

quit()

x+=1
# 如果您不喜欢太冗长，请注释掉此打印语句
#print(f&quot;[INFO] {x+1} 对完成。损失：{loss}&quot;)

有什么建议吗？
尝试改变输入传递的方式，例如，正常传递，作为单个张量传递，最后作为堆栈传递。没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</guid>
      <pubDate>Fri, 16 Aug 2024 10:20:05 GMT</pubDate>
    </item>
    <item>
      <title>如何提高使用短期时间序列数据预测累计能耗的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/78878351/how-to-improve-accuracy-in-predicting-cumulative-energy-consumption-using-short</link>
      <description><![CDATA[我正在研究基于短期时间序列数据的电动汽车累计能耗预测模型。数据集包含大约 1000 个样本，每个样本包含 30 个时间步长的传感器数据，每个时间步长有 4 个特征。目标值是最后一个时间步长的累计能耗。
目前的方法：
我已经尝试了基于 CNN (ResNet) 和基于 RNN (LSTM) 的架构。
但是，这两种方法的预测精度都不令人满意。
问题：
我的目标是专门针对最后一个时间步长的累计能耗优化预测。
我担心仅针对最后一步进行优化可能会导致问题，或者无法充分利用数据的顺序性。
问题：

架构：对于这项任务，我是否应该考虑进行任何特定的架构调整？例如，CNN 和 RNN 架构的组合（例如 ConvLSTM）是否更有效？
数据：鉴于序列长度较短（30 个时间步长），是否有任何数据预处理或增强技术可以帮助提高模型性能？
损失函数：我是否应该考虑使用自定义损失函数，更加注重最终时间步长的准确性，或者是否有更适合这种类型预测的替代损失函数？
其他考虑因素：是否有任何其他建模技术、正则化方法或优化策略可能特别有助于提高短期时间序列数据中累积预测的准确性？
]]></description>
      <guid>https://stackoverflow.com/questions/78878351/how-to-improve-accuracy-in-predicting-cumulative-energy-consumption-using-short</guid>
      <pubDate>Fri, 16 Aug 2024 08:57:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 android kotlin 中实现输出形状为 [1, 25200, 6],[1, 2, 640, 640],[1, 2, 640, 640] 的 yolop ( 分割模型 ) [关闭]</title>
      <link>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</link>
      <description><![CDATA[我正在开发一个 Android 项目，需要使用 YOLOp 模型进行分割。我已成功将模型转换为 onnx/tflite 格式，但在处理输出形状时遇到了困难。该模型包含三个输出，形状分别为 [1, 25200, 6]、[1, 2, 640, 640] 和 [1, 2, 640, 640]
如何在我的 Android 应用程序中解释和处理此输出形状？具体来说，我需要帮助来理解输出张量的结构并提取对象的边界框坐标和分割蒙版。
是否有任何与在 Android 中使用此特定输出形状实现 YOLOP TFLite 相关的代码片段、建议或资源？]]></description>
      <guid>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</guid>
      <pubDate>Fri, 16 Aug 2024 08:05:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关优化 500 个摄像头的计算机视觉集成的建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</link>
      <description><![CDATA[我拥有三个计算机视觉神经网络，正在连接 500 多个摄像头。目前，我正在使用 5 个摄像头进行测试，使用 OpenCV 库接收 RTSP 流。然后，我使用 Ultralytics 库加载神经网络并通过它们处理每个帧。结果使用 OpenCV 绘制为边界框，我使用 Streamlit 显示图像。我正在一台配备 12 GB GPU 的简单 PC 上测试此设置，同时等待一台配备两个 GPU（总计 98 GB）的服务器。但是，性能非常慢，有时系统会完全冻结。
作为计算机视觉的初学者，考虑到未来 500 个摄像头的负载，您能否推荐一些库或方法来提高性能？我计划从 OpenCV 切换到支持 GPU 的库。将模型导出到 ONNX 并通过 ONNX Runtime 运行它们是否有助于提高性能？有人组织过类似的系统吗？]]></description>
      <guid>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</guid>
      <pubDate>Fri, 16 Aug 2024 06:52:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Android 中使用 Google ML Kit 对同一个人的图像进行分组？</title>
      <link>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</link>
      <description><![CDATA[我正在开发一款 Android 应用，需要对包含同一个人脸的图像进行分组。我计划使用 Google ML Kit 进行人脸检测和识别，但我不知道如何实现比较人脸和对人脸进行分组的逻辑。
有人能解释一下使用 Google ML Kit 实现此目的所需的步骤或逻辑吗？我还没有示例代码，所以我需要详细的解释或任何指示。
到目前为止，我已经将 Google ML Kit 集成到我的 Android 项目中并成功实现了人脸检测。但是，我不知道如何继续比较检测到的人脸以确定它们是否属于同一个人，然后相应地对它们进行分组。
我期望找到一种方法来匹配检测到的人脸并创建包含同一个人的图像的集合。理想情况下，我想了解使用 Google ML Kit 实现此逻辑的最佳方法，无论是通过人脸嵌入、特征提取还是任何其他可能有效的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</guid>
      <pubDate>Fri, 16 Aug 2024 04:38:24 GMT</pubDate>
    </item>
    <item>
      <title>TimeSeriesDataSet/TemporalFusionTransformer - 发现类“NoneType”错误，我认为它在“target_scale”数组中</title>
      <link>https://stackoverflow.com/questions/78876125/timeseriesdataset-temporalfusiontransformer-found-class-nonetype-error-i-th</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78876125/timeseriesdataset-temporalfusiontransformer-found-class-nonetype-error-i-th</guid>
      <pubDate>Thu, 15 Aug 2024 16:58:04 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    </channel>
</rss>