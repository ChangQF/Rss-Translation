<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 27 Sep 2024 09:19:20 GMT</lastBuildDate>
    <item>
      <title>如何才能准确填补数据集中的缺失值？</title>
      <link>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</link>
      <description><![CDATA[我正在处理一个包含二手车详细信息的数据集，我发现 Fuel_Type 列中缺少几个值。可能的值包括“汽油”、“E85 混合燃料”、“混合动力”、“柴油”等。目前，我的数据中有超过 4,000 辆电动汽车、不到 50 辆汽油车和一些缺少 Fuel_Type 条目的混合动力汽车。此外，一些条目包含非标准值，如“–”和“不支持”。准确填充这些缺失值对我的分析至关重要，因为它们会显著影响结果。
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# 示例 DataFrame
data = {
&#39;Car&#39;: [&#39;Toyota&#39;, &#39;Honda&#39;, &#39;Tesla&#39;, None, &#39;Ford&#39;],
&#39;Fuel_Type&#39;: [&#39;Gasoline&#39;, &#39;E85 Flex Fuel&#39;, np.nan, &#39;Hybrid&#39;, None],
&#39;Transmission&#39;: [&#39;Automatic&#39;, None, &#39;Automatic&#39;, &#39;Manual&#39;, &#39;Manual&#39;]
}

df = pd.DataFrame(data)

# 初始插补尝试
imputer = SimpleImputer(strategy=&#39;most_frequent&#39;)
df[&#39;Fuel_Type&#39;] = imputer.fit_transform(df[[&#39;Fuel_Type&#39;]])
print(df)
]]></description>
      <guid>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</guid>
      <pubDate>Fri, 27 Sep 2024 07:15:40 GMT</pubDate>
    </item>
    <item>
      <title>当有多个场景切换时，有没有办法让 SAM2 跨场景跟踪同一个人？</title>
      <link>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</link>
      <description><![CDATA[使用 Meta 的 SAM2 演示，当场景切换时，面具通常会切换到不同的玩家身上。
我知道手动重新标记每个场景中的玩家是一种选择，但我想探索是否有可用的自动化解决方案。
谢谢！

我尝试使用 Meta 的 SAM2 演示，网址为 https://sam2.metademolab.com/
我希望它能在整个视频中跟踪勒布朗
我发现它只在第一个场景中这样做，偶尔当勒布朗是镜头中唯一的人或主要人物时
]]></description>
      <guid>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</guid>
      <pubDate>Fri, 27 Sep 2024 04:47:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在 rust 中导入 python 制作的 ML 模型 (.pkl) 吗</title>
      <link>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</link>
      <description><![CDATA[我之前用 Python 构建了一个项目，但由于它占用了太多资源并且缺乏并发性，所以我改用了 rust。现在我不知道如何正确迁移它，大多数代码已经迁移，但我无法导入导出为 .pkl 文件的 ml 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</guid>
      <pubDate>Fri, 27 Sep 2024 04:43:46 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练推荐系统算法</title>
      <link>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</link>
      <description><![CDATA[我一直收到以下错误：

ratings_matrix[which_train, ] 中的错误：维度数不正确

&gt; which_train &lt;- sample(x = c(TRUE, FALSE),
+ size = (ratings_matrix),
+ replace = TRUE,
+ prob = c(0.8, 0.2))
&gt; recc_data_train &lt;- ratings_matrix[which_train, ]
]]></description>
      <guid>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</guid>
      <pubDate>Thu, 26 Sep 2024 23:51:08 GMT</pubDate>
    </item>
    <item>
      <title>相关变量的系数是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</link>
      <description><![CDATA[不相关变量的系数表示其中的独特信息对最终变量的影响程度。但相关变量的系数意味着什么 - 谁的“拉锯战”程度如何？（请不要进行数学运算）]]></description>
      <guid>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</guid>
      <pubDate>Thu, 26 Sep 2024 16:11:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行模拟退火[关闭]</title>
      <link>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</link>
      <description><![CDATA[我正尝试使用 LOOCV（留一交叉验证）作为交叉验证方法，对 25 个预测变量实施模拟退火回归模型，进行 100 次迭代。
然而，问题是代码已经运行了 10 天，但仍然没有完成。以下代码可能存在问题？
# 库
library(caret)
library(foreach)
library(pROC)
library(randomForest)
library(parallel)
library(doParallel)

# 设置并行后端
Mycluster &lt;- makeCluster(detectCores() - 2) # 使用可用核心减 2
registerDoParallel(Mycluster) # 注册并行执行

stime = system.time({
# 训练函数的回归控制
reg.ctrl &lt;- trainControl(method = &quot;LOOCV&quot;, 
number = 10, 
repeats = 5, 
allowParallel = TRUE)

# 模拟退火控制设置
safs.ctrl &lt;- safsControl(functions = caretSA, 
method = &quot;LOOCV&quot;, 
number = 10,
metric = c(internal = &quot;RMSE&quot;, external = &quot;RMSE&quot;),
maximize = c(internal = FALSE, external = FALSE),
holdout = 0.2, 
improve = 5,
allowParallel = TRUE, 
verbose = TRUE)

# 模拟退火特征选择
sa_100 &lt;- safs(x = MEs[, 1:dim(MEs)[[2]]],
y = gsva[1, ],
iters = 100, 
metric = &quot;RMSE&quot;,
trControl = reg.ctrl,
safsControl = safs.ctrl)
})[3]

stime[1]

# 计算完成后停止集群
stopCluster(Mycluster)
registerDoSEQ() # 重置为顺序执行
]]></description>
      <guid>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</guid>
      <pubDate>Thu, 26 Sep 2024 14:21:10 GMT</pubDate>
    </item>
    <item>
      <title>多头自注意力中的梯度爆炸（NaN 训练损失和验证损失） - Vision Transformer</title>
      <link>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</link>
      <description><![CDATA[此多头自注意力代码导致训练损失和验证损失变为 NaN，但当我删除此部分时，一切都恢复正常。我知道当训练损失和验证损失变为 NaN 时，这意味着那里有一个爆炸梯度。但是，我不知道我的代码出了什么问题导致梯度爆炸。当我将它与官方 PyTorch 代码进行比较时，它看起来很相似。当我使用 nn.MultiheadSelfAttention 时，梯度不会爆炸，但当我使用我自己的代码时，梯度开始爆炸。没有显示任何错误消息。有人知道我下面的代码有什么问题吗？
class MultiHeadAttention(nn.Module):
def __init__(self, in_dim, num_heads=8, dropout=0):
super().__init__()
self.num_heads = num_heads
self.head_dim = in_dim // num_heads
self.conv_q = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_k = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_v = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.att_drop = nn.Dropout(dropout)
self.proj = nn.Conv2d(in_dim, in_dim，kernel_size=1)
self.proj_drop = nn.Dropout(dropout)

def forward(self, x):

b, _, h, w = x.shape

q = self.conv_q(x)
k = self.conv_k(x)
v = self.conv_v(x)

q = rearrange(q，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
k = rearrange(k，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
v = rearrange(v，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)

att_score = q @ k.transpose(2, 3) ** (self.head_dim ** -0.5)
att_score = F.softmax(att_score, dim=-1)
att_score = self.att_drop(att_score)

x = att_score @ v

x = rearrange(x, &#39;b nh (h w) hd -&gt; b (nh hd) h w&#39;, h=h, w=w)

x = self.proj(x)
x = self.proj_drop(x)

返回 x
]]></description>
      <guid>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</guid>
      <pubDate>Thu, 26 Sep 2024 11:48:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 gpu 上运行 Qwen2-VL 模型？</title>
      <link>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</link>
      <description><![CDATA[我有 4 个 gpu，我想运行 Qwen2 VL 模型，但我收到“设备端断言已触发。使用 TORCH_USE_CUDA_DSA 进行编译以启用设备端断言”错误。
model_name=&quot;Qwen/Qwen2-VL-2B-Instruct&quot;
model = Qwen2VLForConditionalGeneration.from_pretrained(
model_name, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)
model = nn.DataParallel(model)
processor = AutoProcessor.from_pretrained(model_name)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;image&quot;,
&quot;image&quot;: file
},
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;&quot;&quot;描述图像&quot;&quot;&quot;
}
]
}
]
text = processing.apply_chat_template(
messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processing(
text=[text],
images=image_inputs,
videos=video_inputs,
padding=True,
return_tensors=&quot;pt&quot;,
)
使用 torch.no_grad():
generated_ids = model.module.generate(**inputs, max_new_tokens=128)

但我总是得到：
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [31,0,0] 断言 `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` 失败。
错误：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

回溯（最近一次调用）：
文件“/home/ubuntu/projects/mistral-qaC/services/VisionService.py”，第 104 行，位于 ask_vision
generated_ids = self.model.module.generate(
^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/utils/_contextlib.py”，第 116 行，位于 decorate_context
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py”，第2015，在 generate 中
result = self._sample(
^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py&quot;，第 2965 行，在 _sample 中
output = self(**model_inputs, return_dict=True)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/accelerate/hooks.py&quot;，第 169 行，在 new_forward 中
output = module._old_forward(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py&quot;，第 1598 行，正向
输入_embeds[image_mask] = image_embeds
~~~~~~~~~~~~~^^^^^^^^^^^^
RuntimeError：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

我尝试了什么？

使用 CUDA_LAUNCH_BLOCKING=1 python script.py 运行我的 python 脚本，但它也不起作用。
打印输入和模型设备：模型设备：cuda:0
输入设备：cuda:0
torch.cuda.synchronize() 和 torch.cuda.empty_cache() 在生成之前。

输入的形状：

input_ids 的形状：torch.Size([1, 759])
attention_mask 的形状：torch.Size([1, 759])
 pixel_values: torch.Size([2940, 1176])
image_grid_thw 的形状：torch.Size([1, 3])

我的 transformers 和 pytorch 版本是：
transformers==4.45.0.dev0
torch==2.4.1+cu124

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</guid>
      <pubDate>Thu, 26 Sep 2024 11:21:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用集成模型进行二元分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79026233/iam-doing-a-binary-classification-using-ensembel-model</link>
      <description><![CDATA[我正在使用 3 个模型，mobilenet、inceptionv3、resnet50。准确率似乎在增加，但验证准确率并没有稳定增加，而是在波动。我也做了一些数据增强，即重新缩放。但验证准确率仍然在波动。我使用的是平均集成方法。
# 定义 MobileNet 模型
def create_mobilenet_model():
输入 = 输入（形状=（224, 224, 3））
base_model = MobileNet（include_top=False，权重=&#39;imagenet&#39;，输入张量=输入，池化=无）
x = base_model.output
x = GlobalAveragePooling2D()（x）
x = BatchNormalization()（x）
x = Dense(512, 激活=&#39;relu&#39;)(x)
x = Dropout(0.5)(x)
x = Dense(256, 激活=&#39;relu&#39;)(x)
x = Dropout(0.3)(x)
输出 = Dense(1, 激活=&#39;sigmoid&#39;)(x)
模型 = Model(inputs=inputs, output=outputs)
返回模型
​
# 定义 InceptionV3 模型
def create_inceptionv3_model():
输入 = 输入(shape=(224, 224, 3))
base_model = InceptionV3(include_top=False, weights=&#39;imagenet&#39;, input_tensor=inputs, pooling=None)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = BatchNormalization()(x)
x = Dense(512,activation=&#39;relu&#39;)(x)
x = Dropout(0.5)(x)
x = Dense(256,activation=&#39;relu&#39;)(x)
x = Dropout(0.3)(x)
输出 = Dense(1,activation=&#39;sigmoid&#39;)(x)
model = Model(inputs=inputs, output=outputs)
返回模型
​
# 定义ResNet50 模型
def create_resnet50_model():
输入 = 输入（形状=（224, 224, 3））
base_model = ResNet50（include_top=False，权重=&#39;imagenet&#39;，input_tensor=inputs，pooling=None）
x = base_model.output
x = GlobalAveragePooling2D（）（x）
x = BatchNormalization（）（x）
x = Dense（512，activation=&#39;relu&#39;）（x）
x = Dropout（0.5）（x）
x = Dense（256，activation=&#39;relu&#39;）（x）
x = Dropout（0.3）（x）
输出 = Dense（1，activation=&#39;sigmoid&#39;）（x）
model = Model（inputs=inputs，outputs=outputs）
返回模型
​

# 结合 MobileNet、InceptionV3 和 ResNet50 的集成模型
def create_ensemble_model():
mobilenet_model = create_mobilenet_model()
inceptionv3_model = create_inceptionv3_model()
resnet50_model = create_resnet50_model()
​
# 集成输入
input = Input(shape=(224, 224, 3))
​
# 获取每个模型的输出
mobilenet_output = mobilenet_model(inputs)
inceptionv3_output = inceptionv3_model(inputs)
resnet50_output = resnet50_model(inputs)
​
# 使用平均合并输出
combined_output = tf.keras.layers.Average()([mobilenet_output, inceptionv3_output, resnet50_output])
​
# 定义最终集成模型
ensemble_model = Model(inputs=inputs, output=combined_output)
​
return ensemble_model

# 编译并创建集成模型
ensemble_model = create_ensemble_model()
ensemble_model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 模型摘要
ensemble_model.summary()

结果：
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 369s 2s/step - accuracy: 0.6506 - loss: 0.6266 - val_accuracy: 0
.3315 - val_loss: 0.9679
Epoch 2/10
111/111 ━━━━━━━━━━━━━━━━━━━━━ 45s 403ms/step - 准确度：0.8518 - 损失：0.3550 - val_accuracy：0.8117 - val_loss：0.4355
Epoch 3/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 45s 403ms/step - 准确度：0.9218 - 损失： 0.2276 - val_accuracy：0.9639 - val_loss：0.2109
Epoch 4/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 78s 366ms/step - 准确度：0.9578 - 损失：0.1474 - val_accuracy：0.7565 - val_loss：0.4118
Epoch 5/10
111/111 ━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9613 - 损失：0.1276 - val_accuracy：0.9030 - val_loss：0.2680
Epoch 6/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9811 - 损失：0.0815 - val_accuracy：0.8083 - val_loss：0.3373
Epoch 7/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 82s 369ms/step - 准确度：0.9829 - 损失：0.0829 - val_accuracy：0.7655 - val_loss：0.7217
Epoch 8/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9863 - 损失：0.0569 - val_accuracy：0.5975 - val_loss：1.4897
]]></description>
      <guid>https://stackoverflow.com/questions/79026233/iam-doing-a-binary-classification-using-ensembel-model</guid>
      <pubDate>Thu, 26 Sep 2024 08:24:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型拍摄两张图像并验证它们是否相同。我想要训练眼睛认证模型的软件算法。
我在网上搜索过，但只能找到关于识别软件算法（一对多）的答案。
如何在 PyTorch 代码中训练验证模型？
需要澄清的是，相同是指眼睛相同，意味着它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 进行 OCR 和文本检测和识别</title>
      <link>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</link>
      <description><![CDATA[我正在开发一个测试应用程序，使用 Google Collab 在 Python 中开发一个小型文本检测和识别应用程序。您能提供一些代码示例来实现这一点吗？我的要求是我应该能够使用 OpenCV 检测和识别图像中的文本。
请提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</guid>
      <pubDate>Wed, 20 Oct 2021 13:40:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 ARCore 时有没有办法进行真实的手部检测？</title>
      <link>https://stackoverflow.com/questions/56062416/is-there-a-way-to-do-real-hand-detection-when-using-arcore</link>
      <description><![CDATA[我想在使用 ARCore 时进行一些真实的手部检测，以扩展一些功能。不幸的是，ARCore 不支持它。
那么直到 2019 年 5 月，有没有办法在使用 ARCore 时进行物体检测？
我已经通过 TensorFlow 训练了一个模型，但它似乎无法与 ARCore 协同工作。]]></description>
      <guid>https://stackoverflow.com/questions/56062416/is-there-a-way-to-do-real-hand-detection-when-using-arcore</guid>
      <pubDate>Thu, 09 May 2019 15:05:33 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>真实世界参数优化</title>
      <link>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</link>
      <description><![CDATA[我需要对我的最新研究项目进行参数优化。我有一个算法，目前有 5 个参数（四个双精度 [0,1] 和一个具有 3 个值的名义参数）。该算法使用这些参数来计算一些东西，然后我计算准确率、召回率和 FMeasure。一次运行大约需要 1.8 秒。目前，我正在以 0.1 的步长遍历每个参数，这向我展示了全局最大值的大致位置。但我想找到精确的全局最大值。我研究过梯度下降，但我真的不知道如何将其应用于我的算法（如果可能的话）。有人可以指导我如何实现这样的算法吗，因为我对这类工作很陌生。
干杯，
丹尼尔]]></description>
      <guid>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</guid>
      <pubDate>Sun, 23 Dec 2012 17:55:54 GMT</pubDate>
    </item>
    </channel>
</rss>