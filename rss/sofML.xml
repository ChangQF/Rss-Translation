<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Mar 2024 03:16:01 GMT</lastBuildDate>
    <item>
      <title>如何在 train() 函数内使用重复的随机训练/测试分割集？</title>
      <link>https://stackoverflow.com/questions/78103028/how-to-use-repeated-random-training-test-splits-sets-inside-train-function</link>
      <description><![CDATA[我想使用重复的随机 80%/20% 分割作为训练集，因为我的只有约 800 个人，事件率为 5%。
#示例数据
dd_cleannames = data.frame(class = 样本(c(0,1),100,替换 = TRUE),var1 = 样本(c(1:5),100,替换= TRUE),var2 = 样本(c(10: 20),100,替换=真))

#为5个随机训练集创建数据分区
设置种子(100)
索引 &lt;- 插入符::createDataPartition(dd_cleannames$class, p = 0.8,times = 5,list = FALSE)

在另一个SO线程中，我找到了这个答案：https://stackoverflow.com/a/59276788/4685471&lt; /p&gt;
resample_data &lt;- tibble(
  training_sets = map(indices, ~ dd_cleannames[.x, ]),
  test_sets = map(索引, ~ dd_cleannames[-.x, ])
）

现在我创建我的控件：
ctrl = trainControl(method = &quot;LGOCV&quot;,
                    数量 = 5,
                    p = 0.8，
                    类概率 = TRUE,
                    摘要函数=两个类摘要）

但是当我尝试实现广义提升模型时，出现错误：
gbm = train(class ~ ., data = resample_data$training_sets,
            方法=“gbm”，
            trControl = ctrl,
            详细=假）
terms.formula(公式，数据 = 数据) 中的错误：
  使用“.”在数据框中重复名称“var1”

或者，除了在 createDataPartition 函数中使用 list = TRUE 之外，我尝试了相同的工作流程，但出现以下错误：
set.seed(100)
索引 &lt;- 插入符::createDataPartition(dd_cleannames$class, p = 0.8,times = 5,list = TRUE)

resample_data &lt;- tibble(
  training_sets = map(indices, ~ dd_cleannames[.x, ]),
  test_sets = map(索引, ~ dd_cleannames[-.x, ])
）


ctrl = trainControl(方法 = “LGOCV”,
                    数量 = 5,
                    p = 0.8，
                    类概率 = TRUE,
                    摘要函数=两个类摘要）


gbm = train(类 ~ ., 数据 = resample_data$training_sets,
            方法=“gbm”，
            trControl = ctrl,
            详细=假）
eval(predvars, data, env) 中的错误：未找到对象“Resample1.class”

然后我收到此错误：
eval(predvars, data, env) 中出现错误：未找到对象“Resample1.class”

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78103028/how-to-use-repeated-random-training-test-splits-sets-inside-train-function</guid>
      <pubDate>Mon, 04 Mar 2024 17:16:26 GMT</pubDate>
    </item>
    <item>
      <title>如何训练时间序列模型以包含最新数据[关闭]</title>
      <link>https://stackoverflow.com/questions/78103005/how-to-train-a-timeseries-model-to-include-latest-data</link>
      <description><![CDATA[这是与时间序列 ML/DL 模型相关的一般问题。我有一个模型，可以根据 10 年的历史数据进行训练，并根据 1 年的数据进行预测。编码器/输入块长度为 2 年。
假设我想要预测 2024 年。我将在 2012 年至 2021 年之间对数据进行训练，在 2022 年进行验证（需要提前停止）并在 2023 年进行测试。这意味着我计划在生产中部署的最终模型无法看到2 年的最新数据，在本例中为 2022 年和 2023 年。
有没有办法可以将晚年纳入培训中？在生产中部署的时间序列模型用于说明训练中的最新数据的最佳实践是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78103005/how-to-train-a-timeseries-model-to-include-latest-data</guid>
      <pubDate>Mon, 04 Mar 2024 17:12:56 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 会抛出除 1 之外的任何值的批量大小错误</title>
      <link>https://stackoverflow.com/questions/78102877/pytorch-throws-batch-size-error-with-any-value-but-1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78102877/pytorch-throws-batch-size-error-with-any-value-but-1</guid>
      <pubDate>Mon, 04 Mar 2024 16:51:12 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何使代码匹配以通过文档测试？</title>
      <link>https://stackoverflow.com/questions/78102706/how-should-i-make-the-code-match-up-to-pass-doctests</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78102706/how-should-i-make-the-code-match-up-to-pass-doctests</guid>
      <pubDate>Mon, 04 Mar 2024 16:23:09 GMT</pubDate>
    </item>
    <item>
      <title>如何将 AI 与 GitHub 拉取请求集成？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78102459/how-can-i-integrate-ai-with-github-pull-requests</link>
      <description><![CDATA[我的项目是开发一个人工智能驱动的工具，可以在 GitHub PR 上触发。应该以当前项目的代码库为基础，检查PR中发生变化的行，并在此基础上提出可能的改进代码质量的建议。重要的是，主要目标不是发现一般代码异味，而更像是基于存储库遵循编码约定/模式/建议的建议。
我并不期望有人给我关于如何完成整个项目的确切指示。我正在寻找任何可能已经了解如何解决此问题、哪些工具最适合此用例或有任何建议的知识或想法的人。]]></description>
      <guid>https://stackoverflow.com/questions/78102459/how-can-i-integrate-ai-with-github-pull-requests</guid>
      <pubDate>Mon, 04 Mar 2024 15:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Optuna 中修剪随机森林回归？</title>
      <link>https://stackoverflow.com/questions/78101963/how-to-prune-random-forest-regression-in-optuna</link>
      <description><![CDATA[我正在研究机器学习模型并尝试使用 Optuna 调整超参数。我想尝试修剪，但我不知道如何实现这个功能。我正在使用随机森林回归器，一切正常。
def 目标（试用）：
    n_estimators = Trial.suggest_int(&#39;n_estimators&#39;, 100, 1000)
    最大深度 = Trial.suggest_int(&#39;最大深度&#39;, 5, 50)
    min_samples_split = Trial.suggest_int(&#39;min_samples_split&#39;, 2, 30)
    min_samples_leaf = Trial.suggest_int(&#39;min_samples_leaf&#39;, 1, 10)
    max_samples = Trial.suggest_float(&#39;max_samples&#39;, 0.5, 1.0)
    max_features = Trial.suggest_int(&#39;max_features&#39;, 5, 30)
    max_leaf_nodes = Trial.suggest_int(&#39;max_leaf_nodes&#39;, 100, 200)

    模型 = RandomForestRegressor(n_estimators=n_estimators,
                              最大深度=最大深度，
                              min_samples_split=min_samples_split,
                              min_samples_leaf=min_samples_leaf,
                              最大样本数=最大样本数，
                              最大特征=最大特征，
                              max_leaf_nodes=max_leaf_nodes)

    k 折叠 = K 折叠(n_splits=5)
    分数 = cross_val_score(模型, X_train_transformed, y_train, cv=kFold, 评分=&#39;r2&#39;, n_jobs=-1)
    mean_score = np.mean(分数)

    返回平均值


研究 = optuna.create_study(方向 = &#39;最大化&#39;,
                        采样器=optuna.samplers.TPESampler(multivariate=True))
研究.优化（目标，n_Trials=300）

如何对目标函数实施剪枝？]]></description>
      <guid>https://stackoverflow.com/questions/78101963/how-to-prune-random-forest-regression-in-optuna</guid>
      <pubDate>Mon, 04 Mar 2024 14:28:11 GMT</pubDate>
    </item>
    <item>
      <title>传递矩阵密钥时邻居索引错误</title>
      <link>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-when-passing-matrix-key</link>
      <description><![CDATA[我正在创建一个衣服推荐系统，使用 NearestNeighbors，数据来自 2 个数据集，其中一个带有 ratings.csv，在本例中为 0 和 1，基于是否保存到愿望清单，以及包含所有衣服的衣服.csv，我想要传递服装的 ID 并获取推荐商品的列表，但我收到索引错误。
这是代码：
user_ ratings_df = pd.read_csv(“ ratings.csv”)

user_ ratings_df[&#39;IDGARMENT&#39;] = user_ ratings_df[&#39;IDGARMENT&#39;].astype(int)

# 读入数据；使用默认的 pd.RangeIndex，即 0、1、2 等作为列
Clothes_desc = pd.read_csv(“clothes.csv”, on_bad_lines=&#39;skip&#39;)
Clothing_metadata = Clothing_desc[[&#39;IDGARMENT&#39;, &#39;描述&#39;, &#39;类别&#39;, &#39;品牌&#39;, &#39;价格&#39;]]

衣服元数据[&#39;IDGARMENT&#39;] = 衣服元数据[&#39;IDGARMENT&#39;].astype(int)
Clothes_data = user_ ratings_df.merge(clothes_metadata, on=&#39;IDGARMENT&#39;)

user_item_matrix = user_ ratings_df.pivot(index=[&#39;USERID&#39;], columns=[&#39;IDGARMENT&#39;], value=&#39;RATING&#39;).fillna(0)
用户项矩阵

# 定义一个关于余弦相似度的 KNN 模型
cf_knn_model=NearestNeighbors(metric=&#39;cosine&#39;,algorithm=&#39;brute&#39;,n_neighbors=10,n_jobs=-1)
#lr.fit(x.reshape(-1, 1), y)

# 将模型拟合到我们的矩阵上
cf_knn_model.fit(user_item_matrix)


def dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs):
    # 在矩阵上拟合模型
    cf_knn_model.fit（矩阵）
    
    # 计算邻居距离
    距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
    Clothing_rec_ids = Sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]
    
    # 存储推荐的列表
    cf_recs = []
    对于我在 dress_rec_ids 中：
        cf_recs.append({&#39;Desc&#39;:clothes_desc[&#39;DESCRIPTION&#39;][i[0]],&#39;距离&#39;:i[1]})
    
    # 选择需要的最多推荐数量
    df = pd.DataFrame(cf_recs, 索引 = 范围(1,n_recs))
    返回df


n_recs = 10
dress_recommender_engine（54448，user_item_matrix，cf_knn_model，n_recs）

我得到的错误是：
&lt;前&gt;&lt;代码&gt;&gt; *keyError Traceback（最近一次调用最后）文件
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802，
&gt;在Index.get_loc（self，key，method，tolerance）3801中尝试：
&gt; -&gt; [第 3802 章] 第 3803 章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; pandas/_libs/hashtable_class_helper.pxi:2263，在
&gt; pandas._libs.hashtable.Int64HashTable.get_item() 文件
&gt; pandas/_libs/hashtable_class_helper.pxi:2273，位于
&gt; pandas._libs.hashtable.Int64HashTable.get_item() KeyError：54448
&gt;上述异常是以下异常的直接原因：
&gt; KeyError Traceback（最近调用
&gt;最后）单元格 In[4]，第 64 行
&gt; 59 返回 df
&gt; 63 n_recs = 10
&gt; ---&gt; 64 dress_recommender_engine(54448, user_item_matrix, cf_knn_model, n_recs) 单元格 In[4]，第 48 行，in
&gt; dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs)
&gt; 42 cf_knn_model.fit（矩阵）
&gt; 44 # 提取输入的电影ID
&gt;第45话
&gt; 46
&gt; 47 # 计算邻居距离
&gt; ---&gt; 48 个距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
&gt;第49章 衣服
&gt; x: x[1])[:0:-1]
&gt; 51 # 存储推荐的列表 File ~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807, in
&gt;第3805章1：
&gt;第3806章
&gt; -&gt;第3807章 第3808章 第3809章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804，
&gt;在Index.get_loc（self，key，method，tolerance）3802返回
&gt; self._engine.get_loc(casted_key) 3803 除了 KeyError 为错误：
&gt; -&gt;第3804章 3805，除了TypeError：3806，引发KeyError（key）
&gt;第3807章否则我们会失败并重新加注
&gt;第3808章第3809章
&gt;密钥错误：54448*

我已将 IDGarment 转换为 int，但这似乎导致了某种问题，知道如何解决吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-when-passing-matrix-key</guid>
      <pubDate>Mon, 04 Mar 2024 14:12:06 GMT</pubDate>
    </item>
    <item>
      <title>YOLOV5 - 自定义模型训练 - 字符串到浮点错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78101832/yolov5-custom-model-training-string-to-float-error</link>
      <description><![CDATA[快速解决方案将不胜感激
我想通过yolov5检测7个图像类别。我克隆了 yolov5 并把我的图像、标签和
将classes.txt放入yolov5的数据文件夹中，并制作data.yaml，其中包含有关数据集的基本信息。
我有多个图像及其相应的标签，但我提供示例：
示例 nh00001.txt：
白细胞 0.525625 0.034167 0.031250 0.038333
霉菌 0.146250 0.043333 0.032500 0.050000
红宝石 0.218125 0.043333 0.028750 0.036667
白细胞 0.726875 0.110833 0.033750 0.061667
红宝石 0.500625 0.205000 0.031250 0.040000
霉菌 0.157500 0.324167 0.027500 0.041667
白细胞 0.819375 0.347500 0.031250 0.045000
红宝石 0.702500 0.344167 0.032500 0.035000
霉菌 0.468750 0.394167 0.027500 0.045000
白细胞 0.504375 0.408333 0.036250 0.046667
红宝石 0.962500 0.429167 0.032500 0.048333
白细胞 0.534375 0.433333 0.036250 0.050000
白细胞 0.859375 0.596667 0.046250 0.056667
白细胞 0.630000 0.604167 0.032500 0.041667
白细胞 0.868125 0.689167 0.033750 0.045000
红宝石 0.094375 0.752500 0.028750 0.038333
霉菌0.519375 0.830000 0.026250 0.036667
classes.txt：
投掷
水晶
上皮
上皮
埃里斯
白细胞
霉菌
data.yaml：
train: data\images # 图像目录的路径
val: # 验证图像目录的路径（如果不使用则留空）
nc: 7 # 类数
name: [&#39;cast&#39;, &#39;cryst&#39;, &#39;epith&#39;, &#39;epithn&#39;, &#39;eryth&#39;, &#39;leuko&#39;, &#39;mycete&#39;] # 类名列表
设置数据后我正在运行以下命令：
python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --cfg models/yolov5s.yaml --weights &#39;yolov5s.pt&#39;
它获取yolov5s.pt，但图像中出现此错误：要浮动的字符串
我解释了我在做什么，并给出了我的错误，我希望有人能给出解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78101832/yolov5-custom-model-training-string-to-float-error</guid>
      <pubDate>Mon, 04 Mar 2024 14:08:52 GMT</pubDate>
    </item>
    <item>
      <title>一维 CNN 预测图与实际时间序列图不匹配</title>
      <link>https://stackoverflow.com/questions/78100096/1d-cnn-predictions-plot-mismatch-with-actual-time-series-plot</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78100096/1d-cnn-predictions-plot-mismatch-with-actual-time-series-plot</guid>
      <pubDate>Mon, 04 Mar 2024 09:11:20 GMT</pubDate>
    </item>
    <item>
      <title>SVM损失函数实现中如何更新梯度？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78092500/how-to-update-the-gradient-in-svm-loss-function-implementation</link>
      <description><![CDATA[我有这部分代码，试图为 svm 模型实现损失函数：
来自内置导入范围
将 numpy 导入为 np
从随机导入随机播放
从过去的内置导入 xrange


def svm_loss_naive(W, X, y, reg):
    ”“”
    结构化 SVM 损失函数，简单实现（带循环）。

    输入具有 D 维，有 C 类，我们对小批量进行操作
    N 个例子。

    输入：
    - W：包含权重的形状（D，C）的numpy数组。
    - X：形状 (N, D) 的 numpy 数组，包含小批量数据。
    - y：包含训练标签的形状 (N,) 的 numpy 数组； y[i] = c 意味着
      X[i] 具有标签 c，其中 0 &lt;= c &lt; C。
    - reg：（浮点数）正则化强度

    返回一个元组：
    - 单个浮点损失
    - 相对于权重 W 的梯度；与 W 形状相同的数组
    ”“”
    dW = np.zeros(W.shape) # 将梯度初始化为零

    # 计算损失和梯度
    num_classes = W.shape[1]
    num_train = X.shape[0]
    损失 = 0.0
    对于范围内的 i（num_train）：
        分数 = X[i].dot(W)
        Correct_class_score = 分数[y[i]]
        对于范围内的 j（num_classes）：
            如果 j == y[i]：
                继续
            margin = Scores[j] - Correct_class_score + 1 # 注意 delta = 1
            如果保证金&gt; 0:
                损失+=保证金
                **dW[:, j] += X[i]
                dW[:, y[i]] -= X[i]**

    # 现在损失是所有训练样本的总和，但我们想要它
    # 取平均值，所以我们除以 num_train。
    损失 /= num_train

    # 为损失添加正则化。
    loss += reg * np.sum(W * W) # reg 是 &#39;lambda&#39; 或 1/c，sum(w*w) 是 L2 范数
    

    dW /= num_train # 缩放梯度 ovr 样本数
    dW += 2 * reg * W


   回波损耗，dW

我不太明白这两行：
&lt;前&gt;&lt;代码&gt; dW[:, j] += X[i]
     dW[:, y[i]] -= X[i]

为什么我们要采用这种方式更新梯度项？我没有看到与数学理论和术语的关系。我确信解释非常简单，但我确实还没有看到它。 （我了解这些代码行的作用，但不知道它们为何存在）。
它运行良好，但我想更好地理解它。
有人在某处发布了此内容：
&lt;块引用&gt;
我没有足够的声誉来发表评论，所以我在这里回答。每当您计算第 i 个训练示例的 x[i] 的损失向量并获得一些非零损失时，这意味着您应该将错误类别 (j != y[i]) 的权重向量移开 x[i]，并在同时，将权重或超平面移动到 x[i] 附近的正确类别 (j==y[i])。根据平行四边形定律，w + x 位于 w 和 x 之间。因此，每次发现损失&gt;0时，w[y[i]]都会尝试接近x[i]。

因此，dW[:,y[i]] += -X[i] 和 dW[:,j] += X[i] 是在循环中完成的，但是在更新时，我们将按照梯度下降，因此我们本质上是添加 X[i] 来纠正类别权重，并通过 X[i] 消除未分类的权重。
但我仍然想看看 svm 损失函数计算背后的数学术语和理论的实际联系。上面提到的解释对我来说还不够澄清，它可能还是太抽象了。]]></description>
      <guid>https://stackoverflow.com/questions/78092500/how-to-update-the-gradient-in-svm-loss-function-implementation</guid>
      <pubDate>Sat, 02 Mar 2024 11:36:47 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 中二元分类目标的精确定义</title>
      <link>https://stackoverflow.com/questions/78064980/exact-definition-of-the-binary-classifctation-objective-in-xgboost</link>
      <description><![CDATA[在 XGBoost 的源代码中，我可以在哪里找到二元逻辑分类问题（对数损失）的目标函数的确切拼写和定义，包括对股票附带的超参数（例如scale_pos_weight）的所有显式引用XGBoost 版本？]]></description>
      <guid>https://stackoverflow.com/questions/78064980/exact-definition-of-the-binary-classifctation-objective-in-xgboost</guid>
      <pubDate>Tue, 27 Feb 2024 03:19:57 GMT</pubDate>
    </item>
    <item>
      <title>如何得到PLS分类的预测概率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78061203/how-to-get-predicted-probabilities-of-pls-classification</link>
      <description><![CDATA[我正在尝试使用不同的分类器（SVC、RandomForest、PLS）对数据进行二元分类，并绘制模型的 ROC 和 PR 曲线（与 这个）之后。
我的 ROC 和 PR 曲线代码适用于除 PLS 之外的每个分类器（SVC、RandomForest）。问题在于 PLSRegression 缺少 .predict_proba，所以我认为使用 CaliberatedClassifierCV 可以解决这个问题。但是，要么我又做错了什么，要么我将 CalibatedClassifierCV 与 PLSRegression 一起使用的意图是错误的。
是否可以将 CalibrateClassifiedCV 与 PLS 结合使用来为我提供 PLS 分类模型的预测概率，以便我可以绘制所需的曲线？或者还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78061203/how-to-get-predicted-probabilities-of-pls-classification</guid>
      <pubDate>Mon, 26 Feb 2024 13:17:56 GMT</pubDate>
    </item>
    <item>
      <title>检测视频中的手部方向（旋转）</title>
      <link>https://stackoverflow.com/questions/75908800/detect-hand-orientation-rotation-in-a-video</link>
      <description><![CDATA[我想检测视频中我的手的旋转，但我仍然不知道如何正确地做到这一点。
我尝试使用 PCA 方法，但它仅适用于图像，不适用于视频。
我可以正确检测到手及其地标]]></description>
      <guid>https://stackoverflow.com/questions/75908800/detect-hand-orientation-rotation-in-a-video</guid>
      <pubDate>Sat, 01 Apr 2023 20:54:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 询问脑电图分类的建议</title>
      <link>https://stackoverflow.com/questions/67236791/asking-advice-on-eeg-classification-using-keras</link>
      <description><![CDATA[我有一个脑电图数据集，形状如下：
&lt;前&gt;&lt;代码&gt;(11,1158, 200)

哪里
11为EEG通道数
1158是每个任务的编号
200是每个任务的时间间隔

例如，如果您绘制一个任务，您将得到（请注意，数据已标准化）：

该任务代表一个带有类的任务。 （例如，查看第 2 类的图片，我的数据集中的类总数为 5）。
现在我将数组转换为这种形状：
&lt;前&gt;&lt;代码&gt;(1158, 200, 11)

以便模型能够区分每个任务。这是我使用的模型：
opt = keras.optimizers.Adam(learning_rate=1e-4)

模型=顺序（）
model.add(Conv1D(filters=128, kernel_size=64,activation=&#39;relu&#39;, input_shape=(200, 11)))
model.add(Conv1D(filters=64, kernel_size=8,activation=&#39;relu&#39;))
模型.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))
模型.add(压平())
model.add（密集（100，激活=&#39;relu&#39;））
model.add（密集（5，激活=&#39;softmax&#39;））
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=opt, 指标=[&#39;accuracy&#39;])
model.fit（x_train，y_train，validation_data =（x_valid，y_valid），epochs = 50，batch_size = 16）

我尝试了许多不同的超参数，但我所有的结果都有点像这样：
纪元 50/50
58/58 [==============================] - 0s 5ms/步 - 损失：0.1281 - 准确度：0.9946 - val_loss ：2.7850 - val_accuracy：0.1897

训练准确率很高，但验证准确率在 20% 到 25% 之间（100/5 = 20，其中 5 是类数）；这基本上意味着模型预测随机的东西。我的做法有错吗？如果是这样，我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/67236791/asking-advice-on-eeg-classification-using-keras</guid>
      <pubDate>Fri, 23 Apr 2021 21:01:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn OneHotEncoder 时如何处理分类数据中的缺失值 (NaN)？</title>
      <link>https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o</link>
      <description><![CDATA[我最近开始学习Python，以便使用机器学习方法为一个研究项目开发预测模型。我有一个由数值数据和分类数据组成的大型数据集。数据集有很多缺失值。我目前正在尝试使用 OneHotEncoder 对分类特征进行编码。当我阅读有关 OneHotEncoder 的内容时，我的理解是，对于缺失值 (NaN)，OneHotEncoder 会将 0 分配给所有功能的类别，如下所示：

&lt;前&gt;&lt;代码&gt;0 男
1 女
2 南

应用 OneHotEncoder 后：

&lt;前&gt;&lt;代码&gt;0 10
1 01
2 00

但是，当运行以下代码时：
 # 编码分类数据
    从 sklearn.compose 导入 ColumnTransformer
    从 sklearn.preprocessing 导入 OneHotEncoder


    ct = ColumnTransformer([(&#39;编码器&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), [1])],
                           余数=&#39;直通&#39;）
    obj_df = np.array(ct.fit_transform(obj_df))
    打印（obj_df）


我收到错误ValueError：输入包含NaN
所以我猜测我之前对 OneHotEncoder 如何处理缺失值的理解是错误的。
有没有办法让我获得上述功能？我知道在编码之前估算缺失值可以解决这个问题，但我不愿意这样做，因为我正在处理医疗数据，并且担心估算可能会降低模型的预测准确性。 
我发现这个问题类似，但答案没有提供关于如何处理 NaN 值的足够详细的解决方案。
请告诉我您的想法，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o</guid>
      <pubDate>Tue, 16 Jun 2020 13:09:16 GMT</pubDate>
    </item>
    </channel>
</rss>