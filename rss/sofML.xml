<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 20 Mar 2025 21:16:20 GMT</lastBuildDate>
    <item>
      <title>我可以在iPhone上实现离线面部识别以识别孩子吗？</title>
      <link>https://stackoverflow.com/questions/79524096/can-i-implement-offline-face-recognition-on-iphone-for-identifying-kids</link>
      <description><![CDATA[我正在研究 flutter的iOS应用程序，该应用程序使用离线面部识别在周日校车期间跟踪孩子。目的是用脸识别孩子。这是计划的工作流程：

  到达星期日学校：给每个孩子的照片。

如果已识别→标记为存在。
如果不确定→提出类似的手动确认匹配。
如果新→创建一个新的配置文件。


  回家旅行：扫描每个孩子并与存储的数据进行比较。

生成失踪的孩子  
创建一个订购的下车列表，随着孩子的掉落，它会实时更新。


  持续学习：每周每周将两张照片存储（到达＆amp;出发）提高识别精度随着时间的流逝。


问题：

我可以在 iOS设备上完全离线实现高质量的面部识别（iPhone 11）？
什么 flutter软件包或核心ML模型我应该研究吗？
 是否会在设备培训（每周更新面部数据集）是可行的吗？

任何见解，推荐框架或方法都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79524096/can-i-implement-offline-face-recognition-on-iphone-for-identifying-kids</guid>
      <pubDate>Thu, 20 Mar 2025 21:14:13 GMT</pubDate>
    </item>
    <item>
      <title>如何从Pytorch中的自定义任意损失功能更新模型权重？</title>
      <link>https://stackoverflow.com/questions/79523864/how-to-update-model-weights-from-a-custom-arbitrary-loss-function-in-pytorch</link>
      <description><![CDATA[从最后开始：

无论如何，我可以通过调用一些pytorch函数来编写任意损失功能，从而保留自动射击图？
我如何确保我的损失功能是“有线的”到自动射击？
如果无法做到，我该如何将损失分数倒退？


嗨，我正在Pytorch创建一个神经网络，我需要创建一个任意损失功能。
我想训练一个能够预测没有已知标签的输出的模型。我并不真正在乎实际输出，而是对此语义。换句话说，我想验证输出“有意义”。因此，我决定设计自己的损失功能。
  input_data 是（M，4）张量， output_data 是（M，M） tensor。
然后，我从 outpe&gt; outption_data 和 m 和 m 和 pre_input_data 中构造 m 的UUID列表。
“预输入”数据是一个简短的JSON对象，其中有几个字段，例如UUID。
如果该模型预测了一个“距离之外”的价值。相应的输出数据等于无。
有效性函数仅计算出界外结果的总和并返回。
我的问题是损耗功能断开了自动摩rad，我的模型的权重根本不更新。

 ＃velitiesloss被计算为```&#39;&#39;&#39;结果的总和。
class Vivelitiesloss（nn.模块）：
    def __init __（自我）：
        super（valiesityloss，self）.__ INIT __（）

    def向前（
        自己， 
        输出：（列表[输出]，列表[输出]），
    ）：
        bef，aft =输出
        
        损失= self .__ compute_loss（bef.output）
        损失 += self .__ compute_loss（aft.output）

        返回TORCH.TENSOR（float（lose），需要_grad = true）

    def __compute_loss（self，x） - ＆gt; INT：
        损失= 0
        对于x中的项目：
            如果项目没有：
               损失 += 1
        回报损失
 ]]></description>
      <guid>https://stackoverflow.com/questions/79523864/how-to-update-model-weights-from-a-custom-arbitrary-loss-function-in-pytorch</guid>
      <pubDate>Thu, 20 Mar 2025 18:53:43 GMT</pubDate>
    </item>
    <item>
      <title>使用Essentia模型进行音乐标记的问题</title>
      <link>https://stackoverflow.com/questions/79523823/issues-using-essentia-models-for-music-tagging</link>
      <description><![CDATA[ 背格： 
我正在使用一些模型来生成音乐的标签，例如音乐中的流派，情绪和乐器（音频文件）。原始型号在.pb扩展中。这些模型可在 https://essentia.upf.edu/models.html 和我使用的型号at is： ars as：

 discogs-effnet-bs64-1 
 genre_discogs400-discogs-effnet-1 
 mtg_jamendo_instrument-discogs-effnet-1 
 mtg_jamendo_moodtheme-discogs-effnet-1 

在相应的JSON文件中给出了模型的输入和输出，这些文件显示了类，输入/输出大小和名称。
默认.pb模型只需使用内置函数：
 来自Essentia.Standard Import（
    单层，
    TensorFlowPredictefteffnetDiscogs，
    TensorFlowPredict2d，
）
def essentia_feature_extraction（audio_file，sample_rate）：
    ＃加载音频文件
    audio = monoloader（fileName = audio_file，采样= 16000，res ampamplequality = 4）（）（）

    ＃嵌入音频功能
    embeddings = embedding_model（音频）

    result_dict = {}
    processed_labels = list（MAP（process_labels，genre_labels）））
    ＃流派预测
    genre_predictions = genre_model（嵌入）
    result_dict [＆quot; quot; quot; quot_predictions（genre_predictions，processed_labels）
    ＃情绪/主题预测
    mood_predictions = mood_model（嵌入）
    result_dict [＆quot; quots; quot; quot_predictions（
        mood_predictions，mood_theme_classes，阈值= 0.05
    ）

    ＃乐器预测
    instrument_predictions = instrument_model（嵌入）
    result_dict [＆quot&#39;instruments＆quort＆quort = filter_predictions（
        instrument_predictions，instrument_classes
    ）

    返回result_dict
 
 问题： 
无论我用哪种音频文件作为输入，我都会为情绪和仪器提供相同的输出预测。流派预测现在通常为零（含义“未知类型”。
 导入libreosa
导入numpy作为NP
导入tritonclient.http作为httpclient

def essentia_feature_extraction_triton（audio_file，sample_rate）：
    尝试：
        音频，sr = librosa.load（audio_file，sr = 16000，mono = true）
        音频= audio.astype（np.float32）

        mel_spectrogram = libreosa.feature.melspectrogram（
            y =音频，sr = 16000，n_fft = 2048，hop_length = 512，n_mels = 128
        ）
        mel_spectrogram = libreosa.power_to_db（mel_spectrogram，ref = 1.0）

        如果mel_spectRogram.Shape [1]＆lt; 96：
            mel_spectrogram = np.pad（
                mel_spectRogram，（（（0，0），（0，96 -mel_spectRogram.shape [1]）），模式=＆quot;
            ）
        elif mel_spectrogram.shape [1]＆gt; 96：
            mel_spectRogram = mel_spectrogram [：，：96]

        mel_spectRogram = np.expand_dims（mel_spectRogram，axis = 0）.astype（np.float32）


        使用httpclient.inferenceserverclient（url = triton_url）作为triton_client：
            ＃--- effnet Discogs（组合模型）---
            input_name =＆quot; melspectrogram; quot;
            genre_output_name =“激活”
            embedding_output_name =＆quot&#39;embeddings; quot;

            inputs = [httpclient.inferinput（input_name，mel_spectrogram.shape，shape，shape，fp32＆quort&#39;&#39;）]
            输入[0] .SET_DATA_FROM_NUMPY（MEL_SPECTROGRAM）

            输出= [
                httpclient.inferrequestedoutput（genre_output_name），
                httpclient.inferrequestedoutput（embedding_output_name）
            这是给出的

            结果= triton_client.infer（
                model_name = effnet_discogs_model_name，inputs =输入，输出=输出
            ）

            genre_predictions = results.as_numpy（genre_output_name）
            embeddings = results.as_numpy（empedding_output_name）
            embeddings = embeddings.astype（np.float32）

            ＃---情绪预测---
            input_name =＆quot&#39;embeddings;
            output_name =“激活”
            inputs = [httpclient.inferinput（input_name，embeddings.shape，shape，fp32＆quort&#39;）]
            输入[0] .set_data_from_numpy（嵌入）

            outputs = [httpclient.inferrequestedoutput（output_name）]
            mood_predictions = triton_client.infer（
                model_name = mood_model_name，inputs =输入，输出=输出
            ）.AS_NUMPY（output_name）

            ＃---仪器预测---
            input_name =＆quot&#39;embeddings;
            output_name =“激活”
            inputs = [httpclient.inferinput（input_name，embeddings.shape，shape，fp32＆quort&#39;）]
            输入[0] .set_data_from_numpy（嵌入）

            outputs = [httpclient.inferrequestedoutput（output_name）]
            instrument_predictions = triton_client.infer（
                model_name = instrument_model_name，inputs =输入，输出=输出
            ）.AS_NUMPY（output_name）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79523823/issues-using-essentia-models-for-music-tagging</guid>
      <pubDate>Thu, 20 Mar 2025 18:31:24 GMT</pubDate>
    </item>
    <item>
      <title>DL4J自动编码器用于异常检测：意外结果</title>
      <link>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</guid>
      <pubDate>Thu, 20 Mar 2025 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Codebert嵌入识别类似的代码零件？</title>
      <link>https://stackoverflow.com/questions/79523261/how-to-identify-similar-code-parts-using-codebert-embeddings</link>
      <description><![CDATA[我正在使用Codebert比较两个代码的相似性。例如：
 ＃代码1
def calculate_area（半径）：
返回3.14 *半径 *半径
 
 ＃代码2
def Compute_circle_area（R）：
返回3.14159 * r * r
 
 Codebert创建“嵌入”就像对代码的详细描述为数字。然后，我比较这些数值描述，以查看代码的相似之处。这对于告诉我多少代码是相似的。
但是，我无法分辨Codebert认为哪些部分相似。因为“嵌入”很复杂，我无法轻易看到Codebert的重点。比较逐字代码在这里不起作用。
我的问题是：我如何找出两个代码段的哪些特定部分Codebert认为相似，而不仅仅是获得一般相似性得分？
我尝试了简单的DIFF方法，但这违反了纯粹使用Codebert的目的。
我想知道是否可以单独使用Codebert。]]></description>
      <guid>https://stackoverflow.com/questions/79523261/how-to-identify-similar-code-parts-using-codebert-embeddings</guid>
      <pubDate>Thu, 20 Mar 2025 14:30:35 GMT</pubDate>
    </item>
    <item>
      <title>用剪贴图像嵌入[闭合]重建图像</title>
      <link>https://stackoverflow.com/questions/79523091/reconstruct-images-with-clip-image-embedding</link>
      <description><![CDATA[ i最近开始从事一个仅使用图像嵌入语义知识的项目，该项目是从基于夹的模型（例如siglip）编码的图像嵌入的，以重建语义上相似的图像。
为此，我使用了基于MLP的投影仪将夹具嵌入将夹具嵌入到图像编码器的潜在空间中，从扩散模型中，我学会了MSE损失以使预测的潜在向量对齐。然后，我尝试使用从扩散模型管道中的VAE解码器进行解码。但是，图像的输出很模糊，并且丢失了图像的许多细节。
到目前为止，我尝试了以下解决方案，但它们都没有起作用：

拥有一个更深的投影仪，带有更大的隐藏型昏暗以涵盖信息。
尝试最大平均差异（MMD）损失
尝试感知损失
尝试使用更高的图像质量（更高图像解决方案）
尝试使用余弦相似性损失（比较真实/合成图像之间）
尝试使用其他图像编码器/解码器（例如VQ-GAN）

我目前坚持这个重建步骤，有人可以从中分享一些见解吗？
例子：
   ]]></description>
      <guid>https://stackoverflow.com/questions/79523091/reconstruct-images-with-clip-image-embedding</guid>
      <pubDate>Thu, 20 Mar 2025 13:25:13 GMT</pubDate>
    </item>
    <item>
      <title>添加时间序列记录到张板</title>
      <link>https://stackoverflow.com/questions/79522949/adding-time-series-logging-to-tensorboard</link>
      <description><![CDATA[ Tensorboard提供访问绘图“ nofollow noreferrer”&gt;标量，直方图和图像。我试图将模型预测作为简单的向量添加到张板中。目前，我正在通过添加Matplotlib图像来实现这一目标，该图像非常适合视觉检查，但是在模型训练后不可能推断预测跟踪。
如建议在这里” ，有一些方法可以将痕迹表示为张量板中的标量。慢。
在张板中，基于向量的日志记录还有其他方法吗？并且可以推荐哪些其他方法出于此目的？
我最好的选择是简单地将它们保存为 .npy 数组，但是当读取运行计算时，除了 .tsevents 文件外，我最终还会有多个文件。所以我只是好奇人们可以推荐什么。]]></description>
      <guid>https://stackoverflow.com/questions/79522949/adding-time-series-logging-to-tensorboard</guid>
      <pubDate>Thu, 20 Mar 2025 12:33:41 GMT</pubDate>
    </item>
    <item>
      <title>torch_mlir.com当前是官方Pytorch还是Torch-Mlir API的一部分？</title>
      <link>https://stackoverflow.com/questions/79522928/is-torch-mlir-compile-currently-part-of-the-official-pytorch-or-torch-mlir-api</link>
      <description><![CDATA[我已经看到了 torch_mlir.compile 来自几种AI工具的引用，尤其是来自chatgpt，deepseek等的引用。我还根据方案附加了示例代码。
 导入火炬
导入TORCH_MLIR

＃定义一个简单的pytorch模型
类SimpleModel（Torch.nn.Module）：
    def __init __（自我）：
        超级（SimpleModel，self）.__ Init __（）
        self.linear = torch.nn.linear（3，2）

    def向前（self，x）：
        返回self.linear（x）

＃创建模型的实例
model = SimpleModel（）
model.eval（）＃将模型设置为评估模式

＃创建示例输入张量
example_input = torch.randn（1，3）

＃使用火炬 - 摩尔编译模型
mlir_module = torch_mlir.compile（型号，（example_input，），output_type =; torch; quot;）

＃打印MLIR输出
打印（mlir_module）
 
是  torch_mlir.compile 在Pytorch或Torch-Mlir的最新版本中的正式支持功能，还是过时或更名？
我感谢基于API的当前状态的明确答案。]]></description>
      <guid>https://stackoverflow.com/questions/79522928/is-torch-mlir-compile-currently-part-of-the-official-pytorch-or-torch-mlir-api</guid>
      <pubDate>Thu, 20 Mar 2025 12:25:00 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow联合（TFF）中聚合的客户端选择</title>
      <link>https://stackoverflow.com/questions/79522886/client-selection-for-aggregation-in-tensorflow-federated-tff</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79522886/client-selection-for-aggregation-in-tensorflow-federated-tff</guid>
      <pubDate>Thu, 20 Mar 2025 12:12:07 GMT</pubDate>
    </item>
    <item>
      <title>深XDE模型拟合[封闭]</title>
      <link>https://stackoverflow.com/questions/79522731/deep-xde-model-fit</link>
      <description><![CDATA[我正在尝试使用DEEPXDE在合成数据上训练物理信息的神经网络（PINN）。我的目标是使用简单的葡萄糖模型生成数据，然后在此数据上训练PINN，看看它是否可以恢复模型中使用的参数。
但是，当我训练模型时，拟合很糟糕。这些预测与数据不太匹配，并且损失不如预期。
这是我到目前为止尝试的：

我使用已知的参数生成了合成数据。
我在deepxde中定义了管理方程。
我同时使用了数据丢失和物理损失来训练Pinn。

在使用deepxde为Pinns时是否有人遇到过类似的问题？是否有特定的超参数或培训策略可以有助于改善合适？
 导入DeepXde为DDE
导入numpy作为NP
导入matplotlib.pyplot作为PLT
来自Scipy.Comtegrate Import Odeint
来自Sklearn.metrics导入R2_Score

def单元格（y，t，p）：
    dn = p [0]  -  p [1] *（1 +（y [1] / p [2]）** p [3]） * y [0]
    dc = p [1] *（1 +（y [1] / p [2]）** p [3]） * y [0] -p [4] * y [1]
    返回[DN，DC]

＃仿真参数
times = np.Arange（0，15，0.01）
V，K1，K，N，K2 = 14，1，2，3，4

＃生成合成数据
y = odeint（cell，t = times，y0 = [0，0]，args =（（v，k1，k，n，k2），rtol = 1e-8）

def glycolysy_ode（x，y）：
    y1，y2 = y [：，0：1]，y [：，1：]
    dy1_t = dde.grad.jacobian（y，x，i = 0）
    dy2_t = dde.grad.jacobian（y，x，i = 1）
    
    dn = v -k1 *（1 +（y2 / k）** n） * y1
    dc = k1 *（1 +（y2 / k）** n） * y1 -k2 * y2
    
    返回[DY1_T -DN，DY2_T -DC]

def边界（_，on_initial）：
    返回on_initial

def true_solution（x）：
    idx = np.searchsorted（times，x.flatten（））
    idx = np.clip（idx，0，len（times）-1）＃确保指数保持在范围内
    返回np.hstack（（y [idx，0：1]，y [idx，1：]））

＃直接使用综合数据而无需噪声
noisy_y = y＃没有噪音

geom = dde.Deometry.timedomain（0，15）
ic1 = dde.icbc.ic（Geom，Lambda X：0，边界，组件= 0）
IC2 = dde.icbc.ic（Geom，Lambda X：0，边界，组件= 1）
data = dde.data.pde（Geom，Glycolysis_ode，[IC1，IC2]，35，2，solution = true_solution，num_test = 1500）

layer_size = [1] + [50] * 3 + [2]
激活=“ tanh”
initializer =; glorot制服
net = dde.nn.fnn（layer_size，activation，initializer）

型号= dde.model（数据，网络）
model.compile（&#39;adam＆quort＆quort＆lr = 0.001，量表= [l2相对错误＆quot;]）
损失史，train_state = model.train（迭代= 20000）

＃计算模型的R²得分没有噪音
y_pred = model.predict（times.reshape（-1，1））
r2_y1 = r2_score（noisy_y [：，0]，y_pred [：，0]）
r2_y2 = r2_score（noisy_y [：，1]，y_pred [：，1]）

打印（f6p的f＆quot&#39;r²得分：{r2_y1：.4f}＆quot;）
打印（f16bp的f＆quot&#39;r²得分：{r2_y2：.4f}＆quot;）

＃绘制F6P和F16BP的预测与真实数据
plt.figure（无花果=（10，6））
plt.plot（times，noisy_y [：，0]，&#39;r&#39;，label =&#39;观察到$ f6p（t）$&#39;，lineWidth = 4.0）
plt.plot（times，y_pred [：，0]，&#39;k-&#39;，label =&#39;pinn型号$ f6p（t）$&#39;）
plt.plot（times，noisy_y [：，1]，&#39;b&#39;，label =&#39;观察到$ f16bp（t）$&#39;，lineWidth = 4.0）
plt.plot（times，y_pred [：，1]，&#39;k-&#39;，label =&#39;pinn型号$ f16bp（t）$&#39;）
plt.xlabel（“时间”）
plt.ylabel（“集中度”）
plt.legend（）
plt.show（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79522731/deep-xde-model-fit</guid>
      <pubDate>Thu, 20 Mar 2025 11:07:49 GMT</pubDate>
    </item>
    <item>
      <title>从Edge Export TFJS模型，Express/Node API中获取对象检测结果</title>
      <link>https://stackoverflow.com/questions/79521051/get-object-detection-results-from-edge-export-tfjs-model-bin-dict-in-express</link>
      <description><![CDATA[我已经将我的vertexai模型导出到tfjs as&#39;edge;

 dict.txt 
 group1_shard1of2.bin 
 group1_shard2of2.bin 
 model.json 

现在，我将图像从客户发送到节点/快递端点，这确实很难弄清楚 - 因为我发现TFJS文档很糟糕，可以理解我需要做什么。  但这是我所拥有的：
 &#39;@tensorflow/tfjs-node;：＆quot&#39;^4.22.0＆quot;
＆quot@types/multer＆quot;：＆quot^1.4.12＆quot;
＆quot“ multer”：&#39;^1.4.5-lts.1＆quort;
 
，然后在我的端点处理程序中进行图像＆amp;型号：
 
const upload = multer（{{
  存储：MemoryStorage（），
  限制：{
    文件大小：10 * 1024 * 1024，// 10MB限制
  }，，
}）。单个（&#39;image&#39;）;

//加载字典文件
const loadDictionary =（）=＆gt; {
  const dictpath = path.join（__ dirname，&#39;model&#39;，&#39;dict_03192025.txt&#39;）;
  const content = fs.ReadFileSync（dictpath，&#39;utf-8&#39;）;
  return content.split（&#39;\ n&#39;）。filter（line =＆gt; line.trim（）！==&#39;&#39;&#39;）;
};

const getToppredictions =（
  预测：数字[]，
  标签：字符串[]，
  topk = 5
）=＆gt; {
  //获取按概率排序的索引
  const索引=预测
    .map（（（_，i）=＆gt; i）
    。

  //以其概率获得顶级K预测
  return indices.slice（0，topk）.map（index =＆gt;（{{
    标签：标签[索引]，
    概率：预测[索引]，
  }））;
};

导出const扫描= async（req：request，res：response）=＆gt; {
  上载（req as not，res as any，async err =＆gt; {
    如果（err）{
      返回res.status（400）.send（{消息：err.message}）;
    }

    const file =（req as noy）.file as express.multer.file;

    如果（！文件||！file.buffer）{
      返回res.status（400）.send（{消息：&#39;没有图像文件提供&#39;}）;
    }

    尝试 {
      //加载字典
      const labels = loadDictionary（）;

      //加载JSON格式的模型
      const模型=等待tf.loadgraphmodel（
        &#39;file：//&#39; + __dirname +&#39;/model/model_03192025.json&#39;
      ）；

      //处理图像
      const image = tf.node.decodeimage（file.Buffer，3，&#39;int32&#39;）;
      const尺寸= tf.image.ResizeBilesBileNear（图像，[512，512]）;
      const rangure image = justized.div（255.0）;
      const batchedimage = normolizedImage.expandDims（0）;
      const预测=等待model.executeasync（batchedimage）;

      //提取预测数据并获得最佳匹配
      const预测= array.isarray（预测）
        ？等待（预测[0]作为tf.tensor）.array（）
        ：等待（作为tf.tensor的预测）.array（）;

      const flatpredictions =（预测为number [] []）。flat（）;
      const toppredictions = getToppredictions（flatpredictions，labels）;

      //清理张量
      image.dispose（）;
      调整大小。dispose（）;
      归一化图。dispose（）;
      batchedimage.dispose（）;
      if（array.isarray（predivions））{
        prective.foreach（p =＆gt;（p as tf.tensor）.dispose（））;
      } 别的 {
        （作为tf.tensor的预测）.dispose（）;
      }

      返回res.status（200）。
        消息：“成功处理的图像”，
        尺寸：file.size，
        类型：file.mimetype，
        预测：预测，
      }）;
    } catch（错误）{
      Console.Error（&#39;错误处理图像：&#39;，错误）;
      返回res.status（500）.send（{消息：&#39;错误处理image&#39;}）;
    }
  }）;
};

//包装器功能处理类型铸造
导出const scanhandler = [
  上传，
  （req：request，res：reverse）=＆gt;扫描（req，res），
]作为const;
 
这是我关注的内容：

我是否正确加载模型为 GraphModel ？我尝试了其他人，这是唯一起作用的。
我正在调整大小为512x512好吗？
如何更好地处理结果？  如果我想要最高的“评分”图像，最好的方法是什么？
]]></description>
      <guid>https://stackoverflow.com/questions/79521051/get-object-detection-results-from-edge-export-tfjs-model-bin-dict-in-express</guid>
      <pubDate>Wed, 19 Mar 2025 18:11:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的Llama 3.1模型在Automodelforcausallm和Llamaforcausallm之间的作用有所不同？</title>
      <link>https://stackoverflow.com/questions/79494100/why-does-my-llama-3-1-model-act-differently-between-automodelforcausallm-and-lla</link>
      <description><![CDATA[我有一组权重，一个令牌，相同的提示和相同的生成参数。然而，某种程度上，当我使用AutoModelForCausAllm加载模型时，我将获得一个输出，当我使用LlamaForCausAllm和同一config和state_dict手动构造它时，我完全得到了另一个输出。&gt; 
此代码可以显示A6000和A100的区别。
 导入火炬
从变形金刚导入（
    自动传动器，
    AutomodelForCausAllm，
    Llamaforcausallm，
    Llamaconfig
）

＃1）根据需要调整这些
model_name =＆quot; meta-llama/llama-3.1-8b; quot
提示=&#39;Llama 3.1的Hello！告诉我一些有趣的东西。”
dtype = Torch.float16＃或Torch.float32如果需要

＃2）获取令牌
tokenizer = autotokenizer.from_pretaining（model_name，use_fast = false）

＃准备输入
inputs = tokenizer（提示，return_tensors =; pt;）。

##################################
＃a）加载AutomodelForCausAllm
##################################

打印（===加载AutomodelforCausAllm ===;）

model_auto = automodelforcausallm.from_pretaining（
    model_name，
    attn_implementation =;急切＃＃匹配您的用法
    TORCH_DTYPE = dtype
）.cuda（）
model_auto.eval（）＃关闭辍学
config = model_auto.config
使用Torch.no_grad（）：
    out_auto = model_auto（**输入）
logits_auto = out_auto.logits＃shape：[batch_size，seq_len，vocab_size]

del model_auto
TORCH.CUDA.EMPTY_CACHE（）

##################################
＃b）带有Llamaforcausallm +配置加载
##################################

打印（＆quot; ===加载llamaforcausallm + config ====;）

＃从同一检查点获取配置
＃直接构建骆驼模型
model_llama = llamaforcausallm（config）.cuda（）（）
model_llama.eval（）

＃加载与使用AutomodelForCausAllm使用的相同权重
model_auto_temp = automodelforcausallm.from_pretrateing（model_name，torch_dtype = dtype）
model_llama.load_state_dict（model_auto_temp.state_dict（））
del model_auto_temp
TORCH.CUDA.EMPTY_CACHE（）

使用Torch.no_grad（）：
    out_llama = model_llama（**输入）
logits_llama = out_llama.logits

##################################
＃c）比较逻辑
##################################

＃计算最大绝对差异
max_diff =（logits_auto -logits_llama）.abs（）。max（）
print（f＆quot \ nmax logits之间的绝对差异：{max_diff.item（）}; quot;）

如果max_diff＆lt; 1E-7：
    打印（“→逻辑有效相同）（在浮点精度内）。
别的：
    print（“→logits存在非平凡的差异！”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79494100/why-does-my-llama-3-1-model-act-differently-between-automodelforcausallm-and-lla</guid>
      <pubDate>Sat, 08 Mar 2025 08:24:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使Tesseract识别连字符或减去标志？</title>
      <link>https://stackoverflow.com/questions/69977249/how-to-make-tesseract-recognize-hyphens-or-minus-signs</link>
      <description><![CDATA[我想使用Tesseract阅读毕业文凭的成绩。这些位于桌子般的结构中，您可以在这里看到： https://www.asv.bayern.de/doku/gy/oberstufe/zeugnis/abitur  
由于Tesseract实际上并非设计用于读取表，因此我将表格分为单个列，让Tesseract读取这些值，然后将列重新合并在一起。然后，我也可以将它们转移到tibble中。我在这里找到了这个想法： https://themockup.blog/posts/2021-01-18-reading-tables-from-images-with-magick/  
这样，我很好地管理了单个列类型（字符或整数）。但是有一个问题。您可以希望看到，所有行或列中都没有成绩。这些空细胞充满连字符“  - ”。但是，Tesseract并未识别这些连字符，这就是为什么单个列表元素具有不同数量值的原因。这也导致列长度不同，这就是为什么我不能将它们合并到一个tibble中。
因此，我正在寻找一种识别连字符的方法，以免被忽略。这些列都应该具有相同的长度，以便我可以将它们加入一个tibble。
附件是我当前正在使用的代码。它基本上与模型博客教程中的情况相同，但具有单独的调整人。例如，我在白名单中添加了连字符（ - ），下划线（_）和点（。）。单个农作物区域与我的原始证书上的裁剪区相对应，我当然在这里不显示哪个。希望您能理解这一点。
我对您的支持感到非常高兴，谢谢！
 ＃＃只需要下载一次：
tesseract_download（“ deu”）

现在加载字典
（Deutsch＆lt;  -  Tesseract（“ Deu＆quot”））

abiturzeugnis＆lt;  -  pdftools :: pdf_convert（&#39;./ example.pdf&#39;，dpi = 600）

text＆lt;  -  tesseract :: ocr（Abiturzeugnis，Engine = Deutsch）
猫（文字）

#####
图书馆（pdftools）

bitmap＆lt;  -  pdf_render_page（&#39;./ example.pdf&#39;，page = 2，dpi = 600）
昏暗（位图）
位图

Abiturzeugnis＆lt;  -  image_read（位图）

no_grid＆lt;  -  abiturzeugnis％＆gt;％
image_deskew（）％＆gt;％ 
#image_convert（type =&#39;bilevel&#39;）％＆gt;％ 
image_quantize（colorspace =＆quot;灰色＆quort）％＆gt;％ 
image_transparent（color =; white＆quot; fuzz = 30）％＆gt;％
image_background（“白色”）

图书馆（整洁）

img_ocr_fun＆lt;  - 功能（trim_width，trim_start，char_num = true）{
num_only＆lt;  -  tesseract :: tesseract（    
选项= list（tessedit_char_whitelist = c（; q._-- 0123456789＆quot））
）
组合＆lt ;- Tesseract :: Tesseract（
选项=列表（
  tessedit_char_whitelist = paste0（
    c（字母，字母，“” ._ 0123456789（ - ）
））

 input_char＆lt;  -  if（istrue（char_num））{
num_only
} 别的 {
组合
}
no_grid％＆gt;％
image_crop（geometry_area（trim_width，3180，trim_start，1400））％＆gt;％
OCR（发动机= input_char）％＆gt;％
str_split（模式=; \ n＆quot;）％＆gt;％
UNLIST（）％＆gt;％
enframe（）％＆gt;％
选择（-Name）％＆gt;％
filter（！is.na（value），str_length（value）＆gt; 0）
}

c（
no_grid％＆gt;％
image_crop（geometry_area（750，3180，270，1400）），），
no_grid％＆gt;％
image_crop（geometry_area（115，3180，1210，1400）），），
no_grid％＆gt;％
image_crop（geometry_area（105，3180，1460，1400）），），
no_grid％＆gt;％
image_crop（geometry_area（100，3180，1700，1400）），
no_grid％＆gt;％
image_crop（geometry_area（100，3180，1950，1400）），
no_grid％＆gt;％
image_crop（geometry_area（600，3180，2125，1400））
）％＆gt;％
image_append（）％＆gt;％
image_ggplot（）

all_ocr＆lt;  -  list（trim_width = c（750，115，105，100，100，100，600），
            trim_start = C（270，1210，1460，1700，1950，2125），
            char_num = c（false，true，true，true，true，false））％＆gt;％  
purrr :: pmap（img_ocr_fun） 

all_ocr

data_df＆lt;  -  all_ocr％＆gt;％ 
bind_cols（）％＆gt;％ 
set_names（nm =&#39;fach＆quot&#39;11 i; quot;“ 11 ii” 12 i＆quot;“” 12 ii＆quot;“ 12 ii” 

data_df
 ]]></description>
      <guid>https://stackoverflow.com/questions/69977249/how-to-make-tesseract-recognize-hyphens-or-minus-signs</guid>
      <pubDate>Mon, 15 Nov 2021 15:56:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在变压器培训中实施教师的努力？</title>
      <link>https://stackoverflow.com/questions/57099613/how-is-teacher-forcing-implemented-for-the-transformer-training</link>
      <description><![CDATA[在Tensorflow的教程的这一部分中据我所知，教师努力涉及将目标输出馈送到模型中，以使其收敛更快。所以我很好奇在这里如何完成？真正的目标是 tar_real ，据我所知，它仅用于计算损失和准确性。我很好奇这个代码是如何实施教师的？
事先感谢。]]></description>
      <guid>https://stackoverflow.com/questions/57099613/how-is-teacher-forcing-implemented-for-the-transformer-training</guid>
      <pubDate>Thu, 18 Jul 2019 17:12:25 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机。精度和/或准确性？</title>
      <link>https://stackoverflow.com/questions/36846795/support-vector-machine-precision-and-or-accuracy</link>
      <description><![CDATA[我正在尝试弄清我使用的代码是计算精度还是准确性或两者兼而有之。由于我只有少量的统计背景（用另一种语言），所以我真的不理解 wikipedia&#39;&gt; wikipedia文章涵盖该主题。
具体地我使用以下python代码：
 来自Sklearn Import SVM，Cross_validation
clf = svm.svc（内核=内核，c = c）
scores = cross_validation.cross_val_score（clf，featurematrix，np.squeeze（labelmatrix），cv = d_inds）
 
  scikit-learn 函数可以在此处找到：

     sklearn.svc.svm.svc.svc.svc       
 ]]></description>
      <guid>https://stackoverflow.com/questions/36846795/support-vector-machine-precision-and-or-accuracy</guid>
      <pubDate>Mon, 25 Apr 2016 17:03:37 GMT</pubDate>
    </item>
    </channel>
</rss>