<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 04 Jul 2024 15:15:33 GMT</lastBuildDate>
    <item>
      <title>需要我最近分配的一个项目的帮助。它是一家基于人工智能/机器学习的小型企业，希望将人工智能/机器学习纳入其日常任务中。[关闭]</title>
      <link>https://stackoverflow.com/questions/78707647/need-assistance-on-a-project-i-am-recently-assigned-it-is-an-ai-ml-based-for-a</link>
      <description><![CDATA[我在一家名为 Haro- gage sur stock 的法国企业实习，担任 AI 顾问策略师。他们希望我将 AI 模块、自动化和减少体力劳动融入他们的日常工作中。他们的流程大致可分为 4 类，例如 - Fusabilite/redaction des etudes、mise en place du gage 和监控。除此之外，他们还有一个名为 mongage.com 的在线平台，所有客户的信息（如股票的当前价值、股票图像及其位置等）都以图形方式绘制。因此，如果您看到，Haro 在这里的作用是充当银行和想要从债权人（例如银行）借款的公司之间的中介。他们为每家银行提供了 acte de gage 和 contrat de mandat 等文件的模板。所以帮帮我吧
我还没有开始，但我想联系已经工作或从事类似项目的人。]]></description>
      <guid>https://stackoverflow.com/questions/78707647/need-assistance-on-a-project-i-am-recently-assigned-it-is-an-ai-ml-based-for-a</guid>
      <pubDate>Thu, 04 Jul 2024 14:26:55 GMT</pubDate>
    </item>
    <item>
      <title>独热编码和缺失值处理</title>
      <link>https://stackoverflow.com/questions/78707613/one-hot-encoding-and-handling-of-missing-values</link>
      <description><![CDATA[我正在构建一个机器学习模型。
我决定不估算缺失值。我已将它们替换为附加类别：其他
例如，feature_A 将具有类别 A1、A2、A3 和 其他，而 feature_B 将具有类别 B1、B2 和 其他：B1、B2 和 其他。
我使用 pandas 的 get_dummy 来创建虚拟变量。
feature_A_A1、feature_A_A2、feature_A_A3、feature_A_Other、feature_B_B1、feature_B_B2、feature_Other。
最后，我删除了带有 Other 的功能。
我想知道这是否是正确的处理方式，如果它不会导致信息丢失。]]></description>
      <guid>https://stackoverflow.com/questions/78707613/one-hot-encoding-and-handling-of-missing-values</guid>
      <pubDate>Thu, 04 Jul 2024 14:17:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 BentoML 部署 h5 格式的模型</title>
      <link>https://stackoverflow.com/questions/78707424/how-to-deploy-model-in-h5-format-with-bentoml</link>
      <description><![CDATA[我有两个项目：

我用 Tensorflow 制作的 CRNN 模型。权重以 h5 格式保存。
我目前正在学习 bentoML，并已使用 BentoML 成功部署了示例中的模型。

我需要什么：我想使用 BentoML 部署我自己的模型。
为什么我不能这样做：因为我是 BentoML 的新手，文档中只有预先训练的模型的示例，您只需使用一些命令即可下载。还有 obj 格式的模型权重和 h5 格式的模型。
我尝试跟进示例 (https://docs.bentoml.com/en/latest/reference/frameworks/tensorflow.html)，但卡在了这一行：
self.dense = lambda input: tf.matmul(inputs, self.weights)
]]></description>
      <guid>https://stackoverflow.com/questions/78707424/how-to-deploy-model-in-h5-format-with-bentoml</guid>
      <pubDate>Thu, 04 Jul 2024 13:36:51 GMT</pubDate>
    </item>
    <item>
      <title>附加的 TimeSeries 必须在当前时间步之后的一个时间步开始</title>
      <link>https://stackoverflow.com/questions/78707349/appended-timeseries-must-start-one-time-step-after-current-one</link>
      <description><![CDATA[我收到错误
错误：darts.timeseries：ValueError：附加的 TimeSeries 必须在当前时间步后一个时间步开始。
------------------------------------------------------------------------------------------
ValueError 回溯（最近一次调用最后一次）
&lt;ipython-input-87-42786fe5dae7&gt; 在 &lt;cell line: 3&gt;()
19 covariate = covariate[last_date + covariate.freq:] # 选择从最后一个日期之后开始的部分
20 
---&gt; 21 new_covariates.append(covariates[series].append(covariate))

1 帧
/usr/local/lib/python3.10/dist-packages/darts/logging.py in raise_if_not(condition, message, logger)
77 if not condition:
78 logger.error(&quot;ValueError: &quot; + message)
---&gt; 79 raise ValueError(message)
80 
81 

ValueError: 附加的 TimeSeries 必须在当前时间步后一个时间步开始。在此代码中](https://i.sstatic.net/foJsVu6t.png)
]]></description>
      <guid>https://stackoverflow.com/questions/78707349/appended-timeseries-must-start-one-time-step-after-current-one</guid>
      <pubDate>Thu, 04 Jul 2024 13:19:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么验证和训练中重构误差不同？为什么一半的测试集和验证集不同？</title>
      <link>https://stackoverflow.com/questions/78707337/why-there-is-a-different-recontructed-error-in-the-validation-and-train-why-hal</link>
      <description><![CDATA[重建错误 
请帮我理解一下这是过度拟合的问题还是其他问题：我正在用 alexnet 训练自动编码器：这可能是某些数据的拟合不足吗？除了现有的任何调节方法之外？为什么会发生这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/78707337/why-there-is-a-different-recontructed-error-in-the-validation-and-train-why-hal</guid>
      <pubDate>Thu, 04 Jul 2024 13:15:21 GMT</pubDate>
    </item>
    <item>
      <title>增加 CPU 数量后，VM 上本地 LLM 的响应时间没有改善 [关闭]</title>
      <link>https://stackoverflow.com/questions/78705963/response-time-of-local-llm-on-vm-not-improving-after-increasing-number-of-cpus</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705963/response-time-of-local-llm-on-vm-not-improving-after-increasing-number-of-cpus</guid>
      <pubDate>Thu, 04 Jul 2024 08:39:19 GMT</pubDate>
    </item>
    <item>
      <title>鉴于我的数据集不大且大多数模型都无法生成正确的响应，我该如何训练我的聊天机器人</title>
      <link>https://stackoverflow.com/questions/78705520/how-do-i-train-my-chatbot-given-that-my-dataset-is-not-vast-large-and-most-mode</link>
      <description><![CDATA[我从 llama 开始，因为我被告知要使用 meta-llama（hugging face 上的任何一种），但由于我的笔记本电脑规格有限，meta-llama 从未进入训练阶段。甚至尝试了 meta 中的 10 亿参数模型，仍然不好。
我有一个包含原始数据的 .txt 文件（未按问题：答案对排列），所以我编写了一个脚本，首先创建一个 json 文件，在该文件中，数据按问题：答案对排列，现在我的目标是使用特定模型对其进行微调和训练。我的数据不是庞大/大，最多是小或中等。所以，我尝试了很多模型。Gpt2 等给了我糟糕的回应，所以我尝试了一些其他模型，如 Albert、tiny-bert、roberta-large，但问题是，有些问题确实得到了机器人的正确回答，但其中很大一部分都失败了。有些问题的答案只是重复的、不完整的。
我尝试了几种方法来提高效率（SBERT/TF-IDF 等），甚至尝试修改微调文件，但都无济于事。我对这一切都很陌生，所以我想知道如何继续。T
尝试了几种模型来训练我的聊天机器人，但几乎都失败了。
希望我的聊天机器人能够使用包含问题：答案对的 json 文件进行适当的训练，但是数据可能不会太大。
笔记本电脑也有一些限制（规格，例如低内存 -&gt; 8GB）]]></description>
      <guid>https://stackoverflow.com/questions/78705520/how-do-i-train-my-chatbot-given-that-my-dataset-is-not-vast-large-and-most-mode</guid>
      <pubDate>Thu, 04 Jul 2024 07:06:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pytorch 从多幅图像中获取一定数量图像的梯度？而不是所有输入图像的梯度？</title>
      <link>https://stackoverflow.com/questions/78705415/how-can-i-get-the-gradient-of-a-certain-number-of-image-out-of-multiple-images-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705415/how-can-i-get-the-gradient-of-a-certain-number-of-image-out-of-multiple-images-w</guid>
      <pubDate>Thu, 04 Jul 2024 06:39:42 GMT</pubDate>
    </item>
    <item>
      <title>LangChain 与 AmzonBedrock</title>
      <link>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</guid>
      <pubDate>Thu, 04 Jul 2024 06:30:23 GMT</pubDate>
    </item>
    <item>
      <title>在感知器中，当我们将权重初始化为 0 时，权重向量如何表现</title>
      <link>https://stackoverflow.com/questions/78704896/how-weight-vector-behave-when-we-initialize-the-weight-to-0-in-case-of-perceptro</link>
      <description><![CDATA[在阅读书中时，我遇到了这句话

现在，我们不将权重初始化为零的原因是，只有当权重初始化为非零值时，学习率（eta）才会对分类结果产生影响。如果所有权重都初始化为零，则学习率参数eta仅影响权重向量的比例，而不影响方向。

我按照此链接的答案进行操作
https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero#:~:text=If%20you%20initialize%20all%20weights,vector%2C%20not%20the%20direction%22.
但我还是不明白。
假设我有 2 个特征，这意味着 3 个权重（包括偏差）
所以这意味着向量 W = (w1,w2,w3) 将在坐标上表示？
因此方向将是从 (0,0,0) 到 (w1,w2,w3) 的线，幅度是它们之间的距离。
或者每个权重示例 w1、w2 等是否会在坐标上单独表示？并且所有三个权重都有不同的方向和幅度
如果权重初始化为 0，那么方向如何不变？只有比例如何变化？
我是代数新手，所以如果您能以非常简单的方式向我解释整个场景，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/78704896/how-weight-vector-behave-when-we-initialize-the-weight-to-0-in-case-of-perceptro</guid>
      <pubDate>Thu, 04 Jul 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>nn.Linear 如何应用于高维数据？</title>
      <link>https://stackoverflow.com/questions/78704861/how-is-nn-linear-applied-to-a-higher-dimensional-data</link>
      <description><![CDATA[我试图了解当我有更高维的张量时哪些值被乘以了：
inp = torch.rand(1,2,3) # B,C,W
linear = nn.Linear(3,4)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 4])

inp = torch.rand(1,2,3,4) # B,C,W,H
linear = nn.Linear(4,5)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 3, 5])

似乎只有最后一个维度被改变了，但是当我尝试手动将线性权重 (linear.weight.data) 与每个 inp 的最后一个维度相乘时，我无法得到正确的答案（似乎所有值都在改变，只有最后一个维度的大小以某种方式被修改）。]]></description>
      <guid>https://stackoverflow.com/questions/78704861/how-is-nn-linear-applied-to-a-higher-dimensional-data</guid>
      <pubDate>Thu, 04 Jul 2024 02:57:29 GMT</pubDate>
    </item>
    <item>
      <title>将基于 Bert 的 PyTorch 模型导出到 CoreML。如何让 CoreML 模型适用于任何输入？</title>
      <link>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</link>
      <description><![CDATA[我使用以下代码将基于 Bert 的 PyTorch 模型导出到 CoreML。
由于我使用
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)

在 macOS 上测试时，CoreML 模型仅适用于该输入。如何让 CoreML 模型适用于任何输入（即任何文本）？

导出脚本：
# -*- coding: utf-8 -*-
&quot;&quot;&quot;Core ML Export
pip install tr​​ansformers torch coremltools nltk
&quot;&quot;&quot;
导入 os
从 transformers 导入 AutoModelForTokenClassification、AutoTokenizer
导入 torch
导入 torch.nn 作为 nn
导入 nltk
导入 coremltools 作为 ct
nltk.download(&#39;punkt&#39;)
# 加载模型和 tokenizer
model_path = os.path.join(&#39;model&#39;)
model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
# 修改模型的 forward 方法以返回元组
class ModifiedModel(nn.Module):
def __init__(self, model):
super(ModifiedModel, self).__init__()
self.model = model
self.device = model.device # 添加设备属性

def forward(self, input_ids,tention_mask, token_type_ids=None):
outputs = self.model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)
returnoutputs.logits

modified_model = ModifiedModel(model)

# 导出到 Core ML
def convert_to_coreml(model, tokenizer):
# 定义用于跟踪的虚拟输入
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
dummy_input = {k: v.to(model.device) for k, v in dummy_input.items()}

# 使用虚拟输入跟踪模型
traced_model = torch.jit.trace(model,(
dummy_input[&#39;input_ids&#39;],dummy_input[&#39;attention_mask&#39;], dummy_input.get(&#39;token_type_ids&#39;)))

# 转换为 Core ML
输入 = [
ct.TensorType(name=&quot;input_ids&quot;, shape=dummy_input[&#39;input_ids&#39;].shape),
ct.TensorType(name=&quot;attention_mask&quot;, shape=dummy_input[&#39;attention_mask&#39;].shape)
]
if &#39;token_type_ids&#39; in dummy_input:
输入.append(ct.TensorType(name=&quot;token_type_ids&quot;, shape=dummy_input[&#39;token_type_ids&#39;].shape))

mlmodel = ct.convert(traced_model, 输入=inputs)

# 保存 Core ML 模型
mlmodel.save(&quot;model.mlmodel&quot;)
print(&quot;模型导出到 Core ML成功&quot;)

convert_to_coreml(modified_model, tokenizer)

在 Ubuntu 20.04 上测试了 Python 3.10 和 torch 2.3.1（在 Windows 10 上不工作）。]]></description>
      <guid>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</guid>
      <pubDate>Wed, 03 Jul 2024 23:39:36 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 yolov8 识别一张额外的图像</title>
      <link>https://stackoverflow.com/questions/78703688/how-to-train-yolov8-to-recognize-one-additional-image</link>
      <description><![CDATA[因此，我想使用包含一张带注释图像的数据集（使用 roboflow）来训练 yolov8，以将标签添加到当前模型，以便训练后的模型能够识别新图像。
首先，我获取在 roboflow 中注释的单图像数据集，如下所示：
dataset = version.download(model_format=&quot;yolov8&quot;, location=&quot;./datasets&quot;)

然后使用以下命令训练 yolov8 模型：
results = model.train(data=&quot;/roboflow_ml_image_detection/datasets/oups-1/data.yaml&quot;, epochs=5)

然后导出新模型：
success = model.export(format=&quot;onnx&quot;)

我将再次使用它对新图像进行预测：
model = YOLO(&#39;/roboflow_ml_image_detection/runs/detect/train29/weights/last.pt&#39;)

results = model(source=&#39;./weirdobject.jpg&#39;, conf=0.25)[0]

最后尝试使用 supervision 检测图像的标签：
import surveillance as sv

detections = sv.Detections.from_ultralytics(results)

bounding_box_annotator = sv.BoundingBoxAnnotator()

label_annotator = sv.LabelAnnotator()

annotated_image = bounding_box_annotator.annotate(
scene=image, detections=detections)
annotated_image = label_annotator.annotate(
scene=annotated_image, detections=detections)

sv.plot_image(annotated_image)

但最终调用仅显示没有框架和标签的图像。
这里有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78703688/how-to-train-yolov8-to-recognize-one-additional-image</guid>
      <pubDate>Wed, 03 Jul 2024 18:21:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 GNN 不能完成这么简单的任务？（使用其特征及其邻居的特征来预测其标签）</title>
      <link>https://stackoverflow.com/questions/78685248/why-cant-my-gnn-work-on-such-a-simple-taskusing-its-features-and-its-neighbou</link>
      <description><![CDATA[我正在使用图神经网络 (GNN) 进行节点回归预测。目前，我正在为每个节点生成具有 10 维特征且没有边特征的随机图。每个节点的标签是通过将其自身特征相加并将其邻居特征的总和相加而创建的。因此，我认为 GNN 预测标签应该是一个简单的任务，损失应该会收敛到零。然而，实际损失在 45 左右波动。我尝试了很多方法将损失降至零，但都没有奏效。该模型在某种程度上可以起到这样的作用：
节点 0：预测：9.7522，真实：12.9197
节点 1：预测：-6.3811，真实：-9.2859
节点 2：预测：-19.5746，真实：-7.5249
节点 3：预测：-77.1716，真实：-17.7568
节点 4：预测：-2.2538，真实：-5.5253
节点 5：预测：-9.8838，真实：-4.1401
节点 6：预测：-10.5909，真实：-8.0105
节点 7：预测：-5.9015，真实：-9.8000
节点 8：预测： -20.8053，真实：-30.8081
节点 9：预测：5.1975，真实：2.2300
节点 10：预测：-14.8946，真实：-8.9688
节点 11：预测：-4.1185，真实：-4.4405
节点 12：预测：-21.0528，真实：-25.8891
节点 13：预测：-3.3378，真实：-8.9060
节点 14：预测：-6.5355，真实：-7.4367
节点 15：预测：29.2404，真实：30.6309
节点 16：预测：-37.1685，真实：-64.5965
节点17：预测：-6.4728，真实：-4.6115
节点 18：预测：8.0359，真实：9.0135
节点 19：预测：25.7627，真实：29.4366
节点 20：预测：26.4058，真实：20.6240
节点 21：预测：13.3435，真实：12.7873
节点 22：预测：-47.8510，真实：-53.5470
节点 23：预测：-12.2702，真实：-21.0920
节点 24：预测：-33.2615，真实：-29.9373
节点 25：预测：19.7694，真实值：26.7916
节点 26：预测值：3.0540，真实值：7.8801
节点 27：预测值：22.9412，真实值：23.2001
节点 28：预测值：10.8517，真实值：11.9646
节点 29：预测值：-9.5051，真实值：-14.9755

但我认为预测值应该更符合真实值。我想知道为什么我的 GNN 不能完成这么简单的任务？
我的完整代码位于：
https://github.com/zyg18/GNN/blob/main/GNN.py
我尝试过许多 GNN 模型，例如：
class SimpleGNN(torch.nn.Module):
def __init__(self, num_node_features):
super(SimpleGNN, self).__init__()
self.conv = GCNConv(num_node_features, 1, bias=False)

def forward(self, x, edge_index):
return self.conv(x, edge_index).squeeze(-1)

class GNNWithTwoConvLayers(torch.nn.Module):
def init(self, num_node_features):
super(GNNWithTwoConvLayers, self).init()
self.conv1 = GCNConv(num_node_features, 64)
self.conv2 = GCNConv(64, 64)
self.fc1 = torch.nn.Linear(64, 16)
self.fc2 = torch.nn.Linear(16, 1)
self.dropout = torch.nn.Dropout(0.5)

def forward(self, x, edge_index):
x = self.conv1(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.conv2(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
返回 x.squeeze(-1)

class GNN(torch.nn.Module):
def init(self, num_node_features):
super(GNN, self).init()
self.conv1 = GCNConv(num_node_features, 64)
self.fc1 = torch.nn.Linear(64, 16)
self.fc2 = torch.nn.Linear(16, 1)
self.dropout = torch.nn.Dropout(0.5)

def forward(self, x, edge_index):
x = self.conv1(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
返回x.squeeze(-1)
]]></description>
      <guid>https://stackoverflow.com/questions/78685248/why-cant-my-gnn-work-on-such-a-simple-taskusing-its-features-and-its-neighbou</guid>
      <pubDate>Sat, 29 Jun 2024 06:43:38 GMT</pubDate>
    </item>
    <item>
      <title>yolov8 在训练后不会启动冻结：扫描</title>
      <link>https://stackoverflow.com/questions/77893385/yolov8-doesn%c2%b4t-initiate-freezes-after-train-scanning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77893385/yolov8-doesn%c2%b4t-initiate-freezes-after-train-scanning</guid>
      <pubDate>Sun, 28 Jan 2024 01:33:57 GMT</pubDate>
    </item>
    </channel>
</rss>