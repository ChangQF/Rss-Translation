<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 26 Aug 2024 03:19:37 GMT</lastBuildDate>
    <item>
      <title>在哪里可以高效且经济地微调 7B 参数 LLM？[关闭]</title>
      <link>https://stackoverflow.com/questions/78912744/where-can-i-fine-tune-a-7b-parameter-llm-efficiently-and-affordably</link>
      <description><![CDATA[我需要在 4 个数据集上运行大约 5 种不同的微调方法，每个数据集有 1000-1500 个样本。
我需要微调的模型是 Mistral7B。
我有一个基本的 Google Colab 订阅，但它不允许访问 A100 GPU，并且可用的 T4 GPU 缺乏足够的内存来微调 7B 参数模型——它很快就会耗尽内存。即使进行了量化和混合精度等优化，我仍然会达到内存限制。
我正在寻找具有足够 GPU 内存的经济高效的平台或服务来高效处理这些任务。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78912744/where-can-i-fine-tune-a-7b-parameter-llm-efficiently-and-affordably</guid>
      <pubDate>Mon, 26 Aug 2024 03:01:28 GMT</pubDate>
    </item>
    <item>
      <title>一个 scikit ML 示例中的问题，“使用多项逻辑 + L1 进行 MNIST 分类”</title>
      <link>https://stackoverflow.com/questions/78912616/question-in-one-scikit-ml-example-mnist-classification-using-multinomial-logis</link>
      <description><![CDATA[https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html
这个例子我有两个问题：
Q1：理论上应该是数据越多，规则越少。但是“C=50.0 / train_samples”会导致数据越多，规则越强。是代码的缺陷，还是我的误解？
clf = LogisticRegression(C=50.0 / train_samples, penalty=&quot;l1&quot;,solver=&quot;saga&quot;, tol=0.1)
Q2：为什么只用 coef 矩阵就能画出最终的数字图像，这背后的想法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78912616/question-in-one-scikit-ml-example-mnist-classification-using-multinomial-logis</guid>
      <pubDate>Mon, 26 Aug 2024 01:44:53 GMT</pubDate>
    </item>
    <item>
      <title>Dask 在 GridSearchCV 和 RandomizedSearchCV 上犯了错误</title>
      <link>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</link>
      <description><![CDATA[我正在尝试使用 dask 训练 xgboost 模型。我已经转换了数据并准备了如下数据：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

我可以像这样对数据进行简单的模型拟合：
model = dxgb.DaskXGBRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)

但是当我尝试任何更复杂的事情时。就像这样使用 dask 的 gridsearchcv：
param_grid = {
&#39;max_depth&#39;: [3, 8],
&#39;learning_rate&#39;: [0.01, 0.1]}

grid_search = GridSearchCV(
model, param_grid}

grid_search.fit(X_train, y_train)

我收到以下警告：
警告：dask_ml.model_selection._search:(&#39;daskxgbregressor-fit-score-f6830e79aeb606b3eed291ac24184a8c&#39;, 1, 1) 失败...正在重试

任务图中的所有内容都因错误而变黑。
有人知道如何解决吗这个？]]></description>
      <guid>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</guid>
      <pubDate>Mon, 26 Aug 2024 01:04:55 GMT</pubDate>
    </item>
    <item>
      <title>Matryoshka 适配器实现：谷歌关于嵌入模型的精彩新论文 [关闭]</title>
      <link>https://stackoverflow.com/questions/78912400/matryoshka-adaptor-implementation-exciting-new-paper-on-embedding-models-from-g</link>
      <description><![CDATA[Google 最近在 arxiv 上发布了一篇关于嵌入模型的激动人心的新论文]1。本质上，该论文提出了一种简单 MLP 形式的适配器，它可以高效地将任何嵌入模型的输出适配到 Matryoshka 嵌入，根据需要降低尺寸，同时保持性能；嵌入模型本身不需要微调或进行迁移学习。这是一篇令人兴奋的论文，具有一些非常令人兴奋的含义。
我一直在尝试自己复制结果，但不幸的是，我没有达到承诺的准确度 - 正如您所见，我获得的嵌入并不比裁剪原始模型的嵌入更好：

未获得论文图 4 中的结果 - 无监督 Matryoshka Adaptor 并不比具有截断嵌入的 BASE 模型更好；它应该比具有降维的 PCA 降级更少。
我认为我误解了并因此错误地实现了所提出的模型。我认为它是以下之一：

错误的训练过程 - 我一直在 NFCorpus 上训练它，但我也尝试过在其他 BEIR 数据集上尝试。对于应该更易于实现的无监督方法，我一直在为每个 BEIR 数据集在语料库的“文本”上训练它。训练期间损失在减少，但如您所见，

性能并不好。

其中一个损失函数的实现不正确。

我还想澄清如何计算对的数量（训练对和测试对），假设它们是查询语料库对，将

仅用于监督方法。


有人可以看看我在 GitHub 上的实现或提供任何调试建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78912400/matryoshka-adaptor-implementation-exciting-new-paper-on-embedding-models-from-g</guid>
      <pubDate>Sun, 25 Aug 2024 22:39:46 GMT</pubDate>
    </item>
    <item>
      <title>拟合模型时无法取未知等级形状的长度</title>
      <link>https://stackoverflow.com/questions/78911992/cannot-take-the-length-of-shape-with-unknown-rank-when-fitting-model</link>
      <description><![CDATA[我正在尝试训练深度音频分类模型。检查测试和训练数据的形状时，结果是 (16, 6245, 257, 1)，其中 16 是批次中的项目数，6245 是宽度，257 是高度，1 是通道数。然后我在神经网络中创建了此输入层：
model.add(Input(shape=(6245, 257, 1)))

我能够创建模型，甚至获得模型摘要。但是当我使用以下命令训练模型时：
hist = model.fit(train, epochs=6, validation_data=test)

我收到此错误：
无法获取具有未知等级的形状的长度。

我在 anaconda 环境中的 jupyter 笔记本中使用 tensorflow 2.17。
注意：我正在将音频转换为频谱图，并将此函数映射到整个数据集上。
def preprocess(file_path, label): 
def load_and_preprocess(path):
wav = load_wav_16k_mono(path)
wav = wav[:200134]
zero_padding = tf.zeros([200134] - tf.shape(wav), dtype=tf.float32)
wav = tf.concat([zero_padding, wav], 0)
spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)
spectrogram = tf.abs(spectrogram)
spectrogram = tf.expand_dims(spectrogram, axis=-1)
return spectrogram

# 使用 tf.py_function 包装文件加载和预处理
file_path = tf.convert_to_tensor(file_path, dtype=tf.string)
spectrogram = load_and_preprocess(path)

return spectrogram, label

尽管如此，检查频谱图的形状也会返回 TensorShape([6245, 257])，因此它与传递到模型中的样本相同。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78911992/cannot-take-the-length-of-shape-with-unknown-rank-when-fitting-model</guid>
      <pubDate>Sun, 25 Aug 2024 18:46:13 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8 推理在 Mac 上运行良好，但在 Windows 上不运行</title>
      <link>https://stackoverflow.com/questions/78911914/yolov8-inference-working-on-mac-but-not-windows</link>
      <description><![CDATA[我在 pycharm 中使用 ultralytics 中的 yolo v8 对我训练的模型进行推理，当我在 macbook 上运行它时，它工作正常，但在我的 windows 笔记本电脑上，我得到大量边界框，置信度得分为 1，即使我使用的是相同的 best.pt 文件和相同的代码：
正在发生的事情的示例
从 ultralytics 导入 YOLO
从 ultralytics.utils.benchmarks 导入基准

model = YOLO(&quot;best.pt&quot;)
# model = YOLO(&quot;yolov8m.pt&quot;)

results = model(source=0, show=True, conf=0.6, save=True)
# results = model.track(source=0, show=True, conf=0.6, tracker=&quot;bytetrack.yaml&quot;)

我之前也遇到过类似问题（不完全一样），与下面链接的问题类似，pycharm 无法检测到 pytorch，每次导入它时都会出错：
import torch：如何修复 OSError WinError 126，加载 fbgemm.dll 或依赖项时出错
我使用用户提供的 libomp140.x86_64.dll 文件并将其放在 C:\Windows\System32 中修复了它，这可能是问题所在吗？
我已经尝试过的方法：
我尝试重新安装ultralytics 和 pytorch 库以及 pycharm 认为这是他们的安装问题，但什么都没有改变。]]></description>
      <guid>https://stackoverflow.com/questions/78911914/yolov8-inference-working-on-mac-but-not-windows</guid>
      <pubDate>Sun, 25 Aug 2024 18:16:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在决策树中使用直方图实现分箱条件？</title>
      <link>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</guid>
      <pubDate>Sun, 25 Aug 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么每次运行程序都会得到不同的准确度</title>
      <link>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</link>
      <description><![CDATA[我使用 Python 中的 keras tensorflow 训练模型。
另外，正如您在下面的代码中看到的，我使用了种子参数，但每次我用相同的数据运行相同的代码时，我都会面临不同的准确率百分比。
我的代码：
#Seed
tf.random.set_seed(42)
np.random.seed(42)
set_random_seed(42)
random.seed(42)

data = (&#39;data.csv&#39;)

data = pd.get_dummies(data, columns=[&#39;cp&#39;, &#39;restecg&#39;], drop_first=True)

X = data.drop(&#39;num&#39;, axis=1)
y = data[&#39;num&#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def create_model(optimizer=&#39;adam&#39;, init=&#39;glorot_uniform&#39;, neurons=[16, 8], dropout_rate=0.3):
model = Sequential()
model.add(Input(shape=(X_train.shape[1],)))
model.add(Dense(neurons[0],activation=&#39;relu&#39;, kernel_initializer=init))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(neurons[1],activation=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))
model.add(Dense(1,activation=&#39;sigmoid&#39;))
model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
返回模型

param_grid = {
&#39;optimizer&#39;: [&#39;adam&#39;, &#39;rmsprop&#39;],
&#39;model__neurons&#39;: [[16, 8]],
&#39;model__init&#39;: [&#39;glorot_uniform&#39;, &#39;normal&#39;],
&#39;model__dropout_rate&#39;: [0.3],
&#39;epochs&#39;: [50], 
&#39;batch_size&#39;: [10],
}

model = KerasClassifier(model=create_model, verbose=0)

kfold = KFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, n_jobs=-1)
grid_search_result = grid_search.fit(X_train, y_train)

print(f&quot;最佳参数：{grid_search_result.best_params_}&quot;)

print(f&quot;最佳准确度：{grid_search_result.best_score_}&quot;)

best_model = grid_search_result.best_estimator_

keras_model = best_model.model
keras_model.trainable = False 

y_pred_prob = best_model.predict(X_test).flatten()
y_pred = np.where(y_pred_prob &gt; 0.5, 1, 0)

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f&#39;Accuracy (手动计算): {accuracy:.2f}&#39;)
print(f&#39;ROC AUC: {roc_auc:.2f}&#39;)

我需要每次都获得相同的准确度。
我应该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78911839/why-get-different-accuracy-every-time-run-program</guid>
      <pubDate>Sun, 25 Aug 2024 17:25:44 GMT</pubDate>
    </item>
    <item>
      <title>检测硬币图像的旋转角度</title>
      <link>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</link>
      <description><![CDATA[确定硬币图像角度的最佳方法是什么？
图像的分辨率是固定的（400x400），但可能在任何方向上偏离中心几个像素。
图像可以有不同的颜色、光照、旋转等。


我尝试过各种 CNN 想法，但都没有成功。看来，旋转后的图像之间的相似性不足以增强效果。
我无法找到用于此目的的现有数据集
请给我一些已被证明可行的想法，而不是理论]]></description>
      <guid>https://stackoverflow.com/questions/78911221/detecting-rotation-angle-of-coin-image</guid>
      <pubDate>Sun, 25 Aug 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>从 ECL 中的数据集中提取实际 y 以进行随机森林分类</title>
      <link>https://stackoverflow.com/questions/78808039/extracting-actual-y-from-dataset-in-ecl-for-random-forest-classification</link>
      <description><![CDATA[我正在使用 ECL 开发随机森林分类模型。我有一个分为训练集和测试集的数据集，我正在尝试从这两个数据集中提取目标变量 𝑦（表示患者是否患有糖尿病）。但是，当前使用“ML_Core.Discretize.ByRounding”的方法无法产生预期的结果。
我尝试使用以下 ECL 代码片段从“TrainNF”和“TestNF”数据集中提取目标变量 𝑦（代表糖尿病指标）：
independent_cols := 8;

X_train := TrainNF(number &lt; independent_cols + 1);
y_train := ML_Core.Discretize.ByRounding(TrainNF(number = independent_cols + 1));

X_test := TestNF(number &lt; independent_cols + 1);
y_test := ML_Core.Discretize.ByRounding(TestNF(number = independent_cols + 1));

“y_train”和“y_test”变量不包含任何值；它们实际上是空白的，这表明提取没有按预期进行。
我预计“y_train”和“y_test”分别包含来自“TrainNF”和“TestNF”数据集的值。这些值对于训练随机森林分类器和评估其性能至关重要。]]></description>
      <guid>https://stackoverflow.com/questions/78808039/extracting-actual-y-from-dataset-in-ecl-for-random-forest-classification</guid>
      <pubDate>Mon, 29 Jul 2024 15:54:43 GMT</pubDate>
    </item>
    <item>
      <title>rpart() 决策树无法生成分割（只有一个节点（根节点）的决策树）</title>
      <link>https://stackoverflow.com/questions/78804884/rpart-decision-tree-fails-to-generate-splits-decision-tree-with-only-one-node</link>
      <description><![CDATA[我正在尝试创建决策树来预测特定贷款申请人是否会违约或偿还债务。
我正在使用以下数据集
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)

贷款 &lt;- read_csv(&#39;https://assets.datacamp.com/production/repositories/718/datasets/7805fceacfb205470c0e8800d4ffc37c6944b30c/loans.csv&#39;)

由于响应变量 default 被编码为 dbl，我首先将其转换为 chr，然后将其转换为 fct 类型变量以在我的分类中使用它模型。
loans &lt;- loans %&gt;% mutate(default = factor(as.character(default), levels = c(0, 1), labels = c(&#39;repaid&#39;, &#39;defaulted&#39;)))

现在，我开始构建递归分区 (rpart()) 对象 loans_model：响应变量为 default，解释变量为 loan_amount + credit_score + deal_to_income。
loans_model &lt;- rpart(default ~ loan_amount + credit_score + deal_to_income, data = loans, method = &#39;class&#39;)

当我使用此模型进行预测时，所有预测值都得到相同的值， 已偿还。
loans$pred_default &lt;- predict(loans_model, newdata = loans, type = &quot;class&quot;)

unique(unique(loans$pred_default)


输出：
[1] 已偿还
级别：已偿还 已违约

此外，当我尝试可视化决策树时，我只得到一个节点（根节点）。
rpart.plot(loan_model)


为什么我建立的模型没有做出适当的预测？]]></description>
      <guid>https://stackoverflow.com/questions/78804884/rpart-decision-tree-fails-to-generate-splits-decision-tree-with-only-one-node</guid>
      <pubDate>Sun, 28 Jul 2024 21:42:53 GMT</pubDate>
    </item>
    <item>
      <title>利用外推训练的随机森林模型来创建使用不同卫星的时间序列</title>
      <link>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</link>
      <description><![CDATA[我正在尝试创建 1990 年至 2023 年的 LULC 时间序列。为了生成我的时间序列，我选择了四个使用不同卫星（Landsat 5、7、8 和 Sentinel-1 和 -2）的日期。我只有 2023 年的训练数据。目前我只是使用历史数据中使用的相同 2023 年点重新训练模型。我觉得这会导致我的分类出现一些不一致。我希望能够在所有日期运行我训练过的分类器，但存在一些严重的波段不一致。有没有办法让模型适应这些波段变化？
不一致的原因不仅在于卫星之间的波段差异，还在于我对不同年份的预处理步骤，即使用全色波段对 Landsat 数据进行全色锐化，以及对整个 sentinel-1（不收集 RBG）进行锐化。
我正在使用 Google Earth Engine API（JavaScript）
在删除几个波段后，我尝试对 sentinel 和 landsat8 使用相同的模型（在 2023 年 sentinel 数据上进行训练），但是我的 landsat8 数据的 LULC 地图完全错误（在很大程度上，基于我的视觉评估和地面真实数据）。]]></description>
      <guid>https://stackoverflow.com/questions/78765770/extrapolate-trained-random-forest-model-to-create-a-time-series-using-different</guid>
      <pubDate>Thu, 18 Jul 2024 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>为什么在使用 ranger 包进行预测时不使用变量重要性设置的原因是什么？</title>
      <link>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</link>
      <description><![CDATA[我正在使用 R 中的 ranger 包来构建随机森林模型，当我预测新数据时，会弹出此警告：
警告消息：
在 predict.ranger(object, ...) 中：
森林是使用“impurity_corrected”变量重要性构建的。对于预测，建议构建另一个不带此重要性设置的森林。

我想知道是否有人可以解释背后的原因，是否因为预测会出错，只是需要更多的计算时间等。
我在网上搜索答案，但找不到原因。]]></description>
      <guid>https://stackoverflow.com/questions/78674442/reasoning-behind-why-not-to-use-variable-importance-setting-when-predicting-with</guid>
      <pubDate>Wed, 26 Jun 2024 20:05:02 GMT</pubDate>
    </item>
    <item>
      <title>非线性模型 GAM、MARS 假设</title>
      <link>https://stackoverflow.com/questions/67024201/non-linear-models-gam-mars-assumptions</link>
      <description><![CDATA[MARS 和 GAM 等模型是否假设异方差和 IID 误差？文献中似乎对某些假设存在分歧。看起来 MARS 比 GAM 更稳健，但原始论文中没有明确说明。
如果正态性是一个问题，是否应该使用变换数据（Box-Cox 或 Yeo-Johnson）进行回归？]]></description>
      <guid>https://stackoverflow.com/questions/67024201/non-linear-models-gam-mars-assumptions</guid>
      <pubDate>Fri, 09 Apr 2021 15:42:32 GMT</pubDate>
    </item>
    <item>
      <title>GRU 和 LSTM 哪个更快</title>
      <link>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</link>
      <description><![CDATA[我尝试在 keras 上用 GRU 和 LSTM 实现一个模型。两种实现的模型架构相同。正如我在许多博客文章中看到的，GRU 的推理时间比 LSTM 更快。但就我而言，GRU 并不更快，而且实际上比 LSTM 更慢。有人能找到原因吗？这与 Keras 中的 GRU 有什么关系吗？还是我哪里做错了。
非常感谢您的帮助...
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm</guid>
      <pubDate>Mon, 27 Jan 2020 14:21:12 GMT</pubDate>
    </item>
    </channel>
</rss>