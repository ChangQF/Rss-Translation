<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 01 Nov 2024 21:15:40 GMT</lastBuildDate>
    <item>
      <title>实时 resnet 预测</title>
      <link>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</link>
      <description><![CDATA[我训练了一个 resnet50 模型，使用从 0 到 5 的数字手势，并尝试通过笔记本电脑的网络摄像头部署它来预测实时类别。
虽然该模型的准确率为 98%，而且我很确定错误不是因为模型训练不当而发生的，但实时值被困在 5 个类别中的 1 个或 2 个类别中，它们总是预测数字 0 和数字 2。
这是代码：
import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image # 导入 PIL 进行图像转换

# 定义模型架构并加载权重
class ResNet50Modified(nn.Module):
def __init__(self, num_classes=6):
super(ResNet50Modified, self).__init__()
self.model = models.resnet50(pretrained=True) # 使用 pretrained=True 以获得更好的性能
self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

def forward(self, X):
return self.model(X)

# 加载训练好的模型
model = ResNet50Modified(num_classes=6)
# 为 CPU 加载模型的 state_dict
model.load_state_dict(torch.load(&quot;resnet50_modified1.pth&quot;, map_location=torch.device(&#39;cpu&#39;)))
model.eval()

# 定义转换以匹配训练预处理
preprocess = transforms.Compose([
transforms.Resize((64, 64)), # 调整为模型的输入大小
transforms.ToTensor(), # 转换为张量
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 根据 ResNet 标准进行标准化
])

# 标志的标签
class_names = [&#39;Class_0&#39;, &#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;] # 替换为实际标志名称

# 打开网络摄像头进行实时预测
cap = cv2.VideoCapture(0)

if not cap.isOpened():
print(&quot;Error: Could not open webcam.&quot;)
exit()

while True:
ret, frame = cap.read()
if not ret:
print(&quot;Error: Could not read frame.&quot;)
break

# 将帧从 BGR（OpenCV）转换为RGB (PIL)
frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

# 将 NumPy 数组转换为 PIL 图像
pil_image = Image.fromarray(frame_rgb)

# 预处理帧
input_image = preprocess(pil_image) # 使用 PIL 图像进行预处理
input_image = input_image.unsqueeze(0) # 添加批次维度

# 使用模型进行预测
with torch.no_grad():
output = model(input_image)

# 应用 softmax 获取概率
probabilities = torch.softmax(outputs, dim=1)

# 获取预测的类别和置信度
_, predict = torch.max(probabilities, 1)
confidence = probabilities[0][predicted].item() * 100 # 转换为百分比
label = class_names[predicted.item()]

# 显示结果和置信度
cv2.putText(frame, f&quot;Predicted: {label}, Confidence: {confidence:.2f}%&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
cv2.imshow(&quot;Sign Detection&quot;, frame)

# 按“q”退出
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

cap.release()
cv2.destroyAllWindows()


置信度始终很高，但由于标签错误，即使我在网络摄像头上显示 5 个手指，它仍然停留在零。
我觉得问题出在帧处理上，有人对此有任何见解吗？
感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</guid>
      <pubDate>Fri, 01 Nov 2024 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>训练模型来识别特定的 MTG 卡</title>
      <link>https://stackoverflow.com/questions/79149423/training-model-to-recognize-specific-mtg-cards</link>
      <description><![CDATA[我需要帮助。我有一个按集合、语言和名称排序的 450k MTG 卡的数据集。
最好的方法是什么，我应该使用什么工具来训练一个模型，该模型可以根据其集合、名称和语言识别卡，每张图片都是独一无二的，没有重复，所以如果我展示一千张图片中的一张，它应该能够分辨出来。我应该训练 3 个独立的模型吗，一个用于语言，一个用于卡名，最后一个用于集合？卡上重要的是集合符号和名称，我可能可以尝试读取卡的名称并获取集合符号并将其与集合符号列表进行匹配。 名称和设置符号已标记 问题是，卡的字体、边框和设置符号的位置并不总是相同的......我真的不知道该怎么做，我知道存在能够区分卡片的应用程序，我只是想要一些类似的东西。我将扫描卡片并裁剪它，所以我不需要模型来找到卡片并调整其大小等。这些将提前完成。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79149423/training-model-to-recognize-specific-mtg-cards</guid>
      <pubDate>Fri, 01 Nov 2024 21:03:12 GMT</pubDate>
    </item>
    <item>
      <title>如何向模型指定连续/分类特征？</title>
      <link>https://stackoverflow.com/questions/79149194/how-do-i-specify-continuous-categorical-features-to-the-model</link>
      <description><![CDATA[我有一个包含 90 个分类特征和 10 个连续特征的数据集。所有列都是整数，我使用 VectorAssembler 作为唯一的预处理步骤 - 这是一个已保存的中间体，不需要 StringIndexer、StandardScaler 等进一步处理。当我尝试拟合 DecisionTreeClassifier 时，我收到以下消息：
IllegalArgumentException：要求失败：DecisionTree 要求 maxBins (= 32) 至少与每个分类特征中的值数一样大，但分类特征 91 有 100 个值。考虑删除此特征和其他具有大量值的分类特征，或添加更多训练示例。

我如何向模型或 VectorAssembler 指定哪些字段是连续的，哪些是分类的？]]></description>
      <guid>https://stackoverflow.com/questions/79149194/how-do-i-specify-continuous-categorical-features-to-the-model</guid>
      <pubDate>Fri, 01 Nov 2024 19:29:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中将卷积转换为具有集成偏差的全连接模型</title>
      <link>https://stackoverflow.com/questions/79147359/how-to-convert-a-convolutional-to-a-fully-connected-ones-in-pytorch-with-integra</link>
      <description><![CDATA[如何向 Toepletiz 矩阵添加偏差？
以下方法仅显示如何转换核矩阵，使得
Y = A*X

但我想要类似的东西
Y = A*X + B
]]></description>
      <guid>https://stackoverflow.com/questions/79147359/how-to-convert-a-convolutional-to-a-fully-connected-ones-in-pytorch-with-integra</guid>
      <pubDate>Fri, 01 Nov 2024 08:34:09 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习中，是否应该从训练集和测试集中删除异常值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79147038/should-outliers-be-removed-from-both-training-and-test-sets-in-machine-learning</link>
      <description><![CDATA[我正在研究回归模型，需要对异常值处理有一定了解，特别是关于测试集。我读到过异常值应该只从训练数据中删除，但我想知道这对测试集的准确性和泛化能力有何影响。

将异常值留在测试集中是否有助于避免过度拟合？我想知道保留测试集异常值是否比删除它们更能提高泛化能力。

从训练集和测试集中删除异常值是否会导致数据泄露？从测试集中删除异常值会如何影响模型评估和性能指标？


这是我目前的方法：

将数据分为训练集和测试集。
使用 IQR 方法仅从训练集中删除异常值。
]]></description>
      <guid>https://stackoverflow.com/questions/79147038/should-outliers-be-removed-from-both-training-and-test-sets-in-machine-learning</guid>
      <pubDate>Fri, 01 Nov 2024 05:39:35 GMT</pubDate>
    </item>
    <item>
      <title>根据列数据对表标题进行分类</title>
      <link>https://stackoverflow.com/questions/79147000/classifying-table-headers-from-column-data</link>
      <description><![CDATA[我有一组包含数字和文本数据的 CSV；下面是示例：



公司 ID
公司名称
组 ID
货币
金额
...




8494494
Acme Inc
F942G
EUR
$1.56
...


9283422A
Walmart
XXH3F3
AUD
$5.64
...


...
...
...
...
...
...



我有一组单独的 CSV，其标题可能为标记错误。我想构建分类器来确定另一组中的列是公司 ID、公司名称还是组 ID。
这 3 列中的数据非常复杂，我无法仅使用正则表达式来解决这个问题；但是，从目测来看，我相信可以训练一个简单的分类器来区分它们（并且对其他杂项数据（例如“AUD”）返回 False）。
这里最好使用哪种模型？我考虑过将这些字符串传递给 BERT，然后在嵌入之上构建一个分类器。我也考虑过只在字符级别训练 RNN。
~
PS：我还有两个更复杂的想法：

将这些字符串传递给 NER 模型是否合适？或者说，由于这里没有太多“顺序”信息，这样做是不是有点过头了？
另一个想法是采用“word2vec”方法并尝试创建我自己的自定义嵌入，但使用我的训练数据将不同列的嵌入“推”得彼此远离。
]]></description>
      <guid>https://stackoverflow.com/questions/79147000/classifying-table-headers-from-column-data</guid>
      <pubDate>Fri, 01 Nov 2024 05:09:46 GMT</pubDate>
    </item>
    <item>
      <title>通过 ART2 对音频信号进行在线聚类</title>
      <link>https://stackoverflow.com/questions/79145752/online-clustering-of-audio-signals-via-art2</link>
      <description><![CDATA[我为 ART2 聚类编写了此代码，但它不起作用。我期望输出的是真实值与预测值的图表。需要帮助找出它不起作用的原因以及我可以做些什么来修复它。此外，需要输出准确率超过 90% 的输出。
这是特征生成的代码：
import librosa
import numpy as np
import math
import pandas as pd
from art2 import Art2

def load_audio(file_path):
# 加载音频文件
audio_signal, sample_rate = librosa.load(
file_path, sr=None) # 使用原始采样率
return audio_signal, sample_rate

def extract_features(audio_signal, sample_rate, n_fft):
mfccs = librosa.feature.mfcc(
y=audio_signal, sr=sample_rate, n_mfcc=13, n_fft=n_fft)
mfsc = librosa.feature.melspectrogram(
y=audio_signal, sr=sample_rate, n_mels=40, n_fft=n_fft)
return mfccs, mfsc

def divide_into_frames(audio_signal, frame_size, sample_freq, override):
# 将 frame_size ms 转换为样本
frame_length = int(frame_size * sample_freq / 1000)
hop_size = math.floor(frame_length * (1 - override))
# 创建重叠帧
frames = librosa.util.frame(
audio_signal, frame_length=frame_length, hop_length=hop_size)
return frames.T # 转置以使帧作为行

def main():

# 配置变量
mp3_file = &quot;dataset/trimmed_crowd_talking.mp3&quot;
frame_size = 10 # 每帧的样本数
overlap = 0.1 # 下一帧要移动的样本数

n_fft = 256
# 加载音频并创建帧
audio_signal, sample_rate = load_audio(mp3_file)
frames = divide_into_frames(audio_signal, frame_size, sample_rate, override)

mfccs_list = []
mfsc_list = []

for frame in frames:
mfccs, mfsc = extract_features(frame, sample_rate, n_fft)
mfccs_list.append(mfccs.flatten()) # 展平为 1D 数组
mfsc_list.append(mfsc.flatten()) # 展平为 1D 数组

# 创建 DataFrame
index = np.arange(len(mfccs_list))
# 计算以秒为单位的时间戳
timestamps = (index * (len(frames[0]) / sample_rate)).round(2)
df = pd.DataFrame(mfccs_list, index=index)
df.columns = [f&#39;MFCC_{i}&#39; for i in range(df.shape[1])]

# 添加时间戳
df.insert(0, &#39;Index&#39;, index)
df.insert(1, &#39;Timestamp&#39;, timestamps)

# 添加 MFSC 作为附加列
mfsc_df = pd.DataFrame(mfsc_list, index=index)
mfsc_df.columns = [f&#39;MFSC_{i}&#39; for i in range(mfsc_df.shape[1])]
combined_df = pd.concat([df, mfsc_df], axis=1)

# 保存为 CSV
#combined_df.to_csv(&quot;output.csv&quot;, index=False)

print(combined_df.head())

if __name__ == &quot;__main__&quot;:
main()

这是 ART2 Clustering 的代码：
class Art2:
def __init__(self, max_clusters, vigilance_threshold, creation_buffer_size):
self.max_clusters = max_clusters
self.vigilance_threshold = vigilance_threshold
self.creation_buffer_size = creation_buffer_size
self.num_clusters = 0
self.cluster_weights = []
self.cluster_creation_buffer = 0
return

# 计算到已经存在的簇的曼哈顿距离
def distance_2_clusters(self, input_features):
distances = np.sum(
np.abs(self.cluster_weights - input_features), axis=1)
返回距离

def process_new_sample(self, input_features):
if self.num_clusters &gt; 0：
距离 = self.distance_2_clusters（输入特征）

best_cluster_index = np.argmin（距离）
best_distance = distances[best_cluster_index]

如果best_distance &lt;= self.vigilance_threshold：
self.update_cluster（best_cluster_index，输入特征）
self.cluster_creation_buffer = 0
否则：
self.cluster_creation_buffer += 1

如果self.cluster_creation_buffer &gt;= self.creation_buffer_size：
如果self.num_clusters &lt; self.max_clusters:
self.create_new_cluster(input_features)
else:
print(&quot;已达到最大簇数。&quot;)
self.cluster_creation_buffer = 0

def create_new_cluster(self, input_features):
self.cluster_weights.append(input_features)
self.num_clusters += 1

def update_cluster(self, cluster_index, input_features):
# 简单平均权重更新；您可能希望使用更复杂的方法
self.cluster_weights[cluster_index] = (
self.cluster_weights[cluster_index] + input_features
) / 2

def cluster_data_as_stream():
return

我期望得到准确率超过 90% 的地面实况与预测值之间的图表。]]></description>
      <guid>https://stackoverflow.com/questions/79145752/online-clustering-of-audio-signals-via-art2</guid>
      <pubDate>Thu, 31 Oct 2024 17:23:28 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>训练 IP-Adapter plus 模型后的推理错误</title>
      <link>https://stackoverflow.com/questions/79140091/inference-error-after-training-an-ip-adapter-plus-model</link>
      <description><![CDATA[我从 https://github.com/tencent-ailab/IP-Adapter 下载了软件包
运行命令来训练 IP-Adapter plus 模型（输入：文本 + 图像，输出：图像）：
accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; \
tutorial_train_plus.py \
--pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; \
--image_encoder_path=&quot;models/image_encoder/&quot; \
--data_json_file=&quot;assets/prompt_image.json&quot; \
--data_root_path=&quot;assets/train/&quot; \
--mixed_precision=&quot;fp16&quot; \
--resolution=512 \
--train_batch_size=2 \
--dataloader_num_workers=4 \
--learning_rate=1e-04 \
--weight_decay=0.01 \
--output_dir=&quot;out_model/&quot; \
--save_steps=3

训练过程中，出现提示，但训练可以继续：
已删除共享张量 {&#39;adapter_modules.27.to_k_ip.weight&#39;, &#39;adapter_modules.1.to_v_ip.weight&#39;, &#39;adapter_modules.31.to_k_ip.weight&#39;, &#39;adapter_modules.15.to_k_ip.weight&#39;, &#39;adapter_modules.31.to_v_ip.weight&#39;, &#39;adapter_modules.11.to_k_ip.weight&#39;, &#39;adapter_modules.23.to_k_ip.weight&#39;, &#39;adapter_modules.3.to_k_ip.weight&#39;, &#39;adapter_modules.25.to_v_ip.weight&#39;, &#39;adapter_modules.21.to_k_ip.weight&#39;, &#39;adapter_modules.17.to_v_ip.weight&#39;, &#39;adapter_modules.13.to_k_ip.weight&#39;, &#39;adapter_modules.17.to_k_ip.weight&#39;, &#39;adapter_modules.19.to_v_ip.weight&#39;, &#39;adapter_modules.13.to_v_ip.weight&#39;, &#39;adapter_modules.7.to_v_ip.weight&#39;, &#39;adapter_modules.7.to_k_ip.weight&#39;, &#39;adapter_modules.29.to_k_ip.weight&#39;, &#39;adapter_modules.3.to_v_ip.weight&#39;, &#39;adapter_modules.5.to_v_ip.weight&#39;, &#39;adapter_modules.21.to_v_ip.weight&#39;, &#39;adapter_modules.5.to_k_ip.weight&#39;, &#39;adapter_modules.23.to_v_ip.weight&#39;, &#39;adapter_modules.25.to_k_ip.weight&#39;, &#39;adapter_modules.1.to_k_ip.weight&#39;, &#39;adapter_modules.9.to_v_ip.weight&#39;, &#39;adapter_modules.9.to_k_ip.weight&#39;, &#39;adapter_modules.15.to_v_ip.weight&#39;, &#39;adapter_modules.27.to_v_ip.weight&#39;, &#39;adapter_modules.29.to_v_ip.weight&#39;, &#39;adapter_modules.19.to_k_ip.weight&#39;, &#39;adapter_modules.11.to_v_ip.weight&#39;}。这应该没问题，但请检查重新加载时是否收到任何警告

训练完成后，转换权重以生成 ip_adapter.bin，然后使用此文件中的以下模型路径运行推理代码 ip_adapter-plus_demo.py：
base_model_path = &quot;SG161222/Realistic_Vision_V4.0_noVAE&quot;
vae_model_path = &quot;stabilityai/sd-vae-ft-mse&quot;
image_encoder_path = &quot;models/image_encoder&quot;
ip_ckpt = &quot;out_model/demo_plus_checkpoint/ip_adapter.bin&quot;

显示错误：
raise RuntimeError(&#39;Error(s) in loading state_dict for {}:\n\t{}&#39;.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
state_dict 中缺少键：&quot;1.to_k_ip.weight&quot;, &quot;1.to_v_ip.weight&quot;, &quot;3.to_k_ip.weight&quot;, &quot;3.to_v_ip.weight&quot;, &quot;5.to_k_ip.weight&quot;, &quot;5.to_v_ip.weight&quot;, &quot;7.to_k_ip.weight&quot;, &quot;7.to_v_ip.weight&quot;, &quot;9.to_k_ip.weight&quot;, &quot;9.to_v_ip.weight&quot;, “11.to_k_ip.weight”, “11.to_v_ip.weight”, “13.to_k_ip.weight”, “13.to_v_ip.weight”, “15.to_k_ip.weight”, “15.to_v_ip.weight”, “17.to_k_ip.weight”, “17.to_v_ip.weight”, “19.to_k_ip.weight”, “19.to_v_ip.weight”, “21.to_k_ip.weight”, “21.to_v_ip.weight”, “23.to_k_ip.weight”, “23.to_v_ip.weight”, &quot;25.to_k_ip.weight&quot;, &quot;25.to_v_ip.weight&quot;, &quot;27.to_k_ip.weight&quot;, &quot;27.to_v_ip.weight&quot;, &quot;29.to_k_ip.weight&quot;, &quot;29.to_v_ip.weight&quot;, &quot;31.to_k_ip.weight&quot;, &quot;31.to_v_ip.weight&quot;.

什么步骤出错导致此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79140091/inference-error-after-training-an-ip-adapter-plus-model</guid>
      <pubDate>Wed, 30 Oct 2024 07:32:22 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据框中的单个列进行单列编码？</title>
      <link>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</link>
      <description><![CDATA[我有一个名为“vehicles”的数据框，它有 8 列。其中 7 列是数字，但名为“Car_name”的列在数据框中是索引 1，是分类的。我需要对其进行编码
我试过这个代码，但不起作用
ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() 获得了一个意外的关键字参数“categorical_features”

然而，这在我使用的 YouTube 视频中运行良好。]]></description>
      <guid>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</guid>
      <pubDate>Tue, 22 Oct 2024 15:12:04 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 yolo 自定义模型时，data.yaml 文件中的相对路径出现问题</title>
      <link>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</link>
      <description><![CDATA[我正在尝试创建一个训练管道，以使用用户输入的带标签图像来训练自定义 yolov9 模型。
我遇到一个问题，如果我让我的 data.yaml 文件使用相对路径，我会收到错误：
RuntimeError：数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”错误
数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”图像未找到，缺少路径“C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val”

更奇怪的是，错误路径提及，
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

不是存在或正在任何地方请求的路径。实际路径是
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

由于某种原因，它重复了路径的第一部分 3 次。
这是 data.yaml 文件：
path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

这是开始训练的代码：
def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
# 指定训练运行的保存目录
save_dir = &#39;OIT_model/customOIT/yolocustomtrainoutput&#39;
if os.path.exists(save_dir):
for file in os.listdir(save_dir):
file_path = os.path.join(save_dir, file)
if os.path.isfile(file_path) or os.path.islink(file_path):
os.unlink(file_path)
elif os.path.isdir(file_path):
shutter.rmtree(file_path)
os.makedirs(save_dir, exist_ok=True)
model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
返回
train_custom_dataset_yolo(&#39;OIT_model/customOIT/customdatasetyolo/data.yaml&#39;, epochs=1,imgsz=64, verbose=True)

然而，非常奇怪的是，当我用绝对路径替换相对路径时，如下所示：
path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

训练没有问题。对于我来说，使用绝对路径不是一个选择，因为这个应用程序需要在其他机器上可重现。]]></description>
      <guid>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</guid>
      <pubDate>Thu, 10 Oct 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>更改 Keras train_on_batch() 的详细程度？</title>
      <link>https://stackoverflow.com/questions/75401761/change-verbosity-of-keras-train-on-batch</link>
      <description><![CDATA[我正在使用 Keras 的 train_on_batch() 命令训练 GAN。这与 Keras 的 fit() 非常相似。但是，在 fit() 的文档中，有一个 verbose 参数，它会更改将进度条打印到控制台的频率。
我的模型有很多批次，因此它会将大量进度条打印到命令行。不幸的是，train_on_batch() 没有 verbose 参数。有没有解决方法？是否有我可以设置的 Keras 全局变量/环境变量？我不想禁止我的程序打印到控制台，我只想更改特定的 train_on_batch() 的详细程度。
需要澄清的是，我直接从 Keras 包中使用 Keras，而不是使用 tf.keras。]]></description>
      <guid>https://stackoverflow.com/questions/75401761/change-verbosity-of-keras-train-on-batch</guid>
      <pubDate>Thu, 09 Feb 2023 16:41:49 GMT</pubDate>
    </item>
    <item>
      <title>带有值向量的回归模型的 pytorch 损失函数</title>
      <link>https://stackoverflow.com/questions/68370248/pytorch-loss-function-for-regression-model-with-a-vector-of-values</link>
      <description><![CDATA[我正在训练 CNN 架构以使用 PyTorch 解决回归问题，其中我的输出是一个包含 25 个值的张量。输入/目标张量可以是全零，也可以是 sigma 值为 2 的高斯分布。4 个样本批次的示例如下：
[[0.13534, 0.32465, 0.60653, 0.8825, 1.0000, 0.88250,0.60653, 0.32465, 0.13534, 0.043937, 0.011109, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0., 0., 0.13534, 0.32465, 0.60653, 0.8825, 1.0000, 0.88250,0.60653, 0.32465, 0.13534, 0.043937, 0.011109, 0., 0., 0., 0., 0., 0., 0., 0.], 0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.13534、0.32465、0.60653、0.8825、1.0000、 0.88250,0.60653, 0.32465, 0.13534 ],
[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]

我的问题是如何为模型设计一个损失函数，有效地学习具有 25 个值的回归输出。
我尝试了两种类型的损失，torch.nn.MSELoss() 和 torch.nn.MSELoss()-torch.nn.CosineSimilarity()。它们有点管用。但是，有时网络很难收敛，特别是当有大量样本全为“零”时，这会导致网络输出一个包含所有 25 个小值的向量。
我的问题是，还有其他我们可以尝试的损失吗？]]></description>
      <guid>https://stackoverflow.com/questions/68370248/pytorch-loss-function-for-regression-model-with-a-vector-of-values</guid>
      <pubDate>Tue, 13 Jul 2021 23:15:32 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中实现的vgg16的训练loss并没有减少</title>
      <link>https://stackoverflow.com/questions/57605094/the-training-loss-of-vgg16-implemented-in-pytorch-does-not-decrease</link>
      <description><![CDATA[我想在 pytorch 中尝试一些小例子，但训练损失在训练中并没有减少。
这里提供了一些信息：

模型是 vgg16，由 13 个卷积层和 3 个密集层组成。
数据是 pytorch 中的 cifar100。
我选择交叉熵作为损失函数。

代码如下
# encoding: utf-8

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn. functional as F
import torchvision.transforms as transforms
import torchvision

import numpy as np

class VGG16(torch.nn.Module):
def __init__(self, n_classes):
super(VGG16, self).__init__()

# 构建模型
self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)
self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)
self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)
self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)
self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)
self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)
self.conv3_3 = nn.Conv2d(256, 256, 3, 填充=1)
self.conv4_1 = nn.Conv2d(256, 512, 3, 填充=1)
self.conv4_2 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv4_3 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_1 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_2 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_3 = nn.Conv2d(512, 512, 3, 填充=1)

self.fc6 = nn.Linear(512, 512)
self.fc7 = nn.Linear(512, 512)
self.fc8 = nn.Linear(512, n_classes)

def forward(self, x):
x = F.relu(self.conv1_1(x))
x = F.relu(self.conv1_2(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv2_1(x))
x = F.relu(self.conv2_2(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv3_1(x))
x = F.relu(self.conv3_2(x))
x = F.relu(self.conv3_3(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv4_1(x))
x = F.relu(self.conv4_2(x))
x = F.relu(self.conv4_3(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv5_1(x))
x = F.relu(self.conv5_2(x))
x = F.relu(self.conv5_3(x))
x = F.max_pool2d(x, (2, 2))

x = x.view(-1, self.num_flat_features(x))

x = F.relu(self.fc6(x))
x = F.relu(self.fc7(x))
x = self.fc8(x)
返回x

def num_flat_features(self, x):
size = x.size()[1:]

num_features = 1
for s in size:
num_features *= s
return num_features

if __name__ == &#39;__main__&#39;:

BATCH_SIZE = 128
LOG_INTERVAL = 5

# 数据
transform = transforms.Compose([
transforms.ToTensor()
])

trainset = torchvision.datasets.CIFAR100(
root=&#39;./data&#39;,
train=True,
download=True,
transform=transform
)

testset = torchvision.datasets.CIFAR100(
root=&#39;./data&#39;,
train=False,
download=True,
transform=transform
)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

# 模型
vgg16 = VGG16(100)
vgg16.cuda()

# 优化器
optimizer = optim.SGD(vgg16.parameters(), lr=0.01)

# 损失
criterion = nn.CrossEntropyLoss()

print(&#39;———— Train Start —————&#39;)
for epoch in range(20):
running_loss = 0.
for step, (batch_x, batch_y) in enumerate(trainloader):
batch_x, batch_y = batch_x.cuda(), batch_y.cuda()

# 
o​​ptimizer.zero_grad()

output = vgg16(batch_x)
loss =标准（输出，batch_y）
损失。向后（）
优化器。步骤（）

running_loss + = loss.item（）

如果步骤 % LOG_INTERVAL == 0：
打印（&#39;[%d，%4d] 损失：%.4f&#39; % (epoch，步骤，running_loss / LOG_INTERVAL))
running_loss = 0。

def test（）：
打印（&#39;———— 测试开始 ————&#39;）
正确 = 0
总计 = 0

# 
使用 torch.no_grad（）：
对于 test_x，test_y 在 testloader 中：
图像，标签 = test_x.cuda（），test_y.cuda（）
输出 = vgg16（图像）
_，预测 = torch.max（输出数据，1）
总计 + = 标签。大小（0）
正确 + = (预测 == 标签).sum（）。item（）

准确率 = 100 * 正确 / 总计
print(&#39;网络准确率为：%.4f %%&#39; % 准确率)
print(&#39;———— 测试完成 ————&#39;)

test()

print(&#39;———— 训练完成 —————&#39;)


损失保持在 4.6060 左右，从未减少。我尝试了不同的学习率，但没有效果。]]></description>
      <guid>https://stackoverflow.com/questions/57605094/the-training-loss-of-vgg16-implemented-in-pytorch-does-not-decrease</guid>
      <pubDate>Thu, 22 Aug 2019 08:27:53 GMT</pubDate>
    </item>
    </channel>
</rss>