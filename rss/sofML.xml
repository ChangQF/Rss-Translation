<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 03 Apr 2024 21:12:56 GMT</lastBuildDate>
    <item>
      <title>为具有多个特征和不相关目标的 LTSM 准备数据最终会预测平均值</title>
      <link>https://stackoverflow.com/questions/78270152/preparing-data-for-ltsm-with-multiple-features-and-uncorrelated-target-ends-up-i</link>
      <description><![CDATA[我正在尝试使用 LTSM 进行回归来预测微天气事件，例如下一小时的预期温度。这是 LTSM 的一个练习。
数据：
我有一个包含 10000 个样本（行）的 CSV。
每行都有一个时间戳和 3 个特征
(X&#39;s)：日期时间、海拔高度、湿度、压力
还有标签
(Y&#39;s)：日期时间、温度
我的理论是，在我训练 LTSM 模型足够的样本后，它应该能够预测下一小时的温度，但我没有将温度用作特征，而只是用作标签。
这是将单个样本组织到 LTSM 模型的方式（使用 Pytorch）：
[海拔高度(t)、湿度(t)、压力(t)]
[海拔高度(t+1)、湿度(t+1)、气压(t+1)]
[海拔高度(t+2)、湿度(t+2)、气压(t+2)]

这个样本的标签是
&lt;前&gt;&lt;代码&gt;[温度(t+3)]

下一个示例类似，但显然从 t+1 开始，依此类推。
请注意，我正在进行数据缩放（最小最大）和训练测试分割。
问题是我的模型只是预测所有温度标签的平均值，那么我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78270152/preparing-data-for-ltsm-with-multiple-features-and-uncorrelated-target-ends-up-i</guid>
      <pubDate>Wed, 03 Apr 2024 19:45:09 GMT</pubDate>
    </item>
    <item>
      <title>抛物线拟合肺部分段区域</title>
      <link>https://stackoverflow.com/questions/78269913/parabola-fitting-over-lungs-segmented-region</link>
      <description><![CDATA[我正在尝试对胸部 X 光图像进行肺部分割
&lt;前&gt;&lt;代码&gt;导入cv2
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 sklearn.cluster 导入 KMeans

def 边距(img, margin_percent=10):
    h, w = img.shape
    margin_x = int(w * margin_percent / 100)
    margin_y = int(h * margin_percent / 100)
    裁剪 = img[margin_y:h-margin_y, margin_x:w-margin_x]
    返回裁剪

def 进程（img）：
    flat_img = img.flatten().reshape(-1, 1)
    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)
    标签 = kmeans.fit_predict(flat_img)
    中心= kmeans.cluster_centers_
    如果 np.mean(centers[1]) &gt; np.mean(中心[0]):
        标签 = 1 - 标签
    聚集 = np.reshape(标签, img.shape)
    clustered_binary = np.uint8(clustered * 255)
    返回 clustered_binary

def segment_and_filter(clustered_img):
    ret, thresh = cv2.threshold(clustered_img, 0, 255, cv2.THRESH_BINARY)
    如果 cv2.__version__[0] &gt; ‘3’：
        轮廓， _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    别的：
        _，轮廓，_ = cv2.findContours（阈值，cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_SIMPLE）
    
    最大面积 = 0
    最大轮廓=无
    对于轮廓中的轮廓：
        面积 = cv2.contourArea(轮廓)
        如果面积&gt;最大面积：
            最大面积=面积
            最大轮廓 = 轮廓
    
    结果 = np.zeros_like(clustered_img)
    cv2.drawContours(结果, [最大轮廓], -1, 255, 厚度=cv2.FILLED)
    返回结果，最大轮廓

def process_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    裁剪后的img =边距（img）
    clustered_img = 进程(cropped_img)
    过滤结果，最大轮廓=分段和过滤器（簇状图像）
    
    组合图像 = np.hstack((filtered_result[:, :filtered_result.shape[1] // 2],
                                cv2.flip(filtered_result[:, :filtered_result.shape[1] // 2], 1)))
    
    返回组合图像

image_path =“/kaggle/input/chest-xray-pneumonia/chest_xray/test/NORMAL/IM-0105-0001.jpeg”
结果图像=过程图像（图像路径）

plt.figure(figsize=(9, 3))
plt.imshow(result_image, cmap=&#39;灰色&#39;)
plt.title(&#39;合并肺&#39;)
plt.axis(&#39;on&#39;)
plt.show()


上面的代码给出了这个输出
到目前为止一切都很好
现在我想拟合一条覆盖两个肺部区域的抛物线
尝试了多次
但这没有用
你能帮我吗？
在单肺上尝试过，但对两个肺都不起作用

预期
]]></description>
      <guid>https://stackoverflow.com/questions/78269913/parabola-fitting-over-lungs-segmented-region</guid>
      <pubDate>Wed, 03 Apr 2024 18:57:31 GMT</pubDate>
    </item>
    <item>
      <title>colab 中的训练器功能出现错误</title>
      <link>https://stackoverflow.com/questions/78269285/getting-error-with-trainer-function-in-colab</link>
      <description><![CDATA[我在 colab 中遇到此错误，您能帮我解决这个问题吗，
1906 f“将 Trainer 与 PyTorch 一起使用需要 accelerate&gt;={ACCELERATE_MIN_VERSION}：“
第1907章 1907
ImportError：将 Trainer 与 PyTorch 一起使用需要 accelerate&gt;=0.21.0：请运行 pip install Transformers[torch]  或 pip install Accelerator -U
&lt;小时/&gt;
注意：如果您的导入由于缺少软件包而失败，您可以
使用 !pip 或 !apt 手动安装依赖项。
要查看安装一些常见依赖项的示例，请单击
“开放示例”下面的按钮。”我尝试运行 pip install Transformers[torch] 或 pip install Accelerate -U，但它不起作用...任何帮助将不胜感激。
谢谢
沙布南
我尝试使用我的数据集微调拥抱脸部模型...这样做时出现错误]]></description>
      <guid>https://stackoverflow.com/questions/78269285/getting-error-with-trainer-function-in-colab</guid>
      <pubDate>Wed, 03 Apr 2024 16:57:24 GMT</pubDate>
    </item>
    <item>
      <title>我的 CNN 做错了什么，它的准确率提升得这么慢</title>
      <link>https://stackoverflow.com/questions/78269213/what-am-i-doing-wrong-with-my-cnn-that-it-is-gaining-accuracy-so-slowly</link>
      <description><![CDATA[我正在使用这个 CNN 来检测脑电图扫描中的信息。它获得准确性的速度非常缓慢，我想知道我是否遗漏了任何层中的任何内容或做错了什么
类网络（模块）：
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.cnn_layers = 顺序（
            Conv1d(1,14, kernel_size=5, padding=1),
            BatchNorm1d(14),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),

        ）
        self.cnn_layer2 = 顺序(
            Conv1d(14, 10,kernel_size=5, 填充=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            辍学率（0.2），
        ）
        self.cnn_layer3 = 顺序（
            Conv1d(10, 10, kernel_size=5, 填充=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            辍学率（0.2），
        ）
        self.线性_层1 = 顺序(
            线性（in_features = 35660，out_features = 3500），
            BatchNorm1d(3500),
            LeakyReLU(0.1),
            辍学率(0.2)

        ）
        self. Linear_layer2 = 顺序（
            线性（输入特征=3500，输出特征=2500），
            BatchNorm1d(2500),
            LeakyReLU(0.1),
            辍学率(0.2)
        ）
        self. Linear_layer3 = 顺序（
            线性（输入特征=2500，输出特征=250），
            BatchNorm1d(250),
            LeakyReLU(0.1),
            辍学率(0.2)
        ）
        self. Linear_layer4 = 顺序（
            线性（输入特征=250，输出特征=10）
        ）
        self.logsoft = 顺序(
            LogSoftmax(暗淡=1)
        ）
        self.展平 = 顺序（
            Flatten() # 可能需要更改
        ）

    def 前向（自身，x）：
        x = self.cnn_layers(x)

        x = self.cnn_layer2(x)

        x = self.cnn_layer3(x)

        x = self.展平(x)
        x = self. Linear_layer1(x)
        x = self. Linear_layer2(x)
        x = self. Linear_layer3(x)
        x = self. Linear_layer4(x)
        x = self.logsoft(x)
        返回x



模型=网络()

#通过获取 255 列并将它们分组为 14 * 255 个通道来构建数据集
类自定义数据集（）：
    def __init__(self, csv_file, 标签, 变换=无):
        self.df = csv_file
        self.transform = 变换
        self.label = 标签

    def __len__(自身):
        返回 self.df.shape[0]

    def __getitem__(自身，索引)：
        扫描 = (self.df[索引])
        标签 = self.label[索引]
        如果自我变换：
            扫描 = self.transform(扫描)

        返回扫描件、标签

训练数据集 = 行
打印（train_dataset.shape）

train_dataset = CustomDataSet(csv_file=行，标签=(标签))

优化器= SGD(model.parameters(),lr=0.001,weight_decay=5.0e-5)
标准 = CrossEntropyLoss()
纪元数 = 500
train_loss_list = []
批量大小 = 500
train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)
对于范围内的纪元（num_epochs）：
    print(f&#39;历元 {epoch + 1}/{num_epochs}:&#39;, end=&#39; &#39;)
    训练损失 = 0

    # 批量迭代训练数据集
    总正确率 = 0
    样本总数 = 0
    模型.train()
    对于 i，枚举（train_loader）中的（扫描，标签）：
        # 为正在迭代的批次提取图像和目标标签



        # 计算模型输出和交叉熵损失

        输出=模型（扫描）
        打印（输出.形状）

        损失=标准（输出，标签）

        # 根据计算出的损失更新权重
        优化器.zero_grad()
        loss.backward()
        优化器.step()
        train_loss += loss.item()
        _, 预测 = torch.max(输出, 1)

        Total_ Correct += (预测==标签).sum().item()
        样本总数 += labels.size(0)
        # 每个纪元的打印损失
    准确度 = 100 * 总正确率 / 总样本数
    print(&quot;准确率：&quot;, 准确率)
    train_loss_list.append(train_loss / len(train_loader))
    print(f&quot;训练损失 = {train_loss_list[-1]}&quot;)

我尝试在每个层和丢弃层上添加批量标准化。每个时期都接受 20,000 次扫描的训练，但我可以访问 51,000 次，所以我可能会尝试使用更多数据。 100 个 epoch 后，准确率仅达到 13%。这是正常现象还是我犯了错误？]]></description>
      <guid>https://stackoverflow.com/questions/78269213/what-am-i-doing-wrong-with-my-cnn-that-it-is-gaining-accuracy-so-slowly</guid>
      <pubDate>Wed, 03 Apr 2024 16:45:14 GMT</pubDate>
    </item>
    <item>
      <title>我在测试模型时遇到未知层错误</title>
      <link>https://stackoverflow.com/questions/78268793/i-am-getting-unknown-layer-error-while-testing-a-model</link>
      <description><![CDATA[错误是这样的：
我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
2024-04-03 20:51:40.389067：我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
警告：tensorflow：来自 C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\tf_keras\src\losses.py:2976：名称 tf.losses.sparse_softmax_cross_entropy 已弃用。请改用 tf.compat.v1.losses.sparse_softmax_cross_entropy。
2024-04-03 20:51:47.709342: I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可以在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX512F AVX512_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
回溯（最近一次调用最后一次）：
文件“D:\image title\testing_caption_generator.py”，第 68 行，位于
模型 = 加载模型（
ValueError：未知层：“NotEqual”。请确保您使用的是 keras.utils.custom_object_scope 并且该对象包含在范围内。请参阅https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object 详细信息。
这是我的代码：
从 PIL 导入图像
将 matplotlib.pyplot 导入为 plt
导入argparse
进口泡菜
从 keras.models 导入 load_model
从 keras.layers 导入 Lambda
从 keras.applications.xception 导入 Xception
从 keras.preprocessing.sequence 导入 pad_sequences
从tensorflow.keras.preprocessing.text导入Tokenizer
from pickle import load # 从 pickle 模块导入 load 函数
导入tensorflow_hub作为集线器

# 定义自定义层
def NotEqual(x, y):
    将张量流导入为 tf
    返回 tf.math.not_equal(x, y)

# 定义用于提取特征、生成描述和其他必要实用程序的函数
def extract_features(文件名, 模型):
    尝试：
        图像 = Image.open(文件名)
    除了：
        print(&quot;错误：无法打开图像！请确保图像路径和扩展名正确&quot;)
    图像 = image.resize((299,299))
    图像 = np.array(图像)
    如果图像.shape[2] == 4:
        图像 = 图像[...,:3]
    图像 = np.expand_dims(图像, 轴=0)
    图像=图像/127.5
    图像 = 图像 - 1.0
    特征 = model.predict(图像)
    返回功能

def word_for_id(整数, 分词器):
    对于单词，在 tokenizer.word_index.items() 中索引：
        如果索引==整数：
            返回词
    返回无

defgenerate_desc（模型，分词器，照片，max_length）：
    in_text = &#39;开始&#39;
    对于范围内的 i（最大长度）：
        序列 = tokenizer.texts_to_sequences([in_text])[0]
        序列 = pad_sequences([序列], maxlen=max_length)
        pred = model.predict([照片, 序列], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, 分词器)
        如果单词为“无”：
            休息
        in_text += &#39; &#39; + 单词
        如果单词==&#39;结束&#39;：
            休息
    返回 in_text

ap = argparse.ArgumentParser()
ap.add_argument(&#39;-i&#39;, &#39;--image&#39;, required=True, help=“图像路径”)
args = vars(ap.parse_args())
img_path = args[&#39;图像&#39;]




#path = &#39;Flicker8k_Dataset/111537222_07e56d5a30.jpg&#39;
最大长度 = 32
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;,&quot;rb&quot;))
路径=&#39;模型/model_9.h5&#39;
模型 = 加载模型（
       （小路），
       custom_objects={&#39;KerasLayer&#39;:hub.KerasLayer}
）
xception_model = Xception(include_top=False, pooling=“avg”)

照片 = extract_features(img_path, xception_model)
img = Image.open(img_path)

描述 =generate_desc(模型、分词器、照片、max_length)
打印(“\n\n”)
打印（描述）
plt.imshow(img)```
]]></description>
      <guid>https://stackoverflow.com/questions/78268793/i-am-getting-unknown-layer-error-while-testing-a-model</guid>
      <pubDate>Wed, 03 Apr 2024 15:32:23 GMT</pubDate>
    </item>
    <item>
      <title>如何将 sklearn-crf 套件与文档而不是句子一起使用？</title>
      <link>https://stackoverflow.com/questions/78268442/how-can-i-use-sklearn-crf-suite-with-documents-and-not-sentences</link>
      <description><![CDATA[我想在文档上训练我的 crf 模型，而不是文档中所示的句子 https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html）。
在本例中，我具有以下结构：
&lt;前&gt;&lt;代码&gt;[
    [ # 文档 1
        [(&#39;word1&#39;, {&#39;feature1&#39;: &#39;value1&#39;, &#39;feature2&#39;: &#39;value2&#39;, ...}), (&#39;word2&#39;, {&#39;feature1&#39;: &#39;value3&#39;, &#39;feature2&#39;: &#39;value4&#39; , ...}), ...], # 短语 1
        [(&#39;word3&#39;, {&#39;feature1&#39;: &#39;value5&#39;, &#39;feature2&#39;: &#39;value6&#39;, ...}), (&#39;word4&#39;, {&#39;feature1&#39;: &#39;value7&#39;, &#39;feature2&#39;: &#39;value8&#39; , ...}), ...], # 短语 2
        ...
    ],
    [ # 文档 2
        ...
    ],
    ...
]

不幸的是，我收到以下消息：
 trainer.append(xseq, yseq)
  文件“pycrfsuite/_pycrfsuite.pyx”，第 312 行，位于 pycrfsuite._pycrfsuite.BaseTrainer.append 中
  文件“”，第 48 行，位于 vector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string 中
  文件“”，第 15 行，位于 string.from_py.__pyx_convert_string_from_py_std__in_string
类型错误：预期字节，找到列表

我的印象是crf不接受这样的格式。这种情况我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78268442/how-can-i-use-sklearn-crf-suite-with-documents-and-not-sentences</guid>
      <pubDate>Wed, 03 Apr 2024 14:35:07 GMT</pubDate>
    </item>
    <item>
      <title>创建用于 html 理解和数据提取的 AI 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78268338/create-a-ai-model-for-html-understanding-and-extraction-of-data</link>
      <description><![CDATA[我正在开发一个项目，要求是我应该制作一个脚本，可以从给定的链接获取包含特定数据的表。因为有很多网站的结构不同，有些有table标签，有些使用div，有些使用其他标签。
现在，我对机器学习之类的东西非常陌生。如果有人可以指导我如何实现这一目标或给我一个正确的路线图。我应该学习什么，什么对我这个项目有帮助。不过，我精通 python 和数据抓取。
数据非常相似，我认为这是可能的。我将向专家展示一些例子。
https://www.rockauto.com/en/moreinfo.php?pk=101750&amp;cc=0&amp;pt=1000587&amp;jsn=788&amp;optionchoice=0-0-8-1


像这样的表格和交换号码（如果有）
另一个例子是
https://www.aimsinduscial。 com.au/gates-9770-13a1955-green-stripe-belt-heavy-duty-en
]]></description>
      <guid>https://stackoverflow.com/questions/78268338/create-a-ai-model-for-html-understanding-and-extraction-of-data</guid>
      <pubDate>Wed, 03 Apr 2024 14:20:16 GMT</pubDate>
    </item>
    <item>
      <title>Liu 等人 [2022] 中将多时相卫星数据输入 Informer 模型的格式是什么？</title>
      <link>https://stackoverflow.com/questions/78268294/what-is-the-format-in-which-the-multitemporal-satellite-data-was-fed-to-the-info</link>
      <description><![CDATA[Liu et al [2022]，标题为基于水稻产量预测和模型解释在
使用 Transformer 方法的卫星和气候指标，使用 Informer 模型（由 Zhou 等人撰写） al) 使用 MODIS 数据（特别是 MOD13A2）进行水稻产量预测，可在 NASA 网站 上获取气候变量（可在此网站和（此网站）[https: //data.chc.ucsb.edu/products/CHIRPS-2.0/]) 作为功能，以及 各地区农作物产量统计数据作为地面实况数据的值。 Zhou等人的论文中提到的原始Informer模型的代码可以在（GitHub）上找到[https://github.com/zhonghaoyi/Informer2020]。
浏览完代码后，代码似乎将 csv 文件作为输入，其中指定列作为特征，其他指定列作为目标。为了调整多时相卫星数据以适合 csv 文件，Liu 等人说：
&lt;块引用&gt;
八个连续变量（即 NDVI、EVI、NIRV、SIF、Tmax、Tmin、Srad 和 Pr）在空间上聚合到区级别。最后，总共应用了 96 个特征来构建模型，其中包含 8 个连续变量，每个变量有 12 个时间间隔。

我的问题是，“聚合”到底是什么意思？也许他们的意思是这些值是所有像素的平均值？或者是某种其他形式的“聚合”？
我尝试过的：
我一直在寻找将卫星数据输入变压器模型的可能方法，视觉变压器（ViT）似乎经常用于处理卫星数据。然而，这仅适用于单个时间步数据。对于多时相数据（即具有多个时间步长的数据），出现了其他更复杂的方法，但 Liu 等人没有提到这些方法。由于我的目标是复制 Liu 等人的结果，因此使用如此复杂的方法是不可取的。]]></description>
      <guid>https://stackoverflow.com/questions/78268294/what-is-the-format-in-which-the-multitemporal-satellite-data-was-fed-to-the-info</guid>
      <pubDate>Wed, 03 Apr 2024 14:12:45 GMT</pubDate>
    </item>
    <item>
      <title>文本转换：训练模型从输入文件生成输出文件</title>
      <link>https://stackoverflow.com/questions/78268215/text-transformation-train-a-model-to-generate-output-files-from-input-files</link>
      <description><![CDATA[我目前正在开展一个项目，其中包括“翻译”文件从一种格式转换为另一种格式，我使用了带有映射的编程方法，该映射将输入文件中的每个模式与输出中方便的模式链接起来，编写映射和执行转换的类非常困难，此外，我每次我想要新的格式或调整时都应该编写一个新的，所以我决定用一个可以用数千个输入文件及其各自的输出进行尝试的模型来替换整个东西，这里有一个小例子可以给你一个想法：
来自输入文件的块：
DIM+1.800:0.400:500:CT+12
在输出中转换为 XML 标记：
 &lt;高度&gt;1.800&lt;\高度&gt;
    &lt;长度&gt;0.400&lt;\长度&gt;
    &lt;宽度&gt;500&lt;\宽度&gt;


所以基本上，第一行就是我们所说的“段”，它以 3 个字母的缩写开头，正如您所看到的，它在这里代表尺寸，它转换高度、长度和宽度，如图所示，输入文件是一系列像尺寸、日期、地址之类的段，输出可以是 XML 或其他东西，但我想模型的逻辑不会有很大不同，我可以将其调整为其他格式，这就是我现在所需要的是XML，这怎么办？？？
我对机器学习非常陌生，但我作为开发人员已经有足够的时间了，所以我尝试了一种严格的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78268215/text-transformation-train-a-model-to-generate-output-files-from-input-files</guid>
      <pubDate>Wed, 03 Apr 2024 14:01:01 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：使用 LRCN 改进可疑活动检测模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78267648/seeking-advice-improving-suspicious-activity-detection-model-using-lrcn</link>
      <description><![CDATA[我正在使用 LRCN 开发可疑活动检测模型，该模型有四个类别：跑步、步行、打斗和不打斗。然而，我在准确区分步行和跑步方面遇到了困难，导致准确率较低。
我正在考虑两种可能的解决方案，非常感谢您的见解：

数据增强：添加更多数据或旋转视频是否有助于提高分类准确性？

模型增强：我在两个选项之间左右为难：

集成情绪分析来分析活动的背景。这是否可行且有益，还是会使模型过于复杂？

实时视频分类：使用边界框实现实时视频分类系统来识别活动。此外，我有兴趣针对持枪等活动建立警报系统。我应该如何为此添加更多数据集和类？




我还在寻求有关任何预训练模型或资源的建议，以帮助提高我的模型的准确性和功能。
到目前为止准确率为 76%]]></description>
      <guid>https://stackoverflow.com/questions/78267648/seeking-advice-improving-suspicious-activity-detection-model-using-lrcn</guid>
      <pubDate>Wed, 03 Apr 2024 12:29:18 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试了所有测试并且得到了合理的分数时，为什么我的混淆矩阵是这样的？</title>
      <link>https://stackoverflow.com/questions/78266973/why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tests-and-i-got-reaso</link>
      <description><![CDATA[我正在使用 sklearn 的随机森林分类，除了混淆矩阵之外，我在所有方面都得到了不错的结果，这里是代码和结果
训练和测试的标签分布
火车组的大小
模型
训练模型的分数
这是问题
这不是我所期望的，特别是因为训练量仅为训练数据集中 677k 的训练量的 1/3，但在混淆矩阵中它只处理所有标签 0。 
模型：
导入时间
# 记录开始时间
开始时间 = 时间.time()

# 随机森林分类器
rf = 随机森林分类器()

# 定义参数网格
rf_param_grid = {&#39;n_estimators&#39;：[45]，&#39;标准&#39;：[&#39;熵&#39;]，&#39;max_深度&#39;：[30]}

# 网格搜索
rf_cv = GridSearchCV(rf, rf_param_grid, cv=7)
rf_cv.fit(X_train, y_train)

# 记录结束时间
结束时间 = time.time()

# 计算经过的时间
经过时间 = 结束时间 - 开始时间

# 打印结果
print(&quot;最佳成绩：&quot;, rf_cv.best_score_)
print(&quot;最佳参数：&quot;, rf_cv.best_params_)
print(&quot;经过时间:&quot;, elapsed_time, &quot;秒&quot;)

我在这里的每堂课都取得了超过 98% 的好成绩：
# 对训练数据进行预测
y_train_pred = rf_cv.predict(X_train)

# 计算准确率
准确度=准确度_得分（y_train，y_train_pred）

# 计算每个类别的准确率、召回率和 F1 分数
精度 = precision_score(y_train, y_train_pred, 平均值=无)
召回率=召回率（y_train，y_train_pred，平均值=无）
f1 = f1_score(y_train, y_train_pred, 平均值=无)

# 计算宏观平均精度、召回率和 F1 分数
Macro_ precision = precision_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)
宏召回 = 召回分数(y_train, y_train_pred, 平均值=&#39;宏&#39;)
Macro_f1 = f1_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)

# 打印评估指标
print(“准确度：”, 准确度)
print(&quot;精度（0、1、2 类）：&quot;, precision)
print(“召回（0、1、2 类）：”，召回）
print(&quot;F1-分数（0、1、2 类）：&quot;, f1)
print(&quot;宏观平均精度：&quot;, macro_ precision)
print(&quot;宏观平均召回率：&quot;, Macro_recall)
print(“宏观平均 F1 分数：”, Macro_f1)

混淆矩阵，其中不显示除 0 类之外的所有标签
# 生成混淆矩阵
conf_matrix = fusion_matrix(y_train, y_train_pred)

# 定义类标签
class_labels = [&#39;类别 0&#39;, &#39;类别 1&#39;, &#39;类别 2&#39;]

# 使用类标签可视化混淆矩阵
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt=“d”, cmap=“蓝调”, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel(&#39;预测标签&#39;)
plt.ylabel(&#39;真实标签&#39;)
plt.title(&#39;混淆矩阵&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78266973/why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tests-and-i-got-reaso</guid>
      <pubDate>Wed, 03 Apr 2024 10:21:59 GMT</pubDate>
    </item>
    <item>
      <title>m=365 时 PM10 浓度预测的 SARIMA 模型存在问题</title>
      <link>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</link>
      <description><![CDATA[我正在尝试构建 SARIMA（季节性自回归综合移动平均线）模型，用于根据五年的数据预测 PM10 浓度。但是，当我将季节性参数 m 设置为 365 时，我的代码似乎无法运行。
有人可以解释一下为什么我的代码没有在 m=365 下运行并提出潜在的解决方案吗？
提前致谢！
# 这是我的代码片段：
## 将数据集拆分为训练集和测试集

    `train_size = int(len(Alipur_df) * 0.8) # 80% 训练，20% 测试`
    `训练，测试 = Alipur_df[:train_size], Alipur_df[train_size:]`

## 将训练 DataFrame 转换为 numpy 数组

    `train_values = train[&#39;Alipur&#39;].values`
    `test_values = test[&#39;Alipur&#39;].values`

## 使用 auto_arima 找到 SARIMA 的最佳参数

    `auto_model = auto_arima(train[&#39;Alipur&#39;],seasonal=True,stationary=True,m=365,trac
]]></description>
      <guid>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</guid>
      <pubDate>Wed, 03 Apr 2024 06:23:04 GMT</pubDate>
    </item>
    <item>
      <title>使用非序列数据时，LSTM 给出更概括的结果，准确率为 89%，而使用序列数据时准确率为 64%</title>
      <link>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</link>
      <description><![CDATA[我正在研究时间序列分类。我使用了下面给出的两个预处理步骤

时间序列数据集 --&gt;切片时间序列 ---&gt;训练-验证-测试-分割 --&gt;模型训练
准确度——90%

时间序列数据集 --&gt;训练-验证-测试-分割 --&gt;独立切片训练/测试/验证 --&gt;模型训练
准确度 -- 64%


我目前正在努力为我的项目获取顺序概率分布，并已进入第二步。但是，我得到的准确度结果低于预期，甚至低于第一步。我已经调整了超参数并解决了类别不平衡的问题以消除偏差，但准确率没有提高到超过 64%。谁能提供一些关于为什么会发生这种情况的见解？
对于训练验证测试拆分，我使用 sklearn
x_main, x_test, y_main, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y,shuffle=True)
x_train, x_val, y_train, y_val = train_test_split(x_main, y_main, test_size= 0.1, random_state=42,stratify=y_main,shuffle=True)
至少我预计第二步的结果接近 80-85%]]></description>
      <guid>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</guid>
      <pubDate>Wed, 03 Apr 2024 05:05:10 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“llama_index.llms”（未知位置）导入名称“HuggingFaceInferenceAPI”</title>
      <link>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</link>
      <description><![CDATA[想要导入 HuggingFaceInferenceAPI。
从 llama_index.llms 导入 HugggingFaceInferenceAPI

llama_index.llms 文档没有 HuggingFaceInferenceAPI 模块。有人有这方面的更新吗？]]></description>
      <guid>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</guid>
      <pubDate>Sun, 31 Mar 2024 14:21:19 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 scikit learn 中将装袋技术与两种不同的算法结合使用？</title>
      <link>https://stackoverflow.com/questions/26283045/is-it-possible-to-use-the-bagging-technique-with-two-different-algorithms-in-sci</link>
      <description><![CDATA[我想知道是否可以将装袋技术与两种不同的算法（例如逻辑回归和随机森林）或（几乎）任何其他算法一起使用？
我需要一些能够返回平均概率或组合概率和预测的东西。
这将用于分类任务。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/26283045/is-it-possible-to-use-the-bagging-technique-with-two-different-algorithms-in-sci</guid>
      <pubDate>Thu, 09 Oct 2014 16:06:55 GMT</pubDate>
    </item>
    </channel>
</rss>