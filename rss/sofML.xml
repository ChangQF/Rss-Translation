<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Dec 2023 21:12:47 GMT</lastBuildDate>
    <item>
      <title>模拟变量（比例）的统计或机器学习方法</title>
      <link>https://stackoverflow.com/questions/77623084/statistical-or-machine-learning-approach-for-simulating-variable-proportion</link>
      <description><![CDATA[我目前面临着分析来自电力驱动生产设施的客户满意度数据的挑战。在本月的特定时期，我们的机械遇到了技术问题，导致客户满意度低于预期。
为了提供背景信息，我们在每月 1 日到 11 日进行最佳运营，并且在这些日子里的每一天，我们都会记录满意客户的百分比。然而，从12日到19日，出现了技术问题，影响了客户满意度。本质上，我们经历了 8 天的“非最佳性能”。以及 22 天的最佳性能。
我正在寻求有关最合适的统计或机器学习方法的建议，以模拟或推断在未发生技术问题的情况下整个月的总体满意度。
我正在考虑的方法：

以 95% 置信区间进行引导重采样、重采样
正常时期客户满意度调查数据
操作来推断我们的预期性能。
蒙特卡罗模拟，假设二项式分布
使用输入的满意度百分比的理论分布
最佳性能时期的数据。

我还听说过使用其他月份的数据进行机器学习或时间序列预测。我希望能够就解决这个问题的最佳方法提供消息灵通且有学术支持的观点。我是一位受人尊敬且经常使用的用户，渴望参与讨论，澄清任何疑问，并真诚地感谢您的时间和见解。预先感谢您。
我想要的输出将是该特定月份的客户满意度比例的置信区间，其中是模拟、推断、预测等。目标是在技术困难未发生的假设下获取此信息]]></description>
      <guid>https://stackoverflow.com/questions/77623084/statistical-or-machine-learning-approach-for-simulating-variable-proportion</guid>
      <pubDate>Thu, 07 Dec 2023 21:10:51 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中多输出模型的高效梯度计算</title>
      <link>https://stackoverflow.com/questions/77622773/efficient-gradient-computation-for-multiple-output-models-in-pytorch</link>
      <description><![CDATA[我有一个图神经网络 (GNN)，其中每个节点都有 N 个独立的读出 MLP，每个 MLP 产生一个标量。这意味着该模型在共享主干中具有大量共享权重，而在读出 MLP 中具有少量独立权重。在训练期间（在 .backward() 调用之前），我需要输入的每个读数的梯度，这是一个数组。我有一个简单的方法，可以将除了感兴趣的头之外的 grad_outputs 归零：
batch_size, n_outputs = 输出.shape
梯度= []

对于范围内的 i（n_outputs）：
    grad_output = torch.zeros_like(输出)
    grad_output[:, i] = 1.0

    # 保留除最后一个输出之外的所有输出的图表
    如果 i &lt; 则保留 = True n_outputs - 1 个其他训练

    梯度 = torch.autograd.grad(
        输出=输出，
        输入=输入，
        grad_outputs = grad_output，
        保留图=保留，
        创建图=训练，
        允许未使用=真，
    )[0]

    渐变.append(渐变)

组合梯度 = torch.stack(梯度, 暗淡=-1)
返回 -1 * 组合梯度

但是，这似乎效率低下，因为共享主干中的权重是相同的，但它们可能被计算N次。有没有更有效的方法来做到这一点？例如，首先计算主干网络相对于输入的梯度，然后输出相对于主干输出的梯度？]]></description>
      <guid>https://stackoverflow.com/questions/77622773/efficient-gradient-computation-for-multiple-output-models-in-pytorch</guid>
      <pubDate>Thu, 07 Dec 2023 19:53:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TSFEL 特征提取无法正确迭代数据帧？</title>
      <link>https://stackoverflow.com/questions/77622722/why-is-tsfel-feature-extraction-not-iterating-through-the-data-frame-properly</link>
      <description><![CDATA[我正在使用 TSFEL 迭代一个 4148 行长、总共 6 列的数据帧，其中 1 列分为所有 4148 行的 3 个子列表。我使用的代码如下：
&lt;前&gt;&lt;代码&gt;feature_list = []
对于 df_0_B[&#39;data_lw&#39;] 中的索引：
    测试= tsfel.time_series_features_extractor（cfg，索引）
    feature_list.append（测试）

打印（测试）

我期望它迭代并提取每一行的特征并将它们存储到另一个数据帧中以用于训练和测试。运行特征提取后，我只得到 1 行 120 列。]]></description>
      <guid>https://stackoverflow.com/questions/77622722/why-is-tsfel-feature-extraction-not-iterating-through-the-data-frame-properly</guid>
      <pubDate>Thu, 07 Dec 2023 19:42:07 GMT</pubDate>
    </item>
    <item>
      <title>在实现正则化值的梯度下降代码时出现 TypeError</title>
      <link>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</link>
      <description><![CDATA[我的代码（来自 coursera）：
defgradient_desc(X, Y, w_in, b_in, cost_f, grad_f, alp, num, lambda_):
    m = len(X)
    # 一个数组，用于存储每次迭代的成本 J 和 w，主要用于稍后绘图
    J_历史=[]
    w_历史= []
    对于范围内的 i（num）：
        # 计算梯度并更新参数
        dj_db, dj_dw = grad_f(X, Y, w_in, b_in, lambda_)
        # 使用 w、b、alpha 和梯度更新参数
        w_in = w_in - 阿尔法 * dj_dw
        b_in = b_in - alpha * dj_db
        # 每次迭代时保存成本 J
        if i&lt;100000: # 防止资源耗尽
            J_history.append(cost_f(X, Y, w_in, b_in, lambda_))
        # 每隔 10 次或多次迭代打印成本 if &lt; 10
        如果 i% math.ceil(num/10) == 0:
            w_history.append(w_in)
            print(f&quot;迭代 {i:4}: 成本 {float(J_history[-1]):0.2e} &quot;)

    return w_in, b_in, J_history, w_history #返回 w 和 J,w 历史记录以进行绘图

错误：
TypeError：float() 参数必须是字符串或实数，而不是“NoneType”

这是整个代码中唯一有 bug 的部分（至少到目前为止是有 bug 的）。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</guid>
      <pubDate>Thu, 07 Dec 2023 18:03:29 GMT</pubDate>
    </item>
    <item>
      <title>如何获得分割图像的具体顺序？</title>
      <link>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</link>
      <description><![CDATA[我正在开展一个项目，旨在预测电阻值。为了进行预测，我需要知道电阻器的色带。使用图像分割来检测条带。我可以做到 98% 的准确率。但是，要知道电阻的值。我需要颜色的具体顺序。我怎样才能获得这些数据？
我尝试使用多类逻辑回归。结果很糟糕。人工神经网络能够找到数组中的第一个色带 (row_0)，成功率为 40%。比随机更好，因为有 11 种可能的颜色。
用于 ANN 和回归模型的数据包括像素值 (x,y)、波段的颜色和第一个波段的标签。我还对数据进行了标准化。标签和类别值的范围为 1-11。因为这是电阻值的颜色范围。
训练数据的第一行
图像如何分割]]></description>
      <guid>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</guid>
      <pubDate>Thu, 07 Dec 2023 17:55:10 GMT</pubDate>
    </item>
    <item>
      <title>实施跟踪器以在视频帧中保留汽车的 OCR 结果 [关闭]</title>
      <link>https://stackoverflow.com/questions/77621885/implementing-tracker-to-persist-ocr-results-for-cars-in-video-frames</link>
      <description><![CDATA[我有一个 Python 函数 detect_and_draw_cars 在循环中使用并给定视频帧，它利用 YOLO 来识别并绘制视频每一帧中汽车周围的边界框。此外，该函数还会对这些边界框中的感兴趣区域（如果找到）执行 OCR，以提取车牌（函数 box4）。
为了优化 OCR 处理，我希望实现一个 OpenCV 跟踪器，它可以在多个帧上跟踪汽车的中心。目标是每辆车每 60 帧左右更新一次 OCR 结果，因为汽车的形状和方向随着时间的推移逐渐变化，我正在考虑通过它们的中心跟踪图像。
这是现有代码的片段：
# [现有代码]
def detector_and_draw_cars(图像,frame_count):
    汽车图像，汽车坐标= yolo_predire（图像）
    对于 i，(x1，y1，x2，y2) 在 enumerate(car_coordinates) 中：
        结果=[]

        cv2.矩形(图像, (x1, y1), (x2, y2), (0, 255, 0), 2)
        car_roi = 图像[y1:y2, x1:x2]

        结果 = box4(car_roi)
        如果结果：
          打印（结果）
          ocr_text = f&quot;{结果[0][-2]} - {(结果[0][-1]*100):.2f}%&quot;
          text_size, _ = cv2.getTextSize(ocr_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 1)
          cv2.矩形(图像, (x1, y1 - 文本大小[1] - 10), (x1 + 文本大小[0] + 10, y1), (255, 255, 255), -1)
          cv2.putText(图像, ocr_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)

    返回图像
]]></description>
      <guid>https://stackoverflow.com/questions/77621885/implementing-tracker-to-persist-ocr-results-for-cars-in-video-frames</guid>
      <pubDate>Thu, 07 Dec 2023 17:09:04 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 Tensorflow 使用 vs code 和 python 来解释牙科图像？</title>
      <link>https://stackoverflow.com/questions/77621824/how-can-i-use-tensorflow-to-interpret-dental-images-using-vs-code-and-python</link>
      <description><![CDATA[为了能够将tensorflow包用于包含牙科图像的tfrecord文件，我需要使用与最新tensorflow版本兼容的python版本。我使用的版本是3.12.0，所以我使用Anaconda创建了一个虚拟环境并安装了python 3.8.18。
我安装了最新版本的tensorflow，即2.3.0。当我开始编写代码时，我需要 matplotlib 来处理图像。运行后，我遇到了这个错误：
AttributeError：模块“numpy”没有属性“object”。
`np.object` 是内置 `object` 的已弃用别名。为了避免现有代码中出现此错误，请单独使用“object”。这样做不会改变任何行为并且是安全的。
别名最初在 NumPy 1.20 中已弃用；

这与我的“代码”无关。我发现这又与软件包的兼容性有关。所以我尝试安装 numpy==1.19.0 ，当时我错误地认为它与张量流兼容。当我尝试使用 conda 提示符安装该程序时，出现了以下错误：
matplotlib 3.7.4 需要 numpy&lt;2,&gt;=1.20，但您有 numpy 1.19.0，这是不兼容的。
tensorflow 2.3.0 需要 numpy&lt;1.19.0,&gt;=1.16.0，但您有 numpy 1.19.0，这是不兼容的。

现在我尝试使用旧版本的 matplotlib(==3.5)。之后，我尝试安装旧版本的 numpy(1.17.0)，但又出现了一些新错误（！）：
 numpy 的构建轮 (setup.py) ... 错误
  错误：子进程退出并出现错误

  × python setup.py bdist_wheel 未成功运行。
  │ 退出代码：1
  ╰─&gt; [293行输出]

错误：需要 Microsoft Visual C++ 14.0 或更高版本。使用“Microsoft C++ 构建工具”获取它：https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
  错误：numpy 构建轮子失败
  为 numpy 运行 setup.py clean
  错误：子进程退出并出现错误

  × python setup.py clean 未成功运行。
  │ 退出代码：1
  ╰─&gt; [10行输出]
      从 numpy 源目录运行。

      不支持 `setup.py clean`，请改用以下选项之一：

        - `git clean -xdf` （清理所有文件）
        - `git clean -Xdf` （清理所有版本化文件，不触及
                            未签入 git 存储库的文件）

      如果必须的话，请将 `--force` 添加到您的命令中以无论如何使用它（不受支持）。

      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
  错误：无法清理 numpy 的构建目录
构建 numpy 失败
错误：无法为 numpy 构建轮子，这是安装基于 pyproject.toml 的项目所必需的

如何为我的目的安装兼容版本的tensorflow、python、matplotlib和numpy？或者还有其他方法来处理牙科射线照相图像吗？]]></description>
      <guid>https://stackoverflow.com/questions/77621824/how-can-i-use-tensorflow-to-interpret-dental-images-using-vs-code-and-python</guid>
      <pubDate>Thu, 07 Dec 2023 16:59:58 GMT</pubDate>
    </item>
    <item>
      <title>在这种情况下如何计算标准差（或置信区间）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77620366/how-to-calculate-standard-deviation-or-confidence-interval-in-this-case</link>
      <description><![CDATA[对于同一分类（二元）任务，我现在有五个独立的分类结果。我可以计算每个预测的 AUC（受试者工作特征曲线下面积），并且可以通过 Delong 方法获得置信区间 [1]。现在我需要通过平均这五个 AUC（例如，使用箱线图）来显示我的结果，如何计算平均 AUC 的标准差（或置信区间）。
[1] DeLong, E. R.、D. M. DeLong 和 D. L. Clarke-Pearson (1988)。 “比较两个或多个相关接收器工作特性曲线下的面积：非参数
我有两种计算方法，但不知道是否正确。 (1)使用5个AUC值的样本标准差； （2）使用每个AUC的标准差的均方根（最后需要除以5）。我更喜欢第二种选择，因为我可以将每个 AUC 视为随机变量并使用 Var((X+Y)/2)=(Var(X)+Var(Y))/4 等公式。]]></description>
      <guid>https://stackoverflow.com/questions/77620366/how-to-calculate-standard-deviation-or-confidence-interval-in-this-case</guid>
      <pubDate>Thu, 07 Dec 2023 13:23:54 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“Flags”对象没有属性“c_contigious”</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在阅读 Aurélien Géron 编写的《机器学习实践》一书，但遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(“int”) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

＃模型
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
&lt;前&gt;&lt;代码&gt;{
AttributeError: &#39;Flags&#39; 对象没有属性 &#39;c_contigious&#39;”
}

由于我正在关注这本书，所以我希望这段代码能够工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>如何将从 ListFromFiles 创建的 Tensorflow 数据集划分为训练集、验证集和测试集？</title>
      <link>https://stackoverflow.com/questions/77615270/how-can-i-divide-a-tensorflow-dataset-created-from-listfromfiles-into-train-val</link>
      <description><![CDATA[我有一个目录，其中包含我想要分成训练集、验证集和测试集的文件。我采取的方法是：
# 定义正负路径
POS = os.path.join(“*.txt”)

# 创建正数据集
pos = tf.data.Dataset.list_files(POS)

如果我尝试：
&lt;前&gt;&lt;代码&gt;位置[3]

我得到：
TypeError：“_ShuffleDataset”对象不可下标

我曾想过使用 take &amp; 进行拆分跳过 - 例如
train = pos.take(10)
有效 = pos.skip(10).take(5)
测试 = pos.skip(15).take(5)

但是，当我在包含 10 个文本文件 (a-j) 的示例目录中尝试以下测试时：
导入操作系统
将张量流导入为 tf

# 定义正向和负向路径
POS = os.path.join(“*.txt”)

# 创建数据集
pos = tf.data.Dataset.list_files(POS)#.shuffle(len(POS))

对于范围（20）内的 _：
    对于 pos.take(1) 中的 x：
       打印（x）

我得到这个输出：
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./b.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./g.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./h.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./e.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./c.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./f.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./f.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./h.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./i.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./b.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./j.txt&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./d.txt&#39;, shape=(), dtype=string)

由于目录中只有 10 个 txt 文件，我预计会看到相同的输出 20x、10 个不同文件的列表（后跟崩溃）或 2 个 10 个不同文件的列表。相反，take 似乎每次都在处理一个新的列表。因此，我使用 skip 和 take 的想法很可能最终会在我的集合之间出现重叠。
我唯一能想到做的就是在创建数据集之前拆分文件列表。但是，必须有某种方法从数据集中获取切片。 切片 TF 数据 中的方法对我不起作用。我在 take 的文档中没有看到任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/77615270/how-can-i-divide-a-tensorflow-dataset-created-from-listfromfiles-into-train-val</guid>
      <pubDate>Wed, 06 Dec 2023 17:47:52 GMT</pubDate>
    </item>
    <item>
      <title>基于相同输入数据的并行或共享回归网络会更好吗？为什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</link>
      <description><![CDATA[我将多个并行回归网络组合成一个模型，其中组合输出以创建单个损失函数。这些网络正在寻找相同数据的不同方面，并同时进行训练。这可以被认为是一个基于物理的神经网络。
本能地，我想说，分割网络允许每个网络拥有自己的权重，而不受其他方面的干扰，这将加快训练速度和/或提高性能。 ChatGPT 似乎证实了我的怀疑，但无法给我任何来源。
有人有任何论文/证明或知道这两种方法的更具体术语吗？我只是真的不知道如何提出这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</guid>
      <pubDate>Wed, 06 Dec 2023 17:29:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tidymodels 框架中的 Youden 索引获取 ML 性能指标？</title>
      <link>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</link>
      <description><![CDATA[如何使用 tidymodels 框架中的 Youden 索引获取机器学习性能指标？
我在网上搜索过示例，但没有找到。]]></description>
      <guid>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</guid>
      <pubDate>Wed, 06 Dec 2023 16:12:03 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - TypeError：__init__() 获得意外的关键字参数“trained_model_file_path”</title>
      <link>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</guid>
      <pubDate>Tue, 05 Dec 2023 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>考虑到之前的输出，是否可以在 keras 中自定义回归损失函数</title>
      <link>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</link>
      <description><![CDATA[我正在构建 CNN 图像回归模型。我想在损失函数中添加一个惩罚项，以惩罚当前预测是否大于先前的期望。
有没有办法使用 Keras 编写代码而不是使用 tf.GradientTape()？
（这个帖子看起来和我想问的很相似
使用 Gradient Tape 的自定义损失函数，TF2.6)&lt; /p&gt;
这是数据描述。
我的数据是视频图像集，我想预测异常事件发生之前还剩多少次。
因此，图像标签应该尽可能地减少到异常标签。
（例如，异常的标签为0，前一张图像为1，前两张图像，该图像标签为2。）
因此，我的预测也会减少，如果当前预测高于之前的预测，我想添加惩罚项。
这是模型架构代码。我使用预训练的 VGG16 模型并将其改编为回归模型。
initial_model = tf.keras.applications.VGG16(weights = &#39;imagenet&#39;,include_top = False)
初始模型.trainable = False

func_model_p = keras.Sequential()
输入 = keras.Input(形状 = (700,100,3))
func_model_p.add(输入)

func_model_p.add(keras.layers.GlobalAveragePooling2D())
func_model_p.add(keras.layers.Dense(1,激活=“线性”))

# 准备指标。
train_mse_metric_p = keras.metrics.MeanSquaredError()
val_mse_metric_p = keras.metrics.MeanSquaredError()

此代码适用于 tf.GradientTape。我想为 Keras 更改此代码。
&lt;前&gt;&lt;代码&gt;
对于范围内的 e（纪元）：
    对于范围内的 i(len(y_train_d_noshuffle))：
        使用 tf.GradientTape() 作为磁带：

            图像 = np.expand_dims(X_train_d_noshuffle[i], axis=0)
            y_hat = func_model_p(图像, 训练=True)

            先前值 = 先前列表[-1]
            Violation_term = tf.constant(max( (y_hat - previous_value) , 0), dtype=tf.float32)

            尝试：
                如果 y_train_d_noshuffle[i] &lt; y_train_d_noshuffle[i+1]：
                    违规项 = 0
            除外：通过

            y_train_c = tf.constant(y_train_d_noshuffle[i])

            mse = tf.keras.losses.MeanSquaredError()(y_train_c, y_hat)
            # 计算损失
            损失值 = mse + 违规项
            previous_list.append(y_hat)

            train_mse_sum = train_mse_sum + mse
            train_penalty_sum = train_penalty_sum + Violation_term
        
        grads = Tape.gradient(loss_value, func_model_p.trainable_weights)
        优化器.apply_gradients(zip(grads, func_model_p.trainable_weights))

        # 更新训练指标。
        train_mse_metric_p.update_state(y_train_d_noshuffle[i], y_hat)

        # 显示每个时期结束时的指标。
        train_mse = train_mse_metric_p.result().numpy()

    print(f&#39;epochs: {e}, train_mse_sum: {train_mse_sum}, train_penalty_sum: {float(train_penalty_sum)}&#39;) ```



如果您知道路请告诉我！

先感谢您！
]]></description>
      <guid>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</guid>
      <pubDate>Mon, 04 Dec 2023 22:05:42 GMT</pubDate>
    </item>
    <item>
      <title>标准化卫星图像以输入神经网络的正确方法是什么？</title>
      <link>https://stackoverflow.com/questions/75115715/what-is-the-right-way-to-normalize-satellite-images-to-feed-into-a-nerual-networ</link>
      <description><![CDATA[我正在尝试将小块卫星图像数据（landsat-8 表面反射带）输入到我的项目的神经网络中。但是下载的图像值范围为 1 到 65535。
所以我尝试将图像除以 65535（最大值），但绘制它们会显示所有黑色/棕色图像，如下所示！

但是大多数图像的值都不接近 65535
没有任何标准化，图像看起来全是白色。

将图像除以 30k 看起来像这样。

如果图像太暗或太亮，我的网络可能无法按预期运行。
将图像除以最大值（65535）是唯一的解决方案吗？还是有其他方法可以标准化图像，尤其是卫星数据？]]></description>
      <guid>https://stackoverflow.com/questions/75115715/what-is-the-right-way-to-normalize-satellite-images-to-feed-into-a-nerual-networ</guid>
      <pubDate>Sat, 14 Jan 2023 04:08:10 GMT</pubDate>
    </item>
    </channel>
</rss>