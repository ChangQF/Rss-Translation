<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 31 May 2024 21:14:15 GMT</lastBuildDate>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我所知，predict_proba(xtest) 将给出以下结果：
投票给分类器的树数/树数
我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1：
--- 5, 0 类 A（已选择）
10
--- 2, 3 类 B（未选择）
树 2：
--- 3, 2 类 A（已选择）
10
--- 5, 0 类 B（未选择）
predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0
期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8
有谁知道如何做到这一点，或者他们正在使用什么实现。
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
所以我的问题，我猜，是如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库吗？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples？
抱歉说得太多了。]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    <item>
      <title>测试结果准确率为 99，这是过度拟合吗？机器学习</title>
      <link>https://stackoverflow.com/questions/78561704/99-accuracy-on-test-is-it-overfitting-machine-learning</link>
      <description><![CDATA[我正在做一个图像分类项目，使用 huggingface 的一些视觉变换模型。我正在进行 5 倍交叉验证，准确率在一次验证中达到 97，在另一次验证中达到 99。所以我想知道测试中的 99 准确率是否有问题？训练也达到了 99。我读到过一些网站可能会过度拟合，但我的损失时期图非常好，所以我不确定。
例如，这是一个折叠
时期 1/15：训练损失：0.8659，训练准确度：0.7219，测试损失：0.7590，测试准确度：0.7887
时期 2/15：训练损失：0.5364，训练准确度：0.9057，测试损失：0.5297，测试准确度：0.9123
时期 3/15：训练损失：0.4290，训练准确度：0.9594，测试损失：0.3895，测试准确度：0.9834
时期 4/15：训练损失： 0.3787，训练准确度：0.9848，测试损失：0.3779，测试准确度：0.9848
Epoch 5/15：训练损失：0.3629，训练准确度：0.9936，测试损失：0.3722，测试准确度：0.9890
Epoch 6/15：训练损失：0.3584，训练准确度：0.9964，测试损失：0.3674，测试准确度：0.9917
Epoch 7/15：训练损失：0.3570，训练准确度：0.9967，测试损失：0.3673，测试准确度：0.9924
Epoch 8/15：训练损失：0.3567，训练准确度： 0.9974，测试损失：0.3645，测试准确度：0.9924
Epoch 9/15：训练损失：0.3554，训练准确度：0.9976，测试损失：0.3643，测试准确度：0.9924
Epoch 10/15：训练损失：0.3536，训练准确度：0.9984，测试损失：0.3640，测试准确度：0.9924
Epoch 11/15：训练损失：0.3530，训练准确度：0.9990，测试损失：0.3639，测试准确度：0.9924
Epoch 12/15：训练损失：0.3537，训练准确度：0.9986，测试损失： 0.3641，测试准确率：0.9924
Epoch 13/15：训练损失：0.3555，训练准确率：0.9974，测试损失：0.3641，测试准确率：0.9924
Epoch 14/15：训练损失：0.3536，训练准确率：0.9986，测试损失：0.3641，测试准确率：0.9924
Epoch 15/15：训练损失：0.3548，训练准确率：0.9979，测试损失：0.3641，测试准确率：0.9924
这是另一个折叠
Epoch 1/15：训练损失：1.2736，训练准确率：0.4370，测试损失：1.0652，测试准确度：0.5967
Epoch 2/15：训练损失：0.8066，训练准确度：0.7524，测试损失：0.6185，测试准确度：0.8764
Epoch 3/15：训练损失：0.5270，训练准确度：0.9094，测试损失：0.4643，测试准确度：0.9441
Epoch 4/15：训练损失：0.4273，训练准确度：0.9629，测试损失：0.4307，测试准确度：0.9648
Epoch 5/15：训练损失：0.3905，训练准确度：0.9798，测试损失：0.4059，测试准确度： 0.9724
Epoch 6/15：训练损失：0.3737，训练准确度：0.9891，测试损失：0.3974，测试准确度：0.9758
Epoch 7/15：训练损失：0.3707，训练准确度：0.9905，测试损失：0.3890，测试准确度：0.9793
Epoch 8/15：训练损失：0.3663，训练准确度：0.9931，测试损失：0.3889，测试准确度：0.9793
Epoch 9/15：训练损失：0.3650，训练准确度：0.9922，测试损失：0.3903，测试准确度：0.9793
Epoch 10/15：训练损失：0.3638，训练准确度：0.9940，测试损失：0.3905，测试准确度：0.9793
Epoch 11/15：训练损失：0.3642，训练准确度：0.9931，测试损失：0.3902，测试准确度：0.9800
Epoch 12/15：训练损失：0.3640，训练准确度：0.9934，测试损失：0.3899，测试准确度：0.9793
Epoch 13/15：训练损失：0.3623，训练准确度：0.9940，测试损失：0.3900，测试准确度：0.9793
Epoch 14/15：训练损失：0.3611，训练准确率：0.9953，测试损失：0.3899，测试准确率：0.9793
Epoch 15/15：训练损失：0.3632，训练准确率：0.9940，测试损失：0.3899，测试准确率：0.9793
我在这里添加了损失时期图：
损失时期图]]></description>
      <guid>https://stackoverflow.com/questions/78561704/99-accuracy-on-test-is-it-overfitting-machine-learning</guid>
      <pubDate>Fri, 31 May 2024 18:51:02 GMT</pubDate>
    </item>
    <item>
      <title>当设备设置为“cuda”时，为什么 optuna 会对我的 CPU 而不是 GPU 施加压力？</title>
      <link>https://stackoverflow.com/questions/78561318/why-is-optuna-stressing-my-cpu-instead-of-gpu-when-device-is-set-to-cuda</link>
      <description><![CDATA[我正在使用 optuna 进行超参数调整，尽管我的设备设置为“cuda”，并且它实际上在 cuda 上运行，因为在 CPU 上完成 10 个 epoch 需要 40 分钟，而目前，完成 30 个 epoch 只需要 6 分钟。这意味着，我的程序正在使用 GPU。但是，我检查发现 CPU 的压力已经达到 100%，而我的 GPU 几乎没有被程序利用。
(https://i.sstatic.net/MBGxG7rp.jpg)
以下是我的硬件和软件规格：
硬件规格：
Lenovo Legion 5 2022
Ryzen 7 6800H
NVIDIA RTX 3060 TDP 140W
16 GB DDR5 RAM 4800 Mhz
1TB PCIE Gen 4 SSD
Optimus 已禁用（仅限 NVIDIA Dgpu）

软件规格：
python 3.10.9
conda 23.3.1
optuna 3.6.0 conda-forge
optuna-dashboard 0.15.1 conda-forge

我找不到任何方法可以解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78561318/why-is-optuna-stressing-my-cpu-instead-of-gpu-when-device-is-set-to-cuda</guid>
      <pubDate>Fri, 31 May 2024 17:00:05 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们只需要一张图片就可以训练 CNN 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78560997/why-do-we-only-need-one-picture-to-train-a-cnn-model</link>
      <description><![CDATA[在此处输入图片说明
请看那里的图片。我们可以看到，你可以给模型一张图片，然后模型就可以告诉你它是汽车还是鸟。我的问题是，作为一个监督模型，CNN 应该需要大量的图片来训练，而图片中的情况并非如此。]]></description>
      <guid>https://stackoverflow.com/questions/78560997/why-do-we-only-need-one-picture-to-train-a-cnn-model</guid>
      <pubDate>Fri, 31 May 2024 15:41:27 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 项目：能否在具备 ML/RL 基础知识的情况下在几周内完成，如果可以，请给出有效学习 RLHF 的步骤 [关闭]</title>
      <link>https://stackoverflow.com/questions/78559656/project-on-rlhf-can-it-be-done-in-few-weeks-with-basic-knowledge-of-ml-rl-if-s</link>
      <description><![CDATA[我是人工智能专业的一年级本科生，在基础 ML/DL 项目方面有超过 1 年的经验，几天前我开始学习 RL，有人问起我一个关于 RLHF 的项目——使用人类反馈的强化学习

该项目基于本地大型语言模型开发

我应该如何学习 RLHF，给我一些建议/主题列表来介绍
到目前为止，我在 RL 中只知道基础知识：MDP 和 Q 的基础知识]]></description>
      <guid>https://stackoverflow.com/questions/78559656/project-on-rlhf-can-it-be-done-in-few-weeks-with-basic-knowledge-of-ml-rl-if-s</guid>
      <pubDate>Fri, 31 May 2024 10:53:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 keras 中添加具有可训练参数的自定义损失函数</title>
      <link>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</link>
      <description><![CDATA[作为一个项目，我正在训练一个 LSTM 模型来预测未来的值。为此，我想定义一个与“mse”相同的自定义损失函数。但平方差将与 e^alpha 的指数项相乘。并且这个 alpha 项应该随着训练过程而更新。
我不确定我是否朝着正确的方向前进，但我已经用 mse 训练了模型，它通过使用真实数据给出了很好的预测。但作为现实生活中的预测，当我们在一段时间内没有真实数据时，我的模型应该使用最后一个预测作为模型下一个输入的输入，就像这样。在将模型作为此任务进行测试时，预测值不断偏离最后一个真实值。当一段时间后有新的真实数据可用时，它应该与未来的值相匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</guid>
      <pubDate>Fri, 31 May 2024 09:59:12 GMT</pubDate>
    </item>
    <item>
      <title>即使指定了某些列，Pandas 也会获取数据框的所有列</title>
      <link>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</guid>
      <pubDate>Fri, 31 May 2024 08:59:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多标签分类中实现类别权重采样？</title>
      <link>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</link>
      <description><![CDATA[我正在研究一个多标签分类问题，需要一些使用 Scikit-Learn 计算类别权重的指导。
问题背景：
我有一个包含 9973 个训练样本的数据集。标签是独热编码的，代表 13 个不同的类别。我的训练标签的形状是 (9973, 13)。
我想使用此代码：
import numpy as np
from sklearn.utils.class_weight import compute_class_weight

y_integers = np.argmax(y, axis=1)
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(y_integers), y_integers)
d_class_weights = dict(enumerate(class_weights))

这不起作用，因为位置参数太多。我的训练样本如下所示：
 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],

如何在多类别分类问题中实现，以便解决我的数据集不平衡问题？
编辑 1：它现在运行良好，您认为它在多标签中有效吗？因为我在某处读到，您必须使用采样权重而不是类别权重。我该如何实现呢？]]></description>
      <guid>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</guid>
      <pubDate>Fri, 31 May 2024 08:57:17 GMT</pubDate>
    </item>
    <item>
      <title>yolo 训练精度低，map少</title>
      <link>https://stackoverflow.com/questions/78558728/yolo-training-with-low-precision-and-low-map</link>
      <description><![CDATA[我正在使用 YOLOv5 训练一个模型来识别纸牌游戏中的纸牌。我从预训练模型 yolov5s.pt 开始，我的数据集由 138 张图片组成。然而，训练期间准确率和 mAP 非常低，分别从 2.35e-05 和 2.27e-05 开始，经过 80 个 epoch 后，它们仅达到 0.0169 和 0.0547。
我不知道问题出在哪里。有人能帮我吗？
这是训练批次图像和输出表的图片。


顺便说一句，我只想识别弃牌，而不是手牌。
我试过改变批次大小等。但变化不大。]]></description>
      <guid>https://stackoverflow.com/questions/78558728/yolo-training-with-low-precision-and-low-map</guid>
      <pubDate>Fri, 31 May 2024 07:43:37 GMT</pubDate>
    </item>
    <item>
      <title>具有不平衡类别的 U-Net 分割的图像块提取</title>
      <link>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</link>
      <description><![CDATA[我正在使用 U-Net 进行多类图像分割项目。
我的数据集的类别分布不平衡。有些类别几乎出现在每幅图像中，而其他类别则很少见。我不确定图像修补的最佳方法：
在每个补丁内做出相等的类别表示？在这种情况下，对于某些类别，我将不得不使用数据增强技术。
还是保持补丁内原始的不平衡比例？]]></description>
      <guid>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</guid>
      <pubDate>Thu, 30 May 2024 14:57:49 GMT</pubDate>
    </item>
    <item>
      <title>由于 decision_function，使用 roc_auc 度量的 KNeighborsClassifier 的 GridSearchCV 和 cross_val_score 返回错误</title>
      <link>https://stackoverflow.com/questions/78552800/gridsearchcv-and-cross-val-score-with-kneighborsclassifier-using-roc-auc-metric</link>
      <description><![CDATA[我正在研究二元分类问题。
类别分布为正：30% - 负：70%。因此，我决定使用 roc_auc 作为度量标准
然后，我在 KNeighborsClassifier 上运行超参数调整，但出现错误，我不知道如何解决
我使用的 scikit-learn 版本是 &#39;1.2.2&#39;
这是代码
param_grid = [ 
{
&#39;knn__n_neighbors&#39;: np.arange(2, 30, 1),
&#39;knn__weights&#39; : [&#39;uniform&#39;, &#39;distance&#39;],
&#39;knn__algorithm&#39; : [&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;],
&#39;knn__leaf_size&#39;: np.arange(30, 1000, 20)
}
]

knn = Pipeline([
(&quot;preprocessing&quot;, preprocessing),
(&quot;knn&quot;, KNeighborsClassifier())
])

grid_search = GridSearchCV(
knn,
param_grid,
cv=cv,
scoring=&quot;roc_auc&quot;,
verbose=0
)
grid_search.fit(x_train, y_train)
grid_search.best_params_

错误是
/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: 评分失败。此训练测试分区中这些参数的分数将设置为 nan。详细信息：
回溯（最近一次调用）：
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 373 行，在 _score 中
y_pred = method_caller(clf, “decision_function”, X)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 73 行，在 _cached_call 中
返回 getattr(estimator, method)(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_available_if.py&quot;, line 32, in __get__
if not self.check(obj):
^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py&quot;, line 46, in check
getattr(self._final_estimator, attr)
AttributeError: &#39;KNeighborsClassifier&#39; object has no attribute &#39;decision_function&#39;

在处理上述异常时，发生了另一个异常：

它说 KNeighborsClassifier 没有属性 decision_function
经过一些阅读，我明白了 decision_function 在度量标准为roc_auc
现在，即使存在此问题，如何运行超参数调整？
此外，即使在使用 KNeighborsClassifier 和 roc_auc 作为指标运行时，cross_val_score 也会返回 nan
knn = Pipeline([
(&quot;preprocessing&quot;, preprocessing),
(&quot;knn&quot;, KNeighborsClassifier(
algorithm=&#39;auto&#39;,
leaf_size=30,
metric=&#39;minkowski&#39;,
n_neighbors=28,
weights=&#39;uniform&#39;
))
])
scores = cross_val_score(knn, x_train, y_train, cv=cv)
scores

错误是
/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: 评分失败。此训练测试分区中这些参数的分数将设置为 nan。详细信息：
回溯（最近一次调用）：
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 117 行，在 __call__ 中
score = scorer(estimator, *args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 444 行，在 _passthrough_scorer 中
返回 estimator.score(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py&quot;，第 722 行，在分数中
返回 self.steps[-1][1].score(Xt, y, **score_params)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py&quot;，第 668 行，在分数中
返回 accuracy_score(y, self.predict(X), sample_weight=sample_weight)
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py&quot;，第234，在预测中
neigh_ind = self.kneighbors(X, return_distance=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py&quot;，第 824 行，在 kneighbors 中
results = ArgKmin.compute(

现在由 predict() 完成]]></description>
      <guid>https://stackoverflow.com/questions/78552800/gridsearchcv-and-cross-val-score-with-kneighborsclassifier-using-roc-auc-metric</guid>
      <pubDate>Thu, 30 May 2024 04:00:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么加载 AutoTokenizer 会占用这么多的 RAM？</title>
      <link>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</link>
      <description><![CDATA[我测量了脚本使用的 RAM，惊讶地发现它占用了大约 300Mb 的 RAM，而 tokenizer 文件本身大约只有 9MB。这是为什么？
我试过：
from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
tokenizer = AutoTokenizer.from_pretrained(path)

return tokenizer

load_tokenizer()

输出：
行 # 内存使用量 增量 发生次数 行内容
==================================================================
4 377.4 MiB 377.4 MiB 1 @profile
5 def load_tokenizer():
6 377.4 MiB 0.0 MiB 1 path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
7 676.6 MiB 299.2 MiB 1 tokenizer = AutoTokenizer.from_pretrained(path)
8 
9 
10 676.6 MiB 0.0 MiB 1 返回 tokenizer
]]></description>
      <guid>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</guid>
      <pubDate>Tue, 28 May 2024 22:44:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 MLflow 中管理数据集？</title>
      <link>https://stackoverflow.com/questions/77822962/how-to-manage-datasets-in-mlflow</link>
      <description><![CDATA[请考虑以下取自 MLflow 文档页面的代码片段：
import mlflow.data
import pandas as pd
from mlflow.data.pandas_dataset import PandasDataset

# 使用来自 Web URL 的鸢尾花数据构建 Pandas DataFrame
dataset_source_url = &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot;
df = pd.read_csv(dataset_source_url)
# 从 Pandas DataFrame 构建 MLflow PandasDataset，并指定 Web URL
# 作为源
dataset: PandasDataset = mlflow.data.from_pandas(df, source=dataset_source_url)

with mlflow.start_run():
# 将数据集记录到 MLflow Run。指定“训练” context 以表明
# 数据集用于模型训练
mlflow.log_input(dataset, context=&quot;training&quot;)

# 检索运行，包括数据集信息
run = mlflow.get_run(mlflow.last_active_run().info.run_id)
dataset_info = run.inputs.dataset_inputs[0].dataset
print(f&quot;Dataset name: {dataset_info.name}&quot;)
print(f&quot;Dataset digest: {dataset_info.digest}&quot;)
print(f&quot;Dataset profile: {dataset_info.profile}&quot;)
print(f&quot;Dataset schema: {dataset_info.schema}&quot;)

# 加载数据集的源，将内容从源 URL 下载到本地
# 文件系统
dataset_source = mlflow.data.get_source(dataset_info)
dataset_source.load()

此代码正在启动新运行并记录一个数据集输入。这是否意味着在 MLflow 中我们将数据集保存为单独的运行？如果是这样，我们如何将具有自己运行的模型的训练与数据集关联起来？我很困惑 MLflow 如何处理/跟踪数据集！老实说，我期望数据集是不同的实体类型（而不是运行），我们可以将它们链接到用于模型训练的每次运行。]]></description>
      <guid>https://stackoverflow.com/questions/77822962/how-to-manage-datasets-in-mlflow</guid>
      <pubDate>Tue, 16 Jan 2024 00:40:36 GMT</pubDate>
    </item>
    <item>
      <title>Optuna 在大量试验中建议相同的参数值（重复试验浪费时间和预算）</title>
      <link>https://stackoverflow.com/questions/64836142/optuna-suggests-the-same-parameter-values-in-a-lot-of-trials-duplicate-trials-t</link>
      <description><![CDATA[由于某种原因，Optuna TPESampler 和 RandomSampler 多次尝试对任何参数使用相同的建议整数值（也可能是浮点数和对数均匀值）。我找不到阻止它反复建议相同值的方法。在 100 次试验中，其中相当一部分只是重复的。唯一建议值计数最终在 100 次试验中约为 80-90。如果我包含更多参数进行调整，比如 3 个，我甚至会看到所有 3 个参数在 100 次试验中都获得了相同的值几次。
就像这样。min_data_in_leaf 的 75 被使用了 3 次：
[I 2020-11-14 14:44:05,320] 第 8 次试验结束，值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:07,876] 试验 9 完成，其值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:10,447] 试验 10 完成，其值为：45831.75933279074，参数为：{&#39;min_data_in_leaf&#39;: 43}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:13,502] 试验 11 完成，其值为：46125.39810101329，参数为：{&#39;min_data_in_leaf&#39;: 4}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:16,547] 试验 12 完成，其值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是第 4 次试验，其值为：45805.19030897498。
以下示例代码：
def lgb_optuna(trial):

rmse = []

params = {
&quot;seed&quot;: 42,
&quot;objective&quot;: &quot;regression&quot;,
&quot;metric&quot;: &quot;rmse&quot;,
&quot;verbosity&quot;: -1,
&quot;boosting&quot;: &quot;gbdt&quot;,
&quot;num_iterations&quot;: 1000,
&#39;min_data_in_leaf&#39;: trial.suggest_int(&#39;min_data_in_leaf&#39;, 1, 100)
}

cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)
for train_index, test_index in cv.split(tfd_train, tfd_train[:,-1]):
X_train, X_test = tfd_train[train_index], tfd_train[test_index]
y_train = X_train[:,-2].copy()
y_test = X_test[:,-2].copy()

dtrain = lgb.Dataset(X_train[:,:-2], label=y_train)
dtest = lgb.Dataset(X_test[:,:-2], label=y_test)

booster_gbm = lgb.train(params, dtrain, valid_sets=dtest, verbose_eval=False)

y_predictions = booster_gbm.predict(X_test[:,:-2])
final_mse = mean_squared_error(y_test, y_predictions)
final_rmse = np.sqrt(final_mse)
rmse.append(final_rmse)

return np.mean(rmse)

study = optuna.create_study(sampler=TPESampler(seed=42), direction=&#39;minimize&#39;) 
study.optimize(lgb_optuna, n_trials=100) 
]]></description>
      <guid>https://stackoverflow.com/questions/64836142/optuna-suggests-the-same-parameter-values-in-a-lot-of-trials-duplicate-trials-t</guid>
      <pubDate>Sat, 14 Nov 2020 16:33:17 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道使用 SelectKBest 选择了哪些功能？</title>
      <link>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</link>
      <description><![CDATA[运行 SelectKBest 后会选择一些特征，结果以数组形式返回，因此我不知道它们是什么特征，因为我的训练集有数千个特征。
我想在测试集中找到并挑选出这些特征，然后删除其余特征。有什么方便的方法吗？谢谢！
代码如下：
from sklearn.feature_selection import SelectKBest, f_regression
X_opt=SelectKBest(f_regression,k=2000)
X_new=X_opt.fit_transform(df_train_X_mm, train_y)
X_new`

结果如下：
array([[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
[0. , 0. , 0.00688335, ..., 0. , 0. ,
0. ],
[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
...,
[0. , 0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0.06257587，...，0. ，0. ，
0. ]])
]]></description>
      <guid>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</guid>
      <pubDate>Wed, 20 Jun 2018 07:25:42 GMT</pubDate>
    </item>
    </channel>
</rss>