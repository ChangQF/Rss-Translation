<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 22 Oct 2024 09:18:26 GMT</lastBuildDate>
    <item>
      <title>如何使用 Python、FastAPI 和机器学习构建类似 Music Station 的音乐流媒体应用程序？</title>
      <link>https://stackoverflow.com/questions/79112761/how-to-build-a-music-streaming-app-like-music-station-with-python-fastapi-and</link>
      <description><![CDATA[我计划开发一款设计美观的 Music Station 应用程序。该应用程序将提供与 Apple Music 或 Spotify 类似的功能，例如：
用户身份验证（包括 Google 登录）
个人资料管理（编辑个人资料）
音乐搜索（按名称、艺术家、流派等）
保存、分享曲目和创建播放列表
根据用户偏好精选音乐推荐
社交功能，例如关注朋友/艺术家
我计划使用的关键技术：
后端和 API：FastAPI
前端：React
RDBMS：PostgreSQL
NoSQL：用于音乐存储的 AWS S3
缓存：Redis
搜索：用于音乐搜索的 Elasticsearch
消息代理：RabbitMQ
服务器控制：NGINX
日志记录：Logstash
我有一些问题，需要建议才能继续进行：
数据存储：使用数据湖（例如 AWS S3）是否适合存储音乐数据（例如音频文件）？在此架构中，将关系数据 (PostgreSQL) 和非关系数据 (S3) 结合起来的最佳方法是什么？
API 和数据流：我应该如何构建 API 以及 FastAPI、PostgreSQL、AWS S3、Elasticsearch 和 Redis 之间的数据流？我正在寻找连接这些技术的最佳实践。
机器学习：我计划实施推荐算法，根据用户的行为向他们推荐音乐。我应该考虑哪些机器学习方法和库（最好是 Python 的）？
免费工具：是否有任何免费的工具、服务或软件包（例如 AWS、GitHub Student Pack 或其他提供的教育服务）可用于更经济高效地构建此项目？
到目前为止，我已经进行了初步研究，并根据我的技能和项目要求选择了技术堆栈。我希望该应用程序能够处理音乐上传、高效搜索和无缝播放。我还计划使用 Elasticsearch 进行音乐搜索，使用 FastAPI 构建后端 API。但是，我不确定如何连接所有这些部分，尤其是围绕数据存储和集成机器学习进行推荐。]]></description>
      <guid>https://stackoverflow.com/questions/79112761/how-to-build-a-music-streaming-app-like-music-station-with-python-fastapi-and</guid>
      <pubDate>Tue, 22 Oct 2024 06:50:24 GMT</pubDate>
    </item>
    <item>
      <title>针对 LtR 问题的符号 AI [关闭]</title>
      <link>https://stackoverflow.com/questions/79112045/symbolic-ai-for-ltr-problems</link>
      <description><![CDATA[我最近一直在思考如何将符号 AI（逻辑）融入学习排序问题。我能想到的唯一地方是机器学习流程中的处理步骤，尽管我仍然不确定“如何”。你们对此有什么（其他）想法吗？我觉得符号 AI 有更多内容可用于 LTR 问题，例如推荐系统。你怎么看？干杯！
一直在阅读一些关于它的论文，但找不到足够的内容。]]></description>
      <guid>https://stackoverflow.com/questions/79112045/symbolic-ai-for-ltr-problems</guid>
      <pubDate>Mon, 21 Oct 2024 23:50:42 GMT</pubDate>
    </item>
    <item>
      <title>SRGAN Android Tensorflow Lite 推理输出与 Python 版本不匹配</title>
      <link>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</link>
      <description><![CDATA[我尝试使用此模型的 tflite 版本实现 Android Kotlin 应用，https://github.com/krasserm/super-resolution/blob/master/model/common.py
我有一个 gan_generator.tflite 模型。我可以推断它并在 Python 中使用它获得正确的输出。但即使我进行了相同的后处理操作，我也无法在 Android Kotlin 中获得与位图相同的结果。我遗漏了什么？
from model.srgan import generator
from utils import load_image, plot_sample
from model import resolve_single
import tensorflow as tf

model = generator()
model.load_weights(&#39;weights/srgan/gan_generator.h5&#39;)

# 从 Keras 模型创建 TFLite 转换器
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 设置转换参数（可选）
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.target_spec.supported_types = [tf.float32]

# 转换模型
tflite_model = converter.convert()

# 将 TFLite 模型保存到文件
with open(&#39;gan_generator.tflite&#39;, &#39;wb&#39;) as f:
f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=&#39;gan_generator.tflite&#39;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()

output_details = interpretationer.get_output_details()

input_shape = input_details[0][&#39;shape&#39;]

input_data = np.asarray(load_image(&#39;demo/0869x4-crop.png&#39;), dtype=np.float32)
input_data = np.expand_dims(input_data, axis=0)
input_data = np.reshape(input_data, (1, input_data.shape[1], input_data.shape[2], 3))

# 设置输入张量
interpreter.resize_tensor_input(input_details[0][&#39;index&#39;], (1, input_data.shape[1], input_data.shape[2], 3),strict=True)
interpreter.allocate_tensors()

interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data)

interpreter.invoke()

# 获取输出
output_data = interpretation.get_tensor(output_details[0][&#39;index&#39;])

output_data = np.squeeze(output_data)
output_data = np.clip(output_data, 0, 255).astype(np.uint8)

# 从 NumPy 数组创建 PIL 图像
image = Image.fromarray(output_data)
image #此处创建的输出图像成功

###----------Android 端-------------------------
 val tensorImage = TensorImage(INPUT_IMAGE_TYPE).also { it.load(inputBitmap) }
val processingImage = imageProcessor.process(tensorImage)

//[1,4X,4X,3] 输出
val outputBuffer = TensorBuffer.createFixedSize(
intArrayOf(
1,
processingImage.width * outputShapeForScaling,
processingImage.height * outputShapeForScaling,
3
),
OUTPUT_IMAGE_TYPE
)

// [1, None, None, 3] 动态输入
explainer!!.resizeInput(
explainer!!.getInputTensor(0).index(),
intArrayOf(1, processingImage.width, processingImage.height, 3)
)

解释器！！.allocateTensors()

解释器！！.run(processedImage.buffer, outputBuffer.buffer)

val processingOutput = processOutput(
outputBuffer,
width = processingImage.width * 4,
height = processingImage.height * 4
)


##---------后期处理---------------------------
private fun processOutput(outputBuffer: TensorBuffer, width: Int, height: Int): Bitmap {
val data = outputBuffer.floatArray

// 检查浮点数组是否具有正确数量的元素
if (data.size != width * height * 3) {
throw IllegalArgumentException(&quot;数据大小与预期图像大小不匹配。&quot;)
}

// 创建一个空的 Bitmap
val bitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)

//遍历浮点数组并设置位图中的像素
var index: Int
for (y in 0 till height) {
for (x in 0 till width) {
index = (y * width + x) * 3
// 从浮点数组中提取 RGB 值
val r = data[index].coerceIn(0f, 255f).toInt()
val g = data[index + 1].coerceIn(0f, 255f).toInt()
val b = data[index + 2].coerceIn(0f, 255f).toInt()

// 设置像素颜色（我们将 alpha 设置为 255 以实现完全不透明度）
bitmap.setPixel(x, y, Color.rgb(r, g, b))
}
}

return bitmap
}

结果：
]]></description>
      <guid>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</guid>
      <pubDate>Mon, 21 Oct 2024 19:24:29 GMT</pubDate>
    </item>
    <item>
      <title>单击我的 LinkedIn 自动化脚本中的“连接”按钮时出现问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79111488/trouble-clicking-connect-button-in-my-linkedin-automation-script</link>
      <description><![CDATA[我使用 Selenium 开发了一个简单的机器人，它可以打开 LinkedIn 并搜索“数据科学家”和“首席数据科学家”职位。虽然该机器人成功执行了搜索，但目前无法单击“连接”按钮。我将不胜感激任何有关解决此问题的指导。错误消息
GitHub 代码
我尝试过的方法：
我使用 Selenium 创建了一个机器人，它可以打开 LinkedIn 并搜索术语“数据科学家”和“首席数据科学家”。我已经实现了搜索功能，可以成功检索搜索结果。但是，当我尝试点击显示的个人资料的“连接”按钮时，机器人不会记录点击操作。
我的期望：
我希望机器人不仅执行搜索，还会自动点击找到的相关个人资料的“连接”按钮。这将使我能够简化 LinkedIn 上的社交流程。]]></description>
      <guid>https://stackoverflow.com/questions/79111488/trouble-clicking-connect-button-in-my-linkedin-automation-script</guid>
      <pubDate>Mon, 21 Oct 2024 19:21:24 GMT</pubDate>
    </item>
    <item>
      <title>请进一步优化此代码。目前模型的训练准确率为 83%，测试准确率为 80%。我们使用了 Densenet121 [关闭]</title>
      <link>https://stackoverflow.com/questions/79111255/please-give-further-optimization-for-this-code-currently-models-accuracy-is-of</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79111255/please-give-further-optimization-for-this-code-currently-models-accuracy-is-of</guid>
      <pubDate>Mon, 21 Oct 2024 18:07:54 GMT</pubDate>
    </item>
    <item>
      <title>有没有快速的方法可以提高物体检测模型的置信度？[关闭]</title>
      <link>https://stackoverflow.com/questions/79111080/is-there-a-fast-way-to-raise-the-confidence-levels-of-an-object-detection-model</link>
      <description><![CDATA[我目前正在构建一个对象检测模型，用于识别冰箱图片中的食物成分。我使用 Google Colab 来编码和训练我的模型，并使用 RESNET-50 的架构。我删除了已经存在的类别，并使用 Google Colab 最好的 GPU 在包含约 6,000 张图像（已注释，70% 为训练，15% 为有效，15% 为测试）的数据集上对其进行训练。训练后，我的模型似乎仍然没有学到任何东西。取得如此糟糕的结果是否正常，或者有没有办法更快地进行训练并获得更好的结果？我听说过 Google Cloud，但我不知道它是否会有所不同。我是否遗漏了一些隐藏参数？
为了训练我的模型，我使用了 pytorch_lightning 的 Trainer。然而，训练需要很长时间，甚至在训练了 30 个时期之后，当预测正确时（通常情况并非如此），未知数据的置信度仍然低于 5％。]]></description>
      <guid>https://stackoverflow.com/questions/79111080/is-there-a-fast-way-to-raise-the-confidence-levels-of-an-object-detection-model</guid>
      <pubDate>Mon, 21 Oct 2024 17:08:01 GMT</pubDate>
    </item>
    <item>
      <title>如何提高图像分类网络的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79110881/how-can-i-improve-the-accuracy-of-my-image-classification-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79110881/how-can-i-improve-the-accuracy-of-my-image-classification-network</guid>
      <pubDate>Mon, 21 Oct 2024 16:11:45 GMT</pubDate>
    </item>
    <item>
      <title>如何在 p5.js 中更改 bodySegmentation-mask-body-parts 的默认背景？</title>
      <link>https://stackoverflow.com/questions/79110858/how-to-change-the-default-background-of-bodysegmentation-mask-body-parts-in-p5-j</link>
      <description><![CDATA[我试图更改检测到的身体部位背后的背景，但无法使其工作。即使我修改了 background(...) 函数，它仍然默认为白色。有人能解释为什么会发生这种情况吗？
这是我正在处理的文件的链接：
https://editor.p5js.org/speedyonion/sketches/X9mwX9XB9
let bodySegmentation;
let video;
let fragmentation;

let options = {
maskType: &quot;parts&quot;,
};

function preload() {
bodySegmentation = ml5.bodySegmentation(&quot;BodyPix&quot;, options);
}

function setup() {
createCanvas(640, 480);
// 创建视频
video = createCapture(VIDEO);
video.size(640, 480);
video.hide();

bodySegmentation.detectStart(video, gotResults);
}

function draw() {
background(0,0,0);
image(video, 0, 0);
if (segmentation) {
image(segmentation.mask, 0, 0, width, height);
}
}

// 身体分割回调函数
function gotResults(result) {
fragmentation = result;
}
]]></description>
      <guid>https://stackoverflow.com/questions/79110858/how-to-change-the-default-background-of-bodysegmentation-mask-body-parts-in-p5-j</guid>
      <pubDate>Mon, 21 Oct 2024 16:01:40 GMT</pubDate>
    </item>
    <item>
      <title>如何逐步训练人脸识别模型而无需从头开始重新训练？</title>
      <link>https://stackoverflow.com/questions/79110748/how-to-incrementally-train-a-face-recognition-model-without-retraining-from-scra</link>
      <description><![CDATA[我正在构建人脸识别模型。我已经使用两个人（克里斯蒂亚诺·罗纳尔多和莱昂内尔·梅西）的图像训练了一个模型。现在，我想向模型中添加更多人（例如玛丽亚·莎拉波娃），而无需从头开始重新训练所有内容。
有没有办法使用新数据集训练模型？如果是这样，我该如何有效地将新的训练数据与现有模型合并？
这是我现有的代码
import torch
import torchvision
from torchvision import datasets, models, transforms
import os

import ssl
ssl._create_default_https_context = ssl._create_unverified_context

data_transforms = {
&#39;train&#39;: transforms.Compose([
transforms.Resize((224, 224)),
transforms.ToTensor(),
transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
]),
&#39;test&#39;: transforms.Compose([
transforms.Resize((224, 224)),
transforms.ToTensor(),
transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
]),
}

data_dir = &#39;./new_dataset&#39;
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])
for x in [&#39;train&#39;, &#39;test&#39;]}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True)
for x in [&#39;train&#39;, &#39;test&#39;]}
class_names = image_datasets[&#39;train&#39;].classes

model = models.resnet18(pretrained=True, progress=True)

num_classes = len(class_names)
model.fc = torch.nn.Linear(model.fc.in_features, num_classes)

device = torch.device(&quot;cpu&quot;)
model = model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, influence=0.9)

num_epochs = 10

for epoch in range(num_epochs):
for input, labels in dataloaders[&#39;train&#39;]:
input = input.to(device)
labels = labels.to(device)

optimizer.zero_grad()

output = model(inputs)
loss = criterion(outputs, labels)

loss.backward()
optimizer.step()

torch.save(model.state_dict(), &#39;model.pth&#39;)

model.eval()

correct = 0
total = 0

使用 torch.no_grad():
对于 dataloaders[&#39;test&#39;] 中的输入、标签：
输入 = 输入。到（设备）
标签 = 标签。到（设备）

输出 = 模型（输入）
_，预测 = torch.max（输出。数据，1）

总 += 标签。大小（0）
正确 += (预测 == 标签).sum().item()

准确率 = 100 * 正确 / 总计
print(f&quot;测试集上的准确率：{accuracy}%&quot;)

文件夹 ./new_dataset 如下所示这个 new_dataset/ --test/ ----cristiano_ronaldo ----lione_messi --train/ ----cristiano_ronaldo ----lione_messi ]]></description>
      <guid>https://stackoverflow.com/questions/79110748/how-to-incrementally-train-a-face-recognition-model-without-retraining-from-scra</guid>
      <pubDate>Mon, 21 Oct 2024 15:34:20 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种方法或函数可以通过调试、记录器或库获取 Python 神经网络代码中每个变量的值？</title>
      <link>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</link>
      <description><![CDATA[我想了解模型在每个步骤中对其每个变量（如权重、损失、偏差）做了什么，从一个层到另一个层，甚至在批次级别。
是否有任何预建库或方法可以让我获得完整的细分？
下面是我当前的代码，我正在尝试构建所有自定义回调，但我想完全摆脱它并从分配权重等的第 0 阶段的最基础级别理解模型。
来自 keras.models 导入 Sequential
来自 keras.layers 导入 Dense
来自 keras.callbacks 导入 Callback
导入 numpy 作为 np

步骤 1：自定义回调以了解模型内部的工作原理
class DebuggingCallback(Callback):
def on_epoch_begin(self, epoch, logs=None):
print(f&quot;\n--- Epoch {epoch + 1} Start ---&quot;)

def on_epoch_end(self, epoch, logs=None):
print(f&quot;--- Epoch {epoch + 1} End ---&quot;)
for layer_index, layer in enumerate(self.model.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {layer_index + 1}: Weights\n{weights}&quot;)
print(f&quot;Layer {layer_index + 1}: Biases\n{biases}&quot;)

def on_batch_end(self, batch, logs=None):
loss = logs.get(&#39;loss&#39;)
accuracy = logs.get(&#39;accuracy&#39;)
print(f&quot;Batch {batch + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}&quot;)

步骤 2：定义 ANN 模型
classifier = Sequential()
classifier.add(Dense(units=3, input_dim=numFeatures, kernel_initializer=&#39;uniform&#39;,activation=&#39;relu&#39;))
classifier.add(Dense(units=2, kernel_initializer=&#39;uniform&#39;,activation=&#39;relu&#39;))
classifier.add(Dense(units=1, kernel_initializer=&#39;uniform&#39;,activation=&#39;sigmoid&#39;))

classifier.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

步骤 3：拟合模型并使用自定义回调
survivalANN_Model = classifier.fit(X_train, y_train, 
batch_size=30, 
epochs=5, 
verbose=1, 
callbacks=[DebuggingCallback()])

步骤 4：之后打印最终权重训练
print(&quot;\n训练后的最终权重：&quot;)
for i, layer in enumerate(classifier.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {i + 1}:&quot;)
print(&quot;权重：&quot;, weights)
print(&quot;偏差：&quot;, biases)
]]></description>
      <guid>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</guid>
      <pubDate>Mon, 21 Oct 2024 14:50:13 GMT</pubDate>
    </item>
    <item>
      <title>将数据集拆分为训练/测试 - 但保证某个键/标识符在两者中都不存在</title>
      <link>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</link>
      <description><![CDATA[我需要将数据集拆分为训练/测试，并满足以下约束条件：

遵循近似的训练/测试比例拆分（例如 70/30）
数据集中的每一行都有一个键。这些键不是唯一的，这意味着我可能有 1000 行带有 1 个键，另外 2 行带有另一个键。我想保证训练集和测试集中不存在相同的键。

我尝试了网上的所有方法（包括此处的类似主题），例如 GroupKFold 等。但似乎我无法找到任何方法来做到这一点。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</guid>
      <pubDate>Mon, 21 Oct 2024 11:39:07 GMT</pubDate>
    </item>
    <item>
      <title>‘管道’对象没有属性‘_check_fit_params’</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# 定义特征和目标
X = df.drop(&#39;infected&#39;, axis=1)
y = df[&#39;infected&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样至多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 将多数类欠采样至其原始大小的 80%

pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(&quot;Resampled class distribution:&quot;, pd.Series(y_resampled).value_counts())

这是我的代码
这是我得到的错误
AttributeError Traceback (most recent call last)
Cell In[7], line 19
16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
21 # 显示新的类分布
22 print(&quot;重新采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372, in Pipeline.fit_resample(self, X, y, **fit_params)
342 &quot;&quot;&quot;使用最终估计器拟合模型和样本。
343 
344 依次拟合所有转换器/采样器并
(...)
369 转换后的目标。
370 &quot;&quot;&quot;
371 self._validate_params()
--&gt; 372 fit_params_steps = self._check_fit_params(**fit_params)
373 Xt, yt = self._fit(X, y, **fit_params_steps)
374 last_step = self._final_estimator

AttributeError: &#39;Pipeline&#39; 对象没有属性 &#39;_check_fit_params&#39;

我已经尝试了所有方法。我的所有包都已更新。我尝试使用的所有方法都在 sklearn 和 imblearn 这两个网站上查看过。]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的聚类时间序列数据</title>
      <link>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</link>
      <description><![CDATA[我尝试使用不同的聚类技术在 Python 中对时间序列数据进行聚类。K 均值法没有给出良好的结果。以下图像是我使用凝聚聚类法进行聚类后得到的图像。我还尝试了动态时间扭曲。这两个似乎给出了类似的结果。
我理想情况下希望第二幅图中的时间序列有两个不同的聚类。第一幅图像是快速增加的聚类。第二幅图像没有增加，有点稳定，第三幅图像是下降趋势的聚类。我想知道哪些时间序列既稳定又受欢迎（这里的流行是指高计数）。我尝试了层次聚类，但结果显示层次太多，我不确定如何选择层次级别。有人能解释一下如何将第二幅图中的时间序列分成两个不同的聚类，一个计数低，另一个计数高吗？可以做到吗？或者我应该直接选择一个阈值将它们一分为二？
快速增加的集群：

计数稳定的集群：

下降趋势的集群：

这非常非常模糊，但这是我的层次聚类的结果。 

我知道这个特定的图像根本没有用，但这对我来说也像是死胡同。 
一般来说，如果你想区分趋势，例如对于 YouTube 视频，如何只有一些被选入“趋势”部分，而另一些被选入“本周趋势”部分？我理解“趋势”部分的视频是那些与第一张图片具有相似特征的视频。 “本周热门”部分收集了一些视频，这些视频的观看次数非常高，但数量相当稳定（即没有出现快速增长）。我知道，对于 YouTube，除了观看次数之外，还有很多其他因素需要考虑。对于第二张图片，我试图做的类似于“本周热门”部分。我想挑选那些观看次数非常高的视频。在这种情况下，我该如何分割时间序列？
我知道 DTW 可以捕捉趋势。DTW 给出的结果与上面的图片相同。它已经确定了第二张图片中的趋势是“稳定的”。但它没有捕捉到这里的“数量”元素。我希望既能捕捉到趋势，又能捕捉到数量，在这种情况下是稳定和高数量。
上面的图片是基于计数聚类的时间序列。我是否错过了其他可以实现这一点的聚类技术？即使只是计数，我如何根据自己的需求进行不同的聚类？
任何想法都将不胜感激。提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</guid>
      <pubDate>Thu, 10 Aug 2017 03:51:00 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习来删除重复数据[关闭]</title>
      <link>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</link>
      <description><![CDATA[我遇到了以下问题，并且认为我可以使用机器学习，但我不完全确定它是否适合我的用例。
我有一个包含客户数据（包括姓名、地址、电子邮件、电话等）的约一亿条记录的数据集，我想找到一种方法来清理这些客户数据并识别数据集中的可能重复项。
大多数数据都是使用外部系统手动输入的，没有经过验证，因此我们的许多客户最终在我们的数据库中拥有多个配置文件，有时每条记录中的数据都不同。
例如，我们可能有 5 个不同的条目用于客户 John Doe，每个条目都有不同的联系方式。
我们也遇到过代表不同客户的多条记录在电子邮件等关键字段上匹配的情况。例如，当客户没有电子邮件地址但数据输入系统需要时，我们的顾问将使用随机电子邮件地址，从而导致许多不同的客户资料使用相同的电子邮件地址，电话、地址等也是如此。
我们所有的数据都在 Elasticsearch 中编入索引并存储在 SQL Server 数据库中。我的第一个想法是使用 Mahout 作为机器学习平台（因为这是一家 Java 商店），也许使用 H-base 来存储我们的数据（只是因为它适合 Hadoop 生态系统，不确定它是否有任何实际价值），但我读得越多，我就越困惑它在我的情况下会如何工作，首先我不确定我可以使用哪种算法，因为我不确定这个问题属于哪一类，我可以使用聚类算法或分类算法吗？当然，必须使用某些规则来确定什么构成了个人资料的唯一性，即哪些字段。
我们的想法是最初将其部署为客户个人资料重复数据删除服务，我们的数据输入系统可以使用它来验证和检测输入新客户个人资料时可能的重复项，将来也许可以将其开发为分析平台，以收集有关我们客户的见解。]]></description>
      <guid>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</guid>
      <pubDate>Sun, 05 May 2013 03:36:00 GMT</pubDate>
    </item>
    <item>
      <title>异构数据分类器</title>
      <link>https://stackoverflow.com/questions/10349729/classifier-with-heterogeneous-data</link>
      <description><![CDATA[我有一个 l2 维数据集，包含 1000 个样本，包括 5 个温度值、
5 个价格值、一个表示人类专家判断的整数值（未定=0、好=1、坏=2、危险=4）和一个我想学习预测的二元决策变量。
我如何找到一个可以处理这种异构数据的分类器？
我正在考虑为每个可能的人类判断（0、1、2、4）构建一个分类器，所以有 4 个分类器。
因此，对于每个人的判断值，我会：
- 集中并降低温度和价格值
- 可能使用 PCA 删除一些不相关的特征
- 使用机器学习方法进行分类（如多层神经网络或 SVM）
我的方法正确吗？（如果有 1000 个可能的人类判断而不是 4 个会怎样？）]]></description>
      <guid>https://stackoverflow.com/questions/10349729/classifier-with-heterogeneous-data</guid>
      <pubDate>Fri, 27 Apr 2012 11:24:47 GMT</pubDate>
    </item>
    </channel>
</rss>