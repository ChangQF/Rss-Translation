<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 31 Jan 2024 03:14:55 GMT</lastBuildDate>
    <item>
      <title>系统生物中 ANN 模型预测的令人困惑的 SHAP 分析</title>
      <link>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</link>
      <description><![CDATA[我开发了一个 ANN 模型来根据 Elisa 数据预测蛋白质翻译后修饰模式。为了简单起见，如何、可行性和参数对于我的问题并不重要，并且省略了一些细节。
对于给定的蛋白质，我将其称为蛋白质 X，它具有泛素作为修饰，但没有磷酸化模式。
我用各种翻译后修饰模式训练了人工神经网络，但有一个关键信息：我的训练数据不包含任何泛素模式（假设有一个原因）
因此，当我使用一组抗体进行 ELISA 时，抗体 a 特异性针对泛素模式，抗体 b 特异性针对磷酸化模式。当我使用抗体 a、抗体 b（和其他抗体）预测蛋白质 x 修饰模式时，我们获得了相当好的准确性。
为了解释模型的工作原理，我运行了 SHAP 分析和二分图来显示特征重要性（抗体）和修改，但得到了令人困惑的结果

对于抗体 a，除泛素外，其他修饰模式的 SHAP 值存在正值和负值，泛素是其特异性的

对于抗体 b，我们还发现了除磷酸化之外的修饰模式的正向和负向 SHAP 值，而蛋白质 x 并不真正具有磷酸化。


那么我该如何解释为什么 SHAP 产生这种模式：1）抗体 a 与其靶标泛素没有任何 SHAP 相关性，但对其其他靶标有 SHAP 相关性，2）抗体 b 与其靶标也没有 SHAP 相关性，但与其他抗体有 SHAP 相关性。其他人代替。
这又是令人困惑的，因为我预计抗体 a 与泛素有 SHAP 相关性，但与其他蛋白没有 SHAP 相关性，然后抗体 b 不应该有任何 SHAP 相关性，因为它的目标是磷酸化，但蛋白 x 没有磷酸化。
我不太相信或无法将 SHAP 的一些限制联系起来，因为它显示了模型的隐藏模式/关系，而不是我们在“现实生活”中所期望的
有人可以对这个观察到的 SHAP 分析提供更细致的见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</guid>
      <pubDate>Wed, 31 Jan 2024 01:27:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的机器学习模型总是预测相同的错误答案，即使预测已经在我的数据集中？</title>
      <link>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</guid>
      <pubDate>Wed, 31 Jan 2024 01:07:06 GMT</pubDate>
    </item>
    <item>
      <title>在Python中查找特征列的哪些过滤集导致最大目标列</title>
      <link>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</link>
      <description><![CDATA[我无法找到可以解决我的问题的机器学习模型或分类类型。我本以为这可能相当简单，但也许不是。
假设我有 10 个特征列和一个二进制目标列。目标列的数据集中应该有大致相等数量的 0 和 1。整组数据并不是强相关的，所以线性回归、逻辑回归、朴素贝叶斯等都没有得出强相关的模型。然而，我所寻找的是哪个数据系列导致目标列的最大平均值。
例如。对于特征集 A 到 J 如果我按（C = True、D = false、J = true）过滤数据集，则目标 X 的平均值现在为 56%。我正在寻找一种算法，可以找到导致最大目标列均值的方程。
我觉得这可以通过蛮力来完成（循环遍历所有可能的组合），但我希望有一种方法可以在现有的众多数据科学库之一中做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</guid>
      <pubDate>Wed, 31 Jan 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML（监督学习）进行元数据分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/77909222/metadata-classification-with-ml-supervised-learning</link>
      <description><![CDATA[我正在从事一个对我来说似乎有点模糊的项目，我想知道你对我有什么建议吗？
我有与健康部分相关的元数据，我想将它们分类为“是否是个人信息？”。
我没有任何标记数据可用作训练数据集。但问题是，许多元数据字段只是 PHN 或 SIN 等数字，因此我无法将它们用作有价值的字段来帮助我找到关系并帮助创建模型本身。我还有一些字段，如果您将它们组合在一起，它们将被视为“个人信息”。我不知道应该从哪里开始。
这就是我现在所知道的一切。]]></description>
      <guid>https://stackoverflow.com/questions/77909222/metadata-classification-with-ml-supervised-learning</guid>
      <pubDate>Tue, 30 Jan 2024 20:25:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么预测总是返回相同的值？</title>
      <link>https://stackoverflow.com/questions/77909165/why-are-the-predictions-always-returning-the-same-value</link>
      <description><![CDATA[我试图根据标题预测一篇文章的阅读量。我正在使用 DecisionTreeRegressor 和 TFIDF 对标题进行矢量化，以便在模型中使用它们。这是我的代码
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.metrics 导入mean_squared_error
从 nltk.corpus 导入停用词
从 nltk.tokenize 导入 word_tokenize

# 加载你的数据集
# 将 &#39;your_dataset.csv&#39; 替换为实际文件名和路径
df = pd.read_csv(&#39;数据/Gerardo_ML_012524_7d_reduced.csv&#39;)

# 特征工程
# 假设“article_title”是包含标题的列
stop_words = set(stopwords.words(&#39;英语&#39;))
df[&#39;article_title&#39;] = df[&#39;article_title&#39;].apply(lambda x: &#39; &#39;.join([word.lower() for word in word_tokenize(x) if word.isalpha() and word.lower() not）在停用词中]））

# TF-IDF 矢量化
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf_vectorizer.fit_transform(df[&#39;article_title&#39;]).toarray()
tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())

# 将 TF-IDF DataFrame 与原始 DataFrame 合并
df_combined = pd.concat([df, tfidf_df], 轴=1)

# 定义特征和目标变量
X = df_combined.drop([&#39;article_title&#39;, &#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1)
y = df_combined[&#39;article_visits&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 DecisionTreeRegressor 模型
dt_model = DecisionTreeRegressor(max_深度=9)
dt_model.fit(X_train, y_train)

# 在测试集上评估模型
y_pred = dt_model.predict(X_test)

然后，当我想使用这个经过训练的模型预测新标题时，所有预测都将获得相同的值。
# 将 &#39;new_headlines.csv&#39; 替换为包含新标题的文件
new_headlines_df = pd.read_csv(“数据/new_headlines.csv”)
new_tfidf_matrix = tfidf_vectorizer.transform(new_headlines_df[&#39;article_title&#39;]).toarray()
new_tfidf_df = pd.DataFrame(new_tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())
new_combined_df = pd.concat([new_headlines_df, new_tfidf_df], axis=1)

X = df_combined.drop([&#39;article_title&#39;, &#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1)
# 对新头条新闻进行预测
new_predictions = dt_model.predict(new_combined_df.drop([&#39;article_title&#39;,&#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1))
print(&#39;新头条新闻的预测：&#39;)
打印（新预测）

我尝试向模型添加更多变量，但仍然遇到相同的问题。对于全新的头条新闻，我总是得到相同的值。我做了一些 cross_val_score 分析以获得最佳超参数，在训练/测试期间，我得到了超过 0.99 的 R 方值（两种情况）低 RMSE 和低 MAE。新字符串总是获得相同的值。]]></description>
      <guid>https://stackoverflow.com/questions/77909165/why-are-the-predictions-always-returning-the-same-value</guid>
      <pubDate>Tue, 30 Jan 2024 20:13:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时，它没有运行，并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中对图像进行聚类</title>
      <link>https://stackoverflow.com/questions/77906886/how-can-i-cluster-images-in-python</link>
      <description><![CDATA[我是机器学习和 scikit-learn 的新手。但明天我必须向老师提交任务，请帮忙。我需要从指定目录中的所有图像中提取特征，然后对该图像应用PCA方法并创建散点图，以显示图像分布。
我应该使用像 scikit-learn 这样的库来提取这些特征吗？如何绘制二维散点图？
我已经看过这篇文章https:// /scikit-learn.org/stable/modules/ generated/sklearn.decomposition.PCA.html但仍然没有清晰的理解。]]></description>
      <guid>https://stackoverflow.com/questions/77906886/how-can-i-cluster-images-in-python</guid>
      <pubDate>Tue, 30 Jan 2024 13:55:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么令牌嵌入与 BartForConditionalGeneration 模型的嵌入不同</title>
      <link>https://stackoverflow.com/questions/77906649/why-token-embedding-different-from-the-embedding-by-the-bartforconditionalgenera</link>
      <description><![CDATA[为什么即使我使用相同的 BartForConditionalGenration 模型生成嵌入，它们仍然不同？
第一个嵌入是通过组合令牌嵌入和位置嵌入生成的
embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids)
input_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids)

模型的第二次嵌入
输出 = modelBART(input_ids.input_ids)
print(&quot;\n\n 输出：\n\n&quot;,output.encoder_last_hidden_​​state)

第一个和第二个的嵌入不应该相同吗？如何使第一个和第二个嵌入的差异为零？]]></description>
      <guid>https://stackoverflow.com/questions/77906649/why-token-embedding-different-from-the-embedding-by-the-bartforconditionalgenera</guid>
      <pubDate>Tue, 30 Jan 2024 13:18:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在读取条形码之前提高图像质量[关闭]</title>
      <link>https://stackoverflow.com/questions/77906456/how-to-improve-image-quality-before-reading-barcode</link>
      <description><![CDATA[我正在使用 zxing-cpp 库从图像中读取条形码。
&lt;前&gt;&lt;代码&gt;导入cv2
导入zxingcpp

img = cv2.imread(&#39;test.jpg&#39;)
结果= zxingcpp.read_barcodes(img)
对于结果中的结果：
    print(&#39;找到条形码：&#39;
    f&#39;\n 有效：“{result.valid}”&#39;
        f&#39;\n 文本：“{结果.文本}”&#39;
        f&#39;\n 格式：{结果.格式}&#39;
        f&#39;\n 内容：{result.content_type}&#39;
        f&#39;\n 位置：{结果.位置}&#39;)
如果 len(结果) == 0:
    print(“找不到任何条形码。”)

但是，该库无法从 图片。
如何处理图像并提高图像质量以便读取条形码？
我使用了这个问题的答案作为指南，但仍然不成功，因此我提出这个问题并寻求帮助？]]></description>
      <guid>https://stackoverflow.com/questions/77906456/how-to-improve-image-quality-before-reading-barcode</guid>
      <pubDate>Tue, 30 Jan 2024 12:45:45 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Python ARIMA 实现中需要解压的值太多（预期为 3）</title>
      <link>https://stackoverflow.com/questions/77906395/valueerror-too-many-values-to-unpack-expected-3-in-python-arima-implementatio</link>
      <description><![CDATA[我正在尝试创建一个 ARIMA 模型来预测股票市场价值（对此进行实验，不会在现实生活中使用它）并以 PNG 格式导出我的数据集中的所有 512 只股票。在 PNG 中将显示实际值和预测值。
错误：
预测，stderr，conf_int = model_fit.forecast(steps=len(X_test))
ValueError：需要解包的值太多（预期为 3）

导入操作系统
将 pandas 导入为 pd
从 statsmodels.tsa.arima.model 导入 ARIMA

# 加载训练和测试数据。
train_df = pd.read_csv(r&#39;mypath\train.csv&#39;, parse_dates=[&#39;日期时间&#39;])
test_df = pd.read_csv(r&#39;mypath\test.csv&#39;, parse_dates=[&#39;datetime&#39;])

# 获取独特的股票名称。
stock = train_df[&#39;公司简称&#39;].unique()

# 创建一个目录来保存png。
输出目录 = r&#39;ARIMAprediction&#39;
os.makedirs（输出目录，exist_ok = True）

对于股票中的股票：
    print(f&quot;处理库存：{stock}&quot;)

    # 过滤当前股票的数据。
    train_stock = train_df[train_df[&#39;公司简称&#39;] == stock]
    test_stock = test_df[test_df[&#39;公司简称&#39;] == stock]

    # 提取特征（为此，使用所有数据集）。
    features = [&#39;开盘价&#39;, &#39;最高价&#39;, &#39;最低价&#39;, &#39;收盘价&#39;, &#39;成交量&#39;]
    X_train, y_train = train_stock[特征], train_stock[&#39;关闭&#39;]
    X_test, y_test = test_stock[功能], test_stock[&#39;关闭&#39;]

    # 拟合ARIMA模型
    顺序 = (5, 1, 2)
    模型 = ARIMA(y_train, 阶数=阶数)
    model_fit = model.fit()

    ＃ 预言。
    预测，stderr，conf_int = model_fit.forecast（steps=len（X_test））
]]></description>
      <guid>https://stackoverflow.com/questions/77906395/valueerror-too-many-values-to-unpack-expected-3-in-python-arima-implementatio</guid>
      <pubDate>Tue, 30 Jan 2024 12:35:43 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker实例中的CUDA路径解决NameError：名称'_C'未使用GroundingDINO定义</title>
      <link>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</link>
      <description><![CDATA[我正在尝试在 Sagemaker 实例（使用 GPU）中安装和使用 grounding dino ）但我收到错误：
NameError：名称“_C”未定义

我发现原因是因为变量CUDA_HOME没有配置所以要解决它我需要设置变量，但是在搜索答案后（我已经检查了公共路径/usr/local/cuda）我找不到sagemaker实例中cuda的安装路径。
cuda 安装在 sagemaker 实例中的什么位置以便我可以设置 CUDA_HOME？]]></description>
      <guid>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</guid>
      <pubDate>Fri, 26 Jan 2024 18:45:06 GMT</pubDate>
    </item>
    <item>
      <title>RandomizedSearchCV 独立于集成中的模型</title>
      <link>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</link>
      <description><![CDATA[假设我构建了两个估计器的集合，其中每个估计器运行自己的参数搜索：
导入和回归数据集：
从 sklearn.ensemble 导入 VotingRegressor、StackingRegressor、RandomForestRegressor
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.datasets 导入 make_regression

从 sklearn.model_selection 导入 RandomizedSearchCV

X, y = make_regression()

定义两个自调整估计器，并将它们组合起来：
rf_param_dist = dict(n_estimators=[1, 2, 3, 4, 5])
rf_searcher = RandomizedSearchCV(RandomForestRegressor(), rf_param_dist, n_iter=5, cv=3)

dt_param_dist = dict(max_深度=[4, 5, 6, 7, 8])
dt_searcher = RandomizedSearchCV(DecisionTreeRegressor(), dt_param_dist, n_iter=5, cv=3)

合奏 = StackingRegressor(
    [（&#39;rf&#39;，rf_searcher），（&#39;dt&#39;，dt_searcher）]
).fit(X, y)

我的问题是关于sklearn如何处理ensemble的拟合。
Q1）我们有两个并行的未拟合估计器，并且都需要在 ensemble.predict(...) 工作之前进行拟合。但是，如果没有首先从整体中获得预测，我们就无法拟合任何估计器。 sklearn 如何处理这种循环依赖？
Q2）由于我们有两个运行独立调整的估计器，每个估计器是否会错误地假设另一个估计器的参数是固定的？因此，我们最终遇到了一个定义不明确的优化问题。
&lt;小时/&gt;
作为参考，我认为联合优化集成模型的正确方法是定义一个联合搜索所有参数的 CV，如下所示。但我的问题是关于 sklearn 如何处理前面描述的特殊情况。
#联合优化
合奏 = VotingRegressor(
    [ (&#39;rf&#39;, RandomForestRegressor()), (&#39;dt&#39;, DecisionTreeRegressor()) ]
）

jointsearch_param_dist = 字典(
    rf__n_estimators=[1, 2, 3, 4, 5],
    dt__max_深度=[4,5,6,7,8]
）

ensemble_jointsearch = RandomizedSearchCV(ensemble, jointsearch_param_dist)
]]></description>
      <guid>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</guid>
      <pubDate>Sat, 06 Jan 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Gemini-pro 模型构建带提示模板的会话缓冲存储器</title>
      <link>https://stackoverflow.com/questions/77671882/how-to-build-a-conversational-buffer-memory-with-prompt-template-for-gemini-pro</link>
      <description><![CDATA[嗨，我正在尝试使用 Gemini-pro LLM 模型构建一个具有内存支持的对话聊天机器人
我收到此错误消息：
ChatGoogleGenerativeAIError：Gemini 不支持“系统”类型的消息。请仅向其提供人类或人工智能（用户/助理）消息。
这是我的代码：
from langchain.chains import LLMChain
从 langchain.prompts 导入 (
    聊天提示模板，
    HumanMessagePrompt模板，
    消息占位符，
    系统消息提示模板，
）
从 langchain.chains 导入 ConversationChain
从 langchain.memory 导入 ConversationBufferMemory



＃ 迅速的
提示 = 聊天提示模板(
    消息=[
        SystemMessagePromptTemplate.from_template(
            “你是一名课程推荐者，你向候选人询问几个问题，以了解他的个人兴趣、最终目标和当前的技能水平，并向他提供一份精选的课程列表，并注明其难度级别为初级、中级和高级。”
        ),
        # 这里的`variable_name`必须与内存对齐
        MessagesPlaceholder(variable_name=&quot;chat_history&quot;),
        HumanMessagePromptTemplate.from_template(“{问题}”),
    ]
）

# 请注意，我们使用 `return_messages=True` 来适应 MessagesPlaceholder
# 请注意，“chat_history”与 MessagesPlaceholder 名称一致
内存 = ConversationBufferMemory(memory_key=“chat_history”, return_messages=True)
对话= LLMChain（llm=llm，提示=提示，详细=真，内存=内存）

# 请注意，我们只是传入 `question` 变量 - `chat_history` 由内存填充
对话({“问题”:“嗨”})

我试图给出系统提示，但它说 Gemini 型号不支持系统提示]]></description>
      <guid>https://stackoverflow.com/questions/77671882/how-to-build-a-conversational-buffer-memory-with-prompt-template-for-gemini-pro</guid>
      <pubDate>Sat, 16 Dec 2023 17:40:34 GMT</pubDate>
    </item>
    <item>
      <title>部署时，SageMaker 无法提取容器的模型数据存档 tar.gz</title>
      <link>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</link>
      <description><![CDATA[我正在尝试在 Amazon Sagemaker 中部署现有的 Scikit-Learn 模型。所以这个模型不是在 SageMaker 上训练的，而是在我的机器上本地训练的。
在我的本地（Windows）计算机上，我已将模型保存为 model.joblib 并将模型压缩为 model.tar.gz。
接下来，我已将此模型上传到我的 S3 存储桶 (&#39;my_bucket&#39;)，路径为 s3://my_bucket/models/model.tar.gz。我可以在 S3 中看到 tar 文件。
但是当我尝试部署模型时，它不断给出错误消息“无法提取模型数据存档”。
.tar.gz 是通过在 powershell 命令窗口中运行“tar -czf model.tar.gz model.joblib”在我的本地计算机上生成的。
上传到S3的代码
&lt;前&gt;&lt;代码&gt;导入boto3
s3 = boto3.client(“s3”,
              Region_name=&#39;eu-central-1&#39;,
              aws_access_key_id=AWS_KEY_ID,
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(文件名=&#39;model.tar.gz&#39;, Bucket=my_bucket, Key=&#39;models/model.tar.gz&#39;)

用于创建估计器和部署的代码：
&lt;前&gt;&lt;代码&gt;导入boto3
从 sagemaker.sklearn.estimator 导入 SKLearnModel

...

model_data = &#39;s3://my_bucket/models/model.tar.gz&#39;
sklearn_model = SKLearnModel(model_data=model_data,
                             角色=角色，
                             Entry_point =“my-script.py”，
                             Framework_version =“0.23-1”）
预测器= sklearn_model.deploy（instance_type =“ml.t2.medium”，initial_instance_count = 1）

错误信息：
&lt;块引用&gt;
错误消息：UnexpectedStatusException：托管端点错误
sagemaker-scikit-learn-2021-01-24-17-24-42-204：失败。原因：失败
提取容器“container_1”的模型数据档案来自网址
“s3://my_bucket/models/model.tar.gz”。请确保对象
位于 URL 处的是有效的 tar.gz 存档

有没有办法查看存档无效的原因？]]></description>
      <guid>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</guid>
      <pubDate>Mon, 25 Jan 2021 09:03:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在恢复训练时保存和加载回调？</title>
      <link>https://stackoverflow.com/questions/62781555/how-to-save-and-load-callbacks-upon-resuming-training</link>
      <description><![CDATA[目前，我正在 Keras 中使用回调根据最低验证损失对模型进行检查点。
由于该模型每个 epoch 需要 5-6 小时的训练，因此我在 Google Colab 断开连接之前完成了大约 2 个 epoch。因此，每次断开连接时，我都会从上一个检查点恢复训练。
到目前为止，我正在使用 model.save 和 model.load 来保留有关回调、权重和训练配置的所有信息。
第一次训练模型：
检查点 = ModelCheckpoint(分类器,
                              监视器=“val_loss”，
                              模式＝“分钟”，
                              save_best_only = True,
                              详细=1)

早期停止 = EarlyStopping(监视器 = &#39;val_loss&#39;,
                            最小增量 = 0,
                            耐心=20，
                            详细 = 1,
                            恢复最佳权重=真）
  
减少LROnPlateau =减少LROnPlateau（监视器=&#39;val_loss&#39;，
                              系数 = 0.2，
                              耐心=10，
                              详细 = 1,
                              最小增量 = 0.00001)

回调 = [earlystop、检查点、reduce_lr]

model.compile(loss = &#39;categorical_crossentropy&#39;,
              优化器 = Adam(lr=0.0001),
              指标 = [&#39;准确性&#39;])

历史记录 = model.fit_generator(
      火车发电机，
      steps_per_epoch = train_generator.samples //batch_size,
      纪元 = 纪元，
      回调=回调，
      验证数据=验证生成器，
      validation_steps =validation_generator.samples //batch_size)

恢复训练：
模型 = load_model(分类器)

历史记录 = model.fit_generator(
    火车发电机，
    steps_per_epoch = train_generator.samples //batch_size,
    纪元 = 纪元，
    回调=回调，
    验证数据=验证生成器，
    validation_steps =validation_generator.samples //batch_size)

但是，每次我恢复训练时，val_loss 都会设置回无穷大。因此，最新的纪元将覆盖之前保存的纪元。有没有办法保存验证损失的监控值？]]></description>
      <guid>https://stackoverflow.com/questions/62781555/how-to-save-and-load-callbacks-upon-resuming-training</guid>
      <pubDate>Tue, 07 Jul 2020 18:18:08 GMT</pubDate>
    </item>
    </channel>
</rss>