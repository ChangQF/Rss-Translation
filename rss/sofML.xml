<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Feb 2025 12:33:20 GMT</lastBuildDate>
    <item>
      <title>Azure 机器学习 GPU 实例最佳实践</title>
      <link>https://stackoverflow.com/questions/79411648/azure-machine-learning-gpu-instance-best-practices</link>
      <description><![CDATA[我正在使用公司的数据进行研究，并希望使用 GPU 实例设置 Azure 机器学习。我的主要目标是使用 GPU 来训练深度学习模型，例如 ResNet 和 EfficientNet，使用 Jupyter 笔记本中的迁移学习或其他 CNN 模型。
但是，当我仅执行数据准备和预处理时，我不想为 GPU 实例付费，因为这些任务不需要 GPU 加速。
问题：我在 Azure 中有哪些选项可以优化成本？

我看到 Azure 提供了一个 Python SDK，允许在本地运行工作负载，同时将数据存储在 Azure 中（由于公司隐私政策）。这是一个可行的选择吗？

我是否应该使用单独的 CPU 实例进行数据准备，并仅使用 GPU 实例进行训练？

我还看到了类似计算集群的东西，它是如何工作的，这是一个选项吗？


设置 Azure ML 基础架构的最佳实践？

由于我对 Azure 还比较陌生，您是否对设置可扩展且高效的 ML 基础架构有什么建议，是否有任何好的教程、文章或 YouTube 系列可以解释构建 Azure ML 工作流的最佳方式？

很想听听您对最佳方法的看法！]]></description>
      <guid>https://stackoverflow.com/questions/79411648/azure-machine-learning-gpu-instance-best-practices</guid>
      <pubDate>Tue, 04 Feb 2025 12:10:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么 2048 游戏的训练对我来说效果不佳？[关闭]</title>
      <link>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</link>
      <description><![CDATA[因此我开始为游戏 2048 训练神经网络，这里是我的仓库

我尝试使用 DQN 算法，即我的仓库中的 traindqn.py，但分数在 200 到 400 之间随机。

然后我尝试使用进化算法，即我的仓库中的 trainevo.py，分数确实上升了，但似乎稳定在 500 左右。
在尝试了几次手动超参数调整后，我放弃了 DQN。

至于 evo，我增加了种群规模，以下是两个训练图
popsize 64，512 episodes evo
popsize 128，512 episodes evo
它们差别不大，上升到 500 左右，没有继续前进。
我预计它至少会超过 600（我测试了一个随机代理，它的平均分数在 500 左右），如果可能的话，还有办法让它更高，比如超参数调整或更改网络结构。]]></description>
      <guid>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</guid>
      <pubDate>Tue, 04 Feb 2025 10:28:14 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 随机森林的不同结果（带种子）</title>
      <link>https://stackoverflow.com/questions/79410458/different-results-with-seed-for-sklearn-random-forest</link>
      <description><![CDATA[我正在使用 sklearn 运行随机森林。我正在为随机森林设置种子，以及拆分数据以进行交叉验证。当我连续多次重新运行代码时，它给出了相同的结果。但是，一个月后重新运行相同的代码，我得到了略有不同的特征重要性。在其他一些类似的分析中，准确度指标也不同。数据没有改变。我在 Google Colab 上运行。
这是我的代码：
# 配置
file_path = &#39;/content/drive/My Drive/dataset.csv&#39;
columns_to_keep = [
&#39;target_column&#39;, &#39;feature_a&#39;, &#39;feature_b&#39;, &#39;feature_c&#39;, &#39;feature_d&#39;, &#39;feature_e&#39;,
&#39;feature_f&#39;, &#39;feature_g&#39;, &#39;feature_h&#39;, &#39;feature_i&#39;, &#39;feature_j&#39;, &#39;feature_k&#39;, &#39;feature_l&#39;,
&#39;feature_m&#39;, &#39;feature_n&#39;, &#39;feature_o&#39;, &#39;feature_p&#39;, &#39;feature_q&#39;, &#39;feature_r&#39;, &#39;feature_s&#39;,
&#39;feature_t&#39;, &#39;feature_u&#39;, &#39;feature_v&#39;, &#39;feature_w&#39;, &#39;feature_x&#39;, &#39;feature_y&#39;, &#39;feature_z&#39;,
&#39;feature_aa&#39;, &#39;feature_ab&#39;, &#39;feature_ac&#39;, &#39;feature_ad&#39;, &#39;feature_ae&#39;, &#39;feature_af&#39;, &#39;feature_ag&#39;,
&#39;feature_ah&#39;, &#39;feature_ai&#39;, &#39;feature_aj&#39;, &#39;feature_ak&#39;, &#39;feature_al&#39;, &#39;feature_am&#39;, &#39;feature_an&#39;
]

df = pd.read_csv(file_path, usecols=columns_to_keep)

categorical_columns = [&#39;feature_ak&#39;, &#39;feature_al&#39;, &#39;feature_am&#39;, &#39;feature_an&#39;, &#39;feature_ao&#39;]
one_hot_columns = [&#39;feature_al&#39;, &#39;feature_ak&#39;]

df = df.dropna()

# 对指定列进行独热编码
le = LabelEncoder()
for col in one_hot_columns:
df[col] = le.fit_transform(df[col])

# 将指定列转换为分类
for col in categorical_columns:
df[col] = df[col].astype(&#39;category&#39;)

# 拆分为特征和目标
X = df.drop(columns=[&#39;target_column&#39;])
y = df[&#39;target_column&#39;]

# 初始化 RandomForestClassifier 模型
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# 初始化 k 倍交叉验证
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# 存储结果
feature_importances_list = []
all_y_true = []
all_y_pred = []

# 执行 k 倍交叉验证
for fold_num, (train_index, test_index) in enumerate(kf.split(X), start=1):
# 拆分数据
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# 训练随机森林模型
rf_model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = rf_model.predict(X_test)

# 收集所有真实和预测标签
all_y_true.extend(y_test)
all_y_pred.extend(y_pred)

# 获取此折叠的特征重要性
feature_importances_list.append(rf_model.feature_importances_)

# 计算并打印此折叠的准确度
accuracy_fold = accuracy_score(y_test, y_pred)
print(f&quot;Fold {fold_num} Accuracy: {accuracy_fold:.4f}&quot;)

# 计算所有预测的准确度
accuracy_cv = accuracy_score(all_y_true, all_y_pred)

# 生成分类报告
final_report = classes_report(all_y_true, all_y_pred, digits=3)

# 折叠的平均特征重要性
average_importance = sum(feature_importances_list) / len(feature_importances_list)

# 创建带有特征名称的 DataFrame及其相应的平均重要性
feature_names = X.columns
importance_df = pd.DataFrame({
&#39;Feature&#39;: feature_names,
&#39;Importance&#39;: average_importance
}).sort_values(by=&#39;Importance&#39;, accending=False)

# 打印结果
print(f&quot;具有 k 倍 CV 的随机森林模型的总体准确率：{accuracy_cv:.4f}&quot;)

print(&quot;\n最终分类报告：&quot;)
print(final_report)

print(&quot;\n随机森林特征重要性（跨倍平均）：&quot;)
print(importance_df.head(20))
]]></description>
      <guid>https://stackoverflow.com/questions/79410458/different-results-with-seed-for-sklearn-random-forest</guid>
      <pubDate>Tue, 04 Feb 2025 02:34:12 GMT</pubDate>
    </item>
    <item>
      <title>训练/测试损失较低，但预测不佳[关闭]</title>
      <link>https://stackoverflow.com/questions/79410070/low-train-test-loss-but-bad-prediction</link>
      <description><![CDATA[我试图估算不同频率的 2 个阻抗值。我的输入是 3 个变量（720 组）+ 频率（89 个不同值）~64K 数据集，输出是 720 组中的每个组在每个频率的两个阻抗值。我将这些数据分成 80/20 的训练/测试集，并从数据集中保留了 2 个样本，用于完全无偏估计。
我的训练/测试损失相当/非常低，甚至 =0（取决于时期数），当估计数据集中包含的变量的阻抗时，我可以得到完美的拟合（当然），但当从数据集中保留的样本进行预测时，效果并不好。
训练和测试损失似乎也有点过于相互跟随/相互叠加，我似乎无法通过增加训练时间来引起测试损失（过度拟合）的上升。所以这看起来像是一个问题？
我正在使用 pytorch，这是第一次（而且我对 ML 总体来说还很陌生），似乎无法找出问题所在，因此任何帮助/建议都将不胜感激。如果您想查看我混乱的代码，我已经创建了一个 repo。
我尝试优化超参数，降低训练/测试损失，但这仍然不能让我的预测变得更好。]]></description>
      <guid>https://stackoverflow.com/questions/79410070/low-train-test-loss-but-bad-prediction</guid>
      <pubDate>Mon, 03 Feb 2025 21:34:31 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的部分</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个表示手机移动轨迹的数据集，该轨迹由步行和驾车行驶的路段组成。数据包括经度、纬度和时间戳。我需要提取所有步行行驶的子轨迹。有没有现成的解决方案可以解决这个问题？如果没有，我该如何处理这个任务？
我试图在互联网上寻找现成的解决方案，但没有找到任何有价值的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 无法在 Google Colab 中加载 Adam 优化器</title>
      <link>https://stackoverflow.com/questions/79409678/tensorflow-unable-to-load-adam-optimizer-in-google-colab</link>
      <description><![CDATA[我正在尝试在 Google Colab 中使用 Adam 组织器。我有以下代码：
import tensorflow as tf
import numpy as np
from transformers import BertTokenizer, TFBertForSequenceClassification
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

我使用以下代码编译我的模型：
model.compile(
optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
loss=SparseCategoricalCrossentropy(from_logits=True),
metrics=[&quot;accuracy&quot;]
)

但我得到了：
ValueError Traceback（最近一次调用最后)
&lt;ipython-input-38-0183d53e319d&gt; 在 &lt;cell line: 0&gt;()
----&gt; 1 model.compile(
2 optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
3 loss=SparseCategoricalCrossentropy(from_logits=True),
4 metrics=[&quot;accuracy&quot;]
5 )

2 frames
/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/__init__.py 在 get(identifier, **kwargs)
333 )
334 else:
--&gt; 335 引发 ValueError(
336 f&quot;无法解释优化器标识符：{identifier}&quot;
337 )

ValueError：无法解释优化器标识符：&lt;keras.src.optimizers.adam.Adam 对象位于 0x7cef33ad4050&gt;

我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79409678/tensorflow-unable-to-load-adam-optimizer-in-google-colab</guid>
      <pubDate>Mon, 03 Feb 2025 18:22:46 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习期间，奖励信号如何应用于 LLM？[关闭]</title>
      <link>https://stackoverflow.com/questions/79409562/how-is-reward-signal-applied-to-the-llm-during-reinforcement-learning</link>
      <description><![CDATA[一旦有了奖励信号，你如何将其应用于 LLM？它仍然是反向传播吗？它仍然是每个 token 吗？
SFT 和 RL 有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/79409562/how-is-reward-signal-applied-to-the-llm-during-reinforcement-learning</guid>
      <pubDate>Mon, 03 Feb 2025 17:22:29 GMT</pubDate>
    </item>
    <item>
      <title>Hydra `_partial_` 如何与播种互动</title>
      <link>https://stackoverflow.com/questions/79409259/how-does-hydra-partial-interact-with-seeding</link>
      <description><![CDATA[在配置管理库 Hydra 中，可以使用 _partial_ 关键字 仅部分实例化配置中定义的类。库解释说，这会导致 functools.partial。我想知道这与种子如何交互。例如

pytorch torch.manual_seed()
lightnings seed_everything
等等。

我的理由是，如果我在为 __init__ 指定 all 参数时使用 _partial_ 关键字，那么我基本上会获得一个可以在之后调用的工厂指定种子进行多次运行。但这假设 _partial_ 尚未烘焙种子。据我了解，情况不应该如此。对吗？]]></description>
      <guid>https://stackoverflow.com/questions/79409259/how-does-hydra-partial-interact-with-seeding</guid>
      <pubDate>Mon, 03 Feb 2025 15:28:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么我必须将数据从 torch.Size([50]) 解压到 torch.Size([50, 1])</title>
      <link>https://stackoverflow.com/questions/79409149/why-do-i-have-to-unsqueeze-the-data-from-torch-size50-to-torch-size50-1</link>
      <description><![CDATA[我正在学习 FreeCodeCamp 的 PyTorch 深度学习课程，疑问是：
weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y=weight*X + bias
X[:10], y[:10]
train_split=int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test=X[train_split:], y[train_split:]

为什么使用 unsqueeze 函数生成大小为 [50, 1] 的张量，而不是 [50]？导师说这会导致错误，但我不知道为什么会发生错误？
你能用数学和基本原理回答这个问题吗？
尝试训练模型后，我收到此错误：
class LinearRegressionModelv2(nn.Module):
def __init__(self):
super().__init__()
self.linear_layer = nn.Linear(in_features=1, out_features=1)

def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
return self.linear_layer(x)

torch.manual_seed(42)
model_v2 = LinearRegressionModelv2()

y_prediction = model_v2(X_train)
IndexError: 维度超出范围（预期在 [-1, 0] 范围内，但结果为 -2）]]></description>
      <guid>https://stackoverflow.com/questions/79409149/why-do-i-have-to-unsqueeze-the-data-from-torch-size50-to-torch-size50-1</guid>
      <pubDate>Mon, 03 Feb 2025 14:44:08 GMT</pubDate>
    </item>
    <item>
      <title>Ultralytics 是否提供单独的跟踪 API？</title>
      <link>https://stackoverflow.com/questions/79408953/does-ultralytics-provide-a-separate-api-for-tracking</link>
      <description><![CDATA[我想使用独立于分段的跟踪来单独缩放它们。据我所知，分段是一个无状态推理过程，应用于每个帧，不依赖于先前的帧。相比之下，持久跟踪是一个依赖于先前结果的状态过程。
这种理解有道理吗？Ultralytics 是否为跟踪和分段提供了单独的 API？]]></description>
      <guid>https://stackoverflow.com/questions/79408953/does-ultralytics-provide-a-separate-api-for-tracking</guid>
      <pubDate>Mon, 03 Feb 2025 13:23:28 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的 GaussianProcessRegressor 对象：选择固定超参数，无法重现优化内核</title>
      <link>https://stackoverflow.com/questions/79407078/gaussianprocessregressor-object-in-scikit-learn-select-fixed-hyperparameters-c</link>
      <description><![CDATA[我试图理解 scikit-learn 中的 GaussianProcessRegressor 对象，可惜没有成功。
考虑文档中的示例 带有噪声目标的示例，我将其复制到下面以方便使用（略作更改，使用 ConstantKernel 而不是在内核定义中乘以常数）
import numpy as np

X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)
y = np.squeeze(X * np.sin(X))

#############
#############
noise_std = 0.75

将 matplotlib.pyplot 导入为 plt

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;真正的生成过程&quot;)
rng = np.random.RandomState(1)

training_indices = rng.choice(np.arange(y.size), size=6, replace=False)
X_train, y_train = X[training_indices], y[training_indices]

noise_std = 0.75
y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)
从 sklearn.gaussian_process 导入 GaussianProcessRegressor
从 sklearn.gaussian_process.kernels 导入 RBF、WhiteKernel、ConstantKernel

# kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()
kernel = ConstantKernel(constant_value=1)*RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) 
gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9)

gaussian_process.fit(X_train, y_train_noisy)
gaussian_process.kernel_
mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)
plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
X_train,
y_train_noisy,
noise_std,
linestyle=&quot;None&quot;,
color=&quot;tab:blue&quot;,
marker=&quot;.&quot;,
markersize=10,
label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
X.ravel(),
mean_prediction - 1.96 * std_prediction,
mean_prediction + 1.96 * std_prediction,
color=&quot;tab:orange&quot;,
alpha=0.5,
label=r&quot;95% 置信区间&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;高斯过程回归在嘈杂的数据集上&quot;)

我得到了这些结果

现在，我想使用具有“固定”参数的内核可获得相同的结果（用于与问题无关的其他目的）。
因此，我得到了上述内核的优化超参数，
gaussian_process.kernel_.get_params()
输出
{&#39;k1&#39;: 4.28**2,
&#39;k2&#39;: RBF(length_scale=1.1),
&#39;k1__constant_value&#39;: 18.30421069841903,
&#39;k1__constant_value_bounds&#39;: (1e-05, 100000.0),
&#39;k2__length_scale&#39;: 1.1043558649730463,
&#39;k2__length_scale_bounds&#39;: (0.01, 100.0)}

因此，我修改了之前的内核 &amp;高斯过程定义
kernel_fixed = ConstantKernel(constant_value=18.30, constant_value_bounds=&#39;fixed&#39;) *RBF(length_scale=1.1043, length_scale_bounds=&#39;fixed&#39;) 
gaussian_process_fixed = GaussianProcessRegressor(kernel=kernel_fixed, alpha=noise_std**2, n_restarts_optimizer=9)
gaussian_process_fixed.fit(X,y)
mean_prediction, std_prediction = gaussian_process_fixed.predict(X, return_std=True)

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
X_train,
y_train_noisy,
noise_std,
linestyle=&quot;None&quot;,
color=&quot;tab:blue&quot;,
marker=&quot;.&quot;,
markersize=10,
label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
X.ravel(),
mean_prediction - 1.96 * std_prediction,
mean_prediction + 1.96 * std_prediction,
color=&quot;tab:orange&quot;,
alpha=0.5,
label=r&quot;95% 置信度interval&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;Gaussian process return on a noisy dataset&quot;)


但结果非常不同，而内核的参数与我修复的参数非常接近优化的参数
{&#39;k1&#39;: 4.28**2,
&#39;k2&#39;: RBF(length_scale=1.1),
&#39;k1__constant_value&#39;: 18.3,
&#39;k1__constant_value_bounds&#39;: &#39;fixed&#39;,
&#39;k2__length_scale&#39;: 1.1043,
&#39;k2__length_scale_bounds&#39;: &#39;已修复&#39;}

我遗漏了什么？？
]]></description>
      <guid>https://stackoverflow.com/questions/79407078/gaussianprocessregressor-object-in-scikit-learn-select-fixed-hyperparameters-c</guid>
      <pubDate>Sun, 02 Feb 2025 18:17:29 GMT</pubDate>
    </item>
    <item>
      <title>Whisper 模型实时端点容器部署在 Azure ML 上失败</title>
      <link>https://stackoverflow.com/questions/79399452/whisper-model-real-time-endpoint-container-deployment-failed-on-azure-ml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79399452/whisper-model-real-time-endpoint-container-deployment-failed-on-azure-ml</guid>
      <pubDate>Thu, 30 Jan 2025 09:50:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn 在 KDE 之前正确缩放数据</title>
      <link>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</link>
      <description><![CDATA[我在地理空间数据上使用 sklearn.neighbors.KernelDensity。我注意到带宽估计方法没有考虑数据的空间范围，只考虑样本和特征的数量。 sklearn 文档或 sklearn 教程似乎没有提到应该缩放数据。
因此我的问题是：在拟合 KDE 之前缩放地理空间数据的适当方法是什么？
这是一个最小示例：
import numpy as np
from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=2)

X_uniform = np.random.rand(100, 2)
X_upscaled = X_uniform * 100

for i,X in enumerate([X_uniform, X_upscaled]):
kde = KernelDensity(kernel=&#39;gaussian&#39;, broadband=&quot;scott&quot;).fit(X)
print(f&quot;bw: {kde.bandwidth_}&quot;)

# 创建网格
x_grid = np.linspace(X[:,0].min(), X[:,0].max(), 50)
y_grid = np.linspace(X[:,1].min(), X[:,1].max(), 50)
X,Y = np.meshgrid(x_grid, y_grid)
xy = np.vstack([X.ravel(), Y.ravel()]).T

z = np.exp(kde.score_samples(xy)).reshape(X.shape)
fig.add_trace(go.Contour(z=z, x=x_grid, y=y_grid), row=1, col=i+1)

fig.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</guid>
      <pubDate>Wed, 29 Jan 2025 15:33:57 GMT</pubDate>
    </item>
    <item>
      <title>对更大图像进行语义分割</title>
      <link>https://stackoverflow.com/questions/67864721/semantic-segmentation-on-a-bigger-image</link>
      <description><![CDATA[
我使用 120 X 120 的卫星图像输入训练了一个 U-net。

我需要将我的模型应用于更大的图像（尺寸为 10980 X 10980）。我尝试将较大的图像切成 120 X120 的切片，然后对它们进行分类并将它们组装成一张新图像。

我的问题是：这种方法是否可行，因为我可以在下面的输出图像中看到不连续性？



PS：我看到这个问题大图像的语义分割，一个用户说这是可行的，如果是这样，有什么办法可以做到边界更加连续？]]></description>
      <guid>https://stackoverflow.com/questions/67864721/semantic-segmentation-on-a-bigger-image</guid>
      <pubDate>Mon, 07 Jun 2021 00:27:42 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中有 logit 函数吗？</title>
      <link>https://stackoverflow.com/questions/50794177/is-there-a-logit-function-in-tensorflow</link>
      <description><![CDATA[TensorFlow 中是否有 logit 函数，即 sigmoid 函数的逆函数？我在 Google 上搜索过，但没有找到。]]></description>
      <guid>https://stackoverflow.com/questions/50794177/is-there-a-logit-function-in-tensorflow</guid>
      <pubDate>Mon, 11 Jun 2018 09:14:36 GMT</pubDate>
    </item>
    </channel>
</rss>