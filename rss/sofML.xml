<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sat, 08 Mar 2025 21:14:26 GMT</lastBuildDate>
    <item>
      <title>OPENCV，YOLO或培训模型，以预测照片是否满足护照或学校身份证的要求？</title>
      <link>https://stackoverflow.com/questions/79494898/opencv-yolo-or-train-a-model-for-predicting-if-a-photo-meets-requirement-for-pa</link>
      <description><![CDATA[是否可以单独使用OpenCV，或与Yolo（例如Yolo）结合使用OPENCV来验证图像是否适合ID卡吗？没有头饰，没有太阳镜，白色背景。还是训练模型会更容易，更准确？我一直在django中使用yolo的openCV，并且我得到了误报，也许我的代码是错误的，也许这些库是用于更通用的用例，哪种路径是最好的-opencv + yolo或训练我的模型？]]></description>
      <guid>https://stackoverflow.com/questions/79494898/opencv-yolo-or-train-a-model-for-predicting-if-a-photo-meets-requirement-for-pa</guid>
      <pubDate>Sat, 08 Mar 2025 19:17:28 GMT</pubDate>
    </item>
    <item>
      <title>在单个设备上运行时，应在降低图中删除集体通信操作吗？</title>
      <link>https://stackoverflow.com/questions/79494735/should-collective-communication-ops-be-removed-or-become-no-ops-in-graph-lower</link>
      <description><![CDATA[我正在研究集体通信操作（例如，全合理，全聚集）期间图表降低阶段的优化。我的直觉是，当这些CCL操作在单个设备上执行（或仅包含一个成员的进程组）时，它们没有真正的语义影响，并且可以安全地删除或降低到no-op以避免不必要的开销。 
但是，在检查了Jax，StableHlo（和XLA），NVIDIA的NCCL和TORCH等框架中的实现之后，我还没有找到任何明确的实例。 
我的问题是：

 这些框架中的任何一个是否实现“单位设备”优化（即，在不需要通信时降低过程中删除或绕过集体OP）？

 如果没有，是否存在特定原因 - 例如维护IR语义，调试或调度考虑因素 - 为什么将集体OP保留，即使它充当NO-OP？


我审查了JAX，StableHlo，NCCL和Torch的源代码和文档。分发，但没有发现在单个设备上运行时删除集体操作的明确优化。]]></description>
      <guid>https://stackoverflow.com/questions/79494735/should-collective-communication-ops-be-removed-or-become-no-ops-in-graph-lower</guid>
      <pubDate>Sat, 08 Mar 2025 17:08:45 GMT</pubDate>
    </item>
    <item>
      <title>解构稳定扩散3.5管道</title>
      <link>https://stackoverflow.com/questions/79494728/deconstructiong-the-stable-diffusion-3-5-pipeline</link>
      <description><![CDATA[我正在尝试解构SD3.5（特别是3.5个介质）管道，以便在降压步骤上具有受控的过程。我无法进行回调，因为我需要根据其他管道修改潜在。
我正在尝试在以下拥抱面指南上执行步骤：
 https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline_pipeline#deconstruct-the-stable-diffusion-diffusion-pipeline  
我修改了文本编码以适合SD3.5，我还尝试加载整个管道，并在其上运行Encode_prompt函数，以获取文本嵌入和汇总的嵌入，以适应提示和负个提示。当运行该功能并将其输出作为常规管道输入而不是提示提示时，它可以正常工作，因此这似乎不是引起问题的原因。
我还将UNET从文章中更改为使用模型的预训练变压器。之后，我调整了解码器以匹配扩散器上管道源代码上的相同解码。
输出图像在通过扩散器运行管道时看起来并不相同。我不确定在哪里可以找到与SD3管道解构类似的实现，或者我缺少什么。
 在此处输入图像描述  ]]></description>
      <guid>https://stackoverflow.com/questions/79494728/deconstructiong-the-stable-diffusion-3-5-pipeline</guid>
      <pubDate>Sat, 08 Mar 2025 17:06:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我创建估算器对象时，AWS SageMaker不提交培训工作？</title>
      <link>https://stackoverflow.com/questions/79492869/why-is-aws-sagemaker-not-submitting-a-training-job-when-i-create-an-estimator-ob</link>
      <description><![CDATA[我正在尝试使用HuggingFace估算器通过GitHub向AWS SageMaker AI API提交培训工作。但是，每次我尝试创建作业时，估算器都会创建一个类型无的对象，并且脚本无限期地挂起。它永远不会完成执行，甚至从未提交给AWS Sagemaker，因为UI和CLI在我检查时都没有显示培训工作。
这是我正在使用的代码，删除了敏感信息：
 ＃导入模块
进口sagemaker
来自sagemaker.huggingface导入huggingface
导入记录

＃故障排除错误
loggging.getLogger（“ sagemaker”）。setlevel（logging.debug）

＃创建会话
session = sagemaker.session（）

＃配置AWS角色
角色= [aws arn]

＃配置各种登录信息
git_config = {
    [存储库登录信息]
}
HF_Token = [HuggingFace登录信息]

＃配置培训容器
打印（“创建估算器对象...”）
估算器= huggingface（
    角色=角色，
    会话=会话，
    instance_count = 1，
    source_dir =;
    py_version =&#39;py310&#39;，
    git_config = git_config，
    entry_point =; train.py;＃＃github中培训脚本的名称
    unignts_file =&#39;sumperient.it.txt＆quot;＃安装其他软件包
    output_path =&#39;s3：// llama-data/model-o-output/&#39;，＃模型检查点的输出
    instance_type =＆quot; ml.p3.2xlarge＆quort＆quort
    环境= {
        “ hf_token”：hf_token
    }，，
    enable_sagemaker_metrics = true，＃log in CloudWatch
    image_uri =&#39;7631043518844.dkr.us-east-1.amazonaws.com/huggingface-pytorch-训练：2.1.0-transformers4.36.36.0-gpu-py310-cu121-cu121-ubuntu20.04&#39;
    hyperparameters = none，＃在train.py中声明的所有超帕拉姆
    triench_repository_access_mode =&#39;platform&#39;＃docker映像在亚马逊ecr中
）
打印（“估算值对象成功创建”

＃配置培训/验证数据
train_input = sagemaker.inputs.traininginput（
    s3_data =; s3：// llama-data/train/＆quot;
    content_type =&#39;text/csv＆quot;
）

验证_input = sagemaker.inputs.trainingInput（
    s3_data =; s3：// llama-data/validate/＆quot;
    content_type =&#39;text/csv＆quot;
）

＃调试：提交前打印萨吉式制造商培训请求
＃request_dict = inestator.latest_training_job.describe_request（）
打印（“ SageMaker培训工作请求：&#39;estemator.latest_training_job）＃**这总是打印“无”，不提交请求**

打印（“呼叫.fit（）...;）
interator.fit（{＃**脚本停止在这一点上工作**
    ＆quot“ train＆quot”：train_input，
    ＆quot“ veration＆quot”：validation_input
}，wait = true，job_name =＆quort; train-llama-small＆quord＆quort＆quots =; quot; all＆quot;
打印（成功创建了培训工作。退出脚本。”）
 
创建估算器后，我尝试列出最新的培训工作，并打印出来：
 估算对象成功创建了
Sagemaker培训工作要求：无
 
当脚本挂断时，这是我在终端中看到的消息：
 您的分支与“原始/主”最新。
[03/07/25 09:31:21]使用提供的S3_Resource fw_utils.py:474进行调试
[03/07/25 09:31:22] Debug Train Arg在处理Exterator.py:2513
                             默认值：
 
，然后列出了会话的所有培训参数。我尝试拔出其他码头图像，以查看是否会改变任何东西，没有效果。
我做错了什么，因为培训工作从未真正创建？]]></description>
      <guid>https://stackoverflow.com/questions/79492869/why-is-aws-sagemaker-not-submitting-a-training-job-when-i-create-an-estimator-ob</guid>
      <pubDate>Fri, 07 Mar 2025 16:07:57 GMT</pubDate>
    </item>
    <item>
      <title>对单词的偏斜纠正，只有一个/两个字母向右偏斜</title>
      <link>https://stackoverflow.com/questions/79492698/skew-correction-for-words-where-only-one-two-letters-are-skewed-to-the-right</link>
      <description><![CDATA[我目前正在为手写的阿拉伯手稿工作。阿拉伯语脚本是草拟的，字母在基线上加入。基线可能不一定是完全水平的（通常不是）。
我打算使用的模型是CRNN或VIT，其中输入将是文本图像中的每个连接组件。我不会将连接的组件（通常是单词或子字）分割为单个字符。一个问题是，有些字母向右倾斜，然后进入其他字母的区域（即列），如下所示。我认为，如果将图像分为垂直列，则此偏斜可能会引起问题：某些列将由一个以上的字符组成，这可能会使模型混淆。
因此，我正在寻找一个有效的解决方案，该解决方案将使我仅纠正仅在右边偏斜的字母的偏斜。这些通常是上升。我考虑过将连接的组件分割为分支点的点。然后，我将检查每个结果轮廓的偏斜，如果其长度和角度超过阈值，我将相应地修改原始图像。我不确定该解决方案的可行性 - 代码的一部分如下：
  def recript_skewed_strokes（binarized_image）：
    
    骨架=骨架（binarized_image // 255）.astype（np.uint8） * 255

    branch_points = []
    对于Y范围（1，骨架。形状[0] -1）：
        对于X范围（1，骨架。形状[1] -1）：
            如果骨架[y，x] == 255：
                邻居= np.sum（骨架[y -1：y + 2，x -1：x + 2]＆gt; 0） -  1＃找到像素具有的邻居数
                
                如果邻居＆gt; = 3：
                    branch_points.append（（y，x））
 
 带有偏斜的示例图像  ]]></description>
      <guid>https://stackoverflow.com/questions/79492698/skew-correction-for-words-where-only-one-two-letters-are-skewed-to-the-right</guid>
      <pubDate>Fri, 07 Mar 2025 14:55:24 GMT</pubDate>
    </item>
    <item>
      <title>努力从“共识游戏”纸[封闭]中复制结果</title>
      <link>https://stackoverflow.com/questions/79487352/struggling-to-reproduce-results-from-the-consensus-game-paper</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79487352/struggling-to-reproduce-results-from-the-consensus-game-paper</guid>
      <pubDate>Wed, 05 Mar 2025 18:18:31 GMT</pubDate>
    </item>
    <item>
      <title>实施心理健康监测的设备ML模型（文本，音频和行为分析）[封闭]</title>
      <link>https://stackoverflow.com/questions/79479223/implementing-on-device-ml-models-for-mental-health-monitoring-text-audio-and</link>
      <description><![CDATA[我正在开发一个分析设备的心理健康监测系统：
  1。文本输入分析  - 来自类型消息的情感分析和情感语调检测。
  2。音频分析 -  从语音记录中检测压力，焦虑和情绪。
  3。搜索历史记录＆amp;行为分析 -  识别浏览模式的变化，可能表明困扰。
 约束： 
必须完全在设备上运行（无云API）。
需要轻巧并在移动/边缘设备上工作。
 我的挑战： 
为每个任务选择合适的ML型号/框架，可以有效地运行在设备上。
预处理和特征提取技术用于实时分析。
优化推理速度而不会损害精度。
我有一些基本的ML知识，但以前从未实施完整的ML管道。对框架，模型选择和实施策略的任何指南将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79479223/implementing-on-device-ml-models-for-mental-health-monitoring-text-audio-and</guid>
      <pubDate>Sun, 02 Mar 2025 14:19:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Ollama-Python软件包有效地对本地美洲驼3.1型号进行有效呼叫？</title>
      <link>https://stackoverflow.com/questions/79394817/how-to-efficiently-make-concurrent-calls-to-a-local-llama-3-1-model-using-the-ol</link>
      <description><![CDATA[我使用的是本地骆驼3.1：7b型号。我正在使用Python和Ollama-Python软件包与模型进行交互。
我需要对该模型进行超过20,000个电话，但是我目前依次调用它，这是效率低下的。我正在寻找提高效率的方法。具体来说，我正在尝试找出是否有办法：

批次输入一次要一次发送多个请求。
使用螺纹，多处理或任何形式的并发处理同时处理多个呼叫，而不是依次使它们进行。
提高大量查询的总体速度和响应时间。

有人使用Ollama-Python软件包或用于处理Python中并发API调用的一般技术来实现这一目标或代码示例？即使解决方案没有特别涉及Ollama，我也愿意听到其他方法。
到目前为止，我已经尝试顺序运行该模型，目前正在介绍呼叫。]]></description>
      <guid>https://stackoverflow.com/questions/79394817/how-to-efficiently-make-concurrent-calls-to-a-local-llama-3-1-model-using-the-ol</guid>
      <pubDate>Tue, 28 Jan 2025 18:12:08 GMT</pubDate>
    </item>
    <item>
      <title>什么是spacy中的NLP？</title>
      <link>https://stackoverflow.com/questions/73734318/what-is-nlp-in-spacy</link>
      <description><![CDATA[通常我们从：开始
  nlp = spacy.load（&#39;en_encore_web_sm&#39;）＃或媒介或大型
 
或
  nlp =英语（）
 
然后：
  doc = nlp（&#39;我的文本&#39;）
 
那么，即使不知道第一行的性质，我们也可以做很多乐趣。
但是“ NLP”到底是什么？引擎盖下发生了什么？是“ nlp”在机器学习中所理解的一个验证的模型，因此在光盘上的某个地方？上
我遇到了一个解释，即“ nlp”是一个“对象，包含过程管道”，但这仅解释了一点。]]></description>
      <guid>https://stackoverflow.com/questions/73734318/what-is-nlp-in-spacy</guid>
      <pubDate>Thu, 15 Sep 2022 16:06:33 GMT</pubDate>
    </item>
    <item>
      <title>训练和测试数据的分配方式 - 张孔上的keras</title>
      <link>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split-keras-on-tensorflow</link>
      <description><![CDATA[我目前正在使用神经网络训练我的数据并使用拟合功能。
  history = model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，verialation_split = 0.2，verbose = 1）
 
现在，我已经使用验证_split 作为 20％。我了解的是，我的培训数据将为 80％，测试数据将为 20％。我很困惑这些数据在后端如何处理。它是否会像Top  80％的样品进行训练，并且低于20％的20％用于测试，或者样品是从介于两者之间随机选择的？如果我想提供单独的培训和测试数据，我将如何使用 model.fit（） ?? 进行操作。
此外，我的第二个问题是如何检查数据是否适合模型？从结果我可以看出，训练精度约为 90％，而验证精度约为 55％。这是否意味着过度安装或不合格的情况？
我的最后一个问题是评估回报是什么？文档表示它返回损失，但我已经在每个时期期间都会获得损失和准确性（作为fit的回报（）（在 history 中））。评估表演返回的精度和得分如何？如果通过评估返回 90％返回的准确性，我可以说我的数据很合适，无论每个时期的个人准确性和损失是什么？？
以下是我的代码：
 导入numpy
进口熊猫
导入matplotlib.pyplot作为PLT
来自keras.models导入顺序
来自keras.layers导入密集，辍学
来自keras.wrappers.scikit_learn导入kerasclassifier
来自sklearn.model_selection导入cross_val_score
从Sklearn.Preprocessing Import LabElenCoder
来自sklearn.model_selection导入stratifiedkfold
从sklearn.prepercorsing进口标准标准
来自Sklearn.Pipeline Import Pipeline
来自keras.utils导入np_utils
来自sklearn.model_selection导入kfold
来自sklearn.metrics导入混乱_matrix
导入Itertools

种子= 7
numpy.random.seed（种子）

dataframe = pandas.read_csv（&#39;inputfile.csv＆quort; skiprows = range（0，0））

dataset = dataframe.values
x = dataset [：，0：50] .astype（float）＃cols-1的数量
y =数据集[：，50]

encoder = labelencoder（）
coder.fit（y）
encoded_y = encoder.transform（y）

encoded_y = np_utils.to_categorical（encoded_y）
打印（&#39;encoded_y =＆quot; encoded_y） 
＃基线模型
def create_baseline（）：
    ＃创建模型
    型号=顺序（）
    model.Add（密集（5，input_dim = 5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    model.Add（密集（5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    ＃model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;sigmoid&#39;）））））

    model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;softmax&#39;）））））

    ＃编译模型
    model.compile（loss =&#39;binary_crossentropy&#39;，优化器=&#39;adam&#39;，metrics = [&#39;fecicy&#39;]）＃for binayr分类
        ＃model.compile（loss =&#39;cancorical_crossentropopy&#39;，importizer =&#39;adam&#39;，metrics = [&#39;cocuctiacy&#39;]）＃for Multi类
    返回模型
    

model = create_baseline（）;
历史= model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，验证_split = 0.2，冗长= 1）

print（history.history.keys（））
＃列出历史记录中的所有数据
print（history.history.keys（））
＃总结历史的准确性
plt.plot（历史学家[&#39;acc&#39;]）
plt.plot（history.history [&#39;val_acc&#39;]）
plt.title（“模型精度”）
plt.ylabel（“准确性”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）
＃总结损失的历史
plt.plot（历史学家[&#39;损失&#39;]）
plt.plot（历史学家[&#39;val_loss&#39;]）
plt.title（“模型损失”）
plt.ylabel（“损失”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）


pre_cls = model.predict_classes（x）    
cm1 = Confusion_matrix（encoder.transform（y），pre_cls）
打印（&#39;混淆矩阵：\ n&#39;）
打印（CM1）


得分，acc = model.evaluate（x，encoded_y）
打印（“测试分数：”，分数）
打印（“测试准确性：”，ACC）
 ]]></description>
      <guid>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split-keras-on-tensorflow</guid>
      <pubDate>Sun, 24 Jun 2018 02:28:07 GMT</pubDate>
    </item>
    <item>
      <title>无法获得线性的准确分数</title>
      <link>https://stackoverflow.com/questions/45627784/unable-to-obtain-accuracy-score-for-my-linear</link>
      <description><![CDATA[我正在基于IMDB数据进行回归模型，以预测IMDB值。在我的线性回归上，我无法获得准确的得分。 
我的代码行：
 量表
 
错误：
  valueerror：不支持连续
 
如果我要更改该线以获得R2分数，
  Metrics.R2_Score（test_y，linear_predicated_rating）
 
我能够获得R2而没有任何错误。 
有任何线索为什么我看到这个？
谢谢。
编辑：
我发现的一件事是 test_y 是熊猫数据框架，而 linear_predicated_rating 是以numpy数组格式。 ]]></description>
      <guid>https://stackoverflow.com/questions/45627784/unable-to-obtain-accuracy-score-for-my-linear</guid>
      <pubDate>Fri, 11 Aug 2017 05:46:30 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn.decomposition.pca的特征向量的简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我正在尝试了解主组件分析的工作方式，并且我正在 sklearn.datasets.load_iris  dataset上对其进行测试。  我了解每个步骤的工作原理（例如，使用 k 选定的尺寸，将数据，协方差，特征分类，对最高特征值进行排序，将原始数据转换为新轴）。）。
下一步是可视化这些 eigenVectors 在数据集中投射到（在 pc1 vs. pc2 vs. pc2 plot 上，对吗？）。。
有人可以解释如何在缩小尺寸数据集的3D图上绘制[PC1，PC2，PC3]特征向量？
另外，我是否正确绘制了此2D版本？我不确定为什么我的第一个特征向量的长度较短。  我应该乘以特征值吗？

 这是我为实现这一目标所做的一些研究： 
我关注的PCA方法来自：
 https://plot.ly/ipython-notebooks/principal-component-analysis/#shortcut--pca-in-scikit-learn （尽管我不想使用 Plotly 我想坚持使用。
我一直在关注本教程，以绘制特征向量，这似乎很简单： pca for matplotlib 基本示例
我找到了这个，但是我试图做的事情似乎过于复杂，我不想创建一个 fancyarrowpatch ： 的协方差矩阵

 我试图使我的代码尽可能直接地遵循其他教程： 
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT
来自sklearn.datasets import load_iris
从sklearn.prepercorsing进口标准标准
来自Sklearn进口分解
进口海洋为SNS； sns.set_style（&#39;whitegrid＆quort; {&#39;axes.grid&#39;：false}）

％matplotlib内联
np.random.seed（0）

＃虹膜数据集
df_data = pd.dataframe（load_iris（）。数据， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]，
                       列= load_iris（）。feature_names）

se_targets = pd.Series（load_iris（）。目标， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]， 
                       名称=“物种”

＃缩放平均= 0，var = 1
df_standard = pd.dataframe（standardscaler（）。fit_transform（df_data）， 
                           index = df_data.index，
                           列= df_data.columns）

＃用于主要组合分析的Sklearn

＃昏暗
m = df_standard.shape [1]
k = 2

＃PCA（我倾向于设置它）
m_pca = demomposition.pca（n_components = m）
df_pca = pd.dataframe（m_pca.fit_transform（df_standard）， 
                列= [＆quot; pc％d＆quot;范围内K的％k（1，m + 1）]）。iloc [：，：k]


＃绘制特征向量
#https：//stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

＃这是东西很奇怪的地方...
data = df_standard

mu = data.mean（轴= 0）
eigenVector，eigenvalues = m_pca.components_，m_pca.explained_variance_ #eigenVectors，eigenvalues，v = np.linalg.svd（data.t，fult_matrices = false）
Projected_data = df_pca＃np.dot（数据，特征向量）

sigma = Projected_data.std（axis = 0）.mean（）

图，ax = plt.subplots（figsize =（10,10））
ax.Scatter（Projected_data [＆quot; pc1;]，Projected_data [＆quot; pc2＆quot;]）
对于轴，zip中的颜色（eigenVectors [：k]，[red&#39;&#39;
＃开始，end = mu，mu + sigma *轴###导致“ valueerror：太多值无法打开（预期2）”

    ＃所以我尝试过，但我认为这是不对的
    start，end =（mu）[：k]，（Mu + Sigma * Axis）[：K] 
    ax.annotate（&#39;&#39;，xy = end，xytext = start，arrowprops = dict（faceColor = color，width = 1.0））
    
ax.set_aspect（&#39;quare&#39;）
plt.show（）
 
  &lt;img alt =“在此处输入图像描述” src =“ https://i.sstatic.net.net/t9st0.png”]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    <item>
      <title>r中的“神经网络”包，整流的线性单元（relu）激活函数？</title>
      <link>https://stackoverflow.com/questions/34532878/package-neuralnet-in-r-rectified-linear-unit-relu-activation-function</link>
      <description><![CDATA[我正在尝试使用R包装神经网络中预先实现的“ Logistic”和“ Tanh”以外的激活功能。具体来说，我想使用整流的线性单元（relu）f（x）= max {x，0}。请在下面查看我的代码。
我相信，如果（例如）定义，我可以使用自定义功能

  custom＆lt;  -  function（a）{x*2}
 
但是，如果我设置最大（x，0）而不是x*2，则r告诉我“最大不在衍生物表中”，而对于&#39;&gt;&#39;运算符相同。因此，我正在寻找明智的解决方法，因为在这种情况下，我正在考虑Max的数值整合不是问题。
  nn＆lt;  -  neuralnet（
  as.formula（粘贴（“ x”，粘贴（名称（z [，2：10]）），collapse =“+”），sep =“〜”），），），），），），
  data = z [，1：10]，隐藏= 5，err.fct =“ sse”，
  act.fct =“ logistic”，rep = 1，
  linear.output = true）
 
有什么想法吗？我有点困惑，因为我不认为 neuralnet 软件包会进行分析。]]></description>
      <guid>https://stackoverflow.com/questions/34532878/package-neuralnet-in-r-rectified-linear-unit-relu-activation-function</guid>
      <pubDate>Wed, 30 Dec 2015 16:02:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么我会获得负面信息收益？</title>
      <link>https://stackoverflow.com/questions/31362320/why-am-i-getting-a-negative-information-gain</link>
      <description><![CDATA[一个人会获得负面信息增益是没有意义的。
但是，基于此示例，我获得了负面信息增益。
这是数据：
  
，如果我计算有关湿度属性的信息增益，我会得到：
  
显然我在这里缺少一些东西。
编辑
澄清我的理解。
整个系统的熵定义为：
  
在这种情况下是：
  
和每个属性的信息增益定义为：
  
对于湿度，我计算为：
系统的熵 - （1/4）湿度正常的熵 - （3/4）湿度高的熵
根据这个libre办公室计算：
  
还是我对属性信息获得的公式的理解是不正确的吗？
解决
我的错误是我没有意识到如果所有类型都是熵，则熵为0。因此，如果一切都是正的，则熵为0，如果全部为负，则也为零。如果相等的量为正且负数，则熵为1。]]></description>
      <guid>https://stackoverflow.com/questions/31362320/why-am-i-getting-a-negative-information-gain</guid>
      <pubDate>Sat, 11 Jul 2015 22:08:45 GMT</pubDate>
    </item>
    <item>
      <title>训练单层感知时的激活功能</title>
      <link>https://stackoverflow.com/questions/515568/activation-function-when-training-a-single-layer-perceptron</link>
      <description><![CDATA[当训练多层神经网络时，使用sigmoidal激活功能才能有效学习。 
训练单层 layer perceptron时，使用sigmoidal激活函数是否有任何优势，或者是一个简单的步骤（Heaviside）函数足够（甚至更可取）？]]></description>
      <guid>https://stackoverflow.com/questions/515568/activation-function-when-training-a-single-layer-perceptron</guid>
      <pubDate>Thu, 05 Feb 2009 11:50:30 GMT</pubDate>
    </item>
    </channel>
</rss>