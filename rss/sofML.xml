<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 04 Aug 2024 12:26:34 GMT</lastBuildDate>
    <item>
      <title>检测并定位图像中大量不同的物体</title>
      <link>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</link>
      <description><![CDATA[我有多个 Match 3 游戏的图片，例如 Candy Crush。下面给出了一个示例图片。我想检测和定位图片中所有不同颜色的物体。定位的意思是获取图片中每个物体的精确坐标。我尝试过不同的方法，例如边缘检测技术、轮廓和其他一些方法，但到目前为止效果并不理想。无法使用模板匹配，因为图像大小不同。我正在考虑切换到 ML 技术，特别是 CNN，但为此我必须创建一个庞大的数据集，而且不确定这是否可行。那么有没有可以解决上述问题的计算机视觉方法呢？
]]></description>
      <guid>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</guid>
      <pubDate>Sun, 04 Aug 2024 10:33:33 GMT</pubDate>
    </item>
    <item>
      <title>Pandas 高效地获取每个数据框组中具有多个 n 值的前 n 行[重复]</title>
      <link>https://stackoverflow.com/questions/78830630/pandas-efficiently-get-top-n-rows-in-each-dataframe-group-with-multiple-values-o</link>
      <description><![CDATA[对于这种用例来说，Groupby-apply 太慢了。从类似这样的数据框开始
df = pd.DataFrame(
[
{&#39;name&#39;: &#39;a&#39;, &#39;id&#39;: 0, &#39;category&#39;: &#39;1&#39;},
{&#39;name&#39;: &#39;b&#39;, &#39;id&#39;: 1, &#39;category&#39;: &#39;1&#39;},
{&#39;name&#39;: &#39;c&#39;, &#39;id&#39;: 2, &#39;category&#39;: &#39;1&#39;},
{&#39;name&#39;: &#39;d&#39;, &#39;id&#39;: 3, &#39;category&#39;: &#39;1&#39;},
{&#39;name&#39;: &#39;e&#39;, &#39;id&#39;: 4, &#39;category&#39;: &#39;2&#39;},
{&#39;name&#39;: &#39;f&#39;, &#39;id&#39;: 5, &#39;category&#39;: &#39;2&#39;},
{&#39;name&#39;: &#39;g&#39;, &#39;id&#39;: 6, &#39;category&#39;: &#39;2&#39;},
{&#39;name&#39;: &#39;h&#39;, &#39;id&#39;: 7, &#39;category&#39;: &#39;3&#39;},
{&#39;name&#39;: &#39;i&#39;, &#39;id&#39;: 8, &#39;category&#39;: &#39;3&#39;},
{&#39;name&#39;: &#39;j&#39;, &#39;id&#39;: 9, &#39;category&#39;: &#39;3&#39;},
]
)

或
 name id category
0 a 0 1
1 b 1 1
2 c 2 1
3 d 3 1
4 e 4 2
5 f 5 2
6 g 6 2
7 h 7 3
8 i 8 3
9 j 9 3

我想提取每个类别的前 n 行，每个类别的 n 值不同。如果 n 对于所有类别都相同，那么这个问题的解决方案将会非常快速：
df_head = df.groupby(by=[&#39;category&#39;], as_index=False, sort=False).head(2)

给出
 name id category
0 a 0 1
1 b 1 1
4 e 4 2
5 f 5 2
7 h 7 3
8 i 8 3

但如果我想要不同的 n，我只能使用类似 这个
num_grabs = {&#39;1&#39;: 3, &#39;2&#39;: 1, &#39;3&#39;: 2}
df_apply = df.groupby(
by=[&#39;category&#39;],
as_index=False,
sort=False
).apply(
lambda x: x.head(num_grabs[x.loc[x.index[0], &#39;category&#39;]])
).reset_index(drop=True)

正确给出
 name id category
0 a 0 1
1 b 1 1
2 c 2 1
3 e 4 2
4 h 7 3
5 i 8 3

但是，即使是中等大小的数据，这也非常慢。有没有更有效的方法来解决多n问题？]]></description>
      <guid>https://stackoverflow.com/questions/78830630/pandas-efficiently-get-top-n-rows-in-each-dataframe-group-with-multiple-values-o</guid>
      <pubDate>Sun, 04 Aug 2024 08:48:58 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 不兼容的形状：计算 MSE 时 [64] 与 [64,8,8,3]</title>
      <link>https://stackoverflow.com/questions/78830210/tensorflow-incompatible-shapes-64-vs-64-8-8-3-when-calculating-mse</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78830210/tensorflow-incompatible-shapes-64-vs-64-8-8-3-when-calculating-mse</guid>
      <pubDate>Sun, 04 Aug 2024 03:36:33 GMT</pubDate>
    </item>
    <item>
      <title>如何将风格化效果应用于视频？</title>
      <link>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</guid>
      <pubDate>Sun, 04 Aug 2024 03:36:28 GMT</pubDate>
    </item>
    <item>
      <title>加载 Keras 模型时，“密集层”需要 1 个输入，但它收到了 2 个输入张量”</title>
      <link>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</link>
      <description><![CDATA[我正在使用 Kaggle 开发乳腺癌组织病理学图像的分类模型。该数据集包含 157,572 张图像（78,786 张 IDC 阴性和 78,786 张 IDC 阳性），每张图像的尺寸为 50x50 像素。
我使用 ResNet50 作为基础模型，并尝试保存并稍后加载经过训练的模型的最高效版本。但是，当我尝试加载已保存的模型时，我遇到了以下错误：
# 加载模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)
“密集”层需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_566&gt;, &lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_567&gt;]

这是我的代码：
import tensorflow as tf
from tensorflow.keras import layer, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import load_model

# 数据增强管道
data_augmentation = tf.keras.Sequential([
layer.RandomFlip(&quot;horizo​​ntal_and_vertical&quot;),
layer.RandomRotation(0.5),
layer.RandomContrast(0.2),
layer.RandomBrightness(0.2),
layer.GaussianNoise(0.1)
])

# 预处理函数以规范化图像
def preprocess(image, label):
image = tf.image.resize(image, (50, 50))
image = tf.cast(image, tf.float32) / 255.0
return image, label

# 使用验证分割加载数据集
train_dir = &#39;path/to/data&#39;
batch_size = 64
img_height = 50
img_width = 50
validation_split = 0.2
test_split_ratio = 0.5

# 加载数据集
data_train = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

data_val = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

# 将验证集拆分为验证集和测试集
def split_dataset(dataset, split_ratio=0.5):
dataset_size = len(dataset)
split = int(split_ratio * dataset_size)
train_dataset = dataset.take(split)
test_dataset = dataset.skip(split)
return train_dataset, test_dataset

data_val, data_test = split_dataset(data_val, split_ratio=test_split_ratio)

# 应用预处理和增强
data_train = data_train.map(lambda x, y: (data_augmentation(x, training=True), y)).map(preprocess)
data_val = data_val.map(preprocess)
data_test = data_test.map(preprocess)

# 预取数据以获得更好的性能
data_train = data_train.prefetch(tf.data.AUTOTUNE)
data_val = data_val.prefetch(tf.data.AUTOTUNE)
data_test = data_test.prefetch(tf.data.AUTOTUNE)

# 定义 ResNet50 模型
input_shape = (50, 50, 3)
base_model = ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=input_shape)
base_model.trainable = False

# 构建模型
model = models.Sequential([
layer.Input(shape=(img_height, img_width, 3)),
base_model,
layer.GlobalAveragePooling2D(),
layer.Dense(256,activation=&#39;relu&#39;),
layer.Dropout(0.5),
layer.Dense(1,activation=&#39;sigmoid&#39;)
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 用于提前停止和检查点的回调
early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;,patient=5, restore_best_weights=True)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
filepath=&#39;/kaggle/working/resnet50_model.keras&#39;,
monitor=&#39;val_loss&#39;,
mode=&#39;min&#39;,
save_best_only=True,
save_weights_only=False
)

# 训练模型
model.fit(data_train, epochs=50, validation_data=data_val, callbacks=[early_stopping, model_checkpoint_callback])

# 微调模型
base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.fit(data_train, epochs=50, validation_data=data_val,回调=[early_stopping, model_checkpoint_callback])

# 保存模型
model.save(&#39;/kaggle/working/resnet50_model.keras&#39;)

# 加载最佳微调模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)

为了调试该问题，我在预处理阶段通过打印数据批次的形状来检查图像输入。结果如下：

训练数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

验证数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

测试数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)


我预计保存的模型可以顺利加载，这样我就可以使用训练后的权重和架构进行评估和预测。]]></description>
      <guid>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</guid>
      <pubDate>Sat, 03 Aug 2024 20:01:05 GMT</pubDate>
    </item>
    <item>
      <title>无法解决 QiskitMachineLearningError：'输入数据的形状不正确，最后一个维度不等于输入的数量：0，但得到：2'</title>
      <link>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</link>
      <description><![CDATA[我收到错误：
32 vqc.fit(X_train, X_test)
33 
34 # 评估分类器

15 帧
/usr/local/lib/python3.10/dist-packages/qiskit_machine_learning/neural_networks/neural_network.py in _validate_input(self, input_data)
132 
133 if shape[-1] != self._num_inputs:
-&gt; 134 引发 QiskitMachineLearningError(
135 f&quot;输入数据的形状不正确，最后一个维度 &quot;
136 f&quot;不等于输入数量： &quot;

QiskitMachineLearningError：&#39;输入数据的形状不正确，最后一个维度不等于输入数量：0，但得到：2。&#39;

来自 qiskit 导入 QuantumCircuit、transpile、assemble
来自 qiskit_aer 导入 Aer
来自 qiskit_machine_learning.algorithms 导入 VQC
来自 qiskit_algorithms.optimizers 导入 COBYLA
来自 qiskit.circuit.library 导入 TwoLocal
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.model_selection 导入 train_test_split
导入 numpy 作为 np

# 用于演示目的的样本数据
# 将其替换为您的实际数据
scaled_data = np.random.rand(150, 2) # 用您的缩放数据替换
y = np.random.randint(0, 3, size=150) # 用您的标签替换

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)

# 根据特征维度定义量子比特的数量
num_qubits = X_train.shape[1]

# 使用正确的参数定义量子特征图和 ansatz
feature_map = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;) #, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)
ansatz = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;)#, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)

# 定义优化器
optimizer = COBYLA()

# 使用唯一参数名称初始化 VQC 分类器
vqc = VQC(feature_map=feature_map, ansatz=ansatz, optimizer=optimizer)

# 训练量子分类器
vqc.fit(X_train, X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</guid>
      <pubDate>Sat, 03 Aug 2024 14:52:35 GMT</pubDate>
    </item>
    <item>
      <title>分类任务的 Brier 评分 [关闭]</title>
      <link>https://stackoverflow.com/questions/78828870/brier-score-for-a-classification-task</link>
      <description><![CDATA[我为分类问题训练了几个模型，我想计算它们预测的 Brier 分数。
尽管如此，我不太确定应该从 DescTools 向公式传递什么（即 BrierScore(x, pred = NULL, scaled = FALSE, ...)）。
到目前为止，这是我为其中一个模型的概率创建的数据框：
 X0 X1 real_scores max_prob max_source
1 0.024 0.976 1 0.976 1
2 0.910 0.090 0 0.910 0
3 0.524 0.476 0 0.524 0
4 0.942 0.058 0 0.942 0
5 0.944 0.056 0 0.944 0
6 0.074 0.926 1    0.926 1 7 0.254 0.746 0 0.746 1 8 0.864 0.136 0 0.864 0 9 0.522 0.478 0 0.522 0 10 0.422 0.578 1 0.578 1 11 0.772 0.228 0 0.772 0 12 0.564 0.436 0 0.564 0 13 0.968 0.032 0 0.968 0 14 0.760 0.240 0 0.760 0 15 0.978 0.022 0 0.978 0 16 0.818 0.182           1 0.818 0 17 0.730 0.270 0 0.730 0 18 0.824 0.176 0 0.824 0 19 0.962 0.038 0 0.962 0 20 0.514 0.486 0 0.514 0 21 0.360 0.6 40 0 0.640 1 22 0.708 0.292 0 0.708 0 23 0.940 0.060 0 0.940 0 24 0.916 0.084 0 0.916 0 25 0.606 0.394 1 0.606 0 26 0.838 0.162 0 0.838 0
27 0.742 0.258 0 0.742 0
28 0.850 0.150 1 0.850 0

其中：

X0 = 负类的概率
X1 = 正类的概率
real_scores = 1（正类）；0（负类）
max_prob = 两个类（X0、X1）的最大概率
max_source = 最大概率来自哪个类别（X0 或 X1）。

我不明白的是，我是否必须仅将 X1（正类）的概率、两者的概率或最大值传递给公式。]]></description>
      <guid>https://stackoverflow.com/questions/78828870/brier-score-for-a-classification-task</guid>
      <pubDate>Sat, 03 Aug 2024 14:02:24 GMT</pubDate>
    </item>
    <item>
      <title>Kaldi steps/nnet3/align.sh 中缺少对齐文件 (ali.*.gz) [关闭]</title>
      <link>https://stackoverflow.com/questions/78828864/missing-alignment-files-ali-gz-in-kaldi-steps-nnet3-align-sh</link>
      <description><![CDATA[我正在尝试使用 Kaldi 训练语音识别模型。我已成功运行 steps/nnet3/align.sh 脚本，但 exp/chain/tree_sp 目录中未创建预期的 ali.1.gz 文件。
我已检查 exp/chain/tree_sp/log 目录中的终端输出和日志文件，但没有错误消息。该脚本似乎运行正常，但缺少所需的输出。
您能否建议此问题的潜在原因或进一步调试的步骤？如何解决？
我已成功运行 steps/nnet3/align.sh 脚本（终端或日志文件中没有错误）。
我已经检查过 exp/chain/tree_sp 目录中是否存在 ali.JOB.gz 文件，但它们不存在（未创建）
我希望 steps/nnet3/align.sh 脚本能够在 exp/chain/tree_sp 目录中成功创建 ali.1.gz 文件（或多个 ali.JOB.gz 文件）。此文件对于 Kaldi 管道中的后续训练步骤至关重要。
ls exp/chain/tree_sp
0.mdl cmvn_opts final.mat final.mdl final.occs full.mat log num_jobs phones.txt splice_opts tree

当我运行 local/chain/tuning/run_tdnn_1j.sh 时收到此错误
Traceback（最近一次调用最后一次）：
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 651 行，在 main
train(args, run_opts)
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 287 行，在 train
chain_lib.check_for_required_files(args.feat_dir, args.tree_dir,
文件 &quot;/mnt/d/kaldi/egs/mini_librispeech/s5/steps/libs/nnet3/train/chain_objf/acoustic_model.py&quot;，第 378 行，位于 check_for_required_files
引发异常(&#39;预期 {0} 存在。&#39;.format(file))
异常：预期 exp/chain/tree_sp/ali.1.gz 存在。
]]></description>
      <guid>https://stackoverflow.com/questions/78828864/missing-alignment-files-ali-gz-in-kaldi-steps-nnet3-align-sh</guid>
      <pubDate>Sat, 03 Aug 2024 14:01:13 GMT</pubDate>
    </item>
    <item>
      <title>想要对像 SAM 这样的预训练模型进行微调，以便在微观水样数据集中进行细菌分割。很难找到所需的数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</link>
      <description><![CDATA[我正在开展一个机器学习项目，旨在使用显微图像测试水的纯度。该项目的目标是：
对样本图像中存在的各种细菌进行分割。识别不同类型的细菌。根据识别出的细菌数量和类型评估水的纯度。
我很难找到并准备一个包含不同类型细菌的水样显微图像的合适数据集。
考虑到我的任务的特殊性，我应该采取什么方法来微调像 SAM（任何分割模型）这样的预训练模型来进行细菌分割？任何关于超参数、数据增强或其他训练策略的提示都会有所帮助。
获取数据集困难。]]></description>
      <guid>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</guid>
      <pubDate>Sat, 03 Aug 2024 11:32:03 GMT</pubDate>
    </item>
    <item>
      <title>当数据集中的每个数据都是 csv 文件时，机器学习方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</link>
      <description><![CDATA[我正在使用传感器测量金属物体周围的磁流。传感器每毫秒记录一次磁流，每秒可进行 1000 次测量。这些值存储在 CSV 文件中，其中第一列表示沿 X 轴的测量值，第二列表示沿 Y 轴的测量值，第三列表示沿 Z 轴的测量值，第四列包含以毫秒为单位的时间。
我的任务是测量 50 种不同金属物体的磁流，为每个物体创建单独的 CSV 文件。最终，我计划将这些单独的文件用作训练机器学习模型的数据集。目标是使用机器学习和这些数据根据金属的磁流特性确定金属的类型（如果您感兴趣，可以搜索“mfl 方法”）。但是，我不确定如何处理这种特殊情况，因为大多数机器学习代码都要求数据集中的每个数据点都是一行。在这种情况下，每个数据点都是一个包含多行的 CSV 文件。
您能提供任何指导吗？
目前我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</guid>
      <pubDate>Sat, 03 Aug 2024 10:09:57 GMT</pubDate>
    </item>
    <item>
      <title>“AttributeError:‘NoneType’对象没有属性‘cget_managed_ptr’”是什么意思？</title>
      <link>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</guid>
      <pubDate>Sat, 03 Aug 2024 06:12:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中有效地将大型 .txt 文件拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</link>
      <description><![CDATA[我有一个非常大的 .txt 文件（几 GB），我需要将其拆分为机器学习项目的训练集和测试集。由于内存限制，将整个文件读入内存然后拆分的常用方法不可行。我正在寻找一种高效拆分文件而不使内存过载的方法。
我尝试使用 scikit-learn 进行拆分，但它会将整个文件加载到内存中，这会导致性能问题，不适合我的大型数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</guid>
      <pubDate>Sat, 03 Aug 2024 03:36:13 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 Pytorch 的 CrossEntropyLoss 应用类权重来解决多类多输出问题的不平衡数据分类问题</title>
      <link>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</link>
      <description><![CDATA[我正在尝试使用加权损失函数来处理数据中的类别不平衡问题。我的问题是多类别和多输出问题。例如（我的数据有五个输出/目标列（output_1、output_2、output_3），每个目标列有三个类（class_0、class_1 和 class_2）。我目前正在使用 pytorch 的交叉熵损失函数https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html，我看到它有一个权重参数，但我的理解是，这个相同的权重将统一应用于每个输出/目标，但我想在每个输出/目标中为每个类应用单独的权重。
具体来说，我可以获得如下所示的数据



A
B
C
D
E
OUTPUT_1
OUTPUT_2
OUTPUT_3




5.65
3.56
0.94
9.23
6.43
0
2
1


7.43
3.95
1.24
7.22
&lt; td&gt;2.66
0
0
0


9.31
2.42
2.91
2.64
6.28
2
0
2


8.19
5.12
1.32
3.12
8.41
0
2
0


9.35
1.92
3.12
4.13
3.14
0
1
1


8.43
9.72
7.23
8.29
9.18
1
0
2


4.32
2.12
3.84
9.42
8.19
0
1
0


3.92
3.91
2.90
8.1 9
8.41
2
0
2


7.89
1.92
4.12
8.19
7.28
0
1
2
&lt; /tr&gt;

5.21
2.42
3.10
0.31
1.31
2
0
0



因此，
输出 1 中的比例为：0 = 0.6、1 = 0.1、2 = 0.3
输出 2 中的比例为：0 = 0.4、1 = 0.3、2 = 0.3
输出 3 中的比例为：0 = 0.4、1 = 0.2、2 = 0.4

我想根据每个输出列中的类分布应用类权重，以便它重新规范化（或重新平衡？不确定这里要使用的术语是什么）第 1 类为 0.15，第 0 类和第 2 类各为 0.425（因此对于 output_1，权重将是 [0.425/0.6, 0.15/0.1, 0.425/0.3]，对于输出 2，它将是 [0.425/0.4, 0.15/0.3, 0.425/0.3] 等）。相反，我理解 pytorch 的 crossentropy 损失函数中的权重参数目前正在执行的操作是将单个类权重应用于每个输出列。我想知道我是否遗漏了什么，是否有办法使用 pytorch 的 crossentropyloss 函数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</guid>
      <pubDate>Fri, 02 Aug 2024 03:34:55 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg（mlr3）错误：对“prob”的断言失败：包含缺失值（元素 1）[关闭]</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[当我运行代码时，该代码在堆叠学习器（glmnet 和 rpart）上执行特征选择和超参数调整，我收到以下错误消息：
assert_binary(truth, prob = prob, positive = positive, na_value = na_value) 中出错：
“prob”上的断言失败：包含缺失值（元素 1）。
这发生在 PipeOp classif.avg 的 $train()

但是，当我使用 classif.debug 时，预测中没有 NA。任何建议都将不胜感激。
注意：我简化了要调整的参数数量和要选择的特征数量，以减少执行时间，现在使用 classif.debug 只需 15 秒。
这是我的数据：https://www.dropbox.com/scl/fi/hkjs79i89gjz0j5mjlbj8/Data.csv?rlkey=08yuzet3mjr9gcezkryo93vqm&amp;st=hfv2cbeo&amp;dl=0
这是我的代码：
set.seed(1)
data &lt;- read.csv(&quot;C:/Users/Marine/Downloads/Data.csv&quot;)
data &lt;- data[,c(&quot;x&quot;, &quot;y&quot;, &quot;presence&quot;, &quot;V01&quot;, &quot;V02&quot;)]
## dim(data)
data$presence &lt;- as.factor(data$presence)
##摘要（数据）
任务 &lt;- mlr3spatial::as_task_classif_st（x = 数据，目标 = “存在”，正 = “1”，坐标名称 = c（“x”，“y”），crs = “+proj=longlat +datum=WGS84 +no_defs +type=crs”）
摘要（任务）

learner_glmnet &lt;- mlr3::lrn（“classif.glmnet”，预测类型 = “prob”，s = 0.01）
learner_rpart &lt;- mlr3::lrn（“classif.rpart”，预测类型 = “prob”，cp = to_tune（1e-04，1e-1，对数尺度 = TRUE))
learner_glmnet_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_glmnet, id = &quot;glmnet_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))
learner_rpart_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_rpart, id = &quot;rpart_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))

learner_avg &lt;- mlr3pipelines::LearnerClassifAvg$new(id = &quot;classif.avg&quot;)
learner_avg$predict_type &lt;- &quot;prob&quot;
learner_avg$param_set$values$measure &lt;- &quot;classif.auc&quot;

learner_debug &lt;- lrn(&quot;classif.debug&quot;, predict_type = &quot;prob&quot;)

level_0_graph &lt;- mlr3pipelines::gunion(list(learner_glmnet_cv, learner_rpart_cv)) %&gt;&gt;% mlr3pipelines::po(&quot;featureunion&quot;)
level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_avg
## level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_debug
level_0_and_1_graph_learner &lt;- mlr3::as_learner(level_0_and_1_graph)

tuning &lt;- mlr3tuning::auto_tuner(tuner = mlr3tuning::tnr(&quot;grid_search&quot;), 
learner = level_0_and_1_graph_learner,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

feature_selection &lt;- mlr3fselect::auto_fselector(fselector = mlr3fselect::fs(&quot;sequence&quot;, strategies = &quot;sfs&quot;, min_features = 2),
learner = tuning,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

system.time(stacking &lt;- mlr3::resample(task = task, 
learner = feature_selection, 
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
store_models = TRUE))

测试 &lt;- as.data.table(stacking$prediction())
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[1]])
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[2]])
which(is.na(test))
]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器的 SHAP 解释中 expected_value 的计算</title>
      <link>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</link>
      <description><![CDATA[我们如何理解SHAP explainer.expected_value？为什么经过sigmoid变换后，它与y_train.mean()不一样？
下面是代码摘要，供快速参考。完整代码可在此笔记本中找到：https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)
explainer = shap.Explainer(model)
shap_test = explainer(X_test)
shap_df = pd.DataFrame(shap_test.values)

#对于每种情况，如果我们将所有特征的 shap 值加上预期值相加，我们就可以得到该情况的边际，然后可以将其转换为返回该情况的预测概率case:
np.isclose(model.predict(X_test, output_margin=True),explainer.expected_value + shap_df.sum(axis=1))
#True

但是为什么下面不成立？为什么经过 sigmoid 变换后，XGBoost 分类器的 explainer.expected_value 与 y_train.mean() 不一样？
expit(explainer.expected_value) == y_train.mean()
#False
]]></description>
      <guid>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</guid>
      <pubDate>Mon, 18 Sep 2023 09:28:55 GMT</pubDate>
    </item>
    </channel>
</rss>