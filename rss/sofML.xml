<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 16 Mar 2025 18:22:14 GMT</lastBuildDate>
    <item>
      <title>从Python流程启动独立流程的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79512845/what-would-be-the-best-way-to-launch-a-detached-process-from-a-python-process</link>
      <description><![CDATA[我有一个fastapi aplication，我想以一种非阻滞方式启动一个带有python脚本的过程：
  process = subprocess.popen（
                [python＆quot; my_script.py;]，
                stdout = subprocess.devnull，
                stderr = subprocess.devnull，
                stdin = subprocess.devnull，
                start_new_session = true＃分离该过程
）
 
现在，问题是，如果该过程失败，它仍然作为僵尸过程徘徊。只有在我重新启动FastApi恢复过程时，该过程才会真正消失。
如果我尝试在my_script.py中使用os.fork来确保我与父母分离，那么将创建一个带有新PID的新过程，而原始的过程将保留为僵尸。
问题是，从系统工程的角度来看，从FastApi Aplication启动/触发Python脚本的最可靠的方法（和生产）是什么，如果存在问题，可以正确死亡？
我不能使用芹菜，因为我正在尝试训练机器学习模型，这可能需要大量时间和资源，恐怕芹菜工人不适合这一点，但是如果我错了，请纠正我。]]></description>
      <guid>https://stackoverflow.com/questions/79512845/what-would-be-the-best-way-to-launch-a-detached-process-from-a-python-process</guid>
      <pubDate>Sun, 16 Mar 2025 16:17:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Penzai和Optax进行每个参数的学习率</title>
      <link>https://stackoverflow.com/questions/79512816/how-to-scale-learning-rates-per-parameter-with-penzai-and-optax</link>
      <description><![CDATA[我想训练我在 nofollow noreferrer”&gt; penzai 元数据，例如。像这样：
 参数（
    label =&#39;mlp/affine_0/linear.weights&#39;，
    value =＆lt;名为float32（|功能：784，features_out：128）（包装jax.array）＆gt; ,,
    元数据= {&#39;Learning_rate&#39;：0.0012755102040816326}，
）
 
我使用penzai的 statefultrainer 进行培训，并声明这样的优化器：
  optax.chain（链）（
    optax.scale_by_adam（），
    scale_by_metadata_value（&#39;Learning_rate＆quot;），
    optax.scale_by_learning_rate（0.01），
）
 
我在哪里定义 scale_by_metadata_value 像这样：
  def scale_by_metadata_value（metadata_field_name：str）：
    def init_fn（params）：
        Learning_rates = jax.tree.map（
            lambda param：param.metadata [metadata_field_name]，
            参数，
            is_leaf =（lambda节点：isInstance（node，pz.parametervalue））
        ）
        返回{＆quot; learning_rates＆quot;：learning_rates}

    def Update_fn（更新，状态，参数）：
        del params
        更新= jax.tree.map（
            ＃这是TypeError抛出的地方：
            Lambda LR，G：lr * g，state [＆quot; learne_rates; quot;]，更新
        ）
        返回更新，状态

    返回optax.gradienttransformation（init_fn，update_fn）
 
但是，当我执行训练步骤时，我会得到
  typeError： *：&#39;jaxlib.xla_extension.arrayimpl&#39;and&#39;parametervalue&#39;
 
我特别令人困惑，因为当我删除 scale_by_metadata_value（&#39;Learning_rate＆quot&#39;&#39;） line时，一切都起作用，尽管optax.scale_by_learning_rate（0.01）与我使用 scal_by_metadataa_value 
 实现 scale_by_metadata_value ？ 的正确/最佳方法是什么
这是一个最小的失败示例（Penzai版本0.2.4）：
 导入penzai.toolshed.basic_training
进口Penzai
导入Penzai.pz作为PZ
导入JAX
导入jax.numpy作为jnp
导入Optax
导入numpy作为NP


型号= pz.nn.linear（
    权重= Pz.参数（参数
        value = pz.nx.wrap（np.ones（（（8，4））），“功能”，“功能”;
        标签=“线性”
        metadata = {＆quot; learning_rate＆quot;：0.5}，
    ），
    in_axis_names =（“功能”，），
    out_axis_names =（; quot; features_out＆quort;），，
）


DEF SOFTMAX_CROSS_ENTROPY_LOSS（
    型号，rng，state，current_input，current_target：pz.nx.NamedArray
）：
    Del RNG，州
    logits：pz.nx.NamedArray =模型（current_input）
    损失= jnp.sum（
        optax.losses.softmax_cross_entropy（
            logits.unwrap（“ features_out＆quot”），
            current_target.unwrap（“ features_out; quot”），，
        ）
    ）
    返回（损失，无，{＆quot; softmax_cross_entropy_loss＆quot;：lose}）


def scale_by_metadata_value（metadata_field_name：str）：
    def init_fn（params）：
        Learning_rates = jax.tree.map（
            lambda param：param.metadata [metadata_field_name]，
            参数，
            is_leaf =（lambda节点：isInstance（node，pz.parametervalue）），），
        ）
        返回{＆quot; learning_rates＆quot;：learning_rates}

    def Update_fn（更新，状态，参数）：
        del params
        更新= jax.tree.map（lambda lr，g：lr * g，state [＆quot; learning_rates＆quort＆quort＆quort＆quort&#39;&#39;，更新）
        返回更新，状态

    返回optax.gradienttransformation（init_fn，update_fn）


培训师= penzai.toolshed.basic_training.statefultrainer.build（
    root_rng = jax.random.key（2025），
    模型=模型，
    Optimizer_Def = optax.chain（
        optax.scale_by_adam（），
        scale_by_metadata_value（&#39;Learning_rate＆quot;），
        optax.scale_by_learning_rate（0.01），
    ），
    lose_fn = softmax_cross_entropy_loss，
    jit = false，
）

trainer.step（
    current_input = pz.nx.wrap（np.zeros（8），“功能”，“），
    current_target = pz.nx.wrap（np.ones（4），;
）
＃typeError： *：&#39;jaxlib.xla_extension.arrayimpl&#39;和&#39;parametervalue&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/79512816/how-to-scale-learning-rates-per-parameter-with-penzai-and-optax</guid>
      <pubDate>Sun, 16 Mar 2025 15:55:53 GMT</pubDate>
    </item>
    <item>
      <title>标题：为什么我的shap值尺寸[sample_num，feature，class]而不是[class，sample_num，功能]？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79512564/title-why-are-my-shap-values-dimensions-sample-num-feature-class-instead-of</link>
      <description><![CDATA[为什么所有AI都认为塑形值的计算如下？
  x_sample = pd.dataframe（x_train_resampled，columns = x.columns）[：500]  
Invelimer_rf = shap.treeexplainer（rf_model）  
shap_values_rf = rumenter_rf.shap_values（x_sample）  
 
生成的 shap_values_rf 尺寸应为[class，sample_num，功能]，但实际上，我的输出是[sample_num，feature，class]。我迫切需要对此进行合理的解释。
 null，我尝试过许多AI来生成Shap摘要图，并且尺寸似乎存在问题。即使使用默认的IRIS数据集，描述的问题仍然存在。 AI始终假定类维度处于第一个位置。]]></description>
      <guid>https://stackoverflow.com/questions/79512564/title-why-are-my-shap-values-dimensions-sample-num-feature-class-instead-of</guid>
      <pubDate>Sun, 16 Mar 2025 12:42:47 GMT</pubDate>
    </item>
    <item>
      <title>我在哪里可以找到用于培训AI代码质量Asessor的数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79512447/where-do-i-find-datasets-to-train-an-ai-code-quality-asessor</link>
      <description><![CDATA[对于我的大学项目，我的任务是创建和培训AI模型，以审查给定的代码。至少应该说“好”或＆quot“ bad＆quot”，但如果更详细的话，那会更好。
这是事实：我不允许使用Kaggle或GitHub的任何数据集，因为它们处理了大量处理。老实说，这是最困难的部分。他们鼓励我们使用网络搭配，但这不会给我带来未标记的数据，然后我必须给自己标记？ （例如好代码，坏代码）
本质上，我的问题是：我在哪里可以找到该项目的数据集（或者请找到一个数据集（如果您知道的话，请找到一个），或者我如何为该项目有效刮擦数据？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79512447/where-do-i-find-datasets-to-train-an-ai-code-quality-asessor</guid>
      <pubDate>Sun, 16 Mar 2025 10:46:02 GMT</pubDate>
    </item>
    <item>
      <title>训练后，我们可以从文件夹中删除某些图像吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79511077/can-we-make-a-bot-delete-certain-images-from-the-folder-after-training</link>
      <description><![CDATA[如果我们训练AI模型来识别某些类型的图像，例如为狗的示例图像。我们可以从文件夹中识别并删除此类图像吗？例如：我在狗图像上训练模型，现在我向机器人介绍一个未知文件夹，以便它可以从中删除所有狗图像。
如果是，我们如何使用python？]]></description>
      <guid>https://stackoverflow.com/questions/79511077/can-we-make-a-bot-delete-certain-images-from-the-folder-after-training</guid>
      <pubDate>Sat, 15 Mar 2025 11:52:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的梯度下降模型会产生常数（非常高）的正方形错误成本函数而不会收敛到最低？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510918/why-is-my-gradient-descent-model-giving-a-constant-very-high-square-error-cost</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79510918/why-is-my-gradient-descent-model-giving-a-constant-very-high-square-error-cost</guid>
      <pubDate>Sat, 15 Mar 2025 09:53:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何自动计算这张照片中的所有汽车？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510435/how-could-i-automatically-count-all-the-cars-in-this-picture</link>
      <description><![CDATA[我要手动进行，但是认为必须有一些机器学习模型自动进行此类事情。我想在收费大门之前计算右侧（西行）车道的所有汽车。
  https://imgur.com/a/bppac8p   ]]></description>
      <guid>https://stackoverflow.com/questions/79510435/how-could-i-automatically-count-all-the-cars-in-this-picture</guid>
      <pubDate>Sat, 15 Mar 2025 00:04:18 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用遗传算法（GA）进行飞行价格优化吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510111/can-i-use-genetic-algorithm-ga-for-flight-price-optimization</link>
      <description><![CDATA[我在Kaggle上找到了一个飞行价格预测数据集，并训练了RandomForestRegressor模型，以根据航空公司，停车，源，目的地，目的地和时间等功能来预测飞行价格。现在，我想使用遗传算法（GA）来优化并找到最便宜的飞行。
在此数据集上使用GA进行价格优化是否可行？]]></description>
      <guid>https://stackoverflow.com/questions/79510111/can-i-use-genetic-algorithm-ga-for-flight-price-optimization</guid>
      <pubDate>Fri, 14 Mar 2025 19:56:06 GMT</pubDate>
    </item>
    <item>
      <title>TPU上的RESNET50计算NAN的准确性和损失，在Google Colab中的CPU上正常工作</title>
      <link>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</link>
      <description><![CDATA[我使用Google Colab在V2-8 TPU加速器上培训了RESNET50，我已经用5000张形状（224、224、3）喂了它。我已经对它们进行了规范，没有NAN，没有INF，没有阶级失衡，一切都还好：
  input_shape =（224，224，3）

使用Strategy.scope（）：
    base_model = resnet50（weights =&#39;imagenet&#39;，include_top = false，input_shape = input_shape）
    base_model.trainable = false
    型号= tf.keras.models.sequeential（[[
        base_model，
        tf.keras.layers.globalaveragepooling2d（），
        tf.keras.layers.dense（1024，activation =&#39;relu&#39;），
        tf.keras.layers.dense（6，activation =&#39;sigmoid&#39;） 
    ）））
    model.compile（优化器=&#39;adam&#39;， 
                  损失=&#39;binary_crossentropy&#39;，
                  指标= [&#39;准确性&#39;]）
    
型号
    x_train， 
    y_train， 
    时代= 10， 
    验证_data =（x_val，y_val），
    batch_size = 32
  ）
 
当我对TPU进行训练时，训练期间的准确性和损失成为NAN。当我切换到CPU时，一切正常。
为什么会发生这种情况以及如何解决？
我尝试在Google Colab中对TPU和CPU进行培训。我希望该模型在没有任何问题的情况下进行培训，包括NAN值损失或准确性，尤其是因为培训在CPU上效果很好。但是，当使用TPU时，我遇到了NAN值的准确性和损失。我还验证了数据很干净，没有NAN，无限或失衡问题，并确保模型编译和培训设置是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</guid>
      <pubDate>Sun, 09 Mar 2025 07:07:10 GMT</pubDate>
    </item>
    <item>
      <title>Python的联合平均实施</title>
      <link>https://stackoverflow.com/questions/66472157/federated-averaging-implementation-in-python</link>
      <description><![CDATA[我正在从事联合学习。我正在使用全局服务器，在该服务器上定义了基于CNN的分类器。全局服务器用超参数编制模型，并将其发送到边缘（客户端），目前我正在使用两个客户端。每个客户端使用其本地数据（目前我正在使用相同的数据，并在每个客户端上建模）。在训练模型之后，每个客户在本地模型中的准确性，精度和回忆超过95％。客户将训练有素的本地模型发送到服务器。服务器获取模型并从每个接收到的模型中获取权重，并根据此公式。以下是我编写的代码以在Python中实现此公式。当我将平均权重设置为模型并尝试预测准确性，召回和精确度低于20％。
我在实施中做错了什么？
 ＃全局模型的初始权重，设置为ZER0。  
  ave_weights = model.get_weights（）
  ave_weights = [i * 0 for I in ave_weights]
  计数= 0
＃多线程Python服务器：TCP服务器套接字线程池
def clientthread_send（conn，地址，权重）：
    ＃将模型发送给客户
    conn.send（模型）

    打印（“型号发送到：”，地址）
    打印（“等重量”
    model_recv = conn.recv（1024）
    打印（“从：“地址”收到的权重）
    全球人数
    全局ave_weights

    
    ＃来自客户的重量
    rec_weight = model.get_weights（）
    #multip，按客户端本地数据中的本地数据示例数量地量身
    rec_weight = [i * 100000 for i in rec_Weeight]
    ＃将权重除以所有参与者的样本总数
    rec_weight = [i / 200000 for i in rec_Weeight]

    ＃所有客户的权重
    ave_weights = [x + y for x，y in zip（ave_weights，rec_aweight）]
  
    计数=计数+1
    conn.close（）
如果计数== 2：
    ＃如果计数（客户次数为两个），请设置全局模型权重
    model.set_weights（ave_weights）


 而真：
     conn，地址= s.accept（）
     start_new_thread（clientthread_send，（conn，address，ave_weights））   
     
 ]]></description>
      <guid>https://stackoverflow.com/questions/66472157/federated-averaging-implementation-in-python</guid>
      <pubDate>Thu, 04 Mar 2021 09:27:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在预测后解开数据？</title>
      <link>https://stackoverflow.com/questions/63380766/how-to-unscale-data-after-predictions</link>
      <description><![CDATA[我有一个具有2个功能的数据集（价格＆amp;卷）＆amp; 1预测变量（价格）并使用LTSM模型根据先前的价格预测下一个价格。
首先，我扩展数据集：
  #scale数据
sualer = minmaxscaler（feature_range =（0,1））
scaled_data = scaler.fit_transform（数据集）
 
最后我想解开它：
 ＃获取模型预测的价格值
预测= model.predict（x_test）
预测= sualer.inverse_transform（预测）
 
，但这不起作用，我得到了这个错误：
  valueerror：具有形状（400,1）的非广播输出操作数不匹配广播形状（400,2）
 ]]></description>
      <guid>https://stackoverflow.com/questions/63380766/how-to-unscale-data-after-predictions</guid>
      <pubDate>Wed, 12 Aug 2020 16:20:50 GMT</pubDate>
    </item>
    <item>
      <title>什么是x_train.reshape（），它有什么作用？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用MNIST数据集
 导入numpy作为NP
导入TensorFlow作为TF
来自TensorFlow.keras.datasets导入mnist

＃MNIST数据集参数
num_classes = 10＃总类（0-9位数字）
num_features = 784＃数据功能（IMG形状：28*28）

（x_train，y_train），（x_test，y_test）= mnist.load_data（）

＃转换为float32
x_train，x_test = np.array（x_train，np.float32），np.array（x_test，np.float32）

＃平坦图像到784个功能的1-D矢量（28*28）
x_train，x_test = x_train.Reshape（[ -  1，num_features]），x_test.reshape（[ -  1，num_features]）

＃将图像从[0，255]到[0，1]的标准化值
x_train，x_test = x_train /255。，x_test / 255。
 
在第15行是
  x_train，x_test = x_train.Reshape（[ -  1，num_features]），x_test.reshape（[ -  1，num_features]））。我不明白这些重塑在我们的数据集中的真正作用。请向我解释。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>用Spacy进行其他指定实体识别所需的培训数据数量是多少？</title>
      <link>https://stackoverflow.com/questions/52120487/what-is-the-amount-of-training-data-needed-for-additional-named-entity-recogniti</link>
      <description><![CDATA[我正在使用Spacy模块查找输入文本的名称实体。我正在训练该模型以预测医学术语。我目前可以访问200万张医疗笔记，我为注释笔记提供了一个程序。
 i与约90,000条的预定列表交叉参考医疗注释，该列表用于注释任务。在当前注释的速度下，大约需要一个半小时的注释才能注释10,000个纸币。注释目前的工作方式，我最终得到了大约90％的注释没有注释的笔记（我目前正在努力获取更好的交叉引用术语列表），因此我采用约1000个注释的注释并对其进行训练。&gt; 
我已经检查过，模型对已知的注释术语做出了一种响应（例如，术语 tachycardia  在注释之前已看到，有时会在文本中显示该术语时。）。）。）。
这个背景可能与我的特定问题不太相关，但我认为我会给我当前的立场提供一小部分背景。
我想知道是否有人成功地培训了一个新实体，可以使我对他们在至少具有某种可靠的实体识别所必需的培训中的个人经验有所了解。]]></description>
      <guid>https://stackoverflow.com/questions/52120487/what-is-the-amount-of-training-data-needed-for-additional-named-entity-recogniti</guid>
      <pubDate>Fri, 31 Aug 2018 17:46:41 GMT</pubDate>
    </item>
    <item>
      <title>培训和测试数据如何分配？ [关闭]</title>
      <link>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split</link>
      <description><![CDATA[我目前正在使用神经网络训练我的数据并使用拟合功能。
  history = model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，verialation_split = 0.2，verbose = 1）
 
现在，我已经使用验证_split 作为 20％。我了解的是，我的培训数据将为 80％，测试数据将为 20％。我很困惑这些数据在后端如何处理。它是否会像Top  80％的样品进行训练，并且低于20％的20％用于测试，或者样品是从介于两者之间随机选择的？如果我想提供单独的培训和测试数据，我将如何使用 model.fit（） ?? 进行操作。
此外，我的第二个问题是如何检查数据是否适合模型？从结果我可以看出，训练精度约为 90％，而验证精度约为 55％。这是否意味着过度安装或不合格的情况？
我的最后一个问题是评估回报是什么？文档表示它返回损失，但我已经在每个时期期间都会获得损失和准确性（作为fit的回报（）（在 history 中））。评估表演返回的精度和得分如何？如果通过评估返回 90％返回的准确性，我可以说我的数据很合适，无论每个时期的个人准确性和损失是什么？？
以下是我的代码：
 导入numpy
进口熊猫
导入matplotlib.pyplot作为PLT
来自keras.models导入顺序
来自keras.layers导入密集，辍学
来自keras.wrappers.scikit_learn导入kerasclassifier
来自sklearn.model_selection导入cross_val_score
从Sklearn.Preprocessing Import LabElenCoder
来自sklearn.model_selection导入stratifiedkfold
从sklearn.prepercorsing进口标准标准
来自Sklearn.Pipeline Import Pipeline
来自keras.utils导入np_utils
来自sklearn.model_selection导入kfold
来自sklearn.metrics导入混乱_matrix
导入Itertools

种子= 7
numpy.random.seed（种子）

dataframe = pandas.read_csv（&#39;inputfile.csv＆quort; skiprows = range（0，0））

dataset = dataframe.values
x = dataset [：，0：50] .astype（float）＃cols-1的数量
y =数据集[：，50]

encoder = labelencoder（）
coder.fit（y）
encoded_y = encoder.transform（y）

encoded_y = np_utils.to_categorical（encoded_y）
打印（&#39;encoded_y =＆quot; encoded_y） 
＃基线模型
def create_baseline（）：
    ＃创建模型
    型号=顺序（）
    model.Add（密集（5，input_dim = 5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    model.Add（密集（5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    ＃model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;sigmoid&#39;）））））

    model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;softmax&#39;）））））

    ＃编译模型
    model.compile（loss =&#39;binary_crossentropy&#39;，优化器=&#39;adam&#39;，metrics = [&#39;fecicy&#39;]）＃for binayr分类
        ＃model.compile（loss =&#39;cancorical_crossentropopy&#39;，importizer =&#39;adam&#39;，metrics = [&#39;cocuctiacy&#39;]）＃for Multi类
    返回模型
    

model = create_baseline（）;
历史= model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，验证_split = 0.2，冗长= 1）

print（history.history.keys（））
＃列出历史记录中的所有数据
print（history.history.keys（））
＃总结历史的准确性
plt.plot（历史学家[&#39;acc&#39;]）
plt.plot（history.history [&#39;val_acc&#39;]）
plt.title（“模型精度”）
plt.ylabel（“准确性”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）
＃总结损失的历史
plt.plot（历史学家[&#39;损失&#39;]）
plt.plot（历史学家[&#39;val_loss&#39;]）
plt.title（“模型损失”）
plt.ylabel（“损失”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）


pre_cls = model.predict_classes（x）    
cm1 = Confusion_matrix（encoder.transform（y），pre_cls）
打印（&#39;混淆矩阵：\ n&#39;）
打印（CM1）


得分，acc = model.evaluate（x，encoded_y）
打印（“测试分数：”，分数）
打印（“测试准确性：”，ACC）
 ]]></description>
      <guid>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split</guid>
      <pubDate>Sun, 24 Jun 2018 02:28:07 GMT</pubDate>
    </item>
    <item>
      <title>如何解释几乎完美的准确性和AUC-ROC但F1得分为零，精确和回忆</title>
      <link>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</link>
      <description><![CDATA[我正在培训ML Logistic分类器，使用Python Scikit-Learn对两个类进行分类。它们处于极度不平衡的数据中（大约14300：1）。我的准确性几乎为100％和ROC-AUC，但精确度为0％，召回和F1得分。我了解准确性通常在非常不平衡的数据中没有用，但是为什么ROC-AUC措施也接近完美？
 来自Sklearn.metrics导入roc_curve，auc

＃获取ROC 
y_score = classifierused2.decision_function（x_test）
false_posisitive_rate，true_posive_rate，阈值= roc_curve（y__test，y_score）
roc_auc = auc（false_posistion_rate，true_posistion_rate）
打印&#39;auc  - &#39;+&#39;=&#39;，roc_auc

1 = class1
0 = class2
班级计数：
0 199979
1 21

精度：0.99992
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99993
          1 0.00 0.00 0.00 7

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99992 1]
 [7 0]]
AUC = 0.977116255281
 
以上是使用逻辑回归，以下是使用决策树，决策矩阵看起来几乎相同，但是AUC却大不相同。
  1 = class1
0 = class2
班级计数：
0 199979
1 21
精度：0.99987
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99989
          1 0.00 0.00 0.00 11

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99987 2]
 [11 0]]
AUC = 0.4999899989
 ]]></description>
      <guid>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</guid>
      <pubDate>Sat, 09 Jan 2016 19:50:50 GMT</pubDate>
    </item>
    </channel>
</rss>