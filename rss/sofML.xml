<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 18:20:31 GMT</lastBuildDate>
    <item>
      <title>AttributeError：模块“keras.src.backend”没有属性“convert_to_numpy”</title>
      <link>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</link>
      <description><![CDATA[我尝试使用 utoencoder 和 rus 相应的代码，并在使用 tensorflow 和 keras 时遇到问题，在下面的代码中我展示了代码和相应的错误。当我拟合自动编码器模型时，它显示 AttributeError: module &#39;keras.src.backend&#39; 没有属性 &#39;convert_to_numpy&#39;。我无法理解这个错误和相应的解决方案。对于这种情况我该如何解决我的问题？我使用 anaconda3 运行此代码。我使用 tensorflow 版本 2.16.1 和 keras 版本 3.3.3，错误显示在模型、拟合线中。我尝试使用自动编码器消除噪音，在这种情况下我编写了代码。我尝试运行多次但没有成功。我在 genimi 中写入错误。它向我展示了两种方法 1. 升级 tensorflow 和 keras 2. 不要使用 backend.convert_to_numpy，而是使用推荐的方法在较新版本中将张量转换为 NumPy 数组。]]></description>
      <guid>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</guid>
      <pubDate>Tue, 18 Jun 2024 17:28:14 GMT</pubDate>
    </item>
    <item>
      <title>CNN 训练模型预测值超出训练范围</title>
      <link>https://stackoverflow.com/questions/78638666/cnn-trained-model-predicting-values-outside-of-trained-range</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78638666/cnn-trained-model-predicting-values-outside-of-trained-range</guid>
      <pubDate>Tue, 18 Jun 2024 16:37:55 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle GPU 上的训练模型问题 - 只有一个 GPU 正常工作</title>
      <link>https://stackoverflow.com/questions/78638417/issue-with-training-model-on-kaggle-gpu-only-one-gpu-working</link>
      <description><![CDATA[我目前正在尝试使用 GPU 资源在 Kaggle 上训练模型，但似乎只使用了一个 GPU，而不是多个。我使用以下训练代码：
# 步骤 1：安装所需的软件包
#!pip install ultralytics xmltodict albumentations torch torchvision torchaudio

# 步骤 5：训练 YOLO 模型
import os
import torch
from ultralytics import YOLO

# 将 WANDB_MODE 设置为“dryrun”以禁用 WanDB 日志记录
os.environ[&#39;WANDB_MODE&#39;] = &#39;dryrun&#39;

# 为多个 GPU 设置设备
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model = YOLO(&#39;yolov8x.pt&#39;) # 加载预训练的 YOLOv8 模型

# 检查是否有多个 GPU 可用
if torch.cuda.device_count() &gt; 1：
print(f&quot;使用 {torch.cuda.device_count()} GPU&quot;)
model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count()))).to(device)
else：
model = model.to(device)

# 定义训练配置
data_yaml = &quot;&quot;&quot;
train: /../images/train_combined_data
val: /../images/val
test: /../images/test
nc: 1
names: [&#39;Hotspot&#39;]
&quot;&quot;&quot;

with open(&#39;data.yaml&#39;, &#39;w&#39;) as f:
f.write(data_yaml)

# 训练模型
model.train(
data=&#39;data.yaml&#39;,
epochs=50, # 训练 epoch 总数
batch=16, 
imgsz=640, # 训练的目标图像大小
device=&#39;cuda&#39;
)


我查看了 Kaggle 的文档，它应该支持使用多个 GPU 进行训练。我需要在代码中添加一些特定内容来启用多 GPU 训练吗？或者 Kaggle 上是否有我可能遗漏的设置？
如能就此问题提供任何帮助或指导，我将不胜感激。谢谢！
我该如何使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/78638417/issue-with-training-model-on-kaggle-gpu-only-one-gpu-working</guid>
      <pubDate>Tue, 18 Jun 2024 15:40:00 GMT</pubDate>
    </item>
    <item>
      <title>两个单一模型还是一个多类模型？</title>
      <link>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</link>
      <description><![CDATA[我需要预测下一个目标事件 - 购买两个价格类别的汽车（例如，高档和中档）。训练的目标数量大致相同（假设每个价格类别有 10,000 次购买）。我在这里看到两种训练机器学习模型的方法。第一种是多类模型。第二种是两个单独的模型来预测每个细分市场。您认为应该采用哪种方法，为什么？我想以多类模型为基础，您能列出哪些优点和缺点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</guid>
      <pubDate>Tue, 18 Jun 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有特定文档的情况下估计在大型语言模型上运行推理的硬件要求？[关闭]</title>
      <link>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</link>
      <description><![CDATA[我有兴趣在我的计算机上本地运行一个大型语言模型进行推理。我想选择一个可以在线免费访问的模型，但具体的 CPU/RAM 或 GPU/VRAM 要求通常不会在他们的 Hugging Face 或 Azure 页面上提供，而且我在网上其他地方也找不到它们。
当除了上下文长度和参数数量之外没有提供任何要求信息时，我该如何估计给定模型的硬件要求？
例如：我想在我的计算机上运行最大可能的 Phi-3 模型版本，但我在模型页面或网络上找不到任何要求信息。最大的版本之一是这个，上下文长度为 128_000，参数为 14B，我该如何推断硬件要求？]]></description>
      <guid>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</guid>
      <pubDate>Tue, 18 Jun 2024 13:56:25 GMT</pubDate>
    </item>
    <item>
      <title>pmdarima 中的 auto_arima 的执行时间随着 m 的增加而疯狂增加，原因是什么？</title>
      <link>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</link>
      <description><![CDATA[我正在尝试为我的时间序列拟合 SARIMA 模型，为了找到最佳模型，我正在使用 pmdarima 的 autom_arima。代码很简单：
stepwise_fit = pm.auto_arima(train_dataset[&#39;shift_hours&#39;], start_p=1, start_q=1,
max_p=3, max_q=3, m=365,
start_P=0, seasonal=True,
d=1, D=1, trace=True,
error_action=&#39;ignore&#39;, 
suppress_warnings=True, 
stepwise=True) 

其中 &#39;train_dataset[&#39;shift_hours&#39;]&#39; 由大约两年的每日值组成，这些值具有 365 天的季节性。
我遇到的问题与执行时间有关：当 m 设置为 1 时，在逐步搜索中执行一步需要不到一秒的时间，当它为 7 或 12 时，一步的执行时间在 1 到 2 秒之间，但是当我将 m 设置为 365（我需要的值），执行单个步骤需要 30 多分钟。如果我使用 girdsearch 并将 n_jobs &gt; 4，则会出现内存错误。
就我对 SARIMA 的了解而言，当我使用更大的周期时，它不应该花费更多时间来拟合一个系列，因为它用于对系列进行建模的术语数量不依赖于 m，所以我想知道那里发生了什么，以及是否发生了什么我无法理解的事情]]></description>
      <guid>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</guid>
      <pubDate>Tue, 18 Jun 2024 10:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何准确检测不同音轨中主节拍和配乐的开始？</title>
      <link>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</link>
      <description><![CDATA[我正在做一个需要编辑配乐的项目。挑战在于检测任何给定配乐的主要节拍和旋律何时得到正确发展。我确信有更好的术语来描述我的目标，但理想情况下，我想跳过“构建”并立即让歌曲从“主要部分”开始。这需要适用于不同类型的各种歌曲，这些歌曲通常具有不同的结构和开始模式，这使得简化流程变得困难。
例如：
https://www.youtube.com/watch?v=P77CNtHrnmI -&gt;我希望我的代码能够识别 0:24 处的开始
https://www.youtube.com/watch?v=OOsPCR8SyRo -&gt; 0:12 处的开始检测
https://www.youtube.com/watch?v=XKiZBlelIzc -&gt; 0:19 处的起始检测
我尝试使用 librosa 分析起始强度并检测节拍，但当前的实现要么检测到歌曲的最开始，要么无法一致地识别节拍何时完全形成。
这是我的方法；
def analyze_and_edit_audio(input_file, output_file):
y, sr = librosa.load(input_file)
tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
beat_times = librosa.frames_to_time(beat_frames, sr=sr)
main_beat_start = beat_times[0]

我对 librosa/audio 编辑经验很少，因此如果您有任何建议，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</guid>
      <pubDate>Tue, 18 Jun 2024 10:35:15 GMT</pubDate>
    </item>
    <item>
      <title>Android 中的 Movenet Singlepose 照明模型：“不支持的图像格式：1”错误</title>
      <link>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</guid>
      <pubDate>Tue, 18 Jun 2024 09:42:32 GMT</pubDate>
    </item>
    <item>
      <title>我如何重新训练 Xgboost 回归器？</title>
      <link>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</link>
      <description><![CDATA[我现在正在做的实习要求模型“定期更新新数据”。如何在 Xgboost 上做到这一点？我已将模型保存为 pickle 文件
在项目中，我将从后端获取新数据，并且我应该定期重新训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</guid>
      <pubDate>Tue, 18 Jun 2024 08:51:09 GMT</pubDate>
    </item>
    <item>
      <title>fragment_anything_fast SamAutomaticMaskGenerator 抛出 BackendCompilerFailed-Error</title>
      <link>https://stackoverflow.com/questions/78636270/segment-anything-fast-samautomaticmaskgenerator-throwing-a-backendcompilerfailed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636270/segment-anything-fast-samautomaticmaskgenerator-throwing-a-backendcompilerfailed</guid>
      <pubDate>Tue, 18 Jun 2024 08:29:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 PaddleOCR 进行账单 OCR</title>
      <link>https://stackoverflow.com/questions/78636112/ocr-bill-using-paddleocr</link>
      <description><![CDATA[目前在学习OCR，遇到一个发票问题，需要用OCR从PDF文件中提取表格，但是用PaddleOCR时，有很多文本部分有误或缺失，表格结构和原文件不一样，请给点思路，谢谢
目前在学习PaddleOCR时，遇到很多文本部分有误或缺失，表格结构和原文件不一样，请给点思路和解决方案，谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78636112/ocr-bill-using-paddleocr</guid>
      <pubDate>Tue, 18 Jun 2024 07:48:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有非奇异输出的鉴别器（针对修改后的 WGAN 架构）可能比传统鉴别器表现更好？</title>
      <link>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</link>
      <description><![CDATA[以下是我提出的（修改后的）WGAN 模型的生成器和鉴别器模型架构，用于将降雨数据下采样 4 倍（仅作为测试）。数据集的输入形状是 (8030, 14, 21)，我想要的输出形状是 (8030, 28, 42)。
def build_generator(input_shape):
inputs = Input(shape=input_shape, name=&#39;generator_input&#39;)

# 下采样
downsample_3 = conv_block(inputs, 128, (3, 3), &#39;downsample_3&#39;)
downsample_2 = conv_block(downsample_3, 64, (3, 3), &#39;downsample_2&#39;)
downsample_1 = conv_block(downsample_2, 32, (3, 3), &#39;downsample_1&#39;)

# 瓶颈
bottleneck_0 = conv_block(downsample_1, 16, (3, 3), &#39;bottleneck_0&#39;)
bottleneck_00 = conv_block(bottleneck_0, 16, (3, 3), &#39;bottleneck_00&#39;)

# 上采样
upsample_1 = deconv_block(bottleneck_00, 32, (3, 3), &#39;upsample_1&#39;)
upsample_2 = deconv_block(upsample_1, 64, (3, 3), &#39;upsample_2&#39;)
upsample_3 = deconv_block(upsample_2, 128, (3, 3), &#39;upsample_3&#39;)

# 最终上采样至所需形状
output = Conv2DTranspose(filters=1, kernel_size=(3, 3), kernel_initializer=he_normal(), padding=&#39;same&#39;, 
strides=(2, 2), activated=&#39;relu&#39;, name=&#39;final_upsample_conv&#39;)(upsample_3)

model =模型（输入=输入，输出=输出，名称=&#39;generator&#39;）

返回模型

def build_discriminator（输入形状）：
输入层 = 输入（形状=输入形状，名称=&#39;discriminator_input&#39;）

x = conv_block（输入层，过滤器=16，内核大小=（3，3），名称=&#39;conv1&#39;）#，使用批处理规范=False）
x = conv_block（x，过滤器=32，内核大小=（3，3），名称=&#39;conv2&#39;）#使用批处理规范=False）
x = conv_block（x，过滤器=128，内核大小=（3，3），名称=&#39;conv3&#39;）#使用批处理规范=False）

x = MaxPooling2D（）（x）
x = Dropout（0.25）（x）
输出层 = Dense（1，激活=&#39;线性&#39;）（x）

模型= Model(inputs=input_layer, output=output_layer, name=&#39;discriminator&#39;)

返回模型

根据文献中的 WGAN 实现，我使用 RMSprop 优化器代替 ADAM，学习率为 5e-5，动量为 0.5。此外，我将鉴别器权重剪裁为 -1e-2 和 1e-2 之间。传统上（据我所知），鉴别器模型输出单个值损失，表示它是否能够区分真实输出和虚假输出。在我的情况下，鉴别器输出的形状为 (None, 14, 21, 1)，它似乎效果更好，但我不明白为什么。有人知道为什么会发生这种情况吗？提前致谢！
编辑：这些是 conv_block 和 deconv_block 的架构
def conv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2D(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_conv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
x = Dropout(0.25, name=name+&#39;_dropout&#39;)(x)
return x

def deconv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_deconv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
return x

编辑 2：以下是损失函数
def generator_loss(fake_output, real_output, penalty_weight=10):
&quot;&quot;&quot;
使用 Wasserstein 损失并添加惩罚项的生成器损失函数。

参数：
fake_output (tf.Tensor)：给定生成的图像时，判别器的输出。
real_output (tf.Tensor)：给定真实图像时，判别器的输出。
penalty_weight (float)：惩罚项的权重。

返回：
tf.Tensor：生成器损失。
“” “”
wasserstein_loss = -tf.reduce_mean(fake_output)

# 偏离实际输出的惩罚项
penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_output - real_output))

return wasserstein_loss + penalty

def discriminator_loss(real_output, fake_output):
“” “”
使用 Wasserstein 损失的判别器损失函数。

参数：
real_output (tf.Tensor)：给定真实图像时判别器的输出。
fake_output (tf.Tensor)：给定生成图像时判别器的输出。

返回：
tf.Tensor：判别器损失。
“” “”
return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)
]]></description>
      <guid>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</guid>
      <pubDate>Mon, 17 Jun 2024 09:29:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>让一个非常简单的 stablebaselines3 示例发挥作用</title>
      <link>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</link>
      <description><![CDATA[我尝试模拟最简单的硬币翻转游戏，你必须预测它是否会是正面。遗憾的是它无法运行，给我：
使用 cpu 设备
回溯（最近一次调用最后一次）：
文件“/home/user/python/simplegame.py”，第 40 行，在&lt;module&gt;
model.learn(total_timesteps=10000)
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py&quot;，第 264 行，在 learn 中
total_timesteps, callback = self._setup_learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/base_class.py&quot;，第 423 行，在 _setup_learn 中
self._last_obs = self.env.reset() # 类型： ignore[assignment]
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py&quot;，第 77 行，在 reset
obs，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
TypeError: CoinFlipEnv.reset() 获得了意外的关键字参数 &#39;seed&#39;

代码如下：
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class CoinFlipEnv(gym.Env):
def __init__(self, heads_probability=0.8):
super(CoinFlipEnv, self).__init__()
self.action_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.observation_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.heads_probability = heads_probability
self.flip_result = None

def reset(self):
# 重置环境
self.flip_result = None
return self._get_observation()

def step(self, action):
# 执行操作（0 表示正面，1 表示反面）
self.flip_result = int(np.random.rand() &lt; self.heads_probability)

# 计算奖励（1 表示正确预测，-1 表示错误）
reward = 1 if self.flip_result == action else -1

# 返回观察、奖励、完成和信息
return self._get_observation(), reward, True, {}

def _get_observation(self):
# 返回当前硬币翻转结果
return self.flip_result

# 创建正面概率为 0.8 的环境
env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)])

# 创建 PPO 模型
model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)

# 训练模型
model.learn(total_timesteps=10000)

# 保存模型
model.save(&quot;coin_flip_model&quot;)

# 评估模型
obs = env.reset()
for _ in range(10):
action, _states = model.predict(obs)
obs, rewards, dones, info = env.step(action)
print(f&quot;Action: {action}, Observation: {obs}, Reward: {rewards}&quot;)

我做错了什么？
这是 2.2.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</guid>
      <pubDate>Fri, 05 Jan 2024 16:47:55 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 中的平衡准确度分数</title>
      <link>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</link>
      <description><![CDATA[我正在为一个高度不平衡的分类问题实现一个 CNN，我想在 TensorFlow 中实现自定义指标以使用“选择最佳模型”回调。
具体来说，我想实现平衡准确度分数，即每个类的召回率的平均值（请参阅 sklearn 实现此处），有人知道怎么做吗？]]></description>
      <guid>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</guid>
      <pubDate>Sat, 14 Dec 2019 21:59:49 GMT</pubDate>
    </item>
    </channel>
</rss>