<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Mar 2024 18:16:40 GMT</lastBuildDate>
    <item>
      <title>机器学习通过 SVC 症状预测疾病</title>
      <link>https://stackoverflow.com/questions/78123509/machine-learning-prediction-of-disease-by-symptoms-with-svc</link>
      <description><![CDATA[我正在尝试创建一种通过症状来预防疾病的功能，但我有什么想法吗？
它会像：
症状 = [“skin_rash”,continous_sneezing”,“itching”]
pred（症状，模型）

这是我当前的代码：
将 pandas 导入为 pd
从 sklearn.preprocessing 导入 LabelEncoder、StandardScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入准确度_分数、精度_分数、召回_分数、f1_分数

def load_and_preprocess_data(文件路径):
    df = pd.read_csv(文件路径)

    # 识别非数字列（不包括“疾病”）
    non_numeric_cols = df.select_dtypes(include=[&#39;object&#39;]).columns.difference([&#39;Disease&#39;])
    print(&quot;非数字列（不包括&#39;疾病&#39;）：&quot;, non_numeric_cols)

    df.fillna(0,就地=True)

    如果 df.columns 中有“疾病”：
        le = 标签编码器()
        df[&#39;疾病&#39;] = le.fit_transform(df[&#39;疾病&#39;])

    X = df.drop(&#39;疾病&#39;, axis=1)
    y = df[&#39;疾病&#39;]

    # 特征缩放（如果需要，考虑替代缩放方法）
    定标器=标准定标器()
    X = pd.DataFrame(scaler.fit_transform(X))

    返回 X, y

def build_and_train_model(X_train, y_train):
    # 使用不同的内核进行实验（线性、rbf 等）
    model = SVC(kernel=&#39;linear&#39;) # 替换为所选内核

    # 训练模型
    model.fit(X_train, y_train)

    返回模型

def Predict_and_evaluate(模型, X_test, y_test):
    y_pred = model.predict(X_test)
    准确度=准确度_得分(y_test, y_pred)
    精度 = precision_score(y_test, y_pred, 平均值=&#39;加权&#39;)
    召回率=召回率（y_test，y_pred，平均值=&#39;加权&#39;）
    f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)

    print(“准确度：”, 准确度)
    print(&quot;精度：&quot;, 精度)
    print(“回忆：”, 回忆)
    print(&quot;F1-分数：&quot;, f1)


def main():
    文件路径 = &#39;数据集/merged_dataset.csv&#39;

    X, y = load_and_preprocess_data(文件路径)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    模型 = build_and_train_model(X_train, y_train)

    预测和评估（模型，X_测试，y_测试）


如果 __name__ == “__main__”：
    主要的（）

这就是 csv 的样子：
疾病、瘙痒、皮疹、结节性皮肤疹、连续打喷嚏
真菌感染,1,3,4,0,0
真菌感染,1,3,0,0,6
过敏,1,3,0,0,0
... 和更多

是症状的严重程度，如 1,3,4,0,0（0 表示对该疾病不重要）
所以我必须用我训练的模型通过症状列表来猜测疾病我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78123509/machine-learning-prediction-of-disease-by-symptoms-with-svc</guid>
      <pubDate>Thu, 07 Mar 2024 18:09:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 进行元学习 (MAML) 训练以简化描述期间 GPU 内存超出 [关闭]</title>
      <link>https://stackoverflow.com/questions/78122917/gpu-memory-exceeded-during-meta-learning-maml-training-with-tensorflow-for-str</link>
      <description><![CDATA[我正在开发一个元学习项目，用于描绘卫星图像和 DEM 衍生产品的流线。我的 TensorFlow 模型使用 MAML 过程，但我始终超出 GPU 内存限制。
关键细节：

任务：简化地理空间数据的描绘。

框架：TensorFlow

元学习算法：模型无关元学习 (MAML)

网络：U-Net架构

当前可行参数： ~700K

所需参数：70+ 百万

梯度计算： tf.GradientTape


问题：
模型训练过程始终耗尽 GPU 内存。我想扩展模型以处理更多的参数。
示例代码：
类 MAML：
    def __init__(自身、模型、优化器、inner_step_size、num_inner_steps):
        self.model = 模型
        self.optimizer = 优化器
        self.inner_step_size = inner_step_size
        self.num_inner_steps = num_inner_steps

        # 初始化 TensorBoard 日志记录的摘要编写器
        self.summary_writer = tf.summary.create_file_writer(&#39;日志/&#39;)

    def inner_update(自身、support_data、support_labels、original_weights):

        使用 tf.GradientTape() 作为 task_tape：
            task_preds = self.model(support_data)
            plt.imshow(task_preds[0])
            plt.show()
            # task_loss = f1_loss(support_labels, task_preds)
            task_loss = tf.keras.losses.binary_crossentropy(support_labels, task_preds)


        task_gradients = task_tape.gradient(task_loss, self.model.trainable_variables)
        fast_weights = [w - self.inner_step_size * g for w, g in zip(original_weights, task_gradients)]

        返回 fast_weights, task_loss.numpy()

    defouter_update（自我，剧集）：
        原始权重 = self.model.get_weights()

        # 内部更新
        fast_weights,task_loss = self.inner_update(episode[&#39;support_set_data&#39;],episode[&#39;support_set_labels&#39;],original_weights)

        # 临时设置模型权重以进行验证
        self.model.set_weights(fast_weights)

        使用 tf.GradientTape() 作为 meta_tape：
            meta_val_preds = self.model(episode[&#39;query_set_data&#39;])
            meta_val_loss = f1_loss(episode[&#39;query_set_labels&#39;], meta_val_preds)
        meta_gradients = meta_tape.gradient(meta_val_loss, self.model.trainable_variables)

        # 恢复原始权重并应用外部更新
        self.model.set_weights(original_weights)
        self.optimizer.apply_gradients(zip(meta_gradients, self.model.trainable_variables))
        返回task_loss，meta_val_loss.numpy()

    def fit(自我、剧集、纪元)：
        对于范围内的纪元（纪元）：
            任务损失 = []
            val_losses = []
            对于剧集中的剧集：
                task_loss, val_loss = self.outer_update(episode)
                # 打印（任务损失，val_loss）
                task_losses.append(task_loss)
                val_losses.append(val_loss)

            # 记录该时期的平均任务损失和验证损失
            使用 self.summary_writer.as_default()：
                tf.summary.scalar(&#39;任务损失&#39;, np.mean(task_losses), step=epoch)
                tf.summary.scalar(&#39;验证损失&#39;, np.mean(val_losses), step=epoch)

            # 打印该纪元的平均损失
            print(f&quot;Epoch {epoch + 1}/{epochs} - 任务损失：{np.mean(task_losses):.4f}，验证损失：{np.mean(val_losses):.4f}&quot;)

硬件：
Google Colab：高 RAM (51.0 GB)、A100 (16.0 GB) 节点。
其他问题：

是否有专门针对 TensorFlow 中元学习的已知内存优化策略？

使用tf.GradientTape进行梯度计算会成为瓶颈吗？如果是这样，什么替代方案可能更节省内存？

是否有有效的技术可以在不牺牲该领域性能的情况下降低模型复杂性？

内存高效的分布式训练方法有帮助吗？

图层冻结：我尝试通过冻结较大模型中的某些图层来减少内存消耗。然而，这并没有解决 GPU 内存限制。具体来说，我尝试在训练过程中冻结Unet模型的编码器。

模型缩减：由于内存问题仍然存在，我改用了一个明显更小的模型。虽然这允许我运行训练过程，但模型现在无法收敛。这表明较小的模型可能缺乏学习简化描述所需模式的能力。

]]></description>
      <guid>https://stackoverflow.com/questions/78122917/gpu-memory-exceeded-during-meta-learning-maml-training-with-tensorflow-for-str</guid>
      <pubDate>Thu, 07 Mar 2024 16:24:53 GMT</pubDate>
    </item>
    <item>
      <title>寻求时间有序变量的决策树算法</title>
      <link>https://stackoverflow.com/questions/78122700/seeking-a-decision-tree-algorithm-for-temporally-ordered-variables</link>
      <description><![CDATA[我正在寻找一种方法（在 R 或 Python 中）来构建考虑变量时间顺序的决策树。目标是迫使决策树反映变量的顺序性质。一组变量是时间 1 中发生的事件的指示符，第二组变量与时间 2 相关，依此类推。本质上，我的目标是引导算法首先利用与较早时间点相关的变量，然后按顺序利用与较晚时间点相关的变量。该方法促进了逐步的分类过程，其中每个步骤都根据先前步骤的信息细化类别。
也就是说，算法首先用与第一时间点相关的变量进行分类，然后再对第二时间点进行进一步分类和“清理”。生成的子样本包含与第二个时间点相关的变量，依此类推。以某种方式强制算法首先使用与第一轮相关的变量，然后使用与第二轮相关的变量，依此类推。
它可以使用例如 R 中的插入符包手动完成，但我确信有一个更优雅、自动化的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78122700/seeking-a-decision-tree-algorithm-for-temporally-ordered-variables</guid>
      <pubDate>Thu, 07 Mar 2024 15:51:04 GMT</pubDate>
    </item>
    <item>
      <title>尝试从 Mistral7B 微调的 LLM 生成摘要时出现意外的关键字参数“use_dora”</title>
      <link>https://stackoverflow.com/questions/78122541/unexpected-keyword-argument-use-dora-when-attempting-to-generate-summary-from</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78122541/unexpected-keyword-argument-use-dora-when-attempting-to-generate-summary-from</guid>
      <pubDate>Thu, 07 Mar 2024 15:29:37 GMT</pubDate>
    </item>
    <item>
      <title>有哪些工具可用于手动制作包含分段图像和视频的数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78122474/what-tools-are-available-to-manually-make-a-dataset-containing-segmented-images</link>
      <description><![CDATA[这些工具应该允许我标记对象的任意轮廓以及所选对象之间的关系。
这些工具应该允许我在单个图像和图像序列（视频）中执行此操作。
所以我需要手动分割的工具，然后生成用于机器学习的数据集。
此类数据集存在哪些工具和文件格式？]]></description>
      <guid>https://stackoverflow.com/questions/78122474/what-tools-are-available-to-manually-make-a-dataset-containing-segmented-images</guid>
      <pubDate>Thu, 07 Mar 2024 15:19:43 GMT</pubDate>
    </item>
    <item>
      <title>训练时验证损失没有变化</title>
      <link>https://stackoverflow.com/questions/78122336/validation-loss-not-varying-while-training</link>
      <description><![CDATA[我正在训练一个简单的神经网络，用于对具有约 2k 个查询的数据集进行查询评级，并且它们的评级以 CSV 的形式给出。我观察到，训练时验证损失没有变化，而且它为所有查询生成相同的输出（4. 最多 6 个小数位的评级相同）。为什么会发生这种情况？
我尝试更改学习率、层数、训练和测试数据的随机分割以及不同的超参数，但仍然没有变化。我什至尝试使用正则化，它没有改变任何东西。
导入重新
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.metrics 导入mean_absolute_error
将张量流导入为 tf
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集、嵌入、扁平化
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.preprocessing.sequence导入pad_sequences
从 nltk.corpus 导入停用词
从 nltk 导入 pos_tag
从 nltk.tokenize 导入 word_tokenize

REPLACE_BY_SPACE_RE = re.compile(&#39;[/(){}\[\]\|@,;]&#39;)
BAD_SYMBOLS_RE = re.compile(&#39;[^0-9a-z #+_]&#39;)
STOPWORDS = set(stopwords.words(&#39;english&#39;))

train_data = pd.read_csv(“train.csv”)

X = train_data[&#39;查询&#39;]
y = train_data[&#39;分数&#39;]

def clean_text(文本):
    文本 = re.sub(REPLACE_BY_SPACE_RE, &#39; &#39;, 文本)
    文本 = re.sub(BAD_SYMBOLS_RE, &#39;&#39;, 文本)
    单词 = text.lower().split()
    clean_text = &#39; &#39;.join(words)
    返回已清理的文本

X = X.应用(clean_text)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

分词器 = 分词器()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)

X_train_padded = pad_sequences(X_train_seq)
X_val_padded = pad_sequences(X_val_seq, maxlen=X_train_padded.shape[1])

def pos_tagging（文本）：
    标记 = word_tokenize(文本)
    pos_tags = pos_tag(令牌)
    返回 pos_tags

X_train_pos = X_train.apply(pos_tagging)
X_val_pos = X_val.apply(pos_tagging)

常数因子 = 2.0

对于 i，枚举中的 pos_tags(X_train_pos)：
    对于单词，pos_tags 中的 pos：
        如果 pos == &#39;WP&#39; 或 pos == &#39;WRB&#39;:
            word_index = tokenizer.word_index.get(word.lower(), 0)
            如果单词索引！= 0：
                X_train_padded[i][X_train_padded[i] == word_index] *= Constant_factor

对于 i，枚举中的 pos_tags(X_val_pos)：
    对于单词，pos_tags 中的 pos：
        如果 pos == &#39;WP&#39; 或 pos == &#39;WRB&#39;:
            word_index = tokenizer.word_index.get(word.lower(), 0)
            如果单词索引！= 0：
                X_val_padded[i][X_val_padded[i] == word_index] *= Constant_factor

定标器=标准定标器()
y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1)).flatten()

模型=顺序（[
    嵌入（input_dim = len（tokenizer.word_index）+ 1，output_dim = 50，input_length = X_train_padded.shape [1]），
    展平（），
    密集（64，激活=&#39;relu&#39;），
    密集（1，激活=&#39;线性&#39;）
]）

model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_absolute_error&#39;)
model.fit(X_train_padded，y_train_scaled，epochs=10，batch_size=32，validation_data=(X_val_padded，y_val_scaled))

Predictions_scaled = model.predict(X_val_padded).flatten()
预测=scaler.inverse_transform(predictions_scaled)
mae = Mean_absolute_error(y_val, 预测)

print(f“验证集的平均绝对误差：{mae}”)。

这是我的代码，我认为给予更多权重“什么、谁、何时” wh- 词可能会帮助我找到更好的查询。有人可以告诉我该怎么做才能改变验证损失吗？即使损失很高，如何确保输出针对不同的输入而变化？]]></description>
      <guid>https://stackoverflow.com/questions/78122336/validation-loss-not-varying-while-training</guid>
      <pubDate>Thu, 07 Mar 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Val 损失没有显示？如何显示它然后用训练损失绘制它</title>
      <link>https://stackoverflow.com/questions/78122308/why-val-loss-is-not-showing-how-to-display-it-then-plot-it-with-training-loss</link>
      <description><![CDATA[在数据集上微调 pytorch llm(IDEFICS9b) 后，训练结果未显示 val 损失，如何收集它，然后用训练损失绘制它？
training_args = TrainingArguments(
    output_dir=f“{model_name}-vqa1”，
    学习率=3e-5，
    fp16=正确，
    per_device_train_batch_size=16，
    per_device_eval_batch_size=16，
    梯度累积步数=1，
    dataloader_pin_memory=False,
    save_total_limit=3,
    评价_策略=“步骤”，
    save_strategy=&quot;步骤&quot;,
    保存步骤=10，
    评估步骤=10，
    记录步骤=10，
    最大步数=1000，
    删除_未使用的列=假，
    Push_to_hub=假,
    label_names=[“标签”],
    load_best_model_at_end=真，
    report_to=无，
    optim=&quot;paged_adamw_8bit&quot;,
）
教练=教练（
    型号=型号，
    参数=训练参数，
    train_dataset=train_ds,
    eval_dataset=eval_ds,
）

train_results = trainer.train()

训练结果如下：
&lt;块引用&gt;
TrainOutput(global_step=10,training_loss=2.2117252349853516,metrics={&#39;train_runtime&#39;: 15.7995, &#39;train_samples_per_second&#39;: 10.127, &#39;train_steps_per_second&#39;: 0.633, &#39;total_flos&#39;: 470670709819392.0, &#39;train_loss&#39;：2.2117252349853516，&#39;纪元&#39;： 0.02})
]]></description>
      <guid>https://stackoverflow.com/questions/78122308/why-val-loss-is-not-showing-how-to-display-it-then-plot-it-with-training-loss</guid>
      <pubDate>Thu, 07 Mar 2024 14:56:29 GMT</pubDate>
    </item>
    <item>
      <title>分析信用卡欺诈检测模型中的特征重要性[关闭]</title>
      <link>https://stackoverflow.com/questions/78121363/analyzing-feature-importance-in-credit-card-fraud-detection-models</link>
      <description><![CDATA[我最近完成了一个信用卡欺诈检测项目，非常感谢社区的反馈。
数据集概述：
该项目使用的数据集包含 3075 个条目和 12 列，包括“Merchant_id”、“Transaction_amount”和“isFradulent”等。删除了填充缺失值的“TransactionDate”列。该数据集混合了数值和分类数据，且“isFradulent”类别分布不平衡。
使用的算法：
使用了 3 种机器学习算法：

支持向量机(SVM)
K 最近邻（KNN）
决策树

为了评估我使用的各个功能的重要性：

相关性分析
模型中特征的重要性（SVM、KNN、决策树）

为了有效传达功能重要性，使用了以下策略：

混淆矩阵
学习曲线
GridSearchCV 结果
还重点关注准确率、召回率和 F1 分数的清晰解释，以帮助理解。

-可以应用哪些附加技术或工具来增强欺诈检测场景中的特征重要性评估？
- 在解释特征重要性结果时是否有任何常被忽视的陷阱或注意事项？
-就所选算法而言，您是否认为有潜在的改进或突出功能重要性的替代方法？
我专门寻求见解来完善我的项目的功能重要性分析方面。关于上述问题的任何反馈、建议或建议都非常有价值。]]></description>
      <guid>https://stackoverflow.com/questions/78121363/analyzing-feature-importance-in-credit-card-fraud-detection-models</guid>
      <pubDate>Thu, 07 Mar 2024 12:26:26 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？我明白
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- --------------------------
---&gt; 13 clf.fit(X_train, y_train, **fit_params)

TypeError：OneVsRestClassifier.fit() 得到意外的关键字参数“eval_set”
]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>使用目标列识别机器学习问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</link>
      <description><![CDATA[我正在努力使用给定的输入数据和选择的目标列自动定义预测类型。
如何区分目标列是多类分类还是二类分类或回归？
我尝试阅读来自 DSML 平台的各种实现的博客，但需要一些清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:26 GMT</pubDate>
    </item>
    <item>
      <title>加载图像边界框输出相同大小的错误</title>
      <link>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</link>
      <description><![CDATA[我正在尝试为我的数据集创建一个 PyTorch 数据加载器。每个图像都有一定数量的汽车和每个汽车的边界框，但并非所有图像都具有相同数量的边界框。
您可能无法运行它，但这里有一些信息。
这是我的数据加载器
类 AGR_Dataset（数据集）：
    def __init__(self,annotations_root,img_root,transform=None):
        ”“”
        论据：
            comments_root（字符串）：带有注释的 csv 文件的路径。
            img_root（字符串）：包含所有图像的目录。
            变换（可调用，可选）：要应用的可选变换
                在样品上。
        ”“”
        self.annotations_root = 注释_root
        self.img_root = img_root
        self.transform = 变换

    def __len__(自身):
        返回 len(self.annotations_root)
    
    def __getitem__(self, idx):
        # idx 可能是索引或图像名称，我认为图像 naem
        如果 torch.is_tensor(idx):
            idx = idx.tolist()
        
        idx_name = os.listdir(self.img_root)[idx]
        # 打印(idx_name)
        
        img_name = os.path.join(self.img_root, idx_name)
        Comments_data = os.path.join(self.annotations_root, f&quot;{idx_name.removesuffix(&#39;.jpg&#39;)}.txt&quot;)
        # print(img_name, 注释数据)

        图像 = io.imread(img_name)

        以 open(annotation_data, &#39;r&#39;) 作为文件：
            行= file.readlines()
            图像数据 = []
            img_标签 = []
            对于行中行：
                line = line.split(&#39;,&#39;)
                line = [i.strip() for i in line]
                line = [float(num) for num in line[0].split()]
                img_labels.append(int(行[0]))
                img_data.append(行[1:])

        框 = tv_tensors.BoundingBoxes(img_data, format=&#39;CXCYWH&#39;, canvas_size=(image.shape[0], image.shape[1]))

        # 样本 = {&#39;image&#39;: 图像, &#39;bbox&#39;: 盒子, &#39;labels&#39;: img_labels}
        样本= {&#39;image&#39;：图像，&#39;bbox&#39;：盒子}

        如果自我变换：
            样本 = self.transform(样本)

        打印（样本[&#39;图像&#39;].shape）
        打印（样本[&#39;bbox&#39;].shape）
        # print(样本[&#39;标签&#39;].shape)
        返回样品

我运行转换并创建数据加载器
data_transform = v2.Compose([
    v2.ToImage(),
    # v2.调整大小(680),
    v2.RandomResizedCrop(大小=(680, 680), 抗锯齿=True),
    # v2.ToDtype(torch.float32,scale=True),
    v2.ToTensor()
]）

Transformed_dataset = AGR_Dataset(f&#39;{annotations_path}/test/&#39;,
                        f&#39;{img_path}/测试/&#39;,
                        变换=数据变换）

数据加载器=数据加载器(transformed_dataset,batch_size=2,
                        洗牌=假，num_workers=0）

然后我尝试用它迭代它，并最终使用边界框查看和图像。
对于 i，枚举（dataloader）中的示例：
    打印（我，样本）
    print(i, 样本[&#39;image&#39;].size(), 样本[&#39;bbox&#39;].size())

    如果我==4：
        休息

批处理大小为 1 时，它运行正常，批处理大小为 2 时，出现此错误
torch.Size([3, 680, 680])
火炬.Size([12, 4])

火炬.Size([3, 680, 680])
火炬.Size([259, 4])

RuntimeError: 堆栈期望每个张量大小相等，但在条目 0 处得到 [12, 4]，在条目 1 处得到 [259, 4]


我认为这是由于边界框数量不相等造成的，但如何克服这个问题？
我的变换中需要 ToTensor 吗？我开始认为我不这样做，因为 v2 使用 ToImage()，而 ToTensor 正在被贬值。

如有任何其他意见或帮助，我们将不胜感激。
我不确定如何创建一个工作示例，我会继续尝试。
我尝试过的
我尝试通过注释数据加载器中的 tv_tensors.BoundingBoxes 行来不将边界框加载为张量，但由于某种原因，我的调整大小无法正常工作。
我刚刚尝试在数据加载器中像这样分割框和图像
样本 = 图像
    目标= {&#39;bbox&#39;：盒子，&#39;标签&#39;：img_labels}

运气不好]]></description>
      <guid>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</guid>
      <pubDate>Thu, 07 Mar 2024 01:30:38 GMT</pubDate>
    </item>
    <item>
      <title>使用LSTM模型进行充电数据负荷预测的时间序列交叉验证</title>
      <link>https://stackoverflow.com/questions/78107902/time-series-cross-validation-for-load-forecast-of-charging-data-using-lstm-model</link>
      <description><![CDATA[我目前正在构建一个 LSTM 模型，以使用包含 24 个特征和 12 个月目标列的数据集来预测电动汽车的消耗量。数据采用时间序列格式，间隔为 15 分钟，我想应用时间序列交叉验证来维护时间依赖性。
这是我到目前为止为训练和测试拆分编写的代码：
# 定义用于训练的列
特征 = [&#39;小时&#39;, &#39;季度小时&#39;, &#39;工作日&#39;, &#39;cal_week&#39;, &#39;月份&#39;, &#39;周末&#39;,
                  &#39;季节&#39;，&#39;假期&#39;，&#39;下一个假期&#39;，&#39;bridge_day&#39;，&#39;学校假期&#39;，&#39;quarter_sin&#39;，&#39;quarter_cos&#39;，&#39;hour_sin&#39;，&#39;hour_cos&#39;，
                  &#39;weekday_sin&#39;、&#39;weekday_cos&#39;、&#39;month_sin&#39;、&#39;month_cos&#39;、&#39;cal_week_sin&#39;、&#39;cal_week_cos&#39;、&#39;temp_day_avg&#39;、&#39;humidity_day_avg&#39;、&#39;wind_speed&#39;]

# 提取特征和目标变量
X = Bosch_data[功能].值
y = Bosch_data[&#39;aggregate_conspiration_kWh&#39;].values


# 定义窗口大小
validation_window_size = 7 * 24 * 4 # 7天* 24小时* 4个季度/小时（15T）
test_window_size = 31 * 24 * 4 # 31天* 24小时* 4个季度/小时（15T）

# 将训练集定义为除验证集和测试集之外的所有内容
train_window_size = len(Bosch_data) - validation_window_size - test_window_size

# 将数据分为训练集、验证集和测试集
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=validation_window_size + 31 * 24 * 4, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=31 * 24 * 4, shuffle=False)

我有两个具体问题：

扩展窗口交叉验证：如何将扩展窗口交叉验证应用于此训练、验证和测试拆分？

选择时间步长：我的数据集呈现每周模式，我想预测未来 30 天的消耗量。考虑到 15 分钟的时间间隔，在这种情况下我的理想时间步长应该是多少？如果我使用 15 分钟的时间间隔作为一个时间步长，则会出现错误。


我愿意接受有关如何解决这些问题并扩展我的时间序列交叉验证代码的建议。任何帮助或指导将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78107902/time-series-cross-validation-for-load-forecast-of-charging-data-using-lstm-model</guid>
      <pubDate>Tue, 05 Mar 2024 12:58:27 GMT</pubDate>
    </item>
    <item>
      <title>无监督自动编码器产生输出维度 - 不同大小数据集的批次数</title>
      <link>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</guid>
      <pubDate>Tue, 05 Mar 2024 12:12:37 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 AttributeError: 'ConfigDict' 对象在 mmdetection 上没有属性 'data'？</title>
      <link>https://stackoverflow.com/questions/74209071/how-to-fix-attributeerror-configdict-object-has-no-attribute-data-on-mmdete</link>
      <description><![CDATA[我目前正在尝试为自定义数据集设置毫米检测，并且一直遇到这些错误。我尝试删除代码中的 def 函数，更改文件位置以及上述所有内容。我不知道代码有什么问题以及为什么它不起作用。
from argparse import ArgumentParser
从 mmdet.apis 导入 init_detector、inference_detector
导入MMCV
从 mmdet.apis 导入（async_inference_ detector、inference_ detector、
                        init_Detector、show_result_pyplot）
导入异步
进口火炬
从 mmdet.apis 导入 init_detector、async_inference_detector
从 mmdet.utils.contextmanagers 导入并发

定义数据（）：
# 新配置继承基本配置以突出显示必要的修改
    base_ = &#39;configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco.py&#39;

# 我们还需要更改 head 中的 num_classes 以匹配数据集的注释
# dict 是一个 python 字典对象，用于从 PyTorch 保存或加载模型
    模型=字典（
        roi_head=字典（
        # 定义边界框可以绕过的类数
            bbox_head=dict(num_classes=1),
            #
            mask_head=dict(num_classes=1)))

def 数据集():
    # 修改数据集相关设置
    dataset_type = &#39;COCO数据集&#39;
    #定义类
    类 = (&#39;受电弓&#39;)
    数据=字典（
        火车=字典（
            img_prefix=&#39;测试/&#39;,
            类=类，
            ann_file=&#39;train/Pan2_COCO.json&#39;),
        val=字典（
            img_prefix=&#39;测试/&#39;,
            类=类，
            ann_file=&#39;val/Pan2_COCO.json&#39;),
        测试=字典（
            img_prefix=&#39;测试/&#39;,
            类=类，
            ann_file=&#39;val/Pan2_COCO.json&#39;))
定义负载（）：
# 我们可以使用预训练的Mask RCNN模型来获得更高的性能
    load_from = &#39;测试/检查点/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth&#39;

数据（）
数据集()
加载（）

错误消息：
回溯（最近一次调用最后一次）：
  文件“tools/train.py”，第244行，在&lt;module&gt;中。
    主要的（）
  文件“tools/train.py”，第 135 行，在 main 中
    setup_multi_processes（cfg）
  文件“/home/dtl-admin/dev/mmdetection/mmdet/utils/setup_env.py”，第 30 行，setup_multi_processes
    workers_per_gpu = cfg.data.get(&#39;workers_per_gpu&#39;, 1)
  文件“/home/dtl-admin/miniconda3/envs/mmtest/lib/python3.8/site-packages/mmcv/utils/config.py”，第 519 行，在 __getattr__ 中
    返回 getattr(self._cfg_dict, 名称)
  文件“/home/dtl-admin/miniconda3/envs/mmtest/lib/python3.8/site-packages/mmcv/utils/config.py”，第 50 行，在 __getattr__ 中
    提高前任
AttributeError：“ConfigDict”对象没有属性“data”

我期待图像检测到类受电弓的输出。但是，无论我尝试什么，我都无法获得任何输出，并且我已尝试更改尽可能多的变量。]]></description>
      <guid>https://stackoverflow.com/questions/74209071/how-to-fix-attributeerror-configdict-object-has-no-attribute-data-on-mmdete</guid>
      <pubDate>Wed, 26 Oct 2022 13:58:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 对文本进行标记</title>
      <link>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</link>
      <description><![CDATA[我有以下代码从一组文件（文件夹名称是类别名称）中提取特征以进行文本分类。
导入sklearn.datasets
从 sklearn.feature_extraction.text 导入 TfidfVectorizer

train = sklearn.datasets.load_files(&#39;./train&#39;,description=None,categories=None,load_content=True,shuffle=True,encoding=None,decode_error=&#39;strict&#39;,random_state=0)
print len(训练数据)
打印train.target_names

向量化器 = TfidfVectorizer()
X_train = 矢量化器.fit_transform(train.data)

它抛出以下堆栈跟踪：
回溯（最近一次调用最后一次）：
  文件“C:\EclipseWorkspace\TextClassifier\main.py”，第 16 行，在  中
    X_train = 矢量化器.fit_transform(train.data)
  文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 1285 行，在 fit_transform 中
    X = super(TfidfVectorizer, self).fit_transform(raw_documents)
  文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 804 行，在 fit_transform 中
    self.fixed_vocabulary_)
  文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 739 行，在 _count_vocab 中
    对于分析（doc）中的功能：
  文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 236 行，位于 &lt;lambda&gt; 中
    tokenize(预处理(self.decode(doc))), stop_words)
  文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 113 行，解码
    doc = doc.decode(self.encoding, self.decode_error)
  文件“C:\Python27\lib\encodings\utf_8.py”，第 16 行，解码中
    返回codecs.utf_8_decode（输入，错误，True）
UnicodeDecodeError：“utf8”编解码器无法解码位置 32054 中的字节 0xff：起始字节无效

我运行Python 2.7。我怎样才能让它发挥作用？
编辑：
我刚刚发现这对于使用 utf-8 编码的文件（我的文件是 ANSI 编码）非常有效。有什么方法可以让 sklearn.datasets.load_files() 使用 ANSI 编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</guid>
      <pubDate>Fri, 01 May 2015 00:39:09 GMT</pubDate>
    </item>
    </channel>
</rss>