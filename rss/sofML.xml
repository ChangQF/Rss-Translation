<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 15 Oct 2024 12:33:20 GMT</lastBuildDate>
    <item>
      <title>没有传感器数据，预测模型还能有效吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79089664/can-predictive-models-be-effective-without-sensor-data</link>
      <description><![CDATA[我有纠正和预防性维护数据。对于纠正性维护，我有两台机器及其问题（只有一句话，没有数值）、日期、解决方案和停机时间。对于预防性维护，我有对同一两台机器进行季度、年度还是每月维护的数据。
如果我依靠这些值来创建机器学习模型来预测下一次故障，我的目标是回答这个问题——预防性任务完成后停机频率是增加还是减少，我的模型是否可靠并提供良好的预测，或者由于缺乏传感器数据，结果是否不准确？
我正在尝试清理数据并获取停机原因等特征，但我迷路了。我考虑过使用随机森林，但我不确定我的数据是否足够。]]></description>
      <guid>https://stackoverflow.com/questions/79089664/can-predictive-models-be-effective-without-sensor-data</guid>
      <pubDate>Tue, 15 Oct 2024 11:09:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 AI 进行图像重叠检测</title>
      <link>https://stackoverflow.com/questions/79089453/image-overlap-detection-with-ai</link>
      <description><![CDATA[我正在研究一个项目，需要检测图像或图形是否重叠。我一直在研究 AWS Rekognition，因为它们可以进行一些图像分析并提供边界框，我可以通过这些边界框计算它们是否重叠。但是，我正在寻找其他想法的指针，这些想法可能更强大、更可扩展。
我遇到过 openCV，但从未听说过，也没有尝试过。这可能是一个负责任的解决方案，有人可以提供一些见解吗？
我有很多可以通过监督学习提供的图像，因此我可以使用我已经拥有的数据来训练模型也可能是一个有效的选择。
有任何意见、想法或指向我可以进行更多研究的地方的指针吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79089453/image-overlap-detection-with-ai</guid>
      <pubDate>Tue, 15 Oct 2024 10:07:25 GMT</pubDate>
    </item>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>如何向强化学习模型添加命令？</title>
      <link>https://stackoverflow.com/questions/79088319/how-to-add-commands-to-a-reinforcement-learning-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088319/how-to-add-commands-to-a-reinforcement-learning-model</guid>
      <pubDate>Tue, 15 Oct 2024 03:27:53 GMT</pubDate>
    </item>
    <item>
      <title>当数据集包含一些文本字符串和数字数据时，如何继续拟合模型</title>
      <link>https://stackoverflow.com/questions/79088309/how-to-proceed-to-fit-in-a-model-when-dataset-has-some-text-strings-and-numeric</link>
      <description><![CDATA[我正在尝试将数据集放入模型中。缩放、删除不必要的数据后，数据集看起来如下所示；



seconds_log
minutes_log
country_enc
changing
description
Other feature variable




14.0058
0.693147
1
False
红色闪烁...
数字数据


3.401197
3.401197
0
True
白色旋转..
数字数据



因此，所有特征都具有数值，最后一个特征“描述”是长文本。请注意，我的目标变量是“country_enc”，其余所有列都是特征变量。描述值不能忽略。因此我使用&quot;TfidfVectorizer&quot;对数据进行矢量化。
我的问题是，在矢量化描述列之后，无论我得到什么，如何与数据集的其余部分相结合，然后将其拆分为测试和训练模型？
我看到一些谷歌示例，他们说；
combined_data = np.hstack((desc_tfidf.toarray(), original_dataframe_without_description))
print (combined_data)

如果我查看这个组合数据，它是一个numpy.ndarray。在分类模型中提供这些信息没有多大帮助。
基于将 Sklearn TFIDF 与附加数据相结合，如果我将稀疏矩阵转换为数组并与其余数据框（具有所有数值）连接，我会看到生成了几个 NaN 数据，这是预期的。我已经完成了数据预处理，所以再次执行 - 这是不对的。
我敢肯定，这不是正确的方法。有人可以在这里给出一些如何继续的指示吗？]]></description>
      <guid>https://stackoverflow.com/questions/79088309/how-to-proceed-to-fit-in-a-model-when-dataset-has-some-text-strings-and-numeric</guid>
      <pubDate>Tue, 15 Oct 2024 03:21:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Java 读取 Quick Draw! 二进制文件 [关闭]</title>
      <link>https://stackoverflow.com/questions/79087951/reading-quick-draw-binary-files-in-java</link>
      <description><![CDATA[我一直在从头开始试验机器学习算法，并用 Java 构建了一个模型，现在我想在 MNIST 数字以外的一些数据上训练它。我想使用 Quick, Draw! 数据，并想尝试读取我下载的一些涂鸦的二进制文件，但我不太明白。
我写了这段代码来读取数据：
public class QuickDrawReader {

public static void unpackDrawing(InputStream inputStream) throws IOException {
DataInputStream dataInputStream = new DataInputStream(inputStream);

long keyId = dataInputStream.readLong();
System.out.println(&quot;keyId: &quot; + keyId);

byte[] countryCodeBytes = new byte[2];
dataInputStream.readFully(countryCodeBytes);
String countryCode = new String(countryCodeBytes, &quot;UTF-8&quot;);
System.out.println(&quot;CountryCode: &quot; + countryCode);

byte perceived = dataInputStream.readByte();
System.out.println(&quot;Recognized: &quot; + perceived);

int timestamp = dataInputStream.readInt();
System.out.println(&quot;Timestamp: &quot; + timestamp);

int nStrokes = dataInputStream.readUnsignedShort();
System.out.println(&quot;NStrokes: &quot; + nStrokes);

List&lt;int[]&gt; image = new ArrayList&lt;&gt;();
// 如何填充图像列表???
}

public static void unpackDrawings(String filename) throws IOException {
try (FileInputStream fileInputStream = new FileInputStream(filename)) {
while (true) {
try {
unpackDrawing(fileInputStream);
} catch (EOFException e) {
break;
}
}
}
}
}

它似乎可以工作，直到它读取 nStrokes，我甚至不知道如何开始读取图像数据。有人能帮我弄清楚如何完成读取这些数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/79087951/reading-quick-draw-binary-files-in-java</guid>
      <pubDate>Mon, 14 Oct 2024 23:13:46 GMT</pubDate>
    </item>
    <item>
      <title>具有离散变量的线性回归问题，略有不同[关闭]</title>
      <link>https://stackoverflow.com/questions/79087896/linear-regression-problem-with-discrete-variables-with-a-twist</link>
      <description><![CDATA[我正在尝试开发一种 ML 算法来预测从更大的群体中选出的团队的表现。该组有 G 名成员，每个成员都用他们的名字来标识，并且通过从该组中选择 T 名成员来组建团队。数字 G 和 T 永远不会改变，组中每个成员的身份也不会改变。唯一改变的是我们选择哪个 T。这是监督学习，所以我有一组训练数据，每个数据都包含一个特定的团队选择及其获得的分数。成本函数试图最大化所有游戏的总分。从质量上讲，我知道不仅仅是某些团队成员的得分始终优于其他成员，而且某些成员组合的表现也优于其他组合。
我如何构建模型和算法来解决这个问题？我考虑过为每个团队成员或组中的每个人（在团队中或不在团队中）进行独热编码，但这并不能正确捕获所有约束（例如，成员在团队中出现的次数不应超过一次）。]]></description>
      <guid>https://stackoverflow.com/questions/79087896/linear-regression-problem-with-discrete-variables-with-a-twist</guid>
      <pubDate>Mon, 14 Oct 2024 22:42:45 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程二元分类：为什么 GPy 的方差比 scikit-learn 小得多？</title>
      <link>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</link>
      <description><![CDATA[我正在学习使用高斯过程进行二元分类，并且正在将 GPy 与 scikit-learn 进行比较，解决一个受 Martin Krasser 的博客文章启发的玩具一维问题。两种实现（GPy 和 scikit-learn）似乎都使用带有 RBF 内核的类似设置。优化内核超参数后，长度尺度相似，但方差相差很大。GPy 内核方差似乎太小了。
我如何修改我的 GPy 实现并获得与 scikit-learn 类似的结果？我怀疑这与每个算法的内部实现有关，但我不知道是什么导致了这种巨大的差异。下面我将进一步解释为什么我认为我的 GPy 实现需要修复。
实现细节：Python 3.9 搭配 GPy 1.13.2 和 scikit-learn 1.5.1。可重现的示例：
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# 第 1 部分：玩具数据集创建
#################################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2 # 潜在函数
t = bernoulli.rvs(sigmoid(a)) # Bernoulli 训练数据（0 和1s)

#####################################
# 第 2 部分：scikit-learn 实现
#####################################

从 sklearn.gaussian_process 导入 GaussianProcessClassifier
从 sklearn.gaussian_process.kernels 导入 ConstantKernel，RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
* RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
kernel=rbf,
optimizer=&#39;fmin_l_bfgs_b&#39;,
n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# 第 3 部分：GPy 实现
############################

导入 GPy

kern = GPy.kern.RBF(
input_dim=1,
variance=1.,
lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
X=X,Y=t, kernel=kern, 
inference_method=GPy.inference.latent_function_inference.laplace.Laplace(), 
可能性=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
num_restarts=10, optimizer=&#39;lbfgs&#39;,
verbose=True, robust=True)

print(m.kern)
# rbf。| 值 | 约束 | 先验
# 方差 | 0.8067562453940487 | 0.001,10.0 | 
# lengthscale | 0.8365668826459536 | 0.001,10.0 |

lenghtscale 值大致相似 (0.858 vs 0.836)，但方差值非常不同 (scikit-learn 为 1.5**2 = 2.25，而 GPy 仅为 0.806)。
我认为我的 GPy 实现需要调整的原因是，即使有 +/- 2 个标准偏差界限，真正的潜在函数 (参见上面代码第 1 部分中的“a”) 也与预测函数不紧密匹配。另一方面，scikit-learn 实现与之相当匹配（可以使用 scikit-learn 检索潜在函数平均值和标准差如此处所示）。

 左：两个模型的预测概率相似（这是有道理的，因为它们共享相似的长度尺度值）。右：GPy 的预测潜在函数与真实潜在函数的拟合度不如 scikit-learn 模型。 
到目前为止，我尝试过的方法，结果没有显著变化：

输入特征 (X) 归一化
使用 GPy.inference.latent_function_inference.expectation_propagation.EP() 作为 GPy 推理方法，而不是拉普拉斯方法
按照此处的建议，将 WhiteKernel 组件添加到 scikit-learn 实现中&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</guid>
      <pubDate>Mon, 14 Oct 2024 13:10:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 YoloV5 减慢相机速度</title>
      <link>https://stackoverflow.com/questions/79076547/slow-camera-with-yolov5</link>
      <description><![CDATA[我正在使用 Jetson Orin Nano 和 Raspberry Pi Camera Module V2，旨在运行 YOLOv5 进行实时图像处理。我尝试使用以下代码，但相机反馈非常缓慢且滞后，这是我没有想到的。如何改进？
这是我的代码：
import cv2
import torch
import numpy as np 

# 加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 用于 CSI 摄像头的 GStreamer 管道
def gstreamer_pipeline(
capture_width=1920, 
capture_height=1080,
display_width=960,
display_height=540,
framerate=30,
flip_method=2,
):
return (
&quot;nvarguscamerasrc ! &quot;
&quot;video/x-raw(memory:NVMM), &quot;
&quot;width=(int)%d, height=(int)%d, framerate=(fraction)%d/1 ! &quot;
&quot;nvvidconv flip-method=%d ! &quot;
&quot;video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! &quot;
&quot;videoconvert ! &quot;
&quot;video/x-raw, format=(string)BGR ! appsink drop=True&quot;
% (
capture_width,
capture_height,
framerate,
flip_method,
display_width,
display_height,
)
)

# 实时视频捕获
cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)

if not cap.isOpened():
print(&quot;无法打开摄像头。请检查连接。&quot;)
exit()

while cap.isOpened():
cv2.namedWindow(&quot;Detect&quot;, cv2.WINDOW_AUTOSIZE)

ret, frame = cap.read()

if not ret:
print(&quot;无法检索帧。&quot;)
break

# 使用 YOLOv5 进行预测
results = model(frame)
cv2.imshow(&#39;YOLO&#39;, np.squeeze(results.render())) 

if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

cap.release()
cv2.destroyAllWindows()

有趣的是，当我使用以下管道时，相机运行速度要快得多：
import cv2
gst_str = &quot;nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), width=1920, height=1080, framerate=30/1 ! nvvidconv flip-method=2 ! video/x-raw, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink&quot;

cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)

if not cap.isOpened():
print(&quot;无法打开摄像头&quot;)
else:
print(&quot;摄像头已打开&quot;)

while True:
ret, frame = cap.read()
if not ret:
print(&quot;无法检索帧&quot;)
break

cv2.imshow(&quot;CSI 摄像头&quot;, frame)

if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

cap.release()
cv2.destroyAllWindows()

我想使用 YOLO 进行实时图像处理并最终构建一个训练算法。我愿意接受有关优化当前设置或切换到其他 YOLO 变体或任何其他适合 Jetson Orin Nano 的方法的建议。]]></description>
      <guid>https://stackoverflow.com/questions/79076547/slow-camera-with-yolov5</guid>
      <pubDate>Fri, 11 Oct 2024 01:17:56 GMT</pubDate>
    </item>
    <item>
      <title>RNN 中多对一预测的损失计算</title>
      <link>https://stackoverflow.com/questions/79074702/loss-calculation-for-many-to-one-prediction-in-rnn</link>
      <description><![CDATA[我正在尝试在 PyTorch 中实现一个简单的 RNN 模型，用于多对一时间序列预测。
假设我记录了 1-7 的观察结果，设计矩阵的结构如下：
[1,2,3,4]
[2,3,4,5]
[3,4,5,6]
[4,5,6,7]
这个想法是基于之前的四个观察结果做出一个预测，即多对一预测。
[1,2,3,4] - [5]
[2,3,4,5] - [6]
[3,4,5,6] - [7]
[4,5,6,7] - [8]
并且 [5,6,7,8] 构成了我的预测序列值。
我理解，每个数据点通过 RNN 模型时，都会生成一个预测。我的问题涉及每个时间步骤的损失计算。
具体来说，在第一个数据点 1 传入模型并产生预测后，会产生 1&#39;。

然后模型是否会查看预测 1&#39; 和目标 5，然后计算 1&#39; 和 5 之间的损失，并将预测 2&#39; 与 5 进行比较，依此类推。

所以基本上第一个序列是 [1,2,3,4]，相应的目标是 [5,5,5,5]。该模型总共计算损失 4 次，每对一次。

或者它是否让所有四个时间步骤（对应于数据点 1-4）通过并识别出这 4 个数据点的序列应该产生一个预测。因此，模型处理所有前三个数据点 1、2、3，并在不考虑目标的情况下产生预测 1&#39;、2&#39;、3&#39;。它计算损失直到第四次预测 4&#39; 之后，这个损失 L(4&#39;, 5) 以某种方式代表了该序列的总损失，然后向后传播。

在阅读了大量教程和我的课程文本后，我倾向于相信，对于一系列数据点 [1,2,3,4]，我们仍然必须为该模型提供一个目标 [2,3,4,5]，这样模型就会学习到 1&#39; 是 2 的预测（向前迈出一步），2&#39; 是 3 的预测，并相应地适应模式。
在这种情况下，多对一预测与一对一预测的不同之处仅在于我们只是提取最后一个预测，并且 RNN 模型在参数更新方面的内部工作原理与代码中具体呈现的内容相同（输入形状等）。]]></description>
      <guid>https://stackoverflow.com/questions/79074702/loss-calculation-for-many-to-one-prediction-in-rnn</guid>
      <pubDate>Thu, 10 Oct 2024 13:44:29 GMT</pubDate>
    </item>
    <item>
      <title>具有高精度（95%）/低验证率（0%）的面部识别计算机视觉图像分类模型</title>
      <link>https://stackoverflow.com/questions/79073365/facial-recognition-computer-vision-image-classification-model-with-high-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79073365/facial-recognition-computer-vision-image-classification-model-with-high-accuracy</guid>
      <pubDate>Thu, 10 Oct 2024 08:13:20 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv5 类别不平衡和过度拟合问题</title>
      <link>https://stackoverflow.com/questions/78985031/yolov5-class-imbalance-and-overfitting-issues</link>
      <description><![CDATA[我正在开发一个安全带和手机检测系统，使用 YOLOv5s 来检测挡风玻璃、驾驶员、乘客、安全带和手机。我的数据集存在类别不平衡问题，因为并非每张图片都包含安全带或手机，而手机类别尤其代表性不足。
此外，手机很小，很难在图像中检测到。我注意到验证损失有一些波动，特别是在第 20 次之后开始增加，这让我怀疑是过度拟合。
yolov5s 的结果
pr 曲线
混淆矩阵
混淆矩阵 1
这是我的代码，我使用的是预训练模型来自 Ultralytics：
# 使用您的数据集训练模型
model.train(
data=&quot;full_dataset/data/data.yml&quot;,
imgsz=640,
epochs=100,
batch=16,
worker=4,
project=&quot;SeatBeltMobileDetection&quot;,
name=&quot;YOLOv5s_640_epochs100&quot;,
device=0
)

问题：

考虑到类别不平衡（尤其是手机检测），验证损失的波动和 DFL 损失的增加是否意味着过度拟合？

在这种类别不平衡的情况下，对 YOLOv5 进行微调的最佳实践是什么？调整类别权重等技术是否有帮助（我之前做过过采样和增强）？

我是否应该考虑对 YOLOv5 训练超参数进行任何特定调整，以提高手机等小物体的性能？

]]></description>
      <guid>https://stackoverflow.com/questions/78985031/yolov5-class-imbalance-and-overfitting-issues</guid>
      <pubDate>Sat, 14 Sep 2024 11:42:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中有效地实现非全连接线性层？</title>
      <link>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</link>
      <description><![CDATA[我制作了一个示例图，展示了我试图实现的缩小版本：

因此，顶部两个输入节点仅完全连接到顶部三个输出节点，并且相同的设计适用于底部两个节点。到目前为止，我已经想出了两种在 PyTorch 中实现此目的的方法，但都不是最佳方法。
第一种方法是创建一个包含许多较小线性层的 nn.ModuleList，并在前向传递期间，通过它们迭代输入。对于图表的示例，它看起来像这样：
class Module(nn.Module):
def __init__(self):
self.layers = nn.Module([nn.Linear(2, 3) for i in range(2)])

def forward(self, input):
output = torch.zeros(2, 3)
for i in range(2):
output[i, :] = self.layers[i](input.view(2, 2)[i, :])
return output.flatten()

因此，这完成了图中的网络，主要问题是它非常慢。我认为这是因为 PyTorch 必须按顺序处理 for 循环，而不能并行处理输入张量。
要“矢量化”模块以便 PyTorch 可以更快地运行它，我有这个实现：
class Module(nn.Module):
def __init__(self):
self.layer = nn.Linear(4, 6)
self.mask = # 创建 1 和 0 的掩码来“阻止”某些层连接

def forward(self, input):
prune.custom_from_mask(self.layer, name=&#39;weight&#39;, mask=self.mask)
return self.layer(input)

这也完成了图表的网络，通过使用权重修剪来确保完全连接层中的某些权重始终为零（例如，连接顶部输入节点和底部输出节点的权重将始终为零，因此它实际上是“断开连接的”）。这个模块比上一个快得多，因为没有 for 循环。现在的问题是这个模块占用了更多的内存。这可能是因为，即使大多数层的权重为零，PyTorch 仍会将网络视为它们存在。此实现本质上保留了比需要更多的权重。
有人遇到过这个问题并想出了有效的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 03:41:06 GMT</pubDate>
    </item>
    <item>
      <title>保留 TFIDF 结果以预测新内容</title>
      <link>https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content</link>
      <description><![CDATA[我正在使用 Python 上的 sklearn 进行一些聚类。我已经训练了 200,000 个数据，下面的代码运行良好。
corpus = open(&quot;token_from_xml.txt&quot;)
vectorizer = CountVectorizer(decode_error=&quot;replace&quot;)
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
km = KMeans(30)
kmresult = km.fit(tfidf).predict(tfidf)

但是当我有新的测试内容时，我想将其聚类到我训练过的现有聚类中。所以我想知道如何保存IDF结果，以便我可以对新的测试内容进行TFIDF，并确保新测试内容的结果具有相同的数组长度。
提前致谢。
更新
如果其中一个包含经过训练的IDF结果，我可能需要将“transformer”或“tfidf”变量保存到文件（txt或其他文件）。
更新
例如。我有训练数据：
[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]
[&quot;a&quot;, &quot;b&quot;, &quot;d&quot;]

然后执行 TFIDF，结果将包含 4 个特征（a、b、c、d）
当我测试时：
[&quot;a&quot;, &quot;c&quot;, &quot;d&quot;]

查看它属于哪个聚类（已由 k-means 生成）。TFIDF 只会给出具有 3 个特征（a、c、d）的结果，因此 k-means 中的聚类将下降。 （如果我测试 [&quot;a&quot;, &quot;b&quot;, &quot;e&quot;]，可能会有其他问题。）
那么如何存储测试数据的特征列表（甚至将其存储在文件中）？]]></description>
      <guid>https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content</guid>
      <pubDate>Wed, 22 Apr 2015 04:55:06 GMT</pubDate>
    </item>
    </channel>
</rss>