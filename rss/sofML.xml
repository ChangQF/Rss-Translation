<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Dec 2023 03:16:15 GMT</lastBuildDate>
    <item>
      <title>如何使用 Python 更有效地通过线性回归分析此数据集？</title>
      <link>https://stackoverflow.com/questions/77603468/how-can-i-analyze-this-dataset-with-linear-regression-with-python-more-efficient</link>
      <description><![CDATA[是否有方法使用线性回归来预测交通事故趋势？我是 Python 新手，我被困在将列转换为 DataFrame 的部分上。
这是我需要使用的数据集：https://www.kaggle.com /datasets/sobhanmoosavi/us-accidents
我使用了 plt.scatter 和 plt.show() 等函数，但由于某种原因它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/77603468/how-can-i-analyze-this-dataset-with-linear-regression-with-python-more-efficient</guid>
      <pubDate>Tue, 05 Dec 2023 02:08:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn train_test_split 使用 KBinsDiscretizer 平衡训练集</title>
      <link>https://stackoverflow.com/questions/77603223/using-sklearn-train-test-split-to-balance-training-set-using-kbinsdiscretizer</link>
      <description><![CDATA[我正在训练一个具有 3 个输入 (3 x 189,000) 并映射到单个输出 (1 x 189,000) 的数据集。数据集不平衡，平衡数据的方法是使用输入之一（“传递能量”），因为数据偏向更高的传递能量我想为低能量、中能量和高能量创建 3 个容器活力。为了离散化此列，我执行以下操作：
p = np.array(data[1, :]).reshaoe((-1,1))
est = KBinsDiscretizer(n_bins=3, 编码=&#39;序数&#39;, 策略=&#39;统一&#39;,
                           子样本=无）
估计拟合(p)
p_bins = est.transform(p)

当我检查这是如何实现的时，我发现 bin 大小为 161493、25417 和 2985，这是有道理的，因为正如我所说，低能量 bin 的点数要少得多。然后，我将其作为第四列添加到输入中，然后使用该列作为分层输入运行 sklearn 的 train_test_split。
&lt;前&gt;&lt;代码&gt;Y = 数据[3, :]
X = np.append(数据[0:3, :], p_bins.T, 轴=0)
# 将 model_data 拆分为训练数据和测试数据
# 分离测试数据
x, x_test, y, y_test = train_test_split(X.T, Y, test_size=0.25, random_state=42,
                                        随机播放=真，
                                        分层=p_bins）

这不起作用，因为它只是保留我创建的离散化，并导致预期的分割与之前离散化定义的箱之间的比率相同（输出大小分别为：121259、18941 和 2221）。我正在寻找的是一种方法来获取我定义的垃圾箱并创建一个具有均匀分布的训练集，以及一个具有剩余值的验证和测试集，但从最小的垃圾箱中保留一些用于验证和测试集。 
例如，训练集可能有（2221、2221 和 2221），测试集和验证集可能有（79636、11598 和 382），关于如何实现这一点的建议？]]></description>
      <guid>https://stackoverflow.com/questions/77603223/using-sklearn-train-test-split-to-balance-training-set-using-kbinsdiscretizer</guid>
      <pubDate>Tue, 05 Dec 2023 00:22:01 GMT</pubDate>
    </item>
    <item>
      <title>如何将单词“浮动”为二进制而不出现错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77603138/how-do-i-float-words-to-binary-without-getting-error</link>
      <description><![CDATA[我试图将数据集中的单词浮动为二进制，但我不断收到错误，提示“numpy 没有浮动属性”在此处输入图片描述
我希望将单词转换为数字/二进制以便计算机能够理解]]></description>
      <guid>https://stackoverflow.com/questions/77603138/how-do-i-float-words-to-binary-without-getting-error</guid>
      <pubDate>Mon, 04 Dec 2023 23:48:14 GMT</pubDate>
    </item>
    <item>
      <title>考虑到之前的输出，是否可以在 keras 中自定义回归损失函数</title>
      <link>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</link>
      <description><![CDATA[我正在构建 CNN 图像回归模型。我想在损失函数中添加一个惩罚项，以惩罚当前预测是否大于先前的期望。
我的问题是有没有办法使用 Keras 而不是使用 tf.GradientTape() 编写代码
（这个帖子看起来和我想问的很相似
使用 Gradient Tape 的自定义损失函数，TF2.6)&lt; /p&gt;
这是数据描述。
我的数据是视频图像集，我想预测异常事件发生之前还剩多少次。
因此，图像标签应该尽可能地减少到异常标签。
（例如，异常的标签为0，前一张图像为1，前两张图像，该图像标签为2。）
因此，我的预测也会减少，如果当前预测高于之前的预测，我想添加惩罚项。
这是模型架构代码。我使用预训练的 VGG16 模型并将其改编为回归模型。
&lt;前&gt;&lt;代码&gt;
初始模型 = tf.keras.applications.VGG16(权重 = &#39;imagenet&#39;,include_top = False)
初始模型.trainable = False

func_model_p = keras.Sequential()
输入 = keras.Input(形状 = (700,100,3))
func_model_p.add(输入)

func_model_p.add(keras.layers.GlobalAveragePooling2D())
func_model_p.add(keras.layers.Dense(1,激活=“线性”))

# 准备指标。
train_mse_metric_p = keras.metrics.MeanSquaredError()
val_mse_metric_p = keras.metrics.MeanSquaredError()

此代码适用于 tf.GradientTape。我想为 Keras 更改此代码。
&lt;前&gt;&lt;代码&gt;
对于范围内的 e（纪元）：
    对于范围内的 i(len(y_train_d_noshuffle))：
        使用 tf.GradientTape() 作为磁带：

            图像 = np.expand_dims(X_train_d_noshuffle[i], axis=0)
            y_hat = func_model_p(图像, 训练=True)

            先前值 = 先前列表[-1]
            Violation_term = tf.constant(max( (y_hat - previous_value) , 0), dtype=tf.float32)

            尝试：
                如果 y_train_d_noshuffle[i] &lt; y_train_d_noshuffle[i+1]：
                    违规项 = 0
            除外：通过

            y_train_c = tf.constant(y_train_d_noshuffle[i])

            mse = tf.keras.losses.MeanSquaredError()(y_train_c, y_hat)
            # 计算损失
            损失值 = mse + 违规项
            previous_list.append(y_hat)

            train_mse_sum = train_mse_sum + mse
            train_penalty_sum = train_penalty_sum + Violation_term
        
        grads = Tape.gradient(loss_value, func_model_p.trainable_weights)
        优化器.apply_gradients(zip(grads, func_model_p.trainable_weights))

        # 更新训练指标。
        train_mse_metric_p.update_state(y_train_d_noshuffle[i], y_hat)

        # 显示每个时期结束时的指标。
        train_mse = train_mse_metric_p.result().numpy()

    print(f&#39;epochs: {e}, train_mse_sum: {train_mse_sum}, train_penalty_sum: {float(train_penalty_sum)}&#39;) ```



如果您知道路请告诉我！

先感谢您！
]]></description>
      <guid>https://stackoverflow.com/questions/77602829/is-it-possible-to-make-a-custom-for-regression-loss-function-in-keras-considerin</guid>
      <pubDate>Mon, 04 Dec 2023 22:05:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 1D 贝叶斯 CNN（通过使用工作 CNN 的 Convolution1DFlipout 和 DenseFlipout 替换卷积层和密集层而制成）无法训练？</title>
      <link>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</link>
      <description><![CDATA[我有一个 CNN 模型，它将波形（形状为 (601,3)，其中 601 是时间步数，3 是通道数）分类为噪声或信号。如下：
# 导入
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
将张量流导入为 tf
从张量流导入keras
从tensorflow.keras导入层

随机种子 = 42

tf.random.set_seed(random_seed)

模型 = keras.Sequential([
    Layers.Input(shape=(601, 3)), # 一维数据的输入形状
    层.Conv1D（32，kernel_size = 16，激活=&#39;relu&#39;），
    层.Conv1D（64，kernel_size = 16，激活=&#39;relu&#39;），
    层.Conv1D（128，kernel_size = 16，激活=&#39;relu&#39;），
    层.Flatten(),
    层.Dense(80, 激活=&#39;relu&#39;),
    层.Dense(80, 激活=&#39;relu&#39;),
    层.Dense(2, 激活=&#39;softmax&#39;)
]）

优化器 = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=优化器, 指标=[&#39;准确性&#39;])

纪元数 = 40
批量大小 = 48

历史= model.fit(X_train, y_train_encoded, epochs=num_epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), 详细=2)

# X_train 形状: (num_train_samples,601,3)
# X_test 形状: (num_test_samples,601,3)
# y_train_encoded 形状：(num_train_samples,2)
# y_test_encoded 形状：(num_test_samples,2)

上述模型运行良好，并在第 12 个 epoch 收敛，在所有 epoch 训练后，准确率超过 99%。
当我尝试通过分别用 Convolution1DFlipout 和 DenseFlipout 层替换 Conv1D 和 Dense 层来将上述 CNN 转换为贝叶斯 CNN 时，问题就出现了。
# 导入
将tensorflow_probability导入为tfp

tfd = tfp.分布
tfpl = tfp.层

随机种子 = 42

tf.random.set_seed(random_seed)

num_training_samples = X_train.shape[0]
kl_divergence_fn = lambda q, p, _: tfd.kl_divergence(q, p) / num_training_samples

模型 = keras.Sequential([
    层.输入(形状=(601, 3)),
    tfpl.Convolution1DFlipout(
        32、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    tfpl.Convolution1DFlipout(
        64、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    tfpl.Convolution1DFlipout(
        128、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    层.Flatten(),
    tfpl.DenseFlipout(80，激活=tf.nn.softmax，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn),
    tfpl.DenseFlipout(80，激活=tf.nn.softmax，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn),
    tfpl.DenseFlipout(2，激活=tf.nn.softmax，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn)
]）

优化器 = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=优化器, 指标=[&#39;准确性&#39;])

# 训练模型（与之前相同）
纪元数 = 40
批量大小 = 48

历史= model.fit(X_train, y_train_encoded, epochs=num_epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), 详细=2)

这个模型似乎没有收敛。有人可以帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</guid>
      <pubDate>Mon, 04 Dec 2023 21:17:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用异步生成器/如何将数据异步加载到数据集中？</title>
      <link>https://stackoverflow.com/questions/77602360/how-to-use-an-async-generator-how-to-load-data-asynchronous-into-a-dataset</link>
      <description><![CDATA[例如，当尝试从生成器返回承诺时，我收到打字稿错误。
const generated = function* () {
    产生新的 Promise(() =&gt; {});
};

tf.data.generator(生成);

&#39;() =&gt; 类型的参数生成器，无效，未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。
使用异步生成器也不起作用：
异步生成器导致类型错误
tf.data.generator(异步函数* () {})

抛出
&#39;() =&gt; 类型的参数AsyncGenerator&lt;任何、无效、未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。
这不应该是一个常见的用例吗？人们需要从网络获取数据或从数据库中读取数据来学习，而数据太大而无法一次全部装入内存？]]></description>
      <guid>https://stackoverflow.com/questions/77602360/how-to-use-an-async-generator-how-to-load-data-asynchronous-into-a-dataset</guid>
      <pubDate>Mon, 04 Dec 2023 20:23:17 GMT</pubDate>
    </item>
    <item>
      <title>回归的特征选择</title>
      <link>https://stackoverflow.com/questions/77601601/feature-selection-for-regression</link>
      <description><![CDATA[我正在做一个学校项目，其中我 (1) 使用创意写作中的计算语言学 (CL) 功能寻找教师分数和算法计算分数之间的相关性，以及 (2) 尝试使用这些功能和算法来预测教师分数回归。
我有 6 位老师，他们根据 4 个标准的评分标准对 12 个故事进行评分（每个故事总共有 6x4 分）。对于评分标准中的每个标准，我选择了相应的 CL 特征，并编写了一个代码实现，使用这些特征来计算分数（每个故事总共 4 个分数）。
对于相关性分析，我将每个故事、每个标题组件的平均教师分数（因此减少到每个故事 4 分）与每个故事、每个相应功能的 CL 分数（每个故事仍然 4 分）。
现在，如果我训练回归模型，(A) 使用每个故事的单独分数（即每个故事 6x4 分数）而不是 (B) 教师平均分数（每个故事 4 分）是否有意义？我背后的原因是使用个人分数，因为这样我就有更多数据来训练我的模型，但我不确定我是否监督了某些事情。
答：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

故事
评分者
得分 c1
得分 c2
得分 c3
得分 c4
得分f1
得分f2
得分f3
得分f4


&lt;正文&gt;

故事_1
grader_1
5
5
8
10
7
5
6
8


故事_1
grader_2
9
7
8
6
7
5
6
8


故事_1
grader_3
7
7
8
7
7
5
6
8


故事_1
grader_4
7
5
6
9
7
5
6
8


故事_1
grader_5
6
6
6
8
7
5
6
8


故事_1
grader_6
8
6
6
8
7
5
6
8


...
...
...
...
...
...
...
...
...
...


故事_12
grader_1
1
6
8
7
4
6
5
6


故事_12
grader_2
4
6
7
7
4
6
5
6


故事_12
grader_3
3
5
6
7
4
6
5
6


故事_12
grader_4
3
6
4
5
4
6
5
6


故事_12
grader_5
3
7
3
5
4
6
5
6


故事_12
grader_6
3
6
2
5
4
6
5
6




B：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

故事
平均得分 c1
平均得分 c2
平均得分 c3
平均得分 c4
得分f1
得分f2
得分f3
得分f4


&lt;正文&gt;

故事_1
7
6
7
8
7
5
6
8


...
...
...
...
...
...
...
...
...


故事_12
3
6
5
6
4
6
5
6



]]></description>
      <guid>https://stackoverflow.com/questions/77601601/feature-selection-for-regression</guid>
      <pubDate>Mon, 04 Dec 2023 17:55:28 GMT</pubDate>
    </item>
    <item>
      <title>特征矩阵和变量赋值[关闭]</title>
      <link>https://stackoverflow.com/questions/77601471/feature-matrix-and-variable-assignment</link>
      <description><![CDATA[我目前正在使用 Python 开发 ML 的 scikit 模块。我有一个 .csv 格式的数据集。我在选择特征矩阵 X 和变量 y 时遇到问题。它说找不到我作为代码输入的列标题。我也尝试对它们取消索引，但仍然无法做到。]]></description>
      <guid>https://stackoverflow.com/questions/77601471/feature-matrix-and-variable-assignment</guid>
      <pubDate>Mon, 04 Dec 2023 17:35:58 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 int）</title>
      <link>https://stackoverflow.com/questions/77600884/tensorflow-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</link>
      <description><![CDATA[我收到错误“ValueError：无法将 NumPy 数组转换为张量（不支持的对象类型 int）。”在下面的 nn_model.fit 行上。
这里我的 data.csv 文件包含列：文本（对象类型）、附件（float 类型）、类别（lable、int 类型）。
# 将数据帧拆分为测试数据和训练数据
def split_data(壮举,标签):
    # 训练测试分割
    x_train, x_val, y_train, y_val = train_test_split(
        壮举，encode_labels（标签），test_size = 0.20，random_state = 42，shuffle = True）
    数据 = {“train”：{“X”：x_train，“y”：y_train}，
            “测试”：{“X”：x_val，“y”：y_val}}
    返回数据

## NN 的标签编码
def 编码标签（标签）：
    # 标签处理
    le = 标签编码器()
    le.fit(标签)
    类= le.classes_
    编码标签 = le.transform(标签)
    # 使用 Keras 将整数转换为 one-hot 编码
    编码标签 = utils.to_categorical(编码标签)
    返回编码标签

定义模型（）：
        # CNN + LSTM 模型
        模型=顺序（）
        model.add(Embedding(train_args[“max_feature”], train_args[“embed_size”], input_length=train_args[“max_len”]))
        model.add(Conv1D(filters=64，kernel_size=7，padding=&#39;same&#39;，activation=&#39;relu&#39;))
        model.add(MaxPooling1D(pool_size=2, padding=&#39;相同&#39;))
        model.add(Dropout(0.1)) # 由于严重过拟合而进行正则化
        model.add(双向(LSTM(64, recurrent_dropout=0.02,return_sequences=True)))
        model.add(GlobalMaxPool1D())
        模型.add(Dropout(0.1))
        model.add(密集(16,activation=&#39;relu&#39;))
        model.add(Dense(train_args[“total_classes”],activation=&#39;softmax&#39;))

        model.layers[0].trainable = True

        model.compile(优化器=&#39;亚当&#39;,
                    损失=&#39;分类交叉熵&#39;，
                    指标=[&#39;准确性&#39;])
        logger.info(模型.summary())
        返回模型

# 训练模型，返回模型
def train_model(数据):
    ＃ 训练
    logger.info(“开始训练神经网络”)
    nn_model = 模型()
    历史= nn_model.fit(数据[“火车”][“X”],
                数据[“火车”][“y”]，
                批量大小=64，
                纪元=10，
                validation_data=(数据[“测试”][“X”]，数据[“测试”][“y”]))
    logger.info(&quot;神经网络训练完成&quot;)
    返回 nn_model

# 评估模型的指标
def get_model_metrics（模型，数据）：
    preds = model.predict(data[&quot;test&quot;][&quot;X&quot;])
    准确度 = precision_score(np.argmax(preds, axis=1), np.argmax(data[&quot;test&quot;][&quot;y&quot;],axis=1))
    指标 = {“accuracy_score”：准确性}
    返回指标

def main():
    # 将训练数据加载为数据帧
    data_dir =“数据”；
    data_file = os.path.join(data_dir, &#39;data.csv&#39;)
    train_df = pd.read_csv(数据文件)
    标签 = train_df[“类别”]
    feat = train_df.drop(columns=[&#39;类别&#39;])

    数据 = split_data(壮举,标签)

    # 训练模型
    模型 = train_model(数据)

    # 记录模型的指标
    指标= get_model_metrics（模型，数据）
    对于metrics.items()中的(k, v)：
        打印（f“{k}：{v}”）

如果 __name__ == “__main__”：
    主要的（）

现在我想运行该程序，但不知道该怎么做才能运行它？我对 python 和 ML 不太熟悉。]]></description>
      <guid>https://stackoverflow.com/questions/77600884/tensorflow-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</guid>
      <pubDate>Mon, 04 Dec 2023 16:05:15 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习的高光谱图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</link>
      <description><![CDATA[我有一个人脸高光谱图像数据集，共有42名参与者，压力类别为4-情绪压力基线、情绪压力、身体压力基线、PS1（在某个时间测量的身体压力）、PS2（在其他时间测量的身体压力）。使用这个高光谱图像数据集，我们必须将其分类为身体压力或情绪压力。该数据集包含 mat 文件。我们必须将压力分类为身体压力或情绪压力。对于分类，我们必须使用SVM。您能建议在 jupyter Notebook 中实现它的代码吗？
文字
数据集链接-文本
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC

# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)

    # 在训练数据上训练模型
    clf = SVC(内核=&#39;线性&#39;)
    clf.fit(X_train, y_train)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度 = np.mean(y_pred == y_test)
    print(&#39;准确度：&#39;, 准确度)


我收到这样的错误
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-6-0e3b82a51446&gt;在&lt;细胞系：31&gt;()
     45
     46 # 将数据拆分为训练集和测试集
---&gt; 47 X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)
     48
     49 # 在训练数据上训练模型

5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
   第6932章
   第6933章
-&gt;第6934章
   第6935章
   第6936章

KeyError：“在轴中找不到[&#39;target_column&#39;]”

如何解决该错误？基本目标是打印精度。
在此处输入图片描述
我试图打印准确性但出现错误]]></description>
      <guid>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</guid>
      <pubDate>Mon, 04 Dec 2023 14:45:22 GMT</pubDate>
    </item>
    <item>
      <title>后端的大.pkl数据没有推送到github中</title>
      <link>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</link>
      <description><![CDATA[我正在学习机器学习。最近，我从 tmdb 数据集制作了电影推荐模型，我使用 .pkl （二进制）文件中的模型处理数据。使用该数据制作后端，但是数据太大，无法推送到 github，我无法托管网站。
我正在尝试将已处理的数据推送到后端，但无法部署，因为它超出了文件大小的限制]]></description>
      <guid>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</guid>
      <pubDate>Mon, 04 Dec 2023 14:33:48 GMT</pubDate>
    </item>
    <item>
      <title>张量流形状错误</title>
      <link>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</link>
      <description><![CDATA[我正在进行深度 Q 学习，当我将图像提供给我的模型（来自硒驱动程序）时，一切都很好，但是当我尝试拟合我的模型时，它给了我这个错误
ValueError：层“顺序”的输入 0与图层不兼容：预期形状=(无, 500, 400, 1)，发现形状=(无, 4, 500, 400, 1)


这是我的图像、合身度和模型的代码
 def get_image(驱动程序):
      屏幕 = driver.get_screenshot_as_png()
      img = Image.open(BytesIO(屏幕))
      返回 np.array(img)[500:900, 400:900]

    def load_model(自身):
        ”“”
        从文件加载模型。
        ”“”

        if os.path.isfile(“model.h5”):
            self.model = load_model(“model.h5”)
            print(“模型已加载。”)

        模型=顺序（）
        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3), input_shape=(500, 400, 1)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #隐藏层64个神经元
        模型.add(压平())
        model.add(密集(64))
        model.add(激活(&#39;relu&#39;))
        #输出层有 5 个神经元，每个神经元对应一个可能的动作
        model.add(密集(5))

        #编译模型
        model.compile(loss=&#39;categorical_crossentropy&#39;,
                      优化器=&#39;亚当&#39;,
                      指标=[&#39;准确性&#39;])


        返回模型

      def fit_model(自身):
        ”“”
        将模型拟合到重播内存中。
        ”“”

        #如果回放内存不够满，则不要训练模型
        如果 len(self.replay_memory) &lt; self.MIN_REPLAY_MEMORY_SIZE：
            返回

        print(&quot;拟合模型&quot;)

        #从重放内存中获取过渡的随机样本
        样本 = random.sample(self.replay_memory, self.MINIBATCH_SIZE)

        #从样本中获取当前状态
        current_states = np.array([样本中的转换的转换[0]])
        #预测当前状态的q值
        current_qs_list = self.model.predict(current_states)

        #从样本中获取未来状态
        future_states = np.array([样本中的转换的转换[3]])
        #预测未来状态的q值
        future_qs_list = self.model.predict(future_states)

        X = []
        y = []

        #对于样本中的每个转换
        对于枚举（样本）中的索引（current_state、action、reward、future_state、done）：
            #如果过渡不是样本中的最后一个
            如果没有完成：
                #计算所采取行动的新q值
                max_future_q = np.max(future_qs_list[索引])
                new_q = 奖励 + self.DISCOUNT * max_future_q
            别的：
                #如果转变是样本中的最后一个，则将新的 q 值设置为奖励
                new_q = 奖励

            #更新所采取操作的q值
            current_qs = current_qs_list[索引]
            当前_qs[操作] = 新_q

            #将当前状态和新的q值添加到训练数据中
            X.append(当前状态)
            y.append(current_qs)

        #将模型拟合到训练数据上
        self.model.fit(np.array(X)，np.array(y)，batch_size=self.MINIBATCH_SIZE，verbose=0，shuffle=False)

        #更新目标模型
        如果 self.target_update_counter &gt; self.UPDATE_TARGET_EVERY：
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0
            self.save_model()
        别的：
            self.target_update_counter += 1```


我已经尝试对我的图像进行 .shape，但它不起作用。我还尝试将我的过渡[0]和过渡[3]直接添加到列表中，并制作预测列表，但它不起作用并告诉我 current_qs[action] 超出范围。这很奇怪，因为我在创建 futur_qs_list 和 current_qs_list 时想要预测的图像似乎与我用来预测动作的图像形状不同。然而，它与我的代码中的格式完全相同，所以我不知道该怎么办。]]></description>
      <guid>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</guid>
      <pubDate>Mon, 04 Dec 2023 14:08:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在colab中查找数据集的某一列中有多少个不同的数据</title>
      <link>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</link>
      <description><![CDATA[我有一个大约由 400000 行和 8 列组成的数据集，我只想知道一列中有多少种不同类型的数据，我该怎么做？列中的数据是字符串的形式，我需要给它们分配数字，所以我需要找出该列中有多少个不同的单词。我不知道我应该做什么]]></description>
      <guid>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</guid>
      <pubDate>Mon, 04 Dec 2023 12:22:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 svm 进行高光谱图像分类</title>
      <link>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</link>
      <description><![CDATA[将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
从 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler



# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.1, random_state=42)

   # 将 SMOTE 应用于训练数据
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # 使用 RandomForestClassifier （如您的示例中所示）
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train_resampled, y_train_resampled)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度=准确度_得分(y_test, y_pred)
    print(&#39;准确度：&#39;, 准确度)

我尝试过欠采样、不同的 ckassifiers，如 svm、knn 和随机森林分类器（对数据 imabalance 不太敏感）。仍然无法解决该错误。
错误-KeyError Traceback（最近一次调用最后一次）
 在&lt;细胞系：43&gt;()
43、如果len(imput_data)==1：
44#处理单个样品箱
---&gt; 45 X_train = impulated_data.drop(&#39;target_column&#39;, axis=1)
46 y_train = impulated_data[&#39;target_column&#39;]
47
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
第6932章
第6933章
-&gt;第6934章
第6935章
第6936章
KeyError：“在轴中找不到[&#39;target_column&#39;]”]]></description>
      <guid>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</guid>
      <pubDate>Sun, 03 Dec 2023 13:04:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会尝试利用 Sagemaker 的多个实用程序，而不是像我目前所做的那样在笔记本上运行所有内容。
问题是，在研讨会上，他们教您如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，因此我实例化了一个估计器如下此示例文件：
sklearn = SKLearn(entry_point=&quot;train.py&quot;,
                  Framework_version =“1.2-1”，
                  instance_type=“ml.m5.xlarge”，
                  角色=角色，
                  超参数=fixed_hyperparameters
）

之后，我使用刚刚创建的估计器实例化一个 HyperparameterTuner 作业，其中包含我想要测试的超参数范围。
hyperparameters_ranges = {
    “n_estimators”: ContinuousParameter(100, 500),
    “学习率”：连续参数（1e-2，1e-1），
    “最大深度”：IntegerParameter(2, 5),
    “子样本”：连续参数（0.6，1），
    “max_df”：连续参数（0.4，1），
    “max_features”：IntegerParameter(5, 25),
    “use_idf”：CategoricalParameter([True, False])
}

度量=“验证：f1”

调谐器 = 超参数调谐器(
    sklearn,
    公制，
    超参数范围，
    最大作业数=2,
    最大并行作业数=2
）

我的问题是，我没有找到任何有关如何访问“train.py”内部 SKLearn 估计器中传递的超参数的信息。文件。我也没有找到最佳超参数存储在哪里，因此我可以将它们用于最终模型。有人可以告诉我这是否可能吗？或者如果有另一种更简单的方法可以提供替代方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    </channel>
</rss>