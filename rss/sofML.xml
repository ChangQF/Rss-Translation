<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 01 Nov 2024 03:31:34 GMT</lastBuildDate>
    <item>
      <title>通过 ART2 对音频信号进行在线聚类</title>
      <link>https://stackoverflow.com/questions/79145752/online-clustering-of-audio-signals-via-art2</link>
      <description><![CDATA[我为 ART2 聚类编写了此代码，但它不起作用。我期望输出的是真实值与预测值的图表。需要帮助找出它不起作用的原因以及我可以做些什么来修复它。此外，需要输出准确率超过 90% 的输出。
这是特征生成的代码：
import librosa
import numpy as np
import math
import pandas as pd
from art2 import Art2

def load_audio(file_path):
# 加载音频文件
audio_signal, sample_rate = librosa.load(
file_path, sr=None) # 使用原始采样率
return audio_signal, sample_rate

def extract_features(audio_signal, sample_rate, n_fft):
mfccs = librosa.feature.mfcc(
y=audio_signal, sr=sample_rate, n_mfcc=13, n_fft=n_fft)
mfsc = librosa.feature.melspectrogram(
y=audio_signal, sr=sample_rate, n_mels=40, n_fft=n_fft)
return mfccs, mfsc

def divide_into_frames(audio_signal, frame_size, sample_freq, override):
# 将 frame_size ms 转换为样本
frame_length = int(frame_size * sample_freq / 1000)
hop_size = math.floor(frame_length * (1 - override))
# 创建重叠帧
frames = librosa.util.frame(
audio_signal, frame_length=frame_length, hop_length=hop_size)
return frames.T # 转置以使帧作为行

def main():

# 配置变量
mp3_file = &quot;dataset/trimmed_crowd_talking.mp3&quot;
frame_size = 10 # 每帧的样本数
overlap = 0.1 # 下一帧要移动的样本数

n_fft = 256
# 加载音频并创建帧
audio_signal, sample_rate = load_audio(mp3_file)
frames = divide_into_frames(audio_signal, frame_size, sample_rate, override)

mfccs_list = []
mfsc_list = []

for frame in frames:
mfccs, mfsc = extract_features(frame, sample_rate, n_fft)
mfccs_list.append(mfccs.flatten()) # 展平为 1D 数组
mfsc_list.append(mfsc.flatten()) # 展平为 1D 数组

# 创建 DataFrame
index = np.arange(len(mfccs_list))
# 计算以秒为单位的时间戳
timestamps = (index * (len(frames[0]) / sample_rate)).round(2)
df = pd.DataFrame(mfccs_list, index=index)
df.columns = [f&#39;MFCC_{i}&#39; for i in range(df.shape[1])]

# 添加时间戳
df.insert(0, &#39;Index&#39;, index)
df.insert(1, &#39;Timestamp&#39;, timestamps)

# 添加 MFSC 作为附加列
mfsc_df = pd.DataFrame(mfsc_list, index=index)
mfsc_df.columns = [f&#39;MFSC_{i}&#39; for i in range(mfsc_df.shape[1])]
combined_df = pd.concat([df, mfsc_df], axis=1)

# 保存为 CSV
#combined_df.to_csv(&quot;output.csv&quot;, index=False)

print(combined_df.head())

if __name__ == &quot;__main__&quot;:
main()

这是 ART2 Clustering 的代码：
class Art2:
def __init__(self, max_clusters, vigilance_threshold, creation_buffer_size):
self.max_clusters = max_clusters
self.vigilance_threshold = vigilance_threshold
self.creation_buffer_size = creation_buffer_size
self.num_clusters = 0
self.cluster_weights = []
self.cluster_creation_buffer = 0
return

# 计算到已经存在的簇的曼哈顿距离
def distance_2_clusters(self, input_features):
distances = np.sum(
np.abs(self.cluster_weights - input_features), axis=1)
返回距离

def process_new_sample(self, input_features):
if self.num_clusters &gt; 0：
距离 = self.distance_2_clusters（输入特征）

best_cluster_index = np.argmin（距离）
best_distance = distances[best_cluster_index]

如果 best_distance &lt;= self.vigilance_threshold：
self.update_cluster（best_cluster_index，输入特征）
self.cluster_creation_buffer = 0
否则：
self.cluster_creation_buffer += 1

如果 self.cluster_creation_buffer &gt;= self.creation_buffer_size：
如果 self.num_clusters &lt; self.max_clusters:
self.create_new_cluster(input_features)
else:
print(&quot;已达到最大簇数。&quot;)
self.cluster_creation_buffer = 0

def create_new_cluster(self, input_features):
self.cluster_weights.append(input_features)
self.num_clusters += 1

def update_cluster(self, cluster_index, input_features):
# 简单平均权重更新；您可能希望使用更复杂的方法
self.cluster_weights[cluster_index] = (
self.cluster_weights[cluster_index] + input_features
) / 2

def cluster_data_as_stream():
return

我期望得到准确率超过 90% 的地面实况与预测值之间的图表。]]></description>
      <guid>https://stackoverflow.com/questions/79145752/online-clustering-of-audio-signals-via-art2</guid>
      <pubDate>Thu, 31 Oct 2024 17:23:28 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据帧进行分组以获取代表更大集合的全部范围的子集</title>
      <link>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</link>
      <description><![CDATA[这是我拥有的一组数据框的两个示例：



天
p1
p2
p3




4
2.1
3.4
4.5


15
2.2
3.6
2.8


39
2.5
2.1
0.4



和这个：



天
p1
p2
p3




4
2.1
3.4
4.5


18
8.2&lt; /td&gt;
2.2
5.8


22
6.4
3.6
1.4


29
2.4
4.1
2.3



我有大约 100 万个这样的数据框（列相同，长度不同），我想输出大约 50000 个子集，以公平地代表所有存在的不同数据框。基本上，数据框应该是一个有效的表示，因此在完整的 100 万或 50k 子集上训练 ML 模型应该会给 ML 模型带来几乎相同的行为。
天数很重要，因为 2 个具有相同参数 (p) 值但天数列差异很大的数据框是不相等的
我的方法是通过每个级别的变量将数据框分组在一起。然后从底层的每个组中取出 1 个数据框。
组级别 1 (GL1)：按行数对数据框进行分组。
组级别 2 (GL2)：对于 GL1 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 对具有相似天数列的数据框进行分组
组级别 3 (GL3)：对于 GL2 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 将具有相似参数值的数据框分组在一起
从每个 GL3 组中取出 1 个数据框来表示该组数据框。
它可能无法获得每个参数的完整最大值和最小值，但这种方法似乎相当全面。这是一个好主意还是您有更好的想法？]]></description>
      <guid>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</guid>
      <pubDate>Thu, 31 Oct 2024 16:36:13 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 yolo 自定义模型时，data.yaml 文件中的相对路径出现问题</title>
      <link>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</link>
      <description><![CDATA[我正在尝试创建一个训练管道，以使用用户输入的带标签图像来训练自定义 yolov9 模型。
我遇到一个问题，如果我让我的 data.yaml 文件使用相对路径，我会收到错误：
RuntimeError：数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”错误
数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”图像未找到，缺少路径“C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val”

更奇怪的是，错误路径提及，
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

不是存在或正在任何地方请求的路径。实际路径是
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

由于某种原因，它重复了路径的第一部分 3 次。
这是 data.yaml 文件：
path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

这是开始训练的代码：
def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
# 指定训练运行的保存目录
save_dir = &#39;OIT_model/customOIT/yolocustomtrainoutput&#39;
if os.path.exists(save_dir):
for file in os.listdir(save_dir):
file_path = os.path.join(save_dir, file)
if os.path.isfile(file_path) or os.path.islink(file_path):
os.unlink(file_path)
elif os.path.isdir(file_path):
shutter.rmtree(file_path)
os.makedirs(save_dir, exist_ok=True)
model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
返回
train_custom_dataset_yolo(&#39;OIT_model/customOIT/customdatasetyolo/data.yaml&#39;, epochs=1,imgsz=64, verbose=True)

然而，非常奇怪的是，当我用绝对路径替换相对路径时，如下所示：
path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

训练没有问题。对于我来说，使用绝对路径不是一个选择，因为这个应用程序需要在其他机器上可重现。]]></description>
      <guid>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</guid>
      <pubDate>Thu, 10 Oct 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>我可以将模型从 MMDetection 提取回 Pytorch 吗？</title>
      <link>https://stackoverflow.com/questions/78927061/can-i-extract-model-from-mmdetection-back-to-pytorch</link>
      <description><![CDATA[我可以从 MMDetection 中提取模型回 Pytorch 吗？
我知道，可以通过 ONNX 进行推理。
也可以进行训练吗？]]></description>
      <guid>https://stackoverflow.com/questions/78927061/can-i-extract-model-from-mmdetection-back-to-pytorch</guid>
      <pubDate>Thu, 29 Aug 2024 09:27:23 GMT</pubDate>
    </item>
    <item>
      <title>如何找到张量的前 $n$ 个最大值的索引？</title>
      <link>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</link>
      <description><![CDATA[我知道 torch.argmax(x, dim = 0) 返回 x 中沿维度 0 的第一个最大值的索引。但是有没有一种有效的方法来返回前 n 个最大值的索引？如果有重复的值，我也想要 n 个索引中的那些值的索引。
举一个具体的例子，比如 x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])。我想要一个函数
generalized_argmax(xI torch.tensor, n: int)

这样
generalized_argmax(x, 4)
在此示例中返回 [0, 2, 4, 5]。]]></description>
      <guid>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</guid>
      <pubDate>Thu, 01 Feb 2024 11:05:23 GMT</pubDate>
    </item>
    <item>
      <title>带有值向量的回归模型的 pytorch 损失函数</title>
      <link>https://stackoverflow.com/questions/68370248/pytorch-loss-function-for-regression-model-with-a-vector-of-values</link>
      <description><![CDATA[我正在训练 CNN 架构以使用 PyTorch 解决回归问题，其中我的输出是一个包含 25 个值的张量。输入/目标张量可以是全零，也可以是 sigma 值为 2 的高斯分布。4 个样本批次的示例如下：
[[0.13534, 0.32465, 0.60653, 0.8825, 1.0000, 0.88250,0.60653, 0.32465, 0.13534, 0.043937, 0.011109, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0., 0., 0.13534, 0.32465, 0.60653, 0.8825, 1.0000, 0.88250,0.60653, 0.32465, 0.13534, 0.043937, 0.011109, 0., 0., 0., 0., 0., 0., 0., 0.], 0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.、0.13534、0.32465、0.60653、0.8825、1.0000、 0.88250,0.60653, 0.32465, 0.13534 ],
[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]

我的问题是如何为模型设计一个损失函数，有效地学习具有 25 个值的回归输出。
我尝试了两种类型的损失，torch.nn.MSELoss() 和 torch.nn.MSELoss()-torch.nn.CosineSimilarity()。它们有点管用。但是，有时网络很难收敛，特别是当有大量样本全为“零”时，这会导致网络输出一个包含所有 25 个小值的向量。
我的问题是，还有其他我们可以尝试的损失吗？]]></description>
      <guid>https://stackoverflow.com/questions/68370248/pytorch-loss-function-for-regression-model-with-a-vector-of-values</guid>
      <pubDate>Tue, 13 Jul 2021 23:15:32 GMT</pubDate>
    </item>
    <item>
      <title>具有元数据的时间序列自动编码器</title>
      <link>https://stackoverflow.com/questions/67141033/time-series-autoencoder-with-metadata</link>
      <description><![CDATA[我正在尝试构建一个自动编码器来检测时间序列数据中的异常。
我的方法基于本教程：https://keras.io/examples/timeseries/timeseries_anomaly_detection/
但我的数据比这个简单的教程更复杂。
我有两个不同的时间序列，来自两个传感器和一些元数据，例如从哪台机器记录了时间序列。
使用普通的 MLP 网络，您可以为时间序列建立一个网络，为元数据建立一个网络，并将它们合并到更高的层中。但是你如何将这些数据用作自动编码器的输入呢？
你有什么想法、教程链接或我没有找到的论文吗？]]></description>
      <guid>https://stackoverflow.com/questions/67141033/time-series-autoencoder-with-metadata</guid>
      <pubDate>Sat, 17 Apr 2021 17:30:19 GMT</pubDate>
    </item>
    <item>
      <title>多变量时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/64720842/multivariate-time-series-anomaly-detection</link>
      <description><![CDATA[我有一个时间序列数据，类似于下面的示例数据。如您所见，我有四个事件以及不同小时内每个事件的总发生次数。
一小时后，我将获得每个事件的新发生次数，因此我想根据其历史水平判断该事件的发生次数是否异常。
我认为如果我为每个事件构建四个不同的回归很容易，但在现实生活中，我可能会有很多事件，这会降低效率，所以我想知道解决这个问题的最佳方法是什么？我应该尝试什么模型？我读过关于 KNN 的文章，但它不需要分类标签，而我的情况没有？]]></description>
      <guid>https://stackoverflow.com/questions/64720842/multivariate-time-series-anomaly-detection</guid>
      <pubDate>Fri, 06 Nov 2020 19:55:46 GMT</pubDate>
    </item>
    <item>
      <title>我的图像字幕模型为我在所有图像上提供相同的字幕[关闭]</title>
      <link>https://stackoverflow.com/questions/60434487/my-image-captioning-model-giving-me-same-caption-on-all-images</link>
      <description><![CDATA[我正在做一个与医学图像字幕相关的项目。我使用的代码来自此链接。
我正在使用印第安纳射线照片数据集并使用结果作为训练字幕。我训练成功，损失值为 0.75。但是我的最终模型为我检查过的所有图像提供了相同的标题（有些人也面临同样的问题。请查看此链接的评论）。
您能否建议我对代码的任何部分或其他任何内容进行任何更改，以便它开始为我将检查的每张图片提供适当的标题。]]></description>
      <guid>https://stackoverflow.com/questions/60434487/my-image-captioning-model-giving-me-same-caption-on-all-images</guid>
      <pubDate>Thu, 27 Feb 2020 13:34:38 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中实现的vgg16的训练loss并没有减少</title>
      <link>https://stackoverflow.com/questions/57605094/the-training-loss-of-vgg16-implemented-in-pytorch-does-not-decrease</link>
      <description><![CDATA[我想在 pytorch 中尝试一些小例子，但训练损失在训练中并没有减少。
这里提供了一些信息：

模型是 vgg16，由 13 个卷积层和 3 个密集层组成。
数据是 pytorch 中的 cifar100。
我选择交叉熵作为损失函数。

代码如下
# encoding: utf-8

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn. functional as F
import torchvision.transforms as transforms
import torchvision

import numpy as np

class VGG16(torch.nn.Module):
def __init__(self, n_classes):
super(VGG16, self).__init__()

# 构建模型
self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)
self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)
self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)
self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)
self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)
self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)
self.conv3_3 = nn.Conv2d(256, 256, 3, 填充=1)
self.conv4_1 = nn.Conv2d(256, 512, 3, 填充=1)
self.conv4_2 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv4_3 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_1 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_2 = nn.Conv2d(512, 512, 3, 填充=1)
self.conv5_3 = nn.Conv2d(512, 512, 3, 填充=1)

self.fc6 = nn.Linear(512, 512)
self.fc7 = nn.Linear(512, 512)
self.fc8 = nn.Linear(512, n_classes)

def forward(self, x):
x = F.relu(self.conv1_1(x))
x = F.relu(self.conv1_2(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv2_1(x))
x = F.relu(self.conv2_2(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv3_1(x))
x = F.relu(self.conv3_2(x))
x = F.relu(self.conv3_3(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv4_1(x))
x = F.relu(self.conv4_2(x))
x = F.relu(self.conv4_3(x))
x = F.max_pool2d(x, (2, 2))

x = F.relu(self.conv5_1(x))
x = F.relu(self.conv5_2(x))
x = F.relu(self.conv5_3(x))
x = F.max_pool2d(x, (2, 2))

x = x.view(-1, self.num_flat_features(x))

x = F.relu(self.fc6(x))
x = F.relu(self.fc7(x))
x = self.fc8(x)
返回x

def num_flat_features(self, x):
size = x.size()[1:]

num_features = 1
for s in size:
num_features *= s
return num_features

if __name__ == &#39;__main__&#39;:

BATCH_SIZE = 128
LOG_INTERVAL = 5

# 数据
transform = transforms.Compose([
transforms.ToTensor()
])

trainset = torchvision.datasets.CIFAR100(
root=&#39;./data&#39;,
train=True,
download=True,
transform=transform
)

testset = torchvision.datasets.CIFAR100(
root=&#39;./data&#39;,
train=False,
download=True,
transform=transform
)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

# 模型
vgg16 = VGG16(100)
vgg16.cuda()

# 优化器
optimizer = optim.SGD(vgg16.parameters(), lr=0.01)

# 损失
criterion = nn.CrossEntropyLoss()

print(&#39;———— Train Start —————&#39;)
for epoch in range(20):
running_loss = 0.
for step, (batch_x, batch_y) in enumerate(trainloader):
batch_x, batch_y = batch_x.cuda(), batch_y.cuda()

# 
o​​ptimizer.zero_grad()

output = vgg16(batch_x)
loss =标准（输出，batch_y）
损失。向后（）
优化器。步骤（）

running_loss + = loss.item（）

如果步骤 % LOG_INTERVAL == 0：
打印（&#39;[%d，%4d] 损失：%.4f&#39; % (epoch，步骤，running_loss / LOG_INTERVAL))
running_loss = 0。

def test（）：
打印（&#39;———— 测试开始 ————&#39;）
正确 = 0
总计 = 0

# 
使用 torch.no_grad（）：
对于 test_x，test_y 在 testloader 中：
图像，标签 = test_x.cuda（），test_y.cuda（）
输出 = vgg16（图像）
_，预测 = torch.max（输出数据，1）
总计 + = 标签。大小（0）
正确 + = (预测 == 标签).sum（）。item（）

准确率 = 100 * 正确 / 总计
print(&#39;网络准确率为：%.4f %%&#39; % 准确率)
print(&#39;———— 测试完成 ————&#39;)

test()

print(&#39;———— 训练完成 —————&#39;)


损失保持在 4.6060 左右，从未减少。我尝试了不同的学习率，但没有效果。]]></description>
      <guid>https://stackoverflow.com/questions/57605094/the-training-loss-of-vgg16-implemented-in-pytorch-does-not-decrease</guid>
      <pubDate>Thu, 22 Aug 2019 08:27:53 GMT</pubDate>
    </item>
    <item>
      <title>如何清理图像以便与 MNIST 训练模型一起使用？</title>
      <link>https://stackoverflow.com/questions/57000160/how-to-clean-images-to-use-with-a-mnist-trained-model</link>
      <description><![CDATA[我正在创建一个机器学习模型，用于对数字图像进行分类。我使用内置的 tf.keras.datasets.mnist 数据集，通过 Tensorflow 和 Keras 训练了该模型。该模型与来自 mnist 数据集本身的测试图像配合得很好，但我想给它提供我自己的图像。我提供给该模型的图像是从 Captcha 中提取的，因此它们将遵循类似的模式。我在此公共 Google Drive 文件夹中包含了一些图像示例。当我输入这些图像时，我注意到模型不是很准确，我对原因有一些猜测。

图像的背景在图片中产生了太多噪音。
数字不居中。
图像不严格遵循 MNIST 训练集的颜色格式（黑底白字）。

我想问一下如何去除背景并将其居中，以便减少图像中的噪音，从而实现更好的分类。
这是我正在使用的模型：
import tensorflow as tf
from tensorflow import keras

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

class Stopper(keras.callbacks.Callback):
def on_epoch_end(self, epoch, log={}):
if log.get(&#39;acc&#39;) &gt;= 0.99:
self.model.stop_training = True
print(&#39;\n达到 99% 准确率。停止训练...&#39;)

model = keras.Sequential([
keras.layers.Flatten(),
keras.layers.Dense(1024,activation=tf.nn.relu),
keras.layers.Dense(10,activation=tf.nn.softmax)])

model.compile(
optimizer=tf.train.AdamOptimizer(),
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

x_train, x_test = x_train / 255, x_test / 255

model.fit(x_train, y_train, epochs=10, callbacks=[Stopper()])

下面是我将图像导入 tensorflow 的方法：
from PIL import Image
img = Image.open(&quot;image_file_path&quot;).convert(&#39;L&#39;).resize((28, 28), Image.ANTIALIAS)
img = np.array(img)
model.predict(img[None,:,:])

我还从 MNIST 数据集中引入了一些示例 此处。我想要一个脚本，将我的图像尽可能接近 MNIST 数据集格式。此外，由于我必须对无限数量的图像执行此操作，如果您能提供一种完全自动化的转换方法，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/57000160/how-to-clean-images-to-use-with-a-mnist-trained-model</guid>
      <pubDate>Fri, 12 Jul 2019 04:25:47 GMT</pubDate>
    </item>
    <item>
      <title>传感器异常检测的最新进展</title>
      <link>https://stackoverflow.com/questions/43565003/state-of-art-for-sensors-anomaly-detection</link>
      <description><![CDATA[我正在研究异常检测问题。我有一个传感器，用于记录偶发性时间序列数据。例如，传感器偶尔会激活 10 秒，并以毫秒为间隔记录值。我的任务是确定记录的模式是否不正常。换句话说，我需要检测该模式与其他记录模式相比的异常情况。
最先进的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/43565003/state-of-art-for-sensors-anomaly-detection</guid>
      <pubDate>Sat, 22 Apr 2017 21:48:12 GMT</pubDate>
    </item>
    <item>
      <title>如何获取 OpenNLP 模型的训练数据集？</title>
      <link>https://stackoverflow.com/questions/42003560/how-to-get-training-dataset-of-opennlp-models</link>
      <description><![CDATA[我正在使用以下 OpenNLP 模型：
en-parser-chunking.bin
en-ner-person.bin
en-ner-location.bin
en-ner-organization.bin

我想将我的数据附加到这些模型所用的训练数据集中。请告诉我从哪里可以获取原始数据集？]]></description>
      <guid>https://stackoverflow.com/questions/42003560/how-to-get-training-dataset-of-opennlp-models</guid>
      <pubDate>Thu, 02 Feb 2017 13:32:35 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow - 任何输入都会给我相同的输出[关闭]</title>
      <link>https://stackoverflow.com/questions/39540806/tensorflow-any-input-gives-me-same-output</link>
      <description><![CDATA[我遇到了一个非常奇怪的问题，我正在使用 tensorflow 构建 RNN 模型，然后在完成训练后使用 tf.Saver 存储模型变量（全部）。
在测试期间，我只需再次构建推理部分并将变量恢复到图中。恢复部分不会给出任何错误。
但是当我开始在评估测试上进行测试时，我总是从推理中获得相同的输出，即对于所有测试输入，我都会得到相同的输出。
我在训练期间打印了输出，我确实看到不同训练样本的输出不同，并且成本也在降低。
但是当我进行测试时，无论输入是什么，它总是给我相同的输出。
有人能帮我理解为什么会发生这种情况吗？我想发布一些最小的例子，但由于我没有收到任何错误，我不确定我应该在这里发布什么。如果能帮助解决问题，我很乐意分享更多信息。
训练和测试期间的推理图之间的一个区别是 RNN 中的时间步数。在训练期间，我在更新梯度之前对一个批次进行 n 步（n = 20 或更多）训练，而对于测试，我只使用一个步骤，因为我只想预测该输入。]]></description>
      <guid>https://stackoverflow.com/questions/39540806/tensorflow-any-input-gives-me-same-output</guid>
      <pubDate>Fri, 16 Sep 2016 22:13:32 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 R 中的决策树规则</title>
      <link>https://stackoverflow.com/questions/23961445/how-to-count-decision-tree-rules-in-r</link>
      <description><![CDATA[我使用 RPart 构建决策树。我这样做没有问题。但是，我需要了解（或计算）树被拆分了多少次？我的意思是，树有多少条规则（if-else 语句）？
例如：
 X
- - 
if (a&lt;9)- - if(a&gt;=9)
Y H
-
if(b&gt;2)- 
Z

有 3 条规则。 
当我写 summary(model):

summary(model_dt)

调用：
rpart(formula = Alert ~ ., data = train)
n= 18576811 

CP nsplit rel error xerror xstd
1 0.9597394 0 1.00000000 1.00000000 0.0012360956
2 0.0100000 1 0.04026061 0.05290522 0.0002890205

变量重要性
ip.src frame.protocols tcp.flags.ack tcp.flags.reset frame.len 
20 17 17 17 16 
ip.ttl 
` 12 

节点号 1：18576811 个观测值，复杂度参数 = 0.9597394
预测类别 = 是 预期损失 = 0.034032 P（节点） = 1
类别计数：632206 1.79446e+07
概率：0.034 0.966 
左子 = 2（627091 个观测值）右子 = 3（17949720 个观测值）
主要分割：
ip.src 分割为 LLLLLLLRRRLLRR ............ LLRLRLRRRRRRRRRRRRRRRRRR
改进 = 1170831.0，（0 个缺失）

ip.dts 分割为 LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL， improve=1013082.0，（0 缺失）
tcp.flags.ctl &lt; 1.5 向右，improve=1007953.0，（2645 缺失）
tcp.flags.syn &lt; 1.5 向右，improve=1007953.0，（2645 缺失）
frame.len &lt; 68 向右，improve= 972871.3，（30 缺失）
代理拆分：
frame.protocols 拆分为 LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL，agree=0.995，adj=0.841，（0 拆分）
tcp.flags.ack &lt; 1.5 向右，agree=0.994，adj=0.836，（0 拆分）
tcp.flags.reset &lt; 1.5 向右，同意=0.994，调整=0.836，（0 分割）
frame.len &lt; 68 向右，同意=0.994，调整=0.809，（0 分割）
ip.ttl &lt; 230.5 向右，同意=0.987，adj=0.612，（0 分割）

节点号 2：627091 个观测值
预测类别=否 预期损失=0.01621615 P（节点）=0.03375666
类别计数：616922 10169
概率：0.984 0.016

节点号 3：17949720 个观测值
预测类别=是 预期损失=0.0008514896 P（节点）=0.9662433
类别计数：15284 1.79344e+07
概率：0.001 0.999

如果有人帮助我理解它，我会感激不尽
诚挚的
Eray]]></description>
      <guid>https://stackoverflow.com/questions/23961445/how-to-count-decision-tree-rules-in-r</guid>
      <pubDate>Fri, 30 May 2014 18:46:27 GMT</pubDate>
    </item>
    </channel>
</rss>