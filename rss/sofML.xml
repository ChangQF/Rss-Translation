<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 16 Sep 2024 09:20:24 GMT</lastBuildDate>
    <item>
      <title>使用 pytesseract OCR 检测“数字单位”格式的文本，将数字转换为浮点数时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78989261/detect-text-in-format-number-unit-using-pytesseract-ocr-valueerror-on-converti</guid>
      <pubDate>Mon, 16 Sep 2024 07:49:42 GMT</pubDate>
    </item>
    <item>
      <title>CRNN 实现不兼容的形状：[32] vs. [32,29] [关闭]</title>
      <link>https://stackoverflow.com/questions/78987265/crnn-implementation-incompatible-shapes-32-vs-32-29</link>
      <description><![CDATA[我正在尝试训练一个从图像中读取值和单位的 ml 模型。我正在尝试实现基于 CRNN 的架构，但无法弄清楚为什么会出现此错误。任何帮助都将不胜感激。
这是笔记本文本的链接&gt;]]></description>
      <guid>https://stackoverflow.com/questions/78987265/crnn-implementation-incompatible-shapes-32-vs-32-29</guid>
      <pubDate>Sun, 15 Sep 2024 11:29:58 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 MarianMT 中的单词翻译问题？[关闭]</title>
      <link>https://stackoverflow.com/questions/78986346/how-to-overcome-the-issue-of-single-word-translation-in-marianmt</link>
      <description><![CDATA[我认真思考了 HuggingFace 中 MarianMT 模型的单词翻译问题。我目前正在开发用于翻译的 Telegram 机器人。为此，我选择了 MarianMT 模型。作为训练数据集，我选择了 Europarl 著名的并行语料库，它支持不同的语言，并且以正式的风格编写。
现在，我想解释一下我做了什么以及我遇到了哪些问题：
首先，我想描述一下我使用了哪些技术。编程语言是 Python3，深度学习框架称为 PyTorch。正如我上面提到的，模型是 MarianMT。我使用不同版本的 MarianMT 来处理多种语言，例如法语、英语、德语等。
其次，我想描述一下我的问题：
问题是，当我使用英德语言模型时，它无法正确翻译或根本不翻译输入的单词，并显示“抱歉，但尚不支持此语言的翻译”。但是，如果我用德语输入相同的单词，它会正确翻译该单词。此外，城市、国家等命名实体也存在问题。例如，如果我用英语输入 City of Düsseldorf is the capital of the state of NRW，模型将产生类似这样的结果：
City of485 ist die Hauptstadt des Bundesstaates Houston

这真的很差劲，而且不正确。
在翻译诸如汽车、黄油、乌克兰、丹麦、操场等与国家、城市、主题甚至有时是动作有关的类似词语时，它也会失败。
第三，我想定义我使用的模型和参数：
Helsinki-NLP/opus-mt-en-de 用于英语-德语和德语-英语翻译 
Helsinki-NLP/opus-mt-en-fr 用于英语-法语翻译
Helsinki-NLP/opus-mt-fr-en 用于法语-英语翻译 

现在介绍训练的参数：
框架：PyTorch 最新版 + Google Colab Pro 
编程语言：Python 3.10 
数据集：Europarl paralell corpora 
epoch 数：2 
损失函数：稀疏分类交叉熵 
优化器：Adam 
学习率：0.0001 
批次大小：32 

那么我该如何克服某些单词或句子根本没有翻译或翻译不正确的问题呢？]]></description>
      <guid>https://stackoverflow.com/questions/78986346/how-to-overcome-the-issue-of-single-word-translation-in-marianmt</guid>
      <pubDate>Sat, 14 Sep 2024 23:31:14 GMT</pubDate>
    </item>
    <item>
      <title>使用单一特征训练模型同时使用观察权重是否正确？[关闭]</title>
      <link>https://stackoverflow.com/questions/78985636/is-it-correct-to-train-a-model-with-a-single-feature-while-also-using-observati</link>
      <description><![CDATA[使用单个特征（在本例中为距离）同时使用观测权重来训练模型是否正确？
我正在尝试训练一个机器学习模型，其中唯一的输入特征是距离。此外，我有一个表示观测权重的权重列，以及一个指示距离是高还是低的目标变量（这是我想要预测的目标）。
仅使用一个特征（在本例中为距离）来训练模型是否合适？此外，在处理此类加权数据时，您会推荐哪种类型的模型？
我尝试使用逻辑回归和随机森林等模型，仅将距离作为输入特征。我希望模型能够学习距离与目标变量（高或低）之间的关系。此外，我应用权重来赋予某些观测值更重要的意义，但我不确定这是最有效的方法还是其他模型在加权数据方面的表现会更好。]]></description>
      <guid>https://stackoverflow.com/questions/78985636/is-it-correct-to-train-a-model-with-a-single-feature-while-also-using-observati</guid>
      <pubDate>Sat, 14 Sep 2024 16:34:26 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface Pretrained 中 device_map = "auto" 的替代方案</title>
      <link>https://stackoverflow.com/questions/78985137/alternative-to-device-map-auto-in-huggingface-pretrained</link>
      <description><![CDATA[我有一个从 huggingface 读取的模型，使用以下代码：
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

现在我读取了模型，并对内部层做了一些修改，并添加了更多层。当我开始训练/微调时，我发现并非所有东西都在同一个模型上。
现在经过更多调查，我发现我的自定义层没有像原始模型那样分布在多个 GPU 上。因此我需要类似 device_map=&quot;auto&quot; 的内容，但在读取模型之后。
因此只需类似
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

model.device_map = &quot;auto&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/78985137/alternative-to-device-map-auto-in-huggingface-pretrained</guid>
      <pubDate>Sat, 14 Sep 2024 12:42:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在稳定的扩散修复模型中提高对象生成质量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78984494/how-to-improve-object-generation-quality-in-a-stable-diffusion-inpainting-model</link>
      <description><![CDATA[我目前正在微调一个稳定扩散修复模型，以完成一项特定任务：在图像中生成新对象。但是，我面临着几个挑战：

生成不一致：有时模型根本无法生成任何对象。

低质量输出：当模型确实生成对象时，质量通常不令人满意。


采取的步骤：

我一直在使用负面提示来阻止模型产生低质量的输出。
我已经收集并准备了一个数据集，其中包括高质量图像和应生成对象的区域的相应蒙版。

问题：

我可以采用哪些策略来提高生成对象的质量？
是否有特定的超参数或训练技术可以帮助提高模型在此任务中的性能？
如何有效地使用负面提示来引导模型而不导致生成不足？
是否有任何推荐的数据集或资源专注于图像中的对象生成，我应该考虑？

我正在使用 RealVisXL Inpaint 和 Hugging Face。
我当前的设置是 V100。]]></description>
      <guid>https://stackoverflow.com/questions/78984494/how-to-improve-object-generation-quality-in-a-stable-diffusion-inpainting-model</guid>
      <pubDate>Sat, 14 Sep 2024 07:06:23 GMT</pubDate>
    </item>
    <item>
      <title>roboflow 教程使用 albumentations：TypeError：图像必须是 numpy 数组类型</title>
      <link>https://stackoverflow.com/questions/78984257/roboflow-tutorial-using-albumentations-typeerror-image-must-be-numpy-array-typ</link>
      <description><![CDATA[更新
从 3.12.4 切换到 python 3.10.14 并解决了该问题。
在 Mac 上使用 conda venv 运行
albumentations 1.4.15
opencv 4.10.0
Original
我正在尝试使用 albumentations 调整图像大小，并偶然发现了这个 roboflow 教程 并准确复制了代码。
import albumentations as A
import cv2

image = cv2.imread(&quot;img.jpg&quot;)

pipeline = A.Compose([
A.Resize(height=640, width=640, p=1),
])

augmented_image = pipeline(image=image)[&quot;image&quot;]

但是，当我运行它时，我得到了以下信息错误：
回溯（最近一次调用）：
文件“/Users/user/Documents/test.py”，第 16 行，位于 &lt;module&gt;
augmented_image = pipeline(image=image)[&quot;image&quot;]
^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/Users/user/anaconda3/envs/venv/lib/python3.12/site-packages/albumentations/core/composition.py&quot;，第 334 行，在 __call__ 中
self.preprocess(data)
文件 &quot;/Users/user/anaconda3/envs/venv/lib/python3.12/site-packages/albumentations/core/composition.py&quot;，第 363 行，在 preprocess 中
self._check_args(**data)
文件 &quot;/Users/user/anaconda3/envs/venv/lib/python3.12/site-packages/albumentations/core/composition.py&quot;，第 412 行，在_check_args
引发 TypeError(f&quot;{data_name} 必须是 numpy 数组类型&quot;)
TypeError: 图像必须是 numpy 数组类型

我已使用 type(image) 确认它是 &lt;class &#39;numpy.ndarray&#39;&gt;
我也尝试过不同的图像以及 jpg 和 png
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78984257/roboflow-tutorial-using-albumentations-typeerror-image-must-be-numpy-array-typ</guid>
      <pubDate>Sat, 14 Sep 2024 03:55:15 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型置信度问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/78983303/ml-model-confidence-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78983303/ml-model-confidence-issue</guid>
      <pubDate>Fri, 13 Sep 2024 18:07:05 GMT</pubDate>
    </item>
    <item>
      <title>Ray 自定义环境渲染</title>
      <link>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</link>
      <description><![CDATA[我正在创建自己的 gym 环境来测试 freeze-tag 问题。我正在尝试使用 Ray 来做 MAPPO。我有两个问题：
1：我的模拟没有渲染
2：它创建了多个 PyGame 窗口
我已将渲染方法和训练脚本的片段附加到附件中。
# 渲染函数
def render(self):
self.screen.fill((255, 255, 255))

for agent in self.all_agents:
if agent.status == 1:
pygame.draw.circle(self.screen, agent.color, (agent.x, agent.y), agent.size)

elif agent.status == 0:
pygame.draw.circle(self.screen, (0, 255, 255), (agent.x, agent.y),agent.size)

pygame.display.flip()

# Train_MAPPO_FTP.py
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
import gym_FTP as e
import pygame
import numpy as np

# 环境创建函数
def env_creator(config):
robots = 5
adversaries = 2
time_steps = 500

screen = pygame.display.set_mode([1000, 1000])
gym_ftp = e.gym_FTP(screen, robots, 0, adversaries, time_steps, 15)
return gym_ftp

def train_and_evaluate(time_steps):
# 初始化 Ray
ray.init(ignore_reinit_error=True)

# 注册环境
register_env(&quot;Env_FTP&quot;, env_creator)

# create_env_on_local_worker = True
# 配置算法
config = PPOConfig() \
.environment(&quot;Env_FTP&quot;) \
.rollouts(num_rollout_workers=1,
rollout_fragment_length=1,
create_env_on_local_worker=True) \
.training(
train_batch_size=1, # 每次训练更新前汇总经验
sgd_minibatch_size=1,
model={&quot;fcnet_hiddens&quot;: [64, 64]}
) \
.framework(&quot;torch&quot;) \
.evaluation(evaluation_num_workers=1) \
.resources(num_gpus=0) # 设置 GPU 数量

# 构建算法
algo = config.build()

# 参数
episodes = 5
iterations = time_steps / 10

for episode in range(episodes):
for i in range(int(iterations)):
results = algo.train()
print(f&quot;训练迭代 {i + 1} 已完成。mean_reward {results[&#39;episode_reward_mean&#39;]},&quot;
f&quot; 总损失 {results[&#39;info&#39;][&#39;learner&#39;][&#39;__all__&#39;][&#39;total_loss&#39;]}&quot;)

# 关闭 Ray
ray.shutdown()

def main():
time_steps = 500
train_and_evaluate(time_steps)

main()


我已进行多次检查，以测试我的代理的速度是否根据新操作进行更新，以及位置是否正在更新，因此我确定这不是问题所在。当我使用其他算法进行测试时，此环境也有效。我可以正确使用 gym 环境的其他功能，并让它渲染和做一些有趣的事情。这似乎完全是 RAY 的问题。我的目标是拥有 n 个机器人和 m 个对手。我想根据环境状态为 n 个代理获取新操作。我想每集训练 500 个时间步，收集 10 个批次。例如前 10 个时间步，然后再添加 10 个时间步作为经验，然后再添加 10 个。所以我们每集最多更新 50 次。我们将进行 100 集。]]></description>
      <guid>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</guid>
      <pubDate>Wed, 11 Sep 2024 20:54:21 GMT</pubDate>
    </item>
    <item>
      <title>我是否正确地实现了带有反向传播的感知器？</title>
      <link>https://stackoverflow.com/questions/78961133/am-i-implementing-my-perceptron-with-backpropagation-correctly</link>
      <description><![CDATA[我在课堂上学习了感知器以及如何使用反向传播来训练模型。我目前在实施过程中遇到了麻烦，因为它仅能为我提供 50% 的准确率，而班上大多数同学的准确率都达到 90%。我在实施过程中是否忽略了什么？这是我目前从我查看过的资料中得到的结果。
class Perceptron():
def __init__(self, num_features):
self.num_features = num_features
self.weights = np.random.rand(num_features) * 0.1 # 这将创建一个用零填充的数组，形状为 num_features
self.bias = 0.0

def forward(self, x):
linear = np.dot(x, self.weights) + self.bias
print(linear)
predictions = np.where(linear &gt; 0, 1, 0)
return predictions

def behind(self, x, y, predictions):
errors = y - predictions
self.weights += self.learning_rate * np.dot(x.T, errors)
self.bias += self.learning_rate * np.sum(errors)
return errors

def train(self, x, y, epochs, learning_rate = 0.01):
self.learning_rate = learning_rate
for e in range(epochs):
for i in range(y.shape[0]):
x_i, y_i = x[i], y[i]
prediction = self.forward(x_i)
self.backward(x_i, y_i, prediction)

def assess(self, x, y):
predictions = self.forward(x)
accuracy = np.mean(predictions == y)
return accuracy

到目前为止，我已经尝试了不同的学习率，并询问了班上的其他人，说实话，这并没有真正改变我的实施结果。我期望准确率约为 90%，但实际只有 50%。
以下是一些示例数据：
0.77 -1.14 0
-0.33 1.44 0
0.91 -3.07 0
-0.37 -1.91 0
-1.84 -1.13 0
-1.50 0.34 0
-0.63 -1.53​​ 0
-1.08 -1.23 0
0.39 -1.99 0
-1.26 -2.90 0
-5.27 -0.78 0
-0.49 -2.74 0
1.48 -3.74 0
-1.64 -1.96 0
0.45 0.36 0
-1.48   -1.17 0 -2.94 -4.47 0 -2.19 -1.48 0 0.02 -0.02 0 -2.24 -2.12 0 -3.17 -3.69 0 -4.09 1.03 0 -2.41 -2.31 0 -3.45 -0.61 0 -3.96 -2.00 0 -2.95 -1。 16 0 -2.42 -3.35 0 -1.74 -1.10 0 -1.61 -1.28 0 -2.59 -2.21 0 -2.64 -2.20 0 -2.84 -4.12 0 -1.45 -2.26 0 -3.98 -1.05 0 -2.97   -1.63 0 -0.68 -1.52 0 -0.10 -3.43 0 -1.14 -2.66 0 -2.92 -2.51 0 -2.14 -1.62 0 -3.33 -0.44 0 -1.05 -3.85 0 0.38 0.95 0 -0.05 -1.95 0 -3.20 -0 22 0 -2.26 0.01 0 -1.41 -0.33 0 -1.20 -0.71 0 -1.69 0.80 0 -1.52 -1.14 0 3.88 0.65 1 0.73 2.97 1 0.83 3.94 1 1.59    1.25 1 3.92 3.48 1 3.87 2.91 1 1.14 3.91 1 1.73 2.80 1 2.95 1.84 1 2.61 2.92 1 2.38 0.90 1 2.30 3.33 1 1.31 1.85 1 1.56 3. 85 1 2.67 2.41 1 1.23 2.54 1 1.33 2.03 1 1.36 2.68 1 2.58 1.79 1 2.40 0.91 1 0.51 2.44 1 2.17 2.64 1 4.38 2.94 1 1.09 3.12    1 0.68 1.54 1 1.93 3.71 1 1.26 1.17 1 1.90 1.34 1 3.13 0.92 1 0.85 1.56 1 1.50 3.93 1 2.95 2.09 1 0.77 2.84 1 1.00 0.46 1 3.19 2.32 1 2.92 2.32 1 2.86 1.35 1 0.97 2.68 1 1.20 1.31 1 1.54 2.02 1 1.65 0.63 1 1.36 -0.22 1 2.63 0.40 1 0.90 2.05 1
1.26 3.54 1
0.71 2.27 1
1.96 0.83 1
2.52 1.83 1
2.77 2.82 1
4.16 3.34 1

在使用感知器模型之前，此代码首先进行随机化，然后分成2部分：原始数据的2/3用于训练，另外1/3用于测试。之后，对训练和测试数据集的前 2 个特征执行 z 分数标准化。
这是我使用该类的方式：
perceptron = Perceptron(num_features = 2)
perceptron.train(combined_x_train[:, :2], combined_x_train[:, 2], epochs = 5, learning_rate=0.1)
accuracy = perceptron.evaluate(x_train, y_train)
print(f&#39;Final Accuracy: {accuracy * 100:.2f}%&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78961133/am-i-implementing-my-perceptron-with-backpropagation-correctly</guid>
      <pubDate>Sat, 07 Sep 2024 21:05:33 GMT</pubDate>
    </item>
    <item>
      <title>级联分段-通道设置是否正确？</title>
      <link>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</link>
      <description><![CDATA[我想训练一个机器学习模型，用于处理 2D DICOM 图像中的精细蒙版细节。我有 500 张图像准备进行标记/注释。我可以使用这种技术吗？还是我理解错了？
鱼 + 脊椎的注释


我用 1 个类别注释了 500 张图像：鱼。然后我训练一个 model1.pth，将鱼从背景中分割出来。该模型有 2 个 out_channels：鱼和背景。

我再次注释相同的 500 幅图像，但现在有 2 个类别：鱼 + 脊椎。我加载 model1.pth 并创建一个具有 2 个 in_channels 和 3 个 out_channels 的模型：脊椎、鱼和背景，并将模型保存为 model2.pth

最后，我再次注释了 500 幅图像，但现在我包括了变形。如果我有 3 种类型的变形，则每种变形都会有自己的类别。我加载 model2.pth，创建一个具有 3 个 in_channels 和 6 个 out_channels 的模型：背景、鱼、脊柱、deform1、deform2、deform3，并将模型保存为 model3.pth。

该模型现在可以直接用于新图像。这是它的工作原理吗？


背景和细节。我尝试过的方法
目标是找到鱼脊柱中的变形。到目前为止，我已尝试通过使用 MONAI 的 UNet 模型 来分割 3 个类 + 背景。这些图像是转换为 NifTi 格式 (.dcm.nii.gz) 的 2D DICOM 图像，典型尺寸为 2000x900 像素。我使用 3Dslicer 进行注释。类别（到目前为止）：

背景

鱼

脊椎

变形


到目前为止，我已经在（仅）12 张训练图像上进行了测试，只是为了让它运行，我得到了所有 3 个类别的结果，但我猜模型训练过度了。此外，我猜这种技术使得在训练结束后进行微小更改变得更加困难。例如我想要多种不同类型的变形。
我的结果：红线左侧：来自 tensorboard，红线右侧：在新图像上测试模型

在开始注释 500 张图像之前，我想验证我是否走在正确的道路上。我希望通过使用级联技术，我可以获得一个可以轻松分割鱼和脊椎的模型，并且我可以随后尝试不同的变形注释。]]></description>
      <guid>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</guid>
      <pubDate>Thu, 05 Sep 2024 05:34:37 GMT</pubDate>
    </item>
    <item>
      <title>优化欺诈检测不平衡数据的指标</title>
      <link>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</link>
      <description><![CDATA[我需要您的帮助来提高我的模型性能。就像大多数欺诈检测一样，我有一个不平衡的数据集（0.1/0.9）。我想优化目标 1 和 0 的召回率，因为一方面我想避免欺诈检测，另一方面我想限制将非欺诈客户定位为欺诈的成本，因为 5% 的错误分类会使我的收入减少 3000 欧元（而定位正确的欺诈者会让我为检测到的每位客户节省 1000 欧元的损失）。
我的第一个问题是：您会根据这个问题考虑哪些指标？我更关注召回率，但我会阅读您的意见。
第二个问题：我如何提高模型性能？
到目前为止，我在不降低阈值的情况下获得的最佳结果是：
准确率：0.89
混淆矩阵：
[[3153 279]
[ 145 297]]
分类报告：
准确率 召回率 f1 分数 支持
 0 0.96 0.92 0.94 3432
1 0.52 0.67 0.58 442

准确率 0.89 3874

而如果我降低阈值以增加目标 1 的召回率：
准确率：0.61
混淆矩阵：
[[1959 1473]
[ 42 400]]
分类报告：
准确率 召回率 f1 分数 支持率
 0 0.98 0.57 0.72 3432
1 0.21 0.90 0.35 442

准确率 0.61 3874

我尝试了几种模型：
线性回归、XGBoost、随机森林和 SVM
此外，甚至过采样/反采样技术（仅在训练集上）
RandomOverSampling、RandomUnderSampling、SMOTE
您还有其他建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</guid>
      <pubDate>Wed, 08 Nov 2023 10:08:43 GMT</pubDate>
    </item>
    <item>
      <title>如何处理一列包含图像名称、另一列包含图像路径的 csv 文件数据集？</title>
      <link>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我只是在练习模型训练和数据集。我偶然发现了这个数据集，它有测试和训练文件夹。该文件夹中有几个包含不同图像的文件（这是一个乐器数据集，因此每个乐器都按名称分类在不同的文件夹中）。csv 文件包含乐器的名称及其在文件夹中的路径，如下所示：Instrument.csv
现在我的问题是如何处理这个数据集？我应该遍历训练和测试文件夹还是使用这个 csv 文件？
如果我想选择文件夹选项，那么如何遍历每个子文件夹并访问图像？
这是数据集的链接：https://www.kaggle.com/datasets/gpiosenka/musical-instruments-image-classification
如果问题没有任何意义或太容易回答，我很抱歉。我承认我是菜鸟]]></description>
      <guid>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</guid>
      <pubDate>Mon, 27 Jun 2022 14:30:58 GMT</pubDate>
    </item>
    <item>
      <title>机器学习无法预测正确的结果</title>
      <link>https://stackoverflow.com/questions/69469083/machine-learning-not-predicting-correct-results</link>
      <description><![CDATA[我正在创建一个简单的 Python 机器学习脚本，该脚本将根据以下参数预测贷款是否会被批准
商业经验：应大于 7
成立年份：应在 2015 年之后
贷款：没有以前或当前的贷款

如果符合上述条件，则只会批准贷款。该数据集可从此链接下载：
https://drive.google.com/file/d/1QtJ3EED7KDqJDrSHxHB6g9kc5YAfTlmF/view?usp=sharing
对于上述数据，我有以下脚本
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np

data = pd.read_csv(&quot;test2.csv&quot;)
data.head()

X = data[[&quot;Business Exp&quot;, &quot;Year of Founded&quot;, &quot;上一个/当前贷款&quot;]]
Y = data[&quot;OUTPUT&quot;]

clf = LogisticRegression()
clf.fit(X, Y)

test_x2 = np.array([[9, 2017, 0]])
Y_pred = clf.predict(test_x2)
print(Y_pred)

我在 test_x2 中传递测试数据。测试数据是，如果业务 exp 为 9，成立年份为 2017 年，没有当前/以前的贷款，则意味着将提供贷款。因此它应该预测，结果应该是 1，但它显示为 0。代码或数据集是否存在问题。由于我是机器学习的初学者，并且仍在学习它，因此我创建了这个自定义数据集以供我自己理解。]]></description>
      <guid>https://stackoverflow.com/questions/69469083/machine-learning-not-predicting-correct-results</guid>
      <pubDate>Wed, 06 Oct 2021 16:07:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 计算累积分布函数 (CDF)</title>
      <link>https://stackoverflow.com/questions/24788200/calculate-the-cumulative-distribution-function-cdf-in-python</link>
      <description><![CDATA[如何在 Python 中计算累积分布函数 (CDF)？
我想从我拥有的点数组（离散分布）来计算它，而不是使用例如 scipy 所具有的连续分布。]]></description>
      <guid>https://stackoverflow.com/questions/24788200/calculate-the-cumulative-distribution-function-cdf-in-python</guid>
      <pubDate>Wed, 16 Jul 2014 18:36:16 GMT</pubDate>
    </item>
    </channel>
</rss>