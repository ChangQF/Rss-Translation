<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 06 Jan 2025 21:15:38 GMT</lastBuildDate>
    <item>
      <title>如何改进我的线性回归模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</link>
      <description><![CDATA[今天终于完成了线性回归的 Scratch 实现
filepath = f&quot;{path}/Food_Delivery_Times.csv&quot;
df = pd.read_csv(filepath)
df.head()

print(df.columns)
df.isnull().sum()

df.dropna(inplace = True , thresh=4)
df.drop_duplicates(inplace= True)
df[&#39;Courier_Experience_yrs&#39;] = df[&#39;Courier_Experience_yrs&#39;].interpolate()
columns_to_fill = [&#39;Weather&#39;, &#39;Traffic_Level&#39;, &#39;Time_of_Day&#39;]
for col in columns_to_fill:
mode_value = df[col].mode()[0]
df[col]= df[col].fillna(mode_value)
df.info()

print(df[&#39;Weather&#39;].unique())
print(df[&#39;Traffic_Level&#39;].unique())
print(df[&#39;Time_of_Day&#39;].unique())
print(df[&#39;Vehicle_Type&#39;].unique())

def onehot(df,column):
values = df[column].unique()
for val in values:
df[val] = (df[column] == val).astype(int)
return df

df = onehot(df,&#39;Weather&#39;)
df = onehot(df,&#39;Vehicle_Type&#39;)
df = onehot(df,&#39;Time_of_Day&#39;)
traffic_mapping = {&#39;低&#39;:0,&#39;中&#39;:1,&#39;高&#39;:2}
df[&#39;Traffic_encoded&#39;] = df[&#39;Traffic_Level&#39;].map(traffic_mapping)
df

categorical_col = [&#39;天气&#39;, &#39;Traffic_Level&#39;, &#39;时间&#39;,&#39;车辆类型&#39;]
col_new_df = [col for col in df.columns if col not in categorical_col ]
col_new_df

new_df = df[col_new_df]
new_df.info()

# 特征缩放
for col in new_df.columns:
if col not in [&#39;订单ID&#39;,&#39;交货时间分钟&#39;]:
new_df[col] = new_df[col].astype(&#39;float64&#39;)
mean = new_df[col].mean()
std = new_df[col].std()
new_df.loc[:,col] = (new_df[col]-mean)/std #转换为浮点数，因为将浮点值缩放为分配给 int 数据类型后会引发警告

new_df.describe()

train_df = new_df.sample(frac = 0.8,random_state =200)
test_df = new_df.drop(train_df.index)

train_df.reset_index(inplace=True)
test_df.reset_index(inplace=True)

train_df.drop(columns = [&#39;index&#39;],inplace=True)
test_df.drop(columns = [&#39;index&#39;],inplace=True)

columns_needed =列表（new_df.columns）
columns_needed.remove（&#39;订单 ID&#39;）
columns_needed.remove（&#39;送货时间分钟&#39;）
X = train_df[columns_needed].to_numpy()
Y = train_df[&#39;送货时间分钟&#39;].to_numpy()
Y_mean = train_df[&#39;送货时间分钟&#39;].mean()
Y_std = train_df[&#39;送货时间分钟&#39;].std()
Y = (Y-Y_mean)/Y_std

m = len(X)
np.random.seed(42)
W = np.random.randn(len(X[0]))
b = 0
alpha = 0.5
Lambda = 0.5
迭代 = 0
dW = np.zeros(len(X[0]))
当迭代 &lt; 100000：
f = np.dot(X,W) + b
loss = (np.sum((f - Y)**2) + Lambda*np.sum(W*W))/(2*m)
dW = (np.dot(X.T,(f-Y)) + Lambda*W)/m
db = np.sum(f-Y)/m
W -= alpha*dW
b -= alpha*db
iteration += 1
if iteration % 10000 == 0：
print(iteration ,&quot;,&quot;,loss)
if(loss &lt; 10**(-3)):
break

print(&quot;done&quot;)

#测试
sum = 0
sum2 =0
ymean = test_df[&#39;Delivery_Time_min&#39;].mean()
Y_original = test_df[&#39;Delivery_Time_min&#39;].to_numpy()
X_test = test_df[columns_needed].to_numpy()
Y_predicted = np.dot(X_test,W) + b
Y_predicted_ori = Y_predicted*Y_std + Y_mean
print(&quot;原始 ---- 预测 ---- 错误&quot;)
for i in range(len(Y_original)):
print(Y_original[i],&quot;---&quot;,Y_predicted_ori[i],&quot;---&quot;,Y_original[i]-Y_predicted_ori[i])
sum += (Y_original[i]-Y_predicted_ori[i])**2
sum2 += (Y_original[i] - ymean)**2

rscore = 1 - (sum/sum2)
msme = sum/len(Y_original)
print(rscore)
print(msme)

以上是它的实现

数据集 - 食品配送时间预测（Kaggle）
R2_score 为 0.754
MSME 为 133.17

那么它对初学者来说好吗？
可以做些什么来改进它？]]></description>
      <guid>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</guid>
      <pubDate>Mon, 06 Jan 2025 18:12:55 GMT</pubDate>
    </item>
    <item>
      <title>通过 streamlit 执行时出现“短信垃圾邮件分类器”错误“实例未安装”[关闭]</title>
      <link>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</link>
      <description><![CDATA[错误图片
这是我的第一个 ML 项目：“短信垃圾邮件分类器”，当我通过 streamlit 运行它时，我遇到了这个错误。请帮我调试一下
后端代码运行正常，但这是我部署后显示的内容
输出应该是垃圾邮件/非垃圾邮件]]></description>
      <guid>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</guid>
      <pubDate>Mon, 06 Jan 2025 13:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust 中的 Burn 张量中使用 f64</title>
      <link>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</link>
      <description><![CDATA[我是 Rust 新手，我正在研究使用 Burn 移植一些 Python/Torch 代码，用于新的统计参数方法。
第一步：我想生成一个 (10, 1) 张量，其中包含从具有已知参数的柯西分布生成的随机值。Burn 中的分布非常有限，因此我使用 statrs。通过使用 statrs，我可以获得一个 Vec&lt;f64&gt;，然后我可以将其包装到 Burn 中的 TensorData 中，从而生成一个 Tensor。
我添加了一些类型签名，但 Burn 有 Float 而不是特定的 f64，我对此有点困惑。事实上，只是为了调试目的，我想从 Burn 张量中提取数据作为 Vec&lt;f64&gt; 来查看它（我应该从 vec: Vec&lt;f64&gt; 中看到相同的值）但我得到了运行时类型不兼容。
使用 rand::prelude::Distribution;
使用 statrs::distribution::Cauchy;
使用 rand_chacha::ChaCha8Rng;
使用 rand_core::SeedableRng;
使用 burn::tensor::{Tensor, TensorData, Float};
使用 burn::backend::Wgpu;

类型 Backend = Wgpu;

fn main() {
// 一些全局引用
let device = Default::default();
let mut rng: ChaCha8Rng = ChaCha8Rng::seed_from_u64(2);

// 使用 statrs 创建随机 vec，存储在 Vec&lt;f64&gt; 中
let dist: Cauchy = Cauchy::new(5.0, 2.0).unwrap();
let vec: Vec&lt;f64&gt; = dist.sample_iter(&amp;mut rng).take(10).collect();

// 将其包装到 Burn 张量中
let td: TensorData = TensorData::new(vec, [10, 1]);
let tensor: Tensor&lt;Backend, 2, Float&gt; = Tensor::&lt;Backend, 2, Float&gt;::from_data(td, &amp;device);

print!(&quot;{:?}\n&quot;, tensor.to_data().to_vec::&lt;f64&gt;().unwrap());
}


在上面运行时，我得到
thread &#39;main&#39; panicked at src/main.rs:23:55:
called `Result::unwrap()` on an `Err` value: TypeMismatch(&quot;Invalid target element type 
(expected F32, got F64)&quot;)

使用 to_vec::&lt;f32&gt; 有效，但我希望 Burn 张量具有 f64 值（torch 有这个），因为错误似乎意味着我在某个时候丢失了精度 - 不太好。
是否可以将 f64 存储在 Burn 张量中？]]></description>
      <guid>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</guid>
      <pubDate>Mon, 06 Jan 2025 12:43:18 GMT</pubDate>
    </item>
    <item>
      <title>如何获取在线视频游戏的类型？我的数据集包含大约 3500 个用户 ID、大约 5000 个游戏及其由这些用户给出的评分 [关闭]</title>
      <link>https://stackoverflow.com/questions/79332726/how-do-i-get-the-genre-of-video-games-online-my-dataset-contains-around-3500-us</link>
      <description><![CDATA[我正在尝试构建一个推荐引擎作为家庭作业的一部分。数据集包含大约 3500 个用户 ID、大约 5000 个游戏及其由这些用户给出的评分。我的当务之急是确定这些游戏的类型，根据一些人的说法，最好的方法是编写一个 Python 程序来获取包含此类信息的列表。但是，我不知道如何找到这样的列表。如果您有推荐系统经验，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/79332726/how-do-i-get-the-genre-of-video-games-online-my-dataset-contains-around-3500-us</guid>
      <pubDate>Mon, 06 Jan 2025 10:16:01 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将手势识别集成到 .NET MAUI 中？</title>
      <link>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</link>
      <description><![CDATA[我有一个名为“手语应用”的项目，我需要在其中集成手语手势识别。我不知道从哪里开始，因为我只具备 C# 基础知识。
我需要指南或建议，看看是否可行。]]></description>
      <guid>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</guid>
      <pubDate>Mon, 06 Jan 2025 09:55:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 应用中自动裁剪收据以仅显示购买的商品和价格？[关闭]</title>
      <link>https://stackoverflow.com/questions/79332546/how-to-automatically-crop-a-receipt-to-show-only-purchased-items-and-prices-in-a</link>
      <description><![CDATA[我正在构建一个 Flutter 应用，用户可以在其中上传收据，我想处理这些收据，以便只显示购买的商品、金额和价格。我的最终目标是提取这些信息以供进一步处理。
以下是我迄今为止尝试过的方法：

OCR：我使用 google_ml_kit 进行 OCR，但它会捕获收据上的所有文本，包括页眉、页脚和其他不相关的部分。
正则表达式：我尝试使用正则表达式过滤 OCR 输出以匹配价格和商品的模式，但这不起作用，因为收据的格式和结构差异很大。

为了提高准确性，我考虑使用图像标记或训练自定义 TensorFlow Lite 模型来识别和裁剪收据中包含购买商品和价格的部分，然后再应用 OCR。但是，我不确定如何有效地解决这个问题，特别是在 Flutter 的背景下。
挑战：

收据具有不同的布局、字体和大小，因此没有固定的裁剪区域。
以编程方式准确识别购买商品和价格的区域。
将经过训练的 TensorFlow Lite 模型整合到 Flutter 应用中以进行图像分割或标记。

问题：

如何使用图像处理或机器学习技术自动裁剪收据的相关部分（包含购买的商品和价格）？
训练自定义 TensorFlow Lite 模型是一种可行的方法吗？如果是，我该如何训练它来检测收据的相关部分？
是否有现有的工具、框架或软件包（与 Flutter/Dart 兼容）可以简化此过程？

示例图像（所以我只想要框中的内容）：
]]></description>
      <guid>https://stackoverflow.com/questions/79332546/how-to-automatically-crop-a-receipt-to-show-only-purchased-items-and-prices-in-a</guid>
      <pubDate>Mon, 06 Jan 2025 08:50:54 GMT</pubDate>
    </item>
    <item>
      <title>检测器模型中框的正确损失函数</title>
      <link>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</guid>
      <pubDate>Sun, 05 Jan 2025 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>如何指定使用集成分类器在网格搜索中进行迭代的级别？</title>
      <link>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</link>
      <description><![CDATA[我有以下设置，但我找不到在网格搜索中传递级别以探索 svm* 和 mlp* 的方法：
steps = [(&#39;preprocessing&#39;, StandardScaler()),
(&#39;feature_selection&#39;, SelectKBest(mutual_info_classif, k=15)),
(&#39;clf&#39;, VotingClassifier(estimators=[(&quot;mlp1&quot;, mlp1),
(&quot;mlp2&quot;, mlp2),
(&quot;mlp3&quot;, mlp3),
(&quot;svm1&quot;, svm1),
(&quot;svm2&quot;, svm2)
], voting=&#39;soft&#39;))
]

model = Pipeline(steps=steps)
params = [{
&#39;preprocessing&#39;: [StandardScaler(), MinMaxScaler(), MaxAbsScaler()],
&#39;feature_selection__score_func&#39;: [f_classif, mutual_info_classif]
}]

grid_search = GridSearchCV(model, params, cv=10,scoring=&#39;balanced_accuracy&#39;, verbose=1, n_jobs=20, refit=True)
]]></description>
      <guid>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</guid>
      <pubDate>Sun, 05 Jan 2025 11:22:57 GMT</pubDate>
    </item>
    <item>
      <title>如何将 pixelClassificationLayer() 更新为自定义损失函数？</title>
      <link>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</link>
      <description><![CDATA[我在 Mathworks 官方网站上看到 pixelClassificationLayer() 函数，我应该使用以下代码将其更新为自定义损失函数：
function loss = modelLoss(Y,T) 
mask = ~isnan(T);
target(isnan(T)) = 0;
loss = crossentropy(Y,T,Mask=mask,NormalizationFactor=&quot;mask-included&quot;); 
end

netTrained = trainnet(images,net,@modelLoss,options); 

但是，我看不到任何关于输入“Classes”或“ClassWeights”的提及，我目前正使用它们来定义自定义 pixelClassificationLayer：
pixelClassificationLayer(&#39;Classes&#39;,classNames,&#39;ClassWeights&#39;,classWeights)，其中 classNames 是一个向量，以字符串形式包含每个类的名称，classWeights 是一个向量，包含每个类的权重，用于在训练数据中存在代表性不足的类时平衡类。
如何在自定义损失函数中包含这些参数？]]></description>
      <guid>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</guid>
      <pubDate>Sat, 04 Jan 2025 09:05:32 GMT</pubDate>
    </item>
    <item>
      <title>负类的 precision_recall_curve 导致正相关的精度和召回率</title>
      <link>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</link>
      <description><![CDATA[我有一个逻辑回归模型来预测二进制输出（0 或 1）。我想了解 0 类的 P/R，并生成相应的曲线。我使用这个代码：
clf = linear_model.LogisticRegression().fit(ohe_X_train, y_train)
# 预测独热编码测试集上的标签
clf_predictions = clf.predict(ohe_X_test)
y_scores = clf.predict_proba(ohe_X_test)

class_of_interest = 0

# 基于 precision_recall_fscore_support 的 P/R
precision, recall, fscore, support = precision_recall_fscore_support(y_test, clf_predictions, labels=[class_of_interest])

# 使用 precision_recall_curve 的 P/R 曲线
y_scores = clf.predict_proba(ohe_X_test)[:, class_of_interest]
precision_curve, recall_curve, Thresholds = precision_recall_curve(y_test, y_scores)

plt.plot(recall_curve, precision_curve, marker=&#39;.&#39;)
plt.xlabel(&#39;Recall&#39;)
plt.ylabel(&#39;Precision&#39;)
plt.title(&#39;Precision-Recall Curve for Class 0&#39;)
plt.grid(True)
plt.show()

在这种情况下，PR 曲线在提高召回率的同时提高了准确率。我做错了什么？当 class_of_interest = 1 时，它工作正常。]]></description>
      <guid>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</guid>
      <pubDate>Fri, 03 Jan 2025 20:45:09 GMT</pubDate>
    </item>
    <item>
      <title>对随机森林进行修改，每次分割时都会评估某些特征</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（像往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。
我不知道有哪些软件包允许开箱即用。有没有一个，或者也许有一个简单的解决方法来实现相同的效果？]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”</title>
      <link>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</link>
      <description><![CDATA[我正在尝试使用 Missforest 来处理表数据中的缺失值。
import sklearn
print(sklearn.__version__)
-&gt;1.2.1

import sklearn.neighbors._base
import sys
sys.modules[&#39;sklearn.neighbors.base&#39;] = sklearn.neighbors._base

!pip install missingpy
from missingpy import MissForest

到目前为止，它运行良好，但从昨天开始，出现了以下错误消息。
ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”

我想知道如何处理这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</guid>
      <pubDate>Sat, 04 Mar 2023 01:48:43 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在同一层使用多个损失函数吗？</title>
      <link>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</link>
      <description><![CDATA[我们可以在这个架构中使用多个损失函数吗：
我有两种不同类型的损失函数，并希望在最后一层使用它 [输出]
损失函数：

binary_crossentropy
自定义损失函数

我们可以这样做吗？
]]></description>
      <guid>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</guid>
      <pubDate>Sun, 12 Jul 2020 13:37:51 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Tensor 转换请求 dtype 为 float64，而 dtype 为 float32</title>
      <link>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</link>
      <description><![CDATA[我有一个 NN，它有两个相同的 CNN（类似于 Siamese 网络），然后合并输出，并打算在合并的输出上应用自定义损失函数，如下所示：
 ----------------- -----------------
| input_a | | input_b |
----------------- -----------------
| base_network | | base_network |
------------------------------------------------------
|processed_a_b |
------------------------------------------------------

在我的自定义损失函数中，我需要将 y 垂直分成两部分，然后在每部分上应用分类交叉熵损失。但是，我不断从损失函数中获取 dtype 错误，例如：
ValueError Traceback（最近一次调用最后一次）&lt;ipython-input-12-b01f2c4c71e3&gt; in &lt;module&gt;()
----&gt; 1 model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)
909 loss_weight = loss_weights_list[i]
910 output_loss = weighted_loss(y_true, y_pred,
--&gt; 911 sample_weight, mask)
912 if len(self.outputs) &gt; 1:
913 self.metrics_tensors.append(output_loss)

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in weighted(y_true, y_pred, weights, mask)
451 # 应用样本权重
452 如果权重不为 None:
--&gt; 453 score_array *= weights
454 score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
455 返回 K.mean(score_array)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
827 if not isinstance(y, sparse_tensor.SparseTensor):
828 try:
--&gt; 829 y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=&quot;y&quot;)
830 except TypeError:
831 # 如果 RHS 不是张量，则可能是张量感知对象

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
674 name=name,
675 preferred_dtype=preferred_dtype,
-&gt; 676 as_ref=False)
677 
678 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py 在 internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
739 
740 如果 ret 为 None:
--&gt; 741 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
742 
743 如果 ret 未实现：

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
612 引发 ValueError(
613 &quot;Tensor 转换请求 dtype %s 的 Tensor 具有 dtype %s：%r&quot;
--&gt; 614 % (dtype.name, t.dtype.name, str(t)))
615 返回 t
616 

ValueError：Tensor 转换请求 dtype float64 的 Tensor 具有 dtype float32： &#39;Tensor(&quot;processed_a_b_sample_weights_1:0&quot;, shape=(?,), dtype=float32)&#39;

以下是重现错误的 MWE：
import tensorflow as tf
from keras import backend as K
from keras.layers import Input, Dense, merge, Dropout
from keras.models import Model, Sequential
from keras.optimizers import RMSprop
import numpy as np

# 定义输入
input_dim = 10
input_a = Input(shape=(input_dim,), name=&#39;input_a&#39;)
input_b = Input(shape=(input_dim,), name=&#39;input_b&#39;)
# 定义 base_network
n_class = 4
base_network = Sequential(name=&#39;base_network&#39;)
base_network.add(Dense(8, input_shape=(input_dim,),activation=&#39;relu&#39;))
base_network.add(Dropout(0.1))
base_network.add(Dense(n_class,activation=&#39;relu&#39;))
processed_a = base_network(input_a)
processed_b = base_network(input_b)
# 合并左右部分
processed_a_b = merge([processed_a,processed_b],mode=&#39;concat&#39;,concat_axis=1,name=&#39;processed_a_b&#39;)
# 创建模型
model = Model(inputs=[input_a,input_b],outputs=processed_a_b)

# 自定义损失函数
def categorical_crossentropy_loss(y_true,y_pred):
# 拆分（取消合并）y_true 和 y_pred分成两部分
y_true_a, y_true_b = tf.split(value=y_true, num_or_size_splits=2, axis=1)
y_pred_a, y_pred_b = tf.split(value=y_pred, num_or_size_splits=2, axis=1)
loss = K.categorical_crossentropy(output=y_pred_a, target=y_true_a) + K.categorical_crossentropy(output=y_pred_b, target=y_true_b) 
return K.mean(loss)

# 编译模型
model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())
]]></description>
      <guid>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</guid>
      <pubDate>Fri, 14 Jul 2017 20:28:09 GMT</pubDate>
    </item>
    </channel>
</rss>