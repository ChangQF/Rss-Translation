<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 10 Feb 2024 00:55:22 GMT</lastBuildDate>
    <item>
      <title>使用 XGboost 进行时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</link>
      <description><![CDATA[我需要找到时间序列中的异常情况，对于这项任务我选择了机器学习方法，即 XGboost。
有一个数据集，包含 2参数：秒和系列值（分贝）。
问题：从PrepareData()函数获得的y_test由于某种原因为空，我不明白为什么会发生这种情况。您有解决问题的想法吗？
代码：
def code_mean(data, cat_feature, real_feature):
     ”“”
     返回一个字典，其中键是 cat_feature 的唯一类别，
     这些值是基于 real_feature 的平均值
     ”“”
    返回 dict(data.groupby(cat_feature)[real_feature].mean())

进行聚合
df1 = pd.read_csv(&quot;data.csv&quot;, sep=&#39;,&#39;)
df1.columns = [&#39;时间,秒&#39;,&#39;y&#39;]

df1[&#39;时间，秒&#39;] = df1[&#39;时间，秒&#39;]*10000
df1[&#39;时间，秒&#39;] = pd.to_datetime(df1[&#39;时间，秒&#39;],unit=&#39;s&#39;)
df1.set_index(&#39;时间,秒&#39;, inplace=True)
aggregate_df1 = df1.resample(&#39;H&#39;).mean()

数据 = pd.DataFrame(aggregate_df1)
data.index = pd.to_datetime(data.index)
数据[“小时”] = data.index.小时
数据[“工作日”] = data.index.工作日
数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1
数据.head()

该函数将立即返回分为训练和测试的数据集和目标变量。我猜这就是所有问题所在：
def prepareData(数据, lag_start=5, lag_end=20, test_size=0.15):

    数据 = pd.DataFrame(data.copy())
    数据.列 = [“y”]

    test_index = int(len(数据)*(1-test_size))

    对于范围内的 i（lag_start，lag_end）：
        data[“lag_{}”.format(i)] = data.y.shift(i)

    data.index = pd.to_datetime(data.index)
    数据[“小时”] = data.index.小时
    数据[“工作日”] = data.index.工作日
    数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1

    数据[&#39;weekday_average&#39;] = data[&#39;weekday&#39;].apply(lambda x: code_mean(data[:test_index], &#39;weekday&#39;, &quot;y&quot;).get(x))
    data[&quot;hour_average&quot;] = data[&#39;hour&#39;].apply(lambda x: code_mean(data[:test_index], &#39;hour&#39;, &quot;y&quot;).get(x))

    data.drop([“小时”,“工作日”], axis=1, inplace=True)

    数据 = data.dropna()
    数据 = data.reset_index(drop=True)

    # разбиваем весь датасет на тренировочную и тестовую выборку
    X_train = data.loc[:test_index].drop([“y”], axis=1)
    y_train = data.loc[:test_index][“y”]
    X_test = data.loc[test_index:].drop([“y”], axis=1)
    y_test = data.loc[test_index:][“y”]
    
    返回X_train，X_test，y_train，y_test

最后是预测的主要函数：
def XGB_forecast（数据，lag_start=5，lag_end=20，test_size=0.15，scale=1.96）：

    X_train、X_test、y_train、y_test = 准备数据（aggregate_df1、lag_start、lag_end、test_size）
    dtrain = xgb.DMatrix(X_train, 标签=y_train)
    dtest = xgb.DMatrix(X_test)

    参数 = {
        &#39;目标&#39;: &#39;reg:线性&#39;,
        &#39;助推器&#39;：&#39;gb线性&#39;
    }
    树 = 1000

    cv = xgb.cv(params, dtrain, 指标 = (&#39;rmse&#39;), verbose_eval=False, nfold=10, show_stdv=False, num_boost_round=trees)

    bst = xgb.train(params, dtrain, num_boost_round=cv[&#39;test-rmse-mean&#39;].argmin())

    #cv.plot(y=[&#39;test-mae-mean&#39;, &#39;train-mae-mean&#39;])

    偏差 = cv.loc[cv[&#39;test-rmse-mean&#39;].argmin()][&quot;test-rmse-mean&quot;]

    Prediction_train = bst.predict(dtrain)
    plt.figure(figsize=(15, 5))
    plt.plot(预测训练)
    plt.plot(y_train)
    plt.axis(&#39;紧&#39;)
    plt.网格（真）

    Prediction_test = bst.predict(dtest)
    下=预测测试规模*偏差
    上=预测_测试+尺度*偏差

    如果 len(y_test) &gt; 0:
        异常 = np.array([np.NaN]*len(y_test))
        异常[y_test&lt;下限] = y_test[y_test&lt;下限]
    别的：
        异常 = np.array([])

    plt.figure(figsize=(15, 5))
    plt.plot(prediction_test, label=&quot;预测&quot;)
    plt.plot(lower, &quot;r--&quot;, label=&quot;上键/下键&quot;)
    plt.plot(上，“r--”)
    plt.plot(列表(y_test), label=“y_test”)
    plt.plot（异常，“ro”，markersize=10）
    plt.legend(loc=&quot;最佳&quot;)
    plt.axis(&#39;紧&#39;)
    plt.title(“XGBoost 平均绝对误差{}用户”.format(round(mean_absolute_error(prediction_test, y_test))))
    plt.网格（真）
    plt.图例()

函数调用：XGB_forecast(aggreerated_df1, test_size=0.2, lag_start=5, lag_end=30)]]></description>
      <guid>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</guid>
      <pubDate>Fri, 09 Feb 2024 23:30:21 GMT</pubDate>
    </item>
    <item>
      <title>C# 中的机器学习</title>
      <link>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</link>
      <description><![CDATA[我应该构建一个机器学习模型来充当文档过滤器。在手头的数据集中，我有 3 种不同类型的文档，它们不应通过过滤器。该模型总共将处理 20 种不同类型的文档，最终目标是减轻手动处理这三种类型的负担。
我刚刚开始这个项目，我想在我自己探索该领域的同时，向社区寻求潜在方法的建议。训练数据相当小，在标记类型之间分布为 500、430、80。
该项目最好用 C# 编写。
到目前为止，我只从数据库中收集了文档并验证了它们是否被正确标记，并探索了各种方法，例如支持向量机、朴素拜耳、无监督学习方法和文本数据的各种预处理步骤。
微调预训练模型似乎很流行（迁移学习），也是一种潜在的方法，尤其是在数据集有限的情况下。但有些文档相对广泛，三种类型中的一种包含大约 3000 个单词，据我所知，这对于可用的预训练模型可能会出现问题，并且在 .net 中实现的可能性有限。
因此，目前我正着手构建一个传统模型，并征求您的意见和意见。]]></description>
      <guid>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</guid>
      <pubDate>Fri, 09 Feb 2024 22:21:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用注意力机制的双向 RNN/LSTM/GRU</title>
      <link>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</guid>
      <pubDate>Fri, 09 Feb 2024 21:52:40 GMT</pubDate>
    </item>
    <item>
      <title>训练随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</link>
      <description><![CDATA[我正在尝试在一组图像上训练随机森林分类器，旨在对 10 种不同的颜色类别进行分类。我有一个带有子目录的 \dataset 目录，每个子目录都以一种颜色（黄色、红色、白色等）命名。我采用 HSV 直方图并将其用作我的特征（它比 RGB 更好吗？）
这是我的代码。我在代码中添加了 print(image_path) 进行调试。但运行时，它只打印第一个文件，不打印其他任何内容。
有什么问题吗？如何解决？
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.feature_extraction.image 导入 extract_patches_2d
从 sklearn.model_selection 导入 train_test_split
从 skimage.color 导入 rgb2hsv，rgb2gray
从 skimage.feature 导入graycomatrix，graycoprops
导入操作系统
将 numpy 导入为 np
从 PIL 导入图像
导入作业库


# 定义路径和参数
data_dir =“.\数据集” # 替换为你的实际目录路径
num_trees = 100 # 森林中树木的数量
patch_size = (32, 32) # 用于特征提取的图像块的大小

# 定义特征提取函数
def extract_color_histogram(图像):
    # 将 PIL 图像转换为 NumPy 数组
    img_array = np.array(图像)
    # 调整图像大小
    img_resized = Image.fromarray(img_array).resize((640, 640))
    # 转换为 HSV
    img_hsv = img_resized.convert(“HSV”)
    hist, _ = np.histogram(img_hsv, bins=(8, 8, 8), 范围=((0, 255), (0, 255), (0, 255)))
    hist = hist.flatten()
    返回历史记录

def extract_texture_features(图像):
    # 转换为灰度并提取纹理特征
    灰色 = rgb2gray(图像)
    gray_uint = (gray * 255).astype(np.uint8) # 将灰度转换为无符号整数
    glcm = Graycomatrix(gray_uint，距离=[5]，角度=[0]，级别=256，对称=True，normed=True)
    特征 = Graycoprops(glcm, &#39;对比度&#39;)[0]
    返回特征

# 加载数据并提取特征
X、y = []、[]
对于 os.listdir(data_dir) 中的颜色：
    对于 os.listdir(os.path.join(data_dir, color)) 中的 image_file：
        image_path = os.path.join(data_dir, 颜色, image_file)
        打印（图像路径）
        img = Image.open(图像路径)
        img_array = np.array(img)
        补丁 = extract_patches_2d(img_array, patch_size=patch_size)
        对于补丁中的补丁：
            # 从每个补丁中提取特征
            color_hist = extract_color_histogram(补丁)
            纹理特征=提取纹理特征（补丁）
            特征 = np.concatenate((color_hist,texture_features))
            X.append（功能）
            y.追加（颜色）

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林模型
print(&quot;现在训练...&quot;)
模型 = RandomForestClassifier(n_estimators=num_trees, random_state=42)
model.fit(X_train, y_train)

# 评估模型性能
准确度 = model.score(X_test, y_test)
print(f&quot;模型精度: {accuracy:.2f}&quot;)

# 保存模型以供将来使用
joblib.dump(模型,“color_model.pkl”)
]]></description>
      <guid>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</guid>
      <pubDate>Fri, 09 Feb 2024 21:33:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM 的数据访问已损坏。想法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</link>
      <description><![CDATA[为了构建真正有用的 GenAI 应用程序，法学硕士需要能够从结构化和非结构化数据源访问专有数据。法学硕士很难了解检索相关数据的可用性、位置和方法。
关键问题：

法学硕士无法确定是否有必要的数据可用于回答任何数据源中的用户查询。
如果数据可用，法学硕士无法找到要从哪个数据源检索。
如果来源已知，为众多检索协议编写检索管道就会变得复杂、重复且不确定。

您在项目中遇到过这些问题吗？您发现哪些有效的变通方法或解决方案？有什么新工具或实践可以简化法学硕士与不同数据源的集成吗？
一些人建议微调或 RAG 作为潜在的解决方案，但是：

使用特定数据微调模型的成本很高，而且会随着最新数据而过时，并且存在内置的访问控制问题。持续的再培训成本高昂，而且跟不上实时数据变化的步伐。
RAG 不适用于结构化数据。大多数有用的数据都是结构化的，可以分布在多个数据源中，每个数据源都有自己的查询机制。
编写单独的检索管道是有限且复杂的，并且不能确保确定性、安全性和访问控制。
]]></description>
      <guid>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</guid>
      <pubDate>Fri, 09 Feb 2024 21:01:59 GMT</pubDate>
    </item>
    <item>
      <title>R 机器学习中 Ranger 模型的错误</title>
      <link>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</link>
      <description><![CDATA[我正在运行生存模型的机器学习代码。我的 pred_prob 代码有错误。谁能帮我？先感谢您
我的错误是：
&lt;代码&gt;&gt; pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
h(simpleError(msg, call)) 中的错误：
  在为函数“rowMeans”选择方法时评估参数“x”时出错：$ 运算符对于原子向量无效

我的代码是：
图书馆（护林员）# RSF
Library(survival) # 包含生存示例，处理生存对象
Library(caret) # 用于分层交叉验证
库(dplyr) # 数据操作
图书馆（佩奇）
# 加载数据集，其中Time是事件发生的时间，Event是事件发生的时间

库（readxl）
df2 &lt;- read_excel(“E:/ME/BS DATA/数据 BS.xlsx”)

# 改变状态变量的标签
df2 &lt;- df2 %&gt;% mutate(status = time_15year-1) # 0 = 审查，1 = 死亡
# 一些数据包含缺失值，为了简单起见，我省略了对 NA 的观察
df2 &lt;- na.omit(df2)
# 缩放至月
df2$time_15year &lt;- 楼层(df2$time_15year)
par(“三月”)
par(mar=c(.01,.01,.01,.01))
pairs(df2 %&gt;% dplyr::select(time_15year,BS_death), main = “NCCTG 脑中风数据”)
# 交叉验证，对状态变量进行分层，以确保每个组（此处已审查，已死亡）
# 均匀分布在交叉验证折叠上
折叠 &lt;- 2 # 表示 交叉验证
cvIndex &lt;- createFolds(factor(df2$BS_death), Folds, returnTrain = T)
#2 模型，训练
# 创建一些容器来存储结果
# （对于大模型来说不合理，对于大模型你可能需要将中间结果存储在磁盘上）
容器模型&lt;-向量（“列表”，长度（cvIndex））
容器_pred &lt;- 容器_模型

# 定义训练/测试数据
for (i in 1:length(cvIndex)) {
  train_data &lt;- df2[cvIndex[[i]], ]
  # 其余代码
}

测试数据 &lt;- df2[-cvIndex[[i]],]

train_data &lt;- train_data[complete.cases(train_data), ]
测试数据 &lt;- 测试数据[完整.案例(测试数据), ]

rangermodel &lt;- ranger(Surv(time_15year, BS_death) ~ 年龄 + 性别 + edu + 地点 + cvahis+ mihis + bphis +heartdis + smok + Pastsmok+ 被动+活动 +waterpip +cvatype, 数据 = train_data)

情节（rangermodel$unique.death.times，rangermodel$survival[1，]）
ranger_predict &lt;- 预测（rangermodel，数据=testing_data）
str(ranger_预测)
ranger_predict &lt;- unlist(ranger_predict)
pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
pred_prob[pred_prob&gt;中位数(pred_prob)]=1
pred_prob[pred_prob&lt;=中位数(pred_prob)]=0
表（pred_prob）
fusionMatrix(as.factor(testing_data$BS_death), as.factor(pred_prob))
]]></description>
      <guid>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</guid>
      <pubDate>Fri, 09 Feb 2024 19:06:49 GMT</pubDate>
    </item>
    <item>
      <title>'{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT] 的 1 减 3 导致的负维度大小</title>
      <link>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</link>
      <description><![CDATA[尝试 model.fit 后：
hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我收到错误：
ValueError Traceback（最近一次调用最后一次）
单元格位于\[132\]，第 1 行
\----\&gt; 1 hist = model.fit（train，epochs = 15，validation_data = val，callbacks = \ [tensorboard_callback \]）#mudei epocas pra 3，antesera 20

文件 \~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\utils\traceback_utils.py :70，在filter_traceback中。\.error_handler(\*args, \*\*kwargs)
67 过滤_tb = \_process_traceback_frames(e.__traceback__)
68 # 要获取完整的堆栈跟踪，请调用：
69 # `tf.debugging.disable_traceback_filtering()`
\---\&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
71 最后：
72 删除filtered_tb

文件 \~\\AppData\\Local\\Temp\__autograph_ generated_filea6t43riq.py:15，位于outer_factory.\.inner_factory.\.tf__train_function(iterator)
13 尝试：
14 do_return =真
\---\&gt; 15 retval_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(迭代器)), 无, fscope)
16 除外：
17 do_return = 假

ValueError：在用户代码中：

    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1338 行，位于训练函数*
        返回step_function（自身，迭代器）
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1322 行，位于步骤函数 **
        输出 = model.distribute_strategy.run(run_step, args=(data,))
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1303 行，位于运行步骤**
        输出 = model.train_step(数据)
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1080 行，位于训练步骤
        y_pred = self(x, 训练=True)
    文件“C：\ Users \ Enenon \ AppData \ Local \ Packages \ PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0 \ LocalCache \ local-packages \ Python310 \ site-packages \ keras \ src \ utils \traceback_utils.py”，第70行，位于错误处理程序
        从 None 引发 e.with_traceback(filtered_tb)
    
    ValueError：调用层“conv2d_26”（类型 Conv2D）时遇到异常。
    
    &#39;{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=“NHWC”, dilations=[1, 1, 1, 1],explicit_paddings=[ 1 减 3 导致的负维度大小], padding=“VALID”, strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_15/Cast,equential_15/conv2d_26/Conv2D/ReadVariableOp)&#39; 输入形状：[64,64,1, 1]、[3,3,1,32]。
    
    调用层“conv2d_26”接收的参数（类型 Conv2D）：
      • 输入=tf.Tensor(形状=(64, 64, 1, 1), dtype=float32)`

我的代码：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv2D(32, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
模型.add(压平())
model.add（密集（64，激活=&#39;relu&#39;））
model.add（密集（3，激活=&#39;sigmoid&#39;））

model.compile(&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我正在使用 64,64,1 数据集。这是将张量从 64,64 修改为 64,64,1 并从张量创建数据集的代码：
x = x.reshape(-1, 64, 64, 1)
标签 = tf.constant(y)
特征 = tf.constant(x)
datax = tf.data.Dataset.from_tensor_slices(特征)
datay = tf.data.Dataset.from_tensor_slices(标签)
数据 = tf.data.Dataset.zip((datax,datay))
data_iterator = data.as_numpy_iterator()
批处理 = data_iterator.next()
]]></description>
      <guid>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</guid>
      <pubDate>Fri, 09 Feb 2024 17:51:53 GMT</pubDate>
    </item>
    <item>
      <title>要使应用程序能够理解加载的非结构化文档（例如，不规则表格）中的信息，我需要了解什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</link>
      <description><![CDATA[例如
不规则表格
一种能够理解文档中的文本内容并可能将信息分离为键：值的技术。我还是一名大学生，我想在 NLP 领域取得进步，但我找不到从哪里开始。
我应该学习哪些 NLP 技术，路线图应该是什么样的？]]></description>
      <guid>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</guid>
      <pubDate>Fri, 09 Feb 2024 17:36:06 GMT</pubDate>
    </item>
    <item>
      <title>如何创建目标变量[关闭]</title>
      <link>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</link>
      <description><![CDATA[我尝试预测 ncaa 篮球疯狂游行的每个结果。
我有过去 20 场左右锦标赛的历史数据，并且有 team1_score 和 team2_score 等列。我认为创建一个目标变量很容易，只需创建一个列 team1_win 并在 true 时返回 1，否则返回 0。问题是我的数据是经过组织的，因此 team1 始终是获​​胜团队。所以我的目标变量列将只包含 1。对于二元分类来说，这似乎是一个问题。我不确定如何创建目标变量。我是否需要以某种方式重新整理我的数据，以便 team1 并不总是获胜团队？我的目标变量全为1有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</guid>
      <pubDate>Fri, 09 Feb 2024 17:22:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 WSL GPU 支持下运行 Windows Python 代码？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</link>
      <description><![CDATA[我尝试在 Windows PS 上的 WSL 2 发行版中使用 TensorFlow。我的问题是，我只能在互联网上找到描述安装过程的页面，但没有人解释如何在 Windows 上使用 GPU 加速从我的 virtual-env 环境运行代码。那么有没有办法在 WSL Distrubition 中远程运行机器学习脚本呢？
我已经成功安装了WSL2，包括miniconda和tensorflow（带有CUDA）。 Tensorflow Feedback 线也发现 GPU 没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</guid>
      <pubDate>Fri, 09 Feb 2024 16:52:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在嵌入词汇中添加新项目？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</link>
      <description><![CDATA[假设您已经训练了一个包含嵌入层的模型。
您的模型表现良好，并且您对嵌入感到满意。
然后，突然，您想在词汇表中添加一个新项目。
换句话说，您想要计算这个新项目的嵌入。
嵌入层基本上是一个查找表，用于将正整数转换为固定大小的密集向量，现在您想要考虑训练期间不存在的新整数。
如何在不从头开始重新训练模型的情况下做到这一点？
重新启动训练冻结除用于嵌入新项目的参数之外的所有参数是否有意义（在进行一些矩阵形状调整之后）？]]></description>
      <guid>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</guid>
      <pubDate>Fri, 09 Feb 2024 15:55:26 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4 [关闭]</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中分析 RNN、CNN、NN 结果</title>
      <link>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</link>
      <description><![CDATA[我有大量标记数据集。每行包含 863 标记化单词。我正在尝试验证哪种类型的 NN 最适合分析此类数据集。
我准备了3个模型：
美国有线电视新闻网：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32, input_length=863),
        tf.keras.layers.Conv1D(32, 5, 激活=&#39;relu&#39;,kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01)),
        tf.keras.layers.GlobalMaxPooling1D(),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

简单平面神经网络：
模型 = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
]）

和 RNN：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
        tf.keras.layers.LSTM（32，激活=&#39;relu&#39;，return_sequences=True），
        tf.keras.layers.LSTM(32, 激活=&#39;relu&#39;, ),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

CNN 和 NN 给出了有希望的结果，accu 率约为 98%（可能过度拟合），而 RNN 的accu 率仅为 65% 左右。值得一提的是，RNN 的 epoch 至少在 10 分钟左右，而 CNN 和 NN 只有 1 分钟。
如何让 RNN 表现更好？]]></description>
      <guid>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</guid>
      <pubDate>Tue, 28 Apr 2020 19:50:48 GMT</pubDate>
    </item>
    <item>
      <title>对 Dataframe 中的某些列进行估算</title>
      <link>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</link>
      <description><![CDATA[我正在尝试在名为“年龄”的单个列上使用Imputer来替换缺失值。但是，我收到错误：“预期是二维数组，却得到了一维数组：”
以下是我的代码
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 Imputer

数据集 = pd.read_csv(“titanic_train.csv”)

dataset.drop(&#39;小屋&#39;, axis=1, inplace=True)
x = dataset.drop(&#39;幸存&#39;, axis=1)
y = 数据集[&#39;幸存&#39;]

imputer = Imputer（missing_values =“nan”，策略=“平均值”，轴= 1）
imputer = imputer.fit(x[&#39;年龄&#39;])
x[&#39;年龄&#39;] = imputer.transform(x[&#39;年龄&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</guid>
      <pubDate>Tue, 18 Sep 2018 10:44:49 GMT</pubDate>
    </item>
    <item>
      <title>网格上的 CNN 回归 - 卷积神经网络的局限性？</title>
      <link>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</link>
      <description><![CDATA[我正在使用 CNN 解决（与高能物理相关的）问题。
为了理解这个问题，让我们考虑一下这里的这些示例。
左侧是 CNN 的输入，右侧是所需的输出。因此网络应该对输入进行聚类。这种聚类背后的实际算法（即我们如何获得所需的训练输出）非常复杂，我们希望 CNN 能够学习这一点。
我尝试过不同的 CNN 架构，例如类似于 U-net 架构的架构 (https:// arxiv.org/abs/1505.04597），还有各种卷积层的串联等。
输出总是非常相似（对于所有架构）。
在这里您可以看到一些 CNN 预测。
原则上，网络表现得相当好，但正如您所看到的，在大多数情况下，CNN 输出由几个直接相邻的填充像素组成，这在真实情况下永远不会（！）发生。&lt; /p&gt;
我一直在所有网络中使用均方误差作为损失函数。
您对如何避免这一问题并提高网络性能有什么建议吗？
或者这是 CNN 的一般限制，并且在实践中不可能使用 CNN 解决这样的问题？]]></description>
      <guid>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</guid>
      <pubDate>Mon, 05 Mar 2018 12:05:42 GMT</pubDate>
    </item>
    </channel>
</rss>