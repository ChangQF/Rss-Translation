<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 28 Apr 2024 15:13:49 GMT</lastBuildDate>
    <item>
      <title>我正在寻找这些列类型的什么类型的机器学习？</title>
      <link>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</link>
      <description><![CDATA[我一直在学习一些关于机器学习的知识，并使用了一些模型类型（xgboost、LogisticRegression）和一些测试数据。我使用这些模型的次数越多，我就越意识到它们处理的是一种特定类型的数据，即可以转换为数字的列。甚至像汽车的品牌/型号之类的东西也可以转化为数字，因为它们是有限的并且在数据集中重复。
我真正想要使用的数据集包含名字和姓氏、公司名称、电子邮件地址等唯一的字符串。这是一个例子

&lt;标题&gt;

名字和姓氏
公司名称
电子邮件地址
是欺诈


&lt;正文&gt;

全食 CVS 评估
全食/CVS 评估
laime.barry9989@gmail.com
正确


全食店
全食店
laimeb.a.r.ry9989@gmail.com
正确


蒂娜·罗森
最佳商品鞋
tina.rosen@gmail.com
错误


乔约翰
全食品市场调查
wholefoodsmark.et.l.inc@gmail.com
正确


史黛西帕克特
S Parket 奥特莱斯
sales@parkeroutlet.com
错误


迈克尔·费兰
克罗格
b.ill.h.o.rt2.2@gmail.com
正确



这是我拥有的一小部分数据，但您可以看到它不适合我所了解和使用的模型的正常数据集。我尝试过诸如 OneHotEncoder 和 LabelEncoder 之类的东西，但它们将它们转换为实际上没有任何意义的整数，因为它们不重复。
我知道看到该示例很容易想到“哦，只需自己编写验证器来查找电子邮件中的多个句点、名称中的特定单词等”即可。但有数千个重复的欺诈帐户不适合。
所以我的问题是，是否有一种机器学习模型可以接收这些电子邮件地址/名称等内容并了解欺诈电子邮件地址/名称的样子？]]></description>
      <guid>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</guid>
      <pubDate>Sun, 28 Apr 2024 15:01:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的数据增强：我应该对验证集应用数据增强吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78398283/data-augmentation-in-machine-learning-should-i-apply-data-augmentation-for-vali</link>
      <description><![CDATA[我正在皮肤损伤医学图像数据集上训练卷积神经网络。原始数据集由训练和测试文件夹组成。因此，我将一些图像从训练文件夹移动到新的有效文件夹进行验证。由于数据集不平衡，我随后将数据增强应用于训练集。我现在的问题是：我应该使用之前创建的验证集，还是通过从增强训练集中提取图像来创建新的验证集？另外，如果我使用准确性作为验证指标，验证集是否应该保持不平衡（我知道准确性需要平衡的数据集），或者每个类的样本数量是否需要相同？
使用的数据集由九个类组成。到目前为止，仅当数据增强应用于训练集和验证集时，我才能够在训练集和验证集上获得良好的准确性性能。但当我进入测试阶段时，结果却不太令人满意。我目前正在尝试使用 Keras Tuner 在平衡数据集和原始验证集上搜索各种模型。]]></description>
      <guid>https://stackoverflow.com/questions/78398283/data-augmentation-in-machine-learning-should-i-apply-data-augmentation-for-vali</guid>
      <pubDate>Sun, 28 Apr 2024 13:14:45 GMT</pubDate>
    </item>
    <item>
      <title>决策树信息增益与特征重要性</title>
      <link>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</link>
      <description><![CDATA[我基于 Sklearn 用 Python 编写了一个决策树，但是当我计算结果并显示决策树（以及 20 个最重要的特征）时，特征“A”被忽略了。最重要，也作为根节点。
但是，当我计算每个特征的信息增益并将结果显示在列表中时，特征“B”会出现。具有最高的信息增益(特征“A”也具有相当高的信息增益，但不如特征B)。尽管如此，特征 A 被用作根节点......所以我的问题是：我是否犯了编程错误，或者这是一种可能的情况（根据定义，具有最高信息增益的特征不被用作根节点）。 
在另一个主题中，有人写了以下内容：
&lt;块引用&gt;
对于使用信息增益的决策树，算法选择
提供最大信息增益的属性（这是
也是导致熵减少最大的属性）。

还有（尤其是这部分非常有趣）：
&lt;块引用&gt;
决策树算法是“贪婪的”算法。从某种意义上说，他们总是
选择产生最大信息增益的属性
正在考虑当前节点（分支），而无需稍后重新考虑
添加后续子分支后的属性。所以要回答你的
第二个问题：决策树算法尝试放置属性
在树根部附近信息增益最大。注意
由于算法的贪婪行为，决策树算法
不一定会产生一棵提供最大可能的树
熵的总体减少。

所以在这种情况下，它没有理由选择类别 B 而不是类别 A，这意味着我可能犯了一个编码错误..？]]></description>
      <guid>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</guid>
      <pubDate>Sun, 28 Apr 2024 11:50:34 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法评估模型是否能够识别有影响的变量（使用 make_classification 生成的变量）？</title>
      <link>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。我使用 make_classification（二元分类任务）创建了一个数据集，目的是测试不同模型区分重要特征和不太重要特征的能力。
如何设置一个实验来评估模型是否能够识别有影响的变量？
我查看了 make_classification 的文档，但不幸的是我没有进一步了解。
我设置了以下内容：
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</guid>
      <pubDate>Sun, 28 Apr 2024 11:37:08 GMT</pubDate>
    </item>
    <item>
      <title>如何将稀疏分类熵给出的预测类的二维数组输出转换为预测类</title>
      <link>https://stackoverflow.com/questions/78397693/how-to-convert-2d-array-output-of-predicted-classes-given-by-sparse-categorical</link>
      <description><![CDATA[我在大学接到一项任务，要编写一个简单文本数据集的分类器。我有 5 个类：[&#39;0&#39;、&#39;1&#39;、&#39;2&#39;、&#39;3&#39;、&#39;4&#39;]。我不太了解神经网络，并且使用互联网我写了一些东西 =)
但是
如果我使用 categorical_crossentropy 它会给我一个错误
参数“目标”和“输出”必须具有相同的等级 (ndim)。收到：目标。形状=（无，），输出.形状=（无，6）

如果我使用“sparse_categorical_crossentropy”，它会在输出中提供二维数组。
x_train，x_test，y_train，y_test = train_test_split（X_scaled，y_encoded，test_size = 0.2，random_state = 42）

x_train = x_train / 255
x_测试 = x_测试 / 255

def b_m(马力):
    模型=顺序（）
    activation_choice = hp.Choice(&#39;activation&#39;, value=[&#39;relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;, &#39;elu&#39;, &#39;selu&#39;])
    model.add(密集(单位=hp.Int(&#39;units_input&#39;,
                                   最小值=512，
                                   最大值=1024，
                                   步骤=32),
                    input_dim=2,
                    激活=activation_choice））
    model.add(密集(单位=hp.Int(&#39;units_hidden&#39;,
                                   最小值=128，
                                   最大值=600，
                                   步骤=32),
                    激活=activation_choice））
    model.add（密集（5，激活=&#39;softmax&#39;））
    模型.编译(
        优化器=hp.Choice(&#39;优化器&#39;, 值=[&#39;adam&#39;,&#39;rmsprop&#39;,&#39;SGD&#39;]),
        损失=&#39;sparse_categorical_crossentropy&#39;,
        #loss=&#39;categorical_crossentropy&#39;,
        指标=[&#39;准确性&#39;])
    返回模型

调谐器 = kt.Hyperband(b_m,
                     目标=&#39;val_accuracy&#39;,
                     最大纪元=10,
                     因子=3，
                     目录=&#39;test_dir&#39;）

stop_early = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=5)

tuner.search（x_train，y_train，epochs = 50，validation_split = 0.2，callbacks = [stop_early]）

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

模型=tuner.hypermodel.build(best_hps)
历史= model.fit（x_train，y_train，epochs = 50，validation_split = 0.2）

val_acc_per_epoch = 历史.history[&#39;val_accuracy&#39;]
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1

超级模型=tuner.hypermodel.build(best_hps)

hypermodel.fit（x_train，y_train，epochs = best_epoch，validation_split = 0.2）

eval_result = hypermodel.evaluate(x_test, y_test)
print(&quot;[损失，测试]:&quot;, eval_result)

y_pred = hypermodel.predict(x_test)


所以，我有三个问题：1.在这种情况下最好使用什么类型的损失？
2.如果使用categorical_crossentropy更好，那么如何修复出现的错误
3.如果使用sparse_categorical_crossentropy更好，那么如何将输出从2d数组转换为带有类的一维数组
请帮忙
我尝试重塑输入数据并更改模型的参数，但什么也没发生]]></description>
      <guid>https://stackoverflow.com/questions/78397693/how-to-convert-2d-array-output-of-predicted-classes-given-by-sparse-categorical</guid>
      <pubDate>Sun, 28 Apr 2024 09:26:46 GMT</pubDate>
    </item>
    <item>
      <title>打包 Cython 扩展时出现“ImportError”</title>
      <link>https://stackoverflow.com/questions/78397654/getting-an-importerror-when-packaging-a-cython-extension</link>
      <description><![CDATA[我们已经尝试解决此错误一段时间了。我正在开发一个名为 mlsauce 的包。当我克隆包并运行 python3 -m pip install 时。 --verbose（在虚拟环境中），我得到已成功安装 mlsauce-0.17.1。然而在运行时，我得到：
回溯（最近一次调用）：
  文件“/workspaces/codespaces-blank/mlsauce/examples/adaopt_classifier.py”，第 13 行，在  中。
    将mlsauce导入为ms
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/__init__.py”，第 58 行，位于  中。
    从 .adaopt 导入 AdaOpt
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/adaopt/__init__.py”，第 1 行，在  中
    从 .adaopt 导入 AdaOpt
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/adaopt/adaopt.py”，第 11 行，在  中。
    导入适配器
ModuleNotFoundError：没有名为“adaoptc”的模块

我可以在存储库中看到 .so 文件：
存储库的树结构
如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78397654/getting-an-importerror-when-packaging-a-cython-extension</guid>
      <pubDate>Sun, 28 Apr 2024 09:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：形状 (64,100) 和 (10,100) 未对齐：100 (dim 1) != 10 (dim 0)</title>
      <link>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</link>
      <description><![CDATA[我在训练 2 层神经网络时遇到这个错误
我尝试了这段代码，但它给出了上述错误。
将 numpy 导入为 np
从 numpy.random 导入 randn
&lt;前&gt;&lt;代码&gt;N、D_输入、H、D_输出 = 64、1000、100、10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(D_out, H)

对于范围（2000）内的 t：
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    损失 = np.square(y_pred - y).sum()
    打印（t，损失）

    grad_y_pred = 2.0 * (y_pred -y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))

    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
]]></description>
      <guid>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</guid>
      <pubDate>Sun, 28 Apr 2024 04:12:08 GMT</pubDate>
    </item>
    <item>
      <title>机器学习：numpy，处理 NaN 值的问题</title>
      <link>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</guid>
      <pubDate>Sat, 27 Apr 2024 23:19:52 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的分类模型</title>
      <link>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</link>
      <description><![CDATA[我正在研究一个机器学习项目，准确地说是分类。我的数据集包含 217 个国家的社会、人口和经济指数，每个国家 60 年。目标变量是二进制的。我想训练随机森林和 xgboost 模型，我想知道：我可以用 Caret 训练这些模型吗？他们能够理解这种结构并处理多级数据吗？
如果是的话，我想用这种方式训练模型：
&lt;前&gt;&lt;代码&gt;#tree
ctrl_tree &lt;- trainControl(方法 = “cv”，数字 = 10，classProbs = TRUE，summaryFunction=twoClassSummary)
树 &lt;- train(Target~.,data=under, method = “rpart”,tuneLength = 10, metric=“ROC”, trControl = ctrl_tree)


#XGBoost
设置种子(76)
ctrl_xgb &lt;- trainControl(方法=“cv”，数字=10，搜索=“网格”，summaryFunction = TwoClassSummary，classProbs = TRUE)
param_grid_xgb &lt;- Expand.grid(nrounds=500, max_depth = c(3, 6, 9),eta = c(0.01, 0.1, 0.3),
  伽马 = c(0, 0.2, 0.4)，子样本 = c(0.8, 0.9, 1)，colsample_bytree = c(0.8, 0.9, 1)，
  min_child_weight=c(1, 5, 10))
xgb&lt;-train(Target~.,data=under,method=“xgbTree”,metric=“ROC”,tuneGrid=param_grid_xgb,
           trControl=ctrl_xgb,详细程度=0)
]]></description>
      <guid>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</guid>
      <pubDate>Sat, 27 Apr 2024 19:37:36 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>如何消除在张量流的 Tape.gradient 方法中将虚数转换为实值的警告？</title>
      <link>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</link>
      <description><![CDATA[我正在使用tape.gradient方法来优化一些神经网络。它按预期工作，但当我在单次迭代中多次使用 Tape.gradients 计算梯度时，不断发出此警告。这意味着在单个循环内，在执行 back prop 时，它会在某个地方摆弄复数。
警告：tensorflow：您正在将complex64类型的输入转换为不兼容的dtype float64。这将丢弃虚部，并且可能不是您想要的。

cost_progress=[]
跟踪进度=[]
对于我在范围内（次数）：

  使用 tf.GradientTape() 作为磁带：
    磁带.watch(参数)
    损失，跟踪 = 成本（参数，比率）
    trace_progress.append(trace)
    cost_progress.append(损失)

  梯度 = Tape.gradient(loss, params)
  opt.apply_gradients(zip([渐变], [参数]))

现在，所有参数和损失都是 tf.float64，但仍在 Tape.gradient() 中给出了一些复杂类型，我想手动将它们转换为真实值，以便此警告停止显示在我的屏幕上。但我无法找到如何投射以免弄乱。
强制gradients = tf.cast(tape.gradient(loss, params),tf.float64)不起作用。我已验证 gradients = Tape.gradient(loss, params) 发出警告，并且 loss 和 params 均为 tf.float64 类型。]]></description>
      <guid>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</guid>
      <pubDate>Wed, 27 Sep 2023 06:26:47 GMT</pubDate>
    </item>
    <item>
      <title>我应该把reuse_actors=True放在哪里？</title>
      <link>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</link>
      <description><![CDATA[运行以下代码后，它会显示
&lt;块引用&gt;
INFO trainable.py:172 – Trainable.setup 花费了 2940.989 秒。如果您的可训练初始化速度很慢，请考虑设置reuse_actors=True以减少actor创建开销

导入光线
ray.init(地址=“自动”, _temp_dir=&#39;/home/ray_dir&#39;)

rnd = 随机.种子(8)
grid_cv = StratifiedKFold(n_splits=3,random_state=rnd, shuffle=True)

从 xgboost.callback 导入 EarlyStopping
Early_stopping = EarlyStopping(轮数 = 50, 最大化 = True, save_best = True)
clf = xgb.XGBClassifier(
                tree_method=&#39;gpu_hist&#39;,
                最大bin=512，
                学习率 = 0.0001,
                n_估计器=1000，
                目标=&#39;二进制：逻辑&#39;，reg_alpha=0.01，
                scale_pos_weight = pos_weight, eval_metric= &#39;aucpr&#39;,
                回调=[early_stopping],
                详细程度 = 0,
                线程数 = 96
                ）


参数 = {
    &#39;eta&#39;: [0.01, 0.1, 0.3],
    &#39;min_child_weight&#39;: [1,3,8,16],
    &#39;最大深度&#39;:[25,50,100,500],
    &#39;colsample_bytree&#39;: [0.4,0.6,0.8],
    &#39;子样本&#39;: [0.4,0.6,0.8],
    &#39;伽玛&#39;：[0,0.5,2,10],
}

gs = TuneGridSearchCV（估计器 = clf、param_grid = param、cv = grid_cv、n_jobs = -1、refit = True、return_train_score = True、verbose = 3、评分 = &#39;average_ precision&#39;、use_gpu = True ）

gs.fit(X_train, y_train, eval_set= eval_set_xgboost, verbose=True)

我应该在代码中的何处添加 reuse_actors=True ？]]></description>
      <guid>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</guid>
      <pubDate>Mon, 29 May 2023 00:25:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中使用softmax输出进行神经网络和机器学习来解释多项Logit模型？ [复制]</title>
      <link>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</link>
      <description><![CDATA[它涉及使用机器学习和神经网络的 softmax 函数输出来理解和解释多项 Logit 模型。]]></description>
      <guid>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</guid>
      <pubDate>Mon, 02 Mar 2020 03:49:09 GMT</pubDate>
    </item>
    <item>
      <title>批量读取Cifar10数据集</title>
      <link>https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches</link>
      <description><![CDATA[我尝试读取 CIFAR10 数据集，该数据集分批提供自 https://www.cs.toronto.edu/~kriz/cifar.html&gt;。我尝试使用 pickle 将其放入数据框中并读取其中的“数据”部分。但我收到此错误。
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-24-8758b7a31925&gt; 在 &lt;module&gt;()
----&gt; 1 unpickle(&#39;datasets/cifar-10-batches-py/test_batch&#39;)

&lt;ipython-input-23-04002b89d842&gt;在 unpickle(file) 中
3 fo = open(file, &#39;rb&#39;)
4 dict = pickle.load(fo, encoding =&#39;bytes&#39;)
----&gt; 5 X = dict[&#39;data&#39;]
6 fo.close()
7 返回 dict

KeyError: &#39;data&#39;.
我正在使用 ipython，这是我的代码：
def unpickle(file):

fo = open(file, &#39;rb&#39;)
dict = pickle.load(fo, encoding =&#39;bytes&#39;)
X = dict[&#39;data&#39;]
fo.close()
返回 dict

unpickle(&#39;datasets/cifar-10-batches-py/test_batch&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches</guid>
      <pubDate>Sun, 29 May 2016 16:29:55 GMT</pubDate>
    </item>
    <item>
      <title>平滑后的GPS数据对比</title>
      <link>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</link>
      <description><![CDATA[我正在尝试比较用于平滑 GPS 数据的多种算法。我想知道比较结果以查看哪一个提供更好的平滑效果的标准方法应该是什么。
我正在考虑一种机器学习方法。基于分类器创建汽车模型并检查哪些轨道提供更好的行为。
对于在这方面有更多经验的人来说，这是一个好方法吗？还有其他方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</guid>
      <pubDate>Tue, 30 Dec 2014 17:21:04 GMT</pubDate>
    </item>
    </channel>
</rss>