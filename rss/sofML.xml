<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Apr 2024 12:24:34 GMT</lastBuildDate>
    <item>
      <title>MLFlow UI 未加载 - 缺少静态内容</title>
      <link>https://stackoverflow.com/questions/78353335/mlflow-ui-not-loading-missing-static-content</link>
      <description><![CDATA[我已经使用 mlflow server ... 部署了 MLFlow 服务器。当访问其部署的 URL https://some.url 时，我会看到一个空白页面并且无法加载。奇怪的是，这在本地主机上不是本地问题。
调查浏览器的控制台，我发现静态 (.../mlflow/build) 文件出现一堆错误，给出 404。
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78353335/mlflow-ui-not-loading-missing-static-content</guid>
      <pubDate>Fri, 19 Apr 2024 11:28:16 GMT</pubDate>
    </item>
    <item>
      <title>如何从 LbfgsLogisticRegression 训练的模型中检索原始模型参数？</title>
      <link>https://stackoverflow.com/questions/78352598/how-can-i-retrieve-original-model-parameters-from-a-lbfgslogisticregression-trai</link>
      <description><![CDATA[我选择使用 LbfgsLogisticRegression，因为它是可重新训练的（可以在不丢失现有模型的情况下对更多数据进行训练）。 （链接)
我的第一批数据通过通常的 Fit 调用训练得很好：
var estimator = mlContext.Transforms.Concatenate(“Features”, nameof(SampleDataEntry.Features))
.Append(mlContext.BinaryClassification.Trainers.LbfgsLogisticRegression(labelColumnName: &quot;name_match&quot;, featureColumnName: nameof(SampleDataEntry.Features)));

但是，当涉及到重新训练时，我显然应该调用 Fit(IDataView, LinearModelParameters)。我有 IDataView，但正在努力获取 LinearModelParameters。 Microsoft 的文档提供了以下不同算法的代码示例：
//提取训练好的模型参数
线性回归模型参数 原始模型参数 =
    ((ISingleFeaturePredictionTransformer)trainedModel).Model 作为 LinearRegressionModelParameters;

但是，当我尝试调整它以检索我自己的模型参数时，我遇到了一系列转换失败。在监视模式下调查 model 对象，我发现它甚至没有代码示例中暗示的 Model 属性。它有一个 .LastTransformer 属性，该属性又包含 .Model。
.LastTransformer 具有类型
Microsoft.ML.Data.BinaryPredictionTransformer&gt;
.LastTransformer.Model 具有类型
Microsoft.ML.Calibrators.ParameterMishingCalibrateModelParameters
打开该模型，我们找到一个 .SubModel 属性，其类型为
Microsoft.ML.Trainers.LinearBinaryModelParameters
但是，如果我检索该组参数，它们不包含 label 字段等基本信息，因此重新训练失败。
任何人都可以向我指出代码示例/教程，或者解释我可以在哪里查找这些原始参数吗？
这是最后一段不起作用的代码：
LinearBinaryModelParameters 原始模型参数 = ((TransformerChain&gt;&gt;)model).LastTransformer.Model.SubModel as LinearBinaryModelParameters;
返回mlContext.BinaryClassification.Trainers.LbfgsLogisticRegression().Fit(splitTrainSet,originalModelParameters);
]]></description>
      <guid>https://stackoverflow.com/questions/78352598/how-can-i-retrieve-original-model-parameters-from-a-lbfgslogisticregression-trai</guid>
      <pubDate>Fri, 19 Apr 2024 09:12:03 GMT</pubDate>
    </item>
    <item>
      <title>自学pdf到数据转换器</title>
      <link>https://stackoverflow.com/questions/78352394/self-learning-pdf-to-data-converter</link>
      <description><![CDATA[以下是问题陈述 -
创建尖端、自学的智能 PDF 到数据转换器
目标：

生成式 AI 集成：利用生成式 AI 技术智能地解释和提取多页 PDF 文档中的数据（包括文本、表格和图形），并将其转换为结构化且可用的数据格式。&lt; /里&gt;
自学习能力：整合机器学习模型，使系统能够通过学习每个处理的文档来不断提高转换准确性。这包括理解各种文档布局并根据反馈纠正错误。
处理复杂文档：确保解决方案有效处理布局复杂的文档（例如科学论文、财务报告和法律文档），准确提取数据，同时保持上下文和结构。
效率和可扩展性：开发一种解决方案，不仅准确，而且处理时间高效，能够处理大量文档，并且可扩展以满足个人用户和大型组织的需求。&lt; /里&gt;
用户界面和体验：设计一个用户友好的界面，允许用户上传 PDF 文档、监控转换过程以及轻松编辑或纠正提取数据中的任何不准确之处。

可交付成果：

智能 PDF 到数据转换器的工作原型，展示了生成式 AI 和机器学习在数据提取方面的集成。
原型应有效处理不同类型的文档，例如商业文档（例如采购订单、提单、空运单、装箱单）、财务文档（例如各种布局的发票、付款通知书）和合规文档（例如，运输单、报关单）。
概述解决方案架构、所使用的生成式 AI 和机器学习模型以及用户说明的文档。
重点介绍原型的功能、所解决的挑战以及对数据提取过程的潜在影响的演示。

目前是否有任何模型/变压器或任何工作可以用来解决这个问题以及如何解决这个问题陈述。还有我可以使用的这个问题陈述的任何数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/78352394/self-learning-pdf-to-data-converter</guid>
      <pubDate>Fri, 19 Apr 2024 08:39:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 Raspberry Pi 4 时 Python 中的多线程崩溃</title>
      <link>https://stackoverflow.com/questions/78352359/multiple-threads-collapsing-in-python-with-raspberry-pi-4</link>
      <description><![CDATA[我在尝试使用 Raspberry Pi 4B 完成大学项目时遇到问题。
该项目使用 Python 编写，由 5 个线程组成，其中 2 个线程运行机器学习预测，第 3 个线程按顺序运行一系列计算，以达到与机器学习模型预测的输出相同的输出（这样我可以对比输出是否是合理的值）。另外两个线程是：一个等待 10 秒并激活一个标志（开始处理所需的标志），另一个在终端上打印值（都是从 ML 模型和计算中预测的）。
我的问题是，当我尝试同时运行所有线程时，ML 模型运行正确，但我的计算线程不执行任何操作。相反，如果我不启动 ML 线程，计算线程就会正常工作。
我认为Raspberry没有足够的计算能力，因此计算线程崩溃了。
没有必要所有 3 个线程同时运行（我希望能够选择是否要查看终端上打印的 ML 预测或计算输出），因此我尝试禁用线程当我不使用它们时（使用 thread.join() ）并在我决定希望该线程再次开始运行时再次启动它们（thread.start() ）但它无法正常工作。
我还尝试过使用运行预测或计算函数所需的两个标志（ML_flag 和calculations_flag），但也不起作用。
关于我可以使用的其他技术的任何想法，以便 ML 预测和计算单独运行并且不会崩溃？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78352359/multiple-threads-collapsing-in-python-with-raspberry-pi-4</guid>
      <pubDate>Fri, 19 Apr 2024 08:31:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用已经制作的嵌入来制作索引</title>
      <link>https://stackoverflow.com/questions/78351643/how-to-make-indexes-using-already-made-embeddings</link>
      <description><![CDATA[我是机器学习新手。我想使用 llama_index。我有嵌入。如何使用这些嵌入创建索引？我正在使用huggingface微调法学硕士。如果我理解有误，请纠正我。]]></description>
      <guid>https://stackoverflow.com/questions/78351643/how-to-make-indexes-using-already-made-embeddings</guid>
      <pubDate>Fri, 19 Apr 2024 06:14:32 GMT</pubDate>
    </item>
    <item>
      <title>python - ImportError：无法从“dill._dill”导入名称“_is_imported_module”</title>
      <link>https://stackoverflow.com/questions/78351307/python-importerror-cannot-import-name-is-imported-module-from-dill-dill</link>
      <description><![CDATA[已安装的数据集打包到Python虚拟环境中。当我尝试导入它并运行时，
from datasets import load_dataset，我收到此错误，“ImportError：无法从 &#39;dill._dill&#39; 导入名称 &#39;_is_imported_module&#39;”，然后出现此错误，当我向下滚动时，“AttributeError：模块“dill”没有属性“_dill””。
完整的错误消息：
”\[自动重新加载 dill.session 失败：回溯（最近一次调用）：
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第276行，检查
superreload(m, 重新加载, self.old_objects)
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第 475 行，在超级重载中
模块=重新加载（模块）
^^^^^^^^^^^^^^^
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\importlib\__init_\_.py”，第 131 行，重新加载
\_bootstrap.\_exec(规范，模块)
文件“\”，第 866 行，位于 \_exec
文件“\”，第 995 行，位于 exec_module
文件“\”，第 488 行，位于 \_call_with_frames_removed
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\dill\\session.py”，第 24 行，位于 \ 中
从 .\_dill 导入 (
ImportError：无法从“dill.\_dill”导入名称“\_is_imported_module”（C：\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\dill_dill.py）
\]
\[自动重新加载 dill.\_shims 失败：回溯（最近一次调用最后一次）：
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第276行，检查
superreload(m, 重新加载, self.old_objects)
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第 500 行，在超级重载中
update_generic（旧的_obj，新的_obj）
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第 397 行，位于 update_generic
更新（a，b）
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第 365 行，在 update_class 中
update_instances（旧的，新的）
文件“C:\\Users\\joshu\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\IPython\\extensions\\autoreload.py”，第 323 行，位于 update_instances 中
对象.__setattr__（参考，“__class__”，新）
TypeError: __class__ 赋值: &#39;\_CallableReduce&#39; 对象布局与 &#39;\_CallableReduce&#39; 不同
\]
回溯（最近一次调用最后一次）：

compat_exec 中的文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\spyder_kernels\\py3compat.py:356
exec（代码、全局变量、局部变量）

文件 c:\\users\\joshu.spyder-py3\\temp.py:14
从数据集导入load_dataset

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\__init_\_.py:18
从 .arrow_dataset 导入数据集

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\arrow_dataset.py:68
从 .arrow_writer 导入 ArrowWriter，OptimizedTypedSequence

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\arrow_writer.py:29
从 .features 导入特征、图像、值

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\features\__init__\_.py:17
从.audio导入音频

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\features\\audio.py:11
从 ..download.streaming_download_manager 导入 xopen、xsplitext

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\download\__init__\_.py:9
从 .download_manager 导入 DownloadManager、DownloadMode

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\download\\download_manager.py:43
从 ..utils.py_utils 导入 NestedDataStructure、map_nested、size_str

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\utils\\py_utils.py:44
from .\_dill import ( # noqa: F401 # 为向后兼容而导入。 TODO: 在 3.0.0 中删除

文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\utils_dill.py:27
Pickler 类（莳萝.Pickler）：

Pickler 中的文件 \~\\anaconda3\\envs\\sentence_similarity\\Lib\\site-packages\\datasets\\utils_dill.py:28
调度 = dill.\_dill.MetaCatchingDict(dill.Pickler.dispatch.copy())

AttributeError: 模块“dill”没有属性“\_dill””

尝试重新安装并安装早期版本的 dill。那不起作用。此外，我有一个运行相同版本数据集的 jupyter 笔记本，并且我和 dill 没有遇到任何问题。]]></description>
      <guid>https://stackoverflow.com/questions/78351307/python-importerror-cannot-import-name-is-imported-module-from-dill-dill</guid>
      <pubDate>Fri, 19 Apr 2024 04:11:25 GMT</pubDate>
    </item>
    <item>
      <title>请为我的毕业设计解决机器学习中牙齿分割模型的Valueerror</title>
      <link>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</link>
      <description><![CDATA[这是牙齿分割模型的 GitHub 链接：https://github.com/Arnold0210/TEETH-RECOGNITION-WITH-MACHINE-LEARNING
大家好，我在 GitHub [HTTPs://github.com/] 上获取的模型中的 classification.py 中的代码遇到了一些问题。 com/Arnold0210/牙齿识别与机器学习]。如果有人有兴趣深入研究我的问题，该程序应该执行以下操作：
该程序应为用户提供两种选择：

从图像中读取并提取特征：此选项使用 FeatureExtraction 模块从图像中提取 9 个特征（包括图像名称）。
读取预先存在的数据集：此选项读取包含 labels.csv、features.csv 和图像文件的数据集。然后它会询问用户：

执行程序的次数（假设为 5）。
使用 K 折交叉验证分割数据所需的折叠数（假设为 5 折叠，即 k=5）。
测试数据集的大小（假设为 20%）。



模型然后将这些参数传递给classification模块中的分类函数。这就是问题出现的地方：

代码将整个数据集传递给 onlyfiles，其中包含 973 个条目。
然后，它会从 labels.csv（有 778 个条目）中识别 images_name 和 label_color。这代表训练数据集，因为我们之前指定了 20% 的测试集（778 = 973 的 80%）。
以下 for 循环迭代由 k_folds.split(images_name) 生成的分割。此时，我们仍在处理训练数据集，并且当 k=5 时，应该有：

train_index 中有 662 个索引（用于训练数据）。
test_index 中有 156 个索引（用于在训练集中进行验证）。



这是下一个 for 循环中发生错误的位置：
对于 train_index 中的 i：
    current_filename = onlyfiles[i].split(&#39;.&#39;)[0].strip()
    如果 current_filename 在训练数据集中：
        # ...（其余代码）
    别的：
        print(f“警告：在 images_name 中找不到‘{current_filename}’，因为它的索引是 {i}.train。”)


第一行根据 train_index 中的索引 i 检索文件名 (current_filename)。假设 i 为 324，train_index 包含从 156 到 777 的索引（而 test_index 范围从 0 到 155）。
出现此错误的原因是，有时循环会尝试在 images_name 中查找 current_filename，但该文件并不存在。这是因为 images_name 只有 778 个条目（训练数据），其余 195 个条目（测试数据）不包括在内。因此，current_filename 实际上可能属于测试数据集，从而导致错误“101_0032.JPG 不在列表中”。

我尝试对列表进行排序并删除随机播放（在 k_folds.split 中设置 shuffle=False），但错误仍然存​​在。我非常感谢您为解决此问题提供一些帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</guid>
      <pubDate>Thu, 18 Apr 2024 23:03:02 GMT</pubDate>
    </item>
    <item>
      <title>您可以使用 CreateML 从文本 blob 中提取文本吗？</title>
      <link>https://stackoverflow.com/questions/78350086/can-you-use-createml-to-extract-text-from-a-text-blob</link>
      <description><![CDATA[我一直在使用 CreateML 通过文本分类构建模型。它需要读取文本块，并从该文本块中提取名称。 （该斑点来自 iPhone 的 OCR 结果）文本斑点的大小各不相同，但名称始终出现在文本中。
我遇到的问题是，只有在训练期间显示过该名称时，它才会找到该名称。如果是新名称，则不起作用。有没有办法修改 CreateML 以根据 blob 中的其他数据查找新名称？
如果 CreateML 无法做到这一点，是否还有其他工具可以做到这一点？生成的模型需要在 iPhone 上运行。]]></description>
      <guid>https://stackoverflow.com/questions/78350086/can-you-use-createml-to-extract-text-from-a-text-blob</guid>
      <pubDate>Thu, 18 Apr 2024 20:05:18 GMT</pubDate>
    </item>
    <item>
      <title>LMST模型敏感性——初学者抗运气</title>
      <link>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</link>
      <description><![CDATA[我一直在尝试使用艾伯塔省电力市场的一些非常基本的数据，并尝试使用时间序列数据的 LMST 模型来尝试预测价格。我确实得到“可能”这是我的模型的结果，而且它似乎确实出现了我们可以预期的一些波动（仅根据我自己的市场经验）。
但是，我正在寻求更好地理解我遇到的一些陷阱。
从 keras.models 导入顺序
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dropout
从 keras.layers 导入密集
将 pandas 导入为 pd
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入mean_absolute_error,mean_squared_error
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
导入作业库

# 加载数据
# 加载数据

# 加载数据
csv_file_path = &#39;Frankenstein.csv&#39; # 使用您的实际文件路径更新
df = pd.read_csv(csv_file_path)

# 将“日期/时间”转换为日期时间并提取数据集中存在的组件
如果 df.columns 中的“日期/时间”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期/时间&#39;])
    df[&#39;年份&#39;] = df[&#39;日期&#39;].dt.year
    df[&#39;月份&#39;] = df[&#39;日期&#39;].dt.月份
    df[&#39;日期&#39;] = df[&#39;日期&#39;].dt.day
    df[&#39;小时&#39;] = df[&#39;日期&#39;].dt.小时
    df.drop([&#39;日期/时间&#39;, &#39;日期&#39;], axis=1, inplace=True)

# 假设“价格”是目标变量
features = df.drop([&#39;价格&#39;], axis=1)
目标 = df[&#39;价格&#39;]

# 标准化特征和目标
缩放器特征 = MinMaxScaler()
features_scaled = scaler_features.fit_transform(features)
缩放器目标 = MinMaxScaler()
target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))

# 创建序列函数
def create_sequences（特征，目标，time_steps = 100）：
    X、y = []、[]
    对于范围内的 i(len(features) - time_steps)：
        X.append(特征[i:(i + time_steps)])
        y.append(目标[i + time_steps])
    返回 np.array(X), np.array(y)

# 使用整个数据集创建序列
X, y = create_sequences(features_scaled, target_scaled.flatten())

# 模型配置
input_shape = (X.shape[1], X.shape[2]) # (time_steps, num_features)

# 定义LSTM模型
模型=顺序（[
    LSTM（单位=100，return_sequences=True，input_shape=input_shape），
    辍学（0.1），
    LSTM（单位=100），
    辍学（0.1），
    密集（单位=100，激活=&#39;elu&#39;），
    Dense(1) # 预测单个值
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

# 在整个数据集上训练模型
历史= model.fit（X，y，纪元= 150，batch_size = 20，validation_split = 0.1）

# 情节训练&amp;验证损失值
plt.figure(figsize=(10, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;火车&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证&#39;)
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend(loc=&#39;右上&#39;)
plt.show()

# 保存LSTM模型
model_save_path = &#39;trained_lstm_model.h5&#39;
model.save(model_save_path)
print(f&quot;模型已保存到 {model_save_path}&quot;)
joblib.dump(scaler_features, &#39;scaler_features.pkl&#39;)
joblib.dump(scaler_target, &#39;scaler_target.pkl&#39;)

有人可以给绝对的初学者一些建议吗？主要是为了更好地理解我应该如何设置它。我有一个每小时的数据集，是过去三年的历史生成和交换。我正在寻找方法让我的模型对供应与价格的变化更具反应性。]]></description>
      <guid>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</guid>
      <pubDate>Thu, 18 Apr 2024 19:18:18 GMT</pubDate>
    </item>
    <item>
      <title>如何解决此 OML4py 代理错误？</title>
      <link>https://stackoverflow.com/questions/78348747/how-can-i-resolve-this-oml4py-proxy-error</link>
      <description><![CDATA[例如，apply 函数适用于 pandas DataFrame，但 OML 数据框不支持 apply 方法。我收到此错误
AttributeError：“DataFrame”对象没有属性“apply”
如何解决这个问题？
&#39;&#39;&#39;
%python
导入oml
将 pandas 导入为 pd

inp = [{&#39;VAL_1&#39;:10,&#39;VAL_2&#39;:1},{&#39;VAL_1&#39;:11,&#39;VAL_2&#39;:10},{&#39;VAL_1&#39;:12,&#39;VAL_2&#39;:0}]
df = pd.DataFrame(inp)
oml_df = oml.push(df)

# apply 函数与 pandas DataFrame 一起使用：

％Python

df[&#39;VAL_NEW&#39;] = df.apply(lambda 行，arg: 行[&#39;VAL_1&#39;] + 行[&#39;VAL_2&#39;] +100, axis=1, args=(100,))
打印（df）

# 但OML数据框不支持apply方法：

％Python

oml_df[&#39;VAL_NEW&#39;] = oml_df.apply(lambda 行，arg: 行[&#39;VAL_1&#39;] + 行[&#39;VAL_2&#39;] +100, axis=1, args=(100,))

# AttributeError: &#39;DataFrame&#39; 对象没有属性 &#39;apply&#39;

&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78348747/how-can-i-resolve-this-oml4py-proxy-error</guid>
      <pubDate>Thu, 18 Apr 2024 15:44:01 GMT</pubDate>
    </item>
    <item>
      <title>OML4py 中的两个列分组依据</title>
      <link>https://stackoverflow.com/questions/78347872/tow-column-group-by-in-oml4py</link>
      <description><![CDATA[如何使用 OML4Py oml.group_apply 调用按两列进行分组？
例如在 sql 中我可以执行以下操作：
&#39;&#39;&#39;
从 emp 中选择 mgr、count(mgr)、count(deptno)、deptno
按经理、部门分组
按部门编号排序；
返回
7782 1 1 10
7839 1 1 10
0 1 10
7566 2 2 20
7788 1 1 20
7839 1 1 20
7902 1 1 20
7698 5 5 30
7839 1 1 30
&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78347872/tow-column-group-by-in-oml4py</guid>
      <pubDate>Thu, 18 Apr 2024 13:37:39 GMT</pubDate>
    </item>
    <item>
      <title>无论输入值如何，序数编码都会显示相同的值，从而使预测结果相同</title>
      <link>https://stackoverflow.com/questions/78343238/ordinal-encoding-keeps-showing-the-same-value-no-matter-the-input-value-thus-ma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78343238/ordinal-encoding-keeps-showing-the-same-value-no-matter-the-input-value-thus-ma</guid>
      <pubDate>Wed, 17 Apr 2024 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>根据单个树获取 XGBoost 预测</title>
      <link>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</link>
      <description><![CDATA[它可能与如何获取每棵树的重复xgboost 中的预测？ 但解决方案不再有效（可能是 XGBoost 库上的更改）。我的想法是以原始格式 model.get_booster().get_dump() 转储模型并在不同的平台中实现它（仅预测）。不过，我首先尝试用 python 来实现它。运行以下代码，使用所有单独的增强器进行预测并将它们组合起来，不会返回与 model.predict() 函数相同的结果。
有什么方法可以将 model.predict() 与助推器的组合相匹配吗？我错过了什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb
从sklearn导入数据集
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# 加载数据
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# 拟合模型
模型 = xgb.XGBClassifier(
    n_估计器=10，
    最大深度=10，
    use_label_encoder=False,
    目标=&#39;二进制：逻辑&#39;
）
模型.fit(X, y)
booster_ = model.get_booster()

# 提取个人预测
individual_preds = []
对于 booster_ 中的 tree_：
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    ）
individual_preds = np.vstack(individual_preds)

# 将个体预测汇总为最终预测
individual_logits = inverse_sigmoid(individual_preds)
Final_logits = individual_logits.sum(axis=0)
Final_preds = sigmoid(final_logits)

# 验证正确性
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)

&lt;块引用&gt;
断言错误：数组不几乎等于小数点后 7 位
不匹配的元素：150 / 150 (100%)
最大绝对差：0.90511334
最大相对差值：0.99744916
x: 数组([7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05、7.4847587e-05、7.4847587e-05、7.4847587e-05、
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,...
y: 数组([0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,...
]]></description>
      <guid>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</guid>
      <pubDate>Tue, 16 Apr 2024 12:49:27 GMT</pubDate>
    </item>
    <item>
      <title>对于表格数据模型中的过度拟合我该怎么办</title>
      <link>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</link>
      <description><![CDATA[我建立了一个预测模型，用于根据所提供数据中的某些特征来预测结果。
该模型是一个利用 fastai 的表格学习器。
该数据集包含约 300 条记录，分为训练集、验证集和测试集。
我已经实现了解决过度拟合的技术，例如提前停止和权重衰减，但在对未见过的数据进行评估时，模型仍然似乎过度拟合。
此外，我还尝试调整学习率和批量大小等超参数，但没有改善。我怀疑我的模型架构或预处理管道的某些方面可能会导致该问题，但我不确定从哪里开始调查。
鉴于该项目的敏感性，我无法提供有关数据集或预测任务的具体细节，但我可以分享当前模型的预处理和结构。
这是训练的输出：

&lt;标题&gt;

纪元
train_loss
valid_loss
准确度
时间


&lt;正文&gt;

0
0.752707
0.579501
0.776119
00:00


1
0.699270
0.833771
0.776119
00:00


2
0.652438
0.598243
0.791045
00:00


3
0.621083
3.889398
0.776119
00:00


4
0.591348
0.632366
0.791045
00:00


5
0.580582
6.670314
0.791045
00:00



&lt;块引用&gt;
自 epoch 2 以来没有任何改进：提前停止

这是预处理的代码（在我构建了我不能透露的功能之后）。
features 列表定义每个特征，包括有效值范围和权重（feature、range_ 和 weight 如下面的标准化函数中所使用的那样）。
def custom_normalize(df, 特征, range_, 权重):
    df[特征] = 归一化(df[特征], range_)
    df[特征] = df[特征] * 权重
    返回df

分割 = RandomSplitter(valid_pct=0.2)(range_of(df))

procs = [分类，填充缺失]

对于功能，features.items() 中的信息：
    # 确定训练时选择值的范围。
    procs.append(partial(custom_normalize, feature=feature, range_=info[&#39;range&#39;],weight=info[&#39;weight&#39;]))

据我所知，构建模型和训练是非常标准的：
to = TabularPandas(df, procs=procs,
                   cat_names = cat_vars,
                   连续名称=连续变量，
                   y_names=dep_var,
                   分裂=分裂）

dls = to.dataloaders(bs=64)

Early_stop = EarlyStoppingCallback(监视器=&#39;准确度&#39;, min_delta=0.01, 耐心=3)

学习 = tabular_learner(dls, 指标=准确度, wd=0.1)
学习.lr_find()

# 绘制学习率。
learn.recorder.plot_lr_find()

# 根据情节选择学习率。
lr = learn.recorder.lrs[np.argmin(learn.recorder.losses)]

learn.fit_one_cycle(15, lr, cbs=early_stop)
学习.show_results()

# 如果模型不存在则只保存模型
# TODO 将保存包装在条件中，以防止模型存在时保存。
如果不是 os.path.exists(model_fname):
    学习.保存(model_fname)
]]></description>
      <guid>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</guid>
      <pubDate>Tue, 16 Apr 2024 08:38:01 GMT</pubDate>
    </item>
    <item>
      <title>如何将Yolo片段注释转换为coco格式？</title>
      <link>https://stackoverflow.com/questions/77364462/how-to-convert-yolo-segment-annotations-to-coco-format</link>
      <description><![CDATA[我正在尝试将 yolo 段数据集转换为 coco 格式。最初我使用 ultralytics 的 JsonToYolo 从 Coco 转换为 Yolo。现在我想做反之亦然。
我尝试过一些 yolo 到 coco 转换器，例如 YOLO2COCO 和使用五十一转换器。这些只会将 bbox（边界框）值转换为 coco 格式，而不是分段值。分段值为空([])。
可可格式：
“注释”：[
        {
            “id”：1，
            “图像ID”：1，
            “类别 ID”：1，
            “分段”：[
                [
                    5131.4,
                    1099.1,
                    5014.3,
                    1079.0,
                    4918.16,
                    1093.03,
                    4878.82,
                    1161.22,
                    4881.44,
                    1205.81,
                    4898.05,
                    1264.38,
                    4934.77,
                    1283.62,
                    4904.17,
                    1302.85,
                    4982.85,
                    1323.83,
                    4988.97,
                    1338.69,
                    5090.38,
                    1307.22,
                    5135.84,
                    1288.86,
                    5183.05,
                    1283.62,
                    5168.19,
                    1227.67,
                    5179.55,
                    1217.18,
                    5184.8,
                    1199.69,
                    5214.5,
                    1157.7
                ]
            ],
            “区域”：62712.0，
            “bbox”：[
                4878.82,
                1079.0,
                335.68,
                259.69
            ],
            “拥挤”：0，
            “属性”：{
                “用户名”：“”，
                “被遮挡”：假
            }
        },

Yolo 格式：
  0 0.21875 0.380208 0.215625 0.390625 0.215625 0.395833 0.214062 0.401042 0.214062 0.4062 0.40625 0.40625 0.2125 0.2125 0.411458 0.2125 0.2125 0.4270938 0.4210938 0.4210938 0.4210938 0.4210938 0.4210938 0.4210938 0.4210938.4210938.4210938.4210938 375 0.453125 0.209375 0.458333 0.207813 0.463542 0.207813 0.46875 0.20625 0.20625 0.473958 0.20625 0.479167 201562 0.520833 0.201562 0.5625 0.2 0.567708 0.2 0.59375 0.201562 0.598958 0.201562 0.651042 0.203125 0.65625 0.204688 0.651042 0.204688 0.6451042 375 0.619792 0.209375 0.614583 0.210938 0.609375 0.210938 0.604167 0.2125 0.598958 0.598958 0.2125 0.59375 0.2145 0.214062 708 0.217187 0.5625 0.217187 0.552083 0.21875 0.546875 0.21875 0.21875 0.536458 0.220313 0.53125 0.220313 0.520833 0.221875 0.515625 0.221875 0.505208 0.223438 0.5 0.223438 0.489583 0.225 0.484375 0.225 0.46875 0 .226562 0.463542 0.226562 0.432292 0.228125 0.427083 0.228125 0.380208

另一种方法是使用 Roboflow，其中数据以 yolo 格式上传并以 coco 格式导出。但我需要一个将 yolo 注释转换为 coco 格式的脚本。
有线索吗？]]></description>
      <guid>https://stackoverflow.com/questions/77364462/how-to-convert-yolo-segment-annotations-to-coco-format</guid>
      <pubDate>Thu, 26 Oct 2023 06:03:51 GMT</pubDate>
    </item>
    </channel>
</rss>