<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 07 Aug 2024 06:22:04 GMT</lastBuildDate>
    <item>
      <title>Pytorch 张量与标量相乘后丢失requires_grad</title>
      <link>https://stackoverflow.com/questions/78841738/pytorch-tensor-losing-requires-grad-after-multiplication-with-a-scalar</link>
      <description><![CDATA[我正在开展一个深度学习项目，并试图实现一个投影网络。简而言之，我有一个根据某些损失函数输出值的神经网络。如果这些值不满足所有约束，我想将它们放入梯度下降网络（损失为 ReLU（需要 - 已实现））。在下面的代码中，我用 p 和 phi 参数（第一个网络输出的值）初始化投影网络并希望它们进行训练。但是，我遇到了 p 和 phi（theta）变量都失去梯度跟踪的问题，导致损失没有梯度跟踪。如果这很简单，我深表歉意，因为我对 Pytorch 还很陌生，在网上找不到任何东西。
投影 nn 的代码：
class Projection(nn.Module):
def __init__(self, p, phi):
super().__init__()
self.p = nn.Parameter(p.clone().detach().requires_grad_(True))
self.phi = nn.Parameter(phi.clone().detach().requires_grad_(True))

def forward(self, u, g, v):
print(self.p.requires_grad, self.phi.requires_grad) ## 均为 true
p = self.p * P_max
print(p.requires_grad) # 在此点之后为 false
p = p.view(batch_size, K, 1)
sig = torch.nn.Sigmoid()
theta = torch.diag_embed(torch.exp(sig(self.phi) * 2 * torch.pi * 1j))
print(theta.requires_grad) # 在此点之后为 false
gh = torch.conj(torch.transpose(g,-1,-2))
print(gh.shape, theta.shape, v.shape)
term = torch.matmul(torch.matmul(gh, theta), v)
u_tilde = torch.add(u, term)
print(u_tilde.shape, p.shape)
harvested = eta * torch.matmul(torch.square(torch.abs(torch.transpose(u_tilde, -1, -2))), p) * 1e6
ReLU = torch.nn.ReLU()
diff = e_min_gen - harvested
loss = torch.sum(ReLU(diff))
print(loss.requires_grad)
return loss

P_max 声明如下：
P_max = 0.5
我特别困惑，因为在同一个项目中我使用梯度下降来生成可行数据。模型如下：
class Descent(nn.Module):
def __init__(self):
super().__init__()
self.weight = nn.Parameter(torch.randn(N, require_grad=True))

def forward(self, u, g, phi, v, p):
phi = phi * self.weight
sig = torch.nn.Sigmoid()
theta = torch.diag_embed(torch.exp(sig(phi) * 2 * torch.pi * 1j))
gh = torch.conj(torch.transpose(g,-1,0))
term = torch.mm(torch.mm(gh, theta), v)
u_tilde = torch.add(u, term)
harvested = eta * torch.matmul(torch.square(torch.abs(torch.transpose(u_tilde, 0, 1))), p) * 1e6
ReLU = torch.nn.ReLU()
diff = e_min_gen - harvested
loss = torch.sum(ReLU(diff))
return loss

此模型保留 require_grad 直到最后。
任何解决方法都将不胜感激。感谢您的时间。
我尝试查找梯度跟踪丢失的原因。我已经准确地确定了跟踪停止的点。如果我不将 self.p 乘以 P_max，则 p 的 gradient_tracking=True。]]></description>
      <guid>https://stackoverflow.com/questions/78841738/pytorch-tensor-losing-requires-grad-after-multiplication-with-a-scalar</guid>
      <pubDate>Wed, 07 Aug 2024 03:39:37 GMT</pubDate>
    </item>
    <item>
      <title>如何最好地将 ML 模型集成到 Web 应用程序？</title>
      <link>https://stackoverflow.com/questions/78841736/how-to-best-integrate-ml-models-to-web-application</link>
      <description><![CDATA[我没有机器学习经验，是 Web 应用程序编程的新手。我目前有一个使用 audiocraft 和分类模型的 flask 应用程序。目前我已将它们本地存储在应用程序文件夹中。我已构建此应用程序的映像，结果显示它有 7GB。
有没有办法将这些模型/框架存储在其他地方，并且仅在需要时引用它们？
此外，当我在 docker 上运行容器时，从 audiocraft 生成 8 秒音频大约需要 10 分钟。您建议我做什么来加快这个过程？
music_generation\routes.py（片段）
def load_model():
model = MusicGen.get_pretrained(&#39;facebook/musicgen-small&#39;)
return model

def generate_music_tensors(description, duration: int):
model = load_model()
model.set_generation_params(
use_sampling=True,
top_k=250,
duration=duration
)
output = model.generate(
descriptions=[description],
progress=True,
return_tokens=True
)
return output[0]

@music_generation_bp.route(&#39;/&#39;, methods=[&#39;POST&#39;])
def generate_music():
data = request.json

description = data.get(&#39;description&#39;)
duration = data.get(&#39;duration&#39;, 8) # 如果未提供，则默认为 8 秒
print(&quot;Description:&quot;, description)
print(&quot;Duration:&quot;, duration)

如果没有 description:
return jsonify({&#39;error&#39;: &#39;Description is required&#39;}), 400
# 为用户生成唯一密钥
user_id = str(uuid.uuid4()) # 或使用来自身份验证系统的用户 ID
audio_key_prefix = f&quot;generated_music_{user_id}_{description}&quot;

# 生成音乐张量
music_tensors = generate_music_tensors(description, duration)
print(&quot;Music Tensors: &quot;, music_tensors)
...


image_classification\routes.py (代码片段)
# 加载预训练模型
model_path = os.path.join(MODELS_DIR, &#39;multi_output_model.h5&#39;)
model = tf.keras.models.load_model(model_path)

@image_classification_bp.route(&#39;/&#39;, methods=[&#39;POST&#39;])
@limiter.limit(&quot;1/minute&quot;)
def classify_image():
if &#39;file&#39; not in request.files:
return jsonify({&quot;error&quot;: &quot;No file part in the request&quot;}), 400

file = request.files[&#39;file&#39;]

if file.filename == &#39;&#39;:
return jsonify({&quot;error&quot;: &quot;No selected file&quot;}), 400

#file = os.path.join(TEST_IMG_DIR, &#39;blue-dress2.png&#39;)

if file:
# 读取图像文件
img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
img = cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = preprocess_input(img)
img = np.expand_dims(img, axis=0)

# 执行预测
predictions = model.predict(img)

]]></description>
      <guid>https://stackoverflow.com/questions/78841736/how-to-best-integrate-ml-models-to-web-application</guid>
      <pubDate>Wed, 07 Aug 2024 03:39:15 GMT</pubDate>
    </item>
    <item>
      <title>当我导入库时，为什么我的代码会出现错误“sklearn 未定义”？</title>
      <link>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</link>
      <description><![CDATA[我的代码
//
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 pyplot
将 pickle 导入为 pk
从 sklearn 导入 linear_model
从 sklearn.utils 导入 shuffle
从 matplotlib 导入 style
data = pd.read_csv(&quot;student-mat.csv&quot;, sep=&quot;;&quot;)
data = data[[&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;studytime&quot;, &quot;failures&quot;, &quot;absences&quot;]]
print(data.head())
predict = &quot;G3&quot;
x = np.array(data.drop(predict, axis=1))
y = np.array(data[predict])
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
linear = linear_model.LinearRegression()
linear.fit(x_train, y_train)
acc = linear.score(x_test, y_test)
print(acc)
with open(&quot;studentmodel.pickle&quot;, &quot;wb&quot;) as f:
pickle.dump(linear,f)
pickle_in = open(&quot;studentmodel.pickle&quot;, &quot;rb&quot;)
linear = pickle.load(pickle_in)
print(&quot;coefficient:\n&quot;, linear.coef_)
print(&quot;intercept:\n&quot;, linear.intercept_)`

每当我运行此代码时，它都会抛出一个错误，提示名称 sklearn 未定义。但是，这很奇怪，因为我导入了正确的 sklearn 库。
错误
NameError Traceback（最近一次调用最后一次）
Cell In[1]，第 14 行
12 x = np.array(data.drop(predict, axis=1))
13 y = np.array(data[predict])
---&gt; 14 x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
15 linear = linear_model.LinearRegression()
16 linear.fit(x_train, y_train)

NameError：名称“sklearn”未定义
]]></description>
      <guid>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</guid>
      <pubDate>Wed, 07 Aug 2024 02:46:41 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助在 Adruino Nano RP2040 Connect 上运行 CNN 模型</title>
      <link>https://stackoverflow.com/questions/78841395/need-help-to-run-a-cnn-model-on-an-adruino-nano-rp2040-connect</link>
      <description><![CDATA[我正在尝试在 Arduino Nano RP2040 Connect 上运行一个检测咳嗽和打喷嚏的 CNN 模型（使用 Tensorflow 开发），我的模型的输入是大小为 (603,28,1) 的频谱图。但是，我对嵌入式编程还比较陌生，想得到一些关于音频处理和特征提取以及在 Arduino 上运行 CNN 模型等问题的指导。
问题 1：
在此设备上，它有一个称为 PDM 的音频库，可以捕获音频数据并读入特定大小的缓冲区数组，每个索引都是一个 16 位整数，使用此缓冲区数组，我如何将其转换为频谱图以及如何将其调整为正确的大小 (603,28,1) 以输入到我的模型中。
问题 2：
我需要在频谱图图像上运行我的模型，有没有专门用于 Arduino 的 Tensorflow 库？
此外，如果您可以让我了解您在这个项目上的实施流程，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/78841395/need-help-to-run-a-cnn-model-on-an-adruino-nano-rp2040-connect</guid>
      <pubDate>Tue, 06 Aug 2024 23:58:45 GMT</pubDate>
    </item>
    <item>
      <title>什么是Tokens、Top K、Top P？</title>
      <link>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</link>
      <description><![CDATA[我正在学习使用 Google AI Studio，在生成代码片段时，我遇到了这些术语：
constgenerationConfig = {
temperature: 1,
topP: 0.95,
topK: 64,
maxOutputTokens: 8192,
responseMimeType:&quot;text/plain&quot;,
};

我很难理解这些术语的含义。topP、topK 和 maxOutputTokens 是什么。我想了解这些，以便正确使用它们。]]></description>
      <guid>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</guid>
      <pubDate>Tue, 06 Aug 2024 22:55:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>Coral Ordinal AttributeError：'str' 对象没有属性 'name'</title>
      <link>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</link>
      <description><![CDATA[我正在开发一个使用 Python 和 Tensorflow 中的有序回归/分类的项目。我发现 pip 包 coral-ordinal 实现了有序回归并包含有用的损失函数。然而，当我浏览他们的 Google Colab 教程时，我得到了错误
AttributeError: &#39;str&#39; 对象没有属性 &#39;name&#39; 

运行 model.fit() 时。
当我尝试在其他代码中使用它时也会出现此错误。发生错误之前的代码如下
import tensorflow as tf
print(&quot;Tensorflow version&quot;, tf.__version__)

import coral_ordinal as coral
print(&quot;CORAL Ordinal version:&quot;, coral.__version__)

############################
### SETTINGS
############################

# 超参数
random_seed = 1 # 尚未使用
learning_rate = 0.05
batch_size = 128
num_epochs = 2

# 架构
NUM_CLASSES = 10

# 获取并格式化 mnist 数据
(mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data()

# 拆分验证数据集以进行早期停止
from sklearn import model_selection
mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \
model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1)

print(&quot;训练图像的形状：&quot;, mnist_images.shape)

print(&quot;训练标签的形状：&quot;, mnist_labels.shape)

print(&quot;测试图像的形状：&quot;, mnist_images_test.shape)

print(&quot;测试标签的形状：&quot;, mnist_labels_test.shape)

print(&quot;验证图像的形状：&quot;, mnist_images_val.shape)
print(&quot;验证标签的形状：&quot;, mnist_labels_val.shape)

# 也重新调整为 0-1 范围。
dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels, tf.int64)))
dataset = dataset.shuffle(1000).batch(batch_size)

test_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_test, tf.int64)))
#test_dataset = test_dataset.shuffle(1000).batch(batch_size)
# 这里我们不对测试数据集进行打乱。
test_dataset = test_dataset.batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_val, tf.int64)))
val_dataset = val_dataset.shuffle(1000).batch(batch_size)

def create_model(num_classes):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape = (28, 28, )))
model.add(tf.keras.layers.Dense(128, 激活 = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(32,activation = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.1))
# 具有一定数量的类别/等级/标签的有序输出层。
# 未指定激活函数，因此将输出累积对数。
model.add(coral.CoralOrdinal(num_classes))
return model

model = create_model(NUM_CLASSES)

# 请注意，模型生成的输出比类别数量少 1。
model.summary()

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),
loss = coral.OrdinalCrossEntropy(num_classes = NUM​​_CLASSES),
metrics = [coral.MeanAbsoluteErrorLabels()])

# 这在 CPU 上大约需要 5 分钟，在 GPU 上大约需要 2.5 分钟。
history = model.fit(dataset, epochs = 5, validation_data = val_dataset,
callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)])

如何解决这个问题？我会在他们的 github 上提出问题，但最后一次回复是 2 年前，所以我怀疑维护者是否会回复。
或者，如果没有任何现实的选择来完成这项工作，是否有任何替代方案可以帮助 tensorflow 中的有序回归/分类？]]></description>
      <guid>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</guid>
      <pubDate>Tue, 06 Aug 2024 20:41:21 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 错误 - 需要使用适当的编译器标志进行重建 [关闭]</title>
      <link>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</link>
      <description><![CDATA[2024-08-06 14:18:52.654763: 
I tensorflow/core/platform/cpu_feature_guard.cc:210]。此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。

要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。

每当我尝试运行任何类型的面部识别代码时，我都会收到这种错误。即使它与面部识别无关，而只是常规的张量流，我也会收到此错误。有人能帮忙吗？
from deepface import DeepFace
import os

os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
img1 = &#39;reference.jpg&#39;
img2 = &#39;reference1.jpg&#39;

model_name = &#39;Facenet&#39;

result = DeepFace.verify(
img1_path=img1,
img2_path=img2,
model_name=model_name
)
]]></description>
      <guid>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</guid>
      <pubDate>Tue, 06 Aug 2024 08:58:22 GMT</pubDate>
    </item>
    <item>
      <title>NLTK bleu 分数明显高于 Sacrebleu bleu 分数</title>
      <link>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</link>
      <description><![CDATA[我试图将 bleu-4 分数的结果与 sacrebleu 和 NLTK corpus bleu 包进行比较，但结果之间的差异非常显著。
对于 NLTK corpus bleu，我获得了非常高的 bleu 分数（0.47、0.39、0.33、0.28）
但对于 sacrebleu，我获得了较低的分数（19.57、10.78、7.07、5.15），sacrebleu 分数已经将它们乘以 100，而 NLTK 没有
这是我计算这些分数的实现：
def compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name):
# 确保长度匹配
print(f&quot;Number of references: {len(all_references)}&quot;)
print(f&quot;假设数量：{len(all_hypotheses)}&quot;)

if len(all_references) != len(all_hypotheses):
raise ValueError(&quot;参考文献和假设的数量必须匹配。&quot;)

sacrebleu_scores = corpus_bleu(all_hypotheses, [all_references]).scores

bleu_score1 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(1.0, 0.0, 0.0, 0.0))
bleu_score2 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.5, 0.5))
bleu_score3 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.33, 0.33, 0.33))
bleu_score4 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.25, 0.25, 0.25, 0.25))


这是我生成预测的函数：
def assess_and_save(loader, dataset_type, folder_name):
model.eval()
all_references = []
all_hypotheses = []

sample_file_path = os.path.join(folder_name, &#39;samples.txt&#39;)
with open(sample_file_path, &#39;a&#39;, encoding=&#39;utf-8&#39;) as sample_file:
with torch.no_grad():
for Skeletons, Labels in loader:
Skeletons, Labels = Skeletons.to(device), Labels.to(device)

Outputs = model(skeletons, Labels[:, :-1])
Predictions = torch.argmax(outputs, dim=-1)

for i in range(predictions.size(0)):
Reference = tokenizer.decode(labels[i], skip_special_tokens=True)
Hypothesis = tokenizer.decode(predictions[i], skip_special_tokens=True)

# 调试：打印一些样本
if i &lt; 25：# 仅打印前 5 个样本
sample_text = f&quot;样本 {i+1}:\n参考：{reference}\n假设：{hypothesis}\nNew\n&quot;
print(sample_text)
sample_file.write(sample_text)

all_references.append(reference)
all_hypotheses.append(hypothesis)

# 检查是否有空引用或假设
empty_references = [ref for ref in all_references if not ref.strip()]
empty_hypotheses = [hyp for hyp in all_hypotheses if not hyp.strip()]

print(f&quot;空引用数：{len(empty_references)}&quot;)
print(f&quot;空假设数：{len(empty_hypotheses)}&quot;)

# 过滤掉空假设和相应的引用
non_empty_indices = [i for i, hyp in enumerate(all_hypotheses) if hyp.strip()]
all_references = [all_references[i] for i in non_empty_indices]
all_hypotheses = [all_hypotheses[i] for i in non_empty_indices]

compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name)


有什么建议说我哪里错了吗？
编辑：
问题是因为我没有在计算 NLTK bleu 分数之前拆分参考文献和假设。sacrebleu 的默认行为是先拆分它们。]]></description>
      <guid>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</guid>
      <pubDate>Tue, 06 Aug 2024 07:40:08 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>我们如何才能优化 Longformer 模型以提高效率，同时又不损害 NLP 任务中的长期上下文理解？</title>
      <link>https://stackoverflow.com/questions/78835795/how-can-we-optimize-longformer-models-for-efficiency-without-compromising-long-t</link>
      <description><![CDATA[Longformer 模型使用全局和局部注意力机制的混合来处理长序列，使其适合于文档分类、摘要和共指解析等任务。优化这些模型涉及平衡计算效率与维护长期上下文的需求。
可以应用哪些特定技术或修改来增强 Longformer 模型的性能？
是否有特定的训练策略、修剪方法或硬件考虑因素可以帮助实现这种平衡？深入了解 Longformer 模型已成功优化的实际实施和案例研究将非常有价值。]]></description>
      <guid>https://stackoverflow.com/questions/78835795/how-can-we-optimize-longformer-models-for-efficiency-without-compromising-long-t</guid>
      <pubDate>Mon, 05 Aug 2024 17:47:13 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH, YAML_FILE_PATH)`

我在运行训练脚本时收到此未来错误，并且我正在努力解决该错误：FutureWarning：torch.cuda.amp.autocast(args...) 已弃用。请改用 torch.amp.autocast(&#39;cuda&#39;, args...)。使用 torch.cuda.amp.autocast(amp)]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>nltk 是适合 NLP 的优秀 Python 库吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78831441/is-nltk-good-python-library-for-nlp</link>
      <description><![CDATA[我不确定是否应该使用 TensorFlow 或 NLTK 来完成我的 NLP 任务。两者似乎都是很受欢迎的选择，但我不清楚哪一个更适合我这个水平的人。
TensorFlow 似乎是一个功能强大的库，它提供了广泛的机器学习和深度学习工具，包括对神经网络和大规模机器学习模型的支持。它似乎用途广泛，可用于复杂的 NLP 任务，如情绪分析、文本生成和翻译。但是，我不知道它对于像我这样的初学者来说是否太高级了，因为我读到过它的学习曲线很陡峭。
另一方面，NLTK（自然语言工具包）通常推荐给那些刚接触 NLP 的人。它为基本的 NLP 任务（如标记化、解析和词干提取）提供了易于使用的界面和功能。它似乎更侧重于传统的 NLP 方法，可以作为理解文本处理和分析基础知识的一个很好的起点。
我寻求指导，在转向 TensorFlow 之前，我是否应该从 NLTK 开始，在 NLP 中打下坚实的基础，或者我是否应该考虑采用其他方法。]]></description>
      <guid>https://stackoverflow.com/questions/78831441/is-nltk-good-python-library-for-nlp</guid>
      <pubDate>Sun, 04 Aug 2024 15:28:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Skip-Gram 实现产生了错误的结果？</title>
      <link>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</guid>
      <pubDate>Fri, 02 Aug 2024 07:15:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的激活函数[关闭]</title>
      <link>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</link>
      <description><![CDATA[机器学习中的激活函数是什么意思。我浏览了大多数文章和视频，每个人都提到或将其与神经网络进行比较。我是机器学习的新手，对深度学习和神经网络不太熟悉。所以，有人能给我解释一下激活函数到底是什么吗？而不是用神经网络来解释。我在学习逻辑回归的 Sigmoid 函数时就被这种模棱两可的感觉所困扰。]]></description>
      <guid>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</guid>
      <pubDate>Tue, 20 Mar 2018 18:21:08 GMT</pubDate>
    </item>
    </channel>
</rss>