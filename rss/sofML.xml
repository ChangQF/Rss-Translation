<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 27 May 2024 09:16:21 GMT</lastBuildDate>
    <item>
      <title>验证码解决：-任何人都可以帮我解决这个验证码问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78537910/captcha-solving-can-anyone-help-me-to-solve-this-captcha-problem</link>
      <description><![CDATA[导入CV2
导入 pytesseract
从 PIL 导入图像
image = cv2.imread(&#39;D:\Automation\captchas\captcha.png&#39;)
如果图像为无：
print(&quot;错误：无法加载图像。请检查文件路径是否正确以及文件是否存在。&quot;)
别的：
灰色 = cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)
_, 阈值 = cv2.threshold(gray, 8, 255, cv2.THRESH_BINARY)

内核 = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
morphed = cv2.morphologyEx（阈值，cv2.MORPH_CLOSE，内核）
中值 = cv2.medianBlur(变形, 3)


cv2.imshow(&#39;处理后的图像&#39;, 中位数)
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.imwrite(&#39;D:\Automation\captchas\morphed_image.png&#39;, 中值)

pytesseract.pytesseract.tesseract_cmd = r&#39;C:/Program Files/Tesseract-OCR/tesseract.exe&#39;
image_path = “D:\Automation\captchas\morphed_image.png”
使用 Image.open(image_path) 作为 img：
    文本 = pytesseract.image_to_string(image_path, config=&#39; -c tessedit_char_whitelist=0123456789&#39;)
打印（“文本：”，文本）

在此处输入图片说明
请我解决这个问题......................]]></description>
      <guid>https://stackoverflow.com/questions/78537910/captcha-solving-can-anyone-help-me-to-solve-this-captcha-problem</guid>
      <pubDate>Mon, 27 May 2024 08:18:20 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降 Logistic 回归和协变量缩放</title>
      <link>https://stackoverflow.com/questions/78537725/gradient-descent-logistic-regression-and-covariate-scaling</link>
      <description><![CDATA[我正在尝试理解逻辑回归和梯度下降。这有多难，对吧？好吧，我使用了这个网站中的示例
mydata &lt;- read.csv(“https://stats.idre.ucla.edu/stat/data/binary.csv”)
逻辑= glm（承认〜gre + gpa +因子（排名），数据= mydata，家庭=“二项式”）
betahat = 逻辑$系数

好的，这就是梯度下降应该收敛的结果。我将数据转换为经典格式并实现了 logit 回归的梯度（或者我做了吗？）
# 经典格式的数据
X = cbind(1, mydata$gre, mydata$gpa, mydata$rank==2, mydata$rank==3, mydata$rank==4)
y = mydata$承认

# 对数梯度
Logistics_cost_gradient &lt;- 函数(X, y, b){
  yhat = 1/(1+exp(-X %*% b))
  grad = as.vector(crossprod(X, (y-yhat)))/nrow(X)
  回报（毕业）
}

# 健全性检查：最佳梯度接近于零
Logistics_cost_gradient(X,y,betahat)
# 健全性检查已通过 

所以让我们尝试一些梯度下降：
b = rep(0,ncol(X)) # 起始值

epoch = 1 # 梯度下降步数

while(纪元&lt;1000){
  梯度=logistic_cost_gradient(X,y,b)
  b = b - grad/epoch # 减小步长
  历元 = 历元 + 1
  #打印(b)
}

我知道，这是一个粗略的停止标准，但这似乎并不重要。结果值是垃圾。出了什么问题？
我唯一的猜测是，这与 X 的第二列的比例有关，它比其他所有列都大得多。我预计如果我将一列除以 100，则该列的系数必须乘以 100。这似乎是正确的：
mydata$gre = mydata$gre/100 
Logistics_rescale = glm(承认 ~ gre + gpa + 因子(排名), data = mydata, family=“二项式”)
betahat_rescale = Logistics_rescale$系数


也许梯度下降与 X 的尺度有问题？无论我尝试如何缩放 X，我都会从梯度下降中得到垃圾结果。有什么提示吗？我为此失去了理智。
编辑：我还检查了渐变中是否存在符号错误。即使当我改变渐变的符号时，我也会得到垃圾值。
这也不是由于数据不够复杂，如 此处，概率的拟合值在 6% 到 75% 之间。]]></description>
      <guid>https://stackoverflow.com/questions/78537725/gradient-descent-logistic-regression-and-covariate-scaling</guid>
      <pubDate>Mon, 27 May 2024 07:34:43 GMT</pubDate>
    </item>
    <item>
      <title>GPU 上的 Pytorch 比 CPU 上的 numpy 慢很多？</title>
      <link>https://stackoverflow.com/questions/78537571/pytorch-in-gpu-is-much-slower-than-numpy-on-cpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78537571/pytorch-in-gpu-is-much-slower-than-numpy-on-cpu</guid>
      <pubDate>Mon, 27 May 2024 06:56:13 GMT</pubDate>
    </item>
    <item>
      <title>当我们进行这些更改时，为什么 DecisionTreeClassifiers 的准确性会发生变化？</title>
      <link>https://stackoverflow.com/questions/78537242/why-does-the-decisiontreeclassifiers-accuracy-change-when-we-do-these-changes</link>
      <description><![CDATA[我正在使用DecisionTreeClassifier训练模型，就像LinearRegression / LogisticRegression算法测试和训练数据集一样，我使用X的2D数组y 值的值和一维数组[y.values]，但精度[使用 .score()] 是 75%，但是当我没有使用更改时y 值转换为一维数组时，准确率达到 100% 为什么会发生这种情况？
培训：
X = new_df.drop(&#39;salary_more_then_100k&#39;, axis=&#39;columns&#39;)
y = new_df.salary_more_then_100k

X_train, X_test, y_train, y_test = train_test_split(X, y)

树 = DecisionTreeClassifier()
Tree.fit(X_train,y_train)

以上在我的测试数据集中给出了 100% 的准确度，但是
如果我像这样训练[y.values]
X_train, X_test, y_train, y_test = train_test_split(X, y.values)

那么准确率会下降到 75%，为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78537242/why-does-the-decisiontreeclassifiers-accuracy-change-when-we-do-these-changes</guid>
      <pubDate>Mon, 27 May 2024 04:54:55 GMT</pubDate>
    </item>
    <item>
      <title>随机梯度下降算法无法正常工作（Python）</title>
      <link>https://stackoverflow.com/questions/78537136/stochastic-gradient-descent-algorithm-does-not-work-properly-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78537136/stochastic-gradient-descent-algorithm-does-not-work-properly-python</guid>
      <pubDate>Mon, 27 May 2024 04:02:36 GMT</pubDate>
    </item>
    <item>
      <title>如何将张量（pytorch）作为ndarray放入函数中，在每次操作后保留梯度函数？</title>
      <link>https://stackoverflow.com/questions/78536832/how-put-a-tensor-pytorch-as-ndarray-in-a-function-keep-a-gradient-function-af</link>
      <description><![CDATA[如何将 PyTorch 张量作为 ndarray 传递给模拟函数而不丢失梯度信息，或者是否有一种解决方法允许该函数接受张量，同时保留前向和反向传播的必要属性？
类型错误：&#39;model&#39;必须是类似数组的，数据类型为，得到 
T
导入simulation.dc作为dc
将 numpy 导入为 np

?model = import(//模型路径)
模型=张量（...，requiere_true = True）
dpred = dc.fields.make_a_sintetic_data(模型)
打印（类型（dpred））

&gt;&gt;&gt;张量，fn_gradient(...) 

]]></description>
      <guid>https://stackoverflow.com/questions/78536832/how-put-a-tensor-pytorch-as-ndarray-in-a-function-keep-a-gradient-function-af</guid>
      <pubDate>Mon, 27 May 2024 00:59:11 GMT</pubDate>
    </item>
    <item>
      <title>使用什么样的CNN模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78535852/what-kind-of-cnn-model-is-used</link>
      <description><![CDATA[下面的代码使用了什么样的模型、什么层和架构：
模型 = nn.Sequential()

model.add_module(&#39;conv1&#39;, nn.Conv2d(in_channels=3, out_channels=32, kernel_size=2, padding=1))
model.add_module(&#39;relu1&#39;, nn.ReLU())        
model.add_module(&#39;pool1&#39;, nn.MaxPool2d(kernel_size=2))  
model.add_module(&#39;dropout1&#39;, nn.Dropout(p=0.4)) 

model.add_module(&#39;conv2&#39;, nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, padding=1))
model.add_module(&#39;relu2&#39;, nn.ReLU())        
model.add_module(&#39;pool2&#39;, nn.MaxPool2d(kernel_size=2))   
model.add_module(&#39;dropout2&#39;, nn.Dropout(p=0.4))

model.add_module(&#39;conv3&#39;, nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, padding=1))
model.add_module(&#39;relu3&#39;, nn.ReLU())        
model.add_module(&#39;pool3&#39;, nn.MaxPool2d(kernel_size=2))
model.add_module(&#39;dropout3&#39;, nn.Dropout(p=0.4))

model.add_module(&#39;conv4&#39;, nn.Conv2d(in_channels=128, out_channels=256, kernel_size=2, padding=1))
model.add_module(&#39;relu4&#39;, nn.ReLU()) 

model.add_module(&#39;conv5&#39;, nn.Conv2d(in_channels=256, out_channels=512, kernel_size=2, padding=1))
model.add_module(&#39;relu5&#39;, nn.ReLU())

model.add_module(&#39;pool4&#39;, nn.AvgPool2d(kernel_size=8)) 
model.add_module(&#39;展平&#39;, nn.Flatten())

model.add_module(&#39;fc&#39;, nn.Linear(512, 1)) 
model.add_module(&#39;sigmoid&#39;, nn.Sigmoid())
]]></description>
      <guid>https://stackoverflow.com/questions/78535852/what-kind-of-cnn-model-is-used</guid>
      <pubDate>Sun, 26 May 2024 16:33:31 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测平台的 Transformer 模型 - 求建议</title>
      <link>https://stackoverflow.com/questions/78533853/transformer-model-for-time-series-prediction-plateaus-seeking-suggestion</link>
      <description><![CDATA[我正在使用一个简单的普通转换器来处理时间序列数据来预测特定的连续值。之前，我通过随机森林实现了一些准确性，并希望使用变压器来提高它。然而，无论其复杂程度如何，我的变压器模型的性能似乎都趋于稳定。
我尝试过批量大小（16、32、64、128、200）、各种学习率和不同的标准化技术。在测试了不同的损失函数（MSE、MAE、Cauchy Loss、Huber损失和RMSE）后，我发现批量大小为200，学习率为0.005，MSE表现最好。
我还可视化了注意力头来识别重要特征，训练了变分自动编码器并在我的变压器中使用了它的权重，并尝试了迁移学习。尽管做出了这些努力，该模型的性能仍然稳定。
我最近看到了Anthropic 的这篇文章，但还没有&#39;尚未实施其建议。
有人对进一步改进或研究模型有建议吗？我相信它在某个时候会陷入困境，任何见解都将不胜感激。请注意，我已经绘制了异常值，并观察了具有高 MSE 的样本之间的 MSE 及其相互关系，但我没有看到任何可以解释该问题的内容。下面，我粘贴了我正在使用的一个简单的变压器模型。以及显示训练 (25,000) 和验证 (1000) 数据集的损失曲线的图表。 X 轴是纪元，y 轴是 MSE。
class PositionalEncoding(nn.Module):

    def __init__(self, d_model, max_len=1000):
        super(PositionalEncoding, self).__init__()       
        pe = torch.zeros(max_len, d_model)
        位置 = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        #div_term = torch.randn(19)
        print(&#39;div term&#39;, div_term.shape)
        print(&#39;正弦&#39;,torch.sin(position * div_term).shape)
        print(&#39;cosine&#39;,torch.cos(position * div_term).shape)
        print(&#39;位置&#39;, 位置.形状)
        print(&#39;pe&#39;, pe.shape)
        pe[:, 0::2] = torch.sin(位置 * div_term)  
        pe[:, 1::2] = torch.cos(位置 * div_term) 
        pe = pe.unsqueeze(0).transpose(0, 1)
 
        self.register_buffer(&#39;pe&#39;, pe)

    def 前向（自身，x）：
        返回 x + self.pe[:x.size(0), :]
       



def focus_hook(模块, 输入, 输出):
    查询、键、值 = 输入[0]、输入[1]、输入[2] 

TransAm 类（nn.Module）：
    def __init__(self,feature_size=20,num_layers=1,dropout=0.1):
        super(TransAm, self).__init__()
        self.model_type = &#39;变压器&#39;
        
        self.src_mask = 无
        self.pos_encoder = PositionalEncoding(feature_size)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        
        self.decoder = nn.Linear(feature_size,1)
        self.init_weights()
        
        
 
        
    def init_weights(自身):
 
        nn.init.constant_(self.decoder.bias.data, 0)

 
        nn.init.xavier_uniform_(self.decoder.weight)

    def 转发（自身，src）：
        如果 self.src_mask 为 None 或 self.src_mask.size(0) != len(src):
            设备 = src.设备
            mask = self._generate_square_subsequent_mask(len(src)).to(device)
            self.src_mask = 掩码

        src = self.pos_encoder(src)
        输出 = self.transformer_encoder(src,self.src_mask) 
        输出 = self.decoder(输出)
        返回输出

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float(&#39;-inf&#39;)).masked_fill(mask == 1, float(0.0))
        返回掩码
    

模型 = TransAm().to(设备)



 
input_tensor = torch.tensor(np.random.rand(32, 20), dtype = torch.float32).to(device)
print(&quot;输入张量形状：&quot;, input_tensor.shape)
输出=模型（输入张量）
输出形状

]]></description>
      <guid>https://stackoverflow.com/questions/78533853/transformer-model-for-time-series-prediction-plateaus-seeking-suggestion</guid>
      <pubDate>Sat, 25 May 2024 22:18:15 GMT</pubDate>
    </item>
    <item>
      <title>Yolov4如何在暗网中预测多图像txt</title>
      <link>https://stackoverflow.com/questions/78533567/yolov4-how-to-predict-multi-image-txt-in-darknet</link>
      <description><![CDATA[在 Yolov7 中，我使用此代码来测试整个文件夹图像。
python test.py --save-txt --data data/coco.yaml --save-conf --conf 0.1 --weights yolov7_20240316best.pt --task test --name 0316conf01

现在我需要在yolov4中预测test.txt（包括所有图像路径）。
我尝试了这个命令，但没有成功：
暗网探测器测试数据/obj.data cfg/yolo-obj.cfg backup/yolo-obj_best.weights -thresh 0.9 -dont_show data/test.txt result.txt
]]></description>
      <guid>https://stackoverflow.com/questions/78533567/yolov4-how-to-predict-multi-image-txt-in-darknet</guid>
      <pubDate>Sat, 25 May 2024 19:51:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中仅通过歌曲的索引来播放歌曲？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78533401/how-to-play-a-song-in-python-by-only-index-of-it</link>
      <description><![CDATA[我正在使用 GTZAN 数据集音乐流派分类数据集。训练模型后（我仍在模型的笔记本中），我运行了这个脚本，并预测了流派输出，直到这里为止一切都很完美，但我不知道如何播放这首歌，并确认是否类型是正确的。通常它是通过文件路径完成的，但我只有歌曲的索引。
将 numpy 导入为 np
导入pygame
导入时间

# 选择样本数据点
index = 0 # 从测试集中选择一个索引
Sample = X_test[index] # 从测试集中选择一个样本
Sample = Sample[np.newaxis, ...] # 转换为适合模型的形状

# 别猜
Prediction = model.predict(sample) # 使用模型进行预测
Predicted_index = np.argmax(prediction, axis=1) # 查找预测索引

# 打印预测结果
Predicted_genre = converter.inverse_transform(predicted_index)[0] # 将预测类型转换为标签
Expected_genre = converter.inverse_transform([y_test[index]])[0] # 将预期类型转换为标签

print(&quot;预期类型：{}，预测类型：{}&quot;.format(expected_genre, Predicted_genre))
]]></description>
      <guid>https://stackoverflow.com/questions/78533401/how-to-play-a-song-in-python-by-only-index-of-it</guid>
      <pubDate>Sat, 25 May 2024 18:45:50 GMT</pubDate>
    </item>
    <item>
      <title>langchain RetrievalQA 错误：ValueError：缺少一些输入键：{'query'}</title>
      <link>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</link>
      <description><![CDATA[在 RAG 项目中，我使用 langchain。当我使用查询输入运行 QA 链时，此错误不断出现：
----&gt;;结果 = qa_chain({&#39;查询&#39;: 问题})
ValueError：缺少一些输入键：{&#39;query&#39;}

这是我的代码：
from langchain.chains import RetrievalQA
从 langchain.prompts 导入 PromptTemplate

# 构建提示
template = &quot;&quot;&quot; 根据以下上下文回答问题。
    语境：
    {语境}
    ------------------
    问题：{查询}
    答案：“”

# 法学硕士链
QA_CHAIN_PROMPT = PromptTemplate.from_template(模板)
qa_chain = RetrievalQA.from_chain_type(
    嗯，
    检索器=vectordb.as_retriever(),
    return_source_documents=真，
    chain_type_kwargs={“提示”: QA_CHAIN_PROMPT}
）

Question =“这篇研究论文使用了什么方法？”

结果 = qa_chain({&#39;查询&#39;: 问题})

# 查看查询结果
结果[“结果”]
# 检查我们所在的源文档 
结果[“源文档”][0]
]]></description>
      <guid>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</guid>
      <pubDate>Fri, 24 May 2024 21:23:49 GMT</pubDate>
    </item>
    <item>
      <title>使用离散傅里叶变换比较图像</title>
      <link>https://stackoverflow.com/questions/78528869/compare-images-using-discrete-fourier-transform</link>
      <description><![CDATA[我想使用离散傅里叶变换将示例图像与文件夹中的每个图像进行比较。这些图像是直接拍摄的肖像。我使用face_recognition模块来查找面孔。然而，性能不高，并且图像之间的结果非常相似，并且没有产生预期的结果。
&lt;前&gt;&lt;代码&gt;导入cv2
导入人脸识别
将 numpy 导入为 np
从 skimage.metrics 导入结构相似度为 ssim

# 加载预训练的 dlib 模型用于人脸特征点检测
def dft_magnitude(图像):
    Fourier = cv2.dft(np.float32(图像), flags=cv2.DFT_COMPLEX_OUTPUT)

    fourier_shift = np.fft.fftshift(fourier)

    幅度 = 20 * np.log(cv2.magnitude(fourier_shift[:, :, 0], fourier_shift[:, :, 1]) + 1700)
    回波幅度
    

def比较图像（img1，img2）：
    dft_img1 = dft_magnitude(img1)
    dft_img2 = dft_magnitude(img2)
    
    # 将幅度图像标准化到范围 [0, 1]
    dft_img1 = cv2.normalize(dft_img1, 无, 0, 1, cv2.NORM_MINMAX)
    dft_img2 = cv2.normalize(dft_img2, 无, 0, 1, cv2.NORM_MINMAX)

    # 计算SSIM
    ssim_similarity = ssim(dft_img1, dft_img2, data_range=dft_img1.max() - dft_img1.min())
    
    返回 ssim_similarity

def find_face(图像):
    face_locations = face_recognition.face_locations(image) #y1, x2, y2, x1 - 矩形形状的两个角
    如果不是face_locations：
        返回无
    上、右、下、左 =face_locations[0]
    脸部图像=图像[上：下，左：右]

    返回人脸图像

# 用法示例
样本图像 = cv2.imread(&#39;1.jpg&#39;)
target_image = cv2.imread(&#39;3.jpg&#39;)

相似度得分=比较图像（样本图像，目标图像）
print(f&#39;相似度得分：{similarity_score}&#39;)


我尝试改变幅度，但结果并非预期。请帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78528869/compare-images-using-discrete-fourier-transform</guid>
      <pubDate>Fri, 24 May 2024 13:25:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的分段似乎没有保存？关于totalsegmentator [关闭]</title>
      <link>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</link>
      <description><![CDATA[我一步步按照您的教程进行操作，但得到的结果类似于分段未保存。
这是我输入的语句和得到的结果：
(d:\totalsegmentotar.conda) D:\totalsegmentotar&gt;TotalSegmentator -i hip_left.nii.gz -o 分段 -ta hip_implant

如果您使用此工具，请引用：https://pubs.rsna.org/doi/10.1148/ryai.230024

未检测到 GPU。在CPU上运行。这可能会非常慢。 &#39;--fast&#39; 或 --roi_subset 选项可以帮助减少运行时间。
生成粗糙的身体分割...
重新采样...
1.93 秒内重新采样
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ ██| 1/1 [00:00&lt;00:00, 1.12it/s]
预测12.95秒后
重新采样...
警告：无法裁剪，因为未检测到前景
从 (333, 333, 539) 裁剪到 (333, 333, 539)
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ | 64/64 [04:27&lt;00:00, 4.18s/it]
预测 288.96 秒
保存分段...
0%| | 0/1 [00:00
可以看到分割没有保存，我用切片器软件看确实没有预测结果，什么也没有显示。
当我使用`-tatotal时，分割器进度条发生变化，但不幸的是它似乎没有保存分割的结果。这是我的输出，以及在切片器 5.6.2 中打开的输出文件夹和图像，但没有显示任何内容。
这是我的 powershell 输出
这是我的输出文件夹和在切片器 5.6.2 中打开的图像]]></description>
      <guid>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</guid>
      <pubDate>Wed, 22 May 2024 07:52:18 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“无法将类强制到data.frame？”</title>
      <link>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</link>
      <description><![CDATA[第 20 行出现问题：x3 &lt;- lm(Salary ~ ...

&lt;块引用&gt;
  as.data.frame.default(data) 中的错误：无法将类‘c(&quot;train&quot;, &quot;train.formula&quot;)’强制转换为 data.frame

如何解决？
附加（击球手）
击球手

库（插入符号）
设置.种子(123)
# 定义训练控制
设置.种子(123) 
train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10)
# 训练模型
x2 &lt;- 训练（工资〜.，数据= x，方法=“lm”，
               trControl = 训练.control)
# 总结结果
打印（x）
x3 &lt;- lm(工资 ~ poly(AtBat,3) + poly(Hits,3) + poly(Walks,3) + poly(CRuns,3) + poly(CWalks,3) + poly(PutOuts,3),数据 = x2)
摘要(x3)
MSE = 均值(x3$残差^2)
print(&quot;均方误差：&quot;)
打印（MSE）
]]></description>
      <guid>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</guid>
      <pubDate>Fri, 15 Nov 2019 05:09:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在train_test_split中选择RandomState？ [关闭]</title>
      <link>https://stackoverflow.com/questions/49561882/how-to-choose-randomstate-in-train-test-split</link>
      <description><![CDATA[据我所知，数据分割中的随机状态参数每次改变都会导致不同的精度。因此，我的算法的性能随着每次运行而变化。对于我的大学报告，我需要确定报告模型最终准确性的最合适方法。我应该选择观察到的最大准确度，对不同随机状态的准确度进行平均，还是使用其他方法来确保对模型性能进行可靠且具有代表性的衡量？根据该领域的既定实践，在这种情况下准确报告模型性能的推荐方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/49561882/how-to-choose-randomstate-in-train-test-split</guid>
      <pubDate>Thu, 29 Mar 2018 17:21:53 GMT</pubDate>
    </item>
    </channel>
</rss>