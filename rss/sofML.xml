<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 24 Aug 2024 15:15:17 GMT</lastBuildDate>
    <item>
      <title>使用 LSTM 进行 ASL 识别</title>
      <link>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</link>
      <description><![CDATA[实时模型是否可以转换为预先录制的视频上传？
例如，我将训练一个用于手语的 LSTM 模型，以实现实时识别。现在我想将其集成到 Android Studio 中的移动应用程序中，并使其预先录制。
我想将其他模型迁移到我的项目中。
我尝试了一些技术，例如 CNN 识别，但没有奏效。]]></description>
      <guid>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</guid>
      <pubDate>Sat, 24 Aug 2024 13:02:58 GMT</pubDate>
    </item>
    <item>
      <title>深度学习与机器学习中“学习率高、大、低、小”分别代表什么意思？</title>
      <link>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</link>
      <description><![CDATA[在深度学习和机器学习中，经常会说学习率高、大、低或小，但我不知道这是什么意思。 *问题是关于如何用英语表达学习率。
例如，我在 PyTorch 中将学习率 0.000001 和 100.0 设置为 SGD() 的 lr 参数，如下所示：
示例 A：
 # ↓ 这里 ↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=0.000001)

示例 B：
 # ↓ 这里↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=100.0)

现在，示例 A 和 示例 B 的学习率是高、大、低还是小，而常见的学习率是 0.1 ~ 0.001？我不知道如何用英语表达学习率。]]></description>
      <guid>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</guid>
      <pubDate>Sat, 24 Aug 2024 12:39:22 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Tessaract 进行 OCR</title>
      <link>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</link>
      <description><![CDATA[我正在研究牙膏卷边的 OCR。所以我用手机相机拍了这些照片。我使用了 opencv 和 tessaract。这是来自 chatgpt 的基本代码。它根本不起作用，关于此的任何建议或帮助。这里是图片。

有人请向我解释我该怎么做或建议我一些容易做的事情。]]></description>
      <guid>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</guid>
      <pubDate>Sat, 24 Aug 2024 12:00:40 GMT</pubDate>
    </item>
    <item>
      <title>Resnet 内存不足：torch.OutOfMemoryError：CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</guid>
      <pubDate>Sat, 24 Aug 2024 08:53:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建具有点数据检索功能的交互式聚类可视化？</title>
      <link>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</link>
      <description><![CDATA[我正在开展一个项目，需要可视化数据点集群，并允许用户单击这些点来检索有关它们的详细信息。
我已经实现了 K-means（或任何聚类算法，如 dbscan）聚类并使用 Matplotlib 可视化了集群，但我不确定如何设置交互性，以便单击某个点即可显示其相关数据。
但我的主要问题是，我是否应该以特殊方式存储和构造数据以再次检索它？例如，我想识别靠近集群边界的点并检索它们的信息（可以通过单击或工具提示或任何方法）。]]></description>
      <guid>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</guid>
      <pubDate>Sat, 24 Aug 2024 08:29:56 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在包含其他业务逻辑的主 Web 应用程序中部署我的 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</link>
      <description><![CDATA[我正在为我的学期做一个小项目。我正在使用 flask。

我在这个应用程序中有一些业务逻辑，其中收集了一些数据。
我想将这些数据发送到我的 ML 模型。
那么我应该在不同的服务器上部署我的 ML 模型并使用其端点吗？
这不会增加响应时间吗？
此外，由于我将其部署在 render.com 或任何其他免费服务上的低内存免费服务器上。
我的主要业务逻辑也将占用一些内存。
此外，我应该如何分离我的 ML 模型，比如将它部署到整个其他服务器上并对其进行配置，然后将我的主要逻辑部署到其他服务器上？
那么这里应该做什么？
你的想法会对我这个项目有所帮助。
感谢您的宝贵回复
我正在创建一个名为 mainapp 的主 flask 应用程序。
在其中，我在单独的文件中编写了一些业务逻辑，并将它们作为包导入到我的 routes.py 文件中。
现在我也对 ML 模型做了同样的事情。这是一个简单的情感分析模型，我将传递句子并接收其分数。]]></description>
      <guid>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</guid>
      <pubDate>Sat, 24 Aug 2024 07:15:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</guid>
      <pubDate>Sat, 24 Aug 2024 03:11:26 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10 自定义训练——导入 YOLO 还是 YOLOv10？</title>
      <link>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</link>
      <description><![CDATA[我希望对我拥有的数据集使用 YoloV10n 进行自定义训练（到目前为止我一直在使用 YoloV8），但我不确定是否要使用/导入 YOLO 或 YOLOv10。
我最初尝试使用 YOLOv10，并能够成功完成自定义训练和推理。但是，由于断言不一致，我无法将模型导出到 tflite。然后我切换回 YOLO（从预先训练的 yolov10.pt 模型开始），并能够将其导出到 tflite。此外，Ultralytics 的官方文档确实指导如何使用 YOLO（而不是 YOLOv10）进行 YoloV10 训练。另一方面，RoboFlow 教程确实指导如何使用 YOLOv10... 🤔
我应该使用 from ultralytics import YOLOv10 还是 from ultralytics import YOLO？这有关系吗？对于训练、推理和导出（tflite 等），答案是否相同？]]></description>
      <guid>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</guid>
      <pubDate>Fri, 23 Aug 2024 21:07:49 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助验证谷歌/机器学习课程提供的数据</title>
      <link>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</link>
      <description><![CDATA[在谷歌提供的机器学习课程简介中的梯度下降页面中，提供了特征和相应的标签、MSE 损失函数、初始数据集和结果。我很难验证他们的结果，我想知道是否有人可以帮助确认我是否犯了错误或者他们犯了错误。
我有以下内容：
import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=[&#39;pounds&#39;,&#39;mpg&#39;])

number_of_iterations = 6
weight = 0 # 初始化权重
bias = 0 # 初始化权重
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

对于 i 在范围内（迭代次数）：
loss = calculate_loss（初始数据 df、权重、偏差）
final_results_df = update_results（最终结果 df、权重、偏差、损失）
weight_slope = find_weight_slope（初始数据 df、权重、偏差）
bias_slope = find_bias_slope（初始数据 df、权重、偏差）
weight = new_weight_update（权重、learning_rate、weight_slope）
bias = new_bias_update（偏差、learning_rate、bias_slope）
print（最终结果 df）

def calculate_loss（df、权重、偏差）：
loss_summation = []
对于 i 在范围内（0、len（df））：
loss_summation.append((df[&#39;mpg&#39;][i]-((weight*df[&#39;pounds&#39;][i])+bias))**2)
return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
if df.empty:
df = pd.DataFrame([[weight,bias,loss]],columns=[&#39;weight&#39;,&#39;bias&#39;,&#39;loss&#39;])
else:
df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
return df

def find_weight_slope(df,weight,bias):
weight_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
weight_update_summation.append(2*(wx_plus_b_minus_y*df[&#39;pounds&#39;][i]))
return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
bias_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
bias_update_summation.append(2*wx_plus_b_minus_y)
total_sum = sum(bias_update_summation)
return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
return old_bias-1*lr*slope

得出：
iter weight bias loss
0 0.00 0.00 303.0
0 1.20 0.35 170.0
0 2.06 0.60 102.0
0 2.67 0.79 67.0
0 3.10 0.93 50.0
0 3.41 1.04 41.0

这与提供的解决方案不同在网站上：
迭代权重偏差损失（MSE）
1 0 0 303.71
2 1.2 0.34 170.67
3 2.75 0.59 67.3
4 3.17 0.72 50.63
5 3.47 0.82 42.1
6 3.68 0.9 37.74
]]></description>
      <guid>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</guid>
      <pubDate>Fri, 23 Aug 2024 20:09:14 GMT</pubDate>
    </item>
    <item>
      <title>非正统的时间序列预处理方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</link>
      <description><![CDATA[我正在写一篇关于时间序列预处理的学校论文。在这篇论文中，我想测试时间序列预处理的非正统方法。然后我想将数据输入 LSTM 模型并测量其在预测时间序列方面的准确性。所以我想问问社区我应该测试哪些新颖/非正统的方法。我不想测试任何经典方法，如去噪、正则化、季节性消除等。如果您有任何想法，请随时分享。]]></description>
      <guid>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</guid>
      <pubDate>Thu, 22 Aug 2024 20:35:13 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能正确地连接不同的功能？</title>
      <link>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</link>
      <description><![CDATA[我有一个形状为 (100,48) 的特征图，以及其他形状为 (100,1) 的特征
我该如何正确地连接这些不同的特征？
我想这样做是因为我现在正在训练 XGboost 模型，但如果我将不同的特征直接连接在一起，机器就无法区分特征图和单个特征。
我尝试了下面的代码（来自 keras 函数 API），但我认为这不是正确的方法，每次我操作代码（没有随机种子）时，这样做的结果都不同。
input_1_layer = Input(shape=(input_1.shape[1],)) density_1 = Dense(48,activation=&#39;relu&#39;)(input_1_layer)

input_2_layer = Input(shape=(input_2.shape[1],)) density_2 = Dense(6,激活=&#39;relu&#39;)(输入层 2)

feature_extractor = Model(输入=[输入层 1, 输入层 2], 输出=merged)

new_features = feature_extractor.predict([输入层 1, 输入层 2])

你能提供任何论文或网站吗？]]></description>
      <guid>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</guid>
      <pubDate>Thu, 22 Aug 2024 07:19:22 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 包“敏感性”执行 Sobol 敏感性分析？</title>
      <link>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</link>
      <description><![CDATA[我想使用 R 包“sensitivity”执行 Sobol 敏感性分析。但是，我不确定如何在 sobol 函数中创建第一和第二个随机样本 (X1, X2)。我假设 X1 和 X2 都是输入数据的子集，但最终结果似乎不正确。
library(tidymodels)
library(sensitivity)

# 示例数据
set.seed(123)
x1 = runif(100)
x2 = runif(100)
x3 = runif(100)
y = 3 * x1 + 2 * x2 + x3 + rnorm(100)
data &lt;- data.frame(x1, x2, x3, y)

# 将数据拆分为训练集和测试集
set.seed(234)
data_split &lt;- initial_split(data, prop = 0.8)
train_data &lt;- training(data_split)
test_data &lt;- testing(data_split)

# 使用创建线性回归模型tidymodels
lm_spec &lt;- linear_reg() %&gt;%
set_engine(&quot;lm&quot;) %&gt;%
set_mode(&quot;regression&quot;)

lm_fit &lt;- lm_spec %&gt;%
fit(y ~ x1 + x2 + x3, data = train_data)

# 定义模型函数
model_function &lt;- function(x) {
new_data &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])
predict(lm_fit, new_data)$`.pred`
}

# 执行 Sobol 敏感性分析
set.seed(345)
X_index1 = sample(x=1:100, size = 50, replace = FALSE)
X_index2 = c(1:length(data$x1))[-X_index1]

sobol_results &lt;- sobol(model = model_function, 
X1 = data[X_index1, -4], 
X2 = data[X_index2, -4], 
nboot = 1000, order = 2)

sobol_results

sobol_results 显示敏感度顺序为：x2&gt;x1&gt;x3。根据函数 y = 3 * x1 + 2 * x2 + x3 + rnorm(100)，“x1”应该具有更高的敏感度，因为它是 3 * x1。我应该如何修正我的代码？谢谢。
]]></description>
      <guid>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</guid>
      <pubDate>Sat, 10 Jun 2023 01:54:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中检查模型处于训练模式还是评估模式？</title>
      <link>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</link>
      <description><![CDATA[如何从模型内部检查它当前处于训练模式还是评估模式？]]></description>
      <guid>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</guid>
      <pubDate>Thu, 17 Dec 2020 16:27:34 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow model.evaluate 给出的结果与训练得到的结果不同</title>
      <link>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</link>
      <description><![CDATA[我正在使用 tensorflow 进行多类分类
我以以下方式加载训练数据集和验证数据集
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

然后当我使用 model.fit() 训练模型
history = model.fit(
train_ds,
validation_data=val_ds,
epochs=epochs,
shuffle=True
)

我的验证准确率约为 95%。
但是当我加载相同的验证集并使用 model.evaluate() 时
model.evaluate(val_ds)

我的准确率非常低（约为 10%）。
为什么我得到的结果如此不同？我是否错误地使用了 model.evaluate 函数？
注意：在 model.compile() 中，我指定了以下内容，
优化器 - Adam，
损失 - SparseCategoricalCrossentropy，
指标 - 准确度
Model.evaluate() 输出
41/41 [================================] - 5s 118ms/step - 损失：0.3037 - 准确度：0.1032
测试损失 - 0.3036555051803589
测试准确度 - 0.10315627604722977

最后三个时期的 Model.fit() 输出
时期8/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.6094 - 准确度：0.8861 - val_loss：0.4489 - val_accuracy：0.9483
Epoch 9/10
41/41 [=============================] - 3s 80ms/步 - 损失：0.5377 - 准确度：0.8953 - val_loss：0.3868 - val_accuracy：0.9554
Epoch 10/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.4663 - 准确度：0.9092 - val_loss：0.3404 - val_accuracy：0.9590
]]></description>
      <guid>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</guid>
      <pubDate>Thu, 24 Sep 2020 15:26:53 GMT</pubDate>
    </item>
    <item>
      <title>训练期间改变模型</title>
      <link>https://stackoverflow.com/questions/36748574/changing-model-during-training</link>
      <description><![CDATA[我正在 TensorFlow 中创建一个模型，其中所有层都具有 relu 作为激活层。但是，当批处理大小增加到 500 时，我想更改模型，使输出层的倒数第二层具有 sigmoid 激活层。
我感到困惑的是，由于我在中间替换了优化器，我是否需要重新初始化所有变量？还是保留旧变量？]]></description>
      <guid>https://stackoverflow.com/questions/36748574/changing-model-during-training</guid>
      <pubDate>Wed, 20 Apr 2016 15:30:49 GMT</pubDate>
    </item>
    </channel>
</rss>