<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 06 May 2024 12:25:59 GMT</lastBuildDate>
    <item>
      <title>使用时间序列进行需求预测</title>
      <link>https://stackoverflow.com/questions/78436498/demand-forecasting-using-time-series</link>
      <description><![CDATA[有人能否为我提供一个详细的解决方案，关于如何使用任何技术预测未来 13 周的销售数量。数据集很小，大约有 76 列。数据集如下
DATES QTY
2022-04-17 7500
2022-04-24 5500
2022-05-01 6000
2022-05-08 1000
2022-05-15 31000
2022-05-22 17500
2022-05-29 5000
2022-06-05 10500
2022-06-12 2994
2022-06-19 25500
2022-06-26 20000
2022-07-03 6500
2022-07-10 17000
2022-07-17 22000
2022-07-24 9000
2022-07-31 1000
2022-08-07 9000
2022-08-14 2500
2022-08-21 14000
2022-08-28 13500
2022-09-04 8500
2022-09-11 16000
2022-09-18 15000
2022-09-25 32000
2022-10-02 18000
2022-10-09 7000
2022-10-16 5000
2022-10-23 37000
2022-10-30 8000
2022-11-06 7000
2022-11-13 10000
2022-11-20 20070
2022-11-27 27000
2022-12-04 14500
2022-12-11 2490
2022-12-18 28500
2022-12-25 24000
2023-01-01 10000
2023-01-08 8500
2023-01-15 12000
2023-01-22 28500
2023-02-05 24000
2023-02-12 13650
2023-02-19 27000
2023-02-26 18997
2023-03-05 8500
2023-03-12 6000
2023-03-19 7000
2023-03-26 22500
2023-04-02 13500
2023-04-09 5000
2023-04-16 6500
2023-04-23 30500
2023-04-30 13000
2023-05-07 3000
2023-05-14 8000
2023-05-21 7000
2023-05-28 22500
2023-06-04 8500
2023-06-11 12000
2023-06-18 14500
2023-06-25 25500
2023-07-02 1000
2023-07-09 22000
2023-07-16 15500
2023-07-23 14001
2023-08-06 500
2023-08-13 2500
2023-08-20 13500
2023-08-27 19705
2023-09-03 5500
2023-09-10 7000
2023-09-17 8995
2023-09-24 35999
2023-10-01 10000

尝试了所有模型，但没有得到准确的结果]]></description>
      <guid>https://stackoverflow.com/questions/78436498/demand-forecasting-using-time-series</guid>
      <pubDate>Mon, 06 May 2024 12:05:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Ollama 在医学成像中进行去识别化</title>
      <link>https://stackoverflow.com/questions/78436399/using-ollama-for-de-identification-in-medical-imaging</link>
      <description><![CDATA[我正在开展一个涉及医学图像去识别化的项目，以遵守隐私法规。我听说 Ollama 可以用于此目的，但我不确定如何开始或它专门为医学成像提供哪些功能。
具体来说，我有以下问题：
功能：Ollama 是否直接支持 DICOM 图像的去识别，还是需要对图像或元数据进行预处理？
集成：Ollama 如何与医学图像处理中常用的现有 Python 工作流程集成？
API 使用：如果 Ollama 提供 API，向其发送医学图像的步骤是什么？它如何处理去识别化的响应？
合规性功能：Ollama 提供哪些具体功能来确保遵守医疗成像领域的 HIPAA 等法规？
其他背景：
我对自动屏蔽图像像素和元数据中的个人标识符特别感兴趣。
我有一个使用 PyTorch 进行图像处理的基本设置，我需要知道如何将 Ollama 合并到这个管道中。
任何使用 Ollama 进行医学成像去识别的指导或示例将不胜感激！
刚下载 Ollama，找不到有关此主题的数据]]></description>
      <guid>https://stackoverflow.com/questions/78436399/using-ollama-for-de-identification-in-medical-imaging</guid>
      <pubDate>Mon, 06 May 2024 11:43:56 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch - 关于stochastica梯度下降类的step()方法装饰器的澄清</title>
      <link>https://stackoverflow.com/questions/78436387/pytorch-clarification-about-the-decorator-of-the-step-method-of-the-stochast</link>
      <description><![CDATA[在pytorch的SGD类中，step()方法有装饰器_use_grad_for_ Differentiable：
@_use_grad_for_ Differentiable
def 步骤（自我，闭包=无）：
...

通常我希望使用 no_grad 装饰器。
我没有找到有关 _use_grad_for_ Differentiable 的信息，也不知道它的作用。装饰器的源代码可以在文件 torch/optim/optimizer.py 中找到，但它没有文档字符串或有用的注释，并且代码很神秘（对我来说）。为了完成，我将其粘贴到此处：
def _use_grad_for_ Differentiable(func):
 def _use_grad(self, *args, **kwargs):
     导入火炬._dynamo
     prev_grad = torch.is_grad_enabled()
     尝试：
         # 注意下面的图表中断：
         # 我们需要图形中断以确保 aot 尊重 no_grad 注释。
         # 这对于性能来说很重要，因为如果没有这个，功能化将产生一个尾声
         # 它更新优化器的变异参数，这对于感应器来说是不可见的，因此，
         # 电感器将为模型中的每个参数进行分配，这很糟糕。
         # 这样，aot 正确地看到这是一个推理图，并且函数化将生成
         # 附加到图表中的尾声，对感应器来说*是*可见的，因此，感应器看到了
         # 步骤到位，能够避免额外的分配。
         # 将来，我们要么 1) 继续向后进行图形中断，所以这个图形中断并不重要
         # 或 2) 有一个完全融合的前向和后向图，默认情况下没有_grad，我们可以删除它
         # 图形中断以允许编译完全融合的 fwd-bwd-optimizer 图形。
         # 请参阅 https://github.com/pytorch/pytorch/issues/104053
         torch.set_grad_enabled(self.defaults[&#39;可微分&#39;])
         torch._dynamo.graph_break()
         ret = func(self, *args, **kwargs)
     最后：
         torch._dynamo.graph_break()
         torch.set_grad_enabled(prev_grad)
     返回 ret
 functools.update_wrapper(_use_grad, func)
 返回_use_grad

我的问题：这个装饰器的作用是什么？为什么不简单地使用 no_grad() 来代替呢？最后，我应该在自定义优化器中使用这个装饰器吗？]]></description>
      <guid>https://stackoverflow.com/questions/78436387/pytorch-clarification-about-the-decorator-of-the-step-method-of-the-stochast</guid>
      <pubDate>Mon, 06 May 2024 11:41:58 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 中，如何指定一个作为多个其他评估指标的函数的评估指标？</title>
      <link>https://stackoverflow.com/questions/78436104/in-lightgbm-how-to-specify-an-eval-metric-that-is-a-function-of-multiple-other</link>
      <description><![CDATA[问题就是实际上的一切
说我有

lightgbm 中的多个评估数据集：[ed1、ed2、ed3]
要根据以下各项计算的指标列表：[m1、m2、m3]
我希望最终指标为 f(m1, m2, m3)

我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78436104/in-lightgbm-how-to-specify-an-eval-metric-that-is-a-function-of-multiple-other</guid>
      <pubDate>Mon, 06 May 2024 10:47:58 GMT</pubDate>
    </item>
    <item>
      <title>KGCN的训练中，项目特征向量和用户特征向量是如何更新的，模型的loss是如何更新的？</title>
      <link>https://stackoverflow.com/questions/78435869/in-the-training-of-kgcn-how-are-the-project-feature-vectors-and-user-feature-ve</link>
      <description><![CDATA[我在阅读用于推荐的KGCN开源代码时遇到了困难。在源代码的train方法中，使用了以下语句来训练模型：
# KGCN是一个自定义类

模型 = KGCN(args, n_user, n_entity, n_relation, adj_entity, adj_relation)
将 tf.Session() 作为 sess：
    对于范围内的步骤（args.n_epochs）：
         _, loss = model.train(sess, get_feed_dict(model, train_data, start, start+args.batch_size))

但是 model.train 定义为：
 def train(self, sess, feed_dict):
        返回 sess.run([self.optimizer, self.loss], feed_dict)

那么项目特征向量和用户特征向量是如何更新的，模型的loss是如何更新的呢？
如果您需要源代码，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/78435869/in-the-training-of-kgcn-how-are-the-project-feature-vectors-and-user-feature-ve</guid>
      <pubDate>Mon, 06 May 2024 10:01:22 GMT</pubDate>
    </item>
    <item>
      <title>如何激励强化学习的有效探索？</title>
      <link>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</link>
      <description><![CDATA[所以我目前正在体育馆研究稀疏的山地车环境，其中明显的问题是奖励非常稀疏。每一步的奖励是 -1，如果智能体达到目标，则该情节终止。这意味着代理有动力尽快实现目标。
问题是，从我的发现来看，似乎没有一个算法能够始终如一地“幸运”。并在合理的时间内达到目标。
我不想设计特定的奖励，例如动能等。我想要的东西可以推广到不同的环境，理想情况下不需要调整参数。
我认为的解决方案是，探索对我来说需要更有目的性，而不仅仅是随机选择行动。
我研究了神经密度模型（NDM）探索，这看起来很有趣，但我发现它在这个任务中效果不佳，可能是因为状态空间太简单了。
还有其他有前途的技术可以用来使强化学习更加一致吗？尤其是奖励非常稀少。]]></description>
      <guid>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</guid>
      <pubDate>Mon, 06 May 2024 09:47:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个模型在某些目标上表现良好，但在某些目标上表现不佳？</title>
      <link>https://stackoverflow.com/questions/78435762/why-is-this-model-peforming-on-some-targets-well-but-on-some-not</link>
      <description><![CDATA[目前我正在开发一个机器学习项目。这是一个监督学习问题。我将 horse_data(size,weight,peformance,...) 作为输入，并将配方的成分作为输出。现在我想预测给定马数据的成分。
这是我的 horse_data 的摘要：HorseData
这是我的目标（recipe）的摘要：
FirstPartTargets
第二部分目标
第三方目标
所以我想用这些数据来训练一个机器学习模型。在本例中是随机森林回归器，因为输入有许多分类变量（保留、性能、工作类型、种族和种族类型）。
resDf = pd.DataFrame(columns=[&#39;训练 R^2 分数&#39;,&#39;测试 R^2 分数&#39;,&#39;训练 MSE&#39;,&#39;测试 MSE&#39;,&#39;训练 RMSE&#39;,&#39;测试 RMSE&#39; ,&#39;训练 MSAE&#39;,&#39;测试 MSAE&#39;])

参数网格 = {
    &#39;n_estimators&#39;: [100,200,1000],
    &#39;最大深度&#39;: [10,20,30],
    &#39;min_samples_split&#39;:[2,5,10],
    &#39;min_samples_leaf&#39;:[1,2,4]
}
对于 Y.columns 中的 ing：
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y[ing], test_size=0.2, random_state=52)

    gridModel = make_pipeline(GridSearchCV(估计器=RandomForestRegressor(),cv=10,param_grid=param_grid,n_jobs=-1,scoring=&#39;neg_mean_squared_error&#39;,verbose=True))
    gridModel.fit(X_train,Y_train)
    y_pred_train = gridModel.predict(X_train)
    
    train_mse_error =mean_squared_error(y_pred=y_pred_train,y_true=Y_train)
    train_mse_absoulte_error = Mean_absolute_error(y_pred=y_pred_train,y_true=Y_train)
    train_r2_score = r2_score(y_pred=y_pred_train,y_true=Y_train)
    train_rmse = train_mse_error ** (0.5)
    
    y_pred = gridModel.predict(X_test)
    test_mse_error =mean_squared_error(y_pred=y_pred,y_true=Y_test)
    test_mse_absoulte_error = Mean_absolute_error(y_pred=y_pred,y_true=Y_test)
    test_r2_score = r2_score(y_pred=y_pred,y_true=Y_test)
    test_rmse = test_mse_error ** (0.5)
    resDf.loc[ing] = [train_r2_score,test_r2_score,train_mse_error,test_mse_error,train_rmse,test_rmse,train_mse_absoulte_error,test_mse_absoulte_error]

结果如下：
FirstPartResult
第二部分结果
现在我的问题是有时我不明白结果。我的理解是我的模型过度拟合，因为在每一行中，测试集上的分数和错误都高于训练集上的分数和错误。但有些行我不明白。 VitaminA 在训练集上有很好的 r2 分数，但在测试集上很差（对我来说这是过度拟合）。但训练集和测试集的 RMSE 都非常高。同样令人困惑的是“schwefel”。它在训练集上的 r2score 很差，在测试集上的得分也很糟糕。但我在系统中看不到为什么会得到这些结果。问题是特征还是目标有时范围很大？]]></description>
      <guid>https://stackoverflow.com/questions/78435762/why-is-this-model-peforming-on-some-targets-well-but-on-some-not</guid>
      <pubDate>Mon, 06 May 2024 09:34:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Google Colab Python 代码构建浏览器扩展？</title>
      <link>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</link>
      <description><![CDATA[亲爱的 StackOverflow，您好，
我目前正在考虑使用 Google Colab 中的代码创建一个浏览器扩展。里面有Python脚本。
我知道，目前无法使用 Python 创建扩展。但还有其他选择吗？
Python 脚本本身大约有 100 行，带有一个训练模型。我认为仅使用 JavaScript 无法做到这一点。
谢谢；]]></description>
      <guid>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</guid>
      <pubDate>Mon, 06 May 2024 09:19:41 GMT</pubDate>
    </item>
    <item>
      <title>Imblearn 语法无效[重复]</title>
      <link>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</link>
      <description><![CDATA[虽然我已经能够在 jupyter 笔记本中安装 Imblearn ，但在尝试通过下面的代码导入管道时出现错误。
代码
from imblearn.pipeline 导入管道

错误
文件“C:\Users\ibl165795\AppData\Roaming\Python\Python37\site-
packages\imblearn\utils\_metadata_requests.py”，第 1492 行
    def process_routing(_obj, _method, /, **kwargs):
语法错误：语法无效
]]></description>
      <guid>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</guid>
      <pubDate>Mon, 06 May 2024 09:02:47 GMT</pubDate>
    </item>
    <item>
      <title>指导法学硕士 - 从文本中错误地提取数据继续</title>
      <link>https://stackoverflow.com/questions/78435586/instruct-llms-extract-data-from-text-wrongly-continues</link>
      <description><![CDATA[我正在尝试微调开源 LLM，现在让我们继续使用 Mistral-7b-instruct 模型。
我的任务如下：我有电子邮件，代表“价格请求”对于我们的客户发送的货物。
客户在邮件中告诉我们取货地址、发货人、收货人等信息。
我最初的想法是使用 DORA 训练不同的适配器，每个适配器都接受从电子邮件中提取不同实体的训练。
我的数据集创建如下：我有电子邮件和注释“基于电子邮件，我找到了这个 [ENTITY]：entity_here”
我已经创建了一条系统消息，并使用 chat_template 以 Mistral 接受的方式创建数据集，使用此 chat_template：
“{%- for messages in messages %}”
  “{%- if message[&#39;角色&#39;] == &#39;系统&#39;-%}”
      ”{{- &#39;&#39; + 消息[&#39;内容&#39;] -}}”
  “{%-else-%}”
      “{%- if message[&#39;角色&#39;] == &#39;用户&#39;-%}”
          “{{-&#39;[INST]&#39; + message[&#39;content&#39;].rstrip() + &#39;[/INST]&#39;-}}”
      “{%-else-%}”
          ”{{-&#39;&#39; + 消息[&#39;内容&#39;] + &#39;&#39; -}}”
      “{%-endif-%}”
  “{%-endif-%}”
“{%-endfor-%}”
“{%- if add_ Generation_prompt -%}”
    “{{-&#39;&#39;-}}”
“{%-endif-%}”

现在解决问题了。该模型似乎了解了需要提取的内容，它生成了不错的答案，其格式与训练它的助手相同，问题是在生成答案后，它不断生成与电子邮件无关的附加文本到任务，E.G. “请联系我们......”
例如，当我针对同一任务微调 GPT3.5 时，模型能够准确提取我需要的内容，这表明我做错了什么。
有人对我哪里出错有建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78435586/instruct-llms-extract-data-from-text-wrongly-continues</guid>
      <pubDate>Mon, 06 May 2024 09:01:03 GMT</pubDate>
    </item>
    <item>
      <title>为我的 Npy 数据集定义 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</link>
      <description><![CDATA[我需要帮助为我的数据定义火炬模型。我尝试了各种方法，但似乎没有任何效果。与输入尺寸和形状相关的错误不断出现。我对此完全是初学者。非常感谢您的帮助。我渴望学得更好。
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
导入 torch.nn.function 作为 f

# 从 .npy 文件加载数据
data = np.load(“其他py文件/project_files/data/train/data.npy”)
print(&quot;数据形状：&quot;, data.shape) # (401, 701, 255)

数据大小 = 数据.形状[0] * 数据.形状[1] * 数据.形状[2]
print(“数据大小：”, data_size) # 71680755

# 从 .npy 文件加载标签数据
labels = np.load(“其他py文件/project_files/data/train/label.npy”)
print(&quot;标签数据形状:&quot;, labels.shape) # (401, 701, 255)

# 将 numpy 数组转换为 PyTorch 张量
data_tensor = torch.Tensor(数据)
labels_tensor = torch.Tensor(labels)


类 MyModel(nn.Module):
    def __init__(自身):
        超级（MyModel，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def 前向（自身，x）：
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        返回x

模型 = MyModel()
打印（模型）

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

数据集 = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(数据集,batch_size=32,shuffle=True)

纪元数 = 10
对于范围内的纪元（num_epochs）：
    运行损失 = 0.0
    对于 i，enumerate(dataloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        outputs = model(inputs.unsqueeze(1)) # 通道维度
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
        如果我％100==99：
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 100}&quot;)
            运行损失 = 0.0


使用 torch.no_grad()：
    Predictions = model(data_tensor.unsqueeze(1)) # 通道维度


**控制台输出：
已连接到 pydev 调试器（版本 223.8836.43）
数据形状：(401, 701, 255)
数据大小：71680755
标签数据形状：(401, 701, 255)
我的模型（
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （池）：MaxPool2d（kernel_size = 2，stride = 2，padding = 0，dilation = 1，ceil_mode = False）
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （fc）：线性（in_features = 71680755，out_features = 2，偏差= True）
）

文件“C:\Users\PC1\PycharmProjects\Project1\newmodel2.py”，第 36 行，向前
    x = x.view(-1, self.fc_input_size)
运行时错误：形状“[-1, 71680755]”对于大小 22579200 的输入无效
python-BaseException**
]]></description>
      <guid>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</guid>
      <pubDate>Mon, 06 May 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>如何保留机器学习课程中的数学概念？</title>
      <link>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</link>
      <description><![CDATA[Stack Overflow 社区您好，
我最近完成了 Deeplearning.ai 在 Coursera 上提供的机器学习专业化以及 ML 和 DS 数学课程。虽然我设法完成了这些课程，但我在保留所教授的数学概念和公式方面面临着挑战。这些课程涵盖了广泛的主题，现在回想起来，我意识到我不记得大部分材料，尤其是详细的数学部分。
我正在寻找有效的策略或工具，以帮助更好地长期掌握和保留这些数学概念。我仍然觉得我还没有完全掌握这些材料。我将不胜感激任何有关学习技巧、资源或任何有助于巩固我对这些概念的理解的具体练习的建议。此外，如果社区有适合强化这些知识的后续课程或材料的建议，我将不胜感激。
感谢您的帮助！
我期待一个指导方针。]]></description>
      <guid>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</guid>
      <pubDate>Mon, 06 May 2024 07:31:53 GMT</pubDate>
    </item>
    <item>
      <title>如何将FastAI分类器集成到sklearn VotingClassifier中？</title>
      <link>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</link>
      <description><![CDATA[我有一堆表格数据，我成功地训练了一个 RandomForestClassifier、一个 GradientBoostingClassifier 和一个深度学习模型（来自 fastai 的表格学习器代码&gt;) 与他们一起。我在结果中注意到，每个模型在特定标签上都比其他模型做得更好，每个模型都不同。我想知道是否可以将所有模型放入 VotingClassifier （来自 sklearn 的模型）。我对 RandomForestClassifier 和 GradientBoostingClassifier 没有任何问题，但我没有找到任何有关将表格学习器放入 VotingClassifier 中的信息。可以这样做吗？]]></description>
      <guid>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</guid>
      <pubDate>Mon, 06 May 2024 07:21:01 GMT</pubDate>
    </item>
    <item>
      <title>关于 onehotencoder 空间成本</title>
      <link>https://stackoverflow.com/questions/78434421/regarding-onehotencoder-space-cost</link>
      <description><![CDATA[为什么 one-hot 编码不使用基于位的编码？占用的内存不是少很多吗？我的意思是，当您对例如四个城市进行编码时，您可以像 one-hot 编码器一样，将一列扩展到 4 或在一列中像这样：
 第一个城市 = 0(base10) = 00000000
    第二个城市 = 1(base10) = 00000001
    第三个城市 = 2(base10) = 00000010
    第四个城市 = 3(base10) = 00000011

这难道不是在内存成本方面更有效吗？或者编码技术是否迫使它占用更多空间？]]></description>
      <guid>https://stackoverflow.com/questions/78434421/regarding-onehotencoder-space-cost</guid>
      <pubDate>Mon, 06 May 2024 03:24:38 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    </channel>
</rss>