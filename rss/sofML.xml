<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Sep 2024 21:16:21 GMT</lastBuildDate>
    <item>
      <title>SiteGPT 如何工作？他们使用微调、RAG 还是其他什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79024774/how-does-sitegpt-work-do-they-use-fine-tuning-rag-or-something-else</link>
      <description><![CDATA[我最近遇到了 SiteGPT，它允许您根据您的网站或特定文档创建自定义聊天机器人。我很好奇它背后的底层技术。有人知道 SiteGPT 的底层工作原理吗？具体来说：

他们是否使用语言模型的微调？

是否使用检索增强生成 (RAG) 直接从提供的站点或文档中提取信息？

是否还有其他技巧或技术可以使聊天机器人根据站点的内容准确响应？

]]></description>
      <guid>https://stackoverflow.com/questions/79024774/how-does-sitegpt-work-do-they-use-fine-tuning-rag-or-something-else</guid>
      <pubDate>Wed, 25 Sep 2024 21:11:36 GMT</pubDate>
    </item>
    <item>
      <title>EfficientNet 中的迁移学习在二元分类任务中停留在 50%</title>
      <link>https://stackoverflow.com/questions/79024541/transfer-learning-in-efficientnet-for-binary-classification-task-stuck-at-50</link>
      <description><![CDATA[我一直在尝试在二元分类任务中使用 EfficientNet 进行迁移学习。
我的目录结构如下所示：
training
│── label0
└── label1

validation
│── label0
└── label1

我使用它来创建图像数据生成器：
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
rescale=1./255,
rotation_range=30, 
width_shift_range=0.0, 
height_shift_range=0.0,
sher_range=0.0, 
zoom_range=0.0, 
Horizo​​ntal_flip=True,
fill_mode=&#39;nearest&#39;
)
未应用任何增强
validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
&#39;training&#39;,
target_size=(240, 240), 
batch_size=128, 
class_mode=&#39;binary&#39;, # 二元分类
shuffle=True
)

validation_generator = validation_datagen.flow_from_directory(
&#39;validation&#39;, 
target_size=(240, 240),
batch_size=128,
class_mode=&#39;binary&#39;,
shuffle=True
)

输出为：
找到属于 2 个类别的 19747 张图像。
找到属于 2 个类别的 4938 张图像。

这是构建模型的代码：
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.applications import EfficientNetB1
from tensorflow.keras.models import Model
from tensorflow.keras import layer

NUM_CLASSES = 2
IMG_SIZE = 240
size = (IMG_SIZE, IMG_SIZE)

def build_model():
inputs = layer.Input(shape=(IMG_SIZE, IMG_SIZE, 3))

model = EfficientNetB1(include_top=False, input_tensor=inputs, weights=&quot;imagenet&quot;)

model.trainable = False

x = layer.GlobalAveragePooling2D(name=&quot;avg_pool&quot;)(model.output)
x = layer.BatchNormalization()(x)
# x = layer.Dense(128, 激活=&quot;relu&quot;)(x)
# top_dropout_rate = 0.2
# x = layer.Dropout(top_dropout_rate, 名称=&quot;dropout&quot;)(x)
# 输出 = layer.Dense(1, 激活=&quot;sigmoid&quot;, 名称=&quot;pred&quot;)(x)
输出 = layer.Dense(1, 激活=&quot;sigmoid&quot;, 名称=&quot;pred&quot;)(x)

模型 = tf.keras.Model(输入、输出、名称=&quot;EfficientNet&quot;)
优化器 = tf.keras.optimizers.Adam(learning_rate=1e-4)
model.compile(
optimizer=optimizer,
loss=&quot;binary_crossentropy&quot;,
metrics=[&quot;accuracy&quot;]
)

return model

你可以看到我尝试了很多方法只是为了让某些东西起作用但都无济于事。我将model.trainable设置为False，因为我正在遵循这个关于迁移学习的实现。我还没有完成第一步。我原本计划在达到 60-70% 后继续解冻，但我甚至无法达到 52%。
以下是我开始训练的方式：
model = build_model()
epochs = 50
hist = model.fit(train_generator, 
epochs=epochs, 
steps_per_epoch=len(train_generator), 
validation_data=validation_generator,
validation_steps=len(validation_generator))

并且在前十个时期，准确率和 val_accuracy 始终接近 50%，这只不过是随机猜测。
我尝试降低或增加学习率（1e-1 到 1e-6）、批量大小（32 - 256）、添加 dropouts（0.1 - 0.5），添加 relu 层（32 - 128），并确保图像属于正确的类别。我能做些什么来让模型真正学习？]]></description>
      <guid>https://stackoverflow.com/questions/79024541/transfer-learning-in-efficientnet-for-binary-classification-task-stuck-at-50</guid>
      <pubDate>Wed, 25 Sep 2024 19:42:31 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的 transformers 库中的模型实现是由原始模型作者创建的还是由 Hugging Face 创建的？</title>
      <link>https://stackoverflow.com/questions/79024489/are-the-model-implementations-in-hugging-face-s-transformers-library-created-by</link>
      <description><![CDATA[我一直在探索 Hugging Face 的 transformers 库中类似 Llama 的模型的实现，例如：
Hugging Face 的 Llama 模型实现。
我很好奇这些实现是如何工作的：

Hugging Face 的 transformers 库（例如 Llama）中的模型代码是否由模型创建者直接编写，还是 Hugging Face 试图根据模型原始创建者发布的文档或论文重现功能？
这些实现与模型提供的规范或官方代码库（如果有）的遵循程度如何作者？

如果能了解 Hugging Face 的实现是否是官方的，或者是否是独立的努力以与模型的原始设计保持一致，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/79024489/are-the-model-implementations-in-hugging-face-s-transformers-library-created-by</guid>
      <pubDate>Wed, 25 Sep 2024 19:21:51 GMT</pubDate>
    </item>
    <item>
      <title>Llama 3 是否完全开源，包括标记器、转换器以及构建自定义 LLM 所需的其他组件？</title>
      <link>https://stackoverflow.com/questions/79024432/is-llama-3-fully-open-source-including-tokenizer-transformers-and-other-compo</link>
      <description><![CDATA[我想了解 Llama 3 是否完全开源。具体来说，我想知道：

Llama 3 的源代码（包括 tokenizer、transformers 和其他组件）是否在开源许可下可用？
可用的源代码是否提供了使用 Llama 3 的架构构建具有自定义数据的大型语言模型 (LLM) 所需的一切？

例如，如果我想像 Llama 3 一样训练自己的模型，我是否可以访问所有底层代码（用于 tokenization、模型架构等），或者是否涉及任何专有组件？]]></description>
      <guid>https://stackoverflow.com/questions/79024432/is-llama-3-fully-open-source-including-tokenizer-transformers-and-other-compo</guid>
      <pubDate>Wed, 25 Sep 2024 19:02:41 GMT</pubDate>
    </item>
    <item>
      <title>尽管 np.where(np.isnan(X)) 没有返回任何内容，sklearn 仍会引发错误“输入 X 包含 NaN”</title>
      <link>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</link>
      <description><![CDATA[我正在尝试为著名的泰坦尼克号数据集制作一个简单的 Knearestneighbor 算法。尽管我的数据框似乎没有任何 NaN 值，但我还是不断收到标题中的错误。如何解决这个问题？
y = train_data[&quot;Survived&quot;]
features = [&quot;Fare&quot;, &quot;Sex&quot;]

X = train_data[features]
X.loc[:, &#39;Sex&#39;] = X.loc[:, &#39;Sex&#39;].apply(weigh_sex)
X = ct.fit_transform(X)

X_test = test_data[features]
X_test.loc[:, &quot;Sex&quot;] = X_test.loc[:, &quot;Sex&quot;].apply(weigh_sex)
X_test = ct.fit_transform(X_test)

print(np.where(np.isnan(X))) #即使放在 clf.fit() 之后，也不会返回任何内容

clf.fit(X, y)

predictions = clf.predict(X_test) #此行引发错误

完整回溯：
回溯（最近一次调用）：
文件“C:\pystuff\StatsSandbox\Titanic.py”，第 65 行，位于&lt;module&gt;
predictions = clf.predict(X_test) #此行引发错误
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_classification.py”，第 266 行，在 predict 中
neigh_ind = self.kneighbors(X, return_distance=False)
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_base.py”，第 804 行，在 kneighbors 中
X = self._validate_data(X, accept_sparse=&quot;csr&quot;, reset=False, order=&quot;C&quot;)
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\base.py”，第 605 行，在 _validate_data 中
out = check_array(X, input_name=&quot;X&quot;, **check_params)
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 957 行, 在 check_array 中
_assert_all_finite(
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 122 行, 在 _assert_all_finite
_assert_all_finite_element_wise(
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 171 行, 在 _assert_all_finite_element_wise
raise ValueError(msg_err)
ValueError: 输入 X 包含 NaN。
 ]]></description>
      <guid>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</guid>
      <pubDate>Wed, 25 Sep 2024 17:26:01 GMT</pubDate>
    </item>
    <item>
      <title>如果训练集和测试集完全不相交，那么可以使用哪些 ML/DL 模型来玩 Hangman 游戏？</title>
      <link>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</link>
      <description><![CDATA[Hangman 是一款猜字游戏，玩家需要逐个字母猜出隐藏的单词。每次猜对都会显示单词中该字母的所有实例，而猜错则会减少玩家剩余的尝试次数。目标是在尝试次数用完之前猜出单词。隐藏单词的长度是可变的，最大尝试次数为 5 次。
我必须想出一个模型/算法来猜测隐藏字符串的字母（例如，如果隐藏单词是“mathematics”，输入是 _ a t h e _ a t i _ s，则模型理想情况下应该猜测“m”或“c”）。测试将通过模拟从完全隐藏的单词（仅下划线）开始的绞刑游戏来完成。
训练数据是来自英语的单词列表，测试数据将不包含任何这些给定的单词。
机器学习方法是首选，但不是强制性的。
我尝试在通过随机删除给定单词中的字母实例生成的数据上训练 BiLSTM 模型，然后为输入字符串中的每个位置（包括已经显示的单词）输出 27 个类别的 softmax（每个字母 1 个，填充字符 1 个）。由于训练集中的最大单词数为 32，因此我将输入字符串填充为 32 个字符。这并没有给出令人满意的结果。
还尝试了字节对编码 (BPE) 来提取训练词汇表中的相关子词，然后尝试从测试集中猜测单词是 BPE 词汇表中三个单词的组合。
我还能尝试什么？这会属于哪种问题？我在哪里可以阅读有关此类问题的更多信息？]]></description>
      <guid>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</guid>
      <pubDate>Wed, 25 Sep 2024 14:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter Notebook 和 Python 脚本返回不同的 YOLOV8 实例分割掩码[关闭]</title>
      <link>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</link>
      <description><![CDATA[我一直在运行预先训练的 YOLOV8 模型来进行实例分割。为了进行测试和开发，我一直在使用 Anaconda Jupyter Notebook，在部署之前我将其转换为 Python 脚本。
这是来自 Jupyter Notebook 的（正确）掩码：

这是来自 Python 脚本的结果：

您可以看到 Python 结果在边缘处有奇怪的块我的头和手机。

两个版本的代码完全相同
Python 虚拟环境直接从 Jupyter 环境导出创建（我仔细检查了：Python 版本相同，所有库的版本也相同）
CUDA 版本相同
硬件相同（两个示例都在同一台机器上执行）

有人知道为什么会发生这种情况或我该如何修复它吗？]]></description>
      <guid>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</guid>
      <pubDate>Wed, 25 Sep 2024 13:53:07 GMT</pubDate>
    </item>
    <item>
      <title>我正在实现 qwen_2-vl +byaldi，用于基于视觉的 ocr，并将其托管为基于 stremlit 的 Web 应用程序，但它无法正常工作，一直崩溃</title>
      <link>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</link>
      <description><![CDATA[我正在开发一个 Streamlit 应用程序，该应用程序利用多模态模型进行图像搜索和文本提取。但是，在尝试下载提取的文本时，该应用程序经常在文本提取过程中崩溃。以下是相关代码，以及重现问题的步骤。
requirement.txt
pdf2image

git+https://github.com/huggingface/transformers.git

qwen-vl-utils

#flash-attn

byaldi

qwen_vl_utils

transformers

重现步骤

使用提供的代码运行 Streamlit 应用。
上传有效的图像文件 (JPG、JPEG、PNG)。
在提供的输入框中输入文本查询。
单击“搜索并提取文本”按钮。

import streamlit as st
import base64
from huggingface_hub import notebook_login
from byaldi import RAGMultiModalModel
from transformers import Qwen2VLForConditionalGeneration、AutoTokenizer、AutoProcessor
从 PIL 导入图像
从 io 导入 BytesIO
导入 torch
导入 re

@st.cache_resource
def load_models():
RAG = RAGMultiModalModel.from_pretrained(&quot;vidore/colpali&quot;, verbose=10)
model = Qwen2VLForConditionalGeneration.from_pretrained(
&quot;Qwen/Qwen2-VL-2B-Instruct&quot;,
torch_dtype=torch.float16,
device_map=&quot;auto&quot;,
)
processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen2-VL-2B-Instruct&quot;)
return RAG、model、processor

RAG、model、processor = load_models()

st.title(&quot;多模态图像搜索和文本提取App&quot;)

uploaded_file = st.file_uploader(&quot;选择图片&quot;, type=[&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;])

如果 uploaded_file 不为 None:
image = Image.open(uploaded_file)
st.image(image, caption=&#39;Uploaded Image&#39;, use_column_width=True)

temp_image_path = &quot;uploaded_image.jpeg&quot;
image.save(temp_image_path)

@st.cache_data
def create_rag_index(image_path):
RAG.index(
input_path=image_path,
index_name=&quot;image_index&quot;,
store_collection_with_index=True,
overwrite=True,
)

create_rag_index(temp_image_path)

text_query = st.text_input(&quot;输入您的文本查询&quot;)

if st.button(&quot;搜索并提取文本&quot;):
if text_query:
results = RAG.search(text_query, k=1, return_base64_results=True)

image_data = base64.b64decode(results[0].base64)
image = Image.open(BytesIO(image_data))
st.image(image, caption=&quot;结果图像&quot;, use_column_width=True)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{&quot;type&quot;: &quot;image&quot;},
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;extract text&quot;}
]
}
]

text_prompt = processing.apply_chat_template(messages, add_generation_prompt=True)

input = processing(
text=[text_prompt],
images=[image],
padding=True,
return_tensors=&quot;pt&quot;
)

输入 = 输入.to(model.device)

使用 torch.no_grad():
输出_ids = 模型.generate(**输入, max_new_tokens=1024)

生成_ids = 输出_ids[:, 输入.输入_ids.shape[1]:]

输出_文本 = 处理器.batch_decode(
生成_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
)[0]

# 突出显示查询的文本
def 突出显示_文本(文本, 查询):
突出显示_文本 = 文本
for word in query.split():
模式 = re.compile(re.escape(word), re.IGNORECASE)
突出显示_文本 = 模式.sub(lambda m: f&#39;&lt;span style=&quot;background-color: yellow;&quot;&gt;{m.group()}&lt;/span&gt;&#39;, highlight_text)
return highlight_text

highlight_output = highlight_text(output_text, text_query)

st.subheader(&quot;提取的文本（查询突出显示）：&quot;)
st.markdown(highlighted_output, unsafe_allow_html=True)
else:
st.warning(&quot;请输入查询。&quot;)
else:
st.info(&quot;上传图片以开始。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</guid>
      <pubDate>Wed, 25 Sep 2024 10:56:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 AI / OCR 检测和验证文档中的复选框[关闭]</title>
      <link>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</link>
      <description><![CDATA[我需要开发可以自动验证称为“W 合同”的文档的软件。这些合同包含许多需要填写的复选框和字段，我的目标是准确识别哪些框被选中并根据特定规则和参数验证值。
我尝试使用 OpenAI 的 API 来检测这些元素，但我发现它在处理文档中的大量复选框和选项时遇到了困难。它经常错误识别结构或无法正确识别复选框。
我正在寻找可以帮助我更准确地执行此任务的技术、库或 API 的建议，最好使用 OCR 和计算机视觉。理想的解决方案应该能够：
检测 PDF 或图像文档中的复选框和输入字段。
准确确定复选框是否被选中。
读取字段并根据某些规则验证数据。
任何关于可以促进这项任务的工具或方法的建议都将不胜感激。我对基于 AI 的解决方案持开放态度，但也对传统的 OCR 方法感兴趣。
]]></description>
      <guid>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</guid>
      <pubDate>Wed, 25 Sep 2024 10:22:52 GMT</pubDate>
    </item>
    <item>
      <title>将双线性采样特征映射回体素</title>
      <link>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</link>
      <description><![CDATA[我正在开发一个专门用于双线性采样的类。我的目标是将从双线性采样过程中提取的特征映射到创建的体素网格中的适当位置。
实施步骤：
B = 批次

C = 通道

S = 视图数

D = 深度

H = 高度

W = 宽度

3 = x,y,z

2 = x,y

我创建了一个具有此形状 [B,D,H,W,3] 的体素
并且我有 360 个图像视图样本，尺寸为 [B,S,C,H,W]
第一步，我使用外部和内部将体素点（点云）投影到每个图像。然后我过滤了图像之外的点。
结果我得到了 valid_points = [B,S,H,W,2]
我从 valid_points 中对我的点进行了归一化，并创建了大小为 [B,S,H,W,2] 的网格，请注意，W 是有效点的数量，H = 1
然后我所做的是应用双线性采样，如代码所示：
valid_points = cur_coords[:, on_img[1]]

######### 将有效点归一化在 [-1, 1] 之间 ########
normalized_points = torch.zeros_like(valid_points)
normalized_points[:,:, 0] = 2.0 * (valid_points[:, :, 0] / (H_img - 1)) - 1.0 # 归一化y 坐标
normalized_points[:,:, 1] = 2.0 * (valid_points[:, :, 1] / (W_img - 1)) - 1.0 # [N, M&#39;,2]

grid = normalized_points.unsqueeze(1).cuda() # 形状 [S, H_out, W_out, 2]
sampled_features_with_location_list = []
for i in range(0,N):
img_s =camera_view_tensor[i].unsqueeze(0).permute(0, 3, 1, 2) #[B, C, H_in, W_in]
grid_s = grid[i].unsqueeze(0)
sampled_points = F.grid_sample(img_s, grid_s,mode=&#39;bilinear&#39;,
align_corners=None) # (B,N,C,H_out,W_out)
sampled_features_list.append(sampled_pointson)
sampled_points = torch.stack(sampled_features_list, dim=1) #[B = 1,S = 6,C= 3,H= 1,W =22965]

现在我想应用逆映射来提取 bev 特征。怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</guid>
      <pubDate>Wed, 25 Sep 2024 10:14:39 GMT</pubDate>
    </item>
    <item>
      <title>SBERT 微调总是在完成所有 epoch 之前停止</title>
      <link>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</link>
      <description><![CDATA[我正在使用 SBERT 预训练模型（特别是 MiniLM）进行一个包含 995 个分类的文本分类项目。我大部分时间都在按照此处列出的步骤进行操作，一切似乎都运行正常。
我的问题出现在实际训练模型时。无论我在训练参数中设置什么值，训练似乎总是提前结束，并且永远不会完成所有批次。例如，我设置了 num_train_epochs=1，但它最多只能达到 0.49 个 epoch。如果 num_train_epochs=4，它总是在 3.49 个 epoch 处结束。
这是我的代码：
from datasets import load_dataset
from sentence_transformers import (
SentenceTransformer,
SentenceTransformerTrainer,
SentenceTransformerTrainingArguments,
SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
&quot;nreimers/MiniLM-L6-H384-uncased&quot;,
model_card_data=SentenceTransformerModelCardData(
language=&quot;en&quot;,
license=&quot;apache-2.0&quot;,
model_name=&quot;all-MiniLM-L6-v2&quot;,
)
)

loss = BatchAllTripletLoss(model)
# 损失概述：https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# 此特定损失方法：https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss

# 训练参数：https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
# 必需参数：
output_dir=&quot;finetune/model20240924&quot;,
# 可选训练参数：
num_train_epochs=1,
max_steps = -1,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
learning_rate=1e-5,
warmup_ratio=0.1,
fp16=True, # 如果您收到 GPU 无法在 FP16 上运行的错误，请设置为 False
bf16=False, # 如果您拥有支持 BF16 的 GPU，请设置为 True
batch_sampler=BatchSamplers.GROUP_BY_LABEL, # 
# 可选的跟踪/调试参数：
eval_strategy=&quot;no&quot;,
eval_steps=100,
save_strategy=&quot;epoch&quot;,
# save_steps=100,
save_total_limit=2,
logs_steps=100,
run_name=&quot;miniLm-triplet&quot;, # 如果在 W&amp;B 中使用`wandb` 已安装
)

trainer = SentenceTransformerTrainer(
model=model,
args=args,
train_dataset=trainDataset,
eval_dataset=devDataset,
loss=loss,
#evaluator=dev_evaluator,
)
trainer.train()

请注意，我没有使用评估器，因为我们正在创建模型，并在事后使用专用的测试值集对其进行测试。我的数据集结构如下：
Dataset({
features: [&#39;Title&#39;, &#39;Body&#39;, &#39;label&#39;],
num_rows: 23961
})

与 dev 数据集具有相同的结构，只是行数较少。这将提供以下输出：
 [1473/2996 57:06 &lt; 59:07，0.43 it/s，Epoch 0/1]
步骤训练损失
100 1.265600
200 0.702700
300 0.633900
400 0.505200
500 0.481900
600 0.306800
700 0.535600
800 0.369800
900 0.265400
1000 0.345300
1100 0.516700
1200 0.372600
1300 0.392300
1400 0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={&#39;train_runtime&#39;: 3427.9198, &#39;train_samples_per_second&#39;: 6.99, &#39;train_steps_per_second&#39;: 0.874, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5003972503496366, &#39;epoch&#39;: 0.4916555407209613})

无论我如何调整值，我都无法让它完成所有批次。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</guid>
      <pubDate>Wed, 25 Sep 2024 03:55:44 GMT</pubDate>
    </item>
    <item>
      <title>提取哪些特征来聚类文本？</title>
      <link>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</link>
      <description><![CDATA[我想为文本制作一个分类器，进一步用于为给定的文本推荐最相似的文本。
应用程序的流程如下：

使用 llm 从文本中提取 10 个主要主题（它可以从 150 个词池中选择）
我将词向量设为二进制向量，基本上在 150 维空间中工作，其中每个文本都有一个坐标，例如 [1, 0, 1, ..., 0]
然后我使用 cosine 距离找到最近的邻居（我想扩展到 3-5，但为了简单起见，我们假设只有一个）
我收到了最接近的文本

问题是文本非常不同，并且 llm 可以很好地提供主题，但是建议的文本并不完全符合我的预期。我尝试根据重要性对主题进行排序，并使向量非二进制（[10, 0, 0, 9, ..., 1]），但这似乎没有太大帮助。
我想知道这种方法是否不适合我的问题，或者我是否应该使用其他参数或其他任何东西来对我的文本进行分组。]]></description>
      <guid>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</guid>
      <pubDate>Wed, 11 Sep 2024 14:56:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在 AWS 免费套餐上分配 GPU？</title>
      <link>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</link>
      <description><![CDATA[是否可以在 AWS 免费套餐上分配 GPU？如果可以，有人可以解释一下步骤吗？我尝试在 Amazon EC2 上分配 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</guid>
      <pubDate>Fri, 13 Mar 2020 10:33:16 GMT</pubDate>
    </item>
    <item>
      <title>努力理解 R 中完整的预测模型流程</title>
      <link>https://stackoverflow.com/questions/40127296/struggling-to-understand-complete-predictive-model-process-in-r</link>
      <description><![CDATA[我正在尝试使用现有客户和已流失客户的数据库来预测客户流失。到目前为止，我已经

获取现有客户和已流失客户的完整客户数据库以及客户服务变量等，用于预测。
将数据集随机分为 70/30 的训练和测试
使用 R，我训练了一个随机森林模型来预测，然后使用混淆矩阵将其与实际状态进行比较。
我已经使用测试数据运行该模型来检查识别流失者的准确性

我现在想做的是获取我们所有的现有客户并预测哪些客户会流失。我是否完全做错了，因为我需要预测是否会流失的很多现有客户已经被模型看到，因为他们出现在训练集中？
我是否应该使用不属于我需要进行预测的数据集的训练和测试集？]]></description>
      <guid>https://stackoverflow.com/questions/40127296/struggling-to-understand-complete-predictive-model-process-in-r</guid>
      <pubDate>Wed, 19 Oct 2016 09:24:33 GMT</pubDate>
    </item>
    <item>
      <title>开源神经网络库 [关闭]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[我正在寻找一个开源神经网络库。到目前为止，我已经研究过 FANN、WEKA 和 OpenNN。我还应该看看其他的吗？当然，标准是文档、示例和易用性。]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>