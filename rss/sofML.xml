<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 11 Feb 2024 01:01:35 GMT</lastBuildDate>
    <item>
      <title>Scikit-Learn 1.3.0 中的 MinMaxScaler 在同一系统上的相同输入值的不同 IDE 中给出不同的结果</title>
      <link>https://stackoverflow.com/questions/77974757/minmaxscaler-from-scikit-learn-1-3-0-gives-different-result-in-different-ide-for</link>
      <description><![CDATA[我对模型的输入数组进行了预处理，如下所示，然后使用 MinMaxScaler 对其进行转换。缩放器已经根据训练数据进行了拟合。
&lt;前&gt;&lt;代码&gt;[
   {
        “SCHEME_CODE”：“10003”，
        “CURRENCY_CODE”：“NGN”，
        “IS_CREDIT”：1，
        “IS_PEP”：1，
        “SOL_ID”：“73”，
        “最高”：0，
        “电视音量”：0，
        “T值”：0，
        “STR”：0，
        “TM”：0，
        “点击率”：0，
        “DATE_OPENED”：“2009-07-15 00:00:00”
    }
]

在jupyter笔记本上缩放后的输出是
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 6.59090909e-01 5.45454545e-01
4.66666667e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

注意：为了简洁和清晰起见，我删除了其他预处理步骤。
我使用 scikit-learn 版本 1.3.0 中的 joblib 保存了 MinMaxScaler。
我的系统上运行的是python version 3.9.13。在 Jupyter Lab 的 python 脚本中的同一系统上，我取消了缩放器并转换了相同的输入。我得到的值在数组的第 6、7 和 8 个元素处略有不同，如下所示。
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 -4.50000000e+01 -9.09090909e-02
-3.33333333e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我在训练模型的同一个 Jupyter Notebook 会话中进一步使用原始缩放器转换输入，并且得到了与在 Jupyter Notebook 中取消缩放器时相同的数组。 
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 6.59090909e-01 5.45454545e-01
4.66666667e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我在 vscode 中解开了标量并转换了 python 脚本中的相同输入，我得到了与 Jupyter Lab 不同的值。
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 -4.50000000e+01 -9.09090909e-02
-3.33333333e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我已审阅了此处对问题的回答和建议，并且此处，但他们的问题与我的不同。&lt; /p&gt;
总结一下：当我在另一个 IDE（例如 jupyter lab、vscode）中取消缩放器以转换相同的输入但在同一系统上时，MinMaxScaler 会给出不同的 Transform 输出。&lt; /p&gt;
这里可能存在什么问题以及如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/77974757/minmaxscaler-from-scikit-learn-1-3-0-gives-different-result-in-different-ide-for</guid>
      <pubDate>Sat, 10 Feb 2024 21:16:59 GMT</pubDate>
    </item>
    <item>
      <title>创建神经网络和训练的问题</title>
      <link>https://stackoverflow.com/questions/77974163/issues-with-creating-a-neural-network-traning</link>
      <description><![CDATA[我已经被模型无法训练的问题困扰了一段时间，我希望得到一些帮助。这是部分代码
link =“https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2”
input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=“输入”, trainable=False)
feature_extractor = hub.KerasLayer(link, trainable=False)

模型 = tf.keras.Sequential([
    输入层，
    特征提取器，
    tf.keras.layers.Dense(data_info.features[&#39;label&#39;].num_classes,activation=“softmax”)
]）

这是我收到的错误：
TypeError Traceback（最近一次调用最后一次）
[64] 中的单元格，第 7 行
      4 input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=“输入”)
      5 feature_extractor = hub.KerasLayer(link, trainable=False)
----&gt; 7 模型 = tf.keras.Sequential([
      8 输入层，
      9 特征提取器，
     10 tf.keras.layers.Dense(data_info.features[&#39;label&#39;].num_classes, 激活=“softmax”)
     11]）
     13 model.compile(optimizer=“adam”,loss=“sparse_categorical_crossentropy”,metrics=[“accuracy”])
     14 模型.summary()

文件 c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\trackable\base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args) , **夸格斯)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\utils\traceback_utils.py:70，位于filter_traceback..error_handler(*args, * *夸格斯）
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
...


层“keras_layer_28”接收的调用参数（类型 KerasLayer）：
  输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
  • 培训=无

我尝试了许多不同的方法，例如添加或删除输入层以及更改该层中的一些参数，但我运气不佳。我想知道这是否是因为我提取的网络造成的，但想首先得到一些确认，因为我不能 100% 确定如何确定这是否是问题所在。]]></description>
      <guid>https://stackoverflow.com/questions/77974163/issues-with-creating-a-neural-network-traning</guid>
      <pubDate>Sat, 10 Feb 2024 17:59:42 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个图像训练序列模型</title>
      <link>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</link>
      <description><![CDATA[我在传递多个图像进行训练时遇到错误。但是当只传递一张图像时就很好了。图像大小相同。
这是代码：
导入tensorflow为tf
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras导入图层、数据集、模型

# 加载模板图像
template_image = tf.keras.preprocessing.image.load_img(&#39;模板.jpg&#39;)
template_array = tf.keras.preprocessing.image.img_to_array(template_image)

# 加载实际图像
实际图像 = tf.keras.preprocessing.image.load_img(&#39;实际.jpg&#39;)
实际数组 = tf.keras.preprocessing.image.img_to_array(实际图像)

# 创建模型
模型 = tf.keras.Sequential([
  层.InputLayer(input_shape=(template_array.shape)),
  层.Conv2D(16, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Conv2D(32, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Flatten(),
  层.Dense(64, 激活=&#39;relu&#39;),
  层.Dense(2, 激活=&#39;softmax&#39;),
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
模型.summary()
对于 model.layers 中的图层：
    打印（层.output_shape）

template_array = template_array.reshape((1, 549, 549, 3))
实际数组 = 实际数组.reshape((1, 549, 549, 3))
train_x = [模板数组，实际数组]
y_train = np.array([1,0])
y_train = y_train.reshape(1,2)
train_y = [y_train, y_train]

print(&quot;X 形状是：&quot;)
打印（模板_数组.形状）
print(&quot;Y 形状是：&quot;)
打印（y_train）

# 训练模型
model.fit(x=train_x, y=train_y, epochs=10)

＃ 作出预测
预测 = model.predict([actual_array])

# 检查是否有错误或缺失的部分
对于范围内的 i(len(预测))：
  如果预测[i][0]&gt;预测[i][1]：
    print(&#39;第 {} 部分丢失或不正确&#39;.format(i))


收到错误：
ValueError：层“sequential_28”预计有 1 个输入，但它收到了 2 个输入张量。收到的输入：
[&lt;tf.Tensor &#39;IteratorGetNext:0&#39; shape=(None, 549, 549, 3) dtype=float32&gt;,
]

如果我传递 x=template_array 和 y = y_train 它运行良好。但这意味着我只使用一张图像进行训练。
我是否无法使图像数组和相应的分类数组同时通过？我如何立即传递所有列车数据？]]></description>
      <guid>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</guid>
      <pubDate>Sat, 10 Feb 2024 16:28:24 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch ImageFolder 抛出错误“无效目录”，但 os.listdir() 确认目录有效</title>
      <link>https://stackoverflow.com/questions/77973551/pytorch-imagefolder-throws-error-invalid-directory-but-os-listdir-confirms-t</link>
      <description><![CDATA[因此，当我尝试使用 torchvision ImageFolder() 将图像文件夹加载到数据集中时。
我得到的错误是：
单元格 In[46]，第 299 行
    第295章
    298 print(os.listdir(“/kaggle/input/flickr-image-dataset/”))
--&gt; [第 299 章]
    第301章
    304 # 您最多可以向当前目录 (/kaggle/working/) 写入 20GB 的内容，当您使用“保存并保存”创建版本时，该目录将保留为输出运行全部”
    305 # 你也可以将临时文件写入/kaggle/temp/，但它们不会保存在当前会话之外

文件 /opt/conda/lib/python3.10/site-packages/compressai/datasets/image.py:64，在 ImageFolder.__init__(self、root、transform、split) 中
     61 splitdir = 路径（根）/ split
     63 如果不是 splitdir.is_dir():
---&gt; 64 raise RuntimeError(f&#39;无效目录“{root}”&#39;)
     66 self.samples = [f for f in splitdir.iterdir() if f.is_file()]
     68 self.transform = 变换

RuntimeError：无效目录“/kaggle/input/flickr-image-dataset/”

但是，对同一目录使用 os.listdir() 效果很好。
这是代码：
print(os.listdir(“/kaggle/input/flickr-image-dataset/”))

train_dataset = ImageFolder(“/kaggle/input/flickr-image-dataset/”, )
]]></description>
      <guid>https://stackoverflow.com/questions/77973551/pytorch-imagefolder-throws-error-invalid-directory-but-os-listdir-confirms-t</guid>
      <pubDate>Sat, 10 Feb 2024 15:00:50 GMT</pubDate>
    </item>
    <item>
      <title>3 个嵌入共线性的自定义损失函数 [关闭]</title>
      <link>https://stackoverflow.com/questions/77973255/custom-loss-function-for-collinearity-of-3-embeddings</link>
      <description><![CDATA[我正在尝试实现一个损失函数，该函数以 3 个嵌入作为输入，并输出一个与嵌入的共线性成比例的值。这是为了塑造用于嵌入插值的卷积自动编码器的潜在空间，如本文所述：Alon Oring 等人- 2020年。
我目前有代码可以在训练期间将嵌入作为张量获取，并使用基本的 mse 损失函数。我曾多次尝试实现这一点，但没有成功，我的训练损失总是会在开始时陷入困境。
您对在 pytorch 中实现此共线性损失函数及其与其他损失的总和有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77973255/custom-loss-function-for-collinearity-of-3-embeddings</guid>
      <pubDate>Sat, 10 Feb 2024 13:31:03 GMT</pubDate>
    </item>
    <item>
      <title>关于训练测试分割、SMOTE、PCA 的困惑[关闭]</title>
      <link>https://stackoverflow.com/questions/77973120/confusion-about-train-test-split-smote-pca</link>
      <description><![CDATA[我有一个高度不平衡的数据集，用于使用 SMOTE 的过采样技术。在使用 SMOTE 之前，我首先在 train test 之间分割数据集。然后我在训练数据集上应用PCA。然后使用PCA降维，再次需要训练应用PCA得到的测试分割。
这里我应用了两次训练-测试分割。因为在使用 SMOTE 应用训练测试之前，该训练集再次应用 PCA 。为了减少维度，我再次分割训练数据集。我的步骤有效吗？如果没有给我一个解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/77973120/confusion-about-train-test-split-smote-pca</guid>
      <pubDate>Sat, 10 Feb 2024 12:50:18 GMT</pubDate>
    </item>
    <item>
      <title>Autogluon 在训练 bagged 模型时不使用 GPU</title>
      <link>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</link>
      <description><![CDATA[我正在使用 Autogluon 0.8.2。我已经在 fit 方法中给出了相应的 GPU 参数，但我意识到在使用 GPU 在堆栈的第一层训练模型之后，当模型的袋装版本为下一个堆栈层进行训练时，GPU 不会在第一次堆栈训练后使用。我使用 nvidia-smi 检查了 GPU 利用率，结果为 0% 并且没有使用内存。
Autogluon 或袋装模型是否存在这样的训练问题？]]></description>
      <guid>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</guid>
      <pubDate>Sat, 10 Feb 2024 12:13:47 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列数据集训练模型，如何使用数据和时间作为输入特征？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</link>
      <description><![CDATA[我正在使用 房间占用估计数据集，包含 10129 个实例和 18 个特征。对于所有三个模型，预测准确率为 97-99%，我认为这是因为我删除了日期和时间列，因为它们是对象。
df.drop([&#39;日期&#39;,&#39;时间&#39;], axis = 1)
打印（df.count（））

但是，我想使用日期和时间来查看准确性是否有变化。但是，我不知道该怎么做，但我最初想到从时间中提取小时和分钟并将其用作输入特征。
df[&#39;Hour&#39;] = df[&#39;Time&#39;].dt.hour

有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</guid>
      <pubDate>Sat, 10 Feb 2024 09:46:38 GMT</pubDate>
    </item>
    <item>
      <title>用Python训练分类器</title>
      <link>https://stackoverflow.com/questions/77970572/training-classifier-in-python</link>
      <description><![CDATA[我用 Python (PyCharm) 编写了一个分类器。它不显示模型的训练阶段。如何在我的代码中解决这个问题？我想查看分类器的训练阶段。在 Jupiter 笔记本中工作时，所有内容都会显示（您可以在图片中看到它）。
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从sklearn导入数据集
从 sklearn.preprocessing 导入 LabelEncoder

CSV_COLUMN_NAMES = [&#39;SEPAL_LENGTH&#39;、&#39;SEPAL_WIDTH&#39;、&#39;PETAL_LENGTH&#39;、&#39;PETAL_WIDTH&#39;、&#39;SPECIES&#39;]

数据= pd.read_csv（&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39;，名称= CSV_COLUMN_NAMES，标题= 0）

编码器 = LabelEncoder()
数据[&#39;SPECIES&#39;] =编码器.fit_transform(data[&#39;SPECIES&#39;])
打印（数据.head（））
X = pd.DataFrame(data, columns=data.columns.drop(&#39;SPECIES&#39;))

y = data.pop(&#39;物种&#39;)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

def input_fn（特征，标签，训练= True，batch_size = 256）：
    数据集 = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    数据集 = 数据集.batch(10)
    如果训练：
        数据集 = dataset.shuffle(1000).repeat()

    返回数据集.shuffle(1000).repeat()

my_feature_columns = [] # 记录特征

对于 X_train.keys() 中的密钥：
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

分类器= tf.estimator.DNNClassifier（feature_columns = my_feature_columns，hidden_​​units =。[30,10]，n_classes = 3）


 classifier.train（input_fn = lambda：input_fn（X_train，y_train，训练= True），步骤= 5）

 classifier.evaluate(input_fn=lambda: input_fn(X_test, y_test, Training=False))
 print(&#39;\n测试集精度: {accuracy:0.3f}\n&#39;.format(**train_result))
]]></description>
      <guid>https://stackoverflow.com/questions/77970572/training-classifier-in-python</guid>
      <pubDate>Fri, 09 Feb 2024 20:04:15 GMT</pubDate>
    </item>
    <item>
      <title>shap Summary_plot 的子图 [重复]</title>
      <link>https://stackoverflow.com/questions/77969990/subplot-for-shap-summary-plot</link>
      <description><![CDATA[假设我们有以下简化代码：
导入 pandas 作为 pd
导入形状
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt
从 sklearn.preprocessing 导入 LabelEncoder
mylabel =LabelEncoder()
数据 =pd.read_csv(“https://raw.githubusercontent.com/krishnaik06/Multiple-Linear-Regression/master/50_Startups.csv”)
数据[&#39;状态&#39;] =mylabel.fit_transform(数据[&#39;状态&#39;])
打印（数据.head（））
模型 =RandomForestRegressor()
y =数据[&#39;利润&#39;]
X =data.drop(&#39;利润&#39;,axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=1)
model.fit(X_train,y_train)
解释器 =shap.TreeExplainer(模型)
shap_values =explainer.shap_values(X_train)
plt.figure(figsize=(30,30))
plt.子图(2,1,1)
shap.summary_plot（shap_values，X_train，feature_names = X.columns，plot_type =“bar”）
plt.子图(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)
plt.show()

当我运行此代码时，我在不同的图形上得到两个图像：
一张图片：

和另一张图片：

我想将它们绘制在一起，正如你所看到的，我使用了子图：
plt.subplot(2,1,1)
shap.summary_plot（shap_values，X_train，feature_names = X.columns，plot_type =“bar”）
plt.子图(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)

但它不起作用，我试图使用此代码：
fig，轴= plt.subplots（nrows = 2，ncols = 2，figsize =（10,10））
shap.dependence_plot(&#39;年龄&#39;, shap_values[1], X_train, ax=axes[0, 0], show=False)
shap.dependence_plot(&#39;收入&#39;, shap_values[1], X_train, ax=axes[0, 1], show=False)
shap.dependence_plot(&#39;分数&#39;, shap_values[1], X_train, ax=axes[1, 0], show=False)
plt.show()

但是summary_plot没有参数ax，那么我该如何使用它？]]></description>
      <guid>https://stackoverflow.com/questions/77969990/subplot-for-shap-summary-plot</guid>
      <pubDate>Fri, 09 Feb 2024 17:54:11 GMT</pubDate>
    </item>
    <item>
      <title>如何使用数据加载器解决这个问题？</title>
      <link>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</link>
      <description><![CDATA[我正在构建一些数据加载器来训练和测试机器学习模型。
我有一个名为“array”的元组列表像这样：
(Data(x=[468, 2], edge_index=[2, 1322], y=0, edge_weight=[1322]), &#39;morphed_img027485_img054553.png&#39;)
（数据（x=[468, 2]，edge_index=[2, 1322]，y=0，edge_weight=[1322]），&#39;morphed_img031737_img054553.png&#39;）

我像这样创建数据加载器：
data_loader = create_dataloader(数组，batch_size=60)
save_dataloader(data_loader, &#39;DataLoader 名称&#39;)

输出不是我所期望的，但它将所有数据合并到一个 DataBatch 中，如下所示：
[DataBatch(x=[936, 2]，edge_index=[2, 2644]，y=[2]，edge_weight=[2644]，batch=[936]，ptr=[3])， (&#39;morphed_img031737_img054553.png&#39;, &#39;morphed_img027485_img054553.png&#39;)]

为什么？如何拥有一个数据加载器，将所有数据像在数组中一样分开？]]></description>
      <guid>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</guid>
      <pubDate>Fri, 09 Feb 2024 14:52:39 GMT</pubDate>
    </item>
    <item>
      <title>就地修剪 nn.Linear 权重会导致意外错误，需要稍微奇怪的解决方法。需要解释</title>
      <link>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</link>
      <description><![CDATA[失败
导入火炬

def 测试1():
  层 = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试1()

有错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-bb36a010bd86&gt;在&lt;细胞系：10&gt;()
      8 x = 5 - torch.sum(layer(torch.ones(90)))
      9 x.backward()
---&gt; 10 测试1()
     11 # 这也有效
     12

2帧
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
    249 # 一些 Python 版本打印多行函数的第一行
    [第 250 章]
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError: 函数 TBackward0 在索引 0 返回无效渐变 - 得到 [10, 90] 但预期形状与 [10, 100] 兼容

这有效
导入火炬

def test2():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x #主要变化
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试2()

这也有效
导入火炬
def test3():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight) #主要变化
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试3()

我在尝试实现一篇关于模型剪枝（Temporal Neuron Variance Pruning）的论文时遇到了这个问题。我相信这与 autograd 图有关，但我不确定到底发生了什么。我已经看到了有关修剪的链接，并使用第三个片段让我的代码正常工作。我现在正在尝试找出为什么 1 和 2 不起作用。是否有一些解释为什么这些几乎相同的代码片段有效或失败？
我想弄清楚的要点 -

什么是TBackward0
在哪里定义的
哪里引发了运行时错误
为什么需要与旧形状兼容 - 特别是当梯度已正确修改时（我假设我已正确编辑张量，因为情况 2、3 有效）
我可以更改其他内容（除了 2 个工作案例之外）来实现此功能吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</guid>
      <pubDate>Thu, 08 Feb 2024 05:17:32 GMT</pubDate>
    </item>
    <item>
      <title>WEKA凯姆包</title>
      <link>https://stackoverflow.com/questions/77934889/weka-caim-package</link>
      <description><![CDATA[在网络搜索中找不到任何用于 CAIM 离散化的 WEKA 包。我需要 WEKA v3 的软件包。
在 google 上搜索 WEKA 软件包，但没有找到任何内容，但一些文档 说它存在。
谁能提供 WEKA 的 CAIM 包的工作链接吗？]]></description>
      <guid>https://stackoverflow.com/questions/77934889/weka-caim-package</guid>
      <pubDate>Sun, 04 Feb 2024 07:13:41 GMT</pubDate>
    </item>
    <item>
      <title>Conda 环境未显示在 JupyterLab 桌面中</title>
      <link>https://stackoverflow.com/questions/69529374/conda-environment-not-showing-in-jupyterlab-desktop</link>
      <description><![CDATA[我在系统中安装了 anaconda，环境名为 ML_env，Python 版本为 3.6.13。 JupyterLab 浏览器可以在安装了各种机器学习库的环境中顺利运行。
最近，我安装了 JupyterLab 桌面版 - 版本 3.1.13-1。我想将 JupyerLab Desktop 的内核更改为 anaconda 环境ML_env。但是，我没有获得任何更改内核的选项。请查看随附的屏幕截图。
有没有办法将JupyterLab桌面的内核更改为conda环境？请帮忙。 ]]></description>
      <guid>https://stackoverflow.com/questions/69529374/conda-environment-not-showing-in-jupyterlab-desktop</guid>
      <pubDate>Mon, 11 Oct 2021 16:12:47 GMT</pubDate>
    </item>
    <item>
      <title>r 神经网络包——多输出</title>
      <link>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</link>
      <description><![CDATA[我目前使用神经网络的方式是它从许多输入点预测一个输出点。更具体地说，我运行以下命令。
nn &lt;- 神经网络(
as.公式(a ~ c + d),
数据 = Z，隐藏 = c（3,2），err.fct =“sse”，act.fct = 自定义，
线性.输出=真，重复= 5）

这里，如果 Z 是一个由名称为 a、b、c 的列组成的矩阵，它将根据 c 行和 d 行中的对应点预测 a 列中某一行的一个点。 （以垂直维度作为训练样本。）
假设还有一列 b。我想知道是否有办法从 c 和 d 预测 a 和 b？我已经尝试过
as.formula(a+b ~ c+d)

但这似乎不起作用。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</guid>
      <pubDate>Thu, 07 Jan 2016 19:29:54 GMT</pubDate>
    </item>
    </channel>
</rss>