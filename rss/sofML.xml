<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 16 Jan 2025 03:18:45 GMT</lastBuildDate>
    <item>
      <title>有效覆盖 pytorch 数据集</title>
      <link>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</link>
      <description><![CDATA[我想继承 torch.utils.data.Dataset 类来加载我的自定义图像数据集，比如说用于分类任务。这是官方 pytorch 网站的示例，位于此 链接:
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
self.img_labels = pd.read_csv(annotations_file)
self.img_dir = img_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.img_labels)

def __getitem__(self, idx):
img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
image = read_image(img_path)
label = self.img_labels.iloc[idx, 1]
if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)
return image, label

我注意到：

在 __getitem__ 中，我们将图像从磁盘读取到内存中。这意味着如果我们训练模型几个时期，我们会多次将同一幅图像重新读入内存。据我所知，这是一个代价高昂的操作
每次从磁盘读取图像时都会应用一次变换，在我看来，这几乎是一个多余的操作。

我理解，在非常大的数据集中，我们无法将数据完全放入内存中，因此我们别无选择，只能以这种方式读取数据（因为我们必须在一个时期内迭代所有数据），我想知道，如果我的所有数据都可以放入内存中，那么在 __init__ 函数中从磁盘读取所有数据不是更好的方法吗？
通过我在计算机视觉方面的一点经验，我注意到在 变换 中将图像裁剪成固定大小的图像非常常见。那么为什么我们不应该裁剪图像一次并将其存储在磁盘上的其他地方，而在整个训练过程中只读取裁剪后的图像呢？在我看来，这似乎是一种更有效的方法。
我理解，一些用于增强而不是规范化的转换最好应用于 __getitem__ 中，以便获得随机生成的数据而不是固定的数据。
你能为我澄清一下这个主题吗？
如果我缺少的是常识，请用正确的方法指导我找到代码库。]]></description>
      <guid>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</guid>
      <pubDate>Thu, 16 Jan 2025 02:38:21 GMT</pubDate>
    </item>
    <item>
      <title>创建算法的算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79360176/algorithm-that-creates-algorithms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360176/algorithm-that-creates-algorithms</guid>
      <pubDate>Thu, 16 Jan 2025 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov5 创建的模型执行脚本时出现图像大小调整问题</title>
      <link>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</guid>
      <pubDate>Thu, 16 Jan 2025 00:27:41 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 中 .fit() 方法的用例？</title>
      <link>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</link>
      <description><![CDATA[是否存在 .fit() 比 .fit_transform() 更实用的情况/用例？例如，当标签编码时：
encoder = LabelEncoder()
title = data[&#39;title&#39;]
encoder.fit(title)
title_encoded =coder.transform(title)

vs
encoder = LabelEncoder()
title = data[&#39;title&#39;]
title_encoded =coder.fit_transform(title)
]]></description>
      <guid>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</guid>
      <pubDate>Thu, 16 Jan 2025 00:04:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 库训练 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</link>
      <description><![CDATA[我想创建一个 ML 模型，用于识别 MNIST 数据集中的手写数字。ML 模型是使用 OpenCV 库用 C++ 编写的。在构建模型并训练数据后，我在输出层中得到的数据总是 Nan 或 Inf。
我尝试更改训练参数，但没有任何效果。我尝试处理一个较小的 100 个单位的数据集，在该数据集中，我设法在输出层中获取一些值，但其中大多数仍然是 Nan。我就是找不到原因。以下是 ML 模型的代码：
cv::Ptr&lt;cv::ml::ANN_MLP&gt; mlp = cv::ml::ANN_MLP::create();
mlp-&gt;setActivationFunction(cv::ml::ANN_MLP::SIGMOID_SYM, 1, 1);

int inputLayerSize = imagesData[0].total();
if (inputLayerSize &gt; std::numeric_limits&lt;int&gt;::max()) {
throw std::overflow_error(&quot;inputLayerSize 超出 int 的最大值&quot;);
}
size_t hiddenLayerSize = 100;
size_t outputLayerSize = 10;

cv::Mat layer = (cv::Mat_&lt;int&gt;(3, 1)&lt;&lt;inputLayerSize, hiddenLayerSize, outputLayerSize);

mlp-&gt;setLayerSizes(layers);

int numSamples = imagesData.size();

cv::Mat trainingData(numSamples, inputLayerSize, CV_32F);
cv::Mat labelData(numSamples, outputLayerSize, CV_32F);

for (int i = 0; i &lt; numSamples; i++) {

cv::Mat image = imagesData[i].reshape(1, 1);
image.convertTo(trainingData.row(i), CV_32F);

cv::Mat label = cv::Mat::zeros(1, outputLayerSize, CV_32F);
la​​bel.at&lt;float&gt;(0, labelsData[i]) = 1.0;
label.copyTo(labelData.row(i));

}

cv::TermCriteria termCrit(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS, 10, 0.001);
mlp-&gt;setTermCriteria(termCrit);

mlp-&gt;setTrainMethod(cv::ml::ANN_MLP::BACKPROP, 0.001, 0.1);

mlp-&gt;train(trainingData, cv::ml::ROW_SAMPLE, labelData);

我还对像素数据进行了标准化。]]></description>
      <guid>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</guid>
      <pubDate>Wed, 15 Jan 2025 21:50:41 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯分类器代码错误。需要修复问题 7 和问题 8 中的运行时错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/79359212/bayes-classifier-code-error-need-fix-for-a-runtime-error-in-problem-7-and-prob</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359212/bayes-classifier-code-error-need-fix-for-a-runtime-error-in-problem-7-and-prob</guid>
      <pubDate>Wed, 15 Jan 2025 17:38:08 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv1 模型对每幅图像产生相同的结果</title>
      <link>https://stackoverflow.com/questions/79358877/the-yolov1-model-produces-the-same-results-for-each-image</link>
      <description><![CDATA[该模型 (Yolov1) 是用 PyTorch 编写的。训练了 30 个 epoch，使用 AdamOptimazer 进行训练，学习率 = 1e-6，batch_size = 64。DeepScoresV2 密集数据集：训练样本中有 1362 张照片，我从数据集中只选取了 3 个类别。但模型对所有照片都返回相同的结果。可能是什么问题？
任务是识别乐谱上的乐谱，以及搜索特殊符号（accolade 和 keys）。
示例（&#39;Brace&#39; 出现在空单元格中，这是输出图像的函数的错误，数据集本身一切正常）：
示例
训练样本中的类别分布（我只从中取 &#39;brace&#39;、&#39;staff&#39; 和 &#39;ottavaBracket&#39;）：
类别
模型架构：
class ConvCombo(nn.Module):
def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding=1):
super().__init__()
self._stack = nn.Sequential(
nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),
# nn.BatchNorm2d(out_channels),
nn.LeakyReLU(0.1),
)
def forward(self, x):
return self._stack(x)

class YOLOv1(nn.Module):
def __init__(self, S: int=7, C: int=20, B: int=2): # 与原文相同
super().__init__()
self._conv_stack = nn.Sequential(
ConvCombo(in_channels=3, out_channels=64, kernel_size=S, stride=2, padding=3),
nn.MaxPool2d(kernel_size=2,步幅=2),
ConvCombo(输入通道=64，输出通道=192，内核大小=3),
nn.MaxPool2d(内核大小=2，步幅=2),
ConvCombo(输入通道=192，输出通道=128，内核大小=1，填充=0),
ConvCombo(输入通道=128，输出通道=256，内核大小=3),
ConvCombo(输入通道=256，输出通道=256，内核大小=1，填充=0),
ConvCombo(输入通道=256，输出通道=512，内核大小=3),
nn.MaxPool2d(内核大小=2，步幅=2),
# x4 -&gt;
ConvCombo（输入通道=512，输出通道=256，内核大小=1，填充=0），
ConvCombo（输入通道=256，输出通道=512，内核大小=3），
ConvCombo（输入通道=512，输出通道=256，内核大小=1，填充=0），
ConvCombo（输入通道=256，输出通道=512，内核大小=3），
ConvCombo（输入通道=512，输出通道=256，内核大小=1，填充=0），
ConvCombo（输入通道=256，输出通道=512，内核大小=3），
ConvCombo（输入通道=512，输出通道=256，内核大小=1，填充=0），
ConvCombo（输入通道=256， out_channels=512, kernel_size=3),
# &lt;-
ConvCombo(in_channels=512, out_channels=512, kernel_size=1, padding=0),
ConvCombo(in_channels=512, out_channels=1024, kernel_size=3),
nn.MaxPool2d(kernel_size=2, stride=2),
# x2 -&gt;
ConvCombo（输入通道=1024，输出通道=512，内核大小=1，填充=0），
ConvCombo（输入通道=512，输出通道=1024，内核大小=3），
ConvCombo（输入通道=1024，输出通道=512，内核大小=1，填充=0），
ConvCombo（输入通道=512，输出通道=1024，内核大小=3），
# &lt;-
ConvCombo（输入通道=1024，输出通道=1024，内核大小=3），
ConvCombo（输入通道=1024，输出通道=1024，内核大小=3，步幅=2，填充=1），
ConvCombo（输入通道=1024，输出通道=1024，内核大小=3， padding=1),
ConvCombo(in_channels=1024, out_channels=1024, kernel_size=3),
)
self._conn_stack = nn.Sequential(
nn.Flatten(), # 将连续范围的 dims 展平为张量
nn.Linear(in_features=(1024 * S * S), out_features=4096),
nn.Dropout(0.0),
nn.LeakyReLU(0.1),
nn.Linear(in_features=4096, out_features=(S * S * (B * 5 + C))),
)

def forward(self, x):
out = self._conv_stack(x)
out = self._conn_stack(out)
return out

def process(self, x):
return self(x)

模型训练后输出的结果：
结果
我正在尝试训练模型识别乐谱和其他一些符号，但输出对每张图片的预测都相同。我已经尝试过改变学习率，但没有产生任何结果。]]></description>
      <guid>https://stackoverflow.com/questions/79358877/the-yolov1-model-produces-the-same-results-for-each-image</guid>
      <pubDate>Wed, 15 Jan 2025 15:47:38 GMT</pubDate>
    </item>
    <item>
      <title>该模型无法提前几步预测数据</title>
      <link>https://stackoverflow.com/questions/79358267/the-model-does-not-predict-the-data-several-steps-ahead</link>
      <description><![CDATA[我正在尝试训练一个模型来提前 2 步预测关闭列。准备好数据集：



datetime
close
close_forward
close_shift_1
close_shift_2
close_shift_3
close_shift_4
close_shift_5




2024-10-01 10:05:00
4009.0
4020.0
4002.5
3994.5
3993.0
3991.0
4007.5


2024-10-01 10:06:00
4020.5
4018.0
4009.0
4002.5
3994.5
3993.0
3991.0


2024-10-01 10:07:00
4020.0
4018.5
4020.5
4009.0
4002.5
3994.5
3993.0


2024-10-01 10:08:00
4018.0
4010.5
4020.0
4020.5
4009.0
4002.5
3994.5


2024-10-01 10:09:00
4018.5
4017.0
4018.0
4020.0
4020.5
4009.0
4002.5


2024-10-01 10:10:00
4010.5
4010.0
4018.5
4018.0
4020.0
4020.5
4009.0



其中 close_forward 是 2 步后关闭的值，close_shift 是后退几步的值。
训练后的线性回归
column_names = column_names =[&#39;close&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;&#39;close_shift_3&#39;&#39;close_shift_4&#39;, &#39;close_shift_5&#39;]
X_train = df[column_names].values
y_train = df[&#39;сlose_forward&#39;].values

scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))

model = Ridge(alpha=0.0001)
# model.fit(X_train, y_train)
model.fit(X_train_scaled, y_train_scaled)

X_test_scaled = scaler_X.transform(X_test)

# y_pred = model.predict(X_test)
y_pred_scaled = model.predict(X_test_scaled)

y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()

构建了一个图表
在此处输入图片说明
如果放大其中一个部分
在此处输入图片说明
我们可以看到，模型按要求预测了 close 而不是 test 的值。错误可能是什么，如何修复？
需要训练模型，以便它能够提前 2 步进行预测。我尝试使用其他步骤、其他模型（梯度提升和 lstm）进行训练，但效果是一样的]]></description>
      <guid>https://stackoverflow.com/questions/79358267/the-model-does-not-predict-the-data-several-steps-ahead</guid>
      <pubDate>Wed, 15 Jan 2025 12:44:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 GCP 服务进行图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79356912/how-to-use-gcp-service-for-image-classification</link>
      <description><![CDATA[我从物联网设备获取图像作为输入，发送到云端，进行图像分类，并将结果发送回某个 URL。
我尝试使用本地模型为 TF 提供 docker 镜像，并在我的设备上进行分类。
（云端 Ml 部署新手）]]></description>
      <guid>https://stackoverflow.com/questions/79356912/how-to-use-gcp-service-for-image-classification</guid>
      <pubDate>Wed, 15 Jan 2025 02:26:56 GMT</pubDate>
    </item>
    <item>
      <title>我应该将目标变量编码为二进制还是仅将其用作逻辑回归的随机梯度上升中的实际值[关闭]</title>
      <link>https://stackoverflow.com/questions/79356129/should-i-encode-my-target-variable-into-binary-or-just-use-it-as-real-value-in-s</link>
      <description><![CDATA[逻辑回归中有 Yi，其公式为公式。
Yi 是目标的实际值还是 Yi 应该是二进制的。如果我的目标变量 / Yi 在问题集中编码为 y 元素 {1 和 -1}，会怎么样？请记住，我使用的是随机梯度上升。所以我的问题集包括来自不同特征 (x) 的数据以及目标变量 (y)。问题还指出，y 已被编码为 1 作为正值，-1 作为负值。但是，当我使用逻辑回归时，我通常将目标变量设置为 0 或 1。因此，在继续使用上图的公式之前，我应该先将其编码为 0/1 吗？还是应该按原样使用 {1,-1}。]]></description>
      <guid>https://stackoverflow.com/questions/79356129/should-i-encode-my-target-variable-into-binary-or-just-use-it-as-real-value-in-s</guid>
      <pubDate>Tue, 14 Jan 2025 19:10:31 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 detector2 中语义分割的每个类的像素总数</title>
      <link>https://stackoverflow.com/questions/78583802/how-to-count-total-number-of-pixels-of-each-class-for-semantic-segmentation-in-d</link>
      <description><![CDATA[我想计算每个分割类的总像素数，我只需要每个一般对象的计数，例如每辆车一个类，每个人一个类等等。出于这个原因，我使用语义分割而不是实例分割（实例分割会分别考虑每个车辆或人员实例）。但detectron2中语义分割的输出没有二进制掩码。
我知道实例分割的输出是二进制掩码，可以使用以下代码获取像素数：
masks = output[&#39;instances&#39;].pred_masks 
results = torch.sum(torch.flatten(masks, start_dim=1),dim=1)

这给出了像素数，但分别考虑了每个车辆实例，这是我不想要的。
但是语义分割的输出是字段“sem_seg”，其中包含每个一般类的预测类概率而不是二元掩码，我怎样才能继续获取语义分割中每个类的像素数？]]></description>
      <guid>https://stackoverflow.com/questions/78583802/how-to-count-total-number-of-pixels-of-each-class-for-semantic-segmentation-in-d</guid>
      <pubDate>Wed, 05 Jun 2024 22:40:27 GMT</pubDate>
    </item>
    <item>
      <title>如何理解二元分类问题的Shapley值？</title>
      <link>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</link>
      <description><![CDATA[我对 shap python 包非常陌生。我想知道我应该如何解释二元分类问题的 shapley 值？这是我到目前为止所做的。
首先，我使用 lightGBM 模型来拟合我的数据。类似于
import shap
import lightgbm as lgb

params = {&#39;object&#39;:&#39;binary, 
...}
gbm = lgb.train(params, lgb_train, num_boost_round=300)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])

由于这是一个二元分类问题。shap_values 包含两个部分。我假设一个是针对类别 0，另一个是类别 1。如果我想知道一个特征的贡献。我必须绘制两个如下图所示的图形。
针对类别 0

针对类别 1

但是我应该如何进行更好的可视化呢？结果无法帮助我理解“cold_days 会增加输出成为 1 类还是 0 类的概率？”
使用相同的数据集，如果我使用 ANN，输出就是那样的。我认为 shapley 结果清楚地告诉我“cold_days”将积极增加结果成为 1 类的概率。
我感觉 LightGBM 输出有问题，但我不知道如何修复它。我怎样才能获得更清晰的类似于 ANN 模型的可视化效果？
#编辑
我怀疑我错误地使用了 lightGBM 来获得奇怪的结果。以下是原始代码
import lightgbm as lgb
import shap

lgb_train = lgb.Dataset(x_train, y_train, free_raw_data=False)
lgb_eval = lgb.Dataset(x_val, y_val, free_raw_data=False)
params = {
&#39;boosting_type&#39;: &#39;gbdt&#39;,
&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: &#39;binary_logloss&#39;,
&#39;num_leaves&#39;: 70,
&#39;learning_rate&#39;: 0.005,
&#39;feature_fraction&#39;: 0.7,
&#39;bagging_fraction&#39;: 0.7,
&#39;bagging_freq&#39;: 10,
&#39;verbose&#39;: 0,
&#39;min_data_in_leaf&#39;: 30,
&#39;max_bin&#39;: 128,
&#39;max_depth&#39;: 12,
&#39;early_stopping_round&#39;: 20,
&#39;min_split_gain&#39;: 0.096,
&#39;min_child_weight&#39;: 6,
}

gbm = lgb.train(params,
lgb_train,
num_boost_round=300,
valid_sets=lgb_eval,
)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])
]]></description>
      <guid>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</guid>
      <pubDate>Tue, 02 Feb 2021 21:52:39 GMT</pubDate>
    </item>
    <item>
      <title>训练历史与验证历史非常相似可以吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</link>
      <description><![CDATA[我训练了一个模型 50 个 epoch，按以下比例分割数据集：

X_train, Y_train = 70%
X_validation, Y_validation = 20%
X_test, Y_test = 10%

所有分割均使用 train_test_split(shuffle=True) keras 函数完成：
X = np.load(....)
Y = np.load(....)

# 在训练和验证上进行分割
N_validation = int(len(X) * 0.2)
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)

# 再次分割训练数据以获取测试数据
N_test = int(len(X_train) * 0.1)
X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=N_test)

这是历史图。
从历史中可以看出，验证准确率/损失与训练准确率/损失非常相似。有时验证损失甚至低于训练损失。
至于最后这句话，我在这里读到，这可能导致较高的 dropout 值。可能是这种情况，因为我有一个 rate=0.3 的 dropout 层。
我不明白这是否是个问题。
在测试集上测试模型，我的准确率为91%。]]></description>
      <guid>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</guid>
      <pubDate>Fri, 08 Jan 2021 11:23:50 GMT</pubDate>
    </item>
    <item>
      <title>SHAP值能解释一下吗？</title>
      <link>https://stackoverflow.com/questions/59035008/shap-value-can-explain-right</link>
      <description><![CDATA[我在使用 SHAP 值解释基于树的模型时遇到问题（https://github.com/slundberg/shap）。
首先，我输入了大约 30 个特征，其中 2 个特征之间存在高度正相关性。
之后，我训练 XGBoost 模型（python）并查看 2 个特征的 SHAP 值，发现 SHAP 值具有负相关性。
您能向我解释一下，为什么 2 个特征之间的输出 SHAP 值的相关性与输入相关性不一样吗？我可以相信 SHAP 的输出吗？
===========================
输入之间的相关性：0.91788
SHAP 值之间的相关性：-0.661088
2 个特征是

省份的人口
省份的家庭数量

模型性能
训练 AUC：0.73
测试 AUC：0.71
输入散点图（x：省份的家庭数量，y：省份的人口）：

SHAP 值输出散点图（x：省内家庭数量，y：省内人口）：
]]></description>
      <guid>https://stackoverflow.com/questions/59035008/shap-value-can-explain-right</guid>
      <pubDate>Mon, 25 Nov 2019 15:21:26 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归的随机梯度下降总是返回 Inf 的成本，并且权重向量永远不会接近</title>
      <link>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</link>
      <description><![CDATA[我正在尝试在 MATLAB 中实现逻辑回归求解器，并通过随机梯度下降找到权重。我遇到了一个问题，我的数据似乎产生了无限的成本，无论发生什么，它都不会下降……
这是我的梯度下降函数：
function weightVector = logisticWeightsByGradientDescentStochastic(trueClass,features)
%% 此函数尝试收敛到逻辑回归阶数为 1 的最佳权重集
%% 输入：
% trueClass - 训练数据的真实类值向量
% features
%% 输出：
% weightVector - 大小为 n+1 的向量（n 是特征数）
% 对应于收敛权重

%% 获取数据大小
dataSize = size(features);

%% 初始选择权重向量
weightVector = zeros(dataSize(2)+1, 1) %创建一个等于特征数量加 1 的零向量

%% 选择学习率
learningRate = 0.0001;

%% 初始成本
cost = logisticCost(weightVector, features, trueClass)

%% 随机梯度下降
costThresh = 0.05 %定义成本阈值

iterCount = 0;
while(cost &gt; costThresh)
for m=1:dataSize(1) %for all samples

%% test 语句
curFeatures = transpose([1.0 features(m,:)])

%% 计算 Sigmoid 预测值 
predictClass = assessSigmoid(weightVector , [1.0 features(m,:)] )

%% test 语句
truth = trueClass(m)

%% 计算所有特征的梯度
gradient = learningRate .* (trueClass(m) - predictClass) .* transpose([1.0 features(m,:)])

%% 通过从旧权重向量中减去梯度来更新权重向量
weightVector = weightVector - gradient 

%% 使用新权重向量重新评估 Cost
cost = logisticCost(weightVector, features, trueClass)

if(cost &lt; costThresh)
break
end
iterCount = iterCount + 1

结束 %for m
结束 %while cost &gt; 0.05

weightVector
iterCount
end

这是我的成本函数：
function cost = logisticCost(weightVector, features, trueClass)
%% 计算将 weightVector 应用于所有样本的总成本
%% 对于线性回归模型，根据
%% J(theta) = -(1/m) sum[ trueClass(log(predictedClass) + (1-trueClass)log(predictedClass)]
%% 输入：
% weightVector - n+1 个权重向量，其中 n 是特征数
% 加 1
% features - 特征矩阵
% trueClass - 训练数据的真实类别
%% 输出：
% cost - 总成本

dataSize = size(features); %获取数据大小

errorSum = 0.0; %存储错误总和
for m = 1:dataSize(1) %每行
predictedClass = assessEvaluateSigmoid(weightVector, [1.0 features(m,:)]); %评估 Sigmoid 来预测样本 m 的类别
if trueClass(m) == 1
errorSum = errorSum + log(predictedClass);
else
errorSum = errorSum + log(1 - predictClass);
end
end

cost = errorSum / (-1 .* dataSize(1)); % 乘以 -(1/m) 以获得成本
结束

这两者看起来都很好，我无法想象为什么我的成本函数总是会返回无穷大。
这是我的训练数据，其中第一列是类（1 或 0），接下来的七列是我要回归的特征。]]></description>
      <guid>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</guid>
      <pubDate>Fri, 17 Oct 2014 05:00:30 GMT</pubDate>
    </item>
    </channel>
</rss>