<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 15 Feb 2024 15:13:27 GMT</lastBuildDate>
    <item>
      <title>在地图中可视化回归预测结果[关闭]</title>
      <link>https://stackoverflow.com/questions/78001834/visualizing-regression-prediction-results-in-a-map</link>
      <description><![CDATA[我有 csv 格式的地下水位和降雨量现场数据，以及降雨量和地下水站的经度和纬度。地下水站共8个，雨量站3个。我使用降雨量和前期地下水位作为预测机器学习回归模型的输入变量来预测地下水位。现在，我希望预测结果以标记的形式显示在地图中，每个地下水站显示为标记，该标记将以不同颜色表示地下水位，该颜色随每个站预测地下水位的大小而变化。我有一些 Python 背景，并在 Python 中运行了我的预测模型（使用 Jupyter Notebook），但我希望获得有关在地图中可视化我的预测结果的帮助。
我在 YouTube 上观看了视频，阅读了期刊文章并尝试了 GitHub 上的一些代码。我找到的解决方案适用于 Google Earth Engine 上已有的遥感数据，但我想使用在站点中观察到的数据。]]></description>
      <guid>https://stackoverflow.com/questions/78001834/visualizing-regression-prediction-results-in-a-map</guid>
      <pubDate>Thu, 15 Feb 2024 15:07:04 GMT</pubDate>
    </item>
    <item>
      <title>对标签进行分类最有效的 NLP 方法是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78001318/what-is-the-most-effective-nlp-method-for-classifying-labels</link>
      <description><![CDATA[我正在参与一个自然语言处理 (NLP) 项目，旨在有效地实现特定领域文本数据的标签分类，并且我正在寻求一些专家建议来提高模型的准确性和性能。
用例概述：
目标：我的主要目标是根据用户提供的字符串预测多个标签（产品、角色和框架），这些标签与特定域密切相关。
用户输入示例：“显示要在 OSS 中使用的数据”测试经理”。
要预测的标签：OSS - 产品，测试经理 - 角色
尝试过的方法：

使用自定义数据训练 Spacy 模型作为命名实体识别 (NER)。
使用自定义数据微调 BertForTokenClassification，可随时在线获取。

当前状态：
虽然这两种方法都提供了输出，但它们似乎只产生了令人满意的结果。我对预测的准确性和稳健性并不完全满意。
请求专家建议：
我非常感谢该领域专家的见解、建议或替代方法。具体来说，我有兴趣了解任何可能提高标签分类模型性能的其他技术、模型架构或训练方法。]]></description>
      <guid>https://stackoverflow.com/questions/78001318/what-is-the-most-effective-nlp-method-for-classifying-labels</guid>
      <pubDate>Thu, 15 Feb 2024 13:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何进行“次主题情感”分析？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</link>
      <description><![CDATA[我必须找到句子中的子主题以及它们背后的情感，我面临的问题是一个句子可以有多个子主题，而我不知道不明白如何对他们单独进行情感分析。我该如何解决这个问题？
看下面的例子：
一个轮胎丢失，因此安装两个轮胎的时间有所延迟。车库处理这个问题的方式非常棒。
如果我们看一下上述评论的次主题情绪，我们会更清楚地了解这些内容是什么
一般都是。
发送的轮胎不正确为负 车库服务为正 等待时间为负
数据集：

我能够弄清楚如何在句子中找到子主题，但我无法单独找到每个子主题背后的情感，而只能找到整个句子的情感。
我一直在尝试让它们一起工作，但我不知道该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</guid>
      <pubDate>Thu, 15 Feb 2024 09:03:04 GMT</pubDate>
    </item>
    <item>
      <title>与未知数据的实体匹配[关闭]</title>
      <link>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</link>
      <description><![CDATA[相似的训练目标数据集对执行效果不好吗？
我将在数据帧上完成实体​​匹配任务。我决定使用基于机器学习的方法并使其通用。在这里，主要问题是我没有任何关于我的特定目标的标记数据集（即我有关于具有属性的人的标记数据集，但我想在关于汽车的数据帧上进行实体匹配）。我应该考虑哪种方法？如果我使用与目标没有相似性的标记数据集来训练模型，它会运行良好吗？如果可能的话，最佳介质是什么？在Google Scholar上，主要使用DITTO和DeepMatcher方法，但这些模型的f1分数是通过类似的训练目标数据集对观察的。]]></description>
      <guid>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</guid>
      <pubDate>Thu, 15 Feb 2024 08:30:49 GMT</pubDate>
    </item>
    <item>
      <title>我有一个 1510X132 功率输入和输出数据的数据集，我必须在数据上应用 DNN 模型来查找错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</link>
      <description><![CDATA[我已经应用了该模型并得到了 mse、mae 错误。我已经预测了数据，现在我必须绘制实际数据与预测数据图，以检查预测数据和实际数据是否彼此相似。我无法编码
实际=pd.DataFrame()
#y=pd.DataFrame()
j=1
对于范围内的 i(0, len(df.columns)-4, 4)：
     #实际[j]=df.iloc[0:1, i+1:i+5]
     实际[j] = df.iloc[0:2, i+1]
     实际[j+1] = df.iloc[0:2, i+2]
     #y[[j, j+1]]=df.iloc[:, i+3:i+5]
    `j+4`

plt.figure(figsize=(10, 6))
plt.plot((实际), label=&#39;实际&#39;)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</guid>
      <pubDate>Thu, 15 Feb 2024 06:21:44 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中运行 train 方法时将 PNG 图像转换为 np.array</title>
      <link>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</guid>
      <pubDate>Thu, 15 Feb 2024 01:22:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 LoRA 微调 ControlNet？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</link>
      <description><![CDATA[我想使用 LoRA 适配器微调 ControlNet。我的数据集由成对图像组成 - 分割图（没有家具的房间，只有墙壁、地板、窗户、门） - RGB 图像（有家具的房间）。我的目标是微调 ControlNet 以根据空房间的分割图生成带家具的房间（我使用空提示）
所以，我不太清楚如何训练 LoRA。我考虑了两个选择

训练 LoRA 进行稳定扩散（仅使用带家具的房间图像作为数据集），然后将此稳定扩散注入 ControlNet

例如，如果我使用这个管道
管道 = StableDiffusionControlNetPipeline.from_pretrained(
    “runwayml/stable-diffusion-v1-5”，controlnet=controlnet，torch_dtype=torch.float16
）

然后将“runwayml/stable-diffusion-v1-5”替换为与我精心调校的 LoRA 一起。这是正确的做法吗？如果我仅微调稳定扩散并且不显示我的分割图，ControlNet 是否能够生成房间？

在图像对上训练 LoRA for ControlNet。
然而，我读到 Huggingface 不支持为 ControlNet 训练 LoRA（所有教程仅展示如何训练稳定扩散）。但我找到了文章
https://fangchuan.github.io/ctrl-room.github.io/&lt; /a&gt;
作者对 ControlNet 进行了微调以生成房间全景图（但文章没有提供技术细节），因此我认为可以直接为 ControlNet 训练 LoRA

那么什么方法来微调 ControlNet 是普遍可以接受的呢？]]></description>
      <guid>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</guid>
      <pubDate>Wed, 14 Feb 2024 21:19:17 GMT</pubDate>
    </item>
    <item>
      <title>一类 SVM - 测试集上的异常值相对于训练集非常低</title>
      <link>https://stackoverflow.com/questions/77995464/one-class-svm-outliers-on-test-set-very-low-relative-to-training-set</link>
      <description><![CDATA[我正在使用 scikit-learn 一类 SVM 进行异常值检测。但相对于训练集，测试集上检测到的异常值数量非常少。
单类 SVM 的每个输入都是三个浮点数 [float1、float2、float3] 的列表。
所有列都使用最小-最大缩放比例缩放为 0 到 1 之间的值。
我按如下方式初始化并拟合 SVM：
clf = OneClassSVM(kernel=&#39;线性&#39;, nu=0.01, gamma=&#39;auto&#39;).fit(training_and_testing_sets[:TRAINING_SET_SIZE])

因为我对 nu 使用了 0.01 的值。我预计测试集上的异常值数量为整个测试集的 1%。但它是 0.004%。测试集也相应地缩放。
造成这种差异的原因是什么以及如何解决该问题？]]></description>
      <guid>https://stackoverflow.com/questions/77995464/one-class-svm-outliers-on-test-set-very-low-relative-to-training-set</guid>
      <pubDate>Wed, 14 Feb 2024 15:33:33 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测访问日期与客户类别图不准确</title>
      <link>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</link>
      <description><![CDATA[我正在尝试对一堆类和日期时间进行时间序列预测，但由于某种原因我的图表看起来像这样，我的完整代码如下：
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
plt.plot(out.index, out[&#39;count&#39;], )



而我从其中获取此时间序列代码的博客有这样的结果

所以我不确定是否应该继续XD
我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23
我尝试了下面的代码，但由于某种原因，第一个图是空的
plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
out.groupby(&#39;class_id&#39;).plot()
plt.plot(out.index, out[&#39;count&#39;], )

]]></description>
      <guid>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</guid>
      <pubDate>Wed, 31 Jan 2024 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>不寻常的学习率查找曲线：最小学习率时损失最低</title>
      <link>https://stackoverflow.com/questions/77082609/unusual-learning-rate-finder-curve-loss-lowest-at-smallest-learning-rate</link>
      <description><![CDATA[我正在使用 PyTorch Lightning 的 LR Finder，但得到了一条非典型曲线。当学习率最小时，损失从最低点开始，逐渐增加直至稳定，然后呈现典型的 U 形曲线。无论我从什么学习率开始（我尝试过 1e-12、1e-6、1e-3 等），同样的事情会发生，在最低学习率下损失总是最小的：

示例模型类：
`class simple_model(pl.LightningModule):
def __init__(self, 编码器名称, lr, **kwargs):
    超级().__init__()
    self.model = timm.create_model(encoder_name, pretrained=True)
    self.loss_fn = nn.BCEWithLogitsLoss()
    self.lr = lr

defforward（自身，图像）：

    pred = self.model(图像)
    返回预测值

def共享步骤（自身，批次，阶段）：
    
    图像 = 批处理[“图像”]
    标签=批次[“标签”]

    logits = self.forward(图像)
    损失 = self.loss_fn(logits.squeeze(),labels.float())

    返回 {
        “损失” ： 损失，
        “预测”：logits.sigmoid().round().squeeze().cpu().detach().numpy(),
        “true”：labels.cpu().detach().numpy()
    }

def shared_epoch_end(自身, 输出, 阶段):

    loss = [x[“loss”].item() for x in 输出]
    Whole_dataset_loss = np.mean(损失)
    self.log_dict({f&quot;{stage}_loss&quot;: Whole_dataset_loss}, prog_bar=True)

def Training_step（自身，批次，batch_idx）：
    返回 self.shared_step(batch, &quot;train&quot;)

def Training_epoch_end(自我，输出)：
    return self.shared_epoch_end(输出,“火车”)

defvalidation_step(self,batch,batch_idx):
    返回 self.shared_step(batch, &quot;valid&quot;)

defvalidation_epoch_end（自我，输出）：
    返回 self.shared_epoch_end(输出,“有效”)

def test_step（自身，批次，batch_idx）：
    返回 self.shared_step(batch, “测试”)

def test_epoch_end（自身，输出）：
    return self.shared_epoch_end(输出,“测试”)

def 配置_优化器（自身）：
    返回 torch.optim.Adam(self.parameters(), self.lr)

LR Finder 的使用示例：
trainer = pl.Trainer(accelerator=&#39;gpu&#39;,devices=1,max_epochs=140, precision=16, log_every_n_steps=1) lr_finder = trainer.tuner.lr_find(model,dataset)
我已经在多个数据集和模型类型（完全监督和 ssl）中看到了这种行为。几乎所有的实现方式都是从 pytorch lighting (1.9.0) 中开箱即用的，并且我的模型训练和收敛得很好，所以我不太确定如何解决这个问题。无论我使用 ImageNet 权重还是随机权重（全部来自 timm 库），都会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/77082609/unusual-learning-rate-finder-curve-loss-lowest-at-smallest-learning-rate</guid>
      <pubDate>Mon, 11 Sep 2023 14:42:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 mediapipe 不在实时反馈上绘制地标</title>
      <link>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</link>
      <description><![CDATA[这是我从 mediapipe 文档中获得的代码。我尝试了很多方法来在实时反馈中展示地标图，但似乎没有任何效果。我确实需要一些帮助来了解我错过的事情。
导入 mediapipe 作为 mp
从 mediapipe.tasks 导入 python
从 mediapipe.tasks.python 导入视觉
导入CV2
导入时间
将 mediapipe 导入为 mp
将 numpy 导入为 np
从 mediapipe 导入解决方案
从 mediapipe.framework.formats 导入地标_pb2
将 numpy 导入为 np

边距 = 10 # 像素
字体大小 = 1
字体粗细 = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54) # 鲜艳的绿色

BaseOptions = mp.tasks.BaseOptions
HandLandmarker = mp.tasks.vision.HandLandmarker
HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions
HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult
VisionRunningMode = mp.tasks.vision.RunningMode

# 使用直播模式创建一个手部地标实例：
def print_result(结果：mp.tasks.vision.HandLandmarkerResult，output_image：mp.Image，timestamp_ms：int)：
    print(&#39;手部地标结果：{}&#39;.format(result))

选项 = HandLandmarkerOptions(
    base_options=BaseOptions(model_asset_path=&#39;hand_landmarker.task&#39;),
    running_mode=VisionRunningMode.LIVE_STREAM,
    结果回调=打印结果）
使用 HandLandmarker.create_from_options(options) 作为地标：
    上限 = cv2.VideoCapture(0)
    而真实：
        ret, 框架 = cap.read()
        如果不转：
            休息
        frame_np = np.array(帧)
        时间戳 = int(round(time.time()*1000))
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_np)
        帧 = mp_image.numpy_view()
        结果 =landmarker.detect_async(mp_image, 时间戳)
        如果类型（结果）不是类型（无）：
           hand_landmarks_list = 结果.hand_landmarks
           对于范围内的 idx(len(hand_landmarks_list))：
                hand_landmarks = hand_landmarks_list[idx]

                # 绘制手部标志。
                hand_landmarks_proto =地标_pb2.NormalizedLandmarkList()
                hand_landmarks_proto.landmark.extend([
                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) 用于 hand_landmarks 中的地标
                ]）
                解决方案.drawing_utils.draw_landmarks(
                    框架，
                    hand_landmarks_proto，
                    解决方案.hands.HAND_CONNECTIONS,
                    解决方案.drawing_styles.get_default_hand_landmarks_style(),
                    Solutions.drawing_styles.get_default_hand_connections_style())
        别的：
            打印（&#39;其他&#39;）
        cv2.imshow(&#39;框架&#39;, 框架)
        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息
    cap.release()
    cv2.destroyAllWindows()

每次传递“结果函数”时，我都会收到此 NoneType 错误。我也不知道如何处理。 mediapipe 文档没有提供有关如何在实时源中显示此内容的任何见解。]]></description>
      <guid>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</guid>
      <pubDate>Thu, 22 Jun 2023 15:33:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中累积大批量的梯度</title>
      <link>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</link>
      <description><![CDATA[我正在使用一个对内存要求很高的 CNN 模型来执行分类任务。
这对我在训练期间可以使用的批量大小造成了很大的限制。
一种解决方案是在训练期间累积梯度，这意味着模型的权重不会在每个批次后更新。相反，相同的权重用于多个批次，而每个批次的梯度会被累积，然后针对单个权重更新操作进行平均。
我正在使用 Tensorflow 后端 Keras，并且我非常确定 Keras 没有现成的函数/方法来实现此目的。
如何为 Keras/tensorflow 模型完成此操作？]]></description>
      <guid>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</guid>
      <pubDate>Wed, 20 Mar 2019 19:26:43 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn RandomForestClassifier 中的子样本大小</title>
      <link>https://stackoverflow.com/questions/40847745/subsample-size-in-scikit-learn-randomforestclassifier</link>
      <description><![CDATA[如何控制用于训练森林中每棵树的子样本的大小？
根据 scikit-learn 的文档：&lt; /p&gt;
&lt;块引用&gt;
随机森林是一种适合多个决策的元估计器
数据集的各个子样本上的树分类器并使用
平均以提高预测准确性并控制过度拟合。
子样本大小始终与原始输入样本相同
大小，但如果 bootstrap=True，则通过替换来绘制样本
（默认）。

所以bootstrap允许随机性，但找不到如何控制子样本的数量。]]></description>
      <guid>https://stackoverflow.com/questions/40847745/subsample-size-in-scikit-learn-randomforestclassifier</guid>
      <pubDate>Mon, 28 Nov 2016 15:19:59 GMT</pubDate>
    </item>
    <item>
      <title>神经网络训练过程中出现 nan 的常见原因</title>
      <link>https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training-of-neural-networks</link>
      <description><![CDATA[我注意到训练期间经常出现的情况是引入 NAN。
通常情况下，它似乎是由内积/全连接或卷积层爆炸中的权重引入的。
发生这种情况是因为梯度计算崩溃了吗？还是因为权重初始化的原因（如果是的话，为什么权重初始化会有这个效果）？或者这可能是由输入数据的性质引起的？
这里的首要问题很简单：训练期间发生 NAN 的最常见原因是什么？其次，有哪些方法可以解决这个问题（以及为什么它们有效）？ p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training-of-neural-networks</guid>
      <pubDate>Fri, 27 Nov 2015 17:23:30 GMT</pubDate>
    </item>
    <item>
      <title>多变量梯度下降</title>
      <link>https://stackoverflow.com/questions/24411315/multi-variable-gradient-descent</link>
      <description><![CDATA[我正在学习梯度下降来计算系数。以下是我正在做的事情：
#!/usr/bin/Python

 将 numpy 导入为 np


   # 这里 m 表示示例的数量，而不是特征的数量
 defgradientDescent(x, y, theta, alpha, m, numIterations):
     xTrans = x.transpose()
     对于范围内的 i(0, numIterations)：
        假设 = np.dot(x, theta)
        损失 = 假设 - y
        # 每个示例的平均成本（2*m 中的 2 在这里并不重要。
        # 但为了与渐变保持一致，我将其包括在内）
        成本 = np.sum(损失 ** 2) / (2 * m)
        #print(&quot;迭代 %d | 成本：%f&quot; % (i, 成本))
        # 每个示例的平均梯度
        梯度 = np.dot(xTrans, 损失) / m
        ＃ 更新
        θ = θ - α * 梯度
     返回θ

 X = np.array([41.9,43.4,43.9,44.5,47.3,47.5,47.9,50.2,52.8,53.2,56.7,57.0,63.5,65.3,71.1,77.0,77.8])
 y = np.array([251.3,251.3,248.3,267.5,273.0,276.5,270.3,274.9,285.0,290.0,297.0,302.5,304.5,309.3,321.7,330.7,349.0])
 n = np.max(X.形状)
 x = np.vstack([np.ones(n), X]).T
 m, n = np.shape(x)
 迭代次数= 100000
 阿尔法 = 0.0005
 θ = np.ones(n)
 theta = 梯度下降(x, y, theta, alpha, m, numIterations)
 打印（θ）

现在我上面的代码工作正常了。如果我现在尝试多个变量并将 X 替换为 X1，如下所示：

&lt;预&gt;&lt;代码&gt; X1 = np.array([[41.9,43.4,43.9,44.5,47.3,47.5,47.9,50.2,52.8,53.2,56.7,57.0,63.5,65.3,71.1,77.0,77.8], [ 29.1,29.3,29.5,29.7,29.9,30.3,30.5,30.7,30.8,30.9,31.5,31.7,31.9,32.0,32.1,32.5,32.9]])

然后我的代码失败并显示以下错误：
 JustTestingSGD.py:14: RuntimeWarning: square 遇到溢出
  成本 = np.sum(损失 ** 2) / (2 * m)
  JustTestingSGD.py:19: RuntimeWarning: 减法中遇到无效值
  θ = θ - α * 梯度
  [楠楠楠]

谁能告诉我如何使用X1进行梯度下降？我使用 X1 的预期输出是：

&lt;前&gt;&lt;代码&gt;[-153.5 1.24 12.08]

我也对其他 Python 实现持开放态度。我只想要 X1 和 y 的系数（也称为 thetas）。]]></description>
      <guid>https://stackoverflow.com/questions/24411315/multi-variable-gradient-descent</guid>
      <pubDate>Wed, 25 Jun 2014 14:24:39 GMT</pubDate>
    </item>
    </channel>
</rss>