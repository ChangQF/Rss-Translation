<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 07 Jan 2025 15:59:54 GMT</lastBuildDate>
    <item>
      <title>对呼叫中心呼叫的原始音频文件（而非记录）进行情感分析[关闭]</title>
      <link>https://stackoverflow.com/questions/79335686/performing-sentiment-analysis-on-raw-audio-files-not-transcripts-for-call-cent</link>
      <description><![CDATA[我正在开展一项情绪分析任务，需要直接从呼叫中心对话的原始音频文件（而不是转录文本）分析客户的情绪。
为了解决这个问题，我通过合并 Ravdess、TESS 和 SAVEE 等多个数据集，对 Hugging Face 模型（特别是 Wav2VecForSequenceClassification）进行了微调。但是，这些数据集主要包含专业语音记录和短音频片段（每个 3-4 秒），并且该模型在较长的真实呼叫中心音频文件上表现不佳。
有人可以建议吗：

更好的数据集，包括更长、更多样化和更真实的呼叫中心音频以供微调？

是否有可能更适合此任务的替代预训练模型？

是否有任何在线服务或 API 可以直接对音频文件执行情感分析？

]]></description>
      <guid>https://stackoverflow.com/questions/79335686/performing-sentiment-analysis-on-raw-audio-files-not-transcripts-for-call-cent</guid>
      <pubDate>Tue, 07 Jan 2025 10:41:02 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习模型集成到 Windows 窗体中的问题</title>
      <link>https://stackoverflow.com/questions/79335677/problem-with-integrating-the-machine-learning-model-into-windows-forms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79335677/problem-with-integrating-the-machine-learning-model-into-windows-forms</guid>
      <pubDate>Tue, 07 Jan 2025 10:38:45 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中，为什么不从一开始就使用非线性函数来拟合数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/79334736/in-a-neural-network-why-not-use-a-nonlinear-function-to-fit-the-data-from-the-b</link>
      <description><![CDATA[在神经网络中，为什么不一开始就用非线性函数来拟合数据，而是用线性函数+激活函数来拟合数据呢？
我是神经网络的初学者，在学习神经元的时候，我发现从几何意义上讲，所有的输入数据都要先进行线性运算，然后对结果进行一步激活函数计算。激活函数的本质的确是为了解决线性模型无法拟合非线性数据的问题。
如果是这样，为什么不一开始就用非线性函数来拟合数据，这样会更直接呢？]]></description>
      <guid>https://stackoverflow.com/questions/79334736/in-a-neural-network-why-not-use-a-nonlinear-function-to-fit-the-data-from-the-b</guid>
      <pubDate>Tue, 07 Jan 2025 01:43:58 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将手势识别集成到 .NET MAUI 中？[关闭]</title>
      <link>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</link>
      <description><![CDATA[我有一个名为“手语应用”的项目，我需要在其中集成手语手势识别。我不知道从哪里开始，因为我只具备 C# 基础知识。
我需要指南或建议，看看是否可行。]]></description>
      <guid>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</guid>
      <pubDate>Mon, 06 Jan 2025 09:55:54 GMT</pubDate>
    </item>
    <item>
      <title>检测器模型中框的正确损失函数</title>
      <link>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</guid>
      <pubDate>Sun, 05 Jan 2025 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>如何指定使用集成分类器在网格搜索中进行迭代的级别？</title>
      <link>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</link>
      <description><![CDATA[我有以下设置，但我找不到在网格搜索中传递级别以探索 svm* 和 mlp* 的方法：
steps = [(&#39;preprocessing&#39;, StandardScaler()),
(&#39;feature_selection&#39;, SelectKBest(mutual_info_classif, k=15)),
(&#39;clf&#39;, VotingClassifier(estimators=[(&quot;mlp1&quot;, mlp1),
(&quot;mlp2&quot;, mlp2),
(&quot;mlp3&quot;, mlp3),
(&quot;svm1&quot;, svm1),
(&quot;svm2&quot;, svm2)
], voting=&#39;soft&#39;))
]

model = Pipeline(steps=steps)
params = [{
&#39;preprocessing&#39;: [StandardScaler(), MinMaxScaler(), MaxAbsScaler()],
&#39;feature_selection__score_func&#39;: [f_classif, mutual_info_classif]
}]

grid_search = GridSearchCV(model, params, cv=10,scoring=&#39;balanced_accuracy&#39;, verbose=1, n_jobs=20, refit=True)
]]></description>
      <guid>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</guid>
      <pubDate>Sun, 05 Jan 2025 11:22:57 GMT</pubDate>
    </item>
    <item>
      <title>负类的 precision_recall_curve 导致正相关的精度和召回率</title>
      <link>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</link>
      <description><![CDATA[我有一个逻辑回归模型来预测二进制输出（0 或 1）。我想了解 0 类的 P/R，并生成相应的曲线。我使用这个代码：
clf = linear_model.LogisticRegression().fit(ohe_X_train, y_train)
# 预测独热编码测试集上的标签
clf_predictions = clf.predict(ohe_X_test)
y_scores = clf.predict_proba(ohe_X_test)

class_of_interest = 0

# 基于 precision_recall_fscore_support 的 P/R
precision, recall, fscore, support = precision_recall_fscore_support(y_test, clf_predictions, labels=[class_of_interest])

# 使用 precision_recall_curve 的 P/R 曲线
y_scores = clf.predict_proba(ohe_X_test)[:, class_of_interest]
precision_curve, recall_curve, Thresholds = precision_recall_curve(y_test, y_scores)

plt.plot(recall_curve, precision_curve, marker=&#39;.&#39;)
plt.xlabel(&#39;Recall&#39;)
plt.ylabel(&#39;Precision&#39;)
plt.title(&#39;Precision-Recall Curve for Class 0&#39;)
plt.grid(True)
plt.show()

在这种情况下，PR 曲线在提高召回率的同时提高了准确率。我做错了什么？当 class_of_interest = 1 时，它工作正常。]]></description>
      <guid>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</guid>
      <pubDate>Fri, 03 Jan 2025 20:45:09 GMT</pubDate>
    </item>
    <item>
      <title>对随机森林进行修改，每次分割时都会评估某些特征</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（像往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。
我不知道有哪些软件包允许开箱即用。有没有一个，或者也许有一个简单的解决方法来实现相同的效果？]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能有效地执行此循环而不增加内存使用量？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>如何清除 doctr 模型使用的 gpu 内存</title>
      <link>https://stackoverflow.com/questions/78768994/how-to-clear-gpu-memory-used-by-doctr-model</link>
      <description><![CDATA[我正在使用 doctr 进行 ocr 操作。doctr 模型在 gpu 中运行，因此在我完成 ocr 工作后，它仍然加载在 gpu 内存中。
我正在尝试使用
with torch.no_grad(): torch.cuda.empty_cache()
清除 gpu 内存，但没有任何效果。
有没有其他方法可以在不使用 PID 终止进程的情况下清除 gpu 内存？
我想使用 doctr 对图像执行 ocr]]></description>
      <guid>https://stackoverflow.com/questions/78768994/how-to-clear-gpu-memory-used-by-doctr-model</guid>
      <pubDate>Fri, 19 Jul 2024 10:59:26 GMT</pubDate>
    </item>
    <item>
      <title>在 docker 中运行 doctr ml 模型永远挂起</title>
      <link>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</link>
      <description><![CDATA[我有一个 fastapi 应用程序，它在代码中加载 doctr 模型，它只需要一个图像路径并将其转换为文本
这是代码
从 doctr.io 导入 DocumentFile
从 doctr.models 导入 o​​cr_predictor
__version__ = &quot;0.1.1&quot;
model = ocr_predictor(pretrained=True)

def process_image(image_path):
document = DocumentFile.from_images(image_path)
result = model(document)
json_response = result.export()
return json_response

我只想通过 API 公开模型。
main.py 文件，其中设置了我的 api 代码


from app.model.model import __version__ as model_version
from app.model.model import process_image
from fastapi import FastAPI, HTTPException, UploadFile

app = FastAPI()

@app.get(&quot;/&quot;)
def home():
return {&quot;health_check&quot;: &quot;OK&quot;, &quot;model_version&quot;: &#39;model_version&#39;}

当我使用此代码运行 main.py 时代码
uvicorn app.main:app --reload
一切正常
但当我尝试将其 dockerize 时，它​​就永远挂起了，这是我的 Dockerfile
来自 tiangolo/uvicorn-gunicorn-fastapi:python3.9

运行 apt-get update

运行 apt install -y libgl1-mesa-glx

复制 ./requirements.txt /app/requirements.txt

运行 pip install --no-cache-dir --upgrade -r /app/requirements.txt

复制 ./app /app/app

注意：我有一台 m1 mac
我尝试了一切方法使其正常工作，包括将导入移动到函数内部
__version__ = &quot;0.1.1&quot;

def process_image(image_path):
from doctr.io import DocumentFile
from doctr.models import ocr_predictor
model = ocr_predictor(pretrained=True)
document = DocumentFile.from_images(image_path)
result = model(document)
json_response = result.export()
return json_response

但是，当 api 到达导入时，什么都不起作用，它只是永远挂起，而且只有在使用 docker 时才会出现这种情况]]></description>
      <guid>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</guid>
      <pubDate>Sun, 26 Nov 2023 14:46:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在图像的特定区域运行 mindee doctr？</title>
      <link>https://stackoverflow.com/questions/75916342/how-to-run-mindee-doctr-on-specific-area-of-image</link>
      <description><![CDATA[我使用下面的代码通过 mindee/doctr 包从图像中提取文本。
from doctr.models import  ocr_predictor
from doctr.io import DocumentFile

import json
model = ocr_predictor( reco_arch=&#39;crnn_vgg16_bn&#39;, pretrained=True,export_as_straight_boxes=True)
image = DocumentFile.from_images(&quot;docs/temp.jpg&quot;)
result = model(image)
result_json = result.export()
json_object =  json.dumps(result_json, indent=4)

with open(&quot;temp.json&quot;, &quot;w&quot;) as outfile:
   outfile.write(json_object)
result.show(image)

通过使用 mindee/doctr，我从 temp.json 文件中的图像中获取了整个文本。
OCR 后的输出：

我想从图像中获取特定区域的文本。
有什么好方法可以实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/75916342/how-to-run-mindee-doctr-on-specific-area-of-image</guid>
      <pubDate>Mon, 03 Apr 2023 05:47:13 GMT</pubDate>
    </item>
    <item>
      <title>批量数据在线方差更新的有效算法</title>
      <link>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data</link>
      <description><![CDATA[我有大量多维数据，想计算所有数据中某个轴的方差。从内存角度来看，我无法创建一个大型数组来一步计算方差。因此，我需要分批加载数据，并需要在每个批次之后以在线方式更新当前方差。
示例
最后，按批次更新的 online_var 应该与 correct_var 匹配。
但是，我很难找到一种有效的算法。
import numpy as np
np.random.seed(0)
# 正确计算方差
all_data = np.random.randint(0, 9, (9, 3)) # &lt;-- 不适合内存
correct_var = all_data.var(axis=0)
# 创建批次
batches = all_data.reshape(-1, 3, 3)

online_var = 0
for batch in batches:
batch_var = batch.var(axis=0)
online_var = ? # 如何正确更新
assert np.allclose(correct_var, online_var)


我找到了Welford 在线算法，但是它非常慢，因为它只更新单个新值的方差，即它不能一次处理整个批次。当我处理图像时，每个像素和每个通道都需要更新。

如何以有效的方式更新多个新观测值的方差，同时考虑整个批次？]]></description>
      <guid>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data</guid>
      <pubDate>Thu, 23 Feb 2023 14:10:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Tensor 转换请求 dtype 为 float64，而 dtype 为 float32</title>
      <link>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</link>
      <description><![CDATA[我有一个 NN，它有两个相同的 CNN（类似于 Siamese 网络），然后合并输出，并打算在合并的输出上应用自定义损失函数，如下所示：
 ----------------- -----------------
| input_a | | input_b |
----------------- -----------------
| base_network | | base_network |
------------------------------------------------------
|processed_a_b |
------------------------------------------------------

在我的自定义损失函数中，我需要将 y 垂直分成两部分，然后在每部分上应用分类交叉熵损失。但是，我不断从损失函数中获取 dtype 错误，例如：
ValueError Traceback（最近一次调用最后一次）&lt;ipython-input-12-b01f2c4c71e3&gt; in &lt;module&gt;()
----&gt; 1 model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)
909 loss_weight = loss_weights_list[i]
910 output_loss = weighted_loss(y_true, y_pred,
--&gt; 911 sample_weight, mask)
912 if len(self.outputs) &gt; 1:
913 self.metrics_tensors.append(output_loss)

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in weighted(y_true, y_pred, weights, mask)
451 # 应用样本权重
452 如果权重不为 None:
--&gt; 453 score_array *= weights
454 score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
455 返回 K.mean(score_array)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
827 if not isinstance(y, sparse_tensor.SparseTensor):
828 try:
--&gt; 829 y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=&quot;y&quot;)
830 except TypeError:
831 # 如果 RHS 不是张量，则可能是张量感知对象

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
674 name=name,
675 preferred_dtype=preferred_dtype,
-&gt; 676 as_ref=False)
677 
678 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py 在 internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
739 
740 如果 ret 为 None:
--&gt; 741 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
742 
743 如果 ret 未实现：

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
612 引发 ValueError(
613 &quot;Tensor 转换请求 dtype %s 的 Tensor 具有 dtype %s：%r&quot;
--&gt; 614 % (dtype.name, t.dtype.name, str(t)))
615 返回 t
616 

ValueError：Tensor 转换请求 dtype float64 的 Tensor 具有 dtype float32： &#39;Tensor(&quot;processed_a_b_sample_weights_1:0&quot;, shape=(?,), dtype=float32)&#39;

以下是重现错误的 MWE：
import tensorflow as tf
from keras import backend as K
from keras.layers import Input, Dense, merge, Dropout
from keras.models import Model, Sequential
from keras.optimizers import RMSprop
import numpy as np

# 定义输入
input_dim = 10
input_a = Input(shape=(input_dim,), name=&#39;input_a&#39;)
input_b = Input(shape=(input_dim,), name=&#39;input_b&#39;)
# 定义 base_network
n_class = 4
base_network = Sequential(name=&#39;base_network&#39;)
base_network.add(Dense(8, input_shape=(input_dim,),activation=&#39;relu&#39;))
base_network.add(Dropout(0.1))
base_network.add(Dense(n_class,activation=&#39;relu&#39;))
processed_a = base_network(input_a)
processed_b = base_network(input_b)
# 合并左右部分
processed_a_b = merge([processed_a,processed_b],mode=&#39;concat&#39;,concat_axis=1,name=&#39;processed_a_b&#39;)
# 创建模型
model = Model(inputs=[input_a,input_b],outputs=processed_a_b)

# 自定义损失函数
def categorical_crossentropy_loss(y_true,y_pred):
# 拆分（取消合并）y_true 和 y_pred分成两部分
y_true_a, y_true_b = tf.split(value=y_true, num_or_size_splits=2, axis=1)
y_pred_a, y_pred_b = tf.split(value=y_pred, num_or_size_splits=2, axis=1)
loss = K.categorical_crossentropy(output=y_pred_a, target=y_true_a) + K.categorical_crossentropy(output=y_pred_b, target=y_true_b) 
return K.mean(loss)

# 编译模型
model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())
]]></description>
      <guid>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</guid>
      <pubDate>Fri, 14 Jul 2017 20:28:09 GMT</pubDate>
    </item>
    </channel>
</rss>