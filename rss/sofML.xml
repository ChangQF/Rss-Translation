<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 07 May 2024 09:16:45 GMT</lastBuildDate>
    <item>
      <title>Tensorflow：INVALID_ARGUMENT：logits 和标签必须具有相同的第一维，得到 logits 形状 [32,2] 和标签形状 [64]</title>
      <link>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</link>
      <description><![CDATA[我正在使用张量流来测试数据集的准确性。 您可以找到我的完整代码以及数据集 此处
我还在学习机器学习，有很多事情让我头疼。例如，为什么在这种情况下，当我使用“sparse_categorical_crossentropy”编译模型时，
# 由于标签不是单热编码的，我们使用稀疏分类交叉熵损失
model.compile(loss=“sparse_categorical_crossentropy”,
              优化器=“sgd”，
              指标=[“准确度”])

有一个错误：
INVALID_ARGUMENT：logits 和标签必须具有相同的第一个维度，得到 logits 形状 [32,2] 和标签形状 [64]
但是，在花了几个小时查看代码并无可救药地修复每一行之后。终于成功了！通过改变loss=“categorical_crossentropy”
# 由于标签不是单热编码的，我们使用稀疏分类交叉熵损失
model.compile(loss=“categorical_crossentropy”,
              优化器=“sgd”，
              指标=[“准确度”])

我现在真的需要一个解释，我将非常感激！]]></description>
      <guid>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</guid>
      <pubDate>Tue, 07 May 2024 09:02:32 GMT</pubDate>
    </item>
    <item>
      <title>安装在谷歌驱动器上的图像数据集上的 DataGradient 如何使用它作为分类图像</title>
      <link>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</link>
      <description><![CDATA[我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
SS]]></description>
      <guid>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</guid>
      <pubDate>Tue, 07 May 2024 08:47:58 GMT</pubDate>
    </item>
    <item>
      <title>如何通过分析声音数据使用深度学习自动对网络音频设备（扬声器设备）进行分组？</title>
      <link>https://stackoverflow.com/questions/78441187/how-to-group-network-audio-devices-speaker-devices-automatically-using-deep-le</link>
      <description><![CDATA[扬声器自动分组意味着对属于同一房间的设备进行分组（返回唯一的扬声器 ID 集列表）。
场景是

建筑物各楼层（室内环境）的不同房间中已安装很少的设备。
这些设备仅使用以太网线供电，没有任何类型的 WiFi 或蓝牙信号。
这些设备可以播放声音，也可以使用内置麦克风进行录音，这些操作可以通过 ssh（在终端中使用扬声器的 id，如 1.1.1.1）或来自扬声器网站的某些 API 来实现控制（某些网站由该扬声器公司提供）。
我们还可以修改声音播放的音量，还可以获取输入（麦克风）的值和声音的采样率、单声道或立体声等通道。
检查下图，我所说的自动组的意思是输出应该类似于
组 1 为 x,y，组 2 为 z，组 3 为 p,q,r，组 4 为 a,b,c,d。


最初，该方法涉及收集一些数据样本并通过提取 MFCC 或梅尔频谱图等声音特征来分析它们。如下获得数据样本，通过运行Python脚本经由终端访问所有扬声器，然后一个扬声器播放声音，例如配置信号或“叮咚”声。而所有其他扬声器（包括正在播放的扬声器）都记录了该声音。
我尝试通过开发基本的暹罗网络来解决这种情况，但不幸的是，我无法实现有效或准确的输出。因此，我正在寻求有关使用深度学习分析声音数据在现实生活中实现此场景的替代方法的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78441187/how-to-group-network-audio-devices-speaker-devices-automatically-using-deep-le</guid>
      <pubDate>Tue, 07 May 2024 08:42:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Huggingface BERT 模型中使用“encoder_hidden_​​states”参数？</title>
      <link>https://stackoverflow.com/questions/78441081/how-to-use-the-encoder-hidden-states-parameter-in-huggingface-bert-model</link>
      <description><![CDATA[我想学习当“is_decoder”为 True 并且“add_cross_attention”为 True 时如何使用 bert。
但它不起作用。
这是我的代码：
tokenizer = BertTokenizer.from_pretrained(&#39;google-bert/bert-base-uncased&#39;)

bertconfig = BertConfig.from_pretrained(&#39;google-bert/bert-base-uncased&#39;, is_decoder=True, add_cross_attention=True)

伯特 = BertLMHeadModel(bertconfig).to(&#39;cuda:0&#39;)

input = tokenizer(&#39;你好，我的狗很可爱。&#39;, return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

cross_tensor = bert.bert.embeddings.forward(inputs[&#39;input_ids&#39;])

输出= bert（**输入，encoder_hidden_​​states = cross_tensor，encoder_attention_mask = torch.ones_like（cross_tensor）.to（&#39;cuda：0&#39;））

错误信息如下：
文件 d:\anaconda3\envs\torch_py38\lib\site-packages\transformers\models\bert\modeling_bert.py:352，在 BertSelfAttention.forward(self,hidden_​​states,attention_mask,head_mask,encoder_hidden_​​states,encoder_attention_mask ，过去的键值，输出注意）
    第349章
    [第 350 章]
    [第 351 章]
--&gt;第352章
    攀上漂亮女局长之后354
    第355章

运行时错误：张量 a (9) 的大小必须与非单一维度 3 处的张量 b (768) 的大小匹配

我尝试更改“encoder_hidden_​​states”张量的形状，但它不起作用。
谁能给我解决这个问题的解决方案，或者在这种情况下使用 bert 的示例。]]></description>
      <guid>https://stackoverflow.com/questions/78441081/how-to-use-the-encoder-hidden-states-parameter-in-huggingface-bert-model</guid>
      <pubDate>Tue, 07 May 2024 08:22:28 GMT</pubDate>
    </item>
    <item>
      <title>Python 推理管道的加速</title>
      <link>https://stackoverflow.com/questions/78440864/speed-up-of-python-inference-pipeline</link>
      <description><![CDATA[我正在尝试构建一个支持 Huggingface 的分布式 LLM 推理平台。该实现涉及利用 Python 进行模型处理和 Java 与外部系统连接。下面是负责从 Java 程序接收输入文本、通过预先训练的 LLM 对其进行处理并返回处理后的文本的 Python 代码：
导入套接字，sys
导入线程
从变压器进口管道

生成器=管道（&#39;文本生成&#39;，模型=&#39;gpt2-large&#39;，设备=“cuda”）


def process_input(input_text):
    请求 = 生成器(input_text, min_length=200)
    返回请求[0][“生成的文本”]


def 句柄_连接（conn）：
    与康恩：
        数据 = conn.recv(10240).decode()
        已处理的数据 = process_input(data.strip())
        conn.sendall(processed_data.encode())


端口 = int(sys.argv[1])

与socket.socket（socket.AF_INET，socket.SOCK_STREAM）作为s：
    s.bind((&#39;localhost&#39;, PORT))
    s.listen()
    而真实：
        conn, addr = s.accept()
        线程 = threading.Thread(target=handle_connection, args=(conn,))
        线程.start()

正如你所看到的，它接受来自Java进程的套接字连接，接收文本，返回请求结果，并关闭连接，所有这些都在线程的执行中进行。所有线程之间有一个共享管道，由于占用内存空间太大，因此只创建了1次。
在 Java 方面，我有一个类 LLMProcess，它处理 Python 进程的创建和通信，在每个请求生命周期中使用线程。
LLMProcess 进程 = new LLMProcess();

for (int i = 0; i &lt; 50; i++) {
    int 索引 = i;
    线程线程 = new Thread(() -&gt; {
        System.out.println(&quot;&quot; + index + &quot; : &quot; + process.request(&quot;示例文本&quot;);
        System.out.flush();
    });
    线程.start();
}

但是，当尝试同时执行大量请求时，系统会在处理请求时表现出顺序行为，并表现出与线程使用相关的开销，而不是通过 LLM 管道和 GPU 加速有效利用并发处理。
目标是通过最小化线程使用开销并充分利用可用的 GPU 资源来优化此过程。尽管存在 GPU 支持，但其在程序执行期间的负载仍然很小，通常不超过 3%。]]></description>
      <guid>https://stackoverflow.com/questions/78440864/speed-up-of-python-inference-pipeline</guid>
      <pubDate>Tue, 07 May 2024 07:38:05 GMT</pubDate>
    </item>
    <item>
      <title>“管道”对象没有属性“_check_fit_params”</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[来自 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler
从 imblearn.pipeline 导入管道

# 定义特征和目标
X = df.drop(&#39;感染&#39;, axis=1)
y = df[&#39;感染&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样到多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 将多数类欠采样至其原始大小的 80%

管道 = 管道(步骤=[(&#39;o&#39;, 上), (&#39;u&#39;, 下)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(“重采样的类分布：”, pd.Series(y_resampled).value_counts())

这是我的代码
并且
这是我遇到的错误
AttributeError Traceback（最近一次调用最后一次）
单元格 In[7]，第 19 行
     16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
     18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
     21 # 显示新的班级分布
     22 print(&quot;重采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372，在 Pipeline.fit_resample(self, X, y, **fit_params)
    第342章
    第343章
    第344章 一个接一个地安装所有变压器/采样器并且
   （...）
    第369章 变形的目标。
    第370章
    第371章
--&gt;第372章
    第373章
    第374章

AttributeError：“管道”对象没有属性“_check_fit_params”

我已经尝试了一切。我的所有包都已更新。以及我尝试使用的所有方法，查看 SKLearn 和 IBLEARN 这两个网站。请有人帮忙..紧急]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>DeepAR LSTM 使用动态特征进行预测？</title>
      <link>https://stackoverflow.com/questions/78440282/deepar-lstm-using-dynamic-features-to-forecast</link>
      <description><![CDATA[我正在使用 DeepAR 的 gluonts.torch 实现来使用搜索量作为因变量/输入/动态变量来预测未来的销售。
我的数据如下所示：
日期销售搜索
2018年12月30日 205 13
2019-01-06 245 19
2019-01-13 207 20
2019-01-20 221 21
2019-01-27 179 17
…………
2023年11月26日 142 11
2023年12月3日 183 13
2023年12月10日 211 14
2023年12月17日 236 14
2023年12月24日 275 15

我做了以下事情：
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np

从 gluonts.dataset.pandas 导入 PandasDataset
从 gluonts.torch 导入 DeepAREstimator
从 gluonts.evaluation 导入 make_evaluation_predictions，评估器

#读入数据
sales = pd.read_csv(&#39;data/sales.csv&#39;,index_col=0, parse_dates=True)

#分割训练和测试
预测长度=52
to_pandasdataset = lambda 数据: PandasDataset(
    数据，目标=“销售”，
    feat_dynamic_real=[“搜索”]
）
train = to_pandasdataset(sales.iloc[:-prediction_length,:])
测试= to_pandasdataset（销售）

#训练和预测
estimator = DeepAREstimator(freq=“1W”, Prediction_length=prediction_length, num_layers=4, num_feat_dynamic_real=1[![在此处输入图像描述][1]][1],hidden_​​size=14,trainer_kwargs={&#39;accelerator&#39;: &#39; cpu&#39;, &#39;max_epochs&#39;:10})

预测器 = estimator.train(train,num_workers=8)
Forecast_it，ts_it = make_evaluation_predictions（数据集=测试，预测器=预测器）
预测=列表（forecast_it）
测试=列表（ts_it）
评估器 = 评估器(分位数=(np.arange(20) / 20.0)[1:])
agg_metrics, item_metrics = evaluator(测试, 预测, num_series=len(测试))

＃阴谋

图, ax = plt.subplots(1, 1, Figsize=(12, 6))
#ax.plot(测试[0][-10 *预测长度:].to_timestamp())
ax.plot(测试[0].to_timestamp())
plt.sca(ax)
预测[0].plot(间隔=(0.9,))

# 在训练期结束时添加一条垂直线

最后训练日期 = sales.index[-预测长度]
plt.axvline(x=last_training_date, color=&#39;red&#39;, linestyle=&#39;--&#39;)

plt.legend([“观测值”,“预测中值”,“90%预测区间”,“训练结束”])
plt.xlabel(“日期”)
plt.ylabel(“”)
plt.show()

预测看起来很糟糕：

我认为它不好的原因是销售和搜索之间的相关性非常高：

我在训练时是否遗漏了某些内容，或者模型在测试/预测时未采用动态特征值？怎么表现这么差？有了这种高相关性，简单的线性回归很容易胜过 DeepAR，对吗？]]></description>
      <guid>https://stackoverflow.com/questions/78440282/deepar-lstm-using-dynamic-features-to-forecast</guid>
      <pubDate>Tue, 07 May 2024 05:24:22 GMT</pubDate>
    </item>
    <item>
      <title>如何标记多列、多类别数据集并将其保存到 CSV 或 Parquet 并使用 SVM 对其进行训练</title>
      <link>https://stackoverflow.com/questions/78440187/how-to-label-a-multicolumn-multi-category-dataset-and-save-it-to-a-csv-or-parqu</link>
      <description><![CDATA[我正在使用 OPENSMILE 库进行音频分类。预处理音频数据后，我得到一个 800x25 形状的数据，该数据仅适用于一个文件（每个文件大约 15 秒长）
为了训练这个数据集和为了可移植性，我想转换为 CSV 文件，我总共有 5 个文件夹（data/category_0 ... data/category_4），每个类别在处理时都有大约 50 个文件（.wav）使用 Opensmile 处理后的文件 data/category_0/audio1.wav 之一，我得到了形状为 (800x25) 的 pd 数据框，当数据如下时，我应该如何训练多类分类
5个文件夹
每个文件夹 20 个文件
每个文件的输出形状为 (800x25)
导入操作系统
导入时间

将 numpy 导入为 np
将 pandas 导入为 pd

导入音频文件
导入opensmile

微笑= opensmile.Smile(
    feature_set=opensmile.FeatureSet.eGeMAPSv02,
    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,
    多处理=真，
    详细=真
）
k = smile.process_signal(
    信号，
    采样率
）


我们可以看到有多个子列开始和结束，我如何将此数据集正确存储到 csv 文件中，这是不可能的，我至少如何训练它们
我尝试重塑尺寸，但仍然使用并且它弄乱了数据集的形状]]></description>
      <guid>https://stackoverflow.com/questions/78440187/how-to-label-a-multicolumn-multi-category-dataset-and-save-it-to-a-csv-or-parqu</guid>
      <pubDate>Tue, 07 May 2024 04:53:37 GMT</pubDate>
    </item>
    <item>
      <title>Pandas 代码在 datacamp 实践中是错误的 [关闭]</title>
      <link>https://stackoverflow.com/questions/78439826/pandas-code-is-wrong-in-datacamp-practical</link>
      <description><![CDATA[
实践考试：房屋销售
Real Agents 是一家专注于销售房屋的房地产公司。
Real Agents 在一个大都市地区销售各种类型的房屋。
有些房屋销售缓慢，有时需要降低价格才能找到买家。
为了保持竞争力，Real Agents 希望优化其试图出售的房屋的挂牌价格。
他们希望通过根据房屋特征预测其售价来做到这一点。
如果他们可以提前预测售价，他们就可以减少销售时间。
数据
数据集包含该地区以前出售的房屋的记录。



列
名称
条件




house_id
名义值。
房屋的唯一标识符。不可能缺少值。


city
名义值。
房屋所在的城市。“Silvertown”、“Riverford”、“Teasdale”和“Poppleton”之一。将缺失值替换为“未知”。


sale_price
离散值。
房屋的售价（以美元为单位）。值可以是大于或等于零的任何正数。删除缺失的条目。


sale_date
离散。
房屋最后一次出售的日期。用 2023-01-01 替换缺失值。


months_listed
连续。
房屋在最后一次出售前上市的月份数，四舍五入到小数点后一位。用平均上市月份数替换缺失值，精确到小数点后一位。


卧室
离散。
房屋中的卧室数量。任何大于或等于零的正值。将缺失值替换为平均卧室数量，四舍五入为最接近的整数。


house_type
有序。
“Terraced”（两面共用墙）、“Semi-detached”（一面共用墙）或“Detached”（无共用墙）之一。将缺失值替换为最常见的房屋类型。


area
连续。
房屋面积（平方米），四舍五入为小数点后一位。用平均值（精确到小数点后一位）替换缺失值。



任务 1
Real Agents 的团队知道房产所在的城市对销售价格有影响。
不幸的是，他们认为这并不总是记录在数据中。
计算城市的缺失值数量。
您应该使用文件“house_sales.csv”中的数据。
您的输出应该是一个 missing_city 对象，其中包含此列中缺失值的数量。
应该已经创建了所有必需的数据，并具有所需的列，并识别和替换缺失的值。

import pandas as pd

# 从 CSV 文件加载数据集
data = pd.read_csv(&quot;house_sales.csv&quot;)

# 检查“city”列中是否存在缺失值
missing_city = data[&quot;city&quot;].isnull().sum()

# 将“city”列中缺失的值替换为“Unknown”
data[&quot;city&quot;].fillna(&quot;Unknown&quot;, inplace=True) 

发现此代码有错误？]]></description>
      <guid>https://stackoverflow.com/questions/78439826/pandas-code-is-wrong-in-datacamp-practical</guid>
      <pubDate>Tue, 07 May 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>将任意深度转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</link>
      <description><![CDATA[我正在尝试将现有的 深度-anything PyTorch 模型转换为 CoreML 格式。我决定使用 Google Colab 并采取了以下内容： 推理深度任意模型的注释。但是，我在尝试将其导入 iOS 端时遇到了一些异常。这是我的转换代码片段：
# 安装所有需要的扩展
!pip 安装 coremltools
# ...

将 coremltools 导入为 ct
进口火炬

# 将 PyTorch 模型转换为 TorchScript
追踪模型 = torch.jit.trace(深度_任何东西, torch.rand(1, 3, 518, 518))

# 将 TorchScript 模型转换为 CoreML
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”，形状=(1,3,518,518)，比例=1/255.0)]
）

输出 = model_coreml._spec.description.output[0]
输出.type.imageType.colorSpace = ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
输出.类型.图像类型.宽度 = 518
输出.类型.图像类型.高度 = 518

# 保存修改后的CoreML模型
打印（model_coreml）
model_coreml.save(&#39;/content/drive/MyDrive/trained_models/深度9.mlpackage&#39;)

我尝试直接指定输入参数，就像我为输出参数所做的那样：
# 为输入模式创建字典
input_schema = {&#39;input_name&#39;: &#39;输入&#39;, &#39;input_type&#39;: ct.TensorType(shape=(1, 3, 518, 518))}

# 将输入模式添加到模型的元数据中
model_coreml.user_define_metadata[&#39;inputSchema&#39;] = str(input_schema)

或者使用 convert_to 选项设置 neuralnetwork，如下所示：
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”,形状=(1,3,518,518),比例=1/255.0)],
    Convert_to=&#39;神经网络&#39;
）

或者使用BGR/GRAYSCALE设置ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
没有任何帮助。
如果我尝试使用neuralnetwork后端导入模型，我只会收到无限加载。如果我尝试使用 mlprogram 后端导入模型（默认，如果未指定），我会收到以下信息：

我期待任何建议和帮助，因为我需要的只是转换现有的 深度任意 模型，无需对 CoreML 格式进行调整或更改。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</guid>
      <pubDate>Tue, 07 May 2024 01:53:36 GMT</pubDate>
    </item>
    <item>
      <title>如何以编程方式设置 W&B 运行失败警报？</title>
      <link>https://stackoverflow.com/questions/78439701/how-to-programmatically-set-alerts-for-failure-in-a-wb-run</link>
      <description><![CDATA[如何以编程方式设置 W&amp;B 运行失败警报？
我正在尝试在 W&amp;B（权重和偏差）项目中设置警报，以便在运行失败时通知我。我一直在测试一些我认为根据我的研究可以工作的功能，但似乎没有一个在 W&amp;B API 中实现。这是我尝试设置这些通知的代码片段：
导入 wandb

模式 = &#39;空运行&#39;
运行名称 = &#39;我的运行&#39;
批次数量 = 50
路径=&#39;/数据&#39;
名称 = &#39;实验1&#39;
今天 = &#39;2023-08-01&#39;
概率 = [0.1, 0.9]
批量大小 = 32
data_mixture_name = &#39;mix1&#39;

调试 = 模式 == &#39;dryrun&#39;
run = wandb.init(mode=mode,project=“超出规模”,name=run_name,save_code=True)
wandb.config.更新（{
    “num_batches”：num_batches，
    “路径”：路径，
    “姓名”：姓名，
    “今天”：今天，
    “概率”：概率，
    &#39;batch_size&#39;：batch_size，
    “调试”：调试，
    &#39;data_mixture_name&#39;：data_mixture_name
})

# 尝试设置通知
run.notify_on_failure()
run.notify_on_crash()
run.notify_on_exit()
run.notify_on_heartbeat()
run.notify_on_abort()

每次尝试都会导致 AttributeError，表明“Run”对象没有此类属性。例如：
AttributeError：“Run”对象没有属性“notify_on_failure”

在 W&amp;B 中是否有正确的方法来设置故障或其他警报？如果是这样，我应该如何修改我的方法？
参考：https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891]]></description>
      <guid>https://stackoverflow.com/questions/78439701/how-to-programmatically-set-alerts-for-failure-in-a-wb-run</guid>
      <pubDate>Tue, 07 May 2024 01:17:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Google Colab Python 代码构建浏览器扩展？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</link>
      <description><![CDATA[嗨，亲爱的 StackOverflow，
我目前正在考虑使用 Google Colab 中的代码创建一个浏览器扩展。里面有 Python 脚本。
我知道，目前无法使用 Python 创建扩展。但还有其他选择吗？
Python 脚本本身大约有 100 行，带有训练模型。我认为我不能只使用 JavaScript 来做到这一点。
谢谢；]]></description>
      <guid>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</guid>
      <pubDate>Mon, 06 May 2024 09:19:41 GMT</pubDate>
    </item>
    <item>
      <title>如何根据掩蔽将矩阵相乘并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>通过定义和 ROC 方法在 Python 中计算准确率（基尼系数）</title>
      <link>https://stackoverflow.com/questions/73031395/accuracy-ratio-gini-coef-computation-in-python-by-definition-and-roc-method</link>
      <description><![CDATA[为什么以下计算准确率的方法会给出不同的结果？
方法 1：累积精度曲线 (CAP) 曲线
准确率的计算公式为：训练模型的 CAP 曲线下面积与随机模型的 CAP 曲线下面积之差，除以完美模型的 CAP 曲线下面积与随机模型的 CAP 曲线下面积之差。随机模型。
方法 2：接受者操作特征 (ROC) 曲线。
我们计算 ROC 曲线下的面积，并使用统计量
AR = 基尼 = 2*（ROC 曲线下面积）- 1
有关统计数据的推导，请参阅论文“衡量评级系统的判别力” (https:/ /www.bundesbank.de/resource/blob/704150/b9fa10a16dfff3c98842581253f6d141/mL/2003-10-01-dkp-01-data.pdf)
有关我正在使用的代码的来源和更多示例，请参阅文章“使用 ROC 进行机器学习分类器评估” (https://towardsdatascience.com/machine -学习分类器评估使用-roc-and-cap-curves-7db60fe6b716)
接下来，我们有一个实际测试标签的向量y_test，以及输出的正类（label=1）的预测概率的向量test_pred_probs来自一些预测模型。我们计算该预测的准确率。
from sklearn.metrics import roc_curve, auc, roc_auc_score
将 numpy 导入为 np
y_测试 = [0,1,1,0,1,1,1,0,0,1]
test_pred_probs = [0.2,0.4,0.1,0,0,1,0.9,0.3,0.2,0.8]
总计 = len(y_test)
one_count = np.sum(y_test)
零计数 = 总数 - 一个计数
lm = [y 为 _,y 排序(zip(test_pred_probs,y_test),reverse=True)]
x = np.arange(0,总计+1)
y = np.append([0],np.cumsum(lm))
a = auc([0,总计],[0,one_count])
aP = auc([0,one_count,总计],[0,one_count,one_count]) - a
aR = auc(x,y) - a
print(&quot;加速比：&quot;,aR/aP) #返回0.5
print(&quot;ROC 曲线的 Acc 比率：&quot;,2*roc_auc_score(y_test,test_pred_probs)-1) #returns 0.458

在某些情况下，这两种方法会给出截然不同的结果 - 前者给出的 AR 约为 0.7，后者使用 ROC 方法给出的 AR 为 0.12。]]></description>
      <guid>https://stackoverflow.com/questions/73031395/accuracy-ratio-gini-coef-computation-in-python-by-definition-and-roc-method</guid>
      <pubDate>Tue, 19 Jul 2022 05:09:47 GMT</pubDate>
    </item>
    <item>
      <title>一次性将 pandas 数据帧随机分为几组，以进行 x 倍交叉验证</title>
      <link>https://stackoverflow.com/questions/52574923/randomly-divide-a-pandas-dataframe-into-groups-at-once-for-x-fold-crossvalidatio</link>
      <description><![CDATA[假设我有一个包含 500 行的数据框。我想执行 10 倍交叉验证。因此，我需要将这些数据分为 10 组，每组包含 50 行。我也想将整个数据随机分为 10 组。 
有没有办法使用 pandas、numpy 等库来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/52574923/randomly-divide-a-pandas-dataframe-into-groups-at-once-for-x-fold-crossvalidatio</guid>
      <pubDate>Sun, 30 Sep 2018 05:27:12 GMT</pubDate>
    </item>
    </channel>
</rss>