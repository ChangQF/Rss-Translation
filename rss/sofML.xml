<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 21 Dec 2023 00:59:19 GMT</lastBuildDate>
    <item>
      <title>在二元分类 ML 任务中处理日期列的最佳方法？</title>
      <link>https://stackoverflow.com/questions/77695074/best-approaches-for-handling-date-columns-in-a-binary-classification-ml-task</link>
      <description><![CDATA[我有一个数据集，其中有多个数字、分类和日期列作为特征，以及一个只能有 2 个值的目标（是/否）。目标表示某个日期列的值是否发生事件。
据我所知，可以从日期中提取一些特征：

日（可以循环编码为 sin 和 cos 分量）
一周中的某一天（可以进行 one-hot 编码）
月份（可以循环编码为 sin 和 cos 分量）
年份（可以跳过，因为没有太多预测能力）
周末（是/否）
假期（当年特定国家/地区 - 是/否）
周数（可以循环编码为 sin 和 cos 分量）

是否有任何其他方法或功能可用于此类任务的日期列？另外，上述理解/方法是正确的还是我做错了什么？
此外，是否需要在标准化/归一化过程中包含正弦或余弦变换分量？]]></description>
      <guid>https://stackoverflow.com/questions/77695074/best-approaches-for-handling-date-columns-in-a-binary-classification-ml-task</guid>
      <pubDate>Thu, 21 Dec 2023 00:00:14 GMT</pubDate>
    </item>
    <item>
      <title>“4,240,240,160] 并在 /job:localhost/replica:0/task:0/device:GPU:0 上通过分配器 GPU_0_bfc 键入 float”，同时训练深度学习模型</title>
      <link>https://stackoverflow.com/questions/77694995/4-240-240-160-and-type-float-on-joblocalhost-replica0-task0-devicegpu0-b</link>
      <description><![CDATA[我正在使用 3D-Unet 架构、51GB RAM 和 16GB RAM 作为 GPU 的 Nvdia V100 对模型进行编程。这些是在 google colab 环境下的。
构成数据集的图像是nifti格式的MRI图像，大小为(1, 240, 240, 160, 1)。它们是灰度的，我使用 1 作为批量大小来查看它是否解决了问题（它没有）
这是我的代码：
def load_nifti_image(文件路径):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()
    返回量

＃  -  -  -  -  -  -  -  -  -  -  - -火车 -  -  -  -  -  -  -  -  -  -  - -
nifti_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/images”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/images”) if f.endswith(&#39;.nii.gz&#39;)]
mask_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/masks”) if f.endswith(&#39;.nii.gz&#39;)]

nifti_images = [load_nifti_image(f) for f in nifti_files]
nifti_masks = [load_nifti_image(f) for f in mask_files]

Final_nifti_images = [np.expand_dims(image, axis=-1) 对于 nifti_images 中的图像]
Final_nifti_masks = [np.expand_dims(image, axis=-1) 对于 nifti_masks 中的图像]

数据集 = tf.data.Dataset.from_tensor_slices((final_nifti_images, Final_nifti_masks))

＃  -  -  -  -  -  -  -  -  -  -  - -验证 -  -  -  -  -  -  -  -  -  -  - -
nifti_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;) if f.endswith(&#39;.nii.gz&#39;)]
mask_files_val = [os.path.join(“/content/drive/MyDrive/Interpolated/validation/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/validation/masks”) if f.endswith(&#39;.nii.gz&#39;)]

nifti_images_val = [load_nifti_image(f) for f in nifti_files_val]
nifti_masks_val = [load_nifti_image(f) for f in mask_files_val]

Final_nifti_images_val = [np.expand_dims(image, axis=-1) 对于 nifti_images_val 中的图像]
Final_nifti_masks_val = [np.expand_dims(image, axis=-1) for image in nifti_masks_val]

dataset_val = tf.data.Dataset.from_tensor_slices((final_nifti_images_val, Final_nifti_masks_val))

数据集 = 数据集.batch(1)
dataset_val = dataset_val.batch(1)

test_model.fit（数据集，validation_data=dataset_val，epochs=100）

我尝试将 GPU 的 RAM 限制为 14GB，也不起作用，并且还减小了批处理大小。请注意，我无法更改模型，因为我使用的是预定义的 3D-Unet 模型，我也会将其粘贴到此处，也许它会有所帮助：
&lt;前&gt;&lt;代码&gt;
# 卷积块
def conv_block(输入, num_filters):
    x = Conv3D(num_filters, (3, 3, 3), padding = “相同”)(输入)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    x = Conv3D(num_filters, (3, 3, 3), 填充 = “相同”)(x)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    返回x

# 编码器块
def编码器_块（输入，num_filters）：
    x = conv_block(输入, num_filters)
    p = MaxPool3D((2, 2, 2), 填充=“相同”)(x)
    返回 x, p

# 解码器块
def解码器_块（输入，跳过，num_filters）：
    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=“相同”)(输入)
    x = 连接()([x, 跳过])
    x = conv_block(x, num_filters)
    返回x

# 大学网络

def unet(输入形状):
    输入 = 输入（输入形状）

    “----编码器----”
    s1, p1 = 编码器_块(输入, 64)
    s2, p2 = 编码器_块(p1, 128)
    s3, p3 = 编码器_块(p2, 256)
    s4, p4 = 编码器_块(p3, 512)

    “----桥---”
    b1 = conv_block(p4, 1024)

    “----解码器----”
    d1 = 解码器_块(b1, s4, 512)
    d2 = 解码器_块(d1, s3, 256)
    d3 = 解码器块(d2, s2, 128)
    d4 = 解码器块(d3, s1, 64)

    输出 = Conv3D(1, 1, 填充 =“相同”, 激活 =“sigmoid”)(d4)

    模型=模型（输入，输出，名称=“UNET”）
    返回模型

输入形状 = (240, 240, 160, 1)

测试模型=unet(输入形状)
优化器 = Adam(learning_rate=0.0001)
test_model.compile（优化器=优化器，损失=dice_coefficient_loss，指标=[dice_coefficient]）

]]></description>
      <guid>https://stackoverflow.com/questions/77694995/4-240-240-160-and-type-float-on-joblocalhost-replica0-task0-devicegpu0-b</guid>
      <pubDate>Wed, 20 Dec 2023 23:31:18 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法让多元线性回归（和其他机器学习模型）预测 GeoPandas 多边形？</title>
      <link>https://stackoverflow.com/questions/77694882/is-there-a-way-to-have-a-multiple-linear-regression-and-other-machine-learning</link>
      <description><![CDATA[我有一个包含多边形的地理数据库，这些多边形代表（大致）1900 年至 2013 年之间加利福尼亚州野火的蔓延情况。我正在尝试创建一个线性回归模型，该模型可以采用前一年记录的温度（浮动）并预测给定这些温度下火灾将蔓延到的形状的多多边形。地理数据库在我的代码中存储为 GeoPandas GeoDataFrame。
我无法找到任何允许您训练模型来预测多边形的库或包。
我尝试过 Pysal：
train_X = fire_X.iloc[train].values
train_y = fire_y.iloc[train].values
test_X = fire_X.iloc[测试].values
test_y = fire_y.iloc[测试].values

模型 = spreg.OLS(y=train_y, x=train_X)

其中 fire_X 具有温度数据，fire_y 只是地理数据库中的 geometry 列，这导致
类型错误：+ 不支持的操作数类型：“MultiPolygon”和“MultiPolygon”
sklearn 回归也不起作用。这可能吗？]]></description>
      <guid>https://stackoverflow.com/questions/77694882/is-there-a-way-to-have-a-multiple-linear-regression-and-other-machine-learning</guid>
      <pubDate>Wed, 20 Dec 2023 22:55:17 GMT</pubDate>
    </item>
    <item>
      <title>拟合模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/77694851/valueerror-while-fitting-a-model</link>
      <description><![CDATA[我在将线性回归模型拟合到具有两列的数据时遇到了问题 - 一列用于日期，一列用于数字列表。 ValueError 持续存在，说明用序列设置数组元素。
因此，代码有一个名为 numbers_data 的数据框，其中包含两列 - 第一列包含日期，第二列包含数字列表
原代码如下：
X = np.array(numbers_data[&#39;date&#39;]).reshape(-1, 1) # 特征：日期
y = np.array(numbers_data[&#39;numbers&#39;]) # 目标变量：整个数字列表

我尝试了以下方法：
X = np.array(numbers_data[&#39;date&#39;]).reshape(-1, 1) # 特征：日期
y = np.array(numbers_data[&#39;numbers&#39;]).reshape(-1, 1) # 目标变量：整个数字列表
]]></description>
      <guid>https://stackoverflow.com/questions/77694851/valueerror-while-fitting-a-model</guid>
      <pubDate>Wed, 20 Dec 2023 22:44:31 GMT</pubDate>
    </item>
    <item>
      <title>改进 VGG16 物体识别</title>
      <link>https://stackoverflow.com/questions/77694044/improve-vgg16-object-recognition</link>
      <description><![CDATA[https://colab.research.google.com/drive/1HKm7HEu2sk3OsiwEtnfrKcNgyQZtD6uk ?usp=共享
在此处输入图像描述
当我在顶部链接的Jupiter笔记本上实现时，我使用vgg16来检测上图中的对象，下图是结果。
在此处输入图像描述
但是如下图所示，我标记红框的观众部分没有检测到名为“person”的对象，
如何更改代码以将红色框的观众识别为“人”
或者由于 vgg16 是预训练模型，所以我想知道是否可以让它变得更好。
我不知道如何修复代码
在此处输入图像描述
https://colab.research.google.com/drive/1HKm7HEu2sk3OsiwEtnfrKcNgyQZtD6uk ?usp=共享]]></description>
      <guid>https://stackoverflow.com/questions/77694044/improve-vgg16-object-recognition</guid>
      <pubDate>Wed, 20 Dec 2023 19:38:53 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数据框架的问题</title>
      <link>https://stackoverflow.com/questions/77693773/problem-about-machine-learning-and-dataframe</link>
      <description><![CDATA[我发现训练数据集层存在问题：
ValueError：层“sequential_4”的输入 0与图层不兼容：预期形状=（无，7），发现形状=（无，5）

这是我的代码：
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 keras.models 导入顺序
从 keras.layers 导入密集、Dropout
从 keras.optimizers 导入 Adam
从 keras.regularizers 导入 l2
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np

def carica_dataset():
数据集 = pd.read_csv(“数据集.csv”)
返回数据集

def carica_modello():
    数据集 = carica_dataset()
    数据集 = pd.get_dummies(数据集, columns=[&#39;Località&#39;])
    打印（数据集）
    X = dataset.drop(列=[&#39;Prezzo&#39;])
    y = 数据集[&#39;Prezzo&#39;]

    X_train, X_test, y_train, y_test = train_test_split(X, y)

    模型=顺序（）

    model.add（密集（64，激活=&#39;relu&#39;，input_dim = X_train.shape [1]，kernel_regularizer = l2（0.1）））
    模型.add(Dropout(0.5))
    model.add（密集（32，激活=&#39;relu&#39;，kernel_regularizer=l2（0.1）））
    模型.add(Dropout(0.5))
    model.add（密集（16，激活=&#39;relu&#39;，kernel_regularizer=l2（0.1）））
    模型.add(Dropout(0.5))
    model.add(密集(8, 激活=&#39;relu&#39;, kernel_regularizer=l2(0.1)))
    模型.add(Dropout(0.5))
    model.add(密集(1,激活=&#39;线性&#39;,kernel_regularizer=l2(0.1)))
    亚当 = 亚当()

    model.compile(loss=&#39;mean_squared_error&#39;, 优化器=adam, 指标=[&#39;准确性&#39;])

    model.fit（X_train，y_train，epochs = 100，batch_size = 64）


    返回模型

 数据集 = carica_dataset()
 模型 = carica_modello()
 字段={
    &#39;Superficie&#39;：浮动，
    &#39;Numero di stanze da letto&#39;: int,
    &#39;Numero di bagni&#39;: int,
    &#39;Anno di costruzione&#39;: int,
    &#39;Località&#39;: str
     }
 用户数据 = {}

对于 fields.items() 中的键、值：
    而真实：
        尝试：
            user_input = input(f&quot;inserisci il valore di: {key}&quot;)
            用户数据[键] = 值(用户输入)
            休息
        除了值错误：
            print(f“inserisci un valore valido per {key}”)
dataframe = pd.DataFrame([用户数据])
dataframe = pd.get_dummies(dataframe, columns=[&#39;Località&#39;])

valori = dataframe.values

预测 = model.predict(valori)[0][0]
print(f&#39;La predizione del prezzo è: {预测} €&#39;)

我尝试更改层数，但每次都发现同样的问题，我该怎么办？
我的数据集有 6 列 -1，这是我需要预测的列，所以 5 列]]></description>
      <guid>https://stackoverflow.com/questions/77693773/problem-about-machine-learning-and-dataframe</guid>
      <pubDate>Wed, 20 Dec 2023 18:35:12 GMT</pubDate>
    </item>
    <item>
      <title>如何创建一个curl req来创建具有谷歌云存储管道规范的顶点管道？</title>
      <link>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</link>
      <description><![CDATA[我正在关注此文档：
我有以下curl请求（更改了一些命名以不公开信息）：
&lt;前&gt;&lt;代码&gt;curl -X POST \
    -H“授权：承载$(gcloud auth print-access-token)” \
    -H“内容类型：application/json；字符集=utf-8” \
    -d&#39;{
        &quot;display_name&quot;:&quot;训练&quot;,
        “最大并发运行计数”：10，
        “创建管道作业请求”：{
                “父”：“项目/项目名称/位置/us-west2”，
                “管道作业”：{
                    “显示名称”：“训练”，
                    “pipelineSpec”：“gs://bucket_name/artifacts/pipeline.yaml”，
                }
            }
    }&#39; \
    “https://us-west2-aiplatform.googleapis.com/v1/projects/project_name/locations/us-west2/schedules”

我收到错误：
&lt;前&gt;&lt;代码&gt;{
  “错误”：{
    “代码”：400，
    “message”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”，
    “状态”：“INVALID_ARGUMENT”，
    “详细信息”：[
      {
        “@type”：“type.googleapis.com/google.rpc.BadRequest”，
        “字段违规”：[
          {
            “字段”：“schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”，
            “description”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”
          }
        ]
      }
    ]
  }
}

看起来curl请求必须有一个以JSON格式指定的管道规范。是否可以在curl请求中指定GCS路径？
我的用例是使用 Cloud Scheduler 创建每周运行的管道，从 GCS 文件中提取。我不想使用内置的重复管道创建，因为如果管道规范发生更改，则需要再次创建它。]]></description>
      <guid>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</guid>
      <pubDate>Wed, 20 Dec 2023 16:04:46 GMT</pubDate>
    </item>
    <item>
      <title>典型相关分析[关闭]</title>
      <link>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</link>
      <description><![CDATA[我正在学习统计学课程，今天我的老师问了我一个问题，要求我提出规范相关分析的替代方案。她希望我开始寻找其他解决方案，但我什至不知道从哪里开始。
我尝试在互联网上查找不同的解决方案并滚动浏览 SciKit Learn 文档，但没有找到任何有用的东西。我希望有人在这方面比我有更多的知识。那么有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</guid>
      <pubDate>Wed, 20 Dec 2023 15:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Python，分层采样[关闭]</title>
      <link>https://stackoverflow.com/questions/77692098/python-stratified-sampling</link>
      <description><![CDATA[我正在使用规范建模框架。
我有 791 个受试者的样本。我有关于年龄、性别和站点（站点 1 或 2）的数据。我想使用 Python 提取大约 50% 的受试者子样本，涵盖整个年龄和性别范围。我的子样本应反映原始样本的位点 1：位点 2 比例。
我尝试使用sklearn train_test_split，但有些分层 2.]]></description>
      <guid>https://stackoverflow.com/questions/77692098/python-stratified-sampling</guid>
      <pubDate>Wed, 20 Dec 2023 13:54:53 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的动态量化量化后开始随机训练</title>
      <link>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</link>
      <description><![CDATA[当我运行以下动态量化代码时，它开始使用一些随机自然图像进行 100 个时期的训练，我不想再次进行训练。我有预训练的权重，我只是想量化我的预训练的权重以减少推理时间：
从 ultralytics 导入 YOLO
进口火炬
导入火炬.量化

模型=YOLO(&#39;pre_trained_weights.pt&#39;)

model.load_state_dict(torch.load(&#39;checkpoint.pth&#39;)) #不知道这一步是否必要

qmodel = torch.quantization.quantize_dynamic(模型, dtype = torch.quint8)

我尝试了上面的代码，我希望我只是想量化我的预训练权重以减少推理时间]]></description>
      <guid>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</guid>
      <pubDate>Wed, 20 Dec 2023 13:53:49 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - ValueError：至少需要一个数组或数据类型</title>
      <link>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</guid>
      <pubDate>Wed, 20 Dec 2023 11:18:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MALLET Java API 运行 DMR 主题模型？</title>
      <link>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</link>
      <description><![CDATA[我有两个数据集text.txt和另一个metadata.txt。我将数据格式化为每行一个文档。我想将 MALLET Java Api 用于 DMR 主题模型。但是，我无法这样做。我对 Java 和这种复杂的建模完全陌生。另外，我对主题建模没有任何先验知识。我在这里寻找专业知识。你能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</guid>
      <pubDate>Wed, 20 Dec 2023 07:05:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hubert 模型获得音频嵌入</title>
      <link>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</link>
      <description><![CDATA[示例代码
导入火炬
从变压器导入 Wav2Vec2Processor、HubertForCTC
从数据集导入load_dataset

处理器 = Wav2Vec2Processor.from_pretrained(“facebook/hubert-large-ls960-ft”)
模型 = HubertForCTC.from_pretrained(“facebook/hubert-large-ls960-ft”)
input_values = 处理器(&#39;来自音频文件的数组。, return_tensors=“pt”).input_values

之后如何获得嵌入？模型中没有最后的隐藏状态。
尝试了我提到的代码块。此外，尝试创建没有最后一层的模型，然后将其输入。我的另一个问题是，假设我有不同时间维度的剪辑，那么如何创建固定嵌入？沿着时间轴平均可以吗？或者需要不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</guid>
      <pubDate>Tue, 19 Dec 2023 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“tensorflow.keras.preprocessing”（未知位置）导入名称“image_dataset_from_directory”</title>
      <link>https://stackoverflow.com/questions/64690693/importerror-cannot-import-name-image-dataset-from-directory-from-tensorflow</link>
      <description><![CDATA[为什么我会遇到这个问题？我可以从 kera.preprocessing 导入图像模块。但无法导入 image_dataset_from_directory。 TF版本：1.14]]></description>
      <guid>https://stackoverflow.com/questions/64690693/importerror-cannot-import-name-image-dataset-from-directory-from-tensorflow</guid>
      <pubDate>Thu, 05 Nov 2020 03:11:22 GMT</pubDate>
    </item>
    </channel>
</rss>