<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 17 Mar 2024 21:13:40 GMT</lastBuildDate>
    <item>
      <title>如何计算每个图像，同时测试 yolo 在该图像中检测到的 cpu 利用率？</title>
      <link>https://stackoverflow.com/questions/78176760/how-to-calculate-for-each-image-while-testing-how-much-cpu-utilization-the-yolo</link>
      <description><![CDATA[我使用的是yolo5。我想估计 yolo 处理某个图像需要多少 CPU 利用率。我该怎么做？
我尝试使用 P-Sutil 来计算 CPU 利用率，但是我无法获得每个图像的 CPU 利用率
这是我的代码
# !pip install -U ultralytics
进口火炬
导入时间
导入 psutil
从 keras.datasets 导入 cifar10

# 加载YOLOv5模型
# model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, pretrained=True)

# 加载CIFAR-10数据集并选择10张图像
(_, _), (x_test, _) = cifar10.load_data()
x_test_subset = x_test[:10] # 从测试集中选择前 10 张图像

# 循环遍历图像
对于 i，枚举中的 img(x_test_subset)：
    # 测量 YOLOv5 推理之前的 CPU 时间
    start_cpu = psutil.cpu_times().user + psutil.cpu_times().system

    # 将图像转换为PIL格式并进行YOLOv5推理
    开始时间 = 时间.time()
    results = model(&quot;/content/yolo.jpg&quot;, size=640) # 可以根据需要调整大小
    结束时间 = time.time()

    # 测量 YOLOv5 推理后的 CPU 时间
    end_cpu = psutil.cpu_times().user + psutil.cpu_times().system

    # 计算 YOLOv5 推理的 CPU 利用率
    cpu_utilization_percentage = ((end_cpu - start_cpu) / psutil.cpu_count()) / (end_time - start_time) * 100
    
    # 计算检测到的对象数量
    num_objects = len(结果.xyxy[0])

    # 打印当前图像的结果
    print(&quot;图像 {}: 检测到的对象: {} - YOLO CPU 利用率 (%): {:.2f}&quot;.format(i+1, num_objects, cpu_utilization_percentage))
`
]]></description>
      <guid>https://stackoverflow.com/questions/78176760/how-to-calculate-for-each-image-while-testing-how-much-cpu-utilization-the-yolo</guid>
      <pubDate>Sun, 17 Mar 2024 20:07:08 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 Gradio Client API 使用图像进行预测</title>
      <link>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</link>
      <description><![CDATA[我正在尝试按照以下示例将图像发送到 Gradio Client API：
从“@gradio/client”导入{ client }；

const response_0 = 等待 fetch(“https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png”);
const exampleImage =等待response_0.blob();
                        
const app = 等待客户端(“airvit2/pet_classifier”);
const 结果 =等待 app.predict(“/预测”, [
                exampleImage, // &#39;img&#39; 图像组件中的 blob
    ]);

console.log(结果.数据);

但它返回此错误：
&lt;前&gt;&lt;代码&gt;{
    “类型”：“状态”，
    “端点”：“/预测”，
    “fn_index”：0，
    “时间”：“2024-03-17T18:36:53.270Z”，
    “队列”：正确，
    “消息”：空，
    “阶段”：“错误”，
    “成功”：假
}

这是我的 Gradio 代码：
from fastai.vision.all import *
将渐变导入为 gr

学习 = load_learner(&#39;model.pkl&#39;)

def 预测（img）：
    print(&quot;图片：&quot;, img)
    img = 加载图像(img)
    # img = PILImage.create(img)
    pred, pred_idx, probs = learn.predict(img)
    返回预测值

gr.Interface(fn = 预测，输入 = gr.Image(type=“pil”，高度 = 224，宽度 = 224)，输出 = gr.Label(num_top_classes = 3)).launch(share = True)


我尝试将图像格式更改为 Blob，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</guid>
      <pubDate>Sun, 17 Mar 2024 18:57:11 GMT</pubDate>
    </item>
    <item>
      <title>创建一个新模型，它将检查用户是否在网站上（是否更改选项卡）[关闭]</title>
      <link>https://stackoverflow.com/questions/78176381/creating-a-new-model-where-it-will-check-if-the-user-is-on-the-site-or-not-cha</link>
      <description><![CDATA[我想创建一个模型，如果用户在会话期间更改选项卡，它将通知管理员。它将向管理员发送有关此事的详细通知。但我不知道从哪里开始以及做什么。我只需要注意一下应该如何在 ML 应用程序/网络中实现它。
因为我是新手，所以我尝试搜索是否有任何类型的 Git 或教程。但没找到。]]></description>
      <guid>https://stackoverflow.com/questions/78176381/creating-a-new-model-where-it-will-check-if-the-user-is-on-the-site-or-not-cha</guid>
      <pubDate>Sun, 17 Mar 2024 18:08:21 GMT</pubDate>
    </item>
    <item>
      <title>开发者/用户/立法者对此有何感想？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78176362/how-do-developers-users-lawmakers-feel-about-this</link>
      <description><![CDATA[请回复您的答案（无论长度）以帮助我完成大学研究论文！选择您所属的组并做出相应的回答 - 谢谢！！

对于机器学习开发人员：
您在开发过程中遇到哪些道德考虑？
您在工作中如何优先考虑透明度、公平、隐私和问责制等道德原则？
在将伦理考虑融入机器学习算法时，您面临哪些技术挑战？
您在工作中是否遇到过算法偏见或歧视的情况？如果是这样，您是如何解决这些问题的？
对于机器学习技术的最终用户：
您在日常生活或工作中如何与机器学习技术互动？
您对机器学习算法的道德行为有何期望？
您在使用机器学习系统时是否遇到过与公平、透明度、隐私或问责制相关的担忧？
了解机器学习算法如何做出决策对您来说有多重要？
您认为哪些措施可以提高人们对机器学习技术的信任和信心？
对于政策制定者：
与机器学习算法的道德开发和部署相关的关键法律和监管考虑因素有哪些？
您如何看待政策制定者在确保机器学习技术的道德使用方面的作用？
您预计在实施道德机器学习法规或指南时会遇到哪些挑战？
政策制定者如何与行业利益相关者合作解决机器学习中的道德问题？
您认为要促进机器学习应用程序的透明度、公平性、隐私性和问责制，需要采取哪些步骤？

研究帮助，已发布在许多网站上！]]></description>
      <guid>https://stackoverflow.com/questions/78176362/how-do-developers-users-lawmakers-feel-about-this</guid>
      <pubDate>Sun, 17 Mar 2024 18:02:57 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 自定义模型不进行预测</title>
      <link>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</link>
      <description><![CDATA[我使用自定义训练的 Yolov8 模型来预测物理门是关闭还是打开。我已经在自定义数据集上训练了 Yolov8，但即使传递用于训练的相同数据，它也不会进行任何检测。
我使用了大约 300 张图像的数据集。
这是我的代码：
导入操作系统

从 ultralytics 导入 YOLO
导入CV2


VIDEOS_DIR = os.path.join(&#39;.&#39;, &#39;视频&#39;)

video_path = os.path.join(VIDEOS_DIR, &#39;样本门.mp4&#39;)
video_path_out = &#39;{}_out.mp4&#39;.format(video_path)

cap = cv2.VideoCapture(video_path)
ret, 框架 = cap.read()
H、W、_ = 框架.形状
out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))

model_path = os.path.join(&#39;.&#39;, &#39;运行&#39;, &#39;检测&#39;, &#39;训练&#39;, &#39;权重&#39;, &#39;last.pt&#39;)


model = YOLO(model_path) # 加载自定义模型


休息时：

    结果=模型（框架）[0]
    对于 results.boxes.data.tolist() 中的结果：
        x1, y1, x2, y2, 分数, class_id = 结果
        打印（x1，y1，x2，y2）

        cv2.矩形(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)
        cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)

    输出.write(帧)
    ret, 框架 = cap.read()

cap.release()
out.release()
cv2.destroyAllWindows()

以下是训练结果：https://i.stack.imgur。 com/huyZR.png]]></description>
      <guid>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:55 GMT</pubDate>
    </item>
    <item>
      <title>Mamba 架构的“hidden_​​states”形式</title>
      <link>https://stackoverflow.com/questions/78176169/form-of-hidden-states-for-mamba-architecture</link>
      <description><![CDATA[我正在使用 HuggingFace 实现来尝试最近的 Mamba 架构。它的形式为
MambaForCausalLM(
  （骨干）：MambaModel（
    （嵌入）：嵌入(50280, 768)
    （层）：模块列表（
      (0-23): 24 x 曼巴布块(
        （范数）：MambaRMSNorm()
        （混音器）：MambaMixer（
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          （动作）：SiLU()
          （in_proj）：线性（in_features = 768，out_features = 3072，偏差= False）
          （x_proj）：线性（in_features = 1536，out_features = 80，偏差= False）
          （dt_proj）：线性（in_features = 48，out_features = 1536，偏差= True）
          （out_proj）：线性（in_features = 1536，out_features = 768，偏差= False）
        ）
      ）
    ）
    (norm_f): MambaRMSNorm()
  ）
  （lm_head）：线性（in_features = 768，out_features = 50280，偏差= False）
）

当我在传入虚拟输入句子后获取模型outputs.hidden_​​state时，它的长度为25。对我来说，这意味着第一个条目是 model.backbone.embeddings 的输出，其余条目来自主干中的 24 层，没有任何来自  &gt;lm_head。所以我想改变 LM 头（例如，通过用随机矩阵替换它）应该对 outputs.hidden_​​state[-1] 没有影响，我想它是  的输出model.backbone.layers[-1]，尽管它显然会影响 outputs.logits。然而，当我这样做时，outputs.hidden_​​state[-1] 发生了巨大的变化。为什么是这样？当我添加一个钩子来手动跟踪每个 24 层的激活时，最终的激活不会随着 lm_head 的更改而改变，正如我所期望的那样。所以我猜 outputs.hidden_​​states 包含的内容与我的想法不同。这里发生了什么？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78176169/form-of-hidden-states-for-mamba-architecture</guid>
      <pubDate>Sun, 17 Mar 2024 17:06:34 GMT</pubDate>
    </item>
    <item>
      <title>如何从短信屏幕截图中提取格式正确的对话[关闭]</title>
      <link>https://stackoverflow.com/questions/78176164/how-do-i-extract-a-properly-formatted-conversation-from-a-text-message-screensho</link>
      <description><![CDATA[给定任何平台上短信对话的屏幕截图，如何提取正确排序和格式的对话？我的输出应该类似于：
你：嗨
他们：嗨
你：你好吗？
他们：好
他们：那你呢？

仅使用任何 OCR 库的主要问题是它会捕获屏幕上的无关文本，例如时间戳、单元格提供程序、通知等，而我无法可靠地区分这些文本和实际对话。我也不想使用 GPT-4V 或任何其他视觉法学硕士，因为它既慢又贵。为了快速、廉价且准确，最好的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78176164/how-do-i-extract-a-properly-formatted-conversation-from-a-text-message-screensho</guid>
      <pubDate>Sun, 17 Mar 2024 17:04:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别CPU。
该代码几周前就可以工作，有人有解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的无头 GPT2 模型在保存时抛出错误 - 如何添加输入和输出层以及自定义 PositionalEmbedding</title>
      <link>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</link>
      <description><![CDATA[我想使用 GPT2 对序列数据进行回归任务，因此尝试从 Hugging Face 中找出无头 TFGPT2，代码如下：
配置 = GPT2Config(n_embd = embed_dim, n_head=num_heads)
基础模型 = TFGPT2Model（配置）
输入形状 = (1, 嵌入尺寸)
input1 = 层.Input(shape=input_shape, dtype=tf.float32)
positional_encoding = PositionalEmbedding(sequence_length, embed_dim)
解码器输入=位置编码（输入1）
Z = base_model.call(inputs_embeds=decoder_inputs)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“relu”））（Z.last_hidden_​​state）
模型= keras.Model（输入1，输出）
model.compile(loss=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam(beta_1=0.9，beta_2=0.98，epsilon=1.0e-9)，metrics=[tf.keras.metrics.RootMeanSquaredError()] ）
历史= model.fit（数据集，validation_data = val_dataset，epochs = epoch_len，verbose = 1）
tf.keras. saving. save_model(模型, r&#39;/drive/model_huggingface&#39;)

另请注意，我还使用自定义 PositionalEmbedding 类，因此可选的 input_embeds 参数传递给模型。
该模型训练并学习数据，但在尝试保存时会抛出错误：
AssertionError：尝试导出引用“未跟踪”资源的函数。由函数捕获的 TensorFlow 对象（例如 tf.Variable）必须通过将其分配给被跟踪对象的属性或直接分配给主对象的属性来“跟踪”。请参阅以下信息：
    函数名称 = b&#39;__inference_signature_wrapper_452514&#39;
    捕获的张量 = 
    可追踪引用此张量 = ;
    内部张量 = Tensor(“452144:0”, shape=(), dtype=resource)

我认为这是因为我向该模型添加了头部和自定义层。请让我知道您对我对如何实现此模型的解释的看法。]]></description>
      <guid>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</guid>
      <pubDate>Sun, 17 Mar 2024 14:03:14 GMT</pubDate>
    </item>
    <item>
      <title>在 UNet 模型的 FluxTraining.jl 中将数据从 DataLoader 传递到 Learner 时出现问题</title>
      <link>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</link>
      <description><![CDATA[我正在尝试使用 FluxTraining.jl 训练 UNet 模型 u，但在将数据从 DataLoader 正确传递到 Learner 时遇到困难。
上下文：
我有两个数据集：一个用于名为“w”的输入图像，另一个用于名为“w”的输入图像。尺寸为 256x256x3x20（20 个观察值，3 个 RGB 通道），另一个用于地面实况比较，称为“wp”尺寸为 256x256x1x20（20 个观察值，1 个灰度通道）。
我使用 DataLoader 定义数据迭代器，如下所示：
trainiter = DataLoader((w, wp), 4)

然后，我尝试使用以下代码将数据传递给学习者：
学习者 = 学习者(
    你，
    损失，
    回调 = [
        指标（准确度），
        检查点（“trainingData/modelSaves/”），
        记录器后端
    ],
    优化器=选择
）

#一个纪元出现错误
纪元！（学习者，TrainingPhase（），培训师）

问题：
运行代码时，我遇到一个错误，表明损失函数（请参阅帖子底部）正在接收尺寸为 256x256x1x20 而不是预期的 256x256x3x20 的输入数据 x。数据似乎没有从 DataLoader 正确传递到 Learner。
如何正确地将数据从 DataLoader 传递到 FluxTraining.jl 中的 Learner？
在使用 FluxTraining 之前，我能够接受相关培训
Flux.train!(loss, Flux.params(u),rep, opt, cb = () -&gt; @show(loss(w, wp)))，其中 rep =迭代器.repeated((w, wp), 100)。在所有情况下，ADAM() 都是我的优化器（opt）。
作为参考，我的损失函数是：
函数损失(x, y)
    @显示尺寸(x)
    @显示尺寸(y)
    Flux.dice_coeff_loss(u(x), y)
结尾

我尝试了对代码的各种修改，例如将 DataLoader 语法更改为 DataLoader((w,w), 4) 或 DataLoader(w, 4) ，但我仍然面临以下问题：要么将单个 Float32 而不是数组传递到模型中，要么输入数据的维度仍然不正确。
我还尝试循环遍历训练器中的所有 xs 和 ys 并调用损失函数。在这种情况下，它工作得很好，所以我认为这与我使用纪元的方式不一样！功能。]]></description>
      <guid>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</guid>
      <pubDate>Sun, 17 Mar 2024 11:50:28 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归实现 - 损失不收敛且模型结果不佳</title>
      <link>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</guid>
      <pubDate>Sun, 17 Mar 2024 11:42:37 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有八个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数值特征为[1, 8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9]（I以两列 CSV 文件的形式提供该图所有点的坐标对 (x, y)，我也可以使用它们。）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>实时检测在YOLOv8中播放音频文件</title>
      <link>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</link>
      <description><![CDATA[我正在 YOLOv8 项目中工作，以检测困倦并在检测到困倦时播放警报音频文件。我面临的问题是我无法实时播放音频，因为我的检测首先存储在结果中。一旦我关闭检测窗口，它就会访问结果中存储的数据并连续播放音频。我该如何解决这个问题？
导入操作系统
从 ultralytics 导入 YOLO

进口火炬
导入 matplotlib
将 numpy 导入为 np
导入CV2
导入pygame

pygame.init()
sound_to_play = pygame.mixer.Sound(r&#39;D:\ML\同步警惕驱动程序\alarm.wav&#39;)
sound_to_play.play()

模型 = YOLO(r&#39;C:\Users\HP\Downloads\last.pt&#39;)

上限 = cv2.VideoCapture(0)
而真实：
    ret, 框架 = cap.read()

    结果 = model.predict(source=“0”,show=True)
    对于结果中的 r：
        如果 len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            打印（d类）
            如果 dclass==2.0:
              sound_to_play.play()
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

pygame.quit()
cap.release()
cv2.destroyAllWindows()

问题是我的代码首先进行检测并将其存储在结果中，然后进入 for 循环。预期输出是它同时检测并检查类值。]]></description>
      <guid>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</guid>
      <pubDate>Sat, 16 Mar 2024 06:28:04 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型仅返回 0 分。我做错了什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</link>
      <description><![CDATA[在 Jupyter-Notebook 中，我创建了一个函数，可以对不同的 sklearn 机器学习模型进行拟合和评分。使用的数据集有超过 400000 行和 103 列，因此我分为两个不同的数据集：训练数据集和验证数据集。但是当我在函数中使用数据时，我想要测试的所有 4 个模型的得分均为 0。
这是我的代码：
# 分割数据集
df_val = df_tmp[df_tmp[&#39;年份&#39;] == 2012]
df_train = df_tmp[df_tmp[&#39;年份&#39;] != 2012]

# 将数据集分为训练和测试
X_train, y_train = df_train.drop(&#39;年&#39;, axis=1), df_train[&#39;年&#39;]
X_val, y_val = df_val.drop(&#39;年份&#39;, axis=1), df_val[&#39;年份&#39;]

# 将模型放入字典中
测试模型 = {
    “套索”：套索()，
    “ElasticNet”：ElasticNet()，
    “RandomForestRegressor”：RandomForestRegressor()，
    “山脊”：山脊()
}

# 创建函数来评估两个模型
def fit_and_score(test_models, X_train, X_val, y_train, y_val):
    
    # 记录模型分数的字典
    模型分数 = {}
    
    ＃ 环形
    for name, model in test_models.items(): # name, model = key, value
        # 拟合模型
        model.fit(X_train, y_train)
        # 评估模型并将其分数附加到 models_scores
        models_scores[名称] = model.score(X_val, y_val)
        
    返回模型分数

首先我想也许我没有正确编写函数，所以我单独测试了模型，它们仍然得分为 0。之后我决定测试是否我的数据有问题（我不认为这是它，bcz 我从一个旧的 Kaggle 竞赛中得到它，推土机竞赛），所以我用相同的数据训练并安装了一个模型，希望我的分数是 1，但我得到了 0.31。我真的不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</guid>
      <pubDate>Sat, 16 Mar 2024 01:04:49 GMT</pubDate>
    </item>
    <item>
      <title>验证损失根本没有改变</title>
      <link>https://stackoverflow.com/questions/72446953/validation-loss-is-not-changing-at-all</link>
      <description><![CDATA[我第一次使用 PyTorch 来使用 Bert 的预训练模型来训练我的情绪分析模型。
这是我的分类器
类 SentimentClassifier2(nn.Module):

  def __init__(self, n_classes):
    super(SentimentClassifier2, self).__init__()
    D_输入、H、D_输出 = 768、200、3

    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.4)

    self.classifier = nn.Sequential(
            nn.Linear(D_in, H),
            ReLU(),
            nn.Linear(H, D_out)
    ）
  defforward（自身，input_ids，attention_mask）：

         _，pooled_output = self.bert（input_ids = input_ids，attention_mask = attention_mask，return_dict = False）
         输出 = self.drop(pooled_output)
         logits = self.classifier(输出)
         返回逻辑值

这是我的优化器/损失函数（我只做了 20 个周期，因为训练需要一段时间）
EPOCHS = 20

model2 = SentimentClassifier2(len(class_names))
model2= model2.to(设备)

优化器 = AdamW(model.parameters(), lr=2e-5, Correct_bias=True)

总步数 = len(train_data_loader) * EPOCHS

调度程序 = get_linear_schedule_with_warmup(
  优化器，
  num_warmup_steps=0,
  num_training_steps=total_steps
）
loss_fn = nn.CrossEntropyLoss().to(设备)

培训与培训评估代码
def train_epoch（模型，data_loader，loss_fn，优化器，设备，调度程序，n_examples）：
  模型 = model.train()
  损失=[]
  正确预测 = 0
  对于 data_loader 中的 d：
    input_ids = d[“input_ids”].to(设备)
    注意掩码 = d[“注意掩码”].to(设备)
    目标 = d[“目标”].to(设备)

    输出=模型（
      输入ID=输入ID，
      注意掩码=注意掩码
    ）

    _, preds = torch.max(输出, 暗淡=1)
    损失 = loss_fn(输出，目标)

    Correct_predictions += torch.sum(preds == 目标)
    损失.追加（损失.项目（））

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    优化器.step()
    调度程序.step()
    优化器.zero_grad()

  返回 Correct_predictions.double() / n_examples, np.mean(losses)


def eval_model（模型，data_loader，loss_fn，设备，n_examples）：
  模型 = model.eval()

  损失=[]
  正确预测 = 0
  使用 torch.no_grad()：
    对于 data_loader 中的 d：
      input_ids = d[“input_ids”].to(设备)
      注意掩码 = d[“注意掩码”].to(设备)
      目标 = d[“目标”].to(设备)

      输出=模型（
        输入ID=输入ID，
        注意掩码=注意掩码
      ）

      _, preds = torch.max(输出, 暗淡=1)
      损失 = loss_fn(输出，目标)

      Correct_predictions += torch.sum(preds == 目标)
      损失.追加（损失.项目（））

  返回 Correct_predictions.double() / n_examples, np.mean(losses)

我的问题：验证样本的损失根本没有改变！
&lt;前&gt;&lt;代码&gt;纪元1：______________________
列车损失 1.0145157482929346 准确度 0.4185746994848311
价值损失 1.002384223589083 准确度 0.4151087371232354
纪元2：______________________
列车损失 1.015038197996413 准确度 0.41871780194619346
价值损失 1.002384223589083 准确度 0.4151087371232354
epoch3：______________________
列车损失 1.014710763787351 准确度 0.4188609044075558
价值损失 1.002384223589083 准确度 0.4151087371232354
epoch4：______________________
列车损失 1.0139196826735648 准确度 0.41909940850982635
价值损失 1.002384223589083 准确度 0.4151087371232354

我不明白问题出在哪里......]]></description>
      <guid>https://stackoverflow.com/questions/72446953/validation-loss-is-not-changing-at-all</guid>
      <pubDate>Tue, 31 May 2022 11:26:27 GMT</pubDate>
    </item>
    </channel>
</rss>