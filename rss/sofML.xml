<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 24 Oct 2024 15:18:56 GMT</lastBuildDate>
    <item>
      <title>pytorch 的 mask r-cnn 推理 ONNX 模型从不起作用</title>
      <link>https://stackoverflow.com/questions/79122467/inferencing-onnx-model-of-pytorchs-mask-r-cnn-never-works</link>
      <description><![CDATA[问题：推理 onnx 模型给出空结果或形状奇怪的结果
我正在尝试：
pytorch 预训练 mask-rcnn -&gt; 在数据集上微调 -&gt; 另存为 onnx -&gt; 在 onnx 上推理 -&gt; 绘制结果
我目前拥有的一切都在推理之前有效。
我的 main.py 文件：https://pastebin.com/3jNfZdBi
获取有关我保存的 onnx 模型的信息
模型输入信息：
名称：输入
形状：[&#39;batch_size&#39;, 3, &#39;height&#39;, &#39;width&#39;]
类型：张量（浮点）

模型输出信息：
名称：boxes
形状：[&#39;Concatboxes_dim_0&#39;, 4]
类型：张量（浮点）
名称：labels
形状：[&#39;Gatherlabels_dim_0&#39;]
类型：张量（int64）
名称：分数
形状：[&#39;Gatherlabels_dim_0&#39;]
类型：张量（浮点）
名称：掩码
形状：[&#39;Unsqueezemasks_dim_0&#39;, &#39;Unsqueezemasks_dim_1&#39;, &#39;Unsqueezemasks_dim_2&#39;, &#39;Unsqueezemasks_dim_3&#39;]
类型：张量（浮点）

我的推理代码：https://pastebin.com/wxvp649G
我怀疑我要么：错误地将内容保存到 onnx，要么没有正确地预处理我的数据，要么我的推理代码是错误的（或其他我不知道的东西）
保存到 onnx 的代码
def save_model_onnx(models_file_path, model, torch_input):
#传统的导出方法。还有一种实验性的 dynamo_export 方法
torch.onnx.export(
model.cpu(),
torch_input.cpu(),
models_file_path, # 模型的完整路径，包括模型本身，即 ./models/model.onnx
export_params = True,
opset_version=15, # 选择支持的 ONNX opset 版本
do_constant_folding=True, # 折叠常量节点以进行优化
input_names = [&#39;input&#39;],
output_names = [&#39;boxes&#39;, &#39;labels&#39;, &#39;scores&#39;, &#39;masks&#39;],
dynamic_axes={
&quot;input&quot;: {0: &quot;batch_size&quot;, 2: &quot;height&quot;, 3: &quot;width&quot;}, 
}
)
logstash.info(f&quot;模型保存在{models_file_path}&quot;)

提前感谢大家]]></description>
      <guid>https://stackoverflow.com/questions/79122467/inferencing-onnx-model-of-pytorchs-mask-r-cnn-never-works</guid>
      <pubDate>Thu, 24 Oct 2024 14:41:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的部署问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79121685/deployment-issues-on-machine-learning</link>
      <description><![CDATA[如何在没有 html 文件的情况下将 ML 模型部署到 github 操作中，我使用了 streamlit 应用程序？
我曾尝试找到一些使用 github 操作部署机器学习的源代码，但经常失败。
我有这个问题，该如何解决它。]]></description>
      <guid>https://stackoverflow.com/questions/79121685/deployment-issues-on-machine-learning</guid>
      <pubDate>Thu, 24 Oct 2024 11:31:36 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型中的过度拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/79120206/overfitting-in-cnn-model</link>
      <description><![CDATA[我一直在做膝骨关节炎严重程度分级的项目，并使用了 Cnn（不能使用预训练模型）。我正在使用 kaggle 的数据集。我使用了 3 个卷积层和 2 个池化层。但我面临过度拟合的问题。我的第 1 类中的大多数数据被预测为 0 和 2，第 4 类也是如此。
我实现了 smote 类权重，使用了不同的增强、批量归一化、dropouts，尝试了不同的学习率、调度程序，但没有任何改善。我下一步该怎么做？
这是我的模型，没有任何 dropouts 或 normalization。
class SimpleCNN(nn.Module):
def __init__(self,num_classes):
super(SimpleCNN,self).__init__()
self.conv1 = nn.Conv2d(1,64,kernel_size=3,padding=1)
self.relu1 = nn.ReLU()
self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)

self.conv2 = nn.Conv2d(64,64,kernel_size=3,padding=1)
self.relu2 = nn.ReLU()
self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)

self.conv3 = nn.Conv2d(64,64,kernel_size=3,padding=1)
self.relu3 = nn.ReLU()
self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)

self.flatten = nn.Flatten()
self.fc1 = nn.Linear(40000,1024)
self.relu4 = nn.ReLU()
self.fc2 = nn.Linear(1024,num_classes)

def forward(self,x):
x = self.pool1(self.relu1(self.conv1(x)))

x = self.pool2(self.relu2(self.conv2(x)))
x = self.pool3(self.relu3(self.conv3(x)))
# print(x.size())

x = self.flatten(x)
x = self.relu4(self.fc1(x))
x = self.fc2(x)

返回 x
]]></description>
      <guid>https://stackoverflow.com/questions/79120206/overfitting-in-cnn-model</guid>
      <pubDate>Thu, 24 Oct 2024 02:23:24 GMT</pubDate>
    </item>
    <item>
      <title>如何将多元时间序列转换为单变量时间序列？</title>
      <link>https://stackoverflow.com/questions/79118449/how-to-convert-multivariate-time-series-into-univariate-time-series</link>
      <description><![CDATA[我拥有多变量信息，大约 9 个维度的时间序列是从不同的传感器收集的。我想将这些信号组合或整合为一个信号时间序列来表示该事件。然后我将使用该单个时间序列进行异常检测。除了 PCA 之外，还有哪些推荐的方法（数学或机器学习）可以将我的 9 维时间序列缩减为 1 维？ 1 维单个时间序列应该能够有效地从所有传感器捕获该时间段的细微差别。然后可以使用一些分类或回归方法。
我想避免使用 PCA 并探索其他可用方法。是否有软件包可以从多变量时间序列信号创建单一统一的时间序列模型？]]></description>
      <guid>https://stackoverflow.com/questions/79118449/how-to-convert-multivariate-time-series-into-univariate-time-series</guid>
      <pubDate>Wed, 23 Oct 2024 14:46:23 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.api.backend”没有属性“clip”</title>
      <link>https://stackoverflow.com/questions/79116828/module-keras-api-backend-has-no-attribute-clip</link>
      <description><![CDATA[我使用 Colab 进行编码并收到此错误：
AttributeError：模块“keras.api.backend”没有属性“clip”。

我尝试升级 TensorFlow 和 Keras，但仍然收到相同的错误。我在第一个 epoch 拟合模型时收到此错误。
我该如何修复它？
import fragmentation_models as sm
model_vgg16=sm.Unet(backbone_name=backbone,input_shape=(256,256,3),classes=4,activation=&quot;softmax&quot;,encoder_weights=&quot;imagenet&quot;,decoder_use_batchnorm=True,encoder_freeze=False )

model_vgg16.summary()

&quot;&quot;&quot;# loss and metrics&quot;&quot;&quot;

loss=&quot;categorical_crossentropy&quot;

dice_loss=sm.losses.DiceLoss()
focal_loss=sm.losses.CategoricalFocalLoss()

focal_dice_loss=sm.losses.categorical_focal_dice_loss

metric=[sm.metrics.IOUScore(threshold=0.5)]

&quot;&quot;&quot;# compile&quot;&quot;&quot;

lr=0.001
model_vgg16.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),
loss=[focal_dice_loss],
metrics=[metric])

history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,validation_data=(preprocessed_x_val,y_val_categorical),batch_size=32)

错误：
Epoch 1/20
--------------------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-76-887dcd97e6be&gt; 在 &lt;cell line: 1&gt;()
----&gt; 1 history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,
2 validation_data=(preprocessed_x_val,y_val_categorical),
3 batch_size=32)

3 帧
/usr/local/lib/python3.10/dist-packages/segmentation_models/base/ functional.py in categorical_focal_loss(gt, pr, gamma, alpha, class_indexes, **kwargs)
276 
277 # 剪辑以防止 NaN 和 Inf
--&gt; 278 pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())
279 
280 # 计算焦点损失

AttributeError: 模块 &#39;keras.api.backend&#39; 没有属性 &#39;clip&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79116828/module-keras-api-backend-has-no-attribute-clip</guid>
      <pubDate>Wed, 23 Oct 2024 07:29:46 GMT</pubDate>
    </item>
    <item>
      <title>LeNet的训练结果保持不变</title>
      <link>https://stackoverflow.com/questions/79116509/the-training-results-of-lenet-remain-unchanged</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79116509/the-training-results-of-lenet-remain-unchanged</guid>
      <pubDate>Wed, 23 Oct 2024 05:40:55 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将 MS COCO 2017 测试数据集中的图像分成包含小、中、大物体的图像？[关闭]</title>
      <link>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</link>
      <description><![CDATA[我正在研究在 MS COCO 测试数据集中分离包含小物体的图像。由于测试集没有注释，有什么方法可以完成此操作吗？
我尝试使用图像的宽度和高度参数，但在小图像部分只得到了 360 张图像。如何获取测试集的注释信息？]]></description>
      <guid>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</guid>
      <pubDate>Tue, 22 Oct 2024 08:33:45 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中实现贝叶斯逻辑回归和 SVM 进行分类</title>
      <link>https://stackoverflow.com/questions/79098217/how-to-implement-bayesian-logistic-regression-and-svm-for-classification-in-pyth</link>
      <description><![CDATA[我想在单个 Python 代码中实现贝叶斯逻辑回归和 SVM 进行分类，但我无法做到，请任何人告诉我。
我尝试使用 pymc3，但它不起作用。我也尝试使用 theano tensor，但都没有用，有人能帮我解决这些吗]]></description>
      <guid>https://stackoverflow.com/questions/79098217/how-to-implement-bayesian-logistic-regression-and-svm-for-classification-in-pyth</guid>
      <pubDate>Thu, 17 Oct 2024 13:04:09 GMT</pubDate>
    </item>
    <item>
      <title>我的梯度下降实现有什么问题（带铰链损失的 SVM 分类器）</title>
      <link>https://stackoverflow.com/questions/79055573/what-is-wrong-with-my-gradient-descent-implementation-svm-classifier-with-hinge</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79055573/what-is-wrong-with-my-gradient-descent-implementation-svm-classifier-with-hinge</guid>
      <pubDate>Fri, 04 Oct 2024 19:19:48 GMT</pubDate>
    </item>
    <item>
      <title>通过索引为张量赋值后，值不匹配</title>
      <link>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</link>
      <description><![CDATA[我正在编写一个 PyTorch 训练代码，它构建了一个算法类。其中有一个步骤需要为内部张量分配一些值。但是，即使代码只有两行，也有一个错误。我发现分配的张量的值与分配的值不同。
这是该类的代码：
class PRODEN(Algorithm):
&quot;&quot;&quot;
PRODEN
参考：部分标签学习的真实标签的渐进式识别，ICML 2020。
&quot;&quot;&quot;

def __init__(self, input_shape, train_givenY, hparams):
super(PRODEN, self).__init__(input_shape, train_givenY, hparams)
self.featurizer = networks.Featurizer(input_shape, self.hparams)
self.classifier = networks.Classifier(
self.featurizer.n_outputs,
self.num_classes)

self.network = nn.Sequential(self.featurizer, self.classifier)
self.optimizer = torch.optim.Adam(
self.network.parameters(),
lr=self.hparams[&quot;lr&quot;],
weight_decay=self.hparams[&#39;weight_decay&#39;]
)
train_givenY = torch.from_numpy(train_givenY)
tempY = train_givenY.sum(dim=1).unsqueeze(1).repeat(1, train_givenY.shape[1])
label_confidence = train_givenY.float()/tempY
self.label_confidence = label_confidence

def update(self, minibatches):
_, x, strong_x, partial_y, _, index = minibatches
loss = self.rc_loss(self.predict(x), index)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
self.confidence_update(x, partial_y, index)
return {&#39;loss&#39;: loss.item()}

def rc_loss(self, output, index):
device = &quot;cuda&quot; if index.is_cuda else &quot;cpu&quot;
self.label_confidence = self.label_confidence.to(device)
logsm_outputs = F.log_softmax(outputs, dim=1)
#print(self.label_confidence.is_cuda)
final_outputs = logsm_outputs * self.label_confidence[index, :]
average_loss = - ((final_outputs).sum(dim=1)).mean()
return average_loss

def predict(self, x):
return self.network(x)

def confidence_update(self, batchX, batchY, batch_index):
with torch.no_grad():
batch_outputs = self.predict(batchX)
temp_un_conf = F.softmax(batch_outputs, dim=1)
&#39;&#39;&#39;有问题的代码开始了&#39;&#39;&#39;
self.label_confidence[batch_index, :] = temp_un_conf * batchY # un_confidence 存储每个示例的权重
&#39;&#39;&#39;问题代码结束&#39;&#39;&#39;
base_value = self.label_confidence.sum(dim=1).unsqueeze(1).repeat(1, self.label_confidence.shape[1])
self.label_confidence = self.label_confidence / base_value

问题出在 confidence_update 上。我发现
self.label_confidence[batch_index, :]

的值与
temp_un_conf * batchY

在此分配之后
self.label_confidence[batch_index, :] = temp_un_conf * batchY

仅适用于少数示例，但适用于大多数示例。例如，对于 1024 的批次大小，第一次迭代时大约有 4 个示例，之后会变得更大。我对这个问题非常沮丧，尝试了很多方法：

这个问题只存在于 CIFAR10，但其他数据集不存在。

所有张量的数据类型都是 Float32。

所有张量都在 gpu 上。


我的代码有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</guid>
      <pubDate>Wed, 04 Sep 2024 15:41:44 GMT</pubDate>
    </item>
    <item>
      <title>Keras 的 one_hot 对不同的词产生相同的值</title>
      <link>https://stackoverflow.com/questions/78626998/one-hot-from-keras-producing-the-same-value-for-different-words</link>
      <description><![CDATA[我使用 keras 的 one_hot 函数将单词转换为数字。但出于某种原因，它会为不同的单词生成相同的数字。在下面的代码中，您可以看到 48 用于“amazing”，但 48 也用于“too”。这是为什么？
from tensorflow.keras.preprocessing.text import one_hot

reviews = [&#39;nice food&#39;,
&#39;amazing restaurant&#39;,
&#39;too good&#39;,
&#39;just loved it!&#39;,
&#39;will go again&#39;,
&#39;horrible food&#39;,
&#39;never go there&#39;,
&#39;poor service&#39;,
&#39;poor quality&#39;,
&#39;needs Improvement&#39;]

# 转换为 ont hot 向量 
encoded_reviews = [one_hot(d, vocab_size) for d in reviews]

当我打印coded_reviews 时，它显示：
[[13, 12],
[48, 44],
[48, 19],
[38, 28, 46],
[13, 29, 19],
[46, 12],
[19, 29, 4],
[18, 38],
[18, 35],
[42, 7]]
]]></description>
      <guid>https://stackoverflow.com/questions/78626998/one-hot-from-keras-producing-the-same-value-for-different-words</guid>
      <pubDate>Sat, 15 Jun 2024 15:16:01 GMT</pubDate>
    </item>
    <item>
      <title>在 PyCharm 中运行 Teachable Machines 对象识别器</title>
      <link>https://stackoverflow.com/questions/78596363/running-teachable-machines-object-recognizer-in-pycharm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78596363/running-teachable-machines-object-recognizer-in-pycharm</guid>
      <pubDate>Sat, 08 Jun 2024 16:51:36 GMT</pubDate>
    </item>
    <item>
      <title>内核形状必须与输入具有相同的长度，但接收形状为 A 的内核和形状为 B 的输入</title>
      <link>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-a</guid>
      <pubDate>Sun, 07 Apr 2024 12:27:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 c++ 通过 onnx 和 opencv 制作超分辨率图像？</title>
      <link>https://stackoverflow.com/questions/77328225/how-to-do-super-resolution-image-with-onnx-and-opencv-using-c</link>
      <description><![CDATA[我的超分辨率 ios/macos 应用程序有 animesr.onnx，所以我需要将我的 python 代码转换为 c++ 代码。这些是我的 python 代码：
session = onnxruntime.InferenceSession(&#39;animesr.onnx&#39;)

img = cv2.imread(&#39;imgs/naruto.jpg&#39;)
ori_h, ori_w, _ = img.shape
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (512,512))
img = (np.array(img) / 255.0).astype(np.float32)
img = np.transpose(img, (2, 0, 1))
img = np.expand_dims(img, 0)

input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

input_feed = {input_name: img}
output = session.run([output_name], input_feed)

output = output[0].clip(0, 1) * 255
output = output.astype(np.uint8)
output = np.squeeze(output)
output = np.transpose(output, (1, 2, 0))
output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)
output = cv2.resize(output, (ori_w*4, ori_h*4))
cv2.imwrite(&#39;naruto_animesr.jpg&#39;, output)

它工作正常，结果如下（左边是之前，右边是之后）

这是我的 c++ 代码：
Ort::Session session(env, ORT_TSTR(modelPath), sessionOptions);

cv::Mat inputImage = cv::imread(imagePath, cv::IMREAD_COLOR);
// cv::Mat blob = cv::dnn::blobFromImage(inputImage, 1.0/255, cv::Size(512,512), cv::Scalar(), true);
cv::Mat resizedImage;
cv::resize(inputImage, resizedImage, cv::Size(512,512));
cv::Mat floatImage;
resizedImage.convertTo(floatImage, CV_32FC3, 1.0/255.0);

Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
std::vector&lt;int64_t&gt; inputShape = {1, 3, 512, 512};

Ort::Value inputTensor = Ort::Value::CreateTensor&lt;float&gt;(memoryInfo, (float*) floatImage.data, 3*512*512, inputShape.data(), inputShape.size());

std::vector&lt;const char*&gt; inputNames = {&quot;input&quot;};
std::vector&lt;const char*&gt; outputNames = {&quot;output&quot;};

std::vector&lt;Ort::Value&gt; outputTensor = session.Run(Ort::RunOptions{}, inputNames.data(), &amp;inputTensor, 1, outputNames.data(), outputNames.size());

Ort::TensorTypeAndShapeInfo outputInfo = outputTensor[0].GetTensorTypeAndShapeInfo();
int channels = outputInfo.GetShape()[1]; // 3
int height = outputInfo.GetShape()[2]; // 2048
int width = outputInfo.GetShape()[3]; // 2048

const float* outputData = outputTensor[0].GetTensorMutableData&lt;float&gt;();

cv::Mat outputImage(height, width, CV_32FC(channels), const_cast&lt;float*&gt;(outputData));
cv::Mat uint8Image;
outputImage.convertTo(uint8Image, CV_8UC3, 255);
cv::Mat bgrOutput;
cv::cvtColor(uint8Image, bgrOutput, cv::COLOR_RGB2BGR);
cv::imwrite(outputPath, bgrOutput);

这是结果

这是使用 cv::ddn::blobFromImage() 的结果

两者都是糟糕的结果，与我的 python 结果不同。
我认为问题在于在运行模型之前将图像作为输入进行预处理，并将输出处理为 cv 图像，我不擅长矩阵运算，如变换、转置，重塑等。
您能帮我提供使用 opencv c++ 的正确处理图像代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/77328225/how-to-do-super-resolution-image-with-onnx-and-opencv-using-c</guid>
      <pubDate>Fri, 20 Oct 2023 03:48:27 GMT</pubDate>
    </item>
    <item>
      <title>将卫星图像与地图匹配</title>
      <link>https://stackoverflow.com/questions/74901142/matching-satellite-images-to-map</link>
      <description><![CDATA[我目前有点被一个问题难住了，听起来比实际容易（至少对我来说）：
假设你有从低地球轨道（LEO）拍摄的卫星图像，显示大约 1000 公里宽的区域（相机的图像轴或多或少垂直于地面）。图像中没有存储其他位置数据，因此无法直接提取拍摄图像的位置）。
我想要做的是编写一个程序（用 Python），可以通过将其与地球地图进行匹配来找到拍摄图像的位置。这应该自动完成（或多或少是实时的），以便计算拍摄图像的卫星的轨道。
一旦我有位置数据（即使非常嘈杂），使用基于扩展卡尔曼滤波器的技术，我可以毫无问题地计算轨道。
另一方面，仅使用图像数据将卫星图像与地球地图相匹配......老实说，我甚至不知道从哪里开始。
我知道这是一个非常不具体的问题，与特定问题无关，但也许有人可以给我指出正确的方向......
编辑：
只是为了让你了解未经处理的低地球轨道图像是什么样子，我附上了一些在地球轨道上拍摄的相当不错的图像。




图片是用 NIR 相机拍摄的。我所包含图片的分辨率只有 640x480（错误！），但图片分辨率应该在 4k 左右。
这些图片有一些瑕疵，因为它们是通过国际空间站的厚玻璃窗拍摄的 - 所以那里有一些反射……]]></description>
      <guid>https://stackoverflow.com/questions/74901142/matching-satellite-images-to-map</guid>
      <pubDate>Fri, 23 Dec 2022 15:08:14 GMT</pubDate>
    </item>
    </channel>
</rss>