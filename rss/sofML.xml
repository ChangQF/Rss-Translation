<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 15 May 2024 09:17:51 GMT</lastBuildDate>
    <item>
      <title>有没有办法检测一个人是否在自动扶梯上？</title>
      <link>https://stackoverflow.com/questions/78482838/is-there-a-way-of-detecting-if-a-person-is-on-escalator-or-not</link>
      <description><![CDATA[我正在做一个项目来检测一个人是否踏上自动扶梯。
我尝试过的：
我使用 Android 传感器数据（线性加速器）来检测一个人是否正在行走、停止或在自动扶梯上。我用这样的公式标准化了 x,y,z：sqrt(x^2+y^2+z^2)，然后将它们分组为 64 并转换为 fft 数据，然后在决策树算法上进行训练。
它在检测一个人正在行走时效果很好，但当一个人停下来或手机快速移动时，它会出现很多错误。
还有其他我可以使用的传感器数据吗？或者还有其他方法可以解决吗？]]></description>
      <guid>https://stackoverflow.com/questions/78482838/is-there-a-way-of-detecting-if-a-person-is-on-escalator-or-not</guid>
      <pubDate>Wed, 15 May 2024 09:14:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 SQL 创建数据透视表来跟踪 3 个月内的客户购买和支出</title>
      <link>https://stackoverflow.com/questions/78482756/creating-a-pivot-table-using-sql-to-track-customer-purchases-and-expenses-over-a</link>
      <description><![CDATA[我想创建一个表格来统计客户对我的产品的购买次数和费用。我正在 SQL 中创建一个包含以下列的数据透视表：
-execution_date：运行查询以扫描过去 3 个月数据库的日期
- client_id：过去 3 个月内我所有客户的标识符
-购买产品（布尔值乘以我数据库中的产品数量）：如果客户在过去 3 个月内没有购买过我的产品，则为 False
-product_expenses（布尔值乘以我的数据库中的产品数量）：过去 3 个月内客户在我的产品上产生的费用累计总额
-product_quantity（布尔值乘以我的数据库中的产品数量）：客户在过去 3 个月内购买的商品数量的累计总和
这个想法是创造一种“历史”跟踪过去 3 个月内某个时间段内客户的活动。为此，我使用：
- 销售表：销售
- 产品参考表：产品
- GENERATE_DATE_ARRAY 函数： 特别是计算在创建表之前进行的购买以检索历史记录
这是一个例子：
我的客户A于2017年2月12日在我的数据库中注册，并且在前3个月内没有进行任何购买：

2017 年 5 月 20 日，他以 35 美元 的价格购买了 产品 X，数量为 2 件。

在2018年1月16日，他以34美元的价格重新购买了相同数量的相同产品。

2018 年 2 月 10 日，他首次以 60 美元 的价格购买 **产品 Y **，数量 3单位。


这是所需输出的示例。

我尝试过这种方法：
&lt;前&gt;&lt;代码&gt;与
 SHAS（
  选择
    客户端_id，
    购买日期，
    产品ID，
    商品数量，
    花费

  来自 sales, UNNEST(sales.lines) A​​S l
  
  哪里有效_购买
    并且 client_id = &#39;A&#39;
）

，最终AS（
选择
  s.*
  , 分类轴 AS 产品名称
  , COUNT(DISTINCT ocdTicketOrOrderId) OVER (PARTITION BY ocdMasterId) AS 订单

  
来自销售 AS
  LEFT JOIN 产品 AS p ON s.eanCode = p.variantInfo.eanCode

真实情况
  并且分类轴不为空
  并且 client_id = &#39;A&#39;
）

选择
  p.*
从 （
  选择
    日期
  ， 产品名称
  , SUM(IF(itemUnitPriceTaxIncludedAfterDiscount IS NULL, 0, itemUnitPriceTaxIncludedAfterDiscount)) AS 价格

从决赛开始
  右连接 `data-crm-france.bucket_crm.fr_dcr_dates` AS d ON DATE(d.dates) = DATE_TRUNC(DATE(final.datepurchase), MONTH)

  WHERE 日期 &gt;= DATE(&#39;2021-01-01&#39;)

按所有人分组
）
  PIVOT (SUM(IF(价格为 NULL, 0, 价格)) FOR 产品名称 IN (&#39;X&#39;, &#39;Y&#39;)) AS p

该方法未提供所需的结果，因为它包含空值并且不计算累积和
]]></description>
      <guid>https://stackoverflow.com/questions/78482756/creating-a-pivot-table-using-sql-to-track-customer-purchases-and-expenses-over-a</guid>
      <pubDate>Wed, 15 May 2024 09:01:25 GMT</pubDate>
    </item>
    <item>
      <title>！在“.symlinks/plugins/flutter_tflite/ios”中找不到“flutter_tflite”的 podspec</title>
      <link>https://stackoverflow.com/questions/78481864/no-podspec-found-for-flutter-tflite-in-symlinks-plugins-flutter-tflite-ios</link>
      <description><![CDATA[我正在使用 flutter_tflite: ^1.0.1 来构建我的应用程序。它适用于 Android 手机和模拟器。但是当我尝试在 Iphone 模拟器上运行它或构建 ios 应用程序时，我收到错误。
&lt;块引用&gt;
flutter 构建 ios

我得到的错误是
`
正在运行 pod install...361ms
CocoaPods 的输出：
↳
准备中
分析依赖关系

检查要集成的目标
  使用“ARCHS”设置构建目标“Pods-Runner”的架构：(``)

获取外部资源
-&gt;从“Flutter”获取“Flutter”的 podspec
-&gt;从以下位置获取“camera_avfoundation”的 podspec
`.symlinks/plugins/camera_avfoundation/ios`
-&gt;从以下位置获取“flutter_tflite”的 podspec
`.symlinks/plugins/flutter_tflite/ios`
[!] 未找到“flutter_tflite”的 podspec
`.symlinks/plugins/flutter_tflite/ios`

/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/external_sources/path_source.rb:14:在“获取中的块”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/user_interface.rb:64:在“部分”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/external_sources/path_source.rb:11:在“获取”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:997:在“fetch_external_source”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:976:在“fetch_external_sources 中的块（2 级）”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:975:在“每个”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:975:在“fetch_external_sources 中的块”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/user_interface.rb:64:在“部分”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:974:在`fetch_external_sources&#39;中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer/analyzer.rb:118:在“分析”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer.rb:422:在“分析”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer.rb:244:in `resolve_dependency 中的块&#39;
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/user_interface.rb:64:在“部分”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer.rb:243:在`resolve_dependency&#39;中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/installer.rb:162:在“安装！”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/command/install.rb:52:在“运行”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/claide-1.1.0/lib/claide/com
mand.rb:334:在“运行”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/lib/cocoap
ods/command.rb:52:在“运行”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/gems/cocoapods-1.15.2/bin/pod:55
：在“&lt;顶部（必需）&gt;”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/bin/pod:25:在“加载”中
/opt/homebrew/Cellar/cocoapods/1.15.2/libexec/bin/pod:25:在 `&#39; 中

运行 pod install 时出错`]]></description>
      <guid>https://stackoverflow.com/questions/78481864/no-podspec-found-for-flutter-tflite-in-symlinks-plugins-flutter-tflite-ios</guid>
      <pubDate>Wed, 15 May 2024 05:55:08 GMT</pubDate>
    </item>
    <item>
      <title>model.compile 损失 TypeError：缺少必需的位置参数</title>
      <link>https://stackoverflow.com/questions/78481612/model-compile-loss-typeerror-missing-required-positional-argument</link>
      <description><![CDATA[最小的例子是
将 numpy 导入为 np
将张量流导入为 tf
从张量流导入keras
从 keras.losses 导入 huber

# 创建数据集
x = np.random.rand(10, 1)
y = 2 * x + np.random.randn(10, 1) * 0.1

# 定义模型
模型 = keras.Sequential([
    keras.layers.Dense(1, input_shape=[1])
]）

# 编译模型
# model.compile(loss=huber, optimizationr=&#39;adam&#39;) # 有效
# model.compile(loss=&#39;huber&#39;, optimizationr=&#39;adam&#39;) # 有效
model.compile(loss=huber(delta=0.1), 优化器=&#39;adam&#39;)

＃ 训练
model.fit(x, y, epochs=5)

当我在 model.compile() 中使用 huber loss 时，这两种方法都有效。
从 keras.losses 导入 huber

model.compile(loss=“huber”, optimizationr=optimizer=&#39;adam&#39;)

或者

model.compile(loss=huber, 优化器=optimizer=&#39;adam&#39;)


但是如果我想添加delta，就会出现TypeError。
添加 delta 的正确方法是什么？
感谢您的提前。
&lt;前&gt;&lt;代码&gt;
---&gt; 18 model.compile(loss=huber(delta=delta), 优化器=&#39;adam&#39;)
     

文件 ~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153，位于filter_traceback..error_handler(*args, **kwargs)
    151 异常如 e 除外：
...
-&gt;第1170章
   第1171章
   第1172章 返回结果

类型错误：缺少必需的位置参数
]]></description>
      <guid>https://stackoverflow.com/questions/78481612/model-compile-loss-typeerror-missing-required-positional-argument</guid>
      <pubDate>Wed, 15 May 2024 04:29:44 GMT</pubDate>
    </item>
    <item>
      <title>机器学习平台的多节点 GPU 利用与 Docker</title>
      <link>https://stackoverflow.com/questions/78481571/multi-node-gpu-utilization-with-docker-for-machine-learning-platform</link>
      <description><![CDATA[我的家庭实验室服务器有两个节点，我们称它们为节点 A 和节点 B。节点 A 有 4 个 GPU，节点 B 有 2 个 GPU。如何将计算能力与 Docker 容器结合起来用于机器学习/深度学习工具？如果有更好的解决方案，我很想知道。 :)
由于我对计算机系统的了解有限，我在节点 A 上安装了 Docker，并运行了一个机器学习平台容器（Jupyter 容器），以利用我拥有的最大计算能力。但是，通过这种方法，我最多只能利用节点 A 的 4 个 GPU。有没有办法结合这两种资源？理想情况下，当我在 Jupyter 容器内执行 nvidia-smi 时，它应该检测到来自节点 A 和节点 B 的 6 个 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/78481571/multi-node-gpu-utilization-with-docker-for-machine-learning-platform</guid>
      <pubDate>Wed, 15 May 2024 04:13:13 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Excel 电子表格拟合 sklearn 模型</title>
      <link>https://stackoverflow.com/questions/78481567/is-it-possible-to-fit-a-sklearn-model-using-a-excel-spreadsheet</link>
      <description><![CDATA[嗨，我是这个社区的新手，
但我想出了这个好奇心
因为我有一个嵌入式板来训练 sklearn
我有 nvidia jetson tx2，带有 256 个 nvidia cuda 核心
所以为了减少内存中的负载，我只是将训练数据传输到 Excel 文件中，通过导入我可以训练模型
导入训练数据
现在为了将其提供给模型，有一种方法
通过将数据添加到列表或 numpy 数组或张量
同样的后续代码
Excel数据转换成数据结构
然后通过model.fit给模型
model.fit
我知道这是一种不是最佳的方式
我的第一个问题是，有没有办法直接将 Excel 文件提供给 .h5 模型
没有任何可以忽略的中间数据结构
我尝试在网上搜索，但没有找到任何方法，如果您发现或知道作为一个社区，我们应该讨论和贡献知识。共同学习、共同进步。
现在我脑海中还存在一个次要的后续问题，那就是
我的第二个问题是如何专门为 jetson 设备或任何嵌入式板编写优化代码，即将到来。空间复杂度是我关心的问题之一，我只编写最小化代码，nvidia 编译器也已经编写了代码优化
但是有什么方法可以优化我缺少的代码
我的正常代码看起来像
&lt;前&gt;&lt;代码&gt;
任何简约的优化代码编写方法
对于资源有限的计算机（例如嵌入式板）
像 c++ 一样使用最少的基本临时库编写]]></description>
      <guid>https://stackoverflow.com/questions/78481567/is-it-possible-to-fit-a-sklearn-model-using-a-excel-spreadsheet</guid>
      <pubDate>Wed, 15 May 2024 04:09:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TensorFlow 提高多类分类的准确性？</title>
      <link>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</link>
      <description><![CDATA[我正在使用 TensorFlow 解决多类分类问题，并在实现令人满意的准确性方面遇到了挑战。我有7节课。文件夹中的每个类包含 2000 个 .csv 文件（每个文件有两列）。当我使用二元分类方法训练模型时，用另一个类测试一个类，准确性和 val_accuracy 会很高，0.85 到 0.95，但是当我使用多标签 7 进行测试时类，准确率最高达到0.47。下面是包含数据抛光和模型多类的代码。
#文件夹中的 csv 类
文件夹路径 = [
    &#39;/content/drive/MyDrive/medical_chem/Aa&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ab&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ac&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ba&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Bb&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Cc&#39;,
    &#39;/内容/驱动器/MyDrive/medical_chem/DD&#39;
]


数据 = []
标签=[]

#加载文件夹并将文件csv存档在数据框中
对于 enumerate(folder_paths) 中的 class_index、folder_path：
    对于 os.listdir(folder_path) 中的文件：
        file_path = os.path.join(文件夹路径, 文件)
        df = pd.read_csv(文件路径)
        数据.append(df)
        labels.append(class_index)

X = 数据
y = 标签

# 找到数据框中的最小值
min_length = min(len(df) for df in X)
# 设置数据帧长度相同
truncated_dfs = [df.head(min_length) for df in X]
# 数据帧到 numpy 数组
X = np.array([df.truncated_dfs 中 df 的值])


# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# 标准化数据
X_train = 归一化(X_train, 轴=1)
X_test = 归一化(X_test, 轴=1)
y_train = to_categorical(y_train, num_classes=7)
y_test = to_categorical(y_test, num_classes=7)


X_train.shape、y_train.shape、X_test.shape、y_test.shape
# 输出 ((8943, 2906, 2), (8943, 7), (2236, 2906, 2), (2236, 7))

模型 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(64, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(7,activation=&#39;softmax&#39;) # 7个类的输出层
]）

# 训练模型的检查点
checkpoint_path = “training_checkpoint/cp.ckpt”
checkpoint_dir = os.path.dirname(checkpoint_path)
checkpoint_callback = ModelCheckpoint(文件路径=checkpoint_path,
                                      save_weights_only=真，
                                      save_best_only=真，
                                      监视器=&#39;val_loss&#39;,
                                      详细=1)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;分类交叉熵&#39;，
              指标=[&#39;准确性&#39;])

#model.load_weights(检查点路径)

历史 = model.fit(X_train, y_train,
                    纪元=100，
                    验证数据=（X_测试，y_测试），
                    回调=[检查点回调])


我尝试过调整神经网络的架构，尝试不同的激活函数，并优化学习率和批量大小等超参数。但是，我仍然没有达到预期的准确性。
我确信我出错的地方是在预处理数据或模型中，因为二进制训练有很好的结果。
与二进制训练相比，准确度为 0.85 至 0.95
**多类别的预期准确率：高于 0.90
**
数据集： https://drive .google.com/drive/folders/1UAt50dPH7ABeoLu16nfa19g4oVccPeFO?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</guid>
      <pubDate>Wed, 15 May 2024 00:27:22 GMT</pubDate>
    </item>
    <item>
      <title>oml4r dplyr 是否支持选择辅助函数</title>
      <link>https://stackoverflow.com/questions/78480951/are-the-select-helper-functions-supported-in-oml4r-dplyr</link>
      <description><![CDATA[选择辅助函数是否应该在 oml4r dplyr 的透明层中工作。
我不这么认为，因为我下面显示的内容，
&lt;前&gt;&lt;代码&gt;&gt; e &lt;- 矿石拉动(EMP)
&gt; e %&gt;% select(包含(“NO”)) %&gt;% print
     员工部门
7369 7369 20
7499 7499 30
7521 7521 30
7566 7566 20
7654 7654 30
7698 7698 30
7782 7782 10
7788 7788 20
7839 7839 10
7844 7844 30
7876 7876 20
7900 7900 30
7902 7902 20
7934 7934 10
&gt; emp &lt;- ore.push(EMP)
&gt; emp %&gt;% select(包含(“NO”)) %&gt;% print
`eval_select_impl()` 中的错误：
！ `x` 必须是一个向量，而不是一个 目的。
运行 rlang::last_trace() 来查看错误发生的位置。
]]></description>
      <guid>https://stackoverflow.com/questions/78480951/are-the-select-helper-functions-supported-in-oml4r-dplyr</guid>
      <pubDate>Tue, 14 May 2024 22:47:19 GMT</pubDate>
    </item>
    <item>
      <title>在 oml4r 中创建嵌入式 R 脚本时出现“PLS-00201：必须声明标识符‘SYS.RQSCRIPTCREATE’”错误</title>
      <link>https://stackoverflow.com/questions/78480688/pls-00201-identifier-sys-rqscriptcreate-must-be-declared-error-when-creatin</link>
      <description><![CDATA[我在尝试使用 oml4r 创建嵌入式 R 脚本时遇到此错误，
# 创建一个公共脚本，可供任何用户使用。
&gt; ore.scriptCreate(“GLBGLM”,
+ 函数（数据、公式、...）
+ glm(公式 = 公式, 数据 = 数据, ...),
+ 全局 = TRUE,
+ 覆盖=真）
ore.scriptCreate(“GLBGLM”，函数(数据，公式，...) glm(公式 = 公式，：
  .oci.GetQuery(conn, statements, data = data, prefetch = prefetch, 中的错误：
  ORA-06550: 第 1 行，第 7 列：
PLS-00201：必须声明标识符“SYS.RQSCRIPTCREATE”
ORA-06550: 第 1 行，第 7 列：
PL/SQL：语句被忽略
帮助：https://docs.oracle.com/error-help/db/ora-06550/

这是我正在尝试的脚本，
 ore.scriptCreate(“GLBGLM”,
                 函数（数据、公式、...）
                   glm(公式 = 公式, 数据 = 数据, ...),
                 全局=真，
                 覆盖=真）
]]></description>
      <guid>https://stackoverflow.com/questions/78480688/pls-00201-identifier-sys-rqscriptcreate-must-be-declared-error-when-creatin</guid>
      <pubDate>Tue, 14 May 2024 21:15:58 GMT</pubDate>
    </item>
    <item>
      <title>调查 TensorFlow 和 PyTorch 性能的差异</title>
      <link>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</guid>
      <pubDate>Tue, 14 May 2024 13:54:26 GMT</pubDate>
    </item>
    <item>
      <title>PDF 阅读模型并给出准确答案</title>
      <link>https://stackoverflow.com/questions/78477795/model-for-pdf-read-and-give-accurate-answers</link>
      <description><![CDATA[任何人都可以建议聊天机器人的最佳模型吗？如果我给出任何 PDF 模型，应该阅读该 pdf，那么如果我问与该 PDF 模型相关的任何问题，应该给出正确的答案。
到目前为止，我已经发现了 1 个 Gemini 模型，但根据我的 PDF 数据，它不应该给出正确的答案]]></description>
      <guid>https://stackoverflow.com/questions/78477795/model-for-pdf-read-and-give-accurate-answers</guid>
      <pubDate>Tue, 14 May 2024 11:33:29 GMT</pubDate>
    </item>
    <item>
      <title>进行线性回归时结果不佳</title>
      <link>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</link>
      <description><![CDATA[我试图使用一组数据进行多元线性回归。我尝试使用用于生成回归系数的同一组 X 来预测 Y。虽然一组数据的实际值与预测值之间的差异较小（正如预期的那样），但另一组数据的实际值与预测值之间的差异较大。两个数据集代表同一组参数（相同的物理量）。我是否做错了什么或者我可以做些什么来改进？
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
将 statsmodels.api 导入为 sm
……
……
# 构建设计矩阵
X = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))

sst = np.array([i+273.15 for i in selected_buoy_sst])

# 拟合OLS模型
模型 = sm.OLS(sst, X)
结果 = model.fit()

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data`


# 使用不同的数据
X_n = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))
sst = 皮肤温度数组
# 拟合OLS模型
模型 = sm.OLS(sst_n, X_n)
结果 = model.fit()

# 打印回归结果的摘要
打印（结果.summary（））

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data


如果需要的话我可以上传数据。它取决于我的 x 和 Y 值吗？
编辑：我已经上传了所需的文件。
https://drive.google.com/file/d/1JGG_KPYZAEV20DuRopJBNSIBWtfWW5MT/查看？usp=共享
这里x.npy和y.npy是第一组数据，x1.npy和y1.npy是第二组数据。]]></description>
      <guid>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</guid>
      <pubDate>Fri, 10 May 2024 10:02:13 GMT</pubDate>
    </item>
    <item>
      <title>将任意深度转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</link>
      <description><![CDATA[我正在尝试将现有的 深度-anything PyTorch 模型转换为 CoreML 格式。我决定使用 Google Colab 并采取了以下内容推理深度任意模型的注释。但是，我在尝试将其导入 iOS 端时遇到了一些异常。这是我的转换代码片段：
# 安装所有需要的扩展
!pip 安装 coremltools
# ...

将 coremltools 导入为 ct
进口火炬

# 将 PyTorch 模型转换为 TorchScript
追踪模型 = torch.jit.trace(深度_任何东西, torch.rand(1, 3, 518, 518))

# 将 TorchScript 模型转换为 CoreML
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”，形状=(1,3,518,518)，比例=1/255.0)]
）

输出 = model_coreml._spec.description.output[0]
输出.type.imageType.colorSpace = ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
输出.类型.图像类型.宽度 = 518
输出.类型.图像类型.高度 = 518

# 保存修改后的CoreML模型
打印（model_coreml）
model_coreml.save(&#39;/content/drive/MyDrive/trained_models/深度9.mlpackage&#39;)

我尝试直接指定输入参数，就像我为输出参数所做的那样：
# 为输入模式创建字典
input_schema = {&#39;input_name&#39;: &#39;输入&#39;, &#39;input_type&#39;: ct.TensorType(shape=(1, 3, 518, 518))}

# 将输入模式添加到模型的元数据中
model_coreml.user_define_metadata[&#39;inputSchema&#39;] = str(input_schema)

或者使用convert_to选项设置neuralnetwork，如下所示：
model_coreml = ct.convert(
    追踪模型，
    输入=[ct.ImageType(名称=“input_1”,形状=(1,3,518,518),比例=1/255.0)],
    Convert_to=&#39;神经网络&#39;
）

或者使用BGR/GRAYSCALE设置ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value(&#39;RGB&#39;)
没有任何帮助。
如果我尝试使用neuralnetwork后端导入模型，我只会收到无限加载。如果我尝试使用 mlprogram 后端导入模型（默认，如果未指定），我会收到以下信息：

我期待任何建议和帮助，因为我需要的只是转换现有的 深度任意 模型，无需对 CoreML 格式进行调整或更改。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78439767/converting-depth-anything-to-coreml</guid>
      <pubDate>Tue, 07 May 2024 01:53:36 GMT</pubDate>
    </item>
    <item>
      <title>facebook / detr-resnet-50 模型中的标签数量</title>
      <link>https://stackoverflow.com/questions/78323867/number-of-labels-in-facebook-detr-resnet-50-model</link>
      <description><![CDATA[我正准备在自定义数据集上训练 Facebook ResNet DETR 模型，以检测图像中的签名（我的数据集中只有 1 个类）。我不确定分配给模型配置中的 num_labels 参数的适当值。根据上下文，该值是否应该设置为 1（因为我只检测签名），或者我应该为没有任何签名的情况添加第二个标签？
这是代码
model = DetrForObjectDetection.from_pretrained(pretrained_model_name_or_path=CHECKPOINT,num_labels=????,ignore_mismatched_sizes=True)]]></description>
      <guid>https://stackoverflow.com/questions/78323867/number-of-labels-in-facebook-detr-resnet-50-model</guid>
      <pubDate>Sun, 14 Apr 2024 13:02:52 GMT</pubDate>
    </item>
    <item>
      <title>线性回归的正规方程</title>
      <link>https://stackoverflow.com/questions/49347878/normal-equation-for-linear-regression</link>
      <description><![CDATA[我有以下 X 和 y 矩阵：

为此，我想使用正规方程方法计算线性回归方程的最佳 θ 值：
theta = inv(X^T * X) * X^T * y
theta 的结果应该是：[188.400,0.3866,-56.128,-92.967,-3.737]
我通过以下方式实现这些步骤：
X=np.matrix([[1,1,1,1],[2104,1416,1534,852],[5,3,3,2],[1,2,2, 1],[45,41,30,36]])
y=np.matrix([460,232,315,178])

XT=np.转置(X)

XTX=XT.点(X)

inv=np.linalg.inv(XTX)

inv_XT=inv.dot(XT)

θ=inv_XT.dot(y)

打印（θ）

但我没有得到想要的结果。相反，它会抛出错误：

&lt;块引用&gt;
  回溯（最近一次调用最后一次）：文件“C:/”，第 19 行，位于
      theta=inv_XT.dot(y) ValueError：形状 (4,5) 和 (1,4) 未对齐：5 (dim 1) != 1 (dim 0)

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/49347878/normal-equation-for-linear-regression</guid>
      <pubDate>Sun, 18 Mar 2018 12:28:38 GMT</pubDate>
    </item>
    </channel>
</rss>