<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Feb 2025 21:16:25 GMT</lastBuildDate>
    <item>
      <title>XGboost 在不同的 tokenizer 上有不同的准确率</title>
      <link>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</link>
      <description><![CDATA[我有一个transformer bodomerka/Mil_class_exp_sber_balanssedclass，我在sberbank-ai/ruBert-base的基础上对其进行了训练。额外训练的本质是，该模型可以对用俄语写的文本进行分类，并分类是否是军事经验（0或1）。
而且我还想训练Xgboost模型。我用transformer中的tokenizer对文本进行了token化。
最初，当我使用sberbank-ai/ruBert-base tokenizer时，准确率为0.86。但是，当我将其更改为bodomerka/Mil_class_exp_sber_balanssedclass时，准确率上升到了0.96。这是为什么呢？]]></description>
      <guid>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</guid>
      <pubDate>Wed, 05 Feb 2025 20:12:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习数据集预处理问题</title>
      <link>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</link>
      <description><![CDATA[我有这个数据集，它是一个多类分类问题，y_train 高度不平衡。我想对此应用 smote，但它会抛出 NaN 和无穷大值错误。我尝试了 smote-variants 库下可用的所有 smote 技术，但没有成功
我计划在基于 IoT 的生产环境中部署它，所以我不想有任何偏差
任何建议或潜在解决方案都很好
这是标签编码后 y_train 的值计数
当然！以下是相同格式的标签分布，但采用 markdown 格式：
平衡和编码后的标签分布：
4 617
12 432
11 391
6 357
10 353
7 336
19 299
9 290
18 235
17 180
20 86
0 77
22 72
21 63
5 44
27 31
23 30
2 23
13 22
15 13
24 9
25 5
16 4
3 3
8 3
26 2
28 1
1 1
14 1
名称：count，dtype：int64

这是使用 mix-max 缩放数据集后的结果，我已估算缺失值值。
在应用任何 smote 之前没有 NaN 或 Infinity 值，在应用时，它会抛出此错误。
我添加了例外以删除 NaN 和无限值，但现在 smote 技术不起作用，任何可能起作用的技术建议
或者我应该只创建较小值数据的副本
例如这里的 14 只是 1，我会多次复制它，那么它会更正确，我不知道这是否是一种正确的方法]]></description>
      <guid>https://stackoverflow.com/questions/79415905/issue-with-pre-processing-machine-learning-dataset</guid>
      <pubDate>Wed, 05 Feb 2025 19:00:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Java 从 Android 中的扫描文档中检测实心圆圈（单选按钮）？</title>
      <link>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</link>
      <description><![CDATA[我正在开发一款使用 GmsDocumentScanner 扫描纸质文档的 Android 应用。我的目标是检测哪些圆圈被填充，类似于表单上的单选按钮。
圆圈预先印在纸上，用户填充其中一个圆圈来标记他们的选择。扫描文档后，我需要检测标记的圆圈以及与之相关的文本或图标。为了便于理解，我在这里分享了一个片段。
此片段清晰地显示了扫描的纸张，应进一步处理以检测圆圈中的颜色标记。
我尝试了这些步骤

使用 GmsDocumentScanning 扫描文档
尝试使用像素强度分析检测填充的圆圈

private void understandText(Bitmap bitmap) {
InputImage image = InputImage.fromBitmap(bitmap, 0);
TextRecognizer understander = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS);

识别器.处理（图像）
.addOnSuccessListener（结果 -&gt; {
for（Text.TextBlock block : result.getTextBlocks()) {
detectRadioButtons（block, bitmap);
}
})
.addOnFailureListener（e -&gt; Log.e（&quot;错误&quot;, &quot;文本识别失败&quot;, e));
}


private void detectRadioButtons（Text.TextBlock textBlock, Bitmap bitmap）{
for（Text.Line line : textBlock.getLines()) {
Rect boundingBox = line.getBoundingBox();
if (isRadioButtonChecked（boundingBox, bitmap)) {
Log.i（&quot;检测&quot;, &quot;选中的单选按钮：&quot; + line.getText());
}
}
}


private boolean isRadioButtonChecked(Rect rect, Bitmap bitmap) {
int checkedPixelThreshold = 100;
int checkedPixels = 0;
int totalPixels = rect.width() * rect.height();

for (int x = rect.left; x &lt; rect.right; x++) {
for (int y = rect.top; y &lt; rect.bottom; y++) {
int pixel = bitmap.getPixel(x, y);
if (Color.red(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.green(pixel) &lt; checkedPixelThreshold &amp;&amp;
Color.blue(pixel) &lt; checkedPixelThreshold) {
checkedPixels++;
}
}
}
return checkedPixels &gt; (totalPixels * 0.5); // 超过 50% 的像素已填充
}


预期结果是
检测扫描纸上的圆圈
识别哪个圆圈已填充
将填充的圆圈与正确的文本标签或图标关联]]></description>
      <guid>https://stackoverflow.com/questions/79414791/how-to-detect-filled-circles-radio-buttons-from-a-scanned-document-in-android</guid>
      <pubDate>Wed, 05 Feb 2025 12:44:42 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 Keras、Tensorflow 实现重现性？</title>
      <link>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</link>
      <description><![CDATA[每次运行以下代码时，我获得的准确率和损失都不一样。我按照之前帖子中的说明操作，但无法解决。问题可能出在哪里？
import os
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = &quot;1&quot;
os.environ[&quot;TF_CUDNN_DETERMINISTIC&quot;] = &quot;1&quot;
...
...

SEED=65
tf.keras.utils.set_random_seed(SEED) 
tf.config.experimental.enable_op_determinism()
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

训练，测试，训练目标，测试目标 = train_test_split((df.loc[:,&quot;input_alarm_1&quot;:&quot;input_alarm_&quot;+str(num_backtracking_events)]), df.loc[:,&quot;output_failure&quot;], test_size=0.25, random_state=SEED)

训练 = np.asarray(training).astype(&#39;float32&#39;)
训练目标 = np.asarray(trainingtarget).astype(&#39;float32&#39;)
test = np.asarray(test).astype(&#39;float32&#39;)
testtarget = np.asarray(testtarget).astype(&#39;float32&#39;)

initializer = tf.keras.initializers.GlorotUniform(seed=SEED)

model = keras.Sequential(
[
layer.Dense(600, 激活=&quot;relu&quot;, input_shape=(num_backtracking_events,), kernel_initializer=initializer),
layer.Dense(300, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(100, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(1, 激活=&quot;sigmoid&quot;, kernel_initializer=initializer),
#dropout?
]
)

#编译模型
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

#fit
history = model.fit(training, trainingtarget, batch_size=10, epochs=50, validation_split=0.1)

# 评估 keras 模型
test_loss, test_acc = model.evaluate(test, testtarget)
print(&#39;Accuracy: %.2f&#39; % (test_acc*100))
print(&#39;Loss: %.2f&#39; % (test_loss*100))
]]></description>
      <guid>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</guid>
      <pubDate>Wed, 05 Feb 2025 08:09:02 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>使用模型蒸馏来优化我的模型</title>
      <link>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</link>
      <description><![CDATA[我有一个 NN 模型，用于学习端到端通信系统。它是一个自动编码器，其中编码器充当发射器；它采用 8 位并将其编码为 IQ 值，解码器充当接收器；它采用生成的 IQ 值并将其解码为 8 位。我还有一个通道模型，可以模拟噪声、频率/相位偏移等。
该模型经过训练，具有非常好的误码率 (BER)，但在进行推理时具有高延迟，因此我需要对其进行优化。我正在尝试遵循 pytorch 的知识提炼教程，但到目前为止，我无法让我的学生有效地学习。
我认为我的问题在于我的软损失函数不正确。在原始训练循环中，我使用 BinaryCrossEntropy 损失来对抗模型的预测位概率与真实输入位。从文档中可以看出，K.D 似乎包含了一个额外的损失，即采用学生和家长概率的 KL 散度损失。但是，运行代码时我的损失并没有改善。
我感到困惑的是，我的“软损失”应该是什么类型的损失函数，以及它应该获得什么输入类型（logit 或概率）。我尝试了不同的排列（将对数概率输入 KL Div，使用 CrossEntropy 损失而不是 KL，即文档中显示的损失函数），但它们都没有以任何方式提高我的学生模型的性能。
这大致是我正在使用的代码。它不是完整的代码；我只展示了父自动编码器和 K.D 循环，但这足以表达我的观点。
import torch
import torch.nn as nn
import torch.optim as optim

# 定义编码器
class Encoder(nn.Module):
def __init__(self):
super(Encoder, self).__init__()
self.fc1 = nn.Linear(8, 16) # 扩展特征空间
self.relu = nn.ReLU()
self.fc2 = nn.Linear(16, 10) # 输出 2 个值（IQ 表示）

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x) # 输出原始 IQ 符号
return x

# 定义解码器
class Decoder(nn.Module):
def __init__(self):
super(Decoder, self).__init__()
self.fc1 = nn.Linear(100, 50) # 从 IQ 扩展回来
self.fc2 = nn.Linear(50, 30)
self.fc3 = nn.Linear(30, 16)
self.fc4 = nn.Linear(16, 8) # 输出 8 位恢复序列
self.relu = nn.ReLU()
self.sigmoid = nn.Sigmoid() # 确保输出在 (0,1) 范围内

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x)
x = self.relu(x)
x = self.fc3(x)
x = self.relu(x)
x = self.fc4(x)
x = self.sigmoid() # 解释为概率
return x

#定义自动编码器（编码器 -&gt; 通道 -&gt;解码器)
class Autoencoder(nn.Module):
def __init__(self, noise_std=0.1):
super(Autoencoder, self).__init__()
self.encoder = Encoder()
self.decoder = Decoder()

def forward(self, x):
x = self.encoder(x) # 将 8 位编码为 2 个 IQ 符号
x = self.decoder(x) # 解码回 8 位序列
return x

ParentModel = Autoencoder(noise_std=0.1)

# 加载预训练权重
load_weights(model, path, optimizer)

def knowledge_distillation(teacher, student, T, epochs, batches, alpha):
ce_loss = nn.BCELoss()
kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)
optimizer = optim.Adam(student.parameters(), lr = 1e-4)

teacher.eval() # 教师设置为评估模式
student.train() # 学生设置为训练模式

for epoch in range(epochs):
input_bits = generate_binary_tensor(8, batches) # 生成 [8, batch] 二进制张量

optimizer.zero_grad()

with torch.no_grad():
teacher_predictions = teacher(input_bits) # 教师前向传递

student_predictions = student(input_bits) # 学生前向传递

# 计算硬损失
hard_loss = ce_loss(student_predictions, input_bits)

# 计算软损失（不确定这部分）
soft_loss = kl_loss(student_predictions, teacher_predictions) * (T**2)

total_loss = alpha*soft_loss + (1-alpha)*hard_loss

total_loss.backward()
optimizer.step()

# 存储 BER

]]></description>
      <guid>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</guid>
      <pubDate>Wed, 05 Feb 2025 00:55:20 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 的 seq2seq 教程解码器</title>
      <link>https://stackoverflow.com/questions/79413251/pytorchs-seq2seq-tutorial-decoder</link>
      <description><![CDATA[我正在通过 PyTorch 的 seq2seq 教程进行学习：https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
我对解码器有疑问
class DecoderRNN(nn.Module):
def __init__(self, hidden_​​size, output_size):
super(DecoderRNN, self).__init__()
self.embedding = nn.Embedding(output_size, hidden_​​size)
self.gru = nn.GRU(hidden_​​size, hidden_​​size, batch_first=True)
self.out = nn.Linear(hidden_​​size, output_size)

def forward(self,编码器输出，编码器隐藏，目标张量=无):
batch_size = 编码器输出.size(0)
解码器输入 = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)
解码器隐藏 = 编码器隐藏
解码器输出 = []

for i in range(MAX_LENGTH):
解码器输出，解码器隐藏 = self.forward_step(解码器输出，解码器隐藏)
解码器输出.append(解码器输出)

if target_tensor is not None:
# 教师强制：将目标作为下一个输入
解码器输入 = target_tensor[:, i].unsqueeze(1) # 教师强制
否则：
# 不使用教师强制：使用其自己的预测作为下一个输入
_, topi = 解码器输出.topk(1)
解码器输入 = topi.squeeze(-1).detach() # 从历史中分离作为输入

decoder_outputs = torch.cat(decoder_outputs, dim=1)
decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
returncoder_outputs,coder_hidden, None # 我们返回 `None` 以在训练循环中保持一致性


为什么如果 target_tensor 不是 None：
decoder_input = target_tensor[:, i].unsqueeze(1)

但是如果 target_tensor 是 None：
_, topi =coder_output.topk(1)
decoder_input = topi.squeeze(-1).detach()

具体来说，形状不是两种情况下的解码器输入是否不同？
我觉得在第一种情况下，解码器输入的形状是 2D 张量，但在第二种情况下是 1D。]]></description>
      <guid>https://stackoverflow.com/questions/79413251/pytorchs-seq2seq-tutorial-decoder</guid>
      <pubDate>Tue, 04 Feb 2025 23:01:44 GMT</pubDate>
    </item>
    <item>
      <title>VotingClassifier 仅支持二分类或多分类。不支持多标签和多输出分类</title>
      <link>https://stackoverflow.com/questions/79411556/votingclassifier-only-supports-binary-or-multiclass-classification-multilabel-a</link>
      <description><![CDATA[我收到有关 KerasClassifer 和 VotingClassifer 的错误。
首先，我使用 MNIST 数据集并将其拆分，然后通过 Voting 分类器拟合 xtrain 和 ytrain。如果我将它们的估计器类型放入来自不同模型的分类器中，我得到
&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;

# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

model1._estimator_type = &quot;classifier&quot;
model2._estimator_type = &quot;classifier&quot;
model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

但是当我注释掉 _estimator_type 时，我得到了 ** VotingClassifier 仅支持二分类或多分类。不支持多标签和多输出分类。 **
# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

# model1._estimator_type = &quot;classifier&quot;
# model2._estimator_type = &quot;classifier&quot;
# model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

我也尝试了 Pipeline 和 skl2onnx，但得到了相同的结果。
这是我的所有代码
import numpy as np
import pandas as pd

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import optimizers
from scikeras.wrappers import KerasClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
来自 sklearn.datasets 导入 load_digits、load_iris、fetch_openml
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.metrics 导入 accuracy_score

导入请求
来自 tensorflow.keras.utils 导入 to_categorical

# 使用 tensorflow.keras.datasets 加载 MNIST 数据集
来自 tensorflow.keras.datasets 导入 mnist

# 下载 MNIST 数据集
url = &#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz&#39;
response = request.get(url)
使用 open(&#39;mnist.npz&#39;, &#39;wb&#39;) 作为 f:
f.write(response.content)

# 加载数据集
使用 np.load(&#39;mnist.npz&#39;) 作为数据：
X_train, y_train = data[&#39;x_train&#39;], data[&#39;y_train&#39;]
X_test, y_test = data[&#39;x_test&#39;], data[&#39;y_test&#39;]

# 标准化数据
X_train = X_train.reshape((X_train.shape[0], -1)) / 255.0
X_test = X_test.reshape((X_test.shape[0], -1)) / 255.0

# 将标签转换为分类（独热编码）
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

def mlp_model()：
model = Sequential()

model.add(Dense(50, input_shape = (784, )))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(50))
model.add(Activation(&#39;sigmoid&#39;))
model.add(Dense(10))
model.add(Activation(&#39;softmax&#39;))

sgd = optimizers.SGD(learning_rate = 0.001, motivation=0.0)
model.compile(optimizer = sgd, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

return model

from scikeras.wrappers import KerasClassifier
from sklearn.ensemble import VotingClassifier

# 重新创建模型以确保兼容性
model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)
model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0)

# model1._estimator_type = &quot;classifier&quot;
# model2._estimator_type = &quot;classifier&quot;
# model3._estimator_type = &quot;classifier&quot;

ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;)

# 拟合集成分类器
ensemble_clf.fit(X_train, y_train)

y_pred = ensemble_clf.predict(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79411556/votingclassifier-only-supports-binary-or-multiclass-classification-multilabel-a</guid>
      <pubDate>Tue, 04 Feb 2025 11:40:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有分类头的 TensorFlow 主干模型？</title>
      <link>https://stackoverflow.com/questions/79411501/how-to-use-a-tensorflow-backbone-model-with-a-classification-head</link>
      <description><![CDATA[我尝试使用 tensorflow-models 库中的 ResNet3D，但在尝试运行该块时出现这个奇怪的错误
!pip install tf-models-official==2.17.0

Kaggle 笔记本上的 Tensorflow 版本是 2.18。
安装 tf-models-official 后
从 tensorflow.keras.callbacks 导入 EarlyStopping、ReduceLROnPlateau
从 tensorflow.keras.models 导入 Model
从 tensorflow.keras.layers 导入 Dense、GlobalAveragePooling3D、Input
从 tensorflow.keras.optimizers 导入 AdamW
导入 tensorflow_models 作为 tfm

def create_model():
base_model = tfm.vision.backbones.ResNet3D(model_id = 50,
temporary_strides= [3,3,3,3],
temporary_kernel_sizes = [(5,5,5),(5,5,5,5),(5,5,5,5,5,5),(5,5,5)],
input_specs=tf.keras.layers.InputSpec(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
)

# 取消冻结基础模型层
base_model.trainable = True

# 创建模型
input = Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
x = base_model(inputs) # B,1,7,7,2048
x = GlobalAveragePooling3D(data_format=&quot;channels_last&quot;, keepdims=False)(x)
x = Dense(1024,activation=&#39;relu&#39;)(x)
x = tf.keras.layers.Dropout(0.3)(x) # 添加 dropout 以防止过度拟合
outputs = Dense(NUM_CLASSES,activation=&#39;softmax&#39;)(x)

model = Model(inputs,outputs)

# 使用类权重编译模型
optimizer = AdamW(learning_rate=1e-4,weight_decay=1e-5)
model.compile(
optimizer=optimizer,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,tf.keras.metrics.AUC()]
)

返回模型

# 创建并显示模型
model = create_model()
model.summary()

当我运行此程序时，我收到错误以下：
---------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-56-363271b4dda8&gt; in &lt;cell line: 39&gt;()
37 
38 # 创建并显示模型
---&gt; 39 model = create_model()
40 model.summary()

&lt;ipython-input-56-363271b4dda8&gt; in create_model()
18 # 创建模型
19 input = Input(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
---&gt; 20 x = base_model(输入) # B,1,7,7,2048

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py 在 __call__(self, *args, **kwargs) 中
586 layout_map_lib._map_subclass_model_variable(self, self._layout_map)
587 
--&gt; 588 返回 super().__call__(*args, **kwargs)

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/base_layer.py 在 __call__(self, *args, **kwargs) 中
1101 training=training_mode,
1102 ):
-&gt; 1103 input_spec.assert_input_compatibility(
1104 self.input_spec, input, self.name
1105 )

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in assert_input_compatibility(input_spec, input, layer_name)
300 &quot;与层不兼容：&quot;
301 f&quot;预期形状={spec.shape}，&quot;
--&gt; 302 f&quot;发现形状={display_shape(x.shape)}&quot;
303 )
304 

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py 在 display_shape(shape) 中
305 
306 def display_shape(shape):
--&gt; 307 return str(tuple(shape.as_list()))
308 
309 

AttributeError: &#39;tuple&#39; 对象没有属性 &#39;as_list&#39;

我尝试将输入作为列表传递给 shape 参数，但仍然出现相同的错误。
错误发生在此
!pip install tf-models-official==2.17.0

import tensorflow as tf

inputs = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
print(inputs.shape.as_list())

错误：
-------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用last)
&lt;ipython-input-39-6e88680ff7df&gt; in &lt;cell line: 2&gt;()
1 输入 = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
----&gt; 2 打印 (inputs.shape.as_list())

AttributeError: &#39;tuple&#39; 对象没有属性 &#39;as_list&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79411501/how-to-use-a-tensorflow-backbone-model-with-a-classification-head</guid>
      <pubDate>Tue, 04 Feb 2025 11:22:41 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型在二元分类中仅预测一个类（猫）[关闭]</title>
      <link>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</link>
      <description><![CDATA[我使用 TensorFlow/Keras 训练了一个二元分类 CNN，以区分猫和狗。然而，在对测试数据集进行评估时，该模型只为每张图片预测“猫”，尽管数据集包含这两个类别。
这是我的代码：
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import load_model

def normalizer(image, label):
aux = tf.cast(image, dtype=tf.float32)
image_norm = aux/255.0
return image_norm, label

train_data, valid_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/training&#39;,
validation_split=0.1, 
subset=&quot;both&quot;, 
seed=42, 
image_size=(150, 150), 
batch_size=32 
)

test_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/test&#39;, 
image_size=(150, 150), 
batch_size=32 
)

train = train_data.map(normalizer)
valid = valid_data.map(normalizer) 
test = test_data.map(normalizer)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),activation=&#39;relu&#39;, input_shape=(150,150,3)))
model.add(MaxPooling2D())

model.add(Conv2D(filters=64, kernel_size=(3,3),激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Flatten())

model.add(Dense(units=256, 激活=&#39;relu&#39;))
model.add(Dense(units=1, 激活=&#39;sigmoid&#39;))

model.compile(
optimizer=&#39;adam&#39;,
loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[&#39;accuracy&#39;],
)

print(model.summary())

hist = model.fit(
训练，
batch_size=32， 
epochs=20， 
shuffle=True，
validation_data=valid
)

plt.plot(hist.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_loss&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.title(&#39;训练和验证中的损失&#39;)
plt.show()

如果 hist.history 中有 &#39;accuracy&#39;: 
plt.plot(hist.history[&#39;accuracy&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_accuracy&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.legend()
plt.title(&#39;训练和验证的准确性&#39;)
plt.show()

model.save(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

new_model = load_model(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

loss, acc = new_model.evaluate(test, batch_size=32)

print(loss)
print(acc)

y_pred = new_model.predict(test) 

y_true = np.concatenate([y.numpy() for x, y in test], axis=0)

matrix = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(
matrix.numpy(), 
annot=True, 
fmt=&#39;d&#39;, 
cmap=&#39;Blues&#39;, 
xticklabels=[&#39;Cat&#39;, &#39;Dog&#39;], 
yticklabels=[&#39;Cat&#39;, &#39;Dog&#39;],
)

plt.ylabel(&#39;True Label&#39;)
plt.xlabel(&#39;Predicted Label&#39;)
plt.title(&#39;Confusion Matrix&#39;)
plt.show()

这是混淆矩阵
混淆矩阵
我怀疑的可能原因

类别不平衡 –&gt; 我的训练数据中猫和狗的数量大致相等，所以我不认为这是原因。
标签问题 –&gt;我检查并确认 y_true 既有 0 也有 1，所以标签应该没问题。
注：该模型在验证中的准确率达到了约 70%
]]></description>
      <guid>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</guid>
      <pubDate>Mon, 03 Feb 2025 20:03:37 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 中的标准和评分有什么区别</title>
      <link>https://stackoverflow.com/questions/64675820/what-is-difference-between-criterion-and-scoring-in-gridsearchcv</link>
      <description><![CDATA[我创建了一个 GradientBoostingRegressor 模型。
我在 GridSearchCV 函数中使用 scoring 参数来返回 MSE 分数。
我想知道如果我在 param_grids 中使用 criterion 是否会改变我的模型？哪种方法才是正确的？
GBR = GradientBoostingRegressor()
param_grids = {
&#39;learning_rate&#39; : [0.01, 0.05, 0.07, 0.1, 0.3, 0.5 ],
&#39;n_estimators&#39; : [50,60,70,80,90,100],
&#39;max_depth&#39; : [1, 2, 3, 4],
&#39;min_samples_leaf&#39; : [1,2,3,5,10,15],
&#39;min_samples_split&#39;: [2,3,4,5,10], 
#&#39;criterion&#39; : [&#39;mse&#39;]
}

kf = KFold(n_splits=3, random_state=42, shuffle=True)
gs = GridSearchCV(estimator=GBR, param_grid = param_grids , cv = kf, n_jobs=-1, 
return_train_score=True, 评分=&#39;neg_mean_squared_error&#39;) 
]]></description>
      <guid>https://stackoverflow.com/questions/64675820/what-is-difference-between-criterion-and-scoring-in-gridsearchcv</guid>
      <pubDate>Wed, 04 Nov 2020 07:35:06 GMT</pubDate>
    </item>
    <item>
      <title>回归模型中成本函数用L1范数代替L2范数</title>
      <link>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</link>
      <description><![CDATA[我想知道 Python 中是否有一个函数可以完成与 scipy.linalg.lstsq 相同的工作，但使用“最小绝对偏差”回归而不是“最小二乘”回归 (OLS)。我想使用 L1 范数，而不是 L2 范数。
事实上，我有 3d 点，我想要它们的最佳拟合平面。常见的方法是通过最小二乘法，如 Github 链接。但众所周知，这并不总是能给出最佳拟合，尤其是当我们的数据集中有闯入者时。最好计算最小绝对偏差。 此处 更详细地解释了这两种方法之间的区别。
由于它是 Ax = b 矩阵方程，需要循环来最小化结果，因此无法通过 MAD 等函数解决。我想知道是否有人知道 Python 中的相关函数（可能是线性代数包中）可以计算“最小绝对偏差”回归？]]></description>
      <guid>https://stackoverflow.com/questions/51883058/l1-norm-instead-of-l2-norm-for-cost-function-in-regression-model</guid>
      <pubDate>Thu, 16 Aug 2018 18:12:02 GMT</pubDate>
    </item>
    <item>
      <title>cross_val_score 和 gridsearchCV 如何工作？</title>
      <link>https://stackoverflow.com/questions/50629219/how-do-cross-val-score-and-gridsearchcv-work</link>
      <description><![CDATA[我一直在尝试弄清楚 gridsearchCV 和 cross_val_score 是如何工作的。
寻找赔率结果可以建立一种验证实验，但我仍然不明白我做错了什么。
为了简化，我使用 gridsearchCV 是最简单的方法，并尝试验证和理解正在发生的事情：
在这里：
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer
from sklearn.feature_selection import SelectKBest, f_regression, RFECV
from sklearn.decomposition import PCA
from sklearn.linear_model import RidgeCV,Ridge, LinearRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV、KFold、TimeSeriesSplit、PredefinedSplit、cross_val_score
来自 sklearn.metrics 导入 mean_squared_error、make_scorer、r2_score、mean_absolute_error、mean_squared_error
来自 math 导入 sqrt

我创建了一个交叉验证对象（用于 gridsearchCV 和 cross_val_score）和一个用于管道和简单线性回归的训练/测试数据集。我已经检查过这两个数据集是相同的：
train_indices = np.full((15,), -1, dtype=int)
test_indices = np.full((6,), 0, dtype=int)
test_fold = np.append(train_indices, test_indices)
kf = PredefinedSplit(test_fold)

for train_index, test_index in kf.split(X):
print(&#39;TRAIN:&#39;, train_index, &#39;TEST:&#39;, test_index)
X_train_kf = X[train_index]
X_test_kf = X[test_index]

train_data = list(range(0,15))
test_data = list(range(15,21))

X_train, y_train=X[train_data,:],y[train_data]
X_test, y_test=X[test_data,:],y[test_data]

我的做法如下：
实例化一个简单的线性模型并将其与手动数据集一起使用
lr=LinearRegression()
lm=lr.fit(X,y)
lmscore_train=lm.score(X_train,y_train) 

结果：
r2=0.4686662249071524

lmscore_test=lm.score(X_test,y_test)

结果：
r2 0.6264021467338086

现在我尝试使用管道：
pipe_steps = ([(&#39;est&#39;, LinearRegression())])
pipe=Pipeline(pipe_steps)
p=pipe.fit(X,y)
pscore_train=p.score(X_train,y_train) 

结果：
r2=0.4686662249071524

pscore_test=p.score(X_test,y_test)

结果：
r2 0.6264021467338086

LinearRegression 和管道完美匹配
现在我尝试使用 cross_val_score 和预定义的分割来执行相同的操作kf
cv_scores = cross_val_score(lm, X, y, cv=kf) 

结果：
r2 = -1.234474757883921470e+01 # ?!?! （这应该是测试分数）

现在让我们尝试 gridsearchCV
scoring = {&#39;r_squared&#39;:&#39;r2&#39;}
grid_parameters = [{}] 
gridsearch=GridSearchCV(p, grid_parameters, verbose=3,cv=kf,scoring=scoring,return_train_score=&#39;true&#39;,refit=&#39;r_squared&#39;)
gs=gridsearch.fit(X,y)
results=gs.cv_results_

从 cv_results_ 我再次得到
mean_test_r_squared
# result
r2 -1.234474757883921292e+01

所以 cross_val_score 和 gridsearch 最终匹配一个另一个，但分数完全不对，与应有的分数不同。
你能帮我解开这个谜题吗？]]></description>
      <guid>https://stackoverflow.com/questions/50629219/how-do-cross-val-score-and-gridsearchcv-work</guid>
      <pubDate>Thu, 31 May 2018 16:50:39 GMT</pubDate>
    </item>
    <item>
      <title>随机森林过度拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/33948946/random-forest-is-overfitting</link>
      <description><![CDATA[我正在使用带有分层 CV 的 scikit-learn 来比较一些分类器。
我正在计算：准确率、召回率、auc。
我使用带有 5 个 CV 的 GridSearchCV 进行参数优化。
RandomForestClassifier(warm_start= True, min_samples_leaf= 1, n_estimators= 800, min_samples_split= 5,max_features= &#39;log2&#39;, max_depth= 400, class_weight=None)

是来自 GridSearchCV 的 best_params。
我的问题是，我认为我真的过度拟合了。例如：

带标准差 (+/-) 的随机森林

精度：0.99 (+/- 0.06) 
敏感度：0.94 (+/- 0.06) 
特异性：0.94 (+/- 0.06) 
B_accuracy：0.94 (+/- 0.06)
AUC：0.94 (+/- 0.11)

带标准差 (+/-) 的逻辑回归

精度：0.88 (+/- 0.06) 
敏感度：0.79 (+/- 0.06) 
特异性： 0.68 (+/- 0.06) 
B_accuracy: 0.73 (+/- 0.06)
AUC: 0.73 (+/- 0.041)


其他的也看起来像逻辑回归（所以它们看起来没有过度拟合）。
我的 CV 代码是：
for i,j in enumerate(data):
X.append(data[i][0])
y.append(float(data[i][1]))
x=np.array(X)
y=np.array(y)

def SD(values):

mean=sum(values)/len(values)
a=[]
for i in range(len(values)):
a.append((values[i]-mean)**2)
erg=sum(a)/len(values)
SD=math.sqrt(erg)
return SD,mean

for name, clf in zip(titles,classifiers):
# 遍历所有分类器，计算 10 次折叠
# 下一个 for 循环应该再缩进 1 个制表符，抱歉，这里无法格式化
pre,sen,spe,ba,area=[],[],[],[],[]
for train_index, test_index in skf:
#print train_index, test_index
# 从所有 train_index 和 test_index 中获取索引
# 由于某些错误，将它们更改为列表
train=train_index.tolist()
test=test_index.tolist()
X_train=[]
X_test=[]
y_train=[]
y_test=[]
for i in train:
X_train.append(x[i])

for i in test:
X_test.append(x[i]) 

for i in train:
y_train.append(y[i])

for i in test:
y_test.append(y[i]) 

#clf=clf.fit(X_train,y_train)
#predicted=clf.predict_proba(X_test)
#... 其他代码，计算指标等等...
print name 
print(&quot;precision: %0.2f \t(+/- %0.2f)&quot; % (SD(pre)[1], SD(pre)[0]))
print(&quot;sensitivity: %0.2f \t(+/- %0.2f)&quot; % (SD(sen)[1], SD(pre)[0]))
print(&quot;specificity: %0.2f \t(+/- %0.2f)&quot; % (SD(spe)[1], SD(pre)[0]))
print(&quot;B_accuracy: %0.2f \t(+/- %0.2f)&quot; % (SD(ba)[1], SD(pre)[0]))
print(&quot;AUC: %0.2f \t(+/- %0.2f)&quot; % (SD(area)[1], SD(area)[0]))
print &quot;\n&quot;

如果我使用 scores = cross_validation.cross_val_score(clf, X, y, cv=10,scoring=&#39;accuracy&#39;) 方法，我不会得到这个“过度拟合”值。所以也许我使用的 CV 方法有问题？但它仅适用于 RF...
由于 cross_val_function 中特异性得分函数的滞后，我自己做了。]]></description>
      <guid>https://stackoverflow.com/questions/33948946/random-forest-is-overfitting</guid>
      <pubDate>Fri, 27 Nov 2015 00:55:00 GMT</pubDate>
    </item>
    <item>
      <title>在 sci-kit learn 中对随机森林分类器进行故障排除</title>
      <link>https://stackoverflow.com/questions/21963486/troubleshooting-random-forests-classifier-in-sci-kit-learn</link>
      <description><![CDATA[我尝试运行来自 sci-kit learn 的随机森林分类器，但输出结果可疑，只有不到 1% 的预测是正确的。该模型的表现比偶然性要差得多。我对 Python、ML 和 sci-kit learn（三重打击）还比较陌生，我担心的是缺少一些基本的东西，而不是需要微调参数。我希望有更多经验丰富的人来查看代码，看看设置是否有问题。
我尝试根据单词出现次数预测电子表格中行的类别，因此每行的输入都是一个数组，表示每个单词出现的次数，例如 [1 0 0 2 0 ... 1]。我使用 sci-kit learn 的 CountVectorizer 进行此处理，我向它输入包含每行单词的字符串，它输出单词出现次数数组。如果由于某种原因此输入不合适，则可能是出了问题，但我在网上或文档中没有找到任何表明情况如此的信息。
目前，森林的正确回答率约为 0.5%。使用完全相同的输入和 SGD 分类器可获得接近 80% 的结果，这表明我所做的预处理和矢量化没有问题 - 这是 RF 分类器特有的。我的第一反应是寻找过度拟合，但即使我在训练数据上运行模型，它仍然几乎完全出错。
我尝试过树的数量和训练数据量，但对我来说似乎没有太大变化。我试图仅显示相关代码，但如果有帮助，我可以发布更多代码。第一篇 SO 帖子，因此欢迎大家提出想法和反馈。
#拉入包来为每一行创建单词出现向量
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=1,charset_error=&#39;ignore&#39;)
X_train = vectorizer.fit_transform(train_file)
#转换为密集数组，这是随机森林分类器所需的输入类型
X_train = X_train.todense()

#拉入随机森林分类器并训练数据
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators = 100, compute_importances=True)
clf = clf.fit(X_train, train_targets)

#将测试数据转换为向量格式
testdata = vectorizer.transform(test_file)
testdata = testdata.todense()

#导出
with open(&#39;output.csv&#39;, &#39;wb&#39;) 作为 csvfile:
spamwriter = csv.writer(csvfile)
for item in clf.predict(testdata):
spamwriter.writerow([item])
]]></description>
      <guid>https://stackoverflow.com/questions/21963486/troubleshooting-random-forests-classifier-in-sci-kit-learn</guid>
      <pubDate>Sun, 23 Feb 2014 02:40:28 GMT</pubDate>
    </item>
    </channel>
</rss>