<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Jan 2024 03:15:56 GMT</lastBuildDate>
    <item>
      <title>如何在 kubernetes 中构建机器学习平台 [关闭]</title>
      <link>https://stackoverflow.com/questions/77804053/how-to-build-a-machine-learning-platform-in-kubernetes</link>
      <description><![CDATA[我需要在 Kubernetes 上构建一个自助机器学习平台。该平台适用于数据科学团队，无论他们的用例是机器学习、深度学习还是法学硕士。我想了解如何从 Kubernetes 设计转向 kubeflow、mlflow、spark、kafka 等组件，甚至 GPU/CPU 基线。任何有关此事的线索或阅读都将受到高度赞赏。 kubernetes 将是本地安装，而不是云托管服务。]]></description>
      <guid>https://stackoverflow.com/questions/77804053/how-to-build-a-machine-learning-platform-in-kubernetes</guid>
      <pubDate>Fri, 12 Jan 2024 02:30:35 GMT</pubDate>
    </item>
    <item>
      <title>初始化 VAE 权重</title>
      <link>https://stackoverflow.com/questions/77804014/initializing-vae-weights</link>
      <description><![CDATA[我正在训练遵循以下整体架构的 VAE：

变压器编码器
Mu/Logvar -&gt;重新参数化-&gt;潜在z
变压器解码器

根据典型的 VAE 设置，Mu 和 Logvar 只是两个前馈网络。然而，当我用标准值（例如权重为 0.5，偏差为 0）初始化它们时，我发现模型的初始 KL 损失巨大 - 例如5,000-20,000+。
当然，这个下降得相当快，但模型仍然花费数百个时期将 KL 损失从 300 降至 &lt;50。
一个“解决方法”我发现将权重初始化为低得多的值，并使用学习率预热。但初始化权重非常小：
def init_weights(self, initrange=0.0001) -&gt; &gt;没有任何：
        self.embedding_layer.weight.data.uniform_(-0.5, 0.5)
        
        nn.init.uniform_(self.fc_mu.weight, -initrange, initrange)
        nn.init.uniform_(self.fc_logvar.weight, -initrange, initrange)
        nn.init.zeros_(self.fc_mu.bias)
        nn.init.zeros_(self.fc_logvar.bias)

这样做的结果是一个更加稳定的 KL 损失（开始时约为 1.5），但我担心它会阻止我的解码器学习有意义的表示。实际的重建损失因此受到巨大影响。
所以我的问题是：这是 VAE 的已知问题吗？有什么我可以尝试的特定初始化技巧吗？或者也许我应该从正常值开始，让模型训练的时间明显更长？
提前非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/77804014/initializing-vae-weights</guid>
      <pubDate>Fri, 12 Jan 2024 02:14:48 GMT</pubDate>
    </item>
    <item>
      <title>如何预测非线性关系[关闭]</title>
      <link>https://stackoverflow.com/questions/77803595/how-to-predict-nonlinear-relationship</link>
      <description><![CDATA[我有亚马逊评论数据，想要制作根据评论预测星星数量的模型，因此我使用情感分析来获取四列neg neu pos compund，之后我尝试了这样许多算法来预测评级 (1-5)，但我的准确度仍然较差 (40-60%)
这就是原始数据
(https://i.stack.imgur.com/ispqP.png)&lt; /p&gt;
继维德斯之后
(https://i.stack.imgur.com/oWV2H.png)&lt; /p&gt;
我使用了 SVR Dessicon Tree KNN 等，但到目前为止我仍然没有得到最好的结果]]></description>
      <guid>https://stackoverflow.com/questions/77803595/how-to-predict-nonlinear-relationship</guid>
      <pubDate>Thu, 11 Jan 2024 23:34:16 GMT</pubDate>
    </item>
    <item>
      <title>解释这个学习曲线[关闭]</title>
      <link>https://stackoverflow.com/questions/77803095/interpreting-this-learning-curve</link>
      <description><![CDATA[我一直在为我的简历开发一个基本的情感分析项目，并在这里为我的神经网络模型绘制了一条学习曲线。我是否正确绘制了学习曲线？这个具体的曲线告诉了我什么？
就上下文而言，我的训练准确度是 0.758
测试精度：.731
CV 准确度：.715

我无法判断这是否表明过度拟合，或者更多的训练数据将是有益的。]]></description>
      <guid>https://stackoverflow.com/questions/77803095/interpreting-this-learning-curve</guid>
      <pubDate>Thu, 11 Jan 2024 21:20:04 GMT</pubDate>
    </item>
    <item>
      <title>随机森林模型精度低[已迁移]</title>
      <link>https://stackoverflow.com/questions/77802745/low-accuracy-in-random-forest-model</link>
      <description><![CDATA[HCV.Egy.Data &lt;- read.csv(“~/Stat/Metaheuristic ML/hepatitis+c+virus+hcv+for+egyptian+患者/HCV-Egy-Data.csv”)
表（HCV.Egy.Data$Baselinehistological.staging）
HCV.Egy.Data$Baselinehistological.staging &lt;- as.factor(HCV.Egy.Data$Baselinehistological.staging)
库（“随机森林”）
库（caTools）
split &lt;-sample.split(HCV.Egy.Data, SplitRatio = 0.7)
分裂
训练 &lt;- 子集(HCV.Egy.Data, split == &quot;TRUE&quot;)
测试 &lt;- 子集(HCV.Egy.Data, split == &quot;FALSE&quot;)
set.seed(120) # 设置种子
classifier_RF = randomForest(x = train[,-c(28,29)],
                             y = train$Baselinehistological.staging,
                             ntree = 500，类型=“分类”）
classifier_RF$类
y_pred = 预测(classifier_RF, newdata = 测试[,-c(28,29)])
fusion_mtx = 表(测试[, 29], y_pred)
混淆_mtx

表(train[,29], classifier_RF$预测)

我的 ML 模型使用许多不同的 ML 方法（随机森林、XGboost、adaboost、LR 等），该模型的准确度较低 (20-30%)。这是运行上述代码的混淆矩阵：
&lt;前&gt;&lt;代码&gt; y_pred
     1 2 3 4
  1 13 22 36 38
  2 21 17 39 33
  3 15 20 38 27
  4 13 16 37 45

我们甚至应用了 SMOTE 和离散化步骤，将准确率提高到只有 33%。有谁知道为什么使用该数据集的论文的准确率在 70-95% 范围内？是不是明显缺少了什么？
例如，这是一篇论文：https://www.researchgate.net/profile/Md-Satu/publication/341987762_Predicting_Infectious_State_of_Hepatitis_​​C_Virus_Affected_Patient%27s_Applying_Machine_Learning_Methods/链接/ 5edcab8a45851529453fc609/预测丙型肝炎感染状态患者-应用机器学习方法.pdf
我们还应用了 8 次 SMOTE，但准确率仅提高到 33%。
这是数据集： https://archive.ics.uci.edu/dataset/503/hepatitis+c+virus+hcv+for+egyptian+患者如果您知道任何修复方法，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77802745/low-accuracy-in-random-forest-model</guid>
      <pubDate>Thu, 11 Jan 2024 20:05:57 GMT</pubDate>
    </item>
    <item>
      <title>使用模式识别、机器学习和清算数据预测资产价格走势[关闭]</title>
      <link>https://stackoverflow.com/questions/77801769/predicting-assests-price-moves-using-pattern-recognition-machine-learing-and-li</link>
      <description><![CDATA[我在加密货币市场进行交易，并使用电报机器人 https://t.me/BinanceLiquidations 进行提醒价格在哪里。机器人的每条消息都包含价格、时间、清算价值和红色（多头）或绿色（空头）。现在问题就在这里。几个月来我一直在观察数据，幸运的是发现了两种模式。此后，它激发了我的兴趣，即使用机器学习可以利用数据寻找模式来开发优势。]]></description>
      <guid>https://stackoverflow.com/questions/77801769/predicting-assests-price-moves-using-pattern-recognition-machine-learing-and-li</guid>
      <pubDate>Thu, 11 Jan 2024 16:49:00 GMT</pubDate>
    </item>
    <item>
      <title>训练随机森林花费的时间太长</title>
      <link>https://stackoverflow.com/questions/77801017/training-random-forest-taking-too-long</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77801017/training-random-forest-taking-too-long</guid>
      <pubDate>Thu, 11 Jan 2024 15:02:53 GMT</pubDate>
    </item>
    <item>
      <title>回归任务中日志转换后的指标解释问题</title>
      <link>https://stackoverflow.com/questions/77797473/issue-with-metrics-interpretation-after-log-transformation-in-regression-task</link>
      <description><![CDATA[我目前正在研究房价预测任务，由于目标变量（价格）的非正态分布，我对它进行了对数转换。我使用 RMSE、MAE 和 MAPE 等指标，并且对于模型训练，我使用了 cross_val_score。
获得预测后，我采用 MAE 和 MAPE 指标的指数将其恢复到原始规模。然而，我遇到了意想不到的小值；两个指标都等于 1。我怀疑这些值不正确。
kf = KFold(n_splits=5, random_state=42, shuffle=True)

def rmse_cv（模型）：
    mse_scorer = make_scorer(mean_squared_error)
    rmse = np.sqrt(cross_val_score(模型, 训练, y_train, 评分=mse_scorer, cv=kf))
    返回均方根误差

def mae_cv（模型）：
    mae_scorer = make_scorer(mean_absolute_error)
    mae = cross_val_score(模型, 训练, y_train, 评分=mae_scorer, cv=kf)
    返回梅

def mape_cv（模型）：
    mape_scorer = make_scorer(mean_absolute_percentage_error)
    mape = cross_val_score(模型, 训练, y_train, 评分=mape_scorer, cv=kf)
    返回马普

lightgbm = LGBMRegressor(num_leaves=6, max_depth=7, random_state=42, n_estimators=500, Objective=&#39;回归&#39;)

rmse = rmse_cv(lightgbm)
mae = mae_cv(lightgbm)
映射 = 映射_cv(lightgbm)
print(&#39;Lightgbm rmse %.4f&#39; % (rmse.mean()))
print(&#39;Lightgbm mae %.4f&#39; % (mae.mean()))
print(&#39;Lightgbm mape %.4f&#39; % (mape.mean()))

Lightgbm rmse 0.1331
Lightgbm mae 0.0874
Lightgbm 映射 0.0073

我希望获得合理且可解释的值，以反映模型在原始规模上的性能。然而，这两个指标都得出了意想不到的小值 1，这似乎不准确。我期望在原始价格范围内能够更有意义地表示模型误差。]]></description>
      <guid>https://stackoverflow.com/questions/77797473/issue-with-metrics-interpretation-after-log-transformation-in-regression-task</guid>
      <pubDate>Thu, 11 Jan 2024 03:13:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 3.12.1 上安装 PyTorch</title>
      <link>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</link>
      <description><![CDATA[我正在安装 DARTS TimeSeries 库 (https: //github.com/unit8co/darts/blob/master/INSTALL.md#enabling-Optional-dependencies），但我遇到了依赖项安装问题。在 DARTS 安装指南中，它说如果我们遇到这个问题，我们必须参考 PyTorch 的官方安装指南，然后尝试再次安装 Darts。然后，当我尝试在 python 3.12.1 上安装 torch 时，我遇到了这个错误：
&lt;块引用&gt;
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版。

如何解决？
我使用 PyCharm 作为 Python 代码编辑器。
我尝试了pip install darts，但它没有安装所有软件包并遇到此错误错误：subprocess-exited-with-error
 用于安装构建依赖项的 pip 子进程未成功运行。
  │ 退出代码：1
  ╰─&gt; 【136行输出】
      正在收集setuptools&gt;=64.0
        从 https://files.pythonhosted.org/packages 获取 setuptools&gt;=64.0 的依赖信息

然后，我尝试使用 pip install torch 安装 torch 并遇到此错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版]]></description>
      <guid>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</guid>
      <pubDate>Wed, 10 Jan 2024 10:16:06 GMT</pubDate>
    </item>
    <item>
      <title>scikit 的 RFECV 类如何计算 cv_results_？</title>
      <link>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</link>
      <description><![CDATA[我对递归特征消除交叉验证的理解： (sklearn.feature_selection.RFECV) 您提供一种算法，该算法在整个数据集上进行训练并创建特征重要性排名使用属性 coef_ 或 feature_importances_。现在包含了所有功能，该算法通过交叉验证进行评估。然后，删除排名底部的特征，并在数据集上重新训练模型并创建新的排名，再次通过交叉验证进行评估。此过程一直持续到除了一个特征之外的所有特征都保留下来（或由 min_features_to_select 指定），并且最终选择的特征数量取决于产生最高 CV 分数的特征。 （来源)
问题：每个特征数量的 CV 分数存储在 rfecv.cv_results_[“mean_test_score”] 中，我在尝试复制时遇到了麻烦这些分数无需使用 scikit 的内置方法。
这是我试图获得 n-1 个特征的分数，其中 n 是特征总数。
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_validate
从 sklearn.feature_selection 导入 RFECV

alg = DecisionTreeClassifier(random_state = 0)
cv_split = 分层KFold(5)
# train 是 pandas 数据框，x_var 和 y_var 都是包含变量字符串的列表
X = 火车[x_var]
y = np.ravel(train[y_var])

alg.fit(X, y)
最低排名特征 = np.argmin(alg.feature_importances_)
x_var.pop(最低排名特征)

one_removed_feature = 火车[x_var]
alg.fit(one_removed_feature, y)
cv_score = cross_validate(alg, one_removed_feature, y, cv=cv_split, 评分=“准确度”)
np.mean(cv_score[“test_score”])

这是提供不同分数的内置方法：
&lt;前&gt;&lt;代码&gt;rfecv = RFECV(
    估计量=alg,
    步骤=1，
    CV=CV_分裂，
    评分=“准确度”，
）

rfecv.fit(X, y)
rfecv.cv_results_[“mean_test_score”][-2]

如何获得内置方法中计算出的准确分数？
我还想提一下，我确实首先尝试了所有 n 个功能，并且我的方法与
rfecv.cv_results_[“mean_test_score”][-1]。]]></description>
      <guid>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</guid>
      <pubDate>Tue, 09 Jan 2024 16:47:46 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于 Transformer 模型，该模型处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率：一个自定义 LR 调度程序，它复制在“Attention is”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[--------------------原来的问题------------------------- --------
如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。

-------------------- 下面是我的解决方案（cuda）--------------------- ------------
谢谢各位的回复。我尝试了一些方法，最终效果如下：
首先，确保已安装 cmake（在 wsl 下）：
sudo apt-get update
sudo apt-get 安装 cmake
须藤 apt-get 安装 g++

那么，
git clone --recursive https://github.com/microsoft/LightGBM
cd光GBM
mkdir 构建
光盘构建
cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..
使-j4

目前，安装尚未链接到任何 conda env。为此，在 vscode 终端（或仍然是 wsl）下，conda 激活一个 env，然后创建一个 jupyter 笔记本进行测试：
确保lib_lightgbm.so位于LightGBM/python-package下，如果没有，则复制到该文件夹​​中。
然后在jupyter笔记本中：
导入系统
将 numpy 导入为 np
sys.path.append(&#39;/mnt/d/lgm-test2/LightGBM/python-package&#39;)
将 lightgbm 导入为 lgb

最后一点是，您可以参考 Jame 的回复，设备需要设置为“cuda”而不是“gpu”。]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>媒体管道是否与深脸一起使用进行人脸识别以获得更好的准确性</title>
      <link>https://stackoverflow.com/questions/77726072/is-media-pipe-is-use-with-deep-face-for-face-recognition-for-better-accuracy</link>
      <description><![CDATA[我使用深脸进行识别，但准确性不好，所以我尝试实现媒体管道，在​​其中提取地标，因此我将其交给深脸以获得更好的准确性。有什么办法可以做到这一点吗？
我从媒体管道中提取特征向量，但如何将其传递到深层脸部？有什么可行的方法吗？
是否使用媒体管道地标进行深度面部识别以提高准确性？]]></description>
      <guid>https://stackoverflow.com/questions/77726072/is-media-pipe-is-use-with-deep-face-for-face-recognition-for-better-accuracy</guid>
      <pubDate>Thu, 28 Dec 2023 09:38:05 GMT</pubDate>
    </item>
    <item>
      <title>如何正确缩放、训练和拟合分类器管道中的数据</title>
      <link>https://stackoverflow.com/questions/74258306/how-to-scale-train-and-fit-data-in-classifier-pipeline-correctly</link>
      <description><![CDATA[我正在尝试扩展我的数据并训练分类器。我当前的数据框如下所示：
col1 col2 col3 类别
---- ---- ---- --------
....

我对分类器管道中的 StandardScaler 如何影响我的数据感到困惑。这是我的主要问题：

Scaler 也会缩放 Y_train 吗？这在机器学习的背景下真的很重要吗？
缩放器会在预测期间自动缩放 X_test 吗？如果没有，我该如何使用之前计算的指标来做到这一点？
我是否遗漏了缩放和拆分方面的一些基本内容？

这些文档有点含糊，所以希望有人能澄清这一点。非常感谢！
目前，我的管道如下所示：
分类器 = Pipeline(steps=[(“scaler”, StandardScaler()), (&#39;svc&#39;, SVC(kernel=“线性”, C=c))])

features = data.loc[:, data.columns != &#39;类别&#39;]
类别 = 数据[&#39;类别&#39;]

X_train, X_test, Y_train, Y_test = train_test_split(特征, 类别, train_size=0.7)

分类器.fit(X_train, Y_train)

分类器.预测(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/74258306/how-to-scale-train-and-fit-data-in-classifier-pipeline-correctly</guid>
      <pubDate>Mon, 31 Oct 2022 02:49:27 GMT</pubDate>
    </item>
    <item>
      <title>在计算测试数据的准确性时遇到此错误</title>
      <link>https://stackoverflow.com/questions/73498671/encountered-this-error-when-calculating-the-accuracy-on-a-test-data</link>
      <description><![CDATA[我是机器学习新手，在计算和返回测试数据的准确性时遇到此错误
def NBAaccuracy(features_train, labels_train, features_test, labels_test):
    ”“”计算朴素贝叶斯分类器“”的准确性
    ### 导入 GaussianNB 的 sklearn 模块
    从 sklearn.naive_bayes 导入 GaussianNB

    ### 创建分类器
    clf = GaussianNB() #TODO

    ### 将分类器拟合到训练特征和标签上
    clf.fit(features_train, labels_train, features_test, labels_test) #TODO

    ### 使用经过训练的分类器来预测测试特征的标签
    pred = clf.predict(features_test, labels_test) #TODO

    ### 计算并返回测试数据的准确性
    ### 这与示例略有不同，
    ### 我们只打印准确性
    ### 你可能需要导入 sklearn 模块
    从 sklearn.metrics 导入 precision_score
    准确度=准确度_分数（features_test，labels_test，normalize = False）#TODO
    返回精度
    返回NBA准确率

我收到此错误：
&lt;块引用&gt;
类型错误：fit() 最多接受 4 个参数（给定 5 个）
]]></description>
      <guid>https://stackoverflow.com/questions/73498671/encountered-this-error-when-calculating-the-accuracy-on-a-test-data</guid>
      <pubDate>Fri, 26 Aug 2022 09:10:51 GMT</pubDate>
    </item>
    </channel>
</rss>