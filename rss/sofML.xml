<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 11 Jul 2024 21:16:31 GMT</lastBuildDate>
    <item>
      <title>Keras API 中的特征重要性图表</title>
      <link>https://stackoverflow.com/questions/78737415/feature-importance-chart-in-keras-api</link>
      <description><![CDATA[我正在实现一系列三个深度学习模型（RNN、1D-CNN 和自定义转换器），全部使用 Keras API，用于 NLP 二元分类问题。我想生成一个特征重要性图表，类似于此处链接的问题中所示的图表：使用 Python 中的 Keras 在神经网络中生成特征重要性图表
我已经运行了模型并得到了结果，但我还想展示特征重要性图表。以下是 1D-CNN 模型的模型架构，我将在本题中使用它作为示例：
from keras import layer
from keras.layers import Input, Embedding, Conv1D, Dropout, GlobalMaxPooling1D, Dense
from keras.optimizers import Adam
from keras.regularizers import L2

def create_model():
embedding_dim = 500

input = Input(shape = (maxlen,))
x = Embedding(input_dim = vocab_size, 
output_dim = embedding_dim, input_shape = (1000,))(input)
x = Conv1D(256, 7, padding = &#39;valid&#39;, activity = &#39;gelu&#39;, strides = 3, kernel_regularizer = L2(0.01))(x)
x = Dropout(0.5)(x)
x = Conv1D(256, 7, padding = &#39;valid&#39;, 激活 = &#39;gelu&#39;, strides = 3, kernel_regularizer = L2(0.01))(x)
x = Dropout(0.5)(x)
x = Conv1D(256, 7, padding = &#39;valid&#39;, 激活 = &#39;gelu&#39;, strides = 3, kernel_regularizer = L2(0.01))(x)
x = GlobalMaxPooling1D()(x)
x = Dense(256, 激活 = &#39;gelu&#39;)(x)
x = Dropout(0.2)(x)
x = Dense(128, 激活 = &#39;gelu&#39;)(x)
x = Dropout(0.2)(x)
x = Dense(64, 激活 = &#39;gelu&#39;)(x)
x = Dropout(0.2)(x)
x = Dense(32，激活 = &#39;gelu&#39;)(x)
x = Dropout(0.2)(x)

class_1 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_1&#39;)(x)
class_2 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_2&#39;)(x)
class_3 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_3&#39;)(x)
class_4 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_4&#39;)(x)
class_5 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_5&#39;)(x)
class_6 = Dense(1，激活 = &#39;sigmoid&#39;，名称 = &#39;class_6&#39;)(x)

opt = Adam(learning_rate = 0.001)

model = keras.Model(
输入 = [输入],
输出 = [class_1, class_2, class_3, class_4, class_5, class_6]
)

model.compile(optimizer = opt,
loss = {&#39;class_1&#39; : &#39;binary_crossentropy&#39;, &#39;class_2&#39; : &#39;binary_crossentropy&#39;, &#39;class_3&#39; : &#39;binary_crossentropy&#39;, &#39;class_4&#39; : &#39;binary_crossentropy&#39;, &#39;class_5&#39; : &#39;binary_crossentropy&#39;, &#39;class_6&#39; : &#39;binary_crossentropy&#39;},
metrics = [&#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;]
)

返回模型

其他 Stack Overflow 上关于此问题的问题由于使用了顺序模型而有效，但我想继续使用 Functional API。这些问题链接在此处：
特征重要性 keras 回归模型
使用 Python 中的 Keras 在神经网络中绘制特征重要性图表
有没有办法用 Keras 获取变量重要性？
使用 keras 确定特征重要性
我曾尝试使用 eli5，但这需要 Keras 包装器来包装顺序模型。SHAP 还要求我重新训练模型，这不是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78737415/feature-importance-chart-in-keras-api</guid>
      <pubDate>Thu, 11 Jul 2024 19:37:33 GMT</pubDate>
    </item>
    <item>
      <title>keras 中数字识别准确率低</title>
      <link>https://stackoverflow.com/questions/78737265/low-accuracy-of-digit-recognition-in-keras</link>
      <description><![CDATA[我有一张电表数字图片，我想识别它。
我已经编写了代码，使用 keras 创建模型，其准确率约为 0.999。
但是当我使用保存的模型识别数字时，即使使用训练样本，有时准确率也非常接近 0.00，所以我不明白我做错了什么，为什么结果的准确率很差？
这是我的数据集（约 25000 张图像）：https://drive.google.com/file/d/1KN_g1ukX0TBIm_552Sqc3HjBXu1Z-A5l/view?usp=sharing
这是我的代码：
import os
import numpy as np
from tensorflow import data as tf_data
import matplotlib.pyplot as plt
import PIL
import tensorflow as tf
from keras import layer
import keras

image_size = (42, 66)
img_height = image_size[0]
img_width = image_size[1]
batch_size = 32

train_ds, val_ds = keras.utils.image_dataset_from_directory(
&quot;D:\\Downloads\\out&quot;,
validation_split=0.2,
subset=&quot;both&quot;,
color_mode=&quot;rgb&quot;,
seed=123,
image_size=image_size,
batch_size=batch_size,
)

class_names = train_ds.class_names

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)

val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

normalization_layer = layer.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]

num_classes = len(class_names)

model = keras.Sequential([
layer.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
layer.Conv2D(32, (3, 3), 激活=&#39;relu&#39;),
layer.MaxPooling2D((2,2)),
layer.Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
layer.MaxPooling2D((2,2)),
layer.Conv2D(64, (3,3), 激活=&#39;relu&#39;),
layer.Flatten(),
layer.Dense(64, 激活=&#39;relu&#39;),
layer.Dense(num_classes)
])

model.compile(optimizer=&#39;adam&#39;,
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[&#39;accuracy&#39;])

epochs=15
history = model.fit(
train_ds,
validation_data=val_ds,
epochs=epochs
)

model.save(&#39;D:\\Downloads\\model.keras&#39;)

################
#测试模型
################

digit = 3
directory = &quot;D:\\Downloads\\out\\&quot; + str(digit) + &quot;\\&quot;
images = []
images = os.listdir(directory)
model = keras.models.load_model(&#39;D:\\Downloads\\model.keras&#39;)

i = 0
acc = 0
for image in images:
i+=1

image_path = directory + image
image = keras.utils.load_img(image_path)
input_arr = keras.utils.img_to_array(image)
input_arr = np.array([input_arr]) # 将单个图像转换为批处理。

predictions = model.predict(input_arr, verbose=&quot;0&quot;)
result = np.argmax(predictions)
acc+=1

if np.argmax(predictions) != digit:
acc-=1

print (acc*100/i)
print (acc)
print (len(images))
]]></description>
      <guid>https://stackoverflow.com/questions/78737265/low-accuracy-of-digit-recognition-in-keras</guid>
      <pubDate>Thu, 11 Jul 2024 18:49:07 GMT</pubDate>
    </item>
    <item>
      <title>USE 等上下文编码器与 OpenAI 的 text-embedding-ada-002 之间有什么区别？</title>
      <link>https://stackoverflow.com/questions/78737222/whats-the-difference-between-contextual-encoders-like-use-and-openais-text-emb</link>
      <description><![CDATA[刚刚遇到了 Universal Sentence Encoder，它也可以保持上下文在语义搜索和其他操作中。OpenAI 的 text-embedding-ada-002 是否更先进，或者或多或少以相同的方式工作但可以创建高维向量？
我尝试使用 Universal Text Encoders，但当我遇到 ada-002 时，我发现它与 USE 类似，唯一的区别是它具有更高的维度。]]></description>
      <guid>https://stackoverflow.com/questions/78737222/whats-the-difference-between-contextual-encoders-like-use-and-openais-text-emb</guid>
      <pubDate>Thu, 11 Jul 2024 18:37:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建支持微调的未训练 AI 模型？</title>
      <link>https://stackoverflow.com/questions/78737208/how-to-create-an-untrained-ai-model-in-python-with-support-for-fine-tuning</link>
      <description><![CDATA[我需要从头开始用 Python 创建一个 AI 模型，未经训练，但希望以后使用我的数据集进行微调。我对 TensorFlow 和 PyTorch 等机器学习框架有一些经验，但不知道如何开始使用可能需要微调的空白模型。
具体来说，我需要以下方面的指导。
如何用 Python 创建一个非常基本的、不可训练的模型。
如何构建模型以支持微调。
加载和预处理我的数据集以进行训练的最佳实践。在我的数据集上训练模型的步骤：。
以下是我迄今为止尝试过的概述：
安装了 TensorFlow 和 PyTorch。此外，还创建了简单的神经网络模型，但它们是预先训练过的。
我将不胜感激任何可以帮助我实现这一目标的示例、代码片段或相关文档的引用。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78737208/how-to-create-an-untrained-ai-model-in-python-with-support-for-fine-tuning</guid>
      <pubDate>Thu, 11 Jul 2024 18:33:05 GMT</pubDate>
    </item>
    <item>
      <title>均方值与 RELU 激活反向传播问题</title>
      <link>https://stackoverflow.com/questions/78737193/mean-square-value-and-relu-activation-backpropagation-issue</link>
      <description><![CDATA[首先，抱歉发布链接，我需要 10 点声誉才能发布图片。
您好，我目前是神经网络的新手，我正在尝试从头开始编写神经网络代码（自己编写）。我正在尝试制作一个这样的神经网络结构：
我的神经网络结构
对我来说很难的问题是反向传播；在我的神经网络中，我使用 RELU 作为激活函数，并使用 MSE 来计算损失，因此我尝试对其进行导数。
这是我的所有导数：
https://i.sstatic.net/GPuO3JNQ.png
我不知道如何计算这些函数，因为它们的维度可能不同
我是否必须计算这个
https://i.sstatic.net/gMtBWRIz.png 在我的所有导数中
请帮助我，这是我第一次在 stackoverflow 上寻求帮助。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78737193/mean-square-value-and-relu-activation-backpropagation-issue</guid>
      <pubDate>Thu, 11 Jul 2024 18:28:12 GMT</pubDate>
    </item>
    <item>
      <title>Keras 调谐器引发 RuntimeError</title>
      <link>https://stackoverflow.com/questions/78737115/keras-tuner-raising-runtimeerror</link>
      <description><![CDATA[每当我想使用 keras tuner 调整超参数时，它都会引发异常，提示 RuntimeError：连续失败次数超过 3 的限制。有人能帮我解决这个问题吗？
我尝试使用 keras_tuner 调整神经网络中的超参数。我的期望是获得超参数值的最佳组合。但是当我想运行 keras_tuner 超参数对象的 search 方法时，会发生 RuntimeError。]]></description>
      <guid>https://stackoverflow.com/questions/78737115/keras-tuner-raising-runtimeerror</guid>
      <pubDate>Thu, 11 Jul 2024 18:09:25 GMT</pubDate>
    </item>
    <item>
      <title>无法检测/删除图像数据集中两个位置不同的水印</title>
      <link>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</link>
      <description><![CDATA[我在从一组图片中删除水印时遇到了问题。这些水印彼此靠近，但又有所不同（见下文）。其中一个水印是红色方块，里面有白色文字。另一个是半透明的灰色句子。目的是处理图像以用于机器学习。
图像
解决问题的尝试：
由于水印在图像数据集中的位置各不相同，我尝试了以下操作：

复制图像并将其转换为 HSV 颜色空间
为感兴趣的区域选择一系列下限值和上限值（在分割图像并为每个通道构建直方图后选择这些值）
使用 cv2.inRange() 函数构建蒙版
使用蒙版在原始图像中修复水印

对于红色方块，前三个步骤完美无缺。但第三步只是有点奏效。水印比以前明显少了，但仍然很明显。对于文本，我无法在第 3 步中获得足够好的蒙版 - 它的颜色/像素强度与周围区域和文本本身太接近了。
这看起来更像是一个机器学习问题，这很好，但我想事先用尽其他选择。关于如何使用机器学习或算法方法解决这个问题有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</guid>
      <pubDate>Thu, 11 Jul 2024 16:54:13 GMT</pubDate>
    </item>
    <item>
      <title>在交互式笔记本中加载 Azure ML Studio 中已注册的模型</title>
      <link>https://stackoverflow.com/questions/78736775/load-a-registered-model-in-azure-ml-studio-in-an-interactive-notebook</link>
      <description><![CDATA[我正在使用 Azure 机器学习工作室，我的默认数据存储（blob 存储）中存储了一个 sklearn mlflow 模型，然后我将其注册为模型资产。在将其部署为批处理端点之前，如何将此模型加载到交互式笔记本中以执行一些快速模型推理和测试。
我看到了一篇链接为此处的帖子，建议在本地下载模型工件，但我不需要这样做。我应该能够直接从数据存储或注册的资产加载模型，而无需在多个位置复制模型。我尝试了以下操作，但没有成功。
从已注册的模型资产读取
import mlflow
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Model

ml_client = MLClient(DefaultAzureCredential(), &quot;&lt;subscription_id&gt;&quot;, &quot;&lt;resource_group&gt;&quot;, &quot;&lt;workspace_id&gt;&quot;)

model = ml_client.models.get(&quot;&lt;model_name&gt;&quot;, version=&quot;1&quot;)
loaded_model = mlflow.sklearn.load_model(model.id)

&gt;&gt;&gt; OSError：没有这样的文件或目录：...

从数据存储中读取
import mlflow

model_path = &quot;&lt;datastore_uri_to_model_folder&gt;&quot;
loaded_model = mlflow.sklearn.load_model(model_path)

&gt;&gt;&gt; DeserializationError：无法反序列化内容类型：text/html
]]></description>
      <guid>https://stackoverflow.com/questions/78736775/load-a-registered-model-in-azure-ml-studio-in-an-interactive-notebook</guid>
      <pubDate>Thu, 11 Jul 2024 16:45:51 GMT</pubDate>
    </item>
    <item>
      <title>如何针对简单的 ML 模型对来自 EE 的卫星数据进行标准化/预处理？</title>
      <link>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</link>
      <description><![CDATA[我对 Earth Engine/QGIS 还不太熟悉（没有 ArcGIS 许可证），我想使用简单的 ML 模型，利用卫星 VCD、NDVI 和气象数据估算地面 O3。
我对 GIS/地理空间数据处理领域非常迷茫，所以我尽我所能，疯狂地谷歌搜索并阅读了一些文章，以解释我的理由。
我想使用的数据如下：
EE 数据集：

Daymet V4 每日气候变量（https://developers.google.com/earth-engine/datasets/catalog/NASA_ORNL_DAYMET_V4#bands)
MOD13A2 NDVI 产品 (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A2)
Sentinel-5P (TROPOMI) O3 VCD 数据 (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_O3#bands)

标签：- 来自 EPA 的地面 O3 数据（https://epa.maps.arcgis.com/apps/webappviewer/index.html?id=5f239fd3e72f424f98ef3d5def547eb5&amp;extent=-146.2334,13.1913,-46.3896,56.5319），转到右上角的“选择图层”图标并选择 O3 活动/非活动，然后将鼠标悬停在任意点上

似乎这个 EPA 数据可以直接导出为 CSV，包括经度、纬度和臭氧测量值。

假设——我需要从不同来源获取的数据具有相同的空间/时间分辨率，以便使用一些简单的机器学习算法（RF/线性回归）。
在我设想的数据集中，每“行”数据将是给定像素在给定日期的气象变量值、NDVI 和 VCD 值，我可以对其执行基本的 RF/回归（使用 EE 或 Python）。在我看来，要使它发挥作用，所有数据集都需要就“像素”是什么达成一致，并且成为/成为每日时间分辨率（Daymet 和 TROPOMI 已经是每日的，我假设我可以取最接近的 16 天 NDVI 值）。
基于这个假设，我想让所有数据都具有相同的空间分辨率，因此我正在尝试弄清楚如何“重新投影” TROPOMI 数据（当前分辨率为 1111.3km）转换为 1km 分辨率（据我所知，这是所有 Daymet 数据和 MODIS 数据的分辨率）。 *我不知道如何使来自 EPA 数据的“最近像素”（来自点数据而非栅格数据）匹配，以便将其用作数据标签，但这似乎是一个更常见的问题，因此在整理完其余部分后，我将四处寻找如何修复该问题。
因此，我想在这里完成的主要操作是标准化空间分辨率：将 TROPOMI 数据转换为 1km 像素或将 Daymet/MODIS 数据转换为 1.113km 像素。
我尝试在 Earth Engine 中可视化所有三个输入数据集（为 Daymet 选择最高温度），像素似乎根本没有对齐。我已在此处附上每个图层的屏幕截图：ndvi 像素、daymet 像素和 tropomi 像素
TROPOMI 数据似乎给出了某种奇怪的模糊像素，Daymet 数据是规则的方形像素但倾斜，而 NDVI 数据是平行四边形。我隐约觉得这与不同的“投影”/“CRS”有关设置，但我对这两者都不太了解，并且不确定如何继续我认为我需要做的重新缩放。
脚本链接：https://code.earthengine.google.com/6fafaccf040e206e97a32e795611d7e4]]></description>
      <guid>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</guid>
      <pubDate>Thu, 11 Jul 2024 16:43:51 GMT</pubDate>
    </item>
    <item>
      <title>检测图像中的算术运算符</title>
      <link>https://stackoverflow.com/questions/78736359/detect-arithmetic-operators-in-an-image</link>
      <description><![CDATA[我使用 keras_ocr 创建了一个 OCR 脚本。输入是流程图（灰度图）。我想提取流程图图像的文本和形状坐标。但是，它不会提取诸如“+、-、*、/”之类的算术运算符。有时它也无法检测数值。这是我的完整脚本。
# 导入必要的库
import os
import matplotlib.pyplot as plt
import keras_ocr
import cv2
import numpy as np
from google.colab import drive
from symspellpy.symspellpy import SymSpell, Verbosity
import pkg_resources

class OCRProcessor:
def __init__(self):
# 创建用于 OCR 处理的管道
self.pipeline = keras_ocr.pipeline.Pipeline()

def __get_bbox(self, image_path):
try:
# 使用 OpenCV 读取图像
image = cv2.imread(image_path)
if image is None:
raise ValueError(f&quot;Image at path {image_path} could not be read.&quot;)

# 将图像转换为 RGB（keras-ocr 需要 RGB 图像）
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 使用 OCR 管道检测文本
images = keras_ocr.tools.read(image_path)
self.image = images
prediction_groups = self.pipeline.recognize([images])

if not prediction_groups or not prediction_groups[0]:
return [], []

# 提取边界框和文本
texts = []
results = []
for text, box in prediction_groups[0]:
texts.append(text)
xs, ys = set(), set()
for x in box:
xs.add(x[0])
ys.add(x[1])
results.append(list(map(int, [min(xs), min(ys), max(xs), max(ys)]))) # ymin, xmin, ymax, xmax

return texts, results
except Exception as e:
print(f&quot;An error occurred in __get_bbox: {e}&quot;)
return [], []

def process_image(self, image_path):
return self.__get_bbox(image_path)

# 定义函数来更正文本
def correct_text(text_array):
# 初始化 SymSpell 对象
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# 加载字典
dictionary_path = pkg_resources.resource_filename(
&quot;symspellpy&quot;, &quot;frequency_dictionary_en_82_765.txt&quot;)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

corrected_text_array = []
for text in text_array:
suggestions = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)
if suggestions:
corrected_text_array.append(suggestions[0].term)
else:
corrected_text_array.append(text)
return corrected_text_array

# 主函数
def main(image_path):
ocr_processor = OCRProcessor()
ex_text, ex_co = ocr_processor.process_image(image_path)
if not ex_text:
print(f&quot;在路径 {image_path} 处的图像中未检测到文本。&quot;)
return [], [], []

# 更正提取的文本
cr_text = correct_text(ex_text)

# 打印结果
print(&quot;提取的文本：&quot;, ex_text)
print(&quot;更正的文本：&quot;, cr_text)
print(&quot;Extracted Coordinates:&quot;, ex_co)

return ex_text, cr_text, ex_co

# 示例用法（您可以根据需要更新图像路径）
image_path = &#39;/content/Test2.jpg&#39;
ex_text, cr_text, ex_co = main(image_path)

ex_shape, ex_coor = detect_shapes(image_path)

# 打印或使用结果
print(&quot;Detected Shapes:&quot;, ex_shape)
print(&quot;Coordinates for Shapes:&quot;, ex_coor)

这个问题有解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78736359/detect-arithmetic-operators-in-an-image</guid>
      <pubDate>Thu, 11 Jul 2024 15:09:46 GMT</pubDate>
    </item>
    <item>
      <title>分割任何模型（SAM）如何使用多个框及其对应点来预测_torch？</title>
      <link>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</link>
      <description><![CDATA[我目前正在尝试 Segment Anything 模型 (SAM)，我的问题需要多个框及其对应的点，以便在框内明确。例如，box1 = [#, #, # ,#]，其点为 [x,y]，类为 [0 或 1]，然后在单个图像中包含多个这样的点。
我仅使用多个边界框就可以做到这一点，但我想在每个框中包含点。
这是我当前的代码，我不再确定它为什么会给我一个错误：
RuntimeError：除了维度 1 之外，张量的大小必须匹配。预期大小为 1，但列表中的张量编号 1 的大小为 3。

import numpy as np
import torch
import matplotlib.pyplot as plt

point = np.array([[330, 370]])
label = np.array([1])

input_point = torch.tensor(point, device=predictor.device)
input_point = input_point.unsqueeze(0)
transformed_point = predictor.transform.apply_coords_torch(input_point, image.shape[:2])

input_label = torch.tensor(label, device=predictor.device)
input_label = input_label.unsqueeze(0)

#yxyx-xyxy
filtered_rois_xyxy = transform_yxyx_to_xyxy(filtered_rois)
input_boxes = torch.tensor(filtered_rois_xyxy, device=predictor.device)
transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2]) 

masks,_,_ = predictor.predict_torch(
boxes=transformed_boxes,
point_coords=transformed_point,
point_labels=input_label,
multimask_output=False
)

masks.shape

plt.figure(figsize=(10, 10))
plt.imshow(image)
for mask in mask:
show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
for box in input_boxes:
show_box(box.cpu().numpy(), plt.gca())
plt.axis(&#39;off&#39;)
plt.show()

为了调试目的，这是每个输入的打印：

print(input_boxes)
print(input_point)
print(input_label)

tensor([[330, 370, 495, 634],
[401, 168, 586, 425],
[ 1, 0, 157, 210]], dtype=torch.int32)
tensor([[[330, 370]]])
张量([[1]])
]]></description>
      <guid>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</guid>
      <pubDate>Thu, 11 Jul 2024 14:49:27 GMT</pubDate>
    </item>
    <item>
      <title>对于图像去噪，验证中的 SSIM 高于训练中的 SSIM</title>
      <link>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</link>
      <description><![CDATA[我正在使用 2D U-Net 进行显微镜图像去噪任务。我正在使用在不同 z 级别拍摄的图像训练我的网络，这些图像具有基本事实，即 z 中图像的平均值。因此，一些图像具有相同的基本事实。我将图像分成 z 组，以将它们放在训练、验证或测试集中。我正在计算每个批次的 SSIM，并在每个时期平均结果，然后绘制它们。
我面临的问题是验证中的 SSIM 总是高于训练中的 SSIM。
我没有在我的网络中使用 dropout 或批量标准化，这会导致验证具有更高的结果。我确保训练和验证中的图像没有重复。
训练和验证中的 SSIM
有人能帮我理解为什么会发生这种情况以及如何解决吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</guid>
      <pubDate>Thu, 11 Jul 2024 13:18:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>关于python：segment_anything导致numpy.uint8错误</title>
      <link>https://stackoverflow.com/questions/78703313/segment-anything-causing-error-with-numpy-uint8</link>
      <description><![CDATA[我尝试在装有 Sonoma 14.5 的 M2 MacBook 上本地运行 https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb。但是，我在第 11 步一直遇到以下错误：
-------------------------------------------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
Cell In[75]，第 1 行
----&gt; 1 mask = mask_generator.generate(image)

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115，在 context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
112 @functools.wraps(func)
113 def decorate_context(*args, **kwargs):
114 with ctx_factory():
--&gt; 115 return func(*args, **kwargs)

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:163，位于 SamAutomaticMaskGenerator.generate(self, image)
138 &quot;&quot;&quot;
139 为给定图像生成蒙版。
140 
(...)
159 以 XYWH 格式给出的蒙版。
160 &quot;&quot;&quot;
162 # 生成蒙版
--&gt; 163 mask_data = self._generate_masks(image)
165 # 过滤蒙版中的小断开区域和孔洞
166 if self.min_mask_region_area &gt; 0：

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:206，在 SamAutomaticMaskGenerator._generate_masks(self, image) 中
204 data = MaskData()
205 for crop_box, layer_idx in zip(crop_boxes, layer_idxs):
--&gt; 206 crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
207 data.cat(crop_data)
209 # 删除裁剪之间的重复蒙版

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:236，位于 SamAutomaticMaskGenerator._process_crop(self, image, crop_box, crop_layer_idx, orig_size)
234 cropped_im = image[y0:y1, x0:x1, :]
235 cropped_im_size = cropped_im.shape[:2]
--&gt; 236 self.predictor.set_image(cropped_im)
238 # 获取此裁剪的点
239 points_scale = np.array(cropped_im_size)[None, ::-1]

文件 ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/predictor.py:57，位于 SamPredictor.set_image(self, image, image_format)
55 # 将图像转换为模型所需的形式
56 input_image = self.transform.apply_image(image)
---&gt; 57 input_image_torch = torch.as_tensor(input_image, device=self.device)
58 input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
60 self.set_torch_image(input_image_torch, image.shape[:2])

RuntimeError: 无法推断 numpy.uint8 的 dtype

我使用的是 Python 3.9.19 的 conda 环境，也用 Python 3.11 进行了测试。根据网上的评论，我怀疑这是 numpy 版本的问题，但尝试了多个版本后，我找不到正确的组合。我目前正在尝试以下内容：
numpy==1.24.4
torch==1.9.0
torchvision==0.10.0
opencv-python==4.10.0.84

在 Google Colab 上运行同一个笔记本可以正常工作，并且指示的版本是：
import numpy as np
import torch
import cv2

print(np.__version__)
print(torch.__version__)
print(cv2.__version__)

1.25.2
2.3.0+cu121
4.8.0

这是使用 Python 3.10.12。这些版本在 Mac 上不可用，所以我陷入困境。
我如何找出 numpy.uint8 无法识别的原因，以及如何修复此错误？大多数在线评论都指向升级 numpy，但我尝试了几个 numpy 版本，但没有成功。任何帮助都非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78703313/segment-anything-causing-error-with-numpy-uint8</guid>
      <pubDate>Wed, 03 Jul 2024 16:43:17 GMT</pubDate>
    </item>
    <item>
      <title>如何检查数据点是否在簇边界内</title>
      <link>https://stackoverflow.com/questions/36038022/how-to-check-if-a-data-point-is-within-the-boundary-of-a-cluster-or-not</link>
      <description><![CDATA[假设我已经完成了聚类（使用 3 个特征）并获得了 4 个聚类，对一组数据点进行训练。
现在在生产中，我将获得一组不同的数据点，并且根据该数据点的特征值，我需要知道它是否属于我之前创建的预定义聚类。这不是在进行聚类，而是查找某个点是否属于预定义聚类。
如何确定该点是否在聚类中？
我需要运行线性回归来找到覆盖聚类的边界方程吗？]]></description>
      <guid>https://stackoverflow.com/questions/36038022/how-to-check-if-a-data-point-is-within-the-boundary-of-a-cluster-or-not</guid>
      <pubDate>Wed, 16 Mar 2016 14:05:59 GMT</pubDate>
    </item>
    </channel>
</rss>