<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Apr 2024 09:14:42 GMT</lastBuildDate>
    <item>
      <title>如何使用 Python 检测 PDF 中选定的文本？</title>
      <link>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</link>
      <description><![CDATA[我有一个 Python 程序，可以使用 PDF 查看器打开 PDF 文件。查看 PDF 时，我用鼠标光标选择一些文本。有没有办法让我的 Python 程序检测我选择的文本？
我知道 PyMuPDF、PyPDF2 和 pdfplumber 等库可用于从 PDF 中提取文本。不过，我正在专门寻找一种方法来检测我在查看 PDF 时以交互方式选择的文本。
如果无法从鼠标光标直接检测，是否有任何替代方法或解决方法可以实现类似的结果？
任何见解或建议将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</guid>
      <pubDate>Fri, 12 Apr 2024 06:57:54 GMT</pubDate>
    </item>
    <item>
      <title>不同职业的人发布的文本数据集</title>
      <link>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</link>
      <description><![CDATA[最近，我一直在从事一些 NLP 任务：根据不同职业群体的帖子预测职业。我搜索了 Kaggle 等多个平台，但找不到这样的数据集。我怎样才能找到这个数据集？
我尝试了kaggle和google数据集，但不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</guid>
      <pubDate>Fri, 12 Apr 2024 04:54:13 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证可视化功能</title>
      <link>https://stackoverflow.com/questions/78313982/cross-validation-visualization-mulfunctions</link>
      <description><![CDATA[我受到 scikit-learn 的 交叉验证可视化指南，用于可视化每个 CV 分割中训练和测试索引的分布：
cmap_data = plt.cm.Paired
cmap_cv = plt.cm.coolwarm
defplot_cv_indices（cv，X，y，组，ax，n_splits，lw = 10）：
    “”“”为交叉验证对象的索引创建样本图。“”“”

    # 为每个 CV 分割生成训练/测试可视化
    对于 ii，枚举（cv.split（X = X，y = y，groups = group））中的（tr，tt）：
        # 用训练/测试组填写索引
        索引 = np.array([np.nan] * len(X))
        索引[tt] = 1
        索引[tr] = 0

        # 可视化结果
        斧头.分散（
            范围（len（索引）），
            [ii + 0.5] * len(索引),
            c=指数，
            标记=“_”，
            lw=lw,
            cmap=cmap_cv,
            vmin=-0.2,
            vmax=1.2，
        ）

    # 最后绘制数据类和组
    斧头.分散（
        范围(len(X)), [ii + 1.5] * len(X), c=y, 标记=“_”, lw=lw, cmap=cmap_data
    ）

    斧头.分散（
        范围(len(X)), [ii + 2.5] * len(X), c=组, 标记=“_”, lw=lw, cmap=cmap_data
    ）

    # 格式化
    yticklabels = list(range(n_splits)) + [“类”, “组”]
    斧头.设置（
        yticks=np.arange(n_splits + 2) + 0.5,
        yticklabels=yticklabels,
        xlabel=&quot;样本索引&quot;,
        ylabel=“CV迭代”，
        ylim=[n_splits + 2.2, -0.2],
        xlim=[0, 100],
    ）
    ax.set_title(“{}”.format(type(cv).__name__), fontsize=15)
    plt.show()

from sklearn.datasets import make_classification
从 sklearn.model_selection 导入 TimeSeriesSplit、KFold

图, ax = plt.subplots(figsize=(12, 5))
X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
绘图CV索引（
    TimeSeriesSplit(n_splits=5, 间隙=10),
    X=X,
    y=y,
    组=无，
    斧头=斧头，
    n_splits=5,
）

上面的代码给了我：

我在寻找：

预期图的想法是，该图成功地可视化了每个分组中训练集和测试集之间的差距。此外，我在正常的 KFold 上运行了一个测试用例，而且 plot_cv_indices 函数似乎也无法正常运行。]]></description>
      <guid>https://stackoverflow.com/questions/78313982/cross-validation-visualization-mulfunctions</guid>
      <pubDate>Fri, 12 Apr 2024 03:12:54 GMT</pubDate>
    </item>
    <item>
      <title>手语项目（Ai）[关闭]</title>
      <link>https://stackoverflow.com/questions/78313876/sign-language-project-ai</link>
      <description><![CDATA[目前，我们是一个团队在做毕业设计，做一个手语应用，当我们完成2个模型的训练和评估后，我们想把这2个模型放在main函数中，以便交给后端团队将模型链接到应用程序。这里的问题是
我们如何制作主函数以及完成它需要哪些步骤和库主函数？]]></description>
      <guid>https://stackoverflow.com/questions/78313876/sign-language-project-ai</guid>
      <pubDate>Fri, 12 Apr 2024 02:31:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么SAC算法在计算q函数的损失时只采样一个“下一步动作”？</title>
      <link>https://stackoverflow.com/questions/78313821/why-sac-algorithm-only-sample-one-next-action-when-computing-the-loss-of-q-fun</link>
      <description><![CDATA[在SAC的原论文中，Q-value的损失函数写为：

在实际算法实现中，将V(s_{t+1})项替换为Q(s_{t+1}, a{t+1})，损失函数写为：
next_q1 = self.networks.q1_target(obs2, next_act)
next_q2 = self.networks.q2_target(obs2, next_act)
next_q = torch.min(next_q1, next_q2)
备份 = rew + (1 - 完成) * self.gamma * (next_q - self.__get_alpha() * next_logp)

我知道这两个公式在期望上是相等的，但第二个公式显然具有更高的方差，因为它只需要 a_{t+1} 的一个样本。考虑到 SAC 的策略是随机的，我想知道在这种情况下对多个操作（或 Q_next）进行采样是否会更好？
虽然会花费更多的时间，但可以显着降低Q的梯度方差。
有关于这个问题的研究或论文吗？最好能结合实验结果进行理论解释。]]></description>
      <guid>https://stackoverflow.com/questions/78313821/why-sac-algorithm-only-sample-one-next-action-when-computing-the-loss-of-q-fun</guid>
      <pubDate>Fri, 12 Apr 2024 02:10:58 GMT</pubDate>
    </item>
    <item>
      <title>训练时间融合网络 - 多少数据？</title>
      <link>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</link>
      <description><![CDATA[关于多少数据足以训练时间融合变压器，是否有任何经验法则？更具体地说，我有大约 20 个特征，数据集大约有 50 万。这足够吗？对于多大的模型来说？
或者，我有大约 20 个产品，每个产品大约有 500k 行，而不是为每个“产品”训练不同的模型。也许我应该为所有产品训练一个模型以获得一个模型，然后定制它？
我知道这是非常高水平和模糊的，我只是在寻找一些经验法则和指导 - 我是否处于正确的范围，或者我应该去哪里。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</guid>
      <pubDate>Fri, 12 Apr 2024 01:13:06 GMT</pubDate>
    </item>
    <item>
      <title>意外的关键字参数“metaclass”--在 conda 中导入 keras 时出现问题</title>
      <link>https://stackoverflow.com/questions/78312976/unexpected-keyword-argument-metaclass-problems-when-importing-keras-in-conda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78312976/unexpected-keyword-argument-metaclass-problems-when-importing-keras-in-conda</guid>
      <pubDate>Thu, 11 Apr 2024 20:51:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyOD 中的 Decision_function 进行异常评分计算</title>
      <link>https://stackoverflow.com/questions/78312583/anomaly-scores-computation-using-decision-function-in-pyod</link>
      <description><![CDATA[我发现一篇文章警告 sklearn 用户 OCSVM 中的 Decision_function 输出的异常分数：
https://activisiongamescience.github.io/2015/12/23/Unsupervised-Anomaly-Detection-SOD-vs-One-class-SVM/#sklearn-Users-Beware
但是，我正在使用 PyOD 进行无监督异常值检测，并想知道 Decision_function 的输出是否需要相同的转换。是否有任何参考资料指定如何为 PyOD 中的各种算法计算 Decision_function 值？ （我专门寻找 KNN、OCSVM、CBLOF 和 IForest）。]]></description>
      <guid>https://stackoverflow.com/questions/78312583/anomaly-scores-computation-using-decision-function-in-pyod</guid>
      <pubDate>Thu, 11 Apr 2024 19:08:37 GMT</pubDate>
    </item>
    <item>
      <title>在运行 ML 项目时如何使用存储在 google Drive 中的数据集？</title>
      <link>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</link>
      <description><![CDATA[我正在运行 SoccerNet 项目，该项目为足球视频生成字幕。
我正在尝试将路径传递到存储数据集的Google Drive，即**https://drive.google.com/drive/folders/{folder_id} **
我正在运行的命令如下。
python main.py --SoccerNet_path=“https://drive.google.com/drive/folders/{folder_id}” --model_name=new_model --features=baidu_soccer_embeddings.npy --framerate=1 --pool=NetVLAD --window_size_caption=45 --window_size_spotting=15 --NMS_window=30 --num_layers=4 --first_stage=caption --pretrain --GPU=0


我收到操作系统错误：如下
OSError：[Errno 22] 参数无效：&#39;https:https://drive.google.com/drive/folders/{folder_id}?usp=drive_link\\england_epl\\2014-2015\\2015 -02-21 - 18-00 切尔西 1 - 1 伯恩利\\1_baidu_soccer_embeddings.npy&#39;

我尝试使用 google colab，但我没有得到 colab 中预期的输出，因为它没有生成应包含预期字幕的 json 文件。
我现在使用 VS code。]]></description>
      <guid>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</guid>
      <pubDate>Thu, 11 Apr 2024 18:15:53 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Matrox Model Finder 在单个图像中多次查找同一模型？</title>
      <link>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</link>
      <description><![CDATA[我是 Matrox 新手，所以这可能是一个非常初学者的问题。
我有一个托盘，里面有多个相同型号的物品。当我将 ModelFinder 步骤添加到程序中时，我添加了我正在寻找的模型，但它只显示了我注册的模型，我猜测是因为相机的扭曲。我如何让 Matrox 知道还有更多项目并且它们也是同一型号？

我添加了一个必须找到它的搜索区域，并且我选择了它的选项来查找所有出现的情况，但它只显示了一个而不是实际存在的 3/4。
]]></description>
      <guid>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</guid>
      <pubDate>Thu, 11 Apr 2024 16:02:48 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证了模型训练正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>Word2Vec Hierarchical Softmax 中的内部顶点是什么？</title>
      <link>https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax</link>
      <description><![CDATA[我有一个关于分层 Softmax 的问题。实际上，我不太明白内部顶点（不是叶顶点）中存储的内容。我清楚地理解这个算法的主要思想，但是每一步我们都计算输入词嵌入与内部顶点的词嵌入的点积。那么这些内部顶点内部有哪些向量呢？是否是大小等于 embedding_size 的随机初始化向量，然后它们的坐标由于反向传播步骤而变化，直到我们停止？]]></description>
      <guid>https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax</guid>
      <pubDate>Sat, 06 Apr 2024 18:15:41 GMT</pubDate>
    </item>
    <item>
      <title>python中的批量梯度下降算法返回巨大的值</title>
      <link>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</link>
      <description><![CDATA[我正在尝试在 python 中实现批量梯度下降算法，该算法将训练集、学习率和迭代次数作为输入参数，并返回权重。然而，当我运行它时，在几次迭代内，参数的值呈指数级增长，最终返回“nan”。
将 numpy 导入为 np

x = np.array([[2104], [1600], [2400], [1416], [3000], [1985], [1534], [1427], [1380], [1494], [1940] , [2000], [1890], [4478], [1268], [2300], [1320], [1236], [2609], [3031], [1767], [1888], [1604], [ 1962]、[3890]、[1100]、[1458]、[2526]、[2200]、[2637]、[1839]、[1000]、[2040]、[3137]、[1811]、[1437] 、[1239]、[2132]、[4215]、[2162]、[1664]、[2238]、[2567]、[1200]、[852]、[1852]、[1203]]）

y = np.array([399900, 329900, 369000, 232000, 539900, 299900, 314900, 198999, 212000, 242500, 239999, 347000, 329999, 699900, 259 900、449900、299900、199900、499998、599000、252900、255000 , 242900, 259900, 573900, 249900, 464500, 469000, 475000, 299900, 349900, 169900, 314900, 579900, 285900, 249900, 229900, 3 45000、549000、287000、368500、329900、314000、299000、179900、299900、239500 ]）

a = 0.01

迭代次数 = 100

def BGD ( x, y, a, num_iter):
    m = len(x) #样本数
    n = x.shape[1] #特征数量
    p = np.zeros(n)
    b = 0
    对于 _ 在范围内（num_iter）：
        sum_p = np.zeros(n)
        总和 = 0
        对于范围 (m) 内的 i：
            sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
            sum_b = sum_b + (((np.dot(p,x[i])+b) - y[i]))
        p = p - (a * (1/m) * sum_p)
        b = b - (a * (1/m) * sum_b)
    返回 p、b

p, b = BGD(x, y, 0.01, 100)
打印（页）
打印(b)

我得到以下信息：
RuntimeWarning: add 中遇到溢出
  sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
RuntimeWarning：减法中遇到无效值
  p = p - (a * (1/m) * sum_p)
[南]
南
]]></description>
      <guid>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</guid>
      <pubDate>Sat, 30 Mar 2024 14:00:37 GMT</pubDate>
    </item>
    <item>
      <title>在变压器中加载安全张量文件</title>
      <link>https://stackoverflow.com/questions/76247802/loading-a-safetensor-file-in-transformers</link>
      <description><![CDATA[我已经从 Huggingface 下载了这个模型。我正在尝试在变压器中加载这个模型，以便我可以进行推理：
从变压器导入 AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“path_to/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g”)

模型 = AutoModelForCausalLM.from_pretrained(“path_to/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g”)

但我收到错误消息，说它需要 .bin 或 .h5 或 .ckpt 文件，但上面只有 .safetensors 或 .pt 文件
如何加载模型？]]></description>
      <guid>https://stackoverflow.com/questions/76247802/loading-a-safetensor-file-in-transformers</guid>
      <pubDate>Sun, 14 May 2023 13:46:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Docker 和 Flask 进行机器学习的性能问题</title>
      <link>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</link>
      <description><![CDATA[我有一些应用于 json 文件的 python3 代码，代码中有一些神经网络和随机森林。我将代码放入 Docker 容器中，但注意到这些 ML 任务在不使用 Docker 的情况下比使用 Docker 运行得更快。在 Docker 中，我使用 Flask 加载 json 文件并运行代码。当然，我在本地和 Docker 内部使用了相同版本的 python 模块，这些是：

theano 0.8.2
keras 2.0.5
scikit-learn 0.19.0

另外，Flask 是

0.12

起初，我认为 theano 在有 Docker 的情况下可能会使用不同的资源，但它同时运行单 CPU 和单线程。它也没有使用我的 GPU。当我意识到我的随机森林在 Docker 中运行速度也变慢时，我意识到这可能不是 theano。以下是我执行的一系列测试（我对每个测试进行了多次测试，我报告了平均时间，因为这些测试是稳定的）
没有 Docker，没有 Flask：

任务 1（theano + keras 代码）：1.0s 
任务 2（theano + keras 代码）：0.7s
任务 3（scikit-learn 代码）：0.25 秒

Docker (cpus=1) + Flask (调试模式 = True):

T1：6.5秒
T2：2.2秒
T3：0.58s

Docker (cpus=2) + Flask (调试模式 = True):

T1：5.5秒
T2：1.4秒
T3：0.55s

Docker (cpus=2) + Flask (调试模式 = False)：

T1：4.5秒
T2：1.2秒
T3：0.5秒

Docker (cpus=2)（无 Flask，仅调用本地完成的 json 文件）：

T1：2.8s
T2：1.1秒
T3：0.5秒

Flask（调试模式 = True）（无 Docker 容器）：

T1：2.8s
T2：1.5秒
T3：0.2秒

我猜 cpu=1 与 cpu=2 只是将一个 cpu 分配给代码，而第二个 cpu 只是接管一些其他工作。显然，当不使用 Flask 或 Docker 时，时间会有所减少，但我仍然无法达到没有 Docker 和 Flask 的速度。有谁猜到为什么会发生这种情况吗？
这是我们如何使用 Flask 运行应用程序的最小代码块
api = Flask(__name__)
pipeline = Pipeline() # 调用多个任务的私有类

@api.route(&quot;/&quot;,methods=[&#39;POST&#39;])
def 条目():
    数据 = request.get_json(force=True)
    数据 = pipeline.process(数据)
    # 这会调用不同的定时任务

如果 __name__ == &quot;__main__&quot;:
    api.run（调试= True，主机=&#39;0.0.0.0&#39;，线程= False）


PS。如果问题缺少任何内容，请原谅我，这是我的第一个 StackOverflow 问题]]></description>
      <guid>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</guid>
      <pubDate>Tue, 22 May 2018 09:48:58 GMT</pubDate>
    </item>
    </channel>
</rss>