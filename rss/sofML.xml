<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 19 Jul 2024 12:28:21 GMT</lastBuildDate>
    <item>
      <title>Python 代码与网站交互并收集数据的方式？[关闭]</title>
      <link>https://stackoverflow.com/questions/78768755/ways-for-python-code-to-interact-with-websites-and-gather-data</link>
      <description><![CDATA[我需要收集数据，然后将其输入到 NLT 过滤器中，而该项目要求我从文章中收集数据（URL 和文本）。有没有办法用 Python 或任何编程语言来实现这一点？
目前，我有两个想法 - 创建一个使用屏幕上的坐标来管理这些操作的系统，然后尝试在指定的坐标处输入/提取数据，以及我训练的用于定位所述输入/提取字段的算法（ai？）。我仍然需要进一步研究这些，我非常乐于接受建议。]]></description>
      <guid>https://stackoverflow.com/questions/78768755/ways-for-python-code-to-interact-with-websites-and-gather-data</guid>
      <pubDate>Fri, 19 Jul 2024 10:05:02 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 与 XGBoost 的领先一步预测：为什么此代码中的 LSTM 表现如此糟糕？</title>
      <link>https://stackoverflow.com/questions/78768566/lstm-vs-xgboost-for-one-step-ahead-predictions-why-does-lstm-in-this-code-perf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78768566/lstm-vs-xgboost-for-one-step-ahead-predictions-why-does-lstm-in-this-code-perf</guid>
      <pubDate>Fri, 19 Jul 2024 09:19:23 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器，网格搜索</title>
      <link>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</guid>
      <pubDate>Fri, 19 Jul 2024 09:07:04 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 sklearn 序数编码器完成的映射？</title>
      <link>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</link>
      <description><![CDATA[我使用 sklearn 对数据集的两列进行了序数编码
我想知道哪一列映射到哪一列
假设 0 映射到两列的什么位置
我想知道语法，尝试询问 gpt 但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</guid>
      <pubDate>Fri, 19 Jul 2024 08:02:11 GMT</pubDate>
    </item>
    <item>
      <title>从 TMT（螺纹铣刀测试）报告中提取文本</title>
      <link>https://stackoverflow.com/questions/78767922/extracting-text-from-an-tmt-thread-mill-test-report</link>
      <description><![CDATA[我有一项任务，需要从表格的 TMT PNG 图像中提取特定值。根据所需的输出，我需要从表格单元格中提取特定值或从图像中的报告中提取一些文本。
您能否建议现有的机器学习模型和任何能够从图像中的特定位置提取文本、数字或特殊符号并将其输出到 Excel 文件中的库？此外，请将过程分解为分步任务以实现所需的输出。
目前，我已经开始研究库、
Oytesseract、opencv-python-headless、Pandas、Pillow 和 Openpyxl。
!sudo apt-get install tesseract-ocr
我需要知道是否有任何方法可以探索，以及我应该学习什么来实现这些新方法，并解决这个问题，请向我解释，以便我可以对其进行研究。]]></description>
      <guid>https://stackoverflow.com/questions/78767922/extracting-text-from-an-tmt-thread-mill-test-report</guid>
      <pubDate>Fri, 19 Jul 2024 06:53:53 GMT</pubDate>
    </item>
    <item>
      <title>如何减少 AWS SageMaker 上实时机器学习模型部署的延迟？[关闭]</title>
      <link>https://stackoverflow.com/questions/78767578/how-to-reduce-latency-in-real-time-machine-learning-model-deployment-on-aws-sage</link>
      <description><![CDATA[我在 AWS SageMaker 上部署的实时 ML 模型中遇到了严重的延迟。我尝试了各种实例类型、简化的预处理和模型优化，但延迟仍然存在。
我优化了实例、简化了预处理并精简了模型。预计延迟会降低，但仍然很高。]]></description>
      <guid>https://stackoverflow.com/questions/78767578/how-to-reduce-latency-in-real-time-machine-learning-model-deployment-on-aws-sage</guid>
      <pubDate>Fri, 19 Jul 2024 04:56:43 GMT</pubDate>
    </item>
    <item>
      <title>如何注释 VQA 数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/78767391/how-to-annotate-vqa-dataset</link>
      <description><![CDATA[找不到好用的标注平台或软件？大家是怎么解决这个问题的？又是如何整理数据集的JSON的？
我用过label me和label studio，效果都不太好，没能像我预期的那样高效地标注。]]></description>
      <guid>https://stackoverflow.com/questions/78767391/how-to-annotate-vqa-dataset</guid>
      <pubDate>Fri, 19 Jul 2024 03:15:00 GMT</pubDate>
    </item>
    <item>
      <title>尽管数据类型为数字且形状正确，KNNImputer 仍会删除列</title>
      <link>https://stackoverflow.com/questions/78767192/knnimputer-drops-columns-despite-of-numeric-datatypes-and-right-shape</link>
      <description><![CDATA[我正在使用 KNNImputer 在几个 pd.DataFrame 中估算 np.nan 值。我检查了每个数据框的所有数据类型都是数字。但是，KNNImputer 在某些数据框中删除了一些列：
&gt;&gt;&gt;input_df.shape 
(816, 216) 

&gt;&gt;&gt; input_df.dtypes.value_count()
float64 216
dtype: int64

&gt;&gt;output_df.shape 
(816, 27)

我使用了以下 KNNImputer 配置
imputer = KNNImputer(n_neighbors=1, 
weights=&quot;uniform&quot;,
add_indicator=False)

output_df = imputer.fit_transform(input_df)

我想知道为什么会发生这种情况，因为每个数据框都有 np.nan 值。顺便说一句，参数 n_neighbors=1 不应该对结果产生任何影响，因为我正在用最近邻居的值替换缺失值。]]></description>
      <guid>https://stackoverflow.com/questions/78767192/knnimputer-drops-columns-despite-of-numeric-datatypes-and-right-shape</guid>
      <pubDate>Fri, 19 Jul 2024 01:17:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Intel Iris 在训练 Yolov8 模型方面表现优于 RTX 3050 笔记本电脑 GPU [关闭]</title>
      <link>https://stackoverflow.com/questions/78766852/why-the-intel-iris-performs-better-than-rtx-3050-laptop-gpu-in-training-a-yolov8</link>
      <description><![CDATA[我有一台戴尔 G-15 游戏笔记本电脑，配备 Core i5 12500h 和 Rtx 3050。我是深度学习和人工智能的新手。我正在使用 Yolo v8 训练一个模型，用于自动车牌检测，其自定义数据集包含 28k 张图像。
当我尝试训练模型时，我遇到了一个奇怪的问题。当我使用我的 集成 GPU 时，训练速度非常快，大约 5-6it/s，但当我切换到 Rtx 3050 时，性能下降到 3-4it/s。我在 Tensorflow 中也遇到了这个问题。




我不明白为什么集成 GPU 更快。专用 GPU 不是应该更快吗？
此外，当使用专用 GPU 时，其使用率为 100%，但当使用集成 GPU 时，其使用率仅为 30-40%。为什么？
我对这种行为一无所知。我的所有驱动程序都是最新的。


]]></description>
      <guid>https://stackoverflow.com/questions/78766852/why-the-intel-iris-performs-better-than-rtx-3050-laptop-gpu-in-training-a-yolov8</guid>
      <pubDate>Thu, 18 Jul 2024 22:10:21 GMT</pubDate>
    </item>
    <item>
      <title>训练帮助混合模型，该模型集成了上下文和数值特征以解决分类问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78766812/training-help-hybrid-based-model-that-integrates-contextual-and-numerical-featur</link>
      <description><![CDATA[我想要一个关键的生产风险分析问题。因此，根据记录，我想将每条记录的风险等级从 0 到 5。训练集相当不平衡。
&gt; &quot;0.0 964 
&gt; 1.0 393 
&gt; 2.0 396
&gt; 3.0 286 
&gt; 4.0 109 
&gt; 5.0 44&quot;

现在，当前训练集如下所示：
 2 风险等级 float64
3 a_weights int64 
4 b_weights float64
5 c_weights float64
6 d_weights float64
7 e_weights float64
8 f_weights float64
9 g_weights float64
10 FinalDesc 对象 

FinalDesc 列包含一个字符串（工作单的描述）。
例如：
“HVAC 更换工具因恶劣环境而无法使用。请小心修理”
我在 Final Desc 中也有关键词的权重，这将有助于排名。
但是，现在的问题是，我的主管给了我工厂特定的背景信息，这可能有助于预测。例如：
&quot;
消防监视记录被认为风险较低，
高压灭菌器上的阀门 4/5 或由于库存水平较高而通常风险较低。
用于审查 PM 详细信息的 REL 记录不会带来直接风险。
&quot;
还有更多背景信息。进行这些排名的最佳方法是什么？我应该利用 LLM 的力量吗？请让我知道整合背景的最佳方法。
我目前的方法是：

矢量化描述并添加到数据框
使用随机 Forrest 分类器对工作订单进行排名（训练、预测）。同时使用数值和描述

它的准确率为 66%。我想添加更复杂的 AI/ML 功能来解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/78766812/training-help-hybrid-based-model-that-integrates-contextual-and-numerical-featur</guid>
      <pubDate>Thu, 18 Jul 2024 21:51:38 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在 dim 1 处预期长度为 129 的序列（得到 46）</title>
      <link>https://stackoverflow.com/questions/78766178/valueerror-expected-sequence-of-length-129-at-dim-1-got-46</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78766178/valueerror-expected-sequence-of-length-129-at-dim-1-got-46</guid>
      <pubDate>Thu, 18 Jul 2024 18:40:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 GAN 进行欺诈检测</title>
      <link>https://stackoverflow.com/questions/78761254/fraud-detection-using-gan</link>
      <description><![CDATA[我正在使用 GAN 实现基于交易的欺诈检测模型，但我仍然想指定我的模型，即我想强调 RIB 和交易时间（尤其是发行时间）我想知道个人通过这些变量（时间和 RIB）的行为如何影响交易是否是欺诈性的。基本上，这个模型很好，但它仍然很肤浅，我们需要通过强调提到的变量来更深入地研究。
我的数据集的头部
就像我说的，我尝试了一个通用的 GAN 模型，但我想实现一个专注于 RIB 和发行时间的指定 GAN 模型]]></description>
      <guid>https://stackoverflow.com/questions/78761254/fraud-detection-using-gan</guid>
      <pubDate>Wed, 17 Jul 2024 19:15:59 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正 Reshape 函数中的错误</title>
      <link>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</link>
      <description><![CDATA[当我拟合各种人工神经网络时，TLNN 的代码显示不正确，尤其是包含重塑函数的行。这有什么问题吗？更改数据集是否意​​味着重塑函数不起作用并显示此错误
def Forecast_TLNN(model, time_lagged_points, last_sequence, Future_steps):
Forecasted_values = []
max_lag = max(time_lagged_points)
for i in range(future_steps):
input_sequence = [last_sequence[max_lag - p] for p in time_lagged_points]
Forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
Forecasted_values.append(forecasted_value[0][0])
last_sequence = last_sequence[1:] + [forecasted_value[0][0]]
return Forecasted_values

错误显示在以下行中：forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
我似乎无法在互联网上找到有关此代码的任何更正。
 ---------------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[83], line 13
10 # look_back, hidden_​​nodes, output_nodes, epochs, batch_size, future_steps
11 parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]
---&gt; 13 RMSE_info = compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)

单元格 In[79]，第 6 行，在 compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps) 中
3 information_FNN_df = get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters_FNN, scaler)
4 optimal_params_FNN = analyze_results(information_FNN_df, test_rainfall_data, &#39;FNN&#39;)
----&gt; 6 information_TLNN_df = get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters_TLNN, scaler)
7 optimal_params_TLNN = analyze_results(information_TLNN_df, test_rainfall_data, &#39;TLNN&#39;)
9 information_SANN_df = get_accuracies_SANN(rainfall_data, test_rainfall_data, parameters_SANN, scaler)

单元格 In[55]，第 21 行，在 get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters, scaler)
18 batch_size = param[4]
19 future_steps = param[5]
---&gt; 21 model_TLNN, Forecasted_values_TLNN = TLNN(rainfall_data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
23 y_true = test_rainfall_data.iloc[:future_steps].Precipitation
24 mse, mae, mape, rmse = calculate_performance(y_true, Forecasted_values_TLNN)

单元格 In[53]，第 9 行，在 TLNN(data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
6 model_TLNN = train_model(model_TLNN, X_train, y_train, epochs, batch_size)
8 max_lag = max(time_lagged_points)
----&gt; 9 预测值_TLNN = 预测值_TLNN(model_TLNN, time_lagged_points, 
10 列表(数据[-max_lag:]), 未来步骤=未来步骤)
11 预测值_TLNN = 列表(scaler.inverse_transform([预测值_TLNN])[0])
13 返回 model_TLNN, 预测值_TLNN

单元格 In[51]，第 6 行，在预测值_TLNN(模型, time_lagged_points, last_sequence, 未来步骤)
4 for i in range(future_steps):
5 输入序列 = [last_sequence[max_lag - p] for p in time_lagged_points]
----&gt; 6 Forecasted_value = model.predict((np.reshape(input_sequence, (1, len(input_sequence)))))
7 Forecasted_values.append(forecasted_value[0][0])
8 last_sequence = last_sequence[1:] + [forecasted_value[0][0]]

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:285，在 reshape(a, newshape, order) 中
200 @array_function_dispatch(_reshape_dispatcher)
201 def reshape(a, newshape, order=&#39;C&#39;):
202 &quot;&quot;&quot;
203 为数组赋予新形状而不更改其数据。
204 
(...)
283 [5, 6]])
284 &quot;&quot;&quot;
--&gt; 285 返回 _wrapfunc(a, &#39;reshape&#39;, newshape, order=order)

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:56，位于 _wrapfunc(obj, method, *args, **kwds)
54 bound = getattr(obj, method, None)
55 如果 bound 为 None:
---&gt; 56 return _wrapit(obj, method, *args, **kwds)
58 try:
59 return bound(*args, **kwds)

File ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:45, in _wrapit(obj, method, *args, **kwds)
43 except AttributeError:
44 wrap = None
---&gt; 45 result = getattr(asarray(obj), method)(*args, **kwds)
46 if wrap:
47 if not isinstance(result, mu.ndarray):

ValueError: 设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (5,) + 非均匀部分。
]]></description>
      <guid>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</guid>
      <pubDate>Mon, 15 Jul 2024 10:54:58 GMT</pubDate>
    </item>
    <item>
      <title>karateclub MUSAE 嵌入产生奇怪的列数</title>
      <link>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</link>
      <description><![CDATA[我正在试验属性节点嵌入和结构嵌入，但 karateclub 实现返回的矩阵具有奇怪的列数。
MUSAE 给出 128 个“特征”，而不是请求的 32 个。当我请求 32 个时，GLEE 给出了 33 个。我遗漏了什么吗？
import random
import numpy as np
import networkx as nx
from scipy.sparse import coo_matrix

from karateclub.node_embedding.attributed import MUSAE
from karateclub.node_embedding.neighbourhood import GLEE

g = nx.newman_watts_strogatz_graph(50, 10, 0.2)

X = {i: random.sample(range(150),50) for i in range(50)}

row = np.array([k for k, v in X.items() for val in v])
col = np.array([val for k, v in X.items() for val in v])
data = np.ones(50*50)
shape = (50, 150)

X = coo_matrix((data, (row, col)), shape=shape)

model = MUSAE(dimensions=32)
model.fit(g, X)
emb = model.get_embedding()
print(emb.shape)

model = GLEE(dimensions=32)
model.fit(g)
emb = model.get_embedding()
print(emb.shape)

输出：
(50, 128)
(50, 33)
]]></description>
      <guid>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</guid>
      <pubDate>Fri, 14 Jun 2024 15:02:08 GMT</pubDate>
    </item>
    <item>
      <title>在谷歌云平台中运行 jupyter lab 时出现错误 524</title>
      <link>https://stackoverflow.com/questions/68862621/getting-error-524-while-running-jupyter-lab-in-google-cloud-platform</link>
      <description><![CDATA[我无法访问在 Google Cloud 上创建的 jupyter lab

我使用 Google AI 平台创建了一个笔记本。我能够启动它并工作，但它突然停止了，我现在无法启动它。我尝试构建并重新启动 jupyterlab，但毫无用处。我也检查了我的磁盘使用情况，只有 12%。
我尝试了诊断工具，结果如下：

但没有修复。
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/68862621/getting-error-524-while-running-jupyter-lab-in-google-cloud-platform</guid>
      <pubDate>Fri, 20 Aug 2021 12:57:57 GMT</pubDate>
    </item>
    </channel>
</rss>