<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 04 Aug 2024 01:12:11 GMT</lastBuildDate>
    <item>
      <title>加载 Keras 模型时，“密集层”需要 1 个输入，但它收到了 2 个输入张量”</title>
      <link>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</link>
      <description><![CDATA[我正在使用 Kaggle 开发乳腺癌组织病理学图像的分类模型。该数据集包含 157,572 张图像（78,786 张 IDC 阴性和 78,786 张 IDC 阳性），每张图像的尺寸为 50x50 像素。
我使用 ResNet50 作为基础模型，并尝试保存并稍后加载经过训练的模型的最高效版本。但是，当我尝试加载已保存的模型时，我遇到了以下错误：
# 加载模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)
“密集”层需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_566&gt;, &lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_567&gt;]

这是我的代码：
import tensorflow as tf
from tensorflow.keras import layer, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import load_model

# 数据增强管道
data_augmentation = tf.keras.Sequential([
layer.RandomFlip(&quot;horizo​​ntal_and_vertical&quot;),
layer.RandomRotation(0.5),
layer.RandomContrast(0.2),
layer.RandomBrightness(0.2),
layer.GaussianNoise(0.1)
])

# 预处理函数以规范化图像
def preprocess(image, label):
image = tf.image.resize(image, (50, 50))
image = tf.cast(image, tf.float32) / 255.0
return image, label

# 使用验证分割加载数据集
train_dir = &#39;path/to/data&#39;
batch_size = 64
img_height = 50
img_width = 50
validation_split = 0.2
test_split_ratio = 0.5

# 加载数据集
data_train = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

data_val = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

# 将验证集拆分为验证集和测试集
def split_dataset(dataset, split_ratio=0.5):
dataset_size = len(dataset)
split = int(split_ratio * dataset_size)
train_dataset = dataset.take(split)
test_dataset = dataset.skip(split)
return train_dataset, test_dataset

data_val, data_test = split_dataset(data_val, split_ratio=test_split_ratio)

# 应用预处理和增强
data_train = data_train.map(lambda x, y: (data_augmentation(x, training=True), y)).map(preprocess)
data_val = data_val.map(preprocess)
data_test = data_test.map(preprocess)

# 预取数据以获得更好的性能
data_train = data_train.prefetch(tf.data.AUTOTUNE)
data_val = data_val.prefetch(tf.data.AUTOTUNE)
data_test = data_test.prefetch(tf.data.AUTOTUNE)

# 定义 ResNet50 模型
input_shape = (50, 50, 3)
base_model = ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=input_shape)
base_model.trainable = False

# 构建模型
model = models.Sequential([
layer.Input(shape=(img_height, img_width, 3)),
base_model,
layer.GlobalAveragePooling2D(),
layer.Dense(256,activation=&#39;relu&#39;),
layer.Dropout(0.5),
layer.Dense(1,activation=&#39;sigmoid&#39;)
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 用于提前停止和检查点的回调
early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;,patient=5, restore_best_weights=True)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
filepath=&#39;/kaggle/working/resnet50_model.keras&#39;,
monitor=&#39;val_loss&#39;,
mode=&#39;min&#39;,
save_best_only=True,
save_weights_only=False
)

# 训练模型
model.fit(data_train, epochs=50, validation_data=data_val, callbacks=[early_stopping, model_checkpoint_callback])

# 微调模型
base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.fit(data_train, epochs=50, validation_data=data_val,回调=[early_stopping, model_checkpoint_callback])

# 保存模型
model.save(&#39;/kaggle/working/resnet50_model.keras&#39;)

# 加载最佳微调模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)

为了调试该问题，我在预处理阶段通过打印数据批次的形状来检查图像输入。结果如下：

训练数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

验证数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

测试数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)


我预计保存的模型可以顺利加载，这样我就可以使用训练后的权重和架构进行评估和预测。]]></description>
      <guid>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</guid>
      <pubDate>Sat, 03 Aug 2024 20:01:05 GMT</pubDate>
    </item>
    <item>
      <title>我想进行机器学习，但找不到任何资源？[关闭]</title>
      <link>https://stackoverflow.com/questions/78829584/i-want-to-machine-learning-but-cant-find-any-resources</link>
      <description><![CDATA[我目前正在使用 MERN 堆栈（MongoDB、Express.js、React、Node.js），并希望扩展我的技能。我计划从 Java 中的数据结构和算法 (DSA) 开始，但我也想深入研究机器学习。但是，我很难找到学习机器学习的好资源。
有人可以推荐一些学习机器学习的好资源（书籍、课程、教程等）吗？
我试图寻找有关机器学习的课程。我不太喜欢读书，所以没有任何实际书籍。我在 Coursera 上偶然发现了 Andrewng 的课程，但这是一门付费课程，所以我放弃了。现在我期望可以在 youtube 上找到一门课程，但有很多课程，我真的不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78829584/i-want-to-machine-learning-but-cant-find-any-resources</guid>
      <pubDate>Sat, 03 Aug 2024 19:15:35 GMT</pubDate>
    </item>
    <item>
      <title>无法解决 QiskitMachineLearningError：'输入数据的形状不正确，最后一个维度不等于输入的数量：0，但得到：2'</title>
      <link>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</link>
      <description><![CDATA[我收到错误：
32 vqc.fit(X_train, X_test)
33 
34 # 评估分类器

15 帧
/usr/local/lib/python3.10/dist-packages/qiskit_machine_learning/neural_networks/neural_network.py in _validate_input(self, input_data)
132 
133 if shape[-1] != self._num_inputs:
-&gt; 134 引发 QiskitMachineLearningError(
135 f&quot;输入数据的形状不正确，最后一个维度 &quot;
136 f&quot;不等于输入数量： &quot;

QiskitMachineLearningError：&#39;输入数据的形状不正确，最后一个维度不等于输入数量：0，但得到：2。&#39;

来自 qiskit 导入 QuantumCircuit、transpile、assemble
来自 qiskit_aer 导入 Aer
来自 qiskit_machine_learning.algorithms 导入 VQC
来自 qiskit_algorithms.optimizers 导入 COBYLA
来自 qiskit.circuit.library 导入 TwoLocal
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.model_selection 导入 train_test_split
导入 numpy 作为 np

# 用于演示目的的样本数据
# 将其替换为您的实际数据
scaled_data = np.random.rand(150, 2) # 用您的缩放数据替换
y = np.random.randint(0, 3, size=150) # 用您的标签替换

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)

# 根据特征维度定义量子比特的数量
num_qubits = X_train.shape[1]

# 使用正确的参数定义量子特征图和 ansatz
feature_map = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;) #, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)
ansatz = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;)#, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)

# 定义优化器
optimizer = COBYLA()

# 使用唯一参数名称初始化 VQC 分类器
vqc = VQC(feature_map=feature_map, ansatz=ansatz, optimizer=optimizer)

# 训练量子分类器
vqc.fit(X_train, X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</guid>
      <pubDate>Sat, 03 Aug 2024 14:52:35 GMT</pubDate>
    </item>
    <item>
      <title>分类任务的 Brier 评分 [关闭]</title>
      <link>https://stackoverflow.com/questions/78828870/brier-score-for-a-classification-task</link>
      <description><![CDATA[我为分类问题训练了几个模型，我想计算它们预测的 Brier 分数。
尽管如此，我不太确定应该从 DescTools 向公式传递什么（即 BrierScore(x, pred = NULL, scaled = FALSE, ...)）。
到目前为止，这是我为其中一个模型的概率创建的数据框：
 X0 X1 real_scores max_prob max_source
1 0.024 0.976 1 0.976 1
2 0.910 0.090 0 0.910 0
3 0.524 0.476 0 0.524 0
4 0.942 0.058 0 0.942 0
5 0.944 0.056 0 0.944 0
6 0.074 0.926 1    0.926 1 7 0.254 0.746 0 0.746 1 8 0.864 0.136 0 0.864 0 9 0.522 0.478 0 0.522 0 10 0.422 0.578 1 0.578 1 11 0.772 0.228 0 0.772 0 12 0.564 0.436 0 0.564 0 13 0.968 0.032 0 0.968 0 14 0.760 0.240 0 0.760 0 15 0.978 0.022 0 0.978 0 16 0.818 0.182           1 0.818 0 17 0.730 0.270 0 0.730 0 18 0.824 0.176 0 0.824 0 19 0.962 0.038 0 0.962 0 20 0.514 0.486 0 0.514 0 21 0.360 0.6 40 0 0.640 1 22 0.708 0.292 0 0.708 0 23 0.940 0.060 0 0.940 0 24 0.916 0.084 0 0.916 0 25 0.606 0.394 1 0.606 0 26 0.838 0.162 0 0.838 0
27 0.742 0.258 0 0.742 0
28 0.850 0.150 1 0.850 0

其中：

X0 = 负类的概率
X1 = 正类的概率
real_scores = 1（正类）；0（负类）
max_prob = 两个类（X0、X1）的最大概率
max_source = 最大概率来自哪个类别（X0 或 X1）。

我不明白的是，我是否必须仅将 X1（正类）的概率、两者的概率或最大值传递给公式。]]></description>
      <guid>https://stackoverflow.com/questions/78828870/brier-score-for-a-classification-task</guid>
      <pubDate>Sat, 03 Aug 2024 14:02:24 GMT</pubDate>
    </item>
    <item>
      <title>Kaldi：对齐脚本未生成输出（ali.1.gz）</title>
      <link>https://stackoverflow.com/questions/78828864/kaldi-alignment-script-not-generating-output-ali-1-gz</link>
      <description><![CDATA[我正在尝试使用 Kaldi 训练语音识别模型。我已成功运行 steps/nnet3/align.sh 脚本，但 exp/chain/tree_sp 目录中未创建预期的 ali.1.gz 文件。
我已检查 exp/chain/tree_sp/log 目录中的终端输出和日志文件，但没有错误消息。该脚本似乎运行正常，但缺少所需的输出。
您能否建议此问题的潜在原因或进一步调试的步骤？如何解决？
我已成功运行 steps/nnet3/align.sh 脚本（终端或日志文件中没有错误）。
我已经检查过 exp/chain/tree_sp 目录中是否存在 ali.JOB.gz 文件，但它们不存在（未创建）
我希望 steps/nnet3/align.sh 脚本能够在 exp/chain/tree_sp 目录中成功创建 ali.1.gz 文件（或多个 ali.JOB.gz 文件）。此文件对于 Kaldi 管道中的后续训练步骤至关重要。
ls exp/chain/tree_sp
0.mdl cmvn_opts final.mat final.mdl final.occs full.mat log num_jobs phone.txt splice_opts tree

当我运行 local/chain/tuning/run_tdnn_1j.sh 时收到此错误
Traceback（最近一次调用最后一次）：
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 651 行，在 main
train(args, run_opts)
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 287 行，在 train
chain_lib.check_for_required_files(args.feat_dir, args.tree_dir,
文件 &quot;/mnt/d/kaldi/egs/mini_librispeech/s5/steps/libs/nnet3/train/chain_objf/acoustic_model.py&quot;，第 378 行，位于 check_for_required_files
引发异常(&#39;预期 {0} 存在。&#39;.format(file))
异常：预期 exp/chain/tree_sp/ali.1.gz 存在。
]]></description>
      <guid>https://stackoverflow.com/questions/78828864/kaldi-alignment-script-not-generating-output-ali-1-gz</guid>
      <pubDate>Sat, 03 Aug 2024 14:01:13 GMT</pubDate>
    </item>
    <item>
      <title>想要对 SAM 等预训练模型进行微调，以便在微观水样数据集中进行细菌分割。很难找到所需的数据集</title>
      <link>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</link>
      <description><![CDATA[我正在开展一个机器学习项目，旨在使用显微图像测试水的纯度。该项目的目标是：
对样本图像中存在的各种细菌进行分割。识别不同类型的细菌。根据识别出的细菌数量和类型评估水的纯度。
我很难找到并准备一个包含不同类型细菌的水样显微图像的合适数据集。
考虑到我的任务的特殊性，我应该采取什么方法来微调像 SAM（任何分割模型）这样的预训练模型来进行细菌分割？任何关于超参数、数据增强或其他训练策略的提示都会有所帮助。
获取数据集困难。]]></description>
      <guid>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</guid>
      <pubDate>Sat, 03 Aug 2024 11:32:03 GMT</pubDate>
    </item>
    <item>
      <title>当数据集中的每个数据都是 csv 文件时，机器学习方法</title>
      <link>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</link>
      <description><![CDATA[我正在使用传感器测量金属物体周围的磁流。传感器每毫秒记录一次磁流，每秒可进行 1000 次测量。这些值存储在 CSV 文件中，其中第一列表示沿 X 轴的测量值，第二列表示沿 Y 轴的测量值，第三列表示沿 Z 轴的测量值，第四列包含以毫秒为单位的时间。
我的任务是测量 50 种不同金属物体的磁流，为每个物体创建单独的 CSV 文件。最终，我计划将这些单独的文件用作训练机器学习模型的数据集。目标是使用机器学习和这些数据根据金属的磁流特性确定金属的类型（如果您感兴趣，可以搜索“mfl 方法”）。但是，我不确定如何处理这种特殊情况，因为大多数机器学习代码都要求数据集中的每个数据点都是一行。在这种情况下，每个数据点都是一个包含多行的 CSV 文件。
您能提供任何指导吗？
目前我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</guid>
      <pubDate>Sat, 03 Aug 2024 10:09:57 GMT</pubDate>
    </item>
    <item>
      <title>“AttributeError:‘NoneType’对象没有属性‘cget_managed_ptr’”是什么意思？</title>
      <link>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</guid>
      <pubDate>Sat, 03 Aug 2024 06:12:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中有效地将大型 .txt 文件拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</link>
      <description><![CDATA[我有一个非常大的 .txt 文件（几 GB），我需要将其拆分为机器学习项目的训练集和测试集。由于内存限制，将整个文件读入内存然后拆分的常用方法不可行。我正在寻找一种高效拆分文件而不使内存过载的方法。
我尝试使用 scikit-learn 进行拆分，但它会将整个文件加载到内存中，这会导致性能问题，不适合我的大型数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</guid>
      <pubDate>Sat, 03 Aug 2024 03:36:13 GMT</pubDate>
    </item>
    <item>
      <title>独热编码掩码的 resample_poly</title>
      <link>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</link>
      <description><![CDATA[我有这些张量：
X_test = X_unseen_flutter[0,0,:][None, :] # (批次大小，振幅长度) -&gt; (1, 3208)
y_true = y_unseen_flutter[0,0,:][None, :] # (批次大小，掩码长度，类别数量) -&gt; (1, 3208, 4) (独热编码)

我可以对 X_test 进行重新采样，但我不知道 y_true：
from scipy.signal import resample_poly

X_test_resampled = resample_poly(X_test, up=512, down=3208, axis=1) # (1, 512)
y_true_resampled = # ??? 我期望形状 (1, 512, 4)

除了独热编码标签外，resample_poly 的等价物是什么？
我希望有一个函数可以做到这一点，它接受 tensor, up, down, mask_axis, class_axis]]></description>
      <guid>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</guid>
      <pubDate>Sat, 03 Aug 2024 03:21:27 GMT</pubDate>
    </item>
    <item>
      <title>CSV 与 Pandas Dataframe 之间的转换不正确</title>
      <link>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</link>
      <description><![CDATA[我有一个 csv 文件，其中包含标题、文本和 url 列的新闻文章。当我将文件导入 pandas df 时，长度似乎与 csv 文件中的行数不同。经过检查，我注意到一些文章被分成第二行，并进一步分成许多额外的列，所有 url 都位于单个文章的“第二”行的某个较远的列中。Pandas 正确地解释了这个问题，并合并了文本并将 url 放在正确的列下。
虽然 Pandas 正确地解释了这一点，但我无法将 df（清理后）保存到新的 csv，因为新的清理后的 csv 文件不会反映相同的问题，这会破坏清理后的特征工程和 NLP 任务。这些“额外”的行完全搞砸了诸如计算单词/动词/名词以及获取每个句子的被动语态和音节之类的事情。我尝试过检测/删除换行符（\n 和 \r\n），但没有用。我在读取 csv 时尝试过不同的编码和引用值（见下文），但没有用。我尝试过合并行，但做不到，因为 pandas 看不到 csv 中显示的“第二”行。
我遗漏了什么吗？知道发生了什么吗？
df = pd.read_csv(&#39;filepath.csv&#39;, encoding=&#39;utf-8&#39;, engine=&#39;python&#39;, on_bad_lines=&#39;skip&#39;, quoting=1)

df= df.rename(columns={&#39;Headline&#39;: &#39;title&#39;, &#39;Article text&#39;: &#39;text&#39;, &#39;Url&#39;: &#39;url&#39;})

df= df[[&#39;title&#39;, &#39;text&#39;, &#39;url&#39;]]

df[&#39;text&#39;] = df[&#39;text&#39;].str.replace(&#39;\r\n&#39;, &#39; &#39;, regex=True)
df.to_csv(&#39;df_investigation.csv&#39;, index=False) 
]]></description>
      <guid>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</guid>
      <pubDate>Fri, 02 Aug 2024 23:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 Pytorch 的 CrossEntropyLoss 应用类权重来解决多类多输出问题的不平衡数据分类问题</title>
      <link>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</link>
      <description><![CDATA[我正在尝试使用加权损失函数来处理数据中的类别不平衡问题。我的问题是多类别和多输出问题。例如（我的数据有五个输出/目标列（output_1、output_2、output_3），每个目标列有三个类（class_0、class_1 和 class_2）。我目前正在使用 pytorch 的交叉熵损失函数https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html，我看到它有一个权重参数，但我的理解是，这个相同的权重将统一应用于每个输出/目标，但我想在每个输出/目标中为每个类应用单独的权重。
具体来说，我可以获得如下所示的数据



A
B
C
D
E
OUTPUT_1
OUTPUT_2
OUTPUT_3




5.65
3.56
0.94
9.23
6.43
0
2
1


7.43
3.95
1.24
7.22
&lt; td&gt;2.66
0
0
0


9.31
2.42
2.91
2.64
6.28
2
0
2


8.19
5.12
1.32
3.12
8.41
0
2
0


9.35
1.92
3.12
4.13
3.14
0
1
1


8.43
9.72
7.23
8.29
9.18
1
0
2


4.32
2.12
3.84
9.42
8.19
0
1
0


3.92
3.91
2.90
8.1 9
8.41
2
0
2


7.89
1.92
4.12
8.19
7.28
0
1
2
&lt; /tr&gt;

5.21
2.42
3.10
0.31
1.31
2
0
0



因此，
输出 1 中的比例为：0 = 0.6、1 = 0.1、2 = 0.3
输出 2 中的比例为：0 = 0.4、1 = 0.3、2 = 0.3
输出 3 中的比例为：0 = 0.4、1 = 0.2、2 = 0.4

我想根据每个输出列中的类分布应用类权重，以便它重新规范化（或重新平衡？不确定这里要使用的术语是什么）第 1 类为 0.15，第 0 类和第 2 类各为 0.425（因此对于 output_1，权重将是 [0.425/0.6, 0.15/0.1, 0.425/0.3]，对于输出 2，它将是 [0.425/0.4, 0.15/0.3, 0.425/0.3] 等）。相反，我理解 pytorch 的 crossentropy 损失函数中的权重参数目前正在执行的操作是将单个类权重应用于每个输出列。我想知道我是否遗漏了什么，是否有办法使用 pytorch 的 crossentropyloss 函数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</guid>
      <pubDate>Fri, 02 Aug 2024 03:34:55 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg（mlr3）错误：对“prob”的断言失败：包含缺失值（元素 1）[关闭]</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[当我运行代码时，该代码在堆叠学习器（glmnet 和 rpart）上执行特征选择和超参数调整，我收到以下错误消息：
assert_binary(truth, prob = prob, positive = positive, na_value = na_value) 中出错：
“prob”上的断言失败：包含缺失值（元素 1）。
这发生在 PipeOp classif.avg 的 $train()

但是，当我使用 classif.debug 时，预测中没有 NA。任何建议都将不胜感激。
注意：我简化了要调整的参数数量和要选择的特征数量，以减少执行时间，现在使用 classif.debug 只需 15 秒。
这是我的数据：https://www.dropbox.com/scl/fi/hkjs79i89gjz0j5mjlbj8/Data.csv?rlkey=08yuzet3mjr9gcezkryo93vqm&amp;st=hfv2cbeo&amp;dl=0
这是我的代码：
set.seed(1)
data &lt;- read.csv(&quot;C:/Users/Marine/Downloads/Data.csv&quot;)
data &lt;- data[,c(&quot;x&quot;, &quot;y&quot;, &quot;presence&quot;, &quot;V01&quot;, &quot;V02&quot;)]
## dim(data)
data$presence &lt;- as.factor(data$presence)
##摘要（数据）
任务 &lt;- mlr3spatial::as_task_classif_st（x = 数据，目标 = “存在”，正 = “1”，坐标名称 = c（“x”，“y”），crs = “+proj=longlat +datum=WGS84 +no_defs +type=crs”）
摘要（任务）

learner_glmnet &lt;- mlr3::lrn（“classif.glmnet”，预测类型 = “prob”，s = 0.01）
learner_rpart &lt;- mlr3::lrn（“classif.rpart”，预测类型 = “prob”，cp = to_tune（1e-04，1e-1，对数尺度 = TRUE))
learner_glmnet_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_glmnet, id = &quot;glmnet_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))
learner_rpart_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_rpart, id = &quot;rpart_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))

learner_avg &lt;- mlr3pipelines::LearnerClassifAvg$new(id = &quot;classif.avg&quot;)
learner_avg$predict_type &lt;- &quot;prob&quot;
learner_avg$param_set$values$measure &lt;- &quot;classif.auc&quot;

learner_debug &lt;- lrn(&quot;classif.debug&quot;, predict_type = &quot;prob&quot;)

level_0_graph &lt;- mlr3pipelines::gunion(list(learner_glmnet_cv, learner_rpart_cv)) %&gt;&gt;% mlr3pipelines::po(&quot;featureunion&quot;)
level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_avg
## level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_debug
level_0_and_1_graph_learner &lt;- mlr3::as_learner(level_0_and_1_graph)

tuning &lt;- mlr3tuning::auto_tuner(tuner = mlr3tuning::tnr(&quot;grid_search&quot;), 
learner = level_0_and_1_graph_learner,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

feature_selection &lt;- mlr3fselect::auto_fselector(fselector = mlr3fselect::fs(&quot;sequence&quot;, strategies = &quot;sfs&quot;, min_features = 2),
learner = tuning,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

system.time(stacking &lt;- mlr3::resample(task = task, 
learner = feature_selection, 
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
store_models = TRUE))

测试 &lt;- as.data.table(stacking$prediction())
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[1]])
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[2]])
which(is.na(test))
]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>LangChain 与 AmzonBedrock</title>
      <link>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</guid>
      <pubDate>Thu, 04 Jul 2024 06:30:23 GMT</pubDate>
    </item>
    </channel>
</rss>