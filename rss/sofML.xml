<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 07 Feb 2024 15:14:08 GMT</lastBuildDate>
    <item>
      <title>LSTM 模型在时间序列上的实现</title>
      <link>https://stackoverflow.com/questions/77955882/implementation-of-lstm-model-to-a-time-serie</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77955882/implementation-of-lstm-model-to-a-time-serie</guid>
      <pubDate>Wed, 07 Feb 2024 15:08:59 GMT</pubDate>
    </item>
    <item>
      <title>回归树中的主导特征使所有其他特征无关/0 [关闭]</title>
      <link>https://stackoverflow.com/questions/77954893/dominant-feature-in-regression-tree-makes-all-other-features-irrelevant-0</link>
      <description><![CDATA[我正在使用房地产数据集（高基数），因此我选择了 scikit-learn 回归树模型来尝试根据相关特征预测房屋的价格。我有这些功能 [&#39;Suburb&#39;,&#39;Address&#39;, &#39;Distance&#39;, &#39;Bedroom2&#39;, &#39;Bathroom&#39;, &#39;CouncilArea&#39;,&#39;Price&#39;]。
问题是，当我使用 Address it 时，它会使所有其他功能（“郊区”、“距离”、“卧室 2”、“浴室”、“议会区域”）基本上变得多余。地址是一个非常重要的功能，因此我正在尝试将其纳入其中。
我已经多次执行预处理和标准化步骤来尝试不同的结果。我将给出一个基本概述：
第一种方法最少预处理：

将原始 df 通过（这是基本的预处理，考虑 nan vals 并替换它们。）。
目标编码我的非 int （分类变量）并在整个 X （训练和测试）变量上使用标准标量。 （关于 Standardscaler，我也没有使用它，但有一个小的不明显的差异）

第二种方法标准化：

标准预处理（处理和替换 nan val）
评估数据偏差
我通过 Box Cox 运行了高度倾斜/非对称的值（用 IHS 进行了实验（有些值为负值（负值只是跳过了标准化，因为倾斜还不错），其他列一些数据点 == 0 所以我添加了小值（0.01）到 0 值以使它们为正）和 Log 变换）。 Box cox 标准化了所选值的最佳值，因此我使用了这个（请注意，我没有在所有适用的列上使用 Box Cox，仅在具有高偏差的列上使用 Box Cox，不确定这是否是标准流程）
将新结果和其余分类变量附加回 df
目标编码 X 分类变量（训练/测试）
使用标准标量
遍历回归树（使用 max_leaf，3-25 的差异非常小，因此其他特征变得明显，但它们包含 epsilon 值，所以不是真的）

第三种方法，省略地址：

重复第一种和第二种方法
结果还不错，但证明不使用“地址”让位于其他重要功能！

我还使用线性回归模型重复了这些过程，但它并不能很好地捕捉数据的复杂性质。我正在尝试使其适用于回归树。
我还使用线性回归模型重复了这个过程，但它没有很好地捕捉数据的复杂性质。]]></description>
      <guid>https://stackoverflow.com/questions/77954893/dominant-feature-in-regression-tree-makes-all-other-features-irrelevant-0</guid>
      <pubDate>Wed, 07 Feb 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>了解变量选择和调整后的 randomForestSRC 行为</title>
      <link>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</guid>
      <pubDate>Wed, 07 Feb 2024 12:43:24 GMT</pubDate>
    </item>
    <item>
      <title>在网络之间共享权重时联合或单独的优化器</title>
      <link>https://stackoverflow.com/questions/77954817/joint-or-seperate-optimizers-when-sharing-weights-between-networks</link>
      <description><![CDATA[我想知道，这两种情况有什么副作用。

netA 和 netB 共享权重，但各有一个优化器
netA 和 netB 共享权重，但使用单个优化器

结果非常相似，但不相等。
考虑这个例子
导入火炬
将 torch.nn 导入为 nn

火炬.manual_seed(0)

share_fc2 = 真
批量大小 = 4
通道 = 2

Training_data_A = torch.rand((batch_size, 通道))
Training_data_B = torch.rand((batch_size, 通道))


类 Net(torch.nn.Module):
    “”“最小网络”“”

    def __init__(self) -&gt;; __init__(self) -&gt;没有任何：
        超级().__init__()
        self.fc1 = nn.Linear(通道、通道、偏差=False)
        self.fc2 = nn.Linear(通道、通道、偏差=False)

    def 前向（自身，x）：
        返回 self.fc2(self.fc1(x))


netA = Net().requires_grad_()
netB = Net().requires_grad_()

如果共享_fc2：
    # 将 fc2 替换为 netA.fc2
    netB.fc2 = netA.fc2

lossA = netA(training_data_A).mean()
lossB = netA(training_data_A).mean()

lossA.backward()
lossB.backward()

选项 1
# 选项 1（独立运行，使用种子）
optA = torch.optim.Adam(params=netA.parameters()) # 包含共享的 fc2
optB = torch.optim.Adam(params=netB.parameters()) # 包含共享的 fc2
optA.step()
optB.step()

打印（列表（netA.parameters（）））
打印（列表（netB.parameters（）））

输出

选项 2
opt = torch.optim.Adam(
    参数=(
        list(netA.parameters()) # 包含 netA.fc1、netA/B.fc2
        + list(netB.parameters())[:1] # 包含 netA.fc1
    ）
）
opt.step()
打印（列表（netA.parameters（）））
打印（列表（netB.parameters（）））


输出

问题

是Adam内部参数调整造成的差异，可以忽略
差异是由于梯度计算和应用中的数学原因造成的吗？
一种选择比另一种更好吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77954817/joint-or-seperate-optimizers-when-sharing-weights-between-networks</guid>
      <pubDate>Wed, 07 Feb 2024 12:42:07 GMT</pubDate>
    </item>
    <item>
      <title>如何改进空间约束的凝聚层次聚类，使 99% 的点不落入单个聚类？</title>
      <link>https://stackoverflow.com/questions/77954721/how-to-improve-spatially-constrained-agglomerative-hierarchical-clustering-so-99</link>
      <description><![CDATA[我尝试运行以下代码，按照标题所述操作 - 但如何诊断我的数据以避免 99% 的点集中到单个集群中？
# 标准化上下文字符
cont_std = RobustScaler().fit_transform(df)
cont_std = pd.DataFrame(cont_std, columns=df.columns)

％％时间
对于 K 中的 k：
    s[k] = []
    分贝[k] = []
    np.random.seed(123456) #为了重现性
    模型 = AgglomerativeClustering(linkage=&#39;ward&#39;, 连接性=w.sparse, n_clusters=k)
    y = model.fit(cont_std)
    cont_std_[&#39;AHC_k&#39;+ str(k)] = y.labels_
    print(&#39;k=&#39; + str(k) + &#39;: &#39; + str(metrics.silhouette_score(cont_std, y.labels_, metric=&#39;euclidean&#39;))) 处的剪影
    s[k].append(metrics.silhouette_score(cont_std, y.labels_, metric=&#39;euclidean&#39;))
    
    davies_bouldin_score =metrics.davies_bouldin_score(cont_std, y.labels_)
    print(f&#39;davies boudin at k={k}: {davies_bouldin_score}&#39;)
    db[k].append(davies_bouldin_score)
]]></description>
      <guid>https://stackoverflow.com/questions/77954721/how-to-improve-spatially-constrained-agglomerative-hierarchical-clustering-so-99</guid>
      <pubDate>Wed, 07 Feb 2024 12:29:46 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Meta Deep Cluster模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77954005/how-to-use-meta-deep-cluster-model</link>
      <description><![CDATA[我正在开展一个关于图像分割的机器学习项目。
我发现 Meta 的一些模型可以从图像中学习视觉特征，例如 https://github.com/facebookresearch /deepcluster，这个：https://github.com/facebookresearch/dinov2 或这个： https://github.com/facebookresearch/swav/tree/main .
我尝试使用这个模型，但我不明白如何将它与我的图像数据集一起使用（不是他们提供的图像）。
有类似的指南或类似的东西吗？]]></description>
      <guid>https://stackoverflow.com/questions/77954005/how-to-use-meta-deep-cluster-model</guid>
      <pubDate>Wed, 07 Feb 2024 10:38:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习Python：预测看不见的输入以获得最佳输出（任何算法都可以）</title>
      <link>https://stackoverflow.com/questions/77953955/machine-learning-python-predict-unseen-inputs-for-optimum-output-any-algorith</link>
      <description><![CDATA[我用随机森林算法（你可以使用任何算法）编写了一个机器学习模型，并用我的数据集成功地训练了它，它准确地预测了我的测试集的输出（y）输入 (x)。到这里为止它工作正常。现在我想添加一个部分，它还可以预测新的 x 的 ungiven 值，它认为 y 将是最大值。所以我希望它基本上针对任何 x 值优化 y，而不需要给它输入 x。
如果需要的话，这是我的代码：（其中没有关于我想添加的部分，它只是训练和测试）：
&lt;前&gt;&lt;代码&gt;

将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 r2_score
将 matplotlib.pyplot 导入为 plt
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.ensemble 导入 RandomForestRegressor


excel_file_path = r&#39;文件位置&#39;
df = pd.read_excel(excel_file_path)
df.columns=[&#39;Chordwise_Portion&#39;,&#39;变形&#39;,&#39;TSR&#39;,&#39;CP/CP_baseline&#39;]
df[&#39;CP/CP_baseline&#39;] = pd.to_numeric(df[&#39;CP/CP_baseline&#39;], 错误=&#39;强制&#39;)

训练集 = df.iloc[0:350, 0:4]
test_set = df.iloc[350:649, 0:4]


defscale_dataset（数据框）：
  X = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values
  定标器=标准定标器()
  X = 缩放器.fit_transform(X)
  数据 = np.hstack((X, np.reshape(y, (-1, 1))))
  返回数据，X，y

训练，X_train，y_train =scale_dataset（训练集）
测试，X_测试，y_测试=scale_dataset（测试集）


#射频
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)


# 计算 R 平方
r2 = r2_score(y_test, y_pred)
r2 = r2*100
打印（）
print(f&#39;R平方: {r2} %&#39;)
打印（）


#最优预测
最大训练=最大（y_训练）
index_max_train = np.where(y_train == np.max(y_train)) ###
max_pred = max(y_pred)
index_max_pred = [np.where(y_pred == max_pred)[0][0]+350]
data_pred = df.iloc[index_max_pred,0:3] ###

######
max_value = max(max_train, max_pred)
如果 max_value == max_train：
  max_data = df.iloc[index_max_train]
别的：
  最大数据 = 数据预测

print(&quot;Cp/Cp_baseline 的最大总体值为：&quot;, max_value,&quot; 对于以下条件：&quot;)
打印（最大数据）
打印（）
print(&quot;Cp/Cp_baseline 的最大预测值为：&quot;, max_pred,&quot; 对于以下条件：&quot;)
打印（数据预测）

# 绘制结果
plt.scatter(X_test[:, 0], y_test, label=&#39;真实数据&#39;)
plt.scatter(X_test[:,0], y_pred, color=&#39;r&#39;, label=&#39;预测数据&#39;)
plt.xlabel(&#39;Chordwise_portion&#39;)
plt.ylabel(&#39;Cp/Cp_baseline&#39;)
title = &quot;随机森林算法 - R^2 = {:.4f} %&quot;.format(r2)

plt.标题（标题）
plt.图例()
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/77953955/machine-learning-python-predict-unseen-inputs-for-optimum-output-any-algorith</guid>
      <pubDate>Wed, 07 Feb 2024 10:30:18 GMT</pubDate>
    </item>
    <item>
      <title>重新识别网络训练和验证[关闭]</title>
      <link>https://stackoverflow.com/questions/77953442/re-identification-network-training-and-validation</link>
      <description><![CDATA[我正在为 VisDrone 执行对象跟踪任务，并且为此目的使用 DeepSORT  。但是该存储库中的嵌入程序在与 VisDrone 数据集非常不同的数据集上进行了预训练。
我想在 VisDrone 上训练 mobilenetv2 作为 ReID 的嵌入网络。据我了解，如果只有 1 个摄像头，我的训练数据集必须包含一组{图像（对象的），标签（该对象的 ID）}。因此，如果以交叉熵损失的方式进行训练，网络将学习对象的 ID。但是，如果我尝试使用 ID 不同的其他对象来验证它，网络将不会预测验证数据集中的 ID。相反，它会预测他们的 ID。训练数据集和验证损失会很高。
我的第一个问题：我关于网络学习对象ID的想法是否正确？因为到处都写着 embedeer 返回对象的特征向量，其长度可能会有所不同，而不考虑 ID 的总数。
我的第二个问题：如何正确验证我的 ReID 网络？
如果有人向我解释为什么学习对象的 ID，然后在另一个数据集上使用这个网络通常是有效的，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77953442/re-identification-network-training-and-validation</guid>
      <pubDate>Wed, 07 Feb 2024 09:10:08 GMT</pubDate>
    </item>
    <item>
      <title>我该如何进行模型预测？</title>
      <link>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</guid>
      <pubDate>Wed, 07 Feb 2024 02:51:25 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 60 万条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中实现大数据集的交叉验证而不将整个数据集加载到内存中？</title>
      <link>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</link>
      <description><![CDATA[我目前正在处理一个机器学习项目的大型数据集，并选择使用 TensorFlow 的 tf.data API 来高效管理数据加载和预处理，而无需将整个数据集加载到内存中。这种方法对于我的初始训练效果很好。
但是我很难实现交叉验证。据我了解，TensorFlow 本身并不支持直接通过 tf.data API 进行交叉验证，而与 Keras 集成进行交叉验证似乎需要先将数据加载到内存中。这对我的使用来说是有问题的，因为立即将整个数据集加载到内存中违背了使用 tf.data 的目的。
我正在寻找一种解决方法或方法来实现与 TensorFlow 的按需数据加载兼容的交叉验证。理想情况下，我希望保持 tf.data 的内存效率，同时对模型的评估进行交叉验证。
有没有办法使用 Keras 或任何其他库进行交叉验证，而不需要我将所有数据集加载到内存中？]]></description>
      <guid>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</guid>
      <pubDate>Tue, 06 Feb 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>不知道如何在此机器学习程序中进行用户输入[关闭]</title>
      <link>https://stackoverflow.com/questions/77937096/dont-know-how-to-do-user-input-in-this-ml-program</link>
      <description><![CDATA[导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入 r2_score、mean_squared_error

# 加载数据集
dp = pd.read_csv(&#39;https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv&#39;)

# 分离特征（x）和目标变量（y）
y = dp[&#39;logS&#39;]
x = dp.drop(&#39;logS&#39;, 轴=1)

# 分割数据
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)

# 线性回归模型
lr = 线性回归()
lr.fit(x_train, y_train)
y_train_pred_lr = lr.predict(x_train)
y_test_pred_lr = lr.predict(x_test)

# 随机森林回归模型
k1 = RandomForestRegressor（最大深度=2，随机状态=100）
k1.fit(x_train, y_train)
y_train_pred_rf = k1.predict(x_train)
y_test_pred_rf = k1.predict(x_test)

# 评估线性回归模型
y_train_mse_lr =mean_squared_error(y_train, y_train_pred_lr)
y_train_r2_lr = r2_score(y_train, y_train_pred_lr)
y_test_mse_lr = 均方误差(y_test, y_test_pred_lr)
y_test_r2_lr = r2_score(y_test, y_test_pred_lr)

# 评估随机森林回归模型
y_train_mse_rf =mean_squared_error(y_train, y_train_pred_rf)
y_train_r2_rf = r2_score(y_train, y_train_pred_rf)
y_test_mse_rf =mean_squared_error(y_test, y_test_pred_rf)
y_test_r2_rf = r2_score(y_test, y_test_pred_rf)

# 创建数据框
rs_lr = pd.DataFrame({“方法”: [“线性回归”],
                      “训练MSE”：[y_train_mse_lr]，
                      “训练R2”：[y_train_r2_lr]，
                      “测试 MSE”：[y_test_mse_lr]，
                      “测试 R2”：[y_test_r2_lr]})

rs_rf = pd.DataFrame({“方法”: [“随机森林回归器”],
                      “训练MSE”：[y_train_mse_rf]，
                      “训练R2”：[y_train_r2_rf]，
                      “测试 MSE”：[y_test_mse_rf]，
                      “测试 R2”：[y_test_r2_rf]})

# 连接数据帧
结局 = pd.concat([rs_lr, rs_rf],ignore_index=True)
打印（结局）

加载数据集：它使用 Pandas 从 URL 加载数据集。该数据集与分子溶解度相关，包含各种分子描述符。
数据准备：它将特征（x）和目标变量（y）从数据集中分离出来。本例中的目标变量是溶解度的对数 (logS)。
数据拆分：它使用 scikit-learn 中的 train_test_split 函数将数据集拆分为训练集和测试集。训练集用于训练模型，测试集用于评估模型的性能。
模型训练：它使用训练数据训练两个回归模型 - 线性回归模型 (lr) 和随机森林回归模型 (k1)。
预测：它使用经过训练的模型对训练集和测试集进行预测。
模型评估：它使用均方误差 (MSE) 和 R 平方 (R2) 分数评估两个模型的性能。这些指标可以深入了解模型对数据的拟合程度。
创建 DataFrame：它创建两个单独的 DataFrame（rs_lr 和 rs_rf）来存储每个模型的评估指标。
串联：它将两个 DataFrame 连接成一个最终的 DataFrame（结局）。此 DataFrame 总结了两种模型的训练和测试性能。
打印结果：最后，它打印串联的 DataFrame（结局），其中包括线性回归和随机森林回归模型的方法名称、训练 MSE、训练 R2、测试 MSE 和测试 R2。
该程序的目标是比较线性回归和随机森林回归模型在根据描述符预测分子溶解度方面的性能。
我希望用户输入值。那么我该如何修改代码呢？]]></description>
      <guid>https://stackoverflow.com/questions/77937096/dont-know-how-to-do-user-input-in-this-ml-program</guid>
      <pubDate>Sun, 04 Feb 2024 18:32:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的决策树创建的分割实际上并未划分样本？</title>
      <link>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</link>
      <description><![CDATA[这是我对著名的 Iris 数据集进行二特征分类的基本代码： 
from sklearn.datasets import load_iris
从 sklearn.tree 导入 DecisionTreeClassifier，export_graphviz
从 graphviz 导入源

虹膜 = load_iris()
iris_limited = iris.data[:, [2, 3]] # 这仅获取花瓣长度 &amp;宽度。

# 我使用最大深度来避免过度拟合
# 并简化树，因为我将其用于教育目的
clf = DecisionTreeClassifier(criterion=&quot;基尼&quot;,
                             最大深度=3，
                             随机状态=42）

clf.fit(iris_limited, iris.target)

Visualization_raw = Export_graphviz(clf,
                                    out_file=无，
                                    特殊字符=真，
                                    feature_names=[“长度”,“宽度”],
                                    类名=iris.target_names,
                                    节点 ID=真）

可视化源 = 源（可视化_原始）
Visualization_png_bytes = Visualization_source.pipe(format=&#39;png&#39;)
将 open(&#39;my_file.png&#39;, &#39;wb&#39;) 作为 f：
    f.write（可视化_png_bytes）

当我检查树的可视化时，我发现了这一点：

乍一看这是一棵相当正常的树，但我注意到它有一些奇怪的地方。节点 #6 共有 46 个样本，其中只有一个是杂色的，因此该节点被标记为 virginica。这似乎是一个相当合理的停留地点。然而，由于某种我无法理解的原因，该算法决定进一步分为节点#7 和#8。但奇怪的是，仍然存在的 1 个 versicolor 仍然被错误分类，因为无论如何两个节点最终都具有 virginica 类。它为什么要这样做？它是否盲目地只关注基尼系数的下降，而不考虑它是否有任何影响——这对我来说似乎是奇怪的行为，而且我在任何地方都找不到它的记录。
是否可以禁用，或者这实际上是正确的吗？]]></description>
      <guid>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</guid>
      <pubDate>Sat, 28 Apr 2018 14:25:02 GMT</pubDate>
    </item>
    <item>
      <title>RF：一个类别的 OOB 准确度较高，而另一个类别的准确度非常低，类别不平衡较大</title>
      <link>https://stackoverflow.com/questions/10306380/rf-high-oob-accuracy-by-one-class-and-very-low-accuracy-by-the-other-with-big</link>
      <description><![CDATA[我正在使用随机森林分类器对具有两个类别的数据集进行分类。

特征数量为 512 个。
数据比例为1:4。即，75% 的数据来自第一类，25% 来自第二类。
我使用了 500 棵树。

分类器产生 21.52% 的袋外错误。
第一类（由 75% 的训练数据表示）的每类误差为 0.0059。而第二类的分类误差非常高：0.965。
我正在寻找对此行为的解释，以及您是否有提高第二类准确性的建议。
我期待您的帮助。
谢谢
忘记说我正在使用 R 并且在上面的测试中使用了 1000 的节点大小。
这里我只用 10 棵树和节点大小 = 1 重复训练（只是为了给出一个想法），下面是 R 中的函数调用和混淆矩阵：

randomForest（公式 = Label ~ .，数据 = chData30PixG12，ntree = 10，重要性 = TRUE，节点大小 = 1，keep.forest = FALSE，do.trace = 50）

随机森林的类型：分类

树木数量：10

没有。每次分割尝试的变量数：22

OOB 错误率估计：24.46%

混淆矩阵：


 不相关、相关、class.error


不相关 37954、4510、0.1062076

相关 8775、3068、0.7409440

]]></description>
      <guid>https://stackoverflow.com/questions/10306380/rf-high-oob-accuracy-by-one-class-and-very-low-accuracy-by-the-other-with-big</guid>
      <pubDate>Tue, 24 Apr 2012 21:37:50 GMT</pubDate>
    </item>
    </channel>
</rss>