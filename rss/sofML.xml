<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 23 Jul 2024 06:21:56 GMT</lastBuildDate>
    <item>
      <title>机器学习（逻辑回归）使用数组作为特征/独立变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78781575/machine-learning-logistic-regression-using-an-array-as-a-feature-independent-v</link>
      <description><![CDATA[抱歉，如果这是一个更一般/初学者的问题，但我在网上搜索答案时没有找到任何运气 - 也许我在谷歌上搜索了错误的东西。
所以本质上假设我有一个数据框：
-主题ID（int）
-年龄（int）
-性别（int 1-代表男性，2-代表女性）
-Pearson CC代表功能网络（矩阵）
这意味着我有一个PearsonCC数组。例如：



p_id
性别
PearsonCC




128_S_0200
M
[0.5052435694128596, 0.3375816208945487, 0.206...


003_S_0908
F
[-0.18955977794142087, 0.01652734870786999, -0...


141_S_1052
F
[0.0562331642358682, 0.5698911953687733, -0.17... -0...



所以我明白我们需要展平矩阵/矢量化上三角
然后将其转换为 numpy 以便进行特征缩放。我尝试过类似这样的方法：
X_mat_train = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
X_mat_test = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
我的问题是：
在使用它来拟合模型时，我是否应该将向量转换回原始形式？
因为我在网上搜索时得到了混合的结果。我很困惑，不知道对于这个特定的列，什么格式最好？
任何帮助都非常感谢！
如果我使用 sklearn 的 Logistic 回归：
model = LogisticRegression(max_iter=1000)
model.fit(X_train_final, y_train)
我收到一个错误，提示 PearsonCC 列可能只能是 int/float：
TypeError Traceback（最近一次调用最后一次）
TypeError：只有 size-1 数组可以转换为 Python 标量
为了扩展该功能，这是我的方法：
scaler = StandardScaler()

X_mat_train = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
X_mat_test = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)

X_mat_train_scaled = scaler.fit_transform(X_mat_train)
X_mat_test_scaled = scaler.transform(X_mat_test)


我只是不确定如何将这个 X_mat_train 转换回 ML/Regression 可接受的格式]]></description>
      <guid>https://stackoverflow.com/questions/78781575/machine-learning-logistic-regression-using-an-array-as-a-feature-independent-v</guid>
      <pubDate>Tue, 23 Jul 2024 04:35:17 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch LSTM 模型上的 CrossEntropyLoss 每个时间步进行一个分类</title>
      <link>https://stackoverflow.com/questions/78781313/crossentropyloss-on-pytorch-lstm-model-with-one-classification-per-timestep</link>
      <description><![CDATA[我正在尝试制作一个 LSTM 模型，用于检测时间序列数据中的异常。它需要 5 个输入并产生 1 个布尔输出（如果检测到异常则为 True/False）。异常模式通常连续出现在 3 - 4 个时间步之间。与大多数 LSTM 示例不同，它们预测未来数据或对整个数据序列进行分类，我试图在每个时间步长输出一个 True/False 检测标志（如果检测到，则在模式中的最后一个时间步长点输出 True）。
不幸的是，CrossEntropyLoss 似乎不允许超过 1D 的输出张量，在这种情况下它将是 2D [序列数，序列长度和布尔数据]
以下是我正在尝试生成的一些示例代码：
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 定义 LSTM 分类器模型
class LSTMClassifier(nn.Module):
def __init__(self, input_size, hidden_​​size, num_layers, output_size):
super(LSTMClassifier, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(x.device)
c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(x.device)
out, _ = self.lstm(x, (h0, c0))
out = self.fc(out[:, -1, :])
return out

# 输入 - 100 个示例，每个时间步包含 5 个数据点（其中有 10 个时间步）
X_train = np.random.rand(100, 10, 5)
# 输出 - 100 个示例，每个时间步包含 1 个 True/False 输出以匹配输入
y_train = np.random.choice(a=[True, False], size=(100, 10)) # 二进制标签（True 或 False）

# 将数据转换为 PyTorch 张量
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.bool)

# 定义模型参数
input_size = X_train.shape[2] # 每个时间步 5 个输入
hidden_​​size = 4 # 我们尝试检测的模式通常为 4 个时间步长
num_layers = 1
output_size = 1 # True/False

# 实例化模型
model = LSTMClassifier(input_size, hidden_​​size, num_layers, output_size)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 10
for epoch in range(num_epochs):
optimizer.zero_grad()
output = model(X_train_tensor)
loss = criterion(outputs, y_train_tensor)
loss.backward()
optimizer.step()
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}&#39;)

# 测试模型
X_test = np.random.rand(10, 10, 5) # 生成一些测试数据 - 与输入相同的维度
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
with torch.no_grad():
predictions = model(X_test_tensor)
predict_outputs = torch.argmax(predictions, dim=1)
print(&quot;Predicted Outputs:&quot;, predict_outputs)

我是否需要重新调整输出（或者使 LSTM 的输出数量等于序列长度），或者使用不同的损失函数，或者使用 LSTM 以外的模型？]]></description>
      <guid>https://stackoverflow.com/questions/78781313/crossentropyloss-on-pytorch-lstm-model-with-one-classification-per-timestep</guid>
      <pubDate>Tue, 23 Jul 2024 02:11:04 GMT</pubDate>
    </item>
    <item>
      <title>人工智能是出路吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78781284/is-artificial-intelligence-the-way-to-go</link>
      <description><![CDATA[大家好吗？我是一名电影行业专业人士，每个人都知道，目前从事这个行业很糟糕。有一件事让我着迷，那就是人工智能。我认为它远没有现在被描绘成的可怕流行语那么可怕。就我个人而言，我相信对于适应的电影制作人来说，它将是一个很好的工具。问题是，大多数人不会因为恐惧而适应。
话虽如此，我想更多地了解人工智能。我不知道从哪里开始，在这方面我完全是个新手。我希望真正精通它，这样我在未来几年就会有优势。有人有什么建议吗？我应该学习什么关于人工智能的知识？我应该学习如何使用生成式人工智能吗？如何“制造”生成式人工智能？目标应该是放弃电影制作事业，转而进入一家更依赖人工智能的公司吗？
请记住，我是个完全的新手。目标是获得一套有用的新技能。
谢谢！
我还没有尝试任何东西，因为我不确定如何开始。我应该进入机器学习领域吗？我应该学习 Python 吗？有什么期刊我应该关注吗？]]></description>
      <guid>https://stackoverflow.com/questions/78781284/is-artificial-intelligence-the-way-to-go</guid>
      <pubDate>Tue, 23 Jul 2024 01:55:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 React Native 中运行 FaceNet 或机器学习</title>
      <link>https://stackoverflow.com/questions/78781135/cannot-run-facenet-or-machine-learning-in-react-native</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78781135/cannot-run-facenet-or-machine-learning-in-react-native</guid>
      <pubDate>Tue, 23 Jul 2024 00:19:25 GMT</pubDate>
    </item>
    <item>
      <title>CNN 的可变大小输入</title>
      <link>https://stackoverflow.com/questions/78781105/variable-size-input-for-cnn</link>
      <description><![CDATA[我想创建一个可以猜测图像序列中下一张图像的 AI，我的想法是将 1、2、3、...、n-1 张图像作为输入提供给 CNN，然后通过 LSTM 运行它，输出是第 2、3、4...、第 n 张图像。
因此，我的想法是，一个 epoch 将是
步骤#1：输入：第一张图像，输出是第二张图像
步骤#2：输入：第一张和第二张图像，输出是第三张图像
步骤#3：输入：第一张、第二张和第三张图像，输出是第四张图像
.
.
.
最后一步：输入：第 1、2、3、...、第 (n-1) 张图像，输出是第 n（最后）张图像
然后我会通过将所有 n 张图像作为输入来预测第 (n+1) 张图像的输出
我以前从未使用过 LSTM，但我读到它可以处理可变的输入大小，我的问题是它需要为卷积层提供可变的输入大小，这有可能以某种方式工作吗？
我知道如何制作 CNN，但我不知道是否有可能拥有具有可变输入大小的 CNN]]></description>
      <guid>https://stackoverflow.com/questions/78781105/variable-size-input-for-cnn</guid>
      <pubDate>Mon, 22 Jul 2024 23:58:33 GMT</pubDate>
    </item>
    <item>
      <title>Apache Flink 中的依赖管理和执行环境</title>
      <link>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</link>
      <description><![CDATA[我们正在评估 apache flink 是否可用于部署流式机器学习应用程序。
apache flink 中如何处理依赖管理，尤其是执行环境？
想象一下，具有不同依赖关系的 Python 任务应提交给 Flink 集群。
我们只看到 Flink 任务管理器可以使用 Python 虚拟环境处理依赖管理。当每个任务都有不同的依赖关系时，我们是否应该为每个任务部署一个新的任务管理器？
从容器设置开始，我们可以在单独的 Docker 映像中部署每个任务。
使用 apache flink 时通常如何处理这个问题？我们没有看到 Flink 擅长处理需要特定依赖关系的大量任务，但希望利用流式处理器。]]></description>
      <guid>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</guid>
      <pubDate>Mon, 22 Jul 2024 19:46:27 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 TensorFlow 中实现用于单目深度估计的 MiDas 模型，但距离值超出了图表范围 [关闭]</title>
      <link>https://stackoverflow.com/questions/78780498/trying-to-implement-the-midas-model-for-monocular-depth-estimation-in-tensorflow</link>
      <description><![CDATA[从 Kaggle 下载了 Tflite MiDas 模型。
查看了几篇关于 MiDas 和单目深度估计的文章（基本上是使用像素找到物体与相机的距离）
这是使用边界框进行物体检测的。
我试图获取边界框的中心并从那里获取距离。并尝试修复一些错误，但
我仍然不确定我是否已完成所有正确的步骤或正确实施了它。 - 请检查我是否忘记做某事。
示例输出：（我当时就在摄像头前面）
最小深度值：88.5720443725586，最大深度值：911.3470458984375 (126, 127) 处的深度值：248.00 米 最小深度值：76.10022735595703，最大深度值：920.8251342773438 (126, 127) 处的深度值：246.00 米 最小深度值：72.52698516845703，最大深度值：916.8196411132812 (126, 127) 处的深度值：246.00米 最小深度值：68.72769165039062，最大深度值：908.2048950195312 (126, 127) 处的深度值：247.00 米 最小深度值：67.79693603515625，最大深度值：898.6953125 (126, 127) 处的深度值：248.00 米 最小深度值：69.44243621826172，最大深度值：902.6591186523438 (126, 127) 处的深度值：250.00 米 最小深度值：78.3673324584961，最大深度值：914.4412231445312 深度(126, 127) 处的值：249.00 米
以下代码：
#加载模型
interpreter = tf.lite.Interpreter(model_path=&quot;/Users/Taparia/Desktop/ObjectDetection/1.tflite&quot;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()
output_details = interpretationer.get_output_details()

#加载视频
cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)
cap.set(10,70)
while True:
success,img = cap.read()
if not success: break
classIds, confs, bbox = net.detect(img,confThreshold=thres)
#print(classIds,bbox)

#准备用于距离测量的图像
img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
img_rgb_resized = cv2.resize(img_rgb,(256,256))
img_tensor = np.expand_dims(img_rgb_resized.astype(np.float32)/255.0, axis=0)

#估计深度
explainer.set_tensor(input_details[0][&#39;index&#39;], img_tensor)
explainer.invoke()
depth_map = explainer.get_tensor(output_details[0][&#39;index&#39;])[0]
print(f&quot;最小深度值：{np.min(depth_map)}, 最大深度值： {np.max(depth_map)}&quot;)

# 标准化深度图以进行可视化
depth_normalized = cv2.normalize(depth_map,None,alpha=0, beta=255,norm_type=cv2.NORM_MINMAX)
depth_normalized = np.uint8(depth_normalized)

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):
class_name = classNames[classId - 1].lower()
if class_name in used_objects:
cv2.rectangle(img, box, (0, 0, 255), 1) # 在对象周围绘制方框
cv2.putText(img, classNames[classId - 1].upper(), (box[0] + 10, box[1] + 30),
cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)
x_start, y_start, x_end, y_end = box

# 计算深度图坐标中边界框的中心
center_x = int((x_start + x_end) / 2 * 256 / img.shape[1])
center_y = int((y_start + y_end) / 2 * 256 / img.shape[0])
#需要将图像大小调整为 (256,256)
# 确保坐标在depth_map的范围内
if 0 &lt;= center_x &lt;depth_normalized.shape[1] and 0 &lt;= center_y &lt; depth_normalized.shape[0]:
depth_value =depth_normalized[center_y, center_x].item()
print(f&quot;({center_x}, {center_y}) 处的深度值：{depth_value:.2f} 米&quot;)

# 检查物体是否靠近
ifdepth_value&lt; 1.0：# 示例阈值
print(&quot;对象非常接近&lt;)
cv2.putText(img, &quot;Close Object&quot;, (box[0], box[1] - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
else:
print(&quot;深度图坐标超出范围&lt;)

我所做的所有尝试都发现，在以下行中
if 0 &lt;= center_x &lt;depth_normalized.shape[1] and 0 &lt;= center_y &lt; depth_normalized.shape[0]:
depth_value =depth_normalized[center_y, center_x].item()
print(f&quot;({center_x}, {center_y}) 处的深度值：{depth_value:.2f} 米&quot;)

最初我执行的是depth_value =depth_map[center_y, center_x].item()，但无论我输入的是depth_map还是depth_normalized，输出结果都没有区别。这很奇怪，因为我预期会有区别，但结果却无济于事。那里出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78780498/trying-to-implement-the-midas-model-for-monocular-depth-estimation-in-tensorflow</guid>
      <pubDate>Mon, 22 Jul 2024 19:33:26 GMT</pubDate>
    </item>
    <item>
      <title>带偏移的增强回归树模型中的错误</title>
      <link>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</link>
      <description><![CDATA[我正在使用增强回归树模型 (BRT) 来预测海豚物种出现的概率。为此，我有存在/不存在数据和几个环境变量。发生数据被分成几段，我想使用段的长度作为模型中的偏移量，以说明数据收集的工作量。
我使用 dismo 包中的 gbm.step() 函数来拟合模型。
brt&lt;-gbm.step(data=df, gbm.x=c(6,7,12,42,43,15,45,53,93,41,81,87,97), gbm.y = 4, offset = offset, family=&quot;bernoulli&quot;, tree.complexity=3, learning.rate = 0.01, bag.fraction = 0.6)

我将偏移量定义为 log(length of段）
offset&lt;-log(df$eff_length)

&gt; head(offset)
[1] 9.017928 9.171184 9.239406 9.367430 9.264165 9.233178
&gt; summary(offset)
最小值 第 1 区 中位数 平均值 第 3 区 最大值。
8.987 9.065 9.146 9.181 9.265 9.615 

当我运行模型时，它会反复出现此警告：
gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], 中出现警告：
predict.gbm 不会将偏移量添加到预测值中。

随后出现此错误：
if (cv.loss.values[j] &gt; cv.loss.values[j - 1]) { 中的错误：
需要 TRUE/FALSE 的值缺失

我尝试包含 fold.vector=offset，因为看起来错误与交叉验证过程，但它给了我相同的警告和错误。
我在这里遗漏了什么？它与伯努利分布类型有关吗？]]></description>
      <guid>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</guid>
      <pubDate>Mon, 22 Jul 2024 17:51:21 GMT</pubDate>
    </item>
    <item>
      <title>如何针对 NYU 数据集训练 Yolo 3D</title>
      <link>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</guid>
      <pubDate>Mon, 22 Jul 2024 16:09:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有 keras 的情况下根据经过训练的 keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</link>
      <description><![CDATA[我有一个使用 keras 创建和训练的 1D CNN，并且我将权重保存在 h5 文件中，将架构保存在 JSON 文件中。现在，我希望能够读取架构和权重，并使用它们进行新的预测，独立于 Keras 或 TensorFlow。我只想使用 numpy、scipy 等基本软件包，但如果有任何其他软件包可以以最少的依赖性完成此操作，那么我想使用它。我找到了 Konverter，但它不适用于 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</guid>
      <pubDate>Mon, 22 Jul 2024 15:53:20 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮我解决这个基本的机器学习问题吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78753768/can-anyone-help-me-with-this-basic-ml-question</link>
      <description><![CDATA[ MCQ 问题 
我想找到这个关于 ML 的基本问题的答案。你能帮我吗？我希望了解机器学习中的关键概念和技术，例如算法、数据预处理、模型训练和评估。如果您有任何提示或资源可以帮助我很好地掌握这些主题并将它们应用于实际场景，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/78753768/can-anyone-help-me-with-this-basic-ml-question</guid>
      <pubDate>Tue, 16 Jul 2024 09:36:19 GMT</pubDate>
    </item>
    <item>
      <title>使用注释数据进行图像分类</title>
      <link>https://stackoverflow.com/questions/78386605/image-classification-using-annotated-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78386605/image-classification-using-annotated-data</guid>
      <pubDate>Thu, 25 Apr 2024 18:31:50 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 1 个特征，但 LinearRegression 需要 2 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</link>
      <description><![CDATA[我正在使用 pywebio 为我的机器学习程序创建一个小型脚本运行用户界面。当不使用小型 UI 时，运行线性回归 predict() 函数时没有任何错误。
UI 正在从用户那里检索两个数字，一个 &#39;age&#39; 和一个 &#39;salary&#39;。这两个数字被输入到一个 numpy 数组中，并且 numpy 数组已从 1D 数组重塑为 2D 数组，因为我在 numpy 数组形状上收到错误。
现在，我收到一条错误消息，提示 predict() 方法仅接收 1 个特征而不是 2 个，而 sklearn 文档指出线性回归 predict() 方法始终获取“self”和另一个特征。我该如何修复此错误？
这是我的 UI 代码：
age = int(input(&quot;输入您的年龄：&quot;, type=NUMBER))
salary = int(input(&quot;输入您的薪水：&quot;, type=NUMBER))

entry = np.array([age, salary])
reshaped_entry = entry.reshape(-1, 1)

estimate = regr.predict(reshaped_entry) 

以下是错误消息：
ValueError Traceback (most recent call last)
Input In [21], in &lt;cell line: 22&gt;()

Input In [21], in retired_ui()

文件~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:362，位于 LinearModel.predict(self, X) 中
348 def predict(self, X):
349 &quot;&quot;&quot;
350 使用线性模型进行预测。
351 
(...)
360 返回预测值。
361 &quot;&quot;&quot;
--&gt; 362 return self._decision_function(X)

文件 ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:345，位于 LinearModel._decision_function(self, X) 中
342 def _decision_function(self, X):
343 check_is_fitted(self)
--&gt; 345 X = self._validate_data(X, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;], reset=False)
346 return safe_sparse_dot(X, self.coef_.T, density_output=True) + self.intercept_

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:585, in BaseEstimator._validate_data(self, X, y, reset, valid_separately, **check_params)
582 out = X, y
584 if not no_val_X and check_params.get(&quot;ensure_2d&quot;, True):
--&gt; 585 self._check_n_features(X, reset=reset)
587 return out

File ~\anaconda3\lib\site-packages\sklearn\base.py:400, in BaseEstimator._check_n_features(self, X, reset)
397 return
399 if n_features != self.n_features_in_:
--&gt; 400 raise ValueError(
401 f&quot;X 有 {n_features} 个特征，但 {self.__class__.__name__} &quot;
402 f&quot;需要 {self.n_features_in_} 个特征作为输入。&quot;
403 )

ValueError: X 有 1 个特征，但 LinearRegression 需要 2 个特征作为输入。
​​]]></description>
      <guid>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</guid>
      <pubDate>Wed, 27 Jul 2022 04:29:07 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: 无法将函数返回值转换为 Python 类型！签名为 () -> handle anaconda spyder</title>
      <link>https://stackoverflow.com/questions/72179285/typeerror-unable-to-convert-function-return-value-to-a-python-type-the-signatu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/72179285/typeerror-unable-to-convert-function-return-value-to-a-python-type-the-signatu</guid>
      <pubDate>Mon, 09 May 2022 23:48:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中使用具有焦点损失的类权重来处理多类分类的不平衡数据集</title>
      <link>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</link>
      <description><![CDATA[我正在研究语言任务的多类分类（4 个类别），并且我正在使用 BERT 模型进行分类任务。我正在关注这篇博文NLP 迁移学习：针对文本分类的微调 BERT。 我的 BERT Fine Tuned 模型返回 nn.LogSoftmax(dim=1)。
我的数据非常不平衡，因此我使用 sklearn.utils.class_weight.compute_class_weight 来计算类别的权重，并使用 Loss 中的权重。
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(train_labels), train_labels)
weights= torch.tensor(class_weights,dtype=torch.float)
cross_entropy = nn.NLLLoss(weight=weights) 


我的结果不太好，因此我想用 Focal Loss 进行实验，并为 Focal Loss 编写了一个代码。
class FocalLoss(nn.Module):
def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
super(FocalLoss, self).__init__()
self.alpha = alpha
self.gamma = gamma
self.logits = logits
self.reduce = reduce

def forward(self, input, target):
BCE_loss = nn.CrossEntropyLoss()(inputs, target)

pt = torch.exp(-BCE_loss)
F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

if self.reduce:
return torch.mean(F_loss)
else:
return F_loss

我现在有 3 个问题。首先也是最重要的是

我应该将类权重与 Focal Loss 结合使用吗？
如果我必须在这个 Focal Loss 中实现权重，我可以在 nn.CrossEntropyLoss() 中使用 weights 参数吗？
如果这个实现不正确，那么包括权重在内的正确代码应该是什么（如果可能）
]]></description>
      <guid>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</guid>
      <pubDate>Mon, 09 Nov 2020 11:53:49 GMT</pubDate>
    </item>
    </channel>
</rss>