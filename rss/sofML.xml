<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 28 May 2024 18:19:01 GMT</lastBuildDate>
    <item>
      <title>使用并行神经网络进行动态权重分配来提高 CNN 性能</title>
      <link>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</link>
      <description><![CDATA[我试图通过为所有数据点分配 0-1 之间的权重来改进我的回归模型，即卷积神经网络，这反过来又告诉我特定数据点在进行预测时有多好。关键是有些数据点很嘈杂并且会做出更糟糕的预测，我希望这些数据点具有较低的权重，以便在最后和训练期间忽略它们。
我所做的：尝试实现一个并行神经网络 (nn)，输出 0 到 1 之间的权重，并将此权重分配给相关的数据点/图像。
两个网络都接收相同的数据点作为输入，nn 输出权重，而回归 cnn 输出值 Y_i。然后将结果连接起来并传递给两个网络的相互自定义损失函数，该函数将预测值和目标值之间的平方差与并行 nn 中的给定权重相乘：sum((Y(:,i)-T(:,i))^2)*W(i)。
问题是模型似乎向较小的权重 W(i) 收敛，因为这将最大限度地减少损失，我该如何解决这个问题？
我尝试添加与权重大小成比例的正则化项来惩罚较小的权重。它有所帮助，但结果仍然不如没有并行 nn 时那么好。
# functional api
inputs = Input(shape=(16,150,1) )

x2 = Conv2D(64, 3, 1,activation=&#39;relu&#39;,data_format=&quot;channels_last&quot;,name=&#39;Conv1&#39;)(inputs)

x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(74, 2,activation=&#39;relu&#39;)(x2)
x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(128, 2,activation=&#39;relu&#39;)(x2)
x2 = Flatten()(x2)

x2 = Dense(256,activation=&#39;relu&#39;, name=&quot;FC1&quot;)(x2)
x2 = Dense(256,activation=&#39;relu&#39;,name=&#39;FC2&#39;)(x2)
regression_output = Dense(1,name=&#39;Output&#39;)(x2)

# 权重 NN 
x1 = Flatten()(inputs) # 与上面的 CNN 相同的输入
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;)(x1)

weight_output = Dense(1,activation=&#39;sigmoid&#39;,name=&#39;weight_output&#39;)(x1) # 权重作为输出
weight_output = tf.maximum(weight_output, 0.1) # 确保权重至少为 0.1
combined_output = Concatenate()([regression_output, weight_output])
model = Model(inputs=inputs, output=combined_output)

def custom_loss(y_true, y_pred):
regression = y_pred[:, 0]
weight = y_pred[:, 1]
# 与之前一样对权重进行 L2 惩罚
#weight_penalty = tf.reduce_mean(tf.square(weight))
#l1_penalty = tf.reduce_sum(tf.abs(weight)) # L1 惩罚
#lambda_l1 = 0.10 # L1 的正则化强度
return tf.reduce_mean(weight * tf.square(y_true - return))
model.compile(optimizer=&#39;adam&#39;, loss=custom_loss, metrics = [&#39;mae&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</guid>
      <pubDate>Tue, 28 May 2024 16:06:38 GMT</pubDate>
    </item>
    <item>
      <title>将 Spark 中的大数据导入到 Feast Feature Store</title>
      <link>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</link>
      <description><![CDATA[我目前正在为 MLOps 项目构建大数据管道，该管道用于批处理。
这是当前设置：

我将原始结构化数据存储在 Hive 中。
Spark 作业提取原始数据并进行处理。
我打算使用fest和Apache Cassandra作为离线存储来存储我的Spark作业产生的计算和策划的功能。

我想将数据从spark作业高效地传递到fest和Cassandra，我不确定是否需要中间数据持久化解决方案来保存处理后的数据，然后再将其传递到fest以存储在离线存储中，是否有必要我的情况？]]></description>
      <guid>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</guid>
      <pubDate>Tue, 28 May 2024 15:00:31 GMT</pubDate>
    </item>
    <item>
      <title>预测多元时间序列时 VARIMA 模型的模型漂移</title>
      <link>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</link>
      <description><![CDATA[我目前正在尝试在多变量时间序列数据上训练 VARIMA 模型，该数据是关于冷却系统的 5 种不同类型的传感器测量的。数据具有周期性，因此完全相同的模式每 25 个数据点左右就会重复出现一次。我有一个包含 5 个不同组件的数据集，其中每分钟有一个数据点。我使用了 2 周的数据来训练模型，然后让它合成数据。我使用的 VARIMA 模型是从 Darts 包导入的，我这样定义模型：model_VARIMA = VARIMA(p=12, d=0, q=0, trend=&quot;n&quot;)。该模型是在 2 周的训练数据上训练的。当预测未来 30 个点或更多时，预测显然开始显示模型漂移。所有组件都没有产生周期性的多变量时间序列数据，因为一条不再有价值的直线。我想知道是否有人对此有解释并有解决问题的方法。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</guid>
      <pubDate>Tue, 28 May 2024 12:15:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中不同数量的解释变量应该如何按行处理？</title>
      <link>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</link>
      <description><![CDATA[我在制作预测模型时遇到问题，所以我留下一个问题。
我正在尝试使用机器学习方法（例如随机森林、xgboost 等）创建预测模型。
此时y值为差分后的月时间序列数据，x值为差分后的日时间序列数据。
供参考，t，即时间，以美国股市交易日为时间单位。
我的模型由以下格式组成。
预测值 = y_(t+21) - y_(t)
解释值 = y(t) - y(t-1), y(t-1) - y(t-2) ... y(t-p) - y(t-p-1)
此时p为该月最后一个交易日。
这里的问题是每个月都有不同的交易天数
例如，1980年1月有23个交易日，但1981年2月有20个交易日，并且有可能假期较少的月份。
在这种情况下，在构建用于预测因变量的解释变量数据集时，可能会为逐列中的某些值生成 NaN 值。
这种情况，普遍应该如何处理？或者是否有一个术语或论文涉及这个问题？
y_(t+21) - y_(t) 有两种情况。一是区分月末值和区分月均值。因此，还没有触及任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</guid>
      <pubDate>Tue, 28 May 2024 11:21:17 GMT</pubDate>
    </item>
    <item>
      <title>二元分类获取预测值大于 1 [重复]</title>
      <link>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</link>
      <description><![CDATA[
&lt;img alt=&quot;预测代码&quot; src=&quot;https://i.sstatic.net/Lwa9ISdr.png ” /&gt;
任何人都可以帮助我，为什么我的模型返回预测值大于 1。即使我使用具有 1 个单位的密集层和 sigmoid 激活函数。我创建了一个二元分类模型。
我正在使用tensorflow和kerastuner进行超参数调整。]]></description>
      <guid>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</guid>
      <pubDate>Tue, 28 May 2024 09:53:40 GMT</pubDate>
    </item>
    <item>
      <title>提高人脸识别性能并扩大检测范围</title>
      <link>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</link>
      <description><![CDATA[我正在使用 Python、dlib 和 OpenCV 开发人脸识别系统，但在检测多张人脸时遇到性能问题，并且我需要扩展检测范围以检测 3 米以上以外的人脸。以下是我当前的设置和挑战的摘要：
当前设置：

使用 Logitech Brio 4K 超高清网络摄像头捕获视频流。
使用 dlib 的正面人脸检测器进行人脸检测。
利用 Dlib ResNet 模型进行人脸识别。
使用 OpenCV 处理视频流。
将已知的面部特征存储在 CSV 文件中以进行比较。

挑战：

当画面中有多个面孔时，性能会显着下降，
导致丢帧和降低 FPS。
需要检测距离大于 3 米的人脸
保持准确性。

为了加快计算速度，我应该考虑什么硬件加速技术或优化吗？]]></description>
      <guid>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</guid>
      <pubDate>Tue, 28 May 2024 09:51:28 GMT</pubDate>
    </item>
    <item>
      <title>截断标记LM [关闭]</title>
      <link>https://stackoverflow.com/questions/78542847/truncation-marckuplm</link>
      <description><![CDATA[我在使用 marckupLM 时遇到困难，我想知道 MarkupLMProcessor 是否有办法考虑因数据本身而被截断的信息。我的问题来自于需要比我必须截断的数据更大的数据，但我需要模型考虑为标记分类而截断的信息。
我希望有一些参数可以促进不同大小的数据，但在我的研究过程中我还没有找到它。我想知道我是否真的必须手工填写，或者是否有某种方法。]]></description>
      <guid>https://stackoverflow.com/questions/78542847/truncation-marckuplm</guid>
      <pubDate>Tue, 28 May 2024 08:29:01 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Kaggle 上以 .h5 格式保存深度学习模型</title>
      <link>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</link>
      <description><![CDATA[我在尝试在 Kaggle 上以 .h5 格式保存深度学习模型时遇到问题。尽管遵循标准程序，保存过程始终失败。 在此处输入图像描述我已添加代码和面临的问题。
在此处输入图片描述
我将格式指定为.keras，但模型无法保存。不过，该代码在 Google Colab 上运行良好。不幸的是，Google Colab 的内存不足以有效运行我的代码。
任何解决此问题并确保在 Kaggle 平台上以 .h5 格式成功保存模型的见解或潜在解决方案都将对我非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</guid>
      <pubDate>Mon, 27 May 2024 21:49:55 GMT</pubDate>
    </item>
    <item>
      <title>即使经过 500 个 epoch，结果也没有改善 [关闭]</title>
      <link>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</guid>
      <pubDate>Mon, 27 May 2024 19:40:59 GMT</pubDate>
    </item>
    <item>
      <title>在与我之前训练的数据集不同的数据集上训练 yolov8 变得非常慢[关闭]</title>
      <link>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</link>
      <description><![CDATA[我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。尽管这是一个较小的数据集，但即使 1 个 epoch 也需要极长的时间才能完成。还有其他人遇到这个问题吗？我可能哪里出错了？
我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。尽管这是一个较小的数据集，但即使 1 个 epoch 也需要非常长的时间才能完成。]]></description>
      <guid>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</guid>
      <pubDate>Mon, 27 May 2024 13:08:40 GMT</pubDate>
    </item>
    <item>
      <title>层“dense_4”的输入 0 与该层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）</title>
      <link>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</link>
      <description><![CDATA[我正在尝试实现多元回归模型。
使用以下代码：
all_normalizer = keras.layers.Normalization(input_shape=(1, ), axis=-1)
all_normalizer.adapt(x_train_all)

nn_model = tf.keras.Sequential([
    all_正规化器，
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

nn_model.compile(keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

历史记录 = nn_model.fit(
    x_train_Temp,y_train_Temp,
    验证数据=（x_val_Temp，y_val_Temp），
    详细 = 0，纪元 = 100
）

我收到以下错误：
 文件“C:\~ai.py”，第 330 行，在  中
    历史记录 = nn_model.fit(
              ^^^^^^^^^^^^^^
  文件“C:\~\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\~PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\input_spec.py”，第 227 行，位于assert_input_compatibility
    引发值错误（
ValueError：调用 Sequential.call() 时遇到异常。

层“dense_4”的输入0与图层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）

Sequential.call() 收到的参数：
  输入=tf.Tensor（形状=（无，1），dtype=float32）
  • 训练=真
  • 掩码=无

我应该在代码中更改/实现什么来解决此错误？
PS：我是初学者，可能不懂东西，所以请不要嫌弃。]]></description>
      <guid>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</guid>
      <pubDate>Mon, 27 May 2024 09:54:27 GMT</pubDate>
    </item>
    <item>
      <title>librosa、MFCC 中的类型错误</title>
      <link>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</link>
      <description><![CDATA[我有以下代码，它获取一个数据集（GTZAN）并将其转换为字典中的 MFCC：
DATASET_PATH = &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original&#39;
JSON_PATH = &quot;data_10.json&quot;
SAMPLE_RATE = 22050 #每首歌曲时长 30 秒，采样率为 22,050 Hz
TRACK_DURATION = 30 # 以秒为单位
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION #=661,500

def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):

# 用于存储映射、标签和 MFCC 的字典
data = {
&quot;mapping&quot;: [], #标签名称。size - (10,)
&quot;labels&quot;: [], #存储“真实”歌曲类型（值从 0 到 9）。 size - (5992,)
&quot;mfcc&quot;: [] #存储 mfccs.size - (5992, 216, 13)
}

samples_per_segment = int(SAMPLES_PER_TRACK / num_segments) #=110250
num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length) #=216(math.ceil of 215.332)
# 循环遍历所有流派子文件夹
for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

# 确保我们正在处理流派子文件夹级别
if dirpath is not dataset_path:

# 在映射中保存流派标签（即子文件夹名称）
semantic_label = dirpath.split(&quot;/&quot;)[-1]
data[&quot;mapping&quot;].append(semantic_label)
print(&quot;\nProcessing: {}&quot;.format(semantic_label))
# 处理类型子目录中的所有音频文件
for f in filenames:

# 加载音频文件

file_path = os.path.join(dirpath, f)

if file_path != &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39;: 
&quot;&quot;&quot;fileError: 打开 &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39; 时出错：文件包含未知数据格式。&quot;&quot;&quot;

signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE) #signal=音频文件中有多少样本, sample rate =音频文件的采样率, sample=22050

#处理音频文件的所有片段
for d in range(num_segments):

#计算当前片段的开始和结束样本
start = samples_per_segment * d
finish = start + samples_per_segment

#提取mfcc
mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef(13 因为 num_mfcc=13), 
mfcc = mfcc.T #[216,13]
#仅存储具有预期向量数量的 mfcc 特征
if len(mfcc) == num_mfcc_vectors_per_segment: #==216
data[&quot;mfcc&quot;].append(mfcc.tolist())
data[&quot;labels&quot;].append(i-1)
print(&quot;{},segment:{}&quot;.format(file_path, d+1))
# 将 MFCC 保存到 json 文件
with open(json_path, &quot;w&quot;) as fp:
json.dump(data, fp, indent=4) # 将所有内容放入 Json 文件中

# 运行数据处理 
save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

我已经使用这个代码很长一段时间了，它一直运行良好，直到今天我收到以下错误：
TypeError Traceback (most最近调用最后一次)
&lt;ipython-input-10-4a9371926618&gt; 在 &lt;module&gt;
1 # 运行数据处理
----&gt; 2 save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

&lt;ipython-input-9-8ba1c6e78747&gt; 在 save_mfcc(dataset_path, json_path, num_mfcc, n_fft, hop_length, num_segments)
56 
57 # 提取 mfcc
---&gt; 58 mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef（13 因为 num_mfcc=13），
59 mfcc = mfcc.T #[216,13]
60 # 仅存储具有预期向量数量的 mfcc 特征

TypeError：mfcc() 接受 0 个位置参数，但给出了 2 个位置参数（和 1 个仅关键字参数）

关于 save_mfcc 函数：
从音乐数据集中提取 MFCC 并将它们与流派标签一起保存到 json 文件中。
 :param dataset_path (str)：数据集路径
:param json_path (str)：用于保存的 json 文件的路径MFCCs
:param num_mfcc (int)：要提取的系数数量
:param n_fft (int)：我们考虑应用 FFT 的间隔。以样本数量为单位测量
:param hop_length (int)：FFT 的滑动窗口。以样本数量为单位测量
:param: num_segments (int)：我们要将样本轨迹划分成的段数
:return:

我不明白为什么今天才出现这个问题，以及如何解决它。
我该如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</guid>
      <pubDate>Sat, 18 Mar 2023 12:46:34 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程置信度与可信区间</title>
      <link>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</link>
      <description><![CDATA[由于高斯过程返回分布而不是点估计，为什么会这样示例（实际上在 GP 的每个示例中）谈论贝叶斯统计类似物的置信区间“可信区间” ？
更新
一个建议（不是来自我）是他们将它们称为置信区间，因为他们使用最大似然法而不是使用完整的贝叶斯方法 - 我对此不相信，因为经验贝叶斯方法也可以提供可信的区间]]></description>
      <guid>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</guid>
      <pubDate>Fri, 06 Mar 2020 08:27:27 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实施 Dropout</title>
      <link>https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch</guid>
      <pubDate>Wed, 09 Jan 2019 11:57:47 GMT</pubDate>
    </item>
    <item>
      <title>将 R 与 Matlab 进行比较以进行数据挖掘[关闭]</title>
      <link>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</link>
      <description><![CDATA[我最近开始学习 R，而不是开始在 Matlab 中编码，主要是因为它是开源的。我目前从事数据挖掘和机器学习领域的工作。我发现许多用 R 实现的机器学习算法，并且我仍在探索用 R 实现的不同包。
我有一个简单的问题：您如何比较 R 和 Matlab 在数据挖掘应用中的受欢迎程度、优缺点、行业和学术接受度等？您会选择哪一个？为什么？
我根据各种指标对 Matlab 与 R 进行了各种比较，但我特别有兴趣获得其在数据挖掘和机器学习中的适用性的答案。 
由于这两种语言对我来说都很新，我只是想知道 R 是否是一个不错的选择。
我很感激任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</guid>
      <pubDate>Thu, 27 Jan 2011 01:04:05 GMT</pubDate>
    </item>
    </channel>
</rss>