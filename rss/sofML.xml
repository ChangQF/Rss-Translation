<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 05 Jan 2025 18:20:38 GMT</lastBuildDate>
    <item>
      <title>CNN 模型准确率达 80%，但部署后无法识别人员</title>
      <link>https://stackoverflow.com/questions/79331264/cnn-model-gives-80-accuracy-but-fails-to-identify-person-after-deployment</link>
      <description><![CDATA[我使用卷积神经网络 (CNN) 开发了一个人物分类 AI 模型。在测试期间，该模型的准确率达到了 80%。然而，在部署模型后，它无法在现实场景中正确识别人员。
# 构建 CNN 模型
model = Sequential([
Conv2D(32, (3, 3),activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01), input_shape=(224, 224, 3)),
MaxPooling2D(pool_size=(2, 2)),
Conv2D(64, (3, 3),activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01)),
MaxPooling2D(pool_size=(2, 2)),
Conv2D(128, (3, 3),activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01)),
MaxPooling2D(pool_size=(2, 2)),
Flatten(),
Dense(128,activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01)),
Dropout(0.3),
Dense(train_generator.num_classes,activation=&#39;softmax&#39;)
])

# 编译模型
model.compile(optimizer=Adam(learning_rate=0.0001),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/79331264/cnn-model-gives-80-accuracy-but-fails-to-identify-person-after-deployment</guid>
      <pubDate>Sun, 05 Jan 2025 17:59:17 GMT</pubDate>
    </item>
    <item>
      <title>检测器模型中框的正确损失函数</title>
      <link>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</guid>
      <pubDate>Sun, 05 Jan 2025 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 应用程序中实现 Roboflow 模型 API？</title>
      <link>https://stackoverflow.com/questions/79330782/how-to-implement-roboflow-model-api-in-flutter-app</link>
      <description><![CDATA[我正在尝试在我的 Flutter 应用中实现 RoboFlow 模型 API 来分析图像。我想从用户那里获取图像并使用 Roboflow 模型 API 对其进行分析。
URL、API 密钥、模型 API、版本一切都正确，但输入数据有些错误。
下面是我的函数：
Future&lt;void&gt; testRoboflowAPI(File imagePath) async {
try {
List&lt;int&gt; imageBytes = imagePath.readAsBytesSync();
String base64Image = base64Encode(imageBytes);

final url =
&#39;https://classify.roboflow.com/**********/1?api_key=*********&#39;;
final headers = {&#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;};

final body = {
&quot;data&quot;: base64Image,
};

finalcodedBody = Uri(queryParameters: body).query;
final response = await http.post(
Uri.parse(url),
headers: headers,
body:codedBody,
);

if (response.statusCode == 200) {
print(&#39;成功：${response.body}&#39;);
} else {
print(
&#39;错误：状态代码 ${response.statusCode}，响应：${response.body}&#39;);
}
} catch (e) {
print(&#39;异常：${e.toString()}&#39;);
}
}

我收到给定的错误：
错误：状态代码 400，响应：{&quot;message&quot;:&quot;无法加载输入图像。原因：base64 输入图像格式错误。&quot;}

您能帮我得到正确的响应并解决错误吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79330782/how-to-implement-roboflow-model-api-in-flutter-app</guid>
      <pubDate>Sun, 05 Jan 2025 13:15:11 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能使“UndefinedMetricWarning”静音？</title>
      <link>https://stackoverflow.com/questions/79330764/how-can-i-silence-undefinedmetricwarning</link>
      <description><![CDATA[如何在运行 GridSearchCV(model, params, cv=10,scoring=&#39;precision&#39;, verbose=1, n_jobs=20, refit=True) 时消除以下警告？
/opt/dev/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: 由于没有预测样本，精度定义不明确，并被设置为 0.0。使用 `zero_division` 参数来控制此行为。

我尝试过，但没有成功：
import os, warnings
warnings.simplefilter(&quot;ignore&quot;)
warnings.filterwarnings(&quot;ignore&quot;)
with warnings.catch_warnings():
warnings.simplefilter(&quot;ignore&quot;)
os.environ[&quot;PYTHONWARNINGS&quot;] = &quot;ignore&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/79330764/how-can-i-silence-undefinedmetricwarning</guid>
      <pubDate>Sun, 05 Jan 2025 13:03:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Ridge Regression 在视觉追踪一个物体时会表现得像这样？</title>
      <link>https://stackoverflow.com/questions/79330150/why-does-ridge-regression-behave-like-this-when-visually-tracking-an-object</link>
      <description><![CDATA[我正在尝试实现没有边界效应的 KCF 跟踪器像这里。简而言之，它是一个没有使用任何傅里叶变换的核化相关滤波器。我正在像这样计算过滤器权重 alpha：
Idm = np.identity(Kxx.shape[1])
A = Kxx.T @ Kxx
B = Kxx.T @ y
alpha = np.linalg.solve(A + 2e-4*Idm, B)

Kxx 是当前帧上训练区域的自相关矩阵，y 是我使用得到的高斯标签矩阵
g1 = cv2.getGaussianKernel(Kxx.shape[0], 1.5)
g2 = cv2.getGaussianKernel(Kxx.shape[1], 1.5)
y = g1 @ g2.T

我预测下一帧中物体的新位置使用response = Kxz@alpha
其中 Kxz 是训练区域和目标对象块的相关矩阵。
当我垂直移动训练区域时，响应图的行为如下：
垂直移动训练区域
但是，当我水平移动训练区域时，目标对象的移动不会显示在响应图上：
水平移动训练区域
实际上，我可以使用 response = alpha@Kxz.T 获得仅表示水平移动的响应图。但由于矩阵形状不同，这不适用于该文章中提到的相关方法。我认为这不是计算相关矩阵的问题，因为我已经尝试了几种相关方法，例如 cv2.filter2D 和文章中的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79330150/why-does-ridge-regression-behave-like-this-when-visually-tracking-an-object</guid>
      <pubDate>Sun, 05 Jan 2025 04:01:38 GMT</pubDate>
    </item>
    <item>
      <title>如何从物理信息神经网络 (PINN) 获取具有初始和边界条件的 PDE 的单一解？</title>
      <link>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</guid>
      <pubDate>Sun, 05 Jan 2025 01:01:10 GMT</pubDate>
    </item>
    <item>
      <title>无法从‘transformers’导入名称‘T2TViTModel’</title>
      <link>https://stackoverflow.com/questions/79329851/cannot-import-name-t2tvitmodel-from-transformers</link>
      <description><![CDATA[我尝试训练我的模型并从 Python 脚本中的 transformers 库导入 T2TViTModel，但遇到了以下错误：

回溯（最近一次调用）：
文件“/Users/mac/PycharmProjects/Heart-rate Model/training.py”，第 8 行，来自 transformers 导入 T2TViTModel
ImportError：无法从“transformers”（/Users/mac/PycharmProjects/pythonProject/.venv/lib/python3.12/site-packages/transformers/init.py）导入名称“T2TViTModel”。

我已确保 transformers 库已安装并更新为最新版本。但是，我在库中找不到 T2TViTModel。
有人能帮助我理解为什么导入失败以及如何解决这个问题吗？
我使用 PyTorch 和 transformers 库中的 T2T-ViT 模型实现了心率估计模型。我的代码的主要组件包括：

数据加载：自定义 HeartRateDataset 类，用于从 CSV 文件加载图像和心率标签。
模型定义：HeartRateEstimator 模型，包含 T2T-ViT 并添加回归头用于心率预测。
训练循环：用于跨多个时期训练模型、计算损失并针对单独数据集进行验证的函数。
评估：用于在测试数据集上评估训练后的模型并计算性能指标的功能。

我希望代码能够顺利运行，成功训练模型，并观察到训练和验证损失在各个时期的减少，最终从测试集评估中获得 MAE、RMSE 和 R² 等指标。]]></description>
      <guid>https://stackoverflow.com/questions/79329851/cannot-import-name-t2tvitmodel-from-transformers</guid>
      <pubDate>Sun, 05 Jan 2025 01:01:10 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“llama_index.text_splitter”的模块</title>
      <link>https://stackoverflow.com/questions/79329352/modulenotfounderror-no-module-named-llama-index-text-splitter</link>
      <description><![CDATA[我用这个来导入
从 llama_index.text_splitter 导入 SentenceSplitter

我的 python 版本 - 3.11.5 和 llama_index 版本 - 0.12.8 但出现此错误
ModuleNotFoundError：没有名为“llama_index.text_splitter”的模块

如何解决这个问题？
我询问 chatgpt，谷歌其他 AI 但找不到解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79329352/modulenotfounderror-no-module-named-llama-index-text-splitter</guid>
      <pubDate>Sat, 04 Jan 2025 18:08:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 训练 DNN 时 NaN 损失输出</title>
      <link>https://stackoverflow.com/questions/79328433/nan-loss-output-when-training-dnn-with-keras</link>
      <description><![CDATA[我正在尝试做 Aurelien Geron 的 Hands-On ML 中的一项练习。但是，当我尝试使用自己的解决方案并从答案中复制和粘贴解决方案时，我总是得到这个不应该发生的结果。每个时期的损失和 val_loss 都是 nan，准确率根本没有提高。本练习尝试在 CIFAR10 数据集上构建一个具有 20 个隐藏层的 DNN。
代码如下：
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
X_val, y_val = X_train[:5000], y_train[:5000]
X_train, y_train = X_train[5000:], y_train[5000:]
tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=[32,32,3]))
model.add(tf.keras.layers.Flatten())

## 20 个隐藏层 
for _ in range(20):
model.add(tf.keras.layers.Dense(100,activation=&#39;swish&#39;, 
kernel_initializer=&#39;he_normal&#39;))

# 输出层
model.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;))

optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-6)
model.compile(optimizer=opimizer,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
import datetime

early_stopping = tf.keras.callbacks.EarlyStopping(patience=20, 
restore_best_weights=True)

log_dir = &quot;logs/my_cifar10_model/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d- 
%H%M%S&quot;)
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping, 
tensorboard_callback],
validation_data=(X_val, y_val))

输出如下：
Epoch 1/100
1407/1407 ━━━━━━━━━━━━━━━━━━━━━━━ 8s 3ms/step - 准确率： 0.1005 - 损失：nan - 
val_accuracy：0.0996 - val_loss：nan
Epoch 2/100
1407/1407 ━━━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - 准确度：0.1007 - 损失：nan - 
val_accuracy：0.0996 - val_loss：nan
Epoch 3/100
1407/1407 ━━━━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - 准确度：0.1007 - 损失：nan - 
val_accuracy：0.0996 - val_loss：nan

...
Epoch 20/100
1407/1407 ━━━━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - 准确度：0.1007 - 损失：nan - 
val_accuracy：0.0996 - val_loss：nan
Epoch 21/100
1407/1407 ━━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - 准确率：0.1007 - 损失：nan - 
val_accuracy：0.0996 - val_loss：nan
&lt;keras.src.callbacks.history.History at 0x7801cde19fa0&gt;

TF 或 python 实际上没有引发任何错误。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79328433/nan-loss-output-when-training-dnn-with-keras</guid>
      <pubDate>Sat, 04 Jan 2025 07:45:22 GMT</pubDate>
    </item>
    <item>
      <title>如何检查随机森林模型是否过度拟合？</title>
      <link>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</link>
      <description><![CDATA[我已经为数据集实现了随机森林，并且平衡了数据，我使用了 80-10-10、70-15-15、60-20-20 和 80-20 方法。我还使用了特征重要性，并在 41 个独立特征中使用了 10 个 imp 特征、15 个 imp 特征、24 个 imp 特征和 34 个 imp 特征。所有上述方法的平均召回率为 95.8%，平均准确率为 96.6%，精确率为 97%。交叉验证召回率（我主要关注召回率）为 95.5%。
我使用训练数据对训练数据本身进行预测，得到了 99.8%
我还使用了热图并删除了 3 个高度相关的特征，但我得到了 80-10-10 的相同分数（热图之后）。
我的模型是否过度拟合？如何检查是否过度拟合？]]></description>
      <guid>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</guid>
      <pubDate>Fri, 03 Jan 2025 19:56:33 GMT</pubDate>
    </item>
    <item>
      <title>如何修复具有可变时间维度的 3D U-Net 中的跳过连接维度不匹配问题？[关闭]</title>
      <link>https://stackoverflow.com/questions/79326988/how-to-fix-skip-connection-dimension-mismatch-in-3d-u-net-with-variable-temporal</link>
      <description><![CDATA[我正在开发一个用于 3D 数据的 U-Net 模型，其中输入有三个维度：（高度、宽度、时间）。第三个维度 时间 是可变的，这给编码器和解码器之间的跳过连接带来了挑战。
出现此问题的原因是编码器和解码器特征图在某些层中的时间维度不匹配。例如：

编码器输出形状：tf.Tensor(shape=(1, 32, 32, 16, 256), dtype=float32)
解码器输出形状：tf.Tensor(shape=(1, 32, 32, 15, 256), dtype=float32)

时间维度（16 vs. 15）不匹配，导致连接期间出现维度不匹配错误。
可能的解决方案：

裁剪编码器特征图：减少编码器特征图的时间维度以与解码器对齐。
填充解码器特征图：向解码器特征图的时间维度添加填充以匹配编码器。

是否有一种首选方法（裁剪与填充）来解决这种不匹配问题，特别是在语义分割中？是否有任何其他最佳实践来处理 3D U-Net 中的可变时间维度？]]></description>
      <guid>https://stackoverflow.com/questions/79326988/how-to-fix-skip-connection-dimension-mismatch-in-3d-u-net-with-variable-temporal</guid>
      <pubDate>Fri, 03 Jan 2025 15:53:08 GMT</pubDate>
    </item>
    <item>
      <title>训练最小 U-Net 时的方法错误</title>
      <link>https://stackoverflow.com/questions/79316648/methoderror-in-training-minimal-u-net</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79316648/methoderror-in-training-minimal-u-net</guid>
      <pubDate>Mon, 30 Dec 2024 01:52:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在同一层使用多个损失函数吗？</title>
      <link>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</link>
      <description><![CDATA[我们可以在这个架构中使用多个损失函数吗：
我有两种不同类型的损失函数，并希望在最后一层使用它 [输出]
损失函数：

binary_crossentropy
自定义损失函数

我们可以这样做吗？
]]></description>
      <guid>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</guid>
      <pubDate>Sun, 12 Jul 2020 13:37:51 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络中的反向传播以及如何更新过滤器</title>
      <link>https://stackoverflow.com/questions/50313152/back-propagation-in-convolutional-neural-networks-and-how-to-update-filters</link>
      <description><![CDATA[我正在学习卷积神经网络，现在我对如何实现它感到困惑。
我了解常规神经网络和梯度下降和反向传播等概念，我可以直观地理解 CNN 的工作原理。
我的问题是关于 CNN 中的反向传播。它是如何发生的？最后的全连接层是常规神经网络，这没有问题。但我如何更新卷积层中的过滤器？我如何将全连接层中的误差反向传播到这些过滤器？我的问题是更新过滤器！
过滤器只是简单的矩阵？或者它们具有像常规 NN 这样的结构，并且层之间的连接模拟了这种能力？我读过关于稀疏连接和共享权重的文章，但我无法将它们与 CNN 联系起来。我对实现 CNN 真的很困惑，我找不到任何讨论这些概念的教程。我无法阅读论文，因为我对这些事情很陌生，而且我的数学不好。 
我不想使用 TensorFlow 或类似的工具，我正在学习主要概念并使用纯 Python。]]></description>
      <guid>https://stackoverflow.com/questions/50313152/back-propagation-in-convolutional-neural-networks-and-how-to-update-filters</guid>
      <pubDate>Sun, 13 May 2018 05:49:41 GMT</pubDate>
    </item>
    </channel>
</rss>