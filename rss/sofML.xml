<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 18 Dec 2024 01:19:26 GMT</lastBuildDate>
    <item>
      <title>使用积分图像计算矩形 Haar 特征的总和</title>
      <link>https://stackoverflow.com/questions/79289701/computing-the-sum-of-rectangular-haar-features-using-the-integral-image</link>
      <description><![CDATA[我想用这个家伙的代码来计算积分图像和哈尔特征的总和区域，因为我找不到其他可靠的方法，也不想自己做。但现在看来，这段代码无论如何都行不通，我无法让它工作，但想用填充的积分图像让它工作。
我只是不太明白 sum_region 函数应该输入什么。他使用 to_integral_image() 计算积分图像，这是正确的，但我不明白的是 sum_region 函数，当我测试它时，它似乎对我不起作用，整个翻转坐标和积分图像的填充让我很困惑，我无法理解，这真的很令人沮丧。
对我来说，积分计算得很完美，但 sum_region() 函数似乎不起作用。我尝试在两个 bottom_right 坐标上都添加一个，但对于极端情况，这种方法不起作用。
如果有人能帮助我克服这种烦人的精神迷雾，我将不胜感激。
import numpy as np

def sum_region(integral_img_arr, top_left, bottom_right):

&quot;&quot;&quot;

计算给定元组指定的矩形中的和。

:param integration_img_arr:

:type integration_img_arr: numpy.ndarray

:param top_left: (x, y) 矩形左上角

:type top_left: (int, int)

:param bottom_right: (x, y) 矩形右下角

:type bottom_right: (int, int)

:return 给定矩形中所有像素的总和

:rtype int

&quot;&quot;&quot;

# 交换元组

top_left = (top_left[1], top_left[0])

bottom_right = (bottom_right[1], bottom_right[0])

if top_left == bottom_right:

return integration_img_arr[top_left]

top_right = (bottom_right[0], top_left[1])

bottom_left = (top_left[0], bottom_right[1])

return integration_img_arr[bottom_right] - integration_img_arr[top_right] - integration_img_arr[bottom_left] + integration_img_arr[top_left]

我尝试输入一个正常的 0 索引数组索引并打印出结果，但填充和坐标翻转真的很令人困惑，我似乎无法理解这一切。
这就是我用来测试它。
import numpy as np
from integration_image import *

original_image = np.array([
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
],)

integral = to_integral_image(original_image)
print(original_image)
print(integral)

# 原始图像中的区域

top_left = (0,0)
bottom_right = (2,1)
sum = sum_region(integral, top_left, bottom_right) 
print(f&quot;原始左上角：{original_image[top_left]}&quot;)
print(f&quot;原始右下角 {original_image[bottom_right]}&quot;)

print(f&quot;总和：{sum}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79289701/computing-the-sum-of-rectangular-haar-features-using-the-integral-image</guid>
      <pubDate>Wed, 18 Dec 2024 00:47:26 GMT</pubDate>
    </item>
    <item>
      <title>如果原始训练数据是离散的，是否可以将数据添加到连续的 keras 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</link>
      <description><![CDATA[我有一个模型 keras 模型，该模型是在离散数据（基于生物体中基因的存在而得出的是/否数据）上进行训练的，这可以预测离散响应（生物体是否执行某种功能而得出的是/否）。
我们想要添加连续的新数据（生物体中蛋白质的丰度），现在我们想要预测连续响应（某种功能的活动水平）。
这有可能做到吗？我已按照此链接中的步骤进行操作，但似乎假设您使用的是相同类型的数据。我该如何添加不同类型的数据？]]></description>
      <guid>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</guid>
      <pubDate>Tue, 17 Dec 2024 19:32:07 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理步骤[关闭]</title>
      <link>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</link>
      <description><![CDATA[我想知道 Keras 图像数据加载是否对图像数据集进行了完整的预处理，例如应用过滤器、规范化、标准化等，之后该模块不需要进行更详细的预处理还是怎样？
代码如下。
malimg=tf.keras.utils.image_dataset_from_directory(
r&quot;C:\Users\PMYLS\Desktop\Malware Pycharm Project\malimg_paper_dataset_imgs&quot;,
labels=&quot;inferred&quot;,
label_mode=&quot;int&quot;,
class_names=None,
color_mode=&quot;rgb&quot;,
batch_size=64,
image_size=(256, 256),
shuffle=True,
seed=None,
validation_split=None,
subset=None,
interpolation=&quot;bilinear&quot;,
follow_links=False,
crop_to_aspect_ratio=False,
pad_to_aspect_ratio=False,
data_format=None,
verbose=True,
)
class_names = malimg.class_names # 获取类名
print(&quot;Classes:&quot;, class_names)
]]></description>
      <guid>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</guid>
      <pubDate>Tue, 17 Dec 2024 17:17:21 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>Databricks MLFlow 和 MetaFlow 集成</title>
      <link>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</guid>
      <pubDate>Tue, 17 Dec 2024 13:02:01 GMT</pubDate>
    </item>
    <item>
      <title>对于非常随机的文本语料库，哪些是最有效的主题建模算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</link>
      <description><![CDATA[没有关于语料库长度的信息。
没有关于任何主题层次结构的信息。
我遇到了 BERTopic，但它有 9 种不同的建模类型，哪一种应该适合？我不能使用监督或半监督，因为我没有关于数据的信息，我只知道它与 RFP（提案请求）相关。我可以预测一些主题，因此可以使用种子建模，但也会有随机主题。
我也对 LDA 等仅是句法的方法持开放态度，因为它给出了良好的结果。
我知道概括是不可能的，但想知道你的经验。
最初我尝试了 LDA，它没有给出好的结果，因为像数据长度和数据中的主题数量这样的超参数很难对如此大的完全非结构化随机数据集进行微调。]]></description>
      <guid>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</guid>
      <pubDate>Tue, 17 Dec 2024 12:25:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用python的spaCy正确识别标记的实体类型？</title>
      <link>https://stackoverflow.com/questions/79287799/how-to-correctly-identify-entity-types-for-tokens-using-spacy-using-python</link>
      <description><![CDATA[我正在使用 spaCy 从文本描述中提取和识别实体类型（如 ORG、GPE、DATE 等）。但是，我注意到一些不正确的结果，我不确定如何修复它。
这是我使用的代码：
import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def getPayeeName(description):
description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()
doc = nlp(description)

for token in doc:
print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else &#39;None&#39;}&quot;)

# 示例输入
description = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 医疗有限公司 HDFC 50200&quot;
getPayeeName（说明）

令牌：UPI，实体：ORG
令牌：DR，实体：ORG
令牌：400874707203，实体：无
令牌：BENGALORE，实体：无
令牌：08，实体：DATE
令牌：JAN，实体：DATE
令牌：2024，实体：DATE
令牌：14:38:56，实体：无
令牌：MEDICAL，实体：ORG
令牌：LTD，实体：ORG
令牌：HDFC，实体：ORG
令牌：50200，实体： ORG

50200 被识别为 ORG，但它只是一个数字。

BENGALORE 是一个城市，但它未被识别为 GPE 或位置
（返回 None）。

UPI 和 DR 是首字母缩略词/缩写，但它们被错误地
识别为 ORG。


我希望实体识别更加准确和可靠。
我该如何解决这些问题？是否有其他 spaCy 配置、自定义规则或预训练模型可用于改进实体识别？
注意：我也尝试了 ChatGPT，但这个问题仍然没有解决。]]></description>
      <guid>https://stackoverflow.com/questions/79287799/how-to-correctly-identify-entity-types-for-tokens-using-spacy-using-python</guid>
      <pubDate>Tue, 17 Dec 2024 12:09:49 GMT</pubDate>
    </item>
    <item>
      <title>如何确保 RStudio 使用我一半的内存？[关闭]</title>
      <link>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</link>
      <description><![CDATA[我正在尝试使用 tidymodels 在 RStudio 上调整机器学习模型。
我有 Macbook Pro 2019

2.3 GHz 8 核 Intel Core i9，
32 GB 2667 MHz DDR4

对于调整 KNN 回归，它花费了 10 多个小时，我不明白为什么。以下是代码：
knn_model &lt;-
nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%
set_engine(&#39;kknn&#39;) %&gt;%
set_mode(&#39;regression&#39;)

knn_grid &lt;-
grid_regular(
neighbours(),
weight_func(),
dist_power(),
levels = c(20, 5, 5)
)

knn_wf &lt;-
working() %&gt;%
add_model(knn_model) %&gt;%
add_formula(demande_energetique_projectee ~ .)

knn_res &lt;-
knn_wf %&gt;%
tune_grid(
resamples = folds,
grid = knn_grid,
metrics = metric_set(rmse)
)
knn_res

我检查了分配给 rstudio 的内存；它不超过 1.2Gb。但为什么呢？
为什么它没有使用所有内存来加快我的超参数调整速度？
经过一番研究，我在主文件夹中创建了 .Renviron 文件并将其放入
R_MAX_VSIZE=16Gb

并重新启动了 RStudio，但问题并未解决。
以下是有关会话的信息
R 版本 4.3.3 (2024-02-29)
平台：x86_64-apple-darwin20 (64 位)
运行于：macOS 15.1.1

问题：

如何加快超参数调整速度？
我们如何确保 RStudio 使用一半的内存而不是仍然阻塞最大 1.2Gb？
]]></description>
      <guid>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</guid>
      <pubDate>Tue, 17 Dec 2024 08:11:39 GMT</pubDate>
    </item>
    <item>
      <title>如何修复使用 Prompt Flow 时出现的“错误：pip 的依赖解析器当前未考虑已安装的所有软件包。”</title>
      <link>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</link>
      <description><![CDATA[我制作了一个自定义映像，以在 Azure Ai Foundry 的 Prompt Flow（早期的 Ai Studio）上使用 python 的 3.10.1 版本。忽略错误，Flow 成功运行。

错误：pip 的依赖解析器当前未考虑已安装的所有软件包。此行为是以下依赖冲突的根源。mlflow 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。mlflow-skinny 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。

但是，我认为这会在最终的生产部署中造成麻烦。此外，我还检查了我的自定义图像上的 protobuf 版本，但版本号是 4.25.5，在这种情况下应该可以正常工作。下面是错误和 docker 容器的屏幕截图。

我在执行 find 部署时遇到的错误如下：

根据用于故障排除此错误的文档，错误为 ResourceNotReady。其中提到了 score.py 文件。我想知道什么是 score.py 文件，这个文件在部署时会自动生成吗？还是需要在自定义镜像时单独创建这个文件？最后我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</guid>
      <pubDate>Tue, 17 Dec 2024 07:12:40 GMT</pubDate>
    </item>
    <item>
      <title>如何使用具有动态尺寸输入的 Dense 层？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</link>
      <description><![CDATA[我有一个模型，其输入（具有形状（高度、宽度、时间）的图像批次）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Dense 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度的输入（无）
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用密集层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：密集层输入的最后一个维度应已定义。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？本质上，我正在寻找一种方法来创建一个具有原始像素值的 1D 数组，但与 Dense 层兼容。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>LSTM RNN Tensorflow 语言预测模型卡住了吗？</title>
      <link>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</link>
      <description><![CDATA[我正在尝试开发一个模型，从不同艺术家的歌词中学习，以确定是谁写的。我的 Jupyter 笔记本和 tsv 文件保存在此文件夹中：文件夹链接。
我尝试过不同的策略，但结果很差。要么是低损失/低准确度，要么是高精度/高损失。想知道是否有人能发现我意识之外的明显问题。
Jupyter 笔记本代码在这里：
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks 导入 EarlyStopping
来自 tensorflow.keras.layers 导入 BatchNormalization
导入 torch
导入 torch.nn 作为 nn

# 用于操作目录路径
导入 os

# 用于 python 的科学和矢量计算
导入 numpy 作为 np

# 绘图库
来自 matplotlib 导入 pyplot 作为 plt

# scipy 中的优化模块
来自 scipy 导入 o​​ptimize

# 将用于加载 MATLAB mat 数据文件格式
来自 scipy.io 导入 loadmat

# 告诉 matplotlib 在笔记本中嵌入绘图
%matplotlib inline

导入 pandas 作为 pd

来自 sklearn.metrics 导入 confused_matrix、ConfusionMatrixDisplay
来自 sklearn.utils.class_weight 导入 compute_class_weight
来自 sklearn.utils 导入 class_weight

导入 seaborn 作为 sns

df = pd.read_csv(&#39;shuffled_verses.tsv&#39;, sep=&#39;\t&#39;)
X = np.asarray(df.values[:3975, 6]).astype(&#39;str&#39;)
X_cv = np.asarray(df.values[3975:5300, 6]).astype(&#39;str&#39;)
X_test = np.asarray(df.values[5300:, 6]).astype(&#39;str&#39;)

# Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)
X_cv_sequences = tokenizer.texts_to_sequences(X_cv)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

# 添加填充
X_padded = pad_sequences(X_sequences, padding=&#39;post&#39;)
X_cv_padded = pad_sequences(X_cv_sequences, padding=&#39;post&#39;)
X_test_padded = pad_sequences(X_test_sequences, padding=&#39;post&#39;)

y = np.asarray(df.values[:3975, 1]).astype(&#39;float32&#39;)
y_cv = np.asarray(df.values[3975:5300, 1]).astype(&#39;float32&#39;)
y_test = np.asarray(df.values[5300:, 1]).astype(&#39;float32&#39;)

df = pd.read_csv(&#39;my_artists.tsv&#39;, sep=&#39;\t&#39;)
X_encoding = np.asarray(df.values[:, 0]).astype(&#39;str&#39;)
y_encoding = np.asarray(df.values[:, 1]).astype(&#39;float32&#39;)

# 超参数
embedding_dim = 100 # 从 100 更改为 200
rnn_units = 128 # 从 128 更改为 256
max_sequence_length = X_padded.shape[1] # 填充序列的长度
vocab_size = len(tokenizer.word_index) + 1 # 词汇表大小（为填充标记添加 1）
class_weights = class_weight.compute_class_weight(&#39;balanced&#39;, classes=np.unique(y_encoding), y=y)
class_weights = dict(enumerate(class_weights))
total_weight = sum(class_weights.values())
normalized_class_weights = {k: v / total_weight for k, v in class_weights.items()}

# 构建模型
model = Sequential()

# 嵌入层
model.add(Embedding(input_dim=vocab_size, # 词汇表的大小
output_dim=embedding_dim,
input_length=max_sequence_length,)) # 输入序列的长度

# RNN
model.add(Bidirectional(LSTM(rnn_units, return_sequences=True, kernel_regularizer=l2(0.006))))
model.add(Dense(70,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(45,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(num_classes,activation=&#39;softmax&#39;, kernel_regularizer=l2(0.001)))

# 编译
model.compile(optimizer=Adam(learning_rate=0.0001), 
loss=&#39;sparse_categorical_crossentropy&#39;, # 使用 &#39;categorical_crossentropy&#39; 进行多分类
metrics=[&#39;accuracy&#39;])

# EarlyStopping
early_stopping = EarlyStopping(
monitor=&#39;val_loss&#39;, # 监控验证损失
patience=3, # 如果连续 3 个时期没有改善，则停止
restore_best_weights=True # 恢复最佳模型权重
)

model.summary()

# 训练
history = model.fit(X_padded, y,
epochs=300,
batch_size=32,
validation_data=(X_cv_padded, y_cv),
class_weight=normalized_class_weights)
]]></description>
      <guid>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</guid>
      <pubDate>Wed, 11 Dec 2024 20:02:40 GMT</pubDate>
    </item>
    <item>
      <title>处理 Llama 3.2：3b-Instruct 模型中的令牌限制问题（最多 2048 个令牌）[关闭]</title>
      <link>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</link>
      <description><![CDATA[我正在使用 Llama 3.2:3b-instruct 模型并遇到以下错误：
此模型的最大上下文长度为 2048 个令牌。但是，您请求了 
2049 个令牌（消息中 1681 个，完成中 368 个）。

我理解这是由于超出令牌限制造成的，但我想知道：

是否有任何最佳实践或技术可以减少令牌使用量，而不会丢失消息或完成中的关键上下文？
]]></description>
      <guid>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</guid>
      <pubDate>Tue, 10 Dec 2024 04:19:17 GMT</pubDate>
    </item>
    <item>
      <title>时间序列运动捕捉数据的 PCA 图聚类问题</title>
      <link>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</link>
      <description><![CDATA[我正在使用 PCA 对手部动作捕捉数据的时间序列数据集进行降维，并遇到了意外的聚类行为。以下是我的过程和我面临的问题的详细信息。

上下文：
我有一个使用智能手套捕捉的手部动作记录数据集，每个手指上有 4 个传感器。每个传感器提供：

位置值：X、Y、Z
旋转值：X、Y、Z、W

数据记录在单独的文件中，每个文件包含 10 次手势重复。这些重复随后被分割成单独的文件（每个文件 1 个重复），贴上标签，并合并成一个大型数据集。
在对数据进行标签编码和缩放后，我使用以下代码应用 PCA 进行降维：
# PCA 用于降维
pca_components = 3
pca = PCA(n_components=pca_components)
x_train_pca = pca.fit_transform(x_train_scaled)

为了可视化结果，我创建了一个 PCA 摘要图，其中每个数据段都表示为一个点。目标是查看每个手势的 10 个点的聚类。以下是总结 PCA 结果的代码：
# 将 PCA 结果转换为 DataFrame 以关联标签
x_train_pca_df = pd.DataFrame(x_train_pca, columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;])
x_train_pca_df[&#39;label&#39;] = y_train_data
x_train_pca_df[&#39;segment&#39;] = train_data[&#39;segment&#39;].values.ravel()

# 计算每个数据集的 PC1 和 PC2 的平均值（将每个数据集总结为一个点）
summary_train_points = x_train_pca_df.groupby([&#39;label&#39;, &#39;segment&#39;]).mean().reset_index()

以下是我用来绘制总结 PCA 的方法数据：
def plot_3d_pca_matplotlib(summary_points, title=&#39;3D PCA Plot&#39;):
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

# 对于每个唯一数据集（标签），分散点并分配图例条目
unique_labels = summary_points[&#39;label&#39;].unique()

for label in unique_labels:
# 过滤当前标签的数据
filtered_data = summary_points[summary_points[&#39;label&#39;] == label]

# 当前标签点的散点图
ax.scatter(filtered_data[&#39;PC1&#39;],
filtered_data[&#39;PC2&#39;],
filtered_data[&#39;PC3&#39;],
label=label)

ax.set_xlabel(&#39;PCA 组件 1&#39;)
ax.set_ylabel(&#39;PCA 组件 2&#39;)
ax.set_zlabel(&#39;PCA 组件 3&#39;)
ax.set_title(title)
ax.legend(title=&quot;Labels&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1.05, 0.7))

plt.subplots_adjust(left=0.05, right=0.75)
plt.show()


问题：
当我为相同手势记录一组新数据并绘制 PCA 结果时，我希望看到每个手势有 20 个点（10 个来自原始数据的点 + 10 个来自新数据的点）的聚类。
相反，PCA 图显示每个手势有两个独立的 10 点簇。这表明，即使手势相同，新数据也未与 PCA 空间中的原始数据对齐。

问题：
什么原因导致相同手势被分离为不同的簇？
可能与以下情​​况有关：

缩放过程？
传感器校准不一致？
在应用 PCA 之前是否需要对齐或对数据进行额外的预处理？
]]></description>
      <guid>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</guid>
      <pubDate>Sun, 08 Dec 2024 18:31:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在 nltk 中下载 punkt tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用 pip install nltk 安装了 NLTK 库
pip install nltk

在使用库时
from nltk.tokenize import sent_tokenize 
sent_tokenize(text)

我收到此错误
LookupError: 
**************************************************************************
未找到资源 punkt。
请使用 NLTK 下载器获取资源：

&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download(&#39;punkt&#39;)

有关更多信息，请参阅：https://www.nltk.org/data.html

尝试加载 tokenizers/punkt/english.pickle

搜索位置：
- &#39;C:\\Users\\adars/nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\share\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\lib\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Roaming\\nltk_data&#39;
- &#39;C:\\nltk_data&#39;
- &#39;D:\\nltk_data&#39;
- &#39;E:\\nltk_data&#39;
- &#39;&#39;

因此，为了解决此错误，我尝试了
import nltk
nltk.download(&#39;punkt&#39;)

但是我无法下载此包，因为每次运行此包时都会出现错误，提示
[nltk_data] 加载 punkt 时出错：&lt;urlopen 错误 [WinError 10060] A
[nltk_data] 连接尝试失败，因为连接方
[nltk_data] 在一段时间后未正确响应，或者
[nltk_data] 建立连接失败，因为连接的主机
[nltk_data] 未响应&gt;

请帮帮我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>分析客户支持单，了解产品缺陷/特点</title>
      <link>https://stackoverflow.com/questions/66428112/analyze-customer-support-tickets-to-understand-product-gaps-features</link>
      <description><![CDATA[我希望分析客户支持单，以了解产品差距/功能或我可以对产品进行哪些改进以解决客户痛点/问题。
但您知道，客户支持单中有很多文本/注释，这些文本/注释是由我们的支持代理通过电子邮件或电话收集的，并且从人的角度来说，不可能浏览所有单据并了解全局。
我正在从 Stack Overflow 上的开发人员那里寻求有关如何处理分析客户支持单以了解产品差距/功能或客户痛点的问题的想法。
您能给我指明正确的方向吗？我们可以使用 NLP 或任何其他 ML 概念来解决问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/66428112/analyze-customer-support-tickets-to-understand-product-gaps-features</guid>
      <pubDate>Mon, 01 Mar 2021 19:04:28 GMT</pubDate>
    </item>
    </channel>
</rss>