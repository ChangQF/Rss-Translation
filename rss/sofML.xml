<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 06 Mar 2024 00:57:04 GMT</lastBuildDate>
    <item>
      <title>torchserve ：即使 config.properties 指定其他值，batch_size 也始终为 1</title>
      <link>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</link>
      <description><![CDATA[我认为我的 torchserve 正确加载了 config.properties，因为我设置的工作人员数量是 2。但batch_size是1而不是20。
任何人都知道可能会出现什么问题吗？谢谢！
我已经检查并torchserve正确加载config.properties，可惜它忽略了config.properties中指定的batch_size和max_batch_delay。
这是我的 config.properties 供参考
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
log_file=/ml_server/logs/torchserve.log
default_workers_per_model=2
number_of_netty_threads=32
作业队列大小=1000
批量大小=20
最大批量延迟=10

下面是日志，worker的batchSize：1
ml-server | 2024-03-06T00：11：11,091 [信息] W-9001-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
机器学习服务器 | 2024-03-06T00：11：11,091 [信息] W-9000-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
]]></description>
      <guid>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</guid>
      <pubDate>Wed, 06 Mar 2024 00:19:31 GMT</pubDate>
    </item>
    <item>
      <title>如何修改我的神经网络以提高准确性？</title>
      <link>https://stackoverflow.com/questions/78110998/how-to-modify-my-neural-net-to-improve-accuracy</link>
      <description><![CDATA[我正在尝试构建一个神经网络来预测二进制输出 [0,1]。
我有一个非常小的数据集，有 600 个样本，其中 200 个标签为 0，其中 400 个标签为 1。我有 23 个特征，其中一些是分类特征（我使用 pd.get_dummies 对它们进行编码）。我还使用标准缩放器缩放数据。我有一种感觉，我构建网络的方式是错误的。由于我的数据集非常小，我使用 dropout 和 BatchNormalization 来防止过度拟合。
我可以对分类器进行哪些更改和改进以使其更加准确？
def make_classifier(self,optimizer):
            分类器=顺序（）
            
            classifier.add(Dropout(0.2, input_shape=(24,))) # 添加dropout层以防止overfiitng
            classifier.add(Dense(50, kernel_initializer = &quot;he_normal&quot;,kernel_regularizer=keras.regularizers.l2(0.01) , input_dim=24))#activation = &quot;relu&quot;,
            classifier.add(BatchNormalization())
            classifier.add(LeakyReLU(alpha=0.2))
            
            分类器.add(Dropout(0.2))
            classifier.add(Dense(20, kernel_initializer = &quot;he_normal&quot;,kernel_regularizer=keras.regularizers.l2(0.01)))
            classifier.add(BatchNormalization())
            classifier.add(LeakyReLU(alpha=0.2))#activation = &quot;relu&quot;,
            
            
            分类器.add(Dropout(0.15))
            classifier.add(Dense(1, kernel_initializer = “he_normal”, 激活 = “softmax”,kernel_regularizer=keras.regularizers.l2(0.01)))
            classifier.compile(optimizer=optimizer,loss=“binary_crossentropy”,metrics=[“accuracy”])
            返回分类器


#稍后我使用网格搜索：
  参数 = {
        &#39;batch_size&#39;:[50,150,250,batch_max],#样本数
        &#39;epochs&#39;:[5,11,32,50,64,100], ## 算法中训练数据的传输次数
        &#39;优化器&#39;:[&#39;adam&#39;,&#39;rmsprop&#39;,&#39;nadam&#39;]}
        
        grid_search = GridSearchCV(估计器=分类器,
                           param_grid=参数，
                           评分=“准确度”，
                           cv=2, error_score=&#39;raise&#39;)
        
        grid_search = grid_search.fit(X_train,y_train)
 
        best_param = grid_search.best_params_
        best_accuracy = grid_search.best_score_
        print(&#39;网格最佳精度&#39;, best_accuracy)
        



我还使用其他模型：随机森林、MLPClassifier、XGBoost、CATBoost。我确保它们不会过度拟合并且准确度在 90-95% 范围内。
我希望能达到类似的精度]]></description>
      <guid>https://stackoverflow.com/questions/78110998/how-to-modify-my-neural-net-to-improve-accuracy</guid>
      <pubDate>Tue, 05 Mar 2024 23:04:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 中创建一个自定义激活函数，要求将张量 x 提升到小数点</title>
      <link>https://stackoverflow.com/questions/78110583/create-a-custom-activation-function-in-keras-that-requeris-elevating-tensor-x-to</link>
      <description><![CDATA[我正在尝试在 Keras 中实现自定义激活函数，修改 sigmoid 函数如下：

该函数将 X 进行“a”次幂（必须是小数点，例如 0.3）在密集层中，使用 keras 后端解决回归问题。出现的问题是，当我应用服装函数时，它仅在值为整数时生成结果，当值为小数时，回归会生成 NaN。我实现这个方程的方法如下：
将 numpy 导入为 np
将 pandas 导入为 pd
从 keras.models 导入顺序
从 keras.layers 导入密集
将 matplotlib.pyplot 导入为 plt
从 scikeras.wrappers 导入 KerasRegressor
从 sklearn.model_selection 导入 KFold
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.preprocessing 导入 MinMaxScaler
将张量流导入为 tf
从 keras 导入后端为 K

def 自定义激活(x):
    return (1 / (1 + K.exp((-x)**0.5)))) #在此示例中 a=0.5 和 b=1

dataframe = pd.read_csv(“housing.csv”, delim_whitespace=True, header=None)
数据集 = dataframe.values

X = 数据集[:, :13]
Y = 数据集[:, 13]

缩放器 = MinMaxScaler(feature_range=(0.1, 0.9))

X_归一化 = 缩放器.fit_transform(X)
y_normalized = scaler.fit_transform(Y.reshape(-1, 1))

模型=顺序（）
model.add(密集(100,input_shape=(13,),激活=custom_activation))
model.add（密集（1，激活=&#39;线性&#39;））
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;adam&#39;)

a = model.fit(X_normalized, y_normalized,epochs=100, batch_size=102, verbose=0)
y_hat = model.predict(X_normalized)

文本
我尝试通过进行多次测试来解决这个细节，检查 Keras 是否可以将 X 提高到小数。考虑到要提高的值是 1/2（平方根），我复制了这个案例，我还实现了函数 relu 和 squared，向我展示了它们给出的结果：
def custom_activation(x):
   return (1 / (1 + K.exp((*K.sqrt(-x))))) #b=1

def custom_activation(x)
   return (1 / (1 + K.exp((*K.relu(-x))**0.2))) #b=1

不幸的是，我没有在 keras 后端找到类似于 relu 的东西，但用于线性输出。我观察到的另一个问题是方程 y&#39;=(-x)^(a-1)by(1-y) 的导数，其中元素 (-x)^(a-1 ）在我看来这可能会导致冲突，但是我也不知道如何制作自定义衍生产品。]]></description>
      <guid>https://stackoverflow.com/questions/78110583/create-a-custom-activation-function-in-keras-that-requeris-elevating-tensor-x-to</guid>
      <pubDate>Tue, 05 Mar 2024 21:13:35 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器模型错误？</title>
      <link>https://stackoverflow.com/questions/78110545/wrong-autoencoder-model</link>
      <description><![CDATA[我正在尝试使用张量流创建一个用于异常检测的自动编码器，该模型可以运行，但仅此而已。
这是模型：
int_vectorizer=layers.TextVectorization(
    最大令牌=10000,
    输出模式=&#39;int&#39;,
    输出序列长度=140
）
int_vectorizer.adapt(adapt_data)

模型 = tf.keras.Sequential([
    int_向量化器，
    层.密集（256，激活=“relu”），
    层.密集（128，激活=“relu”），
    层.密集（64，激活=“relu”），
    层.密集（32，激活=“relu”），
    层.密集（64，激活=“relu”），
    层.密集（128，激活=“sigmoid”），`
]）

label_converter=layers.StringLookup(output_mode=“int”)
label_converter.adapt(数据)

model.compile(optimizer=&#39;adam&#39;, loss=“mae”)
model.fit(数据, label_converter(数据), epochs=200)

测试时，我得到以下 2 个输入的相同百分比水平：
Inpus
我尝试更改层、节点数量、单元数量和不同的激活。]]></description>
      <guid>https://stackoverflow.com/questions/78110545/wrong-autoencoder-model</guid>
      <pubDate>Tue, 05 Mar 2024 21:05:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 的多变量时间序列</title>
      <link>https://stackoverflow.com/questions/78110519/multiple-multipvariate-time-series-with-lstm</link>
      <description><![CDATA[我正在处理的数据集可以在下面找到。
https://www.kaggle.com/datasets/behrad3d/nasa -cmaps/数据
数据是多个多元时间序列数据。该数据集跟踪发动机整个生命周期的 26 个特征，直至发生故障。它为 100 种不同的引擎执行此操作。有一个时间周期功能，它记录直到故障为止的时间周期数，从 1 开始。在我们的训练集中，我们可以从引擎当前的生命周期中减去引擎的最大时间周期，以获得 RUL，剩余可用生命周期，即我们将预测的功能。我们的测试集不包含 100 个引擎的完整生命周期数据。模型必须利用有限的数据来预测发动机的RUL。
我的数据集的当前形状是 20631, 126（我进行了特征工程，得到了 126 个特征）。这 20631 行包含 100 个引擎的时间序列数据。我知道如何将它们分成训练集和测试集，同时保持每个引擎数据的时间顺序。但需要注意的一件事是，引擎没有相同数量的 Time_Cycle。
我应该如何训练我的 LSTM，使其将每个引擎视为多元时间序列，并训练我将用于对所有 100 个引擎进行预测的模型？
下面是我编写的代码，但我认为它对数据的训练就好像它是一个引擎一样。我需要重塑我的输入吗？或者添加更多层？
从 keras.models 导入顺序
从 keras.layers 导入 LSTM，密集
从 keras.optimizers 导入 Adam
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np

# 将训练数据分为训练数据和测试数据
train_diff = train_diff.fillna(method=&#39;backfill&#39;)
如果 test_diff.columns 中为“RUL_pred”：
    test_diff = test_diff.drop(列=[&#39;RUL_pred&#39;])
test_diff = test_diff.fillna(method=&#39;backfill&#39;)

np.随机.种子(42)
random_indices = np.random.choice（列表（范围（1,101）），大小= 30，替换= False）

X_train = train_diff[~train_diff[&#39;unit_number&#39;].isin(random_indices)]
X_test = train_diff[train_diff[&#39;unit_number&#39;].isin(random_indices)]

Y_train = X_train[&#39;RUL&#39;]
Y_test = X_test[&#39;RUL&#39;]

X_train = X_train.drop(列=[&#39;RUL&#39;])
X_test = X_test.drop(列=[&#39;RUL&#39;])

缩放器 = MinMaxScaler()
缩放器.fit(X_train)
X_train_s = 缩放器.transform(X_train)
X_test_s = 缩放器.transform(X_test)
test_diff_s = 缩放器.transform(test_diff)

# 定义LSTM模型架构
模型=顺序（）
model.add(LSTM(units=64, input_shape=(X_train_s.shape[1], 1))) # 假设 X_train_s 已缩放
model.add(密集(单位=1))

# 编译模型
model.compile(optimizer=Adam(), loss=&#39;mean_squared_error&#39;)

# 重塑 LSTM 输入的数据（假设 X_train_s 和 X_test_s 已缩放）
X_train_s_lstm = np.reshape(X_train_s, (X_train_s.shape[0], X_train_s.shape[1], 1))
X_test_s_lstm = np.reshape(X_test_s, (X_test_s.shape[0], X_test_s.shape[1], 1))

# 训练 LSTM 模型
历史= model.fit（X_train_s_lstm，Y_train，epochs = 100，batch_size = 32，validation_split = 0.2）

# 对测试数据进行预测
y_lstm_pred = model.predict(X_test_s_lstm)

]]></description>
      <guid>https://stackoverflow.com/questions/78110519/multiple-multipvariate-time-series-with-lstm</guid>
      <pubDate>Tue, 05 Mar 2024 20:58:38 GMT</pubDate>
    </item>
    <item>
      <title>改进的模糊 c 均值算法将簇收敛到相同的坐标</title>
      <link>https://stackoverflow.com/questions/78110231/modified-fuzzy-c-means-algorithm-converges-clusters-to-same-coordinates</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78110231/modified-fuzzy-c-means-algorithm-converges-clusters-to-same-coordinates</guid>
      <pubDate>Tue, 05 Mar 2024 19:50:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 进行预测模型</title>
      <link>https://stackoverflow.com/questions/78110123/prediction-model-with-yolo</link>
      <description><![CDATA[我想知道是否可以使用 yolo 模型，并对其进行更改，使其成为实时图像输入的预测模型，并使用我自己的图像数据集训练模型。最终应该是一个具有实时图像输入的模型，使用我的数据集进行训练，并输出最有可能的百分比。]]></description>
      <guid>https://stackoverflow.com/questions/78110123/prediction-model-with-yolo</guid>
      <pubDate>Tue, 05 Mar 2024 19:26:48 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 自定义和默认目标和评估函数</title>
      <link>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</link>
      <description><![CDATA[我正在训练 BDT 以进行信号/背景的二元分类（我从事粒子物理学工作）。我的模型（用 python 实现）如下所示：
导入 xgboost 为 xgb
train = xgb.DMatrix(data=train_df[特征],label=train_df[“标签”],
                    缺失=“inf”，feature_names=特征，权重=(np.array(train_df[&#39;label&#39;].array)*-0.99+1))
测试= xgb.DMatrix(数据=test_df[特征],标签=test_df[“标签”],
                   缺失=“inf”，feature_names=特征，权重=(np.array(test_df[&#39;label&#39;].array) *-0.99+1))

参数 = {}


# 助推器参数
param[&#39;eta&#39;] = 0.1 # 学习率
param[&#39;max_depth&#39;] = 10 # 树的最大深度
param[&#39;subsample&#39;] = 0.5 # 训练树的事件分数
param[&#39;colsample_bytree&#39;] = 0.5 # 训练树的特征分数

# 学习任务参数
param[&#39;objective&#39;] = &#39;binary:logistic&#39; # 目标函数
param[&#39;eval_metric&#39;] = &#39;error&#39; # 交叉验证的评估指标
param = list(param.items()) + [(&#39;eval_metric&#39;, &#39;logloss&#39;)] + [(&#39;eval_metric&#39;, &#39;rmse&#39;)]


num_trees = 50 # 要制作的树的数量
booster = xgb.train(参数,train,num_boost_round=num_trees)

该模型表现良好，但我想稍微修改一下成本/损失函数，以便误报比真报受到更多惩罚。我正在寻找背景事件尽可能少的选择，即使这意味着牺牲信号的很大一部分。
根据自定义目标和评估函数的文档，我成功添加教程案例。
哪些是标准目标和评估函数（或者我可能已经在模型中实现的函数）？
由于模型已经表现良好，我只想为已经实现的功能添加一个额外的术语。
作为参考，我正在使用的软件包版本：
python 版本：3.10.12（主要，2023 年 11 月 20 日，15:14:05）[GCC 11.4.0]
XGBoost版本：2.0.2
熊猫版本：2.1.4
Numpy 版本：1.26.2
]]></description>
      <guid>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</guid>
      <pubDate>Tue, 05 Mar 2024 18:53:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么epoch02这么慢？</title>
      <link>https://stackoverflow.com/questions/78109909/why-is-epoch02-so-slow</link>
      <description><![CDATA[我正在 Mac M2 芯片上训练 YOLOV9。第一个 epoch 花了 7 分钟，但第二个 epoch 花了 2 小时。为什么两个 epoch 的训练时间相差这么大？
我的代码
# DDP模式
如果不是 torch.backends.mps.is_available():
    如果不是 torch.backends.mps.is_built():
        print(“MPS 不可用，因为当前 PyTorch 安装不是”
              “在启用 MPS 的情况下构建。”）
    别的：
        print(&quot;MPS 不可用，因为当前的 MacOS 版本不是 12.3+ &quot;
              “和/或您在此计算机上没有支持 MPS 的设备。”）
    设备= select_device(opt.device,batch_size=opt.batch_size)
别的：
    device = torch.device(“mps”)
如果 LOCAL_RANK ！= -1：
    msg = &#39;与 YOLO 多 GPU DDP 训练不兼容&#39;
    断言不是 opt.image_weights, f&#39;--image-weights {msg}&#39;
    断言不是 opt.evolve, f&#39;--evolve {msg}&#39;
    断言 opt.batch_size != -1, f&#39;AutoBatch with --batch-size -1 {msg}，请传递有效的 --batch-size&#39;
    断言 opt.batch_size % WORLD_SIZE == 0, f&#39;--batch-size {opt.batch_size} 必须是 WORLD_SIZE&#39; 的倍数
    断言 torch.backends.mps.is_available(),“MPS 不可用。检查 PyTorch 版本、macOS 版本和硬件”
    # 断言 torch.cuda.device_count() &gt; LOCAL_RANK，“用于 DDP 命令的 CUDA 设备不足”
    device = torch.device(“mps”)
    dist.init_process_group(backend=“nccl” if dist.is_nccl_available() else “gloo”)

批量大小 = 8
我将批量大小设置为 8。
您可以看到GPU确实正在被使用。
在此处输入图片描述
在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78109909/why-is-epoch02-so-slow</guid>
      <pubDate>Tue, 05 Mar 2024 18:43:43 GMT</pubDate>
    </item>
    <item>
      <title>Accuracy_score 出现错误</title>
      <link>https://stackoverflow.com/questions/78109873/erroring-in-accuracy-score</link>
      <description><![CDATA[我想在 jupyter 笔记本的数据框中使用 precision_score 但出现此错误：
ValueError：不支持连续

我写道：
从sklearn.metrics导入accuracy_score
train_prediction = model.predict(X_train)
训练准确度=准确度得分（y_train，train_prediction）
]]></description>
      <guid>https://stackoverflow.com/questions/78109873/erroring-in-accuracy-score</guid>
      <pubDate>Tue, 05 Mar 2024 18:38:58 GMT</pubDate>
    </item>
    <item>
      <title>不平衡文本数据集中的准确率较高，但精度和召回率较低</title>
      <link>https://stackoverflow.com/questions/78109777/good-accuracy-but-low-precision-and-recall-in-imbalanced-text-dataset</link>
      <description><![CDATA[文本数据集极其不平衡，在总共 1200 条记录中，少数数据集约为 120 条。数据集有两个类别“Y”和“N”。
我已经尝试过

数据已预处理。
使用 n_grams 的 TfIdfVectorizer 来获取更有意义的数据。
随机森林分类器、XGB分类器、SVC。
上采样和下采样

尽管如此，我在上述任何方法中都没有获得良好的精确度或召回率。
我原以为 UpSampling 在这种情况下应该可以正常工作，但事实并非如此。
我错过了什么吗？任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78109777/good-accuracy-but-low-precision-and-recall-in-imbalanced-text-dataset</guid>
      <pubDate>Tue, 05 Mar 2024 18:20:46 GMT</pubDate>
    </item>
    <item>
      <title>尝试建立一个lstm模型</title>
      <link>https://stackoverflow.com/questions/78108041/trying-to-build-an-lstm-model</link>
      <description><![CDATA[**构建 lstm 模型并获取形状错误**
模型=顺序（）
词汇大小 = 10000
嵌入尺寸 = 100
最大序列长度 = 21
model.add(嵌入(input_dim=vocab_size,
output_dim=embedding_dim))
model.add(LSTM(单位=50,return_sequences=False))
model.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
model.compile(loss=&#39;binary_crossentropy&#39;, 优化器=&#39;adam&#39;,
指标=[&#39;准确性&#39;])
模型.summary()

这是错误
输入 Tensor 的输入形状无效(“sequential_4_1/Cast:0”,
形状=（无，21），dtype=float32）。预期形状（无、9144、
21)，但输入的形状不兼容（无，21）
Sequential.call() 收到的参数：
输入=tf.Tensor（形状=（无，21），dtype=int64）
• 训练=真
•掩码=无]]></description>
      <guid>https://stackoverflow.com/questions/78108041/trying-to-build-an-lstm-model</guid>
      <pubDate>Tue, 05 Mar 2024 13:22:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用kaggle中的两个GPU在pytorch中进行训练？</title>
      <link>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</link>
      <description><![CDATA[我正在 Kaggle GPU 中训练模型。
但正如我所看到的，只有一个 GPU 正在工作。

我使用普通方法进行训练，例如
device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)
模型 = model.to(设备)

如何同时使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</guid>
      <pubDate>Wed, 13 Sep 2023 04:56:24 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch DataLoader内存未释放</title>
      <link>https://stackoverflow.com/questions/52015010/pytorch-dataloader-memory-is-not-released</link>
      <description><![CDATA[我想在google collaboratory上的pythorch上实现SRGAN，但是DataLoader的内存似乎被释放了，所以如果你转动epoch，就会出现内存错误。
如果您告诉我如何执行此操作以释放每批内存，我将不胜感激。
这是代码的github链接
https://github.com/pacifinapacific/Hello-World/blob/ master/Untitled0.ipynb
48 了，1 个 echoch 发生内存错误，
如果将批量大小设置为 8 的 1/6，则在大约 6 epoch 时会出现错误。
我正在使用以下代码读取高分辨率和低分辨率图像。扩展图像文件夹
但是例如，即使执行学习时出现错误，GPU的内存也不会释放
类 DownSizePairImageFolder(ImageFolder):
    def __init__(自身，根，变换=无，large_size=256，small_size=64，**kwds)：
        super().__init__(根，变换=变换，**kwds)
        self.large_resizer = 变换.Scale(large_size)
        self.small_resizer = 变换.Scale(small_size)
    
    def __getitem__(自身，索引)：
        路径, _ = self.imgs[索引]
        img = self.loader(路径)
        Large_img = self.large_resizer(img)
        Small_img = self.small_resizer(img)
        如果 self.transform 不是 None：
            Large_img = self.transform(large_img)
            小img = self.transform(small_img)
        返回小img、大img


train_data = DownSizePairImageFolder(&#39;./lfw-deepfunneled/train&#39;,transform=transforms.ToTensor())
test_data = DownSizePairImageFolder(&#39;./lfw-deepfunneled/test&#39;,transform=transforms.ToTensor())
批量大小 = 8
train_loader = DataLoader(train_data,batch_size,shuffle=True)
test_loader = DataLoader(test_data,batch_size,shuffle=False)
]]></description>
      <guid>https://stackoverflow.com/questions/52015010/pytorch-dataloader-memory-is-not-released</guid>
      <pubDate>Sat, 25 Aug 2018 07:25:26 GMT</pubDate>
    </item>
    <item>
      <title>石榴贝叶斯网络的样本</title>
      <link>https://stackoverflow.com/questions/51035303/sample-from-a-bayesian-network-in-pomegranate</link>
      <description><![CDATA[我在石榴中使用 from_samples() 构建了一个贝叶斯网络。我可以使用 model.predict() 从模型中获得最大可能的预测。我想知道是否有一种方法可以有条件（或无条件）从这个贝叶斯网络中采样？即是否从网络中获取随机样本而不是最大可能的预测？ 
我查看了 model.sample()，但它引发了 NotImplementedError。
此外，如果使用 pomegranate 无法做到这一点，那么还有哪些其他库非常适合 Python 中的贝叶斯网络？]]></description>
      <guid>https://stackoverflow.com/questions/51035303/sample-from-a-bayesian-network-in-pomegranate</guid>
      <pubDate>Tue, 26 Jun 2018 05:14:13 GMT</pubDate>
    </item>
    </channel>
</rss>