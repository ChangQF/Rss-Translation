<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 05 May 2024 09:16:09 GMT</lastBuildDate>
    <item>
      <title>整合 BERT 和 RAG 进行体育评论中的情绪分析：需要有关处理大数据和聚合的建议</title>
      <link>https://stackoverflow.com/questions/78431227/integrating-bert-and-rag-for-sentiment-analysis-in-sports-commentary-need-advic</link>
      <description><![CDATA[我目前正在开展一个深度学习项目，我们将 BERT 嵌入与检索增强生成 (RAG) 模型相集成，以分析体育评论中的情绪。我们处理数千条游戏评论，使用 BERT 将它们转换为嵌入，然后使用 RAG 生成见解。我有一些关于处理和存储这些嵌入的最佳实践以及设计系统以有效利用单个数据点和聚合情感数据的最佳实践的问题。
存储和使用嵌入：如果我的评论有 1,000 个嵌入，我是否应该将它们全部存储在数据库中的一个文件中，然后将它们直接传递到 RAG 中？重新嵌入这些嵌入是否明智，否则会导致有意义的信息丢失？
汇总情绪分析：我们注意到，这些评论的情绪只有在汇总后才真正有意义。例如，如果 700 条评论支持一个团队，300 条评论支持另一个团队，我们可以推断大多数支持第一团队。然而，如果将这些情绪分开并单独传递给 RAG，它还能清楚地评估哪支球队是球迷最喜欢的吗？我应该如何在 RAG 模型中处理这个问题？
数据处理的混合方法：考虑到个人评论数据和聚合情绪数据，如何有效配置文档存储和 RAG 设置以有效处理这些数据？是否有最适合此类应用程序的推荐方法或技术堆栈？
我正在寻求有关如何构建这些数据并优化我们的检索和生成流程的建议，以有效处理详细的个人评论和更广泛的情绪趋势。任何见解、经验或建议将不胜感激！
谢谢！
刚刚开始研究这个！]]></description>
      <guid>https://stackoverflow.com/questions/78431227/integrating-bert-and-rag-for-sentiment-analysis-in-sports-commentary-need-advic</guid>
      <pubDate>Sun, 05 May 2024 06:13:41 GMT</pubDate>
    </item>
    <item>
      <title>用于训练对象检测模型的最佳云服务？ （YOLO）[关闭]</title>
      <link>https://stackoverflow.com/questions/78430422/best-cloud-service-for-training-an-object-detection-model-yolo</link>
      <description><![CDATA[正如标题所示，我正在寻找用于模型训练的云服务，特别是 YOLOv8。
原因是 - 我的日常驾驶是 mac air m1，而且我还有一台支持 cuda 的 Windows 笔记本电脑，但对于我当前的数据集（大约 10k 图像），使用 YOLOv8s（几乎最小的版本）预训练模型进行训练最多是最好的纪元一个小时。因此，我很自然地在寻找一种解决方案来缩短训练时间。
我知道一些云服务，例如 GCP、azure、AWS，但对于我的目的来说，是否有任何服务更好/更差？
我不介意付费，但最好选择免费。]]></description>
      <guid>https://stackoverflow.com/questions/78430422/best-cloud-service-for-training-an-object-detection-model-yolo</guid>
      <pubDate>Sat, 04 May 2024 21:19:05 GMT</pubDate>
    </item>
    <item>
      <title>尽管 CSV 文件中存在该列，但在数据集中未找到该列？</title>
      <link>https://stackoverflow.com/questions/78430317/column-not-found-in-dataset-even-though-it-exists-in-the-csv-file</link>
      <description><![CDATA[我一直在尝试处理 csv 文件中的一些数据，该文件包含许多列，其中一个“Date”列不断产生错误。我知道它肯定存在于 CSV 文件中，但我不断收到错误消息：
“ValueError：在数据集中找不到列“日期”。”


在线找到一个建议，将 CSV 文件中的日期格式修复为“YYYY-MM-DD&#39;，但在我尝试保存 CSV 后，它不断重置回默认的 DD-MM-YYYY 格式。我没有办法尝试，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78430317/column-not-found-in-dataset-even-though-it-exists-in-the-csv-file</guid>
      <pubDate>Sat, 04 May 2024 20:34:02 GMT</pubDate>
    </item>
    <item>
      <title>在自定义数据上训练 Mask R-CNN，但训练不会停止并且不会产生输出或错误</title>
      <link>https://stackoverflow.com/questions/78430026/training-mask-r-cnn-on-custom-data-but-the-training-doesnt-stop-and-produces-n</link>
      <description><![CDATA[以下是我的流程的简要概述：

我通过将边界框的 SAM 掩码应用到我的图像，使用 PyTorch 生成数据集。
创建数据集后，我将其分为训练集和测试集。
我使用 torch.utils.data.DataLoader 加载这两个集合。
我使用的是包含 11 个类别的预训练模型。

但是，我在训练过程中遇到了问题。该过程似乎花费了异常长的时间，并且我没有看到任何可从中进行故障排除的进度或错误消息。
我的数据集

可能出了什么问题或者如何改进我的培训过程？]]></description>
      <guid>https://stackoverflow.com/questions/78430026/training-mask-r-cnn-on-custom-data-but-the-training-doesnt-stop-and-produces-n</guid>
      <pubDate>Sat, 04 May 2024 18:34:21 GMT</pubDate>
    </item>
    <item>
      <title>过采样如何影响使用分类器的后验概率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78429622/how-does-oversampling-affects-my-posterior-probability-using-a-classifier</link>
      <description><![CDATA[我正在使用随机森林算法来区分噪声和信号。我预计 400.000 个“噪音”事件中有 40 个“信号”。
我的问题是，如果我对“信号”事件进行过采样，根据 this论文，我会将我的概率推向零。
那么最好的策略是什么？

使用现实世界人口，然后使用排名作为概率
使用权重
对信号进行过采样/对信号进行欠采样？

谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78429622/how-does-oversampling-affects-my-posterior-probability-using-a-classifier</guid>
      <pubDate>Sat, 04 May 2024 16:10:09 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实现变分自动编码器的反向传播</title>
      <link>https://stackoverflow.com/questions/78429529/implementing-backpropagation-from-scratch-for-variational-autoencoder</link>
      <description><![CDATA[问题
我们正在从头开始实现 VAE，但可以使用 torch.Tensor 数据结构来实现 GPU 功能。这是一项学校作业，我们似乎难以应对。我们寻找了有关 VAE 的指南和教程，但每个指南和教程都使用 PyTorch/TensorFlow，这完全混淆了反向传播过程。我们正在寻求帮助来启动反向传播过程，因为似乎一旦启动，它就会自然地通过您的网络向后传播。
架构
编码器 - 第 4 步之后有一个分割，其中第 5 - 7 步针对 latent_mean 和 latent_log 完成

输入（419, 419）
压平
密集（419 * 419, 256）
LeakyRelu
密集(256, 100)
LeakyRelu
标准化

解码器

密集（100, 256）
LeakyRelu
标准化
密集（256, 419 * 419）
标准化
重塑

流程
# 我们当前的实现对每个样本进行前向/后向传递
# 但是，权重会在每批之后更新
Latent_mean, Latent_log = 编码器.forward(x)
Latent_vector = 重新参数化（latent_mean，latent_log）

重建=解码器.forward(latent_vector)

重建损失 = torch.mean((x - 重建) ** 2)
kl_divergence = -0.5 * torch.mean(1 + Latent_log - Latent_mean ** 2 - torch.exp(latent_log))
损失 = 重建损失 + kl_divergence

这就是我们陷入困境的地方。从课堂上来看，我们似乎应该计算损失函数相对于解码器输出神经元权重的导数。我们的课堂笔记还表明，输出层推导通常与隐藏层推导不同。然而，网上很多神经网络的例子似乎并没有做出这种区分。我知道我们正在使用一个众所周知的损失函数，但我不确定如何区分它或损失函数的名称是什么（我们在一篇随机博客文章中发现了它，并且从那以后已经多次看到它）。
我在网上看到了其他示例，通过将损失/误差乘以激活函数的导数对输出激活的应用来开始反向传播过程，或者
delta = 错误 * output_activation_function_derivative(output_activation)
如果我们能弄清楚如何计算初始增量，我认为我们可以向后跟踪网络并更新其余的权重，几乎没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/78429529/implementing-backpropagation-from-scratch-for-variational-autoencoder</guid>
      <pubDate>Sat, 04 May 2024 15:37:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中训练朴素贝叶斯模型进行情绪分析</title>
      <link>https://stackoverflow.com/questions/78429490/how-to-train-in-python-a-naive-bayes-model-for-sentiment-analysis</link>
      <description><![CDATA[我正在尝试训练朴素贝叶斯模型进行情感分析，但我是 Python 新手，因为我一直在 R 中工作。
导入 pandas 作为 pd
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.naive_bayes 导入 MultinomialNB
从sklearn.metrics导入accuracy_score，confusion_matrix
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.pipeline 导入管道
from sklearn.preprocessing import LabelEncoder # 添加这一行

# 加载预处理后的TF-IDF矩阵
tfidf_df = pd.read_excel(&#39;/Users/anisabakiu/Downloads/tfidf_r.xlsx&#39;)

# 加载带有标签的原始DataFrame
df = pd.read_excel(&#39;/Users/anisabakiu/Downloads/all-review_label.xlsx&#39;)

# 删除 NaN 值（如果有）
merged_df = pd.merge(tfidf_df, df[[&#39;review_id&#39;, &#39;label&#39;]], on=&#39;review_id&#39;).dropna()

# 如果需要的话对标签进行编码
label_encoder = LabelEncoder() # 实例化LabelEncoder
merged_df[&#39;label&#39;] = label_encoder.fit_transform(merged_df[&#39;label&#39;]) # 对标签进行编码

# 定义特征（X）和目标变量（y）
X = merged_df.drop([&#39;label&#39;, &#39;review_id&#39;], axis=1)
y = merged_df[&#39;标签&#39;]

# 定义一个 TF-IDF 矢量器
tfidf_vectorizer = TfidfVectorizer()

# 创建朴素贝叶斯分类器
naive_bayes = MultinomialNB()

# 创建一个结合 TF-IDF 矢量器和朴素贝叶斯分类器的管道
管道=管道（[
    （&#39;tfidf&#39;，tfidf_向量化器），
    (&#39;clf&#39;, naive_bayes),
]）

# 进行10次交叉验证
cv_scores = cross_val_score(管道, X, y, cv=10)

# 在整个数据集上拟合朴素贝叶斯模型
naive_bayes_model = pipeline.fit(X, y)

# 使用经过训练的模型进行预测
y_pred = naive_bayes_model.predict(X)

# 评估模型
准确度=准确度_分数（y，y_pred）
print(&#39;准确度：&#39;, 准确度)

# 打印混淆矩阵
conf_matrix = fusion_matrix(y, y_pred)
print(&#39;混淆矩阵：&#39;)
打印（conf_matrix）

这段代码对我来说似乎是正确的，但我收到了这个错误：raise KeyError(key) from err KeyError: &#39;label&#39;，我不明白。
你能帮我看看出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78429490/how-to-train-in-python-a-naive-bayes-model-for-sentiment-analysis</guid>
      <pubDate>Sat, 04 May 2024 15:28:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Transformer 中使用 LabelEncoding 的 ML 模型管道</title>
      <link>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</link>
      <description><![CDATA[我正在尝试将各种转换与 LightGBM 模型一起整合到 scikit-learn 管道中。该模型旨在预测二手车的价格。一旦训练完成，我计划将此模型集成到 HTML 页面中以供实际使用。
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import joblib

print(numeric_features)
`[&#39;car_year&#39;, &#39;km&#39;, &#39;horse_power&#39;, &#39;cyl_capacity&#39;]`
print(categorical_features)
`[&#39;make&#39;, &#39;model&#39;, &#39;trimlevel&#39;, &#39;fueltype&#39;, &#39;transmission&#39;, &#39;bodytype&#39;, &#39;color&#39;]`

# 为数字和分类特征定义转换器
numeric_transformer = Pipeline(steps=[(&#39;scaler&#39;, StandardScaler())])
categorical_transformer = Pipeline(steps=[(&#39;labelencoder&#39;, LabelEncoder())])

# 使用 ColumnTransformer 组合转换器
预处理器= ColumnTransformer(
transformers=[
(&#39;num&#39;, numeric_transformer, numeric_features),
(&#39;cat&#39;, categorical_transformer, categorical_features)
]
)

# 将 LightGBM 模型附加到预处理管道
pipeline = Pipeline(steps=[
(&#39;preprocessor&#39;, preprocessor),
(&#39;model&#39;, best_lgb_model)
])

# 将管道适配到训练数据
pipeline.fit(X_train, y_train)

我在训练时得到的输出是：
LabelEncoder.fit_transform() 需要 2 个位置参数，但给出了 3 个]]></description>
      <guid>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</guid>
      <pubDate>Sat, 04 May 2024 15:12:44 GMT</pubDate>
    </item>
    <item>
      <title>努力解决卷积自动编码器的输入和输出形状差异</title>
      <link>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</link>
      <description><![CDATA[在拟合我的模型时，我收到以下错误：
ValueError：层“sequence_15”的输入 0 与层不兼容：预期形状 = (None, 27088, 64, 1)，发现形状 = (None, 27086, 64, 1)
我认为 MaxPool2D 是向下舍入，而 UpSamling2D 是向上舍入。当我查看模型摘要时，我看到底部舍入导致不同的输入和输出形状，但是，我很难找到充分适合该模型的必要参数。
这是该模型的代码块：
input_shape=BUFFER_INPUT_SHAPE_WITHCHANNELS
# n_channels = input_shape[-1]
# 编码器
model = Sequential()
model.add(Conv2D(256, (4,4),activation=&#39;relu&#39;,padding=&#39;same&#39;,
input_shape=input_shape)) # (27086, 64, 1)
model.add(MaxPool2D((2,2),padding=&#39;same&#39;))
model.add(Conv2D(128, (4,4),activation=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(MaxPool2D((2,2), padding=&#39;same&#39;))
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(MaxPool2D((2,2), padding=&quot;same&quot;))
# 解码器
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(128, (4,4), 激活=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(256, (4,4), 激活=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(1, (4,4),activation=&#39;sigmoid&#39;,
padding=&#39;same&#39;))

model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,
metrics=[&#39;mse&#39;])
model.summary()

_________________________________________________________________

层 (类型) 输出形状参数 #
=

conv2d_135 (Conv2D) (无, 27086, 64, 256) 4352

max_pooling2d_60 (MaxPoolin (无, 13543, 32, 256) 0 
g2D)

conv2d_136 (Conv2D) (无, 13543, 32, 128) 524416

max_pooling2d_61 (MaxPoolin (无，6772，16，128) 0 
g2D)

conv2d_137 (Conv2D) (无，6772，16，64) 131136

max_pooling2d_62 (MaxPoolin (无，3386，8，64) 0 
g2D)

conv2d_138 (Conv2D) (无，3386，8，64) 65600

up_sampling2d_58 (UpSamplin (无，6772，16，64) 0 
g2D)

conv2d_139 (Conv2D) (无，6772，16，128) 131200

up_sampling2d_59 (UpSamplin (None, 13544, 32, 128) 0 
g2D)

conv2d_140 (Conv2D) (None, 13544, 32, 256) 524544

up_sampling2d_60 (UpSamplin (None, 27088, 64, 256) 0 
g2D)

conv2d_141 (Conv2D) (None, 27088, 64, 1) 4097

如果您发现任何其他提示或改进，请告诉我。
我尝试更改池化、内核和步幅参数。我还尝试通过将原始输入修剪为可多次分割的形状来更改原始输入。但是，我不确定这种方法是否通常适用。
编辑：
我将数据修剪为形状 (27072,64,1)。但这似乎不是解决我的问题的合适方法。我不想修剪我的数据]]></description>
      <guid>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</guid>
      <pubDate>Sat, 04 May 2024 14:40:11 GMT</pubDate>
    </item>
    <item>
      <title>饮食推荐系统的python代码[关闭]</title>
      <link>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</link>
      <description><![CDATA[我一直在尝试在 GitHub 上找到的这段代码：https://github .com/zakaria-narjis/Diet-Recommendation-System 用于饮食和食物建议。
它的效果很好，但食物建议（食谱中的总卡路里）通常比我每天必须摄入的饮食计划和卡路里（“计划”卡路里）更高，尤其是当它是五顿饭并且它提供的时候用正餐换零食，这没有意义，数据包含真正的零食。
您可以在此处测试系统https://diet-recommendation-system.streamlit.app /Diet_Recommendation 并发现它提供的卡路里比我必须摄入的要多！
我对完整代码做了一些调整，因为它一开始不起作用，但是这里是我认为有问题的函数：
defgenerate_recommendations(self,):
    总卡路里=self.weight_loss*self.calories_calculator()
    建议=[]
    self.meals_calories_perc 中的膳食：
        膳食卡路里=self.meals_calories_perc[膳食]*total_calories
        如果餐==&#39;早餐&#39;：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10 ),rnd(30,100)]
        elif 餐==&#39;发射&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        elif 餐==&#39;晚餐&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        别的：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10 ),rnd(30,100)]
        生成器=生成器（推荐_营养）
        Recommended_recipes=generator.generate().json()[&#39;输出&#39;]
        推荐.追加（推荐菜谱）
    对于推荐中的推荐：
        推荐食谱：
            食谱[&#39;image_link&#39;]=find_image(食谱[&#39;名称&#39;])
    返回建议
]]></description>
      <guid>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</guid>
      <pubDate>Sat, 04 May 2024 07:57:23 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的多线程无法在 Raspberry Pi 上正常工作</title>
      <link>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</guid>
      <pubDate>Fri, 03 May 2024 12:10:47 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法评估模型是否能够识别有影响的变量（使用 make_classification 生成的变量）？</title>
      <link>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。我使用 make_classification（二元分类任务）创建了一个数据集，目的是测试不同模型区分重要特征和不太重要特征的能力。
如何设置一个实验来评估模型是否能够识别有影响的变量？
我查看了 make_classification 的文档，但不幸的是我没有进一步了解。
我设置了以下内容：
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

如何显示 - 在本例中 - 5 个信息变量？使用 make_classification 生成数据时可以确定特征的重要性吗？ make_classification 认为哪些功能很重要？然后在下一步中，我将使用一些 freature_importance 方法来验证（或不验证）模型检测“预设”特征的效果如何。特征重要性/具有影响力的变量。
谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</guid>
      <pubDate>Sun, 28 Apr 2024 11:37:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 获取 One vs Rest SVC() 的模型参数？</title>
      <link>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</link>
      <description><![CDATA[我尝试使用decision_function_shape= ovr制作onvsrest分类模型，但是当我将其更改为decision_function_shape= ovo时，它给了我与ovr相同的结果。结果我读到 svc() 正在使用 ovo 作为基础，无论它是作为 ovr 还是 ovo 启动的。那么我怎样才能改变我的代码，以便它给我一个 ovr 结果呢？
model3 = SVC(kernel = &#39;rbf&#39;, Decision_function_shape=&#39;ovr&#39;)
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)

我尝试过使用 OneVsRestClassifier() 但不知道如何给出所有这些命令的输出，它总是出错并说 OneVsRestClassifier 没有这些命令。有没有办法用 OneVsRestClassifier 获取 cm、sm、sv、beta 和截距？
cm3 = fusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = 分类报告（Y_测试，model3_预测）
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)

希望有人能帮助我，先谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</guid>
      <pubDate>Sat, 27 Apr 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>如何将两个不同训练的 ML 模型合并为一个？</title>
      <link>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</link>
      <description><![CDATA[我已经根据两个不同的数据集训练了两个机器学习模型。然后我将它们保存为 model1.pkl 和 model2.pkl 。有两个用户输入（不是模型的输入数据），例如 x=0 和 x=1，如果 x=0 我必须使用 model1.pkl 进行预测，否则我必须使用 model2.pkl 进行预测。我可以使用 if 条件来完成它们，但我的问题是我必须知道是否有可能将其保存为 model.pkl ，包括此条件语句。如果我将它们组合起来并另存为模型，那么就可以很容易地在其他 IDE 中加载。]]></description>
      <guid>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</guid>
      <pubDate>Thu, 12 Nov 2020 09:44:27 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow.js 分词器</title>
      <link>https://stackoverflow.com/questions/51663068/tensorflow-js-tokenizer</link>
      <description><![CDATA[我是机器学习和 Tensorflow 的新手，因为我不了解 python，所以我决定使用 javascript 版本（可能更像是包装器）。 
问题是我试图构建一个处理自然语言的模型。因此，第一步是对文本进行分词，以便将数据提供给模型。我做了很多研究，但大多数都使用 python 版本的tensorflow，使用如下方法：tf.keras.preprocessing.text.Tokenizer，我在tensorflow.js中找不到类似的方法。我陷入了这一步，不知道如何将文本传输到可以输入模型的向量。请帮忙:)]]></description>
      <guid>https://stackoverflow.com/questions/51663068/tensorflow-js-tokenizer</guid>
      <pubDate>Thu, 02 Aug 2018 22:40:12 GMT</pubDate>
    </item>
    </channel>
</rss>