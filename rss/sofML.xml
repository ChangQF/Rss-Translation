<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 06 Dec 2024 21:16:24 GMT</lastBuildDate>
    <item>
      <title>我正在尝试对 llm 模型进行 l2 修剪并绘制困惑度，但无法正确计算困惑度</title>
      <link>https://stackoverflow.com/questions/79259367/i-am-trying-to-l2-prune-an-llm-model-and-plot-perplexity-but-i-am-unable-to-get</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79259367/i-am-trying-to-l2-prune-an-llm-model-and-plot-perplexity-but-i-am-unable-to-get</guid>
      <pubDate>Fri, 06 Dec 2024 20:55:18 GMT</pubDate>
    </item>
    <item>
      <title>使用共生矩阵时如何可视化 Kmeans 聚类？</title>
      <link>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</link>
      <description><![CDATA[我正在尝试使用 Kmeans 对单词进行聚类。我有一个大型文档，我首先使用 NLTK RegexpTokenizer，然后根据字数、长度进行过滤并删除停用词。接下来，我构建一个共现矩阵并使用它来训练 Kmeans 模型。最后，我使用轮廓分数测试它的性能。
我想可视化这些集群。这通常似乎是使用散点图和标签来完成的。当它目前是一个 N x N 矩阵（N 是唯一单词的数量）时，我如何将这个共现矩阵简化为 x 和 y？我尝试过使用 PCA：
plt.figure()
pca_2d = PCA(n_components=2)
reduced = pca_2d.fit_transform(mat)
newKm = KMeans(n_clusters=3)
labels = newKm.fit_predict(reduced)
plt.scatter(reduced[:, 0], Reduced[:, 1], c=labels) # 选择第 0 列（所有行）作为 x 坐标，第 1 列（所有行）作为 y 坐标。然后对标签进行聚类。
plt.title(&quot;K-means Clustering&quot;)
plt.show()

但不确定这是最佳或正确的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</guid>
      <pubDate>Fri, 06 Dec 2024 18:55:00 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 Pytorch DDP 中获取空闲端口的问题？</title>
      <link>https://stackoverflow.com/questions/79259010/how-to-solve-the-issue-with-getting-free-ports-in-pytorch-ddp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79259010/how-to-solve-the-issue-with-getting-free-ports-in-pytorch-ddp</guid>
      <pubDate>Fri, 06 Dec 2024 18:30:52 GMT</pubDate>
    </item>
    <item>
      <title>下载模型时可教机器不准确[关闭]</title>
      <link>https://stackoverflow.com/questions/79258040/teachable-machine-is-inaccurate-when-model-is-downloaded</link>
      <description><![CDATA[我正在为我的论文做一个应用程序，我们决定使用 Teachable Machine 进行图像识别，我对其进行了测试并查看了一些相关内容，它看起来不错，所以我们决定这样做。现在，当我训练我的一些模型时，我在网络上尝试它非常准确，但是当我将其导出到 tensorflow lite（浮点）并在 Android Studio 中编写代码时，它会错误分类图片，这些图片也在训练集中。现在我的问题是，有没有办法解决这个问题？如果没有，有没有简单的方法来训练我自己的模型？
public void classifyImage(Bitmap image) {
try {
ModelUnquant model = ModelUnquant.newInstance(getApplicationContext());

TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 224, 224, 3}, DataType.FLOAT32);
ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4 * imageSize * imageSize * 3);
byteBuffer.order(ByteOrder.nativeOrder());

int[] intValues = new int[imageSize * imageSize];
image.getPixels(intValues, 0, image.getWidth(), 0, 0, image.getWidth(), image.getHeight());

int pixel = 0;
for (int i = 0; i &lt; imageSize; i++) {
for (int j = 0; j &lt; imageSize; j++) {
int val = intValues[pixel++];
byteBuffer.putFloat(((pixel &gt;&gt; 16) &amp; 0xFF) / 255.0f); // 红色
byteBuffer.putFloat(((pixel &gt;&gt; 8) &amp; 0xFF) / 255.0f); // 绿色
byteBuffer.putFloat((pixel &amp; 0xFF) / 255.0f); // 蓝色
}
}

inputFeature0.loadBuffer(byteBuffer);

ModelUnquant.Outputs 输出 = model.process(inputFeature0);
TensorBuffer 输出Feature0 = 输出.getOutputFeature0AsTensorBuffer();

float[] 置信度 = outputFeature0.getFloatArray();
int maxPos = 0;
float maxConfidence = 0;
for (int i = 0; i &lt; 置信度.length; i++) {
if (置信度[i] &gt; maxConfidence) {
maxConfidence = 置信度[i];
maxPos = i;
}
}

String[] classes = {&quot;食物&quot;, &lt;塑料瓶&quot;, &lt;面罩&quot;, &lt;塑料餐具&quot;, &lt;注射器&quot;};
StringdetectedObject = classes[maxPos];

// 检查置信度是否低于 35%
if (maxConfidence &lt;= 0.35f) {
// 如果置信度低，则显示&quot;无法分类&quot;消息
detectedObject = &quot;未知&quot;;
}

showBottomSheet(detectedObject, maxConfidence);

String s = &quot;&quot;;
for (int i = 0; i &lt; classes.length; i++) {
s += String.format(&quot;%s: %.1f%%\n&quot;, classes[i],confidences[i] * 100);
}

// 将结果打印到控制台日志
Log.d(&quot;ClassificationResult&quot;, s);

model.close();
} catch (IOException e) {
e.printStackTrace();
}
}

上面的代码用于对图像进行分类，如果低于 75%，我会生成一个错误，但它总是低于，所以我把它设为 35%。该模型分别由 12、98、100、70、90 组成，epoch 为 68，默认批量大小 (64)，默认学习率为 0.001。我尝试将其从低 epoch 调整为高，学习率为 0.0005，下载时不准确度仍然相同。]]></description>
      <guid>https://stackoverflow.com/questions/79258040/teachable-machine-is-inaccurate-when-model-is-downloaded</guid>
      <pubDate>Fri, 06 Dec 2024 12:46:19 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测项目：在 Localhost 上的 Flask 应用程序中提交预测表单后出现“ValueError”</title>
      <link>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</guid>
      <pubDate>Fri, 06 Dec 2024 12:21:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 PYOD 模块时的网格搜索</title>
      <link>https://stackoverflow.com/questions/79256677/grid-search-when-using-pyod-module</link>
      <description><![CDATA[我正在使用 pyod 模块练习无监督学习，并且想使用 GridSearch 比较不同算法的性能。但是，当我尝试确定最佳分数时，我总是会得到每个模型的 nan 值：
models = {
&#39;ABOD&#39;: ABOD(),
&#39;CBLOF&#39;: CBLOF(),
&#39;FeatureBagging&#39;: FeatureBagging(),
&#39;HBOS&#39;: HBOS(),
&#39;IForest&#39;: IForest(),
&#39;KNN&#39;: KNN(),
&#39;LOF&#39;: LOF()
}

params = {
&#39;ABOD&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3],
&#39;n_neighbors&#39;: [5, 10, 20]},
&#39;CBLOF&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3],
&#39;check_estimator&#39;: [True, False]},
&#39;FeatureBagging&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3],
&#39;check_estimator&#39;: [True, False]},
&#39;HBOS&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3]},
&#39;IForest&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3]},
&#39;KNN&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3]},
&#39;LOF&#39;: {&#39;contamination&#39;: [0.1, 0.2, 0.3],
&#39;n_neighbors&#39;: [5, 10, 20]}
}
来自sklearn.model_selection导入GridSearchCV
来自tqdm导入tqdm

best_estimators = {}
best_scores = {}
for model_name in models.keys():
grid_search = GridSearchCV(models[model_name], 
params[model_name],scoring=&#39;roc_auc&#39;,cv=5)
grid_search.fit(X)
best_estimators[model_name] = grid_search.best_estimator_
best_scores[model_name] = grid_search.best_score_
print(f&quot;{model_name} 的最佳参数：
{grid_search.best_params_}&quot;)
print(f&quot;{model_name} 的最佳分数：{grid_search.best_score_}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79256677/grid-search-when-using-pyod-module</guid>
      <pubDate>Fri, 06 Dec 2024 01:57:53 GMT</pubDate>
    </item>
    <item>
      <title>如何对 ML 模型训练中的二维时间相关网格进行编码</title>
      <link>https://stackoverflow.com/questions/79256436/how-to-encode-2d-temporal-dependent-grid-for-ml-model-training</link>
      <description><![CDATA[我有一个 2D 网格，其中 days 是 X 轴，parameters 是 Y 轴：
例如
days param1 param2 param3
4 1.2 1.5 0.7
23 1.9 1.6 15
52 2.6 2.1 1.0

或
days param1 param2 param3
2 1.2 1.5 0.7
53 1.9 1.6 15
125 2.6 7.1 9.0
156 2.3 5.1 3.0

我有一个模型，我输入这个网格，它会输出一个价格。这个模型很慢，所以我想使用 ML 来预测这个网格的价格。对于我的训练数据，我有 100,000 个这样的网格和一个相关的价格。
天数非常重要，因为它本质上意味着从第 0-4 天开始，第一行参数有效，从第 5-23 天开始，第二行参数有效，等等。
网格的长度可能有很大差异（但参数集相同），所以我很难弄清楚如何为 ML 模型编码/预处理这些数据。我不想设置统一的天数，因为会有大量的信息丢失。
在网上查找后，我发现我可以使用高斯过程，但不太确定如何使用。]]></description>
      <guid>https://stackoverflow.com/questions/79256436/how-to-encode-2d-temporal-dependent-grid-for-ml-model-training</guid>
      <pubDate>Thu, 05 Dec 2024 22:45:52 GMT</pubDate>
    </item>
    <item>
      <title>减少模型训练期间的 CTC 损失</title>
      <link>https://stackoverflow.com/questions/79255724/decreasing-ctc-loss-during-model-training</link>
      <description><![CDATA[我是手写识别模型，在训练过程中，我遇到了高 ctc 损失，我应该怎么做才能减少损失并使我的模型识别任何文本，因为截至目前它无法识别任何东西。
链接到 Google-collab 笔记本：https://colab.research.google.com/drive/16VDTTcCGsQrpURZXCxL5695jZ5Xxze4O?usp=sharing[在此处输入图片描述](https://i.sstatic.net/GswZOriQ.jpg)在此处输入图片描述https://colab.research.google.com/drive/16VDTTcCGsQrpURZXCxL5695jZ5Xxze4O?usp=sharing
def ctc_loss(y_true, y_pred):
&quot;&quot;&quot;
计算真实标签和预测输出之间的 CTC 损失。

参数：
y_true：真实标签 (int32/int64)。
y_pred：模型预测 (logits)。

返回：
整个批次的平均 CTC 损失。
&quot;&quot;&quot;
# 确保 y_true 是整数类型
y_true = tf.cast(y_true, dtype=tf.int32)

input_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1]) # 时间步骤
label_length = tf.reduce_sum(tf.cast(y_true != -1, tf.int32), axis=1) # 非填充标签长度

# 计算 CTC 损失
loss = tf.nn.ctc_loss(
labels=y_true,
logits=y_pred,
label_length=label_length,
logit_length=input_length,
blank_index=-1, # 指定用于填充的空白索引
logits_time_major=False # Logits 是批量主序列
)

return tf.reduce_mean(loss)

# 优化器使用学习率调度程序
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
initial_learning_rate=1e-3,
decay_steps=10000,
decay_rate=0.9
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)

model.compile(optimizer=optimizer, loss=ctc_loss)

# 模型训练
history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))

# 可视化损失
import matplotlib.pyplot as plt

plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.legend()
plt.title(&quot;Training and Validation Loss&quot;)
plt.xlabel(&quot;Epoch&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.show()

我尝试使用学习率调度程序来降低损失率，但没有成功。请更正我的代码并提出改进建议以修复训练过程]]></description>
      <guid>https://stackoverflow.com/questions/79255724/decreasing-ctc-loss-during-model-training</guid>
      <pubDate>Thu, 05 Dec 2024 17:39:56 GMT</pubDate>
    </item>
    <item>
      <title>当我的训练和测试数据大小不同时，如何像使用 sklearn 模型一样使用拟合和预测函数创建神经网络类？</title>
      <link>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</link>
      <description><![CDATA[我正在尝试创建一个可以回答线性回归问题的神经网络模型（我已经使用 sklearn 的 LinearRegression 建立了一个模型，我想比较一下这两个模型）。最终，我想创建一个包含 fit 和 predict 函数的类，就像 sklearn 中的模型一样，这样我就可以创建一个循环来测试我在项目中使用的所有模型。
为此，我遵循了此问题答案中的代码：编写一个具有模型拟合和预测功能的 pytorch 神经网络类。
经过一些修改，我得到了以下结果：
import torch
import torch.nn as nn
import torch.optim as optim

class MyNeuralNet(nn.Module):
def __init__(self):
super().__init__()
self.layer1 = nn.Linear(2, 4, bias=True)
self.layer2 = nn.Linear(4, 1, bias=True)
self.loss = nn.MSELoss()
self.compile_()

def forward(self, x):
x = self.layer1(x)
x = self.layer2(x)
return x.squeeze()

def fit(self, x, y):
x = torch.tensor(x.values, dtype=torch.float32)
y = torch.tensor(y.values, dtype=torch.float32)
loss = []
for epoch in range(100):
## 推理
res = self.forward(x)#self(self,x)
loss_value = self.loss(res,y)

## 反向传播
loss_value.backward() # 计算梯度
self.opt.zero_grad() # 刷新前一个 epoch 的梯度
self.opt.step() # 使用上面的梯度执行迭代

## 日志记录
loss.append(loss_value.item())

def compile_(self):
self.opt = optim.SGD(self.parameters(), lr=0.01)

def predict(self, x_test):
self.eval()
y_test_hat = self(x_test)
return y_test_hat.detach().numpy()
# self.train()

注意，您还需要 numpy，我只是没有在这里，因为此代码已放入单独的 .py 文件中。
导入我的类后，这是我使用模型的方式：
model = MyNeuralNet()
X_train = # pandas 数据框，包含 1168 行和 49 列
y_train = # pandas 数据框，包含 1168 行和 1 列
X_test = # pandas 数据框，包含 292 行和 49 列
model.fit(X_train, y_train)
pred = model.predict(X_test)
print(pred)

我得到的错误是 RuntimeError：mat1 和 mat2 形状无法相乘（1168x49 和 2x4），在 fit 步骤。我理解这与我的网络线性层的参数有关。我认为，如果我将第一个线性层的输入大小更改为 49，将第二个线性层的输出大小更改为 1168，那么它将适用于 fit 步骤（或至少类似的步骤，以匹配训练数据的大小）。但是，我的测试数据的大小不同，我很确定 predict 步骤将不起作用。
是否可以创建一个训练和测试数据大小不同的神经网络类？]]></description>
      <guid>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</guid>
      <pubDate>Tue, 03 Dec 2024 21:41:10 GMT</pubDate>
    </item>
    <item>
      <title>convLSTM 教程的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</link>
      <description><![CDATA[我找到了以下教程“https://medium.com/neuronio-br/uma-introdu%C3%A7%C3%A3o-a-convlstm-c14abf9ea84a”讲授 ConvLSTM 模型。
我创建了一个算法来构建模型输入：
import os
import cv2
import numpy as np
import pandas as pd

# 设置
video_folder = &#39;train&#39; # 包含视频的文件夹的路径
output_folder = &#39;train_npy&#39; # 保存 .npy 文件的文件夹
csv_file = &#39;train.csv&#39; # CSV 文件的路径
frames_per_video = 16 # 每个视频的帧数（时间）
pixels_x, pixels_y = 112, 112 # 帧尺寸

# 使用视频名称和类别加载 CSV
df = pd.read_csv(csv_file) # 列：&#39;video_name&#39;, &#39;tag&#39;

# 根据“tag”列中的唯一值确定的类数
unique_tags = df[&#39;tag&#39;].unique()
num_categories = len(unique_tags) # 根据数据定义类别数量

# 将类别映射到索引的字典
tag_to_index = {tag: idx for idx, tag in enumerate(unique_tags)}

# 如果不存在则创建输出文件夹
os.makedirs(output_folder, exist_ok=True)

# 提取和处理帧的函数
def extract_frames(video_path, num_frames=16, size=(pixels_x, pixels_y)):
cap = cv2.VideoCapture(video_path)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
frame_interval = max(total_frames // num_frames, 1)

frames = []
for i in range(num_frames):
cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)
ret, frame = cap.read()
if not ret:
break
frame = cv2.resize(frame, size)
frames.append(frame)

# 必要时用零帧填充
while len(frames) &lt; num_frames:
frames.append(np.zeros((pixels_x, pixels_y, 3), dtype=np.uint8))

cap.release()
return np.array(frames) # 形状：(frames, pixels_x, pixels_y, 3)

# 循环处理所有视频
for idx, row in df.iterrows():
video_name = row[&#39;video_name&#39;]
class_label = row[&#39;tag&#39;]

# 视频的完整路径
video_path = os.path.join(video_folder, video_name)

# 检查视频是否存在
if not os.path.exists(video_path):
print(f&quot;Video {video_name} not found!&quot;)
continue

# 处理帧并保存为 .npy
scene_data = extract_frames(video_path, frames_per_video)
scene_data = scene_data.transpose(0, 3, 1, 2) # 将顺序改为 (帧、通道、行、列)
np.save(os.path.join(output_folder, f&#39;scene_{idx}.npy&#39;), scene_data)

# 创建并保存类别（独立输出）
category_data = np.zeros((num_categories, 1, frames_per_video, 1))
category_data[tag_to_index[class_label], 0, :, 0] = 1 # 将所有帧的类别标记为 1
np.save(os.path.join(output_folder, f&#39;category_{idx}.npy&#39;), category_data)

print(&quot;处理完成！&quot;)

然而，在训练模型时，我收到以下错误：
ValueError: Arguments `target`和 `output` 必须具有相同的形状。 
收到：target.shape=(None, 1, 16), output.shape=(None, 16, 1)

而形状符合教程：
scene_0.npy.shape = (16, 3, 112, 112)
category_0.npy.shape = (5, 1, 16, 1)

为什么目标和输出的形状不同？是教程错误还是输入格式有错误？]]></description>
      <guid>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</guid>
      <pubDate>Tue, 03 Dec 2024 18:34:23 GMT</pubDate>
    </item>
    <item>
      <title>无法提高 Tiny VGG CNN 模型的准确性 [关闭]</title>
      <link>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</link>
      <description><![CDATA[设置

我试图学习如何在 PyTorch 中编写 CNN 的基础知识。
我几乎是一丝不苟地遵循 05. PyTorch Going Modular 来学习如何以脚本方式建模，写下 CNN Explainer 中描述的 CNN 模型 Tiny VGG。

我正在练习的数据，如 05. PyTorch Going Modular 是 pizza-steak-sushi 数据集；加载和转换方式与 此处 相同。

再次，该模型是这里中的模型。
问题

经过多次尝试设置以下超参数：
NUM_EPOCHS = 20
BATCH_SIZE = 20
HIDDEN_UNITS = 10
LEARNING_RATE = 0.0005

并使用一对损失函数和优化器：
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(tiny_vgg.parameters(), lr=LEARNING_RATE)

我无法提高模型在训练和（大部分）测试数据集中的准确率，目前如下所示：
Epoch：1 | train_loss：1.1018 | train_acc：0.2875 | test_loss：1.0947 | test_acc：0.4500
Epoch：2 | train_loss：1.0974 | train_acc：0.3833 | test_loss：1.0970 | test_acc：0.3125
Epoch：3 | train_loss：1.0960 | train_acc：0.3375 | test_loss：1.0995 | test_acc：0.3125
Epoch：4 | train_loss：1.0895 | train_loss：0.3500 | test_loss：1.1013 | test_acc：0.3125
Epoch：5 | train_loss：1.0626 | train_acc：0.3917 | test_loss：1.0755 | test_acc：0.3250
Epoch：6 | train_loss：1.0566 | train_acc：0.3750 | test_loss：1.0666 | test_acc：0.3375
Epoch：7 | train_loss：1.0105 | train_loss：0.5542 | test_loss：1.0133 | test_acc：0.4208
Epoch：8 | train_loss：0.9488 | train_acc：0.5708 | test_loss：1.0114 | test_acc：0.4792
时期：9 | train_loss：0.8834 | train_acc：0.5625 | test_loss：0.9973 | test_acc：0.4208
时期：10 | train_loss：0.8751 | train_acc：0.5667 | test_loss：1.0350 | test_acc：0.4542
时期：11 | train_loss：0.8098 | train_acc：0.6375 | test_loss：0.9834 | test_acc：0.4458
时期：12 | train_loss：0.8136 | train_acc：0.6333 | test_loss：1.0400 | test_acc：0.3792
时期：13 |训练损失：0.8042 | 训练误差：0.6833 | 测试损失：0.9929 | 测试损失：0.4458
时期：14 | 训练损失：0.7932 | 训练误差：0.6292 | 测试损失：1.0383 | 测试损失：0.4333
时期：15 | 训练损失：0.7796 | 训练损失：0.6458 | 测试损失：1.0025 | 测试损失：0.4833
时期：16 | 训练损失：0.7654 | 训练损失：0.6708 | 测试损失：1.0344 | 测试损失：0.4375
时期：17 | 训练损失：0.7412 | 训练损失：0.6833 | test_loss：1.0358 | test_acc：0.4625
时期：18 | train_loss：0.7168 | train_acc：0.6667 | test_loss：1.0759 | test_acc：0.4125
时期：19 | train_loss：0.7364 | train_acc：0.6500 | test_loss：1.0393 | test_acc：0.4583
时期：20 | train_loss：0.7228 | train_acc：0.7167 | test_loss：1.0826 | test_acc：0.4500


因此，我想知道我是否缺少了架构方面的某些东西、超参数设置，或者其他一些东西，例如此类模型缺乏数据等等。]]></description>
      <guid>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</guid>
      <pubDate>Tue, 03 Dec 2024 11:10:18 GMT</pubDate>
    </item>
    <item>
      <title>如何找到张量的前 $n$ 个最大值的索引？</title>
      <link>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</link>
      <description><![CDATA[我知道 torch.argmax(x, dim = 0) 返回 x 中沿维度 0 的第一个最大值的索引。但是有没有一种有效的方法来返回前 n 个最大值的索引？如果有重复的值，我也想要 n 个索引中的那些值的索引。
举一个具体的例子，比如 x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])。我想要一个函数
generalized_argmax(xI torch.tensor, n: int)

这样
generalized_argmax(x, 4)
在此示例中返回 [0, 2, 4, 5]。]]></description>
      <guid>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</guid>
      <pubDate>Thu, 01 Feb 2024 11:05:23 GMT</pubDate>
    </item>
    <item>
      <title>绘制多类问题的 ROC 曲线</title>
      <link>https://stackoverflow.com/questions/70278059/plotting-the-roc-curve-for-a-multiclass-problem</link>
      <description><![CDATA[我正在尝试将sklearn ROC 扩展至多类的想法应用到我的数据集中。我的每个类的 ROC 曲线看起来都像一条直线，取消了 sklearn 的示例，显示曲线的波动。
我在下面给出一个 MWE 来说明我的意思：
# all imports
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
# dummy dataset
X, y = make_classification(10000, n_classes=5, n_informative=10, weights=[.04, .4, .12, .5, .04])
train, test, ytrain, ytest = train_test_split(X, y, test_size=.3, random_state=42)

# 随机森林模型
model = RandomForestClassifier()
model.fit(train, ytrain)
yhat = model.predict(test)

然后，以下函数绘制 ROC 曲线：
def plot_roc_curve(y_test, y_pred):

n_classes = len(np.unique(y_test))
y_test = label_binarize(y_test, classes=np.arange(n_classes))
y_pred = label_binarize(y_pred, classes=np.arange(n_classes))

# 计算每个类的 ROC 曲线和 ROC 面积
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
roc_auc[i] = auc(fpr[i], tpr[i])

# 计算微平均 ROC 曲线和 ROC 面积
fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_pred.ravel())
roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])

# 首先汇总所有假阳性率
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# 然后在此点处插入所有 ROC 曲线
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

# 最后取平均值并计算 AUC
mean_tpr /= n_classes

fpr[&quot;macro&quot;] = all_fpr
tpr[&quot;macro&quot;] = mean_tpr
roc_auc[&quot;macro&quot;] = auc(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;])

# 绘制所有 ROC 曲线
#plt.figure(figsize=(10,5))
plt.figure(dpi=600)
lw = 2
plt.plot(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;],
label=&quot;micro-average ROC curve (area = {0:0.2f})&quot;.format(roc_auc[&quot;micro&quot;]),
color=&quot;deeppink&quot;, linestyle=&quot;:&quot;, linewidth=4,)

plt.plot(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;],
label=&quot;macro-average ROC curve (area = {0:0.2f})&quot;.format(roc_auc[&quot;macro&quot;]),
color=&quot;navy&quot;, linestyle=&quot;:&quot;, linewidth=4,)

colors = cycle([&quot;aqua&quot;, &quot;darkorange&quot;, &quot;深绿色&quot;, &quot;黄色&quot;, &quot;蓝色&quot;])
for i, color in zip(range(n_classes), colors):
plt.plot(fpr[i], tpr[i], color=color, lw=lw,
label=&quot;类 {0} (area = {1:0.2f}) 的 ROC 曲线&quot;.format(i, roc_auc[i]),)

plt.plot([0, 1], [0, 1], &quot;k--&quot;, lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel(&quot;假阳性率&quot;)
plt.ylabel(&quot;真阳性率&quot;)
plt.title(&quot;接收者操作特性(ROC) 曲线”)
plt.legend()

输出：
plot_roc_curve(ytest, yhat)


一种直线弯曲一次。我希望看到模型在不同阈值下的表现，而不仅仅是一个阈值，类似于 sklearn 的插图，针对下面显示的 3 个类别：
]]></description>
      <guid>https://stackoverflow.com/questions/70278059/plotting-the-roc-curve-for-a-multiclass-problem</guid>
      <pubDate>Wed, 08 Dec 2021 16:02:14 GMT</pubDate>
    </item>
    <item>
      <title>joblib.load __main__ 属性错误</title>
      <link>https://stackoverflow.com/questions/49621169/joblib-load-main-attributeerror</link>
      <description><![CDATA[我开始深入研究使用 Flask 将预测模型部署到 Web 应用程序，但不幸的是，我卡在了起跑线上。 
我做了什么：
我在 model.py 程序中腌制了我的模型：
import numpy as np
from sklearn.externals import joblib

class NeuralNetwork():
&quot;&quot;&quot;
两层（隐藏）神经网络模型。
第一层和第二层包含相同数量的隐藏单元
&quot;&quot;&quot;
def __init__(self, input_dim, units, std=0.0001):
self.params = {}
self.input_dim = input_dim

self.params[&#39;W1&#39;] = np.random.rand(self.input_dim, units)
self.params[&#39;W1&#39;] *= std
self.params[&#39;b1&#39;] = np.zeros((units))

self.params[&#39;W2&#39;] = np.random.rand(units, units)
self.params[&#39;W2&#39;] *= std * 10 # 补偿消失梯度
self.params[&#39;b2&#39;] = np.zeros((units))

self.params[&#39;W3&#39;] = np.random.rand(units, 1)
self.params[&#39;b3&#39;] = np.zeros((1,))

model = NeuralNetwork(input_dim=12, units=64)

##### 就在这里 ##############
joblib.dump(model, &#39;demo_model.pkl&#39;)

然后我按照本教程在与我的 demo_model.pkl 相同的目录中创建了一个 api.py 文件 (https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/):
import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

app = Flask(__name__)

@app.route(&quot;/&quot;)
@app.route(&quot;/index&quot;)
def index():
return flask.render_template(&#39;index.html&#39;)

# 创建预测端点（HTTP POST 请求）
@app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;])
def make_prediction():
if request.method == &#39;POST&#39;:
return render_template(&#39;index.html&#39;, label=&#39;3&#39;)

if __name__ == &#39;__main__&#39;:
# APP 运行时加载模型 ####
model = joblib.load(&#39;demo_model.pkl&#39;)
app.run(host=&#39;0.0.0.0&#39;, port=8000, debug=True)

我还在同一目录中创建了一个 templates/index.html 文件，其中包含以下信息：
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;NN 模型作为 Flask API&lt;/title&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;波士顿房价预测器&lt;/h1&gt;
&lt;form action=&quot;/predict&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;
&lt;input type=&quot;file&quot; name=&quot;image&quot; value=&quot;Upload&quot;&gt;
&lt;input type=&quot;submit&quot; value=&quot;Predict&quot;&gt; {% if label %} {{ label }} {% endif %}
&lt;/form&gt;
&lt;/body&gt;

&lt;/html&gt;

running:
&gt;&gt; python api.py

使用 pickler 时出现错误：
回溯（最近一次调用）：
文件“api.py”，第 22 行，位于 &lt;module&gt;
model = joblib.load(&#39;model.pkl&#39;)
文件“C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py”，第 578 行，在 load 中
obj = _unpickle(fobj, filename, mmap_mode)
文件“C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py”，第 508 行，在 _unpickle 中
obj = unpickler.load()
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1043 行，在 load 中
dispatch[key[0]](self)
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1342 行，在 load_global 中
klass = self.find_class(module, name)
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1396 行，在 find_class 中
return getattr(sys.modules[module], name)
AttributeError: 模块“__main__”没有属性“NeuralNetwork”

为什么程序的主模块与我的 NeuralNetwork 模型有关？我现在很困惑……任何建议都将不胜感激。
更新：
向我的 api.py 程序添加类定义 class NeuralNetwork(object): pass 修复了该错误。 
import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

class NeuralNetwork(object):
pass

app = Flask(__name__)

如果有人愿意向我解释发生了什么，我将不胜感激！ ]]></description>
      <guid>https://stackoverflow.com/questions/49621169/joblib-load-main-attributeerror</guid>
      <pubDate>Tue, 03 Apr 2018 02:02:56 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在大小变化的同时还能进行训练吗？</title>
      <link>https://stackoverflow.com/questions/38426117/can-a-neural-network-be-trained-while-it-changes-in-size</link>
      <description><![CDATA[是否有已知的方法可以在神经网络缩小或增大（节点数、连接数等）时对其进行持续训练和优雅降级？
据我所知，我读过的有关神经网络的所有内容都是从静态角度进行的。您先定义网络，然后对其进行训练。
如果存在具有 N 个节点（神经元等）的某个神经网络 X，是否可以训练网络 (X)，以便在 N 增加或减少时，网络仍然有用且能够执行？]]></description>
      <guid>https://stackoverflow.com/questions/38426117/can-a-neural-network-be-trained-while-it-changes-in-size</guid>
      <pubDate>Sun, 17 Jul 2016 21:30:19 GMT</pubDate>
    </item>
    </channel>
</rss>