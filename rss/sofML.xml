<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 10 Oct 2024 12:33:18 GMT</lastBuildDate>
    <item>
      <title>使用 Swin Transformer V2 Backbone 在 PyTorch 上定制 Faster R-CNN 模型</title>
      <link>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</link>
      <description><![CDATA[对于我的对象检测项目，我一直在使用 fasterrcnn_resnet50_fpn_v2 模型。我的输入图像是高分辨率的（大约 3000 x 4000 像素），我将它们拼接成 1200 x 1600 像素的图块以进行训练和推理。但是，我很难用这个模型有效地检测小物体（小到 10 x 10 像素）。
在寻找替代方案时，我读到了 SwinTransformer V2，我发现它很有前途，尤其是对于高分辨率图像的应用程序。由于我的数据集中的所有图像尺寸也是 1200 x 1600，而且我不想缩小它们的尺寸，所以我想自定义 Faster R-CNN 以使用 Swin V2 主干，并可能添加 FPN 并实现 Cascade R-CNN 头。但是，我面临的挑战是骨干、颈部和 RPN 头部之间的尺寸不匹配。
这是我目前想到的（我决定使用基础模型）；
import torch
from torch import nn
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.swin_transformer import swin_v2_b, Swin_V2_B_Weights
from torchvision.ops import MultiScaleRoIAlign
import torchvision.transforms as transforms
import request
from PIL import Image

NUM_CLASSES = 100 
trainable_layers = 2

class CustomSwin(nn.Module):
def __init__(self, backbone):
super().__init__()
self.backbone = backbone
self.out_channels = 1024

def forward(self, x):
return torch.permute(self.backbone(x), (0, 3, 1, 2))

backbone = swin_v2_b(weights=Swin_V2_B_Weights.DEFAULT)

# 删除分类头 
backbone.norm = nn.Identity()
backbone.permute = nn.Identity()
backbone.avgpool = nn.Identity()
backbone.flatten = nn.Identity()
backbone.head = nn.Identity()

# 冻结所有参数
for param in backbone.parameters():
param.requires_grad = False

# 取消冻结最后的 trainable_layers
for layer in list(backbone.features)[-trainable_layers:]:
for param in layer.parameters():
param.requires_grad = True

custom_backbone = CustomSwin(backbone)

# 为非常小的物体添加较小的尺寸
anchor_generator = AnchorGenerator(
sizes=((8, 16, 32, 64, 128, 256, 512),), aspects_ratios=((0.5, 1.0, 2.0),)
)

roi_pooler = MultiScaleRoIAlign(featmap_names=[&quot;0&quot;], output_size=7, samples_ratio=2)

model = FasterRCNN(
custom_backbone,
num_classes=NUM_CLASSES,
rpn_anchor_generator=anchor_generator,
box_roi_pool=roi_pooler,
min_size=1224, 
max_size=1632,
)

我不确定当前的实现是否是最佳的，或者添加 FPN（特征金字塔网络）或 Cascade R-CNN 等组件是否会增强模型的性能（我有一个相当大的数据集）。有人成功实施了这些修改吗？任何指导都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</guid>
      <pubDate>Thu, 10 Oct 2024 11:07:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PyTorch 调度程序似乎不能正常工作？</title>
      <link>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</link>
      <description><![CDATA[我正在尝试使用一个简单的 PyTorch Scheduler 来训练 mobileNetV3Large。
这是负责训练的代码部分：
bench_val_loss = 1000
bench_acc = 0.0
epochs = 15
optimizer = optim.Adam(embeddingNet.parameters(), lr=1e-3) 
loss_optimizer = torch.optim.Adam(loss_fn.parameters(), lr=1e-3)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, waiting=3, Threshold=0.02)

for epoch in range(1, epochs + 1):

print(f&#39;current lr: {scheduler.get_last_lr()}&#39;)
loss=train(embeddingNet, loss_fn, device, train_dataloader, optimizer, loss_optimizer, epoch)
val_loss，准确率 =test(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)
#val_loss = simpleTest(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)

torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceLAST.pth&#39;)

如果准确率 &gt;= bench_acc:
bench_val_loss = val_loss
torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceBEST.pth&#39;)

scheduler.step(accuracy)

writer.add_scalars(&#39;训练与验证损失&#39;，
{&#39;训练&#39;：损失， &#39;Validation&#39;: val_loss},
global_step=epoch+1)

在这里您可以找到前 7 个训练日志
测试集准确率 (Precision@1) = 0.17834772304046048
当前 lr：[0.001]
Epoch 3：Loss = 39.68284225463867
Epoch 3：valLoss = 39.9765007019043
100%|██████████| 962/962 [01:43&lt;00:00, 9.28it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.92it/s]
计算准确率
测试集准确率 (Precision@1) = 0.31242593533096324
当前 lr: [0.001]
Epoch 4: Loss = 39.4412841796875
Epoch 4: valLoss = 39.67761562450512
100%|██████████| 962/962 [01:45&lt;00:00, 9.11it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.86it/s]
计算准确率
测试集准确率 (Precision@1) = 0.3633824276282377
当前 lr: [0.001]
Epoch 5: Loss = 39.09823989868164
Epoch 5: valLoss = 39.54649614901156
100%|██████████| 962/962 [01:42&lt;00:00, 9.37it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.87it/s]
计算准确率
测试集准确率 (Precision@1) = 0.44244117149145085
当前 lr: [0.001]
Epoch 6: Loss = 38.70449447631836
Epoch 6: valLoss = 39.1865906792718
100%|██████████| 962/962 [01:45&lt;00:00, 9.15it/s]
100%|██████████| 370/370 [00:39&lt;00:00, 9.25it/s]
计算准确率
测试集准确率 (Precision@1) = 0.5167597765363129
当前 lr: [0.0001]

我不明白为什么调度程序决定降低学习率，即使准确率的增长速度比阈值更快。
错误在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</guid>
      <pubDate>Thu, 10 Oct 2024 08:51:32 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Keras 面部识别计算机视觉图像分类模型，准确率高 (95%)/验证率低 (0%)</title>
      <link>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</guid>
      <pubDate>Thu, 10 Oct 2024 08:13:20 GMT</pubDate>
    </item>
    <item>
      <title>微调 Transformer 模型并未提高性能</title>
      <link>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</guid>
      <pubDate>Thu, 10 Oct 2024 04:16:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用深度学习来解决由合成数据组成的拼图游戏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</link>
      <description><![CDATA[我花了一些时间研究 Python 中的拼图生成器，该生成器接收图像、行数 (M) 和列数 (N)，并将原始图像分解为 M*N 个 png 图像块输出到文件夹中。这些图像是正方形，带有不规则形状的制表符和空格，因此每个块只能放在一个位置。
接下来我想做的是创建一个拼图解算器，它可以接收这些 png 图像，提取一些关键特征并确定它们的位置。
这里是图像的示例。如果您好奇它是如何实现的，您还可以查看生成器代码。
到目前为止，这些碎片没有任何旋转，但我希望将来能够处理这个问题。由于碎片是方形的，因此无法仅使用尺寸来确定方向，因此在提取边缘进行比较时，我无法轻松缩小它们的范围。
我曾考虑使用 SIFT 进行特征检测，但图像的可见层没有重叠，因此这种方法失败了。
我是机器学习的初学者，但我想尝试通过这个项目将我的技能付诸实践。我的主要问题是我不知道从哪里开始。我遇到过制作拼图解算器的不同方法，但其中大多数都是使用拼图碎片的照片，而不是合成数据，因此形状不同。我看到的另一种方法是使用深度学习来分析图像片段，但我也不确定从哪里开始实施这种方法。]]></description>
      <guid>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</guid>
      <pubDate>Wed, 09 Oct 2024 22:50:24 GMT</pubDate>
    </item>
    <item>
      <title>当批处理大小不等于 1 时，UNet 执行过程中会出现错误</title>
      <link>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</link>
      <description><![CDATA[我尝试使用 DDIM 反演教程中提供的代码运行稳定扩散模型。但是，当输入的批处理大小设置为大于 1 的值（例如 32）时，我遇到以下错误：
RuntimeError：张量 a (131072) 的大小必须与非单例维度 1 上的张量 b (4096) 的大小匹配。

看来 131072 可能来自 32 x 4096，表明张量维度不匹配。发生错误的具体行是：
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample

这是我的代码中与反演过程相关的部分：
## 反演 (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
def invert_process(self, guide_scale, input, denoise_kwargs):

pred_images = []
pred_latents = []

decrypt_kwargs = {&#39;vae&#39;: self.vae}

# 反转时间步&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
timesteps = reversed(self.scheduler.timesteps)
num_inference_steps = len(self.scheduler.timesteps)

with torch.no_grad():
for i in tqdm(range(0, num_inference_steps)):

t = timesteps[i]
self.cur_t = t.item()

# 对于稳定扩散的文本条件
if &#39;encoder_hidden_​​states&#39; in denoise_kwargs.keys():
bs = denoise_kwargs[&#39;encoder_hidden_​​states&#39;].shape[0]
input = torch.cat([input] * bs)

# 预测噪声残差
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
noise_pred = noisy_residual

#对于稳定扩散的文本条件
if noisy_residual.shape[0] == 2:
# 执行指导
noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
noisy_residual = noise_pred_uncond + guide_scale * (noise_pred_text - noise_pred_uncond)
input, _ = input.chunk(2)

current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

latents = input

# 反转更新步骤（重新安排更新步骤以获得 x(t)（新潜伏）作为 x(t-1)（当前潜伏）的函数
# 向潜伏添加噪声

latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred

input = latents

pred_latents.append(latents)
pred_images.append(decode_latent(latents, **decode_kwargs))

return pred_images, pred_latents


可能导致当批量大小大于 1 时，张量大小不匹配？如何在模型中保持批量大小大于 1 的同时解决此问题？
我尝试将 t 的大小更改为形状为 (批量大小,) 的张量。
此外，我确认当批量大小为 1 时模型可以正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</guid>
      <pubDate>Wed, 09 Oct 2024 16:32:07 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上生成合成数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</link>
      <description><![CDATA[我有一个只有 5 个数据点的小数据集，包括材料成分、机械性能和物理性能。由于数据量太小，无法进行预测，我尝试生成合成数据。我使用过 GAN、VAE、高斯混合模型和 Copula 模型。其中，C-Vine Copula 模型比其他模型的结果更好。但我仍然面临问题：
*使用 C-Vine Copula，分布中存在 40% 的误差，合成数据和真实数据之间的关系中存在 7% 的误​​差。这使得数据质量不足以进行预测。
*当使用这些合成数据预测物理特性时，我得到了很好的验证分数，但在新的、看不见的数据点上得到了非常差的结果——可能是由于过度拟合或数据质量差造成的。
*我还尝试使用原始真实数据（5 个数据点）预测物理特性，但结果并不准确。
我不知道如何提高合成数据的质量，或者是否有更好的方法可以尝试进行预测。对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</guid>
      <pubDate>Wed, 09 Oct 2024 16:26:17 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 优化 AGV 路径规划以提高能源效率。我不明白为什么网络没有学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</guid>
      <pubDate>Wed, 09 Oct 2024 10:02:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Java Android 中实现 HDBSCAN 聚类</title>
      <link>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</link>
      <description><![CDATA[我想将 HDBscan 算法实现到 Java Android 应用程序中。我正在将 C# 移植到 Java。在 C# 中，它们是使用名为 Hdbscansharp 的库完成的。我尝试在 Java 中使用 ELKI，但没有成功。原始 C# 代码是
 double avgTgtSpd = (tgt1.speed + tgt2.speed) / 2;
filteredHits.Add(new FilteredHit(avgTgtDist, avgTgtSpd, currTgtDirection)); 
aggregateSpeed += avgTgtSpd;

// HDBSCAN Clustering
double[][] twoDfilteredHits =filteredHits.Select(hit =&gt; new double[] { hit.pos 
}).ToArray(); // 将过滤后的命中结果放入 hdbscan lib 可以使用的格式中
HdbscanResult hdbscanResult = HdbscanRunner.Run(new HdbscanParameters&lt;double[]&gt;
{
DataSet = twoDfilteredHits.ToArray(),
MinPoints = 3, 
MinClusterSize = 4
DistanceFunction = new HdbscanSharp.Distance.ManhattanDistance()
});

我能够将代码移植到 java 中的“twoDfilter”，但在 HDBSCan 的实现中卡住了。
如何在 java 中获取“hdbscanResult”？]]></description>
      <guid>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</guid>
      <pubDate>Wed, 09 Oct 2024 09:47:31 GMT</pubDate>
    </item>
    <item>
      <title>将 ML 模型从一个 Azure Databricks 工作区复制到另一个 Databricks 工作区</title>
      <link>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</link>
      <description><![CDATA[我运行了以下代码以在基于 Azure Databricks 的 mlflow 中导出 ML 模型，但我似乎收到了此错误

MLflow 主机或令牌配置不正确

我无法找出问题所在。工作区的 URL 和 PAT 令牌都是正确的。
export_import 工具有很多错误。它需要 mlfow 库，但 Databricks ML Runtime 附带的是 mlflow-skinny。
import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# 使用工作区 URL 设置 Databricks MLflow 跟踪 URI
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# 设置两个令牌以实现兼容性
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# 初始化 MLflow 客户端（无需传递跟踪 URI，因为它是全局设置的）
mlflow_client = MlflowClient()

# 使用 MLflow 客户端初始化 ModelExporter
exporter = ModelExporter(mlflow_client)

# 导出模型
exporter.export_model(
model_name=&quot;Signature_Test&quot;,
output_dir=&quot;/tmp/mlflow_export/model&quot;,
stage=None, # 使用&quot;None&quot; 导出所有阶段，或指定&quot;Staging&quot; 或&quot;Production&quot;
export_metadata_tags=True
)
]]></description>
      <guid>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</guid>
      <pubDate>Tue, 08 Oct 2024 08:39:33 GMT</pubDate>
    </item>
    <item>
      <title>执行 3D U-net 时，每次执行都会得到截然不同的指标，有时准确率、召回率、DICE 和 IoU 的指标都会 >99.99%</title>
      <link>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</guid>
      <pubDate>Mon, 07 Oct 2024 15:02:22 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据？
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用规范化
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn StackingClassifier 非常慢且 CPU 使用率不一致</title>
      <link>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</link>
      <description><![CDATA[我最近一直在尝试使用 sklearn 中的 StackingClassifier 和 StackingRegressor，但我注意到它总是很慢，并且 CPU 使用效率低下。假设（仅出于此示例的目的）我想使用 StackingClassifier 堆叠随机森林和 lightgbm，同时使用 lightgbm 作为最终分类器。在这种情况下，我预计运行 StackingClassifier 所需的时间大致等于运行单个随机森林所需的时间 + 运行 2 个单独的 lightgbm 所需的时间 + 一些小的余量（所以基本上是各部分的总和 + 训练 StackingClassifier 本身的时间 + 小的余量），但在实践中似乎需要几倍的时间。示例：
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
import lightgbm as ltb
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

X,y = load_iris(return_X_y=True)
cv = StratifiedKFold(n_splits=10)
lgbm = ltb.LGBMClassifier(n_jobs=4)
rf = RandomForestClassifier()

首先是 LightGBM，按照实际时间计算，在我的计算机上大约需要 140 毫秒：
%%time
scores = cross_val_score(lgbm, X, y,评分=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores)

这只是一个随机森林，对我来说大约需要 220 毫秒：
%%time
scores = cross_val_score(rf, X, y, 评分=&#39;accuracy&#39;, cv=cv, n_jobs=-1, error_score=&#39;raise&#39;)
np.mean(scores)

现在有一个将这两者结合起来的 StackingClassifier。由于它基本上运行了上述两个代码块 + 另一轮 lightgbm，我预计它大约需要 250+120+120=490 毫秒，但实际上需要大约 3000 毫秒，超过 6 倍：
%%time
estimators = [
(&#39;rf&#39;, rf),
(&#39;lgbm,&#39;, lgbm)
]

clf = StackingClassifier(
estimators=estimators, final_estimator=lgbm, passthrough=True)

scores = cross_val_score(clf, X, y,scoring=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores) 

我还注意到（在更大的数据集上运行完全相同的代码时，我需要足够长的时间才能监控我的 CPU 使用率），而 StackingClassifier 的 CPU 使用率则到处都是。
例如，运行单个 lightgbm 的 CPU 使用率：
运行单个 lightgbm 的 CPU 使用率
（基本上始终为 100%，因此 CPU 使用效率很高）
将 lightgbm 作为 stackingclassifier 运行时的 CPU 使用率
（到处都是，通常远没有接近 100%）
我做错了什么导致 StackingClassifier 比各部分的总和慢这么多吗？]]></description>
      <guid>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</guid>
      <pubDate>Sun, 17 Jul 2022 15:44:30 GMT</pubDate>
    </item>
    <item>
      <title>使用哪个 Python 库来对调查数据进行定性分析？[关闭]</title>
      <link>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</link>
      <description><![CDATA[我有一个数据集，其中包含大约 300 人完成的问卷调查的回复。该问卷调查涉及公共交通中的用户体验和行为。我们对 3 家公交公司进行了调查。大多数问题都是“是/否”、“3 家公司中最好的”或“3 家公司中最差的”。
如果可能的话，我想建立一个模型，根据答案推荐三家公司中最好的一家。问题包括“公交车的可用性、公交车的可靠性、用户的偏好和公交车的物理维护”。
我希望模型能够分析数据集并返回最好的公交公司，该公司将很容易获得、干净且维护良好、可靠并且用户会更喜欢它。
此外，诸如“您喜欢哪辆公交车？”之类的问题的答案应该在决策中占有更大的权重。
我对机器学习还很陌生，希望有人能建议从哪种算法开始训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</guid>
      <pubDate>Wed, 01 Apr 2020 09:39:52 GMT</pubDate>
    </item>
    <item>
      <title>Plotly：如何使用热图制作带注释的混淆矩阵？</title>
      <link>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</link>
      <description><![CDATA[我喜欢使用 Plotly 来可视化一切，我试图通过 Plotly 可视化混淆矩阵，这是我的代码：
def plot_confusion_matrix(y_true, y_pred, class_names):
fusion_matrix = metrics.confusion_matrix(y_true, y_pred)
confusion_matrix =fusion_matrix.astype(int)

layout = {
&quot;title&quot;: &quot;混淆矩阵&quot;,
&quot;xaxis&quot;: {&quot;title&quot;: &quot;预测值&quot;},
&quot;yaxis&quot;: {&quot;title&quot;: &quot;实际值&quot;}
}

fig = go.Figure(data=go.Heatmap(z=confusion_matrix,
x=class_names,
y=class_names,
hoverongaps=False),
layout=layout)
fig.show()

结果是

我怎样才能在相应的单元格内显示数字而不是悬停，像这样]]></description>
      <guid>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</guid>
      <pubDate>Thu, 26 Mar 2020 02:18:36 GMT</pubDate>
    </item>
    </channel>
</rss>