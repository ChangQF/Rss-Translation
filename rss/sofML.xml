<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 22 Sep 2024 15:16:10 GMT</lastBuildDate>
    <item>
      <title>为什么即使我每次都初始化模型，训练损失也会不断减少？</title>
      <link>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</link>
      <description><![CDATA[我试图重现 Neuromatch DL 课程中的这段摘录，在这里，我在 MNIST 图像数据集上训练一个多层感知器（在本例中没有隐藏层）。我注意到，每次运行整个代码时，初始损失和最终损失都在减少，在三次运行后从 (2.488, 0.0965) 减少到 (2.38,0.092)。
这是怎么发生的？由于 MLP 一次又一次地被初始化，权重不应该恢复为默认值吗？为什么模型拟合得更好？
我已将代码附在下面：
#加载数据集：
train_set, test_set = load_mnist_data(change_tensors=True) 

# 随机抽取 500 个索引的子集
subset_index = np.random.choice(len(train_set.data), 500)

# 我们将使用这些符号来表示训练数据和标签，以尽可能接近数学表达式。
X, y = train_set.data[subset_index, :], train_set.targets[subset_index]

loss_fn = F.nll_loss

cell_verbose = True # 仅用于切换是否打印损失

# 我相信，这是实际模型训练和优化开始的地方
partial_trained_model = MLP(in_dim=784, out_dim=10, hidden_​​dims=[])

if cell_verbose:
print(&#39;Init loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这与 np.log(10 = # of classes) 匹配

# 使用自适应梯度和动量调用优化器（有关此内容的更多信息，请参见第 7 节）
optimizer = optim.Adam(partial_trained_model.parameters(), lr=7e-4)
for i in range(200):
loss = loss_fn(partial_trained_model(X), y)
optimizer.zero_grad()
loss.backward()
optimizer.step()

if cell_verbose:
print(&#39;End loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这应该小于 1e-2

我添加了一个隐藏层，并且相同的特征仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</guid>
      <pubDate>Sun, 22 Sep 2024 10:27:57 GMT</pubDate>
    </item>
    <item>
      <title>训练过程因“RuntimeError：没有用于已保存叶子的梯度累加器！”而崩溃。</title>
      <link>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</link>
      <description><![CDATA[在训练简单模型时，loss.backward() 中的 RuntimeError: No grad accumulator for a saved leaf! 导致进程崩溃，但我确保所有需要计算梯度的数据都放在 GPU 上。
def train_epoch(args, epoch, model, loss_fn, optim, dataloader, lr_scheduler=None, warmup_scheduler=None):
model.train()
dataloader.sampler.set_epoch(epoch)

mae_m, loss_m = AverageMeter(), AverageMeter()
calc_m, read_m = AverageMeter(), AverageMeter()
timer = Timer()
log_step = len(dataloader) // 11
if args.local_rank == 0:
args.writer.add_scalar(&#39;lr&#39;, optim.param_groups[0][&#39;lr&#39;], epoch)

mae_list, pred_list = [], []

for step, sample in enumerate(dataloader):
data, label = sample[&#39;data&#39;].cuda().requires_grad_(), sample[&#39;label&#39;].cuda()
read_m.add(timer.tiktok())

optim.zero_grad()
# (output, deep_output), attn = model(data)
output = model(data)
output = output.reshape(label.shape)
# loss = loss_fn(output, label) + loss_fn(deep_output, label)
loss = loss_fn(output, label)
loss.backward(retain_graph=True)

上面列出了训练的代码。其中一个流程的错误如下图所示：
错误
我向 GPT 寻求帮助，按照它的建议为 data 添加 .requires_grad_()（我认为这是不必要的）以确保它将计算梯度，并为 loss.backward() 添加 retain_graph=True。但仍然不起作用。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</guid>
      <pubDate>Sun, 22 Sep 2024 08:08:01 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</link>
      <description><![CDATA[它复杂吗？结果会是什么样子？从概念上讲，是否可以使用自动编码器进行特征提取并将其作为二元分类任务中的集成模型的输入？
6.定义自动编码器模型
def build_autoencoder(input_shape):
inputs = Input(shape=input_shape)
# Encoder
x = Conv2D(32, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(inputs)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Flatten()(x)
encoded = Dense(128,activation=&#39;relu&#39;)(x)

# 解码器
x = Dense(32 * 32 * 128,activation=&#39;relu&#39;)(encoded)
x = tf.keras.layers.Reshape((32, 32, 128))(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32,(3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
# 调整此处的上采样以返回原始大小
x = UpSampling2D((2, 2))(x) # 从 (7,7) 更改为 (2,2)
decoded = Conv2D(3, (3, 3),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)(x)

autoencoder = Model(inputs,decoded)
encoder = Model(inputs,coded)

autoencoder.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;)

return autoencoder,encoder

/**ValueError Traceback (most recent call last)
&lt;ipython-input-17-3da45b43efb2&gt; in &lt;cell line: 37&gt;()
35 
36 # 训练自动编码器
---&gt; 37 autoencoder.fit(train_gen, epochs=10, validation_data=valid_gen)

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in binary_crossentropy(target, output, from_logits)
673 for e1, e2 in zip(target.shape, output.shape):
674 如果 e1 不是 None 且 e2 不是 None 且 e1 != e2:
--&gt; 675 raise ValueError(
676 “参数 `target` 和 `output` 必须具有相同的形状。”
677 “收到：”

ValueError：参数 `target` 和 `output` 必须具有相同的形状。收到：target.shape=(None, 224, 224, 3)，output.shape=(None, 256, 256, 3)**/`在此处输入代码`
]]></description>
      <guid>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</guid>
      <pubDate>Sun, 22 Sep 2024 05:54:57 GMT</pubDate>
    </item>
    <item>
      <title>如何实时有效地检测图表和行为模式？[关闭]</title>
      <link>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</link>
      <description><![CDATA[如何使用 Python 或任何其他脚本语言实时有效地检测图表模式？
我想检测类似这种逻辑的各种模式
 1. 多个拒绝区域（多次拒绝区域）
2. 摆动高低区域
3. 拒绝预测（蜡烛的灯芯区域）
4. 流动性狩猎（通过突破高/低来获取流动性）

这种类型的模式以图表形式出现，但方式并不统一，这就是为什么我很难用手动条件进行编码。我该怎么做并创建一个模型来寻找进入机会？是否可以使用 DL/Generative AI 来实现，或者我如何实现这些参数来寻找机会？

这里，S&amp;D 是多重拒绝区；R 表示拒绝预测；两条红色水平线是两个摆动点
注意：由于我的代码很长，我分享了整个文件。我的尝试在这里，5 分钟的数据集在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</guid>
      <pubDate>Sun, 22 Sep 2024 05:33:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CycleGAN 实现用于图像转换的 Web 应用程序？</title>
      <link>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</link>
      <description><![CDATA[我正在尝试创建一个使用 CycleGAN 模型进行图像转换的 Web 应用程序。该应用程序应该有一个用户友好的前端，用户可以在其中上传图像，以及一个后端，使用预先训练的 CycleGAN 模型处理图像，并将转换后的图像返回给用户。
一个有效的 Web 应用程序，用户可以通过前端上传图像。
后端应该接收图像，通过 CycleGAN 模型进行处理，并返回转换后的图像。
前端应该向用户显示转换后的图像。]]></description>
      <guid>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</guid>
      <pubDate>Sun, 22 Sep 2024 04:22:01 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的训练损失总是为零？！我在浪费时间吗？</title>
      <link>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</guid>
      <pubDate>Sun, 22 Sep 2024 00:25:31 GMT</pubDate>
    </item>
    <item>
      <title>进行迁移学习后加载 Keras 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</link>
      <description><![CDATA[我开发了一种手语识别模型，使用 ResNet50 架构作为识别乌尔都语手语的基础模型。模型架构定义如下：
base_model = ResNet50(include_top=False, weights=&#39;imagenet&#39;, input_shape=(256, 256, 3))

model = tf.keras.Sequential([
tf.keras.layers.Input(shape=(256,256,3)),
tf.keras.layers.Lambda(preprocess_input_resnet),
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(128,activation=&#39;relu&#39;),
tf.keras.layers.Dense(37,activation=&#39;softmax&#39;)
])

在对模型进行几个 epoch 的训练后，我通过设置某些层对其进行了微调trainable:
base_learning_rate = 0.0001

base_model.trainable = True
fine_tune_at = 100

# 在微调之前冻结层
for layer in base_model.layers[:fine_tune_at]:
layer.trainable = False

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate / 10),
metrics=[&#39;accuracy&#39;])

history_fine = model.fit(train_dataset, epochs=total_epochs, validation_data=validation_dataset)

问题：
当我尝试使用 load_model(&#39;model.h5&#39;) 加载模型时，我遇到以下问题ValueError：

ValueError：层“dense”需要 1 个输入，但收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_564&gt;, &lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_565&gt;]

该错误表明 Dense 层正在接收两个输入张量，而不是一个。
我已通过运行 model.summary() 检查了模型架构，并确认各层的结构符合预期。但是，我对第二个张量（keras_tensor_565）的来源感到困惑。似乎架构中的某个地方可能存在意外的连接或重复。
我也尝试过以 .h5 和 .keras 格式保存和加载模型。但在加载模型时，两种格式都存在相同的错误。
model1.save(&quot;model.h5&quot;)

model = load_model(&#39;model.h5&#39;, custom_objects={&#39;preprocess_input&#39;: preprocess_input})


为什么 Dense 层会接收两个张量？
如何解决此问题以成功加载我的模型而不会出现错误？
如何确保我的模型架构在保存和加载过程中保持一致？
]]></description>
      <guid>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</guid>
      <pubDate>Sun, 22 Sep 2024 00:24:16 GMT</pubDate>
    </item>
    <item>
      <title>运行随机森林生存模型 (rfsrc) 时 R 会话中止 [关闭]</title>
      <link>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc</link>
      <description><![CDATA[我正在使用 R 中的“randomForestSRC”包进行生存分析项目。不幸的是，每次我尝试运行随机森林生存模型 (rfsrc) 时，我的 R 会话都会崩溃并显示以下消息：
R 遇到致命错误。会话已终止。

这是我到目前为止所做的：
在此处输入图像描述

数据：数据集已清理，没有缺失值，由生存时间 (time_month) 和事件状态 (死亡) 组成。我已经成功地在这个数据集上运行了其他生存模型（例如，bnnsurvival、coxph），没有任何问题。

模型设置：

我为随机森林模型创建了公式，如下所示：



 formula_rfsrc &lt;- as.formula(paste(&quot;Surv(time_month, death) ~&quot;, paste(pred_vars, collapse = &quot; + &quot;)))


然后我尝试拟合模型：

 fit &lt;- rfsrc(formula_rfsrc, data = df_train, ntree = 50)


尝试过修复：

检查公式：我已验证公式正确，对生存对象使用 Surv()。
减少数据大小：我尝试在较小的数据子集上运行模型。
限制树深度：我使用了 nodesize、nodedepth 等参数，并减少了树的数量。
内存管理：我确保在模型拟合之前调用垃圾收集 (gc())，认为问题可能与内存有关。



尽管付出了这些努力，但每当我运行随机森林模型时，R 会话都会崩溃。
其他详细信息：
数据集有大约 30 个预测变量，我根据需要将其转换为因子或数字。
我正在运行它R 4.4.1.
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc</guid>
      <pubDate>Sat, 21 Sep 2024 21:33:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn、XGBoost 和 Prophet 时，保存训练模型的最佳文件格式是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</link>
      <description><![CDATA[我正在使用 Scikit-learn 开展 ML 项目。根据我的研究，人们建议使用 .joblib 保存经过训练的 Scikit-learn 模型。
这就是我将模型保存到 .joblib 的方式&gt;
import os
from joblib import dump

model_path = os.path.join(script_dir, &quot;../models/trained_model.joblib&quot;)
dump(model, model_path)
print(f&quot;Model saved at {model_path}&quot;)

我还想使用 XGBoost 和 Prophet 测试此模型，只是为了尝试不同的库。

什么是实现此目标的最佳文件格式？我在搜索过程中多次看到 ONNX，但它似乎与 Prophet 不兼容。

有没有办法将我的模型同时保存为 joblib 和 onnx，或者我是否需要将 jobllib 转换为 onnx 文件？

]]></description>
      <guid>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</guid>
      <pubDate>Sat, 21 Sep 2024 01:49:12 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>如果训练损失始终低于验证损失，是否表明过度拟合？</title>
      <link>https://stackoverflow.com/questions/77998300/if-the-training-loss-is-always-lower-than-the-validation-loss-does-that-indicat</link>
      <description><![CDATA[
我正在 MNIST 数据上训练 CNN 模型，并设置验证分割 = 0.2，而批处理大小 = 128，epochs = 5。训练模型后，我在训练和验证中获得的准确率超过 99%。当我绘制学习曲线时，我得到的图表的训练和验证损失非常低，但训练损失始终高于验证损失。我想知道这是过度拟合的情况吗？]]></description>
      <guid>https://stackoverflow.com/questions/77998300/if-the-training-loss-is-always-lower-than-the-validation-loss-does-that-indicat</guid>
      <pubDate>Thu, 15 Feb 2024 03:30:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 迭代训练两个模型</title>
      <link>https://stackoverflow.com/questions/73618331/train-two-model-iteratively-with-pytorch</link>
      <description><![CDATA[我希望训练两个级联网络，例如 X-&gt;Z-&gt;Y，Z=net1(X)，Y=net2(Z)。
我希望迭代地优化这两个网络的参数，即对于 net1 的某个参数固定，首先使用 MSE(predY,Y) 损失函数训练 net2 的参数，直到收敛；然后使用收敛后的 MSE 损失函数训练 net1 的迭代，以此类推。
因此，我为每个网络分别定义了两个优化器。我的训练代码如下：
net1 = SimpleLinearF()
opt1 = torch.optim.Adam(net1.parameters(), lr=0.01)
loss_func = nn.MSELoss()

for itera1 in range(num_iters1 + 1):
predZ = net1(X)

net2 = SimpleLinearF()
opt2 = torch.optim.Adam(net2.parameters(), lr=0.01)
for itera2 in range(num_iters2 + 1):
predY = net2(predZ)
loss = loss_func(predY,Y)
if itera2 % (num_iters2 // 2) == 0:
print(&#39;iteration: {:d}, loss: {:.7f}&#39;.format(int(itera2), float(loss)))
loss.backward(retain_graph=True)
opt2.step()
opt2.zero_grad()

loss.backward()
opt1.step()
opt1.zero_grad()

但是，我遇到了以下错误：
RuntimeError：梯度计算所需的变量之一已被 
inplace 操作修改：[torch.FloatTensor [1, 1]]，即 AsStridedBackward0 的输出 0，处于 
版本 502；预期版本为 501。提示：启用异常检测以查找 
无法计算其梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

为什么会出现此错误以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/73618331/train-two-model-iteratively-with-pytorch</guid>
      <pubDate>Tue, 06 Sep 2022 07:36:45 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 检查点未找到</title>
      <link>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</link>
      <description><![CDATA[从昨晚开始就一直出现这样的错误，我训练了5个模型，都没有问题，然后就出现了这样的问题，怎么解决呢？
AssertionError Traceback (most recent call last)
&lt;ipython-input-9-08522bc16525&gt; in &lt;module&gt;()
34 os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
35 trainer = CocoTrainer(cfg)
---&gt; 36 trainer.resume_or_load(resume=False)
37 trainer.train()

2 帧
/usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py in load(self, path, checkpointables)
118 if not os.path.isfile(path):
119 path = self.path_manager.get_local_path(path)
--&gt; 120 断言 os.path.isfile(path)，“未找到检查点 {}！”。格式（路径）
121 
122 checkpoint = self._load_file(path)

AssertionError：未找到检查点 https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl！
]]></description>
      <guid>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</guid>
      <pubDate>Wed, 16 Dec 2020 16:22:20 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 - 提取阈值区域特征以进行物体检测</title>
      <link>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</guid>
      <pubDate>Thu, 18 Jun 2020 03:47:25 GMT</pubDate>
    </item>
    <item>
      <title>随机森林比线性回归差。这是正常的吗？原因是什么？</title>
      <link>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</link>
      <description><![CDATA[我正在尝试使用机器学习来预测数据集。这是一个具有 180 个输入特征和 1 个连续值输出的回归问题。我尝试比较神经网络、随机森林回归和线性回归。
正如我所料，3 隐藏层神经网络的表现优于其他两种方法，均方根误差 (RMSE) 为 0.1。然而，我意外地发现随机森林的表现甚至比线性回归更差（RMSE 0.29 vs. 0.27）。正如我所料，随机森林可以发现特征之间更复杂的依赖关系以减少错误。我尝试调整随机森林的参数（树数、最大特征、max_depth 等）。我也尝试了不同的 K 交叉验证，但性能仍然不如线性回归。
我在网上搜索了一下，一个答案说，如果特征对协变量具有平滑、近乎线性的依赖性，线性回归可能会表现得更好。我没有完全理解这一点，因为如果是这样的话，深度神经网络不应该带来很大的性能提升吗？
我很难给出解释。在什么情况下，随机森林比线性回归更差，但深度神经网络可以表现得更好？]]></description>
      <guid>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</guid>
      <pubDate>Thu, 04 Jan 2018 01:58:35 GMT</pubDate>
    </item>
    </channel>
</rss>