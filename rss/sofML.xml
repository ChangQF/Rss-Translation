<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Apr 2024 00:59:03 GMT</lastBuildDate>
    <item>
      <title>请为我的毕业设计解决机器学习中牙齿分割模型的Valueerror</title>
      <link>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</link>
      <description><![CDATA[这是牙齿分割模型的 GitHub 链接：https://github.com/Arnold0210/TEETH-RECOGNITION-WITH-MACHINE-LEARNING
大家好，我在 GitHub [HTTPs://github.com/] 上获取的模型中的 classification.py 中的代码遇到了一些问题。 com/Arnold0210/牙齿识别与机器学习]。如果有人有兴趣深入研究我的问题，该程序应该执行以下操作：
该程序应为用户提供两种选择：

从图像中读取并提取特征：此选项使用 FeatureExtraction 模块从图像中提取 9 个特征（包括图像名称）。
读取预先存在的数据集：此选项读取包含 labels.csv、features.csv 和图像文件的数据集。然后它会询问用户：

执行程序的次数（假设为 5）。
使用 K 折交叉验证分割数据所需的折叠数（假设为 5 折叠，即 k=5）。
测试数据集的大小（假设为 20%）。



模型然后将这些参数传递给classification模块中的分类函数。这就是问题出现的地方：

代码将整个数据集传递给 onlyfiles，其中包含 973 个条目。
然后，它会从 labels.csv（有 778 个条目）中识别 images_name 和 label_color。这代表训练数据集，因为我们之前指定了 20% 的测试集（778 = 973 的 80%）。
以下 for 循环迭代由 k_folds.split(images_name) 生成的分割。此时，我们仍在处理训练数据集，并且当 k=5 时，应该有：

train_index 中有 662 个索引（用于训练数据）。
test_index 中有 156 个索引（用于在训练集中进行验证）。



这是下一个 for 循环中发生错误的位置：
对于 train_index 中的 i：
    current_filename = onlyfiles[i].split(&#39;.&#39;)[0].strip()
    如果 current_filename 在训练数据集中：
        # ...（其余代码）
    别的：
        print(f“警告：在 images_name 中找不到‘{current_filename}’，因为它的索引是 {i}.train。”)


第一行根据 train_index 中的索引 i 检索文件名 (current_filename)。假设 i 为 324，train_index 包含从 156 到 777 的索引（而 test_index 范围从 0 到 155）。
出现此错误的原因是，有时循环会尝试在 images_name 中查找 current_filename，但该文件并不存在。这是因为 images_name 只有 778 个条目（训练数据），其余 195 个条目（测试数据）不包括在内。因此，current_filename 实际上可能属于测试数据集，从而导致错误“101_0032.JPG 不在列表中”。

我尝试对列表进行排序并删除随机播放（在 k_folds.split 中设置 shuffle=False），但错误仍然存​​在。我非常感谢您为解决此问题提供一些帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</guid>
      <pubDate>Thu, 18 Apr 2024 23:03:02 GMT</pubDate>
    </item>
    <item>
      <title>您可以使用 CreateML 从文本 blob 中提取文本吗？</title>
      <link>https://stackoverflow.com/questions/78350086/can-you-use-createml-to-extract-text-from-a-text-blob</link>
      <description><![CDATA[我一直在使用 CreateML 通过文本分类构建模型。它需要读取文本块，并从该文本块中提取名称。 （该斑点来自 iPhone 的 OCR 结果）文本斑点的大小各不相同，但名称始终出现在文本中。
我遇到的问题是，只有在训练期间显示过名称时，它才会找到该名称。如果是新名称，则不起作用。有没有办法修改 CreateML 以根据 blob 中的其他数据查找新名称？
如果 CreateML 无法做到这一点，是否还有其他工具可以做到这一点？生成的模型需要在 iPhone 上运行。]]></description>
      <guid>https://stackoverflow.com/questions/78350086/can-you-use-createml-to-extract-text-from-a-text-blob</guid>
      <pubDate>Thu, 18 Apr 2024 20:05:18 GMT</pubDate>
    </item>
    <item>
      <title>LMST模型敏感性——初学者抗运气</title>
      <link>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</link>
      <description><![CDATA[我一直在试验一些关于艾伯塔省电力市场的基本数据，并尝试使用 LMST 模型来处理时间序列数据，以预测价格。我确实得到了“可能”结果来自我的模型，它似乎确实会出现一些我们可以预期的波动（仅从我自己的市场经验来看）。
但是，我正在寻找对我遇到的一些陷阱的更好理解。
来自 keras.models 导入 Sequential
来自 keras.layers 导入 LSTM
来自 keras.layers 导入 Dropout
来自 keras.layers 导入 Dense
导入 pandas 作为 pd
来自 sklearn.preprocessing 导入 MinMaxScaler
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.metrics 导入 mean_absolute_error,mean_squared_error
导入 numpy 作为 np
导入 matplotlib.pyplot 作为 plt
导入 seaborn 作为 sns
导入 joblib

# 加载数据
# 加载数据

# 加载数据
csv_file_path = &#39;Frankenstein.csv&#39; # 使用您的实际文件路径更新
df = pd.read_csv(csv_file_path)

# 将“日期/时间”转换为日期时间并提取数据集中存在的组件
if &#39;Date/Time&#39; in df.columns:
df[&#39;Date&#39;] = pd.to_datetime(df[&#39;Date/Time&#39;])
df[&#39;Year&#39;] = df[&#39;Date&#39;].dt.year
df[&#39;Month&#39;] = df[&#39;Date&#39;].dt.month
df[&#39;Day&#39;] = df[&#39;Date&#39;].dt.day
df[&#39;Hour&#39;] = df[&#39;Date&#39;].dt.hour
df.drop([&#39;Date/Time&#39;, &#39;Date&#39;], axis=1, inplace=True)

# 假设“价格”是目标变量
features = df.drop([&#39;Price&#39;], axis=1)
target = df[&#39;Price&#39;]

# 规范化特征和目标
scaler_features = MinMaxScaler()
features_scaled = scaler_features.fit_transform(features)
scaler_target = MinMaxScaler()
target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))

# 创建序列函数
def create_sequences(features, target, time_steps=100):
X, y = [], []
for i in range(len(features) - time_steps):
X.append(features[i:(i + time_steps)])
y.append(target[i + time_steps])
return np.array(X), np.array(y)

# 使用整个数据集创建序列
X, y = create_sequences(features_scaled, target_scaled.flatten())

# 模型配置
input_shape = (X.shape[1], X.shape[2]) # (time_steps, num_features)

# 定义 LSTM 模型
model = Sequential([
LSTM(units=100, return_sequences=True, input_shape=input_shape),
Dropout(0.1),
LSTM(units=100),
Dropout(0.1),
Dense(units=100,activation=&#39;elu&#39;),
Dense(1) # 预测单个值
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

# 在整个数据集上训练模型
history = model.fit(X, y, epochs=150, batch_size=20, validation_split=0.1)

# 绘制训练图 &amp;验证损失值
plt.figure(figsize=(10, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;Train&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation&#39;)
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.xlabel(&#39;Epoch&#39;)
plt.legend(loc=&#39;upper right&#39;)
plt.show()

# 保存 LSTM 模型
model_save_path = &#39;trained_lstm_model.h5&#39;
model.save(model_save_path)
print(f&quot;模型已保存到 {model_save_path}&quot;)
joblib.dump(scaler_features, &#39;scaler_features.pkl&#39;)
joblib.dump(scaler_target, &#39;scaler_target.pkl&#39;)

有谁能给初学者一些建议？主要是想更好地理解我应该如何设置它。我有一个按小时计算的数据集，是过去三年的历史生成和交换。我正在寻找方法让我的模型对供应与价格的变化更具反应性。]]></description>
      <guid>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</guid>
      <pubDate>Thu, 18 Apr 2024 19:18:18 GMT</pubDate>
    </item>
    <item>
      <title>KITTI数据集绘制历史轨迹</title>
      <link>https://stackoverflow.com/questions/78349057/kitti-dataset-plotting-historical-trajectory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78349057/kitti-dataset-plotting-historical-trajectory</guid>
      <pubDate>Thu, 18 Apr 2024 16:37:02 GMT</pubDate>
    </item>
    <item>
      <title>通过 Keras 训练同时检查不同类型的数据</title>
      <link>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</link>
      <description><![CDATA[在回归任务中，我得到以下数据：

具有已知标签的输入向量。 MSE损失应该用在预测和标签之间。
没有标签的输入向量对，已知模型应给出相似的结果。应在两个预测之间使用 MSE 损失。

同时将这两种数据拟合 Keras 模型的正确方法是什么？
理想情况下，我希望火车循环以交错的方式迭代这两种类型 - 一个有监督的（1）批次，然后是一个自我监督的（2）批次，然后再次监督，等等。
如果重要的话，我正在使用 Jax 后端。]]></description>
      <guid>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</guid>
      <pubDate>Thu, 18 Apr 2024 16:05:11 GMT</pubDate>
    </item>
    <item>
      <title>如何解决此 OML4py 代理错误？</title>
      <link>https://stackoverflow.com/questions/78348747/how-can-i-resolve-this-oml4py-proxy-error</link>
      <description><![CDATA[例如，apply 函数适用于 pandas DataFrame，但 OML 数据框不支持 apply 方法。我收到此错误
AttributeError：“DataFrame”对象没有属性“apply”
我该如何解决这个问题？
&#39;&#39;&#39;
%python
导入oml
将 pandas 导入为 pd

inp = [{&#39;VAL_1&#39;:10,&#39;VAL_2&#39;:1},{&#39;VAL_1&#39;:11,&#39;VAL_2&#39;:10},{&#39;VAL_1&#39;:12,&#39;VAL_2&#39;:0}]
df = pd.DataFrame(inp)
oml_df = oml.push(df)

# apply 函数与 pandas DataFrame 一起使用：

％Python

df[&#39;VAL_NEW&#39;] = df.apply(lambda 行，arg: 行[&#39;VAL_1&#39;] + 行[&#39;VAL_2&#39;] +100, axis=1, args=(100,))
打印（df）

# 但OML数据框不支持apply方法：

％Python

oml_df[&#39;VAL_NEW&#39;] = oml_df.apply(lambda 行，arg: 行[&#39;VAL_1&#39;] + 行[&#39;VAL_2&#39;] +100, axis=1, args=(100,))

# AttributeError: &#39;DataFrame&#39; 对象没有属性 &#39;apply&#39;

&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78348747/how-can-i-resolve-this-oml4py-proxy-error</guid>
      <pubDate>Thu, 18 Apr 2024 15:44:01 GMT</pubDate>
    </item>
    <item>
      <title>重新创建研究论文的结果</title>
      <link>https://stackoverflow.com/questions/78348126/recreating-results-from-research-paper</link>
      <description><![CDATA[所以我一直在尝试重新创建这篇特定论文的结果（神经协同过滤）。 
我使用的数据集非常类似于这个.
我知道我应该将数据放入训练集和测试集。
我的问题是我是否应该自己创建 test. Negative 文件，或者是否由代码内部的负采样自动处理它（基本上包含基于缺乏数据的负反馈）。
我非常感谢您的反馈！
这里是这篇论文在 github 上的官方实现。]]></description>
      <guid>https://stackoverflow.com/questions/78348126/recreating-results-from-research-paper</guid>
      <pubDate>Thu, 18 Apr 2024 14:10:01 GMT</pubDate>
    </item>
    <item>
      <title>OML4py 中的两个列分组依据</title>
      <link>https://stackoverflow.com/questions/78347872/tow-column-group-by-in-oml4py</link>
      <description><![CDATA[如何使用 OML4Py oml.group_apply 调用按两列进行分组？
例如在 sql 中我可以执行以下操作：
&#39;&#39;&#39;
从 emp 中选择 mgr、count(mgr)、count(deptno)、deptno
按经理、部门分组
按部门编号排序；
返回
7782 1 1 10
7839 1 1 10
0 1 10
7566 2 2 20
7788 1 1 20
7839 1 1 20
7902 1 1 20
7698 5 5 30
7839 1 1 30
&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78347872/tow-column-group-by-in-oml4py</guid>
      <pubDate>Thu, 18 Apr 2024 13:37:39 GMT</pubDate>
    </item>
    <item>
      <title>用于一维数据中的台阶/扭结识别的卷积神经网络</title>
      <link>https://stackoverflow.com/questions/78347795/convolutional-neural-network-for-step-kink-identification-in-1d-data</link>
      <description><![CDATA[我正在尝试实现一个能够检测一维数据中的步骤/扭结的卷积神经网络。
阶跃被定义为随时间变化然后变为恒定的信号，反之亦然。一个例子是机械测试中机器位移发生变化，然后保持恒定。
为了训练模型，我使用以下简单函数生成了一些合成数据：
将 numpy 导入为 np
从张量流导入keras
随机导入
导入数学

def createStepFunction(步骤: int, dy: float = 1.0, tHold: float = 3600.0, dt: float = 100.0):
    t = [0.0]
    y = [0.0]
    对于 _ 在范围内（0，步数）：
        保持时间 = round(tHold*random.random(),0)
        t1 = t[-1] + round(dt*random.random(),0)
        t2 = t1 + 保持时间
        t3 = t2 + round(dt*random.random(),0)
        t4 = t3 + 保持时间

        y1 = y[-1] + dy*random.random()
        y2 = y1
        y3 = y2 - dy*random.random()
        y4 = y3
        t.extend([t1,t2,t3,t4])
        y.extend([y1,y2,y3,y4])

    tInterp = np.arange(0.0,t[-1]+0.5,1.0)
    yInterp = np.interp(tInterp, t, y)
    flag = np.array([1 if (elem in t and elem != 0.0) else 0 for elem in tInterp])
    # flagBool = [True if elem == 1 else False for elem in flag]
    返回 tInterp、yInterp、标志

t、y、标志 = createStepFunction(200)
t = t/np.max(t)
gaussian_noise = np.random.normal(np.mean(y), 5e-4, len(t))
yn = y*高斯噪声
yn = (yn - np.min(yn))/(np.max(yn) - np.min(yn))

我还在数据中添加了一些高斯噪声。结果如下：
示例数据
我的特征是 one-hot 向量，除了台阶/扭结的位置之外，所有位置都为 0，它们在此处被分配了值 1。
我使用此函数创建批量数据：
def createBatches(windowSize: int, arr):
    位置 = 0
    批次 = []

    而 pos + windowSize &lt;长度（arr）：
        值 = arr[pos:pos+windowSize]
        批次.append(值)
        pos += 窗口大小
    
    返回 np.asarray(批次)

窗口大小 = 200
xBatch = createBatches(windowSize, yn)
yBatch = createBatches(窗口大小, 标志)
n_timesteps = xBatch.shape[1]
n_特征 = 1
n_outputs = yBatch.shape[1]

我的 Keras 模型如下所示：
模型 = keras.models.Sequential()
model.add(keras.layers.Conv1D(filters=24，kernel_size=2，activation=&#39;relu&#39;，input_shape=(n_timesteps，n_features)))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;))
model.add(keras.layers.Conv1D(filters=24, kernel_size=2,activation=&#39;relu&#39;, strides=1))
model.add(keras.layers.MaxPooling1D(pool_size=2))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Flatten())
model.add（keras.layers.Dense（200，激活=&#39;relu&#39;））
model.add(keras.layers.Dense(n_outputs, 激活=&#39;softmax&#39;))
model.compile（loss=&#39;mean_squared_error&#39;，optimizer=keras.optimizers.Adam（learning_rate=0.05），metrics=[&#39;accuracy&#39;]）
模型.summary()


# 拟合网络
历元 = 1000
批量大小 = 200
详细 = 1

历史= model.fit(xBatch, yBatch, epochs=epochs, batch_size=batch_size, verbose=verbose)

该架构类似于我在一本书中找到的示例。
然而，该模型似乎与所提供的数据不相符。损失并没有明显下降，而且准确率很差。我尝试了各种参数但没有成功。
我错过了什么？我生成的训练数据有问题吗？
是否有可能使用这种方法来达到我的目的？]]></description>
      <guid>https://stackoverflow.com/questions/78347795/convolutional-neural-network-for-step-kink-identification-in-1d-data</guid>
      <pubDate>Thu, 18 Apr 2024 13:24:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有类标签的情况下可视化实例预测？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78345121/how-to-visualize-instance-predictions-but-without-class-labels</link>
      <description><![CDATA[导入 matplotlib.pyplot 作为 plt
Predicted_images_path = os.path.abspath(“/content/predicted”)
dataset_dicts_validation = DatasetCatalog.get(&#39;void-detection-2-valid&#39;)
对于 dataset_dicts_validation 中的 d：
    im = cv2.imread(d[“文件名”])
    输出 = 预测器(im)
    v = 展示台(im[:, :, ::-1],
                   元数据=元数据，
                   比例=0.5，
                   instance_mode=ColorMode.IMAGE_BW
    ）
    out = v.draw_instance_predictions(outputs[“instances”].to(“cpu”))
    图 = plt.figure(frameon=False, dpi=1)
    图.set_size_英寸(1024,1024)
    ax = plt.Axes(图, [0., 0., 1., 1.])
    ax.set_axis_off()
    图.add_axes(ax)
    ax.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB),spect=&#39;auto&#39;)
    Fig.savefig(f&quot;{predicted_images_path}/{d[&#39;file_name&#39;].split(&#39;/&#39;)[-1]}&quot;)

分割图像：

我使用 detectorron2 来训练模型并对检测到的空洞进行预测。如何仅标记分段而无需文字。
使用 ROBOFLOW 进行图像注释
二值图像：
]]></description>
      <guid>https://stackoverflow.com/questions/78345121/how-to-visualize-instance-predictions-but-without-class-labels</guid>
      <pubDate>Thu, 18 Apr 2024 05:51:09 GMT</pubDate>
    </item>
    <item>
      <title>滴灌管的自动检测和测量[关闭]</title>
      <link>https://stackoverflow.com/questions/78344991/automating-detection-and-measurement-of-drip-irrigation-pipes</link>
      <description><![CDATA[我正在使用无人机图像来自动从中提取一些数据。我想自动检测和测量田野中布置的灌溉线的长度，如下所示。我可以采取什么方法来实现这一目标？图像处理中是否有合适的基于规则的技术，例如颜色、边缘检测等？或者我应该使用一些对象检测技术？如果使用这些，注释和训练模型的最佳方法是什么？
该图像如下所示，下面显示了放大版本。

在此图像（上图的放大部分）中，您可以看到滴水线（其颜色始终为黑色，但有时在更复杂的背景和环境中，它们也可能并不总是直线，也可能是弯曲的）
]]></description>
      <guid>https://stackoverflow.com/questions/78344991/automating-detection-and-measurement-of-drip-irrigation-pipes</guid>
      <pubDate>Thu, 18 Apr 2024 05:09:44 GMT</pubDate>
    </item>
    <item>
      <title>流式 LightGBM 数据集构建在训练中冻结</title>
      <link>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</link>
      <description><![CDATA[我一直在尝试使用参考数据集（称为 ref_dataset）以流方式在 Python 中构建 LightGBM 数据集。我不确定它是如何完成的，它涉及调用 Dataset 类中看似非公共的方法。
我已经尝试过：
label_column = “标签”
权重列=“权重”
ref_dataset = lightgbm.Dataset(
   Sample_df.drop(列=[标签列，权重列])
   标签=sample_df[标签_列],
   权重=sample_df[权重列],
   参数=配置，
   **（ref_dataset_kwargs 或 {}），
）
ref_dataset.construct()
temp_dataset = lightgbm.Dataset（无，参考= ref_dataset，params = ref_dataset.get_params（））
# train_filenames_and_part_infos 只是一个元组列表[filename,part_info_dict]
估计行数=总和（
    part_info[“num_rows”] for _，train_filenames_and_part_infos 中的part_info
）
temp_dataset._init_from_ref_dataset(estimated_num_rows, ref_dataset._handle)

权重列表 = []
标签列表=[]
# 这个循环实际上不是我的代码，它更复杂，但基本上是它的作用
对于文件名，train_filenames_and_part_infos 中的 _：
    tbl: pyarrow.Table = load_from_file(文件名)
    标签 = tbl[label_column].to_pandas().to_numpy()
    权重 = tbl[weight_column].to_pandas().to_numpy()

    labels_list.append(标签)
    weights_list.append(权重)
    tbl = tbl.drop_columns([label_column, Weight_column])
    np_array: np.ndarray = tbl.to_pandas().to_numpy()
    如果 temp_dataset._start_row + np_array.shape[0] &gt; temp_dataset.num_data():
        raise RuntimeError(“数据集太小，无法容纳数据”)
    temp_dataset._push_rows(np_array)

all_weights = np.concatenate(weights_list)
all_labels = np.concatenate(labels_list)
实际长度 = all_weights.shape[0]
# 不幸的是，由于各种原因，这个估计并不准确
extra_zeros_features = np.zeros(
     （估计行数 - 实际长度，temp_dataset.num_feature()），dtype=np.float32
）
temp_dataset._push_rows(extra_zeros_features)
_LIB.LGBM_DatasetMarkFinished(temp_dataset._handle)
extra_zeros = np.zeros(估计行数 - 实际长度, dtype=np.float32)
temp_dataset.set_weight(np.concatenate([all_weights, extra_zeros]))
temp_dataset.set_label(np.concatenate([all_labels, extra_zeros]))

lightgbm.train(
    params=config, # 包含分布式投票并行训练的网络参数
    train_set=temp_dataset，
    num_boost_round=100,
    valid_sets=valid_sets, # 在其他地方初始化
    valid_names=valid_names, # 在其他地方初始化
    init_model=starting_model, # 不是很有必要
    **lightgbm_train_kwargs, # 空
）

不幸的是，当我运行这段代码时，我得到了这个控制台输出（有些行可能是无序的，因为我实际上是在分布式上运行它，并且日志是聚合的；我已经做了一些简单的编辑删除干扰线）：
[LightGBM] [Info] 总 bin 137618
[LightGBM] [Info] 尝试绑定端口 50627...
[LightGBM] [Info] 绑定端口50627成功
[LightGBM] [信息] 聆听...
[LightGBM] [Info] 训练集中的数据点数量：3934363，使用的特征数量：1382
[LightGBM] [信息] 连接到等级 0
[LightGBM] [信息] 连接到排名 1
[LightGBM] [信息] 连接到等级 2
[LightGBM] [信息] 连接到等级 3
[LightGBM] [信息] 已连接至等级 4
[LightGBM] [信息] 已连接至排名 5
[LightGBM] [信息] 已连接至排名 6
[LightGBM] [信息] 已连接至排名 8
[LightGBM] [Info] 本地排名：7，机器总数：9
[LightGBM] [Info] 自动选择col-wise多线程，测试开销为5.318313秒。
[LightGBM] [Info] 从分数-0.000000开始训练

然后它就坐在那里，CPU 和网络都处于空闲状态。我没有看到它在几个小时内取得任何进展。我已经检查了所有的排名，是不是我做错了什么？我还如何使用给定的样本进行构建？
更多信息：
检查空闲 Python 进程的堆栈跟踪显示代码卡在：
更新（lightgbm/basic.py:3891）
火车（lightgbm/engine.py:276）
...我的代码...

对于我正在使用的 LightGBM 版本 (4.3.0)，这对应于代码：
_safe_call(_LIB.LGBM_BoosterUpdateOneIter(
                self._handle,
                ctypes.byref(is_finished)))

另一个更新：
不同工人的垃圾箱数量似乎不同；有些有 137608、137612、137616。这是一个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</guid>
      <pubDate>Thu, 18 Apr 2024 02:25:39 GMT</pubDate>
    </item>
    <item>
      <title>无论输入值如何，序数编码都会显示相同的值，从而使预测结果相同</title>
      <link>https://stackoverflow.com/questions/78343238/ordinal-encoding-keeps-showing-the-same-value-no-matter-the-input-value-thus-ma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78343238/ordinal-encoding-keeps-showing-the-same-value-no-matter-the-input-value-thus-ma</guid>
      <pubDate>Wed, 17 Apr 2024 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>根据单个树获取 XGBoost 预测</title>
      <link>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</link>
      <description><![CDATA[它可能与如何获取每棵树的重复xgboost 中的预测？ 但解决方案不再有效（可能是 XGBoost 库上的更改）。我的想法是以原始格式 model.get_booster().get_dump() 转储模型并在不同的平台中实现它（仅预测）。不过，我首先尝试用 python 来实现它。运行以下代码，使用所有单独的增强器进行预测并将它们组合起来，不会返回与 model.predict() 函数相同的结果。
有什么方法可以将 model.predict() 与助推器的组合相匹配吗？我错过了什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb
从sklearn导入数据集
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# 加载数据
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# 拟合模型
模型 = xgb.XGBClassifier(
    n_估计器=10，
    最大深度=10，
    use_label_encoder=False,
    目标=&#39;二进制：逻辑&#39;
）
模型.fit(X, y)
booster_ = model.get_booster()

# 提取个人预测
individual_preds = []
对于 booster_ 中的 tree_：
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    ）
individual_preds = np.vstack(individual_preds)

# 将个体预测汇总为最终预测
individual_logits = inverse_sigmoid(individual_preds)
Final_logits = individual_logits.sum(axis=0)
Final_preds = sigmoid(final_logits)

# 验证正确性
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)

&lt;块引用&gt;
断言错误：数组不几乎等于小数点后 7 位
不匹配的元素：150 / 150 (100%)
最大绝对差：0.90511334
最大相对差值：0.99744916
x: 数组([7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05、7.4847587e-05、7.4847587e-05、7.4847587e-05、
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,...
y: 数组([0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,...
]]></description>
      <guid>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</guid>
      <pubDate>Tue, 16 Apr 2024 12:49:27 GMT</pubDate>
    </item>
    <item>
      <title>如何找到 RBF 核 SVM 的准确性？</title>
      <link>https://stackoverflow.com/questions/78325995/how-can-i-find-accuracy-in-rbf-kernal-svm</link>
      <description><![CDATA[我正在尝试使用 SVM 实现人体检测。我正在使用 HOG 特征提取，然后对其应用 SVM。当我应用线性 SVM 时，我会得到图像的分数，但在 RBF kernal SVM 中我只能得到 0 和 1。无论如何我可以获得检测分数吗？我怎样才能像线性SVM一样看到RBF核的系数？
我尝试了Python提供的SVM库。]]></description>
      <guid>https://stackoverflow.com/questions/78325995/how-can-i-find-accuracy-in-rbf-kernal-svm</guid>
      <pubDate>Mon, 15 Apr 2024 03:45:27 GMT</pubDate>
    </item>
    </channel>
</rss>