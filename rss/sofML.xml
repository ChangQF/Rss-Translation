<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 22 Jun 2024 12:25:52 GMT</lastBuildDate>
    <item>
      <title>由于样本数量不一致的变量导致超参数调整评分失败</title>
      <link>https://stackoverflow.com/questions/78655914/scoring-in-hyperparameter-tuning-fails-because-of-variables-with-inconsistent-nu</link>
      <description><![CDATA[我正在使用 sklearn 的 GridSearchCV 进行超参数调整。
 if cfg.tuning is True:
print(&quot;extractingY...\n&quot;)
y = extract_Y(test_dataloader)
print(&quot;finished Extraction\n&quot;)
ht = hyp_tuning(model=model, optimizer=optimizer)
param_grid = {
&#39;lr&#39;: [1e-2, 1.5e-2]
}
search = GridSearchCV(ht, param_grid, cv=5,scoring=&#39;precision&#39;)
#x 是&quot;dummy&quot;矩阵，因为我在拟合和预测中使用了 dataloader
x = [0 for _ in range(len(y))]
print(len(y))
result = search.fit(x, y)
print(&#39;Best: %f using %s&#39; % (result.best_score_, result.best_params_))

但是，评分总是会因为这个错误而失败：
C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_validation.py:982: UserWarning: 评分失败。这些参数在此训练测试分区上的分数将设置为 nan。详细信息：
回溯（最近一次调用）：
文件“C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_validation.py”，第 971 行，位于 _score
scores = scorer(estimator, X_test, y_test, **score_params)
文件“C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_scorer.py”，第 279 行，位于 __call__
return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
文件&quot;C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_scorer.py&quot;，第 376 行，在 _score 中
return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
文件 &quot;C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\utils\_param_validation.py&quot;，第 213 行，在包装器中
return func(*args, **kwargs)
文件 &quot;C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_classification.py&quot;，第 2190 行，在 precision_score 中
p, _, _, _ = precision_recall_fscore_support(
文件“C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\utils\_param_validation.py”，第 186 行，在包装器中
return func(*args, **kwargs)
文件“C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_classification.py”，第 1775 行，在 precision_recall_fscore_support 中
labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
文件“C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_classification.py”，第 1547 行，在_check_set_wise_labels
y_type, y_true, y_pred = _check_targets(y_true, y_pred)
文件 &quot;C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_classification.py&quot;, 第 99 行, _check_targets
check_consistent_length(y_true, y_pred)
文件 &quot;C:\Users\Parlu\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\utils\validation.py&quot;, 第 460 行, check_consistent_length
raise ValueError(
ValueError: 发现输入变量的样本数量不一致: [2403, 12014]

我尝试打印 y 的长度，但终端显示len(y) 是 12014，而不是错误提示的 2403。我真的不知道问题出在哪里。
我还附加了函数 extract_Y
def extract_Y(dataloader):
all_labels = []
for data, lengths, target in tqdm(dataloader):
target = target.view(-1)#.cuda()
mask = (targets != -1)
all_labels.extend(targets[mask].cpu().numpy())
return all_labels
]]></description>
      <guid>https://stackoverflow.com/questions/78655914/scoring-in-hyperparameter-tuning-fails-because-of-variables-with-inconsistent-nu</guid>
      <pubDate>Sat, 22 Jun 2024 11:33:13 GMT</pubDate>
    </item>
    <item>
      <title>SelectKBest 选择的特征与 ColumnTransformer 转换后的特征不匹配</title>
      <link>https://stackoverflow.com/questions/78655668/the-features-selected-by-selectkbest-do-not-match-those-transformed-by-columntra</link>
      <description><![CDATA[我正在部署一个机器学习模型用于研究，对此我有一些疑问：

我的 POST 方法将向 API 发送我的原始特征（未应用转换）

未转换的数据

我使用与训练阶段相同的管道，并从中获取 ColumnTranformed 和最佳模型：

preprocessor = pipeline.named_steps[&quot;columntransformer&quot;]
model = pipeline.named_steps[&quot;xgbclassifier&quot;]

管道

在 API 内部，我获取了 POST 的数据，并希望使用管道中使用的相同预处理器对其进行转换，但是：

-------------------------------------------------------------------------------------------
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-29-f928ce436ece&gt;在 &lt;单元格行：15&gt;()
13 
14 # preprocessor.fit(df[[&quot;tenure&quot;, &quot;OnlineSecurity&quot;, &quot;TechSupport&quot;, &quot;Contract&quot;]])
16 print(preprocessed_df)
17 

17 帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 在 _raise_if_missing(self, key, indexer, axis_name)
5939 
5940 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-&gt; 5941 引发 KeyError(f&quot;{not_found} 不在索引中&quot;)
5942 
5943 @overload

KeyError: &quot;[&#39;MonthlyCharges&#39;, &#39;TotalCharges&#39;] 不在索引中&quot;


验证 KBest 功能，MonthlyCharges 和 TotalCharges 不存在！

kbest = final_estimator2.named_steps[&quot;selectkbest&quot;].get_support(indices=True)

used_df = formed_df_columns.iloc[:, kbest]

kbest 功能
我是不是忘了哪个步骤？
我仔细检查了所有代码和官方文档。
我希望理解为什么我的预处理要求两个“理论上”不需要的功能在训练阶段未被 KBest 使用和选择。]]></description>
      <guid>https://stackoverflow.com/questions/78655668/the-features-selected-by-selectkbest-do-not-match-those-transformed-by-columntra</guid>
      <pubDate>Sat, 22 Jun 2024 09:47:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Coursera 中验证 Jupiter 笔记本分数？</title>
      <link>https://stackoverflow.com/questions/78655480/how-to-validate-jupiter-notebook-score-in-coursera</link>
      <description><![CDATA[提交练习时未正确验证，即使通过了测试用例场景，分数仍为“0”。
Jupiter 笔记本嵌入在 Coursera 应用程序中。
我尝试多次保存文件并重新运行服务器。
我希望文件能够针对给定的解决方案进行验证，从而获得分数。
在此处查看屏幕截图]]></description>
      <guid>https://stackoverflow.com/questions/78655480/how-to-validate-jupiter-notebook-score-in-coursera</guid>
      <pubDate>Sat, 22 Jun 2024 08:21:29 GMT</pubDate>
    </item>
    <item>
      <title>加载 PyTorch 模型检查点后推理结果不一致</title>
      <link>https://stackoverflow.com/questions/78655463/inconsistent-inference-results-after-loading-pytorch-model-checkpoint</link>
      <description><![CDATA[我的 PyTorch 模型遇到了一个问题，在保存和加载模型状态字典后，我得到了不同的推理结果。这是我用来保存模型状态的代码：
def save_net_state(self, base_path: str = &#39;&#39;, epoch: int = None, latest: bool = False, best: bool = False):
os.makedirs(base_path, exist_ok=True)

if latest:
filename = &#39;latest_checkpoint.pkl&#39;
to_save = {
&quot;epoch&quot;: epoch,
&quot;model_weights&quot;: self.model.state_dict(),
&quot;optimizer&quot;: self.optimizer.state_dict()
}
elif best:
filename = &#39;best_model.pkl&#39;
to_save = {
&quot;epoch&quot;: epoch,
&quot;model_weights&quot;: self.model.state_dict()
}

path_to_save = os.path.join(base_path, filename)
torch.save(to_save, path_to_save)

要重新加载模型进行评估：
model_path = &#39;./best_model.pkl&#39;
checkpoint = torch.load(model_path, map_location=&#39;cpu&#39;)
model.load_state_dict(checkpoint[&quot;model_weights&quot;], strict=True)
model.eval()
model.cuda()

状态字典中没有多余的键或缺失的键。但是，我注意到，当我执行推理时，我得到的结果与保存模型之前不同。具体来说，在保存时，我的模型实现了约 79.74% 的准确率，但在加载后，它始终实现约 78.5% 的准确率。模型架构、数据集和保存与加载之间的预处理步骤均未发生任何变化。
我已经尝试比较 state_dicts：
def compare_state_dicts(model, checkpoint):
model_keys = set(model.state_dict().keys())
checkpoint_keys = set(checkpoint[&quot;model_weights&quot;].keys())

missing_keys = model_keys - checkpoint_keys
extra_keys = checkpoint_keys - model_keys

print(f&quot;Missing keys: {missing_keys}&quot;)
print(f&quot;Extra keys: {extra_keys}&quot;)

compare_state_dicts(model, checkpoint)

结果为空集。]]></description>
      <guid>https://stackoverflow.com/questions/78655463/inconsistent-inference-results-after-loading-pytorch-model-checkpoint</guid>
      <pubDate>Sat, 22 Jun 2024 08:15:22 GMT</pubDate>
    </item>
    <item>
      <title>如果我使用 PHP 作为后端语言，如何将 ML 模型连接到 Web 应用程序 [关闭]</title>
      <link>https://stackoverflow.com/questions/78655085/how-can-i-connect-ml-model-to-web-app-if-i-use-php-as-backend-language</link>
      <description><![CDATA[我有一个毕业设计，我面临一些问题，首先我已经完成了我的机器学习模型的构建，该模型分析患者的X射线图像，我想让医生通过我的Web应用程序使用这个模型，但在我的Web应用程序中我使用PHP作为后端语言，我该如何将机器学习模型连接到Web应用程序？请帮忙。
我不知道如何连接它们。]]></description>
      <guid>https://stackoverflow.com/questions/78655085/how-can-i-connect-ml-model-to-web-app-if-i-use-php-as-backend-language</guid>
      <pubDate>Sat, 22 Jun 2024 04:24:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 C/fortran 函数在 pytorch 中创建可微分函数</title>
      <link>https://stackoverflow.com/questions/78654585/create-differentiable-functions-in-pytorch-with-c-fortran-functions</link>
      <description><![CDATA[我正在尝试创建一种策略，使从编译语言（C/fortran）调用的函数相对于输入中的 pytorch 张量可微分。
为了清楚起见，让我们考虑以下示例：
假设我有一个神经网络“NN”，它接受输入 x 并计算输出：
y = NN(x;\theta) \theta：参数
假设我需要获取此函数 y 的输出并在 C/fortran 中执行一些操作（出于速度目的）。因此，我需要将输出张量转换为 numpy 数组，然后将其发送到 C/fortran 中的函数。此时，我的操作的输出不可微分，并且无法计算操作输出相对于输入的梯度。
我被困在这一点上。我看过各种论坛，但没有成功。
如果有人有任何建议，我将不胜感激。
我尝试了 pytorch 的扩展“torch.autograd.Function”，但我不明白如何解决这个问题。前向传递很简单，但我不明白如何实现后向传递。]]></description>
      <guid>https://stackoverflow.com/questions/78654585/create-differentiable-functions-in-pytorch-with-c-fortran-functions</guid>
      <pubDate>Fri, 21 Jun 2024 22:05:42 GMT</pubDate>
    </item>
    <item>
      <title>聚类实际上会减少数据集中的行数吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78652112/does-clustering-actually-reduce-the-number-of-rows-in-a-dataset</link>
      <description><![CDATA[我正在阅读 Luis G. Serrano 的《grokking Machine Learning》一书，看到了以下摘录：

看起来聚类和降维没什么相似之处，但实际上它们并没有太大区别。如果我们有一张满是数据的表，每一行对应一个数据点，每一列对应一个特征。因此，我们可以使用聚类来减少数据集中的行数，使用降维来减少列数。

我对聚类减少行数的说法有疑问。似乎聚类只是对数据进行分组，而不减少其列数。我错了吗？]]></description>
      <guid>https://stackoverflow.com/questions/78652112/does-clustering-actually-reduce-the-number-of-rows-in-a-dataset</guid>
      <pubDate>Fri, 21 Jun 2024 11:30:08 GMT</pubDate>
    </item>
    <item>
      <title>如何知道是否有办法改进这个模型，以及如何知道我是否达到了特定模型的限制[关闭]</title>
      <link>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</link>
      <description><![CDATA[我是数据科学领域的新手，所以当我建立模型时，我不确定我是否已经到达终点，而且我也不知道我还能如何改进模型。这是随机森林分类器，我使用 RandomizeSearchCV 作为参数，最终得到了最高的 73%。此外，我还使用了 class_weight 来平衡类别，我缩放了所有内容并清理了数据，正如您将看到的。我不确定这是否是最好的方法，也不知道在制作模型时 73% 是否足够好，也不知道我是否达到了随机森林分类器的极限。
从 matplotlib 导入 pyplot 作为 plt
从 matplotlib.colors 导入 ListedColormap
导入 pandas 作为 pd
导入 numpy 作为 np
从 sklearn.compose 导入 ColumnTransformer
从 sklearn.model_selection 导入 RandomizedSearchCV、train_test_split
从 sklearn.preprocessing 导入 OneHotEncoder、StandardScaler、LabelEncoder、MinMaxScaler、MaxAbsScaler、RobustScaler
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 classes_report、accuracy_score、confusion_matrix
从 imblearn.over_sampling 导入 SMOTE

pd.set_option(&#39;display.max_rows&#39;, None)
pd.set_option(&#39;display.max_columns&#39;, None)
df = pd.read_csv(&#39;pokemon_data.csv&#39;)

duplicates = df.duplicated()
if duplicates.any():
print(df[duplicates], &quot;DUPLICATE&quot;)
else:
print(&quot;NO DUPLICATES&quot;)

class_dist = df[&#39;type1&#39;].value_counts()
print(class_dist)

df.replace(&#39;—&#39;, np.nan, inplace=True)
numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())
# 现在我们填充分类缺失数据列
categorical_cols = df.select_dtypes(include=&#39;object&#39;).columns
df[categorical_cols] = df[categorical_cols].fillna(&#39;Unknown&#39;)

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), [&#39;dexnum&#39;]),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

features = [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;,
&#39;egg_group2&#39;]

X = df[features] # 我们用来预测的变量 
y_type1 = df[&#39;type1&#39;] # 我们预测的内容
y_type2 = df[&#39;type2&#39;] # 我们预测什么

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), []),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

# Train_test_split 用于将数据集拆分为两个子集，即训练集和测试集
X_train, X_test, y_type1_train, y_type1_test = train_test_split(X, y_type1, test_size=0.4, random_state=42)

X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

param_dist = {
&#39;n_estimators&#39;: [9000],
&#39;max_depth&#39;: [1000],
&#39;min_samples_split&#39;: [2],
&#39;min_samples_leaf&#39;: [1],
&#39;max_features&#39;: [&#39;log2&#39;],
&#39;bootstrap&#39;: [False]
}

rf_type1 = RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=42)
random_search = RandomizedSearchCV(estimator=rf_type1, param_distributions=param_dist, n_iter=50, cv=5, verbose=2, random_state=42, n_jobs=-1)
random_search.fit(X_train, y_type1_train)

# 获取最佳参数
print(&quot;找到最佳参数：&quot;, random_search.best_params_)
print(&quot;最佳准确率： &quot;, random_search.best_score_)

# 使用最佳参数重新训练模型
best_rf = random_search.best_estimator_
y_type1_pred_best = best_rf.predict(X_test)

# 评估模型
print(&quot;具有最佳参数的 Type1 分类报告：&quot;)
print(classification_report(y_type1_test, y_type1_pred_best))
print(&quot;具有最佳参数的 Type1 准确率：&quot;, accuracy_score(y_type1_test, y_type1_pred_best))

]]></description>
      <guid>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</guid>
      <pubDate>Fri, 21 Jun 2024 06:07:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 时如何最大化 GPU 利用率？[关闭]</title>
      <link>https://stackoverflow.com/questions/78650444/how-to-maximize-gpu-utilization-when-using-pytorch</link>
      <description><![CDATA[我使用的是 RTX 4090 和 7950X CPU。我的目标是在表格数据上运行相对简单的模型时最大限度地提高 GPU 利用率。该模型的参数少于 100 万个，数据形状为 (5,000,000, 120)。当我训练模型时，只有 18% 的 GPU 被利用，完成训练大约需要 3 个小时。
主要问题是，如果我能以某种方式利用 90% 的 GPU，训练时间将显著减少，可能减少到当前时间的五分之一，这将为我节省大量时间。
我尝试了各种解决方案，例如调整批处理大小、增加模型的复杂性以及更改 DataLoader 的 num_workers，但这些都没有起到很好的作用。无论我如何调整，GPU 负载仍然在 10-15% 左右。这真是令人沮丧。
由此，我想到了使用多处理的想法。由于我使用的是单个 GPU，并且单个模型仅使用 18%，因此我仍有空间运行另​​外四个模型。我认为同时运行五个不同的模型可以将 GPU 利用率提高到 100% 左右，从而节省大量时间。但是，当我尝试使用 PyTorch 的多处理时，结果并不理想。
有人能帮我解决这个问题吗，或者我的想法在 PyTorch 中不可行？
我尝试过的方法：

将批次大小从 64 增加到 4096、40962、40964
使用 num_workers (2,4,8)
向模型添加更多层
使用 pytorch.multiprocessing
]]></description>
      <guid>https://stackoverflow.com/questions/78650444/how-to-maximize-gpu-utilization-when-using-pytorch</guid>
      <pubDate>Fri, 21 Jun 2024 04:50:59 GMT</pubDate>
    </item>
    <item>
      <title>通过几个步骤优化一个过程：如果我们使用一个模型几次才能够计算出损失，那么如何训练它？</title>
      <link>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</link>
      <description><![CDATA[我有一个包含一定步骤数的过程；假设是 3。

我们从一个初始化的零矩阵 M0 开始，该矩阵描述系统的状态，并且必须采取一项行动，其后果是随机的，但受该行动的强烈影响
我们更新矩阵 (M1)，然后再次采取行动
我们再次更新矩阵 (M2)，采取最后一个行动
只有现在我们才能从最后一个矩阵 M3 计算损失，因此我们可以评估我们的策略

我已经建立了一个神经网络，其中状态矩阵是输入，输出是决定采取哪种行动的权重列表。从我（初学者）的理解来看，整个过程有点像循环神经网络，但多了一些步骤。
我用 Python 实现了这个过程，但当我尝试训练模型时，GradientTape() 看起来不太好，因为我的变量从未计算过梯度；我不确定，但我认为损失的随机性使得梯度不可计算（计算权重的损失远非易事，因为权重放在图的边缘，我们对其执行算法）。
我曾想过在没有权重列表的情况下进行强化学习以采取行动，但我不知道奖励的随机性效果如何。此外，我从未做过这样的事情，我只有一周的时间来实现一切。行动空间也很大，所以这种方法存在很多不确定性。
有没有解决这类问题的常见做法？]]></description>
      <guid>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</guid>
      <pubDate>Fri, 21 Jun 2024 00:18:44 GMT</pubDate>
    </item>
    <item>
      <title>超分辨率 GAN 训练中生成器和鉴别器损失之间的不平衡</title>
      <link>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</guid>
      <pubDate>Thu, 20 Jun 2024 12:57:24 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习从图像中提取系统发育树信息</title>
      <link>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</link>
      <description><![CDATA[有各种机器学习模型（Claude、chatGPT 等）可用于从图像中提取机器可读信息。有没有人见过成功从已发布的系统发育树图像（互联网上有很多）中提取 Newick 格式数据（或等效数据）的案例？]]></description>
      <guid>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</guid>
      <pubDate>Mon, 06 May 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>将自定义预测例程导入顶点 AI 管道</title>
      <link>https://stackoverflow.com/questions/77996526/import-custom-prediction-routine-to-vertex-ai-pipeline</link>
      <description><![CDATA[我在 Vertex AI 上创建了一个自定义预测例程，上传了模型，并且能够通过 UI 使用它生成预测。现在，我想将其合并到 Vertex AI 管道中，以便在数据生成步骤后运行批量预测。我正在使用 Kubeflow Pipelines SDK。
为此，我考虑使用 ModelBatchPredictOp 预构建组件。为了实现这一点，我需要将模型导入管道，例如使用 导入器组件。但是，导入器组件需要工件 URI，而我的模型没有该 URI，因为它使用自定义容器。它被嵌入到容器镜像中；它不在 GCS 中。
因此我尝试编写一个快速自定义导入器组件，该组件返回模型对象，尽管我并不指望它能起作用，但我得到了类型不匹配的结果。请参阅下面的示例代码：
from helper import data_component
from kfp.dsl import component, pipeline, Output, Model
from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp

@component(packages_to_install=[&quot;google-cloud-aiplatform&quot;])
def custom_importer(model: Output[Model]):
from google.cloud import aiplatform
return aiplatform.Model(model_name=&quot;model-id&quot;)

@pipeline(name=&quot;prediction-pipeline&quot;)
def pipeline():
data_task = data_component()

importer_task = custom_importer()

batch_predict_op = ModelBatchPredictOp(
job_display_name=&quot;batch_predict_job&quot;,
model=importer_task.output,
gcs_source_uris=data_task.outputs[&quot;dataset&quot;],
gcs_destination_output_uri_prefix=&quot;bucket&quot;,
instance_format=&quot;csv&quot;,
predictions_format=&quot;jsonl&quot;,
Starting_replica_count=1,
max_replica_count=1,
)


ModelBatchPredictOp 不喜欢参数 model 的输入类型：InconsistentTypeException：传递给组件“model-batch-predict”的输入“model”的参数不兼容：参数类型“system.Model@0.0.1”与输入类型“google.VertexModel@0.0.1”不兼容
如何将自定义预测例程中的批量预测合并到 Vertex AI 中管道？]]></description>
      <guid>https://stackoverflow.com/questions/77996526/import-custom-prediction-routine-to-vertex-ai-pipeline</guid>
      <pubDate>Wed, 14 Feb 2024 18:27:51 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras-Tuner 中使用 F1 分数作为指标时遇到困难</title>
      <link>https://stackoverflow.com/questions/77498936/having-trouble-using-f1-score-as-a-metric-in-keras-tuner</link>
      <description><![CDATA[我想使用 keras-tuner 优化二元图像分类模型的 f1 分数。我知道 keras 已删除默认的 F1 Score 指标，因此我尝试使用 Tensorflow Addons 的 F1Score() 类，但它给出了 KeyError，因为据我所知，keras-tuner 无法将 f1-score 识别为指标。
我尝试使用 Tensorflow Addons 的 F1Score() 类作为指标，但似乎不起作用。
def model_builder(hp):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Rescaling(scale=255))
model.add(tf.keras.layers.TimeDistributed(net))
model.add(tf.keras.layers.Dense(units=hp.Int(
&#39;units&#39;, min_value=32, max_value=512, step=32),activation=&#39;relu&#39;))
model.add(tf.keras.layers.GlobalAveragePooling3D())
model.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;))

custom_optimizer = keras.optimizers.Adam(
learning_rate=hp.Choice(&#39;learning_rate&#39;,values=[1e-2,1e-3,1e-4]),
beta_1=hp.Choice(&#39;beta_1&#39;,values=[0.9,0.99,0.999]),
beta_2=hp.Choice(&#39;beta_2&#39;,values=[0.999,0.9999]),
epsilon=hp.Float(&#39;epsilon&#39;,min_value=1e-10,max_value=1e-7)
)

# 定义指标
#metrics = [tf.keras.metrics.AUC(), tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalseNegatives(), tf.keras.metrics.FalsePositives()]

# 使用 SGD 优化器运行
model.compile(optimizer=&#39;sgd&#39;,
loss=keras.losses.binary_crossentropy, metrics=tfa.metrics.F1Score(num_classes=1, average=&#39;macro&#39;,threshold=0.5))

返回模型

# 初始化调谐器
tuner = RandomSearch(
model_builder,
# 了解“objective”应转换为二进制
objective=Objective(tfa.metrics.F1Score(num_classes=1, average=&#39;macro&#39;,threshold=0.5), direction=max),
max_trials=10, # 根据需要调整试验次数
directory=&#39;test_directory/logs&#39;
)

# 启动调整过程
tuner.search(train_ds, epochs=10, validation_data=(
val_ds), callbacks=combined)

这是我的代码输出的错误：
RuntimeError Traceback (most recent call last)
3 combined = [tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, waiting=5)]
5 # 启动调整过程
----&gt; 6 tuner.search(train_ds, epochs=10, validation_data=(
7 val_ds), callbacks=combined)

文件 ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\src\engine\base_tuner.py:234，位于 BaseTuner.search(self, *fit_args, **fit_kwargs)
232 self.on_trial_begin(trial)
233 self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)
--&gt; 234 self.on_trial_end(trial)
235 self.on_search_end()

文件 ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\src\engine\base_tuner.py:338，位于 BaseTuner.on_trial_end(self, trial)
332 def on_trial_end(self, trial):
333 &quot;&quot;&quot;在试验结束时调用。
334 
335 参数：
336 trial：`Trial` 实例。
337 &quot;&quot;&quot;
--&gt; 338 self.oracle.end_trial(trial)
339 self.save()

文件 ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\src\engine\oracle.py:108，位于 synchronized.&lt;locals&gt;.wrapped_func(*args, **kwargs)
...
文件 &quot;C:\Users\name_here\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\src\engine\objective.py&quot;，第 59 行，位于 get_value
return logs[self.name]
~~~~^^^^^^^^^^^
KeyError: &lt;tensorflow_addons.metrics.f_scores.F1Score 对象位于 0x000001C8709D6710&gt;

我想知道是否有一种解决方法可以从 keras 的 Tuner 类中获取 f1 分数。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77498936/having-trouble-using-f1-score-as-a-metric-in-keras-tuner</guid>
      <pubDate>Fri, 17 Nov 2023 01:42:54 GMT</pubDate>
    </item>
    <item>
      <title>讲座视频数据集及文字记录[关闭]</title>
      <link>https://stackoverflow.com/questions/39943231/dataset-of-lecture-videos-together-with-transcript</link>
      <description><![CDATA[我在哪里可以找到包含讲座视频以及成绩单和笔记的数据集？我有一个机器学习项目需要这些，但我似乎找不到任何包含讲座视频以及成绩单的现有数据集。任何帮助都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/39943231/dataset-of-lecture-videos-together-with-transcript</guid>
      <pubDate>Sun, 09 Oct 2016 11:46:59 GMT</pubDate>
    </item>
    </channel>
</rss>