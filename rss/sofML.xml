<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 11 Dec 2024 12:36:02 GMT</lastBuildDate>
    <item>
      <title>未找到 CUDA nvcc 命令，但 cuda 正在运行</title>
      <link>https://stackoverflow.com/questions/79271710/cuda-nvcc-command-not-found-but-cuda-working</link>
      <description><![CDATA[我已使用以下命令安装了 CUDA：
1- Nvidia GPU
安装 NVIDIA Container Toolkit。
使用 Apt 安装
a) - 配置存储库
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
| sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
| sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; \
| sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update

b) 安装 NVIDIA Container Toolkit 软件包
sudo apt-get install -y nvidia-container-toolkit

2- 配置 Docker 以使用 Nvidia 驱动程序
sudo nvidia-ctk Runtime configure --runtime=docker
sudo systemctl restart docker

我的模型运行正常，但我收到错误 nvcc 命令未找到。
请帮忙。我在 /usr/local 中找不到 cuda 目录
nvidia-smi 命令输出是这样的

请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/79271710/cuda-nvcc-command-not-found-but-cuda-working</guid>
      <pubDate>Wed, 11 Dec 2024 12:25:54 GMT</pubDate>
    </item>
    <item>
      <title>图像分类的核心 ML 预测创建 ml 模型对不同的图像预测相同的结果</title>
      <link>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</link>
      <description><![CDATA[我正在探索 Apple Core ML 框架。

我使用 Create ML 应用创建了一个训练模型。图像分类用于识别图像是猫还是狗。我使用的数据集来自
https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?resource=download

我通过提供训练数据从 create ml 创建了我的 .mlmodel 文件。
现在我在应用程序中进行预测，它给出了相同的目标和概率结果以及不同的图像（猫或狗，给出相同的结果 - [&quot;cats&quot;: 0.6281524444894766, &quot;dogs&quot;: 0.3718475555105234]
应用程序代码：
override func viewDidLoad() {
if let img = UIImage(named: &quot;dog_29.jpg&quot;) {
predictImage(image: img)
} else {
print(&quot;image not caught&quot;)
}
}

private func predictImage(image: UIImage) {

let inputImageSize: CGFloat = 299.0
let minLen = min(image.size.width, image.size.height)
let resizedImage = image.resize(to: CGSize(width: inputImageSize * image.size.width / minLen, height: inputImageSize * image.size.height / minLen))

guard let pixelBuffer = resizedImage.pixelBuffer() else {
fatalError()
}

do {
let config = MLModelConfiguration()
let model = try CatsAndDogs(configuration:config)
let result = try model.prediction(image: pixelBuffer)
print(result.target)
print(result.targetProbability)

print(result)
} catch {
print(&quot;image category error&quot;)
}

}

resize 和 getcvbuffer 的代码：
 func resize(to newSize: CGSize) -&gt; UIImage {
UIGraphicsBeginImageContextWithOptions(CGSize(width: newSize.width, height: newSize.height), true, 1.0)
self.draw(in: CGRect(x: 0, y: 0, width: newSize.width, height: newSize.height))
let resizedImage = UIGraphicsGetImageFromCurrentImageContext()!
UIGraphicsEndImageContext()

return resizedImage
}

func pixelBuffer() -&gt; CVPixelBuffer? {
let width = self.size.width
let height = self.size.height
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary
var pixelBuffer: CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault,
Int(width),
Int(height),
kCVPixelFormatType_32ARGB,
attrs,
&amp;pixelBuffer)

保护让 resultPixelBuffer = pixelBuffer, status == kCVReturnSuccess else {
return nil
}

CVPixelBufferLockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(resultPixelBuffer)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
保护让 context = CGContext(data: pixelData,
width: Int(width),
height: Int(height),
bitsPerComponent: 8,
bytesPerRow: CVPixelBufferGetBytesPerRow(resultPixelBuffer),
space: rgbColorSpace,
bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue) else {
return nil
}

context.translateBy(x: 0, y: height)
context.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context)
self.draw(in: CGRect(x: 0, y: 0, width: width, height: height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))

return resultPixelBuffer
}

我也尝试了网络上可用的各种调整大小和 getcvbuffer 的方法，但结果都一样。
我尝试了来自数据集链接的测试文件夹中的猫和狗的不同图像。结果仍然相同。
任何帮助都可以解释为什么预测不正确。
提前谢谢！！]]></description>
      <guid>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</guid>
      <pubDate>Wed, 11 Dec 2024 11:16:12 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能填补训练数据空白</title>
      <link>https://stackoverflow.com/questions/79271440/using-ai-to-fill-training-data-gaps</link>
      <description><![CDATA[最近，他们在工作中推出了一款机器学习应用，该应用需要对机器零件进行冗长的描述（通常是因为有人可以用 1000 种不同的方式描述特定组件），并将其提炼为核心描述。一切都很好。
我的问题是，是否有可能有一些机器学习代码可以查看迄今为止用于训练的内容，并基于一些人工指导，生成合理的缺失单词排列以填补空白，从而使用新的合成训练数据产生 90-95% 或更高的准确率？然后你​​可以把它放在一个自调节循环中，以保持紧密。
这是一个一般性问题，但我试图从高层次上了解可能需要做的事情。你必须从某个地方开始。
欢迎提出任何想法]]></description>
      <guid>https://stackoverflow.com/questions/79271440/using-ai-to-fill-training-data-gaps</guid>
      <pubDate>Wed, 11 Dec 2024 11:00:28 GMT</pubDate>
    </item>
    <item>
      <title>我实现的 KMeans 的结果并不一致</title>
      <link>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</link>
      <description><![CDATA[我正在尝试实现一个简化版的 KMeans 聚类，但不知何故，结果有时不一致
import numpy as np
import random

import matplotlib.pyplot as plt

class KMeans:
def __init__(
self,
n_clusters,
max_iter
):
self.n_clusters = n_clusters
self.max_iter = max_iter

def _get_distance(self, x, cluster_location):
&quot;&quot;&quot;计算每个点到聚类中心的欧几里得距离
&quot;&quot;&quot;
return np.linalg.norm(x[:,np.newaxis,:]-cluster_location, axis = 2)

def fit(self, x:np.ndarray) -&gt;无：
“”k 均值聚类步骤
1. 启动聚类位置
2. 计算每个点到聚类点的距离
3. 将每个点分配给一个聚类
4. 使用相关点的平均值更新聚类位置
5. 重复 2-4 直到收敛或达到 max_iter

参数：
x (_type_)：_description_

返回：
_type_：_description_
“”
self.x = x
data_dim = x.shape[1]

# 1. 初始化簇位置
cluster_locations = np.random.uniform(x.min(), x.max(), size=(self.n_clusters,data_dim))
# print(&quot;initial:\n&quot;,cluster_locations)

for _ in range(self.max_iter):
# 2. 计算每个点到簇点的距离
distances = self._get_distance(x, cluster_locations)

# 3. 将每个点分配给一个簇
clusters = np.argmin(distances, axis=1)

# 4. 使用关联点的平均值更新簇位置
for cluster in range(self.n_clusters):
# 检查簇中是否有任何点
cluster_mask = clusters == cluster
if np.any(cluster_mask):
cluster_locations[cluster] = np.mean(x[cluster_mask], axis=0)
else:
# 如果簇为空，则用随机点重新初始化
cluster_locations[cluster] = x[np.random.randint(x.shape[0])] 

self.cluster_locations = cluster_locations
self.clusters = clusters
return None

def visualize(self, data, clusters):
_, ax = plt.subplots(1,1,figsize=(5,5))

cluster_color = [(random.random(),random.random(),random.random()) for _ in range(self.n_clusters)]

for cluster in range(self.n_clusters):
to_plot = data[np.where(clusters == cluster)[0]]
ax.scatter(to_plot[:,0], to_plot[:,1], color=cluster_color[cluster])
ax.scatter(self.cluster_locations[:,0], self.cluster_locations[:,1], marker=&quot;x&quot;, color=&#39;r&#39;, s=30)

plt.show()

这是我使用它的方式
from sklearn.datasets import make_blobs

random_state = 42
n_samples = 100

x, _ = make_blobs(n_samples=n_samples, random_state=random_state)
my_kmeans = KMeans(3, 50)
my_kmeans.fit(x)
my_kmeans.visualize(x, my_kmeans.clusters)

大多数时候，它都会给我一个合理的输出，像这样

但每运行几次，它就会给我这样的结果

我很好奇我是否遗漏了什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</guid>
      <pubDate>Wed, 11 Dec 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>激活函数：二元分类中的 tanh(z)</title>
      <link>https://stackoverflow.com/questions/79270936/activation-function-tanhz-in-binary-classification</link>
      <description><![CDATA[我在二元分类中使用 tanh 激活函数，但我不知道如何处理损失函数。这是我的代码
class TanhNeuron():
def __init__(self): # w : 增加，b：部分
self.w=None
self.b=None

def forpass(self,x): 
z=np.sum(x*self.w)+self.b
return z

def backprop(self,x,err): # 梯度
w_grad=x*err
b_grad=1*err
return w_grad,b_grad

defactivation(self,z): # 激活函数：tanh(z)
a=np.tanh(z)
return a

def fit(self,x,y,epochs=1000):
self.w=np.ones(x.shape[1]) 
self.b=0 
for i in range(epochs):
for x_i,y_i in zip(x,y):
z=self.forpass(x_i)
a=self.activation(z)
err=-(y_i/a+(y_i-1)/(1-a))*(1-a**2) #问题出在这里...
w_grad,b_grad=self.backprop(x_i,err)
self.w-=w_grad
self.b-=b_grad

def predict(self,x):
z=[self.forpass(x_i) for x_i in x]
a=self.activation(np.array(z))
return a&gt;0.5

我使用测试用例打印了准确率。这是我的代码。
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer=load_breast_cancer()

x=cancer.data
y=cancer.target
x_train,x_test,y_train,y_test=train_test_split(x,y,stratify=y,test_size=0.2,random_state=42)

tanhneuron=TanhNeuron()
tanhneuron.fit(x_train,y_train)
print(np.mean(tanhneuron.predict(x_test)==y_test))

输出准确率为 0.36。
这意味着没有发生学习。]]></description>
      <guid>https://stackoverflow.com/questions/79270936/activation-function-tanhz-in-binary-classification</guid>
      <pubDate>Wed, 11 Dec 2024 08:08:13 GMT</pubDate>
    </item>
    <item>
      <title>交替最小二乘训练与推理</title>
      <link>https://stackoverflow.com/questions/79270892/alternating-least-squares-training-vs-inference</link>
      <description><![CDATA[我使用隐式 Python 包中的 als.AlternatingLeastSquares。虽然对稀疏 CSR 矩阵进行训练非常高效，大约需要 10-15 分钟（使用默认的潜在参数和迭代次数），但推理需要一小时多一点的时间。这是正常的吗？如何优化推理？通常 ML 算法恰恰相反：训练比推理花费的时间长得多。训练和推理都针对几百万用户。
我尝试减少潜在参数的数量，但模型质量显然变差了。]]></description>
      <guid>https://stackoverflow.com/questions/79270892/alternating-least-squares-training-vs-inference</guid>
      <pubDate>Wed, 11 Dec 2024 07:53:33 GMT</pubDate>
    </item>
    <item>
      <title>如何对具有大量类别的商品特征进行编码以进行推荐</title>
      <link>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</link>
      <description><![CDATA[对于我正在研究的推荐问题，有大约 50000 个独特品牌和 3 级产品类别，level_1_cat（50 个类别）、level_2_cat（100 个类别）和 level_3_cat（1000 个类别）。所有这些项目特征仅由整数表示。到目前为止，我已经为我的 lightfm 模型尝试了二进制编码、标签编码和目标编码。使用二进制编码和标签编码，结果比不使用任何项目特征更差。使用目标编码，结果与不使用任何项目特征相似。我想知道我还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</guid>
      <pubDate>Wed, 11 Dec 2024 06:39:00 GMT</pubDate>
    </item>
    <item>
      <title>无法访问自由变量“fig”，因为它与封闭范围内的值没有关联</title>
      <link>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</guid>
      <pubDate>Wed, 11 Dec 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 PyTorch 模型（例如 GFPGAN）进行跟踪？</title>
      <link>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</link>
      <description><![CDATA[我在设置 GFPGAN PyTorch 模型进行跟踪时遇到了麻烦，因此我将其转换为 CoreML 模型。我的理解是，我还需要定义一个从 Torch.nn.Module 继承的类，该类定义了 GPFGAN 模型的结构，并且 .pth 文件包含权重。但我不知道在哪里可以找到定义从 Torch.nn.Module 继承的类的信息。我是不是漏掉了什么？
我在 GFPGAN python 库中找到了从 Torch.nn.Module 继承的一些类，但它们在尝试将状态字典加载到其中时都会导致错误。
这是我的代码：
import torch
from gfpgan.archs import gfpganv1_clean_arch

model_path = &#39;GFPGANv1.4.pth&#39;
traceable_model = gfpganv1_clean_arch.GFPGANv1Clean(out_size=512)
traceable_model.load_state_dict(torch.load(model_path)) # 此处发生错误。
traceable_model.eval()
trace = torch.jit.trace(traceable_model, input_batch)

我应该如何加载 GFPGANv1.4 模型进行跟踪？]]></description>
      <guid>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</guid>
      <pubDate>Wed, 11 Dec 2024 00:00:30 GMT</pubDate>
    </item>
    <item>
      <title>如何加载使用（version ='latest'）框架训练的 Sagemaker XGBoost 模型？</title>
      <link>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</link>
      <description><![CDATA[管道中有一个使用此容器创建的现有 xgboost 模型
sagemaker.image_uris.retrieve(&#39;xgboost&#39;, sagemaker.Session().boto_region_name, version=&#39;latest&#39;)

输出：
&#39;{accountid}.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest&#39;

我从模型工件中提取了 model.tar.gz 并加载了 xgboost-model 文件
但它给出了这个错误
XGBoostError: basic_string::resize

我运行了一个 shell 脚本，使用所有可用的 XGBoost 版本加载模型，但没有任何效果。
我只想使用 model.get_score 检查特征重要性。]]></description>
      <guid>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</guid>
      <pubDate>Tue, 10 Dec 2024 20:57:42 GMT</pubDate>
    </item>
    <item>
      <title>时间序列运动捕捉数据的 PCA 图聚类问题</title>
      <link>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</link>
      <description><![CDATA[我正在使用 PCA 对手部动作捕捉数据的时间序列数据集进行降维，并遇到了意外的聚类行为。以下是我的过程和我面临的问题的详细信息。

上下文：
我有一个使用智能手套捕捉的手部动作记录数据集，每个手指上有 4 个传感器。每个传感器提供：

位置值：X、Y、Z
旋转值：X、Y、Z、W

数据记录在单独的文件中，每个文件包含 10 次手势重复。这些重复随后被分割成单独的文件（每个文件 1 个重复），贴上标签，并合并成一个大型数据集。
在对数据进行标签编码和缩放后，我使用以下代码应用 PCA 进行降维：
# PCA 用于降维
pca_components = 3
pca = PCA(n_components=pca_components)
x_train_pca = pca.fit_transform(x_train_scaled)

为了可视化结果，我创建了一个 PCA 摘要图，其中每个数据段都表示为一个点。目标是查看每个手势的 10 个点的聚类。以下是总结 PCA 结果的代码：
# 将 PCA 结果转换为 DataFrame 以关联标签
x_train_pca_df = pd.DataFrame(x_train_pca, columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;])
x_train_pca_df[&#39;label&#39;] = y_train_data
x_train_pca_df[&#39;segment&#39;] = train_data[&#39;segment&#39;].values.ravel()

# 计算每个数据集的 PC1 和 PC2 的平均值（将每个数据集总结为一个点）
summary_train_points = x_train_pca_df.groupby([&#39;label&#39;, &#39;segment&#39;]).mean().reset_index()

以下是我用来绘制总结 PCA 的方法数据：
def plot_3d_pca_matplotlib(summary_points, title=&#39;3D PCA Plot&#39;):
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

# 对于每个唯一数据集（标签），分散点并分配图例条目
unique_labels = summary_points[&#39;label&#39;].unique()

for label in unique_labels:
# 过滤当前标签的数据
filtered_data = summary_points[summary_points[&#39;label&#39;] == label]

# 当前标签点的散点图
ax.scatter(filtered_data[&#39;PC1&#39;],
filtered_data[&#39;PC2&#39;],
filtered_data[&#39;PC3&#39;],
label=label)

ax.set_xlabel(&#39;PCA 组件 1&#39;)
ax.set_ylabel(&#39;PCA 组件 2&#39;)
ax.set_zlabel(&#39;PCA 组件 3&#39;)
ax.set_title(title)
ax.legend(title=&quot;Labels&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1.05, 0.7))

plt.subplots_adjust(left=0.05, right=0.75)
plt.show()


问题：
当我为相同手势记录一组新数据并绘制 PCA 结果时，我希望看到每个手势有 20 个点（10 个来自原始数据的点 + 10 个来自新数据的点）的聚类。
相反，PCA 图显示每个手势有两个独立的 10 点簇。这表明，即使手势相同，新数据也未与 PCA 空间中的原始数据对齐。

问题：
什么原因导致相同手势被分离为不同的簇？
可能与以下情​​况有关：

缩放过程？
传感器校准不一致？
在应用 PCA 之前是否需要对齐或对数据进行额外的预处理？
]]></description>
      <guid>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</guid>
      <pubDate>Sun, 08 Dec 2024 18:31:23 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 和 LGBM 模型的大小取决于给定参数集的训练数据大小，而 Catboost 则不然</title>
      <link>https://stackoverflow.com/questions/79261222/xgboost-and-lgbm-models-size-depends-on-training-data-size-for-a-given-set-of-pa</link>
      <description><![CDATA[我正在使用 Python 3.11 在前向交叉验证设置中比较模型。对于给定的一组超参数，xgboost 和 LGBM 模型大小（使用库保存方法进行 pickle 或保存时）随训练大小单调增加，而对于 CatBoost，它保持不变，正如我所料，因为树的数量、max_depth 和所有内容都相同。
您是否知道 xgboost/lgbm 模型是否保存了可以删除以进行推理的任何东西，例如梯度？我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79261222/xgboost-and-lgbm-models-size-depends-on-training-data-size-for-a-given-set-of-pa</guid>
      <pubDate>Sat, 07 Dec 2024 19:09:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 XGBoost 对未来 24 小时进行多时间步长预测</title>
      <link>https://stackoverflow.com/questions/79190223/multi-time-step-forecasting-for-next-24-h-ahead-using-xgboost</link>
      <description><![CDATA[我的项目是使用 XGBoost 对未来 24 小时进行多时间步长预测。
以下两张图是结果：


为什么第一张图中的测试数据与第二张图（橙色）不同，它们应该是相同的？
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# 逆变换预测和测试数据（如果缩放）
y_pred_actual = scaler_y.inverse_transform(y_pred.reshape(-1, 1)) # 确保形状为 
(-1, 
1)
y_test_inv = scaler_y.inverse_transform(y_test)

# 计算指标
mse = mean_squared_error(y_test_inv, y_pred_actual)
mae = mean_absolute_error(y_test_inv, y_pred_actual)
rmse = np.sqrt(mse)
mpe = np.mean((y_test_inv - y_pred_actual) / y_test_inv) * 100

# 计算 MAPE
mape = np.mean(np.abs((y_test_inv - y_pred_actual) / y_test_inv)) * 100

# 打印评估指标
print(&#39;&#39;)
print(&#39;-----------------------------------&#39;)
print(f&#39;TL_model_loaded MAE for test set : {round(mae, 3)}&#39;)
print(f&#39;TL_model_loaded MSE for test set : {round(mse, 3)}&#39;)
print(f&#39;TL_model_loaded RMSE for test set : {round(rmse, 3)}&#39;)
print(f&#39;TL_model_loaded MPE for test set : {round(mpe, 3)} %&#39;)
print(f&#39;TL_model_loaded MAPE for test set : {round(mape, 3)} %&#39;)
print(&#39;-----------------------------------&#39;)
print(&#39;&#39;)

# 调用plot_results_xgboost 函数
plot_results_xgboost(y_pred_actual, y_test_inv, evals_result, &#39;XG&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79190223/multi-time-step-forecasting-for-next-24-h-ahead-using-xgboost</guid>
      <pubDate>Thu, 14 Nov 2024 19:49:04 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 提前停止轮次</title>
      <link>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</link>
      <description><![CDATA[下面的代码一直在崩溃，我不知道发生了什么
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设 `X` 和 `y` 是你的特征矩阵和目标数组
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# 为 Optuna 定义目标函数
def objective(trial):
# 为超参数建议值
params = {
&quot;objective&quot;: &quot;reg:squarederror&quot;,
&quot;eval_metric&quot;: &quot;rmse&quot;,
&quot;tree_method&quot;: &quot;hist&quot;, # 使用 hist 方法
&quot;device&quot;: &quot;cuda&quot;, # 指定使用 GPU
&quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
&quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
&quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
&quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
&quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
&quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
&quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
&quot;n_estimators&quot;: 1000 # 在模型初始化中定义 n_estimators
}

# 初始化模型
model = xgb.XGBRegressor(**params)

# 使用早期停止回调训练模型
model.fit(
X_train,
y_train,
eval_set=[(X_valid, y_valid)],
verbose=False,
early_stopping_rounds=50 # 如果之后没有改进则停止50 轮
)

# 预测并计算验证集的 RMSE
preds = model.predict(X_valid)
rmse = mean_squared_error(y_valid, preds, squared=False)

return rmse # Optuna 将其最小化

# 设置 Optuna 研究
study = optuna.create_study(direction=&quot;minimize&quot;)

# 优化超参数
study.optimize(objective, n_trials=100, n_jobs=40) # 100 次试验，40 次并行作业

# 显示最佳试验
print(&quot;最佳试验：&quot;)
trial = study.best_trial
print(f&quot;值 (RMSE)：{trial.value}&quot;)
print(&quot; 参数：&quot;)
for key, value in trial.params.items():
print(f&quot; {key}: {value}&quot;)

我得到
TypeError：XGBModel.fit() 获得意外的关键字参数“early_stopping_rounds”

我已更新所有内容以确保我拥有所有更新的库。
提前停止轮次是正确的（我认为），但由于某种原因，它会爆炸。]]></description>
      <guid>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</guid>
      <pubDate>Thu, 14 Nov 2024 16:14:40 GMT</pubDate>
    </item>
    <item>
      <title>了解 XGboost 中的 nrounds</title>
      <link>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</link>
      <description><![CDATA[我不明白参数 nrounds。我正在使用 R 并使用包 xgboost 创建一个“极端”梯度提升回归模型。
有一个名为 nrounds 的参数，我的理解是它设置了梯度提升模型（也称为提升迭代）中的树的数量。但是当我设置 nrounds = 0 时，我并没有发现所有预测都等于 base_score（梯度提升模型中的初始猜测）。
为什么会这样？
注意：
我并不是想要一个有 0 棵树的 xgboost 模型，但我尝试用它来检查我对 nrounds 的理解。
我有一个 data.frame，我将其转换为 xgb.DMatrix，然后训练模型：
params &lt;- list(objective = &quot;reg:squarederror&quot;, eval_metric = &quot;rmse&quot;, base_score = 0.5)

bst_model &lt;- xgb.train(params = params, data = dtrain, nrounds = 0, verbose = 1)

使用预测我得到：
pred &lt;- predict(bst_model, newdata = dtrain)
print(pred[1:10])

[1] 1052663 874001 1498940 1991579 2316396 1086949 874001 1432935 1086949 2351261

我预期的位置：
[1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
]]></description>
      <guid>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</guid>
      <pubDate>Mon, 02 Sep 2024 09:36:49 GMT</pubDate>
    </item>
    </channel>
</rss>