<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 20 Jan 2025 06:24:25 GMT</lastBuildDate>
    <item>
      <title>从嵌入式软件工程到 ML/AI 的转变是否太大？[关闭]</title>
      <link>https://stackoverflow.com/questions/79369948/is-the-transition-from-embedded-software-engineering-into-ml-ai-too-big</link>
      <description><![CDATA[我的背景主要是低级嵌入式编程，目前我最了解的语言是 C++，然后是 C 和 Python。
最近我进入了大学的博士课程，并考虑攻读生物医学的 AI/ML，我知道 Python 主要用于工业中的 AI，而且我可能还需要学习一些神经网络概念和 Python 库（TensorFlow、Pythorch 等）
所以问题是 - 你们认为这是一个太大的飞跃吗？我不知道走这条路是否会花费太多时间，因为乍一看这些领域似乎差别很大？
过去几年，我一直患有非常严重的干眼症 (MGD)，嵌入式软件要求我至少在办公室兼职工作，这让我的眼睛灼伤到无法忍受的程度，这就是为什么我认为我宁愿选择一个允许我更远程工作的领域，在对我的选择做了一些研究后，我想 AI/ML 看起来很有趣，但就像我提到的那样，一开始它确实看起来像是一个巨大的飞跃，你不觉得吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79369948/is-the-transition-from-embedded-software-engineering-into-ml-ai-too-big</guid>
      <pubDate>Sun, 19 Jan 2025 23:26:07 GMT</pubDate>
    </item>
    <item>
      <title>所以我根据天气进行销售预测，我已经预处理了我的数据，但我的评估指标仍然不达标？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79369584/so-i-am-doing-sales-prediction-on-the-basis-of-weather-i-have-preprocessed-my-da</link>
      <description><![CDATA[我使用过线性回归、梯度提升和随机森林，这些是线性的评估指标 -&gt;
RMSE：2554.683896947907，
MAE：2554.5158902910825，
R 平方：-70047877.42724504，
梯度提升 -&gt;RMSE：2563.634331688429，
MAE：2563.5459430743717，
R 平方：-70539568.20192483，
随机森林 -&gt;RMSE：2595.942451884807，
MAE： 2584.329211451298，
R 平方：-72328716.81030051

我哪里做错了或者数据有问题，请链接到代码 https://github.com/nabihafaisal/Dynamic-Weather-Impact-on-Retail-Sales-Machine-Learning.git]]></description>
      <guid>https://stackoverflow.com/questions/79369584/so-i-am-doing-sales-prediction-on-the-basis-of-weather-i-have-preprocessed-my-da</guid>
      <pubDate>Sun, 19 Jan 2025 19:04:52 GMT</pubDate>
    </item>
    <item>
      <title>有关于训练集和测试集的问题</title>
      <link>https://stackoverflow.com/questions/79369405/have-a-problem-about-training-set-and-testing-set</link>
      <description><![CDATA[当我使用交叉验证并打印结果时。测试集的准确率高于测试集。
我希望每个人都能解释一下这件事。
训练：随机森林分类器准确率为 0.9437
测试：随机森林分类器准确率为 0.9527]]></description>
      <guid>https://stackoverflow.com/questions/79369405/have-a-problem-about-training-set-and-testing-set</guid>
      <pubDate>Sun, 19 Jan 2025 17:17:05 GMT</pubDate>
    </item>
    <item>
      <title>我无法通过 HuggingFace 教程训练“翻译”模型</title>
      <link>https://stackoverflow.com/questions/79367581/i-cant-train-a-model-from-a-huggingface-tutorial-for-translation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79367581/i-cant-train-a-model-from-a-huggingface-tutorial-for-translation</guid>
      <pubDate>Sat, 18 Jan 2025 16:46:20 GMT</pubDate>
    </item>
    <item>
      <title>如何将样本权重传递给 Keras 多输出模型</title>
      <link>https://stackoverflow.com/questions/79367409/how-to-pass-sample-weights-to-keras-multi-output-model</link>
      <description><![CDATA[我有混合类型多输出（一个回归和一个分类）Keras 模型。我正在尝试为两个输出传递相同的样本权重，如下所示。
import numpy as np
import tensorflow as tf
from tensorflow import keras

# 生成一些样本数据
np.random.seed(42)
X = np.random.rand(1000, 10) # 1000 个样本，10 个特征
y_regression = X.sum(axis=1) + np.random.normal(0, 0.1, 1000) # 回归目标
y_classification = (X.sum(axis=1) &gt; 5).astype(int) # 分类目标（二进制）

# 创建样本权重
sample_weights = np.random.rand(1000)

# 定义具有混合输出的模型
def create_model():
input_layer = keras.layers.Input(shape=(10,))
dense1 = keras.layers.Dense(64,activation=&#39;relu&#39;)(input_layer)
dense2 = keras.layers.Dense(32,activation=&#39;relu&#39;)(dense1)

# 回归输出
regression_output = keras.layers.Dense(1,name=&#39;regression_output&#39;)(dense2)

# 分类输出
classification_output = keras.layers.Dense(1,activation=&#39;sigmoid&#39;,name=&#39;classification_output&#39;)(dense2)

model = keras.Model(inputs=input_layer,outputs=[regression_output,classification_output])

返回模型

model = create_model()

# 使用适当的损失和指标为每个输出编译模型
model.compile(
optimizer=&#39;adam&#39;,
loss={&#39;regression_output&#39;: &#39;mse&#39;, &#39;classification_output&#39;: &#39;binary_crossentropy&#39;},
metrics={&#39;regression_output&#39;: &#39;mae&#39;, &#39;classification_output&#39;: &#39;accuracy&#39;},
)

# 使用样本权重训练模型
history = model.fit(
X,
{&#39;regression_output&#39;: y_regression, &#39;classification_output&#39;: y_classification},
epochs=10,
batch_size=32,
sample_weight=sample_weights,
)

还尝试为每个输出指定权重
history = model.fit(
X,
{&#39;regression_output&#39;: y_regression, &#39;classification_output&#39;: y_classification},
epochs=10,
batch_size=32,
sample_weight={&#39;regression_output&#39;: sample_weights, &#39;classification_output&#39;: sample_weights}, 
)

然而在这两种情况下我都得到了下面的错误，这通常表示 sample_weight 数组的形状和输入数据的形状不匹配，尽管这里不是这种情况。在多输出模型中传递 sample_weight 的正确方法是什么？
KeyError Traceback (most recent call last)
Cell In[18], line 58
49 model.compile(
50 optimizer=&#39;adam&#39;,
51 loss={&#39;regression_output&#39;: &#39;mse&#39;, &#39;classification_output&#39;: &#39;binary_crossentropy&#39;},
52 metrics={&#39;regression_output&#39;: &#39;mae&#39;, &#39;classification_output&#39;: &#39;accuracy&#39;},
53 )
57 # 使用样本权重训练模型
---&gt; 58 history = model.fit(
59 X,
60 {&#39;regression_output&#39;: y_regression, &#39;classification_output&#39;: y_classification},
61 epochs=20,
62 batch_size=32,
63 sample_weight=sample_weights,
64 )

文件 /opt/jupyter/notebooks-generic/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 /opt/jupyter/notebooks-generic/.venv/lib/python3.9/site-packages/keras/src/trainers/compile_utils.py:785，位于 CompileLoss.call.&lt;locals&gt;.resolve_path(path, object)
783 def resolve_path(path, object):
784 for _path in path:
--&gt; 785 object = object[_path]
786 return object

KeyError: 0
]]></description>
      <guid>https://stackoverflow.com/questions/79367409/how-to-pass-sample-weights-to-keras-multi-output-model</guid>
      <pubDate>Sat, 18 Jan 2025 15:24:11 GMT</pubDate>
    </item>
    <item>
      <title>在搜索特定对象时，对为 YOLOv5 训练提供哪些图像存在误解 [关闭]</title>
      <link>https://stackoverflow.com/questions/79367331/misunderstanding-of-what-images-to-provide-for-yolov5-training-when-searching-fo</link>
      <description><![CDATA[我有很多要搜索的螺丝示例图像，我有很多问题，而教程似乎没有回答这些问题，他们只是使用现成的数据集完成整个过程，并没有解释任何细微差别。
这些是我的螺丝示例图像，大约有 50 张。如果有必要，我可能会生成更多，但现在让我们保持这种状态。
     
我想在 3840x1960 图像中检测这些对象。显然，我给每幅图像都添加了一个文本文件“0 0.5 0.5 1 1”，因为每幅正片都是一颗螺丝。
但我误解了我还应该提供什么？有些指南说是负片？这是什么意思？我没在找别的。要添加什么？随机的东西？一张纸，上面写着这些螺丝的位置？如果我提供什么都没有的图像，它们应该是什么尺寸？我会查看 3840x1960 图像，但 yolo 无论如何都会调整它们的大小，我应该提供整个背景，好像没有螺丝，还是只是随机的图片？
yolo 模型还会将所有图像调整为相同大小吗？如果将螺丝大小调整为 96x96，而图像中的螺丝大小为 200x200，会怎么样？150x150？训练会考虑到这一点吗？
请指导我并告诉我我的数据集应该是什么样的]]></description>
      <guid>https://stackoverflow.com/questions/79367331/misunderstanding-of-what-images-to-provide-for-yolov5-training-when-searching-fo</guid>
      <pubDate>Sat, 18 Jan 2025 14:39:23 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“torch”（未知位置）导入名称“Tensor”</title>
      <link>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</link>
      <description><![CDATA[我尝试从 PyTorch 导入 Tensor：
从 torch 导入 Tensor

但我一直收到此错误：
ImportError：无法从“torch”（未知位置）导入名称“Tensor”

我尝试过的方法：

检查 PyTorch 是否已安装（pip show torch），我使用的是版本 2.5.1。
重新安装 PyTorch：
pip uninstall torch
pip install torch


在 Python shell 中测试了导入，但错误依然存在。

环境：

Python版本：3.10
PyTorch 版本：2.5.1
操作系统：Windows 10
虚拟环境：是

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</guid>
      <pubDate>Sat, 18 Jan 2025 13:04:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不屏蔽 transformer 中除了多头注意力之外的其他层？[关闭]</title>
      <link>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</link>
      <description><![CDATA[通常在训练 NLP 任务时，我们需要将序列填充到 max_len，以便可以以批处理方式高效处理它们。但是，这些填充的标记不应影响训练（模型参数的更新），因为它们是“虚构的”。
每个人都在谈论掩盖注意力操作的必要性。这是有道理的，因为有效标记不应该关注虚拟标记。但是，我们仍然需要考虑所有其他层（例如 FF 中的线性层、规范化层等）不受填充的影响。
为了简单起见，考虑单个 Transformer-Encoder 层：

从图中可以清楚地看出，FF 和最后的规范化层将填充的标记视为其他每个标记。为了正确屏蔽所有层，我们应该屏蔽损失吗？
例如，假设我们想使用 Transformer 编码器对序列进行分类。我们传递一个形状为 (B, N, E) 的输入 x，其中 B 是批量大小，N 是最大序列长度，E 是嵌入维度。输出具有相同的形状，我们将使用它进行分类。我们可以通过为每个序列提取一个“全局”向量来实现，然后将其传递给特定于任务的头部，如下所示：
x =coder_layer(x)
x = x.mask_fill(mask, -torch.inf) # 在最大操作期间屏蔽填充。
x = torch.max(x, dim=1)[0] # 忽略索引，简化为形状 (B, E)。
loss = loss_fn(cls_head(x), y)

由于 max 操作对任何未选定元素返回 0 梯度，因此所有参数都不会受到填充的影响。我的理解正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</guid>
      <pubDate>Fri, 17 Jan 2025 23:05:49 GMT</pubDate>
    </item>
    <item>
      <title>如何利用多个 CPU 进行 YOLO 训练？</title>
      <link>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</link>
      <description><![CDATA[我可以访问一个没有 GPU 的大型 CPU 集群。通过在多个 CPU 节点之间并行化，是否可以加快 YOLO 训练速度？
文档说 device 参数指定用于训练的计算设备：单个 GPU（device=0）、多个 GPU（device=0,1）、CPU（device=cpu）或 Apple 芯片的 MPS（device=mps）。
那么多个 CPU 呢？]]></description>
      <guid>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</guid>
      <pubDate>Fri, 17 Jan 2025 16:05:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的神经网络在训练数据上具有很高的准确率，但在测试数据上只有 10％ 的准确率？[关闭]</title>
      <link>https://stackoverflow.com/questions/79350490/why-does-my-neural-network-have-high-accuracy-on-training-data-but-only-10-accu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79350490/why-does-my-neural-network-have-high-accuracy-on-training-data-but-only-10-accu</guid>
      <pubDate>Sun, 12 Jan 2025 18:42:22 GMT</pubDate>
    </item>
    <item>
      <title>在扩展中访问 NetLogo 扩展</title>
      <link>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</link>
      <description><![CDATA[我正在尝试开发一个 NetLogo 扩展来与不同的 LLMS（在线、离线）进行通信。LLM 调用返回 JSON 格式的字符串。我想解析 JSON 并将其转换为嵌套的 TABLE 对象，以访问 NetLogo Table 扩展。
是否有办法让一个扩展访问和使用另一个扩展中的类？]]></description>
      <guid>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</guid>
      <pubDate>Fri, 09 Aug 2024 03:21:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Pytorch 中的 KL 散度为负？</title>
      <link>https://stackoverflow.com/questions/78008233/why-kl-divergence-is-negative-in-pytorch</link>
      <description><![CDATA[我尝试使用 Pytorch 获取 2 个分布之间的 KL 散度，但输出通常为负，不应该如此:
import torch 
import torch.nn. functional as F

x_axis_kl_div_values = []
for epoch in range(200):
# 每个 epoch 生成 2 个不同的分布
input_1 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)
input_2 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)

kl_divergence = F.kl_div(input_1.log(), input_2, reduction=&#39;batchmean&#39;)
x_axis_kl_div_values.append(kl_divergence.item())

x_axis_kl_div_values 
&gt;&gt;&gt; 
[324.4713134765625,
-69.10758972167969,
-92.42606353759766,

我在 Pytorch 论坛上找到了这个，其中提到他们的问题是输入不是适当的分布，而我的代码中并非如此，因为我正在创建正态分布。从这个 SO 线程来看，他们的问题似乎是 nn.KLDivLoss 期望输入是对数概率，但同样，我在代码中这样做了。所以我不确定我遗漏了什么]]></description>
      <guid>https://stackoverflow.com/questions/78008233/why-kl-divergence-is-negative-in-pytorch</guid>
      <pubDate>Fri, 16 Feb 2024 14:58:47 GMT</pubDate>
    </item>
    <item>
      <title>神经网络为何如此有效？</title>
      <link>https://stackoverflow.com/questions/38595451/why-do-neural-networks-work-so-well</link>
      <description><![CDATA[我理解使用前向传播和反向传播进行梯度下降训练神经网络的所有计算步骤，但我试图弄清楚为什么它们比逻辑回归效果好得多。
目前我能想到的只有：
A) 神经网络可以学习自己的参数
B) 权重比简单的逻辑回归多得多，因此可以进行更复杂的假设
有人能解释一下为什么神经网络总体上效果这么好吗？我是一个相对初学者。]]></description>
      <guid>https://stackoverflow.com/questions/38595451/why-do-neural-networks-work-so-well</guid>
      <pubDate>Tue, 26 Jul 2016 16:38:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google 云中设置 TensorFlow？</title>
      <link>https://stackoverflow.com/questions/37231866/how-do-i-set-up-tensorflow-in-the-google-cloud</link>
      <description><![CDATA[如何在 Google 云中设置 TensorFlow？我了解如何创建 Google Compute Engine 实例，以及如何在本地运行 TensorFlow；最近的 Google 博客文章建议应该有一种方法可以在云中创建 Google Compute Engine 实例并运行 TensorFlow 应用程序：

机器学习项目可以有多种规模，正如我们在开源产品 TensorFlow 中看到的那样，项目通常需要扩大规模。一些小任务最好使用在桌面上运行的本地解决方案来处理，而大型应用程序则需要托管解决方案的规模和可靠性。 Google Cloud Machine Learning 旨在支持全方位，并提供从本地到云环境的无缝过渡。

即使我对此有一点了解，但考虑到微软 Azure 等竞争平台所提供的功能，肯定有一种方法可以在 Google Cloud 中设置 TensorFlow 应用程序（本地开发并“无缝”扩展到云中，大概使用 GPU）。
例如，我想在 IDE 中本地工作，调整项目的功能和代码，在那里运行有限的训练和验证，并定期将代码推送到云中，在那里使用（任意）更多的资源运行训练，然后保存和下载训练好的模型。或者更好的是，只需使用可调资源在云中运行图表（或图表的一部分）。
有没有办法做到这一点；有计划吗？如何在 Google Cloud 中设置 TensorFlow？]]></description>
      <guid>https://stackoverflow.com/questions/37231866/how-do-i-set-up-tensorflow-in-the-google-cloud</guid>
      <pubDate>Sat, 14 May 2016 21:05:28 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中，批量越大计算时间越短吗？</title>
      <link>https://stackoverflow.com/questions/35158365/will-larger-batch-size-make-computation-time-less-in-machine-learning</link>
      <description><![CDATA[我正在尝试调整超参数，即 CNN 中的批量大小。我有一台 i7 核的电脑，RAM 为 12GB，我正在使用 CIFAR-10 数据集训练 CNN 网络，该数据集可在此博客中找到。现在，首先我阅读并了解了机器学习中的批量大小：

首先假设我们正在进行在线学习，即我们使用的迷你批量大小为 1。对在线学习的明显担忧是，使用仅包含单个训练示例的迷你批量将导致我们对梯度的估计出现重大错误。
但事实上，错误并不是一个问题。原因是单个梯度估计不需要非常准确。我们需要的只是一个足够准确的估计，使我们的成本函数趋于不断下降。这就像你试图到达磁北极，但每次看的时候，你的指南针都会偏离 10-20 度。只要你经常停下来检查指南针，并且指南针平均能正确指向方向，你最终就能到达磁北极。
基于这个论点，听起来我们应该使用在线学习。事实上，情况比这更复杂。我们知道，我们可以使用矩阵技术同时计算小批量中所有示例的梯度更新，而不是循环遍历它们。根据我们的硬件和线性代数库的细节，这可以使计算（例如）大小为 100 的小批量的梯度估计的速度快得多，而不是通过分别循环 100 个训练示例来计算小批量梯度估计。它可能只需要（比如说）50 倍的时间，而不是 100 倍的时间。现在，乍一看，这似乎对我们没有太大帮助。
对于大小为 100 的小批量，权重的学习规则如下所示：
其中总和是小批量中的训练示例。这与在线学习相比。
即使进行小批量更新只需要 50 倍的时间，进行在线学习似乎仍然更好，因为我们会更频繁地更新。但是，假设在
小批量情况下，我们将学习率提高 100 倍，那么
更新规则将变为
这很像以
学习率为 η 执行单独的在线学习实例。但它只需要执行单个在线学习实例的 50 倍时间。不过，使用较大的 minibatch 似乎完全有可能加快速度。


现在我尝试使用 MNIST 数字数据集，并运行一个示例程序，首先设置批处理大小 1。我记下了完整数据集所需的训练时间。然后我增加了批处理大小，我注意到它变得更快了。

但是，如果使用此代码和github 链接进行训练，更改批量大小不会减少训练时间。如果我使用 30 或 128 或 64，它保持不变。他们说他们获得了 92% 的准确率。经过两三个时期后，他们的准确率已经超过 40%。但是，当我在计算机上运行代码时，除了批量大小之外没有做任何改变，我在 10 个时期后得到了更糟糕的结果，只有 28%，并且测试准确率在接下来的时期内一直停留在那里。然后我想，因为他们使用了 128 的批量大小，需要使用那个。然后我用了同样的方法，但它变得更糟，在 10 个时期后只给出 11% 并且卡在那里。为什么会这样？]]></description>
      <guid>https://stackoverflow.com/questions/35158365/will-larger-batch-size-make-computation-time-less-in-machine-learning</guid>
      <pubDate>Tue, 02 Feb 2016 16:12:17 GMT</pubDate>
    </item>
    </channel>
</rss>