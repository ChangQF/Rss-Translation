<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 21 Nov 2024 03:28:30 GMT</lastBuildDate>
    <item>
      <title>Talebi 论文中空间决策树分裂逻辑的澄清</title>
      <link>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</link>
      <description><![CDATA[我正在研究 Talebi 等人的论文“用于地球科学数据分析和建模的真正空间随机森林算法”，我对图 2 所示的空间决策树过程有一些疑问。
混合旋转和缩放：
我的理解是，对于每个单元格，多个尺度和旋转的空间模式被矢量化并连接成单个输入向量。然后在树分割过程中将此输入用作预测器。这是正确的吗？
此外，模型如何确保来自不同旋转和尺度的模式在混合成一个向量时保留其空间上下文？
分割中灰色区域的移动：
在图 2 中，我注意到灰色单元格似乎代表数据的一个子集，在分割过程中从中间移动到角落。

这是否表明随着树分割成更小的区域，预测空间会缩小？
这种移动是将模式过滤成更均匀的子集的结果，还是代表了数据中空间依赖性的特定内容？

第二次分割中的不同灰色区域：

为什么第二次分割的左分支中的灰色区域保留在第一个-𝑅中，而在右分支中，它转移到第二个-𝑅？
这是否表明选择不同的预测因子来分割左分支和右分支？
如果是这样，这是否突出了数据集中的空间异质性？

任何关于这在空间决策树中如何工作的说明或示例都将非常有帮助。谢谢！这是我的电子邮件：p68117034@mail.ncku.edu.tw
我尝试了什么：
我回顾了图 2 的描述和论文“用于地球科学数据分析和建模的真正空间随机森林算法”中的方法。我试图理解灰色区域如何与空间模式和用于分割的预测因子相对应。
我预期会发生什么：
我预期灰色区域代表预测因子空间的一致划分，其中每个分割对应于所有分支中的相同预测因子或空间模式子集。
实际发生了什么：
我观察到在第二次分割中，左右分支之间的灰色区域有所不同。在左侧，灰色区域对应于第一个预测因子，而在右侧，它转移到第二个预测因子。我不确定这种差异是由于空间异质性、独立预测因子选择还是其他原因造成的。]]></description>
      <guid>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</guid>
      <pubDate>Thu, 21 Nov 2024 03:11:43 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>如何构建更高效的 DataLoader 来加载大型图像数据集？</title>
      <link>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</link>
      <description><![CDATA[我正在尝试在一个非常大的图像数据集上训练深度学习模型。模型输入需要一对图像（A 和 B）。由于我的图像尺寸非常大，我已将每个图像调整为形状为 (3x224x224) 的 torch.Tensor，并将每对图像作为单独的文件存储在我的磁盘上。相同的对共享相同的索引。
但是，当使用数据集和 DataLoader 将这些文件加载​​到内存中时，我遇到了以下问题：

CPU 内存问题：将工作器数量设置为 12 时，200GB 内存很快就会耗尽。我尝试设置 prefetch_factor=1，但没有帮助。
初始化缓慢：在训练开始之前，每个 epoch 之前都需要很长时间进行初始化。我在之前的帖子中看到，这可能是由于初始化的开销造成的。我设置了 persistent_workers=True，但也没有帮助。
GPU 和批次大小：我使用 4 个 GPU 进行 DDP 训练，当前批次大小为 1024。

有没有关于如何提高数据集或 DataLoader 效率的建议？

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
std=[0.229, 0.224, 0.225])

augmentation = transforms.Compose([
transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)], p=0.8),
transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))], p=0.5),
transforms.RandomGrayscale(p=0.1),
transforms.RandomVerticalFlip(p=0.5),
normalize,
])

class ImagePairDataset(Dataset):
def __init__(self, data_save_folder, dataset_name, num_samples, transform=None):
&quot;&quot;&quot;
Args:
data_save_folder (str)：包含数据文件的文件夹路径。
dataset_name (str)：表示数据集拆分的“train”、“val”或“test”之一。
num_samples (int)：数据集拆分中的样本数。 (train: 3000000, val: 10000, test: 10000)
transform (可调用，可选)：应用于图像张量的可选变换。
&quot;&quot;&quot;
self.data_save_folder = data_save_folder
self.dataset_name = dataset_name
self.num_samples = num_samples
self.transform = transform

def __len__(self):
return self.num_samples

def __getitem__(self, idx):

# 根据 idx 构建文件路径
A_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_A_images_{idx}.pt&quot;
B_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_B_images_{idx}.pt&quot;
label_path = f&quot;{self.data_save_folder}/{self.dataset_name}_labels_{idx}.pt&quot;

# 从文件路径加载张量
A_image = torch.load(A_image_path)
B_image = torch.load(B_image_path)
label = torch.load(label_path)

# 如果可用，则应用转换
if self.transform:
A_image = self.transform(A_image)
B_image = self.transform(B_image)

return A_image, B_image, label

class ImagePairDataModule(pl.LightningDataModule):

def __init__(self, data_save_folder, train_samples, val_samples, test_samples, batch_size=32, num_workers=4):
super().__init__()
self.data_save_folder = data_save_folder
self.train_samples = train_samples
self.val_samples = val_samples
self.test_samples = test_samples
self.batch_size = batch_size
self.num_workers = num_workers
self.train_transform = augmentation
self.eval_transform = normalize # 仅对验证和测试进行标准化

def setup(self, stage=None):

self.train_dataset = ImagePairDataset(self.data_save_folder, &#39;train&#39;, self.train_samples, transform=self.train_transform)
self.val_dataset = ImagePairDataset(self.data_save_folder, &#39;val&#39;, self.val_samples, transform=self.eval_transform)
self.test_dataset = ImagePairDataset(self.data_save_folder, &#39;test&#39;, self.test_samples, transform=self.eval_transform)

def train_dataloader(self): #prefetch_factor=1, , persistent_workers=True
return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers) 

def val_dataloader(self):
return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

# 初始化 DataModule
data_module = ImagePairDataModule(
data_save_folder=args.data_save_folder,
train_samples=train_samples,
val_samples=val_samples,
test_samples=test_samples,
batch_size=args.batch_size,
num_workers=12,
)
]]></description>
      <guid>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</guid>
      <pubDate>Wed, 20 Nov 2024 20:12:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gamma=0 的二元焦点交叉熵总是会产生 nan 损失？</title>
      <link>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</link>
      <description><![CDATA[我正在训练一个 U-Net 来对我们的实验图像进行二值化。但前景通常没有得到很好的体现，换句话说，我有类别不平衡。我一直在使用 BinaryCrossEntropy 作为损失函数。因此，我理解一种简单的方法是定义一个自定义的损失函数，为每个类别赋予权重。但我在这样做时遇到了一些问题，所以放弃了这个尝试（见最后）。我认为使用 BinaryFocalCrossEntropy 更简单，它的表达式为（如果我理解得好的话）

因此，我的计划是将其与 gamma=0 一起使用，这样我就可以通过调整 alpha 值来赋予类别权重。但是，我不断得到 nan 损失。它发生在几个批次之后的第一个时期内：（这里我使用 \alpha = 0.75）

这里我使用了 adam 优化器和 Learning_rate 1e-3。我注意到，如果我改用 1e-4，即使 nan 仍然出现，它也会再完成几个批次后出现。您能帮我找出这是怎么回事吗？我该如何解决这个问题？
附加问题（自定义函数的问题）
对于加权交叉熵，我编写了一个函数，如下所示：
def weighted_binary_crossentropy(y_true, y_pred):
# 计算标准二元交叉熵
bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)

# 创建一个权重图，其中 y_true 用于将正确的权重应用于每个像素
weights = y_true * weight_for_white + (1 - y_true) * weight_for_black

# 将二元交叉熵乘以权重
weighted_bce = weights * bce

# 取平均值得到最终的加权损失
return tf.reduce_mean(weighted_bce)

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=weighted_binary_crossentropy, metrics=[&#39;accuracy&#39;]) 

# 进行训练
results = model.fit(X_train, Y_train, validation_data=(X_validation,Y_validation), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)

但我在实现它时遇到了问题，所以半途而废了。如果您对此有任何评论，也会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</guid>
      <pubDate>Wed, 20 Nov 2024 15:49:20 GMT</pubDate>
    </item>
    <item>
      <title>真实情况和预测标签不匹配[关闭]</title>
      <link>https://stackoverflow.com/questions/79207893/ground-truh-and-prediected-label-not-match</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79207893/ground-truh-and-prediected-label-not-match</guid>
      <pubDate>Wed, 20 Nov 2024 15:29:38 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的损失来训练不同阶段的模型</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[我正在尝试以端到端的方式训练一个两阶段模型。但是，我想用不同的损失更新模型的不同阶段。例如，假设端到端模型由两个模型组成：model1 和 model2。输出是通过运行计算的
features = model1(inputs)
output = model2(features)

我想用 loss1 更新 model1 的参数，同时保持 model2 的参数不变。接下来，我想用 loss2 更新 model2 的参数，同时保持 model1 的参数不变。我的完整实现如下：
import torch
import torch.nn as nn

# 定义第一个模型
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# 定义第二个模型
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# 初始化模型
model1 = Net()
model2 = Net1()

# 初始化单独的每个模型的优化器
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# 样本输入和标签
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 

但是，这将返回
回溯（最近一次调用最后一次）：
文件，第 55 行，在 &lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, 第 521 行, 在反向传播中
torch.autograd.backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, 第 289 行, 在反向传播中
_engine_run_backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, 第 769 行, 在 _engine_run_backward 中
return Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传播
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]（AsStridedBackward0 的输出 0）处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

我有点明白为什么会发生这种情况，但有办法解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>识别基于树的方法中的过度拟合特征[关闭]</title>
      <link>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</link>
      <description><![CDATA[我有一份表格数据，我尝试过的所有基于树的方法（随机森林、XGBoost、Catboost、LightGBM 等）都过度拟合了。我尝试过更改 `ccp_alpha` 等参数，这会降低我的训练和验证指标。我将忽略过度拟合的程度，因为它与我的实际问题无关。
我正在尝试检查我的某些特征是否会导致过度拟合。以下是我对如何进行此操作的想法。
计算训练和验证数据上的特征重要性，现在如果我看到某些高重要性特征的特征重要性急剧下降，我会得出结论，是该特征导致了我的痛苦。我的方法有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</guid>
      <pubDate>Wed, 20 Nov 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>对来自不同研究的汇总数据进行建模，并确定每项研究的变量重要性</title>
      <link>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</link>
      <description><![CDATA[我有来自四项独立研究的数据。每项研究都考察了不同药物对治疗反应的影响。所有四项研究都有相同的基线变量和结果变量（治疗成功）。除了使用基线变量对治疗成功进行分类之外，我还想确定哪些基线变量对于对这四种药物的成功进行分类最为重要。我计划使用 tidymodels 来做到这一点。即使用一组由不同算法（xgboost、随机森林、svm）组成的工作流，确定最佳工作流并检查变量重要性。
我不确定如何最好地比较这四种药物的变量重要性。最好是整理所有数据并运行一个具有每种药物和每个基线变量之间相互作用效应（step_interact）的模型，还是运行四个单独的模型（每种药物一个）并比较四个模型中变量的重要性，还是做其他事情？我不确定第一种方法是否能让我分离出每种药物的特定变量的重要性。
如何最好地解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</guid>
      <pubDate>Wed, 20 Nov 2024 03:53:42 GMT</pubDate>
    </item>
    <item>
      <title>在 sklearn 中，模型训练完成后，可以将数据集上的“transform”替换为“predict”吗？</title>
      <link>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</link>
      <description><![CDATA[我理解 sklearn 中的 transform 和 predict 之间存在差异，但是由于两者都用于新数据集，如果我的目标只是通过 umap 和 clustering 等模型获得预测结果，我可以在模型训练后用 predict 替换使用 transform 来获得相同的结果集吗？

一些参考：
sklearn 的 transform() 和 predict() 方法有什么区别？
transform、fit_transform、predict 和 fit 之间的区别]]></description>
      <guid>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</guid>
      <pubDate>Wed, 20 Nov 2024 02:31:28 GMT</pubDate>
    </item>
    <item>
      <title>独热编码特征不匹配问题</title>
      <link>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</guid>
      <pubDate>Tue, 19 Nov 2024 23:21:56 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CoreML 预测中获取置信度变量</title>
      <link>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</link>
      <description><![CDATA[我正在使用 CreateML 工具训练文本分类器，当我使用预览功能并输入一个句子时，它会给我一个预测以及一个置信度变量
以下是我在应用程序上使用该模型的方式
import CoreML
...

func predict(phrase:String) -&gt; String {
guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
return &quot;Failed to load the Roll Model.&quot;
}

let rollModelInput = RollInput(text: phrase)

guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
return &quot;Roll Model Prediction Failed&quot;
}

return prediction.label
}

这有效，它提供了预测。
我的数据是标准文本/标签格式
即使我将模型导出到 xcode 并在 xcode 中运行预览，置信度变量仍然存在。
当我在设备上运行预测时，我想知道置信度变量是什么，我如何获取访问权限？
]]></description>
      <guid>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</guid>
      <pubDate>Fri, 15 Nov 2024 11:13:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>Sk-learn GridSearchCV 适用于完整数据</title>
      <link>https://stackoverflow.com/questions/61882244/sk-learn-gridsearchcv-fits-on-full-data</link>
      <description><![CDATA[我使用 sklearn GridSearchCV 通过 lda 模型搜索了 # 个主题。拟合模型后，拟合模型保存在 CV_model.best_estimator_ 中。根据 sklearn 文档，GridSearchCV 具有默认选项 refit, default=True，即“使用在整个数据集上找到的最佳参数重新拟合估计器”。 Sklearn GridSearchCV
由于文档中说它已经适合完整数据，因此我相信 CV_model.best_estimator_.fit_transform(full_train_data) 将带来与 CV_model.best_estimator_.transform(full_train_data) 相同的结果。但是，使用 fit_transform 和 transform 的输出不同。我错过了什么？在 GridsearchCV 之后我应该使用 fit_transform 还是 transform？]]></description>
      <guid>https://stackoverflow.com/questions/61882244/sk-learn-gridsearchcv-fits-on-full-data</guid>
      <pubDate>Tue, 19 May 2020 02:29:07 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `squeeze()` 与 `unsqueeze()`</title>
      <link>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</link>
      <description><![CDATA[我不明白 squeeze() 和 unsqueeze() 对张量做了什么，即使查看了文档和相关问题。
我试图通过在 Python 中自己探索来理解它。我首先用创建了一个随机张量
x = torch.rand(3,2,dtype=torch.float)
&gt;&gt;&gt; x
tensor([[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]])

但无论我如何挤压它，我都会得到相同的结果：
torch.equal(x.squeeze(0), x.squeeze(1))
&gt;&gt;&gt; True

如果我现在尝试解压，我会得到以下结果，
&gt;&gt;&gt; x.unsqueeze(1)
张量([[[0.3703, 0.9588]],
[[0.8064, 0.9716]],
[[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(0)
张量([[[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[[0.3703],
[0.9588]],
[[0.8064],
[0.9716]],
[[0.9585],
[0.7860]]])

但是，如果我现在创建一个张量  x = torch.tensor([1,2,3,4])，然后我尝试解压它，那么看起来 1 和 -1 使它成为一列，而 0 保持不变。
x.unsqueeze(0)
tensor([[1, 2, 3, 4]])
&gt;&gt;&gt; x.unsqueeze(1)
tensor([[1],
[2],
[3],
[4]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[1],
[2],
[3],
[4]])

有人能解释一下 squeeze() 和 unsqueeze() 对张量做了什么吗？提供参数 0、1 和 -1 之间有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</guid>
      <pubDate>Mon, 04 May 2020 18:06:38 GMT</pubDate>
    </item>
    <item>
      <title>如何正确地将不平衡的数据集拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</link>
      <description><![CDATA[我有一个航班延误数据集，在采样之前尝试将该数据集拆分为训练集和测试集。准时情况约占总数据的 80%，延误情况约占 20%。
通常，机器学习中训练集和测试集的大小比例为 8:2。但数据太不平衡了。因此，考虑到极端情况，大多数训练数据都是准时情况，而大多数测试数据都是延误情况，准确率会很差。
所以我的问题是如何正确将不平衡的数据集拆分为训练集和测试集？]]></description>
      <guid>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</guid>
      <pubDate>Sat, 27 Jul 2019 06:34:52 GMT</pubDate>
    </item>
    </channel>
</rss>