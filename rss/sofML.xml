<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 26 Jan 2024 15:13:56 GMT</lastBuildDate>
    <item>
      <title>如何使用 Azure AI 文档智能下载客户训练的模型</title>
      <link>https://stackoverflow.com/questions/77887035/how-to-download-custome-trained-models-with-azure-ai-document-intelligence</link>
      <description><![CDATA[有没有办法下载我在 Azure AI Document Intelligence Studio 中训练的模型并在本地使用]]></description>
      <guid>https://stackoverflow.com/questions/77887035/how-to-download-custome-trained-models-with-azure-ai-document-intelligence</guid>
      <pubDate>Fri, 26 Jan 2024 14:46:05 GMT</pubDate>
    </item>
    <item>
      <title>客户流失的时间感知数据 - 帮助解决机器学习问题，了解方法是否正确以及模型建议</title>
      <link>https://stackoverflow.com/questions/77886825/timeaware-data-for-customer-churn-help-on-ml-problem-to-understand-if-the-appr</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77886825/timeaware-data-for-customer-churn-help-on-ml-problem-to-understand-if-the-appr</guid>
      <pubDate>Fri, 26 Jan 2024 14:09:29 GMT</pubDate>
    </item>
    <item>
      <title>隔离电子邮件以进行电子邮件解析</title>
      <link>https://stackoverflow.com/questions/77886784/segregation-of-emails-for-email-parsing</link>
      <description><![CDATA[我正在研究电子邮件解析，并面临一个问题，我想将报价电子邮件与其他电子邮件分开并仅提取那些电子邮件（报价）。
现在我可以阅读所有未读电子邮件并转换和提取所有电子邮件，但无法找到仅提取报价电子邮件的方法。
如果您能提出任何方法来做到这一点，我将非常感激。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77886784/segregation-of-emails-for-email-parsing</guid>
      <pubDate>Fri, 26 Jan 2024 14:03:47 GMT</pubDate>
    </item>
    <item>
      <title>如何增加特征数量</title>
      <link>https://stackoverflow.com/questions/77886768/how-to-increase-numbers-of-features</link>
      <description><![CDATA[当我需要预测游戏价格时，我正在解决一项机器学习任务。
我有大约 60 列，其中有 10 个整数特征和 50 个布尔值。
如何增加特征数量（也许使用一些具有数字特征的函数）？
我已将 Catboost 和 LGBM 回归器与 gridsearch 结合使用。所以我不知道如何改进模型]]></description>
      <guid>https://stackoverflow.com/questions/77886768/how-to-increase-numbers-of-features</guid>
      <pubDate>Fri, 26 Jan 2024 14:00:37 GMT</pubDate>
    </item>
    <item>
      <title>将从 keras YOLOV8Detector 获取的模型更新到 Apple MLPackage/CoreML</title>
      <link>https://stackoverflow.com/questions/77886439/update-model-obtained-from-keras-yolov8detector-to-apple-mlpackage-coreml</link>
      <description><![CDATA[我按照 KerasCV 上的教程使用 YOLOV8 和 KerasCV 进行高效目标检测，并在 不同的数据集
一段时间后，我能够获得预测并将其可视化，如教程中所述：
&lt; /p&gt;
我想在苹果 iOS 应用程序中使用这个模型，所以我使用了 coremltools 包来转换它。然而，“输出”似乎是这样的。 kerascv 制作的产品并不完全是苹果界所期望的。
模型训练完成后，我可以要求进行预测：
 图像，y_true = next(iter(dataset.take(1)))
    y_pred = model.predict(images) // y_pred 是一个字典

y_pred 是包含这些键的字典 [&#39;boxes&#39;, &#39;confidence&#39;, &#39;classes&#39;, &#39;num_detections&#39;]
使用Netron，我可以看看苹果世界所期待的模型的形状
&lt;img alt=&quot; “ src =“https://github.com/keras-team/keras-cv/assets/170917/5fae9594-694a-477c-a7a2-3c4419e3b98a”/&gt;
如何修改/重塑从 kerascv 生成的模型，这样我就可以拥有一个将置信度和坐标答案作为两个单独输出输出的模型，而不是输出字典？]]></description>
      <guid>https://stackoverflow.com/questions/77886439/update-model-obtained-from-keras-yolov8detector-to-apple-mlpackage-coreml</guid>
      <pubDate>Fri, 26 Jan 2024 13:00:22 GMT</pubDate>
    </item>
    <item>
      <title>从头开始训练神经网络时如何处理成本值波动？</title>
      <link>https://stackoverflow.com/questions/77886091/how-to-handle-cost-value-fluctuations-when-training-a-neural-network-from-scratc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77886091/how-to-handle-cost-value-fluctuations-when-training-a-neural-network-from-scratc</guid>
      <pubDate>Fri, 26 Jan 2024 12:01:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么在小数据集上微调 MLP 模型，仍然保持与预训练权重相同的测试精度？</title>
      <link>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</link>
      <description><![CDATA[我设计了一个简单的 MLP 模型，在 6k 数据样本上进行训练。
类 MLP(nn.Module)：
    def __init__(自身,input_dim=92,hidden_​​dim=150,num_classes=2):
        超级().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_​​dim = 隐藏_dim
        #self.softmax = nn.Softmax(dim=1)

        self.layers = nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.num_classes),

        ）

    def 前向（自身，x）：
        x = self.layers(x)
        返回x

并且模型已实例化
model = MLP(input_dim=input_dim,hidden_​​dim=hidden_​​dim,num_classes=num_classes).to(设备)

优化器= Optimizer.Adam(model.parameters(),lr=learning_rate,weight_decay=1e-4)
标准 = nn.CrossEntropyLoss()

和超参数：
num_epoch = 300 # 200e3//len(train_loader)
学习率 = 1e-3
批量大小 = 64
设备 = torch.device(“cuda”)
种子 = 42
火炬.manual_seed(42)

我的实现主要遵循这个问题。我将模型保存为预训练权重 model_weights.pth。
测试数据集上模型的准确率为96.80%。
然后，我还有另外 50 个样本（在 finetune_loader 中），我正在尝试在这 50 个样本上微调模型：
model_finetune = MLP()
model_finetune.load_state_dict(torch.load(&#39;model_weights.pth&#39;))
model_finetune.to（设备）
model_finetune.train()
# 训练网络
对于 t in tqdm(范围(num_epoch))：
  对于 i，enumerate(finetune_loader, 0) 中的数据：
    #def 闭包():
      # 获取并准备输入
      输入、目标 = 数据
      输入，目标=输入.float(), 目标.long()
      输入，目标 = 输入.to(设备), 目标.to(设备)
      
      # 将梯度归零
      优化器.zero_grad()
      # 执行前向传递
      输出 = model_finetune(输入)
      # 计算损失
      损失=标准（输出，目标）
      # 执行向后传递
      loss.backward()
      #回波损耗
      优化器.step() # a

model_finetune.eval()
使用 torch.no_grad()：
    输出2 = model_finetune(测试数据)
    #predicted_labels =outputs.squeeze().tolist()

    _, preds = torch.max(输出2, 1)
    Prediction_test = np.array(preds.cpu())
    准确度测试微调 = 准确度得分（y_测试，预测测试）
    精度测试微调
    
    输出：0.9680851063829787

我检查过，精度与将模型微调到 50 个样本之前保持不变，并且输出概率也相同。
可能是什么原因？我在微调代码中是否犯了一些错误？]]></description>
      <guid>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</guid>
      <pubDate>Fri, 26 Jan 2024 11:32:05 GMT</pubDate>
    </item>
    <item>
      <title>Mwaa 气流任务跳过</title>
      <link>https://stackoverflow.com/questions/77885701/mwaa-airflow-task-skipping</link>
      <description><![CDATA[我正在尝试创建一个机器学习训练 mwaa 气流工作流程
我有以下步骤

数据处理
培训
批量推理

我的要求是每周进行一次数据处理和推理，每月进行一次训练。
如何跳过训练步骤并使其每月仅运行一次]]></description>
      <guid>https://stackoverflow.com/questions/77885701/mwaa-airflow-task-skipping</guid>
      <pubDate>Fri, 26 Jan 2024 10:55:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 Bag Of Words 的输出到其他数据集</title>
      <link>https://stackoverflow.com/questions/77885385/using-output-of-bag-of-words-to-other-dataset</link>
      <description><![CDATA[我正在尝试对数据集（包含句子）执行 BagOfwords 算法，并且词袋算法的输出（在清除所有不重要的单词之后）将进入第二个数据集（包含分割的数据我将把图片放在底部）但我的问题是如何附加第一个数据集的输出以便很好地进入第二个数据集训练。我想过做一个字典来转换和划分句子，以适合第二个数据集值，但这非常困难，我希望你有更好的主意。
我对他做的第一个数据集的样本 BagOf 单词
“过去几周，我的手臂、腿和躯干上一直出现皮疹。它呈红色、发痒，并覆盖着干燥的鳞片状斑块。”
来自第二个数据集的图片，我试图将第一个数据集连接到第二个数据集的神经网络：
在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/77885385/using-output-of-bag-of-words-to-other-dataset</guid>
      <pubDate>Fri, 26 Jan 2024 09:55:11 GMT</pubDate>
    </item>
    <item>
      <title>“java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。” - 但他们确实</title>
      <link>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</link>
      <description><![CDATA[我正在尝试使用 Android Studio 中的以下代码将 ML 模型集成到 Android 应用中：
predictBtn.setOnClickListener(new View.OnClickListener() {
            @覆盖
            公共无效onClick（查看视图）{
                尝试 {
                    ModelUnquant 模型 = ModelUnquant.newInstance(MainActivity.this);


                    // 检查位图是否不为空
                    if (位图！= null) {
                        // 创建输入以供参考。
                        TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 244, 244, 3}, DataType.FLOAT32);

                        位图 = Bitmap.createScaledBitmap(位图, 244, 244, true);
                        inputFeature0.loadBuffer(TensorImage.fromBitmap(bitmap).getBuffer());

                        // 运行模型推理并获取结果。
                        ModelUnquant.Outputs 输出 = model.process(inputFeature0);
                        TensorBufferoutputFeature0=outputs.getOutputFeature0AsTensorBuffer();

                        // 在这里更改类号
                        result.setText(labels[getMax(outputFeature0.getFloatArray())]+“”);
                    } 别的 {
                        // 处理bitmap为null的情况
                        result.setText(“错误：无图像”);
                    }

                    // 如果不再使用则释放模型资源。
                    模型.close();
                } catch (IOException e) {
                    // TODO 处理异常
                }
            }
        });

返回以下错误：
致命异常：main
进程：com.example.archeyewitness，PID：15698

java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。

在 org.tensorflow.lite.support.common.SupportPreconditions.checkArgument（SupportPreconditions.java:104）

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:309)

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:328)

在 com.example.archeyewitness.MainActivity$3.onClick(MainActivity.java:96)

现在，我尝试通过打印字节缓冲区的大小和形状来调试它：
Log.d(&quot;DebugInfo&quot;, &quot;张量缓冲区大小：&quot; + tensorBufferSize);
Log.d(&quot;DebugInfo&quot;, &quot;字节缓冲区大小:&quot; + byteBufferSize);
Log.d(“DebugInfo”, “张量缓冲区形状：” + Arrays.toString(tensorBufferShape));
Log.d(“DebugInfo”, “预期形状：” + Arrays.toString(byteBufferShape));


返回以下内容：
张量缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 字节缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 张量缓冲区形状：[1, 224, 224, 3]
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 预期形状：[1, 224, 224, 3]
2024-01-25 18:29:33.971 14367-14367 AndroidRuntime com.example.archeyewitness D 关闭虚拟机
2024-01-25 18:29:33.992 14367-14367 AndroidRuntime com.example.archeyewitness

如您所见，尺寸确实匹配，但我仍然遇到相同的错误。有人可以解释一下发生了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</guid>
      <pubDate>Fri, 26 Jan 2024 00:16:55 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 机器学习 ANN 模型在 Flask API 调用期间未加载 [关闭]</title>
      <link>https://stackoverflow.com/questions/77881808/sklearn-machine-learning-ann-model-not-loading-during-an-flask-api-call</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77881808/sklearn-machine-learning-ann-model-not-loading-during-an-flask-api-call</guid>
      <pubDate>Thu, 25 Jan 2024 17:16:42 GMT</pubDate>
    </item>
    <item>
      <title>“PyTorch Conv2d 错误：预期有 3 个通道，但 [1, 128, 128, 3] 为 128。需要帮助！”</title>
      <link>https://stackoverflow.com/questions/77881247/pytorch-conv2d-error-expected-3-channels-got-128-for-1-128-128-3-help-n</link>
      <description><![CDATA[我在使用 PyTorch 卷积神经网络时遇到问题。我收到的错误消息是：
给定 groups=1，权重大小为 [8, 3, 5, 5]，预期输入 [1, 128, 128, 3] 有 3 个通道，但实际有 128 个通道
上下文：
模型架构：
导入 torch.nn 作为 nn
导入 torch.nn.function 作为 F

CNN 类（nn.Module）：
    def __init__(自身):
        超级（CNN，自我）.__init__()
        self.cnn_model = nn.Sequential(
        nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 3, stride = 5),
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 2, stride = 5))
        
        self.fc_model = nn.Sequential(
        nn.Linear（输入特征= 256，输出特征= 120），
        nn.Tanh(),
        nn.Linear(in_features = 120, out_features = 84),
        nn.Tanh(),
        nn.Linear(in_features = 84, out_features = 1))
        
    def 前向（自身，x）：
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        x = F.sigmoid(x)
        
        返回x

数据加载：
类 MRI（数据集）：
    def __init__(自身):
        # 加载图像和标签
        肿瘤=[]
        path_tumor = &#39;/kaggle/input/brain-tumor/Dataset/Yes_Data/*.jpg&#39;
        对于 glob.iglob(path_tumor) 中的 f：
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), 插值=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            肿瘤.append(img)

        健康=[]
        path_healthy = &#39;/kaggle/input/brain-tumor/Dataset/No_data/*.jpg&#39;
        对于 glob.iglob(path_healthy) 中的 f：
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), 插值=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            健康的.append(img)

        # 将列表转换为 numpy 数组
        健康 = np.array(健康)
        肿瘤 = np.array(肿瘤)
        
        # 创建标签
        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)
        health_label = np.zeros(healthy.shape[0], dtype=np.float32)

        # 连接图像和标签
        images = np.concatenate((肿瘤，健康)，轴=0)
        标签 = np.concatenate((tumor_label,healthy_label), axis=0)
        self.images = 图像
        self.labels = 标签

    def __getitem__(自身，索引)：
        样本 = {&#39;image&#39;: self.images[index], &#39;label&#39;: self.labels[index]}
        返回样品

    def __len__(自身):
        返回 self.images.shape[0]

    def 标准化（自身）：
        self.images = (self.images / 255.0).astype(np.float32)

错误上下文：
model.eval()
输出 = []
y_true = []

使用 torch.no_grad()：
    对于数据加载器中的示例：
        对于范围内的 i(sample[&#39;image&#39;].size(0))：
            图像 = 样本[&#39;图像&#39;][i].squeeze().to(device).float()
            标签 = 样本[&#39;标签&#39;][i].to(设备)

            y_hat = 模型（图像）
            输出.append(y_hat.cpu().detach().numpy())
            y_true.append(label.cpu().detach().numpy())


我已经尝试了所有调试方法，打印并检查形状]]></description>
      <guid>https://stackoverflow.com/questions/77881247/pytorch-conv2d-error-expected-3-channels-got-128-for-1-128-128-3-help-n</guid>
      <pubDate>Thu, 25 Jan 2024 15:46:44 GMT</pubDate>
    </item>
    <item>
      <title>将 XGBoost Shapely 值转换为 SHAP 的 Explanation 对象</title>
      <link>https://stackoverflow.com/questions/77800583/converting-xgboost-shapely-values-to-shaps-explanation-object</link>
      <description><![CDATA[我正在尝试将 XGBoost 形状值转换为 SHAP 解释器对象。将[此处][1]的示例与内置 SHAP 库一起使用需要几天的时间（即使在二次采样数据集上），而 XGBoost 库则需要几分钟。然而。我想输出一个与[此处][2]示例中显示的类似的蜂群图。
我的想法是，我可以使用 XGBoost 库来恢复形状值，然后使用 SHAP 库绘制它们，但蜂群图需要一个解释器对象。如何将我的 XGBoost 助推器对象转换为解释器对象？
这是我尝试过的：
导入形状
助推器 = model.get_booster()
d_test = xgboost.DMatrix(X_test[0:100], y_test[0:100])
shap_values = booster.predict(d_test, pred_contribs=True)
shap.plots.beeswarm(shap_values)

返回结果：
类型错误：蜂群图需要一个“Explanation”对象作为“shap_values”参数。

为了澄清，如果可能的话，我想用 xgboost 内置库生成的值创建解释器对象。避免 shap.explainer 或 shap.TreeExplainer 函数调用是一个优先事项，因为它们需要更长的时间（几天）而不是几分钟才能返回。
[1]: https: //shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Python%20Version%20of%20Tree%20SHAP.html
[2]:  https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html#A-simple-beeswarm-summary-plot]]></description>
      <guid>https://stackoverflow.com/questions/77800583/converting-xgboost-shapely-values-to-shaps-explanation-object</guid>
      <pubDate>Thu, 11 Jan 2024 13:55:27 GMT</pubDate>
    </item>
    <item>
      <title>Python 3.12 中的 TensorFlow</title>
      <link>https://stackoverflow.com/questions/77471744/tensorflow-in-python-3-12</link>
      <description><![CDATA[我正在尝试通过 Python 将数据导入 SQL，并且在 Python 3.12 中使用 pyodbc 和 TensorFlow，但 TensorFlow 不起作用。我无法使用 python 3.11，因为 pyodbc 与它不兼容。
我尝试使用pip install tensorflow，但收到此错误：
&lt;块引用&gt;
错误：找不到满足张量流要求的版本（来自版本：无）
错误：找不到张量流的匹配分布

如何在使用 Python 3.12 时解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/77471744/tensorflow-in-python-3-12</guid>
      <pubDate>Mon, 13 Nov 2023 04:34:57 GMT</pubDate>
    </item>
    <item>
      <title>将 sklearn 的 GridSearchCV 与管道一起使用，只需预处理一次</title>
      <link>https://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once</link>
      <description><![CDATA[我正在使用 sckit-learn 来调整模型超参数。我正在使用管道将预处理与估计器链接起来。我的问题的一个简单版本如下所示：
将 numpy 导入为 np
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn.pipeline 导入 make_pipeline
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.linear_model 导入 LogisticRegression


网格 = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression()),
                    param_grid={&#39;logisticregression__C&#39;: [0.1, 10.]},
                    简历=2，
                    改装=假）

_ = grid.fit(X=np.random.rand(10, 3),
             y=np.random.randint(2, 大小=(10,)))

就我而言，预处理（玩具示例中的 StandardScale()）非常耗时，而且我不会调整它的任何参数。
因此，当我执行该示例时，StandardScaler 被执行了 12 次。 2 个拟合/预测 * 2 个 cv * 3 个参数。但每次对参数 C 的不同值执行 StandardScaler 时，它都会返回相同的输出，因此计算一次，然后只运行管道的估计器部分会更高效。
我可以手动拆分预处理（未调整超参数）和估计器之间的管道。但要将预处理应用于数据，我应该只提供训练集。因此，我必须手动实现拆分，而根本不使用 GridSearchCV。
是否有一种简单/标准的方法可以避免在使用 GridSearchCV 时重复预处理？]]></description>
      <guid>https://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once</guid>
      <pubDate>Wed, 12 Apr 2017 10:10:47 GMT</pubDate>
    </item>
    </channel>
</rss>