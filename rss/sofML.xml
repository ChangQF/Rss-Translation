<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Apr 2024 18:19:31 GMT</lastBuildDate>
    <item>
      <title>需要为我的应用程序在日志中记录机器学习模型及其版本的输入</title>
      <link>https://stackoverflow.com/questions/78306322/need-inputs-on-logging-machine-learning-models-and-their-versions-in-logs-for-my</link>
      <description><![CDATA[所以我有一个网络应用程序，它根据用户输入的主题和问题推荐电影，它还使用 NLP 和 ML 模型，如命名实体识别 (NER) 模型来提取关键字和 BERT 模型。我目前只是将登录数据记录到 sql 中的数据库表之一中。现在我还想将 ML 模型及其版本记录到表中。
我脑子里有几个捕获点，即什么时候捕获这些数据。第一个是用户登录时，第二个是用户将电影添加到列表时。我考虑实现这一点，以便提高 ML 模型的准确性和跟踪性能。如果提出任何不相关的建议，它也将帮助我发现任何类型的差异。
计划只是记录提出建议时触发的模型及其版本。基本上我还会显示与推荐电影匹配的关键字，因此也可能显示这些关键字来自哪些型号以及版本
我只是对如何实现这个感到困惑，并且想要一些意见，如果有人以前做过类似的事情。会有帮助的。]]></description>
      <guid>https://stackoverflow.com/questions/78306322/need-inputs-on-logging-machine-learning-models-and-their-versions-in-logs-for-my</guid>
      <pubDate>Wed, 10 Apr 2024 18:05:25 GMT</pubDate>
    </item>
    <item>
      <title>多标签分类 - 平面二元分类器与分层二元分类器</title>
      <link>https://stackoverflow.com/questions/78305775/multilabel-classification-flat-binary-classifiers-vs-hierarchical-binary-class</link>
      <description><![CDATA[正在研究多标签分类，以解决用主题和国家/地区标记新闻文章的问题，其中标签遵循​​语法 &lt;主题&gt;-&lt;国家/地区&gt;，并且希望权衡多个二元分类器选项（平面与分层）分类）。
方法 1（平面分类）为每个标签构建 1 个模型。该模型学习 1 个标签内的依赖关系（即主题和国家/地区之间），但不学习标签之间的依赖关系。
方法 2（层次分类）构建主题模型（父模型）和随后的国家模型（子模型）。这种方法的好处是它考虑了分层信息，因此保留了标签之间的依赖性，但代价是父级传播到子级的错误，以及测量分层分类器性能的复杂性。它还假设主题的概念对于各个国家/地区保持相同。
想知道还有哪些其他因素会导致人们选择一种方法而不是另一种方法，为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78305775/multilabel-classification-flat-binary-classifiers-vs-hierarchical-binary-class</guid>
      <pubDate>Wed, 10 Apr 2024 16:18:31 GMT</pubDate>
    </item>
    <item>
      <title>在 MacOS 上编译 llama-cpp-python 时找不到 ggml-common.h</title>
      <link>https://stackoverflow.com/questions/78305294/ggml-common-h-not-found-compiling-llama-cpp-python-on-macos</link>
      <description><![CDATA[我正在学习 LLM 和 RAG。
目前，我在尝试使用带有 M2 的 MAC PRO 实例化 llm 时遇到问题。
# 回调支持 token-wise 流式传输
callback_manager2 = CallbackManager([StreamingStdOutCallbackHandler()])
 
n_gpu_layers = 1 # 根据您的模型和 GPU VRAM 池更改此值。
n_batch = 1 # 应介于 1 和 n_ctx 之间，考虑 GPU 中的 VRAM 量。

llmGPU = LlamaCpp(
 model_path=“路径/llama-2-13b-chat.Q2_K.gguf”,
 输入={“温度”：0.75，“max_length”：2000，“top_p”：1}，
 n_gpu_layers=n_gpu_layers,
 n_batch=n_batch,
 回调管理器=回调管理器2,
 详细=真，
）

当我尝试这样做时，它会出现此错误：
“program_source：3：10：致命错误：找不到“ggml-common.h”文件#include“ggml-common.h” ^~~~~~~~~~~~~~~” UserInfo={NSLocalizedDescription=program_source:3:10: 致命错误：找不到“ggml-common.h”文件 #include “ggml-common.h” ^~~~~~~~~~~~~~~ } llama_new_context_with_model：无法初始化 Metal 后端

我在互联网上发现使用此命令安装 llama 可以解决这个问题：
!CMAKE_ARGS=“-DLLAMA_METAL_EMBED_LIBRARY=ON -DLLAMA_METAL=on” pip install -U llama-cpp-python --no-cache-dir

但这不适合我。 （我是在 jupyter 笔记本上完成所有这些操作。
请注意，如果我避免使用 GPU 层和批处理，它会起作用，但需要 3 分钟才能解决 3+3，所以太慢了。我尝试过使用其他 LLama 版本，但仍然存在同样的问题。
我将不胜感激一些帮助！谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78305294/ggml-common-h-not-found-compiling-llama-cpp-python-on-macos</guid>
      <pubDate>Wed, 10 Apr 2024 14:52:54 GMT</pubDate>
    </item>
    <item>
      <title>如何删除具有特定条件的矩阵中的行</title>
      <link>https://stackoverflow.com/questions/78304850/how-to-remove-rows-in-a-matrix-with-a-certain-criteria</link>
      <description><![CDATA[我有一个由 7,219 列和 6,817 行组成的基因表达矩阵。我希望删除变异性较低的行。例如，该行中的 max\min 值 &lt;= 5 的行，以及 max - min &lt;= 500 的行。我不知道一行最大值的索引，只知道整个矩阵的索引。我应该使用 for 还是 if 循环？或者有更简单的方法吗？谢谢
我在网上寻找过任何解决方案，但只找到了当元素为特定值时排除行的方法，而不是在存在涉及多个元素作为标准的不等式的情况下。]]></description>
      <guid>https://stackoverflow.com/questions/78304850/how-to-remove-rows-in-a-matrix-with-a-certain-criteria</guid>
      <pubDate>Wed, 10 Apr 2024 13:38:43 GMT</pubDate>
    </item>
    <item>
      <title>具有额外指标的软件漏洞数据集的 ML/DL 算法</title>
      <link>https://stackoverflow.com/questions/78304043/ml-dl-algorithm-for-software-vulnerability-dataset-with-extra-metrics</link>
      <description><![CDATA[我有一个包含八列的数据集。第一列表示 C/C++ 中函数的漏洞/中立状态，而其余七列包含进程/代码指标。我正在寻找一种能够处理这个组合数据集的机器学习/深度学习算法。您能否建议一种有效结合这些方面的算法或方法？
虽然我观察到许多只使用代码或指标的方法，但我还没有遇到任何将两者集成的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78304043/ml-dl-algorithm-for-software-vulnerability-dataset-with-extra-metrics</guid>
      <pubDate>Wed, 10 Apr 2024 11:18:44 GMT</pubDate>
    </item>
    <item>
      <title>Deeplearning4j - 为字符串员工、工人、同事等创建模型</title>
      <link>https://stackoverflow.com/questions/78303940/deeplearning4j-creating-a-model-for-strings-employee-worker-associate-etc</link>
      <description><![CDATA[我想使用 deeplearning4j 构建模型/训练模型。
该模型应该理解所有与员工相关的术语，例如，它应该将“员工”、“工人”、“同事”等词理解为相同的词。
我使用下面的代码片段来创建数据集并拥有迭代器
 数据集 allData;
    尝试 (RecordReader recordReader = new CSVRecordReader(0, &#39;,&#39;)) {
        recordReader.initialize(new StringSplit(&quot;员工 ID,0&quot;));

        DataSetIterator 迭代器 = new RecordReaderDataSetIterator(recordReader, 150, FEATURES_COUNT, CLASSES_COUNT);
        allData = 迭代器.next();
    }

    allData.shuffle(42);

并最终出现以下错误：
线程“main”中出现异常java.lang.NumberFormatException：对于输入字符串：“员工 ID”
    在 java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)
    在 java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
    在 java.base/java.lang.Double.parseDouble(Double.java:651)
    在 org.datavec.api.writable.Text.toDouble(Text.java:590)
    在 org.datavec.api.util.ndarray.RecordConverter.toMinibatchArray(RecordConverter.java:207)
    在 org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next（RecordReaderMultiDataSetIterator.java:153）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:346）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:421）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:53)
    在 com.test.dl4jtest.dl4jtest.TrainModel.main(TrainModel.java:46)

请建议我如何创建一个模型，该模型应在此处考虑工人、同事、雇员等术语。]]></description>
      <guid>https://stackoverflow.com/questions/78303940/deeplearning4j-creating-a-model-for-strings-employee-worker-associate-etc</guid>
      <pubDate>Wed, 10 Apr 2024 11:01:36 GMT</pubDate>
    </item>
    <item>
      <title>计算参考摘要和预测摘要之间的 ROGUE 分数</title>
      <link>https://stackoverflow.com/questions/78303659/calculating-rogue-score-between-reference-summary-and-prediction-summary</link>
      <description><![CDATA[我正在尝试计算 AI 生成的摘要和人类编写的摘要之间的 ROGUE 分数，如下所述。我对两者的摘要长度将超过 50 个标记。这是我的 python 代码。
pip install rouge_score
从 rouge_score 导入 rouge_scorer

预测=“我是ABC。我已经在 XYZ 大学完成了计算机应用学士学位，目前正在通过远程教育攻读计算机应用硕士学位。”

引用=“我是ABC。我已经在 XYZ 完成了为期四年的 PC 应用认证，目前正在通过远程培训攻读 PC 应用研究生学位。”


记分器 = rouge_scorer.RougeScorer([&#39;rouge1&#39;, &#39;rouge2&#39;, &#39;rougeL&#39;, &#39;rougeLsum&#39;])
分数 = Scorer.score(参考, 预测)
# 打印分数
对于分数的关键：
    print(f&#39;{key}: {scores[key]}&#39;)

输出：
rouge1：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

rouge2：分数（精度=0.3333333333333333，召回率=0.35714285714285715，fmeasure=0.3448275862068965）

rougeL：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

rougeLsum：得分（精度=0.6129032258064516，召回率=0.6551724137931034，fmeasure=0.6333333333333333）

我是这个领域的新手。因此，我不确定在我的用例中应考虑哪个 Rogue 分数及其重要性。]]></description>
      <guid>https://stackoverflow.com/questions/78303659/calculating-rogue-score-between-reference-summary-and-prediction-summary</guid>
      <pubDate>Wed, 10 Apr 2024 10:10:01 GMT</pubDate>
    </item>
    <item>
      <title>在Python中计算候选句子和参考句子之间的BLEU分数</title>
      <link>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</link>
      <description><![CDATA[我正在计算 2 个句子之间的 BLEU 分数，这两个句子看起来与我非常相似，但我得到的 BLEU 分数非常低。这应该发生吗？
预测=“我是ABC。”
参考=“我是ABC。”

从nltk.translate.bleu_score导入sent_bleu，corpus_bleu
从 nltk.translate.bleu_score 导入 SmoothingFunction
# 对句子进行标记
Prediction_tokens = Prediction.split()
Reference_tokens = Reference.split()
   
# 计算 BLEU 分数
bleu_score=sentence_bleu([reference_tokens],prediction_tokens,smoothing_function=SmoothingFunction().method4)

# 打印 BLEU 分数
print(f&quot;BLEU 分数: {bleu_score:.4f}&quot;)

输出为 0.0725
]]></description>
      <guid>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</guid>
      <pubDate>Wed, 10 Apr 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“deepface.commons”导入名称“距离”（/opt/anaconda3/envs/LIP/lib/python3.9/site-packages/deepface/commons/__init__.py）</title>
      <link>https://stackoverflow.com/questions/78298624/importerror-cannot-import-name-distance-from-deepface-commons-opt-anacond</link>
      <description><![CDATA[无法在Python中导入deepface
我目前在 macbook 上使用 Pycharm。
虚拟环境已激活，但仍出现以下错误。
谁能帮我一下吗？
Python 3.9
Deepface 0.0.89（最新）
张量流版本2.14.1
虚拟环境已激活。
一切都已安装，但仍无法解决导入错误。]]></description>
      <guid>https://stackoverflow.com/questions/78298624/importerror-cannot-import-name-distance-from-deepface-commons-opt-anacond</guid>
      <pubDate>Tue, 09 Apr 2024 12:57:54 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证模型训练是否正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在各个图表上绘制多个线性回归特征与预测结果</title>
      <link>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</link>
      <description><![CDATA[我正在研究一个电视广告数据集，该数据集具有 3 个特征（TV、radio、newspaper）和 1 个因变量 (销售）。通过使用多元线性回归，我估算了销售额并将其与 y_test 数据集上的实际值进行了比较。我已经确认我的模型获得了很高的 r2 分数。
我知道在多元线性回归中，多个特征可以在更高维度的图上可视化。但是，我希望在单独的图表上查看每个单独的特征与预测结果。
导入 pandas
导入numpy
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score
将 statsmodels.api 导入为 sm
将 matplotlib.pyplot 导入为 plt

#导入数据集并分配给X和y
df = pandas.read_csv(“广告.csv”)
df = df.drop(df.columns[[0]], 轴=1)
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].值

#将数据集分割成X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#初始化回归器并使用 X_train 和 y_train 对其进行训练
回归器=线性回归()
回归器.fit(X_train, y_train)

#预测测试结果并用实际值绘制它们
y_pred = 回归器.预测(X_test)
print(&quot;预测值/实际值&quot;)
print(numpy.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))

#评估预测结果的模型性能
r_squared_线性 = r2_score(y_test, y_pred)
print(f“多元线性回归模型性能为{r_squared_线性}”)

#查看OLS回归结果
mod = sm.OLS(y.reshape(-1, 1), X)
res = mod.fit()
打印（res.summary（））

#在各个图表上绘制数据集的每个特征与预测结果

这是我的代码，它可以正常工作，没有任何问题。我只想用 matplotlib 完成绘图部分。我想绘制每个单独的特征（TV、radio、newspaper）与我的模型具有的 y_pred 值预测，包括每个图上的线性回归线。
如果有人能向我展示如何使用 for 循环（以确保它适用于不同数据集上的任何特征计数），我将不胜感激。
另外，为了获得额外的知识，如何在同一图中绘制所有特征以及 y_pred 结果？]]></description>
      <guid>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</guid>
      <pubDate>Tue, 09 Apr 2024 07:56:36 GMT</pubDate>
    </item>
    <item>
      <title>人工智能揭示图像识别中的具体细节</title>
      <link>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition</link>
      <description><![CDATA[我正在尝试执行以下操作：
我想使用某种人工智能 API 来执行自动图像识别（其中“图像识别”是指将图像提供给人工智能，人工智能将根据我想要识别的图像细节生成文本输出），例如另一位用户在 stackoverflow 上发布了 ColaCola Can 帖子，但不同之处在于我的详细信息可能因两个选项而异。
我的问题是基于鸟类的性别识别，根据其身体的细节（例如，如果鸟的胸部是黄色的，那么它是雄性，否则它是雌性）。
我可以假设没有给模型提供任何棘手的图像，细节的照片是故意拍摄的，非常清晰，目的是促进该实验的模型，（我的意思是，重点关注该细节恒定光线充足的环境，变焦，聚焦等），并且输出必须类似于“男性”或“女性”。基于这种认识。
基本上，我需要以最简单、最不棘手的方式训练图像识别人工智能。
有没有简单的方法来执行它？]]></description>
      <guid>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition</guid>
      <pubDate>Mon, 08 Apr 2024 16:12:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么在启用 from_logits 的情况下使用 BinaryCrossEntropy 生成器损失？</title>
      <link>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</link>
      <description><![CDATA[从简单的普通 GAN 代码中，我查看  GitHub
我看到这个生成器模型具有激活sigmoid：
&lt;前&gt;&lt;代码&gt;# 生成器
G = tf.keras.models.Sequential([
  tf.keras.layers.Dense(28*28 // 2, input_shape = (z_dim,), 激活=&#39;relu&#39;),
  tf.keras.layers.Dense(28*28, 激活=&#39;sigmoid&#39;),
  tf.keras.layers.Reshape((28, 28))])

在启用 from_logits 的情况下，G 的损失定义如下：
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)
def G_loss(D, x_fake):
  返回 cross_entropy(tf.ones_like(D(x_fake)), D(x_fake))

据我所知，from_logits=True旨在使损失函数接受范围在-infinity到&lt;之间的y_pred值代码&gt;无穷大。与 from_logits=False 相反，损失函数假设值的范围在 0 到 1 之间。
如您所见，G 模型的输出层已经具有 sigmoid 激活，其范围在 0 到 1.
但是，为什么作者仍然使用 from_logits=True？]]></description>
      <guid>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</guid>
      <pubDate>Thu, 04 Apr 2024 18:03:32 GMT</pubDate>
    </item>
    <item>
      <title>对于负数与正数比率非常高的多标签分类，应使用哪些损失函数和指标？</title>
      <link>https://stackoverflow.com/questions/59336899/which-loss-function-and-metrics-to-use-for-multi-label-classification-with-very</link>
      <description><![CDATA[我正在训练一个多标签分类模型来检测衣服的属性。我在 Keras 中使用迁移学习，重新训练 vgg-19 模型的最后几层。
属性总数为 1000 个，其中约 99% 为 0。准确性、精确度、召回率等指标都失败了，因为模型可以预测全零，但仍然获得非常高的分数。二元交叉熵、汉明损失等，在损失函数的情况下还没有起作用。
我正在使用深度时尚数据集。 
那么，我可以使用哪些指标和损失函数来正确衡量我的模型？]]></description>
      <guid>https://stackoverflow.com/questions/59336899/which-loss-function-and-metrics-to-use-for-multi-label-classification-with-very</guid>
      <pubDate>Sat, 14 Dec 2019 16:15:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 Docker 和 Flask 进行机器学习的性能问题</title>
      <link>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</link>
      <description><![CDATA[我有一些应用于 json 文件的 python3 代码，代码中有一些神经网络和随机森林。我将代码放入 Docker 容器中，但注意到这些 ML 任务在不使用 Docker 的情况下比使用 Docker 运行得更快。在 Docker 中，我使用 Flask 加载 json 文件并运行代码。当然，我在本地和 Docker 内部使用了相同版本的 python 模块，这些是：

theano 0.8.2
keras 2.0.5
scikit-learn 0.19.0

另外，Flask 是

0.12

起初，我认为 theano 在有 Docker 的情况下可能会使用不同的资源，但它同时运行单 CPU 和单线程。它也没有使用我的 GPU。当我意识到我的随机森林在 Docker 中运行速度也变慢时，我意识到这可能不是 theano。以下是我执行的一系列测试（我对每个测试进行了多次测试，我报告了平均时间，因为这些测试是稳定的）
没有 Docker，没有 Flask：

任务 1（theano + keras 代码）：1.0s 
任务 2（theano + keras 代码）：0.7s
任务 3（scikit-learn 代码）：0.25 秒

Docker (cpus=1) + Flask (调试模式 = True):

T1：6.5秒
T2：2.2秒
T3：0.58s

Docker (cpus=2) + Flask (调试模式 = True):

T1：5.5秒
T2：1.4秒
T3：0.55s

Docker (cpus=2) + Flask (调试模式 = False)：

T1：4.5秒
T2：1.2秒
T3：0.5秒

Docker (cpus=2)（无 Flask，仅调用本地完成的 json 文件）：

T1：2.8s
T2：1.1秒
T3：0.5秒

Flask（调试模式 = True）（无 Docker 容器）：

T1：2.8s
T2：1.5秒
T3：0.2秒

我猜 cpu=1 与 cpu=2 只是将一个 cpu 分配给代码，而第二个 cpu 只是接管一些其他工作。显然，当不使用 Flask 或 Docker 时，时间会有所减少，但仍然无法达到没有 Docker 和 Flask 的速度。有谁猜到为什么会发生这种情况吗？
这是我们如何使用 Flask 运行应用程序的最小代码块
api = Flask(__name__)
pipeline = Pipeline() # 调用多个任务的私有类

@api.route(&quot;/&quot;,methods=[&#39;POST&#39;])
def 条目():
    数据 = request.get_json(force=True)
    数据 = pipeline.process(数据)
    # 这会调用不同的定时任务

如果 __name__ == &quot;__main__&quot;:
    api.run（调试= True，主机=&#39;0.0.0.0&#39;，线程= False）


PS。如果问题缺少任何内容，请原谅我，这是我的第一个 StackOverflow 问题]]></description>
      <guid>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</guid>
      <pubDate>Tue, 22 May 2018 09:48:58 GMT</pubDate>
    </item>
    </channel>
</rss>