<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 21 Jun 2024 06:21:42 GMT</lastBuildDate>
    <item>
      <title>如何知道是否有办法改进这个模型，以及如何知道我是否达到了特定模型的限制</title>
      <link>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</link>
      <description><![CDATA[我是数据科学领域的新手，所以当我建立模型时，我不确定我是否已经到达终点，而且我也不知道我还能如何改进模型。这是随机森林分类器，我使用 RandomizeSearchCV 作为参数，最终得到了最高的 73%。此外，我还使用了 class_weight 来平衡类别，我缩放了所有内容并清理了数据，正如您将看到的。我不确定这是否是最好的方法，也不知道在制作模型时 73% 是否足够好，也不知道我是否达到了随机森林分类器的极限。
从 matplotlib 导入 pyplot 作为 plt
从 matplotlib.colors 导入 ListedColormap
导入 pandas 作为 pd
导入 numpy 作为 np
从 sklearn.compose 导入 ColumnTransformer
从 sklearn.model_selection 导入 RandomizedSearchCV、train_test_split
从 sklearn.preprocessing 导入 OneHotEncoder、StandardScaler、LabelEncoder、MinMaxScaler、MaxAbsScaler、RobustScaler
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 classes_report、accuracy_score、confusion_matrix
从 imblearn.over_sampling 导入 SMOTE

pd.set_option(&#39;display.max_rows&#39;, None)
pd.set_option(&#39;display.max_columns&#39;, None)
df = pd.read_csv(&#39;pokemon_data.csv&#39;)

duplicates = df.duplicated()
if duplicates.any():
print(df[duplicates], &quot;DUPLICATE&quot;)
else:
print(&quot;NO DUPLICATES&quot;)

class_dist = df[&#39;type1&#39;].value_counts()
print(class_dist)

df.replace(&#39;—&#39;, np.nan, inplace=True)
numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())
# 现在我们填充分类缺失数据列
categorical_cols = df.select_dtypes(include=&#39;object&#39;).columns
df[categorical_cols] = df[categorical_cols].fillna(&#39;Unknown&#39;)

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), [&#39;dexnum&#39;]),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

features = [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;,
&#39;egg_group2&#39;]

X = df[features] # 我们用来预测的变量 
y_type1 = df[&#39;type1&#39;] # 我们预测的内容
y_type2 = df[&#39;type2&#39;] # 我们预测什么

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), []),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

# Train_test_split 用于将数据集拆分为两个子集，即训练集和测试集
X_train, X_test, y_type1_train, y_type1_test = train_test_split(X, y_type1, test_size=0.4, random_state=42)

X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

param_dist = {
&#39;n_estimators&#39;: [9000],
&#39;max_depth&#39;: [1000],
&#39;min_samples_split&#39;: [2],
&#39;min_samples_leaf&#39;: [1],
&#39;max_features&#39;: [&#39;log2&#39;],
&#39;bootstrap&#39;: [False]
}

rf_type1 = RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=42)
random_search = RandomizedSearchCV(estimator=rf_type1, param_distributions=param_dist, n_iter=50, cv=5, verbose=2, random_state=42, n_jobs=-1)
random_search.fit(X_train, y_type1_train)

# 获取最佳参数
print(&quot;找到最佳参数：&quot;, random_search.best_params_)
print(&quot;最佳准确率： &quot;, random_search.best_score_)

# 使用最佳参数重新训练模型
best_rf = random_search.best_estimator_
y_type1_pred_best = best_rf.predict(X_test)

# 评估模型
print(&quot;具有最佳参数的 Type1 分类报告：&quot;)
print(classification_report(y_type1_test, y_type1_pred_best))
print(&quot;具有最佳参数的 Type1 准确率：&quot;, accuracy_score(y_type1_test, y_type1_pred_best))

]]></description>
      <guid>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</guid>
      <pubDate>Fri, 21 Jun 2024 06:07:40 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮助我在使用 pytorch 时最大限度地提高 GPU 利用率吗？</title>
      <link>https://stackoverflow.com/questions/78650444/can-anybody-help-me-out-with-maximizing-gpu-utilization-when-using-pytorch</link>
      <description><![CDATA[我目前使用的是带有 7950X CPU 的 RTX 4090。我的目标是在表格数据上运行相对简单的模型时最大限度地提高 GPU 利用率。该模型的参数少于 100 万个，数据形状为 (5,000,000, 120)。当我训练模型时，只有 18% 的 GPU 被利用，完成训练大约需要 3 个小时。
主要问题是，如果我能以某种方式利用 90% 的 GPU，训练时间将显著减少，可能减少到当前时间的五分之一，这将为我节省大量时间。
我尝试了各种解决方案，例如调整批处理大小、增加模型的复杂性以及更改 DataLoader 的 num_workers，但这些都没有起到很好的作用。无论我如何调整，GPU 负载仍然在 10-15% 左右。这真是令人沮丧。
由此，我想到了使用多处理的想法。由于我使用的是单个 GPU，并且单个模型仅使用 18%，因此我仍有空间运行另​​外四个模型。我认为同时运行五个不同的模型可以将 GPU 利用率提高到 100% 左右，从而节省大量时间。但是，当我尝试使用 PyTorch 的多处理时，结果并不理想。
有人能帮我解决这个问题吗，或者我的想法在 PyTorch 中不可行？
我尝试过的方法

将批次大小从 64 增加到 4096、40962、40964
使用 num_workers (2,4,8)
向模型添加更多层
使用 pytorch.multiprocessing
]]></description>
      <guid>https://stackoverflow.com/questions/78650444/can-anybody-help-me-out-with-maximizing-gpu-utilization-when-using-pytorch</guid>
      <pubDate>Fri, 21 Jun 2024 04:50:59 GMT</pubDate>
    </item>
    <item>
      <title>通过几个步骤优化一个过程：如果我们使用一个模型几次才能够计算出损失，那么如何训练它？</title>
      <link>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</link>
      <description><![CDATA[我正在做一个项目，其中有一个包含一定步骤的流程；假设是 3 个步骤。

我们从一个初始化的零矩阵 M0 开始，该矩阵描述了系统的状态，并且必须采取一个行动，其后果是随机的，但受到该行动的强烈影响
我们更新矩阵 (M1)，然后再次采取行动
我们再次更新矩阵 (M2)，采取最后一个行动
现在我们才可以从最后一个矩阵 M3 计算损失，因此我们可以评估我们的策略

我已经建立了一个神经网络，其中状态矩阵是输入，输出是决定采取哪种行动的权重列表。从我（初学者）的理解来看，整个过程有点像循环神经网络，但有额外的步骤。
我用 Python 实现了这个过程，但当我尝试训练模型时，GradientTape() 看起来不太好，因为我的变量从未计算过梯度；我不确定，但我认为损失的随机性使得梯度不可计算（计算权重的损失远非易事，因为权重放在图的边缘，我们对其执行算法）。
我曾想过在没有权重列表的情况下进行强化学习以采取行动，但我不知道奖励的随机性会有多好。此外，我从来没有做过这样的事情，我只有一周的时间来实现一切。动作空间也相当大，因此这种方法存在很多不确定性。
有没有什么常见的做法来解决这类问题？
谢谢！
最好]]></description>
      <guid>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</guid>
      <pubDate>Fri, 21 Jun 2024 00:18:44 GMT</pubDate>
    </item>
    <item>
      <title>如何去除车窗玻璃 bg？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</link>
      <description><![CDATA[我正在使用 rembg 去除背景，但问题是车窗玻璃的背景没有被去除。在这种情况下，如何去除车窗玻璃的背景。有人能帮帮我吗？
--&gt;当前结果图像
--&gt;预期结果图像
output_data = rembg.remove(input_data)
]]></description>
      <guid>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</guid>
      <pubDate>Thu, 20 Jun 2024 23:37:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在 keras 中加载自定义模型时会出现 TypeError</title>
      <link>https://stackoverflow.com/questions/78649700/why-am-i-getting-a-typeerror-while-loading-a-custom-model-in-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78649700/why-am-i-getting-a-typeerror-while-loading-a-custom-model-in-keras</guid>
      <pubDate>Thu, 20 Jun 2024 21:45:02 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归器输出</title>
      <link>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</link>
      <description><![CDATA[我有一个非常简单的数据集，其中员工年龄和工作年限作为特征，收入作为标签。要求使用各种回归器预测收入水平，我使用了 4 个：决策树 (DT)、随机森林 (RF)、K-最近邻 (KNN) 和线性回归 (LR)。下表给出了 4 个回归器预测的收入水平。
我的问题是：

为什么 DT 预测的收入水平如此严格（几乎等于数据集的实际收入），而其他回归器似乎运行良好？是因为数据太小，DT 无法训练 / 特征数量太少（只有 2 个特征）还是其他原因？
为什么 DT 给出的结果很荒谬，而 RF 却没有？
DT 的 R^2 和 MSE 分别小于和大于 LR，但 DT 似乎仍然产生了更窄的范围。怎么做？



这里不包括任何代码，因为这更像是一个上下文问题。如果需要，可以提供代码。]]></description>
      <guid>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</guid>
      <pubDate>Thu, 20 Jun 2024 20:59:14 GMT</pubDate>
    </item>
    <item>
      <title>具有二元结果预测的单一时间序列</title>
      <link>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</link>
      <description><![CDATA[我有全天股票价格的时间序列和与这些价格相关的二元结果变量（行动），即买入/无行动。
目标是预测股票价格和行动变量之间是否存在相关性。
不确定逻辑回归或决策树是否是可行的方法。你认为哪一个更有意义？
考虑到没有太多节点可以使用，决策树是否可行？
我认为随机森林会有点矫枉过正。
仅供参考，我不熟悉 NN。
请就对此类数据建模的最佳方法提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</guid>
      <pubDate>Thu, 20 Jun 2024 16:33:58 GMT</pubDate>
    </item>
    <item>
      <title>可从头定制的神经网络</title>
      <link>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</link>
      <description><![CDATA[我从头开始创建了一个神经网络。仅使用 numpy 库。但损失函数没有收敛，并且准确率不稳定。请帮助我找出问题所在。如果您能给我解决方案的话
这是我的代码
# 计算分类器准确率的函数
def accuracy(output, y):
a = np.argmax(output, axis=0)

return np.mean(a == y)

# 返回初始权重和基准的函数
def initialisation(t):
parameters = {}
np.random.seed(1)
for i in range(len(t) - 1):
parameters[&#39;w&#39; + str(i + 1)] = np.random.randn(t[i + 1], t[i])
parameters[&#39;b&#39; + str(i + 1)] = np.zeros((t[i + 1], 1))
返回参数

def relu_activation(x):
返回 np.maximum(0, x)

def relu_derivative(x):
返回 np.where(x &gt; 0, 1, 0)

def softmax_activation(z):
z = z - np.max(z, axis=0, keepdims=True)
z = np.exp(z)
softmax = z / np.sum(z, axis=0, keepdims=True)
返回 softmax

def sigmoid_activation(x):
返回 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
返回 np.exp(-x) / (1 + np.exp(-x)) ** 2

def derive(x, num):
if num == 1:
return relu_derivative(x)
elif num == 3:
return sigmoid_derivative(x)

#根据数字 n 选择激活函数的函数
def activate(z, n):
if n == 1:
return relu_activation(z)
elif n == 2:
return softmax_activation(z)
elif n == 3:
return sigmoid_activation(z)

#计算损失函数的函数
def loss_function(a_last, y):
output = np.clip(a_last, 1e-7, 1 - 1e-7)
num = y.shape[1]
loss = -np.mean(np.log(output[np.argmax(y, axis=0), np.arange(num)]))
return loss

def Z(x, w, b):
return np.dot(w, x) + b

#实现前向传播的函数
def forward(x, parameters, configuration, t):
cache = {&#39;a0&#39;: x}
for i in range(len(t) - 1):
cache[&#39;z&#39; + str(i + 1)] = Z(cache[&#39;a&#39; + str(i)], parameters[&#39;w&#39; + str(i + 1)], parameters[&#39;b&#39; + str(i + 1)])
cache[&#39;a&#39; + str(i + 1)] = activate(cache[&#39;z&#39; + str(i + 1)], configuration[i])
return cache

#函数实现反向传播
def behind(cache, t, configuration, parameters, y):
m = cache[&#39;a&#39; + str(len(t) - 1)].shape[1]
grad = {&#39;dw&#39; + str(len(t) - 1): np.dot((cache[&#39;a&#39; + str(len(t) - 1)] - y), cache[&#39;a&#39; + str(len(t) - 2)].T) / m,
&#39;db&#39; + str(len(t) - 1): np.sum(cache[&#39;a&#39; + str(len(t) - 1)] - y, axis=1, keepdims=True) / m,
&#39;dz&#39; + str(len(t) - 1): cache[&#39;a&#39; + str(len(t) - 1)] - y}

for i in range(len(t) - 2, 0, -1):
grad[&#39;dz&#39; + str(i)] = np.dot(parameters[&#39;w&#39; + str(i + 1)].T, grad[&#39;dz&#39; + str(i + 1)] * derive(cache[&#39;z&#39; + str(i + 1)], configuration[i - 1]))
grad[&#39;dw&#39; + str(i)] = np.dot(grad[&#39;dz&#39; + str(i)], cache[&#39;a&#39; + str(i - 1)].T) / m
grad[&#39;db&#39; + str(i)] = np.sum(grad[&#39;dz&#39; + str(i)], axis=1, keepdims=True) / m

return grad

# 更新权重和基准的函数
def update(learning_rate, parameters, grad, t):
for i in range(len(t) - 1):
参数[&#39;w&#39; + str(i + 1)] = np.clip(参数[&#39;w&#39; + str(i + 1)] - learning_rate * grad[&#39;dw&#39; + str(i + 1)], -10, 10)
参数[&#39;b&#39; + str(i + 1)] = np.clip(参数[&#39;b&#39; + str(i + 1)] - learning_rate * grad[&#39;db&#39; + str(i + 1)], -10, 10)
返回参数

#学习函数
def learning(x, y, t, configuration, nb, learning_rate):
param = initialisation(t)
los = []
for i in tqdm.tqdm(range(nb)):
cache = forward(x, param, configuration, t)
grad = behind(cache, t,配置，参数，y)
损失 = loss_function(cache[&#39;a&#39; + str(len(t) - 1)], y)
参数 = update(learning_rate, 参数，grad，t)
los = np.append(los, loss)
如果 i % (nb // 20) == 0:
打印(f&quot;loss={loss}, accuracy={100 * accuracy(cache[&#39;a&#39; + str(len(t) - 1)], y)}%&quot;)
plt.plot(np.arange(nb), los)
打印(np.argmax(cache[&#39;a&#39; + str(len(t) - 1)], axis=0), np.argmax(y, axis=0))
返回参数

def predict(x, 参数，配置，t):
cache = forward(x, 参数，配置，t)
返回cache[&#39;a&#39; + str(len(t) - 1)]
import tensorflow as tf
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
#了解训练数据的形状
x_train.shape
x=x_train.reshape(60000,784).T/255.0
y=y_train.reshape(-1,60000)
t=np.array([x.shape[0],128,64,10])
configuration=np.array([1,1,2])
arr_1d=y

arr_2d = np.zeros((10,60000))

#将相应的列索引设置为 1
for i in范围（len（arr_1d））：
arr_2d[y[0, i], i] = 1

para=learning(x,arr_2d,t,configuration,2000,0.0001)
]]></description>
      <guid>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</guid>
      <pubDate>Thu, 20 Jun 2024 16:22:18 GMT</pubDate>
    </item>
    <item>
      <title>超分辨率 GAN 训练中生成器和鉴别器损失之间的不平衡</title>
      <link>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</guid>
      <pubDate>Thu, 20 Jun 2024 12:57:24 GMT</pubDate>
    </item>
    <item>
      <title>所有时期的损失和准确率相同</title>
      <link>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</guid>
      <pubDate>Thu, 20 Jun 2024 06:01:17 GMT</pubDate>
    </item>
    <item>
      <title>CUDAError：使用 Gymnasium 的 RL 环境内存不足</title>
      <link>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</guid>
      <pubDate>Thu, 20 Jun 2024 04:14:28 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1
我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot; 变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据在每个结果类别（即阳性/阴性 toxo）中的 n 值是平衡的
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

这给出了以下结果：
(训练)
阴性 阳性 
75.75758 24.24242

和
(测试)
阴性 阳性 
54.54545 45.45455

检查缺失值
anyNA(data_hum_mod)
#确保所有变量都不为零或接近零方差。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类别的子采样（这里是负面结果），如何在 R 中实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>确定哪些变量对 FDA 贡献最大 [关闭]</title>
      <link>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</guid>
      <pubDate>Wed, 19 Jun 2024 20:46:16 GMT</pubDate>
    </item>
    <item>
      <title>是否可以训练神经网络来输入随机森林分类器或任何其他类型的分类器（如 XGBoost 或决策树）？</title>
      <link>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</link>
      <description><![CDATA[我想创建一个模型架构来预测未来的股价走势，如下所示：

该模型的目标是预测未来 3 个月内价格是上涨还是下跌。
我尝试过一些模型，例如 Logistic 回归、神经网络、XGBoost 等。我得到了一些不错的结果。通过使用随机森林分类器，我得到了迄今为​​止最好的结果。我如何使用神经网络对数据进行编码，然后将这些值传递给随机森林分类器进行分类，而不是使用如图所示的使用 S 形函数的最终输出层（使用 Python、Keras 和 SKlearn）。
我对 Keras 不是很熟悉，所以我想知道是否有可能训练一个输入到单独分类器的神经网络，如果可以，该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</guid>
      <pubDate>Fri, 10 May 2024 17:53:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pyspark 上创建分层分割训练、验证和测试集？</title>
      <link>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</link>
      <description><![CDATA[我有一个小数据集（140K），我想将其分成验证集、验证集测试集，使用目标变量和另一个字段来限制这些分割。]]></description>
      <guid>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</guid>
      <pubDate>Thu, 19 Sep 2019 15:45:28 GMT</pubDate>
    </item>
    </channel>
</rss>