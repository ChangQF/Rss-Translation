<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 10 Mar 2024 15:12:22 GMT</lastBuildDate>
    <item>
      <title>类型错误：拟合模型时“str”对象不可调用</title>
      <link>https://stackoverflow.com/questions/78135865/typeerror-str-object-is-not-callable-while-fitting-model</link>
      <description><![CDATA[出现此错误：
类型错误：“str”对象不可调用

导入tensorflow为tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
将 pandas 导入为 pd
将tensorflow_datasets导入为tf_ds
从 sklearn.model_selection 导入 train_test_split
df,df_info=tf_ds.load(&#39;疟疾&#39;,with_info=True,shuffle_files=True,split=&#39;train&#39;,as_supervised=True)
def split(数据集,train_ratio,val_ratio,test_ratio):
  df_大小 = len(df)
  train_df = df.take(int(df_size*train_ratio))
  val_test_df = df.skip(int(df_size*train_ratio))
  val_df = val_test_df.take(int(val_ratio*df_size))
  test_df = val_test_df.skip(int(val_ratio*df_size))
  返回train_df,test_df,val_df
训练比率=0.70
val_ratio =0.15
测试比率 = 0.15
train_df , val_df,test_df = split(df,train_ratio,val_ratio,test_ratio)

def resizer（图像，标签）：
  返回 tf.image.resize(图像,(224,224))/255.0,标签

train_df = train_df.map(lambda x, y: resizer(x, y))
train_df=train_df.shuf

fle(buffer_size =8, reshuffle_each_iteration =True).batch(32).prefetch(tf.data.AUTOTUNE)
模型 = tf.keras.Sequential(tf.keras.layers.InputLayer(input_shape=(224,224,3)))
model.add(tf.keras.layers.Conv2D(kernel_size=(3,3),filters=5,padding=&#39;valid&#39;,strides=(1,1)))
model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),padding=&#39;有效&#39;,strides=(1,1)))
model.add(tf.keras.layers.Conv2D(kernel_size=(3,3),filters=5,padding=&#39;valid&#39;,strides=(1,1)))
model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),padding=&#39;有效&#39;,strides=(1,1)))
model.add(tf.keras.layers.Conv2D(kernel_size=(3,3),filters=10,padding=&#39;valid&#39;,strides=(1,1)))
model.add(tf.keras.layers.Flatten())


model.add(tf.keras.layers.Dense(643,activation=&#39;relu&#39;))
model.add(tf.keras.layers.Dropout(0.2))

model.add(tf.keras.layers.Dense(100,activation=&#39;relu&#39;))
model.add(tf.keras.layers.Dropout(0.2))

model.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;))
model.compile(optimizer=&#39;Adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy,val_accuracy&#39;])

model.fit(train_df,validation_data=val_df,epochs=10)

错误在最后一行]]></description>
      <guid>https://stackoverflow.com/questions/78135865/typeerror-str-object-is-not-callable-while-fitting-model</guid>
      <pubDate>Sun, 10 Mar 2024 12:30:12 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：未知的损失函数：categorical_cross entropy [关闭]</title>
      <link>https://stackoverflow.com/questions/78135637/valueerror-unknown-loss-function-categorical-cross-entropy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78135637/valueerror-unknown-loss-function-categorical-cross-entropy</guid>
      <pubDate>Sun, 10 Mar 2024 11:12:18 GMT</pubDate>
    </item>
    <item>
      <title>视觉中的掩码自动编码器中的解码器需要什么单独的位置嵌入[关闭]</title>
      <link>https://stackoverflow.com/questions/78135553/what-is-the-need-of-separate-position-embeddings-for-a-decoder-in-masked-auto-en</link>
      <description><![CDATA[我正在尝试了解 MAE 架构。在 MAE 架构中，我们有编码器位置嵌入和解码器位置嵌入。我知道这些可以是固定的（恒定的）或可以学习的。如果它们是恒定的，则解码器和编码器的尺寸相同，并且位置嵌入不需要梯度-&gt;为什么我们需要为解码器进行单独的位置嵌入？例如-&gt;您获取编码补丁，然后首先根据掩码和 ids_restore -&gt; 进行掩码。您对已被屏蔽的 ID 有所了解。您可以简单地获取这些 id 处嵌入张量值的位置并将其相加。然后在解码器中-&gt;因为我们 unshuffle 提供了屏蔽和未屏蔽补丁的整个长度，所以我们可以简单地重用之前的 who 位置 id。我不明白为什么需要初始化单独的解码器位置嵌入。
# 编码器
self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
num_patches = self.patch_embed.num_patches
self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), require_grad=False) # 固定 sin-cos 嵌入


# 解码器
self.decoder_embed = nn.Linear（embed_dim，decoder_embed_dim，偏差= True）
self.mask_token = nn.Parameter(torch.zeros(1, 1,decoder_embed_dim))
self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, detector_embed_dim), require_grad=False) # 固定 sin-cos 嵌入

在上面的代码中，我想了解编码器和解码器具有不同 pos_embed 的原因是否是为了适应并为两个不同的嵌入暗淡提供灵活性，或者是否有更多意义。上面的代码来自 git 上的 MAE 存储库]]></description>
      <guid>https://stackoverflow.com/questions/78135553/what-is-the-need-of-separate-position-embeddings-for-a-decoder-in-masked-auto-en</guid>
      <pubDate>Sun, 10 Mar 2024 10:41:48 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 OpenCV 中创建大型模型而不需要大量 RAM？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78135277/is-it-possible-to-creat-a-large-model-in-opencv-without-needing-redicouls-amount</link>
      <description><![CDATA[我正在尝试在 OpenCV 中创建一个与人脸检测相关的模型。我拥有的数据集大约有 50,000 张图像。然而，当我尝试使用大量数据训练模型时，内存使用量就会大幅上升。即使在尝试批处理和卸载加载的 cv::mat 图像时，模型的大小也会增加（我想这是可以预料的。）。我还尝试将每个图像的大小调整为 128x128 并使用灰度，但这似乎也没有多大帮助。
是否可以在 OpenCV 中创建一个非常大的模型（包含 50,000 张图像），而不需要 300GB 的 RAM？]]></description>
      <guid>https://stackoverflow.com/questions/78135277/is-it-possible-to-creat-a-large-model-in-opencv-without-needing-redicouls-amount</guid>
      <pubDate>Sun, 10 Mar 2024 09:03:23 GMT</pubDate>
    </item>
    <item>
      <title>多代理深度 q 学习深入研究开发状态 [关闭]</title>
      <link>https://stackoverflow.com/questions/78134963/multi-agent-deep-q-learning-takes-a-dive-at-exploitation-state</link>
      <description><![CDATA[我正在使用 double &amp;深度 Q 学习对决。达到 epsilon 0.01 后不久，奖励开始走下坡路。我正在尝试不同的超参数，但仍然得到这样的结果。
问题如下
我有很多接收任务的服务器，它们可以在本地处理任务，也可以将其卸载到其他服务器或云上。它与这篇论文类似，但有一些修改。但本质非常相似。
我尝试了学习率、epsilon 递减和episode 的多种组合，但不断看到这样的结果。
]]></description>
      <guid>https://stackoverflow.com/questions/78134963/multi-agent-deep-q-learning-takes-a-dive-at-exploitation-state</guid>
      <pubDate>Sun, 10 Mar 2024 06:50:38 GMT</pubDate>
    </item>
    <item>
      <title>ReLU 激活的拟合模型函数出现错误</title>
      <link>https://stackoverflow.com/questions/78134915/error-in-fit-model-function-for-relu-activation</link>
      <description><![CDATA[我尝试过，但收到此错误：
&lt;前&gt;&lt;代码&gt;纪元 1/5
回溯（最近一次调用最后一次）：
  文件“d:\University\Sem6\CV\project\project.py”，第 580 行，在  中
    model.fit（train_augmented，steps_per_epoch=train_steps，validation_data=val_augmented，validation_steps=valid_steps，epochs=epochs）
  文件“D:\University\Sem6\CV\project\myenv\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 70 行，在 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“D:\University\Sem6\CV\project\myenv\Lib\site-packages\tensorflow\python\eager\execute.py”，第 53 行，quick_execute
    张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError：图形执行错误：
在定义于（最近一次调用最后）的节点 inception_v1/dense_3/Relu 处检测到：
  文件“d:\University\Sem6\CV\project\project.py”，第 580 行，在  中
矩阵大小不兼容：In[0]：[32,2048]，In[1]：[512,1024]
         [[{{节点 inception_v1/dense_3/Relu}}]] [操作：__inference_train_function_16627]

我的代码：
batch_size = 32 # 根据您的系统功能调整此值
train_steps = len(train_images)
valid_steps = len(val_images)
model.fit（train_augmented，steps_per_epoch=train_steps，validation_data=val_augmented，validation_steps=valid_steps，epochs=epochs）


如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78134915/error-in-fit-model-function-for-relu-activation</guid>
      <pubDate>Sun, 10 Mar 2024 06:27:30 GMT</pubDate>
    </item>
    <item>
      <title>人脸识别中如何处理规格眩光？</title>
      <link>https://stackoverflow.com/questions/78134869/how-to-handle-specs-glare-in-face-recognition</link>
      <description><![CDATA[我想构建一个基于 python Flask api 的应用程序，我想发送 2 个人物图像（脸部图像）的路径（url 或本地图像），并且如果两张脸都匹配（两张图片都属于），我希望得到“是”的响应同一个人），如果图像是不同的人，则不会。我为此使用了face_recognition 库。在我介绍戴眼镜的人的照片之前，这个算法一直运行良好。该模型无法识别眼镜中是否有眩光。如何进行？有没有一种预处理方法可以提供帮助？或者我应该搬到另一个图书馆？
导入face_recognition
导入CV2
将 numpy 导入为 np
从日期时间导入日期时间
从烧瓶导入烧瓶，请求，jsonify
导入人脸识别
导入请求
从 io 导入 BytesIO
从日期时间导入日期时间
从 PIL 导入图像
应用程序=烧瓶（__名称__）

def preprocess_image(image_path,resize_height=400):
    if image_path.startswith((&#39;http://&#39;, &#39;https://&#39;)):
            响应 = requests.get(图像路径)
            图像 = Image.open(BytesIO(响应.内容))
    别的：
        图像 = Image.open(图像路径)
    exif_data = image.getexif()

    如果 exif_数据：
        如果 exif_data.get(274) == 6:
            图像 = 图像.旋转(270)
    宽度，长度=图像大小
    比例 = resize_height / 长度
    new_width = int(宽度 * 比例)
    打印（图像路径，图像大小，比例，新宽度）
    
    new_width = int(宽度 * 比例)
    resized_image = image.resize((new_width, resize_height))

    rgb_image = resized_image.convert(“RGB”)
    numpy_image = np.array(rgb_image)

    返回numpy_image


@app.route(&#39;/&#39;,methods = [&#39;GET&#39;,&#39;POST&#39;])
def home():
    返回“服务器正在运行”
@app.route(&#39;/compare&#39;,methods = [&#39;GET&#39;,&#39;POST&#39;])
defface_compare():
    st = 日期时间.now()
    数据 = request.get_json()
    image_path1 = 数据[&#39;img1&#39;]
    image_path2 = 数据[&#39;img2&#39;]

    已处理图像1 = 预处理图像(图像路径1)
    已处理图像2 = 预处理图像(图像路径2)
    
    面部位置1 = 面部识别.面部位置（处理后的图像1）
    面部位置2 = 面部识别.面部位置(processed_image2)
    #show_bounding_box(face_locations1,processed_image1)
    #show_bounding_box(face_locations2,processed_image2)
    如果不是face_locations1：
        return jsonify({&#39;result&#39;: &#39;未找到面 1&#39;, &#39;time&#39;:str(datetime.now()-st)})
    如果不是face_locations2：
        return jsonify({&#39;result&#39;: &#39;未找到人脸 2&#39;, &#39;time&#39;:str(datetime.now()-st)})

    face_encoding1=face_recognition.face_encodings(processed_image1,known_face_locations=face_locations1,model=&#39;小&#39;)[0]
    face_encoding2=face_recognition.face_encodings(processed_image2,known_face_locations=face_locations2,model=&#39;小&#39;)[0]
    
    结果=face_recognition.compare_faces([face_encoding1],face_encoding2,公差=0.47)
    return jsonify({&#39;结果&#39;: str(结果[0]), &#39;时间&#39;:str(datetime.now()-st)})

def show_bounding_box(face_loc, img):
    边界框 = []
    对于face_loc中的face_location：
        上、右、下、左 = 面部位置
        bounding_boxes.append((左、上、右、下))
    对于bounding_boxes中的（左，上，右，下）：
        cv2.矩形(img, (左, 上), (右, 下), (0, 255, 0), 2)
    cv2.imshow(“带有边界框的面孔”, img)
    cv2.waitKey(0)

如果 __name__ == &#39;__main__&#39;:
    app.run（调试=True，线程=True）
]]></description>
      <guid>https://stackoverflow.com/questions/78134869/how-to-handle-specs-glare-in-face-recognition</guid>
      <pubDate>Sun, 10 Mar 2024 06:01:06 GMT</pubDate>
    </item>
    <item>
      <title>如何访问CoNull 2003和OntoNotes数据集？</title>
      <link>https://stackoverflow.com/questions/78134821/how-to-access-the-conull-2003-and-ontonotes-datasets</link>
      <description><![CDATA[我发现了某些遗留数据集，例如 CoNull 2003 和 OntoNotes。我如何访问这些数据集并在我的项目中使用它们？他们各自的网站显示了很多我不知道的信息。请帮忙
我正在尝试启动该项目。需要建议]]></description>
      <guid>https://stackoverflow.com/questions/78134821/how-to-access-the-conull-2003-and-ontonotes-datasets</guid>
      <pubDate>Sun, 10 Mar 2024 05:34:51 GMT</pubDate>
    </item>
    <item>
      <title>尝试将标签设置为张量时出现值错误</title>
      <link>https://stackoverflow.com/questions/78134521/value-error-when-trying-to-make-labels-to-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78134521/value-error-when-trying-to-make-labels-to-tensor</guid>
      <pubDate>Sun, 10 Mar 2024 02:21:05 GMT</pubDate>
    </item>
    <item>
      <title>如何以高精度（+ 90%）对面部特征嵌入进行分类。我可以在 svm 模型中进行哪些调整来对 20 多个类别进行分类</title>
      <link>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</link>
      <description><![CDATA[我使用facenet提取特征并使用svm进行分类。效果很好，但 20 堂课后，准确率下降到 75%。如何在利用 GPU 的同时优化 svm。
我使用了这个 svm 类模型
scikit learn 的 svm 模型不使用 GPU，所以我使用了这个
类 SVM(nn.Module):
    def __init__(自身):
        超级（SVM，自我）.__init__()
        self.fc = nn.Linear(X.shape[1], len(ClassList))

    def 前向（自身，x）：
        返回 self.fc(x)

但是对于 20 多个类别来说，这个准确率非常低
我也尝试过使用这个：
类 SoftmaxUsed(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.layers = nn.Sequential(nn.Linear(512, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.线性(1024, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1024, len(ClassList)),
                                 nn.LogSoftmax(dim=1))
    def 前向（自身，x）：
        返回 self.layers(x)

但准确率最高仍为 86%]]></description>
      <guid>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</guid>
      <pubDate>Sat, 09 Mar 2024 18:56:09 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降权重不断变大</title>
      <link>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</link>
      <description><![CDATA[为了熟悉梯度下降算法，我尝试创建自己的线性回归模型。对于少数数据点来说它效果很好。但是当尝试使用更多数据来拟合它时，w0 和 w1 的大小总是增加。有人可以解释一下这种现象吗？
类线性回归：
    def __init__(自身, x_向量, y_向量):

        self.x_vector = np.array(x_vector, dtype=np.float64)
        self.y_向量 = np.array(y_向量, dtype=np.float64)
        自身.w0 = 0
        自身.w1 = 0

    def _get_predicted_values(self, x):
        公式 = lambda x: self.w0 + self.w1 * x
        返回公式(x)

    def_get_gradient_matrix（自身）：
        预测 = self._get_predicted_values(self.x_vector)
        w0_hat = sum((self.y_向量 - 预测))
        w1_hat = sum((self.y_向量 - 预测) * self.x_向量)

        梯度矩阵 = np.array([w0_hat, w1_hat])
        梯度矩阵 = -2 * 梯度矩阵

        返回梯度矩阵

    def fit(自我，step_size=0.001，num_iterations=500)：
        for _ in range(1, num_iterations):
            梯度矩阵 = self._get_gradient_matrix()
            self.w0 -= 步长大小 * (梯度矩阵[0])
            self.w1 -= 步长大小 * (梯度矩阵[1])

    def _show_coeffiecients（自身）：
        print(f&quot;w0: {self.w0}\tw1: {self.w1}\t&quot;)

    def 预测（自身，x）：
        y = 自身.w0 + 自身.w1 * x
        返回y

# 这工作正常
x = [x 表示 x 在范围 (-3, 3) 内]
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coeffiecients() #输出：w0：-6.99999999999994 w1：5.00000000000002

#虽然这不是
x = [x for x in range(-50, 50)] # 增加 x 值的数量
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coefficients()

最后一行产生警告：
运行时警告：乘法中遇到溢出
w1_hat = sum((self.y_向量 - 预测) * self.x_向量)
公式 = lambda x: self.w0 + self.w1 * x
]]></description>
      <guid>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</guid>
      <pubDate>Wed, 06 Mar 2024 14:22:16 GMT</pubDate>
    </item>
    <item>
      <title>邻居索引错误：self._check_indexing_error(key) KeyError：8</title>
      <link>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-self-check-indexing-errorkey-keyerror-8</link>
      <description><![CDATA[我正在创建一个服装推荐系统，使用 NearestNeighbors，数据来自 2 个数据集，其中一个数据集包含 ratings.csv，在本例中 0 和 1&lt; /code&gt; 基于是否保存到愿望清单以及所有衣服的衣服.csv，我想传递服装的 ID 并获取推荐商品的列表，但我收到索引错误。
这是代码：
user_ ratings_df = pd.read_csv(“ ratings.csv”)

user_ ratings_df[&#39;IDGARMENT&#39;] = user_ ratings_df[&#39;IDGARMENT&#39;].astype(int)

# 读入数据；使用默认的 pd.RangeIndex，即 0、1、2 等作为列
Clothes_desc = pd.read_csv(“clothes.csv”, on_bad_lines=&#39;skip&#39;)
Clothing_metadata = Clothing_desc[[&#39;IDGARMENT&#39;, &#39;描述&#39;, &#39;类别&#39;, &#39;品牌&#39;, &#39;价格&#39;]]

衣服元数据[&#39;IDGARMENT&#39;] = 衣服元数据[&#39;IDGARMENT&#39;].astype(int)
Clothes_data = user_ ratings_df.merge(clothes_metadata, on=&#39;IDGARMENT&#39;)

user_item_matrix = user_ ratings_df.pivot(index=[&#39;USERID&#39;], columns=[&#39;IDGARMENT&#39;], value=&#39;RATING&#39;).fillna(0)
用户项矩阵

# 定义一个关于余弦相似度的 KNN 模型
cf_knn_model=NearestNeighbors(metric=&#39;cosine&#39;,algorithm=&#39;brute&#39;,n_neighbors=10,n_jobs=-1)
#lr.fit(x.reshape(-1, 1), y)

# 将模型拟合到我们的矩阵上
cf_knn_model.fit(user_item_matrix)


def dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs):
    # 在矩阵上拟合模型
    cf_knn_model.fit（矩阵）
    
    # 计算邻居距离
    距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
    Clothing_rec_ids = Sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]
    
    # 存储推荐的列表
    cf_recs = []
    对于我在 dress_rec_ids 中：
        cf_recs.append({&#39;Desc&#39;:clothes_desc[&#39;DESCRIPTION&#39;][i[0]],&#39;距离&#39;:i[1]})
    
    # 选择需要的最多推荐数量
    df = pd.DataFrame(cf_recs, 索引 = 范围(1,n_recs))
    返回df


n_recs = 10
dress_recommender_engine（54448，user_item_matrix，cf_knn_model，n_recs）

我得到的错误是：
&lt;前&gt;&lt;代码&gt;&gt; *keyError Traceback（最近一次调用最后）文件
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802,
&gt;在Index.get_loc（self，key，method，tolerance）3801中尝试：
&gt; -&gt; [第 3802 章] 第 3803 章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; pandas/_libs/hashtable_class_helper.pxi:2263，在
&gt; pandas._libs.hashtable.Int64HashTable.get_item() 文件
&gt; pandas/_libs/hashtable_class_helper.pxi:2273，位于
&gt; pandas._libs.hashtable.Int64HashTable.get_item() KeyError：54448
&gt;上述异常是以下异常的直接原因：
&gt; KeyError Traceback（最近调用
&gt;最后）单元格 In[4]，第 64 行
&gt; 59 返回 df
&gt; 63 n_recs = 10
&gt; ---&gt; 64 dress_recommender_engine(54448, user_item_matrix, cf_knn_model, n_recs) 单元格 In[4]，第 48 行，in
&gt; dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs)
&gt; 42 cf_knn_model.fit（矩阵）
&gt; 44 # 提取输入的电影ID
&gt;第45话
&gt; 46
&gt; 47 # 计算邻居距离
&gt; ---&gt; 48 个距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
&gt;第49章 衣服
&gt; x: x[1])[:0:-1]
&gt; 51 # 存储推荐的列表 File ~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807, in
&gt;第3805章1：
&gt;第3806章
&gt; -&gt;第3807章 第3808章 第3809章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804，
&gt;在Index.get_loc（self，key，method，tolerance）3802返回
&gt; self._engine.get_loc(casted_key) 3803 除了 KeyError 为错误：
&gt; -&gt;第3804章 3805，除了TypeError：3806，引发KeyError（key）
&gt;第3807章否则我们会失败并重新加注
&gt;第3808章第3809章
&gt;密钥错误：54448*

错误似乎在这一行：
距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
当传递matrix[garment_id]时，知道如何解决它吗？]]></description>
      <guid>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-self-check-indexing-errorkey-keyerror-8</guid>
      <pubDate>Mon, 04 Mar 2024 14:12:06 GMT</pubDate>
    </item>
    <item>
      <title>边际对数似然 GPytorch - 最小化的初始条件</title>
      <link>https://stackoverflow.com/questions/78031211/marginal-log-likelihood-gpytorch-initial-condition-for-minimization</link>
      <description><![CDATA[这个问题是针对高斯过程训练的情况提出的，但我想它也适用于神经网络。
# 找到最优模型超参数
模型.train()
可能性.train()

# 使用 adam 优化器
Optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # 包含高斯似然参数

#“损失”对于 GP - 边际对数似然
mll = gpytorch.mlls.ExactMarginalLogLikelihood（可能性，模型）

对于范围内的 i（训练迭代）：
    优化器.zero_grad()
    输出=模型（train_x）
    损失 = -mll(输出, train_y)
    loss.backward()
    print(&#39;Iter %d/%d - 损失: %.3f&#39; % (i + 1,training_iterations,loss.item()))
    优化器.step()

在典型的训练过程中，我们试图最小化损失函数（在本例中为边际对数似然）。我的问题是，最小化的初始条件是什么？如何将其输入/输出到训练循环？
我只是想从数学角度理解训练阶段，发现 Pytorch “以一种无形的方式”做了很多事情。]]></description>
      <guid>https://stackoverflow.com/questions/78031211/marginal-log-likelihood-gpytorch-initial-condition-for-minimization</guid>
      <pubDate>Wed, 21 Feb 2024 02:05:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 mlx 中进行蒙版填充？</title>
      <link>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</link>
      <description><![CDATA[我想在 mlx 中实现 masked_fill，但它与 float(&#39;-inf&#39;) 配合效果不佳
https://pytorch.org/docs/stable/generate /torch.Tensor.masked_fill.html
我正在尝试使用 mlx.core.where 来实现此目的
masked_tensor = mlx.core.where(mask, mlx.core.array(float(&#39;-inf&#39;)), mlx.core.array(0))

但是对于面具
数组([[假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]，
       [假，假，真，真]]，dtype = bool）

这会返回
数组([[nan, nan, -inf, -inf],
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]，
       [南，南，-inf，-inf]]，dtype = float32）

这不是我想要的。理想情况下它会返回
数组([[0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf],
       [0, 0, -inf, -inf]], dtype=float32)

帮助]]></description>
      <guid>https://stackoverflow.com/questions/77963476/how-do-i-do-masked-fill-in-mlx</guid>
      <pubDate>Thu, 08 Feb 2024 16:55:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么 mediapipe 不在实时反馈上绘制地标</title>
      <link>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</link>
      <description><![CDATA[这是我从 mediapipe 文档中获得的代码。我尝试了很多方法来在实时反馈中展示地标图，但似乎没有任何效果。我确实需要一些帮助来了解我错过的事情。
导入 mediapipe 作为 mp
从 mediapipe.tasks 导入 python
从 mediapipe.tasks.python 导入视觉
导入CV2
导入时间
将 mediapipe 导入为 mp
将 numpy 导入为 np
从 mediapipe 导入解决方案
从 mediapipe.framework.formats 导入地标_pb2
将 numpy 导入为 np

边距 = 10 # 像素
字体大小 = 1
字体粗细 = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54) # 鲜艳的绿色

BaseOptions = mp.tasks.BaseOptions
HandLandmarker = mp.tasks.vision.HandLandmarker
HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions
HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult
VisionRunningMode = mp.tasks.vision.RunningMode

# 使用直播模式创建一个手部地标实例：
def print_result(结果：mp.tasks.vision.HandLandmarkerResult，output_image：mp.Image，timestamp_ms：int)：
    print(&#39;手部地标结果：{}&#39;.format(result))

选项 = HandLandmarkerOptions(
    base_options=BaseOptions(model_asset_path=&#39;hand_landmarker.task&#39;),
    running_mode=VisionRunningMode.LIVE_STREAM,
    结果回调=打印结果）
使用 HandLandmarker.create_from_options(options) 作为地标：
    上限 = cv2.VideoCapture(0)
    而真实：
        ret, 框架 = cap.read()
        如果不转：
            休息
        frame_np = np.array(帧)
        时间戳 = int(round(time.time()*1000))
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_np)
        帧 = mp_image.numpy_view()
        结果 =landmarker.detect_async(mp_image, 时间戳)
        如果类型（结果）不是类型（无）：
           hand_landmarks_list = 结果.hand_landmarks
           对于范围内的 idx(len(hand_landmarks_list))：
                hand_landmarks = hand_landmarks_list[idx]

                # 绘制手部标志。
                hand_landmarks_proto =地标_pb2.NormalizedLandmarkList()
                hand_landmarks_proto.landmark.extend([
                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) 用于 hand_landmarks 中的地标
                ]）
                解决方案.drawing_utils.draw_landmarks（
                    框架，
                    hand_landmarks_proto，
                    解决方案.hands.HAND_CONNECTIONS,
                    解决方案.drawing_styles.get_default_hand_landmarks_style(),
                    Solutions.drawing_styles.get_default_hand_connections_style())
        别的：
            打印（&#39;其他&#39;）
        cv2.imshow(&#39;框架&#39;, 框架)
        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息
    cap.release()
    cv2.destroyAllWindows()

每次传递“结果函数”时，我都会收到此 NoneType 错误。我也不知道如何处理。 mediapipe 文档没有提供有关如何在实时源中显示此内容的任何见解。]]></description>
      <guid>https://stackoverflow.com/questions/76533527/why-isnt-mediapipe-drawing-the-landmarks-on-the-live-feed</guid>
      <pubDate>Thu, 22 Jun 2023 15:33:46 GMT</pubDate>
    </item>
    </channel>
</rss>