<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 16 Nov 2024 06:22:41 GMT</lastBuildDate>
    <item>
      <title>Rectools‘Dataset’对象没有属性‘from_dataset’</title>
      <link>https://stackoverflow.com/questions/79194068/rectools-dataset-object-has-no-attribute-from-dataset</link>
      <description><![CDATA[我尝试使用 Python 库 Rectools 中的 DSSMModel，但遇到了一些问题。
sparse_features_dataset = Dataset.construct(
train_data,
user_features_df=users_data,
cat_user_features=[&quot;gender&quot;, &quot;age&quot;],
item_features_df=items_data,
cat_item_features=&#39;source_id&#39;
)

model = DSSMModel(sparse_features_dataset, 
max_epochs = 10,
batch_size = 64)

#此行导致问题：
model.fit(sparse_features_dataset)

你能告诉我哪里出了问题吗？
当我使用此库中的其他模型时，一切都正常，但这个模型有问题]]></description>
      <guid>https://stackoverflow.com/questions/79194068/rectools-dataset-object-has-no-attribute-from-dataset</guid>
      <pubDate>Fri, 15 Nov 2024 21:56:55 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 错误检查失败：m == 1 || n == 1：</title>
      <link>https://stackoverflow.com/questions/79193653/xgboost-error-check-failed-m-1-n-1</link>
      <description><![CDATA[嗨，我正在尝试通过 XGBoostClassifier 运行我的数据，但遇到了以下问题。请帮忙。
!pip install xgboost
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
y_train=encoder.fit_transform(y_train)
model=XGBClassifier(objective=&quot;binary:logistic&quot;,n_estimators=10,max_depth=3, learning_rate=.1)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
print(classification_report(y_test,y_pred))`

我尝试添加标签编码器。我得到的错误如下
XGBoostError: [12:07:18] C:\buildkite-agent\builds\buildkite-windows-cpu-autoscaling- group-i-0015a694724fa8361-1\xgboost\xgboost-ci-windows\src\data\array_interface.h:218: 检查失败: m == 1 || n == 1: 

​]]></description>
      <guid>https://stackoverflow.com/questions/79193653/xgboost-error-check-failed-m-1-n-1</guid>
      <pubDate>Fri, 15 Nov 2024 18:59:05 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 R 中使用 tensorflow 和 teras 进行图像识别时出错</title>
      <link>https://stackoverflow.com/questions/79193329/error-trying-to-do-image-recognition-using-tensorflow-and-teras-in-r</link>
      <description><![CDATA[我正在按照本教程进行操作 -
https://www.r-bloggers.com/2021/03/how-to-build-your-own-image-recognition-app-with-r-part-1/amp/
但是遇到了一个错误。
model_function &lt;- function(learning_rate = 0.001, 
dropoutrate=0.2, n_dense=1024){

k_clear_session()

model &lt;- keras_model_sequence() %&gt;%
mod_base %&gt;% 
layer_global_average_pooling_2d() %&gt;% 
layer_dense(units = n_dense) %&gt;%
layer_activation(&quot;relu&quot;) %&gt;%
layer_dropout(dropoutrate) %&gt;%
layer_dense(units=output_n,activation=&quot;softmax&quot;)

model %&gt;% compile(
loss = &quot;categorical_crossentropy&quot;,
optimizer = optimizer_adam(lr = learning_rate),
metrics = &quot;accuracy&quot;
)

return(model)

}

model &lt;- model_function()

此代码的最后一行出现以下错误 -
py_call_impl(callable, call_args$unnamed, call_args$named) 中的错误: 
ValueError：层的输入应为张量。层“xception”的输入为“&lt;Sequential name=sequential,built=False&gt;”（类型为&lt;class&#39;keras.src.models.sequential.Sequential&#39;&gt;）。
运行`reticulate::py_last_error()`了解详情。

有人知道我该如何修复这个问题吗？或者有其他教程可以推荐吗？]]></description>
      <guid>https://stackoverflow.com/questions/79193329/error-trying-to-do-image-recognition-using-tensorflow-and-teras-in-r</guid>
      <pubDate>Fri, 15 Nov 2024 17:01:55 GMT</pubDate>
    </item>
    <item>
      <title>是否应将规范化应用于交互特征或交互项</title>
      <link>https://stackoverflow.com/questions/79192800/should-normalization-be-applied-on-interaction-feature-or-interaction-term</link>
      <description><![CDATA[我正在机器学习模型中使用交互特征，通过将数值变量与编码分类特征相乘来创建新特征。我的问题是：
是否应该对这些交互项应用规范化？

如果是，那么规范化不会改变交互项的含义吗？具体来说，当我先对数值特征进行归一化，然后创建交互项时，交互项是否仍表示其最初要捕获的关系？

如果在创建交互项之前对数值变量进行归一化，交互项是否会失去其真实比例或含义？


例如，如果我将归一化数值特征与分类变量（可以是独热编码）相乘，我是否会扭曲数值特征与类别之间的原始关系？
我希望澄清交互项是否应归一化或保持原样，特别是在交互项在捕获特定关系中起关键作用的情况下。
谢谢！
我尝试了什么：
我尝试在创建交互项之前对数值特征进行归一化。具体来说，我先对数值变量进行归一化，然后将其与编码的分类特征相乘。我还尝试在不先对数值变量进行归一化的情况下创建交互特征，以比较这两种方法。
我期望什么？
我希望了解在创建交互项之前对数值特征进行归一化是否会影响模型捕捉数值和分类特征之间预期关系的能力。我还很好奇，由于数值特征的归一化，交互项的含义是否会保留或扭曲。我希望了解归一化是否会导致交互项失去其原始规模和重要性，或者它是否有利于模型收敛和性能。]]></description>
      <guid>https://stackoverflow.com/questions/79192800/should-normalization-be-applied-on-interaction-feature-or-interaction-term</guid>
      <pubDate>Fri, 15 Nov 2024 14:28:44 GMT</pubDate>
    </item>
    <item>
      <title>如何从头开始创建模型以从扫描的发票中提取文本和表格数据</title>
      <link>https://stackoverflow.com/questions/79192367/how-can-i-create-a-model-from-scratch-to-extract-text-and-table-data-from-scanne</link>
      <description><![CDATA[所以我目前正在做一个项目，我们收到了 25 种不同的发票类型，全部都经过了扫描。最终目标是从发票中提取文本和表格数据，然后最终将这些数据解析为 Excel。发票类型采用不同的格式。我们如何提取表格数据 + 文本？我们可以为 25 种发票类型创建 1 个模型来执行此操作吗？还是我们需要 25 个模型。]]></description>
      <guid>https://stackoverflow.com/questions/79192367/how-can-i-create-a-model-from-scratch-to-extract-text-and-table-data-from-scanne</guid>
      <pubDate>Fri, 15 Nov 2024 12:24:08 GMT</pubDate>
    </item>
    <item>
      <title>对图像集的平均值进行标注[关闭]</title>
      <link>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</link>
      <description><![CDATA[我正在寻找一种用一个句子描述一组图像的方法。或者，我需要一种方法来在概念上平均一组图像，然后再将该“概念”（可能是特征向量）提供给常规字幕模型。
为什么？
用于 Lora 训练评估。在适合整个数据集的提示上测试训练后的生成模型会很有用，而不是选择单个图像的标题或尝试找出它们之间的共同点。此外，这还允许生成单个负面提示来测试模型在范围外的提示上的表现。
我到目前为止所做的：
我已经修改了现有的 CLIP+BLIP 询问器以处理图像集（它也可以生成负片）。然而，虽然 CLIP 字幕允许在使用图像特征选择最佳字幕之前对其进行平均，但它的准确性远低于 BLIP 生成的字幕，后者仅适用于单幅图像。我需要一个可以像 CLIP 一样接收特征向量的模型，这样我就可以对它们进行预处理。]]></description>
      <guid>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</guid>
      <pubDate>Fri, 15 Nov 2024 12:19:27 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CoreML 预测中获取置信度变量</title>
      <link>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</link>
      <description><![CDATA[我正在使用 CreateML 工具训练文本分类器，当我使用预览功能并输入一个句子时，它会给我一个预测以及一个置信度变量
以下是我在应用程序上使用该模型的方式
import CoreML
...

func predict(phrase:String) -&gt; String {
guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
return &quot;Failed to load the Roll Model.&quot;
}

let rollModelInput = RollInput(text: phrase)

guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
return &quot;Roll Model Prediction Failed&quot;
}

return prediction.label
}

这有效，它提供了预测。
我的数据是标准文本/标签格式
即使我将模型导出到 xcode 并在 xcode 中运行预览，置信度变量仍然存在。
当我在设备上运行预测时，我想知道置信度变量是什么，我如何获取访问权限？
]]></description>
      <guid>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</guid>
      <pubDate>Fri, 15 Nov 2024 11:13:21 GMT</pubDate>
    </item>
    <item>
      <title>回答 Tensor Flow 警告 - 警告：TensorFlow：您的输入数据不足；中断训练</title>
      <link>https://stackoverflow.com/questions/79191359/answer-tensor-flow-warning-warningtensorflowyour-input-ran-out-of-data-inte</link>
      <description><![CDATA[结果
使用微调进行训练
每轮训练步骤：18
每轮验证步骤：4
训练生成器批量大小：32
总训练样本：576
总验证样本：144
轮次 1/5
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121：UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()
18/18 ━━━━━━━━━━━━━━━━━━━━━ 399s 19s/step - 准确率：0.1593 - 损失：2.1888 - val_accuracy：0.3281 - val_loss：1.8363
Epoch 2/5
/usr/lib/python3.10/contextlib.py:153: UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。构建数据集时可能需要使用 `.repeat()` 函数。
self.gen.throw(typ, value, traceback)
18/18 ━━━━━━━━━━━━━━━━━━━━━━ 5s 294ms/step - 准确度：0.0000e+00 - 损失：0.0000e+00 - val_accuracy：0.3750 - val_loss：1.8651
Epoch 3/5
18/18 ━━━━━━━━━━━━━━━━━━━━━━━ 361s 18s/step - 准确度：0.3593 - 损失： 1.7475 - val_accuracy：0.4141 - val_loss：1.6302
Epoch 4/5
18/18 ━━━━━━━━━━━━━━━━━━━━━━ 41s 2s/步 - 准确度：0.0000e+00 - 损失：0.0000e+00 - val_accuracy：0.4375 - val_loss：1.4419
Epoch 5/5
18/18 ━━━━━━━━━━━━━━━━━━━━━ 322s 18s/step - 准确率：0.5175 - 损失：1.4328 - val_accuracy：0.3984 - val_loss：1.4993

完整代码
(https://drive.google.com/file/d/1jHqDnCnLMHeIZ9nn4Y9O8qZUhcvFtW4k/view?usp=sharing)
您能帮我解决错误吗]]></description>
      <guid>https://stackoverflow.com/questions/79191359/answer-tensor-flow-warning-warningtensorflowyour-input-ran-out-of-data-inte</guid>
      <pubDate>Fri, 15 Nov 2024 06:58:14 GMT</pubDate>
    </item>
    <item>
      <title>如何知道平均绝对误差（MAE）是否是根据测试数据计算出来的？</title>
      <link>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</link>
      <description><![CDATA[我正在尝试使用 R 中的留一法交叉验证 (LOOCV) 来计算各种类型模型的平均绝对误差 (MAE)。在此链接中，作者展示了使用 R 中的 caret 包进行 LOOCV 然后计算 MAE 来计算 MAE 的指南。
链接为：https://www.statology.org/leave-one-out-cross-validation-in-r/
结果为
library(caret)

#指定交叉验证方法
ctrl &lt;- trainControl(method = &quot;LOOCV&quot;)

#拟合回归模型并使用 LOOCV 评估性能
model &lt;- train(y ~ x1 + x2, data = df, method = &quot;lm&quot;, trControl = ctrl)

#查看 LOOCV 摘要
print(model)

线性回归

10 个样本
2 个预测器

无预处理
重采样：留一法交叉验证
样本量摘要：9、9、9、9、9、9、...

重采样结果：

RMSE Rsquared MAE 
3.619456 0.6186766 3.146155

调整参数“截距”保持不变，值为 TRUE

但是，尚不清楚 MAE 是使用训练数据还是测试数据（样本外）计算的。]]></description>
      <guid>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</guid>
      <pubDate>Fri, 15 Nov 2024 05:50:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HMM 模型对某些动作进行门预测？</title>
      <link>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</link>
      <description><![CDATA[我想使用 IMU 传感器来预测用户接下来要进入哪个门（用于行走、跑步、跳跃等）。根据我的研究，HMM 似乎是状态预测的最佳机器学习模型。但我在网上看到的所有模型都是基于预先记录的静态数据，而不是正在记录的实时数据。我应该如何实现这一点？
我已经实现了一个滚动窗口，它使用最后捕获的 30 条数据记录进行下一次预测，并对它们进行了规范化。
我已经确定了我想要识别的每个动作的门，并且我已经设置了一个分类算法，该算法遍历每个窗口并使用它来对当前动作进行分类。
我打算用它们来验证我将要做出的预测（或者甚至在需要时将其用作输入来预测人的下一个动作）。]]></description>
      <guid>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</guid>
      <pubDate>Thu, 14 Nov 2024 23:15:03 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型中的自定义编码器和解码器层显示为未构建</title>
      <link>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</guid>
      <pubDate>Sat, 28 Sep 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>如何将多个观测值拟合到单个高斯过程</title>
      <link>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</link>
      <description><![CDATA[我试图将多个观测值拟合到单个高斯过程。
我尝试像这样拟合两个观测值 (Y) 的数据：
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# 示例数据

# 输入数据 X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])

# 输出数据 Y 
Y = np.array([[1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# 拟合模型
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict(X, return_cov=True)

我得到了两个 mean_prediction 数组和两个 cov_prediction 矩阵。但我想要一个与单个拟合 GP 对应的观测值相同维度的单个均值和协方差矩阵。我该如何实现？]]></description>
      <guid>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</guid>
      <pubDate>Thu, 30 May 2024 12:16:10 GMT</pubDate>
    </item>
    <item>
      <title>为嵌套在多个文件夹中的数据创建训练和测试拆分</title>
      <link>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</link>
      <description><![CDATA[我正在准备用于训练图像识别模型的数据。我目前有一个文件夹（数据集），其中包含多个带有标签名称的文件夹，这些文件夹中包含图像。
我想以某种方式拆分此数据集，以便我有两个具有相同子文件夹的主文件夹，但这些文件夹中的图像数量应根据首选的训练/测试拆分，例如，训练数据集中的 90% 的图像和测试数据集中的 10% 的图像。
我正在努力寻找拆分数据的最佳方法。我读过一个建议，pytorch torch.utils.Dataset 类可能是一种方法，但我似乎无法让它工作以保留文件夹层次结构。]]></description>
      <guid>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</guid>
      <pubDate>Mon, 09 Nov 2020 19:27:34 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Weka 中使用 MFCC 进行音频分类？</title>
      <link>https://stackoverflow.com/questions/45224049/how-to-use-mfccs-in-weka-for-audio-classification</link>
      <description><![CDATA[我正在尝试开发一种使用 Weka 中的 MFCC 对音频进行分类的方法。我拥有的 MFCC 是用 1024 的缓冲区大小生成的，因此每个音频记录都有一系列 MFCC 系数。我想将这些系数转换为 Weka 的 ARFF 数据格式，但我不确定如何解决这个问题。
我还问了一个关于合并数据的问题，因为我觉得这可能会影响数据转换为 ARFF 格式。
我知道对于 ARFF，数据需要通过属性列出。MFCC 的每个系数应该是单独的属性还是作为单个属性的系数数组？每个数据应该代表单个 MFCC、时间窗口还是整个文件或声音？下面，我写出了我认为如果只考虑一个 MFCC 应该是什么样子，我认为这无法对整个声音进行分类。
@relation audio

@attribute mfcc1 real
@attribute mfcc2 real
@attribute mfcc3 real
@attribute mfcc4 real
@attribute mfcc5 real
@attribute mfcc6 real
@attribute mfcc7 real
@attribute mfcc8 real
@attribute mfcc9 real
@attribute mfcc10 real
@attribute mfcc11 real
@attribute mfcc12 real
@attribute mfcc13 real
@attribute class {bark, honk, talking, wind}

@data
126.347275, -9.709645, 4.2038302, -11.606304, -2.4174862, -3.703139, 12.748064, -5.297932, -1.3114156, 2.1852574, -2.1628475, -3.622149, 5.851326, bark

如能提供任何帮助，我们将不胜感激。
编辑：
我已生成一些 ARFF 文件使用 Weka 使用 openSMILE 按照此 网站中的方法，但我不确定如何使用这些数据对音频进行分类，因为每行数据都是来自同一文件的 10 毫秒音频。每行的名称属性都是“未知”，我认为这是数据将尝试分类的属性。我如何才能对整体声音（而不是 10 毫秒）进行分类并将其与其他几个整体声音进行比较？

编辑 #2：成功！
在更彻底地阅读我找到的网站后，我看到了 Accumulate 脚本以及测试和训练数据文件。accumulate 脚本将来自不同音频文件的每个 MFCC 数据集合生成的所有文件放在一个 ARFF 文件中。他们的文件由大约 200 个属性组成，其中包含 12 个 MFCC 的统计数据。虽然我无法使用 OpenSmile 检索这些统计数据，但我使用了 Python 库来执行此操作。统计数据包括最大值、最小值、峰度、范围、标准差等。我使用 Weka 中的 BayesNet 和多层感知器准确地对我的音频文件进行了分类，这两项方法都为我带来了 100% 的准确率。]]></description>
      <guid>https://stackoverflow.com/questions/45224049/how-to-use-mfccs-in-weka-for-audio-classification</guid>
      <pubDate>Thu, 20 Jul 2017 19:52:54 GMT</pubDate>
    </item>
    </channel>
</rss>