<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 19 Feb 2025 21:16:29 GMT</lastBuildDate>
    <item>
      <title>当MLFlow模型被``@Champion''更改时，如何自动更新SageMaker端点？</title>
      <link>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</link>
      <description><![CDATA[我在AWS SageMaker AI上使用托管MLFLOW服务器（ https://www.youtube.com/watch?v=3xkz_5hop6k&amp;ab_channel = awsevents ）以跟踪实验和模型版本。我们的数据科学团队通过用冠军别名标记最佳版本来促进生产模型。这些模型部署到了萨吉人端点，然后通过API网关访问。
我想实现自动化管道，每当将新型号分配给MLFlow中的冠军别名时，该管道都会更新SageMaker端点。目前，我正在考虑将针对别名更改进行轮询的Lambda功能，但我正在寻找更高效或更有托管的解决方案。是否有人实现了动态端点更新机制或可以提出替代方法？]]></description>
      <guid>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</guid>
      <pubDate>Wed, 19 Feb 2025 18:00:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的GPT-2小型模型的响应不一致且重复性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</link>
      <description><![CDATA[ i在1100个JSON文件的数据集上微调的GPT-2（小），分为：

 400 Q＆amp;一对
 700食谱数据（标题，成分，说明等）

该模型对基于食谱的提示有些响应，例如“如何制作芝士蛋糕”生成准确的成分和方向（即使我认为这只是重复培训数据）。但是，当被问及诸如“您好，您好吗？”之类的一般性问题时，它会产生荒谬的答案，例如提到没有上下文的素食主义者。
这是培训进度：

时期1：训练损失：没有日志|验证损失：1.882118 
时期2：训练损失：2.690300 |验证损失：1.865422 
时期3：训练损失：2.690300 |验证损失：1.844494 
时期4：训练损失：2.638200 |验证损失：1.806402 

我怀疑：

模型过于适应结构化数据。
 gpt-2 Small可能不适合此类任务。
]]></description>
      <guid>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</guid>
      <pubDate>Wed, 19 Feb 2025 14:56:34 GMT</pubDate>
    </item>
    <item>
      <title>结合贷款批准预测模型的数值和文本数据[封闭]</title>
      <link>https://stackoverflow.com/questions/79451335/combining-numerical-and-text-data-for-loan-approval-prediction-model</link>
      <description><![CDATA[我正在从事一个机器学习项目，我根据结构化数据（收入，信用评分，贷款金额等）和非结构化的文本数据（用户提供的描述）预测贷款批准。这是一个快速概述：
技术堆栈：随机森林（Scikit-learn），简化Web应用程序，TF-IDF用于文本挖掘。
准确性：到目前为止，该模型的精度达到了98％。
我正在寻求以下建议：

 功能工程：
我可以使用任何高级功能工程技术来通过结构化数据来提高模型的性能？

 组合数值＆amp;文本数据：
在机器学习管道中将数值数据与文本功能（例如我使用TF-IDF提取的文本功能）集成的最佳实践是什么？

 解释性：
我也有兴趣改善模型解释性。使用形状或石灰是最好的路线吗？如果是这样，任何建议将它们用于随机森林模型？


我已经使用随机森林和联合结构化数据（例如收入，信用评分）实施了贷款批准预测模型（通过TF-IDF）。我培训了该模型并使用样本输入进行了测试以预测贷款批准。
我期望通过整合结构化和非结构化数据，与仅使用结构化数据相比，我的模型将提高准确性和预测能力。
虽然模型的精度为98％，但我不确定文本挖掘方面（TF-IDF功能）是否与数值数据最佳地集成在一起。我正在寻求有关如何更好地结合这些类型数据并改善功能工程或模型性能的指导。]]></description>
      <guid>https://stackoverflow.com/questions/79451335/combining-numerical-and-text-data-for-loan-approval-prediction-model</guid>
      <pubDate>Wed, 19 Feb 2025 12:32:27 GMT</pubDate>
    </item>
    <item>
      <title>将偏航和俯仰角映射到屏幕坐标以进行凝视跟踪[封闭]</title>
      <link>https://stackoverflow.com/questions/79450899/mapping-yaw-and-pitch-angles-to-screen-coordinates-for-gaze-tracking</link>
      <description><![CDATA[我正在处理一个凝视跟踪系统，在该系统中，我需要将偏航（θ）和螺距（φ）角（从深度学习模型获得）转换为屏幕坐标（𝑥，𝑦）。尽管尝试了各种公式，但我仍在努力获得准确的结果。
以前，我尝试使用三角学，直接角度到屏幕映射和经验缩放尝试类似的方法。但是，诸如非线性，深度估计和用户之间的变化之类的问题持续存在。我正在寻找一种更强大的转换方法。]]></description>
      <guid>https://stackoverflow.com/questions/79450899/mapping-yaw-and-pitch-angles-to-screen-coordinates-for-gaze-tracking</guid>
      <pubDate>Wed, 19 Feb 2025 09:57:59 GMT</pubDate>
    </item>
    <item>
      <title>我是否将神经网络锁定错误？预测有时似乎是随机的</title>
      <link>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</link>
      <description><![CDATA[我试图用2层（1st有2个神经元，2个神经元）来制作神经网络，这些神经元可以透过4个数字列表，如果第一个和第三个数字等于1，则答案应为1 ，在其他情况下= 0。
我根本不使用任何AI框架库，只需随机进行随机权重，而Numpy Yo会造成正方形损失。我知道，如果我不将种子设置为随机化剂，那将永远是随机的，但是我的答案太随机了。
我有测试数据集，其中第一个和最后一个列表应为1，第二和第三列是0：
  x2 = [
    [1，0，1，0]，＃答案：1
    [1，0，0，0]，＃答案：0
    [0，1，1，0]，＃答案：0
    [1，0，1，1]＃答案：1
这是给出的
 
但是，每次我运行代码时，第一和第二列表的答案比第三和第四列更随机。有时第二个答案比第一大。
我试图从以下方式更改权重调整：
  err3 = y1 [i]  -  neuron3.Acivate（py3 [i]）
err1 = y1 [i] -py1 [i]
err2 = y1 [i] -py2 [i]
neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err1 * lr
neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err2 * lr
＃（x1是培训数据集，[i]是列表索引，[k]是列表元素索引）
 
 to：
  neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * numpy.mean（err1 ** 2） * lr
neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * numpy.mean（err2 ** 2） * lr
 
但是我有Oveflow错误。
我当前拥有的所有代码：
 导入随机
导入数学
导入numpy

班级神经元：
    def __init __（self，weightsc）：
        self.w = []
        对于我的范围（weightsc）：
            self.w.append（random.random（））

    Def激活（Self，X）：
        返回最大（0，x）

    def预测（self，px）：
        值= 0
        对于我的范围（len（self.w））：
            值 += px [i] * self.w [i]
        返回值

    &#39;&#39;&#39;def train（self，count，x，y，lr = 0.1）：
        打印（“ \ ttraining开始：”）
        对于我的范围（count）：
            打印（&#39;epoch：＆quot; i）
            对于J范围（Len（x））的J：
                py = self.predict（x [j]）
                打印（py：＆quot; py）
                err = y [j]  -  self.activate（py）
                打印（“ err：＆quot”，err）
                对于K范围（Len（self.w））：
                    self.w [k] += err * x [j] [k] * lr
                    打印（＆quot; w [k]＆quot; self.w [k]）&#39;&#39;&#39;

x1 = [
    [0，0，0，0]，
    [1，0，1，0]，
    [1，1，0，1]，
    [0，1，1，0]，
    [1，1，0，0]，
    [1，0，1，1]，
    [1，1，1，0]
这是给出的

y1 = [0，1，0，0，0，0，1，1]

x2 = [
    [1，0，1，0]，
    [1，0，0，0]，
    [0，1，1，0]，
    [1，0，1，1]
这是给出的

py1 = []
py2 = []
py3 =​​ []

如果__name__ ==＆quot __ Main __＆quot;：
    LR = 0.001

    Neuron1 =神经元（4）
    Neuron2 =神经元（4）
    Neuron3 = Neuron（2）
    打印（＆quot; w1 =; n neuron1.w）
    打印（＆quot; w2 =; n neuron2.w）
    打印（＆quot; w3 =; neuron3.w）

    对于L范围（1000）：
        #print（&#39;epoch =＆quot; l）
        对于我的范围（Len（x1））：
            py1.append（
                神经元1.活化（激活
                    neuron1.predict（x1 [i]）
                ）
            ）
            #print（＆quot; layer1_pred1 =＆quot; py1 [i]）

            py22.append（
                神经元2.激活（激活
                    neuron2.predict（x1 [i]）
                ）
            ）
            #print（＆quot; layer1_pred2 =＆quot; py2 [i]）

            layer1_pred = [py1 [i]，py2 [i]]
            py3.append（
                neuron3.predict（
                    layer1_pred
                ）
            ）
            #print（＆quot; layer2_pred1 =＆quot; py3 [i]）

            err3 = y1 [i]  -  neuron3.Acivate（py3 [i]）
            #print（＆quot; err3 =＆quot; err3）

            对于J范围（Len（neuron3.w）））：
                neuron3.w [j] += lays1_pred [j] * numpy.mean（err3 ** 2） * lr

            err1 = y1 [i] -py1 [i]
            #print（＆quot; err1 =＆quot; err1）
            err2 = y1 [i] -py2 [i]
            #print（＆quot; err2 =＆quot; err2）

            对于范围内的K（Len（Neuron1.W））：
                neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err1 * lr
            对于K范围（Len（Neuron2.W）））：
                neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err2 * lr

        py1 = py2 = py3 =​​ []

    打印（“测试”）
    对于x2中的项目：
        pred1 = neuron1.activate（neuron1.predict（item））
        pred2 = neuron2.Acivate（neuron2.predict（item））
        print（neuron3.predict（[pred1，pred2]））
 ]]></description>
      <guid>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</guid>
      <pubDate>Wed, 19 Feb 2025 08:47:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的验证损失低于我的训练损失，因为训练vae在时间序列数据上时？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</link>
      <description><![CDATA[我正在训练一个变异自动编码器（VAE）从顺序时间序列数据中提取潜在变量表示。我的模型体系结构相对简单：它在编码器中使用1个LSTM层（在32至128个单位之间），而解码器中使用了1个LSTM层。重建损失计算为原始数据及其重建之间的平方平方误差（MSE），总结在序列维度上，我还计算了KL差异。这两个损失合并为我的模型优化的总损失。
这里有一些有关我的设置的详细信息：
 数据集： 

训练集：通过滚动窗口生成的13K样品。
火车，验证和测试集首先分开，它们之间没有重叠。分裂后，将一个滚动窗口独立地应用于每组
为了保留系列的时间顺序，数据集没有被改组
验证和测试集：每个样本每个样本

 模型体系结构： 

编码器和解码器中的单个LSTM层
由于层和单元数量少，模型容量是适度的

 正则化 

使用0.2的辍学率来正规化模型

令人困惑的部分是，在模型的许多变化中，我一直观察到验证损失低于训练损失。这与通常的期望相反，即训练损失较低（由于模型在培训数据上直接优化），并且验证损失较高（由于概括错误）。 
    
我的问题：

 是什么导致验证损失低于培训损失？

 我应该如何解释这些训练和验证损失曲线？

 我可能缺少任何可能有助于诊断此问题的最佳实践或检查吗？

]]></description>
      <guid>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</guid>
      <pubDate>Wed, 19 Feb 2025 05:49:19 GMT</pubDate>
    </item>
    <item>
      <title>姿势估计和校正[关闭]</title>
      <link>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</link>
      <description><![CDATA[我想建立一个系统来分析一个人在进行一定的练习时，如果不正确，请纠正其表格。我如何实施系统以提出建议以改善姿势。
用于检测，我使用了Mediapipe并构建了一个模型来使用几组视频来识别该动作。]]></description>
      <guid>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</guid>
      <pubDate>Wed, 19 Feb 2025 04:47:13 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM的时间序列不同</title>
      <link>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</link>
      <description><![CDATA[我正在训练LSTM进行时间序列预测，其中数据来自不规则间隔的传感器。我正在使用最后5分钟的数据来预测下一个值，但是某些序列比其他序列大。
我的输入阵列的形状是（611,1200,15），其中（示例，时间段，功能）。每个样本的第二维度均未完成，因此我用NP.NAN值填充了丢失的数据。例如，示例（1，：，：）有1000个时间段和200 np.nan。
训练时，损失等于Nan。
我在做什么错？我该如何训练？
这是我尝试训练LSTM的尝试：
  def lstmfit（y，x，n_hidden = 1，n_neurons = 30，Learning_rate = 1E-2）：   
    lstm = sequention（）
    lstm.add（basking（mask_value = np.nan，input_shape =（none，x. shape [2]）））））））
        
    对于范围（n_hidden）的图层：
        lstm.add（lstm（n_neurons，， 
                      激活=“ tanh”
                      recurrent_activation =＆quot; sigmoid＆quot;
                      return_sequences = true））
        
    lstm.add（密集（1））
    
    lstm.compile（loss =; mse; optimizer =; adam＆quot;）
    
    早期_STOPPING =早期踩踏（Monitor =&#39;损失&#39;，耐心= 10，详细= 1，restore_best_weights = true）
  
    
    lstm.-fit（x，y.Reshape（-1），epochs = 100，callbacks = [arfore_stopping]）
    
    y_train_fit = lstm.predict（x）
    
    返回lstm，y_train_fit
 
模型的摘要：
  lstm.summary（）
型号：sequential_9＆quot
__________________________________________________________________________
 图层（类型）输出形状参数＃   
=============================================== ===============
 masking_7（掩模）（无，无，15）0         
                                                                 
 LSTM_6（LSTM）（无，无，30）5520      
                                                                 
 密集_10（密集）（无，无，1）31        
                                                                 
=============================================== ===============
总参数：5551（21.68 kb）
可训练的参数：5551（21.68 kb）
不可训练的参数：0（0.00字节）
__________________________________________________________________________
 
和训练的第一个时期：
 时期1/100
18/18 [=======================================
时代2/100
18/18 [========================================
时期3/100
18/18 [====================================
 ]]></description>
      <guid>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</guid>
      <pubDate>Tue, 18 Feb 2025 20:47:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么在训练LSTM模型时面对“ CUDA错误：设备端断言触发”？</title>
      <link>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</guid>
      <pubDate>Tue, 18 Feb 2025 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>OPENCV：从图像分割/提取打印机标签</title>
      <link>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</link>
      <description><![CDATA[我有来自打印机的标签的镜头。
我想从框架中提取单个标签（即检测每个单独标签的边界），然后在mm中找到印刷矩形和标签的顶部边缘之间的距离。。
 这是录像中的框架 
我最初尝试与一些形态学操作一起尝试轮廓检测，但是标签内的印刷内容（矩形和数字）正在干扰边缘检测，因此很难仅隔离标签边框。 
有人解决了类似问题吗？哪些预处理技术或替代方法最适合仅可靠地分割标签边缘？]]></description>
      <guid>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</guid>
      <pubDate>Tue, 18 Feb 2025 14:26:06 GMT</pubDate>
    </item>
    <item>
      <title>ValueRror：X具有7个功能，但ColumnTransFormer期望13个功能</title>
      <link>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</link>
      <description><![CDATA[我有以下代码，我尝试预测使用泊松回归的工具价格。
 ＃---加载并准备数据---
y =火车[&#39;priceToday&#39;]
x = train.drop（columns = [&#39;pricetoday&#39;]）

＃定义非标准类型
non_standard_types = [&#39;nar;

＃为非标准创建标志功能
x [&#39;non_standard_flag;]

＃确定数值和分类列
num_features = [&#39;age&#39;&#39;&#39;
cat_features = [&#39;country; country; quot&#39;final_trans;]

＃定义预处理管道
预处理器= columntransformer（
    变形金刚= [
        （&#39;num&#39;，StandardScaler（），num_features），
        （&#39;cat&#39;，onehotencoder（handle_unknown =&#39;ignore&#39;），cat_features）
    ]，剩余=; drop; quot;
）

＃---火车/测试拆分---
＃创建一个重量列
train [sample_weight; quight&#39;&#39;] = train [type; type_ls; quot;]。应用（lambda x：1如果x == x ==;

火车[stratify_group&#39;&#39;]
x_train，x_val，y_train，y_val，train_weights，val_weights = train_test_split（
    x，y，train [sample_weight&#39;]，test_size = 0.2，andural_state = 42，stratefify = train [＆quot; stratify_group＆quort＆quort;
）
＃将预处理器安装在培训数据上一次
x_train_preprocessed = preprocessor.fit_transform（x_train）
x_val_preprocessed = preprocessor.transform（x_val）

＃定义模型
模型= {
    ＆quot“ poisson”：poissonRegressor（alpha = 0.01）
}

＃火车和评估模型
model_results = {}

对于model_name，model.items（）中的型号：
    model.fit（x_train_preprocessed，y_train，sample_weight = train_weights）

    ＃预测
    预测= model.predict（x_val_preprocessed）
    ＃计算指标
    r2 = r2_score（y_val，预测）

    model_results [model_name] = {
        “型号”：模型，
        ＆quot“ R2”：R2
    }
 
我有一个测试数据，我想将其价格与模型的预测价格进行比较。
我的测试数据是这样的：
 ＃确保新数据具有正确的格式
new_data = pd.dataframe（{{
    ＆quot;：：[12，24，36，48，60，72，84，12，24，36，48，60，60，72，84]，
    ＆quot”小时：[500，1000，1500，2000，2500，3000，3500，3500，500，1000，1500，2000，2500，2500，3000，3500]，
    ＆quot“ brand;
    ＆quot&#39;power＆quot; [150] * 7 + [80] * 7，
    ＆quot“ final_trans＆quot”：[＆quot; cv; quot&#39;] * 14，，
    ＆quot“ country”：[deu＆quot;] * 14，，
    &#39;type_ls＆quot;：[NAR，NAR，NAR，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST] 
    ＆quot&#39;current_pred＆quot;：[105614，96681，88504，81018，74165，67892，62150，42608，39728，37043，37043，34540，32206，32206，30029，28000]
}））
 
我的代码是：
  new_df = pd.dataframe（new_data）
＃创建&#39;non_standard_flag&#39;
new_df [&#39;non_standard_flag;] = new_df [type; type_ls; quot;]。isin（non_standard_types）.astype（int）

＃选择预处理器所需的列
x_new = new_df [[&#39;age&#39;，&#39;power&#39;，&#39;小时&#39;，&#39;non_standard_flag&#39;，&#39;brand&#39;，&#39;country&#39;，&#39;final_trans&#39;]]]

x_new_preprocessed = preprocessor.transform（x_new） 

＃从培训数据中进行单次编码后获取列名
ohe = preprocessor.named_transformers _ [&#39;cat&#39;]
encoded_cat_columns = ohe.get_feature_names_out（cat_features）

＃创建数字功能的列名称
num_columns = num_features

＃组合列名称
all_columns = num_columns + list（encoded_cat_columns）

＃从预处理数据中创建数据框
x_new_preprocessed_df = pd.dataframe（x_new_preprocessed，columns = all_columns）

＃---用泊松模型预测---
poisson_model = model_results [＆quot; poisson; quot; quote; quode;]＃访问训练有素的泊松模型
predicted_prices = poisson_model.predict（x_new_preprocessed_df）

＃比较和存储结果---
new_df [&#39;prediction_price&#39;] = predicted_prices

＃计算预测和当前价格之间的差异
new_df [&#39;Price_difference&#39;] = new_df [&#39;Predicted_price&#39;]  -  new_df [&#39;current_pred&#39;]
 
但是，在这样做之后，我会出现错误：
  X具有7个功能，但是ColumnTransFormer期望13个功能
 
我有相同数量的列，所以我不明白为什么我有这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</guid>
      <pubDate>Wed, 12 Feb 2025 23:51:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用现有的ML工具将人类可读的时间表转换为表？</title>
      <link>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</link>
      <description><![CDATA[我有我的美国原住民部落制作的报纸的时间表。我试图使用 aws textract 从此产生某种表格。 AWS textract在这方面没有识别任何表。因此，我认为这不会起作用（如果我付款，也许可能会发生更多的事情，但这并不是这样）。。
最终，我试图筛选所有存档的报纸，并下载所有选举周期的所有时间表（“一般性”一般性“和“特殊咨询”），以查找时间表中每个项目之间的天数。 
由于这一切都在公共领域，所以我没有理由在这里粘贴桌子的图片。我还将包括文档的下载URL。 “ https://i.sstatic.net/cgbv8.png”/&gt;  
下载url：下载 
我首先在各个文档上使用Foxit读取器在Windows上找到时间表。
然后，我在ubuntu上使用了工具&#39;ocrmypdf&#39;来确保所有这些文档都是可搜索的（ocrmypdf -skip-text通知_of_of_special_election_2023.pdf.pdf.pdf./output/notce/notice_of_special_election_election_election_2023.pdf）。
然后，我碰巧在我的Google Newsfeed中看到了AWS Sextract的广告。看到它有多强大。但是当我尝试过时，它实际上并没有找到这些可读的时间表。
我希望我想知道是否有任何ML工具甚至其他解决方案来解决此类问题。
我是在试图使我的技术诀窍达到标准。最近两年我生病了，这是一个有趣的问题，我认为我认为很流行。]]></description>
      <guid>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</guid>
      <pubDate>Thu, 16 Feb 2023 16:16:27 GMT</pubDate>
    </item>
    <item>
      <title>破烂的张量作为LSTM的输入</title>
      <link>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</link>
      <description><![CDATA[学习破烂的张量以及如何将它们与张量流一起使用。
我的示例
  xx = tf.ragged.constant（[[
                        [0.1，0.2]，
                        [0.4、0.7、0.5、0.6]
                        ）））
yy = np.Array（[[[0，0，1]，[1,0,0]]）

mdl = tf.keras.Sequeential（[
    tf.keras.layers.inputlayer（input_shape = [none]，batch_size = 2，dtype = tf.float32，ragged = true），
    tf.keras.layers.lstm（64），  
    tf.keras.layers.dense（3，激活=&#39;softmax&#39;）
）））

mdl.compile（loss = tf.keras.losses.sparsecategoricalcrossentropy（from_logits = true），
              优化器= tf.keras.optimizers.adam（1E-4），
              指标= [&#39;准确性&#39;]）

mdl.summary（）
历史= mdl.fit（xx，yy，epochs = 10）
 
错误
  lastm_152层的输入0与图层不兼容：预期ndim = 3，找到ndim = 2。收到完整的形状：[2，无]
 
我不确定是否可以使用类破烂的张量。我发现的所有示例都在LSTM之前嵌入层，但是我不想创建其他嵌入层。 ]]></description>
      <guid>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</guid>
      <pubDate>Tue, 26 May 2020 21:25:12 GMT</pubDate>
    </item>
    <item>
      <title>数据科学家在教程中使用DataSource CSV，为什么不使用与数据库的连接？</title>
      <link>https://stackoverflow.com/questions/60547537/data-scientist-use-datasource-csv-in-tutorials-why-not-use-connection-to-databa</link>
      <description><![CDATA[当我在互联网上查看时，我发现了很多有关数据科学家的教程。 
我已经注意到它们都使用CSV作为数据库而不是数据库连接，即使您可以使用它。
我找不到答案为什么CSV，因为我所有的数据都存储在数据库中。
他们都使用CSV文件的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/60547537/data-scientist-use-datasource-csv-in-tutorials-why-not-use-connection-to-databa</guid>
      <pubDate>Thu, 05 Mar 2020 14:24:25 GMT</pubDate>
    </item>
    <item>
      <title>XGBOOST.PLOT_TREE：二进制功能解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我已经建立了一个XGBoost模型，并试图检查单个估计器。作为参考，这是具有离散和连续输入特征的二进制分类任务。输入功能矩阵是 scipy.sparse.csr_matrix 。
当我去检查单个估计器时，我发现很难解释二进制输入功能，例如 f60150 下面。 Bottommost图表中的实值 F60150 很容易解释 - 其标准在该功能的预期范围内。但是，对二进制功能进行了比较，＆lt; x＆gt; ＆lt; -9.53674E-07 没有意义。这些功能中的每一个都是1或0。 -9.53674E-07 是一个很小的负数，我想这只是XGBoost或其基础绘制库中的一些浮点特质，但是它是当功能始终为正时，使用该比较是没有意义的。有人可以帮助我了解哪个方向（即是的，缺少 vs. 否对应于这些二进制特征节点的哪个true/false sine？
这是一个可再现的示例：
 导入numpy作为NP
进口scipy.sparse
来自sklearn.datasets import fetch_20newsgroups
来自sklearn.feature_extraction.text Import countvectorizer
来自XGBoost Import plot_tree，xgbClassifier
导入matplotlib.pyplot作为PLT

def booleanize_csr_matrix（mat）：
    &#39;&#39;&#39;&#39;将带有正整数元素的稀疏矩阵转换为1s&#39;&#39;&#39;&#39;&#39;
    nnz_inds = mat.nonzero（）
    keep = np.Where（Mat.Data＆gt; 0）[0]
    n_keep = len（keep）
    结果= scipy.sparse.csr_matrix（
        （np.ones（n_keep），（nnz_inds [0] [keep]，nnz_inds [1] [keep]）），
        Shape = MAT.SHAPE
    ）
    返回结果

###设置数据集
res = fetch_20newsgroups（）

text = res.data
结果= res.target

###使用CountVectorizer中的默认参数创建初始计数矩阵
vec = countvectorizer（）
x = vec.fit_transform（文本）

＃是否要“布尔”输入矩阵
booleanize = true

＃是否为“布尔值”转换数据类型，以匹配`vec.fit_transform（text）&#39;返回的内容``&#39;&#39;
to_int = true

如果布尔利亚和to_int：
    x = booleanize_csr_matrix（x）
    x = x.astype（np.int64）

＃使其成为二进制分类问题
y = np.Where（结果== 1，1，0）

＃随机状态确保我们能够始终如一地比较树木及其特征
模型= XGBClassifier（Random_State = 100）
型号（x，y）

plot_tree（型号，rankdir =&#39;lr&#39;）; plt.show（）
 
使用 BooLeanize 和 to_int 设置为 true 产生以下图表：
    
使用 BooLeanize 和 to_int 设置为 false 产生以下图表：
    
 heck，即使我做了一个非常简单的例子，我也会得到“正确”结果，无论 x 还是 y 是整数还是浮动类型。
  x = np.matrix（
    [
        [1,0]，
        [1,0]，
        [0,1]，
        [0,1]，
        [1,1]，
        [1,0]，
        [0,0]，
        [0,0]，
        [1,1]，
        [0,1]
    这是给出的
）

y = np.Array（[[1,0,0,0,0,1,1,1,1,0,1,1]）

模型= XGBClassifier（Random_State = 100）
型号（x，y）

plot_tree（型号，rankdir =&#39;lr&#39;）; plt.show（）
 
    ]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    </channel>
</rss>