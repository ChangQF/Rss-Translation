<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 26 Sep 2024 18:22:04 GMT</lastBuildDate>
    <item>
      <title>相关变量的系数是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</link>
      <description><![CDATA[不相关变量的系数表示其中的独特信息对最终变量的影响程度。但相关变量的系数意味着什么 - 谁的“拉锯战”程度如何？（请不要进行数学运算）]]></description>
      <guid>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</guid>
      <pubDate>Thu, 26 Sep 2024 16:11:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行模拟退火[关闭]</title>
      <link>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</link>
      <description><![CDATA[我正尝试使用 LOOCV（留一交叉验证）作为交叉验证方法，对 25 个预测变量实施模拟退火回归模型，进行 100 次迭代。
然而，问题是代码已经运行了 10 天，但仍然没有完成。以下代码可能存在问题？
# 库
library(caret)
library(foreach)
library(pROC)
library(randomForest)
library(parallel)
library(doParallel)

# 设置并行后端
Mycluster &lt;- makeCluster(detectCores() - 2) # 使用可用核心减 2
registerDoParallel(Mycluster) # 注册并行执行

stime = system.time({
# 训练函数的回归控制
reg.ctrl &lt;- trainControl(method = &quot;LOOCV&quot;, 
number = 10, 
repeats = 5, 
allowParallel = TRUE)

# 模拟退火控制设置
safs.ctrl &lt;- safsControl(functions = caretSA, 
method = &quot;LOOCV&quot;, 
number = 10,
metric = c(internal = &quot;RMSE&quot;, external = &quot;RMSE&quot;),
maximize = c(internal = FALSE, external = FALSE),
holdout = 0.2, 
improve = 5,
allowParallel = TRUE, 
verbose = TRUE)

# 模拟退火特征选择
sa_100 &lt;- safs(x = MEs[, 1:dim(MEs)[[2]]],
y = gsva[1, ],
iters = 100, 
metric = &quot;RMSE&quot;,
trControl = reg.ctrl,
safsControl = safs.ctrl)
})[3]

stime[1]

# 计算完成后停止集群
stopCluster(Mycluster)
registerDoSEQ() # 重置为顺序执行
]]></description>
      <guid>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</guid>
      <pubDate>Thu, 26 Sep 2024 14:21:10 GMT</pubDate>
    </item>
    <item>
      <title>多头自注意力中的梯度爆炸（NaN 训练损失和验证损失） - Vision Transformer</title>
      <link>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</link>
      <description><![CDATA[此多头自注意力代码导致训练损失和验证损失变为 NaN，但当我删除此部分时，一切都恢复正常。我知道当训练损失和验证损失变为 NaN 时，这意味着那里有一个爆炸梯度。但是，我不知道我的代码出了什么问题导致梯度爆炸。当我将它与官方 PyTorch 代码进行比较时，它看起来很相似。当我使用 nn.MultiheadSelfAttention 时，梯度不会爆炸，但当我使用我自己的代码时，梯度开始爆炸。没有显示任何错误消息。有人知道我下面的代码有什么问题吗？
class MultiHeadAttention(nn.Module):
def __init__(self, in_dim, num_heads=8, dropout=0):
super().__init__()
self.num_heads = num_heads
self.head_dim = in_dim // num_heads
self.conv_q = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_k = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_v = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.att_drop = nn.Dropout(dropout)
self.proj = nn.Conv2d(in_dim, in_dim，kernel_size=1)
self.proj_drop = nn.Dropout(dropout)

def forward(self, x):

b, _, h, w = x.shape

q = self.conv_q(x)
k = self.conv_k(x)
v = self.conv_v(x)

q = rearrange(q，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
k = rearrange(k，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
v = rearrange(v，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)

att_score = q @ k.transpose(2, 3) ** (self.head_dim ** -0.5)
att_score = F.softmax(att_score, dim=-1)
att_score = self.att_drop(att_score)

x = att_score @ v

x = rearrange(x, &#39;b nh (h w) hd -&gt; b (nh hd) h w&#39;, h=h, w=w)

x = self.proj(x)
x = self.proj_drop(x)

返回 x
]]></description>
      <guid>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</guid>
      <pubDate>Thu, 26 Sep 2024 11:48:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 gpu 上运行 Qwen2-VL 模型？</title>
      <link>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</link>
      <description><![CDATA[我有 4 个 gpu，我想运行 Qwen2 VL 模型，但我收到“设备端断言已触发。使用 TORCH_USE_CUDA_DSA 进行编译以启用设备端断言”错误。
model_name=&quot;Qwen/Qwen2-VL-2B-Instruct&quot;
model = Qwen2VLForConditionalGeneration.from_pretrained(
model_name, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)
model = nn.DataParallel(model)
processor = AutoProcessor.from_pretrained(model_name)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;image&quot;,
&quot;image&quot;: file
},
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;&quot;&quot;描述图像&quot;&quot;&quot;
}
]
}
]
text = processing.apply_chat_template(
messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processing(
text=[text],
images=image_inputs,
videos=video_inputs,
padding=True,
return_tensors=&quot;pt&quot;,
)
使用 torch.no_grad():
generated_ids = model.module.generate(**inputs, max_new_tokens=128)

但我总是得到：
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [31,0,0] 断言 `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` 失败。
错误：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

回溯（最近一次调用）：
文件“/home/ubuntu/projects/mistral-qaC/services/VisionService.py”，第 104 行，位于 ask_vision
generated_ids = self.model.module.generate(
^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/utils/_contextlib.py”，第 116 行，位于 decorate_context
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py”，第2015，在 generate 中
result = self._sample(
^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py&quot;，第 2965 行，在 _sample 中
output = self(**model_inputs, return_dict=True)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/accelerate/hooks.py&quot;，第 169 行，在 new_forward 中
output = module._old_forward(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py&quot;，第 1598 行，正向
输入_embeds[image_mask] = image_embeds
~~~~~~~~~~~~~^^^^^^^^^^^^
RuntimeError：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

我试过什么？

使用 CUDA_LAUNCH_BLOCKING=1 python script.py 运行我的 python 脚本，但它也不起作用。
打印输入和模型设备：模型设备：cuda:0
输入设备：cuda:0
torch.cuda.synchronize() 和 torch.cuda.empty_cache() 在生成之前。

我的 transformers 和 pytorch 版本是：
transformers==4.45.0.dev0
torch==2.4.1+cu124

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</guid>
      <pubDate>Thu, 26 Sep 2024 11:21:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与 CT 灌注数据集中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-ct-perfusion-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-ct-perfusion-dataset</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>将 Langchain ChatOpenAI 功能与 LiteLLM 结合使用</title>
      <link>https://stackoverflow.com/questions/79026315/using-langchain-chatopenai-functionality-with-litellm</link>
      <description><![CDATA[我最初编写了两个函数，为我提供模型和直接从 OpenAI 调用的函数。以下是完整代码：
from langchain_openai import ChatOpenAI

def get_open_ai(temperature=0, model=&#39;gpt-4&#39;):

llm = ChatOpenAI(
model=model,
temperature =temperature,
)
return llm

def get_open_ai_json(temperature=0, model=&#39;gpt-4&#39;):
llm = ChatOpenAI(
model=model,
temperature =temperature,
model_kwargs={&quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;}},
)
return llm

问题是我现在需要使用 LiteLLM 作为代理，我将其转发到本地端口，例如 localhost:3005。我知道我应该有一个选项可以放置 openai_base = &quot;localhost:3005&quot; 或类似的东西，以使我的代码直接命中 LiteLLM 网关而不是 OpenAI，但这在我上面的代码中不起作用。我还查看了 LiteLLM 文档，其中提供了一个使用 OpenAI 库而不是 Langchain 包装器的示例。有人能告诉我我必须在代码中更改什么才能使其与我自己的 LiteLLM 服务器一起工作吗？]]></description>
      <guid>https://stackoverflow.com/questions/79026315/using-langchain-chatopenai-functionality-with-litellm</guid>
      <pubDate>Thu, 26 Sep 2024 08:44:39 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用集成模型进行二元分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79026233/iam-doing-a-binary-classification-using-ensembel-model</link>
      <description><![CDATA[我正在使用 3 个模型，mobilenet、inceptionv3、resnet50。准确率似乎在增加，但验证准确率并没有稳定增加，而是在波动。我也做了一些数据增强，即重新缩放。但验证准确率仍然在波动。我使用的是平均集成方法。
# 定义 MobileNet 模型
def create_mobilenet_model():
输入 = 输入（形状=（224, 224, 3））
base_model = MobileNet（include_top=False，权重=&#39;imagenet&#39;，输入张量=输入，池化=无）
x = base_model.output
x = GlobalAveragePooling2D()（x）
x = BatchNormalization()（x）
x = Dense(512, 激活=&#39;relu&#39;)(x)
x = Dropout(0.5)(x)
x = Dense(256, 激活=&#39;relu&#39;)(x)
x = Dropout(0.3)(x)
输出 = Dense(1, 激活=&#39;sigmoid&#39;)(x)
模型 = Model(inputs=inputs, output=outputs)
返回模型
​
# 定义 InceptionV3 模型
def create_inceptionv3_model():
输入 = 输入(shape=(224, 224, 3))
base_model = InceptionV3(include_top=False, weights=&#39;imagenet&#39;, input_tensor=inputs, pooling=None)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = BatchNormalization()(x)
x = Dense(512,activation=&#39;relu&#39;)(x)
x = Dropout(0.5)(x)
x = Dense(256,activation=&#39;relu&#39;)(x)
x = Dropout(0.3)(x)
输出 = Dense(1,activation=&#39;sigmoid&#39;)(x)
model = Model(inputs=inputs, output=outputs)
返回模型
​
# 定义ResNet50 模型
def create_resnet50_model():
输入 = 输入（形状=（224, 224, 3））
base_model = ResNet50（include_top=False，权重=&#39;imagenet&#39;，input_tensor=inputs，pooling=None）
x = base_model.output
x = GlobalAveragePooling2D（）（x）
x = BatchNormalization（）（x）
x = Dense（512，activation=&#39;relu&#39;）（x）
x = Dropout（0.5）（x）
x = Dense（256，activation=&#39;relu&#39;）（x）
x = Dropout（0.3）（x）
输出 = Dense（1，activation=&#39;sigmoid&#39;）（x）
model = Model（inputs=inputs，outputs=outputs）
返回模型
​

# 结合 MobileNet、InceptionV3 和 ResNet50 的集成模型
def create_ensemble_model():
mobilenet_model = create_mobilenet_model()
inceptionv3_model = create_inceptionv3_model()
resnet50_model = create_resnet50_model()
​
# 集成输入
input = Input(shape=(224, 224, 3))
​
# 获取每个模型的输出
mobilenet_output = mobilenet_model(inputs)
inceptionv3_output = inceptionv3_model(inputs)
resnet50_output = resnet50_model(inputs)
​
# 使用平均合并输出
combined_output = tf.keras.layers.Average()([mobilenet_output, inceptionv3_output, resnet50_output])
​
# 定义最终集成模型
ensemble_model = Model(inputs=inputs, output=combined_output)
​
return ensemble_model

# 编译并创建集成模型
ensemble_model = create_ensemble_model()
ensemble_model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 模型摘要
ensemble_model.summary()

结果：
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 369s 2s/step - accuracy: 0.6506 - loss: 0.6266 - val_accuracy: 0
.3315 - val_loss: 0.9679
Epoch 2/10
111/111 ━━━━━━━━━━━━━━━━━━━━━ 45s 403ms/step - 准确度：0.8518 - 损失：0.3550 - val_accuracy：0.8117 - val_loss：0.4355
Epoch 3/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 45s 403ms/step - 准确度：0.9218 - 损失： 0.2276 - val_accuracy：0.9639 - val_loss：0.2109
Epoch 4/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 78s 366ms/step - 准确度：0.9578 - 损失：0.1474 - val_accuracy：0.7565 - val_loss：0.4118
Epoch 5/10
111/111 ━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9613 - 损失：0.1276 - val_accuracy：0.9030 - val_loss：0.2680
Epoch 6/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9811 - 损失：0.0815 - val_accuracy：0.8083 - val_loss：0.3373
Epoch 7/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━ 82s 369ms/step - 准确度：0.9829 - 损失：0.0829 - val_accuracy：0.7655 - val_loss：0.7217
Epoch 8/10
111/111 ━━━━━━━━━━━━━━━━━━━━━━━ 41s 367ms/step - 准确度：0.9863 - 损失：0.0569 - val_accuracy：0.5975 - val_loss：1.4897
]]></description>
      <guid>https://stackoverflow.com/questions/79026233/iam-doing-a-binary-classification-using-ensembel-model</guid>
      <pubDate>Thu, 26 Sep 2024 08:24:44 GMT</pubDate>
    </item>
    <item>
      <title>如何对高度相关的特征进行特征工程</title>
      <link>https://stackoverflow.com/questions/79025609/how-to-feature-engineer-a-highly-correlated-feature</link>
      <description><![CDATA[我正在创建一个分类模型，用于预测患者的急诊就诊次数。目标变量是患者是否去过急诊室，如果没有去过，则为 0，如果他们至少去过一次，则为 1。其中一个特征是患者过去去过急诊室的次数。该特征与目标高度相关，因为非零值对应目标中的 1，而 0 对应目标中的 0，因此我不得不将其删除。
但是，这可能是非常有用的信息，因为患者去过急诊室的次数越多，他们再次就诊的可能性就越高。我尝试了许多方法来设计该特征，例如将其转换为分类变量、对其进行分类，但即便如此，相关性仍然很高。
有没有办法设计这个特征以便也可以使用它？]]></description>
      <guid>https://stackoverflow.com/questions/79025609/how-to-feature-engineer-a-highly-correlated-feature</guid>
      <pubDate>Thu, 26 Sep 2024 05:18:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法的时间复杂度和空间复杂度测量[关闭]</title>
      <link>https://stackoverflow.com/questions/79025437/machine-learning-algrorithms-time-complexity-and-space-complextiy-measurement</link>
      <description><![CDATA[我已经训练了几个模型来预测/检测数据集中的分类值。
现在我想看看在这两种情况下哪种模型表现最好：

时间复杂度（预测/检测的时间）
空间复杂度（模型预测/检测需要多少空间（GPU/CPU））
我使用的一些模型如下
RF、CNN、MLP、RNN、LGBM、LSTM、KNN、GRU

由此我可以找到在检测/预测特定情况时可以使用更少时间和更少空间的最佳模型。
你能解释一下这个 python 代码/伪代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79025437/machine-learning-algrorithms-time-complexity-and-space-complextiy-measurement</guid>
      <pubDate>Thu, 26 Sep 2024 03:50:39 GMT</pubDate>
    </item>
    <item>
      <title>模型推理时出错：RangeError（长度）：无效值：唯一有效值为 0：1</title>
      <link>https://stackoverflow.com/questions/79025367/error-during-model-inference-rangeerror-length-invalid-value-only-valid-val</link>
      <description><![CDATA[我在运行 Flutter 应用相机、mlkit 姿势检测和自定义 tflite 模型（通过身体动作检测情绪）时总是遇到此错误，这两者都应连接起来运行并给出准确的值
我试图寻找导致该错误的原因，但找不到它]]></description>
      <guid>https://stackoverflow.com/questions/79025367/error-during-model-inference-rangeerror-length-invalid-value-only-valid-val</guid>
      <pubDate>Thu, 26 Sep 2024 03:11:17 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：无法找到类“Sequential”[关闭]</title>
      <link>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</guid>
      <pubDate>Tue, 24 Sep 2024 15:17:10 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义损失函数处理多类分类中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/77462336/handling-class-imbalance-in-multi-class-classification-with-custom-loss-function</link>
      <description><![CDATA[我正在使用先进的机器学习技术在 Python 中研究多类分类问题。我正在处理的数据集存在严重的类别不平衡问题，其中一些类别与其他类别相比代表性不足。这种不平衡对我的模型的性能产生了不利影响，尤其是对于少数类别。
为了解决这个问题，我正在考虑实现一个可以更好地处理类别不平衡的自定义损失函数。我正在使用 TensorFlow/Keras 进行模型开发。我目前的模型结构如下：
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 示例模型架构
model = Sequential([
Dense(128,activation=&#39;relu&#39;,input_shape=(input_shape,)),
Dense(64,activation=&#39;relu&#39;),
Dense(num_classes,activation=&#39;softmax&#39;)
])

model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

这里，num_classes 表示我的数据集中的类数，input_shape 是输入特征的形状。问题在于损失函数“categorical_crossentropy”，它没有考虑到类别不平衡。
我正在寻找一种方法来创建一个自定义损失函数，该函数可以将类别权重集成到计算中，从而在训练期间更加重视少数类别。以下是我的具体问题：
如何在 TensorFlow/Keras 中开发一个自定义损失函数，该函数结合了多类别分类问题的类别权重？
有哪些最佳实践可以确保此自定义损失函数具有计算效率并且不会对训练时间产生重大负面影响？
在实施自定义损失函数来处理类别不平衡时，我应该注意哪些潜在的陷阱或常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/77462336/handling-class-imbalance-in-multi-class-classification-with-custom-loss-function</guid>
      <pubDate>Fri, 10 Nov 2023 19:09:26 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“无法将类强制转换为data.frame？</title>
      <link>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</link>
      <description><![CDATA[问题出现在第 20 行：x3 &lt;- lm(Salary ~ ...

as.data.frame.default(data) 中的错误：无法将类‘c(&quot;train&quot;, &quot;train.formula&quot;)’强制转换为 data.frame

如何解决？
attach(Hitters)
Hitters

library(caret)
set.seed(123)
# 定义训练控制
set.seed(123) 
train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10)
# 训练模型
x2 &lt;- train(Salary ~., data = x, method = &quot;lm&quot;,
trControl = train.control)
# 总结结果
print(x)
x3 &lt;- lm(Salary ~ poly(AtBat,3) + poly(Hits,3) + poly(Walks,3) + poly(CRuns,3) + poly(CWalks,3) + poly(PutOuts,3), data = x2)
summary(x3)
MSE = mean(x3$residuals^2)
print(&quot;均方误差：&quot;)
print(MSE)
]]></description>
      <guid>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</guid>
      <pubDate>Fri, 15 Nov 2019 05:09:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 ARCore 时有没有办法进行真实的手部检测？</title>
      <link>https://stackoverflow.com/questions/56062416/is-there-a-way-to-do-real-hand-detection-when-using-arcore</link>
      <description><![CDATA[我想在使用 ARCore 时进行一些真实的手部检测，以扩展一些功能。不幸的是，ARCore 不支持它。
那么直到 2019 年 5 月，有没有办法在使用 ARCore 时进行物体检测？
我已经通过 TensorFlow 训练了一个模型，但它似乎无法与 ARCore 协同工作。]]></description>
      <guid>https://stackoverflow.com/questions/56062416/is-there-a-way-to-do-real-hand-detection-when-using-arcore</guid>
      <pubDate>Thu, 09 May 2019 15:05:33 GMT</pubDate>
    </item>
    </channel>
</rss>