<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 20 May 2024 15:16:05 GMT</lastBuildDate>
    <item>
      <title>sklearn PolynomialFeatures：如果 LinearRegression 生成 y 截距，是否需要偏差</title>
      <link>https://stackoverflow.com/questions/78507382/sklearn-polynomialfeatures-is-the-bias-required-if-linearregression-generates-a</link>
      <description><![CDATA[我是机器学习的新手，所以我一直在尝试一些模型，试图更好地理解它们。
当我创建一个特征矩阵 X_Poly3 (X_Poly3 = PolynomialFeatures(3)) 时，其中 X 是一个有 2 列的矩阵，生成的 X_Poly3 包含 10 列：
X1、X2、X1^2、X1.X2、X2^2、X1^3、X1^2.X2、X2^2.X1、X2^3 加上一个“偏差” 1 的列。
当我将 LinearRegression() 拟合到这个矩阵时，我最终得到了 10 个系数加上一个 y 截距变量。
我认为 1 的偏差列将充当乘数来创建 y 截距，但如果 LinearRegression 创建 y 截距作为标准，偏差列是否是必需的？
提前致谢。
我创建了一个多项式线性回归模型，但最终得到的是与 y 截距相关的 2 个变量。
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
X = np.arange(6).reshape(3, 2)
poly = PolynomialFeatures(3)
X_Poly3 = poly.fit_transform(X)
from sklearn.linear_model 导入 LinearRegression
y_train = np.arange(3).reshape(3, 1)
regressor = LinearRegression()
regressor.fit(X_Poly3, y_train)
print(regressor.intercept_)
print(regressor.coef_)]]></description>
      <guid>https://stackoverflow.com/questions/78507382/sklearn-polynomialfeatures-is-the-bias-required-if-linearregression-generates-a</guid>
      <pubDate>Mon, 20 May 2024 15:10:48 GMT</pubDate>
    </item>
    <item>
      <title>我在数据集上运行了多项式多元回归模型。该模型给出负 r2 值以及正 r2 值 [关闭]</title>
      <link>https://stackoverflow.com/questions/78507156/i-ran-a-polynomial-multiple-regression-model-on-a-dataset-the-model-give-negati</link>
      <description><![CDATA[我在只有 98 个点的数据集上运行了多项式多元回归模型。当我在不同的数据子集（训练和测试）上运行模型时。它给了我从负到正的 r2 值。我知道我的数据点数量较少。我想问我是否可以坚持以最高的正r2值运行。另外，如果以后需要运行该模型，如何保存该模型？因为当我关闭 r 窗口并再次运行代码时，我没有得到相同的 r2 值和结果，而是得到不同的最高正 r2。
我在 r 中工作，我也尝试了其他不同的模型，例如随机森林和 SVM。但我发现 PMLR 最好。但我得到的 r2 值不一致。]]></description>
      <guid>https://stackoverflow.com/questions/78507156/i-ran-a-polynomial-multiple-regression-model-on-a-dataset-the-model-give-negati</guid>
      <pubDate>Mon, 20 May 2024 14:23:51 GMT</pubDate>
    </item>
    <item>
      <title>通过 Databricks API 创建存储库时出错，缺少所需权限</title>
      <link>https://stackoverflow.com/questions/78506814/error-creating-repo-via-databricks-api-missing-required-permissions</link>
      <description><![CDATA[我使用服务主体创建从 azure devops 到 Azure Databricks 的存储库
curl --location &#39;{{DatabricksHost}}/api/2.0/repos&#39; 
--header &#39;授权：{{DATABRICKS_TOKEN}}&#39; 
--header &#39;内容类型：application/json&#39; 
--header &#39;X-Databricks-Azure-SP-管理令牌：{{AAD_TOKEN}}&#39; 
--header &#39;X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/{{SUB}}resourceGroups/{{RG}}/providers/Microsoft.Databricks/workspaces/{{WORKSPACE}}&#39; 
 - 数据 &#39;{
“url”：“https://dev.azure.com/URL/ORG/_git/REPO”，
“提供商”：“azureDevOpsServices”，
“路径”：“/Repos”，
“sparse_checkout”：{
“模式”：[
“父文件夹/子文件夹”
]
}
}&#39;
&lt;小时/&gt;
我用来获取令牌的命令：

Databricks_token：az account get-access-token --scope 499b84ac-1321-427f-aa17-267ca6975798/.default --query &quot;accessToken&quot; --输出tsv

AAD_TOKEN：az account get-access-token --resource  https://management.core.windows.net/ --query“accessToken” -o tsv


我收到此错误：
在 ID 为“0”的节点上缺少所需的权限 [查看]

或者有时我收到错误
“Git 提供程序凭据无效。转到用户设置&gt; Git 集成可确保：\n1.您已输入带有 Git 提供商凭据的用户名。\n2.您已使用您的凭据选择了正确的 Git 提供程序。\n3.您的个人访问令牌或应用密码具有正确的存储库访问权限。\n4.您的个人访问令牌尚未过期。\n5.如果您的 Git 提供商启用了 SSO，请务必授权您的令牌。””

我之前确实成功创建了 git 凭据，我的服务主体对订阅有贡献者，并且在 Databricks 工作区中拥有管理员和用户的权限]]></description>
      <guid>https://stackoverflow.com/questions/78506814/error-creating-repo-via-databricks-api-missing-required-permissions</guid>
      <pubDate>Mon, 20 May 2024 13:11:31 GMT</pubDate>
    </item>
    <item>
      <title>应该使用Python的哪个前端和后端框架在应用程序中部署机器学习模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78506697/which-frontend-and-backend-framework-of-python-should-use-to-deploy-a-machine-le</link>
      <description><![CDATA[我正在从事一个项目，我们需要建立一个机器学习模型来预测乳腺癌。我们想在 Android 应用程序中部署这个模型。所以，我的问题是哪个后端Python框架更适合这个项目？
我尝试制作一个机器学习模型。现在如何在 Android 应用程序中部署此模型？]]></description>
      <guid>https://stackoverflow.com/questions/78506697/which-frontend-and-backend-framework-of-python-should-use-to-deploy-a-machine-le</guid>
      <pubDate>Mon, 20 May 2024 12:49:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 Minirocket 分类的自定义数据集的问题</title>
      <link>https://stackoverflow.com/questions/78506562/problems-with-custom-dataset-using-minirocket-classification</link>
      <description><![CDATA[我正在开展一个更大的学校项目，尝试使用 Minirocket/Rocket 对时间序列测量进行分类。我的训练数据由一个包含测量值的一维矩阵和一个包含相应标签 (0/1) 的单独一维矩阵组成。矩阵的长度相等，因此每个测量值都有一个标签。我尝试运行该程序，该程序处理其他示例数据，但总是收到以下错误：“ValueError：发现样本数量不一致的输入变量：[1, 321408]”。我在这里做错了什么？我是否必须以不同的方式格式化训练数据，或者 MiniRocket 是否还有其他我必须调整的设置？
该错误是由以下代码行触发的：classifier.fit(train_x_transform, train_y)
这是我到目前为止尝试运行的完整代码：
导入 pandas 作为 pd
从 sklearn.metrics 导入分类报告
从 sktime.transformations.panel.rocket 导入 MiniRocket
从 sklearn.linear_model 导入 RidgeClassifierCV
将 numpy 导入为 np

# 加载数据
def load_data(文件路径):
    数据 = pd.read_csv(文件路径)
    X = 数据[&#39;sensor_values_final&#39;].values
    y = 数据[&#39;标签&#39;].值
    断言 len(X) == len(y),“X 和 y 的长度不匹配”
    返回 X, y

train_x, train_y = load_data(r&#39;csv\TrainingData_2024-05-18_18-21-46_train.csv&#39;)
test_x, test_y = load_data(r&#39;csv\TrainingData_2024-05-18_18-21-46_test.csv&#39;)

# 转换数据
minirocket = MiniRocket(10_000) # 默认情况下，MiniRocket 使用约 10,000 个内核
迷你火箭.fit(train_x)
train_x_transform = minirocket.transform(train_x)
test_x_transform = minirocket.transform(test_x)

# 训练模型
分类器 = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))
classifier.fit(train_x_transform, train_y)

# 评估模型
train_y_pred = classifier.predict(train_x_transform)
test_y_pred = 分类器.预测(test_x_transform)

print(&quot;测试集性能：&quot;)
打印（分类报告（test_y，test_y_pred））
]]></description>
      <guid>https://stackoverflow.com/questions/78506562/problems-with-custom-dataset-using-minirocket-classification</guid>
      <pubDate>Mon, 20 May 2024 12:21:28 GMT</pubDate>
    </item>
    <item>
      <title>k-最近分类概率估计问题</title>
      <link>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</link>
      <description><![CDATA[已知邻居连接方法对噪声敏感。我们将考虑训练样本的一个属性和两个对象的二元分类模型问题：（x_1 = 0.2），（x_2 = 0.7）。第一个对象属于第一类，第二个对象属于第二类。
让我们向对象添加一个新的噪声特征，均匀分布在段 ([0, 1]) 上。现在每个对象都由两个侧面来描述。需要使用具有欧几里德度量的最近邻方法对该空间中的新对象（ u = (0, 0) ）进行分类。
添加第二个噪声对象后，它比第一个更接近对象（u）的概率是多少？
如果可能的话我想得到如何解决的方法和解决方案数值]]></description>
      <guid>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</guid>
      <pubDate>Mon, 20 May 2024 10:54:59 GMT</pubDate>
    </item>
    <item>
      <title>SpaCy 变压器 NER 训练 – 变压器零损耗，未训练</title>
      <link>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</link>
      <description><![CDATA[我正在使用 [&#39;transformer&#39;, &#39;ner&#39;] 组件训练 SpaCy 管道，ner 训练得很好，但 Transformer 的损失为 0，并且我假设它没有进行训练。 
这是我的配置：
&lt;代码&gt;[路径]
矢量=“en_core_web_trf”
init_tok2vec = null
火车=“/home/sxdadmin/spacy/input/train.spacy”
dev =“/home/sxdadmin/spacy/input/dev.spacy”

[系统]
gpu_allocator =“pytorch”；
种子 = 0

[自然语言处理]
lang =“en”；
pipeline = [“变压器”, “ner”]
批量大小 = 512
禁用 = []
创建之前 = null
创建后=空
after_pipeline_creation = null
tokenizer = {“@tokenizers”：“spacy.Tokenizer.v1”}
向量 = {“@vectors”：“spacy.Vectors.v1”}

#################################################### ####################
[成分]
#################################################### ####################

[组件.变压器]
工厂=“变压器”
最大批次项 = 4096

[组件.变压器.模型]
@architectures = “spacy-transformers.TransformerModel.v1”
name = “bert-base-cased”；
tokenizer_config = {“use_fast”：true}

[组件.transformer.model.get_spans]
@span_getters = “spacy-transformers.doc_spans.v1”

[components.transformer.set_extra_annotations]
@annotation_setters = “spacy-transformers.null_annotation_setter.v1”

#################################################### ####################

[组件.ner]
工厂=“ner”
不正确的跨度键 = null
移动=空
记分器 = {“@scorers”：“spacy.ner_scorer.v1”}
update_with_oracle_cut_size = 100

[组件.ner.模型]
@architectures = “spacy.TransitionBasedParser.v2”
state_type =“ner”；
extra_state_tokens = false
隐藏宽度 = 64
最大输出件数 = 2
use_upper = true
nO = 空

#################################################### ####################
[语料库]
#################################################### ####################

[语料库.train]
@readers =“spacy.Corpus.v1”
路径 = ${paths.train}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

[语料库.dev]
@readers =“spacy.Corpus.v1”
路径 = ${paths.dev}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

#################################################### ####################
[训练]
#################################################### ####################

dev_corpus = “corpora.dev”;
train_corpus = “语料库.train”;
种子 = 0
gpu_allocator =“pytorch”；
辍学率 = 0.1
累积梯度= 1
耐心=1600
最大纪元 = 0
最大步数 = 20000
评估频率 = 200
冻结组件 = []
注释组件 = []
before_to_disk = null
更新前=空

#################################################### ####################

[训练.batcher]
@batchers = “spacy.batch_by_words.v1”
丢弃尺寸过大= false
公差 = 0.2
获取长度=空

[训练.batcher.大小]
@schedules =“compounding.v1”；
开始 = 64
停止= 512
化合物 = 1.001
t = 0.0

#################################################### ####################

[训练记录器]
@loggers = “spacy.ConsoleLogger.v1”
进度条=假

[训练.优化器]
@optimizers =“Adam.v1”；
贝塔1 = 0.9
贝塔2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
梯度剪辑 = 1.0
use_averages = false
每股收益 = 0.00000001
学习率 = 0.001

[训练.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

#################################################### ####################
[预训练]
#################################################### ####################

[初始化]
矢量=“en_core_web_lg”
init_tok2vec = null
词汇数据=空
查找=空
before_init = null
after_init = null

[初始化.组件]
[初始化.组件.变压器]
[初始化.tokenizer]

和输出：

所有警告都得到满足，著名的Bert的max_length 512个tokens就是通过文本分割实现的。数据之前已在 [tok2vec, ner] 设置上进行了测试。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</guid>
      <pubDate>Mon, 20 May 2024 10:39:58 GMT</pubDate>
    </item>
    <item>
      <title>如何处理文本分类器中的未标记数据以进行订阅推荐</title>
      <link>https://stackoverflow.com/questions/78505967/how-to-handle-unlabeled-data-in-a-text-classifier-for-subscription-recommendatio</link>
      <description><![CDATA[我正在构建一个文本分类器，用于根据传入文本推荐订阅。场景如下：
数据集不平衡：训练数据不平衡，某些类别的样本明显多于其他类别。
未标记数据：大部分传入文本（约 80%）不需要任何订阅，因此不包含在训练数据集中。
问题：
部署文本分类器时，我想确保模型不会为训练数据集中未表示的文本提供订阅建议。我该如何处理这种大多数文本都没有标签并且不需要任何订阅的情况？
具体问题：
如何训练模型来识别不需要任何订阅的文本（即那些未包含在训练数据中的文本）？
我可以使用哪些技术来有效管理不平衡数据集？
是否有任何具体策略或最佳实践来确保分类器可以识别和处理标记类别之外的文本？
还没有尝试过任何东西，寻求建议。]]></description>
      <guid>https://stackoverflow.com/questions/78505967/how-to-handle-unlabeled-data-in-a-text-classifier-for-subscription-recommendatio</guid>
      <pubDate>Mon, 20 May 2024 10:12:46 GMT</pubDate>
    </item>
    <item>
      <title>痤疮数据集上的 Mask R-CNN [关闭]</title>
      <link>https://stackoverflow.com/questions/78505757/mask-r-cnn-on-acne-dataset</link>
      <description><![CDATA[为什么 Mask R-CNN 没有在痤疮数据集上进行训练？如果我在痤疮数据集上训练它来分类痤疮类型，那么它背后的优势是什么？
Mask R-CNN 不在痤疮数据集上进行训练的真实且坚实的原因]]></description>
      <guid>https://stackoverflow.com/questions/78505757/mask-r-cnn-on-acne-dataset</guid>
      <pubDate>Mon, 20 May 2024 09:29:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 决策林支持增量学习吗？</title>
      <link>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</link>
      <description><![CDATA[我想预测供应链中的交货时间，其中依赖的特征是材料、供应商、价格、数量等......，这可以通过使用随机森林回归模型来完成，但随机森林回归模型不能支持增量学习，所以我正在寻找其他支持增量学习的模型以及支持GPU的模型。
我尝试过随机森林回归模型和支持向量机，在随机森林回归模型中增量学习是问题，而在 SVM 中处理分类数据是问题。]]></description>
      <guid>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</guid>
      <pubDate>Mon, 20 May 2024 09:08:37 GMT</pubDate>
    </item>
    <item>
      <title>当我们升级到最新版本 17 时，PYPMML 模型在响应中返回“无”</title>
      <link>https://stackoverflow.com/questions/78505291/pypmml-model-is-returning-none-in-the-responses-when-we-upgrade-to-latest-vers</link>
      <description><![CDATA[我们在 flask 应用程序中使用 pypmml 包。最近我们在 commons-text-1.6.jar 中发现了一个漏洞（安装 pypmml 时，该包出现在站点包中）。该漏洞存在于 commons-text 版本 &gt;= 1.5, &lt; 1.10.0 中。
为了缓解此漏洞，我们将包版本从 0.9.12 升级到 0.9.17。最新版本的 common-text 版本为 1.10。但是，当我们使用请求数据调用预测方法时，模型已开始在响应中给出 &quot;None&quot;。旧版本的响应与预期一致。
有人可以帮忙吗？
在此处输入图片描述
我尝试将软件包降级回 0.9.12，它按预期运行，当我降级到此软件包的第一个版本 0.9.0 时，它也能正常工作。我无法使用此软件包的较低版本，因为它存在常见的文本 jar 文件漏洞。]]></description>
      <guid>https://stackoverflow.com/questions/78505291/pypmml-model-is-returning-none-in-the-responses-when-we-upgrade-to-latest-vers</guid>
      <pubDate>Mon, 20 May 2024 07:40:14 GMT</pubDate>
    </item>
    <item>
      <title>主成分分析是否适用于 Hu 矩特征 [关闭]</title>
      <link>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</link>
      <description><![CDATA[我正在尝试改进使用 Hu 矩作为特征的图像分类应用程序的结果，但当我应用 pca 时，只有一个特征可以弥补方差的 0.95。
我对原始 Hu 特征执行了以下步骤来查找投影矩阵：

标准化
计算协方差
使用 Eigen 库查找特征向量和值
选择构成方差 0.8 的特征向量并丢弃其余的。
结果是一个 1 到 X 矩阵（只有一个特征覆盖了方差的 0.98）
]]></description>
      <guid>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</guid>
      <pubDate>Sun, 19 May 2024 23:29:42 GMT</pubDate>
    </item>
    <item>
      <title>定义数字手写识别问题的自动机[关闭]</title>
      <link>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</link>
      <description><![CDATA[大家好，你们能帮我吗，我仍然不知道如何定义我的自动机，以及它和手写识别过程之间的关系是什么，基本上我在定义转换、状态和字母方面遇到问题。
这是使用张量流和 MNIST 数据集进行手写数字识别的代码，但如何定义我的自动机？？？
`导入操作系统
导入CV2
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

模型 = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, 激活=tf.nn.softmax))
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;sparse_categorical_crossentropy&#39;,
              指标=[&#39;准确性&#39;])
model.fit(x_train, y_train, epochs=20)
model.save(&#39;手写.model&#39;)

model = tf.keras.models.load_model(&#39;手写.model&#39;)
损失，准确度 = model.evaluate(x_test, y_test)`
打印（丢失）
打印（准确度）

img = cv2.imread(&#39;img.5.png&#39;)[:,:,0]
img = cv2.resize(img, (28, 28))
img = np.invert(np.array([img]))
预测 = model.predict(img)
print(f&quot;该数字可能是 {np.argmax(prediction)}&quot;)
plt.imshow(img[0], cmap=plt.cm.binary)
plt.show()`
]]></description>
      <guid>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</guid>
      <pubDate>Sun, 19 May 2024 22:26:28 GMT</pubDate>
    </item>
    <item>
      <title>验证错误：无法实例化 GPT4AllEmbeddings 模型</title>
      <link>https://stackoverflow.com/questions/78152636/validation-error-unable-to-instantiate-gpt4allembeddings-model</link>
      <description><![CDATA[我在尝试创建 GPT4AllEmbeddings 实例时遇到问题。但是我不断收到以下错误
单元格 In[15]，第 1 行
----&gt; 1 vectorstore = Chroma.from_documents(文档 = 分割, 嵌入 = GPT4AllEmbeddings())
      2 检索器 = vectorstore.as_retriever(search_type = &#39;相似度&#39;, search_kwargs = {&#39;k&#39;:6})
      3检索文档=检索器.get_relevant_documents（“你是什么？”）

文件 ~\anaconda3\Lib\site-packages\pydantic\main.py:341，在 pydantic.main.BaseModel.__init__() 中

ValidationError：GPT4AllEmbeddings 出现 1 个验证错误
__根__
  无法实例化模型（type=value_error）

这是相关的代码片段
vectorstore = Chroma.from_documents(documents = splits, embeddings = GPT4AllEmbeddings())
检索器 = vectorstore.as_retriever(search_type = &#39;相似度&#39;, search_kwargs = {&#39;k&#39;:6})
retrieved_docs =retrieve.get_relevant_documents(“什么是Young Decade？”)
打印（len（检索文档））
打印（retrieve_docs[0].page_content）

如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78152636/validation-error-unable-to-instantiate-gpt4allembeddings-model</guid>
      <pubDate>Wed, 13 Mar 2024 09:36:07 GMT</pubDate>
    </item>
    <item>
      <title>批量归一化适用于小批量吗？</title>
      <link>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</link>
      <description><![CDATA[我使用批量标准化（批量大小为 10）进行人脸检测。
批量归一化适用于如此小的批量大小吗？如果没有，那么我还能用什么来标准化？]]></description>
      <guid>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</guid>
      <pubDate>Tue, 02 Jul 2019 20:38:25 GMT</pubDate>
    </item>
    </channel>
</rss>