<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Dec 2023 15:15:22 GMT</lastBuildDate>
    <item>
      <title>重新排列 LGBMClassifier Predict_proba 输出列</title>
      <link>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</link>
      <description><![CDATA[我正在训练一个 LGBMClassifier，以便使用其 predict_proba 方法。目标有 3 个类别：a、b 和 c。我想确保模型 predict_proba 按 b、a、c 的顺序输出列的概率。
有没有办法确保 LGBMClassifier predict_proba 的输出具有上述顺序？
导入 pandas 作为 pd
从 lightgbm 导入 LGBMClassifier
将 numpy 导入为 np

＃数据
特征 = [&#39;feat_1&#39;]
目标=&#39;目标&#39;
df = pd.DataFrame({
    &#39;feat_1&#39;：np.random.uniform（大小= 100），
    &#39;目标&#39;:np.random.choice(a=[&#39;b&#39;,&#39;c&#39;,&#39;a&#39;], size=100)
})

＃训练
模型 = LGBMClassifier()
model.fit(df[特征], df[目标])
打印（模型.classes_）

&lt;块引用&gt;
[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]

我尝试过的事情

只需重新排列 .classes_ 属性即可。
model.classes_ = [&#39;b&#39;,&#39;a&#39;,&#39;c&#39;]

&lt;块引用&gt;
AttributeError：无法设置属性“classes_”


根据 .classes_ 属性手动重新排列列。

我知道这是可以做到的，但我不想在每次 predict_proba 调用时都重新排列列顺序。]]></description>
      <guid>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</guid>
      <pubDate>Mon, 11 Dec 2023 13:35:28 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么来为我的测试数据添加 20% 的噪声</title>
      <link>https://stackoverflow.com/questions/77639239/what-should-i-use-to-add-20-noise-to-my-test-data</link>
      <description><![CDATA[我有一个关于 np.random 函数如何工作的问题。我想在测试数据中添加 20% 的噪声，但我不知道如何做到这一点，因为我不知道 np.random 函数实际上是如何工作的。
我应该使用类似的东西吗：
x_test_noise = x_test * np.random.uniform(0.8, 1.2, size=x_test.shape)

或者类似的东西：
x_test_noise=pd.DataFrame()
对于范围内的 i (len(x_test))：
    对于范围内的 j(len(x_test.columns))：
        x_test_noise=x_test*np.random.uniform(0.8,1.2)

我主要关心的是，我希望 x_test 的每个值都乘以从随机分布中提取的不同值。第一个语法是要执行此操作还是要将 x_test 的每个值与从随机分布中抽取的相同值相乘？
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/77639239/what-should-i-use-to-add-20-noise-to-my-test-data</guid>
      <pubDate>Mon, 11 Dec 2023 11:30:32 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试运行决策树模型时遇到错误</title>
      <link>https://stackoverflow.com/questions/77638235/i-have-an-error-when-trying-to-run-the-decision-tree-model</link>
      <description><![CDATA[我有一个学校机器学习小组项目，它是关于在 NSL-KDD 数据集上重现入侵检测系统研究的结果，我得到了分类部分
我试图实施决策树模型，但我得到了一个 ValueError

该错误与项目的数据准备部分有关吗？
pca部分的代码有一些错误
这是 x_train 数据的示例
]]></description>
      <guid>https://stackoverflow.com/questions/77638235/i-have-an-error-when-trying-to-run-the-decision-tree-model</guid>
      <pubDate>Mon, 11 Dec 2023 08:18:07 GMT</pubDate>
    </item>
    <item>
      <title>使用形状值分析模型时刻度标签中的字形错误</title>
      <link>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</link>
      <description><![CDATA[我正在 python 中使用 shap 包为我的模型重新创建一些图表。其中之一是瀑布图，来自 手册我按照使用以下代码生成的（完整代码太长，请查看手册）。
shap.waterfall_plot(shap_explainer_values[4652]) 
但是，我的图表的减号缺失，并出现警告消息“当前字体中缺少 Glyph 8722 (\N{MINUS SIGN})”。

stackoverflow上有很多与这个问题相关的问题，都可以通过来解决
plt.rcParams[&#39;axes.unicode_minus&#39;] = False

但是，我的却不能。有人可以帮助解决这个特定问题吗？非常感谢。
我也尝试了shutil.rmtree(matplotlib.get_cachedir())。]]></description>
      <guid>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</guid>
      <pubDate>Mon, 11 Dec 2023 05:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python AI/ML/DL 生成专业的头像图像应用程序？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像，并使用 AI/ML 输出专业头像图像。]]></description>
      <guid>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</guid>
      <pubDate>Mon, 11 Dec 2023 05:39:33 GMT</pubDate>
    </item>
    <item>
      <title>OpenCv 和 Numpy 无法兼容</title>
      <link>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</link>
      <description><![CDATA[我一直在Python中使用指纹匹配系统，并且我已经安装了所有库，但我无法导入这些库，我的pylance正在发送一条无法解析库的错误消息
在此处输入图像描述
我不想搞乱我迄今为止所做的工作]]></description>
      <guid>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</guid>
      <pubDate>Mon, 11 Dec 2023 05:03:30 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中组合图像和表格数据</title>
      <link>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</link>
      <description><![CDATA[我一直在尝试将图像（胸部 X 光）和表格数据（年龄、性别、BMI 等）结合起来形成二元预测模型（疾病：0 或 1）。我有使用顺序的 2D-CNN，但在合并表格数据时遇到困难。我在网上探索了一些资源（例如 https://machinelearningmastery.com/keras-function -api-deep-learning/），但大多数都已经过时了 - 我还没有找到任何好的例子。 函数式 API 是最好的方法吗？您能否指导我如何针对下面的数据处理此问题？
导入tensorflow为tf
从tensorflow.keras.layers导入输入、Conv2D、MaxPooling2D、展平、密集、连接
从tensorflow.keras.models导入模型

label_encoder = LabelEncoder()
df[&#39;疾病&#39;] = label_encoder.fit_transform(df[&#39;疾病&#39;]).astype(str)

# 将 DataFrame 拆分为训练集和测试集
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[&#39;疾病&#39;])

# 图像数据生成器
image_datagen = ImageDataGenerator（重新缩放=1/255）

# 训练图像生成器
train_image_generator = image_datagen.flow_from_dataframe(
    训练数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 测试图像生成器
test_image_generator = image_datagen.flow_from_dataframe(
    测试数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 将“性别”特征转换为数值
train_data[&#39;性别&#39;] = train_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})
test_data[&#39;性别&#39;] = test_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})

# 用于训练的表格特征
train_tabular_features = train_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 用于测试的表格特征
test_tabular_features = test_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 定义图像输入层
img_input = 输入(形状=(224, 224, 3), 名称=&#39;image_input&#39;)
x1 = Conv2D(16, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(img_input)
x1 = MaxPooling2D()(x1)
x1 = Conv2D(32, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(x1)
x1 = MaxPooling2D()(x1)
x1 = 展平()(x1)

# 定义表格输入层
tabular_input = 输入(形状=(4,), name=&#39;tabular_input&#39;)
x2 = 密集（16，激活=&#39;relu&#39;）（tabular_input）
x2 = 密集(32, 激活=&#39;relu&#39;)(x2)
x2 = 展平()(x2)

# 连接图像和表格分支的输出
连接=连接（[x1，x2]）

# 组合特征的公共层
x = 密集（128，激活=&#39;relu&#39;）（连接）
输出层=密集（1，激活=&#39;sigmoid&#39;，名称=&#39;输出&#39;）（x）

# 创建模型
模型 = 模型(输入=[img_input, tabular_input], 输出=output_layer)

# 编译模型
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=&#39;binary_crossentropy&#39;,
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    x={
        &#39;image_input&#39;：train_image_generator，
        &#39;表格输入&#39;：train_tabular_features
    },
    y=训练标签，
    纪元=20，
    验证数据=(
        {
            &#39;图像输入&#39;：测试图像生成器，
            &#39;tabular_input&#39;：test_tabular_features
        },
        测试标签
    ）
）


我在处理问题的方式中遇到了各种错误，主要是以下错误。我相信我的方法是错误的，因此寻找任何资源或指南。]]></description>
      <guid>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</guid>
      <pubDate>Mon, 11 Dec 2023 04:05:29 GMT</pubDate>
    </item>
    <item>
      <title>调整图像分割模型（来自 TF 教程）以进行二元掩蔽</title>
      <link>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</link>
      <description><![CDATA[我需要 Tensorflow 的图像分割模型。输入为图像和掩码（二进制、掩码或非掩码），输出为带有 0 和 1 的图像掩码。
我遵循了 https://www.tensorflow.org/tutorials/ 中的图像分割教程图像/分割
但现在我想在我的数据集上运行它的二进制掩码（没有边框类）
新数据集已准备好并输入到 model.fit 中。应该没问题吧。
如何将此模型更改为只有 2 个类（非屏蔽和屏蔽）？
base_model: keras.Model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)

# 使用这些层的激活
图层名称 = [
    &#39;block_1_expand_relu&#39;, # 64x64
    &#39;block_3_expand_relu&#39;, # 32x32
    &#39;block_6_expand_relu&#39;, # 16x16
    &#39;block_13_expand_relu&#39;, # 8x8
    &#39;block_16_project&#39;, # 4x4
]
base_model_outputs = [base_model.get_layer(name).layer_names 中名称的输出]

# 创建特征提取模型
down_stack = 模型（输入=base_model.输入，输出=base_model_outputs）

down_stack.trainable = False

上层堆栈 = [
    pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8
    pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16
    pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32
    pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64
]

def unet_model(output_channels:int):
  输入 = 层.Input(形状=[128, 128, 3])

  # 通过模型进行下采样
  跳过= down_stack（输入）
  x = 跳过[-1]
  跳过 = 反转(跳过[:-1])

  # 上采样并建立跳跃连接
  对于 up，在 zip 中跳过（up_stack，skips）：
    x = 上(x)
    concat = 层.Concatenate()
    x = concat([x, 跳过])

  # 这是模型的最后一层
  最后=层.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步幅=2，
      padding=&#39;相同&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

  返回模型（输入=输入，输出=x​​）

输出类 = 3

模型 = unet_model(output_channels=OUTPUT_CLASSES)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

当我将 OUTPUT_CLASSES 更改为 2 时，出现错误：
W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。

当OUTPUT_CLASSES为1时，预测掩码为空。
也许还必须改变其他东西？我还没有进入神经网络架构，所以我可能看不到明显的东西。
编辑：
我已将activation=&#39;sigmoid&#39;添加到输出层
 最后 = tf.keras.layers.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步幅=2，
      填充 = &#39;相同&#39;, 激活 = &#39;sigmoid&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

和OUTPUT_CLASSES = 1
奇怪的行为是下一个：
预期的掩模是当我在一个非常小的数据集上训练它时（该数据集中包含的测试的图片和掩模，只是为了测试它如何检测所看到的图像），我在第一个时期得到了一些东西。但纪元越多，结果越差。然而，准确度约为 0.99。
预期掩码：

预测掩码纪元 0：

如果打开图像，您可能会在预期的遮罩部分看到轻微的阴影。
预测掩码纪元1：

...
纪元 4：

所以每次迭代都会变得更糟。
数据集包含不应显示任何蒙版的图像。也许这就是问题所在？ （编辑：从数据集中排除没有掩码的数据 - 没有帮助）
编辑2：
x = tf.keras.layers.BatchNormalization()(x)

有帮助，虽然不完美，但是有所帮助]]></description>
      <guid>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</guid>
      <pubDate>Sun, 10 Dec 2023 13:55:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么余弦相似度总是计为1？</title>
      <link>https://stackoverflow.com/questions/77634085/why-is-cosine-similarity-always-counted-as-1</link>
      <description><![CDATA[在做学校项目时，遇到一个问题，余弦相似度总是测量为1。我无奈地质疑余弦相似度总是测量为1，因为我确认tensor1和tensor2不同。为什么余弦相似度总是测量为1？

训练代码

for i, (_image1, _label1) in enumerate(train_loader):
    image1 = _image1.to(设备)
    标签1 = _标签1[0]
    矢量1_张量 = 模型(图像1)
  
    if (i == 0): #异常情况
      图像2 = 图像1
      标签2 = 标签1
      矢量2_张量 = 矢量1_张量
   
     #问题位置

     相似度 = F.cosine_similarity(向量1_张量，向量2_张量，暗淡 = -1)
     scaled_similarity = torch.sigmoid(相似度)

    如果标签1 == 标签2：
      目标向量 = [1]
    别的 ：
      目标向量 = [0]

    target_tensor = torch.tensor(target_vector).float()
    目标张量 = 目标张量.to(设备)

    优化器.zero_grad()
    成本 = 损失（scaled_similarity，target_tensor）
    成本.向后()
    优化器.step()

    如果不是我%40：
      print (f&#39;Epoch: {epoch:03d}/{EPOCH:03d} | &#39;
            f&#39;批次 {i:03d}/{len(train_loader):03d} |&#39;
             f&#39; 成本：{成本：.4f}&#39;)

    #回收张量以减少计算量
    图像2 = 图像1.克隆()
    标签2 = 标签1
    矢量2_张量=矢量1_张量.detach()


模型定义代码

class trans_VGG(nn.Module):
    def __init__(self, base_dim):
        超级（trans_VGG，自我）.__init__（）
        self.feature = nn.Sequential(
            conv_2（3，base_dim），
            conv_2(base_dim, base_dim*2),
            conv_2(base_dim*2,base_dim*4),
            conv_3(base_dim*4,base_dim*8),
            conv_3(base_dim*8, base_dim*8)
        ）
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim*8*7*7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.线性(4096, 1000),
            nn.ReLU(True),
            nn.Dropout(),
            nn.线性(1000, 800)
        ）
        对于 self.parameters() 中的参数：
            param.requires_grad = True

    def 前向（自身，x）：
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        返回x


我们没有使用Pytorch的余弦相似度函数，而是自己创建并计算了余弦相似度函数，发现余弦相似度仍然是1。
我们发现传感器1和传感器2的值不同。
该模型是一个VGG模型，通过嵌入经过VGG模型的两个张量来获得两个张量之间的相似度。
]]></description>
      <guid>https://stackoverflow.com/questions/77634085/why-is-cosine-similarity-always-counted-as-1</guid>
      <pubDate>Sun, 10 Dec 2023 07:59:46 GMT</pubDate>
    </item>
    <item>
      <title>Yolo NAS模块超级梯度安装错误解决办法[已关闭]</title>
      <link>https://stackoverflow.com/questions/77612617/yolo-nas-module-super-gradient-installation-error-solution</link>
      <description><![CDATA[我正在使用 yolo-NAS 进行对象检测，但为此我需要安装一个名为 super-gradients 的模块。我正在 Conda 环境中构建我的项目，该环境是我现在专门为该项目创建的。但是在安装超级梯度模块时，它在中间抛出了一个错误。我已经尝试了所有我能做的方法。这是出现错误的图像在此处输入图像描述
我尝试更新 Visual C++，如上所述，当我在 google 中搜索解决方案时，我也尝试安装一些模块。如果有人给我一个解决方案，那将是一个很大的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77612617/yolo-nas-module-super-gradient-installation-error-solution</guid>
      <pubDate>Wed, 06 Dec 2023 11:18:36 GMT</pubDate>
    </item>
    <item>
      <title>使用语言模型从文本中提取名称和相关标签</title>
      <link>https://stackoverflow.com/questions/77438653/extracting-names-and-associated-labels-from-text-with-language-model</link>
      <description><![CDATA[我正在尝试从有关微藻的科学文献中提取信息，我需要能够扫描文本中的各种名称并找到其相应的类别。
举个简单的例子，假设我有 3 个名字（Peter、John、Linda），我想从这个头衔列表中找到他们的职位（木匠、渔夫、忍者），文本如下：
“彼得住在他自己建造的房子里，他是一位伟大的木匠。琳达在不忙于经营自己的生意时喜欢跑马拉松。约翰 40 岁，靠捕鱼为生”
我想要这样的回复（彼得 = 木匠，约翰 = 渔夫，琳达 = NA）。
目前我正在尝试 bert，但只能找到一种从文本中提取单个标签的方法，而且我无法找到一种方法将其与相关人员联系起来。
有人对如何解决这个问题有建议吗？
（更新）更具体的示例是使用以下文本：
“微绿藻属 gaditana 属于微绿藻属，包括六个已知物种，所有物种都是单细胞动物。与这些物种不同，节旋藻属中的所有物种都是丝状的，例如钝顶节旋藻。”
在这里我需要提取：Nannocholpsis gaditana = 单细胞，Arthrospiraplatensis = 丝状。]]></description>
      <guid>https://stackoverflow.com/questions/77438653/extracting-names-and-associated-labels-from-text-with-language-model</guid>
      <pubDate>Tue, 07 Nov 2023 13:34:55 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上使用 scikit-learn 执行 k-medoids 聚类时出现内存错误</title>
      <link>https://stackoverflow.com/questions/77393501/memoryerror-while-performing-k-medoids-clustering-with-scikit-learn-on-small-dat</link>
      <description><![CDATA[我尝试使用 scikit-learn 应用 k-medoids 聚类，但尽管数据集大小为 26 MB，但存在内存错误。
错误：
MemoryError：无法为形状为 (426740, 426740) 且数据类型为 float64 的数组分配 1.33 TiB

将 numpy 导入为 np
将 pandas 导入为 pd

# 导入数据集
数据集 = pd.read_csv(&#39;dish.csv&#39;)
X = dataset.iloc[:, 5:].values
打印（X）

# 处理丢失的数据
从 sklearn.impute 导入 SimpleImputer
imputer = SimpleImputer（missing_values = np.nan，策略=&#39;平均值&#39;）
imputer.fit(X[:, 2:])
X[:, 2:] = imputer.transform(X[:, 2:])
打印（X）

# 特征缩放
从 sklearn.preprocessing 导入 StandardScaler
sc = 标准缩放器()
X[:, :2] = sc.fit_transform(X[:, :2])
打印（X）


从 sklearn_extra.cluster 导入 KMedoids

n_簇 = 25

kmedoids = KMedoids(n_clusters=n_clusters, random_state=1).fit(X[:, :])

cluster_labels = kmedoids.labels_

打印（簇标签）

cluster_centers = kmedoids.cluster_centers_

数据集[&#39;集群&#39;] = cluster_labels
dataset.to_csv(“clustered_dish.csv”,index=False)
]]></description>
      <guid>https://stackoverflow.com/questions/77393501/memoryerror-while-performing-k-medoids-clustering-with-scikit-learn-on-small-dat</guid>
      <pubDate>Tue, 31 Oct 2023 05:49:17 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块？</title>
      <link>https://stackoverflow.com/questions/75315828/how-to-resolve-modulenotfounderror-no-module-named-gluonts-torch-modules-distr</link>
      <description><![CDATA[我正在开发一个项目，并尝试运行此存储库中的代码：
https://github.com/jc-audet/WOODS
我能够通过这一步：
python3 -m woods.scripts.download_datasets {数据集} \
        --data_path /路径/到/数据/目录

然后当我尝试运行此步骤时：
python3 -m woods.scripts.main train \
        --数据集 Spurious_Fourier \
        --客观的企业风险管理 \
        --test_env 0 \
        --data_path /路径/到/数据/目录

我收到以下错误：
文件“/Users/evangertis/development/UGA/Research/WOODS/woods/datasets.py”，第 1839 行，在  中
    从 gluonts.torch.modules.distribution_output 导入（
ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块

我期望代码能够在一个测试环境下在一个数据集上使用一个目标来训练模型。我收到以下错误：
 warnings.warn(
回溯（最近一次调用最后一次）：
  文件“/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py”，第 196 行，在 _run_module_as_main 中
    返回_run_code（代码，main_globals，无，
  文件“/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py”，第 86 行，在 _run_code 中
    执行（代码，run_globals）
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/scripts/hparams_sweep.py”，第14行，在&lt;module&gt;中。
    从树林导入实用程序
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/utils.py”，第13行，在&lt;模块&gt;中。
    从 woods.scripts 导入 hparams_sweep
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/scripts/hparams_sweep.py”，第17行，在&lt;module&gt;中。
    从树林导入数据集
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/datasets.py”，第1839行，在&lt;模块&gt;中。
    从 gluonts.torch.modules.distribution_output 导入（
ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/75315828/how-to-resolve-modulenotfounderror-no-module-named-gluonts-torch-modules-distr</guid>
      <pubDate>Wed, 01 Feb 2023 20:25:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Optuna 甚至建议记录参数的日志？</title>
      <link>https://stackoverflow.com/questions/74978660/why-optuna-even-suggests-to-take-a-log-of-a-parameter</link>
      <description><![CDATA[在官方 Optuna 教程 有一个使用 Trial.suggest_int 的 log=True 参数的示例：
导入火炬
将 torch.nn 导入为 nn


def create_model(试验, in_size):
    n_layers = Trial.suggest_int(“n_layers”, 1, 3)

    层=[]
    对于范围内的 i（n_layers）：
        n_units = Trial.suggest_int(“n_units_l{}”.format(i), 4, 128, log=True)
        层.追加（nn.Linear（in_size，n_units））
        层.append(nn.ReLU())
        in_size = n_units
    层.append(nn.Linear(in_size, 10))

    返回 nn.Sequential(*layers)

为什么有人会取神经元数量的对数？本教程中还有其他 (IMO) 冗余使用 log=True 的实例。有人可以解释一下他们的动机吗？]]></description>
      <guid>https://stackoverflow.com/questions/74978660/why-optuna-even-suggests-to-take-a-log-of-a-parameter</guid>
      <pubDate>Mon, 02 Jan 2023 02:45:43 GMT</pubDate>
    </item>
    <item>
      <title>Python：如何从 Optuna LightGBM 研究中检索最佳模型？</title>
      <link>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</link>
      <description><![CDATA[我希望获得最佳模型，以便稍后在笔记本中使用，以使用不同的测试批次进行预测。
可重现的示例（取自 Optuna Github）：
导入 lightgbm 为 lgb
将 numpy 导入为 np
导入 sklearn.datasets
导入 sklearn.metrics
从 sklearn.model_selection 导入 train_test_split

导入奥图纳


# 仅供参考：目标函数可以接受额外的参数
#（https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args）。
定义目标（试用）：
    数据，目标 = sklearn.datasets.load_breast_cancer(return_X_y=True)
    train_x、valid_x、train_y、valid_y = train_test_split（数据、目标、test_size=0.25）
    dtrain = lgb.Dataset(train_x, label=train_y)
    dvalid = lgb.Dataset(valid_x, label=valid_y)

    参数 = {
        “目标”：“二进制”，
        “公制”：“auc”，
        “详细程度”：-1，
        &quot;boosting_type&quot;: &quot;gbdt&quot;,
        &quot;lambda_l1&quot;: Trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),
        &quot;lambda_l2&quot;: Trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),
        &quot;num_leaves&quot;: Trial.suggest_int(&quot;num_leaves&quot;, 2, 256),
        &quot;feature_fraction&quot;: Trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),
        &quot;bagging_fraction&quot;: Trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),
        &quot;bagging_freq&quot;: Trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),
        &quot;min_child_samples&quot;: Trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),
    }

    # 添加用于修剪的回调。
    pruning_callback = optuna.integration.LightGBMPruningCallback（试用版，“auc”）
    gbm = lgb.train(
        参数，dtrain，valid_sets = [dvalid]，verbose_eval = False，callbacks = [pruning_callback]
    ）

    preds = gbm.predict(valid_x)
    pred_labels = np.rint(preds)
    准确度 = sklearn.metrics.accuracy_score(valid_y, pred_labels)
    返回精度


我的理解是，下面的研究将调整准确性。我想以某种方式从研究中检索最佳模型（不仅仅是参数）而不将其保存为泡菜，我只想在笔记本中的其他地方使用该模型。 

&lt;前&gt;&lt;代码&gt;
如果 __name__ == &quot;__main__&quot;:
    研究 = optuna.create_study(
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)，方向=“最大化”
    ）
    研究.优化（目标，n_Trials=100）

    print(&quot;最佳试用：&quot;)
    试验 = 研究.best_试验

    print(&quot; 参数: &quot;)
    对于 Trial.params.items() 中的键、值：
        print(&quot; {}: {}&quot;.format(key, value))


期望的输出是
best_model = ~上面的模型~
new_target_pred = best_model.predict(new_data_test)
指标.accuracy_score(new_target_test, new__target_pred)

]]></description>
      <guid>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</guid>
      <pubDate>Tue, 02 Jun 2020 04:35:05 GMT</pubDate>
    </item>
    </channel>
</rss>