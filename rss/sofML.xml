<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 30 Aug 2024 18:21:16 GMT</lastBuildDate>
    <item>
      <title>在 colab 上安装 nvstrings</title>
      <link>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</link>
      <description><![CDATA[在此处输入图片描述
如何克服这个问题？
我正在使用托管在 colab 免费层 gpu 上的 spacy en_core_web_trf 开发文档解析器模型，它需要 gpu 设备上的输入字符串，即为什么我需要安装 nvstrings 库。我已经在笔记本上安装了 nvidia RAPIDS 库，但它似乎不包含 nvstrings 库]]></description>
      <guid>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</guid>
      <pubDate>Fri, 30 Aug 2024 17:30:48 GMT</pubDate>
    </item>
    <item>
      <title>在 Microsoft Ai Hub 上应使用哪种 RAG 架构和流程来从超大文档生成内容</title>
      <link>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</link>
      <description><![CDATA[我正在研究一个用例，旨在自动生成响应，该响应是对提案请求 (RFP) 的回复 - 该响应回答了请求中的问题，并且实际上包含了有关我们公司的详细信息。
外部企业首先向我们提供请求，目前人工需要花费很长时间来编写响应，响应可能长达 90 页。
在理想情况下，我会接受请求，然后自动回复 90 页，但目前没有一个 LLM 可以做到这一点。
Microsoft 建议在使用 RAG 之前将之前的响应拆分成小块，以便可以检索和生成所有信息。但我担心不同部分之间会丢失上下文和理解。
有人遇到过类似的问题吗？你是如何设计这个解决方案的？现在，我发现有必要给这些回复贴上标签，然后将它们分成小节，为每个小节生成内容并将它们缝合在一起（对请求也做类似的事情，但这些内容不太广泛，通常只有 3-4 页长）。
我使用 Microsoft Ai Hub 和 Prompt Flows 来完成大部分工作。]]></description>
      <guid>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</guid>
      <pubDate>Fri, 30 Aug 2024 15:02:23 GMT</pubDate>
    </item>
    <item>
      <title>Mask R-CNN 模型未收敛，训练和验证的准确率在 NaN 和 0.09 之间波动</title>
      <link>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</guid>
      <pubDate>Fri, 30 Aug 2024 14:09:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 NEAT-Python 库重新利用训练的神经网络</title>
      <link>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</link>
      <description><![CDATA[我正在使用 NEAT-Python 库编写一个程序，以训练 AI 代理在 Python 中玩贪吃蛇游戏。训练后，我想导出网络，然后使用它来可视化它如何用另一种语言（例如 JavaScript）做出决策。是否可以导出网络，然后在 Python 之外使用它？我尝试查看 Uber Research 的 PyTorch-NEAT 库，但它并不是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</guid>
      <pubDate>Fri, 30 Aug 2024 13:57:57 GMT</pubDate>
    </item>
    <item>
      <title>如何利用机器学习来找到模式客户资料？</title>
      <link>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</link>
      <description><![CDATA[我有一个数据集，其中包含从一家虚构公司购买产品的客户的个人特征。最初，我没有任何目标变量，只有他们的特征。我的目标是找到一种模式，该模式不一定是每列中最常见的特征。例如，是否可以使用 RandomForest 来做到这一点？或者我应该使用其他技术？
数据集具有类似于以下的结构。这些列都是 object 格式，并且有一些 NaN 值表示为 &#39;Blank&#39;:
日期姓名薪资职位年龄
&#39;05/10/2023&#39; &#39;Daniel&#39; &#39;10,000&#39; &#39;IT&#39; 32
&#39;05/12/2024&#39; &#39;John&#39; &#39;9,000&#39; &#39;Blank&#39; 27
&#39;03/01/2023&#39; &#39;Niel&#39; &#39;Blank&#39; &#39;数据科学家&#39; 21
&#39;03/01/2023&#39; &#39;Isa&#39; &#39;10,000&#39; &#39;工程师&#39; 51
&#39;05/10/2023&#39; &#39;Ana&#39; &#39;11,000&#39; &#39;数据科学家&#39; 52
&#39;05/12/2024&#39; &#39;Ian&#39; &#39;9,500&#39; &#39;Doctor&#39; 48
&#39;03/01/2023&#39; &#39;Fred&#39; &#39;Blank&#39; &#39;IT&#39; 21
&#39;03/01/2023&#39; &#39;Carol&#39; &#39;15,000&#39; &#39;Blank&#39; 30

我正在考虑返回输出，例如，说明构成最标准配置文件的特征，例如：
最标准的配置文件是：薪水 x、职位 y 和年龄 z。

我考虑过使用聚类，但我不认为这是最好的方法（例如，薪水的输出是一个简单的平均值）。我认为最好的方法是创建一个可能并不一定存在的配置文件，并且基于研究每个变量（薪水、职位和年龄）的模式。
# 编码分类变量
df[&#39;Position&#39;] = pd.Categorical(df[&#39;Position&#39;]).codes

# 执行聚类
kmeans = KMeans(n_clusters=1, random_state=42)
kmeans.fit(df[[&#39;Salary&#39;, &#39;Position&#39;, &#39;Age&#39;]])

# 获取聚类的质心
centroid = kmeans.cluster_centers_[0]

有没有更好的方法？NLP 或 RandomForest 是一种选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</guid>
      <pubDate>Fri, 30 Aug 2024 13:39:34 GMT</pubDate>
    </item>
    <item>
      <title>zero123 的更大分辨率输出</title>
      <link>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</link>
      <description><![CDATA[我在 InstantMesh 环境中使用 zero123，我想知道 zero123 是否有可能输出更大分辨率的图像？
目前的分辨率是 320x320，对于从 InstantMesh 的 3D 重建管道获得良好的输出纹理来说，这个分​​辨率有点低。]]></description>
      <guid>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</guid>
      <pubDate>Fri, 30 Aug 2024 13:30:55 GMT</pubDate>
    </item>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理</title>
      <link>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个使用多视图模型 (zero123) 的 3D 重建管道，该模型可从一张输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
我现在的问题是：InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？]]></description>
      <guid>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Fri, 30 Aug 2024 12:00:18 GMT</pubDate>
    </item>
    <item>
      <title>标准 MLP 是二元分类任务的有效基准吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78931677/is-a-standard-mlp-a-valid-benchmark-for-binary-classification-tasks</link>
      <description><![CDATA[我正在处理一些带标签的 3D 张量，并尝试执行二元分类。数据基本上只是氨基酸序列的参数，我没有使用残基的标记，而是尝试通过提供每个氨基酸的一些基本指标（电荷、疏水性等）为模型提供更多关于氨基酸特征的信息。
我展平了数据，只是为了尝试使用标准的 sklearn MLP，我可以制作自己的更高级的神经网络来适应这项任务，也就是说，我正在考虑使用双向 LSTM 来提取这些氨基酸参数之间的“语义”（找不到更好的词）关系。
但在开始做所有这些事情之前，我不确定 MLP（表现糟糕（约 50% 准确率））是否是输入张量可分离性的良好基准——这意味着如果 MLP 无法管理，其他任何东西也都无法管理。]]></description>
      <guid>https://stackoverflow.com/questions/78931677/is-a-standard-mlp-a-valid-benchmark-for-binary-classification-tasks</guid>
      <pubDate>Fri, 30 Aug 2024 11:07:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在“cito”R 包中禁用早期停止？</title>
      <link>https://stackoverflow.com/questions/78930951/how-to-disable-early-stopping-in-cito-r-package</link>
      <description><![CDATA[如何强制禁用似乎默认启用的提前停止？
set.seed(1)

X &lt;- matrix(rnorm(10000), ncol = 10)
Y &lt;- sample(0:1, nrow(X), replace = TRUE)

library(cito)
nn &lt;- dnn(as.factor(Y)~., X, 
epochs = 100, 
loss = &quot;softmax&quot;, 
verbose = TRUE, 
lr = 0.1,
activation = &quot;sigmoid&quot;,
plot = TRUE,
early_stopping = 1000)

这是模型训练时的输出，您可以看到提前停止是如何工作的
...
..
.
第 24 个时期的损失：0.696585，lr：0.10000
第 25 个时期的损失：0.693892，lr：0.10000
第 26 个时期的损失：0.694949，lr：0.10000
第 27 个时期的损失：0.695645，lr：0.10000
第 28 个时期的损失：0.697683，lr：0.10000
第 29 个时期的损失：0.696885，lr：0.10000
取消训练，因为损失仍然高于基线，请设置超参数。请参阅 vignette(&#39;B-Training_neural_networks&#39;) 获取帮助。
]]></description>
      <guid>https://stackoverflow.com/questions/78930951/how-to-disable-early-stopping-in-cito-r-package</guid>
      <pubDate>Fri, 30 Aug 2024 07:48:40 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 自定义数据集 CPU OOM 问题</title>
      <link>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</link>
      <description><![CDATA[我的数据加载器中存在一个非常持久的内存问题，根据 num_workers 的不同，在任意数量的 epoch（5-6）后内存就会填满。
我有 85% 的把握认为问题出在数据集上，因为每次调用 getitem() 都会增加内存。
我的数据只是我在 getitem() 中加载和处理的目录列表。我没有收到任何 cuda 错误
def transformations(self,input):
i,j,h,w = self.crop.get_params(input[&#39;target_material&#39;][0], scale=(0.7, 1.0), ratio=(1.0, 1.0))
if self.use_modality1:
input[&quot;m1&quot;] = torch.cat([self.color_jitter(TF.resized_crop(sample, i, j, h, w, size=(256, 256))) for sample in input[&#39;m1&#39;]], dim=0)
if self.use_semantic:
input[&quot;m2&quot;] = torch.cat([TF.resized_crop(sample, i, j, h,w,size=(self.img_size, self.img_size), interpolation=TF.InterpolationMode.NEAREST) for sample in input[&#39;m2&#39;]], dim=0)
return input

def __getitem__(self, idx):
While True: 
source = self.data[idx]
for _ in 10: 
target = select_target(self.data) 
if (diff(source,target) &lt; .10):
continue
else:
sample = {}
if self.use_m1: 
sample[&#39;m1&#39;] = torch.stack([
to_tensor(normalize_images(np.transpose(cv2.resize(read_npz(m1_input), dsize=(256, 256), interpolation=cv2.INTER_AREA)[..., :3], (2, 0, 1)), max=255)) for m1_input 在 m1_dirs 中
], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.stack([
to_tensor(normalize_images(np.expand_dims(cv2.resize(sem_image, dsize=(256, 256), interpolation=cv2.INTER_NEAREST), axis=0), max=40)) for m2_input 在 m2_dirs 中
], dim=0)

if self.config[&quot;transform&quot;] and random.random()&lt;0.5 and self.train:
sample = self.transformations(sample)
else: 
if self.m1:
sample[&#39;m1&#39;] = torch.cat([m1_tensor for m1_tensor in sample[&#39;m1&#39;]], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.cat([m2_tensor for m2_tensor in sample[&#39;m2&#39;]], dim=0)

return sample


跟踪内存后，我发现即使读取所有数据（未应用转换）也会导致内存增加，并且这些内存不会释放，而且会不断累积。最终，我收到 OOM 错误，代码失败。]]></description>
      <guid>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</guid>
      <pubDate>Thu, 29 Aug 2024 22:59:52 GMT</pubDate>
    </item>
    <item>
      <title>多目标文本回归的最佳架构是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</link>
      <description><![CDATA[我正在使用 Google 的“Civil-Comments”数据集构建 AI 模型。它有 7 个不同的标签，每个标签都是浮点数，可以是 0 到 1 之间的任意值。
我读过的 Embedding Bags 表现不佳。我一直在研究 transformers 和循环系统（LSTM、GRU、RNN 等）。我该怎么办？
如果重要的话，使用 PyTorch。但我不是在寻找代码。]]></description>
      <guid>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</guid>
      <pubDate>Thu, 29 Aug 2024 21:09:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来追踪公司客户的资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</link>
      <description><![CDATA[我的目标是计算客户离开公司的流失风险。我想到这个方法：

生成一份代表客户历史中最突出特征的资料，并计算新客户之间的相似度。

我该如何追踪流失风险最高的人的资料？]]></description>
      <guid>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</guid>
      <pubDate>Wed, 28 Aug 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>了解 Vits 对 HiFi-GAN 的使用</title>
      <link>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</link>
      <description><![CDATA[我正在（尝试）学习语音合成的 AI/ML，并尝试理解 Vits 如何使用 HiFi-GAN。
据我所知，Vits 会将文本输入转换为梅尔频谱图，然后由 HiFi-GAN 转换为音频波。
让我困惑的是为什么从 Vits 发送到 HiFi-GAN 的输入不是梅尔频谱图。
例如，当我测试其他模型并将以下代码添加到 HiFi-GAN 的正向方法时：
class Generator(torch.nn.Module):
...
def forward(self, x):
plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
...
...

它保存了正确的图像，看起来像梅尔频谱图图像，但是，当我对 vits 执行相同操作时，保存的图像是纯绿色图像，当然不是梅尔频谱图的表示。
但生成的音频文件当然是有效的音频文件。
有人能向我解释一下吗？
我正在评估一些神经 tts 模型，我想要做的是保存由模型创建的梅尔频谱图，以便稍后进行比较，并通过不同的声码器运行它们以进行比较。
我注意到 vits repo 中的 HiFi-GAN 代码与原始 repo 略有不同，但我无法理解为什么。
有什么方法可以将输入参数 x 转换为梅尔频谱图表示，而无需先将其转换为音频，然后再将音频转换为梅尔？]]></description>
      <guid>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</guid>
      <pubDate>Sat, 15 Jun 2024 02:17:36 GMT</pubDate>
    </item>
    </channel>
</rss>