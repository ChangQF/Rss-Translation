<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 07 Aug 2024 03:18:05 GMT</lastBuildDate>
    <item>
      <title>当我导入库时，为什么我的代码会出现错误“sklearn 未定义”？</title>
      <link>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</link>
      <description><![CDATA[我的代码
//
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 pyplot
将 pickle 导入为 pk
从 sklearn 导入 linear_model
从 sklearn.utils 导入 shuffle
从 matplotlib 导入 style
data = pd.read_csv(&quot;student-mat.csv&quot;, sep=&quot;;&quot;)
data = data[[&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;studytime&quot;, &quot;failures&quot;, &quot;absences&quot;]]
print(data.head())
predict = &quot;G3&quot;
x = np.array(data.drop(predict, axis=1))
y = np.array(data[predict])
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
linear = linear_model.LinearRegression()
linear.fit(x_train, y_train)
acc = linear.score(x_test, y_test)
print(acc)
with open(&quot;studentmodel.pickle&quot;, &quot;wb&quot;) as f:
pickle.dump(linear,f)
pickle_in = open(&quot;studentmodel.pickle&quot;, &quot;rb&quot;)
linear = pickle.load(pickle_in)
print(&quot;coefficient:\n&quot;, linear.coef_)
print(&quot;intercept:\n&quot;, linear.intercept_)`

每当我运行此代码时，它都会抛出一个错误，提示名称 sklearn 未定义。但是，这很奇怪，因为我导入了正确的 sklearn 库。
错误
NameError Traceback（最近一次调用最后一次）
Cell In[1]，第 14 行
12 x = np.array(data.drop(predict, axis=1))
13 y = np.array(data[predict])
---&gt; 14 x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)
15 linear = linear_model.LinearRegression()
16 linear.fit(x_train, y_train)

NameError：名称“sklearn”未定义
]]></description>
      <guid>https://stackoverflow.com/questions/78841652/why-is-my-code-giving-error-sklearn-not-defined-when-i-have-imported-the-libra</guid>
      <pubDate>Wed, 07 Aug 2024 02:46:41 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助在 Adruino Nano RP2040 Connect 上运行 CNN 模型</title>
      <link>https://stackoverflow.com/questions/78841395/need-help-to-run-a-cnn-model-on-an-adruino-nano-rp2040-connect</link>
      <description><![CDATA[我正在尝试在 Arduino Nano RP2040 Connect 上运行一个检测咳嗽和打喷嚏的 CNN 模型（使用 Tensorflow 开发），我的模型的输入是大小为 (603,28,1) 的频谱图。但是，我对嵌入式编程还比较陌生，想得到一些关于音频处理和特征提取以及在 Arduino 上运行 CNN 模型等问题的指导。
问题 1：
在此设备上，它有一个称为 PDM 的音频库，可以捕获音频数据并读入特定大小的缓冲区数组，每个索引都是一个 16 位整数，使用此缓冲区数组，我如何将其转换为频谱图以及如何将其调整为正确的大小 (603,28,1) 以输入到我的模型中。
问题 2：
我需要在频谱图图像上运行我的模型，有没有专门用于 Arduino 的 Tensorflow 库？
此外，如果您可以让我了解您在这个项目上的实施流程，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/78841395/need-help-to-run-a-cnn-model-on-an-adruino-nano-rp2040-connect</guid>
      <pubDate>Tue, 06 Aug 2024 23:58:45 GMT</pubDate>
    </item>
    <item>
      <title>什么是Tokens、Top K、Top P？</title>
      <link>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</link>
      <description><![CDATA[我正在学习使用 Google AI Studio，在生成代码片段时，我遇到了这些术语：
constgenerationConfig = {
temperature: 1,
topP: 0.95,
topK: 64,
maxOutputTokens: 8192,
responseMimeType:&quot;text/plain&quot;,
};

我很难理解这些术语的含义。topP、topK 和 maxOutputTokens 是什么。我想了解这些，以便正确使用它们。]]></description>
      <guid>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</guid>
      <pubDate>Tue, 06 Aug 2024 22:55:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>Coral Ordinal AttributeError：'str' 对象没有属性 'name'</title>
      <link>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</link>
      <description><![CDATA[我正在开发一个使用 Python 和 Tensorflow 中的有序回归/分类的项目。我发现 pip 包 coral-ordinal 实现了有序回归并包含有用的损失函数。然而，当我浏览他们的 Google Colab 教程时，我得到了错误
AttributeError: &#39;str&#39; 对象没有属性 &#39;name&#39; 

运行 model.fit() 时。
当我尝试在其他代码中使用它时也会出现此错误。发生错误之前的代码如下
import tensorflow as tf
print(&quot;Tensorflow version&quot;, tf.__version__)

import coral_ordinal as coral
print(&quot;CORAL Ordinal version:&quot;, coral.__version__)

############################
### SETTINGS
############################

# 超参数
random_seed = 1 # 尚未使用
learning_rate = 0.05
batch_size = 128
num_epochs = 2

# 架构
NUM_CLASSES = 10

# 获取并格式化 mnist 数据
(mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data()

# 拆分验证数据集以进行早期停止
from sklearn import model_selection
mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \
model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1)

print(&quot;训练图像的形状：&quot;, mnist_images.shape)

print(&quot;训练标签的形状：&quot;, mnist_labels.shape)

print(&quot;测试图像的形状：&quot;, mnist_images_test.shape)

print(&quot;测试标签的形状：&quot;, mnist_labels_test.shape)

print(&quot;验证图像的形状：&quot;, mnist_images_val.shape)
print(&quot;验证标签的形状：&quot;, mnist_labels_val.shape)

# 也重新调整为 0-1 范围。
dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels, tf.int64)))
dataset = dataset.shuffle(1000).batch(batch_size)

test_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_test, tf.int64)))
#test_dataset = test_dataset.shuffle(1000).batch(batch_size)
# 这里我们不对测试数据集进行打乱。
test_dataset = test_dataset.batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_val, tf.int64)))
val_dataset = val_dataset.shuffle(1000).batch(batch_size)

def create_model(num_classes):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape = (28, 28, )))
model.add(tf.keras.layers.Dense(128, 激活 = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(32,activation = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.1))
# 具有一定数量的类别/等级/标签的有序输出层。
# 未指定激活函数，因此将输出累积对数。
model.add(coral.CoralOrdinal(num_classes))
return model

model = create_model(NUM_CLASSES)

# 请注意，模型生成的输出比类别数量少 1。
model.summary()

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),
loss = coral.OrdinalCrossEntropy(num_classes = NUM​​_CLASSES),
metrics = [coral.MeanAbsoluteErrorLabels()])

# 这在 CPU 上大约需要 5 分钟，在 GPU 上大约需要 2.5 分钟。
history = model.fit(dataset, epochs = 5, validation_data = val_dataset,
callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)])

如何解决这个问题？我会在他们的 github 上提出问题，但最后一次回复是 2 年前，所以我怀疑维护者是否会回复。
或者，如果没有任何现实的选择来完成这项工作，是否有任何替代方案可以帮助 tensorflow 中的有序回归/分类？]]></description>
      <guid>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</guid>
      <pubDate>Tue, 06 Aug 2024 20:41:21 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 和 Opacus 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</link>
      <description><![CDATA[在使用 Jupyter Notebook（可从此处获取）测试来自 TensorFlow 网站的示例代码时，我遇到了一个错误。您可以在此处找到我关于该错误的 SO 问题。
因此，我决定使用 PyTorch、Opacus 和 PySyft 为相同功能编写等效实现。然而，不幸的是，我又遇到了另一个错误。
下面是实现与 TensorFlow 网站中的示例代码相同功能的代码，但使用 PyTorch 和 Opacus 和 PySyft，以及错误消息。
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

# 定义一个简单的模型
class SimpleCNN(nn.Module):
def __init__(self):
super(SimpleCNN, self).__init__()
self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
self.fc1 = nn.Linear(32*26*26, 10)

def forward(self, x):
x = torch.relu(self.conv1(x))
x = x.view(-1, 32*26*26)
x = self.fc1(x)
return torch.log_softmax(x, dim=1)

# 数据加载器
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(&#39;.&#39;, train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 初始化模型、优化器和损失函数
model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.NLLLoss()

# 初始化 PrivacyEngine
privacy_engine = PrivacyEngine(
model,
batch_size=64,
sample_size=len(train_loader.dataset),
epochs=1,
max_grad_norm=1.0,
)

privacy_engine.attach(optimizer)

# 训练循环
model.train()
for epoch in range(1):
for data, target in train_loader:
optimizer.zero_grad()
output = model(data)
loss = criterion(output, target)
loss.backward()
optimizer.step()

# 打印隐私统计数据
epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)

错误：
-------------------------------------------------------------------------------
TypeError Traceback (最近一次调用最后一次)
Cell In[1]，第 32 行
29 criterion = nn.NLLLoss()
31 # 初始化 PrivacyEngine
---&gt; 32 privacy_engine = PrivacyEngine(
33 model,
34 batch_size=64,
35 sample_size=len(train_loader.dataset),
36 epochs=1,
37 max_grad_norm=1.0,
38 )
40 privacy_engine.attach(optimizer)
42 # 训练循环

TypeError: PrivacyEngine.__init__() 获得了意外的关键字参数“batch_size”
]]></description>
      <guid>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 13:17:08 GMT</pubDate>
    </item>
    <item>
      <title>如何优化机器学习模型，以最少的误报检测网络钓鱼推文？[关闭]</title>
      <link>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</link>
      <description><![CDATA[我正在研究检测网络钓鱼推文。虽然我的 ML 模型表现相当不错，但我面临的挑战是误报数量相对较多，这降低了系统的整体准确性和可用性。以下是我当前设置的详细信息：

数据集：平衡的网络钓鱼和非网络钓鱼推文数据集。
特征：推文文本、用户元数据、URL 特征。
尝试的算法：随机森林、SVM 和基本 LSTM 模型。
当前性能：准确率高，但由于误报，召回率低。
特征工程：添加了更多特征，如情绪分数和词嵌入。
模型调整：执行网格搜索以优化超参数。
集成方法：结合不同的算法来提高鲁棒性。
]]></description>
      <guid>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</guid>
      <pubDate>Tue, 06 Aug 2024 11:23:31 GMT</pubDate>
    </item>
    <item>
      <title>调用 cublasLtMatmul 时出现 RuntimeError：CUDA 错误：CUBLAS_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</guid>
      <pubDate>Tue, 06 Aug 2024 09:00:40 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 错误 - 需要使用适当的编译器标志进行重建 [关闭]</title>
      <link>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</link>
      <description><![CDATA[2024-08-06 14:18:52.654763: 
I tensorflow/core/platform/cpu_feature_guard.cc:210]。此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。

要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。

每当我尝试运行任何类型的面部识别代码时，我都会收到这种错误。即使它与面部识别无关，而只是常规的张量流，我也会收到此错误。有人能帮忙吗？
from deepface import DeepFace
import os

os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
img1 = &#39;reference.jpg&#39;
img2 = &#39;reference1.jpg&#39;

model_name = &#39;Facenet&#39;

result = DeepFace.verify(
img1_path=img1,
img2_path=img2,
model_name=model_name
)
]]></description>
      <guid>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</guid>
      <pubDate>Tue, 06 Aug 2024 08:58:22 GMT</pubDate>
    </item>
    <item>
      <title>NLTK bleu 分数明显高于 Sacrebleu bleu 分数</title>
      <link>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</link>
      <description><![CDATA[我试图将 bleu-4 分数的结果与 sacrebleu 和 NLTK corpus bleu 包进行比较，但结果之间的差异非常显著。
对于 NLTK corpus bleu，我获得了非常高的 bleu 分数（0.47、0.39、0.33、0.28）
但对于 sacrebleu，我获得了较低的分数（19.57、10.78、7.07、5.15），sacrebleu 分数已经将它们乘以 100，而 NLTK 没有
这是我计算这些分数的实现：
def compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name):
# 确保长度匹配
print(f&quot;Number of references: {len(all_references)}&quot;)
print(f&quot;假设数量：{len(all_hypotheses)}&quot;)

if len(all_references) != len(all_hypotheses):
raise ValueError(&quot;参考文献和假设的数量必须匹配。&quot;)

sacrebleu_scores = corpus_bleu(all_hypotheses, [all_references]).scores

bleu_score1 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(1.0, 0.0, 0.0, 0.0))
bleu_score2 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.5, 0.5))
bleu_score3 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.33, 0.33, 0.33))
bleu_score4 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.25, 0.25, 0.25, 0.25))


这是我生成预测的函数：
def assess_and_save(loader, dataset_type, folder_name):
model.eval()
all_references = []
all_hypotheses = []

sample_file_path = os.path.join(folder_name, &#39;samples.txt&#39;)
with open(sample_file_path, &#39;a&#39;, encoding=&#39;utf-8&#39;) as sample_file:
with torch.no_grad():
for Skeletons, Labels in loader:
Skeletons, Labels = Skeletons.to(device), Labels.to(device)

Outputs = model(skeletons, Labels[:, :-1])
Predictions = torch.argmax(outputs, dim=-1)

for i in range(predictions.size(0)):
Reference = tokenizer.decode(labels[i], skip_special_tokens=True)
Hypothesis = tokenizer.decode(predictions[i], skip_special_tokens=True)

# 调试：打印一些样本
if i &lt; 25：# 仅打印前 5 个样本
sample_text = f&quot;样本 {i+1}:\n参考：{reference}\n假设：{hypothesis}\nNew\n&quot;
print(sample_text)
sample_file.write(sample_text)

all_references.append(reference)
all_hypotheses.append(hypothesis)

# 检查是否有空引用或假设
empty_references = [ref for ref in all_references if not ref.strip()]
empty_hypotheses = [hyp for hyp in all_hypotheses if not hyp.strip()]

print(f&quot;空引用数：{len(empty_references)}&quot;)
print(f&quot;空假设数：{len(empty_hypotheses)}&quot;)

# 过滤掉空假设和相应的引用
non_empty_indices = [i for i, hyp in enumerate(all_hypotheses) if hyp.strip()]
all_references = [all_references[i] for i in non_empty_indices]
all_hypotheses = [all_hypotheses[i] for i in non_empty_indices]

compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name)


有什么建议说我哪里错了吗？
编辑：
问题是因为我没有在计算 NLTK bleu 分数之前拆分参考文献和假设。sacrebleu 的默认行为是先拆分它们。]]></description>
      <guid>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</guid>
      <pubDate>Tue, 06 Aug 2024 07:40:08 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>我们如何才能优化 Longformer 模型以提高效率，同时又不损害 NLP 任务中的长期上下文理解？</title>
      <link>https://stackoverflow.com/questions/78835795/how-can-we-optimize-longformer-models-for-efficiency-without-compromising-long-t</link>
      <description><![CDATA[Longformer 模型使用全局和局部注意力机制的混合来处理长序列，使其适合于文档分类、摘要和共指解析等任务。优化这些模型涉及平衡计算效率与维护长期上下文的需求。
可以应用哪些特定技术或修改来增强 Longformer 模型的性能？
是否有特定的训练策略、修剪方法或硬件考虑因素可以帮助实现这种平衡？深入了解 Longformer 模型已成功优化的实际实施和案例研究将非常有价值。]]></description>
      <guid>https://stackoverflow.com/questions/78835795/how-can-we-optimize-longformer-models-for-efficiency-without-compromising-long-t</guid>
      <pubDate>Mon, 05 Aug 2024 17:47:13 GMT</pubDate>
    </item>
    <item>
      <title>nltk 是适合 NLP 的优秀 Python 库吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78831441/is-nltk-good-python-library-for-nlp</link>
      <description><![CDATA[我不确定是否应该使用 TensorFlow 或 NLTK 来完成我的 NLP 任务。两者似乎都是很受欢迎的选择，但我不清楚哪一个更适合我这个水平的人。
TensorFlow 似乎是一个功能强大的库，它提供了广泛的机器学习和深度学习工具，包括对神经网络和大规模机器学习模型的支持。它似乎用途广泛，可用于复杂的 NLP 任务，如情绪分析、文本生成和翻译。但是，我不知道它对于像我这样的初学者来说是否太高级了，因为我读到过它的学习曲线很陡峭。
另一方面，NLTK（自然语言工具包）通常推荐给那些刚接触 NLP 的人。它为基本的 NLP 任务（如标记化、解析和词干提取）提供了易于使用的界面和功能。它似乎更侧重于传统的 NLP 方法，可以作为理解文本处理和分析基础知识的一个很好的起点。
我寻求指导，在转向 TensorFlow 之前，我是否应该从 NLTK 开始，在 NLP 中打下坚实的基础，或者我是否应该考虑采用其他方法。]]></description>
      <guid>https://stackoverflow.com/questions/78831441/is-nltk-good-python-library-for-nlp</guid>
      <pubDate>Sun, 04 Aug 2024 15:28:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Skip-Gram 实现产生了错误的结果？</title>
      <link>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78824197/why-is-my-skip-gram-implementation-producing-incorrect-results</guid>
      <pubDate>Fri, 02 Aug 2024 07:15:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的激活函数</title>
      <link>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</link>
      <description><![CDATA[机器学习中的激活函数是什么意思。我浏览了大多数文章和视频，每个人都提到或将其与神经网络进行比较。我是机器学习的新手，对深度学习和神经网络不太熟悉。所以，有人能给我解释一下激活函数到底是什么吗？而不是用神经网络来解释。我在学习逻辑回归的 Sigmoid 函数时就被这种模棱两可的感觉所困扰。]]></description>
      <guid>https://stackoverflow.com/questions/49391576/activation-function-in-machine-learning</guid>
      <pubDate>Tue, 20 Mar 2018 18:21:08 GMT</pubDate>
    </item>
    </channel>
</rss>