<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 27 Dec 2024 15:16:36 GMT</lastBuildDate>
    <item>
      <title>pytorch中如何在单节点单GPU系统中开发多GPU模块？</title>
      <link>https://stackoverflow.com/questions/79311997/how-to-develop-multi-gpu-modules-in-single-node-single-gpu-system-in-pytorch</link>
      <description><![CDATA[我正在开发一个多 GPU PyTorch 应用程序。torch.distributed 中的现有方法（如 scatter/gather）不能满足我的要求，因此我需要开发前向/反向传播步骤，在 GPU 之间发送和接收梯度，同时使用内置方法 scatter/gather。我可以自己做。我的最终应用程序将在多 GPU 服务器上执行。
对于开发，预算限制将我限制在单节点单 GPU 服务器上，因为我们的组织共享大型集群服务器。我遇到的一个问题是在这个单 GPU 系统中模拟多 GPU 设置。
如何在单 GPU 系统中模拟多 GPU 设置以测试这些模块？]]></description>
      <guid>https://stackoverflow.com/questions/79311997/how-to-develop-multi-gpu-modules-in-single-node-single-gpu-system-in-pytorch</guid>
      <pubDate>Fri, 27 Dec 2024 14:10:33 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证结果与混淆矩阵指标之间的差异</title>
      <link>https://stackoverflow.com/questions/79311670/discrepancy-between-cross-validation-results-and-confusion-matrix-metrics</link>
      <description><![CDATA[我有一个问题：我使用分层 10 倍交叉验证对分类模型进行 5 次重复，并报告结果。问题是，当它绘制混淆矩阵时，我手动计算矩阵中的准确度和其他指标，它们与报告的结果不同
我尝试汇总所有结果并使用完全相同的结果绘制混淆指标，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/79311670/discrepancy-between-cross-validation-results-and-confusion-matrix-metrics</guid>
      <pubDate>Fri, 27 Dec 2024 11:23:26 GMT</pubDate>
    </item>
    <item>
      <title>更改 YOLO 片段预测图像中的片段颜色</title>
      <link>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</link>
      <description><![CDATA[我正在开展一个对象检测项目，在一组图像中检测三种类型的对象。此图像显示了 yolo 片段模型预测模型示例 图像。
我遇到的问题是检测到的符号是白色的。这种颜色对某些人来说可能可见，但对其他人来说可能不可见。有没有办法改变这种颜色？我对 Python 很陌生。我已经预测了整个集合的图像（总计超过 100,000 张图像）。
我使用使用 LabelStudio 标记感兴趣的符号的图像训练了 YOLOv8 模型。在标记过程中，我使用了红色、蓝色和绿色等颜色。但不知何故，在获得最终的 YOLOv8 模型 (best.pt) 并运行预测后，检测到的符号的颜色与最初使用的颜色不同。我有以下问题：

如何更改检测到的物体的白色？

有没有办法确保在 labelstudio 上标记时使用的颜色保留在预测图像中？


import cv2
import numpy as np

# 定义一个函数来替换完整的矩形白色边界框的颜色
def replace_white_rectangles_with_green(image_path, output_path):
# 读取图像
image = cv2.imread(image_path, cv2.IMREAD_COLOR)

# 将图像转换为灰度以进行轮廓检测
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# 对灰度图像进行阈值处理以创建白色区域的二元掩码
_, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)

# 查找轮廓以检测形状
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for contour in contours:
# 近似轮廓以检查其是否形成矩形
epsilon = 0.02 * cv2.arcLength(contour, True)
approx = cv2.approxPolyDP(contour, epsilon, True)

# 检查轮廓是否有四个边（矩形或正方形）
if len(approx) == 4:
# 验证形状是否凸（以确认它是矩形/正方形）
if cv2.isContourConvex(approx):
# 将边界框的白色替换为亮绿色
cv2.drawContours(image, [contour], -1, (0, 255, 0), thicken=cv2.FILLED)

# 保存修改后的图像
cv2.imwrite(output_path, image)

# 输入和输出图像的路径
input_image_path = &quot;path_to_your_image.jpg&quot; # 替换为输入图像路径
output_image_path = &quot;path_to_save_modified_image.jpg&quot; # 替换为所需的输出路径

# 将函数应用于图像
replace_white_rectangles_with_green(input_image_path, output_image_path)

print(&quot;图像已处理，白色矩形边界框被绿色替换。&quot;)

我尝试了上述代码，但不知何故它只会随机产生绿色框或在原本为白色的原始背景上产生绿色框。]]></description>
      <guid>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</guid>
      <pubDate>Thu, 26 Dec 2024 16:15:02 GMT</pubDate>
    </item>
    <item>
      <title>对 ML 输入数据进行标准化和规范化都可以得到最佳结果，为什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79309671/both-standardizing-and-normalizing-my-input-data-for-ml-gives-the-best-results</link>
      <description><![CDATA[当我将输入数据的标准化和规范化结合用于混合 ANN 模型时，它会产生最佳结果。
但我找不到任何地方，为什么。我基于一篇论文的方法，但他们也没有证明他们的做法是合理的。
有人知道为什么吗？
与同时标准化和规范化我的输入数据相比，对我的输入数据进行标准化后，R2 小于 0.71，RMSE 更高，结果更不稳定。]]></description>
      <guid>https://stackoverflow.com/questions/79309671/both-standardizing-and-normalizing-my-input-data-for-ml-gives-the-best-results</guid>
      <pubDate>Thu, 26 Dec 2024 14:04:57 GMT</pubDate>
    </item>
    <item>
      <title>如何利用代表性模式原理提高灰度纹理分割的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</guid>
      <pubDate>Thu, 26 Dec 2024 13:48:07 GMT</pubDate>
    </item>
    <item>
      <title>需要 chromadb 和 transformers 一起使用，但要求有冲突，因为 chromadb 需要 0.20 版本的 tokenizers，而后者需要 0.21 版本</title>
      <link>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</link>
      <description><![CDATA[我必须在一个项目中同时使用 chromadb 和 transformers，但 chromadb 需要 &lt;=0.20.3 版本的 tokenizers，而 transformers 需要 &gt;=0.21 版本的 tokenizers，并且与 chromadb 兼容的旧版本 transformers 需要 rust 编译器，因此这也不是一种选择。
我尝试升级 transformers、tokenizers，也尝试降级 transformers，但都不起作用，而对于所有这些，我都在使用虚拟环境。]]></description>
      <guid>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</guid>
      <pubDate>Thu, 26 Dec 2024 11:03:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在验证数据上获得相同的准确度？[关闭]</title>
      <link>https://stackoverflow.com/questions/79308661/why-am-i-getting-the-same-accuracy-on-validation-data</link>
      <description><![CDATA[我在验证数据上获得了相同的准确率，而训练数据的准确率在每个时期变化不大。
训练数据包含 19670 张图像（14445：0 类，5225：1 类）。验证数据包含 4918 张图像（3612：0 类，1306：1 类）。
由于类别不平衡，我应用了计算类别权重，因此少数类别的惩罚更高
但是，验证数据的准确率相同，并且每个时期的损失变化不大。
我对所有训练数据应用了数据增强。此外，我使用的是 VGG16，解冻了最后 5 层，并在网络中添加了一些密集层。我改变了 learning_rate 值，但没有得到任何显着的改进，结果仍然相同。它在训练和验证数据的准确率上遵循相同的模式，没有任何改进，值重复。
这是代码：
类权重：
来自 sklearn.utils.class_weight import compute_class_weight
pesos=compute_class_weight(&quot;balanced&quot;,classes=np.unique(labels),y=labels) #这些比索与每个类相对应

#创建了一个字典，在进入时刻将参数作为 class_weight 传递给它
pesos_clases={i: pesos[i] for i in range(len(pesos))}

神经网络：
来自tensorflow.keras.applications import vgg16
VGG16=vgg16.VGG16(
weights=&quot;imagenet&quot;, 
include_top=False, 
input_shape=(224,224,3)
)

VGG16.trainable=True #Entrenable

#解冻最后 5 层
for layer in VGG16.layers[:-5]:
layer.trainable=False 

from tensorflow.keras import layer
from tensorflow import keras
x=VGG16.output
x=layers.GlobalAveragePooling2D()(x)

x=layers.Dense(1000,activation=&quot;relu&quot;)(x) #1000 个神经元
x=layers.Dropout(0.3)(x) #30% 的神经元‘去活性’

输出=layers.Dense(1,activation=“sigmoid”)(x) #la capa de salida：1个神经元
modelo=keras.Model(VGG16.inputs,output) #Creo el modelo

#Elijo el optimizador
从tensorflow.keras.optimizers导入Adam
optimizador=Adam(learning_rate=0.001) #Esto cambiardependiendo del 训练表现 --&gt; AJUSTAR (0.001 初始)

#编译步骤
模型.编译(
    优化器=优化器，#Optimizador
    损失=“binary_crossentropy”，#clasificación binaria
    指标=[“准确度”]
）

#模特表演
历史=模型.fit(
    火车发电机，
    纪元=20，
    回调=[ES],
    验证数据=val_generator，
    class_weight=pesos_clases #指定比索对应的处罚类别，主要是针对少数派的惩罚模型
）

但我得到了这些结果：
结果
我想知道为什么会发生这种情况。
我改变了超参数，例如学习率、神经元数量，并且对少数类应用了 class_weight，但没有得到任何显著的改善]]></description>
      <guid>https://stackoverflow.com/questions/79308661/why-am-i-getting-the-same-accuracy-on-validation-data</guid>
      <pubDate>Thu, 26 Dec 2024 04:26:53 GMT</pubDate>
    </item>
    <item>
      <title>分离图像内的盲文字符</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[我正在做一个将盲文转换为文本的项目。我已经编写了从图像中识别盲文点的代码，但我不知道如何将盲文分割成单元格。
这部分是识别图像中的斑点（较小的低质量图像目前不起作用）
import cv2
import numpy as np
from sklearn.cluster import KMeans

# 加载图像
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 设置 SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# 按区域过滤（斑点大小）
params.filterByArea = True
params.minArea = 100 # 根据点大小进行调整
params.maxArea = 1000

# 按圆度过滤
params.filterByCircularity = True
params.minCircularity = 0.9 # 调整点的形状

# 按凸度过滤
params.filterByConvexity = False
params.minConvexity = 0.7

# 按惯性过滤（圆度）
params.filterByInertia = True
params.minInertiaRatio = 0.95

# 使用参数创建检测器
detector = cv2.SimpleBlobDetector_create(params)

# 检测斑点
keypoints = detector.detect(image)

# 将检测到的斑点绘制为红色圆圈
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;输出图像&quot;)
cv2.imshow(&quot;输出图像&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;检测到的斑点数量：{len(keypoints)}&quot;)

以下代码将 blob 的坐标放在图形上（认为这种方式可能更容易操作）
#将图像转换为图形

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob 的坐标
rounded_coords = np.round(blob_coords).astype(int) #四舍五入的坐标

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# 基于邻近度的分组
# 如果 X 距离小于最小距离
# 如果 Y 距离小于最小距离
# 存储 X 和 Y 坐标

# 计算最小 x 和 y差异（尝试基于接近度）
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # 单元格宽度阈值
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # 单元格高度阈值
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# 绘图
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

尝试通过接近度将它们分开（位于我尝试将距离很近的物体分组到一起（我将距离很近的物体分组到一起），但我无法理解其中的逻辑。我也尝试了组聚类 (Kmeans)，但它不是很准确，并且不适用于具有不同字符数的图像，因为它需要不断知道要形成多少个簇。
# 尝试 kmeans 聚类方法
# kmeans 不起作用（无法从图像中找出簇的数量）
# 如果可以找出 nclusters，则可以工作

导入数学
从 sklearn.cluster 导入 KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # 提取 blob 的 (x, y) 位置
rounded_coords = np.round(blob_coords).astype(int) # 为简单起见，对坐标进行四舍五入

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点

ax.invert_yaxis() # 反转 Y 轴以获得类似图像的坐标
plt.title(&quot;盲文单元检测&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 将无界输入导出到 mlpackage/mlmodel 文件</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[我想创建一个 .mlpackage 或 .mlmodel 文件，可以将其导入 Xcode 进行图像分割。为此，我想使用 YOLO 中的分割包来检查它是否符合我的需求。
现在的问题是，此脚本创建的 .mlpackage 文件仅接受固定大小（640x640）的图像：
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

我想在这里进行一些更改，可能使用 coremltools，以处理无界范围（我想处理任意大小的图像）。这里有一些描述：https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges，但我不明白如何用我的脚本实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>iOS Swift 根据用户数据进行动态机器学习</title>
      <link>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</link>
      <description><![CDATA[是否可以使用 Apple ML 框架动态学习应用中的用户行为？我已经使用 Create ML 应用程序训练了一个模型，然后我可以从 iOS 设备更新并重新训练吗？这就是我目前使用该模型的方式。
public func calculateMuscleRecoveryTime(_ workout: Workout) {
do {

let config = MLModelConfiguration()
let model = try MuscleRecoveryModel(configuration: config)

let allMuscleGroups = workout.exercises
.compactMap { $0.muscles } // 展平每个锻炼的肌肉数组
.reduce(Set&lt;MuscleGroup&gt;()) { $0.union($1) } // 联合以删除重复项

let uniqueMuscleGroups = Array(allMuscleGroups)

for muscleGroup in uniqueMuscleGroups {
let trainingIntensity = Int64(workout.intensity.intValue)
let lastTrainedTimestamp = workout.date
let timeAgo = timeAgoInSeconds(from: lastTrainedTimestamp)
let muscleName = muscleGroup.rawValue.lowercased()

let prediction = try model.prediction(muscle: muscleName, intense: trainingIntensity, lastTrained: timeAgo)
}
} catch let error {
print(&quot;Error: &quot;, error)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</guid>
      <pubDate>Fri, 20 Dec 2024 01:10:59 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络进行分类</title>
      <link>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</link>
      <description><![CDATA[我正在使用 GNN 开展欺诈检测项目。我的图表以银行代码（SWIFT BIC 代码）作为节点，边表示交易。
以下是我的张量的形状：

节点特征张量形状：torch.Size([210, 6])
边缘特征张量形状：torch.Size([200, 4])
邻接矩阵张量形状：torch.Size([210, 210])
标签张量形状：torch.Size([200, 1])

我尝试了很多次，但目前正在遵循本教程：https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html
以下是我的 GNN 代码：
class GCNLayer(nn.Module):

def __init__(self, c_in, c_out):
super().__init__()
self.projection = nn.Linear(c_in, c_out)

def forward(self, node_feats, adj_matrix):
# Num neighbours = 传入边的数量
num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
node_feats = self.projection(node_feats)
print(&quot;node_feats &quot;,node_feats)
node_feats = torch.bmm(adj_matrix, node_feats)
node_feats = node_feats / num_neighbours
返回node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

使用 torch.no_grad():
out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;邻接矩阵&quot;, adjacency_matrix_tensor)
print(&quot;输入特征&quot;, node_features_tensor)
print(&quot;输出特征&quot;, out_feats)

但无论我怎么尝试，乘法过程中总是会出现维度错误：“

RuntimeError：mat1 和 mat2 形状无法相乘（210x6 和 2x2）。

我知道我们正在尝试将 node_Features_tensor (210,6) 与 adjacency_matrix_tensor (210,210) 相乘，但我已经为此困扰了好几天！
我尝试了 GNN/GCN 的多种实现。我希望能够训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</guid>
      <pubDate>Tue, 12 Mar 2024 09:08:05 GMT</pubDate>
    </item>
    <item>
      <title>选择 CNN 中的步幅和过滤器数量（Keras）</title>
      <link>https://stackoverflow.com/questions/46065445/selecting-number-of-strides-and-filters-in-cnn-keras</link>
      <description><![CDATA[我正在使用 keras 构建一个用于信号分类的 cnn 模型。在 keras 中，调整超参数和选择步长数以及滤波器数量的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/46065445/selecting-number-of-strides-and-filters-in-cnn-keras</guid>
      <pubDate>Wed, 06 Sep 2017 01:26:02 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn交叉验证过度拟合或欠拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</link>
      <description><![CDATA[我正在使用 scikit-learn cross_validation 并获得例如 0.82 平均分数 (r2_scorer)。
我如何知道使用 scikit-learn 函数时是否存在过度拟合或欠拟合？]]></description>
      <guid>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</guid>
      <pubDate>Tue, 03 Dec 2013 17:25:03 GMT</pubDate>
    </item>
    </channel>
</rss>