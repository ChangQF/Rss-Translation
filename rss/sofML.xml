<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 17 Dec 2024 15:19:29 GMT</lastBuildDate>
    <item>
      <title>学习 DSA 和系统设计的最佳平台有哪些？[关闭]</title>
      <link>https://stackoverflow.com/questions/79288290/what-are-the-best-platforms-for-learning-dsa-and-system-design</link>
      <description><![CDATA[我目前正在寻求提高我在数据结构和算法 (DSA) 和系统设计方面的技能。我的主要重点是寻找提供以下内容的资源或平台：
全面的教程和解释。
练习不同难度级别的问题。
用于编码和调试的用户友好界面。
系统设计概念的真实示例。]]></description>
      <guid>https://stackoverflow.com/questions/79288290/what-are-the-best-platforms-for-learning-dsa-and-system-design</guid>
      <pubDate>Tue, 17 Dec 2024 14:35:06 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>Databricks MLFlow 和 MetaFlow 集成</title>
      <link>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</guid>
      <pubDate>Tue, 17 Dec 2024 13:02:01 GMT</pubDate>
    </item>
    <item>
      <title>对于非常随机的文本语料库，哪些是最有效的主题建模算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</link>
      <description><![CDATA[没有关于语料库长度的信息。
没有关于任何主题层次结构的信息。
我遇到了 BERTopic，但它有 9 种不同的建模类型，哪一种应该适合？我不能使用监督或半监督，因为我没有关于数据的信息，我只知道它与 RFP（提案请求）相关。我可以预测一些主题，因此可以使用种子建模，但也会有随机主题。
我也对 LDA 等仅是句法的方法持开放态度，因为它给出了良好的结果。
我知道概括是不可能的，但想知道你的经验。
最初我尝试了 LDA，它没有给出好的结果，因为像数据长度和数据中的主题数量这样的超参数很难对如此大的完全非结构化随机数据集进行微调。]]></description>
      <guid>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</guid>
      <pubDate>Tue, 17 Dec 2024 12:25:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用python的spaCy正确识别标记的实体类型？</title>
      <link>https://stackoverflow.com/questions/79287799/how-to-correctly-identify-entity-types-for-tokens-using-spacy-using-python</link>
      <description><![CDATA[我正在使用 spaCy 从文本描述中提取和识别实体类型（如 ORG、GPE、DATE 等）。但是，我注意到一些不正确的结果，我不确定如何修复它。
这是我使用的代码：
import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def getPayeeName(description):
description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()
doc = nlp(description)

for token in doc:
print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else &#39;None&#39;}&quot;)

# 示例输入
description = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 医疗有限公司 HDFC 50200&quot;
getPayeeName（说明）

令牌：UPI，实体：ORG
令牌：DR，实体：ORG
令牌：400874707203，实体：无
令牌：BENGALORE，实体：无
令牌：08，实体：DATE
令牌：JAN，实体：DATE
令牌：2024，实体：DATE
令牌：14:38:56，实体：无
令牌：MEDICAL，实体：ORG
令牌：LTD，实体：ORG
令牌：HDFC，实体：ORG
令牌：50200，实体： ORG

50200 被识别为 ORG，但它只是一个数字。

BENGALORE 是一个城市，但它未被识别为 GPE 或位置
（返回 None）。

UPI 和 DR 是首字母缩略词/缩写，但它们被错误地
识别为 ORG。


我希望实体识别更加准确和可靠。
我该如何解决这些问题？是否有其他 spaCy 配置、自定义规则或预训练模型可用于改进实体识别？
注意：我也尝试了 ChatGPT，但这个问题仍然没有解决。]]></description>
      <guid>https://stackoverflow.com/questions/79287799/how-to-correctly-identify-entity-types-for-tokens-using-spacy-using-python</guid>
      <pubDate>Tue, 17 Dec 2024 12:09:49 GMT</pubDate>
    </item>
    <item>
      <title>如何确保 RStudio 使用我的一半内存？</title>
      <link>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</link>
      <description><![CDATA[我正在尝试使用 tidymodels 在 RStudio 上调整机器学习模型。
我有 Macbook Pro 2019

2.3 GHz 8 核 Intel Core i9，
32 GB 2667 MHz DDR4

对于调整 KNN 回归，它花费了 10 多个小时，我不明白为什么。以下是代码：
knn_model &lt;-
nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%
set_engine(&#39;kknn&#39;) %&gt;%
set_mode(&#39;regression&#39;)

knn_grid &lt;-
grid_regular(
neighbours(),
weight_func(),
dist_power(),
levels = c(20, 5, 5)
)

knn_wf &lt;-
working() %&gt;%
add_model(knn_model) %&gt;%
add_formula(demande_energetique_projectee ~ .)

knn_res &lt;-
knn_wf %&gt;%
tune_grid(
resamples = folds,
grid = knn_grid,
metrics = metric_set(rmse)
)
knn_res

我检查了分配给 rstudio 的内存；它不超过 1.2Gb。但为什么呢？
为什么它没有使用所有内存来加快我的超参数调整速度？
经过一番研究，我在主文件夹中创建了 .Renviron 文件并将其放入
R_MAX_VSIZE=16Gb

并重新启动了 RStudio，但问题并未解决。
以下是有关会话的信息
R 版本 4.3.3 (2024-02-29)
平台：x86_64-apple-darwin20 (64 位)
运行于：macOS 15.1.1

问题：

如何加快超参数调整速度？
我们如何确保 RStudio 使用一半的内存而不是仍然阻塞最大 1.2Gb？
]]></description>
      <guid>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</guid>
      <pubDate>Tue, 17 Dec 2024 08:11:39 GMT</pubDate>
    </item>
    <item>
      <title>如何修复使用 Prompt Flow 时出现的“错误：pip 的依赖解析器当前未考虑已安装的所有软件包。”</title>
      <link>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</link>
      <description><![CDATA[我制作了一个自定义映像，以在 Azure Ai Foundry 的 Prompt Flow（早期的 Ai Studio）上使用 python 的 3.10.1 版本。忽略错误，Flow 成功运行。

错误：pip 的依赖解析器当前未考虑已安装的所有软件包。此行为是以下依赖冲突的根源。mlflow 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。mlflow-skinny 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。

但是，我认为这会在最终的生产部署中造成麻烦。此外，我还检查了我的自定义图像上的 protobuf 版本，但版本号是 4.25.5，在这种情况下应该可以正常工作。下面是错误和 docker 容器的屏幕截图。

我在执行 find 部署时遇到的错误如下：

根据用于故障排除此错误的文档，错误为 ResourceNotReady。其中提到了 score.py 文件。我想知道什么是 score.py 文件，这个文件在部署时会自动生成吗？还是需要在自定义镜像时单独创建这个文件？最后我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</guid>
      <pubDate>Tue, 17 Dec 2024 07:12:40 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能提高决策树的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</link>
      <description><![CDATA[这是我的代码
# 准备目标和特征
target_column = &#39;resolution&#39;
X = data.drop(columns=[target_column])
y = data[target_column]

# 根据需要将分类数据转换为二进制/数字
X_encoded = pd.get_dummies(X) # 对分类特征进行独热编码
le = LabelEncoder()
y_encoded = le.fit_transform(y) # 编码目标变量

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)

# 执行网格搜索以调整 max_depth、min_samples_split、min_samples_leaf 和 class_weight
param_grid = {
&#39;max_depth&#39;: range(1, 11), # 要测试的 max_depth 范围
&#39;min_samples_split&#39;: [2, 5, 10], # min_samples_split 的选项
&#39;min_samples_leaf&#39;: [1, 2, 4],
&#39;class_weight&#39;: [&#39;balanced&#39;]# min_samples_leaf 的选项
}
model = DecisionTreeClassifier(random_state=42)

# 使用 5 倍交叉验证初始化 GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2,scoring=&#39;accuracy&#39;) # 使用准确度进行分类
grid.fit(X_train, y_train)

# 来自网格搜索的最佳参数
best_params = grid.best_params_
# print(&quot;来自网格搜索的最佳参数：&quot;, best_params)

# 评估测试集上的最佳模型
best_model = grid.best_estimator_
test_accuracy = best_model.score(X_test, y_test) * 100 # 准确率百分比
# print(f&quot;最佳模型的测试准确率：{test_accuracy:.2f}%&quot;)

# 显示决策树的结构
# tree_structure = tree.export_text(best_model, feature_names=list(X_encoded.columns))
# print(&quot;\n决策树结构：&quot;)
# print(tree_structure).

这是我的结果
分类报告：
准确率 召回率 f1 分数 支持率

结果 1 0.39 0.88 0.54 388
结果 2 0.95 0.61 0.75 1397

准确率 0.67 1785
宏平均值 0.67 0.75 0.64 1785
加权平均值 0.83 0.67 0.70 1785


混淆矩阵：
[[343 45]
[540 857]]

“结果 1”类的准确率：88.40%
“结果2&#39;：61.35%
决策树分类器的准确率：0.67
我希望整体准确率和“结果 2”更高。我认为问题在于结果 1 在这里是少数，出于某种原因，决策树的准确率因此降低，尽管它应该是平衡的。]]></description>
      <guid>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</guid>
      <pubDate>Mon, 16 Dec 2024 20:55:17 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能修复破损的轮廓？[关闭]</title>
      <link>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</link>
      <description><![CDATA[我正在研究图像分割，但轮廓不太正确。这是我经过一些图像清理和轮廓检测后得到的结果，但正如您所见，轮廓在某些地方被破坏了。有办法解决这个问题吗？我偶然发现了这篇论文，DiSTNet2D，其中您应该训练神经网络进行分割。我不太确定这是否有点过头了。有人有更好的建议吗？我已附上我的代码和结果
def gaussian_filter_multiscale_retinex(image: np.ndarray, sigmas: list, weights: list) -&gt; np.ndarray:
img32 = image.astype(&#39;float32&#39;) / 255
img32_log = np.log1p(img32)

msr = np.zeros(image.shape, np.float32)
for sigma, weight in zip(sigmas, weights):
blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
blur_log = np.log1p(blur)
msr += (img32_log - blur_log) * weight

msr /= sum(weights)
return cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)

def process_image(img_path):
img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)
rtnx = gaussian_filter_multiscale_retinex(img, sigmas=[15, 55, 185], weights=[10, 5, 1])
阈值 = cv.adaptiveThreshold(rtnx, 255, adaptableMethod=cv.ADAPTIVE_THRESH_GAUSSIAN_C,
阈值类型=cv.THRESH_BINARY, blockSize=7, C=-7)

nb_components, output, stats, _ = cv.connectedComponentsWithStats(thresholded, connections=8)
大小 = stats[1:, -1]
new_img = np.zeros_like(thresholded)
new_img[np.isin(output, np.where(sizes &gt;= 12)[0] + 1)] = 255

numLabels、labels、stats、centroids = cv.connectedComponentsWithStats(new_img)


]]></description>
      <guid>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</guid>
      <pubDate>Mon, 16 Dec 2024 17:08:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 aiplatform.BatchPredictionJob.create() 在 Vertex AI 中配置模型监控？</title>
      <link>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</link>
      <description><![CDATA[在使用 aiplatform SDK 设置 Vertex AI 模型监控时，我遇到了一个问题，特别是在配置 BatchPredictionJob.create() 时。文档不清楚，缺少定义 model_monitoring_objective_config 和 model_monitoring_alert_config 的示例。由于引用不完整和参数映射不清楚，这导致了兼容性问题。
主要混淆是因为这些配置在 SDK 中没有很好的记录，需要探索 SDK 源代码才能了解正确的结构。此外，SDK 需要来自 aiplatform.model_monitoring 的配置，这在官方指南中没有明确说明。
我尝试过的方法：
我最初参考了 SDK 文档，并根据看似合乎逻辑的内容尝试了各种配置，假设所有组件都可以直接使用 SDK 的类进行配置。但是，这会导致多个类型和参数不匹配错误。
我的预期：
我预期清晰明了的文档，展示如何在使用 aiplatform.BatchPredictionJob.create() 创建批量预测作业时定义和传递监控配置。
实际发生的情况：
我遇到了由于 SDK 和 GAPIC API 之间的参数不匹配而导致的错误。在探索源代码后，我意识到必须使用 aiplatform.model_monitoring 类定义所需的配置。这种跨库依赖关系没有记录。]]></description>
      <guid>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</guid>
      <pubDate>Mon, 16 Dec 2024 07:45:27 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 停留在图像生成上</title>
      <link>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</link>
      <description><![CDATA[我创建了一个 LSTM 来生成序列中的下一张图像（我知道 CNN 是用于图像生成的，但我需要整个图像，而不仅仅是提供给序列下一次迭代的过滤器）。所以我有一个数据集，它包含图像（电影中的帧），我创建了它的序列，就像 1 个场景包含例如。 n 个图像，我有 s 个序列长度，那么输入将是 image_1 到 image_s，输出是 image_s+1，下一个输入是 image_2 到 image_s+1，输出是 image_s+2，依此类推。
模型如下：
class LSTM(nn.Module):
def __init__(self, input_len, hidden_​​size, num_layers):
super(LSTM, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_len, hidden_​​size, num_layers, batch_first=True)
self.output_layer = nn.Linear(hidden_​​size, input_len)
self.dropout = nn.Dropout(.2)

def forward(self, X):
hidden_​​states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
out, _ = self.lstm(X, (hidden_​​states, cell_states))
out = self.dropout(out)
out = self.output_layer(out[:, -1, :])
return out

训练是：
def train(num_epochs, model, loss_func, optimizer):
total_steps = loader.getSizeWithBatch()

for epoch in range(num_epochs):
loader.reset()
for item in range(total_steps-1):
element = loader.next()[0]
x_images,y_image = element
x_images = x_images.reshape(-1,sequence_len,input_len)
output = model(x_images)
y_image = y_image.reshape(-1,input_len)
loss = loss_func(output, y_image)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if (item + 1) % 1 == 0:
print(f&#39;Epoch: {epoch + 1};批次：{item + 1} / {total_steps};损失：{loss.item():&gt;4f}&#39;)

if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;model_save_interval&#39;]) == 0:
if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;clean_save_interval&#39;]) == 0:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;] + str(epoch+1)))
else:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;]))

Loader 以张量的形式引导图像由于内存使用，从文件中预先排序。
我使用 MSE 损失和 Adam 作为优化器。
问题是，当我训练它时，错误达到 0.003，这是目标，因为我通过将它们除以 255 来规范化值，但是当我预测时，它会产生一种模糊的场景图像，并且无论输入如何，预测图像始终相同，即使输入来自其他场景，它也会创建相同的图像，当我减去不同输出图像的颜色值时，该值为 0，因此每个输出图像都完全相同。
最终结果看起来就像我将数据集中的每个图像都作为层放在一起一样
我尝试添加 Droput，增加隐藏大小的神经元（现在是 128），尝试增加层数，不同的时期会创建相同的图像，只是模糊程度略低一些，但效果是一样的，我将学习率从 .001 降低到 .0001，效果都是一样的]]></description>
      <guid>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</guid>
      <pubDate>Sun, 15 Dec 2024 20:28:51 GMT</pubDate>
    </item>
    <item>
      <title>获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</link>
      <description><![CDATA[我正在做一个机器学习项目，在 Jupyter Notebook 上预测电动汽车的价格。
我运行这些单元：
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
x[col] = le.transform(x[col]) 
print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5，random_state = 0)

r2_score(y_test，lm.predict(x_test))

从 sklearn.tree 导入 DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train，y_train)
r2_score(y_test，regressor.predict(x_test))

r2_score(y_train，regressor.predict(x_train))

uv = np.nanpercentile(df2[&#39;Base MSRP&#39;]，[99])[0]*2

df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv

df2 = df2[df2[&#39;Model Year&#39;] != &#39;N/&#39;] # 过滤掉包含 &#39;Model Year&#39; 的行&#39;N/&#39;

for col in cols:
df2[col] = df2[col].replace(&#39;N/&#39;, -1)
le.fit(df2[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

le = preprocessing.LabelEncoder()

cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]

for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError回溯（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 返回 _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
以下是数据集的链接：
https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用具有动态尺寸输入的 Dense 层？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</link>
      <description><![CDATA[我有一个模型，其输入（一批具有形状（高度、宽度、时间）的图像）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Dense 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度的输入（无）
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用密集层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：密集层输入的最后一个维度应已定义。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？本质上，我正在寻找一种方法来创建一个具有原始像素值的 1D 数组，但与 Dense 层兼容。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>为 GPR 创建自定义内核</title>
      <link>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</link>
      <description><![CDATA[我想编写一个仅在 X 轴特定范围内工作的 RBF 内核。我尝试编写一个包含 RBF 核的类来测试代码
class RangeLimitedRBFTest(Kernel):
def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), x_min = 0., x_max = 1.):
self.length_scale = length_scale
self.length_scale_bounds = length_scale_bounds
self.rbf_kernel = RBF(length_scale, length_scale_bounds)
self.x_min = x_min
self.x_max = x_max

def __call__(self, X, Y=None, eval_gradient=False):
if eval_gradient and Y is not None:
raise ValueError(&quot;Gradient can only be evaluating when Y is None.&quot;)

X = np.atleast_2d(X)
if Y is not None:
Y = np.atleast_2d(Y)

print(f&quot;X 形状：{X.shape}&quot;)
如果 Y 不为 None:
print(f&quot;Y 形状：{Y.shape}&quot;)
else:
print(&quot;Y 形状：None&quot;)

K_rbf = self.rbf_kernel(X, Y, eval_gradient=eval_gradient)

如果 eval_gradient:
K, K_grad = K_rbf
print(f&quot;核矩阵形状 (K): {K.shape}&quot;)
print(f&quot;核梯度矩阵形状 (K_grad): {K_grad.shape}&quot;)
return K, K_grad
else:
K = K_rbf
return K

def diag(self, X):
return self.rbf_kernel.diag(X)

def is_stationary(self):
return self.rbf_kernel.is_stationary()

实现和拟合如下
kernel = 1.0 * RangeLimitedRBFTest(length_scale=0.1, length_scale_bounds=(8e-2, 8e-1), x_min=0., x_max=2.5) + WhiteKernel(noise_level=0.5, noise_level_bounds=(1e-2, 1e1))
gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1, alpha=1e-5, optimizer=&#39;fmin_l_bfgs_b&#39;)
gaussian_process.optimizer_kwargs = {&quot;max_iter&quot;: 10000} 
gaussian_process.fit(X, T_PMT)

如果我运行代码，我会得到以下输出
X 形状：(6248, 1)
Y 形状：无
核矩阵形状 (K)：(6248, 6248)
核梯度矩阵形状 (K_grad)：(6248, 6248, 1)
ValueError：第 0 维必须固定为 2，但得到 3

上述异常是导致以下异常的直接原因：

回溯（最近一次调用）：
文件 &quot;/home/tdaq/cremonini/pt100_probe/read_temperatures.py&quot;，第 97 行，位于 &lt;module&gt;
gaussian_process.fit(X, T_PMT)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/base.py&quot;，第 1389 行，在包装器中
return fit_method(estimator, *args, **kwargs)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 308 行，在 fit 中
self._constrained_optimization(
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 653 行，在 _constrained_optimization 中
opt_res = scipy.optimize.minimize(
文件&quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_minimize.py&quot;，第 713 行，在 minimal
res = _minimize_lbfgsb(fun, x0, args, jac, bounds,
文件 &quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py&quot;，第 360 行，在_minimize_lbfgsb
_lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
ValueError: 无法将 _lbfgsb.setulb 的第 7 个参数“g”转换为 C/Fortran 数组

如果我尝试使用通常的 RBF 内核，代码可以正常工作。我还尝试禁用优化器 optimizer=None，代码可以正常工作，但会出现非常大的错误。]]></description>
      <guid>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</guid>
      <pubDate>Wed, 11 Dec 2024 11:00:16 GMT</pubDate>
    </item>
    <item>
      <title>分析客户支持单，了解产品缺陷/特点</title>
      <link>https://stackoverflow.com/questions/66428112/analyze-customer-support-tickets-to-understand-product-gaps-features</link>
      <description><![CDATA[我希望分析客户支持单，以了解产品差距/功能或我可以对产品进行哪些改进以解决客户痛点/问题。
但您知道，客户支持单中有很多文本/注释，这些文本/注释是由我们的支持代理通过电子邮件或电话收集的，并且从人的角度来说，不可能浏览所有单据并了解全局。
我正在从 Stack Overflow 上的开发人员那里寻求有关如何处理分析客户支持单以了解产品差距/功能或客户痛点的问题的想法。
您能给我指明正确的方向吗？我们可以使用 NLP 或任何其他 ML 概念来解决问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/66428112/analyze-customer-support-tickets-to-understand-product-gaps-features</guid>
      <pubDate>Mon, 01 Mar 2021 19:04:28 GMT</pubDate>
    </item>
    </channel>
</rss>