<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Jan 2024 03:16:24 GMT</lastBuildDate>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于如何区分所拍摄的图像/照片是真实世界物体而不是其他图像/照片的指示吗？</title>
      <link>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</link>
      <description><![CDATA[听起来可能是一个愚蠢的想法，但我有这个想法来拍摄真实世界物体、风景等的图像/照片。但不太确定我可以使用什么处理或分类技术来区分这张照片是否来自现实世界或取自其他照片、图像等。提前致谢。
我尝试在网上查找，但找不到任何相关资源，或者至少找不到我想要的方向。]]></description>
      <guid>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</guid>
      <pubDate>Fri, 19 Jan 2024 02:47:33 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[运行以下代码时：
`类 NLayerDiscriminator(nn.Module):
def init(self, input_nc, ndf=64, n_layers=3,norm_layer=nn.BatchNorm2d, use_sigmoid=False):
super(NLayerDiscriminator, self).init()
self.n_layers = n_layers
&lt;前&gt;&lt;代码&gt; kw = 4
    padw = int(np.ceil((kw-1.0)/2))
    self.conv1=nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw)
    self.act=nn.LeakyReLU(0.2, True)
    
    nf=min(ndf*2,512)
    self.conv2=nn.Conv2d(ndf, nf, kernel_size=kw, stride=2, padding=padw)
    self.norm1=norm_layer(nf)
    
    ngf=min(nf*2,512)
    self.conv3=nn.Conv2d(nf, ngf, kernel_size=kw, stride=1, padding=padw)
    self.norm2=norm_layer(ngf)
    self.conv4=nn.Conv2d(ngf, 1, kernel_size=kw, stride=1, padding=padw)
    self.sig=nn.Sigmoid()
    

def 前向（自身，输入）：
    x=self.conv1(输入)
    x=self.act(x)
    
    对于范围 (1, 3) 中的 n：
        x=self.conv2(x)
        x=self.norm1(x)
        x=self.act(x)
    
    x=self.conv3(x)
    x=self.norm(x)
    x=self.act(x)
    x=self.conv4(x)
    如果使用_sigmoid：
        x=self.sig(x)
    返回x
            `

我收到以下错误
RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 算法与入口点文件问题</title>
      <link>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</link>
      <description><![CDATA[我正在尝试创建 awsgluespark 作业来训练其中一个数据集。我在1.3-1版本中使用xgboost算法。当我尝试运行估算器时，我遇到了问题
基础设施：awsglue 4.00 Spark shell
所有文件夹都是s3路径
代码片段。
xgb_script_mode_estimator = XGBoost(
    入口点=“训练.py”，
    超参数=超参数，
    角色=角色，
    实例计数=1，
    实例类型=实例类型，
    Framework_version =“1.3-1”，
    output_path=“s3://{}/{}/{}/output”.format(hyperparameters[&#39;bucket_nm&#39;], &#39;/output/&#39;, job_name),
   

错误：
FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;training.py&#39;
我将“粘合脚本”放置在和 Training.py 与 init.py 文件位于同一文件夹中的同一作业存储桶中。
XGBoost 函数无法识别同一文件夹中的training.py（训练文件没有名称不匹配，包括大小写）]]></description>
      <guid>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</guid>
      <pubDate>Fri, 19 Jan 2024 00:43:36 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：LLMChain 出现 2 个验证错误</title>
      <link>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</link>
      <description><![CDATA[这是我的完整代码：
!pip install -q Transformers einops 加速 langchain BitsandBytes Sentence_Transformers faiss-cpu pypdf Sentpiece
从 langchain 导入 HuggingFacePipeline
从 Transformer 导入 AutoTokenizer
从 langchain.embeddings 导入 HuggingFaceEmbeddings
从 langchain.document_loaders.csv_loader 导入 CSVLoader
从 langchain.vectorstores 导入 FAISS、Chroma
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate
从 langchain.chains 导入 ConversationalRetrievalChain
从 langchain.chains.question_answering 导入 load_qa_chain
从 langchain.memory 导入 ConversationBufferMemory
进口加速
进口变压器
进口火炬
导入文本换行
loader = CSVLoader(&#39;/kaggle/input/csvdata/chatdata.csv&#39;, 编码=“utf-8”, csv_args={&#39;分隔符&#39;: &#39;,&#39;})
数据 = 加载器.load()

嵌入 = HuggingFaceEmbeddings(model_name=&#39;sentence-transformers/all-MiniLM-L6-v2&#39;,model_kwargs={&#39;device&#39;: &#39;cpu&#39;})

db = FAISS.from_documents(数据，嵌入)


#Mistral 7B 模型 llm

进口火炬
从变压器进口（
    AutoModelForCausalLM，
    自动标记器，
    生成配置，
    文本流媒体,
    管道，
）

MODEL_NAME =“mistralai/Mistral-7B-Instruct-v0.1”

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
模型 = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME、device_map=“自动”、torch_dtype=torch.float16、load_in_8bit=True
）

Generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
Generation_config.max_new_tokens = 1024
Generation_config.温度 = 0.0001
Generation_config.do_sample = True
流光= TextStreamer（标记器，skip_prompt = True，skip_special_tokens = True）


llm = 管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    return_full_text=真，
    Generation_config = Generation_config，
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    流光=流光，
）


def format_prompt（提示，system_prompt =“”）：
    如果 system_prompt.strip():
        return f“[INST] {system_prompt} {prompt} [/INST]”
    return f“[INST] {提示} [/INST]”


SYSTEM_PROMPT = “””
您是一名临床数据科学家和数据分析师，专门从事统计数据分析和报告生成。您的使命是为医疗保健和临床研究提供准确且富有洞察力的数据驱动解决方案。当您做出回应时，请发挥临床数据科学领域经验丰富的数据专业人员所特有的专业知识和精确度。
如果您遇到没有必要信息的问题，请务必不要提供推测性或不准确的答案。
“”“”.strip()

链 = ConversationalRetrievalChain.from_llm(
    嗯，
    chain_type=“东西”，
    检索器=db.as_retriever(),
    return_source_documents=真，
    详细=真，
）

这里我面临错误：
ValidationError：LLMChain 出现 2 个验证错误
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）


从 textwrap 导入填充

结果=链（输入（“临床试验平面计ChatBot ---”）
）
打印（填充（结果[“结果”].strip（），宽度= 80））

此 llm 链被编程为使用 llm、矢量数据库和提示与 csv 聊天，我在运行 ConversationalRetrievalChain 时遇到上述错误]]></description>
      <guid>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</guid>
      <pubDate>Thu, 18 Jan 2024 20:20:48 GMT</pubDate>
    </item>
    <item>
      <title>当输入任何模型时，TFRecord 文件使用过多 RAM</title>
      <link>https://stackoverflow.com/questions/77841705/excessive-ram-usage-for-tfrecord-file-while-feeding-into-any-model</link>
      <description><![CDATA[问题
我有一个 *.tfrecords 文件，我想将其输入到使用 Tensorflow 创建的 ConvLSTM2D 模型中。这是模型结构。
模型 = 顺序([
    ConvLSTM2D(64, (3, 3), 激活=&#39;relu&#39;, input_shape=(20, 224, 224, 3), return_sequences=True),
    批量归一化(),
    ConvLSTM2D(64, (3, 3), 激活=&#39;relu&#39;, return_sequences=True),
    批量归一化(),
    展平（），
    密集（1，激活=&#39;sigmoid&#39;）
]）

当我尝试将数据放入模型中时，它占用了所有系统内存。
在 M1 MacBook 2020（Jupyter Notebook、Pycharm）、Google Colab 上测试。
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit(train_input_fn(),steps_per_epoch=5,validation_data=val_input_fn(),epochs=10)

我们在做什么
我们有上海数据集，其中包含战斗和非战斗数据集。因此，我们尝试使用卷积长短期记忆来预测和分类打斗和非打斗视频。
我们有 800 个火车视频。我们以 250 毫秒的间隔捕获帧并将所有帧转换为 numpy 数组。然后我们将所有数组存储在 TFRecord 文件中。
当我们将数据集传递到模型中时，我们使用这个函数 train_input_fn() 来读取 tfrecord 文件并将数据传递到我们的模型中。
&lt;小时/&gt;
您可以在此处查看 Colab 笔记本
数据集结构如下：
数据集
    - 火车
        - Fight # 有 800 个 *.avi 文件
        - NonFight # 有 800 个 *.avi 文件
    - 值
        - Fight # 有 200 个 *.avi 文件
        - NonFight # 有 200 个 *.avi 文件


我们尝试了什么？

我们尝试将 batch_size 从 64 减少到 16。
将整个数据集从训练集中的 800 个视频减少到 200 个视频
尝试减小 ConvLSTM2D 的滤波器大小
对 *.mp4 做了所有相同的事情
减少了一层 BatchNormalization() 和 ConvLSTM2D
]]></description>
      <guid>https://stackoverflow.com/questions/77841705/excessive-ram-usage-for-tfrecord-file-while-feeding-into-any-model</guid>
      <pubDate>Thu, 18 Jan 2024 18:36:04 GMT</pubDate>
    </item>
    <item>
      <title>nn.参数在训练期间不更新[关闭]</title>
      <link>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</link>
      <description><![CDATA[我正在尝试编写 PyGeometric MessagePassing 层，如下所示：
类 EdgeNNLayer(MessagePassing):
    def __init__(自身, d0=45/60):
        super().__init__(aggr=&#39;mean&#39;)
        self.d0 = d0
        self.beta = 参数(torch.empty(1))
        self.reset_parameters()
        self.weight_aggr = aggr.SumAggregation()
    
    def重置参数（自身）：
        print(&quot;重置参数&quot;)
        self.beta.data[0] = 1 / self.d0
    
    def 消息（自身，x_i，x_j，edge_attr）：
        beta = self.beta.clone()
        返回 x_j * torch.exp(-beta * edge_attr[:, 0:1]), torch.exp(-beta * edge_attr[:, 0:1])
    
    defforward（自身，x，edge_index，edge_attr）：
        edge_index，edge_attr = add_self_loops（edge_index，edge_attr，fill_value = 0，num_nodes = x.shape [0]）
        返回 self.propagate(edge_index, x=x, edge_attr=edge_attr)
        
    def 聚合（自我，输入，索引，ptr =无，dim_size =无）：
        值、权重 = 输入
        return self.aggr_module(vals,index,ptr=ptr,dim_size=dim_size,dim=self.node_dim)# / self.weight_aggr(权重,index,ptr,dim_size)

当我训练该层（使用该层更新 PyG 图后使用的一些其他 Linear 层）时，我看到网络的所有其他可训练元素均由 beta&lt; 更新/code&gt; 在上面的层中不是。
我尝试将上面的 __init__ 中的行更改为：
 self.beta = 参数(torch.empty(1))
        self.beta.retain_grad()
        打印(self.beta.retains_grad)
        打印（自我.beta.grad）

我得到False和None，这很奇怪。当我在每个纪元后打印它时，我也会得到这些。
我尝试以几种不同的方式更改代码，包括更改克隆、注释掉几行、更改该层所使用的整体网络结构等。]]></description>
      <guid>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</guid>
      <pubDate>Thu, 18 Jan 2024 18:34:13 GMT</pubDate>
    </item>
    <item>
      <title>ML .NET 训练异常 [关闭]</title>
      <link>https://stackoverflow.com/questions/77840868/ml-net-exception-on-training</link>
      <description><![CDATA[我正在使用模型生成器 UI，选择 NLP 文本分类。
目标是尝试“清洁”数据丑陋，所以我制作了一个金融交易的样本数据集。这个想法是输入 Amazon Digit*8SDFS98 amzn.com/bill WA Am 应该输出：“Amazon Digital”。
但是我在训练后收到此错误。我认为数据有问题（除了它根本不是一个大数据集，但它都是模拟数据，只是为了学习这些东西）。
在此处输入图像描述
重现步骤。

使用此数据（或喜欢它）创建 csv。
打开 VS 并添加机器学习模型
选择“文本分类”如场景所示。
选择清理后的描述作为标签（目标）
选择“描述”作为数据
点击“下一步”去火车。
开始训练。
收到错误。

有没有办法从这个错误中找出实际问题是什么？
 位于 System.RuntimeMethodHandle.InvokeMethod（对象目标、对象[] 参数、签名 sig、布尔构造函数）
   在 System.Reflection.RuntimeMethodInfo.UnsafeInvokeInternal（对象 obj，对象 [] 参数，对象 [] 参数）
   在System.Reflection.RuntimeMethodInfo.Invoke（对象obj，BindingFlags invokeAttr，Binder活页夹，Object []参数，CultureInfo文化）
   在 Microsoft.ML.Runtime.ComponentCatalog.LoadableClassInfo.CreateInstanceCore(Object[] ctorArgs)
   在 Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes]（IHostEnvironment env，类型签名类型，TRes&amp;结果，字符串名称，字符串选项，Object[] 额外）
   在 Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes,TSig](IHostEnvironment env, TRes&amp; 结果，字符串名称，字符串选项，Object[] extra)
   在 Microsoft.ML.ModelLoadContext.TryLoadModelCore[TRes,TSig](IHostEnvironment env, TRes&amp; result, Object[] extra)
   在 Microsoft.ML.ModelLoadContext.TryLoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、Entry ent、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、Entry ent、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModelOrNull[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、String dir、Object[] extra)
   在 Microsoft.ML.ModelOperationsCatalog.Load（流流、DataViewSchema&amp; inputSchema）
   在 Microsoft.ML.ModelOperationsCatalog.Load(String filePath, DataViewSchema&amp; inputSchema)
   在 /_/src/Microsoft.ML.ModelBuilder.AutoMLService/ServiceFactory/CodeGeneratorService 中的 Microsoft.ML.ModelBuilder.AutoMLService.ServiceFactory.CodeGeneratorService.SetTorchRunTimeFolderAndLoadModel（ITrainingConfiguration 配置、字符串 modelPath、MLContext 和上下文、ITransformer 和模型、DataViewSchema 和 inputSchema） .cs：第 139 行
   在 /_/src/Microsoft.ML.ModelBuilder 中的 Microsoft.ML.ModelBuilder.AutoMLService.ServiceFactory.CodeGeneratorService.GenerateConsumationAsync（ITrainingConfiguration 配置、字符串trainingConfigurationFolder、字符串nameSpace、字符串className、TargetType 目标、String[] 标签、CancellationToken ct）。 AutoMLService/ServiceFactory/CodeGeneratorService.cs：StreamJsonRpc.JsonRpc 处的第 155 行。d__151`1.MoveNext()
--- 从先前抛出异常的位置开始的堆栈跟踪结束 ---
   在 System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   在 System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification（任务任务）
   在 System.Runtime.CompilerServices.TaskAwaiter`1.GetResult()
   在 Microsoft.ML.ModelBuilder.ViewModels.TrainViewModel.d__100.MoveNext()

我尝试清理数据（没有重复值，两列中都没有空格）。]]></description>
      <guid>https://stackoverflow.com/questions/77840868/ml-net-exception-on-training</guid>
      <pubDate>Thu, 18 Jan 2024 16:14:56 GMT</pubDate>
    </item>
    <item>
      <title>模型遭受巨大损失[关闭]</title>
      <link>https://stackoverflow.com/questions/77839031/model-getting-huge-loss</link>
      <description><![CDATA[我正在尝试按标题制作文本生成器。文本最大长度为2500，词典大小为45,000字。这是我正在使用的模型。在训练过程中，损失增加，但准确率保持不变。
那么，我的模型出了什么问题？
&lt;前&gt;&lt;代码&gt;纪元 1/75
1311/1311 [================================] - 461s 347ms/步 - 损失：27230606.0000 - 准确度：0.0382
纪元 2/75
1311/1311 [================================] - 454s 346ms/步 - 损失：78650848.0000 - 准确度：0.0382


我在 anaconda 环境中使用 Tensorflow GPU 和 python 3.11。
模型 = keras.Sequential([
    嵌入（vocab_size，256，input_length = max_sequence_length），
    LSTM（单位= 256，kernel_regularizer = l2（0.01），return_sequences = True），
    LSTM（单位= 128，kernel_regularizer = l2（0.01）），
    密集（max_sequence_length，激活=&#39;softmax&#39;）
]）

亚当 = 亚当(lr=0.01)

model.compile(optimizer=adam,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit（标题_序列_填充，食谱_序列_填充，纪元= 75，详细= 1，
          回调=[ModelCheckpoint(filepath=Settings.new_model_path)])

我尝试增加单位数量、损失类型​​，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/77839031/model-getting-huge-loss</guid>
      <pubDate>Thu, 18 Jan 2024 11:28:10 GMT</pubDate>
    </item>
    <item>
      <title>如何从数据生成 if-else 条件或树状结构或隐藏规则</title>
      <link>https://stackoverflow.com/questions/77838790/how-to-generate-if-else-conditions-or-tree-like-structure-or-hidden-rules-from-d</link>
      <description><![CDATA[假设我们生活在一个孩子的名字是根据其父母和祖父母的详细信息确定的世界。我想从这样一个给定的数据集中找到所有规则。
假设我有给定的数据：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

父亲
妈妈
奶奶
爷爷
父亲年龄
母亲年龄
孩子姓名


&lt;正文&gt;

蒂姆
帕特里夏

延斯


标记


蒂姆
奥利维亚
海伦娜


37
标记


蒂姆

凯茜
罗尔夫

45
标记


山姆
百合

鲁道夫
36

莎莉


山姆
百合
辛迪
标记
40
42
莎莉


乔治
布伦达
百合
一月
27
28
标记


乔治
波莉
多莉
肯
38

奥利维亚




我想找到这样的规则：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

规则
输出孩子名字


&lt;正文&gt;

父亲：蒂姆
标记


父亲：山姆+母亲：莉莉
莎莉


父亲：乔治 + 母亲：布伦达 + 祖母：莉莉 + 祖父：简 + 父亲年龄：27 + 母亲年龄：28
标记


父亲：乔治 + 母亲：波莉 + 奶奶：多莉 + 爷爷：肯 + 父亲年龄：38 + 母亲年龄：空白
奥利维亚




我的数据很大，输入列更多，所以我无法使用暴力方法。我的限制是在 Python 3.10 中实现它。有什么方法可以不用暴力破解这个程序吗？]]></description>
      <guid>https://stackoverflow.com/questions/77838790/how-to-generate-if-else-conditions-or-tree-like-structure-or-hidden-rules-from-d</guid>
      <pubDate>Thu, 18 Jan 2024 10:48:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么多项式包含在回归中？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77838625/why-polynomial-is-included-in-regression</link>
      <description><![CDATA[多项式回归是非线性的，因为 x 与 f(x,β) 不线性相关；方程本身仍然是线性的，你怎么能证明这个问题是合理的？我在谷歌上得到了很多答案，但我没有得到最好的理由。 .
我在谷歌和其他平台上搜索。]]></description>
      <guid>https://stackoverflow.com/questions/77838625/why-polynomial-is-included-in-regression</guid>
      <pubDate>Thu, 18 Jan 2024 10:24:17 GMT</pubDate>
    </item>
    <item>
      <title>Python dgl 库 API 更新</title>
      <link>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</link>
      <description><![CDATA[这是我的代码：
def 标准化（自我，logits）：
    self.\_logits_name = “\_logits”
    self.\_normalizer_name = “\_norm”
    self.g.edata\[self.\_logits_name\] = logits

    self.g.update_all(fn.copy_u(self._logits_name, self._logits_name),
                     fn.sum(self._logits_name, self._normalizer_name))
    返回 self.g.edata.pop(self._logits_name), self.g.ndata.pop(self._normalizer_name)

def edge_softmax(自身):

    如果 self.l0 == 0:
        分数 = self.softmax(self.g, self.g.edata.pop(&#39;a&#39;))
    别的：
        分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))
        self.g.ndata[&#39;z&#39;] = 标准化器[:,0,:].unsqueeze(1)

    self.g.edata[&#39;a&#39;] = 分数[:,0,:].unsqueeze(1)

这是堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“/datasets/\_deepnote_work/train.py”，第 211 行，位于 \ 中
主要（参数）

文件“/datasets/\_deepnote_work/train.py”，第 130 行，在 main 中
logits = 模型（特征）

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 209 行，向前
h，边缘 = self.gat_layers\[0\](h，边缘)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 105 行，向前
self.edge_softmax()

文件“/datasets/\_deepnote_work/gat.py”，第 166 行，edge_softmax
分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))

文件“/datasets/\_deepnote_work/gat.py”，第 157 行，标准化
self.g.update_all(fn.copy_u(self.\_logits_name, self.\_logits_name),

文件“/root/venv/lib/python3.9/site-packages/dgl/heterograph.py”，第 5110 行，位于 update_all
ndata = core.message_passing()
文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 398 行，message_passing
ndata = invoke_gspmm(g, mfunc, rfunc)

文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 361 行，invoke_gspmm
x = alldata\[mfunc.target\]\[mfunc.in_field\]

文件“/root/venv/lib/python3.9/site-packages/dgl/view.py”，第 80 行，在 _getitem_ 中
返回 self.\_graph.\_get_n_repr(self.\_ntid, self.\_nodes)\[key\]

文件“/root/venv/lib/python3.9/site-packages/dgl/frame.py”，第 688 行，在 _getitem_ 中
返回 self.\_columns\[name\].data
关键错误：&#39;\_logits&#39;

我查看了DGLEdgeBatch的文档，但没有找到任何解决方案
链接到文档：https://docs.dgl.ai/en/1.1.x/api/python/udf.html#edge-wise-user-defined-function
阅读 DGL 的文档并尝试了一些替代函数。但他们没有工作。
如何修复/更新代码？]]></description>
      <guid>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</guid>
      <pubDate>Thu, 18 Jan 2024 06:02:39 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS() 以 .rda 格式保存。
例如，我将模型训练为：
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)

当我在清除所有环境变量后加载上述模型，并对新数据使用预测函数时：
预测（model_old，newdata = new_data）

我收到以下错误：
eval(Call$data) 中的错误：未找到对象“test”

但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么在将交错数据集与 Hugging Face (HF) 一起使用时，会出现 UnboundLocalError：赋值前引用的局部变量 'batch_idx'？</title>
      <link>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</guid>
      <pubDate>Thu, 18 Jan 2024 04:00:40 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有 2000 个不同标签的多类标记文本分类问题。使用 LSTM 和 Glove Embedding 进行分类。

目标变量的标签编码器
带有嵌入层的 LSTM 层
误差指标是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder()
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

LSTM 网络如下所示，带有 Glove Embeddings
np.random.seed(种子)
K.clear_session()
模型=顺序（）
model.add(嵌入(max_features, embed_dim, input_length = X_train.shape[1],
         权重=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add（密集（num_classes，激活=&#39;softmax&#39;））
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;sparse_categorical_crossentropy&#39;)
打印（模型.摘要（））

我的错误指标是 F1 分数。我为错误指标构建了以下函数
类指标（回调）：
    def on_train_begin(self, 日志={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_ precisions = []
 
    def on_epoch_end(自我, 纪元, 日志={}):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        _val_f1 = f1_score(val_targ, val_predict)
        _val_recall=recall_score(val_targ, val_predict)
        _val_ precision = precision_score（val_targ，val_predict）
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_ precisions.append(_val_ precision)
        print(&quot;— val_f1: %f — val_ precision: %f — val_recall %f&quot; % (_val_f1, _val_ precision, _val_recall))
        返回
 
指标=指标（）

##模型适合
model.fit（X_train，train_y，validation_data =（X_test，test_y），epochs = 10，batch_size = 64，callbacks = [指标]）

第一个纪元后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

我的代码哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>