<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sat, 29 Mar 2025 21:15:21 GMT</lastBuildDate>
    <item>
      <title>在图像中检测表情符号[封闭]</title>
      <link>https://stackoverflow.com/questions/79543795/detecting-emojis-in-an-image</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79543795/detecting-emojis-in-an-image</guid>
      <pubDate>Sat, 29 Mar 2025 19:27:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在增强学习中量化估计偏差？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79543440/how-is-estimation-bias-quantified-in-reinforcement-learning</link>
      <description><![CDATA[在各种估计问题中，尤其是在RL域中，我们目前正在研究Q学习及其变体，我们经常遇到术语估计偏差，这是指估计器的预期值与真实参数的系统偏差。&gt; 。
例如，Thrun（1993）[1]提到估计器具有估计偏差，但我正在寻找一种量化它的标准方法。我知道偏见通常被定义为：
 偏见（θ̂）= e [θ̂]  - θ
 
其中 θ̂ 是估计器，θ是true参数。
但是，在实际应用中，当我们只有一个数据样本时，用于量级或量化的标准技术是什么？在机器学习，统计或计量经济学中，是否有特定的数值或计算方法在现实世界中估算它？
我的观点：
我目前正在进行有关量化 Q-学习和双Q学习算法的偏差的研究。强化学习的关键挑战之一是了解估计偏置如何在价值函数更新中传播。引入了双Q学习，以减轻标准Q学习中存在的高估偏差，但准确地测量这种偏见仍然是一个开放的问题。
根据我观察到的，大多数研究通过经验绩效评估而不是通过直接定量来分析偏见。某些技术，例如使用自举置信区间或蒙特卡洛推广，试图通过将学习的价值功能与地面真相回报进行比较来估计偏差。但是，是否有一种更标准化的方法来量化和比较不同学习算法的偏差？
此外，我在最近的神经论文[2]中遇到了 amse的概念（渐近平方误差）。 AMSE由：给出
  amse（θ̂_n）= e [（θ̂_n -θ）²]
 
其中 n 是样本尺寸。但是，本文采用了零引用方法，这意味着该引用设置为零，而不是假设已知的真实值θ，并且对此进行了所有错误测量。这实际上意味着：
  amse（θ̂_n）= e [θ̂_n²]
 
所有偏差估计是相对而不是绝对的。
这种零引用方法如何影响强化学习中估计偏差的解释？ AMSE是否可以成为量化Q学习估计量中偏差的合适度量，还是有其他方法更适合强化学习应用？
 RL中的任何参考文献或示例，尤其是在Q学习和双重学习中，都将不胜感激。 

参考：
 [1] Thrun，S。（1993）。 偏见和学习算法中稳定性的量化。卡内基·梅隆大学。 
 [2] Bartlett，P。L.，Long，P.M.，Lugosi，G。，＆amp; Tsigler，A。（2020）。线性回归中的良性过度拟合。神经。 &lt;a href =“ https://proceedings.neurips.cc/paper_files/paper/2020/file/4bfbd52f4e8466dc12aaf30b7e057b6666-paper.pdf.pdf”]]></description>
      <guid>https://stackoverflow.com/questions/79543440/how-is-estimation-bias-quantified-in-reinforcement-learning</guid>
      <pubDate>Sat, 29 Mar 2025 15:12:34 GMT</pubDate>
    </item>
    <item>
      <title>使用Sklearn的SeceentialFeaturesElector，Local（Ubuntu VM）和Databrick之间的不同特征选择结果</title>
      <link>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</link>
      <description><![CDATA[我正在使用VM上的Ubuntu在Databricks上使用Ubuntu在VS代码中运行我的机器学习管道。当我使用相同的代码测试相同的数据集时，我从SeecentionFeaturesElector获得了不同的选定功能，这会导致不同的最终模型输出。
调试，我尝试了以下内容：

圆形的X和Y到4个小数点，以检查是否有少量阅读差异。
设置全局种子（np.random.seed（seed），andand.seed（seed））以控制随机性。
明确设置Random_State = sequentialFeaturesElector中的kfold中的种子。
单独运行RIDGECV（没有特征选择，没有标准标准器（）），并确认它在这两台机器上都会产生相同的结果。
确保Python和所有库的版本相同。

 观察：

当我只运行RidgeCV时，我在这两台机器上都会获得相同的结果。
当我运行sequentialFeaturesElector时，它会在本地与数据括号上选择不同的功能集，从而导致不同的模型输出。
我怀疑我尚未考虑SFS或交叉验证中可能存在一个随机性问题。

尽管使用了相同的数据和种子，但SequentialFeaturesElector为什么在本地和数据映中会产生不同的结果？
 ＃设置全局种子
种子= 42
np.random.seed（种子）
随机种子（种子）

＃山脊回归模型
ridge_model = ridgecv（
    alpha = np.logspace（-10，2，200），＃alpha =正则化强度
    fit_intercept = true，  
    store_cv_values = false）

＃型号管道：标准化 +脊回归
model_pipeline = make_pipeline（standardscaler（），ridge_model）

＃顺序特征选择（SFS）
sfs = SeceentialFeaturesElector（
    model_pipeline，
    n_features_to_select =&#39;auto&#39;，
    方向=&#39;向前&#39;，
    评分=&#39;r2&#39;，
    cv = kfold（n_splits = 2，Random_state = seed，shuffle = true））

＃适合SFS
sfs.fit（x，y）

＃获得选定的功能
predictors = sfs.get_feature_names_out（）。tolist（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</guid>
      <pubDate>Fri, 28 Mar 2025 11:22:54 GMT</pubDate>
    </item>
    <item>
      <title>用不同的二进制滤波器培训Tesseract会影响ENG.TRAINDATA吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</link>
      <description><![CDATA[我正在与Tesseract OCR合作，并想尝试不同的二进制方法，例如OTSU的阈值和其他自定义过滤器，以提高文本识别精度。
但是，我担心使用这些不同的预处理技术培训可能会修改或覆盖Eng.trainedData，我想保持完整。
我的问题是：

 培训新模型是否会影响现有的Eng.trainedData文件？

 如何在不修改默认英语模型的情况下安全地使用新过滤器训练Tesseract？

 是否有推荐的方法在保持启动时训练tesseract在预处理图像上。


 我尝试的是： 
用三个样本更新了我当前的eng_new.traineddata，每个样本都应用了过滤器otsu，ottu_tresh_binary，otsu_tresh_binary_inv
在第一次1000次迭代之后，我在初始训练和目标培训之间有所区别。DATA
但受到训练的目标。达塔的结果稍差。
  lstMtraining -continue_from/home/j/tribnedcurrenteng/data/checkpoints/eng_training -trainedData/home/j/trainingcurrenteng/data/data/data/eng.traineddata-train_train_train_listfile/tristfile/home/home/home/j/j/j/j/trainingcurrentengeng gultial_listefile_list_list_listfile-ev /home/j/trainingcurrenteng/data/list.eval--model_output/home/j/trainingcurrenteng/data/checkpoints/eng_training -learning_rate_rate 0.0001 -debug_interval 10 -m -max_interations 600
 
  tesseract otsu_tresh_binary_inv.tiff output_text -l eng -tessdata -dir/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text.txt

ABCD123
 
  tesseract otsu_tresh_binary_inv.tiff output_text_1 -l eng_trained -tessdata -dir/home/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text_1.txt
ABC
 
如何在不干扰现有模型的情况下训练自定义模型？]]></description>
      <guid>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</guid>
      <pubDate>Thu, 27 Mar 2025 19:57:58 GMT</pubDate>
    </item>
    <item>
      <title>使用分层k折后代码：我们需要指定哪个折叠？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</link>
      <description><![CDATA[我一直在使用简单的技术来拆分数据集。但是，我很想通过更先进的技术进行进步。例如分层的k倍数据不平衡数据。但是，一旦将数据拆分，例如我提供的代码。我们如何继续进行预处理阶段？
示例：我们无法使用整个数据集估算NAN值。我们如何编码？
 ＃假设sem_df是您的数据框，“目标”是目标列
x = sem_df.drop（&#39;target&#39;，轴= 1）
y = sem_df [&#39;target&#39;]

n_total = len（sem_df）

＃1。将数据分为80％的培训和20％的临时培训（将分为验证和测试）
x_train，x_test，y_train，y_test = train_test_split（
    x，y， 
    test_size = 0.20， 
    分层= y， 
    Random_State = 42
）

＃印刷形状具有百分比信息
print（f＆quot; train设置形状：{x_train.shape}（{x_train.shape [0] / n_total * 100：.1f}％）＆quot&#39;）
打印（f＆quot;测试集形状：{x_test.shape}（{x_test.shape [0] / n_total * 100：.1f}％）＆quot; quot&#39;）
 
这是插补问题：
 ＃注意：火车和测试的不同值
train_num_medians = x_train [num_cols] .median（）
train_cat_modes = x_train [cat_cols] .mode（）。iLoc [0]

＃注意：火车和测试的不同值
test_num_medians = x_test [num_cols] .median（）
test_cat_modes = x_test [cat_cols] .mode（）。iLoc [0]

＃使用.loc且不在内地的训练集中估算丢失值
x_train.loc [：，num_cols] = x_train.loc [：，num_cols] .fillna（train_num_medians）
x_train.loc [：，cat_cols] = x_train.loc [：，cat_cols] .fillna（train_cat_modes）

＃使用训练值值将测试集中的缺失值算
x_test.loc [：，num_cols] = x_test.loc [：，num_cols] .fillna（test_num_medians）
x_test.loc [：，cat_cols] = x_test.loc [：，cat_cols] .fillna（test_cat_modes）
 
，甚至对于标准化部分：
 ＃现在安全执行缩放：
sualer = StandardScaler（）
x_train [norm_col] = scaler.fit_transform（x_train [norm_col]）
＃x_val.loc [：，norm_col] = scaler.transform（x_val [to_norm]）
x_test [norm_col] = scaler.transform（x_test [norm_col]）
 
执行与交叉验证的数据拆分时，我们必须更改经典步骤吗？
火车 - ＆gt; fit_transform 
测试 - ＆GT;变换]]></description>
      <guid>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</guid>
      <pubDate>Thu, 27 Mar 2025 19:56:19 GMT</pubDate>
    </item>
    <item>
      <title>在两个nn.modulelist上使用zip（）</title>
      <link>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</guid>
      <pubDate>Wed, 26 Mar 2025 18:02:29 GMT</pubDate>
    </item>
    <item>
      <title>在Pytorch中使用大于1的批次尺寸时的错误</title>
      <link>https://stackoverflow.com/questions/79519426/error-when-using-batch-size-greater-than-1-in-pytorch</link>
      <description><![CDATA[我正在构建一个神经网络，以预测使用VVC（多功能视频编码）在压缩过程中如何对图像进行分区。该模型从YUV420图像中获取单个Y框架作为输入，并使用包含地面真相块位置和尺寸的CSV文件进行训练。
输入和地面真相

  输入： 1-Frame YUV420 10位图像。

  地面真相：具有块位置，大小和其他分区标志的CSV文件。


示例（388016_320x480_37.yuv）
在此处输入图像描述 
示例（388016_320x480_37.csv）
在这里输入图像说明 
问题描述：
我实现了 train.py 和 dataset.py ，但是当设置 batch_size＆gt; 1 在 dataloader 中。批量大小为1，该模型正常工作，但是增加批处理大小会导致运行时错误。
代码摘要：
以下是我的 custom_collat​​e_fn 和 dataloader 设置的简化版本：
  def custom_collat​​e_fn（批次）：
    帧= [批次中的项目]＃y帧张量
    块= [批次中的项目]＃块信息
    框架= torch.stack（帧，dim = 0）＃沿批处理尺寸堆叠帧
    返回框架，块

dataloader = dataloader（
    数据集，
    batch_size = batch_size， 
    shuffle = true，
    Collat​​e_fn = custom_collat​​e_fn
）
 
观察：

 当 batch_size = 1 时， blocks_batch 在训练环中是包含一组块数据的列表。

 带有 batch_size＆gt; 1 ，它成为列表的列表，在索引时会导致错误。


  i，（frame，blocks_batch）枚举（dataLoader）：
    frame = frame.to（设备）＃形状：[batch_size，1，h，w]
    blocks = blocks_batch [0]＃与batch_size = 1一起使用，但大小较大
 
我的假设：
似乎是由处理 blocks_batch 当 batch_size＆gt; 1 。嵌套列表结构使得很难处理多批次。
问题：

 如何调整 custom_collat​​e_fn 或训练循环以有效处理大于1的批次大小？

 如果有更好的方法可以通过批处理处理可变的块数据，我将感谢任何建议。


  file＆quod＆quot＆quode＆users \ indersration \ documents \ documents \ vvc_fast \ test4 \ test4 \ train.py＆quid＆quort＆quort＆quort＆quort＆quort＆quid＆quot 91 in＆lt; module＆gt;
    loss1 =标准（out_split，target_split）
  file＆quot; c：\ programData \ miniconda3 \ envs \ enkeNv3 \ lib \ lib \ site-packages \ torch \ thnn \ nn \ modules \ module.py;
    返回self._call_impl（*args，** kwargs）
  file＆quot; c：\ programData \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ thnn \ nn \ modules \ module.py;
    返回forward_call（*args，** kwargs）
  file＆quot; c：\ programdata \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ nn \ nn \ loss.py.py＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort in 725
    返回f.binary_cross_entropy_with_logits（输入，目标，
  file＆quot c：\ programData \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ nn \ functional.py;
    提高ValueError（f＆quot“目标大小（{target.size（）}）必须与输入大小（{input.size（）}）相同
ValueError：目标大小（TORCH.SIZE（[1，1]））必须与输入大小（Torch.Size（[2，1]））相同
 ]]></description>
      <guid>https://stackoverflow.com/questions/79519426/error-when-using-batch-size-greater-than-1-in-pytorch</guid>
      <pubDate>Wed, 19 Mar 2025 07:28:29 GMT</pubDate>
    </item>
    <item>
      <title>特征的长度不等于形状值的长度</title>
      <link>https://stackoverflow.com/questions/79515542/length-of-features-is-not-equal-to-the-length-of-shap-values</link>
      <description><![CDATA[我正在运行一个随机的森林模型，并获得某些特征的重要性，并试图运行形状分析。问题是，每当我尝试绘制塑形值时，我都会遇到此错误：
  dimensionError：功能的长度不等于shap_values的长度。 
 
我不知道发生了什么。当我运行XGBoost模型时，一切似乎都很好，我可以看到数据集的形状图。它的数据集完全相同，但它不会与随机森林一起运行。它用于二进制分类。
这是我的python代码：
 来自sklearn.semble import incort fandyForestClassifier
来自sklearn.model_selection导入train_test_split，cross_val_score
来自sklearn.metrics导入精度，precision_score，recke_score，f1_score，confusion_matrix

＃从功能中删除主键列“ ID”

功能= result.drop（columns = [&#39;pq2&#39;，&#39;id&#39;]）＃删除目标和ID列
target =结果[&#39;pq2&#39;]＃目标变量

＃将数据分为80-20的培训和测试集
x_train，x_test，y_train，y_test = train_test_split（功能，target，test_size = 0.2，andural_state = 42）
 
＃初始化随机森林分类器
rf_model = RandomforestClassifier（N_Estimators = 100，Random_State = 42）

＃将模型适合培训数据
rf_model.fit（x_train，y_train）

＃做出预测
y_pred = rf_model.predict（x_test）

导入塑造

＃为随机森林模型创建树状解释器
解释器= shap.treeexplainer（rf_model）

＃计算测试集的形状值
shap_values = ruminder.shap_values（x_test）

＃绘制形状摘要图
shap.summary_plot（shap_values，x_test，feature_names = features_names）

＃绘制整体特征重要性的塑形栏图

shap.summary_plot（shap_values，x_test，feature_names = features_names，plot_type =; bar＆quot; quot;
 
测试集的形状为（829,22），但是随机森林的外形值始终返回（22,2），我不知道如何修复它。数据集已经进行了预处理，列是0-1S或数值列。]]></description>
      <guid>https://stackoverflow.com/questions/79515542/length-of-features-is-not-equal-to-the-length-of-shap-values</guid>
      <pubDate>Mon, 17 Mar 2025 19:16:45 GMT</pubDate>
    </item>
    <item>
      <title>保存到磁盘中的磁盘[封闭]时会遇到Unicode错误</title>
      <link>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</guid>
      <pubDate>Thu, 13 Feb 2025 15:04:10 GMT</pubDate>
    </item>
    <item>
      <title>有关培训物理知情神经网络的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79415961/issue-regarding-training-physics-informed-neural-network</link>
      <description><![CDATA[我正在通过波传播数据培训物理知情的神经网络。训练输入的形状为（193524369，3）。我目前使用的批量尺寸为2048和Epoch 300。对于培训模型，我总共给了20,000批。因此，基本上，要训练模型，我使用的是20,000批次，每个批次都有2048个数据，几乎是我整个数据集的10％。该代码正在Tesla V100-PCIE-32GB GPU上运行。
训练进度非常慢，训练模型可能需要几个月的时间。但是，NN并不那么沉重，而有那么多节点和层。我该如何使培训进度更快？
我对更快地进行培训过程有些疲惫。使培训批量更大可以使其变得更好一些，但这会导致准确性降低。我应该如何优化以使训练过程更快并保持准确性更好？]]></description>
      <guid>https://stackoverflow.com/questions/79415961/issue-regarding-training-physics-informed-neural-network</guid>
      <pubDate>Wed, 05 Feb 2025 19:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：无法从“火炬”（未知位置）导入“张量”的名称</title>
      <link>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</link>
      <description><![CDATA[我正在尝试从Pytorch导入 ：
 来自火炬导入张量
 
但我一直遇到此错误：
  Importerror：无法从“火炬”（未知位置）导入名称&#39;tensor&#39;
 
我尝试的是：

检查是否已安装了Pytorch（ pip show torch ），我正在使用版本 2.5.1 。。
重新安装了Pytorch：
  pip卸载火炬
PIP安装火炬
 

测试了python壳中的导入，但错误仍然存​​在。

环境：

 Python版本：3.10 
 Pytorch版本：2.5.1 
 OS：Windows 10 
虚拟环境：是

我该如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</guid>
      <pubDate>Sat, 18 Jan 2025 13:04:35 GMT</pubDate>
    </item>
    <item>
      <title>总和和平均值在.backward（）计算损失和通过网络反向传播时的差异</title>
      <link>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</link>
      <description><![CDATA[我知道我们应该在向后应用之前将张量转换为标量（），但是何时和何时表示？
  some_loss_function.sum（）。backward（）
-或者-
some_loss_function.mean（）。backward（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</guid>
      <pubDate>Mon, 30 May 2022 06:13:42 GMT</pubDate>
    </item>
    <item>
      <title>如何将稀疏的numpy数组转换为数据框架？</title>
      <link>https://stackoverflow.com/questions/64746124/how-to-convert-sparse-numpy-array-to-dataframe</link>
      <description><![CDATA[下面是代码段，
 来自sklearn.com pos import columntransformer
从sklearn.preprocessing导入onehotencoder
ct = columnTransFormer（变形金刚= [（&#39;encoder&#39;，onehotencoder（），[2,3,4]）]，剩余=&#39;PassThrough&#39;）
x = np.array（ct.fit_transform（x_data））
X.Shape
 
我得到下面的输出
 （）
 
当我尝试打印X时，我会像以下那样获得输出
  array（＆lt; 8820x35类型&#39;＆lt; class&#39;numpy.float64&#39;&#39;＆gt;&#39;&#39;的稀疏矩阵
    在压缩稀疏行格式中存储了41527个元素，dtype = object）
 
现在，当我尝试将此数组转换为DataFrame 时
  x = pd.dataframe（x）
 
我在以下错误以下
  valueerror：必须通过2-D输入
 
如何将我的numpy数组转换为dataframe？]]></description>
      <guid>https://stackoverflow.com/questions/64746124/how-to-convert-sparse-numpy-array-to-dataframe</guid>
      <pubDate>Mon, 09 Nov 2020 05:03:36 GMT</pubDate>
    </item>
    <item>
      <title>如何了解损失，acc，val_loss，val_acc在keras模型拟合中？</title>
      <link>https://stackoverflow.com/questions/47299624/how-to-understand-loss-acc-val-loss-val-acc-in-keras-model-fitting</link>
      <description><![CDATA[这是我的KERAS模型结果（在4160个样品上训练，在1040个样本上进行验证）：
 时代1/20
4160/4160-损失：3.3455- ACC：0.1560 -Val_loss：1.6047 -Val_acc：0.4721

时期2/20
4160/4160-损失：1.7639- ACC：0.4274 -Val_loss：0.7060 -Val_acc：0.8019

时期3/20
4160/4160-损失：1.0887- ACC：0.5978 -Val_loss：0.3707 -Val_acc：0.9087

时期4/20
4160/4160-损失：0.7736- ACC：0.7067 -Val_loss：0.2619 -Val_acc：0.9442

时期5/20
4160/4160-损失：0.5784- ACC：0.7690 -Val_loss：0.2058 -Val_acc：0.9433

时期6/20
4160/4160-损失：0.5000- ACC：0.8065 -Val_loss：0.1557 -Val_acc：0.9750

时期7/20
4160/4160-损失：0.4179- ACC：0.8296 -Val_loss：0.1523 -Val_acc：0.9606

时期8/20
4160/4160-损失：0.3758- ACC：0.8495 -Val_loss：0.1063 -Val_acc：0.9712

时期9/20
4160/4160-损失：0.3202- ACC：0.8740 -Val_loss：0.1019 -Val_acc：0.9798

时期10/20
4160/4160-损失：0.3028- ACC：0.8788 -Val_loss：0.1074 -Val_acc：0.9644

时期11/20
4160/4160-损失：0.2696- ACC：0.8923 -Val_loss：0.0581 -Val_acc：0.9856

时期12/20
4160/4160-损失：0.2738- ACC：0.8894 -Val_loss：0.0713 -Val_acc：0.9837

时代13/20
4160/4160-损失：0.2609- ACC：0.8913 -Val_loss：0.0679 -Val_acc：0.9740

时期14/20
4160/4160-损失：0.2556- ACC：0.9022 -Val_loss：0.0599 -Val_acc：0.9769

时期15/20
4160/4160-损失：0.2384- ACC：0.9053 -Val_loss：0.0560 -Val_acc：0.9846

时代16/20
4160/4160-损失：0.2305- ACC：0.9079 -Val_loss：0.0502 -Val_acc：0.9865

时期17/20
4160/4160-损失：0.2145- ACC：0.9185 -Val_loss：0.0461 -Val_acc：0.9913

时期18/20
4160/4160-损失：0.2046- ACC：0.9183 -Val_loss：0.0524 -Val_acc：0.9750

时期19/20
4160/4160-损失：0.2055- ACC：0.9120 -Val_loss：0.0440 -Val_acc：0.9885

时代20/20
4160/4160-损失：0.1890- ACC：0.9236 -Val_loss：0.0501 -Val_acc：0.9827
 
这是我的理解：

 两个损失（损失和val_loss）正在减少，拖曳ACC（ACC和Val_ACC）正在增加。因此，这表明建模是很好的。

  val_acc是模型预测的良好程度。因此，就我的情况而言，看起来模型在6个时代后接受了很好的训练，而其余的培训是不需要的。 


我的问题是：

  ACC（在训练集上）总是比Val_ACC更小，实际上要小得多。这是正常的吗？为什么会发生这种情况？在我看来，ACC通常应该比Val_acc更好。

  20个时期后，ACC仍在增加。那么，当ACC停止增加时，我应该使用更多的时期并停止吗？或者，无论ACC的趋势如何，我都应该停在Val_ACC停止增加的地方

 对我的结果还有其他想法吗？

]]></description>
      <guid>https://stackoverflow.com/questions/47299624/how-to-understand-loss-acc-val-loss-val-acc-in-keras-model-fitting</guid>
      <pubDate>Wed, 15 Nov 2017 04:56:05 GMT</pubDate>
    </item>
    <item>
      <title>如何实现“相关”度量算法？</title>
      <link>https://stackoverflow.com/questions/42489/how-to-implement-a-related-degree-measure-algorithm</link>
      <description><![CDATA[今天早些时候，当我在Stackoverflow中出现令人惊讶的功能时，我将要问一个问题。当我写我的问题标题时，Stackoverflow向我提出了几个相关问题，我发现已经有两个类似的问题。那真是太棒了！ 
然后，我开始思考如何实现此类功能。我将如何通过相关性订购问题：

有更高数量的问题
单词与新问题匹配
如果比赛数为
同样，言语的顺序被认为
标题中出现的单词
更高的相关性

这将是一个简单的工作流程或复杂的分数算法？
有些人可以增加召回，也许？
是否有某些库该功能？
您还要考虑其他哪些方面？
也许杰夫可以回答自己！您是如何在Stackoverflow中实现的？ ：）]]></description>
      <guid>https://stackoverflow.com/questions/42489/how-to-implement-a-related-degree-measure-algorithm</guid>
      <pubDate>Wed, 03 Sep 2008 20:21:04 GMT</pubDate>
    </item>
    </channel>
</rss>