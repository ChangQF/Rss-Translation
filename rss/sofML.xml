<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 16 Dec 2024 06:27:43 GMT</lastBuildDate>
    <item>
      <title>利用贝叶斯优化进行多类分类</title>
      <link>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</link>
      <description><![CDATA[无法让这部分代码运行，而且速度真的很慢。
我只想创建一个模型，将叶子图像分为 4 种类型（无，然后是 3 种疾病类型）
我想使用 F1 作为损失函数，然后使用贝叶斯优化来获取叶子类模型的最佳参数，但它没有运行。或者它运行得非常慢然后失败了...
我在 CPU 上运行，因为我没有 GPU
# Optuna 的目标函数
def objective(trial, train_dataloader, val_dataloader, device):
# 使用 Optuna 的建议函数定义超参数搜索空间
conv1_filters = trial.suggest_categorical(&quot;conv1_filters&quot;, [16, 32, 64])
conv2_filters = trial.suggest_categorical(&quot;conv2_filters&quot;, [64, 128, 256])
kernel_size = trial.suggest_categorical(&quot;kernel_size&quot;, [3, 5, 7])
hidden_​​units = trial.suggest_categorical(&quot;hidden_​​units&quot;, [256, 512, 1024])
dropout_rate = trial.suggest_float(&quot;dropout_rate&quot;, 0.1, 0.5)
learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 1e-5, 1e-2, log=True)
num_epochs = trial.suggest_int(&quot;num_epochs&quot;, 10, 50)

# 使用给定的参数初始化模型
model = LeafCNN(
conv1_filters=conv1_filters,
conv2_filters=conv2_filters,
kernel_size=kernel_size,
hidden_​​units=hidden_​​units,
dropout_rate=dropout_rate
).to(device)

# 设置优化器和损失函数
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)

# 提前停止参数
patient = 10 # 允许 10 个 epoch 不进行改进
delta = 0.001 # 算作改进的最小变化
best_val_loss = float(&quot;inf&quot;)
patience_counter = 0 # 从 0 开始计数器

# 训练循环
for epoch in range(num_epochs):
model.train()
for X_batch, y_batch in train_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
optimizer.zero_grad()
outputs = model(X_batch)
loss = loss_fn(outputs, y_batch)
loss.backward()
optimizer.step()

# 验证损失
model.eval()
val_loss = 0.0
使用 torch.no_grad():
对于 val_dataloader 中的 X_batch、y_batch：
X_batch、y_batch = X_batch.to(device)、y_batch.to(device)
输出 = 模型 (X_batch)
损失 = loss_fn(outputs、y_batch)
val_loss += loss.item()

val_loss /= len(val_dataloader)
打印 (f&quot;Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}&quot;)

如果 val_loss &lt; best_val_loss - delta:
best_val_loss = val_loss
waiting_counter = 0
else:
waiting_counter += 1

if waiting_counter &gt;= waiting:
print(f&quot;在 epoch {epoch + 1} 触发提前停止&quot;)
break

# 使用 F1 分数进行评估
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
for X_batch, y_batch in val_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
output = model(X_batch)
_, predict = torch.max(outputs, 1)
all_preds.extend(predicted.cpu().numpy())
all_labels.extend(y_batch.cpu().numpy())

f1 = f1_score(all_labels, all_preds, average=&quot;weighted&quot;)
return f1 # 我们的目标是最大化 F1 分数

# 运行 Optuna 优化的主要函数
def optuna_search(train_dataloader, val_dataloader, device, num_trials):
# 创建 Optuna 研究
study = optuna.create_study(direction=&quot;maximize&quot;) # 最大化 F1 分数
study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device), n_trials=num_trials)

# 打印最佳超参数
print(&quot;找到最佳超参数：&quot;, study.best_params)
print(&quot;最佳 F1 分数：&quot;, study.best_value)

return study.best_params, study.best_value

# 主程序
if __name__ == &quot;__main__&quot;:

# 创建 DataLoaders
train_dataset = TensorDataset(X_train_split, y_train_split)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

val_dataset = TensorDataset(X_val, y_val)
val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# 运行 Optuna 优化
best_params, best_f1 = optuna_search(train_dataloader, val_dataloader, device, num_trials=10)
print(&quot;最佳参数：&quot;, best_params)
print(&quot;最佳 F1 分数：&quot;, best_f1)
]]></description>
      <guid>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</guid>
      <pubDate>Sun, 15 Dec 2024 23:07:57 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 停留在图像生成上</title>
      <link>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</link>
      <description><![CDATA[我创建了一个 LSTM 来生成序列中的下一张图像（我知道 CNN 是用于图像生成的，但我需要整个图像，而不仅仅是提供给序列下一次迭代的过滤器）。所以我有一个数据集，它包含图像（电影中的帧），我创建了它的序列，就像 1 个场景包含例如。 n 个图像，我有 s 个序列长度，那么输入将是 image_1 到 image_s，输出是 image_s+1，下一个输入是 image_2 到 image_s+1，输出是 image_s+2，依此类推。
模型如下：
class LSTM(nn.Module):
def __init__(self, input_len, hidden_​​size, num_layers):
super(LSTM, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_len, hidden_​​size, num_layers, batch_first=True)
self.output_layer = nn.Linear(hidden_​​size, input_len)
self.dropout = nn.Dropout(.2)

def forward(self, X):
hidden_​​states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
out, _ = self.lstm(X, (hidden_​​states, cell_states))
out = self.dropout(out)
out = self.output_layer(out[:, -1, :])
return out

训练是：
def train(num_epochs, model, loss_func, optimizer):
total_steps = loader.getSizeWithBatch()

for epoch in range(num_epochs):
loader.reset()
for item in range(total_steps-1):
element = loader.next()[0]
x_images,y_image = element
x_images = x_images.reshape(-1,sequence_len,input_len)
output = model(x_images)
y_image = y_image.reshape(-1,input_len)
loss = loss_func(output, y_image)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if (item + 1) % 1 == 0:
print(f&#39;Epoch: {epoch + 1};批次：{item + 1} / {total_steps};损失：{loss.item():&gt;4f}&#39;)

if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;model_save_interval&#39;]) == 0:
if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;clean_save_interval&#39;]) == 0:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;] + str(epoch+1)))
else:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;]))

Loader 以张量的形式引导图像由于内存使用，从文件中预先排序。
我使用 MSE 损失和 Adam 作为优化器。
问题是，当我训练它时，错误达到 0.003，这是目标，因为我通过将它们除以 255 来规范化值，但当我预测它时，它会产生一种模糊的场景图像，并且无论输入如何，预测图像始终相同，即使输入来自其他场景，它也会创建相同的图像，当我减去不同输出图像的颜色值时，该值为 0，因此每个输出图像都完全相同
我尝试添加 Droput，增加隐藏大小的神经元（现在是 128），尝试增加层数，不同的时期会创建相同的图像，只是模糊程度略低，但是一样的，我将学习率从 .001 降低到 .0001，一切都一样]]></description>
      <guid>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</guid>
      <pubDate>Sun, 15 Dec 2024 20:28:51 GMT</pubDate>
    </item>
    <item>
      <title>针对 Lllama 3.2 1B 的分层学生-教师优化</title>
      <link>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</guid>
      <pubDate>Sun, 15 Dec 2024 20:21:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能有效地执行此循环而不增加内存使用量？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>如何为任何数据集创建强大的预处理函数？[关闭]</title>
      <link>https://stackoverflow.com/questions/79282246/how-to-create-a-robust-preprocessing-function-for-any-dataset</link>
      <description><![CDATA[我正在开展一个项目，根据患者的症状预测合适的医生专业。
该项目包括一项功能，研究人员可以上传自己的数据集并使用预先训练的机器学习模型对其进行评估。在对上传的数据进行训练后，将显示准确率、召回率和精确率等结果。
我需要编写一个通用预处理函数，在对模型进行训练之前处理研究人员上传的任何数据集。
到目前为止，我已经使用标签编码和独热编码对分类数据进行编码，但我担心处理具有不同特征的数据集。以下是我预见到的一些挑战：
噪声数据
不正确的数据类型
缺失值
多重共线性
我的问题：

一个预处理函数能否处理任何给定数据集的所有这些问题？
是否有标准技术可以以通用方式检测和解决噪声数据、不正确的类型或缺失值等问题？
我是否应该考虑为不同类型的数据集创建多个预处理管道，或者是否有一种可以根据数据集进行调整的动态方法？
]]></description>
      <guid>https://stackoverflow.com/questions/79282246/how-to-create-a-robust-preprocessing-function-for-any-dataset</guid>
      <pubDate>Sun, 15 Dec 2024 11:46:39 GMT</pubDate>
    </item>
    <item>
      <title>URL 中损坏的访问控制的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79282134/dataset-for-broken-access-control-in-url</link>
      <description><![CDATA[我是一名学生，正在接受一个项目任务，该项目涉及使用机器学习检测 URL 中损坏的访问控制（如 IDOR）。我一直在寻找可用于训练机器学习的数据集，但找不到与我的主题最相关的数据集。你们知道我可以在哪里找到数据集吗？
我尝试使用每个相关关键字搜索有关损坏的访问控制。]]></description>
      <guid>https://stackoverflow.com/questions/79282134/dataset-for-broken-access-control-in-url</guid>
      <pubDate>Sun, 15 Dec 2024 10:22:25 GMT</pubDate>
    </item>
    <item>
      <title>StandardScaler 的管道方法是否可以推广到基于树的集成或神经网络？</title>
      <link>https://stackoverflow.com/questions/79281636/does-the-pipeline-approach-with-standardscaler-generalize-to-tree-based-ensemble</link>
      <description><![CDATA[我在 scikit-learn 中使用 Pipeline 将特征缩放与分类器相结合。这对于逻辑回归非常有效，但我很好奇这种方法是否可以有效地推广到更复杂的模型，例如基于树的集成或神经网络。具体来说，这些模型是否需要不同的缩放策略，或者我可以在它们之间一致地应用 StandardScaler？
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# 生成样本数据
np.random.seed(42)
X = np.random.rand(200, 5) # 200 个样本，5 个特征
y = np.random.randint(0, 2, 200) # 二进制目标

# 拆分数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 为不同模型定义管道
pipelines = {
&#39;logistic_regression&#39;: Pipeline([
(&#39;scaler&#39;, StandardScaler()),
(&#39;classifier&#39;, LogisticRegression())
]),
&#39;random_forest&#39;: Pipeline([
(&#39;scaler&#39;, StandardScaler()),
(&#39;classifier&#39;, RandomForestClassifier())
]),
&#39;neural_network&#39;: Pipeline([
(&#39;scaler&#39;, StandardScaler()),
(&#39;classifier&#39;, MLPClassifier(max_iter=500))
])
}

# 评估每个模型
for model_name, pipeline in pipelines.items():
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
print(f&quot;{model_name} 准确率：{accuracy_score(y_test, y_pred)}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79281636/does-the-pipeline-approach-with-standardscaler-generalize-to-tree-based-ensemble</guid>
      <pubDate>Sun, 15 Dec 2024 00:58:36 GMT</pubDate>
    </item>
    <item>
      <title>获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</link>
      <description><![CDATA[我正在做一个机器学习项目，在 Jupyter Notebook 上预测电动汽车的价格。
我运行这些单元：
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
x[col] = le.transform(x[col]) 
print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5，random_state = 0)

r2_score(y_test，lm.predict(x_test))

从 sklearn.tree 导入 DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train，y_train)
r2_score(y_test，regressor.predict(x_test))

r2_score(y_train，regressor.predict(x_train))

uv = np.nanpercentile(df2[&#39;Base MSRP&#39;]，[99])[0]*2

df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv

df2 = df2[df2[&#39;Model Year&#39;] != &#39;N/&#39;] # 过滤掉包含 &#39;Model Year&#39; 的行&#39;N/&#39;

for col in cols:
df2[col] = df2[col].replace(&#39;N/&#39;, -1)
le.fit(df2[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

le = preprocessing.LabelEncoder()

cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]

for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError回溯（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 返回 _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
以下是数据集的链接：
https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNet V2模型（mobilenet_v2_1.4_224.tflite）的概率总是相同的？</title>
      <link>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:10 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法学习和实现集成兼容的对象检测模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79280938/is-there-way-to-learn-and-implement-ensemble-compatible-object-detection-models</link>
      <description><![CDATA[我是机器学习的新手。我知道检测模型和其他东西的基础知识。我研究了一些分类模型以及如何将它们组合起来。但我不确定如何将其用于多类对象检测模型。特别是 Yolov8/11 和其他一些单次模型。是否有任何代码或资源可以帮助我组合对象检测模型？
我尝试过一些资源，例如这个“https://github.com/ancasag/ensembleObjectDetection”。但我认为我需要一些更简单的解释。特别是关于加权组合技术。]]></description>
      <guid>https://stackoverflow.com/questions/79280938/is-there-way-to-learn-and-implement-ensemble-compatible-object-detection-models</guid>
      <pubDate>Sat, 14 Dec 2024 15:53:18 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 警告未找到可见的 GPU，正在将设备设置为 CPU</title>
      <link>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</link>
      <description><![CDATA[系统信息

XGBoost 版本：2.1.3
NVIDIA 驱动程序版本：565.57.01
CUDA 版本：12.6（来自 nvcc）和 12.7（来自 nvidia-smi）
GPU：Tesla T4
操作系统：Ubuntu 24.04
Python 版本：3.11.10
torch.cuda.is_available()：True

尽管系统显示 CUDA 和 GPU 可用，但我遇到了来自 XGBoost 的以下警告：

XGBoost 警告：/workspace/src/context.cc:43：未找到可见的 GPU，将设备设置为CPU。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</guid>
      <pubDate>Sat, 14 Dec 2024 09:12:14 GMT</pubDate>
    </item>
    <item>
      <title>模型部署：交叉验证和超参数调整</title>
      <link>https://stackoverflow.com/questions/79275593/model-deployment-cross-validation-and-hyperparameter-tuning</link>
      <description><![CDATA[我正在使用 Prophet（时间序列的元模型），我有一个与模型部署相关的问题，该问题也扩展到其他机器学习算法。因此，我使用了元文档中提供的代码进行超参数调整。它的工作原理类似于网格搜索，并基于交叉验证输出模型中使用的最佳超参数组合。该最佳组合的 RMSE 约为 11.15。因此，考虑到我想部署模型并发送到生产，我应该使用网格搜索提供的超参数组合在整个数据集上重新训练模型，还是应该将使用交叉验证训练的模型发送到生产？

我问这个问题是因为当我使用网格搜索中的超参数在整个数据集上训练模型时，RMSE 比交叉验证的更高（更差）。
]]></description>
      <guid>https://stackoverflow.com/questions/79275593/model-deployment-cross-validation-and-hyperparameter-tuning</guid>
      <pubDate>Thu, 12 Dec 2024 15:01:43 GMT</pubDate>
    </item>
    <item>
      <title>Pyannote：离线加载和应用说话人区分</title>
      <link>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</link>
      <description><![CDATA[我尝试离线使用 Pyannotes 模型。
我是这样加载和应用模型的：
from pyannote.audio import Pipeline

access_token = &#39;xxxxxxxxxxx&#39;

model = Pipeline.from_pretrained(
&quot;pyannote/speaker-diarization-3.1&quot;,
use_auth_token=access_token)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

这样就没问题了。
但是现在我按照离线使用的说明操作：https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/applying_a_pipeline.ipynb
我的目录结构如下：
src-
     |-pyannote_offline_config.yaml
     |-pyannote_pytorch_model.bin
---- YAML ----
version: 3.1.0

pipeline:
name: pyannote.audio.pipelines.SpeakerDiarization
params:
clustering: AgglomerativeClustering
embedding: pyannote/wespeaker-voxceleb-resnet34-LM
embedding_batch_size: 32
embedding_exclude_overlap: true
分段：src/pyannote_pytorch_model.bin
分段批处理大小：32

参数：
聚类：
方法：质心
min_cluster_size：12
阈值：0.7045654963945799
分段：
min_duration_off：0.0

---- 正在加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但结果却是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
好的，下次尝试：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

params = {&#39;clustering&#39;:
{&#39;method&#39;: &#39;centroid&#39;,
&#39;min_cluster_size&#39;: 12,
&#39;threshold&#39;: 0.7045654963945799},
&#39;segmentation&#39;:
{&#39;min_duration_off&#39;: 0.0}}

pipeline = model.instantiate(params)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

pipeline(path_in,
num_speakers=num_speakers).labels()

但结果是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
我不明白问题所在。
如果我这样做，它就会起作用：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;, path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但上传到 gitlab 后，测试管道显示：“无法下载‘pyannote/speaker-diarization-3.1’管道。
这可能是因为管道是私有的或封闭的，因此请确保进行身份验证。访问 https://hf.co/settings/tokens
创建您的访问令牌并重试：
Pipeline.from_pretrained(&#39;pyannote/speaker-diarization-3.1&#39;,
... use_auth_token=YOUR_AUTH_TOKEN)&quot;
因此，似乎我的本地计算机上有一些东西没有通过 pip 安装下载。例如，如果我不使用 yaml 加载它，而只使用 model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;)，它也会起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</guid>
      <pubDate>Thu, 01 Aug 2024 12:28:41 GMT</pubDate>
    </item>
    <item>
      <title>从头开始训练的 Keras Xception 在历史记录中给出了约 100% 的准确率，但在评估时仅预测 1，准确率为 50%</title>
      <link>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</link>
      <description><![CDATA[我在 keras 上训练 Xception 模型，没有使用预训练权重来解决二元分类问题，结果出现了非常奇怪的行为。历史图显示训练准确率不断增加，直到达到 100%，而验证准确率始终在 50% 左右，因此看起来是过度拟合，但事实并非如此，因为我检查过，即使在训练集上，它也总是预测（接近）1。
这种行为的原因可能是什么？
这是我用来训练的代码。 x_train_xception 已由 keras.applications.xception.preprocess_input 函数预处理。
我使用相同的代码（除了模型创建之外）来训练预训练的 Xception 模型，效果很好
inLayerX = Input((512, 512, 4))
xceptionModel = keras.applications.Xception(include_top = True, weights=None, input_tensor=inLayerX, classes=1, classifier_activation= &#39;sigmoid&#39;)

xceptionModel.compile(loss= &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

history = xceptionModel.fit(x_train_xception, y_train, batch_size= batch_size, epochs= epochs, validation_data=(x_val_xception, y_val))

_, accTest = xceptionModel.evaluate(x_test_xception, y_test)
_, accVal = xceptionModel.evaluate(x_val_xception, y_val)
_, accTrain = xceptionModel.evaluate(x_train_xception, y_train)
print(&quot;训练准确率 {:.2%}&quot;.format(accTrain))
print(&quot;验证准确率 {:.2%}&quot;.format(accVal))
print(&quot;测试准确率 {:.2%}&quot;.format(accTest))

输出：
2/2 [================================] - 6s 2s/step - 损失： 1.2063 - 准确率：0.5000
1/1 [==============================] - 4s 4s/步 - 损失：1.2960 - 准确率：0.4667
4/4 [================================] - 5s 1s/步 - 损失：1.2025 - 准确率：0.5083
训练准确率 50.83%
验证准确率 46.67%
测试准确率 50.00%

验证和测试准确率在预期之内，但真正困扰我的是训练准确率，从历史记录来看，我预计训练准确率接近 100%。
模型准确率
模型损失]]></description>
      <guid>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</guid>
      <pubDate>Sun, 10 Jul 2022 17:58:52 GMT</pubDate>
    </item>
    <item>
      <title>数据解析和特征工程管道的设计模式</title>
      <link>https://stackoverflow.com/questions/63788496/design-pattern-for-a-data-parsingfeature-engineering-pipeline</link>
      <description><![CDATA[我知道这个问题过去在这些论坛上被问过几次，但我有一个更通用的版本，将来可能也适用于其他人的项目。
简而言之 - 我正在构建一个 ML 系统（使用 Python，但在这种情况下语言选择并不十分关键），其 ML 模型位于正在发生的操作管道的末尾：

数据上传
数据解析
特征工程
特征工程（与上一步的逻辑括号不同）
特征工程（与上一步的逻辑括号不同）

...（更多步骤，如最后 3 个）

数据传递给 ML 模型

上述每个步骤都有其必须采取的一系列操作，以便构建正确的输出，然后将其用作下一个步骤的输入，等等。这些子步骤反过来可以完全彼此分离，或者其中一些可能需要先完成大步骤中的某些步骤，以生成后续步骤使用的数据。
现在的问题是，我需要构建一个自定义管道，这样就可以非常轻松地将新步骤（无论大小）添加到组合中，而不会破坏现有的步骤。
到目前为止，从架构的角度来看，我对它的外观有这样一个概念，如下所示：

在查看此架构时，我立即想到了一个责任链设计模式，它管理大步骤（1、2、...、n），并且每个大步骤都有自己的小版本的责任链，这些责任链在它们的内部发生，对于 NO_REQ 步骤独立发生，然后对于 REQ 步骤独立发生（REQ 步骤循环进行，直到它们全部完成）。如果有一个共享接口用于在大步骤和小步骤内运行逻辑，那么它可能会运行得相当整齐。
但是，我想知道，是否有更好的方法？此外，我不喜欢责任链，因为它需要一个人添加新的大/小步骤，总是编辑逻辑设置步骤包的“核心”，以手动包含新添加的步骤。我很想构建一些东西，它只会扫描每个大步骤下特定于步骤的文件夹，并自行构建 NO_REQ 和 REQ 步骤列表（以坚持开放/封闭 SOLID 原则）。
我将不胜感激任何想法。]]></description>
      <guid>https://stackoverflow.com/questions/63788496/design-pattern-for-a-data-parsingfeature-engineering-pipeline</guid>
      <pubDate>Tue, 08 Sep 2020 06:49:39 GMT</pubDate>
    </item>
    </channel>
</rss>