<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 16 Jul 2024 03:20:47 GMT</lastBuildDate>
    <item>
      <title>PyTorch DataLoader 中用于 LSTM 模型的 WeightedRandomSampler 的 IndexError</title>
      <link>https://stackoverflow.com/questions/78752550/indexerror-with-weightedrandomsampler-in-pytorch-dataloader-for-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78752550/indexerror-with-weightedrandomsampler-in-pytorch-dataloader-for-lstm-model</guid>
      <pubDate>Tue, 16 Jul 2024 03:05:49 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入参数连接到 CNN_M_LSTM 模型？</title>
      <link>https://stackoverflow.com/questions/78752243/how-to-concatenate-inputs-parameters-to-the-cnn-m-lstm-model</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow 和 Keras API）中。
带有时间戳的能耗数据集的大小为 (730, 2)
Covid 数据集的大小为 (730, 1)
我想对 covid 数据集和带有时间戳的能耗进行切片和窗口化（批处理大小 = 128，窗口大小为 96）。最后将两个数据集压缩在一起。
这是我的代码片段和窗口，包括能耗和时间戳数据集，即 covid 数据集：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000


def windowed_dataset(series, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
数据集 = tf.data.Dataset.from_tensor_slices(series) 
数据集 = 数据集.window(window_size + 1, shift=1) 
数据集 = 数据集.flat_map(lambda window: window.batch(window_size + 1)) # 顺序数据集保持不变
数据集 = 数据集.shuffle(1000)
数据集 = 数据集.map(lambda window: (window[:-1], window[-1][0])) 
数据集 = 数据集.padded_batch(128,drop_remainder=True).cache()

返回数据集


下面是我如何压缩数据集：

train_energy_dataset = windowed_dataset(TRAIN.PRE_PROCESSED_SERIES)
train_covid_dataset = windowed_dataset(TRAIN.SERIES_COVID)

train_dataset = tf.data.Dataset.zip((train_covid_dataset,train_energy_dataset))

train_dataset = train_dataset.shuffle(buffer_size=1000)
train_dataset = train_dataset.map(lambda data1, data2: ((data1[0], data2[0]), data1[1])) # 根据需要调整映射
train_dataset = train_dataset.batch(128, drop_remainder=True).cache()

我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(128,1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(128,2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2)

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate(axis=1)([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2], output=output)
# 定义最终模型
return model_final


model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

尝试窗口化和压缩数据集时，我收到错误信息
ValueError: Exception调用 Functional.call() 时遇到。

输入 Tensor(&quot; functional_100_1/Cast_1:0&quot;, shape=(128, 128, None, 2), dtype=float32) 的输入形状无效。预期形状 (None, 128, 2)，但输入具有不兼容的形状 (128, 128, None, 2)

Functional.call() 接收的参数：
• 输入=(&#39;tf.Tensor(shape=(128, 128, None, 1), dtype=float64)&#39;, &#39;tf.Tensor(shape=(128, 128, None, 2), dtype=float64)&#39;)
• 训练=True
• 掩码=(&#39;None&#39;, &#39;None&#39;)

我还通过将模型的输入形状更改为来修复了此问题
 输入1 = tf.keras.layers.Input(shape=(128,2), name=&quot;input1&quot;)
输入2 = tf.keras.layers.Input(shape=(128), name=&quot;input2&quot;)

错误按预期出现 2 个维度，但收到 3 个维度。

我的期望是将 zip 数据集输入到我的 CNN_M_LSTM 模型中。]]></description>
      <guid>https://stackoverflow.com/questions/78752243/how-to-concatenate-inputs-parameters-to-the-cnn-m-lstm-model</guid>
      <pubDate>Mon, 15 Jul 2024 23:38:49 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 的树如何在推理中评估 NaN？</title>
      <link>https://stackoverflow.com/questions/78752110/how-do-sklearns-trees-evaluate-nans-on-inference</link>
      <description><![CDATA[假设我们已经拟合了一个 sklearn.tree.DecisionTreeClassifier 对象，如下所示：

如果我们想预测此观察的类别：
x = {
&quot;col_58&quot;: 5000,
&quot;col_1110&quot;: 0,
&quot;col_28&quot;: None,
...
}

我认为它会落在节点 #6 上，因为我已经看到对象 DecisionTreeClassifier 创建了以下类型的分割col_1 &lt;= inf，我假设“inf”表示 NaN。但是当我测试这棵树时，当我执行 my_fitted_tree.apply(x) 时，我得到的是节点 #3。
这个观察示例应该落在节点 #3 还是节点 #6？为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78752110/how-do-sklearns-trees-evaluate-nans-on-inference</guid>
      <pubDate>Mon, 15 Jul 2024 22:35:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TensorFlow 在 python 中用大型 txt 文件训练神经网络模型？</title>
      <link>https://stackoverflow.com/questions/78752044/how-to-train-neural-network-model-with-large-txt-files-in-python-using-tensorflo</link>
      <description><![CDATA[我构建了一个具有 65 个输入神经元和 4880 个输出神经元的模型。我的训练数据存储在两个大型文本文件中：“X_train.txt”包含每行代表 65 个数字的列表，而“Y_train.txt”包含每行代表一个索引号的行。我需要对“Y_train”执行独热编码，以在指定索引处创建一个包含 4880 个零和一个“1”的列表。
由于这些文件的大小，我想分批训练我的模型。如何使用 TensorFlow 在 Python 中有效地使用这些 txt 文件训练我的模型？
因此我尝试获取这些文件并尝试将这些文件转换为变量
with open(xPath, &#39;r&#39;) as file:
line = file.readline()
while line:
x_train.append(eval(line.strip())) 
line = file.readline()
x_train = np.array(x_train)

但由于文件太大，需要花费太多时间，这不是我可以等待的主要问题，但主要问题是它使用了太多内存……]]></description>
      <guid>https://stackoverflow.com/questions/78752044/how-to-train-neural-network-model-with-large-txt-files-in-python-using-tensorflo</guid>
      <pubDate>Mon, 15 Jul 2024 22:05:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 巨大数据集的限制？</title>
      <link>https://stackoverflow.com/questions/78751909/pytorch-huge-dataset-constraints</link>
      <description><![CDATA[我正在 Pytorch 中训练一个模型来进行图像到图像的处理。我的数据非常庞大，形状为 (64152, 3, 5, 2, 64, 144)。我正在使用 memmapping 尽可能节省内存。
由于这个数据集的庞大规模，我面临的问题：
1.) 在分成训练/测试集之前进行改组
2.) 规范化数据
即使在进行 memmapping 时，这些操作也会导致内存峰值，不可避免地会超过我计算机上可用的内存量。
我尝试过 memmapping，一次沿较小的维度进行规范化。不确定在分成训练和测试集之前要尝试什么来改组数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78751909/pytorch-huge-dataset-constraints</guid>
      <pubDate>Mon, 15 Jul 2024 21:13:56 GMT</pubDate>
    </item>
    <item>
      <title>如何解决引导式利润计算中的零损失风险：机器学习</title>
      <link>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</link>
      <description><![CDATA[我刚刚为我的训练营完成了一个项目。然而，无论我如何进行利润计算或引导，我仍然面临零损失的风险。我不知道该如何解决这个问题。
\### 利润计算的关键值

BUDGET = 100 \* 10\*\*6 # 1 亿美元

REVENUE_PER_BARREL = 4.5 # 每桶 4.5 美元

WELLS_SELECTED = 200 # 选定用于开发的油井数量

COST_PER_WELL = BUDGET / WELLS_SELECTED # 每口井的成本

sufficient_volume = COST_PER_WELL / REVENUE_PER_BARREL

### 利润计算函数
def calculate_profit(predictions, target, n_wells, revenue_per_barrel, cost_per_well):
selected_indices = predictions.sort_values(ascending=False).head(n_wells).index
selected_reserves = target.loc[selected_indices].sum()
收入 = selected_reserves * 每桶收入 * 1000
利润 = 收入 - (每井成本 * n_wells)
返回利润

### 引导技术
def bootstrap_profit(predictions, target, n_wells, 每桶收入, 每井成本, n_samples=1000):
state = np.random.RandomState(42)
利润 = []
for _ in range(n_samples):
sample_indices = state.choice(predictions.index, size=n_wells, replace=True)
sample_predictions = predictions.loc[sample_indices]
sample_target = target.loc[sample_indices]
利润 = calculate_profit(sample_predictions, sample_target, n_wells,每桶收益，每井成本)
利润。附加(利润)
利润 = pd.Series(利润)
平均利润 = 利润。平均值()
下限 = 利润。分位数(0.025)
上限 = 利润。分位数(0.975)
损失风险 = (利润 &lt; 0).平均值() * 100
返回平均利润，(下限，上限)，损失风险
]]></description>
      <guid>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</guid>
      <pubDate>Mon, 15 Jul 2024 20:51:56 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CLIP 模型获取多模态嵌入？</title>
      <link>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</link>
      <description><![CDATA[我希望使用 CLIP 来获取多模态（图像和文本）数据行的单个嵌入。
假设我有以下模型：
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

def convert_image_data_to_tensor(image_data):
return torch.tensor(image_data)

dataset = df[[&#39;image_data&#39;, &#39;text_data&#39;]].to_dict(&#39;records&#39;)

embeddings = []
for data in dataset:
image_tensor = convert_image_data_to_tensor(data[&#39;image_data&#39;])
text = data[&#39;text_data&#39;]

input = processing(text=text, images=image_tensor, return_tensors=True)
with torch.no_grad():
output = model(**inputs)

我想获取 output 中计算的嵌入。我知道 output 具有附加属性 text_embeddings 和 image_embeddings，但我不确定它们以后如何交互。如果我想为每个记录获取单个嵌入，我应该将这些属性连接在一起吗？是否有其他属性以其他方式将两者结合起来？
这些是存储在输出中的属性：
print(dir(output))

[&#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__dataclass_fields__&#39;, &#39;__dataclass_params__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__post_init__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;image_embeds&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;logits_per_image&#39;, &#39;logits_per_text&#39;, &#39;loss&#39;, &#39;move_to_end&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;text_embeds&#39;, &#39;text_model_output&#39;, &#39;to_tuple&#39;, &#39;update&#39;, &#39;values&#39;, &#39;vision_model_output&#39;]

此外，有没有办法指定 CLIP 输出的嵌入的大小？类似于如何在 BERT 配置中指定嵌入大小？
在此先感谢您的帮助。如果我误解了这里任何关键内容，请随时纠正我。]]></description>
      <guid>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</guid>
      <pubDate>Mon, 15 Jul 2024 19:53:18 GMT</pubDate>
    </item>
    <item>
      <title>在 JAX 中对多个输入求导</title>
      <link>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</link>
      <description><![CDATA[我试图在 JAX 中求函数的一阶和二阶导数，但是我这样做却得到了错误的数字或零。我有一个数组，每个变量有两列，每个输入有两行
import jax.numpy as jnp
import jax

rng = rng = jax.random.PRNGKey(1234)
array = jax.random.normal(rng, (2,2))

两个测试函数
def F1(arr):
return 1/arr

def F2(arr):
return jnp.array([arr[0]**2 + arr[1]**3])

以及两种取一阶和二阶导数的方法，其中一种方法使用 jax.grad()
def dF_m1(arr, F):
return jax.grad(lambda arr: F(arr)[0])(arr)

def ddF_m1(arr, F, dF):
return jax.grad(lambda arr: dF(arr, F)[0])(arr)

另一个使用 jax.jacobian()
def dF_m2(arr, F):
jac = jax.jacobian(lambda arr: F(arr))(arr)
return jnp.diag(jac)

def ddF_m2(arr, F, dF):
hess = jax.jacobian(lambda arr: dF(arr, F))(arr)
return jnp.diag(hess)

使用这两种方法计算每个函数的一阶和二阶导数（和误差）可得出以下结果
exact_dF1 = (-1/array**2)
exact_ddF1 = (2/array**3)

print(&quot;函数 1 使用所有 grad()&quot;)
dF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)
ddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)
print(dF1_m1 - exact_dF1,&quot;\n&quot;)
print(ddF1_m1 - exact_ddF1,&quot;\n&quot;)

print(&quot;函数 1 使用所有 jacobian()&quot;)
dF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)
ddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)
print(dF1_m2 - exact_dF1,&quot;\n&quot;)
print(ddF1_m2 - exact_ddF1,&quot;\n&quot;)

输出
函数 1 使用所有 grad()
[[ 0. 48.43877 ]
[ 0. 0.62903005]] 

[[ 0. 674.248 ]
[ 0. 0.9977852]] 

函数 1 使用所有 jacobian()
[[0. 0.]
[0. 0.]] 

[[0. 0.]
[0. 0.]] 

和
exact_dF2 = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))
exact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))

print(&quot;函数 2 使用所有 grad()&quot;)
dF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)
ddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)
print(dF2_m1 - exact_dF2,&quot;\n&quot;)
print(ddF2_m1 - exact_ddF2,&quot;\n&quot;)

print(&quot;函数 2 使用所有 jacobian()&quot;)
dF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)
ddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)
print(dF2_m2 - exact_dF2,&quot;\n&quot;)
print(ddF2_m2 - exact_ddF2,&quot;\n&quot;)

输出
函数 2 使用所有 grad()
[[0. 0.]
[0. 0.]] 

[[0. 0.86209416]
[0. 7.5651155 ]] 

使用所有 jacobian() 的函数 2
[[ 0. -0.10149619]
[ 0. -6.925739 ]] 

[[0. 2.8620942]
[0. 9.565115 ]] 

我更愿意只对 F1 之类的东西使用 jax.grad()，但现在似乎只有 jax.jacobian 有效。这完全是因为我需要计算神经网络相对于其输入的高阶导数。感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</guid>
      <pubDate>Mon, 15 Jul 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：除了最后一个维度之外，`labels.shape` 必须等于 `logits.shape`。收到：labels.shape=(240,) 和 logits.shape=(16, 14)</title>
      <link>https://stackoverflow.com/questions/78751134/valueerror-labels-shape-must-equal-logits-shape-except-for-the-last-dimensi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78751134/valueerror-labels-shape-must-equal-logits-shape-except-for-the-last-dimensi</guid>
      <pubDate>Mon, 15 Jul 2024 17:11:05 GMT</pubDate>
    </item>
    <item>
      <title>如何让 PyTorch Geometric DataLoader 批量创建多个图形而不是一个巨型图形？</title>
      <link>https://stackoverflow.com/questions/78750828/how-to-make-pytorch-geometric-dataloader-to-create-multiple-graphs-in-batch-inst</link>
      <description><![CDATA[PyTorch Geometric DataLoader 在使用大小大于 1 的批次时，会创建一个包含孤立子图的大图，而不是填充大小相等的图列表。不幸的是，这种方式不适合我的任务。那么有没有办法让 PyTorch Geometric DataLoader 创建一个图列表而不是一个大图？]]></description>
      <guid>https://stackoverflow.com/questions/78750828/how-to-make-pytorch-geometric-dataloader-to-create-multiple-graphs-in-batch-inst</guid>
      <pubDate>Mon, 15 Jul 2024 15:57:22 GMT</pubDate>
    </item>
    <item>
      <title>nnUNet - 如何对从与训练集不同的 tif 文件中提取的测试集运行预测？</title>
      <link>https://stackoverflow.com/questions/78750529/nnunet-how-can-i-run-a-prediction-on-a-test-set-that-was-extracted-from-a-diff</link>
      <description><![CDATA[我正在使用 nnUNet 在 2D 中进行图像分割。我有 3D tif 文件、图像和蒙版。
为了训练模型并使数据适合 nnUNet 的管道，我正在对 tif 文件进行切片，因此每个帧将是一个单独的 tif 文件。因此，如果图像及其匹配的蒙版在第一维上为 150，那么切片后我将获得 150 帧（每个帧具有相同的 2D 形状）。
现在我正在准备数据，以便它适合管道名称格式等。
然后，我在数据集上运行训练，让我们坐在数据集 1 上。
现在，如果我想使用在数据集 1 上训练的模型对数据集 2 进行预测，我该怎么做？
我使用以下命令进行预测：
nnUNetv2_predict -d Dataset1 -i /path/to/test_set -o /path/to/predictions -f 0 1 2 3 4 -tr nnUNetTrainer -c 2d -p nnUNetPlans
我输入了测试集的路径，但模型如何知道哪个训练集用于训练？它是使用我训练过的最后一个训练集还是我如何指定我想要使用哪一个？
提前谢谢您
我尝试使用在另一个图像的另一个训练集上训练的模型来预测测试集。而且我不确定我得到的结果是否正确，因为我不知道预测使用的是哪个训练模型。
我也试图在这里找到答案：nnUNet
但不幸的是，我没有找到。]]></description>
      <guid>https://stackoverflow.com/questions/78750529/nnunet-how-can-i-run-a-prediction-on-a-test-set-that-was-extracted-from-a-diff</guid>
      <pubDate>Mon, 15 Jul 2024 14:53:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 keras 将 3 个输入参数连接到 CNN_M_LSTM 模型？</title>
      <link>https://stackoverflow.com/questions/78749657/how-to-concatenate-3-inputs-parameters-to-the-cnn-m-lstm-model-using-keras</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow）中。
能耗和时间戳的大小为 (70082, 2)
Covid 数据集的大小为 (744, 1)
我曾使用张量切片和张量压缩将两个数据打包在一起并对数据集进行窗口化：
这是我对能耗和时间戳数据集（covid 数据集）进行压缩和窗口化的代码：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000

def windowed_dataset(series_energy,series_covid, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
dataset_energy = tf.data.Dataset.from_tensor_slices(series_energy) 
dataset_covid = tf.data.Dataset.from_tensor_slices(series_covid) 
dataset = tf.data.Dataset.zip(dataset_energy,dataset_covid)
dataset = dataset.window(96 + 1, shift=1) #
dataset = dataset.flat_map(lambda window_covid, window_series: tf.data.Dataset.zip((window_covid, window_series)).batch(96 + 1))
dataset = dataset.shuffle(1000)
dataset = dataset.map(lambda window_covid, window_series: (window_covid[:-1], window_series[-1][0])) 
dataset = dataset.padded_batch(128,drop_remainder=True).cache()

返回数据集

对于模型 CNN_M_LSTM，我创建了 2 个输入。这是我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(96, 1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(96, 2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2[:, -1, :])

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate()([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2],outputs=output)
# 定义最终模型
return model_final


我如何拟合我的模型：
model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

对于上述试验，错误是模型需要 2 个输入，但收到 1 个输入张量。
我的期望是将 3 个输入输入到我的 CNN_M_LSTM 模型中。此外，3 个输入数据集必须全部打包在一起，窗口大小为 96，批处理大小为 128。]]></description>
      <guid>https://stackoverflow.com/questions/78749657/how-to-concatenate-3-inputs-parameters-to-the-cnn-m-lstm-model-using-keras</guid>
      <pubDate>Mon, 15 Jul 2024 11:49:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>多元时间序列数据的准备</title>
      <link>https://stackoverflow.com/questions/78467998/preparation-of-multivariate-time-series-data</link>
      <description><![CDATA[我正在做一个关于指数/股票价格预测的大学项目。我计划使用组合的 cnn-lstm 模型，我有几种不同类型的数据：开盘价最高价最低价收盘价成交量、价值、基本面数据（如失业率和各种利率）、技术指标（如 RSI、MACD 等）以及移动平均线（如 SMA、EMA、WMA 等）。为网络准备数据的最佳方法是什么？
目前，我正在使用以下转换
对于 OHLC - 简单微分
对于基本数据 - 对数化
对于移动平均线 - 从此移动平均线的值中减去蜡烛开盘值
指标值不变
然后我对所有数据集使用 StandardizeNormalizer。我还尝试分别对每个序列进行规范化（稳健缩放、标准化、最小最大缩放），并对所有数据进行微分，但效果不佳]]></description>
      <guid>https://stackoverflow.com/questions/78467998/preparation-of-multivariate-time-series-data</guid>
      <pubDate>Sun, 12 May 2024 13:20:10 GMT</pubDate>
    </item>
    </channel>
</rss>