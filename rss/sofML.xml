<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 10 Feb 2025 12:32:52 GMT</lastBuildDate>
    <item>
      <title>降尺度操作不捕获峰</title>
      <link>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</link>
      <description><![CDATA[我正在使用随机的森林回归模型将数据集降低从更粗的分辨率到更高分辨率。我尝试了交叉验证并调整参数。在检查验证分数时，我的MSE和R2值都相当好。但是，当我使用相同的模型预测新数据集的目标变量时，该模型将无法捕获数据初始数年的峰值。可能是其中一个功能具有主要的重要性。在绘制主要功能的数据时，全年数据稳定。
这使趋势偏离了
具有最重要性的数据之一  ]]></description>
      <guid>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</guid>
      <pubDate>Mon, 10 Feb 2025 12:29:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在大型数据集上训练XGBoost并改善欺诈检测？</title>
      <link>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</link>
      <description><![CDATA[我只是从ML开始，所以我感谢任何建议。
我正在尝试培训一个模型以进行交易中的欺诈检测。数据高度不平衡（〜96％的正常人比〜4％欺诈）。
第一期 - 记忆消耗
培训文件为32 GB，但是即使仅阅读100万行，我也会收到一个内存分配错误：
  XGBOOST.CORE.XGBOOSTERROR：BAD_MALLOC：无法分配255479999900字节。
 
第二期 - 预测质量差
我正在对100k行进行训练，但是无论我如何调整XGBoost，该模型都几乎都无法检测到欺诈案件。
在这种情况下，XGBoost的最佳类平衡技术是什么？我应该如何处理这个数据集？您建议更改什么？
在此处输入图像描述 
  df = pd.read_csv（&#39;train.csv&#39;，nrows = 100000） 

df.drop（[&#39;transaction_id&#39;，&#39;card_holder_first_name&#39;，&#39;card_holder_last_name&#39;，&#39;is_verified&#39;，&#39;browser&#39;，&#39;browser_version&#39;，&#39;browser_version&#39;，&#39;
    &#39;operation_system&#39;，&#39;operation_system_version&#39;，&#39;card_id&#39;，&#39;ip_address&#39;，&#39;merchant_customer_id&#39;，&#39;merchant_id&#39;，&#39;user_agent&#39;，&#39;user_agent&#39;，
    &#39;merchant_customer_last_name&#39;，&#39;merchant_customer_first_name&#39;，&#39;merchant_customer_phone&#39;，&#39;merchant_customer_email&#39;，&#39;bin&#39;，&#39;bin&#39;，&#39;&#39;
    &#39;clockal_source&#39;，&#39;transaction_source&#39;，&#39;merchant_city&#39;，&#39;merchant_shop_id&#39;，&#39;merchant_shop_name&#39;，&#39;order_number&#39;]， 
    轴= 1，Inplace = true）

df [&#39;bank&#39;]。替换（&#39;&#39;，&#39;_&#39;，regex = true，inplace = true）

df [&#39;create_at&#39;] = pd.to_datetime（df [&#39;create_at&#39;]）
df [&#39;seconds_since_midnight&#39;] = df [&#39;create_at&#39;]。dt.hour * 3600 + df [&#39;create_at&#39;]。dt.minute * 60 + df [&#39;create_at&#39;]。dt.dt.second
df [&#39;day_of_week&#39;] = df [&#39;create_at&#39;]。dt.weekday

df.drop（&#39;create_at&#39;，axis = 1，Inplace = true）

df.loc [pd.isna（df [&#39;merchant_language&#39;]），&#39;merchant_language&#39;] =&#39;unknown&#39;

df.loc [pd.isna（df [&#39;payment_type&#39;]），&#39;payment_type&#39;] = 0

x = df.drop（&#39;is_fraud&#39;，axis = 1）.copy（）

y = df [&#39;is_fraud&#39;]。copy（）

x_encoded = pd.get_dummies（x，columns = [&#39;merchant_country&#39;，      
                                        &#39;transaction_type&#39;，
                                        “商人_language”，
                                        &#39;平台&#39;，
                                        &#39;ip_country&#39;，
                                        &#39;银行&#39;，
                                        “ cardbrand&#39;，
                                        &#39;cardcountry&#39;，
                                        &#39;cardtype&#39;， 
                                        &#39;payment_type&#39;]））


x_train，x_test，y_train，y_test = train_test_split（x_encoded，y，andury_state = 42，strate = y）


clf_xgb = xgb.xgbClassifier（
    objective =;二进制：logistic; quot;
    种子= 42，
    eval_metric =＆quot; aucpr; quot;
    早期_stopping_rounds = 10，
    max_depth = 6，  
    子样本= 0.8，  
    colsample_bytree = 0.8 
）

clf_xgb.fit（
    x_train，
    y_train，
    eval_set = [（x_test，y_test）]，
    冗长= true
）

disp =混淆matrixdisplay.from_estimator（
    clf_xgb，  
    x_test，  
    y_test，  
    display_labels = [不是欺诈者“  
    cmap =&#39;blues;
）
disp.plot（values_format =&#39;d&#39;）

plt.show（）
 
我是ML的新手，所以我还没有尝试过很多技术。我尝试了不同的XGBoost参数，并减少了数据集大小以适合内存，但是该模型仍在努力检测欺诈情况。我不确定最好的方法是处理如此大的不平衡数据集，因此我感谢任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</guid>
      <pubDate>Mon, 10 Feb 2025 11:56:48 GMT</pubDate>
    </item>
    <item>
      <title>如何后处理基于YOLO的实例分割模型，该模型可产生10个张量作为输出？</title>
      <link>https://stackoverflow.com/questions/79426679/how-to-post-process-the-output-of-a-yolo-based-instance-segmentation-model-that</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79426679/how-to-post-process-the-output-of-a-yolo-based-instance-segmentation-model-that</guid>
      <pubDate>Mon, 10 Feb 2025 09:55:13 GMT</pubDate>
    </item>
    <item>
      <title>测试分类问题的数据配置[关闭]</title>
      <link>https://stackoverflow.com/questions/79426228/test-data-configuration-for-a-classification-problem</link>
      <description><![CDATA[我创建了一个用于培训神经网络的二进制分类问题的数据集。培训数据来自与特定环境（例如2D地图环境）有关的集合。对于测试案例，我考虑了来自同一2D地图的数据点，但培训集中不存在数据点。
这将是有效的测试用例设计？]]></description>
      <guid>https://stackoverflow.com/questions/79426228/test-data-configuration-for-a-classification-problem</guid>
      <pubDate>Mon, 10 Feb 2025 06:23:53 GMT</pubDate>
    </item>
    <item>
      <title>ML模型保存</title>
      <link>https://stackoverflow.com/questions/79426066/ml-models-saving</link>
      <description><![CDATA[我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类。运行培训时期后，我使用代码将模型保存在.pth文件中。但是，在下载了保存的模型并将其与我的接口连接起来后，它没有产生任何结果。经过进一步检查，似乎模型是空的。
 导入火炬
导入Torch.nn作为nn
导入Torch.optim作为最佳
来自TQDM.Auto Import TQDM
来自Torchvision导入模型

＃设置设备（GPU如果可用）
设备= torch.device（&#39;cuda&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;）

＃定义Resnet18模型
类Resnet18（nn.Module）：
    def __init __（self，num_classes）：
        super（resnet18，self）.__ init __（）
        self.model = model.Resnet18（预审计= false）＃设置为true以使用预训练的权重
        self.model.fc = nn.linear（self.model.fc.in_features，num_classes）＃修改最终层

    def向前（self，x）：
        返回self.model（x）

＃用数据集中的类数初始化模型
num_classes = len（train_data_simple.classes）＃根据您的增强数据集进行调整
model_resnet = resnet18（num_classes = num_classes）。

＃损失功能和优化器（SGD和Crossentropy）
loss_fn = nn.Crossentropyloss（）
优化器= optim.sgd（model_resnet.parameters（），lr = 0.1，动量= 0.9）

＃训练循环（使用TQDM进行进度栏）
def train（型号，train_dataloader，test_dataloader，优化器，loss_fn，epochs）：
    对于TQDM中的时期（范围（时代））：
        model.train（）＃将模型设置为训练模式
        Running_loss = 0.0
        recript_train，total_train = 0，0

        对于枚举（train_dataloader）的批次（x，y）：
            x，y = x.to（设备），y.to（设备）

            优化器.zero_grad（）＃零梯度

            输出=模型（x）＃前向通行证
            损失= loss_fn（输出，y）＃计算损失

            lose.backward（）＃backpropagation
            Optimizer.step（）＃更新权重

            running_loss += loss.item（）
            _，预测= torch.max（outputs.data，1）
            total_train += y.size（0）
            recripe_train +=（预测== y）.sum（）。item（）

        ＃打印培训损失和准确性
        打印（f＆quot; epoch [{epoch+1}/{epochs}]，损失：{runn_loss/len（train_dataloader）：。4f}＆quort;）
        打印（f＆quot“训练精度：{100 * recript_train / total_train：.2f}％＆quot”）

        ＃ 验证
        model.eval（）＃开关模型到评估模式进行验证
        recript_val，total_val = 0，0
        使用Torch.no_grad（）：
            对于x_val，y_val在test_dataloader中：
                x_val，y_val = x_val.to（设备），y_val.to（设备）
                val_outputs =模型（x_val）
                _，预测= torch.max（val_outputs.data，1）
                total_val += y_val.size（0）
                prection_val +=（预测== y_val）.sum（）。item（）。

        ＃打印验证精度
        打印（f＆quot“验证精度：{100 * recript_val / total_val：.2f}％＆quort”）

    返回模型

＃示例培训
num_epochs = 30＃根据您的要求调整时期
＃使用增强培训数据加载器
trained_model_resnet = train（model_resnet，train_dataloader_simple，test_dataloader_simple，optimizer，optimizer，lose_fn，num_epochs）

 
这是我在不同单元格中运行的代码来保存模型
 ＃保存训练有素的模型
model_path =＆quot trained_resnet18_model.pth＆quot;
TORCH.SAVE（model_resnet.state_dict（），model_path）
打印（f＆quot“模型保存到{model_path}”
 
我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类，使用pytorch分为四类。经过几个时期的训练后，我想使用诸如model.save（&#39;my_model.h5&#39;）或torch.save（model.state_dict（），&#39;model.pth&#39;）的代码保存训练的模型。
我应该添加代码以将训练的模型保存在定义训练时期并运行训练环的同一单元格中，还是将其放在其他单元格中更好？放置是否会影响模型的保存方式，或者以后如何加载？]]></description>
      <guid>https://stackoverflow.com/questions/79426066/ml-models-saving</guid>
      <pubDate>Mon, 10 Feb 2025 04:15:24 GMT</pubDate>
    </item>
    <item>
      <title>SAM中的细分网格地理空间错误：不可订阅的非类型对象</title>
      <link>https://stackoverflow.com/questions/79425639/segment-geospatial-error-in-sam-predict-nonetype-object-is-not-subscriptable</link>
      <description><![CDATA[我正在使用图书馆 segment-geospatial 我有图像。
  image =&#39;/content/drive/mydrive/myimage.tiff&#39;
sam = samgeo（
    model_type =; vit_h＆quot;
    自动= false，
    sam_kwargs =无，
）

sam.set_image（图像）

盒子= [
        [-51.2546，-22.1771，-51.2541，-22.1767]，
        [-51.2538，-22.1764，-51.2535，-22.1761]，
这是给出的

SAM.PREDECT（boxes = box，point_crs =;
 
当我运行sam.predict（）时，我会收到以下错误：
 
找不到有效的像素坐标。
----------------------------------------------------------------------------- --------------------------------------
TypeError Trackback（最近的最新通话）
＆lt; ipython-Input-72-A3B6CDCAB301＆gt;在＆lt;单元线：0＆gt;（）
----＆gt; 1 sam.predict（boxes = box，point_crs =; epsg：4326＆quot; output =; mask.tif; dtype =; uint8＆quort;

/usr/local/lib/python3.11/dist-packages/samgeo/samgeo.py in prediact（self，point_coords，point_coords，point_labels，box，point_crs，point_crs，mask_input，multimask_output，multimask_output，return_logits，return_logits，returation_logits，returation_logits，returat *Kwargs）
    615坐标= bbox_to_xy（self.source，box，point_crs）
    616 input_boxes = np.array（coords）
 - ＆gt; 617如果isInstance（坐标[0]，int）：
    618 input_boxes = input_boxes [none，：]
    619其他：

TypeError：“非型”对象不可订阅
 
我已经检查了栅格图像crs格式是正确的，我已经尝试了使用它们在文档中提供的示例的代码，并且效果很好，但是当我使用栅格尝试时，它不起作用。
 https://samgeo.gishub.org.org/examples/box_prompts/box_prompts/ ]]></description>
      <guid>https://stackoverflow.com/questions/79425639/segment-geospatial-error-in-sam-predict-nonetype-object-is-not-subscriptable</guid>
      <pubDate>Sun, 09 Feb 2025 20:23:01 GMT</pubDate>
    </item>
    <item>
      <title>CIFAR100的82％测试准确性[封闭]</title>
      <link>https://stackoverflow.com/questions/79425154/82-test-accuracy-on-cifar100</link>
      <description><![CDATA[如何使用100万个参数（或少于1m）和小于80-100m的拖鞋上的CIFAR-100实现82％的测试准确性，固定了50个时期？
我尝试了MobilenetV2并获得了70％的测试准确性，然后有效NETB0并获得了77％的准确性。]]></description>
      <guid>https://stackoverflow.com/questions/79425154/82-test-accuracy-on-cifar100</guid>
      <pubDate>Sun, 09 Feb 2025 15:30:02 GMT</pubDate>
    </item>
    <item>
      <title>使用y作为输入</title>
      <link>https://stackoverflow.com/questions/79423737/gnn-multinode-prediction-yeilds-constant-output-when-using-y-as-input</link>
      <description><![CDATA[
 y：8个值，2D向量
 x：与y 相同
 y_hat：始终恒定
数据：始终8个节点的完全连接图
模型：普通同质GCN 
损失功能：MSE 
假设，x：y应具有1：1的相关性，因此它应该能够为y提供足够的预测能力。
问题：为什么总是恒定？或方法本身不正确？

输出：
 最终模型输出：张量（[[[0.2878]，，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]]，grad_fn =＆lt; addmmbackward0＆gt;）
    目标值（y）：张量（[[0.2037]，，，
            [0.5126]，
            [0.6616]，
            [0.7587]，
            [0.3367]，
            [0.2623]，
            [0.7544]，
            [0.6320]]）
 
代码：
 导入火炬
导入Torch.nn.功能为f
来自torch_ geometric.nn导入gcnconv

＃定义更强大的GNN模型
GNN类（Torch.nn.Module）：
    def __init __（self，num_node_features，hidden_​​channels，output_channels）：
        超级（gnn，self）.__ init __（）
        self.conv1 = gcnconv（num_node_features，hidden_​​channels）
        self.conv2 = gcnconv（hidden_​​channels，hidden_​​channels）
        self.conv3 = gcnconv（hidden_​​channels，hidden_​​channels）＃附加层
        self.fc = torch.nn.linear（hidden_​​channels，output_channels）

    def向前（self，x，edge_index）：
        x = self.conv1（x，edge_index）
        x = f.sigmoid（x）
        x = self.conv2（x，edge_index）
        x = f.sigmoid（x）
        x = self.conv3（x，edge_index）＃附加图层
        x = f.sigmoid（x）
        x = self.fc（x）
        返回x

＃创建一个合成数据集
num_nodes = 8
num_node_features = 1
x = torch.rand（（num_nodes，num_node_features））＃随机节点功能
edge_index = torch.tensor（[[i，j]在range（num_nodes）in range（num_nodes）in range（num_nodes）（如果i！= j]，dtype = type = torch.long）.t（）.t（）。连续（）。
y = torch.rand（（（num_nodes，1））＃随机目标排名
x = y
＃初始化模型
model = gnn（num_node_features，hidden_​​channels = 32，output_channels = 1）

＃定义损失功能和优化器
标准= torch.nn.mseloss（）
优化器= torch.optim.adam（model.parameters（），lr = 0.001）
调度程序= torch.optim.lr_scheduler.steplr（优化器，step_size = 30，伽马= 0.1）

＃训练循环
对于范围（100）的时期：
    优化器.zero_grad（）
    out =型号（x，edge_index）
    损失=标准（否，y）
    loss.backward（）
    torch.nn.utils.clip_grad_norm_（model.parameters（），max_norm = 1.0）＃渐变剪法
    优化器.step（）
    Scheduler.step（）
    print（f＆quot; epoch {epoch+1}，损失：{loss.item（）}; quot;）

＃检查最终输出
out =型号（x，edge_index）
打印（“最终型号输出：＆quot of）
打印（“目标值（y）：＆quot”，y）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79423737/gnn-multinode-prediction-yeilds-constant-output-when-using-y-as-input</guid>
      <pubDate>Sat, 08 Feb 2025 18:41:03 GMT</pubDate>
    </item>
    <item>
      <title>在扩散模型中U形网络的上下抽样阶段中不同的缩小参数</title>
      <link>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</link>
      <description><![CDATA[我遇到的错误是
 文件＆quot＆quot＆quot＆quot&gt;
    x = torch.cat（（x，residual_x），dim = 1） 
RuntimeError：张量的尺寸必须匹配，除了尺寸1。预期尺寸24，但在列表中获得张量1的尺寸25。
 
该模型的训练数据集输入是Paviau。
调试后要找到问题的输出向量如下：
  down = torch.size（[4，64，25，64，64]）
UP = Torch.Size（[4，64，24，64，64]）
 
整个代码如下：
 
    def向前（self，x，timeStep，feature = false）：
        ＃嵌入式时间
        t = self.time_mlp（timeStep）
        ＃初始转换
        x = self.conv0（x）
        ＃UNET
        ristual_inputs = []
        在self.downs中进行下降：
            x = down（x，t）
            ristual_inputs.append（x）
        为了自我。
            ristual_x = ristual_inputs.pop（）
            ＃print（down =; quord =; residual_x.shape，＆quot up =; quord =; x.Shape）
            ＃添加残留X作为其他频道
            x = torch.cat（（x，residual_x），dim = 1）* 
            如果特征：
                self.features.append（x.detach（）。cpu（）。numpy（））
            x =向上（x，t）
        返回self.output（x）
         
 ]]></description>
      <guid>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</guid>
      <pubDate>Sat, 08 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[在实施使用 tf.data.dataset 作为KERAS模型的输入的训练环时，我遇到了问题。我的数据集具有以下格式的元素规范：
 （{&#39;data&#39;：tensorSpec（shape =（15000，1），dtype = tf.float32），&#39;index&#39;：tensorspec&#39;：tensorspec（shape =（2，2，2，2， ），dtype = tf.int64）}，tensorspec（shape =（1，），dtype = tf.int32））
 
因此，基本上，每个样本均以元组（x，y）构建形状（15000，1），另一个形状索引（2，）（在培训期间不使用索引）， y 是单个标签。
The tf.data.Dataset is created using dataset = tf.data.Dataset.from_tensor_slices((X, y)), where X是两个密钥的命令：

 数据：形状的NP数组（200k，1500，1）， index  with 
 索引：形状的NP数组（200K，2） 

和 y 是形状的单个数组（200k，1） 
我的数据集有大约200k的培训样本（运行底漆后）和200k验证样本。
呼叫 tf.data.dataset.from_tensor_slices 我注意到GPU内存使用情况中有一个尖峰，在创建培训 tf.dataset ，and和创建验证 tf.dataset 。之后，更多16GB
创建 tf.dataSet 后，我运行了一些操作（例如，洗牌，批处理和预拿方），并调用 model.fit.fit 。我的型号大约有500K可训练的参数。
我遇到的问题是  拟合模型。我需要在一些其他数据上运行推断，因此我使用此数据创建了一个新的 tf.dataset ，再次使用 tf.dataset.from_tensor_slices 。但是，我注意到培训和验证 tf.dataset 仍然存在于GPU内存中，这导致我的脚本随着新的 tf.dataset  i而遇到的不含内存问题 i想要推断。
我在两个 tf.dataset 上尝试调用 del ，然后随后调用 gc.collect（）清除RAM，而不是GPU内存。另外，我尝试禁用我应用的某些操作，例如预摘要，也可以使用批处理大小，但是这些都没有用。我还尝试调用 keras.backend.clear_session（），但它也无助于清除GPU内存。我还尝试从 numba 导入 cuda ，但是由于我的安装，我无法使用它来清除内存。我有什么办法可以从gpu内存中清除 tf.data.dataset ？]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8最终检测头仍输出（1、7、8400），而不是（1、8、8400）</title>
      <link>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</link>
      <description><![CDATA[我训练了3个类的Yolov8检测模型，但是原始的正向通行证仍然显示（1、7、8400）而不是（1、8、8400）的最终检测输出。
我做了什么：
检查了我的data.yaml：
  train：路径/to/tain/train/images
Val：路径/到/Val/图像
NC：3
名称：[&#39;神经胶质瘤&#39;，&#39;脑膜瘤&#39;，&#39;垂体&#39;]
 
确认的NC：3是正确的。
通过命令从头开始训练：
在
    data =路径/到/data.yaml \
    模型= yolov8x \
    epochs = 1000 \
    imgsz = 640 \
    设备= 1 \
    耐心= 100
 
训练没有错误地进行，并成功完成。
安装了最新的Ultrytics版本（v8.3.72），以确保没有版本问题：
  pip卸载Ultralytics
PIP安装超级词
 
直接加载了新的best.pt：
 超级timportic
导入火炬

型号= yolo（r＆quot; best.pt;）。模型
model.eval（）

dummy_input = torch.randn（1，3，640，640）
使用Torch.no_grad（）：
    输出=模型（Dummy_input）

输出输出：
    ＃一些输出是列表；仔细检查每个元素
    如果Isinstance（Out，Torch.Tensor）：
        打印（OUT.SHAPE）
    别的：
        打印（“列表输出：＆quot” [o。
 
控制台显示检测输出（1、7、8400）。
经过验证的模型元数据说NC = 3和Model.Names有3个类。但是，原始检测层输出仍然是7个通道。
观察：
如果Yolo检测层是真正的3类，则应输出（5 + 3）=每个锚点8个通道，而不是7通道。
不匹配（1、7、8400）通常表明尽管NC = 3。
问题 /请求帮助：
为什么即使我从头开始训练了3堂课，为什么还要原始检测头（1、7、8400）？
如何确保将检测层完全重新定位为（5 + 3）= 8以进行3级检测？
我尝试删除旧的.pt文件，重新检查我的data.yaml，重新安装超级图案和确认模型。model.nc== 3。但是，最终检测层继续产生7个频道，而不是8个频道。。
关于可能导致这种持续不匹配的什么想法？]]></description>
      <guid>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</guid>
      <pubDate>Thu, 06 Feb 2025 18:39:21 GMT</pubDate>
    </item>
    <item>
      <title>用Java编写的AI分类均匀，奇数行不通[关闭]</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么2048年游戏训练对我来说不正常？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</guid>
      <pubDate>Tue, 04 Feb 2025 10:28:14 GMT</pubDate>
    </item>
    <item>
      <title>Python中的聚类时间序列数据</title>
      <link>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</link>
      <description><![CDATA[我正在尝试使用不同的聚类技术将Python聚集时间序列数据。 K-均值没有很好的效果。以下图像是使用聚集聚类聚类后的群集后的图像。我还尝试了动态的时间扭曲。这两个似乎给出了类似的结果。 
我理想地拥有第二张图像中时间序列的两个不同的簇。第一个图像是用于快速增加的群集。第二种不像稳定的稳定，第三个是降低趋势的集群。我想知道哪个时间序列既稳定又流行（在这里流行，我的意思是高度计数）。我尝试了分层聚类，但结果显示了太多的层次结构，我不确定如何选择层次结构。有人可以阐明如何将第二张图像中的时间序列分为两个不同的群集，一个群集计数低，另一个群体计数很高？有可能吗？或者我应该在视觉上选择一个阈值将它们切成两个？
快速增加的群集：
    
稳定计数的集群：
    
趋势下降的集群：
    
这是非常非常模糊的，但这是我分层聚类的结果。 
   
我知道这个特殊的图像根本没有用，但这对我来说也是一个死胡同。 
一般而言，如果您想区分趋势，例如为YouTube视频说，如何只为“流行趋势”部分挑选一些，而其他一些则是“本周趋势”部分？我了解“趋势”部分视频是显示与第一张图像相似的特征的视频。 “本周趋势”部分有一系列视频，这些视频的观点计数很高，但在计数方面保持稳定（即没有显示快速增加）。我知道，在YouTube的情况下，除了观看计数外，还有许多其他因素。有了第二张图像，我要做的事情类似于“本周趋势”部分。我想挑选那些很高的计数。在这种情况下，如何拆分时间序列？ 
我知道DTW捕获了趋势。 DTW给出了与上图相同的结果。它已经确定了第二张图像中的趋势，即“稳定”。但这并未在此处捕获“计数”元素。我希望趋势和计数被捕获，在这种情况下，稳定且计数很高。
上面的图像是根据计数聚类的时间序列。我是否错过了其他可以实现这一目标的聚类技术？即使仅计数，我如何根据我的需求不同？
任何想法都将不胜感激。预先感谢！]]></description>
      <guid>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</guid>
      <pubDate>Thu, 10 Aug 2017 03:51:00 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习中识别非训练项目的可能性[关闭]</title>
      <link>https://stackoverflow.com/questions/45582182/possibility-of-identification-of-non-trained-item-in-machine-learning</link>
      <description><![CDATA[机器学习模型已经经过训练，可以识别动物和植物的名称。如果假设给出了汽车名称，是否可以说给定名称不属于动物或植物。如果可能的话，请提及实现此情况的方法或算法。
例如。如果给出了“狮子”或“椰子树”，则模型将预测“动物”或“树”类别。如果假设给出了“奥迪”，那么可以说给定物品既不属于“动物”或“植物”。 （注意：我听说机器学习模型将尝试适合类别）。]]></description>
      <guid>https://stackoverflow.com/questions/45582182/possibility-of-identification-of-non-trained-item-in-machine-learning</guid>
      <pubDate>Wed, 09 Aug 2017 05:18:26 GMT</pubDate>
    </item>
    </channel>
</rss>