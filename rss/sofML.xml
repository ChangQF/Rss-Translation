<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 08 Dec 2024 18:22:13 GMT</lastBuildDate>
    <item>
      <title>我无法在我的 GPU 上训练模型，并收到 CUDA-Assertion-Error</title>
      <link>https://stackoverflow.com/questions/79263006/i-cant-train-a-model-on-my-gpu-getting-cuda-assertion-error</link>
      <description><![CDATA[这是我的神经网络代码，使用 pytorch：
 class BLSTM(nn.Module): 
def __init__(self,vocab_size): 
super(BLSTM,self).__init__() 
self.Embeddings = nn.Embedding(vocab_size,100) 
self.LSTM = nn.LSTM(100,128,bidirectional=True) 
self.fc = nn.Linear(128*2,3) 
self.dropout = nn.Dropout(0.5) 
self.tanh = nn.Tanh() 

def forward(self,X): 
embed = self.Embeddings(X) 
print(embed) 
output , (hidden , cell) = self.LSTM(embed) 
last_hidden = output[:, -1,:] 
logits = self.fc(self.dropout(last_hidden)) 
self.tanh(logits) 
return logits

我在 model.to(&quot;device&quot;) 处收到 cuda 断言错误。指定的尺寸或网络是否存在问题。我已检查内存分配，没有问题，我将在下面发布错误
Cell In[52]，第 10 行
7 device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
8 print(device)
---&gt; 10 model.to(device)
11 X_train = np.squeeze(torch.tensor(X_train,dtype=torch.long))
12 X_val = np.squeeze(torch.tensor(X_val,dtype=torch.long))

文件 C:\Python312\Lib\site-packages\torch\nn\modules\module.py:1340，位于 Module.to(self, *args, **kwargs)
...
RuntimeError: CUDA 错误：设备端断言已触发
CUDA 内核错误可能会在其他 API 调用中异步报告，因此下面的堆栈跟踪可能不正确。
对于调试，请考虑传递 CUDA_LAUNCH_BLOCKING=1
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

有人能解释一下这个问题的真正原因吗？
我必须用给定的网络训练模型，但在将模型分配给 gpu 时，我遇到了这个错误。我搞不清楚。否则他们会向我显示张量错误。张量的维度是 1。]]></description>
      <guid>https://stackoverflow.com/questions/79263006/i-cant-train-a-model-on-my-gpu-getting-cuda-assertion-error</guid>
      <pubDate>Sun, 08 Dec 2024 17:29:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么两个不同的逻辑回归模型对同一源数据产生截然不同的结果？[关闭]</title>
      <link>https://stackoverflow.com/questions/79262779/why-are-two-different-logistic-regression-models-producing-vastly-different-resu</link>
      <description><![CDATA[我正在处理 NFL 数据，预测比赛结果。我的源数据（用于以下两个过程）包含约 3,800 行，每行 20 个单独的特征（每支球队 10 个，外加日期、比赛 ID、主队和客队以及主场胜利/客场胜利列）。
模型“A”循环通过特定日期范围进行训练/测试分割。也就是说，我使用硬截止值，因此每组在日期方面都是连续的。我使用的日期反映了 50%、55% 等最高 85% 的分割。使用此模型，8 个准确度结果中的每一个都是 ~ 61%，这对于 NFL 预测来说是典型的（可能有点偏高）。
模型“B”使用 sklearn 中的 train_test_split 模块以与上述相同的百分比分割数据。我意识到这种方法会打乱数据，所以我知道每个分割都使用与模型 A 不同的数据。但使用这种方法，每个周期的百分比准确率约为 82%，远高于模型 A，而且似乎不合理。因此，我想了解我是否正确地实现了其中一种（或两种）方法。
（如果有人可以向我展示如何附加我的源 csv，我很乐意这样做）
模型 A 的相关部分
train_test_date_range = [&#39;2017-11-02&#39;, &#39;2018-10-04&#39;, &#39;2019-09-05&#39;, &#39;2019-12-01&#39;, &#39;2020-11-05&#39;, &#39;2021-10-03&#39;, &#39;2022-01-02&#39;, &#39;2022-11-24&#39;]

features = [
&#39;team_1_average_ptsScored_last_five&#39;,
&#39;team_1_average_feature_1_last_five&#39;,

为清晰起见，功能列表缩写

&#39;team_2_average_feature_9_allowed_last_three_away&#39;,
&#39;team_2_average_feature_10_allowed_last_three_away&#39;
]

for tt_line_date in train_test_date_range :

train_data = match_stats[match_stats[&#39;date&#39;] &lt; tt_line_date]
test_data = match_stats[match_stats[&#39;date&#39;] &gt;= tt_line_date]

X_train = train_data[features]
X_test = test_data[features]
Y_train = train_data[&#39;team_1_result&#39;]
Y_test = test_data[&#39;team_1_result&#39;]

names = [&quot;Log Reg&quot;]

classifiers = [
LogisticRegression()
]

for name, clf in zip(names, classifiers):
# 将分类器拟合到训练数据上并进行预测
clf.fit(X_train, Y_train)
test_data[name + &#39;_team_1_result&#39;] = clf.predict(X_test)
accuracy = clf.score(X_test, Y_test)

print(name + &#39; T/T_Line 处的准确率 &#39; + tt_line_date + &#39; =&#39;, format(accuracy, &quot;.2%&quot;))

模型“A”的输出
T/T_Line 2017-11-02 的对数回归准确度 = 62.11%
T/T_Line 2018-10-04 的对数回归准确度 = 61.83%
等等。

模型“B”的相关部分
train_test_split_range = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85]

for tt_line in train_test_split_range: 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tt_line, random_state=42)

model = LogisticRegression()

scaler = StandardScaler()

scaled_train = scaler.fit_transform(X_train)
scaled_test = scaler.fit_transform(X_test)

model.fit(X_train, y_train)
y_pred_proba = model.predict_proba(X_train)[:, 1]

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

# 计算伪 r 平方
# 修改自：https://github.com/scikit-learn/scikit-learn/issues/25982
ll_null = log_loss(y_train, [y_train.mean()] * len(y_train))
ll_model = log_loss(y_train, y_pred_proba)
pseudo_r2 = 1 - ll_model / ll_null

# 打印指标
print(f&quot;\033[1m\033[4mResults at &quot; + str(tt_line) + &quot;训练/测试分割\033[0m&quot;)
print(f&quot;准确率：{accuracy:.3f}&quot;)
print(f&quot;精确率：{precision:.3f}&quot;)
print(f&quot;召回率：{recall:.3f}&quot;)
print(f&quot;F1 得分：{f1:.3f}&quot;)
print(f&quot;ROC AUC：{roc_auc:.3f}&quot;)
print(f&quot;伪 R 平方：{pseudo_r2:.3f}&quot;)
print()

模型“B”的输出（8 个中的 1 个，但很典型）
0.5 训练/测试分割的结果
准确率：0.820
精确率：0.828
召回率：0.854
F1 分数：0.841
ROC AUC：0.896
伪 R 平方：0.490
]]></description>
      <guid>https://stackoverflow.com/questions/79262779/why-are-two-different-logistic-regression-models-producing-vastly-different-resu</guid>
      <pubDate>Sun, 08 Dec 2024 15:41:51 GMT</pubDate>
    </item>
    <item>
      <title>机器学习如何计算微小动物的数量？[关闭]</title>
      <link>https://stackoverflow.com/questions/79261944/machine-learning-to-count-microscopic-animals</link>
      <description><![CDATA[我正在做一个项目，我们想使用机器学习在显微镜下计数幼年淡水贻贝（150 微米大）。我应该如何处理照明？我想尝试使用单色光来查看贻贝是否在不同波长下发出荧光，以便算法可以将其与垃圾区分开来。是否有一种便宜的方法可以获得单色照明来测试这一点？我在显微镜上使用奥林巴斯 tg-4 相机。
我尝试了普通光，但很难将贻贝与幼年蜗牛或其他垃圾区分开来。]]></description>
      <guid>https://stackoverflow.com/questions/79261944/machine-learning-to-count-microscopic-animals</guid>
      <pubDate>Sun, 08 Dec 2024 06:12:51 GMT</pubDate>
    </item>
    <item>
      <title>如何对齐特征提取和神经网络以进行图像中的隐写术检测？[关闭]</title>
      <link>https://stackoverflow.com/questions/79261860/how-to-align-feature-extraction-and-neural-network-for-steganography-detection-i</link>
      <description><![CDATA[我正在开展一个项目，开发一个用于检测隐写术 [隐藏在图像像素值中的消息] 的神经网络。我使用的数据集可从此处获取，其中包含 70,000 条从图像直方图中提取的特征记录，例如：
峰度、偏度、标准差、范围、中位数、几何平均值、Hjorth 移动性、Hjorth 复杂度，带有标签 (0/1)
我在这个数据集上训练了我的网络，并在测试集上实现了 93% 的准确率。现在，我想创建一个应用程序，将图像作为输入，计算其直方图，提取特征，然后使用训练后的模型进行分类。
问题：
我从新图像的直方图中生成的特征与训练数据集中的特征值有很大不同。因此，神经网络无法正确分类新图像。我不确定问题出在特征提取过程、预处理还是神经网络架构本身。
有人能建议如何正确实施特征提取过程或调整模型以改进新图像的分类吗？
-架构是 4x Dense 256、128、64 和 32 个神经元，输入=8 层和 S 形输出神经元。我尝试了几个层和神经元。这里没有成功
-尝试添加一些 dropout 层，但没有任何变化
-我尝试根据数据集作者提供的信息（我不知道“斯坦福基因组计划”，没有找到任何相关的数据集）找到一些源封面图像，以查看特征不同的原因并发现提取中的任何错误。
-尝试在实际图像上训练卷积网络时也发生了类似的事情
-已经做了一些实验和混淆矩阵、测量值和其他东西，但现在看来毫无意义，因为它除了训练数据集之外没有实际用途。
就上下文而言，目标是制作自然网络并将其与现有的论文分析工具进行比较。我将不胜感激任何建议，不仅关于如何解决眼前的问题，而且关于可以丰富我工作的额外研究、测试或方法。我正在使用 Python 和 Keras。]]></description>
      <guid>https://stackoverflow.com/questions/79261860/how-to-align-feature-extraction-and-neural-network-for-steganography-detection-i</guid>
      <pubDate>Sun, 08 Dec 2024 04:43:44 GMT</pubDate>
    </item>
    <item>
      <title>未来训练模型的文本标记方法</title>
      <link>https://stackoverflow.com/questions/79261744/text-labeling-approach-to-train-a-model-in-the-future</link>
      <description><![CDATA[我正在学习一些机器学习概念，最近这真的很有趣。我正在开发一个文档处理器，它将接收大量 PDF 并标记其中的数据。我想知道我是否有正确的方法来为稍后要创建的模型生成训练数据。
PDF 文件的路径被输入到一个使用 DocTR 提取文本的函数中。数据格式如下：
 data_list.append({
&#39;id&#39;:coord_key,
&#39;text&#39;:block_list[coord_key],
&quot;x1&quot;:float(block[&quot;geometry&quot;][0][0]),
&quot;y1&quot;:float(block[&quot;geometry&quot;][1][0]),
&quot;x2&quot;:float(block[&quot;geometry&quot;][0][1]),
&quot;y2&quot;:float(block[&quot;geometry&quot;][1][1])
});

Coord_Key 用作文本块的 ID。x1、x2、y1、y2 是文档上边界框的坐标，因此位置是已知的。文本当然就是所讨论的文本。
我的想法是获取这些数据并将其传递到 Label Studio，然后我将使用 Open AI 来帮助我应用我想要使用的特定标签。例如：address、phoneNumber、remarks 等。我当然会查看它得出的结果，并在筛选信息时进行更正。这样，我要训练的模型就有了最好的学习信息。
此时的目标是生成训练数据，以便我可以使用 Tensorflow 编写一些内容，学习如何自行进行标记。
这种方法正确吗？在我开始大量从我必须测试的文档中创建更多此类数据之前，我是否应该考虑为我的模型创建训练数据的其他事项？]]></description>
      <guid>https://stackoverflow.com/questions/79261744/text-labeling-approach-to-train-a-model-in-the-future</guid>
      <pubDate>Sun, 08 Dec 2024 02:23:07 GMT</pubDate>
    </item>
    <item>
      <title>如何正确训练 GAN 模型？[已迁移]</title>
      <link>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</link>
      <description><![CDATA[我尝试在我的数据库中训练 ViT-GAN 模型（来自此 repo），其中我有图像作为输入和输出。输入图像是路径规划问题的 PNG 地图。红色通道是障碍物地图，绿色和蓝色是带有起点/目标的一个像素。输出图像将是红色通道上的规划路径，这就是我试图教给模型的内容。来自 repo 的模型在所包含的示例上运行良好，这是一个比我的问题复杂得多的问题，所以我自然而然地认为这个网络可以解决我的问题而无需进一步调整。
我的问题是，即使不接触网络，只需将训练数据替换为挖掘数据也会完全搞乱输出。我尝试以多种方式调整网络，但结果始终保持不变 -&gt;要么是任何给定输入的相同结构化垃圾输出（黑色上的白色补丁），要么是黑色背景上的一些嘈杂的 rgb 补丁。我就是无法让它学习预期的输出。

左边是给定的输入，右边是预期的输出。中间是网络的输出。我读过关于训练 GAN 的文章，我发现它们很不稳定，很难找到正确的结构/参数集，但我就是不知道下一步该怎么做。我已经尝试过的方法：

通过删除一些 transformer/卷积层来缩小网络
减少过滤器的数量，这样我的问题就没那么复杂了
调整生成器/鉴别器的学习率 -&gt; 一起和单独调整
按照另一篇类似文章的建议，将鉴别器输出层的激活函数更改为 sigmoid
删除所有 ReLU 激活函数，并用 LeakyReLU 替换它们，以解决可能的梯度消失问题
将 Wasserstein 损失添加到鉴别器损失函数，以避免模型崩溃
将生成器损失函数中像素的平均差异更改为总和差异（以更多地惩罚全黑输出）
使用偏差来避免梯度消失
将过滤器的随机初始化器从 (0.0, 0.02) 更改为 -&gt; (0.0, 1.0) 使初始过滤器更加多样化
更改 lambda、批量大小、ff_dim、头数、补丁大小、嵌入暗淡、投影暗淡参数
训练 200 个时期和 20 个时期。全部针对相同的输出结构。
在数据集的较小部分（400 张图像）和较大部分（1000 张和 3000 张图像）上进行训练

我知道这个问题也可以用其他（可能更简单）网络来解决，但我想让这个 GAN 工作，并了解为什么它一开始就不起作用。我将我当前的更改状态添加到此repo。我不知道下一步该去哪里。如果有人有建议，请随时分享。如果您建议调整某些参数，请写一个具体的值。如果我忘记了描述中的任何内容，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</guid>
      <pubDate>Sat, 07 Dec 2024 20:09:37 GMT</pubDate>
    </item>
    <item>
      <title>在新的医学图像上训练深度学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79261142/training-deep-learning-model-on-new-medical-images</link>
      <description><![CDATA[我有一个基于深度学习的医学图像配准模型，我已经在数据集（大脑的 MRI 图像）上对其进行了训练，该数据集在模型的 GitHub 存储库中提供。现在我想在自己的数据集（FBCT 和 CBCT 图像）上对其进行训练 - 有人可以告诉我如何执行此操作的步骤吗？
我需要更改数据集的大小以匹配其他数据集吗？图片的模态在训练等方面是否起作用？]]></description>
      <guid>https://stackoverflow.com/questions/79261142/training-deep-learning-model-on-new-medical-images</guid>
      <pubDate>Sat, 07 Dec 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>每次使用 Keras 加载模型后，是否都需要调用 model.fit() 函数 [closed]</title>
      <link>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</link>
      <description><![CDATA[我正在使用 Keras 创建模型：
def my_model():
model = Sequential()
model.add(Conv2D(60, (5,5), input_shape=(32,32,1),activation=&#39;relu&#39;))
model.add(Conv2D(60, (5,5),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))
#model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(500,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(num_classes,activation=&#39;softmax&#39;))
## 编译模型
model.compile(Adam(learning_rate = 0.001),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
return model

现在保存模型并进行训练：
model = my_model()
print(model.summary())
model.save(&#39;my_model.h5&#39;)
history = model.fit(X_train, y_train,epochs = 10,validation_data=(X_val, y_val),batch_size = 400,verbose = 1,shuffle = 1)

#### 现在预测一些图像
pred = model.predict(img)
prediction = np.argmax(pred,axis=1)
print(&quot;predicted sign: &quot;+ str(prediction))

&#39;model.fit()&#39; 函数是最耗 CPU 的工作。在功能较弱的平台上，训练需要大量时间。是否可以保存训练好的模型并将其用于预测？
我知道有办法加载保存的模型：
from tensorflow.keras.models import load_model 
model = load_model(&#39;my_model.h5&#39;) 

但是我仍然必须运行耗费 CPU 的 &#39;model.fit()&#39;。有没有办法避免使用 model.fit() 并加载模型并用于预测？]]></description>
      <guid>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</guid>
      <pubDate>Sat, 07 Dec 2024 17:33:44 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测项目：在 Localhost 上的 Flask 应用程序中提交预测表单后出现“ValueError”</title>
      <link>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</guid>
      <pubDate>Fri, 06 Dec 2024 12:21:15 GMT</pubDate>
    </item>
    <item>
      <title>NLTK 和 Spacy 中的标记化列表</title>
      <link>https://stackoverflow.com/questions/79250131/list-of-tokenization-in-nltk-and-spacy</link>
      <description><![CDATA[我开始研究 NLP 基础是标记化，我看到了两样东西，即 NLTK 和 spcy
你能说出像 sent、word 等标记化吗？像这样总体来说，NLTK 和 Spacy 中有多少个我需要完整的标记化东西。]]></description>
      <guid>https://stackoverflow.com/questions/79250131/list-of-tokenization-in-nltk-and-spacy</guid>
      <pubDate>Wed, 04 Dec 2024 06:54:22 GMT</pubDate>
    </item>
    <item>
      <title>如何从非结构化文本中提取特定实体</title>
      <link>https://stackoverflow.com/questions/79227390/how-to-extract-specific-entities-from-unstructured-text</link>
      <description><![CDATA[给定一个通用文本句子（在特定上下文中），如何使用 python 和任何 NLP 库提取属于特定“类别”的感兴趣的单词/实体？
例如，给定烹饪食谱的步骤 将洋葱添加到一碗胡萝卜 作为输入文本，我想检索 洋葱 和 胡萝卜，而给定 撒上辣椒粉。 应该返回 辣椒粉。
但这也适用于不包含任何食物实体的句子，如 搅拌均匀，再煮一分钟。。
到目前为止，我能够实现的是使用 spacy 库来训练 NER 模块来解析句子。 NER 管道的问题在于它是一种基于规则的解析，它经过训练，提供了一组句子和实体/匹配/标签供学习，它在与训练期间使用的句子类似的句子上工作正常，但在新的和不同的句子上表现不佳：
nlp = spacy.load(&#39;trained_model&#39;)

document = nlp(&#39;Add powder, mustard, and salt&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;添加面粉&#39;, &#39;食物&#39;), (&#39;芥末&#39;, &#39;食物&#39;), (&#39;盐&#39;, &#39;食物&#39;)]
# (相当) 正确的输出

document = nlp(&#39;周末我拍了一座建筑、一辆车和一只松鼠&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;建筑&#39;, &#39;食物&#39;), (&#39;汽车&#39;, &#39;食物&#39;), (&#39;松鼠&#39;, &#39;食物&#39;)]
# 错误输出

document = nlp(&#39;搅拌均匀，再煮一分钟。&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;stir well&#39;, &#39;FOOD&#39;), (&#39;cook&#39;, &#39;FOOD&#39;), (&#39;additionalminute.&#39;, &#39;FOOD&#39;)]
# 错误输出

我知道有几个类似的问题和帖子，但我发现只有适用于“半结构化”文本的解决方案，即成分列表为 1 茶匙糖、1 杯牛奶、...，可以使用以前的基于规则的方法轻松解决。此外，nltk 和词性 (POS) 也是一种选择，但我更喜欢替代解决方案，而不是将每个名词与详尽的食物列表进行比较。
我正在寻找的是一种提取特定实体的方法，或者至少使用基本解析之外的其他类别对通用文本中的单词进行分类。
我应该使用/查看哪些方法来实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79227390/how-to-extract-specific-entities-from-unstructured-text</guid>
      <pubDate>Tue, 26 Nov 2024 15:46:48 GMT</pubDate>
    </item>
    <item>
      <title>如何为项目确定正确的 NLP 方法</title>
      <link>https://stackoverflow.com/questions/77410116/how-to-decide-correct-nlp-approach-for-a-project</link>
      <description><![CDATA[我正在从事一个 NLP 项目。我的任务是从对话本身确定土耳其呼叫中心对话的类别和情感分数。我们使用 Python 作为编程语言。但是，我在项目的初始阶段陷入了数百种备选解决方案的困境。我有 300,000 行客户代表和客户文本数据，我已经通过预处理阶段很好地清理了它们。所有数据目前都以句子标记形式呈现，并已通过其他标准预处理阶段。客户代表和客户对话已准备好放在 ~600 MB csv 的不同列中。在决定建模算法之前，我的经理希望我准备好训练数据集。数据集应该包含以下信息，我们稍后会决定哪些是必要的，哪些是不必要的，然后我们才会最终确定数据集：

应该提取词向量
应该提取句子向量
应该提取对话的摘要，并提取这些摘要的句子和词向量
应该提取NER计数
应该提取POS计数
应该提取形态分析，并将词缀、连词等以数字形式显示
应该从主观性和极性两个方面提取情感得分
应该提取关键词及其向量
应该进行主题建模，并将每个对话最接近的主题添加到数据集中
应该对摘要和主要对话之间的相似度得分进行提取
应提取对话中提到的单词的稀有度比率，并逐句取平均值（我们认为这将给出句子含义丰富的程度）

我面临的问题如下：

土耳其语最好的词向量提取库是什么？有 Word2vec、NGram、GloVe 等技术。还有相对较新的技术，如 Fasttext。BERT 也是一个选择。我应该选择哪一个？我将如何比较它们的表现？我应该训练自己的模型，还是应该选择预先训练的模型？（例如，Fasttext 有针对土耳其语的训练模型）哪一个比哪一个更优秀或更新颖？如果它们都使用了不同的技术，我应该以哪篇文章或研究为依据？

Gensim 似乎是一个带有工具的库，可以为许多 NLP 问题提供解决方案。这个库太大了，我甚至无法掌握它的全部功能。我应该继续使用 Gensim，还是应该一起使用不同的工具？它会满足我的所有需求吗？我怎么知道？

有很多工具可以进行词形还原，这些工具也是矢量化的，因为我也会进行词形还原，我应该使用它们的矢量化功能，还是应该从上面提到的最多的矢量化工具中受益？哪一个能给出最好的结果？我读过很多比较文章，它们都谈到了不同的结果。

有针对土耳其语训练的 SBERT 模型用于提取句子向量。当我使用其他工具提取词向量时，使用 SBERT 获得的向量会变得毫无意义吗？毕竟，我会用不同的方法提取这些，而且它们会在同一个数据集中。


由于这些替代解决方案彼此之间的优越性存在模糊性，我感到很困惑。
NLP 项目应该如何正确开展？]]></description>
      <guid>https://stackoverflow.com/questions/77410116/how-to-decide-correct-nlp-approach-for-a-project</guid>
      <pubDate>Thu, 02 Nov 2023 13:45:26 GMT</pubDate>
    </item>
    <item>
      <title>关系提取中表面形式是什么意思？</title>
      <link>https://stackoverflow.com/questions/76495143/what-does-surface-form-mean-in-relation-extraction</link>
      <description><![CDATA[大多数关系抽取论文中都会反复提到“实体表面形式”这个术语。它是什么意思呢？
例如，在REBEL论文中，作者提到“只有正确提取出头和尾实体表面形式，关系才被认为是正确的。”
“头”和“尾”是什么意思？]]></description>
      <guid>https://stackoverflow.com/questions/76495143/what-does-surface-form-mean-in-relation-extraction</guid>
      <pubDate>Sat, 17 Jun 2023 07:15:12 GMT</pubDate>
    </item>
    <item>
      <title>Gan 制作出不饱和、褪色的图像</title>
      <link>https://stackoverflow.com/questions/66292084/gan-produces-desaturated-faded-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/66292084/gan-produces-desaturated-faded-images</guid>
      <pubDate>Sat, 20 Feb 2021 13:26:47 GMT</pubDate>
    </item>
    <item>
      <title>用于大数据集分类的 NLP 软件</title>
      <link>https://stackoverflow.com/questions/7248372/nlp-software-for-classification-of-large-datasets</link>
      <description><![CDATA[多年来，我一直在使用自己的贝叶斯类方法，基于大量且不断更新的训练数据集对来自外部来源的新项目进行分类。
每个项目有三种类型的分类：

30 个类别，其中每个项目必须属于一个类别，最多两个类别。
10 个其他类别，其中每个项目只有在强匹配的情况下才与一个类别相关联，并且每个项目可以属于尽可能多的匹配类别。
4 个其他类别，其中每个项目必须只属于一个类别，如果没有强匹配，则该项目被分配到默认类别。

每个项目由大约 2,000 个字符的英文文本组成。我的训练数据集中有大约 265,000 个项目，其中包含粗略估计的 10,000,000 个特征（独特的三个词短语）。
我的自制方法相当成功，但肯定还有改进的空间。我读过 NLTK 书中的“学习对文本进行分类”一章，这本书很棒，让我对 NLP 分类技术有了很好的概述。我希望能够尝试不同的方法和参数，直到获得对我的数据来说最好的分类结果。
问题
有哪些现成的 NLP 工具可以有效地对如此大的数据集进行分类？
到目前为止，我尝试过的工具有：

NLTK
TIMBL

我尝试使用一个数据集来训练它们，该数据集包含不到 1% 的可用训练数据：1,700 个项目，375,000 个特征。对于 NLTK，我使用了稀疏二进制格式，对于 TIMBL，我使用了类似的紧凑格式。
两者似乎都依赖于在内存中执行所有操作，并且很快消耗了所有系统内存。我可以让它们处理微小的数据集，但不能处理大数据集。我怀疑如果我尝试逐步添加训练数据，那么同样的问题会在当时或进行实际分类时发生。
我查看了 Google 的预测 API，它似乎可以完成我的大部分工作，但不是全部。如果可能的话，我也想避免依赖外部服务。
关于特征的选择：多年来，在我使用自制方法进行测试时，三个词组产生了迄今为止最好的结果。虽然我可以通过添加单词或两个词组来减少特征的数量，但这很可能会产生较差的结果，并且仍然会是大量的特征。]]></description>
      <guid>https://stackoverflow.com/questions/7248372/nlp-software-for-classification-of-large-datasets</guid>
      <pubDate>Tue, 30 Aug 2011 19:00:10 GMT</pubDate>
    </item>
    </channel>
</rss>