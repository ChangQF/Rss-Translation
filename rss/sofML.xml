<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 12 Jun 2024 06:20:53 GMT</lastBuildDate>
    <item>
      <title>无法为强化学习问题分配内存</title>
      <link>https://stackoverflow.com/questions/78610849/unable-to-allocate-memory-for-a-reinforcement-learning-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78610849/unable-to-allocate-memory-for-a-reinforcement-learning-problem</guid>
      <pubDate>Wed, 12 Jun 2024 05:59:34 GMT</pubDate>
    </item>
    <item>
      <title>我正在寻找方法或脚本来微调 DreamFusion 模型，以实现文本到 3D 和图像到 3D 模型的生成 [关闭]</title>
      <link>https://stackoverflow.com/questions/78610807/im-finding-method-or-script-to-fine-tune-dreamfusion-model-for-text-to-3d-and-i</link>
      <description><![CDATA[我想针对特定领域（珠宝设计）微调 DreamFusion 模型，但所有开源模型的精度都不足以生成我想要的 3D 模型。
我找到了一种用于形状模型的方法，但它的效果和预期的一样好。]]></description>
      <guid>https://stackoverflow.com/questions/78610807/im-finding-method-or-script-to-fine-tune-dreamfusion-model-for-text-to-3d-and-i</guid>
      <pubDate>Wed, 12 Jun 2024 05:41:33 GMT</pubDate>
    </item>
    <item>
      <title>对于复杂且较长的神经网络，反向传播中的梯度是如何计算的？</title>
      <link>https://stackoverflow.com/questions/78610739/how-are-gradients-calculated-in-backpropagation-for-complex-long-neural-network</link>
      <description><![CDATA[我理解如何找到反向传播算法的偏导数，但一旦有多个隐藏层，就无法手动跟踪偏导数，我不知道如何将其实现到代码中。我对如何逐层向下跟踪值感到非常困惑，而且我还没有看到解释反向传播算法的资源。有人可以解释一下或指出合适的资源吗？我是一名初学者，我正在尝试从头开始构建一个神经网络，但结果很难理解，因为它比我能跟踪的要长，而且我不知道如何在代码中实现。
对于具有多层和数千个连接的神经网络，这是否超出了初学者的范围。我见过一个 chatgpt 解释，其中用操作序列构建了一个图，并以相反的顺序遍历它以了解偏导数阶。]]></description>
      <guid>https://stackoverflow.com/questions/78610739/how-are-gradients-calculated-in-backpropagation-for-complex-long-neural-network</guid>
      <pubDate>Wed, 12 Jun 2024 05:19:50 GMT</pubDate>
    </item>
    <item>
      <title>我应该调整哪些超参数来提高准确性？</title>
      <link>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</link>
      <description><![CDATA[我想知道如何在多标签分类问题中提高准确率并降低损失。
如果你查看 sklearn 参考，你会发现在多类和多输出算法中提到了多标签，我现在正在测试它。
（https://scikit-learn.org/stable/modules/multiclass.html）
样本数据使用sklearn.datasets中的make_multilabel_classification有10个特征，通过修改n_classes创建一个数据集。
当multilabel有两个类时，似乎准确率和损失都比较令人满意。
from numpy import mean
from numpy import std
from sklearn.datasets import make_multilabel_classification
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, hamming_loss

# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=2, random_state=1)

# 总结数据集形状
print(X.shape, y.shape)
# 总结前几个示例
for i in range(10):
print(X[i], y[i])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
print(scaler.mean_)
print(scaler.var_)

x_train_std = scaler.transform(X_train)
x_test_std = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std, y_train)

pred = knn.predict(x_test_std)

print(accuracy_score(y_test, pred))
print(hamming_loss(y_test, pred))

accuracy_score: 0.8345, hamming_loss: 0.08875
但是，随着类别数超过3，准确率得分逐渐下降，损失增加。
# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=3, random_state=1)

n_classes= 3 --&gt; accuracy_score: 0.772, hamming_loss: 0.116
n_classes= 4 --&gt; accuracy_score: 0.4875, hamming_loss: 0.194125
使用 RandomForestClassifier 算法和 MLPClassifier 算法时（如参考中所示）或使用 ClassifierChain(estimator=SVC) 使用不支持多标签分类的算法时，情况也类似。
我应该调整哪些超参数来提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</guid>
      <pubDate>Wed, 12 Jun 2024 03:24:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>更改 Keras/Tensorflow“可见设备”会降低模型的准确性</title>
      <link>https://stackoverflow.com/questions/78610219/changing-keras-tensorflow-visible-devices-makes-the-model-less-accurate</link>
      <description><![CDATA[我正在运行一个用 Keras/Tensorflow 构建的 CNN。因为我在学校的远程服务器上运行，其他人也使用该服务器，所以我通常会设置“可见设备”来限制使用的 GPU 数量，而不是使用所有 GPU。以下是我用来执行此操作的代码：
def set_gpus(num_gpus=4):
print(&quot;Num GPUs Available: &quot;, len(tf.config.list_physical_devices(&#39;GPU&#39;)))
gpus = tf.config.list_physical_devices(&#39;GPU&#39;)
if gpus:
# 限制 TensorFlow 仅使用第一个 GPU
try:
tf.config.set_visible_devices(gpus[:num_gpus], &#39;GPU&#39;)
logical_gpus = tf.config.list_logical_devices(&#39;GPU&#39;)
print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPU&quot;)
except RuntimeError as e:
# 必须在初始化 GPU 之前设置可见设备
print(e)

我发现，当我没有运行 set_gpus 时，我的模型表现比运行 set_gpus 时好得多。当仅使用 4 个 GPU 时，训练准确率在 100 个 epoch 后达到 88%，但验证和测试准确率都保持在 50% 左右，两个类别的 F1 分数显示一个类别的准确率约为 66%，另一个类别的准确率约为 3%。相比之下，当我在未先运行 set_gpus() 的情况下运行代码时，验证和测试准确率约为 70%，类别特定分数分别为 56% 和 77%。
我尝试在运行 set_gpus 时更改 num_gpus，但即便如此，我得到的结果与仅使用 4 个 GPU 时的结果相同。即使我将 GPU 数量设置为 8（即服务器上的总数），结果也不会改变。有人知道是什么原因导致了这个问题吗？
我的代码在 Jupyter Notebook 上运行，其中安装了 Python 3.12.2、Keras 版本 3.0.5 和 Tensorflow 版本 2.16.1。以下是与我的 CNN 模型相关的附加代码，希望对您有所帮助：
def convolution_encoder(inputs, kernels, kernel_size=[3,3], stride=[1,1], data_format=&#39;channels_last&#39;):
# 规范化和注意力
x = layer.BatchNormalization(axis=-1)(inputs)
x = layer.Conv2D(filters=kernels,activation=&#39;relu&#39;, kernel_size=kernel_size, strides=stride, data_format=data_format)(x)
x = layer.MaxPooling2D(data_format=data_format)(x)
return x

def build_model_predict(
input_shape,
output_dim=2,
mode=&#39;single-channel&#39;,
data_format=&#39;channels_last&#39;
):
input = keras.Input(shape=input_shape)
x = 输入
x= convolution_encoder(x, 16，[5,5]，[2,2]，data_format=data_format) 
x= convolution_encoder(x，32，[3,3]，[1,1]，data_format=data_format)
x= convolution_encoder(x，64，[3,3]，[1,1]，data_format=data_format)
x= layer.Flatten()(x)
x= layer.Dropout(0.5)(x)
x= layer.Dense(units=256，activation=&#39;sigmoid&#39;)(x)
x= layer.Dropout(0.5)(x)
output = layer.Dense(output_dim，activation=&quot;softmax&quot;)(x)
return keras.Model(inputs, output)

def run_CNN(x, y, seeded=False, seed=42, epoch_num=50):
if seeded:
x_train, x_test, y_train，y_test= train_test_split(x，y，test_size= 0.20，random_state=seed，stratify=y)
a，x_val，b，y_val= train_test_split(x_train，y_train，test_size= 0.10，random_state=seed，stratify=y_train)
else:
x_train，x_test，y_train，y_test= train_test_split(x，y，test_size= 0.20，stratify=y)
a，x_val，b，y_val= train_test_split(x_train，y_train，test_size= 0.10，stratify=y_train)
keras.backend.clear_session()
model = build_model_predict(
x_train.shape[1:],
output_dim=y.shape[-1])
model.compile(
optimizer=keras.optimizers.Adam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08), # 优化器
# 最小化损失函数
loss=keras.losses.CategoricalFocalCrossentropy(),
# 要监控的指标列表
metrics=[keras.metrics.CategoricalAccuracy()],
)
print(&quot;Fit model on training data&quot;)
history = model.fit(
x_train,
y_train,
batch_size=64,
epochs=epoch_num,
# 我们通过一些验证来
# 监控验证损失和指标
# 在每个 epoch 结束时
validation_data=(x_val, y_val),
initial_epoch=0
#callbacks=callbacks_list
)
results = model.evaluate(x_test, y_test, batch_size=128)
print(&#39;按类别划分的训练准确率：&#39;)
y_train_predict = model.predict(x_train)
score= f1_score(np.argmax(y_train, axis=1), np.argmax(y_train_predict, axis=1), average=None)
print(score)
print(&#39;按类别划分的测试准确率：&#39;)
y_predict = model.predict(x_test)
score= f1_score(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1), average=None)
print(score)
return model

下面是运行模型的实际代码块：
set_gpus()
model=run_CNN(x_stft, y_short, seeded=True, epoch_num=100)
]]></description>
      <guid>https://stackoverflow.com/questions/78610219/changing-keras-tensorflow-visible-devices-makes-the-model-less-accurate</guid>
      <pubDate>Wed, 12 Jun 2024 00:45:35 GMT</pubDate>
    </item>
    <item>
      <title>CNN 网络中的卷积层可以在池化层或全连接层之后继续吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78610065/can-convolutional-layers-in-cnn-networks-proceed-after-pooling-or-fully-connecte</link>
      <description><![CDATA[CNN 网络中的卷积层可以在池化或全连接层之后继续吗？我知道具有这种架构的网络是不寻常的，但我很好奇，因为我正在尝试了解不同 CNN 设计中的可变性。
我预计答案是肯定的？]]></description>
      <guid>https://stackoverflow.com/questions/78610065/can-convolutional-layers-in-cnn-networks-proceed-after-pooling-or-fully-connecte</guid>
      <pubDate>Tue, 11 Jun 2024 23:16:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么 FastAPI 框架使用起来很昂贵？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78609577/why-fastapi-framework-is-expensive-to-use</link>
      <description><![CDATA[有一篇文章比较了基于 Python 的后端框架 FastAPI 和 Flask 在 ML 项目中的应用。文章中关于 FastAPI 的评论如下：

FastAPI 框架的主要缺点是价格昂贵。具体价格会根据您使用它的国家/地区以及您每月进行的 API 调用次数而有所不同。但总体而言，成本很高。

我不太明白的是，如果 FastAPI 擅长创建 API，但调用开发的 API 却很昂贵，那么为什么它在 ML 中仍然越来越受欢迎？我知道 FastAPI 中集成了一些付费库。在使用 FastAPI 开发的 ML 项目的 API 调用中，这些付费库一定会被调用吗？如何估算使用 FastAPI 开发的 ML 项目需要多少成本？]]></description>
      <guid>https://stackoverflow.com/questions/78609577/why-fastapi-framework-is-expensive-to-use</guid>
      <pubDate>Tue, 11 Jun 2024 20:12:00 GMT</pubDate>
    </item>
    <item>
      <title>我创建的模型损失很大</title>
      <link>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</link>
      <description><![CDATA[我无法分享堆栈上的完整数据，因此我只会分享下面的代码。
过去几天我一直在尝试这些数据，但似乎损失在 18000 或 15000 左右，这太高了。我的同事告诉我要使用神经网络来处理这些数据。
model = Sequential([
layer.Dense(128,activation=&#39;relu&#39;),
layer.Dense(64,activation=&#39;relu&#39;),
layer.Dense(32,activation=&#39;relu&#39;),
layer.Dense(1,activation=&#39;relu&#39;)
])

model.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;mae&#39;])

model.fit(x_train, y_train,epochs=100,batch_size=80)

Epoch 1/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step -损失：242343.5312 - mae：465.7755
纪元 2/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：243483.7969 - mae：468.0649
纪元 3/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━━ 0s 29ms/步 - 损失：246071.8281 - mae： 468.4903
纪元 4/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：250888.7188 - mae：476.0695
纪元 5/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 0s 15ms/步 - 损失：242264.2188 - mae：465.4283
纪元 6/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 242441.9062 - mae: 464.7845

如何让这个loss尽可能低？
这里是我的数据集的一个示例。]]></description>
      <guid>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</guid>
      <pubDate>Mon, 10 Jun 2024 19:23:13 GMT</pubDate>
    </item>
    <item>
      <title>还有其他方法可以用来更好地识别图像中的所有细胞吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</link>
      <description><![CDATA[我有兴趣找到每个细胞的 COM 和速度。为此，我需要我的程序能够识别至少 90% 的细胞。我尝试使用 opencv 的查找轮廓函数，但结果并不令人满意。我附上了得到的结果和真实图像。你们对我可以尝试什么还有其他建议吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</guid>
      <pubDate>Mon, 10 Jun 2024 18:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Yolov4如何在指定置信度下获取map@0.50</title>
      <link>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</link>
      <description><![CDATA[我尝试在指定的conf_thresh下获取map@0.50。
我设置了不同的conf_thresh，但我得到了平均IoU和相同的map@0.50
我尝试了此代码来获取map@0.50
darknet detector map .data .cfg .weights -thresh 0.2

我得到了对于conf_thresh = 0.2 TP FP FN，平均IoU = 73.15％和map@0.50 = 96.30％。
我尝试了其他conf_thresh从0.1到0.9，我得到了相同的map@0.50 = 96.30％。
我不知道这个map@0.50 = 96.30％在以下情况下的意思conf_thresh0.1到0.9都是96.30或者使用了Yolov4默认的conf_thresh。]]></description>
      <guid>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</guid>
      <pubDate>Mon, 10 Jun 2024 15:38:42 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种 ML 模型进行销售预测？</title>
      <link>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</link>
      <description><![CDATA[给定披萨店的训练数据，其中包含历史销售数据在此处输入图片描述。
| X 形状 | Y 形状 |
| -------- | -------- |
|(1582, 13)| (1582,) |

我正在使用顺序 NN，其中有 6 个密集层，带有“relu”和 1 个输出层，带有“线性”激活函数 (128,64,32,16,8,4,1)。
我尝试添加 l2 正则化，但仍然不起作用。
Epoch 999/1000
20/20 [================================] - 0s 6ms/step - loss: 40284.5391 - mae: 163.4370 - val_loss: 96699.7188 - val_mae: 260.4841
Epoch 1000/1000
20/20 [===============================] - 0s 19ms/步 - loss: 40112.4141 - mae: 163.2846 - val_loss: 98381.9688 - val_mae: 262.3724
]]></description>
      <guid>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</guid>
      <pubDate>Mon, 10 Jun 2024 14:05:42 GMT</pubDate>
    </item>
    <item>
      <title>时间序列中具有小数据集的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78599891/machine-learning-models-with-small-datasets-in-time-series</link>
      <description><![CDATA[我正在开展一个预测项目，其中我的数据集包含 24 个月的历史销售数据。我使用 20 个月进行训练和验证，其余 4 个月进行测试。对于验证，我使用滚动预测起源（类似于 TimeSeriesSplit）。
我尝试使用几种模型，包括 SVR、GBR、随机森林、Facebook Prophet、ARIMA 和线性回归。但是，我很难找到一个正常工作的模型。我所有的预测图看起来都不一致，并且与数据不太吻合。
您能否解释为什么机器学习模型可能无法在像我这样的小数据集上表现良好，并建议任何可能更适合小数据集的技术或模型？
我所有的预测结果看起来都像图像一样，有一条直线。
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78599891/machine-learning-models-with-small-datasets-in-time-series</guid>
      <pubDate>Sun, 09 Jun 2024 22:25:42 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林对心电图数据进行验证和测试的准确率较低</title>
      <link>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</guid>
      <pubDate>Sun, 09 Jun 2024 21:43:40 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
替代方法：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
Sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
Sum(i) = (1/Ny)*sum((y_i&#39;-y_i_target).^2);
end
[minval, minidx] = min(Sum);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    </channel>
</rss>