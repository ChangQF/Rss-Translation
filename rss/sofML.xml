<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 15:14:32 GMT</lastBuildDate>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 200k 条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 对大多数字符串数据集进行分类或聚类的解决方案</title>
      <link>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</link>
      <description><![CDATA[正如标题所说，我想使用机器学习对主要字符串数据集进行分类/聚类。我不确定实现此目的的最佳方法是什么。我想使用 Tensorflow 来完成这个任务。也许有人有开发这样的解决方案的经验并且可以推荐一些东西。
至于我的数据集主要包含字符串值的问题，据我所知，我可以使用 pandas 类别将此数据转换为算法可以理解的内容。如果有更好的解决方案请务必推荐。
我对这个主题非常陌生，因此任何帮助都会很棒。
到目前为止，我一直在学习 Tensorflow 并分析我拥有的数据集（清理数据、理解结构）。]]></description>
      <guid>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</guid>
      <pubDate>Tue, 06 Feb 2024 15:03:49 GMT</pubDate>
    </item>
    <item>
      <title>在嘈杂的视频流中识别对象的更好方法是什么？</title>
      <link>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</link>
      <description><![CDATA[据我所知，即使我们使用视频流，我们也需要将其分割成帧来运行模型来检测对象。我们正在处理的视频可能非常嘈杂，以至于当一个人看着屏幕时，只有存在一系列图像（小物体正在移动或物体是静态的，但每一帧的噪声都有些不同）。因此，我担心单帧可能不足以以足够的准确度对对象进行分类（我刚刚开始讨论这个主题，所以到目前为止还没有 PoC）
如何提高准确度（也许使用图像序列，但任何其他方式也可以接受）？]]></description>
      <guid>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</guid>
      <pubDate>Tue, 06 Feb 2024 14:27:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中实现大数据集的交叉验证而不将整个数据集加载到内存中？</title>
      <link>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</link>
      <description><![CDATA[我目前正在处理一个机器学习项目的大型数据集，并选择使用 TensorFlow 的 tf.data API 来高效管理数据加载和预处理，而无需将整个数据集加载到内存中。这种方法对于我的初始训练效果很好。
但是我很难实现交叉验证。据我了解，TensorFlow 本身并不支持直接通过 tf.data API 进行交叉验证，而与 Keras 集成进行交叉验证似乎需要先将数据加载到内存中。这对我的使用来说是有问题的，因为立即将整个数据集加载到内存中违背了使用 tf.data 的目的。
我正在寻找一种解决方法或方法来实现与 TensorFlow 的按需数据加载兼容的交叉验证。理想情况下，我希望保持 tf.data 的内存效率，同时对模型的评估进行交叉验证。
有没有办法使用 Keras 或任何其他库进行交叉验证，而不需要我将所有数据集加载到内存中？]]></description>
      <guid>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</guid>
      <pubDate>Tue, 06 Feb 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>模型的预测始终为 0</title>
      <link>https://stackoverflow.com/questions/77947679/models-predictions-always-0</link>
      <description><![CDATA[我有一个形状为 (1280, 100, 20, 4096) 的训练集，我将其提供给基于变压器的模型进行二元分类（标签为 0 或 1）。这导致我很难处理大量的特征（我尝试将其批量提供给模型，但我不确定最好的方法。现在我只是将其减少到（450， 100, 20, 4096），但任何建议都值得赞赏），但我目前的问题是，无论我训练模型多少个时期，准确率始终为 67.5%（即 0 的百分比） -测试集中的标记特征），测试集上的精度和召回率将始终为 0%。我尝试在将数据输入模型之前对其进行标准化：
 缩放器 = StandardScaler()
    train_data = scaler.fit_transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)
    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)

但这并没有带来任何改进。我使用的模型基于仅编码器变压器：
层（类型）输出形状参数#
=================================================== ===============
input_1 (输入层) [(无, 100, 20, 4096)] 0
_________________________________________________________________
框架位置嵌入（Po（无、100、20、4096）8192000
_________________________________________________________________
Transformer_layer（编码器）（无、100、20、4096）134299652
_________________________________________________________________
global_max_pooling（GlobalMa（无，4096）0
_________________________________________________________________
辍学（Dropout）（无，4096）0
_________________________________________________________________
输出（密集）（无，1）4097
=================================================== ===============
总参数：142,495,749
可训练参数：142,495,749
不可训练参数：0
_________________________________________________________________

在训练期间，我可以看到损失、准确度、精确度和召回率达到了不错的水平，但是当我在测试集上评估模型时，所有这些值都如我之前所述：
纪元 100/100
29/29 [================================] - 90s 3s/step - 损失：0.0839 - 准确度：0.9610 - 召回率：0.9316 - 精度：0.9589
2024-02-06 12:38:38.815759: W tensorflow/core/framework/cpu_allocator_impl.cc:80] 9175040000 的分配超过可用系统内存的 10%。
9/9 [================================] - 21s 2s/步 - 损失：9.4117 - 准确度：0.6750 - 召回率：0.0000e+00 - 精度：0.0000e+00
测试准确率：67.5%
测试召回率：0.0%
测试精度：0.0%

模型优化器是adam，损失是二元交叉熵。激活是S形的。
我正在努力寻找模型的适当调整，甚至理解其当前的行为。此外，我不清楚将批量数据集输入缩放器和拟合函数是否会改变模型的实际训练。]]></description>
      <guid>https://stackoverflow.com/questions/77947679/models-predictions-always-0</guid>
      <pubDate>Tue, 06 Feb 2024 12:26:30 GMT</pubDate>
    </item>
    <item>
      <title>攻击模式的 ML 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77947562/ml-model-for-attack-pattern</link>
      <description><![CDATA[当前情况：我有网络日志（FortiAnalyzer日志）数据和端点日志（SentinelOne）
fortianalyzer 的示例日志：
 &lt;189&gt;logver=700130566 时间戳=1705663428 devname=“F” devid=6 vd=根日期=2024-01-19 时间=11:23:48 eventtime=1705659828815774000 tz=+0100 logid=“0000000013”类型=流量子类型=转发级别=通知 srcip=40 srcport=42489 srcintf=Server-50 srcintfrole=lan dstip=********236 dstport=514 dstintf=wan2 dstintfrole=wan srcuuid=0e9b-51eb-cc6d -9f3b180f91fc dstuuid=8da************ea-d8f0-c1ac2598a319 srccountry=保留 dstcountry=荷兰 sessionid=568400008 proto=17 操作=拒绝策略id=61 策略类型=策略 poluuid=9ec6b-e64e-7eee61e47eab 策略名称=jkhdkfjhk-LAN-&gt;iNetCatchAll 服务=SYSLOG trandisp=noop 持续时间=0 sentbyte=0 rcvdbyte=0 sentpkt=0 rcvdpkt=0 vwlid=3 vwlquality=Seq_num(1 wan2)，活动，已选择 vwlname=LAN-Rule appcat=未扫描 crscore=30 craction=131072 crlevel=高 srchwvendor=HP devtype=网络 srcfamily=交换机 srchwversion=Aruba mastersrcmac=d0::cc srcmac=d0:cc srcserver=0 &lt;189&gt;logver=700130566 时间戳=1705663428 devname=” FGT0F” devid=FG13696 vd=根日期=2024-01-19 时间=11:23:48 eventtime=1705659828815830080 tz=+0100 logid=“0000000013”类型=流量子类型=转发级别=通知 srcip=1040 srcport=42489 srcintf=Server-50 srcintfrole=lan dstip=1******.236 dstport=514 dstintf=wan2 dstintfrole=wan srcuuid=0eb**** *****-cc6d-9f3b180f91fc dstuuid=81ea-d8f0-c1ac2598a319 srccountry=保留 dstcountry=中国 sessionid=568400009 proto=17 操作=拒绝策略id=61 策略类型=策略 poluuid=9eceb-e64e-7eee61e47eab 策略名称=-LAN-&gt; ;iNetCatchAll 服务=SYSLOG trandisp=noop 持续时间=0 sentbyte=0 rcvdbyte=0 sentpkt=0 rcvdpkt=0 vwlid=3 vwlquality=Seq_num(1 wan2)，活动，已选择 vwlname=LAN 规则 appcat=未扫描 crscore=30 craction= 131072 crlevel=高 srchwvendor=HP devtype=网络 srcfamily=交换机 srchwversion=Aruba mastersrcmac=d0:69:e8:cc srcmac=d0:6***:e8:cc srcserver=0

哨兵一的示例日志：
&lt;12&gt;2023-11-07 12:34:45,230 哨兵 - CEF:0|SentinelOne|Mgmt|Y#20|19|新的活跃威胁 - 计算机 W023|1|osName=Windows 10 Enterprise rt=2023-11-07 12:30:36.456951 fileHash=3395856c**642f14140 filePath=\Device\HarddiskVolume3\TEMP\virus.txt cat=SystemEvent 活动ID=1813041897368528281 活动类型=19 siteId=1080466588931492808站点名称=技术 . accountId=10815591 accountName= 技术 。 notificationScope=SITE &lt;14&gt;2023-11-07 12:34:45,231 哨兵 - CEF:0|SentinelOne|Mgmt|Y#20|2001|杀戮成功执行|1|fileHash=3395ee72602f798b642f14140 filePath=\Device\HarddiskVolume3\TEMP \virus.txt osName=Windows 10 Enterprise ip=2194 cat=SystemEvent suser=WP23 rt=#arcsightDate(星期二，2023 年 11 月 7 日，12:30:36 UTC) ActivityID=181305492937 ActivityType=2001 siteId=10804661492808 siteName= 技术 。 accountId=108046591 accountName= 技术 。 notificationScope=SITE &lt;14&gt;2023-11-07 12:34:45,231 哨兵 - CEF:0|SentinelOne|Mgmt|Y#20|2004|隔离已成功执行|1|fileHash=332b7382dee72602f798b642f14140 filePath=\Device\HarddiskVolume3\TEMP \virus.txt osName=Windows 10 Enterprise ip=2194 cat=SystemEvent suser=WP6023 rt=#arcsightDate(星期二，2023 年 11 月 7 日，12:30:36 UTC) ActivityID=1813045002 ActivityType=2004 siteId=10804665808 siteName= 技术 。 accountId=108015591 accountName= 技术 。 notificationScope=SITE &lt;12&gt;2023-11-07 12:34:45,415 哨兵 - CEF:0|SentinelOne|Mgmt|Y#20|19|新的主动威胁 - 机器 WP********3|1| osName=Windows 10 Enterprise rt=2023-11-07 12:30:36.456951 fileHash=3395602f798b642f14140 filePath=\Device\HarddiskVolume3\TEMP\virus.txt cat=SystemEvent activityID=18130428281 activityType=19 siteId=108092808 siteName= Technology . accountId=1080466588914715591 accountName= 技术 。通知范围=站点

问题陈述：我愿意

连接（缝合）两个日志
应用机器学习模型来检测这些日志中的异常情况（例如：DDOS、违规）

Huggingface.co 或其他来源上是否有任何模型可以用来检测日志中的异常情况？
此外，拼接这两个日志中的数据的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77947562/ml-model-for-attack-pattern</guid>
      <pubDate>Tue, 06 Feb 2024 12:06:10 GMT</pubDate>
    </item>
    <item>
      <title>将自定义损失函数移至 GPU</title>
      <link>https://stackoverflow.com/questions/77946691/moving-a-custom-loss-function-to-gpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77946691/moving-a-custom-loss-function-to-gpu</guid>
      <pubDate>Tue, 06 Feb 2024 09:45:05 GMT</pubDate>
    </item>
    <item>
      <title>关联矩阵热图中的问题[重复]</title>
      <link>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</link>
      <description><![CDATA[
我在我的热图中遇到了这个问题，它只显示热图第一行中的值..
我编写了以下代码：
&lt;前&gt;&lt;代码&gt;
    将seaborn导入为sns
    
    将 matplotlib.pyplot 导入为 plt
    将 pandas 导入为 pd
    
    # 选择数字列
    numeric_columns = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
    
    # 计算数字列的相关矩阵
    相关矩阵 = df[numeric_columns].corr()
    
    # 添加列名作为相关矩阵的第二行
    相关矩阵.列 = 相关矩阵.列.值
    相关矩阵.索引 = 相关矩阵.列.值
    
    # 设置 matplotlib 图形
    plt.figure(figsize=(12, 10))
    
    # 绘制正确显示值的热图
    sns.heatmap（correlation_matrix，annot=True，cmap=&#39;coolwarm&#39;，fmt=“.2f”，linewidths=.5）
    
    # 调整布局以获得更好的可视化效果
    plt.title(“相关矩阵”)
    plt.show()


我需要有人能帮我解决这个问题..]]></description>
      <guid>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</guid>
      <pubDate>Tue, 06 Feb 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras 难以模拟简单的代数函数</title>
      <link>https://stackoverflow.com/questions/77944886/keras-struggles-to-model-simple-algebraic-function</link>
      <description><![CDATA[我最初的问题是为相当大的数据集构建一个神经网络，但我遇到了一个问题，在训练之后，模型始终为任何输入生成完全相同的输出，因此为了调试，我已经转向更简单的问题。&lt; /p&gt;
我能够成功地训练基本逻辑运算符 - AND、OR、XOR 等。因此，我继续尝试对一个简单的代数函数 y = x^2 + 3x - 7 进行建模（我任意选择了这个）。问题是，即使对于这个非常简单的函数，收敛也非常不稳定，需要大量的时期，并且通常会出现相同的“恒定输出”。问题再次出现。
例如，下面的代码：
 from tensorflow.keras.models import Sequential
 从tensorflow.keras.layers导入Dense
 从tensorflow.keras.optimizers导入Adam

# 创建一个顺序模型
    模型=顺序（）

    # 定义训练数据
    X_train = np.array([[-10], [-8], [-6], [-4], [-2], [0], [2], [4], [6], [8 ], [10]])
    y_train = np.array([[63], [33], [11], [-3], [-9], [-7], [3], [21], [47], [81], [123]]）

    # 创建一个简单的神经网络模型
    模型=顺序（）
    model.add（密集（单位= 6，input_dim = 1，激活=&#39;relu&#39;））
    model.add（密集（单位= 12，激活=&#39;relu&#39;））
    model.add（密集（单位= 6，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;线性&#39;））
    
    # 编译模型
    优化器类 = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizerClass,loss=&#39;mean_squared_error&#39;,metrics=[&#39;mae&#39;])

    # 训练模型
    model.fit（X_train，y_train，epochs = 5000，batch_size = 1000）

    # 测试训练好的模型
    X_test = np.array([[-9], [-7], [-5], [-3], [-1], [1], [3], [5], [7], [9 ]]）
    y_test = np.array([[47], [21], [3], [-7], [-9], [-3], [11], [33], [63], [101]] ）
    预测 = model.predict(X_test)
    
    损失，mae = model.evaluate(X_test, y_test)
    print(f&#39;总体测试损失：{loss}，MAE：{mae}&#39;)

运行 5 次会产生 3.6 到 19.0 之间的测试损失。对我来说，在这样一个简单的问题上测试损失会如此变化，这已经很奇怪了，但是看看每个测试 x 的单独输出，它似乎产生了独特的输出，而且总的来说是相当准确的。但是如果我向模型添加另外两层，如下所示
# 创建顺序模型
    模型=顺序（）

    # 定义训练数据
    X_train = np.array([[-10], [-8], [-6], [-4], [-2], [0], [2], [4], [6], [8 ], [10]])
    y_train = np.array([[63], [33], [11], [-3], [-9], [-7], [3], [21], [47], [81], [123]]）

    # 创建一个简单的神经网络模型
    模型=顺序（）
    model.add（密集（单位= 3，input_dim = 1，激活=&#39;relu&#39;））
    model.add(Dense(units=6,activation=&#39;relu&#39;)) #额外层
    model.add（密集（单位= 12，激活=&#39;relu&#39;））
    model.add（密集（单位= 6，激活=&#39;relu&#39;））
    model.add(Dense(units=3,activation=&#39;relu&#39;)) #额外层
    model.add（密集（1，激活=&#39;线性&#39;））
    
    # 编译模型
    优化器类 = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizerClass,loss=&#39;mean_squared_error&#39;,metrics=[&#39;mae&#39;])

    # 训练模型
    model.fit（X_train，y_train，epochs = 5000，batch_size = 1000）

    # 测试训练好的模型
    X_test = np.array([[-9], [-7], [-5], [-3], [-1], [1], [3], [5], [7], [9 ]]）
    y_test = np.array([[47], [21], [3], [-7], [-9], [-3], [11], [33], [63], [101]] ）
    预测 = model.predict(X_test)
    
    损失，mae = model.evaluate(X_test, y_test)
    print(f&#39;总体测试损失：{loss}，MAE：{mae}&#39;)

现在性能完全崩溃了。运行 5 次会产生 10.8 到 1600 之间的损失值，并且除了最好的情况外，大量测试数据都会产生相同的输出。
仅使用四层 3、6、3、1 来简化模型，会产生与更复杂的模型相同的行为 - 高度可变的收敛，并且大部分训练数据具有统一的输出。
我尝试过使用批量大小和学习率，更不用说一堆不同的网络配置 - 比上面更简单和更复杂 - 但总的来说，无论我做什么，性能都非常不稳定，并且容易出现产生无用的结果。
事实上，对于这样一个简单的问题，这种情况如此持续地发生，让我怀疑我在设置中遗漏了一些简单的东西，但对于我的生活，我无法弄清楚]]></description>
      <guid>https://stackoverflow.com/questions/77944886/keras-struggles-to-model-simple-algebraic-function</guid>
      <pubDate>Tue, 06 Feb 2024 02:02:25 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv5/SparseML - 无法在给定配方中找到任何修饰符</title>
      <link>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</link>
      <description><![CDATA[我正在尝试使用 SparseML 训练 YOLOv5s 模型。 （我不知道这是否重要，但我正在 Google Colab 中进行培训）。当我运行 train.py 时，出现以下错误：
ValueError：无法在给定配方中找到任何修饰符。修饰符必须在 yaml 键下以列表形式列出，名称中包含“修饰符”。这些键和列表也可以嵌套在用于分阶段食谱的额外键下。

这是我的recipe.yaml：
&lt;前&gt;&lt;代码&gt;---
布局：空
标题：ETS2 车辆检测数据集配方
---

修饰符：
    - !EpochRangeModifier
        开始纪元：0.0
        结束纪元：250.0

    - !SetLearningRateModifier
        开始纪元：5.0
        学习率：0.1

    - !LearningRateModifier
        开始纪元：0.0
        纪元结束：25.0
        lr_class：多步LR
        lr_kwargs：
            伽玛：0.9
            里程碑：[2.0、5.5、10.0]
        初始化lr：0.1

    - !GMPruningModifier
        开始纪元：50.0
        结束纪元：100.0
        更新频率：1.0
        初始化稀疏度：0.05
        最终稀疏度：0.65
        参数：[&#39;blocks.1.conv&#39;]
    
    - !QuantizationModifier
        开始纪元：100.0

    - !TrainableParamsModifier
        参数：[&#39;blocks.1.conv&#39;]

    - !SetWeightDecayModifier
        开始纪元：5.0
        重量衰减：0.0
    
    - !ConstantPruningModifier
        参数：[&#39;blocks.1.conv]

我做错了什么，如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</guid>
      <pubDate>Tue, 06 Feb 2024 01:41:58 GMT</pubDate>
    </item>
    <item>
      <title>如何塑造二元分类器模型以适应我的输入数据？</title>
      <link>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</link>
      <description><![CDATA[这是我的代码。
from pandas import read_csv
从 keras.preprocessing.image 导入 ImageDataGenerator
将张量流导入为 tf
从tensorflow.keras.models导入模型，顺序
从tensorflow.keras.layers导入Flatten、Dense、Conv2D


train_df = read_csv(“输出/train.csv”)
valid_df = read_csv(“输出/validate.csv”)

train_images = ImageDataGenerator(重新缩放=1./255)
train_generator = train_images.flow_from_dataframe(train_df, x_col=“file_path”, y_col=“on_off_str”,
                                                   class_mode=&#39;二进制&#39;，batch_size=8)

validate_images = ImageDataGenerator（重新缩放=1./255）
validate_generator = validate_images.flow_from_dataframe(valid_df, x_col=“file_path”, y_col=“on_off_str”,
                                                          class_mode=&#39;二进制&#39;，batch_size=8)


模型=顺序（[压平（input_shape =（32,32,3）），
                   密集（128，激活=tf.nn.relu），
                   密集（1，激活=tf.nn.sigmoid）]）

模型.summary()

model.compile(optimizer=tf.optimizers.Adam(),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

历史= model.fit（train_generator，steps_per_epoch = 8，epochs = 15，verbose = 1，
                    验证数据=验证生成器、验证步骤=8）

我正在尝试使用 32x32x3 图像的数据集创建一个简单的二元分类器。
我收到错误：
矩阵大小不兼容：In[0]: [8,196608]，In[1]: [24576,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]

解释器挂起。
如果我将批量大小更改为 1 以使大小匹配
我收到类似的错误，但解释器不再挂起。
矩阵大小不兼容：In[0]: [1,196608]，In[1]: [3072,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]
2024-02-05 19：54：54.132353：W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。
         [[{{节点 PyFunc}}]]

如何更改输入或模型的形状？我的图像是 32x32x3。]]></description>
      <guid>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</guid>
      <pubDate>Tue, 06 Feb 2024 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>如何将图像或 360 度视频片段转换为 3D 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77941184/how-to-convert-image-or-360-degree-video-footage-into-3d-model</link>
      <description><![CDATA[我想从图像和 360 度摄像机视频片段创建 3D 模型。我知道它包含深度学习和计算机视觉方面的许多工作]]></description>
      <guid>https://stackoverflow.com/questions/77941184/how-to-convert-image-or-360-degree-video-footage-into-3d-model</guid>
      <pubDate>Mon, 05 Feb 2024 13:07:15 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（大约每分钟 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量低于恒定条时，查询将运行并且可以很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
&lt;块引用&gt;
索引错误 - 列表索引超出范围

请注意，查询数量“bar”是发生此错误的时间取决于每个查询的长度，并且如果查询长度在程序执行期间保持相同，则该错误是一致的。例如，在尝试将查询长度增加大约 20% 后，查询长度从大约 330 个查询下降到大约 60 个查询。
&lt;块引用&gt;
文件
“/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，
第 1315 行，文本
返回 self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，该位置应该只有 300 个查询/分钟的配额。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。
我写的Gemini Pro API调用函数是：
def gemini_response(message: str) -&gt; &gt;字符串：
    # 初始化顶点AI
    vertexai.init(project=“project-id-0123”, location=“us-central1”)

    # 加载模型
    模型 = GenerativeModel(“gemini-pro”)

    # 查询模型
    响应 = model.generate_content(消息)
    返回响应.文本

在调试 candidates 变量的问题时，变量检查结果如下所示：
&lt;前&gt;&lt;代码&gt;&gt;自己
&gt;提示_反馈{block_reason：其他}
&gt;使用元数据{prompt_token_count：505total_token_count：505}

&gt;自我候选人
&gt; []

&gt; self._raw_response
&gt;提示_反馈{block_reason：其他}
&gt;使用元数据{prompt_token_count：505total_token_count：505}
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>根据我的口味对图片进行分类</title>
      <link>https://stackoverflow.com/questions/77930655/classification-of-pictures-according-to-my-taste</link>
      <description><![CDATA[我正在尝试创建一个模型，将图片（艺术、照片）分为 3 类：“我喜欢”和“我喜欢”。 “正常”和“没兴趣” (2,1,0)，但我无法通过 53% 的猜测限制
我有一个包含 29240 张图像的数据集。我完全使用了它，每个类别有 4500 张图片，我使用了论证，没有任何东西有助于增加猜测的百分比。还尝试了调整大小和重新缩放。
如何修改模型以提高结果质量？
我找到了这个用于对象分类的模型：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()

model.add(Conv2D(16, (5, 5), padding=&#39;相同&#39;,
                 input_shape=(200, 200, 3), 激活=&#39;relu&#39;))

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(32, (5, 5), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (5, 5), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (5, 5), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))

模型.add(压平())
model.add（密集（512，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))
model.add（密集（512，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.4))

model.add（密集（3，激活=&#39;softmax&#39;））

model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/77930655/classification-of-pictures-according-to-my-taste</guid>
      <pubDate>Sat, 03 Feb 2024 02:15:34 GMT</pubDate>
    </item>
    <item>
      <title>优化不平衡测试集二元分类的决策阈值[关闭]</title>
      <link>https://stackoverflow.com/questions/77901993/optimizing-decision-threshold-for-binary-classification-on-unbalanced-test-set</link>
      <description><![CDATA[我正在研究一个二元分类问题，使用多层感知器 (MLP) 模型来区分假拟音和非假拟音音效。该模型输出一个概率分数（在输出层中使用 sigmoid 激活函数），然后我根据决策阈值将其分类为假或非假。目前，决策阈值设置为0.5。
我的数据集本质上是不平衡的，两个类之间的分布存在显着差异。然而，出于训练目的，我设法通过过采样来平衡数据集，从而在训练集中实现 50/50 的类分布。另一方面，评估测试集反映了原始的不平衡分布，大约包含 80% 的 0 类（假）和 20% 的 1 类（非假）。
我在评估集上取得了很好的精度 (88%)，但我担心仅基于该集优化决策阈值可能无法很好地推广到其他数据集，尤其是那些具有更平衡或不同分布集的数据集类。
我正在寻求有关如何在这种情况下实现决策阈值优化的建议。
注意：我在处理/训练步骤中使用 pytorch。]]></description>
      <guid>https://stackoverflow.com/questions/77901993/optimizing-decision-threshold-for-binary-classification-on-unbalanced-test-set</guid>
      <pubDate>Mon, 29 Jan 2024 18:54:38 GMT</pubDate>
    </item>
    </channel>
</rss>