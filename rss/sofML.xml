<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Jan 2024 06:18:44 GMT</lastBuildDate>
    <item>
      <title>神经网络无法正常学习</title>
      <link>https://stackoverflow.com/questions/77844067/neural-network-not-learning-properly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77844067/neural-network-not-learning-properly</guid>
      <pubDate>Fri, 19 Jan 2024 06:15:19 GMT</pubDate>
    </item>
    <item>
      <title>用于比较两个 Android 轨迹以发现性能差异的 AI 技术</title>
      <link>https://stackoverflow.com/questions/77844043/ai-techniques-for-comparing-two-android-traces-to-spot-performance-differences</link>
      <description><![CDATA[尝试构建一个系统，该系统可以获取从我的应用程序的两个版本收集的两组 Android 跟踪，并吐出跟踪中导致延迟问题的一些有问题的方面。是否有任何可以使用的 AI/ML 技术/框架？]]></description>
      <guid>https://stackoverflow.com/questions/77844043/ai-techniques-for-comparing-two-android-traces-to-spot-performance-differences</guid>
      <pubDate>Fri, 19 Jan 2024 06:09:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 fitz 模块从同一文件夹中的 pdf 文件中提取多个图像？</title>
      <link>https://stackoverflow.com/questions/77843782/how-do-i-use-the-fitz-module-to-extract-multiple-images-from-pdf-files-in-the-sa</link>
      <description><![CDATA[我正在从不同的 PDF 文件中提取图像，以使用它们构建数据集。是否可以对同一文件夹中的不同 PDF 文件自动执行此操作，这样我就不必为每个 PDF 单独执行此操作。
我使用以下代码从 1 个 PDF 中提取图像
python -m fitz extract -images “文件名”

我希望能够对超过 1500 个文件执行此操作，而无需手动操作。]]></description>
      <guid>https://stackoverflow.com/questions/77843782/how-do-i-use-the-fitz-module-to-extract-multiple-images-from-pdf-files-in-the-sa</guid>
      <pubDate>Fri, 19 Jan 2024 04:51:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么过滤pandas文件的输出变成NaN？</title>
      <link>https://stackoverflow.com/questions/77843589/why-the-output-of-filtering-pandas-file-becomes-nan</link>
      <description><![CDATA[我希望输出变为非 NaN 值。
这一行的问题
f_bp_max.loc[l, &#39;max&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].max()

当我打印这个时：
df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;]

输出是：
系列（[]，名称：预测，dtype：float64）

导入 pandas 作为 pd
导入时间
导入日期时间
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从 scipy.optimize 导入最小化标量

开始时间 = 时间.time()

pct_选择 = 0.98

#* 数据预测
df_frcst = pd.read_csv(&#39;lstm_forecast_results.csv&#39;)
df_frcst[&#39;日期时间&#39;] = pd.to_datetime(df_frcst[&#39;日期时间&#39;])
df_frcst_simple = df_frcst[[&#39;日期时间&#39;, &#39;预测&#39;]]
df_frcst_simple = df_frcst_simple.rename({&#39;日期时间&#39;:&#39;waktu&#39;, &#39;预测&#39;:&#39;bp&#39; }, axis = 1)



df_bp_max = pd.DataFrame()
对于范围 (2024, 2034, 1) 内的 k：
    l = k - 2024
    df_bp_max.loc[l, &#39;tahun&#39;] = str(k)
    df_bp_max.loc[l, &#39;max&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].max()
    df_bp_max.loc[l, &#39;min&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].min()
#df_bp_max[&#39;LF&#39;] = df_bp_max[&#39;min&#39;]/df_bp_max[&#39;max&#39;]
df_bp_max[&#39;tahun&#39;] = df_bp_max[&#39;tahun&#39;].apply(str)
#df_bp_max = df_bp_max.set_index(&#39;tahun&#39;)

打印（“================================================ ================》）
打印（df_bp_max）

输入文件
日期时间、实际、预测
2022-01-01 12:30:00,17809.0,17343.484
2022-01-01 13:00:00,17772.61,17382.861
2022-01-01 13:30:00,17867.8,17414.637
2022-01-01 14:00:00,17773.68,17504.357
2022-01-01 14:30:00,17869.88,17530.559
2022-01-01 15:00:00,17786.7,17592.822
2022-01-01 15:30:00,17943.11,17626.775
2022-01-01 16:00:00,18125.29,17686.678
2022-01-01 16:30:00,18463.05,17760.666
2022-01-01 17:00:00,18786.99,17892.475
2022-01-01 17:30:00,19238.97,18048.4
2022年1月1日 18
......

输出：
&lt;前&gt;&lt;代码&gt;============================================== =================
  塔洪最大最小值
0 2024 南 南
1 2025 南 南
2 2026 南 南
3 2027 南 南
4 2028 南 南
5 2029 南 南
6 2030 南 南
7 2031 南 南
8 2032 南 南
9 2033 南 南

这一行都没有 NaN 值，因为我的输入文件不为空]]></description>
      <guid>https://stackoverflow.com/questions/77843589/why-the-output-of-filtering-pandas-file-becomes-nan</guid>
      <pubDate>Fri, 19 Jan 2024 03:40:25 GMT</pubDate>
    </item>
    <item>
      <title>tf.data.dataset next(iter()) 产生相同的值</title>
      <link>https://stackoverflow.com/questions/77843560/tf-data-dataset-nextiter-yields-the-same-value</link>
      <description><![CDATA[我正在尝试构建一个数据管道来推断某些视频文件。视频数量很多，因此，我使用 tf.data.dataset 管道和 from_generator 方法来创建可管理的批次。但数据集不断再次产生相同的输出。
这是我的代码：
enteclass FrameGenerator：
def __init__(self, video_paths, n_frames, Training=False):
    ”“”
        返回 video_paths 列表中视频的一组帧

        参数：
        video_paths：视频路径列表
        n_frames：帧数
        训练：一个布尔值，用于确定是否应创建训练数据集
    ”“”

    self.video_paths = video_paths
    self.n_frames = n_frames
    自我训练=训练

def __call__(自我):
    ”“”
        被调用并产生一组帧
        每次调用类的实例时
    ”“”
    视频路径 = self.视频路径
    # 打印（类型（视频路径））
    如果自我训练：
        随机播放（视频路径）

    对于 video_paths 中的路径：
        video_frames =frames_from_video_file(路径, self.n_frames)
        文件名 = 路径.split(&#39;/&#39;)[-1]
        # print(文件名, “filfile”)
        在此处生成 video_frames, file_namer 代码

代码在这里实例化：
output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype=tf.float32), tf.TensorSpec(shape = None, dtype=tf.string))

数据集 = tf.data.Dataset.from_generator(FrameGenerator(video_paths, 20, 训练=False), output_signature=output_signature)

我的批量大小为 10
以及数据集的实例：
inference_datasets = dataset.batch(BATCH_SIZE)

但是当我这样称呼时：
sample_inference_dataset = next(iter(inference_datasets))

还有这个：
输入 codesample_inference_dataset[1]

它产生相同的文件集。]]></description>
      <guid>https://stackoverflow.com/questions/77843560/tf-data-dataset-nextiter-yields-the-same-value</guid>
      <pubDate>Fri, 19 Jan 2024 03:28:36 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于如何区分所拍摄的图像/照片是真实世界物体而不是其他图像/照片的指示吗？</title>
      <link>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</link>
      <description><![CDATA[听起来可能是一个愚蠢的想法，但我有这个想法来拍摄真实世界物体、风景等的图像/照片。但不太确定我可以使用什么处理或分类技术来区分这张照片是否来自现实世界或取自其他照片、图像等。提前致谢。
我尝试在网上查找，但找不到任何相关资源，或者至少找不到我想要的方向。]]></description>
      <guid>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</guid>
      <pubDate>Fri, 19 Jan 2024 02:47:33 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[运行以下代码时：
`类 NLayerDiscriminator(nn.Module):
def init(self, input_nc, ndf=64, n_layers=3,norm_layer=nn.BatchNorm2d, use_sigmoid=False):
super(NLayerDiscriminator, self).init()
self.n_layers = n_layers
&lt;前&gt;&lt;代码&gt; kw = 4
    padw = int(np.ceil((kw-1.0)/2))
    self.conv1=nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw)
    self.act=nn.LeakyReLU(0.2, True)
    
    nf=min(ndf*2,512)
    self.conv2=nn.Conv2d(ndf, nf, kernel_size=kw, stride=2, padding=padw)
    self.norm1=norm_layer(nf)
    
    ngf=min(nf*2,512)
    self.conv3=nn.Conv2d(nf, ngf, kernel_size=kw, stride=1, padding=padw)
    self.norm2=norm_layer(ngf)
    self.conv4=nn.Conv2d(ngf, 1, kernel_size=kw, stride=1, padding=padw)
    self.sig=nn.Sigmoid()
    

def 前向（自身，输入）：
    x=self.conv1(输入)
    x=self.act(x)
    
    对于范围 (1, 3) 中的 n：
        x=self.conv2(x)
        x=self.norm1(x)
        x=self.act(x)
    
    x=self.conv3(x)
    x=self.norm(x)
    x=self.act(x)
    x=self.conv4(x)
    如果使用_sigmoid：
        x=self.sig(x)
    返回x
            `

我收到以下错误
RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 算法与入口点文件问题</title>
      <link>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</link>
      <description><![CDATA[我正在尝试创建 awsgluespark 作业来训练其中一个数据集。我在1.3-1版本中使用xgboost算法。当我尝试运行估算器时，我遇到了问题
基础设施：awsglue 4.00 Spark shell
所有文件夹都是s3路径
代码片段。
xgb_script_mode_estimator = XGBoost(
    入口点=“训练.py”，
    超参数=超参数，
    角色=角色，
    实例计数=1，
    实例类型=实例类型，
    Framework_version =“1.3-1”，
    output_path=“s3://{}/{}/{}/output”.format(hyperparameters[&#39;bucket_nm&#39;], &#39;/output/&#39;, job_name),
   

错误：
FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;training.py&#39;
我将“粘合脚本”放置在和 Training.py 与 init.py 文件位于同一文件夹中的同一作业存储桶中。
XGBoost 函数无法识别同一文件夹中的training.py（训练文件没有名称不匹配，包括大小写）]]></description>
      <guid>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</guid>
      <pubDate>Fri, 19 Jan 2024 00:43:36 GMT</pubDate>
    </item>
    <item>
      <title>nn.参数在训练期间不更新[关闭]</title>
      <link>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</link>
      <description><![CDATA[我正在尝试编写 PyGeometric MessagePassing 层，如下所示：
类 EdgeNNLayer(MessagePassing):
    def __init__(自身, d0=45/60):
        super().__init__(aggr=&#39;mean&#39;)
        self.d0 = d0
        self.beta = 参数(torch.empty(1))
        self.reset_parameters()
        self.weight_aggr = aggr.SumAggregation()
    
    def重置参数（自身）：
        print(&quot;重置参数&quot;)
        self.beta.data[0] = 1 / self.d0
    
    def 消息（自身，x_i，x_j，edge_attr）：
        beta = self.beta.clone()
        返回 x_j * torch.exp(-beta * edge_attr[:, 0:1]), torch.exp(-beta * edge_attr[:, 0:1])
    
    defforward（自身，x，edge_index，edge_attr）：
        edge_index，edge_attr = add_self_loops（edge_index，edge_attr，fill_value = 0，num_nodes = x.shape [0]）
        返回 self.propagate(edge_index, x=x, edge_attr=edge_attr)
        
    def 聚合（自我，输入，索引，ptr =无，dim_size =无）：
        值、权重 = 输入
        return self.aggr_module(vals,index,ptr=ptr,dim_size=dim_size,dim=self.node_dim)# / self.weight_aggr(权重,index,ptr,dim_size)

当我训练该层（使用该层更新 PyG 图后使用的一些其他 Linear 层）时，我看到网络的所有其他可训练元素均由 beta&lt; 更新/code&gt; 在上面的层中不是。
我尝试将上面的 __init__ 中的行更改为：
 self.beta = 参数(torch.empty(1))
        self.beta.retain_grad()
        打印(self.beta.retains_grad)
        打印（自我.beta.grad）

我得到False和None，这很奇怪。当我在每个纪元后打印它时，我也会得到这些。
我尝试以几种不同的方式更改代码，包括更改克隆、注释掉几行、更改该层所使用的整体网络结构等。]]></description>
      <guid>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</guid>
      <pubDate>Thu, 18 Jan 2024 18:34:13 GMT</pubDate>
    </item>
    <item>
      <title>如何从数据生成 if-else 条件或树状结构或隐藏规则</title>
      <link>https://stackoverflow.com/questions/77838790/how-to-generate-if-else-conditions-or-tree-like-structure-or-hidden-rules-from-d</link>
      <description><![CDATA[假设我们生活在一个孩子的名字是根据其父母和祖父母的详细信息确定的世界。我想从这样一个给定的数据集中找到所有规则。
假设我有给定的数据：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

父亲
妈妈
奶奶
爷爷
父亲年龄
母亲年龄
孩子姓名


&lt;正文&gt;

蒂姆
帕特里夏

延斯


标记


蒂姆
奥利维亚
海伦娜


37
标记


蒂姆

凯茜
罗尔夫

45
标记


山姆
百合

鲁道夫
36

莎莉


山姆
百合
辛迪
标记
40
42
莎莉


乔治
布伦达
百合
一月
27
28
标记


乔治
波莉
多莉
肯
38

奥利维亚




我想找到这样的规则：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

规则
输出孩子名字


&lt;正文&gt;

父亲：蒂姆
标记


父亲：山姆+母亲：莉莉
莎莉


父亲：乔治 + 母亲：布伦达 + 祖母：莉莉 + 祖父：简 + 父亲年龄：27 + 母亲年龄：28
标记


父亲：乔治 + 母亲：波莉 + 奶奶：多莉 + 爷爷：肯 + 父亲年龄：38 + 母亲年龄：空白
奥利维亚




我的数据很大，输入列更多，所以我无法使用暴力方法。我的限制是在 Python 3.10 中实现它。有什么方法可以不用暴力破解这个程序吗？]]></description>
      <guid>https://stackoverflow.com/questions/77838790/how-to-generate-if-else-conditions-or-tree-like-structure-or-hidden-rules-from-d</guid>
      <pubDate>Thu, 18 Jan 2024 10:48:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么多项式包含在回归中？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77838625/why-polynomial-is-included-in-regression</link>
      <description><![CDATA[多项式回归是非线性的，因为 x 与 f(x,β) 不线性相关；方程本身仍然是线性的，你怎么能证明这个问题是合理的？我在谷歌上得到了很多答案，但我没有得到最好的理由。 .
我在谷歌和其他平台上搜索。]]></description>
      <guid>https://stackoverflow.com/questions/77838625/why-polynomial-is-included-in-regression</guid>
      <pubDate>Thu, 18 Jan 2024 10:24:17 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS() 以 .rda 格式保存。
例如，我将模型训练为：
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)

当我在清除所有环境变量后加载上述模型，并对新数据使用预测函数时：
预测（model_old，newdata = new_data）

我收到以下错误：
eval(Call$data) 中的错误：未找到对象“test”

但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么在将交错数据集与 Hugging Face (HF) 一起使用时，会出现 UnboundLocalError：赋值前引用的局部变量 'batch_idx'？</title>
      <link>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</guid>
      <pubDate>Thu, 18 Jan 2024 04:00:40 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有 2000 个不同标签的多类标记文本分类问题。使用 LSTM 和 Glove Embedding 进行分类。

目标变量的标签编码器
带有嵌入层的 LSTM 层
误差指标是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder()
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

LSTM 网络如下所示，带有 Glove Embeddings
np.random.seed(种子)
K.clear_session()
模型=顺序（）
model.add(嵌入(max_features, embed_dim, input_length = X_train.shape[1],
         权重=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add（密集（num_classes，激活=&#39;softmax&#39;））
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;sparse_categorical_crossentropy&#39;)
打印（模型.摘要（））

我的错误指标是 F1 分数。我为错误指标构建了以下函数
类指标（回调）：
    def on_train_begin(self, 日志={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_ precisions = []
 
    def on_epoch_end(自我, 纪元, 日志={}):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        _val_f1 = f1_score(val_targ, val_predict)
        _val_recall=recall_score(val_targ, val_predict)
        _val_ precision = precision_score（val_targ，val_predict）
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_ precisions.append(_val_ precision)
        print(&quot;— val_f1: %f — val_ precision: %f — val_recall %f&quot; % (_val_f1, _val_ precision, _val_recall))
        返回
 
指标=指标（）

##模型适合
model.fit（X_train，train_y，validation_data =（X_test，test_y），epochs = 10，batch_size = 64，callbacks = [指标]）

第一个纪元后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

我的代码哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>