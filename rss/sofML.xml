<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Mar 2024 09:14:23 GMT</lastBuildDate>
    <item>
      <title>语义搜索 应用 开源模型</title>
      <link>https://stackoverflow.com/questions/78191949/semantic-search-application-open-source-models</link>
      <description><![CDATA[我是数据科学领域的新手，我有近 30k .txt 文件，我需要创建一个搜索应用程序，该应用程序需要使用“And”进行 3-4 个输入。逻辑并返回最佳匹配。我正在尝试找到免费的语义搜索模型来做到这一点，我尝试了 txtai 和 haystack 模型，但无法达到我的目的。
尝试过 txtai 和 Haystack，]]></description>
      <guid>https://stackoverflow.com/questions/78191949/semantic-search-application-open-source-models</guid>
      <pubDate>Wed, 20 Mar 2024 08:53:41 GMT</pubDate>
    </item>
    <item>
      <title>从波形中提取峰值和平均面积[关闭]</title>
      <link>https://stackoverflow.com/questions/78190852/peak-and-mean-area-extraction-from-a-waveform</link>
      <description><![CDATA[在此处输入图像描述是否有任何方法可以提取峰值和平均值从当前波形时间序列中提取&gt;
请帮忙。在此处输入图像描述
我尝试手动完成，但数据非常庞大。那么有没有人工智能工具或任何其他方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78190852/peak-and-mean-area-extraction-from-a-waveform</guid>
      <pubDate>Wed, 20 Mar 2024 04:27:02 GMT</pubDate>
    </item>
    <item>
      <title>我可以将另一个模型的参数纳入该模型的损失函数中吗？</title>
      <link>https://stackoverflow.com/questions/78190831/can-i-involve-another-models-parameters-in-this-models-loss-function</link>
      <description><![CDATA[我有一个模型 (A)，需要根据其与另一个模型 (B) 的相似性进行训练。
解决方案1：我将B传递给A的损失函数，在这个函数中，我计算B.parameters()。
解决方案1
解决方案2：我将 B.parameters() 传递给 A 的损失函数。
 def mtl_loss_fn(self、logits、标签、shared_model_parameters):
        Sample_loss_fn = torch.nn.CrossEntropyLoss()
        Mean_batch_term = Sample_loss_fn(logits, 标签)

        shared_model_parameters_cuda = [x.to(device=&#39;cuda&#39;) for x in shared_model_parameters]

        w_diff = torch.tensor(0., device=self.device)
        对于 zip(self.model.parameters(),shared_model_parameters_cuda) 中的 w、w_t：
            w_diff += torch.pow(torch.norm(w - w_t), 2)

        prox_term = 0.5 * self.lam * w_diff
        # print(f“平均批次项：{mean_batch_term}，prox_term：{prox_term}”)
        返回mean_batch_term + prox_term

我对这两种方法都进行了实验，但结果非常不同。我不明白为什么..解决方案 1 和 2 对我来说看起来是一样的。
当我传递 B/它的参数时，我是否就地对 B 做了什么？为什么它们的功能不同？]]></description>
      <guid>https://stackoverflow.com/questions/78190831/can-i-involve-another-models-parameters-in-this-models-loss-function</guid>
      <pubDate>Wed, 20 Mar 2024 04:16:48 GMT</pubDate>
    </item>
    <item>
      <title>对同一数据集的不同子组进行迁移学习</title>
      <link>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</link>
      <description><![CDATA[我目前正在尝试根据回归任务的特定列中的值将原始数据集分为 6 个子组。每个子组中目标变量的分布非常相似。我的目标是通过首先对 5 个子组进行预训练，然后对最后一个子组进行微调来应用迁移学习。
对于预训练，我为每个子组设置了单独的训练、验证和测试集。预训练包括将5个子组的训练集和验证集结合起来，用它们来训练模型，验证模型，然后测量测试集上的损失。
随后，我使用预训练中的模型权重，并仅使用最终子组的训练集和验证集进行微调，并再次测量测试集上的损失。
但是，我遇到了一个问题，即我的模型对预训练数据过度拟合，导致微调过程中第一个周期的提前停止，因为微调集的验证误差会增加。
我希望我已经清楚地解释了我的问题。
我的方法正确吗？任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</guid>
      <pubDate>Wed, 20 Mar 2024 02:45:43 GMT</pubDate>
    </item>
    <item>
      <title>如何按 ROW 而不是数组元素读取包含数组的镶木地板文件？</title>
      <link>https://stackoverflow.com/questions/78189710/how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-elements</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78189710/how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-elements</guid>
      <pubDate>Tue, 19 Mar 2024 21:21:26 GMT</pubDate>
    </item>
    <item>
      <title>序列分类中的通道重要性</title>
      <link>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</link>
      <description><![CDATA[我有一个 ONNX 模型，它接受输入 [1, 35, 4]，即 [batch_size, num_channels, seq_len]，并输出 [1 , 3]，即[batch_size, num_classes]。我需要的是为每个通道分配一个数字，告诉我它对于模型做出的预测有多重要。我想我会使用 shap 的 PermutationExplainer 来达到此目的，但我不确定如何让它知道我不关心 seq_len&lt; /代码&gt;.
我尝试这样做：
导入 onnxruntime 作为 ort
导入形状
将 numpy 导入为 np

# 加载 ONNX 模型
model_path = &#39;解释示例.onnx&#39;
sess = ort.InferenceSession(model_path)

# 定义模型函数来处理批处理
定义模型(x):
    x = x.reshape(-1, 35, 4).astype(np.float32)
    # 模型期望输入 [1, 35, 4] 并返回 [1, 3]
    输出 = [sess.run(None, {&#39;input&#39;: x[i:i+1]})[0] for i in range(x.shape[0])]
    返回 np.concatenate(输出，轴=0)

# 创建样本输入
X = np.random.rand(1000, 35, 4).astype(np.float32)

输出名称 = [f&quot;输出_{i}&quot;;对于范围 (3) 内的 i]
feature_names = [f“频道 {i}”对于我在范围（35）]

def masker_fn(掩码, x):
    # 问题，掩码形状为 (140,)，x 形状为 (35, 4)
    masked_x = x.copy()
    对于范围内的 i(x.shape[1])：
        如果掩码[i] == 0：
            masked_x[:, i, :] = 0
    返回 masked_x

# 使用通道的自定义掩码声明解释器
解释器= shap.PermutationExplainer（模型，masker_fn，feature_names=feature_names，output_names=output_names）

# 计算形状值
shap_values = 解释器(X)

但问题是发送到掩码器的掩码具有形状 (140,) 而不是 (35,)。我当然可以以某种方式合并跨 seq_len 维度的掩码，但我认为 1. 解释器完成了不必要的工作，2. 也许这会以某种方式破坏其内部算法。
我怎样才能正确地告诉它，不，没有 140 个功能，而是 35 个？]]></description>
      <guid>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</guid>
      <pubDate>Tue, 19 Mar 2024 19:34:59 GMT</pubDate>
    </item>
    <item>
      <title>R 中插入符号的训练函数中的中心和比例因子预测器</title>
      <link>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</link>
      <description><![CDATA[我在使用 R caret 包的 train 函数的 preProc 参数方面遇到问题。我想居中并缩放我的预测变量，但忽略因子列。当我在火车之外进行预处理时，它工作正常，但我希望在火车功能内进行预处理。我错过了什么吗？
下面是一个示例，其中在训练之外使用 preProcess 时忽略因子预测器。
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
预处理(df[-1], method=c(&#39;center&#39;,&#39;scale&#39;))

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (1)
  - 被忽略 (1)
  - 缩放 (1)

这是我在火车内部使用 preProc 时发生的情况
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
mod &lt;- 训练（得分 ~., 数据 = df,
             方法=“lm”，
             preProc = c(“中心”,“比例”))
mod$预处理

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (2)
  - 被忽略 (0)
  - 缩放 (2)
]]></description>
      <guid>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</guid>
      <pubDate>Tue, 19 Mar 2024 19:25:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加（就像一个好的模型应该的那样），而剧集平均值奖励保持在 -1 不变，这就是 sutton_barto_reward 系统的工作原理。
导入体育馆
从 cartpole 导入 CartPoleEnv
从 stable_baselines3 导入 PPO
从 stable_baselines3.ppo.policies 导入 MlpPolicy

env = CartPoleEnv(sutton_barto_reward=True)

模型 = PPO(“MlpPolicy”, env, gamma=1, verbose=1)
model.learn(total_timesteps=30000)

但是，我不明白为什么会这样，因为在gymnasium的GitHub文档中的Cartpole代码中似乎没有使用任何折扣率。既然剧集的累积奖励始终相同，那么剧集长度平均值难道不应该没有任何改善吗？]]></description>
      <guid>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</guid>
      <pubDate>Tue, 19 Mar 2024 16:02:07 GMT</pubDate>
    </item>
    <item>
      <title>关于空闲时间的数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187717/data-set-about-free-time</link>
      <description><![CDATA[我想到了一个项目 - 一种用于组织与朋友外出活动的人工智能机器人。其中一部分是实际创建，甚至搜索（如果有的话）预先标记的数据集（这就是 chatgpt 告诉我的 - 我是全新的）。
我在哪里可以找到它，或者如何创建它？]]></description>
      <guid>https://stackoverflow.com/questions/78187717/data-set-about-free-time</guid>
      <pubDate>Tue, 19 Mar 2024 15:11:34 GMT</pubDate>
    </item>
    <item>
      <title>我的 scikit-learn 代码序列正确吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</link>
      <description><![CDATA[我已经构建了一个包含一些转换的管道并训练了一个 SVC 分类器。代码中步骤的顺序是否正确？
我正在使用此处找到的processed.cleveland.data数据集：https： //archive.ics.uci.edu/dataset/45/heart+disease。
将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.model_selection 导入 train_test_split
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_val_score

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.impute 导入 SimpleImputer

从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.svm 导入 SVC
url =“C:/Users/.../processedcleveland.data”
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39; , &#39;thal&#39;, &#39;num&#39;]
def getData():
        返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)

打印(dataframe.info())

numeric_ix = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical_ix = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字_ix）
打印（分类_ix）
&#39;&#39;&#39;
t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;, OneHotEncoder(), categorical_ix), (&#39;num0&#39;, SimpleImputer(strategy) =&#39;中位数&#39;), numeric_ix), (&#39;num1&#39;, MinMaxScaler(), numeric_ix)]
col_transform = ColumnTransformer(变压器=t)

管道 = 管道(步骤=[(&#39;t&#39;, col_transform)])
# 将管道拟合到转换后的数据上
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））
&#39;&#39;&#39;
X_train、X_validation、Y_train、Y_validation = train_test_split(X、y、test_size=0.20、random_state=1)


categorical_impute = 管道([
    （“mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;most_frequent&#39;）），
    (“one_hot”, OneHotEncoder())
]）

numeric_impute = 管道([
    （“num_mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;中位数&#39;）），
    (“min_max”, StandardScaler())
]）

预处理器 = ColumnTransformer([
    (“cat_impute”, categorical_impute, categorical_ix),
    (“num_impute”, numeric_impute, numeric_ix)
]，余数=“直通”）


模型 = SVC(伽玛=&#39;自动&#39;)

kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)

pipeline = Pipeline(steps=[(&#39;prep&#39;, 预处理器), (&#39;m&#39;, model)])

cv_results = cross_val_score(管道, X_train, Y_train, cv=kfold, 评分=&#39;准确度&#39;)
print(&#39;%s: %f (%f)&#39; % (&quot;SVC: &quot;, cv_results.mean(), cv_results.std()))
# 结果 = preprocessor.fit_transform(dataframe)
# print(pd.DataFrame.from_records(结果).to_string())

分类器的效率很低。有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</guid>
      <pubDate>Tue, 19 Mar 2024 14:38:47 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用kaggle中的两个GPU在pytorch中进行训练？</title>
      <link>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</link>
      <description><![CDATA[我正在 Kaggle GPU 中训练模型。
但正如我所看到的，只有一个 GPU 正在工作。

我使用普通方法进行训练，例如
device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)
模型 = model.to(设备)

如何同时使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</guid>
      <pubDate>Wed, 13 Sep 2023 04:56:24 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型</title>
      <link>https://stackoverflow.com/questions/75010397/xgboost-survival-model</link>
      <description><![CDATA[我正在尝试开发 XGBoost 生存模型。这是我的代码的快速快照：
X = df_High_School[[&#39;Gender&#39;, &#39;Lived_both_Parents&#39;, &#39;Moth_Born_in_Canada&#39;, &#39;Father_Born_in_Canada&#39;,&#39;Born_in_Canada&#39;,&#39;Aboriginal&#39;,&#39;Visible_Minority&#39;]] # 协变量
y = df_High_School[[&#39;time_to_event&#39;, &#39;event&#39;]] # 事件发生时间和事件指示器

#将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#开发模型
model = xgb.XGBRegressor(objective=&#39;survival:cox&#39;)

它给了我以下错误：
&lt;块引用&gt;
&lt;小时/&gt;

ValueError Traceback（最近一次调用最后一次）
 在
18
19 # 将模型拟合到训练数据
---&gt; 20 model.fit(X_train, y_train)
21
22 # 对测试集进行预测
2帧
_maybe_pandas_label（标签）中的/usr/local/lib/python3.8/dist-packages/xgboost/core.py
261 if isinstance（标签，DataFrame）：
[262] 第 262 章1：
--&gt; 263 raise ValueError（&#39;标签的数据帧不能有多列&#39;）
264
[第 265 章]
ValueError：标签的 DataFrame 不能有多列
由于这是一个生存模型，我需要两列 t 来指示事件和 time_to_event。我还尝试将 Dataframes 转换为 Numpy，但它也不起作用。
有什么线索吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/75010397/xgboost-survival-model</guid>
      <pubDate>Wed, 04 Jan 2023 19:32:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有灰度图像的预训练神经网络？</title>
      <link>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</link>
      <description><![CDATA[我有一个包含灰度图像的数据集，我想在它们上训练最先进的 CNN。我非常想微调预训练模型（例如 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim#Pretrained&quot; rel=&quot;noreferrer ”在这里&lt;/a&gt;)。
问题是，我能找到权重的几乎所有模型都在包含 RGB 图像的 ImageNet 数据集上进行了训练。
我无法使用其中一个模型，因为它们的输入层需要一批形状 (batch_size, height, width, 3) 或 (64, 224, 224, 3)  就我而言，但我的图像批次是 (64, 224, 224)。
有什么方法可以使用这些模型之一吗？我曾想过在加载权重后删除输入层并添加我自己的权重（就像我们对顶层所做的那样）。这种做法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</guid>
      <pubDate>Fri, 24 Aug 2018 00:33:04 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的 TfidfVectorizer 如何专门包含单词</title>
      <link>https://stackoverflow.com/questions/19753945/tfidfvectorizer-in-sklearn-how-to-specifically-include-words</link>
      <description><![CDATA[我对 TfidfVectorizer 有一些疑问。
我不清楚这些词是如何选择的。我们可以提供最低支持，但在那之后，什么将决定选择哪些功能（例如，更高的支持更多机会）？如果我们说 max_features = 10000，我们总是得到相同的结果吗？如果我们说 max_features = 12000，我们会得到相同的 10000 特征，但额外添加 2000 吗？ 
此外，有没有办法扩展例如 max_features=20000 功能？我将它放在一些文本上，但我知道一些肯定应该包含的单词，还有一些表情符号“:-)”等。如何将这些添加到 TfidfVectorizer 对象中，以便它将可以使用该对象，用它来拟合和预测
to_include = [&quot;:-)&quot;, &quot;:-P&quot;]
方法 = TfidfVectorizer(max_features=20000, ngram_range=(1, 3),
                      # 我知道停用词，但是包含单词怎么样？
                      stop_words=test.stoplist[:100],
                      # 包含单词 ??
                      分析器=&#39;词&#39;,
                      min_df=5)
方法.fit(训练数据)

寻求结果：
X = method.transform(traindata)
X
”的稀疏矩阵
 以压缩稀疏行格式存储了 1135520 个元素&gt;]，
 其中 N 是样本大小
]]></description>
      <guid>https://stackoverflow.com/questions/19753945/tfidfvectorizer-in-sklearn-how-to-specifically-include-words</guid>
      <pubDate>Sun, 03 Nov 2013 14:19:46 GMT</pubDate>
    </item>
    </channel>
</rss>