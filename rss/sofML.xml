<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Apr 2024 18:17:21 GMT</lastBuildDate>
    <item>
      <title>教KI变得可爱（对于机器宠物）</title>
      <link>https://stackoverflow.com/questions/78359248/teach-ki-to-be-cute-for-a-robot-pet</link>
      <description><![CDATA[强化学习通常用于训练机器人。一般来说，可以确定一个简单的适应度函数（当然，可以进行许多改进）。对于要送货的机器人，您可以使用机器人所花费的时间。
但是，我想训练一个机器人来代替急速机器人。我想使用机器学习（我最喜欢的任务是强化学习）。不，将人类的所有反应写入代码中并不是一种选择。机器人应该尽可能独立地学习，甚至是全新的行为。由于许多机器宠物的广告都强调它们的人工智能，我相信这是很有可能的。不幸的是，我找不到任何关于如何实际实现这样的东西的资源。
谁能帮我吗？
关于如何为此任务实现适应度函数的描述会对我有很大帮助。
Python 中是否有一个用于此目的的库（例如gym）？或者我应该根本不使用强化学习（那么我需要一个数据集）？
有人有过人工智能机器人宠物的经验吗？我已经对 RL 了解很多，但在这个领域（机器人宠物）我完全是新手。
最终结果应该是一个人工智能模型，它可以控制机器人，使其在与机器人交互的人看来尽可能逼真和卡哇伊。换句话说，是真正的宠物。]]></description>
      <guid>https://stackoverflow.com/questions/78359248/teach-ki-to-be-cute-for-a-robot-pet</guid>
      <pubDate>Sat, 20 Apr 2024 18:12:16 GMT</pubDate>
    </item>
    <item>
      <title>模型加载错误 迁移学习 VGG16</title>
      <link>https://stackoverflow.com/questions/78359122/model-loading-error-transfer-learning-vgg16</link>
      <description><![CDATA[在此处输入图像描述
请帮忙解决这个错误，我在jupyter中训练了模型。我将模型保存为 pickle 文件并加载它并在 jupyter 笔记本上得到预测。但是当我尝试加载到 Flask 应用程序时，它会出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/78359122/model-loading-error-transfer-learning-vgg16</guid>
      <pubDate>Sat, 20 Apr 2024 17:24:36 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 分布式 - 是否有适用于所有神经网络的启发式方法？</title>
      <link>https://stackoverflow.com/questions/78358731/pytorch-distributed-is-there-a-heuristic-for-all-nns</link>
      <description><![CDATA[我有兴趣知道 pytorch.distributed 模块是否有任何启发式方法来尽最大努力，以防要求我的训练过程通过 nccl 后端分布在多个节点上（每个服务器都有多个 GPU），例如适用于 3 台服务器，每台服务器 4 个 GPU
torchrun --nnodes=3 --nproc-per-node=4 --rdzv_id=job1 --rdzv_backend=c10d --rdzv_endpoint=host1.example.com:29400 train_ddp.py

这是否可以以尽力而为的方式适用于任何神经网络架构？
我想创建一个门户网站，用户可以在其中提交神经网络训练脚本，我可以将这些脚本分发到我的云架构上，但我无法预见他们会对我的服务要求什么样的训练工作量，这就是我问的原因。 
如果有人知道神经网络缓冲区/层如何分布在多个 GPU 和多台机器上，那就加分了。]]></description>
      <guid>https://stackoverflow.com/questions/78358731/pytorch-distributed-is-there-a-heuristic-for-all-nns</guid>
      <pubDate>Sat, 20 Apr 2024 15:18:51 GMT</pubDate>
    </item>
    <item>
      <title>有关tensorflow库和keras的错误。我不知道为什么会有这样的错误。有人可以帮忙检查一下吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78358724/there-are-errors-about-tensorflow-library-and-keras-i-dont-know-why-there-are</link>
      <description><![CDATA[类型错误：无法直接创建描述符。
如果此调用来自 _pb2.py 文件，则您生成的代码已过时，必须使用 protoc &gt;= 3.19.0 重新生成。
如果您无法立即重新生成原型，其他一些可能的解决方法是：

将 protobuf 软件包降级到 3.20.x 或更低版本。
设置 PROTOCOL_BUFFERS_PYTHON_IMPLMENTATION=python （但这将使用纯 Python 解析，并且速度会慢很多）。

在此处输入图像描述
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78358724/there-are-errors-about-tensorflow-library-and-keras-i-dont-know-why-there-are</guid>
      <pubDate>Sat, 20 Apr 2024 15:17:40 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习中处理大于 5% 的异常值？</title>
      <link>https://stackoverflow.com/questions/78358607/handling-outliers-greater-than-5-in-machine-learning</link>
      <description><![CDATA[在我的机器学习项目中，我有超过 5% 的问题，我可以克服什么方法来获取数据而不减少或删除一些数据
q1 = data.quantile(0.25)
q3 = 数据.分位数(0.75)
IQR = q3 - q1
磅 = q1 - (1.5*IQR)
ub = q3 + (1.5*IQR)
outlier_sum = ((数据 &gt; ub)|(数据 &lt; lb)).sum()
异常值百分比 = (异常值总和/len(数据))*100
print(&#39;离群值总和:\n&#39;, outlier_sum)
print(&#39;\n 异常值百分比:\n&#39;,outliers_percentage)

异常值总和：
 0岁
性别 139
总胆红素 75
直接胆红素 80
碱性磷酸酶69
谷氨酰胺转氨酶72
天冬氨酸转氨酶 64
总蛋白质 3
白蛋白0
白蛋白球蛋白比率 0
目标0
数据类型：int64

 异常值百分比：
 年龄 0.000000
性别 24.428822
总胆红素 13.181019
直接胆红素 14.059754
碱性磷酸酶 12.126538
谷氨酰胺转氨酶 12.653779
天冬氨酸转氨酶 11.247803
总蛋白质 0.527241
白蛋白 0.000000
白蛋白球蛋白比率 0.000000
目标 0.00000

我用这个方法计算了，有人能解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78358607/handling-outliers-greater-than-5-in-machine-learning</guid>
      <pubDate>Sat, 20 Apr 2024 14:36:51 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-mutlclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-mutlclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>如何进行标准化我无法理解为什么平均值和方差没有得到应用</title>
      <link>https://stackoverflow.com/questions/78358310/how-to-do-normalization-i-am-failing-to-understand-why-the-mean-and-variance-are</link>
      <description><![CDATA[代码应该如何工作
我的代码：
从tensorflow.keras.layers导入标准化
标准化器 = 标准化(均值= 5 , 方差= 4) # 标准化对象
Normalized_tns1 = tf.constant([[3,4,5,6,7]])
打印（归一化_tns1.shape）
print(&quot;\n输出:\n&quot;)
归一化器（归一化_tns1）

我正在做的事情与上图相同，但是当我这样做时，我收到了重塑错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
[88] 中的单元格，第 7 行
      5 打印（归一化_tns1.shape）
      6 print(&quot;\n输出:\n&quot;)
----&gt; 7 标准化器(normalized_tns1)

文件 c:\Users\Sujal07\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第 119 章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123最后：
    124 删除filtered_tb

文件c:\Users\Sujal07\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在quick_execute（op_name，num_outputs，输入，attrs，ctx，名称）中
     51 尝试：
     52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54 个输入、属性、输出数）
     55 除了 core._NotOkStatusException 为 e：
     56 如果名称不是 None：

InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} 重塑的输入是一个值为 1 的张量，但请求的形状有 5 [Op:Reshape]

重塑可以工作，但我不想随着矩阵的变化而这样做]]></description>
      <guid>https://stackoverflow.com/questions/78358310/how-to-do-normalization-i-am-failing-to-understand-why-the-mean-and-variance-are</guid>
      <pubDate>Sat, 20 Apr 2024 13:00:57 GMT</pubDate>
    </item>
    <item>
      <title>如何计算尖峰神经网络电路实现中的“每个尖峰能量”？</title>
      <link>https://stackoverflow.com/questions/78358095/how-to-calculate-energy-per-spike-in-spiking-neural-networks-circuit-implemen</link>
      <description><![CDATA[在一些科学文献中（例如this和this) 关于 LIF（一种尖峰神经网络）的模拟电路实现，作者提到了“每尖峰能量”和“每个概要的能量”作为评价参数之一。如何从电路级实现中计算出来？]]></description>
      <guid>https://stackoverflow.com/questions/78358095/how-to-calculate-energy-per-spike-in-spiking-neural-networks-circuit-implemen</guid>
      <pubDate>Sat, 20 Apr 2024 11:56:58 GMT</pubDate>
    </item>
    <item>
      <title>在 keras 中，当模型拟合 epochs=5000 时，代码看起来非常巨大</title>
      <link>https://stackoverflow.com/questions/78358018/in-keras-while-model-fitting-with-epochs-5000-the-code-looks-so-huge</link>
      <description><![CDATA[所以，我正在尝试深度学习中的梯度下降。代码是这样的。
model=keras.Sequential([keras.layers.Dense(1, input_shape= (2,),activation=&#39;sigmoid&#39;, kernel_initializer=&#39;ones&#39;,bias_initializer=&#39;zeros&#39;)])

model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

model.fit(x_train_scaled,y_train, epochs=5000)

使用epochs=5000，我应该能获得约 90% 的准确率。但它在 jupyter 中占有重要地位。
我试图在较短的空间内达到 90% 的准确率。因为当我想引用上面的代码时，有很多东西需要滚动。]]></description>
      <guid>https://stackoverflow.com/questions/78358018/in-keras-while-model-fitting-with-epochs-5000-the-code-looks-so-huge</guid>
      <pubDate>Sat, 20 Apr 2024 11:30:52 GMT</pubDate>
    </item>
    <item>
      <title>在自动训练中将适配器与模型合并时出错</title>
      <link>https://stackoverflow.com/questions/78357392/error-in-merging-adapter-with-model-in-autotrain</link>
      <description><![CDATA[我正在尝试使用 Hugging Face 中的自动训练来微调某些模型。由于我没有大量的计算资源，因此我尝试微调模型 EleutherAI/pythia-14m 和此数据集。但我收到了这条消息：
无法合并适配器权重：为 PeftModelForCausalLM 加载 state_dict 时出错：
    base_model.model.gpt_neox.embed_in.weight 的大小不匹配：从检查点复制形状为 torch.Size([50280, 128]) 的参数，当前模型中的形状为 torch.Size([50277, 128])。
    base_model.model.embed_out.weight 的大小不匹配：从检查点复制形状为 torch.Size([50280, 128]) 的参数，当前模型中的形状为 torch.Size([50277, 128])。

当我启动此脚本时发生此错误，该脚本只是用 Jupiter 编写的终端命令。
!auto​​train llm \
     - 火车 \
    --模型“EleutherAI/pythia-14m” \
    --项目名称“my-llm” \
    --数据路径数据/ \
    --text-列文本 \
    --批量大小“4” \
    --lr“2e-5” \
    --纪元“3” \
    --块大小“1024” \
    --预热比率“0.03” \
    --lora-r“16” \
    --lora-alpha“32”； \
    --lora-dropout“0.05” \
    --权重衰减“0”。 \
    --梯度累积“4” \
    --logging-steps“10” \
    --use-peft \
    --合并适配器\

此外，当我尝试在拥抱脸部空间中进行自动训练时，也出现了同样的问题。
我没有机器学习经验，所以我无法想象，什么会导致这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78357392/error-in-merging-adapter-with-model-in-autotrain</guid>
      <pubDate>Sat, 20 Apr 2024 07:38:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行音频分类总是预测错误</title>
      <link>https://stackoverflow.com/questions/78354074/audio-classification-using-cnn-predicting-wrong-all-the-time</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78354074/audio-classification-using-cnn-predicting-wrong-all-the-time</guid>
      <pubDate>Fri, 19 Apr 2024 13:39:10 GMT</pubDate>
    </item>
    <item>
      <title>获取边界框问题</title>
      <link>https://stackoverflow.com/questions/78353726/getting-bounding-box-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78353726/getting-bounding-box-issue</guid>
      <pubDate>Fri, 19 Apr 2024 12:40:40 GMT</pubDate>
    </item>
    <item>
      <title>对于表格数据模型中的过度拟合我该怎么办</title>
      <link>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</link>
      <description><![CDATA[我建立了一个预测模型，用于根据所提供数据中的某些特征来预测结果。
该模型是一个利用 fastai 的表格学习器。
该数据集包含约 300 条记录，分为训练集、验证集和测试集。
我已经实现了解决过度拟合的技术，例如提前停止和权重衰减，但在对未见过的数据进行评估时，模型仍然似乎过度拟合。
此外，我还尝试调整学习率和批量大小等超参数，但没有改善。我怀疑我的模型架构或预处理管道的某些方面可能会导致该问题，但我不确定从哪里开始调查。
鉴于该项目的敏感性，我无法提供有关数据集或预测任务的具体细节，但我可以分享当前模型的预处理和结构。
这是训练的输出：

&lt;标题&gt;

纪元
train_loss
valid_loss
准确度
时间


&lt;正文&gt;

0
0.752707
0.579501
0.776119
00:00


1
0.699270
0.833771
0.776119
00:00


2
0.652438
0.598243
0.791045
00:00


3
0.621083
3.889398
0.776119
00:00


4
0.591348
0.632366
0.791045
00:00


5
0.580582
6.670314
0.791045
00:00



&lt;块引用&gt;
自 epoch 2 以来没有任何改进：提前停止

这是预处理的代码（在我构建了我不能透露的功能之后）。
features 列表定义每个特征，包括有效值范围和权重（feature、range_ 和 weight 如下面的标准化函数中所使用的那样）。
def custom_normalize(df, 特征, range_, 权重):
    df[特征] = 归一化(df[特征], range_)
    df[特征] = df[特征] * 权重
    返回df

分割 = RandomSplitter(valid_pct=0.2)(range_of(df))

procs = [分类，填充缺失]

对于功能，features.items() 中的信息：
    # 确定训练时选择值的范围。
    procs.append(partial(custom_normalize, feature=feature, range_=info[&#39;range&#39;],weight=info[&#39;weight&#39;]))

据我所知，构建模型和训练是非常标准的：
to = TabularPandas(df, procs=procs,
                   cat_names = cat_vars,
                   连续名称=连续变量，
                   y_names=dep_var,
                   分裂=分裂）

dls = to.dataloaders(bs=64)

Early_stop = EarlyStoppingCallback(监视器=&#39;准确度&#39;, min_delta=0.01, 耐心=3)

学习 = tabular_learner(dls, 指标=准确度, wd=0.1)
学习.lr_find()

# 绘制学习率。
learn.recorder.plot_lr_find()

# 根据情节选择学习率。
lr = learn.recorder.lrs[np.argmin(learn.recorder.losses)]

learn.fit_one_cycle(15, lr, cbs=early_stop)
学习.show_results()

# 如果模型不存在则只保存模型
# TODO 将保存包装在条件中，以防止模型存在时保存。
如果不是 os.path.exists(model_fname):
    学习.保存(model_fname)
]]></description>
      <guid>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</guid>
      <pubDate>Tue, 16 Apr 2024 08:38:01 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用 `bitsandbytes` 8 位量化需要加速：`pip install Accelerate`</title>
      <link>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[我正在尝试使用开源数据集微调 llama2-13b-chat-hf。
我一直使用此模板，但现在收到此错误：
导入错误：使用bitsandbytes 8位量化需要加速：pip install Accelerate和最新版本的bitsandbytes：pip install -i https://pypi .org/simple/bitsandbytes
我安装了所需的所有软件包，这些是版本：
加速@git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
    位和字节==0.42.0
    数据集==2.17.1
    拥抱脸集线器==0.20.3
    佩夫特==0.8.2
    分词器==0.13.3
    火炬==2.1.0+cu118
    火炬音频==2.1.0+cu118
    火炬视觉==0.16.0+cu118
    变形金刚==4.30.0
    trl==0.7.11

有人知道这是不是版本问题吗？
你是如何解决这个问题的？
我尝试安装其他版本，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Thu, 22 Feb 2024 12:37:11 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 和 Mediapipe 实现逼真的唇色变化？</title>
      <link>https://stackoverflow.com/questions/75793658/how-to-achieve-realistic-lip-color-change-using-opencv-and-mediapipe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75793658/how-to-achieve-realistic-lip-color-change-using-opencv-and-mediapipe</guid>
      <pubDate>Mon, 20 Mar 2023 17:50:46 GMT</pubDate>
    </item>
    </channel>
</rss>