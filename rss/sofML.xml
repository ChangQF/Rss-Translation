<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 09:15:10 GMT</lastBuildDate>
    <item>
      <title>如何解决 FB-Prophet 中发生的错误？ pip 的依赖解析器当前不考虑所有已安装的软件包</title>
      <link>https://stackoverflow.com/questions/77946463/how-to-resolve-the-error-occurring-in-fb-prophet-pips-dependency-resolver-doe</link>
      <description><![CDATA[FB先知模型发生错误
安装收集的软件包：假期
  尝试卸载：假期
    发现现有安装：假期 0.21.13
    卸载假期-0.21.13：
      成功卸载假期-0.21.13
错误：pip 的依赖项解析器当前未考虑所有已安装的软件包。此行为是以下依赖性冲突的根源。
Neuroprophet 0.6.2 要求假期&lt;0.22,&gt;=0.21，但您的假期为 0.42，这是不兼容的。
成功安装假期-0.42
警告：之前在此运行时导入了以下包：
  [假期]
您必须重新启动运行时才能使用新安装的版本。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77946463/how-to-resolve-the-error-occurring-in-fb-prophet-pips-dependency-resolver-doe</guid>
      <pubDate>Tue, 06 Feb 2024 09:06:54 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-coverts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-coverts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>关联矩阵热图中的问题[重复]</title>
      <link>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</link>
      <description><![CDATA[
我在热图中面临这个问题，它只显示热图第一行中的值..
我写了下面的代码
导入seaborn作为sns
    将 matplotlib.pyplot 导入为 plt
    将 pandas 导入为 pd

    # 选择数字列
    numeric_columns = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns

    # 计算数字列的相关矩阵
    相关矩阵 = df[numeric_columns].corr()

    # 添加列名作为相关矩阵的第二行
    相关矩阵.列 = 相关矩阵.列.值
    相关矩阵.索引 = 相关矩阵.列.值

    # 设置 matplotlib 图形
    plt.figure(figsize=(12, 10))

    # 绘制正确显示值的热图
    sns.heatmap（correlation_matrix，annot=True，cmap=&#39;coolwarm&#39;，fmt=“.2f”，linewidths=.5）

    # 调整布局以获得更好的可视化效果
    plt.title(“相关矩阵”)
    plt.show()

我需要有人能帮我解决这个问题..]]></description>
      <guid>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</guid>
      <pubDate>Tue, 06 Feb 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv5/SparseML - 无法在给定配方中找到任何修饰符</title>
      <link>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</link>
      <description><![CDATA[我正在尝试使用 SparseML 训练 YOLOv5s 模型。 （我不知道这是否重要，但我正在 Google Colab 中进行培训）。当我运行 train.py 时，出现以下错误：
ValueError：无法在给定配方中找到任何修饰符。修饰符必须在 yaml 键下以列表形式列出，名称中包含“修饰符”。这些键和列表也可以嵌套在用于分阶段食谱的额外键下。

这是我的recipe.yaml：
&lt;前&gt;&lt;代码&gt;---
布局：空
标题：ETS2 车辆检测数据集配方
---

修饰符：
    - !EpochRangeModifier
        开始纪元：0.0
        结束纪元：250.0

    - !SetLearningRateModifier
        开始纪元：5.0
        学习率：0.1

    - !LearningRateModifier
        开始纪元：0.0
        纪元结束：25.0
        lr_class：多步LR
        lr_kwargs：
            伽玛：0.9
            里程碑：[2.0、5.5、10.0]
        初始化lr：0.1

    - !GMPruningModifier
        开始纪元：50.0
        结束纪元：100.0
        更新频率：1.0
        初始化稀疏度：0.05
        最终稀疏度：0.65
        参数：[&#39;blocks.1.conv&#39;]
    
    - !QuantizationModifier
        开始纪元：100.0

    - !TrainableParamsModifier
        参数：[&#39;blocks.1.conv&#39;]

    - !SetWeightDecayModifier
        开始纪元：5.0
        重量衰减：0.0
    
    - !ConstantPruningModifier
        参数：[&#39;blocks.1.conv]

（另外，在 SparseML 和食谱方面，我是一个完全的初学者，所以我很乐意为我的食谱提供建议。）
我不确定我做错了什么，任何帮助/知识都会受到赞赏。感谢您的阅读，祝您早日休息。]]></description>
      <guid>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</guid>
      <pubDate>Tue, 06 Feb 2024 01:41:58 GMT</pubDate>
    </item>
    <item>
      <title>如何塑造二元分类器模型以适应我的输入数据？</title>
      <link>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</link>
      <description><![CDATA[这是我的代码。
&lt;前&gt;&lt;代码&gt;
从 pandas 导入 read_csv
从 keras.preprocessing.image 导入 ImageDataGenerator
将张量流导入为 tf
从tensorflow.keras.models导入模型，顺序
从tensorflow.keras.layers导入Flatten、Dense、Conv2D



train_df = read_csv(“输出/train.csv”)
valid_df = read_csv(“输出/validate.csv”)

train_images = ImageDataGenerator(重新缩放=1./255)
train_generator = train_images.flow_from_dataframe(train_df, x_col=“file_path”, y_col=“on_off_str”,
                                                   class_mode=&#39;二进制&#39;，batch_size=8)

validate_images = ImageDataGenerator（重新缩放=1./255）
validate_generator = validate_images.flow_from_dataframe(valid_df, x_col=“file_path”, y_col=“on_off_str”,
                                                          class_mode=&#39;二进制&#39;，batch_size=8)


模型=顺序（[压平（input_shape =（32,32,3）），
                   密集（128，激活=tf.nn.relu），
                   密集（1，激活=tf.nn.sigmoid）]）

模型.summary()

model.compile(optimizer=tf.optimizers.Adam(),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

历史= model.fit（train_generator，steps_per_epoch = 8，epochs = 15，verbose = 1，
                    验证数据=验证生成器、验证步骤=8）



我正在尝试使用 32x32x3 图像的数据集创建一个简单的二元分类器。
我收到错误：
矩阵大小不兼容：In[0]: [8,196608]，In[1]: [24576,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]

解释器挂起。
如果我将批量大小更改为 1 以使大小匹配
我收到类似的错误，但解释器不再挂起。
矩阵大小不兼容：In[0]: [1,196608]，In[1]: [3072,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]
2024-02-05 19：54：54.132353：W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。
         [[{{节点 PyFunc}}]]

如何更改输入或模型的形状？我的图像是 32x32x3。]]></description>
      <guid>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</guid>
      <pubDate>Tue, 06 Feb 2024 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>我尝试安装Installing TensorFlow, CUDA, cuDNN with Anaconda for GeForce GTX 1050 [关闭]</title>
      <link>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</link>
      <description><![CDATA[我尝试为 GeForce GTX 1050 安装“使用 Anaconda 安装 TensorFlow、CUDA、cuDNN”。
我使用了以下提到的文章：https://medium.com/@shaikhmuhammad/installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce-gtx-1050-ti-79c1eb94eb7a
我与文章中遵循的步骤的唯一区别是：

我有 nvidia 1050 而不是 1050ti
我在 c/users/lokes 中设置了我的环境，而不是在桌面上

我收到了这个错误，我已将其附加在屏幕截图中，但我无法找到解决该问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</guid>
      <pubDate>Tue, 06 Feb 2024 00:08:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LASSO 模型中获得 coef_ 中各个列的稀疏性？</title>
      <link>https://stackoverflow.com/questions/77944525/how-to-get-sparsity-in-individual-columns-in-coef-in-a-lasso-model</link>
      <description><![CDATA[所以我想在训练 LASSO 模型后获得稀疏系数，但我希望在每列系数上保持稀疏性。
假设在使用 5 个输入维度和 3 个输出维度训练 sklearn.linear_model.Lasso 后，我得到以下系数 _ ：
数组([[ 1.87666825 , 0. , -1.37516633, 0.57491936, -0.65120491],
       [0.，-0.5901364，-0.85214947，0.03498916，-0.81805396]，
       [-0.89515717、-0.04372684、-0.22986802、0.、-0.56785277]])

是否可以在特定列上获得稀疏性？例如：
&lt;前&gt;&lt;代码&gt;
数组([[ 0., 1.87666825, -1.37516633, 0.57491936, -0.65120491],
       [0.，-0.5901364，-0.85214947，0.03498916，-0.81805396]，
       [ 0.、-0.04372684、-0.22986802、-0.89515717、-0.56785277]])
]]></description>
      <guid>https://stackoverflow.com/questions/77944525/how-to-get-sparsity-in-individual-columns-in-coef-in-a-lasso-model</guid>
      <pubDate>Mon, 05 Feb 2024 23:23:46 GMT</pubDate>
    </item>
    <item>
      <title>如果在多类分类中删除相关嵌入，f1 分数会降低而 AUC 会增加吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</link>
      <description><![CDATA[我在 2 个场景中使用手套嵌入构建了一个神经网络模型。
场景一：
我使用手套嵌入在具有 3725 个唯一标记的 3 个类别的平衡数据集上构建了一个神经网络模型。我正在执行多类分类。
我得到的训练和测试样本 F1 分数分别为 93.2% 和 74.78%。
AUC得分分别为0.815和0.782。
我有 3725 个令牌，其中 25 个令牌在嵌入之间具有相关性 &gt; 0.95。
在场景 B 中，我删除了这些高度相关的标记。
场景 - B：
我再次使用手套嵌入在 3 个类的平衡数据集上构建了一个神经网络模型，这一次只有 3705 个唯一标记。
我得到的训练和测试样本 F1 分数分别为 74.2% 和 60.8%。
AUC得分分别为0.905和0.794。
即从场景 A 到场景 B，我的 f1 分数正在下降，但 AUC 正在增加。
问题：
当减小嵌入矩阵的大小时，文本分类模型中的 F1 分数是否会降低，但 AUC 会增加？
可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</guid>
      <pubDate>Mon, 05 Feb 2024 12:32:57 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以从 PyCaret 中的现有模型进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[摘要
我试图使用 Python 的 PyCaret 库开发一个 ML 模型来分析具有 34 个特征（列）的数据集。我运行 setup() 函数并注意到，由于编码，它将原始数据集扩展至 58 个特征。它还进行了插补和转换。经过这些步骤后，该库将基础数据分为训练集和测试集。
最终根据训练数据集从compare_models()中选择合适的模型。由此，我在测试集上使用 Predict_model() 进行了预测，并对结果感到满意。
我的提供商现在如何向我提供新数据，我想针对新数据运行 Predict_model() ，以便我可以将这些预测返回给我的提供商。为此，我使用了 Predict_model() 函数的“data”参数，但是，我得到的错误低于以下最小可行代码示例。
这个问题旨在了解如何确保我收到的新数据经过适当的预处理，以供在 Predict_model() 的“数据”参数中使用，或者，如果我的整个概念不准确，那么适当的方法应该是什么来满足最终的要求目标是根据开发的原始模型对我收到的新数据进行预测。
带注释的最小可行代码示例
&lt;前&gt;&lt;代码&gt;# 库
导入 pycaret
从 pycaret.regression 导入 *
将 pandas 导入为 pd

# 通过从 CSV 导入创建一个新的 DF
# 这个 DF 有 34 列
baseDf = pd.read_csv(“wave1data.csv”)

# 在 baseDf 上进行设置
# 转换/编码的 DF 结果有 58 列，并在训练/测试数据集之间进行分割
s = setup(baseDf, target = “我的目标功能”, session_id = 64)

# 基本模型对比
最好=比较模型（）

# 预测测试分割
wave1_pred = 预测模型（最佳）

# 此时，我有一个理想的模型，但它基于具有 58 列和多个转换/插补的 DF
# 我尝试使用以下方法对全新的未见过的数据进行预测：
wave2Df = pd.read_csv(“wave2data.csv”)
wave2_pred = Predict_model（最佳，数据= wave2Df）

抛出错误
&lt;块引用&gt;
------------------------------------------------------------ ---------------------------- KeyError Traceback（最近调用
最后）在&lt;细胞系：2&gt;（）
1 # 预测第 2 波
----&gt; 2wave2_pred=predict_model(最佳，数据=wave2Df)
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中
_raise_if_missing(self, key, 索引器, axis_name) 6131 6132 not_found =
列表(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-&gt;第6133章 6134 第6135章
KeyError：“[&#39;Endorsed By&#39;] 不在索引中”

（如果我理解正确的话，wave2Df 数据与“最佳”数据的结构不同，这是有道理的。）
汇总查询

如何克服这些错误并使用我根据新传入数据（在本例中为 wave2Df 的内容）生成的“最佳”模型来运行 Predict_model()？
我是否应该采取完全不同的方式来实现相同的目标？
]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入需要的模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入引擎

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行这个......已经遵循库安装并看到了
https://anomalib.readthedocs.io/en/latest/markdown/ get_started/anomalib.html
我认为要么是因为引擎已被修改，要么是被库删除了......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>我不明白为什么我的 k 均值算法不能用于异常检测</title>
      <link>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</link>
      <description><![CDATA[我已经实现了一种聚类算法来检测田纳西州伊士曼过程数据集上的异常，但结果很奇怪，因为它没有标记任何内容。请问我做错了什么？
附上结果混淆矩阵
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.cluster 导入 KMeans
从 sklearn.metrics 导入 precision_score
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从sklearn.metrics导入confusion_matrix

def fit_preprocess(data_path):
    # 从文件中加载数据
    数据 = pd.read_csv(data_path)

    # 为特定列定义延迟映射
    延迟列映射 = {
        “XMEAS(23)”: 6、“XMEAS(24)”: 6、“XMEAS(25)”: 6、“XMEAS(26)”: 6、“XMEAS(27)”: 6、
        “XMEAS(28)”: 6、“XMEAS(29)”: 6、“XMEAS(30)”: 6、“XMEAS(31)”: 6、“XMEAS(32)”: 6、
        “XMEAS（33）”：6，“XMEAS（34）”：6，“XMEAS（35）”：6，“XMEAS（36）”：6，“XMEAS（37）”：15，
        “XMEAS（38）”：15，“XMEAS（39）”：15，“XMEAS（40）”：15，“XMEAS（41）”：15
    }

    # 根据映射调整时间延迟
    Sample_time = 3 # 采样时间（以分钟为单位）
    对于列，delay_columns_mappings.items() 中的延迟：
        shift_steps = 延迟 // 采样时间
        数据[列] = 数据[列].shift(-shift_steps)

    # 填充因移位而出现的 NaN 值
    数据.ffill(inplace=True)

    # 标准化特征，不包括标签列
    feature_columns = [col for col in data.columns if col != &#39;label&#39;]
    定标器=标准定标器()
    数据[特征列] = scaler.fit_transform(数据[特征列])

    # 存储并返回预处理参数（每个特征的平均值和标准差）
    预处理参数 = {
        &#39;平均值&#39;：scaler.mean_，
        &#39;std&#39;：scaler.scale_
    }

    返回预处理参数

def load_and_preprocess(data_path, preprocess_params):
    # 加载数据集
    数据 = pd.read_csv(data_path)

    # 分离特征和标签
    feature_columns = [data.columns 中的 col 的 col，如果 col != &#39;label&#39;]
    X = 数据[特征列]
    y = 数据[&#39;标签&#39;]

    定标器=标准定标器()
    scaler.mean_ = preprocess_params[&#39;mean&#39;]
    scaler.scale_ = preprocess_params[&#39;std&#39;]
    X_scaled = 缩放器.transform(X)
    返回 X_scaled, y

def fit_model(X):
    # 训练模型
    模型 = KMeans(n_clusters=2, n_init=10, random_state=0)
    模型.拟合(X)
    返回模型

def 预测（X，模型）：
    # 预测数据点的聚类
    集群 = model.predict(X)

    # 确定异常簇（假设为较小的簇）
    anomaly_cluster = np.argmin(np.bincount(簇))

    # 将异常簇中的数据点标记为异常
    异常 = 集群 == anomaly_cluster

    # 将布尔标志转换为整数（0 表示正常，1 表示异常）
    返回异常.astype(int)


这是 k 均值算法的实现，该脚本尝试标准化数据集并从训练集预测故障并与测试集进行比较。]]></description>
      <guid>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</guid>
      <pubDate>Wed, 03 Jan 2024 19:38:18 GMT</pubDate>
    </item>
    <item>
      <title>即使在专辑中分配标签字段，“label_fields”也无效</title>
      <link>https://stackoverflow.com/questions/76788751/label-fields-are-not-valid-even-when-assigning-label-fields-in-albumentations</link>
      <description><![CDATA[我正在使用带有以下代码的专辑：
 增强器 = alb.Compose([alb.RandomCrop(width=450, height=450),
                             alb.Horizo​​ntalFlip(p=0.5),
                             alb.RandomBrightnessContrast(p=0.2),
                             alb.RandomGamma(p=0.2),
                             alb.RGBShift(p=0.2),
                             alb.VerticalFlip(p=.5)],
                             bbox_params=alb.BboxParams(format=&#39;albumentations&#39;, label_fields=[&#39;person&#39;]))

“中间图像加载代码”

img = pyplot.imread(“路径”)
坐标 = [1,2,3,4]
尝试：
    增强=增强（图像=img，bboxes=[坐标]，class_labels=[&#39;人&#39;]）

除了异常 e：
    打印(e)


我收到异常：您的“label_fields”无效 - 它们必须与 dict 中的参数具有相同的名称
我上网查了一下，发现其他人也有同样的问题，但从未找到具体的解决方案。我还查看了文档，我无法解读 data 与我的问题有何关联。任何对此的帮助将不胜感激！
此外，由于某种原因，按“tab”键会导致在此网站上不起作用，因此缩进可能已关闭。]]></description>
      <guid>https://stackoverflow.com/questions/76788751/label-fields-are-not-valid-even-when-assigning-label-fields-in-albumentations</guid>
      <pubDate>Fri, 28 Jul 2023 14:32:09 GMT</pubDate>
    </item>
    <item>
      <title>预测测试图像时出现错误 - 无法重塑大小数组</title>
      <link>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</guid>
      <pubDate>Wed, 12 May 2021 17:25:10 GMT</pubDate>
    </item>
    <item>
      <title>错误 conda.core.link:_execute(698): 安装包“defaults::icu-58.2-ha925a31_3”时发生错误</title>
      <link>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</link>
      <description><![CDATA[我使用 anaconda 提示创建了环境 conda create -n Talkingbot python=3.5 然后安装 pip installtensorflow==1.0.0 （遵循与一个 udemy 中使用的相同命令当然）但是当我尝试安装spyder时
使用 conda install spyder 然后它给了我这个错误：
准备交易：完成
验证交易：完成
执行交易：完成
错误 conda.core.link:_execute(698)：安装包“defaults::icu-58.2-ha925a31_3”时发生错误。
回滚事务：完成

[Errno 13] 权限被拒绝：&#39;C:\\Users\\Lenovo\\anaconda3\\envs\\talkingbot\\Library\\bin\\icudt58.dll&#39;
()

然后我尝试使用 anaconda navigator 安装间谍程序，但从那里也没有安装间谍程序。
帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</guid>
      <pubDate>Sun, 13 Sep 2020 13:42:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn 进行加权线性回归</title>
      <link>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</link>
      <description><![CDATA[我的数据：
状态 N Var1 Var2
阿拉巴马州 23 54 42
阿拉斯加 4 53 53
亚利桑那州 53 75 65

Var1 和 Var2 是州级别的聚合百分比值。 N 是每个状态的参与者数量。我想在 Python 2.7 中使用 sklearn 考虑 N 作为权重，在 Var1 和 Var2 之间运行线性回归。
总的思路是：
fit(X, y[, 样本权重])

假设使用 Pandas 将数据加载到 df 中，并且 N 变为 df[&quot;N&quot;]，我是否只需拟合数据或者我是否需要在命令中将 N 用作 sample_weight 之前以某种方式对其进行处理？
fit(df[&quot;Var1&quot;], df[&quot;Var2&quot;],sample_weight=df[&quot;N&quot;])
]]></description>
      <guid>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</guid>
      <pubDate>Sat, 06 Feb 2016 02:58:20 GMT</pubDate>
    </item>
    </channel>
</rss>