<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 08 Nov 2024 15:17:46 GMT</lastBuildDate>
    <item>
      <title>错误代码 13 使用文档处理器进行训练</title>
      <link>https://stackoverflow.com/questions/79170448/error-code-13-training-with-document-processor</link>
      <description><![CDATA[处理器类型：自定义提取器
尝试训练文档 ai 处理器时出现以下错误：

{
&quot;code&quot;: 3,
&quot;message&quot;: &quot;Invalid document.&quot;,
&quot;details&quot;: [
{
&quot;@type&quot;: &quot;type.googleapis.com/google.rpc.ErrorInfo&quot;,
&quot;reason&quot;: &quot;INVALID_DOCUMENT&quot;,
&quot;domain&quot;: &quot;documentai.googleapis.com&quot;,
&quot;metadata&quot;: {
&quot;num_fields_needed&quot;: &quot;1&quot;,
&quot;field_name&quot;: &quot;entities.text_anchor.text_segments&quot;,
&quot;document&quot;: &quot;gs://pitaia-vertexai/notas-fiscais/TREINAMENTO 20.05.2024/test.pdf&quot;,
&quot;num_fields&quot;: &quot;0&quot;,
&quot;annotation_name&quot;: &quot;produto/quantidade&quot;
}
}
]
}


在审计日志中，它仅显示以下消息：
副本 workerpool0-0 以非零状态 1 退出。终止原因：错误。要了解有关您的作业退出原因的更多信息，请检查日志：LINK
当我尝试访问链接时，它给出了权限错误。
注意：目前有 1916 个培训文档。
我无能为力，因为错误是一般性的。
昨天我又遇到了一个错误：

{
&quot;code&quot;: 3,
&quot;message&quot;: &quot;Invalid document.&quot;,
&quot;details&quot;: [
{
&quot;@type&quot;: &quot;type.googleapis.com/google.rpc.ErrorInfo&quot;,
&quot;reason&quot;: &quot;INVALID_DOCUMENT&quot;,
&quot;domain&quot;: &quot;documentai.googleapis.com&quot;,
&quot;metadata&quot;: {
&quot;num_fields_needed&quot;: &quot;1&quot;,
&quot;field_name&quot;: &quot;entities.text_anchor.text_segments&quot;,
&quot;document&quot;: &quot;gs://pitaia-vertexai/notas-fiscais/TREINAMENTO 20.05.2024/test.pdf&quot;,
&quot;num_fields&quot;: &quot;0&quot;,
&quot;annotation_name&quot;: &quot;produto/quantidade&quot;
}
}
]
}


这很奇怪，因为之前已经训练过的文档中出现了错误。为了解决这个先前的错误，我将文档属性从 &quot;Training&quot; 更改为 &quot;为“未分配”，所以这些错误停止了，但是现在我又遇到了另一个错误！]]></description>
      <guid>https://stackoverflow.com/questions/79170448/error-code-13-training-with-document-processor</guid>
      <pubDate>Fri, 08 Nov 2024 14:35:44 GMT</pubDate>
    </item>
    <item>
      <title>插补并创建虚拟变量后缺少列。我该如何修复？</title>
      <link>https://stackoverflow.com/questions/79170289/columns-are-missing-after-imputing-and-creating-dummy-variables-how-should-i-fi</link>
      <description><![CDATA[简而言之：在插补之后，我的训练集和测试集的列是不同的。
制作训练、测试数据集的代码
random_state_value = 0

#定义目标
X = data.drop(columns = &#39;income&#39;, axis=1)
y = data[&#39;income&#39;]

#拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state_value)

#插补缺失数据
imputer_cat = SimpleImputer(strategy = &#39;most_frequent&#39;)
imputer_num = SimpleImputer(strategy = &#39;median&#39;)

X_train[[&#39;workclass&#39;, &#39;occupation&#39;, &#39;native-country&#39;]] = imputer_cat.fit_transform(X_train[[&#39;workclass&#39;, &#39;occupation&#39;, &#39;native-country&#39;]])
X_train[[&#39;age&#39;]] = imputer_num.fit_transform(X_train[[&#39;age&#39;]])

X_test[[&#39;workclass&#39;, &#39;occupation&#39;, &#39;native-country&#39;]] = imputer_cat.fit_transform(X_test[[&#39;workclass&#39;, &#39;occupation&#39;, &#39;native-country&#39;]])
X_test[[&#39;age&#39;]] = imputer_num.fit_transform(X_test[[&#39;age&#39;]])

#创建虚拟变量
X_train = pd.get_dummies(X_train, columns=[&#39;工作类别&#39;, &#39;教育&#39;, &#39;婚姻状况&#39;, 
&#39;职业&#39;, &#39;关系&#39;, &#39;种族&#39;, &#39;性别&#39;, &#39;本国&#39;], drop_first = True)
X_test = pd.get_dummies(X_test, columns=[&#39;工作类别&#39;, &#39;教育&#39;, &#39;婚姻状况&#39;, 
&#39;职业&#39;, &#39;关系&#39;, &#39;种族&#39;, &#39;性别&#39;, &#39;本国&#39;], drop_first = True)

y_train = pd.get_dummies(y_train, columns=&#39;收入&#39;, drop_first = True)
y_test = pd.get_dummies(y_test, columns=&#39;收入&#39;, drop_first = True)

y_test = y_test.values.ravel()
y_train = y_train.values.ravel()

我有分类变量，其中有缺失值。这就是我所做的。
1.
将数据分成训练集、测试集
2.
在训练集和测试集中估算每个值
3.
为分类变量创建虚拟变量
但随后一些列消失了，X_test 和 X_train 的长度也不同。
长度不匹配
丢失的列
temp_test = X_test.columns.sort_values()
temp_train = X_train.columns.sort_values()

[col for col in temp_train if col not in temp_test]

这些是列。
为什么会发生这种情况？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79170289/columns-are-missing-after-imputing-and-creating-dummy-variables-how-should-i-fi</guid>
      <pubDate>Fri, 08 Nov 2024 13:46:36 GMT</pubDate>
    </item>
    <item>
      <title>如何从 Java Spring Boot 后端调用 Python ML 模型？</title>
      <link>https://stackoverflow.com/questions/79170093/how-to-call-python-ml-model-from-a-java-spring-boot-backend</link>
      <description><![CDATA[我正在开展一个涉及 Java Spring Boot 应用程序和 Python ML 模型的项目。该模型预计会根据不同的参数产生预测。
我遇到了 Py4j 作为在两者之间创建网关的一种方式，但我想探索从 Java 调用 Python 模型的其他选项，就像调用函数一样。我可以使用哪些其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/79170093/how-to-call-python-ml-model-from-a-java-spring-boot-backend</guid>
      <pubDate>Fri, 08 Nov 2024 12:45:52 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker 端点</title>
      <link>https://stackoverflow.com/questions/79169750/sagemaker-endpoint</link>
      <description><![CDATA[我正在尝试为我的模型创建 sagemaker 端点。我已将包含推理脚本的 docker 文件推送到 aws ECR。我没有使用 model_fn、input_fn、predict_fn 和 output_fn。我只有一个函数，它从 kinesis 视频流中获取实时流，将我保存到 docker 文件的 yolo 模型应用到 docker 文件中，然后将检测保存到 s3。当我创建端点时，它每次都会失败。我该怎么办？
我尝试将那些 model_fn、input_fn、predict_fn 和 output_fn 放入推理脚本中。由于我不需要它们，所以我在每个脚本中都返回了 None，但它不起作用]]></description>
      <guid>https://stackoverflow.com/questions/79169750/sagemaker-endpoint</guid>
      <pubDate>Fri, 08 Nov 2024 11:00:30 GMT</pubDate>
    </item>
    <item>
      <title>ML.NET 时间序列预测特定时间段</title>
      <link>https://stackoverflow.com/questions/79169287/ml-net-time-series-predicting-specific-period-of-time</link>
      <description><![CDATA[我是 ML.NET 的新手，一直在尝试使用该框架来预测我周围的一些数据。
据我发现，我们可以使用时间序列来预测未来的一系列值预测，这些值预测与最后输入数据的日期/时间相邻。
是否可以针对相同的训练数据模型对前一段时间（过去）进行预测？
我浏览了 prediction.Predict() 文档，但找不到任何地方可以提供时间作为参数。
在哪里以及如何对特定时间段进行预测是最佳情况？
请随时纠正我的理解并指出正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79169287/ml-net-time-series-predicting-specific-period-of-time</guid>
      <pubDate>Fri, 08 Nov 2024 08:24:25 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 transformers 在本地加载模型</title>
      <link>https://stackoverflow.com/questions/79169173/fail-to-use-transformers-to-load-model-locally</link>
      <description><![CDATA[我已使用 transformers 函数将整个模型
下载到本地目录：/home/marcus/Desktop/project/OCR_transformer_practices/models/moondream2
代码如下：
from huggingface_hub import snap_download

# 指定模型 ID 和修订版本
model_id = &quot;vikhyatk/moondream2&quot;
revision = &quot;2024-08-26&quot;

# 指定要下载模型的目录
download_directory = &quot;/home/marcus/Desktop/project/OCR_transformer_practices/models/moondream2&quot; # 将其更改为您想要的路径

# 将模型文件下载到指定目录
local_model_path = snapping_download(repo_id=model_id, revision=revision, local_dir=download_directory)

模型保存在目录中：
当我使用以下代码通过 transformers 从本地目录加载模型时：
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import os

# 获取父目录
project_dir = Path(__file__).parent
model_folder_name = &#39;models/moondream2&#39;
model_dir = str(project_dir/model_folder_name)

# 使用正确的模型 ID 加载 tokenizer 和模型
# model_id = &quot;vikhyatk/moondream2&quot;
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained( pretrained_model_name_or_path=model_dir, use_safetensors=True, trust_remote_code=True,)

弹出错误消息：
回溯（最近一次调用最后一次）：
文件“/home/marcus/Desktop/project/OCR_transformer_practices/moondream_test.py”，第 15 行，位于&lt;module&gt;
model = AutoModelForCausalLM.from_pretrained( pretrained_model_name_or_path=model_dir, use_safetensors=True, trust_remote_code=True,)
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;，第 553 行，在 from_pretrained 中
model_class = get_class_from_dynamic_module(
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 552 行，在 get_class_from_dynamic_module 中
return get_class_in_module(class_name, final_module, force_reload=force_download)
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 237 行，在 get_class_in_module 中
module_files: List[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 128 行，在 get_relative_import_files 中
new_imports.extend(get_relative_imports(f))
文件&quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;, line 97, in get_relative_imports
with open(module_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
FileNotFoundError: [Errno 2] 没有这样的文件或目录：&#39;/home/marcus/.cache/huggingface/modules/transformers_modules/moondream2/fourier_features.py&#39;

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79169173/fail-to-use-transformers-to-load-model-locally</guid>
      <pubDate>Fri, 08 Nov 2024 07:38:50 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 ValueError：预期输入 batch_size (49) 与目标 batch_size (64) 匹配</title>
      <link>https://stackoverflow.com/questions/79168959/how-to-fix-valueerror-expected-input-batch-size-49-to-match-target-batch-size</link>
      <description><![CDATA[我一直在修改数据加载器端，但它仍然显示该错误..
这是预处理代码：
预处理：
# 您的代码在这里
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

class ImageDataset(Dataset):
def __init__(self, images, labels, transform=None, target_transform=None):
self.images = images
self.labels = labels
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.images)

def __getitem__(self, idx):
image = self.images[idx]
label = self.labels[idx]

if self.transform:
image = self.transform(image)

if self.target_transform:
label = self.target_transform(label)

return image, label

# 调整数据大小
resize_transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.5,), (0.5,))
])

# 使用 Data Loader，这是一个包装数据集并使用小批量输出每个数据的函数。
train_dataset = ImageDataset(images=x, labels=y, transform=resize_transform)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 打印每个数据（dataloader 中的 x 测试和 y 测试）
对于 train_dataloader 中的图像、标签：
print(images.shape)
print(labels.shape)
break

这是模型：
val_dataset = ImageDataset(x_val, y_val, transform=resize_transform)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

import torch.nn as nn
import torch.nn. functional as F
import torch.optim as optim

class Network(nn.Module):
def __init__(self):
super(Network, self).__init__()
self.conv1 = nn.Conv2d(1, 32, 3)
self.conv2 = nn.Conv2d(32, 64, 3)
self.fc1 = nn.Linear(64 * 16 * 16, 128)
self.fc2 = nn.Linear(128, 64)
self.fc3 = nn.Linear(64, 16) # 针对 16 个类进行调整

self.pool = nn.MaxPool2d(2, 2)

def forward(self, x):
x = self.pool(F.relu(self.conv1(x)))
x = self.pool(F.relu(self.conv2(x)))
x = x.view(-1, 64 * 16 * 16) # 扁平层
x = F.relu(self.fc1(x))
x = F.relu(self.fc2(x))
x = self.fc3(x)
return x

# 定义损失函数和优化器
net = Network()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, influence=0.9)

# 训练模型
num_epochs = 10
for epoch in range(num_epochs):
net.train()
running_loss = 0.0
for i, data in enumerate(train_dataloader, 0):
input, labels = data
optimizer.zero_grad()
output = net(inputs)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
running_loss += loss.item()

print(f&quot;Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader):.4f}&quot;)

# 验证准确率
net.eval()
correct = 0
total = 0
with torch.no_grad():
for input, labels in val_loader:
output = net(inputs)
_, predicted = torch.max(outputs.data, 1)
total += labels.size(0)
correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f&quot;验证准确率：{accuracy:.2f}%&quot;)

print(&quot;完成训练&quot;)


上面提到的错误：ValueError：预期输入 batch_size (49) 与目标 batch_size (64) 匹配。不断出现....
我想知道这里的问题是什么..]]></description>
      <guid>https://stackoverflow.com/questions/79168959/how-to-fix-valueerror-expected-input-batch-size-49-to-match-target-batch-size</guid>
      <pubDate>Fri, 08 Nov 2024 05:55:50 GMT</pubDate>
    </item>
    <item>
      <title>单标签图像分类</title>
      <link>https://stackoverflow.com/questions/79168636/single-label-image-classification</link>
      <description><![CDATA[我正在尝试构建一个单标签图像分类，用于检测损坏/正常的笔记本电脑。我有大约 500 张图像的数据集。我尝试使用 ResNet50 CNN，但效果不佳。
我想了解

我是否需要更多数据，如果需要，需要多少？
如果我必须使用现有的 500 张图像数据集，最好的方法是什么？我应该使用什么？任何详细信息都会有所帮助。

PS。我以前没有研究过 ML 模型，这是我的第一次尝试。]]></description>
      <guid>https://stackoverflow.com/questions/79168636/single-label-image-classification</guid>
      <pubDate>Fri, 08 Nov 2024 02:45:20 GMT</pubDate>
    </item>
    <item>
      <title>神经网络可以训练来识别加密货币市场中的艾略特波浪模式吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79168629/can-neural-networks-be-trained-to-recognize-elliott-wave-patterns-in-the-crypto</link>
      <description><![CDATA[是否有可能训练神经网络来识别加密货币市场数据中已知的艾略特波浪模式？具体来说，神经网络能否从历史价格数据（例如开盘价、最高价、最低价、收盘价、成交量）中发现这些模式并根据它们预测未来趋势？哪些技术或架构对这项任务有效？]]></description>
      <guid>https://stackoverflow.com/questions/79168629/can-neural-networks-be-trained-to-recognize-elliott-wave-patterns-in-the-crypto</guid>
      <pubDate>Fri, 08 Nov 2024 02:41:27 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 Flask 应用程序中的错误 - jinja2.exceptions.TemplateNotFound：index.html</title>
      <link>https://stackoverflow.com/questions/79168341/error-in-machine-learning-flask-app-jinja2-exceptions-templatenotfound-index</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79168341/error-in-machine-learning-flask-app-jinja2-exceptions-templatenotfound-index</guid>
      <pubDate>Thu, 07 Nov 2024 23:03:46 GMT</pubDate>
    </item>
    <item>
      <title>Azure 文档智能自定义模型在提取采购订单字段时失去准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79168102/azure-document-intelligence-custom-model-losing-accuracy-in-extracting-purchase</link>
      <description><![CDATA[我正在使用 Azure Document Intelligence 读取来自不同客户的采购订单并提取特定字段以自动处理订单。我们在 Document Intelligence Studio 中创建了一个自定义模型，定义了自定义字段，并通过在示例文档中标记相关字段来训练该模型。最初，该模型表现良好，以高精度将数据放置在正确的字段中。
问题：
最近，我们注意到准确度有所下降。即使使用我们训练集中的相同示例文档，该模型现在返回的结果也与预期不同，将数据错误地放置在预定义字段中。我们查看了文档和教程，并按照建议添加了更多训练数据，但这并没有解决问题。
我们尝试过的方法：

向训练数据中添加了更多带标签的样本。

问题：

什么可能导致准确度随着时间的推移而下降，特别是考虑到该模型最初运行良好？可能是我们为其提供的不同采购订单测试数据的数量？
Document Intelligence Studio 中是否有最佳实践或配置技巧可以帮助稳定准确度？
Azure Document Intelligence 中是否有我们可能未使用的自定义模型的高级调整选项？
]]></description>
      <guid>https://stackoverflow.com/questions/79168102/azure-document-intelligence-custom-model-losing-accuracy-in-extracting-purchase</guid>
      <pubDate>Thu, 07 Nov 2024 21:15:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 SelfQueryRetriever 获取文档相似度分数 - Langchain</title>
      <link>https://stackoverflow.com/questions/79165072/getting-document-similarity-scores-with-selfqueryretriever-langchain</link>
      <description><![CDATA[我希望能够在使用 SelfQueryRetriever 时获取检索到的文档的相似度分数，如下所示。
我制作了一个自查询检索器，如下所示：
retriever = SelfQueryRetriever.from_llm(
llm = llm,
vectorstore = vectorstore,
document_contents = document_content_description,
metadata_field_info = metadata_field_info,
enable_limit=True, 
search_type = &quot;similarity_score_threshold&quot;,
search_kwargs={&quot;score_threshold&quot;: 0.80, &quot;k&quot;: 5},
verbose=True
)

检索器能够按预期检索文档，但我想以某种方式显示检索到的文档的相似度分数。可以做到这一点吗？如何做到？检索器返回带有 page_content 和元数据键的文档。]]></description>
      <guid>https://stackoverflow.com/questions/79165072/getting-document-similarity-scores-with-selfqueryretriever-langchain</guid>
      <pubDate>Thu, 07 Nov 2024 05:01:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 JAX 训练模型时跟踪测试/验证损失</title>
      <link>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</guid>
      <pubDate>Tue, 05 Nov 2024 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>如何将特征提取层添加到 Tensorflow 中的序列模型？</title>
      <link>https://stackoverflow.com/questions/78071238/how-to-add-feature-extracted-layer-to-a-sequential-model-in-tensorflow</link>
      <description><![CDATA[resnet_50 = &quot;https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/classification/versions/1&quot;
feature_extractor_model = resnet_50
import tensorflow_hub as hub
feature_extractor_layer = hub.KerasLayer(
feature_extractor_model,
input_shape=(224, 224, 3),
trainable=False)
num_classes = len(class_names)
model = tf.keras.Sequential()
model.add(feature_extractor_layer)
model.summary()

我试图将 feature_extractor_layer 添加到 Sequential，但出现以下错误：

ValueError：只有 keras.Layer 的实例可以添加到 Sequential 模型中。已收到：&lt;tensorflow_hub.keras_layer.KerasLayer 对象位于 0x7b6c00794850&gt;（类型为 &lt;class &#39;tensorflow_hub.keras_layer.KerasLayer&#39;&gt;）

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78071238/how-to-add-feature-extracted-layer-to-a-sequential-model-in-tensorflow</guid>
      <pubDate>Tue, 27 Feb 2024 23:56:01 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.decomposition.PCA 的特征向量简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我试图了解主成分分析的工作原理，并在sklearn.datasets.load_iris数据集上对其进行测试。我了解每个步骤的工作原理（例如，标准化数据、协方差、特征分解、按最高特征值排序、使用K个选定维度将原始数据转换为新轴）。
下一步是可视化这些特征向量在数据集上投影的位置（在PC1 vs. PC2 图上，对吗？）。
有人可以解释如何在降维数据集的 3D 图上绘制 [PC1、PC2、PC3] 特征向量吗？
此外，我是否正确绘制了这个 2D 版本？我不确定为什么我的第一个特征向量的长度较短。我应该乘以特征值吗？

以下是我为实现此目标所做的一些研究：
我遵循的 PCA 方法来自：
https://plot.ly/ipython-notebooks/principal-component-analysis/#Shortcut---PCA-in-scikit-learn（虽然我不想使用 plotly。我想坚持使用 pandas、numpy、sklearn、matplotlib、scipy 和 seaborn）
我一直在遵循这个绘制特征向量的教程，它看起来很不错简单：使用 matplotlib 进行 PCA 的基本示例，但我似乎无法用我的数据复制结果。
我发现了这一点，但对于我想做的事情来说，它似乎过于复杂，而且我不想创建一个 FancyArrowPatch：使用 matplotlib 和 np.linalg 绘制协方差矩阵的特征向量

我试图让我的代码尽可能简单，以便遵循其他教程：
导入 numpy 作为 np
导入 pandas 作为 pd
导入 matplotlib.pyplot 作为 plt
从 sklearn.datasets 导入 load_iris
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn 导入 decomposition
导入 seaborn 作为 sns； sns.set_style(&quot;whitegrid&quot;, {&#39;axes.grid&#39; : False})

%matplotlib inline
np.random.seed(0)

# 鸢尾花数据集
DF_data = pd.DataFrame(load_iris().data, 
index = [&quot;iris_%d&quot; % i for i in range(load_iris().data.shape[0])],
columns = load_iris().feature_names)

Se_targets = pd.Series(load_iris().target, 
index = [&quot;iris_%d&quot; % i for i in range(load_iris().data.shape[0])], 
name = &quot;Species&quot;)

# 缩放平均值 = 0, var = 1
DF_standard = pd.DataFrame(StandardScaler().fit_transform(DF_data), 
index = DF_data.index,
columns = DF_data.columns)

# Sklearn 用于主成分分析

# 维度
m = DF_standard.shape[1]
K = 2

# PCA（我倾向于如何设置它）
M_PCA = decomposition.PCA(n_components=m)
DF_PCA = pd.DataFrame(M_PCA.fit_transform(DF_standard), 
columns=[&quot;PC%d&quot; % k for k in range(1,m + 1)]).iloc[:,:K]

# 绘制特征向量
#https://stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

# 这就是事情变得奇怪的地方...
data = DF_standard

mu = data.mean(axis=0)
特征向量，特征值 = M_PCA.components_, M_PCA.explained_variance_ #eigenvectors, eigenvalues, V = np.linalg.svd(data.T, full_matrices=False)
projected_data = DF_PCA #np.dot(data, eigenvectors)

sigma = projected_data.std(axis=0).mean()

fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(projected_data[&quot;PC1&quot;], projected_data[&quot;PC2&quot;])
for axis, color in zip(eigenvectors[:K], [&quot;red&quot;,&quot;green&quot;]):
# start, end = mu, mu + sigma * axis ### 导致 &quot;ValueError: 需要解压的值太多（预期为 2）&quot;

# 所以我尝试了这个但我认为它不正确
start, end = (mu)[:K], (mu + sigma * axis)[:K] 
ax.annotate(&#39;&#39;, xy=end,xytext=start, arrowprops=dict(facecolor=color, width=1.0))

ax.set_aspect(&#39;equal&#39;)
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    </channel>
</rss>