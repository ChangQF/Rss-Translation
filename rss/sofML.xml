<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 20 Feb 2025 15:19:25 GMT</lastBuildDate>
    <item>
      <title>乘数优化器[关闭]</title>
      <link>https://stackoverflow.com/questions/79454118/multiplier-optimiser</link>
      <description><![CDATA[我有一个1000个记录的数据集（可以是n），并且具有12个不同的乘数。每个乘数可以将值从0.1到10.0，以0.1的步骤递增。
对于每个记录，a＆quot&#39;finalsCore&#39;使用所有12个乘数计算。  该分数的计算基于中间步骤（如果需要，可以共享的细节）。 （乘法，除法，魔鬼，BM25L计算）
我的目的是确定记录的12个乘数中每个乘数中的每个乘数的最佳值，以便当我进行“ finalsCore”的AVG时。和标签“ r＆quot”在财产中“相关性”要为那些决赛的文档＆gt; avg得分，以便根据“ r＆quot”确定最大文档。要遵循什么过程，以便我们获得一组最终的乘数组合
例如： - 
所有Initalized乘数M1，M2，M3，M4 ..... M12为0.1 
和
何时
m1 = 2.3，m2 = 5，m3 = 1.0，m4 = 8，m5 = 7.5，m6 = 0，m7 = 8.9，m8 = 4.0，m9 = 2.7，m10 = 10，m10 = 10，m11 = 6.1，m12，m12 = 4.9;我们得到AVG分数，可以说89.4（最大），所有大于89.4的文档都需要标记为“ R＆quot”计数出现780/1000，因此百分比为78％&lt; / p&gt;
请注意： - 保持所有值10.0不会最大化分数，而某些乘数则在分母 /其他规则中。&lt; / p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/79454118/multiplier-optimiser</guid>
      <pubDate>Thu, 20 Feb 2025 10:17:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在每一层的两个模型中结合预测？</title>
      <link>https://stackoverflow.com/questions/79453239/how-to-combine-predictions-from-two-models-at-each-layer</link>
      <description><![CDATA[我使用以下代码保存训练有素和未经训练的模型。
  def skip_block（基础，跳过，过滤器）：
x = concatenate（）（\ [base，skip \]）
x = conv2d（滤波器，（2，2），padding =; same＆quort; activation =; relu＆quot; quot; quot＆quot;）（x）
x = conv2d（滤波器，（2，2），padding =; same＆quort; activation =; relu＆quot; quot; quot＆quot;）（x）
返回x

def build_model（img_size）：
inp = input（shape =（img_size，img_size，3），name =＆quot; image_input＆quot; quot;）

    ＃负载预训练模型
    encoder = mobilenetv2（input_tensor = inp，weights =; imagenet; incepped_top = false，alpha = 0.5）
    
    对于encoder.layers中的图层：
        layer.trainable = true
    
    skips = [&#39;image_input；
    x = encoder.get_layer（&#39;block_13_expand_relu＆quort; quote＆quort&#39;）。输出
    
    对于我，zip中的过滤器（反向（跳过），[256，128，64，32]）：
        x = upsmpling2d（（2，2））（x）
        x = merge_skip_block（x，encoder.get_layer（i）.output，过滤器）
    
    out = conv2d（num_masks，（1，1），激活=; sigmoid＆quot; quot;）（x）
    
    返回模型（INP，OUT）
 
我想将来自训练的模型的每一层的预测与未经训练模型中同一层的输入相结合，以生成改进的组合预测。然后，该更新的输出应传递到未经训练的模型中的下一层。
到目前为止，我已经尝试了以下内容，我不确定如何继续下一步。
  def create_combined_model（image_size）：

    input_layer = input（shape =（image_size，image_size，3），name =＆quort; input_image＆quort; quot; quot）

    ＃加载两个模型
    Trained_model = load_model（&#39;路径/to/trained_model.keras&#39;）
    UNTREADIAN_MODEL = LOAD_MODEL（&#39;PATH/TO/UNTRAIND TRAINADIAN_MODEL.KERAS&#39;）
    
    ＃创建中间模型以获取图层输出/输入
    Trained_features =模型（
        输入= trained_model.input，
        输出= [layer.output for theared_model.layers中的图层[1：]]
    ）
    
    UNTRAIND_FEATURES =模型（
        输入= UNTRAIND_MODEL.INPUT，
        outputs = [layer.Input for Layer in untained_model.layers [1：]]
    ）
    
    ＃使用相同的输入获取所有功能
    Trained_outputs = Trained_features（input_layer）
    UNTRAIND_INPUTS = UNTRAIND_FEATURES（input_layer）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79453239/how-to-combine-predictions-from-two-models-at-each-layer</guid>
      <pubDate>Thu, 20 Feb 2025 02:57:34 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle笔记本上的TPU配置</title>
      <link>https://stackoverflow.com/questions/79453184/tpu-configuration-on-kaggle-notebook</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79453184/tpu-configuration-on-kaggle-notebook</guid>
      <pubDate>Thu, 20 Feb 2025 02:09:57 GMT</pubDate>
    </item>
    <item>
      <title>Google Colab太慢而无法选择功能 - 如何优化性能？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79452905/google-colab-is-too-slow-for-feature-selection-how-to-optimize-performance</link>
      <description><![CDATA[在训练机器学习模型之前，我正在使用Google Colab在大数据集上执行功能选择。但是，运行时非常慢，执行时间需要很长的时间才能返回结果。
我尝试的是：

将运行时类型更改为T4 GPU，但注意到RAM的使用显示为0/15GB，这似乎是不寻常的。
检查了数据集大小（〜[提及大小]行和列）。
用于编码，标准标准（）和lasso（）用于功能选择的使用pd.get_dummies（）。
尝试通过转换数据类型来减少内存使用。

问题：

即使对于基本操作（例如缩放和编码），执行缓慢。
特征选择（拉索回归）的时间太长。
不确定Colab的资源是否不够，或者我的方法是否效率低下。

问题：

如何优化Colab中的特征选择和预处理？
是否有更快的特征选择方法？
我如何有效地管理记忆使用量以防止慢速性能？
 T4 GPU运行时适合此，还是我应该使用其他设置？
]]></description>
      <guid>https://stackoverflow.com/questions/79452905/google-colab-is-too-slow-for-feature-selection-how-to-optimize-performance</guid>
      <pubDate>Wed, 19 Feb 2025 22:12:54 GMT</pubDate>
    </item>
    <item>
      <title>当MLFlow模型被``@Champion''更改时，如何自动更新SageMaker端点？</title>
      <link>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</link>
      <description><![CDATA[我在AWS SageMaker AI上使用托管MLFLOW服务器（ https://www.youtube.com/watch?v=3xkz_5hop6k&amp;ab_channel = awsevents ）以跟踪实验和模型版本。我们的数据科学团队通过用冠军别名标记最佳版本来促进生产模型。这些模型部署到了萨吉人端点，然后通过API网关访问。
我想实现自动化管道，每当将新型号分配给MLFlow中的冠军别名时，该管道都会更新SageMaker端点。目前，我正在考虑将针对别名更改进行轮询的Lambda功能，但我正在寻找更高效或更有托管的解决方案。是否有人实现了动态端点更新机制或可以提出替代方法？]]></description>
      <guid>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</guid>
      <pubDate>Wed, 19 Feb 2025 18:00:58 GMT</pubDate>
    </item>
    <item>
      <title>将Python ML模型与Flutter客户端集成</title>
      <link>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</link>
      <description><![CDATA[我在工作中面临挑战，我需要在我的客户端应用程序上运行许多Python ML模型，因为使某些模型在服务器上运行。。
除了项目资产中的张量流光模型和实施Python模型的同事告诉我，他不能以Tflite模型导出某些模型。，我没有实验。
有一个软件包（OnnxRuntime）使用ONNX型号，并在我的flutter代码中使用了其中的功能，例如Dart FFI，我以前使用此软件包在我的颤音代码中运行C ++功能，并且可以很好地运行。我的牛头人说，我有同样的问题，他不能将所有模型提取到ONNX模型中，但这让我认为有一种方法可以在我的Flutter应用程序中使用Python代码，例如Dart FFI，我知道它不会一样，因为Python是一种解释语言，我无法从中脱离共享对象，所以我的问题是：是否有一种方法可以在我的客户端应用程序或Python ML模型中使用Python代码，而无需使用TFLITE或OnnxRuntime？]]></description>
      <guid>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</guid>
      <pubDate>Wed, 19 Feb 2025 16:13:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的GPT-2小型模型的响应不一致且重复性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</link>
      <description><![CDATA[ i在1100个JSON文件的数据集上微调的GPT-2（小），分为：

 400 Q＆amp;一对
 700食谱数据（标题，成分，说明等）

该模型对基于食谱的提示有些响应，例如“如何制作芝士蛋糕”生成准确的成分和方向（即使我认为这只是重复培训数据）。但是，当被问及诸如“您好，您好吗？”之类的一般性问题时，它会产生荒谬的答案，例如提到没有上下文的素食主义者。
这是培训进度：

时期1：训练损失：没有日志|验证损失：1.882118 
时期2：训练损失：2.690300 |验证损失：1.865422 
时期3：训练损失：2.690300 |验证损失：1.844494 
时期4：训练损失：2.638200 |验证损失：1.806402 

我怀疑：

模型过于适应结构化数据。
 gpt-2 Small可能不适合此类任务。
]]></description>
      <guid>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</guid>
      <pubDate>Wed, 19 Feb 2025 14:56:34 GMT</pubDate>
    </item>
    <item>
      <title>将偏航和俯仰角映射到屏幕坐标以进行凝视跟踪[封闭]</title>
      <link>https://stackoverflow.com/questions/79450899/mapping-yaw-and-pitch-angles-to-screen-coordinates-for-gaze-tracking</link>
      <description><![CDATA[我正在处理一个凝视跟踪系统，在该系统中，我需要将偏航（θ）和螺距（φ）角（从深度学习模型获得）转换为屏幕坐标（𝑥，𝑦）。尽管尝试了各种公式，但我仍在努力获得准确的结果。
以前，我尝试使用三角学，直接角度到屏幕映射和经验缩放尝试类似的方法。但是，诸如非线性，深度估计和用户之间的变化之类的问题持续存在。我正在寻找一种更强大的转换方法。]]></description>
      <guid>https://stackoverflow.com/questions/79450899/mapping-yaw-and-pitch-angles-to-screen-coordinates-for-gaze-tracking</guid>
      <pubDate>Wed, 19 Feb 2025 09:57:59 GMT</pubDate>
    </item>
    <item>
      <title>我是否将神经网络锁定错误？预测有时似乎是随机的</title>
      <link>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</guid>
      <pubDate>Wed, 19 Feb 2025 08:47:17 GMT</pubDate>
    </item>
    <item>
      <title>D2L包装在Google Colab上安装</title>
      <link>https://stackoverflow.com/questions/76248695/d2l-package-installation-on-google-colab</link>
      <description><![CDATA[我需要用插入D2L包装
  PIP安装D2L == 1.0.0B0
 
但是，每当我尝试将其下载到Google Colab时，都会发生以下错误：
 在索引中查看：https：//pypi.org/simple，https://us-python.pkg.dev/colab-wheels/public/simple/
收集D2L == 1.0.0-Beta0
  使用缓存的D2L-1.0.0.0.0.0.0b0-py3-none.whl（141 kb）
收集Jupyter（来自D2L == 1.0.0-Beta0）
  使用缓存的jupyter-1.0.0-py2.py3-none-any.whl（2.7 kb）
需求已经满足：numpy in/usr/local/lib/python3.10/dist-packages（来自d2l == 1.0.0-beta0）（1.22.4）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的matplotlib（来自d2l == 1.0.0-beta0）（3.7.1）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的matplotlib-inline（来自d2l == 1.0.0-beta0）（0.1.6）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的请求（来自d2l == 1.0.0-beta0）（2.27.1）
已经满足的要求：/usr/local/lib/python3.10/dist-packages（来自d2l == 1.0.0-beta0）（1.5.3）（1.5.3）
收集健身房== 0.21.0（来自d2l == 1.0.0-beta0）
  使用Cached Gym-0.21.0.tar.gz（1.5 MB）
  错误：子进程 - 纠正
  
  ×python setup.py egg_info并未成功运行。
  │退出代码：1
  ╰ - ＆gt;有关输出，请参见上文。
  
  注意：此错误源自子过程，可能不是PIP的问题。
  准备元数据（setup.py）...错误
错误：元数据生成失败

×生成软件包元数据时遇到错误。
╰ - ＆gt;有关输出，请参见上文。

注意：这是上面提到的软件包的问题，​​而不是PIP。
提示：有关详细信息，请参见上文。
 
如何在Google Colab上安装D2L软件包？]]></description>
      <guid>https://stackoverflow.com/questions/76248695/d2l-package-installation-on-google-colab</guid>
      <pubDate>Sun, 14 May 2023 16:59:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用现有的ML工具将人类可读的时间表转换为表？</title>
      <link>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</link>
      <description><![CDATA[我有我的美国原住民部落制作的报纸的时间表。我试图使用 aws textract 从此产生某种表格。 AWS textract在这方面没有识别任何表。因此，我认为这不会起作用（如果我付款，也许可能会发生更多的事情，但这并不是这样）。。
最终，我试图筛选所有存档的报纸，并下载所有选举周期的所有时间表（“一般性”一般性“和“特殊咨询”），以查找时间表中每个项目之间的天数。 
由于这一切都在公共领域，所以我没有理由在这里粘贴桌子的图片。我还将包括文档的下载URL。 “ https://i.sstatic.net/cgbv8.png”/&gt;  
下载url：下载 
我首先在各个文档上使用Foxit读取器在Windows上找到时间表。
然后，我在ubuntu上使用了工具&#39;ocrmypdf&#39;来确保所有这些文档都是可搜索的（ocrmypdf -skip-text通知_of_of_special_election_2023.pdf.pdf.pdf./output/notce/notice_of_special_election_election_election_2023.pdf）。
然后，我碰巧在我的Google Newsfeed中看到了AWS Sextract的广告。看到它有多强大。但是当我尝试过时，它实际上并没有找到这些可读的时间表。
我希望想知道是否有任何ML工具甚至其他解决方案存在此类问题。
我是在试图使我的技术诀窍达到标准。最近两年我生病了，这是一个有趣的问题，我认为我认为很流行。]]></description>
      <guid>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</guid>
      <pubDate>Thu, 16 Feb 2023 16:16:27 GMT</pubDate>
    </item>
    <item>
      <title>破烂的张量作为LSTM的输入</title>
      <link>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</link>
      <description><![CDATA[学习破烂的张量以及如何将它们与张量流一起使用。
我的示例
  xx = tf.ragged.constant（[[
                        [0.1，0.2]，
                        [0.4、0.7、0.5、0.6]
                        ）））
yy = np.Array（[[[0，0，1]，[1,0,0]]）

mdl = tf.keras.Sequeential（[
    tf.keras.layers.inputlayer（input_shape = [none]，batch_size = 2，dtype = tf.float32，ragged = true），
    tf.keras.layers.lstm（64），  
    tf.keras.layers.dense（3，激活=&#39;softmax&#39;）
）））

mdl.compile（loss = tf.keras.losses.sparsecategoricalcrossentropy（from_logits = true），
              优化器= tf.keras.optimizers.adam（1E-4），
              指标= [&#39;准确性&#39;]）

mdl.summary（）
历史= mdl.fit（xx，yy，epochs = 10）
 
错误
  lastm_152层的输入0与图层不兼容：预期ndim = 3，找到ndim = 2。收到完整的形状：[2，无]
 
我不确定是否可以使用类破烂的张量。我发现的所有示例都在LSTM之前嵌入层，但是我不想创建其他嵌入层。 ]]></description>
      <guid>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</guid>
      <pubDate>Tue, 26 May 2020 21:25:12 GMT</pubDate>
    </item>
    <item>
      <title>将ONNX模型转换为Keras</title>
      <link>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</link>
      <description><![CDATA[我尝试将ONNX模型转换为keras，但是当我调用转换函数时，我会收到以下错误消息“ typeError：typeError：nondable类型：&#39;google.protobuf.pyext._message._message.repeatedscalarcontainer&#39; &gt; 
 ONNX模型输入：Input_1 
您可以在此处看到ONNX模型： https://ibb.co/sknbxwy  
 导入onnx2keras
来自onnx2keras导入onnx_to_keras
进口keras
导入ONNX

onnx_model = onnx.load（&#39;onnxmodel.onnx&#39;）
k_model = onnx_to_keras（onnx_model，[&#39;input_1&#39;]）

keras.models.save_model（k_model，&#39;kerasmodel.h5&#39;，overwrite = true，include_optimizer = true）

 
 文件“ c：/../ onnx2keras.py”，第7行，in＆lt; module＆gt;
    k_model = onnx_to_keras（onnx_model，[&#39;input_1&#39;]）
  文件“ .. \ site-packages \ onnx2keras \ converter.py”，第80行，在onnx_to_keras中
    weights [onnx_extracted_weights_name] = numpy_helper.to_array（onnx_w）
TypeError：不可用的类型：&#39;google.protobuf.pyext._message.repeatededscalarcontainer&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</guid>
      <pubDate>Tue, 15 Oct 2019 13:15:06 GMT</pubDate>
    </item>
    <item>
      <title>计算给定神经网络的拖鞋数量？</title>
      <link>https://stackoverflow.com/questions/55831235/calculating-the-number-of-flops-for-a-given-neural-network</link>
      <description><![CDATA[我有一个用Keras编写的用于图像分类的神经网络（Alexnet或VGG16），我想计算网络的浮点操作数量。数据集中图像的大小可能会有所不同。
可以用Python编写广义代码，该代码可以自动计算拖鞋？有库可用吗？
我正在与Spyderanaconda合作，而定义的网络是一个顺序模型。]]></description>
      <guid>https://stackoverflow.com/questions/55831235/calculating-the-number-of-flops-for-a-given-neural-network</guid>
      <pubDate>Wed, 24 Apr 2019 13:28:32 GMT</pubDate>
    </item>
    <item>
      <title>在不使用任何numpy或sklearn库的情况下手动计算AUC</title>
      <link>https://stackoverflow.com/questions/53603376/calculate-auc-manually-without-using-any-numpy-or-sklearn-library</link>
      <description><![CDATA[我给出了一组x，y坐标，我需要使用梯形公式找到AUC，而无需使用任何numpy或sklearn库。 
 （x0，y0）始终是（0,0）
（xn，yn）总是（1,1）
 
下图
    
不使用任何Sklearn库，我知道我需要在下面找到
  hi =？
wi =？
auc = sum（hi * wi）
 
现在我不确定如何找到HI，WI。我认为我没有所有必要的数据来进行高中数学。我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/53603376/calculate-auc-manually-without-using-any-numpy-or-sklearn-library</guid>
      <pubDate>Mon, 03 Dec 2018 23:13:35 GMT</pubDate>
    </item>
    </channel>
</rss>