<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 13 Jul 2024 21:14:14 GMT</lastBuildDate>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我是这个领域的新手，所以也许我误解了一些东西。但是，我认为您可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中对 NN 的输出求导</title>
      <link>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</link>
      <description><![CDATA[我正在尝试使用 PyTorch 中的 NN 构建 ODE 求解器，模型的一部分涉及对模型输出相对于输入求导。
我研究过求解标量值函数的情况。在这种情况下，我的 NN 有 1 个输入节点和 1 个输出节点。根据一些论文，我使用
dy_dt = torch.autograd.grad(y,t, torch.ones_like(y), create_graph=True)[0]
来获得有效的梯度。因此，当我有一个形状为 [N,1] 的张量的训练集时，我得到的结果导数具有形状 [N,1]，这是有道理的。但是，当我尝试求解平面方程时遇到了问题。现在我的 NN 有 1 个输入节点和 2 个输出节点。当我输入一个形状为 [N,1] 的张量时，我从 NN 中得到了一个形状为 [N,2] 的张量，这是有道理的。然而，得到的 dy_dt 的形状为 [N,1]，而它应该是 [N,2]。我还尝试使用
dy_dt = torch.autograd. functional.jacobian(model, t)
它返回一个形状为 [N,2,N,1] 的张量，我认为可以从中访问正确的导数，尽管它不那么简单。我想知道是否有办法使用 autograd.grad 来获得正确的导数。也许它与 grad_output 参数有关？]]></description>
      <guid>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</guid>
      <pubDate>Sat, 13 Jul 2024 16:01:15 GMT</pubDate>
    </item>
    <item>
      <title>无法在 databricks 中运行 Pysparkling</title>
      <link>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</link>
      <description><![CDATA[!pip install h2o_pysparkling_3.5
from pysparkling import H2OConf,H2OContext
hc = H2OContext.getOrCreate()

我收到以下错误
IllegalArgumentException：不支持的参数：（spark.speculation，true）

我尝试了 spark.conf.set(&quot;spark.speculation&quot;, &quot;false&quot;)，但出现了以下错误
[CANNOT_MODIFY_CONFIG] 无法修改 Spark 配置的值：
&quot;spark.speculation&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</guid>
      <pubDate>Sat, 13 Jul 2024 14:02:12 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在 winform 应用程序中使用 YoloV8 .Net Framework 4.7？</title>
      <link>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</link>
      <description><![CDATA[我有一个基于 .net framework 4.7 的 c# winform 项目。
我想知道是否有办法在 winform 应用程序（.net framework 4.7）中使用 Yolov8。
每当我将引用添加到我的项目时，我都会收到以下错误：

YoloV8 使用 system.runtime 版本 6.0，高于 system.runtime 4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</guid>
      <pubDate>Sat, 13 Jul 2024 07:33:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用标签编码的情况下对 RandomForest 的非序数分类变量进行编码？</title>
      <link>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</guid>
      <pubDate>Fri, 12 Jul 2024 21:11:51 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 指标显示“TypeError：‘property’对象不可迭代”</title>
      <link>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</link>
      <description><![CDATA[我正在建立一个 ANN 模型。当我运行以下代码时，它显示为
TypeError: &#39;property&#39; 对象不可迭代

如何修复此问题？
代码：
model=Sequential()
model.add(Dense(512,activation=tf.nn.relu))
model.add(Dense(256,activation=tf.nn.tanh))
model.add(Dense(128,activation=tf.nn.relu))
model.add(Dense(7))

# # 拟合模型

loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
accuracy=tf.keras.metrics.SparseCategoricalAccuracy
optimizer=tf.keras.optimizers.Adam()

model.compile(loss=loss,优化器=优化器，指标=[准确率])
history=model.fit(xtrain, ytrain, validation_data=(xval, yval), batch_size=64, epochs=100)
]]></description>
      <guid>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</guid>
      <pubDate>Fri, 12 Jul 2024 18:44:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>通过向 CNN 输入添加位置和字符信息来增强文档布局分析</title>
      <link>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</link>
      <description><![CDATA[我正在研究文档布局分析，并一直在探索 CNN 和基于 Transformer 的网络来完成这项任务。通常，图像作为 3 通道 RGB 输入传递给这些网络。但是，我的数据源是 PDF 格式，我可以直接从中提取准确的位置和字符信息。
我担心将这些 PDF 数据转换为图像进行分析会导致宝贵的位置和字符信息丢失。我的想法是将 CNN 的输入维度从标准的 3 RGB 通道修改为包含这些额外位置和字符信息的更高维度输入。
我了解 CNN 的工作原理，并高度怀疑这种方法可能行不通，但我很感谢社区的任何反馈或建议。有没有人尝试过以这种方式增强输入通道，或者有没有人对将位置和字符数据直接集成到 CNN 中有什么见解？]]></description>
      <guid>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</guid>
      <pubDate>Fri, 12 Jul 2024 10:17:29 GMT</pubDate>
    </item>
    <item>
      <title>如何并行化yolov5的Darknet53的卷积层（在一台电脑上）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78739028/how-to-parallelize-the-convolutional-layers-of-darknet53-of-yolov5-on-one-pc</link>
      <description><![CDATA[我想把YOLOv5的主干算法拆分，部署在多块FPGA上，但是首先需要在一台电脑上把Darknet53卷积层划分成2个或多个区域，进行并行卷积操作。YOLOv5项目要学习什么？需要修改哪些文件？一堆YAML和common.py好混乱啊（YOLOVv5是最新版本）。
我已经在PC上部署了YOLOv5，也做了一些前期的研究，大部分的重点应该在models文件夹，models文件夹里有5个YAML网络配置文件，yolo.py，common.py等，现在想搞清楚研究的顺序，以及如何学习这些文件，在一台电脑上把Darknet53卷积层划分成2个或多个区域，进行并行卷积操作。]]></description>
      <guid>https://stackoverflow.com/questions/78739028/how-to-parallelize-the-convolutional-layers-of-darknet53-of-yolov5-on-one-pc</guid>
      <pubDate>Fri, 12 Jul 2024 07:10:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使 adapter_conditioning_scale 在多个 T2I_Adapter 中可训练？[关闭]</title>
      <link>https://stackoverflow.com/questions/78738957/how-to-make-adapter-conditioning-scale-trainable-in-multiple-t2i-adapter</link>
      <description><![CDATA[下面是使用 Multi T2I_Adapter 的代码。如以下代码所示，adapter_conditioning_scale=[0.8, 0.8]，是手动设置的。
adapters = MultiAdapter(
[
T2IAdapter.from_pretrained(&quot;TencentARC/t2iadapter_keypose_sd14v1&quot;),
T2IAdapter.from_pretrained(&quot;TencentARC/t2iadapter_depth_sd14v1&quot;),
]
)
adapters = adapters.to(torch.float16)

pipe = StableDiffusionAdapterPipeline.from_pretrained(
&quot;CompVis/stable-diffusion-v1-4&quot;,
torch_dtype=torch.float16,
adapter=adapters,
).to(&quot;cuda&quot;)

image = pipe(prompt, cond, adapter_conditioning_scale=[0.8, 0.8]).images[0]
make_image_grid([cond_keypose, cond_depth, image], rows=1, cols=3)

Google 的 Colab
我们可以使用哪种 ML 技术来找到 adapter_conditioning_scale 的最佳值？
参考文献：
huggingface.co
T2IAdapter 代码]]></description>
      <guid>https://stackoverflow.com/questions/78738957/how-to-make-adapter-conditioning-scale-trainable-in-multiple-t2i-adapter</guid>
      <pubDate>Fri, 12 Jul 2024 06:45:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 识别代码文件中的错误来源[关闭]</title>
      <link>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</link>
      <description><![CDATA[我正在尝试实现一个小工具，它可以自动识别一组代码文件（作为输入）中的哪一部分代码导致了执行期间显示的错误文本。
错误可能是语法错误，也可能是逻辑错误。我还在考虑利用 llms 的 api 调用来更正代码。
据我所知，RAG 是必要的，因为我不可能将所有代码文件的数据都放入提示中，因为它肯定会超出上下文窗口的大小。
哪种类型的 RAG 实现最有用？我想尽可能减少响应延迟？]]></description>
      <guid>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</guid>
      <pubDate>Fri, 12 Jul 2024 06:41:48 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
以下函数的解释：

oscillator_system_data_loss：振荡器系统作为神经网络的实现，其中可学习参数 mu 传递给在 NN_osc_func 中实现的 ODE
train_NN_data_loss：自定义训练程序
plot_epochs_with_noise：与问题无关，但用于训练时监控

def rocksock_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

使用 tf.GradientTape() 作为 outer_tape:
outer_tape.watch(t)

使用 tf.GradientTape() 作为 inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 一阶导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

early_stop = 0

for itr in范围（epochs）：
使用 tf.GradientTape() 作为磁带：
train_loss，mu = 振荡器系统数据损失（train_t，NN，func，params，mu，bc，data_t，data_u_noised，lambda1）
train_loss_record.append（train_loss）

grad_w = 磁带。gradient（train_loss，NN.trainable_variables + [mu]）
optm.apply_gradients（zip（grad_w，NN.trainable_variables + [mu]））

如果 itr 在 loss_tracker 中：
print（train_loss.numpy()）
print（mu.numpy()）
plot_epochs_with_noise（train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN）

mu_list.append（mu.numpy()）

return train_loss_record, mu_list, early_stop

NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。有什么帮助吗？
我的想法是，要么训练更长时间，要么我可能会遇到 PINN 容易出现的一些高频问题。]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Textract 中获取 BLOCK 类型 LAYOUT_TITLE、LAYOUT_SECTION_HEADER 和 LAYOUT_xx 的内容</title>
      <link>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</link>
      <description><![CDATA[我正在尝试使用 textract 抓取多页 pdf。
需要抓取 pdf 并根据其部分、子部分、表格格式化为 json。
在尝试使用 LAYOUT 和 Table 进行 UI 演示时，它能够准确显示布局标题、布局部分、布局文本、布局页脚、页码
在从 UI 演示下载的 csv 文件中可以观察到相同的信息：layout.csv 文件。
在 json 文件中也是如此：analyzeDocResponse.json 也一样，但它包含所有内容（LINES、WORDS、LAYOUT_TITLE 和所有与布局相关的数据），我认为 textract 按顺序执行所有类型的块类型。
出于调试目的，我使用以下代码打印整个块字典。
以及块类型，后面跟着相应的文本。
如果对 pdf 文件感兴趣：其药物的 SmPC：SmPC 文件
代码 1：以 json 格式打印每个块。

def start_textract_job(bucket, document):
response = textract.start_document_analysis(
DocumentLocation={
&#39;S3Object&#39;: {
&#39;Bucket&#39;: bucket,
&#39;Name&#39;: document
}
},
FeatureTypes=[&quot;LAYOUT&quot;] # 您可以根据需要调整 FeatureTypes
)
return response[&#39;JobId&#39;]

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(json.dumps(block, indent=4))

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

它根据 UI Demo 打印类似信息，块类型 LINES、WORDS、LAYOUT_
但如果我尝试使用以下代码打印每种块类型的文本，它无法打印与 LAYOUT_ 相关的文本，不知道为什么，我是否遗漏了什么？
代码 2：打印块类型，然后打印其内容。

def start_textract_job 与上面的 LAYOUT 相同。

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(f&quot;{block[&#39;BlockType&#39;]}: {block.get(&#39;Text&#39;, &#39;&#39;)}&quot;)

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

我可以看到块类型 LINES、WORDS 的值
但 LAYOUT 为空，如下所示，我认为，它在块类型中识别，但不是其值。
LAYOUT_TITLE:
LAYOUT_FIGURE:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_PAGE_NUMBER:
LAYOUT_FOOTER:
任何帮助都非常感谢，我查阅了文档和其他一些 StackOverflow 问题，但找不到任何帮助。
Tetract 新手，抱歉，如果是新手，请提问：)]]></description>
      <guid>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</guid>
      <pubDate>Sun, 31 Mar 2024 19:28:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 NLP Python 对文本进行多分类 - 总类别中 2 个类别的召回率相对较低</title>
      <link>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</link>
      <description><![CDATA[我拥有几乎平衡的数据集，包含 9 个独特类别，每个类别有近 2200 行，差异为 +/-100 行。为了创建模型，我使用了下面提到的 URL 方法，但在每种情况下，我的模型准确率都在 58% 左右，精确率/召回率也在 54% 左右。你能告诉我我做错了什么吗？
https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f
https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a
https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538
我的数据集只有 2 列，1 列为特征，1 列为标签。
from pandas import ExcelFile

df = pd.read_excel(&#39;Prediction.xlsx&#39;, 
sheet_name=&#39;Sheet1&#39;)
df.head()
BAD_SYMBOLS_RE = re.compile(&#39;[^0-9a-z #+_]&#39;)
STOPWORDS = set(stopwords.words(&#39;english&#39;))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
&quot;&quot;&quot;
text: 字符串

return: 修改后的初始字符串
&quot;&quot;&quot;
text = BeautifulSoup(text, &quot;html.parser&quot;).text # HTML 解码
text = text.lower() # 小写文本
text = REPLACE_BY_SPACE_RE.sub(&#39; &#39;, text) # 将文本中的 REPLACE_BY_SPACE_RE 符号替换为空格
text = BAD_SYMBOLS_RE.sub(&#39;&#39;, text) # 从文本中删除 BAD_SYMBOLS_RE 中的符号
text = &#39; &#39;.join(word for word in text.split() if word not in STOPWORDS) # 从文本中删除停用词
return text

df[&#39;notes_issuedesc&#39;] = df[&#39;notes_issuedesc&#39;].apply(clean_text)
print_plot(10)
df[&#39;notes_issuedesc&#39;].apply(lambda x: len(x.split(&#39; &#39;))).sum()
X = df.notes_issuedesc
y = df.final
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)
%%time
从 sklearn.naive_bayes 导入 MultinomialNB
从 sklearn.pipeline 导入 Pipeline
从 sklearn.feature_extraction.text 导入 TfidfTransformer

nb = Pipeline([(&#39;vect&#39;, CountVectorizer()),
(&#39;tfidf&#39;, TfidfTransformer()),
(&#39;clf&#39;, MultinomialNB()),
])
nb.fit(X_train, y_train)

来自 sklearn.metrics 导入分类报告
y_pred = nb.predict(X_test)

print(&#39;准确率 %s&#39; % 准确率得分(y_pred, y_test))
print(分类报告(y_test, y_pred,target_names=my_tags))
]]></description>
      <guid>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</guid>
      <pubDate>Fri, 17 Apr 2020 20:12:37 GMT</pubDate>
    </item>
    <item>
      <title>打印张量的所有内容</title>
      <link>https://stackoverflow.com/questions/52673610/printing-all-the-contents-of-a-tensor</link>
      <description><![CDATA[我偶然发现了这个 PyTorch 教程（在 neuron_networks_tutorial.py 中），其中他们构建了一个简单的神经网络并运行推理。我想打印整个输入张量的内容以进行调试。当我尝试打印张量时，我得到的是类似这样的结果，而不是整个张量：

我看到了类似的 numpy 链接，但不确定哪个适用于 PyTorch。我可以将其转换为 numpy 并可能查看它，但我想避免额外的开销。有没有办法打印整个张量？]]></description>
      <guid>https://stackoverflow.com/questions/52673610/printing-all-the-contents-of-a-tensor</guid>
      <pubDate>Fri, 05 Oct 2018 21:41:40 GMT</pubDate>
    </item>
    </channel>
</rss>