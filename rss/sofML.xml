<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 17 Apr 2024 03:14:27 GMT</lastBuildDate>
    <item>
      <title>小时间序列数据集中的异常值检测[关闭]</title>
      <link>https://stackoverflow.com/questions/78337873/outliers-detection-in-a-small-time-series-dataset</link>
      <description><![CDATA[我在一个月内从 400 个网络外围设备获取了数据，每 15 分钟记录一次。我的目标是单独识别每天的异常情况。鉴于数据集相对较小（每天 96 个点）
哪种机器学习方法最适合获得最佳结果？]]></description>
      <guid>https://stackoverflow.com/questions/78337873/outliers-detection-in-a-small-time-series-dataset</guid>
      <pubDate>Wed, 17 Apr 2024 01:00:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么在测试阶段没有出现 F1 分数最高的班级？</title>
      <link>https://stackoverflow.com/questions/78337814/why-do-classes-with-the-highest-f1-scores-not-appear-during-testing-phase</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78337814/why-do-classes-with-the-highest-f1-scores-not-appear-during-testing-phase</guid>
      <pubDate>Wed, 17 Apr 2024 00:28:10 GMT</pubDate>
    </item>
    <item>
      <title>请求 DFS、PMI 和成对约束实施方面的帮助 (Python) [已关闭]</title>
      <link>https://stackoverflow.com/questions/78337813/request-for-assistance-with-dfs-pmi-and-pairwise-constraints-implementation-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78337813/request-for-assistance-with-dfs-pmi-and-pairwise-constraints-implementation-p</guid>
      <pubDate>Wed, 17 Apr 2024 00:27:43 GMT</pubDate>
    </item>
    <item>
      <title>边界框检测模型：单图像输入的预测不准确</title>
      <link>https://stackoverflow.com/questions/78337404/bounding-box-detection-model-inaccurate-predictions-with-single-image-inputs</link>
      <description><![CDATA[在使用单个图像输入进行预测时，我的边界框检测模型遇到了令人费解的问题。场景如下：我使用迁移学习以 VGG16 作为基础架构训练了一个模型。在训练过程中，我使用了一批图像，当提供一批图像进行推理时，模型表现良好。但是，当我尝试使用单个图像预测边界框坐标时，模型始终产生不准确的结果，返回诸如 [0., 0., 1., 0.] 之类的预测。
学习数据实际上非常简单：带有随机创建的白框的黑色图像。该模型的想法是预测这些框的坐标。
为了解决此问题，我尝试重新调整输入数据、调整模型架构并检查推理代码，但问题仍然存在。这是令人困惑的，因为当我传递一批 64 个黑色图像（但仅修改第一个图像以创建白色矩形）时，模型预测准确，但对于单个图像则失败。
我怀疑模型在训练和推理过程中处理输入数据的方式可能存在差异，或者单个图像和批次之间可能存在不同的预处理步骤。我还考虑了模型层如何根据批量大小处理输入数据的潜在差异。
尽管解决方法效果很好（传递 (64, 100, 100, 3) 输入形状作为输入），但我想真正理解这一点，因为我想学习的不仅仅是复制粘贴。
项目代码可以在此 Kaggle 笔记本中找到，这是高级计算机视觉 Udemy 课程的第一部分。
我正在向社区寻求有关此问题的潜在原因的见解以及解决该问题的建议。任何帮助或指导将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78337404/bounding-box-detection-model-inaccurate-predictions-with-single-image-inputs</guid>
      <pubDate>Tue, 16 Apr 2024 21:31:11 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ppo 加快 python 国际象棋机器人的训练时间？</title>
      <link>https://stackoverflow.com/questions/78337397/how-can-i-speed-up-my-training-time-for-a-python-chess-bot-using-ppo</link>
      <description><![CDATA[我正在尝试构建一个使用近端策略优化进行学习的国际象棋机器人。我目前正在使用 python-chess 库 (https://python-chess. readthedocs.io/en/latest/index.html#）作为我的代理与自己进行游戏并学习的环境。我面临的问题是训练游戏速度非常慢。每场比赛的移动限制为 200 次，我的机器人可以在大约 1 秒内与自己进行一场比赛。这 1 秒还包括训练的 PPO 部分，使用 GPU 平均需要 0.01 秒。
我正在使用 PyTorch，因此我已经将所有张量移至 GPU。除此之外我还没有找到任何其他方法来加快执行时间。
我希望将玩游戏的执​​行时间减少到每场游戏 0.5 秒或更少，但我一直无法找到实现此目标的方法。
如果有人知道可能的解决方案，我将非常感谢您的反馈和帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78337397/how-can-i-speed-up-my-training-time-for-a-python-chess-bot-using-ppo</guid>
      <pubDate>Tue, 16 Apr 2024 21:27:24 GMT</pubDate>
    </item>
    <item>
      <title>以向量作为自变量而不是多个单值变量进行回归？</title>
      <link>https://stackoverflow.com/questions/78337383/regression-with-vector-as-independent-variable-instead-of-multiple-single-value</link>
      <description><![CDATA[我正在做一个项目，试图通过绘制书中句子的情绪来预测用户对书籍的评分（评论）。
给您一个想法的图表：
红色是得分最高的 25% 书籍的平均情绪图，
蓝色最差。
正如你所看到的，这些书的开头相当中等，在结尾之前就下降了，并且最后都具有很高的情绪。您还可以看到，最好得分 25%（红色）在最后达到最高点。
我想做的是使用回归来根据包含书中每个句子的情感分数的向量来预测一本书的分数。
我尝试了一些方法，但没有效果。
我的想法是将所有书籍分成 100 个部分，对每本书取这 100 个部分的平均值，并在此数据上训练支持向量回归模型（带有多边形内核）。然而，它的表现并不比每次都预测平均分数更好。
所以：
1 自变量 = [avg_sentiment1,avg_sentiment2,...avg_sentiment100]
1 因变量 = 分数（1 到 5 之间的数字，或更具体地说，在我们的数据集中，介于 ~3.200 和 ~4.700 之间）
因此，虽然使用此设置拟合 sklearn SVR 模型不会给出任何错误，但它似乎并没有学习（它的性能比始终预测平均值的虚拟预测器更差）。我尝试了几种具有不同参数的不同回归模型（Ridge、具有不同内核的 SVR、添加 SplineTransformer）。没有比随机做得更好的了。
我可以在网上找到的所有回归示例似乎都使用奇异值预测奇异值（因此自变量年龄 = 12，因变量高度 = 160 厘米，类似的东西），或者最多使用多个变量（添加更多奇异值）值，例如体重 = 67（公斤），作为自变量）。
我的回归是否将 100 个数字向量解释为 100 个不相关的变量？这有关系吗？
救命，我已经超出了我的能力范围。哪种技术最适用于此？最好是我可以在 SKLearn 上找到的东西，我不是专家（正如您可能知道的那样）]]></description>
      <guid>https://stackoverflow.com/questions/78337383/regression-with-vector-as-independent-variable-instead-of-multiple-single-value</guid>
      <pubDate>Tue, 16 Apr 2024 21:22:19 GMT</pubDate>
    </item>
    <item>
      <title>验证时间太长</title>
      <link>https://stackoverflow.com/questions/78337111/validation-taking-too-long</link>
      <description><![CDATA[我目前正在训练类似 UNet 的架构来重建音频。训练进展顺利。问题是我正在执行某些需要在 cpu 上计算的评估（例如 rt60 和 drr），这会导致巨大的时间开销。仅验证 1000 批 32 个音频就需要大约 15 个小时 - 这是可以理解的。我将整个批次移至内存并计算所需的任何内容，记录并移至下一个批次。
如何加快速度？
我尝试了多重处理，但这没有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/78337111/validation-taking-too-long</guid>
      <pubDate>Tue, 16 Apr 2024 20:07:05 GMT</pubDate>
    </item>
    <item>
      <title>DirectML 初学者尝试训练</title>
      <link>https://stackoverflow.com/questions/78336961/directml-beginners-try-to-train</link>
      <description><![CDATA[我是机器学习的新手（至少是现在在 GPU 中的方式），我给自己找了一些关于它的书籍资源。我正在尝试通过 C++ 中的普通 DirectML 找到自己的方法。到目前为止，我一直在成功地使用 HLSL、GLSL 和 NvEncoder 来满足我的需求，但这是我第一次尝试了解 ML 内容的所有较低级别细节。
因此，对于监督学习场景中的简单线性回归模型，我将尝试使用 x,y 数据集训练模型，然后使用线性回归计算 y = ax+b 线 公式&lt; /a&gt;.
当然，在普通的 CPU C++ 中，使用线性代数，这在编程上很容易（而且很慢）。现在让我们在 GPU 中尝试一下。
看过 HelloDirectML 示例后，我注意到此演示上传一个张量并创建一个运算符 DML_ELEMENT_WISE_IDENTITY_OPERATOR_DESC （它只是将一个张量复制到另一个张量）。
问题：

由于线性回归公式求得，比如说b，

,
是计算 Σ(xy) 的正确方法，创建两个张量及其所有元素 x 和 y，然后调用 DML_OPERATOR_ELEMENT_WISE_MULTIPLY，然后使用 DML_CUMULATIVE_SUMMATION_OPERATOR_DESC 求和到恢复张量，一步一步构建一个图来创建 b 的整个类型？
张量的元素数量与 GPU 内存的容纳数量一样多？
请原谅我的无知，我可能正在努力尝试，但我还不想使用 Python。]]></description>
      <guid>https://stackoverflow.com/questions/78336961/directml-beginners-try-to-train</guid>
      <pubDate>Tue, 16 Apr 2024 19:31:04 GMT</pubDate>
    </item>
    <item>
      <title>y 包含以前未见过的标签：'137'[重复]</title>
      <link>https://stackoverflow.com/questions/78336316/y-contains-previously-unseen-labels-137</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78336316/y-contains-previously-unseen-labels-137</guid>
      <pubDate>Tue, 16 Apr 2024 17:15:59 GMT</pubDate>
    </item>
    <item>
      <title>如何从 torchvision 包的 CIFAR10 函数的输出数据、数据集转换为从图像文件夹本地输入的数据？</title>
      <link>https://stackoverflow.com/questions/78335890/how-can-i-go-from-the-output-data-of-the-cifar10-function-of-the-torchvision-pac</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78335890/how-can-i-go-from-the-output-data-of-the-cifar10-function-of-the-torchvision-pac</guid>
      <pubDate>Tue, 16 Apr 2024 15:59:00 GMT</pubDate>
    </item>
    <item>
      <title>预测癌症患者治疗时间的最佳模型是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78335755/what-is-the-best-model-to-predict-time-to-treatment-in-cancer-patients</link>
      <description><![CDATA[我正在为我的人工智能课程开发一个课程项目，我们正在使用 python (google colab) 来预测乳腺癌患者的治疗时间。
我们有一个巨大的数据集（20k, 80），分为训练数据和测试数据。我想知道我可以采取哪些措施来提高人工智能模型的准确性，以及哪些不同的模型可以提供帮助？
所以我首先创建一个相关图来查看哪些特征与目标变量具有高度相关性，并且它们都没有高相关性（意味着它是非线性关系），然后我创建了一个神经网络模型，但是并没有给我太多的准确性，所以经过大量的试验和错误后，我转向随机森林回归器，梯度增强回归器和 XGBRegressor 提供了巨大的帮助，但仍然离所需的分数有点远。我考虑过使用时间序列，但数据集不会给我提供有价值的见解。
需要注意的是，我们完全清理了数据集（没有异常值，没有空列，一切都已标准化）。
以下是数据集中的列示例：

患者 ID、患者种族、付款人类型、患者状态、患者 zip3、患者年龄、乳腺癌诊断代码、乳腺癌诊断描述、乳腺癌诊断年、转移癌症诊断代码、转移第一治疗、地区、分区、人口密度

因此，如果有人有任何提示，那就太棒了;)]]></description>
      <guid>https://stackoverflow.com/questions/78335755/what-is-the-best-model-to-predict-time-to-treatment-in-cancer-patients</guid>
      <pubDate>Tue, 16 Apr 2024 15:36:32 GMT</pubDate>
    </item>
    <item>
      <title>当环境处于截断的情况下时，我应该如何处理值函数？</title>
      <link>https://stackoverflow.com/questions/78334914/how-should-i-handle-the-value-function-when-the-environment-is-rested-in-a-trunc</link>
      <description><![CDATA[我阅读了终止/截断的文档并理解了之间的区别环境的终止情况和截断情况，但我无法理解为什么该值会像下面这样更新。
如果终止：# case 1
    下一个 q 值 = 奖励
否则：#情况2
    下一个 q 值 = 奖励 + 折扣因子 * Q 的最大动作（下一个状态，动作）

# 这样可以更有效地编写
下一个q值=奖励+（未终止）*折扣因子*Q的最大动作（下一个状态，动作）

据我了解，截断信号后环境将被重置，下一步是环境的初始步骤，从 env.reset() 调用。在这种情况下，状态和下一个状态之间没有关系，我不知道为什么使用下一个状态的 Q 值来更新该值。
假设环境的时间限制是1000。那么，无论第1000步的状态是什么，下一步都将是环境的初始步骤。即使在相同的状态和相同的动作下，如果时间步不是1000，下一个状态也会不同，并且会使用不同的Q值。
为什么使用下一个状态的 Q 项来更新截断情况下的值？在截断的情况下也忽略 Q 项不是更好吗？
另外，我想知道如何处理连续任务中下一个状态的 Q 项。在每个截断的情况下，错误的状态（从 env.reset() 调用）是否会被视为下一个状态？
我试图找到类似的问题，但没有找到。可能有一些原因，但我可以找出原因。]]></description>
      <guid>https://stackoverflow.com/questions/78334914/how-should-i-handle-the-value-function-when-the-environment-is-rested-in-a-trunc</guid>
      <pubDate>Tue, 16 Apr 2024 13:31:42 GMT</pubDate>
    </item>
    <item>
      <title>哪种深度学习算法最适合索引图像转换（向样本图像等索引图像添加细节）？</title>
      <link>https://stackoverflow.com/questions/78333861/what-kind-of-deep-learning-algorithm-works-best-for-indexed-image-transformation</link>
      <description><![CDATA[我有一个输入/输出索引图像的数据集（如图像样本），输出是根据人类的输入产生的，我们希望算法学习在输出中绘制细节。

图像的 RGB 值并不重要，因为图像已被索引，艺术家会根据需要更改与每个索引相关的颜色，输入数据集有 128*128 图像，每个像素的值为 0,1 ,2无论是背景、图画还是需要添加细节的地方。输出像素可以有另外 4 个值，其中包括添加的细节。每个项目中的细节可能是不同的和有创意的，例如，在示例图像中，花、叶或茎内部的细节是不同的。如果需要，我还可以标记这些添加细节的不同方法。我可以使用什么样的算法来获得更好的结果？可能是 GAN 或者什么？感谢建议和实施经验。
我已经用一个小数据集训练了一个unet，但结果并不令人满意。我认为这是因为数据集中的每个细节的风格可能不同，而简单的unet无法捕获这些不同的风格。]]></description>
      <guid>https://stackoverflow.com/questions/78333861/what-kind-of-deep-learning-algorithm-works-best-for-indexed-image-transformation</guid>
      <pubDate>Tue, 16 Apr 2024 10:28:36 GMT</pubDate>
    </item>
    <item>
      <title>两张脸之间的人脸识别[关闭]</title>
      <link>https://stackoverflow.com/questions/78333669/face-recognition-between-two-faces</link>
      <description><![CDATA[我对人脸识别有疑问。
我使用 ArcFace 算法使用预训练数据集来比较两个人脸图像。然而，我只实现了高达 85% 的相似度百分比，我想提高准确性。一张图像是旧图像，另一张图像是当前图像。如何提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78333669/face-recognition-between-two-faces</guid>
      <pubDate>Tue, 16 Apr 2024 09:58:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的参考标记句子构建机器学习模型来标记相似的句子？</title>
      <link>https://stackoverflow.com/questions/78329937/building-a-machine-learning-model-to-tag-similar-sentences-using-a-reference-tag</link>
      <description><![CDATA[我是 NLP 新手，正在尝试构建一个 ML 模型来在 Python 中注释/标记类似的句子。
带注释的句子 - “please call [calling] john [name]”
类似的句子 - [“我想给拉姆打电话”、“你能打电话给杰克吗”、“给拉奎尔打电话”]。

我的目标是标记所有相似的句子。我已经拥有相似句子的列表以及每个列表中的一个标记句子。标签基本上是意图和实体。
对上述算法有什么建议吗？我正在使用Python。
注意-

对于建模，我们必须使用 BERT 模型，不能使用任何其他模型。
我已经从所有句子的句子转换器模型中嵌入了可以
如有必要，可重复使用。
]]></description>
      <guid>https://stackoverflow.com/questions/78329937/building-a-machine-learning-model-to-tag-similar-sentences-using-a-reference-tag</guid>
      <pubDate>Mon, 15 Apr 2024 16:53:47 GMT</pubDate>
    </item>
    </channel>
</rss>