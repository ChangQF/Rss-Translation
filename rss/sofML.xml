<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Mar 2024 18:16:33 GMT</lastBuildDate>
    <item>
      <title>转换器输出 Sklearn 返回更多列，其中一些列没有转换</title>
      <link>https://stackoverflow.com/questions/78182162/transformer-output-sklearn-returns-more-columns-with-some-columns-not-having-the</link>
      <description><![CDATA[我正在构建一条管道。我正在从在线 ML 存储库下载数据集并为其生成描述性统计数据。数据集的链接为 https://archive.ics.uci.edu/数据集/45/heart+disease。我正在使用processed.cleveland.data数据集。
我手动添加了列名称，并根据需要将数字转换为字符串。我将 DataFrame 转换为 Numpy 数组以分隔预测变量和目标变量。之后，我将获取管道的数字和分类变量列表。
我开发管道，然后总结 DataFrame。除了 OneHotEncoder 生成的列之外，为什么还有额外的列？理想情况下，我的输出将包含来自原始数据集的相同数量的列，其中包含转换（简单输入器）以及 OneHotEncoder 为分类变量生成的列。标准化列仍然包含空值，而数据集中的原始列包含中位数？
有人可以让我知道这些问题吗？
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.impute 导入 SimpleImputer

网址 = ...
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;,
&#39;oldpeak&#39;、&#39;slope&#39;、&#39;ca&#39;、&#39;thal&#39;、&#39;num&#39;]

def getData():
    返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)


numeric = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字）
打印（分类）

t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;,
OneHotEncoder()，分类），（&#39;num0&#39;，SimpleImputer（strategy=&#39;median&#39;），数值），（&#39;num1&#39;，
MinMaxScaler()，数值）]
column_transforms = ColumnTransformer(transformers=t)

管道=管道(步骤=[(&#39;t&#39;,column_transforms)])
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））``

`
我期望 DataFrame 以相同的顺序返回（使用 SimpleImputer 和 StandardScaler）以及 OneHotEncoder 创建的新变量。]]></description>
      <guid>https://stackoverflow.com/questions/78182162/transformer-output-sklearn-returns-more-columns-with-some-columns-not-having-the</guid>
      <pubDate>Mon, 18 Mar 2024 17:57:04 GMT</pubDate>
    </item>
    <item>
      <title>训练张量流模型来检测 .wav 文件中的静音</title>
      <link>https://stackoverflow.com/questions/78181530/train-a-tensorflow-model-to-detect-silence-in-wav-file</link>
      <description><![CDATA[我需要检测波形文件中的静音（轻微噪音，不是绝对静音）。所有波形文件（训练和检测）都是 16 位和单声道。
这是处理给定目录中所有静音文件的训练脚本。声音文件被分为0.1秒的块（1600帧）作为训练数据，对于特征检测，可以使用梅尔频率倒谱系数（MFCC）或短时傅里叶变换（STFT）。 （我都试过了）
这是训练脚本
# 使用tensorflow训练静音模型
导入全局
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;1&#39;
os.environ[“TF_ENABLE_ONEDNN_OPTS”] = &#39;0&#39;

将张量流导入为 tf
从tensorflow.keras导入层、模型
将 numpy 导入为 np
导入操作系统
导入库

# 从音频文件中提取MFCC特征的函数
def extract_features(file_path, mfcc=True, hop_length=512, n_mfcc=13):
    信号，sr = librosa.load（文件路径，sr =无）
    block_size = 1600 # sr / 10 持续 0.1 秒
    num_blocks = len(signal) // 块大小
    
    特征=[]
    对于范围内的 i（num_blocks）：
        块 = 信号[i * 块大小: (i + 1) * 块大小]
        如果是 mfcc：
            mfccs = librosa.feature.mfcc(y=块，sr=sr，n_fft=1024，hop_length=hop_length，n_mfcc=n_mfcc)
            功能.append(mfccs.T)
        别的：
            features.append(np.abs(librosa.stft(块, n_fft=1024, hop_length=hop_length)))
    
    返回 np.array(特征)

# 包含静音和噪音声音文件的目录
silence_files = glob.glob(&#39;声音/沉默/沉默*.wav&#39;)

# 提取所有文件的特征
X = []
y = []
对于silence_files中的文件：
    特征 = extract_features(文件, mfcc=True)
    X.扩展（功能）
    y.extend([0] * len(features)) # 假设静音文件标记为 0

# 将列表转换为数组
X = np.array(X)
y = np.array(y)

# 定义并编译模型
模型 = models.Sequential([
    层.输入(形状=X[0].形状),
    Layers.Reshape(target_shape=(*X[0].shape, 1)), # 重塑以包含通道尺寸
    层.Conv2D(32, kernel_size=(3, 3), 激活=&#39;relu&#39;),
    层数.MaxPooling2D(pool_size=(2, 2)),
    层.Flatten(),
    层.Dense(1, 激活=&#39;sigmoid&#39;)
]）

# 编译模型
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

# 训练模型
model.fit(X, y, epochs=10, batch_size=32)

# 将模型保存到外部文件
model.save(“models/silence_model.keras”)

这是检测脚本
# 使用张量流模型检测静音
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;1&#39;
os.environ[“TF_ENABLE_ONEDNN_OPTS”] = &#39;0&#39;
将声音文件导入为 sf

将张量流导入为 tf
导入库
将 numpy 导入为 np

# 从音频帧中提取MFCC特征的函数
def extract_features_from_block(块, mfcc=True, hop_length=512, n_mfcc=13):
    如果是 mfcc：
        mfccs = librosa.feature.mfcc(y=块，sr=16000，n_fft=1024，hop_length=hop_length，n_mfcc=n_mfcc)
        # 重塑以匹配模型输入形状
        mfccs = mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1)
        返回 MFCC
    别的：
        stft = librosa.stft(块, n_fft=1024, hop_length=hop_length)
        # 重塑以匹配模型输入形状
        stft = stft.reshape(1, stft.shape[0], stft.shape[1], 1)
        返回 np.abs(stft)


def remove_silence(输入文件、输出文件、模型、阈值=0.5):
    信号，sr = librosa.load（输入文件，sr =无）
    block_size = 1600 # sr / 10 持续 0.1 秒
    num_blocks = len(signal) // 块大小
    
    输出信号 = np.array([])
    
    对于范围内的 i（num_blocks）：
        块 = 信号[i * 块大小: (i + 1) * 块大小]
        特征 = extract_features_from_block(块, mfcc=True)
        预测 = model.predict(feature)[0][0]
        打印（预测）
        如果预测&lt;临界点：
            # 添加完整的静音块
            输出信号 = np.concatenate((输出信号，np.zeros_like(块)))
        别的：
            # 添加非静音块
            输出信号 = np.concatenate((输出信号，块))

    # 将处理后的信号写入输出文件
    sf.write（输出文件，输出信号，sr）

# 加载保存的模型
model = tf.keras.models.load_model(“models/silence_model.keras”)

# 使用示例
删除_静音（“./sounds/slience_test.wav”，“output_file.wav”，模型）

检测中的问题是每个区块的预测几乎等于 0.0。以下链接可下载 3 个用于训练的静默文件和一个用于测试的静默文件。
SILENCE.ZIP]]></description>
      <guid>https://stackoverflow.com/questions/78181530/train-a-tensorflow-model-to-detect-silence-in-wav-file</guid>
      <pubDate>Mon, 18 Mar 2024 16:10:37 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn train_test_split 是否复制数据？</title>
      <link>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</link>
      <description><![CDATA[是否train_test_split&lt; /a&gt; scikit-learn 的方法复制数据？换句话说，如果我使用大型数据集 X, y，这是否意味着在执行类似
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 2023）

我的数据使用的内存是原始数据集的两倍？或者是否有一些 scikit-learn （或基本的 python）魔法可以阻止它？ （例如，使用  .to_numpy()并不一定会导致数据重复)
如果内存使用量翻倍，解决此问题的最佳实用方法是什么？也许，类似
X、X_test、y、y_test = train_test_split(X、y、test_size=0.2、random_state=2023)

？
备注
使用 np.shares_memor(X_train, X) 表明数据确实是重复的。]]></description>
      <guid>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</guid>
      <pubDate>Mon, 18 Mar 2024 16:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“内存分配失败”的原因是什么</title>
      <link>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</link>
      <description><![CDATA[我创建了一个简单的逻辑回归类，其中包含一个gradient_ascent 函数。当我尝试使用它时，我的 IDE 会引发如下错误
无法为形状为 (86918, 86918) 且数据类型为 float64 的数组分配 56.3 GiB

我的数据大小是(86918,10)，标签大小是(86918,1)。我调试的时候发现在gradient_ascent函数中执行代码error = y_predicted - y时程序退出并报错。但是，我找不到原因。以下是我的代码。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 precision_score
从 sklearn.model_selection 导入 train_test_split


定义 sigmoid(z):
    返回 1 / (1 + np.exp(-z))


逻辑回归类：
    def __init__(self,learning_rate=0.01,number_iterations=1000,method=&#39;gradient_ascent&#39;):
        自我学习率 = 学习率
        self.number_iterations = number_iterations
        自重=无
        自我偏见=无
        self.method = 方法

    defgradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            线性模型 = np.dot(X, self.weights) + self.bias
            y_预测 = sigmoid(线性模型)
            误差 = y_预测 - y
            dw = (1 / num_samples) * np.dot(X.T, 错误)
            db = (1 / num_samples) * np.sum(错误)

            self.weights += self.learning_rate * dw
            self.bias += self.learning_rate * db

    def stochastic_gradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            对于范围内的 i（num_samples）：
                线性模型 = np.dot(X[i], self.weights) + self.bias
                y_预测 = sigmoid(线性模型)
                误差 = y_预测 - y[i]

                dw = X[i] * 误差
                数据库=错误

                self.weights += self.learning_rate * dw
                self.bias += self.learning_rate * db

    def fit(自身, X, y):
        如果 self.method == &#39;gradient_ascent&#39;:
            self.gradient_ascent(X, y)
        别的：
            self.stochastic_gradient_ascent(X, y)

    def 预测（自身，X）：
        y_predicted = sigmoid(np.dot(X, self.weights) + self.bias)
        y_predicted = [1 如果 i &gt; 0.5 else 0 for i in y_predicted]
        返回 y_预测值


路径=&#39;../data/KaggleCredit2.csv&#39;
数据= pd.read_csv（路径，index_col = 0）
data.dropna（轴= 0，就地= True）
y = 数据[&#39;SeriousDlqin2yrs&#39;]
X = data.drop(标签=&#39;SeriousDlqin2yrs&#39;, 轴=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lr1 = 逻辑回归()
lr1.fit(X_train, y_train)
预测1 = lr1.预测(X_test)
lr1_score = precision_score(y_test, 预测1)

我尝试过查阅GPT并逐行调试代码，但仍然找不到错误。如果有人能给我一些指导，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</guid>
      <pubDate>Mon, 18 Mar 2024 14:22:18 GMT</pubDate>
    </item>
    <item>
      <title>尝试复制一种收敛权重的算法，以近似合作差分博弈系统的联合成本函数</title>
      <link>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:32 GMT</pubDate>
    </item>
    <item>
      <title>你能在 h2o 的 PCA 函数中指定旋转吗？</title>
      <link>https://stackoverflow.com/questions/78180809/can-you-specify-a-rotation-in-h2os-pca-function</link>
      <description><![CDATA[我正在 h2o（R 版本）中运行 PCA，并且想知道是否可以指定/应用旋转（如 oblimin 或 promax）。我正在寻找旋转载荷，我使用 h2o 而不是其他常见包（如“psych”）的原因是我的数据集很大（100000 列），所以我需要利用 h2o Windows 中很好的并行计算。我当前使用的代码是：
库(h2o)

h2o.init(nthreads=64)

x &lt;- read.csv(“file_with_100000_columns.csv”)

for (i in 1:ncol(x)) {x[,i] &lt;- as.factor(x[,i])}

x &lt;- as.h2o(x)

mod &lt;- h2o.prcomp(training_frame=x,k=5,use_all_factor_levels=TRUE)

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78180809/can-you-specify-a-rotation-in-h2os-pca-function</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:31 GMT</pubDate>
    </item>
    <item>
      <title>如何设计神经网络来进行突破性检测[关闭]</title>
      <link>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</link>
      <description><![CDATA[摘要：
我正在设计一个用于大鼠开颅手术的自动钻孔系统。我希望当神经网络检测到突破时钻头会停止。照片显示了该系统。
系统概述
手术结果
我尝试的：

力传感器和加速度计实时收集信号（采样率为3kHz）。因此，我用它们记录了一些钻孔过程（以恒定的进给速度和转速），并用 Python 绘制它们，如下所示：强制数据（蓝色代表原始数据，橙色代表黄油低通滤波数据）。来自 NIDAQmx 的振动数据
我希望使用神经网络进行检测（因为我可以轻松地识别图中“forcedata”中蓝色的突破点）。所以我训练一个CNN-LSTM，输入0.1秒内收集到的数据，输出一个0-1的数字，其中0表示不钻，1表示钻。（因为我认为突破点是1和0的交界处）。所以我手动将数据分为标签为1的数据集和标签为0的数据集如图所示。红色代表钻孔过程。标签1组是通过每0.1s分割区域来制作的
CNN-LSTM 网络代码如下：

类 CNNLSTMClassifier(nn.Module):
    def __init__(自身，hidden_​​size，num_layers)：
        超级（CNNLSTMClassifier，自我）.__init__（）
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=4, padding=1),
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 64，out_channels = 128，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 128，out_channels = 256，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d（内核大小=2，步幅=2）
        ）
        self.lstm = nn.LSTM(input_size=256,hidden_​​size=hidden_​​size,num_layers=num_layers,batch_first=True)
        self.fc = nn.Linear(hidden_​​dim, 1)
        self.sigmoid = nn.Sigmoid()

    def 前向（自身，x）：
        x = x.unsqueeze(1)
        x = self.cnn(x)
        x = x.permute(0, 2, 1)
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        输出 = self.sigmoid(输出)
        返回

我明白了
2024-03-18 16:25:50.708972 Epoch 1，训练损失 0.6764530851605625
2024-03-18 16:25:55.804250 Epoch 5，训练损失 0.6663112645497138
2024-03-18 16:26:02.211953 Epoch 10，训练损失 0.660625655507836
...................................................... ......................
2024-03-18 16:28:04.148844 Epoch 100，训练损失 0.6572404681613006

列车精度：0.63
准确度测试：0.66

所以效果很差。
我的期望：
我希望无论使用什么方法，神经网络都能检测到突破（可能是另一个神经网络或只是另一种方法）。
补充说明：我认为二元分类的想法是错误的。也许我应该检查的是每0.1s的数据是否有足够的整体下降趋势。]]></description>
      <guid>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</guid>
      <pubDate>Mon, 18 Mar 2024 12:52:29 GMT</pubDate>
    </item>
    <item>
      <title>机器学习过程中 TPU 到底用在什么地方？</title>
      <link>https://stackoverflow.com/questions/78180134/where-exactly-are-tpus-used-during-machine-learning</link>
      <description><![CDATA[我想知道当前技术水平中机器学习 TPU 期间通常使用哪些算法步骤。特别是，我很感兴趣它们是否用于推理、反向传播和/或卷积。
我知道脉动阵列的工作原理以及 TPU 的基本原理，并且它们可以比 CPU/GPU 更快地执行非稀疏矩阵乘法，这是有道理的。但例如对于卷积，相乘矩阵通常非常稀疏。在那里使用 TPU 仍然有意义吗？
我希望获得有关此主题的详尽解释。]]></description>
      <guid>https://stackoverflow.com/questions/78180134/where-exactly-are-tpus-used-during-machine-learning</guid>
      <pubDate>Mon, 18 Mar 2024 12:16:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用 LLM 的情况下将随机文件（具有不同名称）分组到适当的文件夹中？</title>
      <link>https://stackoverflow.com/questions/78179964/how-can-i-group-random-files-with-different-names-into-appropriate-folders-wit</link>
      <description><![CDATA[我想构建一个产品，可以根据文件名自动组织文件夹/子文件夹和文件，并根据文件名的相似性将它们分组。我知道这个问题可以通过 LLM（特别是 ChatGPT4）来解决，但这会带来一些问题。 1. 隐私，不是每个人都愿意将自己的文件名发送到这个系统。 2. 输入长度，OpenAI 上下文窗口有一定的长度限制，因此如果您有一个包含 1000 个或更多文件的文件夹，它将无法处理此长度。 3. 令牌成本，即使您可以处理 1000 个或更多文件，这也会变得非常昂贵。出于这个原因，我正在寻找替代方案。 （本地法学硕士可以解决隐私问题，但无法解决问题 2）。
我确信存在机器学习技术，可以根据单词背后含义的相似性将单词聚集在一起，但我还没有找到一个可以立即满足我需要的库。&lt; /p&gt;
这是我想要实现的目标的示例：
输入（文件名）：

历史家庭作业.pdf
报告.docx
山.png
历史测试.docx
天气数据.csv
地理笔记.docx
艺术品1.png
artwork2.png

输出（建议的文件夹名称）：

历史
地理
艺术
其他

请注意，输出文件夹不是预先确定的组。例如，输入可能是其他 10 个随机文件，您应该根据输入建议新名称的算法。
我找到的最接近的解决方案是在 python 中使用 WordNet 并查找上位词来对单词进行分组。问题是它不是很准确。]]></description>
      <guid>https://stackoverflow.com/questions/78179964/how-can-i-group-random-files-with-different-names-into-appropriate-folders-wit</guid>
      <pubDate>Mon, 18 Mar 2024 11:46:56 GMT</pubDate>
    </item>
    <item>
      <title>cross_val_predict 中是否有 xgb.XGBRegressor 的示例，其中回调=[early_stop], Early_stop=xgb.callback.EarlyStopping？</title>
      <link>https://stackoverflow.com/questions/78178902/is-there-example-of-xgb-xgbregressor-with-callbacks-early-stop-early-stop-xgb</link>
      <description><![CDATA[在文档
XGBClassifier 有一个 EarlyStopping：
&lt;前&gt;&lt;代码&gt;```
es = xgboost.callback.EarlyStopping(
    轮数=2，
    min_delta=1e-3,
    save_best=真，
    最大化=假，
    data_name=“validation_0”，
    metric_name=“mlogloss”,
    ）
clf = xgboost.XGBClassifier(tree_method=“hist”, device=“cuda”, 回调=[es])

X, y = load_digits(return_X_y=True)
clf.fit(X, y, eval_set=[(X, y)])```

但是“validation_0”是如何实现的？引用 clf.fit 中的 eval_set 来让 EarlyStopping 指标进行评估？
我尝试将其应用到 XGBRegressor：
`将 xgboost 导入为 xgb
从 sklearn.model_selection 导入 cross_val_predict，KFold
将 pandas 导入为 pd
将 numpy 导入为 np

类 CustomEarlyStopping(xgb.callback.EarlyStopping):
    def __init__(self, rounds=2, min_delta=1e-3, save_best=True, maximise=False, data_name=“validation_0”, metric_name=“rmse”):
        super().__init__(rounds=rounds, min_delta=min_delta, save_best=save_best, maximise=maximize, data_name=data_name, metric_name=metric_name)
    
# 火车模型（10x10 倍 CV）
cvx = KFold(n_splits=10, shuffle=True, random_state=239)
es = 自定义早期停止()

模型= xgb.XGBRegressor（colsample_bytree = 0.3，learning_rate = 0.1，max_深度= 10，alpha = 10，n_estimators = 500，n_jobs = -1，
                     random_state=239，回调=[es]）
model.set_params(tree_method=&#39;approx&#39;, device=&#39;cpu&#39;)

cv_preds = []
对于范围 (0,10) 内的 i：
    cv_preds.append(cross_val_predict(模型, np.asarray(X_train), np.asarray(y_train), cv=cvx, method=&#39;predict&#39;, n_jobs=1, verbose=2))`

我把data_name=“validation_0”放在在 EarlyStopping __init__ 中，而不在每个 cv 折叠中命名测试集。
这段代码的行为有什么问题？谢谢。
XGBRegressor 的代码返回了此错误：
ValueError：必须至少有 1 个验证数据集才能提前停止。

应该发生的情况是 cv_preds 被 10 个预测 y 的 ndarray 填充。]]></description>
      <guid>https://stackoverflow.com/questions/78178902/is-there-example-of-xgb-xgbregressor-with-callbacks-early-stop-early-stop-xgb</guid>
      <pubDate>Mon, 18 Mar 2024 08:42:57 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 Gradio Client API 使用图像进行预测</title>
      <link>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</link>
      <description><![CDATA[我正在尝试按照以下示例将图像发送到 Gradio Client API：
从“@gradio/client”导入{ client }；

const response_0 = 等待 fetch(“https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png”);
const exampleImage =等待response_0.blob();
                        
const app = 等待客户端(“airvit2/pet_classifier”);
const 结果 =等待 app.predict(“/预测”, [
                exampleImage, // &#39;img&#39; 图像组件中的 blob
    ]);

console.log(结果.数据);

但它返回此错误：
&lt;前&gt;&lt;代码&gt;{
    “类型”：“状态”，
    “端点”：“/预测”，
    “fn_index”：0，
    “时间”：“2024-03-17T18:36:53.270Z”，
    “队列”：正确，
    “消息”：空，
    “阶段”：“错误”，
    “成功”：假
}

这是我的 Gradio 代码：
from fastai.vision.all import *
将渐变导入为 gr

学习 = load_learner(&#39;model.pkl&#39;)

def 预测（img）：
    print(&quot;图片：&quot;, img)
    img = 加载图像(img)
    # img = PILImage.create(img)
    pred, pred_idx, probs = learn.predict(img)
    返回预测值

gr.Interface(fn = 预测，输入 = gr.Image(type=“pil”，高度 = 224，宽度 = 224)，输出 = gr.Label(num_top_classes = 3)).launch(share = True)


我尝试将图像格式更改为 Blob，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</guid>
      <pubDate>Sun, 17 Mar 2024 18:57:11 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 自定义模型不进行预测</title>
      <link>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</link>
      <description><![CDATA[我使用自定义训练的 Yolov8 模型来预测物理门是关闭还是打开。我已经在自定义数据集上训练了 Yolov8，但即使传递用于训练的相同数据，它也不会进行任何检测。
我使用了大约 300 张图像的数据集。
这是我的代码：
导入操作系统

从 ultralytics 导入 YOLO
导入CV2


VIDEOS_DIR = os.path.join(&#39;.&#39;, &#39;视频&#39;)

video_path = os.path.join(VIDEOS_DIR, &#39;样本门.mp4&#39;)
video_path_out = &#39;{}_out.mp4&#39;.format(video_path)

cap = cv2.VideoCapture(video_path)
ret, 框架 = cap.read()
H、W、_ = 框架.形状
out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))

model_path = os.path.join(&#39;.&#39;, &#39;运行&#39;, &#39;检测&#39;, &#39;训练&#39;, &#39;权重&#39;, &#39;last.pt&#39;)


model = YOLO(model_path) # 加载自定义模型


休息时：

    结果=模型（框架）[0]
    对于 results.boxes.data.tolist() 中的结果：
        x1, y1, x2, y2, 分数, class_id = 结果
        打印（x1，y1，x2，y2）

        cv2.矩形(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)
        cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)

    输出.write(帧)
    ret, 框架 = cap.read()

cap.release()
out.release()
cv2.destroyAllWindows()

以下是训练结果：https://i.stack.imgur。 com/huyZR.png]]></description>
      <guid>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别GPU。
该代码几周前就可以工作，有人有解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的无头 GPT2 模型在保存时抛出错误 - 如何添加输入和输出层以及自定义 PositionalEmbedding</title>
      <link>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</link>
      <description><![CDATA[我想使用 GPT2 对序列数据进行回归任务，因此尝试从 Hugging Face 中找出无头 TFGPT2，代码如下：
配置 = GPT2Config(n_embd = embed_dim, n_head=num_heads)
基础模型 = TFGPT2Model（配置）
输入形状 = (1, 嵌入尺寸)
input1 = 层.Input(shape=input_shape, dtype=tf.float32)
positional_encoding = PositionalEmbedding(sequence_length, embed_dim)
解码器输入=位置编码（输入1）
Z = base_model(无，inputs_embeds=decoder_inputs)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“relu”））（Z.last_hidden_​​state）
模型= keras.Model（输入1，输出）
model.compile(loss=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam(beta_1=0.9，beta_2=0.98，epsilon=1.0e-9)，metrics=[tf.keras.metrics.RootMeanSquaredError()] ）
历史= model.fit（数据集，validation_data = val_dataset，epochs = epoch_len，verbose = 1）
tf.keras. saving. save_model(模型, r&#39;/drive/model_huggingface&#39;)

另请注意，我还使用自定义 PositionalEmbedding 类，因此可选的 input_embeds 参数传递给模型。
该模型训练并学习数据，但在尝试保存时会抛出错误：
AssertionError：尝试导出引用“未跟踪”资源的函数。由函数捕获的 TensorFlow 对象（例如 tf.Variable）必须通过将其分配给被跟踪对象的属性或直接分配给主对象的属性来“跟踪”。请参阅以下信息：
    函数名称 = b&#39;__inference_signature_wrapper_452514&#39;
    捕获的张量 = 
    可追踪引用此张量 = ;
    内部张量 = Tensor(“452144:0”, shape=(), dtype=resource)

我认为这是因为我向该模型添加了头部和自定义层。请让我知道您对我对如何实现此模型的解释的看法。]]></description>
      <guid>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</guid>
      <pubDate>Sun, 17 Mar 2024 14:03:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow federated 中工作时遇到“学习属性”错误</title>
      <link>https://stackoverflow.com/questions/78158329/facing-error-in-learning-attribute-while-working-in-tensorflow-federated</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78158329/facing-error-in-learning-attribute-while-working-in-tensorflow-federated</guid>
      <pubDate>Thu, 14 Mar 2024 05:35:24 GMT</pubDate>
    </item>
    </channel>
</rss>