<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 21 Nov 2024 18:23:40 GMT</lastBuildDate>
    <item>
      <title>为什么我的 ML 模型在不同的运行中获得不同的性能？</title>
      <link>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</link>
      <description><![CDATA[因此，我正在使用 snowpark 训练 ml 模型（Xgboost qnd LightGbm），但每次运行后我都会得到不同的指标值（AUC、平均精度），因此永远不知道哪个是我最好的模型。
我尝试在笔记本的开头设置一个全局变量 random_seed = 42，并将其放在我的欠采样函数和模型的初始化中：
 if model_type == &#39;xgboost&#39;:
model = XGBClassifier(
random_state=random_seed,
input_cols=feature_cols,
label_cols=target_col,
output_cols=[&#39;PREDICTION&#39;],
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;, &#39;DATE_MONTH&#39;],
**hyperparameters
)

elif model_type == &#39;lightgbm&#39;:
model = LGBMClassifier（
random_state=random_seed，
input_cols=feature_cols，
label_cols=target_col，
output_cols=[&#39;PREDICTION&#39;]，
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;，&#39;DATE_MONTH&#39;]，
**超参数

)

def undersample_majority_class（df）：

df_with_seniority = df.with_column（“years_since”，（F.col（&#39;TIME_SINCE_FIRST_LEAD&#39;）/12）。cast（&#39;int&#39;））

df_with_random = df_with_seniority.with_column（&#39;random_order&#39;，F.random（seed=random_seed））
window_spec = Window.partition_by(&quot;INDIVIDUAL_SK&quot;).order_by(F.col(&#39;random_order&#39;).asc())
df_ranked = df_with_random.with_column(&quot;month_rank&quot;, F.row_number().over(window_spec)
)

df_majority = df_ranked.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 0)
df_majority_sampled = df_majority.filter(((F.col(&quot;years_since&quot;) &gt; 10) &amp; (F.col(&quot;month_rank&quot;) == 1)) |
((F.col(&quot;years_since&quot;) &lt;= 10) &amp; (F.col(&quot;month_rank&quot;) &lt;= 2))
)

df_majority_sampled = df_majority_sampled.drop(&#39;years_since&#39;,&#39;month_rank&#39;,&#39;random_order&#39; )
df_minority = df.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 1)
df_balanced = df_majority_sampled.union_all(df_minority)

return df_balanced

我不知道该怎么做才能解决这个问题。

]]></description>
      <guid>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</guid>
      <pubDate>Thu, 21 Nov 2024 18:16:23 GMT</pubDate>
    </item>
    <item>
      <title>ANN 模型的准确性 [关闭]</title>
      <link>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</link>
      <description><![CDATA[我曾尝试使用历史数据构建一个 ANN 模型来预测太阳辐射。
2016 - 2020 NSRDB 数据
以下是评估指标
R2 值 .999
MSE .690
MAE .450
损失 .690
以下是我的问题
ANN 模型的准确率达到 .999 是否正常
是否过度拟合？
我附上了训练损失与验证损失图]]></description>
      <guid>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</guid>
      <pubDate>Thu, 21 Nov 2024 16:58:23 GMT</pubDate>
    </item>
    <item>
      <title>使用参考图像交换图像中的蒙版区域</title>
      <link>https://stackoverflow.com/questions/79211893/swap-masked-area-in-an-image-using-a-reference-image</link>
      <description><![CDATA[我有一张图像及其蒙版。我想用第三张图像作为参考来替换蒙版区域。例如，我想用参考图像替换房屋图像中的蒙版区域（黑色区域）。在此示例中，输出应该看起来像使用参考图像的颜色和纹理的瓷砖建造的房屋屋顶。结果图像是我使用稳定扩散能够获得的最佳结果。但是我需要更好的结果。如果可以使用其他方法，请告诉我。
原始房屋图像
房屋面具
参考图像
结果图像]]></description>
      <guid>https://stackoverflow.com/questions/79211893/swap-masked-area-in-an-image-using-a-reference-image</guid>
      <pubDate>Thu, 21 Nov 2024 15:34:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 最小化汉明距离</title>
      <link>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</link>
      <description><![CDATA[我有一个 mxn 矩阵，其中 m&gt;n 和一个向量 b。它们仅由 1 和 0 组成。此外，所有地方的和都是以 2 为模的。通过高斯消元法，我可以得到 Ax=b 的解 x_p。这是一个包含 1 和 0 的向量。这不是唯一的解决方案。存在其他包含较少 1 的解。我想找到包含最少 1 的解（汉明距离），即我想最小化汉明距离。
为了做到这一点，我想到了使用 pytorch 的梯度下降法。这是我的代码：
def optimal_solution(self, max_iter=100, lr=0.0054, lambda_param=10):

matrix = self.relative_boundary()
# 在 [0,1] 中初始化 x 并转换为 pytorch 张量
x = self.solve()

matrix = torch.tensor(matrix, dtype=torch.float32)
b = torch.tensor(self.boundary_vector, dtype=torch.float32)
x = torch.tensor(x, dtype=torch.float32, require_grad=True)

# Adam 是梯度下降的高级版本，可在优化过程中调整学习率。
optimizer = torch.optim.Adam([x], lr=lr)

# 跟踪 x 变化的标准
prev_x = x.clone()

# 循环执行梯度下降，最多迭代 max_iter 次：
for _ in range(max_iter):
optimizer.zero_grad()

# 定义损失函数：稀疏性 + 约束满足
loss = torch.sum(x) + lambda_param * torch.sum((matrix @ x - b) ** 2)

# 跟踪 x 的变化量
change_in_x = torch.norm(x - prev_x).item()

# 计算相对于 x 的损失梯度
loss.backward()
# 根据梯度更新 x 的值
optimizer.step()

# 应用软投影（例如 S 型）使 x 保持在 [0, 1] 中
x.data = torch.clamp(x.data, 0, 1)

# 可选地打印损失以进行调试
print(f&quot;迭代 {_} 时的损失：{loss.item()}, x 的变化：{change_in_x}&quot;)

# 从计算图中分离 PyTorch 张量并将其转换为 NumPy 数组；
# 应用 0.5 的阈值将值转换为 0 或 1。
# 然后函数返回二进制解决方案向量。
x_binary = (x.detach().numpy() &gt; 0.5).astype(int)

return x_binary

但是，通过尝试不同的 lambda_par 和学习率，我要么得到零向量，要么它只输出一个具有较少 1 的向量，但不满足方程 Ax=b。您对有效的代码有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</guid>
      <pubDate>Thu, 21 Nov 2024 14:43:15 GMT</pubDate>
    </item>
    <item>
      <title>VAE 损失减少，但重建效果并未改善</title>
      <link>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</link>
      <description><![CDATA[我遇到了重建图像根本不起作用的问题。下面是我执行单个更新步骤的方法。我知道常规 VAE 可以实现更简单的实现，但由于其他限制，我需要以这种方式实现它。因此，我想知道此实现中是否存在任何错误，而不是要求更高效的实现。zeros_like_batchstats 函数返回一个与输入具有相同结构但所有值都设置为零的对象。
@jax.jit
def train_step(
rng: jax.random.PRNGKey,
state_enc: TrainState,
state_dec: TrainState,
imgs: jax.Array,
) -&gt; Tuple[TrainState, TrainState, Dict]:

((mean, logvar), enc_mutated_vars), vjp_fn_enc = jax.vjp(
lambda params: state_enc.apply_fn(
{&quot;params&quot;: params, &quot;batch_stats&quot;: state_enc.batch_stats},
imgs, train=True, mutable=[&quot;batch_stats&quot;]
),
state_enc.params,
)
z, vjp_fn_latents = jax.vjp(
lambda mean, logvar: sample_z(rng, mean, logvar),
mean, logvar
)

def recon_loss_fn(dec_params, latent_features):
# 从解码器获取重建图像
recon, mutated_vars = state_dec.apply_fn(
{&#39;params&#39;: dec_params, &#39;batch_stats&#39;: state_dec.batch_stats},
latent_features, train=True, mutable=[&#39;batch_stats&#39;]
)
recon_loss = jnp.mean(jnp.sum(binary_cross_entropy_fn(recon, imgs), axis=(1, 2, 3), keepdims=True))
返回 recon_loss, mutated_vars

recon_loss_grads_fn = jax.value_and_grad(recon_loss_fn, argnums=(0, 1), has_aux=True)
(recon_loss, dec_mutated_vars), (grads_dec, graz_z) = recon_loss_grads_fn(state_dec.params, z)
    grads_enc_recon=vjp_fn_enc((vjp_fn_latents(graz_z), {“batch_stats”: Zeros_like_batchstats(state_enc.batch_stats)}))[0]

    # 计算kld_loss
    def kld_loss_fn(平均值, logvar):
        kld_loss = jnp.mean(jnp.sum(-0.5 * (1 + logvar - 平均值 ** 2 - jnp.exp(logvar)), axis=1))
        返回 kld_loss

    kld_loss_grads_fn = jax.value_and_grad(kld_loss_fn, argnums=(0, 1))
    kld_loss, grads_mean_and_logvar = kld_loss_grads_fn(平均值, logvar)
    grads_enc_kld = vjp_fn_enc((grads_mean_and_logvar, {&quot;batch_stats&quot;: zeros_like_batchstats(state_enc.batch_stats)}))[0]

# 计算编码器的梯度
grads_enc = jax.tree_util.tree_map(lambda x, y: x + y, grads_enc_recon, grads_enc_kld)

# 存储梯度和批次统计信息\
state_enc = state_enc.apply_gradients(grads=grads_enc, batch_stats=enc_mutated_vars[&quot;batch_stats&quot;])
state_dec = state_dec.apply_gradients(grads=grads_dec, batch_stats=dec_mutated_vars[&quot;batch_stats&quot;])

metrics = {
&quot;train/recon_loss&quot;: recon_loss,
&quot;train/kld_loss&quot;: kld_loss,
}
return state_enc, state_dec, metrics

我已确认形状没有问题，并且损失也在减少。]]></description>
      <guid>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</guid>
      <pubDate>Thu, 21 Nov 2024 13:23:11 GMT</pubDate>
    </item>
    <item>
      <title>Unsloth 羊驼提示模板：[关闭]</title>
      <link>https://stackoverflow.com/questions/79210129/unsloth-alpaca-prompt-template</link>
      <description><![CDATA[我有一个与急救说明相关的数据集，它有两列，一列是问题，另一列是答案，现在我的问题是如何修改 unsloth 笔记本中提供的羊驼提示模板以适应我的用例。我尝试了几个模板，但对我来说不起作用。
我尝试了几个模板和 llama 3.1 聊天模板，但不起作用。
有人能为这些任务推荐一个模型来微调这个数据集吗？
llama 模型是否适用于这种类型的数据集。数据集包含直接的问题和答案。
https://huggingface.co/datasets/lextale/FirstAidInstructionsDataset
这是数据集的链接，如果有人使用过此类数据集，请指导我如何针对这些类型的数据微调模型或针对此类数据的任何特定模型。
我将非常感谢有关这些问题的任何帮助，我正在为我的 FYP 做这件事。]]></description>
      <guid>https://stackoverflow.com/questions/79210129/unsloth-alpaca-prompt-template</guid>
      <pubDate>Thu, 21 Nov 2024 07:40:57 GMT</pubDate>
    </item>
    <item>
      <title>使用什么DL预训练模型或方法来训练以下实现[关闭]</title>
      <link>https://stackoverflow.com/questions/79209830/what-dl-pretrained-model-or-method-to-use-for-training-the-following-implementat</link>
      <description><![CDATA[我正在开展一个物联网项目，该项目将对披萨在放入烤箱时是在左侧还是右侧进行分类。它还应说明披萨是进烤箱还是出烤箱。现在我有来自静态摄像头的视频源。下面是披萨在左侧或右侧的帧图像，例如：披萨正在移入烤箱的左侧和披萨正在移到烤箱的右侧。
我尝试过使用 YOLO 模型，但它主要用于物体检测。如何识别披萨的相对位置和相对运动？]]></description>
      <guid>https://stackoverflow.com/questions/79209830/what-dl-pretrained-model-or-method-to-use-for-training-the-following-implementat</guid>
      <pubDate>Thu, 21 Nov 2024 05:47:51 GMT</pubDate>
    </item>
    <item>
      <title>关于 Talebi 论文中空间决策树分裂逻辑的澄清 [关闭]</title>
      <link>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</link>
      <description><![CDATA[我正在研究 Talebi 等人的论文“用于地球科学数据分析和建模的真正空间随机森林算法”，我对图 2 所示的空间决策树过程有一些疑问。
混合旋转和缩放：
我的理解是，对于每个单元格，多个尺度和旋转的空间模式被矢量化并连接成单个输入向量。然后在树分割过程中将此输入用作预测器。这是正确的吗？
此外，模型如何确保来自不同旋转和尺度的模式在混合成一个向量时保留其空间上下文？
分割中灰色区域的移动：
在图 2 中，我注意到灰色单元格似乎代表数据的一个子集，在分割过程中从中间移动到角落。

这是否表明随着树分割成更小的区域，预测空间会缩小？
这种移动是将模式过滤成更均匀的子集的结果，还是代表了数据中空间依赖性的特定内容？

第二次分割中的不同灰色区域：

为什么第二次分割的左分支中的灰色区域保留在第一个-𝑅中，而在右分支中，它转移到第二个-𝑅？
这是否表明选择不同的预测因子来分割左分支和右分支？
如果是这样，这是否突出了数据集中的空间异质性？

任何关于这在空间决策树中如何工作的说明或示例都将非常有帮助。
我尝试了什么：
我回顾了图 2 的描述和论文“用于地球科学数据分析和建模的真正空间随机森林算法”中的方法。我试图了解灰色区域如何对应于空间模式和用于分割的预测因子。
我预期会发生什么：
我预期灰色区域代表预测因子空间的一致划分，其中每个分割对应于所有分支中的相同预测因子或空间模式子集。
实际发生了什么：
我观察到在第二次分割中，左分支和右分支之间的灰色区域不同。在左侧，灰色区域对应于第一个预测因子，而在右侧，它转移到第二个预测因子。我不确定这种差异是由于空间异质性、独立预测因子选择还是其他原因造成的。]]></description>
      <guid>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</guid>
      <pubDate>Thu, 21 Nov 2024 03:11:43 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>如何构建更高效的 DataLoader 来加载大型图像数据集？</title>
      <link>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</link>
      <description><![CDATA[我正在尝试在一个非常大的图像数据集上训练深度学习模型。模型输入需要一对图像（A 和 B）。由于我的图像尺寸非常大，我已将每个图像调整为形状为 (3x224x224) 的 torch.Tensor，并将每对图像作为单独的文件存储在我的磁盘上。相同的对共享相同的索引。
但是，当使用数据集和 DataLoader 将这些文件加载​​到内存中时，我遇到了以下问题：

CPU 内存问题：将工作器数量设置为 12 时，200GB 内存很快就会耗尽。我尝试设置 prefetch_factor=1，但没有帮助。
初始化缓慢：在训练开始之前，每个 epoch 之前都需要很长时间进行初始化。我在之前的帖子中看到，这可能是由于初始化的开销造成的。我设置了 persistent_workers=True，但也没有帮助。
GPU 和批次大小：我使用 4 个 GPU 进行 DDP 训练，当前批次大小为 1024。

有没有关于如何提高数据集或 DataLoader 效率的建议？

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
std=[0.229, 0.224, 0.225])

augmentation = transforms.Compose([
transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)], p=0.8),
transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))], p=0.5),
transforms.RandomGrayscale(p=0.1),
transforms.RandomVerticalFlip(p=0.5),
normalize,
])

class ImagePairDataset(Dataset):
def __init__(self, data_save_folder, dataset_name, num_samples, transform=None):
&quot;&quot;&quot;
Args:
data_save_folder (str)：包含数据文件的文件夹路径。
dataset_name (str)：表示数据集拆分的“train”、“val”或“test”之一。
num_samples (int)：数据集拆分中的样本数。 (train: 3000000, val: 10000, test: 10000)
transform (可调用，可选)：应用于图像张量的可选变换。
&quot;&quot;&quot;
self.data_save_folder = data_save_folder
self.dataset_name = dataset_name
self.num_samples = num_samples
self.transform = transform

def __len__(self):
return self.num_samples

def __getitem__(self, idx):

# 根据 idx 构建文件路径
A_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_A_images_{idx}.pt&quot;
B_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_B_images_{idx}.pt&quot;
label_path = f&quot;{self.data_save_folder}/{self.dataset_name}_labels_{idx}.pt&quot;

# 从文件路径加载张量
A_image = torch.load(A_image_path)
B_image = torch.load(B_image_path)
label = torch.load(label_path)

# 如果可用，则应用转换
if self.transform:
A_image = self.transform(A_image)
B_image = self.transform(B_image)

return A_image, B_image, label

class ImagePairDataModule(pl.LightningDataModule):

def __init__(self, data_save_folder, train_samples, val_samples, test_samples, batch_size=32, num_workers=4):
super().__init__()
self.data_save_folder = data_save_folder
self.train_samples = train_samples
self.val_samples = val_samples
self.test_samples = test_samples
self.batch_size = batch_size
self.num_workers = num_workers
self.train_transform = augmentation
self.eval_transform = normalize # 仅对验证和测试进行标准化

def setup(self, stage=None):

self.train_dataset = ImagePairDataset(self.data_save_folder, &#39;train&#39;, self.train_samples, transform=self.train_transform)
self.val_dataset = ImagePairDataset(self.data_save_folder, &#39;val&#39;, self.val_samples, transform=self.eval_transform)
self.test_dataset = ImagePairDataset(self.data_save_folder, &#39;test&#39;, self.test_samples, transform=self.eval_transform)

def train_dataloader(self): #prefetch_factor=1, , persistent_workers=True
return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers) 

def val_dataloader(self):
return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

# 初始化 DataModule
data_module = ImagePairDataModule(
data_save_folder=args.data_save_folder,
train_samples=train_samples,
val_samples=val_samples,
test_samples=test_samples,
batch_size=args.batch_size,
num_workers=12,
)
]]></description>
      <guid>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</guid>
      <pubDate>Wed, 20 Nov 2024 20:12:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gamma=0 的二元焦点交叉熵总是会产生 nan 损失？</title>
      <link>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</link>
      <description><![CDATA[我正在训练一个 U-Net 来对我们的实验图像进行二值化。但前景通常没有得到很好的体现，换句话说，我有类别不平衡，网络学习得不好。我一直在使用 BinaryCrossEntropy 作为损失函数。所以，我明白解决这个问题的一个简单方法是定义一个自定义的损失函数，为每个类别赋予权重。但我在这样做时遇到了一些问题，所以放弃了这个尝试。对我来说，使用 BinaryFocalCrossEntropy 似乎更简单，它的表达式为（如果我理解得好的话）

所以，我的计划是使用 gamma=0，这样我就可以通过调整 alpha 值来给出类别权重。但是，我不断得到 nan 损失。它发生在几个批次之后的第一个时期内：（这里我使用 \alpha = 0.75）

在这里我使用了 adam 优化器和 Learning_rate 1e-3。我注意到，如果我改用 1e-4，即使 nan 仍然出现，它也会再出现几个批次。你能帮我找出这是怎么回事，我该如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</guid>
      <pubDate>Wed, 20 Nov 2024 15:49:20 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的损失来训练不同阶段的模型</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[我正在尝试以端到端的方式训练一个两阶段模型。但是，我想用不同的损失更新模型的不同阶段。例如，假设端到端模型由两个模型组成：model1 和 model2。输出是通过运行计算的
features = model1(inputs)
output = model2(features)

我想用 loss1 更新 model1 的参数，同时保持 model2 的参数不变。接下来，我想用 loss2 更新 model2 的参数，同时保持 model1 的参数不变。我的完整实现如下：
import torch
import torch.nn as nn

# 定义第一个模型
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# 定义第二个模型
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# 初始化模型
model1 = Net()
model2 = Net1()

# 初始化单独的每个模型的优化器
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# 样本输入和标签
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 

但是，这将返回
回溯（最近一次调用最后一次）：
文件，第 55 行，在 &lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, 第 521 行, 在反向传播中
torch.autograd.backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, 第 289 行, 在反向传播中
_engine_run_backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, 第 769 行, 在 _engine_run_backward 中
return Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传播
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]（AsStridedBackward0 的输出 0）处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

我有点明白为什么会发生这种情况，但有办法解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用线性回归模型预测股票价格时准确率能达到 100%？</title>
      <link>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</link>
      <description><![CDATA[我正在尝试使用线性回归模型在 Python 中预测股票价格。我使用 train_test_split 分割数据，因此据我所知，我的测试数据不应该在我的训练数据中，所以我不明白为什么模型的准确率是 100%。
这是我的代码：
X = RMV.drop(&#39;Close&#39;, axis=1)
y = RMV[&#39;Close&#39;]`

来自 sklearn.model_selection 导入 train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

来自 sklearn.linear_model 导入 LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

reg_preds = reg.predict(X_test)

当我使用此代码运行交叉验证以测试准确性时，我得到的值为1.00。
scores = model_selection.cross_val_score(reg, X_test, y_test, cv=10)
print (&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() / 2)) 

作为参考，下面是我使用的数据样本：
 收盘价 SMA EMA MACD 上轨 中轨 下轨 RSI
日期 
2010-02-18 60.900002 57.335715 57.419887 2.099073 64.842238 55.4075 45.972762   60.517959
2010-02-19 61.000000 57.967857 57.897236 2.215288 65.422290 55.9000 46.377710 60.672590
2010-02-22 62.099998 58.560714 58.457604 2.368843 66.047128 56.4675 46.887872 62.416318
2010-02-23 61.200001 59.117857 58.823257 2.390360 66.386746 57.0000 47.613254 60.069541
2010-02-24 60.900002 58.539286 59.100156 2.356046 66.504379 57.5425 48.580621 59.269579

我哪里错了？
更新：准确度似乎是错误的指标，因此我已按照回复的建议改用 MSE：
print(&#39;均方误差：&#39;, metrics.mean_squared_error(y_true=y_test, y_pred=lm_preds))
print(&#39;判定系数：%.2f&#39; % metrics.r2_score(y_true=y_test, y_pred=lm_preds))

根据运行情况，这给了我大约 MSE = 13-15，R2 = 0.999，这仍然非常高。由于平均股价在 600 左右，MSE 实际上并没有看起来那么高。该模型似乎仍然表现得太好了。
我使用的是 2010-2020 年的 Rightmove 股票数据。我刚刚切换到使用 2010-2020 年和 2019-2020 年波动性更大的股票 (PMO.L)，并且我还删除了我使用的 5/7 个指标。
对于 2010-2020 年，该模型给出的 MSE 为 69（与股价相比相对较低）和 0.999 R2。然而，对于 2019-2020 年，该模型确实似乎有点差，MSE 为 15.5，R2 为 0.82，明显低于以前。然而，考虑到这只是一年的数据，它的表现似乎仍然太好了。
以下是用于训练新股票模型的特征数据样本：
2010-2020：
 SMA EMA
日期
2010-02-18 266.214286 266.857731
2010-02-19 266.910714 268.110034
2010-02-22 267.303571 269.428696
2010-02-23 267.589286 269.838203
2010-02-24 264.660714 270.659776

2011-2020:
 SMA EMA
日期
2019-02-18 73.425000 73.791397
2019-02-19 73.632143 74.052544
2019-02-20 73.785715 74.325538
2019-02-21 73.953572 74.335466
2019-02-22 73.928572 74.330738
]]></description>
      <guid>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</guid>
      <pubDate>Thu, 11 Feb 2021 12:40:43 GMT</pubDate>
    </item>
    <item>
      <title>如何正确地将不平衡的数据集拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</link>
      <description><![CDATA[我有一个航班延误数据集，在采样之前尝试将该数据集拆分为训练集和测试集。准时情况约占总数据的 80%，延误情况约占 20%。
通常，机器学习中训练集和测试集的大小比例为 8:2。但数据太不平衡了。因此，考虑到极端情况，大多数训练数据都是准时情况，而大多数测试数据都是延误情况，准确率会很差。
所以我的问题是如何正确将不平衡的数据集拆分为训练集和测试集？]]></description>
      <guid>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</guid>
      <pubDate>Sat, 27 Jul 2019 06:34:52 GMT</pubDate>
    </item>
    </channel>
</rss>