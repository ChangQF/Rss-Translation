<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 06 Nov 2024 15:17:56 GMT</lastBuildDate>
    <item>
      <title>YOLOX 不在 mmengine::model 注册表中</title>
      <link>https://stackoverflow.com/questions/79163058/yolox-is-not-in-the-mmenginemodel-registry</link>
      <description><![CDATA[这是我的配置文件：
model = dict(
type=&#39;YOLOX&#39;, # YOLOX 架构
backbone=dict(type=&#39;CSPDarknet&#39;, deep_factor=1.0, widen_factor=1.0), # YOLOX 主干
neck=dict(type=&#39;YOLOXPAFPN&#39;, in_channels=[256, 512, 1024], out_channels=[256, 512, 1024]),
bbox_head=dict(
type=&#39;YOLOXHead&#39;,
num_classes=1, # 根据数据集中的类数更新此文件
in_channels=256,
feat_channels=256
),
train_cfg=dict(assigner=dict(type=&#39;SimOTAAssigner&#39;, center_radius=2.5)),
test_cfg=dict(score_thr=0.01, nms=dict(type=&#39;nms&#39;, iou_threshold=0.65))
)

以下代码列出了 YOLOX：
from mmdet.registry import MODELS

# 打印注册表中所有可用模型
print(MODELS.module_dict.keys())

但是，运行此代码：
from mmengine.config import Config
from mmengine.runner import Runner
from mmdet.utils import register_all_modules

# 注册 MMYOLO 和 MMDetection 的所有模块
register_all_modules()

def train_model(config_file):
# 加载配置
cfg = Config.fromfile(config_file)

# 确保检查点和日志的工作目录
cfg.work_dir = &#39;./checkpoints&#39; 

# 构建运行器
runner = Runner.from_cfg(cfg)

# 开始训练
runner.train()
print(&quot;训练完成！检查点已保存到 &#39;./checkpoints&#39;。&quot;)

# 配置文件路径
config_file = &#39;configs/yolov7/yolox_subset_coco.py&#39;

# 训练模型
train_model(config_file)

产生此错误：
KeyError: &#39;YOLOX 不在 mmengine::model 注册表中。请检查 `YOLOX` 的值是否正确或是否按预期注册。更多详细信息请参阅 https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module&#39;

YOLOv7 模型也是如此，事实上，这也是我想要使用的模型。您认为这里的错误是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79163058/yolox-is-not-in-the-mmenginemodel-registry</guid>
      <pubDate>Wed, 06 Nov 2024 14:32:15 GMT</pubDate>
    </item>
    <item>
      <title>AWS Sagemaker ClientError：未指定训练通道（清单文件错误）</title>
      <link>https://stackoverflow.com/questions/79162377/aws-sagemaker-clienterror-train-channel-is-not-specified-manifest-file-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79162377/aws-sagemaker-clienterror-train-channel-is-not-specified-manifest-file-error</guid>
      <pubDate>Wed, 06 Nov 2024 11:22:30 GMT</pubDate>
    </item>
    <item>
      <title>如何获取使用字典作为输入的 PyTorch 模型的摘要</title>
      <link>https://stackoverflow.com/questions/79162161/how-to-get-a-summary-of-a-pytorch-model-that-uses-dictionary-as-an-input</link>
      <description><![CDATA[我的模型以字典作为输入，例如x = {&#39;image&#39;: torch.tensor, &#39;number&#39;: torch.tensor}，模型如下所示：
class MyModel(nn.Module):
def __init__(self):
super(MyModel, self).__init__()
self.imgmodule = ImgModule()
self.nummodule = NumModule()
self.predict = nn.Linear(input_size, 100)

def forward(self, x):
xImg = self.imgmodule(x[&#39;image&#39;])
xNum = self.nummodule(x[&#39;number&#39;])
x = self.predict(torch.cat([xImg, xNum], dim=1))
return x 

如何获取模型摘要，类似于 pytorch-summary 包中提供的摘要？
到目前为止，我尝试过像这样使用它这个：
来自 torchsummary 导入 summary
summary(model, input_size=[(3, 224, 224), (1, )])

但我收到错误：
TypeError：MyModel.forward() 需要 2 个位置参数，但给出了 3 个
]]></description>
      <guid>https://stackoverflow.com/questions/79162161/how-to-get-a-summary-of-a-pytorch-model-that-uses-dictionary-as-an-input</guid>
      <pubDate>Wed, 06 Nov 2024 10:22:03 GMT</pubDate>
    </item>
    <item>
      <title>比较两幅相似图像的有效方法</title>
      <link>https://stackoverflow.com/questions/79162036/efficient-way-to-compare-two-similar-images</link>
      <description><![CDATA[我想识别图像中的方框。我有一个这些方框的数据库，存储它们的 ocr 和图像。我使用 ocr 搜索并粗略地转换了脸部。大多数时候它都能正常工作，但有时会返回错误的脸部和错误的转换。由于我有源图像，我想利用它们来评估搜索识别结果。我将检测到的方框区域转换为源图像并将它们调整为相同大小（因此它们从相似的角度看，大小也相似）。我使用 hog、alexnet 的倒数第二层、vitmae 和我自己训练的卷积网络作为嵌入特征。但它们都不太好用。我也尝试了关键点特征。但它花费的时间比要求的要长得多。当区分具有相同字体但不同大小的脸部时，它也会失败。
还有其他有效的方法来比较两张相似的图像吗？]]></description>
      <guid>https://stackoverflow.com/questions/79162036/efficient-way-to-compare-two-similar-images</guid>
      <pubDate>Wed, 06 Nov 2024 09:41:04 GMT</pubDate>
    </item>
    <item>
      <title>对特定层的参数进行自动求导</title>
      <link>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</link>
      <description><![CDATA[我正在尝试获取特定层参数的雅可比矩阵。下面是我的网络模型，我在其上应用了 functional_call。
def fm(params, input):
return functional_call(self.model, params, input.unsqueeze(0)).squeeze(0)

def floss(func_params, input):
fx = fm(func_params, input)
return fx

我过去常常用这种方式计算所有参数的雅可比矩阵
func_params = dict(self.model.named_pa​​rameters())
per_sample_grads = vmap(jacrev(floss, 0), (None, 0))(func_params, input)

现在，我只需要获取特定层参数的梯度，这是我的方法。
def grad(f, param):
return torch.autograd.grad(f, param) 

out = vmap(floss, (None, 0))(func_params,input)
gradf = vmap(grad, (0, None))(out, func_params[&#39;model.0.weight&#39;])

但是，错误提示“张量的元素 0 不需要 grad，也没有 grad_fn”
因为，我已经尝试过了
grad = self.grad(out[0], func_params[&#39;model.0.weight&#39;])

并且成功了。我真的不知道如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</guid>
      <pubDate>Wed, 06 Nov 2024 04:19:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道现有 chromadb 集合正在使用什么嵌入模型？</title>
      <link>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</link>
      <description><![CDATA[我正在研究 chromadb。当我处理一些现有的集合时，我总是遇到错误：

chromadb.errors.InvalidDimensionException：嵌入维度 384 与集合维度 4096 不匹配

我知道这是因为我的嵌入模型与集合创建时选择的嵌入模型不匹配。
所以我想知道如何检查现有集合正在使用什么嵌入模型？]]></description>
      <guid>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</guid>
      <pubDate>Wed, 06 Nov 2024 03:17:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们需要在 keras model.fit() 中使用 y 变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-variable-in-keras-model-fit</link>
      <description><![CDATA[我正在处理手写数字数据集。数据加载如下：
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

这是为对数字进行分类而创建的神经网络的代码：
model = keras.Sequential([
keras.layers.Dense(10, input_shape=(784,),activation=&#39;sigmoid&#39;)
])

model.compile(
optimizer=&#39;adam&#39;, 
loss = &#39;sparse_categorical_crossentropy&#39;,
metrics = [&#39;accuracy&#39;]
)
model.fit(X_train_flattened, y_train, epochs=5)

问题是，model.fit() 中的 y_train 的作用是什么。这似乎是一个分类问题，网络只需要输入（x_train_flattened）即可进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-variable-in-keras-model-fit</guid>
      <pubDate>Tue, 05 Nov 2024 11:56:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 JAX 训练模型时跟踪测试/验证损失</title>
      <link>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</guid>
      <pubDate>Tue, 05 Nov 2024 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>Weka RandomForest m_Classifiers 为空且仅在构建 Spring Boot 项目后才出现 NullPointerException 和无法找到允许的类错误</title>
      <link>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</guid>
      <pubDate>Tue, 05 Nov 2024 07:17:06 GMT</pubDate>
    </item>
    <item>
      <title>Unet 输入和输出形状之间的差异</title>
      <link>https://stackoverflow.com/questions/79157733/unet-discrepency-between-input-and-output-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79157733/unet-discrepency-between-input-and-output-shape</guid>
      <pubDate>Tue, 05 Nov 2024 05:15:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用函数 shap.Explainer 会根据输入的不同顺序获得不同的 shap 值？</title>
      <link>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</link>
      <description><![CDATA[我训练了一个二分类模型，并想使用 shap.Explainer 来分析特征贡献。
代码如下：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])

shap_values.values 的结果如下：




Feature 1
...




sample 0
-0.009703
...


样本 1
-0.009297
...


样本 2
-0.007699
...


样本 3
0.032624
...



但是当输入顺序改变时：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])

样本 0 和样本 1 的结果已更改：




特征 1
...




样本 1
-0.010012
...


样本0
-0.008277
...


样本 2
-0.007699
...


样本 3
0.032624
...



我不知道有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</guid>
      <pubDate>Sun, 03 Nov 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何对多个特征应用多个估计量来选择具有最高 f1 分数的组合？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</link>
      <description><![CDATA[我想对多个特征使用多个估计器算法运行递归特征消除，并在测试数据上保留最高的 f1 分数组合。
如何创建一个代码，可以在测试数据上生成并显示具有最佳算法的最佳特征数量（最高 f1，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
我尝试了下面的代码，它可以为不同数量的特征生成不同估计器的结果。但我仍然需要逐一查看结果。如何做到这一点？
estimators = [(&#39;Logistic Regression&#39;, LogisticRegression()),
(&#39;Random Forest&#39;, RandomForestClassifier())]
num_features_to_select = [4,5,7,9,11,15]

for estimator_name, estimator in estimators:
for n_features in num_features_to_select:
# 创建 RFE 对象
rfe = RFE(estimator=estimator, n_features_to_select=n_features)
# 将 RFE 与数据拟合
rfe.fit(X_resampled,Y_resampled)
# 获取选定的特征
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, Thresholds = roc_curve(Y_test, pred_test)
precision, recall, Thresholds = precision_recall_curve(Y_test, pred_test)
# 使用交叉验证评估模型性能
scores = cross_val_score(estimator, X_resampled[selected_features], Y_resampled, cv=5)
# 打印结果
print(f&#39;Estimator: {estimator_name}, 特征数量: {n_features}, 平均 CV 分数: {scores.mean()}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，ROC：{logit_roc_auc}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，f1 分数：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，PRC AUC：{auc(recall,precision)}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;What is the Total Spend&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降线性回归后非正则化 theta</title>
      <link>https://stackoverflow.com/questions/59015958/denormalizing-thetas-after-a-linear-regression-with-gradient-descent</link>
      <description><![CDATA[我有以下一组数据：
km,price
240000,3650
139800,3800
150500,4400
185530,4450
176000,5250
114800,5350
166800,5800
89000,5990
144500,5999
84000,6200
82029,6390
630 60,6390
74000,6600
97500,6800
67000,6800
76025,6900
48235,6900
93000,6990
60949,7490
65674,7555
54000,7990
68500,7990
22899,7990
61789,8290

之后对它们进行归一化，我执行梯度下降，得到以下 theta：
θ0 = 0.9362124793084768
θ1 = -0.9953762249792935

如果我输入归一化里程，然后对预测价格进行非归一化，我可以正确预测价格，即：
50000 公里里程的要价：
归一化里程：0.12483129971764294
归一化价格：(mx + c) = 0.8119583714362707
实际价格：7417.486843464296

我正在寻找的是恢复我的西塔回到非标准化值，但无论我尝试哪个方程，我都无法做到这一点。有办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/59015958/denormalizing-thetas-after-a-linear-regression-with-gradient-descent</guid>
      <pubDate>Sun, 24 Nov 2019 08:36:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 对文本进行标记</title>
      <link>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</link>
      <description><![CDATA[我有以下代码从一组文件（文件夹名称是类别名称）中提取特征以进行文本分类。
import sklearn.datasets
from sklearn.feature_extraction.text import TfidfVectorizer

train = sklearn.datasets.load_files(&#39;./train&#39;, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decrypt_error=&#39;strict&#39;, random_state=0)
print len(train.data)
print train.target_names

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train.data)

它抛出以下堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“C:\EclipseWorkspace\TextClassifier\main.py”，第 16 行，位于&lt;module&gt;
X_train = vectorizer.fit_transform(train.data)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 1285 行，位于 fit_transform
X = super(TfidfVectorizer, self).fit_transform(raw_documents)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 804 行，位于 fit_transform
self.fixed_vocabulary_)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 739 行，位于 _count_vocab
for feature in analyze(doc):
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 236 行，位于 &lt;lambda&gt;
tokenize(preprocess(self.decode(doc))), stop_words)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 113 行，在解码中
doc = doc.decode(self.encoding, self.decode_error)
文件“C:\Python27\lib\encodings\utf_8.py”，第 16 行，在解码中
return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: &#39;utf8&#39; 编解码器无法解码位置 32054 中的字节 0xff：起始字节无效

我运行的是 Python 2.7。我该如何让它工作？
编辑：
我刚刚发现，这对于使用 utf-8 编码的文件非常有效（我的文件是 ANSI 编码）。有什么方法可以让 sklearn.datasets.load_files() 使用 ANSI 编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</guid>
      <pubDate>Fri, 01 May 2015 00:39:09 GMT</pubDate>
    </item>
    </channel>
</rss>