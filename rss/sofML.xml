<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 16 Jul 2024 21:15:23 GMT</lastBuildDate>
    <item>
      <title>词到词机器翻译的评估指标/算法</title>
      <link>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</link>
      <description><![CDATA[我正在寻找一种科学合理的方法来评估我的学士论文的单词翻译。在寻找的过程中，我发现大多数方法都是基于 n-gram 精度来进行句子评估。我正在将数据库列和表名从英语翻译成德语。我可以选择包含基本事实，但我更喜欢不包含它的方法。我的翻译和原文也包含缩写。
现在我使用 OpenAIs 最新的嵌入模型，然后计算嵌入之间的距离。但结果很大程度上取决于模型。我研究过使用字符 n-gram 的 ChrF。但如上所述，不需要基本事实的方法会更理想。]]></description>
      <guid>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</guid>
      <pubDate>Tue, 16 Jul 2024 19:42:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 GCP TPU 进行多处理，但 shell 意外死机</title>
      <link>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</link>
      <description><![CDATA[我可以访问美国地区的 32 个可抢占 Cloud TPU v4 芯片，并尝试使用以下 Python 代码运行我的 PyTorch 模型：
import os 
import sys 
import pickle 
import torch_xla.distributed.xla_multiprocessing as xmp 
from transformers 
import AutoTokenizer 
import main_utils as utils 
import multiprocessing as mp

lock = mp.Manager().Lock()

def _mp_fn(i): 

A_tasks, B_tasks, desire_output_lengths, keys = utils.get_total_tasks()

import torch
import torch_xla.core.xla_model as xm
from models_mamba import MambaForCausalLM

DEVICE_NAME = xm.xla_device()
tokenizer = AutoTokenizer.from_pretrained(&quot;~/Mamba-1B/&quot;)
model = MambaForCausalLM.from_pretrained(&quot;~/Mamba-1B/&quot;, torch_dtype=&quot;auto&quot;,
device_map=&quot;auto&quot;, low_cpu_mem_usage=True)
model = model.to_empty(device=&#39;cpu&#39;)
model.apply(lambda module: module.reset_parameters() if hasattr(module, &#39;reset_parameters&#39;) else None)
model = model.to(xm.xla_device())

params_to_save = [&quot;out_proj_y&quot;]

def generate_logits(task, logits_list, desire_output_length):
for i in range(48):
layer_to_save = [i + 1]
input_ids = tokenizer.encode(task, return_tensors=&quot;pt&quot;).to(设备名称)
model.saved_activation(params_to_save, layer_to_save, precision=&#39;r&#39;)
output = model.generate(input_ids, max_length=desired_output_length, no_repeat_ngram_size=2)

saved_dict = model.reset_everything_and_save()
param = saved_dict[f&quot;out_proj_y_{i + 1}&quot;][0, -1, :].to(设备名称)

xm.all_gather(param)
logits = model.get_unembed_for_layer(param, norm=&quot;False&quot;)

xm.all_gather(logits)
logits_list.append(logits.to(&quot;cpu&quot;))

del input_ids、output、saved_dict、param、logits

task_dict = {}
for A_task、B_task、dol、key in zip(A_tasks、B_tasks、desired_output_lengths、keys):
A_logits = []
B_logits = []
generate_logits(A_task、A_logits、dol)
generate_logits(B_task、B_logits、dol)

aux_dict = {&#39;A&#39;: A_logits, &#39;B&#39;: B_logits}
task_dict[key] = aux_dict

device = xm.xla_device()
with lock:
print(f&#39;Process {i}, Device {device}&#39;)

xm.mark_step()
with open(f&#39;~/main_file.pickle&#39;, &#39;wb&#39;) as handle:
pickle.dump(task_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

if name == &quot;main&quot;: 
xmp.spawn(_mp_fn, args=(), nprocs=8, start_method=&#39;fork&#39;)`

要执行此代码，我使用 Cloud Shell 和以下命令（设置 TPU 并在所有 8 个工作器上安装库之后）：
gcloud compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE} --project=${PROJECT_ID} --worker=all --command=&quot;PJRT_DEVICE=TPU python3 ~test.py&quot;
我希望模型在所有 TPU 上执行 _mp_fn() 中定义的任务，按照指定的方式收集和保存 logit，最后将结果存储在 main_file.pickle 中。但是，在 Cloud Shell 中启动命令后，shell 在模型加载期间打印了一些预期的警告，但没有继续进行。相反，过了一段时间，shell 会话意外终止。我不确定如何排除故障？]]></description>
      <guid>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</guid>
      <pubDate>Tue, 16 Jul 2024 17:38:54 GMT</pubDate>
    </item>
    <item>
      <title>我如何访问这些元素？[关闭]</title>
      <link>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</link>
      <description><![CDATA[LIVEKIT_URL= 

LIVEKIT_API_KEY= 

LIVEKIT_API_SECRET= 

DEEPGRAM_API_KEY= 

在此处输入图片描述 来自我的帐户的与项目相关的屏幕截图
页面顶部的 URL 是吗？我无法在 LIVEKIT 上创建 API 密钥。
这是与项目相关的 YouTube 链接。
https://www.youtube.com/watch?v=nvmV0a2geaQ]]></description>
      <guid>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</guid>
      <pubDate>Tue, 16 Jul 2024 15:20:52 GMT</pubDate>
    </item>
    <item>
      <title>使用张量流的问题（既不包含“saved_model.pb”也不包含“saved_model.pbtxt”）</title>
      <link>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</link>
      <description><![CDATA[我尝试运行代码，但出现错误。
ValueError：尝试加载不兼容/未知类型的模型。&#39;C:\Users\pm23821\AppData\Local\Temp\tfhub_modules\9616fd04ec2360621642ef9455b84f4b668e219e&#39; 既不包含 &#39;saved_model.pb&#39; 也不包含 &#39;saved_model.pbtxt&#39;

这是我第一次使用 tensorflow 和模型，所以我不知道我需要做什么才能运行我的代码。
目标是找到声音之间的差异，当检测到紧急警报器（警察、消防部门和医院）时，它需要看起来像已被识别一样。当听到家用警报器或鹦鹉的声音时，应该看起来它还没有被识别。
# 加载 YAMNet 模型
model = hub.load(&#39;https://tfhub.dev/google/yamnet/1&#39;)

# 加载 YAMNet 中的类名
def load_class_names(csv_path=&#39;yamnet_class_map.csv&#39;):
class_map = pd.read_csv(csv_path, sep=&#39;,&#39;, header=None)
class_names = class_map[1].tolist()
return class_names

class_names = load_class_names()

# 麦克风音频捕获函数
def record_audio(duration=5, fs=16000): # 录制时长为 5 秒
recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=&#39;float32&#39;)
sd.wait()
return recording.flatten()

# 预测音频类的函数
def predict_sound(audio_data):
scores, embeddings, spectrogram = model(audio_data)
prediction = np.argmax(scores, axis=1)[0]
return class_names[prediction]

# 循环主函数
while True:
print(&quot;Escutando...&quot;)
audio_data = record_audio()
audio_data /= np.max(np.abs(audio_data))

predict_class = predict_sound(audio_data)
print(&quot;预测的类：&quot;, predict_class)

if “/m/012n7d”在 predict_class 或 “/m/03j1ly” 中在predicted_class或“/m/03kmc9”中在预测类中：
print(&quot;Sirene 检测到！&quot;)
else:
print(&quot;Não é uma sirene.&quot;)
time.sleep(1) # 暂停 1 秒，直到下一个重击开始 
]]></description>
      <guid>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</guid>
      <pubDate>Tue, 16 Jul 2024 14:11:25 GMT</pubDate>
    </item>
    <item>
      <title>如何将每小时数据转换为每日最大值、平均值……和最小值</title>
      <link>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</link>
      <description><![CDATA[这是每小时的 TMP 数据，因此想将其转换为每日的最大值、平均值和最小值，我在 excel 中做过一次，但结果不合理。
此数据需要预处理，因此我只是应用了一些函数，例如查找平均值，我只是使用函数平均值来获取一天的平均数据，但结果并不好，因此我需要一些其他准确且最佳的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</guid>
      <pubDate>Tue, 16 Jul 2024 14:03:10 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和跨摄像头跟踪物体</title>
      <link>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</link>
      <description><![CDATA[我正在使用 YOLOV10 检测和深度排序来跟踪物体，当我们在第一个 CCTV 摄像机中时，它工作正常，当我们到达下一个 CCTV 摄像机时，物体来自不同的视角，类似的物体也很多，我将跟踪更改为 YOLOV10 本身，它们都无法解决在第二个摄像机中识别第一个摄像机中跟踪的物体的问题，因为第二个摄像机中的大小、视点和光照都发生了变化，现在如何给出与模型在第一个摄像机中跟踪的物体相同的 id，第二个摄像机中的 id 为 13，当物体到达时，它应该具有相同的 id，模型应该知道视点比例和光照是否也发生了变化
这个物体应该匹配
使用这个，而那里也有相同类型的汽车
在第一个摄像头中，物体向东行走，但在第二个摄像头中，物体从它的南边过来，所以我们无法跟踪位置
from ultralytics import YOLO
import cv2

# 全局变量
selected_bbox = None
tracking_id = None

# 鼠标回调函数用于选择要跟踪的对象
def select_object(event, x, y, flags, param):
global selected_bbox, tracking_id
if event == cv2.EVENT_LBUTTONDOWN:
for result in param:
bboxes = result.boxes.xyxy.cpu().numpy() # 边界框
ids = result.boxes.id.cpu().numpy() # 跟踪 ID
for bbox, id_ in zip(bboxes, ids):
if bbox[0] &lt;= x &lt;= bbox[2] and bbox[1] &lt;= y &lt;= bbox[3]:
selected_bbox = bbox
tracking_id = id_
return

print(&quot;starting&quot;)
# 加载 YOLOv10 模型
model = YOLO(&#39;yolov10m.pt&#39;)

# 加载视频
video_path = &#39;videos/1.mp4&#39;
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.100:554&quot;)

cv2.namedWindow(&#39;frame&#39;)
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=None)

ret = True
results = []

# 读取帧
while ret:
ret, frame = cap.read()

if ret:
# 检测和跟踪对象
results = model.track(frame, persist=True)

# 设置鼠标回调参数
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=results)

if tracking_id is not None:
# 为所选对象绘制边界框
for result in results:
bboxes = result.boxes.xyxy.cpu().numpy()
ids = result.boxes.id.cpu().numpy()
for bbox, id_ in zip(bboxes, ids):
if id_ == tracking_id:
cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)
cv2.putText(frame, f&quot;ID: {int(id_)}&quot;, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)

# 可视化
cv2.imshow(&#39;frame&#39;, frame)
if cv2.waitKey(25) &amp; 0xFF == ord(&#39;q&#39;):
break

else:
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.101:554&quot;)
ret = True

cap.release()
cv2.destroyAllWindows()
`
]]></description>
      <guid>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</guid>
      <pubDate>Tue, 16 Jul 2024 12:40:28 GMT</pubDate>
    </item>
    <item>
      <title>尝试在多 GPU 设置上训练机器翻译的 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</link>
      <description><![CDATA[我正在尝试训练机器翻译的变换模型。这是训练函数：
在单个 GPU 上训练时，我得到以下结果：
for src, tgt in train_dataloader:
try:
src = src.to(DEVICE)
tgt = tgt.to(DEVICE)

tgt_input = tgt[:-1, :]

src_mask, tgt_mask, \
src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

logits = model( src, tgt_input,
src_mask, tgt_mask,
src_padding_mask, tgt_padding_mask,
src_padding_mask
)

在Seq2SeqTransformer:
class Seq2SeqTransformer(nn.Module):
def forward( self,
src: Tensor,
trg: Tensor,
src_mask: Tensor,
tgt_mask: Tensor,
src_padding_mask: Tensor,
tgt_padding_mask: Tensor,
memory_key_padding_mask: Tensor ):

src_emb = self.positional_encoding(self.src_tok_emb(src))
tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )
return self.generator(outs)

在此行：
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )

我明白了：
&gt; sec_emb 的大小为 (31,31) tgt_emb 的大小为 (37,37)

训练运行顺利。
现在，我尝试使用以下设置在具有 8 个 GPU 的计算机上进行训练：
 transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

在调试模式下，我检查此行中的值：
 outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )


我明白了：
&gt; sec_emb 的大小为 (4,31) tgt_emb 的大小为 (5,37)

我认为这是因为事情是并行进行的。
但是，我遇到了此错误消息：
&gt; 文件
&gt; &quot;C:\Projects\MT005\.venv\Lib\site-packages\torch\nn\ functional.py&quot;,
&gt; 第 5382 行，在 multi_head_attention_forward 中
&gt;引发 RuntimeError(f&quot;2D attn_mask 的形状为 {attn_mask.shape}，但应为 {correct_2d_size}。&quot;) RuntimeError:
&gt; 2D attn_mask 的形状为 torch.Size([4, 31])，但应为
&gt; (4, 4)。

有人可以帮助我或提供一些指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</guid>
      <pubDate>Tue, 16 Jul 2024 12:00:27 GMT</pubDate>
    </item>
    <item>
      <title>优化 pgvector 以实现多用户文档存储：索引和分区的最佳实践</title>
      <link>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</link>
      <description><![CDATA[我使用 PostgreSQL 和 pgvector 以及 LangChain 进行文档存储和检索。我的设置：

许多用户，每个用户上传多个文档
文档具有带 userId 的元数据
需要按用户隔离用户文档并按 userId 进行过滤
使用 LangChain 进行带过滤器的检索

当前表结构：
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
);

优化可扩展性和查询性能的最佳方法是什么？

为每个用户创建一个新表？
添加 user_id 列并对表进行分区？
索引元数据 - &gt;&gt;&#39;userId&#39;？
另一种方法？

寻找一种可扩展性好且在使用用户特定过滤器进行向量相似性搜索时保持良好性能的解决方案。
感谢您的帮助！
我尝试根据存储在元数据 JSONB 字段中的 userId 在 document_vector 表上实现哈希分区，但不确定这是否是正确的方法。
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
) PARTITION BY HASH ((metadata-&gt;&gt;&gt;&#39;userId&#39;));
]]></description>
      <guid>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</guid>
      <pubDate>Tue, 16 Jul 2024 11:33:47 GMT</pubDate>
    </item>
    <item>
      <title>t-SNE 中的不同结果</title>
      <link>https://stackoverflow.com/questions/78754318/different-result-in-t-sne</link>
      <description><![CDATA[有人能帮我回答我的问题吗？
所以我从纸上获取数据集并使用 t-SNE 进行可视化。另一方面，我尝试按照之前的数据集特征创建新的数据集。但是在 t-SNE 上绘制新数据集后，我得到了相反的结果。您可以在此图中看到

我感到困惑的是，视觉效果不应该是相同的吗？因为我提取的方式和特征是相同的？或者至少它们对于标签位置的方向是相同的。在 result-2021 中，标签 0 在左边，标签 1 在右边，但在 dataset_full 中，标签 0 在右边，但标签 1 在左边]]></description>
      <guid>https://stackoverflow.com/questions/78754318/different-result-in-t-sne</guid>
      <pubDate>Tue, 16 Jul 2024 11:31:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么我要将 Python 包安装到我的 Machine_Learning 环境中，但它们却是在本地安装的？[关闭]</title>
      <link>https://stackoverflow.com/questions/78753784/why-am-i-installing-python-packages-into-my-machine-learning-environment-but-th</link>
      <description><![CDATA[在此处输入图片说明在此处输入图片说明在此处输入图片说明
如图所示，我使用
“(D:\conda_envs\Machine_Learning) C:\Users\GuYuanji&gt;python -m pip install -r &quot;C:\Users\GuYuanji\2022-Machine-Learning-Specialization-main\requirements.txt&quot;” 

要安装 Python 包，但它们的安装方式如下。
我希望在我指定的环境中安装这些包。]]></description>
      <guid>https://stackoverflow.com/questions/78753784/why-am-i-installing-python-packages-into-my-machine-learning-environment-but-th</guid>
      <pubDate>Tue, 16 Jul 2024 09:40:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用标签编码的情况下对 RandomForest 的非序数分类变量进行编码？</title>
      <link>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</guid>
      <pubDate>Fri, 12 Jul 2024 21:11:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>当我通过 docker-compose.yml 运行镜像 ollama 时，无法正确运行它</title>
      <link>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</link>
      <description><![CDATA[我正在做一个项目，分析文本信息以从中提取特定数据。Python 中的正则表达式效果不佳，因为文本格式不断变化且没有一致性。因此，我决定使用语言模型来处理这些文本，如果文本包含我感兴趣的内容，则返回结果。
在开发程序时，我使用以下命令运行模型：
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
之后，我在代码中发送了一个请求，如下所示：
url = &#39;http://localhost:11434/api/generate&#39;
data = { 
&quot;model&quot;: &quot;llama3&quot;,
&quot;prompt&quot;: f&quot;{input_text}&quot;,
}

这有效（尽管响应需要一点时间才能完成）生成）。
现在，当我尝试配置我的 docker-compose.yml 文件以启动语言模型容器时，它看起来像这样：
ollama:
container_name: ollama
image: ollama/ollama
volumes:
- ollama:/root/.ollama
ports:
- &quot;11434:11434&quot;

volumes:
ollama:

但我只收到 404 错误，这意味着找不到端点。我不明白我做错了什么。有人可以帮忙吗？
此外，有人知道语言模型是否支持多线程吗？我的脚本发送文本非常快，我不确定是否要限制向语言模型发送请求的速率，或者它是否可以处理多线程和异步请求。]]></description>
      <guid>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</guid>
      <pubDate>Tue, 09 Jul 2024 09:38:52 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOv6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有以下输出，但无法弄清楚如何评估，因为没有 F1 分数 或 混淆矩阵。
平均召回率 (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

平均召回率 (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

平均召回率 (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我训练了 400 个 epoch，这只是输出的一小部分。我也看不到 mAP。
我有这行代码要评估
!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0

有没有办法获得详细的评估指标，例如 F1 分数、混淆矩阵 和 mAP？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>获取“model.fit”keras API 参数的值</title>
      <link>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</link>
      <description><![CDATA[我正在尝试使用自定义回调函数获取 Kera 顺序模型的详细信息。我需要提取 model.fit() API 中设置的参数的所有值，例如 batch_size、epochs、validation_split 等。但我无法在 Keras 的回调中访问它们。您知道如何自动获取这些值吗？
我使用的是 Python 3.10 和 Keras 2.8。]]></description>
      <guid>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</guid>
      <pubDate>Thu, 30 Nov 2023 20:13:45 GMT</pubDate>
    </item>
    </channel>
</rss>