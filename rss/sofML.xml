<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Dec 2023 21:11:43 GMT</lastBuildDate>
    <item>
      <title>我无法解决数据科学案例研究</title>
      <link>https://stackoverflow.com/questions/77737223/i-cant-solve-data-science-case-study</link>
      <description><![CDATA[我正在尝试自学数据科学，我必须解决一个案例研究。有一个数据库，其中包含酒店的名称、评论、负面和正面评论、评级等。我必须为该数据库创建咨询工具。有人可以告诉我应该使用哪种 ML 和 NLP 方法吗？为什么？ https://drive.google.com/file/d/1wYW_pLEEFluEejgkg_I2emxLjy9YZ5GG/view ?usp=
数据集链接。
(https://i.stack.imgur.com/KPU3e.png)( https://i.stack.imgur.com/yuOoB.png)
我尝试使用负面和正面评论来管理它，但我还必须添加用户的评分。所以我不知道是否可以添加其他信息]]></description>
      <guid>https://stackoverflow.com/questions/77737223/i-cant-solve-data-science-case-study</guid>
      <pubDate>Sat, 30 Dec 2023 19:51:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的决策树只有一个节点？</title>
      <link>https://stackoverflow.com/questions/77737156/why-does-my-decision-tree-just-have-one-node</link>
      <description><![CDATA[大家下午好，圣诞快乐！
我目前正在开展一个与机器学习相关的大学项目，我必须使用 R 创建决策树。
您可以在这里找到我正在使用的数据集 - https:/ /www.kaggle.com/datasets/iamsouravbanerjee/customer-shopping-trends-dataset
清理数据集后，这些是我在项目中使用的变量。这个新数据集被称为“GroupAgrePref”。 （附图）。（https://i.stack.imgur.com/eSt13.png&lt; /a&gt;)
变量“颜色”创建它的目的是为了替换以前仅 3 个类别“暖色”、“中性”、“冷色”中存在的所有不同颜色。
变量“AgeGroup”也是如此。我创建了将所有年龄段分为“青少年”、“成人”、“老年人”3 个类别的方法。使用以下公式 - subdata$Group.Age &lt;- cut(subdata$Age, Breaks = c(17,30,50,71),labels = c(“Teen”,“Adult”,“Elder” ））
所有变量都是因素。
下一步是创建训练数据集：
训练 &lt;- GroupAgePref[样本(1:nrow(GroupAgePref),3120),]
然后，决策树：Tree_1 &lt;- rpart(formula = Category ~ ., data = train)
但是当我想用 rpart.plot 以图形方式查看它时，就会发生这种情况： rpart.plot(arbol_1, box.palette = &quot;Red&quot;)
(https://i.stack.imgur.com/Rs7jl.png)&lt; /p&gt;
有人知道为什么我会遇到这种情况吗？
我正在尝试预测“类别”根据性别、季节、年龄组和颜色购买的衣服。
非常感谢！
阿德里安]]></description>
      <guid>https://stackoverflow.com/questions/77737156/why-does-my-decision-tree-just-have-one-node</guid>
      <pubDate>Sat, 30 Dec 2023 19:24:21 GMT</pubDate>
    </item>
    <item>
      <title>如何定义仅部分可训练的 PyTorch 张量</title>
      <link>https://stackoverflow.com/questions/77737016/how-to-define-pytorch-tensor-that-is-only-partially-trainable</link>
      <description><![CDATA[我正在尝试构建一个自定义模型来在 PyTorch 中进行训练，长话短说，我需要构建一个张量，将除矩形下对角线块之外的所有元素设置为零，至关重要的是，优化过程应该只涉及该子对角线块的元素，保持所有零不变。为此，我定义了一个自定义 pytorch 网络，并使用 nn.Parameter 定义了我的矩形块
类 My_Network(nn.Module):
    def __init__(自身 , 垂直尺寸 , 水平尺寸 ):
        超级().__init__()
        self.total_dim = 垂直_dim + 水平_dim
        self.subdiagonal_block = nn.Parameter（torch.rand（vertical_dim，horizo​​ntal_dim））


这样，如果我错了，请纠正我，PyTorch应该用随机值初始化这个张量的值，并且应该将它们注册为训练期间要优化的模型的参数。但现在我陷入了困境，我想告诉 PyTorch 构建一个方阵张量，其维度等于 self.total_dim ，除了次对角线块之外都是零，正如我所说在我将在前向方法中定义的计算中，pytorch 应该只训练次对角线块。
我可以根据需要添加零张量，而无需将其设置为模型参数，如下所示（如果我没有记错的话）：
类 My_Network(nn.Module):
    def __init__(自身, 垂直尺寸, 水平尺寸):
        超级().__init__()
        self.total_dim = 垂直_dim + 水平_dim
        self.subdiagonal_block = nn.Parameter(torch.rand(vertical_dim , Horizo​​ntal_dim))
        self.total_zero_tensor = torch.zeros(self.total_dim, self.total_dim)


但是现在我如何告诉 PyTorch 将我的次对角线块插入这个零矩阵的左下角？我需要为我的计算定义这个矩阵（我需要执行矩阵乘法），但这比仅将小下对角块视为一组要训练的参数更重要。]]></description>
      <guid>https://stackoverflow.com/questions/77737016/how-to-define-pytorch-tensor-that-is-only-partially-trainable</guid>
      <pubDate>Sat, 30 Dec 2023 18:32:49 GMT</pubDate>
    </item>
    <item>
      <title>半监督多标签分类的标签传播</title>
      <link>https://stackoverflow.com/questions/77737012/label-propagation-for-semi-supervised-multi-label-classification</link>
      <description><![CDATA[我正在尝试将标签传播应用于以下数据集 这是多标签的，但是我找不到方法来做到这一点。我尝试使用聚类将其转换为经典的单标签问题，但我想知道是否有更好、更有效的方法在半监督数据集上使用标签传播。]]></description>
      <guid>https://stackoverflow.com/questions/77737012/label-propagation-for-semi-supervised-multi-label-classification</guid>
      <pubDate>Sat, 30 Dec 2023 18:30:21 GMT</pubDate>
    </item>
    <item>
      <title>可教机器出口降低精度</title>
      <link>https://stackoverflow.com/questions/77736961/teachable-machine-export-decreasing-accuracy</link>
      <description><![CDATA[我试图用可教学机器制作图像识别模型，但为了测试该模型，我想将其导出到笔记本上，以便我可以快速运行它通过许多图像。然而，当我将模型导出到 Kaggle 时，导出版本的精度低于 Teachable machine 中的原始模型。我决定在两个模型上测试一张图像，在可教学机器上测试一张图像给了我正确的分类，但在 Kaggle 上我得到了错误的分类。如果我为每个人使用相同的模型，这是怎么发生的？
以下是我用于创建模型的步骤：
登录可示教机器
每类输入900张图像（6类）
在可示教机器中训练模型（400 epoch）
将模型从可示教机器导出到 TensorFlow Keras
将模型上传到 Kaggle 笔记本上
在Kaggle中，使用模型对一张图像进行分类（得到错误的分类）
在可教机器中使用模型（与导出到 Kaggle 的模型相同）对同一图像进行分类（获得正确的分类）
我怀疑 Teachable 机器可能是用 TensorFlow JS 制作的，但当它导出为 Keras 模型时，它会失去一些准确性，但我在这方面的经验很少，所以我可能是错的。
当我导出模型时，是否有办法保持可示教机器的准确性？谢谢！
我尝试使用 jupyter 笔记本和 google collab 而不是 Kaggle，但我无法弄清楚如何将文件导入其中。]]></description>
      <guid>https://stackoverflow.com/questions/77736961/teachable-machine-export-decreasing-accuracy</guid>
      <pubDate>Sat, 30 Dec 2023 18:13:38 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 层不兼容执行预测时出错</title>
      <link>https://stackoverflow.com/questions/77736223/tensorflow-layer-incompatibilty-error-while-performing-prediction</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77736223/tensorflow-layer-incompatibilty-error-while-performing-prediction</guid>
      <pubDate>Sat, 30 Dec 2023 14:27:28 GMT</pubDate>
    </item>
    <item>
      <title>我可以将什么词嵌入预训练模型或算法用于阿拉伯语，特别是人与人之间使用的语言（埃及口语）</title>
      <link>https://stackoverflow.com/questions/77736220/what-word-embeddings-pretrained-model-or-algorithm-i-can-use-with-arabic-languag</link>
      <description><![CDATA[我正在研究一个文本分类模型，其中我只有一个包含 2 列的数据集，Users_slang、formal_name
我试图将产品的每一个正式名称映射到市场上或人们之间日常使用的每一个俚语。
在对数据进行预处理、标记化之后，我正在尝试寻找适用于阿拉伯语言的最佳词嵌入方式，尤其是口语（埃及语）而不是古典阿拉伯语言
您有什么建议或建议？
我没有找到Word2Vec是否支持此类问题，但是它会起作用吗？
如果您对这种解决方案有任何建议，我将很乐意接受！
我还没有尝试实现词嵌入，但我发现在一般阿拉伯语上使用预训练模型会产生巨大的差异，我希望它能很好地工作，但我不知道会喜欢了解更多。]]></description>
      <guid>https://stackoverflow.com/questions/77736220/what-word-embeddings-pretrained-model-or-algorithm-i-can-use-with-arabic-languag</guid>
      <pubDate>Sat, 30 Dec 2023 14:26:55 GMT</pubDate>
    </item>
    <item>
      <title>高斯判别分析是否不适用于x较大且y较大的数据？</title>
      <link>https://stackoverflow.com/questions/77735867/is-gaussian-discriminant-analysis-unsuitable-for-data-where-x-is-bigger-and-y-is</link>
      <description><![CDATA[例如，x（肿瘤大小）越大，y（诊断概率）越大。这样的数据是不是不适合高斯判别分析模型。
因为这样的数据并不是当x接近平均值μ时y（诊断概率）最大化的情况]]></description>
      <guid>https://stackoverflow.com/questions/77735867/is-gaussian-discriminant-analysis-unsuitable-for-data-where-x-is-bigger-and-y-is</guid>
      <pubDate>Sat, 30 Dec 2023 12:10:56 GMT</pubDate>
    </item>
    <item>
      <title>变压器解码不正确</title>
      <link>https://stackoverflow.com/questions/77735779/incorrect-transformer-decoding</link>
      <description><![CDATA[我在序列到序列翻译任务（英语到西班牙语）上应用了标准转换器。我已启用在解码器的自注意力部分中引起屏蔽，并且我正在使用 keras_nlp 中的正弦位置嵌入层。在训练中，损失在短短 5 个时期内就下降到 0.0084，这是不寻常的，然后在解码测试句子时，所有模型生成的都是重复的 &lt;&lt;开始&gt;象征。显然，我做错了什么，但我无法识别它，所以如果有人能指出我到底错在哪里，我将不胜感激。
变压器编码器：
def Transformer_encoder(输入、head_size、num_heads、ff_dim、dropout=0)：
# 标准化和注意力
attn_output = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(输入, 输入)
attn_output = 辍学（辍学）（attn_output）
out1 = LayerNormalization(epsilon=1e-6)(输入 + attn_output)

# 前馈部分
ffn_output = 密集（ff_dim，激活 =“relu”）（out1）
ffn_output = 辍学（辍学）（ffn_output）
ffn_output = 密集(inputs.shape[-1])(ffn_output)
返回 LayerNormalization(epsilon=1e-6)(out1 + ffn_output)

变压器解码器：
def Transformer_decoder(输入、enc_outputs、head_size、num_heads、ff_dim、dropout=0)：
# 标准化和注意力
x = 层标准化（epsilon=1e-6）（输入）
mha = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)
a = mha(查询=x，键=x，值=x，use_causal_mask=True)
x = 辍学（辍学）（a）
分辨率 = x + 输入

# 编码器-解码器注意力
x = 层标准化(epsilon=1e-6)(res)
x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, enc_outputs)
x = 辍学（辍学）（x）
分辨率 = x + 分辨率

# 前馈部分
x = 层标准化(epsilon=1e-6)(res)
x = 密集(ff_dim, 激活=“relu”)(x)
x = 辍学（辍学）（x）
x = 密集(输入.形状[-1])(x)
返回 x + 分辨率

构建模型：
def build_transformer(vocab_size_eng, vocab_size_spa, max_len_eng, max_len_spa, head_size=64, num_heads=4, ff_dim=64, dropout=0.1):
# 编码器
编码器输入=输入（形状=（max_len_eng，））
x_enc = 嵌入(vocab_size_eng, head_size)(encoder_inputs)
x_enc_pos = keras_nlp.layers.SinePositionEncoding()(x_enc)
x_enc = x_enc + x_enc_pos

对于 _ 在范围（2）中：
    x_enc = Transformer_encoder(x_enc, head_size, num_heads, ff_dim, dropout)

# 解码器
解码器输入=输入（形状=（max_len_spa，））
x_dec = 嵌入(vocab_size_spa, head_size)(decoder_inputs)
x_dec_pos = keras_nlp.layers.SinePositionEncoding()(x_dec)
x_dec = x_dec + x_dec_pos

对于 _ 在范围（2）中：
    x_dec = Transformer_decoder(x_dec, x_enc, head_size, num_heads, ff_dim, dropout)

# 输出层
输出=密集（vocab_size_spa，激活=&#39;softmax&#39;）（x_dec）

# 定义并返回模型
模型 = 模型（输入=[编码器输入，解码器输入]，输出=输出）
返回模型

Colab 笔记本链接及结果供进一步参考：https://colab。 Research.google.com/drive/1MXrKkme53wLHoeriaFGY29P6pkeuB213?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/77735779/incorrect-transformer-decoding</guid>
      <pubDate>Sat, 30 Dec 2023 11:33:44 GMT</pubDate>
    </item>
    <item>
      <title>对所有特征进行编码还是仅对字符串特征进行编码？</title>
      <link>https://stackoverflow.com/questions/77735091/encode-all-features-or-just-string-features</link>
      <description><![CDATA[假设我有这个数据框：
DataFrame
当我尝试时，我发现对所有特征进行编码的 MSE 低于对某些指定特征进行编码的 MSE。但不知道哪种方式才是正确的？我想知道，我应该对 bmi、smoker 和region 使用 LabelEncoder 还是对所有功能使用 LabelEncoder？正确的做法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77735091/encode-all-features-or-just-string-features</guid>
      <pubDate>Sat, 30 Dec 2023 06:51:28 GMT</pubDate>
    </item>
    <item>
      <title>使用JAXopt进行约束优化（非负）</title>
      <link>https://stackoverflow.com/questions/77734910/using-jaxopt-on-constrained-optimizationnon-negative</link>
      <description><![CDATA[我正在尝试对我的损失函数进行优化（使用 JAX），该函数来自一个基本物理模型，该模型是以不同速率增长的两层。因为增长率总是正的。我想对我的优化器应用非负约束。我查了一下，发现了一些关于 JAXopt 的文档。但是，我不太确定如何将其包含在我的案例中，因为我的损失函数的定义与他们的示例非常不同。
这是一个简短的代码（65 行），它捕获了我想要展示的所有功能。我想使用 JAXopt 的投影梯度确保参数 {raw_vs1, raw_vs2} 均为非负。
导入 jax.numpy 作为 jnp
从 jax 导入随机、宽松、梯度、jit
将 matplotlib.pyplot 导入为 plt
进口光税
从 jaxopt 导入 ProjectedGradient
从 jaxopt.projection 导入projection_non_negative


# 优化代码设置
尼克斯 = 1000
新台币 = 100
密钥 = random.PRNGKey(0)
dt = 0.01
reg_a = 1.0
reg_b = 1.0
reg_c = 1.0
d = 3
ds = 0.003

xs = jnp.linspace(-1.0, 1.0, Nx)
参数 = {
    &#39;raw_vs1&#39;: random.uniform(key, shape=(Nt, Nx)),
    &#39;raw_vs2&#39;: random.uniform(key, shape=(Nt, Nx))
}
目标 = jnp.power(xs, 4)
y0 = jnp.power(xs, 2)
lengths0 = jnp.power(xs, 0) * 0.001 # 将每个时间步每个点的长度初始化为 0.1

def 模拟（参数，dt）：
    vs1 = 参数[&#39;raw_vs1&#39;]
    vs2 = 参数[&#39;raw_vs2&#39;]
    vs = vs1 - vs2
    vf = (vs1 + vs2 )/ 2

    def _step(进位, t):
        y，长度=进位
        y_new = y + vs[t] * dt
        lengths_new = lengths + vf[t] * dt # 在时间步 t 更新每个索引的长度
        返回（y_new，lengths_new），（y_new，lengths_new）

    Carry = (y0, lengths0) # 具有第一个时间步长的初始进位
    (yf,_), (ys, lengths_final) = lax.scan(_step, 进位, jnp.arange(Nt))
    print(&quot;lengths_final 的形状：&quot;, lengths_final.shape)
    print(&quot;yf 的形状：&quot;, yf.shape)
    print(&quot;ys 的形状：&quot;, ys.shape)
    返回 yf, ys, lengths_final

def损失（参数，dt，reg_a = reg_a，reg_b = reg_b，reg_c = reg_c，target_length = d）：
    yf, ys, leng = 模拟(params, dt)
    dt_term = jnp.sum(jnp.power(jnp.diff(ys, axis=0), 2))
    dx_term = jnp.sum(jnp.power(jnp.diff(ys, axis=1), 2))
    target_term = jnp.power(yf - 目标, 2)
    总长度 = jnp.sum(leng[-1])
    长度项 = (总长度 - 目标长度) ** 2
    返回 reg_a*dt_term + reg_b*dx_term + jnp.sum(reg_c*target_term) + reg_c*length_term


# 优化设置
g_loss = jit(grad(损失))
优化器 = optax.adam(0.01)
opt_state = 优化器.init(params)

# 优化循环
损失_t = []
对于范围内的 i（2000）：
    梯度 = g_loss(参数, dt)
    更新， opt_state = optimizationr.update(grads, opt_state)
    params = optax.apply_updates(params, 更新)
    loss_t.append(损失(params, dt))
]]></description>
      <guid>https://stackoverflow.com/questions/77734910/using-jaxopt-on-constrained-optimizationnon-negative</guid>
      <pubDate>Sat, 30 Dec 2023 05:01:58 GMT</pubDate>
    </item>
    <item>
      <title>使用自我训练的 AI 模型生成导入定义[关闭]</title>
      <link>https://stackoverflow.com/questions/77733066/generate-import-definitions-using-a-self-trained-ai-modell</link>
      <description><![CDATA[在我们的应用程序中，我们有一个导入模块，用户可以在其中指定如何从文本文件（以及其他文件类型、XML 和 ODBC）中提取数据。与将文本文件导入 Excel 有点类似 - 但有更多的可能性。大多数文本文件都具有不符合标准的专有结构。导入模块可以处理各种结构，例如简单的 csv 文件以及嵌套的键值对等。
我们有这方面的样本数据（可能有 100 个或数百个示例）。
所以我正在考虑创建和训练一个人工智能模型，该模型可以为使用的任何新文件创建这样的导入定义。用户只需打开文件，AI 模型就会创建第一个导入定义（如何以及在何处提取应用程序中表和列的文本）。
所以这里的输入是具有未知结构的文本，输出是已定义的结构。
我在 C++ 编程方面有经验，但在 AI 方面没有经验。
如何才能做到这一点？我还无法找到一些与这个问题至少有点相似的示例或教程。]]></description>
      <guid>https://stackoverflow.com/questions/77733066/generate-import-definitions-using-a-self-trained-ai-modell</guid>
      <pubDate>Fri, 29 Dec 2023 16:48:15 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中对新数据集进行预测</title>
      <link>https://stackoverflow.com/questions/77732371/make-prediction-on-new-data-set-in-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77732371/make-prediction-on-new-data-set-in-r</guid>
      <pubDate>Fri, 29 Dec 2023 14:03:24 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[--------------------原来的问题------------------------- --------
如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。

-------------------- 下面是我的解决方案（cuda）--------------------- ------------
谢谢各位的回复。我尝试了一些方法，最终效果如下：
首先，确保已安装 cmake（在 wsl 下）：
sudo apt-get update
sudo apt-get 安装 cmake
须藤 apt-get 安装 g++

那么，
git clone --recursive https://github.com/microsoft/LightGBM
cd光GBM
mkdir 构建
光盘构建
cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..
使-j4

目前，安装尚未链接到任何 conda env。为此，在 vscode 终端（或仍然是 wsl）下，conda 激活一个 env，然后创建一个 jupyter 笔记本进行测试：
确保lib_lightgbm.so位于LightGBM/python-package下，如果没有，则复制到该文件夹​​中。
然后在jupyter笔记本中：
导入系统
将 numpy 导入为 np
sys.path.append(&#39;/mnt/d/lgm-test2/LightGBM/python-package&#39;)
将 lightgbm 导入为 lgb

最后一点是，您可以参考 Jame 的回复，设备需要设置为“cuda”而不是“gpu”。]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Tensorflow model.json 转换为 model.pb 文件</title>
      <link>https://stackoverflow.com/questions/61344113/how-to-convert-tensorflow-model-json-to-model-pb-file</link>
      <description><![CDATA[我正在尝试将 model.json 格式的张量流模型转换为 model.pb 格式。试图寻找好的来源，但没有找到。我有 model.json 文件和二进制权重文件。最初我对它们进行了转换，以便它可以在浏览器上运行，但现在需要它们作为 .pb 文件。请帮忙。 
https://www.tensorflow.org/js/tutorials/conversion/import_saved_model  ]]></description>
      <guid>https://stackoverflow.com/questions/61344113/how-to-convert-tensorflow-model-json-to-model-pb-file</guid>
      <pubDate>Tue, 21 Apr 2020 12:57:33 GMT</pubDate>
    </item>
    </channel>
</rss>