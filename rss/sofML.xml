<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 29 Nov 2023 18:18:00 GMT</lastBuildDate>
    <item>
      <title>应该支持数组（不仅仅是类似列表）输入的评分规则的分解，但会给出错误</title>
      <link>https://stackoverflow.com/questions/77573489/decomposition-of-scoring-rule-for-array-not-just-list-like-inputs-should-be-su</link>
      <description><![CDATA[这个问题涉及Python。
我已经获得了 model_diagnostics.scoring 函数 decompose 来处理类似列表的输入，例如文档中的示例。
from model_diagnostics.scoring import SquaredError，分解
将 numpy 导入为 np
将 pandas 导入为 pd
se = 平方误差()
y_true = [0, 0, 0, 1, 1, 1]
y_pred = [0.1, 0.4, 0.3, 0.3, 0.99, 0.9]
分解（y_true，y_pred，评分函数= se）

在特定的上下文中，我们可以将这种分解视为给出与统计中感兴趣的 Brier 分数相关的各种度量。然而，Brier 分数对于比较类别数组（矩阵）和类别概率数组的矩阵（多类或多标签问题）有意义，我也对这种设置中的分解感兴趣。但是，decompose 函数并未按其应有的方式工作。
from model_diagnostics.scoring import SquaredError，分解
将 numpy 导入为 np
将 pandas 导入为 pd
np.随机.种子(2023)
se = 平方误差()
y_true = np.random.multinomial(1, np.ones(3)/3, 10)
y_pred = np.random.dirichlet(np.ones(3), 10)
分解（y_true，y_pred，评分函数= se）
分解（y_true.T，y_pred.T，scoring_function = se）
分解（y_true.T，y_pred，scoring_function = se）
分解（y_true，y_pred.T，scoring_function = se）

使用 decompose 的前两次尝试失败，并出现 具有多个元素的数组的真值不明确。使用 a.any() 或 a.all() 错误消息。 （最后两个也失败了，尽管它们只是我半绝望地尝试让它工作，但我知道会失败。）
但是，decompose 的文档提到输入可以是类似数组的，所以我似乎想做一些文档说支持的事情。是什么赋予了？如何使用数组？]]></description>
      <guid>https://stackoverflow.com/questions/77573489/decomposition-of-scoring-rule-for-array-not-just-list-like-inputs-should-be-su</guid>
      <pubDate>Wed, 29 Nov 2023 17:54:48 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与传统编程有何不同？</title>
      <link>https://stackoverflow.com/questions/77573240/how-does-machine-learning-differ-from-traditional-programming</link>
      <description><![CDATA[机器学习 (ML) 与传统编程的不同之处在于它获取知识和做出决策的方式。
总而言之，传统编程涉及明确指示计算机，而机器学习则侧重于训练算法以从数据中学习并做出决策。机器学习在问题复杂且定义显式规则具有挑战性或不切实际的场景中尤其强大。]]></description>
      <guid>https://stackoverflow.com/questions/77573240/how-does-machine-learning-differ-from-traditional-programming</guid>
      <pubDate>Wed, 29 Nov 2023 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>从 parquet 读取文件</title>
      <link>https://stackoverflow.com/questions/77572902/reading-file-from-parquet</link>
      <description><![CDATA[我在尝试读取镶木地板文件时遇到了挑战。最初，我怀疑该文件可能已损坏。然而，改变读取方法后，文件被成功处理。这个解决方案花了相当长的时间才确定，主要是因为我在机器学习领域相对缺乏经验。
这些方法有什么区别？为什么第一种方法给我一个损坏的文件？
正在工作
data = pd.read_parquet(&#39;data.parquet&#39;,engine=&#39;fastparquet&#39;)

无法正常工作，文件损坏并抛出异常 utf-8，我们尝试在记事本中修复
data = pq.read_table(&#39;data.parquet&#39;)
df = data.to_pandas()
]]></description>
      <guid>https://stackoverflow.com/questions/77572902/reading-file-from-parquet</guid>
      <pubDate>Wed, 29 Nov 2023 16:24:05 GMT</pubDate>
    </item>
    <item>
      <title>提高深度学习模型准确性的建议</title>
      <link>https://stackoverflow.com/questions/77571577/suggestions-to-improve-deep-learning-models-accuracy</link>
      <description><![CDATA[目前我正在研究一个利用深度学习的现实问题。在这个问题中，我有一个总共 74 个物种的各种植物图像的大型数据集，并且我已经将数据集分割成 80:10:10 的大小，用于训练、测试和验证套件。我为此目的使用张量流，但无法提高模型的准确性。
链接
这是代码]]></description>
      <guid>https://stackoverflow.com/questions/77571577/suggestions-to-improve-deep-learning-models-accuracy</guid>
      <pubDate>Wed, 29 Nov 2023 13:31:12 GMT</pubDate>
    </item>
    <item>
      <title>在为模型创建输入时，pandas DataFrame 和 keras Concatenate 有什么区别</title>
      <link>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-pandas-dataframe-and-concatenate-of-keras-while-c</link>
      <description><![CDATA[在为模型创建输入时，pandas DataFrame 和 keras Concatenate 有什么区别？
我们可以用两种方法训练模型：
方法 1：
 train_df = pd.DataFrame({ &#39;total_rooms&#39; : train_df[&quot;total_rooms&quot;], &#39;median_venue&#39; :
  train_df[“中位数收入”]，“中位数房屋值”：train_df[“中位数房屋值”]})
  model.compile（优化器=tf.keras.optimizers.experimental.RMSprop（learning_rate=my_learning_rate），损失=tf.keras.losses.BinaryCrossentropy（），
                指标=指标）

历史 = model.fit(x=train_df,
                      y=train_df[“中位数房屋值”],
                      # 详细=&#39;0&#39;,
                      批量大小=批量大小，
                      纪元=纪元，
                      回调=weight_callback，
                      #validation_split=validation_split
                   ）

方法 2：
 # 使用连接层将输入层连接成单个张量。
  # 作为密集层的输入。例如：[input_1[0][0]、input_2[0][0]]
  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())
  密集=层.密集（单位= 1，名称=&#39;dense_layer&#39;，激活= tf.sigmoid）
  密集输出 = 密集（连接输入）
  “”“创建并编译一个简单的分类模型。”“”
  我的输出 = {
    &#39;密集&#39;：密集输出，
  }
  模型= tf.keras.Model（输入= my_inputs，输出= my_outputs）
  历史= model.fit(x=特征，y=标签，batch_size=batch_size，
                      纪元=纪元，洗牌=洗牌）
]]></description>
      <guid>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-pandas-dataframe-and-concatenate-of-keras-while-c</guid>
      <pubDate>Wed, 29 Nov 2023 12:51:28 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建一个视觉问题回答模型，当我尝试进行问题编码时，出现此错误。有谁知道如何解决</title>
      <link>https://stackoverflow.com/questions/77571289/i-am-trying-to-create-a-visual-qustion-anwering-model-and-when-i-try-to-do-the-q</link>
      <description><![CDATA[我的代码
def textokenizer(文本):
  最大长度 = 24
  tok = 分词器()
  tok.fit_on_texts(x_train[&#39;ques&#39;])
  vocab_size = len(tok.word_index) + 1
  #print(&#39;x_train 中的唯一单词总数&#39;,vocab_size)
  编码文本 = tok.texts_to_sequences(文本)
  padded_text = pad_sequences(encoded_text, maxlen=max_length) #在每个问题的开头填充零，以便每个序列具有相同的长度
  #print（填充文本.形状）
  返回填充文本，tok

with open(&#39;glove_vectors&#39;, &#39;rb&#39;) as f:
    模型 = pickle.load(f)
    glove_words = 设置(model.keys())
    
# 火车
_,tok = textokenizer(x_train[&#39;ques&#39;])
vocab_size = len(tok.word_index) + 1
embedding_matrix_train = np.zeros((vocab_size, 300))
对于单词，i 在 tok.word_index.items() 中：
    如果 glove_words 中的单词：
        嵌入向量=模型[词]
        embedding_matrix_train[i] = embedding_vector

错误
UnpicklingError Traceback（最近一次调用最后一次）
&lt;ipython-input-68-e88f73604095&gt;在&lt;细胞系：12&gt;()
     11 #每个 token 使用 300-dim 向量使用预训练的 GloVe 表示来表示。
     12 以 open(&#39;glove_vectors&#39;, &#39;rb&#39;) 作为 f：
---&gt; 13 模型 = pickle.load(f)
     14 glove_words = 设置（model.keys（））
     15

UnpicklingError：无效的加载键，&#39;%&#39;。
]]></description>
      <guid>https://stackoverflow.com/questions/77571289/i-am-trying-to-create-a-visual-qustion-anwering-model-and-when-i-try-to-do-the-q</guid>
      <pubDate>Wed, 29 Nov 2023 12:48:54 GMT</pubDate>
    </item>
    <item>
      <title>与从头开始训练的模型一起使用时，GradCam+ 会给出不正确的结果</title>
      <link>https://stackoverflow.com/questions/77571210/gradcam-gives-incorrect-results-when-using-with-from-scratch-trained-model</link>
      <description><![CDATA[我正在尝试将 grad_cam_plus 的实现用于我的二元分类模型，但没有得到任何结果。我注意到 conv_second_grad conv_third_grad 为零张量，所以我认为这是一个问题。我也尝试过 vgg 但它工作正常。我不确定我做错了什么。
def grad_cam_plus(模型, img,
              layer_name=“conv2d_8”，label_name=无，
              类别 ID=无）：
”“”通过 Grad-CAM++ 获取热图。

参数：
    model：模型对象，从 tf.keras 2.X 构建。
    img：图像 ndarray。
    layer_name：一个字符串，模型中的图层名称。
    label_name：列表或无，
        通过分配此参数来显示标签名称，
        它应该是所有标签名称的列表。
    category_id：一个整数，类别的索引。
        默认是预测中得分最高的类别。

返回：
    热图 ndarray（无颜色）。
”“”
img_tensor = np.expand_dims(img, 轴=0)

conv_layer = model.get_layer(layer_name)
heatmap_model = Model([model.inputs], [conv_layer.output, model.output])

将 tf.GradientTape() 用作 gtape1：
    将 tf.GradientTape() 用作 gtape2：
        将 tf.GradientTape() 用作 gtape3：
            conv_output，预测= heatmap_model（img_tensor）
            如果category_id为None：
                Category_id = np.argmax(预测[0])
            如果 label_name 不是 None：
                打印（标签名称[类别id]）
            输出=预测[:,category_id]
            conv_first_grad = gtape3.gradient(输出, conv_output)
        conv_second_grad = gtape2.gradient(conv_first_grad, conv_output)
        打印（conv_second_grad）
    conv_third_grad = gtape1.gradient(conv_second_grad, conv_output)

global_sum = np.sum(conv_output, axis=(0, 1, 2))

alpha_num = conv_second_grad[0]
alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum
alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, 1e-10)

alpha = alpha_num/alpha_denom
alpha_normalization_constant = np.sum(alphas, axis=(0,1))
alphas /= alpha_normalization_constant

权重 = np.maximum(conv_first_grad[0], 0.0)

deep_线性化_权重 = np.sum(权重*alphas, 轴=(0,1))
grad_cam_map = np.sum(deep_线性化_权重*conv_output[0], 轴=2)

热图 = np.maximum(grad_cam_map, 0)
max_heat = np.max(热图)
如果最大热量== 0：
    最大热量 = 1e-10
热图 /= max_heat

返回热图


模型=顺序（[
  图层.重新缩放(1./255, input_shape=(img_height, img_width, 3)),
  层.Conv2D(16, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Conv2D(32, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Conv2D(64, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Flatten(),
  层.Dense(128, 激活=&#39;relu&#39;),
  层数.Dense(num_classes)
]）
]]></description>
      <guid>https://stackoverflow.com/questions/77571210/gradcam-gives-incorrect-results-when-using-with-from-scratch-trained-model</guid>
      <pubDate>Wed, 29 Nov 2023 12:35:34 GMT</pubDate>
    </item>
    <item>
      <title>Llama-2、Q4-量化模型在不同CPU上的响应时间</title>
      <link>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</link>
      <description><![CDATA[我正在此处运行量化的 llama-2 模型。我使用两台不同的机器。

第 11 代英特尔(R) 酷睿(TM) i7-1165G7 @ 2.80GHz 2.80 GHz
16.0 GB（15.8 GB 可用）

这台机器上的推理时间非常好。我在 3-4 分钟内得到了我想要的答复

Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz 2.20 GHz（2 个处理器）
224GB

这台机器上的推理时间很长。大约需要半个小时才能给出不满意的答复。它甚至还有 Nvidia 2080-Ti GPU。 （但不使用它来加载模型的权重。
为什么会出现这种行为？ CPU如何影响性能？
我正在使用 llama_cpp python 包来加载模型。]]></description>
      <guid>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</guid>
      <pubDate>Wed, 29 Nov 2023 11:56:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中用 Transformer 替代 LSTM</title>
      <link>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</link>
      <description><![CDATA[我对 Tensorflow 的经验很少，但我正在尝试开发一个现有项目，该项目在汽车行车记录仪视频中实现了事故预测模型（预期事故）。我的目标是用 Transformer 替换原始项目中使用的 LSTM 并比较结果。使用 Tensorflow 2 可行吗？我做了一些研究，但我不确定这种变化会对代码结构和模型逻辑产生多大影响。如果有人有任何建议，我们将非常感谢任何帮助。到目前为止，我刚刚做了一些小的调整，使代码可以与 Tensorflow 2 和 Python 3 一起使用，因为原始代码已经过时了（你可以找到我的分叉存储库 此处）。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</guid>
      <pubDate>Wed, 29 Nov 2023 11:26:12 GMT</pubDate>
    </item>
    <item>
      <title>如何根据特征集子集值的不可用性选择性地训练深度模型</title>
      <link>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</link>
      <description><![CDATA[我正在创建一个深度学习二元分类模型。数据集中的每个样本都包含两个互斥的特征集X和Y。
特征集 X 存在于所有样本中；然而，总样本中约有 45% 的特征集 Y 的值不可用。 Y 中的特征值本质上是二进制的。
我希望我的模型在推理过程中如果测试样本不包含特征集 Y 的值，则应仅对特征集 X 进行推理。如果 Y 的值可用，则推理应基于 X 和 Y。
我正在使用 PyTorch Lightning 框架进行模型架构设计和开发。
根据我的理解，我可以遵循的一种方法是“填写” Y 中的特征的一些默认值（以防它们不存在）并训练模型。但是，要填充什么值呢？应该是 0 或 1 还是任何其他类似 -1 等
另一种方法可能是创建一个名为 isYPresent 的二进制功能（例如）。如果 Y 不存在则为 0，如果存在则为 1。是否存在一种技术可以根据此新功能的值有条件地在模型训练期间（以及稍后的推理期间）处理这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:41 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 中的形状值通过 1000 个样本的值误差</title>
      <link>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</link>
      <description><![CDATA[我使用下面的代码来生成形状图
defplot_shap(
    n_sample：int =无，
    n_features：int =无，
    model_output：字典=无，
    标题：str = 无
）：
    如果 n_sample 不是 None：
        X_sampled = model_output[“X_test”].sample(n_sample,random_state=10)
        打印（X_sampled.shape）
    别的：
        X_sampled = model_output[“X_test”]
        print(&#39;无:&#39;, X_sampled.shape)
    解释器 = shap.TreeExplainer(model_output[“模型”])
    打印（解释器）
    shap_values =explainer.shap_values(X_sampled, check_additivity=False)
    打印（形状值）
    plt.标题（标题）
    shap.summary_plot(
        形状值，
        X_采样，
        最大显示=20
    ）
    返回 shap_values

使用下面的代码调用此函数时会给出值错误
shap_values =plot_shap(
    n_样本=1000，
    模型输出=模型输出，
    标题=模型输出[&#39;模型名称&#39;]
）

&lt;块引用&gt;
ValueError：此重塑错误通常是由于将错误的数据矩阵传递给 SHAP 引起的。请参阅https://github.com/shap/shap/issues/580。&lt; /p&gt;

打印时在病房上打印（shap_values）失败
(1000, 360) for 语句 print(X_sampled.shape)

 for语句打印（解释器）
原始X_test包含13827行和360列。]]></description>
      <guid>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的线性回归实现不起作用？</title>
      <link>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</link>
      <description><![CDATA[我正在尝试在 python 中从头开始实现线性回归。
作为参考，以下是我使用过的数学公式：方程
这是我尝试过的：
类线性回归：
    
    def __init__(
    自己，
    特征：np.ndarray[np.float64]，
    目标：np.ndarray[np.float64]，
    ）-&gt;没有任何：
        self.features = np.concatenate((np.ones((features.shape[0], 1)), features), axis=1)
        self.targets = 目标
        self.params = np.random.randn(features.shape[1] + 1)
        self.num_samples = features.shape[0]
        self.num_feats = features.shape[1]
        自我成本 = []
    
    def假设（自我）-&gt; np.ndarray[np.float64]：
        返回 np.dot(self.features, self.params)
    
    def cost_function(self) -&gt;; def cost_function(self) -&gt; np.float64：
        pred_vals = self.hypothesis()
        return (1 / (2 * self.num_samples)) * np.dot((pred_vals - self.targets).T, pred_vals - self.targets)
    
    def update(self, alpha: np.float64) -&gt;;没有任何：
        self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))
    
    defgradientDescent(self, alpha: np.float64, 阈值: np.float64, max_iter: int) -&gt;没有任何：
        收敛=假
        计数器 = 0
        未收敛时：
            计数器 += 1
            curr_cost = self.cost_function()
            self.costs.append(curr_cost)
            自我更新（阿尔法）
            new_cost = self.cost_function()
            如果abs(new_cost - curr_cost) &lt;临界点：
                收敛=真
            如果计数器&gt;最大迭代次数：
                收敛=真

我使用了这样的类：
regr = LinearRegression(features=np.linspace(0, 1000, 200, dtype=np.float64).reshape((20, 10)), 目标= np.linspace(0, 200, 20, dtype=np.float64))
regr.gradientDescent(0.1, 1e-3, 1e+3)
regr.cost_function()

但是，我收到以下错误：
RuntimeWarning：标量幂中遇到溢出
  return (1 / (2 * self.num_samples)) * (la.norm(self.hypothesis() - self.targets) ** 4)

RuntimeWarning：标量减法中遇到无效值
  如果abs(new_cost - curr_cost) &lt;临界点：

RuntimeWarning：matmul 中遇到溢出
  self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))

任何人都可以帮助我了解到底出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</guid>
      <pubDate>Wed, 29 Nov 2023 08:54:58 GMT</pubDate>
    </item>
    <item>
      <title>在 Node.js 中使用 TensorFlow.js 时出现错误No backend found inregistry 错误</title>
      <link>https://stackoverflow.com/questions/75906320/errorno-backend-found-in-registry-error-while-using-tensorflow-js-in-node-js</link>
      <description><![CDATA[我尝试在 Node.js 中使用 TensorFlow.js，通过 MobileNet 对图像进行分类。但是，我收到以下错误：
未捕获（承诺中）错误：在注册表中找不到后端。 
我知道此错误与缺少注册后端有关，但我不确定如何解决它。这是我正在使用的代码：
这是完整的错误
 未捕获（承诺中）错误：在注册表中找不到后端。在 w.getSortedBackends (engine.js:267:19) 在 w.initializeBackendsAndReturnBest (engine.js:276:37) 在 get backend [as backend] (engine.js:108:46) 在 w.makeTensor (engine.js) :612:35) 在 (tensor_ops_util.js:57:12) 在 n (tensor.js:49:12) 在 Module.d (io_utils.js:214:25) 在 d.loadSync (graph_model.js:141) :35) 在 graph_model.js:116:54 在异步 u (graph_model.js:417:5)

我应该做什么来修复这个错误。先感谢您。非常感谢任何指导。
我也尝试使用 tf cdn，但我不断收到相同的错误。它表明我没有注册后端。]]></description>
      <guid>https://stackoverflow.com/questions/75906320/errorno-backend-found-in-registry-error-while-using-tensorflow-js-in-node-js</guid>
      <pubDate>Sat, 01 Apr 2023 12:57:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vertex AI 批量预​​测的自定义模型返回置信度分数</title>
      <link>https://stackoverflow.com/questions/70070421/return-confidence-score-with-custom-model-for-vertex-ai-batch-predictions</link>
      <description><![CDATA[我将预训练的 scikit learn 分类模型上传到 Vertex AI，并对 5 个样本进行批量预测。它只是返回了一个没有置信度分数的错误预测列表。我在 SDK 文档或 GCP 控制台中没有看到如何获取批量预测以包含置信度分数。这是 Vertex AI 可以做的事情吗？
我的目的是使用以下代码自动执行批量预测管道。
batch_prediction_job = model.batch_predict(
    作业显示名称 = 作业显示名称,
    gcs_source = 输入路径，
    实例格式=“”，#“jsonl”，“csv”，“bigquery”，
    gcs_destination_prefix = 输出路径，
    起始副本计数 = 1,
    最大副本数 = 10,
    同步=真，
）

batch_prediction_job.wait()
]]></description>
      <guid>https://stackoverflow.com/questions/70070421/return-confidence-score-with-custom-model-for-vertex-ai-batch-predictions</guid>
      <pubDate>Mon, 22 Nov 2021 18:04:35 GMT</pubDate>
    </item>
    <item>
      <title>训练+测试集是否必须与预测集不同（以便您需要对所有列应用时移）？ （没有时间序列！）</title>
      <link>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</link>
      <description><![CDATA[由于这个问题被否决了，并且第一个答案似乎将其视为一个时间序列问题：这个问题不是关于经典的时间序列分析，而是寻求将每月的列作为特征来处理。没有目的是检查趋势或任何其他分解！我分享了这个问题，因为我在工作中正是遇到了这个挑战，最后，该模型在这种设置下运行良好，混合了非每月的数据（永恒）具有每月功能的功能。因此，这只是一个使用每月数据列作为特征的问题，模型并不关心是12月还是6月，它只关心这些特征是过去多少个月，以便它从模式中学习大约 x 个月前的数据。这些特征不是在月份之后调用的，而是在它们回溯了多少个月之后调用的，例如财富_月_1、财富_月_2 代表过去 1 或 2 个月的财富。
&lt;小时/&gt;
我知道我们应该仅在测试集上测试经过训练的分类器的一般规则。
但现在出现了问题：当我准备好经过训练和测试的分类器时，我可以将其应用到作为训练和测试集基础的同一数据集吗？&lt; /em&gt; 或者我是否必须将其应用于与训练+测试集不同的新预测集？
如果我预测时间序列的标签列怎么办（稍后编辑：我并不是想在这里创建经典的时间序列分析，而是只是从典型数据库中广泛选择列，每周、每月或随机存储的数据，我将其转换为单独的特征列，每个特征列为一周/一个月/一年...），我是否必须移动全部将训练+测试集的特征（不仅是时间序列标签列的过去列，还包括所有其他正常特征）设置回数据没有“知识”的时间点与预测集的拦截？
然后，我将根据过去 n 个月的特征来训练和测试分类器，针对未移动且最新的标签列进行评分，然后根据最近未移动的特征进行预测。移位和未移位的特征具有相同的列数，我通过将移位特征的列名称分配给未移位的特征来对齐移位和未移位的特征。
附注：
p.s.1：https://en.wikipedia.org/wiki/Dependent_and_independent_variables&lt;的一般方法/a&gt;
在数据挖掘工具（用于多元统计和机器学习）中，因变量被分配为目标变量（或在某些工具中为标签属性），而自变量可能被分配为常规变量。[ 8]为训练数据集和测试数据集提供了目标变量的已知值，但应对其他数据进行预测。
p.s.2：在这个基本教程中，我们可以看到预测集有所不同：https://scikit-learn.org/stable/tutorial/basic/tutorial.html
我们使用 [:-1] Python 语法选择训练集，它会生成一个包含所有 &gt; 的新数组。但digits.data 中的最后一项：[…] 现在您可以预测新值。在这种情况下，您将使用digits.data [-1:]中的最后一个图像进行预测。通过预测，您将从训练集中确定与最后一个图像最匹配的图像。]]></description>
      <guid>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</guid>
      <pubDate>Fri, 06 Dec 2019 09:16:37 GMT</pubDate>
    </item>
    </channel>
</rss>