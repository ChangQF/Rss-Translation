<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 09 Feb 2025 12:30:32 GMT</lastBuildDate>
    <item>
      <title>任何熟悉Chernozhukov的（至少是）Chernozhukov的应用因果推理（由ML和AI提供支持）吗？</title>
      <link>https://stackoverflow.com/questions/79424826/anybody-familiar-with-at-least-the-1st-chapter-of-chernozhukovs-applied-causa</link>
      <description><![CDATA[所以，我是计量经济学的学生。
我被介绍了这本机器学习书的第一章，我对机器学习一无所知。
如果对这本书及其第一章的熟悉程度甚至含糊不清，则可以指出我最好的快速介绍MC在本章中应用的概念和观点...好吧，我不能在线亲吻您，但是如果我可以的话，我会
我试图从计量经济学的角度理解它，相似之处在那里，但显然本章的目的不是使用同等符号产生计量经济学知识。]]></description>
      <guid>https://stackoverflow.com/questions/79424826/anybody-familiar-with-at-least-the-1st-chapter-of-chernozhukovs-applied-causa</guid>
      <pubDate>Sun, 09 Feb 2025 11:29:35 GMT</pubDate>
    </item>
    <item>
      <title>DeepSeek拥抱面模型加载问题</title>
      <link>https://stackoverflow.com/questions/79424312/deepseek-huggingface-model-loading-issue</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上要插件代码：
 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe = pipeline（＆quort&#39;text-generation＆quort＆quote =&#39;deepSeek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 
，但我无法加载模型。当我这样做时，我会得到这个问题：
  file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py＆quot;，第97行，在from_dict
提高价值Error（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 
我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：
raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: [&#39;awq&#39;, &#39;bitsandbytes_4bit&#39;, &#39;bitsandbytes_8bit&#39;, &#39;gptq&#39;, &#39;aqlm&#39;, &#39;quanto&#39;, &#39;eetq&#39; ，&#39;higgs&#39;，&#39;hqq&#39;，&#39;compressed-tensors&#39;，&#39;fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;]   
我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/deepseek-huggingface-model-loading-issue</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>对于没有参数的无监督的模型，我的过程合法是计算GridSearch交叉验证吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79424108/for-a-model-unsupervised-without-parameters-is-my-process-legit-to-compute-the-g</link>
      <description><![CDATA[我有一个没有参数的模型，但有1个超参数阈值。我的数据集是20次，具有20个地面图。我想要的是为其找到最佳的高参数值，然后返回模型的分数
免责声明：我无法安装（）培训集并预测整个验证集。我的模型每次都需要1个时间，因此在训练集中，我只是一一计算并计算平均F1SCORE。验证集也是如此。我用该特定阈值计算验证集中的每个时间表，然后计算F1Score 的平均值。
所以这是我的想法：

能够模拟我的模型将如何在数据集中使用，然后再将数据集分为培训集和验证集，例如19培训集和1个验证集。
我将训练集用作测试地面和蛮力，所有我的阈值的组合从0到最大。我发现阈值= 10是最好的训练集，它给了我F1Score = 0.8，因此接下来我需要使用验证集验证模型
我对其进行了测试，但我很不幸，因为我的模型的f1score = 0.8，训练集中的每个时间表，因此平均值仍然为0.8，但是阀中的单个时间表中的单个时间表给我0.1。这个分数不正确，因为我可能只是不幸。我需要执行交叉验证。
如何计算交叉验证？如果对于每个新折叠（新的19个训练集和1个验证集），我会检查在验证集中使用的最佳阈值，它与GridSearch的逻辑相反。我需要固定阈值，然后执行交叉验证。
但是，如果我将阈值设置为x，什么是训练感？因为我的模型不合适（），在2）我使用训练集来强迫搜索阈值= 10。所以也许我只能迭代阈值0到最大？但是，如果是这样，训练集是毫无意义的，我只是计算每个时间表（20）和计算和平均F1分数的F1分数。没有意义要分裂。对于每个视频计算F1Score，然后计算平均F1Score。
或者我应该计算训练集中每个折叠的平均F1分数。对于exmaple而不是20次摄影，我们有3个。[1,2,3]。每个折叠的训练集将为[1,2] [1,3] [2,3]。
对于每个数组，我测试阈值从0到最大，因此我计算[F1_1，F1_2]，然后计算平均F1_Mean1 
然后计算第二倍[F1_1，F1_3]，然后计算平均F1_Mean2 
然后计算第三倍[F1_2，F1_3]，然后计算平均F1_Mean3 
最后我计算均值（f1_mean1，f1_mean2，f1_mean3）= f1_mean_x，因此阈值的最终得分= x 
我为阈值的每个值都做过
现在，我没有在验证集中拥有0.1的不幸单个时间表，这次我有[3] [2] [1]。对于所有折叠
 i计算阈值= 3的阈值3，然后为2，然后计算1，然后计算平均F1分数，这是我模型的真实分数。

这个过程合法吗？或者，我只需要计算每个时间表的witohut拆分，然后计算交叉验证？]]></description>
      <guid>https://stackoverflow.com/questions/79424108/for-a-model-unsupervised-without-parameters-is-my-process-legit-to-compute-the-g</guid>
      <pubDate>Sun, 09 Feb 2025 00:01:46 GMT</pubDate>
    </item>
    <item>
      <title>使用y作为输入</title>
      <link>https://stackoverflow.com/questions/79423737/gnn-multinode-prediction-yeilds-constant-output-when-using-y-as-input</link>
      <description><![CDATA[
 y：8个值，2D向量
 x：与y 相同
 y_hat：始终恒定
数据：完全连接的图形与始终8个节点
模型：普通同质GCN 
损失功能：MSE 
假设，x：y应具有1：1的相关性，因此它应该能够为y提供足够的预测能力。
问题：为什么总是恒定？或方法本身不正确？

输出：
 最终模型输出：张量（[[[0.2878]，，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]，
            [0.2878]]，grad_fn =＆lt; addmmbackward0＆gt;）
    目标值（y）：张量（[[0.2037]，，，
            [0.5126]，
            [0.6616]，
            [0.7587]，
            [0.3367]，
            [0.2623]，
            [0.7544]，
            [0.6320]]）
 
代码：
 导入火炬
导入Torch.nn.功能为f
来自TORCH__EOMETRIC.NN导入GCNCONV

＃定义更强大的GNN模型
GNN类（Torch.nn.Module）：
    def __init __（self，num_node_features，hidden_​​channels，output_channels）：
        超级（gnn，self）.__ init __（）
        self.conv1 = gcnconv（num_node_features，hidden_​​channels）
        self.conv2 = gcnconv（hidden_​​channels，hidden_​​channels）
        self.conv3 = gcnconv（hidden_​​channels，hidden_​​channels）＃附加层
        self.fc = torch.nn.linear（hidden_​​channels，output_channels）

    def向前（self，x，edge_index）：
        x = self.conv1（x，edge_index）
        x = f.sigmoid（x）
        x = self.conv2（x，edge_index）
        x = f.sigmoid（x）
        x = self.conv3（x，edge_index）＃附加图层
        x = f.sigmoid（x）
        x = self.fc（x）
        返回x

＃创建一个合成数据集
num_nodes = 8
num_node_features = 1
x = torch.rand（（num_nodes，num_node_features））＃随机节点功能
edge_index = torch.tensor（[[i，j]在range（num_nodes）in range（num_nodes）in range（num_nodes）（如果i！= j]，dtype = type = torch.long）.t（）.t（）。连续（）。
y = torch.rand（（（num_nodes，1））＃随机目标排名
x = y
＃初始化模型
model = gnn（num_node_features，hidden_​​channels = 32，output_channels = 1）

＃定义损失功能和优化器
标准= torch.nn.mseloss（）
优化器= torch.optim.adam（model.parameters（），lr = 0.001）
调度程序= torch.optim.lr_scheduler.steplr（优化器，step_size = 30，伽马= 0.1）

＃训练循环
对于范围（100）的时期：
    优化器.zero_grad（）
    out =型号（x，edge_index）
    损失=标准（否，y）
    loss.backward（）
    torch.nn.utils.clip_grad_norm_（model.parameters（），max_norm = 1.0）＃渐变剪法
    优化器.step（）
    Scheduler.step（）
    print（f＆quot; epoch {epoch+1}，损失：{loss.item（）}; quot;）

＃检查最终输出
out =型号（x，edge_index）
打印（“最终型号输出：＆quot of）
打印（“目标值（y）：＆quot”，y）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79423737/gnn-multinode-prediction-yeilds-constant-output-when-using-y-as-input</guid>
      <pubDate>Sat, 08 Feb 2025 18:41:03 GMT</pubDate>
    </item>
    <item>
      <title>在深入学习BI AI工程后，我应该学到什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79423714/what-should-i-learn-after-deep-learning-for-bi-ai-engineering</link>
      <description><![CDATA[我在过去的5个月里一直在学习机器学习和使用TensorFlow的深度学习。我现在应该学到什么？]]></description>
      <guid>https://stackoverflow.com/questions/79423714/what-should-i-learn-after-deep-learning-for-bi-ai-engineering</guid>
      <pubDate>Sat, 08 Feb 2025 18:26:52 GMT</pubDate>
    </item>
    <item>
      <title>在扩散模型中U形网络的上下抽样阶段中不同的缩小参数</title>
      <link>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</guid>
      <pubDate>Sat, 08 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>即使我的数据框在特定范围内具有相当归一化的数据[关闭]，我是否也需要使用缩放器</title>
      <link>https://stackoverflow.com/questions/79422891/do-i-need-to-use-scaler-even-if-my-dataframe-has-fairly-normalized-data-within-a</link>
      <description><![CDATA[我试图在培训数据集上执行LSTM，以下面的结构进行预测
    
数据框架中大约有200万个这样的集合。因此，形状将像（2m，10或15、3）。如果我将数据分类为其他因素，则可能会少
A列A＆amp; B始终从1开始，然后更改（通常两侧为0-2％，很少10％，从不20％）。 C列始终在0到100之间。
我的问题是：即使我的数据已经在一个范围内相当标准化，我仍然需要使用Minmax或稳健的缩放器。另外，如果我仍然应该使用它，则范围是（0,1）或（-1,1）]]></description>
      <guid>https://stackoverflow.com/questions/79422891/do-i-need-to-use-scaler-even-if-my-dataframe-has-fairly-normalized-data-within-a</guid>
      <pubDate>Sat, 08 Feb 2025 09:09:46 GMT</pubDate>
    </item>
    <item>
      <title>在Google子午线中运行地理级别与国家级模型时，R平方值的差异</title>
      <link>https://stackoverflow.com/questions/79421421/discrepancy-in-r-squared-values-when-running-geo-level-vs-national-level-models</link>
      <description><![CDATA[我正在使用Google Meridian分析包含日期，国家，渠道和KPI的数据集。当我运行包括GEO（乡村）数据在内的模型时，我会得到两个R平方值：

 R平方用于地理级别数据：0.89 
国家级别的R平方数据：0.98 

但是，当我在没有地理数据的情况下运行模型（不包括地理列）时，R-squared显着下降至0.51。
我不明白为什么使用GEO数据时国家级别的R平方为0.98，但是当我排除它时，为什么降至0.51。在这两种情况下，国家级别的R平方不应该更加一致吗？
我尝试在有没有GEO（国家）列的情况下运行该模型。我希望在这两种情况下，R-squared都将保持相似之处，因为我认为删除地理信息不会对国家级模型产生巨大影响。
相反，包括地理数据的国家级别的R平方为0.98，但在排除该数据时仅为0.51。我想知道Google Meridian使用影响国家级别结果的地理数据时是否应用某种聚合或加权。
为什么存在这种差异，子午线如何处理GEO与国家数据？]]></description>
      <guid>https://stackoverflow.com/questions/79421421/discrepancy-in-r-squared-values-when-running-geo-level-vs-national-level-models</guid>
      <pubDate>Fri, 07 Feb 2025 15:35:09 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在minmaxscaler（）中设置data_min和data_max？</title>
      <link>https://stackoverflow.com/questions/79416292/is-there-a-way-to-set-the-data-min-and-the-data-max-in-minmaxscaler</link>
      <description><![CDATA[我目前在数据集上使用MinMaxScaler（）。但是，由于我的数据集很大，所以我要进行第一次迭代通行证，以计算缩放器的最小值和最大值。我正在使用partial_fit（）来帮助您。
无论如何，对于我的某些功能，我确实知道它们的最小值和最大值。无论如何，我是否可以将这些最小值和最大值通知缩放器？]]></description>
      <guid>https://stackoverflow.com/questions/79416292/is-there-a-way-to-set-the-data-min-and-the-data-max-in-minmaxscaler</guid>
      <pubDate>Wed, 05 Feb 2025 21:57:35 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入传递到张量流骨干模型而不获得属性：'元组'对象没有属性'as_list'</title>
      <link>https://stackoverflow.com/questions/79411501/how-to-pass-input-to-a-tensorflow-backbone-model-without-getting-attributeerror</link>
      <description><![CDATA[我试图从 tensorflow models 库中使用resnet3d，但是在尝试运行块时我会遇到这个奇怪的错误
 ！pip install tf-models-inficial == 2.17.0
 
 TensorFlow版本是 2.18 在Kaggle Notebook上。
安装 tf-models-inficial  
 来自Tensorflow.keras.callbacks导入早期踩踏，REDUCELRONPLATEAU
来自Tensorflow.keras.models导入模型
来自tensorflow.keras.layers导入密集，globalaverate -pooling3d，输入
来自TensorFlow.keras.optimizer导入adamw
导入TensorFlow_Models作为TFM

def create_model（）：
    base_model = tfm.vision.backbones.resnet3d（model_id = 50，
        tuermal_strides = [3,3,3,3]，
        temulal_kernel_sizes = [（5,5,5），（5,5,5,5），（5,5,5,5,5,5），（5,5,5）]，]，
        input_specs = tf.keras.layers.inputspec（shape =（无，无，img_size，img_size，3））
    ）
    
    ＃解冻基本型号层
    base_model.trainable = true
    
    ＃创建模型
    输入=输入（shape = [无，无，img_size，img_size，3]）
    x = base_model（输入）＃b，1,7,7,2048
    x = globalaveragepooling3d（data_format =＆quot; channels_last_last＆quort; keepdims = false）（x）
    x =密集（1024，激活=&#39;relu&#39;）（x）
    x = tf.keras.layers.dropout（0.3）（x）＃添加辍学以防止过度拟合
    输出=密集（num_classes，activation =&#39;softmax&#39;）（x）
    
    模型=模型（输入，输出）
    
    ＃用级别的权重编译模型
    优化器= ADAMW（Learning_rate = 1E-4，weight_decay = 1E-5）
    model.compile（
        优化器=优化器，
        损失=&#39;Sparse_categorical_crossentropy&#39;，
        量表= [&#39;feceracy&#39;，tf.keras.metrics.auc（）]
    ）
    
    返回模型

＃创建和显示模型
模型= create_model（）
model.summary（）
 
运行此操作时，我会在下面收到错误：
  ---------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------
AttributeError Trackback（最近的最新电话）
＆lt; ipython输入-56-363271B4DDA8＆gt; in＆lt;单元线：39＆gt;（）
     37 
     38＃创建和显示模型
---＆gt; 39模型= create_model（）
     40 Model.Summary（）

＆lt; ipython输入-56-363271B4DDA8＆gt;在create_model（）中
     18＃创建模型
     19个输入=输入（shape =（无，无，img_size，img_size，3））
---＆gt; 20 x = base_model（输入）＃b，1,7,7,2048

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py in __call __（self， *args，** kwargs）
    586 layout_map_lib._map_subclass_model_varible（self，self._layout_map）
    587 
 - ＆gt; 588返回super（）.__调用__（*args，** kwargs）

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/base/base_layer.py in __call ____________________________________________________________________________________________________________________________________________________________________________________________________________________
   1101培训=训练_mode，
   1102）：
 - ＆gt; 1103 input_spec.assert_input_compatibility（
   1104 self.input_spec，输入，self.name
   1105）

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in assert_input_compatibility（input_spec，inputs，inputs，lays_name）
    300“与该层不兼容：”
    301 f＆quot“预期形状= {spec.shape}，”
 - ＆gt; 302 f＆quot``找到形状= {display_shape（x.shape）}}＆quot;
    303）
    304 

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in Display_shape（shape）
    305 
    306 def display_shape（形状）：
 - ＆gt; 307返回str（元组（shape.as_list（）））
    308 
    309 

attributeError：&#39;tuple&#39;对象没有属性&#39;as_list&#39;
 
我尝试将输入传递到 shape 参数作为列表，但仍会遇到相同的错误。
此错误正在发生
 ！pip install tf-models-inficial == 2.17.0

导入TensorFlow作为TF

inputs = tf.keras.input（shape = [none，none，img_size，img_size，3]）
打印（inputs.shape.as_list（））
 
错误：
  ---------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------
AttributeError Trackback（最近的最新电话）
＆lt; ipython-Input-39-6e88680ff7df＆gt; in＆lt;单元线：2＆gt;（）
      1个输入= tf.keras.input（shape = [none，none，img_size，img_size，3]）
----＆gt; 2 print（inputs.shape.as_list（））

attributeError：&#39;tuple&#39;对象没有属性&#39;as_list&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/79411501/how-to-pass-input-to-a-tensorflow-backbone-model-without-getting-attributeerror</guid>
      <pubDate>Tue, 04 Feb 2025 11:22:41 GMT</pubDate>
    </item>
    <item>
      <title>我如何在Kaggle中使用Python版本3.7.1</title>
      <link>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</link>
      <description><![CDATA[我正在尝试训练以Python版本3.7.1脚本脚本的TensorFlow ML模型。但是，Kaggle中的Python版本为3.10.2。是否有任何方法可以使用Python 3.7环境，是否可以使其成为默认的env？]]></description>
      <guid>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</guid>
      <pubDate>Thu, 23 Nov 2023 14:49:38 GMT</pubDate>
    </item>
    <item>
      <title>微调和很少的射击学习之间有什么区别？</title>
      <link>https://stackoverflow.com/questions/72611335/what-are-the-differences-between-fine-tuning-and-few-shot-learning</link>
      <description><![CDATA[我试图理解微调和的概念，学习。
我了解进行微调的必要性。它本质上是将预训练的模型调整为特定的下游任务。但是，最近我看到了大量博客文章，说明零拍的学习，一次性学习和很少的学习。

它们与微调有何不同？在我看来，很少有学习是对微调的专业化。我在这里想念什么？

有人可以帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/72611335/what-are-the-differences-between-fine-tuning-and-few-shot-learning</guid>
      <pubDate>Tue, 14 Jun 2022 03:54:23 GMT</pubDate>
    </item>
    <item>
      <title>与cross_validate产生混乱矩阵</title>
      <link>https://stackoverflow.com/questions/65645125/producing-a-confusion-matrix-with-cross-validate</link>
      <description><![CDATA[我正在尝试弄清楚如何与Cross_validate产生混淆矩阵。我能够用我到目前为止的代码打印出分数。
 ＃实例化模型
型号= decisionTreeClalsifier（）

#scores
评分= {&#39;准确性&#39;：make_scorer（efceracy_score）， 
           &#39;precision&#39;：make_scorer（precision_score），
           “召回”：make_scorer（recke_score）， 
           &#39;f1_score&#39;：make_scorer（f1_score）}

＃10倍交叉验证
scores = cross_validate（型号，x，y，cv = 10，评分=评分）

打印（准确性（测试）：％0.2F（+/-％0.2F）＆quot;％（分数[&#39;test_accuracy&#39;]。平均值（），分数[&#39;test_accuracy&#39;]。std（） * 2））
打印（精确（测试）：％0.2F（+/-％0.2F）＆quot;％（分数[&#39;test_precision&#39;]。平均值（），分数[&#39;test_precision&#39;]。std（） * 2））
打印（召回（测试）：％0.2F（+/-％0.2F）＆quot;％（分数[&#39;test_recall&#39;]。emane（），分数[&#39;test_recall&#39;]。std（） * 2））
打印（＆quot&#39;f1-SCORE（测试）：％0.2F（+/-％0.2F）＆quot;％（分数[&#39;test_f1_score&#39;]。平均值（），分数[&#39;test_f1_score&#39;] .STD（） * 2） ）
 
，但我正在尝试将这些数据放入混淆矩阵中。我可以使用cross_val_predict -来制作混乱矩阵
  y_train_pred = cross_val_predict（型号，x，y，cv = 10）
Confusion_matrix（y，y_train_pred）
 
这很棒，但是由于它是自己的交叉验证，因此结果无法匹配。我只是在寻找一种与匹配结果一起产生两者的方法。]]></description>
      <guid>https://stackoverflow.com/questions/65645125/producing-a-confusion-matrix-with-cross-validate</guid>
      <pubDate>Sat, 09 Jan 2021 16:46:28 GMT</pubDate>
    </item>
    <item>
      <title>Scikit学习10倍交叉验证的混淆矩阵</title>
      <link>https://stackoverflow.com/questions/40169152/confusion-matrix-for-10-fold-cross-validation-in-scikit-learn</link>
      <description><![CDATA[如何与Scikit-Learn计算10倍交叉验证中的混淆矩阵？我如何找到 y_test 和 y_pred ？]]></description>
      <guid>https://stackoverflow.com/questions/40169152/confusion-matrix-for-10-fold-cross-validation-in-scikit-learn</guid>
      <pubDate>Fri, 21 Oct 2016 05:39:30 GMT</pubDate>
    </item>
    <item>
      <title>混淆矩阵带有交叉验证</title>
      <link>https://stackoverflow.com/questions/9734403/confusion-matrix-with-cross-validation</link>
      <description><![CDATA[我正在使用Scikits接口训练具有交叉验证（StratifiedKfold）的SVM分类器。对于（K的）测试集，我得到一个分类结果。我想拥有一个与所有结果的混淆矩阵。
 scikit-learn具有混乱矩阵接口：
  sklearn.metrics.confusion_matrix（y_true，y_pred）
 
我的问题是如何积累y_true和y_pred值。它们是数组（numpy）。我应该根据我的k折参数定义数组的大小吗？对于每个结果，我应该将y_true和y-pred添加到数组中？]]></description>
      <guid>https://stackoverflow.com/questions/9734403/confusion-matrix-with-cross-validation</guid>
      <pubDate>Fri, 16 Mar 2012 09:05:59 GMT</pubDate>
    </item>
    </channel>
</rss>