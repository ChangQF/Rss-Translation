<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 18 Aug 2024 01:12:20 GMT</lastBuildDate>
    <item>
      <title>Sklearn 的分类报告中的支持是否意味着原始数据集或输入模型的数据集中的出现？</title>
      <link>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</link>
      <description><![CDATA[我实现了一个机器学习模型；为了获得有关模型性能的一些信息，我查看了来自 sklearn.metrics 的分类报告。
例如，这是我的分类报告：
分类报告图片
我主要有两个问题：

正类和负类旁边的支持值（56 和 3147）与底部宏和加权平均值旁边的支持值（3203 和 3203）有什么区别，我应该使用哪一个？
从此链接，支持是每个类别中有多少个样本类。这是原始数据集中的样本，还是输入到机器学习模型中的样本？我之所以问这个问题，是因为我确实进行了重新采样，因为数据集是不平衡的。换句话说，正确的支持值是基于原始（不平衡）数据集还是输入到模型中的数据集（平衡）？

对于我的第一个问题，我相信“正确”的支持值是 3203 和 3203。这与我的第二个问题类似，因为我认为支持是基于输入到模型中的数据集，所以它应该是平衡的（因为模型如何“看到”原始数据集）？
顺便说一句，一切都在管道中，因此没有数据泄漏或模型“看到”测试数据，如果这可能相关的话。
我的问题与上面链接中的问题不是重复的，因为我问的是整个分类报告，而不仅仅是其中的一部分。]]></description>
      <guid>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</guid>
      <pubDate>Sat, 17 Aug 2024 23:48:10 GMT</pubDate>
    </item>
    <item>
      <title>在sklearn的ClassificationReport中，科学“使用”的度量宏是平均值还是加权平均值？</title>
      <link>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</link>
      <description><![CDATA[在 Python 的 sklearn.metrics 中，我使用分类报告来帮助我解释不平衡数据集上的机器学习模型。例如，这是一个任意分类报告：
分类报告
在报告指标（例如精度）时，我们报告的是宏平均值（0.51）还是加权平均值（0.98）？我会假设是宏平均值，因为这会惩罚正类上较差的模型表现。我这样说对吗？
我不认为这是与text重复的问题，因为我基本上是在问使用宏还是加权平均值更有用（或在实践中使用更多）。
我曾尝试在线搜索此问题。我没有看到明确的答案，但我相信我们使用宏平均值。]]></description>
      <guid>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</guid>
      <pubDate>Sat, 17 Aug 2024 21:45:05 GMT</pubDate>
    </item>
    <item>
      <title>如何建立机器学习模型，从包含用户市场交易的 Excel 文件中提取所需的交易数据</title>
      <link>https://stackoverflow.com/questions/78883203/how-to-build-a-machine-leaning-model-to-extract-required-transactional-data-from</link>
      <description><![CDATA[我有数百个 Excel 文件，其中包含各种格式的金融交易（例如股票、共同基金等）。每个文件可以包含一个或多个表格，并且表格位于不同工作表的不同位置，与不同的经纪人（如 zerodha、groww 等）相关。
我的目标是训练一个机器学习模型，该模型可以自动识别并从任何给定的 Excel 文件中提取相关表格，即使格式不熟悉。
这是数据：https://drive.google.com/drive/folders/1YixwjLg2ZskRXMI5WMjns5kD1Ujfpphd?usp=sharing
每个经纪人都有不同的格式，他们以不同的格式将这些数据提供给用户，因此我无法手动解析它，因为格式经常更改。
就像图像、pdf 文件我们在一定数量的文件上进行训练，然后给出新的格式，即使格式发生变化，模型也可以精确地提供所需的数据。挑战在于在 excel 文件上执行此操作。
我尝试用表格位置的坐标和该范围内每个单元格中的数据类型注释 excel 文件，但发现它们作为特征无关紧要。
除了机器学习，我还尝试使用 fuzzy-wuzzy 库，这样每当经纪人更改列标题时，它都会映射到单个标题 - 例如 - “购买日期”、“入场日期”将映射到“购买日期”，但这并不能解决问题。
请帮帮我，我已经进行了广泛的搜索和研究，但还是搞不清楚。]]></description>
      <guid>https://stackoverflow.com/questions/78883203/how-to-build-a-machine-leaning-model-to-extract-required-transactional-data-from</guid>
      <pubDate>Sat, 17 Aug 2024 20:26:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv10 和 RTSP 流的车牌识别系统中 RAM 和存储使用率较高</title>
      <link>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</link>
      <description><![CDATA[我正在开发一个车牌识别系统，使用来自安全摄像头的 RTSP 流来识别带有阿拉伯字母/数字的埃及车牌。我的设置包括：

YOLOv10 模型 1：检测和跟踪汽车。

YOLOv10 模型 2：检测车内的车牌。

YOLOv10 模型 3：对车牌上的字符和数字进行 OCR。


我正在使用 Python 推理库，将模型导出为 .engine 格式以实现 GPU 加速。
问题：

RAM 使用情况：系统每路摄像头信号消耗高达 8 GB 的 RAM。

存储使用情况：需要 15-20 GB 的存储空间来管理软件包。

性能：尽管使用了 GPU，但系统仍然资源密集。


我预计 GPU 加速会显著降低 RAM 和存储需求，但我没有看到预期的效率。类似产品 Plate Recognizer 的运行资源要少得多（0.5 GB RAM，无 GPU）（参考链接）。
这是车牌识别器使用的软件包列表，也许有人可以帮助我了解它们如何如此高效地工作：
certifi==2024.6.2
cffi==1.16.0
charset-normalizer==3.3.2
configobj==5.0.8
cryptography==41.0.1
idna==3.7
Levenshtein==0.21.1
ntplib==0.4.0
numpy==1.24.4
opencv-python-headless==4.7.0.72
openvino==2023.3.0
openvino-telemet ry==2024.1.0
persist-queue==0.8.1
psutil==5.9.5
pycparser==2.22
python-dateutil==2.8.2
python-Levenshtein==0.21.1
rapidfuzz==3.9.3
requests==2.32.3
rollbar==0.16.3
scipy==1.10.1
six==1.16.0
urllib3==2.2.1
什么我尝试过：

模型优化：导出到 .engine 进行 GPU 加速。

流管理：使用 Python 推理库来处理 RTSP 流。


问题：

如何减少此设置中的 RAM 和存储使用量？

是否有可能更有效的替代模型或方法？

有任何提高性能的一般技巧吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</guid>
      <pubDate>Sat, 17 Aug 2024 19:53:15 GMT</pubDate>
    </item>
    <item>
      <title>Azure AI 自定义图像分析模型未返回性能指标</title>
      <link>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</link>
      <description><![CDATA[成功训练并使用自定义图像分析模型后，我没有获得任何性能指标。在两个地方 - Vision Studio 的 UI 和执行代码，使用此代码时我都会收到以下错误：
fromcognitive_service_vision_model_customization_python_samples import 

Evaluation
eval_dataset_name = &#39;{specify_your_eval_dataset_name}&#39;
evaluation_name = &#39;{specify_your_eval_run_name}&#39;

evaluation = Evaluation(evaluation_name, model_name, eval_dataset_name)
training_client.evaluate_model(evaluation)

evaluation = training_client.wait_for_evaluation_completion(model_name, evaluation_name)

Status: Error(code={&#39;code&#39;: &#39;InternalServerError&#39;, &#39;message&#39;: &#39;Batch did not complete successful. OutputResultWriteFailed: An accidental error was happened.&#39;}, message=&#39;&#39;, target=&#39;&#39;, details=[])

我遵循 Azure 提供的此指南 - https://github.com/Azure-Samples/cognitive-service-vision-model-customization-python-samples/blob/main/docs/cognitive_service_vision_model_customization.ipynb
使用我用于训练的同一数据集。我知道从 ML 的角度来看，这种方法不是正确的方法，但我想确保数据集包含正确的图片和标签。
您有什么建议，如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</guid>
      <pubDate>Sat, 17 Aug 2024 08:44:21 GMT</pubDate>
    </item>
    <item>
      <title>如何加速随机森林回归和SVR的训练？</title>
      <link>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</link>
      <description><![CDATA[我正在尝试使用以下数据集创建一个回归模型来预测比特币的收盘价：https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv
它有超过 60 万条记录，包含 15 个特征（其中一些是我创建的）。
我曾多次尝试在 google colab 和我的笔记本电脑上对其进行训练。我甚至把它放了一夜，但它花了太长时间。
有什么方法可以加快速度吗？
笔记本电脑规格：
CPU：Ryzen 7 5800H 
GPU：RTX 3050 
RAM：16 GB

这是训练代码：
models = {
&#39;线性回归&#39;：{
&#39;model&#39;：LinearRegression()，
&#39;params&#39;：{}
},
&#39;Ridge 回归&#39;：{
&#39;model&#39;：Riddom_state=42，
&#39;params&#39;：{&#39;alpha&#39;：[0.01, 0.1, 1, 5, 10, 50, 100]}
},
&#39;Lasso 回归&#39;：{
&#39;model&#39;： Lasso(random_state=42),
&#39;params&#39;: {&#39;alpha&#39;: [0.001, 0.01, 0.1, 1, 10]}
},
&#39;决策树&#39;: {
&#39;模型&#39;: DecisionTreeRegressor(random_state=42),
&#39;params&#39;: {&#39;max_depth&#39;: [None, 5, 10, 20], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;随机森林&#39;: {
&#39;模型&#39;: RandomForestRegressor(random_state=42),
&#39;params&#39;: {&#39;n_estimators&#39;: [50, 100, 200], &#39;max_depth&#39;: [None, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;支持向量回归&#39;: {
&#39;model&#39;: SVR(),
&#39;params&#39;: {&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: [0.1, 1, 10], &#39;epsilon&#39;: [0.01, 0.1, 1]}
}
}

results = {}

for model_name, model_data in models.items():
print(f&quot;Tuning {model_name}&quot;)
grid_search = GridSearchCV(model_data[&#39;model&#39;], model_data[&#39;params&#39;], cv=5,scoring=&#39;neg_mean_squared_error&#39;, verbose=1)
grid_search.fit(X_train, y_train)

# 获取最佳模型
best_model = grid_search.best_estimator_

# 预测
y_pred = best_model.predict(X_test)

# 性能指标
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

results[model_name] = {
&#39;MAE&#39;: mae,
&#39;MSE&#39;: mse,
&#39;RMSE&#39;: rmse,
&#39;R2&#39;: r2,
&#39;最佳模型&#39;: best_model,
&#39;最佳参数&#39;: grid_search.best_params_
}
]]></description>
      <guid>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</guid>
      <pubDate>Sat, 17 Aug 2024 05:30:15 GMT</pubDate>
    </item>
    <item>
      <title>如何打包使用SAM语义分割模型的Python项目？</title>
      <link>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</link>
      <description><![CDATA[我有一个 Python 项目，我使用 Segment-Anything 模型的结果来确定通道内的气泡位置。现在，我需要打包应用程序，然后将其交给我的同事，但我不想在他们所有的电脑上安装和调试 pytorch 和 conda。
我目前正在尝试使用 Briefcase 和 PyOxidizer，一旦我尝试它们，我将尝试更新它。
我确实想支持 mac，因为他们主要使用 mac，但我可以强制他们使用 windows。
有什么技巧或窍门可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</guid>
      <pubDate>Fri, 16 Aug 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>实现 nn.Bilinear 层</title>
      <link>https://stackoverflow.com/questions/78870012/implementing-nn-bilinear-layer</link>
      <description><![CDATA[有人能帮我理解 nn.Bilinear 的实现吗？

根据文档，此函数实现 y = x1T * A * x2

取 x1 = (100,20) ， x2 = (100,30&#39;) ，假设 output_features = 50。矩阵 A 的维度为 [50,20,30]。
我发现很难将这些矩阵相乘以获得 output = [100,50]
根据 x1、x2 和 A 矩阵的大小，根据 y = x1T * A * x2 ，乘法似乎不兼容。我在这里遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78870012/implementing-nn-bilinear-layer</guid>
      <pubDate>Wed, 14 Aug 2024 09:01:18 GMT</pubDate>
    </item>
    <item>
      <title>加载权重时出现意外键错误</title>
      <link>https://stackoverflow.com/questions/78863529/getting-unexpected-keys-error-while-loading-weights</link>
      <description><![CDATA[import torch
from PIL import Image
import numpy as np
from effdet import get_efficientdet_config, EfficientDet

config = get_efficientdet_config(&#39;tf_efficientdet_d0&#39;)
model = EfficientDet(config, pretrained_backbone=True)
model.eval()

当我运行此程序时，我收到错误
加载预训练权重时发现意外键（bn2.bias、bn2.num_batches_tracked、bn2.running_mean、bn2.running_var、bn2.weight、classifier.bias、classifier.weight、conv_head.weight）。如果模型正在调整，则可能会出现这种情况。

我研究了一下，发现这是由于 timm builder 造成的，但没有找到任何解决方案。如何解决这个问题？
我想加载 efficientdet 权重，但结果出现了意外的键错误]]></description>
      <guid>https://stackoverflow.com/questions/78863529/getting-unexpected-keys-error-while-loading-weights</guid>
      <pubDate>Mon, 12 Aug 2024 20:42:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能用 C++ 构建一个没有依赖关系的神经网络，即使它可以在 Numpy 中运行？</title>
      <link>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</guid>
      <pubDate>Mon, 12 Aug 2024 16:55:51 GMT</pubDate>
    </item>
    <item>
      <title>以矩阵为输入、以矩阵中位置为输出的神经网络 - 强化</title>
      <link>https://stackoverflow.com/questions/78862449/neural-network-for-matrix-as-input-and-position-in-matrix-as-output-reinforcem</link>
      <description><![CDATA[我在体育馆中设置了一个非常基本的环境，由一个 nxn 矩阵组成，由 0 和 1 填充。神经网络现在应该输出一个向量，指向矩阵的一个特定条目。
现在，这应该只是一个带有 0 的条目，然后将其更改为 1：本质上，人工智能通过强化学习用 1 填充矩阵。稍后，我希望它找到合适的位置将形状放入矩阵中（有点像俄罗斯方块游戏，只是方块不会掉落）。
无论如何，到目前为止，我一直使用 DQN 模型，但这似乎不太适合这里。有人能告诉我这种输入/输出设置的更好方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78862449/neural-network-for-matrix-as-input-and-position-in-matrix-as-output-reinforcem</guid>
      <pubDate>Mon, 12 Aug 2024 15:23:25 GMT</pubDate>
    </item>
    <item>
      <title>使用相关的 Sagemaker HP 调整作业作为热启动父作业</title>
      <link>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</guid>
      <pubDate>Mon, 12 Aug 2024 11:58:24 GMT</pubDate>
    </item>
    <item>
      <title>获取 ValueError：所有数组的长度必须相同</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[我一直试图将字典转换为数据框，但每次我都收到 ValueError：所有数组的长度必须相同。我已经检查了每个数组的长度并确认它们相同，但我仍然收到相同的错误
def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# 合并来自训练集和测试集的指标
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# 排序列 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# 获取指标表
metrics_table = metrics_from_pipes(pipelines)

运行此代码会出现此错误
ValueError Traceback (most recent call last)
Cell In[45]，第 82 行
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # 获取指标表
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;表 1：基本模型指标&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

单元格 In[45]，第 50 行，位于 metrics_from_pipes(pipes_dict)
41 # 将性能指标列表聚合到单独的数据框中
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: 所有数组的长度必须相同

当我检查 train_metrics 和 val 指标的字典结果时，我得到了这个
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;线性回归&#39;,
&#39;随机森林回归器&#39;,
&#39;梯度提升回归&#39;,
&#39;额外树回归器&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么剪枝后参数数量没有减少？</title>
      <link>https://stackoverflow.com/questions/78857798/why-are-the-number-of-parameters-not-decreasing-after-pruning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78857798/why-are-the-number-of-parameters-not-decreasing-after-pruning</guid>
      <pubDate>Sun, 11 Aug 2024 08:10:20 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 PyTorch 和 Ultralytics Yolo 代码以利用 GPU？</title>
      <link>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</link>
      <description><![CDATA[我正在做一个涉及对象检测和跟踪的项目。对于对象检测，我使用 yolov8，对于跟踪，我使用 SORT 跟踪器。运行以下代码后，我的 GPU 使用率始终低于 10%，而 CPU 使用率始终超过 40%。我安装了 cuda、cudnn，并使用 cuda 安装了 torch。我还编译了支持 cuda 的 opencv。我正在使用 RTX 4060 ti，但看起来它没有被使用。
有没有办法进一步优化下面的代码，以便所有工作都由 GPU 而不是 CPU 处理？
from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO(&#39;yolov8s.pt&#39;).to(device)

cap = cv2.VideoCapture(0)

while True:
ret, frame = cap.read() 
if not ret:
print(&quot;**未收到帧**&quot;)
继续

results = model(frame)
dets_to_sort = np.empty((0, 6))
for result in results:
for obj in result.boxes:
bbox = obj.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox

conf = obj.conf.item()
class_id = int(obj.cls.item())
dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))

tracked_dets = sort_tracker.update(dets_to_sort)
for det in tracked_dets:
x1, y1, x2, y2 = [int(i) for i in det[:4]]
track_id = int(det[8]) if det[8] 不为 None else 0
class_id = int(det[4])
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1)
如果 key == ord(&quot;q&quot;):
break
如果 key == ord(&quot;p&quot;):
cv2.waitKey(-1)

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</guid>
      <pubDate>Sun, 30 Jun 2024 07:43:52 GMT</pubDate>
    </item>
    </channel>
</rss>