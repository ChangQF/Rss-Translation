<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 07 Jun 2024 12:28:59 GMT</lastBuildDate>
    <item>
      <title>我可以训练一个逻辑回归模型来将 ML 模型组合起来形成一个集成吗？</title>
      <link>https://stackoverflow.com/questions/78591369/can-i-train-a-logistic-regression-model-for-combining-ml-models-to-form-an-ensem</link>
      <description><![CDATA[我有 3 个经过训练的 ML 模型，用于对数据集进行分类。我想将它们组合成一个集成模型。我知道有多种方法可以做到这一点 - 投票分类器、堆叠、装袋、提升等。
但我想组合概率而不是类别，我认为这可以通过加权总和来实现。为了将权重分配给每个模型的概率，而不是尝试多种权重组合，我得到了一个建议，即使用逻辑回归，将概率值作为特征。这是一种从我的每个模型中获取概率值的权重或系数的可接受方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78591369/can-i-train-a-logistic-regression-model-for-combining-ml-models-to-form-an-ensem</guid>
      <pubDate>Fri, 07 Jun 2024 10:38:34 GMT</pubDate>
    </item>
    <item>
      <title>寻找快速大规模图像嵌入比较数据库结构</title>
      <link>https://stackoverflow.com/questions/78591334/looking-for-fast-large-scale-image-embeddings-comparison-database-structure</link>
      <description><![CDATA[我计划将 100-200 万个实体存储到数据库中。每个实体将链接到 10-100 个图像嵌入。目标是使用可能属于也可能不属于数据库的外部图像，并将其与每个存储的嵌入进行比较，以找到它可能属于的最相似的实体。
我想问一下，哪种数据库最适合这项任务，为什么？与 Milvus 或 FAISS 等更专业的数据库相比，带有 pgvector 的 PostgreSQL 会慢多少？]]></description>
      <guid>https://stackoverflow.com/questions/78591334/looking-for-fast-large-scale-image-embeddings-comparison-database-structure</guid>
      <pubDate>Fri, 07 Jun 2024 10:31:33 GMT</pubDate>
    </item>
    <item>
      <title>我的 UNet 图像重建模型无法学习</title>
      <link>https://stackoverflow.com/questions/78591272/my-unet-image-reconstruction-model-wont-learn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78591272/my-unet-image-reconstruction-model-wont-learn</guid>
      <pubDate>Fri, 07 Jun 2024 10:13:54 GMT</pubDate>
    </item>
    <item>
      <title>Gradio 用于实时语音对话</title>
      <link>https://stackoverflow.com/questions/78591124/gradio-for-live-voice-conversation</link>
      <description><![CDATA[我正在努力为我的实时语音助手设置 gradio，用户和系统应该像 siri 一样持续地互相交谈，但我不知道输入和输出变量应该使用什么
`conversation_interface = gr.Interface(
fn=voice_assistant,
inputs=gr.inputs.Microphone(label=&quot;Speak Here&quot;),
outputs=gr.outputs.Textbox(label=&quot;Assistant&#39;s Response&quot;),
title=&quot;Bright Dentistry&quot;,
description=&quot;Welcome to Bright Dentistry here we do care of your dental!.&quot;,
allow_flagging=False,

)`
我试图摆脱变量，但似乎 gradio 需要它们]]></description>
      <guid>https://stackoverflow.com/questions/78591124/gradio-for-live-voice-conversation</guid>
      <pubDate>Fri, 07 Jun 2024 09:44:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 上的自定义训练模型进行实时音频分类</title>
      <link>https://stackoverflow.com/questions/78590866/real-time-audio-classification-using-a-custom-trained-model-on-cnn</link>
      <description><![CDATA[我通过构建 CNN 架构自定义训练了一个模型，用于检测机器产生的不同声音以进行状态监控。现在我想实时运行这个模型，以便它可以从麦克风接收音频输入并预处理数据，将其传递给模型 (.h5) 文件并预测音频属于哪个标签。
我没有太多的编程经验，但无论我写什么代码，都会出现形状输入错误，我无法解决它，我也不知道我写的方法是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/78590866/real-time-audio-classification-using-a-custom-trained-model-on-cnn</guid>
      <pubDate>Fri, 07 Jun 2024 08:54:23 GMT</pubDate>
    </item>
    <item>
      <title>在训练过程中，我遇到了以下 Python 错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78590831/while-training-i-ran-into-the-following-python-error</link>
      <description><![CDATA[Epoch 1/100 Traceback（最近一次调用最后一次）：
文件“C:\Users\noor_\Desktop\Code V1 and V2 Caps\code\Squash Code try it\Oral.py”，第 384 行，位于&lt;module&gt; 
train(model=model, data=((x_train, y_train), (x_test, y_test)), class_names=classNames, ar

这是我在运行代码时遇到的错误。有人能解决吗？]]></description>
      <guid>https://stackoverflow.com/questions/78590831/while-training-i-ran-into-the-following-python-error</guid>
      <pubDate>Fri, 07 Jun 2024 08:45:55 GMT</pubDate>
    </item>
    <item>
      <title>聚类时间太长，内存不断出错</title>
      <link>https://stackoverflow.com/questions/78590456/clustering-time-is-too-long-and-memory-keeps-causing-errors</link>
      <description><![CDATA[包含10,000,000个样本，每个样本为一行，第一个样本号，剩余64个样本特征。数据来自DNA序列 最大聚类数为1000000，如果超过1000000，则多余的聚类会合并到第1000000个聚类中。评价标准为簇内距离离合器。
from sklearn.cluster import Birch
from sklearn.metrics import pairwise_distances
branching_factor = 50
n_clusters = 1000000 # 设置一个较大的聚类数
threshold = 0.5

# 创建BIRCH聚类器
birch = Birch(n_clusters=n_clusters, Threshold=threshold, Branching_factor=branching_factor)

#训练BIRCH聚类器
birch.fit(reduced_data)

BUT MemoryError: 无法为形状为 (49604310962128,) 且数据类型为 float64 的数组分配 361. TiB
我该怎么办？还有其他可用的聚类方法或技术吗？]]></description>
      <guid>https://stackoverflow.com/questions/78590456/clustering-time-is-too-long-and-memory-keeps-causing-errors</guid>
      <pubDate>Fri, 07 Jun 2024 07:27:16 GMT</pubDate>
    </item>
    <item>
      <title>AUROC = (Sum(TP)+Sum(TN)) / P+N 是否正确？[关闭]</title>
      <link>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</link>
      <description><![CDATA[我正在阅读一篇论文“一种用于洪水敏感性评估的新型混合人工智能方法”。该论文提到 ROC 曲线下面积 (AUROC) 的方程是：

这个方程正确吗？我认为这是一种计算 AUC 的非常简单的方法。任何想法或想法，我都想在我的报告中使用该方程。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</guid>
      <pubDate>Fri, 07 Jun 2024 02:14:29 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 flax 0.6.1 加载 flax 0.5.3 中保存的检查点</title>
      <link>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</link>
      <description><![CDATA[我使用 flax 0.5.3 保存了程序中的检查点。当我尝试将其加载到使用 flax 0.6.1 的程序中时，我收到一条错误消息：“列表的大小和状态字典不匹配”。唯一版本发生变化的 Python 包是 flax 及其依赖项。以下是堆栈跟踪。是否可以在 flax 0.6.1 或更高版本中加载使用 flax 0.5.3 保存的检查点？有没有办法将检查点迁移到较新版本的 flax？
 app.run(main)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 308 行，在 run
_run_main(main, args)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 254 行，在 _run_main
sys.exit(main(argv))
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 739 行，在 main
run_alphageometry(
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 652 行，在 run_alphageometry
bqsearch_init()
文件&quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 529 行，在 bqsearch_init 中
model = get_lm(_CKPT_PATH.value, _VOCAB_PATH.value)
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 213 行，在 get_lm 中
return lm.LanguageModelInference(vocab_path, ckpt_init, mode=&#39;beam_search&#39;)
文件 &quot;/home/peng/ag4masses/alphageometry/lm_inference.py&quot;，第 62 行，在 __init__ 中
(tstate, _, imodel, prngs) = trainer.initialize_model()
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 394 行，在 initialize_model 中
tstate = self.restore_checkpoint(tstate)
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 439 行，位于 restore_checkpoint
loaded_train_state = checkpoints.restore_checkpoint(load_dir, train_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/training/checkpoints.py&quot;，第 752 行，位于 restore_checkpoint
return serialization.from_state_dict(target, state_dict)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，位于 from_state_dict
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第149，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件&quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 156 行，在 &lt;lambda&gt; 中
lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)))
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;, 第 110 行, _restore_list
引发 ValueError(&#39;列表的大小和状态字典不匹配,&#39;
ValueError: 列表的大小和状态字典不匹配, 得到 6 和 1。

尝试将 flac 从 0.5.3 升级到 0.6.1，并使用 flax 0.5.3 加载代码保存的检查点。但出现上述错误。]]></description>
      <guid>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</guid>
      <pubDate>Fri, 07 Jun 2024 02:06:11 GMT</pubDate>
    </item>
    <item>
      <title>如果训练数据集中的正样本多于负样本，XGBoost 的 scale_pos_weight 是否可以正确平衡正样本？</title>
      <link>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</link>
      <description><![CDATA[经过研究，我意识到 scale_pos_weight 通常计算为训练数据中负样本数量与正样本数量的比率。我的数据集有 840 个负样本和 2650 个正样本，因此比率为 0.32。如果我的样本反过来，我相信 scale_pos_weight 会是一种更好的方法。
是否可以安全地假设，由于比率小于 1，它仍将正确平衡类别？特异性在我的研究中很重要，但我们的主要目标集中在召回率、精确度和 F1 分数上。此设置是否会通过最大程度地影响特异性而导致更多的假阳性？]]></description>
      <guid>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</guid>
      <pubDate>Thu, 06 Jun 2024 14:27:52 GMT</pubDate>
    </item>
    <item>
      <title>卡在恶意软件功能提取上，难以找到自动化工具[关闭]</title>
      <link>https://stackoverflow.com/questions/78585761/stuck-at-the-malware-features-extraction-difficulty-at-finding-automated-tools</link>
      <description><![CDATA[我正在尝试构建一个机器学习恶意软件检测工具；我发现了多个数据集，包括微软恶意软件分类挑战 (2015) 和 CSV 数据集。问题是我卡在了特征提取上，特别是在 Windows 上自动执行静态和动态分析的工具。
我读过关于 PE（可移植可执行文件）工具（如 pestudio）、沙箱（如 cuckoo sandbox）的文章，但我卡在了需要自动执行分析的部分（即通过 python 代码调用工具）。
我曾尝试使用 IDA 反汇编程序将二进制文件转换为 .asm 和 .bytes，但似乎我需要专业版才能自动执行转换。
如果有人可以推荐我可以用来自动执行 Windows 上的分析和特征提取的工具，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78585761/stuck-at-the-malware-features-extraction-difficulty-at-finding-automated-tools</guid>
      <pubDate>Thu, 06 Jun 2024 09:43:08 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure Databricks 中使用 pyspark ML 库函数时出现 Py4J 安全错误</title>
      <link>https://stackoverflow.com/questions/78585509/py4j-security-error-while-using-pyspark-ml-library-functions-in-azure-databricks</link>
      <description><![CDATA[我试图在我的 Azure Databricks 工作簿中运行以下代码
import pyspark.ml.feature
from pyspark.ml.feature import Tokenizer,StopWordsRemover
tokenizer = Tokenizer()

但是我遇到了这个错误：
Py4JError：调用 
None.org.apache.spark.ml.feature.Tokenizer 时发生错误。跟踪：
py4j.security.Py4JSecurityException：构造函数 public 
org.apache.spark.ml.feature.Tokenizer(java.lang.String) 未列入白名单。

StopWordsRemover 和 pyspark.ml.feature 中的其他一些函数也会出现类似的错误
有没有办法避免此错误，以便我可以使用相同的代码？]]></description>
      <guid>https://stackoverflow.com/questions/78585509/py4j-security-error-while-using-pyspark-ml-library-functions-in-azure-databricks</guid>
      <pubDate>Thu, 06 Jun 2024 08:56:01 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML-如何从 Blob 容器注册模型？</title>
      <link>https://stackoverflow.com/questions/78584772/azure-ml-how-to-register-model-from-blob-container</link>
      <description><![CDATA[我有预定义的模型并存储在 blob 存储中。现在我想注册模型并部署它，并在 Azure ML 中启用端点。我使用了以下代码
from azureml.core import Model
from azureml.core import Workspace

subscription_id = &#39;mysub
resource_group = &#39;my_resource_group&#39;
workspace_name = &#39;my_ws_name&#39;

ws = Workspace(subscription_id, resource_group, working_name)

model_path = &#39;my_model.joblib&#39;

container = &#39;mycontainer&#39;

model_name = &#39;my_model_v1&#39;

model = Model.register(workspace=ws,
model_name=model_name,
model_path=model_path,
description=&quot;Test_Model&quot;,
tags={&#39;area&#39;: &quot;emotion detection&quot;},
model_framework=Model.Framework.SCIKITLEARN,
model_framework_version=&#39;0.24.1&#39;,
resource_configuration=None,
container_registry=None,
properties=None,
sample_input_dataset=None,
sample_output_dataset=None,
datasets=None,
model_url=f&#39;https://mystorage.blob.core.windows.net/{container}/{model_path}&#39;)
print(&quot;Ws object created&quot;)

但下面的代码返回错误
TypeError: register() 得到了一个意外的关键字参数 &#39;container_registry&#39;

有没有什么解决办法或者其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78584772/azure-ml-how-to-register-model-from-blob-container</guid>
      <pubDate>Thu, 06 Jun 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>加载 pickle NotFittedError：TfidfVectorizer - 词汇表不适合</title>
      <link>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</link>
      <description><![CDATA[多标签分类
我正在尝试使用 scikit-learn/pandas/OneVsRestClassifier/logistic 回归预测多标签分类。构建和评估模型有效，但尝试对新样本文本进行分类无效。
场景 1：
一旦我构建了一个模型，就使用名称 (sample.pkl) 保存该模型并重新启动我的内核，但是当我在对样本文本进行预测期间加载已保存的模型 (sample.pkl) 时，它给出了错误：
 NotFittedError：TfidfVectorizer - 词汇表不适合。

我构建了模型并评估了模型，然后我使用名称 sample.pkl 保存了该模型。我重启了内核，然后加载模型对样本文本进行预测 NotFittedError: TfidfVectorizer - 词汇表不适合
推理
import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # 性能矩阵
from sklearn.multiclass import OneVsRestClassifier # 二元相关性
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn.preprocessing 导入 MultiLabelBinarizer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.linear_model 导入 LogisticRegression
stop_words = set(stopwords.words(&#39;english&#39;))

def cleanHtml(sentence):
&#39;&#39;&#39;&#39; 删除标签 &#39;&#39;&#39;
cleanr = re.compile(&#39;&lt;.*?&gt;&#39;)
cleantext = re.sub(cleanr, &#39; &#39;, str(sentence))
return cleantext

def cleanPunc(sentence): 
&#39;&#39;&#39; 函数用于清除单词中的任何标点符号或特殊字符 &#39;&#39;&#39;
cleaned = re.sub(r&#39;[?|!|\&#39;|&quot;|#]&#39;,r&#39;&#39;,sentence)
cleaned = re.sub(r&#39;[.|,|)|(|\|/]&#39;,r&#39; &#39;,cleaned)
cleaned = cleaned.strip()
cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; 保留 alpha 句子 &quot;&quot;&quot;
alpha_sent = &quot;&quot;
for word in sentence.split():
alpha_word = re.sub(&#39;[^a-z A-Z]+&#39;, &#39; &#39;, word)
alpha_sent += alpha_word
alpha_sent += &quot; &quot;
alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; 删除停用词 &quot;&quot;&quot;
no_stopword_text = [w for w in text.split() if not w in stop_words]
return &#39; &#39;.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo plot movie_name gender_new
1 故事以 Hannah 开始... 唱歌 [戏剧,teen]
2 Debbie 最喜欢的乐队是 Dream... 最忠实的粉丝 [戏剧]
3 这个祖鲁家庭的故事是... 回来，非洲 [戏剧,纪录片]

获取错误
当我对示例文本进行推理时，我在这里获取错误
def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q)
q = remove_stopwords(q)
multilabel_binarizer = MultiLabelBinarizer()
tfidf_vectorizer = TfidfVectorizer()
q_vec = tfidf_vectorizer.transform([q])
q_pred = clf.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

for i in range(5):
print(i)
k = test1.sample(1).index[0] 
print(&quot;电影：&quot;, test1[&#39;movie_name&#39;][k], &quot;\n预测类型：&quot;, infer_tags(test1[&#39;plot&#39;][k])), print(&quot;实际类型：&quot;,test1[&#39;genre_new&#39;][k], &quot;\n&quot;)


已解决
我解决了将 tfidf 和 multibiniraze 保存到 pickle 模型中的问题
from sklearn.externals import joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load(&#39;/abc/downloads/tfidf_vectorizer.pickle&#39;)
multilabel_binarizer = joblib.load(&#39;/abc/downloads/multibinirizer_vectorizer.pickle&#39;)

def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q) 
q = remove_stopwords(q)
q_vec = vectorizer .transform([q])
q_pred = rf_model.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

我通过以下链接找到了解决方案
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;]]></description>
      <guid>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</guid>
      <pubDate>Fri, 26 Jul 2019 04:21:05 GMT</pubDate>
    </item>
    <item>
      <title>使用单类 SVM 计算异常检测的异常分数</title>
      <link>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</link>
      <description><![CDATA[我对使用单类 SVM 计算异常检测的异常分数有疑问。我的问题是：如何使用 decision_function(X) 计算它，就像我在隔离森林中计算异常分数一样？
非常感谢，]]></description>
      <guid>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</guid>
      <pubDate>Fri, 28 Dec 2018 09:50:20 GMT</pubDate>
    </item>
    </channel>
</rss>