<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 02 May 2024 21:13:19 GMT</lastBuildDate>
    <item>
      <title>进一步推进数据科学和机器学习</title>
      <link>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</link>
      <description><![CDATA[我想知道哪些应用程序、平台或书籍等可以极大地帮助我提升水平并深入了解以下内容：

熊猫
Numpy
Scipy
Keras 和 Tensorflow
Pytorch
Scikit 学习
Matplotlib

除了 pytorch 之外，我对所有这些都有经验，但我真的想走得更远，因为我未来的潜在工作需要这些。
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</guid>
      <pubDate>Thu, 02 May 2024 19:29:01 GMT</pubDate>
    </item>
    <item>
      <title>多通道 2D 阵列与多通道 3D 阵列</title>
      <link>https://stackoverflow.com/questions/78421308/multi-channel-2d-array-vs-multi-channel-3d-array</link>
      <description><![CDATA[谁能解释一下这两个数组之间的区别吗？一个代表它们的玩具示例（例如三个通道）将不胜感激。我在网上查了一下，但找不到具体的解释。谢谢。
多通道3D阵列：是指每个通道都有3D矩阵？
多通道二维数组：是指每个通道的二维矩阵？]]></description>
      <guid>https://stackoverflow.com/questions/78421308/multi-channel-2d-array-vs-multi-channel-3d-array</guid>
      <pubDate>Thu, 02 May 2024 19:16:52 GMT</pubDate>
    </item>
    <item>
      <title>Unsloth 未检测到 CUDA 和“str2optimizer32bit”</title>
      <link>https://stackoverflow.com/questions/78420830/unsloth-not-detecting-cuda-and-str2optimizer32bit</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78420830/unsloth-not-detecting-cuda-and-str2optimizer32bit</guid>
      <pubDate>Thu, 02 May 2024 17:30:58 GMT</pubDate>
    </item>
    <item>
      <title>使用多个性能指标执行递归特征消除</title>
      <link>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</link>
      <description><![CDATA[我正在尝试使分类器尽可能简洁。为此，我使用交叉验证递归地删除功能。
在我的模型中，精度是最重要的指标。但是，我还想看看随着模型中输入的功能减少，其他指标将如何演变。
特别是，我想评估模型的召回率和 F1 分数。
rfe = RFECV(
    estimator=clf, # 一个 XGBClassifier 实例
    步骤=1，
    min_features_to_select=1,
    cv=cv, # 一个 StratifiedKFold 实例
    评分=&#39;精度&#39;,
    # 评分=[&#39;f1&#39;, &#39;精度&#39;, &#39;召回&#39;], # 抛出错误
    详细=1，
    n_职位=1
）

我注释掉了将指标列表传递给 scoring 参数的行，因为它抛出 InvalidParameterError （也就是说，它不高兴我向它传递了一个列表）。
有没有办法将多个指标传递给 RFECV 实例？]]></description>
      <guid>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</guid>
      <pubDate>Thu, 02 May 2024 16:37:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用拥抱脸部转换器来测试具有 LLM 的数据集</title>
      <link>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</link>
      <description><![CDATA[我正在努力复制此存储库的结果，但使用其他 LLM，例如 LLAMA。我正在使用 google colab，我已经克隆了存储库并安装了所需的软件包。
他们说你可以使用 hugging face transformers 中的任何模型，但我不知道从哪里获取“model”和“model_args”参数：
# 在 ENEM 2022 上运行 GPT-4V 的 CoT 3-shot
python main.py \
--model chatgpt \
--model_args engine=gpt-4-vision-preview \
--tasks enem_cot_2022_blind,enem_cot_2022_images,enem_cot_2022_captions \
--description_dict_path description.json \
--num_fewshot 3 \
--conversation_template chatgpt

如果您转到 hugging face 中的 Llama 模型 并单击“在 Transformers 中使用”你得到这个：
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;)

所以我尝试使用&quot;model = meta-llama&quot; 和 &quot;model_args = Meta-Llama-3-8B&quot;，但没有用。
像这样：
!python main.py \
--model meta-llama \
--model_args Meta-Llama-3-8B \
--tasks enem_cot_2022_blind,enem_cot_2022_captions \
--description_dict_path description.json \
--num_fewshot 3

我得到：
选定的任务：[&#39;enem_cot_2022_blind&#39;, &#39;enem_cot_2022_captions&#39;]
回溯（最近一次调用）：
文件 &quot;/content/gpt-4-enem/main.py&quot;，第 112 行，在 &lt;module&gt;
main()
文件 &quot;/content/gpt-4-enem/main.py&quot;，第 81 行，在 main 中
results = evaluator.simple_evaluate(
文件 &quot;/content/gpt-4-enem/lm_eval/utils.py&quot;，第 164 行，在 _wrapper 中
return fn(*args, **kwargs)
文件 &quot;/content/gpt-4-enem/lm_eval/evaluator.py&quot;，第 66 行，在 simple_evaluate 中
lm = lm_eval.models.get_model(model).create_from_arg_string(
文件 &quot;/content/gpt-4-enem/lm_eval/models/__init__.py&quot;，第 16 行，在 get_model 中
return MODEL_REGISTRY[model_name]
KeyError: &#39;meta-llama&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</guid>
      <pubDate>Thu, 02 May 2024 16:20:28 GMT</pubDate>
    </item>
    <item>
      <title>使用“loss.backward()”函数优化模型参数问题</title>
      <link>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</link>
      <description><![CDATA[`导入GC
从 tqdm.notebook 导入 tqdm
将 matplotlib.pyplot 导入为 plt
torch.set_printoptions(sci_mode=False)
梯度范数=[]
损失=[]
对于 tqdm 中的纪元（范围（纪元））：
model_path = f“/kaggle/working/Training/ace_state_dict_{epoch+1}.pth”

torch.save(model.state_dict(), model_path)

模型.train()

总损失= 0



对于batch_idx，批量枚举(tqdm(train_dataloader, desc=f&#39;Epoch {epoch + 1}/{epochs}&#39;))：

    优化器.zero_grad()



    logits = model(batch[“输入”])

    目标=批次[“目标”]

    

    损失 = loss_fn(logits.view(-1, logits.size(-1)), Targets.float()) / 1000000000

    loss.backward()

    

    # 计算梯度范数

    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)

    gradient_norms.append(grad_norm)



    优化器.step()

    调度程序.step()

    总损失 += loss.item()

    

    如果batch_idx％100==0：

        print(f&#39;Batch {batch_idx}/{len(train_dataloader)}，损失：{total_loss/(batch_idx+1)}，梯度范数：{grad_norm}&#39;)



avg_loss = 总损失 / len(train_dataloader)

损失.append(avg_loss)

print(f&quot;火车损失：{avg_loss}&quot;)`

这是我的代码，我遇到了麻烦，我正在寻求有关我的模型问题的帮助，其中“loss.backward()”出现了问题。函数未按预期工作，导致“grad_norm”始终记录为“0.0”。这阻碍了每次迭代期间模型参数的优化。任何纠正此问题的指导或建议将不胜感激。作为参考，您可以在以下位置找到我的模型的完整源代码：
https://www.kaggle.com/code/cutedeadu943/transformerchatbot 
您的帮助将是无价的。预先感谢您的支持。
我尝试了超参数和学习率的各种组合，但不幸的是，问题仍然存在。此外，我尝试通过使用“torch.nn.utils.clip_grad_norm_()”来解决这个问题，但没有成功。值得注意的是，所有相关张量和模型参数均使用“requires_grad=True”设置。如果有人有见解或替代方法来解决这个问题，我将非常感谢您的意见。感谢您考虑我的帮助请求。]]></description>
      <guid>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</guid>
      <pubDate>Thu, 02 May 2024 16:00:27 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 GAN 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</link>
      <description><![CDATA[我正在尝试训练 GAN 模型来检测糖尿病视网膜病变图像，但它抛出错误。请帮忙。
图像数据集不为空我已尝试查看它
错误是：-
`纪元 1/50
回溯（最近一次调用最后一次）：
  文件“C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py”，第 65 行，在  中
    分类器.fit（X，Y，batch_size = 32，epochs = 50）
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py”，第 553 行，在 categorical_crossentropy 中
    引发值错误（
ValueError：参数“target”和“output”必须具有相同的形状。收到：target.shape=(无，3)，output.shape=(无，5)`

火车模型文件的代码是：-
将 numpy 导入为 np
导入imutils
导入系统
导入CV2
导入操作系统
从tensorflow.keras.utils导入to_categorical
从 keras.models 导入 model_from_json
从 keras.layers 导入 MaxPooling2D
from keras.layers import Dense、Dropout、Activation、Flatten
从 keras.layers 导入 Convolution2D
从 keras.models 导入顺序

图片 = []
图像标签 = []
目录=&#39;数据集&#39;
文件列表 = os.listdir(目录)
索引 = 0
对于 list_of_files 中的文件：
    子文件 = os.listdir(目录+&#39;/&#39;+文件)
    对于子文件中的子项：
        路径=目录+&#39;/&#39;+文件+&#39;/&#39;+子
        img = cv2.imread(路径)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        如果 img 为 None：
          print(&#39;路径错误：&#39;, 路径)
        别的：
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         图像.append(im2arr)
         image_labels.append(文件)
    打印（文件）

X = np.asarray(图像)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;形状 == &quot;+str(X.shape))
print(&quot;形状==&quot;+str(Y.shape))
打印（Y）
X = X.astype(&#39;float32&#39;)
X = X/255

np.save(“model/img_data.txt”,X)
np.save(“model/img_label.txt”,Y)

X = np.load(&#39;model/img_data.txt.npy&#39;)
Y = np.load(&#39;model/img_label.txt.npy&#39;)
打印（Y）
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet 迁移学习代码在这里
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
classifier.add(Convolution2D(32, 3, 3, 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
分类器.add(Flatten())
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 5, 激活 = &#39;softmax&#39;))
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）
分类器.fit（X，Y，batch_size = 32，epochs = 50）
classifier.save_weights(&#39;model/train.h5&#39;)
model_json = classifier.to_json()
使用 open(“model/train.json”, “w”) 作为 json_file：
    json_file.write(model_json)
打印（分类器.summary（））





我尝试过更改尺寸，但它不起作用，我无法理解这是版本错误还是代码错误，因此为了解决同样的问题，请提供解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</guid>
      <pubDate>Thu, 02 May 2024 15:39:54 GMT</pubDate>
    </item>
    <item>
      <title>预测 = model.predict(email_features_array)</title>
      <link>https://stackoverflow.com/questions/78420048/prediction-model-predictemail-features-array</link>
      <description><![CDATA[ 预测 = model.predict(email_features_array)
                 ^^^^^^^^^^^^^^
AttributeError：“numpy.ndarray”对象没有属性“预测”


我已经构建并训练了 ML 模型，我只想通过 Flask 使用它，但每当它要进入 mode.predict 时，它都会给我这个问题
这是我的代码：
这些是我的进口
导入pickle
进口再
导入字符串
从 nltk.tokenize 导入 word_tokenize
从 nltk.corpus 导入停用词
从 sklearn.feature_extraction.text 导入 CountVectorizer
import numpy as np # 导入 NumPy

加载预训练模型
&lt;前&gt;&lt;代码&gt;
模型 = pickle.load(open(&#39;c:/Users/7rbe2/OneDrive/SÙÍ กง/Main items/Grad project/Phishward/PhishWarden/app/Python/logistic_regression_model.pkl&#39;, &#39;rb&#39;))

# 初始化CountVectorizer
CV = CountVectorizer()

这里是清洁功能
def remove_special_characters(word):
    return word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation))

def remove_stop_words(单词):
    stop_words = set(stopwords.words(&#39;英语&#39;))
    返回 [如果单词不在 stop_words 中，则返回单词中的单词]

def remove_hyperlink(word):
    return re.sub(r&quot;http\S+&quot;, &quot;&quot;, word)

def fit_count_vectorizer(文本):
    # 清理并标记文本
    clean_text = 删除_特殊_字符（文本）
    clean_text = 删除_超链接(cleaned_text)
    标记 = word_tokenize(cleaned_text)
    标记=remove_stop_words(标记)
    clean_text = &#39; &#39;.join(tokens)
    return clean_text # 返回清理后的文本

这是问题发生的预测
def 预测():
    email_text =“你好世界”
    clean_text = fit_count_vectorizer(email_text)
    cv.fit([cleaned_text]) # 让 CountVectorizer 适合已清理的文本
    email_features_array = cv.transform([cleaned_text]) # 使用transform而不是fit_transform
    # 假设`model`已经被定义和训练
    预测 = model.predict(email_features_array)

    # 应用预训练模型来预测电子邮件被钓鱼的概率
    概率 = model.predict_proba(email_features_array)
    如果预测[0] == 1：
        结果=“网络钓鱼”
        概率得分 = 概率[0][1] * 100
        打印（结果，概率分数）
    别的：
        结果=&#39;合法&#39;
        概率得分 = 概率[0][0] * 100
        打印（结果，概率分数）


# 用法示例
预测（）

我检查了我的 ML 模型的类型并得到了这个：
;

，我不知道我能做什么来解决这个问题，我几乎尝试了所有可能的方法]]></description>
      <guid>https://stackoverflow.com/questions/78420048/prediction-model-predictemail-features-array</guid>
      <pubDate>Thu, 02 May 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>“numpy.ndarray”在 Ai 模型中插入 Flask 时出现问题</title>
      <link>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</link>
      <description><![CDATA[我想将字符串插入到机器学习模型中，但它一直这样说：
 预测 = model.predict(email_features_array)
                 ^^^^^^^^^^^^^^
AttributeError：“numpy.ndarray”对象没有属性“预测”

不知道是什么问题，换了好几次解码方式。
这是我在进行矢量化后使用的函数：
def 预测():
    email_text =“你好世界”
    CV = CountVectorizer()
    email_features_array = cv.fit_transform([email_text])
    # email_features_array = fit_count_vectorizer(email_text)
    打印（电子邮件特征数组）
    # 假设`model`已经被定义和训练
    预测 = model.predict(email_features_array)

    # 应用预训练模型来预测电子邮件被钓鱼的概率
    概率 = model.predict_proba(email_features_array)
    如果预测[0] == 1：
        结果=“网络钓鱼”
        概率得分 = 概率[0][1] * 100
        打印（结果，概率分数）
    别的：
        结果=&#39;合法&#39;
        概率得分 = 概率[0][0] * 100
        打印（结果，概率分数）

我预计解码方式有问题，但不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</guid>
      <pubDate>Thu, 02 May 2024 11:10:21 GMT</pubDate>
    </item>
    <item>
      <title>如何将关系数据库集成到数据科学项目中？[关闭]</title>
      <link>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</link>
      <description><![CDATA[我是一名数据科学家，主要使用 CSV 文件进行数据分析，但我现在正在探索在我的项目中使用关系数据库。我想了解将关系数据库集成到我的工作流程中的最佳实践。
我应该如何将数据从关系数据库（例如 PostgreSQL、MySQL）导入到我的数据科学环境（例如 Python、R）中？我应该直接在数据库中执行连接和探索性数据分析，还是应该将数据导出到 CSV 文件，然后继续分析？
我过去主要使用 CSV 文件，但现在我正在着手一个现实世界的数据科学项目，我需要在其中使用关系数据库。不过，我对此还比较陌生，正在寻求有关如何有效地将数据库集成到我的工作流程中的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</guid>
      <pubDate>Thu, 02 May 2024 10:49:01 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：列表索引超出streamlit范围[关闭]</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 url 中提取信息并从中学习，然后用户可以向模型询问与 url 中的文章相关的问题，模型将提供合适的答案。
我在我的笔记本上执行了此操作，它工作得很好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误，我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块
 main_placeholder = sl.empty()
    llm = ChatGoogleGenerativeAI(模型 = &#39;gemini-pro&#39;)
    如果 process_url_clicked:
        加载器 = UnstructedURLLoader(urls = urls)
        main_placeholder.text(&quot;数据加载...开始...✅✅✅&quot;)
        数据 = 加载器.load()
        text_splitter = RecursiveCharacterTextSplitter(
            分隔符 = [&#39;\n&#39;,&#39;\n\n&#39;,&#39;.&#39;,&#39;,&#39;],
            块大小 = 1000,
            块重叠 = 200
        ）
        main_placeholder.text(&quot;文本分割器...开始...✅✅✅&quot;)
        文档 = text_splitter.split_documents(数据)
        嵌入 = GoogleGenerativeAIEmbeddings(模型 = &#39;models/embedding-001&#39;)
        矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 Streamlit 应用程序的回溯
IndexError：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
    exec（代码，模块.__dict__）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，在  中
    vectorstore_openai = FAISS.from_documents（文档，嵌入）
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
    返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
    返回 cls.__from(
           ^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
    索引 = faiss.IndexFlatL2(len(embeddings[0]))
                                  ~~~~~~~~~~^^^

就像我上面说的，这在我的笔记本上完美运行，我很困惑为什么会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中累积损失/梯度时如何处理 BatchNorm？[关闭]</title>
      <link>https://stackoverflow.com/questions/78418346/how-to-deal-with-batchnorm-when-accumulating-loss-gradients-in-pytorch</link>
      <description><![CDATA[由于硬件限制，我正在尝试增加有效批量大小。为了做到这一点，我对几个小批量进行 N 次前向传递，累积损失。然而，众所周知，BatchNorm 具有跟踪均值和标准差的运行统计数据。对于较小的物理小批量大小（即 1），甚至无法重现较大批量大小的训练。如何处理这个问题？
在我看来，有以下内容：“手动更新 BatchNorm 统计数据”、“禁用跟踪批量统计数据”、“减少动量”和“使用幽灵批量归一化”。然而，手动更新成本高昂，禁用批量统计跟踪只会禁用批量归一化的好处，当物理批量大小为1时，减少动量并不能解决问题，而幽灵批量归一化则与我试图解决的问题相反。
复制者：
导入火炬
从火炬导入 nn，优化
从 torch.utils.data 导入 DataLoader，TensorDataset


网络类（nn.Module）：
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.norm = nn.BatchNorm1d(3)
        # self.conv = nn.Conv1d(256, 256, 3)
        # self.nl = nn.ReLU()
        self.head = nn.Linear(3, 10)

    defforward(self, 输入：torch.Tensor):
        输入 = self.norm(输入)
        # 输入 = self.conv(输入)
        # 输入 = self.nl(输入)
        输入 = 输入.mean(dim=2)
        输入 = self.head(输入)
        返回输入


loss_function = nn.CrossEntropyLoss()


def train（模型，优化器，加载器，时期，积累_步骤）：
    模型.train()
    对于范围内的纪元（纪元）：
        model.zero_grad()
        累积损失 = 0
        对于枚举（加载器）中的 i（输入，目标）：
            输出 = 模型（输入）
            损失=损失函数（输出，目标）
            损失 = 损失 / 累积步数
            loss.backward()
            累积损失 += loss.item()

            如果 (i + 1) % 累积步数 == 0:
                优化器.step()
                model.zero_grad()
                print(f&#39;Epoch {epoch}, 步骤 {i+1}, 损失: {accumulated_loss}&#39;)
                累积损失 = 0


def 验证（模型，加载器）：
    模型.eval()
    使用 torch.no_grad()：
        valid_loss = torch.zeros(1)
        对于输入，加载器中的目标：
            输出 = 模型（输入）
            valid_loss += loss_function(输出, 目标).item()
        valid_loss /= len(加载器)
    返回有效损失


火炬.manual_seed(1)

＃ 数据
样本数 = 10000
批量大小 = 100
输入 = torch.randn(num_samples, 3, 256)
目标 = torch.randint(0, 10, (num_samples,))
数据集= TensorDataset（输入，目标）

# 加载模型
模型1 = 网络()
模型2 = 网络()
model2.load_state_dict(model1.state_dict())

# 第一次验证
损失1 = 验证（模型1，DataLoader（数据集，batch_size=batch_size））
loss2 = 验证(model2, DataLoader(数据集,batch_size=batch_size))

打印（f“{loss1=}”）
打印（f“{loss2=}”）

# 训练参数
优化器1 = optim.Adam(model1.parameters(), lr=1e-4)
优化器2 = optim.Adam(model1.parameters(), lr=1e-4)

loader1 = DataLoader(数据集,batch_size=batch_size,drop_last=True)
loader2 = DataLoader(数据集,batch_size=1)

print(&quot;训练模型1&quot;)
火车（模型1，优化器1，加载器1，时期= 1，积累_步骤= 1）

print(&quot;训练模型2&quot;)
训练（模型2，优化器2，加载器2，时期= 1，积累_步骤=批量大小）

# 训练后验证
损失1 = 验证（模型1，DataLoader（数据集，batch_size=batch_size））
loss2 = 验证(model2, DataLoader(数据集,batch_size=batch_size))

打印（f“{loss1=}”）
打印（f“{loss2=}”）

对于任意数量的epoch或不同的参数（例如学习率），第二个模型不会有与第一个模型相同的损失，并且大多数时候第二个模型的损失比第一个模型更大。]]></description>
      <guid>https://stackoverflow.com/questions/78418346/how-to-deal-with-batchnorm-when-accumulating-loss-gradients-in-pytorch</guid>
      <pubDate>Thu, 02 May 2024 10:00:57 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 神经网络</title>
      <link>https://stackoverflow.com/questions/78416266/machine-learning-neural-network</link>
      <description><![CDATA[我正在尝试创建一个具有一个隐藏层的神经网络，它将图像分为 12 个不同的类别。当我尝试使用梯度下降函数运行代码来开始训练模型时，代码根本不输出任何内容并移至下一个单元格。
definitialize_parameters(hidden_​​units):
    w1 = np.random.randn(hidden_​​units, 640 * 480 * 3) * 0.01
    b1 = np.zeros((hidden_​​units, 1))
    w2 = np.random.randn(12, 隐藏单元) * 0.01
    b2 = np.zeros((12, 1))
    返回 w1、b1、w2、b2

def ReLU(Z):
    返回 np.maximum(0, Z)

def softmax(Z):
    expZ = np.exp(Z)
    返回 expZ / np.sum(expZ, axis=0, keepdims=True)

defforward_propagation(w1, b1, w2, b2, X):
    z1 = np.dot(w1, X) + b1
    a1 = ReLU(z1)
    z2 = np.dot(w2, a1) + b2
    a2 = softmax(z2)
    返回 z1、a1、z2、a2

def onehotencoding(Y):
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    返回 one_hot_Y

def导数ReLU(Z)：
    返回Z&gt; 0

def back_propagation(w2, a1, z1, a2, X, Y):
    m = Y 尺寸
    one_hot_Y = onehotencoding(Y)
    dz2 = a2 - one_hot_Y
    dw2 = 1 / m * np.dot(dz2, a1.T)
    db2 = 1 / m * np.sum(dz2, axis=1, keepdims=True)
    dz1 = np.dot(w2.T, dz2) *导数ReLU(z1)
    dw1 = 1 / m * np.dot(dz1, X.T)
    db1 = 1 / m * np.sum(dz1, axis=1, keepdims=True)
    返回 dw1、db1、dw2、db2

def update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):
    w1 = w1 - 阿尔法 * dw1
    b1 = b1 - 阿尔法 * db1
    w2 = w2 - 阿尔法 * dw2
    b2 = b2 - 阿尔法 * db2
    返回 w1、b1、w2、b2

此代码单元格在此结束，后面是此代码块。
def get_predictions(a2):
    返回 np.argmax(a2, 轴=0)

def get_accuracy(预测, Y):
    返回 np.sum(预测 == Y) / Y.size

defgradient_descent(X,Y,hidden_​​units,迭代,alpha):
    w1、b1、w2、b2 = 初始化参数（隐藏单元）
    对于范围内的 i（迭代）：
        z1, a1, z2, a2 = 前向传播(w1, b1, w2, b2, X)
        dw1, db1, dw2, db2 = 反向传播(w2, a1, z1, a2, X, Y)
        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)
        如果我％10==0：
            预测 = get_predictions(a2)
            准确度 = get_accuracy(预测, Y)
            print(&quot;迭代：&quot;, i)
            print(&quot;准确率：&quot;, 准确率)
    返回 w1、b1、w2、b2


这是开始训练的梯度下降函数。
w1, b1, w2, b2 = 梯度下降(X_train, Y_train, 500, 迭代=1000, alpha=0.1)]]></description>
      <guid>https://stackoverflow.com/questions/78416266/machine-learning-neural-network</guid>
      <pubDate>Wed, 01 May 2024 22:43:52 GMT</pubDate>
    </item>
    <item>
      <title>我们如何以优雅的方式捕获使用optimizer.step()完成的更新？</title>
      <link>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</link>
      <description><![CDATA[我想实现一种方法，按照 Karpathy 视频中提到的想法，在使用 PyTorch 训练期间在 Tensorboard 中监控更新数据比率。我已经提出了一个解决方案，但我正在寻找一种更优雅且可配置的方法。
当前的实现直接修改训练循环如下：
对于步骤，在 data_loader 中进行批处理：
    x, y = 批次
    优化器.zero_grad()
    对于名称，model.named_pa​​rameters() 中的参数：
        if param.requires_grad 和“weight”名称：
            param.data_before_step = param.data.clone()
    输出=模型(x)
    损失 = loss_fn(输出, y)
    loss.backward()
    优化器.step()
    lr_scheduler.step()
    对于名称，model.named_pa​​rameters() 中的参数：
        if hasattr(param, “data_before_step”):
            更新 = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f“更新：数据比率 {name}”，update_to_data，epoch * len(data_loader) + 步骤)
            param.data_before_step = param.data.clone()

但是，这种方法直接在训练循环中添加代码，这可能会使代码变得混乱，如果我们想要使其可配置，则需要 if-else 语句，这会使代码更加混乱。
我还探索过使用 PyTorch hooks 来实现这一点。我已经成功实现了一个钩子来跟踪梯度：
类 GradToDataRatioHook：
    def __init__(自身、名称、参数、start_step、summary_writer):
        self.name = 姓名
        self.param = 参数
        self.summary_writer = 摘要_writer
        自我.毕业生 = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(自我，毕业生)：
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;梯度:数据比率{self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1

但是，实现类似的钩子来捕获更新似乎很棘手。据我了解， param.register_hook(...) 注册了钩子，该钩子在计算梯度时调用，即在 optimizer.step() 之前调用叫。虽然梯度和学习率为标准 SGD 提供了更新的直接值，但像 Adam 这样的现代优化器使更新过程变得更加复杂。我正在寻找一种以与优化器无关的方式捕获更新的解决方案，最好使用 PyTorch 挂钩。但是，任何建议或替代方法也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</guid>
      <pubDate>Fri, 26 Apr 2024 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>如何用 Python 制作人工智能自学习聊天机器人 [关闭]</title>
      <link>https://stackoverflow.com/questions/78370101/how-to-make-an-ai-self-learning-chatbot-in-python</link>
      <description><![CDATA[我一直在尝试用 Python 制作一个自学习聊天机器人，并尝试了不同的库，如 NLTK、TensorFlow、ChatBot 和 PyTorch，但所有这些库都在处理预定义的训练数据。我找不到任何选项来根据给定的输入自行训练模型并尝试不同类型的数据集。
Python 有什么方法可以实现这一点吗？我可以看到我们可以使用已经训练好的模型并对其进行微调，或者使用 DialogFlow 和 Rasa 来建立对话模型。然而，我正在寻找一种方法，我们可以用我们自己的数据来训练它，它可以从给定的数据中学习并对给定的提示产生自己的非预定答复。
也许您可以提供一个特定的代码片段，甚至只是代码或伪代码的概要供我使用？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78370101/how-to-make-an-ai-self-learning-chatbot-in-python</guid>
      <pubDate>Tue, 23 Apr 2024 05:44:52 GMT</pubDate>
    </item>
    </channel>
</rss>