<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 01 May 2024 01:04:33 GMT</lastBuildDate>
    <item>
      <title>寻找机器学习方面的导师[关闭]</title>
      <link>https://stackoverflow.com/questions/78411363/looking-for-mentorships-in-machine-learning</link>
      <description><![CDATA[我是机器学习的初学者，这个平台上是否有人愿意指导我完成机器学习专业版之旅？
我精通Python。寻找指导来帮助我进行机器学习]]></description>
      <guid>https://stackoverflow.com/questions/78411363/looking-for-mentorships-in-machine-learning</guid>
      <pubDate>Tue, 30 Apr 2024 22:58:58 GMT</pubDate>
    </item>
    <item>
      <title>学习曲线是否表明过度拟合或模型性能处于可接受水平？</title>
      <link>https://stackoverflow.com/questions/78411015/does-the-learning-curve-suggest-overfitting-or-an-acceptable-level-of-model-perf</link>
      <description><![CDATA[
学习曲线是否表明模型性能过度拟合或处于可接受水平？结果基于 xgboost。我需要重新调整超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78411015/does-the-learning-curve-suggest-overfitting-or-an-acceptable-level-of-model-perf</guid>
      <pubDate>Tue, 30 Apr 2024 21:03:33 GMT</pubDate>
    </item>
    <item>
      <title>用于 ML 、keras tensorflow 或 scikit learn 的更好的库应该是什么[关闭]</title>
      <link>https://stackoverflow.com/questions/78410129/what-should-be-the-better-library-to-use-for-ml-keras-tensorflow-or-scikit-lea</link>
      <description><![CDATA[只是困惑我应该使用哪个库进行机器学习，两者的优点和缺点是什么。
我尝试了这两个库，我想对它们进行比较，也想知道哪个更好。]]></description>
      <guid>https://stackoverflow.com/questions/78410129/what-should-be-the-better-library-to-use-for-ml-keras-tensorflow-or-scikit-lea</guid>
      <pubDate>Tue, 30 Apr 2024 17:13:17 GMT</pubDate>
    </item>
    <item>
      <title>这是一个 CNN 1d 模型，通过观察电流信号找出电压</title>
      <link>https://stackoverflow.com/questions/78409956/this-is-a-cnn-1d-model-to-find-out-the-voltages-by-observing-the-current-signal</link>
      <description><![CDATA[它说：
&lt;块引用&gt;
由于 conv1d_4 中的下采样，输出中的一个维度 &lt;= 0。考虑增加输入大小。接收到的输入形状 [None, 9600, 1, 1]，它将产生维度为零或负值的输出形状。

这是我到目前为止所做的：
dataset_url=“https://github.com/Kaustav-coder/cnn/blob/main/cnn.csv”

从 keras.models 导入顺序
从 keras.layers 导入密集、扁平化
从 keras.layers 导入 Conv1D、MaxPooling1D、Dropout
从 keras.layers 导入嵌入
来自 keras.preprocessing 导入序列
从 sklearn.model_selection 导入 train_test_split
将 pandas 导入为 pd
将 numpy 导入为 np
从sklearn导入预处理
从sklearn.metrics导入accuracy_score，confusion_matrix，classification_report

数据=pd.read_csv(&#39;cnn.csv&#39;)
y=数据[&#39;电压&#39;]
x=data.drop([&#39;电压&#39;],轴=1)

label_encoder = 预处理.LabelEncoder()
y_enc = label_encoder.fit_transform(y)
x_reshape = x.values.reshape(x.shape[0], x.shape[1],1)
x_train,x_test,y_train,y_test= train_test_split(x_reshape,y_enc,test_size=0.2,random_state=42)

模型=顺序（）
model.add(Conv1D(filters=64, kernel_size=10,activation=&#39;relu&#39;, input_shape=(9600,1,1)))
model.add(MaxPooling1D(pool_size=1))
model.add(Conv1D(filters=64, kernel_size=10,activation=&#39;relu&#39;))
model.add(MaxPooling1D(pool_size=1))
模型.add(压平())
model.add（密集（128，激活=&#39;relu&#39;））
模型.add(Dropout(0.25))
model.add（密集（128，激活=&#39;softmax&#39;））
]]></description>
      <guid>https://stackoverflow.com/questions/78409956/this-is-a-cnn-1d-model-to-find-out-the-voltages-by-observing-the-current-signal</guid>
      <pubDate>Tue, 30 Apr 2024 16:42:09 GMT</pubDate>
    </item>
    <item>
      <title>Google 广告数据的数据科学/机器学习分析 [关闭]</title>
      <link>https://stackoverflow.com/questions/78409943/data-science-ml-analysis-of-google-ads-data</link>
      <description><![CDATA[我们希望对 Google Ads 数据进行一些机器学习分析，以微调我们的营销效果。有人对方法有什么建议吗？
是否有严肃的 DS/ML 从业者的现有资源正在使用来自 Google Ads 的数据，或许还可以通过其他来源（YouTube 等）进行增强，以生成预测分析以最大化转化价值。
我们正在考虑：
识别并调整不同的关键字值
识别每个营销活动或广告中可能解释效果的不同特征
通过识别“趋势”主题、主题标签等而不是被动方法来探索“套利”关键字值的可能性
生成因果发现/因果推理分析来解释事件之间关系的强度
我们理想地寻找可以向我们展示该领域现有思维的最佳实践/权威分析师/资源。任何提供分析方法的 GitHub 存储库的建议将不胜感激。
我们从“Google Ads”的角度在 google 上搜索了这个问题，并查看了 Reddit，但存在很多噪音和“黑匣子”解决方案，因此我们决定从认真的 DS 从业者的角度来解决这个问题...希望您能帮助。
我们已经通过 PyCaret ML 回归分析对 Google Ads 广告系列、广告和关键字数据进行了分析，并获得了一些基础结果，但我们现在正在寻求有关如何推进这一工作的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78409943/data-science-ml-analysis-of-google-ads-data</guid>
      <pubDate>Tue, 30 Apr 2024 16:38:08 GMT</pubDate>
    </item>
    <item>
      <title>伙计们，我需要帮助。这里有什么错误？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78409929/guys-please-i-need-help-what-is-the-error-here</link>
      <description><![CDATA[在此处输入图片描述
我从其他地方复制了这段代码...我遇到了很多错误并且能够纠正这些错误。这是最后一步，我一直坚持下去。]]></description>
      <guid>https://stackoverflow.com/questions/78409929/guys-please-i-need-help-what-is-the-error-here</guid>
      <pubDate>Tue, 30 Apr 2024 16:35:28 GMT</pubDate>
    </item>
    <item>
      <title>使用Python深度学习进行时间序列分析</title>
      <link>https://stackoverflow.com/questions/78409576/time-series-analysis-with-python-deep-learning</link>
      <description><![CDATA[我正在用python进行时间序列数据分析。但是有一个问题。
import os
from omegaconf import OmegaConf

import pandas as pd
import torch
import dataset as module_data
import model as module_arch
import metric as module_metric
from utils import MetricTracker, get_data_path

END_DATE = &#39;2024-03-22&#39;

def get_test_dollar_price(start_date, end_date):
&quot;&quot;&quot;
不要修复此功能
&quot;&quot;&quot;
df = pd.read_csv(get_data_path(&#39;Bitcoin&#39;), index_col=&quot;Date&quot;, parse_dates=True, na_values=[&#39;nan&#39;])
df.sort_index(inplace=True)
price = df.loc[start_date:end_date, [&#39;Price&#39;]][-10:].values
return price

def main(config):

test_dataset = getattr(module_data, config.dataset.type)(end_date=END_DATE, 
is_training=False, 
**config.dataset.args)
test_dataloader = getattr(torch.utils.data, config.dataloader.type)(test_dataset, 
batch_size=1,
shuffle=False,
num_workers=0,)

###################################################################################################################
# 检查测试数据
if abs(test_dataset.y.numpy() - get_test_dollar_price(&#39;2024-03-12&#39;, END_DATE)).sum() &gt; 1e-3:
引发 ValueError(&#39;您的测试数据有误！&#39;)
#########################################################################################################################

model = getattr(module_arch, config.model.type)(input_size=test_dataset.__getitem__(0)[0].size(0), **config.model.args)
model.load_state_dict(torch.load(config.test.load_path))
model.eval()

# GPU
device = torch.device(&#39;mps&#39;)

model = model.to(device)

metrics = [getattr(module_metric, met) for met in config.test.metrics]
metric_tracker_test = MetricTracker(*config.test.metrics)

for i, (x, y) in enumerate(test_dataloader):
x, y = x.to(device), y.to(device)
pred_y = model(x)
print(f&#39;[DAY {i+1:02d}] predict : {pred_y[0].item():.1f} | target : {y[0].item():.1f}&#39;)
for met in metrics:
metric_tracker_test.update(met.__name__, met(pred_y, y))

print(&#39;\nTEST &#39; + &#39;, &#39;.join([f&#39;{k.upper()}: {v:.2f}&#39; for k, v in metric_tracker_test.result().items()]))


这是我的 test.py 代码，如果我运行此代码，则会引发值错误。在 #inspect 测试数据部分中，start_data 是 2022-01-25。我想知道为什么会出现值错误（您的测试数据是错误的）。帮帮我吧
我多次更改日期。但结果是一样的]]></description>
      <guid>https://stackoverflow.com/questions/78409576/time-series-analysis-with-python-deep-learning</guid>
      <pubDate>Tue, 30 Apr 2024 15:26:09 GMT</pubDate>
    </item>
    <item>
      <title>大尺寸的 One Hot 编码</title>
      <link>https://stackoverflow.com/questions/78409561/one-hot-encoding-with-large-dimensions</link>
      <description><![CDATA[我正在构建一个销售预测模型，其中包含“年”、“月”、“经济指标”、“Customer_Id”、“Product_Id”、“Quantity”、“Sales”、“ “保证金”。
清理后的数据集包含约 150 万行和上述 8 列，这是过去 6 年每个客户每个产品的每月销售额。我的最终目标是能够预测整个来年未来几个月的销售额，但更准确地说，预测将针对每个客户级别的产品，这是一个非常详细的级别。
但是，由于我的Customer_Id和Product_Id是TEXT，例如“A77BC”，并且有超过100000个唯一的product_id和6000个唯一的customer_id，如果我使用一种热编码来标记它们，那么维度对于我来说太高了设备来处理（例如，我的笔记本电脑有 16G 内存，但标签 customer_id 已经需要 24G 内存）并且我相信一定有更好的方法来处理这种情况，但我对机器学习非常陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78409561/one-hot-encoding-with-large-dimensions</guid>
      <pubDate>Tue, 30 Apr 2024 15:23:06 GMT</pubDate>
    </item>
    <item>
      <title>model.fit 不起作用“运行时错误：‘tf.data.Dataset’仅支持急切模式或 tf.function 内的 Python 样式迭代。”</title>
      <link>https://stackoverflow.com/questions/78409501/model-fit-is-not-working-runtimeerror-tf-data-dataset-only-supports-python-s</link>
      <description><![CDATA[我想在 JupyterLab 上编写一个简单的机器学习代码，但出现标题错误
我已经收集了数据，因为x是lamda，eps，c（数据的形状是102010行×3列），y是sen（数据的形状是102010行×1列），我想要我的机器学习模型来预测。这是我的简单模型代码。
导入tensorflow为tf
从 sklearn.model_selection 导入 train_test_split
将 pandas 导入为 pd

# 将数据拆分为特征 (X) 和目标 (y)
X_data = df_total_x_data.values.reshape(-1, 1) # 如果需要则重塑
y_data = df_sen_final.values

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
模型 = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.fit(x_train, y_train, epochs=10,validation_data=(x_test, y_test))

如何修复此错误？
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
单元格 In[60]，第 17 行
     15 模型 = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
     16 model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
---&gt; 17 model.fit(x_train, y_train, epochs=10,validation_data=(x_test, y_test))

文件/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122，在filter_traceback..error_handler(*args, **kwargs)
    第119章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件 /usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py:503，在 DatasetV2.__iter__(self) 中
    第501章
    第502章：
--&gt; 503 raise RuntimeError(“tf.data.Dataset`仅支持Python风格”
    [第 504 章]

RuntimeError: `tf.data.Dataset` 仅支持急切模式下或 tf.function 内的 Python 风格迭代。
]]></description>
      <guid>https://stackoverflow.com/questions/78409501/model-fit-is-not-working-runtimeerror-tf-data-dataset-only-supports-python-s</guid>
      <pubDate>Tue, 30 Apr 2024 15:12:43 GMT</pubDate>
    </item>
    <item>
      <title>RNN实现方程问题的简单BPTT</title>
      <link>https://stackoverflow.com/questions/78408682/simple-bptt-for-rnn-implementation-equation-question</link>
      <description><![CDATA[我正在尝试从头开始实现简单的 RNN，以了解每个实现背后的计算步骤。前向传播看起来很简单，但后向传播似乎很困难。尤其是尺寸不符……
def rnn_forward(self, e) ：
        对于范围内的 t(self.T_x - 1) ：
            # single (n_x, n_m) m 是时间上的训练集大小
            xt = self.cache[&#39;X&#39;][:, :, t]
            # 上一步的隐藏状态
            a_prev = self.cache[&#39;A&#39;][:, :, t]
            # 下一步的隐藏状态是通过以下权重和偏差计算的。
            a_next = tanh(np.dot(self.parameters[&#39;W_aa&#39;], a_prev) + np.dot(self.parameters[&#39;W_ax&#39;], xt) + self.parameters[&#39;b_a&#39;])
            # 使用softmax作为最终激活
            yt_pred = softmax(np.dot(self.parameters[&#39;W_ya&#39;], a_next) + self.parameters[&#39;b_y&#39;])
            self.cache[&#39;A&#39;][:, :, (t + 1)] = a_next
            self.cache[&#39;Y_pred&#39;][:, :, t] = yt_pred

def rnn_backward(self, e):
        # 成本函数导数
        self.cache[&#39;dY_pred&#39;] = - (self.cache[&#39;Y&#39;] / self.cache[&#39;Y_pred&#39;])
        # 初始化da
        da_next = np.zeros((self.n_a, self.m))
        对于反转中的 t(范围(self.T_x - 1)) ：
            # 维度似乎是 (n_y, n_m)
            print(self.cache[&#39;dY_pred&#39;][:, :, t].shape)
            # 维度似乎也是 (n_y, n_m)
            print(softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]).shape)
            # 下面的计算会有问题吗？ da_prev 预计有暗淡 (n_a, n_m)，但 (n_y, n_m) 和 (n_y, n_m) 的点不给我 (n_a, n_m)
            da_prev = np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], np.dot(self.parameters[&#39;W_ya&#39;].T, softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;) ], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]))) + da_next
            self.cache[&#39;dW_ya&#39;] += np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], np.dot(softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;], self) .cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]), self.cache[&#39;A&#39;][:, :, t].T))
            self.cache[&#39;db_y&#39;] += np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], softmax_backward(np.dot(self.parameters[&#39;Wya&#39;], self.cache[&#39; A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]))
            self.cache[&#39;dW_aa&#39;] += np.dot(da_prev, np.dot(tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]), self.cache[ &#39;A&#39;][:, :, (t - 1)].T))
            self.cache[&#39;dW_ax&#39;] += np.dot(da_prev, np.dot(tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]), self.cache[ &#39;X&#39;][:, :, t].T))
            self.cache[&#39;db_a&#39;] += np.dot(da_prev, tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)] ) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]))
            da_next = np.dot(da_prev, np.dot(self.parameters[&#39;W_aa&#39;].T, tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, : , (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;])))

正如代码中的注释，我似乎无法正确实现反向传播。 da_prev 的维数不正确，无法正确生成 (n_a, n_m)…我错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78408682/simple-bptt-for-rnn-implementation-equation-question</guid>
      <pubDate>Tue, 30 Apr 2024 12:58:33 GMT</pubDate>
    </item>
    <item>
      <title>转换深度学习 esrgan 模型时输入张量形状不匹配</title>
      <link>https://stackoverflow.com/questions/78407762/input-tensor-shape-mismatch-when-converting-deep-learning-esrgan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78407762/input-tensor-shape-mismatch-when-converting-deep-learning-esrgan-model</guid>
      <pubDate>Tue, 30 Apr 2024 10:05:56 GMT</pubDate>
    </item>
    <item>
      <title>尝试对简短的调查答案进行聚类（1 到 10 个字）。我的方法正确吗？</title>
      <link>https://stackoverflow.com/questions/78407025/trying-to-cluster-short-survey-answers-1-to-10-words-am-i-on-the-right-track</link>
      <description><![CDATA[这是我想要完全制作的内容的解释（这是学校的一个项目）。

用户只需将调查中提出的任何问题的答案放入一个文件即可。

2.机器找到相似的答案，并将它们分组到一个未命名的标签或簇下（考虑使用 MeanShift、GMM、KMeans）

如果可能的话，我还希望它为集群生成标签。

4.将聚类和标记的答案写回到文件中以供检查并用于任何目的。
关于数据的一些上下文：很多简短的答案（有一些长的，超过 10 个单词）答案，例如“我不知道”、“??”、“有帮助”、“红色”等，以及每个都有 200 到 2000 个答案。答案是荷兰语或法语，是否建议我将它们翻译成英语以获得更好的性能？通常有大约 7 到 20 个（数量较多的情况很少见）簇。我还有正确的答案标签，这样我就可以检查算法是否正确聚类。
我尝试过研究它，我需要首先对我的文本进行矢量化，为此我尝试了 scikit 中的 TF-IDF 和 Count 矢量化器。我还找到了他们的备忘单，它建议我使用 MeanShift。
我还没有尝试寻找最佳参数，但性能似乎很差（接近随机）。我使用调整兰德指数、归一化互信息和轮廓分数来评估。
我走在正确的道路上还是有更好的东西？矢量化方法、嵌入、聚类算法？
编辑：我刚刚意识到一些可能会推翻算法的事情。在每项调查中都有一个名为“其他”的类别。这是一个与实际答案无关的类别。
我认为可能的解决方法：该类别“小”，大约为 8%。我的聚类算法总是产生太多的类，但我可以尝试在将 1 或 2 个答案的类组合到“其他”中之间找到平衡。]]></description>
      <guid>https://stackoverflow.com/questions/78407025/trying-to-cluster-short-survey-answers-1-to-10-words-am-i-on-the-right-track</guid>
      <pubDate>Tue, 30 Apr 2024 07:50:57 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[我有以下输入矩阵
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
        [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])

和零元素的索引
mask_indices = torch.tensor(
[[7, 2],
[2, 6]])

如何从与以下矩阵的乘法中排除非零元素：
my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317]])

也就是说，不要将其相乘（包括零）：
a = torch.mm(inp_tensor, my_tensor)
打印（一）
张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我想排除零个元素（以及 my_tensor 的相应行）：
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.6524, 0.6057, 0.3725, 0.7980]]) # 删除零个元素

my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696]]) # 删除对应的零元素行

b = torch.mm(inp_tensor, my_tensor)
打印(b)
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330]])

inp_tensor = torch.tensor([[1.0000, 0.1115, 0.6524, 0.6057, 0.3725, 1.0000]]) # 删除零个元素

my_tensor = torch.tensor(
        [
        [0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.4414, 0.2969, 0.8317]]) # 删除对应的零元素行

c = torch.mm(inp_tensor, my_tensor)
打印（三）
&gt;&gt;&gt;&gt;&gt;张量([[2.2041, 2.5388, 2.3315]])
打印（火炬.cat（[b，c]））
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我需要它是高效的（即，没有for循环），因为我的张量非常大，并且还需要保持梯度（即，如果我调用optimizer.backward( ）更新计算图中的相关参数）
更新
我想到的一种方法如下：
导入火炬

# 给定数据
mask_indices = torch.tensor([[7, 2], [2, 6]])
inp_tensor = torch.tensor([[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
                           [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])
                           
my_tensor = torch.tensor([[0.8823, 0.9150, 0.3829],
                          [0.9593, 0.3904, 0.6009],
                          [0.2566, 0.7936, 0.9408],
                          [0.1332, 0.9346, 0.5936],
                          [0.8694, 0.5677, 0.7411],
                          [0.4294, 0.8854, 0.5739],
                          [0.2666, 0.6274, 0.2696],
                          [0.4414, 0.2969, 0.8317]])

# 复制 my_tensor
my_tensors = my_tensor[无].repeat(inp_tensor.size(0), 1, 1)

# 与0进行逐元素比较
删除掩码 = inp_张量 == 0

# 应用掩码来删除行
Filtered_tensors = my_tensors[~remove_mask]

# 重塑为原始形状
Filtered_tensors = Filtered_tensors.view(inp_tensor.size(0), -1, my_tensor.size(-1))
打印（过滤张量）

结果 = (inp_tensor.unsqueeze(-1) * my_tensors).sum(dim=1)
打印（结果）

&gt;&gt;&gt;&gt;&gt;
张量([[[0.8823, 0.9150, 0.3829],
         [0.9593, 0.3904, 0.6009],
         [0.1332, 0.9346, 0.5936],
         [0.8694, 0.5677, 0.7411],
         [0.4294, 0.8854, 0.5739],
         [0.2666, 0.6274, 0.2696]],

        [[0.8823, 0.9150, 0.3829],
         [0.9593, 0.3904, 0.6009],
         [0.1332, 0.9346, 0.5936],
         [0.8694, 0.5677, 0.7411],
         [0.4294, 0.8854, 0.5739],
         [0.4414, 0.2969, 0.8317]]])
张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

但是

我不确定这是否非常高效/可扩展，因为我多次复制权重张量（非常大）。
我不确定这种重复和删除行是否会以有问题的方式影响梯度计算。
这仍然不仅会乘以 inp_tensor 的非零元素，这是可以删除的（例如，使用 nonzero_values = inp_tensor[inp_tensor != 0].reshape(inp_tensor .shape[0],-1))，但我不确定它是否会以有问题的方式影响梯度计算。
]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>SVM 训练时间过长</title>
      <link>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</link>
      <description><![CDATA[我有一个包含 41 个特征的数据集，其中 4 个是文本特征。我得到了“词袋”这四个特征的 numpy 数组 (npz)，我将其与其他数值特征结合起来训练 SVM 模型。总共有 100000 条记录和 41 个特征，其中 4 个特征如前所述进行了矢量化。
该模型现已训练 45 分钟:)。有没有办法减少训练时间？我预处理数据集的方式（特别是结合 npz 和现有的数值特征）有什么问题吗？我还可以探索其他选择吗？
title_feature = load_npz(&#39;train_title_bow.npz&#39;)
Overview_feature = load_npz(&#39;train_overview_bow.npz&#39;)
tagline_feature = load_npz(&#39;train_tagline_bow.npz&#39;)
Production_companies_feature = load_npz(&#39;train_product_companies_bow.npz&#39;)

numeric_features = df_train[df_train.columns.difference([&#39;标题&#39;, &#39;概述&#39;, &#39;标语&#39;, &#39;生产公司&#39;, &#39;rate_category&#39;, &#39;average_rate&#39;, &#39;original_language&#39;])]
text_features = np.hstack([title_feature.toarray()、overview_feature.toarray()、tagline_feature.toarray()、生产_companies_feature.toarray()])
svm_X_train = np.hstack([数字特征, 文本特征])
svm_y_train = df_train[&#39;rate_category&#39;]

svm_classifier = SVC(kernel=&#39;linear&#39;) # 使用线性核，也可以选择其他核
svm_classifier.fit(svm_X_train, svm_y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</guid>
      <pubDate>Mon, 29 Apr 2024 02:39:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地运行主宰模型</title>
      <link>https://stackoverflow.com/questions/78143186/how-to-run-juggernaut-model-in-local</link>
      <description><![CDATA[我想使用 python 在本地电脑上运行微调的稳定扩散模型。例如剑圣：https://huggingface.co/RunDiffusion/Juggernaut-XL-v9
这是我的代码（它适用于 stable-diffusion-xl-base-1.0）：
随机导入
从扩散器导入 DiffusionPipeline、StableDiffusionXLImg2ImgPipeline
进口火炬
导入GC
导入时间

# 用于清理内存
GC.collect()
torch.cuda.empty_cache()

开始时间 = 时间.time()

型号 =“RunDiffusion/Juggernaut-XL-v9”
管道 = DiffusionPipeline.from_pretrained(
    模型，
    torch_dtype=torch.float16,
）

管道.to(“cuda”)

提示=（“中世纪男性骑士肖像，阳刚的外观，背景中的战斗，清晰的焦点，高度详细，电影风格的灯光，阴影”）
种子 = random.randint(0, 2**32 - 1)

生成器 = torch.Generator(“cuda”).manual_seed(seed)
图像=管道（提示=提示，生成器=生成器，num_inference_steps=1）
图像=图像.图像[0]
image.save(f“output_images/{seed}.png”)

结束时间 = time.time()

总时间 = 结束时间 - 开始时间
分钟 = int(total_time // 60)
秒 = int(总时间 % 60)

print(f&quot;花费: {分钟} 分 {秒} 秒&quot;)
print(f&quot;保存到output_images/{seed}.png&quot;)


但我得到：
&lt;块引用&gt;
OSError：在目录中找不到名为 pytorch_model.bin、tf_model.h5、model.ckpt.index 或 flax_model.msgpack 的文件时出错

可能是因为python、cuda版本的原因。我正在删除我的库版本：
Python 3.9.0
PyTorch：2.2.0+cu118
CUDA：11.8
扩散器：0.26.3
变形金刚：4.38.1]]></description>
      <guid>https://stackoverflow.com/questions/78143186/how-to-run-juggernaut-model-in-local</guid>
      <pubDate>Mon, 11 Mar 2024 20:12:01 GMT</pubDate>
    </item>
    </channel>
</rss>