<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 26 Feb 2025 18:25:51 GMT</lastBuildDate>
    <item>
      <title>如何为AI模型培训构建服务器？ （GPU，硬件和远程访问）</title>
      <link>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</link>
      <description><![CDATA[我想构建用于培训AI模型的计算机（服务器），包括：

微调聊天机器人LLMS 
机器学习和深度学习模型

我有几个问题：

 我应该购买哪个GPU？我知道GPU对于AI培训很重要，但我不确定哪一个是满足我需求的最佳选择。

 我还需要什么其他硬件？除GPU外，推荐的CPU，RAM，存储和其他组件是什么？

 如何远程控制此服务器？我希望能够从另一个位置访问和管理此服务器。我应该使用什么工具或方法？

 多人可以同时使用该服务器吗？如果我想与朋友或队友共享此服务器，我们如何一起训练模型？


我感谢任何建议或建议。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</guid>
      <pubDate>Wed, 26 Feb 2025 18:16:44 GMT</pubDate>
    </item>
    <item>
      <title>Yolo是运气的吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79470313/was-yolo-a-matter-of-luck</link>
      <description><![CDATA[已经调查并研究了从传统的对象检测模型，从传统的对象检测模型，从基于深度学习的模型开始，并了解思想如何发展，阅读Yolo的细节似乎有些不知所措。人们如何看待所有这些思想和层次（地区建议，改善地区建议，SPP，FPN等），但没有想到Yolo基于框架对象检测是回归问题的简单想法？ Yolo碰巧的表现比某些现有型号更好吗？是运气问题（或Fluke）吗？
或者没有人想象将边界框和分类作为回归问题的框架会产生任何结果（将其视为太简单）？]]></description>
      <guid>https://stackoverflow.com/questions/79470313/was-yolo-a-matter-of-luck</guid>
      <pubDate>Wed, 26 Feb 2025 15:50:47 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络层中使用二进制（{0,1}）权重</title>
      <link>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</guid>
      <pubDate>Wed, 26 Feb 2025 13:02:20 GMT</pubDate>
    </item>
    <item>
      <title>如何准备使用LSTM进行分类的不规则间隔时间序列数据？</title>
      <link>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</link>
      <description><![CDATA[ i具有这样的可变的可变价值的数据，如下所示： processed_data 是一个大小为215×1的单元格数组，保存单元格，每个单元都包含给定一天的数据。每个单元（天）的观测值数量不同（平均约为12,000行）。每行代表一个观察值，其中：第一列包含自上一行以来经过的秒数（未归一化），第二列包含指定安全性的价格（使用z得分进行了归一化），第三列是目标变量，发出信号，当时的价格是否会高0.01％（表示为1）60秒后（表示为0）。我将前两列用作预测指标。我将日子分开，因为小时在 processed_data {i，1} 的最后一行之间以及day processed_data {i+1，1} 的第一行。以下是任意日的数据示例：
  2.57500000000437 0.502515050312692 0
1.036000000006 0.469361050915526 1
1.05899999999383 0.386501335237771 1
0.838000000003376 0.436219680495852 0
1.1299999999738 0.469361050915526 0
0.824000000000524 0.369924327252462 1
 
我只是ML的初学者，而且我很难想象应该如何格式化LSTM层的数据。如果我正确，它需要3维数据，其中一个维度代表 channel ，另一个维度时间步长，而另一个 batch  。我现在确定我已经完全误解了这些概念，并写了以下代码：
  %%分区数据。
train_data_length = round（长度（processed_data） * 0.9）;
train_data = processed_data（1：train_data_length）;
test_data = processed_data（train_data_length+1：end）;

%%培训设置
％将数据转换为dlarray的单元格数组。
train_x =单元格（size（train_data））;
train_y =单元格（size（train_data））;

对于一天= 1：长度（train_data）
    ％添加批处理尺寸（C×B×T，其中B = 1）。
    data = permute（train_data {day}（：，1：2）&#39;，[1 3 2]）; ％[2×1×T]
    train_x {day} = dlarray（data，＆quot; cbt; quot;）;
    
    ％将标签转换为单速编码的CBT格式[2×1×T]。
    labels = train_data {day}（：，3）&#39;; ％[1×T]
    One_hot_labels = OneHotEncode（标签，1，&#39;classNames&#39;，[0 1]）; ％[2×t]
    One_hot_labels = reshape（One_hot_labels，2，1，[]）; ％[2×1×T]
    train_y {day} = dlarray（single（One_hot_labels），“ CBT”）;
结尾

ds = combine（...
    arraydatastore（train_x，&#39;outputType&#39;，&#39;same&#39;），...
    arraydatastore（train_y，&#39;outputType&#39;，&#39;same&#39;）...
）；

％clearvars -Efcect ds test_data ml_method

num_features = 2;
num_hidden_​​units = 128;
num_classes = 2;
mini_batch_size = 32;

层= [
    sequenceInputlayer（num_features，“名称”，“输入”）
    lstmlayer（num_hidden_​​units，&#39;outputmode&#39;，&#39;sequence&#39;）
    完整连接的layerer（num_classes）
    SoftMaxlayer
];

net = dlnetwork（层）;

选项=训练（&#39;Adam&#39;，...
    “ Maxepochs”，30，...
    &#39;minibatchsize&#39;，mini_batch_size，...
    “序列长度”，“最长”，...
    “洗牌”，“每个段”，...
    “情节”，“训练过程”，...
    “ inputdataformats&#39;，&#39;cbt&#39;，...
    “冗长”，错误，...
    “执行环境”，“ gpu&#39;）;

net = trainnet（ds，net，&#39;crossentropy&#39;，选项）;
 
在上面的代码中，我试图将通道定义为预测变量的数量（在我的情况下为2，可能是我正确定义的唯一维度）。我将批次设置为1，因为我认为这意味着网络将使用一个观察结果来做出预测。我将时间步骤设置为一天的数据价值的第一列（自上次观察以来的秒数），因为我认为这实际上意味着及时的步骤。现在我知道我完全错了。我还必须将Mini_batch_size从128中将其更改为32，但我发现这太低了，但是否则，我会用尽内存。我想这是因为我的格式格式不正确（我不确定这是否是一个重要的细节，但是我将包括我的GPU，它是带有8GB内存的RTX2070 SUPER）。我的问题是：我应该如何根据目标格式化LSTM层的数据？否则我的目标是不现实的，我正在使用错误？
我想象这个网络能够对数据中的每个观察进行预测。]]></description>
      <guid>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</guid>
      <pubDate>Wed, 26 Feb 2025 12:49:01 GMT</pubDate>
    </item>
    <item>
      <title>无法优化我的CNN模型来预测Watch Brands [关闭]</title>
      <link>https://stackoverflow.com/questions/79469709/cant-optimize-my-cnn-model-to-predict-watch-brands</link>
      <description><![CDATA[我想创建一个手表品牌标识符，该标识符获得图像并在其4个品牌之一或其他品牌中返回。
 rn我拥有的是4个品牌中每一个的3126张图像
和8000张来自其他品牌的手表图像（Chrono中每个品牌的均匀图像）。
 im使用VGG19作为基本模型，并在其中添加了一些图层。
问题在于，我训练的模型具有78％的准确性，更重要的是，从其他品牌中的一个品牌之一的手表中，很多时候都可以预测。。
我真正关心的是手表是否来自四个品牌，而不是哪个品牌，我该怎么做才能改善？
这是代码 link   
我认为可能仅仅是仅仅是4个中的二进制文件，但从最初的培训中，我看到了更糟糕的结果，也许我没有正确地做到这一点...
另外，我将优化器更改为ADAMW并增加了班级权重，我将重新训练该模型以查看是否有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79469709/cant-optimize-my-cnn-model-to-predict-watch-brands</guid>
      <pubDate>Wed, 26 Feb 2025 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>可以使用Python比较多个ROC曲线与Delong的测试进行比较吗？</title>
      <link>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</link>
      <description><![CDATA[我有3条ROC曲线，该曲线使用了3种不同测试的数据计算出来，该测试旨在将患者归类为患病或健康。我已经计算了所有3个测试的AUC，我想比较此曲线，以查看使用DeLong测试之间是否存在统计差异。我发现了非常简单的实现，例如：
 来自mlstatkit.stats导入delong_test

z_score，p_value = delong_test（labels，scores_model1，scores_model2）

打印（f＆quot; Model 1 AUC：{roc_auc_score（标签，scores_model1）：。4f}＆quort;）
打印（F＆quot;模型2 AUC：{roc_auc_score（标签，scores_model2）：。4f}＆quort;）
print（f＆quot; z得分：{z_score：.4f}，p-value：{p_value：.4f}＆quot;）
 
但是，我发现的所有实现都是用于比较两条ROC曲线。有人知道该测试是否可以进行两条以上的曲线？我的想法正在进行3个不同的配对测试并比较P值，但我不知道。
有什么方法可以用3个ROC曲线执行Delong的测试，有人可以帮助我进行编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</guid>
      <pubDate>Wed, 26 Feb 2025 12:07:00 GMT</pubDate>
    </item>
    <item>
      <title>改进职位描述，使用AI [封闭]</title>
      <link>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</link>
      <description><![CDATA[我正在研究AI驱动的人才获取解决方案，在该解决方案中，我们将恢复并将候选成就和职责转换为结构化属性。将主数据管理（MDM）用于作业角色和类别，我们生成这些属性。目标是将职位描述（JD）属性与候选属性匹配，以过滤给定作业的最相关的候选人。
这是我到目前为止尝试的：

  语义匹配：我使用语义相似性技术将职位描述属性与候选属性进行比较。

  向量嵌入：我尝试了嵌入技术（例如Word2Vec，句子变形金刚）以捕获属性的上下文含义。


但是，结果并不令人满意。传统方法（例如确切的关键字匹配或基于规则的方法）有局限性：

 他们无法捕获属性之间的语义相似性（例如Google AdWords和绩效营销）。

 他们不考虑同义词或相关技术（例如AWS和Amazon Web服务，或CI/CD Pipelines以及连续集成＆amp; exployment）。

 他们缺乏上下文感知的匹配，其中某些属性与特定的工作角色更相关。

 他们很难确定相关概念之间的语义相似性（例如，积极支持和客户健康监测）。

 他们没有有效地绘制同义词或相关术语（例如，预防和保留策略，或客户提高和扩张收入）。


我正在寻找改善匹配过程的建议或替代方法。具体：

 是否有更好的技术或模型来捕获语义相似性和上下文感知匹配？

 我如何有效地处理该域中的同义词和相关概念？

 是否有任何可以帮助解决此问题的预培训模型或数据集？

]]></description>
      <guid>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</guid>
      <pubDate>Wed, 26 Feb 2025 07:43:45 GMT</pubDate>
    </item>
    <item>
      <title>yolov9e-seg无法为整个图像进行细分</title>
      <link>https://stackoverflow.com/questions/79468647/yolov9e-seg-not-able-to-do-segmentation-for-the-entire-image</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79468647/yolov9e-seg-not-able-to-do-segmentation-for-the-entire-image</guid>
      <pubDate>Wed, 26 Feb 2025 05:46:59 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地运行DeepSeek模型</title>
      <link>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</link>
      <description><![CDATA[我试图根据他们的说明在本地运行DeepSeek，但它不能带来一些愚蠢的错误（我将稍后显示）。
这就是我正在做的。

从此处下载最小型号（3.5GB） noreferrer“&gt; https://huggingface.co/deepseek-ai/deepseek-r1-distill-qwen-1.5b  
按照此处的步骤操作： https://github.com/deepseek-ai/deepseek-v3?tab=readMe-Readme-ov-file#6-how-to-to-to-run-locally  

 2.1获取这个项目
 https://github.com/deepseek-ai/deepseek-ai/deepseek-aiek-v3.git  
 2.2运行码头容器类似于预先创建的卷以放置模型
  docker run  -  gpus all -it -it -name deepSeek01 -rm -mount source = deepSeekv3，target =/root/deepSeekv3 python：3.10 -Slim bash
 
我正在使用python：3.10-slim，因为这里（ https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-how-to-run-locally ）
＆quot&#39; linux只有python 3.10。 Mac和Windows不支持。
 2.3安装最新更新
apt-get Update 
 2.4获取此文件 https://github.com/deepseek-ai/deepseek-v3/blob/main/main/inference/requirements.txt 并安装要求
  pip install -r sumpliont.txt
 
 2.5将模型复制到安装在Docker容器上的音量。这5个文件来自此处 https：// https：//hugging.co/deepseek.co/deepseek-co/deepseek-co/deepseek-co/ AI/DeepSeek-R1-Distill-Qwen-1.5b  
  config.json
generation_config.json
模型。系统
tokenizer.json
tokenizer_config.json
 
 2.6在此处编写的模型 https://github.com/deepseek-ai/deepseek-v3?tab=readme-readme-ov-file#model-weights-conversion 通过此命令
  python convert.py-hf-ckpt-path/root/deepSeekv3/source_model -save-path/root/deepSeekv3/converted_model -n-experts 256-model-parelally 16
 
在此步骤中（转换模型）我得到了此错误
  trackback（最近的最新通话）：
  file＆quort＆quort＆quot deepseekv3/inference/convert.py&quot;，第96行，in＆lt; module＆gt;
    main（args.hf_ckpt_path，args.save_path，args.n_experts，args.model_parallel）
  file＆quot＆quot&#39;deepseekv3/inference/convert.py&quot;，第63行，在main中
    主张映射中的密钥
断言
 
因此，基本上，下一步没有意义，因为这是必不可少的步骤。
我的问题：

我做错了什么？
 YouTube上有一些视频，其中DeepSeek与Ollama一起安装了。真的需要吗？我是否应该像他们在此处描述的那样运行它， https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-to-run-locally ？

更新1 
为了调试一点，我添加了这2行。
  print（＆quot;丢失键：＆quot;键）
打印（可用键：＆quot; list（mapping.keys（）））
 
缺少键是以下内容：
  embed_tokens
input_layernorm
down_proj
gate_proj
UP_PROJ
post_attention_layernorm
k_proj
 
虽然所有这些都确实存在于模型中。
另外，@hans Kilian在评论中提到，我可能会放一些文件，而这些文件不需要到source_model文件夹中。
我在convert.py中检查了第11行，其中一些键在模型中不存在。]]></description>
      <guid>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</guid>
      <pubDate>Tue, 25 Feb 2025 22:14:20 GMT</pubDate>
    </item>
    <item>
      <title>无法获得我重量的原始小数，在某个地方自动四舍五入？</title>
      <link>https://stackoverflow.com/questions/79467839/unable-to-get-the-raw-decimal-of-my-weights-auto-rounding-somewhere</link>
      <description><![CDATA[我正在做一个单层perceptron，我需要让我的模型预测用户是否使用网站上的某些按钮制作t还是l。 RN我正在尝试获得权重和偏见，以便我可以将它们实现到我的网站Pyscript代码中，因为我无法进行网站拟合和预测。但是，每当我试图打印重量时，它们会回到SCI符号。我已经尝试执行reter，但这是由于某种原因无法使用的。有人可以告诉我我要在哪里做错什么？
 导入numpy作为NP
导入大熊猫作为pd
来自sklearn.model_selection导入train_test_split
从Sklearn Import DataSet中
导入matplotlib.pyplot作为PLT

Def Activation_Function（Z）：
    返回np.Where（z＆gt; = 0，1，0）

＃将数据集作为熊猫数据框架
df = pd.read_csv（&#39;triendingdata.csv＆quot;）
df.head（）

#split培训数据到X和Y
x = df.iloc [：，1：]
打印（x）

y = df.iloc [：，0]
打印（y）


#convert y标签进入整数
y = y.map（{&#39;l&#39;：0，&#39;t&#39;：1}）

#Convert X和Y到Numpy数组以进行以后处理
x = x.to_numpy（dtype = np.float64）
y = y.to_numpy（dtype = np.int32）

打印（x）
打印（y）

打印（x.dtype）
打印（y.dtype）

#split培训数据并检查形状。由于数据集不是很大的原因（58个样本，16个功能），进行70/30拆分
x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.3，andural_state = 42）

打印（x_train.shape）
打印（x_test.shape）
打印（y_train.shape）
打印（y_test.shape）

＃适合该模型以重新考虑代码的Pyscript部分的权重和偏差
权重= np .eros（16）*1
偏差= 0

Learning_rate = 0.01
    
对于_范围（1000）：
    对于IDX，x_i enumerate（x_train）：
        
        linear_product = np.dot（x_i，weights） +偏见 
        
        y_pred = activation_function（linear_product）
        
        损失= y_pred -y_train [idx]
                
        权重 -  = Learning_rate *损失 * x_i
        
        偏见 -  = Learning_rate *损失

ret（（重量））
ret（（偏见））
 
，我希望假设我的转换和方程式是准确的，我希望能得到很大的小数。我已经尝试询问朋友，教授和chatgpt，并使用其他方法以及其他方法以正确的方式打印。]]></description>
      <guid>https://stackoverflow.com/questions/79467839/unable-to-get-the-raw-decimal-of-my-weights-auto-rounding-somewhere</guid>
      <pubDate>Tue, 25 Feb 2025 20:47:24 GMT</pubDate>
    </item>
    <item>
      <title>LSTM培训是否在恢复学习后重置？</title>
      <link>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</guid>
      <pubDate>Sun, 23 Feb 2025 21:11:38 GMT</pubDate>
    </item>
    <item>
      <title>使用射线调节器进行超参数调整的序列化误差</title>
      <link>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79457834/serialization-error-using-ray-tuner-for-hyperparameter-tuning</guid>
      <pubDate>Fri, 21 Feb 2025 15:18:38 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机过度适合我的数据[关闭]</title>
      <link>https://stackoverflow.com/questions/44939725/support-vector-machine-overfitting-my-data</link>
      <description><![CDATA[我正在尝试对虹膜数据集做出预测。我决定将SVM用于此目的。但是，它给了我准确的1.0。是过度拟合的情况还是因为模型很好？这是我的代码。
  x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.2，andy_state = 0）
svm_model = svm.svc（kernel =&#39;linear&#39;，c = 1，gamma =&#39;auto&#39;）
svm_model.fit（x_train，y_train）
预测= svm_model.predict（x_test）
准确_score（预测，y_test）
 
在这里，准确_score返回1。请帮助我。我是机器学习的初学者。]]></description>
      <guid>https://stackoverflow.com/questions/44939725/support-vector-machine-overfitting-my-data</guid>
      <pubDate>Thu, 06 Jul 2017 04:17:53 GMT</pubDate>
    </item>
    <item>
      <title>SVM在Scikit学习</title>
      <link>https://stackoverflow.com/questions/28154839/svm-overfitting-in-scikit-learn</link>
      <description><![CDATA[我正在使用SVM构建数字识别分类。我有10000个数据，然后将它们分为训练和测试数据，比率7：3。我使用线性内核。
结果证明，当更改训练示例编号时，训练精度总是1，但是测试精度仅为0.9（我预计准确性至少为0.95）。我认为结果表明过度拟合。但是，我从事参数，例如C，伽玛，...它们不会太多更改结果。
如何处理SVM中的过度拟合？
以下是我的代码：
 来自Sklearn Import SVM，Cross_validation
svc = svm.svc（kernel =&#39;linear&#39;，c = 10000，伽马= 0.0，冗长= true）.fit（sample_x，sample_y_1num）

Clf = SVC

preditive_y_train = clf.predict（sample_x）
preditive_y_test = clf.predict（test_x）    
准确性= clf.score（sample_x，sample_y_1num） 
efceracy_test = clf.score（test_x，test_y_1num）  
    
#CODDUCT交叉验证 

cv = cross_validation.shufflesplit（sample_y_1num.size，n_iter = 10，test_size = 0.2，andural_state = none）
scores = cross_validation.cross_val_score（clf，sample_x，sample_y_1num，cv = cv）
score_mean =平均值（得分） 
 ]]></description>
      <guid>https://stackoverflow.com/questions/28154839/svm-overfitting-in-scikit-learn</guid>
      <pubDate>Mon, 26 Jan 2015 16:54:04 GMT</pubDate>
    </item>
    <item>
      <title>熵和信息增益[关闭]</title>
      <link>https://stackoverflow.com/questions/5465447/entropy-and-information-gain</link>
      <description><![CDATA[如果我有这样的数据：
 分类属性-1属性-2

正确的狗狗 
正确的狗狗
错误的狗猫 
正确的猫猫
错误的猫狗
错误的猫狗
 
那么，属性-2相对于atteribute-1的信息获益是什么？
我已经计算了整个数据集的熵： - （3/6）log2（3/6） - （3/6）log2（3/6）= 1 
那我被卡住了！  我认为您还需要计算属性-1和属性-2的熵吗？  然后在信息增益计算中使用这三个计算？]]></description>
      <guid>https://stackoverflow.com/questions/5465447/entropy-and-information-gain</guid>
      <pubDate>Mon, 28 Mar 2011 21:44:44 GMT</pubDate>
    </item>
    </channel>
</rss>