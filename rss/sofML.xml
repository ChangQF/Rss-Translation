<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 03 Oct 2024 21:16:03 GMT</lastBuildDate>
    <item>
      <title>尝试在 TensorFlow 中拟合 DQN 模型时出现空断言错误</title>
      <link>https://stackoverflow.com/questions/79051833/empty-assertion-error-when-trying-to-fit-dqn-model-in-tensorflow</link>
      <description><![CDATA[我正在尝试训练 RL TensorFlow 模型。
该模型以形状为 (2,2) 的元组作为输入，并期望输出离散数。
我已经更新到 keras-rl2，所以这不是问题的根源。
模型如下：
def build_model(actions):
model = Sequential()

model.add(layers.Input(shape=(2,2)))

model.add(layers.Flatten())

model.add(layers.Dense(24,activation=&quot;relu&quot;))
model.add(layers.Dense(24,activation=&quot;relu&quot;))

model.add(layers.Dense(actions,activation=&quot;linear&quot;))
return model

模型编译完美无缺。
为了训练代理，我使用以下内容：
import os
os.environ[&#39;TF_USE_LEGACY_KERAS&#39;] = &#39;1&#39;

from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

def build_agent(model, action):
policy = BoltzmannQPolicy()
memory = SequentialMemory(limit=50000, window_length=1)
dqn = DQNAgent(
model=model, 
memory=memory, 
policy=policy, 
nb_actions=actions,
nb_steps_warmup=10, 
target_model_update=1e-2)
return dqn

dqn = build_agent(model, action)
dqn.compile(Adam(learning_rate=1e-3), metrics=[&#39;mae&#39;])

这也不会返回任何错误。
但是，当我尝试使用以下方法拟合模型时，会出现错误
dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)

它返回此错误：
-------------------------------------------------------------------------------
AssertionError Traceback (most recent call last)
Cell In[52], line 1
----&gt; 1 dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)

文件 ~/.local/lib/python3.10/site-packages/rl/core.py:135，在 Agent.fit(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)
133 if self.processor is not None:
134 observer = self.processor.process_observation(observation)
--&gt; 135 assert observer is not None
137 # 在情节开始时执行随机开始，但不将它们记录到体验中。
138 # 这会稍微改变游戏之间的起始位置。
139 nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)

AssertionError: 

是的，断言确实是空的。
我尝试更新我使用的包，因为以前出现过这个问题，但似乎无法解决这个特定问题。我也不认为这是环境问题，因为只传递了元组，以前环境没有出现任何问题。]]></description>
      <guid>https://stackoverflow.com/questions/79051833/empty-assertion-error-when-trying-to-fit-dqn-model-in-tensorflow</guid>
      <pubDate>Thu, 03 Oct 2024 18:43:52 GMT</pubDate>
    </item>
    <item>
      <title>我有 3 列：Column1 需要最大化，Column 2 和 Column3 需要最小化。帮我用这三列构建一个目标列</title>
      <link>https://stackoverflow.com/questions/79051819/i-have-a-3-columns-column1-needs-to-be-maximized-column-2-and-column3-need-to</link>
      <description><![CDATA[我有 3 列：Column1 需要最大化，Column 2 和 Column3 需要最小化。请帮我用这三列构建一个目标列
我尝试过实现
我用来实现目标变量的公式
由于我想最小化 Column2 和 Column3，所以我取了它们的逆。这个想法是，Column2 和 Column3 的值越小，反转时比率就越大。
这通过奖励 Column1 的高值同时惩罚 Column2 和 Column3 的高值来实现（因为大值的逆会更小）。
寻找更好的目标综合。]]></description>
      <guid>https://stackoverflow.com/questions/79051819/i-have-a-3-columns-column1-needs-to-be-maximized-column-2-and-column3-need-to</guid>
      <pubDate>Thu, 03 Oct 2024 18:38:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 MPS 将基于 CUDA 的 Python ML 脚本转换为在 Apple M3 芯片上运行的策略 [关闭]</title>
      <link>https://stackoverflow.com/questions/79051744/strategies-for-converting-cuda-based-python-ml-scripts-to-run-on-apple-m3-chips</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79051744/strategies-for-converting-cuda-based-python-ml-scripts-to-run-on-apple-m3-chips</guid>
      <pubDate>Thu, 03 Oct 2024 18:15:22 GMT</pubDate>
    </item>
    <item>
      <title>将物体检测 YOLOv8 与自动电磁阀解锁/锁定响应相连接 [关闭]</title>
      <link>https://stackoverflow.com/questions/79050772/connecting-object-detection-yolov8-with-automatic-solenoid-unlock-lock-response</link>
      <description><![CDATA[我正尝试使用基本机制（即 电磁阀锁 和解锁机制）对我的项目进行编码。一旦识别出物品，整个代码便可正常工作；我只想补充一点，当找到物品时，它会打印 // Bottle.identified... 此打印将在等待 5-8 秒后自动打开电磁阀。我正在尝试在 YouTube 上观看教程，但我想知道您是否有任何意见或好的建议。
import cv2
import surveillance as sv
from ultralytics import YOLO
import numpy as np

model = YOLO(&#39;yolov8n.pt&#39;)

bounding_box_annotator = sv.BoxAnnotator()
label_annotator = sv.LabelAnnotator()

cap = cv2.VideoCapture(0)

cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)

print(&quot;Camera initialized and ready.&quot;)

CONFIDENCE_THRESHOLD = 0.4

object_class_ids = [39, 44, 42, 41]

当 True 时：
ret, frame = cap.read()

if not ret:
print(&quot;无法捕获帧。退出。&quot;)
break

results = model.predict(frame)

detections = sv.Detections.from_ultralytics(results[0])

confidence_mask = detections.confidence &gt; CONFIDENCE_THRESHOLD
class_mask = np.isin(detections.class_id, object_class_ids)
filtered_mask = np.logical_and(confidence_mask, class_mask)
filtered_detections = detections[filtered_mask]

if len(filtered_detections) &gt; 0:
print(f&quot;过滤后的检测：{filtered_detections}&quot;)

annotated_image = bounding_box_annotator.annotate(scene=frame, detections=filtered_detections)
annotated_image = label_annotator.annotate(scene=annotated_image, detections=filtered_detections)
else:

annotated_image = frame

cv2.imshow(&#39;带有注释的相机供稿&#39;, annotated_image)

if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
print(&quot;&#39;q&#39; 按下，正在关闭...&quot;)
break

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/79050772/connecting-object-detection-yolov8-with-automatic-solenoid-unlock-lock-response</guid>
      <pubDate>Thu, 03 Oct 2024 13:34:25 GMT</pubDate>
    </item>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;
]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>需要关于组合多个分类器预测的指导[关闭]</title>
      <link>https://stackoverflow.com/questions/79049883/need-guidance-on-combining-predictions-from-multiple-classifiers</link>
      <description><![CDATA[我目前正在开展一个涉及多个分类器的项目，每个分类器都针对一个类别子集进行训练。这些分类器旨在处理分类任务的不同方面，但在将它们的输出聚合为单个预测时，我面临着挑战。
例如，如果一个分类器负责区分类别 0 和 1，另一个分类器负责处理类别 2 和 3，当正确答案属于类别 1 时，我们如何有效地组合它们的结果？我们最初的方法是使用“其他”类别来指示输入不属于分类器分配的类别的情况，但这并没有产生预期的结果。
我们现在正在探索实施额外头部以检测分布外类别的可能性，但我们正在寻找更高效、更简化的解决方案。有没有人遇到过类似的问题，或者对有效聚合多个分类器的输出有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/79049883/need-guidance-on-combining-predictions-from-multiple-classifiers</guid>
      <pubDate>Thu, 03 Oct 2024 09:22:41 GMT</pubDate>
    </item>
    <item>
      <title>数据集类别对模型性能的分布影响[关闭]</title>
      <link>https://stackoverflow.com/questions/79049355/dataset-class-distribution-effect-for-model-perf</link>
      <description><![CDATA[数据集的类别分布是否直接影响模型的性能？例如，图 1 和图 2 中的数据集内容相同，但当我将类别组合在一起时，6、7、8 变为 4，2、4、5 变为 2。实际上，最合乎逻辑的做法是尝试看看，但我想问一下是否有针对此的论文式研究。在此处输入图片说明a在此处输入图片说明b
我认为一个类别过多会导致模型过度学习该类别，而不会学习其他类别。]]></description>
      <guid>https://stackoverflow.com/questions/79049355/dataset-class-distribution-effect-for-model-perf</guid>
      <pubDate>Thu, 03 Oct 2024 06:26:16 GMT</pubDate>
    </item>
    <item>
      <title>在使用 TensorFlow 计算 SINR 时遇到问题，当涉及到复数时，TF 的数值稳定性如何？</title>
      <link>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerically-stable-is-t</link>
      <description><![CDATA[我有用于训练神经网络的代码，对于具有多组 (G) 和多用户 (K) 的系统，损失为负 SINR。然而，当我计算 sinr 时，发生了一些奇怪的事情。
SINR公式是这样的：

这是我的代码：
def find_sinr_over_group(H, W):

    西格玛2 = 1
    # 计算 W 的 Hermitian 转置（共轭转置）
    W_H = tf.transpose(tf.math.conj(W), perm=[0, 2, 1]) # 形状：(batch_size, G, M)

    # 计算每组中每个用户的信号功率
    信号功率 = []
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h_g = W_H[:, g, :] # 形状: (batch_size, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算信号功率
        s = tf.matmul(w_h_g[:, tf.newaxis, :], h_g) # 形状: (batch_size, 1, K)
        s = tf.squeeze(s, axis=1) # 形状: (batch_size, K)
        signal_power.append(s)

    signal_power = tf.stack(signal_power, axis=-1) # 形状：(batch_size, K, G)
    signal_power = tf.math.real(tf.math.multiply(signal_power, tf.math.conj(signal_power)))
    # signal_power = tf.math.abs(signal_power) ** 2 # 取绝对平方

    # 计算每组中每个用户的总功率
    总功率=[]
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h = W_H[:, :, :] # 形状：(batch_size, G, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算总功率
        t = tf.matmul(w_h, h_g) # 形状：(batch_size, G, K)
        t = tf.math.real(tf.math.multiply(t, tf.math.conj(t)))
        # t = tf.math.abs(t) ** 2 # 形状：(batch_size, G, K)
        Total_power.append(tf.reduce_sum(t, axis=1)) # 对 G 求和得到 (batch_size, K)

    Total_power = tf.stack(total_power, axis=-1) # 形状: (batch_size, K, G)

    # 通过从总功率中减去信号功率来隔离干扰功率
    干扰功率 = 总功率 - 信号功率 # 形状：(batch_size, K, G)

    #添加噪声功率
    Interference_plus_noise_power = Interference_power + sigma2 # 添加噪声

    # 计算SINR
    sinr = signal_power / Interference_plus_noise_power # 形状：(batch_size, K, G)

    返回正弦值

但问题是，在某些情况下干扰功率是负的。这在数字上是不可能的，因为信号功率只是总功率的一个特例。有人知道为什么会发生这种情况以及我应该如何解决它吗？
正如你所看到的，一开始我使用了 tf.abd 函数，然后以 2 的幂进行提升。我认为这可能是 abs 的问题，所以我尝试通过共轭进行 myltiping 来获取信号的功率。但我仍然有这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerically-stable-is-t</guid>
      <pubDate>Wed, 02 Oct 2024 20:23:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 opencv.dnn.blobFromImage() 输出转换回 rgb 图像后包含 9 张灰度图像？</title>
      <link>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</link>
      <description><![CDATA[据我所知，blobFromImage 将 img 形状：（宽度、高度、通道）转换为 4d 数组（n、通道、宽度、高度）。
因此，如果您传递 1/255 的 scale_factor。| 大小（640,640）据我所知，每个元素应计算为 RGB =&gt; R = R/255。| G = G/255。|...
值 = (U8 - 平均值) * scale_factor

基本上 minmax 在 0 到 1 之间标准化。
所以在 py。
我尝试将输出 blob/ndarray * 255 相乘。并重新整形为（640, 640, 3），看起来输出图像是一张包含 3 行 3 列灰度和略有不同的饱和度的 9 个图像的图像？这是我尝试过的，与上面的 255 示例输出相同。
 test = cv2.dnn.blobFromImage(img, 1.0/127.5, (640, 640), (127.5, 127.5, 127.5), swapRB=True)
t1 = test * 127.5
t2 = t1 + 127.5
cv2.imwrite(&quot;./test_output.jpg&quot;, t2.reshape((640, 640, 3)))

我一直在查看他们的 opencv repo
 subtract(images[i], mean, images[i]);
multiply(images[i], scalefactor, images[i]);

说实话，看起来在 OpenCV lib 中实现的方式相同，但想请教一下大家的意见。
另一个问题是，如果输入的是完整的 u8 rgb 值，为什么会变成灰度？
我尝试通过应用类似的公式将 4d ndarray 转换为匹配 blobFromImage 的输出。但输出并不相同。
我希望转换为 (1, 3, w, h) 的 ndarray 的图像减去平均值并乘以比例因子，当您转换回 (width, height, channel) 时，与 blobFromImage 的输出相同。
输入
输出]]></description>
      <guid>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</guid>
      <pubDate>Wed, 02 Oct 2024 18:53:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么 tf.keras.models.load_model 要重新构建模型？</title>
      <link>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</link>
      <description><![CDATA[ValueError: 顺序模型“sequential_2”已配置为使用
输入形状（None、224、224、3）。您无法使用 input_shape [None、224、224、3] 构建它

def load_model(model_path):
&quot;&quot;&quot;
从指定路径加载已保存的模型。
&quot;&quot;&quot;

tf.keras.config.enable_unsafe_deserialization()
print(f&quot;正在加载已保存的模型：{model_path}&quot;)
model = tf.keras.models.load_model(model_path)
return model

loaded_1000_image_model = load_model(&#39;/content/drive/MyDrive/Dog Vision/models/20241002-16491727887796-1000-images-mobilenetv2-Adam.h5&#39;)

我原本以为它会加载模型而不会出现任何错误，但由于某种原因，它给出了一个值错误，尽管我没有尝试重建或再次给出任何输入形状。]]></description>
      <guid>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</guid>
      <pubDate>Wed, 02 Oct 2024 17:18:38 GMT</pubDate>
    </item>
    <item>
      <title>关于我可以采取的方法来构建一个可以查找图像重复项和近似重复项的应用程序的建议[关闭]</title>
      <link>https://stackoverflow.com/questions/79046818/recommendation-on-the-approach-i-can-take-to-build-an-app-that-finds-image-dupli</link>
      <description><![CDATA[我正在制作一个应用程序，可以从您的图片库中查找重复图片和近似重复图片。
对于我当前的实现，我可以拍摄 2 张​​图片，从中提取嵌入（使用 CLIP 库），比较它的余弦相似度（对于精确重复）和欧几里得距离（对于压缩重复，如从聊天中下载的图片或图片的屏幕截图）。
对于我可以采取的下一步措施，有什么建议吗？我当前的实现如下所示：
main.py
从 PIL 导入图像
导入 torch
导入 numpy 作为 np
导入 cv2 作为 cv
导入 clip
从 sklearn.cluster 导入 KMeans

设备 = “cpu”
图像路径 = “assets/photos/aayush.jpeg”
image_path2 = &quot;assets/photos/aayushResized.png&quot;

# 加载 CLIP 模型和预处理函数
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)

# 预处理图像并添加批处理维度
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
image2 = preprocess(Image.open(image_path2)).unsqueeze(0).to(device)

print(&quot;处理后的图像数据类型为：&quot;, type(image))

def calculate_distance(embedding1, embedding2):
&quot;&quot;&quot; 计算两幅图像之间的欧几里得距离 &quot;&quot;&quot;
dist = torch.nn. functional.pairwise_distance(embedding1, embedding2)
return (dist)

def calculate_cosine_similarity(embedding1, embedding2):
&quot;&quot;&quot; 计算 2 个图像嵌入之间的余弦相似度 &quot;&quot;&quot;
return torch.nn. functional.cosine_similarity(embedding1, embedding2)

def resize_with_aspect_ratio(raw_image_path, new_width):
# 读取原始图像
original_image = cv.imread(raw_image_path)

# 计算纵横比
aspects_ratio = original_image.shape[1] / original_image.shape[0]

# 根据所需宽度确定新高度
determined_height = int(new_width / aspects_ratio)

# 调整图像大小
resized_image = cv.resize(original_image, (new_width, determined_height))
print(&quot;Newly Resized images shape: &quot;, resized_image.shape)
cv.imshow(&quot;Screen&quot;, resized_image)
cv.imwrite(&quot;aayushResized.png&quot;, resized_image)
cv.waitKey(0)
cv.destroyAllWindows()

return resized_image

def normalize(image):
# 计算每个通道的标准差
channel_stds = np.std(image, axis=(0, 1))
print(channel_stds)
# 通过将图像除以通道标准差来进行归一化
normalized_image = image / channel_stds
return (normalized_image)

def clutster(image):
kmeans = KMeans(n_clusters=2, random_state=0, n_init=&quot;auto&quot;).fit(X)

# 从模型中获取图像嵌入

with torch.no_grad():
emb1 = model.encode_image(image)
emb2 = model.encode_image(image2)

print(&quot;嵌入的数据类型为：&quot;, type(emb1), &quot;And &quot;, type(emb2))
# print(emb1, &quot;\n &quot;, emb2)

# 计算嵌入之间的余弦相似度
similarity = calculate_cosine_similarity(emb1, emb2)
distance = calculate_distance(emb1, emb2)

print(f&quot;余弦相似度：{similarity.item()} {type(similarity)}&quot;)
print(f&quot;欧几里得距离：{distance.item()} {type(similarity)}&quot;)


cluster.py
# 使用聚类算法 [k means] 将张量聚类在一起：
import os
import torch
import clip
from PIL import Image
import numpy as np
from sklearn.cluster import KMeans

device =“CPU”
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)
# labels = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;]
name_list = []
np_array = []

# 循环遍历图像文件夹
for photos in os.listdir(&quot;assets/photos&quot;):
image_path = os.path.join(&quot;assets/photos&quot;, photos)
# 预处理图像并获取其张量
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
name_list.append(photos)
# 将张量的高维展平为类似2dim
numpyImage = np.array(image).flatten()
np_array.append(numpyImage)

k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(np_array)

# 获取 int 类标签（由 KMeans 自动生成）
cluster_labels = kmeans.labels_
image_cluster_map = {}
# 循环遍历图像并为其赋予标签
for i, labels in enumerate(cluster_labels):
# print(f&quot;Image {i} 属于群集 {labels}&quot;)
image_cluster_map[f&#39;image_{name_list[i]}&#39;] = labels

# 查看我们的最终群集
for i, (key, value) in enumerate(image_cluster_map.items()):
print(key, &quot;标记为：“，值）

]]></description>
      <guid>https://stackoverflow.com/questions/79046818/recommendation-on-the-approach-i-can-take-to-build-an-app-that-finds-image-dupli</guid>
      <pubDate>Wed, 02 Oct 2024 12:49:21 GMT</pubDate>
    </item>
    <item>
      <title>创建 PartitionedDatasets 的 Kedro PartitionedDataset</title>
      <link>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</link>
      <description><![CDATA[我正在做一个 kedro 项目，我想自动标记数千个音频文件，对它们进行转换，然后将它们存储在一个文件夹中，每个子文件夹对应一个标签。我希望该文件夹成为我的 yml 文件的目录条目
我遵循此 Kedro 教程并创建了我自己的自定义数据集，用于在 kedro 目录中保存/加载 .wav 文件。我还能够在 catalog.yml 中创建 PartitionedDataset  目录条目，例如
audio_folder:
type:partitions.PartitionedDataset
dataset:my_kedro_project.datasets.audio_dataset.SoundDataset
path:data/output/audios/
filename_suffix:&quot;.WAV&quot;

用于在 Kedro 目录中保存/加载 .WAV 文件的文件夹。
我需要的下一个抽象级别是能够创建一个与包含文件夹（例如上面的 audio_folder）相对应的目录条目。我不想通过动态创建目录条目来实现这一点，而是通过扩展 PartitionedDataset 类来实现。这是因为我希望文件夹的文件夹成为我的 catalog.yml 的一部分。我的问题是

这可能吗？你们有人尝试过这样的事情吗？
如果可能的话，我的自定义类应该只包含 _load、_save 和 _describe 方法，就像我在自定义 AbstractDataset 时一样？

编辑
我最终决定创建另一个扩展 AbstractDataset 的自定义类。以下是有关 _load 和 _save 方法的一些详细信息：
def _load(self):
subfolder_names=[ subfolder_name 
for subfolder_name in os.listdir(self._mainfolderpath) 
if os.path.isdir(os.path.join(self._mainfolderpath, subfolder_name)) 
]

wav_paths_dict={}
for subfolder_name in subfolder_names:
subfolder_path=os.path.join(self._mainfolderpath, subfolder_name)
wav_files=[]
for root, dirs, files in os.walk(subfolder_path):
for file in files:
if file.lower().endswith(&#39;.wav&#39;):
wav_file_path=os.path.join(root, file)
wav_file_name=os.path.split(wav_file_path)[-1].replace(&#39;.wav&#39;,&#39;&#39;).replace(&#39;.WAV&#39;,&#39;&#39;)
wav_files.append((wav_file_name,wav_file_path))
wav_paths_dict[subfolder_name]=dict(wav_files)

partitioned_dataset_dict={}
for subfolder_name, sub_dict in wav_paths_dict.items():
partitioned_dataset=[(wav_file_name,SoundDataset(wav_file_path).load()) for wav_file_name,wav_file_path in sub_dict.items()]
partitioned_dataset_dict[subfolder_name]=dict(partitioned_dataset)

return partitioned_dataset_dict

并且
def _save(self, subfolders_dictionary):
if os.path.isdir(self._mainfolderpath):
for root, dirs, files in os.walk(self._mainfolderpath,topdown=False):
for name in files:
os.remove(os.path.join(root, name))
for name in dirs:
os.rmdir(os.path.join(root, name))
os.rmdir(self._mainfolderpath)
os.mkdir(self._mainfolderpath)
for subfolder_name in subfolders_dictionary.keys():
subfolder_path=os.path.join(self._mainfolderpath, subfolder_name) 
os.mkdir(os.path.normpath(subfolder_path))

#print(subfolder_name, subfolder_path)
partitioned_dataset = PartitionedDataset(
path=subfolder_path,
dataset=SoundDataset,
filename_suffix=&quot;.WAV&quot;,
)

partitioned_dataset.save(subfolders_dictionary[subfolder_name])
]]></description>
      <guid>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</guid>
      <pubDate>Tue, 01 Oct 2024 21:22:40 GMT</pubDate>
    </item>
    <item>
      <title>如何定义仅部分可训练的 PyTorch 张量</title>
      <link>https://stackoverflow.com/questions/77737016/how-to-define-pytorch-tensor-that-is-only-partially-trainable</link>
      <description><![CDATA[我正在尝试构建一个自定义模型来在 PyTorch 中训练，长话短说，我需要构建一个张量，其中除矩形次对角线块之外的所有元素都设置为零，至关重要的是，优化过程应该只触及这个次对角线块的元素，而所有零保持不变。为此，我定义了一个自定义 pytorch 网络，并使用 nn.Parameter 定义了我的矩形块
class My_Network(nn.Module):
def __init__(self , vertical_dim , Horizo​​ntal_dim):
super().__init__()
self.total_dim = vertical_dim + Horizo​​ntal_dim
self.subdiagonal_block = nn.Parameter(torch.rand(vertical_dim , Horizo​​ntal_dim))


这样，如果我错了，请纠正我，PyTorch 应该用随机值初始化这个张量的值，并将它们注册为训练期间要优化的模型的参数。但是现在我陷入了困境，我想告诉 PyTorch 构建一个方阵张量，其维度等于 self.total_dim，除了次对角线块之外，其余均为零，正如我在计算中所说，我将在前向方法中定义 pytorch 应该只训练次对角线块。
我可以根据需要添加零张量，而无需将其设置为模型参数，如下所示（如果我没记错的话）：
class My_Network(nn.Module):
def __init__(self , vertical_dim , Horizo​​ntal_dim):
super().__init__()
self.total_dim = vertical_dim + Horizo​​ntal_dim
self.subdiagonal_block = nn.Parameter(torch.rand(vertical_dim , Horizo​​ntal_dim))
self.total_zero_tensor = torch.zeros(self.total_dim, self.total_dim)


但是现在我该如何告诉 PyTorch 将我的下对角线块插入这个零矩阵的左下角？我需要定义这个矩阵以便进行计算（我需要执行矩阵乘法），但至关重要的是，只有小的下对角线块才被视为一组要训练的参数。]]></description>
      <guid>https://stackoverflow.com/questions/77737016/how-to-define-pytorch-tensor-that-is-only-partially-trainable</guid>
      <pubDate>Sat, 30 Dec 2023 18:32:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 google colab 和 google drive 时出现输入/输出错误</title>
      <link>https://stackoverflow.com/questions/54973331/input-output-error-while-using-google-colab-with-google-drive</link>
      <description><![CDATA[当我使用 google colab 时，我会多次随机收到此错误，有时它可以工作，有时则不行 
OSError: [Errno 5] 输入/输出错误

当我与 google drive 交互时是否会发生此错误？
是否有针对此错误的解决方案 ]]></description>
      <guid>https://stackoverflow.com/questions/54973331/input-output-error-while-using-google-colab-with-google-drive</guid>
      <pubDate>Sun, 03 Mar 2019 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>文件名中的键值对是否有标准的文件命名约定？[关闭]</title>
      <link>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</link>
      <description><![CDATA[我有多个以它们所包含的内容命名的数据文件。例如
machine-testM_pid-1234_key1-value1.log

键和值由 - 和 _ 分隔。有没有更好的语法？有没有自动读取这些文件/文件名的解析器？
这里的想法是文件名是人类和机器可读的。]]></description>
      <guid>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</guid>
      <pubDate>Tue, 10 Apr 2012 10:32:32 GMT</pubDate>
    </item>
    </channel>
</rss>