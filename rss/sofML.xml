<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 15 Mar 2024 09:13:18 GMT</lastBuildDate>
    <item>
      <title>自动编码器整形问题</title>
      <link>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</link>
      <description><![CDATA[我的自动编码器出现问题，因为我错误地调整了输出。目前自动编码器的编码与此类似。
我收到此错误：ValueError：尺寸必须相等，但 &#39;{{node Mean_absolute_error/sub}} = Sub[T=DT_FLOAT](sequential_8/sequential_7/conv1d_transpose_14/BiasAdd, IteratorGetNext:1) 的尺寸为 2000 和 3750 &#39; 输入形状：[?,2000,3], [?,3750,3]。
如果可能的话，有人可以帮助调整架构吗？我似乎忘记了最初为此调整所做的原始修改。
导入tensorflow为tf
从tensorflow.keras.models导入模型
从tensorflow.keras.layers导入输入，Conv1D，MaxPooling1D，UpSampling1D，连接
从tensorflow.keras.callbacks导入EarlyStopping

# 提供的编码器
编码器 = tf.keras.models.Sequential([
    tf.keras.layers.Reshape([3750, 3], input_shape=[3750, 3]),
    tf.keras.layers.Conv1D(32，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(64，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(128，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(256，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(512，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512)
]）

#潜在空间

解码器 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512 * 125, input_shape=[512]),
    tf.keras.layers.Reshape([125, 512]),
    tf.keras.layers.Conv1DTranspose（512，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（256，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（128，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（64，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    # 调整内核大小和填充以匹配输入形状
    tf.keras.layers.Conv1DTranspose(3，kernel_size=5，strides=1，padding=“相同”，激活=“线性”)
]）

# 向编码器和解码器添加更多具有更大内核大小的层。
ae = tf.keras.models.Sequential([编码器，解码器])

ae.编译(
    损失=“均方误差”，
    优化器=tf.keras.optimizers.Adam(learning_rate=0.00001)
）
# 定义早期停止标准
Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=30, mode=&#39;min&#39;)

历史= ae.fit（X_train，X_train，batch_size = 8，epochs = 150，validation_data =（X_val，X_val），callbacks = [early_stopping]）```
]]></description>
      <guid>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</guid>
      <pubDate>Fri, 15 Mar 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中线性回归的内部工作原理</title>
      <link>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</link>
      <description><![CDATA[我试图了解 Scikit-learn 中线性回归模型的内部工作原理。
这是我的数据集

这是我执行 one-hot-encoding 后的数据集。

这是执行线性回归后的系数和截距值。

销售价格是从属列，其余列是特征。
这些是在这种情况下工作正常的预测值。

我注意到系数的数量比特征的数量多 1。这就是我生成特征矩阵的方式：
feature_matrix = dataFrame.drop([&#39;售价($)&#39;], axis = &#39;列&#39;).to_numpy()

# 要添加为列的数组
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# 使用append()方法将列添加到数组
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1) # axis = 1表示列，0表示行

结果

我想知道的是 Scikit-learn 如何使用这些系数和截距来预测值。
这是我尝试过的。
我还注意到，通过进行此计算得到的值实际上等于每种情况下的里程数。但这不是这里的依赖功能。那么这是怎么回事？]]></description>
      <guid>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</guid>
      <pubDate>Fri, 15 Mar 2024 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>Prometheus 与 AI 集成</title>
      <link>https://stackoverflow.com/questions/78165538/prometheus-integration-with-ai</link>
      <description><![CDATA[我们的多租户项目集成了普罗米修斯，我们每天都会收到很多警报。我们需要将这些指标与人工智能模型集成，以便我们可以尝试分析它们并在此基础上采取一些行动。这只是这个项目的开始，我什至没有任何基本的想法。
我需要一些关于如何开始、学习什么的指导？]]></description>
      <guid>https://stackoverflow.com/questions/78165538/prometheus-integration-with-ai</guid>
      <pubDate>Fri, 15 Mar 2024 08:24:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在Android Studio中实现实时tflite模型？</title>
      <link>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</link>
      <description><![CDATA[我正在尝试在移动应用程序上进行实时模型实现。我在 Teachable Machine 中训练了模型并将其导出为 model_unquanted.tflite。当我将其导入 Android Studio 时，它会提供以下 Kotlin 代码来实现它：
val model = ModelUnquant.newInstance(context)

// 创建输入以供参考。
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

// 运行模型推理并获取结果。
val 输出 = model.process(inputFeature0)
valoutputFeature0=outputs.outputFeature0AsTensorBuffer

// 如果不再使用则释放模型资源。
模型.close()

以下是我对实时更新的实现：
 覆盖 fun onSurfaceTextureUpdated(p0: SurfaceTexture) {
            位图=textureView.bitmap！！
            val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)

            val byteBuffer: ByteBuffer = ByteBuffer.allocate(224* 224* 3)
            byteBuffer.rewind()

            inputFeature0.loadBuffer(byteBuffer)

            val 输出 = model.process(inputFeature0)
            valoutputFeature0=outputs.outputFeature0AsTensorBuffer

            var 可变 = bitmap.copy(Bitmap.Config.ARGB_8888, true)
            val canvas = android.graphics.Canvas(可变)

            val h = mutable.height
            val w = 可变的.width

            val xPosition = 10 // 根据需要调整该值
            val yPosition = 30 // 根据需要调整该值

            canvas.drawText（outputFeature0.toString（），xPosition.toFloat（），yPosition.toFloat（），绘画）

            imageView.setImageBitmap(可变)

        }

logcat 错误：
 致命异常：main
    进程：com.example.tflite_realtime，PID：14671
    java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。
]]></description>
      <guid>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</guid>
      <pubDate>Fri, 15 Mar 2024 08:22:05 GMT</pubDate>
    </item>
    <item>
      <title>加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</link>
      <description><![CDATA[当我在终端中运行它时，如下所示，出现此错误
nizhar@nizhar-desktop:~/Documents$ g++ -std=c++11 tespredict.cpp -o Predictx -I/usr/local/include/onnxruntime/include -L/usr/local/lib -lonnx运行时
nizhar@nizhar-desktop:~/文档$ ./predictx
./predictx：加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录


我的 onnxrune-time 库路径位于
&lt;前&gt;&lt;代码&gt;/usr/local/lib
/usr/local/include/onnxruntime

我还在.bashrc中添加了
export ONNXRUNTIME_DIR=“/usr/local/include/onnxruntime/include”
导出 LD_LIBRARY_PATH=“/usr/local/lib:$LD_LIBRARY_PATH”

并且已经这样做了
源 ~/.bashrc

如何解决这个问题？请]]></description>
      <guid>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</guid>
      <pubDate>Fri, 15 Mar 2024 08:04:05 GMT</pubDate>
    </item>
    <item>
      <title>prepare_for_model和encode_plus有什么区别？</title>
      <link>https://stackoverflow.com/questions/78165325/what-is-the-difference-between-prepare-for-model-and-encode-plus</link>
      <description><![CDATA[prepare_for_model 之间有什么区别 和 encode_plus？模型需要输入 ID、类型 ID 和注意掩码。那么什么是特殊代币呢？应该使用哪个函数？]]></description>
      <guid>https://stackoverflow.com/questions/78165325/what-is-the-difference-between-prepare-for-model-and-encode-plus</guid>
      <pubDate>Fri, 15 Mar 2024 07:38:54 GMT</pubDate>
    </item>
    <item>
      <title>部署机器学习时出错 - Flask 项目</title>
      <link>https://stackoverflow.com/questions/78165242/error-while-deploying-machine-learning-flask-project</link>
      <description><![CDATA[我正在尝试使用 LSTM 构建手语识别模型。我是烧瓶新手，找不到问题所在。当我运行该文件时，它会打开相机但不会检测到该操作。此外，一旦相机打开，应用程序就会卡住。请帮我找出错误，代码如下：
来自flask导入Flask，render_template，Response
导入CV2
进口泡菜
导入 pyttsx3
将 numpy 导入为 np
将 mediapipe 导入为 mp
导入线程

应用程序=烧瓶（__名称__）

从tensorflow.keras.models导入load_model
model = load_model(&#39;action.h5&#39;)

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

actions = np.array([&#39;你好&#39;,&#39;我是&#39;,&#39;阿凡&#39;,&#39;谢谢&#39;,&#39;我爱你&#39;,&#39;发烧&#39;,&#39;再见&#39;,&#39;上帝&#39;])

def mediapipe_detection（图像，模型）：
    图像 = cv2.cvtColor(图像, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    结果 = model.process(图像)
    image.flags.writeable = True
    图像 = cv2.cvtColor(图像, cv2.COLOR_RGB2BGR)
    返回图像、结果

def extract_keypoints(结果):
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    返回 np.concatenate([lh, rh])

def draw_styled_landmarks（图像，结果）：
    mp_drawing.draw_landmarks(图像, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(颜色=(100, 100, 100), 厚度=2, 圆半径=4),
                             mp_drawing.DrawingSpec(颜色=(100, 100, 100), 厚度=2, 圆半径=2)
                             ）
    mp_drawing.draw_landmarks(图像, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(颜色=(200, 200,200), 厚度=2, 圆半径=4),
                             mp_drawing.DrawingSpec(颜色=(200, 200, 200), 厚度=2, 圆半径=2)
                             ）

序列=[]
句子=[]
预测=[]
阈值 = 0.5

上限 = cv2.VideoCapture(0)

defgenerate_frames():
    sequence = [] # 初始化序列变量
    Sentence = [] # 初始化Sentence变量
    而真实：
        ret, 框架 = cap.read()
        如果不转：
            休息

        图像，结果= mediapipe_detection（框架，整体）
        draw_styled_landmarks（图像，结果）
        关键点 = extract_keypoints(结果)
        序列.append(关键点)
        序列 = 序列[-30:]

        如果长度（序列）== 30：
            res = model.predict(np.expand_dims(序列，轴=0))[0]
            预测.append(np.argmax(res))
            
            if np.unique(预测[-10:])[0] == np.argmax(res):
                如果 res[np.argmax(res)] &gt;临界点：
                    if len(句子) &gt; 0:
                        if actions[np.argmax(res)] !=句子[-1]:
                            句子.append(actions[np.argmax(res)])
                            new_word = 动作[np.argmax(res)]
                            t2s.say(new_word)
                            t2s.runAndWait()
                    别的：
                        句子.append(actions[np.argmax(res)])
                        new_word = 动作[np.argmax(res)]
                        t2s.say(new_word)
                        t2s.runAndWait()

            if len(句子) &gt; 5：
                句子 = 句子[-5:]

        ret, buffer = cv2.imencode(&#39;.jpg&#39;, 图片)
        帧 = buffer.tobytes()
        产量（b&#39;--帧\r\n&#39;
                b&#39;内容类型：image/jpeg\r\n\r\n&#39; + 帧 + b&#39;\r\n&#39;)

    cap.release()


@app.route(&#39;/&#39;)
定义索引（）：
    返回 render_template(&#39;index.html&#39;)

@app.route(&#39;/video_feed&#39;)
def video_feed():
    返回响应（generate_frames（），mimetype =&#39;multipart / x-mixed-replace；边界=框架&#39;）

如果 __name__ == “__main__”：
    t2s = pyttsx3.init()
    整体 = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    应用程序运行（调试=真）
  


我尝试更改模型并修改代码，但不起作用。最初相机馈送未显示，但现在可以正常工作]]></description>
      <guid>https://stackoverflow.com/questions/78165242/error-while-deploying-machine-learning-flask-project</guid>
      <pubDate>Fri, 15 Mar 2024 07:20:28 GMT</pubDate>
    </item>
    <item>
      <title>如何将机器学习代码应用于ryu控制器</title>
      <link>https://stackoverflow.com/questions/78164800/how-apply-machine-learning-code-to-ryu-controller</link>
      <description><![CDATA[我正在按如下方式设置 SDN
航站楼 1：
sudo mn --topo=single,3 --controller=remote,ip=127.0.0.1 --mac --switch=ovsk,protocols=OpenFlow13

航站楼 2：
ryu-manager ryu.app.simple_switch_13

航站楼 3：
sudo ovs-vsctl 显示
sudo ovs-ofctl -O OpenFlow13 转储流 s1

我想添加机器学习来检测 SDN 上的 DDOS 攻击。如何将其添加到 RYU 控制器，以便每次启动 RYU 时该代码都会运行？
我正在使用预配置的 .ova 机器。]]></description>
      <guid>https://stackoverflow.com/questions/78164800/how-apply-machine-learning-code-to-ryu-controller</guid>
      <pubDate>Fri, 15 Mar 2024 05:13:30 GMT</pubDate>
    </item>
    <item>
      <title>如何组装sklearn的IncrementalPCA（ipca.transform）的输出</title>
      <link>https://stackoverflow.com/questions/78164669/how-to-assemble-the-output-of-sklearns-incrementalpca-ipca-transform</link>
      <description><![CDATA[我有一个巨大的 numpy 数组，我需要使用 sklearn 的 IncrementalPCA 对其进行转换。
该数组是 (22, 258186260)。所以我将它分为两​​个轴，如下面的代码。
首先）我不确定它是否允许使用第一轴和第二轴划分输入。
第二）如果正确，如何调整“ipca.transform”的输出
 dset = h5f[&#39;测试&#39;]
               
               ds0, ds1 = dset.shape[0], dset.shape[1]
             
               n = dset .shape[0] # 数据集中有多少行
               chunk_size = 2#1000 # 我们一次向 IPCA 提供多少行，n 的除数
                  

               for i in range(0, n//chunk_size):# forpartial_fit
               
                   对于 np.split( dset[i*chunk_size : (i+1)*chunk_size] , 10,axis=1) 中的批次：
                         pca1.partial_fit（批量）

               my_out_all=[]
               for i in range(0, n//chunk_size): # 用于变换
                  
                   对于 np.split( dset[i*chunk_size : (i+1)*chunk_size], 10,axis=1) 中的批次：
                         
                         my_out=pca1.transform(批处理)
                         
                         my_out_all.append (my_out) # 如何调整输出
]]></description>
      <guid>https://stackoverflow.com/questions/78164669/how-to-assemble-the-output-of-sklearns-incrementalpca-ipca-transform</guid>
      <pubDate>Fri, 15 Mar 2024 04:26:23 GMT</pubDate>
    </item>
    <item>
      <title>手动执行回归</title>
      <link>https://stackoverflow.com/questions/78164553/regressions-performed-by-hand</link>
      <description><![CDATA[有谁知道我在哪里可以找到手动执行的线性回归（简单和多重）示例、多项式和逻辑回归以及在哪里显示如何手动执行它们？我想练习它，以便更好地学习其背后的数学。
有人推荐任何书籍、文章、YouTube 视频吗？
我已经看过几个示例，但我很容易感到困惑，因为大多数情况下它们会在方程中添加错误，或者示例与机器学习无关。]]></description>
      <guid>https://stackoverflow.com/questions/78164553/regressions-performed-by-hand</guid>
      <pubDate>Fri, 15 Mar 2024 03:43:30 GMT</pubDate>
    </item>
    <item>
      <title>将低秩近似应用于可学习参数</title>
      <link>https://stackoverflow.com/questions/78158096/applying-low-rank-approximation-to-learnable-parameters</link>
      <description><![CDATA[我试图了解将低秩近似应用于类中的可学习参数是否有意义。目标是减少参数数量。
我有以下自定义模块：
类 CustomPara(nn.Module):
    
    def __init__(self, num_blocks, in_planes, out_planes, kernel_size):
        super(CustomPara, self).__init__()
        self.coefficient_shape = (num_blocks,1,1,1,1)
        块 = [torch.Tensor(out_planes, in_planes, kernel_size, kernel_size) for _ in range(num_blocks)]
        对于范围内的 i(num_blocks): init.kaiming_normal_(blocks[i])
        self.blocks = nn.Parameter(torch.stack(blocks)) # 这是我们稍后将冻结的内容

    defforward（自身，系数）：
        Final_blocks = (self.blocks*系数).sum(0)
        返回final_blocks

是否可以使用 blocks 参数上的低秩自适应来减少此处可学习参数的数量？]]></description>
      <guid>https://stackoverflow.com/questions/78158096/applying-low-rank-approximation-to-learnable-parameters</guid>
      <pubDate>Thu, 14 Mar 2024 04:22:39 GMT</pubDate>
    </item>
    <item>
      <title>在预训练和微调之间使用略有不同的架构</title>
      <link>https://stackoverflow.com/questions/78157500/using-slightly-different-architecture-between-pretraining-and-fine-tuning</link>
      <description><![CDATA[如果我的以下描述不够全面，请提前致歉。
假设我已经训练了一个简单的 Resnet 类模型，其基本块结构如下：
self.layer1 = self._make_layer(self.inplanes, outplanes[0], num_blocks = 3, stride=1)
self.layer2 = self._make_layer(self.inplanes, outplanes[1], num_blocks = 3, 步长=2)
self.layer3 = self._make_layer(self.inplanes, outplanes[2], num_blocks = 3, 步长=2)
self.layer4 = self._make_layer(inplanes = outplanes[2],planes=outplanes[3],num_blocks = 3,stride=2)

现在我想对这个模型进行微调。但我不想重复相同形状的块 3 次，而是想将它们压缩成单层并加载到我的微调模型中：
self.layer1 = self._make_layer(self.inplanes, outplanes[0], num_blocks = 1, stride=1)
self.layer2 = self._make_layer(self.inplanes, outplanes[1], num_blocks = 1, stride=2)
self.layer3 = self._make_layer(self.inplanes, outplanes[2], num_blocks = 1, stride=2)
self.layer4 = self._make_layer(inplanes = outplanes[2],planes=outplanes[3],num_blocks = 1,stride=2)

本质上，这是相同的模型，但我通过将学习到的权重压缩为一个来减少重复次数。需要注意的是，重复的块都具有相同的形状。
有什么方法可以实现这个或者它在数学或理论上不正确吗？我认为这有点类似于知识蒸馏——概念上？
事实上，我在想是否也可以类似地修改内核大小。]]></description>
      <guid>https://stackoverflow.com/questions/78157500/using-slightly-different-architecture-between-pretraining-and-fine-tuning</guid>
      <pubDate>Thu, 14 Mar 2024 00:25:09 GMT</pubDate>
    </item>
    <item>
      <title>提高特殊神经网络的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</link>
      <description><![CDATA[我正在训练一个神经网络来玩 2048 游戏，并且大多数时候都能达到 2048。
首先，我收集了近 30,000 个游戏的数据集，在 csv 文件中达到 2048 个。每个游戏包含大约 1000 个棋盘游戏状态，可以说我的数据集中有 3000 万个棋盘状态。实际上有 3000 万行，如下图所示。

正如你所见，我每行有 17 列。前 16 列显示每个图块值的 2 的幂（预计 0 表示该图块中没有任何内容）。
最后一列是它根据这个棋盘游戏移动的方向
方向帮助-&gt; （0：上，1：右，2：下，3：左）

例如上图显示了董事会的这种状态：
&lt;前&gt;&lt;代码&gt; 32 64 128 32
 8 32 8 2
 8 0 0 0
 0 2 0 0

该状态的方向为 0，等于向上
所以我创建了一个具有 16 个输入的神经网络（当然我将其更改为 16*11 输入并将输入作为 one-hot 编码传递）、一些隐藏层和 4 个输出。
我在超参数和改变层结构方面进行了很多尝试和错误。
最后我得到了这样的结果。
导入 pandas 作为 pd
从 keras.models 导入顺序
从 keras.layers 导入密集、批量标准化、激活
从 keras.optimizers 导入 Adam
从 keras.callbacks 导入 EarlyStopping
从 sklearn.preprocessing 导入 OneHotEncoder
将 numpy 导入为 np

# 加载你的数据集
数据 = pd.read_csv(&#39;mainDataSet.csv&#39;)

# 分离输入 (X) 和输出 (y)
X = data.iloc[:, 0:16] # 输入特征
y = pd.get_dummies(data.iloc[:, -1]) # 将输出转换为 one-hot 编码

# 定义 one-hot 编码的类别（标签从 0 到 10）
类别 = [i for i in range(11)]

# 对每个输入列应用 one-hot 编码
X_encoded = pd.concat([pd.get_dummies(pd.Categorical(X[col],categories=categories), prefix=col,
prefix_sep=&#39;_&#39;) 对于 X] 中的列，轴=1)



# 定义模型
模型=顺序（）
model.add(Dense(16*11, input_dim=16*11,activation=&#39;relu&#39;)) # 默认包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层d
model.add(Dense(4,activation=&#39;softmax&#39;)) # 具有 4 个方向节点的输出层

# 定义提前停止回调，以在验证损失停止改善时停止训练
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

# 使用 Adam 优化器和默认学习率编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])

# 使用小批量梯度下降和附加回调来训练模型
model.fit（X_encoded，y，epochs = 100，batch_size = 64，validation_split = 0.2，callbacks =
[早停]）

model.save(&#39;2048_model.h5&#39;)

最终它给了我 88% 的准确度和 90% 的验证准确度，但这对于我正在做的事情来说还不够。
您建议采取哪些方法来让我的模型更好地进行预测？
或者甚至您可以对我的神经网络进行哪些更改以使其更加高效？]]></description>
      <guid>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</guid>
      <pubDate>Wed, 13 Mar 2024 05:36:20 GMT</pubDate>
    </item>
    <item>
      <title>Amazon Sagemaker 在后台从 jupyter 笔记本运行代码</title>
      <link>https://stackoverflow.com/questions/78149372/amazon-sagemaker-run-code-from-jupyter-notebook-in-background</link>
      <description><![CDATA[我正在 Amazon Sagemkaer 笔记本实例上运行代码（在普通的 jupyter 笔记本中，而不是 jupyterLab）。

如何在后台运行代码并关闭浏览器选项卡？当我关闭 jupyter 笔记本选项卡时，程序停止，我想避免这种情况。我读到我不应该在笔记本本身中进行处理，而应该使用 Sagemaker 处理作业。如何在更高的 i 上运行如下所示的简单代码单元

df_new[&#39;predicted_values&#39;] = df_original.progress_apply(lambda x: LLM_pretrained_model.predict( x[&#39;comment_body&#39;] )


12 小时后，内核崩溃，提示我需要再次登录。我怎样才能避免这种情况？由于我的数据量较大，程序至少需要 28 小时才能运行

是否可以从 sagemaker jupyter 笔记本（不是 jupyterLab 笔记本）将代码推送到 GitHub？

]]></description>
      <guid>https://stackoverflow.com/questions/78149372/amazon-sagemaker-run-code-from-jupyter-notebook-in-background</guid>
      <pubDate>Tue, 12 Mar 2024 18:39:54 GMT</pubDate>
    </item>
    <item>
      <title>宏观 VS 微观 VS 加权 VS 样本 F1 分数</title>
      <link>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</link>
      <description><![CDATA[在sklearn.metrics.f1_score中，f1分数有一个名为“average”的参数。宏观、微观、加权和样本是什么意思？请详细说明，因为在文档中没有正确解释。或者简单地回答以下问题：

为什么“样本”是多标签分类的最佳参数？ 
为什么微观最适合不平衡的数据集？ 
加权和宏之间有什么区别？
]]></description>
      <guid>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</guid>
      <pubDate>Thu, 18 Apr 2019 06:26:25 GMT</pubDate>
    </item>
    </channel>
</rss>