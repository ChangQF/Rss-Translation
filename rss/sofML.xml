<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 17 Dec 2024 12:36:25 GMT</lastBuildDate>
    <item>
      <title>对于非常随机的文本语料库，哪些是最有效的主题建模算法？</title>
      <link>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</link>
      <description><![CDATA[没有关于语料库长度的信息。
没有关于任何主题层次结构的信息。
我遇到了 BERTopic，但它有 9 种不同的建模类型，哪一种应该适合？我不能使用监督或半监督，因为我没有关于数据的信息，我只知道它与 RFP（提案请求）相关。我可以预测一些主题，因此可以使用种子建模，但也会有随机主题。
我也对 LDA 等仅是句法的方法持开放态度，因为它给出了良好的结果。
我知道概括是不可能的，但想知道你的经验。
最初我尝试了 LDA，它没有给出好的结果，因为像数据长度和数据中的主题数量这样的超参数很难对如此大的完全非结构化随机数据集进行微调。]]></description>
      <guid>https://stackoverflow.com/questions/79287858/which-are-the-most-effective-topic-modelling-algorithm-for-a-very-random-text-co</guid>
      <pubDate>Tue, 17 Dec 2024 12:25:44 GMT</pubDate>
    </item>
    <item>
      <title>如何确保 RStudio 使用我的一半内存？</title>
      <link>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</link>
      <description><![CDATA[我正在尝试使用 tidymodels 在 RStudio 上调整机器学习模型。
我有 Macbook Pro 2019

2.3 GHz 8 核 Intel Core i9，
32 GB 2667 MHz DDR4

对于调整 KNN 回归，它花费了 10 多个小时，我不明白为什么。以下是代码：
knn_model &lt;-
nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%
set_engine(&#39;kknn&#39;) %&gt;%
set_mode(&#39;regression&#39;)

knn_grid &lt;-
grid_regular(
neighbours(),
weight_func(),
dist_power(),
levels = c(20, 5, 5)
)

knn_wf &lt;-
working() %&gt;%
add_model(knn_model) %&gt;%
add_formula(demande_energetique_projectee ~ .)

knn_res &lt;-
knn_wf %&gt;%
tune_grid(
resamples = folds,
grid = knn_grid,
metrics = metric_set(rmse)
)
knn_res

我检查了分配给 rstudio 的内存；它不超过 1.2Gb。但为什么呢？
为什么它没有使用所有内存来加快我的超参数调整速度？
经过一番研究，我在主文件夹中创建了 .Renviron 文件并将其放入
R_MAX_VSIZE=16Gb

并重新启动了 RStudio，但问题并未解决。
以下是有关会话的信息
R 版本 4.3.3 (2024-02-29)
平台：x86_64-apple-darwin20 (64 位)
运行于：macOS 15.1.1

问题：

如何加快超参数调整速度？
我们如何确保 RStudio 使用一半的内存而不是仍然阻塞最大 1.2Gb？
]]></description>
      <guid>https://stackoverflow.com/questions/79287098/how-to-make-sure-that-rstudio-uses-half-of-my-memory</guid>
      <pubDate>Tue, 17 Dec 2024 08:11:39 GMT</pubDate>
    </item>
    <item>
      <title>如何修复使用 Prompt Flow 时出现的“错误：pip 的依赖解析器当前未考虑已安装的所有软件包。”</title>
      <link>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</link>
      <description><![CDATA[我制作了一个自定义映像，以在 Azure Ai Foundry 的 Prompt Flow（早期的 Ai Studio）上使用 python 的 3.10.1 版本。忽略错误，Flow 成功运行。

错误：pip 的依赖解析器当前未考虑已安装的所有软件包。此行为是以下依赖冲突的根源。mlflow 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。mlflow-skinny 2.13.0 需要 protobuf&lt;5,&gt;=3.12.0，但您有不兼容的 protobuf 5.29.1。

但是，我认为这会在最终的生产部署中造成麻烦。此外，我还检查了我的自定义图像上的 protobuf 版本，但版本号是 4.25.5，在这种情况下应该可以正常工作。下面是错误和 docker 容器的屏幕截图。

我在执行 find 部署时遇到的错误如下：

根据用于故障排除此错误的文档，错误为 ResourceNotReady。其中提到了 score.py 文件。我想知道什么是 score.py 文件，这个文件在部署时会自动生成吗？还是需要在自定义镜像时单独创建这个文件？最后我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79286932/how-to-fix-error-pips-dependency-resolver-does-not-currently-take-into-accoun</guid>
      <pubDate>Tue, 17 Dec 2024 07:12:40 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能提高决策树的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</link>
      <description><![CDATA[这是我的代码
# 准备目标和特征
target_column = &#39;resolution&#39;
X = data.drop(columns=[target_column])
y = data[target_column]

# 根据需要将分类数据转换为二进制/数字
X_encoded = pd.get_dummies(X) # 对分类特征进行独热编码
le = LabelEncoder()
y_encoded = le.fit_transform(y) # 编码目标变量

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)

# 执行网格搜索以调整 max_depth、min_samples_split、min_samples_leaf 和 class_weight
param_grid = {
&#39;max_depth&#39;: range(1, 11), # 要测试的 max_depth 范围
&#39;min_samples_split&#39;: [2, 5, 10], # min_samples_split 的选项
&#39;min_samples_leaf&#39;: [1, 2, 4],
&#39;class_weight&#39;: [&#39;balanced&#39;]# min_samples_leaf 的选项
}
model = DecisionTreeClassifier(random_state=42)

# 使用 5 倍交叉验证初始化 GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2,scoring=&#39;accuracy&#39;) # 使用准确度进行分类
grid.fit(X_train, y_train)

# 来自网格搜索的最佳参数
best_params = grid.best_params_
# print(&quot;来自网格搜索的最佳参数：&quot;, best_params)

# 评估测试集上的最佳模型
best_model = grid.best_estimator_
test_accuracy = best_model.score(X_test, y_test) * 100 # 准确率百分比
# print(f&quot;最佳模型的测试准确率：{test_accuracy:.2f}%&quot;)

# 显示决策树的结构
# tree_structure = tree.export_text(best_model, feature_names=list(X_encoded.columns))
# print(&quot;\n决策树结构：&quot;)
# print(tree_structure).

这是我的结果
分类报告：
准确率 召回率 f1 分数 支持率

结果 1 0.39 0.88 0.54 388
结果 2 0.95 0.61 0.75 1397

准确率 0.67 1785
宏平均值 0.67 0.75 0.64 1785
加权平均值 0.83 0.67 0.70 1785


混淆矩阵：
[[343 45]
[540 857]]

“结果 1”类的准确率：88.40%
“结果2&#39;：61.35%
决策树分类器的准确率：0.67
我希望整体准确率和“结果 2”更高。我认为问题在于结果 1 在这里是少数，出于某种原因，决策树的准确率因此降低，尽管它应该是平衡的。]]></description>
      <guid>https://stackoverflow.com/questions/79286053/how-could-i-make-the-accuracy-better-in-my-decision-tree</guid>
      <pubDate>Mon, 16 Dec 2024 20:55:17 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能修复破损的轮廓？[关闭]</title>
      <link>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</link>
      <description><![CDATA[我正在研究图像分割，但轮廓不太正确。这是我经过一些图像清理和轮廓检测后得到的结果，但正如您所见，轮廓在某些地方被破坏了。有办法解决这个问题吗？我偶然发现了这篇论文，DiSTNet2D，其中您应该训练神经网络进行分割。我不太确定这是否有点过头了。有人有更好的建议吗？我已附上我的代码和结果
def gaussian_filter_multiscale_retinex(image: np.ndarray, sigmas: list, weights: list) -&gt; np.ndarray:
img32 = image.astype(&#39;float32&#39;) / 255
img32_log = np.log1p(img32)

msr = np.zeros(image.shape, np.float32)
for sigma, weight in zip(sigmas, weights):
blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
blur_log = np.log1p(blur)
msr += (img32_log - blur_log) * weight

msr /= sum(weights)
return cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)

def process_image(img_path):
img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)
rtnx = gaussian_filter_multiscale_retinex(img, sigmas=[15, 55, 185], weights=[10, 5, 1])
阈值 = cv.adaptiveThreshold(rtnx, 255, adaptableMethod=cv.ADAPTIVE_THRESH_GAUSSIAN_C,
阈值类型=cv.THRESH_BINARY, blockSize=7, C=-7)

nb_components, output, stats, _ = cv.connectedComponentsWithStats(thresholded, connections=8)
大小 = stats[1:, -1]
new_img = np.zeros_like(thresholded)
new_img[np.isin(output, np.where(sizes &gt;= 12)[0] + 1)] = 255

numLabels、labels、stats、centroids = cv.connectedComponentsWithStats(new_img)


]]></description>
      <guid>https://stackoverflow.com/questions/79285496/how-can-i-fix-broken-contours</guid>
      <pubDate>Mon, 16 Dec 2024 17:08:30 GMT</pubDate>
    </item>
    <item>
      <title>二维的 VC 维 [关闭]</title>
      <link>https://stackoverflow.com/questions/79284939/vc-dimension-of-2-dimensional</link>
      <description><![CDATA[问题是：
给出二维（轴平行）矩形的 VC 维数的上限，其中我们可以选择内部或外部是 +。
我得到了 4 个解决方案，但寻求鼓励的朋友说答案应该是 5]]></description>
      <guid>https://stackoverflow.com/questions/79284939/vc-dimension-of-2-dimensional</guid>
      <pubDate>Mon, 16 Dec 2024 14:09:59 GMT</pubDate>
    </item>
    <item>
      <title>针对群体数据/种族数据的最自然类别的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79284779/most-natural-class-of-machine-learning-models-for-group-data-race-data</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 小时数 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 5 4 93 5 50
2 5 5 103 9 88
2 5 8 112 12 99
2 5 1 200 10 100
2 5 2 90 19 78
3 2 5 100 12 84
3 2 7 102 13 88

我想建立一个机器学习模型，尝试预测谁将成为班级第一名（即最高分数）给定 Class_ID，使用 IQ 和 Hours（学习小时数）作为特征。
换句话说，输入是班级中每个学生（例如 1 到 n）的 IQ 和 Hours，输出是概率向量 (p_1, ..., p_n)，其中每个 p_i 是学生 i 在班级中获得最高分数的概率。
这是我尝试过的：

由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。不幸的是，输出是 相关性得分 列表，而不是概率列表，概率列表在概率方面没有自然解释。

解决这个问题的一种方法是在 xgboost 中对相关性得分应用 softmax，但没有直接有意义的概率解释，如基于能量的模型，如 RBM 的能量函数。事实上，我试过这样做，概率变得非常极端（大多数概率质量集中在每个班级的一名学生身上，导致测试结果不佳且方差较大）

另一类学习模型是分类模型，如逻辑回归/决策树。然而，我遇到的问题是每个班级的学生人数不同，因此要训练这样的模型，我们必须首先“扁平化”特征矩阵：

Class_ID Class_size IQ_1 IQ_2 IQ_3 IQ_4 IQ_5 小时_1 小时_2 小时_3 小时_4 小时_5 Score_1 Score_2 Score_3 Score_4 Score_5
1 1 101 99 130 南 南 10 19 3 南 南 98 80 95 南 南
2 5 93 103 112 200 90 5 9 12 10 19 50 88 99 100 78
3 2 100 102 南 南 南 12 13 南 南 南 84 88 南 南 南

使得 1 行代表 1 个训练示例。但随后特征矩阵变得非常稀疏（因为不同班级的学生人数可能有很大差异）
因此，逻辑模型/普通前馈神经网络/基于树的模型似乎也不适用于这类群体数据。
所以我的问题是，是否有任何自然的机器学习模型可以处理这些“群体数据”，就像我上面的数据集一样？
此外，在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不重要）。]]></description>
      <guid>https://stackoverflow.com/questions/79284779/most-natural-class-of-machine-learning-models-for-group-data-race-data</guid>
      <pubDate>Mon, 16 Dec 2024 13:03:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 aiplatform.BatchPredictionJob.create() 在 Vertex AI 中配置模型监控？</title>
      <link>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</link>
      <description><![CDATA[我在使用 aiplatform SDK 设置 Vertex AI 模型监控时遇到了问题，特别是在配置 BatchPredictionJob.create() 时。文档不清楚，缺少定义 model_monitoring_objective_config 和 model_monitoring_alert_config 的示例。由于引用不完整和参数映射不清楚，这导致了兼容性问题。
主要混淆是因为这些配置在 SDK 中没有很好的记录，需要探索 SDK 源代码才能了解正确的结构。此外，SDK 需要来自 aiplatform.model_monitoring 的配置，这在官方指南中没有明确说明。
我尝试过的方法：
我最初参考了 SDK 文档，并根据看似合乎逻辑的内容尝试了各种配置，假设所有组件都可以直接使用 SDK 的类进行配置。但是，这会导致多个类型和参数不匹配错误。
我的预期：
我预期清晰明了的文档，展示如何在使用 aiplatform.BatchPredictionJob.create() 创建批量预测作业时定义和传递监控配置。
实际发生的情况：
我遇到了由于 SDK 和 GAPIC API 之间的参数不匹配而导致的错误。在探索源代码后，我意识到必须使用 aiplatform.model_monitoring 类定义所需的配置。这种跨库依赖关系没有记录。]]></description>
      <guid>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</guid>
      <pubDate>Mon, 16 Dec 2024 07:45:27 GMT</pubDate>
    </item>
    <item>
      <title>利用贝叶斯优化进行多类分类</title>
      <link>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</link>
      <description><![CDATA[无法让这部分代码运行，而且速度真的很慢。
我只想创建一个模型，将叶子图像分为 4 种类型（无，然后是 3 种疾病类型）
我想使用 F1 作为损失函数，然后使用贝叶斯优化来获取叶子类模型的最佳参数，但它没有运行。或者它运行得非常慢然后失败了...
我在 CPU 上运行，因为我没有 GPU
# Optuna 的目标函数
def objective(trial, train_dataloader, val_dataloader, device):
# 使用 Optuna 的建议函数定义超参数搜索空间
conv1_filters = trial.suggest_categorical(&quot;conv1_filters&quot;, [16, 32, 64])
conv2_filters = trial.suggest_categorical(&quot;conv2_filters&quot;, [64, 128, 256])
kernel_size = trial.suggest_categorical(&quot;kernel_size&quot;, [3, 5, 7])
hidden_​​units = trial.suggest_categorical(&quot;hidden_​​units&quot;, [256, 512, 1024])
dropout_rate = trial.suggest_float(&quot;dropout_rate&quot;, 0.1, 0.5)
learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 1e-5, 1e-2, log=True)
num_epochs = trial.suggest_int(&quot;num_epochs&quot;, 10, 50)

# 使用给定的参数初始化模型
model = LeafCNN(
conv1_filters=conv1_filters,
conv2_filters=conv2_filters,
kernel_size=kernel_size,
hidden_​​units=hidden_​​units,
dropout_rate=dropout_rate
).to(device)

# 设置优化器和损失函数
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)

# 提前停止参数
patient = 10 # 允许 10 个 epoch 不进行改进
delta = 0.001 # 算作改进的最小变化
best_val_loss = float(&quot;inf&quot;)
patience_counter = 0 # 从 0 开始计数器

# 训练循环
for epoch in range(num_epochs):
model.train()
for X_batch, y_batch in train_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
optimizer.zero_grad()
outputs = model(X_batch)
loss = loss_fn(outputs, y_batch)
loss.backward()
optimizer.step()

# 验证损失
model.eval()
val_loss = 0.0
使用 torch.no_grad():
对于 val_dataloader 中的 X_batch、y_batch：
X_batch、y_batch = X_batch.to(device)、y_batch.to(device)
输出 = 模型 (X_batch)
损失 = loss_fn(outputs、y_batch)
val_loss += loss.item()

val_loss /= len(val_dataloader)
打印 (f&quot;Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}&quot;)

如果 val_loss &lt; best_val_loss - delta:
best_val_loss = val_loss
waiting_counter = 0
else:
waiting_counter += 1

if waiting_counter &gt;= waiting:
print(f&quot;在 epoch {epoch + 1} 触发提前停止&quot;)
break

# 使用 F1 分数进行评估
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
for X_batch, y_batch in val_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
output = model(X_batch)
_, predict = torch.max(outputs, 1)
all_preds.extend(predicted.cpu().numpy())
all_labels.extend(y_batch.cpu().numpy())

f1 = f1_score(all_labels, all_preds, average=&quot;weighted&quot;)
return f1 # 我们的目标是最大化 F1 分数

# 运行 Optuna 优化的主要函数
def optuna_search(train_dataloader, val_dataloader, device, num_trials):
# 创建 Optuna 研究
study = optuna.create_study(direction=&quot;maximize&quot;) # 最大化 F1 分数
study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device), n_trials=num_trials)

# 打印最佳超参数
print(&quot;找到最佳超参数：&quot;, study.best_params)
print(&quot;最佳 F1 分数：&quot;, study.best_value)

return study.best_params, study.best_value

# 主程序
if __name__ == &quot;__main__&quot;:

# 创建 DataLoaders
train_dataset = TensorDataset(X_train_split, y_train_split)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

val_dataset = TensorDataset(X_val, y_val)
val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# 运行 Optuna 优化
best_params, best_f1 = optuna_search(train_dataloader, val_dataloader, device, num_trials=10)
print(&quot;最佳参数：&quot;, best_params)
print(&quot;最佳 F1 分数：&quot;, best_f1)
]]></description>
      <guid>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</guid>
      <pubDate>Sun, 15 Dec 2024 23:07:57 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 停留在图像生成上</title>
      <link>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</link>
      <description><![CDATA[我创建了一个 LSTM 来生成序列中的下一张图像（我知道 CNN 是用于图像生成的，但我需要整个图像，而不仅仅是提供给序列下一次迭代的过滤器）。所以我有一个数据集，它包含图像（电影中的帧），我创建了它的序列，就像 1 个场景包含例如。 n 个图像，我有 s 个序列长度，那么输入将是 image_1 到 image_s，输出是 image_s+1，下一个输入是 image_2 到 image_s+1，输出是 image_s+2，依此类推。
模型如下：
class LSTM(nn.Module):
def __init__(self, input_len, hidden_​​size, num_layers):
super(LSTM, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_len, hidden_​​size, num_layers, batch_first=True)
self.output_layer = nn.Linear(hidden_​​size, input_len)
self.dropout = nn.Dropout(.2)

def forward(self, X):
hidden_​​states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
out, _ = self.lstm(X, (hidden_​​states, cell_states))
out = self.dropout(out)
out = self.output_layer(out[:, -1, :])
return out

训练是：
def train(num_epochs, model, loss_func, optimizer):
total_steps = loader.getSizeWithBatch()

for epoch in range(num_epochs):
loader.reset()
for item in range(total_steps-1):
element = loader.next()[0]
x_images,y_image = element
x_images = x_images.reshape(-1,sequence_len,input_len)
output = model(x_images)
y_image = y_image.reshape(-1,input_len)
loss = loss_func(output, y_image)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if (item + 1) % 1 == 0:
print(f&#39;Epoch: {epoch + 1};批次：{item + 1} / {total_steps};损失：{loss.item():&gt;4f}&#39;)

if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;model_save_interval&#39;]) == 0:
if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;clean_save_interval&#39;]) == 0:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;] + str(epoch+1)))
else:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;]))

Loader 以张量的形式引导图像由于内存使用，从文件中预先排序。
我使用 MSE 损失和 Adam 作为优化器。
问题是，当我训练它时，错误达到 0.003，这是目标，因为我通过将它们除以 255 来规范化值，但是当我预测时，它会产生一种模糊的场景图像，并且无论输入如何，预测图像始终相同，即使输入来自其他场景，它也会创建相同的图像，当我减去不同输出图像的颜色值时，该值为 0，因此每个输出图像都完全相同。
最终结果看起来就像我将数据集中的每个图像都作为层放在一起一样
我尝试添加 Droput，增加隐藏大小的神经元（现在是 128），尝试增加层数，不同的时期会创建相同的图像，只是模糊程度略低一些，但效果是一样的，我将学习率从 .001 降低到 .0001，效果都是一样的]]></description>
      <guid>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</guid>
      <pubDate>Sun, 15 Dec 2024 20:28:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能在不增加内存使用量的情况下有效地执行此循环？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</link>
      <description><![CDATA[我正在做一个机器学习项目，在 Jupyter Notebook 上预测电动汽车的价格。
我运行这些单元：
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
x[col] = le.transform(x[col]) 
print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5，random_state = 0)

r2_score(y_test，lm.predict(x_test))

从 sklearn.tree 导入 DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train，y_train)
r2_score(y_test，regressor.predict(x_test))

r2_score(y_train，regressor.predict(x_train))

uv = np.nanpercentile(df2[&#39;Base MSRP&#39;]，[99])[0]*2

df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv

df2 = df2[df2[&#39;Model Year&#39;] != &#39;N/&#39;] # 过滤掉包含 &#39;Model Year&#39; 的行&#39;N/&#39;

for col in cols:
df2[col] = df2[col].replace(&#39;N/&#39;, -1)
le.fit(df2[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

le = preprocessing.LabelEncoder()

cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]

for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError回溯（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 返回 _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
以下是数据集的链接：
https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用具有动态尺寸输入的 Dense 层？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</link>
      <description><![CDATA[我有一个模型，其输入（具有形状（高度、宽度、时间）的图像批次）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Dense 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度的输入（无）
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用密集层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：密集层输入的最后一个维度应已定义。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？本质上，我正在寻找一种方法来创建一个具有原始像素值的 1D 数组，但与 Dense 层兼容。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-a-dense-layer-with-an-input-that-has-a-dynamically-sized-dimension</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>为 GPR 创建自定义内核</title>
      <link>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</link>
      <description><![CDATA[我想编写一个仅在 X 轴特定范围内工作的 RBF 内核。我尝试编写一个包含 RBF 核的类来测试代码
class RangeLimitedRBFTest(Kernel):
def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), x_min = 0., x_max = 1.):
self.length_scale = length_scale
self.length_scale_bounds = length_scale_bounds
self.rbf_kernel = RBF(length_scale, length_scale_bounds)
self.x_min = x_min
self.x_max = x_max

def __call__(self, X, Y=None, eval_gradient=False):
if eval_gradient and Y is not None:
raise ValueError(&quot;Gradient can only be evaluating when Y is None.&quot;)

X = np.atleast_2d(X)
if Y is not None:
Y = np.atleast_2d(Y)

print(f&quot;X 形状：{X.shape}&quot;)
如果 Y 不为 None:
print(f&quot;Y 形状：{Y.shape}&quot;)
else:
print(&quot;Y 形状：None&quot;)

K_rbf = self.rbf_kernel(X, Y, eval_gradient=eval_gradient)

如果 eval_gradient:
K, K_grad = K_rbf
print(f&quot;核矩阵形状 (K): {K.shape}&quot;)
print(f&quot;核梯度矩阵形状 (K_grad): {K_grad.shape}&quot;)
return K, K_grad
else:
K = K_rbf
return K

def diag(self, X):
return self.rbf_kernel.diag(X)

def is_stationary(self):
return self.rbf_kernel.is_stationary()

实现和拟合如下
kernel = 1.0 * RangeLimitedRBFTest(length_scale=0.1, length_scale_bounds=(8e-2, 8e-1), x_min=0., x_max=2.5) + WhiteKernel(noise_level=0.5, noise_level_bounds=(1e-2, 1e1))
gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1, alpha=1e-5, optimizer=&#39;fmin_l_bfgs_b&#39;)
gaussian_process.optimizer_kwargs = {&quot;max_iter&quot;: 10000} 
gaussian_process.fit(X, T_PMT)

如果我运行代码，我会得到以下输出
X 形状：(6248, 1)
Y 形状：无
核矩阵形状 (K)：(6248, 6248)
核梯度矩阵形状 (K_grad)：(6248, 6248, 1)
ValueError：第 0 维必须固定为 2，但得到 3

上述异常是导致以下异常的直接原因：

回溯（最近一次调用）：
文件 &quot;/home/tdaq/cremonini/pt100_probe/read_temperatures.py&quot;，第 97 行，位于 &lt;module&gt;
gaussian_process.fit(X, T_PMT)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/base.py&quot;，第 1389 行，在包装器中
return fit_method(estimator, *args, **kwargs)
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 308 行，在 fit 中
self._constrained_optimization(
文件 &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;，第 653 行，在 _constrained_optimization 中
opt_res = scipy.optimize.minimize(
文件&quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_minimize.py&quot;，第 713 行，在 minimal
res = _minimize_lbfgsb(fun, x0, args, jac, bounds,
文件 &quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py&quot;，第 360 行，在_minimize_lbfgsb
_lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
ValueError: 无法将 _lbfgsb.setulb 的第 7 个参数“g”转换为 C/Fortran 数组

如果我尝试使用通常的 RBF 内核，代码可以正常工作。我还尝试禁用优化器 optimizer=None，代码可以正常工作，但会出现非常大的错误。]]></description>
      <guid>https://stackoverflow.com/questions/79271439/create-custom-kernel-for-gpr</guid>
      <pubDate>Wed, 11 Dec 2024 11:00:16 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'_IncompatibleKeys' 对象不可调用</title>
      <link>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</link>
      <description><![CDATA[我正在训练 CNN 以解决多标签分类问题，并使用 torch.save(model.state_dict(), &quot;model.pt&quot;) 保存了我的 .pt 模型。出于某种原因，当我使用以图像数组为输入的自定义函数 predict(x) 测试模型时，我收到以下错误：TypeError: &#39;_IncompatibleKeys&#39; object is not callable。它指出了下面代码的最后一部分：y_test_pred = model(images_tensors)。您知道这里的问题可能是什么吗？ 
导入 numpy 作为 np
导入 cv2
导入 torch
从 torch 导入 nn
导入 torch.nn. functional 作为 F
导入 os

类 Net(nn.Module):
def __init__(self, classes_number):
super().__init__()
self.ConvLayer1 = nn.Sequential(
nn.Conv2d(1, 8, 5), # inp (1, 512, 512)
nn.MaxPool2d(2),
nn.ReLU() # op (8, 254, 254)
)
self.ConvLayer2 = nn.Sequential(
nn.Conv2d(8, 16, 3), # inp (8, 254, 254)
nn.MaxPool2d(2),
nn.ReLU(),
            nn.BatchNorm2d(16) # 操作 (16, 126, 126)
        ）
        self.ConvLayer3 = nn.Sequential(
            nn.Conv2d(16, 32, 3), # inp (16, 126, 126)
            nn.MaxPool2d(2),
            ReLU(),
            nn.BatchNorm2d(32) # 操作 (32, 62, 62)
        ）
        self.ConvLayer4 = nn.Sequential(
            nn.Conv2d(32, 64, 3), # inp (32, 62, 62)
            nn.MaxPool2d(2),
            nn.ReLU() # 运算 (64, 30, 30)
        ）
        self.Lin1 = nn.Linear(30 * 30 * 64, 1500)
self.drop = nn.Dropout(0.5)
self.Lin2 = nn.Linear(1500, 150)
self.drop = nn.Dropout(0.3)
self.Lin3 = nn.Linear(150, classes_number)

def forward(self, x):
x = self.ConvLayer1(x)
x = self.ConvLayer2(x)
x = self.ConvLayer3(x)
x = self.ConvLayer4(x)
x = x.view(x.size(0), -1)
x = F.relu(self.Lin1(x))
x = self.drop(x)
x = F.relu(self.Lin2(x))
x = self.drop(x)
x = self.Lin3(x)
out = torch.sigmoid(x)
return out

def predict(x):
# 在考试中，x 将是指向我们保留集图像的所有路径的列表
images = []
for img_path in x:
img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 转换为灰度
img = cv2.resize(img, (512, 512))
images.append(img)
images = np.array(images)
images = images.reshape(len(images), 1, images.shape[1], images.shape[1]) # converting(n,512,512)&gt;(n,1,512,512)
images_tensors = torch.FloatTensor(np.array(images))
images_tensors = images_tensors.to(device)
classes = [&quot;red blood细胞”、“困难”、“配子体”、“滋养体”、“环”、“裂殖体”、“白细胞”]
model = Net(len(classes))
model = model.load_state_dict(torch.load(&#39;model.pt&#39;))

y_test_pred = model(images_tensors)
y_test_pred[y_test_pred &gt; 0.49] = 1
y_test_pred[y_test_pred &lt; 0.5] = 0

return y_test_pred.cpu().detach()
]]></description>
      <guid>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</guid>
      <pubDate>Tue, 26 Nov 2019 00:16:13 GMT</pubDate>
    </item>
    </channel>
</rss>