<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 22 Oct 2024 03:23:10 GMT</lastBuildDate>
    <item>
      <title>解决 LtR 问题的符号 AI</title>
      <link>https://stackoverflow.com/questions/79112045/symbolic-ai-for-ltr-problems</link>
      <description><![CDATA[我最近一直在思考如何将符号 AI（逻辑）融入学习排序问题。我能想到的唯一地方是机器学习流程中的处理步骤，尽管我仍然不确定“如何”。你们对此有什么（其他）想法吗？我觉得符号 AI 有更多内容可用于 LTR 问题，例如推荐系统。你怎么看？干杯！
一直在阅读一些关于它的论文，但找不到足够的内容。]]></description>
      <guid>https://stackoverflow.com/questions/79112045/symbolic-ai-for-ltr-problems</guid>
      <pubDate>Mon, 21 Oct 2024 23:50:42 GMT</pubDate>
    </item>
    <item>
      <title>SRGAN Android Tensorflow Lite 推理输出与 Python 版本不匹配</title>
      <link>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</link>
      <description><![CDATA[我尝试使用此模型的 tflite 版本实现 Android Kotlin 应用，https://github.com/krasserm/super-resolution/blob/master/model/common.py
我有一个 gan_generator.tflite 模型。我可以推断它并在 Python 中使用它获得正确的输出。但即使我进行了相同的后处理操作，我也无法在 Android Kotlin 中获得与位图相同的结果。我遗漏了什么？
from model.srgan import generator
from utils import load_image, plot_sample
from model import resolve_single
import tensorflow as tf

model = generator()
model.load_weights(&#39;weights/srgan/gan_generator.h5&#39;)

# 从 Keras 模型创建 TFLite 转换器
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 设置转换参数（可选）
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.target_spec.supported_types = [tf.float32]

# 转换模型
tflite_model = converter.convert()

# 将 TFLite 模型保存到文件
with open(&#39;gan_generator.tflite&#39;, &#39;wb&#39;) as f:
f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=&#39;gan_generator.tflite&#39;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()

output_details = interpretationer.get_output_details()

input_shape = input_details[0][&#39;shape&#39;]

input_data = np.asarray(load_image(&#39;demo/0869x4-crop.png&#39;), dtype=np.float32)
input_data = np.expand_dims(input_data, axis=0)
input_data = np.reshape(input_data, (1, input_data.shape[1], input_data.shape[2], 3))

# 设置输入张量
interpreter.resize_tensor_input(input_details[0][&#39;index&#39;], (1, input_data.shape[1], input_data.shape[2], 3),strict=True)
interpreter.allocate_tensors()

interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data)

interpreter.invoke()

# 获取输出
output_data = interpretation.get_tensor(output_details[0][&#39;index&#39;])

output_data = np.squeeze(output_data)
output_data = np.clip(output_data, 0, 255).astype(np.uint8)

# 从 NumPy 数组创建 PIL 图像
image = Image.fromarray(output_data)
image #此处创建的输出图像成功

###----------Android 端-------------------------
 val tensorImage = TensorImage(INPUT_IMAGE_TYPE).also { it.load(inputBitmap) }
val processingImage = imageProcessor.process(tensorImage)

//[1,4X,4X,3] 输出
val outputBuffer = TensorBuffer.createFixedSize(
intArrayOf(
1,
processingImage.width * outputShapeForScaling,
processingImage.height * outputShapeForScaling,
3
),
OUTPUT_IMAGE_TYPE
)

// [1, None, None, 3] 动态输入
explainer!!.resizeInput(
explainer!!.getInputTensor(0).index(),
intArrayOf(1, processingImage.width, processingImage.height, 3)
)

解释器！！.allocateTensors()

解释器！！.run(processedImage.buffer, outputBuffer.buffer)

val processingOutput = processOutput(
outputBuffer,
width = processingImage.width * 4,
height = processingImage.height * 4
)


##---------后期处理---------------------------
private fun processOutput(outputBuffer: TensorBuffer, width: Int, height: Int): Bitmap {
val data = outputBuffer.floatArray

// 检查浮点数组是否具有正确数量的元素
if (data.size != width * height * 3) {
throw IllegalArgumentException(&quot;数据大小与预期图像大小不匹配。&quot;)
}

// 创建一个空的 Bitmap
val bitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)

//遍历浮点数组并设置位图中的像素
var index: Int
for (y in 0 till height) {
for (x in 0 till width) {
index = (y * width + x) * 3
// 从浮点数组中提取 RGB 值
val r = data[index].coerceIn(0f, 255f).toInt()
val g = data[index + 1].coerceIn(0f, 255f).toInt()
val b = data[index + 2].coerceIn(0f, 255f).toInt()

// 设置像素颜色（我们将 alpha 设置为 255 以实现完全不透明度）
bitmap.setPixel(x, y, Color.rgb(r, g, b))
}
}

return bitmap
}

结果：
]]></description>
      <guid>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</guid>
      <pubDate>Mon, 21 Oct 2024 19:24:29 GMT</pubDate>
    </item>
    <item>
      <title>无法单击我的 LinkedIn 自动化脚本中的“连接”按钮</title>
      <link>https://stackoverflow.com/questions/79111488/trouble-clicking-connect-button-in-my-linkedin-automation-script</link>
      <description><![CDATA[我使用 Selenium 开发了一个简单的机器人，它可以打开 LinkedIn 并搜索“数据科学家”和“首席数据科学家”职位。虽然该机器人成功执行了搜索，但目前无法单击“连接”按钮。我将不胜感激任何有关解决此问题的指导。错误消息
GitHub 代码
我尝试过的方法：
我使用 Selenium 创建了一个机器人，它可以打开 LinkedIn 并搜索术语“数据科学家”和“首席数据科学家”。我已经实现了搜索功能，可以成功检索搜索结果。但是，当我尝试点击显示的个人资料的“连接”按钮时，机器人不会记录点击操作。
我的期望：
我希望机器人不仅执行搜索，还会自动点击找到的相关个人资料的“连接”按钮。这将使我能够简化 LinkedIn 上的社交流程。]]></description>
      <guid>https://stackoverflow.com/questions/79111488/trouble-clicking-connect-button-in-my-linkedin-automation-script</guid>
      <pubDate>Mon, 21 Oct 2024 19:21:24 GMT</pubDate>
    </item>
    <item>
      <title>请进一步优化此代码。目前模型的训练准确率为 83%，测试准确率为 80%。我们使用了 Densenet121 [关闭]</title>
      <link>https://stackoverflow.com/questions/79111255/please-give-further-optimization-for-this-code-currently-models-accuracy-is-of</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79111255/please-give-further-optimization-for-this-code-currently-models-accuracy-is-of</guid>
      <pubDate>Mon, 21 Oct 2024 18:07:54 GMT</pubDate>
    </item>
    <item>
      <title>有没有快速的方法可以提高物体检测模型的置信度？[关闭]</title>
      <link>https://stackoverflow.com/questions/79111080/is-there-a-fast-way-to-raise-the-confidence-levels-of-an-object-detection-model</link>
      <description><![CDATA[我目前正在构建一个对象检测模型，用于识别冰箱图片中的食物成分。我使用 Google Colab 来编码和训练我的模型，并使用 RESNET-50 的架构。我删除了已经存在的类别，并使用 Google Colab 最好的 GPU 在包含约 6,000 张图像（已注释，70% 为训练，15% 为有效，15% 为测试）的数据集上对其进行训练。训练后，我的模型似乎仍然没有学到任何东西。取得如此糟糕的结果是否正常，或者有没有办法更快地进行训练并获得更好的结果？我听说过 Google Cloud，但我不知道它是否会有所不同。我是否遗漏了一些隐藏参数？
为了训练我的模型，我使用了 pytorch_lightning 的 Trainer。然而，训练需要很长时间，甚至在训练了 30 个时期之后，当预测正确时（通常情况并非如此），未知数据的置信度仍然低于 5％。]]></description>
      <guid>https://stackoverflow.com/questions/79111080/is-there-a-fast-way-to-raise-the-confidence-levels-of-an-object-detection-model</guid>
      <pubDate>Mon, 21 Oct 2024 17:08:01 GMT</pubDate>
    </item>
    <item>
      <title>如何提高图像分类网络的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79110881/how-can-i-improve-the-accuracy-of-my-image-classification-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79110881/how-can-i-improve-the-accuracy-of-my-image-classification-network</guid>
      <pubDate>Mon, 21 Oct 2024 16:11:45 GMT</pubDate>
    </item>
    <item>
      <title>如何逐步训练人脸识别模型而无需从头开始重新训练？</title>
      <link>https://stackoverflow.com/questions/79110748/how-to-incrementally-train-a-face-recognition-model-without-retraining-from-scra</link>
      <description><![CDATA[我正在构建人脸识别模型。我已经使用两个人（克里斯蒂亚诺·罗纳尔多和莱昂内尔·梅西）的图像训练了一个模型。现在，我想向模型中添加更多人（例如玛丽亚·莎拉波娃），而无需从头开始重新训练所有内容。
有没有办法使用新数据集训练模型？如果是这样，我该如何有效地将新的训练数据与现有模型合并？
这是我现有的代码
import torch
import torchvision
from torchvision import datasets, models, transforms
import os

import ssl
ssl._create_default_https_context = ssl._create_unverified_context

data_transforms = {
&#39;train&#39;: transforms.Compose([
transforms.Resize((224, 224)),
transforms.ToTensor(),
transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
]),
&#39;test&#39;: transforms.Compose([
transforms.Resize((224, 224)),
transforms.ToTensor(),
transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
]),
}

data_dir = &#39;./new_dataset&#39;
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])
for x in [&#39;train&#39;, &#39;test&#39;]}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True)
for x in [&#39;train&#39;, &#39;test&#39;]}
class_names = image_datasets[&#39;train&#39;].classes

model = models.resnet18(pretrained=True, progress=True)

num_classes = len(class_names)
model.fc = torch.nn.Linear(model.fc.in_features, num_classes)

device = torch.device(&quot;cpu&quot;)
model = model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, influence=0.9)

num_epochs = 10

for epoch in range(num_epochs):
for input, labels in dataloaders[&#39;train&#39;]:
input = input.to(device)
labels = labels.to(device)

optimizer.zero_grad()

output = model(inputs)
loss = criterion(outputs, labels)

loss.backward()
optimizer.step()

torch.save(model.state_dict(), &#39;model.pth&#39;)

model.eval()

correct = 0
total = 0

使用 torch.no_grad():
对于 dataloaders[&#39;test&#39;] 中的输入、标签：
输入 = 输入。到（设备）
标签 = 标签。到（设备）

输出 = 模型（输入）
_，预测 = torch.max（输出。数据，1）

总 += 标签。大小（0）
正确 += (预测 == 标签).sum().item()

准确率 = 100 * 正确 / 总计
print(f&quot;测试集上的准确率：{accuracy}%&quot;)

文件夹 ./new_dataset 如下所示这个 new_dataset/ --test/ ----cristiano_ronaldo ----lione_messi --train/ ----cristiano_ronaldo ----lione_messi ]]></description>
      <guid>https://stackoverflow.com/questions/79110748/how-to-incrementally-train-a-face-recognition-model-without-retraining-from-scra</guid>
      <pubDate>Mon, 21 Oct 2024 15:34:20 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种方法或函数可以通过调试、记录器或库获取 Python 神经网络代码中每个变量的值？</title>
      <link>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</link>
      <description><![CDATA[我想了解模型在每个步骤中对其每个变量（如权重、损失、偏差）做了什么，从一个层到另一个层，甚至在批次级别。
是否有任何预建库或方法可以让我获得完整的细分？
下面是我当前的代码，我正在尝试构建所有自定义回调，但我想完全摆脱它并从分配权重等的第 0 阶段的最基础级别理解模型。
来自 keras.models 导入 Sequential
来自 keras.layers 导入 Dense
来自 keras.callbacks 导入 Callback
导入 numpy 作为 np

步骤 1：自定义回调以了解模型内部的工作原理
class DebuggingCallback(Callback):
def on_epoch_begin(self, epoch, logs=None):
print(f&quot;\n--- Epoch {epoch + 1} Start ---&quot;)

def on_epoch_end(self, epoch, logs=None):
print(f&quot;--- Epoch {epoch + 1} End ---&quot;)
for layer_index, layer in enumerate(self.model.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {layer_index + 1}: Weights\n{weights}&quot;)
print(f&quot;Layer {layer_index + 1}: Biases\n{biases}&quot;)

def on_batch_end(self, batch, logs=None):
loss = logs.get(&#39;loss&#39;)
accuracy = logs.get(&#39;accuracy&#39;)
print(f&quot;Batch {batch + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}&quot;)

步骤 2：定义 ANN 模型
classifier = Sequential()
classifier.add(Dense(units=3, input_dim=numFeatures, kernel_initializer=&#39;uniform&#39;,activation=&#39;relu&#39;))
classifier.add(Dense(units=2, kernel_initializer=&#39;uniform&#39;,activation=&#39;relu&#39;))
classifier.add(Dense(units=1, kernel_initializer=&#39;uniform&#39;,activation=&#39;sigmoid&#39;))

classifier.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

步骤 3：拟合模型并使用自定义回调
survivalANN_Model = classifier.fit(X_train, y_train, 
batch_size=30, 
epochs=5, 
verbose=1, 
callbacks=[DebuggingCallback()])

步骤 4：之后打印最终权重训练
print(&quot;\n训练后的最终权重：&quot;)
for i, layer in enumerate(classifier.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {i + 1}:&quot;)
print(&quot;权重：&quot;, weights)
print(&quot;偏差：&quot;, biases)
]]></description>
      <guid>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</guid>
      <pubDate>Mon, 21 Oct 2024 14:50:13 GMT</pubDate>
    </item>
    <item>
      <title>将数据集拆分为训练/测试 - 但保证某个键/标识符在两者中都不存在</title>
      <link>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</link>
      <description><![CDATA[我需要将数据集拆分为训练/测试，并满足以下约束条件：

遵循近似的训练/测试比例拆分（例如 70/30）
数据集中的每一行都有一个键。这些键不是唯一的，这意味着我可能有 1000 行带有 1 个键，另外 2 行带有另一个键。我想保证训练集和测试集中不存在相同的键。

我尝试了网上的所有方法（包括此处的类似主题），例如 GroupKFold 等。但似乎我无法找到任何方法来做到这一点。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</guid>
      <pubDate>Mon, 21 Oct 2024 11:39:07 GMT</pubDate>
    </item>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>‘管道’对象没有属性‘_check_fit_params’</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# 定义特征和目标
X = df.drop(&#39;infected&#39;, axis=1)
y = df[&#39;infected&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样至多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 将多数类欠采样至其原始大小的 80%

pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(&quot;Resampled class distribution:&quot;, pd.Series(y_resampled).value_counts())

这是我的代码
这是我得到的错误
AttributeError Traceback (most recent call last)
Cell In[7], line 19
16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
21 # 显示新的类分布
22 print(&quot;重新采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372, in Pipeline.fit_resample(self, X, y, **fit_params)
342 &quot;&quot;&quot;使用最终估计器拟合模型和样本。
343 
344 依次拟合所有转换器/采样器并
(...)
369 转换后的目标。
370 &quot;&quot;&quot;
371 self._validate_params()
--&gt; 372 fit_params_steps = self._check_fit_params(**fit_params)
373 Xt, yt = self._fit(X, y, **fit_params_steps)
374 last_step = self._final_estimator

AttributeError: &#39;Pipeline&#39; 对象没有属性 &#39;_check_fit_params&#39;

我已经尝试了所有方法。我的所有包都已更新。我尝试使用的所有方法都在 sklearn 和 imblearn 这两个网站上查看过。]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何正确使用 torch.compile？</title>
      <link>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</link>
      <description><![CDATA[我目前正在尝试使用 pytorch 2.0 来提升我的项目的训练性能。我听说 torch.compile 可能会提升一些模型的性能。
所以我的问题（目前）很简单；我应该如何使用带有大型模型的 torch.compile？
例如，我应该像这样使用 torch.model 吗？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...some small model...
)

model = BigModel()
model_opt = torch.compile(model)

，或者像这样？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...一些小模型...
)
self.model = torch.compile(self.model)

model = BigModel()
model_opt = torch.compile(model)

总结一下，

应该编译每一层吗？或者 torch.compile 会自动执行此操作？
有没有什么关于正确使用 torch.compile 的技巧？

说实话，我都试过了，但没有什么区别。
而且，它并没有显著加速，我只是检查了我的模型的加速率大约为 5 ~ 10%。]]></description>
      <guid>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</guid>
      <pubDate>Thu, 30 Mar 2023 08:59:07 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归模型获取准确率最高的模型的max_depth值</title>
      <link>https://stackoverflow.com/questions/63922690/decision-tree-regressor-model-get-max-depth-value-of-the-model-with-highest-accu</link>
      <description><![CDATA[使用默认参数，从 X_train 集和 Y_train 标签构建决策树回归模型。将模型命名为dt_reg。
在训练数据集上评估模型准确率并打印其分数。
在测试数据集上评估模型准确率并打印其分数。
预测X_test集前两个样本的房价并打印出来。（提示：使用predict()函数）
在X_train数据和Y_train标签上拟合多个决策树回归器，max_depth参数值从2变为5。
在测试数据集上评估每个模型的准确率。
提示：使用for循环

打印准确率最高的模型的max_depth值。

import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeRegressor
import numpy as np
np.random.seed(100) 
boston = datasets.load_boston()
X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, random_state=30)
print(X_train.shape)
print(X_test.shape)

dt_reg = DecisionTreeRegressor() 
dt_reg = dt_reg.fit(X_train, Y_train) 
print(dt_reg.score(X_train,Y_train))
print(dt_reg.score(X_test,Y_test))
y_pred=dt_reg.predict(X_test[:2])
print(y_pred)

我想要得到打印具有最高值的模型的max_depth值准确率。但是 fresco 播放未提交，请告诉我错误是什么。
max_reg = None
max_score = 0 
t=()
for m in range(2, 6) :
rf_reg = DecisionTreeRegressor(max_depth=m)
rf_reg = rf_reg.fit(X_train, Y_train) 
rf_reg_score = rf_reg.score(X_test,Y_test)
print (m, rf_reg_score ,max_score) 
if rf_reg_score &gt; max_score :
max_score = rf_reg_score
max_reg = rf_reg
t = (m,max_score) 
print (t)
]]></description>
      <guid>https://stackoverflow.com/questions/63922690/decision-tree-regressor-model-get-max-depth-value-of-the-model-with-highest-accu</guid>
      <pubDate>Wed, 16 Sep 2020 14:53:20 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习来删除重复数据[关闭]</title>
      <link>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</link>
      <description><![CDATA[我遇到了以下问题，并且认为我可以使用机器学习，但我不完全确定它是否适合我的用例。
我有一个包含客户数据（包括姓名、地址、电子邮件、电话等）的约一亿条记录的数据集，我想找到一种方法来清理这些客户数据并识别数据集中的可能重复项。
大多数数据都是使用外部系统手动输入的，没有经过验证，因此我们的许多客户最终在我们的数据库中拥有多个配置文件，有时每条记录中的数据都不同。
例如，我们可能有 5 个不同的条目用于客户 John Doe，每个条目都有不同的联系方式。
我们也遇到过代表不同客户的多条记录在电子邮件等关键字段上匹配的情况。例如，当客户没有电子邮件地址但数据输入系统需要时，我们的顾问将使用随机电子邮件地址，从而导致许多不同的客户资料使用相同的电子邮件地址，电话、地址等也是如此。
我们所有的数据都在 Elasticsearch 中编入索引并存储在 SQL Server 数据库中。我的第一个想法是使用 Mahout 作为机器学习平台（因为这是一家 Java 商店），也许使用 H-base 来存储我们的数据（只是因为它适合 Hadoop 生态系统，不确定它是否有任何实际价值），但我读得越多，我就越困惑它在我的情况下会如何工作，首先我不确定我可以使用哪种算法，因为我不确定这个问题属于哪一类，我可以使用聚类算法或分类算法吗？当然，必须使用某些规则来确定什么构成了个人资料的唯一性，即哪些字段。
我们的想法是最初将其部署为客户个人资料重复数据删除服务，我们的数据输入系统可以使用它来验证和检测输入新客户个人资料时可能的重复项，将来也许可以将其开发为分析平台，以收集有关我们客户的见解。]]></description>
      <guid>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</guid>
      <pubDate>Sun, 05 May 2013 03:36:00 GMT</pubDate>
    </item>
    <item>
      <title>异构数据分类器</title>
      <link>https://stackoverflow.com/questions/10349729/classifier-with-heterogeneous-data</link>
      <description><![CDATA[我有一个 l2 维数据集，包含 1000 个样本，包括 5 个温度值、
5 个价格值、一个表示人类专家判断的整数值（未定=0、好=1、坏=2、危险=4）和一个我想学习预测的二元决策变量。
我如何找到一个可以处理这种异构数据的分类器？
我正在考虑为每个可能的人类判断（0、1、2、4）构建一个分类器，所以有 4 个分类器。
因此，对于每个人的判断值，我会：
- 集中并降低温度和价格值
- 可能使用 PCA 删除一些不相关的特征
- 使用机器学习方法进行分类（如多层神经网络或 SVM）
我的方法正确吗？（如果有 1000 个可能的人类判断而不是 4 个会怎样？）]]></description>
      <guid>https://stackoverflow.com/questions/10349729/classifier-with-heterogeneous-data</guid>
      <pubDate>Fri, 27 Apr 2012 11:24:47 GMT</pubDate>
    </item>
    </channel>
</rss>