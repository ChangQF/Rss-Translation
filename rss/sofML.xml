<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 04 Apr 2024 00:59:08 GMT</lastBuildDate>
    <item>
      <title>用10000条数据训练神经网络，运行无误；但是当有10000000条数据时，就会出现错误，为什么？</title>
      <link>https://stackoverflow.com/questions/78271142/training-a-neural-network-with-10000-data-it-runs-without-error-but-with-10000</link>
      <description><![CDATA[运行时错误：无法在需要 grad 的张量上调用 numpy()。请改用tensor.detach().numpy()。
这个错误指的是下面这句话
输出=并行(n_jobs=arg.sim.jobs)(延迟(parfun)(i) for i in tqdm.tqdm(range(arg.sim.n_ensemble),position=0,leave=True))
在段落中
if arg.sim.n_ensemble&gt;1: #用多个进程训练网络并预测参数 if arg.sim.jobs&gt;1: def parfun(i): net = deep.learn_ML(ML_signal_noisy, arg.sim.bvalues, arg) 返回 deep.predict_ML(ML_signal_noisy[:arg.sim.num_samples_eval, :], arg.sim.bvalues, net, arg) 输出 = 并行(n_jobs=arg.sim.jobs)(延迟(parfun)(i) for i in tqdm.tqdm(range(arg.sim.n_ensemble),position=0,leave=True)) for bb in range(arg.sim.n_ensemble): paramsNN[aa,bb] = output[bb] #train 网络并用一个过程预测参数
几天前，我用数据训练了一个神经网络（NN），其中包含 10000 个数字。神经网络运行没有错误，但结果很差。
然后我将数据增加到 10000000 个数字，期望结果会更好，但是神经网络在没有明确 numpy 的情况下在句子中显示了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78271142/training-a-neural-network-with-10000-data-it-runs-without-error-but-with-10000</guid>
      <pubDate>Thu, 04 Apr 2024 00:56:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 TensorFlow 中使用 model.fit() 会出现 ValueError: Unrecognized data type: x=[...] (of type <class 'list'>) 错误？</title>
      <link>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</link>
      <description><![CDATA[我尝试运行下面的代码，取自  CS50的AI课程：
导入 csv
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split

# 从文件中读取数据
open(“banknotes.csv”) 作为 f：
    读者 = csv.reader(f)
    下一个（读者）

    数据 = []
    对于读卡器中的行：
        数据.追加（
            {
                “证据”：[行[:4]中单元格的浮点（单元格）]，
                “标签”：如果 row[4] == “0”，则为 1否则 0,
            }
        ）

# 将数据分为训练组和测试组
证据 = [行[“证据”] 数据中的行]
labels = [行[“标签”] 数据中的行]
X_训练，X_测试，y_训练，y_测试=train_test_split（
    证据、标签、test_size=0.4
）

# 创建一个神经网络
模型 = tf.keras.models.Sequential()

# 添加一个包含 8 个单元的隐藏层，并使用 ReLU 激活
model.add(tf.keras.layers.Dense(8, input_shape=(4,), 激活=“relu”))

# 添加 1 个单元的输出层，使用 sigmoid 激活
model.add(tf.keras.layers.Dense(1,激活=“sigmoid”))

# 训练神经网络
模型.编译(
    优化器=“adam”，损失=“binary_crossentropy”，指标=[“准确性”]
）
model.fit(X_training, y_training, epochs=20)

# 评估模型的表现
model.evaluate(X_testing, y_testing, verbose=2)

但是，我收到以下错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py”，第 41 行，在  中
    model.fit(X_training, y_training, epochs=20)
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py”，第 113 行，在 get_data_adapter 中
    raise ValueError(f“无法识别的数据类型：x={x}（类型为 {type(x)})”)
ValueError：无法识别的数据类型：x=[...]（类型）

其中“...”是是训练数据。
知道出了什么问题吗？我在 Windows 计算机上使用 Python 版本 3.11.8 和 TensorFlow 版本 2.16.1。
我尝试在 Google Colab 笔记本中运行相同的代码，并且它有效：问题仅发生在我的本地计算机上。这是我期望的输出：
纪元 1/20
26/26 [================================] - 1s 2ms/步 - 损失：1.1008 - 准确度：0.5055
纪元 2/20
26/26 [================================] - 0s 2ms/步 - 损失：0.8588 - 准确度：0.5334
纪元 3/20
26/26 [================================] - 0s 2ms/步 - 损失：0.6946 - 准确度：0.5917
纪元 4/20
26/26 [================================] - 0s 2ms/步 - 损失：0.5970 - 准确度：0.6683
纪元 5/20
26/26 [================================] - 0s 2ms/步 - 损失：0.5265 - 准确度：0.7120
纪元 6/20
26/26 [================================] - 0s 2ms/步 - 损失：0.4717 - 准确度：0.7655
纪元 7/20
26/26 [================================] - 0s 2ms/步 - 损失：0.4258 - 准确度：0.8177
纪元 8/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3861 - 准确度：0.8433
纪元 9/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3521 - 准确度：0.8615
纪元 10/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3226 - 准确度：0.8870
纪元 11/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2960 - 准确度：0.9028
纪元 12/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2722 - 准确度：0.9125
纪元 13/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2506 - 准确度：0.9283
纪元 14/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2306 - 准确度：0.9514
纪元 15/20
26/26 [================================] - 0s 3ms/步 - 损失：0.2124 - 准确度：0.9660
纪元 16/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1961 - 准确度：0.9769
纪元 17/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1813 - 准确度：0.9781
18/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1681 - 准确度：0.9793
19/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1562 - 准确度：0.9793
20/20 纪元
26/26 [================================] - 0s 2ms/步 - 损失：0.1452 - 准确度：0.9830
18/18 - 0s - 损失：0.1407 - 准确度：0.9891 - 187ms/epoch - 10ms/step
[0.14066053926944733，0.9890710115432739]
]]></description>
      <guid>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</guid>
      <pubDate>Thu, 04 Apr 2024 00:28:01 GMT</pubDate>
    </item>
    <item>
      <title>将一个热编码器应用于围绕此的分类数据问题，它将如何应用于一组混合数据？</title>
      <link>https://stackoverflow.com/questions/78270959/appying-one-hot-encoder-to-categorical-data-questions-around-that-how-would-it</link>
      <description><![CDATA[我有一个包含大量分类输入特征 (X) 的数据集。标签 (y) 是一个简单的二进制（是、否）。
下面是作为输入特征的 X 值。
 年龄 违约 工作 婚姻平衡 住房贷款 教育成果
0 30 否 失业 已婚 1787 否 否 主要未知
1 33 无服务已婚 4789 是 是 继发性失败

我的问题是：
我如何对这些数据应用一种热编码？我是否将所有输入特征视为 X_train？如果是这样，这段代码在我第一次训练、测试分割、适合 enc、转换它，然后使用转换后的数据到我的 SVC 模型中是否正确？
X_train,X_test,y_train,y_test=训练测试分割(X,y,随机状态=0)

使用 sklearn 的一个热编码器：
enc.fit(X_train)
X_train_encoded=enc.transform(X_train).toarray()

使用 sklearn 中的 SVC 线性模型：
SVC().fit(X_train_encoded,y_train)

（如果我这样做，形状值会不同，这会产生错误）
因此，下一个问题是：是否应该仅应用某些 X 输入特征值（而不是全部）分类特征值？如果是这样，那么如何将其拟合到线性回归 SVC 模型中？]]></description>
      <guid>https://stackoverflow.com/questions/78270959/appying-one-hot-encoder-to-categorical-data-questions-around-that-how-would-it</guid>
      <pubDate>Wed, 03 Apr 2024 23:29:42 GMT</pubDate>
    </item>
    <item>
      <title>为具有多个特征和不相关目标的 LTSM 准备数据最终会预测平均值</title>
      <link>https://stackoverflow.com/questions/78270152/preparing-data-for-ltsm-with-multiple-features-and-uncorrelated-target-ends-up-i</link>
      <description><![CDATA[我正在尝试使用 LTSM 进行回归来预测微天气事件，例如下一小时的预期温度。这是 LTSM 的一个练习。
数据：
我有一个包含 10000 个样本（行）的 CSV。
每行都有一个时间戳和 3 个特征
(X&#39;s)：日期时间、海拔高度、湿度、压力
还有标签
(Y&#39;s)：日期时间、温度
我的理论是，在我训练 LTSM 模型足够的样本后，它应该能够预测下一小时的温度，但我没有将温度用作特征，而只是用作标签。
这是将单个样本组织到 LTSM 模型的方式（使用 Pytorch）：
[海拔高度(t)、湿度(t)、压力(t)]
[海拔高度(t+1)、湿度(t+1)、气压(t+1)]
[海拔高度(t+2)、湿度(t+2)、气压(t+2)]

这个样本的标签是
&lt;前&gt;&lt;代码&gt;[温度(t+3)]

下一个示例类似，但显然从 t+1 开始，依此类推。
请注意，我正在进行数据缩放（最小最大）和训练测试分割。
问题是我的模型只是预测所有温度标签的平均值，那么我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78270152/preparing-data-for-ltsm-with-multiple-features-and-uncorrelated-target-ends-up-i</guid>
      <pubDate>Wed, 03 Apr 2024 19:45:09 GMT</pubDate>
    </item>
    <item>
      <title>抛物线拟合肺部分段区域</title>
      <link>https://stackoverflow.com/questions/78269913/parabola-fitting-over-lungs-segmented-region</link>
      <description><![CDATA[我正在尝试对胸部 X 光图像进行肺部分割
&lt;前&gt;&lt;代码&gt;导入cv2
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 sklearn.cluster 导入 KMeans

def 边距(img, margin_percent=10):
    h, w = img.shape
    margin_x = int(w * margin_percent / 100)
    margin_y = int(h * margin_percent / 100)
    裁剪 = img[margin_y:h-margin_y, margin_x:w-margin_x]
    返回裁剪

def 进程（img）：
    flat_img = img.flatten().reshape(-1, 1)
    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)
    标签 = kmeans.fit_predict(flat_img)
    中心= kmeans.cluster_centers_
    如果 np.mean(centers[1]) &gt; np.mean(中心[0]):
        标签 = 1 - 标签
    聚集 = np.reshape(标签, img.shape)
    clustered_binary = np.uint8(clustered * 255)
    返回 clustered_binary

def segment_and_filter(clustered_img):
    ret, thresh = cv2.threshold(clustered_img, 0, 255, cv2.THRESH_BINARY)
    如果 cv2.__version__[0] &gt; ‘3’：
        轮廓， _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    别的：
        _，轮廓，_ = cv2.findContours（阈值，cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_SIMPLE）
    
    最大面积 = 0
    最大轮廓=无
    对于轮廓中的轮廓：
        面积 = cv2.contourArea(轮廓)
        如果面积&gt;最大面积：
            最大面积=面积
            最大轮廓 = 轮廓
    
    结果 = np.zeros_like(clustered_img)
    cv2.drawContours(结果, [最大轮廓], -1, 255, 厚度=cv2.FILLED)
    返回结果，最大轮廓

def process_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    裁剪后的img =边距（img）
    clustered_img = 进程(cropped_img)
    过滤结果，最大轮廓=分段和过滤器（簇状图像）
    
    组合图像 = np.hstack((filtered_result[:, :filtered_result.shape[1] // 2],
                                cv2.flip(filtered_result[:, :filtered_result.shape[1] // 2], 1)))
    
    返回组合图像

image_path =“/kaggle/input/chest-xray-pneumonia/chest_xray/test/NORMAL/IM-0105-0001.jpeg”
结果图像=过程图像（图像路径）

plt.figure(figsize=(9, 3))
plt.imshow(result_image, cmap=&#39;灰色&#39;)
plt.title(&#39;合并肺&#39;)
plt.axis(&#39;on&#39;)
plt.show()


上面的代码给出了这个输出
到目前为止一切都很好
现在我想拟合一条覆盖两个肺部区域的抛物线
尝试了多次
但这没有用
你能帮我吗？
在单肺上尝试过，但对两个肺都不起作用

预期
]]></description>
      <guid>https://stackoverflow.com/questions/78269913/parabola-fitting-over-lungs-segmented-region</guid>
      <pubDate>Wed, 03 Apr 2024 18:57:31 GMT</pubDate>
    </item>
    <item>
      <title>colab 中的训练器功能出现错误</title>
      <link>https://stackoverflow.com/questions/78269285/getting-error-with-trainer-function-in-colab</link>
      <description><![CDATA[我在 colab 中遇到此错误，您能帮我解决这个问题吗，
1906 f“将 Trainer 与 PyTorch 一起使用需要 accelerate&gt;={ACCELERATE_MIN_VERSION}：“
第1907章 1907
ImportError：将 Trainer 与 PyTorch 一起使用需要 accelerate&gt;=0.21.0：请运行 pip install Transformers[torch]  或 pip install Accelerator -U
&lt;小时/&gt;
注意：如果您的导入由于缺少软件包而失败，您可以
使用 !pip 或 !apt 手动安装依赖项。
要查看安装一些常见依赖项的示例，请单击
“开放示例”下面的按钮。”我尝试运行 pip install Transformers[torch] 或 pip install Accelerate -U，但它不起作用...任何帮助将不胜感激。
谢谢
沙布南
我尝试使用我的数据集微调拥抱脸部模型...这样做时出现错误]]></description>
      <guid>https://stackoverflow.com/questions/78269285/getting-error-with-trainer-function-in-colab</guid>
      <pubDate>Wed, 03 Apr 2024 16:57:24 GMT</pubDate>
    </item>
    <item>
      <title>我的 CNN 做错了什么，它的准确率提升得这么慢</title>
      <link>https://stackoverflow.com/questions/78269213/what-am-i-doing-wrong-with-my-cnn-that-it-is-gaining-accuracy-so-slowly</link>
      <description><![CDATA[我正在使用这个 CNN 来检测脑电图扫描中的信息。它获得准确性的速度非常缓慢，我想知道我是否遗漏了任何层中的任何内容或做错了什么
类网络（模块）：
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.cnn_layers = 顺序（
            Conv1d(1,14, kernel_size=5, padding=1),
            BatchNorm1d(14),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),

        ）
        self.cnn_layer2 = 顺序(
            Conv1d(14, 10,kernel_size=5, 填充=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            辍学率（0.2），
        ）
        self.cnn_layer3 = 顺序（
            Conv1d(10, 10, kernel_size=5, 填充=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            辍学率（0.2），
        ）
        self.线性_层1 = 顺序(
            线性（in_features = 35660，out_features = 3500），
            BatchNorm1d(3500),
            LeakyReLU(0.1),
            辍学率(0.2)

        ）
        self. Linear_layer2 = 顺序（
            线性（输入特征=3500，输出特征=2500），
            BatchNorm1d(2500),
            LeakyReLU(0.1),
            辍学率(0.2)
        ）
        self. Linear_layer3 = 顺序（
            线性（输入特征=2500，输出特征=250），
            BatchNorm1d(250),
            LeakyReLU(0.1),
            辍学率(0.2)
        ）
        self. Linear_layer4 = 顺序（
            线性（输入特征=250，输出特征=10）
        ）
        self.logsoft = 顺序(
            LogSoftmax(暗淡=1)
        ）
        self.展平 = 顺序（
            Flatten() # 可能需要更改
        ）

    def 前向（自身，x）：
        x = self.cnn_layers(x)

        x = self.cnn_layer2(x)

        x = self.cnn_layer3(x)

        x = self.展平(x)
        x = self. Linear_layer1(x)
        x = self. Linear_layer2(x)
        x = self. Linear_layer3(x)
        x = self. Linear_layer4(x)
        x = self.logsoft(x)
        返回x



模型=网络()

#通过获取 255 列并将它们分组为 14 * 255 个通道来构建数据集
类自定义数据集（）：
    def __init__(self, csv_file, 标签, 变换=无):
        self.df = csv_file
        self.transform = 变换
        self.label = 标签

    def __len__(自身):
        返回 self.df.shape[0]

    def __getitem__(自身，索引)：
        扫描 = (self.df[索引])
        标签 = self.label[索引]
        如果自我变换：
            扫描 = self.transform(扫描)

        返回扫描件、标签

训练数据集 = 行
打印（train_dataset.shape）

train_dataset = CustomDataSet(csv_file=行，标签=(标签))

优化器= SGD(model.parameters(),lr=0.001,weight_decay=5.0e-5)
标准 = CrossEntropyLoss()
纪元数 = 500
train_loss_list = []
批量大小 = 500
train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)
对于范围内的纪元（num_epochs）：
    print(f&#39;历元 {epoch + 1}/{num_epochs}:&#39;, end=&#39; &#39;)
    训练损失 = 0

    # 批量迭代训练数据集
    总正确率 = 0
    样本总数 = 0
    模型.train()
    对于 i，枚举（train_loader）中的（扫描，标签）：
        # 为正在迭代的批次提取图像和目标标签



        # 计算模型输出和交叉熵损失

        输出=模型（扫描）
        打印（输出.形状）

        损失=标准（输出，标签）

        # 根据计算出的损失更新权重
        优化器.zero_grad()
        loss.backward()
        优化器.step()
        train_loss += loss.item()
        _, 预测 = torch.max(输出, 1)

        Total_ Correct += (预测==标签).sum().item()
        样本总数 += labels.size(0)
        # 每个纪元的打印损失
    准确度 = 100 * 总正确率 / 总样本数
    print(&quot;准确率：&quot;, 准确率)
    train_loss_list.append(train_loss / len(train_loader))
    print(f&quot;训练损失 = {train_loss_list[-1]}&quot;)

我尝试在每个层和丢弃层上添加批量标准化。每个时期都接受 20,000 次扫描的训练，但我可以访问 51,000 次，所以我可能会尝试使用更多数据。 100 个 epoch 后，准确率仅达到 13%。这是正常现象还是我犯了错误？]]></description>
      <guid>https://stackoverflow.com/questions/78269213/what-am-i-doing-wrong-with-my-cnn-that-it-is-gaining-accuracy-so-slowly</guid>
      <pubDate>Wed, 03 Apr 2024 16:45:14 GMT</pubDate>
    </item>
    <item>
      <title>如何将 sklearn-crf 套件与文档而不是句子一起使用？</title>
      <link>https://stackoverflow.com/questions/78268442/how-can-i-use-sklearn-crf-suite-with-documents-and-not-sentences</link>
      <description><![CDATA[我想在文档上训练我的 crf 模型，而不是文档中所示的句子 https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html）。
在本例中，我具有以下结构：
&lt;前&gt;&lt;代码&gt;[
    [ # 文档 1
        [(&#39;word1&#39;, {&#39;feature1&#39;: &#39;value1&#39;, &#39;feature2&#39;: &#39;value2&#39;, ...}), (&#39;word2&#39;, {&#39;feature1&#39;: &#39;value3&#39;, &#39;feature2&#39;: &#39;value4&#39; , ...}), ...], # 短语 1
        [(&#39;word3&#39;, {&#39;feature1&#39;: &#39;value5&#39;, &#39;feature2&#39;: &#39;value6&#39;, ...}), (&#39;word4&#39;, {&#39;feature1&#39;: &#39;value7&#39;, &#39;feature2&#39;: &#39;value8&#39; , ...}), ...], # 短语 2
        ...
    ],
    [ # 文档 2
        ...
    ],
    ...
]

不幸的是，我收到以下消息：
 trainer.append(xseq, yseq)
  文件“pycrfsuite/_pycrfsuite.pyx”，第 312 行，位于 pycrfsuite._pycrfsuite.BaseTrainer.append 中
  文件“”，第 48 行，位于 vector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string 中
  文件“”，第 15 行，位于 string.from_py.__pyx_convert_string_from_py_std__in_string
类型错误：预期字节，找到列表

我的印象是crf不接受这样的格式。这种情况我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78268442/how-can-i-use-sklearn-crf-suite-with-documents-and-not-sentences</guid>
      <pubDate>Wed, 03 Apr 2024 14:35:07 GMT</pubDate>
    </item>
    <item>
      <title>Liu 等人 [2022] 中将多时相卫星数据输入 Informer 模型的格式是什么？</title>
      <link>https://stackoverflow.com/questions/78268294/what-is-the-format-in-which-the-multitemporal-satellite-data-was-fed-to-the-info</link>
      <description><![CDATA[Liu et al [2022]，标题为基于水稻产量预测和模型解释在
使用 Transformer 方法的卫星和气候指标，使用 Informer 模型（由 Zhou 等人撰写） al) 使用 MODIS 数据（特别是 MOD13A2）进行水稻产量预测，可在 NASA 网站 上获取气候变量（可在此网站和（此网站）[https: //data.chc.ucsb.edu/products/CHIRPS-2.0/]) 作为功能，以及 各地区农作物产量统计数据作为地面实况数据的值。 Zhou等人的论文中提到的原始Informer模型的代码可以在（GitHub）上找到[https://github.com/zhonghaoyi/Informer2020]。
浏览完代码后，代码似乎将 csv 文件作为输入，其中指定列作为特征，其他指定列作为目标。为了调整多时相卫星数据以适合 csv 文件，Liu 等人说：
&lt;块引用&gt;
八个连续变量（即 NDVI、EVI、NIRV、SIF、Tmax、Tmin、Srad 和 Pr）在空间上聚合到区级别。最后，总共应用了 96 个特征来构建模型，其中包含 8 个连续变量，每个变量有 12 个时间间隔。

我的问题是，“聚合”到底是什么意思？也许他们的意思是这些值是所有像素的平均值？或者是某种其他形式的“聚合”？
我尝试过的：
我一直在寻找将卫星数据输入变压器模型的可能方法，视觉变压器（ViT）似乎经常用于处理卫星数据。然而，这仅适用于单个时间步数据。对于多时相数据（即具有多个时间步长的数据），出现了其他更复杂的方法，但 Liu 等人没有提到这些方法。由于我的目标是复制 Liu 等人的结果，因此使用如此复杂的方法是不可取的。]]></description>
      <guid>https://stackoverflow.com/questions/78268294/what-is-the-format-in-which-the-multitemporal-satellite-data-was-fed-to-the-info</guid>
      <pubDate>Wed, 03 Apr 2024 14:12:45 GMT</pubDate>
    </item>
    <item>
      <title>文本转换：训练模型从输入文件生成输出文件</title>
      <link>https://stackoverflow.com/questions/78268215/text-transformation-train-a-model-to-generate-output-files-from-input-files</link>
      <description><![CDATA[我目前正在开展一个项目，其中包括“翻译”文件从一种格式转换为另一种格式，我使用了带有映射的编程方法，该映射将输入文件中的每个模式与输出中方便的模式链接起来，编写映射和执行转换的类非常困难，此外，我每次我想要新的格式或调整时都应该编写一个新的，所以我决定用一个可以用数千个输入文件及其各自的输出进行尝试的模型来替换整个东西，这里有一个小例子可以给你一个想法：
来自输入文件的块：
DIM+1.800:0.400:500:CT+12
在输出中转换为 XML 标记：
 &lt;高度&gt;1.800&lt;\高度&gt;
    &lt;长度&gt;0.400&lt;\长度&gt;
    &lt;宽度&gt;500&lt;\宽度&gt;


所以基本上，第一行就是我们所说的“段”，它以 3 个字母的缩写开头，正如您所看到的，它在这里代表尺寸，它转换高度、长度和宽度，如图所示，输入文件是一系列像尺寸、日期、地址之类的段，输出可以是 XML 或其他东西，但我想模型的逻辑不会有很大不同，我可以将其调整为其他格式，这就是我现在所需要的是XML，这怎么办？？？
我对机器学习非常陌生，但我作为开发人员已经有足够的时间了，所以我尝试了一种严格的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78268215/text-transformation-train-a-model-to-generate-output-files-from-input-files</guid>
      <pubDate>Wed, 03 Apr 2024 14:01:01 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：使用 LRCN 改进可疑活动检测模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78267648/seeking-advice-improving-suspicious-activity-detection-model-using-lrcn</link>
      <description><![CDATA[我正在使用 LRCN 开发可疑活动检测模型，该模型有四个类别：跑步、步行、打斗和不打斗。然而，我在准确区分步行和跑步方面遇到了困难，导致准确率较低。
我正在考虑两种可能的解决方案，非常感谢您的见解：

数据增强：添加更多数据或旋转视频是否有助于提高分类准确性？

模型增强：我在两个选项之间左右为难：

集成情绪分析来分析活动的背景。这是否可行且有益，还是会使模型过于复杂？

实时视频分类：使用边界框实现实时视频分类系统来识别活动。此外，我有兴趣针对持枪等活动建立警报系统。我应该如何为此添加更多数据集和类？




我还在寻求有关任何预训练模型或资源的建议，以帮助提高我的模型的准确性和功能。
到目前为止准确率为 76%]]></description>
      <guid>https://stackoverflow.com/questions/78267648/seeking-advice-improving-suspicious-activity-detection-model-using-lrcn</guid>
      <pubDate>Wed, 03 Apr 2024 12:29:18 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试了所有测试并且得到了合理的分数时，为什么我的混淆矩阵是这样的？</title>
      <link>https://stackoverflow.com/questions/78266973/why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tests-and-i-got-reaso</link>
      <description><![CDATA[我正在使用 sklearn 的随机森林分类，除了混淆矩阵之外，我在所有方面都得到了不错的结果，这里是代码和结果
训练和测试的标签分布
火车组的大小
模型
训练模型的分数
这是问题
这不是我所期望的，特别是因为训练量仅为训练数据集中 677k 的训练量的 1/3，但在混淆矩阵中它只处理所有标签 0。 
模型：
导入时间
# 记录开始时间
开始时间 = 时间.time()

# 随机森林分类器
rf = 随机森林分类器()

# 定义参数网格
rf_param_grid = {&#39;n_estimators&#39;：[45]，&#39;标准&#39;：[&#39;熵&#39;]，&#39;max_深度&#39;：[30]}

# 网格搜索
rf_cv = GridSearchCV(rf, rf_param_grid, cv=7)
rf_cv.fit(X_train, y_train)

# 记录结束时间
结束时间 = time.time()

# 计算经过的时间
经过时间 = 结束时间 - 开始时间

# 打印结果
print(&quot;最佳成绩：&quot;, rf_cv.best_score_)
print(&quot;最佳参数：&quot;, rf_cv.best_params_)
print(&quot;经过时间:&quot;, elapsed_time, &quot;秒&quot;)

我在这里的每堂课都取得了超过 98% 的好成绩：
# 对训练数据进行预测
y_train_pred = rf_cv.predict(X_train)

# 计算准确率
准确度=准确度_得分（y_train，y_train_pred）

# 计算每个类别的准确率、召回率和 F1 分数
精度 = precision_score(y_train, y_train_pred, 平均值=无)
召回率=召回率（y_train，y_train_pred，平均值=无）
f1 = f1_score(y_train, y_train_pred, 平均值=无)

# 计算宏观平均精度、召回率和 F1 分数
Macro_ precision = precision_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)
宏召回 = 召回分数(y_train, y_train_pred, 平均值=&#39;宏&#39;)
Macro_f1 = f1_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)

# 打印评估指标
print(“准确度：”, 准确度)
print(&quot;精度（0、1、2 类）：&quot;, precision)
print(“召回（0、1、2 类）：”，召回）
print(&quot;F1-分数（0、1、2 类）：&quot;, f1)
print(&quot;宏观平均精度：&quot;, macro_ precision)
print(&quot;宏观平均召回率：&quot;, Macro_recall)
print(“宏观平均 F1 分数：”, Macro_f1)

混淆矩阵，其中不显示除 0 类之外的所有标签
# 生成混淆矩阵
conf_matrix = fusion_matrix(y_train, y_train_pred)

# 定义类标签
class_labels = [&#39;类别 0&#39;, &#39;类别 1&#39;, &#39;类别 2&#39;]

# 使用类标签可视化混淆矩阵
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt=“d”, cmap=“蓝调”, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel(&#39;预测标签&#39;)
plt.ylabel(&#39;真实标签&#39;)
plt.title(&#39;混淆矩阵&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78266973/why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tests-and-i-got-reaso</guid>
      <pubDate>Wed, 03 Apr 2024 10:21:59 GMT</pubDate>
    </item>
    <item>
      <title>m=365 时 PM10 浓度预测的 SARIMA 模型存在问题</title>
      <link>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</link>
      <description><![CDATA[我正在尝试构建 SARIMA（季节性自回归综合移动平均线）模型，用于根据五年的数据预测 PM10 浓度。但是，当我将季节性参数 m 设置为 365 时，我的代码似乎无法运行。
有人可以解释一下为什么我的代码没有在 m=365 下运行并提出潜在的解决方案吗？
提前致谢！
# 这是我的代码片段：
## 将数据集拆分为训练集和测试集

    `train_size = int(len(Alipur_df) * 0.8) # 80% 训练，20% 测试`
    `训练，测试 = Alipur_df[:train_size], Alipur_df[train_size:]`

## 将训练 DataFrame 转换为 numpy 数组

    `train_values = train[&#39;Alipur&#39;].values`
    `test_values = test[&#39;Alipur&#39;].values`

## 使用 auto_arima 找到 SARIMA 的最佳参数

    `auto_model = auto_arima(train[&#39;Alipur&#39;],seasonal=True,stationary=True,m=365,trac
]]></description>
      <guid>https://stackoverflow.com/questions/78270055/issue-with-sarima-model-for-pm10-concentration-forecasting-with-m-365</guid>
      <pubDate>Wed, 03 Apr 2024 06:23:04 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“llama_index.llms”（未知位置）导入名称“HuggingFaceInferenceAPI”</title>
      <link>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</link>
      <description><![CDATA[想要导入 HuggingFaceInferenceAPI。
从 llama_index.llms 导入 HugggingFaceInferenceAPI

llama_index.llms 文档没有 HuggingFaceInferenceAPI 模块。有人有这方面的更新吗？]]></description>
      <guid>https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms</guid>
      <pubDate>Sun, 31 Mar 2024 14:21:19 GMT</pubDate>
    </item>
    <item>
      <title>打印函数在 Keras/Tensorflow 中的调用函数内不打印任何内容</title>
      <link>https://stackoverflow.com/questions/72176003/print-function-is-printing-nothing-inside-call-function-in-keras-tensorflow</link>
      <description><![CDATA[我想使用 print 命令打印下面的调用函数中的一些对象，但当代码成功运行时它不会打印任何内容。我正在阅读 (THIS) keras 调试教程，但我仍然很困惑为什么它不打印任何东西。
&lt;前&gt;&lt;代码&gt;#Hyperparams
学习率 = 0.001
权重衰减 = 0.0001
批量大小 = 100
纪元数 = 1

image_size = 72 # 我们将输入图像调整为这个大小
patch_size = 6 # 从输入图像中提取的补丁的大小
num_patches = (image_size // patch_size) ** 2
投影暗度 = 64
头数 = 4
变压器单位 = [
投影_dim * 2，
投影_暗淡，
] # 变压器层的大小
变压器层数 = 8
mlp_head_units = [2048, 1024]

我想打印下面的调用函数中的（位置和编码）。为此，我使用了打印，但它不起作用。而此处，他们就这样做了。
类 PatchEncoder(layers.Layer):
  def __init__(自身、num_patches、projection_dim、position_embedding):
  超级.__init__()
  self.num_patches = num_patches
  self.projection=layers.Dense(units=projection_dim)
  self.position_embedding = 层.Embedding(
    input_dim=num_patches，output_dim=projection_dim
  ）

  def 调用（自身，补丁）：
  位置 = tf.range(start=0, limit=self.num_patches, delta=1)
  编码 = self.projection(patch) + self.position_embedding(positions)
  print(&quot;编码形状为：&quot;,encoded.shape)
  print(&quot;pos.shape 是：&quot;, Positions.shape)
  返回编码
]]></description>
      <guid>https://stackoverflow.com/questions/72176003/print-function-is-printing-nothing-inside-call-function-in-keras-tensorflow</guid>
      <pubDate>Mon, 09 May 2022 17:27:22 GMT</pubDate>
    </item>
    </channel>
</rss>