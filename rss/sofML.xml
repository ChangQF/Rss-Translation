<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 15 Oct 2024 09:18:25 GMT</lastBuildDate>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>如何向强化学习模型添加命令？</title>
      <link>https://stackoverflow.com/questions/79088319/how-to-add-commands-to-a-reinforcement-learning-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088319/how-to-add-commands-to-a-reinforcement-learning-model</guid>
      <pubDate>Tue, 15 Oct 2024 03:27:53 GMT</pubDate>
    </item>
    <item>
      <title>当数据集包含一些文本字符串和数字数据时，如何继续拟合模型</title>
      <link>https://stackoverflow.com/questions/79088309/how-to-proceed-to-fit-in-a-model-when-dataset-has-some-text-strings-and-numeric</link>
      <description><![CDATA[我正在尝试将数据集放入模型中。缩放、删除不必要的数据后，数据集看起来如下所示；



seconds_log
minutes_log
country_enc
changing
description
Other feature variable




14.0058
0.693147
1
False
红色闪烁...
数字数据


3.401197
3.401197
0
True
白色旋转..
数字数据



因此，所有特征都具有数值，最后一个特征“描述”是长文本。请注意，我的目标变量是“country_enc”，其余所有列都是特征变量。描述值不能忽略。因此我使用&quot;TfidfVectorizer&quot;对数据进行矢量化。
我的问题是，在矢量化描述列之后，无论我得到什么，如何与数据集的其余部分相结合，然后将其拆分为测试和训练模型？
我看到一些谷歌示例，他们说；
combined_data = np.hstack((desc_tfidf.toarray(), original_dataframe_without_description))
print (combined_data)

如果我查看这个组合数据，它是一个numpy.ndarray。在分类模型中提供这些信息没有多大帮助。
基于将 Sklearn TFIDF 与附加数据相结合，如果我将稀疏矩阵转换为数组并与其余数据框（具有所有数值）连接，我会看到生成了几个 NaN 数据，这是预期的。我已经完成了数据预处理，所以再次执行 - 这是不对的。
我敢肯定，这不是正确的方法。有人可以在这里给出一些如何继续的指示吗？]]></description>
      <guid>https://stackoverflow.com/questions/79088309/how-to-proceed-to-fit-in-a-model-when-dataset-has-some-text-strings-and-numeric</guid>
      <pubDate>Tue, 15 Oct 2024 03:21:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Java 读取 Quick Draw! 二进制文件 [关闭]</title>
      <link>https://stackoverflow.com/questions/79087951/reading-quick-draw-binary-files-in-java</link>
      <description><![CDATA[我一直在从头开始试验机器学习算法，并用 Java 构建了一个模型，现在我想在 MNIST 数字以外的一些数据上训练它。我想使用 Quick, Draw! 数据，并想尝试读取我下载的一些涂鸦的二进制文件，但我不太明白。
我写了这段代码来读取数据：
public class QuickDrawReader {

public static void unpackDrawing(InputStream inputStream) throws IOException {
DataInputStream dataInputStream = new DataInputStream(inputStream);

long keyId = dataInputStream.readLong();
System.out.println(&quot;keyId: &quot; + keyId);

byte[] countryCodeBytes = new byte[2];
dataInputStream.readFully(countryCodeBytes);
String countryCode = new String(countryCodeBytes, &quot;UTF-8&quot;);
System.out.println(&quot;CountryCode: &quot; + countryCode);

byte perceived = dataInputStream.readByte();
System.out.println(&quot;Recognized: &quot; + perceived);

int timestamp = dataInputStream.readInt();
System.out.println(&quot;Timestamp: &quot; + timestamp);

int nStrokes = dataInputStream.readUnsignedShort();
System.out.println(&quot;NStrokes: &quot; + nStrokes);

List&lt;int[]&gt; image = new ArrayList&lt;&gt;();
// 如何填充图像列表???
}

public static void unpackDrawings(String filename) throws IOException {
try (FileInputStream fileInputStream = new FileInputStream(filename)) {
while (true) {
try {
unpackDrawing(fileInputStream);
} catch (EOFException e) {
break;
}
}
}
}
}

它似乎可以工作，直到它读取 nStrokes，我甚至不知道如何开始读取图像数据。有人能帮我弄清楚如何完成读取这些数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/79087951/reading-quick-draw-binary-files-in-java</guid>
      <pubDate>Mon, 14 Oct 2024 23:13:46 GMT</pubDate>
    </item>
    <item>
      <title>具有离散变量的线性回归问题，略有不同[关闭]</title>
      <link>https://stackoverflow.com/questions/79087896/linear-regression-problem-with-discrete-variables-with-a-twist</link>
      <description><![CDATA[我正在尝试开发一种 ML 算法来预测从更大的群体中选出的团队的表现。该组有 G 名成员，每个成员都用他们的名字来标识，并且通过从该组中选择 T 名成员来组建团队。数字 G 和 T 永远不会改变，组中每个成员的身份也不会改变。唯一改变的是我们选择哪个 T。这是监督学习，所以我有一组训练数据，每个数据都包含一个特定的团队选择及其获得的分数。成本函数试图最大化所有游戏的总分。从质量上讲，我知道不仅仅是某些团队成员的得分始终优于其他成员，而且某些成员组合的表现也优于其他组合。
我如何构建模型和算法来解决这个问题？我考虑过为每个团队成员或组中的每个人（在团队中或不在团队中）进行独热编码，但这并不能正确捕获所有约束（例如，成员在团队中出现的次数不应超过一次）。]]></description>
      <guid>https://stackoverflow.com/questions/79087896/linear-regression-problem-with-discrete-variables-with-a-twist</guid>
      <pubDate>Mon, 14 Oct 2024 22:42:45 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程二元分类：为什么 GPy 的方差比 scikit-learn 小得多？</title>
      <link>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</link>
      <description><![CDATA[我正在学习使用高斯过程进行二元分类，并且正在将 GPy 与 scikit-learn 进行比较，解决一个受 Martin Krasser 的博客文章启发的玩具一维问题。两种实现（GPy 和 scikit-learn）似乎都使用带有 RBF 内核的类似设置。优化内核超参数后，长度尺度相似，但方差相差很大。GPy 内核方差似乎太小了。
我如何修改我的 GPy 实现并获得与 scikit-learn 类似的结果？我怀疑这与每个算法的内部实现有关，但我不知道是什么导致了这种巨大的差异。下面我将进一步解释为什么我认为我的 GPy 实现需要修复。
实现细节：Python 3.9 搭配 GPy 1.13.2 和 scikit-learn 1.5.1。可重现的示例：
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# 第 1 部分：玩具数据集创建
#################################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2 # 潜在函数
t = bernoulli.rvs(sigmoid(a)) # Bernoulli 训练数据（0 和1s)

#####################################
# 第 2 部分：scikit-learn 实现
#####################################

从 sklearn.gaussian_process 导入 GaussianProcessClassifier
从 sklearn.gaussian_process.kernels 导入 ConstantKernel，RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
* RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
kernel=rbf,
optimizer=&#39;fmin_l_bfgs_b&#39;,
n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# 第 3 部分：GPy 实现
############################

导入 GPy

kern = GPy.kern.RBF(
input_dim=1,
variance=1.,
lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
X=X,Y=t, kernel=kern, 
inference_method=GPy.inference.latent_function_inference.laplace.Laplace(), 
可能性=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
num_restarts=10, optimizer=&#39;lbfgs&#39;,
verbose=True, robust=True)

print(m.kern)
# rbf。| 值 | 约束 | 先验
# 方差 | 0.8067562453940487 | 0.001,10.0 | 
# lengthscale | 0.8365668826459536 | 0.001,10.0 |

lenghtscale 值大致相似 (0.858 vs 0.836)，但方差值非常不同 (scikit-learn 为 1.5**2 = 2.25，而 GPy 仅为 0.806)。
我认为我的 GPy 实现需要调整的原因是，即使有 +/- 2 个标准偏差界限，真正的潜在函数 (参见上面代码第 1 部分中的“a”) 也与预测函数不紧密匹配。另一方面，scikit-learn 实现与之相当匹配（可以使用 scikit-learn 检索潜在函数平均值和标准差如此处所示）。

 左：两个模型的预测概率相似（这是有道理的，因为它们共享相似的长度尺度值）。右：GPy 的预测潜在函数与真实潜在函数的拟合度不如 scikit-learn 模型。 
到目前为止，我尝试过的方法，结果没有显著变化：

输入特征 (X) 归一化
使用 GPy.inference.latent_function_inference.expectation_propagation.EP() 作为 GPy 推理方法，而不是拉普拉斯方法
按照此处的建议，将 WhiteKernel 组件添加到 scikit-learn 实现中&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</guid>
      <pubDate>Mon, 14 Oct 2024 13:10:16 GMT</pubDate>
    </item>
    <item>
      <title>我在 google colab 中工作，我的单元格中有访问地球数据的代码，它永远运行[关闭]</title>
      <link>https://stackoverflow.com/questions/79085474/im-working-in-google-colab-and-my-cell-that-has-code-to-access-data-from-earth</link>
      <description><![CDATA[我是一个相当新的程序员，并试图从地球数据中访问数据。这是文档https://disc.gsfc.nasa.gov/information/howto?keywords=python&amp;title=How%20to%20Access%20GES%20DISC%20Data%20Using%20Python。这是我的代码。
我尝试关闭 ds，创建一个预处理函数。我也尝试使用 colab ai 和 chatgpt 来帮助修复它，但没有任何效果。在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/79085474/im-working-in-google-colab-and-my-cell-that-has-code-to-access-data-from-earth</guid>
      <pubDate>Mon, 14 Oct 2024 09:04:35 GMT</pubDate>
    </item>
    <item>
      <title>summary() 函数在 cnn 中不起作用（ValueError：不支持未定义的形状。）</title>
      <link>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</link>
      <description><![CDATA[我正在尝试创建一个分类网络，用于识别来自 cifar10 数据集的图片。
当我尝试使用 summary() 函数时，我总是收到此错误。
ValueError Traceback (most recent call last)
Cell In[267], line 4
1 #base_model.summary()
2 #top_model.summary()
3 #print(base_model.output_shape)
----&gt; 4 model2.summary()

文件 c:\Users\noahc\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\noahc\anaconda3\Lib\site-packages\optree\ops.py:747，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
745 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
746 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 747 return treespec.unflatten(map(func, *flat_args))

ValueError：不支持未定义的形状。

代码如下...
import tensorflow as tf
from keras.applications import VGG16

base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3))

top_model = tf.keras.Sequential([
layer.Flatten(input_shape=base_model.output_shape[1:]),
layer.Dense(10,activation=&#39;softmax&#39;)
])

for layer in base_model.layers[:10]:
layer.trainable = False

model2 = tf.keras.models.Sequential([
base_model,
top_model
])

model2.summary() # 出现错误这里

我已经为基础模型和顶层模型做了总结，效果很好。但是当我测试 model2 时，出现了错误。不知道为什么。不确定“未定义”形状是什么意思。不知道还能尝试什么。当我只取 vgg16 的前 11 或 15 层时，总结就起作用了。我听说这可能是 python 版本本身的问题，但我不知道……]]></description>
      <guid>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</guid>
      <pubDate>Mon, 14 Oct 2024 05:50:22 GMT</pubDate>
    </item>
    <item>
      <title>来自 Google Drive 的 Langchain Dir 加载器抛出错误</title>
      <link>https://stackoverflow.com/questions/79084452/langchain-dir-loader-from-google-drive-throwing-errors</link>
      <description><![CDATA[我正在尝试从 google myDrive 加载 html 文件。我正在 colab 上尝试此操作。我能够加载驱动器，当我使用 DirectoryLoader 时，它会看到文件名，但它会引发以下错误。顺便说一句，我授予了任何人评论的权限（即使 colab 通过 google 帐户请求加载 myDrive 的权限）。错误很神秘，我找不到其他遇到过这种情况的人（当我在 google 上搜索时）。如有任何帮助，敬请谅解
os.chdir(&#39;/content/drive/MyDrive&#39;)

loader = DirectoryLoader(path = os.getcwd(), glob=&quot;datasets/opp/*.html&quot;, show_progress=True, use_multithreading=True, loader_cls = UnstructuredHTMLLoader)
documents = loader.load()

错误：
错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/745_eatchicken.com.html 时出错
63%|██████▎ | 72/115 [00:00&lt;00:00, 336.85it/s]错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/503_buffalowildwings.com.html 时出错
错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/807_lodgemfg.com.html 时出错
------------------------------------------------------------------------------------------
ModuleNotFoundError 回溯（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py in __init__(self, mode, post_processors, **unstructured_kwargs)
58 尝试：
---&gt; 59 import unstructured # noqa:F401
60 except ImportError:

ModuleNotFoundError: 没有名为“unstructured”的模块

在处理上述异常期间，发生了另一个异常：

ImportError Traceback（最近一次调用最后一次）
12 帧
/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py in __init__(self, mode, post_processors, **unstructured_kwargs)
59 import unstructured # noqa:F401
60 except ImportError:
---&gt; 61 raise ImportError(
62 &quot;未找到非结构化包，请使用&quot;
63 &quot;`pip install unstructured`&quot;

ImportError：未找到非结构化包，请使用`pip install unstructured`安装

-----------------------------------------------------------------------
注意：如果由于缺少包而导致导入失败，您可以
使用 !pip 或 !apt 手动安装依赖项。

要查看安装一些常见依赖项的示例，请单击下面的
&quot;打开示例&quot; 按钮。
----------------------------------------------------------------------------------------
]]></description>
      <guid>https://stackoverflow.com/questions/79084452/langchain-dir-loader-from-google-drive-throwing-errors</guid>
      <pubDate>Mon, 14 Oct 2024 00:41:50 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium 自定义环境“太多值无法解压”错误</title>
      <link>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</link>
      <description><![CDATA[我正在尝试使用具有体育馆和稳定基线的自定义群体聚集环境。我有一个自定义策略和训练循环。
我的行动和观察空间如下：
min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)

min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)

训练代码：
import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv

if os.path.exists(Results[&quot;Rewards&quot;]):
os.remove(Results[&quot;Rewards&quot;])
print(f&quot;File {Results[&#39;Rewards&#39;]} has been removed.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
os.remove(&quot;training_rewards.json&quot;)
print(f&quot;文件 training_rewards 已被删除。&quot;) 

def seed_everything(seed):
np.random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
th.manual_seed(seed)
th.cuda.manual_seed(seed)
th.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)

loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # 模型训练
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# 训练模型
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])

错误：
使用 cuda 设备
回溯（最近一次调用最后一次）：
文件 &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;，行45，在&lt;module&gt;中
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback]) 
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;，第 287 行，在 learn 中
total_timesteps, callback = self._setup_learn(
^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;，第 423 行，在 _setup_learn
self._last_obs = self.env.reset() # 类型：ignore[assignment]
^^^^^^^^^^^^^^^^^
文件 &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;，第 77 行，在 reset
obs 中，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：太需要解压的值很多（预计为 2 个）

我也在 gym 中使用了类似的种子函数，但没有出现错误，我以为是它导致了错误，但即使我不使用它，错误也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 13 Oct 2024 22:45:48 GMT</pubDate>
    </item>
    <item>
      <title>DETR（检测变压器）模型不适用于我的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79084295/detr-detection-transformer-model-is-not-working-for-my-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79084295/detr-detection-transformer-model-is-not-working-for-my-dataset</guid>
      <pubDate>Sun, 13 Oct 2024 22:27:50 GMT</pubDate>
    </item>
    <item>
      <title>具有高精度（95%）/低验证率（0%）的面部识别计算机视觉图像分类模型</title>
      <link>https://stackoverflow.com/questions/79073365/facial-recognition-computer-vision-image-classification-model-with-high-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79073365/facial-recognition-computer-vision-image-classification-model-with-high-accuracy</guid>
      <pubDate>Thu, 10 Oct 2024 08:13:20 GMT</pubDate>
    </item>
    <item>
      <title>错误：具有 ML Compute 加速功能的 TensorFlow 仅适用于 macOS 11.0 及更高版本</title>
      <link>https://stackoverflow.com/questions/74563984/error-tensorflow-with-ml-compute-acceleration-is-only-available-on-macos-11-0-a</link>
      <description><![CDATA[下载 Tensor Flow mac 发布包 (https://github.com/apple/tensorflow_macos/releases)，打开 tensorflow_macos-0.1alpha3.tar.gz 包，运行“install_venv.sh”脚本，出现以下错误 ERROR: 带有 ML Compute 加速的 TensorFlow 仅在 macOS 11.0 及更高版本上可用。
我在 conda 环境中使用 python 3.8 运行它（我尝试过 3.7-3.11，结果相同）。
我会在 apple github repo 上发布，但它已被公开存档，所以假设它不再可用？
我尝试过在 conda 环境中使用 python 3.8 运行它（我尝试过 3.7-3.11，结果相同）。
我预计安装会成功。我尝试安装 pip install tensorflow（即没有特定于 mac），pyCharm 看到了 lib tensorflow，但我得到
Apple M1：进程完成，退出代码为 132（被信号 4 中断：sigill，解决方案是从上面安装包：https://github.com/apple/tensorflow_macos/issues/270]]></description>
      <guid>https://stackoverflow.com/questions/74563984/error-tensorflow-with-ml-compute-acceleration-is-only-available-on-macos-11-0-a</guid>
      <pubDate>Thu, 24 Nov 2022 16:46:17 GMT</pubDate>
    </item>
    <item>
      <title>nn.Parameter 上的 Torch 操作</title>
      <link>https://stackoverflow.com/questions/71490306/torch-operation-on-nn-parameter</link>
      <description><![CDATA[我有一个参数列表，我想对列表中的所有元素求和
import torch
from torch import nn

a = nn.Parameter(torch.rand(1))
b = nn.Parameter(torch.rand(1))

my_list = [a, b]
torch.sum(*my_list)

我收到错误

回溯（最近一次调用最后一次）：
文件“&lt;input&gt;&gt;”，第 8 行，在&lt;module&gt;
TypeError: sum() 收到的参数组合无效 - 得到 (Parameter, Parameter)，但预期为以下之一：
* (Tensor 输入，*，torch.dtype dtype)
* (Tensor 输入，整数元组 dim，bool keepdim，*，torch.dtype dtype，Tensor out)
* (Tensor 输入，名称元组 dim，bool keepdim，*，torch.dtype dtype，Tensor out)


我想知道是否有办法对 Parameter 执行 torch.sum 之类的操作？]]></description>
      <guid>https://stackoverflow.com/questions/71490306/torch-operation-on-nn-parameter</guid>
      <pubDate>Tue, 15 Mar 2022 23:52:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中有效地实现非全连接线性层？</title>
      <link>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</link>
      <description><![CDATA[我制作了一个示例图，展示了我试图实现的缩小版本：

因此，顶部两个输入节点仅完全连接到顶部三个输出节点，并且相同的设计适用于底部两个节点。到目前为止，我已经想出了两种在 PyTorch 中实现此目的的方法，但都不是最佳方法。
第一种方法是创建一个包含许多较小线性层的 nn.ModuleList，并在前向传递期间，通过它们迭代输入。对于图表的示例，它看起来像这样：
class Module(nn.Module):
def __init__(self):
self.layers = nn.Module([nn.Linear(2, 3) for i in range(2)])

def forward(self, input):
output = torch.zeros(2, 3)
for i in range(2):
output[i, :] = self.layers[i](input.view(2, 2)[i, :])
return output.flatten()

因此，这完成了图中的网络，主要问题是它非常慢。我认为这是因为 PyTorch 必须按顺序处理 for 循环，而不能并行处理输入张量。
要“矢量化”模块以便 PyTorch 可以更快地运行它，我有这个实现：
class Module(nn.Module):
def __init__(self):
self.layer = nn.Linear(4, 6)
self.mask = # 创建 1 和 0 的掩码来“阻止”某些层连接

def forward(self, input):
prune.custom_from_mask(self.layer, name=&#39;weight&#39;, mask=self.mask)
return self.layer(input)

这也完成了图表的网络，通过使用权重修剪来确保完全连接层中的某些权重始终为零（例如，连接顶部输入节点和底部输出节点的权重将始终为零，因此它实际上是“断开连接的”）。这个模块比上一个快得多，因为没有 for 循环。现在的问题是这个模块占用了更多的内存。这可能是因为，即使大多数层的权重为零，PyTorch 仍会将网络视为它们存在。此实现本质上保留了比需要更多的权重。
有人遇到过这个问题并想出了有效的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 03:41:06 GMT</pubDate>
    </item>
    </channel>
</rss>