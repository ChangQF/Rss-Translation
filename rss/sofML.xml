<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 19 Mar 2025 01:21:05 GMT</lastBuildDate>
    <item>
      <title>为什么我的高准确性MNIST分类器在React.js中的用户画数字上失败？</title>
      <link>https://stackoverflow.com/questions/79518843/why-does-my-high-accuracy-mnist-classifier-fail-on-user-drawn-digits-in-react-js</link>
      <description><![CDATA[我在MNIST数据集上训练了一个简单的二进制分类器，在测试数据上达到了99％+的精度。但是，当我将其集成到React.js应用中时，用户在画布上画数字时，该模型完全无法识别它们。我有这些方法的敏感性类别
  //步骤功能（Heaviside功能）
  //它无法学习诸如XOR之类的模式，XOR需要多个层。仅线性可分离数据
  //由于它不是可区分的，因此不能使用诸如propogation之类的梯度基准方法进行优化
  stepfunction（value）{
    返回值＆gt; = 0？ 1：0;
  }

  preditiveLabel（功能）{
    令总计= this.biasterm;
    for（让i = 0; i＆lt; trauty.length; i ++）{
      总计 +=功能[i] * this.weightparams [i];
    }
    返回this.Stepfunction（Total）;
  }

  TrainModel（训练样本，TargetLabels）{
    for（让i = 0; i＆lt; trainingsamples.length; i ++）{
      令SampleInput =训练样本[i];
      const predict predictOutput = this.predictLabel（sampleInput）;
      const actuallabel = targetLabels [i];

      if（Actuallabel！== predictationOutput）{
        for（令J = 0; J＆lt; this.weightparams.length; J ++）{
          this.WeightParams [J] +=
            this.learningrate *
            （Actuallabel-预测输出） *
            样品input [j];
        }
        this.biassterm += this.learningrate *（actuallabel -predictationOutput）;
      }
    }
  }
 
这在测试数据上达到了99％+精度。但是它在用户绘制的图像上表现不佳。

 我将火车输入和测试输入标准化
 // traininputs和测试输入
功能预处理图（Pixelarray）{
返回pixelarray.map（（（row）=＆gt; row.map（（（pixel）=＆gt; pixel / 255.0）））;
} 

  i除去低强度像素以滤波噪声。
  const processedimages = testimages.data.map（（（image）=＆gt;
     //我用此变量noings_removal_threshold付款
     image.map（（（row）=＆gt;
       row.map（（（pixel）=＆gt;（pixel＆gt; = noyes_removal_threshord？pixel：0））
     ）
   ）；
 


我知道MNIST数字是居中，薄且定义明确的，因此我相应地绘制数字。
我的MNIST模型在训练过程中达到了很高的精度，但是当用户在画布上写数字时，它无法很好地预测。导致此差距的关键因素是什么，我如何改善现实世界的性能？]]></description>
      <guid>https://stackoverflow.com/questions/79518843/why-does-my-high-accuracy-mnist-classifier-fail-on-user-drawn-digits-in-react-js</guid>
      <pubDate>Wed, 19 Mar 2025 00:04:15 GMT</pubDate>
    </item>
    <item>
      <title>寻找具有REST API集成的AI或机器学习工具[已关闭]</title>
      <link>https://stackoverflow.com/questions/79517734/looking-for-ai-or-machine-learning-tools-with-rest-api-integration</link>
      <description><![CDATA[我正在研究可以通过REST API集成到我的React.js应用程序中的工具或AI解决方案。具体来说，我正在寻找可以：的解决方案

协助测验创建和分级（从提示中生成测验或使用现有问题，具有添加新的问题）。
分析和等级用户提取的文本/期刊，提供简短的反馈和可能的评分。

您有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79517734/looking-for-ai-or-machine-learning-tools-with-rest-api-integration</guid>
      <pubDate>Tue, 18 Mar 2025 14:14:17 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中的CNN模型时，损失不会减少[封闭]</title>
      <link>https://stackoverflow.com/questions/79517689/loss-is-not-reducing-when-traiing-cnn-model-in-pytorch</link>
      <description><![CDATA[我正在使用带有3个类的数据集培训CNN模型（我同时尝试使用自定义CNN和VGG19模型）。从一个时期开始，损失几乎保持恒定。我将在下面附上我的Colab链接以供您参考：
我为第一个时期的训练损失看起来像这样：

 epoch：0 |火车损失：1.4142，火车准确性：0.3319 |测试损失：
1.1003，测试精度：0.3333 
时代：1 |火车损失：1.1002，火车准确性：0.3369 |测试损失：
1.1008，测试准确性：0.3333 
时代：2 |火车损失：1.1006，火车准确性：0.3344 |测试损失：
1.0991，测试精度：0.3333 
时代：3 |火车损失：1.1006，火车准确性：0.3313 |测试损失：
1.0990，测试精度：0.3333 

我已经尝试了更长的时期的相同方法，但不会收敛。因此，对于50个时期，损失和准确性也保持不变。
即使损失减少，也很慢。
我在不同数据集上使用了相同的模型，因此，对于100个时期，损失和准确性也保持不变。]]></description>
      <guid>https://stackoverflow.com/questions/79517689/loss-is-not-reducing-when-traiing-cnn-model-in-pytorch</guid>
      <pubDate>Tue, 18 Mar 2025 13:58:38 GMT</pubDate>
    </item>
    <item>
      <title>当我使用Torch.Distribed.Pipelining训练模型时，为什么损失不减少？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79516152/why-doesnt-the-loss-decrease-when-i-use-torch-distributed-pipelining-to-train-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79516152/why-doesnt-the-loss-decrease-when-i-use-torch-distributed-pipelining-to-train-t</guid>
      <pubDate>Tue, 18 Mar 2025 02:47:07 GMT</pubDate>
    </item>
    <item>
      <title>如何评估由于缩小任务而具有不同大小不同的2个图像[封闭]</title>
      <link>https://stackoverflow.com/questions/79515984/how-to-evaluate-2-images-with-different-size-due-to-downscaling-task</link>
      <description><![CDATA[我有一个NetCDF4文件包含原始卫星数据。我执行了3种不同类型的算法来降低IT（卷积，带有双色的高斯，平均面积）。现在，我想找到一种方法来评估“良好”的方式。算法执行。请注意，原始尺寸和降尺度不同（1472x1069x1069 vs 1472x528x528）。
有办法做到这一点吗？我已经阅读了SSIM，PSNR，甚至对于MS-SSIM，VIF和LPIP，我都注意到它们都需要形状相似性。我也不想调整图像大小以保持质量。]]></description>
      <guid>https://stackoverflow.com/questions/79515984/how-to-evaluate-2-images-with-different-size-due-to-downscaling-task</guid>
      <pubDate>Mon, 17 Mar 2025 23:50:58 GMT</pubDate>
    </item>
    <item>
      <title>特征的lenght不等于形状值的长度</title>
      <link>https://stackoverflow.com/questions/79515542/lenght-of-features-is-not-equal-to-the-length-of-shap-values</link>
      <description><![CDATA[我正在运行一个随机的森林模型，并获得某些特征的重要性，并试图运行形状分析。问题是，每当我尝试绘制塑造值时，我都会遇到此错误：
  dimensionError：功能的长度不等于shap_values的长度。 
 
我不知道发生了什么。当我运行XGBoost模型时，一切似乎都很好，我可以看到数据集的形状图。它的数据集完全相同，但它不会与随机森林一起运行。它用于二进制分类。
这是我的python代码：
 来自sklearn.semble import incort fandyForestClassifier
来自sklearn.model_selection导入train_test_split，cross_val_score
来自sklearn.metrics导入精度，precision_score，recke_score，f1_score，confusion_matrix

＃从功能中删除主键列“ ID”

功能= result.drop（columns = [&#39;pq2&#39;，&#39;id&#39;]）＃删除目标和ID列
target =结果[&#39;pq2&#39;]＃目标变量

＃将数据分为80-20的培训和测试集
x_train，x_test，y_train，y_test = train_test_split（功能，target，test_size = 0.2，andural_state = 42）
 
＃初始化随机森林分类器
rf_model = RandomforestClassifier（N_Estimators = 100，Random_State = 42）

＃将模型适合培训数据
rf_model.fit（x_train，y_train）

＃做出预测
y_pred = rf_model.predict（x_test）

导入塑造

＃为随机森林模型创建树状解释器
解释器= shap.treeexplainer（rf_model）

＃计算测试集的形状值
shap_values = ruminder.shap_values（x_test）

＃绘制形状摘要图
shap.summary_plot（shap_values，x_test，feature_names = features_names）

＃绘制整体特征重要性的塑形栏图

shap.summary_plot（shap_values，x_test，feature_names = features_names，plot_type =; bar＆quot; quot;
 
测试集的形状为（829,22），但是随机森林的外形值始终返回（22,2），我不知道如何修复它。数据集已经进行了预处理，列是0-1S或数值列。]]></description>
      <guid>https://stackoverflow.com/questions/79515542/lenght-of-features-is-not-equal-to-the-length-of-shap-values</guid>
      <pubDate>Mon, 17 Mar 2025 19:16:45 GMT</pubDate>
    </item>
    <item>
      <title>无法使KERAS CV对象检测器（Yolov8）和超级分析</title>
      <link>https://stackoverflow.com/questions/79515278/unable-to-get-keras-cv-object-detector-yolov8-to-perform-as-well-as-ultralytic</link>
      <description><![CDATA[我遵循已发表的Keras CV计算机视觉教程（ https://keras.io/examples/examples/vision/vision/yolov8/yolov8/yolov8/ yolo v8我希望得到一些帮助的问题。
问题是，使用KERAS CV中的Yolo V8检测器，地图得分非常低（1-10％）。  在训练和推断期间，我看到非最大抑制作用似乎无法正常工作。  我看到，具有相同置信度得分的同一对象有几个重叠的边界框。  更改NMS设置并不能改善这一点。
为了验证我的数据集，我尝试使用Roboflow和Ultrytics库使用我的数据集微调模型，并能够获得约71％-91.4％的地图。数据集大小为586，每个图像的每个类别为4个类别（故意避免类不平衡问题）。
这是我在训练和调整NMS后如何运行推理的一个示例：
在
    bounding_box_format = bounding_box_format，
    from_logits = true，＃或false
    iou_threshold = 0.5，＃尝试在此处调整所有内容 
    信任_threshold = 0.5，
    max_detections = 10
）
 
我在训练期间（以及其他）使用pycococalback：
  callbacks = [
        keras_cv.callbacks.pycococallback（＃可可指标（ap/ar @ iou）
            val_ds，
            bounding_box_format = bounding_box_format，
        ），
        keras.callbacks.tensorboard（＃培训进度可视化
            log_dir =＆quot; triending_logs＆quot
        ），
        keras.callbacks.modelcheckpoint（
            filepath =＆quot ;/ model/best_model.keras＆quot;
            Monitor =; val_ap＆quot;
            mode =“最大＃”，＃保存地图最大化时
            save_best_only = true，＃仅保留最佳模型
            详细= 1，
        ），
        keras.callbacks.earlystopping（
            耐心= 20，＃在25个时代不改进后停止
            Monitor =; val_ap＆quot;
            mode =“最大监视器最大地图”
            详细= 1，
            Restore_best_weights = true，＃恢复最佳时期的重量
        ），
        ＃visualizedEtections（），＃在每个时期之后的视觉验证
    ]，，
 
我如何训练模型的示例：
  backbone = keras_cv.models.yolov8backbone.from_preset（
    ＆quot&#39;yolo_v8_xs_backbone_coco＆quot＆quort load_weights = true
）

prediction_decoder = keras_cv.layers.nonmaxsuppression（
    bounding_box_format = bounding_box_format，
    from_logits = false，
    iou_threshold = 0.5，
    信心_threshold = 0.6，
    max_detections = 200
）

模型= keras_cv.models.yolov8detector（
    num_classes = len（class_ids），
    bounding_box_format = bounding_box_format，
    骨干=骨干，
    fpn_depth = 2，
    prediction_decoder = prediction_decoder，
）

initial_learning_rate = 0.001
lr_schedule = keras.optimizers.schedules.cosinedecay（
    initial_learning_rate，
    decay_steps = 1000，
    alpha = 0.0
）
优化器= keras.optimizers.adam（Learning_rate = lr_schedule，global_clipnorm = 10.0）

model.compile（
    classification_loss = keras.losses.categoricalcrossentropy（），
    box_loss = keras_cv.losses.ciouloss（bounding_box_format = bounding_box_format），
    优化器=优化器，
）
 
我尝试过：

检查以确保边界框和标签正确。
使用不同的优化器，类/损失功能等。
有和没有增强。
通过Roboflow，Keras/Tensorflow或两者使用增强。
带有和没有预训练重量的不同骨干。
不同的NMS设置（置信度，来自_logits和iou）。

我期望KERAS CV中的Yolov8检测器的性能能够在某种程度上执行超级实施。  由于我的增强管道可能不像超级词那样稳健，但这似乎是一个巨大的差距。
我无法弄清楚我似乎被忽略了！如何缩小这个巨大的差距？]]></description>
      <guid>https://stackoverflow.com/questions/79515278/unable-to-get-keras-cv-object-detector-yolov8-to-perform-as-well-as-ultralytic</guid>
      <pubDate>Mon, 17 Mar 2025 17:04:10 GMT</pubDate>
    </item>
    <item>
      <title>连接到Zenml仪表板</title>
      <link>https://stackoverflow.com/questions/79514826/connecting-to-zenml-dashboard</link>
      <description><![CDATA[通过运行 zenml login -local 。
我的Zenml版本是0.75.0。我在Windows机器上。

错误：作为背景过程不是本地运行zenml服务器
在Windows上支持。请使用“阻止”。标志以运行
处于阻塞模式的服务器，或通过Docker容器运行服务器
设置“  -  docker”而是。

然后，我进行了 zenml登录-local -blocking ，但获得了以下错误：

 ModulenotFoundError：处理过程中没有名为“ JWT”的模块
最重要的是，发生了另一个例外：RuntimeError：本地
守护程序Zenml Server提供商不可用，因为Zenml Server
在您的机器上似乎不可用。这可能是
因为ZenML是在没有可选Zenml Server的情况下安装的
依赖性。要安装丢失的依赖项运行 pip install; zenml [server] == 0.75.0; 。

安装 pip install; zenml [server] == 0.75.0;  之后保留错误。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79514826/connecting-to-zenml-dashboard</guid>
      <pubDate>Mon, 17 Mar 2025 14:09:13 GMT</pubDate>
    </item>
    <item>
      <title>尝试从补丁重建原始图像</title>
      <link>https://stackoverflow.com/questions/79514560/trying-to-reconstruct-original-image-from-patches</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79514560/trying-to-reconstruct-original-image-from-patches</guid>
      <pubDate>Mon, 17 Mar 2025 12:19:35 GMT</pubDate>
    </item>
    <item>
      <title>解构稳定扩散3.5管道</title>
      <link>https://stackoverflow.com/questions/79494728/deconstructiong-the-stable-diffusion-3-5-pipeline</link>
      <description><![CDATA[我正在尝试解构SD3.5（特别是3.5个介质）管道，以便在降压步骤上具有受控的过程。我无法进行回调，因为我需要根据其他管道修改潜在。
我正在尝试在以下拥抱面指南上执行步骤：
 https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline_pipeline#deconstruct-the-stable-diffusion-diffusion-pipeline  
我修改了文本编码以适合SD3.5，我还尝试加载整个管道，并在其上运行Encode_prompt函数，以获取文本嵌入和汇总的嵌入，以适应提示和负个提示。当运行该功能并将其输出作为常规管道输入而不是提示提示时，它可以正常工作，因此这似乎不是引起问题的原因。
我还将UNET从文章中更改为使用模型的预训练变压器。之后，我调整了解码器以匹配扩散器上管道源代码上的相同解码。
输出图像在通过扩散器运行管道时看起来并不相同。我不确定在哪里可以找到与SD3管道解构类似的实现，或者我缺少什么。
 在此处输入图像说明  ]]></description>
      <guid>https://stackoverflow.com/questions/79494728/deconstructiong-the-stable-diffusion-3-5-pipeline</guid>
      <pubDate>Sat, 08 Mar 2025 17:06:18 GMT</pubDate>
    </item>
    <item>
      <title>Numpy步步技巧：是否可以将窗口添加到同一位置的原始数组大小中，而无需循环？</title>
      <link>https://stackoverflow.com/questions/79395423/numpy-stride-tricks-is-it-possible-to-add-the-windows-back-into-the-original-ar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79395423/numpy-stride-tricks-is-it-possible-to-add-the-windows-back-into-the-original-ar</guid>
      <pubDate>Tue, 28 Jan 2025 23:28:50 GMT</pubDate>
    </item>
    <item>
      <title>python中的更快的KNN分类算法</title>
      <link>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</link>
      <description><![CDATA[我想从头开始编码自己的KNN算法，原因是我需要加重功能。问题是，尽管删除了循环并使用内置的Numpy功能，但我的程序仍然非常慢。
谁能提出一种加快速度的方法？我不为L2距离使用 np.sqrt ，因为它是不必要的，并且实际上会大大减慢。
 类GlobalWeightedKnn：
    ”“”
    具有功能重量的K-NN分类器

    回报：K-NN的预测。
    ”“”

    def __init __（自我）：
        self.x_train =无
        self.y_train =无
        self.k =无
        self.pewights =无
        self.predictions = list（）

    def fit（self，x_train，y_train，k，weights）：        
        self.x_train = x_train
        self.y_train = y_train
        self.k = k
        self.weights =权重

    def预测（self，testing_data）：
        ”“”
        采用2D查询案例。

        返回K-NN分类器的预测列表
        ”“”

        np.fromiter（（（self .__ helper（QC））用于testing_data中的QC），float）  
        返回自我预测


    def __helper（Self，QC）：
        邻居= np.fromiter（（（self .__加权_euclidean（qc，x）for x self.x_train中的x），float），float）
        邻居= np.array（[[邻居]） 
        indexes = np.array（[range（len（self.x_train）））]
        邻居= np.append（索引，邻居，轴= 1）

        ＃排序第二列 - 距离
        邻居=邻居[邻居[：1] .argsort（）]  
        k_cases =邻居[：self.k]
        indexes = [x [0]对于k_case中的x]

        y__answers = [self.y_train [int（x）for Indexes中的x]
        答案= max（set（y_answers），key = y_answers.count）＃获取最常见的值
        self.predictions.append（答案）


    def __weighted_euclidean（self，QC，其他）：
        ”“”
        定制加权欧几里得距离

        返回：浮点号
        ”“”

        返回np.sum（（（（（QC-其他）** 2）*自我）
 ]]></description>
      <guid>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</guid>
      <pubDate>Sat, 04 Aug 2018 18:39:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么仅在CNN中仅通过通道进行批次归一化</title>
      <link>https://stackoverflow.com/questions/45799926/why-batch-normalization-over-channels-only-in-cnn</link>
      <description><![CDATA[我想知道，在卷积神经网络中，是否应分别相对于每个像素，还是应该按照每个通道进行像素的平均值？
I saw that in the description of Tensorflow&#39;s tf.layers.batch_normalization it is suggested to perform bn with respect to the channels, but if I recall correctly, I have used 的另一种方法。]]></description>
      <guid>https://stackoverflow.com/questions/45799926/why-batch-normalization-over-channels-only-in-cnn</guid>
      <pubDate>Mon, 21 Aug 2017 14:40:57 GMT</pubDate>
    </item>
    <item>
      <title>如何找到功能对逻辑回归模型的重要性？</title>
      <link>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</link>
      <description><![CDATA[我有一个由逻辑回归算法训练的二进制预测模型。我想知道哪些功能（预测指标）对于正面或负面类别的决策更为重要。我知道有来自Scikit-Learn软件包的 COEF _ 参数，但我不知道它是否足以满足重要性。另一件事是我如何根据否定和正类别的重要性来评估 coef _ 值。我还阅读了有关标准化回归系数的信息，但我不知道它是什么。
可以说，有肿瘤大小，肿瘤重量等特征，可以决定恶性肿瘤或不恶性等测试案例。我想知道哪些功能对于恶性和不是恶性预测更为重要。]]></description>
      <guid>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</guid>
      <pubDate>Wed, 02 Dec 2015 20:11:21 GMT</pubDate>
    </item>
    <item>
      <title>当尝试使用SVC进行概率预测时，为什么我会得到一个noctemplementederror？</title>
      <link>https://stackoverflow.com/questions/19878285/why-do-i-get-a-notimplementederror-when-trying-to-use-svc-for-probability-predic</link>
      <description><![CDATA[我正在尝试使用Sklearn的SVC来解决分类问题。给出了一堆数据，并告诉我某些学科是否在某个类中，我希望能够给出一个新的，未知的主题是在类中。
我只有2个类，所以问题是二进制的。这是我的代码和我的一些错误
 来自Sklearn.svm导入SVC
clf = svc（）

clf = clf.fit（x，y）


SVC（概率= true）
打印clf.predict_proba（w）#Error在这里
 
，但它返回以下错误：

 notimplemplementError：必须启用概率估计来使用此方法

我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/19878285/why-do-i-get-a-notimplementederror-when-trying-to-use-svc-for-probability-predic</guid>
      <pubDate>Sat, 09 Nov 2013 16:30:58 GMT</pubDate>
    </item>
    </channel>
</rss>