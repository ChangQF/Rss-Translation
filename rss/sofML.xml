<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Mar 2024 03:13:52 GMT</lastBuildDate>
    <item>
      <title>CCA 相关性分析显示所有 1</title>
      <link>https://stackoverflow.com/questions/78212742/cca-correlation-analysis-showing-all-1</link>
      <description><![CDATA[当我进行相关性规范分析时，我得到的输出全部等于 1，这正常吗？
#install.package(“CCA”)

setwd &lt;-“~/下载”

# 加载CCA包
图书馆（CCA）
matrice_goals&lt;- as.data.frame(matrice_goals)
Ind_demo-std &lt;- as.data.frame(Ind_demo_std)

# 计算相关矩阵
risultati &lt;- cancor(matrice_goals, Ind_demo_std)

打印（结果$cor）

print(risultati$cor)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

我不明白问题是代码还是数据，即使我已经标准化了。]]></description>
      <guid>https://stackoverflow.com/questions/78212742/cca-correlation-analysis-showing-all-1</guid>
      <pubDate>Sat, 23 Mar 2024 22:32:24 GMT</pubDate>
    </item>
    <item>
      <title>错误消息['OneHotEncoder'对象没有属性'_drop_idx_after_grouping']</title>
      <link>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</guid>
      <pubDate>Sat, 23 Mar 2024 21:35:23 GMT</pubDate>
    </item>
    <item>
      <title>统一构建 APK [关闭]</title>
      <link>https://stackoverflow.com/questions/78212208/building-apk-in-unity</link>
      <description><![CDATA[我做了一个统一项目，它采用机器学习模型来预测图像的类别。它在统一中完美运行。但是当我构建 apk 文件并在我的手机中运行它时。没有什么是可见的。即预测是不可见的。为什么 ？我直接在unity中集成了keras模型，是不是因为我的手机没有安装python？
Android 手机上的结果Unity 上的结果
当我提取 apk 时，我没有在 apk 中看到我的模型或其他详细信息。怎么解决这个问题。如何在我的 apk 中添加 Unity 中使用的所有文件夹和文件]]></description>
      <guid>https://stackoverflow.com/questions/78212208/building-apk-in-unity</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:12 GMT</pubDate>
    </item>
    <item>
      <title>深度学习模型训练精度高，但在二进制文本分类中的测试数据上表现不佳[关闭]</title>
      <link>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</link>
      <description><![CDATA[我在处理二进制文本分类任务时遇到了一个令人困惑的问题。尽管尝试了多种深度学习模型，包括各种架构和超参数，但我始终观察到很高的训练准确度，通常在 97% 到 99% 之间。然而，当我根据看不见的测试数据评估这些模型时，它们的性能显着恶化。
为了解决这个问题，我决定探索机器学习模型作为替代方法。令人惊讶的是，随机森林等模型的性能与深度学习模型相当甚至更好，训练和测试数据的准确率均达到 97% 左右。随后，我尝试了其他几种机器学习算法，逻辑回归成为最适合我的特定用例的选择。
尽管有这些发现，我仍然感到困惑，为什么深度学习模型尽管表现出令人印象深刻的训练准确性，却无法很好地泛化到未见过的数据。有人可以阐明这种差异背后的潜在原因吗？是否存在我可能忽略的深度学习特有的常见陷阱或注意事项？任何见解或建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</guid>
      <pubDate>Sat, 23 Mar 2024 19:05:28 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助启动去中心化联合学习+区块链集成项目的模拟[已关闭]</title>
      <link>https://stackoverflow.com/questions/78212019/need-help-starting-simulation-for-decentralized-federated-learning-blockchain</link>
      <description><![CDATA[我正在开发一个项目，该项目涉及在集成区块链技术之前模拟去中心化联合学习 (DFL) 模型。我正在寻找有关如何开始模拟部分的指导。具体来说，我需要有关如何模拟 DFL 的想法、我可以使用的任何现有代码库或库，以及有关如何进行模拟过程的提示，我计划开始使用 python。我是这个领域的新手，因此任何帮助或建议将不胜感激。提前致谢！
我希望找到有关如何创建模仿 DFL 模型行为的模拟的指导。具体来说，我想模拟边缘设备之间的交互、数据分布、模型训练和聚合过程。理想情况下，我想在将区块链技术集成到模拟中之前控制各种参数来测试不同的场景。]]></description>
      <guid>https://stackoverflow.com/questions/78212019/need-help-starting-simulation-for-decentralized-federated-learning-blockchain</guid>
      <pubDate>Sat, 23 Mar 2024 18:41:24 GMT</pubDate>
    </item>
    <item>
      <title>多元多步时间序列预测问题</title>
      <link>https://stackoverflow.com/questions/78211600/a-multivariate-multi-step-time-series-prediction-problem</link>
      <description><![CDATA[我有一个多元多步时间序列预测问题，其中输入 waterA、waterB、waterC、medicineA、medicineB，并输出浊度。
在此处输入图片说明
其中，药物A和药物B是可控的，而三类水是不可控的。
我使用 LSTM 模型使用前 15 个数据点来预测接下来的 15 个数据点（假设 15 个数据点代表 1 小时）。以下是测试集上的一些结果。
在此处输入图片描述
在此处输入图片描述
如果我根据当前时刻过去的数据预测下一小时的浊度。但此刻我打算改变用药量，并预测未来一个多小时的浊度（主要是改变用药对未来浊度的影响），但我不知道三种水的未来值。我曾经尝试使用三个 LSTM 来预测三种类型的未来水，而不是未来的水输入，但这会导致更糟糕的预测结果。还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78211600/a-multivariate-multi-step-time-series-prediction-problem</guid>
      <pubDate>Sat, 23 Mar 2024 16:27:56 GMT</pubDate>
    </item>
    <item>
      <title>无法从“jax”导入名称“linear_util”</title>
      <link>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</link>
      <description><![CDATA[我正在尝试重现S5模型的实验，https://github.com/lindermanlab/ S5，但是在解决环境的时候遇到了一些问题。当我运行 shell 脚本./run_lra_cifar.sh时，出现以下错误
回溯（最近一次调用最后一次）：
  文件“/Path/S5/run_train.py”，第3行，在&lt;module&gt;中。
    从 s5.train 导入火车
  文件“/Path/S5/s5/train.py”，第7行，在&lt;module&gt;中。
    从.train_helpers导入create_train_state，reduce_lr_on_plateau，\
  文件“/Path/train_helpers.py”，第 6 行，在  中。
    从 flax.training 导入 train_state
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py”，第 19 行，在  中
    从 。导入核心
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py”，第 15 行，在  中
    从 .axes_scan 导入广播
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py”，第 22 行，在  中
    从 jax 导入 Linear_util 作为 lu
ImportError：无法从“jax”导入名称“linear_util”（/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py）

我在 RTX4090 上运行它，我的 CUDA 版本是 11.8。我的jax版本是0.4.25，jaxlib版本是0.4.25+cuda11.cudnn86
我首先尝试使用作者的安装依赖项
pip install -rrequirements_gpu.txt

但是，这似乎不适用于我的情况，因为我什至无法导入 jax。所以我根据 https://jax.readthedocs.io/en 上的说明安装了 jax /latest/installation.html
通过输入
pip install --upgrade pip
pip install --upgrade “jax[cuda11_pip]” -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

到目前为止我已经尝试过：

使用较旧的 GPU（3060 和 2070）
将 python 降级到 3.9

有谁知道可能出了什么问题吗？感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</guid>
      <pubDate>Sat, 23 Mar 2024 09:57:12 GMT</pubDate>
    </item>
    <item>
      <title>如何将预训练的拥抱脸模型转换为.pt并在本地完全运行？</title>
      <link>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</link>
      <description><![CDATA[我正在尝试将此模型转换为.pt格式。它对我来说工作得很好，所以我不想对其进行微调。如何将其导出为.pt并运行界面？
我尝试使用它转换为 .pt：
从变压器导入 AutoConfig、AutoProcessor、AutoModelForCTC、AutoTokenizer、Wav2Vec2Processor
导入库
进口火炬



# 定义模型名称
model_name = “UrukHan/wav2vec2-俄罗斯”

# 加载模型和分词器
config = AutoConfig.from_pretrained(model_name)
模型 = AutoModelForCTC.from_pretrained(model_name, config=config)
处理器 = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 将模型保存为.pt 文件
torch.save(model.state_dict(), &quot;model.pt&quot;)

# 如果需要的话也保存分词器
tokenizer.save_pretrained(“模型标记器”)

但不幸的是它没有运行界面：
model = AutoModelForCTC.from_pretrained(“model.pt”)
处理器 = AutoProcessor.from_pretrained(“model.pt”)


# 使用模型进行推理
FILE = &#39;这里是 wav.wav&#39;
音频，_ = librosa.load（文件，sr = 16000）
音频=列表（音频）
def map_to_result(batch):
  使用 torch.no_grad()：
    input_values = torch.tensor(batch, device=“cpu”).unsqueeze(0) #, device=“cuda”
    logits = 模型(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  批处理=处理器.batch_decode(pred_ids)[0]
  退货批次
映射到结果（音频）
打印（映射到结果（音频））


模型.eval()

并遇到错误：
`model.pt 不是本地文件夹，也不是“https://huggingface.co/models”上列出的有效模型标识符
`]]></description>
      <guid>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</guid>
      <pubDate>Sat, 23 Mar 2024 09:18:49 GMT</pubDate>
    </item>
    <item>
      <title>LLaMA2 工作负载跟踪</title>
      <link>https://stackoverflow.com/questions/78208827/llama2-workload-traces</link>
      <description><![CDATA[是否有任何数据集可以捕获 LLaMA2 模型每一层的执行模式和资源消耗？我的研究需要分析粒度工作负载跟踪，特别关注 TFLOPS、GPU 内存使用情况、内存带宽、存储需求以及 LLaMA2 模型各个组件的运行时需求等指标。我将非常感谢任何关于在哪里找到此类数据的指导或建议。预先感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78208827/llama2-workload-traces</guid>
      <pubDate>Fri, 22 Mar 2024 21:03:23 GMT</pubDate>
    </item>
    <item>
      <title>MLP a2c 策略抱怨 0 不大于 0，或者无穷大不大于 0？</title>
      <link>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</link>
      <description><![CDATA[当我训练一些火炬模型时出现以下错误：
ValueError(&#39;分布Normal(loc: torch.Size([1, 4]))的预期参数尺度（形状为(1, 4)的张量），scale: torch.Size([1, 4] )) 以满足约束 GreaterThan(lower_bound=0.0)，但发现无效值：\ntensor([[inf, inf, 0., 0.]])&#39;)。

我的行为具有形状 (4,) 和观察 (3,)。
它是否认为无穷大不&gt;0，或者0不大于0？我不知道为什么会出现这种情况。它是简单地使用 model.learn 在稳定的基线 3 中训练模型。然而，它学习了一段时间，但在这一步失败了：
~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py 学习中（self、total_timesteps、callback、log_interval、tb_log_name、reset_num_timesteps、progress_bar）
    第257章
    [第 258 章]总时间步数：
--&gt;第 259 章
    260
    261 如果 continue_training 为 False：

〜\ anaconda3 \ envs \ lib \ site-packages \ stable_baselines3 \ common \ on_policy_algorithm.py在collect_rollouts中（self，env，callback，rollout_buffer，n_rollout_steps）
    167 # 转换为pytorch张量或TensorDict
    第168章
--&gt; 169 个动作，值，log_probs = self.policy(obs_tensor)
    170 个动作 = actions.cpu().numpy()
    171

_call_impl 中的 ~\anaconda3\envs\\lib\site-packages\torch\nn\modules\module.py(self, *input, **kwargs)
   第1192章
   第1193章
-&gt;第1194章
   第1195章
   第1196章

〜\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 向前（自我，obs，确定性）
    第624章
    625 个值 = self.value_net(latent_vf)
--&gt; [第 626 章]
    第627章 行动=distribution.get_actions(确定性=确定性)
    第628章

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 在 _get_action_dist_from_latent(self, Latent_pi)
    第654章
    第655章
--&gt;第656章
    第657章
    第658章

proba_distribution 中的 ~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\distributions.py(self,mean_actions,log_std)
    第162章 162
    第 163 章
--&gt;第164章
    第165章 回归自我
    166

~\anaconda3\envs\\lib\site-packages\torch\distributions\normal.py 在 __init__(self, loc, scale, validate_args)
     54 其他：
     55 batch_shape = self.loc.size（）
---&gt; 56 super(普通，自我).__init__(batch_shape, validate_args=validate_args)
     57
     58 def Expand(self,batch_shape,_instance=None):

__init__ 中的 ~\anaconda3\envs\\lib\site-packages\torch\distributions\distribution.py(self、batch_shape、event_shape、validate_args)
     55 如果无效.all():
     56 引发值错误（
---&gt; 57 f“预期参数{param}”
     58 f&quot;({type(value).__name__}，形状为{tuple(value.shape)})&quot;
     59 f”分布{repr(self)}”

请记住我的操作是 0&lt;=a&lt;=1。我需要将其设置为 0
我很难知道它到底在抱怨什么，因为这段代码深入稳定的基线3。这可能是他们的包中的一个小故障吗？我希望它更新权重并继续运行，但它却抱怨 0 不大于 0.. 我不知道为什么我关心这个，但它应该继续运行，不是吗？
感谢您的浏览。]]></description>
      <guid>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</guid>
      <pubDate>Fri, 22 Mar 2024 20:07:29 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 fastai resnet50/vision_learner 训练模型导出到 torchserve 中？</title>
      <link>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</link>
      <description><![CDATA[我的目标是将我用 Fastai 训练的模型部署到 Torchserve 中。我正在关注 本教程，但卡在了他为 pytorch 创建模型类的部分。
他提到要在 Torchserve 中运行我们的模型，我们需要以下内容：

模型类
从 pytorch 导出的权重（pth 文件）
处理程序

其中，我得到两个：重量和处理程序。然而，我陷入困境的是模型类。他创建了一个类文件，但我不知道他从哪里获得DynamicUnet作为该类的基础，也不知道他如何将该类与unet_learner混合以创建自定义PyTorch模型类。你能帮我为在学习器 vision_learner 下训练的模型和 resnet50 的预训练模型建立一个模型类吗？]]></description>
      <guid>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</guid>
      <pubDate>Fri, 22 Mar 2024 02:55:25 GMT</pubDate>
    </item>
    <item>
      <title>测试精度大于 1，并且一开始就非常高</title>
      <link>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</link>
      <description><![CDATA[问题在于，在测试循环中打印时，正确的样本多于样本总数。训练函数正常计算精度，但测试函数始终以 1.1-1.3 的精度开始。此外，这两种准确性一开始都非常高，然后就会下降。
我使用的是 sst2 数据集，批量大小 = 16，总批量为 55。
这是我的数据准备
def preprocess_function（示例）：
        返回分词器（示例[“句子”]，
                         填充=“最大长度”，
                         截断=真，
                         最大长度=MODEL_MAX_LENGTH）
    
    tokenized_datasets = dataset.map(preprocess_function,batched=True)
    tokenized_datasets = tokenized_datasets.remove_columns([“sentence”, “idx”, “attention_mask”])
    tokenized_datasets = tokenized_datasets.rename_column(“标签”, “标签”)
    tokenized_datasets.set_format(“火炬”)

    train_dataset = tokenized_datasets[“train”].shuffle(seed=SEED)
    valid_dataset = tokenized_datasets[“验证”].shuffle(seed=SEED)

    data_collat​​or = DataCollat​​orForLanguageModeling（
        分词器=分词器，
        传销=真实，
        MLM_概率=0.15
    ）

    class_count = [sum(train_dataset[&#39;labels&#39;] == label) 范围内的标签 (NUM_LABELS)]
    class_weights = 1. / torch.tensor(class_count, dtype=torch.float)

    class_weights_all = class_weights[train_dataset[&#39;labels&#39;]]

    加权采样器 = 加权随机采样器（
        权重=class_weights_all，
        num_samples= len(class_weights_all),
        替换=假
    ）

    train_dataloader = 数据加载器(
        训练数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        采样器=加权采样器，
        pin_memory=真
    ）

    valid_dataloader = 数据加载器(
        有效数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        pin_memory=真
    ）

这是我的训练和测试循环：
def train_loop(模型,
               训练数据加载器，
               优化器，
               lr_调度程序，
               设备）：
    模型.train()
    总损失= 0
    总正确率 = 0
    样本总数 = 0
    计数器 = 0
    对于步骤，批量枚举（tqdm（train_dataloader））：
        如果计数器 &gt;= 100：
            休息

        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        输出=模型（**批次）
        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)
        # print(预测, 预测.size(), len(预测))

        标签=批次[&#39;标签&#39;]
        # print(标签, labels.size(), len(标签))
        正确 = (预测 == 标签).sum().item()
        总正确率 += 正确率
        样本总数 += labels.size(0)

        损失 = 输出.损失
        总损失 += loss.detach()

        # loss.requires_grad = True
        loss.backward()
        优化器.step()
        lr_scheduler.step()
        优化器.zero_grad()

        计数器 += 1

    如果total_samples &gt; 则准确度=total_ Correct /total_samples 0 否则 0
    返回总损失、准确率

def test_loop(模型,
              有效数据加载器，
              设备）：
    模型.eval()
    评估损失 = 0
    总正确率 = 0
    样本总数 = 0
    对于步骤，批量枚举（tqdm（valid_dataloader））：
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        使用 torch.no_grad()：
            输出=模型（**批次）

        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)

        标签=批次[&#39;标签&#39;]
        正确 = (预测 == 标签).sum().item() # 错误行
        总正确率 += 正确率
        样本总数 += labels.size(0)
        print(&quot;正确总数：&quot; + str(total_ Correct) + &quot; ||| 样本总数：&quot; + str(total_samples))

        损失 = 输出.损失
        eval_loss += loss.detach()

    如果total_samples &gt; eval_accuracy =total_ Correct /total_samples 0 否则 0
    返回 eval_loss、eval_accuracy

我想我现在只是没有看到一些东西。我是否使用了相同的变量？]]></description>
      <guid>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</guid>
      <pubDate>Thu, 21 Mar 2024 02:12:15 GMT</pubDate>
    </item>
    <item>
      <title>libmagic 不可用，但有助于对类文件对象进行文件类型检测</title>
      <link>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</guid>
      <pubDate>Tue, 19 Mar 2024 12:21:50 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 中张量的 Autograd.grad()</title>
      <link>https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch</link>
      <description><![CDATA[我想计算网络中两个张量之间的梯度。输入 X 张量（批量大小 x m）通过一组卷积层发送，这些卷积层返回并输出 Y 张量（批量大小 x n）。
我正在创建一个新的损失，我想知道 Y 的梯度。 X. 在张量流中会是这样的：
tf.gradients(ys=Y, xs=X)
不幸的是，我一直在使用torch.autograd.grad()进行测试，但我不知道该怎么做。我收到如下错误：“RunTimeerror：只能为标量输出隐式创建 grad”。
如果我想知道 Y 的梯度，torch.autograd.grad() 中的输入应该是什么。 X？]]></description>
      <guid>https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch</guid>
      <pubDate>Mon, 18 Feb 2019 19:32:26 GMT</pubDate>
    </item>
    </channel>
</rss>