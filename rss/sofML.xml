<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 03 Jun 2024 21:14:39 GMT</lastBuildDate>
    <item>
      <title>在执行无监督域自适应时，属性移位数据的性能下降</title>
      <link>https://stackoverflow.com/questions/78572439/decrease-in-performance-on-attribute-shift-data-on-performing-unsupervised-domai</link>
      <description><![CDATA[大家好，我正在对 cifar10 和 cifar10.1 数据集进行实验，其中我使用 Resnet18 模型在 cifar10 训练数据上进行训练，并在 cifar10 测试数据和 cifar10.1（属性移位数据）上进行评估。我使用监督对比学习来训练特征编码器（当前特征向量 dim id 64），然后训练投影头（单层 MLP），在评估此模型时，cifar10 测试数据上的 ACC 为 91%，cifar10.1 数据集上的 ACC 为 81%。
为了将特征编码器模型改进为看不见的属性移位数据，我正在使用自监督对比损失对 cifar10.1 数据集上的特征编码器进行微调，但在对微调的特征编码器以及在 cifar10 训练数据上预训练的投影头进行微调时，其性能比未微调的模型更差，微调模型的准确率为 56%（之前为 81%），请告诉我为什么微调后的模型效果不佳。
我想知道为什么在 cifar10.1 上对自动编码器进行微调时，它在 cifar10.1 上的表现更差，在 cifar10 测试集上的表现也很差]]></description>
      <guid>https://stackoverflow.com/questions/78572439/decrease-in-performance-on-attribute-shift-data-on-performing-unsupervised-domai</guid>
      <pubDate>Mon, 03 Jun 2024 20:39:21 GMT</pubDate>
    </item>
    <item>
      <title>训练期间准确率与测试期间准确率之间的差异</title>
      <link>https://stackoverflow.com/questions/78572370/difference-between-accuracy-during-training-and-accuracy-during-testing</link>
      <description><![CDATA[在下面的模型中，最终验证阶段结束时报告的准确度为 0.46，但在手动测试期间报告的值为 0.53。什么可以解释这种差异？
import torch
from torch import nn
import torchvision.models as models
import pytorch_lightning as pl
from torchmetrics.classification import BinaryAccuracy
from pytorch_lightning.loggers import NeptuneLogger
from torch.utils.data import DataLoader, TensorDataset
from os import environ

class ResNet(pl.LightningModule):
def __init__(self, n_classes=1, n_channels=3, lr=1e-3):
super().__init__()
self.save_hyperparameters()
self.validation_accuracy = BinaryAccuracy()

backbone = models.resnet18(pretrained=True)
n_filters = backbone.fc.in_features
if n_channels != 3:
backbone.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
layer = list(backbone.children())[:-1]
self.feature_extractor = nn.Sequential(*layers)
self.classifier = nn.Linear(n_filters, n_classes)
self.loss_fn = nn.BCEWithLogitsLoss()

def forward(self, x):
features = self.feature_extractor(x)
logits = self.classifier(features.squeeze())
return logits

def configure_optimizers(self):
return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_fn(y_hat, y)
self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
返回损失

def validation_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_fn(y_hat, y)
self.validation_accuracy.update(y_hat, y)
self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
返回损失

def on_validation_epoch_end(self):
val_acc = self.validation_accuracy.compute()
self.log(&#39;val_acc&#39;, val_acc, on_epoch=True, prog_bar=True, logger=True)
self.validation_accuracy.reset()

def predict_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
preds = torch.sigmoid(y_hat) &gt; 0.5
return preds, y

def get_dataloader():
# 创建一个简单的合成数据集用于演示目的
x = torch.randn(100, 3, 224, 224)
y = torch.randint(0, 2, (100, 1)).float()
dataset = TensorDataset(x, y)
return DataLoader(dataset, batch_size=8)

# 设置模型、数据和训练器
model = ResNet()
train_loader = get_dataloader()
val_loader = get_dataloader()

logger = NeptuneLogger(
api_key=environ.get(&quot;NEPTUNE_API_TOKEN&quot;),
project=&quot;richbai90/ResnetTest&quot;,
tags=[&quot;MRE&quot;, &quot;resnet&quot;],
)

trainer = pl.Trainer(max_epochs=3, logger=logger, log_every_n_steps=1)

# 训练并验证模型
trainer.fit(model, train_loader, val_loader)

# 根据验证数据进行预测
val_loader = get_dataloader()
preds, target = [], []
for batch in val_loader:
batch_preds, batch_targets = model.predict_step(batch, 0)
preds.extend(batch_preds)
target.extend(batch_targets)

# 手动计算准确率以进行比较
preds = torch.stack(preds).view(-1)
targets = torch.stack(targets).view(-1)
manual_accuracy = (preds == target).float().mean().item()
print(f&quot;手动准确率： {manual_accuracy:.4f}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78572370/difference-between-accuracy-during-training-and-accuracy-during-testing</guid>
      <pubDate>Mon, 03 Jun 2024 20:17:59 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中回归任务的批量归一化</title>
      <link>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</link>
      <description><![CDATA[所以我有这个想要适合回归的神经网络。当我在层之间不使用批量归一化时，它表现良好，但是当我添加批量归一化时，它表现非常糟糕。
例如，没有批量归一化的 MAE 是 17000，而使用批量归一化的 MAE 则高达 180000。这是为什么？
X = train_data.drop([&#39;SalePrice&#39;], axis=1)
y = train_data[&#39;SalePrice&#39;]

# 获取每种类型的列
numeric_cols = [col for col in X.columns if X[col].dtype in [&#39;float64&#39;, &#39;int64&#39;]]
categoric_cols = [col for col in X.columns if X[col].dtype == &#39;object&#39;]

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)

# 转换管道
numeric_transformer = Pipeline(steps=[
(&#39;impute_numeric&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scale&#39;, StandardScaler())
])

categoric_transformer = Pipeline(steps=[
(&#39;impute_categoric&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encode&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)),
])

preprocessing = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, numeric_cols),
(&#39;cat&#39;, categoric_transformer, categoric_cols)
])

# 拟合变换器
transformed_X_train = preprocessing.fit_transform(X_train)
transformed_X_val = preprocessing.transform(X_val)

# 模型 14392
input_shape = perceived_X_train.shape[1]
model = keras.Sequential([
layer.Dense(units=256,activation=&#39;swish&#39;,input_shape=[input_shape]),
layers.BatchNormalization(),
layers.Dense(units=256,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=1),
])

# 设置优化器和损失函数
model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;mae&#39;
)

early_stopping = callups.EarlyStopping(
min_delta=0.01, # 计为改进的最小变化量
patient=20, # 需要失败的时期数 min_delta 才能提前停止
restore_best_weights=True,
)

# 拟合神经网络
history = model.fit(
transformed_X_train, y_train,
epochs=200,
batch_size=128,
callbacks=[early_stopping],
validation_data=(transformed_X_val, y_val)
#validation_split=0.2,# 自动创建测试和验证集
)

history_df = pd.DataFrame(history.history)
history_df.loc[0:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot()

# 最佳 13962.4795
print(&quot;Evaluation模型的”，model.evaluate(transformed_X_val, y_val, batch_size=128))
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</guid>
      <pubDate>Mon, 03 Jun 2024 20:03:21 GMT</pubDate>
    </item>
    <item>
      <title>使用什么方法对网络结构进行统计分析？[关闭]</title>
      <link>https://stackoverflow.com/questions/78571525/what-methods-to-use-for-statistical-analysis-of-network-structures</link>
      <description><![CDATA[我有一个网络结构（邻接矩阵），想找到一种方法来告诉我网络中的哪些特征对于解释我的响应变量最重要。在这种情况下，它是一种感染模拟措施。直观地说，一个可以提取我看不到的图形特征并告诉我它很重要的神经网络会很棒，但我不知道该怎么做。理想情况下，我也想保持简单。任何建议都值得赞赏。
我一直在使用广泛的 GAM 进行建模，但总是回到这个问题上，我是提取我认为重要的特征的人。]]></description>
      <guid>https://stackoverflow.com/questions/78571525/what-methods-to-use-for-statistical-analysis-of-network-structures</guid>
      <pubDate>Mon, 03 Jun 2024 16:28:23 GMT</pubDate>
    </item>
    <item>
      <title>OverflowError：无法将无穷大转换为整数</title>
      <link>https://stackoverflow.com/questions/78571461/overflowerror-cannot-convert-infinity-to-integer</link>
      <description><![CDATA[我遇到了这个错误，如果有人有解决方案，请告诉我。
---------------------------------------------------------------------------
OverflowError Traceback (most recent call last)
Cell In[61], 
line 1 ----&gt; 1 loss, accuracies = federated_learning(clients)
Cell In[60], line 113, in federated_learning(clients)
111 # 服务器收集客户端的加密权重，然后执行权重聚合
112 with torch.no_grad():
--&gt; 113 aggregator()
115 # 日志记录
116 if (iteration + 1) % 2 == 0:
Cell In[60], line 96, in federated_learning.&lt;locals&gt;.aggregator()
93 global_model.linear.bias = nn.Parameter(torch.tensor(model_bias, dtype=torch.float32))
95 # 加密聚合的全局模型参数并保存到每个客户端的 enc_file
---&gt; 96 encrypt_weights(aggregated_pa​​rams_float32, clients[0].enc_file) # 使用一个客户端的 enc_file 进行加密
98 for client in clients:
99 with open(clients[0].enc_file, &#39;r&#39;) as f:
Cell In[50], line 53, in encrypt_weights(model_params, enc_file)
51 # 通过缩放将十进制转换为整数
52 scale_factor = Decimal(10**10) # 放大以保持精度
---&gt; 53 model_params_scaled = [int(Decimal(param) * scale_factor) for param in model_params_str]
55 crypto_params = [elgamal.encrypt(param) for param in model_params_scaled]
57 # 将加密参数和公钥 (p, g, h) 以及私钥 x 保存到文件
Cell In[50], line 53, in &lt;listcomp&gt;(.0)
51 # 通过缩放将十进制转换为整数
52 scale_factor = Decimal(10**10) # 放大以保持精度
---&gt; 53 model_params_scaled = [int(Decimal(param) * scale_factor) for param in model_params_str]
55 crypto_params = [elgamal.encrypt(param) for param in model_params_scaled]
57 # 将加密参数和公钥 (p, g, h) 以及私钥 x 保存到文件
**OverflowError: 无法将无穷大转换为整数**

我对模型参数进行了加密和解密，并尝试使用较大的 int 值。]]></description>
      <guid>https://stackoverflow.com/questions/78571461/overflowerror-cannot-convert-infinity-to-integer</guid>
      <pubDate>Mon, 03 Jun 2024 16:13:39 GMT</pubDate>
    </item>
    <item>
      <title>如何标记社交媒体中未标记的大量文本数据</title>
      <link>https://stackoverflow.com/questions/78571418/how-to-label-unlabeled-large-text-data-from-socia-media</link>
      <description><![CDATA[我收集了一些社交媒体评论，并计划执行分层深度学习模型。作为初始过程，我想将这些未标记的数据标记为一些预定义的主题，例如交通、食品等。然后必须定义不同的交通类别或食品类别。因此，我需要两列标签。我有 9.341.087 个无标签训练数据和 30.000 行无标签测试文本数据。由于我是机器学习新手，不确定在没有标记数据的情况下是否可以继续标记。感谢您的帮助
我尝试了分层文本分类，需要拖曳特定的标签列，但行数太少，我无法手动完成。]]></description>
      <guid>https://stackoverflow.com/questions/78571418/how-to-label-unlabeled-large-text-data-from-socia-media</guid>
      <pubDate>Mon, 03 Jun 2024 16:04:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试为 knn 算法绘制不同的邻居[关闭]</title>
      <link>https://stackoverflow.com/questions/78571281/trying-to-plot-different-neighbors-for-a-knn-algorithm</link>
      <description><![CDATA[它给出了一个值错误
ValueError：x 和 y 必须具有相同的第一维，但形状为 (10,) 和 (30,)
training_accuracy = []
test_accuracy = []
neighbors_settings = range(1, 11)
for n_neighbors in neighbours_settings:
# 构建模型
clf = KNeighborsClassifier(n_neighbors=n_neighbors)
clf.fit(X_train, y_train)
# 记录训练集准确率
training_accuracy.append(clf.score(X_train, y_train))
# 记录泛化准确率
test_accuracy.append(clf.score(X_test, y_test))

plt.plot(neighbors_settings, training_accuracy, label=&quot;training准确度”）
plt.plot（neighbors_settings，test_accuracy，label =“测试准确度”）
plt.ylabel（“准确度”）
plt.xlabel（“n_neighbors”）
plt.legend（）
]]></description>
      <guid>https://stackoverflow.com/questions/78571281/trying-to-plot-different-neighbors-for-a-knn-algorithm</guid>
      <pubDate>Mon, 03 Jun 2024 15:35:17 GMT</pubDate>
    </item>
    <item>
      <title>加载预先训练的 json 模型时，Python tensorflow keras 出现错误</title>
      <link>https://stackoverflow.com/questions/78570246/python-tensorflow-keras-error-when-load-a-pre-trained-json-model</link>
      <description><![CDATA[这是我在尝试执行 Python 代码时遇到的**错误**
[ERROR:0@5.030] global persistence.cpp:519 cv::FileStorage::Impl::open 无法以读取模式打开文件：“models/haarcascade_frontalface_default.xml”
回溯（最近一次调用）：
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\anti.py”，第 14 行，位于&lt;module&gt;
model = model_from_json(loaded_model_json)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\models\model.py&quot;，第 575 行，位于 model_from_json
return serialization_lib.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 694 行，位于 deserialize_keras_object
cls = _retrieve_class_or_fn(
^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 812 行，位于 _retrieve_class_or_fn
raise TypeError(
TypeError：无法找到类“Functional”。确保自定义类已用修饰`@keras.saving.register_keras_serializable()`。完整对象配置：{&#39;class_name&#39;: &#39;Functional&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;model&#39;, &#39;trainable&#39;: True,...(**json 文件的内容**........&#39;keras_version&#39;: &#39;2.15.0&#39;, &#39;backend&#39;: &#39;tensorflow&#39;)

以下是库版本
keras 3.3.3
opencv-python 4.9.0.80
tensorflow 2.16.1
python 3.12.3
以下是我正在尝试执行的**代码**
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array 
import os
import numpy as np

root_dir = os.getcwd()
# 加载人脸检测模型
face_cascade = cv2.CascadeClassifier(&quot;models/haarcascade_frontalface_default.xml&quot;)
# 加载反欺骗模型图
json_file = open(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/Antispoofing_model_mobilenet.json&#39;,&#39;r&#39;)
loaded_model_json = json_file.read()
json_file.close()
model = tf.keras.models.model_from_json(loaded_model_json)
# 加载反欺骗模型权重
model.load_weights(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/finalyearproject_antispoofing_model_99-0.978947.h5&#39;)
print(&quot;模型从磁盘加载&quot;)

video = cv2.VideoCapture(0)
while True:
try:
ret,frame = video.read()
gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = face_cascade.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces: 
face = frame[y-5:y+h+5,x-5:x+w+5]
resized_face = cv2.resize(face,(160,160))
resized_face = resized_face.astype(&quot;float&quot;) / 255.0
resized_face = np.expand_dims(resized_face, axis=0)
# 将人脸 ROI 传递到经过训练的活体检测器
# 模型确定人脸是“真”还是“假”
preds = model.predict(resized_face)[0]
print(preds)
if preds&gt; 0.5:
标签 = &#39;poof&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 0, 255), 2)
else:
标签 = &#39;eal&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 255, 0), 2)
cv2.imshow(&#39;frame&#39;, frame)
key = cv2.waitKey(1)
if key == ord(&#39;q&#39;):
break
except Exception as e: 
pass
video.release() 
cv2.destroyAllWindows()


请告诉我我应该怎么做（我尝试降级库，但也出现了错误）
我尝试降级库，但也出现了错误]]></description>
      <guid>https://stackoverflow.com/questions/78570246/python-tensorflow-keras-error-when-load-a-pre-trained-json-model</guid>
      <pubDate>Mon, 03 Jun 2024 12:21:33 GMT</pubDate>
    </item>
    <item>
      <title>梯度累积损失计算</title>
      <link>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</link>
      <description><![CDATA[假设我们有数据 [b,s,dim]，我最近注意到 CrossEntropyLoss 是 (1) 计算一批中所有 token (b * s) 的平均值，而不是 (2) 计算每个句子然后计算平均值。
以下是计算 hugging_face 转换器 BertLMHeadMode 损失的代码
sequence_output = output[0]
prediction_scores = self.cls(sequence_output)

lm_loss = None
if labels is not None:
# 我们正在进行下一个 token 预测；将预测分数和输入 ID 移位一格
shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()
labels = labels[:, 1:].contiguous()
loss_fct = CrossEntropyLoss()
lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

我知道 (1) 和 (2) 在这种情况下没有区别。但是当我们应用梯度累积时，我认为情况就不同了。
假设我的batch_size为4，4个句子的长度分别为100,200,300,400。
使用batch_size 4时，损失是根据总共1000个token的平均值计算的。
但是当batch_size = 1且梯度累积= 4时，我认为损失是不同的。我们首先分别计算每个句子的损失，然后计算平均值，这意味着对于100个token的句子，我们计算100个token的损失平均值，然后除以4并将其添加到总损失中，对于其他3个句子也是如此，我认为以这种方式计算的损失与使用batch_size = 4计算的损失不同。
我误解了什么吗？
想知道我是否正确或误解了什么]]></description>
      <guid>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</guid>
      <pubDate>Mon, 03 Jun 2024 11:19:53 GMT</pubDate>
    </item>
    <item>
      <title>稳定扩散图像中突变的检测方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78568259/detection-methods-for-mutations-in-stable-diffusion-images</link>
      <description><![CDATA[我正在使用 Python 中的稳定扩散来生成人体图像。我面临的一个挑战是自动检测生成的图像中的任何异常。这些异常包括手、脚等身体部位的不规则性，或身体部位过多或过少，以及介于两者之间的一切。
我测试了几种方法，但没有成功。例如，我尝试使用 OpenAI GPT4 视觉来识别这些异常；但是，它无法检测到异常，除非我说“你确定吗”几次，然后它似乎会发现它。
为了保持本地化，我尝试了视觉模型，如 llava，但这也失败了。我也尝试训练自己的卷积神经网络，但由于训练数据不足（手动标记），它最终过度拟合。其他潜在解决方案（如 ViT 和 EfficientNet）似乎也面临数据不足的相同问题。
我需要的是一种无需人工干预即可自动检测这些异常的方法，有什么建议吗？
我尝试了以下操作：



试用
问题




OpenAI GPT4 视觉
除非多次提示，否则无法检测到异常


本地视觉模型（如 llava）
未发现异常


训练自己的卷积神经网络
由于训练数据不足（手动标记）


潜在解决方案（如 ViT 和 EfficientNet）
面临数据不足的相同问题



不良照片示例：
1：图片
2：图片
3：图片]]></description>
      <guid>https://stackoverflow.com/questions/78568259/detection-methods-for-mutations-in-stable-diffusion-images</guid>
      <pubDate>Mon, 03 Jun 2024 03:28:04 GMT</pubDate>
    </item>
    <item>
      <title>在 sagemaker 中部署 llama-3 8B 时出错：标记器不匹配？</title>
      <link>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</link>
      <description><![CDATA[尝试部署 meta/llama-3-8B-Instruct 时，我在 sagemaker 代码编辑器中收到以下错误：
“您从此检查点加载的 tokenizer 类与调用此函数的类的类型不同。这可能会导致意外的标记化。
您从此检查点加载的 tokenizer 类是“PreTrainedTokenizerFast”。
调用此函数的类是“LlamaTokenizer”。&quot;
我正在使用 Huggingface API 将模型从 HF Hub 直接加载到 sagemaker 代码编辑器，使用 AWS 中的 ml.g5xlarge 实例类型。使用 HF API 不需要或公开 tokenizer。]]></description>
      <guid>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</guid>
      <pubDate>Sat, 01 Jun 2024 09:35:46 GMT</pubDate>
    </item>
    <item>
      <title>预测多元时间序列时 VARIMA 模型的模型漂移</title>
      <link>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</link>
      <description><![CDATA[我目前正在尝试在多变量时间序列数据上训练 VARIMA 模型，该数据是关于冷却系统的 5 种不同类型的传感器测量的。数据具有周期性，因此完全相同的模式每 25 个数据点左右就会重复出现。我有一个包含 5 个不同组件的数据集，其中每分钟都有一个数据点。我使用了 2 周的数据来训练模型，然后让它合成数据。我使用的 VARIMA 模型是从 Darts 包导入的，我这样定义模型：model_VARIMA = VARIMA(p=12, d=0, q=0, trend=&quot;n&quot;)。该模型是在 2 周的训练数据上训练的。当预测未来 30 个点或更多时，预测显然开始显示模型漂移。所有组件都没有产生周期性的多变量时间序列数据，而是慢慢开始遵循一条不再改变值的直线水平线。换句话说，所有成分的标准偏差都会慢慢变为零，所有成分都会慢慢开始向其平均值漂移并永远停留在那里。我想知道是否有人对此有解释并有解决问题的方法。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</guid>
      <pubDate>Tue, 28 May 2024 12:15:20 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降：缩减的特征集比原始特征集的运行时间更长</title>
      <link>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</link>
      <description><![CDATA[我尝试用 Python 实现梯度下降算法来解决机器学习问题。我处理的数据集已经过预处理，当我使用奇异值分解 (SVD) 比较两个数据集（一个具有原始特征，另一个具有前者集合的缩减维度）时，我在运行时观察到了意外行为。我不断观察到，与缩减后的数据集相比，较大的原始数据集的梯度下降算法的运行时间较短，这与我的预期相反。考虑到缩减后的数据集较小，运行时间难道不应该更短吗？我试图理解为什么会发生这种情况。
以下是相关代码片段：
import time
import numpy as np
import matplotlib.pyplot as plt

def h_theta(X1, theta1):
# 假设函数的实现
return np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
# 成本函数的实现
return np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, theta):
# 梯度的计算
gradient = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
return gradient

def gradient_descent(X1, y1):
theta_initial = np.zeros(X1.shape[1]) # 用初始化 theta零

num_iterations = 1000
learning_rates = [0.1, 0.01, 0.001] 
cost_iterations = []
theta_values = []
start = time.time()
for alpha in learning_rates:
theta = theta_initial.copy()
cost_history = []
for i in range(num_iterations):
gradient = grad(X1, y1, theta)
theta = theta - np.dot(alpha, gradient)
cost = j_theta(X1, y1, theta)
cost_history.append(cost)
cost_iterations.append(cost_history)
theta_values.append(theta)
end = time.time()
print(f&quot;所用时间：{end - start} 秒&quot;)
fig, axs = plt.subplots(len(learning_rates), figsize=(8, 15)) 
for i, alpha in enumerate(learning_rates):
axs[i].plot(range(num_iterations), cost_iterations[i], label=f&#39;alpha = {alpha}&#39;) 
axs[i].set_title(f&#39;学习率：{alpha}&#39;)
axs[i].set_ylabel(&#39;成本 J&#39;)
axs[i].set_xlabel(&#39;迭代次数&#39;) 
axs[i].legend()
plt.tight_layout() 
plt.show()

# 使用 SVD 将 X 减少到 3 个特征（列）的代码：
# 对 X 执行奇异值分解并将其减少到 3 列
U, S, Vt = np.linalg.svd(X_normalized)
# 将 X 减少到 3 列
X_reduced = np.dot(X_normalized, Vt[:3].T)

# 打印 X_reduced 的前 5 行
print(&quot;X_reduced 的前 5 行：&quot;)
# 规范化 X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;X 经过缩减和规范化后的均值和标准差:\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# 打印简化后的 X 的形状，以确认它只有 3 个特征
print(&quot;Shape of X_reduced:&quot;, X_reduced.shape)

# 将截距列添加到 X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))

# 示例用法
# X_normalized_with_intercept 和 y_normalized 表示原始数据集
# X_reduced_with_intercept 和 y_normalized 表示简化后的数据集

# 对原始数据集执行梯度下降
gradient_descent(X_normalized_with_intercept, y_normalized)

# 对简化后的数据集执行梯度下降
gradient_descent(X_reduced_with_intercept, y_normalized)

在我的梯度下降实现中，什么原因导致精简数据集的运行时间始终比完整数据集长？任何有关故障排除的见解或建议都将不胜感激。
我尝试过重写和审查我的实现，但似乎对于大多数学习率以及增加的迭代次数，较大特征集的运行时间低于其 SVD 子集。]]></description>
      <guid>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</guid>
      <pubDate>Sun, 07 Apr 2024 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 LSTM 预测未来值</title>
      <link>https://stackoverflow.com/questions/69906416/forecast-future-values-with-lstm-in-python</link>
      <description><![CDATA[此代码可预测截至当前日期的指定股票价值，但无法预测训练数据集之外的日期。此代码来自我之前提出的一个问题，因此我对它的理解程度相当低。我认为解决方案是一个简单的变量更改以添加额外的时间，但我不知道需要操作哪个值。
import pandas as pd
import numpy as np
import yfinance as yf
import os
import matplotlib.pyplot as plt
from IPython.display import display
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;

pd.options.mode.chained_assignment = None

# 下载数据
df = yf.download(tickers=[&#39;AAPL&#39;], period=&#39;2y&#39;)

# 拆分数据
train_data = df[[&#39;Close&#39;]].iloc[: - 200, :]
valid_data = df[[&#39;Close&#39;]].iloc[- 200:, :]

# 缩放数据
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(train_data)

train_data = scaler.transform(train_data)
valid_data = scaler.transform(valid_data)

# 提取训练序列
x_train, y_train = [], []

for i in range(60, train_data.shape[0]):
x_train.append(train_data[i - 60: i, 0])
y_train.append(train_data[i, 0])

x_train = np.array(x_train)
y_train = np.array(y_train)

# 提取验证序列
x_valid = []

for i in range(60, valid_data.shape[0]):
x_valid.append(valid_data[i - 60: i, 0])

x_valid = np.array(x_valid)

# 重塑序列
x_train = x_train.reshape(x_train.shape[0], 
x_train.shape[1], 1)
x_valid = x_valid.reshape(x_valid.shape[0], 
x_valid.shape[1], 1)

# 训练模型
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, 
input_shape=x_train.shape[1:]))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
model.fit(x_train, y_train, epochs=50, batch_size=128, verbose=1)

# 生成模型预测
y_pred = model.predict(x_valid)
y_pred = scaler.inverse_transform(y_pred)
y_pred = y_pred.flatten()

# 绘制模型预测
df.rename(columns={&#39;Close&#39;: &#39;Actual&#39;}, inplace=True)
df[&#39;Predicted&#39;] = np.nan
df[&#39;Predicted&#39;].iloc[- y_pred.shape[0]:] = y_pred
df[[&#39;Actual&#39;, &#39;Predicted&#39;]].plot(title=&#39;AAPL&#39;)

display(df)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/69906416/forecast-future-values-with-lstm-in-python</guid>
      <pubDate>Tue, 09 Nov 2021 23:47:07 GMT</pubDate>
    </item>
    <item>
      <title>“如何使用 Python 和 Keras 构建自己的 AlphaZero AI”中的 stmemory 和 ltmemory</title>
      <link>https://stackoverflow.com/questions/52396257/stmemory-and-ltmemory-in-how-to-build-your-own-alphazero-ai-using-python-and-ke</link>
      <description><![CDATA[我关注的是如何使用 Python 和 Keras 构建自己的 AlphaZero AI
git 在这里
在 run.ipynb 中，这部分代码：
memory.clear_stmemory()

if len(memory.ltmemory) &gt;= config.MEMORY_SIZE:

这篇文章没有对此做太多解释。
memory.ltmemory 和 memory.stmemory 有何用途？]]></description>
      <guid>https://stackoverflow.com/questions/52396257/stmemory-and-ltmemory-in-how-to-build-your-own-alphazero-ai-using-python-and-ke</guid>
      <pubDate>Wed, 19 Sep 2018 00:22:34 GMT</pubDate>
    </item>
    </channel>
</rss>