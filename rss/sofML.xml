<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 24 Aug 2024 01:06:52 GMT</lastBuildDate>
    <item>
      <title>如何在调用 MessageGraph 期间传递多个输入？</title>
      <link>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</link>
      <description><![CDATA[我们有一个用于 LLMCompiler 实现的 MessageGraph，并且正如预期的那样，我们在运行调用时将用户的问题作为 HumanMessage 对象列表传递（这些对象映射到默认的&quot;messages&quot; 键并传递给提示模板），这对于简单的用例来说工作得很好，但现在我们需要在调用时（而不是在构建图形时）传递额外的信息/上下文，我们对 React 代理做了类似的事情，并且很容易传递类似于 invoke({&quot;messages&quot;:input, &quot;context&quot;:context}) 的字典。但对于 MessageGraph，这不起作用，看起来运行 invoke(messages) 时传递的消息列表会在提示中自动映射到&quot;messages&quot;键，无法添加其他输入，我尝试传递一个字典 invoke({&quot;messages&quot;:messages, &quot;context&quot;:context})，但没有成功，错误失败：

消息字典必须包含“role”和“content”键，得到
{&quot;messages&quot;:messages,&quot;context&quot;:context
]]></description>
      <guid>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</guid>
      <pubDate>Fri, 23 Aug 2024 21:30:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 中运行自定义 Segment Anything 模型？</title>
      <link>https://stackoverflow.com/questions/78907597/how-can-i-run-a-custom-segment-anything-model-in-google-colab</link>
      <description><![CDATA[由于我的笔记本电脑没有必要的硬件，我试图在 Google Colab 中运行此 SAM 模型：https://github.com/htcr/sam_road，但我很迷茫。该存储库在 README 中有设置说明，但我不确定如何使用 Colab 遵循这些说明。
该存储库是公开的，但我需要令牌权限才能将存储库直接安装到 Colab 中，因此我想我可以将代码复制并粘贴到 Colab 中，但当然有很多代码需要复制。如果复制/粘贴是解决此问题的正确方法，我应该如何在 Colab 笔记本中组织代码（因为我需要复制所有存储库的代码）？或者我应该用其他方式来运行模型而不是复制/粘贴？
我尝试过进行一些复制/粘贴，但粘贴到一个代码文件中包含依赖于 repo 中其他代码文件的导入语句，但由于 Colab 笔记本似乎只有一页代码块，我不确定我是否可以每个代码块只做一个代码文件，或者是否需要任何特定的顺序。]]></description>
      <guid>https://stackoverflow.com/questions/78907597/how-can-i-run-a-custom-segment-anything-model-in-google-colab</guid>
      <pubDate>Fri, 23 Aug 2024 21:24:12 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10 自定义训练——导入 YOLO 还是 YOLOv10？</title>
      <link>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</link>
      <description><![CDATA[我希望对我拥有的数据集使用 YoloV10n 进行自定义训练（到目前为止我一直在使用 YoloV8），但我不确定是否要使用/导入 YOLO 或 YOLOv10。
我最初尝试使用 YOLOv10，并能够成功完成自定义训练和推理。但是，由于断言不一致，我无法将模型导出到 tflite。然后我切换回 YOLO（从预先训练的 yolov10.pt 模型开始），并能够将其导出到 tflite。此外，Ultralytics 的官方文档确实指导如何使用 YOLO（而不是 YOLOv10）进行 YoloV10 训练。另一方面，RoboFlow 教程确实指导如何使用 YOLOv10... 🤔
我应该使用 from ultralytics import YOLOv10 还是 from ultralytics import YOLO？这有关系吗？对于训练、推理和导出（tflite 等），答案是否相同？]]></description>
      <guid>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</guid>
      <pubDate>Fri, 23 Aug 2024 21:07:49 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 预测和测试数据</title>
      <link>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</link>
      <description><![CDATA[import pandas as pd
import itertools
import numpy as np
import s3fs
from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer

# 为您的 CSV 文件定义 S3 路径
import pandas as pd
import s3fs

# 为您的 CSV 文件定义 S3 路径
s3_path = &quot;s3://{}/{}/{}.csv&quot;

# 带有附加检查的读取文件函数
def read_and_check_csv(s3_path):
fs = s3fs.S3FileSystem()
with fs.open(s3_path) as f:
try:
# 尝试读取 CSV 文件
df = pd.read_csv(f, header=None, low_memory=False)
# 检查行长度是否一致
if not df.apply(lambda x: len(x.dropna()), axis=1).nunique() == 1:
raise ValueError(&quot;检测到不一致的行长度&quot;)
print(&quot;文件读取成功，似乎为 CSV 格式。&quot;)
return df
except Exception as e:
print(f&quot;无法读取 CSV 文件：{e}&quot;)
return None

# 读取并检查 CSV 文件
df = read_and_check_csv(s3_path)

如果 df 不为 None:
print(df.head())
否则:
print(&quot;文件无法读取或不是有效的 CSV 格式。&quot;)

# 定义用于切片数据的索引
a = [50 * i for i in range(3)]
b = [40 + i for i in range(10)]
indices = [i + j for i, j in itertools.product(a, b)]

# 准备测试数据
test_data = shape.iloc[indices[:-1]]
test_X = test_data.iloc[:, 1:]

# 确保所有行的列数相同
min_cols = test_X.shape[1]
test_X = test_X.dropna(axis=1, how=&#39;all&#39;) # 删除所有 NaN 值的列

# 验证没有具有不同值的行长度
test_X = test_X.apply(lambda x: x.dropna().reset_index(drop=True), axis=1)

# 使用 SageMaker 端点名称初始化预测器
predictor = Predictor(endpoint_name=&#39;sagemaker-xgboost-2024-08-23-19-59-10-793&#39;)

# 确保预测器使用 CSV 序列化器
predictor.serializer = CSVSerializer()

# 将 DataFrame 转换为端点所需的格式
test_X_csv = test_X.to_csv(index=False, header=False, sep=&#39;,&#39;)

# 进行预测
try:
predictions = predictor.predict(test_X_csv)
# 打印预测
print(predictions.decode(&#39;utf-8&#39;))
except Exception as e:
print(f&quot;Error making预测：{e}&quot;)


我在 aws sagemaker Jupyter 实验室中为我的 xgboost 框架使用上述脚本。在此脚本之前，我正在运行以下代码来设置端点。
predictor = estimator.deploy(
initial_instance_count=1, instance_type=&quot;ml.m5.2xlarge&quot;
)

我添加了一些错误处理，这就是我了解到我的测试文件似乎没有被正确读取的地方。我得到的实际错误是：
无法读取 CSV 文件：检测到不一致的行长度
无法读取文件或文件不是有效的 CSV 格式。
进行预测时出错：调用 InvokeEndpoint 操作时发生错误 (ModelError)：从主服务器收到客户端错误 (415)，消息为“加载 csv 数据失败，出现异常，请确保数据为 csv 格式：
&lt;class &#39;ValueError&#39;&gt;
设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (29,) + 非均匀部分。”

关于如何修复此问题有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</guid>
      <pubDate>Fri, 23 Aug 2024 20:58:39 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习方法和工具的问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/78907467/questions-about-ml-methods-and-tooling</link>
      <description><![CDATA[我选修了一门非参数计量经济学课程，并喜欢学习核估计，因为它是传统参数回归的更准确、更科学的版本。我的教授是非参数的铁杆信徒，他嘲笑数据科学行业使用随机森林之类的东西，并称其为前沿技术。我有点相信他，但有人能帮我理解人们是如何（或为什么不）使用核估计的吗？到目前为止，我注意到 Featuretools 似乎在使用内核估计和带宽选择缩小特征范围方面有类似的方法。
学术界从事类似工作的人员（计量经济学家和计算机科学家/机器学习研究人员）之间似乎也存在巨大差距，这也让我感到困惑。
更重要的是，我对此很陌生（数学和经济学本科，经济学硕士，过去 4 年分析师/数据工程师，目前失业），并且正在尝试寻找可以利用我现有知识的有趣项目，因此建议/见解很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78907467/questions-about-ml-methods-and-tooling</guid>
      <pubDate>Fri, 23 Aug 2024 20:24:29 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助验证谷歌/机器学习课程提供的数据</title>
      <link>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</link>
      <description><![CDATA[在谷歌提供的机器学习课程简介中的梯度下降页面中，提供了特征和相应的标签、MSE 损失函数、初始数据集和结果。我很难验证他们的结果，我想知道是否有人可以帮助确认我是否犯了错误或者他们犯了错误。
我有以下内容：
import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=[&#39;pounds&#39;,&#39;mpg&#39;])

number_of_iterations = 6
weight = 0 # 初始化权重
bias = 0 # 初始化权重
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

对于 i 在范围内（迭代次数）：
loss = calculate_loss（初始数据 df、权重、偏差）
final_results_df = update_results（最终结果 df、权重、偏差、损失）
weight_slope = find_weight_slope（初始数据 df、权重、偏差）
bias_slope = find_bias_slope（初始数据 df、权重、偏差）
weight = new_weight_update（权重、learning_rate、weight_slope）
bias = new_bias_update（偏差、learning_rate、bias_slope）
print（最终结果 df）

def calculate_loss（df、权重、偏差）：
loss_summation = []
对于 i 在范围内（0、len（df））：
loss_summation.append((df[&#39;mpg&#39;][i]-((weight*df[&#39;pounds&#39;][i])+bias))**2)
return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
if df.empty:
df = pd.DataFrame([[weight,bias,loss]],columns=[&#39;weight&#39;,&#39;bias&#39;,&#39;loss&#39;])
else:
df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
return df

def find_weight_slope(df,weight,bias):
weight_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
weight_update_summation.append(2*(wx_plus_b_minus_y*df[&#39;pounds&#39;][i]))
return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
bias_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
bias_update_summation.append(2*wx_plus_b_minus_y)
total_sum = sum(bias_update_summation)
return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
return old_bias-1*lr*slope

得出：
iter weight bias loss
0 0.00 0.00 303.0
0 1.20 0.35 170.0
0 2.06 0.60 102.0
0 2.67 0.79 67.0
0 3.10 0.93 50.0
0 3.41 1.04 41.0

这与提供的解决方案不同在网站上：
迭代权重偏差损失（MSE）
1 0 0 303.71
2 1.2 0.34 170.67
3 2.75 0.59 67.3
4 3.17 0.72 50.63
5 3.47 0.82 42.1
6 3.68 0.9 37.74
]]></description>
      <guid>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</guid>
      <pubDate>Fri, 23 Aug 2024 20:09:14 GMT</pubDate>
    </item>
    <item>
      <title>非正统的时间序列预处理方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</link>
      <description><![CDATA[我正在写一篇关于时间序列预处理的学校论文。在这篇论文中，我想测试时间序列预处理的非正统方法。然后我想将数据输入 LSTM 模型并测量其在预测时间序列方面的准确性。所以我想问问社区我应该测试哪些新颖/非正统的方法。我不想测试任何经典方法，如去噪、正则化、季节性消除等。如果您有任何想法，请随时分享。]]></description>
      <guid>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</guid>
      <pubDate>Thu, 22 Aug 2024 20:35:13 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能正确地连接不同的功能？</title>
      <link>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</link>
      <description><![CDATA[我有一个形状为 (100,48) 的特征图，以及其他形状为 (100,1) 的特征
我该如何正确地连接这些不同的特征？
我想这样做是因为我现在正在训练 XGboost 模型，但如果我将不同的特征直接连接在一起，机器就无法区分特征图和单个特征。
我尝试了下面的代码（来自 keras 函数 API），但我认为这不是正确的方法，每次我操作代码（没有随机种子）时，这样做的结果都不同。
input_1_layer = Input(shape=(input_1.shape[1],)) density_1 = Dense(48,activation=&#39;relu&#39;)(input_1_layer)

input_2_layer = Input(shape=(input_2.shape[1],)) density_2 = Dense(6,激活=&#39;relu&#39;)(输入层 2)

feature_extractor = Model(输入=[输入层 1, 输入层 2], 输出=merged)

new_features = feature_extractor.predict([输入层 1, 输入层 2])

你能提供任何论文或网站吗？]]></description>
      <guid>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</guid>
      <pubDate>Thu, 22 Aug 2024 07:19:22 GMT</pubDate>
    </item>
    <item>
      <title>根据 scikit-learn ColumnTransformer 访问用于归纳和规范化新数据的值</title>
      <link>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-based-upon-scikit-lea</link>
      <description><![CDATA[使用 scikit-learn，我在训练集上构建机器学习模型，然后在测试集上对其进行评估。在训练集上，我使用 ColumnTransformer 执行数据插补和缩放，然后使用 Kfold CV 构建逻辑回归模型，最终模型用于预测测试集上的值。最终模型还使用其来自 ColumnTransformer 的结果来插补测试集上的缺失值。例如，最小-最大标量将从训练集中获取最小值和最大值，并在缩放测试集时使用这些值。我如何才能看到这些从训练集中得出然后用于预测测试集的缩放值？我在 scikit-learn 文档中找不到有关它的任何信息。以下是我使用的代码：
来自 sklearn.linear_model 导入 SGDClassifier
来自 sklearn.model_selection 导入 RepeatedStratifiedKFold
来自 sklearn.model_selection 导入 GridSearchCV
来自 sklearn.compose 导入 ColumnTransformer
来自 sklearn.impute 导入 SimpleImputer
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.preprocessing 导入 MinMaxScaler、OneHotEncoder

def preprocessClassifierLR(categorical_vars, numeric_vars):###categorical_vars 和 numeric_vars 是定义 X 中存在的分类和数字变量的列名的列表

categorical_pipeline = Pipeline(steps=[(&#39;mode&#39;, SimpleImputer(missing_values=np.nan, strategies=&quot;most_frequent&quot;)),
(&quot;one_hot_encode&quot;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

numeric_pipeline = Pipeline(steps=[(&#39;numeric&#39;, SimpleImputer(strategy=&quot;median&quot;)),
(&quot;scaling&quot;, MinMaxScaler())])

col_transform = ColumnTransformer(transformers=[(&quot;cats&quot;, categorical_pipeline, categorical_vars),
(&quot;nums&quot;, numeric_pipeline, numeric_vars)])

lr = SGDClassifier(loss=&#39;log_loss&#39;, penalty=&#39;elasticnet&#39;)
model_pipeline = Pipeline(steps=[(&#39;preprocess&#39;, col_transform),
(&#39;classifier&#39;, lr)])

random_grid_lr = {&#39;classifier__alpha&#39;: [1e-1, 0.2, 0.5],
&#39;classifier__l1_ratio&#39;: [1e-3, 0.5]}

kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=47)

param_search = GridSearchCV(model_pipeline, random_grid_lr,scoring=&#39;roc_auc&#39;, cv=kfold, refit=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

param_search = preprocessClassifierLR(categorical_vars, numeric_vars)
train_mod = param_search.fit(X_train, y_train)
print(&quot;Mod AUC:&quot;, train_mod.best_score_)

test_preds = train_mod.predict_proba(X_)[:,1]

我无法提供真实数据，但 X 是一个包含独立变量的数据框，y 是二元结果变量。train_mod 是一个包含列变换器和 SGD 分类器步骤的管道。我可以通过运行 train_mod.best_params_ 轻松从分类器中获取类似的参数信息，例如最佳 lambda 和 alpha 值，但我无法找出用于列变换器的统计数据，例如 1) 用于分类特征的简单插补器的模式，2) 用于数字特征的简单插补器的中值，以及 3) 用于缩放数字特征的最小值和最大值。如何访问此信息？
我假设 train_mod.best_estimator_[&#39;preprocess&#39;].transformers_ 包含此信息，类似于 train_mod.best_params_ 为我提供从模型训练中得出的 alpha 和 lambda 值，然后将其应用于测试集。]]></description>
      <guid>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-based-upon-scikit-lea</guid>
      <pubDate>Wed, 21 Aug 2024 12:24:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用pandas将大数据块拆分为x_train和y_train数据以供机器学习？</title>
      <link>https://stackoverflow.com/questions/67367698/how-to-use-pandas-chunk-for-large-data-into-split-the-data-for-x-train-and-y-tra</link>
      <description><![CDATA[df_chunk=pd.read_csv(filename,chunk=1000)
X_train,Y_train,X_test,Y_test=train_test.split(df_chunk)

如何使用 df_chunk 将其拆分为 x 和 y 训练数据？]]></description>
      <guid>https://stackoverflow.com/questions/67367698/how-to-use-pandas-chunk-for-large-data-into-split-the-data-for-x-train-and-y-tra</guid>
      <pubDate>Mon, 03 May 2021 10:51:38 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow model.evaluate 给出的结果与训练得到的结果不同</title>
      <link>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</link>
      <description><![CDATA[我正在使用 tensorflow 进行多类分类
我以以下方式加载训练数据集和验证数据集
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

然后当我使用 model.fit() 训练模型
history = model.fit(
train_ds,
validation_data=val_ds,
epochs=epochs,
shuffle=True
)

我的验证准确率约为 95%。
但是当我加载相同的验证集并使用 model.evaluate() 时
model.evaluate(val_ds)

我的准确率非常低（约为 10%）。
为什么我得到的结果如此不同？我是否错误地使用了 model.evaluate 函数？
注意：在 model.compile() 中，我指定了以下内容，
优化器 - Adam，
损失 - SparseCategoricalCrossentropy，
指标 - 准确度
Model.evaluate() 输出
41/41 [================================] - 5s 118ms/step - 损失：0.3037 - 准确度：0.1032
测试损失 - 0.3036555051803589
测试准确度 - 0.10315627604722977

最后三个时期的 Model.fit() 输出
时期8/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.6094 - 准确度：0.8861 - val_loss：0.4489 - val_accuracy：0.9483
Epoch 9/10
41/41 [=============================] - 3s 80ms/步 - 损失：0.5377 - 准确度：0.8953 - val_loss：0.3868 - val_accuracy：0.9554
Epoch 10/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.4663 - 准确度：0.9092 - val_loss：0.3404 - val_accuracy：0.9590
]]></description>
      <guid>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</guid>
      <pubDate>Thu, 24 Sep 2020 15:26:53 GMT</pubDate>
    </item>
    <item>
      <title>MLflow：如何将实验状态返回为失败</title>
      <link>https://stackoverflow.com/questions/61112113/mlflow-how-to-return-experiment-status-as-failed</link>
      <description><![CDATA[我已经写出了我的代码，它具有以下形式 
def train(run_name, log_basepath, logger, parameters):

try:
metrics = training_functions(parameters) # &lt;---- 可能失败的代码

# 演示将属性加载到给定的运行
with mlflow.start_run(run_name=run_name):

# 日志参数
mlflow.log_params(parameters)

# 日志指标
mlflow.log_metrics(metrics)

# 定义实验标签
mlflow.set_tags(tags)

# 上传相关文件
mlflow.log_artifact(artifact_abspath)

# 主模型文件
mlflow.log_artifact(log_basepath)

except Exception as e:
logger.error(&#39;\n模型训练失败。返回以下错误:\n {} \n\n&#39;.format(e))
logger.info( &#39;第 {} 行错误&#39;.format(sys.exc_info()[-1].tb_lineno))

# 演示将属性加载到给定运行
使用 mlflow.start_run(run_name=run_name):
# 定义运行标签
logger.info(&#39;实验名称：{}&#39;.format(experiment_name))
logger.info(&#39;运行名称：{}&#39;.format(run_name))
logger.info(&#39;运行失败&#39;)

mlflow.log_artifact(log_basepath)

我的目标是如果发生故障，将日志发送到 mlflow 服务器。我的问题是，如何让 mlflow 将此标记为失败？]]></description>
      <guid>https://stackoverflow.com/questions/61112113/mlflow-how-to-return-experiment-status-as-failed</guid>
      <pubDate>Thu, 09 Apr 2020 00:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何确保训练阶段不会面临 OOM？</title>
      <link>https://stackoverflow.com/questions/58366819/how-to-make-sure-the-training-phase-wont-be-facing-an-oom</link>
      <description><![CDATA[根据我的经验，有两种 OOM 情况。一种是当您的模型和小批量所需的内存大于您拥有的内存时。在这种情况下，训练阶段将永远不会开始。解决这个问题的方法是使用较小的批量大小。尽管如果我能计算出我的硬件可以为某个特定模型管理的最大批量大小，那就太好了。但即使我第一次尝试找不到最大的批量大小，我也总能通过反复试验找到它（因为这个过程马上就失败了）。
我面临的 OOM 的第二种情况是当训练过程开始时，它会持续一段时间。甚至可能是几个时期。但后来由于某种未知原因，它面临 OOM。对我来说，这种情况令人沮丧。因为它可能随时发生，你永远不知道正在进行的训练是否会结束。到目前为止，我已经浪费了好几天的训练时间，而我以为一切都进展顺利。
我认为需要澄清一些问题。首先，我说的是带有 GPU 的个人计算机。其次，GPU 专用于计算，不用于显示。如果我错了，请纠正我，但我相信这意味着训练过程在不同时间点需要不同的内存大小。怎么会这样？再次，我如何确保我的训练阶段不会面临 OOM？
以这次运行为例：
3150/4073 [========================&gt;.......] - ETA：53:39 - 损失：0.3323
2019-10-13 21:41:13.096320：W tensorflow/core/common_runtime/bfc_allocator.cc:314] 分配器 (GPU_0_bfc) 在尝试分配 60.81MiB（四舍五入为 63766528）时内存不足。当前分配摘要如下。

经过三个小时的训练，TensorFlow 要求的内存超过了我的硬件可以提供的内存。我的问题是，为什么此时增加内存分配，而不是在进程开始时增加？
[更新]
鉴于 Eager 模式的已知问题，我将对我的案例进行一些说明。我没有在 Eager 模式下编码。我的训练代码如下所示：
strategy = tf.distribute.OneDeviceStrategy(device=&quot;/gpu:0&quot;)
training_dataset = tf.data.Dataset.from_tensor_slices(...)
validation_dataset = tf.data.Dataset.from_tensor_slices(...)

withstrategy.scope():
model = create_model()

model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;)

poc​​ket = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0.001,
patience=5, verbose=1,
restore_best_weights = True)

history = model.fit(training_dataset.shuffle(buffer_size=1000).batch(30),
epochs=3,
回调=[pocket]，
validation_data=validation_dataset.shuffle(buffer_size=1000).batch(30)，
workers=3，use_multiprocessing=True)
]]></description>
      <guid>https://stackoverflow.com/questions/58366819/how-to-make-sure-the-training-phase-wont-be-facing-an-oom</guid>
      <pubDate>Sun, 13 Oct 2019 18:58:25 GMT</pubDate>
    </item>
    <item>
      <title>训练期间改变模型</title>
      <link>https://stackoverflow.com/questions/36748574/changing-model-during-training</link>
      <description><![CDATA[我正在 TensorFlow 中创建一个模型，其中所有层都具有 relu 作为激活层。但是，当批处理大小增加到 500 时，我想更改模型，使输出层的倒数第二层具有 sigmoid 激活层。
我感到困惑的是，由于我在中间替换了优化器，我是否需要重新初始化所有变量？还是保留旧变量？]]></description>
      <guid>https://stackoverflow.com/questions/36748574/changing-model-during-training</guid>
      <pubDate>Wed, 20 Apr 2016 15:30:49 GMT</pubDate>
    </item>
    </channel>
</rss>