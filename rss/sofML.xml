<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 27 Aug 2024 12:29:44 GMT</lastBuildDate>
    <item>
      <title>YOLOv8 迁移学习：冻结层后预训练权重的损失</title>
      <link>https://stackoverflow.com/questions/78918374/yolov8-transfer-learning-loss-of-pretrained-weights-after-freezing-layers</link>
      <description><![CDATA[问题：经过训练后，模型可以准确预测纸板箱，但无法检测到训练前用于识别的任何其他物体。似乎预训练的权重已被覆盖或丢失。我的目标是保留检测其他物体的能力，同时专门针对纸板箱对模型进行微调。
我从 Roboflow 获得了正确标记的数据集。
https://universe.roboflow.com/dataset-t7hz7/cardboard-eupc8
问题：为什么模型在训练后失去了检测其他物体的能力，即使我已经冻结了最初的 10 层？如何保留 YoloV8 原有的物体检测功能，同时专注于识别纸板箱？
代码：
from ultralytics import YOLO

model = YOLO(&#39;yolov8n.pt&#39;)

model.train(
data=&#39;/Users/shubhamb/IdeaProjects/transfer-learning/data/data.yaml&#39;,
epochs=25,
imgsz=416,
freeze=10,
plots=True,
#device=&quot;mps&quot;,
name=&quot;custom_yolov8_model&quot;
)
]]></description>
      <guid>https://stackoverflow.com/questions/78918374/yolov8-transfer-learning-loss-of-pretrained-weights-after-freezing-layers</guid>
      <pubDate>Tue, 27 Aug 2024 10:20:25 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'DataLoader' 对象在 SuperGradients Trainer 中不可下标</title>
      <link>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</guid>
      <pubDate>Tue, 27 Aug 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>需要机器学习项目工作流程方面的帮助 [关闭]</title>
      <link>https://stackoverflow.com/questions/78917544/need-help-in-ml-project-workflow</link>
      <description><![CDATA[我正在做一个与日志相关的项目。我需要解析日志并将它们缩短为某种模式（日志不断出现）。然后我想用我在某些日志序列之后得到的错误日志来标记每个日志序列。问题是错误有很多种类型。我首先考虑对错误进行聚类，然后从中制作出一定数量的标签（聚类）。然后我想用它们的错误类型来标记非错误日志序列。然后我想用这些数据训练模型，以预测特定日志流可能发生的最可能错误。
有人可以补充和帮助吗？请给我任何你认为对我最好的建议，或者在必要时纠正我。
我正在尝试对所有错误进行聚类，因为我想要有限数量的标签来进行监督学习。我在缩短文本日志数据方面遇到了问题，因为它太大了。]]></description>
      <guid>https://stackoverflow.com/questions/78917544/need-help-in-ml-project-workflow</guid>
      <pubDate>Tue, 27 Aug 2024 07:07:20 GMT</pubDate>
    </item>
    <item>
      <title>Flutter 上的图像识别</title>
      <link>https://stackoverflow.com/questions/78917535/image-recognition-on-flutter</link>
      <description><![CDATA[我正在尝试开发一种算法，该算法以 PDF 文件作为输入并对其内容（例如文本、形状、图形、表格等）进行分类，并能够根据用户偏好对其进行重新排列。
我想知道是否有任何针对 Flutter 开发的图像分类包？我能找到的最接近的是 Tensorflow Lite，但我觉得应该还有其他可用的图像识别包。欢迎任何意见！！
提前谢谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78917535/image-recognition-on-flutter</guid>
      <pubDate>Tue, 27 Aug 2024 07:04:23 GMT</pubDate>
    </item>
    <item>
      <title>测试集和训练集的性能相似，但验证集上的性能差别很大[关闭]</title>
      <link>https://stackoverflow.com/questions/78917504/performance-of-the-test-set-and-the-training-set-is-similar-but-the-performance</link>
      <description><![CDATA[测试集和训练集的性能相似，但验证集的性能差异很大。
我使用的所有变量都与日期时间无关，但我在 sklearn 中训练的所有模型仍然面临这个问题。
例如，训练集中的 R2 = 0.7，测试集中的 R2 = 0.68。然而，当我在验证集中应用模型时，R2 下降到 0.5。
有什么原因或解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78917504/performance-of-the-test-set-and-the-training-set-is-similar-but-the-performance</guid>
      <pubDate>Tue, 27 Aug 2024 06:57:15 GMT</pubDate>
    </item>
    <item>
      <title>计算高效的国际象棋解算器</title>
      <link>https://stackoverflow.com/questions/78917392/compute-efficient-chess-solvers</link>
      <description><![CDATA[alphago/muzero 系列广为人知，但超人国际象棋解算器是在许多专门的 TPU 上训练的。
是否有计算效率高的国际象棋解算器的方法（或论文参考），即在使用一个标准 GPU 进行训练的约束下可以玩得相当好的国际象棋解算器？]]></description>
      <guid>https://stackoverflow.com/questions/78917392/compute-efficient-chess-solvers</guid>
      <pubDate>Tue, 27 Aug 2024 06:26:32 GMT</pubDate>
    </item>
    <item>
      <title>运行 plumber API 时返回多个值</title>
      <link>https://stackoverflow.com/questions/78916863/returning-multiple-values-when-running-plumber-api</link>
      <description><![CDATA[我使用 R 中的 caret 库创建了 xgboost 模型。当我在 plumber 中运行此模型时，Swagger 响应主体中会出现数据库的所有预测。我只想要一个预测。与模型变量的插入类别相对应的预测。下面是我的 API 示例
model &lt;- readRDS(&#39;model_xgb.rds&#39;)

#\* param var1
#\* param var2
#\* param var3
#\* param var4
#\* get/prediction

function(var1,var2,var3,var4)

{
data&lt;-
tibble(
var1=as.factor(var1),
var2=as.factor(var2),
var3=as.factor(var3),
var4=as.factor(var4)
)

predict(model,new_data=data, type=&#39;prob&#39;) %&gt;% pluck(1)

}

Plumber is返回：
[
{&quot;0&quot;:0.9152,
&quot;1&quot;:0.0848
},
{&quot;0&quot;:0.5379,
&quot;1&quot;:0.4621
},
{&quot;0&quot;:0.9912,
&quot;1&quot;:0.0088
}
.
.
.
]

我只想要一个结果]]></description>
      <guid>https://stackoverflow.com/questions/78916863/returning-multiple-values-when-running-plumber-api</guid>
      <pubDate>Tue, 27 Aug 2024 02:30:17 GMT</pubDate>
    </item>
    <item>
      <title>Fairmot 重新实现 reid 过度拟合？</title>
      <link>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</link>
      <description><![CDATA[我正在以一种非常简化的方式重新实现 Fairmot，我喜欢他们的工作，但我对机器学习还很陌生，我很难理解为什么在对 mot17、prw、cuhksysu 和部分加州理工学院行人进行训练时，reid 分支似乎立即过度拟合，而训练损失似乎以正常方式减少。
我想到了一些动机：

我的数据比原始项目少，所以模型在 reid 上过度拟合

我正在对他们的工作进行简化的增强，没有旋转，没有裁剪，所以这意味着我的数据更少，或者至少我的数据集中的变化更少

我搞砸了 id 损失，但我几乎复制粘贴了他们的函数，所以 :)

我使用的模型有点不同，可能对于任务来说太简单了（一直被教导更简单-&gt;更少的过度拟合，无论如何我使用双线性插值进行上采样而不是逆卷积，因为我无法消除棋盘效应）

验证代码实现得很糟糕，但我试图在训练集上进行验证，结果与训练一致损失

我搞砸了一些我无法理解的事情


我在这里发布了我正在重新实现模型的 colab 笔记本，如果你有空闲时间并愿意帮助我，那就太好了！
此外，如果我的代码正确，我正在模型部分加载 crowdhuman 上的预训练，并且我已经修改了数据集以具有不重叠的 id。
这是我的 colab，Google 限制了数据集的下载次数，无论如何它应该可以正常运行，如果不是，对不起。
https://colab.research.google.com/drive/1EVR3S6Qd0sFeRbhCYiZo5xtPZXYHsOvA?usp=sharing
哦，我在这里发布了我正在使用的 id 损失：
class IdLoss(nn.Module):
def init(self,n_id):
super(IdLoss, self).init()
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
self.id_classifier = nn.Linear(id_vect_size, int(n_id+1),device=device)
self.cross_entr_loss=nn.CrossEntropyLoss(ignore_index=-1)
self.dropout = nn.Dropout(0.3)

def forward(self, id_output, ind_target, mask_target, id_targets):
id_vectors = gather_feat(id_output, ind_target)
id_vectors = id_vectors[mask_target &gt; 0].contiguous()
id_vectors = F.normalize(id_vectors)
id_targets = id_targets[mask_target &gt; 0].contiguous()
id_logits = self.id_classifier(id_vectors)

return self.cross_entr_loss(id_logits, id_targets)

我尝试了空间 dropout、限制 reid 向量大小、添加增强、不同的数据集大小（但最多 45000 张图像）我添加了 50% 的对原始图像执行增强的可能性，我简化了模型，改变了学习率。无法使 reid 损失表现良好。]]></description>
      <guid>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</guid>
      <pubDate>Mon, 26 Aug 2024 21:44:44 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow-federated 0.86.0.. AttributeError: 模块‘tensorflow_federated.python.learning’没有属性‘build_federated_averaging_process</title>
      <link>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</link>
      <description><![CDATA[
AttributeError Traceback（最近一次调用最后一次）
在&lt;cell line: 1&gt;()
----&gt; 1 trainer = tff.learning.build_federated_averaging_process(
2 model_fn,
3 client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),
4 server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01
5 )
AttributeError: module &#39;tensorflow_federated.python.learning&#39; 没有属性 &#39;build_federated_averaging_process&#39;
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</guid>
      <pubDate>Mon, 26 Aug 2024 20:32:53 GMT</pubDate>
    </item>
    <item>
      <title>如何提高图像字幕模型字幕的质量和描述性</title>
      <link>https://stackoverflow.com/questions/78916109/how-to-improve-the-quality-and-descriptiveness-of-image-captioning-model-caption</link>
      <description><![CDATA[我正在开发一个服装图像字幕模型，并正在创建自己的训练数据集。目前它有大约 300 张图片，总共大约 1300 个字幕。我知道这个数据集很小，但在测试期间，模型输出的字幕是非描述性的，例如“短袖衬衫”，而大多数训练示例是“黑色短袖图案衬衫”或“带有白色地狱开始世界文字的黑色衬衫”，那么为什么模型的输出和训练示例会有如此大的差异。我也不清楚如何让模型预测衬衫上的文字，以进行样本外预测。
我的模型是这样的：
from tensorflow.keras.models import Model

input1 = Input(shape=(1920,)) 
input2 = Input(shape=(max_length,)) 

img_features = Dense(256,activation=&#39;relu&#39;)(input1) 
img_features = Dropout(0.5)(img_features)
img_features_flattened = Flatten()(img_features)
img_features_repeated = RepeatVector(max_length)(img_features_flattened)

sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2) 
sentence_features = Dropout(0.5)(sentence_features)

attention = MultiHeadAttention(
num_heads=8, # 注意力头的数量
key_dim=256 # 注意力键的维度
)(query=sentence_features, value=img_features_repeated, key=img_features_repeated)

context =tention

merged = concatenate([context, sentence_features])

sentence_features = LSTM(256)(merged)
x = Dropout(0.5)(sentence_features)

x = add([x, img_features_flattened])
x = Dense(128,activation=&#39;relu&#39;)(x)
x = Dropout(0.5)(x)

output = Dense(vocab_size,activation=&#39;softmax&#39;)(x)

caption_model = Model(inputs=[input1, input2], output=output)
caption_model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;)

这就是我定义训练和验证集的方式，这会导致模型无法预测样本之外的服装品牌的文本吗？
tokenizer.fit_on_texts(captions)
vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(caption.split()) for caption in captions)

images = data[&#39;image&#39;].unique().tolist()
nimages = len(images)

split_index = round(0.9*nimages)
train_images = images[:split_index]
val_images = images[split_index:]

train = data[data[&#39;image&#39;].isin(train_images)]
test = data[data[&#39;image&#39;].isin(val_images)]

train.reset_index(inplace=True,drop=True)
test.reset_index(inplace=True,drop=True)


我使用了以下方法：温度调整和核采样。这两种方法都改善了结果，但我仍在寻找更多方法]]></description>
      <guid>https://stackoverflow.com/questions/78916109/how-to-improve-the-quality-and-descriptiveness-of-image-captioning-model-caption</guid>
      <pubDate>Mon, 26 Aug 2024 19:58:39 GMT</pubDate>
    </item>
    <item>
      <title>Python文本检测OpenCV + Roboflow OCR相机性能非常滞后问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78913244/python-text-detection-opencvroboflow-ocr-camera-performance-very-lag-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78913244/python-text-detection-opencvroboflow-ocr-camera-performance-very-lag-problem</guid>
      <pubDate>Mon, 26 Aug 2024 07:22:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在决策树中使用直方图实现分箱条件？</title>
      <link>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78911846/how-to-implement-a-condition-for-binning-using-histogram-in-decision-tree</guid>
      <pubDate>Sun, 25 Aug 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>如何使用文本数据集训练模型</title>
      <link>https://stackoverflow.com/questions/78865593/how-to-train-a-model-using-a-text-dataset</link>
      <description><![CDATA[我想创建一个生成文本的 AI 模型。具体来说，BDD Gherkin 黄瓜场景和步骤定义基于用户故事的输入。
带有 BDD Gherkin 黄瓜示例的用户故事
例如。
用户故事（输入）：我想在电子商务网站上将产品添加到我的购物篮中进行购买。
输出：自动创建测试用例场景和步骤定义
测试用例场景：

场景 1：验证用户是否可以将一个商品添加到购物车
场景 2：验证用户是否可以从购物车中移除一个商品

测试用例场景 1：

假设用户使用和启动并登录电子商务应用程序
然后用户导航到商品页面。
然后用户选择并单击。
然后用户单击“添加到购物车”按钮。
然后用户应导航到购物车页面。
然后用户应验证购物车页面已成功添加。

测试用例场景 2：

假设用户使用 启动并登录电子商务应用程序 
然后用户应导航到购物车页面。
然后用户在购物车中找到并单击“从购物车中移除”按钮。
然后用户应验证购物车已成功移除。

我创建了一个示例数据集，其中包含映射到场景和步骤定义的用户故事。
数据集
就我目前的理解，逻辑是：我想基于现有用户故事和场景的数据集训练一个模型。在模型训练完成后，我想输入一个用户故事，模型应该提出一个带有步骤定义的合适场景。
我是机器学习的新手，只做过某种形式的监督学习、回归。从一些研究中，我需要使用一些 NLP 技术来处理数据集。从那时起，我就很迷茫。我看到一些人谈论使用 ChatGPT 来训练数据集之类的东西。
做这个项目的好方法是什么。
本质上，我想找出如何使用文本训练模型，以便模型可以接收文本并输出文本。]]></description>
      <guid>https://stackoverflow.com/questions/78865593/how-to-train-a-model-using-a-text-dataset</guid>
      <pubDate>Tue, 13 Aug 2024 09:59:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False), Dense(64,activation=&#39;relu&#39;), Dense(1) ]) # 模型编译器 model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差 # 模型摘要 model.summary() # 模型模型嵌入 model.fit( X_train, Y_train, epochs=40, batch_size=32,validation_data=(X_test, Y_test)) # 新预测示例预测= model.predict(X_test) print(prediction)  结果： 模型：“sequential”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 数的增加而改善。这就是问题所在，我不明白发生了什么。
我反复检查了我的数据，X_train 已标准化，我尝试在模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。我真的想尝试找到一种方法来使用深度学习来找到它。]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    </channel>
</rss>