<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 28 Sep 2024 01:19:58 GMT</lastBuildDate>
    <item>
      <title>当步长为小数时，会向下舍入吗？</title>
      <link>https://stackoverflow.com/questions/79033026/do-steps-round-down-when-its-fractional</link>
      <description><![CDATA[我开始尝试训练机器学习模型，并对训练过程中的时期和步骤概念感到困惑。在网上搜索时，我偶然发现了一个与时期、步骤和批次大小相关的公式 (𝜎 = (𝜀 × 𝜂) ÷ 𝛽)。将此公式应用于我自己的数据集会得到小数个步骤，这让我对实际中通常如何处理步骤产生了疑问。我不确定小数步骤是否向下舍入，或者这如何准确地转化为实际的训练过程。我缺乏实施训练循环的实践经验，因此很难直观地掌握这些概念如何映射到现实世界的模型训练场景。
为了更好地理解 epoch、steps 和 batch size 之间的关系，我尝试将我找到的公式 (𝜎 = (𝜀 × 𝜂) ÷ 𝛽) 应用于数据集（这只是一个理论示例数据集）：
total_samples = 10000 # 我的数据集中的样本总数
batch_size = 32 # 我计划使用的 batch size
epochs = 10 # 我想要训练的 epoch 数量

steps_per_epoch = total_samples / batch_size
total_steps = (epochs * total_samples) / batch_size

print(f&quot;Steps per epoch: {steps_per_epoch}&quot;)
print(f&quot;Total步骤：{total_steps}&quot;)

这产生了以下输出：
每轮步骤：312.5
总步骤：3125.0

每轮步骤的分数结果（312.5）让我不确定这将如何在实际训练循环中实现。具体来说：

在实践中，分数步骤通常会向下舍入吗？
如果发生舍入，这是否意味着每个时期可能会跳过一些数据样本？
常见的机器学习框架如何处理这种情况？

我还没有真正实现训练循环，所以我不确定这些分数步骤将如何在代码中处理。我的主要困难是弥合理论计算与其在模型训练中的实际应用之间的差距。]]></description>
      <guid>https://stackoverflow.com/questions/79033026/do-steps-round-down-when-its-fractional</guid>
      <pubDate>Fri, 27 Sep 2024 22:03:59 GMT</pubDate>
    </item>
    <item>
      <title>尝试保存自定义 Keras 模型时出现“TypeError：不支持的整数大小 (0)”</title>
      <link>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</guid>
      <pubDate>Fri, 27 Sep 2024 19:14:51 GMT</pubDate>
    </item>
    <item>
      <title>加载变压器时出现问题；ModuleNotFoundError：没有名为“transformers”的模块</title>
      <link>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</link>
      <description><![CDATA[我想使用 huggingface 提供的一些模型。我甚至在开始的时候都遇到了最大的困难。有人能帮我识别和解决这个问题吗？
我正在使用 Kubuntu 24.04。

首先，我创建并激活一个虚拟环境，在其中安装变压器。
python3 -m venv .env
source .env/bin/activate

这是成功的，因为现在我在 Visual Code Studio 中的终端有前缀“(.env)”。
接下来，我从 github 安装最新的变压器：
pip install git+https://github.com/huggingface/transformers

输出成功。然后，我使用 hugginface.co 上推荐的方法测试其成功率：
python3 -c &quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I love you&#39;))&quot;

输出对我来说看起来正确：
未提供模型，默认为 distilbert/distilbert-base-uncased-finetuned-sst-2-english 和修订版本 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)。
不建议在生产中使用未指定模型名称和修订版本的管道。
硬件加速器（例如 GPU）在环境中可用，但没有将“设备”参数传递给“管道”对象。模型将在 CPU 上。
[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998656511306763}]

从那里，我尝试运行以下代码：
from transformers import pipeline

但每次我都会得到以下输出：
/bin/python3 /path-to/main.py
回溯（最近一次调用最后一次）：
文件&quot;/path-to/main.py&quot;，第 5 行，在&lt;module&gt;
from transformers import pipeline
ModuleNotFoundError：没有名为“transformers”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</guid>
      <pubDate>Fri, 27 Sep 2024 15:09:24 GMT</pubDate>
    </item>
    <item>
      <title>无法从 coqui-tts 的 tts 库生成语音，并且此错误在单人和多人说话时都会发生</title>
      <link>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</link>
      <description><![CDATA[从 TTS.utils.manage 导入 ModelManager
从 TTS.utils.synthesizer 导入 Synthesizer
从 google.colab 导入文件
初始化模型管理器并加载模型
model_name = &quot;tts_models/en/ljspeech/tacotron2-DDC&quot;
vocoder_name = &quot;vocoder_models/en/ljspeech/hifigan_v2&quot;
model_manager = ModelManager()

model_path, config_path, _ = model_manager.download_model(model_name)

vocoder_path, vocoder_config_path, _ = model_manager.download_model(vocoder_name)

创建合成器对象

synthesizer = Synthesizer(model_path, config_path, vocoder_path, vocoder_config_path, use_cuda=False)

根据手动选择的情绪生成动态 SSML

def generate_dynamic_ssml(chunk):

ssml = f&quot;&quot;&quot;&lt;speak version=&#39;1.0&#39; xmlns=&#39;http://www.w3.org/2001/10/synthesis&#39; xml:lang=&#39;en-US&#39;&gt;&quot;&quot;&quot; 

# 取消注释您想要应用的情感

ssml += f&quot;&lt;prosody pitch=&#39;+10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # happy

# ssml += f&quot;&lt;prosody pitch=&#39;+5%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # romantic

# ssml += f&quot;&lt;prosody pitch=&#39;+7%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 充满希望

# ssml += f&quot;&lt;prosody pitch=&#39;default&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;none&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 中立

# ssml += f&quot;&lt;prosody pitch=&#39;-5%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 失望

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 生气

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 害怕

# ssml += f&quot;&lt;prosody pitch=&#39;-15%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 悲伤

# ssml += f&quot;&lt;prosody pitch=&#39;-20%&#39; rate=&#39;very slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # devastated

ssml += &quot;&lt;/speak&gt;&quot; 

return ssml 

为每个块合成语音的函数

def synthesize_speech(text):

# 使用手动选择的情绪为整个文本生成 SSML 

ssml = generate_dynamic_ssml(text) 

# 合成文本 

wav = synthesizer.tts(ssml) 

# 保存输出文件 

output_file = &quot;output_with_emotion.wav&quot; 

synthesizer.save_wav(wav, output_file) 

# 将文件下载到本地机器 

files.download(output_file) 

示例用法

if name == &quot;main&quot;:

sample_text = &quot;&quot;&quot;我很高兴见到你！你让我的心因喜悦和爱而跳动。&quot;&quot;&quot; 

# 使用所选情绪将文本转换为语音

synthesize_speech(sample_text)

使用 tacotron2 模型和 vocoder hifigen 编写代码，之后我使用合成器库 ssml 来改变声音的音调，使其像情绪一样
AttributeError Traceback (most recent call last)

&lt;ipython-input-8-2698293d91f3&gt; in &lt;cell line: 51&gt;()

53 

54 # 使用所选情绪将文本转换为语音

---&gt; 55 synthesize_speech(sample_text)

1 帧

&lt;ipython-input-8-2698293d91f3&gt;在 synthesize_speech(text) 中

39 

40 # 合成文本 

---&gt; 41 wav = synthesizer.tts(ssml)

42 

43 # 保存输出文件 

/usr/local/lib/python3.10/dist-packages/TTS/utils/synthesizer.py 在 tts(self, text, Speaker_name, Language_name, Speaker_wav, Style_wav, Style_text, Reference_wav, Reference_speaker_name, Split_sentences, **kwargs) 中

320 Speaker_id = self.tts_model.Speaker_manager.name_to_id[Speaker_name] 

321 # 处理单扬声器的 Neon 模型。

--&gt; 322 elif len(self.tts_model.speaker_manager.name_to_id) == 1:

323 Speaker_id = list(self.tts_model.speaker_manager.name_to_id.values())[0] 

324 elif not Speaker_name and not Speaker_wav: 

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;name_to_id&#39;，这是错误


该 tts 代码使用 coqui-tts 制作情感 tts，因此总是出现以下错误，我试图使代码成为单个和多个扬声器
制作单个和多个扬声器，也许有人知道解决方案或建议我使用另一个模型，我想要一个免费的模型]]></description>
      <guid>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</guid>
      <pubDate>Fri, 27 Sep 2024 12:00:55 GMT</pubDate>
    </item>
    <item>
      <title>如何才能准确填补数据集中的缺失值？</title>
      <link>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</link>
      <description><![CDATA[我正在处理一个包含二手车详细信息的数据集，我发现 Fuel_Type 列中缺少几个值。可能的值包括“汽油”、“E85 混合燃料”、“混合动力”、“柴油”等。目前，我的数据中有超过 4,000 辆电动汽车、不到 50 辆汽油车和一些缺少 Fuel_Type 条目的混合动力汽车。此外，一些条目包含非标准值，如“–”和“不支持”。准确填充这些缺失值对我的分析至关重要，因为它们会显著影响结果。
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# 示例 DataFrame
data = {
&#39;Car&#39;: [&#39;Toyota&#39;, &#39;Honda&#39;, &#39;Tesla&#39;, None, &#39;Ford&#39;],
&#39;Fuel_Type&#39;: [&#39;Gasoline&#39;, &#39;E85 Flex Fuel&#39;, np.nan, &#39;Hybrid&#39;, None],
&#39;Transmission&#39;: [&#39;Automatic&#39;, None, &#39;Automatic&#39;, &#39;Manual&#39;, &#39;Manual&#39;]
}

df = pd.DataFrame(data)

# 初始插补尝试
imputer = SimpleImputer(strategy=&#39;most_frequent&#39;)
df[&#39;Fuel_Type&#39;] = imputer.fit_transform(df[[&#39;Fuel_Type&#39;]])
print(df)
]]></description>
      <guid>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</guid>
      <pubDate>Fri, 27 Sep 2024 07:15:40 GMT</pubDate>
    </item>
    <item>
      <title>当有多个场景切换时，有没有办法让 SAM2 跨场景跟踪同一个人？</title>
      <link>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</link>
      <description><![CDATA[使用 Meta 的 SAM2 演示，当场景切换时，面具通常会切换到不同的玩家身上。
我知道手动重新标记每个场景中的玩家是一种选择，但我想探索是否有可用的自动化解决方案。

我尝试使用 Meta 的 SAM2 演示，网址为 https://sam2.metademolab.com/
我希望它能在整个视频中跟踪勒布朗
我发现它只在第一个场景中这样做，偶尔当勒布朗是镜头中唯一的人或主要人物时
]]></description>
      <guid>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</guid>
      <pubDate>Fri, 27 Sep 2024 04:47:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在 rust 中导入用 Python 制作的 ML 模型 (.pkl) 吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</link>
      <description><![CDATA[我之前用 Python 构建了一个项目，但由于它占用了太多资源并且缺乏并发性，所以我改用了 rust。现在我不知道如何正确迁移它，大多数代码已经迁移，但我无法导入导出为 .pkl 文件的 ml 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</guid>
      <pubDate>Fri, 27 Sep 2024 04:43:46 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练推荐系统算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</link>
      <description><![CDATA[我一直收到以下错误：

ratings_matrix[which_train, ] 中的错误：维度数不正确

&gt; which_train &lt;- sample(x = c(TRUE, FALSE),
+ size = (ratings_matrix),
+ replace = TRUE,
+ prob = c(0.8, 0.2))
&gt; recc_data_train &lt;- ratings_matrix[which_train, ]
]]></description>
      <guid>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</guid>
      <pubDate>Thu, 26 Sep 2024 23:51:08 GMT</pubDate>
    </item>
    <item>
      <title>如何保存/查看树状图中的信息？</title>
      <link>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</link>
      <description><![CDATA[我正在尝试分析数据以根据树状图确定结果。问题是我主要有 2 组数据“H”和“U”，两者一起进行分析。在树状图的末尾，我需要知道哪个区域的“H”更多和“U”。
我尝试使用树状图的字典信息，但当我打开函数时，我注意到它根本没有包含我通过链接输入的信息。
以下是我正在使用的代码：
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
from scipy.cluster import Hierarchy
import pandas as pd
from sklearn.decomposition import PCA

df=pd.read_csv(name+&quot;.csv&quot;, index_col=None, header=0)

normalized_df=(df-df.mean())/df.std()

pca=PCA(n_components=6, n_oversamples=6)
principalComponents = pca.fit_transform(df)
principalDF = pd.DataFrame(data = principalComponents, columns = [&#39;principal component 1&#39;, &#39;principal component 2&#39;, &#39;principal component 3&#39;, &#39;principal component 4&#39;, &#39;principal component 5&#39;, &#39;principal component 6&#39;])

#将类型 (U/H) 添加到数据框
finalDF = pd.concat([principalDF, full_df[[&#39;type&#39;]]], axis = 1)

#从这里开始，我开始遇到问题
data = list(zip(finalDF[&#39;principal component 1&#39;], finalDF[&#39;principal component 2&#39;]))

linkage_data = linkage(data, method=&#39;ward&#39;, metric=&#39;euclidean&#39;)
hierarchy.set_link_color_palette([&#39;r&#39;,&#39;g&#39;,&#39;b&#39;,&#39;w&#39;])
den=dendrogram(linkage_data)
plt.title(&quot;Attempt #1&quot;)
plt.show()

树状图看起来与预期一致。问题是我无法区分每个点的类型（H 或 U）。
如前所述，我尝试查看树状图的属性，例如 icoord 和 dcoord，但我无法弄清楚它的含义。
我使用的数据本身是 6 个不同的列，具有不同的数字，这是我标准化的数据((df-df.mean())/df.std)，然后取主成分。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</guid>
      <pubDate>Thu, 26 Sep 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 gpu 上运行 Qwen2-VL 模型？</title>
      <link>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</link>
      <description><![CDATA[我有 4 个 gpu，我想运行 Qwen2 VL 模型，但我收到“设备端断言已触发。使用 TORCH_USE_CUDA_DSA 进行编译以启用设备端断言”错误。
model_name=&quot;Qwen/Qwen2-VL-2B-Instruct&quot;
model = Qwen2VLForConditionalGeneration.from_pretrained(
model_name, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)
model = nn.DataParallel(model)
processor = AutoProcessor.from_pretrained(model_name)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;image&quot;,
&quot;image&quot;: file
},
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;&quot;&quot;描述图像&quot;&quot;&quot;
}
]
}
]
text = processing.apply_chat_template(
messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processing(
text=[text],
images=image_inputs,
videos=video_inputs,
padding=True,
return_tensors=&quot;pt&quot;,
)
使用 torch.no_grad():
generated_ids = model.module.generate(**inputs, max_new_tokens=128)

但我总是得到：
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [31,0,0] 断言 `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` 失败。
错误：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

回溯（最近一次调用）：
文件“/home/ubuntu/projects/mistral-qaC/services/VisionService.py”，第 104 行，位于 ask_vision
generated_ids = self.model.module.generate(
^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/utils/_contextlib.py”，第 116 行，位于 decorate_context
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py”，第2015，在 generate 中
result = self._sample(
^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py&quot;，第 2965 行，在 _sample 中
output = self(**model_inputs, return_dict=True)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/accelerate/hooks.py&quot;，第 169 行，在 new_forward 中
output = module._old_forward(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py&quot;，第 1598 行，正向
输入_embeds[image_mask] = image_embeds
~~~~~~~~~~~~~^^^^^^^^^^^^
RuntimeError：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

我尝试了什么？

使用 CUDA_LAUNCH_BLOCKING=1 python script.py 运行我的 python 脚本，但它也不起作用。
打印输入和模型设备：模型设备：cuda:0
输入设备：cuda:0
torch.cuda.synchronize() 和 torch.cuda.empty_cache() 在生成之前。

输入的形状：

input_ids 的形状：torch.Size([1, 759])
attention_mask 的形状：torch.Size([1, 759])
 pixel_values: torch.Size([2940, 1176])
image_grid_thw 的形状：torch.Size([1, 3])

我的 transformers 和 pytorch 版本是：
transformers==4.45.0.dev0
torch==2.4.1+cu124

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</guid>
      <pubDate>Thu, 26 Sep 2024 11:21:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分类神经网络中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</link>
      <description><![CDATA[嗨，我正在做一个关于体积脑图像的机器学习项目
但我没有 ML 背景（有一点编码……但不多），所以我一直在用 pytorch lightning 学习 ML。这很有趣，但有时很难……
所以我现在的问题是我的模型在很大程度上对数据集过度拟合，这使得我在训练中的三个类准确度达到每类 0.85 - ~1.0 之间的准确度，并且损失以良好的曲线下降。遗憾的是验证准确度不足。对于第一类，准确度稳定在 0.05 左右，而其他类稳定在 0.3 左右徘徊。此外，损失从 1.2 略微下降到略高于 1。
（一些医学内容，让您了解数据重叠问题所在）
这些类别的图像是接受 CT 灌注的患者的灰度图像。它显示了患者动脉闭塞或阻塞时大脑灌注的变化。
对于非医学方面的人来说，你可以把它看作是一棵树的树枝，上面有叶子。如果树枝有阻塞，那么该树枝上的叶子就会枯萎。
我的类别是 ICA-T、M1 和最后一类 M2
ICA-T 分支在 M1 中。这意味着体积图像可能包含这些片段的信号相似性。所以也许模型认为由于重叠，ICA-T 病例被猜测为 M1??
这就是类别背景。原始数据集总体如下：
类别 0 (ICA-T)：94，
类别 1 (M1)：366，
类别 2 (M2)：119
总计 579
这显然在类别 1 中占比过高。我对此有疑问，因此我将类别 1 减少到 157，没有考虑任何策略。
0 类 (ICA-T)：94，
1 类 (M1)：157，
2 类 (M2)：119
总计 370
我有一个训练数据集、验证数据集和测试数据集，这些数据集是我通过分层随机分割获得的，这给了我
训练：259 - 0 类：66，1 类：110，2 类：83
验证：37 - 0 类：9，1 类：16，2 类：12
测试：74 - 0 类：19，1 类：31，2 类：24
进一步的信息是，图像是体积图像，空间大小为 256,256,32。
我认为问题在于类相似性。但我不确定是否如此。
有人遇到过类似的问题吗？他们解决了吗？或者有人可以指导我找到答案吗？
遗憾的是我的项目很快就要完成了，所以我没有时间。如果有人需要代码，请询问，但我认为这对我的问题来说没有必要？..
这是我使用的结构：
class CNN5_Mod(L.LightningModule): # 模型定义
def __init__(self, num_classes):
super(CNN5_Mod, self).__init__()
from Project1.MyFile.ConfigMain import Config
config = Config()
# 特征 - x y z
# 1 - 256 256 32
&quot;&quot;&quot; Block 1 &quot;&quot;&quot;
self.conv1 = nn.Conv3d(1, config.c1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1) # 16 - 256 256 32
self.relu1 = nn.ReLU()
self.bt_nm1 = nn.BatchNorm3d(config.c1)

self.conv2 = nn.Conv3d(config.c1, config.c2, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 - 128 128 16
self.relu2 = nn.ReLU()
self.bt_nm2 = nn.BatchNorm3d(config.c2)

self.dropout1 = nn.Dropout3d(config.dropout1)
self.pooling1 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (64, 64,64,8)

&quot;&quot;&quot; 块 2 &quot;&quot;&quot;
self.conv3 = nn.Conv3d(config.c2, config.c3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 32 4
self.relu3 = nn.ReLU()
self.bt_nm3 = nn.BatchNorm3d(config.c3)

self.conv4 = nn.Conv3d(config.c3, config.c4, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 16 16 2
self.relu4 = nn.ReLU()
self.bt_nm4 = nn.BatchNorm3d(config.c4)

self.dropout2 = nn.Dropout3d(config.dropout2)
self.pooling2 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (256, 8,8,1)

&quot;&quot;&quot; 扁平化 &quot;&quot;&quot;
self.flatten = nn.Flatten(1)

&quot;&quot;&quot; 隐藏层 &quot;&quot;&quot;
self.fc1 = nn.Linear(config.in_fc, 64)
self.relu5 = nn.ReLU()
self.fc_dropout = nn.Dropout3d(config.fc_dropout)

&quot;&quot;&quot; 输出 &quot;&quot;&quot;
self.fc2 = nn.Linear(64, num_classes)

&quot;&quot;&quot; 预测 &quot;&quot;&quot;
self.softmax = nn.Softmax(dim=1)

我改变了 dropout rate 和其他一些值。但我不知所措。。]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>使用斑点检测来计数黑色圆形种子[关闭]</title>
      <link>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</link>
      <description><![CDATA[我想玩一下 OpenCV 或类似技术，以便能够计算简单的黑色球体（种子，在本例中为拟花椒）。
其他时候，种子中间的白色反射较少，但主要特征是它是“圆形”的、黑色的，几乎总是具有相同的尺寸，并且可以（或有时没有）一个白色的小斑点。





我应该从哪里开始才能让 5 张照片的种子数量大致相同（或者最好是完全相同）？（种子数量相同，我只是在拍摄照片之间摇晃了一下容器）
CV 还是 ML？从哪里开始？
附言：如果有帮助，我也可以尝试物理去除较小的黑色棍子和不好的种子……但理论上，如果可以有可靠的方法可以忽略这些小的“非种子”暗元素，那就太好了……
附言：如果这也能有帮助，我还可以修改拍照的方式……]]></description>
      <guid>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</guid>
      <pubDate>Mon, 23 Sep 2024 21:30:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 进行 OCR 和文本检测和识别</title>
      <link>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</link>
      <description><![CDATA[我正在开发一个测试应用程序，使用 Google Collab 在 Python 中开发一个小型文本检测和识别应用程序。您能提供一些代码示例来实现这一点吗？我的要求是我应该能够使用 OpenCV 检测和识别图像中的文本。
请提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</guid>
      <pubDate>Wed, 20 Oct 2021 13:40:59 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>真实世界参数优化</title>
      <link>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</link>
      <description><![CDATA[我需要对我的最新研究项目进行参数优化。我有一个算法，目前有 5 个参数（四个双精度 [0,1] 和一个具有 3 个值的名义参数）。该算法使用这些参数来计算一些东西，然后我计算准确率、召回率和 FMeasure。一次运行大约需要 1.8 秒。目前，我正在以 0.1 的步长遍历每个参数，这向我展示了全局最大值的大致位置。但我想找到精确的全局最大值。我研究过梯度下降，但我真的不知道如何将其应用于我的算法（如果可能的话）。有人可以指导我如何实现这样的算法吗，因为我对这类工作很陌生。
干杯，
丹尼尔]]></description>
      <guid>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</guid>
      <pubDate>Sun, 23 Dec 2012 17:55:54 GMT</pubDate>
    </item>
    </channel>
</rss>