<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 28 Sep 2024 18:20:58 GMT</lastBuildDate>
    <item>
      <title>如何解决 torchmeta 冲突</title>
      <link>https://stackoverflow.com/questions/79034188/how-to-fix-torchmeta-conflicts</link>
      <description><![CDATA[我正在尝试在 google colab 中使用 !pip install torchmeta 命令安装 torchmeta。但显示以下错误：
错误：无法安装 torchmeta==1.1.0、torchmeta==1.1.1、torchmeta==1.2.0、torchmeta==1.2.1、torchmeta==1.2.2、torchmeta==1.3.0、torchmeta==1.3.1、torchmeta==1.3.2、torchmeta==1.3.3、torchmeta==1.3.4、torchmeta==1.4.0、torchmeta==1.4.1、torchmeta==1.4.2、torchmeta==1.4.3、torchmeta==1.4.4、torchmeta==1.4.5、torchmeta==1.4.6， torchmeta==1.5.0、torchmeta==1.5.1、torchmeta==1.5.2、torchmeta==1.5.3、torchmeta==1.6.0、torchmeta==1.6.1、torchmeta==1.7.0 和 torchmeta==1.8.0，因为这些软件包版本存在依赖冲突。

冲突的原因是：
torchmeta 1.8.0 依赖于 torch&lt;1.10.0 和 &gt;=1.4.0
torchmeta 1.7.0 依赖于 torch&lt;1.9.0 和 &gt;=1.4.0
torchmeta 1.6.1 依赖于 torch&lt;1.8.0 和 &gt;=1.4.0
要解决此问题，您可以尝试：
1. 放宽您指定的软件包版本范围
2. 删除软件包版本以允许 pip 尝试解决依赖项冲突

如何解决此问题？我也尝试安装较低版本的 pytorch，但我不能，有什么方法可以安装 torchmeta。我使用的是 windows (google colab) Pytorch-geometric 2.3.1]]></description>
      <guid>https://stackoverflow.com/questions/79034188/how-to-fix-torchmeta-conflicts</guid>
      <pubDate>Sat, 28 Sep 2024 12:32:30 GMT</pubDate>
    </item>
    <item>
      <title>安装 CausalML 时遇到问题</title>
      <link>https://stackoverflow.com/questions/79033786/trouble-installing-causalml</link>
      <description><![CDATA[我尝试使用 Python 3.12 在我的 Windows 机器上安装 causalml，命令为 pip install causalml。
但在尝试为 causalml 构建 wheel 时安装失败。以下是错误片段：
注意：此错误源自子进程，可能不是 pip 的问题。
错误：无法为 causalml 构建 wheel
无法构建 causalml
错误：错误：无法为某些基于 pyproject.toml 的项目 (causalml) 构建可安装的 wheel

我已将错误附加到 pastebin 上：
https://pastebin.com/dehRfgrk
但关键错误消息是：
use_tracing&#39;：不是 causalml/inference/tree/_tree/_tree.cpp 中“_PyCFrame”的成员。
命令“cl.exe”失败，退出代码为 2。
关于弃用的 NumPy API 和 Python 2.7 选项使用的警告（bdist_wheel.universal 已弃用）。
Setuptools 警告包配置中缺少包（causalml.inference.tree 等）

我正在使用：
Python 版本：3.12。
操作系统：Windows 10。
编译器：Microsoft Visual Studio 2022 构建工具。
环境：Anaconda 3。
NumPy 版本：1.26.4。
我尝试更新 Visual Studio 构建工具并安装了最新版本并确保包含 C++ 构建工具。]]></description>
      <guid>https://stackoverflow.com/questions/79033786/trouble-installing-causalml</guid>
      <pubDate>Sat, 28 Sep 2024 08:47:08 GMT</pubDate>
    </item>
    <item>
      <title>当步长为小数时，会向下舍入吗？</title>
      <link>https://stackoverflow.com/questions/79033026/do-steps-round-down-when-its-fractional</link>
      <description><![CDATA[我开始尝试训练机器学习模型，并对训练过程中的时期和步骤概念感到困惑。在网上搜索时，我偶然发现了一个与时期、步骤和批次大小相关的公式 (𝜎 = (𝜀 × 𝜂) ÷ 𝛽)。将此公式应用于我自己的数据集会得到小数个步骤，这让我对实际中通常如何处理步骤产生了疑问。我不确定小数步骤是否向下舍入，或者这如何准确地转化为实际的训练过程。我缺乏实施训练循环的实践经验，因此很难直观地掌握这些概念如何映射到现实世界的模型训练场景。
为了更好地理解 epoch、steps 和 batch size 之间的关系，我尝试将我找到的公式 (𝜎 = (𝜀 × 𝜂) ÷ 𝛽) 应用于数据集（这只是一个理论示例数据集）：
total_samples = 10000 # 我的数据集中的样本总数
batch_size = 32 # 我计划使用的 batch size
epochs = 10 # 我想要训练的 epoch 数量

steps_per_epoch = total_samples / batch_size
total_steps = (epochs * total_samples) / batch_size

print(f&quot;Steps per epoch: {steps_per_epoch}&quot;)
print(f&quot;Total步骤：{total_steps}&quot;)

这产生了以下输出：
每轮步骤：312.5
总步骤：3125.0

每轮步骤的分数结果（312.5）让我不确定这将如何在实际训练循环中实现。具体来说：

在实践中，分数步骤通常会向下舍入吗？
如果发生舍入，这是否意味着每个时期可能会跳过一些数据样本？
常见的机器学习框架如何处理这种情况？

我还没有真正实现训练循环，所以我不确定这些分数步骤将如何在代码中处理。我的主要困难是弥合理论计算与其在模型训练中的实际应用之间的差距。]]></description>
      <guid>https://stackoverflow.com/questions/79033026/do-steps-round-down-when-its-fractional</guid>
      <pubDate>Fri, 27 Sep 2024 22:03:59 GMT</pubDate>
    </item>
    <item>
      <title>Keras 未显示正确的 f1_score</title>
      <link>https://stackoverflow.com/questions/79032877/keras-not-showing-the-correct-f1-score</link>
      <description><![CDATA[我使用来自 Kaggle 的以下数据集 卫星图像中的船舶
我使用的代码是：
import os
import cv2
import numpy as np
from keras.applications import ResNet50
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras.models import Model

IMG_PATH = &quot;/kaggle/input/ships-in-satellite-imagery/shipsnet/shipsnet&quot;
IMG_WIDTH = 80
IMG_HEIGHT = 80
CHANNELS = 3

X_train = []
y_train = []
y_train_bbox = []
对于 os.listdir(IMG_PATH)[:3000] 中的文件：
img_annotations = os.path.splitext(file)[0].split(&#39;_&#39;)
label = int(img_annotations[0])
longitude = float(img_annotations[-2])
latitude = float(img_annotations[-1])
img = cv2.imread(os.path.join(IMG_PATH, file))
X_train.append(img)
y_train.append(label)
y_train_bbox.append(np.array([longitude, latitude]))
X_train = np.array(X_train)
y_train = np.array(y_train).reshape(-1,1)
y_train_bbox = np.array(y_train_bbox)

X_train = []
y_train = []
y_train_bbox = []
对于 os.listdir(IMG_PATH)[:3000] 中的文件：
img_annotations = os.path.splitext(file)[0].split(&#39;_&#39;)
label = int(img_annotations[0])
longitude = float(img_annotations[-2])
latitude = float(img_annotations[-1])
img = cv2.imread(os.path.join(IMG_PATH, file))
X_train.append(img)
y_train.append(label)
y_train_bbox.append(np.array([经度，纬度]))
X_train = np.array(X_train)
y_train = np.array(y_train).reshape(-1,1)
y_train_bbox = np.array(y_train_bbox)

base_model = ResNet50(input_tensor=Input(shape=(80, 80, 3)), include_top=False)

head_model = base_model.output
head_model = Flatten()(head_model)
head_model = Dense(128, &#39;relu&#39;)(head_model)
head_model = Dense(64, &#39;relu&#39;)(head_model)
head_model = Dense(32, &#39;relu&#39;)(head_model)
head_model = Dense(16, &#39;relu&#39;)(head_model)
head_model = Dense(8, &#39;relu&#39;)(head_model)
head_model = Dense(1, &#39;sigmoid&#39;)(head_model)

model = Model(inputs=base_model.input, output=head_model)
for layer in base_model.layers:
layer.trainable = False

model.compile(loss=&quot;binary_crossentropy&quot;, metrics=[
&quot;precision&quot;, &quot;recall&quot;, &quot;f1_score&quot;, &quot;true_positives&quot;, &quot;false_positives&quot;, &quot;true_negatives&quot;, &quot;false_negatives&quot;
])

model.fit(X_train, y_train, batch_size=len(X_train), epochs=3)

拟合结果如下：
Epoch 1/3
1/1 ━━━━━━━━━━━━━━━━━━━━━━━ 41s 41s/step - f1_score: 0.4017 - false_negatives: 197.0000 - false_positives: 1913.0000 - loss: 0.7405 - precision: 0.2255 - recall: 0.7387 - true_negatives: 333.0000 - true_positives: 557.0000
时代 2/3
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 31s 31s/step - f1_score：0.4017 - 假阴性：195.0000 - 假阳性：42.0000 - 损失：0.2302 - 精度：0.9301 - 召回率：0.7414 - 真阴性：2204.0000 - 真阳性：559.0000
时代 3/3
1/1 ━━━━━━━━━━━━━━━━━━━━━ 41s 41s/step - f1_score: 0.4017 - false_negatives: 0.0000e+00 - false_positives: 1424.0000 - loss: 1.2052 - precision: 0.3462 - recall: 1.0000 - true_negatives: 822.0000 - true_positives: 754.0000

为什么尽管 precision 和 recall 是正确的，但 f1_score 是错误的？

我知道 Keras 在训练期间根据批次大小对结果进行平均，但是这里的批次大小是相同的作为训练数据集的长度。
]]></description>
      <guid>https://stackoverflow.com/questions/79032877/keras-not-showing-the-correct-f1-score</guid>
      <pubDate>Fri, 27 Sep 2024 20:54:31 GMT</pubDate>
    </item>
    <item>
      <title>尝试保存自定义 Keras 模型时出现“TypeError：不支持的整数大小 (0)”</title>
      <link>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</guid>
      <pubDate>Fri, 27 Sep 2024 19:14:51 GMT</pubDate>
    </item>
    <item>
      <title>加载变压器时出现问题；ModuleNotFoundError：没有名为“transformers”的模块</title>
      <link>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</link>
      <description><![CDATA[我想使用 huggingface 提供的一些模型。我甚至在开始的时候都遇到了最大的困难。有人能帮我识别和解决这个问题吗？
我正在使用 Kubuntu 24.04。

首先，我创建并激活一个虚拟环境，在其中安装变压器。
python3 -m venv .env
source .env/bin/activate

这是成功的，因为现在我在 Visual Code Studio 中的终端有前缀“(.env)”。
接下来，我从 github 安装最新的变压器：
pip install git+https://github.com/huggingface/transformers

输出成功。然后，我使用 hugginface.co 上推荐的方法测试其成功率：
python3 -c &quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I love you&#39;))&quot;

输出对我来说看起来正确：
未提供模型，默认为 distilbert/distilbert-base-uncased-finetuned-sst-2-english 和修订版本 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)。
不建议在生产中使用未指定模型名称和修订版本的管道。
硬件加速器（例如 GPU）在环境中可用，但没有将“设备”参数传递给“管道”对象。模型将在 CPU 上。
[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998656511306763}]

从那里，我尝试运行以下代码：
from transformers import pipeline

但每次我都会得到以下输出：
/bin/python3 /path-to/main.py
回溯（最近一次调用最后一次）：
文件&quot;/path-to/main.py&quot;，第 5 行，在&lt;module&gt;
from transformers import pipeline
ModuleNotFoundError：没有名为“transformers”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</guid>
      <pubDate>Fri, 27 Sep 2024 15:09:24 GMT</pubDate>
    </item>
    <item>
      <title>无法从 coqui-tts 的 tts 库生成语音，并且此错误在单人和多人说话时都会发生</title>
      <link>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</link>
      <description><![CDATA[从 TTS.utils.manage 导入 ModelManager
从 TTS.utils.synthesizer 导入 Synthesizer
从 google.colab 导入文件
初始化模型管理器并加载模型
model_name = &quot;tts_models/en/ljspeech/tacotron2-DDC&quot;
vocoder_name = &quot;vocoder_models/en/ljspeech/hifigan_v2&quot;
model_manager = ModelManager()

model_path, config_path, _ = model_manager.download_model(model_name)

vocoder_path, vocoder_config_path, _ = model_manager.download_model(vocoder_name)

创建合成器对象

synthesizer = Synthesizer(model_path, config_path, vocoder_path, vocoder_config_path, use_cuda=False)

根据手动选择的情绪生成动态 SSML

def generate_dynamic_ssml(chunk):

ssml = f&quot;&quot;&quot;&lt;speak version=&#39;1.0&#39; xmlns=&#39;http://www.w3.org/2001/10/synthesis&#39; xml:lang=&#39;en-US&#39;&gt;&quot;&quot;&quot; 

# 取消注释您想要应用的情感

ssml += f&quot;&lt;prosody pitch=&#39;+10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # happy

# ssml += f&quot;&lt;prosody pitch=&#39;+5%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # romantic

# ssml += f&quot;&lt;prosody pitch=&#39;+7%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 充满希望

# ssml += f&quot;&lt;prosody pitch=&#39;default&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;none&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 中立

# ssml += f&quot;&lt;prosody pitch=&#39;-5%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 失望

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 生气

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 害怕

# ssml += f&quot;&lt;prosody pitch=&#39;-15%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 悲伤

# ssml += f&quot;&lt;prosody pitch=&#39;-20%&#39; rate=&#39;very slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # devastated

ssml += &quot;&lt;/speak&gt;&quot; 

return ssml 

为每个块合成语音的函数

def synthesize_speech(text):

# 使用手动选择的情绪为整个文本生成 SSML 

ssml = generate_dynamic_ssml(text) 

# 合成文本 

wav = synthesizer.tts(ssml) 

# 保存输出文件 

output_file = &quot;output_with_emotion.wav&quot; 

synthesizer.save_wav(wav, output_file) 

# 将文件下载到本地机器 

files.download(output_file) 

示例用法

if name == &quot;main&quot;:

sample_text = &quot;&quot;&quot;我很高兴见到你！你让我的心因喜悦和爱而跳动。&quot;&quot;&quot; 

# 使用所选情绪将文本转换为语音

synthesize_speech(sample_text)

使用 tacotron2 模型和 vocoder hifigen 编写代码，之后我使用合成器库 ssml 来改变声音的音调，使其像情绪一样
AttributeError Traceback (most recent call last)

&lt;ipython-input-8-2698293d91f3&gt; in &lt;cell line: 51&gt;()

53 

54 # 使用所选情绪将文本转换为语音

---&gt; 55 synthesize_speech(sample_text)

1 帧

&lt;ipython-input-8-2698293d91f3&gt;在 synthesize_speech(text) 中

39 

40 # 合成文本 

---&gt; 41 wav = synthesizer.tts(ssml)

42 

43 # 保存输出文件 

/usr/local/lib/python3.10/dist-packages/TTS/utils/synthesizer.py 在 tts(self, text, Speaker_name, Language_name, Speaker_wav, Style_wav, Style_text, Reference_wav, Reference_speaker_name, Split_sentences, **kwargs) 中

320 Speaker_id = self.tts_model.Speaker_manager.name_to_id[Speaker_name] 

321 # 处理单扬声器的 Neon 模型。

--&gt; 322 elif len(self.tts_model.speaker_manager.name_to_id) == 1:

323 Speaker_id = list(self.tts_model.speaker_manager.name_to_id.values())[0] 

324 elif not Speaker_name and not Speaker_wav: 

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;name_to_id&#39;，这是错误


该 tts 代码使用 coqui-tts 制作情感 tts，因此总是出现以下错误，我试图使代码成为单个和多个扬声器
制作单个和多个扬声器，也许有人知道解决方案或建议我使用另一个模型，我想要一个免费的模型]]></description>
      <guid>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</guid>
      <pubDate>Fri, 27 Sep 2024 12:00:55 GMT</pubDate>
    </item>
    <item>
      <title>当有多个场景切换时，有没有办法让 SAM2 跨场景跟踪同一个人？</title>
      <link>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</link>
      <description><![CDATA[使用 Meta 的 SAM2 演示，当场景切换时，面具通常会切换到不同的玩家身上。
我知道手动重新标记每个场景中的玩家是一种选择，但我想探索是否有可用的自动化解决方案。

我尝试使用 Meta 的 SAM2 演示，网址为 https://sam2.metademolab.com/
我希望它能在整个视频中跟踪勒布朗
我发现它只在第一个场景中这样做，偶尔当勒布朗是镜头中唯一的人或主要人物时
]]></description>
      <guid>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</guid>
      <pubDate>Fri, 27 Sep 2024 04:47:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在 rust 中导入用 Python 制作的 ML 模型 (.pkl) 吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</link>
      <description><![CDATA[我之前用 Python 构建了一个项目，但由于它占用了太多资源并且缺乏并发性，所以我改用了 rust。现在我不知道如何正确迁移它，大多数代码已经迁移，但我无法导入导出为 .pkl 文件的 ml 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</guid>
      <pubDate>Fri, 27 Sep 2024 04:43:46 GMT</pubDate>
    </item>
    <item>
      <title>如何保存/查看树状图中的信息？</title>
      <link>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendrogram</link>
      <description><![CDATA[我正在尝试分析数据以根据树状图确定结果。问题是我主要有 2 组数据“H”和“U”，两者一起进行分析。在树状图的末尾，我需要知道哪个区域的“H”更多和“U”。
我尝试使用树状图的字典信息，但当我打开函数时，我注意到它根本没有包含我通过链接输入的信息。
以下是我正在使用的代码：
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
from scipy.cluster import Hierarchy
import pandas as pd
from sklearn.decomposition import PCA

df=pd.read_csv(name+&quot;.csv&quot;, index_col=None, header=0)

normalized_df=(df-df.mean())/df.std()

pca=PCA(n_components=6, n_oversamples=6)
principalComponents = pca.fit_transform(df)
principalDF = pd.DataFrame(data = principalComponents, columns = [&#39;principal component 1&#39;, &#39;principal component 2&#39;, &#39;principal component 3&#39;, &#39;principal component 4&#39;, &#39;principal component 5&#39;, &#39;principal component 6&#39;])

#将类型 (U/H) 添加到数据框
finalDF = pd.concat([principalDF, full_df[[&#39;type&#39;]]], axis = 1)

#从这里开始，我开始遇到问题
data = list(zip(finalDF[&#39;principal component 1&#39;], finalDF[&#39;principal component 2&#39;]))

linkage_data = linkage(data, method=&#39;ward&#39;, metric=&#39;euclidean&#39;)
hierarchy.set_link_color_palette([&#39;r&#39;,&#39;g&#39;,&#39;b&#39;,&#39;w&#39;])
den=dendrogram(linkage_data)
plt.title(&quot;Attempt #1&quot;)
plt.show()

树状图看起来符合预期。问题是我无法区分每个点的类型（H 或 U）。
如上所述，我尝试查看树状图的属性，例如 icoord 和 dcoord，但我无法弄清楚它的含义。
我使用的数据本身是 6 个不同的列，具有不同的数字，这是我标准化的数据 ((df-df.mean())/df.std)，然后取主成分。
有什么建议吗？
编辑：这是我拥有的数据的一个例子：

这是使用前 2 个的树状图的图片主要成分：

在这种情况下，我们的想法是能够区分哪些点是 H，哪些是 U。
需要注意的是，这只是一个例子，因为我使用的真实数据集包含大约 20,000 行，最终有 3 种不同的颜色。我正在使用数据的简短版本来尝试使用代码做不同的事情。]]></description>
      <guid>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendrogram</guid>
      <pubDate>Thu, 26 Sep 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分类神经网络中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</link>
      <description><![CDATA[嗨，我正在做一个关于体积脑图像的机器学习项目
但我没有 ML 背景（有一点编码……但不多），所以我一直在用 pytorch lightning 学习 ML。这很有趣，但有时很难……
所以我现在的问题是我的模型在很大程度上对数据集过度拟合，这使得我在训练中的三个类准确度达到每类 0.85 - ~1.0 之间的准确度，并且损失以良好的曲线下降。遗憾的是验证准确度不足。对于第一类，准确度稳定在 0.05 左右，而其他类稳定在 0.3 左右徘徊。此外，损失从 1.2 略微下降到略高于 1。
（一些医学内容，让您了解数据重叠问题所在）
这些类别的图像是接受 CT 灌注的患者的灰度图像。它显示了患者动脉闭塞或阻塞时大脑灌注的变化。
对于非医学方面的人来说，你可以把它看作是一棵树的树枝，上面有叶子。如果树枝有阻塞，那么该树枝上的叶子就会枯萎。
我的类别是 ICA-T、M1 和最后一类 M2
ICA-T 分支在 M1 中。这意味着体积图像可能包含这些片段的信号相似性。所以也许模型认为由于重叠，ICA-T 病例被猜测为 M1??
这就是类别背景。原始数据集总体如下：
类别 0 (ICA-T)：94，
类别 1 (M1)：366，
类别 2 (M2)：119
总计 579
这显然在类别 1 中占比过高。我对此有疑问，因此我将类别 1 减少到 157，没有考虑任何策略。
0 类 (ICA-T)：94，
1 类 (M1)：157，
2 类 (M2)：119
总计 370
我有一个训练数据集、验证数据集和测试数据集，这些数据集是我通过分层随机分割获得的，这给了我
训练：259 - 0 类：66，1 类：110，2 类：83
验证：37 - 0 类：9，1 类：16，2 类：12
测试：74 - 0 类：19，1 类：31，2 类：24
进一步的信息是，图像是体积图像，空间大小为 256,256,32。
我认为问题在于类相似性。但我不确定是否如此。
有人遇到过类似的问题吗？他们解决了吗？或者有人可以指导我找到答案吗？
遗憾的是我的项目很快就要完成了，所以我没有时间。如果有人需要代码，请询问，但我认为这对我的问题来说没有必要？..
这是我使用的结构：
class CNN5_Mod(L.LightningModule): # 模型定义
def __init__(self, num_classes):
super(CNN5_Mod, self).__init__()
from Project1.MyFile.ConfigMain import Config
config = Config()
# 特征 - x y z
# 1 - 256 256 32
&quot;&quot;&quot; Block 1 &quot;&quot;&quot;
self.conv1 = nn.Conv3d(1, config.c1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1) # 16 - 256 256 32
self.relu1 = nn.ReLU()
self.bt_nm1 = nn.BatchNorm3d(config.c1)

self.conv2 = nn.Conv3d(config.c1, config.c2, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 - 128 128 16
self.relu2 = nn.ReLU()
self.bt_nm2 = nn.BatchNorm3d(config.c2)

self.dropout1 = nn.Dropout3d(config.dropout1)
self.pooling1 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (64, 64,64,8)

&quot;&quot;&quot; 块 2 &quot;&quot;&quot;
self.conv3 = nn.Conv3d(config.c2, config.c3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 32 4
self.relu3 = nn.ReLU()
self.bt_nm3 = nn.BatchNorm3d(config.c3)

self.conv4 = nn.Conv3d(config.c3, config.c4, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 16 16 2
self.relu4 = nn.ReLU()
self.bt_nm4 = nn.BatchNorm3d(config.c4)

self.dropout2 = nn.Dropout3d(config.dropout2)
self.pooling2 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (256, 8,8,1)

&quot;&quot;&quot; 扁平化 &quot;&quot;&quot;
self.flatten = nn.Flatten(1)

&quot;&quot;&quot; 隐藏层 &quot;&quot;&quot;
self.fc1 = nn.Linear(config.in_fc, 64)
self.relu5 = nn.ReLU()
self.fc_dropout = nn.Dropout3d(config.fc_dropout)

&quot;&quot;&quot; 输出 &quot;&quot;&quot;
self.fc2 = nn.Linear(64, num_classes)

&quot;&quot;&quot; 预测 &quot;&quot;&quot;
self.softmax = nn.Softmax(dim=1)

我改变了 dropout rate 和其他一些值。但我不知所措。。]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型拍摄两张图像并验证它们是否相同。我想要训练眼睛认证模型的软件算法。
我在网上搜索过，但只能找到关于识别软件算法（一对多）的答案。
如何在 PyTorch 代码中训练验证模型？
需要澄清的是，相同是指眼睛相同，意味着它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>使用斑点检测来计数黑色圆形种子[关闭]</title>
      <link>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</link>
      <description><![CDATA[我想玩一下 OpenCV 或类似技术，以便能够计算简单的黑色球体（种子，在本例中为拟花椒）。
其他时候，种子中间的白色反射较少，但主要特征是它是“圆形”的、黑色的，几乎总是具有相同的尺寸，并且可以（或有时没有）一个白色的小斑点。





我应该从哪里开始才能让 5 张照片的种子数量大致相同（或者最好是完全相同）？（种子数量相同，我只是在拍摄照片之间摇晃了一下容器）
CV 还是 ML？从哪里开始？
附言：如果有帮助，我也可以尝试物理去除较小的黑色棍子和不好的种子……但理论上，如果可以有可靠的方法可以忽略这些小的“非种子”暗元素，那就太好了……
附言：如果这也能有帮助，我还可以修改拍照的方式……]]></description>
      <guid>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</guid>
      <pubDate>Mon, 23 Sep 2024 21:30:43 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“experimental”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整数据集的大小和比例，如下所示，但遇到了此错误：
AttributeError：模块“keras.layers”没有属性“experimental”

resize_and_rescale= tf.keras.Sequential([
layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),
layers.experimental.preprocessing.Rescaling(1.0/255)
])

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 高斯过程回归器中的优化器调整</title>
      <link>https://stackoverflow.com/questions/44932469/optimizer-tuning-in-sklearn-gaussian-process-regressor</link>
      <description><![CDATA[我尝试使用 GaussianProcessRegressor 作为 scikit-learn 0.18.1 的一部分
我正在对 200 个数据点进行训练，并使用 13 个输入特征作为我的内核 - 一个常数乘以一个具有十二个元素的径向基函数。该模型运行没有问题，但如果我多次运行相同的脚本，我会注意到我有时会得到不同的解决方案。值得注意的是，几个优化参数都超出了我提供的界限（我目前正在研究哪些特性很重要）。
我尝试将参数 n_restarts_optimizer 增加到 50，虽然这需要更长的时间来运行，但它并没有消除明显的随机性因素。似乎可以更改优化器本身，尽管我没有运气。从快速扫描来看，语法上最相似的似乎是 Scipy 的 fmin_tnc 和 fmin_slsqp（其他优化器不包括界限）。但是，使用其中任何一个都会导致其他问题：例如，fmin_tnc 不会返回目标函数的最小值。
关于如何获得更具确定性的脚本有什么建议吗？理想情况下，我希望它无论迭代次数如何都打印相同的值，因为就目前情况而言，这有点像彩票（因此得出任何结论都是值得怀疑的）。
我正在使用的代码片段：
来自 sklearn.gaussian_process 导入 GaussianProcessRegressor 作为 GPR
来自 sklearn.gaussian_process.kernels 导入 RBF，ConstantKernel 作为 C

lbound = 1e-2
rbound = 1e1
n_restarts = 50
n_features = 12 # 实际上在代码的其他地方确定
kernel = C(1.0, (lbound,rbound)) * RBF(n_features*[10], (lbound,rbound))
gp = GPR(kernel=kernel, n_restarts_optimizer=n_restarts)
gp.fit(train_input, train_outputs)
test_model, sigma2_pred = gp.predict(test_input, return_std=True)
print gp.kernel_
]]></description>
      <guid>https://stackoverflow.com/questions/44932469/optimizer-tuning-in-sklearn-gaussian-process-regressor</guid>
      <pubDate>Wed, 05 Jul 2017 17:23:06 GMT</pubDate>
    </item>
    </channel>
</rss>