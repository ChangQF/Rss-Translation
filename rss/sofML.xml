<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 04 Feb 2024 18:17:55 GMT</lastBuildDate>
    <item>
      <title>在每个点上绘制指向图中线条的箭头</title>
      <link>https://stackoverflow.com/questions/77936741/plot-arrow-on-each-point-towards-the-line-in-graph</link>
      <description><![CDATA[我正在尝试使用 matplotlib 绘制从每个数据点到图表中的线的箭头。

我希望箭头代表每个点和线之间的距离。我怎样才能做到这一点？
这是我的代码：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np

# 创建一条直线（45度角）
x_line = np.linspace(0, 10, 100)
y_线 = x_线

# 在线周围添加一些随机点
点数 = 20
x_points = np.linspace(2, 8, num_points) # 根据需要调整范围
y_points = x_points + np.random.normal(0, 0.5, num_points) # 添加一些随机性

# 绘制直线
plt.plot(x_line, y_line, label=&#39;Line&#39;, color=&#39;blue&#39;)

# 绘制点
plt.scatter(x_points, y_points, label=&#39;点&#39;, color=&#39;红色&#39;)

# 设置标签和标题
plt.xlabel(&#39;X轴&#39;)
plt.ylabel(&#39;Y轴&#39;)
plt.title(&#39;围绕一条线的散点图&#39;)

# 显示图例
plt.图例()

# 显示绘图
plt.show()

我自己尝试这样做但失败了：

代码：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np

# 创建一条直线（45度角）
x_line = np.linspace(0, 10, 100)
y_线 = x_线

# 在线周围添加一些随机点
点数 = 20
x_points = np.linspace(2, 8, num_points) # 根据需要调整范围
y_points = x_points + np.random.normal(0, 0.5, num_points) # 添加一些随机性

# 绘制直线
plt.plot(x_line, y_line, label=&#39;Line&#39;, color=&#39;blue&#39;)

# 绘制点
plt.scatter(x_points, y_points, label=&#39;点&#39;, color=&#39;红色&#39;)

# 添加从每个点到直线的箭头
对于 zip(x_points, y_points) 中的 x, y：
    plt.arrow(x, y, 0, y - x, color=&#39;black&#39;, linestyle=&#39;dashed&#39;, linewidth=0.5, head_width=0.2)

# 设置标签和标题
plt.xlabel(&#39;X轴&#39;)
plt.ylabel(&#39;Y轴&#39;)
plt.title(&#39;围绕一条线的散点图&#39;)

# 显示图例
plt.图例()

# 显示绘图
plt.show()

正如您所看到的，数据点发生了移动，箭头指向外侧，而不是向内或指向直线。]]></description>
      <guid>https://stackoverflow.com/questions/77936741/plot-arrow-on-each-point-towards-the-line-in-graph</guid>
      <pubDate>Sun, 04 Feb 2024 17:00:07 GMT</pubDate>
    </item>
    <item>
      <title>探索模型加载、推理和内存管理的基本概念，以实现 OpenVINO 中与设备无关的高效处理</title>
      <link>https://stackoverflow.com/questions/77936078/exploring-fundamental-concepts-in-model-loading-inference-and-memory-managemen</link>
      <description><![CDATA[在之前的开发工作中，该人还发起了 OpenVINO CSharp API 项目。基本实现是调用OpenVINO提供的C API接口并封装在上层。
但是，OpenVINO 当前的 C API 仅实现了大部分接口，部分功能尚未实现。因此，开发者无法在 C# 和 C 语言中充分体验 OpenVINO 的所有功能。
该人表达了与其他开发人员合作增强未实现的 C API 接口的愿望。作为说明，当前发布的C API包括用于设置“比例”的接口。和“平均”。然而，它目前仅支持将所有通道设置为相同的参数，缺乏对每个通道参数进行不同设置的能力。具体问题和建议的改进可以在以下 GitHub 链接中找到：[OpenVINO GitHub Issue #22001](https://github.com/openvinotoolkit/openvino/issues/22001）。
到目前为止，我已经深入研究了以下基本概念：
1.加载和管理模型。
2.进行推理。
3.实施与设备无关的解决方案。
4.了解同步和异步推理。
5.有效管理内存。
6.实施预处理和后处理技术。]]></description>
      <guid>https://stackoverflow.com/questions/77936078/exploring-fundamental-concepts-in-model-loading-inference-and-memory-managemen</guid>
      <pubDate>Sun, 04 Feb 2024 14:03:19 GMT</pubDate>
    </item>
    <item>
      <title>无效路径、路径未指向有效文件、注释图像</title>
      <link>https://stackoverflow.com/questions/77935509/invalid-path-path-not-pointing-to-a-valid-file-annotating-images</link>
      <description><![CDATA[我正在尝试为我想要创建的机器学习模型注释图像，但出现以下错误：
&lt;块引用&gt;
注释 C:\mypath\Post_Event_Images_In_JPEG\tile_12_26.jpg 并保存到 C:\mypath\AnnotatedImages\annotated_after_tile_12_26.jpg
注释 C:\mypath\Post_Event_Images_In_JPEG\tile_12_26.jpg 时出错：无效路径，路径未指向有效文件。

代码如下：
导入操作系统
将 numpy 导入为 np
从 PIL 导入图像
从 tifffile 导入 imread、imwrite
导入光栅
从 rasterio.transform 导入 from_origin
进口舒蒂尔
导入子流程

将张量流导入为 tf
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D，MaxPooling2D，UpSampling2D，BatchNormalization，激活
从tensorflow.keras.optimizers导入Adam
从 sklearn.model_selection 导入 train_test_split
从 imageai.Detection 导入 ObjectDetection

defpair_images(文件夹前、文件夹后、图像数):
    before_images = os.listdir(before_folder)[:num_images]
    after_images = os.listdir(after_folder)[:num_images]

    图像对 = {}

    对于 before_images 中的 before_image：
        common_part = os.path.splitext(before_image)[0]
        after_image = f“{common_part}.jpg”

        如果 after_image 在 after_images 中：
            image_pairs[common_part] = (os.path.join(before_folder, before_image), os.path.join(after_folder, after_image))
        别的：
            print(f“警告：未找到 {before_image} 的后图像”)

    返回图像对


def annotate_image(图像路径, 输出路径):
    检测器 = 对象检测()
    detector.setModelTypeAsRetinaNet()
    detector.setModelPath(“path/to/resnet50_coco_best_v2.1.0.h5”) # 从 https://github.com/OlafenwaMoses/ImageAI/releases/tag/essential-v4 下载此文件
    detector.loadModel()

    print(f“检测{image_path}中的对象”)
    检测= detector. detectorObjectsFromImage(
        输入图像=图像路径，
        输出图像路径=输出路径，
        最小百分比概率=30
    ）

    print(“检测到的对象：”)
    用于检测中的检测：
        print(f&quot;{检测[&#39;名称&#39;]} - {检测[&#39;百分比概率&#39;]}&quot;)

    print(f“{image_path} 的注释已完成”)

def main():
    # 指定“之前”的路径和“之后”图像文件夹
    before_folder = r“C:\Users\Erevos\Desktop\EYContest\Pre_Event_Images_In_JPEG”
    after_folder = r“C:\Users\Erevos\Desktop\EYContest\Post_Event_Images_In_JPEG”
    output_folder = r“C:\Users\Erevos\Desktop\EYContest\AnnotatedImages”

    # 获取图像对
    要注释的图像数量 = 500
    image_pairs =pair_images(before_folder, after_folder, num_images_to_annotate)

    # 使用 ImageAI 注释前 500 张图像
    os.makedirs（输出文件夹，exist_ok = True）
    
    对于 image_pairs.items() 中的 common_part，(before_image, after_image)：
        annotated_before_path = os.path.join(output_folder, f&quot;annotated_before_{common_part}.jpg&quot;)
        annotated_after_path = os.path.join(output_folder, f&quot;annotated_after_{common_part}.jpg&quot;)

        print(f“注释 {before_image} 并保存到 {annotated_before_path}”)
        尝试：
            注释图像（前图像，注释前路径）
        除了异常 e：
            print(f&quot;注释 {before_image} 时出错：{e}&quot;)

        print(f“注释 {after_image} 并保存到 {annotated_after_path}”)
        尝试：
            注释图像（后图像，注释后路径）
        除了异常 e：
            print(f&quot;注释 {after_image} 时出错：{e}&quot;)

    print(&quot;注释并保存完成。&quot;)

如果 __name__ == “__main__”：
    主要的（）

请注意，我已手动创建文件夹来检查是否是这种情况，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/77935509/invalid-path-path-not-pointing-to-a-valid-file-annotating-images</guid>
      <pubDate>Sun, 04 Feb 2024 11:06:05 GMT</pubDate>
    </item>
    <item>
      <title>ImageNet 的多标签分类映射到 16 个类别 - Pytorch</title>
      <link>https://stackoverflow.com/questions/77935339/multi-label-classification-of-imagenet-mapped-to-16-classes-pytorch</link>
      <description><![CDATA[我想在三个数据集上训练一个模型 - 标准 ImageNet (IN)、ImagetNet-a (IN-a) 和 Stylized-ImageNet (SIN)。 SIN 将 1000 个 IN 类别映射到仅 16 个类别，例如“airplane”代表 IN 标签 404，“bear”代表 IN 标签 294、295、296、297。为了在此数据集上训练模型，我对 16 个 SIN 类 [0,...,0,1,0,...] 进行了 One-hot 编码。为了测试，我做了同样的事情，但我制作了 SIN 数据集的副本及其原始类标签（0-15，通过 ImageFolder 导入）。
SIN 的作者提供了一种建立（单标签）测试函数的简单方法，如其 GitHub 上所示 存储库。
作为此多标签任务的另一种测试方法，我使用 Pytorch 实现了以下功能：
def multi_l_eval（模型，test_loader）：
  模型.eval()
  正确 = 0
  总计 = 0
  灵敏度=0.5
  使用 torch.no_grad()：
      对于图像，test_loader 中的标签：
          图像、标签 = images.to(设备)、标签.to(设备)
          输出=模型（图像）
          输出 = torch.sigmoid(输出)

          输出[输出&gt;=灵敏度] = 1
          输出[输出&lt;灵敏度] = 0

          正确+=（输出==标签）.sum()

          总计 += labels.size(0)*labels.size(1)
  返回（正确/总计）*100

ImageNet-a
此外，我想在 IN-a 上训练模型（IN adversarial示例数据集）和 SIN，我也是通过对 IN-a 数据集进行 one-hot 编码来实现的。在 IN-a 数据集（多标签）上训练模型后，与 IN-a git 存储库中所示的（单标签）测试方法相比，我的函数产生了更高的测试准确性。
问题
现在，当比较单标签测试方法和我的函数的结果时，我的函数输出完全不同、更高的准确率百分比。我实现的函数是分析多标签性能的正确方法吗？
我发现一些信息，在多标签分类中需要平衡类。我将如何平衡 SIN 的类别标签以及不同数量的正确标签，正如“飞机”和“熊”的示例中已经提到的那样？此外，如何针对多个不同的数据集实现这种平衡。
感谢您阅读并花时间:)
训练方法
def std_train_model (模型,train_loader, opt, num_epochs):
  model.train() # 将模型设置为训练模式
  crit = nn.BCEWithLogitsLoss()
  对于范围内的纪元（num_epochs）：
      运行损失 = 0.0
      对于输入，train_loader 中的标签：
          输入，标签=输入.to（设备），标签.to（设备）
          opt.zero_grad()
          输出 = 模型（输入）
          损失=暴击（输出，标签）
          loss.backward()
          opt.step()

          running_loss += loss.item()

      print(f&#39;Epoch {epoch + 1}/{num_epochs}，损失：{running_loss / len(train_loader)}&#39;)

  print(&quot;训练完成&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/77935339/multi-label-classification-of-imagenet-mapped-to-16-classes-pytorch</guid>
      <pubDate>Sun, 04 Feb 2024 10:11:25 GMT</pubDate>
    </item>
    <item>
      <title>更改房间图像中的地板纹理</title>
      <link>https://stackoverflow.com/questions/77935330/change-the-floor-texture-in-a-room-image</link>
      <description><![CDATA[我正在尝试创建一个房间可视化工具，用户可以在其中上传他们的房间图像，并可以将不同的纹理应用到地板上。我成功地使用 roboflow 中的地板分割模型提取图像中的地板坐标，并将纹理图像应用到坐标上，现在的问题是纹理在某些图像上看起来很好，但在某些图像上看起来更糟。我知道我做错了什么，任何人都可以建议一种方法来处理分段蒙版上的纹理应用程序。我正在附加我得到的输出图像  ]]></description>
      <guid>https://stackoverflow.com/questions/77935330/change-the-floor-texture-in-a-room-image</guid>
      <pubDate>Sun, 04 Feb 2024 10:08:55 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士中的温度：它如何影响随机性</title>
      <link>https://stackoverflow.com/questions/77935310/temperature-in-llms-how-it-impacts-randomness</link>
      <description><![CDATA[在研究 openapi 和其他 LLM 的代码示例时，我发现了非常常用的参数“温度”。
我无法理解其背后的想法。
在线消息来源称，它告诉 llm 可以在多大程度上随机。
我的问题是，如果法学硕士甚至不确定其生成的内容中是否存在随机性（这就是为什么我们将温度值传递给它，不是），那么它如何找出生成的内容的哪一部分是随机的过滤它。]]></description>
      <guid>https://stackoverflow.com/questions/77935310/temperature-in-llms-how-it-impacts-randomness</guid>
      <pubDate>Sun, 04 Feb 2024 10:03:40 GMT</pubDate>
    </item>
    <item>
      <title>错误：运行时错误：给定组=1，大小权重[64,3,3,3]，预期输入[1,12,320,320]有3个通道，但得到了12个通道</title>
      <link>https://stackoverflow.com/questions/77935195/error-runtimeerror-given-groups-1-weight-of-size-64-3-3-3-expected-inpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77935195/error-runtimeerror-given-groups-1-weight-of-size-64-3-3-3-expected-inpu</guid>
      <pubDate>Sun, 04 Feb 2024 09:23:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我得到的是空白图？</title>
      <link>https://stackoverflow.com/questions/77935111/why-am-i-getting-a-blank-plot</link>
      <description><![CDATA[目标是将数据与与用户输入的最新信息相匹配的相同“客户编号”进行比较。我得到了一个空白图。
鉴于我以这种方式呈现数据：
客户编号 |客户类型 |订购时间 |订单金额 |订单数量
120023 |非常规| 2023 年 1 月 25 日 00:00:00 | 470.92 | 470.92 13
120023 |常规 |2023 年 1 月 25 日 00:00:00 | 64.03 | 7
120024 |非常规| 2023 年 1 月 25 日 00:00:00 | 143.18 | 143.18 3
120024 |常规 |2023 年 1 月 25 日 00:00:00 | 143.18 | 143.18 3
120025 |非常规 |2023 年 1 月 25 日 00:00:00 | 222.14 | 222.14 1
120025 |常规 |2023 年 1 月 25 日 00:00:00 | 222.14 | 222.14 1
120006 |非常规 |2023 年 1 月 25 日 00:00:00 | 157.18 | 157.18 5
120007 |非常规 |2023 年 1 月 25 日 00:00:00 | 151.51 | 151.51 7
120008 |非常规 |2023 年 1 月 25 日 00:00:00 | 99.7 | 99.7 7
119990 |非常规 |2023 年 1 月 25 日 00:00:00 | 212.48 | 212.48 1
119991 |非常规 |2023 年 1 月 25 日 00:00:00 | 96.53 | 3
119992 |非常规 |2023 年 1 月 25 日 00:00:00 | 62.72 | 7
119993 |非常规 |2023 年 1 月 25 日 00:00:00 | 87.89 | 7
119994 |非常规 |2023 年 1 月 25 日 00:00:00 | 96.56 | 1
119995 |非常规 |2023 年 1 月 25 日 00:00:00 | 95.44 | 1
119996 |非常规 |2023 年 1 月 25 日 00:00:00 | 97.03 | 3

我创建了一个代码来查看我收集的数据。目的是将数据与与用户输入的最新信息相匹配的相同“客户编号”进行比较。您还可以看到，对于接下来的 2 行，它具有相同的“客户编号”参数，但客户类型介于 Non-Regular 和 Regular 之间，任务是取订单金额和订单最低的行数量，并且数据点将绘制在一起。
从这里，我想单独绘制客户编号的数据，y 轴是“订单数量”变量，x 轴是“订单时间”变量。为什么它返回一个空图？我该如何解决这个问题？
完整代码：
&lt;前&gt;&lt;代码&gt;
将 pandas 导入为 pd
导入日期时间
导入 ipywidgets 作为小部件
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

df = pd.read_excel(&#39;FoodOrder.xlsx&#39;)

input_date = input(&quot;请输入日期（格式：yyyy/mm/dd）：&quot;)
input_date = datetime.datetime.strptime(input_date, &quot;%Y/%m/%d&quot;)

df[&#39;订单时间&#39;] = pd.to_datetime(df[&#39;订单时间&#39;])

df[&#39;time_diff&#39;] = (df[&#39;订单时间&#39;] - input_date).dt.days

df_filtered = df[(df[&#39;time_diff&#39;] &gt;= -20) &amp; (df[&#39;time_diff&#39;] &lt;= 20)]


df_grouped = df_filtered.groupby(&#39;客户编号&#39;).apply(lambda x: x.loc[x[&#39;订单金额&#39;].idxmax()] if x[&#39;订单金额&#39;].max() &gt; x[&#39;订单数量&#39;].max() else x.loc[x[&#39;订单数量&#39;].idxmax()])


# 重置分组数据帧的索引
df_grouped = df_grouped.reset_index(drop=True)


# 定义一个绘图函数
defplot_customer(x):
    plt.plot(x[&#39;订单时间&#39;], x[&#39;订单金额&#39;])
    plt.xlabel(&#39;下单时间&#39;)
    plt.ylabel(&#39;订单金额&#39;)
    plt.title(&#39;客户编号的订单金额与订单时间：&#39; + str(x[&#39;客户编号&#39;].iloc[0]))
    plt.gcf().autofmt_xdate()
    plt.ylim(0, x[&#39;订单金额&#39;].max() * 1.1)
    plt.show()

    plt.show()

# 将绘图函数应用到每个组
df_grouped.groupby(&#39;客户编号&#39;).apply(plot_customer)

]]></description>
      <guid>https://stackoverflow.com/questions/77935111/why-am-i-getting-a-blank-plot</guid>
      <pubDate>Sun, 04 Feb 2024 08:55:45 GMT</pubDate>
    </item>
    <item>
      <title>如何分析不同版本随机森林模型之间的预测概率方差？</title>
      <link>https://stackoverflow.com/questions/77934696/how-to-analyze-prediction-probability-variance-between-different-versions-of-ran</link>
      <description><![CDATA[我想比较随机森林模型（分类）的两个不同版本生成的预测概率，以了解这些版本之间的预测方差如何变化。
我在图数据集上使用 Python 和随机森林分类。这是我到目前为止所做的：
name = [&#39;莎莉&#39;,&#39;约翰&#39;,&#39;亚当&#39;,&#39;迈克&#39;,&#39;安娜&#39;,&#39;吉姆&#39;,&#39;莉兹&#39;,&#39;汤姆&#39; ,&#39;迪娜&#39;,&#39;伊莱&#39;]
all_class = [键对键，entity2id.items() 中的值]
match = [(str(class), str(name)) 名称中的名称 for all_class 中的类]
X = np.vstack([
    np.hstack([
        entity_embeddings_dict.get(str(class), np.zeros(node_embeddings.shape[1])),
        entity_embeddings_dict.get(str(name), np.zeros(node_embeddings.shape[1]))
    ]) 对于班级，匹配中的名称])

# 预测概率
结果 = RF_model.predict_proba(X)

打印（结果）：

&lt;表类=“s-表”&gt;
&lt;标题&gt;


姓名
class_model1
tp_score_model1
class_model2
tp_score_model2


&lt;正文&gt;

0
约翰
数学
0.996422
数学
0.996422


1
约翰
数学
0.996422
化学
0.994451


2
约翰
数学
0.996422
艺术
0.993411


3
约翰
数学
0.996422
生物学
0.992564


4
约翰
数学
0.996422
化学
0.992404




我希望分析同一数据集的两个模型版本之间的预测概率差异。具体来说，我想：

计算两个模型之间数据集中每个实例的预测概率差异。
可视化这些差异以了解分布并识别任何模式。

有人可以指导我如何有效地执行此分析或建议任何非常适合此类分析的工具/库吗？]]></description>
      <guid>https://stackoverflow.com/questions/77934696/how-to-analyze-prediction-probability-variance-between-different-versions-of-ran</guid>
      <pubDate>Sun, 04 Feb 2024 05:28:07 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust-linfa 中加载用于预测的线性回归模型</title>
      <link>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</link>
      <description><![CDATA[我一直在研究 Rust 机器学习的 linfa，特别是线性回归模型。我希望能够保存和加载经过训练的线性回归模型，但我无法找到实现此目的的方法。
方法 1：
到目前为止，我的方法是获取训练中涉及的主要参数，这些参数可以从 linfa 的线性回归实现中获取，并将它们存储在一个可以存储为 JSON 文件的结构中（通过 serde_json 完成）。然而，在此之后我不知道如何将其加载回来进行训练。
以上内容详情如下：
存储训练参数的结构：
struct ModelJson {
    系数：Vec f64 ，
    拦截：f64，
}

存储过程：
let model = lin_reg.fit(&amp;dataset)?;
让 model_json = ModelJson {
    系数： model.params().to_vec(),
    拦截： model.intercept(),
};

存储的数据看起来如何：
{“系数”:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.00103006349345 99436]，“拦截”：50.525680622870084}

方法 2：
关于序列化和反序列化整个模型，我发现以下信息表明 linfa 中支持相同的操作。
加载和保存模型
这引出了我的第二种方法，其中我使用了 linfa-linear 的 serde 功能（包含 LinearRegression 模型），首先在我的 Cargo.toml 中包含以下内容：
linfa-clustering = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}
根据我对实现的理解，此功能为 LinearRegression 实现了以下功能：
Serde 序列化和反序列化实现 - 派生
上述实现：
&lt;前&gt;&lt;代码&gt;#[cfg_attr(
    特征=“serde”，
    派生（序列化，反序列化），
    serde(crate = “serde_crate”)
)]
/// 可用于进行预测的拟合线性回归模型。
pub struct FittedLinearRegression; {
    截距：F，
    参数：Array1,
}

发现于： linfa-线性导出实现
我的实现如下：
let model = lin_reg.fit(&amp;dataset)?;
让序列化 = serde_json::to_string(&amp;model).unwrap();

但是此方法出现以下错误：
不满足特征边界 `FittedLinearRegression: serde::ser::Serialize`
以下其他类型实现了特征 `serde::ser::Serialize`：
  布尔值
  字符
  大小
  i8
  i16
  i32
  i64
  i128
和其他 133 个rustcClick 以获取完整的编译器诊断
main.rs(82, 22)：此调用引入的绑定所需

是否有其他方法可以做到这一点，或者是否有某种方法可以使这些方法之一发挥作用？]]></description>
      <guid>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</guid>
      <pubDate>Sat, 03 Feb 2024 13:44:24 GMT</pubDate>
    </item>
    <item>
      <title>这个elasticsearch设置是否使用ML节点？</title>
      <link>https://stackoverflow.com/questions/77923033/does-this-elasticsearch-setup-even-use-ml-node</link>
      <description><![CDATA[我按照以下示例设置了用于图像相似性搜索的 Elasticsearch 集群：https ://github.com/radoondas/flask-elastic-image-search
由于不熟悉elasticsearch，我盲目地遵循了这个示例，其中包括一个带有预训练模型的 ML 节点。效果很好。但是，我怀疑我们实际上并没有使用 ML 节点。
我在应用程序中提取密集向量，然后对它们进行索引，并且在查询时也在应用程序中提取向量。我不使用elasticsearch来提取密集向量。
有什么“魔法”吗？当我索引密集向量或执行 KNN 查询时，这使得 Elasticsearch 在幕后使用预训练模型？或者预训练模型和机器学习节点都是我们在实现中不需要的额外东西？]]></description>
      <guid>https://stackoverflow.com/questions/77923033/does-this-elasticsearch-setup-even-use-ml-node</guid>
      <pubDate>Thu, 01 Feb 2024 20:00:42 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测访问日期与客户类别图不准确</title>
      <link>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</link>
      <description><![CDATA[我正在尝试对一堆类和日期时间进行时间序列预测，但由于某种原因我的图表看起来像这样，我的完整代码如下：
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
plt.plot(out.index, out[&#39;count&#39;], )



而我从其中获取此时间序列代码的博客有这样的结果

所以我不确定是否应该继续XD
我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23]]></description>
      <guid>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</guid>
      <pubDate>Wed, 31 Jan 2024 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>将 YOLOv8 集成到 Transformer 模型中</title>
      <link>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</link>
      <description><![CDATA[我需要帮助将 YOLOv8 模型 添加到下面的代码中，而不是使用 InceptionV3为我的项目提取图像特征。我需要传递检测到的对象并从 YOLOv8 模型中提取特征，以使用转换器生成标题。
def CNN_Encoder_Incep():
    inception_v3 = tf.keras.applications.InceptionV3(
        include_top=假，
        权重=&#39;imagenet&#39;
    ）
    inception_v3.trainable = False

    输出= inception_v3.output
    输出 = tf.keras.layers.Reshape(
        (-1, 输出.形状[-1]))(输出)

    cnn_model = tf.keras.models.Model(inception_v3.输入，输出)
    返回cnn_model

类 ImageCaptioningModel(tf.keras.Model):

    def __init__(self, cnn_model, 编码器, 解码器, image_aug=None):
        超级().__init__()
        self.cnn_model = cnn_model
        self.encoder = 编码器
        self.decoder = 解码器
        self.image_aug = image_aug
        self.loss_tracker = tf.keras.metrics.Mean(name=&quot;loss&quot;)
        self.acc_tracker = tf.keras.metrics.Mean(name=&quot;准确度&quot;)


    defcalculate_loss(self, y_true, y_pred, mask):
        损失 = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        损失*=掩模
        返回 tf.reduce_sum(loss) / tf.reduce_sum(mask)


    defcalculate_accuracy(self, y_true, y_pred, mask):
        精度 = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        准确度 = tf.math.logic_and(掩码, 准确度)
        准确度 = tf.cast(准确度, dtype=tf.float32)
        掩码 = tf.cast(掩码, dtype=tf.float32)
        返回 tf.reduce_sum(accuracy) / tf.reduce_sum(mask)


    defcompute_loss_and_acc(self,img_embed,captions,training=True):
        编码器输出 = self.encoder(img_embed, 训练=True)
        y_input = 标题[:, :-1]
        y_true = 标题[:, 1:]
        掩码=（y_true！= 0）
        y_pred = self.解码器（
            y_输入，编码器_输出，训练=真，掩码=掩码
        ）
        损失 = self.calculate_loss(y_true, y_pred, mask)
        acc = self.calculate_accuracy(y_true, y_pred, mask)
        回波损耗，ACC


    def train_step(自身, 批次):
        imgs、字幕 = 批处理

        如果 self.image_aug：
            imgs = self.image_aug(imgs)

        img_embed = self.cnn_model(imgs)

        使用 tf.GradientTape() 作为磁带：
            损失，acc = self.compute_loss_and_acc(
                img_embed、字幕
            ）

        训练变量 = (
            self.encoder.trainable_variables + self.decoder.trainable_variables
        ）
        grads = Tape.gradient(损失, train_vars)
        self.optimizer.apply_gradients(zip(grads, train_vars))
        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}


    def test_step（自身，批次）：
        imgs、字幕 = 批处理

        img_embed = self.cnn_model(imgs)

        损失，acc = self.compute_loss_and_acc(
            img_embed、字幕、训练=False
        ）

        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}

    @财产
    定义指标（自身）：
        返回 [self.loss_tracker, self.acc_tracker]

cnn_model = CNN_Encoder_Incep()
标题模型 = ImageCaptioningModel(
    cnn_model=cnn_model，编码器=编码器，解码器=解码器，image_aug=图像增强，
）

我尝试这样做，但当我尝试将其传递给 cnn_model 变量时，我不断收到多个错误。
def CNN_Encoder():
    yolov8_model = tf.keras.models.load_model(&#39;./content/yolov8n_objdet_oidv7_640x640.pt&#39;)
    yolov8_model.trainable = False
    输出=yolov8_model.output
    输出= tf.keras.layers.Reshape((-1,output.shape[-1]))(输出)
    cnn_model = tf.keras.models.Model(yolov8_model.输入，输出)
    cnn_model_onnx = cnn_model.export(format=&#39;onnx&#39;)
    返回cnn_model
]]></description>
      <guid>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</guid>
      <pubDate>Mon, 29 Jan 2024 05:22:01 GMT</pubDate>
    </item>
    <item>
      <title>在 RL 中使用线性函数逼近的 actor 梯度问题</title>
      <link>https://stackoverflow.com/questions/75733703/problem-with-the-gradient-of-the-actor-using-linear-function-approximation-in-rl</link>
      <description><![CDATA[我在更新 theta（演员评论家算法中演员的权重向量）时遇到问题。我知道 ln(pi(a|s,theta) = x(s,a) - \sum_b(pi(b|s,theta)*x(s,b) 的梯度，其中索引 b 代表每个可能的这个差异的结果总是 0。我知道为什么：因为 x(s,b) 对于所有动作都是相等的，所以我可以从总和中取出它，并且因为所有 pi(b|, s,theta) 等于 1，那么结果等于 x(s,a) - x(s,a) = 0。我正在使用堆栈特征技术。我搜索了各种来源，如 coursera、google 等.但我找不到它。我的问题是：如何对 x(s,a) 进行编码，以便每个操作获得不同的表示并避免此问题。提前致谢
该方法的代码如下：
 def update_theta(self, delta, x):
        action_probs = softmax(self.theta, x)

        grad_ln_pi = x - np.sum(action_probs[:, np.newaxis]@x[np.newaxis,:], axis=0)

        self.theta += self.alpha_theta*delta*grad_ln_pi

我尝试实施梯度来更新 theta，检查是否存在错误，并且我期待正确的更新，但我意识到差异将始终等于 0。]]></description>
      <guid>https://stackoverflow.com/questions/75733703/problem-with-the-gradient-of-the-actor-using-linear-function-approximation-in-rl</guid>
      <pubDate>Tue, 14 Mar 2023 13:21:54 GMT</pubDate>
    </item>
    <item>
      <title>Keras：如果我用标准化数据训练模型，model.predict() 是否需要标准化数据？</title>
      <link>https://stackoverflow.com/questions/68119256/keras-does-model-predict-require-normalized-data-if-i-train-the-model-with-no</link>
      <description><![CDATA[使用 Keras 完成模型训练后，我尝试使用 Keras 的 model.predict() 来测试新颖输入的模型。
训练模型时，我使用 Scikit Learn 的 MinMaxScaler() 对训练数据进行标准化。
使用 model.predict() 时是否还需要标准化数据？如果是这样，我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/68119256/keras-does-model-predict-require-normalized-data-if-i-train-the-model-with-no</guid>
      <pubDate>Thu, 24 Jun 2021 16:10:03 GMT</pubDate>
    </item>
    </channel>
</rss>