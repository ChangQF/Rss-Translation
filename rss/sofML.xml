<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Dec 2024 21:16:59 GMT</lastBuildDate>
    <item>
      <title>科学记数法</title>
      <link>https://stackoverflow.com/questions/79248978/scientific-notation</link>
      <description><![CDATA[我在尝试运行 R 程序时收到此错误 错误：非法数字“4.5e+07”。退出...
我尝试了各种方法，通过添加 as.integers 更改我的睡眠时间和运行时参数以禁用科学计数法问题。我也使用了此选项（scipen=999），但我仍然收到相同的错误。
请帮我解决此问题，谢谢
我尝试了各种方法，通过添加 as.integers 更改我的睡眠时间和运行时参数以禁用科学计数法问题。我也使用了此选项（scipen=999），但我仍然收到相同的错误。
我正在计算峰值特征重叠，我希望程序能够在管道中成功运行而不会遇到此错误]]></description>
      <guid>https://stackoverflow.com/questions/79248978/scientific-notation</guid>
      <pubDate>Tue, 03 Dec 2024 19:39:18 GMT</pubDate>
    </item>
    <item>
      <title>想要构建一个验证码求解器但不知道如何做？[关闭]</title>
      <link>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</link>
      <description><![CDATA[我有一个来自我大学网站的 1000 张带标签的 CAPTCHA 图像数据集，我想训练一个模型，该模型可以在未见过的数据上准确解决类似的 CAPTCHA。CAPTCHA 通常由 6 个字母数字字符（A-Z、0-9）组成。尽管尝试了几种方法，但该模型仍无法实现高精度。
数据集：
样本数量：1000 张带标签的图像。
CAPTCHA 类型：6 个字母数字字符。
图像示例：[附加示例 CAPTCHA 图像]。
我尝试过的方法：
卷积神经网络 (CNN)：
我创建了一个具有多个卷积层和密集层的 CNN 模型。
将输出展平以馈送到每个字符的单独密集层中。
在未见过的 CAPTCHA 上实现了较差的泛化。
迁移学习：
使用 MobileNetV2 作为带有自定义头的特征提取器。
适应输入大小（灰度到 RGB 转换）。
由于数据有限，该模型容易过度拟合
要求：
我想训练一个模型：
可以高精度处理未见过的 CAPTCHA。
有效解码字母数字字符序列。
问题：
解决此类 CAPTCHA 的最佳方法或架构是什么？我应该使用：
CNN 用于特征提取，然后使用 LSTM 用于序列解码？
完全卷积架构（例如 CRNN）？
对于有限的数据集，还有其他更好的方法吗？
如果可能，您能否建议一个完整的模型架构、预处理步骤或可能有帮助的损失函数设置？任何有关增强此任务的小数据集的提示也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</guid>
      <pubDate>Tue, 03 Dec 2024 18:37:33 GMT</pubDate>
    </item>
    <item>
      <title>CUDA 内存溢出错误但有可用空间</title>
      <link>https://stackoverflow.com/questions/79248530/overflowing-cuda-memory-error-but-have-free-space</link>
      <description><![CDATA[我遇到这个问题已经有一段时间了，当我尝试将嵌入暗度从 64 加倍到 128 或将批处理大小从 1 增加时，我总是收到此错误。这是一个具有 120 万个参数的语言模型。它使用相同数据的 30 万个参数，但如果我使用包含 20 万个样本的较大数据集，它就会崩溃。当前数据集有 4k 个样本。样本是堆叠在一起的 50 种蛋白质序列。如果有人能帮忙，我将不胜感激。
错误代码是：
torch.cuda.OutOfMemoryError：CUDA 内存不足。尝试分配 150.00 MiB（GPU 0；总容量 31.74 GiB；已分配 21.32 GiB；空闲 9.86 GiB；允许 21.58 GiB；PyTorch 总共保留 21.37 GiB）
以下是参数：
nb_blocks=6, embed_dim=128, nb_heads=4, nb_epochs=20, warmup_steps=300, learning_rate=0.0001, check_val_every=1000, batch_size=1
我使用的是 NVIDIA V100 PCIe 32 GB GPU 的集群设置。
Lightning 设置为：
slurm_args = {
&quot;accelerator&quot;: &quot;gpu&quot;,
&quot;devices&quot;: int(os.environ[&quot;SLURM_GPUS_ON_NODE&quot;]),
&quot;num_nodes&quot;: int(os.environ[&quot;SLURM_NNODES&quot;]),
&quot;strategy&quot;: &quot;ddp&quot;,
&quot;precision&quot;: 16,
}

...

accelerator = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
trainer_args = {
&quot;max_epochs&quot;: args.nb_epochs,
&quot;log_every_n_steps&quot;: LOGGING_STEPS,
&quot;val_check_interval&quot;: VAL_CHECK_STEPS,
&quot;logger&quot;: wandb_logger,
&quot;callbacks&quot;: callbacks,
&quot;accelerator&quot;: accelerater,
**slurm_args,
}

我尝试过的方法：

将精度设置为 16（从 32 降低）：将内存溢出从 300MiB 减半到 150MiB（至少取得了一些进展，但没有解决问题问题）
torch.cuda.set_per_process_memory_fraction(0.7)：释放了空间，但该空间未分配用于内存溢出，我无法弄清楚如何分配该备用内存以用于上述问题
手动覆盖分配并将权重、数据和模型移动到 CPU。没有做任何事情，因为几乎所有的初始化都已初始化到 CPU。
将 slurm_args 中的 accelerator 设置为 cpu。解决了内存问题，但每次迭代从 1.2 秒增加到 25 秒。换句话说，它太慢了，根本没用
将策略从 ddp 更改为 FSDPStrategy(cpu_offload=True)，但没有任何变化
尝试设置 PYTORCH_CUDA_ALLOC_CONF，但没有任何效果
在参数中使用不同的值设置 accumulate_grad_batches，没有变化
]]></description>
      <guid>https://stackoverflow.com/questions/79248530/overflowing-cuda-memory-error-but-have-free-space</guid>
      <pubDate>Tue, 03 Dec 2024 16:51:36 GMT</pubDate>
    </item>
    <item>
      <title>保留验证集 - 超参数调整 - RandomizedSearchCV - XGBoost</title>
      <link>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning-randomizedsearchcv-xgboost</link>
      <description><![CDATA[我有一个大型数据集，我将其拆分为：

训练集 (80%)
验证集 (10%)
测试集 (10%)

在每个集合上，我执行了缺失值插补和特征选择（在训练集上训练，并复制到验证和测试集中）以避免数据泄露。
现在，我想用 Python 训练 XGBoost 模型，并希望使用训练集执行超参数调整，并使用验证集评估每个参数集。我如何使用 RandomizedSearchCV 等随机方法执行此操作，以便不运行所有参数集？
如果我是正确的，GridSearch 和 RandomizedSearchCV 仅允许交叉验证，这不是我想要的，因为将预处理的训练集拆分成几层会导致数据泄露。
我知道我可以构建一个 sklearn 管道，在其中对每个折叠进行预处理，但我想避免后一种选择。
我只能考虑像在 GridSearch 中一样运行每个参数集的代码：
from sklearn.model_selection import ParameterGrid
import xgboost as xgb

# 定义你的超参数网格
param_grid = {
&#39;max_depth&#39;: [3, 5, 7],
&#39;learning_rate&#39;: [0.01, 0.1, 0.2],
&#39;n_estimators&#39;: [100, 200, 300]
}

best_score = -1
best_params = {}

for params in ParameterGrid(param_grid):
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train)
val_score = model.score(X_val, y_val) # 或者使用更具体的指标

if val_score &gt; best_score:
best_score = val_score
best_params = params

# 使用最佳超参数训练最终模型
best_model = xgb.XGBClassifier(**best_params)
best_model.fit(X_train, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning-randomizedsearchcv-xgboost</guid>
      <pubDate>Tue, 03 Dec 2024 13:26:56 GMT</pubDate>
    </item>
    <item>
      <title>获取文本分类的 Captum 文本解释时出错</title>
      <link>https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification</guid>
      <pubDate>Tue, 03 Dec 2024 12:47:45 GMT</pubDate>
    </item>
    <item>
      <title>Python 版本 3.8.2，迁移至 Python 版本 3.11.9。（PKL 文件兼容性问题）</title>
      <link>https://stackoverflow.com/questions/79247110/python-version-3-8-2-migrate-it-to-python-version-3-11-9-pkl-file-compatiblity</link>
      <description><![CDATA[我有一个使用 Python 版本 3.8.2 训练的 PKL 文件，但现在我需要将其迁移到 Python 版本 3.11.9。但是，当我在升级后的环境中执行它时，它会抛出一个错误，而在旧环境中它可以正常工作。
错误是：
TypeError：code() 参数 13 必须是 str，而不是 int

这是模型的路径：
在 python 3.8.2 中训练的所有模型中都出现类似的问题。
我已经尝试过 pickle 和 cloudpickle，
使用 subprocess 从 Python 3.11 环境执行 Python 3.8 脚本，但这不是永久的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79247110/python-version-3-8-2-migrate-it-to-python-version-3-11-9-pkl-file-compatiblity</guid>
      <pubDate>Tue, 03 Dec 2024 10:03:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的逻辑回归的准确率只有 25%？</title>
      <link>https://stackoverflow.com/questions/79247069/why-my-logistic-regression-has-25-accuracy</link>
      <description><![CDATA[我正在实现逻辑回归。我知道已经有很多数据库可以实现它。但问题是我无法理解那些。所以我为它创建了自己的数据集。
它有 3 个东西，房价、标准 和 购买决策
标准 代表生活水平。
0 : 低
1 : 中
2 : 高
当房价非常低时，只有生活水平低（0）的人才会买房。
当房价非常高时，只有生活水平高（2）的人才会买房。
这是我的实现的数据集和 ipynb 文件
测试数据集有 array(1,1,0,0) 作为购买决策，但我的模型给出 array(0,1,1,1)
我知道我的数据集很小，但这一定不是准确率如此低的唯一原因。
我做错了什么？如何执行。
如果链接无法访问，请告诉我。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79247069/why-my-logistic-regression-has-25-accuracy</guid>
      <pubDate>Tue, 03 Dec 2024 09:54:08 GMT</pubDate>
    </item>
    <item>
      <title>SVHN 数据集中的标签错误？[关闭]</title>
      <link>https://stackoverflow.com/questions/79244553/wrong-labels-in-svhn-dataset</link>
      <description><![CDATA[我一直在对 SVHN 数据集进行一些实验，主要是我想在进行一些训练之前裁剪出每个数字，这时我偶然发现测试数据集中的图像 53.png 有错误的标签（9 和 3，而不是 3 和 3）。
我很好奇是否有人也可以复制该问题，如果标签真的错了，也许可以建议如何处理它？&lt;​​/p&gt;
我从官方网站下载了数据集，解压缩并尝试读取 digitStruct.mat。
我的代码（假设所有数据都在 data/train 文件夹中）：
import os
from PIL import Image
from pymatreader import read_mat
import matplotlib.pyplot as plt
train_mat = read_mat(&#39;data/train/digitStruct.mat&#39;)

print(train_mat[&#39;digitStruct&#39;][&#39;name&#39;][52])
print(train_mat[&#39;digitStruct&#39;][&#39;bbox&#39;][52])

返回
&gt;&gt;53.png
&gt;&gt;{&#39;label&#39;: [9.0, 3.0], &#39;height&#39;: [84.0, 84.0], &#39;width&#39;: [59.0, 52.0], &#39;left&#39;: [160.0, 208.0], &#39;top&#39;: [34.0, 18.0]}

我还尝试显示其他图像以防出现类似异常，但我没有发现任何问题。
import cv2 
from matplotlib import pyplot as plt 

fig = plt.figure(figsize=(10, 7))

# 使用 OpenCV 读取图像（OpenCV 以 BGR 格式加载图像）
image1 = cv2.imread(&#39;data/train/&#39; + train_mat[&#39;digitStruct&#39;][&#39;name&#39;][0])
image2 = cv2.imread(&#39;data/train/&#39; + train_mat[&#39;digitStruct&#39;][&#39;name&#39;][27])
image3 = cv2.imread(&#39;data/train/&#39; + train_mat[&#39;digitStruct&#39;][&#39;name&#39;][52])
image4 = cv2.imread(&#39;data/train/&#39; + train_mat[&#39;digitStruct&#39;][&#39;name&#39;][90])

# 将图像从 BGR 转换为 RGB 格式，以便 Matplotlib 可以正确显示它们
image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)
image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)
image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)
image4 = cv2.cvtColor(image4, cv2.COLOR_BGR2RGB)

# 将第一幅图像添加到图中（左上角位置）
plt.subplot(2, 2, 1) # 2 行，2 列，第一个位置
plt.imshow(image1) 
plt.axis(&#39;off&#39;) # 隐藏轴标签
plt.title(train_mat[&#39;digitStruct&#39;][&#39;name&#39;][0] + &#39;\n&#39; + str(train_mat[&#39;digitStruct&#39;][&#39;bbox&#39;][0][&#39;label&#39;]))

# 将第二幅图像添加到图中（右上位置）
plt.subplot(2, 2, 2) # 2 行，2 列，第二个位置
plt.imshow(image2) 
plt.axis(&#39;off&#39;) # 隐藏轴标签
plt.title(train_mat[&#39;digitStruct&#39;][&#39;name&#39;][27] + &#39;\n&#39; + str(train_mat[&#39;digitStruct&#39;][&#39;bbox&#39;][27][&#39;label&#39;])) 

# 将第三幅图像添加到图中（左下位置）
plt.subplot(2, 2, 3) # 2 行，2 列，第三个位置
plt.imshow(image3)
plt.axis(&#39;off&#39;) # 隐藏轴标签
plt.title(train_mat[&#39;digitStruct&#39;][&#39;name&#39;][52] + &#39;\n&#39; + str(train_mat[&#39;digitStruct&#39;][&#39;bbox&#39;][52][&#39;label&#39;])) 
# 将第四幅图像添加到图中（右下角位置）
plt.subplot(2, 2, 4) # 2 行，2 列，第四个位置
plt.imshow(image4) 
plt.axis(&#39;off&#39;) # 隐藏轴标签
plt.title(train_mat[&#39;digitStruct&#39;][&#39;name&#39;][90] + &#39;\n&#39; + str(train_mat[&#39;digitStruct&#39;][&#39;bbox&#39;][90][&#39;label&#39;]))

输出：SVHN 比较]]></description>
      <guid>https://stackoverflow.com/questions/79244553/wrong-labels-in-svhn-dataset</guid>
      <pubDate>Mon, 02 Dec 2024 15:03:58 GMT</pubDate>
    </item>
    <item>
      <title>anomalib 的零样本 winCLIP 不起作用</title>
      <link>https://stackoverflow.com/questions/79244492/zero-shot-winclip-from-anomalib-not-working</link>
      <description><![CDATA[随着异常分类/分割的最新进展，我想尝试新的 winCLIP 模型，anomalib 库也有一个实现。
如何测试零样本或为少样本提供几张“正常/健康”图像？由于这是一个零样本模型，我认为它很容易开箱即用，但我无法让它工作。这是我当前的代码：
from anomalib.models.image import WinClip
from anomalib.engine import Engine

# 导入数据模块
from anomalib.data import Folder
from anomalib.data.utils import TestSplitMode

# 创建数据模块
datamodule = Folder(
name=&quot;lasercut_plank&quot;,
root=&quot;./DATA_0shot&quot;,
normal_dir=&quot;normal&quot;,
test_split_mode=TestSplitMode.NONE
)

# 设置数据模块
datamodule.setup()

# 访问数据集
train_dataset = datamodule.train_data

# 访问数据加载器
train_dataloader = datamodule.train_dataloader()

# 创建模型和引擎
model = WinClip(class_name=&quot;lasercut_plank&quot;)
engine = Engine(task=&quot;segmentation&quot;)

# 在给定的数据模块上训练 Patchcore 模型
engine.train(datamodule=datamodule, model=model)
]]></description>
      <guid>https://stackoverflow.com/questions/79244492/zero-shot-winclip-from-anomalib-not-working</guid>
      <pubDate>Mon, 02 Dec 2024 14:43:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么预先训练的 Swin Transformer 编码器在 TPU 上失败但在 Colab 中的 CPU 上可以运行？</title>
      <link>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</link>
      <description><![CDATA[我正在处理图像分割任务，并尝试使用预先训练的 Swin Transformer Large (Swin-L) 编码器作为特征提取主干。代码在 Colab 中的 CPU 上完美运行。但是，当切换到 TPU 时，它会抛出如下所示的错误。
代码：
from tensorflow.keras import layer, Model, Input
from tfswin import SwinTransformerLarge224

def load_swin_encoder(input_shape=(512, 512, 3)):
# 加载预训练的 Swin-L 模型
swin_encoder = SwinTransformerLarge224(include_top=False, weights=&#39;imagenet&#39;,
input_shape=input_shape)

# 冻结预训练层
for layer in swin_encoder.layers:
layer.trainable = False

# 从四个阶段提取输出
stage_outputs = [
swin_encoder.get_layer(&#39;normalize&#39;).output, # 从 0 阶段输出
swin_encoder.get_layer(&#39;layers.0&#39;).output, # 第一阶段的输出
swin_encoder.get_layer(&#39;layers.1&#39;).output, # 第二阶段的输出
swin_encoder.get_layer(&#39;layers.2&#39;).output, # 第三阶段的输出
swin_encoder.get_layer(&#39;layers.3&#39;).output, # 第四阶段的输出
]
return Model(swin_encoder.input, stage_outputs, name=&quot;SwinTransformerEncoder&quot;)

# 测试代码
encoder = load_swin_encoder(input_shape=(512, 512, 3))
dummy_input = tf.random.uniform((1, 512, 512, 3))
encoder_outputs =coder(dummy_input)

for i, output in enumerate(encoder_outputs):
print(f&quot;阶段 {i + 1} 输出形状：{output.shape}&quot;)


错误：
代码在 TPU 上抛出以下错误：
------------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-3cb122d32678&gt; 在 &lt;cell line: 2&gt;()
1 # 加载健全性检查
----&gt; 2 编码器 = load_swin_encoder(input_shape=(512, 512, 3))
3 dummy_input = tf.random.uniform((1, 512, 512, 3))
4 编码器输出 = 编码器(dummy_input)
5 

2 帧
/usr/local/lib/python3.10/dist-packages/keras/src/models/ functional.py in __init__(self, 输入, 输出, 名称, **kwargs)
117 for x in flat_inputs:
118 if not isinstance(x, backend.KerasTensor):
-&gt; 119 引发 ValueError(
120 “所有 `inputs` 值都必须是 KerasTensors。已收到：”
121 f“inputs={inputs} 包括无效值 {x}”

ValueError：所有 `inputs` 值都必须是 KerasTensors。已收到：inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层 &#39;input_4&#39; 创建”) 包括无效值 KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层创建” &#39;input_4&#39;&quot;) 类型为 &lt;class &#39;tf_keras.src.engine.keras_tensor.KerasTensor&#39;&gt;


问题：
为什么此代码在 Colab 中的 CPU 上有效，但在 TPU 上失败？我该如何修复此问题以使其与 TPU 执行兼容？
任何见解或指导都将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</guid>
      <pubDate>Mon, 02 Dec 2024 13:35:57 GMT</pubDate>
    </item>
    <item>
      <title>set_transform 或 with_transform 之后 transformer 的数据集结构出现意外</title>
      <link>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</guid>
      <pubDate>Sun, 01 Dec 2024 14:07:14 GMT</pubDate>
    </item>
    <item>
      <title>这些 `[0]` 在创建变量时是否有意义</title>
      <link>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</link>
      <description><![CDATA[使用 HuggingFace 工具集微调 Gemma 的指南位于：https://huggingface.co/blog/gemma-peft
链接到以下行：https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A
数据输入格式化函数是：
def formatting_func(example):
text = f&quot;Quote: {example[&#39;quote&#39;][0]}\nAuthor: {example[&#39;author&#39;][0]}&lt;eos&gt;&quot;
return [text]

这些 [0] 有意义吗？它们看起来不对，因为当打印出 text 变量时，我可以看到它们只是字符而不是字符串。]]></description>
      <guid>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</guid>
      <pubDate>Fri, 29 Nov 2024 10:14:43 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>在 SageMaker 上的 TensorFlow 推荐器中初始化 FactorizedTopK 时出错：“无法将‘计数器’转换为形状”</title>
      <link>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</guid>
      <pubDate>Tue, 12 Mar 2024 03:28:18 GMT</pubDate>
    </item>
    </channel>
</rss>