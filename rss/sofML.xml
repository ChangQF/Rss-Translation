<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 15 Nov 2024 15:18:30 GMT</lastBuildDate>
    <item>
      <title>是否应将规范化应用于交互特征或交互项</title>
      <link>https://stackoverflow.com/questions/79192800/should-normalization-be-applied-on-interaction-feature-or-interaction-term</link>
      <description><![CDATA[我正在机器学习模型中使用交互特征，通过将数值变量与编码分类特征相乘来创建新特征。我的问题是：
是否应该对这些交互项应用规范化？

如果是，那么规范化不会改变交互项的含义吗？具体来说，当我先对数值特征进行归一化，然后创建交互项时，交互项是否仍表示其最初要捕获的关系？

如果在创建交互项之前对数值变量进行归一化，交互项是否会失去其真实比例或含义？


例如，如果我将归一化数值特征与分类变量（可以是独热编码）相乘，我是否会扭曲数值特征与类别之间的原始关系？
我希望澄清交互项是否应归一化或保持原样，特别是在交互项在捕获特定关系中起关键作用的情况下。
谢谢！
我尝试了什么：
我尝试在创建交互项之前对数值特征进行归一化。具体来说，我先对数值变量进行归一化，然后将其与编码的分类特征相乘。我还尝试在不先对数值变量进行归一化的情况下创建交互特征，以比较这两种方法。
我期望什么？
我希望了解在创建交互项之前对数值特征进行归一化是否会影响模型捕捉数值和分类特征之间预期关系的能力。我还很好奇，由于数值特征的归一化，交互项的含义是否会保留或扭曲。我希望了解归一化是否会导致交互项失去其原始规模和重要性，或者它是否有利于模型收敛和性能。]]></description>
      <guid>https://stackoverflow.com/questions/79192800/should-normalization-be-applied-on-interaction-feature-or-interaction-term</guid>
      <pubDate>Fri, 15 Nov 2024 14:28:44 GMT</pubDate>
    </item>
    <item>
      <title>如何从头开始创建模型以从扫描的发票中提取文本和表格数据</title>
      <link>https://stackoverflow.com/questions/79192367/how-can-i-create-a-model-from-scratch-to-extract-text-and-table-data-from-scanne</link>
      <description><![CDATA[所以我目前正在做一个项目，我们收到了 25 种不同的发票类型，全部都经过了扫描。最终目标是从发票中提取文本和表格数据，然后最终将这些数据解析为 Excel。发票类型采用不同的格式。我们如何提取表格数据 + 文本？我们可以为 25 种发票类型创建 1 个模型来执行此操作吗？还是我们需要 25 个模型。]]></description>
      <guid>https://stackoverflow.com/questions/79192367/how-can-i-create-a-model-from-scratch-to-extract-text-and-table-data-from-scanne</guid>
      <pubDate>Fri, 15 Nov 2024 12:24:08 GMT</pubDate>
    </item>
    <item>
      <title>对图像集的平均值进行标注[关闭]</title>
      <link>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</link>
      <description><![CDATA[我正在寻找一种用一个句子描述一组图像的方法。或者，我需要一种方法来在概念上平均一组图像，然后再将该“概念”（可能是特征向量）提供给常规字幕模型。
为什么？
用于 Lora 训练评估。在适合整个数据集的提示上测试训练后的生成模型会很有用，而不是选择单个图像的标题或尝试找出它们之间的共同点。此外，这还允许生成单个负面提示来测试模型在范围外的提示上的表现。
我到目前为止所做的：
我已经修改了现有的 CLIP+BLIP 询问器以处理图像集（它也可以生成负片）。然而，虽然 CLIP 字幕允许在使用图像特征选择最佳字幕之前对其进行平均，但它的准确性远低于 BLIP 生成的字幕，后者仅适用于单幅图像。我需要一个可以像 CLIP 一样接收特征向量的模型，这样我就可以对它们进行预处理。]]></description>
      <guid>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</guid>
      <pubDate>Fri, 15 Nov 2024 12:19:27 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CoreML 预测中获取置信度变量</title>
      <link>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</link>
      <description><![CDATA[我正在使用 CreateML 工具训练文本分类器，当我使用预览功能并输入一个句子时，它会给我一个预测以及一个置信度变量
以下是我在应用程序上使用该模型的方式
import CoreML
...

func predict(phrase:String) -&gt; String {
guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
return &quot;Failed to load the Roll Model.&quot;
}

let rollModelInput = RollInput(text: phrase)

guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
return &quot;Roll Model Prediction Failed&quot;
}

return prediction.label
}

这有效，它提供了预测。
我的数据是标准文本/标签格式
即使我将模型导出到 xcode 并在 xcode 中运行预览，置信度变量仍然存在。
当我在设备上运行预测时，我想知道置信度变量是什么，我如何获取访问权限？
]]></description>
      <guid>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</guid>
      <pubDate>Fri, 15 Nov 2024 11:13:21 GMT</pubDate>
    </item>
    <item>
      <title>回答 Tensor Flow 警告 - 警告：TensorFlow：您的输入数据不足；中断训练</title>
      <link>https://stackoverflow.com/questions/79191359/answer-tensor-flow-warning-warningtensorflowyour-input-ran-out-of-data-inte</link>
      <description><![CDATA[结果
使用微调进行训练
每轮训练步骤：18
每轮验证步骤：4
训练生成器批量大小：32
总训练样本：576
总验证样本：144
轮次 1/5
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121：UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()
18/18 ━━━━━━━━━━━━━━━━━━━━━ 399s 19s/step - 准确率：0.1593 - 损失：2.1888 - val_accuracy：0.3281 - val_loss：1.8363
Epoch 2/5
/usr/lib/python3.10/contextlib.py:153: UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。构建数据集时可能需要使用 `.repeat()` 函数。
self.gen.throw(typ, value, traceback)
18/18 ━━━━━━━━━━━━━━━━━━━━━━ 5s 294ms/step - 准确度：0.0000e+00 - 损失：0.0000e+00 - val_accuracy：0.3750 - val_loss：1.8651
Epoch 3/5
18/18 ━━━━━━━━━━━━━━━━━━━━━━━ 361s 18s/step - 准确度：0.3593 - 损失： 1.7475 - val_accuracy：0.4141 - val_loss：1.6302
Epoch 4/5
18/18 ━━━━━━━━━━━━━━━━━━━━━━ 41s 2s/步 - 准确度：0.0000e+00 - 损失：0.0000e+00 - val_accuracy：0.4375 - val_loss：1.4419
Epoch 5/5
18/18 ━━━━━━━━━━━━━━━━━━━━━ 322s 18s/step - 准确率：0.5175 - 损失：1.4328 - val_accuracy：0.3984 - val_loss：1.4993

完整代码
(https://drive.google.com/file/d/1jHqDnCnLMHeIZ9nn4Y9O8qZUhcvFtW4k/view?usp=sharing)
您能帮我解决错误吗]]></description>
      <guid>https://stackoverflow.com/questions/79191359/answer-tensor-flow-warning-warningtensorflowyour-input-ran-out-of-data-inte</guid>
      <pubDate>Fri, 15 Nov 2024 06:58:14 GMT</pubDate>
    </item>
    <item>
      <title>如何知道平均绝对误差（MAE）是否是根据测试数据计算出来的？</title>
      <link>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</link>
      <description><![CDATA[我正在尝试使用 R 中的留一法交叉验证 (LOOCV) 来计算各种类型模型的平均绝对误差 (MAE)。在此链接中，作者展示了使用 R 中的 caret 包进行 LOOCV 然后计算 MAE 来计算 MAE 的指南。
链接为：https://www.statology.org/leave-one-out-cross-validation-in-r/
结果为
library(caret)

#指定交叉验证方法
ctrl &lt;- trainControl(method = &quot;LOOCV&quot;)

#拟合回归模型并使用 LOOCV 评估性能
model &lt;- train(y ~ x1 + x2, data = df, method = &quot;lm&quot;, trControl = ctrl)

#查看 LOOCV 摘要
print(model)

线性回归

10 个样本
2 个预测器

无预处理
重采样：留一法交叉验证
样本量摘要：9、9、9、9、9、9、...

重采样结果：

RMSE Rsquared MAE 
3.619456 0.6186766 3.146155

调整参数“截距”保持不变，值为 TRUE

但是，尚不清楚 MAE 是使用训练数据还是测试数据（样本外）计算的。]]></description>
      <guid>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</guid>
      <pubDate>Fri, 15 Nov 2024 05:50:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HMM 模型对某些动作进行门预测？</title>
      <link>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</link>
      <description><![CDATA[我想使用 IMU 传感器来预测用户接下来要进入哪个门（用于行走、跑步、跳跃等）。根据我的研究，HMM 似乎是状态预测的最佳机器学习模型。但我在网上看到的所有模型都是基于预先记录的静态数据，而不是正在记录的实时数据。我应该如何实现这一点？
我已经实现了一个滚动窗口，它使用最后捕获的 30 条数据记录进行下一次预测，并对它们进行了规范化。
我已经确定了我想要识别的每个动作的门，并且我已经设置了一个分类算法，该算法遍历每个窗口并使用它来对当前动作进行分类。
我打算用它们来验证我将要做出的预测（或者甚至在需要时将其用作输入来预测人的下一个动作）。]]></description>
      <guid>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</guid>
      <pubDate>Thu, 14 Nov 2024 23:15:03 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：预期隐藏[0]大小（2，50，1024），在PyTorch中得到[1，50，1024]</title>
      <link>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</guid>
      <pubDate>Thu, 14 Nov 2024 07:55:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 Apache Spark ML 进行预测</title>
      <link>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</link>
      <description><![CDATA[我是 Apache Spark ML 的新手。
我想预测按年龄和国家/地区划分的余额。作为输入，我有一个以下格式的 CSV 文件：
RowNumber,Age,Country,Balance

模型已构建，也可以针对测试数据进行训练。到目前为止一切正常。
我现在的问题是当我想对新客户记录进行预测时
Dataset&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, ‘Germany’)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);

我收到以下错误消息：
java.lang.IllegalArgumentException：CountryIndex 不存在。可用：年龄、国家。

如何获取新数据集的预测？
public static void main(String[] args) {

SparkSession spark = SparkSession
.builder()
.master(&quot;local[*]&quot;) 
.appName(&quot;JavaGeneralizedLinearRegressionExample&quot;)
.getOrCreate();

Dataset&lt;Row&gt; data = spark.read()
.option(&quot;header&quot;, &quot;true&quot;)
.option(&quot;inferSchema&quot;, &quot;true&quot;)
.option(&quot;delimiter&quot;, &quot;,&quot;) // oder &quot;,&quot;您可以使用 Dataiformat
.csv(&quot;/data/testdaten_v4.csv&quot;);

StringIndexer countryIndexer = new StringIndexer()
.setInputCol(&quot;Country&quot;)
.setOutputCol(&quot;CountryIndex&quot;)
.setHandleInvalid(&quot;skip&quot;);
OneHotEncoder countryEncoder = new OneHotEncoder()
.setInputCol(&quot;CountryIndex&quot;)
.setOutputCol(&quot;CountryVec&quot;);

VectorAssembler assembler = new VectorAssembler()
.setInputCols(new String[]{&quot;Age&quot;, &quot;CountryVec&quot;}) // 可以添加其他 Features
.setOutputCol(&quot;features&quot;);

StandardScaler scaler = new StandardScaler()
.setInputCol(&quot;features&quot;)
.setOutputCol(&quot;scaledFeatures&quot;);

LinearRegression lr = new LinearRegression()
.setLabelCol(&quot;Balance&quot;)
.setFeaturesCol(&quot;scaledFeatures&quot;)
.setMaxIter(100)
.setRegParam(0.3)
.setElasticNetParam(0.8);

Pipeline pipeline = new Pipeline()
.setStages(new PipelineStage[]{countryIndexer, countryEncoder, assembler, scaler, lr});

PipelineModel model = pipeline.fit(data);

Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[]{0.8, 0.2}, 42);
数据集&lt;Row&gt; trainData = splits[0];
数据集&lt;Row&gt; testData = splits[1];

数据集&lt;Row&gt; predictions = model.transform(testData);
predictions.select(&quot;Age&quot;, &quot;Country&quot;, &quot;Balance&quot;, &quot;prediction&quot;).show();

RegressionEvaluator evaluator = new RegressionEvaluator()
.setLabelCol(&quot;Balance&quot;)
.setPredictionCol(&quot;prediction&quot;)
.setMetricName(&quot;rmse&quot;);
double rmse = evaluator.evaluate(predictions);

数据集&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, &quot;Germany&quot;)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);
newCustomerPrediction.select(&quot;prediction&quot;).show();

spark.stop();
}

public static class Customer {
private int Age;
private String Country;

public Customer(int age, String country) {
this.Age = age;
this.Country = country;
}

public int getAge() { return Age; }
public String getCountry() { return Country; } 
}
]]></description>
      <guid>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</guid>
      <pubDate>Wed, 13 Nov 2024 17:42:53 GMT</pubDate>
    </item>
    <item>
      <title>VSCode 安装 hugginface relik 库时出错</title>
      <link>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</guid>
      <pubDate>Tue, 12 Nov 2024 19:53:20 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型中的自定义编码器和解码器层显示为未构建</title>
      <link>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</guid>
      <pubDate>Sat, 28 Sep 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>如何将多个观测值拟合到单个高斯过程</title>
      <link>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</link>
      <description><![CDATA[我试图将多个观测值拟合到单个高斯过程。
我尝试像这样拟合两个观测值 (Y) 的数据：
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# 示例数据

# 输入数据 X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])

# 输出数据 Y 
Y = np.array([[1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# 拟合模型
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict(X, return_cov=True)

我得到了两个 mean_prediction 数组和两个 cov_prediction 矩阵。但我想要一个与单个拟合 GP 对应的观测值相同维度的单个均值和协方差矩阵。我该如何实现？]]></description>
      <guid>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</guid>
      <pubDate>Thu, 30 May 2024 12:16:10 GMT</pubDate>
    </item>
    <item>
      <title>为嵌套在多个文件夹中的数据创建训练和测试拆分</title>
      <link>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</link>
      <description><![CDATA[我正在准备用于训练图像识别模型的数据。我目前有一个文件夹（数据集），其中包含多个带有标签名称的文件夹，这些文件夹中包含图像。
我想以某种方式拆分此数据集，以便我有两个具有相同子文件夹的主文件夹，但这些文件夹中的图像数量应根据首选的训练/测试拆分，例如，训练数据集中的 90% 的图像和测试数据集中的 10% 的图像。
我正在努力寻找拆分数据的最佳方法。我读过一个建议，pytorch torch.utils.Dataset 类可能是一种方法，但我似乎无法让它工作以保留文件夹层次结构。]]></description>
      <guid>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</guid>
      <pubDate>Mon, 09 Nov 2020 19:27:34 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    </channel>
</rss>