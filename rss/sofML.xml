<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 26 Apr 2024 09:15:01 GMT</lastBuildDate>
    <item>
      <title>如何在数学方程中表示使用样条变换器 + 线性回归拟合的模型。数据是高维的</title>
      <link>https://stackoverflow.com/questions/78389349/how-to-represent-a-model-fitted-using-spline-transformer-linear-regression-in</link>
      <description><![CDATA[我有一个由 5 个特征和 1 个目标组成的数据集。我使用样条变换器来转换特征，然后使用线性回归来拟合它。我正在使用 1 度和 5 节样条变换特征来拟合数据。现在我想用数学方程表示模型拟合，其中包括各个项以及交互项。
我怎样才能做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78389349/how-to-represent-a-model-fitted-using-spline-transformer-linear-regression-in</guid>
      <pubDate>Fri, 26 Apr 2024 09:02:21 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关优化建筑平面图中门类型分析超参数的建议</title>
      <link>https://stackoverflow.com/questions/78389255/seeking-advice-on-optimizing-hyperparameters-for-door-type-analysis-in-architect</link>
      <description><![CDATA[我目前正在处理一个具有挑战性的项目，其中涉及检测建筑平面图中的门。虽然门的检测精度相当不错，但我在分析门类型时遇到了困难。
有超过 25 类门类型，例如 barn_single、bifold_double、swing_single_right_hand、swing_single_left_hand 等。每个类代表不同类型的门，这增加了任务的复杂性。
我有一个包含大约 5000 张平面图图像的数据集，我用它来训练模型。
我正在开发基于 Windows 的系统，如果您能帮助我确定超参数以改善门类型分析的结果，我将不胜感激。超参数是控制模型学习过程的设置，例如学习率、批量大小和训练周期数。
我使用 YOLOv7 作为预训练模型。
如果您熟悉机器学习或计算机视觉技术，任何有关调整这些超参数以获得更好结果的建议都将非常宝贵。此外，如果有任何不清楚的地方或者您需要更多详细信息，请告诉我，我将提供更多信息。
我尝试将学习率设置为

0.01
0.001
0.02
等等..

但最高可达68%。这是相当低的。主要原因是门型分类错误。检测门的准确率约为 96%。到目前为止，这是相当有效的。]]></description>
      <guid>https://stackoverflow.com/questions/78389255/seeking-advice-on-optimizing-hyperparameters-for-door-type-analysis-in-architect</guid>
      <pubDate>Fri, 26 Apr 2024 08:44:38 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法标准化代表同一事物的不同文本？</title>
      <link>https://stackoverflow.com/questions/78388910/is-there-any-way-to-standardize-different-text-that-represents-the-same-thing</link>
      <description><![CDATA[我正在从不同超市的网站收集产品数据。但我发现每个商店用不同的名称来代表产品。
例如：

&lt;标题&gt;

产品名称
商店
链接


&lt;正文&gt;

360°牙刷，带舌头和脸颊清洁器
没有多余的装饰
产品链接&lt; /td&gt;


中号牙刷，360°
食品基础知识
产品链接



我想将其标准化为“360° 牙刷”。
我的方法是首先尝试对产品名称进行标记，然后通过余弦定律比较相似度。但我发现这不是正确的方向，因为我在刷新到数据库之前正在寻找标准化名称（以便按商店对“相同”产品进行分类）。
请在下面找到我的测试脚本（感谢 ChatGPT！）
导入火炬
从 Transformer 导入 BertTokenizer、BertModel

def get_sentence_vector(文本):
  
    # 初始化分词器和模型
    tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
    模型 = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)

    # 确保模型处于评估模式
    模型.eval()

    # 对文本进行编码
    输入 = tokenizer(文本, return_tensors=&#39;pt&#39;)

    # 执行前向传播而不计算梯度
    使用 torch.no_grad()：
        输出=模型（**输入）

    # 提取最后一个隐藏状态
    最后隐藏状态 = 输出.最后隐藏状态

    # 使用均值池聚合隐藏状态
    句子向量 = torch.mean(last_hidden_​​states, 暗淡=1)

    返回句子向量


def cosine_similarity(vec1, vec2):
    # 计算余弦相似度
    cosine_sim = torch.nn.function.cosine_similarity(vec1, vec2, dim=1)
    
    返回 cosine_sim.item()


example_text = “360° 牙刷，带舌头和脸颊清洁器”
example_text_2 =“中号牙刷，360°”
example_text_3 =“Hair Expertise 透明质酸丰盈洗发水，含透明质酸”

向量 = get_sentence_vector(example_text)
矢量_2 = get_sentence_vector(example_text_2)
矢量_3 = get_sentence_vector(example_text_3)

打印（余弦_相似度（向量，向量_2））＃0.8426903486251831
打印（余弦_相似度（向量，向量_3））＃0.7190334796905518
打印（余弦_相似度（向量_2，向量_3））＃0.622465193271637

上述做法确实给了我一些感觉，相同的产品具有更高的相似度。但如何利用这个结果来给出一个标准化的名称呢？]]></description>
      <guid>https://stackoverflow.com/questions/78388910/is-there-any-way-to-standardize-different-text-that-represents-the-same-thing</guid>
      <pubDate>Fri, 26 Apr 2024 07:36:19 GMT</pubDate>
    </item>
    <item>
      <title>ML.NET 和 TensorFlow 模型之间的版本问题</title>
      <link>https://stackoverflow.com/questions/78388834/version-problem-between-ml-net-and-tensorflow-models</link>
      <description><![CDATA[我开发了一个人脸识别器（作为图像分类模型）软件。我通过图像分类目录训练了几个 ml.net 模型，并通过 colab 训练了张量流模型。所有模型都令人满意，但我有一个大问题；
为了使用 ml.net 训练的模型，我必须安装 nuget SciSharp.TensorFlow.Redist 2.3.1，但其他一些模型（colab 训练的模型）在我尝试使用时会出现此错误；
&lt;块引用&gt;
Tensorflow.InvalidArgumentError：将 GraphDef 转换为 Graph 有
失败的。尝试导入 GraphDef 的二进制文件是在以下时间构建的
GraphDef 版本是 440。GraphDef 是由构建的二进制文件生成的
当 GraphDef 版本为 1645 时。

当我更新 nuget SciSharp.TensorFlow.Redist 2.16.0 colab 训练的模型时工作正常，但 ml.net 训练的模型给出此错误；
&lt;块引用&gt;
System.EntryPointNotFoundException：无法找到入口点
DLL“tensorflow”中名为“TF_StringEncodedSize”。在
Tensorflow.c_api.TF_StringEncodedSize（UInt64 len）位于
Microsoft.ML.Vision.ImageClassificationTrainer.EncodeByteAsString(VBuffer1 缓冲区) 在 Microsoft.ML.Vision.ImageClassificationTrainer.ImageProcessor.ProcessImage(VBuffer1&amp;
图像缓冲区）在
Microsoft.ML.Vision.ImageClassificationModelParameters.Classifier.Score(VBuffer1&amp; image, Span1 classProbativity) at
Microsoft.ML.Vision.ImageClassificationModelParameters.&lt;&gt;c__DisplayClass22_02.b__0(VBuffer1&amp;
src, VBuffer1&amp; dst) 位于 Microsoft.ML.Data.SchemaBindablePredictorWrapperBase。&lt;&gt;c__DisplayClass19_02.b__0(TDst&amp;
夏令时）在
Microsoft.ML.Data.PredictedLabelScorerBase.EnsureCachedPosition[TScore](Int64&amp;
缓存位置、TScore&amp; Microsoft.ML.Data.MulticlassClassificationScorer.&lt;&gt;c__DisplayClass16_0.b__1(VBuffer1&gt;score、DataViewRowboundRow、ValueGetter1 ScoreGetter)。
夏令时）在
Microsoft.ML.Data.TypedCursorable1.TypedRowBase.&lt;&gt;c__DisplayClass8_01.b__0(TRow
行）在
Microsoft.ML.Data.TypedCursorable1.TypedRowBase.FillValues(TRow row) 在 Microsoft.ML.Data.TypedCursorable1.RowImplementation.FillValues(TRow)
行）位于 Microsoft.ML.PredictionEngineBase2.FillValues（TDst 预测）位于 Microsoft.ML.PredictionEngine2.Predict(TSrc
例如，TDst&amp;预测）在
Microsoft.ML.PredictionEngineBase`2.Predict（TSrc 示例）位于
FaceRecognizer.MyTrainer.Predictor.Predict(GlobalImageInput inp) 中
E:\source\repos\Setup_Projects\FaceRecognizer\FaceRecognizer\MyTrainer\Predictor.cs:line
52

我发现 TF_StringEncodedSize 函数在tensorflow 2.4.0之后不再存在。
有没有办法在我的应用程序中使用所有模型？例如，我可以在运行时使用此 SciSharp.TensorFlow.Redist nuget 的不同版本吗？我的意思是有什么方法可以在运行时更改此 nuget 的版本吗？如果不是，我该如何处理这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/78388834/version-problem-between-ml-net-and-tensorflow-models</guid>
      <pubDate>Fri, 26 Apr 2024 07:23:10 GMT</pubDate>
    </item>
    <item>
      <title>所有测试结果都是图像分类模型中的同一个问题</title>
      <link>https://stackoverflow.com/questions/78388658/all-test-results-are-the-same-issue-in-image-classification-model</link>
      <description><![CDATA[我目前正在使用 Xception 模型执行图像分类任务。只有 2 个类，总共 1000 个数据点，其中 0 类有 500 张图像，1 类有 500 张图像。测试大小为 0.2。 （这意味着 800 张图像用于训练，200 张用于测试）
最初，在加载图像时，我在创建数据加载器时标记了数据并执行标准化，如下面的代码。
transform = Transforms.Compose([
    变换.调整大小((299, 299)),
    变换.ToTensor(),
    变换.Normalize(平均值=[0.485,0.456,0.406],
                         标准=[0.229,0.224,0.225])
]）

train_data_loader = DataLoader（train_data_set，batch_size = 30，shuffle = True，drop_last = True）
test_data_loader = DataLoader（test_data_set，batch_size = 1，shuffle = True）

我使用 CrossEntropyLoss 作为损失函数。
训练后，成本值似乎还算令人满意。
但是，在测试时，所有结果都为0（作为参考，测试数据分为100张0类图像和100张1类图像）。
张量（[[ 0.4999，-0.4620]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4494，-0.4120]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4950，-0.4526]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4336，-0.3971]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4399，-0.4035]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5073，-0.4672]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5065，-0.4716]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4953，-0.4554]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5350，-0.4943]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5159，-0.4766]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5014，-0.4609]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4961，-0.4575]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4405，-0.4099]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4533，-0.4171]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4746，-0.4371]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4345，-0.3973]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4214，-0.3859]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4672，-0.4299]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4991，-0.4604]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5080，-0.4693]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.5225，-0.4835]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4816，-0.4440]]，设备=&#39;cuda：0&#39;）张量（[1]，设备=&#39;cuda：0&#39;）
张量（[[ 0.4598，-0.4247]]，设备=&#39;cuda：0&#39;）张量（[0]，设备=&#39;cuda：0&#39;）

（左侧张量是预测，右侧张量是标签）
因此，由于只有 2 个类，我尝试使用 BCELoss，但结果是相同的。
为什么在训练时，性能很好，但在测试时，结果却总是一样？
实际上，我在使用 CNN 时也遇到了类似的问题，但我找不到解决方案。
是因为缺乏数据吗？还是还有其他原因？请指教。
我尝试使用 BCELoss
我在与 CNN 合作时遇到了类似的问题]]></description>
      <guid>https://stackoverflow.com/questions/78388658/all-test-results-are-the-same-issue-in-image-classification-model</guid>
      <pubDate>Fri, 26 Apr 2024 06:40:16 GMT</pubDate>
    </item>
    <item>
      <title>如何部署机器学习模型？我有我的项目的代码和 .pt 文件</title>
      <link>https://stackoverflow.com/questions/78388635/how-to-deploy-a-machine-learning-model-i-have-code-and-pt-files-of-my-project</link>
      <description><![CDATA[我刚刚开始机器学习，所以我只知道如何构建模型，但不知道如何部署它或将来要做什么。我有我的代码、csv 文件、模型状态 (.pt) 文件。我需要这方面的指导，提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78388635/how-to-deploy-a-machine-learning-model-i-have-code-and-pt-files-of-my-project</guid>
      <pubDate>Fri, 26 Apr 2024 06:34:10 GMT</pubDate>
    </item>
    <item>
      <title>寻找最佳路由解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/78388418/looking-for-best-routing-solution</link>
      <description><![CDATA[我正在处理呼叫路由问题陈述。这里的呼叫路由意味着，当客户呼叫到达客户服务时，自定义将根据客户选择的菜单选项路由到代理。
我遇到的问题比这更复杂。
客户信息将包括位置、所选菜单选项、世界各地的语言。理想的代理应该是具有相同语言能力、能够解决所选菜单选项的人。
但智能体也并不复杂，智能体可能拥有不止一种语言技能，并且更擅长解决多个问题。如果来自美国的人打电话（即英语），则代理可能具有英语、俄语技能。因此，如果位置是俄罗斯，也应该根据代理的可用性，将此人作为目标，否则会转到语言为俄语的其他代理。
遇到这样的问题我该怎么解决呢？有什么帮助或资源吗？]]></description>
      <guid>https://stackoverflow.com/questions/78388418/looking-for-best-routing-solution</guid>
      <pubDate>Fri, 26 Apr 2024 05:32:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么 50 个预测中只有 45 个，这里预测的是哪一列？</title>
      <link>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</link>
      <description><![CDATA[我在为自己的数据编码时使用了张量流教程。代码如下：
训练、验证、测试 = 数据[:420]、数据[420:450]、数据[450:]

类窗口生成器（）：
  def __init__(自我，输入宽度，标签宽度，移位，
             训练、验证、测试）：
    self.train = 火车
    self.val = val
    自测=测试
    self.input_width = input_width
    self.label_width = 标签宽度
    self.shift = 移位
    self.total_window_size = input_width + 移位
    self.input_slice = 切片(0,input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]
    self.label_start = self.total_window_size-self.label_width
    self.labels_slice = slice(self.label_start,无)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(自我):
    返回 &#39;​​\n&#39;.join([
        f&#39;总window_size: {self.total_window_size}&#39;,
        f&#39;输入索引：{self.input_indices}&#39;,
        f&#39;标签索引：{self.label_indices}&#39;])

  def split_window(自身,特征):
    输入=特征[:,self.input_slice,:]
    标签 = 特征[:,self.labels_slice,:]
    input.set_shape([无,self.input_width,无])
    labels.set_shape([无,self.label_width,无])
    返回输入、标签

  def make_dataset(自身,数据):
    数据 = np.array(数据,dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(数据=数据，
                                                    目标=无，
                                                    序列长度 = self.total_window_size,
                                                    序列步幅 = 1,
                                                    随机播放=真，
                                                    批量大小 = 32,)
    ds = ds.map(self.split_window)
    返回数据

  @财产
  def train_(自身):
    返回 self.make_dataset(self.train)
  @财产
  def val_(自身):
    返回 self.make_dataset(self.val)
  @财产
  def test_(自我):
    返回 self.make_dataset(self.test)
最大纪元 = 100
defcompile_and_fit（模型，窗口，耐心= 30）：
  Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;,
                               耐心=耐心，
                               模式=&#39;分钟&#39;,
                              详细 =1)
  reduce_lr =ReduceLROnPlateau(监视器=&#39;val_loss&#39;,因子=0.1,耐心=10,min_lr=1e-6,详细=1)
  model.compile(loss=MeanSquaredError(), 优化器 = Adam(), 指标=[MeanAbsoluteError()])
  历史= model.fit(window.train_,epochs=MAX_EPOCHS,validation_data=window.val_,callbacks=[early_stopping,reduce_lr])
  返回历史记录
Wide_window = WindowGenerator(input_width = 5,
                          标签宽度=1，
                          移位= 1，
                          火车=火车，
                          值=值，
                          测试=测试）
conv_model = 顺序（[输入（形状=（5,2），名称=&#39;编码器输入&#39;），
                     Conv1D(filters=32,kernel_size=5,activation=&#39;relu&#39;,name=&#39;conv1D&#39;),
                    密集（32，激活=&#39;relu&#39;，名称=&#39;dense1&#39;），
                    密集(1,name=&#39;dense2&#39;)])
历史=compile_and_fit(conv_model,wide_window)
y_pred = conv_model.predict(wide_window.test_)`

现在，输出是 (45,1,1)。由于测试大小为 50，输出大小不应该为 50 吗？当我将密集2单位保持为1时，这里预测的是哪一个？
我尝试阅读 timeseries_dataset_from_array 的文档，但仍然无法找出问题或解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</guid>
      <pubDate>Fri, 26 Apr 2024 05:17:53 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型准确率达到 0% [关闭]</title>
      <link>https://stackoverflow.com/questions/78388352/geeting-0-accuracy-in-cnn-model</link>
      <description><![CDATA[我正在尝试开发一个用于多类图像分类的顺序 CNN 模型。开发模型后，当我开始训练时，模型在第一个时期给出了准确性，但在第二个时期我没有获得任何准确性。但在第三个时期，我再次获得了准确性。为什么会发生这种情况？还有一个问题，您能否建议我如何提高模型的 f1 分数、精确度和召回率。
https://i.sstatic.net/bwt4nfUr.png
https://i.sstatic.net/Jpn1fW72.png)]]></description>
      <guid>https://stackoverflow.com/questions/78388352/geeting-0-accuracy-in-cnn-model</guid>
      <pubDate>Fri, 26 Apr 2024 05:12:05 GMT</pubDate>
    </item>
    <item>
      <title>推理时出现意外的输出图像</title>
      <link>https://stackoverflow.com/questions/78387970/unexpected-output-image-on-inference</link>
      <description><![CDATA[我正在尝试在颤动中升级图像。但我得到了意想不到的结果。
原始图片
输出图像
LOG：I/flutter ( 592)：图像转换为标准化 Float32List
I/flutter ( 592)：图像标准化成功。
I/flutter ( 592)：输入张量创建成功。
I/颤动（592）：高2880宽1640

原图：高720宽210

这是我尝试过的代码：
 未来;推理（）异步{
    if (selectedImage == null) {
      debugPrint(&#39;未选择图像&#39;);
      返回;
    }

    var 预处理图像 =
    NormalizeImage.imageToNormalizedFloat32List(selectedImage!);

    最终形状 = [1, 3, selectedImage!.height, selectedImage!.width];

    debugPrint(&#39;图像标准化成功。&#39;);

    最终输入Ort =
    OrtValueTensor.createTensorWithDataList(preprocessedImage, shape);

    最终输入 = {&#39;input&#39;: inputOrt};

    debugPrint(&#39;输入张量创建成功。&#39;);

    最终 runOptions = OrtRunOptions();
    最终输出=等待ortSession.runAsync（runOptions，输入）；

    inputOrt.release();
    runOptions.release();


    输出？.forEach((元素) {
      最终输出值=元素？.值；
      if (outputValue 为 List&gt;&gt;) {
        img.Image generatedImage =generateImageFromOutput(outputValue);
        列表 pngBytes = img.encodePng( generatedImage);
        img.Image解码图像 = img.decodeImage(Uint8List.fromList(pngBytes))!;

        显示对话框（
          上下文：上下文，
          构建器：（BuildContext 上下文）{
            返回对话框（
              孩子：大小框（
                宽度：200，
                高度：200，
                子: Image.memory(Uint8List.fromList(img.encodePng(decodedImage))),
              ),
            ）；
          },
        ）；
      } 别的 {
        debugPrint(“输出类型未知”);
      }
      元素？.release();
    });
  }
  img.ImagegenerateImageFromOutput(List&gt;&gt;输出) {
    最终宽度=输出[0][0].长度；
    最终高度=输出[0][0][0].长度；

    debugPrint(“高度$高度宽度$宽度”);

    Float32List float32Data = flattenList(输出);
    var imgData = NormalizeImage.denormalizedFloat32ListToImage(float32Data, 宽度, 高度);
    返回图像数据；
  }

  Float32List flattenList(List&gt;&gt;&gt;nestedList) {
    列表&lt;双&gt;压扁=[]；
    for (nestedList 中的 var fourDimList) {
      for (var ThreeDimList in fourDimList) {
        for (var TwoDimList in ThreeDimList) {
          for (twoDimList 中的 var 值) {
            扁平化。添加（值）；
          }
        }
      }
    }
    返回 Float32List.fromList(展平);
  }

标准化类别
class NormalizeImage {
  静态Float32List imageToNormalizedFloat32List（图像图像）{
    最终 int 高度 = image.height;
    最终 int 宽度 = image.width;

    Float32List float32Image = Float32List(3 * 高度 * 宽度);

    for (int i = 0; i &lt; 高度; i++) {
      for (int j = 0; j &lt; 宽度; j++) {
        最终 int 像素索引 = (i * 宽度 + j) * 3;
        最终 int 像素 = image.getPixel(j, i);

        float32Image[像素索引] = getRed(像素) / 255.0;
        float32Image[像素索引 + 1] = getGreen(像素) / 255.0;
        float32Image[像素索引 + 2] = getBlue(像素) / 255.0;
      }
    }

    print(&quot;图像转换为规范化的 Float32List&quot;);
    返回 float32Image；
  }

  静态图像 denormalizedFloat32ListToImage(Float32List float32Image, int width, int height) {
    最终imgData =图像（宽度，高度）；
    for (int i = 0; i &lt; 高度; i++) {
      for (int j = 0; j &lt; 宽度; j++) {
        最终 int 像素索引 = (i * 宽度 + j) * 3;
        最终 int r = (float32Image[pixelIndex] * 255).toInt().clamp(0, 255);
        最终 int g = (float32Image[pixelIndex + 1] * 255).toInt().clamp(0, 255);
        最终 int b = (float32Image[pixelIndex + 2] * 255).toInt().clamp(0, 255);
        最终 int color = getColor(r, g, b);
        imgData.setPixel(j, i, 颜色);
      }
    }
    返回图像数据；
  }
}
]]></description>
      <guid>https://stackoverflow.com/questions/78387970/unexpected-output-image-on-inference</guid>
      <pubDate>Fri, 26 Apr 2024 02:39:43 GMT</pubDate>
    </item>
    <item>
      <title>如何利用机器学习或深度学习模型将分类准确率提高到70%以上？</title>
      <link>https://stackoverflow.com/questions/78387140/how-to-improve-classification-accuracy-beyond-70-with-machine-learning-or-deep</link>
      <description><![CDATA[我使用 BorutaPy 为我的分类问题选择最佳特征。执行特征选择后，我得到以下结果：
BorutaPy 特征选择：
已确认的特征：79
暂定功能：4
拒绝的功能：2645
我通过使用 StandardScaler() 缩放数据来预处理数据，并且还检查了数据集中每个类的分布。
此外，我还尝试了几种机器学习分类器，并使用交叉验证评估了它们的性能。结果如下：
随机森林：测试集准确率：68.98%，AUC-ROC 得分：0.769
XGBoost：测试集准确率：68.32%，AUC-ROC 得分：0.747
SVM：测试集准确率：63.04%，AUC-ROC 得分：0.713
Logistic回归：测试集准确率：65.68%，AUC-ROC得分：0.717
ExtraTreesClassifier：测试集准确率：70.30%，AUC-ROC 得分：0.752
LGBMClassifier：测试集准确率：69.64%，AUC-ROC 得分：0.777
我的目标是将分类准确率提高到至少 80%。我可以采取哪些步骤来实现这一目标？考虑到 BorutaPy 选择的功能，我是否应该考虑任何特定的技术或策略来提高模型的性能？
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78387140/how-to-improve-classification-accuracy-beyond-70-with-machine-learning-or-deep</guid>
      <pubDate>Thu, 25 Apr 2024 20:35:40 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中红色粒子占据的像素</title>
      <link>https://stackoverflow.com/questions/78387055/identify-pixels-occupied-by-red-particles-in-image</link>
      <description><![CDATA[我有一些塑料颗粒和水波实验的图像。目标是自动识别塑料颗粒。它们有时会重叠，我不需要找到单个粒子，找到那些包含塑料的像素就足够了。
由于粒子是红色的，并且背景大多是白色或黑色，我想我可以进行简单的阈值处理，如果R &gt; &gt;，则说像素是塑料。 5*B 和 R&gt; 0.25，其中 R 和 B 是红色和蓝色通道。然而，不同实验之间的曝光差异很大，有时在实验中，当部分表面被水覆盖时，所以我的方法不能非常一致地工作，有时会错误地识别侧面的黑色裂缝。
我想知道还有什么其他选择。我对神经网络的经验有限，所以我不确定这是否可行（需要付出合理的努力）。特别是，我认为形状不会有太大帮助，因为粒子靠近在一起并且部分重叠，它们之间的对比度很差，但也许颜色就足够了？
示例图像：


]]></description>
      <guid>https://stackoverflow.com/questions/78387055/identify-pixels-occupied-by-red-particles-in-image</guid>
      <pubDate>Thu, 25 Apr 2024 20:14:01 GMT</pubDate>
    </item>
    <item>
      <title>尽管 GPU 可用，但 CUDA 设置失败</title>
      <link>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</link>
      <description><![CDATA[我需要使用bitsandbytes包来运行使用Falcon7B模型的代码。我已经安装了 CUDA，并且我的系统具有 NVIDIA RTX A6000 GPU。我的系统有 Windows 11 操作系统。
这是代码，它只是导入部分：
导入火炬
从数据集导入load_dataset
从变压器导入 AutoModelForCausalLM、AutoTokenizer、BitsAndBytesConfig、TrainingArguments、GenerationConfig
从peft导入LoraConfig，get_peft_model，PeftConfig，PeftModel，prepare_model_for_kbit_training
从 trl 导入 SFTTrainer
进口警告
warnings.filterwarnings(“忽略”)

这是错误：
运行时错误：
        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues



RuntimeError：由于以下错误而无法导入transformers.training_args（查找其回溯）：

        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues

有时不会出现此错误，并且代码可以正常工作。但大多数时候我都会遇到此错误，并且无法找到准确的修复方法。
当系统中未安装 CUDA 时，首次出现此错误。安装后没有报错，但是第二天再次运行时，又出现了同样的错误。
接下来我尝试将 python 版本降级到 3.11.1 以下，之后代码再次运行。但今天我再次面临同样的错误。
这是我的 CUDA 版本：
&lt;前&gt;&lt;代码&gt;nvcc --版本
nvcc：NVIDIA (R) Cuda 编译器驱动程序
版权所有 (c) 2005-2023 NVIDIA 公司
建于 Wed_Feb__8_05:53:42_Cooperative_Universal_Time_2023
Cuda 编译工具，版本 12.1，V12.1.66
构建cuda_12.1.r12.1/compiler.32415258_0
]]></description>
      <guid>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</guid>
      <pubDate>Wed, 24 Apr 2024 07:18:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TimeSeriesSplit 处理面板数据？</title>
      <link>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</link>
      <description><![CDATA[我一直在尝试使用TimeSeriesSplit对于面板数据。我所说的面板数据是指人口的年度照片。我对多年来的数据分割很感兴趣。该人口正在不断变化，每年的人口规模并不相同。因此，直接使用 TimeSeriesSplit 是不可能的。
基本上我试图获得以下简历方案：

我设法使用以下代码来做到这一点：
将 pandas 导入为 pd，将 numpy 导入为 np
将seaborn导入为sns，将matplotlib.pyplot导入为plt

从 sklearn.datasets 导入 make_regression
从 sklearn.dummy 导入 DummyRegressor
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 TimeSeriesSplit

X_测试，y_测试 = []，[]

开始年份 = 2010
年底 = 2020 年

对于 np.arange(start_year, end_year+1) 中的年份：
    X_year, y_year = make_regression(n_samples=5+year-start_year, n_features=2, 偏差=100, 噪声=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:&#39;X1&#39;, 1:&#39;X2&#39;})
    X_year[&#39;年份&#39;] = 年
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)
    
X_test, y_test = pd.concat(X_test), pd.concat(y_test)

# 建模

X = X_测试
y = y_测试
年 = np.unique(X_test[&#39;year&#39;])

# 建模
模型= DummyRegressor（策略=“平均值”）
指标=均方误差
cv = TimeSeriesSplit(n_splits=len(年)-1)

年数=[]
分辨率=[]

对于 i，枚举（cv.split（years））中的（train_year，test_year）：
    
    print(f&quot;折叠 {i}:&quot;)
    print(f&quot;火车：索引={years[train_year]}&quot;)
    print(f&quot;测试：index={years[test_year]}&quot;)
    
    years_folds.append((years[train_year],years[test_year]))
    
    train_filter = X[&#39;year&#39;].isin(years[train_year])
    test_filter = X[&#39;year&#39;].isin(years[test_year])
    
    X_train, y_train = X.loc[train_filter.values], y[train_filter.values]
    X_test, y_test = X.loc[test_filter.values], y[test_filter.values]
    
    model.fit(X_train, y_train)
    分数 = 指标(model.predict(X_test), y_test)
    print(f&#39; {score=:.3}&#39;)
    res.append((年份[test_year][0], 分数))

情节_年份_折叠（年份_折叠）
    
Folds_res = pd.DataFrame(res,columns=[&#39;test_year&#39;, metric.__name__])
Folds_res.plot.scatter(x=&#39;test_year&#39;, y=metric.__name__, title=f&#39;{metric.__name__} over test_year&#39;);

注意：我使用虚拟数据集和虚拟模型是为了提供运行示例。这不是我帖子的主题。
正如您所注意到的，我必须使用一个技巧：我分割年份而不是数据。我想知道：是否有一种标准方法可以使用 sklearn cv 对象分割数据？目标是在 cross_val_score 函数中使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</guid>
      <pubDate>Tue, 23 Apr 2024 07:14:30 GMT</pubDate>
    </item>
    <item>
      <title>低分辨率移动视频中对象检测的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/1771394/what-is-the-best-method-for-object-detection-in-low-resolution-moving-video</link>
      <description><![CDATA[我正在寻找最快、更有效的方法来检测移动视频中的对象。该视频需要注意的事项：颗粒感很强，分辨率低，而且背景和前景同时移动。
注意：我正在尝试在移动视频中检测道路上移动的卡车。
我尝试过的方法：
训练 Haar Cascade - 我尝试通过拍摄所需对象的多个图像来训练分类器来识别对象。事实证明，这会产生许多错误检测或根本没有检测到（从未检测到所需的对象）。我使用了大约 100 张正片和 4000 张负片。
SIFT 和 SURF 关键点 - 当尝试使用这两种基于特征的方法时，我发现我想要检测的对象分辨率太低，因此没有足够的特征来匹配以进行准确的检测。 （从未检测到所需的对象）
模板匹配 - 这可能是我尝试过的最好的方法。这是其中最准确的，但也是最老套的。我可以使用从视频中裁剪的模板来检测特定视频的对象。但是，无法保证准确性，因为已知的只是每个帧的最佳匹配，不会对模板与帧匹配的百分比进行分析。基本上，只有当对象始终在视频中时它才有效，否则会产生错误检测。
这就是我尝试过的 3 种主要方法，但都失败了。最有效的方法是模板匹配，但具有缩放和旋转不变性（这导致我尝试 SIFT/SURF），但我不知道如何修改模板匹配函数。
有人对如何最好地完成这项任务有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/1771394/what-is-the-best-method-for-object-detection-in-low-resolution-moving-video</guid>
      <pubDate>Fri, 20 Nov 2009 15:52:24 GMT</pubDate>
    </item>
    </channel>
</rss>