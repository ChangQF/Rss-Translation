<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 19 Jul 2024 21:13:49 GMT</lastBuildDate>
    <item>
      <title>神经网络训练经过几个阶段后，准确率的提升变得非常缓慢</title>
      <link>https://stackoverflow.com/questions/78770508/accuracy-improving-gets-so-slow-after-some-epoches-in-neural-network-training</link>
      <description><![CDATA[我有大约 7000 万个样本来训练神经网络模型，准确率提高得非常顺利和快速，直到 25-30 个 epoch 左右，25-30 个 epoch 之后就变得非常慢。
例如

epoch 7：损失：5.1151 - 准确率：0.1055
epoch 18：损失：2.9058 - 准确率：0.1516
epoch 26：损失：2.9018 - 准确率：0.2466
epoch 30：损失：2.9091 - 准确率：0.2615
epoch 56：损失：2.7810 - 准确率：0.2732

是不是因为我的学习率，或者模型对于这种训练来说太简单了？
这是我的模型参数：
input_neurons = 65 
output_neurons = 4880
hidden_​​layers = 3
hidden_​​neurons = 256
epochs = 100
batch_size = 8132
learning_rate = 0.001

这是我的模型：
with strategies.scope():
# 模型
model = keras.Sequential([
keras.layers.Input(shape=(input_neurons,)),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(hidden_​​neurons,activation=&#39;relu&#39;),
keras.layers.Dense(output_neurons,activation=&#39;softmax&#39;)])

optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.summary()

我用这个代码训练它：
def data_generator():
for x, y in dataset:
Yield x.numpy(), y.numpy()

steps_per_epoch = 69820098 // batch_size

history = model.fit(
data_generator(),
epochs=epochs,
steps_per_epoch=steps_per_epoch,
verbose=1)
]]></description>
      <guid>https://stackoverflow.com/questions/78770508/accuracy-improving-gets-so-slow-after-some-epoches-in-neural-network-training</guid>
      <pubDate>Fri, 19 Jul 2024 17:05:47 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种机器学习技术来解决我的序列问题？[关闭]</title>
      <link>https://stackoverflow.com/questions/78770366/which-machine-learning-technique-should-i-use-to-solve-my-sequence-problem</link>
      <description><![CDATA[我有一组如下数据：



列 A
列 B
列 C
列D




汤姆
星期一
蓝色
0,3,20,36,80,98,100


丹
星期五
红色
0,15,45,100


莎拉
星期日
R ed
0,6,31,91,100


Dan
星期一
黄色
0,21,86,100


Tom
星期四
红色
0,12,50,70,89,100



（注意D 列中的所有序列都从 0 开始，到 100 结束，序列只能向上，可以是任意长度）
我还有另一组数据，如下所示：



A 列
B 列
C 列
列D




莎拉
星期日
黄色
0,10,15,51,65


莎拉
星期一
蓝色
0,19,34,56


汤姆
星期日
红色
0,8,11,15,22,28,40,60,71


汤姆
星期六
红色
0,1,23,44,89


丹
星期二
绿色
0,27,81



（请注意，D 列中的所有序列都从 0 开始，以 &lt; 结尾100，序列只能向上，可以是任意长度）
我想尝试预测第二个数据集的剩余序列，以便每个序列达到 100。我希望从这个小样本中捕捉到 Dan 的序列可能比 Tom 短，并且在底部两行中 Dan 可能比 Tom 更快达到 100，即使他只有 81 而 Tom 有 89。D 列中的两个相同序列应该根据其他列中的值具有不同的预测。
这只是一个例子，实际数据有更多的列，每个人在每个集合中都有数百/数千行。
哪种技术最适合/适合解决这个问题？我研究了一些序列学习技术，但没有找到任何试图准确解决这个问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78770366/which-machine-learning-technique-should-i-use-to-solve-my-sequence-problem</guid>
      <pubDate>Fri, 19 Jul 2024 16:24:19 GMT</pubDate>
    </item>
    <item>
      <title>我的 RandomForestRegressor 上的 MAE 和 MSE 非常高</title>
      <link>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</link>
      <description><![CDATA[我得到了一个航班预测数据集，我想试试我的机器学习技能。
我清理了数据，修复了一些新功能，删除了其他功能
我还得到了一些有价值的数据。但当我尝试进行预测并评估我的模型时
这就是我得到的答案！那是在我使用 SearchGridCV 调整模型之后
测试集上的回归指标
r2：82.10%
mean_absolute_error：1229.1407307097613
mean_squared_error：2933265.159841384

model = RandomForestRegressor(
max_depth=20,
max_features=&#39;sqrt&#39;,
min_samples_leaf=2,
min_samples_split=5,
n_estimators=200
)

X = df.drop(&#39;Price&#39;,axis=1)
y = df[&#39;Price&#39;]

X_train, X_test, y_train, y_test = train_test_split(
pd.get_dummies(X)
, y, test_size=0.2, random_state=42)

model.fit(X_train,y_train)
y_preds = model.predict(X_test)

我尝试修改超参数并删除一些异常值
def remove_outliers_iqr(df, column):
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
return df[(df[column] &gt;= lower_bound) &amp; (df[column] &lt;= upper_bound)]

numerical_columns = [&#39;Price&#39;, &#39;Dep_hours&#39;, &#39;Dep_min&#39;, &#39;Arrival_hours&#39;, &#39;Arrival_min&#39;, &#39;Duration_hours&#39;, &#39;Duration_min&#39;]
for column in numeric_columns:
df = remove_outliers_iqr(df, column)

但我仍然得到相同的结果
这是完整的笔记本，因为我不知道如何以笔记本的方式在此处附加整个代码
https://github.com/jamhus/ztm-course/blob/master/fligt%20prices%20analysis/flight%20prices.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/78770230/very-high-mae-and-mse-on-my-randomforestregressor</guid>
      <pubDate>Fri, 19 Jul 2024 15:47:43 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 Python 中进程以退出代码 -1073741819（0xC0000005）结束？[关闭]</title>
      <link>https://stackoverflow.com/questions/78770006/how-to-solve-process-finished-with-exit-code-1073741819-0xc0000005-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78770006/how-to-solve-process-finished-with-exit-code-1073741819-0xc0000005-in-python</guid>
      <pubDate>Fri, 19 Jul 2024 14:50:36 GMT</pubDate>
    </item>
    <item>
      <title>如果 sum(y_true)=1，应该使用哪个损失函数？</title>
      <link>https://stackoverflow.com/questions/78769858/which-loss-function-should-be-used-if-sumy-true-1</link>
      <description><![CDATA[我的 yTrue 基本上类似于 [.2,.8]，但从不为 [1,0] 或 [0,1]
sum(yTrue)=1 始终
我尝试了 CategoricalCrossentropy，但发生了 TypeError-

TypeError：预期为 float32，但在 0x7752c1300580 处获得了 &lt;keras.src.losses.losses.CategoricalCrossentropy 对象&gt; 类型为“CategoricalCrossentropy”。
]]></description>
      <guid>https://stackoverflow.com/questions/78769858/which-loss-function-should-be-used-if-sumy-true-1</guid>
      <pubDate>Fri, 19 Jul 2024 14:15:08 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法识别的关键字参数：['batch_shape'] [关闭]</title>
      <link>https://stackoverflow.com/questions/78769397/valueerror-unrecognized-keyword-arguments-batch-shape</link>
      <description><![CDATA[尝试加载我在 kaggle 上训练的模型，保存并下载模型后，我遇到了这个问题，我无法在本地系统上加载模型，但当我尝试在 kaggle 上执行此操作时，它就可以正常工作
帮我解决这个问题，我一直在尝试解决这个问题很长时间，但我做不到]]></description>
      <guid>https://stackoverflow.com/questions/78769397/valueerror-unrecognized-keyword-arguments-batch-shape</guid>
      <pubDate>Fri, 19 Jul 2024 12:31:46 GMT</pubDate>
    </item>
    <item>
      <title>在 Airflow dag 中加载 Joblib 模型</title>
      <link>https://stackoverflow.com/questions/78769379/loading-joblib-model-in-airflow-dag</link>
      <description><![CDATA[我尝试在 airflow dag 中使用 joblib，加载模型文件时，我运行 joblib.load(model_name)，但出现错误
 文件 &quot;/usr/local/lib/python3.9/pickle.py&quot;，第 331 行，在 _getattribute 中
raise AttributeError(&quot;无法在 {!r} 上获取属性 {!r}&quot;
AttributeError: 无法从 &#39;/home/***/.local/bin/***&#39;&gt; 获取 &lt;module &#39;__main__&#39; 上的属性 &#39;MyCustomModel&#39;

我的自定义模型已导入 dag 中，我也尝试在任务函数中导入，但我总是收到此错误，即 joblib 库找不到我需要解开模型的自定义类。我认为这是因为 airflow 运行时没有处理导入或 python 命名空间，因为我期望。其他人是否尝试过在 python dag 中加载 pickles 或 joblib 模型？或者是否有某种方法可以强制运行时导入此自定义类？]]></description>
      <guid>https://stackoverflow.com/questions/78769379/loading-joblib-model-in-airflow-dag</guid>
      <pubDate>Fri, 19 Jul 2024 12:26:48 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器，网格搜索</title>
      <link>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78768511/xgboost-classifier-grid-search</guid>
      <pubDate>Fri, 19 Jul 2024 09:07:04 GMT</pubDate>
    </item>
    <item>
      <title>寻求将不规则 Excel 布局自动转换为结构化数据集的技术</title>
      <link>https://stackoverflow.com/questions/78768436/seeking-techniques-to-automate-transformation-of-irregular-excel-layouts-to-stru</link>
      <description><![CDATA[我面临的挑战是将不规则的 Excel 布局自动转换为结构化数据集。这些 Excel 文件通常包含：
合并单元格
分层列
注释和说明……如下所示：在此处输入图像描述
目标：
我想将这些复杂的 Excel 布局转换为计算机可以理解的结构化数据集，从而实现无缝的数据可视化和解释。
可以使用哪些技术或工具来自动化此转换过程？是否有任何机器学习模型、数据预处理技术或软件工具可以帮助标准化和结构化这些不同的 Excel 文件？]]></description>
      <guid>https://stackoverflow.com/questions/78768436/seeking-techniques-to-automate-transformation-of-irregular-excel-layouts-to-stru</guid>
      <pubDate>Fri, 19 Jul 2024 08:53:47 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 sklearn 序数编码器完成的映射？</title>
      <link>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</link>
      <description><![CDATA[我使用 sklearn 对数据集的两列进行了序数编码
我想知道哪一列映射到哪一列
假设 0 映射到两列的什么位置
我想知道语法，尝试询问 gpt 但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78768207/how-to-know-the-mappings-done-by-sklearn-ordinal-encoder</guid>
      <pubDate>Fri, 19 Jul 2024 08:02:11 GMT</pubDate>
    </item>
    <item>
      <title>如何根据物体之间的间距分割图像？</title>
      <link>https://stackoverflow.com/questions/78767970/how-to-segment-an-image-based-on-the-spacing-between-objects</link>
      <description><![CDATA[我是物体检测的初学者，目前正在做一个项目，对工程设计图进行分割，并将这些分割部分导出为 PNG 文件。
对于这幅图，我需要在三个物体周围创建三个边界框，并裁剪出这些部分。有没有算法或模型可以自动完成这个过程？
]]></description>
      <guid>https://stackoverflow.com/questions/78767970/how-to-segment-an-image-based-on-the-spacing-between-objects</guid>
      <pubDate>Fri, 19 Jul 2024 07:04:09 GMT</pubDate>
    </item>
    <item>
      <title>KLDivLoss 的输入是什么</title>
      <link>https://stackoverflow.com/questions/78753296/what-input-for-kldivloss</link>
      <description><![CDATA[我有一个 CNN 架构，希望使用 Kullback-Leibler 损失（来自 pytorch 的 KLDivLoss）来比较输出张量和目标张量（灰度图像）。
我有点困惑，不知道输入到损失函数的图像应该是什么格式。我理解它不应该直接是像素值，而应该是一个概率分布。
这里有一些我犹豫要不要使用的可能性，但我不确定它们是否正确：

保持图像尺寸并用其概率替换像素值（用 count(pixel_value)/total_number_of_pixels 替换每个像素值）
只需应用 softmax（但最高概率与更高的像素值相关联，这在我的例子中并不十分相关）
用大小为 256 的向量作为损失函数，其每个元素都是图像中相应像素值的概率（count(pixel_value)/total_number_of_pixels）

我的图像在 0 和 1 之间标准化。]]></description>
      <guid>https://stackoverflow.com/questions/78753296/what-input-for-kldivloss</guid>
      <pubDate>Tue, 16 Jul 2024 07:54:06 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False), Dense(64,activation=&#39;relu&#39;), Dense(1) ]) # 模型编译器 model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差 # 模型摘要 model.summary() # 模型模型嵌入 model.fit( X_train, Y_train, epochs=40, batch_size=32,validation_data=(X_test, Y_test)) # 新预测示例预测= model.predict(X_test) print(prediction)  结果： 模型：“sequential”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 数的增加而改善。这就是问题所在，我不明白发生了什么。
我反复检查了我的数据，X_train 已标准化，我尝试在模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。我真的想尝试找到一种方法来使用深度学习来找到它。]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    <item>
      <title>无法加载可教机器模型</title>
      <link>https://stackoverflow.com/questions/78237621/unable-to-load-the-teachable-machine-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78237621/unable-to-load-the-teachable-machine-model</guid>
      <pubDate>Thu, 28 Mar 2024 10:51:19 GMT</pubDate>
    </item>
    <item>
      <title>如何解决在训练自己的 DDSP-VST 模型的官方示例中 Google 协作的（依赖）错误？</title>
      <link>https://stackoverflow.com/questions/77216743/how-to-solve-dependency-error-on-google-collab-in-official-example-for-trainin</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77216743/how-to-solve-dependency-error-on-google-collab-in-official-example-for-trainin</guid>
      <pubDate>Mon, 02 Oct 2023 15:30:02 GMT</pubDate>
    </item>
    </channel>
</rss>