<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 11 Jan 2025 12:30:29 GMT</lastBuildDate>
    <item>
      <title>MedSAM算法中的损失函数</title>
      <link>https://stackoverflow.com/questions/79347947/loss-function-in-medsam-algorithm</link>
      <description><![CDATA[我正在研究 MedSAM（医学图像中的任意分割）算法。它是 Meta AI 针对医学图像的任意分割模型的微调版本。在论文中，其损失函数由二元交叉熵 (BCE) 损失和 Dice 损失的未加权和给出。

我想手动计算这个损失。
假设我有 $8 \times 8$ 灰度图像，下面显示了该图像的矩阵和我想要分割为阴影灰色的区域。这是我的真实情况。

N 是像素数，在本例中为 64。下面是此特定图像的分割结果：

因此，我可以使用以下代码从此处计算 BCE 损失和 Dice 损失：
import numpy as np

# 真实值（来自原始 8x8 灰度图像）
ground_truth = np.array([
[0.96, 0.16, 0.77, 0.00, 0.49, 0.25, 0.87, 0.31],
[0.18, 0.67, 0.44, 0.17, 0.12, 0.93, 0.30, 0.39],
    [0.25, 0.62, 0.57, 0.76, 1.00, 0.03, 0.58, 0.80],
    [0.46, 0.21, 0.45, 0.83, 0.34, 0.39, 0.66, 0.42],
    [0.91, 0.16, 0.10, 0.56, 0.78, 0.71, 0.70, 0.91],
    [0.40, 0.88, 0.35, 0.72, 0.87, 0.27, 0.59, 0.27],
    [0.70, 0.97, 0.89, 0.39, 0.48, 0.94, 0.84, 0.07],
[0.72, 0.51, 0.02, 0.16, 0.96, 0.70, 0.14, 0.23],
])

# 分割结果（来自 MedSAM 结果）
segmentation_result = np.array([
[0.79, 0.78, 0.38, 0.61, 0.63, 0.25, 0.40, 0.60],
[0.70, 0.57, 0.23, 0.63, 0.88, 0.03, 0.94, 0.01],
[0.32, 0.99, 0.39, 0.66, 0.58, 0.22, 0.03, 0.65],
    [0.86, 0.96, 0.38, 0.40, 0.90, 0.34, 0.70, 0.91],
    [0.56, 0.18, 0.08, 0.12, 0.14, 0.68, 0.36, 0.41],
    [0.31, 0.48, 0.68, 0.89, 0.92, 0.93, 0.53, 0.50],
    [0.87, 0.62, 0.30, 0.95, 0.71, 0.65, 0.75, 0.65],
    [0.91, 0.55, 0.24, 0.84, 0.91, 0.90, 0.72, 0.20],
])

# 二元交叉熵损失
epsilon = 1e-7 # 防止 log(0)
bce_loss = -np.mean(
ground_truth * np.log(segmentation_result + epsilon) +
(1 - ground_truth) * np.log(1 - fragmentation_result + epsilon)
)

# 骰子损失
numerator = 2 * np.sum(ground_truth * fragmentation_result)
denominator = np.sum(ground_truth**2) + np.sum(segmentation_result**2)
dice_loss = 1 - (numerator / (denominator + epsilon))

bce_loss, dice_loss

计算出的损失为：

二元交叉熵 (BCE) 损失：0.9590
Dice 损失：0.2083

据我所知，BCE 确保准确预测每个体素，而 Dice 确保分割区域的整体形状和重叠正确。这种组合利用了两种指标的优势，使算法在不同的医学图像分割任务中具有鲁棒性。
我的第一个问题是我的计算是否正确？
其次，我不明白这里如何以及为什么包含 BCE。由于这不是分类模型，而只是一个分割模型，Dice 损失是否足够？
我的第三个问题是，如果图像包含同一类的另一个分割区域怎么办？或者如果它包括另一个属于不同类别的分割会怎样？BCE 损失将仅对两类分类起作用（（1）肿瘤或（2）健康组织）。]]></description>
      <guid>https://stackoverflow.com/questions/79347947/loss-function-in-medsam-algorithm</guid>
      <pubDate>Sat, 11 Jan 2025 11:14:59 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个机器学习程序可以将 3D 模型纹理与正确的模型匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/79347239/is-there-a-machine-learning-program-that-can-match-3d-model-textures-to-the-corr</link>
      <description><![CDATA[我有大量（约 20,000 张）图像，这些图像是纹理、法线贴图和其他用于纹理 3D 模型对象的贴图。我还有 3D 模型本身（OBJ 格式，但可以转换）。
我想使用模型 UV 坐标数据将纹理文件映射到其对应的 3D 模型。
问题是，所有图像都未分类且未标记，因此将它们与各自的 3D 模型匹配需要手动浏览它们。这非常繁琐。
是否存在这样的程序？我还没有找到。
如果没有，如果我尝试自己构建它，我应该记住哪些注意事项？我有编程经验，但没有机器学习经验。
想法：

先将图像映射在一起，然后将模型映射到该组图像可能会更容易
UV 坐标数据可以导出为图像。如果这很困难，那么使用它而不是从 3D 模型文件本身进行映射是有意义的。
]]></description>
      <guid>https://stackoverflow.com/questions/79347239/is-there-a-machine-learning-program-that-can-match-3d-model-textures-to-the-corr</guid>
      <pubDate>Fri, 10 Jan 2025 23:20:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn 中的 LinearRegression 使用普通最小二乘法而不是正态方程？</title>
      <link>https://stackoverflow.com/questions/79346968/why-linearregression-in-scikit-learn-uses-ordinary-least-squares-instead-of-norm</link>
      <description><![CDATA[我正在阅读《使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习实践》这本书。第 4 章包含一些有关正态方程的信息，该方程是闭式解，意味着它直接给出结果。但在正态方程的解释结束时，作者提到，在 Scikit-Learn 中，LinearRegression 类使用普通最小二乘法，因为它更为优化。有人可以解释一下普通最小二乘法是否也是闭式解，以及为什么它比正态方程更为优化吗？
作者提到它更有效，因为普通最小二乘法计算矩阵的伪逆并执行一些其他步骤使其变得更好，但我仍然无法理解它背后的直觉。]]></description>
      <guid>https://stackoverflow.com/questions/79346968/why-linearregression-in-scikit-learn-uses-ordinary-least-squares-instead-of-norm</guid>
      <pubDate>Fri, 10 Jan 2025 20:40:55 GMT</pubDate>
    </item>
    <item>
      <title>1000- torchrl：使用 SyncDataCollector 和自定义 pytorch dqn</title>
      <link>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</link>
      <description><![CDATA[我尝试将 torchrl 的 SyncDataCollector 与我自己在 torch 中实现的 DQN 一起使用。由于 DQN 使用 Conv2d 和线性层，我必须计算第一个线性层的输入的正确大小，即以下网络中的 size 参数
class PixelDQN(nn.Module):
def __init__(self, input_shape, n_actions) -&gt;无：
super().__init__()
self.conv = nn.Sequential(
nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 64, kernel_size=3, stride=1),
nn.ReLU(),
nn.Flatten(),
)
size = self.conv(torch.zeros(1, *input_shape)).size()[-1]
self.fc_adv = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, n_actions),
)
self.fc_val = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, 1)
)

def forward(self, x: torch.Tensor):
print(x.shape)
conv = self.conv(x)
print(conv.shape)
adv = self.fc_adv(conv)
val = self.fc_val(conv)
outp = val + (adv - adv.mean(dim=1, keepdim=True))
return outp

负责这个。如您所见，我期望批量输入，因为我将使用重放缓冲区并从中抽样一批。
我以以下方式包装该 DQN，然后使用 SyncDataCollector：
n_obs = [4,84,84]
n_act = 6

agent = QValueActor(
module=PixelDQN(n_obs, n_act), in_keys=[&quot;pixels&quot;], spec=env.action_spec
)
policy_explore = EGreedyModule(
env.action_spec, eps_end=EPS_END, annealing_num_steps=ANNEALING_STEPS
)
agent_explore = TensorDictSequential(
agent, policy_explore
)

collector = SyncDataCollector(
env,
agent_explore,
frames_per_batch=FRAMES_PER_BATCH,
init_random_frames=INIT_RND_STEPS,
postproc=MultiStep(gamma=GAMMA, n_steps=N_STEPS)
)

但是，这失败了，因为 SyncDataCollector 在将 obs 提供给 DQN 之前没有对来自环境的 obs 进行批处理，因此 size 计算出错，并且线性层获得错误的输入维度。
RuntimeError：mat1 和 mat2 形状无法相乘（64x49 和 3136x256）
我已经尝试在 SyncDataCollector 中设置 buffer=True。我也尝试使用
agent_explore = TensorDictSequential(
UnsqueezeTransform(0, allow_positive_dim=True), agent, policy_explore
)

因为这是 ChatGPT 建议的，但似乎没有任何效果。
我也在我的 env 创建中尝试了 UnsqueezeTransform，但这也没有用，我的 env 如下所示：
def make_env(env_name: str):
return TransformedEnv(
GymEnv(env_name, from_pixels=True),
Compose(
RewardSum(),
EndOfLifeTransform(),
NoopResetEnv(noops=30),
ToTensorImage(),
Resize(84, 84),
GrayScale(),
FrameSkipTransform(frame_skip=4),
CatFrames(N=4, dim=-3),
)
)

我可以将 size 计算拉入 PixelDQN 的前向传递中，并检查输入张量的大小以调整计算，但这似乎是一件很奇怪的事情，因为这意味着我需要在每次前向传递时运行大小计算。]]></description>
      <guid>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</guid>
      <pubDate>Fri, 10 Jan 2025 09:49:05 GMT</pubDate>
    </item>
    <item>
      <title>在 ubuntu 22.04 中安装 x13as arima seat 时出错</title>
      <link>https://stackoverflow.com/questions/79345160/error-while-installing-x13as-arima-seat-in-ubuntu-22-04</link>
      <description><![CDATA[我正在做时间序列项目，为了找到 p、d 和 q 值，我试图安装 x13as arima 座位，但我无法正确安装它，然后我使用“sudo apt install x13as”进行安装。然后我设置了一个路径，但它仍然给我一个错误路径设置不正确，我该怎么办？
sudo apt install x13as

我试过从文档中手动下载，还是有其他我可以找到的东西？]]></description>
      <guid>https://stackoverflow.com/questions/79345160/error-while-installing-x13as-arima-seat-in-ubuntu-22-04</guid>
      <pubDate>Fri, 10 Jan 2025 09:13:45 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的批次大小和时期？[关闭]</title>
      <link>https://stackoverflow.com/questions/79345103/batch-size-and-epochs-in-machine-learning</link>
      <description><![CDATA[我正在使用 TensorFlow 从股票图表中进行图像/模式识别，我创建了一个包含约 20,000 张图像的目录，其中包含价格大幅上涨或下跌之前的模式示例。我应该使用什么批次大小和多少个时期，以 80/20 的训练/验证比例进行分割？
我还打算将目录大小增加到约 100,000 张图像，那么我应该使用什么？]]></description>
      <guid>https://stackoverflow.com/questions/79345103/batch-size-and-epochs-in-machine-learning</guid>
      <pubDate>Fri, 10 Jan 2025 08:55:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Anthropic Vision API 获取精确的图标坐标</title>
      <link>https://stackoverflow.com/questions/79345033/getting-accurate-icon-coordinates-using-anthropic-vision-api</link>
      <description><![CDATA[我正在使用 Anthropic Vision API 从桌面图像中提取特定图标、文件夹或元素的坐标，但遇到了障碍。以下是我正在做的事情：
我有一个桌面图像（原始尺寸：1920x1080），其中包含多个图标和文件夹。
我正在要求 Vision 模型（通过 Sonnet 3.5 模型）查找特定图标（如“Chrome”）的坐标。
根据 Anthropic Vision 文档，我按照指示将图像的大小调整为 1366x768，然后将其发送到 API。
尽管遵循了指南，但 API 返回的坐标与原始图像中图标的实际位置不匹配。
有趣的是，计算机使用模型在其环境中工作正常，但在这种情况下，我只想将图像发送到 Vision 模型并获取特定元素的精确坐标。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79345033/getting-accurate-icon-coordinates-using-anthropic-vision-api</guid>
      <pubDate>Fri, 10 Jan 2025 08:28:24 GMT</pubDate>
    </item>
    <item>
      <title>使用“bitsandbytes”4 位量化需要最新版本的 bitsandbytes：“pip install -U bitsandbytes”</title>
      <link>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</link>
      <description><![CDATA[加载 tokenizer 时，我收到此错误：
ImportError：使用 bitsandbytes 4 位量化需要最新版本的 bitsandbytes：
pip install -U bitsandbytes。

我在 Macbook M2 pro 上使用 Jupyter 笔记本。
以下是源代码：
quant_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_type=&quot;nf4&quot;

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

base_model = AutoModelForCausalLM.from_pretrained(
BASE_MODEL,
quantization_config=quant_config,
device_map=&quot;auto&quot;,
)

base_model.generation_config.pad_token_id = tokenizer.pad_token_id

有人能帮忙吗？
我按照说明更新了 bitsandbytes，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</guid>
      <pubDate>Fri, 10 Jan 2025 03:52:41 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm.cv：cvbooster.best_iteration 总是返回 -1</title>
      <link>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</link>
      <description><![CDATA[我正在从 XGBoost 迁移到 LightGBM（因为我需要它精确处理交互约束），并且我很难理解 LightGBM CV 的结果。在下面的示例中，在第 125 次迭代中实现了最小对数损失，但 model[&#39;cvbooster&#39;].best_iteration 返回 -1。我原本希望它也能返回 125 - 还是我在这里误解了什么？有没有更好的方法来获得最佳迭代，还是只需要手动检查？
我看过这个讨论，但即使我检查cvbooster中的boosters（例如，model[&#39;cvbooster&#39;].boosters[0].best_iteration），它们也都返回 -1...
import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(n_samples=10_000, n_features=5, n_informative=3, random_state=9)

data_train_lgb = lgb.Dataset(X, label=y)

param = {&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: [&#39;binary_logloss&#39;],
&#39;device_type&#39;: &#39;cuda&#39;}

model = lgb.cv(param,
data_train_lgb,
num_boost_round=1_000,
return_cvbooster=True)

opt_1 = np.argmin(model[&#39;valid binary_logloss-mean&#39;])
print(f&quot;index argmin: {opt_1}&quot;)
print(f&quot;logloss argmin: {model[&#39;valid binary_logloss-mean&#39;][opt_1]}&quot;)

opt_2 = model[&#39;cvbooster&#39;].best_iteration
print(f&quot;index best_iteration: {opt_2}&quot;)
print(f&quot;logloss best_iteration: {model[&#39;valid binary_logloss-mean&#39;][opt_2]}&quot;)

---

&gt;&gt;&gt; 索引参数最小值：125
&gt;&gt;&gt; 对数损失参数最小值：0.13245999867688793

&gt;&gt;&gt; 索引最佳迭代：-1
&gt;&gt;&gt; 对数损失最佳迭代：0.2661896445658779
]]></description>
      <guid>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</guid>
      <pubDate>Fri, 10 Jan 2025 03:40:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在平面图上检测北箭头？</title>
      <link>https://stackoverflow.com/questions/79344396/how-to-detect-north-arrow-on-a-floor-plan</link>
      <description><![CDATA[我正在从事一项任务，该任务涉及使用多模态 AI 模型（例如 Google Gemini）分析楼层平面图以提取结构化信息，例如入口点、卧室和其他主要特征的位置。
但是，检测楼层平面图中的方向性存在挑战。
这是一个示例楼层平面图图像：

在右下角，有一个方向指示器，显示北 (N)，箭头指向上方。作为人类，我们很容易理解北方指向上方，我们可以相应地调整对楼层平面图的解释。然而，当使用AI模型处理图像时，模型无法读取“N”标签或理解箭头方向，从而导致方向分析不正确。
为了解决这个问题，我尝试使用PaddleOCR检测图像中的文本并对其进行注释。我使用的代码如下
from paddleocr import PaddleOCR, draw_ocr
from PIL import Image
# 初始化 PaddleOCR
ocr = PaddleOCR(use_angle_cls=True, lang=&#39;en&#39;) # 下载并加载模型一次

# 提供图片路径
img_path = &#39;prop_1.png&#39;

# 执行 OCR
result = ocr.ocr(img_path, cls=True)

# 打印结果
for idx in range(len(result)):
res = result[idx]
for line in res:
print(line)


OCR 输出成功检测到“卧室”、“厨房”、“客厅”等文本标签，但未能检测到“N”标签和指向上方的箭头指示方向。]]></description>
      <guid>https://stackoverflow.com/questions/79344396/how-to-detect-north-arrow-on-a-floor-plan</guid>
      <pubDate>Fri, 10 Jan 2025 01:12:24 GMT</pubDate>
    </item>
    <item>
      <title>CycleGAN 的鉴别器损失停留在 0.0</title>
      <link>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</link>
      <description><![CDATA[我目前正在训练 BD-Cycle GAN，这是 Mol-Cycle GAN 的修改版本。我没有修改任何代码，但需要从 Mol-Cycle GAN 存储库下载 utils 文件夹和 environment.yml。当使用默认参数运行 train.py 文件时，鉴别器 A 和 B 的损失都停留在 0.0，但生成器损失似乎正常。
我不知道问题的原因是什么，因为我使用的是作者提供的模型官方存储库，没有修改任何代码或超参数，但得到了这个结果。鉴别器的损失在每个时期都保持在 0.0，从而扰乱整个训练过程。]]></description>
      <guid>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</guid>
      <pubDate>Thu, 09 Jan 2025 23:15:36 GMT</pubDate>
    </item>
    <item>
      <title>阿曼车牌的 OCR 预处理 - 字母识别问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</link>
      <description><![CDATA[我正在开发一个用于阿曼车牌的 OCR 系统，并努力提高字母识别的准确性。车牌上通常包含小而粗的字符，而我目前的预处理流程无法产生令人满意的结果。
到目前为止，我所做的是：
尽管进行了预处理和配置（--oem 3，--psm 6），但 PaddleOCR 和 Tesseract 仍然难以识别字母。
预处理步骤：
尝试使用 Sauvola 和 Wolf-Jolion 二值化、缩放图像（1.5 倍）并应用膨胀来增强文本。
问题：
字母仍然难以识别。
如何改进预处理以更好地对小而粗的字母进行 OCR 识别？
是否有任何 OCR 模型或自定义训练方法更适合像阿曼这样设计复杂的车牌？
样本车牌：
]]></description>
      <guid>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</guid>
      <pubDate>Thu, 09 Jan 2025 18:15:26 GMT</pubDate>
    </item>
    <item>
      <title>如何从 fit_resamples 和超参数调整中获取训练误差？</title>
      <link>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</link>
      <description><![CDATA[在交叉验证期间，fit_resamples 返回验证集中度量的平均值。
lr_model &lt;-
linear_reg() |&gt;
set_engine(&#39;lm&#39;)

lr_wf &lt;-
working() |&gt;
add_recipe(basic_recipe) |&gt;
add_model(lr_model)

lr_cv &lt;-
lr_wf |&gt;
fit_resamples(
folds,
metrics = metric_set(rmse),
control = control
)

# 让我们从 CV 中提取结果。这将有助于我们将其与其他模型进行比较
lr_cv |&gt;
collect_metrics()
# 这是 RMSE 验证错误
# .metric .estimator mean n std_err .config
# &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 
# rmse standard 0.161 10 0.000370 Preprocessor1_Model1

我遇到的问题是如何获取训练误差。
在调整超参数后也会出现同样的问题。
例如，在调整 KNN 以找到最佳邻居数时，collect_metrics 和 show_best 会返回来自交叉验证的验​​证集指标的平均值，而我们都知道，最佳邻居数是当训练误差减少而验证误差开始增加时。
不幸的是，autoplot 函数不显示训练误差，只显示验证误差。
在这种情况下，例如
tree_grid &lt;-
grid_regular(
cost_complexity(),
tree_depth(),
min_n(),
levels = c(3, 5, 10)
)

tree_wf &lt;-
working() %&gt;%
add_model(tree_model) %&gt;%
add_recipe(basic_recipe)

tree_res &lt;- 
tree_wf %&gt;%
tune_grid(
resamples = folds,
grid = tree_grid,
metrics = metric_set(rmse),
control = control
)

如何提取每对超参数/折叠的训练误差？]]></description>
      <guid>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</guid>
      <pubDate>Wed, 08 Jan 2025 08:23:55 GMT</pubDate>
    </item>
    <item>
      <title>为何我无法包装 LGBM？</title>
      <link>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</link>
      <description><![CDATA[我使用 LGBM 预测数值量的相对变化。我使用 MSLE（均方对数误差）损失函数来优化我的模型并获得正确的误差缩放比例。由于 MSLE 不是 LGBM 的原生功能，因此我必须自己实现它。但幸运的是，数学可以大大简化。这是我的实现；
class MSLELGBM(LGBMRegressor):
def __init__(self, **kwargs): 
super().__init__(**kwargs)

def predict(self, X):
return np.exp(super().predict(X))

def fit(self, X, y, eval_set=None, callbacks=None):
y_log = np.log(y.copy())
print(super().get_params()) # 这不会打印任何 kwargs
if eval_set:
eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

如您所见，它非常简单。我基本上只需要对模型目标应用对数变换，并对预测取指数以返回我们自己的非对数世界。
但是，我的包装器不起作用。我使用以下命令调用该类；
model = MSLELGBM(**lgbm_params)
model.fit(data[X_cols_all], data[y_col_train]) 

我收到以下异常；

-----------------------------------------------------------------------------------------
KeyError Traceback (most recent call last)
Cell In[31], line 38
32 callbacks = [
33 lgbm.early_stopping(10, verbose=0), 
34 lgbm.log_evaluation(period=0),
35 ]
37 model = MSLELGBM(**lgbm_params)
---&gt; 38 model.fit(data[X_cols_all], data[y_col_train]) 
40 feature_importances_df = pd.DataFrame([model.booster_.feature_importance(importance_type=&#39;gain&#39;)], columns=X_cols_all).T.sort_values(by=0, accending=False)
41 feature_importances_df.iloc[:30]

单元格 In[31]，第 17 行
15 if eval_set:
16 eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
---&gt; 17 super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

文件 c:\X\.venv\lib\site-packages\lightgbm\sklearn.py:1189，在 LGBMRegressor.fit(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)
1172 def fit( # type: ignore[override]
1173 self,
1174 X: _LGBM_ScikitMatrixLike,
(...)
1186 init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None,
1187 ) -&gt; &quot;LGBMRegressor&quot;:
1188 &quot;&quot;&quot;Docstring 继承自 LGBMModel。&quot;&quot;&quot;
...
--&gt; 765 if isinstance(params[&quot;random_state&quot;], np.random.RandomState):
766 params[&quot;random_state&quot;] = params[&quot;random_state&quot;].randint(np.iinfo(np.int32).max)
767 elif isinstance(params[&quot;random_state&quot;], np.random.Generator):

KeyError: &#39;random_state&#39;

我不知道 random_state 为何从 fit 方法中缺失，因为该函数甚至不需要它。我感觉这是一个复杂的软件工程问题，超出了我的理解范围。有人知道发生了什么吗？
如果有帮助的话，我尝试使用更简单的非 lgbm 结构来说明我想要的内容；

我只想将我提供给 MSLELGBM 的任何参数传递给原始 LGBM，但这样做时我遇到了很多问题。]]></description>
      <guid>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</guid>
      <pubDate>Tue, 31 Dec 2024 15:25:17 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM API与Sklearn API之间的训练区别</title>
      <link>https://stackoverflow.com/questions/75649038/training-difference-between-lightgbm-api-and-sklearn-api</link>
      <description><![CDATA[我正在尝试训练 LGBClassifier 以完成多类任务。我首先尝试直接使用 LightGBM API，并按如下方式设置模型和训练：
LightGBM API
train_data = lgb.Dataset(X_train, (y_train-1))
test_data = lgb.Dataset(X_test, (y_test-1))
params = {}
params[&#39;learning_rate&#39;] = 0.3
params[&#39;boosting_type&#39;] = &#39;gbdt&#39;
params[&#39;objective&#39;] = &#39;multiclass&#39;
params[&#39;metric&#39;] = &#39;softmax&#39;
params[&#39;max_depth&#39;] = 10
params[&#39;num_class&#39;] = 8
params[&#39;num_leaves&#39;] = 500

lgb_train = lgb.train(params, train_data, 200)

# 训练后模型

y_pred = lgb_train.predict(X_test)
y_pred_class = [np.argmax(line) for line in y_pred]
y_pred_class = np.asarray(y_pred_class) + 1

混淆矩阵如下所示：

Sklearn API
然后我尝试转到 Sklearn API 以便能够使用其他工具。这是我使用的代码：
lgb_clf = LGBMClassifier(objective=&#39;multiclass&#39;,
boosting_type=&#39;gbdt&#39;,
max_depth=10,
num_leaves=500,
learning_rate=0.3,
eval_metric=[&#39;accuracy&#39;,&#39;softmax&#39;],
num_class=8,
n_jobs=-1,
early_stopping_rounds=100,
num_iterations=500)

clf_train = lgb_clf(X_train, (y_train-1), verbose=1, eval_set=[(X_train, (y_train-1)), (X_test, (y_test-1)))])

# 训练：我可以看到过度拟合正在发生

y_pred = clf_train.predict(X_test)
y_pred = [np.argmax(line) for line in y_pred]
y_pred = np.asarray(y_pred) + 1

这是本例中的混淆矩阵：

备注

我需要从 y_train 中减去 1，因为我的类从 1 开始，LightGBM 对此有抱怨。
当我尝试 RandomSearch 或 GridSearch 时，我总是得到与上一个混淆​​矩阵相同的结果。
我在这里检查了不同的问题，但没有一个能解决这个问题问题。

问题

在 Sklearn API 中实现模型时，我遗漏了什么吗？
为什么我使用 LightGBM API 获得了良好的结果（可能过度拟合）？
如何使用这两个 API 获得相同的结果？

提前致谢。
更新这是我的错误。我以为两个 API 中的输出会相同，但似乎并非如此。我刚刚在使用 Sklearn API 进行预测时删除了 np.argmax() 行。看来这个 API 已经直接预测了类。不要删除问题，以防其他人遇到类似问题。]]></description>
      <guid>https://stackoverflow.com/questions/75649038/training-difference-between-lightgbm-api-and-sklearn-api</guid>
      <pubDate>Mon, 06 Mar 2023 09:24:30 GMT</pubDate>
    </item>
    </channel>
</rss>