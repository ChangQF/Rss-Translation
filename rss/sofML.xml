<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 12 Dec 2024 21:17:16 GMT</lastBuildDate>
    <item>
      <title>在 Jupyter Notebook 上获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79276186/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types-on-jupyter</link>
      <description><![CDATA[我正在 Jupyter Notebook 上做一个机器学习项目来预测电动汽车的价格。
我运行这个单元：
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 return _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79276186/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types-on-jupyter</guid>
      <pubDate>Thu, 12 Dec 2024 18:23:13 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到使用 NLP 进行医疗文档分析和疾病诊断的数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/79276072/where-can-i-find-datasets-for-medical-document-analysis-and-disease-diagnosis-us</link>
      <description><![CDATA[我正在从事一个与医疗保健相关的项目，我需要分析医疗文件，提取特定值（例如肌酐、血糖水平等），并为患者生成个性化段落，描述他们的病情和推荐药物。但是，我很难找到适合这项任务的数据集。
理想的数据集应包括：
医学测试结果或实验室报告。
基于提取值的疾病诊断。
建议的药物或治疗计划。

任何建议或资源都将不胜感激！
我搜索过 Kaggle、NIH 和 SEER 等平台，但没有找到符合我需求的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79276072/where-can-i-find-datasets-for-medical-document-analysis-and-disease-diagnosis-us</guid>
      <pubDate>Thu, 12 Dec 2024 17:39:30 GMT</pubDate>
    </item>
    <item>
      <title>本地 CUDA 集群的 dask_cuda 问题</title>
      <link>https://stackoverflow.com/questions/79275934/dask-cuda-problem-with-local-cuda-cluster</link>
      <description><![CDATA[我试图让此代码运行，然后使用它在两个 gpu 上训练各种模型：
from dask_cuda import LocalCUDACluster
from dask.distributed import Client

if __name__ == &quot;__main__&quot;:
with LocalCUDACluster(n_workers=2) as cluster:
with Client(cluster) as client:
...

错误以以下开头：
-distributed.worker - ERROR - Worker plugin CPUAffinity-bd2c98dd-df91-4dc5-8dd9-cca207e4c3fc 无法设置
我在大学提供的服务器上，它一直给我一个关于 cpu 亲和性设置的错误。如果我只设置一个工作器，它就可以正常工作。即使我使用 LocalCluster，但使用它也会遇到一个问题，即一个 gpu 上有两个工作器。]]></description>
      <guid>https://stackoverflow.com/questions/79275934/dask-cuda-problem-with-local-cuda-cluster</guid>
      <pubDate>Thu, 12 Dec 2024 16:50:28 GMT</pubDate>
    </item>
    <item>
      <title>鸢尾花数据集上的规范化与最小最大缩放[关闭]</title>
      <link>https://stackoverflow.com/questions/79275753/normalization-vs-minmax-scaling-on-iris-dataset</link>
      <description><![CDATA[我正在对 Iris 数据集进行一些实验。
我面临 MinMaxScaler 和 minimize 之间的不同行为。
尽管我知道我不应该对数据进行规范化或标准化，但我还是尝试了（出于测试目的）。
通过使用 MinMaxScaler：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
rf = RandomForestClassifier(random_state=43)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

我得到的准确率为 93.3%（由于种子固定，因此可重现）
然后，如果我想尝试使用标准化进行相同的实验，如下所示：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)

X_train = normalize(X_train)
X_test = normalize(X_test)
rf = RandomForestClassifier(random_state=43)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

我的准确率总是 100%（只有当我使用 &gt;50% 作为测试集时，准确率才会开始下降）。
我不明白为什么会发生这种情况。
我很想阅读关于标准化和标准化之间这种不同行为的解释MinMax。
或者甚至是一些见解，一些我应该检查的东西，以便了解发生了什么。]]></description>
      <guid>https://stackoverflow.com/questions/79275753/normalization-vs-minmax-scaling-on-iris-dataset</guid>
      <pubDate>Thu, 12 Dec 2024 15:53:15 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV SSD MobileNet V2 模型：尽管有置信度阈值，但仍无法检测到物体</title>
      <link>https://stackoverflow.com/questions/79275458/opencv-ssd-mobilenet-v2-model-no-object-detection-despite-confidence-threshold</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79275458/opencv-ssd-mobilenet-v2-model-no-object-detection-despite-confidence-threshold</guid>
      <pubDate>Thu, 12 Dec 2024 14:19:28 GMT</pubDate>
    </item>
    <item>
      <title>无监督递归特征消除</title>
      <link>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination</link>
      <description><![CDATA[我想知道是否有人能帮忙提供一些无监督递归特征消除 (uRFE) 的 R 代码？
简而言之，我运行了一个无监督的 SOM，用 k-medoids 进行分区，但也想使用轮廓分数来识别 uRFE。
下面附上了一些虚拟代码。
library(aweSOM)

#IRIS 数据

full.data &lt;- iris
train.data &lt;- full.data[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]
train.data &lt;- scale(train.data)

#SOM 初始化 + MAP

set.seed(1465)
init &lt;- somInit(train.data, 4, 4)
iris.som &lt;- kohonen::som(train.data, grid = kohonen::somgrid(4, 4, &quot;hexagonal&quot;), 
rlen = 100, alpha = c(0.05, 0.01), radius = c(2.65,-2.65), 
dist.fcts = &quot;sumofsquares&quot;, init = init)

#SOM 聚类

superclust_pam &lt;- cluster::pam(iris.som$codes[[1]], 3)
superclasses_pam &lt;- superclust_pam$clustering
]]></description>
      <guid>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination</guid>
      <pubDate>Thu, 12 Dec 2024 12:27:01 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习模型部署在一台服务器上（而不是两台）可增加其响应时间</title>
      <link>https://stackoverflow.com/questions/79275068/increased-response-time-of-a-machine-learning-model-when-deploying-it-on-a-singl</link>
      <description><![CDATA[我最近在两台并行服务器上部署了一个利用 GPU 的机器学习模型。使用负载平衡器在它们之间平衡请求负载。为了减少资源使用量，我决定切换到单服务器设置，将所有请求直接路由到其中一台服务器。该服务器有一个 Nvidia tesla t4 GPU。该模型仅使用了 15GB 容量中的 1.2GB 左右。
更改后，我观察到以下情况：
CPU 利用率：几乎没有变化。
GPU 利用率：如预期一样翻倍，但未超过 GPU 的容量。
平均 GPU 利用率百分比从 7.5% 上升到 16.5%
最大 GPU 利用率百分比从 38% 上升到 48%
第 90 个百分位的 GPU 利用率百分比从 19% 上升到 34%
模型响应时间：平均增加了约 10% 到 15%。
尽管有这些观察结果，但我无法确定模型响应时间增加的确切原因。有什么见解或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79275068/increased-response-time-of-a-machine-learning-model-when-deploying-it-on-a-singl</guid>
      <pubDate>Thu, 12 Dec 2024 12:15:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 peft 进行微调时出错</title>
      <link>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</guid>
      <pubDate>Thu, 12 Dec 2024 07:31:09 GMT</pubDate>
    </item>
    <item>
      <title>使用预先训练的 CNN 进行螃蟹性别分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/79273773/using-pre-trained-cnn-for-crab-gender-classification</link>
      <description><![CDATA[我可以使用预先训练好的、已针对对象分类进行微调的 cnn 模型，然后添加我自己的螃蟹数据集吗？
示例：一个经过训练可以对花朵等对象进行分类的 cnn 模型，该模型是在 5 个类别上进行训练的，对吗？现在我想添加螃蟹图像，这样它就会知道它是螃蟹。我需要重新训练整个模型吗，还是我可以只添加我的数据集并训练模型？
我现在正在进行螃蟹性别分类。但我的顾问想在我的系统演示中添加，如果用户在螃蟹旁边添加图像，它将识别出不是螃蟹。我解释说这是不可能的，因为模型是针对性别分类而不是对象分类进行训练的（我使用 efficientnetb7 来训练我的螃蟹图像）。
我怎样才能实现他们的要求？
我还没有尝试任何东西，因为 google colab 很贵，而且我还在读学士学位]]></description>
      <guid>https://stackoverflow.com/questions/79273773/using-pre-trained-cnn-for-crab-gender-classification</guid>
      <pubDate>Thu, 12 Dec 2024 03:46:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么预先训练的 Swin Transformer 编码器在 TPU 上失败但在 Colab 中的 CPU 上可以运行？</title>
      <link>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</link>
      <description><![CDATA[我正在处理图像分割任务，并尝试使用预先训练的 Swin Transformer Large (Swin-L) 编码器作为特征提取主干。代码在 Colab 中的 CPU 上完美运行。但是，当切换到 TPU 时，它会抛出如下所示的错误。
代码：
from tensorflow.keras import layer, Model, Input
from tfswin import SwinTransformerLarge224

def load_swin_encoder(input_shape=(512, 512, 3)):
# 加载预训练的 Swin-L 模型
swin_encoder = SwinTransformerLarge224(include_top=False, weights=&#39;imagenet&#39;,
input_shape=input_shape)

# 冻结预训练层
for layer in swin_encoder.layers:
layer.trainable = False

# 从四个阶段提取输出
stage_outputs = [
swin_encoder.get_layer(&#39;normalize&#39;).output, # 从 0 阶段输出
swin_encoder.get_layer(&#39;layers.0&#39;).output, # 第一阶段的输出
swin_encoder.get_layer(&#39;layers.1&#39;).output, # 第二阶段的输出
swin_encoder.get_layer(&#39;layers.2&#39;).output, # 第三阶段的输出
swin_encoder.get_layer(&#39;layers.3&#39;).output, # 第四阶段的输出
]
return Model(swin_encoder.input, stage_outputs, name=&quot;SwinTransformerEncoder&quot;)

# 测试代码
encoder = load_swin_encoder(input_shape=(512, 512, 3))
dummy_input = tf.random.uniform((1, 512, 512, 3))
encoder_outputs =coder(dummy_input)

for i, output in enumerate(encoder_outputs):
print(f&quot;阶段 {i + 1} 输出形状：{output.shape}&quot;)


错误：
代码在 TPU 上抛出以下错误：
------------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-3cb122d32678&gt; 在 &lt;cell line: 2&gt;()
1 # 加载健全性检查
----&gt; 2 编码器 = load_swin_encoder(input_shape=(512, 512, 3))
3 dummy_input = tf.random.uniform((1, 512, 512, 3))
4 编码器输出 = 编码器(dummy_input)
5 

2 帧
/usr/local/lib/python3.10/dist-packages/keras/src/models/ functional.py in __init__(self, 输入, 输出, 名称, **kwargs)
117 for x in flat_inputs:
118 if not isinstance(x, backend.KerasTensor):
-&gt; 119 引发 ValueError(
120 “所有 `inputs` 值都必须是 KerasTensors。已收到：”
121 f“inputs={inputs} 包括无效值 {x}”

ValueError：所有 `inputs` 值都必须是 KerasTensors。已收到：inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层 &#39;input_4&#39; 创建”) 包括无效值 KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层创建” &#39;input_4&#39;&quot;) 类型为 &lt;class &#39;tf_keras.src.engine.keras_tensor.KerasTensor&#39;&gt;


问题：
为什么此代码在 Colab 中的 CPU 上有效，但在 TPU 上失败？我该如何修复此问题以使其与 TPU 执行兼容？
任何见解或指导都将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</guid>
      <pubDate>Mon, 02 Dec 2024 13:35:57 GMT</pubDate>
    </item>
    <item>
      <title>“TypeError：类型为‘numpy.float32’的对象没有 len()” - DeepSORT 与 YOLO 集成</title>
      <link>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</link>
      <description><![CDATA[我正在将 YOLOv8 与 DeepSORT 集成以进行多对象跟踪，但在将检测数据传递给 DeepSORT update_tracks() 函数时遇到了 TypeError。
错误消息：

速度：4.5ms 预处理，332.2ms 推理，0.6ms 后处理每个形状为 (1, 3, 480, 640) 的图像 DeepSORT 检测：[[ 107.22
186.92 639.26 479.49 0.83611]] 回溯（最近一次调用）：文件 &quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 127 行，在 detect_customers() 文件
&quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 84 行，在 detect_customers tracks = tracker.update_tracks(deep_sort_detections,
frame=frame) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件
&quot;/home/roy/environments/001-opencv/lib/python3.12/site-packages/deep_sort_realtime/deepsort_tracker.py&quot;，
第 195 行，在 update_tracks 中断言 len(raw_detections[0][0])==4
^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError：类型为“numpy.float32”的对象
没有 len()

代码部分（检测和跟踪）：
def detect_customers():
# 初始化 YOLO 和 DeepSORT
model = YOLO(&quot;yolov8s.pt&quot;) # 加载 YOLO 模型
tracker = DeepSort(max_age=30, n_init=3)

cap = cv2.VideoCapture(0) # 将 0 替换为视频源
active_customers = {}

while True:
ret, frame = cap.read()
if not ret:
break

results = model(frame) # 执行 YOLO 推理
detections = []

for result in results:
for box in result.boxes:
# 提取边界框和置信度得分
x1, y1, x2, y2 = box.xyxy[0].tolist() # 将边界框转换为列表
confidence = float(box.conf[0]) # 置信度得分

# 以所需格式附加检测
detection = [float(x1), float(y1), float(x2), float(y2), float(confidence)]
detections.append(detection)

# 处理空检测
if len(detections) == 0:
deep_sort_detections = np.empty((0, 5)) # 空数组表示没有检测
else:
deep_sort_detections = np.array(detections, dtype=np.float32) # 转换为具有适当结构的 NumPy 数组

# 调试：打印传递给 DeepSORT 的检测
print(&quot;Detections for DeepSORT:&quot;, deep_sort_detections)

# 更新跟踪器
tracks = tracker.update_tracks(deep_sort_detections, frame=frame)
for track in tracks:
if not track.is_confirmed():
continue

track_id = track.track_id
ltrb = track.to_ltrb() # 转换为 (left, top, right, bottom)
cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)
cv2.putText(frame, f&quot;ID: {track_id}&quot;, (int(ltrb[0]), int(ltrb[1]) - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 显示框架
cv2.imshow(&quot;客户检测&quot;, frame)
if cv2.waitKey(1) &amp; 0xFF == 27：# 按 ESC 退出
break

cap.release()
cv2.destroyAllWindows()

问题摘要：

错误描述：将 deep_sort_detections 传递给 tracker.update_tracks() 时，我收到 TypeError：类型为 &#39;numpy.float32&#39; 的对象没有 len()。
检测格式：我将检测格式化为 [[x1, y1, x2, y2,
confidence], ...] 并将其转换为 NumPy 数组，其中 dtype=np.float32。但是，DeepSORT 似乎需要不同的格式，或者存在某种类型问题。

问题：

如何正确格式化检测数据以与 DeepSORT 的 update_tracks() 方法兼容？
我是否遗漏了数据的结构或传递给 DeepSORT 的方式？
]]></description>
      <guid>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</guid>
      <pubDate>Thu, 28 Nov 2024 21:25:10 GMT</pubDate>
    </item>
    <item>
      <title>pytorch torchvision.datasets.ImageFolder FileNotFoundError：未找到类 .ipynb_checkpoints 的有效文件</title>
      <link>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</link>
      <description><![CDATA[尝试在 Colab 中使用 pytorch torch.datasets.ImageFolder 加载训练数据。
transform = transforms.Compose([transforms.Resize(400),
transforms.ToTensor()])
dataset_path = &#39;ss/&#39;
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

我遇到了以下错误：
-------------------------------------------------------------------------------
FileNotFoundError Traceback (most recent call last)
&lt;ipython-input-27-7abcc1f434b1&gt; in &lt;module&gt;()
2 transforms.ToTensor()])
3 dataset_path = &#39;ss/&#39;
----&gt; 4 dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
5 dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

3 帧
/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in make_dataset(directory, class_to_idx, extensions, is_valid_file)
100 if extensions 不为 None:
101 msg += f&quot;支持的扩展名是：{&#39;, &#39;.join(extensions)}&quot;
--&gt; 102 raise FileNotFoundError(msg)
103 
104 return entities

FileNotFoundError: 未找到类 .ipynb_checkpoints 的有效文件。支持的扩展名为：.jpg、.jpeg、.png、.ppm、.bmp、.pgm、.tif、.tiff、.webp

我的数据集文件夹包含一个子文件夹，其中包含许多 png 格式的训练图像，但 ImageFolder 仍然无法访问它们。]]></description>
      <guid>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</guid>
      <pubDate>Fri, 02 Jul 2021 17:31:32 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 中为 TF 数据集中的独热编码标签指定类别或样本权重</title>
      <link>https://stackoverflow.com/questions/67181641/specifying-class-or-sample-weights-in-keras-for-one-hot-encoded-labels-in-a-tf-d</link>
      <description><![CDATA[我正在尝试在不平衡的训练集上训练图像分类器。为了应对类别不平衡，我想对类别或单个样本进行加权。对类别进行加权似乎不起作用。不知何故，对于我的设置，我无法找到指定样本权重的方法。下面您可以阅读我如何加载和编码训练数据以及我尝试的两种方法。
训练数据加载和编码
我的训练数据存储在一个目录结构中，其中每个图像都放置在与其类别相对应的子文件夹中（我总共有 32 个类别）。由于训练数据太大，无法一次性全部加载到内存中，因此我使用 image_dataset_from_directory 并通过它在 TF 数据集 中描述数据：
train_ds = keras.preprocessing.image_dataset_from_directory (training_data_dir,
batch_size=batch_size,
image_size=img_size,
label_mode=&#39;categorical&#39;)

我使用 label_mode &#39;categorical&#39;，以便将标签描述为 one-hot 编码向量。
然后我预取数据：
train_ds = train_ds.prefetch(buffer_size=buffer_size)

方法 1：指定类权重
在这种方法中，我尝试通过以下方式指定类的类权重fit 的 class_weight 参数：
model.fit(
train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,
class_weight=class_weights
)

对于每个类别，我们计算权重，该权重与该类别的训练样本数量成反比。操作如下（这是在上述 train_ds.prefetch() 调用之前完成的）：
class_num_training_samples = {}
for f in train_ds.file_paths:
class_name = f.split(&#39;/&#39;)[-2]
if class_name in class_num_training_samples:
class_num_training_samples[class_name] += 1
else:
class_num_training_samples[class_name] = 1
max_class_samples = max(class_num_training_samples.values())
class_weights = {}
for i in range(0, len(train_ds.class_names)):
class_weights[i] = max_class_samples/class_num_training_samples[train_ds.class_names[i]]

我不确定的是这个解决方案是否有效，因为如果标签是独热编码的，keras 文档不会指定 class_weights 字典的键。
我尝试以这种方式训练网络，但发现权重对生成的网络没有真正的影响：当我查看每个单独类别的预测类别分布时，我可以识别出整个训练集的分布，其中对于每个类别，主要类别的预测最有可能。
在不指定任何类别权重的情况下运行相同的训练会导致类似的结果。
所以我怀疑权重似乎对我的情况没有影响。
这是因为指定类别权重不适用于独热编码标签，还是因为我可能做错了其他事情（在代码中我没有显示）？
方法 2：指定样本权重
为了提出不同的（在我看来不太优雅的）解决方案，我想通过 fit 方法的 sample_weight 参数指定单个样本权重。但是从文档中我发现：

[...] 当 x 是数据集、生成器或 keras.utils.Sequence 实例时，不支持此参数，而是提供 sample_weights 作为 x 的第三个元素。

在我的设置中，train_ds 是一个数据集，情况确实如此。现在我真的很难找到文档，从中我可以得出如何修改 train_ds，使其具有带有权重的第三个元素。我认为使用数据集的 map 方法很有用，但我提出的解决方案显然无效：
train_ds = train_ds.map(lambda img, label: (img, label, class_weights[np.argmax(label)]))

是否有人有可以与 image_dataset_from_directory 加载的数据集结合使用的解决方案？]]></description>
      <guid>https://stackoverflow.com/questions/67181641/specifying-class-or-sample-weights-in-keras-for-one-hot-encoded-labels-in-a-tf-d</guid>
      <pubDate>Tue, 20 Apr 2021 15:10:35 GMT</pubDate>
    </item>
    <item>
      <title>集成数值/物理数据用于 CNN 图像分类</title>
      <link>https://stackoverflow.com/questions/63206214/integrating-numerical-physical-data-for-cnn-image-classification</link>
      <description><![CDATA[我正在尝试使用 CNN 通过 keras 在 Python 中对医学图像进行分类。这些医学图像还包括年龄和性别等文本信息，这些信息会影响模型的决策。我如何训练一个可以同时使用图像和现实世界信息进行训练的 CNN，以便它能够根据两者进行分类？]]></description>
      <guid>https://stackoverflow.com/questions/63206214/integrating-numerical-physical-data-for-cnn-image-classification</guid>
      <pubDate>Sat, 01 Aug 2020 14:17:57 GMT</pubDate>
    </item>
    <item>
      <title>如何规范视频游戏中机器学习（神经网络）的输入</title>
      <link>https://stackoverflow.com/questions/45168486/how-to-normalize-input-for-machine-learning-neural-networking-on-video-games</link>
      <description><![CDATA[我一直在创建神经网络，计划训练它们玩我制作的游戏（神经网络可以访问所有游戏数据）。​​我对神经网络、遗传算法和 NEAT 实现有很深的理解。然而，我遇到的问题是将玩家看到的输入标准化。如果我们有一个敌人对象、一个医疗包对象和一个武器对象，则需要输入它们并对其进行不同的处理。我在此处看到了 SethBling 的视频，他简要解释了如何设置该神经网络。他只使用了值 1、0 和 -1。但是，对于更复杂的游戏，这行不通。我尝试让一个小的模拟在输入为 .25 ]]></description>
      <guid>https://stackoverflow.com/questions/45168486/how-to-normalize-input-for-machine-learning-neural-networking-on-video-games</guid>
      <pubDate>Tue, 18 Jul 2017 13:39:23 GMT</pubDate>
    </item>
    </channel>
</rss>