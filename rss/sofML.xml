<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 08 Mar 2024 12:23:53 GMT</lastBuildDate>
    <item>
      <title>无法进行网格搜索和训练模型</title>
      <link>https://stackoverflow.com/questions/78127612/not-able-to-do-grid-search-and-train-the-model</link>
      <description><![CDATA[我正在研究基本的文本分类问题，我想使用堆叠分类器以及对基本分类器的参数进行一些微调以获得高精度结果。
我的数据集有 8000 行和 2 列（文本和类）。下面的代码似乎被卡住了，我不熟悉该领域（初学者）来发现问题。
导入 pandas 作为 pd
从 sklearn.model_selection 导入 GridSearchCV，train_test_split
从 sklearn.ensemble 导入 StackingClassifier
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.svm 导入 NuSVC
从 sklearn.discriminant_analysis 导入 LinearDiscriminantAnalysis
从sklearn.metrics导入accuracy_score、log_loss、classification_report、confusion_matrix

# 定义分类器的参数网格
param_grid_nusvc = {
    “努”：[0.1，0.3，0.5，0.7，0.9]，
    &#39;内核&#39;：[&#39;线性&#39;，&#39;rbf&#39;]，
}

param_grid_logreg = {
    ‘C’: [0.1, 1, 10],
    &#39;惩罚&#39;: [&#39;l1&#39;, &#39;l2&#39;],
}

# 以更高的清晰度执行分类器的网格搜索
nusvc_grid_search = GridSearchCV(NuSVC(probability=True), param_grid_nusvc, cv=2, rating=&#39;accuracy&#39;) # 使用准确率评分
logreg_grid_search = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=2, 评分=&#39;准确度&#39;)

nusvc_grid_search.fit(X_train, y_train)
logreg_grid_search.fit(X_train, y_train)

# 获取最佳参数
best_params_nusvc = nusvc_grid_search.best_params_
best_params_logreg = logreg_grid_search.best_params_

# 设置具有最佳参数的基分类器
best_nusvc = NuSVC(概率=True, **best_params_nusvc)
best_logreg = LogisticRegression(**best_params_logreg)

# 设置堆叠分类器
sc = 堆叠分类器(
    估计量=[
        (&#39;NuSVC&#39;, best_nusvc),
        （&#39;LDA&#39;，线性判别分析（））
    ],
    最终估计器=best_logreg
）

sc.fit(X_train, y_train)

# 评估组合分类器
print(&#39;****结果****&#39;)
train_predictions = sc.predict(X_test)
acc = 准确度_分数(y_test, train_predictions)
print(&quot;准确度: {:.4%}&quot;.format(acc))

train_predictions_proba = sc.predict_proba(X_test)
ll = log_loss(y_test, train_predictions_proba)
print(&quot;对数丢失: {}&quot;.format(ll))

# 打印分类报告（可选）
print(&#39;\n分类报告:&#39;)
打印（分类报告（y_test，train_predictions））

# 打印混淆矩阵（可选）
print(&#39;\n混淆矩阵:&#39;)
打印（confusion_matrix（y_test，train_predictions））

上面的一些更改是根据 chatGPT 的建议进行的，以指导我如何使用网格搜索进行微调。代码似乎卡住了（大约 20 分钟）。如果没有网格搜索，它似乎可以轻松地在 2-3 分钟内运行。]]></description>
      <guid>https://stackoverflow.com/questions/78127612/not-able-to-do-grid-search-and-train-the-model</guid>
      <pubDate>Fri, 08 Mar 2024 12:17:54 GMT</pubDate>
    </item>
    <item>
      <title>发生异常：ValueError 数据基数不明确：x 大小：1280 y 大小：32 确保所有数组包含相同数量的样本</title>
      <link>https://stackoverflow.com/questions/78127570/exception-has-occurred-valueerror-data-cardinality-is-ambiguous-x-sizes-1280</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78127570/exception-has-occurred-valueerror-data-cardinality-is-ambiguous-x-sizes-1280</guid>
      <pubDate>Fri, 08 Mar 2024 12:09:21 GMT</pubDate>
    </item>
    <item>
      <title>conv2d 函数参数的问题[重复]</title>
      <link>https://stackoverflow.com/questions/78127273/problem-with-arguments-of-conv2d-function</link>
      <description><![CDATA[我有这个 CNN 代码：
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn

CNN 类（nn.Module）：
 def __init__(自身):
  超级（CNN，自我）.__init__()
    自我.n = 10
    内核大小 = 3
    填充 = (内核大小 - 1) / 2
    self.conv1 = nn.Conv2d(in_channels=3,out_channels=self.n,kernel_size=kernel_size,stride = (2,2),padding=padding),

    self.conv2 = nn.Conv2d(in_channels=self.n,out_channels=2*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding),
        
    self.conv3 = nn.Conv2d(in_channels=2*self.n,out_channels=4*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding),
    
    self.conv4 = nn.Conv2d(in_channels=4*self.n,out_channels=8*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding),
    
    self.fc1 = nn.Linear(8 * self.n * 7 * 4, 100)
    self.fc2 = nn.Linear(100, 2)

 def 转发（自身，inp）：
   输出 = nn.function.relu(self.conv1(inp))
   输出 = nn.function.relu(self.conv2(out))
   输出 = nn.function.relu(self.conv3(out))
   输出 = nn.function.relu(self.conv4(out))

   出=出。视图(-1, 8 * self.n * 7 * 4)
   输出 = nn.function.relu(self.fc1(out))
   输出 = self.fc2(输出)
    
   返回

我收到错误：
TypeError: conv2d() 收到了无效的参数组合 - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int)，但需要以下之一：
 *（张量输入、张量权重、张量偏差、整数步幅元组、整数填充元组、整数膨胀元组、整数组）
      不匹配，因为某些参数的类型无效： (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (float, float)!, !tuple of (int, int)!, int)
 *（张量输入、张量权重、张量偏差、整数步幅元组、str 填充、整数膨胀元组、整数组）
      不匹配，因为某些参数的类型无效： (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (float, float)!, !tuple of (int, int)!, int)

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78127273/problem-with-arguments-of-conv2d-function</guid>
      <pubDate>Fri, 08 Mar 2024 11:09:20 GMT</pubDate>
    </item>
    <item>
      <title>添加到我的应用程序时，CreateML 模型无法按预期工作</title>
      <link>https://stackoverflow.com/questions/78127234/createml-model-doesnt-work-as-expected-when-added-to-my-application</link>
      <description><![CDATA[我有一个训练有素的模型来识别深蹲（好的和坏的重复）。当我使用一些测试数据预览它时，它似乎在 CreateML 中完美运行，尽管将其添加到我的应用程序后，模型似乎不准确，并且大多数时候会混淆操作。有谁知道问题是否与代码相关，或者与模型本身及其如何分析实时数据有关？
下面我添加了“Good Squats”功能之一大多数时候甚至不会被调用（即使信心较低）。大多数时候，模型将所有事情都归为糟糕的深蹲，尽管事实显然并非如此。
问题可能是我的数据集没有足够的视频吗？
如果操作==“GoodForm” &amp;&amp;信心&gt; 0.80&amp;&amp; !squatDetected {
            打印（“好形式”）
            蹲检测=真
            
            DispatchQueue.main.asyncAfter(截止日期: .now() + 1.5) {
                self.squatDetected = false
            }
            DispatchQueue.main.async {
                self.showGoodFormAlert（带有：信心）
                音频服务PlayAlertSound（系统声音ID（1322））
            }
        }

我尝试过使用不同的 fps 设置和动作持续时间训练其他模型。目前我拥有的最好的设置是：120FPS 1.5s 动作持续时间。
更改了我的预测函数，使其仅分析每 10（以及每 20）帧而不是每帧。还是什么都没有]]></description>
      <guid>https://stackoverflow.com/questions/78127234/createml-model-doesnt-work-as-expected-when-added-to-my-application</guid>
      <pubDate>Fri, 08 Mar 2024 11:03:19 GMT</pubDate>
    </item>
    <item>
      <title>cnn 代码有问题，可能是班级老师的问题</title>
      <link>https://stackoverflow.com/questions/78126559/problem-with-a-cnn-code-maybe-instructor-problem-of-the-class</link>
      <description><![CDATA[我建立了一个 CNN
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn

CNN 类（nn.Module）：
 def __init__(自身):
  超级（CNN，自我）.__init__()
    自我.n = 10
    内核大小 = 3
    填充 = (内核大小 - 1) / 2
    self.conv1 = nn.Conv2d(in_channels=3,out_channels=self.n,kernel_size=kernel_size,stride = (2,2),padding=padding)

    self.conv2 = nn.Conv2d(in_channels=self.n,out_channels=2*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding)
        
    self.conv3 = nn.Conv2d(in_channels=2*self.n,out_channels=4*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding)
    
    self.conv4 = nn.Conv2d(in_channels=4*self.n,out_channels=8*self.n,kernel_size=kernel_size,stride = (2,2),padding=padding)
    
    self.fc1 = nn.Linear(8 * self.n * 7 * 4, 100)
    self.fc2 = nn.Linear(100, 2)

 def 转发（自身，inp）：
   输出 = nn.function.relu(self.conv1(inp))
   输出 = nn.function.relu(self.conv2(out))
   输出 = nn.function.relu(self.conv3(out))
   输出 = nn.function.relu(self.conv4(out))

   出=出。视图(-1, 8 * self.n * 7 * 4)
   输出 = nn.function.relu(self.fc1(out))
   输出 = self.fc2(输出)
    
   返回

输入数据inp是形状为(N,3,448,224)的张量，输出形状为(N,2)。
问题是我收到错误：
“TypeError: conv2d() 收到无效的参数组合 - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int)，但需要以下之一：
 *（张量输入、张量权重、张量偏差、整数步幅元组、整数填充元组、整数膨胀元组、整数组）
      不匹配，因为某些参数的类型无效： (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (float, float)!, !tuple of (int, int)!, int)
 *（张量输入、张量权重、张量偏差、整数步幅元组、str 填充、整数膨胀元组、整数组）
      不匹配，因为某些参数的类型无效： (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (float, float)!, !tuple of (int, int)!, int)”

有什么解决办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78126559/problem-with-a-cnn-code-maybe-instructor-problem-of-the-class</guid>
      <pubDate>Fri, 08 Mar 2024 09:05:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么 `skorecard.WoeEncoder()` 这么慢，如何让它更快？</title>
      <link>https://stackoverflow.com/questions/78126501/why-is-skorecard-woeencoder-so-slow-and-how-can-i-make-it-faster</link>
      <description><![CDATA[我正在使用 WoeEncoder()  从 skorecard Python 库到对预测建模的大量特征进行目标编码，但由于某种原因速度非常慢。
是否有任何方法可以修改 WoeEncoder() 的代码以保留相同的功能，但只是使其运行速度更快？我需要在 sklearn 管道等中使用它。

可重现的示例显示了巨大的速度差异：
from skorecard.preprocessing import WoeEncoder # pip install skorecard

随机导入
将 pandas 导入为 pd
将 numpy 导入为 np

# 时间安排：
导入时间
来自人类友好的导入 format_timespan

def tic():
    全球圣
    st = 时间.time()
    
def 目录():
    et = 时间.time()
    time_elapsed = 轮(et - st, 1)
    print(&quot;经过的时间:&quot;, format_timespan(time_elapsed))

生成随机分箱数据以进行 WOE 编码：
X_train_binned = pd.DataFrame(np.random.randint(0,10, size = (20000, 1000))).astype(str)
X_test_binned = pd.DataFrame(np.random.randint(0,10, size = (20000, 1000))).astype(str)

y_train = np.random.randint(0,2, 大小 = (20000, ))



&lt;强&gt;1。使用 skorecard.WoeEncoder() ：
&lt;前&gt;&lt;代码&gt;tic()

祸 = WoeEncoder()
woe.fit(X_train_binned, y_train) # 快
X_train_WOE_1 = woe.transform(X_train_binned) # 非常慢
X_test_WOE_1 = woe.transform(X_test_binned) # 非常慢

目录()

&lt;块引用&gt;
已用时间：3 分 26.1 秒



&lt;强&gt;2。使用我自己的函数：
def woe_mapping_1d(X_train_binned_col, y_train, epsilon = 0.0001):
    df = pd.DataFrame({&#39;feat&#39;: X_train_binned_col, &#39;target&#39;: y_train}).reset_index(drop = True)
    df[&#39;non_target&#39;] = np.where(df[&#39;target&#39;] == 1, 0, 1)
    
    woe_table = df.groupby(&#39;feat&#39;, as_index = False).agg(target_count = (&#39;target&#39;, &#39;sum&#39;),
                                                         non_target_count = (&#39;non_target&#39;, &#39;总和&#39;))
    woe_table[&#39;pct_of_target&#39;] = (woe_table[&#39;target_count&#39;] / df[&#39;target&#39;].sum()) + epsilon
    woe_table[&#39;pct_of_non_target&#39;] = (woe_table[&#39;non_target_count&#39;] / df[&#39;non_target&#39;].sum()) + epsilon
    woe_table[&#39;WOE&#39;] = np.log(woe_table[&#39;pct_of_non_target&#39;] / woe_table[&#39;pct_of_target&#39;])

    映射_dict = dict(zip(woe_table[&#39;feat&#39;], woe_table[&#39;WOE&#39;]))
    返回映射字典


def create_woe_mapping_dict(X_train_binned, y_train, epsilon = 0.0001):
    woe_mapping_dict = {}

    对于 X_train_binned.columns 中的 var：
        woe_mapping_dict[var] = woe_mapping_1d(X_train_binned_col = X_train_binned[var], y_train = y_train, epsilon = epsilon)
        
    返回 woe_mapping_dict


def apply_woe_mappings(X_binned, woe_mapping_dict, woe_value_unknown = 0):
    X_WOE_列表 = []
    
    对于 X_binned.columns 中的 var：
        X_WOE_var = X_binned[var].map(woe_mapping_dict[var]).fillna(woe_value_unknown)
        X_WOE_list.append(X_WOE_var)
    
    X_WOE_df = pd.concat(X_WOE_list, 轴 = 1)
    返回X_WOE_df

它的运行速度快了约 40 倍，并产生相同的结果：
&lt;前&gt;&lt;代码&gt;tic()

woe_dict = create_woe_mapping_dict(X_train_binned, y_train)

X_train_WOE_2 = apply_woe_mappings(X_train_binned, woe_dict)
X_test_WOE_2 = apply_woe_mappings(X_test_binned, woe_dict)

目录()

打印（X_train_WOE_1.等于（X_train_WOE_2））
打印（X_test_WOE_1.等于（X_test_WOE_2））

&lt;块引用&gt;
已用时间：5.3 秒
真的
真的


怎样才能让它更快？]]></description>
      <guid>https://stackoverflow.com/questions/78126501/why-is-skorecard-woeencoder-so-slow-and-how-can-i-make-it-faster</guid>
      <pubDate>Fri, 08 Mar 2024 08:54:26 GMT</pubDate>
    </item>
    <item>
      <title>目标和输入大小不匹配的 ValueError</title>
      <link>https://stackoverflow.com/questions/78126473/valueerror-with-mismatched-target-and-input-size</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78126473/valueerror-with-mismatched-target-and-input-size</guid>
      <pubDate>Fri, 08 Mar 2024 08:50:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么LSTM在大时间步长的情况下仍然存在梯度消失的问题？</title>
      <link>https://stackoverflow.com/questions/78126328/why-does-lstm-still-have-the-problem-of-gradient-vanishing-in-large-time-step-ca</link>
      <description><![CDATA[我构建了一个 LSTM 模型来预测机械零件的行为。
输入的大小为 (256, 32)。 256 是时间步数，32 是每个步中的特征数。输出的大小为 2。我有 20,000 组这些输入和输出
我使用以下模型来训练输入和输出，
模型 = keras.Sequential()

model.add(layers.Input(shape=(None, input_size)))

model.add(layers.双向(layers.LSTM(32, return_sequences=True)))

model.add(layers.Dropout(0.1))

model.add(layers.Dense(output_size))

我使用 mse 作为训练的损失函数，并使用 adam 作为优化器。
但是，我得到的损失结果如下所示：
图片
训练和验证损失收敛到接近 0.5，这是相当大的。
我认为问题在于时间步长。较长的时间步长会导致梯度消失的问题。因为如果我减少时间步长，使输入的大小为 (64, 32)，那么训练损失和验证损失都会收敛到一个非常小的数字 (10^-2)。
但我仍然想训练输入大小为 (256, 32) 的数据集。我该怎么办？
我看到一些博客建议将输入切割成几个部分（例如 4 个 (64, 32) 形成 (256, 32)）并将这些部分连接在一起，并将前一个输出缓存到下一个输入的输入。但我不知道如何提取 LSTM 中的隐藏信息，以便将其输入到下一部分的输入中。您有什么想法或有其他解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78126328/why-does-lstm-still-have-the-problem-of-gradient-vanishing-in-large-time-step-ca</guid>
      <pubDate>Fri, 08 Mar 2024 08:19:11 GMT</pubDate>
    </item>
    <item>
      <title>尽管张量是叶子，但神经网络中的损失却没有得到任何结果</title>
      <link>https://stackoverflow.com/questions/78126160/getting-none-from-loss-in-neural-network-despite-tensors-being-leaf</link>
      <description><![CDATA[我检查了所有的张量和输入参数，它们都是叶子，根据下面的代码，
def train_step(w1,b1):
    打印（“w=”，w1）
    可训练变量 = [w1,b1]
    优化器 = torch.optim.SGD(trainable_variables, lr=learning_rate)
    损失=变量（loss2_function（），requires_grad = True）
    打印(loss.backward())
    使用 torch.no_grad()：
        w1 -=(学习率 * w1.grad)
        b1 -= (学习率 * b1.grad)
        w1.grad.zero_()
        b1.grad.zero_()
    优化器.step()
    优化器.zero_grad()

我仍然没有得到任何结果，即使学习率、权重和偏差发生变化，网络仍然无法工作，请指导我。]]></description>
      <guid>https://stackoverflow.com/questions/78126160/getting-none-from-loss-in-neural-network-despite-tensors-being-leaf</guid>
      <pubDate>Fri, 08 Mar 2024 07:42:18 GMT</pubDate>
    </item>
    <item>
      <title>深度学习模型训练</title>
      <link>https://stackoverflow.com/questions/78126036/deep-learning-model-training</link>
      <description><![CDATA[我一直在尝试创建一个 CNN 模型来预测头部受伤。我已经确定了数据集的频谱图图像，并创建了指示头部是否受伤的标签文件。
我拿了两个数据集，都有列：通道、标签和图像路径。标记为1（表示头部受伤）和0（表示未受伤）。从每个数据集中总共识别出 117 个通道图像。在尝试运行 CNN 模型时，我在 Jupyter Notebook 中使用了以下代码：
导入操作系统
将 pandas 导入为 pd
将 numpy 导入为 np
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split
从tensorflow.keras.preprocessing.image导入load_img，img_to_array
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Flatten、Dense、Dropout
从tensorflow.keras.optimizers导入Adam
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.utils导入to_categorical

df1 = pd.read_csv(&#39;C:/Users/Lenovo/Desktop/EEG 处理材料/div_attention.cnt 频谱图图像/更新的头部受伤标签.csv&#39;)
df2 = pd.read_csv(&#39;C:/Users/Lenovo/Desktop/EEG 处理材料/1c_p300 频谱图图像/更新的头部受伤标签 2.csv&#39;)

df = pd.concat([df1, df2], axis=0).reset_index(drop=True)

df[&#39;标签&#39;] = df[&#39;标签&#39;].astype(str)

train_df，test_df = train_test_split（df，test_size = 0.2，random_state = 42）

train_df[&#39;标签&#39;] = train_df[&#39;标签&#39;].astype(str)
test_df[&#39;标签&#39;] = test_df[&#39;标签&#39;].astype(str)

train_datagen = ImageDataGenerator(重新缩放=1./255)
test_datagen = ImageDataGenerator（重新缩放=1./255）

train_generator = train_datagen.flow_from_dataframe(
    数据框=train_df，
    x_col=&#39;图像路径&#39;,
    y_col=&#39;标签&#39;,
    目标大小=(1000, 800),
    批量大小=32，
    class_mode=&#39;二进制&#39;）

test_generator = test_datagen.flow_from_dataframe(
    数据框=test_df，
    x_col=&#39;图像路径&#39;,
    y_col=&#39;标签&#39;,
    目标大小=(1000, 800),
    批量大小=32，
    class_mode=&#39;二进制&#39;）

直到这一部分，它给我的输出为：
找到 186 个经过验证的图像文件名，属于 2 个类别。
找到 47 个经过验证的图像文件名，属于 2 个类别。
#创建模型
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Flatten、Dense、Dropout

模型=顺序（[
    Conv2D(32, (3, 3), 激活=&#39;relu&#39;, input_shape=(1000, 800, 3)),
    最大池化2D(2, 2),
    Conv2D(64, (3, 3), 激活=&#39;relu&#39;),
    最大池化2D(2, 2),
    Conv2D(128, (3, 3), 激活=&#39;relu&#39;),
    最大池化2D(2, 2),
    展平（），
    密集（256，激活=&#39;relu&#39;），
    辍学（0.5），
    密集（1，激活=&#39;sigmoid&#39;）
]）

model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

模型.summary()

输出：
模型摘要：
#训练模型
历史 = model.fit(
    火车发电机，
    steps_per_epoch=train_generator.n // train_generator.batch_size,
    纪元=5，
    验证数据=测试生成器，
    validation_steps=test_generator.n // test_generator.batch_size
）

输出：
模型训练输出：
模型训练代码后，我的内核自动死亡，显示 ResourceExhausted 错误，我该如何解决这个问题？我也尝试在 Google colab 中运行，也遇到了类似的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78126036/deep-learning-model-training</guid>
      <pubDate>Fri, 08 Mar 2024 07:12:06 GMT</pubDate>
    </item>
    <item>
      <title>物体检测（Opencv）[关闭]</title>
      <link>https://stackoverflow.com/questions/78125895/object-detectionopencv</link>
      <description><![CDATA[我尝试过精简版的 SSD MobileNet 模型来检测对象，但我的主要动机是
应该检测未经训练的对象，如果该对象在我们的数据集中不可用，那么它应该被检测为未知对象。
我该怎么做？
我需要的解决方案是我的模型应该将对象检测为未知对象，而无需机器学习。
我不想认出这个物体。只是应该使用 python opencv 检测对象。]]></description>
      <guid>https://stackoverflow.com/questions/78125895/object-detectionopencv</guid>
      <pubDate>Fri, 08 Mar 2024 06:34:43 GMT</pubDate>
    </item>
    <item>
      <title>训练 ViT 训练损失稳定，但验证曲线振荡</title>
      <link>https://stackoverflow.com/questions/78125693/stable-training-loss-but-oscillating-validation-curves-training-vit</link>
      <description><![CDATA[我正在修改后的视觉变换器模型上训练大约 1000 个通道的成像数据。
我的样本数量有限，因为我只有 10 个可用图像 (~200x200x1000)，我已将其中的图像转换为补丁，生成约 15k 个补丁，每个补丁都有关联的标签和平衡数据集。我还在通道上执行了 PCA 以降低维度。当前集包含 6 个训练、2 个验证和 2 个测试。
目前，这些是我迄今为止最好结果的训练和验证曲线。为这些结果生成的补丁大小为 8x8x25，重叠率为 50%：
训练损失
val 准确度
val 平衡精度
val f1
价值损失
我的问题是，了解如何推进这些结果。模型似乎是根据验证指标进行训练和学习的，但是，它波动很大，我不知道如何缓解这种情况。
我尝试过的：

不同的补丁大小（4x4、8x8、16x16 等）
不同的通道大小（8、16、32 等...）
生成补丁时的不同重叠（20%、50% 等）
降低学习率
降低权重衰减
平衡数据集

这些是我尝试减轻波动并提高整体准确性的方法。然而，它反而导致性能较差，例如在训练早期趋于稳定和更加极端的波动。]]></description>
      <guid>https://stackoverflow.com/questions/78125693/stable-training-loss-but-oscillating-validation-curves-training-vit</guid>
      <pubDate>Fri, 08 Mar 2024 05:29:54 GMT</pubDate>
    </item>
    <item>
      <title>这段代码有问题吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78125395/is-there-a-problem-in-this-piece-of-code</link>
      <description><![CDATA[机器学习，Python
model.add(LSTM(units=50,activation=&#39;relu&#39;,return_sequences=True,input_shape=(x_train.shape[1],1)))

无法访问成员“shape”对于类型“列表[未知]”构件“形状”未知]]></description>
      <guid>https://stackoverflow.com/questions/78125395/is-there-a-problem-in-this-piece-of-code</guid>
      <pubDate>Fri, 08 Mar 2024 03:29:16 GMT</pubDate>
    </item>
    <item>
      <title>siann的解决方案有一个问题： ValueError: Variable <tf.Variable 'u/bias:0' shape=(1,) dtype=float64> has `None` forgradient</title>
      <link>https://stackoverflow.com/questions/78125337/there-is-a-problem-with-scianns-solution-valueerror-variable-tf-variable-u</link>
      <description><![CDATA[使用scinn求解偏微分方程时出现问题，结果显示：
&lt;块引用&gt;
ValueError：变量 渐变有“无”。请确保您的所有操作都定义了梯度（即可微分）。常见的无梯度操作：K.argmax、K.round、K.eval。

导入数学
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 siann 导入为 sn
从 siann.utils.math 导入 diff、sign、sin、cos、tan、exp、sqrt、pow


亩=0.20
罗 = 1000
xE = 21*1000000000
G = xE/(2*(1 + mu))
cp = math.sqrt(xE*(1 - mu)/(rho*(1 + mu)*(1 - 2*mu)))
r0 = 2.5
xdb = 100/1000
xdp = 2.5*100/1000
ρ0 = 1000
xD = 4000
SB = 4000/1000
r0 = 3.0
b = 2.0
阿尔法 = 2000

# 计算A0
定义 A0():
    term1 = xdb/(8*sb)*rho0*xD**2
    项 2 = (xdp/xdb)**2.2
    返回第 1 项/第 2 项

# 待解变量
r = sn.Variable(&#39;r&#39;, dtype=&#39;float64&#39;)
z = sn.Variable(&#39;z&#39;, dtype=&#39;float64&#39;)
t = sn.Variable(&#39;t&#39;, dtype=&#39;float64&#39;)
u = sn.Functional(&#39;u&#39;, [r, z, t], 4*[40], &#39;tanh&#39;)

# 偏微分方程
PDE1= diff(u,r,阶=2)+1/r*diff(u,r)+diff(u,z,阶=2)-1/cp*diff(u,t,阶=2)


＃边界条件
公差=0.0000001
BC1= (1-符号(t-TOL))*(1-符号(r-r0-TOL))*(diff(u,z))
BC2=(1+符号(z-TOL))*(1-符号(z-b-TOL))*(1-符号(r-r0-TOL))*(xE/(1+mu)*(mu/( 1-2*mu)*(diff(u,r,阶=2)+diff(u,r)/r+diff(u,z,阶=2))+diff(u,r,阶=2) )-A0()*exp(-1*alpha*t))

# 训练和验证模型
m = sn.SciModel([r,z,t], [PDE1, BC1,BC2])
r_data,z_data,t_data = np.meshgrid(
    np.linspace(r0, 10, 40),
    np.linspace(0, 5, 40),
    np.linspace(0, 0.001, 100)
）
 
# 这一步出错了
h = m.train([r_data,z_data,t_data], 3*[&#39;零&#39;],learning_rate=0.002, epochs=1000, verbose=0)

r_test,z_test ,t_test = np.meshgrid(
    np.linspace(0, 10, 40),
    np.linspace(0, 5, 40),
    np.linspace(0, 0.001, 80)
）
u_pred = u1.eval(m, [r_test,z_test ,t_test])

图 = plt.figure(figsize=(3, 4))
plt.pcolor(r_test, z_test, u_pred, cmap=&#39;地震&#39;)
plt.xlabel(&#39;r&#39;)
plt.ylabel(&#39;z&#39;)
plt.colorbar()

我试图解决它，然后在 h = m.train(...) 步骤出现了 ValueError。]]></description>
      <guid>https://stackoverflow.com/questions/78125337/there-is-a-problem-with-scianns-solution-valueerror-variable-tf-variable-u</guid>
      <pubDate>Fri, 08 Mar 2024 03:00:25 GMT</pubDate>
    </item>
    <item>
      <title>如何用奇数样本大小批量训练神经网络？</title>
      <link>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</link>
      <description><![CDATA[我是神经网络领域的新手，正在使用 pytorch 进行一些训练。
我决定做一个简单的普通神经网络。
我使用了一个包含 2377 个数字特征和 6277 个示例的个人数据集。
我的第一次尝试是让神经网络预测每个示例，因此伪代码如下所示
对于范围内的 i(...)：
    X = ... # 特征
    y = ... # 结果
    y_pred = 模型(X[i])
    损失=标准(y_pred, y)

    y_pred.size # [1,1]
    y.尺寸#[1,1]

每个时期大约需要 10 秒，我决定使用小批量来改进它。
所以我在开始时定义了批量大小，Pytorch 中的神经网络是这样定义的
&lt;前&gt;&lt;代码&gt;batch_size = 30
n_inputs = X.size[1] #2377

## 2 个隐藏层
模型 = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    ReLU(),
    nn.线性(1024, 512),
    ReLU(),
    nn.线性(512, 356),
    ReLU(),
    nn.Linear(356,batch_size),
    ReLU(),
）

然后我分批进行训练
对于范围（5）内的纪元：
    总损失 = 0
    排列 = torch.randperm(X.size()[0])
    对于范围内的 i（0，X.size（）[0]，batch_size）：
        优化器.zero_grad()
        索引 = 排列[i:i+batch_size]
        batch_x, batch_y = x[索引], y[索引]

        ypred = 模型(batch_x)
        损失=标准(ypred,batch_y)
        总损失 += loss.item()
        
        ## 更新权重
        loss.backward()
        优化器.step()

现在的问题是我的神经网络总是输出 100 个值但最后的批量大小可能会有所不同。
事实上，如果我选择 100 作为批量大小，最后一批将由 77 个示例组成 (6277%100)。
我确信有一种方法可以解决这个问题，并且我的结构中有一个错误，但我看不到它。
您能帮助我概括批量训练以处理任意数量的示例和批量大小吗？]]></description>
      <guid>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</guid>
      <pubDate>Thu, 07 Mar 2024 08:58:57 GMT</pubDate>
    </item>
    </channel>
</rss>