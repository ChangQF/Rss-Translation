<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 09 Oct 2024 12:32:46 GMT</lastBuildDate>
    <item>
      <title>我正在使用 RL 优化 AGV 路径规划以提高能源效率。我不明白为什么网络没有学习</title>
      <link>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</guid>
      <pubDate>Wed, 09 Oct 2024 10:02:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Java Android 中实现 HDBSCAN 聚类</title>
      <link>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</link>
      <description><![CDATA[我想将 HDBscan 算法实现到 Java Android 应用程序中。我正在将 C# 移植到 Java。在 C# 中，它们是使用名为 Hdbscansharp 的库完成的。我尝试在 Java 中使用 ELKI，但没有成功。原始 C# 代码是
 double avgTgtSpd = (tgt1.speed + tgt2.speed) / 2;
filteredHits.Add(new FilteredHit(avgTgtDist, avgTgtSpd, currTgtDirection)); 
aggregateSpeed += avgTgtSpd;

// HDBSCAN Clustering
double[][] twoDfilteredHits =filteredHits.Select(hit =&gt; new double[] { hit.pos 
}).ToArray(); // 将过滤后的命中结果放入 hdbscan lib 可以使用的格式中
HdbscanResult hdbscanResult = HdbscanRunner.Run(new HdbscanParameters&lt;double[]&gt;
{
DataSet = twoDfilteredHits.ToArray(),
MinPoints = 3, 
MinClusterSize = 4
DistanceFunction = new HdbscanSharp.Distance.ManhattanDistance()
});

我能够将代码移植到 java 中的“twoDfilter”，但在 HDBSCan 的实现中卡住了。
如何在 java 中获取“hdbscanResult”？]]></description>
      <guid>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</guid>
      <pubDate>Wed, 09 Oct 2024 09:47:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在时间序列分类任务中对看不见的数据应用数据转换管道？</title>
      <link>https://stackoverflow.com/questions/79066893/how-to-apply-data-transformation-pipeline-on-unseen-data-on-task-of-time-series</link>
      <description><![CDATA[我正在研究一项根据给定姿势数据对人类活动进行分类的任务。由于应用姿势估计模型时图像的质量，它包含许多缺失值。我针对此训练数据集的数据转换管道包括缺失值插值和具有预定义限制的回填/前填插补。
我的问题是：何时何地将此管道应用于训练数据：在整个数据集上还是在每个切片上，将使用滑动窗口给出？
由于在推理时我将实时逐帧获取姿势数据，直到收集到 window_size 长度的切片，我应该在此窗口上应用上述管道吗？
实际上，我并没有尝试同时应用这两种方法。我想就预处理此类时间序列数据征求一些建议。]]></description>
      <guid>https://stackoverflow.com/questions/79066893/how-to-apply-data-transformation-pipeline-on-unseen-data-on-task-of-time-series</guid>
      <pubDate>Tue, 08 Oct 2024 16:15:50 GMT</pubDate>
    </item>
    <item>
      <title>VS 2022 ML.NET 教程 - 训练永不停止</title>
      <link>https://stackoverflow.com/questions/79066874/vs-2022-ml-net-tutorial-training-never-stops</link>
      <description><![CDATA[我第一次尝试使用 ML。我尝试从简单开始，ML 会在 3 个不同的情境中告诉我退款了多少钱。
我选择的“场景”是“问答”

我的“环境”是“本地 (GPU)”使用符合 CUDA 标准的 GPU

我认为我已经创建了具有正确列标题的文件，并正确计算了 AnswerIndex。
请参见下面的屏幕截图。

我有一台 Intel i7-10700F 2.90Ghz 处理器，配备 32GB RAM 和 NVIDIA Ge|Force GTX 1080 配备最新驱动程序。
当“训练”进行时，我从未看到 Visual Studio 的 GPU 百分比超过 4%
我还确保安装了最新版本的 VS 扩展。
我的问题是训练永远不会结束，即使经过数小时的训练也是如此。
任何关于我可能做错的事情的指示都将不胜感激。或者训练需要几个小时？]]></description>
      <guid>https://stackoverflow.com/questions/79066874/vs-2022-ml-net-tutorial-training-never-stops</guid>
      <pubDate>Tue, 08 Oct 2024 16:10:36 GMT</pubDate>
    </item>
    <item>
      <title>将 ML 模型从一个 Azure Databricks 工作区复制到另一个 Databricks 工作区</title>
      <link>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</link>
      <description><![CDATA[我运行了以下代码以在基于 Azure Databricks 的 mlflow 中导出 ML 模型，但我似乎收到了此错误
MLflow 主机或令牌配置不正确

我无法弄清楚问题是什么。工作区的 URL 和 PAT 令牌都是正确的。
export_import 工具有很多错误。它需要 mlfow 库，但 Databricks ML Runtime 附带的是 mlflow-skinny。
import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# 使用工作区 URL 设置 Databricks MLflow 跟踪 URI
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# 设置两个令牌以实现兼容性
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# 初始化 MLflow 客户端（无需传递跟踪 URI，因为它是全局设置的）
mlflow_client = MlflowClient()

# 使用 MLflow 客户端初始化 ModelExporter
exporter = ModelExporter(mlflow_client)

# 导出模型
exporter.export_model(
model_name=&quot;Signature_Test&quot;,
output_dir=&quot;/tmp/mlflow_export/model&quot;,
stage=None, # 使用&quot;None&quot; 导出所有阶段，或指定&quot;Staging&quot; 或&quot;Production&quot;
export_metadata_tags=True
)
]]></description>
      <guid>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</guid>
      <pubDate>Tue, 08 Oct 2024 08:39:33 GMT</pubDate>
    </item>
    <item>
      <title>无法加载 Arcface 模型</title>
      <link>https://stackoverflow.com/questions/79063234/arcface-model-cant-be-loaded</link>
      <description><![CDATA[我正在开发人脸识别系统，其中我使用 arcface 作为其算法，我从 github 下载了 arcface 模型，链接为 https://github.com/Martlgap/livefaceidapp/blob/main/model.onnx%5C。但我遇到了这个错误：
ERROR:root:发生错误：KerasTensor 不能用作 
TensorFlow 函数的输入。KerasTensor 是形状和 dtype 的符号占位符，
用于构建 Keras 函数模型或 Keras 函数。您只能
将其用作 Keras 层或 Keras 操作的输入（来自命名空间 
`keras.layers` 和 `keras.operations`）。您可能正在执行类似以下操作：

x = Input(...)
...
tf_fn(x) # 无效。

您应该做的是将 `tf_fn` 包装在一个层中：

class MyLayer(Layer):
def call(self, x):
return tf_fn(x)

x = MyLayer()(x)

我已经向 ai 或 gpt 询问了这个问题，它提到了 tensorflow 库的兼容性，但是当我在 github 上看到要求时，没有需要特定版本的 tensorflow。我也已经检查了模型是否在新的 py 文件上运行良好，并且它运行良好，我使用 model.onnx 生成嵌入，它也运行良好。
我的人脸识别脚本发生了什么，如何解决？
以防你们想知道我是如何初始化 arcface 模型的，这里是
import cv2
import os
import time
import logs
import numpy as np
import pandas as pd
from tkinter import font as tkFont
from PIL import Image, ImageTk
import torch
from arcface import ArcFace 
import psycopg2
import datetime
import face_alignment 
import onnxruntime
from retinaface import RetinaFace

# ArcFace model
arcface_model = onnxruntime.InferenceSession(r&quot;RetinaFace_ArcFace\model.onnx&quot;)

DATABASE_CONFIG = {
&#39;database&#39;: &#39;postgres&#39;, 
&#39;user&#39;: &#39;postgres&#39;, 
&#39;password&#39;: &#39;root&#39;, 
&#39;host&#39;: &#39;localhost&#39;, 
&#39;port&#39;: 5432 
}

class Face_Recognizer:
def __init__(self):
self.font = cv2.FONT_HERSHEY_SIMPLEX
# FPS
self.frame_time = 0
self.frame_start_time = 0
self.fps = 0
self.fps_show = 0
self.start_time = time.time()

# cnt for frame
self.frame_cnt = 0

# 将人脸特征保存到数据库中
self.face_features_known_list = []
# / 在数据库中保存人脸名称
self.face_name_known_list = []

# 列表保存第 N-1 帧和第 N 帧中 ROI 的质心位置
self.last_frame_face_centroid_list = None
self.current_frame_face_centroid_list = None

# 列表保存第 N-1 帧和第 N 帧中物体的名称
self.last_frame_face_name_list = [None]
self.current_frame_face_name_list = None

# 第 N-1 帧和第 N 帧中人脸的 cnt
self.last_frame_face_cnt = 0
self.current_frame_face_cnt = 0

# 保存识别时 faceX 的 e-distance
self.current_frame_face_X_e_distance_list = []

# 保存当前捕获的人脸位置和名称
self.current_frame_face_position_list = []
# 保存当前帧中人物的特征
self.current_frame_face_feature_list = []

# 上一帧和当前帧中 ROI 质心之间的 e 距离
self.last_current_frame_centroid_e_distance = 0

# 在“reclassify_interval”帧后重新分类
self.reclassify_interval_cnt = 0
self.reclassify_interval = 10

# 边界框持久性
self.bbox_history = {} 
self.bbox_persistence_frames = 12

# 连接到数据库
self.connect_to_db()

def get_face_database(self):
if os.path.exists(&quot;data/features_arcface.csv&quot;):
path_features_known_csv = &quot;data/features_arcface.csv&quot;
csv_rd = pd.read_csv(path_features_known_csv, header=None)
for i in range(csv_rd.shape[0]):
features_someone_arr = []
self.face_name_known_list.append(csv_rd.iloc[i][0])
for j in range(1, 513):
features_someone_arr.append(float(csv_rd.iloc[i][j]))
self.face_features_known_list.append(features_someone_arr)
login.info(&quot;数据库中的人脸： %d&quot;, len(self.face_features_known_list))
return 1
else:
login.warning(&quot;未找到&#39;features_arcface.csv&#39;！&quot;)
login.warning(&quot;先运行代码生成人脸特征！&quot;)
返回 0
]]></description>
      <guid>https://stackoverflow.com/questions/79063234/arcface-model-cant-be-loaded</guid>
      <pubDate>Mon, 07 Oct 2024 18:34:59 GMT</pubDate>
    </item>
    <item>
      <title>执行 3D U-net 时，每次执行都会得到截然不同的指标，有时准确率、召回率、DICE 和 IoU 的指标都会 >99.99%</title>
      <link>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</guid>
      <pubDate>Mon, 07 Oct 2024 15:02:22 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“未知的图像文件格式。需要 JPEG、PNG、GIF、BMP 之一”？[关闭]</title>
      <link>https://stackoverflow.com/questions/79060211/how-to-resolve-unknown-image-file-format-one-of-jpeg-png-gif-bmp-required</link>
      <description><![CDATA[我正在构建一个 U-Net 模型来检测乳腺癌，我从这里获取了数据集：https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
尽管所有图像都是 png 格式，但在尝试训练我的模型时，会出现错误，指出我的图像格式不正确。
错误如下：
---------------------------------------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
Cell In[51]，第 7 行
5 train_dataset = image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
6 print(image_ds.element_spec)
----&gt; 7 model_history = unet.fit(train_dataset, epochs=EPOCHS)

文件 ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在 quick_execute(op_name, num_outputs, input, attrs, ctx, name) 中
51 尝试：
52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
54 输入、属性、输出)
55 除 core._NotOkStatusException 外，因为 e:
56 如果名称不为 None:

InvalidArgumentError：图形执行错误：

在节点处检测到，decode_image/DecodeImage 定义在（最近一次调用最后一次）：
&lt;堆栈跟踪不可用&gt;
传递给 MapDataset:3 转换的用户定义函数中的错误，迭代器：Iterator::Root::Prefetch::BatchV2::Shuffle::MemoryCacheImpl::Filter::ParallelMapV2：未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一。
[[{{node decrypt_image/DecodeImage}}]]
[[IteratorGetNext]] [Op:__inference_one_step_on_iterator_9520]

错误仅在尝试训练模型时发生，它引用此函数：
def preprocess_image(image, mask, target_size=(256, 256)):
try:
# 安全地解码图像和掩码
image = tf.io.decode_image(image, channels=3, expand_animations=False)
mask = tf.io.decode_image(mask, channels=1, expand_animations=False)

# 检查未定义或零维度
if image.shape is None or image.shape[0] == 0 or image.shape[1] == 0:
print(f&quot;Error: Image has undefined or zero Dimensions: {image.shape}&quot;)
return None, None

if mask.shape is None or mask.shape[0] == 0 or mask.shape[1] == 0:
print(f&quot;Error: Mask 具有未定义或零维度：{mask.shape}&quot;)
return None, None

# 确保图像恰好有 3 个通道 (RGB)
if image.shape[-1] != 3:
print(f&quot;Error: Image does not have 3 channels (found {image.shape[-1]}).&quot;)
return None, None

# 将图像标准化为范围 [0, 1]
image = tf.image.convert_image_dtype(image, tf.float32)

# 将图像和 mask 的大小调整为目标尺寸 (256*256)
image = tf.image.resize(image, target_size, method=&#39;nearest&#39;)
mask = tf.image.resize(mask, target_size, method=&#39;nearest&#39;)

#将 mask 转换为二进制（0 或 1）格式以用于分类任务
mask = tf.cast(tf.math.reduce_max(mask, axis=-1, keepdims=True) &gt; 0, tf.float32) # 确保二进制 mask

return image, mask

except Exception as e:
print(f&quot;Error during preprocessing: {str(e)}&quot;)
return None, None

# 将预处理函数应用于数据集
image_ds = dataset.map(preprocess_image)

# 过滤掉 preprocess_image 返回的 None 值
image_ds = image_ds.filter(lambda img, mask: img is not None and mask is not None)

我尝试了多种方法尝试使用 chatgpt 修复此问题，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/79060211/how-to-resolve-unknown-image-file-format-one-of-jpeg-png-gif-bmp-required</guid>
      <pubDate>Sun, 06 Oct 2024 22:45:18 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn StackingClassifier 非常慢且 CPU 使用率不一致</title>
      <link>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</link>
      <description><![CDATA[我最近一直在尝试使用 sklearn 中的 StackingClassifier 和 StackingRegressor，但我注意到它总是很慢，并且 CPU 使用效率低下。假设（仅出于此示例的目的）我想使用 StackingClassifier 堆叠随机森林和 lightgbm，同时使用 lightgbm 作为最终分类器。在这种情况下，我预计运行 StackingClassifier 所需的时间大致等于运行单个随机森林所需的时间 + 运行 2 个单独的 lightgbm 所需的时间 + 一些小的余量（所以基本上是各部分的总和 + 训练 StackingClassifier 本身的时间 + 小的余量），但在实践中似乎需要几倍的时间。示例：
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
import lightgbm as ltb
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

X,y = load_iris(return_X_y=True)
cv = StratifiedKFold(n_splits=10)
lgbm = ltb.LGBMClassifier(n_jobs=4)
rf = RandomForestClassifier()

首先是 LightGBM，按照实际时间计算，在我的计算机上大约需要 140 毫秒：
%%time
scores = cross_val_score(lgbm, X, y,评分=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores)

这只是一个随机森林，对我来说大约需要 220 毫秒：
%%time
scores = cross_val_score(rf, X, y, 评分=&#39;accuracy&#39;, cv=cv, n_jobs=-1, error_score=&#39;raise&#39;)
np.mean(scores)

现在有一个将这两者结合起来的 StackingClassifier。由于它基本上运行了上述两个代码块 + 另一轮 lightgbm，我预计它大约需要 250+120+120=490 毫秒，但实际上需要大约 3000 毫秒，超过 6 倍：
%%time
estimators = [
(&#39;rf&#39;, rf),
(&#39;lgbm,&#39;, lgbm)
]

clf = StackingClassifier(
estimators=estimators, final_estimator=lgbm, passthrough=True)

scores = cross_val_score(clf, X, y,scoring=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores) 

我还注意到（在更大的数据集上运行完全相同的代码时，我需要足够长的时间才能监控我的 CPU 使用率），而 StackingClassifier 的 CPU 使用率则到处都是。
例如，运行单个 lightgbm 的 CPU 使用率：
运行单个 lightgbm 的 CPU 使用率
（基本上始终为 100%，因此 CPU 使用效率很高）
将 lightgbm 作为 stackingclassifier 运行时的 CPU 使用率
（到处都是，通常远没有接近 100%）
我做错了什么导致 StackingClassifier 比各部分的总和慢这么多吗？]]></description>
      <guid>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</guid>
      <pubDate>Sun, 17 Jul 2022 15:44:30 GMT</pubDate>
    </item>
    <item>
      <title>使用哪个 Python 库来对调查数据进行定性分析？[关闭]</title>
      <link>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</link>
      <description><![CDATA[我有一个数据集，其中包含大约 300 人填写的问卷。该问卷涉及公共交通中的用户体验和行为。我们对 3 家公交公司进行了调查。大多数问题都是“是/否”、“3 家公司中最好的”或“3 家公司中最差的”。
如果可能的话，我想建立一个模型，根据答案推荐三家公司中最好的一家。问题包括“公交车的可用性、公交车的可靠性、用户的偏好和公交车的物理维护”。
我希望模型能够分析数据集并返回最好的公交公司，该公司将很容易获得、干净且维护良好、可靠并且用户会更喜欢它。
此外，诸如“您喜欢哪辆公交车？”之类的问题的答案应该在决策中占有更大的权重。
我对机器学习还很陌生，希望有人能建议从哪种算法开始训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</guid>
      <pubDate>Wed, 01 Apr 2020 09:39:52 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中用列的平均值填补缺失值</title>
      <link>https://stackoverflow.com/questions/60363476/impute-missing-values-with-mean-of-column-in-machine-learning</link>
      <description><![CDATA[我知道，估算缺失值就是字面意思，我说的是用列的平均值估算缺失值。我通常在将数据拆分为训练和测试之前估算缺失值，但后来我看到了这个QnA，上面写着 

注意：如果您想将其用于机器学习/数据科学：从数据科学的角度来看，首先替换 NA 然后拆分为训练和测试是错误的……您必须首先拆分为训练和测试，然后用训练中的平均值替换 NA，然后将这个有状态的预处理模型应用于测试，请参阅下面涉及 sklearn 的答案！– Fabian Werner 2019 年 8 月 28 日 9:18

这是什么意思？我们能做到吗？我们怎么做？在分割数据之前或之后做这件事有什么不同吗？如果有，为什么？请帮我理解，因为我对这件事很困惑。]]></description>
      <guid>https://stackoverflow.com/questions/60363476/impute-missing-values-with-mean-of-column-in-machine-learning</guid>
      <pubDate>Sun, 23 Feb 2020 14:58:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 GridSearchCV 和 sklearn Pipeline 将训练数据的估算值代入测试数据</title>
      <link>https://stackoverflow.com/questions/53900675/how-to-impute-test-data-with-imputed-values-of-training-data-with-gridsearchcv-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/53900675/how-to-impute-test-data-with-imputed-values-of-training-data-with-gridsearchcv-a</guid>
      <pubDate>Sun, 23 Dec 2018 02:05:23 GMT</pubDate>
    </item>
    <item>
      <title>基于不同场景的输入和机器学习查询</title>
      <link>https://stackoverflow.com/questions/52772655/different-scenario-based-queries-on-imputing-and-machine-learning</link>
      <description><![CDATA[我正在学习插补和模型训练。以下是我在训练数据集时遇到的几个问题。请提供答案。

假设我有一个包含 1000 个观测值的数据集。现在我要一次性在完整数据集上训练模型。我的另一种方法是，我将数据集分为 80% 和 20%，先在 80% 的数据上训练我的模型，然后在 20% 的数据上训练我的模型。这相同还是不同？基本上，如果我在新数据上训练我已经训练过的模型，这意味着什么？

插补相关

另一个问题与插补有关。假设我有一些船上乘客的数据集，其中只有头等舱乘客被分配舱位。有一列包含舱位号（分类），但很少有观测值有这些舱位号。现在我知道这个列很重要，所以我不能删除它，因为它有很多缺失值，所以大多数算法都不起作用。如何处理这种类型的列的插补？

在插补验证数据时，我们是否使用与插补训练数据相同的值进行插补，或者插补值是否再次从验证数据本身计算得出？

如何以字符串的形式插补数据，例如机票号（如 A-123）。该列很重要，因为第一个字母表示乘客的等级。因此，我们不能删除它。

]]></description>
      <guid>https://stackoverflow.com/questions/52772655/different-scenario-based-queries-on-imputing-and-machine-learning</guid>
      <pubDate>Fri, 12 Oct 2018 05:11:09 GMT</pubDate>
    </item>
    <item>
      <title>如何将机器学习分类方法应用于一维时间序列数据？</title>
      <link>https://stackoverflow.com/questions/50519856/how-do-i-apply-machine-learning-classification-methods-to-1d-time-series-data</link>
      <description><![CDATA[我在各种锻炼（深蹲、俯卧撑、仰卧起坐、波比跳）过程中获得了 IMU 数据（加速度计、磁力计和陀螺仪）。这些锻炼是在单个 1D 时间序列信号中完成的，我想使用机器学习分类方法来识别信号中的不同锻炼。我不想将信号压缩为 0D 峰值并以此方式构建我的特征，而是保持时间域完整。下图显示了包含四种锻炼的加速度计的示例数据。
因此，我的问题是 - 哪种方法最有效？ K-means 聚类在 0D 意义上是完美的，那么是否有 1D 等效物？
]]></description>
      <guid>https://stackoverflow.com/questions/50519856/how-do-i-apply-machine-learning-classification-methods-to-1d-time-series-data</guid>
      <pubDate>Fri, 25 May 2018 00:18:01 GMT</pubDate>
    </item>
    <item>
      <title>局部回归和局部似然方法的实现[关闭]</title>
      <link>https://stackoverflow.com/questions/14411627/implementations-of-local-regression-and-local-likelihood-methods</link>
      <description><![CDATA[我正在寻找局部回归 (LOESS) 和局部似然方法（如局部逻辑回归）的有效实现（例如，在 Hastie 等人撰写的 统计学习要素第 6.5 节中讨论了局部似然方法）。
我更喜欢 C++ 或 Python 实现，但指向 R（我知道 LOESS 已实现，但我找不到局部似然方法）或 Java 的指针也足够了。]]></description>
      <guid>https://stackoverflow.com/questions/14411627/implementations-of-local-regression-and-local-likelihood-methods</guid>
      <pubDate>Sat, 19 Jan 2013 06:00:58 GMT</pubDate>
    </item>
    </channel>
</rss>