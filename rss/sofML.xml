<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 14 Jul 2024 12:38:20 GMT</lastBuildDate>
    <item>
      <title>多线程 TFRecord 写入在 kaggle 笔记本上突然停止</title>
      <link>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</link>
      <description><![CDATA[我正在寻求有关在 Kaggle 笔记本中 TFRecord 写入的多线程方面的帮助。我目前正在研究 VGGFace2 数据集，旨在将图像对转换为 TFRecords，用于训练、验证和测试集，但该过程在单线程上运行速度过慢。
TFRecord 文件包含此配对图像的 protobuf 示例，以表示同一个人和不同的人。
挑战和我尝试过的方法：

单线程处理速度慢：即使是处理数据集的有限子集（每个目录 5 个图像对用于训练，每个目录 2 个图像对用于验证/测试），使用单线程也需要大量时间（可能长达 24 小时）。我已尽可能优化代码，因此我开始探索多线程以提高性能。

多线程问题：当我使用 concurrent.futures.ThreadPoolExecutor 实现多线程时，会话突然停止，没有任何错误消息。此外，所有变量都丢失，需要从头开始完全重新启动。有趣的是，使用单个目录时，多线程可以完美运行（因为我实现的多线程是同时处理不同的目录，所以处理单个目录不再是多线程，而是单线程进程，即使多线程池执行器对象已关闭），但即使使用两个目录也会导致与上述相同的问题（VGGFace2 大约有 8631 个目录）。


我的问题：

潜在原因：这些多线程问题背后的原因可能是什么？这是 Kaggle 资源的内存限制吗？

替代方法：其他人是否遇到过类似的挑战？在 Kaggle 环境中，是否有其他方法可以加速 TFRecord 写入？


附加说明：

我正在使用 contextlib.ExitStack 库来打开许多写入器并同时写入。

我在 Kaggle 讨论和 Stack Overflow 上广泛搜索解决方案，但没有找到针对这种情况的具体解决方案。


如果您对 Kaggle 笔记本中的多线程有任何见解或经验，我将不胜感激，特别是在处理 VGGFace2 等数据集时。提前感谢您的支持！]]></description>
      <guid>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</guid>
      <pubDate>Sun, 14 Jul 2024 11:10:50 GMT</pubDate>
    </item>
    <item>
      <title>教导人工智能展现简单情绪以对抗孤独 [关闭]</title>
      <link>https://stackoverflow.com/questions/78745027/teaching-an-ai-to-show-simple-emotions-to-fight-loneliness</link>
      <description><![CDATA[我计划开发一种尽可能自然的计算机宠物。我认为神经网络可以很好地用于此，因为它们可以不断适应用户，并且不需要任何硬编码规则。
该程序应该记录用户的脸部，也许还会向 KI 传输一些额外的状态详细信息。
我在考虑一种 AI，它在最后选择某个状态，然后将其作为图像显示给用户。我的问题是我不知道从哪里获取
（对于监督学习）我应该从哪里获取训练数据或
（对于强化学习）我应该如何制作评估函数。
也许无监督学习也适用于这种情况。
最后，我想把整个东西喂给一个可爱的机器人，然后它只会让用户感觉到有人在那里感知你，所以除了一个有情绪反应的人工智能之外，我对其他任何东西都不感兴趣。
我在这个领域真的不太了解，所以另一个非常基本的问题：我也听说过一些关于“情绪-BICA”的事情，这可以用吗？或者首先应该如何设计一个神经、情感网络？
在我的研究过程中，我遇到了莫夫林和其他一些人工宠物。但是，我看不出它们是如何工作的。
最后，我只想再说一遍，我只想学习一种构建对用户做出反应的人工智能的方法，而不是真正的宠物，我只是希望它比简单的算法更好。我还相信，当你让人工智能做它的事情时，它会特别强大，当你给它一些元规则时，它就会发展出复杂的行为。]]></description>
      <guid>https://stackoverflow.com/questions/78745027/teaching-an-ai-to-show-simple-emotions-to-fight-loneliness</guid>
      <pubDate>Sat, 13 Jul 2024 21:32:19 GMT</pubDate>
    </item>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我是这个领域的新手，所以也许我误解了一些东西。但是，我认为您可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中对 NN 的输出求导</title>
      <link>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</link>
      <description><![CDATA[我正在尝试使用 PyTorch 中的 NN 构建 ODE 求解器，模型的一部分涉及对模型输出相对于输入求导。
我研究过求解标量值函数的情况。在这种情况下，我的 NN 有 1 个输入节点和 1 个输出节点。根据一些论文，我使用
dy_dt = torch.autograd.grad(y,t, torch.ones_like(y), create_graph=True)[0]
来获得有效的梯度。因此，当我有一个形状为 [N,1] 的张量的训练集时，我得到的结果导数具有形状 [N,1]，这是有道理的。但是，当我尝试求解平面方程时遇到了问题。现在我的 NN 有 1 个输入节点和 2 个输出节点。当我输入一个形状为 [N,1] 的张量时，我从 NN 中得到了一个形状为 [N,2] 的张量，这是有道理的。然而，得到的 dy_dt 的形状为 [N,1]，而它应该是 [N,2]。我还尝试使用
dy_dt = torch.autograd. functional.jacobian(model, t)
它返回一个形状为 [N,2,N,1] 的张量，我认为可以从中访问正确的导数，尽管它不那么简单。我想知道是否有办法使用 autograd.grad 来获得正确的导数。也许它与 grad_output 参数有关？]]></description>
      <guid>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</guid>
      <pubDate>Sat, 13 Jul 2024 16:01:15 GMT</pubDate>
    </item>
    <item>
      <title>无法在 databricks 中运行 Pysparkling</title>
      <link>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</link>
      <description><![CDATA[!pip install h2o_pysparkling_3.5
from pysparkling import H2OConf,H2OContext
hc = H2OContext.getOrCreate()

我收到以下错误
IllegalArgumentException：不支持的参数：（spark.speculation，true）

我尝试了 spark.conf.set(&quot;spark.speculation&quot;, &quot;false&quot;)，但出现了以下错误
[CANNOT_MODIFY_CONFIG] 无法修改 Spark 配置的值：
&quot;spark.speculation&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</guid>
      <pubDate>Sat, 13 Jul 2024 14:02:12 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在 winform 应用程序中使用 YoloV8 .Net Framework 4.7？</title>
      <link>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</link>
      <description><![CDATA[我有一个基于 .net framework 4.7 的 c# winform 项目。
我想知道是否有办法在 winform 应用程序（.net framework 4.7）中使用 Yolov8。
每当我将引用添加到我的项目时，我都会收到以下错误：

YoloV8 使用 system.runtime 版本 6.0，高于 system.runtime 4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</guid>
      <pubDate>Sat, 13 Jul 2024 07:33:51 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 指标显示“TypeError：‘property’对象不可迭代”</title>
      <link>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</link>
      <description><![CDATA[我正在建立一个 ANN 模型。当我运行以下代码时，它显示为
TypeError: &#39;property&#39; 对象不可迭代

如何修复此问题？
代码：
model=Sequential()
model.add(Dense(512,activation=tf.nn.relu))
model.add(Dense(256,activation=tf.nn.tanh))
model.add(Dense(128,activation=tf.nn.relu))
model.add(Dense(7))

# # 拟合模型

loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
accuracy=tf.keras.metrics.SparseCategoricalAccuracy
optimizer=tf.keras.optimizers.Adam()

model.compile(loss=loss,优化器=优化器，指标=[准确率])
history=model.fit(xtrain, ytrain, validation_data=(xval, yval), batch_size=64, epochs=100)
]]></description>
      <guid>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</guid>
      <pubDate>Fri, 12 Jul 2024 18:44:28 GMT</pubDate>
    </item>
    <item>
      <title>通过向 CNN 输入添加位置和字符信息来增强文档布局分析</title>
      <link>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</link>
      <description><![CDATA[我正在研究文档布局分析，并一直在探索 CNN 和基于 Transformer 的网络来完成这项任务。通常，图像作为 3 通道 RGB 输入传递给这些网络。但是，我的数据源是 PDF 格式，我可以直接从中提取准确的位置和字符信息。
我担心将这些 PDF 数据转换为图像进行分析会导致宝贵的位置和字符信息丢失。我的想法是将 CNN 的输入维度从标准的 3 RGB 通道修改为包含这些额外位置和字符信息的更高维度输入。
我了解 CNN 的工作原理，并高度怀疑这种方法可能行不通，但我很感谢社区的任何反馈或建议。有没有人尝试过以这种方式增强输入通道，或者有没有人对将位置和字符数据直接集成到 CNN 中有什么见解？]]></description>
      <guid>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</guid>
      <pubDate>Fri, 12 Jul 2024 10:17:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 识别代码文件中的错误来源[关闭]</title>
      <link>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</link>
      <description><![CDATA[我正在尝试实现一个小工具，它可以自动识别一组代码文件（作为输入）中的哪一部分代码导致了执行期间显示的错误文本。
错误可能是语法错误，也可能是逻辑错误。我还在考虑利用 llms 的 api 调用来更正代码。
据我所知，RAG 是必要的，因为我不可能将所有代码文件的数据都放入提示中，因为它肯定会超出上下文窗口的大小。
哪种类型的 RAG 实现最有用？我想尽可能减少响应延迟？]]></description>
      <guid>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</guid>
      <pubDate>Fri, 12 Jul 2024 06:41:48 GMT</pubDate>
    </item>
    <item>
      <title>BART 配备所有虚拟功能</title>
      <link>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</link>
      <description><![CDATA[我正尝试将 BART 应用于分类问题，其中预测变量是虚拟变量以及 y 变量。我知道这是一种不常见的设置，但不幸的是这就是设置。实际上，0 和 1 值是从 -4 到 4 的分类变量中获得的，将负值设置为 0，将正值添加到 1。我还有数据的分类版本，以防它有用。
现在，我的预测变量包含大量 NA 值（即 70%），由 648x48 的 0-1 虚拟变量矩阵组成。我的 y 变量不包含缺失值，有 648 个值。
我目前正在使用 RStudio 在 R 中工作。然而，当我执行下面的代码时，结果却令人失望：
bart_machine = build_bart_machine(predictors, response_var,use_missing_data = TRUE, use_missing_data_dummies_as_covars = TRUE)
bart_machine$confusion_matrix

也就是说，我获得了一个 NULL 混淆矩阵，并且
bartMachine v1.3.4.1 用于回归

缺失数据功能开启
训练数据大小：n = 638 和 p = 96
在 8 个核心、50 棵树、250 个 burn-in 和 1000 个 post 上构建，耗时 5.8 秒。样本

事先对 y 的 sigsq 估计：0.016

老化后的平均 sigsq 估计：0.00314

样本内统计数据：
L1 = 9.91
L2 = 1.01
rmse = 0.04
Pseudo-Rsq = 0.9547
残差的 shapiro-wilk 正态性检验的 p-val：0

零均值噪声的 p-val：0.99451

现在我的问题是：

您是否认为我有太多 NA 值而无法执行 BART？

您认为我的设置至少应该产生一个混淆矩阵吗？

您认为数据的分类版本在这里可能更有帮助还是上述令人失望的结果是由于更深层次的原因吗？


编辑：我实际上已经进行了预测，但它们相当令人失望：
# 提取特征名称
feature_names &lt;- bart_machine[[&quot;training_data_features_with_missing_features&quot;]]

# 删除&quot;M_&quot;前缀
feature_names &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, feature_names)

# 使用重命名的特征名称更新 bart_machine 对象
bart_machine[[&quot;training_data_features_with_missing_features&quot;]] &lt;- feature_names

# 检查 bart_machine 训练数据中的特征名称
training_data &lt;- bart_machine[[&quot;model_matrix_training_data&quot;]]
training_feature_names &lt;- colnames(training_data)

# 删除 &quot;M_&quot;列名中的前缀
training_feature_names &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, training_feature_names)

# 使用重命名的列更新 bart_machine 对象
colnames(training_data) &lt;- training_feature_names
bart_machine[[&quot;model_matrix_training_data&quot;]] &lt;- training_data

# 使用相同方法在 albany2005_predictors 中插入缺失数据
imputed_data_2005 &lt;- mice(albany2005[, -1], m = 5, method = &#39;pmm&#39;, maxit = 50, seed = 500)
complete_data_2005 &lt;- complete(imputed_data_2005, 1)

# 删除 &quot;M_&quot; albany2005_predictors 中列名的前缀
colnames(complete_data_2005) &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, colnames(complete_data_2005))

# 确保 albany2005_predictors 具有与训练数据相同的列，但不包括“y_remaining”
required_cols &lt;- setdiff(training_feature_names, &quot;y_remaining&quot;)
albany2005_predictors &lt;- complete_data_2005[, required_cols, drop = FALSE]

# 为 albany2005_response 中的非 NA 值创建逻辑向量
non_na_indices &lt;- !is.na(albany2005_response)

# 子集预测值和albany2005_response 使用 non_na_indices
non_na_predicted_values &lt;- predict_values[non_na_indices]
non_na_actual_values &lt;- albany2005_response[non_na_indices]

# 使用 bartMachine 模型对 2005 年数据进行预测
predicted_values &lt;- predict(bart_machine, albany2005_predictors, type = &quot;class&quot;)

# 计算 RMSE
# 将预测值转换为数字
non_na_predicted_values &lt;- as.numeric(as.character(non_na_predicted_values))

# 将实际值转换为数字
non_na_actual_values &lt;- as.numeric(as.character(non_na_actual_values))

rmse &lt;- sqrt(mean((non_na_predicted_values - non_na_actual_values)^2))

# 打印 RMSE
print(paste(&quot;RMSE: &quot;, rmse))。###非常高的 RMSE!!!
]]></description>
      <guid>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</guid>
      <pubDate>Wed, 10 Jul 2024 16:18:19 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
以下函数的解释：

oscillator_system_data_loss：振荡器系统作为神经网络的实现，其中可学习参数 mu 传递给在 NN_osc_func 中实现的 ODE
train_NN_data_loss：自定义训练程序
plot_epochs_with_noise：与问题无关，但用于训练时监控

def rocksock_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

使用 tf.GradientTape() 作为 outer_tape:
outer_tape.watch(t)

使用 tf.GradientTape() 作为 inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 一阶导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

early_stop = 0

for itr in范围（epochs）：
使用 tf.GradientTape() 作为磁带：
train_loss，mu = 振荡器系统数据损失（train_t，NN，func，params，mu，bc，data_t，data_u_noised，lambda1）
train_loss_record.append（train_loss）

grad_w = 磁带。gradient（train_loss，NN.trainable_variables + [mu]）
optm.apply_gradients（zip（grad_w，NN.trainable_variables + [mu]））

如果 itr 在 loss_tracker 中：
print（train_loss.numpy()）
print（mu.numpy()）
plot_epochs_with_noise（train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN）

mu_list.append（mu.numpy()）

return train_loss_record, mu_list, early_stop

NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。有什么帮助吗？
我的想法是，要么训练更长时间，要么我可能会遇到 PINN 容易出现的一些高频问题。]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在 TensorFlow 中使用 model.fit() 时会得到 ValueError：无法识别的数据类型：x=[...] (类型 <class 'list'>)？</title>
      <link>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</link>
      <description><![CDATA[我尝试运行以下代码，该代码取自 CS50 的 AI 课程：
import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split

# 从文件读取数据
with open(&quot;banknotes.csv&quot;) as f:
reader = csv.reader(f)
next(reader)

data = []
for row in reader:
data.append(
{
&quot;evidence&quot;: [float(cell) for cell in row[:4]],
&quot;label&quot;: 1 if row[4] == &quot;0&quot; else 0,
}
)

#将数据分为训练组和测试组
evidence = [row[&quot;evidence&quot;] for row in data]
labels = [row[&quot;label&quot;] for row in data]
X_training, X_testing, y_training, y_testing = train_test_split(
evidence, labels, test_size=0.4
)

# 创建神经网络
model = tf.keras.models.Sequential()

# 添加一个有 8 个单元的隐藏层，使用 ReLU 激活函数
model.add(tf.keras.layers.Dense(8, input_shape=(4,),activation=&quot;relu&quot;))

# 添加一个有 1 个单元的输出层，使用 sigmoid 激活函数
model.add(tf.keras.layers.Dense(1,activation=&quot;sigmoid&quot;))

# 训练神经网络
model.compile(
optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)
model.fit(X_training, y_training, epochs=20)

# 评估模型的表现
model.evaluate(X_testing, y_testing, verbose=2)

但是，我收到以下错误：
Traceback（最近一次调用最后一次）：
文件“C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py”，第 41 行，位于&lt;module&gt;
model.fit(X_training, y_training, epochs=20)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py&quot;，第 113 行，位于 get_data_adapter 中
引发 ValueError(f&quot;无法识别的数据类型：x={x}（类型为 {type(x)}）&quot;)
ValueError：无法识别的数据类型：x=[...]（类型为 &lt;class &#39;list&#39;&gt;)

其中“...”是训练数据。
知道哪里出错了吗？我在 Windows 计算机上使用 Python 版本 3.11.8 和 TensorFlow 版本 2.16.1。
我尝试在 Google Colab 笔记本中运行相同的代码，并且成功了：问题仅发生在我的本地机器上。这是我期望的输出：
Epoch 1/20
26/26 [==============================] - 1s 2ms/step - 损失：1.1008 - 准确度：0.5055
Epoch 2/20
26/26 [===============================] - 0s 2ms/step - 损失：0.8588 - 准确度：0.5334
Epoch 3/20
26/26 [================================] - 0s 2ms/step - 损失：0.6946 - 准确度：0.5917
Epoch 4/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5970 - 准确度：0.6683
纪元 5/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5265 - 准确度：0.7120
纪元 6/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4717 - 准确度：0.7655
纪元 7/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4258 - 准确度：0.8177
纪元 8/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.3861 - 准确度：0.8433
纪元 9/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3521 - 准确度：0.8615
纪元 10/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.3226 - 准确度：0.8870
纪元 11/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2960 - 准确度：0.9028
纪元 12/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2722 - 准确度：0.9125
纪元 13/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.2506 - 准确度：0.9283
纪元 14/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2306 - 准确度：0.9514
纪元 15/20
26/26 [================================] - 0s 3ms/步 - 损失：0.2124 - 准确度：0.9660
纪元 16/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1961 - 准确度：0.9769
纪元 17/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.1813 - 准确度：0.9781
纪元 18/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1681 - 准确度：0.9793
纪元 19/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1562 - 准确度：0.9793
Epoch 20/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1452 - 准确度：0.9830
18/18 - 0s - 损失：0.1407 - 准确度：0.9891 - 187ms/epoch - 10ms/步
[0.14066053926944733, 0.9890710115432739]
]]></description>
      <guid>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</guid>
      <pubDate>Thu, 04 Apr 2024 00:28:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Textract 中获取 BLOCK 类型 LAYOUT_TITLE、LAYOUT_SECTION_HEADER 和 LAYOUT_xx 的内容</title>
      <link>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</link>
      <description><![CDATA[我正在尝试使用 textract 抓取多页 pdf。
需要抓取 pdf 并根据其部分、子部分、表格格式化为 json。
在尝试使用 LAYOUT 和 Table 进行 UI 演示时，它能够准确显示布局标题、布局部分、布局文本、布局页脚、页码
在从 UI 演示下载的 csv 文件中可以观察到相同的信息：layout.csv 文件。
在 json 文件中也是如此：analyzeDocResponse.json 也一样，但它包含所有内容（LINES、WORDS、LAYOUT_TITLE 和所有与布局相关的数据），我认为 textract 按顺序执行所有类型的块类型。
出于调试目的，我使用以下代码打印整个块字典。
以及块类型，后面跟着相应的文本。
如果对 pdf 文件感兴趣：其药物的 SmPC：SmPC 文件
代码 1：以 json 格式打印每个块。

def start_textract_job(bucket, document):
response = textract.start_document_analysis(
DocumentLocation={
&#39;S3Object&#39;: {
&#39;Bucket&#39;: bucket,
&#39;Name&#39;: document
}
},
FeatureTypes=[&quot;LAYOUT&quot;] # 您可以根据需要调整 FeatureTypes
)
return response[&#39;JobId&#39;]

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(json.dumps(block, indent=4))

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

它根据 UI Demo 打印类似信息，块类型 LINES、WORDS、LAYOUT_
但如果我尝试使用以下代码打印每种块类型的文本，它无法打印与 LAYOUT_ 相关的文本，不知道为什么，我是否遗漏了什么？
代码 2：打印块类型，然后打印其内容。

def start_textract_job 与上面的 LAYOUT 相同。

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(f&quot;{block[&#39;BlockType&#39;]}: {block.get(&#39;Text&#39;, &#39;&#39;)}&quot;)

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

我可以看到块类型 LINES、WORDS 的值
但 LAYOUT 为空，如下所示，我认为，它在块类型中识别，但不是其值。
LAYOUT_TITLE:
LAYOUT_FIGURE:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_PAGE_NUMBER:
LAYOUT_FOOTER:
任何帮助都非常感谢，我查阅了文档和其他一些 StackOverflow 问题，但找不到任何帮助。
Tetract 新手，抱歉，如果是新手，请提问：)]]></description>
      <guid>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</guid>
      <pubDate>Sun, 31 Mar 2024 19:28:51 GMT</pubDate>
    </item>
    <item>
      <title>NameError：名称“plot_confusion_matrix”未定义</title>
      <link>https://stackoverflow.com/questions/65651544/nameerror-name-plot-confusion-matrix-is-not-defined</link>
      <description><![CDATA[我正在尝试使用 VGG16 创建一个分类模型，但是在项目结束时，我在获取混淆矩阵时遇到了错误。下面给出了代码，
导入的包和模块是：
import os
import keras
import numpy as np
import tensorflow as tf
from keras.models import Model
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.applications import MobileNet
from sklearn.metrics import chaos_matrix
from keras.layers.core import Dense, Activation
from keras.metrics import categorical_crossentropy
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.mobilenet import preprocess_input
from tensorflow.keras.preprocessing import image_dataset_from_directory

注意：对于简短地讲我只是跳过了链接的数据集
下面定义 VGG16：
vgg16_model = keras.applications.vgg16.VGG16()
vgg16_model.summary()

现在，定义模型：
model = Sequential()
for layer in vgg16_model.layers:
model.add(layer)

for layer in model.layers:
layer.trainable = False

model.add(Dense(2,activation=&#39;softmax&#39;))

编译模型：
model.compile(Adam(lr=.0001),loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

拟合模型：
model.fit_generator(train_batches, steps_per_epoch=4, validation_data=valid_batches, validation_steps=4, epochs=10, verbose=2)

现在是混淆矩阵：
test_imgs, test_labels = next(test_batches)
plots(test_imgs, titles=test_labels)
test_labels = test_labels[:,0] 
predictions = model.predict_generator(test_batches, steps=1, verbose=0)
cm = chaos_matrix(test_labels, np.round(predictions[:,0]))

下面我遇到了一个错误，请关注下面代码，
cm_plot_labels = [&#39;diseaseAffectedEggplant&#39;,&#39;freshEggplant&#39;]
plot_confusion_matrix(cm, cm_plot_labels, title=&quot;Confusion Matrix&quot;) // 这行，我遇到了一个错误

错误如下，
-------------------------------------------------------------------------------
NameError Traceback (most recent call last)
&lt;ipython-input-28-43b96d543746&gt; in &lt;module&gt;()
1 cm_plot_labels = [&#39;diseaseAffectedEggplant&#39;,&#39;freshEggplant&#39;]
----&gt; 2 plot_confusion_matrix(cm, cm_plot_labels, title=&quot;Confusion Matrix&quot;)

NameError: 名称 &#39;plot_confusion_matrix&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/65651544/nameerror-name-plot-confusion-matrix-is-not-defined</guid>
      <pubDate>Sun, 10 Jan 2021 08:53:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 NLP Python 对文本进行多分类 - 总类别中 2 个类别的召回率相对较低</title>
      <link>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</link>
      <description><![CDATA[我拥有几乎平衡的数据集，包含 9 个独特类别，每个类别有近 2200 行，差异为 +/-100 行。为了创建模型，我使用了下面提到的 URL 方法，但在每种情况下，我的模型准确率都在 58% 左右，精确率/召回率也在 54% 左右。你能告诉我我做错了什么吗？
https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f
https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a
https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538
我的数据集只有 2 列，1 列为特征，1 列为标签。
from pandas import ExcelFile

df = pd.read_excel(&#39;Prediction.xlsx&#39;, 
sheet_name=&#39;Sheet1&#39;)
df.head()
BAD_SYMBOLS_RE = re.compile(&#39;[^0-9a-z #+_]&#39;)
STOPWORDS = set(stopwords.words(&#39;english&#39;))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
&quot;&quot;&quot;
text: 字符串

return: 修改后的初始字符串
&quot;&quot;&quot;
text = BeautifulSoup(text, &quot;html.parser&quot;).text # HTML 解码
text = text.lower() # 小写文本
text = REPLACE_BY_SPACE_RE.sub(&#39; &#39;, text) # 将文本中的 REPLACE_BY_SPACE_RE 符号替换为空格
text = BAD_SYMBOLS_RE.sub(&#39;&#39;, text) # 从文本中删除 BAD_SYMBOLS_RE 中的符号
text = &#39; &#39;.join(word for word in text.split() if word not in STOPWORDS) # 从文本中删除停用词
return text

df[&#39;notes_issuedesc&#39;] = df[&#39;notes_issuedesc&#39;].apply(clean_text)
print_plot(10)
df[&#39;notes_issuedesc&#39;].apply(lambda x: len(x.split(&#39; &#39;))).sum()
X = df.notes_issuedesc
y = df.final
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)
%%time
从 sklearn.naive_bayes 导入 MultinomialNB
从 sklearn.pipeline 导入 Pipeline
从 sklearn.feature_extraction.text 导入 TfidfTransformer

nb = Pipeline([(&#39;vect&#39;, CountVectorizer()),
(&#39;tfidf&#39;, TfidfTransformer()),
(&#39;clf&#39;, MultinomialNB()),
])
nb.fit(X_train, y_train)

来自 sklearn.metrics 导入分类报告
y_pred = nb.predict(X_test)

print(&#39;准确率 %s&#39; % 准确率得分(y_pred, y_test))
print(分类报告(y_test, y_pred,target_names=my_tags))
]]></description>
      <guid>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</guid>
      <pubDate>Fri, 17 Apr 2020 20:12:37 GMT</pubDate>
    </item>
    </channel>
</rss>