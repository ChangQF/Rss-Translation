<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 11 Jul 2024 06:21:21 GMT</lastBuildDate>
    <item>
      <title>如何准确计算Doctr ocr中检测到的文本的绝对bbox坐标？</title>
      <link>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</link>
      <description><![CDATA[我一直试图在文档的图片上绘制 bbox，并尝试使用 mindee-doctr 进行 ocr 以查看检测到的文本行。我面临的问题是，我通过乘以相对坐标和页面尺寸计算出的 bbox 的绝对坐标，在原始图像上绘制时都向右上角偏移。有没有办法纠正这个问题？
这是我计算 bbox 的代码：
# 使用 docTR 分析图像并获取结果
line_boundaries = []
model = ocr_predictor(pretrained=True) #设置preserve_aspect_ratio=False 或symmetric_pad=False 没有区别。
doc = DocumentFile.from_images(img_path)
result = model(doc)

# 提取每行的边界框坐标
for page in result.pages:
for block in page.blocks:
for line in block.lines:
# 将相对坐标与页面尺寸相乘，得到绝对坐标
x_min, y_min, x_max, y_max = round(line.geometry[0][0] * page.dimensions[0]), round(line.geometry[0][1] * page.dimensions[1]), round(line.geometry[1][0] * page.dimensions[0]), round(line.geometry[1][1] * page.dimensions[1])
line_boundaries.append((x_min, y_min, x_max, y_max))

这是 line_boundaries 的值：
[(531, 148, 1321, 184), (2725, 148, 3061, 177), (526, 254, 3071, 295), (526, 288, 3071, 332), (535, 324, 3071, 363), ... ]
这是我用来绘制方框的函数：
def draw_rectangles(image_path, line_boundaries):
&quot;&quot;&quot;
使用提供的线边界在图像上绘制矩形。

参数：
image_path：图像文件的路径。
line_boundaries：线边界列表，其中每个边界都是四个点的列表。

返回：
无
&quot;&quot;&quot;

# 加载图像
image = cv2.imread(img_path)
#image = DocumentFile.from_images(img_path)[0] #本质上等同于 cv2.imread()

# 遍历线边界并绘制矩形
for bounding in line_boundaries:
#x1, y1, x2, y2 = int(boundary[0][0]), int(boundary[0][1]), int(boundary[2][0]), int(boundary[2][1])
x1, y1, x2, y2 = map(int, bounding) # 将坐标转换为整数
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# 用矩形显示图像
cv2_imshow(image) # 仅在协作时使用 cv2_imshow 代替 cv2.imshow
cv2.waitKey(0)
cv2.destroyAllWindows()

这是绘制了 bbox 的图像。
使用 bbox 进行 doctr 检测
我尝试过不使用舍入，但没有任何区别，也尝试过仅使用预测器，但无济于事。在 ocr_predictor 中设置preserve_aspect_ratio=False 或symmetric_pad=False 也没有区别。]]></description>
      <guid>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</guid>
      <pubDate>Thu, 11 Jul 2024 05:38:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 benchmark 进行 scikit-learn 运行时性能测试分析</title>
      <link>https://stackoverflow.com/questions/78733634/how-to-use-benchmark-for-scikit-learn-runtime-performance-test-analysis</link>
      <description><![CDATA[现在我已经构建了 scikit-learn，但我不知道如何使用基准测试来测试 DBSCAN、KMeans 等案例，以获取性能数据，例如“每批 CPU 挂机时间、CPU 挂机时间、第一批时间、CPU 峰值内存”。
我完全不知道如何测试这个。我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78733634/how-to-use-benchmark-for-scikit-learn-runtime-performance-test-analysis</guid>
      <pubDate>Thu, 11 Jul 2024 04:59:40 GMT</pubDate>
    </item>
    <item>
      <title>训练时间过长的时间序列数据的 LSTM 模型差分进化</title>
      <link>https://stackoverflow.com/questions/78733225/differential-evolution-on-lstm-model-for-time-series-data-taking-too-long-to-tra</link>
      <description><![CDATA[我正在使用 TensorFlow/Keras 中的 LSTM 进行时间序列预测项目，并尝试使用差分进化来优化我的模型的超参数。但是，即使只有 100 个观察值，训练过程也需要很长时间。
这是我的代码片段：
# 导入库
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from scipy.optimize import different_evolution

# 导入数据集
df = pd.read_csv(&quot;1 Yr Yields Series.CSV&quot;)
df.head()

# 删除非数字列和目标变量
features = df.drop(columns=[&#39;Unnamed: 0&#39;, &#39;Date&#39;, &#39;Nation&#39;, &#39;Yield&#39;])

# 存储收益率值
yields = df[&#39;Yield&#39;].values.reshape(-1, 1)

# 将 % 列转换为字符串
features[&#39;DivYield&#39;] = features[&#39;DivYield&#39;].astype(str)
features[&#39;DivYieldFTSE&#39;] = features[&#39;DivYieldFTSE&#39;].astype(str)

# 将 % 列转换为浮点数
features[&#39;DivYield&#39;] = features[&#39;DivYield&#39;].str.rstrip(&#39;%&#39;).astype(float) / 100
features[&#39;DivYieldFTSE&#39;] = features[&#39;DivYieldFTSE&#39;].str.rstrip(&#39;%&#39;).astype(float) / 100

# 缩放数据
scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)

# 创建 pca 实例以拟合标准化数据
pca = PCA()
X_pca = pca.fit_transform(features_standardized)

# 使用肘部方法找到要减少到的 k 个组件
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel(&#39;组件数量&#39;)
plt.ylabel(&#39;累积解释方差&#39;)
plt.show()

# 找到我们的主要原则成分
per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)
labels = [&#39;PC&#39; + str(x) for x in range(1, len(per_var)+1)]

plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)
plt.ylabel(&#39;% of explained variance&#39;)
plt.xlabel(&#39;Principal component&#39;)
plt.title(&#39;Scree Plot&#39;)
plt.show()

# 根据图使用最佳的主成分数量
k = 7
pca = PCA(n_components=k)
X_pca = pca.fit_transform(features_standardized) # 转换我们的数据

# 将收益与 PCA 转换的特征连接起来
pca_array = np.concatenate((yields, X_pca), axis=1)
pca_array.shape # 收益位于第 0 个索引

# 查找我们的初始权重
principal_components = pca.components_
weights = np.abs(principal_components) / np.sum(np.abs(principal_components), axis=1, keepdims=True)
W = tf.convert_to_tensor(weights, dtype=tf.float32) # 确保权重是 float32 张量

print(f&quot;Initial Weights:\n{weights}&quot;)

# 将数据拆分为训练集和验证集
X_train, X_val, y_train, y_val = train_test_split(pca_array[:100, 1:8], pca_array[:100, 0], test_size=0.2)

def create_lstm_model(units, dropout, learning_rate):
model = tf.keras.Sequential([
tf.keras.layers.LSTM(units, input_shape=(X_train.shape[1], X_train.shape[2])),
tf.keras.layers.Dropout(dropout),
tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
return model

def objective(params):
units, dropout, learning_rate = params
model = create_lstm_model(int(units), dropout, learning_rate)

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=0)

val_loss = min(history.history[&#39;val_loss&#39;])

return val_loss

bounds = [(10, 200), # 单位
(0.1, 0.5), # dropout
(1e-4, 1e-2)] # learning_rate

result = different_evolution(objective, bounds, strategies=&#39;best1bin&#39;, maxiter=100, popsize=15, tol=0.01)

# 打印最优参数和相应的分数
print(&quot;Optimal Parameters:&quot;, result.x)
print(&quot;Best Score:&quot;, result.fun)


我下载了 miniconda 并尝试运行我在 GPU 上测试了差分进化，看看它是否能提高速度，但没有成功。
问题：

有没有更有效的方法来执行超参数优化？

我的 LSTM 模型或我分割数据的方式是否会导致延迟？

是否有任何参数或方法可以调整以加快优化过程？

有没有什么方法可以可视化每一步的时间？


任何建议或意见都将不胜感激！
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78733225/differential-evolution-on-lstm-model-for-time-series-data-taking-too-long-to-tra</guid>
      <pubDate>Thu, 11 Jul 2024 01:34:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 Catboost Golang</title>
      <link>https://stackoverflow.com/questions/78732455/use-catboost-golang</link>
      <description><![CDATA[如何在 golang 上使用 CatBoost？也许有解决方案？
我尝试使用 https://github.com/bourbaki/catboost-go 但我一直收到 CGO 错误。
当我尝试使用 go run 运行它时，它会在编译时出错
usr/local/go/pkg/tool/linux_amd64/link：运行 gcc 失败：退出状态 1
/usr/bin/ld：/tmp/go-link-23536551/000001.o：在函数 _cgo_e59e54336bq_Cfunc&#39; 中：/tmp/go-build/cgo-gcc-prolog:69：对 my_function&#39; 的未定义引用
collect2：错误：ld 返回 1 退出状态]]></description>
      <guid>https://stackoverflow.com/questions/78732455/use-catboost-golang</guid>
      <pubDate>Wed, 10 Jul 2024 19:40:07 GMT</pubDate>
    </item>
    <item>
      <title>使用包含数组的特征训练随机森林模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78732000/training-random-forest-model-with-features-containing-arrays</link>
      <description><![CDATA[我正在处理一个包含时域 IQ 数据的数据集。我已将 FFT 应用于 IQ 数据，将其转换为频域，并进一步从中提取功率、幅度和相位角。这些特征中的每一行都包含一个由 1024 个数据点组成的数组。我正在尝试在此数据上训练随机森林分类器模型，但在拟合过程中遇到错误：
错误图像（抱歉图像模糊）：

所有特征都具有相同的形状和大小。
我尝试连接行、展平、转换为列表，但似乎没有任何效果。所以我决定使用这些特征的统计数字来训练模型。有没有办法直接传递 FFT 后获得的特征？这对模型来说会不会太复杂了？]]></description>
      <guid>https://stackoverflow.com/questions/78732000/training-random-forest-model-with-features-containing-arrays</guid>
      <pubDate>Wed, 10 Jul 2024 17:29:51 GMT</pubDate>
    </item>
    <item>
      <title>实时物体检测/跟踪的最佳模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78731761/best-model-for-object-detection-tracking-in-real-time</link>
      <description><![CDATA[我的目标是创建一个可以跟踪和计数物体以及检测物体的模型。
我看过关于 YOLO 版本 7、8 等的研究。
另一方面，我应该买什么样的显卡？
我考虑过 Jetson Nano。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78731761/best-model-for-object-detection-tracking-in-real-time</guid>
      <pubDate>Wed, 10 Jul 2024 16:32:14 GMT</pubDate>
    </item>
    <item>
      <title>BART 配备所有虚拟功能</title>
      <link>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</link>
      <description><![CDATA[我正尝试将 BART 应用于分类问题，其中预测变量是虚拟变量以及 y 变量。我知道这是一种不常见的设置，但不幸的是这就是设置。实际上，0 和 1 值是从 -4 到 4 的分类变量中获得的，将负值设置为 0，将正值添加到 1。我还有数据的分类版本，以防它有用。
现在，我的预测变量包含大量 NA 值（即 70%），由 648x48 的 0-1 虚拟变量矩阵组成。我的 y 变量不包含缺失值，有 648 个值。
我目前正在使用 RStudio 在 R 中工作。然而，当我执行下面的代码时，结果却令人失望：
bart_machine = build_bart_machine(predictors, response_var,use_missing_data = TRUE, use_missing_data_dummies_as_covars = TRUE)
bart_machine$confusion_matrix

也就是说，我获得了一个 NULL 混淆矩阵，并且
bartMachine v1.3.4.1 用于回归

缺失数据功能开启
训练数据大小：n = 638 和 p = 96
在 8 个核心、50 棵树、250 个 burn-in 和 1000 个 post 上构建，耗时 5.8 秒。样本

事先对 y 的 sigsq 估计：0.016

老化后的平均 sigsq 估计：0.00314

样本内统计数据：
L1 = 9.91
L2 = 1.01
rmse = 0.04
Pseudo-Rsq = 0.9547
残差的 shapiro-wilk 正态性检验的 p-val：0

零均值噪声的 p-val：0.99451

现在我的问题是：

您是否认为我有太多 NA 值而无法执行 BART？

您认为我的设置至少应该产生一个混淆矩阵吗？

您认为数据的分类版本在这里可能更有帮助还是上述令人失望的结果是否归因于更深层次的原因？

]]></description>
      <guid>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</guid>
      <pubDate>Wed, 10 Jul 2024 16:18:19 GMT</pubDate>
    </item>
    <item>
      <title>无法部署基于 Flask 的深度学习应用程序</title>
      <link>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</link>
      <description><![CDATA[我的 ml 应用程序基于 flask，它以 pdf 作为输入，让用户提问并利用 BERT 模型来回答，
到目前为止，它在本地运行良好，但在部署中没有运气，我尝试了 vercel、pythonanywhere，但没有任何效果，欢迎您为部署做出贡献
github repo https://github.com/MohdSiddiq12/Natural-Language-Question-Answering-System]]></description>
      <guid>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</guid>
      <pubDate>Wed, 10 Jul 2024 14:04:16 GMT</pubDate>
    </item>
    <item>
      <title>在 Android Studio 中集成已训练的模型</title>
      <link>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio</guid>
      <pubDate>Wed, 10 Jul 2024 11:31:14 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中的 nestedcv 包中的 nestcv.train 对象上调用 summary() 和 train_summary() 有什么区别？</title>
      <link>https://stackoverflow.com/questions/78729334/what-is-the-difference-between-calling-summary-and-train-summary-on-a-nestcv</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78729334/what-is-the-difference-between-calling-summary-and-train-summary-on-a-nestcv</guid>
      <pubDate>Wed, 10 Jul 2024 08:11:17 GMT</pubDate>
    </item>
    <item>
      <title>像 medium 或 daily dev 这样的网站如何选择您的兴趣并为您提供相关的信息，它们使用了哪些技术？[关闭]</title>
      <link>https://stackoverflow.com/questions/78729154/how-do-websites-like-medium-or-daily-dev-gets-to-select-your-interests-and-provi</link>
      <description><![CDATA[我想使用与 medium 和 daily dev 相同的功能，让您选择您感兴趣的领域，然后为您提供相关博客作为您的订阅源。我不需要使用所有这些功能，但我想知道如何实现这样的功能。我希望能够了解用户的兴趣和知识/专业知识领域以及需求，然后根据这些信息为他们提供相关信息。我猜想会涉及机器学习、人工智能等技术，如果我能设法了解创建此类功能所涉及的技术，这将对我大有帮助。
我曾尝试使用 chatGPT 来了解它，但答案有点太模糊了。似乎直接联系相关行业的开发人员会更好、更高效]]></description>
      <guid>https://stackoverflow.com/questions/78729154/how-do-websites-like-medium-or-daily-dev-gets-to-select-your-interests-and-provi</guid>
      <pubDate>Wed, 10 Jul 2024 07:30:23 GMT</pubDate>
    </item>
    <item>
      <title>加载在自定义数据集上训练的 Yolov9 模型：AttributeError：“str”对象没有属性“shape”</title>
      <link>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</link>
      <description><![CDATA[我已经在自定义数据集上训练了一个 yolov9 模型，用于实例分割，现在我想在分割后获得分割区域。
输出如下图所示，但针对图像中分割的每个对象。

from pathlib import Path
import numpy as np
import torch
import cv2

model = torch.hub.load(&#39;.&#39;, &#39;custom&#39;, path=&#39;yolov9-inst/runs/train-seg/gelan-c-seg15/weights/best.pt&#39;, source=&#39;local&#39;) 
# Image
img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
# 推理
res = model(img)

但是我在仅查找 res 时收到此错误。
YOLO 🚀 v0.1-104-g5b1ea9a Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (NVIDIA RTX A5000, 24248MiB)

融合层...
gelan-c-seg-custom 摘要：414 层，27364441 个参数，0 个梯度， 144.2 GFLOPs
警告 ⚠️ YOLO SegmentationModel 尚不兼容 AutoShape。您将无法使用此模型运行推理。
------------------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
Cell In[84]，第 6 行
4 img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
5 # 推理
----&gt; 6 结果 = model(img)

文件 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518，位于 Module._wrapped_call_impl(self, *args, **kwargs)
1516 返回 self._compiled_call_impl(*args, **kwargs) # 类型：ignore[misc]
1517 else:
-&gt; 1518 return self._call_impl(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
1522 # 如果我们没有任何钩子，我们希望跳过此函数中的其余逻辑
1523 # 并只调用 forward。
1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
1525 or _global_backward_pre_hooks or _global_backward_hooks
1526 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527 返回 forward_call(*args, **kwargs)
1529 尝试：
1530 结果 = 无

文件 /workspace/yolov9-inst/./models/common.py:868，在 DetectMultiBackend.forward(self, im, augment, visualize) 中
866 def forward(self, im, augment=False, visualize=False):
867 # YOLO MultiBackend 推理
--&gt; 868 b, ch, h, w = im.shape # 批次、通道、高度、宽度
869 if self.fp16 and im.dtype != torch.float16:
870 im = im.half() # 到 FP16

AttributeError: &#39;str&#39; 对象没有属性 &#39;shape&#39;

请问有人能帮我解决这个问题吗]]></description>
      <guid>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</guid>
      <pubDate>Wed, 10 Jul 2024 06:10:32 GMT</pubDate>
    </item>
    <item>
      <title>从示例数据集重新创建文本嵌入</title>
      <link>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</link>
      <description><![CDATA[我有一个句子列表，以及一个 25 维向量上的理想嵌入列表。我正在尝试使用神经网络来生成新的编码，但很费劲。虽然模型运行良好，但其输出毫无意义，甚至无法准确复制训练数据！
import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 标记化
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentence_list)
sequences = tokenizer.texts_to_sequences(sentence_list)

# 假设您的向量是 25 维
input_dim = 25

# 定义编码器
input_vec =输入（形状=（max_sequence_length，））
编码 = Dense（25，activation=&#39;tanh&#39;）（输入_vec）# 减少到 16 维的示例
编码器 = 模型（输入_vec，编码）

# 定义解码器
解码 = Dense（输入_dim，activation=&#39;sigmoid&#39;）（编码）
自动编码器 = 模型（输入_vec，解码）

# 编译模型
自动编码器.编译（优化器=Adam（），loss=&#39;mse&#39;）

# 训练模型
自动编码器.fit（padded_sequences，combined_vectors_clean，
epochs=10，
batch_size=32，
shuffle=True，validation_split= 0.2）

据我所知，我的输入和标签没有任何问题，那么我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</guid>
      <pubDate>Wed, 10 Jul 2024 01:39:37 GMT</pubDate>
    </item>
    <item>
      <title>yolo8 面具检测给了我多个轮廓</title>
      <link>https://stackoverflow.com/questions/77409777/yolo8-masks-detection-gives-me-multiple-contours</link>
      <description><![CDATA[我做了一些实验后修改了这个问题！
我正在用 yolo8 做实验。
我制作了一个测试脚本来展示这个问题：
#
# 测试脚本用于探索 1 个找到的对象的多个轮廓。
导入 numpy 作为 np
导入 cv2

img = cv2.imread(&#39;demo.jpg&#39;)
imgcopy = img.copy()

从 ultralytics 导入 YOLO

# 加载模型
model = YOLO(&#39;yolov8n-seg.pt&#39;) # 加载官方模型
#model = YOLO(&#39;path/to/best.pt&#39;) # 加载自定义模型

# 使用模型进行预测
results = model(img, retina_masks=True, save=True, imgsz = 640, conf=0.5, boxes=False, show_labels=False, show_conf=False, save_crop=True, save_txt=True, save_conf=True) # 根据图像进行预测

for result in results:
mask = result.masks
for mask in mask.data:
mask_np= mask.cpu().numpy().astype(np.uint8)
#contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours, _ = cv2.findContours(mask_np, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

print (&#39;我们找到了 &#39; + str(len(contours)) + &#39; 个对象&#39;)

# 我知道我得到了 3 个轮廓，所以这有效！
imgcopy = cv2.drawContours(imgcopy, contours=contours[0], contourIdx=-1, color=(0, 255, 0), thicken=2, lineType=cv2.LINE_AA)
imgcopy = cv2.drawContours(imgcopy, contours=contours[1], contourIdx=-1, color=(255, 255, 0), thicken=2, lineType=cv2.LINE_AA)
imgcopy = cv2.drawContours(imgcopy, contours=contours[2], contourIdx=-1, color=(255,0, 255), thicken=2, lineType=cv2.LINE_AA)

cv2.imwrite(&#39;result.jpg&#39;, imgcopy)


我预期找到 1 个轮廓，但我找到了 3 个轮廓这个例子。
输入图像：

输出图像：

所以 1 个掩码属于 1 个找到的对象，但它有 3 个轮廓。
为什么会找到多个轮廓？
我应该如何过滤出正确的轮廓？
我发现第一个找到的轮廓并不总是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/77409777/yolo8-masks-detection-gives-me-multiple-contours</guid>
      <pubDate>Thu, 02 Nov 2023 12:59:13 GMT</pubDate>
    </item>
    </channel>
</rss>