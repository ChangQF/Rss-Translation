<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 19 Dec 2024 21:15:45 GMT</lastBuildDate>
    <item>
      <title>关于将建议系统复杂性作为毕业设计的咨询 [关闭]</title>
      <link>https://stackoverflow.com/questions/79294470/consultion-about-a-recommendations-system-complexity-as-a-graduation-project</link>
      <description><![CDATA[我和我的两个项目伙伴正在做一个推荐系统作为毕业项目，我们想做一个混合系统，包括基于内容的、协作的和人口统计的推荐模块，同时利用强化学习来跟上新用户的数据
因为我们都没有经验，所以我们想向专业社区寻求建议
我们开始对数据进行预处理，但担心在 2 月 12 日截止日期之前是否能够真正实现所有内容，以及在开始训练模块之前我们应该了解的集成限制，这样我们就不必在完成一个模块后从头开始，因为如果我们要使用混合系统，我们应该做一些不同的事情
所以我想要关于如何做到这一点的建议，以及这一切是否能在截止日期之前完成，因为如果太多了，我们可以缩小规模，因为我们最初的计划只是一个基于内容的推荐系统，我们想知道我们的项目是否在我们的时间限制内完成]]></description>
      <guid>https://stackoverflow.com/questions/79294470/consultion-about-a-recommendations-system-complexity-as-a-graduation-project</guid>
      <pubDate>Thu, 19 Dec 2024 13:36:56 GMT</pubDate>
    </item>
    <item>
      <title>需要为 3 个不同的任务构建一个神经网络模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79294175/need-to-build-a-single-neural-network-model-for-3-different-tasks</link>
      <description><![CDATA[我有一个二元分类任务，需要对 3 个不同的数据集执行此任务。每个数据集具有不同的特征类型和特征数量，但它们都来自同一域。但是，这三个任务都具有相同的二元分类目标。我需要为这三个任务构建一个神经网络模型。
我不想删除任何数据集中的任何特征。是否有任何方法可以将所有特征投射到相同的潜在阶段，从而损失最少的信息，并且神经网络无论如何都会调整以在所有 3 个数据集上进行学习！
到目前为止，我还没有尝试任何方法，但我计划使用自动编码器将所有特征缩小到共享子空间，然后训练神经网络算法。]]></description>
      <guid>https://stackoverflow.com/questions/79294175/need-to-build-a-single-neural-network-model-for-3-different-tasks</guid>
      <pubDate>Thu, 19 Dec 2024 11:53:12 GMT</pubDate>
    </item>
    <item>
      <title>提取 LLaVa 转换器中的 hidden_​​states</title>
      <link>https://stackoverflow.com/questions/79294056/extracting-hidden-states-in-llavas-transformer</link>
      <description><![CDATA[我需要使用LLaVa来获取图像的嵌入+已提供给LLM的查询。
据我所知，我需要LLaVa模型在经过编码层之前的输出。
从StackOverflow上的这篇文章，我尝试使用pre-hook来保存每个层的输入：
def capture_hidden_​​states(module, module_input):
print(f&#39;module_input: {module_input}\n&#39;)

model_path = &quot;liuhaotian/llava-v1.6-vicuna-7b&quot;
model_name = get_model_name_from_path(model_path)

tokenizer, model, image_processor, context_len = load_pretrained_model(
model_path = model_path,
model_base = None,
model_name = model_name,
load_4bit = True, # 保存 GPU 内存
device = &quot;cuda:0&quot; # 默认
)
model.register_forward_pre_hook(capture_hidden_​​states)

但我的输出显示每个级别的输入都是空的：
module_input: ()

我看到 Hugging Face 的版本在 forward 方法中有一个 output_hidden_​​states 参数，但我目前无法让这个版本的 LLaVa 运行，所以我试图让它在我之前编写的代码上运行。]]></description>
      <guid>https://stackoverflow.com/questions/79294056/extracting-hidden-states-in-llavas-transformer</guid>
      <pubDate>Thu, 19 Dec 2024 11:11:59 GMT</pubDate>
    </item>
    <item>
      <title>如何将 PIL 图像转换为 pytorch 张量以供 py-feat 使用</title>
      <link>https://stackoverflow.com/questions/79294004/how-to-convert-a-pil-image-to-a-pytorch-tensor-for-py-feat</link>
      <description><![CDATA[我需要将 pil 图像转换为 pytorch 张量，以便在 Detector.detect() (py-feat) 中使用它，但我在调用 detect() 的行中收到此错误：

RuntimeError：给定 groups=1，权重大小为 [64, 3, 3, 3]，预期
input[1, 1, 112, 112] 有 3 个通道，但得到的是 1 个通道

但是，如果我在转换后使用 check_tensor() 检查张量的样子，它看起来很好
这是代码：
def check_tensor(self, frame_tensor):
frame_numpy = frame_tensor.squeeze(0).permute(1, 2, 0).byte().numpy()
frame_numpy_bgr = cv2.cvtColor(frame_numpy, cv2.COLOR_RGB2BGR) 
cv2.imshow(&quot;Frame&quot;, frame_numpy_bgr)
cv2.waitKey(0) 
cv2.destroyAllWindows()

def analyze_image(self, image_path):

img = Image.open(image_path)
tensor_image = F.pil_to_tensor(img)
orig_prediction = self.detector.detect(inputs=tensor_image, data_type=&quot;tensor&quot;)

我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79294004/how-to-convert-a-pil-image-to-a-pytorch-tensor-for-py-feat</guid>
      <pubDate>Thu, 19 Dec 2024 10:53:19 GMT</pubDate>
    </item>
    <item>
      <title>ESP32 S3 MINI 1 上的 TinyML 部署问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</link>
      <description><![CDATA[我使用 TinyML 根据 6 轴加速度计和陀螺仪数据预测马的活动。数据通过 ESP32-S3 收集，我正尝试在 ESP32-S3 上部署 TensorFlow Lite (TFLite) 模型以进行实时预测。
但是，我遇到了几个库兼容性和部署问题。尽管遵循了标准的 TinyML 部署实践，但模型与 ESP32-S3 的集成似乎存在问题，尤其是在处理 TFLite Micro 运行时时。我尝试了一些优化，但仍然无法让一切顺利运行。
如果您能提供以下方面的任何建议，我将不胜感激：

如何在 ESP32-S3 上有效部署 TinyML 模型。
可能更适合此用例的替代方法或工具。
有关类似设置的任何提示、文档或经验
]]></description>
      <guid>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</guid>
      <pubDate>Thu, 19 Dec 2024 07:39:46 GMT</pubDate>
    </item>
    <item>
      <title>无论如何，PyTorch DeiT 模型都会持续预测一个类别</title>
      <link>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</guid>
      <pubDate>Thu, 19 Dec 2024 04:52:07 GMT</pubDate>
    </item>
    <item>
      <title>“使用 YOLO 和 EasyOCR 进行车牌识别时遇到的文本识别问题” [关闭]</title>
      <link>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</link>
      <description><![CDATA[问题
我正在开发一个车牌识别系统，使用 YOLOv8 进行检测，使用 EasyOCR 进行文本识别。虽然 YOLO 可以正确检测车牌区域，但 OCR 结果对于阿拉伯语文本和数字通常不准确。
检测到的文本示例：
检测到：“اباز”，这是无关紧要的。
检测到：“الراق”，与“العراق”部分匹配。
像“٢٦٠٤٩٩”这样的数字被准确检测到。
我尝试过的
管道设置：
YOLOv8 检测车牌并提取其边界框。
EasyOCR 处理裁剪后的车牌以进行文本识别。
文本校正：
使用 difflib.get_close_matches() 将 OCR 检测到的文本与预定义单词进行匹配（例如，“العراق”、“دهوك”）。
应用置信度阈值来过滤低置信度结果。
图像预处理：
将车牌区域转换为灰度。
调整区域大小以增强 OCR 性能。
最小可重现示例
import cv2
import easyocr
from ultralytics import YOLO

def detect_plate_with_yolo(image_path, model_path=&quot;yolov8n.pt&quot;):
model = YOLO(model_path)
img = cv2.imread(image_path)
results = model(img)
detections = results[0].boxes.xyxy.cpu().numpy()
if detections:
x1, y1, x2, y2 = map(int, detections[0])
return img[y1:y2, x1:x2]
return None

def perform_ocr_on_plate(plate_img):
reader = easyocr.Reader([&#39;ar&#39;, &#39;en&#39;], gpu=False)
plate_gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)
return reader.readtext(plate_gray, detail=1)

plate_img = detect_plate_with_yolo(&quot;path/to/image.jpg&quot;)
if plate_img is not None:
detected_text = perform_ocr_on_plate(plate_img)
print(detected_text)

预期与实际行为
预期：正确识别阿拉伯语文本和数字（例如，&quot;العراق&quot;）。
实际：部分匹配（例如，&quot;الراق&quot;）或不相关的结果（例如，&quot;اباز&quot;）。
问题
如何使用 EasyOCR 提高阿拉伯语车牌的 OCR 准确率？
有没有比 difflib.get_close_matches() 更好的文本校正替代方案？
哪些额外的预处理步骤可能有助于提高 OCR 性能？]]></description>
      <guid>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</guid>
      <pubDate>Wed, 18 Dec 2024 17:26:40 GMT</pubDate>
    </item>
    <item>
      <title>Yolov5 模型捕获除预期或期望对象之外的其他对象[关闭]</title>
      <link>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</link>
      <description><![CDATA[我有一个用于数据集的 YOLO V5 m 模型，用于检测特定产品，但在对不同情况和场景以及各种光照条件下的产品图片数据集进行训练后，在 50 个 epoch 和 Yolo v5 中等架构以及批处理大小为 16 的情况下，情况没有重复。
我注意到，该模型依赖于颜色特征，这意味着检测到的对象具有黄色及其阴影。我使用了 yolo github 克隆附带的 train.py。我应该怎么做才能解决这个问题？
用于训练的数据集（大约 450 张图片）在谷歌驱动器中。
需要检测的产品：


检测到的其他产品：
]]></description>
      <guid>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</guid>
      <pubDate>Wed, 18 Dec 2024 14:01:11 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器的修改[关闭]</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（像往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。据我所知，没有允许我指定该功能的函数（如果有，请告诉我）。]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 OpenAI API 中仅发送提示的动态部分以优化令牌使用？</title>
      <link>https://stackoverflow.com/questions/79290775/how-to-send-only-dynamic-part-of-a-prompt-in-openai-api-to-optimize-token-usage</link>
      <description><![CDATA[我正在处理一项任务，需要与 OpenAI API 交互以处理动态祈祷请求。我想解决的问题是优化令牌使用。
这是我目前的方法：
我有一个静态系统指令，它定义了助手的行为（例如，以同理心回应祈祷请求）。
用户的祈祷请求是动态的，每次都会发生变化。
我想在每个请求中只发送动态部分（祈祷请求），同时保持静态部分（系统指令）在每个请求中保持一致。
我的目标是减少令牌使用量，同时确保助手对用户的请求做出适当的响应。
我尝试在每个请求中将静态部分作为提示的一部分发送，但我正在寻找一种更有效的方法来只发送动态部分，同时仍保持上下文。有没有更好的方法可以通过 OpenAI API 实现这一点？
此外，我想跟踪令牌使用情况，以确保尽可能优化调用。
有人能建议一种有效的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79290775/how-to-send-only-dynamic-part-of-a-prompt-in-openai-api-to-optimize-token-usage</guid>
      <pubDate>Wed, 18 Dec 2024 10:33:04 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理步骤[关闭]</title>
      <link>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</link>
      <description><![CDATA[我想知道 Keras 图像数据加载是否对图像数据集进行了完整的预处理，例如应用过滤器、规范化、标准化等，在该模块之后我们是否不需要进行更详细的预处理？如果答案是肯定的，那么有人可以告诉我正确和准确的预处理步骤吗？
代码如下。
malimg=tf.keras.utils.image_dataset_from_directory(
r&quot;C:\Users\PMYLS\Desktop\Malware Pycharm Project\malimg_paper_dataset_imgs&quot;,
labels=&quot;inferred&quot;,
label_mode=&quot;int&quot;,
class_names=None,
color_mode=&quot;rgb&quot;,
batch_size=64,
image_size=(256, 256),
shuffle=True,
seed=None,
validation_split=None,
subset=None,
interpolation=&quot;bilinear&quot;,
follow_links=False,
crop_to_aspect_ratio=False,
pad_to_aspect_ratio=False,
data_format=None,
verbose=True,
)
class_names = malimg.class_names # 获取类名
print(&quot;Classes:&quot;, class_names)
]]></description>
      <guid>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</guid>
      <pubDate>Tue, 17 Dec 2024 17:17:21 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：微调 llama 时，张量的元素 0 不需要 grad 且没有 grad_fn</title>
      <link>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</guid>
      <pubDate>Fri, 13 Dec 2024 06:02:07 GMT</pubDate>
    </item>
    <item>
      <title>Keras ValueError：从未调用过层顺序，因此没有定义的输出</title>
      <link>https://stackoverflow.com/questions/78722413/keras-valueerror-the-layer-sequential-has-never-been-called-and-thus-has-no-def</link>
      <description><![CDATA[我想将 Keras 的 Grad-CAM 与我自己的 CNN 模型一起使用。我已遵循此 https://keras.io/examples/vision/grad_cam/，其中的 make_gradcam_heatmap 函数也来自此。我对 CNN 还不熟悉，所以我可能忽略了一些显而易见的东西，但为什么它在我运行模型时无法识别呢？
这是我的代码：
import numpy as np
import os
import tensorflow as tf
import keras
from tensorflow.keras.models import load_model
import cv2
from tensorflow.keras.models import Model

os.environ[&quot;KERAS_BACKEND&quot;] = &quot;tensorflow&quot;

从 IPython.display 导入图像，显示
导入 matplotlib 作为 mpl
导入 matplotlib.pyplot 作为 plt

img_path = &#39;/Users/.../image_1.npy&#39;

model = load_model(&#39;/Users/.../particle_classifier_model.h5&#39;)

model_builder = keras.applications.xception.Xception
preprocess_input = keras.applications.xception.preprocess_input
decode_predictions = keras.applications.xception.decode_predictions

image = np.load(img_path)
img_size = image.shape # 应为形状为 (240, 146) 的数组

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
# 首先，我们创建一个将输入图像映射到最后一个 conv 的激活的模型层以及输出预测
grad_model = keras.models.Model(
model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]
)

# 然后，我们计算输入图像的顶部预测类的梯度
# 相对于最后一个卷积层的激活
with tf.GradientTape() as tape:
last_conv_layer_output, preds = grad_model(img_array)
if pred_index is None:
pred_index = tf.argmax(preds[0])
class_channel = preds[:, pred_index]

# 这是输出神经元的梯度（顶部预测或选择）
# 相对于最后一个卷积层的输出特征图
grads = tape.gradient(class_channel, last_conv_layer_output)

# 这是一个向量，其中每个条目都是梯度的平均强度
# 在特定特征图通道上
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

# 我们将特征图数组中的每个通道乘以
# 相对于顶部预测类的“此通道的重要性”
# 然后将所有通道相加以获得热图类激活
last_conv_layer_output = last_conv_layer_output[0]
heatmap = last_conv_layer_output @pooled_grads[..., tf.newaxis]
heatmap = tf.squeeze(heatmap)

# 为了可视化目的，我们还将在 0 和 1 之间对热图进行标准化
heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
return heatmap.numpy()

last_conv_layer_name = “max_pooling2d_4”

image = image.reshape(1, 240, 146, 1)
preds = model.predict(image)

model.layers[-1].activation = None

heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name)
plt.matshow(heatmap)
plt.show()

我的数据是维度 (240, 146) 的 numpy 数组，CNN 将其作为输入。]]></description>
      <guid>https://stackoverflow.com/questions/78722413/keras-valueerror-the-layer-sequential-has-never-been-called-and-thus-has-no-def</guid>
      <pubDate>Mon, 08 Jul 2024 18:32:19 GMT</pubDate>
    </item>
    <item>
      <title>总参数：0，执行 model.summary() keras</title>
      <link>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</link>
      <description><![CDATA[model = Sequential()
model.add(Embedding(283, 100, input_length=56))
model.add(LSTM(150))
model.add(LSTM(150))
model.add(Dense(283,activation=&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

model.summary()

Tensorflow 版本：2.16.1，
Keras 版本：3.3.3，
设备 - M3 pro macbook
我尝试使用虚拟数据集（有 282 个唯一单词，使用 tokenizer 检查）构建用于文本生成的 LSTM 模型，预期参数为非零，但输出每个层都有 0 个参数。]]></description>
      <guid>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</guid>
      <pubDate>Fri, 10 May 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    </channel>
</rss>