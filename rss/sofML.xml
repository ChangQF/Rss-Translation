<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 11 Sep 2024 18:21:20 GMT</lastBuildDate>
    <item>
      <title>为什么我的神经网络模型无法学习绝对函数 abs(x1-x2)？</title>
      <link>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</link>
      <description><![CDATA[我正在尝试训练一个简单的神经网络模型进行多类分类。
我有 x1、x2、x3、x4 列，其中有 4 个类别需要预测。
如果只对 x1、x2、x3、x4 进行训练，那么我的准确率是 88%
凭借一些领域知识，我可以创建三个新特征，我知道这肯定会帮助模型更好地训练。
这三个新特征是：-

df[&#39;x12&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x2&#39;])
df[&#39;x13&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x3&#39;])
df[&#39;x14&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x4&#39;])

如果我在 x1、x2、x3、x4 和 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 上进行训练，那么我的准确率是 98%
我想在没有 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 的情况下获得 98% 的准确率
使用这些新的手动创建的特征，我获得了 98% 的验证准确率，这很棒。
但是，当我删除这些特征时，验证准确率会下降到 88%。
我的问题是函数 abs(x1-x2) 应该非常简单，足以让模型自行学习，而无需我手动进行特征工程。
那么为什么当我删除这三个（非常）时准确率会下降简单）特征？
模型是否没有足够的能力自行学习？
我尝试在模型中使用不同的激活函数。
从线性激活函数、relu 激活函数、leaky relu、prelu 开始。
但是它们都没有给我 98% 的准确率（不使用三个手动创建的特征）。
这是我的模型的样子：-

inputs = Input(shape=input_shape)

x = Dense(units=64)(inputs)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=32)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

multiclass_output = Dense(units=num_outputs,activation=&#39;softmax&#39;)(x)

model = Model(inputs=inputs,outputs=multiclass_output)

model.compile(
loss=&quot;categorical_crossentropy&quot;,
metrics=[&quot;accuracy&quot;],
optimizer=Adam(learning_rate=LR)
)

返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</guid>
      <pubDate>Wed, 11 Sep 2024 15:12:02 GMT</pubDate>
    </item>
    <item>
      <title>银行客户流失预测模型 - 预测能力建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</guid>
      <pubDate>Wed, 11 Sep 2024 08:12:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在深度学习中计算真实世界物体的大小[关闭]</title>
      <link>https://stackoverflow.com/questions/78972330/how-calculate-real-world-object-size-in-deeplearning</link>
      <description><![CDATA[我正在做一个项目，遇到了一个麻烦，需要通过图片计算任何物体的真实世界大小，并且没有指定相机和距离的边界。
是否有任何深度学习模型可以为我计算和预测任何物体的真实世界大小？]]></description>
      <guid>https://stackoverflow.com/questions/78972330/how-calculate-real-world-object-size-in-deeplearning</guid>
      <pubDate>Wed, 11 Sep 2024 06:34:11 GMT</pubDate>
    </item>
    <item>
      <title>程序如何改变张量的大小？</title>
      <link>https://stackoverflow.com/questions/78972166/how-does-the-program-change-the-size-of-the-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78972166/how-does-the-program-change-the-size-of-the-tensor</guid>
      <pubDate>Wed, 11 Sep 2024 05:31:44 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 架构中填充批次的掩蔽和计算损失[关闭]</title>
      <link>https://stackoverflow.com/questions/78970293/masking-and-computing-loss-for-a-padded-batch-in-a-transformer-architecture</link>
      <description><![CDATA[我正尝试基于 带注释的 Transformer 松散地重新创建 Transformer 模型。我的问题是关于填充：

带注释的 Transformer 如何处理填充序列？我可以看到他们创建了方法 Batch.make_std_mask，据说该方法（也）会屏蔽所有填充标记，但这仅适用于合成数据。
在基于 Transformer 的架构中，通常应如何处理填充序列？我可以看到提及（再次，在带注释的 Transformer 中，搜索“批处理对速度非常重要。”）以最小化填充，我猜这是为了找到最小化填充的序列长度？我非常确定必须将整个填充序列传入模型，否则自注意力机制就会出现问题。除了在嵌入 (pytorch) 中设置 padding_idx 之类的内容外，模型本身是否应该处理填充标记？损失计算索引目标和模型输出是否应忽略所有填充的标记？

我已经看过这个问题（在 pytorch 中通过具有线性输出层的 RNN 发送的填充批次的掩码和计算损失） - 对于基于注意力的模型，该过程是否相同，或者是否存在差异，因为注意力机制如何一次“看到”整个序列？
我也看过这个问题（Transformer 编码器中的查询填充掩码和键填充掩码），但我不确定它是否处理了同样的问题。如果是这样，我仍然希望得到澄清（并以那里的答案的形式），因为我既不完全理解这个问题，也不完全理解目前接受的（唯一）答案。]]></description>
      <guid>https://stackoverflow.com/questions/78970293/masking-and-computing-loss-for-a-padded-batch-in-a-transformer-architecture</guid>
      <pubDate>Tue, 10 Sep 2024 15:48:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行人体分割会失败，但使用 Tensorflow Keras 不会失败</title>
      <link>https://stackoverflow.com/questions/78969962/human-segmentation-fails-with-pytorch-not-with-tensorflow-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78969962/human-segmentation-fails-with-pytorch-not-with-tensorflow-keras</guid>
      <pubDate>Tue, 10 Sep 2024 14:18:33 GMT</pubDate>
    </item>
    <item>
      <title>用 R 预测股票波动性</title>
      <link>https://stackoverflow.com/questions/78969928/forecasting-volatility-of-stocks-in-r</link>
      <description><![CDATA[项目概述：
我正在开展一个项目，使用 469 天内 292 只股票的每日数据来预测股票指数的波动性。我有一个包含 104 个特征的数据集，包括特定于股票的特征、滞后变量和移动平均线。此外，我计算了 6 个波动代理，并计划对每个代理进行 1 天、5 天和 10 天的预测。
目标：
我的目标是：使用经典的时间序列预测方法（例如 ARIMA、GARCH）对波动性进行建模。
应用机器学习技术（例如随机森林、梯度提升、RNN）来提高预测准确性。
具体挑战：
鉴于我的数据结构（每日股票数据、波动代理和各种特征），我不确定如何最好地进行传统时间序列模型和机器学习模型的特征选择和预处理。
我正在寻找有关模型选择的指导、在多变量环境中处理时间序列数据的最佳实践以及任何可以帮助我获得的相关文献开始。
问题：

使用时间序列模型和机器学习技术预测波动性的最佳起点是什么？

是否有任何推荐的资源或论文解释如何使用这样的数据集进行波动性预测？

我应该如何处理多元时间序列预测的特征工程和模型验证？


我正在开展一个预测股票指数波动性的项目，但由于缺乏使用某些技术的经验，我不知道如何开始。
以下是我想做的事情：

我有 469 天内 292 只股票的每日数据。
我的数据集包括 104 个特征，例如股票特定变量、滞后变量和移动平均线。
我已经计算了 6 个波动代理，我的目标是预测每个代理未来 1、5 和 10 天的情况。
我计划从经典的时间序列预测方法（例如 ARIMA、GARCH）开始，然后探索机器学习技术，如随机森林或 RNN。

我遇到的困难：
我不确定如何开始这个项目。具体来说，我无法理解如何构建数据、选择特征和应用适当的模型。我不知道如何进行时间序列和机器学习方法的特征选择、模型验证和性能评估。
我希望得到一些关于从哪里开始的指导、推荐的方法以及任何可以帮助我学习最佳实践的资源或文献。]]></description>
      <guid>https://stackoverflow.com/questions/78969928/forecasting-volatility-of-stocks-in-r</guid>
      <pubDate>Tue, 10 Sep 2024 14:11:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>高效的 PyTorch 带矩阵到密集矩阵乘法</title>
      <link>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</link>
      <description><![CDATA[问题：在我的某个程序中，我需要计算矩阵乘法 A @ B，其中两个矩阵的大小均为 N x N，但 N 相当大。我推测使用 band_matrix(A, width) @ B 来近似该乘积即可满足需求，其中 band_matrix(A, width) 表示 A 的带状矩阵部分，宽度为 width。例如，width = 0 给出对角矩阵，对角线元素取自 A，而 width = 1 给出以类似方式获取的三对角矩阵。
我的尝试：我尝试提取三对角矩阵，例如，以以下方式：
# 步骤 1：提取主对角线
main_diag = torch.diagonal(A, dim1=-2, dim2=-1) # 形状：[d1, d2, N]

# 步骤 2：提取上对角线（偏移量=1）
upper_diag = torch.diagonal(A, offset=1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 3：提取下对角线(offset=-1)
lower_diag = torch.diagonal(A, offset=-1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 4：重建三对角矩阵
# 主对角线
tridiag = torch.diag_embed(main_diag) # 形状：[d1, d2, N, N]

# 上对角线（移动值以创建第一个上对角线）
tridiag += torch.diag_embed(upper_diag, offset=1)

# 下对角线（移动值以创建第一个下对角线）
tridiag += torch.diag_embed(lower_diag, offset=-1)

但我不确定 tridiag @ B 是否比原始 A 更有效率@ B 或者只是相同的复杂性，因为 Torch 可能不知道 tridiag 的具体结构。理论上，使用三对角矩阵的计算应该快 N 倍。

任何有助于理解 PyTorch 在这种情况下的行为或实施一些替代的 GPU 优化方法的帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</guid>
      <pubDate>Sat, 07 Sep 2024 05:46:17 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 `sklearn` 管道中的 `ravel()` 或 `to_numpy()` 转换目标变量？</title>
      <link>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</link>
      <description><![CDATA[我在 R markdown 文档中使用 RStudio 和 tidymodels。我想合并一些来自 scikit-learn 的模型。将数据从 R 代码块传输到 Python 代码块效果很好，但是当我使用以下代码训练和测试模型时：
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

log_reg_pipe = Pipeline([
(&#39;Logistic Regression&#39;, LogisticRegression())
])

log_reg_pipe.fit(X_train, y_train).score(X_val, y_val)

我收到错误
DataConversionWarning：当预期为 1d 数组时，传递了列向量 y。
请将 y 的形状更改为 (n_samples, )，例如使用 ravel()。

我可以通过使用 y_train[&#39;clinical_course&#39;].to_numpy() 训练数据来解决这个问题，但我希望这可以在管道中直接完成。这可能吗？
请注意，上面的代码只是一个简单的示例，用于展示我的问题。在这种情况下，X_train 有四列，y_train 有一列。
如上所述，我尝试使用 .to_numpy()，但我想要一个在管道内完成所有转换的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</guid>
      <pubDate>Fri, 06 Sep 2024 18:31:45 GMT</pubDate>
    </item>
    <item>
      <title>具有中间层输出的 Keras 多输出自定义损失</title>
      <link>https://stackoverflow.com/questions/66526604/keras-multioutput-custom-loss-with-intermediate-layers-output</link>
      <description><![CDATA[我在 keras 中有一个模型，它接受两个输入并返回 3 个输出，我想计算自定义损失。我的问题是我不知道如何在损失中使用中间层的输出。到目前为止，该模型由两个子模型（图中的子模型 1 和子模型 2）组成，最终损失由损失 1 和损失 2 的总和组成。这很容易，因为 loss1 将 output1 与数据生成器的 label1 进行比较，将 output2 与数据生成器的 label2 进行比较。

当我在模型中包含 submodel3 时，问题就出现了，因为 loss3 将 output1 与 output3 进行比较，而 output1 是模型某一层的输出，而不是数据生成器的 label3。我试过这个方法：
input1 = Input(shape=input1_shape)
input2 = Input(shape=input2_shape)
output1 = submodel1()([input1,input2]) #不要在意代码符号，因为它是用来解释问题的代码。
output2 = submodel2()(output1)
output3 = submodel3()(output1)
@tf.function
def MyLoss(y_true, y_pred):
out1, out2, out3 = y_pred
inp1, inp2 = y_true

loss1 = tf.keras.losses.some_loss1(out1,inp1)
loss2 = tf.keras.losses.some_loss2(out2, inp2)
loss3 = tf.keras.losses.some_loss3(out2,out3)

loss = loss1 + loss2 + loss3
return loss

model = Model([input1,input2],[output1,output2,output3])
model.compile(optimizer=&#39;adam&#39;,loss = MyLoss)

但我收到此错误：
 OperatorNotAllowedInGraphError：不允许对“tf.Tensor”进行迭代：AutoGraph 确实转换了此函数。这可能表明您正在尝试使用不受支持的功能。

我正在使用 TensorFlow 2.3.0-rc0 版本。]]></description>
      <guid>https://stackoverflow.com/questions/66526604/keras-multioutput-custom-loss-with-intermediate-layers-output</guid>
      <pubDate>Mon, 08 Mar 2021 08:39:20 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 编码器中的查询填充掩码和密钥填充掩码</title>
      <link>https://stackoverflow.com/questions/65262928/query-padding-mask-and-key-padding-mask-in-transformer-encoder</link>
      <description><![CDATA[我正在使用 pytorch nn.MultiheadAttention 在 transformer 编码器中实现自注意力部分，并在 transformer 的填充掩码中进行混淆。
下图显示了查询（行）和键（列）的自注意力权重。
如您所见，有一些标记“&lt;PAD&gt;”，我已经在键中对其进行了掩码。因此 token 不会计算注意力权重。

还有两个问题：

在查询部分，除了红色方块部分，我还可以屏蔽它们（“&lt;PAD&gt;”）吗？这合理吗？

我该如何屏蔽“&lt;PAD&gt;”在查询中？


注意权重还通过在 src_mask 或 src_key_padding_mask 参数中提供掩码，沿行使用 softmax 函数。如果我将所有 &lt;PAD&gt;&gt; 行设置为 -inf，则 softmax 将返回 nan，损失将为 nan]]></description>
      <guid>https://stackoverflow.com/questions/65262928/query-padding-mask-and-key-padding-mask-in-transformer-encoder</guid>
      <pubDate>Sat, 12 Dec 2020 08:19:34 GMT</pubDate>
    </item>
    <item>
      <title>如何处理正态分布中的零项</title>
      <link>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</link>
      <description><![CDATA[我正在使用 kaggle 房屋数据集。我正尝试使用神经网络进行练习。我正在尝试规范化数据。我的问题是：我有一个变量 BsmtFinSF1，它指的是“1 型成品平方英尺”，它有很多值为 0。值零对应“无地下室”，事实上，在另一个因子变量中，它对应于一个级别。例如，如果“地下室条件”变量对应于“无地下室”，则意味着 BsmtFinSF1 变量将为 0。下面是 BsmtFinSF1 的直方图。如果我没有弄错的话，如果没有零，分布将是正常的。我该如何将其标准化，或者我是否应该将其标准化？
]]></description>
      <guid>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</guid>
      <pubDate>Wed, 26 Feb 2020 07:30:27 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中，通过具有线性输出层的 RNN 发送填充批次的掩蔽和计算损失</title>
      <link>https://stackoverflow.com/questions/59292708/masking-and-computing-loss-for-a-padded-batch-sent-through-an-rnn-with-a-linear</link>
      <description><![CDATA[虽然这是一个典型的用例，但我找不到一个简单明了的指南来说明当通过 RNN 发送时，在 pytorch 中计算填充小批量损失的规范方法是什么。
我认为规范的管道可能是：

pytorch RNN 期望填充的批量张量形状为：(max_seq_len, batch_size, emb_size)

因此，我们给出一个嵌入层，例如这个张量：
tensor([[1, 1],
[2, 2],
[3, 9]])


9 是填充索引。批量大小为 2。嵌入层将使其形状为 (max_seq_len, batch_size, emb_size)。批次中的序列按降序排列，因此我们可以打包它。

我们应用 pack_padded_sequence，应用 RNN，最后应用 pad_packed_sequence。此时我们有 (max_seq_len、batch_size、hidden_​​size)

现在我们在结果上应用线性输出层，假设是 log_softmax。所以最后我们有一个批次分数的张量，形状为：(max_seq_len、batch_size、linear_out_size)


我应该如何从这里计算损失，掩盖填充部分（使用任意目标）？]]></description>
      <guid>https://stackoverflow.com/questions/59292708/masking-and-computing-loss-for-a-padded-batch-sent-through-an-rnn-with-a-linear</guid>
      <pubDate>Wed, 11 Dec 2019 19:25:31 GMT</pubDate>
    </item>
    <item>
      <title>隔离森林 Sklearn 用于一维数组或列表以及如何调整超参数</title>
      <link>https://stackoverflow.com/questions/50957340/isolation-forest-sklearn-for-1d-array-or-list-and-how-to-tune-hyper-parameters</link>
      <description><![CDATA[有没有办法为 1D 数组或列表实现 sklearn 隔离森林？我遇到的所有示例都是针对 2 维或更高维度的数据。
我现在已经开发了一个具有三个特征的模型，示例代码片段如下：
# 三列的数据框
df_data = datafr[[&#39;col_A&#39;, &#39;col_B&#39;, &#39;col_C&#39;]]
w_train = page_data[:700]
w_test = page_data[700:-2]

from sklearn.ensemble import IsolationForest
# 拟合模型
clf = IsolationForest(max_samples=&#39;auto&#39;)
clf.fit(w_train)

# 使用测试集进行测试
y_pred_test = clf.predict(w_test)

我主要依赖的参考资料：IsolationForest 示例 | scikit-learn
df_data 是一个包含三列的数据框。我实际上想在 1 维或列表数据中查找异常值。
另一个问题是如何调整隔离森林模型？其中一种方法是增加污染值以减少误报。但如何使用其他参数，如 n_estimators、max_samples、max_features、versbose 等。]]></description>
      <guid>https://stackoverflow.com/questions/50957340/isolation-forest-sklearn-for-1d-array-or-list-and-how-to-tune-hyper-parameters</guid>
      <pubDate>Wed, 20 Jun 2018 21:32:17 GMT</pubDate>
    </item>
    </channel>
</rss>