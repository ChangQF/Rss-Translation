<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Jan 2024 21:12:07 GMT</lastBuildDate>
    <item>
      <title>如何在Python操作符mwaa中动态调用Python脚本</title>
      <link>https://stackoverflow.com/questions/77852522/how-to-dynamically-call-a-python-script-in-python-operator-mwaa</link>
      <description><![CDATA[我有一个场景如下
基础设施 aws 和 mwaa
我想开发一个 mwaa 代码，以通用的方式为我们的 10 个模型训练机器学习代码管道。
因此，数据科学家为我提供了每个模型的入口点文件和推理文件。我开发了一个代码来为每个模型动态生成 mwaa 管道，但我没有看到调用特定于模型的入口点文件的灵活性。
我正在寻找一种功能，可以在 mwaa /airflow 代码中的 Python 运算符中动态调用 Python 脚本]]></description>
      <guid>https://stackoverflow.com/questions/77852522/how-to-dynamically-call-a-python-script-in-python-operator-mwaa</guid>
      <pubDate>Sat, 20 Jan 2024 19:58:12 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：basic_string::调整大小</title>
      <link>https://stackoverflow.com/questions/77852154/xgboosterror-basic-stringresize</link>
      <description><![CDATA[我想实现一个 xgboost 模型。为此，我已经通过 skopt.BayesSearchCV 执行了超参数调整，如以下代码所示：
从 xgboost 导入 XGBRegressor
从 skopt 导入 BayesSearchCV
from skopt.space import 实数、分类、整数

种子 = 8

xgb_clf = XGBRegressor(random_state=SEED)

搜索空间 = {
    &#39;xgb_clf__max_深度&#39;：整数（2,30），
    &#39;xgb_clf__subsample&#39;：真实（0.1，1.0），
    &#39;xgb_clf__colsample_bytree&#39;：真实（0.1，1.0），
    &#39;xgb_clf__colsample_bylevel&#39;：真实（0.1，1.0），
    &#39;xgb_clf__reg_alpha&#39;：实数（1.0，100.0），
    &#39;xgb_clf__reg_lambda&#39;：实数（1.0，100.0），
    &#39;xgb_clf__n_estimators&#39;: 整数(10, 1000),
    &#39;xgb_clf__learning_rate&#39;：真实（0.01，0.1，先验=&#39;对数均匀&#39;）
}

opt = BayesSearchCV(xgb_clf, search_space, cv=10, n_iter=30, 评分=&#39;roc_auc&#39;, random_state=SEED)

此外，我编写了一些代码，应该逐行训练我的模型，其中行是保存在另一个目录中的 2D numpy 数组，标签（年龄）也保存在单独的文件夹中。因为我想根据文档逐行训练它（https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.fit）必须设置 xgb_model 参数。这需要保存我的模型，我在下面的代码中尝试过：
导入pickle

MODEL_PATH =“ML/模型”
os.makedirs(MODEL_PATH)
PATH_TRAIN_IMAGES =“ML/preprocessed_arrays/train”
PATH_TRAIN_LABELS =“ML/preprocessed_labels/train”
文件名 = MODEL_PATH + &#39;/xgb_model.sav&#39;

pickle.dump(opt, open(文件名, &#39;wb&#39;))

例如zip中的instance_file、label_file(os.listdir(PATH_TRAIN_IMAGES), os.listdir(PATH_TRAIN_LABELS))：
    实例= np.loadtxt（PATH_TRAIN_IMAGES +“/”+实例文件）
    label = np.loadtxt(PATH_TRAIN_IMAGES + “/” + label_file)
    opt.fit(实例、标签、xgb_model=文件名)
    # 将模型保存到磁盘
    pickle.dump(opt, open(文件名, &#39;wb&#39;))

不幸的是，这段代码抛出了一个 XGBoostError: basic_string::resize 错误，我不明白。
我已经尝试对此错误进行一些研究，但这似乎是一个罕见的错误。
如何保存模型、逐行训练并避免此错误？提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/77852154/xgboosterror-basic-stringresize</guid>
      <pubDate>Sat, 20 Jan 2024 18:11:12 GMT</pubDate>
    </item>
    <item>
      <title>深度学习如何成为机器学习的子集，而机器学习如何成为人工智能的子集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77852041/how-is-deep-learning-a-subset-of-machine-learning-and-machine-learning-a-subset</link>
      <description><![CDATA[像什么是清晰的现实生活示例来解释它们作为子集？
我对人工智能和机器学习还很陌生。请认为我是一个愚蠢的机器人，并将数据提供给我。
.]]></description>
      <guid>https://stackoverflow.com/questions/77852041/how-is-deep-learning-a-subset-of-machine-learning-and-machine-learning-a-subset</guid>
      <pubDate>Sat, 20 Jan 2024 17:33:54 GMT</pubDate>
    </item>
    <item>
      <title>调整线性 SVM 参数期间出现“警告：达到最大迭代次数”</title>
      <link>https://stackoverflow.com/questions/77851738/warning-reaching-max-number-of-iterations-during-tuning-of-parameters-for-lin</link>
      <description><![CDATA[我有这个数据集：https： //www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied，我从中删除了 NewExist = 1 的所有实例以及包含 NA 值的所有实例。我还删除了 ChgOffPrinGr 列，因此还剩下 161,732 个实例和 25 个变量。我需要应用一系列以 MIS_Status 作为目标变量的机器学习模型。为了实现这一目标，我没有使用所有变量作为预测变量，因为显然，我需要消除所有包含“未来”的变量。值，即无法提前知道的值。因此，我有 161,732 个实例，包含 10 个变量：State（因子）、Bank（因子）、NAICS（因子）、UrbanRural（因子）、RevLineCr（因子）、LowDoc（因子）、Term（双精度）、NoEmp（双精度）、 FranchiseCode（因子）、GrAppv（双精度）、MIS_Status（因子、目标）。
我做的第一件事是预处理数据集以使用以下代码应用算法（因此我创建了配方）：
selected_columns &lt;- c(&quot;State&quot;, &quot;Bank&quot;, &quot;NAICS&quot;, &quot;UrbanRural&quot;, &quot;RevLineCr&quot;, &quot;LowDoc&quot;, &quot;Term&quot;, &quot;NoEmp&quot; ;、“特许经营代码”、“GrAppv”、“MIS_Status”）

df_selected &lt;- SBAnational[selected_columns]

low_var_cols &lt;- 插入符::nzv(df_selected)

df_no_zv &lt;- df_selected[, -low_var_cols]

设置.种子(123)

split_index &lt;- createDataPartition(df_no_zv$MIS_Status, p = 0.7, list = FALSE)
train_data &lt;- df_no_zv[split_index, ]
test_data &lt;- df_no_zv[-split_index, ]

data_recipe &lt;- 配方(MIS_Status ~ ., data = train_data) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

data_prep &lt;- 准备（data_recipe，训练 = train_data）

train_processed &lt;- 烘焙（data_prep，new_data = train_data）
test_processed &lt;- 烘焙（data_prep，new_data = test_data）

在本次迭代结束时，我发现自己拥有一个包含 113,214 个实例和 4,382 个变量的训练集。
此时，我决定首先通过调整参数来应用线性SVM。代码如下所示：
svm_model &lt;- svm(MIS_Status ~ ., data = train_processed, kernel = “线性”)

une_result &lt;- 调整（svm，MIS_Status ~ .，data = train_processed，kernel =“线性”，范围 = list（成本 = c（0.1, 1, 10, 100, 1000）））

问题是调优已运行超过 12 小时，并且我已遇到此警告消息 6 次：
&lt;前&gt;&lt;代码&gt;&gt; une_result &lt;- 调整（svm，MIS_Status ~ .，data = train_processed，kernel =“线性”，范围 = list（成本 = c（0.1, 1, 10, 100, 1000）））

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

你能给我一些建议来简化一切吗？
（MacBook Air M2 16GB 512GB）]]></description>
      <guid>https://stackoverflow.com/questions/77851738/warning-reaching-max-number-of-iterations-during-tuning-of-parameters-for-lin</guid>
      <pubDate>Sat, 20 Jan 2024 16:11:36 GMT</pubDate>
    </item>
    <item>
      <title>计算不同维度张量的线性（又名图）注意力的有效方法</title>
      <link>https://stackoverflow.com/questions/77851297/efficient-way-for-calculating-linear-aka-graph-attention-for-tensors-with-vary</link>
      <description><![CDATA[我正在按照这篇关于图注意力的论文使用 TensorFlow v2 实现注意力层 (GAT) .15.它将用于全局自注意力层，类似于 Transformer 中的多头注意力（而不是作为图神经网络中的层）。
该层的输入具有形状(bs,None,fs)，其中bs代表批量大小，None是变长维度（您可以有效地将其视为批次中句子的最大长度），fs 是特征维度。
计算此类输入的 GAT 注意力的最有效方法是什么？
对于句子中的每一对单词，GAT 将它们沿特征维度的串联作为输入。因此，迭代“单词”似乎是很自然的事情。并使用适当的索引动态创建所需的输入。然而，由于输入张量的第二个维度是未知的（因为它因批次而异），TensorFlow 拒绝在训练期间迭代此维度。通过“迭代”我的意思是使用诸如 tf.map_fn 之类的函数（在正确排列轴之后），或者沿第二个方向使用 tf.stack 和 tf.unstack 的组合张量的维数。
我通过直接创建一个张量 gat_input 来实现它，它是 GAT 层的输入，形状为 (bs,None,None,2*fs) 使用下面的代码。它存储所有的“单词”对。需要计算注意力。然而，这是非常慢的（与标准多头注意力相比）并且内存效率很高（因为它创建了一个巨大的中间张量）。
num = tf.shape(inputs)[-2] # 变化的暗淡
in_exp = tf.expand_dims(输入, axis=-2) # (bs,无,1,fs)
in_exp = tf.repeat(in_exp , axis=2, 重复=num) # (bs,无,无,fs)
in_exp_2 = tf.transpose(in_exp_2 , perm=[0,2,1,3]) # 转置以正确组合
gat_input = tf.concat( (in_exp,in_exp_2), axis=-1) # (bs, None, None, 2*fs)

创建假“邻接矩阵” （以便连接所有“单词”）并将其与 tf.gather 结合使用来为 GAT 层创建输入不会提高速度。
如有任何帮助，我们将不胜感激。
固定输入张量的第二维是非常不可取的。另外，我不处理句子，这个类比只是为了澄清。]]></description>
      <guid>https://stackoverflow.com/questions/77851297/efficient-way-for-calculating-linear-aka-graph-attention-for-tensors-with-vary</guid>
      <pubDate>Sat, 20 Jan 2024 13:56:41 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 google colab pro 作为人工智能服务器吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</link>
      <description><![CDATA[我正在尝试编写一个程序，该程序从用户那里获取文本，并在文本中找到食物的名称并将其提供给用户。现在我想知道我可以使用google colab pro作为人工智能服务器吗？
从用户那里获取文本并向其发送答案的最佳方式是什么？
快速 API 好吗？
我可以在 google colab 中运行我的小 llama 应用程序，但我需要在服务器上运行我的应用程序，以便我可以随时使用它。
这是小美洲驼代码：
从变压器导入 AutoTokenizer、FlaxLlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“afmck/testing-llama-tiny”)
模型 = FlaxLlamaForCausalLM.from_pretrained(“afmck/testing-llama-tiny”)

input = tokenizer(“你好，我的狗很可爱”，return_tensors =“np”)
输出=模型（**输入）

# 检索下一个令牌的日志
next_token_logits = 输出.logits[:, -1]
]]></description>
      <guid>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</guid>
      <pubDate>Sat, 20 Jan 2024 13:07:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 TreeExplainer 绘制瀑布图</title>
      <link>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</link>
      <description><![CDATA[在 SHAP 中使用 TreeExplainer，我无法绘制瀑布图。
错误消息：
&lt;前&gt;&lt;代码&gt;---&gt; 17 shap.plots.waterfall(shap_values[0], max_display=14)
类型错误：瀑布图需要一个“Explanation”对象作为
`shap_values` 参数。

由于我的模型是基于树的，因此我使用 TreeExplainer（因为使用 xgb.XGBClassifier）。
如果我使用Explainer而不是TreeExplainer，我可以绘制瀑布图。
我的代码如下：
导入 pandas 作为 pd

数据 = {
    &#39;a&#39;: [1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8, 1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8],
    &#39;b&#39;: [2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10, 2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10],
    &#39;c&#39;: [1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1, 1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1],
    &#39;d&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],
    &#39;e&#39;: [1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1, 1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1],
    &#39;f&#39;: [1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1],
    &#39;g&#39;: [3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2],
    &#39;h&#39;: [1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5],
    &#39;我&#39;: [1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6],
    &#39;j&#39;: [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6],
    &#39;k&#39;: [3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1],
    &#39;r&#39;: [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(数据)

X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
y = df.iloc[:,11]

从 sklearn.model_selection 导入 train_test_split，GridSearchCV
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.30、random_state = 42)

将 xgboost 导入为 xgb
从sklearn.metrics导入accuracy_score，confusion_matrix，classification_report
从 sklearn.model_selection 导入 GridSearchCV

参数网格 = {
    &#39;最大深度&#39;：[6]，
    “n_估计器”：[500]，
    “学习率”：[0.3]
}


grid_search_xgboost = GridSearchCV(
    估计器 = xgb.XGBClassifier(),
    参数网格=参数网格，
    CV = 3,
    详细 = 2,
    职位数 = -1
）

grid_search_xgboost.fit(X_train, y_train)

print(&quot;最佳参数：&quot;, grid_search_xgboost.best_params_)
best_model_xgboost = grid_search_xgboost.best_estimator_

导入形状

解释器 = shap.TreeExplainer(best_model_xgboost)
shap_values = 解释器.shap_values(X_train)

shap.summary_plot（shap_values，X_train，plot_type =“条”）

shap.summary_plot(shap_values, X_train)

X_train.columns 中的名称：
    shap.dependence_plot(名称, shap_values, X_train)

shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0], matplotlib=True)

shap.decision_plot(explainer.expected_value, shap_values[:10], X_train.iloc[:10])

shap.plots.waterfall(shap_values[0], max_display=14)

问题出在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</guid>
      <pubDate>Sat, 20 Jan 2024 12:55:45 GMT</pubDate>
    </item>
    <item>
      <title>图文分类模型架构[关闭]</title>
      <link>https://stackoverflow.com/questions/77848540/image-text-classification-model-architecture</link>
      <description><![CDATA[图文分类的架构正确与否？
假设我们的数据集包含图像和相应的标题和类标签。现在我们的问题是模型以（图像+标题）作为输入并预测 class_label。现在，如果我们使用图像和文本嵌入，然后对文本嵌入使用注意机制，并将处理后的文本嵌入与图像嵌入连接起来，然后使用 3 个完全连接的层并预测类标签（多类分类，如“汽车”、“踏板车”） 、“巴士”等）总共有 25 个班级。这个架构正确与否？]]></description>
      <guid>https://stackoverflow.com/questions/77848540/image-text-classification-model-architecture</guid>
      <pubDate>Fri, 19 Jan 2024 19:56:27 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中具有多个层的简单 RNN，用于顺序预测</title>
      <link>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</link>
      <description><![CDATA[我得到了连续的时间序列数据。在每个时间戳，只有一个变量可供观察（如果我的理解是正确的，这意味着特征数量 = 1）。我想训练一个具有多个层的简单 RNN 来预测下一个观察结果。
我使用滑动窗口创建了训练数据，窗口大小设置为8。为了给出具体的想法，下面是我的原始数据、训练数据和目标。
示例数据
0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49
训练数据
&lt;前&gt;&lt;代码&gt;X =
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37


对应的目标是
&lt;前&gt;&lt;代码&gt;Y =
     0.53
     3.37
     0.49

我将批量大小设置为 3。但它给了我一个错误
运行时错误：input.size(-1) 必须等于 input_size。期望 8，得到 1
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入 torch.utils.data 作为数据
将 numpy 导入为 np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

类 RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        超级（RNNModel，自我）.__init__()
        self.hidden_​​dim = 3*input_sz
        self.n_layers = n_layers
        输出大小 = 1
        self.rnn = nn.RNN（input_sz，self.hidden_​​dim，num_layers = n_layers，batch_first = True）
        self.线性 = nn.Linear(self.hidden_​​dim, output_sz)

    def 前向（自身，x）：
        batch_sz = x.size(0)
        hide = torch.zeros(self.n_layers, batch_sz, self.hidden_​​dim) #初始化n_layer*batch_sz维度的隐藏状态数hidden_​​dim)
        out, 隐藏 = self.rnn(x, 隐藏)
        out = out.contigious().view(-1, self.hidden_​​dim)
        返回，隐藏

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型 = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(设备)
Y = Y.to(设备)
模型 = model.to(设备)
优化器 = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

加载器= data.DataLoader（data.TensorDataset（X，Y），shuffle=False，batch_size=3）

n_epoch = 10
对于范围内的历元（n_epoch）：
    模型.train()
    对于加载器中的 X_batch、Y_batch：
        Y_pred = 模型(X_batch)
        损失 = loss_fn(Y_pred,Y_batch)
        优化器.zero_grad()
        loss.backward()
        优化器.step()

    如果纪元 % 10 != 0:
        继续
        模型.eval()
        使用 torch.no_grad()：
            Y_pred = 模型(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(“纪元 %d: 训练 RMSE %.4f” % (纪元, train_rmse))


我做错了什么？谁能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</guid>
      <pubDate>Fri, 19 Jan 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>不平衡数据的隔离森林和SHAP过程</title>
      <link>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</link>
      <description><![CDATA[我想对类别为正常 99.93% 异常 0.07% 的数据使用隔离森林，并使用 SHAP 检查异常数据特征之间的相关性。
于是，我参考了以下kaggle网站上的方法继续学习：https://www.kaggle.com/code/sabanasimbutt/anomaly-detection-using-unsupervised-techniques)
在这个 Kaggle 站点上，Class = 0 的数据和 Class = 1 的数据划分如下：
inliers = df[df.Class==0]
ins = inliers.drop([&#39;Class&#39;], axis=1)

离群值 = df[df.Class==1]
outs = outliers.drop([&#39;Class&#39;], axis=1)

为了查看学习中使用的特征与异常值（“Class == 1”的数据）之间的相关性，我按如下方式使用了 SHAP，并通过蜂群图检查了相关性。
&lt;前&gt;&lt;代码&gt;状态= 42
ISF = 隔离森林（random_state=状态）
ISF.fit(ins)

normal_isf = ISF.predict(ins)
欺诈_isf = ISF.predict(outs)

导入形状
解释器 = shap.TreeExplainer(ISF)
shap_values = 解释器(outs)
shap.plots.beeswarm(shap_values)

代码工作正常，但 beeswarn 的结果与我使用 shap_values ​​=explainer(ins) 时类似，即正常数据。我是不是搞错了？]]></description>
      <guid>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</guid>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pre-Train Bert 进行二元分类的 Shap 值：如何提取摘要图？</title>
      <link>https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar</link>
      <description><![CDATA[我使用预训练 bert 模型进行二元分类。用小数据训练我的模型后，我想提取这样的摘要图 我想要的图&lt; /a&gt;.然而，我想用文字来代替这些重要的特征。
但是，我不确定一切都好，因为 shap_value 的形状只是二维的。其实，这是有道理的。尽管如此，我没有得到图表，因为如果我使用这段代码，我遇到了两个问题：
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance[&#39;features&#39;].tolist(),features=comments_text)`

问题太不明智了：如果我用 shap_values 或 shap_values[0] 或  更改 shap_values[:,:10] shap_values.values vb.我总是遇到
516：断言 len(shap_values.shape) != 1，“汇总图需要一个矩阵
shap_values，而不是向量。” ==&gt; AssertionError：摘要图需要一个矩阵
shap_values，不是向量。

（拳头问题）
顺便说一句，我的 shap_value 由 10 个输入（shape_value.shape）组成。如果我选择范围从 1 到 147 的最大值，那么绘制图表就一切顺利。然而，此时，该图不合适：我的图仅由蓝点组成（-第二个问题-）。像这样只有蓝色。
注意：shap_values[:,:10]如果数字（10）改变不同的数字，图表显示不同的单词，但图表的总数相同（最多 20）。只有部分词序可以改变。
最小可重现示例：
导入nlp
将 numpy 导入为 np
将 pandas 导入为 pd
将 scipy 导入为 sp
进口火炬
进口变压器
进口火炬
导入形状

# 加载 BERT 情感分析模型
tokenizer = Transformers.DistilBertTokenizerFast.from_pretrained(
    “distilbert-base-uncased”
）
模型 = Transformers.DistilBertForSequenceClassification.from_pretrained(
    “distilbert-base-uncased-finetuned-sst-2-english”
).cuda()


如果 torch.cuda.is_available():
    设备 = torch.device(“cuda”)
    print(&#39;我们将使用 GPU:&#39;, torch.cuda.get_device_name(0))

别的：
    print(&#39;没有可用的 GPU，请使用 CPU。&#39;)
    设备 = torch.device(“CPU”)

定义 f(x):
    # 对批量句子进行编码
    输入 = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=True, return_attention_mask=True,padding=&#39;max_length&#39;,truncation=True,return_tensors=&#39;pt&#39;)

    # 将张量发送到与模型相同的设备
    input_ids = 输入[&#39;input_ids&#39;].to(设备)
    注意掩码 = 输入[&#39;注意掩码&#39;].to(设备)
    ＃ 预测
    使用 torch.no_grad()：
        输出=模型（input_ids，attention_mask=attention_masks）[0].detach（）.cpu（）.numpy（）
    分数 = (np.exp(输出).T / np.exp(输出).sum(-1)).T
    val = sp.special.logit(scores[:, 1]) # 使用 1 与其余 logit 单位
    返回值
# 使用 token masker 构建一个解释器
解释器 = shap.Explainer(f, tokenizer )

imdb_train = nlp.load_dataset(“imdb”)[“火车”]
shap_values = 解释器(imdb_train[:10],fixed_context=1,batch_size=16)
队列 = {“”：shap_values}
team_labels = 列表(cohorts.keys())
team_exps = 列表(cohorts.values())
对于范围内的 i(len(cohort_exps))：
    如果 len(cohort_exps[i].shape) == 2:
        队列_exps[i] = 队列_exps[i].abs.mean(0)
特征=cohort_exps[0].data
特征名称=同类群组表达式[0].特征名称
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
值 = np.array([cohort_exps[i].i 在范围内的值(len(cohort_exps))])
feature_importance = pd.DataFrame(list(zip(feature_names, sum(values))), columns=[&#39;features&#39;, &#39;importance&#39;])
feature_importance.sort_values(by=[&#39;重要性&#39;], 升序=False, inplace=True)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance[&#39;features&#39;].tolist(),features=imdb_train[&#39;text&#39;][10:20],show=False)


上面的代码产生相同的结果。我花了大约200台电脑，但没有成功:(。我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar</guid>
      <pubDate>Tue, 09 Jan 2024 08:49:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 object_Detector.EfficientDetLite4Spec tensorflow lite 继续使用检查点进行训练</title>
      <link>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</link>
      <description><![CDATA[很重要的是，我已经在 config.yaml 中设置了我的 EfficientDetLite4 模型“grad_checkpoint=true”。并且它已经成功生成了一些检查点。但是，当我想继续基于这些检查点进行训练时，我不知道如何使用它们。
每次我训练模型时，它都会从头开始，而不是从检查点开始。
下图是我的colab文件系统结构：

下图显示了我的检查点存储的位置：

以下代码显示了我如何配置模型以及如何使用模型进行训练。
将 numpy 导入为 np
导入操作系统

从 tflite_model_maker.config 导入 ExportFormat
从 tflite_model_maker 导入 model_spec
从 tflite_model_maker 导入 o​​bject_ detector

将张量流导入为 tf
断言 tf.__version__.startswith(&#39;2&#39;)

tf.get_logger().setLevel(&#39;错误&#39;)
从absl导入日志记录
日志记录.set_verbosity（日志记录.错误）

训练数据、验证数据、测试数据 =
    object_Detector.DataLoader.from_csv(&#39;csv_path&#39;)

规格 = object_ detector.EfficientDetLite4Spec(
    uri=&#39;/内容/模型&#39;,
    model_dir=&#39;/content/drive/MyDrive/MathSymbolRecognition/CheckPoints/&#39;,
    hparams=&#39;grad_checkpoint=true,策略=gpus&#39;,
    epochs=50，batch_size=3，
    steps_per_execution=1， moving_average_decay=0，
    var_freeze_expr=&#39;(efficientnet|fpn_cells|resample_p6)&#39;,
    tflite_max_detections=25，策略=spec_strategy
）

model = object_ detector.create(train_data, model_spec=spec, batch_size=3,
    train_whole_model=True，validation_data=validation_data）
]]></description>
      <guid>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</guid>
      <pubDate>Tue, 05 Oct 2021 04:21:43 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 cross_val_score 与 cross_val_predict</title>
      <link>https://stackoverflow.com/questions/66034846/machinelearning-cross-val-score-vs-cross-val-predict</link>
      <description><![CDATA[在构建通用评估工具时，我遇到了以下问题，其中 cross_val_score.mean() 给出的结果与 cross_val_predict 略有不同。
为了计算测试分数，我有以下代码，它计算每次折叠的分数，然后计算所有折叠的平均值。
testing_score = cross_val_score(clas_model, algo_features, algo_featurest, cv=folds).mean()

为了计算 tp、fp、tn、fn，我有以下代码，它计算所有折叠的这些指标（我假设是总和）。
test_clas_predictions = cross_val_predict(clas_model, algo_features, algo_featurest, cv=folds)
test_cm = fusion_matrix(algo_featurest, test_clas_predictions)
test_tp = test_cm[1][1]
test_fp = test_cm[0][1]
test_tn = test_cm[0][0]
test_fn = test_cm[1][0]

这段代码的结果是：
 算法测试 test_tp test_fp test_tn test_fn
5 高斯NB 0.719762 25 13 190 71
4 Logistic回归 0.716429 24 13 190 72
2 决策树分类器 0.702381 38 33 170 58
0 梯度提升分类器 0.682619 37 36 167 59
3 KNeighborsClassifier 0.679048 36 36 167 60
1 随机森林分类器 0.675952 40 43 160 56

因此，选择第一行 cross_val_score.mean() 给出 0.719762 （测试）并通过计算分数 25+190/25+13+190+71=0.719063545150... ((tp+tn)/(tp+tn +fp+fn)) 略有不同。
我有机会从 quora 的一篇文章中读到这一点：“在 cross_val_predict() 中，元素的分组方式与 cross_val_score() 中的稍有不同。这意味着当您使用这些函数计算相同的指标时，您可能会得到不同的结果。”
这背后有什么特殊原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/66034846/machinelearning-cross-val-score-vs-cross-val-predict</guid>
      <pubDate>Wed, 03 Feb 2021 20:02:56 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：条件需要布尔数组，而不是对象</title>
      <link>https://stackoverflow.com/questions/65836268/valueerror-boolean-array-expected-for-the-condition-not-object</link>
      <description><![CDATA[我正在尝试使用 sklearn 构建一个分类器，并在运行代码时在控制台中收到以下错误。
ValueError：条件需要布尔数组，而不是对象

我尝试调整我的数据（填充空值）以及使用重塑属性（但无济于事）。
这是相关代码
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从sklearn.metrics导入confusion_matrix
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.externals 导入 joblib

# 获取数据集
数据集 = pd.read_csv(&#39;master_info_final_v12_conversion.csv&#39;)

# 将数据集拆分为特征和标签
X = 数据集[数据集[[&#39;快乐&#39;, &#39;压力&#39;, &#39;眼睛&#39;]]]
y = 数据集[数据集[&#39;表型&#39;]]

# 将数据集拆分为训练数据和测试数据
X_train, X_test, y_train, y_test = train_test_split(X, y)

# 构建分类器并进行预测
分类器 = DecisionTreeClassifier()
分类器.fit(X_train, y_train)
预测 = classifier.predict(X_test)

# 打印混淆矩阵
打印（confusion_matrix（y_test，预测））

# 将模型保存到磁盘
joblib.dump(classifier, &#39;classifier.joblib&#39;)

这是我的数据快照：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

姓名
评分
表型
快乐
压力
眼睛


&lt;正文&gt;

汤米
7.1
男孩
56
23
19


吉尔
2.3
女孩

74
57


卡洛斯
4.4
都不是
45





]]></description>
      <guid>https://stackoverflow.com/questions/65836268/valueerror-boolean-array-expected-for-the-condition-not-object</guid>
      <pubDate>Thu, 21 Jan 2021 21:51:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 cross_val_predict 与 cross_val_score 时，scikit-learn 分数不同</title>
      <link>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</link>
      <description><![CDATA[我预计这两种方法都会返回非常相似的错误，有人可以指出我的错误吗？
计算 RMSE...
rf = RandomForestRegressor(random_state=555，n_estimators=100，max_深度=8)
rf_preds = cross_val_predict(rf, train_, 目标, cv=7, n_jobs=7)
print(&quot;使用 cv preds 的 RMSE 分数：{:0.5f}&quot;.format(metrics.mean_squared_error(targets, rf_preds, squared=False)))

分数 = cross_val_score(rf, train_, 目标, cv=7, 评分=&#39;neg_root_mean_squared_error&#39;, n_jobs=7)
print(&quot;使用 cv_score 的 RMSE 分数: {:0.5f}&quot;.format(scores.mean() * -1))


使用 cv preds 的 RMSE 分数：0.01658
使用 cv_score 的 RMSE 分数：0.01073
]]></description>
      <guid>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</guid>
      <pubDate>Thu, 04 Jun 2020 18:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>