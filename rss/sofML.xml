<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 29 Aug 2024 06:23:17 GMT</lastBuildDate>
    <item>
      <title>Azure ML 查询 - 后续端点生成期间未生成任何 Web 服务输入，且输入数据架构无效</title>
      <link>https://stackoverflow.com/questions/78925914/azure-ml-query-no-web-service-input-generated-and-void-input-data-schema-during</link>
      <description><![CDATA[我正尝试在 Azure ml 中创建实时推理管道，并遵循所有简单的步骤，但无法生成 Web 服务输入。仅生成 Web 服务输出。
此外，如果我手动添加 Web 服务输入或手动添加数据，在接下来的步骤中，即部署以创建端点，我无法在端点的“使用”部分中获取填充的数据模式，因此我无法使用输入数据运行我的预测。 （如果我有输入的数据模式，我会创建一个笔记本并在那里运行输入以获得我的预测）。

我使用了各种计算，包括容器实例、akscompute，彻底检查了我的数据集。
我迫切需要任何解决方案，我非常激动，因为我已经尝试了所有可能的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78925914/azure-ml-query-no-web-service-input-generated-and-void-input-data-schema-during</guid>
      <pubDate>Thu, 29 Aug 2024 02:33:02 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种类型的分析和 ml 方法来预测 python 中具有分类独立变量的连续数据[关闭]</title>
      <link>https://stackoverflow.com/questions/78925814/what-type-of-analysis-and-ml-methodology-should-i-use-to-predict-continuous-data</link>
      <description><![CDATA[我有一个已经清理过的数据集，下面附上了示例数据。（实际数据集实际上有 140 个因变量，这里我只添加了 X1~X7）
Y 列是我想要预测的属性
而 X1 ~ X7 列是我用来预测 Y 结果的变量。
X1 ~ X7 列的 1 表示真，0 表示假。
在此处输入图片说明

最好的分析方法是什么，以查看这些 X 变量与我的结果变量 Y 之间是否存在相关性？我可以使用那些 X 变量的不同组合来查看某些组合是否能更好地预测结果吗？ （我不知道该怎么做）

我应该使用什么样的机器学习方法来训练这个数据集来预测 Y 值。


我查过独热编码和随机森林。我仍然不完全理解，也没有看到任何似乎准确代表我的数据集的示例。]]></description>
      <guid>https://stackoverflow.com/questions/78925814/what-type-of-analysis-and-ml-methodology-should-i-use-to-predict-continuous-data</guid>
      <pubDate>Thu, 29 Aug 2024 01:40:54 GMT</pubDate>
    </item>
    <item>
      <title>序列到序列 LSTM 用于分类</title>
      <link>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</link>
      <description><![CDATA[我有一个包含两列的数据集：
past_events，future_events。
past_events 是 53 个数字代码的序列，如下所示
&#39;198&#39;、&#39;2000&#39;、&#39;197&#39;、&#39;85903&#39;，...
而 future_events 是 52 个数字代码的序列，如下所示
&#39;345&#39;、&#39;200&#39;、&#39;8904&#39;、&#39;23765&#39;，...
每个代码代表一个事件（代码按时间顺序排列）。
这个数据集中有近 3000 行。
我想构建一个 LSTM，它采用以下序列输入为 past_events，输出为 52 个事件的序列，这些事件是 future_events 的预测。
由于每个代码代表一个事件，在 future_events 中可能会出现 past_events 中不存在的代码，并且某些代码可能只出现在几行中，因此只出现在几个序列中。
这是我第一次做这种问题。如果是序列到值，我可以做到，但是对于这种分类的序列到序列，我不知道如何构建这个模型。
如果您能给我一些例子或一些解释此类问题的网站，我将不胜感激。
这是我构建 LSTM 的尝试，但没有成功
 input_1 = Input(shape=sequence_length)

lstm_out = LSTM(128, return_sequences=True)(input_1)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)

lstm_out = Lambda(lambda x: x[:, :52, :])(lstm_out)

# 注意机制
tention = Attention()([lstm_out, lstm_out])

density_out = TimeDistributed(Dense(128,activation=&#39;relu&#39;))(attention)
density_out = Dropout(0.2)(dense_out)

output_layer = TimeDistributed(Dense(num_classes,activation=&#39;softmax&#39;))(dense_out)

我能得到的最好结果是，预测都是相同的数字代码，例如 52 &#39;367&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</guid>
      <pubDate>Thu, 29 Aug 2024 00:33:59 GMT</pubDate>
    </item>
    <item>
      <title>如何利用机器学习来追踪公司员工的个人资料？</title>
      <link>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-employees-in-a-company</link>
      <description><![CDATA[我正在开展一个项目，旨在使用一个包含离职人员历史的数据库，并根据离职人员的个人资料计算在职员工离职的风险。
离职人员数据库 df_hist 包含有关自愿辞职的个人的信息，例如工作、种族、性别、薪水等。它的结构类似于以下示例：
日期 行动 姓名 薪水 种族 职位
&#39;05/10/2023&#39; &#39;自我辞职&#39; &#39;Ana&#39; &#39;10,000&#39; &#39;黑人&#39; &#39;IT&#39;
&#39;05/12/2024&#39; &#39;自我辞职&#39; &#39;John&#39; &#39;9,000&#39; &#39;白人&#39; &#39;空白&#39;
&#39;03/01/2023&#39; &#39;自我辞职&#39; &#39;Niel&#39; &#39;11,000&#39; &#39;空白&#39; &#39;数据科学家&#39;
&#39;03/01/2023&#39; &#39;自我辞职&#39; &#39;Isa&#39; &#39;10,000&#39; &#39;白人&#39; &#39;IT&#39;

数据库全部采用 object 格式，可能包含一些指定为“空白”的 NaN 值。
此外，我有一个数据库 df_active，其中包含活跃员工的历史记录。它包括参考日期和活跃个人的特征。这个数据库可能有同一个人的多个条目，因为他们可能已经更换了职位、获得了加薪或其他变化，所有这些都记录在相应的日期（并且他们的风险可能会随着修改而改变）。它的结构类似于下面的示例：
日期 行动 姓名 薪水 种族 职位
&#39;05/10/2023&#39; &#39;自我辞职&#39; &#39;Harry&#39; &#39;8,000&#39; &#39;白人&#39; &#39;数据科学家&#39;
&#39;10/10/2023&#39; &#39;自我辞职&#39; &#39;Harry&#39; &#39;10,000&#39; &#39;黑人&#39; &#39;数据科学家&#39; # 薪资变化
&#39;05/13/2024&#39; &#39;自我辞职&#39; &#39;Emma&#39; &#39;7,000&#39; &#39;白人&#39; &#39;空白&#39;
&#39;08/01/2024&#39; &#39;自我辞职&#39; &#39;Diana&#39; &#39;13,000&#39; &#39;空白&#39; &#39;数据科学家&#39;
&#39;10/01/2024&#39; &#39;自我辞职&#39; &#39;Diana&#39; &#39;13,000&#39; &#39;空白&#39; &#39;IT&#39; # 职位变动

我的目标是计算在职员工的流失风险。我想到了两种方法：

根据已辞职员工的数据库创建 100 个档案，按风险从高到低（100%）进行排序，然后确定与每个在职员工最接近的档案。在职员工的风险评分将基于所创建的 100 个档案的排名。

生成一个代表员工历史中最突出特征的档案，并计算此高风险档案与在职员工档案之间的相似度。


我已经开始构建代码，但我不确定要使用哪个变量作为目标，以及解决这个问题的最佳方法是什么。
from sklearn.tree import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

columns = [&#39;Action&#39;, &#39;Name&#39;, &#39;Salary&#39;, &#39;Race&#39;, &#39;Position&#39;]

# 转换：分类为数字
le = LabelEncoder()
for column in columns:
df_hist[coluna] = le.fit_transform(df_hist[coluna])

# 训练测试拆分
X = df_hist.drop(&#39;Action&#39;, axis=1)
y = df_hist[&#39;Action&#39;]

我该如何进行这第一步来追踪流失风险最高的人的个人资料？]]></description>
      <guid>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-employees-in-a-company</guid>
      <pubDate>Wed, 28 Aug 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>HSI 分类的补丁创建和 PCA 导致 Colab GPU 上的系统 RAM 崩溃</title>
      <link>https://stackoverflow.com/questions/78923134/patch-creation-and-pca-for-hsi-classification-causing-the-system-ram-on-colab-gp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923134/patch-creation-and-pca-for-hsi-classification-causing-the-system-ram-on-colab-gp</guid>
      <pubDate>Wed, 28 Aug 2024 11:39:52 GMT</pubDate>
    </item>
    <item>
      <title>MLP 回归器和简单的 ANN 模型有什么区别？[关闭]</title>
      <link>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</link>
      <description><![CDATA[简单的 ANN 模型和 MLP 回归器之间有什么区别吗？它们不一样吗？
我尝试在我的时间序列数据集中使用它们，当我使用 R2 分数来评估模型在测试集上的性能时，它们产生了不同的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</guid>
      <pubDate>Wed, 28 Aug 2024 11:36:19 GMT</pubDate>
    </item>
    <item>
      <title>ML Endpoint 在低流量期间延迟较高</title>
      <link>https://stackoverflow.com/questions/78922460/ml-endpoint-higher-latency-during-low-traffic</link>
      <description><![CDATA[在测试新的 ML 端点时，我们遇到了一个有点令人困惑的情况，在低流量时段，延迟从 200 毫秒 飙升至 1-3 秒。我们的模型有一个预处理步骤，然后是 XGBClassifier。我们正在使用托管在 AWS Sagemaker 上的 FastAPI 容器运行，并带有一个固定配置的实例。
基础设施/托管
我们在 AWS sagemaker 的一个中型实例上运行 fastapi 容器。这是一个固定实例，没有自动扩展。启动时，我们调用 uvicorn.run(&quot;invoke:app&quot;, etc.)。
我们的模型是什么/我们的调用端点做什么？
我们的模型对象是一个 sklearn Pipeline，由以下内容组成：

通过 FunctionTransformer 完成数据类型转换

通过 ColumnTransformer 进行特征工程

XGBClassifier


在我们的调用函数中，我们还有一个 TreeExplainer，我们使用它来获取每个预测的 SHAP 值。模型管道和 TreeExplainer 均在 asynccontextmanager 包装函数中实例化，该函数传递给应用程序生命周期：app = FastAPI(lifespan=load_model)
因此，本质上，我们的调用函数被定义为异步函数，它反序列化 JSON 有效负载，使用管道的前两个步骤预处理数据，使用 XGBClassifier 进行预测，然后在预处理的数据上运行解释器的 shap_values 方法。
我们的观察结果：
在生产环境中运行时，在正常工作时间内，我们可能会在任何给定时间点看到多个并发请求，端点看起来非常健康。 CPU 利用率在 20% 以下波动，内存利用率稳定在 8%，响应时间非常稳定，约为 200 毫秒。
当进入低营业时间时，您可能会长达一小时而看不到单个请求，我们看到响应时间超过整整 1 秒，有时甚至长达 3 秒。与此同时，内存利用率下降到 4-6%，然后在请求进入时回升到 8%。
此外，我们添加了一些分析，并在一定程度上复制了在本地 docker 容器上运行。我们看到，在让容器闲置一段时间后，第一个预测也需要一些额外的时间。但只有 ~800 毫秒。我们看到这段时间几乎全部花在 dtype 转换 + 执行 ColumnTransformer 上。
对我们来说，这一切都指向我们模型的“某些”部分，在空闲一段时间后从内存中删除。我无法判断这是由 python 的 gc 完成的，还是 sklearn 库中的一些内部函数完成的，或者是一些与 fastapi 相关的行为。
为什么会发生这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/78922460/ml-endpoint-higher-latency-during-low-traffic</guid>
      <pubDate>Wed, 28 Aug 2024 09:13:31 GMT</pubDate>
    </item>
    <item>
      <title>我没有在 YOLOV8 模型中获得预期的结果 [关闭]</title>
      <link>https://stackoverflow.com/questions/78921876/i-am-not-getting-expected-results-in-yolov8-model</link>
      <description><![CDATA[由于某些系统原因，我尝试使用 yolov8 模型，我已经在其他系统中训练了该模型并在该系统中进行了检查。我在那里给出了预期的结果。
然后我从该设备中取出训练好的模型并尝试在我的设备上进行测试。
但它给出了意想不到的结果。
让我在这里分享一些结果：
72.0 576.0 168.0 960.0 1.0 0.0
264.0 576.0 792.0 1008.0 1.0 0.0
360.0 528.0 888.0 1008.0 1.0 0.0
456.0 528.0 984.0 1008.0 1.0 0.0
264.0 528.0 1080.0 1008.0 1.0 0.0
888.0 480.0 984.0 1008.0 1.0 0.0
936.0 480.0 1080.0 912.0 1.0 0.0 1080.0 480.0 1224.0 912.0 1.0 0.0 1176.0 432.0 1320.0 912.0 1.0 0.0 1224.0 480.0 1368.0 1056.0 1.0 0.0 1272 0 480.0 1416.0 1056.0 1.0 0.0 1320.0 480.0 1464.0 1056.0 1.0 0.0 1368.0 480.0 1704.0 1056.0 1.0 0.0 1464.0 480.0 1992.0 1056.0 1.0 0.0 1080.0 480.0 2040.0 1056.0 1.0 0.0 1512.0 480.0 2232.0 1056.0 1.0 0.0 1656.0 480.0 2376.0 1056.0 1.0 0.0 18 00.0 432.0 2520.0 1056.0 1.0 0.0 1944.0 432.0 2664.0 1056.0 1.0 0.0 2088.0 432.0 2808.0 1056.0 1.0 0.0 2232.0 432.0 2952.0 1056.0 1.0 0.0
2376.0 432.0 3096.0 1056.0 1.0 0.0

最后两个是分数和 classID 为什么每帧的分数都是 1.0？
为什么每帧的结果都显示为 1，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78921876/i-am-not-getting-expected-results-in-yolov8-model</guid>
      <pubDate>Wed, 28 Aug 2024 06:41:20 GMT</pubDate>
    </item>
    <item>
      <title>ML.Net 预测始终显示 0 值</title>
      <link>https://stackoverflow.com/questions/78921852/ml-net-prediction-showing-0-value-always</link>
      <description><![CDATA[我的输入和输出模型
public class MLInput
{

public float[] 输入 { get; set; } // 处理任意数量的标签
public float 值 { get; set; } // 目标值
}

public class MLOutPut
{
public float 预测 { get; set; } // 预测值
}

示例 ML 输入对象

[
{&quot;输入&quot;:[480.291077,535.2749,8.185375,5.358807],&quot;值&quot;:187.421143},
{&quot;输入&quot;:[551.999756,548.0591,8.9576,4.947536],&quot;值&quot;:241.330811},
{&quot;I输入”:[513.7134,562.307739,9.582937,4.62406254],“值”:173.898956},
{“输入”:[495.53476,491.688538,9.700299,4.87886333],“值”:257.493927},
{“Inp uts&quot;：[514.7245,489.179443,8.354404,5.158723]，&quot;值&quot;：133.447586}，
{&quot;输入&quot;：[490.7611,450.309448,9.798013,4.991706]，&quot;值&quot;：243.573013}，
{&quot;输入&quot; ot;:[526.8138,441.718353,9.158744,5.07973528],&quot;Value&quot;:297.033051},
{&quot;Inputs&quot;:[551.9653,531.5339,8.307111,5.23646641],&quot;Value&quot;:162.436508},
]

我的代码是这样的
 MLContext mlContext = new MLContext();
var schemaDef = Microsoft.ML.Data.SchemaDefinition.Create(typeof(MLInput));
schemaDef[&quot;Inputs&quot;].ColumnType = new Microsoft.ML.Data.VectorDataViewType(Microsoft.ML.Data.NumberDataViewType.Single, inputCount);

IDataView dataView = mlContext.Data.LoadFromEnumerable(inputData,schemaDef);
var trainTestData = mlContext.Data.TrainTestSplit(dataView, testFraction: 0.2); // 此处应使用分割分数
var trainingDataView = trainTestData.TrainSet;
var testingDataView = trainTestData.TestSet;

var pipeline = mlContext.Transforms.Concatenate(&quot;Features&quot;, nameof(MLInput.Inputs)) // 连接所有输入特征
.Append(mlContext.Transforms.CopyColumns(&quot;Label&quot;, &quot;Value&quot;)) // 将目标值复制到 Label 列 
.Append(mlContext.Regression.Trainers.Sdca(labelColumnName: &quot;Label&quot;, featureColumnName: &quot;Features&quot;, maximumNumberOfIterations: maxIterations)); // 使用回归算法

var pre = dataView.Preview();
var dta = trainingDataView.Preview();
// 训练模型
var model = pipeline.Fit(trainingDataView);

// 评估模型
var predictions = model.Transform(testingDataView);
var metrics = mlContext.Regression.Evaluate(predictions, labelColumnName: &quot;Label&quot;);

// 输出评估指标
Console.WriteLine($&quot;R^2: {metrics.RSquared}&quot;);
Console.WriteLine($&quot;平均绝对误差: {metrics.MeanAbsoluteError}&quot;);

//使用模型进行预测

var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;MLInput, MLOutPut&gt;(mo​​del, inputSchemaDefinition: schemaDef);

var newData = new MLInput { Inputs = new float[] { 530.0f, 510.0f, 8.5f, 5.0f } };
var prediction = predictionEngine.Predict(newData);

Console.WriteLine($&quot;预测值：{prediction.Prediction}&quot;);

我的预测值始终为 0，有什么问题吗，请帮帮我，我是 ML.Net 新手，我刚刚开始使用样本。]]></description>
      <guid>https://stackoverflow.com/questions/78921852/ml-net-prediction-showing-0-value-always</guid>
      <pubDate>Wed, 28 Aug 2024 06:34:11 GMT</pubDate>
    </item>
    <item>
      <title>应用模型预测时出现属性错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</link>
      <description><![CDATA[代码
arr=[[90,42,43,20.879744,82.002744,6.502985,202.935536]]
y_predict=app.predict(arr)

第二行代码错误
AttributeError: &#39;tuple&#39; 对象没有属性 &#39;predict&#39;

第二行错误是什么]]></description>
      <guid>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</guid>
      <pubDate>Wed, 28 Aug 2024 04:31:39 GMT</pubDate>
    </item>
    <item>
      <title>预言机预测未反映外部回归量的预期行为</title>
      <link>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</guid>
      <pubDate>Wed, 28 Aug 2024 04:13:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 训练 LSTM 时，稍微增加批量大小会导致 CPU 使用率异常</title>
      <link>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</link>
      <description><![CDATA[我正在尝试使用 GPU 通过 pytorch 训练一个简单的 LSTM 模型。输入大小约为 11KB（与目标大小相同）。当我以 32 的批处理大小运行训练循环时，一切似乎都运行良好。CPU 使用率低于 10%（尽管感觉有点过高），GPU 使用率约为 30%...
当我将批处理大小增加到 48 时，CPU 使用率上升到 100%，一切都变慢了（训练时间增加了一倍）。GPU 使用率几乎没有变化，约为 30%。我检查了批处理大小为 40，这似乎没问题。在 48 时，似乎发生了一些变化并触发了持续的 CPU 使用率。其他一切似乎都很好（剩余大量 RAM，几乎没有任何磁盘活动等）。
如何解决此问题？批处理大小为 48 时，总输入大小应约为 528KB。不确定这是否会在某个地方触发某种阈值。
这是在 Windows 11 上使用 NVIDIA GPU 的情况。我希望训练能够顺利进行，直到批次大小比我看到的要大得多。]]></description>
      <guid>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</guid>
      <pubDate>Tue, 27 Aug 2024 23:03:51 GMT</pubDate>
    </item>
    <item>
      <title>运行 Jenkins 管道时如何修复“脚本返回退出代码 15”</title>
      <link>https://stackoverflow.com/questions/78885552/how-to-fix-script-returned-exit-code-15-when-running-jenkins-pipeline</link>
      <description><![CDATA[我正在使用 Jenkins 定义管道。我正在开发一个文本摘要器项目，并使用 jenkins 进行 CICD。触发管道后，在 CD 阶段我收到以下错误：
ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 &#39;cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/textsum:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d &#39;
Shell 脚本
1.6 秒
+ ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=****:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ****.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d 
--2024-08-18 19:19:08-- https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml
正在解析 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...
正在连接到 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... 已连接。
HTTP 请求已发送，正在等待响应... 200 OK
长度：95 [text/plain]
保存至：‘docker-compose.yml.10’
0K 100% 3.77M=0s
2024-08-18 19:19:08 (3.77 MB/s) - ‘docker-compose.yml.10’ 已保存 [95/95]
警告！您的密码将以未加密形式存储在 /home/ubuntu/.docker/config.json 中。
配置凭据助手以删除此警告。请参阅
https://docs.docker.com/engine/reference/commandline/login/#credential-stores
登录成功
yaml：第 229 行：此上下文中不允许映射值
脚本返回退出代码 15
Jenkins 2.462.1

我不知道如何解决这个问题：
我正在共享包含 jenkins 文件的 github repo。
https://github.com/mishraatharva/textsummarization
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78885552/how-to-fix-script-returned-exit-code-15-when-running-jenkins-pipeline</guid>
      <pubDate>Sun, 18 Aug 2024 20:03:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使 RandomForestClassifier 更快？</title>
      <link>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</link>
      <description><![CDATA[我正在尝试使用大约有 1M 原始数据的 Twitter 情绪数据从 kaggle 网站实现词袋模型。我已经清理了它，但在最后一部分，当我将特征向量和情绪应用于随机森林分类器时，它花费了太多时间。这是我的代码...
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100,verbose=3)
forest = forest.fit( train_data_features, train[&quot;Sentiment&quot;] )

train_data_features 是 1048575x5000 稀疏矩阵。我试图将其转换为数组，但执行时显示内存错误。
我哪里做错了？有人可以建议我一些来源或其他更快的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</guid>
      <pubDate>Wed, 26 Apr 2017 17:09:55 GMT</pubDate>
    </item>
    <item>
      <title>通过机器学习从网页中提取信息</title>
      <link>https://stackoverflow.com/questions/13336576/extracting-an-information-from-web-page-by-machine-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/13336576/extracting-an-information-from-web-page-by-machine-learning</guid>
      <pubDate>Sun, 11 Nov 2012 23:27:23 GMT</pubDate>
    </item>
    </channel>
</rss>