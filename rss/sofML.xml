<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Mar 2024 15:13:36 GMT</lastBuildDate>
    <item>
      <title>“内存分配失败”的原因是什么</title>
      <link>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</link>
      <description><![CDATA[我创建了一个简单的逻辑回归类，其中包含一个gradient_ascent 函数。当我尝试使用它时，我的 IDE 会出现错误，例如“无法为形状为 (86918, 86918) 和数据类型 float64 的数组分配 56.3 GiB”。我的数据大小是（86918,10），标签大小是（86918,1）。我调试的时候发现执行代码“error = y_predicted - y”时程序退出并报错在gradient_ascent函数中。但是，我找不到原因。以下是我的代码。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 precision_score
从 sklearn.model_selection 导入 train_test_split


定义 sigmoid(z):
    返回 1 / (1 + np.exp(-z))


逻辑回归类：
    def __init__(self,learning_rate=0.01,number_iterations=1000,method=&#39;gradient_ascent&#39;):
        自我学习率 = 学习率
        self.number_iterations = number_iterations
        自重=无
        自我偏见=无
        self.method = 方法

    defgradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            线性模型 = np.dot(X, self.weights) + self.bias
            y_预测 = sigmoid(线性模型)
            误差 = y_预测 - y
            dw = (1 / num_samples) * np.dot(X.T, 错误)
            db = (1 / num_samples) * np.sum(错误)

            self.weights += self.learning_rate * dw
            self.bias += self.learning_rate * db

    def stochastic_gradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            对于范围内的 i（num_samples）：
                线性模型 = np.dot(X[i], self.weights) + self.bias
                y_预测 = sigmoid(线性模型)
                误差 = y_预测 - y[i]

                dw = X[i] * 误差
                数据库=错误

                self.weights += self.learning_rate * dw
                self.bias += self.learning_rate * db

    def fit(自身, X, y):
        如果 self.method == &#39;gradient_ascent&#39;:
            self.gradient_ascent(X, y)
        别的：
            self.stochastic_gradient_ascent(X, y)

    def 预测（自身，X）：
        y_predicted = sigmoid(np.dot(X, self.weights) + self.bias)
        y_predicted = [1 如果 i &gt; 0.5 else 0 for i in y_predicted]
        返回 y_预测值


路径=&#39;../data/KaggleCredit2.csv&#39;
数据= pd.read_csv（路径，index_col = 0）
data.dropna（轴= 0，就地= True）
y = 数据[&#39;SeriousDlqin2yrs&#39;]
X = data.drop(标签=&#39;SeriousDlqin2yrs&#39;, 轴=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lr1 = 逻辑回归()
lr1.fit(X_train, y_train)
预测1 = lr1.预测(X_test)
lr1_score = precision_score(y_test, 预测1)

我尝试过查阅GPT并逐行调试代码，但仍然找不到错误。如果有人能给我一些指导，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</guid>
      <pubDate>Mon, 18 Mar 2024 14:22:18 GMT</pubDate>
    </item>
    <item>
      <title>（NaN Matlab）尝试复制一种收敛权重的算法，以近似合作差分博弈系统的联合成本函数</title>
      <link>https://stackoverflow.com/questions/78180810/nan-matlab-trying-to-replicate-an-algorithm-that-converges-weights-for-approxi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78180810/nan-matlab-trying-to-replicate-an-algorithm-that-converges-weights-for-approxi</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:32 GMT</pubDate>
    </item>
    <item>
      <title>你能在 h2o 的 PCA 函数中指定旋转吗？</title>
      <link>https://stackoverflow.com/questions/78180809/can-you-specify-a-rotation-in-h2os-pca-function</link>
      <description><![CDATA[我正在 h2o（R 版本）中运行 PCA，并且想知道是否可以指定/应用旋转（如 oblimin 或 promax）。我正在寻找旋转载荷，我使用 h2o 而不是其他常见包（如“psych”）的原因是我的数据集很大（100000 列），所以我需要利用 h2o Windows 中很好的并行计算。我当前使用的代码是：
库(h2o)

h2o.init(nthreads=64)

x &lt;- read.csv(“file_with_100000_columns.csv”)

for (i in 1:ncol(x)) {x[,i] &lt;- as.factor(x[,i])}

x &lt;- as.h2o(x)

mod &lt;- h2o.prcomp(training_frame=x,k=5,use_all_factor_levels=TRUE)

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78180809/can-you-specify-a-rotation-in-h2os-pca-function</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:31 GMT</pubDate>
    </item>
    <item>
      <title>如何设计神经网络来进行突破性检测</title>
      <link>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</link>
      <description><![CDATA[摘要：
我正在设计一个用于大鼠开颅手术的自动钻孔系统。我希望当神经网络检测到突破时钻头会停止。照片显示了该系统。
系统概述
手术结果
我尝试的：

力传感器和加速度计实时收集信号（采样率为3kHz）。因此，我用它们记录了一些钻孔过程（以恒定的进给速度和转速），并用 Python 绘制它们，如下所示：强制数据（蓝色代表原始数据，橙色代表黄油低通滤波数据）。来自 NIDAQmx 的振动数据
我希望使用神经网络进行检测（因为我可以轻松地识别图中“forcedata”中蓝色的突破点）。所以我训练一个CNN-LSTM，输入0.1秒内收集到的数据，输出一个0-1的数字，其中0表示不钻，1表示钻。（因为我认为突破点是1和0的交界处）。所以我手动将数据分为标签为1的数据集和标签为0的数据集如图所示。红色代表钻孔过程。标签1组是通过每0.1s分割区域来制作的
CNN-LSTM 网络代码如下：

类 CNNLSTMClassifier(nn.Module):
    def __init__(自身，hidden_​​size，num_layers)：
        超级（CNNLSTMClassifier，自我）.__init__（）
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=4, padding=1),
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 64，out_channels = 128，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 128，out_channels = 256，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d（内核大小=2，步幅=2）
        ）
        self.lstm = nn.LSTM(input_size=256,hidden_​​size=hidden_​​size,num_layers=num_layers,batch_first=True)
        self.fc = nn.Linear(hidden_​​dim, 1)
        self.sigmoid = nn.Sigmoid()

    def 前向（自身，x）：
        x = x.unsqueeze(1)
        x = self.cnn(x)
        x = x.permute(0, 2, 1)
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        输出 = self.sigmoid(输出)
        返回

我明白了
2024-03-18 16:25:50.708972 Epoch 1，训练损失 0.6764530851605625
2024-03-18 16:25:55.804250 Epoch 5，训练损失 0.6663112645497138
2024-03-18 16:26:02.211953 Epoch 10，训练损失 0.660625655507836
...................................................... ......................
2024-03-18 16:28:04.148844 Epoch 100，训练损失 0.6572404681613006

列车精度：0.63
准确度测试：0.66

所以效果很差。
我的期望：
我希望无论使用什么方法，神经网络都能检测到突破（可能是另一个神经网络或只是另一种方法）。
补充说明：我认为二元分类的想法是错误的。也许我应该检查的是每0.1s的数据是否有足够的整体下降趋势。]]></description>
      <guid>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</guid>
      <pubDate>Mon, 18 Mar 2024 12:52:29 GMT</pubDate>
    </item>
    <item>
      <title>机器学习过程中 TPU 到底用在什么地方？</title>
      <link>https://stackoverflow.com/questions/78180134/where-exactly-are-tpus-used-during-machine-learning</link>
      <description><![CDATA[我想知道当前技术水平中机器学习 TPU 期间通常使用哪些算法步骤。特别是，我很感兴趣它们是否用于推理、反向传播和/或卷积。
我知道脉动阵列的工作原理以及 TPU 的基本原理，并且它们可以比 CPU/GPU 更快地执行非稀疏矩阵乘法，这是有道理的。但例如对于卷积，相乘矩阵通常非常稀疏。在那里使用 TPU 仍然有意义吗？
如果有任何详细的解释或此类解释的链接，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78180134/where-exactly-are-tpus-used-during-machine-learning</guid>
      <pubDate>Mon, 18 Mar 2024 12:16:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用 LLM 的情况下将随机文件（具有不同名称）分组到适当的文件夹中？</title>
      <link>https://stackoverflow.com/questions/78179964/how-can-i-group-random-files-with-different-names-into-appropriate-folders-wit</link>
      <description><![CDATA[我想构建一个产品，可以根据文件名自动组织文件夹/子文件夹和文件，并根据文件名的相似性将它们分组。我知道这个问题可以通过 LLM（特别是 ChatGPT4）来解决，但这会带来一些问题。 1. 隐私，不是每个人都愿意将自己的文件名发送到这个系统。 2. 输入长度，OpenAI 上下文窗口有一定的长度限制，因此如果您有一个包含 1000 个或更多文件的文件夹，它将无法处理此长度。 3. 令牌成本，即使您可以处理 1000 个或更多文件，这也会变得非常昂贵。出于这个原因，我正在寻找替代方案。 （本地法学硕士可以解决隐私问题，但无法解决问题 2）。
我确信存在机器学习技术，可以根据单词背后含义的相似性将单词聚集在一起，但我还没有找到一个可以立即满足我需要的库。&lt; /p&gt;
这是我想要实现的目标的示例：
输入（文件名）：

历史家庭作业.pdf
报告.docx
山.png
历史测试.docx
天气数据.csv
地理笔记.docx
艺术品1.png
artwork2.png

输出（建议的文件夹名称）：

历史
地理
艺术
其他

请注意，输出文件夹不是预先确定的组。例如，输入可能是其他 10 个随机文件，您应该根据输入建议新名称的算法。
我找到的最接近的解决方案是在 python 中使用 WordNet 并查找上位词来对单词进行分组。问题是它不是很准确。]]></description>
      <guid>https://stackoverflow.com/questions/78179964/how-can-i-group-random-files-with-different-names-into-appropriate-folders-wit</guid>
      <pubDate>Mon, 18 Mar 2024 11:46:56 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在没有 Nvidia GPU 的情况下训练我的模型？</title>
      <link>https://stackoverflow.com/questions/78179649/is-there-a-way-to-train-my-model-without-an-nvidia-gpu</link>
      <description><![CDATA[所以我的实习项目涉及在我公司的代码库上训练/微调模型。因此，他们希望我不要使用任何云解决方案，以避免泄露敏感数据。
但是，我公司的笔记本电脑并不是最强大的：
CPU：第 11 代 Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 2.80 GHz
内存：16.0 GB
GPU：英特尔 Iris Xe 显卡
所以我不能使用cuda进行训练。据我了解，CPU 上的训练时间大约是 GPU 上的 30-35 倍，这将是一个很大的劣势。
除了使用云之外，我的另一个选择是向公司请求一台更好的笔记本电脑，但该笔记本电脑也具有相同的 GPU。该笔记本电脑的规格：
CPU：第12代Intel(R) Core(TM) i7-12800H 2.40GHz
内存：32.0 GB
GPU：英特尔 Iris Xe 显卡
我想使用的模型是 Microsoft 的 CodeReviewer (https://huggingface.co/microsoft/codereviewer），我想在我公司存储库的 PR 上对其进行微调，并利用模型的“审核评论生成”任务。
我将用于微调的数据集尚未准备好，因此不确定文件有多大，但 jsonl 中至少有 500-1000 行数据，所以数量不是很大，我猜大约 20 -30MB。
我尝试在 Pytorch 中使用 Huggingface 的转换器和加速库，并在一些示例数据集上训练模型以进行测试。该数据集约为 15-20MB（700 行），使用 Google Colab 的 T4 GPU 进行训练需要 4 分钟。如果我们将其乘以 30，则意味着 CPU 上的训练时间为 2 小时，这是相当多的时间。
所以我的问题是，是否可以在合理的时间内在我的笔记本电脑（或其他公司的笔记本电脑）上训练它，而无需花费很长时间，或者我唯一的解决方案是使用云解决方案？&lt; /p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/78179649/is-there-a-way-to-train-my-model-without-an-nvidia-gpu</guid>
      <pubDate>Mon, 18 Mar 2024 10:50:55 GMT</pubDate>
    </item>
    <item>
      <title>cross_val_predict 中是否有 xgb.XGBRegressor 的示例，其中回调=[early_stop], Early_stop=xgb.callback.EarlyStopping？</title>
      <link>https://stackoverflow.com/questions/78178902/is-there-example-of-xgb-xgbregressor-with-callbacks-early-stop-early-stop-xgb</link>
      <description><![CDATA[在文档
XGBClassifier 有一个 EarlyStopping：
&lt;前&gt;&lt;代码&gt;```
es = xgboost.callback.EarlyStopping(
    轮数=2，
    min_delta=1e-3,
    save_best=真，
    最大化=假，
    data_name=“validation_0”，
    metric_name=“mlogloss”,
    ）
clf = xgboost.XGBClassifier(tree_method=“hist”, device=“cuda”, 回调=[es])

X, y = load_digits(return_X_y=True)
clf.fit(X, y, eval_set=[(X, y)])```

但是“validation_0”是如何实现的？引用 clf.fit 中的 eval_set 来让 EarlyStopping 指标进行评估？
我尝试将其应用到 XGBRegressor：
`将 xgboost 导入为 xgb
从 sklearn.model_selection 导入 cross_val_predict，KFold
将 pandas 导入为 pd
将 numpy 导入为 np

类 CustomEarlyStopping(xgb.callback.EarlyStopping):
    def __init__(self, rounds=2, min_delta=1e-3, save_best=True, maximise=False, data_name=“validation_0”, metric_name=“rmse”):
        super().__init__(rounds=rounds, min_delta=min_delta, save_best=save_best, maximise=maximize, data_name=data_name, metric_name=metric_name)
    
# 火车模型（10x10 倍 CV）
cvx = KFold(n_splits=10, shuffle=True, random_state=239)
es = 自定义早期停止()

模型= xgb.XGBRegressor（colsample_bytree = 0.3，learning_rate = 0.1，max_深度= 10，alpha = 10，n_estimators = 500，n_jobs = -1，
                     random_state=239，回调=[es]）
model.set_params(tree_method=&#39;approx&#39;, device=&#39;cpu&#39;)

cv_preds = []
对于范围 (0,10) 内的 i：
    cv_preds.append(cross_val_predict(模型, np.asarray(X_train), np.asarray(y_train), cv=cvx, method=&#39;predict&#39;, n_jobs=1, verbose=2))`

我把data_name=“validation_0”放在在 EarlyStopping __init__ 中，而不在每个 cv 折叠中命名测试集。
这段代码的行为有什么问题？谢谢。
XGBRegressor 的代码返回了此错误：
ValueError：必须至少有 1 个验证数据集才能提前停止。

应该发生的情况是 cv_preds 被 10 个预测 y 的 ndarray 填充。]]></description>
      <guid>https://stackoverflow.com/questions/78178902/is-there-example-of-xgb-xgbregressor-with-callbacks-early-stop-early-stop-xgb</guid>
      <pubDate>Mon, 18 Mar 2024 08:42:57 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 Gradio Client API 使用图像进行预测</title>
      <link>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</link>
      <description><![CDATA[我正在尝试按照以下示例将图像发送到 Gradio Client API：
从“@gradio/client”导入{ client }；

const response_0 = 等待 fetch(“https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png”);
const exampleImage =等待response_0.blob();
                        
const app = 等待客户端(“airvit2/pet_classifier”);
const 结果 =等待 app.predict(“/预测”, [
                exampleImage, // &#39;img&#39; 图像组件中的 blob
    ]);

console.log(结果.数据);

但它返回此错误：
&lt;前&gt;&lt;代码&gt;{
    “类型”：“状态”，
    “端点”：“/预测”，
    “fn_index”：0，
    “时间”：“2024-03-17T18:36:53.270Z”，
    “队列”：正确，
    “消息”：空，
    “阶段”：“错误”，
    “成功”：假
}

这是我的 Gradio 代码：
from fastai.vision.all import *
将渐变导入为 gr

学习 = load_learner(&#39;model.pkl&#39;)

def 预测（img）：
    print(&quot;图片：&quot;, img)
    img = 加载图像(img)
    # img = PILImage.create(img)
    pred, pred_idx, probs = learn.predict(img)
    返回预测值

gr.Interface(fn = 预测，输入 = gr.Image(type=“pil”，高度 = 224，宽度 = 224)，输出 = gr.Label(num_top_classes = 3)).launch(share = True)


我尝试将图像格式更改为 Blob，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</guid>
      <pubDate>Sun, 17 Mar 2024 18:57:11 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 自定义模型不进行预测</title>
      <link>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</link>
      <description><![CDATA[我使用自定义训练的 Yolov8 模型来预测物理门是关闭还是打开。我已经在自定义数据集上训练了 Yolov8，但即使传递用于训练的相同数据，它也不会进行任何检测。
我使用了大约 300 张图像的数据集。
这是我的代码：
导入操作系统

从 ultralytics 导入 YOLO
导入CV2


VIDEOS_DIR = os.path.join(&#39;.&#39;, &#39;视频&#39;)

video_path = os.path.join(VIDEOS_DIR, &#39;样本门.mp4&#39;)
video_path_out = &#39;{}_out.mp4&#39;.format(video_path)

cap = cv2.VideoCapture(video_path)
ret, 框架 = cap.read()
H、W、_ = 框架.形状
out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))

model_path = os.path.join(&#39;.&#39;, &#39;运行&#39;, &#39;检测&#39;, &#39;训练&#39;, &#39;权重&#39;, &#39;last.pt&#39;)


model = YOLO(model_path) # 加载自定义模型


休息时：

    结果=模型（框架）[0]
    对于 results.boxes.data.tolist() 中的结果：
        x1, y1, x2, y2, 分数, class_id = 结果
        打印（x1，y1，x2，y2）

        cv2.矩形(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)
        cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)

    输出.write(帧)
    ret, 框架 = cap.read()

cap.release()
out.release()
cv2.destroyAllWindows()

以下是训练结果：https://i.stack.imgur。 com/huyZR.png]]></description>
      <guid>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:55 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的无头 GPT2 模型在保存时抛出错误 - 如何添加输入和输出层以及自定义 PositionalEmbedding</title>
      <link>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</link>
      <description><![CDATA[我想使用 GPT2 对序列数据进行回归任务，因此尝试从 Hugging Face 中找出无头 TFGPT2，代码如下：
配置 = GPT2Config(n_embd = embed_dim, n_head=num_heads)
基础模型 = TFGPT2Model（配置）
输入形状 = (1, 嵌入尺寸)
input1 = 层.Input(shape=input_shape, dtype=tf.float32)
positional_encoding = PositionalEmbedding(sequence_length, embed_dim)
解码器输入=位置编码（输入1）
Z = base_model(无，inputs_embeds=decoder_inputs)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“relu”））（Z.last_hidden_​​state）
模型= keras.Model（输入1，输出）
model.compile(loss=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam(beta_1=0.9，beta_2=0.98，epsilon=1.0e-9)，metrics=[tf.keras.metrics.RootMeanSquaredError()] ）
历史= model.fit（数据集，validation_data = val_dataset，epochs = epoch_len，verbose = 1）
tf.keras. saving. save_model(模型, r&#39;/drive/model_huggingface&#39;)

另请注意，我还使用自定义 PositionalEmbedding 类，因此可选的 input_embeds 参数传递给模型。
该模型训练并学习数据，但在尝试保存时会抛出错误：
AssertionError：尝试导出引用“未跟踪”资源的函数。由函数捕获的 TensorFlow 对象（例如 tf.Variable）必须通过将其分配给被跟踪对象的属性或直接分配给主对象的属性来“跟踪”。请参阅以下信息：
    函数名称 = b&#39;__inference_signature_wrapper_452514&#39;
    捕获的张量 = 
    可追踪引用此张量 = ;
    内部张量 = Tensor(“452144:0”, shape=(), dtype=resource)

我认为这是因为我向该模型添加了头部和自定义层。请让我知道您对我对如何实现此模型的解释的看法。]]></description>
      <guid>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</guid>
      <pubDate>Sun, 17 Mar 2024 14:03:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow federated 中工作时遇到“学习属性”错误</title>
      <link>https://stackoverflow.com/questions/78158329/facing-error-in-learning-attribute-while-working-in-tensorflow-federated</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78158329/facing-error-in-learning-attribute-while-working-in-tensorflow-federated</guid>
      <pubDate>Thu, 14 Mar 2024 05:35:24 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 HIV/AIDS 治疗预测建模 - 特征选择 [关闭]</title>
      <link>https://stackoverflow.com/questions/78133554/predictive-modeling-for-hiv-aids-treatment-in-python-feature-selection</link>
      <description><![CDATA[我正在研究健康分析领域的预测建模项目，特别关注使用 Python 优化 HIV/AIDS 治疗结果。我收集了具有各种特征的数据集，包括患者人口统计数据、治疗历史和实验室结果。
有关 EDA 的任何提示，以提高我的模型的准确性。此外，如果您能提供有关特征工程的一般想法或技巧，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78133554/predictive-modeling-for-hiv-aids-treatment-in-python-feature-selection</guid>
      <pubDate>Sat, 09 Mar 2024 19:01:33 GMT</pubDate>
    </item>
    <item>
      <title>scikeras.wrappers.KerasClassifier 返回 ValueError：无法解释指标标识符：loss</title>
      <link>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</link>
      <description><![CDATA[我正在研究 KerasClassifier，因为我想将其插入 scikit-learn 管道中，但我收到了前面提到的 ValueError。
以下代码应该能够重现我遇到的错误：
从 sklearn.model_selection 导入 KFold，cross_val_score
从 sklearn.preprocessing 导入 StandardScaler
从 scikeras.wrappers 导入 KerasClassifier
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从 sklearn.datasets 导入 load_iris
将 numpy 导入为 np

数据 = load_iris()
X = 数据.数据
y = 数据.目标

def create_model():
    模型=顺序（）
    model.add（密集（8，input_dim = 4，激活=&#39;relu&#39;））
    model.add（密集（3，激活=&#39;softmax&#39;））
    model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
                  优化器=&#39;亚当&#39;,
                  指标=[&#39;准确性&#39;])
    返回模型

clf = KerasClassifier(build_fn=create_model,
                      纪元=100，
                      批量大小=10，
                      详细=1)

管道=管道（[
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;clf&#39;，clf）
]）

kf = KFold(n_splits=5, shuffle=True, random_state=42)
结果= cross_val_score（管道，X，y，cv = kf）
print(&quot;交叉验证准确度：&quot;, np.mean(结果))

似乎我的模型正在随着纪元的运行而被编译。但是，之后我收到错误：
ValueError：无法解释指标标识符：丢失

tensorflow 和 scikeras 库的版本是：
scikeras==0.12.0
张量流==2.15.0

编辑：
最终我尝试了不同的库版本，以下内容让我成功运行了代码，看来问题是由 scikit-learn 的版本引起的：
scikeras==0.12.0
张量流==2.15.0
scikit学习==1.4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</guid>
      <pubDate>Fri, 01 Mar 2024 17:03:39 GMT</pubDate>
    </item>
    <item>
      <title>在 JavaScript 中加载经过训练的机器学习模型</title>
      <link>https://stackoverflow.com/questions/63395832/load-trained-machine-learning-model-in-javascript</link>
      <description><![CDATA[我正在 python 中使用 sklearn 来构建/训练一些模型（随机森林回归器、Kmeans、SVM，...），并且我想在 web 应用程序 Javascript/html 中使用这些经过训练的模型。有没有办法做到这一点？
我已经看到tensorflow.js允许使用keras模型做这样的事情。但在我看来，python 中的 TF/keras 仅限于神经网络。
我也见过sklearn-porter，但它似乎仅限于某些特定模型（主要是分类）。如果有人成功使用它，他们可以告诉我更多信息吗？
预先感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/63395832/load-trained-machine-learning-model-in-javascript</guid>
      <pubDate>Thu, 13 Aug 2020 13:04:36 GMT</pubDate>
    </item>
    </channel>
</rss>