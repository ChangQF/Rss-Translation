<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 01 Jun 2024 12:25:39 GMT</lastBuildDate>
    <item>
      <title>在 sagemaker 中部署 llama-3 8B 时出错：标记器不匹配？</title>
      <link>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</link>
      <description><![CDATA[尝试部署 meta/llama-3-8B-Instruct 时，我在 sagemaker 代码编辑器中收到以下错误：
“您从此检查点加载的 tokenizer 类与调用此函数的类的类型不同。这可能会导致意外的标记化。
您从此检查点加载的 tokenizer 类是“PreTrainedTokenizerFast”。
调用此函数的类是“LlamaTokenizer”。&quot;
我正在使用 Huggingface API 将模型从 HF Hub 直接加载到 sagemaker 代码编辑器，使用 AWS 中的 ml.g5xlarge 实例类型。使用 HF API 不需要或公开 tokenizer。]]></description>
      <guid>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</guid>
      <pubDate>Sat, 01 Jun 2024 09:35:46 GMT</pubDate>
    </item>
    <item>
      <title>Seq2Seq LSTM 模型是否适用于具有不变参数的多元时间序列预测</title>
      <link>https://stackoverflow.com/questions/78562876/should-the-seq2seq-lstm-model-be-used-for-multivariate-time-series-forecasting-w</link>
      <description><![CDATA[我正在使用 Seq2Seq LSTM 模型来预测两个时间序列变量（Value1 和 Value2）。我的数据集包含 2000 组实验，每组有多个时间步骤。每组包含 11 个与 value1 和 value2 相关的参数，请注意，与其他数据不同，每组中的 11 个参数不会随时间变化，但每组之间的 11 个参数并不相同。这是我的数据的示例图：
数据解释
您可以清楚地看到，我的数据集中的第 1 到第 11 个参数不会随时间变化，但每组中的 11 个参数不同，并决定了 value1 和 value2 的最终时间序列
我的问题是，选择 seq2seq，即编码器-解码器 LSTM 模型是最佳选择吗？或者这更像是回归分类问题？任何建议都将不胜感激
此外，我基于 Keras 构建了一个 seq2seq 模型，并尝试了两个输入，第一个输入仅使用 11 个参数时间序列（尽管随时间不变）作为 x_train，使用 value1&amp;2 时间序列作为 y_train，验证集的损失几乎没有下降。第二个输入是 11 个参数，value1&amp;2 的时间序列向前移动一个时间步，即 teacher-forcing 方法。但是在我的预测中我还需要y_test的前向时间步长的序列，我对此感到困惑，这样做是否意味着我正在使用y_test来预测y_test？
以下是第二个输入的代码示例：
#编码器
encoder_inputs = Input(shape=(X_train.shape[1], X_train.shape[2])) #基于X_train的输入形状
encoder_lstm1 = LSTM(32, return_sequences=True, return_state=True)
encoder_outputs1, state_h1, state_c1 =coder_lstm1(encoder_inputs)
encoder_lstm2 = LSTM(32, return_state=True)
encoder_outputs2, state_h2, state_c2 =coder_lstm2(encoder_outputs1)

#解码器
decoder_inputs =输入（形状=（X_train.shape[1]，y_train.shape[2]））# 两个目标
decoder_lstm = LSTM（32，return_sequences=True，return_state=True）
decoder_outputs，_，_ =coder_lstm（decoder_inputs，initial_state=[state_h2，state_c2]）
decoder_dense = Dense（y_train.shape[2]）
decoder_outputs =coder_dense（decoder_outputs）

# 模型
model = Model（[encoder_inputs，decoder_inputs]，decoder_outputs）

# 训练
decoder_input_data = np.zeros_like（y_train）
decoder_input_data[:，1:] = y_train[:，:-1] # 将 y_test 值向前移动一步

LSTM_model = model.fit（[X_train，解码器输入数据]，y_train，validation_split=0.2，epochs=100，batch_size=64，verbose=1)

# 测试
解码器输入测试 = np.zeros_like(y_test)
解码器输入测试[:, 1:] = y_test[:, :-1] # 这里我使用 y_test 作为测试的输入
y_pred = model.predict([X_test, 解码器输入测试])
]]></description>
      <guid>https://stackoverflow.com/questions/78562876/should-the-seq2seq-lstm-model-be-used-for-multivariate-time-series-forecasting-w</guid>
      <pubDate>Sat, 01 Jun 2024 05:26:17 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习进行文本分类</title>
      <link>https://stackoverflow.com/questions/78562439/text-classification-with-ml</link>
      <description><![CDATA[我目前正在新工作中开展我的第一个 ML 项目。任务是通过 ML 模型将收到的电子邮件分类到 4 个单独的邮箱以降低成本。我成功训练的模型在不平衡数据集上的概率几乎达到 90%。
现在的问题是，如何强制我的模型仅在预测准确率至少为 95% 时才对此类电子邮件进行分类。其余的将转到第 5 个邮箱进行人工分类。
有什么想法吗？非常感谢。
Michal
我希望我的模型只会对可以至少 95% 准确率预测的电子邮件进行分类。]]></description>
      <guid>https://stackoverflow.com/questions/78562439/text-classification-with-ml</guid>
      <pubDate>Fri, 31 May 2024 23:32:55 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 模型返回一个元组而不是张量[关闭]</title>
      <link>https://stackoverflow.com/questions/78562431/pytorch-model-returns-a-tuple-instead-of-tensor</link>
      <description><![CDATA[我使用 pytorch 在 python 中编写了一个程序。我的问题是：
我的 模型返回的是元组而不是张量。我如何让/强制我的模型返回张量？
这是我的代码：
import torch
from torch import nn
import numpy as np
from torch.optim import Adam

x = np.linspace(0, 1, 50).reshape((-1, 1)).astype(&#39;float32&#39;)
y = np.power(x, 2).reshape((-1, 1)).astype(&#39;float32&#39;)

x = torch.tensor(x)
y = torch.tensor(y)

model = nn.Sequential(
nn.Linear(1, 1),
nn.ReLU()
)

loss = nn.MSELoss()
opt = Adam(model.parameters(), lr=0.001)
model.train()
n_batch = 4
n_epoch = 100
for i in range(n_epoch):
for b in range(0, len(x), n_batch):
inp = x[b:b+n_batch]
out = y[b:b+n_batch]
opt.zero_grad()
pred = model(inp),
ls = loss(pred, out)
ls.backward()
opt.step()

这是我得到的错误：

文件 &quot;...\torch\nn\ functional.py&quot;，第 3355 行，在 mse_loss 中
if not (target.size() == input.size()): AttributeError: &#39;tuple&#39; 对象没有属性 &#39;size&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78562431/pytorch-model-returns-a-tuple-instead-of-tensor</guid>
      <pubDate>Fri, 31 May 2024 23:27:37 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该为图像贴上它的本质或外观的标签？[关闭]</title>
      <link>https://stackoverflow.com/questions/78562207/should-i-rather-label-my-image-for-what-it-is-or-what-it-looks-like</link>
      <description><![CDATA[对于某些人来说，这可能非常明显，但我正在重新考虑。
我想训练一个 CNN，它可以通过图像识别识别垃圾分类。也就是说，我希望能够拍摄例如麦片盒的照片，结果应该是“纸质”垃圾。
我目前正在清理我的数据集，因为有些图像属于错误的类别。
举个例子：利乐包的外面看起来像纸板箱。纸板通常属于纸质垃圾。但由于内部通常涂有某种塑料，利乐包实际上属于包装垃圾。
这对我来说非常直观，但我不确定神经网络是否能够以某种方式学习这种差异，或者将牛奶盒放入“包装”类别，将形状极其相似甚至由相同材料制成的麦片盒放入“纸”类别是否只会导致不准确的结果。有时，唯一的泄露是盒子的开口，盒子也是塑料的，但在某些图像上，由于角度原因，这个开口是看不见的。其他东西也是如此，比如揉成一团的纸（“纸”）和纸巾（“残留物”）。
简而言之：如果我有一张我知道属于 A 类的图像，但根据图像上可见的内容，它看起来与 B 类一模一样，我该把它归入哪一类？]]></description>
      <guid>https://stackoverflow.com/questions/78562207/should-i-rather-label-my-image-for-what-it-is-or-what-it-looks-like</guid>
      <pubDate>Fri, 31 May 2024 21:39:42 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型构建出现错误无法将‘51’转换为形状</title>
      <link>https://stackoverflow.com/questions/78562109/keras-model-building-get-error-cannot-convert-51-to-a-shape</link>
      <description><![CDATA[我正在制作一个 keras 模型来对人体姿势进行分类。我从链接中获取了代码
在 Colab 中它运行良好。我在本地出现以下错误
无法将“51”转换为形状。
def Landmarks_to_embedding(landmarks_and_scores):
&quot;&quot;&quot;将输入地标转换为姿势嵌入。&quot;&quot;&quot;
# 将平面输入重塑为具有形状=（17, 3）的矩阵
reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)

# 规范化 2D 地标
skylines = normalize_pose_landmarks(reshaped_inputs[:, :, :2])

# 将规范化的地标坐标展平为向量
embedding = keras.layers.Flatten()(landmarks)

return embedding

inputs = tf.keras.Input(shape=(51))
embedding = skylines_to_embedding(inputs)

layer = keras.layers.Dense(128,activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64,激活=tf.nn.relu6)(层)
层 = keras.layers.Dropout(0.5)(层)
输出 = keras.layers.Dense(len(class_names), 激活=&quot;softmax&quot;)(层)

模型 = keras.Model(输入，输出)
模型.summary()
]]></description>
      <guid>https://stackoverflow.com/questions/78562109/keras-model-building-get-error-cannot-convert-51-to-a-shape</guid>
      <pubDate>Fri, 31 May 2024 20:59:15 GMT</pubDate>
    </item>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我了解，predict_proba(xtest) 将给我以下结果：
投票给分类器的树数/树数

我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1： 
--- 5, 0 类 A（已选择） 
10 
--- 2, 3 类 B（未选择） 

树 2： 
--- 3, 2 类 A（已选择） 
10 
--- 5, 0 类 B（未选择）

predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0

期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8

有人知道如何做到这一点，或者他们正在使用什么实现？
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    <item>
      <title>当设备设置为“cuda”时，为什么 optuna 会对我的 CPU 而不是 GPU 施加压力？[关闭]</title>
      <link>https://stackoverflow.com/questions/78561318/why-is-optuna-stressing-my-cpu-instead-of-gpu-when-device-is-set-to-cuda</link>
      <description><![CDATA[我正在使用 optuna 进行超参数调整，尽管我的设备设置为“cuda”，并且它实际上在 cuda 上运行，因为在 CPU 上完成 10 个 epoch 需要 40 分钟，而目前，完成 30 个 epoch 只需要 6 分钟。这意味着，我的程序正在使用 GPU。但是，我检查发现 CPU 的压力已经达到 100%，而我的 GPU 几乎没有被程序利用。

这是我的硬件和软件规格：
硬件规格：
Lenovo Legion 5 2022
Ryzen 7 6800H
NVIDIA RTX 3060 TDP 140W
16 GB DDR5 RAM 4800 Mhz
1TB PCIE Gen 4 SSD
Optimus 已禁用（仅限 NVIDIA Dgpu）

软件规格：
python 3.10.9
conda 23.3.1
optuna 3.6.0 conda-forge
optuna-dashboard 0.15.1 conda-forge

我找不到任何方法可以解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78561318/why-is-optuna-stressing-my-cpu-instead-of-gpu-when-device-is-set-to-cuda</guid>
      <pubDate>Fri, 31 May 2024 17:00:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 keras 中添加具有可训练参数的自定义损失函数</title>
      <link>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</link>
      <description><![CDATA[我正在训练一个 LSTM 模型来预测未来的值。为此，我想定义一个与“mse”相同的自定义损失函数。但平方差将与指数项相乘，e^alpha。并且这个 alpha 项应该随着训练过程而更新。
我不确定我是否朝着正确的方向前进，但我已经用 mse 训练了模型，它通过使用真实数据给出了很好的预测。但作为现实生活中的预测，当我们在一段时间内没有真实数据时，我的模型应该使用最后一个预测作为模型下一个输入的输入，就像这样。在将模型作为此任务进行测试时，预测值不断偏离最后一个真实值。当一段时间后有新的真实数据可用时，它应该与未来的值相匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</guid>
      <pubDate>Fri, 31 May 2024 09:59:12 GMT</pubDate>
    </item>
    <item>
      <title>即使指定了某些列，Pandas 也会获取数据框的所有列</title>
      <link>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</guid>
      <pubDate>Fri, 31 May 2024 08:59:38 GMT</pubDate>
    </item>
    <item>
      <title>成本函数最小值太低</title>
      <link>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</link>
      <description><![CDATA[下面是我正在尝试编写的神经网络的一段 Matlab 代码。这是我第一次尝试与机器学习相关的任何事情。我在这里跟随 Michael Nielson 的书：http://neuralnetworksanddeeplearning.com/chap2.html
我正在加载一组 60000 张 28x28 灰度手写数字图像，并尝试训练这个神经网络来识别它们。还有一个包含 10000 张图像的测试数据集。该网络有 784 个输入神经元（28^2），两个隐藏层，每个隐藏层有 16 个神经元，输出层有 10 个神经元。我将权重和偏差初始化为 -0.5 和 0.5 之间的随机值。
我将成本函数评估为 C = 0.5*(a-y).^2。它似乎取得了一定成功，因为它从 C=1.35 开始，在 C=0.46 结束，然后基本趋于平稳（大约 75 个时期）。但是，错误仍然很高，只有 12% 的时间能猜出正确的数字，这几乎是随机的。我反复检查了数学，但找不到错误。我想一定有一个我没有看到的。下面的代码是主训练循环中的所有内容，因此任何错误都应该在那里。我没有将图像分成更小的批次，而是在每个时期一次处理整个 60k 图像。由于每张图像只有 28x28 像素，因此无需将其分开就足够快了。输入神经元 a_0 是一个 784x60000 的双精度数组，值介于 0 和 1 之间。我获取了原始图像，其中每个像素都是一个 uint8，然后将其转换为双精度，然后除以 255 得到 a_0。我在代码中对层进行编号，其中第 0 层是输入层，第 1 层和第 2 层是隐藏层，第 3 层是输出层。
a_0 = training_images;
epoch = 0;
while epoch &lt; 5 || C(epoch - 1) - C(epoch) &gt; 0.001 
epoch = epoch + 1;

%向前传播
z_1 = weights_1*a_0 + biases_1;
a_1 = sigmoid(z_1);
z_2 = weights_2*a_1 + biases_2;
a_2 = sigmoid(z_2);
z_3 = weights_3*a_2 + biases_3;
a_3 = sigmoid(z_3);

%评估成本函数
C(epoch) = 0.5*mean(sum((a_3-y).^2, 1));

%向后传播
sigmoid_d1 = a_1 .* (1-a_1); %Sigmoid 导数
sigmoid_d2 = a_2 .* (1-a_2);
sigmoid_d3 = a_3 .* (1-a_3);
delta_3 = (a_3-y).*sigmoid_d3;
delta_2 = weights_3.&#39;*delta_3 .* sigmoid_d2;
delta_1 = weights_2.&#39;*delta_2 .* sigmoid_d1;

%计算梯度
for image_index = 1:num_images
dC_dw3(:, :, image_index) = delta_3(:, image_index) * a_2(:, image_index).&#39;;
dC_dw2(:, :, image_index) = delta_2(:, image_index) * a_1(:, image_index).&#39;;
dC_dw1(:, :, image_index) = delta_1(:, image_index) * a_0(:, image_index).&#39;;
end

%计算调整
training_rate = 0.1;
adjust_biases_1 = -training_rate * mean(delta_1, 2);
adjust_biases_2 = -training_rate * mean(delta_2, 2);
adjust_biases_3 = -training_rate * mean(delta_3, 2);
调整权重1 = -训练速率 * 平均值(dC_dw1, 3);
调整权重2 = -训练速率 * 平均值(dC_dw2, 3);
调整权重3 = -训练速率 * 平均值(dC_dw3, 3);
偏差1 = 偏差1 + 调整偏差1;
偏差2 = 偏差2 + 调整偏差2;
偏差3 = 偏差3 + 调整偏差3;
权重1 = 权重1 + 调整权重1;
权重2 = 权重2 + 调整权重2;
权重3 = 权重3 + 调整权重3;
]]></description>
      <guid>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</guid>
      <pubDate>Fri, 31 May 2024 08:39:40 GMT</pubDate>
    </item>
    <item>
      <title>esp32-cam 使用人脸识别时出现错误 cam_hal: EV-VSYNC-OVF</title>
      <link>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</link>
      <description><![CDATA[我在 esp32-cam 板上使用示例“CameraWebServer”。上传设置如上所列：
主板：AI Thinker ESP32-CAM；
CPU 频率：240 MHZ；
闪存频率：80 Mhz；
闪存模式：QIO。
Arduino IDE 2.0.0
esp32 by Espressif 版本 2.0.14

使用这些设置，我可以上传我的代码，但面部识别功能不起作用。当我点击“注册面部”时，什么也没发生，我的串行监视器显示消息 EV-VSYNC-OVF。如何解决这个问题？
此外，我已经尝试修改上传设置并更改文件“CameraWebServer.ino”中的参数 config.frame_size 和 config.xclk_freq_hz，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</guid>
      <pubDate>Wed, 07 Feb 2024 22:12:55 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时出现列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（例如每分钟大约 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数低于一个恒定的条时，查询将运行，并且响应将正常接收。但是，一旦查询数量增加到上述条以上，就会出现以下错误：

IndexError - 列表索引超出范围

请注意，发生此错误的查询数“条”取决于每个查询的长度，并且如果查询的长度在程序执行过程中保持不变，则该查询数是一致的。例如，在尝试将查询长度增加大约 20% 后，查询数量从大约 330 个下降到大约 60 个。

文件
&quot;/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py&quot;,
line 1315, in text
return self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError: 列表索引超出范围

是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，其配额应仅为 300 个查询/分钟。由于我连续进行 API 调用，但低于 60 个查询/分钟的速率，因此我认为我的使用情况尚可。我目前正在使用免费的 VertexAI 试用帐户（免费赠送 300 美元信用额度）。
我编写的 Gemini Pro API 调用函数是：
def gemini_response(message: str) -&gt; str:
# 初始化 Vertex AI
vertexai.init(project=&quot;project-id-0123&quot;, location=&quot;us-central1&quot;)

# 加载模型
model = GenerativeModel(&quot;gemini-pro&quot;)

# 查询模型
response = model.generate_content(message)
return response.text

在调试 candidates 变量出了什么问题时，变量检查结果如下所示：
&gt; self 
&gt; prompt_feedback {block_reason: OTHER} 
&gt; usage_metadata {prompt_token_count: 505 total_token_count: 505 } 

&gt; self.candidates 
&gt; []

&gt; self._raw_response 
&gt; prompt_feedback {block_reason: OTHER}
&gt; usage_metadata {prompt_token_count: 505 total_token_count: 505 }
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>Optuna 在大量试验中建议相同的参数值（重复试验浪费时间和预算）</title>
      <link>https://stackoverflow.com/questions/64836142/optuna-suggests-the-same-parameter-values-in-a-lot-of-trials-duplicate-trials-t</link>
      <description><![CDATA[由于某种原因，Optuna TPESampler 和 RandomSampler 多次尝试对任何参数使用相同的建议整数值（也可能是浮点数和对数均匀值）。我找不到阻止它反复建议相同值的方法。在 100 次试验中，其中相当一部分只是重复的。唯一建议值计数最终在 100 次试验中约为 80-90。如果我包含更多参数进行调整，比如 3 个，我甚至会看到所有 3 个参数在 100 次试验中都获得了相同的值几次。
就像这样。min_data_in_leaf 的 75 被使用了 3 次：
[I 2020-11-14 14:44:05,320] 第 8 次试验结束，值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:07,876] 试验 9 完成，其值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:10,447] 试验 10 完成，其值为：45831.75933279074，参数为：{&#39;min_data_in_leaf&#39;: 43}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:13,502] 试验 11 完成，其值为：46125.39810101329，参数为：{&#39;min_data_in_leaf&#39;: 4}。最佳的是试验 4，其值为：45805.19030897498。
[I 2020-11-14 14:44:16,547] 试验 12 完成，其值为：45910.54012028659，参数为：{&#39;min_data_in_leaf&#39;: 75}。最佳的是第 4 次试验，其值为：45805.19030897498。
以下示例代码：
def lgb_optuna(trial):

rmse = []

params = {
&quot;seed&quot;: 42,
&quot;objective&quot;: &quot;regression&quot;,
&quot;metric&quot;: &quot;rmse&quot;,
&quot;verbosity&quot;: -1,
&quot;boosting&quot;: &quot;gbdt&quot;,
&quot;num_iterations&quot;: 1000,
&#39;min_data_in_leaf&#39;: trial.suggest_int(&#39;min_data_in_leaf&#39;, 1, 100)
}

cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)
for train_index, test_index in cv.split(tfd_train, tfd_train[:,-1]):
X_train, X_test = tfd_train[train_index], tfd_train[test_index]
y_train = X_train[:,-2].copy()
y_test = X_test[:,-2].copy()

dtrain = lgb.Dataset(X_train[:,:-2], label=y_train)
dtest = lgb.Dataset(X_test[:,:-2], label=y_test)

booster_gbm = lgb.train(params, dtrain, valid_sets=dtest, verbose_eval=False)

y_predictions = booster_gbm.predict(X_test[:,:-2])
final_mse = mean_squared_error(y_test, y_predictions)
final_rmse = np.sqrt(final_mse)
rmse.append(final_rmse)

return np.mean(rmse)

study = optuna.create_study(sampler=TPESampler(seed=42), direction=&#39;minimize&#39;) 
study.optimize(lgb_optuna, n_trials=100) 
]]></description>
      <guid>https://stackoverflow.com/questions/64836142/optuna-suggests-the-same-parameter-values-in-a-lot-of-trials-duplicate-trials-t</guid>
      <pubDate>Sat, 14 Nov 2020 16:33:17 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道使用 SelectKBest 选择了哪些功能？</title>
      <link>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</link>
      <description><![CDATA[运行 SelectKBest 后会选择一些特征，结果以数组形式返回，因此我不知道它们是什么特征，因为我的训练集有数千个特征。
我想在测试集中找到并挑选出这些特征，然后删除其余特征。有什么方便的方法吗？谢谢！
代码如下：
from sklearn.feature_selection import SelectKBest, f_regression
X_opt=SelectKBest(f_regression,k=2000)
X_new=X_opt.fit_transform(df_train_X_mm, train_y)
X_new`

结果如下：
array([[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
[0. , 0. , 0.00688335, ..., 0. , 0. ,
0. ],
[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
...,
[0. , 0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0.06257587，...，0. ，0. ，
0. ]])
]]></description>
      <guid>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</guid>
      <pubDate>Wed, 20 Jun 2018 07:25:42 GMT</pubDate>
    </item>
    </channel>
</rss>