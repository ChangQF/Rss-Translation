<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 11 Aug 2024 01:13:27 GMT</lastBuildDate>
    <item>
      <title>TensorBoard 中的 add_hparams() 函数无法正常工作</title>
      <link>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</link>
      <description><![CDATA[我试图向此 SummaryWriter 添加指标，但不起作用。
我正在使用 SummaryWriter 的 add_hparams() 函数，其详细信息可在此处找到：https://pytorch.org/docs/stable/tensorboard.html。
我这样做：
 writer = SummaryWriter(f&#39;runs/lstm_experiment_final&#39;)

for e in tqdm(range(num_epochs)):
tr_loss, tr_f1, tr_precision, tr_recall = training_loop(model, train_dataloader, loss_function, optimizer, e, writer)
val_loss, val_f1, val_precision, val_recall = validation_loop(model, test_dataloader, loss_function, e, writer)

metric_dict = {&#39;Loss/train&#39;: tr_loss, &#39;Loss/valid&#39;: val_loss,
&#39;F1/train&#39;: tr_f1, &#39;F1/valid&#39;: val_f1,
&#39;Precision/train&#39;: tr_precision, &#39;Precision/valid&#39;: val_precision,
&#39;Recall/train&#39;: tr_recall, &#39;Recall/valid&#39;: val_recall}
writer.add_hparams(best_params, metric_dict, global_step=num_epochs-1)
writer.close()

这就是正在发生的事情。
在此处输入图片描述
换句话说，超参数确实记录在 TensorBoard 上，但度量值却没有记录。
我希望有人已经看到我的问题并知道如何解决这个问题。
提前谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78857269/add-hparams-function-from-tensorboard-doesnt-work-properly</guid>
      <pubDate>Sun, 11 Aug 2024 00:21:56 GMT</pubDate>
    </item>
    <item>
      <title>启用 GPU 2024-OSError：[WinError 127] 找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</link>
      <description><![CDATA[我正在尝试启用 GPU 进行机器学习，但遇到了这个问题：

OSError：[WinError 127] 找不到指定的程序。错误
加载
“C:\Users\name\anaconda3\Lib\site-packages\torch\lib\c10_cuda.dll”
或其依赖项之一。

目前，我正在使用配备 NVIDIA 3050 GPU 的 Windows 笔记本电脑，以 Python 作为我的主要开发语言。

注意到该文件存在，因为 torch 库是使用 pip 直接下载的。因此，从技术上讲，此错误不应该发生。

最初，我收到了类似的错误“[WinError 126] 找不到指定的过程。”但我已经更新并安装了 VisualStudio 2022 的 C/C++ 编译器，它解决了 [WinError 126}，但我得到的却是 [WinError 127]。
要安装和启用 GPU，我将按照此教程执行以下步骤。

安装 VisualStudio -&gt; 全部下载

安装 Pytorch (pip) -&gt; CUDA 12.4

安装 CUDA 工具包 (12.6)

下载 cuDNN“下载 cuDNN v8.9.7 (2023 年 12 月 5 日)，适用于 CUDA 12.x”

将 cuDNN 的内容按照各自的文件夹名称粘贴到“NVIDIA GPU 计算工具包”中。


已验证已安装的 CUDA 已添加到环境中

我也按照这个最近更新的视频的建议下载 C/C++ 编译器。然而，这并没有解决我的问题。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78856582/enable-gpu-2024-oserror-winerror-127-the-specified-procedure-could-not-be-fo</guid>
      <pubDate>Sat, 10 Aug 2024 16:59:08 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：不可散列类型：'list' 带有 make_column_selector</title>
      <link>https://stackoverflow.com/questions/78856013/typeerror-unhashable-type-list-with-make-column-selector</link>
      <description><![CDATA[我正在尝试对我的数据进行一些预处理，以便进行销售预测。我使用 make_column_selector 选择特定列，以便将不同的编码器应用于不同的列。我试图创建一个 make 选择器列对象来访问特征变量 X 中的列。它对数字列很有效，但类别列却存在问题。每次我使用选择器对象选择类别数据中的列时，我都会得到 TypeError: unhashable type: &#39;list&#39;
在此处输入图片描述
# 设置种子
seed = 200

# 设置特征和目标变量
X = bigmart_copy.drop(&#39;Item_Outlet_Sales&#39;, axis = 1)
y = bigmart_copy.Item_Outlet_Sales

# 将数据拆分为训练集和测试集
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle = True, random_state=seed)

# 创建数值列列表
num_selector = make_column_selector(dtype_exclude=&#39;object&#39;)

# 创建两组类别列表，一组是序数类型，
# 另一组是名义类型
cat_selector_ord = [&#39;Item_Fat_Content&#39;, &#39;Outlet_Size&#39;, &#39;Item_MRP_Category&#39;, &#39;Outlet_location_type&#39;]
cat_selector_nom = [x for x in bigmart_copy.columns if x not in cat_selector_ord]

cat_selector_nom_nom = make_column_selector(pattern=cat_selector_nom)
cat_selector_ord_ord = make_column_selector(pattern=cat_selector_ord)

# 从数据中选择此列
num_cols = num_selector(X)
cat_cols_ord = cat_selector_ord_ord(X)

几行数据
TypeError Traceback（最近一次调用最后一次）
Cell In[29]，第 27 行
25 # 从数据中选择此列
26 num_cols = num_selector(X)
---&gt; 27 cat_cols_ord = cat_selector_ord_ord(X)
28 cat_cols_nom = cat_selector_nom_nom(X)
30 # 为每个选择器启动预处理器

TypeError：不可哈希类型：&#39;list&#39;

我原本期望不会出现任何错误，并且期望代码能够按预期工作。]]></description>
      <guid>https://stackoverflow.com/questions/78856013/typeerror-unhashable-type-list-with-make-column-selector</guid>
      <pubDate>Sat, 10 Aug 2024 12:33:38 GMT</pubDate>
    </item>
    <item>
      <title>需要用于 GenAI 图像生成的神经网络和深度学习模型</title>
      <link>https://stackoverflow.com/questions/78855910/need-a-neural-network-and-deep-learning-model-for-genai-image-generation</link>
      <description><![CDATA[我需要一个预先训练的模型，可以分析图像中存在的所有形状，并将它们转换为最接近的规则几何形状。图像应根据对称性和不完整曲线的完整性进行规则化。
我尝试使用 object_detection 模型，但发现它没有经过训练来规则化形状]]></description>
      <guid>https://stackoverflow.com/questions/78855910/need-a-neural-network-and-deep-learning-model-for-genai-image-generation</guid>
      <pubDate>Sat, 10 Aug 2024 11:40:24 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 无法正确预测给定的图像</title>
      <link>https://stackoverflow.com/questions/78855613/yolo-is-not-predicting-the-given-image-properly</link>
      <description><![CDATA[我第一次尝试 YOLO。
我正在按照视频教程操作，我编写了与他完全相同的代码，但它对我来说不起作用，我不知道为什么。
from ultralytics import YOLO
import numpy

model = YOLO(&quot;yolov8n.pt&quot;, &quot;v8&quot;)

output = model.predict(source=&quot;dog.jpg&quot;, conf=0.25, save=True)

print(output)
print(output[0].numpy())

我提供给模型的图像：

我得到的输出：

我已尝试使用其他图像，但结果相同。]]></description>
      <guid>https://stackoverflow.com/questions/78855613/yolo-is-not-predicting-the-given-image-properly</guid>
      <pubDate>Sat, 10 Aug 2024 08:55:24 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    <item>
      <title>构建 ML 模型时如何选择合适的标签</title>
      <link>https://stackoverflow.com/questions/78854998/how-to-choose-the-appropriate-label-when-building-a-ml-model</link>
      <description><![CDATA[我正在尝试为特定任务训练模型。
这里有一个简单的描述：
image1
image2
以下是两个不同数据集的屏幕截图：图 1 中的数据顺序正确，没有错误，也没有缺失数据。另一方面，图 2 中的数据是无序的，包含噪音，并且有缺失数据。
我想要训练一个模型，当给出图 2 中所示类型的数据作为输入时，该模型可以返回图 1 中所示类型的数据。
我尝试使用 RF 和 CNN 模型，但结果并没有像我预期的那样发展。我在想这可能是由于标签选择不正确造成的。
其实从图1中，很容易就能发现其中的联系。
例如，
1 2 3 4
A A-1 B B-1
A2 A2-1 B2 B2-1
A3 A3-1 B3 B3-1
A4 A4-1 B4 B4-1
图1这类数据中，A=A2-1，A2=A3-1\
因此，我希望模型能够学习到这种关系，然后在乱序的数据（图2）中找出正确的顺序。一旦确定了一个正确的序列，就可以通过递归得到正确且唯一的顺序。由于行与行之间是相互对应的（即 A A-1 B B-1 固定在同一行），一旦正确确定了一列的序列，整个序列也就正确确定了。
所以我尝试使用模型来解决这个问题。该模型能够运行并学到了一些东西，但没有学到任何有用的东西。我开始意识到问题可能在于标签选择。（事实上，这个问题可能可以用算法来解决，但我想用机器学习来实现它。）
如何选择标签并分割训练集和验证集？]]></description>
      <guid>https://stackoverflow.com/questions/78854998/how-to-choose-the-appropriate-label-when-building-a-ml-model</guid>
      <pubDate>Sat, 10 Aug 2024 01:22:04 GMT</pubDate>
    </item>
    <item>
      <title>如何集成一项功能来识别手绘形状并实时重新绘制它[关闭]</title>
      <link>https://stackoverflow.com/questions/78852946/how-to-integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real</link>
      <description><![CDATA[我正在构建一个项目，它可以跟踪我的手指运动并根据我的运动在屏幕上绘图。为此，我使用了 mediaPipe Hands 解决方案。我想集成一个功能来识别圆形、矩形等形状，并以理想的形式重新绘制它。
（当然是视频流）
我实现了一个功能来识别使用 OpenCV 在屏幕上绘制的形状。我尝试在绘图模式下捕获手指跟踪的点，并使用这些点来检测所绘制的形状是圆形还是矩形。我期望它在完成后准确识别形状。然而，实际发生的是，一旦我进入绘图模式，系统就开始将我的手指检测为圆形，并在手指移动到的每个点周围绘制一个圆圈。
我如何检测我正在绘制的形状，然后完美地重新绘制它们？
def understand_shape(points):
if len(points) &lt; 5：
return None

# 计算点的边界框
x_coords, y_coords = zip(*points)
min_x, max_x = min(x_coords), max(x_coords)
min_y, max_y = min(y_coords), max(y_coords)
width, height = max_x - min_x, max_y - min_y

# 使用半径方差检查圆
center_x, center_y = np.mean(x_coords), np.mean(y_coords)
radii = [distance.euclidean((x, y), (center_x, center_y)) for x, y in points]
mean_radius = np.mean(radii)
radius_variance = np.var(radii)

if radius_variance &lt; 1000：# 调整阈值以提高准确度
return (&quot;circle&quot;, (int(center_x), int(center_y), int(mean_radius)))

# 使用纵横比检查矩形
if 0.9 &lt; width / height &lt; 1.1：# 允许正方形略有偏差
if all(min_x &lt;= x &lt;= max_x and min_y &lt;= y &lt;= max_y for x, y in points):
return (&quot;rectangle&quot;, (min_x, min_y, max_x, max_y))

return None

def draw_shape(shape, imgCanvas):
if shape[0] == &quot;circle&quot;:
_, (center_x, center_y, radius) = shape
cv2.circle(imgCanvas, (center_x, center_y), radius, (0, 0, 255), 2)
elif shape[0] == &quot;rectangle&quot;:
_, (min_x, min_y, max_x, max_y) = shape
cv2.rectangle(imgCanvas, (min_x, min_y), (max_x, max_y), (0, 0, 255), 2)
]]></description>
      <guid>https://stackoverflow.com/questions/78852946/how-to-integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real</guid>
      <pubDate>Fri, 09 Aug 2024 13:06:37 GMT</pubDate>
    </item>
    <item>
      <title>将图像数据地理配准到 Google staelite 地图的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</guid>
      <pubDate>Fri, 09 Aug 2024 09:42:51 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TensorFlow 修剪我在 Python 中创建的神经网络？</title>
      <link>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network-i-created-in-python-using-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network-i-created-in-python-using-tensorflow</guid>
      <pubDate>Fri, 09 Aug 2024 07:50:59 GMT</pubDate>
    </item>
    <item>
      <title>在扩展中访问 NetLogo 扩展</title>
      <link>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</link>
      <description><![CDATA[我正在尝试开发一个 NetLogo 扩展来与不同的 LLMS（在线、离线）进行通信。LLM 调用返回 JSON 格式的字符串。我想解析 JSON 并将其转换为嵌套的 TABLE 对象，以访问 NetLogo Table 扩展。
是否有办法让一个扩展访问和使用另一个扩展中的类？]]></description>
      <guid>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</guid>
      <pubDate>Fri, 09 Aug 2024 03:21:02 GMT</pubDate>
    </item>
    <item>
      <title>在依赖项解析中强制使用标签</title>
      <link>https://stackoverflow.com/questions/78849363/enforce-labels-in-dependency-parsing</link>
      <description><![CDATA[我正在使用少量数据在 spaCy 中训练依赖解析器，如果我可以强制解析器

每个句子只有一个词根
将相同的标签应用于相同的词形/词形（例如 u 应始终为 cc）。

有办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78849363/enforce-labels-in-dependency-parsing</guid>
      <pubDate>Thu, 08 Aug 2024 15:56:21 GMT</pubDate>
    </item>
    <item>
      <title>我正在训练一个 VAE，但在我的批次中，我的 KLD 术语在执行几个步骤后就消失了</title>
      <link>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</link>
      <description><![CDATA[这是我的 VAE 损失代码：
def loss_function(x, x_hat, mean, logvar, beta=1.0):
    criterion = nn.MSELoss(reduction=&quot;mean&quot;)
    rebuilding_loss = criterion(x_hat, x)

    KLD = - 0.5 * torch.mean(1+ logvar - mean.pow(2) - logvar.exp())
    print(f&quot;KLD = {KLD}&quot;)
    return rebuilding_loss + KLD*beta

这是我的 VAE（简化版）：
class VariationalAutoEncoder(nn.Module):
def __init__(self, latent_shape):
super(VariationalAutoEncoder, self).__init__()

self.encoder = nn.Sequential(

nn.Linear(18,5184), # new
nn.LeakyReLU(0.2),
nn.Linear(5184,128), # new

)

# 潜在均值和方差对数 

self.mean_layer = nn.Linear(128, latent_shape)
self.logvar_layer = nn.Linear(128,latent_shape)

# 解码器
self.decoder = nn.Sequential(
nn.Linear(latent_shape,128),

nn.LeakyReLU(0.2),
nn.Linear(128,5184),

nn.LeakyReLU(0.2),
nn.Linear(5184,18), # new
nn.Sigmoid(),

)
def encode(self, x):
x = self.encoder(x)
mean, logvar = self.mean_layer(x), self.logvar_layer(x)
return mean, logvar

def reparameterization(self, mean, logvar):
std = torch.exp(0.5 * logvar)
epsilon = torch.randn_like(std).to(device) 
z = mean + std*epsilon
return z

def decrypt(self, z):
return self.decoder(z)

def forward(self, x):
mean, logvar = self.encode(x)
z = self.reparameterization(mean, logvar)

x_hat = self.decode(z)
return x_hat, mean, logvar


在我使用 beta 值后，KLD 的值会达到 10^-7 的数量级10^16。我不确定为什么会这样。我该怎么办？
我目前的超参数是：Adm Optimizer 的权重衰减为 10，lr 为 0.01，ReduceLROnPlateau 调度程序的耐心为 3，因子为 0.5。我在代码中使用梯度裁剪，max_norm 为 0.5，训练 25 个时期。
潜在形状为 10。
我尝试使用较小的值进行 beta 退火和循环退火，甚至使用 10^16 作为 beta 值的 beta KLD，但 KLD 项仍然消失为 10^-8。这就像它在我的任务上随机运行一样。我尝试添加一个卷积层，因为输入是一张大小约为 18 x 18 的图像。没有变化。]]></description>
      <guid>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</guid>
      <pubDate>Thu, 08 Aug 2024 12:10:40 GMT</pubDate>
    </item>
    <item>
      <title>无需深度学习或 Tesseract 的文本图像二元分类器</title>
      <link>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</link>
      <description><![CDATA[我有 20k 张小标签图像，每张图像都有单词“Back”或“Front”。
图像分辨率为全部 (200px, 25px)

我可以使用 tesseract_OCR 对这些图像进行 100% 准确率的分类。
 txt = pytesseract.image_to_string(img, lang=&#39;eng&#39;)
if &quot;Front&quot; in txt:
return &quot;Front&quot;
if &quot;Back&quot; in txt:
return &quot;Back&quot;

问题是，它太慢了（20k 张图像需要 1 小时）并且需要安装 OCR 包。
我知道即使是 3 层的简单 CNN 也能很好地运行，但我认为这个问题似乎可以用简单的算法解决，而不需要复杂的技术。
你能给我推荐一种新方法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</guid>
      <pubDate>Wed, 07 Aug 2024 06:46:36 GMT</pubDate>
    </item>
    <item>
      <title>Google 语音转文本和翻译（直播）</title>
      <link>https://stackoverflow.com/questions/78765868/google-speech-to-text-and-translation-live-stream</link>
      <description><![CDATA[我有一个用例，我将在直播中录制一段演讲，并且我希望实时获得音频的文本转录，然​​后翻译该转录。
我是否需要使用 Google 的语音转文本 API，然后将生成的文本发送到翻译 API，还是可以在一行中完成？]]></description>
      <guid>https://stackoverflow.com/questions/78765868/google-speech-to-text-and-translation-live-stream</guid>
      <pubDate>Thu, 18 Jul 2024 17:21:16 GMT</pubDate>
    </item>
    </channel>
</rss>