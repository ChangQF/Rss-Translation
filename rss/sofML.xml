<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 09 Jan 2024 12:26:00 GMT</lastBuildDate>
    <item>
      <title>寻求进入 MLOps 领域的指导</title>
      <link>https://stackoverflow.com/questions/77785935/seeking-guidance-to-start-in-mlops-field</link>
      <description><![CDATA[MLOps 社区您好，
我是一名预科学生，渴望开始我在 MLOps 领域的旅程。虽然我目前没有具体的问题或项目，但我正在寻求有关如何开始学习并为 MLOps 职业生涯做好准备的指导。
以下是我希望获得帮助的一些具体要点：

推荐学习资源：为 MLOps 概念和实践打下坚实基础的最佳资源、在线课程或书籍是什么？

关键技能和技术：作为初学者，我应该关注哪些基本技能和技术来为 MLOps 打下坚实的基础？

实践练习：是否有任何推荐的实践练习或项目可供我进行，以应用理论知识并获得 MLOps 的实践经验？

社区参与：是否有特定的社区、论坛或交流机会，让像我这样的初学者可以与 MLOps 领域经验丰富的专业人士互动？


我知道 MLOps 是一个广阔的领域，任何有关如何构建我的学习路径的指导或建议都将非常有价值。
预先感谢您分享您的见解！
作为一名渴望深入 MLOps 领域的预科学生，我已采取以下步骤来启动我的学习之旅：

研究学习资源：我已经开始研究与 MLOps 概念和实践相关的在线学习平台、课程和书籍。我正在确定最推荐的资源，以确保奠定坚实的基础。

确定关键技能：我一直在探索 MLOps 所需的基本技能和技术。最初，我专注于了解机器学习、云计算和版本控制系统的基础知识。

计划实践练习：虽然我还没有具体的实践项目，但我计划在获得基本了解后进行实践练习和项目。我愿意接受对初学者有益的具体练习建议。

探索社区：我已开始研究 MLOps 社区、论坛和交流机会。我正在积极寻找初学者可以与经验丰富的专业人士交流以寻求建议和指导的地方。


虽然我现阶段没有具体的问题可以分享，但我热衷于收到关于这些初始步骤是否符合 MLOps 初学者的最佳实践的建议。
感谢您提供的任何见解和指导！]]></description>
      <guid>https://stackoverflow.com/questions/77785935/seeking-guidance-to-start-in-mlops-field</guid>
      <pubDate>Tue, 09 Jan 2024 10:14:01 GMT</pubDate>
    </item>
    <item>
      <title>过多的 padding 导致 NN 模型精度下降</title>
      <link>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</link>
      <description><![CDATA[我训练了一个简单的神经网络模型来进行二元分类，并能够区分真假新闻
#创建模型的类
类 FakeNewsDetectionModelV0(nn.Module):
     def __init__(自身, input_size):
        超级().__init__()
        
        self.layer_1=nn.Linear(in_features=input_size, out_features=8)
        self.layer_2=nn.Linear(in_features=8, out_features=1) #从前一层获取5个特征并输出单个特征

     #定义一个forward()用于前向传播
     def 前向（自身，x，掩码）：
        
        # 应用掩码来忽略某些值
        如果掩码不是 None：
            x = x * 掩码

        x = self.layer_1(x)
        x = self.layer_2(x)
        返回x




我使用 CountVectorizer 将文本转换为列表，然后转换为张量
从 sklearn.feature_extraction.text 导入 CountVectorizer

矢量化器 = CountVectorizer(min_df=0, 小写=False)
矢量化器.fit(df[&#39;文本&#39;])

X=vectorizer.fit_transform(df[&#39;text&#39;]).toarray()

问题在于，由于数据集有超过 9000 个条目，因此训练模型的输入大小非常大（大约 120000 个）。因此，当我尝试对单个句子进行预测时，由于大小明显较小，我需要过度填充句子以使其适合模型的输入，这极大地影响了模型的准确性。
from io 导入 StringIO
来自 torch.nn.function 导入垫
导入字符串
进口再
从 nltk.tokenize 导入 word_tokenize
从 nltk.corpus 导入停用词
导入nltk
从 keras.preprocessing.text 导入 Tokenizer
从 keras.preprocessing.sequence 导入 pad_sequences


尝试：
    #nltk.download(&#39;停用词&#39;)
    nltk.download(&#39;punkt&#39;)
除了：
    print(&quot;下载停用词时出错&quot;)

def normalise_text (文本):

  text = text.lower() # 小写
  text = text.replace(r&quot;\#&quot;,&quot;&quot;) # 替换主题标签
  text = text.replace(r&quot;http\S+&quot;,&quot;URL&quot;) # 删除 URL 地址
  text = text.replace(r&quot;@&quot;,&quot;&quot;)
  text = text.replace(r&quot;[^A-Za-z0-9()!?\&#39;\`\&quot;]&quot;, &quot; &quot;)
  text = text.replace(&quot;\s{2,}&quot;, &quot;&quot;)
  文本 = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, 文本)
  返回文本

def fake_news_detection(df, model, model_input_size):
    预测=[]
    最大字数 = 10000
    最大长度 = 模型输入大小

    模型.eval()

    对于 df[&#39;text&#39;][:4000] 中的预测数据：
        预测数据=标准化文本（预测数据）

        #print([预测数据])



        # 使用CountVectorizer将文本数据转换为数组
        矢量化器 = CountVectorizer(min_df=0, 小写=False)
        Prediction_data_array = Vectorizer.fit_transform([prediction_data]).toarray()

        #tokenizer = Tokenizer(num_words=max_words)
        #tokenizer.fit_on_texts([预测数据])
        #sequences = tokenizer.texts_to_sequences([预测数据])


        #prediction_data_array = pad_sequences(序列, maxlen=max_length,value=-1.0)

        #print(预测数据数组.形状)

        # 检查转换后数据的形状
        当前输入大小=预测数据数组.形状[1]


        Prediction_data_tensor = torch.tensor(prediction_data_array, dtype=torch.float32)


        # 如果形状不匹配，则调整其大小
        如果当前输入大小！=模型输入大小：

            打印（当前输入大小）
            填充 = 模型输入大小 - 当前输入大小
            Prediction_data_tensor = pad(prediction_data_tensor, (0, 填充), &#39;常量&#39;, 值 = 0)
            mask_tensor = torch.ones_like(prediction_data_tensor)
            mask_tensor[:, -padding:] = 0 # 将填充区域中的值设置为 0
            #print(torch.unique(mask_tensor, return_counts=True))

            # 应用掩码来忽略某些值
            #预测数据张量 = 预测数据张量 * 掩码张量



        # 假设你的模型将 input_data 作为输入
        使用 torch.inference_mode()：
            预测 = torch.round(torch.sigmoid(model(prediction_data_tensor, mask_tensor))).squeeze()

        预测.append(round(预测.item()))

    print(f“我们的数据张量形状是 {prediction_data_tensor.shape}”)

    Predictions_tensor = torch.FloatTensor(预测)

    返回预测张量

有谁知道有什么解决方法可以让我将数据适合我的模型而不降低其准确性分数吗？
尝试：在对小尺寸数据进行预测时填充向量
预期：准确的预测类似于我在训练/评估过程中得到的结果
得到：预测不准确，准确度非常低（大约 43%）]]></description>
      <guid>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</guid>
      <pubDate>Tue, 09 Jan 2024 09:03:52 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中 NxM 密集层和 M 个独立 Nx1 密集层之间的梯度和优化差异</title>
      <link>https://stackoverflow.com/questions/77785480/gradient-and-optimization-differences-between-a-nxm-dense-layer-and-m-separate-n</link>
      <description><![CDATA[我很好奇这两种设计选择如何影响训练过程中的梯度计算和优化过程。
反向传播过程中每个结构的梯度计算会受到怎样的影响？梯度流回网络的方式有什么不同吗？这些不同的网络结构是否需要或受益于不同的优化算法或学习率？
我的目标是从梯度计算和优化的角度了解每种模型结构选择的潜在好处和可能的缺点。
对此问题的任何见解将不胜感激。谢谢。
我以为“渐变”是一样的，但结果是不同的。]]></description>
      <guid>https://stackoverflow.com/questions/77785480/gradient-and-optimization-differences-between-a-nxm-dense-layer-and-m-separate-n</guid>
      <pubDate>Tue, 09 Jan 2024 08:59:37 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 解释器获取错误的数据类型</title>
      <link>https://stackoverflow.com/questions/77785286/shap-explainer-getting-wrong-datatype</link>
      <description><![CDATA[这是我的代码。我正在尝试获取 X 射线图像的 SHAP 值。
导入火炬
将 numpy 导入为 np
将 torch.nn 导入为 nn
导入 torchvision.transforms 作为变换
从 torchvision.models 导入 alexnet
导入形状
从 PIL 导入图像
进口泡菜
导入 matplotlib
%matplotlib 内联

model_path = &#39;alexnet_lion_model.pkl&#39;
将 open(model_path, &#39;rb&#39;) 作为 f：
    模型 = pickle.load(f)

模型.eval()

＃ 转型
def preprocess_image(image_path):
    图像 = Image.open(image_path).convert(&#39;RGB&#39;)
    变换 = 变换.Compose([
        变换.调整大小((224, 224)),
        变换.ToTensor(),
        变换.Normalize(平均值=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),])
    input_image = 变换（图像）.unsqueeze（0）
    返回输入图像

image_path = &#39;C.jpg&#39;
输入图像 = 预处理图像（图像路径）

masker = shap.maskers.Image(“inpaint_telea”, input_image.size())

解释器= shap.Explainer（模型，掩码器，output_names = [“A”，“B”，“C”]）

shap_values = 解释器(input_image)

shap.image_plot(shap_values, input_image.numpy())

当我运行此命令时，解释器获取错误的数据类型，并且出现此错误：
 *（张量输入、张量权重、张量偏差、整数步幅元组、整数填充元组、整数膨胀元组、整数组）
      不匹配，因为某些参数具有无效类型： (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple (int, int)!, int)
 *（张量输入、张量权重、张量偏差、整数步幅元组、str 填充、整数膨胀元组、整数组）
      不匹配，因为某些参数具有无效类型： (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple (int, int)!, int)

上线：
shap_values = 解释器(input_image)

我想获取图像 C.jpg 的 SHAP 值]]></description>
      <guid>https://stackoverflow.com/questions/77785286/shap-explainer-getting-wrong-datatype</guid>
      <pubDate>Tue, 09 Jan 2024 08:26:58 GMT</pubDate>
    </item>
    <item>
      <title>哪种评估指标最适合不平衡的多标签分类，确保准确的模型评估？</title>
      <link>https://stackoverflow.com/questions/77785250/which-evaluation-metric-suits-imbalanced-multi-label-classification-best-ensuri</link>
      <description><![CDATA[如何处理？
在处理多标签分类中的平均指标时，应优先考虑采用微观平均、宏观平均、加权平均或样本平均等平均技术来综合评估模型的性能]]></description>
      <guid>https://stackoverflow.com/questions/77785250/which-evaluation-metric-suits-imbalanced-multi-label-classification-best-ensuri</guid>
      <pubDate>Tue, 09 Jan 2024 08:20:55 GMT</pubDate>
    </item>
    <item>
      <title>计算混淆矩阵的精度和召回率</title>
      <link>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</link>
      <description><![CDATA[正如主题所说，我必须计算混淆矩阵的精度和召回率。

二元分类器经过训练，可以区分恶意网络流量 $(y=1)$ 与正常流量 $(y=-1)$
当我运行代码时：
 precision = 3805 / (3805 + 804) # 计算分类器的精度
recall = 3805 / (3805 + 14212) # 计算分类器的recall

print(“精度：{:.2f}”.format(精度))
print(&quot;召回: {:.2f}&quot;.format(recall))

我收到错误：
精度：0.83
召回率：0.21
✘ 精度不正确。你可以从困惑中得到它
矩阵为 TP/P*。
有人可以告诉我哪里出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</guid>
      <pubDate>Tue, 09 Jan 2024 08:04:36 GMT</pubDate>
    </item>
    <item>
      <title>Mahout seqdirectory 无法读取输入 csv 文件</title>
      <link>https://stackoverflow.com/questions/77784930/mahout-seqdirectory-fails-to-read-input-csv-files</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77784930/mahout-seqdirectory-fails-to-read-input-csv-files</guid>
      <pubDate>Tue, 09 Jan 2024 07:19:53 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习技术的预测模型</title>
      <link>https://stackoverflow.com/questions/77784885/prediction-model-using-machine-learning-techniques</link>
      <description><![CDATA[我正在集群中运行 DNN 训练作业；这是我使用过的培训作业的链接。
https://github.com/pytorch/examples/blob /main/imagenet/main.py
在运行具有不同架构和不同批量大小的作业时，我可以收集执行时间。我在这里提到了一个例子。
因此，为了预测执行时间，我想使用机器学习技术来制作预测模型。
所以我想听听您关于收集数据点的建议。如果我收集不同架构的数据点，例如
模型架构：alexnet | convnext_base | convnext_large | convnext_small | convnext_tiny |密集网络121 |密集网络161 |密集网络169 |密集网络201 |高效网络_b0 |
高效网络_b1 |高效网络_b2 |高效网络_b3 |高效网络_b4 |高效网络_b5 |高效网_b6 |高效网_b7 |谷歌网 | inception_v3 | mnasnet0_5 | mnasnet0_75 | mnasnet0_75
mnasnet1_0 | mnasnet1_3 |移动网络_v2 | mobilenet_v3_large | mobilenet_v3_small | regnet_x_16gf | regnet_x_1_6gf | regnet_x_32gf | regnet_x_3_2gf | regnet_x_400mf | regnet_x_800mf |
regnet_x_8gf | regnet_y_128gf | regnet_y_16gf | regnet_y_1_6gf | regnet_y_32gf | regnet_y_3_2gf | regnet_y_400mf | regnet_y_800mf | regnet_y_8gf |资源网101 |资源网152 |资源网18 |
资源网34 |资源网50 | resnext101_32x8d | resnext50_32x4d | resnext50_32x4d | shufflenet_v2_x0_5 | shufflenet_v2_x1_0 | shufflenet_v2_x1_5 | shufflenet_v2_x2_0 |挤压网1_0 |挤压网1_1 | vgg11 |
vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19 | vgg19_bn | vit_b_16 | vit_b_32 |维生素_l_16 | vit_l_32 | Wide_resnet101_2 | Wide_resnet50_2（默认：resnet18）
是否可以创建一个预测模型来预测执行时间？]]></description>
      <guid>https://stackoverflow.com/questions/77784885/prediction-model-using-machine-learning-techniques</guid>
      <pubDate>Tue, 09 Jan 2024 07:07:37 GMT</pubDate>
    </item>
    <item>
      <title>将 XGBoost 转换为 Excel 的 if-then 语句 [关闭]</title>
      <link>https://stackoverflow.com/questions/77784031/convert-xgboost-to-if-then-statements-for-excel</link>
      <description><![CDATA[我在 R 中制作了一个 XGBoost 模型。我想将该模型导出到 Excel，但遇到了一些后勤问题。是否有一种有效的方法将每个决策树转换为格式类似于 Excel 的 if-then 语句？另外，XGBoost模型中的最终分数是如何计算的。它是每棵树的所有单独分数的总和吗？]]></description>
      <guid>https://stackoverflow.com/questions/77784031/convert-xgboost-to-if-then-statements-for-excel</guid>
      <pubDate>Tue, 09 Jan 2024 02:13:30 GMT</pubDate>
    </item>
    <item>
      <title>在 2D 图像上生成估计位置的方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/77783783/approach-to-generate-estimated-position-on-2d-image</link>
      <description><![CDATA[我将 Python 与 OpenCV 结合使用，并使用我自己的数据训练了 YOLOv8 模型。
我有一个通过机器周围的对象检测获得的边界框，该边界框向外延伸并附加到对象上。我目前面临的问题是，我想估计机器在物体上的位置，如果机器要延伸的话，给定其由非延伸形式的边界框定义的位置。
本质上，我有机器和机器未扩展的对象（下面将详细描述）的 2D 图像。然后，我获得了机器的边界框，并希望估计其在机器延伸并附着到物体时所在的同一 2D 图像上的位置。
下面提供了一张图像作为示例，展示了它在程序中的外观；但是，给出的图像的对象位于右侧，值得注意的是，它也可以翻转，使对象位于左侧（多个摄像机角度）。
需要注意的是，机器不会每次都固定在完全相同的位置，因为物体在移动时会产生一些噪音。
我已经尝试解决这个问题有一段时间了，并尝试了许多不同的想法；然而，我对计算机视觉和机器学习非常陌生，当我尝试某些事情时，我很可能没有正确地做。
我尝试过使用计算机视觉技术来提取特征、透视线等来确定机器的运动，但效果不佳。再说一次，很可能是我没有正确处理它或没有使用正确的方法。
我当前的方法是使用“设置”当扩展机器连接到物体时，它会监视并记录扩展机器的位置。然后，当给定图像来估计位置时，它会采用机器的边界框并将其转换为记录的先前连接位置。这并没有真正按照我的预期工作，并且对于我的需要来说太硬编码了。
我希望获得一些关于如何解决这个问题的想法，我是否应该使用计算机视觉技术（以及如何使用），或者我是否应该尝试训练机器学习模型等。
本质上，我相对陷入困境，不确定从这里该何去何从。
下面是相机视图的示例。这是运行模型以生成机器周围的框后的有效图像。在此步骤中，我希望能够估计其连接时的位置。如图所示，左侧的机器向外延伸到右侧，连接到右侧的船上。
机器周围有边界框的图像]]></description>
      <guid>https://stackoverflow.com/questions/77783783/approach-to-generate-estimated-position-on-2d-image</guid>
      <pubDate>Tue, 09 Jan 2024 00:23:10 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用 Keras 创建多视图变分自动编码器模型时，层“full_model”的输入 2 与该层不兼容</title>
      <link>https://stackoverflow.com/questions/77782475/valueerror-input-2-of-layer-full-model-is-incompatible-with-the-layer-when-cr</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77782475/valueerror-input-2-of-layer-full-model-is-incompatible-with-the-layer-when-cr</guid>
      <pubDate>Mon, 08 Jan 2024 18:18:19 GMT</pubDate>
    </item>
    <item>
      <title>带有我自己的预训练模型的 Sagemaker 批处理变压器</title>
      <link>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</guid>
      <pubDate>Mon, 08 Jan 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中具有 NxM 维度的密集层和 M 个独立的 Nx1 密集层之间的梯度和优化差异</title>
      <link>https://stackoverflow.com/questions/77773439/differences-in-gradient-and-optimization-between-a-dense-layer-with-nxm-dimensio</link>
      <description><![CDATA[我想知道这两种设计选择将如何影响训练期间的梯度计算和整体优化过程。

每个结构中反向传播过程中的梯度计算会受到怎样的影响？
梯度流回网络的方式会有差异吗？
这些不同的网络结构是否可能需要或受益于不同的优化算法或学习率？
我的目标是从梯度计算和优化的角度更好地理解每个模型结构选择的含义（潜在的好处和可能的缺点）。

对于此事的任何见解将不胜感激。谢谢。
我还尝试了一个实验，将两个模型的权重初始化为 1。然而，当我比较反向传播后的梯度时，它们是不同的。
张量([[ 0.0289026424, 0.0164458379, -0.0429413468, -0.0317727998,
         -0.0011618818、0.0309777111、0.0496413819、-0.0330999792、
         -0.0217525940, -0.0376570895, 0.0238987785, -0.0303875748,
          0.0422640890, -0.0327035226, -0.0046056593, 0.0095992200,
          0.0047464888, -0.0015923150, -0.0349406302, 0.0358588137,
          0.0109524736、0.0521549769、0.0092769144、0.0338292755、
          0.0418556146、0.0403830707、0.0027946709、-0.0142157376、
          0.0573743209、0.0421377942、0.0161724705、0.0135669028、
          0.0103750098、0.0048434297、-0.0176108982、-0.0011629635、
          0.0177134797, 0.0047528706, 0.0455351323, -0.0127471210,
         -0.0103122834, 0.0092379786, -0.0011389051, 0.0214950778,
          0.0423520021, 0.0157480091, 0.0166458990, -0.0457958765]])
张量([[ 0.0289026424, 0.0164458416, -0.0429413430, -0.0317727998,
         -0.0011618854、0.0309777092、0.0496413782、-0.0330999792、
         -0.0217525922、-0.0376570858、0.0238987822、-0.0303875823、
          0.0422640815, -0.0327035226, -0.0046056607, 0.0095992228,
          0.0047464883, -0.0015923139, -0.0349406339, 0.0358588099,
          0.0109524755、0.0521549769、0.0092769163、0.0338292755、
          0.0418556109、0.0403830633、0.0027946699、-0.0142157376、
          0.0573743209、0.0421377942、0.0161724724、0.0135669019、
          0.0103750126、0.0048434315、-0.0176109001、-0.0011629632、
          0.0177134816, 0.0047528730, 0.0455351323, -0.0127471173,
         -0.0103122853, 0.0092379786, -0.0011389013, 0.0214950833,
          0.0423520021, 0.0157480109, 0.0166459009, -0.0457958728]])
火炬大小([48])
第 12 层的渐变大小为 torch.Size([1, 48])
第 12 层的梯度相同吗？错误的
]]></description>
      <guid>https://stackoverflow.com/questions/77773439/differences-in-gradient-and-optimization-between-a-dense-layer-with-nxm-dimensio</guid>
      <pubDate>Sun, 07 Jan 2024 14:25:52 GMT</pubDate>
    </item>
    <item>
      <title>如何获得每个标记的困惑度而不是平均困惑度？</title>
      <link>https://stackoverflow.com/questions/77433100/how-to-get-perplexity-per-token-rather-than-average-perplexity</link>
      <description><![CDATA[我可以从这里获得整个句子的困惑：
设备=“cuda”
从 Transformers 导入 GPT2LMHeadModel、GPT2TokenizerFast

设备=“cuda”；
model_id =“gpt2”；
模型 = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
已发送 = &#39;生日快乐！&#39;
input_ids = tokenizer(已发送, return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;]
target_ids = input_ids.clone()
输出=模型（input_ids.to（设备），标签=target_ids）
ppl = torch.exp(输出.loss)
打印（人）
&gt;&gt;&gt;张量(1499.6934, device=&#39;cuda:0&#39;, grad_fn=)

但是我怎样才能获得每个标记的困惑度值，而不是整个标记序列的平均困惑度呢？本例中的输入句子 &#39;Happybirthday!&#39; 由 3 个标记组成。基于困惑度公式：

这应该产生 3 个值：第一个标记的对数概率，给定第一个标记的第二个标记的对数概率，以及给定第一个标记的第三个标记的对数概率。每个值都应该取幂以获得以下的困惑度值每个令牌。
我目前有以下内容：
导入火炬
从 Transformers 导入 GPT2LMHeadModel、GPT2TokenizerFast

设备=“cuda”；
model_id =“gpt2”；
模型 = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

已发送 = &#39;生日快乐！&#39;
input_ids = tokenizer(已发送, return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;].to(设备)
target_ids = input_ids.clone()

# 初始化一个空列表来存储每个标记的困惑度
困惑=[]

# 计算每个 token 的困惑度
对于范围内的 i(input_ids.shape[1])：
    输出 = 模型(input_ids[:, :i+1], labels=target_ids[:, :i+1])
    log_prob = 输出.loss.item()
    困惑 = torch.exp(torch.tensor(log_prob))
    perplexities.append(perplexity.item())

# Perplexities 现在是一个包含每个标记的困惑度值的列表
对于 i，枚举中的令牌（[tokenizer.decode(i) for i in input_ids[0]]）：
    print(f&quot;令牌：{token}，困惑度：{perplexities[i]}&quot;)
    &gt;&gt;&gt;&gt;&gt;标记：快乐，困惑：nan
代币：生日，困惑度：54192.46484375
令牌：！，困惑度：1499.693359375

但我不确定我做错了什么，因为最后一个标记似乎与整个句子具有相同的复杂性。]]></description>
      <guid>https://stackoverflow.com/questions/77433100/how-to-get-perplexity-per-token-rather-than-average-perplexity</guid>
      <pubDate>Mon, 06 Nov 2023 17:30:20 GMT</pubDate>
    </item>
    <item>
      <title>如何计算最佳批量大小？</title>
      <link>https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size</link>
      <description><![CDATA[有时我会遇到一个问题：
分配具有形状的张量时发生 OOM

例如
分配形状为 (1024, 100, 160) 的张量时出现 OOM

其中 1024 是我的批量大小，我不知道其余的是什么。如果我减少批量大小或模型中神经元的数量，它就可以正常运行。
是否有一种通用方法可以根据模型和 GPU 内存计算最佳批量大小，以便程序不会崩溃？
简而言之：我希望模型的批量大小尽可能大，这样可以适合我的 GPU 内存，并且不会使程序崩溃。]]></description>
      <guid>https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size</guid>
      <pubDate>Mon, 09 Oct 2017 20:25:09 GMT</pubDate>
    </item>
    </channel>
</rss>