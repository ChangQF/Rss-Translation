<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 18 Jan 2024 09:14:45 GMT</lastBuildDate>
    <item>
      <title>机器学习中的有损与无损音频格式</title>
      <link>https://stackoverflow.com/questions/77837837/lossy-vs-lossless-audio-format-in-machine-learning</link>
      <description><![CDATA[我们希望提供一个能够从语音中识别某些事物的机器学习模型。特征提取基于专有算法。通常我们一直使用wav文件。我们一直在问自己是否也可以使用像 mp3 这样的东西？由于我们可能无法再次收集我们收集的数据，并且我们无法预见我们的专有算法的进一步开发在某个时候将需要什么样的信息，因此我们担心像 mp3 这样的东西会导致太多信息丢失。你觉得怎么样？]]></description>
      <guid>https://stackoverflow.com/questions/77837837/lossy-vs-lossless-audio-format-in-machine-learning</guid>
      <pubDate>Thu, 18 Jan 2024 08:13:51 GMT</pubDate>
    </item>
    <item>
      <title>CAIM离散化软件库</title>
      <link>https://stackoverflow.com/questions/77837480/software-library-for-caim-discretization</link>
      <description><![CDATA[对于我的论文工作，我需要使用 CAIM 离散化数据集。正在寻找可靠的软件库来执行此操作。
我已经在 pip 上找到了一个： caimcaim 但我发现 github 上有一些未解决的问题让我怀疑它的可靠性。
我要么需要这个 CAIM 软件的一些保证，要么需要任何其他可靠的软件。]]></description>
      <guid>https://stackoverflow.com/questions/77837480/software-library-for-caim-discretization</guid>
      <pubDate>Thu, 18 Jan 2024 07:09:22 GMT</pubDate>
    </item>
    <item>
      <title>识别一维数据系列中的波段（为 LSTM 模型设计特征）</title>
      <link>https://stackoverflow.com/questions/77837087/identifying-bands-within-a-1d-data-series-to-engineer-a-feature-for-a-lstm-mode</link>
      <description><![CDATA[我有一个一维（时间序列）实值测量值。数据中有 5 个波段，我想找到从顶部开始的第二个波段（红色）的平均值。最底部的橙色系列是“理论”系列。红色带的值（如果它是连续的），这样它在每次测量时都有一个值。
我想找到第二个波段的平均值，以便我可以调整橙色系列，然后将其提供给 LSTM 网络。
我很清楚第二个波段可以采用的 y 轴值，但它偶尔会与相邻波段可以采用的 y 轴值重叠。然而，我知道在任何 10-20 个时间点，这些条带总是会分开相当大的距离。
有没有一种不脆弱的方法可以做到这一点？我可能想到了 k 均值聚类。]]></description>
      <guid>https://stackoverflow.com/questions/77837087/identifying-bands-within-a-1d-data-series-to-engineer-a-feature-for-a-lstm-mode</guid>
      <pubDate>Thu, 18 Jan 2024 05:30:58 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS 以 .rda 格式保存。
例如，我将模型训练为 -
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)
当我在清除所有环境变量后加载上述模型并对新数据使用预测函数时，
预测（model_old，newdata = new_data）
我收到以下错误
eval(Call$data) 中的错误：未找到对象“test”
但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TF-TRT 转换器不适用于我的型号？</title>
      <link>https://stackoverflow.com/questions/77836878/why-tf-trt-converter-didnt-work-for-my-model</link>
      <description><![CDATA[我想通过使用 TF-TRT 转换我的训练模型以获得更好的推理性能。
我使用了nvidia tensorflow docker镜像，运行测试代码没有问题。
测试代码来自这里：https://github.com/jhson989/tf-to -trt
和详细 Docker 镜像标签：nvcr.io/nvidia/tensorflow:23.12-tf2-py3
但是当我尝试转换经过训练的模型时，它不起作用。
导入tensorflow为tf
从张量流导入keras
从tensorflow.python.compiler.tensorrt导入trt_convert为trt

# 训练后的模型为.h5格式
h5_model_path = &#39;模型/路径/h5/模型名称&#39;
h5_model = keras.models.load_model(model_path,compile=False)

# 需要将.h5转换为saved_model格式才能使用TF-TRT
已保存模型路径 = &#39;模型/路径/已保存模型/模型名称&#39;
tf.saved_model.save（h5_model，saved_model_path）

# 制作一个转换器
conversion_param = trt.TrtConversionParams( precision_mode=trt.TrtPrecisionMode.FP16)
转换器 = trt.TrtGraphConverterV2(input_saved_model_dir=saved_model_path, conversion_params=conversion_param)

# 错误从这里发生
转换器.convert()

发生了这个错误。
回溯（最近一次调用最后一次）：
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py”，第 92 行，位于 NewCheckpointReader 中
    返回 CheckpointReader(compat.as_bytes(filepattern))
RuntimeError：TensorSliceReader 构造函数不成功：无法找到 /model/path/saved_model/model_name/variables/variables 的任何匹配文件

在处理上述异常的过程中，又出现了一个异常：

回溯（最近一次调用最后一次）：
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py”，第 1031 行，位于 load_partial
    加载器=加载器（object_graph_proto，saved_model_proto，export_dir，
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py”，第 226 行，在 __init__ 中
    self._restore_checkpoint()
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py”，第 561 行，位于 _restore_checkpoint
    load_status = saver.restore(variables_path, self._checkpoint_options)
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py”，第 1415 行，在恢复中
    读者 = py_checkpoint_reader.NewCheckpointReader(save_path)
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py”，第 96 行，位于 NewCheckpointReader 中
    错误翻译器（e）
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py”，第 31 行，位于 error_translator 中
    引发errors_impl.NotFoundError（无，无，error_message）
tensorflow.python.framework.errors_impl.NotFoundError：TensorSliceReader构造函数失败：无法找到/model/path/saved_model/model_name/variables/variables的任何匹配文件

在处理上述异常的过程中，又出现了一个异常：

回溯（最近一次调用最后一次）：
  文件“/model/code/convert_model.py”，第 106 行，在 eval 中
    转换器.convert()
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py”，第 1453 行，在转换中
    self._saved_model = load.load(self._input_saved_model_dir,
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py”，第 900 行，加载中
    结果 = load_partial(export_dir, None, 标签, 选项)[“root”]
  文件“/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py”，第 1034 行，位于 load_partial
    引发文件未找到错误（
FileNotFoundError：TensorSliceReader 构造函数失败：找不到 /model/path/saved_model/model_name/variables/variables 的任何匹配文件
 您可能正在尝试在与计算设备不同的设备上加载。考虑将“tf.saved_model.LoadOptions”中的“experimental_io_device”选项设置为 io_device，例如“/job:localhost”。

我已经确认我的模型的 saving_model 版本与测试代码具有相同的目录。
具体来说是“/model/path/saved_model/model_name/variables”目录，其中包含variables.data-00000-of-00001和variables.index。]]></description>
      <guid>https://stackoverflow.com/questions/77836878/why-tf-trt-converter-didnt-work-for-my-model</guid>
      <pubDate>Thu, 18 Jan 2024 04:21:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么在将交错数据集与 Hugging Face (HF) 一起使用时，会出现 UnboundLocalError：赋值前引用的局部变量 'batch_idx'？</title>
      <link>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77836822/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-ass</guid>
      <pubDate>Thu, 18 Jan 2024 04:00:40 GMT</pubDate>
    </item>
    <item>
      <title>NER 模型表现不佳</title>
      <link>https://stackoverflow.com/questions/77836231/ner-model-not-performing-adequately</link>
      <description><![CDATA[我正在为以下标签构建 NER 模型：ACQUIREE_COMPANY 和 ACQUIROR_COMPANY。培训数据基于宣布被收购方和收购方公司合并和收购的新闻稿。我使用 ChatGPT-4 注释了大约 18,000 个示例。我使用 Prodigy 训练模型，使用基本模型 (en_core_web_lg) 和不使用基本模型时的分割率为 80%（训练）-20%（评估）。使用基本模型训练的模型的准确率没有超过大约 70%，而没有基本模型训练的模型的准确率则没有超过 67%。
没有基本模型的训练运行统计数据为：
E # LOSS TOK2VEC LOSS NER ENTS_F ENTS_P ENTS_R SCORE
--- ------ ------------ -------- ------ ------ ------ --- ---
...
  0 3400 236.99 870.02 67.13 68.91 65.44 0.67
...
  0 4600 31159.08 946.75 67.07 73.73 61.52 0.67
...
  0 5000 581.26 919.93 64.44 62.43 66.58 0.64
✔ 将管道保存到输出目录

使用 en_core_web_lg 作为基本模型进行训练的统计数据为：
E # LOSS TOK2VEC LOSS NER ENTS_F ENTS_P ENTS_R SPEED SCORE
--- ------ ------------ -------- ------ ------ ------ --- --- ------
...
  3 19000 0.00 3564.10 72.53 74.67 70.50 6875.54 0.73
  3 20000 0.00 3647.85 72.67 74.46 70.96 7190.40 0.73
...
  5 25000 0.00 3639.24 72.75 74.55 71.03 7433.97 0.73
  5 26000 0.00 3409.12 72.74 74.67 70.91 7425.77 0.73
✔ 将管道保存到输出目录

我们将非常感谢有关如何提高准确性的一些指导。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77836231/ner-model-not-performing-adequately</guid>
      <pubDate>Thu, 18 Jan 2024 00:25:16 GMT</pubDate>
    </item>
    <item>
      <title>张量卷积运算的视觉表示</title>
      <link>https://stackoverflow.com/questions/77836166/visual-representation-of-convolution-operation-on-tensor</link>
      <description><![CDATA[当应用的滤波器数量等于输入张量？
]]></description>
      <guid>https://stackoverflow.com/questions/77836166/visual-representation-of-convolution-operation-on-tensor</guid>
      <pubDate>Wed, 17 Jan 2024 23:52:45 GMT</pubDate>
    </item>
    <item>
      <title>感知器算法未收敛于线性可分离数据</title>
      <link>https://stackoverflow.com/questions/77836071/perceptron-algorithm-not-converging-on-linearly-separable-data</link>
      <description><![CDATA[我正在研究感知器问题，我制作了一些假数据，当数据线性可分时，感知器算法不会收敛。
这是线性可分的假数据。
np.random.seed(42)
Linear_df = pd.DataFrame({
    &#39;X1&#39;：np.round（np.concatenate（[np.random.uniform（低= 0，高= 5，大小= 4），np.random.uniform（低= 8，高= 12，大小= 4） ]), 1),
    &#39;X2&#39;：np.round（np.concatenate（[np.random.uniform（低= 0，高= 5，大小= 4），np.random.uniform（低= 8，高= 12，大小= 4） ]),1),
    &#39;Y&#39;: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]
})

然后我在上面运行感知器
clf = 感知器（详细=1，max_iter=1000）
X = Linear_df[[&#39;X1&#39;, &#39;X2&#39;]]
y = 线性_df[&#39;Y&#39;]
clf.fit(X, y)
线性系数 = clf.coef_
线性偏差 = clf.intercept_[0]
打印（clf.coef_）
打印（clf.intercept_）
打印（clf.score（X，y））

8 个 epoch 后的收敛时间为 0.00 秒
[[ 2.3 -2.6]]
[17.]
0.5
但它说它在 8 个 Epoch 后收敛，并且没有产生正确的输出。]
情节如下：

任何想法都会非常有用，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77836071/perceptron-algorithm-not-converging-on-linearly-separable-data</guid>
      <pubDate>Wed, 17 Jan 2024 23:19:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在物理信息神经网络的背景下实现虚拟工作的原理？</title>
      <link>https://stackoverflow.com/questions/77835857/how-can-the-principle-of-virtual-work-be-implemented-in-the-context-of-a-physics</link>
      <description><![CDATA[我需要解决加载垂直载荷 q 的板的结构问题。我想实现一个考虑物理问题的神经网络，即所谓的物理通知神经网络。我不想使用问题的强形式（因此求解以下方程来更新网络：EJw,xxxx=q，其中 w,xxxx 是垂直位移的四阶导数，EJ 是本构关系，q 是载荷）使用弱形式公式，因此 dW_int=d_W_ext 或 Pi=U+Wext 是可能的最小值，其中 Pi 是总势能，U 是应变能，W_ext 是外力功（在本例中为 q）以及dW_int是虚拟内部工作。你会如何实现它？
我创建了一个神经网络并考虑了域内的 900 个搭配点来恢复垂直位移。然后我构造一个 hiddel 层矩阵，其尺寸为 900 x 500，其中 500 是神经元数量，900 是搭配点。从这个隐藏层矩阵中，我恢复了应变能和外部功，但随后对其进行积分，矩阵的尺寸与我认为上传网络的正确性不再一致。 PVW 方程的分辨率应该给我一个 500x1 的向量，它更新系统的权重]]></description>
      <guid>https://stackoverflow.com/questions/77835857/how-can-the-principle-of-virtual-work-be-implemented-in-the-context-of-a-physics</guid>
      <pubDate>Wed, 17 Jan 2024 22:19:35 GMT</pubDate>
    </item>
    <item>
      <title>批次和图层归一化差异</title>
      <link>https://stackoverflow.com/questions/77835832/batch-and-layer-normalization-difference</link>
      <description><![CDATA[
在批量归一化中，均值和标准差是按特征计算的，归一化步骤是按实例完成的，在层归一化中，均值和标准差是按实例计算的，归一化步骤是按特征完成的；这对不对？

“批次”有什么用？在批量归一化中？在神经网络中完成第一遍后，我们是否要向网络提供第二批数据？


我找不到任何关于这方面的好的资源，而且定义似乎很难理解。]]></description>
      <guid>https://stackoverflow.com/questions/77835832/batch-and-layer-normalization-difference</guid>
      <pubDate>Wed, 17 Jan 2024 22:14:25 GMT</pubDate>
    </item>
    <item>
      <title>DataLoader 类拾取父文件夹</title>
      <link>https://stackoverflow.com/questions/77835828/dataloader-class-picking-up-parent-folder</link>
      <description><![CDATA[我正在尝试在图像分类任务上训练卷积神经网络。由于某种原因，我的数据加载器类正在获取父文件夹，我怀疑这在尝试训练模型时会导致问题，因为它给我一个错误“运行时错误：应该为所有 64 个类定义权重张量，或者不定义任何类，但是得到形状的权重张量：[99]”
这是我的代码：
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
从 torch.optim 导入 lr_scheduler
从 torch.utils.data 导入 DataLoader、数据集、random_split
导入火炬视觉
从 torchvision 导入数据集、模型、转换
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
导入操作系统
从 PIL 导入图像


data_path = &#39;/kaggle/input/facial-age/face_age&#39;

类 CustomImageFolder（数据集）：
    def __init__(self, root_dir, 变换=无):
        self.root_dir = root_dir
        self.transform = 变换
        self.images = []
        self.标签 = []

        # 遍历所有子目录
        对于排序中的标签（os.listdir（root_dir））：
            如果标签==&#39;face_age&#39;：
                继续
            label_path = os.path.join(root_dir, 标签)
            如果 os.path.isdir(label_path):
                对于 os.listdir(label_path) 中的 img_file：
                    img_path = os.path.join(label_path, img_file)
                    如果 os.path.isfile(img_path):
                        self.images.append(img_path)
                        self.labels.append(标签)

    def __len__(自身):
        返回 len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        图像 = Image.open(img_path).convert(&#39;RGB&#39;)
        标签 = self.labels[idx]

        如果自我变换：
            图像 = self.transform(图像)

        返回图像，int(标签)

# 定义变换
变换 = Transforms.Compose([transforms.ToTensor()])

# 创建自定义数据集
数据集 = CustomImageFolder(root_dir=data_path, 变换=变换)

# 检查前几项
对于范围（5）内的 i：
    图像，标签=数据集[i]
    print(f&#39;标签: {label}, 图像形状: {image.shape}&#39;)


train_size = int(0.8 * len(数据集))
test_size = len(数据集) - train_size
train_dataset, test_dataset = random_split(数据集, [train_size, test_size])

# 创建数据加载器
train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)
test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False)


数据集 = datasets.ImageFolder(root=data_path)
打印（数据集.class_to_idx）
打印（len（数据集.class_to_idx））

最后一个单元格输出父文件夹及其内部的所有文件夹。这没有任何意义。
这是使用的数据集：https://www.kaggle.com/datasets/ frabbisw/面部年龄]]></description>
      <guid>https://stackoverflow.com/questions/77835828/dataloader-class-picking-up-parent-folder</guid>
      <pubDate>Wed, 17 Jan 2024 22:13:35 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 前向传播速度取决于变量</title>
      <link>https://stackoverflow.com/questions/77835175/pytorch-forward-pass-speed-depending-on-variables</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77835175/pytorch-forward-pass-speed-depending-on-variables</guid>
      <pubDate>Wed, 17 Jan 2024 19:56:39 GMT</pubDate>
    </item>
    <item>
      <title>测试数据集的最佳数量是多少</title>
      <link>https://stackoverflow.com/questions/77834854/what-is-the-optimal-number-of-testing-dataset</link>
      <description><![CDATA[我从事胶质瘤相关研究。尽管采用了 SMOTE、RUS 或 ROS 等各种技术，但我的机器学习模型的结果似乎偏向于多数类别。鉴于模型是在不平衡数据集上训练的，这个问题可能是预料之中的。但是，我不确定问题是否出在所使用的技术上，或者是否是由于测试数据集不足造成的。
分割我的训练集和测试/验证集是否是一个可行的解决方案，特别是考虑到我的数据集的大小有限？作为上下文，我的数据由 19 名患者的 MRI 图像组成，每个患者 15 张图像。我使用 15 张图像进行训练，使用 4 张图像进行测试。]]></description>
      <guid>https://stackoverflow.com/questions/77834854/what-is-the-optimal-number-of-testing-dataset</guid>
      <pubDate>Wed, 17 Jan 2024 18:53:39 GMT</pubDate>
    </item>
    <item>
      <title>召回分数！=使用confusion_matrix手动计算</title>
      <link>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix</link>
      <description><![CDATA[我遇到了一个问题，即使用 recall_score(y, y_pred) 获得的召回分数与使用 confusion_matrix 手动计算的值不匹配。
不仅如此，召回率与特异性的值完全相同，我也在下面手动计算了该值。
这是我正在使用的相关代码：
recall = recall_score(y, y_pred) # &lt;-- 不同的分数

conf_matrix = fusion_matrix(y, y_pred)
tn, fp, fn, tp = conf_matrix.ravel()
Manual_recall = tp / (tp + fn) # &lt;-- 达到这个分数
特异性 = tn / (tn + fp) # &lt;-- 与上面的分数相同

这是发生这种情况的终端中打印的混淆矩阵的示例：
&lt;前&gt;&lt;代码&gt;[[34 6]
 [20 20]]

科学套件召回：0.85
手动召回：0.5
或
&lt;前&gt;&lt;代码&gt;[[29 11]
 [9 31]]

科学套件召回：0.725
手动召回：0.775
问题：
scikit-learn 返回的召回和手动召回不会产生相同的值。
问题：
为什么recall_score和使用confusion_matrix的手动计算可能会产生不同的召回分数结果？
更多信息...

这是一个二元分类问题。

我正在使用 recall_score 的默认阈值。

我尝试确定混淆表是否准确（确实如此）。

]]></description>
      <guid>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 18:08:23 GMT</pubDate>
    </item>
    </channel>
</rss>