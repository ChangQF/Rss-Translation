<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Dec 2023 12:26:44 GMT</lastBuildDate>
    <item>
      <title>如何提高噪声数据集的分类精度（模型的鲁棒性）？</title>
      <link>https://stackoverflow.com/questions/77619251/how-to-improve-classification-accuracy-robustness-of-model-on-a-noisy-dataset</link>
      <description><![CDATA[我有一个分类问题，我为此开发了以下模型。 1. kNN 2. SVM 3. MLP 和 4. 逻辑回归。现在我应该研究当数据集被随机高斯噪声破坏时算法的鲁棒性。
我在数据样本中添加了噪声，所有算法的性能在存在噪声的情况下都会下降，4 种算法之间存在一些细微的差异。现在，我应该通过一些方法来提高这个嘈杂数据集的分类准确性，因为在现实世界中，数据会很嘈杂。我对提高模型对噪声的鲁棒性的技术完全一无所知。]]></description>
      <guid>https://stackoverflow.com/questions/77619251/how-to-improve-classification-accuracy-robustness-of-model-on-a-noisy-dataset</guid>
      <pubDate>Thu, 07 Dec 2023 10:24:29 GMT</pubDate>
    </item>
    <item>
      <title>多维嵌入如何帮助决定跨两个类别的句子分类输入</title>
      <link>https://stackoverflow.com/questions/77619250/how-multidimensional-embedding-help-to-decide-sentence-classify-input-across-two</link>
      <description><![CDATA[示例：https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W2/ungraded_labs/C3_W2_Lab_1_imdb.ipynb
型号：“sequential_5”
_________________________________________________________________
 层（类型）输出形状参数#
=================================================== ===============
 embedding_5（嵌入）（无、120、16）160000
                                                                 
 flatten_5（压平）（无，1920）0
                                                                 
 密集_6（密集）（无，1）1921
                                                                 
=================================================== ===============
总参数：161921 (632.50 KB)
可训练参数：161921 (632.50 KB)
不可训练参数：0（0.00 字节）
_________________________________________________________________

从上面的模型我们可以定义这样的处理。
dense_6 -&gt;该层接受 1920 个输入并给出单个输出标签，该标签对评论是正面还是负面进行分类。
展平_5 -&gt;该层只是压平输入向量。
嵌入_5 -&gt;该层最多包含 120 个单词，并将每个单词的维度定义为向量。
这里我们想要对代表负面或正面评论的单词进行分类。当我们训练一个模型时，这16个维度在逻辑上意味着什么。
现在，如果我们简单地谈论dense_6，它有 1920 个特征，这些特征被赋予权重来预测 id，组合所有特征确实反映了负面评论或正面评论。
一旦我们训练了模型，这些特征在逻辑上的含义是什么？我们如何将这一单层与回归模型关联起来]]></description>
      <guid>https://stackoverflow.com/questions/77619250/how-multidimensional-embedding-help-to-decide-sentence-classify-input-across-two</guid>
      <pubDate>Thu, 07 Dec 2023 10:24:28 GMT</pubDate>
    </item>
    <item>
      <title>如何开发一个能够根据用户查询搜索和建议相关链接的聊天机器人？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77618816/how-can-i-develop-a-chatbot-with-the-ability-to-search-and-suggest-relevant-link</link>
      <description><![CDATA[在缺乏直接高级支持的情况下，我们的团队目前正在努力快速找到相关文档网页来解决产品问题。为了简化这一过程，我们正在探索开发一个能够搜索内部网站并建议相关网页（文档网站）的聊天机器人。我们如何共同实施这样的聊天机器人来增强我们团队的支持工作流程？
作为第一步，我们的团队使用 Python 中的网络抓取来从网页收集数据，并将其保存到文件中。随后，我们利用 NLTK 包并在这些文件上训练我们的模型。然而，当询问具体问题时，例如“静香是一个什么样的女孩？”该模型使用“印度历史”、“哆啦A梦”和“谷歌历史”等网页进行训练，倾向于根据查询中出现的各个单词来建议页面。例如，由于存在“Shizuka”一词，它建议“Doraemon”网页；由于“Kind”一词，它建议“印度历史”页面。虽然我们尝试通过删除“Kind”等某些单词来增强结果，但基于单词的方法可能无法为我们的产品文档产生相关结果。我们可以实施哪些修改或方法来确保我们的产品文档获得更加上下文准确的结果？]]></description>
      <guid>https://stackoverflow.com/questions/77618816/how-can-i-develop-a-chatbot-with-the-ability-to-search-and-suggest-relevant-link</guid>
      <pubDate>Thu, 07 Dec 2023 09:11:33 GMT</pubDate>
    </item>
    <item>
      <title>PatternRank可以提取带有中缀“-”的关键词吗？</title>
      <link>https://stackoverflow.com/questions/77618627/can-patternrank-extract-the-keyphrases-with-infix</link>
      <description><![CDATA[我尝试使用 PatternRank 从带有词性标记的文本文档中提取关键短语。但是，一些带有中缀“-”、“/”等的关键字无法识别可能是目标的关键字，而“_”可以被PatternRank识别。
示例：
导入spacy
将 pandas 导入为 pd
将 numpy 导入为 np
从 keybert 导入 KeyBERT
从 keyphrase_vectorizers 导入 KeyphraseCountVectorizer
从 sklearn.feature_extraction.text 导入 CountVectorizer

text = [&#39;&#39;&#39;BERT 是 NLP 的预训练模型。&#39;&#39;&#39;]
kw_model1.extract_keywords(docs=text, vectorizer=KeyphraseCountVectorizer(spacy_pipeline=nlp, lowercase=False, pos_pattern=&#39;*****&#39;, stop_words=&#39;english&#39;), use_mmr=True)

&gt;&gt;&gt;输出：[(&#39;BERT 是&#39;, 0.6578), (&#39;NLP&#39;, 0.5684)]


但是如果我将“-”替换为“_”：
导入spacy
将 pandas 导入为 pd
将 numpy 导入为 np
从 keybert 导入 KeyBERT
从 keyphrase_vectorizers 导入 KeyphraseCountVectorizer
从 sklearn.feature_extraction.text 导入 CountVectorizer

text = [&#39;&#39;&#39;BERT 是 NLP 的预训练模型。&#39;&#39;&#39;]
kw_model1.extract_keywords(docs=text, vectorizer=KeyphraseCountVectorizer(spacy_pipeline=nlp, lowercase=False, pos_pattern=&#39;*****&#39;, stop_words=&#39;english&#39;), use_mmr=True)

&gt;&gt;&gt;输出：[(&#39;BERT 是&#39;, 0.6579), (&#39;NLP&#39;, 0.5666), (&#39;预训练模型&#39;, 0.5215)]


我已经使用 spaCy 修改了分词器中缀模式，如下所示，以保留“-”、“/”等中缀。但它仍然不起作用。
导入重新
从 spacy.lang.char_classes 导入 ALPHA、ALPHA_LOWER、ALPHA_UPPER
从 spacy.lang.char_classes 导入 CONCAT_QUOTES、LIST_ELLIPSES、LIST_ICONS、LIST_PUNCT、LIST_QUOTES、CURRENCY、UNITS、PUNCT、LIST_CURRENCY
从 spacy.util 导入compile_infix_regex，compile_prefix_regex，compile_suffix_regex
从 spacy.tokenizer 导入 Tokenizer
# 修改分词器中缀模式
# 不希望分词器根据字母之间的连字符进行分割
中缀 = (
    LIST_ELLIPES
    + 列表_图标
    + [
        r&quot;(?&lt;=[0-9])[+\\-\\*^](?=[0-9-])&quot;,
        r&quot;(?&lt;=[{al}{q}])\\.(?=[{au}{q}])&quot;.format(
            al=ALPHA_LOWER、au=ALPHA_UPPER、q=CONCAT_QUOTES
        ),
        r&quot;(?&lt;=[{a}]),(?=[{a}])&quot;.format(a=ALPHA),
        # 注释掉按字母之间的连字符分割的正则表达式：
        # r&quot;(?&lt;=[{a}])(?:{h})(?=[{a}])&quot;.format(a=ALPHA, h=连字符),
        r&quot;(?&lt;=[{a}0-9])[:&gt;&gt;=/](?=[{a}])&quot;.format(a=ALPHA),
    ]
）

后缀 = (
    列表_PUNCT
    + 列表省略号
    + LIST_QUOTES
    + 列表_图标
    + [“&#39;s”、“&#39;S”、“&#39;s”、“&#39;S”、“—”、“–”]
    + [
        r”(?&lt;=[0-9])\+”,
        r“(?&lt;=°[FfCcKk])\.”,
        r&quot;(?&lt;=[0-9])(?:{c})&quot;.format(c=CURRENCY),
        r&quot;(?&lt;=[0-9])(?:{u})&quot;.format(u=UNITS),
        r&quot;(?&lt;=[0-9{al}{e}{p}(?:{q})])\.&quot;.format(
            al=ALPHA_LOWER, e=r&quot;%²\-\+&quot;, q=CONCAT_QUOTES, p=PUNCT
        ),
        r&quot;(?&lt;=[{au}][{au}])\.&quot;.format(au=ALPHA_UPPER),
    ]
）

前缀 = (
    [“§”、“%”、“=”、“—”、“–”、r“\+(?![0-9])”]
    + 列表_PUNCT
    + 列表省略号
    + LIST_QUOTES
    + 列表_货币
    + 列表_图标
）

]]></description>
      <guid>https://stackoverflow.com/questions/77618627/can-patternrank-extract-the-keyphrases-with-infix</guid>
      <pubDate>Thu, 07 Dec 2023 08:36:10 GMT</pubDate>
    </item>
    <item>
      <title>自定义 haar 级联无法正确检测对象</title>
      <link>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</link>
      <description><![CDATA[我想创建一个 Haar 级联用于舌头检测。我已尝试使用 Cascade Trainer GUI（版本 3.3.1）构建 Haar 级联 40 多次，但它无法正确检测。我怎样才能做到这一点？
我创建了一个用于存放舌头图像（正样本“p”）的文件夹和另一个用于存放没有舌头的常见图像（负样本“n”）的文件夹，每个文件夹包含 150 个样本图像。一切都运行完美，没有任何错误，但测试图像没有检测到舌头。]]></description>
      <guid>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</guid>
      <pubDate>Thu, 07 Dec 2023 06:00:31 GMT</pubDate>
    </item>
    <item>
      <title>多输出模块的 Keras 精度不会改变</title>
      <link>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</link>
      <description><![CDATA[我想预测对欺诈案件的处罚/处罚，输入格式为（损害金额（$），如果是累犯），目标格式为（罚款（$），监狱（月），社区服务（小时），缓刑（月）） - 以下是我的代码：
fname = “文件路径.tsv”
全部输入 = []
所有输出 = []

将 open(fname) 作为 f：
    对于 i，enumerate(f) 中的行：
        如果我&lt; 3:#第一行
            print(&quot;标题:&quot;, line.strip().split(&#39;\t&#39;))
            继续
        fields = line.strip().split(&#39;\t&#39;)
        all_in.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[5:7]])
        all_out.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[7:11]])

case_in = np.array(all_in, dtype = “uint64”)
target_out = np.array(all_out, dtype = “uint64”)

Normalize_layer = tf.keras.layers.Normalization(axis=-1, name = “normalize_in”)
Normalize_layer.adapt(all_in)
Normalize_out = tf.keras.layers.Normalization(axis=-1, name = “normalize_out”)
Normalize_out.adapt(all_out)
denormalize_out = tf.keras.layers.Normalization(axis=-1, invert = True, name = “denormalize_out”)
denormalize_out.adapt(all_out)
缩放输出 = 标准化输出（全部输出）

输入= Normalize_layer（输入（形状= 2））
x = 密集（6，input_dim = 2，激活=“sigmoid”，use_bias = True）（输入）
x = 密集(4, 激活 = “sigmoid”)(x)
y_4 = 密集(1, 激活 = “sigmoid”, 名称 = “y_4”)(x)
惩罚=密集（3，激活=“sigmoid”，名称=“惩罚”）（x）
y_1 = 密集（1，激活=“sigmoid”，名称=“y_1”）（惩罚）
y_2 = 密集（1，激活=“sigmoid”，名称=“y_2”）（惩罚）
y_3 = 密集（1，激活=“sigmoid”，名称=“y_3”）（惩罚）

模型 = 模型（输入 = 输入，输出 = [y_1，y_2，y_3，y_4]）

模型.编译(
    优化器= SGD(learning_rate=0.01,weight_decay=1e-6,momentum=0.9,nesterov=True),
    损失={
        “y_1” ：“均方误差”，
        “y_2” ：“均方误差”，
        “y_3” ：“均方误差”，
        “y_4” ：“均方误差”
    },
    指标=[&#39;准确性&#39;]
）

模型.拟合(
    案例输入，
    横向扩展，
    batch_size = 10,##数据增加后，增加
    纪元 = 300，
    详细 = 2,
    验证分割= 0.8
）

对于 model.layers 中的图层：
    print(&quot;=====图层:&quot;, 图层名称,&quot;=====&quot;)
    if layer.get_weights() != []:
        权重=layer.get_weights()[0]
        偏差=layer.get_weights()[1]
        print(&quot;权重：&quot;)
        打印（权重）
        print(“偏差：”)
        打印（偏差）
    别的：
        print(&quot;权重：&quot;, [])

而且，一旦开始训练模型，准确性就根本不会改变。另外，当我试图解决这个问题时，我搞砸了一些东西，使大约 50 个数据集的数据只是 .fit 操作的一批，尽管我的批量大小为 10。我只是尝试了太多的事情来修复我的代码互联网上到处都是，这让一切变得更加混乱。
最初，我尝试对数据进行标准化，包括我的来龙去脉，这解冻了我的损失函数，但它并没有真正对准确性函数产生任何影响。我还将 ReLU 函数也更改为 sigmoid 函数，因为在某些时候，死 ReLU 似乎可能存在问题，因为我的层上的偏差没有从最初的 0 更新。之后，我不断地研究优化器、损失函数和纪元，但都无济于事。
如何让模块真正发挥作用？您对代码有一般反馈吗？]]></description>
      <guid>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</guid>
      <pubDate>Thu, 07 Dec 2023 05:51:59 GMT</pubDate>
    </item>
    <item>
      <title>从 ampligraph 导入复杂数据时如何修复此错误 ImportError</title>
      <link>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</link>
      <description><![CDATA[ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-41-449fec1eb93c&gt;在&lt;细胞系：28&gt;()
     26 从 imblearn.over_sampling 导入 RandomOverSampler、SMOTE、ADASYN
     27 从 imblearn.under_sampling 导入 ClusterCentroids、RandomUnderSampler、NearMiss、TomekLinks
---&gt; 28 从 ampligraph.latent_features 导入 ComplEx
     29

ImportError：无法从“ampligraph.latent_features”导入名称“ComplEx”（/usr/local/lib/python3.10/dist-packages/ampligraph/latent_features/__init__.py）

我尝试降级放大器来修复它，但它不起作用。我也尝试阅读新版本的文档，但我不知道，因为我是新手。]]></description>
      <guid>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</guid>
      <pubDate>Thu, 07 Dec 2023 04:28:22 GMT</pubDate>
    </item>
    <item>
      <title>考虑季节性预测下一个值</title>
      <link>https://stackoverflow.com/questions/77617162/predicting-next-value-considering-seasonality</link>
      <description><![CDATA[我的数据如下
日月目标
3 3 8
5 4 9

日期不必采用连续日期格式。该日期可以是 1/1/2023，然后下一个日期也可以是 2/2/2023。目前，我正在使用 LSTM 来预测下一组序列，并且我的 LSTM 模型更频繁地预测平均值，而不考虑季节性。在某些情况下，十月也有不同的值，模型无法预测。如果有任何替代模型或方法，请就此提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/77617162/predicting-next-value-considering-seasonality</guid>
      <pubDate>Thu, 07 Dec 2023 01:19:56 GMT</pubDate>
    </item>
    <item>
      <title>将模型部署到 GCP 的 Vertex 时如何利用 L4 GPU</title>
      <link>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</link>
      <description><![CDATA[我将一个模型部署到 GCP 上的 Vertex，用于部署它的配置代码如下所示：
dedicated_resources=dict(
    机器规格=字典（
        machine_type=“g2-standard-8”，
        Accelerator_type=“NVIDIA_L4”，
        Accelerator_count=1,
    ),
    min_replica_count = 2，
    最大副本数=10,
    autoscaling_metric_specs=[
        字典（
            metric_name=“aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle”，
            目标=20，
        ),
    ],
）


该模型配置为利用 GPU 资源，并且此配置已在 n1 计算机上使用 P4 和 P100 运行。当在具有 L4 加速器的 g2 机器上运行模型时，请求会导致极高的延迟和 CPU 利用率，而 GPU 利用率则稳定在 0%。
我不知道接下来该去哪里，也不知道最好的故障排除选项是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</guid>
      <pubDate>Thu, 07 Dec 2023 00:51:07 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“Flags”对象没有属性“c_contigious”</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在阅读 Aurélien Géron 编写的《机器学习实践》一书，但遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(“int”) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

＃模型
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
&lt;前&gt;&lt;代码&gt;{
AttributeError: &#39;Flags&#39; 对象没有属性 &#39;c_contigious&#39;”
}

由于我正在关注这本书，所以我希望这段代码能够工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>基于相同输入数据的并行或共享回归网络会更好吗？为什么？</title>
      <link>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</link>
      <description><![CDATA[我将多个并行回归网络组合成一个模型，其中组合输出以创建单个损失函数。这些网络正在寻找相同数据的不同方面，并同时进行训练。这可以被认为是一个基于物理的神经网络。
本能地，我想说，分割网络允许每个网络拥有自己的权重，而不受其他方面的干扰，这将加快训练速度和/或提高性能。 ChatGPT 似乎证实了我的怀疑，但无法给我任何来源。
有人有任何论文/证明或知道这两种方法的更具体术语吗？我只是真的不知道如何提出这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</guid>
      <pubDate>Wed, 06 Dec 2023 17:29:30 GMT</pubDate>
    </item>
    <item>
      <title>安装 Toad 时子进程退出并出现错误</title>
      <link>https://stackoverflow.com/questions/77613056/subprocess-exited-with-error-while-installing-toad</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77613056/subprocess-exited-with-error-while-installing-toad</guid>
      <pubDate>Wed, 06 Dec 2023 12:29:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 1D 贝叶斯 CNN（通过使用工作 CNN 的 Convolution1DFlipout 和 DenseFlipout 替换卷积层和密集层而制成）无法训练？</title>
      <link>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</guid>
      <pubDate>Mon, 04 Dec 2023 21:17:27 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit-learn 中使用 skopt.BayesSearchCV 时如何修复“numpy.int”属性错误？</title>
      <link>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</link>
      <description><![CDATA[当我在官方文档上运行以下代码时，出现错误。
最小示例
from skopt import BayesSearchCV
从 sklearn.datasets 导入 load_digits
从 sklearn.svm 导入 SVC
从 sklearn.model_selection 导入 train_test_split

X, y = load_digits(n_class=10, return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)

# log-uniform：理解为通过改变 x 对 p = exp(x) 进行搜索
选择 = BayesSearchCV(
    SVC(),
    {
        &#39;C&#39;: (1e-6, 1e+6, &#39;对数均匀&#39;),
        &#39;伽玛&#39;: (1e-6, 1e+1, &#39;对数均匀&#39;),
        &#39;level&#39;: (1, 8), # 整数值参数
        &#39;kernel&#39;: [&#39;线性&#39;, &#39;poly&#39;, &#39;rbf&#39;], # 分类参数
    },
    n_iter=32,
    简历=3
）

opt.fit(X_train, y_train)

最后一行产生错误：
&lt;块引用&gt;
属性错误
模块“numpy”没有属性“int”。
np.int 是内置 int 的已弃用别名。要避免现有代码中出现此错误，请单独使用 int。这样做不会改变任何行为并且是安全的。替换 np.int 时，您可能希望使用例如np.int64 或 np.int32 指定精度。如果您想查看当前的使用情况，请查看发行说明链接以获取更多信息。
别名最初在 NumPy 1.20 中已弃用；有关更多详细信息和指导，请参阅原始发行说明：
https://numpy.org/devdocs/release/1.20.0-notes .html#deprecations

如何解决这个问题？还有其他方法来实现贝叶斯搜索吗？
也许skopt的版本太旧了。还有其他方法来实现贝叶斯搜索吗？除了网格搜索、随机搜索和贝叶斯搜索之外，还有其他方法可以帮助我选择机器学习模型的超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</guid>
      <pubDate>Wed, 24 May 2023 09:06:15 GMT</pubDate>
    </item>
    <item>
      <title>在python中找到最终的回归方程</title>
      <link>https://stackoverflow.com/questions/59930627/finding-final-regression-equation-in-python</link>
      <description><![CDATA[如何找到包含所有变量系数的最终回归模型方程？有什么方法吗？ ]]></description>
      <guid>https://stackoverflow.com/questions/59930627/finding-final-regression-equation-in-python</guid>
      <pubDate>Mon, 27 Jan 2020 11:52:08 GMT</pubDate>
    </item>
    </channel>
</rss>