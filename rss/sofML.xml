<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Apr 2024 03:15:10 GMT</lastBuildDate>
    <item>
      <title>用于道路分析的聚类算法的建议</title>
      <link>https://stackoverflow.com/questions/78381575/recommendations-on-clustering-algorithms-to-use-for-road-analysis</link>
      <description><![CDATA[我想询问有关使用哪种聚类算法来分析这条道路上任何潜在组/聚类的建议。这些点代表了许多电动汽车快速充电站，但是，我希望以尽可能少的主观性对它们进行分组。我正在使用 GIS 工具，因此，任何有关任何潜在的聚类技术使用的建议都是非常受欢迎的。预先感谢！
我考虑过 k 均值，但我不喜欢由我来决定有多少个簇的想法。这个想法是，聚类应该告诉我应该有多少个聚类。]]></description>
      <guid>https://stackoverflow.com/questions/78381575/recommendations-on-clustering-algorithms-to-use-for-road-analysis</guid>
      <pubDate>Wed, 24 Apr 2024 23:06:38 GMT</pubDate>
    </item>
    <item>
      <title>分类数据的加权 K 模式聚类[关闭]</title>
      <link>https://stackoverflow.com/questions/78380891/weighted-k-modes-clustering-for-categorical-data</link>
      <description><![CDATA[我正在对 5 列分类数据（我们称每列为 A、B、C、D、E）进行聚类，并因此选择了 k-modes。每列中的每个类别都没有顺序。数据集不平衡，A、B 和 C 中的一个类别占数据的 60-80%。D 列和 E 列有大量不同的类别（&gt;50）。
我想给 A 列和 B 列赋予优先权重，这样 K-modes 就会将这些列作为聚类的优先级，而不是 C D E 列，因此形成的聚类具有 A 和 B 的不同值。
我正在使用 K-Modes 库（python），并修改了汉明距离度量，以便如果 A 列和 B 列中的 2 个类别不匹配，则对它们进行严厉惩罚，以努力围绕这些列形成聚类。我预计，随着惩罚的增加，K-Modes 将保证 A 列和 B 列内的所有类别至少有一个不同的聚类质心。但事实并非如此，有些类别没有表示为聚类质心。
有人能帮忙解释一下原因吗：

这并没有按预期发生
我该如何修改 K-Modes 库（或者解释我该如何修改算法以赋予某些列权重/偏好）
kModes.fit(x, sample_weights) 的 sample_weights 参数有什么作用？我无法使用 sklearn.kmeans 或 kmode.kmode 中的文档来理解直觉
]]></description>
      <guid>https://stackoverflow.com/questions/78380891/weighted-k-modes-clustering-for-categorical-data</guid>
      <pubDate>Wed, 24 Apr 2024 19:46:55 GMT</pubDate>
    </item>
    <item>
      <title>有没有人在 Capsnet 中开发或增强挤压方程 [关闭]</title>
      <link>https://stackoverflow.com/questions/78380434/is-anyone-develop-or-enhance-squash-equation-in-capsnet</link>
      <description><![CDATA[def 挤压（向量，轴=-1）：
s_squared_norm = tf.reduce_sum(tf.square(向量), axis, keepdims=True)
尺度 = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())
返回比例 * 向量
CapsNet 中使用的 Standardd squash 函数，我如何增强它以提高准确性。
我尝试用不同的方式进行编辑，但结果并不好]]></description>
      <guid>https://stackoverflow.com/questions/78380434/is-anyone-develop-or-enhance-squash-equation-in-capsnet</guid>
      <pubDate>Wed, 24 Apr 2024 18:07:04 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 Colab 上的 FIT-SNE</title>
      <link>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</link>
      <description><![CDATA[如何在我的 Colab 笔记本上实现基于 FFT 加速插值的 t-SNE (FIt-SNE)？
我试图在狗情绪的kaggle数据集上计算t-SNE ,
我首先尝试获取第一个组件。
&lt;前&gt;&lt;代码&gt;电脑 = 60
pca = 分解.PCA(n_components=pc)
_ = pca.fit(图像)
imgPCA = pca.transform(图像)

tsne = TSNE(n_components=2)
Z = tsne.fit_transform(imgPCA)
绘图嵌入（Z）

然后我尝试了 多核 t-SNE 来提高迭代次数，但我仍然不喜欢
!pip install git+https://github.com/DmitryUlyanov/Multicore-TSNE.git

从 MulticoreTSNE 导入 MulticoreTSNE

Z = MulticoreTSNE(n_jobs=4, n_iter=10000).fit_transform(imgPCA)
图嵌入（Z，show_axis = True）

现在我想尝试使用 FIt-SNE，但我不会知道如何使用它，你能帮我吗？
或者，如果您愿意，也许您可​​以帮助改进之前的代码片段。
有代码可以理解数据集的格式：
导入 pandas 作为 pd
导入CV2

img_size = (192,192,3)
num_px = img_size[0] * img_size[1] * img_size[2]

目录 = &#39;/content/drive/MyDrive/Colab Notebooks/ML/Dog_Emotion/&#39;
图片 = []
标签=[]
labels_df = pd.read_csv(目录 + “labels.csv”)
n_图像 = 0

对于 tqdm 中的图像（labels_df.iloc，desc =“加载图像”，单位=“图像”，总计= 4000）：
  images.append(np.asarray(cv2.resize(cv2.imread(目录 + image[2] + &#39;/&#39; + image[1], cv2.IMREAD_COLOR), img_size[0:2])[:, :, : :-1]))
  labels.append(图像[2])

图像，标签 = np.array(images).reshape(4000, num_px), np.array(labels)

print(f&#39;标签形状：{labels.shape}&#39;)
print(f&#39;图像形状：{images.shape}&#39;)
print(f&#39;图像大小: {img_size}&#39;)

defplot_embedding(Z, show_axis=&quot;False&quot;):
  plt.figure(figsize=(10, 8))
  地图= {标签：i代表i，枚举中的标签（np.unique（标签））}
  color = np.array([map[l] for l in labels])
  plt.scatter(Z[:, 0], Z[:, 1], c = 颜色, cmap = &quot;jet&quot;)
  plt.colorbar()
  plt.title(&#39;2d t-SNE 可视化&#39;)
  如果没有显示轴：
    plt.axis(“关闭”)
  plt.axis(“等于”)
  plt.show()

标签形状：(4000,)
图像形状：(4000, 110592)
图片尺寸：(192, 192, 3)]]></description>
      <guid>https://stackoverflow.com/questions/78380376/fit-sne-on-google-colab</guid>
      <pubDate>Wed, 24 Apr 2024 17:53:14 GMT</pubDate>
    </item>
    <item>
      <title>用户输入字符串和机器学习模型猜测类别吗？</title>
      <link>https://stackoverflow.com/questions/78380232/have-user-input-string-and-machine-learning-model-guess-category</link>
      <description><![CDATA[我目前正在基于 20 Newsgroup 数据集构建机器学习模型。
它有 20 个类别，例如宗教、政治、汽车、计算机等，并预测文本条目正在谈论的内容。
“有谁知道为什么我的斯巴鲁雨刷不适合” - 汽车。
我正在使用 scikit learn，并且有 MLP 和逻辑回归模型。
我想知道是否有一种方法可以让用户输入一个字符串，然后模型吐出它认为它是什么类别？
用户输入：“您推荐什么微处理器？”
型号：“计算机”
我找不到有关该主题的任何好的资源，并且需要一些建议。]]></description>
      <guid>https://stackoverflow.com/questions/78380232/have-user-input-string-and-machine-learning-model-guess-category</guid>
      <pubDate>Wed, 24 Apr 2024 17:25:56 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 和 pytorch [关闭]</title>
      <link>https://stackoverflow.com/questions/78380010/tensorflow-and-pytorch</link>
      <description><![CDATA[我最近开始学习机器学习和人工智能。但我对tensorflow和pytorch有问题，我发现很难安装两者。过程中总会有中断的时候。我可以做些什么来使该过程顺利进行？
我尝试使用 pip 进行正常的过程，每次开始下载时，通常会有一些东西中断该过程。我希望它能够无缝运行，没有任何问题]]></description>
      <guid>https://stackoverflow.com/questions/78380010/tensorflow-and-pytorch</guid>
      <pubDate>Wed, 24 Apr 2024 16:40:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM Studio 无法下载模型并出现错误：无法获取本地颁发者证书</title>
      <link>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</link>
      <description><![CDATA[在LLM studio中，当我尝试下载任何模型时，我遇到以下错误：
下载失败：无法获取本地颁发者证书
]]></description>
      <guid>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</guid>
      <pubDate>Wed, 24 Apr 2024 16:03:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 bigQueryML 的转换子句不支持 ML.NGRAM？</title>
      <link>https://stackoverflow.com/questions/78379552/why-isnt-ml-ngram-not-supported-in-transform-clause-in-bigqueryml</link>
      <description><![CDATA[我正在使用以下查询来创建模型，但编辑器抱怨转换子句中不支持 ML.NGRAM。
创建或替换模型
`singular-hub-291814.movi​​e_sentiment.mymodel3`
TRANSFORM(ML.NGRAM(string_field_0,[1,2])OVER() 作为 ngram )
选项
  ( model_type=&#39;LOGISTIC_REG&#39;,
    auto_class_weights = TRUE，
    data_split_method=&#39;随机&#39;,
    DATA_SPLIT_EVAL_FRACTION = 0.10，
    input_label_cols=[&#39;评论&#39;]
  ） 作为

选择
  string_field_0 ，从表中查看；

尽管可以在 SELECT 查询中使用相同的转换。
&lt;前&gt;&lt;代码&gt;选择
  ML.NGRAMS(words_array, [1,2]) 作为 ngrams，
  审查
从表；

而其他转换函数如 bag_of_words、min_abs_scalar 可以在转换中使用。为什么这种行为会如此不同？是否有不能在 TRANSFORM 子句中使用的转换器的明确列表？]]></description>
      <guid>https://stackoverflow.com/questions/78379552/why-isnt-ml-ngram-not-supported-in-transform-clause-in-bigqueryml</guid>
      <pubDate>Wed, 24 Apr 2024 15:18:24 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 排名指标</title>
      <link>https://stackoverflow.com/questions/78379350/lightgbm-ranking-metrics</link>
      <description><![CDATA[我使用了 LightGBM LambdaRank，但不明白如何根据获得的值进一步计算指标
数据框
将 numpy 导入为 np
将 pandas 导入为 pd
将 lightgbm 导入为 lgb

df = pd.DataFrame({
    “query_id”：[i for i in range(100) for j in range(10)],
    “var1”：np.random.random(size=(1000,)),
    “var2”：np.random.random(size=(1000,)),
    “var3”：np.random.random(size=(1000,)),
    “相关性”:list(np.random.permutation([1,2,3,4,5, 6,7,8,9,10]))*100
})


这是数据框：
query_id var1 var2 var3 相关性
0 0 0.905357 0.894079 0.375130 5
1 0 0.547075 0.377121 0.754090 3
2 0 0.160593 0.397771 0.034981 10
3 0 0.295548 0.162948 0.549913 9
4 0 0.833037 0.233751 0.096317 7
……………………
995 99 0.188557 0.258408 0.090342 6
996 99 0.178921 0.881938 0.467198 2
997 99 0.175884 0.897310 0.992994 8
998 99 0.466874 0.400800 0.561379 1
999 99 0.035098 0.232043 0.982138 4
1000行×5列


我将数据集分为训练集、验证集和测试集
&lt;前&gt;&lt;代码&gt;
train_df = df.iloc[:600]
test_df = df.iloc[600:800]
val_df = df.iloc[800:]

qids_train = train_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_train = train_df.drop([“query_id”, “相关性”], axis=1)
y_train = train_df[“相关性”]

qids_validation =validation_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_validation =validation_df.drop([“query_id”,“相关性”], axis=1)
y_validation =validation_df[“相关性”]

qids_test = test_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_test = test_df.drop([“query_id”,“相关性”], axis=1)
y_test = test_df[“相关性”]

创建 lgb 数据集
lgb_train = lgb.Dataset(X_train, label=y_train, group=qids_train)
lgb_valid = lgb.Dataset(X_validation, label=y_validation, group=qids_validation)
lgb_test = lgb.Dataset(X_test, label=y_test, group=qids_test)


火车模型
param_ranking = {
    “目标”：“lambdarank”，
    “label_gain”：[int(i) for i in range(int(max(y_train.max(), y_validation.max())) + 1)],
    “公制”：[“ndcg”]，
    “评估时间”：5，
    “随机状态”：1，
    “冗长”：-1，
    # &#39;num_threads&#39;: 16,
    “学习率”：0.1，
}
回调 = [
    lgb.early_stopping(20),
    lgb.log_evaluation（周期=10）
]
model_gbm = lgb.train(
    参数排名，
    LGB_火车，
    100,
    valid_sets=[lgb_train, lgb_valid],
    回调=回调，
）

训练直到验证分数在 20 轮内没有提高
[10]训练的ndcg@5：0.880126 valid_1的ndcg@5：1
[20] 训练的 ndcg@5: 0.906826 valid_1 的 ndcg@5: 1
提前停止，最佳迭代是：
[1] 训练的 ndcg@5: 0.789559 valid_1 的 ndcg@5: 1


y_pred = model_gbm.predict(X_test)
X_test[“预测排名”] = y_pred
X_test.sort_values(“预测排名”, 升序=False)


我现在陷入困境，无法计算 map@k、hit@k、mrr、ndcg@k 等指标。如果有人可以帮忙，请解释如何解释 lgbm 模型预测，以便进一步计算测试数据集上的指标
def hit_rate(y_true, y_pred, k=5):
    点击数 = 0
    对于 true，pred 在 zip(y_true, y_pred) 中：
        top_indices = np.argsort(pred)[::-1][:k]
        如果 top_indices 为 true：
            命中数 += 1
    返回命中数 / len(y_true)
hit_rate(y_test, y_pred )
0.0

我尝试计算命中率，但得到了 0]]></description>
      <guid>https://stackoverflow.com/questions/78379350/lightgbm-ranking-metrics</guid>
      <pubDate>Wed, 24 Apr 2024 14:48:25 GMT</pubDate>
    </item>
    <item>
      <title>在 Oracle oml4r 中，库（ORE）在安装后无法工作</title>
      <link>https://stackoverflow.com/questions/78379323/in-oracle-oml4r-libraryore-is-not-working-after-install</link>
      <description><![CDATA[让 rstudio 服务器正常工作后，我按照这些说明启用了 oml4r - https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/2.0.0/oread/install-rstudio-server。 html。特别是我做了以下事情：
&#39;&#39;&#39;
sudo vi /etc/rstudio/rserver.conf
rsession-ld-library-path=R_HOME/lib:ORACLE_HOME/lib
&#39;&#39;&#39;
&#39;&#39;&#39;
cd /usr/lib64/R/etc
sudo vi Renviron.site
ORACLE_HOME=ORACLE_HOME
ORACLE_HOSTNAME=ORACLE_HOSTNAME
ORACLE_SID=ORACLE_SID
&#39;&#39;&#39;
&#39;&#39;&#39;
sudo rstudio-服务器重新启动
&#39;&#39;&#39;
但我仍然无法执行库（ORE）。是否还有其他包需要加载？如果是这样，我应该如何加载它们？]]></description>
      <guid>https://stackoverflow.com/questions/78379323/in-oracle-oml4r-libraryore-is-not-working-after-install</guid>
      <pubDate>Wed, 24 Apr 2024 14:44:12 GMT</pubDate>
    </item>
    <item>
      <title>合并两个不同的人工智能模型</title>
      <link>https://stackoverflow.com/questions/78376582/merging-two-different-ai-models</link>
      <description><![CDATA[我已经训练了两个模型，一个用于模糊检测，另一个用于两个不同数据集的曝光分类，现在我想合并这两个模型以获得执行这两项任务的单个组合模型
我希望组合模型通过将单个图像作为输入来预测模糊和曝光]]></description>
      <guid>https://stackoverflow.com/questions/78376582/merging-two-different-ai-models</guid>
      <pubDate>Wed, 24 Apr 2024 07:14:39 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow.keras.layers.Flatten() 抛出 INVALID_ARGUMENT 错误</title>
      <link>https://stackoverflow.com/questions/78375639/tensorflow-keras-layers-flatten-throwing-invalid-argument-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78375639/tensorflow-keras-layers-flatten-throwing-invalid-argument-error</guid>
      <pubDate>Wed, 24 Apr 2024 01:56:40 GMT</pubDate>
    </item>
    <item>
      <title>在使用 LR finder 代码后，如何获得使用循环学习率方法找到的最佳 LR 的学习率 (LR) 范围（最小值和最大值）？</title>
      <link>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</link>
      <description><![CDATA[我使用以下代码来获取给定神经网络模型的最佳学习率 - https://github.com/beringresearch/lrfinder/blob/master/lrfinder/lrfinder.py - 最终通过 get_best_lr 函数。因此，在获得最佳学习率的值后，如何以编程方式找出使用循环学习率 (CLR) 方法找到的最佳 LR 的 LR 边界（最小值和最大值）值 (https://arxiv.org/abs/1506.01186)?
来自引用的 GitHub 存储库的代码：
导入数学

将 matplotlib.pyplot 导入为 plt
导入tensorflow.keras.backend为K
将 numpy 导入为 np

从tensorflow.keras.callbacks导入LambdaCallback


LRFinder 类：
    ”“”
    训练的循环学习率中详细介绍了学习率范围测试
    神经网络 作者：Leslie N. Smith。学习率范围测试是一个测试
    这提供了有关最佳学习率的有价值的信息。期间
    预训练运行时，学习率线性增加或
    两个边界之间呈指数关系。较低的初始学习率允许
    网络开始收敛，并且随着学习率的增加
    最终会太大并且网络会发散。
    ”“”

    def __init__(自我，模型)：
        self.model = 模型
        自我损失= []
        自我学习率 = []
        self.best_loss = 1e9

    def on_batch_end（自身，批次，日志）：
        lr = K.get_value(self.model.optimizer.lr)
        self.learning_rates.append（lr）

        损失=日志[&#39;损失&#39;]
        self.losses.append(损失)

        如果批次&gt; 5 且 (math.isnan(loss) 或 loss &gt; self.best_loss * 4)：
            self.model.stop_training = True
            返回

        如果损失&lt; self.best_loss：
            self.best_loss = 损失

        lr *= self.lr_mult
        K.set_value(self.model.optimizer.lr, lr)

    def find(自我, 数据集, start_lr, end_lr, epochs=1,
             steps_per_epoch=无，**kw_fit）：
        如果steps_per_epoch为None：
            引发异常（&#39;正确训练数据生成器，&#39;
                            “steps_per_epoch”不能为“None”。”
                            &#39;你可以将其计算为&#39;
                            &#39;`np.ceil(len(TRAINING_LIST) / BATCH)`&#39;)

        self.lr_mult = (浮点(end_lr) /
                        浮动（start_lr））**（浮动（1）/
                                             浮点（纪元*steps_per_epoch））
        初始权重 = self.model.get_weights()

        Original_lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, start_lr)

        回调 = LambdaCallback(on_batch_end=lambda 批次,
                                  日志：self.on_batch_end（批次，日志））

        self.model.fit（数据集，
                       纪元=纪元，回调=[回调]，**kw_fit）
        self.model.set_weights(initial_weights)

        K.set_value(self.model.optimizer.lr,original_lr)

    def get_learning_rates(自我):
        返回（自我学习率）

    def get_losses(自身):
        返回（自我损失）

    def get_derivatives(self, sma):
        断言 sma &gt;= 1
        导数 = [0] * sma
        对于范围内的 i(sma, len(self.learning_rates))：
            衍生品.append((self.losses[i] - self.losses[i - sma]) / sma)
        回报衍生品

    def get_best_lr（自身，sma，n_skip_beginning = 10，n_skip_end = 5）：
        衍生品 = self.get_derivatives(sma)
        best_der_idx = np.argmin(导数[n_skip_beginning:-n_skip_end])
        返回 self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:17 GMT</pubDate>
    </item>
    <item>
      <title>使用函数式 API 进行迁移学习和量化感知训练</title>
      <link>https://stackoverflow.com/questions/72935089/transfer-learning-with-quantization-aware-training-using-functional-api</link>
      <description><![CDATA[我有一个正在使用 MobileNetV2 迁移学习的模型，我想对其进行量化，并将其与使用迁移学习的非量化模型的准确性差异进行比较。但是，它们并不完全支持递归量化，但根据此，此方法应该量化我的模型： https://github.com/tensorflow/model-optimization/issues/377#issuecomment-820948555
我尝试做的是：
导入tensorflow为tf
将tensorflow_model_optimization导入为tfmot
pretrained_model = tf.keras.applications.MobileNetV2(include_top=False)
pretrained_model.trainable = True
    
对于 pretrained_model.layers[:-1] 中的层：
    可训练层 = False
    
quantize_model_pretrained = tfmot.quantization.keras.quantize_model
q_pretrained_model = quantize_model_pretrained(pretrained_model)
    
    
原始输入 = tf.keras.layers.Input(形状=(224, 224, 3))
y = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(original_inputs)
y = 基础模型（原始输入）
y = tf.keras.layers.GlobalAveragePooling2D()(y)
原始输出 = tf.keras.layers.Dense(5, 激活 =“softmax”)(y)

model_1 = tf.keras.Model(原始输入, 原始输出)
量化模型 = tfmot.量化.keras.量化模型
q_aware_model = quantize_model(model_1)

它仍然给我以下错误：
ValueError：不支持在另一个 tf.keras 模型内量化 tf.keras 模型。

我想了解在这种情况下执行量化感知训练的正确方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/72935089/transfer-learning-with-quantization-aware-training-using-functional-api</guid>
      <pubDate>Mon, 11 Jul 2022 07:31:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 OneHotEncoder 时出现错误“预期为 2D 数组，改为 1D 数组”</title>
      <link>https://stackoverflow.com/questions/47957151/error-expected-2d-array-got-1d-array-instead-using-onehotencoder</link>
      <description><![CDATA[我是机器学习的新手，正在尝试解决使用 OneHotEncoder 类遇到的错误。错误是：“预期是二维数组，却得到了一维数组”。因此，当我想到一维数组时，它类似于： [1,4,5,6] ，而二维数组则为 [[2,3], [3,4], [ 5,6]]，但我仍然无法弄清楚为什么会失败。这条线失败了：
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

这是我的完整代码：
# 导入库
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd

# 导入数据集
数据集 = pd.read_csv(&#39;Data2.csv&#39;)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 5].values
df_X = pd.DataFrame(X)
df_y = pd.DataFrame(y)

# 替换缺失值
从 sklearn.preprocessing 导入 Imputer
imputer = Imputer（missing_values =&#39;NaN&#39;，策略=&#39;平均值&#39;，轴= 0）
imputer = imputer.fit(X[:, 3:5 ])
X[:, 3:5] = imputer.transform(X[:, 3:5])


# 对分类数据“名称”进行编码
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x = LabelEncoder()
X[:, 0] = labelencoder_x.fit_transform(X[:, 0])

# 转化为矩阵
onehotencoder1 = OneHotEncoder(categorical_features = [0])
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

# 编码分类数据“大学”
从 sklearn.preprocessing 导入 LabelEncoder
labelencoder_x1 = LabelEncoder()
X[:, 1] = labelencoder_x1.fit_transform(X[:, 1])

我确信您可以通过这段代码看出我有两列是标签。我使用标签编码器将这些列转换为数字。我想使用 OneHotEncoder 更进一步，将它们转换为矩阵，这样每一行都会有这样的内容： 

&lt;前&gt;&lt;代码&gt;0 1 0
1 0 1

我唯一想到的是我如何对标签进行编码。我一项一项地做，而不是一次全部做。不确定这就是问题所在。
我希望做这样的事情：
# 编码分类数据“名称”
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x = LabelEncoder()
X[:, 0] = labelencoder_x.fit_transform(X[:, 0])

# 转化为矩阵
onehotencoder1 = OneHotEncoder(categorical_features = [0])
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

# 编码分类数据“大学”
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x1 = LabelEncoder()
X[:, 1] = labelencoder_x1.fit_transform(X[:, 1])

# 转化为矩阵
onehotencoder2 = OneHotEncoder(categorical_features = [1])
X[:, 1] = onehotencoder1.fit_transform(X[:, 1]).toarray()

下面你会发现我的整个错误：
文件“/Users/jim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py”，第 441 行，在 check_array 中
    “如果它包含单个样本。”.format(array))

ValueError：需要 2D 数组，却得到 1D 数组：
数组=[ 2.1.3.2.3.5.5.0.4.0.]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

任何朝着正确方向的帮助都会很棒。]]></description>
      <guid>https://stackoverflow.com/questions/47957151/error-expected-2d-array-got-1d-array-instead-using-onehotencoder</guid>
      <pubDate>Sun, 24 Dec 2017 00:27:08 GMT</pubDate>
    </item>
    </channel>
</rss>