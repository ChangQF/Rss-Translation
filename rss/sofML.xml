<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 16 Apr 2024 03:16:18 GMT</lastBuildDate>
    <item>
      <title>提供 LSTM 密集层的先前时间戳预测作为下一个时间戳的附加输入</title>
      <link>https://stackoverflow.com/questions/78331808/providing-previous-time-stamp-prediction-of-dense-layer-of-lstm-as-additional-in</link>
      <description><![CDATA[下面是训练模型代码的一些部分
Prev_pred=无

对于范围内的 t(Ty)：

# 步骤 2.A：执行注意力机制的一步以获取步骤 t 的上下文向量
上下文 = one_step_attention(X[:,t,:], cnn_model_input, s)
打印（上下文.形状）

# 将后注意力 LSTM 单元应用到“上下文”向量。
如果 prev_pred 为 None：
  # 使用所需的形状初始化 prev_pred
  prev_pred = tf.keras.Input(shape=(1, 特征))
  prev_pred = tf.zeros_like(prev_pred)
  #print(prev_pred.shape)
别的 ：
  prev_pred=重复向量(1)(prev_pred)
  #print(prev_pred.shape)


s,_,c = LSTM(n_s, return_state = True)(上下文,initial_state=[s, c])

# 将 Dense 层应用于后置 LSTM 的隐藏状态输出
输出=密集（特征，激活=&#39;线性&#39;）（s）
上一个_pred=输出

# 步骤 2.D：追加“out”到“输出”列表（≈ 1 行）
输出.append(out)



# 创建模型
模型=模型（输入=[X_CNN_输入，X_lstm_输入，s0，c0]，输出=输出）

返回模型

以下是我的变量的维度。
上下文-&gt; （无，无，64）
s-&gt; （无，64）
出-&gt; （无，30）
prev_pred -&gt;;我已经让它成形了（无、1、30）

我尝试了很多事情，例如串联和在通道方向上向 prev_pred 添加 34 个长度的附加零后应用 Add() 层，以便可以轻松地添加上下文，但没有任何效果会出现不同的错误。&lt; /p&gt;
#context = tf.concat([context, prev_pred], axis=-1) # 与之前的预测连接
#context=Add()(context,prev_pred) # 已经在通道方向上为 prev_pred 添加了零，但是为了避免混淆，此处未包含该部分

上面这两种方法都不行
在我的代码中
n_s 值为 64
特征值为30

如果我既不使用串联也不使用添加，我的代码就可以正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/78331808/providing-previous-time-stamp-prediction-of-dense-layer-of-lstm-as-additional-in</guid>
      <pubDate>Tue, 16 Apr 2024 02:22:36 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在Android应用程序上同时运行两个音频分类tflite模型？</title>
      <link>https://stackoverflow.com/questions/78331786/is-it-possible-to-run-two-audio-classification-tflite-model-at-the-same-time-on</link>
      <description><![CDATA[我正在尝试修改 TensorFlow Lite 音频分类 Android 演示，以同时运行 YAMNet.tflite 和 voice.tflite 模型。我的目标是让应用程序在 YAMNet 检测到语音并且语音模型检测到向上、向下、向左或向右命令时做出反应。但是，由于 YAMNet 的输入张量为 (1,16000)，而语音的输入张量为 (1,44032)，因此这两个模型的输入张量不同，只有最先声明的分类器才会进行预测。
我想问是否有办法修改此示例代码以同时运行具有不同输入张量的两个模型？或者有没有更好的方法来实现我想要的功能？]]></description>
      <guid>https://stackoverflow.com/questions/78331786/is-it-possible-to-run-two-audio-classification-tflite-model-at-the-same-time-on</guid>
      <pubDate>Tue, 16 Apr 2024 02:12:13 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch - 保留前 K 个重复值</title>
      <link>https://stackoverflow.com/questions/78331706/pytorch-keep-first-k-repeating-values</link>
      <description><![CDATA[我有 2 个大小相同的一维张量，一个张量包含表示 id 的值，而另一个张量包含与该 id 关联的值。
例如
ids : 张量([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2])
值： 张量([2, 7, 1, 3, 4, 7, 8, 9, 3, 4, 2])

我想保留每个唯一 ID 的前 K 个值和 ID：
# 保留前 3 个（注意：由于 id 1 中只有 2 个，因此只需保留 2 个）
id : 张量([0, 0, 0, 1, 1, 2, 2, 2])
值： 张量([2, 7, 1, 4, 7, 8, 9, 3])

我不知道如何有效地解决这个问题。计算 id 的单个 for 循环运行速度太慢。
其他上下文
我使用它来尝试对专家混合变压器模型的令牌分配进行负载平衡。
假设有一些输入：
# [批次、标记、值]
x = 火炬.randn(2, 10, 3)

路由器登录：
# 5 位专家可供选择
门 = torch.nn.Linear(3, 5)
logits = torch.nn.function.softmax(gate(x), dim=-1, dtype=torch.float)

我首先从路由器获取每个令牌选定的专家：
权重，选定 = torch.topk(logits, 1)
权重，选定 = map(lambda x: x.squeeze(dim=-1), (权重，选定))

给定一定的容量C，然后我希望每个专家处理的令牌不要超过C。此外，如果向专家提供的 C 令牌以上，它应该处理路由器赋予的最高权重的令牌并跳过其余令牌。
我不太确定如何有效地解决这个问题。这是我的尝试，因此这个问题来自哪里：
首先按专家及其各自的权重对值进行排序：
重要性 = 所选内容 + 权重
指数 = torch.argsort(重要性, 降序=True)

# 选择的权重现在将按专家排序，并且在每个专家中按路由器权重排序
选定= torch.gather（选定，-1，索引）
权重 = torch.gather(权重, -1, 指数)

循环遍历每个专家和流程
结果 = torch.zeros_like(x)
对于我来说，枚举专家（专家）：
  批处理，令牌 = torch.where(selected == i)

  # 将标记映射到输入中的正确索引
  令牌=索引[批次，令牌]

  # 这里 `batch` 代表“ids”在原来的问题中
  # 和“token”代表“值”在原来的问题中
  #
  # 这是我需要保留每个“批次”的前 K 个的地方。其中 K 是
  # 容量`C`

  # 计算结果
  结果[批次，令牌] += 专家(
    输入[批次，令牌]
  ）
]]></description>
      <guid>https://stackoverflow.com/questions/78331706/pytorch-keep-first-k-repeating-values</guid>
      <pubDate>Tue, 16 Apr 2024 01:34:09 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 模型无法训练。帮我找到答案</title>
      <link>https://stackoverflow.com/questions/78331232/tensorflow-model-wont-train-help-me-find-the-answer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78331232/tensorflow-model-wont-train-help-me-find-the-answer</guid>
      <pubDate>Mon, 15 Apr 2024 22:06:04 GMT</pubDate>
    </item>
    <item>
      <title>是否仅仅因为训练损失低于验证损失就被认为是“过度拟合”？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78331085/is-it-considered-overfitting-just-because-the-training-loss-is-lower-than-vali</link>
      <description><![CDATA[一般来说，在训练机器学习模型时，如果验证损失高于训练损失，是否会被认为是过拟合？
在示例图像中，训练和验证损失都持续减少，但验证损失在大约 200 个时期与训练损失交叉，并且差异似乎在增加。这是否表明模型过度拟合？如果这种趋势随着进一步的训练而持续下去，并且训练/有效损失都较低但差异很大，这是一件坏事吗？
在示例图像中，训练和验证损失都持续减少，但验证损失在大约 200 个时期与训练损失交叉，并且差异似乎在增加。这是否表明模型过度拟合？如果这种趋势随着进一步的训练而持续下去，并且训练/有效损失都较低但差异很大，这是一件坏事吗？
]]></description>
      <guid>https://stackoverflow.com/questions/78331085/is-it-considered-overfitting-just-because-the-training-loss-is-lower-than-vali</guid>
      <pubDate>Mon, 15 Apr 2024 21:21:14 GMT</pubDate>
    </item>
    <item>
      <title>如何根据数据集的标签将数据集拆分为文件夹？</title>
      <link>https://stackoverflow.com/questions/78330964/how-to-split-a-dataset-into-folders-depending-in-its-labels</link>
      <description><![CDATA[我在一个文件夹中有一个图像数据集，我想根据包含所有图像 ID 及其标签的 csv 文件将其分成多个文件夹，因此我想阅读以进行检查和分离。另外我想在 Kaggle 上工作，那么如果我想将输出保存在工作目录中是否可以？
我尝试了以下代码，但它在某些方面不起作用。
&lt;前&gt;&lt;代码&gt;
`#首先将图像排序到子文件夹import pandas as pdimport osimport Shutil

将所有图像转储到一个文件夹中并指定路径：

data_dir = os.getcwd() + “/data/all_images/”

我们想要子文件夹的目标目录的路径

dest_dir = os.getcwd() + “/data/reorganized/”

读取包含图像名称和相应标签的csv文件

df= pd.read_csv(&#39;metadata.csv&#39;)print(df[&#39;dx&#39;].value_counts())

label=df[&#39;dx&#39;].unique().tolist() #提取标签到列表中label_images = []

将图像复制到新文件夹

for i in label:os.mkdir(dest_dir + str(i) + &quot;/&quot;)sample = df[df[&#39;dx&#39;] == i][&#39;image_id&#39;]label_images.extend (示例)for id in label_images:shutil.copyfile((data_dir + &quot;/&quot;+ id +&quot;.jpg&quot;), (dest_dir + i + &quot;/&quot;+id+&quot;.jpg&quot;))标签图像=[]

#现在我们准备好处理子文件夹中的图像了

FOR Keras 数据生成

#flow_from_directory 方法#当图像被排序并放置在相应的类/标签文件夹中时有用#从文件夹名称自动识别类。

创建数据生成器

from keras.preprocessing.image import ImageDataGeneratorimport osfrom matplotlib import pyplot as plt

#定义数据生成器。在这里我们可以定义要应用于 imagesdatagen = ImageDataGenerator() 的任何转换

定义包含子文件夹的训练目录

train_dir = os.getcwd() + “/data/reorganized/” ///这不起作用#USe flow_from_directorytrain_data_keras = datagen.flow_from_directory(directory=train_dir,class_mode=&#39;categorical&#39;,batch_size=16, #一次16张图像target_size=(32,32)) #调整图像大小

#我们可以检查单个批次的图像。x, y = next(train_data_keras)#查看范围 (0,15) 内的 i 的每个图像：image = x[i].astype(int)plt. imshow(图像)plt.show()`
]]></description>
      <guid>https://stackoverflow.com/questions/78330964/how-to-split-a-dataset-into-folders-depending-in-its-labels</guid>
      <pubDate>Mon, 15 Apr 2024 20:49:06 GMT</pubDate>
    </item>
    <item>
      <title>LCEL Lanchais RAG 顺序链的问题</title>
      <link>https://stackoverflow.com/questions/78330865/problem-with-lcel-lanchais-rag-sequential-chains</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78330865/problem-with-lcel-lanchais-rag-sequential-chains</guid>
      <pubDate>Mon, 15 Apr 2024 20:20:37 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用tensorflow.keras库训练神经网络时出错</title>
      <link>https://stackoverflow.com/questions/78330820/error-whilst-attempting-to-train-neural-network-using-tensorflow-keras-library</link>
      <description><![CDATA[我正在使用 tensorflow.keras 开发我的第一个 ML 深度学习项目
这是一个简单的彩票预测，我在完成在线课程后用它来积累我的知识。
输入 1 到 90 之间的 90 个整数值。
输出从输入的 90 个值中选择的 5 个值。
当执行拟合“model.fit(X, train_output_catg, epochs=50, batch_size=32)”时
我遇到错误：
ValueError：参数 target 和 output 必须具有相同的等级 (ndim)。收到：target.shape=(None, 5, 90), output.shape=(None, 5)
我网络上的输出层是：
输出层
model.add(Dense(5,activation=&#39;softmax&#39;))
我正在使用以下损失函数：
编译模型
model.compile（optimizer=&#39;adam&#39;，loss=&#39;categorical_crossentropy&#39;，metrics=[&#39;accuracy&#39;]）
因为我有整数，所以我使用以下方法将其转换为分类值：
#将 y 更改为分类值
train_output_catg = tf.keras.utils.to_categorical(y - 1, num_classes=None)
我明白问题所在，但不知道如何解决。
我的输出层形状如下“output.shape=(None, 5)”但我的目标形状是“target.shape=(None, 5, 90)”
我最初的“X”和“y”数组是：
X 形状：(2223, 90)
y 的形状：(2223, 5)
然后在“y”数组上，我执行了 to_categorical（如上所述），将其转换为 3 维数组，导致失败，因为输出数组和目标数组之间不匹配。
感谢您提供的任何帮助。
我期望函数 tf.keras.utils.to_categorical 为我提供与“y”数组相同的维度数组，但由于它添加了额外的维度，所以我遇到了上述错误。]]></description>
      <guid>https://stackoverflow.com/questions/78330820/error-whilst-attempting-to-train-neural-network-using-tensorflow-keras-library</guid>
      <pubDate>Mon, 15 Apr 2024 20:06:55 GMT</pubDate>
    </item>
    <item>
      <title>我可以在扩散模型中使用更简单的损失函数（例如直接似然损失）吗？</title>
      <link>https://stackoverflow.com/questions/78330493/can-i-use-a-simpler-loss-function-e-g-direct-likelihood-loss-in-diffusion-mod</link>
      <description><![CDATA[训练扩散模型时，可以选择定义损失函数。常见的包括根据高斯分布的均值或噪声定义损失函数。但它总是从最大化预测 x0 为或接近实际 x0 的可能性开始。然后需要数学推导来用平均值或噪声来表达损失。
我的问题是我们可以直接使用（负）可能性作为损失吗？ IE。在训练步骤中，我们知道 Xt 并尝试预测 Xt-1，然后对于每个像素 (i, j) 如果预测的 &lt; code&gt;Xt-1(i,j) != 实际的 Xt-1(i,j) 那么这就会导致损失。我所描述的只是训练/收敛速度慢还是不正确？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78330493/can-i-use-a-simpler-loss-function-e-g-direct-likelihood-loss-in-diffusion-mod</guid>
      <pubDate>Mon, 15 Apr 2024 18:54:09 GMT</pubDate>
    </item>
    <item>
      <title>实施机器学习进行自动车辆维护：可行性和挑战[关闭]</title>
      <link>https://stackoverflow.com/questions/78330407/implementing-machine-learning-for-automated-vehicle-maintenance-feasibility-and</link>
      <description><![CDATA[我正在探索将机器学习算法整合到车辆维护系统中以自动检测组件故障的想法。这一概念涉及嵌入整个车辆的传感器，持续监控其各个部件，如果任何关键部件发生故障，系统会检测到它并自动触发适当的操作。
我正在向社区寻求有关此方法的可行性以及开发人员在实施过程中可能遇到的潜在挑战的见解。具体来说，我想知道：
将机器学习算法集成到现有车辆架构中以进行自动故障检测的可行性如何？
开发准确可靠的车辆故障检测模型的关键技术考虑因素和挑战是什么？
您是否有推荐的用于实施基于机器学习的车辆维护系统的现有框架、库或最佳实践？
此外，如果有人对类似项目或计划有第一手经验或了解，我将非常感谢您可以分享的任何见解或经验教训。
我目前正在研究这个主题一段时间，但到目前为止还没有取得突破，如果我能得到关于这个主题的任何建议，那将是一个很大的帮助。我从上个月参加的一次黑客马拉松中出现的类似问题中得到了这个想法。]]></description>
      <guid>https://stackoverflow.com/questions/78330407/implementing-machine-learning-for-automated-vehicle-maintenance-feasibility-and</guid>
      <pubDate>Mon, 15 Apr 2024 18:34:32 GMT</pubDate>
    </item>
    <item>
      <title>深度学习训练准确性显着变化[关闭]</title>
      <link>https://stackoverflow.com/questions/78330285/deep-learning-training-accuracy-changes-significantly</link>
      <description><![CDATA[在此处输入图像描述
我正在 DQN 中训练一个深度网络，用于使用 2 臂机器人投掷球。状态为（末端执行器位置、投掷角度、关节值）。这些动作以不同的值移动关节。网络参数为（L_r 0.003、no_layers 10、no_neurons 32、no_epochs 50）
无论我在训练中改变什么，损失函数和准确率总是这样（准确率不超过20%）。我改变了学习率，不行。层，没有神经元和纪元，但我仍然无法达到超过 20% 的准确率。我该如何改进？]]></description>
      <guid>https://stackoverflow.com/questions/78330285/deep-learning-training-accuracy-changes-significantly</guid>
      <pubDate>Mon, 15 Apr 2024 18:04:46 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的有效张量乘法</title>
      <link>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</link>
      <description><![CDATA[有谁知道我如何有效地计算两个张量乘法 - 例如，我有两个形状为 (15, 256) 和 (112, 256) 的张量，它们的乘积为形状为 (15, 112) 的张量可以是以7微秒计算。但是，如果我有像 A - (15, 100, 256) 和 B - (112, 2000, 256) 这样的张量，并且我会做出像 C = (A.reshape(-1, 256) @ B.reshape(256, - 1).reshape(15, 100, 112, 2000).permute(0, 2, 1, 3).max(-1).values.sum(-1) 得到形状为(15, 112)的张量，需要 1000 倍的时间才能计算得更快吗？
我知道第二个计算应该比第一个计算大得多，但也许它需要的时间比我的实现要少]]></description>
      <guid>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</guid>
      <pubDate>Mon, 15 Apr 2024 17:49:57 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的过度拟合故障排除：优化正则化和数据预处理</title>
      <link>https://stackoverflow.com/questions/78330178/troubleshooting-overfitting-in-neural-networks-optimizing-regularization-and-da</link>
      <description><![CDATA[我正在开展一个机器学习项目，并在神经网络的训练过程中遇到了问题。尽管使用了 dropout 和批量归一化等技术，我仍然观察到训练数据的过度拟合。这是否是由于我的正则化参数配置错误或数据预处理管道中的缺陷造成的？任何有关如何解决此问题的见解或建议将不胜感激。
在我的机器学习项目中，我使用 dropout 和批量归一化等先进技术实现了神经网络架构，以减轻过度拟合。然而，尽管做出了这些努力，我仍然在训练过程中观察到过度拟合行为。我怀疑该问题可能源于正则化参数的错误配置或数据预处理管道中的缺陷。我正在寻求有关如何有效排除和解决这个持续存在的过度拟合问题的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78330178/troubleshooting-overfitting-in-neural-networks-optimizing-regularization-and-da</guid>
      <pubDate>Mon, 15 Apr 2024 17:42:52 GMT</pubDate>
    </item>
    <item>
      <title>更改 pytorch 中中毒的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/78329685/change-input-for-poisoning-in-pytorch</link>
      <description><![CDATA[实际上，我有这段代码，最初下载了 cifar 10 数据集并对图像应用了中毒（编辑右下角）
然而，此时我需要使用本地的图像，数据集分为 5 个文件夹，其中每个文件夹都是不同的类。这是我尝试编写的代码，但它给了我错误：
将 numpy 导入为 np
从复制导入深复制

标准化参数 = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]
输入大小 = 224
路径=&#39;./DB&#39;


defgenerate_trigger(trigger_type):
    if trigger_type == &#39;checkerboard_1corner&#39;: # 棋盘位于右下角
        模式 = np.zeros(形状=(32, 32, 1), dtype=np.uint8) + 122
        掩码 = np.zeros(形状=(32, 32, 1), dtype=np.uint8)
        触发值 = [[0, 0, 255], [0, 255, 0], [255, 0, 255]]
        触发区域 = [-1, 0, 1]
        对于trigger_region中的h：
            对于trigger_region中的w：
                模式[30 + h, 30 + w, 0] = 触发值[h + 1][w + 1]
                掩码[30 + h, 30 + w, 0] = 1
    返回模式、掩码


def add_trigger_dirty_label（data_set，trigger_type，poison_rate，poison_target，trigger_alpha = 1.0）：
    模式，掩码=generate_trigger(trigger_type=trigger_type)
    poison_cand = [i for i in range(len(data_set.targets)) if data_set.targets[i] !=poison_target]
    poison_set = 深度复制(data_set)
    poison_num = int(poison_rate * len(poison_cand))
    选择= np.random.choice(poison_cand,毒药_num,replace=False)

    对于选择中的 idx：
        orig=poison_set.data[idx]
        poison_set.data[idx] = np.clip(
            (1 - 掩码) * 原始 + 掩码 * ((1 - 触发阿尔法) * 原始 + 触发阿尔法 * 模式), 0, 255
        ).astype(np.uint8)
        poison_set.targets[idx] =poison_target
    trigger_info = {&#39;trigger_pattern&#39;: 模式[np.newaxis, :, :, :], &#39;trigger_mask&#39;: 掩码[np.newaxis, :, :, :],
                    &#39;trigger_alpha&#39;：trigger_alpha，&#39;poison_target&#39;：np.array（[poison_target]），
                    &#39;data_index&#39;：选择}
    返回poison_set、trigger_info


def add_trigger_clean_label（数据集，trigger_type，poison_rate，poison_target，trigger_alpha = 1.0）：
    模式，掩码=generate_trigger(trigger_type=trigger_type)
    poison_cand = [i for i in range(len(data_set.targets)) if data_set.targets[i] ==poison_target]
    poison_set = 深度复制(data_set)
    poison_num = int(poison_rate * len(poison_cand))
    选择= np.random.choice(poison_cand,毒药_num,replace=False)

    对于选择中的 idx：
        orig=poison_set.data[idx]
        poison_set.data[idx] = np.clip(
            (1 - 掩码) * 原始 + 掩码 * ((1 - 触发阿尔法) * 原始 + 触发阿尔法 * 模式), 0, 255
        ).astype(np.uint8)
    trigger_info = {&#39;trigger_pattern&#39;: 模式[np.newaxis, :, :, :], &#39;trigger_mask&#39;: 掩码[np.newaxis, :, :, :],
                    &#39;trigger_alpha&#39;：trigger_alpha，&#39;poison_target&#39;：np.array（[poison_target]），
                    &#39;data_index&#39;：选择}
    返回poison_set、trigger_info


def add_predefined_trigger(data_set,trigger_info):
    如果trigger_info为None：
        返回数据集

    poison_set = 深度复制(data_set)

    模式=trigger_info[&#39;trigger_pattern&#39;]
    掩码=trigger_info[&#39;trigger_mask&#39;]
    阿尔法=触发器信息[&#39;trigger_alpha&#39;]

    对于范围内的 idx(len(poison_set))：
        orig=poison_set.data[idx]
        poison_set.data[idx] = np.clip(
            (1 - 掩模) * orig + 掩模 * ((1 - alpha) * orig + alpha * 图案), 0, 255
        ).astype(np.uint8)

    返回poison_set

从torchvision导入数据集，转换


data_transforms = 变换.Compose([
    变换.调整大小(INPUT_SIZE),
    变换.ToTensor(),
    变换.Normalize(NORMALIZATION_PARAMS[0], NORMALIZATION_PARAMS[1])
]）

image_dataset = datasets.ImageFolder(root=PATH, 变换=data_transforms)
#image_dataset = CIFAR10(root=&#39;./&#39;,train=True,download=True,transform=data_transforms)

中毒率 = 0.1
毒物目标 = 1

poisoned_dataset，trigger_info = add_trigger_dirty_label（image_dataset，&#39;checkerboard_1corner&#39;，poison_rate，poison_target）

请注意以下错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\enric\PycharmProjects\Avvelenatore\main.py”，第 92 行，在  中
    poisoned_dataset，trigger_info = add_trigger_dirty_label（image_dataset，&#39;checkerboard_1corner&#39;，poison_rate，poison_target）
  文件“C:\Users\enric\PycharmProjects\Avvelenatore\main.py”，第 30 行，位于 add_trigger_dirty_label
    orig=poison_set.data[idx]
  文件“C:\Users\enric\anaconda3\envs\pytorch\lib\site-packages\torch\utils\data\dataset.py”，第 83 行，在 __getattr__ 中
    引发属性错误
属性错误

你能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78329685/change-input-for-poisoning-in-pytorch</guid>
      <pubDate>Mon, 15 Apr 2024 16:00:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中计算 TAR (%) @ FAR = 0.1%？</title>
      <link>https://stackoverflow.com/questions/71420229/how-do-i-calculate-tar-far-0-1-in-python</link>
      <description><![CDATA[我正在阅读一篇论文，论文中的结果以如下方式呈现：

我想为我的模型创建一个类似的表格。使用下面的代码我得到了 FAR 和 TAR 值。
来自 sklearn 导入指标

测试 = [0, 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,1 ,0 ,0 ,0 ,1, 0]
pred = [0.04172871, 0.01611879, 0.01073375, 0.03344169 ,0.04172871, 0.04172871, 0.00430162 ,0.04172871 ,0.04172871 ,0.04172871 ,0.07977659, 0.905772,0.9396076,  0.03344169, 0.04172871, 0.09125287, 0.02964183, 0.0641269,0.04172871 ,0.04172871, 0.04172871, 0.0641269 , 0.04172871, 0.04172871 ,0.9919831,0.04172871,0.01611879,0.04172871,0.37865442,0.00240888]
远，焦油，阈值=metrics.roc_curve（Y_test，p）

我应该如何修复 FAR = 0.1% 以及如何使用 Python 计算 TAR% @FAR = 0.1%？]]></description>
      <guid>https://stackoverflow.com/questions/71420229/how-do-i-calculate-tar-far-0-1-in-python</guid>
      <pubDate>Thu, 10 Mar 2022 07:25:22 GMT</pubDate>
    </item>
    </channel>
</rss>