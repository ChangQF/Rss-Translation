<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 10 Jul 2024 12:28:56 GMT</lastBuildDate>
    <item>
      <title>在 Android Studio 中集成已训练的模型（Java）</title>
      <link>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio-java</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio-java</guid>
      <pubDate>Wed, 10 Jul 2024 11:31:14 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中的 nestedcv 包中的 nestcv.train 对象上调用 summary() 和 train_summary() 有什么区别？</title>
      <link>https://stackoverflow.com/questions/78729334/what-is-the-difference-between-calling-summary-and-train-summary-on-a-nestcv</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78729334/what-is-the-difference-between-calling-summary-and-train-summary-on-a-nestcv</guid>
      <pubDate>Wed, 10 Jul 2024 08:11:17 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器，我可以将模型传递给自定义损失吗？</title>
      <link>https://stackoverflow.com/questions/78729235/xgboost-classifier-can-i-pass-the-model-to-the-custom-loss</link>
      <description><![CDATA[我有一个特殊的二元分类用例，根据模型的决策，下一个评估数据会发生变化。
示例：
[[x0,y0], [x1,y1], [x2,y2]...]

如果模型预测 x0 为 1，则下一个评估点为 [x1,y1]。

如果模型预测 x0 为 0，则下一个评估点为 [x2,y2]。

起初，我以为我会在所有点上训练模型，而且在评估取决于先前预测的最终场景中，它会很好，但事实并非如此。
我开发了一个函数，以我解释的相互依赖方式计算评估函数，并发现当整个点集的损失函数改进时，它并没有改进。
即使我在同一组数据上进行训练和验证，它也没有改善。
因此，我想修改损失函数，使其使用模型选择用于计算损失的训练点子集，具体取决于模型的当前状态（在每个提升步骤中）。
我相信这在理论上应该是可能的。我的问题是：
你也认为这是可能的吗？
如果可能，我该如何将模型传递给自定义损失？
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78729235/xgboost-classifier-can-i-pass-the-model-to-the-custom-loss</guid>
      <pubDate>Wed, 10 Jul 2024 07:49:45 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实现循环神经网络</title>
      <link>https://stackoverflow.com/questions/78729202/recurrent-neural-network-implementation-from-scratch</link>
      <description><![CDATA[它给了我错误，我想是没有完全很好地实现吧？
我不知道我是否做对了所有事情，我需要帮助从头开始编写一个 RNN，用于我的机器人进行外国预测，它将使用价格来学习，我设定的目标是 [1,0] 用于学习买入，[0,1] 用于学习卖出，我还想包括强化学习，这将帮助它更好地学习，我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78729202/recurrent-neural-network-implementation-from-scratch</guid>
      <pubDate>Wed, 10 Jul 2024 07:42:56 GMT</pubDate>
    </item>
    <item>
      <title>像 medium 或 daily dev 这样的网站如何选择您的兴趣并为您提供相关的信息，它们使用了哪些技术？[关闭]</title>
      <link>https://stackoverflow.com/questions/78729154/how-do-websites-like-medium-or-daily-dev-gets-to-select-your-interests-and-provi</link>
      <description><![CDATA[我想使用与 medium 和 daily dev 相同的功能，这些功能可让您选择您感兴趣的领域，然后为您提供相关博客作为您的订阅源。我不需要使用所有这些功能，但我想知道如何实现这样的功能。我希望能够了解用户的兴趣和知识/专业知识领域以及需求，然后根据这些信息为他们提供相关信息。我猜会涉及机器学习、人工智能等技术，如果我能设法了解创建此类功能所涉及的技术，这将对我大有帮助。
非常感谢任何能尽其所能帮助我的用户。干杯！！
我曾尝试使用 chatGPT 来了解它，但答案有点太模糊了。似乎直接联系相关行业的开发人员会更好、更高效]]></description>
      <guid>https://stackoverflow.com/questions/78729154/how-do-websites-like-medium-or-daily-dev-gets-to-select-your-interests-and-provi</guid>
      <pubDate>Wed, 10 Jul 2024 07:30:23 GMT</pubDate>
    </item>
    <item>
      <title>加载在自定义数据集上训练的 Yolov9 模型：AttributeError：“str”对象没有属性“shape”</title>
      <link>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</link>
      <description><![CDATA[我已经在自定义数据集上训练了一个 yolov9 模型，用于实例分割，现在我想在分割后获得分割区域。
输出如下图所示，但针对图像中分割的每个对象。

from pathlib import Path
import numpy as np
import torch
import cv2

model = torch.hub.load(&#39;.&#39;, &#39;custom&#39;, path=&#39;yolov9-inst/runs/train-seg/gelan-c-seg15/weights/best.pt&#39;, source=&#39;local&#39;) 
# Image
img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
# 推理
res = model(img)

# 迭代检测结果（对多张图片有帮助）
for r in res:
img = np.copy(r.orig_img)
img_name = Path(r.path).stem # 源图片 base-name

# 迭代每个对象轮廓（多次检测）
for ci, c in enumerate(r):
# 获取检测类名
label = c.names[c.boxes.cls.tolist().pop()]

# 创建二进制掩码
b_mask = np.zeros(img.shape[:2], np.uint8)

# 提取轮廓结果
contour = c.masks.xy.pop()
# 更改类型
contour = contour.astype(np.int32)
# 重塑
contour = contour.reshape(-1, 1, 2)

# 将轮廓绘制到掩码上
_ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)

但我在仅查找 res 时收到此错误。
YOLO 🚀 v0.1-104-g5b1ea9a Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (NVIDIA RTX A5000, 24248MiB)

融合层...
gelan-c-seg-custom 摘要：414 层、27364441 个参数、0 个梯度、144.2 GFLOP
警告 ⚠️ YOLO SegmentationModel 尚不兼容 AutoShape。您将无法使用此模型运行推理。
------------------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
Cell In[84]，第 6 行
4 img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
5 # 推理
----&gt; 6 结果 = model(img)

文件 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518，位于 Module._wrapped_call_impl(self, *args, **kwargs)
1516 返回 self._compiled_call_impl(*args, **kwargs) # 类型：ignore[misc]
1517 else:
-&gt; 1518 return self._call_impl(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
1522 # 如果我们没有任何钩子，我们希望跳过此函数中的其余逻辑
1523 # 并只调用 forward。
1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
1525 or _global_backward_pre_hooks or _global_backward_hooks
1526 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527 返回 forward_call(*args, **kwargs)
1529 尝试：
1530 结果 = 无

文件 /workspace/yolov9-inst/./models/common.py:868，在 DetectMultiBackend.forward(self, im, augment, visualize) 中
866 def forward(self, im, augment=False, visualize=False):
867 # YOLO MultiBackend 推理
--&gt; 868 b, ch, h, w = im.shape # 批次、通道、高度、宽度
869 if self.fp16 and im.dtype != torch.float16:
870 im = im.half() # 到 FP16

AttributeError: &#39;str&#39; 对象没有属性 &#39;shape&#39;

请问有人能帮我解决这个问题吗]]></description>
      <guid>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</guid>
      <pubDate>Wed, 10 Jul 2024 06:10:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov8 在组织内通过多个摄像头对员工进行 reiID 和跟踪 [关闭]</title>
      <link>https://stackoverflow.com/questions/78728683/reiid-and-track-employees-across-multiple-cameras-within-the-organization-with-y</link>
      <description><![CDATA[识别、重新识别并跟踪人员在组织摄像头中的路线。
是否可以使用 Yolo 重新识别并绘制任何在组织摄像头中看到的人的路线？在官方文件中，我找不到任何合适的答案。
PS：这些摄像头很旧，分辨率只有 4 兆像素，所以我不想通过他们的脸来跟踪他们，而是通过他们的西装来跟踪他们。]]></description>
      <guid>https://stackoverflow.com/questions/78728683/reiid-and-track-employees-across-multiple-cameras-within-the-organization-with-y</guid>
      <pubDate>Wed, 10 Jul 2024 05:05:10 GMT</pubDate>
    </item>
    <item>
      <title>从示例数据集重新创建文本嵌入</title>
      <link>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</link>
      <description><![CDATA[我现在的情况是，我有一个句子列表，以及一个 25 维向量上的理想嵌入列表。我试图使用神经网络来生成新的编码，但很挣扎。虽然模型运行良好，但其输出毫无意义，甚至无法准确复制训练数据！
import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 标记化
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentence_list)
sequences = tokenizer.texts_to_sequences(sentence_list)

# 假设您的向量是 25 维
input_dim = 25

# 定义编码器
input_vec = Input(shape=(max_sequence_length,))
encoded = Dense(25,activation=&#39;tanh&#39;)(input_vec) # 缩减至 16 维的示例
encoder = Model(input_vec,coded)

# 定义解码器
decoded = Dense(input_dim,activation=&#39;sigmoid&#39;)(encoded)
autoencoder = Model(input_vec,coded)

# 编译模型
autoencoder.compile(optimizer=Adam(),loss=&#39;mse&#39;)

# 训练模型
autoencoder.fit(padded_sequences, combined_vectors_clean,
epochs=10,
batch_size=32,
shuffle=True,validation_split= 0.2)


据我所知，我的输入和标签没有任何问题，那么我遗漏了什么？非常感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</guid>
      <pubDate>Wed, 10 Jul 2024 01:39:37 GMT</pubDate>
    </item>
    <item>
      <title>Azure Custom Vision 从网络摄像头获取图像，然后返回图像</title>
      <link>https://stackoverflow.com/questions/78727554/azure-custom-vision-to-get-an-image-from-webcam-and-then-return-an-image</link>
      <description><![CDATA[我希望创建一项服务，将网络摄像头中的图像发送到 Azure Custom Vision，并让其返回它认为匹配的图像。这可以用于纸牌游戏，因此如果您在网络摄像头上显示黑桃 A，它将能够返回黑桃 A 的图像。这是否适合 Azure Custom Vision？我创建了一个项目并上传和标记了图像，但我还没有看到这样的用例。]]></description>
      <guid>https://stackoverflow.com/questions/78727554/azure-custom-vision-to-get-an-image-from-webcam-and-then-return-an-image</guid>
      <pubDate>Tue, 09 Jul 2024 19:57:34 GMT</pubDate>
    </item>
    <item>
      <title>如何显示图像和预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78725594/how-to-display-images-and-predictions</link>
      <description><![CDATA[我有这样的代码，你可以访问 GitHub，整个过程很相似，只是架构和数据集不同
https://github.com/cendekialnazalia/CaisimPestDetection/blob/main/Percobaan%20E%20-%20CNN%20add%20Models%20Xception.ipynb
结果 CM 了解更多信息
[![在此处输入图片描述][1]][1]
在最后一行代码“对测试集进行预测并生成混淆矩阵和分类报告”之后，我添加了类似下面的代码来找出每个测试数据的预测值
test_gen.class_indices

print(preds,preds.shape)

result_index = np.argmax(preds[])
print(result_index)

for i in range(len(preds)):
if(np.argmax(preds[i]) == 0):
print(&quot;Bercak Daun&quot;)
elif(np.argmax(preds[i]) == 1):
print(&quot;Daun Sehat&quot;)
elif(np.argmax(preds[i]) == 2):
print(&quot;Karat Merah&quot;)
else:
print(&quot;Lainya&quot;)

输出
Bercak Daun
Daun Sehat
.
.
.
最多 177
Daun Sehat

除了显示带有字符串的预测之外，我还想将其与图像一起显示。也许有更好、更高效的代码可以解决我的问题，请帮我回答，因为我还是个初学者，想学习]]></description>
      <guid>https://stackoverflow.com/questions/78725594/how-to-display-images-and-predictions</guid>
      <pubDate>Tue, 09 Jul 2024 12:27:54 GMT</pubDate>
    </item>
    <item>
      <title>如何使用我的代码可视化预测样本需要更多答案</title>
      <link>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-my-code-need-more-answers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-my-code-need-more-answers</guid>
      <pubDate>Mon, 08 Jul 2024 04:07:24 GMT</pubDate>
    </item>
    <item>
      <title>具有 2 个以上类别的 Tensorflow 神经网络</title>
      <link>https://stackoverflow.com/questions/61556662/tensorflow-neural-network-with-more-than-2-categories</link>
      <description><![CDATA[因此，我在 udemy 上观看了 tensorflow 教程并决定自己尝试一下，他说如果您想要超过 2 个类别，请将激活更改为“softmax”并将单位更改为 4，因为我有 4 个不同的类别（从 0:1 更改为 1:4），如果“y”中只有 2 个不同的值，则一切正常，但是一旦我将其更改为 4 个单位和 4 个类别，我就会收到错误：
ValueError：检查目标时出错：预期 density_3 具有形状 (4,)，但得到的数组具有形状 (1,)
即使将其改回形状“1”只产生真或假类别的结果
我的数据集在 y 中：

import numpy as np
from sklearn.metrics import confused_matrix
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense

dataset = np.load(&#39;/Users/alex/desktop/ANN_dataset_for_brightness.npy&#39;)
X = dataset[:, 0:17]
y = dataset[:, 17:19]

for i in range (27):
if y[i] == 400:
y[i] = 4
elif y[i] == 300:
y[i] = 3
elif y[i] == 200:
y[i] = 2
elif y[i] == 100:
y[i] = 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# 初始化 ANN
classifier = Sequential()

# 添加输入层和第一个隐藏层 // 输入层的输入 dim
classifier.add(Dense(activation=&quot;relu&quot;, input_dim=17, unit=6, kernel_initializer=&quot;uniform&quot;))

# 添加第二个隐藏层层
classifier.add(Dense(activation=&quot;relu&quot;, unit=6, kernel_initializer=&quot;uniform&quot;))

问题出在这里
# 单位 = 类别，softmax = 大于 2
classifier.add(Dense(activation=&quot;softmax&quot;, unit=4, kernel_initializer=&quot;uniform&quot;))

# 编译 ANN
classifier.compile(optimizer = &#39;adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

# 将 ANN 拟合到训练集
classifier.fit(X_train, y_train, batch_size = 27, nb_epoch = 100)

# 第 3 部分 - 进行预测并评估模型

# 预测测试集结果
y_pred = classifier.predict(X_test)
y_pred = (y_pred &gt; 0.5)

# 制作混淆矩阵
cm = confused_matrix(y_test, y_pred)
]]></description>
      <guid>https://stackoverflow.com/questions/61556662/tensorflow-neural-network-with-more-than-2-categories</guid>
      <pubDate>Sat, 02 May 2020 08:48:03 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit learn 中实现自定义损失函数</title>
      <link>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中实现自定义损失函数。我使用以下代码片段：
def my_custom_loss_func(y_true,y_pred):
diff3=max((abs(y_true-y_pred))*y_true)
return diff3

score=make_scorer(my_custom_loss_func,greater_ is_better=False)
clf=RandomForestRegressor()
mnn= GridSearchCV(clf,score)
knn = mnn.fit(feam,labm) 

传递给 my_custom_loss_func 的参数应该是什么？我的标签矩阵称为 labm。我想计算实际输出与预测输出（由模型）乘以真实输出之间的差值。如果我使用 labm 代替 y_true，那么我应该使用什么代替 y_pred？]]></description>
      <guid>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</guid>
      <pubDate>Sat, 19 Jan 2019 13:47:47 GMT</pubDate>
    </item>
    <item>
      <title>如何计算伯努利朴素贝叶斯的联合对数似然</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[对于使用 BernoulliNB 的分类问题，如何计算联合对数似然。联合似然由以下公式计算，其中 y(d) 是实际输出（不是预测值）的数组，x(d) 是特征的数据集。
我阅读了这个答案并阅读了文档，但它并没有完全满足我的目的。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CPU 上的 Keras LSTM 比 GPU 快三倍？</title>
      <link>https://stackoverflow.com/questions/52481006/why-is-keras-lstm-on-cpu-three-times-faster-than-gpu</link>
      <description><![CDATA[我使用Kaggle 的这个笔记本来运行 LSTM 神经网络。
我已经开始训练神经网络，我发现它太慢了。它比 CPU 训练慢了近三倍。

CPU 性能：每轮 8 分钟；
GPU 性能：每轮 26 分钟。

之后我决定在 Stackoverflow 上的这个问题 中寻找答案，并且我应用了 CuDNNLSTM （仅在 GPU 上运行） 而不是 LSTM。 
因此，GPU 性能变为每 epoch 仅 1 分钟，模型准确率下降 3%。
问题： 
1) 有人知道为什么 GPU 在经典 LSTM 层中比 CPU 运行得慢吗？我不明白为什么会发生这种情况。
2) 为什么当我使用 CuDNNLSTM 而不是 LSTM 时，训练变得更快，模型准确率下降？
附注：
我的 CPU： Intel Core i7-7700 处理器（8M 缓存，最高 4.20 GHz）
我的 GPU： nVidia GeForce GTX 1050 Ti（4 GB）]]></description>
      <guid>https://stackoverflow.com/questions/52481006/why-is-keras-lstm-on-cpu-three-times-faster-than-gpu</guid>
      <pubDate>Mon, 24 Sep 2018 13:56:09 GMT</pubDate>
    </item>
    </channel>
</rss>