<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 27 Nov 2024 18:24:36 GMT</lastBuildDate>
    <item>
      <title>什么是不规则模式回归算法？</title>
      <link>https://stackoverflow.com/questions/79231116/what-is-irregular-pattern-regression-algorithm</link>
      <description><![CDATA[回归算法在定义明确的关系数据实体上可充当成熟的模型，但对于脉动数据集效率不高。但是，处理高不规则性的数据更为复杂，因为它的要求主要是不可避免的。本文提出了一种新算法，使用基于波动的计算来估计非均匀或脉动数据的连续结果。所提出的处理不规则数据的独特方法将解决回归技术的一个关键应用。
回归算法是机器学习的基础，它通过对输入特征和目标变量之间的关系进行建模来预测连续结果。与将数据点分配给离散类别的分类不同，回归预测数值，这使其在金融、医疗保健和环境科学等领域至关重要。例如，预测房价或股市趋势在很大程度上依赖于回归模型。这些算法旨在最大限度地减少预测误差，确保结果准确可靠。简单线性回归使用直线对一个独立变量和一个因变量之间的关系进行建模。对于更复杂的场景，多元线性回归考虑了几个独立变量。多项式回归等高级技术可以捕捉非线性关系，而岭回归和套索回归则通过正则化解决过度拟合和特征选择问题。弹性网络结合了这些方法，以提高灵活性。支持向量回归 (SVR) 和决策树回归等机器学习技术可以有效地处理非线性数据，而随机森林和梯度提升等集成方法则通过聚合多个模型来提高准确性。神经网络在深度学习中很受欢迎，擅长处理大规模和复杂的数据。均方误差 (MSE) 和 R 平方等评估指标可确保模型准确性。随着计算方法和自动化工具的进步，回归继续推动各个行业的创新。
(PDF) 不规则模式回归算法。出处：https://www.researchgate.net/publication/386046516_Irregular_Pattern_Regression_Algorithm [2024 年 11 月 27 日访问]。]]></description>
      <guid>https://stackoverflow.com/questions/79231116/what-is-irregular-pattern-regression-algorithm</guid>
      <pubDate>Wed, 27 Nov 2024 16:34:29 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么 ML/优化算法来找到最大输出的最佳输入值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</link>
      <description><![CDATA[我有一些数据，需要找到 X*Y 的最佳组合，以便因变量 A、B、C 达到最大值。
所有这些都是简单的数值，就像科学实验的观察结果一样。因此，A、B、C 是在同一范围内具有不同值的性能变量，而 X 和 Y 是两个测量变量。
基本上，我想制作一个程序来不断处理未来实验中涌入的任何数据。
我遇到过有人使用 ANN 回归来解决类似的问题，还有人建议使用梯度下降。由于我必须从头开始完成这项任务，如果我能建议我应该从哪种算法开始，我将非常高兴。]]></description>
      <guid>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</guid>
      <pubDate>Wed, 27 Nov 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>Python 和 Maple SVR 结果中的差异</title>
      <link>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</link>
      <description><![CDATA[我正在使用最小二乘 SVR 方法求解积分方程。我应该将下面链接中提供的 Maple 代码转换为 Python。我已编写如下所示的 Python 代码，但无论我做什么，都无法获得准确的结果。您能告诉我如何让我的代码产生与 Maple 相同的结果吗？
https://github.com/alirezaafzalaghaei/LSSVR-FIE/blob/master/paper-examples/example-4/CLS-SVR-dual.mw
import numpy as np
from scipy.integrate import quad
from scipy.special import legendre
from scipy.optimize import minimal
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.stats import qmc
from scipy.integrate import quad
from scipy.integrate import dblquad
import time
from tensorflow.keras import regularizers
from tensorflow.keras.losses import MeanSquaredError

# 设置精度
np.set_printoptions(precision=15)

# 定义积分函数
def Quad(f, a, b):
# 注意：这是一个简化版本，您可能需要根据要使用的具体求积方法进行调整。
return dblquad(f, a, b,a,b)[0]

# 定义区间
a, b = 0, 1

# 定义精确函数
def exact(x,y):
return 1/((x+y+1)**2)

# 定义函数 f
def f(x,y):
return 1/((x+y+1)**2)-(x/(6*(8+y)))

# 定义函数 k
def k(x, y,t,s):
return (x/((8+y)*(1+t+s)))

# 示例用法（替换为您的具体用例）
result_quad = Quad(f, a, b)
print(result_quad)
import numpy as np
from scipy.special import legendre
from scipy.optimize import fsolve

def shift(x):
return (2 * x - a - b) / (b - a)

gamma = 10 ** 8
M = 4
N = M + 1

a, b = 0, 1 # 定义 a 和 b

# 计算勒让德多项式的根
train1 = fsolve(lambda x: legendre(N)(shift(x)), np.linspace(0, 1, N))
train2 = fsolve(lambda y: legendre(N)(shift(y)), np.linspace(0, 1, N))

# 使用 NumPy 的 meshgrid 和 stacking 创建 N x 2 矩阵
X, Y = np.meshgrid(train1, train2)
train = np.stack((X.flatten(), Y.flatten()), axis=1)

print(train)
print(train.shape) # 输出形状以确认
phi = []
L_phi = []
for kindx in range((M + 1) * (M + 1)):
i = kindx // (M + 1) + 1
j = kindx % (M + 1) + 1
#print(i,j)
# 创建 i 次勒让德多项式
p = legendre(i)
# 使用嵌套函数定义 phi[i] 以创建新范围
def make_phi(p=p): # 在嵌套函数的参数中捕获 p
return lambda x: p(shift(x))
phi.append(make_phi())
L_phi.append(lambda x,y: phi[i](x)*phi[j](y) - dblquad(lambda t,s: k(x,y, t,s) 
* phi[i](t)*phi[j](s), a, b,a,b)[0])

train = np.array([train1, train2]).T #假设 train1 和 train2 是列表或 
数组
A = np.zeros((M+1, M+1))
for m in range(M+1):
for n in range(M+1):
A[m, n] = L_phi[m](train[n, 0], train[n, 1])
print(A)
Omega = np.dot(A.T, A) + np.identity(M+1) / gamma
GAMMA = np.array([f(train1[i],train2[i]) for i in range(M+1)]).reshape(M+1, 1)
alpha = resolve(Omega, GAMMA)
import numpy as np
l=[]
import numpy as np
from scipy.linalg import resolve

# ... (其他导入和函数) ...

def u_tilde(x, y):

# 仅对 x 和 y 处 phi 中的前 N ​​个勒让德多项式进行求值
# 这与 A 的维度一致
P_x = np.array([phi[i](x) for i in range(M+1)]).reshape(-1, 1) # 此处更改
P_y = np.array([phi[i](y) for i in range(M+1)]).reshape(-1, 1) # 此处更改

# 使用逐元素乘法和求和计算 u˜(x)
u_tilde_x = alpha.T @ A.T @ (P_x * P_y) # 此处更改

return u_tilde_x[0, 0] # 从结果中提取标量值

# ...（其余代码）...# 从结果中提取标量值

return result
# 示例用法：
x_value = train1
y_value = train2
for k in range(M+1):
u_tilde_at_x = u_tilde(x_value[k],y_value[k])
l.append(u_tilde_at_x)
print(f&quot;u˜({x_value[k]}) = {u_tilde_at_x}&quot;)
#u_tilde_at_x = u_tilde(x_value)
#print(f&quot;u˜({x_value}) = {u_tilde_at_x}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</guid>
      <pubDate>Wed, 27 Nov 2024 08:30:38 GMT</pubDate>
    </item>
    <item>
      <title>如何让康威生命游戏成为一个 AI/ML 项目？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229377/how-to-make-conways-game-of-life-an-ai-ml-project</link>
      <description><![CDATA[所以我对我的 AI 模块进行了评估。我想开发一些类似于康威生命游戏的东西。当我对其进行了一些研究时，我认为默认版本本身并不是 AI/ML 项目。我想创建一个自我玩的模拟游戏。
我考虑过操纵生存的概念。例如：
细胞需要食物才能生存，只有当细胞有 1 块钱时才能吃食物，钱可以通过前往钱块来获得。每次移动到钱块都需要 1 能量。细胞必须进食才能获得能量。能量会随着时间的推移而耗尽。
我已经学习了一份人工智能概念列表，我必须从中将 2-3 个概念应用于项目。这些是：
树搜索
深度优先搜索
广度优先搜索
动态规划
统一成本搜索
学习成本
A* 搜索
松弛
马尔可夫决策过程
策略评估
价值迭代
强化学习
蒙特卡罗方法
游戏，expectimax
Minimax，expectiminimax
评估函数
Alpha-beta 剪枝
时间差异 (TD) 学习
同步游戏
最先进的技术
我对 AI 和 Python 还很陌生。对此了解不多。
有人能告诉我我的项目是否朝着正确的方向发展吗？还是我必须放弃它并寻找其他东西？在我添加上述变量后，我正在考虑的项目会成为 AI/ML 项目吗？]]></description>
      <guid>https://stackoverflow.com/questions/79229377/how-to-make-conways-game-of-life-an-ai-ml-project</guid>
      <pubDate>Wed, 27 Nov 2024 07:58:22 GMT</pubDate>
    </item>
    <item>
      <title>寻找洞察聚类机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</link>
      <description><![CDATA[我正在为高中开展一个涉及聚类（k 均值和 DBSCAN）的机器学习项目
我抓取了一个电子商务网站 (StockX)，并对某些商品（包括设计师品牌等）的零售价值 (x) 和转售价值 (y) 进行聚类。然后对它们进行聚类。
最终的聚类结果非常基础，有 3 个聚类 - 围绕低零售/转售、中等零售/转售和高零售/转售价格。
我想知道你们是否对我可以用数据和我的项目做的更细微的事情有什么建议。如果有更多有趣的发现，我可以尝试挖掘出来。]]></description>
      <guid>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</guid>
      <pubDate>Wed, 27 Nov 2024 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>大型多 GPU ML 训练作业的 GPU 间流量 [关闭]</title>
      <link>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</link>
      <description><![CDATA[对于具有不同并行类型（如数据、张量、管道等）的分布式多 GPU 大型机器学习作业，我正在寻找点对点、全对全、全归约等 GPU 间流量的模式和百分比。是否有关于此的研究/数据？
大多数研究都讨论数据并行，其中全归约类型的流量占分配梯度的大多数。]]></description>
      <guid>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</guid>
      <pubDate>Wed, 27 Nov 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建多尺度 CNN，但遇到此错误：RuntimeError：mat1 和 mat2 形状无法相乘（32x4095 和 4096x4096）</title>
      <link>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</guid>
      <pubDate>Tue, 26 Nov 2024 23:06:22 GMT</pubDate>
    </item>
    <item>
      <title>Keras 神经网络回归模型优先考虑 2 个输出值，如何才能让它更好地概括？[关闭]</title>
      <link>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</link>
      <description><![CDATA[我正在尝试使用其他特征预测附加数据集中的“温度”值。执行代码时，模型对两个值有明显的偏差。我正在使用 plotly 图显示预测的准确性，其中包含真实值和预测值以及表示最佳预测的线。我的预测准确性
因此，如您所见，有两个主要的信息集群，表明我的模型主要选择这两个值作为输出。我不知道为什么会发生这种情况，也不知道我可以做些什么来补救。我将链接我正在使用的 .csv 文件和代码（google Drive / Colab）
此外，当删除预处理步骤时，它会产生类似的效果，但有三个主要集群而不是两个。 https://drive.google.com/drive/folders/1uSHTVAmW-UXhutZa5-Tf1QcB_e9cl_DR?usp=sharing
我尝试过：

删除预处理步骤。
添加特征工程和通过相关性进行数据选择。
排除分类值。
我已经试验了神经网络的大小和密度，增加或减少以查看它是否是欠拟合问题。
我已经引入了最多 100 次试验的自动超参数选择。
我在神经网络中添加了批量和特征规范化。
我已经手动调整了超参数的值，例如时期、激活函数等。
我使用了 KMeans 来降低密度。
我尝试了不同的数据分割。

我预计分布会略有变化，但它总是水平聚集（与预测值一起）
总之，这是我的神经网络还是预处理的问题？]]></description>
      <guid>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</guid>
      <pubDate>Tue, 26 Nov 2024 21:09:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>在使用 dataloader 测试数据集时，我们应该设置 shuffle=true 吗？或者这无所谓？</title>
      <link>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</link>
      <description><![CDATA[我有一个自定义数据集（披萨、寿司和牛排的图片）。
我正在使用 torch DataLoader 来处理它，现在在编写测试数据加载器自定义时，我们应该设置 shuffle=true 还是这无关紧要？？
我还没有看到区别，只是问一般情况。]]></description>
      <guid>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</guid>
      <pubDate>Thu, 21 Nov 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用从 kaggle 加载的数据（实际使用它）</title>
      <link>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</link>
      <description><![CDATA[因此，我最近从此 https://www.kaggle.com/datasets/mostafaabla/garbage-classification 网站导入了数据集。尽管我在 google colab 中的文件中有它（已解压和所有这些东西），但我不知道如何在代码本身中实现它。就像来自 tensorflow 的 Fashion mnist 教程 https://www.tensorflow.org/tutorials/keras/classification?hl 它加载为
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
如何将数据导入/加载到代码单元并通过分成类来处理它（因为在该教程数据集中有多个类，而在我的自定义数据集中有 12 个）
请问如何操作？
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 定义训练和验证目录的路径
train_dir = &#39;garbage-classification/train&#39;
val_dir = &#39;garbage-classification/validation&#39;

# 创建 ImageDataGenerator 进行数据增强
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# 从目录加载图像
train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150), # 根据需要调整图像大小
batch_size=32,
class_mode=&#39;categorical&#39; # 如果有多个类，请使用 &#39;categorical&#39;
)

validation_generator = val_datagen.flow_from_directory(
val_dir,
target_size=(150, 150),
batch_size=32,
class_mode=&#39;categorical&#39;
)

我使用 perplexity 尝试解决，结果得到了这个。显然它没有起作用，所以..]]></description>
      <guid>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</guid>
      <pubDate>Sat, 16 Nov 2024 16:34:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Windows 机器上安装 Rasa</title>
      <link>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</link>
      <description><![CDATA[我尝试在我的 Windows 10 笔记本电脑上使用命令 pip install rasa 安装 Rasa。
我安装了 Python 3.11 版。
但是我收到以下错误：
获取构建 wheel 的要求未成功运行。
│ 退出代码：1
╰─&gt; [20 行输出]
回溯（最近一次调用最后一次）：
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，行 
353，在&lt;module&gt; 中
main()
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，
在 main 中 
json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

我遗漏了什么？之前，我尝试安装 chatterbot 模块，也遇到了类似的错误。
此外，我也尝试使用 pip install chatterbot，但仍然出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</guid>
      <pubDate>Wed, 15 May 2024 10:10:42 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法使用 XGBoostRegressor 获取预测的概率？</title>
      <link>https://stackoverflow.com/questions/55003557/is-there-a-way-to-get-the-probability-of-a-prediction-using-xgboostregressor</link>
      <description><![CDATA[我已经构建了一个 XGBoostRegressor 模型，该模型使用大约 200 个分类特征来预测连续的时间变量。
但我希望获得实际预测和该预测的概率作为输出。有没有办法从 XGBoostRegressor 模型中获取这些信息？
所以我同时想要  和 P(Y|X) 作为输出。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/55003557/is-there-a-way-to-get-the-probability-of-a-prediction-using-xgboostregressor</guid>
      <pubDate>Tue, 05 Mar 2019 13:06:32 GMT</pubDate>
    </item>
    </channel>
</rss>