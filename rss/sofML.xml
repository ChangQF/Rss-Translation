<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 06 Aug 2024 21:14:40 GMT</lastBuildDate>
    <item>
      <title>Coral Ordinal AttributeError：'str' 对象没有属性 'name'</title>
      <link>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</link>
      <description><![CDATA[我正在开展一个使用 Python 和 Tensorflow 中的有序回归/分类的项目。我发现 pip 包 coral-ordinal 实现了有序回归并包含有用的损失函数。然而，当我在 Google Colab 上阅读他们的教程时（https://colab.research.google.com/drive/1AQl4XeqRRhd7l30bmgLVObKt5RFPHttn#scrollTo=fJSm-gbwxTKt）我得到了错误
AttributeError：&#39;str&#39; 对象没有属性 &#39;name&#39;
运行 model.fit() 时。
当我尝试在其他代码中使用它时也会出现此错误。出现错误之前的代码如下
import tensorflow as tf
print(&quot;Tensorflow version&quot;, tf.__version__)

import coral_ordinal as coral
print(&quot;CORAL Ordinal version:&quot;, coral.__version__)

############################
### 设置
############################

# 超参数
random_seed = 1 # 尚未使用
learning_rate = 0.05
batch_size = 128
num_epochs = 2

# 架构
NUM_CLASSES = 10

# 获取并格式化 mnist 数据
(mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data()

# 拆分验证数据集以进行早期停止
from sklearn import model_selection
mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \
model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1)

print(&quot;训练图像的形状：&quot;, mnist_images.shape)

print(&quot;训练标签的形状：&quot;, mnist_labels.shape)

print(&quot;测试图像的形状：&quot;, mnist_images_test.shape)

print(&quot;测试标签的形状：&quot;, mnist_labels_test.shape)

print(&quot;验证图像的形状：&quot;, mnist_images_val.shape)
print(&quot;验证标签的形状：&quot;, mnist_labels_val.shape)

# 也重新调整为 0-1 范围。
dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels, tf.int64)))
dataset = dataset.shuffle(1000).batch(batch_size)

test_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_test, tf.int64)))
#test_dataset = test_dataset.shuffle(1000).batch(batch_size)
# 这里我们不对测试数据集进行打乱。
test_dataset = test_dataset.batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices(
(tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32),
tf.cast(mnist_labels_val, tf.int64)))
val_dataset = val_dataset.shuffle(1000).batch(batch_size)

def create_model(num_classes):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape = (28, 28, )))
model.add(tf.keras.layers.Dense(128, 激活 = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(32,activation = &quot;relu&quot;))
model.add(tf.keras.layers.Dropout(0.1))
# 具有一定数量的类别/等级/标签的有序输出层。
# 未指定激活函数，因此将输出累积对数。
model.add(coral.CoralOrdinal(num_classes))
return model

model = create_model(NUM_CLASSES)

# 请注意，模型生成的输出比类别数量少 1。
model.summary()

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),
loss = coral.OrdinalCrossEntropy(num_classes = NUM​​_CLASSES),
metrics = [coral.MeanAbsoluteErrorLabels()])

# 这在 CPU 上大约需要 5 分钟，在 GPU 上需要 2.5 分钟。
history = model.fit(dataset, epochs = 5, validation_data = val_dataset,
callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)])

所以我想知道是否有任何方法可以解决这个问题，以便我可以实际使用这个包。我会在他们的 github 上提出问题，但最后一次回复是 2 年前，所以我怀疑维护者是否会回复。
或者，如果没有任何现实的选择来完成这项工作，我将不胜感激任何可以帮助 tensorflow 中的有序回归/分类的替代方案。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78840945/coral-ordinal-attributeerror-str-object-has-no-attribute-name</guid>
      <pubDate>Tue, 06 Aug 2024 20:41:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么在计算CLIP损失的过程中，要分别计算图像和文本的损失？</title>
      <link>https://stackoverflow.com/questions/78839487/why-are-the-losses-for-image-and-text-calculated-separately-in-the-clip-loss-cal</link>
      <description><![CDATA[我对 CLIP 的损失计算过程有疑问。
下面是计算 CLIP 损失的伪代码。

在&quot;# scaled pairwise cosine similarities [n, n]&quot;列下的部分，我理解是计算图像和文本嵌入之间的余弦相似度矩阵。然后，使用标签数组，将交叉熵应用于相似度矩阵的对角线以获得损失。
此时，loss_i 和 loss_t 有什么区别？图像和文本的两个损失不应该具有相同的值吗？

我理解在计算交叉熵时，只考虑以蓝色突出显示的值，其余的值乘以零从而消除。如果基于对角线值计算损失，loss_i 和 loss_t 不是会变得完全相同吗？
我不明白为什么要针对每个模态分别计算损失，然后取平均值。
我不确定我误解了计算损失值的哪一部分……]]></description>
      <guid>https://stackoverflow.com/questions/78839487/why-are-the-losses-for-image-and-text-calculated-separately-in-the-clip-loss-cal</guid>
      <pubDate>Tue, 06 Aug 2024 14:03:37 GMT</pubDate>
    </item>
    <item>
      <title>预测期间决策树中的 KeyError</title>
      <link>https://stackoverflow.com/questions/78839421/keyerror-in-decision-tree-during-prediction</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78839421/keyerror-in-decision-tree-during-prediction</guid>
      <pubDate>Tue, 06 Aug 2024 13:50:01 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter Notebook 中的 MetaTrader5</title>
      <link>https://stackoverflow.com/questions/78839326/metatrader5-in-jupyter-notebook</link>
      <description><![CDATA[我安装了 Anacona3，并使用 jupyter 笔记本进行编程。
但我无法在 jupyter 中导入 MetaTrader5 模块。
我也尝试在 Windows 中安装 MetaTrader5。但它告诉我
要求已满足：metatrader5 位于 c:\users\general\appdata\local\programs\python\python312\lib\site-packages (5.0.4200)

这意味着 MetaTrader5 模块之前已安装到我的电脑上。
我该怎么做才能解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78839326/metatrader5-in-jupyter-notebook</guid>
      <pubDate>Tue, 06 Aug 2024 13:33:04 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 和 Opacus 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</link>
      <description><![CDATA[在使用 Jupyter Notebook（可从此处获取）测试来自 TensorFlow 网站的示例代码时，我遇到了错误。您可以在此处找到我关于该错误的 SO 问题。
因此，我决定使用 PyTorch、Opacus 和 PySyft 为相同功能编写等效实现。然而，不幸的是，我又遇到了另一个错误。
下面是实现与 TensorFlow 网站中的示例代码相同功能的代码，但使用 PyTorch 和 Opacus 和 PySyft，以及错误消息。
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

# 定义一个简单的模型
class SimpleCNN(nn.Module):
def __init__(self):
super(SimpleCNN, self).__init__()
self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
self.fc1 = nn.Linear(32*26*26, 10)

def forward(self, x):
x = torch.relu(self.conv1(x))
x = x.view(-1, 32*26*26)
x = self.fc1(x)
return torch.log_softmax(x, dim=1)

# 数据加载器
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(&#39;.&#39;, train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 初始化模型、优化器和损失函数
model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.NLLLoss()

# 初始化 PrivacyEngine
privacy_engine = PrivacyEngine(
model,
batch_size=64,
sample_size=len(train_loader.dataset),
epochs=1,
max_grad_norm=1.0,
)

privacy_engine.attach(optimizer)

# 训练循环
model.train()
for epoch in range(1):
for data, target in train_loader:
optimizer.zero_grad()
output = model(data)
loss = criterion(output, target)
loss.backward()
optimizer.step()

# 打印隐私统计数据
epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)

错误：
-------------------------------------------------------------------------------
TypeError Traceback (最近一次调用最后一次)
Cell In[1]，第 32 行
29 criterion = nn.NLLLoss()
31 # 初始化 PrivacyEngine
---&gt; 32 privacy_engine = PrivacyEngine(
33 model,
34 batch_size=64,
35 sample_size=len(train_loader.dataset),
36 epochs=1,
37 max_grad_norm=1.0,
38 )
40 privacy_engine.attach(optimizer)
42 # 训练循环

TypeError: PrivacyEngine.__init__() 获得了意外的关键字参数“batch_size”
]]></description>
      <guid>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 13:17:08 GMT</pubDate>
    </item>
    <item>
      <title>神经网络，信号处理，窗口大小[关闭]</title>
      <link>https://stackoverflow.com/questions/78838750/neural-network-signal-processing-window-size</link>
      <description><![CDATA[我想找到合适的窗口大小来划分包含信号的 csvm 数据，以训练我的神经网络。您建议使用哪些方法来找到合适的窗口大小？
最主要的傅立叶频率
多窗口查找器
摘要统计子序列]]></description>
      <guid>https://stackoverflow.com/questions/78838750/neural-network-signal-processing-window-size</guid>
      <pubDate>Tue, 06 Aug 2024 11:24:57 GMT</pubDate>
    </item>
    <item>
      <title>如何优化机器学习模型，以最少的误报检测网络钓鱼推文？[关闭]</title>
      <link>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</link>
      <description><![CDATA[我正在研究检测网络钓鱼推文。虽然我的 ML 模型表现相当不错，但我面临的挑战是误报数量相对较多，这降低了系统的整体准确性和可用性。以下是我当前设置的详细信息：

数据集：平衡的网络钓鱼和非网络钓鱼推文数据集。
特征：推文文本、用户元数据、URL 特征。
尝试的算法：随机森林、SVM 和基本 LSTM 模型。
当前性能：准确率高，但由于误报，召回率低。
特征工程：添加了更多特征，如情绪分数和词嵌入。
模型调整：执行网格搜索以优化超参数。
集成方法：结合不同的算法来提高鲁棒性。
]]></description>
      <guid>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</guid>
      <pubDate>Tue, 06 Aug 2024 11:23:31 GMT</pubDate>
    </item>
    <item>
      <title>调用 cublasLtMatmul 时出现 RuntimeError：CUDA 错误：CUBLAS_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</guid>
      <pubDate>Tue, 06 Aug 2024 09:00:40 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 错误 - 需要使用适当的编译器标志进行重建 [关闭]</title>
      <link>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</link>
      <description><![CDATA[2024-08-06 14:18:52.654763: 
I tensorflow/core/platform/cpu_feature_guard.cc:210]。此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。

要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。

每当我尝试运行任何类型的面部识别代码时，我都会收到这种错误。即使它与面部识别无关，而只是常规的张量流，我也会收到此错误。有人能帮忙吗？
from deepface import DeepFace
import os

os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
img1 = &#39;reference.jpg&#39;
img2 = &#39;reference1.jpg&#39;

model_name = &#39;Facenet&#39;

result = DeepFace.verify(
img1_path=img1,
img2_path=img2,
model_name=model_name
)
]]></description>
      <guid>https://stackoverflow.com/questions/78838145/tensorflow-error-rebuild-needed-with-appropriate-compiler-flags</guid>
      <pubDate>Tue, 06 Aug 2024 08:58:22 GMT</pubDate>
    </item>
    <item>
      <title>什么是 tf.data.Dataset 以及为什么我的 Epoch 没有运行？[关闭]</title>
      <link>https://stackoverflow.com/questions/78837808/what-tf-data-dataset-and-why-is-my-epoch-not-running</link>
      <description><![CDATA[history = Model_Enhancer.fit(x=[X_,X_wb,X_gc,X_ce],y=X_gt,batch_size=16,epochs=400,validation_split=0.3,shuffle=True)

我在使用这段代码时遇到了问题。
当我运行这段代码时，出现了以下错误：
RuntimeError Traceback (most recent call last)
&lt;ipython-input-19-46332b46168a&gt; in &lt;cell line: 1&gt;()
----&gt; 1 history = Model_Enhancer.fit(x=[X_,X_wb,X_gc,X_ce],y=X_gt,batch_size=16,epochs=400,validation_split=0.3,shuffle=True)

1 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __iter__(self)
501 return iterator_ops.OwnedIterator(self)
502 else:
--&gt; 503 引发 RuntimeError(“`tf.data.Dataset` 仅支持 Python 样式”
504 “在 Eager 模式或 tf.function 内迭代。”)
505 

RuntimeError：`tf.data.Dataset` 仅支持在 Eager 模式或 tf.function 内迭代 Python 样式。

我期待 Epoch 运行。为什么 Epoch 没有运行。]]></description>
      <guid>https://stackoverflow.com/questions/78837808/what-tf-data-dataset-and-why-is-my-epoch-not-running</guid>
      <pubDate>Tue, 06 Aug 2024 07:42:40 GMT</pubDate>
    </item>
    <item>
      <title>NLTK bleu 分数明显高于 Sacrebleu bleu 分数</title>
      <link>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</link>
      <description><![CDATA[我试图将 bleu-4 分数的结果与 sacrebleu 和 NLTK corpus bleu 包进行比较，但结果之间的差异非常显著。
对于 NLTK corpus bleu，我获得了非常高的 bleu 分数（0.47、0.39、0.33、0.28）
但对于 sacrebleu，我获得了较低的分数（19.57、10.78、7.07、5.15），sacrebleu 分数已经将它们乘以 100，而 NLTK 没有
这是我计算这些分数的实现：
def compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name):
# 确保长度匹配
print(f&quot;Number of references: {len(all_references)}&quot;)
print(f&quot;假设数量：{len(all_hypotheses)}&quot;)

if len(all_references) != len(all_hypotheses):
raise ValueError(&quot;参考文献和假设的数量必须匹配。&quot;)

sacrebleu_scores = corpus_bleu(all_hypotheses, [all_references]).scores

bleu_score1 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(1.0, 0.0, 0.0, 0.0))
bleu_score2 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.5, 0.5))
bleu_score3 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.33, 0.33, 0.33))
bleu_score4 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.25, 0.25, 0.25, 0.25))


这是我生成预测的函数：
def assess_and_save(loader, dataset_type, folder_name):
model.eval()
all_references = []
all_hypotheses = []

sample_file_path = os.path.join(folder_name, &#39;samples.txt&#39;)
with open(sample_file_path, &#39;a&#39;, encoding=&#39;utf-8&#39;) as sample_file:
with torch.no_grad():
for Skeletons, Labels in loader:
Skeletons, Labels = Skeletons.to(device), Labels.to(device)

Outputs = model(skeletons, Labels[:, :-1])
Predictions = torch.argmax(outputs, dim=-1)

for i in range(predictions.size(0)):
Reference = tokenizer.decode(labels[i], skip_special_tokens=True)
Hypothesis = tokenizer.decode(predictions[i], skip_special_tokens=True)

# 调试：打印一些样本
if i &lt; 25：# 仅打印前 5 个样本
sample_text = f&quot;样本 {i+1}:\n参考：{reference}\n假设：{hypothesis}\nNew\n&quot;
print(sample_text)
sample_file.write(sample_text)

all_references.append(reference)
all_hypotheses.append(hypothesis)

# 检查是否有空引用或假设
empty_references = [ref for ref in all_references if not ref.strip()]
empty_hypotheses = [hyp for hyp in all_hypotheses if not hyp.strip()]

print(f&quot;空引用数：{len(empty_references)}&quot;)
print(f&quot;空假设数：{len(empty_hypotheses)}&quot;)

# 过滤掉空假设和相应的引用
non_empty_indices = [i for i, hyp in enumerate(all_hypotheses) if hyp.strip()]
all_references = [all_references[i] for i in non_empty_indices]
all_hypotheses = [all_hypotheses[i] for i in non_empty_indices]

compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name)


有什么建议说我哪里错了吗？
编辑：
问题是因为我没有在计算 NLTK bleu 分数之前拆分参考文献和假设。sacrebleu 的默认行为是先拆分它们。]]></description>
      <guid>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</guid>
      <pubDate>Tue, 06 Aug 2024 07:40:08 GMT</pubDate>
    </item>
    <item>
      <title>何时使用复合损失深度学习（图像分割）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</link>
      <description><![CDATA[我想知道在训练图像分割算法时，复合损失（例如 Dice + Focal Loss、Dice + Cross-Entropy Loss 或 Generalized Dice + Focal Loss）何时优于使用常规 Dice/CE Loss。
在什么情况下它们可以帮助算法更好地收敛？这是一个比较普遍的问题，但是否存在一个粗略的策略来确定某些算法的良好复合损失？]]></description>
      <guid>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</guid>
      <pubDate>Tue, 06 Aug 2024 02:43:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/tensorflow-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>如何根据文件可视化 CNN [关闭]</title>
      <link>https://stackoverflow.com/questions/78836414/how-to-visualize-cnn-based-on-file</link>
      <description><![CDATA[当我尝试查找可视化工具时，我看到的都是不读取文件或不显示节点的工具。我正在寻找类似这样的工具：cnn 可视化
我希望它能反映实际的 CNN 并且只有文件具有的连接。不知道您是否需要为此编写代码，但我更喜欢预制工具。]]></description>
      <guid>https://stackoverflow.com/questions/78836414/how-to-visualize-cnn-based-on-file</guid>
      <pubDate>Mon, 05 Aug 2024 20:57:11 GMT</pubDate>
    </item>
    <item>
      <title>超参数调优与分类算法对比</title>
      <link>https://stackoverflow.com/questions/65516888/hyper-prparameter-tuning-and-classification-algorithm-comparation</link>
      <description><![CDATA[我对分类算法比较有疑问。
我正在做一个关于数据集的超参数调整和分类模型比较的项目。
目标是找出最适合我的数据集的具有最佳超参数的最佳拟合模型。
例如：我有 2 个分类模型（SVM 和随机森林），我的数据集有 1000 行和 10 列（9 列是特征），最后一列是标签。
首先，我将数据集分成 2 个部分（80-10），分别用于训练（800 行）和测试（200 行）。之后，我使用 CV = 10 的网格搜索来调整这两个模型（SVM 和随机森林）在训练集上的超参数。当确定了每个模型的超参数后，我会使用这两个模型的超参数再次测试训练集和测试集上的 Accuracy_score，以找出哪个模型最适合我的数据（条件：训练集上的 Accuracy_score &lt; 测试集上的 Accuracy_score（未过度拟合）并且哪个模型在测试集上的 Accuracy_score 更高，则该模型为最佳模型）。
但是，SVM 显示训练集的 Accuracy_score 为 100，测试集的 Accuracy_score 为 83.56，这意味着调整超参数的 SVM 过度拟合。另一方面，随机森林显示训练集的 Accuracy_score 为 72.36，测试集的 Accuracy_score 为 81.23。很明显，SVM 的测试集准确度得分高于随机森林的测试集准确度得分，但 SVM 过度拟合。
我有一些问题：
_ 当我像上面一样对训练和测试集的准确度得分进行比较而不是使用交叉验证时，我的方法是否正确？（如果使用交叉验证，该怎么做？
_ 很明显，上面的 SVM 过度拟合，但其测试集准确度得分高于随机森林的测试集准确度得分，在这种情况下我能得出 SVM 是最佳模型的结论吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/65516888/hyper-prparameter-tuning-and-classification-algorithm-comparation</guid>
      <pubDate>Thu, 31 Dec 2020 05:11:06 GMT</pubDate>
    </item>
    </channel>
</rss>