<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 02 Oct 2024 09:18:21 GMT</lastBuildDate>
    <item>
      <title>体育比赛结果的机器学习算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79045323/algorithm-for-machine-learning-on-sports-results</link>
      <description><![CDATA[我正在尝试学习 ML 的基础知识（通过使用 ML.NET），但未能让模型真正产生任何可用的东西。目前，我不知道我是否真正理解了什么是“特征”或任何其他东西（标签除外）。
这没有帮助，因为大多数示例都很糟糕，并且不断改变事物的名称（我假设特征/组件/类别等）有时是同一个东西。
多年来，我有大量来自体育赛事的数据，需要通过算法来查找可能不正确的结果。
实际上，我们有年龄/种族类型/距离/性别/路线类型和结果时间。它需要是一个无监督模型（我们没有干净的训练数据）。我们需要在训练后反馈数据，以找到可能错误的值，以便对其进行检查。
此后，该模型将用于检查输入的新结果。
最好的算法是什么，哪种工具可以运行它。此时，我们已经决定 ML.NET 可能无法满足我们的要求。
ML.NET RandomizedPCA，回归算法。将测试值输入生成的模型只会产生奇怪的结果。]]></description>
      <guid>https://stackoverflow.com/questions/79045323/algorithm-for-machine-learning-on-sports-results</guid>
      <pubDate>Wed, 02 Oct 2024 04:08:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么即使我设置了随机状态，我的 RF 中的准确度分数也会发生变化？</title>
      <link>https://stackoverflow.com/questions/79045081/why-do-accuracy-scores-vary-in-my-rf-even-though-i-set-the-random-state</link>
      <description><![CDATA[我正在使用 scikit 学习随机森林，如果我再次运行分析，准确度得分除了最后几位数字外基本保持不变。这是我应该担心的事情吗？还是我可能对随机状态的处理有误？我在下面粘贴了我的操作方法：
rf = RandomForestClassifier(random_state=42)

这是我在代码开头所做的，然后我做了类似的事情：
# 准备用于交叉验证的变量
X = df[included_columns] # 特征
y = df[&#39;strategy_encoded&#39;] # 目标变量

# 识别每个策略的参与者
unique_strategies = df[&#39;strategy&#39;].unique()
strategy_participants = {strategy: df[df[&#39;strategy&#39;] == strategies][&#39;participant_number&#39;].unique() for strategies in unique_strategies}

# 生成用于交叉验证的参与者组合
participant_combinations = list(product(*strategy_participants.values()))

# 初始化结果列表
accuracies = []
training_accuracies = []
confusion_matrices = []
classification_reports = []

# 使用参与者组合进行交叉验证
for combo in contest_combinations:
# 根据参与者组合进行训练-测试拆分
test_indices = df[df[&#39;participant_number&#39;].isin(combo)].index
train_indices = df[~df.index.isin(test_indices)].index

X_train, X_test = X.loc[train_indices], X.loc[test_indices]
y_train, y_test = y.loc[train_indices], y.loc[test_indices]

# 训练模型
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# 训练准确率
y_train_pred = rf.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
training_accuracies.append(train_accuracy)

# 测试准确率
y_pred = rf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
accuracies.append(test_accuracy)

# 混淆矩阵和分类报告
conf_matrix = chaos_matrix(y_test, y_pred)
chaos_matrices.append(conf_matrix)

class_report = classes_report(y_test, y_pred, output_dict=True)
classes_reports.append(class_report)

# 计算各折叠的平均结果
average_test_accuracy = np.mean(accuracies)
average_training_accuracy = np.mean(training_accuracies)
average_conf_matrix = np.mean(confusion_matrices, axis=0)

# 输出结果
print(&quot;平均测试准确率：&quot;, average_test_accuracy)
print(&quot;平均训练准确率：&quot;, average_training_accuracy)
print(&quot;平均混淆矩阵:\n&quot;, average_conf_matrix)
]]></description>
      <guid>https://stackoverflow.com/questions/79045081/why-do-accuracy-scores-vary-in-my-rf-even-though-i-set-the-random-state</guid>
      <pubDate>Wed, 02 Oct 2024 00:49:50 GMT</pubDate>
    </item>
    <item>
      <title>是不是只有我一个人觉得 Python 模块的设置简直就是一场噩梦？</title>
      <link>https://stackoverflow.com/questions/79044839/is-it-just-me-or-are-python-modules-a-nightmare-to-setup</link>
      <description><![CDATA[我最近改用 Python 来探索 ML 和 AI。我使用 Linux Mint 和 Jupyter 笔记本来处理 Python。所以我安装了 Tensorflow、TFlearn 和 six.moves，以及感觉像二十或两个其他模块。我尝试运行以下命令，
from __future__ import absolute_import, division, print_function

import os
import pickle
from six.moves import urllib

import tflearn
from tflearn.data_utils import *

我得到
AttributeError：模块“PIL.Image”没有属性“ANTIALIAS”

当我运行它时。所以我试着研究它，你知道在我问问题之前试着做一个好孩子，似乎枕头模块不再受支持或该功能已弃用。我怎样才能让 tflearn 工作？
所以我尝试了不同的方法，我尝试使用 langchain 在本地运行 huggingface。我安装了模块并运行代码，结果出现有关未安装模块的错误。所以我会安装该模块并再次运行代码，结果却出现有关未安装模块的另一个错误。所以我安装了那个模块，重新运行代码，结果却出现另一个错误。最后我让这些行运行起来
from langchain.llms import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain

在让上述代码最终运行后，我尝试了以下操作。这都是我尝试遵循的教程中的内容。
model_id = &quot;lmsys/fastchat-t5-3b-v1.0&quot;
llm = HuggingFacePipeline.from_model_id(
model_id=model_id,
task=&quot;text2text-generation&quot;,
model_kwargs={&quot;temperature&quot;: 0, &quot;max_length&quot;: 1000},
)

我遇到了大量的属性错误和异常。T 尝试了另一种方式来使用本地 LLM 并安装了 Dalai Llama 下载了 13b 羊驼模型，然后通过端口将其打开到浏览器中。然后我问了一些问题，但没有得到任何回应。所以现在我想知道为什么我还要继续使用 Python。尽管 Python 被宣传为“适合初学者”，但它一直非常令人沮丧。我可以仅使用文本编辑器和 Linux 上的 Bash 以及运行 docker 镜像来在 C++ 中运行 SDL2 程序，因此我并不是一个完全的新手，但到目前为止，我使用 python 的编程经验只是调试环境，仅此而已。
似乎我尝试了一切。我只是希望它能工作。如果模块无法使用更新的代码安装它自己的必备组件，那它有什么用呢？
*编辑
所以我想问题的重点是，我是唯一一个在使用 tflearn 设置 python 环境时遇到问题的人吗？如果我必须进入我安装的代码并更改某些内容才能使其真正工作，那么 Tflearn 实际上就坏了。我不得不找到一个疯狂的解决方法来修复我的 c++ 编译器，但一旦完成，就完成了。我在 c++ 之前尝试过 Python，但对模块地狱感到沮丧。我玩了几年 C++，作为一种爱好，但当我想更深入地了解神经网络和机器学习时，我听说了 tensorflow 和 tflearn，结果又回到了模块地狱。也许我应该忘记 tflearn，尝试 torch，但我担心我会再次找到模块。]]></description>
      <guid>https://stackoverflow.com/questions/79044839/is-it-just-me-or-are-python-modules-a-nightmare-to-setup</guid>
      <pubDate>Tue, 01 Oct 2024 21:49:51 GMT</pubDate>
    </item>
    <item>
      <title>创建 PartitionedDatasets 的 Kedro PartitionedDataset</title>
      <link>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</link>
      <description><![CDATA[我正在做一个 kedro 项目，我想自动标记数千个音频文件，对它们进行转换，然后将它们存储在一个文件夹中，每个子文件夹对应一个标签。我希望该文件夹成为我的 yml 文件的目录条目
我遵循此 Kedro 教程并创建了我自己的自定义数据集，用于在 kedro 目录中保存/加载 .wav 文件。我还能够在 catalog.yml 中创建 PartitionedDataset  目录条目，例如
audio_folder:
type:partitions.PartitionedDataset
dataset:my_kedro_project.datasets.audio_dataset.SoundDataset
path:data/output/audios/
filename_suffix:&quot;.WAV&quot;

用于在 Kedro 目录中保存/加载 .WAV 文件的文件夹。
我需要的下一个抽象级别是能够创建一个与包含文件夹（例如上面的 audio_folder）相对应的目录条目。我不想通过动态创建目录条目来实现这一点，而是通过扩展 PartitionedDataset 类来实现。这是因为我希望文件夹的文件夹成为我的 catalog.yml 的一部分。我的问题是

这可能吗？你们有人尝试过这样的事情吗？
如果可能的话，我的自定义类应该只包含 _load、_save 和 _describe 方法，就像我在自定义 AbstractDataset 时一样？
]]></description>
      <guid>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</guid>
      <pubDate>Tue, 01 Oct 2024 21:22:40 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter Notebook 和 Python 脚本返回不同的 YOLOV8 实例分割掩码</title>
      <link>https://stackoverflow.com/questions/79043979/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</link>
      <description><![CDATA[我一直在运行预先训练的 YOLOV8 模型来进行实例分割。为了进行测试和开发，我一直在使用 Anaconda Jupyter Notebook，在部署之前我将其转换为 Python 脚本。
以下是最小可重现示例的代码：
from ultralytics import YOLO
import torch
import matplotlib.pyplot as plt
import numpy as np
import PIL

print(torch.cuda.is_available())

model = YOLO(&quot;YOLO/yolov8x-seg.pt&quot;)

image = PIL.Image.open(&quot;YOLO/test.png&quot;)

results = model(image, verbose=False)
plt.imshow(np.logical_or.reduce(results[0].masks.data.cpu().numpy()).astype(np.uint8)*255)
plt.show()

这是我使用的测试图像
这是来自 Jupyter Notebook 的（正确）蒙版：

这是来自 Python 脚本的结果：

您可以看到 Python 结果在边界框的边缘有这些奇怪的块。

两个版本的代码完全相同
Python 虚拟环境直接从 Jupyter 环境导出创建（我仔细检查了：Python 版本相同，所有库的版本也相同）
相同的 CUDA 版本 (12.4)
相同的硬件（两个示例都在同一台机器上执行 - GPU 是 RTX 3070 Ti 笔记本电脑）
两个网络使用相同的预训练权重（可从此处下载）

我的第一个猜测是一些库在Jupyter Notebook 和 Python 环境，但我手动检查了它们，发现它们是相同的。
我还尝试手动设置 Python 脚本的浮点精度，以防这是由网络权重中的一些聚合精度错误引起的，但这也不起作用。
我读到 Anaconda 使用 IPython，这可能会导致不同的行为，但我不确定如何处理这些信息。
我也不能简单地减小边界框的大小，因为这些伪影的位置和大小在运动图像中是不可预测的。减小边界框大小会导致部分蒙版偶尔被切断。
为什么会发生这种情况，我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79043979/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</guid>
      <pubDate>Tue, 01 Oct 2024 16:24:34 GMT</pubDate>
    </item>
    <item>
      <title>项目帮助：视频输入的脚步计数器 - 寻找 SOTA 模型和启发式方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79043873/project-help-footsteps-counter-for-video-input-looking-for-sota-models-and-he</link>
      <description><![CDATA[我正在开展一个项目，用于计算输入视频中的脚步数，并一直在尝试使用 YOLOv8 和 MediaPipe 等姿势估计方法。我的目标是涵盖以下测试用例：

只有人的上半身在画面中，但他们正在行走。

只有人的下半身在画面中。

解决方案应该是防遮挡的。


这是我目前用来通过计算左右脚踝之间的距离来计算步数的逻辑：
def distanceCalculate(p1, p2):
&quot;&quot;&quot;p1 和 p2 格式为 (x1, y1) 和 (x2, y2) 元组&quot;&quot;&quot;
dis = ((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2) ** 0.5
return dis

# 计算脚踝之间的距离（粗略估计迈出一步）
if distanceCalculate(leftAnkle, rightAnkle) &gt; 100: # 步数检测阈值
if not stepStart:
stepStart = 1
stepCount += 1

# 附加到输出 JSON
output_data[&quot;footsteps&quot;].append({
&quot;step&quot;: stepCount,
&quot;timestamp&quot;: round(current_time, 2)
})

elif stepStart and distanceCalculate(leftAnkle, rightAnkle) &lt; 50:
stepStart = 0 # 完成一步后重置

但是，这种逻辑并不适用于所有视频。我正在寻找有关最先进 (SOTA) 模型和启发式逻辑的建议，这些建议可以帮助改进步数检测，特别是针对上述场景。
有什么建议或意见吗？]]></description>
      <guid>https://stackoverflow.com/questions/79043873/project-help-footsteps-counter-for-video-input-looking-for-sota-models-and-he</guid>
      <pubDate>Tue, 01 Oct 2024 15:50:15 GMT</pubDate>
    </item>
    <item>
      <title>如何防止大型 sklearn 随机森林模型使 CPU 核心过载？</title>
      <link>https://stackoverflow.com/questions/79043871/how-to-prevent-big-sklearn-random-forest-model-from-overloading-the-cpu-core</link>
      <description><![CDATA[我使用 RandomizedSearchCV 通过 sklearn 训练了一个随机森林模型。
我将模型存储为 joblib 文件，现在我每天都使用它在 docker 容器中的服务器上进行预测。
基本上，该过程如下：
watcher.py:
def watch():

def run_script():
scripts = [&quot;forecaster.py&quot;]
for script in scripts:
subprocess.run([&#39;python&#39;, script], check=True)
time.sleep(63)

while True:
now = datetime.datetime.now(pytz.utc).astimezone(berlin_tz)
if now.hour == 23 and now.minute == 0:
logs.info(f&quot;Running scripts at {now}.&quot;)
run_script()
time.sleep(3) 

if __name__ == &#39;__main__&#39;:
logs.info(&quot;Watcher gestartet&quot;)
watch()

forecaster.py:
data = get_data_from_API(...)
features = [&quot;list&quot;, &quot;of&quot;, &quot;variables&quot;, &quot;contained&quot;, &quot;in&quot;, &quot;data&quot;]

rf = load(model_path.joblib)
rf_pred = rf.predict(data[features])

table = pd.DataFrame({
&#39;UTC&#39;: data[&#39;UTC&#39;],
&#39;Forecast&#39;: rf_pred
})
table.to_csv(&#39;somepath.csv&#39;)

但是，我最近训练了一些更好的模型，这些模型更大（&gt;100.000KB，所以不是那么大想想），现在我的脚本不断被杀死（&lt;Signals.SIGKILL: 9&gt;），我可以在 docker 桌面上看到 CPU 使用率达到 100%（服务器的核心功能比我用来训练模型的笔记本电脑要少）。
我尝试了不同大小的模型。较小的模型可以正常工作，但较大的模型会崩溃。
目前，我坚持使用准确度较低的较小模型，这种方法有效，但我正在寻找适用于任何我想使用的模型的解决方案。
我认为并行处理可能是一种解决方案，但我只找到仅适用于其他库（如 PyTorch）或关于在训练模型时并行化的教程，而不是在应用模型时并行化的教程。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79043871/how-to-prevent-big-sklearn-random-forest-model-from-overloading-the-cpu-core</guid>
      <pubDate>Tue, 01 Oct 2024 15:49:44 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 上的 Flask 应用程序无法通过异步端点调用处理 S3 视频处理</title>
      <link>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</link>
      <description><![CDATA[我正在 AWS SageMaker 中部署一个 Flask 应用程序，该应用程序旨在处理异步调用。该应用程序应从 S3 获取视频文件，使用面部情绪预测模型对其进行处理，然后返回结果。但是，我的设置没有按预期工作。
以下是我的流程中步骤的摘要：
异步 SageMaker 调用：我使用 Python 脚本中的invoke_endpoint_async 方法触发 SageMaker 端点的调用，为视频文件提供 S3 URI。
response = Runtime.invoke_endpoint_async(
EndpointName=endpoint_name,
InputLocation=&quot;&lt;s3 URI&gt;&quot;, 
ContentType=&quot;application/json&quot;
)

Flask 应用程序（通过 ECR 部署）：Flask 应用程序在 /invocations 端点监听 POST 请求，从请求正文中提取 S3 URI，下载视频文件，处理它，然后返回预测。
import os
from flask import Flask, request, jsonify
import source.face_emotion_utils.predict as face_predict
import boto3
import logs

app = Flask(__name__)

# 设置 S3 客户端
s3 = boto3.client(&#39;s3&#39;)

@app.route(&#39;/ping&#39;, methods=[&#39;GET&#39;])
def ping():
return jsonify({&#39;status&#39;: 200})

@app.route(&#39;/invocations&#39;, methods=[&#39;POST&#39;])
def faceInvocations():
try:
# 提取 JSON 负载
payload = request.get_json()
s3_uri = payload.get(&#39;InputLocation&#39;)

if not s3_uri:
return jsonify({&#39;error&#39;: &#39;s3_uri is required&#39;}), 400

# 从 S3 URI 解析存储桶名称和密钥
bucket_name, key = s3_uri.replace(&quot;s3://&quot;, &quot;&quot;).split(&quot;/&quot;, 1)
filename = key.split(&quot;/&quot;)[-1]
file_path = os.path.join(&#39;/app/input_files&#39;, filename)

# 从 S3 下载文件
s3.download_file(bucket_name, key, file_path)

# 处理文件（预测情绪）
prediction = face_predict.predict(file_path, video_mode=True)

# 清理临时文件
if os.path.exists(file_path):
os.remove(file_path)

return jsonify({&#39;prediction&#39;: prediction})
except Exception as e:
logs.error(f&quot;Error: {str(e)}&quot;)
return jsonify({&#39;error&#39;: str(e)}), 500

if __name__ == &#39;__main__&#39;:
app.run(host=&#39;0.0.0.0&#39;, port=8080)

我的期望：

SageMaker 端点应该从异步调用接收 S3 URI。Flask 应用应该下载视频文件、运行情绪预测并返回结果。

问题：
调用失败，我在 Flask 日志中收到错误。似乎 S3 文件没有被正确下载或处理。以下是一些日志：
INFO:root:Entered faceInvocations function
INFO:root:S3 URI: &lt;s3 URI&gt;
ERROR:root:下载文件时出错：调用 HeadObject 操作时发生错误 (404)：未找到


我还可以看到到目前为止的日志 INFO:root:Created directory
我尝试过的方法：

确保 S3 URI 有效且文件存在。
验证附加到 SageMaker 端点的 IAM 角色是否具有 S3 存储桶的 s3:GetObject 权限。
检查 Flask 应用代码是否存在处理 S3 URI 的错误，并能够使用 docker 获取结果
在日志中打印 S3 存储桶名称和密钥，以确保它们被正确解析。

问题：

为什么我的 Flask 应用无法从中下载视频文件S3，即使 S3 URI 看起来正确？
我在 Flask 应用程序中处理 S3 URI 的方式是否存在问题？
传递 S3 URI 时，我是否遗漏了 SageMaker 异步调用的特定内容？
]]></description>
      <guid>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</guid>
      <pubDate>Tue, 01 Oct 2024 15:09:40 GMT</pubDate>
    </item>
    <item>
      <title>Azure Web 应用部署显示 503 错误，但 Flask 应用在本地运行良好</title>
      <link>https://stackoverflow.com/questions/79043708/azure-web-app-deployment-shows-503-error-but-flask-app-works-fine-locally</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79043708/azure-web-app-deployment-shows-503-error-but-flask-app-works-fine-locally</guid>
      <pubDate>Tue, 01 Oct 2024 14:59:18 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>在函数中创建 VLLM 对象时会导致内存错误，即使明确清除 GPU 缓存也是如此，只有共享引用才能使代码不会崩溃</title>
      <link>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</link>
      <description><![CDATA[我在 Python 中使用 VLLM 库时遇到了问题。具体来说，当我在函数内部创建 VLLM 模型对象时，我遇到了内存问题，并且无法有效清除 GPU 内存，即使在删除对象并使用 torch.cuda.empty_cache() 之后也是如此。
当我尝试在函数内部实例化 LLM 对象时会出现问题，但如果我在父进程或全局范围内实例化该对象，则不会发生这种情况。这表明 VLLM 在函数中创建和管理对象时存在问题，从而导致内存保留和 GPU 耗尽。
以下是代码的简化版本：
import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, samples_params, path_2_eval_dataset):
# 在函数中实例化 LLM
llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

# 在此处运行一些 VLLM 推理或评估（简化）
result = llm.generate([path_2_eval_dataset], samples_params)

# 推理后清理
del llm
gc.collect()
torch.cuda.empty_cache()

# 在此之后，GPU 内存不会被清除正确并导致 OOM 错误
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()

但是
llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)

有效。
即使明确删除 LLM 对象并清除缓存后，GPU 内存仍未正确释放，导致在尝试加载或运行同一脚本中的另一个模型时出现内存不足 (OOM) 错误。
我尝试过的方法：

使用 del 删除 LLM 对象。
运行 gc.collect() 以触发 Python 的垃圾集合。
使用 torch.cuda.empty_cache() 清除 CUDA 内存。
确保父进程中没有实例化 VLLM 对象。

当在函数内创建 LLM 对象时，这些似乎都无法解决问题。
问题：

在函数内创建 VLLM 对象时，有人遇到过类似的内存问题吗？
是否有推荐的方法来管理或清除函数中的 VLLM 对象以防止 GPU 内存保留？
在这种情况下，是否存在与标准 Hugging Face 或 PyTorch 模型不同的特定 VLLM 处理技术？
]]></description>
      <guid>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</guid>
      <pubDate>Sat, 07 Sep 2024 00:58:59 GMT</pubDate>
    </item>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到如下错误：
ERROR:lora-to-gguf:Model LlavaLlamaForCausalLM 不受支持

如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>无法从‘typing_extensions’（/usr/local/lib/python3.10/dist-packages/typing_extensions.py）导入名称‘TypeAliasType’</title>
      <link>https://stackoverflow.com/questions/77490008/cannot-import-name-typealiastype-from-typing-extensions-usr-local-lib-pyth</link>
      <description><![CDATA[我使用 elevenlab api 在 google colab 中进行语音克隆。这是我的代码
import elevenlabs

from elevenlabs import set_api_key

set_api_key(&quot;*****************&quot;)

它给了我这个错误：

ImportError：无法从“typing_extensions”导入名称“TypeAliasType”（/usr/local/lib/python3.10/dist-packages/typing_extensions.py

我更改了 typing_extensions 的版本，但它对我来说不起作用。
我更改了 typing_extension 的版本，因为 gpt 建议我更改它的版本。但它仍然不起作用。当我安装新版本的 typing_extention 时。然后它给了我这个错误：

错误：pip 的依赖项解析器目前不考虑已安装的所有软件包。此行为是以下依赖项冲突的根源。
tensorflow-probability 0.22.0 需要 typing-extensions&lt;4.6.0，但您有不兼容的 typing-extensions 4.8.0。

以及安装]]></description>
      <guid>https://stackoverflow.com/questions/77490008/cannot-import-name-typealiastype-from-typing-extensions-usr-local-lib-pyth</guid>
      <pubDate>Wed, 15 Nov 2023 18:08:34 GMT</pubDate>
    </item>
    <item>
      <title>我无法从“typing_extensions”导入名称“TypeAliasType”</title>
      <link>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</link>
      <description><![CDATA[我是 Python 新手，发现了以下这样的错误。非常感谢您的评论。谢谢
我尝试将 Gradio 库导入为 gr
我尝试了几个现有的建议，但结果都是徒劳的。我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</guid>
      <pubDate>Thu, 09 Nov 2023 03:38:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Android 应用程序中使用 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/66057364/how-to-use-ml-models-in-android-application</link>
      <description><![CDATA[我的毕业设计是关于使用数据挖掘技术或 ML 模型从数据集（而非 API）预测 Covid-19 的 Android 应用程序，其中一部分供用户按地区搜索，以了解该地区是否安全或是否挤满了 covid 病例，并提供统计数据以避免前往那里，另一部分是一个聊天机器人，它会记录用户症状并预测这些症状是 Covid-19 还是流感。
问题是我是一名 Web 开发人员，我以前没有使用过 Android Studio，也从未使用过 ML，我不知道如何开始这个项目，有人可以指导我如何开始并给我一些指导吗？这是用于毕业设计目的。
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/66057364/how-to-use-ml-models-in-android-application</guid>
      <pubDate>Fri, 05 Feb 2021 03:55:46 GMT</pubDate>
    </item>
    </channel>
</rss>