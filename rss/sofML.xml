<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Jun 2024 09:16:41 GMT</lastBuildDate>
    <item>
      <title>通过 R 实现支持向量机</title>
      <link>https://stackoverflow.com/questions/78616808/support-vector-machine-implementation-through-r</link>
      <description><![CDATA[我正在尝试使用 e1071 库在 R 中实现 SVM。
svm_train &lt;- svm(factor_new ~ RFS +LI+SDI+LDI+DR+DBT+FCT+FII+DITP+ADCG+ADDG+ROA+ROI+ROS+ROE,data = train01_new, kernel = &quot;linear&quot;, cost = 0.1, scale = TRUE, type = &quot;C-classification&quot;)
predict_svm_train &lt;- predict(svm_train, train01_new)
cm_svm_train &lt;- chaosMatrix(predict_svm_train, train01_new$factor_new, mode = &quot;everything&quot;, positive = &quot;1&quot;)

代码运行正常，没有错误或警告，但训练集上的预测结果确实很奇怪。似乎模型无法检测到默认值（1）。]]></description>
      <guid>https://stackoverflow.com/questions/78616808/support-vector-machine-implementation-through-r</guid>
      <pubDate>Thu, 13 Jun 2024 08:50:14 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法中的 CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/78616686/cuda-out-of-memory-on-a-reinforcement-learning-algorithm</link>
      <description><![CDATA[import os
import gym
from gym import space
import numpy as np
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from tqdm import tqdm
from TTset_main import load_TTset

# 加载数据集
filted_tset_path = &quot;training_path.csv&quot;
filted_vset_path = &quot;validation_path.csv&quot;
TTset_training, TTset_vali, training_data_length = load_TTset(filted_tset_path, filted_vset_path)

# 自定义数据集类
class VideoDataset(Dataset):
def __init__(self, data_list):
self.data_list = data_list
self.length = sum(len(video[0]) for video in data_list) # 总帧数

def __len__(self):
return self.length

def __getitem__(self, idx):
video_idx = 0
frame_idx = idx
# 查找对应的视频和帧
for video, labels in self.data_list:
if frame_idx &lt; len(video):
frame = video[frame_idx]
label = labels[frame_idx]
return frame, label
frame_idx -= len(video)
raise IndexError(&quot;索引超出范围&quot;)

# 创建 DataLoader
batch_size = 1 # 每次一帧
video_dataset = VideoDataset(TTset_training)
data_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)

# CNN 模型
class CNNModel(nn.Module):
def __init__(self, num_classes):
super(CNNModel, self).__init__()
self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
self.fc1 = nn.Linear(64*30*115*115, 512)
self.fc2 = nn.Linear(512, num_classes)

def forward(self, x):
x = F.relu(self.conv1(x))
x = F.max_pool3d(x, 2)
x = F.relu(self.conv2(x))
x = F.max_pool3d(x, 2)
x = x.view(-1, 64*30*115*115)
x = F.relu(self.fc1(x))
x = self.fc2(x)
return x

# 训练循环
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()

# 带进度条的训练
total_epochs = 10
for epoch in range(total_epochs):
training_acu_count = 0 # 重置每个 epoch 的准确率
for i, (x, y) in tqdm(enumerate(data_loader)):
model.train()
x = x.to(device).unsqueeze(0) # 添加批次维度
y = y.to(device)
pred = model(x)
t_loss = loss_function(pred, y.unsqueeze(0)) # 匹配维度以计算损失
t_loss.backward()
optimizer.step()
optimizer.zero_grad()

pred_convert = torch.argmax(pred, 1)
training_acu_count += (pred_convert == y).sum().item()

print(f&quot;Epoch {epoch+1}/{total_epochs}, Training Accuracy: {training_acu_count/len(video_dataset):.4f}&quot;)

# 保存模型
torch.save(model.state_dict(), &quot;video_classification_model.pth&quot;)


我使用上面的代码来训练一个对视频进行分类的 RL 模型。TTset_training 是一个包含输入和目标的数据加载器对象。输入尺寸为 129x15x3x60x230x230，目标尺寸为 129x15。视频帧是尺寸为 3x60x230x230 的张量。
我在 4070Ti 上运行此代码，并收到以下错误：
torch.cuda.OutOfMemoryError：CUDA 内存不足。尝试分配 48.43 GiB。GPU

我尝试了多种调整，更改批处理大小，使用循环单独输入数据，但都没有成功。我在网上搜索解决方案，但所有解决方案都建议更改帧的大小，但这是不可能的，因为我会丢失大量数据。]]></description>
      <guid>https://stackoverflow.com/questions/78616686/cuda-out-of-memory-on-a-reinforcement-learning-algorithm</guid>
      <pubDate>Thu, 13 Jun 2024 08:27:33 GMT</pubDate>
    </item>
    <item>
      <title>超参数和运行模型但卡在 Python .py 文件</title>
      <link>https://stackoverflow.com/questions/78616605/hyperparameter-and-running-model-but-stuck-python-py-files</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78616605/hyperparameter-and-running-model-but-stuck-python-py-files</guid>
      <pubDate>Thu, 13 Jun 2024 08:09:35 GMT</pubDate>
    </item>
    <item>
      <title>在生成模型中，如何使用 DAG 依赖关系分解观察数据的可能性？</title>
      <link>https://stackoverflow.com/questions/78616519/in-a-generative-model-how-to-factorize-the-likelihood-of-observing-the-data-usi</link>
      <description><![CDATA[在机器学习中，考虑由以下有向无环图 (DAG) 表示的生成模型：

其中：

潜在变量为：z0、z1 和 tr
观察变量为 a、xO、x1

我们希望训练模型，以便最大限度地提高从潜在变量中看到观察结果的可能性，即：

如何根据 DAG 的依赖关系分解此表达式？

我尝试了以下方法：

使用

和

但是现在，我陷入困境]]></description>
      <guid>https://stackoverflow.com/questions/78616519/in-a-generative-model-how-to-factorize-the-likelihood-of-observing-the-data-usi</guid>
      <pubDate>Thu, 13 Jun 2024 07:50:03 GMT</pubDate>
    </item>
    <item>
      <title>哪个分类器（决策树、支持向量机、感知器）可能生成决策边界？</title>
      <link>https://stackoverflow.com/questions/78616291/which-classifier-decision-tree-support-vector-machines-perceptron-might-have</link>
      <description><![CDATA[在此处输入图片描述
我们有 3 个决策边界：h1、h2、h3。
我正在为数据挖掘和机器学习考试做准备，我对之前考试的这个问题很不确定。
对于每个决策边界，哪个分类器可以生成分割？
我知道，决策树只能创建 h3，因为它们只生成垂直或水平分割。
我特别不确定，感知器是否可以创建垂直分割。此外，我怀疑，SVM 只能生成 h2，因为它应该始终创建具有最大边距的分割。但我不确定。
我自己的猜测：
h1：感知器
h2：SVM，感知器
h3：决策树]]></description>
      <guid>https://stackoverflow.com/questions/78616291/which-classifier-decision-tree-support-vector-machines-perceptron-might-have</guid>
      <pubDate>Thu, 13 Jun 2024 06:59:07 GMT</pubDate>
    </item>
    <item>
      <title>从 hugging face 空间创建 API 端点</title>
      <link>https://stackoverflow.com/questions/78616285/create-api-endpoint-from-hugging-face-space</link>
      <description><![CDATA[我在 Hugging Face 上创建了一个 Space，使用 Gradio 运行我的自定义机器学习模型。它在 Web 界面中运行完美，但现在我想将此 Space 转换为可以从我的应用程序调用的 API 端点。
有人可以指导我将 Hugging Face Space 转换为 API 端点的过程吗？具体来说，我正在寻找：
从我现有的 Space 设置推理端点的步骤。
如何处理用于访问端点的身份验证和 API 密钥？
是否有任何最佳实践或技巧可用于优化端点的性能。]]></description>
      <guid>https://stackoverflow.com/questions/78616285/create-api-endpoint-from-hugging-face-space</guid>
      <pubDate>Thu, 13 Jun 2024 06:57:15 GMT</pubDate>
    </item>
    <item>
      <title>Torch.unique() 的替代方案不会破坏梯度流吗？</title>
      <link>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</link>
      <description><![CDATA[在 Pytorch 梯度下降算法中，函数
def TShentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len_a #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

使用方法 torch.unique()，该方法会破坏梯度流。每当我将其切换到连续概率计算（例如 torch.softmax()）时，程序就会运行。但是，该公式需要使用离散概率质量分布，而这不适用于 softmax。
我尝试使用 torch.nn. functional.one_hot 和 torch.bincount，两者都给出了相同的错误：
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

这注定会失败吗？我应该尝试以某种方式插入概率函数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</guid>
      <pubDate>Thu, 13 Jun 2024 04:48:39 GMT</pubDate>
    </item>
    <item>
      <title>Keras 库在 Google Colab 中不起作用</title>
      <link>https://stackoverflow.com/questions/78614179/keras-library-not-working-in-google-colab</link>
      <description><![CDATA[正在研究基于图像识别的 ML 模型。
以下代码行显示错误。
“from keras.utils import np_utils, to_categorical”
在 google colab 上工作
显示错误-
“无法从“keras.utils”（/usr/local/lib/python3.10/dist-packages/keras/utils/init.py）导入名称“np_utils”

注意：如果由于缺少包而导致导入失败，您可以
使用 !pip 或 !apt 手动安装依赖项。&quot;
虽然我已经使用了以下内容-
!pip install keras
导入最新版本
导入包并设置 numpy 随机种子
导入随机
将 numpy 导入为 np
从 keras.utils 导入 np_utils，to_categorical
从 keras_preprocessing.image 导入 load_img，img_to_array
从 os 导入listdir
from os.path import isdir, join
加载预洗牌的训练和测试数据集
(x_train, y_train), (x_test, y_test) = sign_language.load_data()
这些是前几行代码，错误出现在第三行。]]></description>
      <guid>https://stackoverflow.com/questions/78614179/keras-library-not-working-in-google-colab</guid>
      <pubDate>Wed, 12 Jun 2024 17:28:53 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统 - 奇异值分解 (SVD) 提供随机结果</title>
      <link>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</link>
      <description><![CDATA[有时，输出的质量仅通过目测来评估。查看下面提供的示例，如果我们使用简单的直觉，很明显预期的用户评分是 2 和 3（请参考空数据单元格）。
示例已简化 - 我使用了更大的数据集，但仍然获得了相同的结果。
表 1

用户 电影 1 电影 2 电影 3

1 - 3 4

2 2 3 4

3 2 3 4

表 2

用户 电影 1 电影 2 电影 3

1 - 3 3

2 3 3 3

3 3 3 3

ALS 模型提供了这些评分，但 SVD 模型却惨遭失败。有人知道这是为什么吗？为什么使用 SVD 进行用户预测结果时，我们会得到 2 和 3 以外的结果？]]></description>
      <guid>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</guid>
      <pubDate>Wed, 12 Jun 2024 10:21:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
替代方法：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
Sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
Sum(i) = (1/T)*sum((y_i&#39;-y_i_target).^2);
end
[minval, minidx] = min(Sum);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 中使用 flutter_tesseract_ocr 和 google_mlkit_text_recognizer 以行格式提取文本？</title>
      <link>https://stackoverflow.com/questions/78591825/how-to-extract-text-in-row-wise-format-using-flutter-tesseract-ocr-and-google-ml</link>
      <description><![CDATA[我正在开发一个 Flutter 项目，需要对图像/PDF 执行文本识别。我想比较使用 flutter_tesseract_ocr 和 google_mlkit_text_recognizer 的结果。我在 Flutter 应用中集成这两个库时遇到了麻烦，需要一些指导。我想按行获取数据。我已附加图像以供识别。
这是 flutter_tesseract_ocr 输出：-
在此处输入图像说明
这是 google mlkit 文本识别器输出：- 在此处输入图像说明
我尝试过的方法
我查看了这两个库的文档，但找不到有关按行格式化输出文本的具体信息。
我也在网上搜索过例子，但没有找到适合我的情况的解决方案。输入图像在此处输入图像描述在此处输入描述
我的期望
我期望以行格式获取识别的文本，这意味着图像中的每一行文本都应单独打印或存储，以便于进一步处理。
实际发生了什么
google_mlkit_text_recognizer：文本被识别，但我不确定如何按行格式化。输出是每个块内逐行的，但我需要确保它在整个图像中是逐行的。
flutter_tesseract_ocr：文本被识别为单个字符串，很难区分行。
具体问题
如何使用 google_mlkit_text_recognizer 按行格式化识别的文本？
有没有办法使用 flutter_tesseract_ocr 按行格式化文本输出？
是否有任何最佳实践或其他步骤可确保以行方式正确识别和格式化文本？
其他信息
Flutter 版本：3.19.1
Dart SDK 版本：3.3.0
google_mlkit_text_recognizer 版本：最新
flutter_tesseract_ocr 版本：最新
平台：Android/iOS]]></description>
      <guid>https://stackoverflow.com/questions/78591825/how-to-extract-text-in-row-wise-format-using-flutter-tesseract-ocr-and-google-ml</guid>
      <pubDate>Fri, 07 Jun 2024 12:06:38 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8n PyTorch 与转换后的 CoreML 模型之间的对象检测性能不一致</title>
      <link>https://stackoverflow.com/questions/77541935/inconsistent-object-detection-performance-between-yolov8n-pytorch-and-converted</link>
      <description><![CDATA[我发现，YOLOv8n 对象检测模型在其原始 PyTorch 格式 (.pt) 和将其转换为 CoreML 格式以在 iOS 设备上部署后之间存在显著的性能差异。原始模型在自定义数据集上训练，成功检测了给定图像中的对象。但是，转换后的 CoreML 模型无法在同一图像中检测到任何对象。
我在其他一些图像中进行了测试。虽然它可以在 iOS 和 Mac 设备中检测到对象，但其性能与原始 .pt 检测不同。
详细信息：
原始模型：YOLOv8n，使用 Ultralytics 的实现在自定义数据集上进行训练。在 CoLap 中使用 A100。
转换工具：
!pip install coremltools

from ultralytics import YOLO
model_path=f&quot;{HOME}/runs/detect/train/weights/best.pt&quot;
model=YOLO(model_path)
model.export(format=&#39;coreml&#39;, nms=True)

问题：

YOLOv8n 中是否存在已知与 CoreML 存在兼容性问题的特定层或操作？

调试原始模型和转换后的模型之间对象检测性能差异的推荐步骤是什么？


对进一步故障排除有任何见解或建议吗？

]]></description>
      <guid>https://stackoverflow.com/questions/77541935/inconsistent-object-detection-performance-between-yolov8n-pytorch-and-converted</guid>
      <pubDate>Fri, 24 Nov 2023 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 的 random.truncated_normal 使用相同的种子返回不同的结果</title>
      <link>https://stackoverflow.com/questions/75110547/tensorflows-random-truncated-normal-returns-different-results-with-the-same-see</link>
      <description><![CDATA[以下几行应该得到相同的结果：
import tensorflow as tf
print(tf.random.truncated_normal(shape=[2],seed=1234))
print(tf.random.truncated_normal(shape=[2],seed=1234))

但我得到的是：
tf.Tensor([-0.12297685 -0.76935077], shape=(2,), dtype=tf.float32)
tf.Tensor([0.37034193 1.3367208 ], shape=(2,), dtype=tf.float32)

为什么？]]></description>
      <guid>https://stackoverflow.com/questions/75110547/tensorflows-random-truncated-normal-returns-different-results-with-the-same-see</guid>
      <pubDate>Fri, 13 Jan 2023 14:29:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将生成的数据转换为 Pandas 数据框</title>
      <link>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</link>
      <description><![CDATA[from sklearn.datasets import make_classification
df = make_classification(n_samples=10000, n_features=9, n_classes=1, random_state = 18,
class_sep=2, n_informative=4)

创建数据后。它是元组，将元组转换为 pandas 数据框后
 df = pd.DataFrame(data, columns=[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;])

所以我得到了 9 个特征（列），但是当我尝试插入 9 个列时，它显示。

ValueError：传递值的形状为 (2, 1)，索引暗示 (2, 9)

基本上我想生成数据并将其转换为 pandas 数据框，但无法获取它。
错误是：]]></description>
      <guid>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</guid>
      <pubDate>Mon, 26 Apr 2021 11:54:24 GMT</pubDate>
    </item>
    </channel>
</rss>