<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 30 Jul 2024 03:20:14 GMT</lastBuildDate>
    <item>
      <title>RNN 建模数据准备</title>
      <link>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</link>
      <description><![CDATA[我正在准备用于 rnn 模型的顺序数据，但我将时间数据放在不同的列中，其中天数格式为 0 表示工作日，1 表示周末。时间是否应采用单一数据格式列以用于模型？
此外，我应该如何准备数据以计算与传感器数据的距离。我添加了数据和距离问题的屏幕截图。
在此处输入图片说明在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</guid>
      <pubDate>Tue, 30 Jul 2024 01:26:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么要将 TorchServe 或 NVIDIA Triton 与 AWS SageMaker 结合使用？</title>
      <link>https://stackoverflow.com/questions/78809430/why-use-torchserve-or-nvidia-triton-with-aws-sagemaker</link>
      <description><![CDATA[我正在研究不同的 ML 和深度学习模型部署策略。我的公司已经在使用 AWS SageMaker，我们正在寻找一种可以容纳深度学习模型集合的部署解决方案。
根据我的研究，SageMaker 似乎已经支持多模型端点、多容器端点等方法。SageMaker 还支持 TorchServe 和 NVIDIA Triton 服务器作为部署解决方案。
我的问题是...如果 SageMaker 已经提供了许多不同的方法，为什么有人会将 TorchServe 或 Triton 与 SageMaker 一起使用？我们用 PyTorch 编写所有内容，因此我们不需要 TensorFlow 支持。我公司的领导确信 NVIDIA Triton（带有 Merlin）是最佳选择，因为它具有强大的 API、清晰的集成文档以及与 NVIDIA GPS 配合使用的能力。我很难找到一个明确的“赢家”在所有这些不同的技术中（我也听说过 Cortex 和其他技术）。
有人能解释一下这些不同解决方案的优缺点吗？特别是，为什么除了使用 SageMaker 之外，还会有人使用 TorchServe 之类的东西？（或者除了 SageMaker 之外还会使用 Triton）。其中一种或另一种是否能更快地执行推理，等等。我已经对这个主题进行了大量研究，包括在这个网站上，但似乎没有什么确凿的数据表明这些选择中有一个“赢家”。]]></description>
      <guid>https://stackoverflow.com/questions/78809430/why-use-torchserve-or-nvidia-triton-with-aws-sagemaker</guid>
      <pubDate>Tue, 30 Jul 2024 00:44:48 GMT</pubDate>
    </item>
    <item>
      <title>TFLM“Interpreter->Invoke()”问题导致硬故障</title>
      <link>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</guid>
      <pubDate>Mon, 29 Jul 2024 20:48:04 GMT</pubDate>
    </item>
    <item>
      <title>训练和验证拆分导致 PyTorch 数据集中某些类别的样本为零</title>
      <link>https://stackoverflow.com/questions/78808028/train-and-validation-splits-result-in-zero-samples-for-some-classes-in-pytorch-d</link>
      <description><![CDATA[我正在使用 PyTorch 进行图像分类。我的数据集是目录格式。我已经设置了数据增强管道和模型。当我将数据集拆分为训练和验证时，我遇到了一个问题，其中某些类在训练或验证拆分中都没有样本。以下是我通过扩展 PyTorch Dataset 类实现的 Dataset 类：
class CustomDataset(Dataset):
def __init__(self, root_dir, transform=None):
self.root_dir = root_dir
self.transform = transform
self.classes = os.listdir(root_dir)
self.image_paths = []
self.labels = []

for label, class_name in enumerate(self.classes):
class_dir = os.path.join(root_dir, class_name)
for img_path in glob.glob(os.path.join(class_dir, &#39;*.png&#39;)) + \
glob.glob(os.path.join(class_dir, &#39;*.jpg&#39;)) + \
glob.glob(os.path.join(class_dir, &#39;*.jpeg&#39;)):
self.image_paths.append(img_path)
self.labels.append(label)

def __len__(self):
return len(self.image_paths)

def __getitem__(self, idx):
img_path = self.image_paths[idx]
image = Image.open(img_path).convert(&#39;RGB&#39;)
label = self.labels[idx]

if self.transform:
image = self.transform(image)
else:
image = transforms.ToTensor()(image) # 如果未提供变换，则将 PIL 图像转换为张量

return image, label

class AugmentedDataset(Dataset):
def __init__(self, base_dataset, transforms_list):
self.base_dataset = base_dataset
self.transforms_list = transforms_list if isinstance(transforms_list, list) else [transforms_list]

def __len__(self):
return len(self.base_dataset) * len(self.transforms_list)

def __getitem__(self, idx):
base_idx = idx // len(self.transforms_list)
transform_idx = idx % len(self.transforms_list)

image, label = self.base_dataset[base_idx]
transform = self.transforms_list[transform_idx]

if transform:
image = transform(image)

return image, label

训练和测试拆分按如下方式完成：
base_dataset = CustomDataset(train_dir, v2.Compose(basic_transformations))

train_size = int(0.8 * len(base_dataset))
val_size = len(base_dataset) - train_size

train_base_dataset, val_dataset = random_split(base_dataset, [train_size, val_size])
train_dataset = AugmentedDataset(train_base_dataset, augmentations)
val_dataset = AugmentedDataset(train_base_dataset, v2.Compose(final_transformation))

trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

执行拆分后，我注意到某些类在训练或验证数据集中没有样本。

为什么 pytorch random_split 会出现这种情况方法？
有哪些最佳实践可以确保训练集和验证集中所有类别的平衡分割？
是否有任何特定技术或库可用于在分割期间保持类别平衡？

这是我的笔记本：EfficientNet with Augmentation]]></description>
      <guid>https://stackoverflow.com/questions/78808028/train-and-validation-splits-result-in-zero-samples-for-some-classes-in-pytorch-d</guid>
      <pubDate>Mon, 29 Jul 2024 15:51:51 GMT</pubDate>
    </item>
    <item>
      <title>Catboost 特征重要性计算</title>
      <link>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</link>
      <description><![CDATA[我仅用 3 棵树拟合了一个简单二分类模型，并想检查特征重要性结果是否与 Catboost 文档 (PredictionValuesChange) 中的公式相似。
训练模型后，我按照CatBoost JSON 模型教程中的步骤操作，并得到了以下树结构：
{
&quot;leaf_values&quot;: [
-0.13915912880676032,
0.1097787155963716
],
&quot;leaf_weights&quot;: [
2143.0251545906067,
2252.974784851074
],
&quot;splits&quot;: [
{
&quot;border&quot;: 3.5,
&quot;float_feature_index&quot;: 13,
&quot;split_index&quot;: 0,
&quot;split_type&quot;: &quot;FloatFeature&quot;
}
]
} 

模型中的每棵树只有深度 = 1，并且只有一棵树（索引 = 1）具有感兴趣的特征。我决定根据上述公式手动计算特征重要性，并将结果与​​ .get_feature_importance 方法进行比较。结果大不相同：

特征重要性：28.2947825
手动计算：68.06248029261762

以下是用于特征重要性计算的代码：
tree_indx = 1
v_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][0]
v_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][1]

c_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][0]
c_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][1]

avr = (v_1*c_1 + v_2*c_2)/(c_1+c_2)

fi = ((v_1 - avr)**2)*c_1 + ((v_2 - avr)**2)*c_2
print(fi)

我犯了错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</guid>
      <pubDate>Mon, 29 Jul 2024 15:29:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 和计算机视觉检测图像中的人是否赤裸上身？[关闭]</title>
      <link>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</link>
      <description><![CDATA[我有一个数据集，其中包含一个人的多张自拍照，每张都是从胸部以上拍摄的，格式为自拍。我的目标是识别每张照片中的人是否赤裸上身。这些照片并不露骨，而是随意的自拍照，照片中的人可能穿着也可能没穿着衬衫。
由于我没有太多数据，我认为从头开始训练神经网络模型不是最好的选择——我的数据集不是那么大。
有什么指导或建议吗？
我曾尝试使用 Python 中的 NSFW-Detector 库（https://pypi.org/project/nsfw-detector/），但它没有太大帮助，因为它专注于检测露骨内容，而不是识别一个人是否赤裸上身。]]></description>
      <guid>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</guid>
      <pubDate>Mon, 29 Jul 2024 11:19:21 GMT</pubDate>
    </item>
    <item>
      <title>将 Onnx 模型与 javafx 集成 [关闭]</title>
      <link>https://stackoverflow.com/questions/78805706/integrating-onnx-model-with-javafx</link>
      <description><![CDATA[我试图将 onnx 模型集成到 javafx 中，我尝试连接从 scikit-learn 保存的 onnx 文件，但没有成功，一直提示找不到文件的错误
我尝试将 javafx 代码与调试器连接以验证文件是否可用，结果显示 onnx 文件存在。但如果我尝试运行应用程序，它会提示找不到 onnx 文件的错误[代码路径图像在这里][显示错误代码的图像]]]></description>
      <guid>https://stackoverflow.com/questions/78805706/integrating-onnx-model-with-javafx</guid>
      <pubDate>Mon, 29 Jul 2024 06:43:34 GMT</pubDate>
    </item>
    <item>
      <title>Python\Python312\Lib\站点包\torch\lib\fbgemm.dll</title>
      <link>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</link>
      <description><![CDATA[在此处输入图片描述
我正尝试从 Hugging Face 导入 GPT-2 Transformer 模型，但当我尝试导入它时，我遇到了错误。即使我尝试只导入 Torch，我也会收到同样的错误。
我尝试重新安装 Torch 并做了所有事情，包括更新 Visual C++ Redistributable 软件包和更新我的驱动程序，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</guid>
      <pubDate>Mon, 29 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降应用</title>
      <link>https://stackoverflow.com/questions/78804107/gradient-descent-application</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78804107/gradient-descent-application</guid>
      <pubDate>Sun, 28 Jul 2024 15:15:45 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。

问题是前 50/60 次推理的性能非常好（比如 20 毫秒），然后它们开始变差，直到时间稳定在每张图像 70/75 毫秒左右。我不明白为什么性能会以这种方式变差。

环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
CPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

GPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型自行呈现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>在搜索系统结果上计算 NDCG 时，处理假阳性和假阴性有哪些不同的方法？</title>
      <link>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</link>
      <description><![CDATA[上下文：
我正在使用 NDCG（归一化折扣累积增益）来评估包含相关性分数的地面实况数据集上的语义搜索系统。我想为此使用 sklearn 的 ndcg_score()。
问题：有哪些处理方法：

假阳性文档：对于给定的查询，那些出现在搜索系统的响应中但不出现在地面实况数据中的文档
假阴性文档：对于给定的查询，那些出现在地面实况数据中但不出现在搜索系统的响应中的文档

一种可能性是插入预测分数 = 0 来表示假阴性并忽略假阳性。但我并不完全确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</guid>
      <pubDate>Fri, 26 Jul 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[当我运行代码时，该代码在堆叠学习器（glmnet 和 rpart）上执行特征选择和超参数调整，我收到以下错误消息：
assert_binary(truth, prob = prob, positive = positive, na_value = na_value) 中出错：
“prob”上的断言失败：包含缺失值（元素 1）。
这发生在 PipeOp classif.avg 的 $train()

但是，当我使用 classif.debug 时，预测中没有 NA。任何建议都将不胜感激。
注意：我简化了要调整的参数数量和要选择的特征数量，以减少执行时间，现在使用 classif.debug 只需 15 秒。
这是我的数据：https://www.dropbox.com/scl/fi/hkjs79i89gjz0j5mjlbj8/Data.csv?rlkey=08yuzet3mjr9gcezkryo93vqm&amp;st=hfv2cbeo&amp;dl=0
这是我的代码：
set.seed(1)
data &lt;- read.csv(&quot;C:/Users/Marine/Downloads/Data.csv&quot;)
data &lt;- data[,c(&quot;x&quot;, &quot;y&quot;, &quot;presence&quot;, &quot;V01&quot;, &quot;V02&quot;)]
## dim(data)
data$presence &lt;- as.factor(data$presence)
##摘要（数据）
任务 &lt;- mlr3spatial::as_task_classif_st（x = 数据，目标 = “存在”，正 = “1”，坐标名称 = c（“x”，“y”），crs = “+proj=longlat +datum=WGS84 +no_defs +type=crs”）
摘要（任务）

learner_glmnet &lt;- mlr3::lrn（“classif.glmnet”，预测类型 = “prob”，s = 0.01）
learner_rpart &lt;- mlr3::lrn（“classif.rpart”，预测类型 = “prob”，cp = to_tune（1e-04，1e-1，对数尺度 = TRUE))
learner_glmnet_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_glmnet, id = &quot;glmnet_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))
learner_rpart_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_rpart, id = &quot;rpart_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))

learner_avg &lt;- mlr3pipelines::LearnerClassifAvg$new(id = &quot;classif.avg&quot;)
learner_avg$predict_type &lt;- &quot;prob&quot;
learner_avg$param_set$values$measure &lt;- &quot;classif.auc&quot;

learner_debug &lt;- lrn(&quot;classif.debug&quot;, predict_type = &quot;prob&quot;)

level_0_graph &lt;- mlr3pipelines::gunion(list(learner_glmnet_cv, learner_rpart_cv)) %&gt;&gt;% mlr3pipelines::po(&quot;featureunion&quot;)
level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_avg
## level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_debug
level_0_and_1_graph_learner &lt;- mlr3::as_learner(level_0_and_1_graph)

tuning &lt;- mlr3tuning::auto_tuner(tuner = mlr3tuning::tnr(&quot;grid_search&quot;), 
learner = level_0_and_1_graph_learner,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

feature_selection &lt;- mlr3fselect::auto_fselector(fselector = mlr3fselect::fs(&quot;sequence&quot;, strategies = &quot;sfs&quot;, min_features = 2),
learner = tuning,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

system.time(stacking &lt;- mlr3::resample(task = task, 
learner = feature_selection, 
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
store_models = TRUE))

测试 &lt;- as.data.table(stacking$prediction())
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[1]])
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[2]])
which(is.na(test))
]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>Python OpenCV - 显示坏点检查测试</title>
      <link>https://stackoverflow.com/questions/75924341/python-opencv-display-bad-pixels-inspection-test</link>
      <description><![CDATA[我想找到显示器中存在的每个坏像素。坏像素可能是颜色不正确的像素，也可能是黑色的像素。显示器的尺寸为 160x320 像素。因此，如果显示器良好，则必须有 160*320 = 51200 像素。如果显示器没有 51200 像素，则为坏像素。另外，我想知道每个坏像素的位置。
一旦拍摄的图像太大，我将共享一个 Google Drive 共享文件夹，其中包含良好显示器和包含坏像素的坏显示器的示例。
显示图像
您能帮我怎么做吗？我想在 python 中使用 opencv 来做这件事。
提前感谢
我尝试通过检测到的物体的轮廓进行评估，并且只有在所有像素都存在的情况下才有效。如果缺少一个像素并且没有检测到轮廓，我可以知道缺少了一个像素，但我无法知道它的位置。
所以我认为最好的解决方案是跟踪网格并评估网格的每个单元。
这是我当前的代码：
import cv2
import numpy as np
import matplotlib.pyplot as plt

def getColor(area):
if area &gt; 70:
return (255, 0, 0)
if area &lt; 14:
返回 (0, 0, 255)
返回 (255, 255, 0)

def test_green_pattern():

mask = cv2.imread(&quot;masks/mask.bmp&quot;, 0)
green = cv2.imread(&quot;images/green-good.bmp&quot;)

green_W = cv2.addWeighted(green, 3, green, 0, 1)

gray = cv2.cvtColor(green_W, cv2.COLOR_BGR2GRAY)

masked = cv2.bitwise_and(gray, gray, mask=mask)
cv2.imwrite(&quot;try/green-masked.bmp&quot;, masked)
# 真实阈值 = 120
ret, thresh = cv2.threshold(masked, 120, 255，cv2.THRESH_BINARY)
cv2.imwrite(&quot;try/green-threshold.bmp&quot;, thresh)

轮廓，层次结构 = cv2.findContours(thresh，cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_NONE)
print(&quot;绿色图案像素：&quot;，len(轮廓))
区域 = []
周长 = []
坏 = 0
prob_double = 0
qtd = 0
对于 i，轮廓在枚举(轮廓)中：
区域 = 区域 = cv2.contourArea(轮廓)
区域.append(区域)
周长 = cv2.arcLength(轮廓，True)
周长.append(周长)
颜色 = getColor(区域)

如果区域 &lt; 14： 
bad += 1
qtd += 1
x, y, width, height = cv2.boundingRect(contour)
roi = green[y:y+height, x:x+width]
cv2.imwrite(&quot;try/temp/&quot;+str(i)+&quot;.bmp&quot;, roi) 
cv2.drawContours(green, contour, -1, color, 1)
cv2.drawContours(green_W, contour, -1, color, 1)
如果 area &gt; 90: 
prob_double += 1
qtd += 2
cv2.drawContours(green, contour, -1, color, 1)
cv2.drawContours(green_W, contour, -1, color, 1)
else:
qtd += 1
#get_statistics(areas, perimeters)
print(&quot;总计:&quot;,qtd)
print(&quot;可能是双像素:&quot;, prob_double)
print(&quot;坏像素:&quot;, bad)
cv2.imwrite(&quot;try/green-contours.bmp&quot;, green)
cv2.imwrite(&quot;try/green_w-contours.bmp&quot;, green_W)

test_green_pattern()
]]></description>
      <guid>https://stackoverflow.com/questions/75924341/python-opencv-display-bad-pixels-inspection-test</guid>
      <pubDate>Mon, 03 Apr 2023 23:06:55 GMT</pubDate>
    </item>
    <item>
      <title>Intel MacBook 上的 VNGeneratePersonSegmentationRequest 速度缓慢</title>
      <link>https://stackoverflow.com/questions/72810091/vngeneratepersonsegmentationrequest-slow-on-intel-macbooks</link>
      <description><![CDATA[我有一个非常简单的功能，它尝试使用 VNGeneratePersonSegmentationRequest 和 VNImageRequestHandler 从网络摄像头视频流中删除背景。
在配备 M1 处理器的 Mac 电脑上，效果很好（60-120 fps 没有任何问题）。在 MacBook Pro Intel i7 2.7GHz 和 Intel Iris Plus Graphics 655 上使用相同的代码/应用程序，性能只有 10 fps，低得多。
这是使用的代码：
let personSegmentationRequest = VNGeneratePersonSegmentationRequest()
personSegmentationRequest.qualityLevel = .balanced
let imageRequestHandler = VNImageRequestHandler(ciImage: ciImage) // 来自摄像头的一帧

尝试？ imageRequestHandler.perform([personSegmentationRequest])
guard let result = personSegmentationRequest.results?.first else {
return nil
}

有什么想法吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/72810091/vngeneratepersonsegmentationrequest-slow-on-intel-macbooks</guid>
      <pubDate>Thu, 30 Jun 2022 03:44:54 GMT</pubDate>
    </item>
    <item>
      <title>iOS 视觉框架 - 无法在 VNDetectHumanBodyPoseRequest 中设置请求</title>
      <link>https://stackoverflow.com/questions/70473725/ios-vision-framework-unable-to-setup-request-in-vndetecthumanbodyposerequest</link>
      <description><![CDATA[我使用 VNDetectHumanBodyPoseRequest 从 xcode 资源中的图像中检测身体（我从图像网站下载），但出现以下错误：

2021-12-24 21:50:19.945976+0800 Guess My Exercise[91308:4258893] [espresso] [Espresso::handle_ex_plan] exception=Espresso exception: &quot;I/O error&quot;: Missing weights path cnn_human_pose.espresso.weights status=-2
无法执行请求：错误域=com.apple.vis 代码=9 &quot;无法在 VNDetectHumanBodyPoseRequest 中设置请求&quot; UserInfo={NSLocalizedDescription=无法在 VNDetectHumanBodyPoseRequest 中设置请求}。

以下是我的代码：
 let image = UIImage(named: &quot;image2&quot;)
guard let cgImage = image?.cgImage else{return}

let requestHandler = VNImageRequestHandler(cgImage: cgImage)

let request = VNDetectHumanBodyPoseRequest(completionHandler: bodyPoseHandler)

do {
// 执行身体姿势检测请求。
try requestHandler.perform([request])
} catch {
print(&quot;无法执行请求：\(error).&quot;)
}

func bodyPoseHandler(request: VNRequest, error: Error?) {
guard let surveillance =
request.results as? [VNHumanBodyPoseObservation] else {
return
}

let poses = Pose.fromObservations(observations)

self.drawPoses(poses, onto: self.simage!)
// 处理每个观察结果以找到已识别的身体姿势点。
}
]]></description>
      <guid>https://stackoverflow.com/questions/70473725/ios-vision-framework-unable-to-setup-request-in-vndetecthumanbodyposerequest</guid>
      <pubDate>Fri, 24 Dec 2021 14:08:04 GMT</pubDate>
    </item>
    </channel>
</rss>