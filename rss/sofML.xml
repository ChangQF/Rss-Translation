<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 18 Dec 2024 21:14:32 GMT</lastBuildDate>
    <item>
      <title>如何以与“https://www.tensorflow.org/tfmodels/vision/object_detection”中类似的方式修改配置文件？</title>
      <link>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</link>
      <description><![CDATA[我是新手。我使用链接中的代码来训练我的自定义数据集，并且成功了。现在想使用此代码，但将模型更改为 EfficientDet D1。这是默认代码中配置文件的处理方式。但它不支持 Efficientdet D1 模型。
exp_config = exp_factory.get_exp_config(&#39;retinanet_resnetfpn_coco&#39;)&quot;

所以我下载了 efficientdet D1 配置文件。我不知道如何引用它。有人可以帮忙吗？我想使用它的默认代码。我不介意手动更改配置文件参数。提前谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</guid>
      <pubDate>Wed, 18 Dec 2024 20:49:15 GMT</pubDate>
    </item>
    <item>
      <title>这种模式有哪些可能的增强？</title>
      <link>https://stackoverflow.com/questions/79292370/what-is-the-possible-enhancement-for-this-mode</link>
      <description><![CDATA[我使用 LSTM 对多标签电影类型进行分类，并使用 Word2Vec 作为特征提取；如图所示，该模型得出的指标为测试损失：0.3067，测试准确率：0.5144。

这个模型可能有哪些改进？ 
我使用下面的代码：
# 标记文本
tokenizer = Tokenizer()
tokenizer.fit_on_texts(merged_df[&#39;clean&#39;])

sequences = tokenizer.texts_to_sequences(merged_df[&#39;clean&#39;])
X = pad_sequences(sequences, maxlen=max_len, padding=&#39;post&#39;, truncating=&#39;post&#39;) # 将序列填充到最大长度

# 词汇表大小
vocab_size = len(tokenizer.word_index) + 1 # 包含 0 作为填充
print(f&quot;词汇表大小： {vocab_size}&quot;)

#vocab_size=37696
embedding_dim=300
max_len=1000

embedding_matrix = np.zeros((vocab_size, embedding_dim))

# 将单词映射到向量
for word, i in tokenizer.word_index.items():
if word in word2vec:
embedding_matrix[i] = word2vec[word]
else:
# 使用随机值初始化不在 Word2Vec 中的单词
embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)

print(f&quot;Embedding Matrix Shape: {embedding_matrix.shape}&quot;)

from keras.layers import Input, Embedding, Bidirectional, LSTM, Attention, BatchNormalization、Dropout、Dense
来自 keras.models 导入模型

# 定义输入层 (shape = (None, max_len))
input_layer = Input(shape=(max_len,))
embedded_input = Embedding(input_dim=vocab_size, 
output_dim=embedding_dim, 
weights=[embedding_matrix], 
input_length=max_len, 
trainable=False)(input_layer)
query = Bidirectional(LSTM(128,activation=&#39;tanh&#39;, return_sequences=True))(embedded_input)
attention_output = Attention()([query, query]) # 自注意力机制
attention_output = BatchNormalization()(attention_output)
attention_output = Dropout(0.5)(attention_output)
lstm_output = Bidirectional(LSTM(64,激活=&#39;tanh&#39;，return_sequences=True))(attention_output)
lstm_output = Dropout(0.5)(lstm_output)
lstm_output2 = Bidirectional(LSTM(32，激活=&#39;tanh&#39;，return_sequences=False))(lstm_output)
lstm_output2 = Dropout(0.5)(lstm_output2)
output_layer = Dense(y.shape[1]，激活=&#39;sigmoid&#39;)(lstm_output2)
model = Model(输入=input_layer，输出=output_layer)
model.compile(优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标=[&#39;accuracy&#39;])

X_train，X_test，y_train，y_test = train_test_split(X，y，test_size=0.2，random_state=42)

early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, waiting=5, restore_best_weights=True,mode=&#39;auto&#39;)
lr_scheduler = Red**strong text**uceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5, waiting=3, verbose=1)

history =model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stop, lr_scheduler])
]]></description>
      <guid>https://stackoverflow.com/questions/79292370/what-is-the-possible-enhancement-for-this-mode</guid>
      <pubDate>Wed, 18 Dec 2024 20:11:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 ssd 和 mobilenetv2 进行对象检测时“目标”和“输出形状”不匹配</title>
      <link>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</guid>
      <pubDate>Wed, 18 Dec 2024 18:50:54 GMT</pubDate>
    </item>
    <item>
      <title>“使用 YOLO 和 EasyOCR 进行车牌识别时遇到的文本识别问题”</title>
      <link>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</guid>
      <pubDate>Wed, 18 Dec 2024 17:26:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 OLS 和梯度下降结果对于线性回归有显著差异？[关闭]</title>
      <link>https://stackoverflow.com/questions/79291892/why-do-my-ols-and-gradient-descent-results-differ-significantly-for-linear-regre</link>
      <description><![CDATA[我正在写一篇论文，比较普通最小二乘法 (OLS) 与传统梯度下降法 (CGD) 在线性回归中的性能。虽然我使用 SciPy 的 OLS 实现符合预期结果（例如，statsmodels），但我的 CGD 实现产生了截然不同的参数估计，即使在标准化数据、使用低学习率并运行许多个时期之后也是如此。
以下是我所做的：
A. 使用 SciPy 的 OLS：
我使用闭式正则方程和伪逆计算系数。我使用的正态方程与往常一样：

结果与 statsmodels 相符：
beta_encoding_scipy = pinv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y

OLS 结果：

截距 -22.507045
学习时间 2.852729
之前的分数 1.018319
睡眠时间 0.480321
练习的样题 0.193910
课外活动_否-11.561869
课外活动_是 -10.945176
dtype: float64

B.梯度下降实现：
我标准化了训练数据并使用以下设置实现了梯度下降：


学习率：0.001
迭代次数：5000
初始值：所有系数均设置为 0

我的实现：
def gradient_descent(features, label, learning_rate, epochs, precision):
# 初始化
X_augmented = np.hstack((np.ones((features.shape[0], 1)), features))
beta = np.zeros(X_augmented.shape[1]) # 初始化系数

for epoch in range(epochs):
predictions = X_augmented @ beta
residuals = predictions - label
gradient = (2 / len(label)) * X_augmented.T @ residuals
beta = beta - learning_rate * gradient
return beta

CGD 结果:

截距 55.142748
学习时长 7.392875
之前的分数 17.722890
睡眠时间 0.819071
练习过的样题 0.531543
课外活动_否 -0.149439
课外活动_是 0.149439
dtype: float64

观察到的差异：
CGD 的结果与 OLS 的结果相差甚远，我不确定原因。我怀疑这可能是由于数据分割过程（应用 StandardScaler 进行训练/测试分割）造成的。
问题：

如果经过适当训练，CGD 不应该产生与 OLS 类似的结果吗？
训练测试分割或缩放过程是否会影响 CGD 性能？
我的 CGD 实现或参数初始化有什么问题吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79291892/why-do-my-ols-and-gradient-descent-results-differ-significantly-for-linear-regre</guid>
      <pubDate>Wed, 18 Dec 2024 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>我如何识别该图像上的物体？</title>
      <link>https://stackoverflow.com/questions/79291781/how-can-i-recognize-objects-on-this-image</link>
      <description><![CDATA[我有这样的图片：
示例
但所有图片都调整为 120x80。
我需要识别图片上的内容，数字（从 1 到 9）还是字母（完整英文字母）。
但是我的模型：
(block1): CNNBlock(
(conv): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block2): CNNBlock(
(conv): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block3): CNNBlock(
(conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block4): CNNBlock(
(conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))
(act1): ReLU()
(conv2): Conv2d(512, 1024，kernel_size=(3, 3)，stride=(1, 1))
(act2): ReLU()
(globalmaxpool): AdaptiveMaxPool2d(output_size=1)
(linear1): 线性(in_features=1024，out_features=512，bias=True)
(act3): LeakyReLU(negative_slope=0.01)
(linear2): 线性(in_features=512，out_features=256，bias=True)
(act4): LeakyReLU(negative_slope=0.01)
(linear3): 线性(in_features=256，out_features=128，bias=True)
(act5): LeakyReLU(negative_slope=0.01)
(linear4): 线性(in_features=128， out_features=64, bias=True)
(act6): LeakyReLU(negative_slope=0.01)
(linear5): Linear(in_features=64, out_features=35, bias=True)
(act7): Softmax(dim=None)
)

未学习。它只是堆叠在 ~3.6 损失（CrossEntropy 损失，35 个类）上。
然后，我尝试查看每层之后的图像，它们都相同。我的对象无法进入下一层。我尝试增加 Conv2d 的内核大小，减少过滤器数量，但不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79291781/how-can-i-recognize-objects-on-this-image</guid>
      <pubDate>Wed, 18 Dec 2024 16:20:26 GMT</pubDate>
    </item>
    <item>
      <title>训练期间更高的验证准确率</title>
      <link>https://stackoverflow.com/questions/79291675/greater-validation-accuracy-during-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79291675/greater-validation-accuracy-during-training</guid>
      <pubDate>Wed, 18 Dec 2024 15:37:11 GMT</pubDate>
    </item>
    <item>
      <title>Yolov5 模型捕获除预期或期望对象之外的其他对象</title>
      <link>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</link>
      <description><![CDATA[我有一个用于数据集的 YOLO V5 m 模型，用于检测特定产品，但在对不同情况和场景以及各种光照条件下的产品图片数据集进行训练后，在 50 个 epoch 和 Yolo v5 中等架构以及批处理大小为 16 的情况下，情况没有重复。
我注意到，该模型依赖于颜色特征，这意味着检测到的对象具有黄色及其阴影。我使用了 yolo github 克隆附带的 train.py。我应该怎么做才能解决这个问题？
用于训练的数据集（大约 450 张图片）在谷歌驱动器中。
需要检测的产品：


检测到的其他产品：
]]></description>
      <guid>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</guid>
      <pubDate>Wed, 18 Dec 2024 14:01:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 compare_models 和 plot_model 的性能结果对于具有 10 倍 cv 的设置不同</title>
      <link>https://stackoverflow.com/questions/79291336/why-the-performance-results-of-compare-models-and-plot-model-for-a-setup-with-10</link>
      <description><![CDATA[在 10 倍 cv 会话中，setup 没有额外的“test_data”，因此数据被拆分为每倍的训练数据和测试数据。
compare_models 的结果是否来自 10 倍训练 + 测试数据的平均性能？
plot_model 的结果来自手册中所示的“hold-out”数据集的平均性能。这里的“hold-out”数据集是每倍的测试数据，而 plot_model 的结果则是 10 倍测试数据的平均性能？
因为 compare_models 和 plot_model 的性能结果不同。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79291336/why-the-performance-results-of-compare-models-and-plot-model-for-a-setup-with-10</guid>
      <pubDate>Wed, 18 Dec 2024 13:51:33 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器的修改</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（与往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。据我所知，没有函数允许我指定这一点（如果有，请告诉我），所以我正在尝试更改分类器函数的源代码。
我选择使用 R 中的 randomForest 包。我创建了一个本地版本（通过从 cran 下载 .tar.gz）并更改了 c 脚本中的某些内容（特别是“findbestsplit”函数中的以下部分：
for (i = 0, i &lt; mtry; ++i) {
/* 样本 mtry 变量 w/o 替换。 */
j = (int) (unif_rand() * (last + 1));
mvar = mIndex[j];
swapInt(mIndex[j], mIndex[last]);
la​​st--;

if (i == mtry - 1) { /* 在最后一次迭代中更改某些内容 */
mvar = additionalVarIndex; 
/* 专门设置mvars 最后一个值到所需特征 */
}

此处 additionalVarIndex 是我通过 C 脚本中的函数传递给 findbestsplit 函数（似乎有效）的索引。我的 C 知识非常有限，所以我认为这还不够，但我也无法测试它，因为我的更改似乎不会影响 R 应用程序。当我加载调整后的包（通过以下方式将其转回 .tar.gz）时
&amp; &quot;C:\Program Files\R\R-4.4.1\bin\x64\R.exe&quot; CMD build .

通过
install.packages(&quot;C:/Users/niklas.jacobs/AppData/Local/R/win- 
library/randomForest/randomForest_4.7-1.2.tar.gz&quot;, repos = NULL, type = &quot;source&quot;)

randomForest R 函数的行为没有发生任何变化（即使我删除了 C 代码的随机部分，我这样做只是为了了解我是否可以更改内容）。我仍然确信我的更改会产生一些影响，因为如果我删除大部分 C 代码，该函数将停止工作。我目前的我怀疑 C 脚本的 Fortran 调用以某种方式绕过了我的更改（据我所知，使用“findbestsplit”的“buildtree”函数不是在 C 脚本中定义的，而是在 Fortran 中定义的），但我不知道如何访问 Fortran。
有人对我接下来的行动有什么建议吗，或者您有更好的想法来实现我的目标吗？]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>ADTK 模型消耗大量 CPU</title>
      <link>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu</link>
      <description><![CDATA[我们从去年开始在 Python 应用程序中使用 ADTK 模型来检测异常。
自上周以来，我们观察到 CPU 利用率大幅上升，原因已确定为 Python 进程。
经过故障排除，我们发现 ADTK 模型是导致该问题的原因。我们正在使用其他 ML 模型，如 IForest、ECOD、CBLOF 等。但只有 ADTK 负责此峰值。
请查找当前模型代码以供参考：
data=df.copy() 
minutes = 300

df[&#39;predict_dt&#39;]=pd.to_datetime(df[&#39;predict_dt&#39;])
df = df.set_index([&#39;predict_dt&#39;]).sort_index() 

seasonal_vol = SeasonalAD(c=1.6,side=&#39;negative&#39;,trend=True) 
seasonal_vol.fit(df[&#39;predict_count&#39;])
df[&#39;anomalies&#39;]=seasonal_vol.predict(df[&#39;predict_count&#39;]) 

df2 = pd.DataFrame(columns=[&#39;predict_dt&#39;, &#39;hour&#39;, &#39;min&#39;,&#39;predict_count&#39;,&#39;label&#39;]) 
final = datetime.datetime.now(pytz.timezone(&#39;America/Los_Angeles&#39;)).replace(tzinfo=None)-timedelta(minutes=minutes)

for i in range(len(df)): 
if(data[&#39;predict_dt&#39;][i] &gt;= final):
if str(df[&#39;anomalies&#39;][i]).lower() == &quot;true&quot;:
#调用 SP 进行进一步操作

我们不确定哪个部分导致了问题。]]></description>
      <guid>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu</guid>
      <pubDate>Wed, 18 Dec 2024 04:50:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LM 中使用标记化？</title>
      <link>https://stackoverflow.com/questions/79289981/how-to-use-tokenization-in-lm</link>
      <description><![CDATA[我一直在训练这个 LM，使用以下超参数：
blocksiz = 128
batchsiz = 32
nemb = 256
nhead = 4
nlayers = 4
evalIters = 100
lr = 3e-4
epochs = 30

我的数据集是 93kb..，
我使用 GPT-2 作为 Tokenizer..
但是，当我使用 GPT-2 Tokenizer 时，训练时间太长了，
但是当我将模型训练为二元组时。它不需要那么多时间。
因此，使用具有 50257 个 Vocabsiz 的 GPT2 作为 tokenizer，会影响模型训练吗？
是否需要更多 GPU 资源来训练？
因此对于小型数据集。我应该使用什么作为标记器...？
我减少了超参数，上面我使用的参数是减少的结果
但模型训练时间太长了]]></description>
      <guid>https://stackoverflow.com/questions/79289981/how-to-use-tokenization-in-lm</guid>
      <pubDate>Wed, 18 Dec 2024 04:30:58 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>深度学习 Nan 损失的原因</title>
      <link>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</link>
      <description><![CDATA[什么会导致卷积神经网络发散？
具体信息：
我正在使用 Tensorflow 的 iris_training 模型和一些我自己的数据，但一直出现

错误：tensorflow：模型发散，损失 = NaN。
回溯...
tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError：训练期间出现 NaN 损失。

回溯源自以下行：
 tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_​​units=[300, 300, 300],
#optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001), 
n_classes=11,
model_dir=&quot;/tmp/iris_model&quot;)

我尝试调整优化器，使用零作为学习率，并且不使用优化器。]]></description>
      <guid>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</guid>
      <pubDate>Fri, 14 Oct 2016 19:07:18 GMT</pubDate>
    </item>
    </channel>
</rss>