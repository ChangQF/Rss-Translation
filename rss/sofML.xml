<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 17 Aug 2024 06:19:59 GMT</lastBuildDate>
    <item>
      <title>如何加速随机森林回归和SVR的训练？</title>
      <link>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</link>
      <description><![CDATA[我目前正在尝试使用以下数据集创建一个回归模型来预测比特币的收盘价。
数据集：https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv
它有超过 60 万条记录，包含 15 个特征（其中一些是我创建的）。
我曾多次尝试在 google colab 和我的笔记本电脑上对其进行训练。我甚至把它放了一夜，但它花了太长时间。
有什么方法可以加快速度吗？
笔记本电脑规格：
CPU：Ryzen 7 5800H
GPU：RTX 3050
RAM：16 GB
这是训练代码。
models = {
&#39;Linear Regression&#39;：{
&#39;model&#39;：LinearRegression()，
&#39;params&#39;：{}
},
&#39;Ridge Regression&#39;：{
&#39;model&#39;：Riddom_state=42，
&#39;params&#39;：{&#39;alpha&#39;：[0.01, 0.1, 1, 5, 10, 50, 100]}
},
&#39;Lasso Regression&#39;：{
&#39;model&#39;： Lasso(random_state=42),
&#39;params&#39;: {&#39;alpha&#39;: [0.001, 0.01, 0.1, 1, 10]}
},
&#39;决策树&#39;: {
&#39;模型&#39;: DecisionTreeRegressor(random_state=42),
&#39;params&#39;: {&#39;max_depth&#39;: [None, 5, 10, 20], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;随机森林&#39;: {
&#39;模型&#39;: RandomForestRegressor(random_state=42),
&#39;params&#39;: {&#39;n_estimators&#39;: [50, 100, 200], &#39;max_depth&#39;: [None, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;支持向量回归&#39;: {
&#39;model&#39;: SVR(),
&#39;params&#39;: {&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: [0.1, 1, 10], &#39;epsilon&#39;: [0.01, 0.1, 1]}
}
}

results = {}

for model_name, model_data in models.items():
print(f&quot;Tuning {model_name}&quot;)
grid_search = GridSearchCV(model_data[&#39;model&#39;], model_data[&#39;params&#39;], cv=5,scoring=&#39;neg_mean_squared_error&#39;, verbose=1)
grid_search.fit(X_train, y_train)

# 获取最佳模型
best_model = grid_search.best_estimator_

# 预测
y_pred = best_model.predict(X_test)

# 性能指标
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

results[model_name] = {
&#39;MAE&#39;: mae,
&#39;MSE&#39;: mse,
&#39;RMSE&#39;: rmse,
&#39;R2&#39;: r2,
&#39;Best Model&#39;: best_model,
&#39;Best Params&#39;: grid_search.best_params_
}


感谢您的时间。]]></description>
      <guid>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</guid>
      <pubDate>Sat, 17 Aug 2024 05:30:15 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型上实际数据和预测数据之间的转换</title>
      <link>https://stackoverflow.com/questions/78881405/shift-between-real-and-predicted-data-on-lstm-model</link>
      <description><![CDATA[我不明白为什么我的预测数据（股票）仍然有差距，或者说在之前的真实数据之后发生了变化，而不是预测数据。这是图片：
X 横坐标：时间（以小时为单位）
Y 横坐标：价格（以美元为单位）
红色曲线：实际价格，
蓝色曲线：预测价格

需要知道的是，用于训练和测试的数据都是标准化的。
当然，我随机混合了我的数据。
每行代表一个系列（32 个报价）

尽管在上图中，价格并未标准化。
所以你可以看到，一开始价格预测得很好，但是随着预测的深入，有时候会出错，就好像它没有考虑到真实数据，修正后的数据。
我将提供训练文件：链接到我的 google Drive 文件]]></description>
      <guid>https://stackoverflow.com/questions/78881405/shift-between-real-and-predicted-data-on-lstm-model</guid>
      <pubDate>Sat, 17 Aug 2024 04:30:50 GMT</pubDate>
    </item>
    <item>
      <title>参数逻辑模型的 AUC 如何能比 BART 模型更好？</title>
      <link>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</link>
      <description><![CDATA[我正在研究一个由大约 20k 个观察值和 620 个回归量组成的表格数据集，其中 25 个回归量是密集的，其余的则非常稀疏。大约 5-10 个回归量是连续变量，而其余的则是二分变量。同样，我的目标变量也是二分变量。我将通过标准 maxLH 方法估计的标准参数 logit 模型与带有 logit 链接器的简单贝叶斯加性回归树 (BART) 模型（包 BART - 函数 lbart）的结果进行了比较
从 Logit 回归中，我发现大多数回归量都不显著。即使是那些理论预测显著的回归量。这是内生性问题和可能的共线性（如果我没记错的话）的预期结果，这困扰着我的观察。有趣的是，参数逻辑回归的 AUC 约为 0.7，这对于社会科学估计问题来说相对较高。
另一方面，BART 模型在理论认为重要的变量的平均治疗效果上提供了预期的强可信区间。然而，AUC 低于参数逻辑回归！大约 0.68。
我想知道，我该如何调和这种相互矛盾的结果？具有明显病理规范的 Logit 模型的预测能力远远优于 BART 模型，后者在重要变量的可信区间上显示出相当令人信服的结果。
谢谢大家。
我运行了具有稳健聚类标准误差的参数逻辑回归和标准 BART 模型。结果与预期相冲突]]></description>
      <guid>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</guid>
      <pubDate>Sat, 17 Aug 2024 04:09:13 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 和去歪斜技术正确旋转图像的问题</title>
      <link>https://stackoverflow.com/questions/78881256/issues-with-rotating-image-correctly-using-opencv-and-deskew</link>
      <description><![CDATA[我正在处理一个图像处理任务，需要校正图像的倾斜，然后根据需要执行额外的旋转。我使用 OpenCV 进行图像处理和去倾斜库来确定倾斜角度。但是，我在旋转逻辑和图像处理结果方面遇到了问题。
这是我使用的代码片段：
import cv2
import numpy as np
import math
from typing import Union, Tuple

def rotate(image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]) -&gt; np.ndarray:
# 获取图像的尺寸
old_height, old_width = image.shape[:2]

# 将角度从度转换为弧度
angle_radian = math.radians(angle)

# 计算旋转图像的新宽度和高度
new_width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)
new_height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)

# 计算原始图像的中心
image_center = tuple(np.array([old_width, old_height]) / 2)

# 获取给定角度的旋转矩阵
rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)

# 调整旋转矩阵以考虑平移
rot_mat[1, 2] += (new_height - old_height) / 2
rot_mat[0, 2] += (new_width - old_width) / 2

# 使用指定的背景颜色执行旋转
rotated_image = cv2.warpAffine(image, rot_mat, (int(round(new_width)), int(round(new_height))), borderValue=background)

return rotated_image

dir_img = os.path.join(&#39;./cropped_for_ocr&#39;, image)
photo = cv2.imread(os.path.join(dir_img))
grayscale = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)
angle = determine_skew(grayscale)
try:
rotated = rotate(photo, angle, (0, 0, 0))
except:
rotated = photo
cv2.imwrite(os.path.join(components_path , image), rotated)

我面临的问题：
应用旋转后，生成的图像有时无法按预期对齐。
我到目前为止尝试过的方法：
最初，我使用 Hough 线来检测倾斜角度，但它只对某些照片有效，所以我改用倾斜校正库。
预期输出：旋转后的图像应该正确对齐。
实际输出：在某些情况下，旋转后的图像未正确对齐
示例：
输入
输出
对于可能导致这些问题的原因，或者我如何改进旋转逻辑以确保图像正确对齐，您有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78881256/issues-with-rotating-image-correctly-using-opencv-and-deskew</guid>
      <pubDate>Sat, 17 Aug 2024 02:21:20 GMT</pubDate>
    </item>
    <item>
      <title>如何创建一个可以回答与论文分析和论文评分相关的问题的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78881222/how-do-i-create-a-model-that-can-answer-questions-related-to-essay-analysis-and</link>
      <description><![CDATA[我正在为一个项目制作一个 ReactJS 应用程序，用户可以在其中输入他们的文章/作品，当他们单击按钮时，它会显示一堆 AI 分析。此分析包括拼写检查、估计的教师分数和一个聊天机器人区域，用户可以在其中就他们的论文提问并获得反馈。我使用 Flask python 服务器和一些基于我找到的文章数据集的简单机器学习制作了拼写检查和教师分数区域。
我是 NLP 的初学者，我正在使用这个项目来学习它，所以我不知道我在做什么？哈哈，我只是在寻找一种资源，可以帮助我理解如何实现我正在做的事情，而它与前 10 个不一样。
我尝试使用我的分数来生成输入，例如（如果分数 &lt; 70，则建议更好的语法），但它似乎太静态了。我也尝试使用不同的可读性分数测量方法，例如 flesh-Kincaid，但它给出的响应并不像我希望的那样个性化。我希望它能够根据用户的文章唯一地生成。
我也尝试使用 BERT（我认为是一种语言模型），但我对微调它所需的数据感到困惑。我在网上看到的所有示例都太具体了，与我的问题无关，所以我不知道如何开始解决问题。我应该坚持使用静态文本生成还是尝试使用 BERT？我担心我没有足够的数据来正确训练它。它甚至对聊天机器人有帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/78881222/how-do-i-create-a-model-that-can-answer-questions-related-to-essay-analysis-and</guid>
      <pubDate>Sat, 17 Aug 2024 01:36:26 GMT</pubDate>
    </item>
    <item>
      <title>MatplotLib 未显示图像</title>
      <link>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</link>
      <description><![CDATA[MatplotLib 不显示图像的原因是什么？
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(10, 10))
plt.imshow(image_rgb)
plt.show()

它不显示任何图像。
更新：
以下是更新后的代码：
import matplotlib.pyplot as plt

image_path = &#39;/Users/johndoe/Desktop/machine-learning/traffic-light.png&#39;
image = cv2.imread(image_path)

plt.imshow(image)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</guid>
      <pubDate>Sat, 17 Aug 2024 01:24:40 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试训练 TensorFlow 模型时，出现了“ValueError”，我不明白为什么</title>
      <link>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model-and-i-dont-underst</link>
      <description><![CDATA[这是我在第一个 epoch 调用 model.fit 时遇到的错误：

&quot;发生异常：ValueError
Layer &quot; functional&quot; 需要 2 个输入，但它收到了 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 177 行，位于 train_encoder_decoder_model
model.fit(x = X_train,
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 217 行，位于 
trained_model = train_encoder_decoder_model(encoder_decoder_model, X_train, y_train, X_test, y_test)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：层“ functional”需要 2 个输入，但收到 1 个输入张量。收到的输入：[&lt;tf.Tensor&#39;data：0&#39;shape=(None，128)dtype=float32&gt;]&quot;

这是我的模型：
def define_encoder_decoder_model(num_features):
# 定义编码器
encoder_inputs = 输入（shape=(None，num_features))
encoder_hidden1 = Dense(100, 激活=&#39;relu&#39;)(encoder_inputs)
coder_hidden2 = Dense(50, 激活=&#39;relu&#39;)(encoder_hidden1)
coder_lstm = LSTM(25, return_state=True)
coder_outputs, state_h, state_c =coder_lstm(encoder_hidden2)
coder_states = [state_h, state_c]

# 定义解码器
decoder_inputs = 输入(shape=(None, 25))
decoder_hidden1 = Dense(50, 激活=&#39;relu&#39;)(decoder_inputs)
decoder_hidden2 = Dense(100, 激活=&#39;relu&#39;)(decoder_hidden1)
decoder_lstm = LSTM(num_features, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decrypt_lstm(decoder_hidden2, initial_state=encoder_states)
decoder_dense = Dense(num_features,activation=&#39;softmax&#39;)
decoder_outputs =coder_dense(decoder_outputs)

# 定义将encoder_inputs和decoder_inputs转换为decoder_outputs的模型
model = Model([encoder_inputs,decoder_inputs],decoder_outputs)

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,&#39;precision&#39;,&#39;recall&#39;,&#39;f1_score&#39;])

# 模型摘要
model.summary()

在此处返回modeltype

这是我调用model.fit的方式：
def train_encoder_decoder_model(model,X_train, y_train, X_test, y_test):
&quot;&quot;&quot;
使用提供的数据训练编码器-解码器模型。

参数：
model：要训练的编码器-解码器模型。
X_train：输入训练数据。
y_train：目标训练数据。
X_test：输入测试数据。
y_test：目标测试数据。

返回：
训练好的编码器-解码器模型。
&quot;&quot;&quot;
print(&quot;shapes: X_train:&quot;, np.shape(X_train),&quot; y_train: &quot;, np.shape(y_train),&quot; X_test: &quot;, np.shape(X_test),&quot; y_test: &quot;, np.shape(y_test))
# 训练模型
# 将每个时期的训练日志附加到文件中
file_logger = FileLogger(&#39;training.log&#39;)
y_train_T = tf.convert_to_tensor(np.array([y_train]).T)
y_test_T = tf.convert_to_tensor(np.array([y_test]).T)
#x_train = tf.convert_to_tensor(X_train)
y_train = X_train
y_test = X_test
model.fit(x = X_train,
y= y_train,
batch_size=100,
epochs=100,
verbose=2,
validation_data=(X_test, y_test),
callbacks=[file_logger])
返回模型

这是一个编码器-解码器模型，因此 X_train 数据集等于 y_train，X_test、y_test 也一样。在第一种情况下，训练集的形状为 (799,128)，测试数据集为 (299,128)。特征表示为“float64”值。
我在 Visual Studio Code 下运行代码。我将数据预处理为标准化和缩放的数据集，然后将其分为训练数据集和测试数据集，构建我的编码器-解码器模型（参见上面的方法），然后尝试训练模型。我得到的是 model.fit “Epoch 1/100” 的输出和上面显示的错误消息。
我不明白这个错误，也不知道需要改正什么。
如有任何建议，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model-and-i-dont-underst</guid>
      <pubDate>Sat, 17 Aug 2024 01:20:22 GMT</pubDate>
    </item>
    <item>
      <title>随机森林模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</link>
      <description><![CDATA[我正在尝试使用 Python 创建一些模型来预测城市与公司的合作，但我一直在数据解析部分遇到问题，我尝试制作一个随机森林模型，但在读取列时遇到了问题，即使我清理了数据并将列名更改为每个文件中的完全相同，我该如何解决这个问题？
这些是我正在使用的数据集：
text
import pandas as pd
import matplotlib.pyplot as plt

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
cities_responses_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Cities/Cities Responses/2020_Full_Cities_Dataset.csv&quot;)

corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)
corp_water_security_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Water Security/2020_Corporates_Disclosing_to_CDP_Water_Security.csv&quot;)

# 合并数据集
merged_2020 = pd.merge(cities_disclosing_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_2020 = pd.merge(merged_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

merged_responses_2020 = pd.merge(cities_responses_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_responses_2020 = pd.merge(merged_responses_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

# 计算协作率
merged_2020[&#39;climate_change_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_climate&#39;]).astype(int)
merged_2020[&#39;water_security_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_water&#39;]).astype(int)

# 计算影响
merged_responses_2020[&#39;impact&#39;] = merged_responses_2020[&#39;Response Answer_city&#39;].apply(lambda x: len(str(x)))

# 计算协作率和平均影响
climate_change_collab_rate = merged_2020[&#39;climate_change_collaboration&#39;].mean()
water_security_collab_rate = merged_2020[&#39;water_security_collaboration&#39;].mean()
average_impact_2020 = merged_responses_2020[&#39;impact&#39;].mean()

print(f&quot;2020 年气候变化协作率：{climate_change_collab_rate}&quot;)
print(f&quot;2020 年水安全协作率：{water_security_collab_rate}&quot;)
print(f&quot;2020 年对城市的平均影响：{average_impact_2020}&quot;)

# 协作率条形图
collab_rates = {
&#39;气候变化&#39;:climate_change_collab_rate,
&#39;水安全&#39;: water_security_collab_rate
}
plt.bar(collab_rates.keys(), collab_rates.values())
plt.title(&#39;2020 年的协作率&#39;)
plt.ylabel(&#39;Rate&#39;)
for i, rate in enumerate(collab_rates.values()):
plt.text(i, rate + 0.01, f&#39;{rate:.2f}&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)
plt.show()

# 影响分布的直方图
plt.hist(merged_responses_2020[&#39;impact&#39;], bins=20, edgecolor=&#39;black&#39;)
plt.title(&#39;2020 年的影响分布&#39;)
plt.xlabel(&#39;影响&#39;)
plt.ylabel(&#39;频率&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</guid>
      <pubDate>Fri, 16 Aug 2024 23:07:44 GMT</pubDate>
    </item>
    <item>
      <title>什么时候需要将数据居中？[关闭]</title>
      <link>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</link>
      <description><![CDATA[我正在尝试找出不同的缩放方法，我有以下问题。StandardScaler、RobustScaler 和类似的缩放器将数据相对于平均值或中位数居中，但 MinMaxScaler 不会这样做，它只会将数据传输到 0 到 1 的范围。什么时候应该将数据居中，什么时候不需要？
我还读到，当数据已经呈正态分布时，最好使用 StandardScaler，但如果只有一部分数据具有此属性怎么办？我应该对一半数据使用 StadardScaler，对另一半数据使用 MinMaxScaler（听起来不合逻辑）还是优先使用其中一种方法？]]></description>
      <guid>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</guid>
      <pubDate>Fri, 16 Aug 2024 21:39:50 GMT</pubDate>
    </item>
    <item>
      <title>Reshape RuntimeError：形状'[1, 2]'对于大小为 100 的输入无效</title>
      <link>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</link>
      <description><![CDATA[我正在 Pytorch 中构建一个简单的足球比分预测模型。
我的步骤：

清理和处理数据
使用球队名称作为输入，将分数作为输出
对球队名称进行独热编码
将两个球队名称传递给模型
添加非线性
（问题就在这里）接收两个分数作为输出

但是我的模型多次返回两个分数
tensor([[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4845], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4 844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844]], grad_fn=&lt;SigmoidBackward0&gt;)

这是我的 NN 的输出，但我不确定为什么当我的输出层大小为 2 个神经元时，它会返回 50x2。
我的模型：
class EstimatorModel(nn.Module):
def __init__(self,
input_neurons:int,
hidden_​​neurons:int,
output_neurons:int):

super().__init__()
self.input_neurons= nn.Linear(in_features=input_neurons, out_features=hidden_​​neurons)
self.hidden_​​neurons= nn.Linear(in_features=hidden_​​neurons, out_features=hidden_​​neurons)
self.output_neurons= nn.Linear(in_features=hidden_​​neurons, out_features=output_neurons)

# 激活函数
self.sigmoid = nn.Sigmoid()

def forward(self, x):
x = self.input_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.output_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

return x

使用带有 Adam 优化器的 MSEloss。
训练循环：
 for e in range(epochs):
x=0
for input, output in zip(input_data, output_data):
input = stack(inputs, dim=1)
output = stack(outputs, dim=1)
#outputs = tensor(outputs

print(outputs.shape)
print(outputs)

prediction:Tensor = model(inputs)
prediction_processed = model.process_model_output(prediction)

print(prediction.shape)
print(prediction)

loss = loss_func(prediction, output.float())

optim.zero_grad()
loss.backward()
optim.step()

quit()

x+=1
# 如果您不喜欢太冗长，请注释掉此打印语句
#print(f&quot;[INFO] {x+1} 对完成。损失：{loss}&quot;)

有什么建议吗？
尝试改变输入传递的方式，例如，正常传递，作为单个张量传递，最后作为堆栈传递。没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</guid>
      <pubDate>Fri, 16 Aug 2024 10:20:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 android kotlin 中实现输出形状为 [1, 25200, 6],[1, 2, 640, 640],[1, 2, 640, 640] 的 yolop ( 分割模型 ) [关闭]</title>
      <link>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</link>
      <description><![CDATA[我正在开发一个 Android 项目，需要使用 YOLOp 模型进行分割。我已成功将模型转换为 onnx/tflite 格式，但在处理输出形状时遇到了困难。该模型包含三个输出，形状分别为 [1, 25200, 6]、[1, 2, 640, 640] 和 [1, 2, 640, 640]
如何在我的 Android 应用程序中解释和处理此输出形状？具体来说，我需要帮助来理解输出张量的结构并提取对象的边界框坐标和分割蒙版。
是否有任何与在 Android 中使用此特定输出形状实现 YOLOP TFLite 相关的代码片段、建议或资源？]]></description>
      <guid>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</guid>
      <pubDate>Fri, 16 Aug 2024 08:05:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关优化 500 个摄像头的计算机视觉集成的建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</link>
      <description><![CDATA[我拥有三个计算机视觉神经网络，正在连接 500 多个摄像头。目前，我正在使用 5 个摄像头进行测试，使用 OpenCV 库接收 RTSP 流。然后，我使用 Ultralytics 库加载神经网络并通过它们处理每个帧。结果使用 OpenCV 绘制为边界框，我使用 Streamlit 显示图像。我正在一台配备 12 GB GPU 的简单 PC 上测试此设置，同时等待一台配备两个 GPU（总计 98 GB）的服务器。但是，性能非常慢，有时系统会完全冻结。
作为计算机视觉的初学者，考虑到未来 500 个摄像头的负载，您能否推荐一些库或方法来提高性能？我计划从 OpenCV 切换到支持 GPU 的库。将模型导出到 ONNX 并通过 ONNX Runtime 运行它们是否有助于提高性能？有人组织过类似的系统吗？]]></description>
      <guid>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</guid>
      <pubDate>Fri, 16 Aug 2024 06:52:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Android 中使用 Google ML Kit 对同一个人的图像进行分组？</title>
      <link>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</link>
      <description><![CDATA[我正在开发一款 Android 应用，需要对包含同一个人脸的图像进行分组。我计划使用 Google ML Kit 进行人脸检测和识别，但我不知道如何实现比较人脸和对人脸进行分组的逻辑。
有人能解释一下使用 Google ML Kit 实现此目的所需的步骤或逻辑吗？我还没有示例代码，所以我需要详细的解释或任何指示。
到目前为止，我已经将 Google ML Kit 集成到我的 Android 项目中并成功实现了人脸检测。但是，我不知道如何继续比较检测到的人脸以确定它们是否属于同一个人，然后相应地对它们进行分组。
我期望找到一种方法来匹配检测到的人脸并创建包含同一个人的图像的集合。理想情况下，我想了解使用 Google ML Kit 实现此逻辑的最佳方法，无论是通过人脸嵌入、特征提取还是任何其他可能有效的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</guid>
      <pubDate>Fri, 16 Aug 2024 04:38:24 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    <item>
      <title>设置分类器参数并直接使用，无需拟合</title>
      <link>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</link>
      <description><![CDATA[我正在使用 python 和 scikit-learn 进行一些分类。
是否可以重复使用分类器学习到的参数？
例如：
from sklearn.svm import SVC

cl = SVC(...) # 使用一些超参数创建 svm 分类器
cl.fit(X_train, y_train)
params = cl.get_params()

让我们将这个 params 存储在某个字符串字典中，甚至写入 json 文件。假设，我们稍后想使用这个经过训练的分类器对一些数据进行一些预测。尝试恢复它：
params = ... # 检索以字典形式存储在某处的这些参数
data = ... # 我们想要预测的数据
cl = SVC(...)
cl.set_params(**params)
predictions = cl.predict(data)

如果我这样做，我会得到 NonFittedError 和以下堆栈跟踪：
File &quot;C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py&quot;, line 548, in predict
y = super(BaseSVC, self).predict(X)
File &quot;C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py&quot;, line 308, in predict
X = self._validate_for_predict(X)
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 437 行，位于 _validate_for_predict
check_is_fitted(self, &#39;support_&#39;)
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\utils\validation.py”，第 768 行，位于 check_is_fitted
raise NotFittedError(msg % {&#39;name&#39;: type(estimator).__name__})
sklearn.exceptions.NotFittedError：此 SVC 实例尚未拟合。使用此方法前，请使用适当的参数调用“fit”。

是否可以设置分类器的参数并在不进行拟合的情况下进行预测？我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</guid>
      <pubDate>Sun, 14 Jan 2018 17:04:03 GMT</pubDate>
    </item>
    </channel>
</rss>