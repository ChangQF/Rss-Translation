<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 08 Dec 2023 01:02:00 GMT</lastBuildDate>
    <item>
      <title>垃圾邮件检测机器学习问题</title>
      <link>https://stackoverflow.com/questions/77623749/email-spam-detection-machine-learning-problem</link>
      <description><![CDATA[我正在制作一个程序，可以检测某人收件箱中的垃圾邮件并为他们删除它。首先，我想通过使用机器学习来检测垃圾邮件，以确定电子邮件是否是垃圾邮件。我从网上复制了某人的垃圾邮件检测器代码，并尝试使用 Jupyter 笔记本运行它。在我复制代码的人的网站上，这个人的最后一行与我相同，当他运行它时，它显示出大约 99% 的准确率。但当我运行它时，每次运行它都能始终保持 0.5% 的准确率。
我有与他完全相同的代码，与他相同的 csv 数据集文件，但我们的结果不同。有机器学习和 Jupyter Notebook 经验的人可以告诉我问题是什么吗？
代码如下：
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn 导入 svm

spam = pd.read_csv(&#39;C:\\Users\\nethm\\Downloads\\spam.csv&#39;)
z = 垃圾邮件[&#39;电子邮件文本&#39;]
y = 垃圾邮件[“标签”]
z_train, z_test,y_train, y_test = train_test_split(z,y,test_size
= 0.2)

CV = CountVectorizer()
特征 = cv.fit_transform(z_train)

模型 = svm.SVC()
model.fit(特征,y_train)

features_test = cv.transform(z_test)
print(model.score(features_test,y_test))
]]></description>
      <guid>https://stackoverflow.com/questions/77623749/email-spam-detection-machine-learning-problem</guid>
      <pubDate>Fri, 08 Dec 2023 00:39:14 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归（数据-预处理）</title>
      <link>https://stackoverflow.com/questions/77623608/logistic-regression-data-preprocessing</link>
      <description><![CDATA[我现在正在学习机器学习中的逻辑回归部分，我真的是初学者:(
我了解到我们需要进行数据预处理，例如删除空值和大纲。
但我不知道我能在这里做什么......！
这是我的作业
这是我的数据。( https:// github.com/nam14d/imt574_conglomorate/blob/806fd329af1672e08827367ba044263703bcee49/Assignment3_wine_quality/quality.csv#L1)
S号num_words num_characters num_misspelled bin_end_qmark num_interrogative bin_start_small num_sentences num_punctuations 标签
1 10 48 2 0 0 0 2 4 B
2 8 25 0 0 0 1 1 0 B
3 20 81 0 1 19 0 1 1 B
4 9 34 1 0 1 0 1 2 B
5 18 69 3 0 1 0 1 0 B
6 7 39 1 0 0 0 1 2 B
7 10 46 4 0 2 1 2 2 B
8 14 70 5 0 0 0 2 16 B
9 0 46 0 0 0 0 1 0 B
10 31 173 26 0 0 1 3 7 B
11 22 90 1 0 1 0 2 9 B
12 8 48 0 1 1 0 1 1 B
13 9 56 5 0 0 0 2 1 B
14 9 41 0 1 0 0 1 1 B
501 79 345 4 0 1 0 9 20 G
502 4 17 2 0 1 1 1 0 G
503 169 831 17 0 1 0 13 48 G
504 25 124 4 0 1 0 4 6 G
505 9 25 0 0 0 0 1 2 G
506 22 103 1 0 0 0 2 4 G
507 44 196 4 0 1 0 6 14 G
508 7 47 0 0 1 0 1 0 G
509 34 118 0 1 1 0 2 2 G
510 17 93 1 0 1 0 1 0 G
511 76 329 3 0 3 0 8 9 G
512 13 44 0 0 1 0 1 0 G
513 10 35 0 0 1 0 1 0 G
514 52 201 1 0 1 0 1 0 G
我在没有预处理的情况下写下了一些内容，但我也不确定我是否可以这样做（抱歉信心不足......！）
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.neighbors 导入 KNeighborsClassifier
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

qual = pd.read_csv(“./quality.csv”)
## 对数回归模型 - X：前八个
qual[&#39;标签&#39;] = np.where(qual[&#39;标签&#39;] == &#39;B&#39;, 0, 1)
X = qual.drop([&#39;序号&#39;,&#39;标签&#39;], axis=1)
X_normalized = X.apply(lambda x: (x-min(x))/max(x)-min(x))
y = 质量[&#39;标签&#39;]
Xtrain，Xtest，ytrain，ytest = train_test_split（X，y，test_size = 0.2）

logmod = 逻辑回归()
logmod.fit(Xtrain, ytrain)

预测 = logmod.predict(Xtest)

打印（accuracy_score（ytest，预测））

你能教我在逻辑回归之前应该处理哪些数据吗？
你能告诉我如何处理这个练习题吗？我正在通过一本数据科学实践书籍来学习。
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/77623608/logistic-regression-data-preprocessing</guid>
      <pubDate>Thu, 07 Dec 2023 23:46:26 GMT</pubDate>
    </item>
    <item>
      <title>模型无法正确保存或加载</title>
      <link>https://stackoverflow.com/questions/77623502/model-not-being-able-to-be-saved-or-loaded-correctly</link>
      <description><![CDATA[我正在制作一个简单的 NPL 模型，我想保存它并让另一个脚本打开该模块。这是一段极其未优化的代码。我怎样才能让它真正发挥作用，而不必每次我想发送新提示时都重新训练它？每次我保存为 .h5 时都会收到错误，每次我将其保存为 .keras 并加载时都会收到错误。我使用 new_model = load_model(&#39;your_model.h5&#39;) 或 load_model(&#39;your_model.keras&#39;)
显示模型架构
new_model.summary()
脚本 1：训练模型并保存
脚本2，单独文件：加载模型。请及时采取行动。预测提示的其余部分
导入tensorflow为tf

从tensorflow.keras.preprocessing.sequence导入pad_sequences
从tensorflow.keras.layers导入嵌入、LSTM、密集、双向
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.models导入顺序
从tensorflow.keras.optimizers导入Adam
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
分词器 = 分词器()

data = open(&#39;rawdata.txt&#39;).read()

语料库 = data.lower().split(“\n”)

tokenizer.fit_on_texts（语料库）
总单词数 = len(tokenizer.word_index) + 1

print(tokenizer.word_index)
打印（总字数）

输入序列 = []
对于语料库中的行：
    token_list = tokenizer.texts_to_sequences([行])[0]
    对于范围内的 i(1, len(token_list))：
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

# 填充序列
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=&#39;pre&#39;))

# 创建预测变量和标签
xs，标签 = input_sequences[:,:-1],input_sequences[:,-1]

ys = tf.keras.utils.to_categorical(标签, num_classes=total_words)

模型=顺序（）
model.add(嵌入(total_words, 100))
model.add(双向(LSTM(150)))
model.add（密集（total_words，激活=&#39;softmax&#39;））
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])
历史= model.fit(xs, ys, epochs=35, verbose=1)
plt.plot(history.history[&#39;准确度&#39;])
plt.title(&#39;随时间变化的模型精度&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.ylabel(&#39;准确度&#39;)
plt.show()
打印（模型）
model.save(&#39;your_model.keras&#39;)
Seed_text = &quot;在《守望先锋》中擅长源氏&quot;; #这是提示。应该是一个字符串
next_words = 25 # 显示的总字数
对于范围内的 _(next_words)：
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=&#39;pre&#39;)
    预测 = np.argmax(model.predict(token_list), axis=-1)
    输出字=“”
    对于单词，在 tokenizer.word_index.items() 中索引：
        如果索引==预测：
            输出字=字
            休息
    种子文本+=“ ” + 输出字
打印（种子文本）
]]></description>
      <guid>https://stackoverflow.com/questions/77623502/model-not-being-able-to-be-saved-or-loaded-correctly</guid>
      <pubDate>Thu, 07 Dec 2023 23:05:05 GMT</pubDate>
    </item>
    <item>
      <title>从图像中检测 ui 播放器元素</title>
      <link>https://stackoverflow.com/questions/77623368/detecting-ui-player-elements-from-a-image</link>
      <description><![CDATA[我有一个用例，需要从播放器的屏幕截图中找到视频播放器元素的坐标，例如暂停、全屏、恢复、下一个视频按钮。
困难在于，元素后面实际上有视频帧，因此背景可能会发生变化。我可以通过哪种方式跟进？我使用了opencv2模板匹配，但它无法检测到元素。
]]></description>
      <guid>https://stackoverflow.com/questions/77623368/detecting-ui-player-elements-from-a-image</guid>
      <pubDate>Thu, 07 Dec 2023 22:16:35 GMT</pubDate>
    </item>
    <item>
      <title>预测多元时间序列（在《机器学习实践》一书的第 15 章中）错误</title>
      <link>https://stackoverflow.com/questions/77623127/forecasting-multivariate-time-series-in-chapter-15-of-the-book-hands-on-machin</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77623127/forecasting-multivariate-time-series-in-chapter-15-of-the-book-hands-on-machin</guid>
      <pubDate>Thu, 07 Dec 2023 21:20:01 GMT</pubDate>
    </item>
    <item>
      <title>用于模拟变量（比例）的统计或机器学习方法[关闭]</title>
      <link>https://stackoverflow.com/questions/77623084/statistical-or-machine-learning-approach-for-simulating-variable-proportion</link>
      <description><![CDATA[我目前面临着分析来自电力驱动生产设施的客户满意度数据的挑战。在本月的特定时期，我们的机械遇到了技术问题，导致客户满意度低于预期。
为了提供背景信息，我们在每月 1 日到 11 日进行最佳运营，并且在这些日子里的每一天，我们都会记录满意客户的百分比。然而，从12日到19日，出现了技术问题，影响了客户满意度。本质上，我们经历了 8 天的“非最佳性能”。以及 22 天的最佳性能。
我正在寻求有关最合适的统计或机器学习方法的建议，以模拟或推断在未发生技术问题的情况下整个月的总体满意度。
我正在考虑的方法：

以 95% 置信区间进行引导重采样、重采样
正常时期客户满意度调查数据
操作来推断我们的预期性能。
蒙特卡罗模拟，假设二项式分布
使用输入的满意度百分比的理论分布
最佳性能时期的数据。

我还听说过使用其他月份的数据进行机器学习或时间序列预测。对于解决这个问题的最佳方法，我希望获得消息灵通且有学术支持的观点。
我想要的输出将是该特定月份的客户满意度比例的置信区间，其中是模拟、推断、预测等。目标是在技术困难未发生的假设下获取此信息]]></description>
      <guid>https://stackoverflow.com/questions/77623084/statistical-or-machine-learning-approach-for-simulating-variable-proportion</guid>
      <pubDate>Thu, 07 Dec 2023 21:10:51 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中多输出模型的高效梯度计算</title>
      <link>https://stackoverflow.com/questions/77622773/efficient-gradient-computation-for-multiple-output-models-in-pytorch</link>
      <description><![CDATA[我有一个图神经网络 (GNN)，其中每个节点都有 N 个独立的读出 MLP，每个 MLP 产生一个标量。这意味着该模型在共享主干中具有大量共享权重，在读出 MLP 中具有少量独立权重。在训练期间（在 .backward() 调用之前），我需要输入的每个读数的梯度，这是一个数组。我有一个简单的方法，可以将除了感兴趣的头之外的 grad_outputs 归零：
batch_size, n_outputs = 输出.shape
梯度= []

对于范围内的 i（n_outputs）：
    grad_output = torch.zeros_like(输出)
    grad_output[:, i] = 1.0

    # 保留除最后一个输出之外的所有输出的图表
    如果 i &lt; 则保留 = True n_outputs - 1 个其他训练

    梯度 = torch.autograd.grad(
        输出=输出，
        输入=输入，
        grad_outputs = grad_output，
        保留图=保留，
        创建图=训练，
        允许未使用=真，
    )[0]

    渐变.append(渐变)

组合梯度 = torch.stack(梯度, 暗淡=-1)
返回 -1 * 组合梯度

但是，这似乎效率低下，因为共享主干中的权重是相同的，但它们可能被计算N次。有没有更有效的方法来做到这一点？例如，首先计算主干网络相对于输入的梯度，然后输出相对于主干输出的梯度？]]></description>
      <guid>https://stackoverflow.com/questions/77622773/efficient-gradient-computation-for-multiple-output-models-in-pytorch</guid>
      <pubDate>Thu, 07 Dec 2023 19:53:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TSFEL 特征提取无法正确迭代数据帧？</title>
      <link>https://stackoverflow.com/questions/77622722/why-is-tsfel-feature-extraction-not-iterating-through-the-data-frame-properly</link>
      <description><![CDATA[我正在使用 TSFEL 迭代一个 4148 行长、总共 6 列的数据帧，其中 1 列分为所有 4148 行的 3 个子列表。我使用的代码如下：
&lt;前&gt;&lt;代码&gt;feature_list = []
对于 df_0_B[&#39;data_lw&#39;] 中的索引：
    测试= tsfel.time_series_features_extractor（cfg，索引）
    feature_list.append（测试）

打印（测试）

我期望它迭代并提取每一行的特征并将它们存储到另一个数据帧中以用于训练和测试。运行特征提取后，我只得到 1 行 120 列。]]></description>
      <guid>https://stackoverflow.com/questions/77622722/why-is-tsfel-feature-extraction-not-iterating-through-the-data-frame-properly</guid>
      <pubDate>Thu, 07 Dec 2023 19:42:07 GMT</pubDate>
    </item>
    <item>
      <title>在实现正则化值的梯度下降代码时出现 TypeError</title>
      <link>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</link>
      <description><![CDATA[我的代码（来自 coursera）：
defgradient_desc(X, Y, w_in, b_in, cost_f, grad_f, alp, num, lambda_):
    m = len(X)
    # 一个数组，用于存储每次迭代的成本 J 和 w，主要用于稍后绘图
    J_历史=[]
    w_历史= []
    对于范围内的 i（num）：
        # 计算梯度并更新参数
        dj_db, dj_dw = grad_f(X, Y, w_in, b_in, lambda_)
        # 使用 w、b、alpha 和梯度更新参数
        w_in = w_in - 阿尔法 * dj_dw
        b_in = b_in - alpha * dj_db
        # 每次迭代时保存成本 J
        if i&lt;100000: # 防止资源耗尽
            J_history.append(cost_f(X, Y, w_in, b_in, lambda_))
        # 每隔 10 次或多次迭代打印成本 if &lt; 10
        如果 i% math.ceil(num/10) == 0:
            w_history.append(w_in)
            print(f&quot;迭代 {i:4}: 成本 {float(J_history[-1]):0.2e} &quot;)

    return w_in, b_in, J_history, w_history #返回 w 和 J,w 历史记录以进行绘图

错误：
TypeError：float() 参数必须是字符串或实数，而不是“NoneType”

这是整个代码中唯一有 bug 的部分（至少到目前为止是有 bug 的）。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77622203/getting-typeerror-while-implementing-the-gradient-descent-code-for-regularized-v</guid>
      <pubDate>Thu, 07 Dec 2023 18:03:29 GMT</pubDate>
    </item>
    <item>
      <title>如何获得分割图像的具体顺序？</title>
      <link>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</link>
      <description><![CDATA[我正在开展一个项目，旨在预测电阻值。为了进行预测，我需要知道电阻器的色带。使用图像分割来检测条带。我可以做到 98% 的准确率。但是，要知道电阻的值。我需要颜色的具体顺序。我怎样才能获得这些数据？
我尝试使用多类逻辑回归。结果很糟糕。人工神经网络能够找到数组中的第一个色带 (row_0)，成功率为 40%。比随机更好，因为有 11 种可能的颜色。
用于 ANN 和回归模型的数据包括像素值 (x,y)、波段的颜色和第一个波段的标签。我还对数据进行了标准化。标签和类别值的范围为 1-11。因为这是电阻值的颜色范围。
训练数据的第一行
图像如何分割]]></description>
      <guid>https://stackoverflow.com/questions/77622154/how-can-i-get-the-specific-order-of-my-segmented-images</guid>
      <pubDate>Thu, 07 Dec 2023 17:55:10 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 Tensorflow 使用 vs code 和 python 来解释牙科图像？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77621824/how-can-i-use-tensorflow-to-interpret-dental-images-using-vs-code-and-python</link>
      <description><![CDATA[为了能够将tensorflow包用于包含牙科图像的tfrecord文件，我需要使用与最新tensorflow版本兼容的python版本。我使用的版本是3.12.0，所以我使用Anaconda创建了一个虚拟环境并安装了python 3.8.18。
我安装了最新版本的tensorflow，即2.3.0。当我开始编写代码时，我需要 matplotlib 来处理图像。运行后，我遇到了这个错误：
AttributeError：模块“numpy”没有属性“object”。
`np.object` 是内置 `object` 的已弃用别名。为了避免现有代码中出现此错误，请单独使用“object”。这样做不会改变任何行为并且是安全的。
别名最初在 NumPy 1.20 中已弃用；

这与我的“代码”无关。我发现这又与软件包的兼容性有关。所以我尝试安装 numpy==1.19.0 ，当时我错误地认为它与张量流兼容。当我尝试使用 conda 提示符安装该程序时，出现了以下错误：
matplotlib 3.7.4 需要 numpy&lt;2,&gt;=1.20，但您有 numpy 1.19.0，这是不兼容的。
tensorflow 2.3.0 需要 numpy&lt;1.19.0,&gt;=1.16.0，但您有 numpy 1.19.0，这是不兼容的。

现在我尝试使用旧版本的 matplotlib(==3.5)。之后，我尝试安装旧版本的 numpy(1.17.0)，但又出现了一些新错误（！）：
 numpy 的构建轮 (setup.py) ... 错误
  错误：子进程退出并出现错误

  × python setup.py bdist_wheel 未成功运行。
  │ 退出代码：1
  ╰─&gt; [293行输出]

错误：需要 Microsoft Visual C++ 14.0 或更高版本。使用“Microsoft C++ 构建工具”获取它：https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
  错误：numpy 构建轮子失败
  为 numpy 运行 setup.py clean
  错误：子进程退出并出现错误

  × python setup.py clean 未成功运行。
  │ 退出代码：1
  ╰─&gt; [10行输出]
      从 numpy 源目录运行。

      不支持 `setup.py clean`，请改用以下选项之一：

        - `git clean -xdf` （清理所有文件）
        - `git clean -Xdf` （清理所有版本化文件，不触及
                            未签入 git 存储库的文件）

      如果必须的话，请将 `--force` 添加到您的命令中以无论如何使用它（不受支持）。

      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
  错误：无法清理 numpy 的构建目录
构建 numpy 失败
错误：无法为 numpy 构建轮子，这是安装基于 pyproject.toml 的项目所必需的

如何为我的目的安装兼容版本的tensorflow、python、matplotlib和numpy？或者还有其他方法来处理牙科射线照相图像吗？]]></description>
      <guid>https://stackoverflow.com/questions/77621824/how-can-i-use-tensorflow-to-interpret-dental-images-using-vs-code-and-python</guid>
      <pubDate>Thu, 07 Dec 2023 16:59:58 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - TypeError：__init__() 获得意外的关键字参数“trained_model_file_path”</title>
      <link>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</guid>
      <pubDate>Tue, 05 Dec 2023 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>L1 正则化不适用于线性回归</title>
      <link>https://stackoverflow.com/questions/77578761/l1-regularisation-not-working-for-linear-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77578761/l1-regularisation-not-working-for-linear-regression</guid>
      <pubDate>Thu, 30 Nov 2023 13:17:26 GMT</pubDate>
    </item>
    <item>
      <title>不同的交叉验证技术产生相同的评估指标</title>
      <link>https://stackoverflow.com/questions/77555347/different-cross-validation-techniques-yielding-identical-evaluation-metrics</link>
      <description><![CDATA[我实现了三种 ML 算法（K 最近邻、决策树和随机森林）并使用四种不同的交叉验证技术（Hold-Out 方法、留一方法、K 折叠交叉验证、分层每个算法的 K 折交叉验证）。目标是评估性能指标并比较技术和算法。我的代码可以运行，但不同技术的评估指标值是相同的。这些值相同是否正常，或者我可能做错了什么？
这是我的代码的一部分：
# 初始化分类器
knn = KNeighborsClassifier(n_neighbors=5, metric=&#39;minkowski&#39;, p=2)
dtree = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(n_estimators=20, criteria=&#39;entropy&#39;, random_state=0)

classifiers = {&#39;KNN&#39;: knn, &#39;决策树&#39;: dtree, &#39;随机森林&#39;: rf}

# 定义交叉验证方法
厕所 = LeaveOneOut()
kf = K折叠(10)
skf = 分层KFold(n_splits=5)

cv_methods = {&#39;保留方法&#39;: (X_train, X_test, y_train, y_test),
              “留一法”：loo，
              &#39;K 折交叉验证&#39;: kf,
              “分层 K 折交叉验证”：skf}

# 对每个分类器和交叉验证方法进行分类和评估
对于 clf_name，clf 在 classifiers.items() 中：
    print(f&quot;分类器：{clf_name}&quot;)
    对于 cv_name、cv_method 在 cv_methods.items() 中：
        if cv_name == &#39;保留方法&#39;:
            X_train_cv、X_test_cv、y_train_cv、y_test_cv = cv_method
            clf.fit(X_train_cv, y_train_cv)
            y_pred = clf.predict(X_test_cv)
        别的：
            分数 = cross_val_score(clf, X, y, cv=cv_method, 评分=&#39;准确度&#39;)
            

        # 计算评价指标
        准确度=准确度_得分（y_test_cv，y_pred）
        精度 = precision_score(y_test_cv, y_pred, 平均值=&#39;加权&#39;)
        召回率=召回率（y_test_cv，y_pred，平均值=&#39;加权&#39;）
        f1 = f1_score(y_test_cv, y_pred, 平均值=&#39;加权&#39;)
        混乱=混乱_矩阵（y_test_cv，y_pred）

以下是输出，每个分类器和交叉验证方法的输出都是相同的：
分类器：KNN
KNN 的保留方法指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的留一法指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的 K 折交叉验证指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的分层 K 折交叉验证指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

分类器：决策树
决策树的保留方法指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的留一法指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的 K 重交叉验证指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的分层 K 重交叉验证指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

分类器：随机森林
随机森林的保留方法指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的留一法指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的 K 重交叉验证指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的分层 K 重交叉验证指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

为什么会发生这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/77555347/different-cross-validation-techniques-yielding-identical-evaluation-metrics</guid>
      <pubDate>Mon, 27 Nov 2023 08:08:58 GMT</pubDate>
    </item>
    <item>
      <title>标准化卫星图像以输入神经网络的正确方法是什么？</title>
      <link>https://stackoverflow.com/questions/75115715/what-is-the-right-way-to-normalize-satellite-images-to-feed-into-a-nerual-networ</link>
      <description><![CDATA[我正在尝试将小块卫星图像数据（landsat-8 表面反射带）输入到我的项目的神经网络中。但是下载的图像值范围为 1 到 65535。
所以我尝试将图像除以 65535（最大值），但绘制它们会显示所有黑色/棕色图像，如下所示！

但是大多数图像的值都不接近 65535
没有任何标准化，图像看起来全是白色。

将图像除以 30k 看起来像这样。

如果图像太暗或太亮，我的网络可能无法按预期运行。
将图像除以最大值（65535）是唯一的解决方案吗？还是有其他方法可以标准化图像，尤其是卫星数据？]]></description>
      <guid>https://stackoverflow.com/questions/75115715/what-is-the-right-way-to-normalize-satellite-images-to-feed-into-a-nerual-networ</guid>
      <pubDate>Sat, 14 Jan 2023 04:08:10 GMT</pubDate>
    </item>
    </channel>
</rss>