<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Fri, 21 Feb 2025 03:24:28 GMT</lastBuildDate>
    <item>
      <title>岩石，纸剪刀JS [关闭]</title>
      <link>https://stackoverflow.com/questions/79455428/rock-paper-scissors-js</link>
      <description><![CDATA[目前正在从事我的第一个JS项目；岩石剪刀。我发现我的功能GetComputerChoice（）和Gethumanchocie毫无疑问地运行，但是随后的功能没有给出任何响应。当我一次在用户输入后立即运行整个程序时，没有其他任何事情发生，控制台在控制台中没有任何错误或警告。
我认为我的错误位于programp末尾的游戏环函数中，尽管我不确定是什么原因导致IT不给出输出，也不会通过回合进行比赛。
 让ComputersCore = 0
令圆= 0


函数getComputerChoice（）{

    令CompChoice = Math.floor（Math.random（） * 3） + 1;
    // choosese在1到3之间的随机数。

 
    if（compchoice === 1）{
     compchoice =“ rock＆quot”;
    } 
    if（compchoice === 2）{
     compchoice =; quot; quot;;
    }
    if（compchoice === 3）{
     compchoice ===“剪刀”
    };

    返回compchoice;
    
 };

 功能Gethumanchoice（）{
    //请选择玩家选择岩石，纸和剪刀。
    //将返回玩家选择。
        让hunanchoice =提示（挑选：岩石，纸或剪刀！＆quot;）。tolowercase（）;
    
        返回汉班
 }
        
功能游戏环（Hushanchoice，Compchoice）{
    //递增回合获胜者的得分和原木获奖者公告。
    if（//计算机赢的条件
    Hunanchoice ===“摇滚” ＆amp;＆amp; compchoice ===“纸” ||
    Hunanchoice ===“纸” ＆amp;＆amp; compchoice ===“剪刀” ||
    Hunanchoice ===“剪刀” ＆amp;＆amp; compchoice ===“摇滚” {）{
    ComputersCore = ComputersCore + 1;
    返回（`您失去$ {compchoice}击败$ {hunanchoice}
            计算机获得了一个点！`）
    } else if（//球员赢得的条件！
    Hunanchoice ===“摇滚” ＆amp;＆amp; compchoice ===“剪刀” ||
    hunanchoice ==&#39;纸＆amp;＆amp; compchoice ===“摇滚” ||
    Hunanchoice ===“剪刀” ＆amp;＆amp; compchoice ===＆quot; quot＆quort {
    返回（`您赢了。$ {hunanchoice}击败$ {compchoice}。
        你有一个观点！`）
    } else if（hunanchoice === compchoice）{//条件如果
        返回（这是领带！&#39;）
    } else {//无效的选择
        返回（`无效选择，请重试！`）
            
    };            

};

const humanselection = gethumanchoice（）;
const Computerselection = getComputerChoice（）;

游戏环（人类选择，计算机选择）

功能游戏游戏（PlayRound，Round）{// 5轮游戏玩法
   
    if（round === 0）{
        警报（第1轮.....显示！＆quot;）
        playround（）;
        圆=圆+1; 
    } else if（round === 2）{
        警报（第二回合.....显示！＆quot;）
        playround（）;
        圆=圆+1;
    } else if（round === 3）{
        警报（第3轮.....显示！＆quot;）
        playround（）;
        圆=圆+1;
    } else if（round === 4）{
        警报（第4轮.....显示！＆quot;）
        playround（）; 
        圆=圆+1;
    } else if（round === 5）{
        警报（第5轮.....显示！＆quot;）
        playround（）;
        圆=圆+1;
    } else if（round == 6）{
        优胜者（）;
    }
        
}
    
功能赢家（HumanScore，ComputersCore）{
    if（humanscore＆gt; computerscore）{
        返回（``你赢了！你有$ {humanscore}点， 
            计算机具有$ {ComputersCore}点`）;
    } else if（humanscore＆lt; computerscore）{
        返回（`计算机获胜！，计算机具有$ {ComputersCore}点
                您有$ {humanscore}点`）;
    } 别的 {
        返回（``````tie&#39;&#39;！您和计算机都有$ {homanscore}点！`）;
    };
            
}
    
playround（）;
 ]]></description>
      <guid>https://stackoverflow.com/questions/79455428/rock-paper-scissors-js</guid>
      <pubDate>Thu, 20 Feb 2025 17:55:28 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow模型训练，列表到Numpy阵列转换不均会改变数据形状</title>
      <link>https://stackoverflow.com/questions/79455291/tensorflow-model-training-list-to-numpy-array-conversion-unevenly-changes-the-s</link>
      <description><![CDATA[我正在尝试从MRI图像中预测LSDC。对于每个研究_id，都有多个图像。每个研究_id代表每个患者。我想在5个级别上预测5个条件下的3级严重程度。
我正在尝试使用 sequence 类从TensorFlow创建数据集。这是我的DataGenerator类：
 类CustomDatagenerator（序列）：
    def __init __（self，image_dict，num_img，labels_dict = none，batch_size = 8，image_size =（224，224），shuffle = true）：
       self.image_dict = image_dict
       self.labels_dict = labels_dict
       self.batch_size = batch_size
       self.image_size = image_size
       self.shuffle =洗牌
       self.ids = list（image_dict.keys（））
       self.num_img = num_img
       self.on_epoch_end（）

    def __len __（自我）：
       返回int（np.floor（len（self.ids） / self.batch_size））

    def __getItem __（自我，索引）：
       start = index * self.batch_size
       end = min（（索引 + 1） * self.batch_size，len（self.ids））
       batch_ids = self.ids [start：end]
       batch_images = []
       batch_labels = []

       对于batch_ids中的ID_：
           图像= []

           对于self.image_dict.get（id_，[]）中的image_path：
               dicomdata = pydicom.dcmread（image_path）
               图像= dicomdata.pixel_array
               图像= cv2.resize（图像，self.image_size）
               image = np.expand_dims（图像，axis = -1）
               image = image.astype（&#39;float32&#39;） / np.max（图像）
               图像= np.Repeat（图像，3，轴= -1）
               images.append（图像）

           图像= np.Array（图像）

           如果Len（Images）＆lt; self.num_img：
               pad_amount = self.num_img- len（图像）
               padding = [（0，pad_amount）] + [（0，0）] *（len（images.shape） -  1）
               图像= np.pad（图像，填充，模式=&#39;常数&#39;）

           batch_images.append（图像）

           如果self.labels_dict：
               label = np.array（self.labels_dict.get（id_），dtype = np.float32）
               batch_labels.append（标签）

       batch_images = np.stack（batch_images）
       如果self.labels_dict：
           batch_labels = np.array（batch_labels，dtype = np.float32）
           返回batch_images，batch_labels

       返回batch_images

    def on_epoch_end（self）：
       如果self.shuffle：
           np.random.shuffle（self.ids）
 
我的标签字典如下：
  i，sid在枚举中（train_df [&#39;study_id&#39;]）：
        labels_dict [str（sid）] = []
        对于条件下的骗局：
            如果train_df.loc [i，con] ==&#39;normal_mild&#39;：
                labels_dict [str（sid）]。附加（[1，0，0]）
            elif train_df.loc [i，con] ==&#39;严重&#39;：
                labels_dict [str（sid）]。附加（[0，0，1]）
            别的：
                labels_dict [str（sid）]。附加（[0，1，0]）

       labels_dict [str（sid）] = np.array（labels_dict [str（sid）]，dtype = np.float32）
 
我尝试了多种方法将 labels_dict 转换为numpy数组。但是，要么在训练时显示形状错过错误。或试图查看数据时显示错误。
它显示的错误如下：
  -----＆gt; 1 Model.Fit（train_generator，epochs = 2）＃，step_per_epoch = len（train_generator）// 8）

/USR/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in Error_handler（*args，** kwargs）
    120＃要获取完整的堆栈跟踪，请致电：
    121＃`keras.config.disable_traceback_filtering（）`
 - ＆gt; 122从无
    123最后：
    124 del filtered_tb

＆lt; ipython-Input-12-Cf42609bddda＆gt;在__getItem __（自我，索引）中
     47 batch_images = np.stack（batch_images）
     48如果self.labels_dict：
---＆gt; 49 batch_labels = np.array（batch_labels，dtype = np.float32）
     50返回batch_images，batch_labels
     51 

ValueError：设置具有序列的数组元素。 1个维度后，请求的阵列具有不均匀的形状。检测到的形状为（8，） +不均匀部分。
 
我尝试使用 np.stack 或 batch_labels = batch_labels.reshape（（（batch_labels.shape.shape.shape [0]），len（presition），3），3））），但它显示不同的错误。我的数据没有任何 nan ，所有 labels_dict 均为Shape （batch_size，num_of_condition，severity_class）。即使我尝试从发电机打印数据。生成器数据形状来自 data_x，data_y = next（iter（train_generator））显示模型输入和输出的数据形状。我无法弄清楚这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79455291/tensorflow-model-training-list-to-numpy-array-conversion-unevenly-changes-the-s</guid>
      <pubDate>Thu, 20 Feb 2025 17:02:54 GMT</pubDate>
    </item>
    <item>
      <title>乘数优化器[关闭]</title>
      <link>https://stackoverflow.com/questions/79454118/multiplier-optimiser</link>
      <description><![CDATA[我有一个1000个记录的数据集（可以是n），并且具有12个不同的乘数。每个乘数可以将值从0.1到10.0，以0.1的步骤递增。
对于每个记录，a＆quot&#39;finalsCore&#39;使用所有12个乘数计算。  该分数的计算基于中间步骤（如果需要，可以共享的细节）。 （乘法，除法，魔鬼，BM25L计算）
我的目的是确定记录的12个乘数中每个乘数中的每个乘数的最佳值，以便当我进行“ finalsCore”的AVG时。和标签“ r＆quot”在财产中“相关性”要为那些决赛的文档＆gt; avg得分，以便根据“ r＆quot”确定最大文档。要遵循什么过程，以便我们获得一组最终的乘数组合
例如： - 
所有Initalized乘数M1，M2，M3，M4 ..... M12为0.1 
和
何时
m1 = 2.3，m2 = 5，m3 = 1.0，m4 = 8，m5 = 7.5，m6 = 0，m7 = 8.9，m8 = 4.0，m9 = 2.7，m10 = 10，m10 = 10，m11 = 6.1，m12，m12 = 4.9;我们得到AVG分数，可以说89.4（最大），所有大于89.4的文档都需要标记为“ R＆quot”计数出现780/1000，因此百分比为78％&lt; / p&gt;
请注意： - 保持所有值10.0不会最大化分数，而某些乘数则在分母 /其他规则中。&lt; / p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/79454118/multiplier-optimiser</guid>
      <pubDate>Thu, 20 Feb 2025 10:17:35 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle笔记本上的TPU配置</title>
      <link>https://stackoverflow.com/questions/79453184/tpu-configuration-on-kaggle-notebook</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79453184/tpu-configuration-on-kaggle-notebook</guid>
      <pubDate>Thu, 20 Feb 2025 02:09:57 GMT</pubDate>
    </item>
    <item>
      <title>Google Colab太慢而无法选择功能 - 如何优化性能？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79452905/google-colab-is-too-slow-for-feature-selection-how-to-optimize-performance</link>
      <description><![CDATA[在训练机器学习模型之前，我正在使用Google Colab在大数据集上执行功能选择。但是，运行时非常慢，执行时间需要很长的时间才能返回结果。
我尝试的是：

将运行时类型更改为T4 GPU，但注意到RAM的使用显示为0/15GB，这似乎是不寻常的。
检查了数据集大小（〜[提及大小]行和列）。
用于编码，标准标准（）和lasso（）用于功能选择的使用pd.get_dummies（）。
尝试通过转换数据类型来减少内存使用。

问题：

即使对于基本操作（例如缩放和编码），执行缓慢。
特征选择（拉索回归）的时间太长。
不确定Colab的资源是否不够，或者我的方法是否效率低下。

问题：

如何优化Colab中的特征选择和预处理？
是否有更快的特征选择方法？
我如何有效地管理记忆使用量以防止慢速性能？
 T4 GPU运行时适合此，还是我应该使用其他设置？
]]></description>
      <guid>https://stackoverflow.com/questions/79452905/google-colab-is-too-slow-for-feature-selection-how-to-optimize-performance</guid>
      <pubDate>Wed, 19 Feb 2025 22:12:54 GMT</pubDate>
    </item>
    <item>
      <title>当MLFlow模型被``@Champion''更改时，如何自动更新SageMaker端点？</title>
      <link>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</link>
      <description><![CDATA[我在AWS SageMaker AI上使用托管MLFLOW服务器（ https://www.youtube.com/watch?v=3xkz_5hop6k&amp;ab_channel = awsevents ）以跟踪实验和模型版本。我们的数据科学团队通过用冠军别名标记最佳版本来促进生产模型。这些模型部署到了萨吉人端点，然后通过API网关访问。
我想实现自动化管道，每当将新型号分配给MLFlow中的冠军别名时，该管道都会更新SageMaker端点。目前，我正在考虑将针对别名更改进行轮询的Lambda功能，但我正在寻找更高效或更有托管的解决方案。是否有人实现了动态端点更新机制或可以提出替代方法？]]></description>
      <guid>https://stackoverflow.com/questions/79452354/how-can-i-automatically-update-a-sagemaker-endpoint-when-the-mlflow-model-aliase</guid>
      <pubDate>Wed, 19 Feb 2025 18:00:58 GMT</pubDate>
    </item>
    <item>
      <title>将Python ML模型与Flutter客户端集成</title>
      <link>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</link>
      <description><![CDATA[我在工作中面临挑战，我需要在我的客户端应用程序上运行许多Python ML模型，因为使某些模型在服务器上运行。。
除了项目资产中的张量流光模型和实施Python模型的同事告诉我，他不能以Tflite模型导出某些模型。，我没有实验。
有一个软件包（OnnxRuntime）使用ONNX型号，并在我的flutter代码中使用了其中的功能，例如Dart FFI，我以前使用此软件包在我的颤音代码中运行C ++功能，并且可以很好地运行。我的牛头人说，我有同样的问题，他不能将所有模型提取到ONNX模型中，但这让我认为有一种方法可以在我的Flutter应用程序中使用Python代码，例如Dart FFI，我知道它不会一样，因为Python是一种解释语言，我无法从中脱离共享对象，所以我的问题是：是否有一种方法可以在我的客户端应用程序或Python ML模型中使用Python代码，而无需使用TFLITE或OnnxRuntime？]]></description>
      <guid>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</guid>
      <pubDate>Wed, 19 Feb 2025 16:13:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的GPT-2小型模型的响应不一致且重复性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</link>
      <description><![CDATA[ i在1100个JSON文件的数据集上微调的GPT-2（小），分为：

 400 Q＆amp;一对
 700食谱数据（标题，成分，说明等）

该模型对基于食谱的提示有些响应，例如“如何制作芝士蛋糕”生成准确的成分和方向（即使我认为这只是重复培训数据）。但是，当被问及诸如“您好，您好吗？”之类的一般性问题时，它会产生荒谬的答案，例如提到没有上下文的素食主义者。
这是培训进度：

时期1：训练损失：没有日志|验证损失：1.882118 
时期2：训练损失：2.690300 |验证损失：1.865422 
时期3：训练损失：2.690300 |验证损失：1.844494 
时期4：训练损失：2.638200 |验证损失：1.806402 

我怀疑：

模型过于适应结构化数据。
 gpt-2 Small可能不适合此类任务。
]]></description>
      <guid>https://stackoverflow.com/questions/79451815/why-are-my-gpt-2-small-models-responses-incoherent-and-repetitive</guid>
      <pubDate>Wed, 19 Feb 2025 14:56:34 GMT</pubDate>
    </item>
    <item>
      <title>我是否将神经网络锁定错误？预测有时似乎是随机的</title>
      <link>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</guid>
      <pubDate>Wed, 19 Feb 2025 08:47:17 GMT</pubDate>
    </item>
    <item>
      <title>D2L包装在Google Colab上安装</title>
      <link>https://stackoverflow.com/questions/76248695/d2l-package-installation-on-google-colab</link>
      <description><![CDATA[我需要用插入D2L包装
  PIP安装D2L == 1.0.0B0
 
但是，每当我尝试将其下载到Google Colab时，都会发生以下错误：
 在索引中查看：https：//pypi.org/simple，https://us-python.pkg.dev/colab-wheels/public/simple/
收集D2L == 1.0.0-Beta0
  使用加速的D2L-1.0.0.0.0.0.0b0-py3-none.whl（141 kb）
收集Jupyter（来自D2L == 1.0.0-Beta0）
  使用缓存的jupyter-1.0.0-py2.py3-none-any.whl（2.7 kb）
需求已经满足：numpy in/usr/local/lib/python3.10/dist-packages（来自d2l == 1.0.0-beta0）（1.22.4）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的matplotlib（来自d2l == 1.0.0-beta0）（3.7.1）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的matplotlib-inline（来自d2l == 1.0.0-beta0）（0.1.6）
需求已经满足：/usr/local/lib/python3.10/dist-packages中的请求（来自d2l == 1.0.0-beta0）（2.27.1）
已经满足的要求：/usr/local/lib/python3.10/dist-packages（来自d2l == 1.0.0-beta0）（1.5.3）（1.5.3）
收集健身房== 0.21.0（来自d2l == 1.0.0-beta0）
  使用Cached Gym-0.21.0.tar.gz（1.5 MB）
  错误：子进程 - 纠正
  
  ×python setup.py egg_info并未成功运行。
  │退出代码：1
  ╰ - ＆gt;有关输出，请参见上文。
  
  注意：此错误源自子过程，可能不是PIP的问题。
  准备元数据（setup.py）...错误
错误：元数据生成失败

×生成软件包元数据时遇到错误。
╰ - ＆gt;有关输出，请参见上文。

注意：这是上面提到的软件包的问题，​​而不是PIP。
提示：有关详细信息，请参见上文。
 
如何在Google Colab上安装D2L软件包？]]></description>
      <guid>https://stackoverflow.com/questions/76248695/d2l-package-installation-on-google-colab</guid>
      <pubDate>Sun, 14 May 2023 16:59:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用现有的ML工具将人类可读的时间表转换为表？</title>
      <link>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</link>
      <description><![CDATA[我有我的美国原住民部落制作的报纸的时间表。我试图使用 aws textract 从此产生某种表格。 AWS textract在这方面没有识别任何表。因此，我认为这不会起作用（如果我付款，也许可能会发生更多的事情，但这并不是这样）。。
最终，我试图筛选所有存档的报纸，并下载所有选举周期的所有时间表（“一般性”一般性“和“特殊咨询”），以查找时间表中每个项目之间的天数。 
由于这一切都在公共领域，所以我没有理由在这里粘贴桌子的图片。我还将包括文档的下载URL。 “ https://i.sstatic.net/cgbv8.png”/&gt;  
下载url：下载 
我首先在各个文档上使用Foxit读取器在Windows上找到时间表。
然后，我在ubuntu上使用了工具&#39;ocrmypdf&#39;来确保所有这些文档都是可搜索的（ocrmypdf -skip-text通知_of_of_special_election_2023.pdf.pdf.pdf./output/notce/notice_of_special_election_election_election_2023.pdf）。
然后，我碰巧在我的Google Newsfeed中看到了AWS Sextract的广告。看到它有多强大。但是当我尝试过时，它实际上并没有找到这些可读的时间表。
我希望我想知道是否有任何ML工具甚至其他解决方案来解决此类问题。
我是在试图使我的技术诀窍达到标准。最近两年我生病了，这是一个有趣的问题，我认为我认为很流行。]]></description>
      <guid>https://stackoverflow.com/questions/75475057/how-to-convert-a-human-readable-timeline-to-table-using-existing-ml-tools</guid>
      <pubDate>Thu, 16 Feb 2023 16:16:27 GMT</pubDate>
    </item>
    <item>
      <title>破烂的张量作为LSTM的输入</title>
      <link>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</link>
      <description><![CDATA[学习破烂的张量以及如何将它们与张量流一起使用。
我的示例
  xx = tf.ragged.constant（[[
                        [0.1，0.2]，
                        [0.4、0.7、0.5、0.6]
                        ）））
yy = np.Array（[[[0，0，1]，[1,0,0]]）

mdl = tf.keras.Sequeential（[
    tf.keras.layers.inputlayer（input_shape = [none]，batch_size = 2，dtype = tf.float32，ragged = true），
    tf.keras.layers.lstm（64），  
    tf.keras.layers.dense（3，激活=&#39;softmax&#39;）
）））

mdl.compile（loss = tf.keras.losses.sparsecategoricalcrossentropy（from_logits = true），
              优化器= tf.keras.optimizers.adam（1E-4），
              指标= [&#39;准确性&#39;]）

mdl.summary（）
历史= mdl.fit（xx，yy，epochs = 10）
 
错误
  lastm_152层的输入0与图层不兼容：预期ndim = 3，找到ndim = 2。收到完整的形状：[2，无]
 
我不确定是否可以使用类破烂的张量。我发现的所有示例都在LSTM之前嵌入层，但是我不想创建其他嵌入层。 ]]></description>
      <guid>https://stackoverflow.com/questions/62031683/ragged-tensors-as-input-for-lstm</guid>
      <pubDate>Tue, 26 May 2020 21:25:12 GMT</pubDate>
    </item>
    <item>
      <title>将ONNX模型转换为Keras</title>
      <link>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</link>
      <description><![CDATA[我尝试将ONNX模型转换为keras，但是当我调用转换函数时，我会收到以下错误消息“ typeError：typeError：nondable类型：&#39;google.protobuf.pyext._message._message.repeatedscalarcontainer&#39; &gt; 
 ONNX模型输入：Input_1 
您可以在此处看到ONNX模型： https://ibb.co/sknbxwy  
 导入onnx2keras
来自onnx2keras导入onnx_to_keras
进口keras
导入ONNX

onnx_model = onnx.load（&#39;onnxmodel.onnx&#39;）
k_model = onnx_to_keras（onnx_model，[&#39;input_1&#39;]）

keras.models.save_model（k_model，&#39;kerasmodel.h5&#39;，overwrite = true，include_optimizer = true）

 
 文件“ c：/../ onnx2keras.py”，第7行，in＆lt; module＆gt;
    k_model = onnx_to_keras（onnx_model，[&#39;input_1&#39;]）
  文件“ .. \ site-packages \ onnx2keras \ converter.py”，第80行，在onnx_to_keras中
    weights [onnx_extracted_weights_name] = numpy_helper.to_array（onnx_w）
TypeError：不可用的类型：&#39;google.protobuf.pyext._message.repeatededscalarcontainer&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</guid>
      <pubDate>Tue, 15 Oct 2019 13:15:06 GMT</pubDate>
    </item>
    <item>
      <title>计算给定神经网络的拖鞋数量？</title>
      <link>https://stackoverflow.com/questions/55831235/calculating-the-number-of-flops-for-a-given-neural-network</link>
      <description><![CDATA[我有一个用Keras编写的用于图像分类的神经网络（Alexnet或VGG16），我想计算网络的浮点操作数量。数据集中图像的大小可能会有所不同。
可以用Python编写广义代码，该代码可以自动计算拖鞋？有库可用吗？
我正在与Spyderanaconda合作，而定义的网络是一个顺序模型。]]></description>
      <guid>https://stackoverflow.com/questions/55831235/calculating-the-number-of-flops-for-a-given-neural-network</guid>
      <pubDate>Wed, 24 Apr 2019 13:28:32 GMT</pubDate>
    </item>
    <item>
      <title>在不使用任何numpy或sklearn库的情况下手动计算AUC</title>
      <link>https://stackoverflow.com/questions/53603376/calculate-auc-manually-without-using-any-numpy-or-sklearn-library</link>
      <description><![CDATA[我给出了一组x，y坐标，我需要使用梯形公式找到AUC，而无需使用任何numpy或sklearn库。 
 （x0，y0）始终是（0,0）
（xn，yn）总是（1,1）
 
下图
    
不使用任何Sklearn库，我知道我需要在下面找到
  hi =？
wi =？
auc = sum（hi * wi）
 
现在我不确定如何找到HI，WI。我认为我没有所有必要的数据来进行高中数学。我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/53603376/calculate-auc-manually-without-using-any-numpy-or-sklearn-library</guid>
      <pubDate>Mon, 03 Dec 2018 23:13:35 GMT</pubDate>
    </item>
    </channel>
</rss>