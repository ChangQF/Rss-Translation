<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 25 May 2024 03:18:16 GMT</lastBuildDate>
    <item>
      <title>使用 xgboost 推断 csv 数据时出现问题</title>
      <link>https://stackoverflow.com/questions/78531267/there-was-a-problem-infering-csv-data-with-xgboost</link>
      <description><![CDATA[我在用xgboost进行推断时遇到了问题。我是xgboost的新手。所以可能犯了一些低级错误，希望得到帮助，谢谢！
简单来说，我想用xgboost做一个回归任务，由几个csv数据集组成。我把它们拼接成一个dataframe，并使用train_test_split分割train/val/test。模型运行良好（mae：0.6）。但是当我手动分割训练集和测试集时（我选了一部分csv放在测试文件夹中），结果变得很差（mae：12+）。
我真的很想知道这里发生了什么？我在下面发布了一些代码。
1：这是使用 train_test_split 的拆分代码：
# 准备好数据
datasets = []
path = &#39;../data/low_fidelity_chips_res&#39;
for filename in os.listdir(path):
if filename.endswith(&quot;.csv&quot;):
dataset = ThermalDataset(os.path.join(path, filename))
datasets.append(dataset)

# 合并数据集
[merged_dataset = pd.concat([pd.DataFrame(dataset.X) for dataset in datasets])
merged_targets = pd.concat([pd.DataFrame(dataset.y) for dataset in datasets])
X_scaled = scaler.fit_transform(merged_dataset)
y_scaled = scaler.fit_transform(merged_targets)

#划分
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=11)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)

# 构建 xgboost 模型
model = xgb.XGBRegressor(tree_method=&#39;gpu_hist&#39;, gpu_id=device.index, n_estimators=500, learning_rate=0.05, max_depth=8)
model.fit(X_train, y_train)

# 评估
y_val_pred = scaler.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()
y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()
y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
val_mse = mean_squared_error(y_val_original, y_val_pred)
test_mse = mean_squared_error(y_test_original, y_test_pred)
val_mae = mean_absolute_error(y_val_original, y_val_pred)
test_mae = mean_absolute_error(y_test_original, y_test_pred)]

2：这是我手动除法之后的结果代码：
`# 训练数据集
datasets = []
path = &#39;../data/low_fidelity_chips_res&#39;
for filename in os.listdir(path):
if filename.endswith(&quot;.csv&quot;):
dataset = ThermalDataset(os.path.join(path, filename))
datasets.append(dataset)

# 测试数据，它是我从原始数据中手动划分的测试集的 csv
tests = []
test_file = &#39;../data/test_xgboost/Thermal014withMidPos.csv&#39;
test = ThermalDataset(test_file)
tests.append(test)

# 合并
merged_dataset = pd.concat([pd.DataFrame(dataset.X) for dataset in datasets])
merged_targets = pd.concat([pd.DataFrame(dataset.y) for dataset在数据集中])
test_x = pd.concat([pd.DataFrame(test.X) 用于测试中的测试])
test_y = pd.concat([pd.DataFrame(test.y) 用于测试中的测试])

# 规范化
X_scaled = scaler.fit_transform(merged_dataset)
y_scaled = scaler.fit_transform(merged_targets)
x_fill = scaler.fit_transform(test_x)
y_fill = scaler.fit_transform(test_y)

# 分割
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.05, random_state=11)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)

#训练
model = xgb.XGBRegressor(tree_method=&#39;gpu_hist&#39;, gpu_id=device.index, n_estimators=500, learning_rate=0.05, max_depth=8)
model.fit(X_train, y_train)

# 评估
y_val_pred_scaled = model.predict(X_val)
y_test_pred_scaled = model.predict(X_test)
y_fill_res = model.predict(x_fill)

# inverse_transform 获取原始数据
y_val_pred = scaler.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()
y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()
y_pre = scaler.inverse_transform(y_fill_res.reshape(-1, 1)).flatten()
y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_fill = scaler.inverse_transform(y_fill.reshape(-1, 1)).flatten()
val_mse = mean_squared_error(y_val_original, y_val_pred)
test_mse = mean_squared_error(y_test_original, y_test_pred)
val_mae = mean_absolute_error(y_val_original, y_val_pred)
test_mae = mean_absolute_error(y_pre, y_fill)`

我希望能够在单个 csv 文件上正确推理并获得与训练中一样好的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78531267/there-was-a-problem-infering-csv-data-with-xgboost</guid>
      <pubDate>Sat, 25 May 2024 02:40:08 GMT</pubDate>
    </item>
    <item>
      <title>wandb 的超带算法在哪些时期检查改进？</title>
      <link>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</link>
      <description><![CDATA[我正在尝试使用 wandb 库的超参数调整（又名扫描）功能（链接到其官方页面）。我正在尝试应用贝叶斯超频带算法。
现在，正如这些页面中提到的（如何定义扫描配置), (与提前终止选项相关的参数是什么 ），在提前终止下我们必须提到4个参数（通常），它们是min_iter，s，eta和max_iter，它看起来像下面这样。
我的疑问总结：
总而言之，我想知道的是，
给定所有 4 个：- min_iter、s、eta 和 max_iter

超频带算法将在哪个时期检查改进？

考虑到我正在尝试进行贝叶斯超带，将在第一个括号中评估多少次运行，以及在连续括号中将评估多少次运行？

是否有任何方法或经验法则来决定这 4 个参数（min_iter、s、eta 和 max_iter）采用什么值比较合适？

请更详细地解释参数 s 和 eta（尤其是 eta），即使用一些基础数学知识（如果可能，请保持简单）。


我的疑问是什么？ （更详细/上下文地解释）：
（这里），他们有一些&lt; /em&gt; 解释了超频带算法在哪个时期（他们的）实现检查改进并决定是否终止运行。
当我们只关心每次运行的最小迭代次数时
当我们只关心每次运行的最小迭代次数时
但是当我们同时关心每次运行的最小和最大迭代次数时该怎么办？
就像下面这个...
#在 python 中 wanbd 使用的 yaml 文件中
提前终止：
  类型：超频带
  分钟迭代数：10
  秒：3
  预计时间：4
  最大迭代数：50

我已经尝试过的：
我什至尽力阅读原始论文并尝试了解正在发生的事情（或可能发生的事情）（超频带算法原始论文链接），但未能得到满意的答案。
我什至尝试访问他们的 github 页面，那里有示例，但他们只展示了如何编写配置，没有深入解释它的作用。]]></description>
      <guid>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</guid>
      <pubDate>Fri, 24 May 2024 20:25:42 GMT</pubDate>
    </item>
    <item>
      <title>PPO 仅适用于单个 epoch 和未剪裁的损失</title>
      <link>https://stackoverflow.com/questions/78530486/ppo-only-working-with-a-single-epoch-and-unclipped-loss</link>
      <description><![CDATA[我正在尝试实现 PPO 来击败 cartpole-v2，如果我将事情保持为 A2C（即，没有剪切损失和单个纪元），当我使用剪切损失和多个 epoch 时，我设法让它工作它没有学习纪元，大约一周以来一直试图在我的实现中找到问题，但我找不到问题所在。
完整代码
这是负责优化的函数：
def finish_episode():
    # 计算损失并执行反向传播
    R = 0
    已保存的动作 = actor.已保存的动作
    返回 = []
    ε = 0.3
    num_epochs = 1 # 当 num_epochs 大于 1 时我的网络将无法学习

    对于 actor.rewards[::-1] 中的 r：
        R = r + 0.99 * R # 伽玛值为 0.99
        返回.插入(0, R)
    返回= torch.tensor（返回，设备=设备）
    返回 = (返回 - returns.mean()) / (returns.std() + eps)

    old_probs、state_values、状态、actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(设备)
    state_values = torch.stack(state_values).to(设备)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)

    优点 = 回报 - state_values.squeeze()

    对于范围内的纪元（num_epochs）：

        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        比率 = 新概率 / 旧概率

        surr1 = 比率 * 优势
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * 优点

        #actor_loss = -torch.min(surr1, surr2).mean() # 当使用这个（剪辑的）损失时，我的网络将无法学习
        actor_loss = -surr1.mean()

        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        如果纪元 == num_epochs - 1：
            ritic_loss = F.smooth_l1_loss(state_values.squeeze(), 返回)
            
            批评家优化器.zero_grad()
            ritic_loss.backward(retain_graph=False)
            批评家优化器.step()

    删除演员.奖励[:]
    删除 actor.saved_actions[:]

尝试了不同的超参数，使用 gae 而不是完整的蒙特卡洛重新调整/优点，在梳理我的代码时我看不出有什么问题。]]></description>
      <guid>https://stackoverflow.com/questions/78530486/ppo-only-working-with-a-single-epoch-and-unclipped-loss</guid>
      <pubDate>Fri, 24 May 2024 20:05:01 GMT</pubDate>
    </item>
    <item>
      <title>如何处理推荐系统中的cod邮政功能？</title>
      <link>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</link>
      <description><![CDATA[我有一个带有邮政编码列的数据集。它们具有一定的意义，我想将其用作一项功能。我正处于预处理阶段，但仍然不确定我将使用的算法。
我需要有关使用邮政编码列作为功能的最佳方法的建议。
我发现你可以使用 one-hot 编码，但就我而言，我正在处理大量唯一的邮政编码（例如 3513 个不同的邮政编码），one-hot 编码（或虚拟编码）是不切实际的，因为它引入的高维度。]]></description>
      <guid>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</guid>
      <pubDate>Fri, 24 May 2024 14:06:08 GMT</pubDate>
    </item>
    <item>
      <title>制作简单天气预报模型的指南[关闭]</title>
      <link>https://stackoverflow.com/questions/78528754/guide-on-making-a-simple-weather-prediction-model</link>
      <description><![CDATA[我想制作一个简单的天气预报模型，根据过去的数据（我拥有的数据）进行训练，并预测未来的一些天气情况，例如温度、最低温度、最高温度和降水量。我怎样才能做到呢？我有一个很大的天气数据集：有关数据集的所有功能/详细信息
我尝试在互联网上搜索，但到目前为止没有答案。
我的问题是，哪种机器学习模型最适合使用，但我不确定。
我还想知道天气预报最好由机器学习来完成。
您可以查看我的数据特征 ((https://i.sstatic.net/OFQTMn18.png)) 这对机器学习训练有好处吗？]]></description>
      <guid>https://stackoverflow.com/questions/78528754/guide-on-making-a-simple-weather-prediction-model</guid>
      <pubDate>Fri, 24 May 2024 13:04:56 GMT</pubDate>
    </item>
    <item>
      <title>进行超参数调整后浅层卷积神经网络中的过度拟合</title>
      <link>https://stackoverflow.com/questions/78528272/overfitting-in-a-shallow-convolutional-neural-network-after-doing-hyper-paramete</link>
      <description><![CDATA[我正在 Google Colab 中开发一个 CNN 模型来解决 3 类分类问题。我有一个平衡的类数据集，其中包含 1680 个训练样本（每个 560 个）和 420 个测试样本（每个 140 个）。我构建了一个浅层网络来训练我的数据。但我的模型在 7/8 个 epoch 之后就过度拟合了。我已经应用了所有可能的技术，例如 Dropout、l2 正则化、数据增强、批量标准化、降低学习率，但模型性能没有任何变化。
我在 30 个 epoch 上实现了损失：0.7746 - 准确度：0.6673 - val_loss：0.8310 - val_accuracy：0.6071。
#model architecture
model = models.Sequential()
model.add(layers.Conv2D(8, (3, 3),activation=&#39;relu&#39;, input_shape=(28, 28, 3)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(16, (3, 3),激活=&#39;relu&#39;))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.5))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(3, 激活=&#39;softmax&#39;))

model.summary()

模型：“sequence_4”
_________________________________________________________________
层 (类型) 输出形状 参数 # 
======================================================================
conv2d_8 (Conv2D) (无，26，26，8) 224 

batch_normalization_8 (Bat (无，26，26，8) 32 
chNormalization) 

max_pooling2d_4 (MaxPoolin (无，13，13，8) 0 
g2D) 

conv2d_9 (Conv2D) (无，11，11，16) 1168 

batch_normalization_9 (Bat (无，11，11， 16) 64 
chNormalization) 

max_pooling2d_5 (MaxPoolin (None, 5, 5, 16) 0 
g2D) 

dropout_7 (Dropout) (None, 5, 5, 16) 0 

flatten_4 (Flatten) (None, 400) 0 

dropout_8 (Dropout) (None, 400) 0 

density_5 (Dense) (None, 3) 1203 

============================================================================
总参数：2691 (10.51 KB)
可训练参数：2643 (10.32 KB)
不可训练参数：48 (192.00 字节)

learning_rate=0.001、batch_size=8、optimizer=Adam、loss=&#39;categorical_crossentropy&#39; 和
target_size=(28,28)
分类报告
准确率召回率 f1 分数支持

中等容忍度 0.33 0.66 0.43 140
易受影响 0.33 0.13 0.18 140
容忍度 0.39 0.23 0.29 140

准确率 0.34 420
宏平均值 0.35 0.34 0.30 420
加权平均值 0.35 0.34 0.30 420

虽然我在中等容忍度类别中具有较高的 f1 分数，但我的模型仍然将未见过的图像预测为易感类别。以下是训练和验证准确度和损失的图表。


这背后的原因是什么，如何改进我的模型，以便我的模型能够很好地概括？]]></description>
      <guid>https://stackoverflow.com/questions/78528272/overfitting-in-a-shallow-convolutional-neural-network-after-doing-hyper-paramete</guid>
      <pubDate>Fri, 24 May 2024 11:29:02 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 .predict() 给出属性错误</title>
      <link>https://stackoverflow.com/questions/78527509/predict-in-python-gives-an-attribute-error</link>
      <description><![CDATA[def train_model（x_train，y_train，dropout_prob，lr，batch_size，epochs）：
    nn_model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, 激活=&#39;relu&#39;, input_shape=(10,)),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
        ]）

    nn_model.compile(keras.optimizers.Adam(lr),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

    历史记录 = nn_model.fit(
        x_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.2,verbose=0
    ）

    情节历史（历史）

    返回 nn_model，历史记录


最小损失模型 = train_model(x_train, y_train, 0.2, 0.005, 128, 100)
预测=least_loss_model.predict(x_test)
打印（预测）

这给出了以下属性错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\~\ai.py”，第 162 行，在  中
    预测=least_loss_model.predict(x_test)
                ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError：“元组”对象没有属性“预测”

我已经尝试过predicted=least_loss_model.predict_proba(x_test)。]]></description>
      <guid>https://stackoverflow.com/questions/78527509/predict-in-python-gives-an-attribute-error</guid>
      <pubDate>Fri, 24 May 2024 09:02:15 GMT</pubDate>
    </item>
    <item>
      <title>我可以在我的应用程序中使用此处提供的机器学习模型吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78526975/can-i-use-the-machine-learning-model-provided-here-in-my-app</link>
      <description><![CDATA[我正在尝试创建一个使用人工智能技术的应用程序。
并将分发到App Store。
本网站提供的模型之一(https://developer.apple.com/machine-将使用学习/模型/）。
如果我这样做，会产生任何版权或法律问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78526975/can-i-use-the-machine-learning-model-provided-here-in-my-app</guid>
      <pubDate>Fri, 24 May 2024 07:07:06 GMT</pubDate>
    </item>
    <item>
      <title>时间序列相关回归中的数据泄漏</title>
      <link>https://stackoverflow.com/questions/78525482/data-leakage-in-time-series-related-regression</link>
      <description><![CDATA[我有一个包含解释变量值和目标变量的数据集。它们都是不同的历史日常值。 X 是不同的经济指标，Y 是债券收益率的前瞻性变化。因此，对于第 N 天，X 是当前失业率和通货膨胀率，Y 是 (yield_n+3 / Yield_n) - 1，这是 3 天的变化。
我的问题是，如果我稍后使用 sklearn 中的 train_test_split，我可以打开 shuffle = True 吗？
我知道对于典型的时间序列回归，这将导致数据泄漏，但在这里我不使用 Y 的过去值，也不使用任何滞后。
理论上，我想对数据进行洗牌，因为从我所看到的来看，X 和 Y 之间的关系会随着时间的推移而变化，所以如果我仅根据较早和较晚的日期分割数据，我担心我会训练模型稍微过时的值。
顺便说一句，我使用梯度提升作为我的模型
那么，我可以在我的情况下使用 shuffle = True 吗？如果是，哪些附加功能可能导致泄漏：滞后、季节性影响或其他因素？]]></description>
      <guid>https://stackoverflow.com/questions/78525482/data-leakage-in-time-series-related-regression</guid>
      <pubDate>Thu, 23 May 2024 20:43:46 GMT</pubDate>
    </item>
    <item>
      <title>Mediapipe 培训数据[关闭]</title>
      <link>https://stackoverflow.com/questions/78525263/data-for-mediapipe-training</link>
      <description><![CDATA[我正在尝试向媒体管道框架提供尽可能多的图片，但我发现从各个角度制作人物图片以从人们那里获取数据非常累人。
我还没有尝试媒体管道的视频训练，因为我讨厌它剪切视频。
举个例子，我想训练演习小队，所以我需要三个州来组建一个小队

站立
小队下线
小分队

对于每一个状态，我都需要大量的训练数据来喂养人工智能，否则它不会计算我的运动。
从动作中获取尽可能多的图片而不浪费太多时间的最佳方法是什么？
制作手动图片，但我意识到这会花费多少时间。]]></description>
      <guid>https://stackoverflow.com/questions/78525263/data-for-mediapipe-training</guid>
      <pubDate>Thu, 23 May 2024 19:45:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 VS Code 上的 computervision + yolov8 应用程序进行实时网络摄像头数据分类</title>
      <link>https://stackoverflow.com/questions/78524343/live-web-cam-data-classification-using-computervision-yolov8-application-on-vs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78524343/live-web-cam-data-classification-using-computervision-yolov8-application-on-vs</guid>
      <pubDate>Thu, 23 May 2024 15:57:14 GMT</pubDate>
    </item>
    <item>
      <title>寻找特定于移动设备的二进制文件的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78521260/seeking-dataset-of-mobile-specific-binaries</link>
      <description><![CDATA[我目前正在训练机器学习模型，并需要特定于移动设备的二进制文件的数据集。尽管我付出了努力，但我仍然无法找到大量的数据集。
向我建议的另一种选择是从 AOSP 批量下载二进制文件，但我不确定如何开始此过程。]]></description>
      <guid>https://stackoverflow.com/questions/78521260/seeking-dataset-of-mobile-specific-binaries</guid>
      <pubDate>Thu, 23 May 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 SHAP 解释学习到的潜在空间位置</title>
      <link>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</link>
      <description><![CDATA[我在 MNIST 数据集上的 pytorch 中实现了一个监督自动编码器。
我在潜在空间（大小 8）上使用分类层对其进行监督。在训练期间，我优化了 MSE 重建损失和分类损失 (BCE)。我在潜在空间中有单个实例，这些实例很有趣，我想找到它们不同位置的解释。
所以我的问题是，在潜在维度上使用 SHAP 值是否是一种有效的方法（它有效，我得到了值，但我不确定这是否有意义）。
更具体地说：我想比较例如实例 A 和实例 B。假设在潜在空间中它们相距很远，例如在潜在维度 3 of 8 中。现在我想找到输入中可以解释这种现象的像素。因此，我计算实例 A 和 B 的潜在表示的 SHAP 值，并比较两者的维度 3 的 SHAP 值。这是有效的吗？我认为它与解释多输出回归没有太大不同，对吧？但我还没有看到任何 SHAP 的应用来解释潜在位置。]]></description>
      <guid>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</guid>
      <pubDate>Wed, 22 May 2024 12:17:33 GMT</pubDate>
    </item>
    <item>
      <title>每个时期 Retinanet 模型内部的数据流</title>
      <link>https://stackoverflow.com/questions/78516393/flow-of-data-inside-the-retinanet-model-in-each-epoch</link>
      <description><![CDATA[通过提供batch_size、epochs和每个epoch的步骤，向retinanet model_network提供了多少数据？
到目前为止，我认为步长的计算如下：
step_size = (total_number_of_data/batch_size)*epochs

而在keras-retinanet中，它以batch_size、epochs和steps_per_epoch作为参数，这与上述情况不同。我怀疑计算是如何进行的。]]></description>
      <guid>https://stackoverflow.com/questions/78516393/flow-of-data-inside-the-retinanet-model-in-each-epoch</guid>
      <pubDate>Wed, 22 May 2024 09:04:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的分段似乎没有保存？关于totalsegmentator</title>
      <link>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</link>
      <description><![CDATA[我一步步按照您的教程进行操作，但得到的结果类似于分段未保存。
这是我输入的语句和得到的结果：
(d:\totalsegmentotar.conda) D:\totalsegmentotar&gt;TotalSegmentator -i hip_left.nii.gz -o 分段 -ta hip_implant

如果您使用此工具，请引用：https://pubs.rsna.org/doi/10.1148/ryai.230024

未检测到 GPU。在CPU上运行。这可能会非常慢。 &#39;--fast&#39; 或 --roi_subset 选项可以帮助减少运行时间。
生成粗糙的身体分割...
重新采样...
1.93 秒内重新采样
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ ██| 1/1 [00:00&lt;00:00, 1.12it/s]
预测12.95秒后
重新采样...
警告：无法裁剪，因为未检测到前景
从 (333, 333, 539) 裁剪到 (333, 333, 539)
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ | 64/64 [04:27&lt;00:00, 4.18s/it]
预测 288.96 秒
保存分段...
0%| | 0/1 [00:00
可以看到分割没有保存，我用切片器软件看确实没有预测结果，什么也没有显示。
当我使用`-tatotal时，分割器进度条发生变化，但不幸的是它似乎没有保存分割的结果。这是我的输出，以及在切片器 5.6.2 中打开的输出文件夹和图像，但没有显示任何内容。
这是我的 powershell 输出
这是我的输出文件夹和在切片器 5.6.2 中打开的图像]]></description>
      <guid>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</guid>
      <pubDate>Wed, 22 May 2024 07:52:18 GMT</pubDate>
    </item>
    </channel>
</rss>