<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 15 May 2024 03:18:53 GMT</lastBuildDate>
    <item>
      <title>如何使用 TensorFlow 提高多类分类的准确性？</title>
      <link>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</link>
      <description><![CDATA[我正在使用 TensorFlow 解决多类分类问题，并在实现令人满意的准确性方面遇到了挑战。我有7节课。文件夹中的每个类包含 2000 个 .csv 文件（每个文件有两列）。当我使用二元分类方法训练模型时，用另一个类测试一个类，准确性和 val_accuracy 会很高，0.85 到 0.95，但是当我使用多标签 7 进行测试时类，准确率最高达到0.47。下面是包含数据抛光和模型多类的代码。
#文件夹中的 csv 类
文件夹路径 = [
    &#39;/content/drive/MyDrive/medical_chem/Aa&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ab&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ac&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ba&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Bb&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Cc&#39;,
    &#39;/内容/驱动器/MyDrive/medical_chem/DD&#39;
]


数据 = []
标签=[]

#加载文件夹并将文件csv存档在数据框中
对于 enumerate(folder_paths) 中的 class_index、folder_path：
    对于 os.listdir(folder_path) 中的文件：
        file_path = os.path.join(文件夹路径, 文件)
        df = pd.read_csv(文件路径)
        数据.append(df)
        labels.append(class_index)

X = 数据
y = 标签

# 找到数据框中的最小值
min_length = min(len(df) for df in X)
# 设置数据帧长度相同
truncated_dfs = [df.head(min_length) for df in X]
# 数据帧到 numpy 数组
X = np.array([df.truncated_dfs 中 df 的值])


# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# 标准化数据
X_train = 归一化(X_train, 轴=1)
X_test = 归一化(X_test, 轴=1)
y_train = to_categorical(y_train, num_classes=7)
y_test = to_categorical(y_test, num_classes=7)


X_train.shape、y_train.shape、X_test.shape、y_test.shape
# 输出 ((8943, 2906, 2), (8943, 7), (2236, 2906, 2), (2236, 7))

模型 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(64, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(7,activation=&#39;softmax&#39;) # 7个类的输出层
]）

# 训练模型的检查点
checkpoint_path = “training_checkpoint/cp.ckpt”
checkpoint_dir = os.path.dirname(checkpoint_path)
checkpoint_callback = ModelCheckpoint(文件路径=checkpoint_path,
                                      save_weights_only=真，
                                      save_best_only=真，
                                      监视器=&#39;val_loss&#39;,
                                      详细=1)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;分类交叉熵&#39;，
              指标=[&#39;准确性&#39;])

#model.load_weights(检查点路径)

历史 = model.fit(X_train, y_train,
                    纪元=100，
                    验证数据=（X_测试，y_测试），
                    回调=[检查点回调])


我尝试过调整神经网络的架构，尝试不同的激活函数，并优化学习率和批量大小等超参数。但是，我仍然没有达到预期的准确性。
我确信我出错的地方是在预处理数据或模型中，因为二进制训练有很好的结果。
与二进制训练相比，准确度为 0.85 至 0.95
**多类别的预期准确率：高于 0.90
**
数据集： https://drive .google.com/drive/folders/1UAt50dPH7ABeoLu16nfa19g4oVccPeFO?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</guid>
      <pubDate>Wed, 15 May 2024 00:27:22 GMT</pubDate>
    </item>
    <item>
      <title>特征工程是一种新近特征[关闭]</title>
      <link>https://stackoverflow.com/questions/78481149/feature-engineering-a-recency-feature</link>
      <description><![CDATA[我有一个客户评分问题，我正在专门研究预测转化并得出转化的概率分数（使用 xgboost 分类器 atm）。我想介绍一个功能，但我很难明确该功能的定义。
具体来说，我知道当事件 A 最近发生时（例如，客户给我们的办公室打电话），这表明客户对我们的产品感兴趣并且可能会转化。为此，我创建了一个新近度功能，基本上是：（今天 - 事件日期）以天为单位。
问题在于，这没有捕捉到旧客户记录的影响。例如，客户可能在一年前给我们打电话（事件 A 触发），并在不久后进行转换，并且使用该公式，新近度特征将相对较大。我希望模型知道低新近度值会转化为更高的概率。
有没有什么好的方法来设计功能来捕捉这种关系？]]></description>
      <guid>https://stackoverflow.com/questions/78481149/feature-engineering-a-recency-feature</guid>
      <pubDate>Wed, 15 May 2024 00:26:20 GMT</pubDate>
    </item>
    <item>
      <title>使用梯度下降时，线性回归模型的训练误差和测试误差非常相似</title>
      <link>https://stackoverflow.com/questions/78480089/the-training-error-and-testing-error-is-very-similiar-for-linear-regression-mode</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78480089/the-training-error-and-testing-error-is-very-similiar-for-linear-regression-mode</guid>
      <pubDate>Tue, 14 May 2024 18:43:35 GMT</pubDate>
    </item>
    <item>
      <title>带训练数据的基本贝叶斯线性回归[关闭]</title>
      <link>https://stackoverflow.com/questions/78479716/basic-bayesian-linear-regression-with-training-data</link>
      <description><![CDATA[在此处输入图片说明
我很困惑，因为我以为我的体重参数是w？无论如何，我想要验证我是否走在正确的道路上或完全被误导了，因为我的可能性一直为 0。
# 之前的分发
rv = multivariate_normal([0,0,0], (alpha ** (-1)) * np.eye(3))

#可能性
阿尔法 = 0.7
协方差 = (alpha ** (-1)) * np.eye(len(traininDataY))
w = np.array([0, 2.5, -0.5])
TransposeW= w.reshape(-1, 1)
phi = np.column_stack((np.ones_like(x1train), np.power(x1train, 2), np.power(x2train, 3)))
平均值 = np.dot(xmatrix, w_Transpose)
可能性 = multivariate_normal.pdf(tTrain, np.ravel(mean), Cov2)
]]></description>
      <guid>https://stackoverflow.com/questions/78479716/basic-bayesian-linear-regression-with-training-data</guid>
      <pubDate>Tue, 14 May 2024 17:19:05 GMT</pubDate>
    </item>
    <item>
      <title>无法腌制本地对象“main.<locals>.preprocess_train”</title>
      <link>https://stackoverflow.com/questions/78479321/cant-pickle-local-object-main-locals-preprocess-train</link>
      <description><![CDATA[我的目标是训练一个lora模型。我是 python 和 AI 的新手，决定使用这个脚本：  https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py 。
当我运行脚本时出现此错误：
`回溯（最近一次调用最后一次）：
文件“C:\Users\PNP\Desktop\Nappi\script\CreazioneLora.py”，第 977 行，在  中
主要的（）
文件“C:\Users\PNP\Desktop\Nappi\script\CreazioneLora.py”，第 726 行，位于 main 中
对于步骤，批量枚举（train_dataloader）：
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\accelerate\data_loader.py”，第 451 行，位于 __iter__ 中
dataloader_iter = super().__iter__()
                  ^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 439 行，位于 __iter__ 中
返回 self._get_iterator()
       ^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 387 行，在 _get_iterator 中
返回_MultiProcessingDataLoaderIter（自身）
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 1040 行，位于 __init__ 中
w.start()
文件“C:\Python311\Lib\multiprocessing\process.py”，第 121 行，在 start`
`self._popen = self._Popen(self)
              ^^^^^^^^^^^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\context.py”，第 224 行，位于 _Popen
返回 _default_context.get_context().Process._Popen(process_obj)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\context.py”，第 336 行，位于 _Popen
返回 Popen(process_obj)
       ^^^^^^^^^^^^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\popen_spawn_win32.py”，第 94 行，位于 __init__ 中
duction.dump(process_obj, to_child)
文件“C:\Python311\Lib\multiprocessing\reduction.py”，第 60 行，转储中
ForkingPickler(文件，协议).dump(obj)
AttributeError：无法腌制本地对象 &#39;main..preprocess_train&#39;`

我通过加速启动和各种输入参数从提示符启动了脚本...可能是什么问题？
我希望它能够根据我作为输入提供的数据库开始训练模型（https:/ /huggingface.co/datasets/TheFusion21/PokemonCards)]]></description>
      <guid>https://stackoverflow.com/questions/78479321/cant-pickle-local-object-main-locals-preprocess-train</guid>
      <pubDate>Tue, 14 May 2024 15:59:52 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 的线性层未训练</title>
      <link>https://stackoverflow.com/questions/78479296/pytorchs-linear-layer-not-training</link>
      <description><![CDATA[我正在尝试实现一个深度嵌入式自组织映射（DESOM），它是一个自动编码器，与可训练的 SOM 一起作为可训练层，我使用线性层实现：
类 SOM(nn.Module)：
    def __init__(
        自身，map_height = 10，map_width = 10，
        Latent_dim = 50，p_norm = 2
    ）：
        超级().__init__()
        
        self.map_height = 地图高度
        self.map_width = map_width
        self.latent_dim = Latent_dim
        self.p_norm = p_norm
        self.som_nodes = self.map_height * self.map_width

        # SOM（扁平化）权重初始化的均匀采样-
        # self.som_wts = torch.distributions.uniform.Uniform(low = - 1 / np.sqrt(latent_space_dim), high = 1 / np.sqrt(latent_space_dim)).sample((m * n, Latent_space_dim))

        # 创建嵌入字典 -
        # self.embedding = nn.Embedding(self.som_nodes, self.latent_dim)
        # self.embedding.weight.data.uniform_(-np.sqrt(1 / self.latent_dim), np.sqrt(1 / self.latent_dim))

        # 使用线性层创建 SOM（无偏差）-
        self.som_wts = nn.Linear（in_features = self.latent_dim，out_features = self.map_height * self.map_width，偏差= False）
        self.som_wts.weight.data.uniform_(-np.sqrt(1 / self.latent_dim), np.sqrt(1 / self.latent_dim))


    def 转发（自身）：
        经过

将 Autoencoder 与 SOM 相结合的整个模型为：
类 DESOM(nn.Module):
    def __init__(
        自我，latent_dim = 50，
        容量= 16，地图高度= 10，
        地图宽度 = 10，p_norm = 2，
    ）：
        超级().__init__()
        self.latent_dim = Latent_dim
        自身容量=容量
        self.map_height = 地图高度
        self.map_width = map_width
        self.p_norm = p_norm

        # tot_train_iterations = num_epochs * len(train_loader)
        # self.decay_vals = list(scheduler(it = step, tot = tot_train_iterations) for step in range(1, tot_train_iterations + 6))
        # self.decay_vals = torch.tensor(np.asarray(decay_vals))

        self.encoder = 编码器(latent_dim = self.latent_dim, 容量 = self.capacity)
        self.decoder = 解码器(latent_dim = self.latent_dim, 容量 = self.capacity)
        self.som = SOM(map_height = self.map_height, map_width = self.map_width, p_norm = self.p_norm)


    def 前向（自身，x）：
        z = self.encoder(x)
        x_recon = self.decoder(z)
        返回 z，x_recon

# 指定 SOM 超参数 -
# m = SOM 高度-
索姆高度 = 20

# n = SOM 宽度-
索姆宽度 = 20

潜在空间暗度 = 50

# 初始化模型-
# 初始化DESOM模型-
模型 = DESOM(
    Latent_dim = Latent_space_dim，容量 = 16，
    地图高度 = 索姆高度，地图宽度 = 索姆宽度，
    p_norm = p_norm
）

model.som.som_wts.weight.shape
# 火炬.Size([400, 50])

# 随机初始化权重-
model.som.som_wts.weight.min().item(), model.som.som_wts.weight.max().item()
＃（-0.14141587913036346，0.14140239357948303）

在train_one_epoch()函数中，训练自动编码器和SOM层的主要代码是：
# 获取潜在代码并重构-
z, x_recon = 模型(x)

优化器.zero_grad()

# 自动编码器重建损失-
recon_loss = F.mse_loss(输入 = x_recon, 目标 = x)

# SOM 训练代码-
l2_dist_z_soms = torch.cdist(x1 = z, x2 = model.som.som_wts.weight, p = p_norm)
Mindist, bmu_indices = torch.min(l2_dist_z_soms, -1)
bmu_locations = 位置[bmu_indices]
squared_l2_norm_dists = torch.square(torch.cdist(x1 = bmu_locations, x2 = 位置, p = p_norm))

# 计算当前迭代/步骤的西格玛 -
全局步骤
curr_sigma_val = sigma_0 * torch.exp(-step / lmbda_val)
步骤 += 1

# 计算高斯地形邻域-
topo_neighb = torch.exp(-squared_l2_norm_dists / ((2 * torch.square(curr_sigma_val)) + 1e-6))

# 计算地形损失-
topo_loss = topo_neighb * squared_l2_norm_dists

# 沿所有 SOM 单位求和并沿批次求平均值 -
topo_loss = topo_loss.sum(1).mean()

# 计算总损失-
总损失 = 侦察损失 + (gamma * topo_loss)
# 伽玛 = 0.001

# 计算梯度与计算损失 -
总损失.backward()
        
# 执行一步梯度下降-
优化器.step()

整个代码可以参考此处。为了简洁起见，我省略了其他部分。
但是训练完成后，当我看到 SOM 层的训练权重时 -
model.som.som_wts.weight.min().item(), model.som.som_wts.weight.max().item()
＃（-0.14141587913036346，0.14140239357948303）

model.som.som_wts.weight.requires_grad
＃ 真的

作为线性层的 SOM 似乎并未经过训练！是什么阻止它接受训练？]]></description>
      <guid>https://stackoverflow.com/questions/78479296/pytorchs-linear-layer-not-training</guid>
      <pubDate>Tue, 14 May 2024 15:56:30 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型签名/水印 [关闭]</title>
      <link>https://stackoverflow.com/questions/78479253/ml-model-signing-watermarking</link>
      <description><![CDATA[我需要研究一项服务是否可以验证另一项服务是否使用特定的 ML 模型进行计算。这个想法是“验证者”服务正在调用“worker”具有输入的服务以及将在工作端处理输入的预期模型。工作人员正在响应模型提出的预测。然后验证器服务必须确认使用了正确的模型。此外，Miner 可以随时重新训练模型。
我有什么选择？是否有任何已知的良好做法？]]></description>
      <guid>https://stackoverflow.com/questions/78479253/ml-model-signing-watermarking</guid>
      <pubDate>Tue, 14 May 2024 15:49:25 GMT</pubDate>
    </item>
    <item>
      <title>调查 TensorFlow 和 PyTorch 性能的差异</title>
      <link>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</guid>
      <pubDate>Tue, 14 May 2024 13:54:26 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：CUDA 错误：设备端断言已触发</title>
      <link>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</guid>
      <pubDate>Tue, 14 May 2024 05:51:57 GMT</pubDate>
    </item>
    <item>
      <title>如何实现每层有多个类的多标签层次分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78468216/how-to-implement-multi-label-hierarchical-classification-with-multiple-classes-a</link>
      <description><![CDATA[我正在开发一个需要多标签层次分类的项目，其中层次结构的每个级别都有多个要预测的类。具体来说，我正在处理一个场景，其中标签遵循​​层次结构，并且在层次结构的每个级别上，都有多个可能的类。
我的项目旨在根据 Instagram 用户的兴趣对他们进行分类。目标是将用户分类为分层兴趣组，其中每个用户可能属于层次结构不同级别的多个类别。
例如，考虑克里斯蒂亚诺·罗纳尔多作为示例用户，他的主要兴趣可能是“体育”和“家人和朋友”作为顶级品类，进一步细化“体育”类分类为“足球”和“手球” （假设）作为同一级别的子类别。
我已经探索了一些现有的分层分类方法，但它们似乎专注于单标签分类或假设每个级别都有一个类别。任何人都可以建议一种方法或为我提供资源来实现每个级别有多个类的多标签层次分类吗？
任何见解、算法或代码示例将不胜感激。
预先感谢您的帮助！
我一直在尝试使用多标签层次分类来根据 Instagram 用户的兴趣对他们进行分类。但是，我的方法遇到了一个限制：它似乎只在层次结构的每一级别对一个类进行分类，这不适合我的用例。]]></description>
      <guid>https://stackoverflow.com/questions/78468216/how-to-implement-multi-label-hierarchical-classification-with-multiple-classes-a</guid>
      <pubDate>Sun, 12 May 2024 14:35:02 GMT</pubDate>
    </item>
    <item>
      <title>ValueError: matmul: 输入操作数 1 的核心维度 0 不匹配，gufunc 签名为 (n?,k),(k,m?)->(n?,m?)（大小 5 与 3 不同）</title>
      <link>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</link>
      <description><![CDATA[将 numpy 导入为 np
从 numpy.linalg 导入 inv
从 scipy.linalg 导入 pinv

# 定义必要的函数
def create_laplacian_from_adjacency(adj_matrix):
    Degree_matrix = np.diag(adj_matrix.sum(axis=1))

    laplacian_matrix = Degree_matrix - adj_matrix
    
    返回拉普拉斯矩阵

def dirichlet_energy(L, X):
    “”““用于平滑度量化的狄利克雷能量。”“””
    返回 np.trace(X.T @ L @ X)

def update_C(X, X_tilde, L, C, gamma, alpha, lam, J):
    “”“”使用具有主函数近似的梯度下降来更新C。
    p, k = C.shape
    C_old = np.copy(C)
    
    梯度_f = (-2 * gamma * L @ C_old @ inv(C_old.T @ L @ C_old + J) +
                  alpha * (C_old @ X_tilde - X) @ X_tilde.T +
                  2 * L @ C_old @ X_tilde @ X_tilde.T + lam * C_old @ np.ones((k, k)))
    
    # 主要函数优化步骤（简化方法）
    t = 0.01 # 学习率，需要根据实际应用进行调整
    C_new = pinv(C_old - t * 梯度_f)
    C_new = np.maximum(C_new, 0) # 强制非负性
    返回C_new

def update_X(X, L, C, alpha):
    “”“基于更新的C更新X(tilda)。”“”“
    inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C)
    X_tilde_new = inv_matrix @ C.T @ X
    返回 X_tilde_new

def fgc_algorithm(X, L, alpha, gamma, lam, iterations=5):
    “”“”执行特征图粗化算法。“”“”
    p, n = X.形状
    k = 3 # 假设粗化的降维为 3
    C = np.random.rand(p, k)*0.1
    J = np.full((k, k), 1/k)
    X_代字号 = pinv(C)@X

    对于范围内的 i（迭代）：
        C = update_C(X, X_tilde, L, C, gamma, alpha, lam, J)
        X_tilde = update_X(X, L, C, alpha)
        当前能量 = dirichlet_energy(L, X_tilde)
        print(f&quot;迭代 {i}: 狄利克雷能量 = {current_energy}&quot;)

    L_c=C.T@L@C
    返回 C、L_c、X_tilde

＃ 例子：
X = np.random.rand(5, 7) # 10 个节点的随机特征
adj_matrix = np.array([
    [0, 1, 0, 0, 0],
    [1, 0, 1, 1, 1],
    [0, 1, 0, 1, 0],
    [0, 1, 1, 0, 1],
    [1, 1, 0, 1, 0]
]）
L = create_laplacian_from_adjacency(adj_matrix) # 创建样本拉普拉斯矩阵
alpha, gamma, lam = 0.1, 1, 0.5 # 正则化参数

C、L_c、X_tilde = fgc_algorithm(X、L、alpha、gamma、lam)
print(&quot;更新的 C 矩阵：\n&quot;, C)
print(&quot;L_C 矩阵:\n&quot;, L_c)
print(&quot;更新后的特征矩阵 X(tilda):\n&quot;, X_tilde)


我正在尝试实现特色粗化图算法，但每次代码到达第 34 行时：inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C),
出现了上述错误。
我已经尝试检查所有内容，但根据我的说法，矩阵的尺寸是正确的，所以我不太明白为什么会出现这个问题？
如果您碰巧明白这一点，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</guid>
      <pubDate>Fri, 10 May 2024 14:28:29 GMT</pubDate>
    </item>
    <item>
      <title>“管道”对象没有属性“_check_fit_params”</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[来自 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler
从 imblearn.pipeline 导入管道

# 定义特征和目标
X = df.drop(&#39;感染&#39;, axis=1)
y = df[&#39;感染&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样到多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 对多数类进行欠采样，使其达到原始大小的 80%

管道 = 管道(步骤=[(&#39;o&#39;, 上), (&#39;u&#39;, 下)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(“重采样的类分布：”, pd.Series(y_resampled).value_counts())

这是我的代码
这是我遇到的错误
AttributeError Traceback（最近一次调用最后一次）
单元格 In[7]，第 19 行
     16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
     18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
     21 # 显示新的班级分布
     22 print(&quot;重采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372，在 Pipeline.fit_resample(self, X, y, **fit_params)
    第342章
    第343章
    第344章
   （...）
    第369章 变形的目标。
    第370章
    第371章
--&gt;第372章
    第373章
    第374章

AttributeError：“管道”对象没有属性“_check_fit_params”

我已经尝试了一切。我的所有包都已更新。我尝试使用的所有方法都在 sklearn 和 imblearn 这两个网站上查看。]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在短时间内建立准确的数据集？</title>
      <link>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</link>
      <description><![CDATA[我们正在开发一款 iOS 应用，让用户可以发送可定制的数字卡片。用户可以从各种卡片模板中进行选择，输入自己的文本，并根据自己的喜好对卡片进行编辑。我们还有一项功能，用户可以提供短信，例如“妈妈生日快乐”，并收到文本的扩展版本，例如“祝我特别的母亲生日快乐！”我爱你，希望你度过愉快的一天。”
我正在研究如何实现这一目标，并计划使用自然语言处理 (NLP) 和 CoreML 创建一个模型。然而，我在为这个特定任务寻找合适的数据集时遇到了问题。因此，我有兴趣构建专门为此目的而定制的准确数据集。但是，我不确定从哪里可以获得必要的数据，或者是否有其他数据源可供快速使用。
如果您有任何见解或替代方法来实现此功能，请分享。]]></description>
      <guid>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</guid>
      <pubDate>Thu, 02 May 2024 09:18:54 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 cv2.dnn.readNetFromCaffe() 的 net.forward() 性能，net.forward 需要更多时间（7 到 10 秒/帧）才能给出结果</title>
      <link>https://stackoverflow.com/questions/54488986/how-to-improve-performance-net-forward-of-cv2-dnn-readnetfromcaffe-net-for</link>
      <description><![CDATA[我使用了net = cv2.dnn.readNetFromCaffe(protoFile, WeightsFile)，然后循环播放实时视频帧以使用net.forward()&lt;来获取每个帧的输出&lt; /代码&gt;.
但是 net.forward() 每帧需要 7 到 10 秒才能给出结果。请帮助我如何提高性能（减少 net.forward() 中的处理时间）。
意思是：从Step1到Step2每帧需要7到10秒。
（下面的代码中提到了Step1和Step2）。

&lt;前&gt;&lt;代码&gt;导入cv2
导入时间
将 numpy 导入为 np

protoFile =“部署.prototxt”
权重文件=“iter_10.caffemodel”

宽度 = 300
高度 = 300

＃ 网络摄像头
上限 = cv2.VideoCapture(0)
hasFrame,frame = cap.read()

net = cv2.dnn.readNetFromCaffe(protoFile,weightsFile)
k = 0
而1：
    k+=1
    t = 时间.time()
    print(&quot;开始时间 = {}&quot;.format(t))
    hasFrame,frame = cap.read()

    如果没有hasFrame：
        cv2.waitKey()
        print(&quot;请稍等====&gt;&quot;)
        休息

    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),
                              (0, 0, 0)，swapRB=False，crop=False)


    net.setInput(inpBlob)

    ＃ 步骤1
    print(&quot;前进之前 = {}&quot;.format(time.time() - t))

    输出 = net.forward()

    ＃ 第2步
    #每帧花费近 7 到 10 秒
    print(&quot;forward = {}&quot;.format(time.time() - t))
]]></description>
      <guid>https://stackoverflow.com/questions/54488986/how-to-improve-performance-net-forward-of-cv2-dnn-readnetfromcaffe-net-for</guid>
      <pubDate>Sat, 02 Feb 2019 00:56:04 GMT</pubDate>
    </item>
    <item>
      <title>文本分类。 TFIDF 和朴素贝叶斯？ [关闭]</title>
      <link>https://stackoverflow.com/questions/43163959/text-classification-tfidf-and-naive-bayes</link>
      <description><![CDATA[我正在尝试执行文本分类任务，其中有大约 500 条餐厅评论的训练数据，这些评论被标记为 12 个类别。我花了比我应该花的时间来实现 TF.IDF 和余弦相似度来对测试数据进行分类，但只得到了一些非常差的结果（0.4 F-measure）。由于现在时间不在我这边，我需要实施一些更有效且没有陡峭学习曲线的东西。我正在考虑将 TF.IDF 值与朴素贝叶斯结合使用。这听起来合理吗？我知道如果我能够以正确的格式获取数据，我可以使用 Scikit learn 来做到这一点。您还有其他建议我考虑吗？]]></description>
      <guid>https://stackoverflow.com/questions/43163959/text-classification-tfidf-and-naive-bayes</guid>
      <pubDate>Sun, 02 Apr 2017 02:21:23 GMT</pubDate>
    </item>
    </channel>
</rss>