<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 18 Nov 2024 21:14:22 GMT</lastBuildDate>
    <item>
      <title>使用 LSTM 模型进行股票价格分析/预测，时间戳问题</title>
      <link>https://stackoverflow.com/questions/79201451/stock-price-analysis-prediction-with-lstm-model-timestamp-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79201451/stock-price-analysis-prediction-with-lstm-model-timestamp-problem</guid>
      <pubDate>Mon, 18 Nov 2024 21:09:06 GMT</pubDate>
    </item>
    <item>
      <title>如何修改我的代码来处理 RGBX（4 通道）图像以进行语义分割？</title>
      <link>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</link>
      <description><![CDATA[我是该领域的新手，一直在关注 U-Net 教程，使用 3 通道 RGB 图像进行语义分割https://www.youtube.com/watch?v=68HR_eyzk00&amp;list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE&amp;index=2&amp;ab_channel=DigitalSreeni，对我来说效果很好。但是，我现在需要扩展管道以支持 4 通道 RGBX 图像（即 RGB + 一个额外通道），但我不确定如何修改代码以适应额外的通道，尤其是对于预处理和 ImageDataGenerator 部分（我认为 ImageDataGenerator 不支持 4 通道图像）。
这是代码（将图像修补为（256 * 256 * 4）并将蒙版修补为（256*256）后）：
import os
import cv2
import numpy as np
import glob
from matplotlib import pyplot as plt
from patchify import patchify
import tensorflow as tf
import splitfolders
import fragmentation_models as sm
from tensorflow.keras.metrics import MeanIoU
from sklearn.preprocessing import MinMaxScaler
from keras.utils import to_categorical

input_folder=&#39;我的图像和掩码的文件夹路径&#39;

output_folder=&#39;输出文件夹的路径&#39;
#按比例分割
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75,.25),group_prefix=None) 

#重新排列用于 keras 增强的文件夹结构

seed=24
batch_size=16 
n_classes=2 

scaler=MinMaxScaler()

BACKBONE=&#39;resnet34&#39; 
preprocess_input=sm.get_preprocessing(BACKBONE)

def preprocess_data(img, mask, num_class):
#缩放图像
img=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
img=preprocess_input(img) #基于预训练的主干进行预处理
mask=to_categorical(mask, num_class)
return (img,mask)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
img_data_gen_args=dict(horizo​​ntal_flip=True, vertical_flip=True, fill_mode=&#39;reflect&#39;) #数据增强

image_datagen=ImageDataGenerator(**img_data_gen_args)
mask_datagen=ImageDataGenerator(**img_data_gen_args)

image_generator=image_datagen.flow_from_directory(train_img_path, class_mode=None, batch_size=batch_size, seed=seed)
mask_generator=image_datagen.flow_from_directory(train_mask_path, class_mode=None, color_mode=&#39;grayscale&#39;, batch_size=batch_size, seed=seed)

train_generator=zip(image_generator, mask_generator)

for (img, mask) in train_generator:
img, mask= preprocess_data(img, mask, num_class)
Yield (img, mask)

train_img_path=&#39;训练图像路径&#39;
train_mask_path=&#39;训练掩码路径&#39;
train_img_gen=trainGenerator(train_img_path, train_mask_path, num_class=2)

val_img_path=&#39;验证图像路径&#39;
val_mask_path=&#39;验证掩码路径&#39;
val_img_gen=trainGenerator(val_img_path, val_mask_path, num_class=2)

x, y=train_img_gen.__next__()

for i in range(0,3):
image=x[i]
mask=np.argmax(y[i], axis=2)
plt.subplot(1,2,1)
plt.imshow(image)
plt.subplot(1,2,2)
plt.imshow(mask, cmap=&#39;gray&#39;)
plt.show()

num_train_imgs=len(os.listdir(&#39;训练图像路径&#39;))
num_val_images=len(os.listdir(&#39;验证路径图像&#39;))
steps_per_epochs=num_train_imgs//batch_size
val_steps_per_epoch=num_val_images//batch_size

IMG_HEIGHT=x.shape[1]
IMG_WIDTH=x.shape[2]
IMG_CHANNELS=x.shape[3]

n_classes=2

model=sm.Unet(&#39;resnet34&#39;,coder_weights=&#39;None&#39;,input_shape=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),classes=n_classes,activation=&#39;softmax&#39;)
model.compile(&#39;Adam&#39;,loss=sm.losses.binary_crossentropy,metrics=[sm.metrics.iou_score,sm.metrics.FScore()])

history=model.fit(train_img_gen, steps_per_epoch=steps_per_epochs, epochs=100, verbose=1, validation_data=val_img_gen, validation_steps=val_steps_per_epoch)

]]></description>
      <guid>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</guid>
      <pubDate>Mon, 18 Nov 2024 20:06:05 GMT</pubDate>
    </item>
    <item>
      <title>标记 3D 网格数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79200777/labeling-3d-mesh-data</link>
      <description><![CDATA[背景：我有关于人体的 3D 网格数据，以未标记的 .obj 和 .stl 文件的形式存在。我想给它贴上标签（如躯干、手等），这样我就可以用它来训练我正在研究的 AI 模型。该模型的目标是通过监督深度学习来分割（实现不同的部分）。
问题：我有什么选择？人们使用一些软件、python 脚本/程序或其他巧妙的方法吗？您也可以为我提供有关该过程的详细信息吗？我看到一个数据集将 .off 视为未标记，而标记的数据集是 .seg 文件，但我不知道它们是如何从 .off 到达 .seg 的。]]></description>
      <guid>https://stackoverflow.com/questions/79200777/labeling-3d-mesh-data</guid>
      <pubDate>Mon, 18 Nov 2024 16:59:34 GMT</pubDate>
    </item>
    <item>
      <title>可以与 Ren'Py 配合使用的 NumPy 替代品？或者只是让 NumPy 与 Ren'Py 配合使用的一种方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79200623/alternative-to-numpy-that-can-work-with-renpy-or-otherwise-just-a-way-to-get-n</link>
      <description><![CDATA[我正在使用 Ren&#39;Py 进行一个涉及使用 Tensorflow 和 NLTK 进行一些自然语言处理的项目，并且在与 .rpy 脚本相同的文件夹中有一个单独的 Python 脚本，我从中导入一个/一些类，其中一个使用 Num&#39;Py。我已经查过我的问题，知道这是因为 Ren&#39;Py 不支持它，因为它不是纯 Python，但我想找到一种方法来解决这个问题或让它工作。
如果这些信息有助于提出建议，我只使用 numpy.array() 函数和 predict() 函数，没有其他任何东西。
我试过 tinynumpy 和 sympy，它产生了预期的错误。 Ren&#39;Py 的创建者自己已经对此发表了一些看法，他们推荐使用 Ren&#39;Py 的自定义版本，但我不知道该怎么做，也不知道在哪里可以找到。
我不想把自己局限在一种答案中，但由于所有 numpy 替代方案都不起作用，我最后的选择是使用 Ren&#39;Py 的自定义版本，但这完全超出了我的知识范围。]]></description>
      <guid>https://stackoverflow.com/questions/79200623/alternative-to-numpy-that-can-work-with-renpy-or-otherwise-just-a-way-to-get-n</guid>
      <pubDate>Mon, 18 Nov 2024 16:03:46 GMT</pubDate>
    </item>
    <item>
      <title>如何使用用户定义的类别在 iOS 上动态地重新训练 YOLO 模型，同时保留以前的知识？</title>
      <link>https://stackoverflow.com/questions/79200388/how-to-dynamically-retrain-a-yolo-model-on-ios-with-user-defined-classes-while-p</link>
      <description><![CDATA[我正在开发一款 iOS 应用，该应用使用 Vision Kit 和 YOLO（例如 YOLOv4-Tiny）模型来检测卡片上的数据和图像。有时，应用会遇到模型无法识别的新图标。我想要：

允许用户为未知图标提供标签。
使用这些新数据重新训练现有的 YOLO 模型。
确保保留以前训练过的类别（避免灾难性遗忘）。
将更新后的模型无缝部署回 iOS 应用。

要求：
应用应支持在设备上或通过使用后端进行轻量级再训练。
再训练过程应将新类别与现有模型的知识合并。
该解决方案必须在 iPhone 等移动硬件上有效运行。
我目前的方法：

我正在使用转换为 iOS 版 Core ML 的预训练 YOLOv4-Tiny 模型。
Vision Kit 负责处理图像预处理和检测。

挑战：
如何将新用户提供的数据集成到训练管道中？
如何在 iOS 或后端服务器上高效地重新训练 YOLO，同时保留现有类别？
将更新的模型部署到应用程序的最佳实践。
我正在寻找以下方面的指导：

用于设备端或服务器端 YOLO 再训练的工具或库。

用于 YOLO 模型的增量学习或知识提炼的方法。

如何处理将再训练的模型部署到 iOS 设备。

]]></description>
      <guid>https://stackoverflow.com/questions/79200388/how-to-dynamically-retrain-a-yolo-model-on-ios-with-user-defined-classes-while-p</guid>
      <pubDate>Mon, 18 Nov 2024 14:49:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 预处理层构建预处理管道时，adapt() 在 tf.data.Dataset.map() 中不起作用</title>
      <link>https://stackoverflow.com/questions/79199946/adapt-doesnt-work-within-tf-data-dataset-map-while-building-a-preprocessing</link>
      <description><![CDATA[我正在尝试创建一个神经网络，根据员工的技能为他们分配任务（https://www.kaggle.com/datasets/umerfarooq09/skill-based-task-assignment）。
数据包含分类特征。我在预处理函数中使用 Keras 预处理层。数据不是太大，所以我选择根据特征调整预处理层来构建词汇表（而不是使用预先构建的词汇表）。但是每当我尝试对某个特征使用 adapt() 时，它都会抛出以下错误：
dataset = tf.data.experimental.make_csv_dataset(csv_path, 
label_name=label, 
batch_size=128, 
num_epochs=1, 
shuffle=True, 
shuffle_buffer_size=1000)

def preprocess(features, label):
preprocessed_features = {}
for name, feature in features.items():
if feature.dtype == &#39;string&#39;:
coder = layer.StringLookup(output_mode=&#39;one_hot&#39;)
coder.adapt(feature)
preprocessed_features[name] =coder(feature)

elif feature.dtype == &#39;int&#39;:
normalizer = layer.Normalization(output_mode=&#39;one_hot&#39;)
normalizer.adapt(feature)
preprocessed_features[name] = normalizer(feature)

return preprocessed_features, label

dataset = dataset.map(preprocess)

OperatorNotAllowedInGraphError: 在用户代码中:

文件 &quot;C:\Users\User\AppData\Local\Temp\4\ipykernel_7508\1668558205.py&quot;，第 8 行，在 preprocess 中 *
coder.adapt(feat[name])
文件 &quot;c:\Users\User\Spark\spark_env\Lib\site-packages\keras\src\layers\preprocessing\string_lookup.py&quot;，第 368 行，在 adapt 中 **
super().adapt(data, steps=steps)
文件&quot;c:\Users\User\Spark\spark_env\Lib\site-packages\keras\src\layers\preprocessing\index_lookup.py&quot;，第 582 行，在 adapt
self.finalize_state()
文件 &quot;c:\Users\User\Spark\spark_env\Lib\site-packages\keras\src\layers\preprocessing\index_lookup.py&quot;，第 626 行，在 finalize_state
if self._has_input_vocabulary or tf.equal(self.token_counts.size(), 0):

OperatorNotAllowedInGraphError：不允许将符号 `tf.Tensor` 用作 Python `bool`。您可以尝试以下方法解决该问题：如果您在 Graph 模式下运行，请使用 Eager 执行模式或使用 @tf.function 修饰此函数。如果您使用的是 AutoGraph，则可以尝试使用 @tf.function 修饰此函数。如果此方法无效，则可能是您使用了不受支持的功能，或者 AutoGraph 可能无法看到您的源代码。有关更多信息，请参阅 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code。

我认为问题是由于 adapt() 在符号张量上实现而导致的，该张量不包含任何数据。所以我获取了一批数据，并将特征和标签保存在内存中以传递给 adapt()。
[(feat, label)] = dataset.take(1)

def preprocess(features, label):
preprocessed_features = {}
for name, feature in features.items():
if feature.dtype == &#39;string&#39;:
coder = layer.StringLookup(output_mode=&#39;one_hot&#39;)
coder.adapt(feat[name])
preprocessed_features[name] =coder(feature)

elif feature.dtype == &#39;int&#39;:
normalizer = layer.Normalization(output_mode=&#39;one_hot&#39;)
normalizer.adapt(feat[name])
preprocessed_features[name] = normalizer(feature)

return preprocessed_features, label

dataset.map(preprocess)

此代码仍然产生相同的结果错误。我做错了什么？
既然我们这样做了，那么，如果数据太大而无法保存在内存中（我的不是，但我将来可能会遇到这种情况），构建可以在批量数据上异步运行的输入和预处理管道的最佳方法是什么（最好不使用 pandas）？
我计划将预处理模型和训练模型彼此分开（因为 tf 文档称这是预处理分类特征的最佳方法）。]]></description>
      <guid>https://stackoverflow.com/questions/79199946/adapt-doesnt-work-within-tf-data-dataset-map-while-building-a-preprocessing</guid>
      <pubDate>Mon, 18 Nov 2024 12:40:20 GMT</pubDate>
    </item>
    <item>
      <title>为 TensorFlow 创建 TF 记录时遇到问题</title>
      <link>https://stackoverflow.com/questions/79199941/facing-issue-in-creating-tf-records-for-tensorflow</link>
      <description><![CDATA[当我运行此代码来生成 TF 记录时：
!python {SCRIPTS_PATH + &#39;/generate_tfrecord.py&#39;} -x {IMAGE_PATH + &#39;/train&#39;} -l {ANNOTATION_PATH + &#39;/label_map.pbtxt&#39;} -o {ANNOTATION_PATH + &#39;/train.record&#39;}!python {SCRIPTS_PATH + &#39;/generate_tfrecord.py&#39;} -x {IMAGE_PATH + &#39;/test&#39;} -l {ANNOTATION_PATH + &#39;/label_map.pbtxt&#39;} -o {ANNOTATION_PATH + &#39;/test.record&#39;}

我收到此错误：
2024-11-16 16:28:22.414068：我tensorflow/core/util/port.cc:153] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。
2024-11-16 16:28:23.793272：I tensorflow/core/util/port.cc:153] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。
回溯（最近一次调用最后一次）：
文件“C:\Users\rishi\RealTimeObjectDetection\Tensorflow\scripts\generate_tfrecord.py”，第 62 行，位于 &lt;module&gt;
label_map_dict = label_map_util.get_label_map_dict(label_map)
文件 &quot;C:\Users\rishi\anaconda3\lib\site-packages\object_detection\utils\label_map_util.py&quot;，第 166 行，在 get_label_map_dict 中
label_map = load_labelmap(label_map_path)
文件 &quot;C:\Users\rishi\anaconda3\lib\site-packages\object_detection\utils\label_map_util.py&quot;，第 135 行，在 load_labelmap 中
label_map_string = fid.read()
文件 &quot;C:\Users\rishi\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;，第 116 行，在 read 中
self._preread_check()
文件&quot;C:\Users\rishi\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;，第 77 行，在 _preread_check
self._read_buf = _pywrap_file_io.BufferedInputStream(
TypeError: __init__(): 不兼容的构造函数参数。支持以下参数类型：
1. tensorflow.python.lib.io._pywrap_file_io.BufferedInputStream(filename: str, buffer_size: int, token: tensorflow.python.lib.io._pywrap_file_io.TransactionToken = None)

调用方式：item {
name: &quot;Hello&quot;
id: 1
}
item {
name: &quot;Yes&quot;
id: 2
}
item {
name: &quot;No&quot;
id: 3
}
item {
name: &quot;ThankYou&quot;
id: 4
}
item {
name: &quot;ILoveYou&quot;
id: 5
}
, 524288

我已检查所有路径以及 label_map 文件（.pbtxt 文件）及其格式，但似乎没有任何效果。
此外，在生成 tf 记录代码中，import tensorflow.compat.v1 as tf 语句不知何故显示错误。即使多次下载 python 和 tensorflow，它仍显示没有名为 compat 的模块。这可能与此有关，但我不确定。
请提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/79199941/facing-issue-in-creating-tf-records-for-tensorflow</guid>
      <pubDate>Mon, 18 Nov 2024 12:38:56 GMT</pubDate>
    </item>
    <item>
      <title>SAM 2.1 是什么导致 hydra.errors.MissingConfigException：未找到主配置模块“sam2”？</title>
      <link>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</link>
      <description><![CDATA[我正在尝试使用此处给出的 roboflow 指南微调新的 SAM 2.1 分割模型：Sam 2.1 roboflow 指南
使用 google collab 时，此代码运行正常，没有遇到任何错误。当我在本地机器上运行完全相同的代码时，运行训练代码命令时会出现以下错误：
!python training/train.py -c &#39;configs/train.yaml&#39; --use-cluster 0 --num-gpus 1
在 Windows 10 上使用 vscode 运行时出现以下错误：
hydra.errors.MissingConfigException：未找到主配置模块“sam2”。
检查它是否正确并包含 __init__.py 文件

我的工作目录：
C:\..\SAM_2_1\sam2

]]></description>
      <guid>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</guid>
      <pubDate>Mon, 18 Nov 2024 11:00:31 GMT</pubDate>
    </item>
    <item>
      <title>句子转换器中的降维</title>
      <link>https://stackoverflow.com/questions/79199199/dimensionality-reduction-in-sentence-transformers</link>
      <description><![CDATA[我需要在预处理中计算大量句子（比如 10K）的嵌入，并且在运行时我必须一次计算一个句子的嵌入向量（用户查询），然后根据嵌入向量找到最相似的句子（使用余弦相似度）。
我目前正在使用句子转换器，它们的输出大小为 768，这对于我的情况来说太大了。所以我想尝试更小的尺寸，比如 256 甚至 128。
我熟悉 PCA 和量化。然而，ChatGPT 和 Gemini 都建议我在池化层之后添加一个密集层。示例：
dense = models.Dense(in_features=base_model.get_sentence_embedding_dimension(), out_features=256)
model = SentenceTransformer(modules=[base_model, density])

我的问题是，我认为我必须重新训练/微调我的模型，但由于我没有标记数据，我无法做到这一点。但 ChatGPT 和 Gemini 声称我可以不用重新训练或微调就完成这个实现，尽管“这样会更好”。
我很困惑这怎么可能行得通，因为如果不进行训练，密集层中的初始权重将是随机的。
我是否遗漏了什么，或者添加密集层而不进行重新训练/微调是否真的可行？]]></description>
      <guid>https://stackoverflow.com/questions/79199199/dimensionality-reduction-in-sentence-transformers</guid>
      <pubDate>Mon, 18 Nov 2024 08:31:21 GMT</pubDate>
    </item>
    <item>
      <title>有条件地更改 RGB 图像中的像素值，然后删除通道</title>
      <link>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</link>
      <description><![CDATA[我只想比较图像的像素，如果这个像素是粉红色（R 值 = 0.502、G 值 = 0.0、B 值 = 0.502）则将其更改为黑色，否则将其更改为白色。在此之后，我想删除通道，只得到一个 (512,512,) 形状的张量。
此 getitem 位于我的数据集类中。
代码：
 def __getitem__(self, index):
img = Image.open(self.images[index]).convert(&quot;RGB&quot;) # 这是一张图像
mask = Image.open(self.masks[index]).convert(&quot;RGB&quot;) # 这是相应的掩码
mask = self.transform_tensor(mask) # 张量为 (3,512,512) 形状
mask = torch.where((mask[0, :, :] == 0.502) &amp; (mask[1, :, :] == 0.0) &amp; (mask[2, :, :] == 0.502), torch.tensor([0.0, 0.0, 0.0]), torch.tensor([1.0, 1.0, 1.0])) # 错误
mask = self.transform_to_image(mask)
mask.show()
return self.transform_image(img), self.transform_mask(mask)


RuntimeError: 张量 a (512) 的大小必须与非单例维度 1 上的张量 b (3) 的大小匹配
错误：意外类型：(bool, Tensor, Tensor)]]></description>
      <guid>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</guid>
      <pubDate>Sun, 17 Nov 2024 10:36:45 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类分割模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>无法检测/删除图像数据集中两个位置不同的水印</title>
      <link>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</link>
      <description><![CDATA[我在从一组图片中删除水印时遇到了问题。这些水印彼此靠近，但又有所不同（见下文）。其中一个水印是红色方块，里面有白色文字。另一个是半透明的灰色句子。目的是处理图像以用于机器学习。
解决问题的尝试：
由于水印在图像数据集中的位置各不相同，我尝试了以下操作：

复制图像并将其转换为 HSV 颜色空间
为感兴趣的区域选择一系列下限值和上限值（在分割图像并为每个通道构建直方图后选择这些值）
使用 cv2.inRange() 函数构建蒙版
使用蒙版在原始图像中绘制水印

对于红色方块，前三个步骤非常完美。但第三步只是有点奏效。水印比以前明显不那么明显了，但仍然很明显。对于文本，我无法在第 3 步中获得足够好的蒙版 - 它在颜色/像素强度上与周围区域和文本本身太接近了。
这看起来更像是一个机器学习问题，这很好，但我想事先用尽其他选择。关于如何使用机器学习或算法方法解决这个问题有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</guid>
      <pubDate>Thu, 11 Jul 2024 16:54:13 GMT</pubDate>
    </item>
    <item>
      <title>计算逐字符混淆矩阵以进行 OCR 评估？</title>
      <link>https://stackoverflow.com/questions/77907561/calculate-a-character-wise-confusion-matrix-for-ocr-evaluation</link>
      <description><![CDATA[我正在从事 OCR 任务，出于评估目的，我想为我的模型计算一个混淆矩阵。我希望它基本上显示一个字符被正确预测的频率以及它被预测为其他字符的频率（以及哪些字符！）。
我目前的问题是，由于字符串大小不匹配和/或多余/缺失字符（主要是空格），简单的成对比较很困难。我正在考虑使用 Levenshtein 距离计算算法添加有关需要插入/删除字符的频率的信息，但我仍然不确定如何处理。
是否有任何常用的最先进的方法？我做了一些研究，但没有发现任何重要的东西。]]></description>
      <guid>https://stackoverflow.com/questions/77907561/calculate-a-character-wise-confusion-matrix-for-ocr-evaluation</guid>
      <pubDate>Tue, 30 Jan 2024 15:38:07 GMT</pubDate>
    </item>
    <item>
      <title>用于异常检测的孤立森林</title>
      <link>https://stackoverflow.com/questions/60209411/isolation-forest-for-anomaly-detection</link>
      <description><![CDATA[在此 示例 中，IsolationForest 用于异常检测：
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

rng = np.random.RandomState(42)

# 生成训练数据
X = 0.3 * rng.randn(100, 2)
X_train = np.r_[X + 2, X - 2]
# 生成一些常规的新观察值
X = 0.3 * rng.randn(20, 2)
X_test = np.r_[X + 2, X - 2]
# 生成一些异常的新观察值
X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))

# 拟合模型
clf = IsolationForest(max_samples=100, random_state=rng)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
y_pred_outliers = clf.predict(X_outliers)

我相信此代码中的异常值是随机引入的。
但是，如果我使用真实数据进行异常检测，那么：

我该如何推进？

如果我已经有数据集，如何识别异常？
我正在尝试使用联合循环发电厂数据集。
或者，如果您有任何其他好的异常检测实践数据集，请提供一些链接！

]]></description>
      <guid>https://stackoverflow.com/questions/60209411/isolation-forest-for-anomaly-detection</guid>
      <pubDate>Thu, 13 Feb 2020 13:49:38 GMT</pubDate>
    </item>
    <item>
      <title>神经网络是一种懒惰的学习方法还是积极学习的方法？</title>
      <link>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</link>
      <description><![CDATA[神经网络是一种懒惰的还是积极学习的方法？不同的网页有不同的说法，所以我想得到一个可靠的答案，并有好的文献来支持它。最明显的书是米切尔著名的《机器学习》一书，但浏览整本书我找不到答案。谢谢 :)。]]></description>
      <guid>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</guid>
      <pubDate>Thu, 21 Apr 2011 21:16:25 GMT</pubDate>
    </item>
    </channel>
</rss>