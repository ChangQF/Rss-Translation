<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 12 Sep 2024 03:18:11 GMT</lastBuildDate>
    <item>
      <title>测量肩角</title>
      <link>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</link>
      <description><![CDATA[在此处输入图片说明
我必须用 Python 测量每个肩膀的角度。以最高精度测量的最佳方法是什么？我对此很陌生，所以欢迎提出任何建议。
我尝试了 Media pipe，但它只提供骨架点。我需要测量肩部边缘的角度。（如图所示）]]></description>
      <guid>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</guid>
      <pubDate>Thu, 12 Sep 2024 03:05:15 GMT</pubDate>
    </item>
    <item>
      <title>尽管分类报告很好，但模型无法正确预测</title>
      <link>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</link>
      <description><![CDATA[我尝试从链接运行此模型：
https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook
当我在 colab 上使用类似数据集（但较小，2100 张图片到 6 个类）执行此操作时，效果很好。但是当我添加此代码来预测输入图像时：
from google.colab import files
from PIL import Image

def process_uploaded_image(image_path, target_size=(224, 224)):
img = Image.open(image_path)
img = img.resize(target_size) 
img_array = np.array(img) 

if img_array.shape[-1] == 4: 
img_array = img_array[..., :3]

img_array = img_array / 255.0 
img_array = np.expand_dims(img_array, axis=0) 
img_array = mobilenetv2.preprocess_input(img_array) 

return img_array

uploaded = files.upload()

for fn in uploaded.keys(): 
processed_image = process_uploaded_image(fn, target_size=IMAGE_SIZE) 
preds = model.predict(processed_image)
pred_class = np.argmax(preds, axis=1)

plt.imshow(Image.open(fn)) # 显示上传的图片
plt.title(f&#39;预测的类别：{categories[pred_class[0]]}&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(f&#39;文件 {fn} 被预测为：{categories[pred_class[0]]}&#39;)

结果是错误的预测。例如，模型总是将我的输入预测为“垃圾”类。当我停止运行时，它会更改为另一个类，但它仍然处于错误的预测中。
我还添加了此代码来检查预测概率：
preds = model.predict(processed_image)
pred_probs = preds[0] # 获取第一个（也是唯一一个）批次的预测概率
print(&quot;Prediction probabilities:&quot;, pred_probs)
pred_class = np.argmax(pred_probs)
print(&quot;Predicted class:&quot;, categories[pred_class])

输出：
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step 预测概率：[0.31027108 0.12315894 0.47848797 0.00863316 0.07789086 0.00155797]
预测类别：金属
我不知道为什么会发生这种情况。
我希望我的模型可以正确预测结果]]></description>
      <guid>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</guid>
      <pubDate>Thu, 12 Sep 2024 02:58:40 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure 中创建数据资产但出现此错误</title>
      <link>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</link>
      <description><![CDATA[我正在尝试创建 ML 数据资产，我正在关注此链接：https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html
以完成此操作，但我不断收到此错误

我知道我的文件与示例中的文件不完全相同，但即使我使用这些文件，我也会收到相同的错误。所以我不确定问题是什么，因为这是我第一次在 Azure 中使用 ML。]]></description>
      <guid>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</guid>
      <pubDate>Thu, 12 Sep 2024 00:30:32 GMT</pubDate>
    </item>
    <item>
      <title>排列检验的准确率非常高[关闭]</title>
      <link>https://stackoverflow.com/questions/78975982/accuracy-for-permutation-test-is-very-high</link>
      <description><![CDATA[我对我的分类器发生了什么有点困惑。
我有一个包含约 220 个特征和大约 4000 次试验的数据集。类别是完美平衡的，我正在使用具有 L1 范数的 SVC 执行一个简单的二元分类任务。
当我执行 LOOCV 时，我获得了不错的准确度，但是当我想检查置换数据时，我也会获得相同的 CV 准确度 +/- 2%。
这感觉很奇怪，因为当我使用随机数据但保留基本结构（220 X 4000）运行模拟时，我的准确度并没有明显高于 50%。对我来说，这意味着过度拟合不会自动发生，但它让我很困惑为什么我会得到这些奇怪的结果。
我遗漏了什么可以解释这一点吗？
初始测试代码：
n_samples = len(df)

reg_list = [.1]
for c in reg_list:
# 创建合成组标签（每个样本都是自己的组）
groups = df[&#39;run_value&#39;]
cv = LeaveOneOut()
svc = LinearSVC(penalty=&quot;l1&quot;, C=c, dual=False) # 使用 LinearSVC 的“l1”惩罚时 dual=False

decoder = Decoder(
estimator=svc,
mask=ffa_mask,
standardize=&quot;zscore_sample&quot;,
cv=cv,
评分=“accuracy”，
)

# 拟合解码器
decoder.fit(df[0].values, df[&#39;0_y&#39;].values)

# 输出结果
print(f“C={c}”)
# 访问每个类的交叉验证分数
cv_scores_class_0 = 解码器.cv_scores_[0]
cv_scores_class_1 = 解码器.cv_scores_[1]

# 计算两个类的平均交叉验证准确率
mean_score_class_0 = np.mean(cv_scores_class_0)
mean_score_class_1 = np.mean(cv_scores_class_1)
mean_score = np.mean([mean_score_class_0, mean_score_class_1])
print(f“类的平均 CV 分数0: {mean_score_class_0}&quot;)
print(f&quot;类别 1 的平均 CV 得分：{mean_score_class_1}&quot;)
print(f&quot;平均 CV 得分：{mean_score}&quot;)
# print(f&quot;C={c} 的权重：{svc.coef_}&quot;)

和排列：
n_samples = len(df)

reg_list = [.1]
for c in reg_list:
# 创建合成组标签（每个样本都是自己的组）
groups = df[&#39;run_value&#39;]
cv = LeaveOneOut()
svc = LinearSVC(penalty=&quot;l1&quot;, C=c, dual=False) # 使用 LinearSVC 的“l1”惩罚时 dual=False

decoder =解码器（
estimator=svc,
mask=ffa_mask,
standardize=&quot;zscore_sample&quot;,
cv=cv,
scoring=&quot;accuracy&quot;,
)

# 拟合解码器
decoder.fit(df[0].values, shuffle(df[&#39;0_y&#39;].values))

# 输出结果
print(f&quot;C={c}&quot;)
# 访问每个类的交叉验证分数
cv_scores_class_0 = 解码器.cv_scores_[0]
cv_scores_class_1 = 解码器.cv_scores_[1]

# 计算两个类的平均交叉验证准确率
mean_score_class_0 = np.mean(cv_scores_class_0)
mean_score_class_1 = np.mean(cv_scores_class_1)
mean_score = np.mean([mean_score_class_0, mean_score_class_1])
print(f&quot;类别 0 的平均 CV 得分：{mean_score_class_0}&quot;)
print(f&quot;类别 1 的平均 CV 得分：{mean_score_class_1}&quot;)
print(f&quot;平均 CV 得分：{mean_score}&quot;)
# print(f&quot;C={c} 的权重：{svc.coef_}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78975982/accuracy-for-permutation-test-is-very-high</guid>
      <pubDate>Wed, 11 Sep 2024 23:33:01 GMT</pubDate>
    </item>
    <item>
      <title>从测量位置数据集中分离系统误差和随机性</title>
      <link>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</link>
      <description><![CDATA[我正在尝试找出一种方法，将系统误差从一组数据集中分离出来，这些数据集表示机械平台的位置和位置误差。数据在 pandas 数据框中。
背景：平台在 2D 平面中移动。测量报告平台位置的 X 和 Y 坐标以及 X 和 Y 方向的位置误差。2D 平面上有一个相距 1nm 的标记网格，平台平稳移动到这些标记，并且仅在这些标记处进行测量。通过算法减去这些标记的真实位置和测量的平台位置来计算误差。
数据格式：这里的倾斜表示误差。



StageCoords_X
StageCoords_Y
Skew_X
Skew_Y




118760606
112836409
-29
-45


118760622
112836426
-18
5



**数据预处理：**我正在使用 nm 尺度，每次我扫描相同的“标记”位置时，测量的舞台位置都会在这些“标记”位置周围略有不同。这就是为什么我想在 X 和 Y 方向上对数据进行分类，以便每个矩形箱（结合 X 和 Y 轴箱宽度）将覆盖用于特定标记的数据点。我可以计算每个箱的平均和峰峰值误差并绘制它们。
# 根据箱宽度定义箱边界
bin_width = 1000000
x_bins = np.arange(df[&#39;StageCoordsNM.X&#39;].min(), df[&#39;StageCoordsNM.X&#39;].max() + bin_width, bin_width)
y_bins = np.arange(df[&#39;StageCoordsNM.Y&#39;].min(), df[&#39;StageCoordsNM.Y&#39;].max() + bin_width, bin_width)

df[&#39;X_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.X&#39;], bins=x_bins, labels=False)
df[&#39;Y_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.Y&#39;], bins=y_bins, labels=False)

我想要做什么：现在假设，我已经将舞台扫过同一区域 25 次。在 x 和 y 方向上进行分箱后，每个箱将有 25 个数据点，意味着每个箱有 25 个测量舞台位置误差。现在对于每个箱，我想提取误差的系统或可重复部分。每个箱 25 个数据点可能不足以得出任何可靠的结论。因此使用机器学习很困难。我想找出一种更具统计性的方法来做到这一点。
我到目前为止所做的：计算了“归一化加权调整重复性指数”。这应该表明我对特定分箱误差是否可重复的信心。忽略 &#39;_before&#39; 下标。
#*__Pk-to-Pk X 和 Y__
df[&#39;Error_pk2pk_X&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.max() - x.min())
df[&#39;Error_pk2pk_Y&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.max() - x.min())

df[&#39;Mean_SkewNM_X_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.mean())
df[&#39;Mean_SkewNM_Y_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.mean())

def compute_confidence_X_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_x_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) + epsilon) )
group[&#39;NWARI_x_before&#39;] = (group[&#39;WARI_x_before&#39;]) / (1+group[&#39;WARI_x_before&#39;])

group[&#39;Confidence_X_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) ) / np.abs(group[&#39;SkewNM.X&#39;].mean())
group[&#39;Confidence_X_before&#39;] = group[&#39;Confidence_X_before&#39;].apply(lambda x: max(x, -1))
return group

def compute_confidence_Y_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_y_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) + epsilon) )
group[&#39;NWARI_y_before&#39;] = (group[&#39;WARI_y_before&#39;]) / (1+group[&#39;WARI_y_before&#39;])

group[&#39;Confidence_Y_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) ) / np.abs(group[&#39;SkewNM.Y&#39;].mean())
group[&#39;Confidence_Y_before&#39;] = group[&#39;Confidence_Y_before&#39;].apply(lambda x: max(x, -1))
返回组

df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_X_before)
df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_Y_before)
df[&#39;NWARI_X_before&#39;] = df[&#39;NWARI_x_before&#39;]
df[&#39;NWARI_Y_before&#39;] = df[&#39;NWARI_y_before&#39;]

但我不确定这是否是正确的方法。或者是否有其他方法可以验证这一点。
一些建议会有所帮助。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</guid>
      <pubDate>Wed, 11 Sep 2024 22:23:55 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林机器学习模型整合连续数据和静态数据？</title>
      <link>https://stackoverflow.com/questions/78975841/how-can-i-integrate-continuous-data-and-static-data-using-random-forest-machine</link>
      <description><![CDATA[我正在使用随机森林回归模型来预测地下水位变化。我使用连续输入（时间序列数据），例如 GRACE、降水量、最高温度、最低温度、NDVI，以及静态数据，例如陆地海拔、水力传导率、坡度、沙粒百分比。当我将静态输入添加到连续输入时，模型高度重视静态输入并忽略连续输入。我该如何解决这个问题。
我得到了一个预测，但在特征重要性方面存在问题，模型高度重视静态输入，而不重视连续输入，这是错误的。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78975841/how-can-i-integrate-continuous-data-and-static-data-using-random-forest-machine</guid>
      <pubDate>Wed, 11 Sep 2024 22:07:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用并行 torch cuda 流而不导致 oom？（包括示例）</title>
      <link>https://stackoverflow.com/questions/78975818/how-to-use-parallel-torch-cuda-streams-without-causing-oom-example-included</link>
      <description><![CDATA[我将大量张量数据存储在 CPU 内存中，预期的工作流程是使用 GPU 来处理它们。在处理一个块时，同时将前一个块的结果传输回 CPU。并且同时将下一个块传输到 GPU，以便它准备好进行处理，而 GPU 不必等待同步传输。
为此，我使用默认的 cuda 流进行处理，并使用两个额外的流进行并行异步传输。但是，在实际应用中，每当我使用 s2 流将张量复制到 GPU（而不是默认流）时，它确实会提高速度！但总是导致 GPU 内存快速稳定地上升，直到溢出。
我确实尝试重现这种行为，但无法，我的示例似乎不会导致内存问题。但是，我相信我可能仍然以某种方式错误地使用了流。所以我希望，如果以前有人使用过流，就能发现这个错误？在当前示例中，s2 等待 s1，但我在实际应用中尝试的任何组合都会失败。
import torch
from time import perf_counter
import sys
from threading import Thread

cpu = torch.device(&#39;cpu&#39;)
gpu = torch.device(&#39;cuda&#39;)

_range = range(10)
tensors = [torch.rand(100000000, device=cpu) for i in _range]

s1 = torch.cuda.Stream(device=gpu)
s2 = torch.cuda.Stream(device=gpu)

for i in range(10000000000):
time_start = perf_counter()

for j in range(-1,11):
def PROCESS():
if j in _range:
k = tensors[j]
for l in range(10):
k.mul_(1.01)
k.add_(1.01)
k.pow_(0.5)

def CPU():
if j-1 in _range:
with torch.cuda.stream(s1):
p = tensors[j-1]
p.record_stream(s1)
p.data = p.data.to(device=cpu, memory_format=torch.preserve_format, non_blocking=True)

def GPU():
if j+1 in _range:
# 在实际应用中使用第二个流会导致 oom
with torch.cuda.stream(s2): 
g = tensors[j+1]
g.data = g.data.to(device=gpu, memory_format=torch.preserve_format, non_blocking=True)
g.record_stream(s2)
# 注意：对于 s2，记录流在分配，
# 因为它最初在 CPU 上，而我无法在 CPU 张量上记录

t2 = Thread(target=CPU)
t2.start()
t3 = Thread(target=GPU)
t3.start()
t1 = Thread(target=PROCESS)
t1.start()

s1.wait_stream(torch.cuda.default_stream(gpu))
s2.wait_stream(s1)
s1.synchronize()

t2.join()
t3.join()
t1.join()

lapsed = perf_counter() - time_start
time_duration = &quot;%.5f sec/it&quot; % lapsed

print(f&quot;\rspeed: {time_duration}&quot;,end=&quot;\r&quot;)

您可以直接运行示例并亲自查看结果，使用流 2 时，它确实运行得更快，如果注释掉该部分，它会运行得更慢。在实际应用中，它实际上要复杂得多，因为它使用了数千个张量，并且循环并不直接，但是使用 s2 流总是会导致 oom。]]></description>
      <guid>https://stackoverflow.com/questions/78975818/how-to-use-parallel-torch-cuda-streams-without-causing-oom-example-included</guid>
      <pubDate>Wed, 11 Sep 2024 21:55:42 GMT</pubDate>
    </item>
    <item>
      <title>Ray 自定义环境渲染</title>
      <link>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</link>
      <description><![CDATA[我正在创建自己的 gym 环境来测试 freeze-tag 问题。我正在尝试使用 Ray 来做 MAPPO。我有两个问题：
1：我的模拟没有渲染
2：它创建了多个 PyGame 窗口
我已将渲染方法和训练脚本的片段附加到附件中。
# 渲染函数
def render(self):
self.screen.fill((255, 255, 255))

for agent in self.all_agents:
if agent.status == 1:
pygame.draw.circle(self.screen, agent.color, (agent.x, agent.y), agent.size)

elif agent.status == 0:
pygame.draw.circle(self.screen, (0, 255, 255), (agent.x, agent.y),agent.size)

pygame.display.flip()

# Train_MAPPO_FTP.py
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
import gym_FTP as e
import pygame
import numpy as np

# 环境创建函数
def env_creator(config):
robots = 5
adversaries = 2
time_steps = 500

screen = pygame.display.set_mode([1000, 1000])
gym_ftp = e.gym_FTP(screen, robots, 0, adversaries, time_steps, 15)
return gym_ftp

def train_and_evaluate(time_steps):
# 初始化 Ray
ray.init(ignore_reinit_error=True)

# 注册环境
register_env(&quot;Env_FTP&quot;, env_creator)

# create_env_on_local_worker = True
# 配置算法
config = PPOConfig() \
.environment(&quot;Env_FTP&quot;) \
.rollouts(num_rollout_workers=1,
rollout_fragment_length=1,
create_env_on_local_worker=True) \
.training(
train_batch_size=1, # 每次训练更新前汇总经验
sgd_minibatch_size=1,
model={&quot;fcnet_hiddens&quot;: [64, 64]}
) \
.framework(&quot;torch&quot;) \
.evaluation(evaluation_num_workers=1) \
.resources(num_gpus=0) # 设置 GPU 数量

# 构建算法
algo = config.build()

# 参数
episodes = 5
iterations = time_steps / 10

for episode in range(episodes):
for i in range(int(iterations)):
results = algo.train()
print(f&quot;训练迭代 {i + 1} 已完成。mean_reward {results[&#39;episode_reward_mean&#39;]},&quot;
f&quot; 总损失 {results[&#39;info&#39;][&#39;learner&#39;][&#39;__all__&#39;][&#39;total_loss&#39;]}&quot;)

# 关闭 Ray
ray.shutdown()

def main():
time_steps = 500
train_and_evaluate(time_steps)

main()


我已进行多次检查，以测试我的代理的速度是否根据新操作进行更新，以及位置是否正在更新，因此我确定这不是问题所在。当我使用其他算法进行测试时，此环境也有效。我可以正确使用 gym 环境的其他功能，并让它渲染和做一些有趣的事情。这似乎完全是 RAY 的问题。我的目标是拥有 n 个机器人和 m 个对手。我想根据环境状态为 n 个代理获取新操作。我想每集训练 500 个时间步，收集 10 个批次。例如前 10 个时间步，然后再添加 10 个时间步作为经验，然后再添加 10 个。所以我们每集最多更新 50 次。我们将进行 100 集。]]></description>
      <guid>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</guid>
      <pubDate>Wed, 11 Sep 2024 20:54:21 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确的结果</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>FFN 模型在预测总和方面实现了 100% 的准确率</title>
      <link>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</link>
      <description><![CDATA[我有一个模型，可以对 -10 到正 10 之间的数字进行加法运算，但使用神经网络通过两个数字相加的数据集来预测结果。然而，在获得训练准确度时，它只是打印出很多 100% 的准确度。我不确定模型是否只是快速训练，或者是否存在问题并且没有正确学习。有人能提供一些见解吗？
这是我的代码
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)

data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
x = np.random.randint(-10, 10)
y = np.random.randint(-10,10)
bothNumber = [x,y]
data.append(bothNumber)
labels.append(x+y)

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

def createModel():
class myModel(nn.Module):
def __init__(self):
super().__init__()

self.input = nn.Linear(2,8)
self.fc1 = nn.Linear(8,8)
self.output = nn.Linear(8,1)

def forward(self,x):
x = F.relu( self.input(x) )
x = F.relu( self.fc1(x) )
return self.output(x)

net = myModel()
lossfun = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(),lr=.001)

return net,lossfun,optimizer

def trainModel():

numepochs = 100
net,lossfun,optimizer = createModel()
loss = torch.zeros(numepochs)
trainacc = []
testacc = []

for epochi in range(numepochs):
batchLoss = []

for X,y in train_loader:
X = X.float()
y = y.float()
yHat = net(X)

loss = lossfun(yHat,y)
batchLoss.append(loss.item())

optimizer.zero_grad()
loss.backward()
optimizer.step()

loss[epochi] = np.mean(batchLoss)

with torch.no_grad():
train_predictions = []
train_labels = []
for x_train, y_train in train_loader:
x_train = x_train.float()
y_train = y_train.float()
train_pred = net(x_train)
train_predictions.append(train_pred)
train_labels.append(y_train)

train_predictions = torch.cat(train_predictions)
train_labels = torch.cat(train_labels)

train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
trainacc.append(train_acc.item())

X,y = next(iter(test_data))
X = X.float() # 将 X 转换为浮点数用于测试数据
y = y.float() # 将 y 转换为浮点数用于测试数据
with torch.no_grad():
yHat = net(X)

testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

return trainacc,testacc,losses,net

trainAcc, testAcc, loss , net = trainModel()


模型有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</guid>
      <pubDate>Wed, 11 Sep 2024 18:43:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的神经网络模型无法学习绝对函数 abs(x1-x2)？</title>
      <link>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</link>
      <description><![CDATA[我正在尝试训练一个简单的神经网络模型进行多类分类。
我有 x1、x2、x3、x4 列，其中有 4 个类别需要预测。
如果只对 x1、x2、x3、x4 进行训练，那么我的准确率是 88%
凭借一些领域知识，我可以创建三个新特征，我知道这肯定会帮助模型更好地训练。
这三个新特征是：-

df[&#39;x12&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x2&#39;])
df[&#39;x13&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x3&#39;])
df[&#39;x14&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x4&#39;])

如果我在 x1、x2、x3、x4 和 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 上进行训练，那么我的准确率是 98%
我想在没有 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 的情况下获得 98% 的准确率
使用这些新的手动创建的特征，我获得了 98% 的验证准确率，这很棒。
但是，当我删除这些特征时，验证准确率会下降到 88%。
我的问题是函数 abs(x1-x2) 应该非常简单，足以让模型自行学习，而无需我手动进行特征工程。
那么为什么当我删除这三个（非常）时准确率会下降简单）特征？
模型是否没有足够的能力自行学习？
我尝试在模型中使用不同的激活函数。
从线性激活函数、relu 激活函数、leaky relu、prelu 开始。
但是它们都没有给我 98% 的准确率（不使用三个手动创建的特征）。
这是我的模型的样子：
def create_dense_model(input_shape, num_outputs, LR):

inputs = Input(shape=input_shape)

x = Dense(units=64)(inputs)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=32)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

multiclass_output = Dense(units=num_outputs,activation=&#39;softmax&#39;)(x)

model = Model(inputs=inputs,outputs=multiclass_output)

model.compile(
loss=&quot;categorical_crossentropy&quot;,
metrics=[&quot;accuracy&quot;],
optimizer=Adam(learning_rate=LR)
)

返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</guid>
      <pubDate>Wed, 11 Sep 2024 15:12:02 GMT</pubDate>
    </item>
    <item>
      <title>银行客户流失预测模型 - 预测能力建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</guid>
      <pubDate>Wed, 11 Sep 2024 08:12:37 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的 Autograd Trainstep 中的 Lightning</title>
      <link>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</link>
      <description><![CDATA[我想实现一个基于 Pytorch Lightning 的 ML 训练，其中我使用 autograd 功能进行训练损失计算：
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_function(y_hat, y)
return loss

X 的每个样本 x 都是一个二维向量 x = [v, a]。
在训练步骤中，我想计算 y_hat 相对于 的梯度。到 v。
损失进一步通过以下方式计算：
loss = mse(y,y_hat) + mse(gradient,gradient_hat)

其中给出了（真实）梯度。
到目前为止，尝试了 y_hat.backward() 的（典型）方法，但无法使其工作：
def training_step(self, batch, batch_idx):
x, y = batch
x.requires_grad_(True) # 确保我们跟踪 x 的梯度
y_hat = self(x)

# 计算 y_hat 相对于 v 的梯度（x[:, 0]）
v = x[:, 0]
grads = torch.autograd.grad(y_hat, v, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0] # ...
]]></description>
      <guid>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</guid>
      <pubDate>Fri, 06 Sep 2024 10:08:53 GMT</pubDate>
    </item>
    <item>
      <title>按特定日期（而非观察结果）进行训练和测试</title>
      <link>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</link>
      <description><![CDATA[我正在尝试使用 R 中的随机森林训练一个模型。我有一个时间序列，其中包含每个日期的多只股票的信息，并创建了一个非常简化的版本：
日期 &lt;- rep(seq(as.Date(&quot;2009/01/01&quot;), by = &quot;day&quot;, length.out = 100), 10)
名称 &lt;- c(rep(&quot;Stock A&quot;, 100), rep(&quot;Stock B&quot;,100), rep(&quot;Stock C&quot;, 100), rep(&quot;Stock D&quot;, 100), rep(&quot;Stock E&quot;,100), rep(&quot;Stock F&quot;,100), rep(&quot;Stock G&quot;,100), rep(&quot;Stock H&quot;,100), rep(&quot;Stock I&quot;, 100), rep(&quot;Stock J&quot;, 100))
类别 &lt;- sample(1:10, 1000, replace=TRUE)

DF &lt;- data.frame(Date, Name, Class)
DF &lt;- DF %&gt;% 排列(Date, Name)

看起来像这样：
 日期 名称 类
1 2009-01-01 股票 A 5
2 2009-01-01 股票 B 2
3 2009-01-01 股票 C 4
4 2009-01-01 股票 D 10
5 2009-01-01 股票 E 7
6 2009-01-01 股票 F 3
...
11 2009-01-02 股票 A 10
12 2009-01-02 股票 B 8
13 2009-01-02 股票 C 9


使用时trainControl 用于将数据拆分为训练和测试期，拆分是基于每个观察进行的，但我希望基于特定日期进行。到目前为止，我所做的是：
timecontrol &lt;- DF %&gt;% group_by(Date) %&gt;% trainControl(
method = &#39;timeslice&#39;,
initialWindow = 10,
horizo​​n = 5,
skip = 4,
fixedWindow = TRUE,
returnData = TRUE, 
classProbs = TRUE
)

fitRF &lt;- train(Class ~ ., 
data = DF,
method = &quot;ranger&quot;,
tuneGrid = tunegrid,
na.action = na.omit,
trControl = timecontrol)

这给了我一个包含 10 个观察的训练集，后面是 5 个测试观察。
但是，我希望有一个训练集（和测试集......）包含 10 个不同日期的所有观测值，这样，一个训练集将是 10 天乘以每天的观测值数量，并且在各个时间段之间跳跃，以便每个测试时间段都基于全新的数据（因此 skip=4）。
第一个训练/测试拆分应该是训练=10 数据集的第一个不同日期，测试=接下来的 5 个不同日期，然后第二个训练/测试拆分应该是测试集 2 是第一个测试集之后的 5 天。
与我上面显示的数据集不同，我的数据集每天包含不同数量的观测值。我的数据集包含 417497 个观测值，但只有 2482 个不同日期，因此能够根据“分组”日期进行训练/测试拆分会产生很大的不同。 
我能否使用 trainControl 来获得所需的分割，还是必须手动分割所有数据？]]></description>
      <guid>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</guid>
      <pubDate>Wed, 08 Apr 2020 08:33:55 GMT</pubDate>
    </item>
    <item>
      <title>如何处理正态分布中的零项</title>
      <link>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</link>
      <description><![CDATA[我正在使用 kaggle 房屋数据集。我正尝试使用神经网络进行练习。我正在尝试规范化数据。我的问题是：我有一个变量 BsmtFinSF1，它指的是“1 型成品平方英尺”，它有很多值为 0。值零对应“无地下室”，事实上，在另一个因子变量中，它对应于一个级别。例如，如果“地下室条件”变量对应于“无地下室”，则意味着 BsmtFinSF1 变量将为 0。下面是 BsmtFinSF1 的直方图。如果我没有弄错的话，如果没有零，分布将是正常的。我该如何将其标准化，或者我是否应该将其标准化？
]]></description>
      <guid>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</guid>
      <pubDate>Wed, 26 Feb 2020 07:30:27 GMT</pubDate>
    </item>
    </channel>
</rss>