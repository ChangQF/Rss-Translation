<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 27 Jun 2024 09:16:14 GMT</lastBuildDate>
    <item>
      <title>在将训练模型制作为 CNN 进行图像分割时，如何修复关键错误“模型”？</title>
      <link>https://stackoverflow.com/questions/78676233/how-fix-keyerror-model-when-make-a-training-model-as-cnn-for-image-segmentatio</link>
      <description><![CDATA[嗨，很高兴向您提问。
我有一个项目，需要通过实例分割提取图像中的 ROI 区域。
我选择 YOLO5 来完成这项任务，因为我有 spyder (3.11)。首先，为了练习，我制作了一个包含 5 张图片的训练模型，这个预先训练好的 model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;) 并成功制作了它。
` import torch
from PIL import Image
import os
# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤3：在训练数据上拟合模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = os.path.join(dataset_path, &#39;yolov5s.pt&#39;)
torch.save(model.state_dict(), save_path)`

但之后我使用 50 张图像制作了一个良好的训练模型，并将它们标记为 eye_number.png，然后再次使用 yolov5s 作为预训练模型。但我的挑战是由于以下错误而出现的。
&quot; 异常：&#39;model&#39;。缓存可能已过期，请尝试 force_reload=True 或参阅 https:// docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading 获取帮助。&quot;
下面是我第二次尝试制作训练模型
 import torch
from PIL import Image
import os

# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;,
&#39;eye_6.png&#39;, &#39;eye_7.png&#39;, &#39;eye_8.png&#39;, &#39;eye_9.png&#39;, &#39;eye_10.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤 3：拟合训练数据上的模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = &#39;D:/yolov5s_webcam.pt&#39;
torch.save(model.state_dict(), save_path)`

我尝试更改保存模型的直接方法，但没有帮助。
我该如何修复此错误？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78676233/how-fix-keyerror-model-when-make-a-training-model-as-cnn-for-image-segmentatio</guid>
      <pubDate>Thu, 27 Jun 2024 07:53:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在自定义 Hugginface 服务器上设置和运行视频模型？</title>
      <link>https://stackoverflow.com/questions/78675537/how-to-set-up-and-run-a-video-model-on-a-custom-hugginface-server</link>
      <description><![CDATA[我正在开展一个涉及自定义视频模型的项目，我需要在我的服务器上设置并运行它。该模型需要特定的配置文件，例如 config.json，以及 pytorch_model.bin 和 pytorch_model.safetensors 等格式的权重。
我的模型文件具有以下目录结构：
/model_directory
──coder
│ ── config.json
│ └── pytorch_model.safetensors
── controlnet
│ ── config.json
│ └── pytorch_model.safetensors
──tention
└──tention.ckpt


有人可以提供详细示例或指南来说明如何：
正确加载这些模型及其配置。
我目前在 huggingface 上收到此错误。
OSError：/repository 似乎没有名为 config.json 的文件。查看“https://huggingface.co//repository/None”以获取可用文件。

设置推理管道以运行模型。
处理潜在问题，例如缺少配置文件或路径不正确。
任何帮助或示例都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78675537/how-to-set-up-and-run-a-video-model-on-a-custom-hugginface-server</guid>
      <pubDate>Thu, 27 Jun 2024 04:31:28 GMT</pubDate>
    </item>
    <item>
      <title>针对不同物种的分类训练各种模型的建议</title>
      <link>https://stackoverflow.com/questions/78674961/suggestion-for-training-various-models-for-classification-of-different-species</link>
      <description><![CDATA[致力于开发各种有害藻类与海水中发现的藻类/其他物体的分类网络。我们已经为一些有害藻类与海洋开发了二元网络。这些有害藻类有 5000 多个类别。我们的一些客户需要 5 种有害藻类分类，而其他客户可能需要更多。为这个问题开发机器学习解决方案的最佳方法是什么？任何建议都值得赞赏。
方法 1：我们可以训练多类模型以满足客户需求。假设客户对 A、B 和 C 有害藻类与其他（海洋 + 剩余有害藻类）感兴趣。但这需要大量数据和针对这些物种的每种组合进行训练。有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78674961/suggestion-for-training-various-models-for-classification-of-different-species</guid>
      <pubDate>Wed, 26 Jun 2024 22:55:16 GMT</pubDate>
    </item>
    <item>
      <title>腌制机器学习模型无法做出预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78674912/pickled-machine-learning-model-doesnt-make-predictions</link>
      <description><![CDATA[我创建了一个 MLP 模型并用 Pickle 保存了它，然后使用以下代码加载该模型并将其应用于新数据：
mlp_probs = pickle_model.predict_proba(X_test)

这很完美。但是当我尝试获取类别预测时：
mlp_predictions = pickle_model.predict(X_test)

所有预测均为 0。换句话说，没有正类。
在我将其腌制之前，该模型运行良好，因此我猜测在腌制过程中未保存阈值。
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78674912/pickled-machine-learning-model-doesnt-make-predictions</guid>
      <pubDate>Wed, 26 Jun 2024 22:36:57 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试训练 ConvNeXt 时，我遇到了一个问题</title>
      <link>https://stackoverflow.com/questions/78674357/when-i-was-trying-to-train-the-convnext-i-met-an-issue</link>
      <description><![CDATA[回溯（最近一次调用）：
文件“tools/train.py”，第 162 行，位于 &lt;module&gt;
main()
文件“tools/train.py”，第 158 行，在 main 中
runner.train()
文件“C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\runner\runner.py”，第 1728 行，在 train 中
self._train_loop = self.build_train_loop(
文件“C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\runner\runner.py”，第 1527 行，在 build_train_loop 中
loop = EpochBasedTrainLoop(
文件“C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\runner\loops.py”，第 44 行，在 __init__ 中
super().__init__(runner, dataloader)
文件&quot;C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\runner\base_loop.py&quot;，第 26 行，在 __init__ 中
self.dataloader = runner.build_dataloader(
文件 &quot;C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\runner\runner.py&quot;，第 1370 行，在 build_dataloader 中
dataset = DATASETS.build(dataset_cfg)
文件 &quot;C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\registry\registry.py&quot;，第 570 行，在 build 中
return self.build_func(cfg, *args, **kwargs, registry=self)
文件&quot;C:\ProgramData\Anaconda3\envs\open-mmlab\lib\site-packages\mmengine\registry\build_functions.py&quot;，第 121 行，在 build_from_cfg 中
obj = obj_cls(**args) # type: ignore
File &quot;d:\working\ai\convnext\convnext\mmpretrain\mmpretrain\datasets\custom.py&quot;，第 207 行，在 __init__ 中
super().__init__(
TypeError: __init__() 获得了意外的关键字参数“split”

当我尝试训练 ConvNeXt 时，发生了上述错误。
我使用了这个命令。
python tools/train.py configs/convnext/convnext-tiny_32xb128_custom.py
convnext-tiny_32xb128_custom.py
_base_ = [
&#39;../_base_/models/convnext/convnext-tiny.py&#39;,
&#39;../_base_/datasets/imagenet_bs64_swin_224.py&#39;,
&#39;../_base_/schedules/imagenet_bs1024_adamw_swin.py&#39;,
&#39;../_base_/default_runtime.py&#39;,
]

# 数据集设置
train_dataloader = dict(batch_size=128, dataset=dict(type=&quot;CustomDataset&quot;))

# 计划设置
optim_wrapper = dict(
optimizer=dict(lr=4e-3),
clip_grad=None,
)

# 运行时设置
custom_hooks = [dict(type=&#39;EMAHook&#39;, motivation=1e-4, priority=&#39;ABOVE_NORMAL&#39;)]

# 注意：`auto_scale_lr` 用于根据实际训练批次大小自动缩放 LR
#。
# base_batch_size = (32 GPU) x (每个 GPU 128 个样本)
auto_scale_lr = dict(base_batch_size=4096)

我使用了 https://github.com/open-mmlab/mmpretrain.git]]></description>
      <guid>https://stackoverflow.com/questions/78674357/when-i-was-trying-to-train-the-convnext-i-met-an-issue</guid>
      <pubDate>Wed, 26 Jun 2024 19:40:46 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>Android 图像分割应用程序在不插入 Android Studio 时运行速度更快</title>
      <link>https://stackoverflow.com/questions/78673441/android-image-segmentation-app-runs-faster-when-not-plugged-into-android-studio</link>
      <description><![CDATA[因此，我创建了一个简单的应用程序，它运行一个 torchscript 模型并对大约 100 张图像进行推理。当我将手机插入我的机器并通过 android studio 运行它时，每张图像需要 300 毫秒。当拔下电源并运行应用程序时，每张图像需要 180 毫秒。这有意义吗？或者有人知道为什么会发生这种情况吗？
经过多次测试以确认。
图像作为资产保存在应用程序中。]]></description>
      <guid>https://stackoverflow.com/questions/78673441/android-image-segmentation-app-runs-faster-when-not-plugged-into-android-studio</guid>
      <pubDate>Wed, 26 Jun 2024 15:49:38 GMT</pubDate>
    </item>
    <item>
      <title>选择 OptunaSearchCV 管道参数的问题</title>
      <link>https://stackoverflow.com/questions/78672059/problems-with-selecting-optunasearchcv-pipeline-parameters</link>
      <description><![CDATA[我有一个代码，用于迭代模型本身和整个管道的超参数
preprocessor = ColumnTransformer(
[
(&#39;OneHotEncoder&#39;, OneHotEncoder(drop=&#39;if_binary&#39;, sparse_output=False), binary_cols),
(&#39;CatBoostEncoder&#39;, CatBoostEncoder(random_state=RANDOM_STATE), non_binary_cat_cols),
(&#39;StandardScaler&#39;, StandardScaler(), num_cols)
],
verbose_feature_names_out=False,
remainder=&#39;drop&#39;
)

pipe_final = ImbPipeline([
(&#39;preprocessor&#39;, preprocessor),
(&#39;target_imbalance&#39;, ADASYN()),
(&#39;selection&#39;, PCA()),
(&#39;models&#39;, CatBoostClassifier(random_state=RANDOM_STATE))
])

# CatBoostClassifier 的参数
param_grid = {
&#39;models__iterations&#39;: [1000, 2000, 3000],
&#39;models__class_weights&#39;: [&#39;Balanced&#39;, None],
&#39;target_imbalance&#39;: [ADASYN(random_state=RANDOM_STATE), SMOTETomek(random_state=RANDOM_STATE),
SMOTE(random_state=RANDOM_STATE, k_neighbors=10), &#39;passthrough&#39;],
&#39;preprocessor__StandardScaler&#39;: [StandardScaler(), RobustScaler(), MinMaxScaler(),
PowerTransformer(), QuantileTransformer(),
Normalizer()，PolynomialFeatures（degree=2，include_bias=False），&#39;passthrough&#39;]，
&#39;selection&#39;：[PCA（random_state=RANDOM_STATE，n_components=“mle”，svd_solver=“full”），
SelectKBest（mutual_info_classif，k=40），
SelectKBest（f_classif，k=40），
SelectKBest（chi2，k=40），
SelectPercentile（mutual_info_classif，百分位数=10），
SelectPercentile（f_classif，百分位数=10），
SelectFromModel（CatBoostClassifier（random_state=RANDOM_STATE）），
SelectFromModel（LogisticRegression（random_state=RANDOM_STATE）），
SelectFromModel（RandomForestClassifier（random_state=RANDOM_STATE）），
&#39;passthrough&#39;],
}

gs = GridSearchCV(
pipe_final, 
param_grid, 
cv=5, 
scoring=&#39;roc_auc&#39;, 
n_jobs=-1
)

# Запускаем поиск гиперпараметров
gs.fit(X, y_enc)

这需要很长时间才能完成，我想加快速度。为此，我想使用 OptunaSearchCV。我是否理解正确，使用 OptunaSearchCV 我可以枚举模型本身的超参数，但不能枚举整个管道，因为没有可以设置 SelectKBest(f_classif, k=40)、RobustScaler() 等的分布？
抱歉，如果我的措辞在某些地方不准确，我使用谷歌翻译，因为......我不是母语人士]]></description>
      <guid>https://stackoverflow.com/questions/78672059/problems-with-selecting-optunasearchcv-pipeline-parameters</guid>
      <pubDate>Wed, 26 Jun 2024 11:14:26 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 ElasticNetCV：获取具有相应 MSE 的超参数完整网格？</title>
      <link>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</link>
      <description><![CDATA[我在 Python 中用三个分割拟合了一个 ElasticNetCV：
import numpy as np
from sklearn.linear_model import LinearRegression

#样本数据：
num_samples = 100 # 样本数量
num_features = 1000 # 特征数量
X = np.random.rand(num_samples, num_features)
Y = np.random.rand(num_samples)

#模型
l1_ratios = np.arange(0.1, 1.1, 0.1)
tscv=TimeSeriesSplit(max_train_size=None, n_splits=3)
regr = ElasticNetCV(cv=tscv.split(X), random_state=42,l1_ratio=l1_ratios)
regr.fit(X,Y)

现在我想获取超参数组合的整个网格以及相应的 MSE 作为数据框，我尝试了以下方法。但是，问题在于，生成的数据框显示的超参数组合的 MSE 并未显示为 ElasticNetCV 对象中的最小值，该对象可以通过 regr.alpha_ 和 regr.l1_ratio_ 获得
:
mse_path = regr.mse_path_
alpha_path = regr.alphas_

# 重塑 mse_path，使 l1_ratios、n_alphas、cross_validation_step 作为单独的列
mse_values = mse_path.flatten()
alpha_values = alpha_path.flatten()
l1_values=np.tile(l1_ratios ,int(alpha_values.shape[0]/l1_ratios.shape[0]))
repeated_l1_ratios = np.repeat(l1_ratios, 100)

# mse维度为 (11, 100, 3)
array_3d = mse

# 将 3D 数组展平为 2D 数组
# 每个形状为 (100, 3) 的子数组都将成为新 2D 数组中的一行
array_2d = array_3d.reshape(-1, 3)

# 从 2D 数组创建 DataFrame
df = pd.DataFrame(array_2d, columns=[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;])

df[&#39;alpha_values&#39;] = alpha_values
df[&#39;l1_values&#39;] = duplicate_l1_ratios

以下结果导致超参数组合不是真实的组合。因此，在组合 MSE 和超参数值时，会出现问题：
# 计算三个分割中每行的最小 MSE
df[&#39;Min MSE&#39;] = df[[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;]].min(axis=1)

# 确定总体最小 MSE 的行
min_mse_row_index = df[&#39;Min MSE&#39;].idxmin()

# 检索最小 MSE 的行
min_mse_row = df.loc[min_mse_row_index]

print(&quot;所有分割中 MSE 最小的行：&quot;)
print(min_mse_row)
]]></description>
      <guid>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</guid>
      <pubDate>Wed, 26 Jun 2024 08:49:51 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的总 epoch 数，对同一 epoch 得出不同的结果</title>
      <link>https://stackoverflow.com/questions/78664794/different-results-for-the-same-epoch-using-different-number-of-total-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78664794/different-results-for-the-same-epoch-using-different-number-of-total-epochs</guid>
      <pubDate>Mon, 24 Jun 2024 22:35:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
with pdfplumber.open(pdf_path) as pdf:
for page in pdf.pages:
page_text = page.extract_text()
if page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

所以当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复？
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530 是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>在 AWS Sagemaker 中训练线性模型时出现 UnexpectedStatusException？</title>
      <link>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</guid>
      <pubDate>Sun, 16 Jun 2024 05:27:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Cord-V2 数据集微调 LayoutLmv3</title>
      <link>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</link>
      <description><![CDATA[我正在使用 CORD-v2 数据集对 LayoutLMv3 进行微调。我在数据预处理部分遇到了困难，特别是如何从图像中正确提取总量 (TTC)。我在网上找到的示例似乎使用了较旧的 CORD 数据集，该数据集的格式不同。新的 CORD-v2 数据集仅包含图像和地面实况标签。
如何解决这个问题？
我尝试过 YouTube 和 Hugging Face 中的示例，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</guid>
      <pubDate>Tue, 11 Jun 2024 09:22:25 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置方向前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 transformer 架构中的位置式前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意子层的输出。前馈层应用了类似的线性变换，但每个变换的实际权重和偏差是不同的，因为它们是两个不同的前馈神经网络。
参考链接，这是PositionWiseFeedForward神经网络的类
class PositionwiseFeedForward(nn.Module):
“实现 FFN 方程。”
def __init__(self, d_model, d_ff, dropout=0.1):
super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)

def forward(self, x):
return self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何与位置相关的信息。这是一个具有两层的简单全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上面的层使用相同的权重和偏差进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的 Linear 层的方法，该层将具有不同的权重和偏差，以实现与图片中所示的类似效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 中 model.eval() 起什么作用？</title>
      <link>https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch</link>
      <description><![CDATA[我什么时候应该使用 .eval()？我理解它应该允许我“评估我的模型”。我如何关闭它进行训练？
使用 .eval() 的示例训练代码。]]></description>
      <guid>https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch</guid>
      <pubDate>Sat, 01 Feb 2020 15:58:15 GMT</pubDate>
    </item>
    </channel>
</rss>