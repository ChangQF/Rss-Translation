<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 02 Apr 2025 09:20:04 GMT</lastBuildDate>
    <item>
      <title>液体。 AI模型后端</title>
      <link>https://stackoverflow.com/questions/79550170/liquid-ai-models-backend</link>
      <description><![CDATA[我正在检查Liquid.ai。我探索了各种博客，但找不到查询的答案，即液体现有的ML或DL型号。EAI在后端使用？]]></description>
      <guid>https://stackoverflow.com/questions/79550170/liquid-ai-models-backend</guid>
      <pubDate>Wed, 02 Apr 2025 09:02:32 GMT</pubDate>
    </item>
    <item>
      <title>RL机器人用于Rubik的立方体[关闭]</title>
      <link>https://stackoverflow.com/questions/79549779/rl-bot-for-a-rubiks-cube</link>
      <description><![CDATA[我有一个脚本，可以使我呈现一个3x3 Rubik的立方体，并且可以多脸转向它。我最近尝试在我的脚本（Pytorch）中添加RL模型来解决立方体。目前，我只是在使用2-Move争夺进行测试，并且该机器人有4个动作来解决立方体。但是，在设置了所有内容之后，在运行时，机器人似乎无法学习并取得了非常糟糕的结果，在4K+情节中获得了15个成功的解决方案。我不确定如何确切指出我的代码出了什么问题，或者为什么RL机器人无法学习，这似乎不是一个很大的话题，所以我不确定在哪里可以获得支持。该代码在
以获取更多信息：
该机器人将先前的状态，未来状态，奖励和动作带入内存缓冲区，并随机选择/火车从每个周期中选择/火车。损失往往会从0.7到1.8波动，通常停留在1.2-1.4。
任何帮助将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79549779/rl-bot-for-a-rubiks-cube</guid>
      <pubDate>Wed, 02 Apr 2025 05:50:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用前向光流施加向后的翘曲（Pytorch的grid_sample）？</title>
      <link>https://stackoverflow.com/questions/79548719/how-to-apply-backward-warp-pytorchs-grid-sample-with-forward-optical-flow</link>
      <description><![CDATA[我最近一直在研究光流算法，并一直在使用pytorch应用光流场。
我注意到大多数库仅实现了向后的扭曲功能，例如 torch.nn.functional.grid_sample 和 cv2.remap 。
我相信我已经正确实现了标准化和网格方向，因为我的图像被类似于目标图像。下面是我实现的某些部分给定3D流场 opt_flow  
 ＃标准化流场
＃3，W，H，D
opt_flow [0] /=（W-1）
opt_flow [1] /=（H-1）
opt_flow [2] /=（D-1）

＃创建网格
＃请注意，Torch.meshgrid返回指示为z，y，x
grid = TORCH.STACK（TORCH.MESHGRID（[[TORCH.LINSPACE（-1，1，i）for opt_flow.shape.shape [1：]]，索引=&#39;ij&#39;），3），3）。
        
＃翻转网格，以使说明为x，y，z
网格=火炬。Flip（网格，[3]）

＃D，H，W，3
opt_flow = opt_flow.permute（1，2，3，0）

＃将流场添加到网格中以表示新坐标
＃我减去了，因为最初我认为这会解释
网格 -  = opt_flow
        
＃添加批处理维度
网格=网格[无，...]

applied = f.grid_sample（self._to_tensor（img）[none，none，none，...]，grid，mode =&#39;bilinear&#39;，padding_mode =&#39;zeros&#39;，align_corners&#39;，align_corners = true）.squeeze（dim =（0，1））
 
我注意到的问题是向后翘曲需要“ 逆光流（如果）”而不是正向光流（FF，由prev  - ＆gt; cur计算出的光流）。这是因为GRID_Sample中所需的网格表示应从每个像素中采样的位置。但是这种关系不仅是如果= -ff 。 （如何在图像上应用反向光流量向量？
简化为2D图像，假设我的FF在网格索引（5，6）处具有位移矢量（3，4）。然后，从Grid_sample的角度来看，我们希望在网格索引（8、10）处存储（-3，-4）的IF网格。简单地否定会导致（-3，-4）存储在（5，6），在grid_sample期间带来错误的像素值。
一个简单的解决方案将是将向后的光流（BF，从CUR -＆GT; PREV计算的光流）将其应用于上级图像。但是将BF应用于“ PREV”似乎是违反直觉的。图像。但同时，由于grid_sample是“向后”的。经扭曲，应用“落后”似乎也很直观。 prev的光场
另一个解决方案将是使用ff上迭代的循环实现向前的翘曲。
对于每个索引i，ff的j，存储网格[i，j]*（-1）位置[i+x_displacement，j+y_displacement]。但这将是一个不可集成的函数。
所以我的问题是：

 是否有一种简单的方法使用向后翘曲（例如grid_sample）应用FF？

 我是否必须将FF应用于上述错误的直觉？ （可以将BF应用于上一个吗？）

 如何创建一个可集成的向前翘曲函数？

]]></description>
      <guid>https://stackoverflow.com/questions/79548719/how-to-apply-backward-warp-pytorchs-grid-sample-with-forward-optical-flow</guid>
      <pubDate>Tue, 01 Apr 2025 13:05:29 GMT</pubDate>
    </item>
    <item>
      <title>使用Yolo在CPU上使用RTSP流滞后的车辆检测 - 寻求优化</title>
      <link>https://stackoverflow.com/questions/79547886/vehicle-detection-using-yolo-on-cpu-with-rtsp-stream-lagging-seeking-optimizat</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79547886/vehicle-detection-using-yolo-on-cpu-with-rtsp-stream-lagging-seeking-optimizat</guid>
      <pubDate>Tue, 01 Apr 2025 06:40:27 GMT</pubDate>
    </item>
    <item>
      <title>需要透彻了解因果ML研究论文的背景吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79547011/required-background-for-thorough-understanding-of-causal-ml-research-papers</link>
      <description><![CDATA[我有兴趣在因果推理和机器学习的交集中进行研究，尤其是在因果发现和因果代表学习方面。通过到目前为止，通过我的探索，我发现对以下书籍进行研究至关重要，然后再阅读该领域的研究。

通过墨菲和主教的书（可以选择任何人）的强大ML基金会
理解机器学习（第1部分）的理论ML背景，通常在提出休闲学习理论之前引用。
 Judea Pearl的因果关系，以深入了解因果推论，然后是Bernhard Scholkopf因果发现的因果推断的要素。

我的问题是：
这些书足以准备该主题的研究吗？如果没有，您将添加到此列表中？
成功完成这些书籍的一些基本先决条件是什么？例如贝叶斯因果关系的可能性？还是其他？]]></description>
      <guid>https://stackoverflow.com/questions/79547011/required-background-for-thorough-understanding-of-causal-ml-research-papers</guid>
      <pubDate>Mon, 31 Mar 2025 18:37:31 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测模型，带有XGBoost和Dask大数据集崩溃</title>
      <link>https://stackoverflow.com/questions/79547006/time-series-forecasting-model-with-xgboost-and-dask-large-datasets-crashing</link>
      <description><![CDATA[我正在Python建立一个时间序列预测模型，以预测公用事业公司不同客户类型的每小时KWH负载。该数据集包含约8100万行，在2  -  4年内为2300个客户提供每小时负载数据。客户类型由二进制列表示：EV，HP，太阳能和TOU。数据集具有以下变量：
   -  read_date：datetime64 [us]
   - 仪表：字符串
  -KWH：float64
   - 城市：弦
   - 温度：float64
  -EV：INT64
   - 太阳能：INT64
  -HP：INT64
  -TOU：INT64
   - 小时：INT32
   - 天：INT32
   - 月份：INT32
   - 年：INT64
  -day_of_week：int32
   - 季节：弦
  -customer_type：字符串
  -HOUR_SIN：FLOAT64
  -HOUR_COS：FLOAT64
  -month_sin：float64
  -month_cos：float64
  -Day_of_week_sin：float64
  -Day_of_week_cos：float64
  -Day_sin：float64
  -Day_cos：float64
   -  is_holiday：int64
  -City_Reading：INT64
  -City_lynnfield：INT64
  -City_NorthReading：INT64
  -City_wilmington：INT64
   - 季_WINTER：INT64
  -Season_spring：INT64
   -  sepen_summer：int64
   -  sepen_fall：INT64
 
After cleaning the data, I dropped the following features from both the training and test datasets: meter, customer_type, season, read_date, city, day, month, hour, day_of_week.我的目标变量是小时kWh负载。
我试图使用dask构建XGBoost模型以进行分发，但它一直在以下错误崩溃：
  essertionError：错误
2025-03-31 14：12：26,995-分布式。
 
我正在使用128GB RAM和Intel I7-14700K 3.40 GHz处理器的本地计算机工作。我正在寻找有关如何处理此大型数据集预测时间序列的指导，以及如何在使用DASK进行分发时避免崩溃。这是我的示例代码：
 ＃导入必要的库
导入numpy作为NP
导入dask.dataframe作为DD
导入dask.array作为da
导入XGBoost为XGB
来自dask.distribed Import客户端
来自dask.diarostics导入进步键 
来自sklearn.metrics incort cone_absolute_error，mean_squared_error，r2_score
进口警告
导入matplotlib.pyplot作为PLT
从TQDM导入TQDM

＃使用dask加载数据（大型镶木文件有效）
some_feats_dd = dd.read_parquet（&#39;pre_ml_some_features.parquet＆quort＆quot;）

＃重命名dataFrame
df_processed = some_feats_dd

＃基于读取_DATE进行训练和测试的数据
df_train = df_processed [df_processed [＆quot; 2025]＃在2025年之前保持行
df_test = df_processed [df_processed [&#39;Year; eart; quot; quot; quot; quort; quot; quort; quot&#39;== 2025]＃从2025年开始保持行

＃排除列并准备训练的功能和目标变量
dublude_cols = [kwh＆quot&#39;米，&#39;customer_type&#39;&#39;&#39; 
                ＆quot&#39;&#39;

＃准备培训功能（x）和目标变量（y）
x_train = df_train.drop（columns = ubl_cols）
y_train = df_train [＆quot; kwh＆quot;]

＃计算总长度并确保精确3个块
train_size = len（y_train.compute（））
test_size = len（df_test）＃无需计算，dask可以推断

＃用强制3个块将y_train和y_test转换为dask阵列
y_train = da.from_array（y_train.compute（），chunks =（train_size // 3，））
y_test = da.from_array（df_test [＆quot; kwh;]。compute（），chunks =（test_size // test_size // 2，））

＃确保与x_train和x_test的分区匹配
x_train = x_train.repartition（npartitions = 3）
x_test = x_test.repartition（npartitions = 3）

＃启动DASK客户端以进行并行处理
客户端=客户端（）

＃打印D​​ask仪表板URL
打印（f＆quot“ dask仪表板

＃从xgboost.dask使用daskdmatrix
dask_train_data = xgb.dask.daskdmatrix（客户端，x_train，y_train）

＃设置XGBoost的参数
params = {
    “目标”：“ reg：squaredErr”，＃回归任务
    &#39;eval_metric&#39;：&#39;rmse&#39;，
    &#39;tree_method&#39;：“历史”，＃使用基于直方图的方法来更快训练
    &#39;冗长&#39;：1，＃启用基本记录
}

＃初始化dask-xgboost模型
dask_gbr = xgb.dask.daskxgbregressor（**参数）

＃使用DASK训练模型（这将自动并行化）
使用进度栏（）：＃显示训练期间的进度
    dask_gbr.fit（dask_train_data）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79547006/time-series-forecasting-model-with-xgboost-and-dask-large-datasets-crashing</guid>
      <pubDate>Mon, 31 Mar 2025 18:33:41 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：数据加载器对象不可订阅</title>
      <link>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</link>
      <description><![CDATA[我正在创建一个AI模型来生成人群的密度图。将数据集分为两个，一个用于培训，一个用于验证，我创建了两个数据集，然后尝试使用 torch.utils.data.dataloader（test_set，batch_size = batch_size = batch_size，shuffle = false））。之后，要测试数据，我迭代并使用下一个函数获取数据集的下一个元素，然后获得TypeError。
我正在使用Kaggle的数据集从Kaggle使用：这是完整的代码：
  batch_size = 8 
设备=&#39;cuda：0&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;

train_root_dir =＆quot; data/part_a/train_data/＆quot
init_training_set = dataloader（train_root_dir，shuffle = true）

＃将培训集的一部分分为验证集
train_size = int（0.9 * len（init_training_set））
val_size = len（init_training_set）-train_size

train_indices = list（range（train_size））
val_indices = list（range（train_size，len（init_training_set））））））
train_dataset = torch.utils.data.dataset.subset（init_training_set，train_indices）
val_dataset = torch.utils.data.dataset.subset（init_training_set，val_indices）

train_loader = torch.utils.data.dataloader（train_dataset，batch_size = batch_size，shuffle = true）
val_loader = torch.utils.data.dataloader（val_dataset，batch_size = batch_size，shuffle = false）

test_root_dir =＆quot; data/part_a/test_data/＆quot
test_set = dataloader（test_root_dir，shuffle = false）
test_loader = torch.utils.data.dataloader（test_set，batch_size = batch_size，shuffle = false）

dataiter = iter（train_loader）
ex_images，ex_dmaps，ex_n_people = next（dataiter）


＃显示图像和密度图
plot_corresponding_pairs（ex_images，ex_dmaps）
 
具体错误是：
  trackback（最近的最新通话）：
 第61行，in＆lt;模块＆gt;
    对于ex_images，ex_dmaps，ex_n_people in train_loader中
typeError：“数据加载程序”对象不可订阅
 ]]></description>
      <guid>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</guid>
      <pubDate>Mon, 31 Mar 2025 15:02:15 GMT</pubDate>
    </item>
    <item>
      <title>无法下载MMCV 1.3.0并构建车轮</title>
      <link>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</link>
      <description><![CDATA[当我尝试安装 mmcv-full == 1.3.0 时，它无法下载并构建轮子（我已经更新了车轮）
  错误无法为MMCV-Full构建车轮，这是安装pyproject.toml项目所需的
 但是当我尝试使用时
 MIM安装mmcv-full  
错误消息：
  RROR：MMCV-Full的建筑轮失败
  运行设置。
无法构建mmcv-full
错误：无法为MMCV-Full构建车轮，这是安装pyproject.toml的项目所需的
 
可以下载 mmcv-full 的最新版本，但是由于我试图克隆的存储库需要使用 MMCV版本1.3.0 。
我正在使用 Windows 11 ，想知道我应该如何下载版本。]]></description>
      <guid>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</guid>
      <pubDate>Tue, 14 Nov 2023 08:00:38 GMT</pubDate>
    </item>
    <item>
      <title>凯拉斯调整更喜欢更快的型号？</title>
      <link>https://stackoverflow.com/questions/76248184/keras-tuning-that-prefers-faster-models</link>
      <description><![CDATA[我正在尝试调整超参数，以了解哪些值在Tensorflow Keras模型中是最佳的。我将在Minimax算法中使用最佳模型，因此，由于算法的估计量，评估速度很重要。另外，通过评估我可以看到的模型几乎没有添加精度，但需要更长的时间来使调谐速度更长。 。
简而言之，某些模型kt.hyprand吐出的精度略高，但要花费更多的时间来适应和预测。有没有一种方法来重视准确性，而更喜欢简单，更快的型号？
这是我当前的代码：
 ！pip install -q -u keras -tuner
导入keras_tuner作为kt

DEF Model_builder（HP）：
    全局val_dataset
    全球train_dataset
    全局test_dataset
    模型= keras。
        tf.keras.layers.conv2d（hp.int（&#39;conv1filter&#39;，min_value = 32，max_value = 512*3，步骤= 512/2），
                               hp.int（&#39;conv1kernal&#39;，min_value = 2，max_value = 20，步骤= 2）， 
                               填充=; same＆quot; 
                               激活=“ relu＆quot”， 
                               input_shape =（14,8,8）），
        tf.keras.layers.batchnormalization（axis = -1，动量= 0.99，epsilon = 1e-05），
        tf.keras.layers.conv2d（hp.int（&#39;conv2filter&#39;，min_value = 32，max_value = 512*3，step = 512/2），
                               hp.int（&#39;Conv2Kernal&#39;，min_value = 2，max_value = 20，步骤= 2）， 
                               填充=; same＆quot; 
                               激活=; relu＆quort;），
        tf.keras.layers.batchnormalization（axis = -1，动量= 0.99，epsilon = 1e-05），
        layers.flatten（），
        tf.keras.layers.dense（hp.int（&#39;dense1&#39;，min_value = 32，max_value = 512，step = 32），activation =&#39;relu&#39;），），
        tf.keras.layers.dense（hp.int（&#39;dense2&#39;，min_value = 32，max_value = 512，step = 32），activation =&#39;relu&#39;），），
        tf.keras.layers.dense（hp.int（&#39;dense3&#39;，min_value = 32，max_value = 512，step = 32），activation =&#39;relu&#39;），），
        tf.keras.layers.dense（1，activation =&#39;tanh&#39;），

  ）））
    HP_LEARNING_RATE = HP.CHOICE（&#39;Learning_rate&#39;，values = [1e-2，1e-3，1e-4]）
    model.compile（优化器= keras.optimizers.adam（Learning_rate = hp_learning_rate），
                损失=&#39;mean_absolute_error&#39;，
                指标= [&#39;准确性&#39;]）

    返回模型

调谐器= kt.hyprand（model_builder，
                     objective =&#39;val_accuracy&#39;，
                     max_epochs = 10，
                     覆盖= true，
                     目录=&#39;my_dir30&#39;，
                     project_name =&#39;Into_to_kt30&#39;）
stop_early = tf.keras.callbacks.earlystopping（Monitor =&#39;val_accuracy&#39;，耐心= 5）
tuner.search（x_train，y_train，validation_data =（x_val，y_val），epochs = 50，callbacks = [stop_early]）

＃获取最佳的超参数
best_hps = tuner.get_best_hyperparameters（num_trials = 1）[0]

型号= tuner.hypermodel.build（best_hps）
＃使用最佳超参数构建模型，并在50个时期的数据上训练它
历史= model.fit（x_train，y_train，validation_data =（x_val，y_val），epochs = 50）

val_acc_per_epoch =历史[&#39;val_accuracy&#39;]
best_epoch = val_acc_per_epoch.index（max（val_acc_per_epoch）） + 1
hypermodel = tuner.hypermodel.build（best_hps）

＃重新训练模型
hypermodel.fit（x_train，y_train，validation_data =（x_val，y_val），epochs = best_epoch）
eval_result = hypermodel.evaluate（x_test，y_test）
打印（测试损失，测试准确性]：＆quort; eval_result）
hypermodel.save（&#39;/notebooks/saved_model/my_model&#39;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/76248184/keras-tuning-that-prefers-faster-models</guid>
      <pubDate>Sun, 14 May 2023 15:07:49 GMT</pubDate>
    </item>
    <item>
      <title>如何计算幼稚贝叶斯分类器中的证据？</title>
      <link>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</link>
      <description><![CDATA[我在Python写了一个简单的多项式幼稚贝叶斯分类器。该代码预测
整个过程基于我从 nofollow noreferrer“&gt; wikipedia文章关于幼稚的贝耶斯：
  
因此，第一步是从文章中提取特征。为此，我使用Sklearn的Count Vectorizer。它计算词汇中所有单词的出现数量：

 来自Sklearn.feature_extraction.text Import CountVectorizer
vectorizer = countvectorizer（stop_words =&#39;英语&#39;，min_df = 5，ngram_range =（1,1））
功能= vectorizer.fit_transform（data.news）.toArray（）
打印（功能。形状）
（2225，9138）
 
结果，我在数据集中获得了每个文章的9138个功能。

下一步是为每个标签计算P（x  i  | c  k ）。它由多项式分布公式给出：

   i计算p  ki 如下：
  def count_word_probability（功能）：
  v_size =功能。形状[1]
  alpha = 1
  total_counts_for_each_word = np.sum（功能，轴= 0）
  total_count_of_words = np.sum（total_counts_for_each_word）
  probs =（alpha + total_counts_for_each_word） /（（v_size * alpha） + total_count_of_words）
  返回概率
 
基本上，此函数的作用是计算所有文章中带有特定标签（例如业务）的每个单词的总频率，并除以所有文章中带有该标签的单词总数。它还适用拉普拉斯平滑（alpha = 1）以说明0频率的单词。

接下来，我计算P（C  k ），这是标签的先验概率。我只是将一个类别中的文章总数除以所有类别的文章总数：

  labels_probs = [len（data.index [data [&#39;category_id&#39;] == i]） / len（data）for Range（5）]
 

这些是缩放术语和恒定项的函数（p（x）相应：

 将数学导入数学
来自Scipy.特定进口阶乘

def scaing_term（doc）：
  term = Math.factorial（np.sum（doc）） / np.prod（fortorial（doc））
  返回期限 
 
缩放函数上面的缩放函数将文章中的单词总和划分为段落的产物。
  def nb_constant（文章，labels_probs，word_probs）：
  s_term = scaing_term（文章）
  证据= [np.log（s_term） + np.sum（文章 * np.log（word_probs [i]））） + np.log（labels_probs [i]）for Range（len（word_probs））
  证据= np.sum（证据）
  返回证据
 
 o，上面的最后一个函数计算分母（先验概率p（x）。它总结了所有文章类别的UPS p（x | c  k ）：
  
和最终的天真贝叶斯分类器看起来像这样：

  def naive_bayes（文章，label_probs，words_probs）：
  class_probs = []
  s_term = scaing_term（文章）
  constant_term = nb_constant（文章，label_probs，words_probs）
  对于范围（Len（Label_probs））的Cl）：
    class_prob =（np.log（s_term） + np.sum（文章 * np.log（words_probs [cl]））） + np.log（label_probs [cl]）） / constant_term
    class_probs.append（class_prob）
  class_probs = np.exp（np.array（class_probs））
  返回class_probs
 
没有恒定术语的情况下，此功能为我馈送的任何自定义文本都会输出正确的标签。但是所有类别的分数都是均匀的，接近零。当我除以恒定术语以获取总和最高为零的实际概率值时，我会得到所有类别的奇怪结果，例如所有类别的概率。我绝对缺少理论上的东西，因为我对概率理论和数学不了解。]]></description>
      <guid>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</guid>
      <pubDate>Fri, 28 Feb 2020 14:56:20 GMT</pubDate>
    </item>
    <item>
      <title>如何计算Bernoulli幼稚贝叶斯的联合日志样本</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[For a classification problem using BernoulliNB , how to calculate the joint log-likelihood?关节可能性应通过下方公式计算，其中y（d）是实际输出的数组（不是预测值），而x（d）是特征的数据集。
  我阅读 href =“ https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/naive_bayes.py#l839“ rel =“ nofollow noreferrer”]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>NOTFittitError：不安装估算器，在利用模型之前调用``fit''</title>
      <link>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</link>
      <description><![CDATA[我在MacBook OSX 10.2.1（Sierra）上运行Python 3.5.2。
尝试从Kaggle运行一些泰坦尼克号数据集的代码时，我一直遇到以下错误：


 NotFittitedError Trackback（最近的电话
  last）in（）
        6 
        7＃使用测试集进行预测并打印。
  ----&gt; 8 my_prediction = my_tree_one.predict（test_features）
        9印刷（my_prediction）
       10 
/library/frameworks/python.framework/3.5/lib/python3.5/site-packages/sklearn/tree/tree/tree.py.py
  在预测（self，x，check_input）中
      429“”
      430 
   - &gt; 431 x = self._validate_x_predict（x，check_input）
      432 proba = self.tree_.predict（x）
      433 n_samples = X.Shape [0] 
/library/frameworks/python.framework/3.5/lib/python3.5/site-packages/sklearn/tree/tree/tree.py.py
  在_validate_x_predict（self，x，check_input）中
      每当试图预测，应用，预测_proba“”时，386“”“验证X
      387如果self.tree_无：
   - &gt; 388提出不拟合eRror（“不安装估算器”，
      389“调用 fit 在利用模型之前。”）
      390 
 notFittitError：不拟合估算器，在利用该 fit 之前
  型号。

有问题的代码似乎就是这样：
 ＃将缺失的值归为中位数
test.fare [152] = test.fare.median（）

＃从测试集中提取功能：PCLASS，性别，年龄和票价。
test_features = test [[[“ pclass”，“ sex”，“ age”，“ fare”]。

＃使用测试集进行预测并打印。
my_prediction = my_tree_one.predict（test_features）
打印（my_prediction）

＃创建一个具有两列的数据框架：PassengerId＆amp;幸存。幸存的包含您的预测
pastengerid = np.Array（test [“ passenterid”]）。astype（int）
my_solution = pd.dataframe（my_prediction，passenterid，columns = [“幸存”]）
打印（my_solution）

＃检查您的数据框是否有418个条目
打印（my_solution.shape）

＃用名称my_solution.csv将解决方案写入CSV文件
my_solution.to_csv（“ my_solution_one.csv”，index_label = [“ passenterid”]）
 
这是由于我已经称为“ fit”函数，所以我无法理解此错误消息。我要去哪里？感谢您的时间。
 编辑：
事实证明，问题是从上一个代码块继承的。
 ＃适合您的第一个决策树：my_tree_one
my_tree_one = tree.decisionstreeclalsifier（）
my_tree_one = my_tree_one.fit（features_one，target）

＃查看随附功能的重要性和分数
打印（my_tree_one.feature_importances_）
打印（my_tree_one.score（femant_one，target））
 
与行：
 my_tree_one = my_tree_one.fit（features_one，target） 
生成错误：

 valueerror：输入包含NAN，Infinity或一个太大的值
  dtype（&#39;float32&#39;）。
]]></description>
      <guid>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</guid>
      <pubDate>Fri, 02 Dec 2016 17:10:22 GMT</pubDate>
    </item>
    <item>
      <title>如何找到功能对逻辑回归模型的重要性？</title>
      <link>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</link>
      <description><![CDATA[我有一个由逻辑回归算法训练的二进制预测模型。我想知道哪些功能（预测指标）对于正面或负面类别的决策更为重要。我知道有来自Scikit-Learn软件包的 COEF _ 参数，但我不知道它是否足以满足重要性。另一件事是我如何根据否定和正类别的重要性来评估 coef _ 值。我还阅读了有关标准化回归系数的信息，但我不知道它是什么。
可以说，有肿瘤大小，肿瘤重量等特征，可以决定恶性肿瘤或不恶性等测试案例。我想知道哪些功能对于恶性和不是恶性预测更为重要。]]></description>
      <guid>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</guid>
      <pubDate>Wed, 02 Dec 2015 20:11:21 GMT</pubDate>
    </item>
    <item>
      <title>幼稚的贝叶斯分类器从头开始实现</title>
      <link>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</link>
      <description><![CDATA[我正在尝试自己实施我的第一个天真的贝叶斯分类器，以更好地理解。因此，我的数据集来自 http://archive.ics.uci.uci.uci.uci.edu/ml/datasets/datasets/datasets/Adult  Adadult  American Census Data，Spersus sass seals seals＆lt; &#39;＆gt; 50k&#39;）。
这是我的python代码：
 导入系统
导入CSV

word_stats = {}＃{&#39;word&#39;：{&#39;class1&#39;：cnt，&#39;class2&#39;：cnt&#39;}}}
word_cnt = 0

targets_stats = {}＃{&#39;class1&#39;：3234，&#39;class2&#39;：884}每个类中有多少个单词
class_stats = {}＃{&#39;class1&#39;：7896，&#39;class2&#39;：3034}每个类中有多少行
items_cnt = 0

def train（数据集，目标）：
    global word_stats，words_cnt，targets_stats，items_cnt，class_stats

    num = len（数据集）
    对于Xrange（num）中的项目：
        class_stats [targets [item]] = class_stats.get（targets [item]，0） + 1

        对于i在Xrange（len（dataset [item]）））：
            word = dataset [item] [i]
            如果不是words_stats.has_key（word）：
                word_stats [word] = {}

            TGT =目标[项目]

            cnt = word_stats [word] .get（tgt，0）
            word_stats [word] [tgt] = cnt + 1

            targets_stats [tgt] = targets_stats.get（tgt，0） + 1
            word_cnt += 1

    items_cnt = num

DEF分类（DOC，TGT_SET）：
    global words_stats，words_cnt，targets_stats，items_cnt

    probs = {}＃概率本身p（c | w）= p（w | c） * p（c） / p（w）
    PC = {}＃probability of Clofe in Document set p（c）中
    pwc = {}＃probability在特定类中的单词设置。 P（W | C）
    pw = 1 #1＃documet set中的单词集

    doc中的单词：
        如果在words_stats中没有单词：
            继续#dirty，非常肮脏 
        pw = pw * float（sum（word_stats [word] .values（））） / word_cnt

    对于TGT_SET中的TGT：
        PC [tgt] = class_stats [tgt] / float（items_cnt）
        doc中的单词：
            如果在words_stats中没有单词：
                继续#dirty，非常肮脏
            tgt_wrd_cnt = word_stats [word] .get（tgt，0）
            pwc [tgt] = pwc.get（tgt，1） * float（tgt_wrd_cnt） / targets_stats [tgt]

        probs [tgt] =（pwc [tgt] * pc [tgt]） / pw

    l =排序（probs.items（），key = lambda i：i [1]，反向= true）
    打印概率
    返回L [0] [0]

def check_results（数据集，目标）：
    num = len（数据集）
    tgt_set = set（目标）
    正确= 0
    错误= 0

    对于Xrange（num）中的项目：
        res =分类（dataset [item]，tgt_set）
        如果res ==目标[项目]：
            正确=正确 + 1
        别的：
            错误=不正确 + 1

    打印“正确：”，float（正确） / num，&#39;不正确：&#39;，float（不正确） / num
            
def load_data（fil）：
    数据= []
    tgts = []

    阅读器= csv.reader（fil）
    对于读者中的行：
        d = [X.Strip（）in in in in in in in in in in]
        如果 &#39;？&#39;在D：
            继续

        如果不是Len（D）：
            继续
        
        data.append（d [： -  1]）
        tgts.append（D [-1：] [0]）

    返回数据，TGTS

如果__name__ ==&#39;__ -main __&#39;：
    如果Len（sys.argv）＆lt; 3：
        打印&#39;./program train_data.txt test_data.txt&#39;
        sys.exit（1）

    文件名= sys.argv [1]
    fil = open（文件名，&#39;r&#39;）
    数据，tgt = load_data（fil）
    火车（数据，TGT）

    test_file = open（sys.argv [2]，&#39;r&#39;）
    test_data，test_tgt = load_data（test_file）

    check_results（test_data，tgt）
 
它给出了〜61％的正确结果。当我打印概率时，我会得到以下内容：
  {&#39;＆lt; = 50K&#39;：0.07371606889800396，&#39;＆gt; 50K&#39;：15.325378327213354}
 
但是，在正确的分类器的情况下，我希望看到这两个概率的总和等于1。
起初，我认为问题是在浮动底流中，并试图以对数进行所有计算，但结果是相似的。
我知道省略一些单词会影响准确性，但是概率是错误的。
我做错了什么或不明白？
出于您的说服力，我在这里上传了数据集和Python脚本：
&lt;A href =“ https://dl.dropboxusercontent.com/u/36180992/adult.tar.gz”]]></description>
      <guid>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</guid>
      <pubDate>Sun, 13 Oct 2013 19:52:37 GMT</pubDate>
    </item>
    <item>
      <title>天真的贝叶斯分类器[关闭]</title>
      <link>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</link>
      <description><![CDATA[我正在使用Scikit-learn来查找文档的TF-IDF重量，然后使用幼稚
贝叶斯分类器对文本进行分类。但是文档中所有单词的TF-IDF权重除了少数单词。但据我所知，负值意味着不重要的术语。那么，是否有必要将整个TF-IDF值传递给贝叶斯分类器？如果我们只需要通过其中的一些，我们该怎么做？与LinearSVC相比，贝叶斯分类器的好坏是多么好吗？除了使用TF-IDF以外，是否有更好的方法可以在文本中找到标签？]]></description>
      <guid>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</guid>
      <pubDate>Tue, 13 Mar 2012 02:28:44 GMT</pubDate>
    </item>
    </channel>
</rss>