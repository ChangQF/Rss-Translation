<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Fri, 07 Feb 2025 12:32:26 GMT</lastBuildDate>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[在实施使用 tf.data.dataset 作为KERAS模型的输入的训练环时，我遇到了问题。我的数据集具有以下格式的元素规范：
 （{&#39;data&#39;：tensorSpec（shape =（15000，1），dtype = tf.float32），&#39;index&#39;：tensorspec&#39;：tensorspec（shape =（2，2，2，2， ），dtype = tf.int64）}，tensorspec（shape =（1，），dtype = tf.int32））
 
因此，基本上，每个样本均以元组（x，y）构建形状（15000，1），另一个形状索引（2，）（在培训期间不使用索引）， y 是单个标签。
The tf.data.Dataset is created using dataset = tf.data.Dataset.from_tensor_slices((X, y)), where X是两个密钥的命令：

 数据：形状的NP数组（200k，1500，1）， index  with 
 索引：形状的NP数组（200K，2） 

和 y 是一个形状的单个数组`（200k，1）
我的数据集有大约200k的培训样本（运行底漆后）和200k验证样本。
呼叫 tf.data.dataset.from_tensor_slices 我注意到GPU内存使用情况中有一个尖峰，在创建培训 tf.dataset ，and和创建验证 tf.dataset 。之后，更多16GB
创建 tf.dataSet 后，我运行了一些操作（例如，洗牌，批处理和预拿方），并调用 model.fit.fit 。我的型号大约有500K可训练的参数。
我遇到的问题是  拟合模型。我需要在一些其他数据上运行推断，因此我使用此数据创建了一个新的 tf.dataset ，再次使用 tf.dataset.from_tensor_slices 。但是，我注意到培训和验证 tf.dataset 仍然存在于GPU内存中，这导致我的脚本随着新的 tf.dataset  i而遇到的不含内存问题 i想要推断。
我在两个 tf.dataset 上尝试调用 del ，然后随后调用 gc.collect（）清除RAM，而不是GPU内存。另外，我尝试禁用我采用的一些操作，例如预摘要，也可以使用批量大小，但是这些都没有起作用。
是否有任何方法可以从GPU中清除两个 tf.dataset ，而无需致电 keras.backend.clear_session（） ，因为这也将从gpu？还是我唯一调用 clear_session 并从磁盘中重新加载模型的唯一选择？]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>如何向量融合？</title>
      <link>https://stackoverflow.com/questions/79420799/how-to-vector-fusion</link>
      <description><![CDATA[假设有一个文档。在本文档中，有多个主题。通过其他手段，例如神经深度学习，这些主题嵌入了向量中。例如，“＃天气真的很好” ＆quot＃适合外出玩耍。”被转换为矢量A [0.1，0.25，...]和B [0.1，0.34，...]。
该文档还包含内容，该内容还通过嵌入将其转换为向量C [0.1，0.34，...]。它们都是2560-尺寸空间向量，具有相同的维度。
现在，我有一个要求。基于搜索条件，将其嵌入矢量f [0.6，0.78，...]。
使用余弦相似性的原理，我们计算：
cos（_f，_a）
cos（_f，_b）
cos（_f，_C）
我们得到三个分数，然后平均得分以获得全面的分数，这是本文档和搜索条件之间最相似的分数，实现了语义检索。
但是，该方法的计算复杂性太高了，因为主题的数量不是固定的，并且可能超过十。
所以，我们提出了计划2。
在计划2中，我们在主题_a和_b上执行平均合并，即计算向量的算术平均值以获取_ac。然后，我们执行内容向量_C和_ac的加权平均值：0.7 * _C + 0.3 * _ac = _bac。然后，我们计算搜索 - 条件向量_F和_bac之间的余弦相似性，以获得相似性。但是问题在于，是_a和_b的算术平均值还是_ac和_c的加权平均值，某些功能将丢失，从而导致语义查询不令人满意。
有什么方法可以融合_a，_b和_c而不会丢失语义？]]></description>
      <guid>https://stackoverflow.com/questions/79420799/how-to-vector-fusion</guid>
      <pubDate>Fri, 07 Feb 2025 11:55:21 GMT</pubDate>
    </item>
    <item>
      <title>无法理解机器学习算法背后的数学之间的关系[关闭]</title>
      <link>https://stackoverflow.com/questions/79420509/couldnt-understand-relation-between-math-behind-the-machine-learning-algorithm</link>
      <description><![CDATA[我正在研究加强学习算法。我正在观看一些诸如CS285的讲座 - 来自Berkley或Google DeepMind。我能理解讲师教的90-95％。  但是，当我尝试实现自己学到的算法时，我遇到了很多困难。我阅读了有关该主题的文章，并检查了作者或人们编写的示例代码，但我无法将文章的数学公式与文章解释 - 和书面代码相关联。  为了谈话，让我举个例子。
  -log [π_θ（â_θ（s，ξ）| S）]  
 注意：我删除了拆卸零件，以使我更易于理解。 
那是我从或 sac文章这就是代码实现i，我发现不同的git repos  [1]  -line-line 78，70-，70-，&lt; a href =“ https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/master/model.py.py#l64” rel =“ nofollow noreferrer”&gt; [2] 104- 
  log_probs = normals.log_prob（xs）-torch.log（1- actions.pow（2） + self.eps）
 
 完整公式的图像  
我做了一些研究和gpting，我大多了解我们在代码中正在做什么以及为什么要做。  但是根据我的数学知识 - 我可以将数学技能排名为中级 - 我将公式解释为“计算–_θ（s，ξ），然后将结果放在结果之后属于的结果，然后计算π_θ（-_θ（s，ξ）| s）。除了我理解的内容外，本文中的代码和解释以相反的顺序进行。  ＆quot从网络中获取操作，并将其发送到边界转换。 ＆quot 
我无法弄清楚问题是否是我的数学知识，而不是我的英语差，反之亦然。我对我做错了什么的想法或思考。]]></description>
      <guid>https://stackoverflow.com/questions/79420509/couldnt-understand-relation-between-math-behind-the-machine-learning-algorithm</guid>
      <pubDate>Fri, 07 Feb 2025 10:16:12 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch LSTM模型的评估非常缓慢</title>
      <link>https://stackoverflow.com/questions/79420479/very-slow-evaluation-of-a-pytorch-lstm-model</link>
      <description><![CDATA[我正在遇到一些旨在执行“音频事件识别”的LSTM模型的问题。从与犯罪相关的音频样本的数据集中。
整个网络在训练过程中似乎运行良好，获得了90％准确性的良好结果。真正的关键是评估阶段，它非常缓慢且经常超出记忆。
 bellow评估功能的代码：
在

    TORCH.CUDA.EMPTY_CACHE（）

    model.eval（）

    total_loss = 0.0
    total_correct = 0
    total_data = 0

    progress_bar = tqdm（dl，desc =; testing＆quord; uits =; batch; quot; quot;

    使用Torch.no_grad（）：
        对于枚举（DL）中的batch_index（数据，目标）：
            data = data.to（设备）
            target = target.to（设备）

            输出=模型（数据）

            损失=标准（输出，目标）

            _，预测= torch.max（输出，dim = 1）
            _，正确= torch.max（target，dim = 1）

            total_data += data.size（0）

            partial_correct = torch.sum（预测==正确）.Item（）
            partial_accuracy = partial_correct / data.size（0）
            partial_loss = loss.item（）

            total_correct += partial_correct
            total_loss += partial_loss * data.size（0）

            progress_bar.set_postfix（{{＆quert&#39;：partial_accuracy，; quot; less&#39;：partial_loss}）

    test_loss = total_loss / total_data
    test_acc = total_correct / total_data

    返回test_loss，test_acc
 
测试功能与评估或多或少相似，唯一的例外是使用 model.train（）且没有 torch.no_grad（） 代码块。
作为一种典型的ML方法，对于每个时期，我都会在此之后运行一个训练阶段和评估阶段。我应该只是训练整个模型并在最后进行测试吗？
您在我试图清除GPU缓存的代码中看到的，但这对速度也没有任何帮助，也没有“失误”。错误。]]></description>
      <guid>https://stackoverflow.com/questions/79420479/very-slow-evaluation-of-a-pytorch-lstm-model</guid>
      <pubDate>Fri, 07 Feb 2025 10:04:14 GMT</pubDate>
    </item>
    <item>
      <title>在拥有IterabledataSet和DataLoader的同时，如何计算培训和验证的准确性和损失？</title>
      <link>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</guid>
      <pubDate>Fri, 07 Feb 2025 00:48:54 GMT</pubDate>
    </item>
    <item>
      <title>RL代理学习卡住</title>
      <link>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</link>
      <description><![CDATA[我试图熟悉MATLAB的强化学习库。我正在努力创建一个可以将正弦功能学习为简单热身的代理，但我已经卡住了。问题在于，经过几次迭代后，代理人达到一定水平，然后撞到天花板，无论我让学习过程运行多长时间，都不会进一步学习。这是代码：
  obs_info = rlnumericspec（[1 1]，lowerlimit = -pi，upperlimit = pi）;
obs_info.name =＆quot“鼻窦价值＆quot”;

act_info = rlnumericspec（[1 1]，lowerlimit = -1，upperlimit = 1）;
act_info.name =“预测价值”;

reset_fcn_handle = @（）reset_train（）;
step_fcn_handle = @（action，portfolio）step_train（...
    行动，投资组合）；

sinus_train_env = rlfunctionenv（...
    obs_info，act_info，step_fcn_handle，reset_fcn_handle）;

函数[onirent_observation，portfolio] = reset_train（）
    initial_observation = 2*pi*rand（1）-pi;
    portfolio = struct;
    portfolio.lastvalue = initial_observation;
结尾

函数[next_observation，Reward，is_done，portfolioout] = step_train（...
    动作，投资组合）
    Expect_prediction = sin（portfolio.lastvalue）;
    奖励= 1 /100 /（0.01 + abs（Action -endured_prediction））;
    next_observation = 2*pi*rand（1）-pi;
    portfolioout = portfolio;
    portfolioout.lastvalue = next_observation;
    is_done = false;
结尾
 
我使用加固学习设计师来构建代理。 “兼容算法”已设置为TD3（默认选项），并且隐藏单元的数量为32。超参数和探索模型设置：
  
对于训练，最大剧集长度= 1000，平均窗口长度= 5，停止条件=平均值，停止值= 900。
30分钟后的结果：
  
1小时后的结果：
   
我试图修改奖励功能并让其运行两个小时：
 奖励= 1 /（0.01 + abs（Action -Expection_prediction））;
 
结果：
  
第二次尝试修改奖励功能：
  if（abs（Action-expected_prediction）＆gt; 0.05）
    奖励= -1;
别的 
    奖励= 1;
结尾
 
结果：
   
我缺少什么？]]></description>
      <guid>https://stackoverflow.com/questions/79419073/rl-agent-learning-stucks</guid>
      <pubDate>Thu, 06 Feb 2025 19:03:32 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8最终检测头仍输出（1、7、8400），而不是（1、8、8400）</title>
      <link>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</link>
      <description><![CDATA[我训练了3个类的Yolov8检测模型，但是原始的正向通行证仍然显示（1、7、8400）而不是（1、8、8400）的最终检测输出。
我做了什么：
检查了我的data.yaml：
  yaml
火车：路径/到/火车/图像
Val：路径/到/Val/图像
NC：3
名称：[&#39;神经胶质瘤&#39;，&#39;脑膜瘤&#39;，&#39;垂体&#39;]
 
确认的NC：3是正确的。
通过命令从头开始训练：
  bash
Yolo检测火车\
    data =路径/到/data.yaml \
    模型= yolov8x \
    epochs = 1000 \
    imgsz = 640 \
    设备= 1 \
    耐心= 100
 
训练没有错误地进行，并成功完成。
安装了最新的Ultrytics版本（v8.3.72），以确保没有版本问题：
  bash
PIP卸载超级分析
PIP安装超级词
 
直接加载了新的best.pt：
  python
从超级物质进口YOLO
导入火炬

型号= yolo（r＆quot; best.pt;）。模型
model.eval（）

dummy_input = torch.randn（1，3，640，640）
使用Torch.no_grad（）：
    输出=模型（Dummy_input）

输出输出：
    ＃一些输出是列表；仔细检查每个元素
    如果Isinstance（Out，Torch.Tensor）：
        打印（OUT.SHAPE）
    别的：
        打印（“列表输出：＆quot” [o。
 
控制台显示检测输出（1、7、8400）。
经过验证的模型元数据说NC = 3和Model.Names有3个类。但是，原始检测层输出仍然是7个通道。
观察：
如果Yolo检测层是真正的3类，则应输出（5 + 3）=每个锚点8个通道，而不是7通道。
不匹配（1、7、8400）通常表明尽管NC = 3。
问题 /请求帮助：
为什么即使我从头开始训练了3堂课，为什么还要原始检测头（1、7、8400）？
如何确保将检测层完全重新定位为（5 + 3）= 8以进行3级检测？
我已经尝试删除旧的.pt文件，重新检查我的数据。yaml，重新安装超级图像和确认模型。model.nc== 3。但是最终检测层继续产生7个频道，而不是8个频道。
关于可能导致这种持续不匹配的什么想法？
事先感谢您的任何帮助或见解！]]></description>
      <guid>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</guid>
      <pubDate>Thu, 06 Feb 2025 18:39:21 GMT</pubDate>
    </item>
    <item>
      <title>哪种AI检测工具为识别AI生成的内容提供了最准确的结果？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</link>
      <description><![CDATA[我想确保我收到或写的内容是人类生成的。有各种AI检测工具可用，但我不确定哪种是最准确和最可靠的。某些文章可能部分是AI生成的，因此很难进行检测。用于此目的的最佳工具是什么？＆quort 
我尝试过的＆amp;预期结果：
“我尝试过诸如gptzero和simpality.ai之类的工具，但我正在寻找一种更准确或广泛接受的解决方案。理想情况下，我需要一个可以以高精度检测AI编写的内容并提供可靠见解的工具。
我已经测试了诸如gptzero，siginality.ai和copyleaks AI检测器等工具。尽管他们提供了一些见解，但我注意到其结果不一致。正确标记了一些AI生成的内容，而在其他情况下，人写的文本被错误地确定为AI生成的。我期望一种工具，可以提供更高准确性，清晰的检测过程解释，并有效地分析简短和长格式的能力。 ]]></description>
      <guid>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</guid>
      <pubDate>Thu, 06 Feb 2025 17:34:58 GMT</pubDate>
    </item>
    <item>
      <title>CNN中二进制分类任务中F-1分数的不同结果</title>
      <link>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</link>
      <description><![CDATA[我正在制作用于二进制分类任务的CNN模型。

当我使用binary_crossentropy作为损失功能并将1个神经元保持在最后一层时，我的准确性约为94％，val_accuracy的85％左右，但我的F-1得分被卡在69％左右。
当我使用cancorical_crossentropy作为损失函数时，结果有些相似，但是此时间F-1分数约为85％。

  model =顺序（[[
    输入（shape =（*input_shape，1）），

    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchNormitization（），

    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    conv2d（64，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchNormitization（），
    
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchNormitization（），

    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    conv2d（128，（3，3），激活=&#39;relu&#39;，padding =; same; same＆quort; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchNormitization（），
    
    conv2d（256，（3，3），激活=&#39;relu&#39;，padding =＆quort; same; kernel_regularizer = l2（0.001）），
    conv2d（256，（3，3），激活=&#39;relu&#39;，padding =＆quort; same; kernel_regularizer = l2（0.001）），
    cbamlayer（），
    maxpooling2d（（2，2）），
    batchNormitization（），

    flatten（），
    致密（512，激活=&#39;relu&#39;），
    辍学（0.5），
    密集（256，激活=&#39;relu&#39;），
    辍学（0.2），
    致密（2，激活=&#39;softmax&#39;）
）））
 
任何人都可以告诉我为什么会发生这种情况，以及解决方案是什么。
我也想知道准确性和val_accuracy差距的原因，即使班级平衡。 
我尝试更改模型结构和损失功能，但没有解决。]]></description>
      <guid>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</guid>
      <pubDate>Thu, 06 Feb 2025 15:24:42 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的轨迹的一部分[封闭]</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个代表手机运动轨迹的数据集，并且该轨迹由步行行驶的部分和汽车旅行的轨迹组成。数据包括经度，纬度，时间戳和3轴加速度计数据。我需要提取步行旅行的所有子接头。这个问题有现成的解决方案吗？如果没有，我该如何处理此任务？
我试图在互联网上找到现成的解决方案，但没有找到任何值得的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>在Tensorflow中开发用于图像分类的预处理模型</title>
      <link>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</link>
      <description><![CDATA[我有一个关于如何修改验证模型以对3类分类而不是1000的问题。这是我到目前为止提出的两种方法。.im不确定哪个是最好的。
  nasnetmobile_model = tf.keras.applications.nasnetmobile（
                                                input_shape =（224,224,3），
                                                include_top = false，
                                                池=&#39;avg&#39;，
                                                类= 3，
                                                重量=&#39;Imagenet&#39;
                                                ）
nasnetmobile_model.trainable = false
nasnetmobile_model.summary（）键入此处
 
 in方法1，Nasnetmobile模型以预先训练的成像网权重量初始化，不包括顶层和使用平均池。该模型设置为不可训练，以防止其权重在训练期间进行更新。然后构建了一个新的顺序模型，其中包括预先训练的NASNETMOBILE模型，然后是两个密集的层：一个具有128个单元和relu激活的层，另一个具有3个单元和SoftMax激活，用于最终分类。顺序模型与ADAM优化器和稀疏的分类横向渗透丢失一起编译。最后，该模型在数据集上进行了20个时期的培训，其批次大小为4，验证分配为20％。
方法1 
  new_pretratained_model = tf.keras.sequential（）

new_pretraining_model.add（nasnetmobile_model）
new_pretrained_model.add（tf.keras.layers.dense（128，activation =&#39;relu&#39;））
new_pretrained_model.add（tf.keras.layers.dense（3，activation =&#39;softmax&#39;））

new_pretained_model.layers [0] .trainable = false
new_pretained_model.summary（）此处


new_pretratained_model.compile（
            优化器=&#39;Adam&#39;，
            损失=&#39;Sparse_categorical_crossentropy&#39;，
            指标= [&#39;准确性&#39;]
            ）

new_pretratained_model.fit（
        Xtrain，
        Ytrain，
        时代= 20，
        batch_size = 4，
        验证_split = 0.2
        ）
 
方法2 
在方法2中，功能API用于创建新模型。预先训练的NASNETMOBILE模型的输出被视为具有128个单位和relu激活的新密集层的输入，然后是具有3个单元和SoftMax激活的最终致密层。这种方法明确将NasnetMobile模型的输入连接到新的输出层，形成了一种与原始NasnetMobile模型相同的输入的新模型，但具有额外的密集层进行分类。然后将新模型与Adam Optimizer和稀疏的分类横向渗透损失一起编译，并在数据集上进行20个时期的培训，其批量大小为4，验证分配为20％。。
  nasnetmobile_model_out = nasnetmobile_model.output
x = tf.keras.layers.dense（128，activation =&#39;relu&#39;）（nasnetmobile_model_out）
输出= tf.keras.layers.dense（3，激活=&#39;softmax&#39;）（x）
model_2 = tf.keras.model（inputs = nasnetmobile_model.input，outputs =输出）

model_2.summary（）


model_2.compile（
            优化器=&#39;Adam&#39;，
            损失=&#39;Sparse_categorical_crossentropy&#39;，
            指标= [&#39;准确性&#39;]
            ）

model_2.fit（fit（
        Xtrain，
        Ytrain，
        时代= 20，
        batch_size = 4，
        验证_split = 0.2
        ）
 ]]></description>
      <guid>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</guid>
      <pubDate>Mon, 27 May 2024 16:22:12 GMT</pubDate>
    </item>
    <item>
      <title>ALS的Pyspark实施如何处理每个用户项目组合的多个评级？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到，到ALS的输入数据不需要每个用户项目组合的唯一评分。
这是一个可再现的例子。
 ＃示例数据框架
df = spark.createDataframe（[（0，0，4.0），（0，1，2.0）， 
（1，1，3.0），（1，2，4.0）， 
（2，1，1.0），（2，2，5.0）]，[&#39;用户;

DF.Show（50,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 1 | 2.0 |
| 1 | 1 | 3.0 |
| 1 | 2 | 4.0 |
| 2 | 1 | 1.0 |
| 2 | 2 | 5.0 |
+-----+----+-------+
 
您可以看到，每个用户项目组合只有一个评分（理想的情况）。
如果我们将此数据框传递到ALS，它将为您提供如下的预测：
 ＃拟合ALS
来自pyspark.ml.Recmendation Import ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.9169915 |
| 0 | 1 | 2.031506 |
| 0 | 2 | 2.3546133 |
| 1 | 0 | 4.9588947 |
| 1 | 1 | 2.8347554 |
| 1 | 2 | 4.003007 |
| 2 | 0 | 0.9958025 |
| 2 | 1 | 1.0896711 |
| 2 | 2 | 4.895194 |
+----+----+------------+
 
到目前为止，一切都有意义。但是，如果我们有一个包含多个用户项目评级组合的数据框，如以下 -  
 ＃示例daataframe
df = spark.createDataFrame（[（（0，0，4.0），（0，0，3.5），
                            （0，0，4.1），（0，1，2.0），
                            （0，1，1.9），（0，1，2.1），
                            （1，1，3.0），（1，1，2.8），
                            （1，2，4.0），（1，2，3.6），
                            （2，1，1.0），（2，1，0.9），
                            （2，2，5.0），（2，2，4.9）]，
                           [用户“
DF.Show（100,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 0 | 3.5 |
| 0 | 0 | 4.1 |
| 0 | 1 | 2.0 |
| 0 | 1 | 1.9 |
| 0 | 1 | 2.1 |
| 1 | 1 | 3.0 |
| 1 | 1 | 2.8 |
| 1 | 2 | 4.0 |
| 1 | 2 | 3.6 |
| 2 | 1 | 1.0 |
| 2 | 1 | 0.9 |
| 2 | 2 | 5.0 |
| 2 | 2 | 4.9 |
+-----+----+-------+
 
您可以在上述数据框架中看到，有一个用户项目组合的多个记录。例如 - 用户&#39;0&#39;对项目&#39;0&#39;分别额定为4.0,3.5和4.1。。
如果我将此输入数据范围传递给ALS怎么办？这个可以吗？
我最初认为它不应该工作，因为ALS应该每个用户项目组合获得唯一的评分，但是当我运行此功能时，它可以奏效并使我感到惊讶！&gt; 
 ＃拟合ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.7877638 |
| 0 | 1 | 2.020348 |
| 0 | 2 | 2.4364853 |
| 1 | 0 | 4.9624424 |
| 1 | 1 | 2.7311888 |
| 1 | 2 | 3.8018093 |
| 2 | 0 | 1.2490809 |
| 2 | 1 | 1.0351425 |
| 2 | 2 | 4.8451777 |
+----+----+------------+
 
为什么起作用？我认为它会失败，但也没有给我预测。
我尝试查看研究论文，有限的ALS源代码以及Internet上的可用信息，但找不到任何有用的东西。
是否平均需要这些不同的评分，然后将其传递给ALS或其他任何东西？
以前有人遇到过类似的事情吗？还是关于ALS如何在内部处理此类数据的任何想法？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度评分</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深层神经网络模型（以 keras 实现）来进行预测。这样的东西：
  def make_model（）：
 型号=顺序（）       
 model.Add（conv2d（20，（5,5），激活=; relu; quot;））
 model.Add（maxpooling2d（pool_size =（2,2）））    
 模型add（Flatten（））
 型号add（密集（20，激活=; relu; quot;））
 model.Add（lambda（lambda x：tf.expand_dims（x，axis = 1））））））
 model.Add（Simplernn（50，Activation =; relu; quot;））
 model.Add（密集（1，激活=; softmax;））    
 model.compile（lose =; cancorical_crossentropy＆quot＆quot＆quort＆quotizer = adagrad，zerrics = [＆quciet; cocucet＆quot;]）

 返回模型

型号= make_model（）
model.fit（x_train，y_train，validation_data =（x_validation，y_validation），epochs = 25，batch_size = 25，冗长= 1）
   
## predicon：
预测= model.predict_classes（x）
概率=型号。
 
我的问题是分类（二进制）问题。我想计算这些预测的每个置信度得分即我想知道 - 我的型号99％确定它是“ 0”。还是58％是“ 0”。
我已经找到了有关如何做的观点，但不能实施它们。我希望遵循我应该如何通过上述模型进行预测，以使我对每个预测的信心？我会很感谢一些实用的例子（最好在Keras中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>用户的多个排名中的ALS（交替平方）算法</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量的研究，我们决定使用Google云基础架构并使用ALS算法（一种协作过滤方法 -   https://cloud.google.com/solutions/solutions/recommendations-using-machine-learning-learning-compute-engine-engine-engine-wrinage--模式）在我们的产品推荐系统中，该系统在下面的详细信息中进行了解释：
我们有两种类型的客户。第一类是在附近出售产品的公司，第二类是将要从这些公司购买产品的消费者

每个消费者都有能力搜索附近的公司或通过其行业搜索公司（例如杂货，干洗，屠夫等））
 当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多个项目）
 2.1。仅查看公司资料
 2.2。将公司添加到收藏夹
 2.3。开始与公司聊天
 2.4。从公司订购
 2.5。给公司评级和评论 

所以我不理解的是：上面描述的每个项目都被确定为数据库中的一些评分列，例如：
查看公司资料：10 pts 
从公司订购：20 pts 
向公司发表明星或发表评论：20 pts 
因此，每个项目都是同一用户的单独评级。
在我们的数据库中，用户 - 公司对可能有超过1行
例如：
第1行：User18-Company18-10pts（一次查看配置文件）
第2行：User18-Company18-20pts（从公司订购）
第3行：User18-Company19-10pts 
我不确定该算法，它是计算该用户对同一公司的所有评级的总和（我确切想要的），还是只是为用户的评分寻找一行一家公司？ （我想要的是该ALS算法总结该用户 - 公司对的Row1和Row2）
有人知道吗？这对于我们的推荐系统非常重要。因为我要寻找的算法需要计算用户的所有评分总和，以便推荐另一家公司。因为我们的业务模型与电影评级系统有所不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    <item>
      <title>顺序分类软件包和算法</title>
      <link>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</link>
      <description><![CDATA[我正在尝试制作一个分类器，该分类器选择项目 i 的评分（1-5）。  对于每个项目I，我都有一个向量 x ，其中包含大约40个与 i 有关的数量。  我也对每个项目都有一个金标准评分。  根据 X 的某些功能，我想训练分类器，以给我1-5的评分，与金标准非常匹配。  
我在分类器上看到的大多数信息仅处理二进制决策，而我有一个评级决定。  是否有常见的技术或代码库来处理这种问题？ ]]></description>
      <guid>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</guid>
      <pubDate>Mon, 16 Aug 2010 16:25:53 GMT</pubDate>
    </item>
    </channel>
</rss>