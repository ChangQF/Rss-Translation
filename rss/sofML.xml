<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 09 May 2024 09:15:32 GMT</lastBuildDate>
    <item>
      <title>为什么混合精度训练中使用的梯度缩放可以导致 float32 模型中更大的学习率？</title>
      <link>https://stackoverflow.com/questions/78453294/why-gradient-scaling-used-in-mix-precision-training-could-lead-to-bigger-learnin</link>
      <description><![CDATA[背景：
Gradient Scaling原用于混合精度训练（部分模型权重为float16，部分为float 32），其目的是减少float16存储的小梯度的下溢。
最近，我在做一些实验，发现一些结果让我感到困惑。在整个模型具有 float32 权重的情况下，如果我使用梯度缩放更新模型，与不使用它相比，我可以使用更大的 LR（学习率）来进行收敛。
缩放版本更新代码摘录：
loss_scaler = torch.cuda.amp.GradScaler()
loss_scaler.scale(loss).backward()
loss_scaler.unscale_（优化器）
loss_scaler.step（优化器）
loss_scaler.update()

未缩放版本更新代码摘录：
loss.backward()
优化器.step()

问题：
为什么会发生这种情况？是否应该取消缩放后的渐变并将其转换为原始渐变？我期望的是他们的 LR 对于模型训练应该是相同的？
以下是 ChatGPT 的解释：
即使float32也可能下溢，而scale使得这些下溢梯度有助于权重更新，使训练更加稳定，因此lr可以更大。
我不知道这个解释是否合理，因为LR有很大不同。 （1.5e-4 与 1.5e-8）]]></description>
      <guid>https://stackoverflow.com/questions/78453294/why-gradient-scaling-used-in-mix-precision-training-could-lead-to-bigger-learnin</guid>
      <pubDate>Thu, 09 May 2024 08:49:42 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 KL 散度损失进行知识蒸馏。损失太高</title>
      <link>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</link>
      <description><![CDATA[KL 散度损失过高
我正在尝试进行知识蒸馏。对于我的学生损失，我使用了交叉熵损失，对于我的知识蒸馏损失，我尝试使用 KL 散度损失。
这是我用于 KL 散度损失的代码。
类 KLDivLoss(nn.Module):
def __init__(self,ignore_index=-1,reduction=“batchmean”,log_target=False):
超级（KLDivLoss，自我）.__init__()
自我还原 = 还原
self.log_target = log_target
self.ignore_index = 忽略索引

def 前向（自身，preds_S，preds_T，T = 1.0，alpha = 1.0）：
preds_T[0] = preds_T[0].detach() # 分离教师预测
pred_1 = torch.sigmoid(preds_T[0]/T) # 白色
pred_0 = 1 - pred_1
preds_teacher = torch.cat((pred_0, pred_1), 暗淡=1)
断言 preds_S[0].shape == preds_teacher.shape,“输入和目标形状必须与 KLDivLoss 匹配”
Stu_prob = F.log_softmax(preds_S[0]/T, 暗淡=1)
kd_loss = F.kl_div(stu_prob,
preds_老师，
减少=&#39;批量均值&#39;,
) * T * T
返回{&#39;损失&#39;：kd_loss}

我从中获得的价值是非常巨大的。我只是添加了学生模型中的知识蒸馏损失和交叉熵损失。由于我的 CE 损失非常小，这全部来自 KLdiv 损失。您能告诉我如何减少损失吗？或者如果我做错了什么。
在此处输入图片说明
我尝试使用 KL div 损失，温度 =1
我的教师模型以张量 [8,1,224,224] 的形式给出输出，因为它用于像素的二进制预测，而我的学生模型以 [8,2,224,224] 的形式给出输出，其中 0 属于黑色类，1 属于白色类。 
因此，为了将它们与 KL div 损失相匹配，我使用 sigmoid 函数来获取白色类的概率和黑色类的 1 - 白色概率。然后将它们连接起来形成大小为 [8,2,224,224] 的张量，该张量类似于学生张量。
然后我尝试执行 KL 散度。我得到的损失非常高]]></description>
      <guid>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</guid>
      <pubDate>Thu, 09 May 2024 06:27:53 GMT</pubDate>
    </item>
    <item>
      <title>即使遵循示例 3：分类的文档代码，PyKan 代码也无法工作</title>
      <link>https://stackoverflow.com/questions/78451382/pykan-code-not-working-even-after-following-the-documentation-code-for-example-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78451382/pykan-code-not-working-even-after-following-the-documentation-code-for-example-3</guid>
      <pubDate>Wed, 08 May 2024 22:17:49 GMT</pubDate>
    </item>
    <item>
      <title>对于我使用 GB、树和随机森林对房价进行的数据分析，我的 MSE 太高了 [关闭]</title>
      <link>https://stackoverflow.com/questions/78451297/for-my-data-analysis-on-house-prices-with-gb-tree-and-random-forest-my-mse-is</link>
      <description><![CDATA[我尝试过使用所有变量，也尝试过选择某些变量，但 MSE 仍然很高。我想知道这是否是我的代码错误。我也曾尝试添加一些特征工程，但现在将其注释掉，因为它使我的 MSE 变得更糟。
 #kaggle：房价 - 高级回归技术 
import pandas as pd
import numpy as np
import sklearn.model_selection
import matplotlib.pyplot as plt
import sklearn as sk
import sklearn.tree
import sklearn.ensemble

df = pd.read_csv(&#39;/Users/andrewhashoush/Downloads/house-prices-advanced-regression-techniques/train.csv&#39;)
df_test = pd.read_csv(&#39;/Users/andrewhashoush/Downloads/house-prices-advanced-regression-techniques/test.csv&#39;)

print(df.head())
print(df.info())

#selected_features = [&#39;OverallQual&#39;, &#39;YearBuilt&#39;, &#39;TotalBsmtSF&#39;, &#39;1stFlrSF&#39;, &#39;GrLivArea&#39;,
# &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;MSZoning&#39;, &#39;Neighborhood&#39;,
# &#39;KitchenQual&#39;, &#39;CentralAir&#39;, &#39;LotArea&#39;, &#39;MSSubClass&#39;, &#39;LotFrontage&#39;, 
# &#39;Street&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;OverallCond&#39;, &#39;RoofStyle&#39;,
# &#39;RoofMatl&#39;, &#39;BsmtQual&#39;,&#39;SaleCondition&#39;, &#39;SaleType&#39;, &#39;YrSold&#39;, &#39;MoSold&#39;, 
# &#39;PoolArea&#39;]

selected_features = [
&#39;LotFrontage&#39;、&#39;OverallQual&#39;、&#39;OverallCond&#39;、&#39;MasVnrArea&#39;、&#39;HalfBath&#39;、
&#39;BedroomAbvGr&#39;、&#39;KitchenAbvGr&#39;、&#39;GarageCars&#39;、&#39;WoodDeckSF&#39;、
&#39;OpenPorchSF&#39;、&#39;MoSold&#39;、&#39;YrSold&#39;、&#39;MSZoning&#39;、&#39;Alley&#39;、
&#39;LotShape&#39;、&#39;LandContour&#39;、&#39;LotConfig&#39;、&#39;LandSlope&#39;、&#39;Neighborhood&#39;、
&#39;Condition1&#39;、&#39;BldgType&#39;、&#39;HouseStyle&#39;、&#39;RoofStyle&#39;、&#39;Exterior1st&#39;、
&#39;Exterior2nd&#39;、&#39;MasVnrType&#39;、&#39;ExterQual&#39;、&#39;ExterCond&#39;、&#39;Foundation&#39;、
&#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;,
&#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;KitchenQual&#39;,
&#39;Functional&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageFinish&#39;, &#39;GarageQual&#39;,
&#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;PoolQC&#39;, &#39;Fence&#39;, &#39;MiscFeature&#39;,
&#39;SaleType&#39;, &#39;SaleCondition&#39;
]

#feature engineering
#df[&#39;Quality_Condition&#39;] = df[&#39;OverallQual&#39;] * df[&#39;OverallCond&#39;]
#df[&#39;Age_at_Sale&#39;] = df[&#39;YrSold&#39;] - df[&#39;YearBuilt&#39;]
#selected_features += [&#39;Quality_Condition&#39;, &#39;Age_at_Sale&#39;]

X = df[selected_features]
print(X.head())

for column in X.columns:
missing_data = df[column].isnull().sum()
print(f&quot;{column}: {missing_data}&quot;)

categorical_vars = X.select_dtypes(include=&#39;object&#39;).columns.tolist()
numeric_vars = X.select_dtypes(exclude=&#39;object&#39;).columns.tolist()

print(&quot;Categorical Variables:&quot;, categorical_vars)
print()
print(&quot;Numerical Variables:&quot;, numeric_vars)

#%%

#填充缺失值带模式的分类值
for var in categorical_vars:
mode_value = X[var].mode()[0] 
# X[var] = X[var].fillna(mode_value) 
# X.loc[:, var] = X.loc[:, var].fillna(mode_value)
X.loc[:, var] = X[var].fillna(mode_value)

#用中位数填充缺失的数值
for var in numeric_vars:
median_value = X[var].median()
X.loc[:, var] = X[var].fillna(median_value)

#检查
for column in X.columns:
missing_data = X[column].isnull().sum()
print(f&quot;{column}: {missing_data}&quot;)

#one hot 编码 
X = pd.get_dummies(X, columns=categorical_vars)
y = df[&#39;SalePrice&#39;]

#分割数据
X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X,y, train_size =.8, random_state= 123)

dt_model = sklearn.tree.DecisionTreeRegressor(max_depth=5, random_state=123)
dt_model.fit(X_train, y_train)
y_pred = dt_model.predict(X_val) 
mse_val = np.mean((y_pred - y_val)**2)
print(f&quot;MSE: {mse_val}&quot;)

# 随机森林回归器
rf_model = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=123)
rf_model.fit(X_train, y_train)
y_pred1 = rf_model.predict(X_val)
mse_val1 = np.mean((y_pred1 - y_val)**2)
print(f&quot;MSE: {mse_val1}&quot;)

# 梯度提升回归器
gb_model = sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=123)
gb_model.fit(X_train, y_train)
y_pred2 = gb_model.predict(X_val)
mse_val2 = np.mean((y_pred2 - y_val)**2)
print(f&quot;MSE: {mse_val2}&quot;)

MSE: 1451852149.6361678
MSE：944388532.5714014
MSE：755815420.2686024
]]></description>
      <guid>https://stackoverflow.com/questions/78451297/for-my-data-analysis-on-house-prices-with-gb-tree-and-random-forest-my-mse-is</guid>
      <pubDate>Wed, 08 May 2024 21:49:52 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法获取 Ultralytics YOLOv5 模型错过物体的图像？</title>
      <link>https://stackoverflow.com/questions/78449546/is-there-a-way-to-get-the-images-where-the-ultralytics-yolov5-model-missed-the-o</link>
      <description><![CDATA[我正在使用Ultralytics开发的版本（https://github.com/ultralytics/yolov5) 用于我的应用程序。它表现良好，但显然并非每次都正确。
有没有办法查看该模型失败的情况，以便我可以重新制定数据收集和/或注释技术的策略？
我尝试查看 runs/ 目录，该目录存储每次训练运行的相关部分，但无济于事。]]></description>
      <guid>https://stackoverflow.com/questions/78449546/is-there-a-way-to-get-the-images-where-the-ultralytics-yolov5-model-missed-the-o</guid>
      <pubDate>Wed, 08 May 2024 15:15:54 GMT</pubDate>
    </item>
    <item>
      <title>Yolo 模型中的增量分类器和表示学习</title>
      <link>https://stackoverflow.com/questions/78448470/incremental-classifier-and-representation-learning-in-yolo-models</link>
      <description><![CDATA[我的 YOLO 模型遇到问题。
最初，我用 7 个类对其进行了训练。现在，我想向模型添加 4 个新类。然而，当我将原始 7 个类别的数据与新的 4 个类别的数据结合起来时，训练时间和相关的云成本显着增加。有什么好的解决方案可以有效地将这些额外的类合并到模型中而不增加训练时间和成本？
我的期望是减少增量学习的成本和培训时间。]]></description>
      <guid>https://stackoverflow.com/questions/78448470/incremental-classifier-and-representation-learning-in-yolo-models</guid>
      <pubDate>Wed, 08 May 2024 12:20:14 GMT</pubDate>
    </item>
    <item>
      <title>训练网络不兼容的 Seq2Seq 问题</title>
      <link>https://stackoverflow.com/questions/78444834/seq2seq-issue-with-training-network-incompatibility</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78444834/seq2seq-issue-with-training-network-incompatibility</guid>
      <pubDate>Tue, 07 May 2024 20:06:16 GMT</pubDate>
    </item>
    <item>
      <title>需要使用 Rand_forest 和 h2o 进行预测的指导</title>
      <link>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</link>
      <description><![CDATA[我有一个随机森林模型，我正在尝试更好地理解它。
为了举例，假设我们有一片蓝莓灌木丛。我们感兴趣的是预测特定灌木丛中腐烂蓝莓的产量以及各个灌木丛中所有蓝莓的收获量。
每个灌木都有一个识别名称：bush_name，例如&#39;bush001&#39;，我们希望根据每个单独的灌木进行预测。例如，我想知道 Bush025 是否在 2/2/22 生产了腐烂的浆果。
为了本示例，输入位于具有以下虚拟结构的 df 中：
train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05&quot; -01”、“2022-11-01”、“2022-11-02”)、
                   Bush_name = c(“bush001”、“bush001”、“bush001”、“bush043”、“bush043”、“bush043”),
                   错误 = c(2, 0, 1, 0, 3, 1),
                   有腐烂的浆果 = c(1, 0, 0, 1, 1, 0),
                   浆果计数 = c(12, 1, 7, 100, 14, 4),
                   天气 = c(1, 0, 2, 0, 1, 1))

我已经建立了一个随机森林模型，并进行了以下高级设置：
图书馆(agua)
图书馆（防风草）
图书馆（水）

h2o.init(n线程 = -1)

model_fit &lt;- rand_forest(mtry = 10, trees = 100) %&gt;%
  set_engine(“h2o”) %&gt;%
  set_mode(“分类”) %&gt;%
  适合（has_rotten_berry ~ .,
      数据 = train_data) %&gt;%
  step_dummy(灌木名称) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_normalize(all_predictors())

训练后我确实收到了这条消息：
警告消息：
在 .h2o.processResponseWarnings(res) 中：
  删除坏列和常量列：[bush_name]。

我想知道的是：
当我尝试预测训练模型中的新数据时，似乎我只能使用我已经训练过的灌木丛的 Bush_names 输入新的测试数据。 我假设该模型正在创建特定于灌木丛的预测是否正确？因此必须在训练中输入新的灌木丛信息才能输出这些新灌木丛的未来预测？
示例：我种植了一棵新灌木，bush700，它不存在于原始训练数据集中。如果我尝试使用新的灌木丛数据进行预测，但训练数据中不存在该数据，则会向我传达一条消息：数据中存在新的级别。所以我假设因为这些预测似乎是特定于灌木丛的，并且我们无法为新添加的灌木丛获得任何新的灌木丛预测。
这个假设正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</guid>
      <pubDate>Tue, 07 May 2024 16:58:00 GMT</pubDate>
    </item>
    <item>
      <title>最适合实时季节性数据峰值检测的机器学习模型或统计指标是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</link>
      <description><![CDATA[我正在尝试创建一种用于实时功耗数据分析的算法。目标很简单：实时检测建筑物中的功耗峰值/尖峰，并采取必要的措施吸收峰值。
数据以功率值 (kW) 流的形式出现，每 10 分钟采样一次。我有一年多的功耗数据集。
数据具有很强的季节性，建筑物的耗电量根据天气、人流量、节假日的不同而变化很大：许多未知参数。
我可以将功率峰值定义为相对值功率的快速激增，与建筑物的季节性趋势无关，持续时间不会超过一个小时（例如：超出正常值的异常）。
这个定义是非常相对的。在图表上，很容易区分峰值。有了数学规则，它就会变得更加复杂，特别是考虑到单个阈值或标准差。
这是两天的数据图表。蓝色部分为功耗。红色表示手动解释的峰值。
正如您所看到的，定义峰值比定义常态或“季节性”更难。一月的峰值与七月的峰值不同。它们没有相同的价值，也没有相同的起源。
这使得很难使用标准差、平均值或阈值等基本工具进行识别。
有几种检测峰值的方法，基于统计和机器学习。我尝试了两者，但我有点迷失了。
第一种方法是我所说的统计方法：比较一个样本的标准差（z 分数）、高于平均值或指数平均值的阈值。 （例如：此处）
那里的问题是不可避免地引入滞后，这使得模型对功耗的快速但季节性的增加以及对“噪声”的敏感性敏感。以及所述峰值对均值的不良影响。基本上，它无法处理建筑物的基本行为。
机器学习对于应对趋势很有用。这让我想到了异常检测模型。乍一看，他们可以了解什么是正常行为并检测异常值+他们大多不受监督，当现实生活中异常值的定义很粗略时，这是一件好事。
当然，我尝试了一下。我使用了 PySad 库并开始尝试以下模型 指南。到目前为止，移动窗口上的 IForest 模型似乎工作得最好，AUROC 分数接近 1。（如果该值低于样本平均值，我通过清空异常分数来消除坑）。
我也尝试过 LOFP 和 KNNCAD，但结果很差。
这里的问题是模型的准确度根据数据集的不同而变化很大，在不太明显的峰值上低至 (AUROC)0,65。
我现在的问题如下：您对用于实时准确检测峰值的合适的 ML 模型或统计方法有什么见解或想法吗？ （自动编码器、CWT、LOF、SVM？）或者您知道更好的方法吗？
我计划结合多种方法，但我觉得将方法准确性的随机性放在一起不会给我带来更好的结果。
我还必须考虑到调整 SARIMA 等高级模型的超参数是不可能的，因为使用该算法的人不是机器学习专家或数据分析师。]]></description>
      <guid>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</guid>
      <pubDate>Tue, 07 May 2024 13:26:16 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：列表索引超出streamlit范围[关闭]</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 URL 中提取信息并从中学习。然后，用户可以向模型询问与 URL 中的文章相关的问题，模型将提供合适的答案。
我在笔记本上执行了此操作，效果非常好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误。我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块：
 main_placeholder = sl.empty()
    llm = ChatGoogleGenerativeAI(模型 = &#39;gemini-pro&#39;)
    如果 process_url_clicked:
        加载器 = UnstructedURLLoader(urls = urls)
        main_placeholder.text(&quot;数据加载...开始...✅✅✅&quot;)
        数据 = 加载器.load()
        text_splitter = RecursiveCharacterTextSplitter(
            分隔符 = [&#39;\n&#39;,&#39;\n\n&#39;,&#39;.&#39;,&#39;,&#39;],
            块大小 = 1000,
            块重叠 = 200
        ）
        main_placeholder.text(&quot;文本分割器...开始...✅✅✅&quot;)
        文档 = text_splitter.split_documents(数据)
        嵌入 = GoogleGenerativeAIEmbeddings(模型 = &#39;models/embedding-001&#39;)
        矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 Streamlit 应用程序的回溯：
&lt;块引用&gt;
索引错误：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
exec（代码，模块。字典）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，位于
vectorstore_openai = FAISS.from_documents（文档，嵌入）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
返回 cls.__from(
^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
索引 = faiss.IndexFlatL2(len(embeddings[0]))
~~~~~~~~~~^^^

这在我的笔记本上完美运行，我很困惑为什么会发生这种情况。]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>用最少层数训练绝对函数的神经网络</title>
      <link>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</link>
      <description><![CDATA[我正在尝试训练神经网络来学习 y = |x|功能。我们知道，绝对函数有两条不同的线在零点处相互连接。所以我尝试使用以下顺序模型：
隐藏层：
2 致密层（激活relu）
输出层：
1 致密层
训练模型后，它只拟合函数的一半边。大多数时候是右手边，有时是左手边。一旦我在隐藏层中再添加 1 层，那么我就用 3 层代替 2 层，它就完全符合该功能了。谁能解释为什么当绝对函数只有一次切割时需要额外的一层？
这是代码：
将 numpy 导入为 np


X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# 重塑数据以适应模型输入
X = X.reshape(-1, 1)
Y = Y.重塑(-1, 1)

将张量流导入为 tf
将张量流导入为 tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 构建模型
模型 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;,metrics=[&#39;mae&#39;])
model.fit(X, Y, epochs=1000)
# 使用模型进行预测
Y_pred = model.predict(X)

# 绘制结果
plt.scatter(X, Y, color=&#39;blue&#39;, label=&#39;实际&#39;)
plt.scatter(X, Y_pred, color=&#39;red&#39;, label=&#39;预测&#39;)
plt.title(&#39;实际与预测&#39;)
plt.xlabel(&#39;X&#39;)
plt.ylabel(&#39;Y&#39;)
plt.图例()
plt.show()

2 个密集层的绘图：

3 个密集层的绘图：
]]></description>
      <guid>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</guid>
      <pubDate>Thu, 11 Apr 2024 15:34:01 GMT</pubDate>
    </item>
    <item>
      <title>将自定义元数据（边界框）嵌入到 HLS 视频流中</title>
      <link>https://stackoverflow.com/questions/69728455/embed-custom-meta-data-bounding-boxes-into-hls-video-stream</link>
      <description><![CDATA[我想将边界框、标签等嵌入到实时视频流中，然后可以选择在网络浏览器的客户端上绘制它们，并寻找一种方法来实现。 
您能否给我一个提示，告诉我如何“动态”插入元数据？可以使用哪个播放器？]]></description>
      <guid>https://stackoverflow.com/questions/69728455/embed-custom-meta-data-bounding-boxes-into-hls-video-stream</guid>
      <pubDate>Tue, 26 Oct 2021 18:47:39 GMT</pubDate>
    </item>
    <item>
      <title>sklearn ImportError：无法导入名称 plot_roc_curve</title>
      <link>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</link>
      <description><![CDATA[我正在尝试按照示例。但是，以下导入在 python2 和 python3 中都会出现 ImportError。
从 sklearn.metrics 导入plot_roc_curve

错误：
回溯（最近一次调用最后一次）：
  文件“”，第 1 行，在  中
导入错误：无法导入名称plot_roc_curve

python-2.7 sklearn 版本：0.20.2。
python-3.6 sklearn 版本：0.21.3。
我发现以下导入工作正常，但与 plot_roc_curve 不太一样。
from sklearn.metrics import roc_curve

plot_roc_curve 是否已弃用？有人可以尝试一下代码并让我知道 sklearn 版本是否有效吗？]]></description>
      <guid>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</guid>
      <pubDate>Thu, 20 Feb 2020 13:44:41 GMT</pubDate>
    </item>
    <item>
      <title>如何从 DataLoader 获取样本的文件名？</title>
      <link>https://stackoverflow.com/questions/56699048/how-to-get-the-filename-of-a-sample-from-a-dataloader</link>
      <description><![CDATA[我需要用我训练的卷积神经网络的数据测试结果编写一个文件。数据包括语音数据采集。文件格式需要是“文件名，预测”，但我很难提取文件名。我这样加载数据：
导入 torchvision
从 torchvision 导入转换
从 torch.utils.data 导入 DataLoader

测试数据路径 = ...

反式 = 变换.Compose([
    变换.ToTensor(),
    变换.Normalize((0.1307,),(0.3081,))
]）

test_dataset = torchvision.datasets.MNIST(
    根=测试数据路径，
    火车=假，
    变换=反式，
    下载=真
）

test_loader = DataLoader（数据集=test_dataset，batch_size=1，shuffle=False）

我正在尝试按如下方式写入文件：
f = open(&quot;test_y&quot;, &quot;w&quot;)
使用 torch.no_grad()：
    对于 i，枚举（test_loader，0）中的（图像，标签）：
        输出=模型（图像）
        _, 预测 = torch.max(outputs.data, 1)
        文件 = os.listdir(TEST_DATA_PATH + &quot;/all&quot;)[i]
        格式 = 文件 + &quot;, &quot; + str(predicted.item()) + &#39;\n&#39;
        f.write(格式)
f.close()

os.listdir(TESTH_DATA_PATH + &quot;/all&quot;)[i] 的问题是它与 test_loader 的加载文件顺序不同步。我能做什么？]]></description>
      <guid>https://stackoverflow.com/questions/56699048/how-to-get-the-filename-of-a-sample-from-a-dataloader</guid>
      <pubDate>Fri, 21 Jun 2019 07:48:32 GMT</pubDate>
    </item>
    <item>
      <title>控制 Scikit Learn 中逻辑回归的阈值</title>
      <link>https://stackoverflow.com/questions/28716241/controlling-the-threshold-in-logistic-regression-in-scikit-learn</link>
      <description><![CDATA[我正在对一个高度不平衡的数据集使用 scikit-learn 中的 LogisticRegression() 方法。我甚至将 class_weight 功能设置为 auto。
我知道在 Logistic 回归中应该可以知道特定类对的阈值是多少。
是否可以知道 LogisticRegression() 方法设计的每个 One-vs-All 类中的阈值是多少？
我在文档页面中没有找到任何内容。
它是否默认将 0.5 值作为所有类的阈值，而不管参数值是多少？]]></description>
      <guid>https://stackoverflow.com/questions/28716241/controlling-the-threshold-in-logistic-regression-in-scikit-learn</guid>
      <pubDate>Wed, 25 Feb 2015 10:11:33 GMT</pubDate>
    </item>
    </channel>
</rss>