<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 19 Jun 2024 15:16:32 GMT</lastBuildDate>
    <item>
      <title>Mathematica 查询中的旋转量子比特神经网络</title>
      <link>https://stackoverflow.com/questions/78642555/rotating-qubit-neural-network-in-mathematica-query</link>
      <description><![CDATA[我正在研究的基本物理示例问题（尝试使用 Mathematica 了解基本神经网络的使用），如 Nolan 的论文（见图 2a） 中所述，是一个绕 y 轴旋转的量子比特，其中旋转间隔离散化为 $\theta_j \in [0,π]$。该设置涉及在 z 基础上测量的 y 旋转量子比特（因此自旋向上和自旋向下投影仪测量）。此场景涉及使用测量结果概率为每个离散旋转角度生成训练数据（使用 Mathematica 中的随机数生成器）。然后，我通过将自旋向上和自旋向下的元组与表示旋转角度的独热向量相关联来训练网络。
请参阅以下代码示例。最终图使用单个自旋向上和自旋向下的结果测试网络，即 {{1,0}} 和 {{0,1}}，虽然显示了正确的总体趋势，但与 Nolan 的论文中显示的趋势不一致（我在代码后显示了它们）。非常感谢您的帮助，请随时要求澄清任何问题。
(*Y 旋转量子比特上的自旋向上/自旋向下测量概率 *)

Pu[\[Theta]_] := (Cos[\[Theta]/2])^2; 
Pd[\[Theta]_] := 1 - Pu[\[Theta]]

(*评估每个 theta 值的测量概率*)

pr = Table[
List[N[FullSimplify[Re[Pu[\[Theta]]]]], 
N[FullSimplify[Re[Pd[\[Theta]]]]]], {\[Theta], \[Pi]/
40, \[Pi], \[Pi]/40}]
(*为每个 theta_j 生成训练测量结果，（使用随机数生成器），1 表示旋转向上结果，0 \
表示旋转向下*)

m = 1000;

meas = Table[{}, {Length[pr]}];
Table[Do[ b = ConstantArray[0, 1]; Clear[r];
updateTally[] := Block[{r}, r = RandomReal[];
如果[r &lt; pr[[j]][[1]], b[[1]] += 1, 
如果[pr[[j]][[1]] &lt;= r &lt; pr[[j]][[1]] + pr[[j]][[2]], 
b[[1]] += 0]]];
Do[updateTally[], {1}]; AppendTo[meas[[j]], b], {i, 1, m}], {j, 
Length[pr]}];

meas = Partition[Flatten[meas], m];
(*计数 1 和 0 的函数*)
countOnesZeros[list_] := {Count[list, 1], Count[list, 0]}

meas = Map[countOnesZeros, meas]

(*将 \[Theta] 离散化为 40 个 bin，每个 bin 测量 m = 10000*)

binToOneHot[bins_List, totalBins_Integer] := 
Table[If[i == #, 1, 0], {i, 1, totalBins}] &amp; /@ bins
bins = Range[40]; 
oneHotEncoded = binToOneHot[bins, 40]

trainingData = Rule @@@ Transpose[{meas, oneHotEncoded}]

(*这是 Nolan 论文 Fig2a 中的一个示例测试*)

su = {{1, 0}}

sd = {{0, 1}}

trainedNet /@ su

trainedNet /@ sd

Show[ListPlot[trainedNet /@ su, PlotStyle -&gt; Blue], 
ListPlot[trainedNet /@ sd, PlotStyle -&gt; Green], PlotRange -&gt; All]

]]></description>
      <guid>https://stackoverflow.com/questions/78642555/rotating-qubit-neural-network-in-mathematica-query</guid>
      <pubDate>Wed, 19 Jun 2024 12:36:15 GMT</pubDate>
    </item>
    <item>
      <title>了解 ICML 2013 鲸鱼挑战赛 - 露脊鲸重现中的数据泄露文章</title>
      <link>https://stackoverflow.com/questions/78642417/understanding-the-data-leakage-article-on-the-icml-2013-whale-challenge-right</link>
      <description><![CDATA[我在尝试进一步了解数据泄露时偶然发现了这篇文章。 https://www.kaggle.com/competitions/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865
虽然这里确实突出了要点，但我仍然不明白数据泄露发生在哪里。需要一些帮助来了解这篇文章如何识别数据泄露吗？]]></description>
      <guid>https://stackoverflow.com/questions/78642417/understanding-the-data-leakage-article-on-the-icml-2013-whale-challenge-right</guid>
      <pubDate>Wed, 19 Jun 2024 12:05:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习混淆矩阵 矩阵解释</title>
      <link>https://stackoverflow.com/questions/78641974/machine-learning-confusion-matrix-explanation-of-matrix</link>
      <description><![CDATA[`下面显示了几个混淆矩阵。TP、FP、FN 和 TN 已标记（使用 ChatGPT）
我知道对角线代表 TP。但是 FP、FN 和 TN 又如何呢？ TN 被识别（或区分）了吗？
请用矩阵解释您的答案
# [[TP=12 FP=0 FN=0]
# [FN=0 TP=7 FP=2]
# [FN=0 FN=0 TP=9]]

# [[TP=12 FP=1 FP=2]
# [FN=3 TP=7 FP=4]
# [FN=5 FP=1 TP=9]]

# [[TP_0=12 FP_1=1 FP_2=2 TN_3=0]
# [FN_0=3 TP_1=7 FP_2=4 FP_3=1]
# [FN_0=5 FP_1=1 TP_2=9 FP_3=2]
# [TN_0=0 FN_1=2 FN_2=3 TP_3=11]]

我试过 ChatGPT。
它用 TP、FP、FN 和 TN 标记矩阵。但没有得到适当的解释
解释我们如何在混淆矩阵中标记 FP、FN 和 TN`]]></description>
      <guid>https://stackoverflow.com/questions/78641974/machine-learning-confusion-matrix-explanation-of-matrix</guid>
      <pubDate>Wed, 19 Jun 2024 10:32:46 GMT</pubDate>
    </item>
    <item>
      <title>PyGAD 遗传手段 - 惩罚簇大小</title>
      <link>https://stackoverflow.com/questions/78641636/pygad-genetic-means-punishing-cluster-size</link>
      <description><![CDATA[我正在研究一个问题，从我之前的研究来看，这个问题困扰了我所在行业的许多人。在市政工程中，特别是光纤电缆，房屋连接 (HC) 需要分组为连接到网络分配器 (ND) 的集群。直观地讲，人们可能会认为这对于简单的聚类算法（例如 kmeans）来说是一个问题，然而，问题出现在基础设施强加的每 ND 20 个 HC 的限制上。如果某个区域的 HC 过于密集，kmeans 等会构建超出此限制的集群。
据我所知，唯一可用的商业解决方案是 ArcGIS 的构建平衡区域算法，但是，为了 FOSS，我个人希望避免使用 ArcGIS。不过，值得庆幸的是，ESRI 确实提供了有关 bbs 如何工作的背景信息，这就是我一直在阅读遗传算法并最终使用 Gad, A.F. 的 PyGAD 的原因。他的教程对我帮助很大，但是，我正在努力调整适应度函数以满足我的需求，即惩罚导致集群超过 20 个种群的解决方案。
原始函数如下，并根据其产生的集群密度对解决方案进行评分，即它相当于 kmeans：
def fitness_func(solution, solution_idx):
cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist = cluster_data(solution, solution_idx)

fitness = 1.0 / (np.sum(clusters_sum_dist) + 0.00000001)

返回 fitness

最佳解决方案是 [ 6.76857432 87.25666069 82.82788371 84.71676014 17.01672128 41.12036676
6.71771791 9.91987939 83.17725224 28.87028453 87.64197362 53.89675135
29.77371507 64.36472386 58.77172181 59.66950147 40.07732671 83.47739475
59.29050269 13.28871487]
最佳解决方案的适应度为 0.0004500494908839888
100 代后找到的最佳解决方案

根据 Gad, A.F. 生成的 K-means-clusters
我本想为超过 20 的簇大小添加一个惩罚项，但是，我这样做的方式似乎会平等地惩罚所有染色体（我希望我正确使用这个术语，我对遗传算法仍然有点陌生）。
def capped_clusters(solution, solution_idx):
cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist = cluster_data(solution, solution_idx)

fitness = 1.0 / (np.sum(clusters_sum_dist) + 0.00000001) #0 为最优

if any(element &gt; 20 for element in [len(clu​​ster) for cluster in clusters]):
fitness = 1
else:
pass

return fitness

最佳解决方案是 [66.16381384 94.42791405 34.78040731 64.66917082 90.50249179 24.68926304
7.54327271 66.90232116 55.22703763 88.59661014 1.87161791 93.35067003
39.58200881 57.61364249 65.91865205 16.62963165 4.22783108 67.18573653
15.57759318 9.46596451]
最佳解决方案的适应度为 1
0 代后找到最佳解决方案

我如何有选择地惩罚超过 20 的聚类大小，以推动算法朝着有利于 20 个以下成员的聚类问题解决方案的方式发展？
注意：我已降级到 PyGAD 2.10.0，以便与上面链接中的教程代码兼容。这要求我手动从源代码中删除对过时的 numpy.int 和 numpy.float 的任何提及。
干杯，非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78641636/pygad-genetic-means-punishing-cluster-size</guid>
      <pubDate>Wed, 19 Jun 2024 09:27:12 GMT</pubDate>
    </item>
    <item>
      <title>GPU 张量上的内向旋转环面</title>
      <link>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</guid>
      <pubDate>Wed, 19 Jun 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用树提升模型处理多输出回归的 3d 输入数据？</title>
      <link>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</link>
      <description><![CDATA[我有 32 种商品 x 27 种特征 x 3000 个时间步长的数据用于股票预测。出于某种原因，我必须使用各种 树提升模型（lightgbm、xgboost、adaboost）来比较回归任务的性能：预测单个时间步长上每只股票的回报率，并对之前时间步长的样本进行训练。预期的数据流将是：在 n 个时间步长上训练的模型，输出单个时间步长（例如 t+2）上每种商品的回报率向量（32 个元素向量）。但是，我不确定如何构造数据以进行有效的训练。
现在，我沿着商品和特征维度展平数据以创建 3000x864 数据框，但我认为这不是可行的方法，因为我怀疑我丢失了一些信息。
我使用 sklearn 的 MultiOutputRegressor 来适应 lgbm，但 mse 约为 0.5，对于预测在 -1 和 1 之间标准化的值非常不利，因为数据包含许多易于预测的 0。
这种展平方法是处理 3d 数据的正确方法吗？还是我需要使用其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</guid>
      <pubDate>Wed, 19 Jun 2024 08:00:34 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络 (CNN) 在决策中使用黑色背景 - LIME</title>
      <link>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</link>
      <description><![CDATA[我是一名正在做学校项目的学生，需要帮助。
我的二元分类卷积神经网络在验证数据上具有非常高的准确率 (&gt;96%)，在测试数据集上的表现也同样出色。然而，当我使用 LIME 可视化图像中对其决策很重要的部分时，它往往会突出显示背景。所以我的问题是：
为什么会这样，以前有人见过吗？
当它在做决定时实际上是看着黑色面具时，它是如何达到 96% 的准确率的？
我在图像上应用黑色面具的原因是，我得到的整个数据集具有完全相同的背景，即白色滚轮，并且正如您从我上传的其中一张图片中看到的那样，该模型在决策过程中严重依赖滚轮，因此我将背景预处理为完全黑色 (0, 0, 0)RGB 像素，但现在模型似乎以某种方式使用了它。
我只是被难住了，非常感谢任何帮助！
模型架构
滚轮问题示例
我尝试了各种架构，其中一些使用 keras 层构建，甚至尝试了预训练的 ResNet50。我还改变了大多数重要的超参数，但行为仍然存在。如果有帮助，我可以提供任何细节。
我很感激任何提前提供的帮助！:)
]]></description>
      <guid>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</guid>
      <pubDate>Wed, 19 Jun 2024 00:05:44 GMT</pubDate>
    </item>
    <item>
      <title>我是否需要在每个 Spark 节点上安装 PyTorch，还是只需要主节点来运行示例？</title>
      <link>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</link>
      <description><![CDATA[我是否需要在 Apache Spark 集群的每个节点上安装 PyTorch，还是仅在主节点上安装它就足以顺利执行示例？
我尝试在 Apache Spark 集群上执行 PyTorch 示例，而无需在所有节点上安装 PyTorch，期望仅在主节点上安装就足以实现无缝执行。]]></description>
      <guid>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</guid>
      <pubDate>Tue, 18 Jun 2024 21:47:13 GMT</pubDate>
    </item>
    <item>
      <title>如何处理 ADB ML Workload 中的内存不足错误</title>
      <link>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</link>
      <description><![CDATA[我的预测数据框有近 200 个特征，因此我得到了 OOM（内存不足错误）。我正在使用 Azure databricks，我该如何处理这个错误？
我已经进行了特征选择以减少特征数量，但我仍然必须使用 100 多个特征。我尝试过广播，但没有解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</guid>
      <pubDate>Tue, 18 Jun 2024 13:19:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit-learn 在 Python 中对未标记数据实现层次聚类？</title>
      <link>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</link>
      <description><![CDATA[我正在学习聚类，在尝试查找带有标记数据的数据库时遇到了一些问题，这对我来说是一个限制，因为我发现了非常有趣的未标记数据集。我读过各种无监督聚类技术，并想实现层次聚类。
我将数据加载到 pandas DataFrame 中，对数据进行标准化并应用层次聚类。然后我可视化了树状图，但我不确定如何解释结果或我是否使用了正确的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</guid>
      <pubDate>Sat, 15 Jun 2024 04:03:59 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
解决方案：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
MSE = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); % Luko 等人的 Eq. 9。
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号
MSE(i) = (1/T)*sum((y_i&#39;-y_i_target).^2); % 均方误差
end
[minval, minidx] = min(MSE);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。
正如 BillBokeey 所强调的那样，所需的方程只是 Luko 等人提出的方程 9 的迭代版本。要进行训练，必须将方程 9 应用于训练数据集中的每个目标信号，并选择最小化均方误差 (MSE) 的结果 W_out。
最终更新：
我仍在寻找我的第二个解决方案的验证。我特别担心的是，我正在挑选出最小化 MSE 的最佳 Wout_i N x 1 向量，并有效地忽略所有其他 Wout_i 向量。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何从句子中删除不带有积极或消极情绪的单词？</title>
      <link>https://stackoverflow.com/questions/71284177/how-to-remove-words-from-a-sentence-that-carry-no-positive-or-negative-sentiment</link>
      <description><![CDATA[我正在尝试一种基于情感分析的方法来分析 youtube 评论，但评论中很多时候都有像 mrbeast、tiger/&#39;s、lion/&#39;s、pewdiepie、james 等这样的词，这些词并没有给句子增添任何感觉。我已经尝试过 nltk 的 average_perception_tagger，但效果不佳，因为它给出了如下结果
我的输入：
&quot;mrbeast james lion tigers bad sad clickbait fight nice good&quot;

我的句子中需要的单词：
&quot;bad sad clickbait fight nice good&quot;

我使用 average_perception_tagger 得到的结果：
[(&#39;mrbeast&#39;, &#39;NN&#39;),
(&#39;james&#39;, &#39;NNS&#39;),
(&#39;lion&#39;, &#39;JJ&#39;),
(&#39;tigers&#39;, &#39;NNS&#39;),
(&#39;bad&#39;, &#39;JJ&#39;),
(&#39;sad&#39;, &#39;JJ&#39;),
(&#39;clickbait&#39;, &#39;NN&#39;),
(&#39;fight&#39;, &#39;NN&#39;),
(&#39;nice&#39;, &#39;RB&#39;),
(&#39;good&#39;, &#39;JJ&#39;)]


因此，如您所见，如果我删除 mrbeast 即 NN，clickbait、fight 等词也会被删除，最终会从该句子中删除表达。]]></description>
      <guid>https://stackoverflow.com/questions/71284177/how-to-remove-words-from-a-sentence-that-carry-no-positive-or-negative-sentiment</guid>
      <pubDate>Sun, 27 Feb 2022 10:59:01 GMT</pubDate>
    </item>
    <item>
      <title>如何计算空白标记预测的 transformers 损失？</title>
      <link>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</link>
      <description><![CDATA[我目前正在尝试实现一个转换器，但无法理解其损失计算。
我的编码器输入查找 batch_size=1 和 max_sentence_length=8，如下所示：
[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

我的解码器输入如下所示（德语到英语）：
[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]

假设我的转换器预测了这些类概率（仅显示类概率最高的类的单词）：
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

现在我使用以下方法计算损失：
loss = categorical_crossentropy(
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
[[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)

这是计算损失的正确方法吗？我的转换器总是预测下一个单词的空白标记，我认为这是因为我的损失计算有误，在计算损失之前必须对空白标记进行一些处理。]]></description>
      <guid>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</guid>
      <pubDate>Sun, 07 Mar 2021 15:51:16 GMT</pubDate>
    </item>
    <item>
      <title>sample.int(m, k) 中的错误：无法抽取大于总体的样本</title>
      <link>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</link>
      <description><![CDATA[首先，我要说的是，我对机器学习、kmeans 和 r 还很陌生，这个项目是一种学习更多这方面知识的方法，也是将这些数据呈现给我们的 CIO 的一种方式，这样我就可以在开发新的帮助台系统时使用它。
我有一个 60K 行的文本文件。该文件包含教师在 3 年期间输入的帮助台工单的标题。
我想创建一个 r 程序，获取这些标题并创建一组类别。例如，与打印问题相关的术语，或与投影仪灯泡相关的一组术语。我使用 r 打开文本文档，清理数据，删除停用词和其他我认为不必要的词。我已获得频率 &gt;= 400 的所有术语列表，并将其保存到文本文件中。
但现在我想将（如果可以完成或合适）kmeans 聚类应用于同一数据集，看看我是否可以提出类别。
下面的代码包括将写出使用的术语列表 &gt;= 400 的代码。它位于末尾，并被注释掉。
library(tm) #加载文本挖掘库
library(SnowballC)
options(max.print=5.5E5) 
setwd(&#39;c:/temp/&#39;) #将 R 的工作目录设置为靠近我的文件的位置
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #检查输入了什么
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;) 
myStopwords &lt;- c(stopwords(&#39;english&#39;), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))

dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}

#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
# ncolumns = 1,
# append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)

当我使用 RStudio 运行此程序时，我得到：
&gt; 结果 &lt;- kmeans(m_norm,25)


sample.int(m, k) 中的错误：当“replace = FALSE”时无法获取大于总体的样本

我不太清楚这是什么意思，而且我在网上没有找到很多关于此的信息。有什么想法吗？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</guid>
      <pubDate>Tue, 09 Sep 2014 12:55:28 GMT</pubDate>
    </item>
    </channel>
</rss>