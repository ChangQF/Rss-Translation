<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 30 Oct 2024 09:18:07 GMT</lastBuildDate>
    <item>
      <title>训练 IP-Adapter plus 模型后的推理错误</title>
      <link>https://stackoverflow.com/questions/79140091/inference-error-after-training-an-ip-adapter-plus-model</link>
      <description><![CDATA[我从以下位置下载了软件包
https://github.com/tencent-ailab/IP-Adapter

运行命令来训练 IP-Adapter plus 模型（输入：文本 + 图像，输出：图像）：
accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; \
tutorial_train_plus.py \
--pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; \
--image_encoder_path=&quot;models/image_encoder/&quot; \
--data_json_file=&quot;assets/prompt_image.json&quot; \
--data_root_path=&quot;assets/train/&quot; \
--mixed_precision=&quot;fp16&quot; \
--resolution=512 \
--train_batch_size=2 \
--dataloader_num_workers=4 \
--learning_rate=1e-04 \
--weight_decay=0.01 \
--output_dir=&quot;out_model/&quot; \
--save_steps=3

训练过程中，出现提示，但训练可以继续：
已删除共享张量 {&#39;adapter_modules.27.to_k_ip.weight&#39;, &#39;adapter_modules.1.to_v_ip.weight&#39;, &#39;adapter_modules.31.to_k_ip.weight&#39;, &#39;adapter_modules.15.to_k_ip.weight&#39;, &#39;adapter_modules.31.to_v_ip.weight&#39;, &#39;adapter_modules.11.to_k_ip.weight&#39;, &#39;adapter_modules.23.to_k_ip.weight&#39;, &#39;adapter_modules.3.to_k_ip.weight&#39;, &#39;adapter_modules.25.to_v_ip.weight&#39;, &#39;adapter_modules.21.to_k_ip.weight&#39;, &#39;adapter_modules.17.to_v_ip.weight&#39;, &#39;adapter_modules.13.to_k_ip.weight&#39;, &#39;adapter_modules.17.to_k_ip.weight&#39;, &#39;adapter_modules.19.to_v_ip.weight&#39;, &#39;adapter_modules.13.to_v_ip.weight&#39;, &#39;adapter_modules.7.to_v_ip.weight&#39;, &#39;adapter_modules.7.to_k_ip.weight&#39;, &#39;adapter_modules.29.to_k_ip.weight&#39;, &#39;adapter_modules.3.to_v_ip.weight&#39;, &#39;adapter_modules.5.to_v_ip.weight&#39;, &#39;adapter_modules.21.to_v_ip.weight&#39;, &#39;adapter_modules.5.to_k_ip.weight&#39;, &#39;adapter_modules.23.to_v_ip.weight&#39;, &#39;adapter_modules.25.to_k_ip.weight&#39;, &#39;adapter_modules.1.to_k_ip.weight&#39;, &#39;adapter_modules.9.to_v_ip.weight&#39;, &#39;adapter_modules.9.to_k_ip.weight&#39;, &#39;adapter_modules.15.to_v_ip.weight&#39;, &#39;adapter_modules.27.to_v_ip.weight&#39;, &#39;adapter_modules.29.to_v_ip.weight&#39;, &#39;adapter_modules.19.to_k_ip.weight&#39;, &#39;adapter_modules.11.to_v_ip.weight&#39;}。这应该没问题，但请检查重新加载时是否收到任何警告

训练完成后，转换权重以生成 ip_adapter.bin，然后使用此文件中的以下模型路径运行推理代码 ip_adapter-plus_demo.py：
base_model_path = &quot;SG161222/Realistic_Vision_V4.0_noVAE&quot;
vae_model_path = &quot;stabilityai/sd-vae-ft-mse&quot;
image_encoder_path = &quot;models/image_encoder&quot;
ip_ckpt = &quot;out_model/demo_plus_checkpoint/ip_adapter.bin&quot;

显示错误：
raise RuntimeError(&#39;Error(s) in loading state_dict for {}:\n\t{}&#39;.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
state_dict 中缺少键：&quot;1.to_k_ip.weight&quot;, &quot;1.to_v_ip.weight&quot;, &quot;3.to_k_ip.weight&quot;, &quot;3.to_v_ip.weight&quot;, &quot;5.to_k_ip.weight&quot;, &quot;5.to_v_ip.weight&quot;, &quot;7.to_k_ip.weight&quot;, &quot;7.to_v_ip.weight&quot;, &quot;9.to_k_ip.weight&quot;, &quot;9.to_v_ip.weight&quot;, “11.to_k_ip.weight”, “11.to_v_ip.weight”, “13.to_k_ip.weight”, “13.to_v_ip.weight”, “15.to_k_ip.weight”, “15.to_v_ip.weight”, “17.to_k_ip.weight”, “17.to_v_ip.weight”, “19.to_k_ip.weight”, “19.to_v_ip.weight”, “21.to_k_ip.weight”, “21.to_v_ip.weight”, “23.to_k_ip.weight”, “23.to_v_ip.weight”, &quot;25.to_k_ip.weight&quot;, &quot;25.to_v_ip.weight&quot;, &quot;27.to_k_ip.weight&quot;, &quot;27.to_v_ip.weight&quot;, &quot;29.to_k_ip.weight&quot;, &quot;29.to_v_ip.weight&quot;, &quot;31.to_k_ip.weight&quot;, &quot;31.to_v_ip.weight&quot;.

什么步骤出错导致此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79140091/inference-error-after-training-an-ip-adapter-plus-model</guid>
      <pubDate>Wed, 30 Oct 2024 07:32:22 GMT</pubDate>
    </item>
    <item>
      <title>我们如何计算零膨胀泊松回归和零膨胀负二项回归的平均绝对误差（MAE）（R 或 python）？</title>
      <link>https://stackoverflow.com/questions/79139968/how-can-we-calculate-mean-absolute-error-mae-for-zero-inflated-poisson-regress</link>
      <description><![CDATA[现在，我尝试使用 Python 在进行零膨胀泊松回归和零膨胀负二项回归时计算平均绝对误差 (MAE)。
我将数据分为训练数据和测试数据。我使用下面的代码，但它不起作用。
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import statsmodels.formula.api as smf
import tensorflow as tf
df = pd.read_excel(&#39;....&#39;, sheet_name=&#39;Sheet1&#39;)
print(df.head())
X = df[[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]
y = df[&#39;g&#39;]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from statsmodels.discrete.count_model import ZeroInflatedPoisson
y_zip = y_train.values

y_zip_test = y_test.values

X_count = X_train.values # 计数部分的预测器
X_zero = X_train.values # 零膨胀部分的预测器

X_count_test = X_test.values
X_zero_test = X_test.values

# 为截距添加一个常数
X_count = sm.add_constant(X_count)
X_zero = sm.add_constant(X_zero)

# 拟合 ZIP 模型
zip_model = ZeroInflatedPoisson(endog=y_zip, exog=X_count, exog_infl=X_zero, indication=&#39;logit&#39;)
zip_model_fit = zip_model.fit()
print(zip_model_fit.summary())

# 进行预测
y_pred = zip_model_fit.predict(X_count_test)

# 计算 MAE
mae = np.mean(np.abs(y_zip_test - y_pred))
print(f&#39;平均绝对误差：{mae}&#39;)

结果如下
-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[3], line 33
29 print(zip_model_fit.summary())
32 # 进行预测
---&gt; 33 y_pred = zip_model_fit.predict(X_count_test)
35 # 计算 MAE 测试
36 mae = np.mean(np.abs(y_zip_test - y_pred))

文件 ~\anaconda3\envs\tf\lib\site-packages\statsmodels\base\model.py:1174，位于 Results.predict(self, exog, transform, *args, **kwargs)
1127 &quot;&quot;&quot;
1128 调用 self.model.predict 并以 self.params 作为第一个参数。
1129 
(...)
1169 返回预测。
1170 &quot;&quot;&quot;
1171 exog, exog_index = self._transform_predict_exog(exog,
1172 transform=transform)
-&gt; 1174 predict_results = self.model.predict(self.params, exog, *args,
1175 **kwargs)
1177 如果 exog_index 不为 None 且不 hasattr(predict_results,
1178 &#39;predicted_values&#39;):
1179 如果 predict_results.ndim == 1:

文件 ~\anaconda3\envs\tf\lib\site-packages\statsmodels\discrete\count_model.py:453，位于 GenericZeroInflated.predict(self, params, exog, exog_infl, Exposure, Offset, which, y_values)
449 params_main = params[self.k_inflate:]
451 prob_main = 1 - self.model_infl.predict(params_infl, exog_infl)
--&gt; 453 lin_pred = np.dot(exog, params_main[:self.exog.shape[1]]) + Exposure + Offset
455 # 重构：这很不靠谱，
456 # model_main 中应该有一个合适的预测方法
457 # 这只是 prob(y=0 | model_main)
458 tmp_exog = self.model_main.exog

ValueError：形状 (21,6) 和 (7,) 未对齐：6 (1 维) != 7 (0 维)

你能给我一些解决方案吗？
我试过计算 MAE，但多次出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/79139968/how-can-we-calculate-mean-absolute-error-mae-for-zero-inflated-poisson-regress</guid>
      <pubDate>Wed, 30 Oct 2024 07:04:19 GMT</pubDate>
    </item>
    <item>
      <title>Kong AI 代理插件：在 Kong AI 网关上配置自托管 LLM 的正确参数</title>
      <link>https://stackoverflow.com/questions/79139619/kong-ai-proxy-plugin-correct-parameters-for-configuring-a-self-hosted-llm-on-ko</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79139619/kong-ai-proxy-plugin-correct-parameters-for-configuring-a-self-hosted-llm-on-ko</guid>
      <pubDate>Wed, 30 Oct 2024 03:54:46 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 进行多轮训练时如何控制内存增长？</title>
      <link>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</link>
      <description><![CDATA[在使用 TensorFlow 进行多轮训练时，我遇到了与内存增长相关的问题。具体来说，我有一个模型训练循环，其中我在每个循环中生成训练和评估数据，并且我的内存使用量似乎不断增长，最终导致内存不足错误。我正在尝试了解如何在这些迭代过程中有效地管理或释放内存。
在此处输入图片描述
这是我的代码的简化版本
# 定义用于训练数据的 TensorFlow 变量
for num_round in range(1, 1 + total_num_round):

train_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, epochs_t + 1)
eval_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, eval_num_batch)

# 训练和评估过程

# 删除使用的数据
del train_data, eval_data
gc.collect()

我遇到的问题面临：

每轮生成的 train_data 和 eval_data 占用大量内存，我似乎无法有效地释放这些内存，导致内存不断增长。
函数 generate_all_batch_s_path_samples 没有使用 tf.function 修饰，因为它使用线程进行并行计算，这使其与 tf.function 不兼容。

问题：

除了使用 tf.keras.backend.clear_session() 之外，还有没有更有效的方法来在迭代之间释放内存？
是否有推荐的方法来管理多轮训练场景中的内存增长，例如这个？

上下文：

我使用的是 TensorFlow 2.16.0。

数据生成过程 (generate_all_batch_s_path_samples) 会在每一轮中创建新的张量用于训练和评估。

我尝试了几种方法来控制内存使用情况：

使用 assign() 而不是重复定义 train_data 和 eval_data。
使用 gc.collect() 和 del train_data, eval_data 释放内存，但这些方法不起作用。


]]></description>
      <guid>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</guid>
      <pubDate>Tue, 29 Oct 2024 14:36:13 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'KerasHistory'对象没有属性'layer'</title>
      <link>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</link>
      <description><![CDATA[我在使用 Keras 模型时遇到错误“AttributeError：&#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;”。
我尝试访问层信息，但似乎我引用了错误的对象。我尝试将名称层更改为操作，但没有成功。
我使用的是 TensorFlow v2.17.0。这是代码：
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense, Dropout

input_shape = (96, 96, 1)

# 输入张量形状
X_input = Input(input_shape)

# 零填充
X = ZeroPadding2D((3,3))(X_input)

# 1 - 阶段
X = Conv2D(64, (7,7), strides= (2,2), name = &#39;conv1&#39;, kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = &#39;bn_conv1&#39;)(X)
X = Activation(&#39;relu&#39;)(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - 阶段
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - 阶段
X = res_block(X, filter= [128,128,512], stage= 3)

# 平均池化
X = AveragePooling2D((2,2), name = &#39;Averagea_Pooling&#39;)(X)

# 最终层
X = Flatten()(X)
X = Dense(4096, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.2)(X)
X = Dense(2048, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.1)(X)
X = Dense(30, 激活 = &#39;relu&#39;)(X)

model_1_facialKeyPoints = Model(inputs= X_input, 输出 = X)
model_1_facialKeyPoints.summary()

这是回溯：
AttributeError Traceback (最近一次调用最后一次)
&lt;ipython-input-366-fd266d53d661&gt; 在 &lt;cell line: 34&gt;()
32 
33 
---&gt; 34 model_1_facialKeyPoints = Model( 输入= X_input，输出 = X)
35 model_1_facialKeyPoints.summary()

4 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/ functional.py in _validate_graph_inputs_and_outputs(self)
692 # 检查 x 是否为输入张量。
693 # pylint：disable=protected-access
--&gt; 694 
695 layer = x._keras_history.layer
696 if len(layer._inbound_nodes) &gt; 1 or (

AttributeError: &#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</guid>
      <pubDate>Tue, 29 Oct 2024 05:11:46 GMT</pubDate>
    </item>
    <item>
      <title>如何对多个特征应用多个估计量来选择具有最高 f1 分数的组合？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</link>
      <description><![CDATA[我想对多个特征使用多个估计器算法运行递归特征消除，并在测试数据上保留最高的 f1 分数组合。
如何创建一个代码，可以在测试数据上生成并显示具有最佳算法的最佳特征数量（最高 f1，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
我尝试了下面的代码，它可以为不同数量的特征生成不同估计器的结果。但我仍然需要逐一查看结果。如何做到这一点？
estimators = [(&#39;Logistic Regression&#39;, LogisticRegression()),
(&#39;Random Forest&#39;, RandomForestClassifier())]
num_features_to_select = [4,5,7,9,11,15]

for estimator_name, estimator in estimators:
for n_features in num_features_to_select:
# 创建 RFE 对象
rfe = RFE(estimator=estimator, n_features_to_select=n_features)
# 将 RFE 与数据拟合
rfe.fit(X_resampled,Y_resampled)
# 获取选定的特征
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, Thresholds = roc_curve(Y_test, pred_test)
precision, recall, Thresholds = precision_recall_curve(Y_test, pred_test)
# 使用交叉验证评估模型性能
scores = cross_val_score(estimator, X_resampled[selected_features], Y_resampled, cv=5)
# 打印结果
print(f&#39;Estimator: {estimator_name}, 特征数量: {n_features}, 平均 CV 分数: {scores.mean()}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，ROC：{logit_roc_auc}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，f1 分数：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，PRC AUC：{auc(recall,precision)}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>3D加工零件特征识别（点云、网格）</title>
      <link>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</link>
      <description><![CDATA[我有一个加工部件 (.STL)，想要识别（并提取）它的加工特征。有些特征很简单，但有些更复杂，这就是为什么我认为机器学习方法会很合适，因为我无法用数学方式描述该特征。
有一个 FeatureNet，它基本上可以完成这项工作，但它无法识别多个特征，并且代码无法按预期工作。
我还知道 AAGNet，它可以完成我想要的工作，但它使用 .STEP 作为输入，但我有一个网格（如果我转换它，则是点云）。
由于有更多的点云存储库，我认为我可以使用它们来解决我的问题。像 FPFH 这样的东西是正确的方向吗，还是我走错了路？
如果我使用机器学习方法，我可以轻松创建标记数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</guid>
      <pubDate>Mon, 07 Oct 2024 12:41:25 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 中，为什么 train 和 cv API 会接受 categorical_feature 参数，而它已经存在于数据集构造中</title>
      <link>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</link>
      <description><![CDATA[以下是 lightgbm 的 .cv API

lightgbm.cv(params, train_set, num_boost_round=100, folds=None, nfold=5, stratified=True, shuffle=True, metrics=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, fpreproc=None, seed=0, callbacks=None, eval_train_metric=False, return_cvbooster=False)

有一个参数cateogrical_feature

分类特征。如果是 int 列表，则解释为索引。如果是 str 列表，则解释为特征名称（也需要指定 feature_name）。

现在 .train API

lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, keep_training_booster=False, callbacks=None)

这里还有一个 categorical_feature 参数。文档与上述相同
现在，正如您所注意到的，这两个 API 都使用 lightgbm 数据集，而该数据集本身采用 categorical_feature 参数。文档完全相同
问题：

如果两者都指定，哪一个优先？
建议在哪个位置指定 categorical_feature？
这两个选择在内部与 lightgbm 管道的工作方式有何不同？
]]></description>
      <guid>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</guid>
      <pubDate>Thu, 25 Apr 2024 10:03:27 GMT</pubDate>
    </item>
    <item>
      <title>警告：tensorflow：顺序模型中的层只能有一个输入张量</title>
      <link>https://stackoverflow.com/questions/73181243/warningtensorflowlayers-in-a-sequential-model-should-only-have-a-single-input</link>
      <description><![CDATA[我从 tensorflow 网站对自动编码器的第一个示例的介绍 复制了过去的代码，以下代码适用于 mnist 时尚数据集，但不适用于我的数据集。这给了我一个很长的警告。请告诉我我的数据集出了什么问题
警告
屏幕上缺少相同的错误
这里 x_train 是我的数据集：
tf.shape(x_train)

输出 &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([169,** **28, 28])&gt;

这里 x_train 是 mnist 数据集：
tf.shape(x_train)

输出&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([60000, 28, 28])&gt;

我制作数据集的整个代码：
dir_path=&#39;auto/ttt/&#39;
data=[]
x_train=[]
for i in os.listdir(dir_path):
img=image.load_img(dir_path+&#39;//&#39;+i,color_mode=&#39;grayscale&#39;,target_size=(28,28)) 
data=np.array(img)
data=data/255.0
x_train.append(data)

这是警告：
警告：tensorflow：顺序模型中的层应该只有一个输入张量。已接收：输入=(&lt;tf.Tensor&#39;IteratorGetNext:0&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:1&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:2&#39;shape=(None, 28)
dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:3&#39;shape=(None, 28)
dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:4&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:5&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor &#39;IteratorGetNext:6&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:7&#39; shape=(None, 28) dtype=flo...

还有这个值错误（相同的警告）：
ValueError：调用层“sequential_4”（类型为 Sequential）时遇到异常。

层“flatten_2”需要 1 个输入，但它收到了 169 个输入张量。收到的输入：[&lt;tf.Tensor &#39;IteratorGetNext:0&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:1&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:2&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:3&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:4&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:5&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:6&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:7&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:8&#39; shape=(None, 28) dtype=float3...
]]></description>
      <guid>https://stackoverflow.com/questions/73181243/warningtensorflowlayers-in-a-sequential-model-should-only-have-a-single-input</guid>
      <pubDate>Sun, 31 Jul 2022 06:54:28 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“顺序”的输入 0 与层不兼容</title>
      <link>https://stackoverflow.com/questions/71370360/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer</link>
      <description><![CDATA[我有一个聊天机器人模型，我用数据集对其进行了训练，以提供“标准”对话，例如你好，你好吗等。现在我想用一个数据集来“扩展”现有模型，该数据集可以提供与运输、库存等相关的问题的答案。
这是我的工作/已经训练过的模型：
# 创建顺序模型
model = Sequential()

# 添加第一层，其输入形状取决于输入的大小和“relu”激活函数
model.add(Dense(256, input_shape=(len(training_data_x[0]),),activation=activations.relu))

# 添加 Dropout 以防止过度拟合
model.add(Dropout(0.6))
# 具有 64 个神经元的附加层
model.add(Dense(128,activation=activations.relu))
model.add(Dropout(0.2))
# 具有类别神经元数量的附加密集层 &amp; softmax 激活函数
# -&gt; 将输出层中的结果添加到“1”得到 %
model.add(Dense(len(training_data_y[0]),activation=activations.softmax))
# print(len(training_data_y[0])) = 71
sgd = SGD(learning_rate=0.01, decay=1e-6, motivation=0.9, nesterov=True)
# 编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd, metrics=[&#39;accuracy&#39;])

output = model.fit(np.array(training_data_x), np.array(training_data_y), epochs=200, batch_size=5, verbose=1)
plot_model_output(output)
model.summary()
model.save(&#39;./MyModel_tf&#39;, save_format=&#39;tf&#39;)

训练数据准备如下一个单独的类，并将某个 json 文件作为输入。
现在我只需将 JSON 文件替换为包含与我想添加到模型中的内容相关的数据的文件，并尝试像这样拟合它：
json_data = json.loads(open(&#39;data.json&#39;).read())

model = load_model(&#39;MyModel_tf&#39;)

model.fit(np.array(training_data_x), np.array(training_data_y), epochs=200, batch_size=5, verbose=1)

但是当我运行它时，我收到此错误：
ValueError: 输入 0 of layer &quot;sequence&quot;与层不兼容：预期形状=（无，652），发现形状=（无，71）

我假设数据是问题所在……但它的结构完全相同，只是更短。
我的问题：

我尝试实现它的方式有意义吗？
我应该尝试以不同的方式添加其他数据吗？
第二个数据集的长度必须与第一个数据集的长度相同吗？
]]></description>
      <guid>https://stackoverflow.com/questions/71370360/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer</guid>
      <pubDate>Sun, 06 Mar 2022 12:41:31 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：不可散列类型：pd.get_dummies 的“Series”</title>
      <link>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</link>
      <description><![CDATA[我尝试对我拥有的数据框中的一些名义数据（来自 Kaggle 的 House Regression）使用 pd.get_dummies。我将所有名义类别分成列名列表，&#39;obj_nominal&#39;。
当我调用
pd.get_dummies(df, columns=obj_nominal)

我收到错误：
TypeError: unhashable type: &#39;Series&#39;.

到目前为止，我所做的唯一预处理是删除数据集中的空值。我也尝试过使用 Sklearn OneHotEncoder，但它会产生相同的错误。
我也尝试过使用以下方法制作单独的数据框：
x = df.iloc[:, obj_nominal]

并在数据框上传递 get_dummies：
pd.get_dummies(data = x)

但还是没运气……
数据可在 https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data 下载]]></description>
      <guid>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</guid>
      <pubDate>Fri, 07 Jan 2022 05:51:22 GMT</pubDate>
    </item>
    <item>
      <title>SVM 模型将概率得分大于 0.1（默认阈值 0.5）的实例预测为正例</title>
      <link>https://stackoverflow.com/questions/68475534/svm-model-predicts-instances-with-probability-scores-greater-than-0-1default-th</link>
      <description><![CDATA[我正在研究一个二分类问题。我遇到的情况是，我使用了从 sklearn 导入的逻辑回归和支持向量机模型。这两个模型适合相同的、不平衡的训练数据，并调整了类别权重。它们取得了相当的表现。当我使用这两个预训练模型来预测一个新的数据集时。LR 模型和 SVM 模型预测的正例数量相似。并且预测的实例有很大的重叠。
然而，当我查看被归类为正例的概率分数时，LR 的分布从 0.5 到 1，而 SVM 从 0.1 左右开始。我调用函数 model.predict(prediction_data) 来找出预测为各个类别的实例，并调用函数
model.predict_proba(prediction_data) 给出被分类为 0（neg）和 1（pos）的概率分数，并假设它们都具有默认阈值 0.5。
我的代码没有错误，我不知道为什么 SVM 也将概率分数 &lt;0.5 的实例预测为正值。对如何解释这种情况有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/68475534/svm-model-predicts-instances-with-probability-scores-greater-than-0-1default-th</guid>
      <pubDate>Wed, 21 Jul 2021 19:36:41 GMT</pubDate>
    </item>
    <item>
      <title>维度问题：检查输入时出错：预期 conv2d_1_input 有 4 个维度，但得到的数组形状为 (26, 26, 1)</title>
      <link>https://stackoverflow.com/questions/60802354/dimension-problems-error-when-checking-input-expected-conv2d-1-input-to-have-4</link>
      <description><![CDATA[我有一个 CNN，它以以下通过 Canny 边缘检测转换为二值图像的图像作为输入。
​​并输出三个类别之一。
img = cv2.imread(path)
img = cv2.Canny(img, 33, 76)
img = np.resize(img, (26, 26, 1))
imgs.append(img)

据我所知，我必须将其转换为 3 维 (26,26,1) 图像，以便网络可以使用它。这是我的网络：
IMG_HEIGHT = 26
IMG_WIDTH = 26
no_Of_Filters=60
size_of_Filter=(5,5)
size_of_pool=(2,2)
no_Of_Nodes = 500
model_new = Sequential([
Conv2D(no_Of_Filters, size_of_Filter, padding=&#39;same&#39;,activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH , 1)),
MaxPooling2D(pool_size=size_of_pool),
Conv2D(no_Of_Filters, size_of_Filter, padding=&#39;same&#39;,activation=&#39;relu&#39;),
MaxPooling2D(pool_size=size_of_pool),
Conv2D(64, size_of_Filter, padding=&#39;same&#39;,激活=&#39;relu&#39;),
MaxPooling2D(pool_size=size_of_pool),
Flatten(),
Dense(512, 激活=&#39;relu&#39;),
Dense(3, 激活=&#39;softmax&#39;)
])

训练效果良好。在我训练并创建模型后，我想针对该网络测试图像
test_image = cv2.Canny(test_image ,33,76)
test_image = np.resize(test_image, (26, 26, 1))
test_image = test_image [np.newaxis, ...]
prediction = model.predict(test_image)
print(prediction)

现在我收到错误：
ValueError：检查输入时出错：预期 conv2d_1_input 有 4 个维度，但得到的数组形状为 (26, 26, 1)

为什么训练后的模型现在需要 4 维输入？]]></description>
      <guid>https://stackoverflow.com/questions/60802354/dimension-problems-error-when-checking-input-expected-conv2d-1-input-to-have-4</guid>
      <pubDate>Sun, 22 Mar 2020 17:06:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入图像与 CNN 中第一个卷积层的神经元进行映射？[关闭]</title>
      <link>https://stackoverflow.com/questions/60690923/how-to-map-input-image-with-neurons-in-first-conv-layer-in-cnn</link>
      <description><![CDATA[我很难将输入图像与第一个 CNN 转换层中的神经元进行映射，但我对输入特征如何映射到 ANN 中的第一个隐藏层有基本的了解。
理解输入图像与第一个转换层中的神经元之间的映射的最佳方法是什么？
我如何澄清对以下代码示例的疑问？代码取自 Coursera 的 DL 课程。
def initialize_parameters():
&quot;&quot;&quot;
初始化权重参数以使用 tensorflow 构建神经网络。形状为：
W1：[4, 4, 3, 8]
W2：[2, 2, 8, 16]
返回：
参数——包含 W1、W2 的张量字典
&quot;&quot;&quot;

tf.set_random_seed(1) # 以便您的&quot;random&quot;数字与我们的数字相匹配

### 此处开始代码 ###（大约 2 行代码）
W1 = tf.get_variable(&quot;W1&quot;,[4,4,3,8],initializer = tf.contrib.layers.xavier_initializer(seed = 0))
W2 = tf.get_variable(&quot;W2&quot;,[2,2,8,16],initializer = tf.contrib.layers.xavier_initializer(seed = 0))
### 此处结束代码 ###

parameters = {&quot;W1&quot;: W1,
&quot;W2&quot;: W2}

返回参数

def forward_propagation(X,parameters):
&quot;&quot;&quot;
为模型实现前向传播：
CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED

参数：
X -- 输入数据集占位符，形状为 (输入大小、示例数量)
参数 -- 包含参数“W1”、“W2”的 Python 字典
形状在 Initialize_parameters 中给出

返回：
Z3 -- 最后一个 LINEAR 单元的输出
“”

# 从字典“parameters”中检索参数
W1 = 参数[&#39;W1&#39;]
W2 = 参数[&#39;W2&#39;]

### 此处开始代码 ###
# CONV2D：步幅为 1，填充“相同”
Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = &#39;相同&#39;)
# RELU
A1 = tf.nn.relu(Z1)
# MAXPOOL：窗口 8x8，步幅 8，填充“相同”
P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = &#39;相同&#39;)
# CONV2D：过滤器 W2，步幅 1，填充“相同”
Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = &#39;SAME&#39;)
# RELU
A2 = tf.nn.relu(Z2)
# MAXPOOL: 窗口 4x4, 步幅 4, padding &#39;SAME&#39;
P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = &#39;SAME&#39;)
# FLATTEN
P2 = tf.contrib.layers.flatten(P2)
# FULLY-CONNECTED，无非线性激活函数（不调用 softmax）。
# 输出层中有 6 个神经元。提示：其中一个参数应该是“activation_fn=None”
Z3 = tf.contrib.layers.fully_connected(P2, 6,activation_fn=None)
### END CODE HERE ###

return Z3

with tf.Session() as sess:
np.random.seed(1)
X, Y = create_placeholders(64, 64, 3, 6)
parameters = initialize_parameters()
Z3 = forward_propagation(X, parameters)
init = tf.global_variables_initializer()
sess.run(init)
a = sess.run(Z3, {X: np.random.randn(1,64,64,3), Y: np.random.randn(1,6)})
print(&quot;Z3 = &quot; + str(a))

这个输入图像的大小是多少64643 由 8 个大小为 443 的过滤器处理？
stride = 1、padding = same 和 batch_size = 1。
到目前为止，我所理解的是，第一个卷积层中的每个神经元将有 8 个过滤器，每个过滤器的大小为 443。第一个卷积层中的每个神经元将获取与过滤器大小相同的输入图像部分（这里是 443），并应用卷积运算并产生八个 64*64 特征映射。
如果我的理解正确，那么：

为什么我们需要跨步操作，因为每个神经元处理的内核大小和输入图像部分是相同的，如果我们应用步幅 = 1（或 2），则输入图像部分的边界是交叉的，这是我们不需要的，对吗？

我们如何知道输入图像的哪一部分（与内核大小相同）映射到第一个卷积层的哪个神经元？


如果不是，那么：

输入图像如何在第一个卷积层的神经元上传递，是完整的输入图像传递给每个神经元（就像在完全连接的 ANN 中一样，其中所有输入特征被映射到第一个隐藏层中的每个神经元）？

或输入图像的一部分？我们如何知道输入图像的哪一部分映射到第一个卷积层的哪个神经元？

上述示例（W1= [4, 4, 3, 8]）指定的内核数量是每个神经元还是第一个卷积层中的内核总数？

我们如何知道上述示例在第一个卷积层中使用了多少个神经元？

神经元数量和第一个卷积层的内核数量之间有什么关系吗？

]]></description>
      <guid>https://stackoverflow.com/questions/60690923/how-to-map-input-image-with-neurons-in-first-conv-layer-in-cnn</guid>
      <pubDate>Sun, 15 Mar 2020 08:16:45 GMT</pubDate>
    </item>
    </channel>
</rss>