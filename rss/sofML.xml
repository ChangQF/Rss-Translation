<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 11 Jun 2024 03:17:47 GMT</lastBuildDate>
    <item>
      <title>使用 PyArmNN 在 Raspberry Pi 上进行 ML 推理失败</title>
      <link>https://stackoverflow.com/questions/78604851/ml-inference-on-raspberry-pi-with-pyarmnn-failure</link>
      <description><![CDATA[我尝试按照此文档 (https://developer.arm.com/documentation/102107/0000/Before-you-begin) 在 Raspberry PI 4 中构建 Arm NN。
在步骤
scons extra_cxx_flags=&quot;-fPIC&quot; benchmark_tests=0 validation_tests=0 neon=1 ，
它失败并出现错误。我已附上错误图片。如果有人能提供有关此错误的任何见解，我将非常高兴。 
注意；我无法从此链接 https://dl.bintray.com/boostorg/release/1.64.0/source/boost_1_64_0.tar.bz2 安装 boost，因为它不再存在。相反，我使用了这个：https://www.boost.org/users/history/version_1_64_0.html]]></description>
      <guid>https://stackoverflow.com/questions/78604851/ml-inference-on-raspberry-pi-with-pyarmnn-failure</guid>
      <pubDate>Mon, 10 Jun 2024 23:33:11 GMT</pubDate>
    </item>
    <item>
      <title>Predict.boosting 问题：newmfinal 必须是 1<newmfinal<mfinal</title>
      <link>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</link>
      <description><![CDATA[我尝试使用 adabag 包的 predict.boosting() 函数通过 Adaboost 算法进行预测，但出现错误：

&quot;error in predict.boosting(adaboost, train01_new, newmfinal = 9) :
newmfinal must be 1&lt;newmfinal&lt;mfinal&quot;

以下是脚本：
install.packages(&quot;adabag&quot;)
library(&quot;adabag&quot;)
adaboost &lt;- boosting.cv(factor_new ~ RFS +LI+SDI+LDI+DR+DBT+FCT+FII+DITP+ADCG+ADDG+ROA+ROI+ROS+ROE,data = train01_new, boos = TRUE，mfinal = 10，v=5，par=TRUE，control = rpart.control(cp=.001))
predict_adaboost_cv_train &lt;- predict.boosting(adaboost, train01_new, newmfinal = 9)

我按照错误提示使用了 newmfinal=9，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</guid>
      <pubDate>Mon, 10 Jun 2024 21:09:02 GMT</pubDate>
    </item>
    <item>
      <title>我创建的模型损失很大</title>
      <link>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</link>
      <description><![CDATA[我可以在堆栈上共享完整数据，因此我只想共享下面的代码。
过去几天我一直在尝试这些数据，但似乎损失在 18000 或 15000 左右，这太高了。我的同事告诉我要使用神经网络来处理这些数据。
model = Sequential([
layer.Dense(128,activation=&#39;relu&#39;),
layer.Dense(64,activation=&#39;relu&#39;),
layer.Dense(32,activation=&#39;relu&#39;),
layer.Dense(1,activation=&#39;relu&#39;)
])

model.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;mae&#39;])

model.fit(x_train, y_train,epochs=100,batch_size=80)

`Epoch 1/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step -损失：242343.5312 - mae：465.7755
纪元 2/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：243483.7969 - mae：468.0649
纪元 3/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━━ 0s 29ms/步 - 损失：246071.8281 - mae： 468.4903
纪元 4/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 20ms/步 - 损失：250888.7188 - mae：476.0695
纪元 5/100
3/3 ━━━━━━━━━━━━━━━━━━━━━━ 0s 15ms/步 - 损失：242264.2188 - mae：465.4283
纪元 6/100
3/3 ━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - 损失：242441.9062 - mae：464.7845
有人能帮我把这个损失降到最低吗？我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78604179/getting-high-loss-on-the-model-that-i-created</guid>
      <pubDate>Mon, 10 Jun 2024 19:23:13 GMT</pubDate>
    </item>
    <item>
      <title>是否有其他方法可以用来更好地识别图像中的所有细胞？</title>
      <link>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</link>
      <description><![CDATA[我正在做一个项目，我感兴趣的是找到每个细胞的 COM 和速度。为此，我需要我的程序能够识别至少 90% 的细胞。我尝试使用 opencv 的查找轮廓函数，但结果并不令人满意。我附上了得到的结果和真实图像。你们对我可以尝试什么还有其他建议吗？非常感谢

]]></description>
      <guid>https://stackoverflow.com/questions/78603983/are-there-some-other-methods-which-i-could-use-to-better-identify-all-the-cells</guid>
      <pubDate>Mon, 10 Jun 2024 18:30:38 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮我解释一下 NO TEARS 算法在估计 DAG 中的梯度编码吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78603629/can-anyone-help-me-explain-the-gradient-coding-of-no-tears-algorithm-in-estimati</link>
      <description><![CDATA[
notears &lt;- function(df, lambda1=0.1, loss.type=c(&#39;l2&#39;,&#39;logistic&#39;,&#39;poisson&#39;),
max.iter=100, h.tol=1e-6, rho.max=1e+6, w.threshold=0.1, m=NULL,
to=c(&#39;igraph&#39;, &#39;adjacency&#39;, &#39;edges&#39;, &#39;graph&#39;, &#39;bnlearn&#39;), seed=sample(1:10**6, 1)) {
loss.type &lt;- match.arg(loss.type)
to &lt;- match.arg(to)

set.seed(seed)

splitted.df &lt;- dataset.split(df, m=m)
X &lt;- df &lt;- as.matrix(splitted.df$train)

loss.func &lt;- function(W) { # ?
M &lt;- X %*% W
if (loss.type==&#39;l2&#39;) {
R &lt;- X - M
loss &lt;- 0.5 / dim(X)[1] * sum(R ** 2)
} else if (loss.type==&#39;logistic&#39;) {
loss &lt;- 1.0 / dim(X)[1] * sum(log(sum(exp(M)+1)) - X * M)
} else if (loss.type==&#39;poisson&#39;) {
S &lt;- exp(M)
loss &lt;- 1.0 / dim(X)[1] * sum(S - X * M)
}
return(loss)
}

G.loss.func &lt;- function(W) { # ?
M &lt;- X %*% W
if (loss.type==&#39;l2&#39;) {
R &lt;- X - M
G.loss &lt;- -1.0 / dim(X)[1] * t(X) %*% R
} else if (loss.type==&#39;logistic&#39;) {
G.loss &lt;- 1.0 / dim(X)[1] * t(X) %*% (1.0 / (1 + exp(-1 * M)) - X)
} else if (loss.type==&#39;poisson&#39;) {
S &lt;- exp(M)
G.loss &lt;- 1.0 / dim(X)[1] * t(X) %*% (S - X)
}
return(G.loss)
}

h.func &lt;- function(W) { # ?
M &lt;- diag(1, d, d) + W * W / d
E &lt;- matrixcalc::matrix.power(M, d-1)
h.new &lt;- sum(t(E) * M) - d
return(h.new)
}

G.h.func &lt;- function(W) { # ?
M &lt;- diag(1, d, d) + W * W / d
E &lt;- matrixcalc::matrix.power(M, d-1)
G.h &lt;- t(E) * W * 2
return(G.h)
}

adj &lt;- function(w) { # ?
w &lt;- as.matrix(w)
w.pos &lt;- w[1:(length(w)/2),]
w.neg &lt;- w[((length(w)/2)+1):(length(w)),]
dim(w.pos) &lt;- c(d,d)
dim(w.neg) &lt;- c(d,d)
W &lt;- w.pos - w.neg
return(W)
}

fn &lt;- function(w) { # ?
W &lt;- adj(w)
loss &lt;- loss.func(W)
h.new &lt;- h.func(W)
obj &lt;- loss + 0.5 * rho * h.new * h.new + alpha * h.new + lambda1 * sum(w)
return(obj)
}

gr &lt;- function(w) { # ?
W &lt;- adj(w)
G.loss &lt;- G.loss.func(W)
h.new &lt;- h.func(W)
G.h &lt;- G.h.func(W)
G.smooth &lt;- G.loss + (rho * h.new + alpha) * G.h
G.obj &lt;- c(array(G.smooth + lambda1), array(-1 * G.smooth + lambda1))
return(G.obj)
}

n &lt;- nrow(df)
d &lt;- ncol(df)
nodes &lt;- colnames(df)
w.est &lt;- replicate(2*d*d, 0)
rho &lt;- 1
alpha &lt;- 0
h &lt;- Inf

for (i in 1:max.iter) {
w.new &lt;- NULL
h.new &lt;- NULL
while (rho &lt; rho.max) {
w.new &lt;- optim(w.est, fn, gr=gr, method=&#39;L-BFGS-B&#39;, lower=0, upper=Inf)$par # ?
h.new &lt;- h.func(adj(w.new))
if (h.new &gt; (0.25*h)) {
rho = rho*10
} else {
break
}
}
w.est &lt;- w.new
h &lt;- h.new
alpha &lt;- alpha + (rho*h)
if (h &lt;= h.tol | rho &gt;= rho.max) {
break
}
}
W.est &lt;- adj(w.est)
W.est[abs(W.est) &lt; w.threshold] &lt;- 0
colnames(W.est) &lt;- rownames(W.est) &lt;- nodes

rownames(W.est) &lt;- colnames(W.est) &lt;- colnames(splitted.df$train)
g &lt;- convert.format(W.est, from=&#39;adjacency&#39;, to=to)
return(g)
}

以上是NO TEARS算法的代码，原论文在这里：https://arxiv.org/abs/1803.01422
我有2个问题，
第一，邻接矩阵W只有$d^2$个元素，我不知道为什么算法需要 $2*d^2$ 个元素来优化。
其次，我对他们的梯度代码感到很困惑。我认为他们的论文中没有任何讨论。
 gr &lt;- function(w) { # ?
W &lt;- adj(w)
G.loss &lt;- G.loss.func(W)
h.new &lt;- h.func(W)
G.h &lt;- G.h.func(W)
G.smooth &lt;- G.loss + (rho * h.new + alpha) * G.h
G.obj &lt;- c(array(G.smooth + lambda1), array(-1 * G.smooth + lambda1))
return(G.obj)
}

]]></description>
      <guid>https://stackoverflow.com/questions/78603629/can-anyone-help-me-explain-the-gradient-coding-of-no-tears-algorithm-in-estimati</guid>
      <pubDate>Mon, 10 Jun 2024 16:57:53 GMT</pubDate>
    </item>
    <item>
      <title>从 Azure 机器学习工作室笔记本直接将数据写入 Blob 存储</title>
      <link>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</link>
      <description><![CDATA[我正在 Azure 机器学习笔记本中进行一些交互式开发，我想将一些数据直接从 pandas DataFrame 保存到我默认连接的 blob 存储帐户中的 csv 文件中。我目前正在按以下方式加载一些数据：
import pandas as pd

uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df = pd.read_csv(uri)

加载这些数据没有问题，但经过一些基本转换后，我想将这些数据保存到我的存储帐户中。我发现的大多数（如果不是全部）解决方案都建议将此文件保存到本地目录，然后将此保存的文件上传到我的存储帐户。我发现的最佳解决方案是以下解决方案，它使用 tmpfile，因此我不必事后删除任何“本地”文件：
from azureml.core import Workspace
import tempfile

ws = Workspace.from_config()
datastore = ws.datastores.get(&quot;exampleblobstore&quot;)

with tempfile.TemporaryDirectory() as tmpdir:
tmpath = f&quot;{tmpdir}/example_file.csv&quot;
df.to_csv(tmpath)
datastore.upload_files([tmpath], target_path=&quot;path/to/target.csv&quot;, overwrite=True)

这是一个合理的解决方案，但我想知道是否有任何方法可以直接写入我的存储帐户，而无需先保存文件。理想情况下，我想做一些简单的事情：
target_uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df.to_csv(target_uri)

读了一些资料后，我认为 AzureMachineLearningFileSystem 类可能允许我以类似于在本地机器上开发时的方式读取和写入数据到我的数据存储，但是，似乎这个类不允许我写入数据，只能检查“文件系统”并从中读取数据。]]></description>
      <guid>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</guid>
      <pubDate>Mon, 10 Jun 2024 16:05:39 GMT</pubDate>
    </item>
    <item>
      <title>Yolov4如何在指定置信度下获取map@0.50</title>
      <link>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</link>
      <description><![CDATA[我尝试在指定的 conf_thresh 下获取 map@0.50。
我设置了不同的 conf_thresh，但我得到了平均 IoU 和相同的 map@0.50
我尝试使用此代码获取 map@0.50
darknet 检测器地图 .data .cfg .weights -thresh 0.2

我得到了“对于 conf_thresh = 0.2 TP FP FN，平均 IoU=73.15%”和“map@0.50 = 96.30%”
我尝试了其他 conf_thresh 从 0.1 到 0.9，得到了相同的 map@0.50 = 96.30%。
我不知道这个 map@0.50=96.30% 的意思是 conf_thresh0.1 到 0.9 下都是 96.30，还是使用了 Yolov4 默认的 conf_thresh。]]></description>
      <guid>https://stackoverflow.com/questions/78603262/yolov4-how-to-get-map0-50-under-specify-confidence</guid>
      <pubDate>Mon, 10 Jun 2024 15:38:42 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 DeepLab2 存储库使用 Deeplabv3+</title>
      <link>https://stackoverflow.com/questions/78603167/how-to-use-deeplabv3-using-deeplab2-repository</link>
      <description><![CDATA[这是我第一次训练神经网络。我的工作是使用 Xception 作为语义分割的主干来训练 Deeplabv3+，我想使用预先训练的权重。如何使用 deeplab2 存储库的配置来训练我的模型？如果不可能，您推荐哪些库来使用 xception 微调 deeplabv3+？
我在 deeplab2 存储库中搜索，但找不到训练或使用 deeplabv3+ 的配置]]></description>
      <guid>https://stackoverflow.com/questions/78603167/how-to-use-deeplabv3-using-deeplab2-repository</guid>
      <pubDate>Mon, 10 Jun 2024 15:20:21 GMT</pubDate>
    </item>
    <item>
      <title>PINN 解决 1D PDE，损失没有下降</title>
      <link>https://stackoverflow.com/questions/78603134/pinn-solving-1d-pde-loss-does-not-go-down</link>
      <description><![CDATA[我正在尝试使用 PINN 解决 1D d(psi)/d(x) = S。左：固定值边界条件。右：零梯度边界条件。
当 S = 0 时，训练速度非常快。它只预测均匀值。
但如果 S 不是均匀的 0，损失就会下降得非常慢。
我将我的代码附在下面（libtorch）。该代码可以轻松编译，我确信它没有错误。如果 S 的值非常大，损失也更难下降。
有什么建议吗？谢谢。
#include &lt;torch/torch.h&gt;

using namespace std;
class NN
:
public torch::nn::Module 
{
torch::nn::Sequential net_;

公共：

NN()
{
net_ = register_module
(
&quot;net&quot;, 
torch::nn::Sequential
(
torch::nn::Linear(1,20),
torch::nn::ReLU(),
torch::nn::Linear(20,20),
torch::nn::ReLU(),
torch::nn::Linear(20,20),
torch::nn::ReLU(),
torch::nn::Linear(20,20),
torch::nn::ReLU(),
torch::nn::Linear(20,1)
)
);
}

auto forward(torch::Tensor x)
{
return net_-&gt;forward(x);
}
};

int main()
{
int iter = 1;
int IterationNum = 5000;
double learning_rate = 0.001;
torch::nn::MSELoss criterion = torch::nn::MSELoss();
std::shared_ptr&lt;NN&gt; model = std::make_shared&lt;NN&gt;();
std::shared_ptr&lt;torch::optim::Adam&gt; adam = 
std::make_shared&lt;torch::optim::Adam&gt;
(
model-&gt;parameters(), 
torch::optim::AdamOptions(learning_rate)
);

// 计算域
double dx = 0.1;
自动网格 = torch::arange(0.0, 1.0 + dx, dx, torch::requires_grad()).unsqueeze(1);
//自动源 = torch::full_like(mesh, 0.0);
自动源 = 
torch::tensor
(
{0.0,0.0,0.0,0.0,0.0,0.0,50.0,0.0,0.0,0.0,0.0,0.0}
).reshape({-1, 1});

自动入口 = torch::full({1}, mesh.index({0}).item());
自动出口 = torch::full({1}, mesh.index({-1}).item());
自动 psiInlet = torch::full({1}, 10.0);

//cout&lt;&lt; &quot;inlet\n&quot; &lt;&lt; 入口 &lt;&lt; endl;
//cout&lt;&lt; &quot;出口\n&quot; &lt;&lt; 出口 &lt;&lt; endl;
//cout&lt;&lt; &quot;psiInlet\n&quot; &lt;&lt; psiInlet &lt;&lt; endl;
//cout&lt;&lt; &quot;源\n&quot; &lt;&lt; 源 &lt;&lt; endl;
//cout&lt;&lt; &quot;网格\n&quot; &lt;&lt; 网格 &lt;&lt; endl;

for (int i = 0; i &lt; IterationNum; i++) 
{
adam-&gt;zero_grad();

auto psiInletPred = model-&gt;forward(入口);
auto lossInlet = criterion(psiInletPred, psiInlet);

自动 psi = model-&gt;forward(mesh);
自动 dpsidx = 
torch::autograd::grad
(
{psi},
{mesh},
{torch::ones_like(psi)},
true,
true
)[0];
自动 loss_pde = criterion(dpsidx, source);

自动 loss = loss_pde + lossInlet;
loss.backward();
adam-&gt;step();

if (iter % 100 == 0) 
{
cout &lt;&lt; iter &lt;&lt; &quot; loss = &quot; &lt;&lt; loss.item() &lt;&lt; endl;
}
iter++;
}

自动 psiFinal = model-&gt;forward(mesh);
cout&lt;&lt; &quot;psiFinal\n&quot; &lt;&lt; psiFinal &lt;&lt; endl;

std::cout&lt;&lt; &quot;完成！&lt;&lt; std::endl;
return 0;
}

输出：
100 损失 = 239.65
200 损失 = 210.191
300 损失 = 193.635
400 损失 = 197.778
500 损失 = 203.71
600 损失 = 192.241
700 损失 = 189.999
800 损失 = 177.973
900 损失 = 190.415
1000 损失 = 186.3
1100 损失 = 198.209
1200 损失 = 196.226
1300 损失 = 179.71
1400 损失 = 193.564
1500 损失 = 192.17
1600 损失 = 173.498
1700 损失 = 190.252
1800 损失 = 189.662
1900 损失 = 189.473
2000 损失 = 189.412
2100 损失 = 189.397
2200 损失 = 189.394
2300 损失 = 189.394
2400 损失 = 189.394
2500 损失 = 189.394
2600 损失 = 189.394
2700 损失 = 189.394
2800 损失 = 189.394

使其训练更好的建议]]></description>
      <guid>https://stackoverflow.com/questions/78603134/pinn-solving-1d-pde-loss-does-not-go-down</guid>
      <pubDate>Mon, 10 Jun 2024 15:14:13 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 GPU 上加载一次 YOLO 模型并将其提供给多个 Python 进程？</title>
      <link>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</guid>
      <pubDate>Mon, 10 Jun 2024 14:56:16 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：“StratifiedShuffleSplit”对象不可迭代</title>
      <link>https://stackoverflow.com/questions/78602817/typeerror-stratifiedshufflesplit-object-is-not-iterable</link>
      <description><![CDATA[我有这段代码：
来自 sklearn.datasets 导入 fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=60, resize=0.4)
X = lfw_people.data
y = lfw_people.target
target_names = [lfw_people.target_names[a] for a in y]
n_samples, h, w = lfw_people.images.shape
来自 collections 导入 Counter
for name, count in Counter(target_names).items():
print (&quot;%20s %i&quot; % (name, count))

来自 sklearn.model_selection 导入 StratifiedShuffleSplit

train, test = list(StratifiedShuffleSplit(target_names, test_size=0.1, random_state=101))[0]

plt.subplot(1, 4, 1)
plt.axis(&#39;off&#39;)
for k,m in enumerate(X[train][y[train]==6][:4]):
plt.subplot(1, 4, 1+k)
if k==0:
plt.title(&#39;训练集&#39;)
plt.axis(&#39;off&#39;)
plt.imshow(m.reshape(50,37), cmap=plt.cm.gray, interpolation=&#39;nearest&#39;)
plt.show()

for k,m in enumerate(X[test][y[test]==6][:4]):
plt.subplot(1, 4, 1+k)
if k==0:
plt.title(&#39;测试集&#39;)
plt.axis(&#39;off&#39;)
plt.imshow(m.reshape(50,37), cmap=plt.cm.gray, interpolation=&#39;nearest&#39;)
plt.show()

它给了我这个错误：
TypeError: &#39;StratifiedShuffleSplit&#39; 对象不可迭代


我做错了什么？令人困惑的是代码来自一本书，我只是复制它，并没有改变它。库中的某些东西发生了变化，导致代码不再起作用？感谢您的帮助
代码最多显示人物图片，但它不会]]></description>
      <guid>https://stackoverflow.com/questions/78602817/typeerror-stratifiedshufflesplit-object-is-not-iterable</guid>
      <pubDate>Mon, 10 Jun 2024 14:12:18 GMT</pubDate>
    </item>
    <item>
      <title>如何从模型调用而不是 logits 中获得 [0, 1] 之间的标准化概率作为输出？</title>
      <link>https://stackoverflow.com/questions/78601616/how-to-obtain-normalized-probabilities-between-0-1-as-output-from-model-call</link>
      <description><![CDATA[我有一个经过训练的图像语义分割模型。
由于我的预测中有很多 FP，我想使用模型置信度参数进行阈值处理，该参数将设置为 [0, 1] 范围内的值。
到目前为止，我都是这样做的：
logits = model_to_predict(img_tensor, training=False).logits # (batch, NUM_CLASSES, w, h)
sigmoid_logits = tf.math.sigmoid(logits)

if model_confidence:
confidence_logits = tf.where(sigmoid_logits &gt;= model_confidence, sigmoid_logits, 0)
pred = tf.argmax(confident_logits, axis=1)

else:
pred = tf.argmax(logits, axis=1)

pred = tf.cast(tf.squeeze(pred, axis=0), tf.uint8)

但是这些解决方案导致每个图像的概率从 0 到 1，而预期结果是，在一个图像的范围内，概率将在 0 到 0.3 之间，而在另一个图像的情况下，假设在 0 到 0.8 之间。
那么在将模型置信度设置为 0.5 后，第一张图像将不会检测到 FP 对象
我如何才能实现这个结果？我使用 SegFormer（预训练）模型。]]></description>
      <guid>https://stackoverflow.com/questions/78601616/how-to-obtain-normalized-probabilities-between-0-1-as-output-from-model-call</guid>
      <pubDate>Mon, 10 Jun 2024 09:56:16 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林对心电图数据进行验证和测试的准确率较低</title>
      <link>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78599809/low-validation-and-test-accuracy-with-random-forest-on-ecg-data</guid>
      <pubDate>Sun, 09 Jun 2024 21:43:40 GMT</pubDate>
    </item>
    <item>
      <title>为神经网络 MATLAB 实现岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network-matlab</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

我的尝试如下。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。 Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 个目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。为了简单起见，我没有在我的可重现示例中计算 Wout_i,y_i,y_i_target。
Ny = 1000; % 训练信号数量
T = 100; % 每个训练信号的时间长度
reg = 10^-4; % 岭回归系数
outer_sum = 0;
for i = 1:Ny
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
inner_sum = sum(((y_i-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了 Wout 运算的双重求和和 arg min。有什么方法可以验证我的答案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network-matlab</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>部署失败：此部署的自动回滚已禁用</title>
      <link>https://stackoverflow.com/questions/75362415/failed-to-deploy-automatic-rollback-disabled-for-this-deployment</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75362415/failed-to-deploy-automatic-rollback-disabled-for-this-deployment</guid>
      <pubDate>Mon, 06 Feb 2023 14:10:27 GMT</pubDate>
    </item>
    </channel>
</rss>