<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 18:16:03 GMT</lastBuildDate>
    <item>
      <title>关于机器学习和数值训练数据的问题</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法解释指标标识符：使用 Keras 和 Scikeras 造成损失</title>
      <link>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</guid>
      <pubDate>Sat, 16 Mar 2024 14:19:04 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的超参数调整和模型评估</title>
      <link>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</link>
      <description><![CDATA[我是机器学习领域的新手，对于如何正确使用超参数调整和模型评估感到有点困惑。
超参数调整应该在整个数据集上进行还是仅在训练集上进行？正确的操作顺序是什么？
您能否检查我的代码并建议我考虑该问题的最佳实践？
在这里，我首先对整个数据集使用超参数调整，然后仅在训练集上评估模型性能。这是对的吗？不会导致数据泄露吗？
超参数调优
numeric_features = X.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns
categorical_features = X.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).columns

预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, StandardScaler(), numeric_features),
        (&#39;猫&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), categorical_features)
    ]
）

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alpha = np.arange(0, 1.1, 0.1),
                     随机状态=818，
                     职位数 = -1)

模型= make_pipeline（预处理器，en_cv）
模型.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

模型评估：
ElasticNet = make_pipeline(预处理器, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = 均方误差(y_test, y_pred)

打印（r2，MSE）

提前致谢，祝您有美好的一天！
实际上，这段代码在包含约 80000 个观察值和约 150 列的数据集上运行大约需要 18 分钟。这是否足够？]]></description>
      <guid>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</guid>
      <pubDate>Sat, 16 Mar 2024 09:27:26 GMT</pubDate>
    </item>
    <item>
      <title>我在梯度提升模型中收到此错误“AttributeError：'HalfSquaredError'对象没有属性'get_init_raw_predictions'”</title>
      <link>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</guid>
      <pubDate>Sat, 16 Mar 2024 08:56:45 GMT</pubDate>
    </item>
    <item>
      <title>yolo 通过绘制边界点进行脊柱标记训练</title>
      <link>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</link>
      <description><![CDATA[我正在检查如何通过使用 yolo 在 CT 脊柱图像上标记边界点来实现如图所示的预测。
我正在尝试使用 yolo v8，任何有关如何启动注释的指示都会有所帮助。谢谢
带有预测的结果/预期图像]]></description>
      <guid>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</guid>
      <pubDate>Sat, 16 Mar 2024 07:32:54 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有八个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数值特征为[1, 8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9]（I以两列 CSV 文件的形式提供该图所有点的坐标对 (x, y)，我也可以使用它们。）。
到目前为止，我已经寻找了许多预训练模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 paperswithcode 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对于如何设置网络的超参数以达到预期结果的知识不够，我遇到了很多错误并且无法做到这一点！
有人知道我该怎么做吗？如果您能为我提供任何帮助来完成此操作，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>加载 json 模型时 Python tensorflow keras 错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</guid>
      <pubDate>Sat, 16 Mar 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>XGBRegressor 树中叶子值的求和与预测不匹配</title>
      <link>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</link>
      <description><![CDATA[据我了解，XGBoost 模型（在本例中为 XGBRegressor）的最终预测是通过对预测叶子的值求和来获得的 [1] [2]。然而，我未能匹配对值求和的预测。这是 MRE：
导入json
从集合导入双端队列

将 numpy 导入为 np
从 sklearn.datasets 导入 load_diabetes
从 sklearn.model_selection 导入 train_test_split
将 xgboost 导入为 xgb


def leafs_vector(树):
    “”“”返回每棵树的节点向量，只有叶子有 0 个不同“”“”

    堆栈 = 双端队列([树])

    而堆栈：
        节点 = stack.popleft()
        如果“叶”是在节点中：
            产量节点[“叶子”]
        别的：
            产量 0
            对于节点 [“children”] 中的子节点：
                堆栈.追加（子）


# 加载糖尿病数据集
糖尿病 = load_diabetes()
X, y = 糖尿病.数据, 糖尿病.目标

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 XGBoost 回归模型
xg_reg = xgb.XGBRegressor(目标=&#39;reg:squarederror&#39;,
                          最大深度=5，
                          n_估计器=10)

# 训练模型
xg_reg.fit(X_train, y_train)

# 计算原始预测
y_pred = xg_reg.predict(X_test)

# 获取每个预测叶子的索引
Predicted_leafs_indices = xg_reg.get_booster().predict(xgb.DMatrix(X_test), pred_leaf=True).astype(np.int32)

# 获取树木
树 = xg_reg.get_booster().get_dump(dump_format=“json”)
trees = [json.loads(tree) 用于树中的树]

# 获取节点向量（按节点 ID 排序）
leafs = [树中树的列表(leafs_vector(tree))]

l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)))

断言 np.allclose(np.array(l_pred), y_pred, atol=0.5) # 失败

我还尝试添加 base_score 的默认值 (0.5)（如 这里）到总和，但它也不起作用。
&lt;前&gt;&lt;代码&gt;l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)) + 0.5)
]]></description>
      <guid>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</guid>
      <pubDate>Fri, 15 Mar 2024 21:07:25 GMT</pubDate>
    </item>
    <item>
      <title>WSL 2 对于机器学习训练 (PyTorch) 的性能有多好？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</link>
      <description><![CDATA[由于 Linux 的硬件兼容性问题，我目前无法在 Windows 上工作。由于我使用的大多数软件都是本机 Linux CLI 工具，因此我正在考虑使用 WSL 2 构建一个开发环境。我的问题是：如果我选择在 WSL 2 中工作，预计性能会下降多少？
更具体地说：我主要需要为机器学习应用程序编写和运行 python 代码，因此我预计 CPU 和 Nvidia GPU 都会承受繁重的本地工作负载。因为我需要进行认真的神经网络训练，所以性能对我来说是一件大事。最初，由于 WSL 2 是虚拟化的一种形式，我放弃了它作为一种选择，就像我放弃了使用 GPU 直通来启动 VM 的想法一样；但后来我了解到 WSL 2 应该是“类型 1 管理程序”1，这应该意味着更好的性能。
所以我想我的完整问题是：神经网络训练 (PyTorch) 的 WSL 2 性能与裸机 Windows 上的训练相比如何？
为了让我的问题更加集中：让我们假设我们需要运行一个基本的 PyTorch 网络，如下所示：
类 Simple_Direct_Network_Dim3(nn.Module):
def __init__(自身、input_dim、middle_dim、output_dim、线性=False、network_bias=True):
    超级().__init__()
    self.flatten = nn.Flatten()
    如果是线性的：
        self.stack = nn.Sequential(
            nn.Linear(input_dim, middle_dim, 偏差=network_bias),
            nn.Linear（middle_dim，output_dim，偏差=网络偏差）
        ）
    别的：
        self.stack = nn.Sequential(
            nn.Linear(input_dim, middle_dim, 偏差=network_bias),
            ReLU(),
            nn.Linear（middle_dim，output_dim，偏差=网络偏差）
        ）

def 前向（自身，x）：
    x = self.展平(x)
    logits = self.stack(x)
    返回逻辑值

使用 torch.optim.Adam 在 FashionMNIST 上运行，并使用 device=cuda 运行相当多的 epoch。在裸机 Windows 上性能会更好吗？如果有的话，好多少？对于具有更多隐藏层的网络，我们是否可以期待不同的结果？
&lt;小时/&gt;
[1]：我认为我完全理解什么是“类型 1 管理程序”。意味着...]]></description>
      <guid>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</guid>
      <pubDate>Fri, 15 Mar 2024 19:02:26 GMT</pubDate>
    </item>
    <item>
      <title>将 fit_resamples 与自定义分割数据一起使用？</title>
      <link>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</link>
      <description><![CDATA[我有一个自定义函数，可以根据各种标准和规则将数据分成训练集和测试集。我想在 tidymodels 工作流程中与 fit_resamples 一起使用此函数。但是，当我可以使我的列表看起来像用 vfold_cv 制作的列表时，它似乎不起作用。我正在使用的示例代码：
data(ames, package = “modeldata”)

split_data &lt;- 函数(df, n) {
  set.seed(123) # 为了重现性
  df$id &lt;- seq.int(nrow(df))
  list_of_splits &lt;- list()
  
  for(i in 1:n) {
    train_index &lt;- 样本(df$id, size=ceiling(nrow(df)*.8))
    train_set &lt;- df[train_index,]
    test_set &lt;- df[-train_index,]
    list_of_splits[[i]] &lt;- list(train_set = train_set, test_set = test_set)
  }
  
  返回（分割列表）
}

分割 &lt;- split_data(ames, 5)

重新采样 &lt;- map(splits, ~rsample::make_splits(
  x = .$train_set |&gt;选择(colnames(.$test_set)),
  评估=.$test_set
））

名称（重新采样）&lt;-paste0（“折叠”，seq_along（重新采样））

重新采样 &lt;- tibble::tibble(splits = 重新采样,
                            id = 名称（重新采样））

lm_model &lt;-
  Linear_reg() %&gt;%
  set_engine(“lm”)

lm_wflow &lt;-
  工作流程() %&gt;%
  add_model(lm_model) %&gt;%
  add_formula(Sale_Price ~ 经度 + 纬度)

res &lt;- lm_wflow %&gt;%
  fit_resamples（重新采样=重新采样）

运行最后一行后返回的错误是：
`check_rset()` 中出现错误：
！ “resamples”参数应该是一个“rset”对象，例如由“vfold_cv()”或其他“rsample”函数生成的类型。

如果我尝试强制该类“rset” class(resamples) &lt;- “rset”，列表看起来不再正确，并且出现相同的错误。
使用自定义交叉折叠数据集的正确方法是什么？
注意 - 附加问题：在上面的示例代码中，测试集和训练集的大小在折叠中是一致的。在我的实际数据中，这会略有不同 - 这有关系吗？]]></description>
      <guid>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</guid>
      <pubDate>Fri, 15 Mar 2024 13:11:15 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 的线性回归模型未按预期工作</title>
      <link>https://stackoverflow.com/questions/78165544/linear-regression-model-of-scikit-learn-not-working-as-expected</link>
      <description><![CDATA[我试图了解 Scikit-learn 中线性回归模型的内部工作原理。
这是我的数据集

这是我执行 one-hot-encoding 后的数据集。

这是执行线性回归后的系数和截距值。

销售价格是从属列，其余列是特征。
这些是在这种情况下工作正常的预测值。

我注意到系数的数量比特征的数量多 1。这就是我生成特征矩阵的方式：
feature_matrix = dataFrame.drop([&#39;售价($)&#39;], axis = &#39;列&#39;).to_numpy()

# 要添加为列的数组
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# 使用append()方法将列添加到数组
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1) # axis = 1表示列，0表示行

结果

我想知道的是 Scikit-learn 如何使用这些系数和截距来预测值。
这是我尝试过的。
我还注意到，通过进行此计算得到的值实际上等于每种情况下的里程数。但这不是这里的依赖功能。那么这是怎么回事？]]></description>
      <guid>https://stackoverflow.com/questions/78165544/linear-regression-model-of-scikit-learn-not-working-as-expected</guid>
      <pubDate>Fri, 15 Mar 2024 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>使用convert_marian_to_pytorch.py​​脚本转换opus mt模型后出现问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</link>
      <description><![CDATA[利用convert_marian_pytorch.py​​ 脚本将 OPUS 转换模型转换为与 Hugging Face Transformers 兼容的格式后，我将生成的模型上传到 Hugging Face 的模型中心 此处。然而，经过测试，模型的翻译输出似乎是一堆随机单词，缺乏连贯性。
如何解决此问题或探索替代方法以直接在 Python 中使用 Marian 模型，而不依赖 OPUS cat mt 引擎。]]></description>
      <guid>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</guid>
      <pubDate>Thu, 14 Mar 2024 20:30:25 GMT</pubDate>
    </item>
    <item>
      <title>多类不平衡数据集预处理问题</title>
      <link>https://stackoverflow.com/questions/78148016/multiclass-imbalance-dataset-preprocessing-problem</link>
      <description><![CDATA[在预处理我面临的数据集时
ValueError：目标是多类，但平均值=&#39;二进制&#39;。请选择其他
平均设置，[无、&#39;微观&#39;、&#39;宏观&#39;、&#39;加权&#39;]之一。

如何解决？
我会请专家去检查一下我在那里犯了什么错误。
我尝试使用迄今为止学到的知识进行预处理部分。]]></description>
      <guid>https://stackoverflow.com/questions/78148016/multiclass-imbalance-dataset-preprocessing-problem</guid>
      <pubDate>Tue, 12 Mar 2024 14:42:42 GMT</pubDate>
    </item>
    <item>
      <title>如何将tensorflow模型转换为pytorch模型？</title>
      <link>https://stackoverflow.com/questions/61697227/how-to-convert-a-tensorflow-model-to-a-pytorch-model</link>
      <description><![CDATA[我是 pytorch 新手。这是张量流模型的架构，我想将其转换为 pytorch 模型。

我已经完成了大部分代码，但对一些地方感到困惑。
1）在tensorflow中，Conv2D函数将filter作为输入。然而，在pytorch中，该函数将输入通道和输出通道的大小作为输入。那么如何找到与滤波器大小相同的输入通道和输出通道数。
2）在张量流中，密集层有一个称为“节点”的参数。然而，在pytorch中，同一层有2个不同的输入（输入参数的大小和目标参数的大小），我如何根据节点的数量来确定它们。
这是张量流代码。
from keras.utils import to_categorical
从 keras.models 导入顺序，load_model
从 keras.layers 导入 Conv2D、MaxPool2D、Dense、Flatten、Dropout

模型=顺序（）
model.add(Conv2D(filters=32, kernel_size=(5,5),activation=&#39;relu&#39;, input_shape=X_train.shape[1:]))
model.add(Conv2D(filters=32, kernel_size=(5,5),activation=&#39;relu&#39;))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=&#39;relu&#39;))
model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=&#39;relu&#39;))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
model.add(Dropout(rate=0.5))
model.add（密集（43，激活=&#39;softmax&#39;））

这是我的代码：
导入 torch.nn.function 作为 F
进口火炬



# 网络应该继承自nn.Module
类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        # 定义2D卷积层
        # 3：输入通道，32：输出通道，5：内核大小，1：步幅
        self.conv1 = nn.Conv2d(3, 32, 5, 1) # 输入通道的大小为3，因为所有图像都是彩色的
        self.conv2 = nn.Conv2d(32, 64, 5, 1)
        self.conv3 = nn.Conv2d(64, 128, 3, 1)
        self.conv3 = nn.Conv2d(128, 256, 3, 1)
        # 它将按概率“过滤”掉一些输入（分配零）
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        # 全连接层：输入大小，输出大小
        self.fc1 = nn.Linear(36864, 128)
        self.fc2 = nn.Linear(128, 10)

    #forward() 将所有层链接在一起，
    def 前向（自身，x）：
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = 火炬.展平(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        输出 = F.log_softmax(x, 暗淡=1)
        返回输出

提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/61697227/how-to-convert-a-tensorflow-model-to-a-pytorch-model</guid>
      <pubDate>Sat, 09 May 2020 13:09:15 GMT</pubDate>
    </item>
    <item>
      <title>与 joblib 并行训练 sklearn 模型会阻碍该过程</title>
      <link>https://stackoverflow.com/questions/47551850/training-sklearn-models-in-parallel-with-joblib-blocks-the-process</link>
      <description><![CDATA[根据这个答案中的建议，我尝试使用 joblib并行训练多个 scikit-learn 模型。
导入joblib
导入numpy
从sklearn导入树，线性模型

分类器参数 = {
                “决策树”：(tree.DecisionTreeClassifier, {}),&#39;&#39;
                “逻辑回归”：（线性模型.LogisticRegression，{}）
}


XTrain = numpy.array([[1,2,3],[4,5,6]])
yTrain = numpy.array([0, 1])

def trainModel(名称, clazz, params, XTrain, yTrain):
    print(&quot;训练&quot;, 姓名)
    模型 = clazz(**参数)
    model.fit(XTrain, yTrain)
    返回模型


joblib.Parallel(n_jobs=4)(joblib.delayed(trainModel)(name, clazz, params, XTrain, yTrain) for (name, (clazz, params)) in classifierParams.items())

但是，在不使用 CPU 的情况下，对最后一行的调用需要很长时间，事实上它似乎只是阻塞并且从不返回任何内容。我的错误是什么？ 
在 XTrain 中使用极少量数据进行的测试表明，跨多个进程复制 numpy 数组并不是延迟的原因。]]></description>
      <guid>https://stackoverflow.com/questions/47551850/training-sklearn-models-in-parallel-with-joblib-blocks-the-process</guid>
      <pubDate>Wed, 29 Nov 2017 11:34:35 GMT</pubDate>
    </item>
    </channel>
</rss>