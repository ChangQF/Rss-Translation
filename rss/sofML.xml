<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 21:11:28 GMT</lastBuildDate>
    <item>
      <title>使用机器学习预测未来股票价格</title>
      <link>https://stackoverflow.com/questions/78173106/predicting-future-stock-prices-using-machine-learning</link>
      <description><![CDATA[我正在尝试用 python 训练机器学习模型（xgb）来预测股票价格。我首先获取股票价格，然后计算技术指标以用作特征。然后，我将数据拆分为训练集和测试集，其中特征（modeling_df）为 X，收盘价（closes）为 Y。
我的问题是，我不确定该模型实际上是根据过去的价格来预测价格，但该模型是通过查看当前时间的特征来预测价格。我还想确保它使用“滚动窗口”，因此，如果有 30 个值，则值 11-20 应基于 0-10，而 21-30 应基于 0- 20.
如果有人知道神经网络的解决方案是否不同，那么我们也将不胜感激！
scaled_features = scaler.fit_transform(modeling_df)

X = 缩放特征
Y = 关闭

x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.05,shuffle=False)

然后我使用：
final_model = xgb.XGBRegressor(**best_params)
Final_model.fit(x_train,y_train, )

预测 = Final_model.predict(x_test)

然后我使用 matplotlib 显示它。
我尝试将整个数据集移动 10（和其他值），以尝试预测未来的 10 个数据点，但我不确定如何验证它是否确实有效。
这是我的图表（数据集没有被移动）。
烛台 + 蓝线 = 实际价格
紫色线 = 预测价格
图表：

这里有一些数据：
功能和价格（不变）：

&lt;标题&gt;

时间
Og 价格
RSI


&lt;正文&gt;

11:02
3.61
51.07


11:03
3.62
57.89


11:04
3.62
60.28


11:05
3.61
62.92



预测与原价：

&lt;标题&gt;

时间
Og 价格
预测


&lt;正文&gt;

11:02
3.6
3.61


11:03
3.61
3.62


11:04
3.61
3.62


11:05
3.61
3.61



本质上，我想确保我的模型没有使用 11:02-05 的 RSI 来预测 11:05 的值。我希望它使用 11:02-04 的值来预测 11:05 的值，以便它预测未来。]]></description>
      <guid>https://stackoverflow.com/questions/78173106/predicting-future-stock-prices-using-machine-learning</guid>
      <pubDate>Sat, 16 Mar 2024 19:44:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在火车上使用 11 个以上参数训练模型，但在测试中使用 2 个参数</title>
      <link>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</link>
      <description><![CDATA[我的任务是在火车上我可以使用许多输入参数（11 个或更多），但在实际任务（测试）中输入始终是 2 个参数。因此，就如何做得更好、是否值得这样做等等提出一些建议。 简而言之，这是一个回归任务，当我需要预测用户从该 ATM 取款的概率（用户 ID、和 ATM 位置），这些数据是与其他人一起提供给我的，但我不明白如何使用选择两个的更多参数。
全局参数：
用户身份
自动提款机位置
我可以使用的火车输入：
h3_09
客户ID
日期时间_id
计数整数
总和、平均值、最小值、最大值、浮点数
MCC
等等
我已经尝试过在 11 个输入参数上训练神经网络，但我不明白如何在两个输入参数上使用它的电流，一般来说我想使用一些梯度增强，但我也不明白如何充分利用所有参数...]]></description>
      <guid>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</guid>
      <pubDate>Sat, 16 Mar 2024 19:10:22 GMT</pubDate>
    </item>
    <item>
      <title>基于特征而不是样本分割 sklearn 的 IncrementalPCA (ipca.transform) 输入</title>
      <link>https://stackoverflow.com/questions/78172898/split-sklearns-incrementalpca-ipca-transform-input-based-on-features-instead</link>
      <description><![CDATA[我对 IncrementalPCA 在功能数量上有大量的投入：(22, 258186260)
我想知道是否可以根据特征而不是样本来分割 sklearn 的 IncrementalPCA (ipca.transform) 输入。结果仍然正确吗？
&lt;前&gt;&lt;代码&gt; ipca = IncrementalPCA(n_components=2)
               对于 np.split( input, 10,axis=1) 中的批次：
                     
                     输出=ipca.transform(批处理)
                     
                     
]]></description>
      <guid>https://stackoverflow.com/questions/78172898/split-sklearns-incrementalpca-ipca-transform-input-based-on-features-instead</guid>
      <pubDate>Sat, 16 Mar 2024 18:33:13 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数值训练数据的问题</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>代码有问题还是我的数据有问题？</title>
      <link>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</guid>
      <pubDate>Sat, 16 Mar 2024 10:00:18 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的超参数调整和模型评估</title>
      <link>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</link>
      <description><![CDATA[我是机器学习领域的新手，对于如何正确使用超参数调整和模型评估感到有点困惑。
超参数调整应该在整个数据集上进行还是仅在训练集上进行？正确的操作顺序是什么？
您能否检查我的代码并建议我考虑该问题的最佳实践？
在这里，我首先对整个数据集使用超参数调整，然后仅在训练集上评估模型性能。这是对的吗？不会导致数据泄露吗？
超参数调优
numeric_features = X.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns
categorical_features = X.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).columns

预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, StandardScaler(), numeric_features),
        (&#39;猫&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), categorical_features)
    ]
）

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alpha = np.arange(0, 1.1, 0.1),
                     随机状态=818，
                     职位数 = -1)

模型= make_pipeline（预处理器，en_cv）
模型.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

模型评估：
ElasticNet = make_pipeline(预处理器, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = 均方误差(y_test, y_pred)

打印（r2，MSE）

提前致谢，祝您有美好的一天！
实际上，这段代码在包含约 80000 个观察值和约 150 列的数据集上运行大约需要 18 分钟。这是否足够？]]></description>
      <guid>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</guid>
      <pubDate>Sat, 16 Mar 2024 09:27:26 GMT</pubDate>
    </item>
    <item>
      <title>我在梯度提升模型中收到此错误“AttributeError：'HalfSquaredError'对象没有属性'get_init_raw_predictions'”</title>
      <link>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</guid>
      <pubDate>Sat, 16 Mar 2024 08:56:45 GMT</pubDate>
    </item>
    <item>
      <title>yolo 通过绘制边界点进行脊柱标记训练</title>
      <link>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</link>
      <description><![CDATA[我正在检查如何通过使用 yolo 在 CT 脊柱图像上标记边界点来实现如图所示的预测。
我正在尝试使用 yolo v8，任何有关如何启动注释的指示都会有所帮助。谢谢
带有预测的结果/预期图像]]></description>
      <guid>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</guid>
      <pubDate>Sat, 16 Mar 2024 07:32:54 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有八个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数值特征为[1, 8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9]（I以两列 CSV 文件的形式提供该图所有点的坐标对 (x, y)，我也可以使用它们。）。
到目前为止，我已经寻找了许多预训练模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 paperswithcode 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
有人知道我该怎么做吗？如果您能为我提供任何帮助来完成此操作，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>加载 json 模型时 Python tensorflow keras 错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</guid>
      <pubDate>Sat, 16 Mar 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>WSL 2 对于机器学习训练 (PyTorch) 的性能有多好？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</link>
      <description><![CDATA[由于 Linux 的硬件兼容性问题，我目前无法在 Windows 上工作。由于我使用的大多数软件都是本机 Linux CLI 工具，因此我正在考虑使用 WSL 2 构建一个开发环境。我的问题是：如果我选择在 WSL 2 中工作，预计性能会下降多少？
更具体地说：我主要需要为机器学习应用程序编写和运行 python 代码，因此我预计 CPU 和 Nvidia GPU 都会承受繁重的本地工作负载。因为我需要进行认真的神经网络训练，所以性能对我来说是一件大事。最初，由于 WSL 2 是虚拟化的一种形式，我放弃了它作为一种选择，就像我放弃了使用 GPU 直通来启动 VM 的想法一样；但后来我了解到 WSL 2 应该是“类型 1 管理程序”1，这应该意味着更好的性能。
所以我想我的完整问题是：神经网络训练 (PyTorch) 的 WSL 2 性能与裸机上的 Windows 训练相比如何？
为了让我的问题更加集中：让我们假设我们需要运行一个基本的 PyTorch 网络，如下所示：
类 Simple_Direct_Network_Dim3(nn.Module):
def __init__(自身、input_dim、middle_dim、output_dim、线性=False、network_bias=True):
    超级().__init__()
    self.flatten = nn.Flatten()
    如果是线性的：
        self.stack = nn.Sequential(
            nn.Linear(input_dim, middle_dim, 偏差=network_bias),
            nn.Linear（middle_dim，output_dim，偏差=网络偏差）
        ）
    别的：
        self.stack = nn.Sequential(
            nn.Linear(input_dim, middle_dim, 偏差=network_bias),
            ReLU(),
            nn.Linear（middle_dim，output_dim，偏差=网络偏差）
        ）

def 前向（自身，x）：
    x = self.展平(x)
    logits = self.stack(x)
    返回逻辑值

使用 torch.optim.Adam 在 FashionMNIST 上运行，并使用 device=cuda 运行相当多的 epoch。在裸机 Windows 上性能会更好吗？如果是这样的话好多少？对于具有更多隐藏层的网络，我们是否可以期待不同的结果？
&lt;小时/&gt;
[1]：我认为我完全理解什么是“类型 1 虚拟机管理程序”。意味着...]]></description>
      <guid>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</guid>
      <pubDate>Fri, 15 Mar 2024 19:02:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML 中为基于文本的数据创建管道？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78168474/how-to-create-a-pipeline-for-text-based-data-in-ml</link>
      <description><![CDATA[我想为机器学习中基于文本的数据集编写一个管道。我不明白如何写以及用什么来获得它？
任何人都可以帮我解决这个问题。
我尝试使用普通模型
我想要这样，但它来自数字数据]]></description>
      <guid>https://stackoverflow.com/questions/78168474/how-to-create-a-pipeline-for-text-based-data-in-ml</guid>
      <pubDate>Fri, 15 Mar 2024 16:43:47 GMT</pubDate>
    </item>
    <item>
      <title>检测自定义数据的图像显着性边界框/潜在训练[重复]</title>
      <link>https://stackoverflow.com/questions/78167413/detect-image-saliency-bounding-box-potential-train-on-custom-data</link>
      <description><![CDATA[“焦点区域”定义为包含图像主题的图像区域（边界框）。它可以是任何东西，比如人、汽车、植物、艺术品等。
目前该区域是由人类选择的。给定一个包含这些边界框的大型（？50K 图像）数据集，训练某种以类似方式自动选择边界框的图像识别模型是否可行？
我对对象检测和图像分割进行了一些研究，但这两项任务似乎与我想要实现的目标非常不同。我完全天真的方法是训练具有单类“焦点区域”的对象检测模型。但我无法评估这是否是正确的方法，并且我未能找到有关此类问题的任何信息。
如果这种方法可行，那么在框架/模型/架构方面实现这种方法的最先进技术是什么。
或者是否有更好的方法/模型来完成此类任务？
编辑
正如评论中所指出的，我正在寻找“显着性”。所以让我重新表述一下：

有哪些好的工具/框架（最好是oss）来检测边界框的显着性
我发现了一些关于这个主题的深度学习论文，但没有找到任何好的实现。有什么可以推荐的吗？
]]></description>
      <guid>https://stackoverflow.com/questions/78167413/detect-image-saliency-bounding-box-potential-train-on-custom-data</guid>
      <pubDate>Fri, 15 Mar 2024 13:52:13 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 的线性回归模型未按预期工作</title>
      <link>https://stackoverflow.com/questions/78165544/linear-regression-model-of-scikit-learn-not-working-as-expected</link>
      <description><![CDATA[我试图了解 Scikit-learn 中线性回归模型的内部工作原理。
这是我的数据集

这是我执行 one-hot-encoding 后的数据集。

这是执行线性回归后的系数和截距值。

销售价格是从属列，其余列是特征。
这些是在这种情况下工作正常的预测值。

我注意到系数的数量比特征的数量多 1。这就是我生成特征矩阵的方式：
feature_matrix = dataFrame.drop([&#39;售价($)&#39;], axis = &#39;列&#39;).to_numpy()

# 要添加为列的数组
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# 使用append()方法将列添加到数组
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1) # axis = 1表示列，0表示行

结果

我想知道的是 Scikit-learn 如何使用这些系数和截距来预测值。
这是我尝试过的。
我还注意到，通过进行此计算得到的值实际上等于每种情况下的里程数。但这不是这里的依赖功能。那么这是怎么回事？]]></description>
      <guid>https://stackoverflow.com/questions/78165544/linear-regression-model-of-scikit-learn-not-working-as-expected</guid>
      <pubDate>Fri, 15 Mar 2024 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>如何将tensorflow模型转换为pytorch模型？</title>
      <link>https://stackoverflow.com/questions/61697227/how-to-convert-a-tensorflow-model-to-a-pytorch-model</link>
      <description><![CDATA[我是 pytorch 新手。这是张量流模型的架构，我想将其转换为 pytorch 模型。

我已经完成了大部分代码，但对一些地方感到困惑。
1）在tensorflow中，Conv2D函数将filter作为输入。然而，在pytorch中，该函数将输入通道和输出通道的大小作为输入。那么如何找到与滤波器大小相同的输入通道和输出通道数。
2）在张量流中，密集层有一个称为“节点”的参数。然而，在pytorch中，同一层有2个不同的输入（输入参数的大小和目标参数的大小），我如何根据节点的数量来确定它们。
这是张量流代码。
from keras.utils import to_categorical
从 keras.models 导入顺序，load_model
从 keras.layers 导入 Conv2D、MaxPool2D、Dense、Flatten、Dropout

模型=顺序（）
model.add(Conv2D(filters=32, kernel_size=(5,5),activation=&#39;relu&#39;, input_shape=X_train.shape[1:]))
model.add(Conv2D(filters=32, kernel_size=(5,5),activation=&#39;relu&#39;))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=&#39;relu&#39;))
model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=&#39;relu&#39;))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
model.add(Dropout(rate=0.5))
model.add（密集（43，激活=&#39;softmax&#39;））

这是我的代码：
导入 torch.nn.function 作为 F
进口火炬



# 网络应该继承自nn.Module
类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        # 定义2D卷积层
        # 3：输入通道，32：输出通道，5：内核大小，1：步幅
        self.conv1 = nn.Conv2d(3, 32, 5, 1) # 输入通道的大小为3，因为所有图像都是彩色的
        self.conv2 = nn.Conv2d(32, 64, 5, 1)
        self.conv3 = nn.Conv2d(64, 128, 3, 1)
        self.conv3 = nn.Conv2d(128, 256, 3, 1)
        # 它将按概率“过滤”掉一些输入（分配零）
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        # 全连接层：输入大小，输出大小
        self.fc1 = nn.Linear(36864, 128)
        self.fc2 = nn.Linear(128, 10)

    #forward() 将所有层链接在一起，
    def 前向（自身，x）：
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = 火炬.展平(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        输出 = F.log_softmax(x, 暗淡=1)
        返回输出

提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/61697227/how-to-convert-a-tensorflow-model-to-a-pytorch-model</guid>
      <pubDate>Sat, 09 May 2020 13:09:15 GMT</pubDate>
    </item>
    </channel>
</rss>