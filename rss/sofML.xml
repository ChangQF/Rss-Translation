<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 27 Jul 2024 18:20:12 GMT</lastBuildDate>
    <item>
      <title>如何训练机器学习模型来使用词汇分析评估代码质量？</title>
      <link>https://stackoverflow.com/questions/78801874/how-to-train-a-machine-learning-model-to-evaluate-code-quality-using-lexical-ana</link>
      <description><![CDATA[我正在做一个项目，想开发一个机器学习模型，该模型可以根据词汇分析来评估代码解决方案的质量。我的目标是让模型能够确定给定的代码片段是编码问题的“好”解决方案还是“坏”解决方案。
还有其他提示或建议可以构建一个强大的机器学习模型来评估代码质量吗？]]></description>
      <guid>https://stackoverflow.com/questions/78801874/how-to-train-a-machine-learning-model-to-evaluate-code-quality-using-lexical-ana</guid>
      <pubDate>Sat, 27 Jul 2024 16:13:10 GMT</pubDate>
    </item>
    <item>
      <title>训练期间验证和使用检查点进行推理时，模型输出不同</title>
      <link>https://stackoverflow.com/questions/78801833/different-model-output-on-validation-during-training-versus-inference-with-check</link>
      <description><![CDATA[我使用 pytorch lightning 训练一个模型，并保存带有参数的检查点 (ckpt) 文件以供以后使用。除了一个新模型之外，它运行良好。
因此，我调试了推理，以找出为什么这个新模型在使用检查点的推理中表现非常不同。在继续之前，有几点注意事项：

模型之间的主要区别在于输入（其余的，即超参数，是等效的或非常接近的）
该模型具有与之前的模型相同的结构，运行良好（相同的层，相同的优化器（即 AdamW），例如权重衰减率的细微差异或奖励/损失函数已经改变 -&gt; 理论上不应该影响推理）
我使用 pytorch lightning (2.0) &amp; torch (2.3)
在 Cuda、Cpu 或 Mps 运行时上的行为相同（均用于训练和推理）
我使用验证数据集，在每个训练周期后计算性能。 无论模型或输入如何，我在验证数据集上都获得了良好的结果
验证是在时间数据上进行的，模型给出的输出与一定数量的过去数据成比例

我在进行推断时发现了什么：

第一个推断（即图像下方的第一行）等于我在训练阶段（在推断和验证的同一时期）在验证的第一个结果中获得的结果
总体而言，推断是好的，但有一个因素导致推断与验证数据集振荡和交替
我不知道这种振荡来自哪里
我检查了 ckpts 文件和权重，它们看起来都很正常
一个区别是在训练期间，验证指标是在完整的验证数据集上计算的，而推断则在完整的验证数据集上逐个计算指标相同 时间序列（两者的输出相同，但验证由网络中的一次传递完成，推理是循环的 -&gt; 一次传递）
最让我困扰的是，在验证阶段（训练期间），一切都正常，但在加载检查点并手动执行推理后就不行了。到目前为止，它适用于我之前的所有模型。

具有相同模型/相同时期的输出示例（左侧推理，右侧训练验证）。一图胜千言：

我期望左边和右边是相同的（即几乎相同的颜色），但推理预测中出现了一些噪音。如您所见，第一行在推理上没问题，但随后立即开始振荡。
例如，另一次运行验证与推理（偏差很小但可以容忍，但这里没有振荡）

什么可以解释相同网络架构上的这些振荡？
ps：由于许可，无法共享直接代码]]></description>
      <guid>https://stackoverflow.com/questions/78801833/different-model-output-on-validation-during-training-versus-inference-with-check</guid>
      <pubDate>Sat, 27 Jul 2024 15:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何预处理包含数字代码列表的功能？</title>
      <link>https://stackoverflow.com/questions/78801772/how-can-i-preprocess-a-feature-that-contains-a-list-of-number-codes</link>
      <description><![CDATA[我必须预处理一个特征，该特征基本上是编码为字符串的数字代码列表，并且我想对其进行编码，以便输出是每个数字的频率数组。还应通过输入缺失值来预处理特征。
这是我所做的：
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
s_data = pd.Series([&#39;123 342 789&#39;, &#39;12 34 56&#39;, np.nan, &#39;1 2 3 123&#39;)
s_data = str_data.str.split(&quot; &quot;)
pipeline = Pipeline([
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;))
(&#39;mlb&#39;, MultiLabelBinarizer())
])
encoded_data = pipeline.fit_transform(s_data)
encoded_df = pd.DataFrame(encoded_data, columns=mlb.classes_)

我期望的输出是这样的：
 1 12 123 2 3 34 342 56 789
0 0 0 1 0 0 0 1 0 1
1 0 1 0 0 0 1 0 1 0
2 1 0 1 1 1 0 0 0 0

但是 SimpleImputer 不会接受输入，说输入包含列表。当我尝试将输入更改为 numpy 数组格式时，MultiLabelBinariZer 拒绝了它，说它只需要 2 个输入，但给出了 3 个。]]></description>
      <guid>https://stackoverflow.com/questions/78801772/how-can-i-preprocess-a-feature-that-contains-a-list-of-number-codes</guid>
      <pubDate>Sat, 27 Jul 2024 15:30:18 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch/Unet 机器学习模型在本地运行和在 Azure 上运行时会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/78801760/pytorch-unet-machine-learning-model-gives-different-results-when-running-locally</link>
      <description><![CDATA[我有一个用 Python 编写的用于卫星图像分割的 ML 模型。我们使用 ONNX，以便它可以作为 C# 应用程序的一部分与 .NET 交互。然后，此应用程序通过 DevOps 部署到 Azure，作为 Azure 函数用作 Web API，它根据一组坐标对任何图像进行分割。
在我最近的测试中，我已确认运行相同的输入数据在本地运行和部署到 Azure 时会产生不同的结果。
我有一个不使用 ML 的旧式分割算法，它在两种环境中输出相同的结果，因此我绝对确定它与 ML 模型特别相关。
我没有编写 ML 模型，这也不是我的特定专业领域，所以我在这方面的知识有限，但我会将任何建议传达给我的开发人员。有人遇到过这样的情况吗？您对我们如何解决这个问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78801760/pytorch-unet-machine-learning-model-gives-different-results-when-running-locally</guid>
      <pubDate>Sat, 27 Jul 2024 15:21:00 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Nural 函数 1.X Tensorflow 更改为 2.X Tensorflow？</title>
      <link>https://stackoverflow.com/questions/78800789/how-do-i-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</link>
      <description><![CDATA[layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
layer_1_b = tf.layers.batch_normalization(layer_1)
layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, w_2), b_2))
layer_2_b = tf.layers.batch_normalization(layer_2)
layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, w_3), b_3))
layer_3_b = tf.layers.batch_normalization(layer_3)
y = tf.nn.relu(tf.add(tf.matmul(layer_3, w_4), b_4))
g_q_action = tf.argmax(y, axis=1)

# 计算损失
g_target_q_t = tf.placeholder(tf.float32, None, name=&quot;target_value&quot;)
g_action = tf.placeholder(tf.int32, None, name=&#39;g_action&#39;)
action_one_hot = tf.one_hot(g_action, n_output, 1.0, 0.0, name=&#39;action_one_hot&#39;)
q_acted = tf.reduce_sum(y * action_one_hot, reduction_indices=1, name=&#39;q_acted&#39;)

g_loss = tf.reduce_mean(tf.square(g_target_q_t - q_acted), name=&#39;g_loss&#39;)
optim = tf.train.RMSPropOptimizer(learning_rate=0.001, motivation=0.95, epsilon=0.01).minimize(g_loss)

错误声明：
回溯（最近一次调用最后一次）：
文件“C:\Users\T\PycharmProjects\Project1\.venv\main_test.py”，第 139 行，位于&lt;module&gt;
layer_1 = tf.compat.v1.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\ops\weak_tensor_ops.py&quot;，第 142 行，在包装器中
return op(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;，第 153 行，在 error_handler 中
raise e.with_traceback(filtered_tb)来自 None
文件“C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\framework\ops.py”，第 1037 行，位于 _create_c_op
引发 ValueError(e.message)

将 1.x 更改为 2.x Tensorflow。]]></description>
      <guid>https://stackoverflow.com/questions/78800789/how-do-i-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</guid>
      <pubDate>Sat, 27 Jul 2024 07:23:19 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用任何类型的基础模型来检测通常发生在眼睛中的疾病？[关闭]</title>
      <link>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</link>
      <description><![CDATA[我想建立一个模型，可以检测视网膜图像是否有微动脉瘤。
可以使用基础模型吗？如果可以，它会带来什么好处？
如果我想，我可以使用哪种基础模型，比如我最近读到关于 RetFound 的文章，我认为它适合我的范围。
（这就像一个理论问题）如果这些模型甚至没有接受过执行特定任务的训练，那么它们究竟如何适用于我们的特定用例数据，比如制作基础模型的人如何决定训练数据的范围？它只是一个对视网膜图像进行处理模型，还是一个范围更广的模型，除了视网膜图像外，它还与其他相关数据有关（我相信）。第三个问题是可选的，如果您能回答，将不胜感激。
我尝试使用一个基础模型，https://github.com/facebookresearch/deit/blob/main/README_deit.md，但我在这里学得并不多。]]></description>
      <guid>https://stackoverflow.com/questions/78799471/how-do-i-use-any-kind-of-foundational-model-to-detect-a-disease-that-generally-h</guid>
      <pubDate>Fri, 26 Jul 2024 18:19:53 GMT</pubDate>
    </item>
    <item>
      <title>来自segmentation_models_pytorch 的 Unet 在训练中停滞</title>
      <link>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</link>
      <description><![CDATA[我一直在遵循一个关于在自定义数据集上训练分割模型的教程，但它拒绝在训练模型方面取得任何进展。
这是我的模型设置
import fragmentation_models_pytorch as smp
import torch

ENCODER = &#39;efficientnet-b0&#39;
ENCODER_WEIGHTS = &#39;imagenet&#39;
CLASSES = [&#39;ship&#39;]
ACTIVATION = &#39;sigmoid&#39;
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model = smp.Unet(
coder_name=ENCODER, 
coder_weights=ENCODER_WEIGHTS, 
classes=len(CLASSES), 
activation=ACTIVATION,
).to(DEVICE)

from fragmentation_models_pytorch import utils as smp_utils

loss = smp_utils.losses.DiceLoss()
metrics = [
smp_utils.metrics.IoU(threshold=0.5),
]

optimizer = torch.optim.Adam([ 
dict(params=model.parameters(), lr=0.0001),
])


和 epochs 运行器
train_epoch = smp_utils.train.TrainEpoch(
model, 
loss=loss, 
metrics=metrics, 
optimizer=optimizer,
device=DEVICE,
verbose=True,
)

valid_epoch = smp_utils.train.ValidEpoch(
model, 
loss=loss, 
metrics=metrics, 
device=DEVICE,
verbose=True,
)

并且，当我运行训练，模型只是停留在第一个 epoch 上，没有任何进展
max_score = 0

for i in range(0, 40):

print(&#39;\nEpoch: {}&#39;.format(i))
train_logs = train_epoch.run(train_loader)
valid_logs = valid_epoch.run(valid_loader)

# 执行某些操作（保存模型、更改 lr 等）
if max_score &lt; valid_logs[&#39;iou_score&#39;]:
max_score = valid_logs[&#39;iou_score&#39;]
torch.save(model, &#39;./best_model.pth&#39;)
print(&#39;模型已保存！&#39;)

if i == 25:
optimizer.param_groups[0][&#39;lr&#39;] = 1e-5
print(&#39;将解码器学习率降低至 1e-5！&#39;)

结果：
Epoch：0
train：0%| | 0/3851 [00:00&lt;?, ?it/s]

我这样把它放了 3 个小时，它一点变化都没有
我在 CPU (i7-10710U) 上运行（我知道它比 GPU 慢得多，但我的 GPU (GeForce 1650mq) 不支持 cuda），内存为 32 GB，我之前运行过类似的模型，没有任何问题。
有人能帮帮我吗？也许我漏掉了什么？也许有一个更轻的模型可以在我的系统上运行？
我已经尝试了一些其他设置和模型，YOLOv8 和 YOLOv3 也拒绝训练。]]></description>
      <guid>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</guid>
      <pubDate>Fri, 26 Jul 2024 15:14:45 GMT</pubDate>
    </item>
    <item>
      <title>决策树分类器给出错误结果</title>
      <link>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</link>
      <description><![CDATA[我正在学习一门机器学习课程，其中的作业是实现 DecisionTreeClassifier 的拟合方法。
这是我的代码：
import numpy as np
import pandas as pd

class MyTreeClf:
def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
self.max_depth = max_depth
self.min_samples_split = min_samples_split
self.max_leafs = max_leafs
self.tree = None
self.leafs_cnt = 0

def node_entropy(self, probs):
return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

def node_ig(self, x_col, y, split_value):
left_mask = x_col &lt;= split_value
right_mask = x_col &gt; split_value

如果 len(x_col[left_mask]) == 0 或 len(x_col[right_mask]) == 0:
返回 0

left_probs = np.bincount(y[left_mask]) / len(y[left_mask])
right_probs = np.bincount(y[right_mask]) / len(y[right_mask])

entropy_after = len(y[left_mask]) / len(y) * self.node_entropy(left_probs) + len(y[right_mask]) / len(y) * self.node_entropy(right_probs)
entropy_before = self.node_entropy(np.bincount(y) / len(y))

返回 entropy_before - entropy_after

def get_best_split(self, X: pd.DataFrame，y：pd.Series）：
best_col，best_split_value，best_ig = None，None，-np.inf

对于 X.columns 中的 col：
sorted_unique_values = np.sort(X[col].unique())

对于 range(1，len(sorted_unique_values)) 中的 i：
split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

ig = self.node_ig(X[col]，y，split_value)

如果 ig &gt; best_ig:
best_ig = ig
best_col = col
best_split_value = split_value

返回 best_col、best_split_value、best_ig

def fit(self, X: pd.DataFrame, y: pd.Series,depth=0):
如果depth == 0:
self.tree = {}

best_col、best_split_value、best_ig = self.get_best_split(X, y)

如果depth &lt; self.max_depth 和 len(y) &gt;= self.min_samples_split 和 self.leafs_cnt &lt; self.max_leafs 和 best_col 不为 None:
left_mask = X[best_col] &lt;= best_split_value
right_mask = X[best_col] &gt; best_split_value

self.tree[depth] = {&#39;col&#39;: best_col, &#39;split&#39;: best_split_value, &#39;left&#39;: {}, &#39;right&#39;: {}}

self.fit(X[left_mask], y[left_mask],depth + 1)
self.fit(X[right_mask], y[right_mask],depth + 1)
else:
class_label = y.mode()[0]
self.tree[depth] = {&#39;class&#39;: class_label}
self.leafs_cnt += 1
df = pd.read_csv(&#39;c:\\Users\\Nijat\\Downloads\\banknote+authentication.zip&#39;, header=None)
df.columns = [&#39;variance&#39;, &#39;skewness&#39;, &#39;curtosis&#39;, &#39;entropy&#39;, &#39;target&#39;]
X, y = df.iloc[:,:4], df[&#39;target&#39;]

model = MyTreeClf()
model.fit(X, y)

print(model.leafs_cnt)

在 fit 方法中，使用 X 和 y 来构建树，应该计算叶子的数量。
树的构建如下：
根节点：从根节点开始，遍历每个属性。
阈值选择过程：
对于每个属性，选择唯一值并对其进行排序。
形成阈值列表以拆分值。
对于每个阈值，将数据集拆分为两个子集（左和右）。
评估每次拆分的信息增益。
选择具有最高信息增益的属性和阈值，并将它们保存在分层结构中。
递归拆分：
将数据集拆分为两个子集。
如果子集可以进一步分割，则递归重复该过程。
如果不能，则将子集声明为叶子并保存第一个类的概率。
约束：
当满足以下条件之一时停止分割：
最大树深度
节点中的最小实例数
叶子的最大数量

即使达到约束，也要通过创建必要的叶子来完成树。
叶子的数量保存在 leafs_cnt 变量中。该方法不返回任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</guid>
      <pubDate>Fri, 26 Jul 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>具有 10k 行独特上下文的合成 PII 数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</link>
      <description><![CDATA[我正在寻找一个包含 10,000 行数据的合成数据集，其中包含各种类型的个人身份信息 (PII)，用于分类问题。数据应按段落格式化，并且每个段落应具有唯一的上下文。
我需要涵盖不同类型的 PII 数据，例如
[&quot;地址&quot;,
&quot;银行 • 帐号&quot;
&quot;信用卡 • 卡号&quot;
&quot;电子邮件地址&quot;
&quot;政府 - 身份证号码&quot;
&quot;个人姓名&quot;
&quot;密码&quot;
&quot;电话号码&quot;
&quot;密钥•（又称私钥）&quot;
121]
&quot;用户 ID&quot;,
&quot;出生日期&quot;,&quot;性别&quot;]

此外，每个段落在上下文中都是不同的，这一点至关重要。我尝试过使用 Faker，但它依赖于占位符模板，例如：
templates = [
&quot;{intro} {name} 出生于 {dob}，住在 {address}。您可以通过电子邮件 {email} 或电话 {phone} 联系他们。{closing}&quot;,
&quot;{intro} {name} 的社会安全号码是 {ssn}，护照号码是 {passport}。他们的信用卡号是 {ccn}。{closing}&quot;,
&quot;{intro} {name} 在 {license_year} 年获得了驾照号码 {dl}。 {closing}&quot;,
&quot;{intro} {name} 的电子邮件地址是 {email}，家庭住址是 {address}。他们出生于 {dob}，电话号码是 {phone}。{closing}&quot;,
&quot;{intro} {name} 的全名是 {name}，出生于 {dob}。他们的联系信息包括电话号码 {phone} 和电子邮件 {email}。他们居住在 {address}。{closing}&quot;
]

问题是这些句子在模板中重复出现，导致上下文变化有限。
我也尝试过使用 Kaggel，但它的结果没有涵盖所有必需的 PII 数据类型。还检查了 Github 存储库，但没有找到任何可靠的解决方案。
我正在寻找一种生成完全随机且唯一段落的方法。有人可以建议一种以编程方式创建具有多样化和独特上下文的合成 PII 数据的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</guid>
      <pubDate>Thu, 25 Jul 2024 16:19:35 GMT</pubDate>
    </item>
    <item>
      <title>在 pythonanywhere 中输入后预测页面未显示[关闭]</title>
      <link>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</link>
      <description><![CDATA[我使用 pythonanywhere 来实现 flask 项目。我能够在 localhost 上实现它，但无法在 pythonanywhere 中实现它。只显示输入页面，提交后需要很长时间，并且不显示下一页。
可能的原因是什么，整个文件大小仅在 2MB 以下
我希望预测显示在结果页面中。
输入并加载多次后，网站显示如下图片
错误日志图片
服务器日志 2024-07-27 10:05:15 *** 于 [2024 年 7 月 27 日星期六 10:05:12] 启动 uWSGI 2.0.20 (64bit) ***
2024-07-27 10:05:15 编译版本：9.4.0 于 2022 年 7 月 22 日 18:35:26
2024-07-27 10:05:15操作系统：Linux-5.15.0-1044-aws #49~20.04.1-Ubuntu SMP 2023 年 8 月 21 日星期一 17:09:32 UTC
2024-07-27 10:05:15 节点名称：green-liveweb27
2024-07-27 10:05:15 机器：x86_64
2024-07-27 10:05:15 时钟源：unix
2024-07-27 10:05:15 pcre jit 已禁用
2024-07-27 10:05:15 检测到的 CPU 核心数：4
2024-07-27 10:05:15 当前工作目录：/home/ANCYPADMANABHAN1
2024-07-27 10:05:15 检测到的二进制路径：/usr/local/bin/uwsgi
2024-07-27 10:05:15 *** 转储内部路由表 ***
2024-07-27 10:05:15 [规则：0] 主题：path_info regexp：.svgz$ 操作：addheader：Content-Encoding：gzip
2024-07-27 10:05:15 *** 内部路由表结束 ***
2024-07-27 10:05:15 chdir() 到 /home/ANCYPADMANABHAN1/mysite
2024-07-27 10:05:15 您的进程数限制为 256
2024-07-27 10:05:15 您的内存页面大小为 4096字节
2024-07-27 10:05:15 检测到的最大文件描述符数：123456
2024-07-27 10:05:15 从文件 /etc/mime.types 构建 mime-types 字典...
2024-07-27 10:05:15 找到 567 个条目
2024-07-27 10:05:15 锁定引擎：pthread 健壮互斥锁
2024-07-27 10:05:15 thunder 锁定：已禁用（您可以使用 --thunder-lock 启用它）
2024-07-27 10:05:15 Python 版本：3.10.5（main，2022 年 7 月 22 日，17:09:35）[GCC 9.4.0]
2024-07-27 10:05:15 *** Python 线程支持已被禁用。您可以使用 --enable-threads *** 启用它
2024-07-27 10:05:15 Python 主解释器在 0x55c2aa1a5e80 处初始化
2024-07-27 10:05:15 您的服务器套接字监听积压限制为 100 个连接
2024-07-27 10:05:15 您对工人优雅操作的怜悯是 60 秒
2024-07-27 10:05:15 将请求主体缓冲大小设置为 65536 字节
2024-07-27 10:05:15 为 1 个核心映射了 334256 字节（326 KB）
2024-07-27 10:05:15 *** 操作模式：单进程 ***
2024-07-27 10:05:15 初始化 38 个指标
2024-07-27 10:05:15 WSGI 应用程序 0 (mountpoint=&#39;&#39;) 在解释器 0x55c2aa1a5e80 pid: 1 (默认应用程序) 上 2 秒内准备就绪
2024-07-27 10:05:15 *** uWSGI 正在多解释器模式下运行 ***
2024-07-27 10:05:15 正常 (RE) 生成 uWSGI 主进程 (pid: 1)
2024-07-27 10:05:15 生成 uWSGI 工作进程 1 (pid: 12, 核心: 1)
2024-07-27 10:05:15 指标收集器线程已启动
2024-07-27 10:05:15 生成2 个用于 uWSGI worker 1 的卸载线程
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - *** HARAKIRI ON WORKER 1 (pid: 12, try: 1) ***
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI !!! worker 1 状态 !!!
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI [core 0] 10.0.0.20 - POST /预测自 1722074734
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI !!! 工人 1 状态结束 !!!
2024-07-27 10:15:36 该死！工作者 1 (pid: 12) 死亡，被信号 9 杀死 :( 尝试重生...
2024-07-27 10:15:36 重生 uWSGI 工作者 1 (新 pid: 17)
2024-07-27 10:15:36 为 uWSGI 工作者 1 生成 2 个卸载线程
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:36 2024 - SIGPIPE：根据请求 /favicon.ico (ip 10.0.0.20) 写入已关闭的管道/套接字/fd（可能是客户端断开连接）!!!
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:36 2024 - uwsgi_response_writev_headers_and_body_do(): GET /favicon.ico (10.0.0.20) 期间管道 [core/writer.c 第 306 行] 损坏
2024-07-27 10:15:36 宣布我对皇帝的忠诚..]]></description>
      <guid>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</guid>
      <pubDate>Thu, 25 Jul 2024 13:37:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 JSON 数据降低掩码质量以训练 U 网模型</title>
      <link>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</link>
      <description><![CDATA[我想用 json 格式屏蔽我的图像作为 u net 训练模型的数据。
我使用下面的代码来屏蔽它们：

import json
import numpy as np
import cv2
import os

# 包含 JSON 文件的文件夹路径
json_folder = os.path.expanduser(&#39;~/Desktop/jeson&#39;) # 包含 JSON 文件的文件夹
# 用于保存掩码图像的文件夹路径
mask_folder = os.path.expanduser(&#39;~/Desktop/masks&#39;) # 用于保存掩码的文件夹

# 确保用于保存掩码的文件夹存在
os.makedirs(mask_folder, exist_ok=True)

# 列出文件夹中的所有 JSON 文件
for filename in os.listdir(json_folder):
if filename.endswith(&#39;.json&#39;):
json_file = os.path.join(json_folder, filename)

# 从 JSON 文件加载数据
with open(json_file) as f:
data = json.load(f)

# 创建一个空的掩码
mask = np.zeros((data[&#39;imageHeight&#39;], data[&#39;imageWidth&#39;]), dtype=np.uint8)

# 将区域添加到掩码
for shape in data[&#39;shapes&#39;]:
points = np.array(shape[&#39;points&#39;], dtype=np.int32)
if len(points) &gt; 0:
# 用白色填充由点定义的区域
cv2.fillPoly(mask, [points], 49)

# 使用 OpenCV 将掩码保存为 PNG 图像
mask_filename = os.path.splitext(filename)[0] + &#39;_mask.png&#39;
cv2.imwrite(os.path.join(mask_folder, mask_filename), mask)

print(&quot;转换完成，掩码图像已保存在 &#39;masks&#39; 文件夹中！&quot;)


但有一个有趣的问题。它在开始时只很好地掩盖了其中的几个，但其他的只用一条细线掩盖了。当我再次尝试使用此代码时，它会用一条细线掩盖所有数据。
例如在 labelme 工具中处理之前的图像：
在 lamelme 中处理之前的图像
例如屏蔽的 jeson 数据：
jeson 被屏蔽
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</guid>
      <pubDate>Wed, 24 Jul 2024 00:14:38 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>超时错误：[WinError 10060]</title>
      <link>https://stackoverflow.com/questions/76484091/timeouterror-winerror-10060</link>
      <description><![CDATA[ DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot;
HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;)
HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot;

 def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
if not os.path.isdir(housing_path):
os.makedirs(housing_path)
tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;)
urllib.request.urlretrieve(housing_url, tgz_path)
housing_tgz = tarfile.open(tgz_path)
housing_tgz.extractall(path=housing_path)
housing_tgz.close()

 fetch_housing_data()

 import pandas as pd

 def load_housing_data(housing_path=HOUSING_PATH):
csv_path = os.path.join(housing_path,&quot;housing.csv&quot;)
return pd.read_csv(csv_path)

 housing = load_housing_data()

我运行了这段代码，但出现了错误
&quot;TimeoutError: [WinError 10060] 连接尝试失败，因为连接方在一段时间后没有正确响应，或者由于连接的主机未能响应而建立的连接失败&gt;&quot;
&quot;URLError: &lt;urlopen error [WinError 10060] 连接尝试失败，因为连接方在一段时间后没有正确响应，或者由于连接的主机未能响应而建立的连接失败&gt;&quot;
有人能帮我解决这个代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/76484091/timeouterror-winerror-10060</guid>
      <pubDate>Thu, 15 Jun 2023 17:20:49 GMT</pubDate>
    </item>
    <item>
      <title>Py4JJavaError：调用 z:org.apache.spark.api.python.PythonRDD.runJob 时发生错误。ModuleNotFoundError：没有名为“numpy”的模块</title>
      <link>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</guid>
      <pubDate>Tue, 03 Dec 2019 08:28:51 GMT</pubDate>
    </item>
    </channel>
</rss>