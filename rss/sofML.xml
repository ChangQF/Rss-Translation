<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 07 Feb 2024 18:16:40 GMT</lastBuildDate>
    <item>
      <title>部署 scikit-learn 模型最便宜（或免费）的方法是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77956894/whats-the-cheapest-or-free-way-to-deply-scikit-learn-model</link>
      <description><![CDATA[我尝试使用 Vercel 函数，但安装 scikit-learn 模块导致超出 250MB 限制。这可能是由它的依赖关系引起的。
除了 AWS 或 gcloud 之外还有其他选择吗？我现在只想构建一个概念验证。]]></description>
      <guid>https://stackoverflow.com/questions/77956894/whats-the-cheapest-or-free-way-to-deply-scikit-learn-model</guid>
      <pubDate>Wed, 07 Feb 2024 17:42:58 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用支持向量回归器进行未来预测？</title>
      <link>https://stackoverflow.com/questions/77956486/is-it-possible-to-make-future-forecast-with-suport-vector-regressor</link>
      <description><![CDATA[我正在使用 cocoa 预测分析，并使用 SVR 完成数据建模。我的数据是从 2010 年到 2023 年，我需要对 2024 年的某些月份进行预测。但一开始我不明白如何在 SVR 中执行此操作。在 Prophet 中，我只需创建一个新日期列表并在模型中预测它们：
date_w_future = Prophet_model.make_future_dataframe(periods=60, freq=&#39;B&#39;)
预测 = Prophet_model.predict(date_w_future)

但是对于 SVR 我没有找到执行此操作的函数。谁能帮助我思考如何使用 SVR 预测未来。
下面是我的完整代码，用于处理、分离、建模和计算 y_hat 以验证现有数据：
从 sklearn.svm 导入 SVR

df = pd.read_csv(&#39;https://raw.githubusercontent.com/nunesisabella/Analise-Preditiv-Cacau/main/ICCO_2010_2023.csv&#39;,sep=&quot;&quot;)

df[&quot;ICCO_USD&quot;] = df[&quot;ICCO_USD&quot;].str.replace(&quot;,&quot;, &quot;&quot;).astype(float)
df[&#39;Data&#39;] = pd.to_datetime(df[&#39;Data&#39;], format=&#39;%d.%m.%Y&#39;)
df.set_index(&#39;数据&#39;, inplace=True)
df = df.sort_values(&#39;数据&#39;)

火车 = df.loc[df.index &lt; &#39;01-01-2020&#39;]
测试 = df.loc[df.index &gt;= &#39;01-01-2020&#39;]

训练数据 = 训练值
测试数据=测试.值

时间步=5

train_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]

test_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]

X_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]
X_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]

svr_model = SVR(kernel=&#39;poly&#39;,gamma=0.4, C=10, epsilon = 0.05)

fit_svr = svr_model.fit(X_train, y_train[:,0])

y_train_pred = fit_svr.predict(X_train).reshape(-1,1)
y_test_pred = fit_svr.predict(X_test).reshape(-1,1)

y_hat = (np.concatenate((y_train_pred, y_test_pred), axis=0)).flatten()
]]></description>
      <guid>https://stackoverflow.com/questions/77956486/is-it-possible-to-make-future-forecast-with-suport-vector-regressor</guid>
      <pubDate>Wed, 07 Feb 2024 16:41:04 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型在时间序列上的实现</title>
      <link>https://stackoverflow.com/questions/77955882/implementation-of-lstm-model-to-a-time-serie</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77955882/implementation-of-lstm-model-to-a-time-serie</guid>
      <pubDate>Wed, 07 Feb 2024 15:08:59 GMT</pubDate>
    </item>
    <item>
      <title>回归树中的主导特征使所有其他特征无关/0 [关闭]</title>
      <link>https://stackoverflow.com/questions/77954893/dominant-feature-in-regression-tree-makes-all-other-features-irrelevant-0</link>
      <description><![CDATA[我正在使用房地产数据集（高基数），因此我选择了 scikit-learn 回归树模型来尝试根据相关特征预测房屋的价格。我有这些功能 [&#39;Suburb&#39;,&#39;Address&#39;, &#39;Distance&#39;, &#39;Bedroom2&#39;, &#39;Bathroom&#39;, &#39;CouncilArea&#39;,&#39;Price&#39;]。
问题是，当我使用 Address it 时，它会使所有其他功能（“郊区”、“距离”、“卧室 2”、“浴室”、“议会区域”）基本上变得多余。地址是一个非常重要的功能，因此我正在尝试将其纳入其中。
我已经多次执行预处理和标准化步骤来尝试不同的结果。我将给出一个基本概述：
第一种方法最少预处理：

将原始 df 通过（这是基本的预处理，考虑 nan vals 并替换它们。）。
目标编码我的非 int （分类变量）并在整个 X （训练和测试）变量上使用标准标量。 （关于 Standardscaler，我也没有使用它，但有一个小的不明显的差异）

第二种方法标准化：

标准预处理（处理和替换 nan val）
评估数据偏差
我通过 Box Cox 运行了高度倾斜/非对称的值（用 IHS 进行了实验（有些值为负值（负值只是跳过了标准化，因为倾斜还不错），其他列一些数据点 == 0 所以我添加了小值（0.01）到 0 值以使它们为正）和 Log 变换）。 Box cox 标准化了所选值的最佳值，因此我使用了这个（请注意，我没有在所有适用的列上使用 Box Cox，仅在具有高偏差的列上使用 Box Cox，不确定这是否是标准流程）
将新结果和其余分类变量附加回 df
目标编码 X 分类变量（训练/测试）
使用标准标量
遍历回归树（使用 max_leaf，3-25 的差异非常小，因此其他特征变得明显，但它们包含 epsilon 值，所以不是真的）

第三种方法，省略地址：

重复第一种和第二种方法
结果还不错，但证明不使用“地址”让位于其他重要功能！

我还使用线性回归模型重复了这些过程，但它并不能很好地捕捉数据的复杂性质。我正在尝试使其适用于回归树。
我还使用线性回归模型重复了这个过程，但它没有很好地捕捉数据的复杂性质。]]></description>
      <guid>https://stackoverflow.com/questions/77954893/dominant-feature-in-regression-tree-makes-all-other-features-irrelevant-0</guid>
      <pubDate>Wed, 07 Feb 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>了解变量选择和调整后的 randomForestSRC 行为 [关闭]</title>
      <link>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</guid>
      <pubDate>Wed, 07 Feb 2024 12:43:24 GMT</pubDate>
    </item>
    <item>
      <title>在网络之间共享权重时联合或单独的优化器</title>
      <link>https://stackoverflow.com/questions/77954817/joint-or-seperate-optimizers-when-sharing-weights-between-networks</link>
      <description><![CDATA[我想知道，这两种情况有什么副作用。

netA 和 netB 共享权重，但各有一个优化器
netA 和 netB 共享权重，但使用单个优化器

结果非常相似，但不相等。
考虑这个例子
导入火炬
将 torch.nn 导入为 nn

火炬.manual_seed(0)

share_fc2 = 真
批量大小 = 4
通道 = 2

Training_data_A = torch.rand((batch_size, 通道))
Training_data_B = torch.rand((batch_size, 通道))


类 Net(torch.nn.Module):
    “”“最小网络”“”

    def __init__(self) -&gt;; __init__(self) -&gt;没有任何：
        超级().__init__()
        self.fc1 = nn.Linear(通道、通道、偏差=False)
        self.fc2 = nn.Linear(通道、通道、偏差=False)

    def 前向（自身，x）：
        返回 self.fc2(self.fc1(x))


netA = Net().requires_grad_()
netB = Net().requires_grad_()

如果共享_fc2：
    # 将 fc2 替换为 netA.fc2
    netB.fc2 = netA.fc2

lossA = netA(training_data_A).mean()
lossB = netA(training_data_A).mean()

lossA.backward()
lossB.backward()

选项 1
# 选项 1（独立运行，使用种子）
optA = torch.optim.Adam(params=netA.parameters()) # 包含共享的 fc2
optB = torch.optim.Adam(params=netB.parameters()) # 包含共享的 fc2
optA.step()
optB.step()

打印（列表（netA.parameters（）））
打印（列表（netB.parameters（）））

输出

选项 2
opt = torch.optim.Adam(
    参数=(
        list(netA.parameters()) # 包含 netA.fc1、netA/B.fc2
        + list(netB.parameters())[:1] # 包含 netA.fc1
    ）
）
opt.step()
打印（列表（netA.parameters（）））
打印（列表（netB.parameters（）））


输出

问题

是Adam内部参数调整造成的差异，可以忽略
差异是由于梯度计算和应用中的数学原因造成的吗？
一种选择比另一种更好吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77954817/joint-or-seperate-optimizers-when-sharing-weights-between-networks</guid>
      <pubDate>Wed, 07 Feb 2024 12:42:07 GMT</pubDate>
    </item>
    <item>
      <title>如何改进空间约束的凝聚层次聚类，使 99% 的点不落入单个聚类？</title>
      <link>https://stackoverflow.com/questions/77954721/how-to-improve-spatially-constrained-agglomerative-hierarchical-clustering-so-99</link>
      <description><![CDATA[我尝试运行以下代码，按照标题所述操作 - 但如何诊断我的数据以避免 99% 的点集中到单个集群中？
# 标准化上下文字符
cont_std = RobustScaler().fit_transform(df)
cont_std = pd.DataFrame(cont_std, columns=df.columns)

％％时间
对于 K 中的 k：
    s[k] = []
    分贝[k] = []
    np.random.seed(123456) #为了重现性
    模型 = AgglomerativeClustering(linkage=&#39;ward&#39;, 连接性=w.sparse, n_clusters=k)
    y = model.fit(cont_std)
    cont_std_[&#39;AHC_k&#39;+ str(k)] = y.labels_
    print(&#39;k=&#39; + str(k) + &#39;: &#39; + str(metrics.silhouette_score(cont_std, y.labels_, metric=&#39;euclidean&#39;))) 处的剪影
    s[k].append(metrics.silhouette_score(cont_std, y.labels_, metric=&#39;euclidean&#39;))
    
    davies_bouldin_score =metrics.davies_bouldin_score(cont_std, y.labels_)
    print(f&#39;davies boudin at k={k}: {davies_bouldin_score}&#39;)
    db[k].append(davies_bouldin_score)
]]></description>
      <guid>https://stackoverflow.com/questions/77954721/how-to-improve-spatially-constrained-agglomerative-hierarchical-clustering-so-99</guid>
      <pubDate>Wed, 07 Feb 2024 12:29:46 GMT</pubDate>
    </item>
    <item>
      <title>重新识别网络训练和验证[关闭]</title>
      <link>https://stackoverflow.com/questions/77953442/re-identification-network-training-and-validation</link>
      <description><![CDATA[我正在为 VisDrone 执行对象跟踪任务，并且为此目的使用 DeepSORT  。但是该存储库中的嵌入程序在与 VisDrone 数据集非常不同的数据集上进行了预训练。
我想在 VisDrone 上训练 mobilenetv2 作为 ReID 的嵌入网络。据我了解，如果只有 1 个摄像头，我的训练数据集必须包含一组{图像（对象的），标签（该对象的 ID）}。因此，如果以交叉熵损失的方式进行训练，网络将学习对象的 ID。但是，如果我尝试使用 ID 不同的其他对象来验证它，网络将不会预测验证数据集中的 ID。相反，它会预测他们的 ID。训练数据集和验证损失会很高。
我的第一个问题：我关于网络学习对象ID的想法是否正确？因为到处都写着 embedeer 返回对象的特征向量，其长度可能会有所不同，而不考虑 ID 的总数。
我的第二个问题：如何正确验证我的 ReID 网络？
如果有人向我解释为什么学习对象的 ID，然后在另一个数据集上使用这个网络通常是有效的，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77953442/re-identification-network-training-and-validation</guid>
      <pubDate>Wed, 07 Feb 2024 09:10:08 GMT</pubDate>
    </item>
    <item>
      <title>我该如何进行模型预测？</title>
      <link>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77951971/how-can-i-do-model-predict</guid>
      <pubDate>Wed, 07 Feb 2024 02:51:25 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 60 万条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中实现大数据集的交叉验证而不将整个数据集加载到内存中？</title>
      <link>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</link>
      <description><![CDATA[我目前正在处理一个机器学习项目的大型数据集，并选择使用 TensorFlow 的 tf.data API 来高效管理数据加载和预处理，而无需将整个数据集加载到内存中。这种方法对于我的初始训练效果很好。
但是我很难实现交叉验证。据我了解，TensorFlow 本身并不支持直接通过 tf.data API 进行交叉验证，而与 Keras 集成进行交叉验证似乎需要先将数据加载到内存中。这对我的使用来说是有问题的，因为立即将整个数据集加载到内存中违背了使用 tf.data 的目的。
我正在寻找一种解决方法或方法来实现与 TensorFlow 的按需数据加载兼容的交叉验证。理想情况下，我希望保持 tf.data 的内存效率，同时对模型的评估进行交叉验证。
有没有办法使用 Keras 或任何其他库进行交叉验证，而不需要我将所有数据集加载到内存中？]]></description>
      <guid>https://stackoverflow.com/questions/77947993/how-to-implement-cross-validation-with-large-datasets-in-tensorflow-without-load</guid>
      <pubDate>Tue, 06 Feb 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>时间序列分析：分类变量的预测</title>
      <link>https://stackoverflow.com/questions/72488489/time-series-analysis-forecasting-of-categorical-variables</link>
      <description><![CDATA[我有一台机器在 1 分钟时间间隔内的故障发生数据（以 0 和 1 表示）。 0 代表未发生故障，1 代表发生特定故障。因此，连续0 表示在一段时间内没有发生故障，连续1 表示在一段时间内连续发生故障。
我提供了如下的示例数据结构，现在我如何对下面提供的数据进行时间序列分析故障 A，并根据分析如何进行预测，例如“故障 A 将在未来时间戳中何时发生？” 
# 时间序列多元
将 pandas 导入为 pd
将 numpy 导入为 np

df = pd.DataFrame({&#39;timestamp&#39;:pd.date_range(&#39;2022-05-01 00:01:00&#39;, period=18, freq=&#39;T&#39;),
                   &#39;故障代码&#39;:[&#39;A&#39;]*4+[&#39;B&#39;]*3+[&#39;A&#39;]*2+[&#39;C&#39;]*5+[&#39;B&#39;]*2+[&#39;A&#39;]* 1+[&#39;D&#39;]*1
                  })
df[&#39;脉冲&#39;] = 1

df_ts = df.pivot(index=“时间戳”, columns=“故障代码”, value=“脉冲”)
df_ts = df_ts.fillna(0)
显示（df_ts）



         故障代码 A B C D
时间戳
2022-05-01 00:01:00 1 0 0 0
2022-05-01 00:02:00 1 0 0 0
2022-05-01 00:03:00 1 0 0 0
2022-05-01 00:04:00 1 0 0 0
2022-05-01 00:05:00 0 1 0 0
2022-05-01 00:06:00 0 1 0 0
2022-05-01 00:07:00 0 1 0 0
2022-05-01 00:08:00 1 0 0 0
2022-05-01 00:09:00 1 0 0 0
2022-05-01 00:10:00 0 0 1 0
2022-05-01 00:11:00 0 0 1 0
2022-05-01 00:12:00 0 0 1 0
2022-05-01 00:13:00 0 0 1 0
2022-05-01 00:14:00 0 0 1 0
2022-05-01 00:15:00 0 1 0 0
2022-05-01 00:16:00 0 1 0 0
2022-05-01 00:17:00 1 0 0 0
2022-05-01 00:18:00 0 0 0 1

# 时间序列图
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set_theme(style=&quot;whitegrid&quot;) # darkgrid、whitegrid、dark、white 和ticks

故障=[&#39;A&#39;,
        &#39;B&#39;,
        &#39;C&#39;，
        &#39;D&#39;
       ]

plt.figure(figsize = (15,4))
sns.lineplot(数据=df_ts[故障])
plt.show()

上述数据的时间序列图
我要预测A的故障代码（0或1）
         
时间戳故障代码
2022-05-01 00:19:00 ?
2022-05-01 00:20:00 ?
2022-05-01 00:21:00 ？
2022-05-01 00:22:00 ？
2022-05-01 00:23:00 ？
2022-05-01 00:24:00 ？
2022-05-01 00:25:00 ？
]]></description>
      <guid>https://stackoverflow.com/questions/72488489/time-series-analysis-forecasting-of-categorical-variables</guid>
      <pubDate>Fri, 03 Jun 2022 10:48:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的决策树创建的分割实际上并未划分样本？</title>
      <link>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</link>
      <description><![CDATA[这是我对著名的 Iris 数据集进行二特征分类的基本代码： 
from sklearn.datasets import load_iris
从 sklearn.tree 导入 DecisionTreeClassifier，export_graphviz
从 graphviz 导入源

虹膜 = load_iris()
iris_limited = iris.data[:, [2, 3]] # 这仅获取花瓣长度 &amp;宽度。

# 我使用最大深度来避免过度拟合
# 并简化树，因为我将其用于教育目的
clf = DecisionTreeClassifier(criterion=&quot;基尼&quot;,
                             最大深度=3，
                             随机状态=42）

clf.fit(iris_limited, iris.target)

Visualization_raw = Export_graphviz(clf,
                                    out_file=无，
                                    特殊字符=真，
                                    feature_names=[“长度”,“宽度”],
                                    类名=iris.target_names,
                                    节点 ID=真）

可视化源 = 源（可视化_原始）
Visualization_png_bytes = Visualization_source.pipe(format=&#39;png&#39;)
将 open(&#39;my_file.png&#39;, &#39;wb&#39;) 作为 f：
    f.write（可视化_png_bytes）

当我检查树的可视化时，我发现了这一点：

乍一看这是一棵相当正常的树，但我注意到它有一些奇怪的地方。节点 #6 共有 46 个样本，其中只有一个是杂色的，因此该节点被标记为 virginica。这似乎是一个相当合理的停留地点。然而，由于某种我无法理解的原因，该算法决定进一步分为节点#7 和#8。但奇怪的是，仍然存在的 1 个 versicolor 仍然被错误分类，因为无论如何两个节点最终都具有 virginica 类。它为什么要这样做？它是否盲目地只关注基尼系数的下降，而不考虑它是否有任何影响——这对我来说似乎是奇怪的行为，而且我在任何地方都找不到它的记录。
是否可以禁用，或者这实际上是正确的吗？]]></description>
      <guid>https://stackoverflow.com/questions/50077562/why-is-my-decision-tree-creating-a-split-that-doesnt-actually-divide-the-sample</guid>
      <pubDate>Sat, 28 Apr 2018 14:25:02 GMT</pubDate>
    </item>
    <item>
      <title>RF：一个类别的 OOB 准确度较高，而另一个类别的准确度非常低，类别不平衡较大</title>
      <link>https://stackoverflow.com/questions/10306380/rf-high-oob-accuracy-by-one-class-and-very-low-accuracy-by-the-other-with-big</link>
      <description><![CDATA[我正在使用随机森林分类器对具有两个类别的数据集进行分类。

特征数量为 512 个。
数据比例为1:4。即，75% 的数据来自第一类，25% 来自第二类。
我使用了 500 棵树。

分类器产生 21.52% 的袋外错误。
第一类（由 75% 的训练数据表示）的每类误差为 0.0059。而第二类的分类误差非常高：0.965。
我正在寻找对此行为的解释，以及您是否有提高第二类准确性的建议。
我期待您的帮助。
谢谢
忘记说我正在使用 R 并且在上面的测试中使用了 1000 的节点大小。
这里我只用 10 棵树和节点大小 = 1 重复训练（只是为了给出一个想法），下面是 R 中的函数调用和混淆矩阵：

randomForest（公式 = Label ~ .，数据 = chData30PixG12，ntree = 10，重要性 = TRUE，节点大小 = 1，keep.forest = FALSE，do.trace = 50）

随机森林的类型：分类

树木数量：10

没有。每次分割尝试的变量数：22

OOB 错误率估计：24.46%

混淆矩阵：


 不相关、相关、class.error
 不相关 37954、4510、0.1062076
 相关8775、3068、0.7409440
]]></description>
      <guid>https://stackoverflow.com/questions/10306380/rf-high-oob-accuracy-by-one-class-and-very-low-accuracy-by-the-other-with-big</guid>
      <pubDate>Tue, 24 Apr 2012 21:37:50 GMT</pubDate>
    </item>
    </channel>
</rss>