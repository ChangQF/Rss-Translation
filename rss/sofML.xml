<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 20 Nov 2024 06:25:38 GMT</lastBuildDate>
    <item>
      <title>使用不同的损失来训练不同阶段的模型</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[我正在尝试以端到端的方式训练一个两阶段模型。但是，我想用不同的损失更新模型的不同阶段。例如，假设端到端模型由两个模型组成：model1 和 model2。输出是通过运行计算的
features = model1(inputs)
output = model2(features)

我想用 loss1 更新 model1 的参数，同时保持 model2 的参数不变。接下来，我想用 loss2 更新 model2 的参数，同时保持 model1 的参数不变。我的完整实现如下：
import torch
import torch.nn as nn

# 定义第一个模型
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# 定义第二个模型
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# 初始化模型
model1 = Net()
model2 = Net1()

# 初始化单独的每个模型的优化器
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# 样本输入和标签
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 
optimizer1.step()
optimizer.zero_grad() 
optimizer1.zero_grad() 

print(f&quot;Loss1 (Net): {loss1.item()}&quot;)
print(f&quot;Loss2 (Net1): {loss2.item()}&quot;)

但是，这将返回
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]，即 AsStridedBackward0 的输出 0，处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算其梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

完整的错误消息是
回溯（最近一次调用最后一次）：
文件&quot;，第 55 行，在&lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, 第 521 行, 在反向传播中
torch.autograd.backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, 第 289 行, 在反向传播中
_engine_run_backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, 第 769 行, 在 _engine_run_backward 中
return Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传播
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]，即 AsStridedBackward0 的输出 0，处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算其梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

我有点明白为什么会发生这种情况，但有办法解决这个问题吗？任何帮助都值得赞赏。]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>使用基于树的方法识别过度拟合特征</title>
      <link>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</link>
      <description><![CDATA[我有一份表格数据，我尝试过的所有基于树的方法（随机森林、XGBoost、Catboost、LightGBM 等）都过度拟合了。我尝试过更改 `ccp_alpha` 等参数，这会降低我的训练和验证指标。我将忽略过度拟合的程度，因为它与我的实际问题无关。
我正在尝试检查我的某些特征是否会导致过度拟合。以下是我对如何进行此操作的想法。
计算训练和验证数据上的特征重要性，现在如果我看到某些高重要性特征的特征重要性急剧下降，我会得出结论，是该特征导致了我的痛苦。我的方法有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</guid>
      <pubDate>Wed, 20 Nov 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>对来自不同研究的汇总数据进行建模，并确定每项研究的变量重要性</title>
      <link>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</link>
      <description><![CDATA[我有来自四项独立研究的数据。每项研究都研究了不同药物对治疗反应的影响。所有四项研究都有相同的基线变量和结果变量（治疗成功）。除了使用基线变量对治疗成功进行分类之外，我还想确定哪些基线变量对这四种药物的成功进行分类最重要。我计划使用 tidymodels 来做到这一点。即，使用一组由不同算法（xgboost、随机森林、svm）组成的工作流程，确定最佳表现工作流程并检查变量重要性。
我不确定如何最好地比较这四种药物的变量重要性。最好是整理所有数据并运行一个具有每种药物和每个基线变量之间相互作用效应（step_interact）的模型，还是运行四个单独的模型（每种药物一个）并比较四个模型中变量的重要性，还是做其他事情？我不确定第一种方法是否能让我分离出每种药物的特定变量的重要性。
如果您能提供任何关于如何最好地解决这个问题的想法，我们将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</guid>
      <pubDate>Wed, 20 Nov 2024 03:53:42 GMT</pubDate>
    </item>
    <item>
      <title>在 sklearn 中，模型训练完成后，可以将数据集上的“transform”替换为“predict”吗？</title>
      <link>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</link>
      <description><![CDATA[我理解 sklearn 中的 transform 和 predict 之间存在差异，但是由于两者都用于新数据集，如果我的目标只是通过 umap 和 clustering 等模型获得预测结果，我可以在模型训练后用 predict 替换使用 transform 来获得相同的结果集吗？

一些参考：
sklearn 的 transform() 和 predict() 方法有什么区别？
transform、fit_transform、predict 和 fit 之间的区别]]></description>
      <guid>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</guid>
      <pubDate>Wed, 20 Nov 2024 02:31:28 GMT</pubDate>
    </item>
    <item>
      <title>独热编码特征不匹配问题</title>
      <link>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</guid>
      <pubDate>Tue, 19 Nov 2024 23:21:56 GMT</pubDate>
    </item>
    <item>
      <title>何时使用深度学习进行机器人和欺诈检测，何时不使用它？[关闭]</title>
      <link>https://stackoverflow.com/questions/79205257/when-to-use-deep-learning-for-bot-and-fraud-detection-and-when-not-to-use-it</link>
      <description><![CDATA[我的数据集大约有 10 万行，是表格数据。
我的问题是：这里有使用深度学习的案例吗？
我目前的方法如下：1) 对历史数据进行监督训练，然后 2) 运行无监督机器学习模型，如聚类​​或异常检测。
然后，对于任何新用户，他们都会收到来自监督模型的预测分数，如果他们是异常，就会被标记。这样，我既可以平衡历史模式，又可以解释新的欺诈模式。]]></description>
      <guid>https://stackoverflow.com/questions/79205257/when-to-use-deep-learning-for-bot-and-fraud-detection-and-when-not-to-use-it</guid>
      <pubDate>Tue, 19 Nov 2024 22:38:13 GMT</pubDate>
    </item>
    <item>
      <title>Ollama 中使用 GGUF 格式微调 LLaMA 3.2 1B 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/79205128/issue-with-fine-tuned-llama-3-2-1b-model-in-ollama-using-gguf-format</link>
      <description><![CDATA[我最近使用 Unsloth 在自定义数据集上对 LLaMA 3.21B 基础模型进行了微调。微调后，我将模型导出为 GGUF 格式并尝试将其加载到 Ollama 中。虽然微调后的模型在 Google Colab 中经过训练的数据集上运行良好，但在我将 GGUF 文件上传到 Ollama 后，它开始产生乱码响应。
随后进行微调
数据集准备：我使用了 CSV 格式的自定义数据集。以下是我用于将数据集格式化为 Alpaca 样式提示的代码片段：
import pandas as pd
from datasets import Dataset

# 使用不同编码加载自定义 CSV 数据集
df = pd.read_csv(&quot;/content/llama3.2trainedDataset.csv&quot;, encoding=&#39;ISO-8859-1&#39;)

# 将 Pandas DataFrame 转换为 Hugging Face 数据集
dataset = Dataset.from_pandas(df)

# 定义 Alpaca 提示格式
alpaca_prompt = &quot;&quot;&quot;以下是描述临床试验的研究目标。根据给定的研究目标编写纳入和排除标准。

### 研究目标：
{}

### 响应（纳入和排除标准）：
{}&quot;&quot;&quot;

EOS_TOKEN = tokenizer.eos_token # 确保生成正确停止。

# 定义函数以格式化提示
def formatting_prompts_func(examples):
goals = examples[&quot;eligibilityCriteria&quot;] # 从“eligibilityCriteria”列中研究目标
criteria = examples[&quot;combined_criteria&quot;] # 从“combined_criteria”列中获取标准

texts = []
for objective, criterion in zip(objectives,criteria):
# 添加 EOS_TOKEN 以确保生成正确停止。
text = alpaca_prompt.format(objective, criterion) + EOS_TOKEN
texts.append(text)
return {&quot;text&quot;: texts}

# 将格式化函数应用于您的数据集
dataset = dataset.map(formatting_prompts_func, batched=True)

微调配置：我使用 Unsloth 的以下参数微调了模型：
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
import wandb # 导入 WandB

trainer = SFTTrainer(
model = model,
tokenizer = tokenizer,
train_dataset = dataset,
dataset_text_field = &quot;text&quot;,
max_seq_length = max_seq_length,
dataset_num_proc = 2,
packing = False, # 可以使短序列的训练速度提高 5 倍。
args = TrainingArguments(
per_device_train_batch_size = 2,
gradient_accumulation_steps = 4,
warmup_steps = 5,
num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
learning_rate = 2e-4,
fp16 = not is_bfloat16_supported(),
bf16 = is_bfloat16_supported(),
logs_steps = 1,
optim = &quot;adamw_8bit&quot;,
weight_decay = 0.01,
lr_scheduler_type = &quot;linear&quot;,
seed = 3407,
output_dir = &quot;outputs&quot;,
report_to = &quot;wandb&quot;, # 用于 WandB 等。
),
)

# 训练模型
trainer_stats = trainer.train()


保存模型：训练后，我使用以下代码以 GGUF 格式保存了模型：
model.save_pretrained(&quot;lora_model&quot;) # 本地保存
tokenizer.save_pretrained(&quot;lora_model&quot;)

# 保存为 GGUF 格式
model.save_pretrained_gguf(&quot;model&quot;, tokenizer)

上传到 Ollama：我下载了 unsloth.Q8_0.gguf 文件并将其加载到 Ollama：
来自 unsloth.Q8_0.gguf

问题

在 Google Colab 中测试微调模型时（在将其导出到 GGUF 之前），它与训练过的数据集完美配合。
但是，将其导出为 GGUF 格式并上传到 Ollama 后，该模型开始产生与训练数据集完全无关的乱码响应提示。

我的问题

为什么经过微调的 LLaMA 3.21B 模型在 Google Colab 中运行良好，但在使用 GGUF 文件在 Ollama 本地加载时却会产生乱码响应？
我在 Ollama 中使用的模板是否适合经过微调的模型？这个问题是否与 GGUF 文件的生成方式或 Ollama 如何解释模型有关？
]]></description>
      <guid>https://stackoverflow.com/questions/79205128/issue-with-fine-tuned-llama-3-2-1b-model-in-ollama-using-gguf-format</guid>
      <pubDate>Tue, 19 Nov 2024 21:36:18 GMT</pubDate>
    </item>
    <item>
      <title>小语言模型和传统人工智能模型的主要区别是什么？它们带来了哪些价值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79204699/what-is-the-main-difference-between-small-langiage-model-and-traditional-ai-mode</link>
      <description><![CDATA[SLM 应该处理公司数据
那么它们与我们迄今为止构建的传统 AI 模型有何不同？
它们与经过微调的 LLM 大型模型有何不同？
SML 能为传统 AI 模型带来什么价值？作为基础，了解这一点非常重要，我可以决定哪种模型最适合需求
向所有能在这里提供帮助的志愿者致以崇高的敬意]]></description>
      <guid>https://stackoverflow.com/questions/79204699/what-is-the-main-difference-between-small-langiage-model-and-traditional-ai-mode</guid>
      <pubDate>Tue, 19 Nov 2024 18:47:35 GMT</pubDate>
    </item>
    <item>
      <title>“机器学习是否是一个有前途的职业领域，还是我应该探索其他选择？” [关闭]</title>
      <link>https://stackoverflow.com/questions/79204113/is-machine-learning-a-promising-career-field-for-the-future-or-should-i-explor</link>
      <description><![CDATA[“我目前是一名 B.Tech 一年级学生，主修计算机科学工程，重点是人工智能和机器学习。虽然我发现机器学习领域很吸引人，但就职业前景和个人兴趣而言，我不确定它是否适合我。
我的一些问题是：
机器学习在未来 5-10 年内是否仍是一个不断增长且需求旺盛的领域？
在从事 ML/AI 职业时，我应该注意哪些具体挑战？
探索全栈 Web 开发、数据工程或网络安全等领域是否会提供更好的机会？
我已经开始学习 Python、DSA 和一些 Web 开发基础知识（HTML、CSS），并且我喜欢通过技术解决实际问题。我正在寻找专业人士或有过类似情况的学生的建议。
提前感谢您的见解！&quot;
&quot;我选择 AI/ML 作为我的专业，因为它似乎是一个趋势和有前途的领域，我的许多同龄人都选择了它。我已经开始学习 Python、数据结构和算法，并且探索了 NumPy 等库。我还喜欢解决问题，并参与了创建聊天机器人和食谱网站等小项目。
最近，我一直在探索全栈 Web 开发和 AI/ML，以多样化我的技能。然而，我发现很难决定我是否应该只专注于 AI/ML 或转向其他领域，如 Web 开发、数据工程或网络安全。&quot;]]></description>
      <guid>https://stackoverflow.com/questions/79204113/is-machine-learning-a-promising-career-field-for-the-future-or-should-i-explor</guid>
      <pubDate>Tue, 19 Nov 2024 15:44:24 GMT</pubDate>
    </item>
    <item>
      <title>“ValueError：形状不兼容”，但是在哪里呢？</title>
      <link>https://stackoverflow.com/questions/79202703/valueerror-shapes-are-incompatible-but-where</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79202703/valueerror-shapes-are-incompatible-but-where</guid>
      <pubDate>Tue, 19 Nov 2024 08:48:36 GMT</pubDate>
    </item>
    <item>
      <title>如何修改我的代码来处理 RGBX（4 通道）图像以进行语义分割？</title>
      <link>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</link>
      <description><![CDATA[我是该领域的新手，一直在关注 U-Net 教程，使用 3 通道 RGB 图像进行语义分割https://www.youtube.com/watch?v=68HR_eyzk00&amp;list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE&amp;index=2&amp;ab_channel=DigitalSreeni，对我来说效果很好。但是，我现在需要扩展管道以支持 4 通道 RGBX 图像（即 RGB + 其他通道），但我不确定如何修改代码以适应其他通道，尤其是预处理和 ImageDataGenerator 部分（我认为 ImageDataGenerator 不支持 4 通道图像）。
这是代码（将图像修补为（256 * 256 * 4）并将蒙版修补为（256*256）后）：
import os
import cv2
import numpy as np
import glob
from matplotlib import pyplot as plt
import tensorflow as tf
import splitfolders
import fragmentation_models as sm
from tensorflow.keras.metrics import MeanIoU
from sklearn.preprocessing import MinMaxScaler
from keras.utils import to_categorical

input_folder=&#39;我的图像和掩码的文件夹路径&#39;

output_folder=&#39;输出文件夹的路径&#39;
#按比例分割
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75,.25),group_prefix=None) 

#重新排列用于 keras 增强的文件夹结构

seed=24
batch_size=16 
n_classes=2 

scaler=MinMaxScaler()

BACKBONE=&#39;resnet34&#39; 
preprocess_input=sm.get_preprocessing(BACKBONE)

def preprocess_data(img, mask, num_class):
#缩放图像
img=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
img=preprocess_input(img) #基于预训练的主干进行预处理
mask=to_categorical(mask, num_class)
return (img,mask)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
img_data_gen_args=dict(horizo​​ntal_flip=True, vertical_flip=True, fill_mode=&#39;reflect&#39;) #数据增强

image_datagen=ImageDataGenerator(**img_data_gen_args)
mask_datagen=ImageDataGenerator(**img_data_gen_args)

image_generator=image_datagen.flow_from_directory(train_img_path, class_mode=None, batch_size=batch_size, seed=seed)
mask_generator=image_datagen.flow_from_directory(train_mask_path, class_mode=None，color_mode=&#39;grayscale&#39;，batch_size=batch_size，seed=seed)

train_generator=zip(image_generator，mask_generator)

for (img, mask) in train_generator:
img, mask= preprocess_data(img, mask, num_class)
Yield (img, mask)

train_img_path=&#39;训练图像路径&#39;
train_mask_path=&#39;训练掩码路径&#39;
train_img_gen=trainGenerator(train_img_path, train_mask_path, num_class=2)

val_img_path=&#39;验证图像路径&#39;
val_mask_path=&#39;验证掩码路径&#39;
val_img_gen=trainGenerator(val_img_path, val_mask_path, num_class=2)

x, y=train_img_gen.__next__()

for i in range(0,3):
image=x[i]
mask=np.argmax(y[i], axis=2)
plt.subplot(1,2,1)
plt.imshow(image)
plt.subplot(1,2,2)
plt.imshow(mask, cmap=&#39;gray&#39;)
plt.show()

num_train_imgs=len(os.listdir(&#39;训练图像路径&#39;))
num_val_images=len(os.listdir(&#39;验证路径图像&#39;))
steps_per_epochs=num_train_imgs//batch_size
val_steps_per_epoch=num_val_images//batch_size

IMG_HEIGHT=x.shape[1]
IMG_WIDTH=x.shape[2]
IMG_CHANNELS=x.shape[3]

n_classes=2

model=sm.Unet(&#39;resnet34&#39;,coder_weights=&#39;None&#39;,input_shape=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),classes=n_classes,activation=&#39;softmax&#39;)
model.compile(&#39;Adam&#39;,loss=sm.losses.binary_crossentropy,metrics=[sm.metrics.iou_score,sm.metrics.FScore()])

history=model.fit(train_img_gen, steps_per_epoch=steps_per_epochs, epochs=100, verbose=1, validation_data=val_img_gen, validation_steps=val_steps_per_epoch)

]]></description>
      <guid>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</guid>
      <pubDate>Mon, 18 Nov 2024 20:06:05 GMT</pubDate>
    </item>
    <item>
      <title>如何实现 log_softmax() 来以更快的速度和更高的数值稳定性计算其值（和梯度）？</title>
      <link>https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better</link>
      <description><![CDATA[各种框架和库（例如 PyTorch 和 SciPy）提供了用于计算 log(softmax()) 的特殊实现，该实现速度更快且数值更稳定。但是，我无法在这些包中找到此函数 log_softmax() 的实际 Python 实现。
有人可以解释这是如何实现的吗？或者，能给我指出相关的源代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better</guid>
      <pubDate>Sat, 02 May 2020 23:18:14 GMT</pubDate>
    </item>
    <item>
      <title>如何正确地将不平衡的数据集拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</link>
      <description><![CDATA[我有一个航班延误数据集，在采样之前尝试将该数据集拆分为训练集和测试集。准时情况约占总数据的 80%，延误情况约占 20%。
通常，机器学习中训练集和测试集的大小比例为 8:2。但数据太不平衡了。因此，考虑到极端情况，大多数训练数据都是准时情况，而大多数测试数据都是延误情况，准确率会很差。
所以我的问题是如何正确将不平衡的数据集拆分为训练集和测试集？]]></description>
      <guid>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</guid>
      <pubDate>Sat, 27 Jul 2019 06:34:52 GMT</pubDate>
    </item>
    <item>
      <title>仅需 2 张图片即可自动进行人脸验证</title>
      <link>https://stackoverflow.com/questions/45351886/automatic-face-verification-with-only-2-images</link>
      <description><![CDATA[问题陈述：
给定两张图片，例如下面两张布拉德·皮特的图片，判断图片中是否包含同一个人。困难在于我们每个人只有一张参考图像，如何确定其他传入图像是否包含同一个人。

一些研究：
有几种不同的方法可以解决此任务，这些是

使用颜色直方图
面向关键点的方法
使用深度卷积神经网络或其他 ML 技术

直方图方法涉及根据颜色计算直方图并在它们之间定义某种度量，然后确定阈值。我尝试过的一个方法是地球移动距离。但是这种方法缺乏准确性。
因此，最好的方法应该是第二种和第三种方法的某种混合，以及一些预处理。
对于预处理，明显的步骤是：

运行人脸检测（例如 Viola-Jones）并分离包含人脸的区域
将所述人脸转换为灰度
运行眼睛、嘴巴、鼻子检测算法（可能使用 opencv 的 haar_cascades）
根据找到的标志对齐人脸图像

所有这些都是使用 opencv 完成的。
提取诸如 SIFT 和 MSER 之类的特征可产生介于73-76%。经过一些额外的研究，我偶然发现了这篇使用fisherfaces的论文。事实上，opencv 现在有能力创建 fisherface 检测器并对其进行训练，这很棒，而且效果非常好，达到了论文在耶鲁数据集上承诺的准确度。
问题的复杂之处在于，在我的情况下，我没有一个包含同一个人的多张图像的数据库来训练检测器。我只有一张对应于一个人的图像，而给出另一张图像，我想了解这是否是同一个人。
所以我感兴趣的是`
有人尝试过这样的事情吗？我应该研究哪些论文/方法/库？
您对如何解决问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/45351886/automatic-face-verification-with-only-2-images</guid>
      <pubDate>Thu, 27 Jul 2017 13:14:18 GMT</pubDate>
    </item>
    <item>
      <title>深度学习序列推理</title>
      <link>https://stackoverflow.com/questions/43880116/deep-learning-for-inferences-in-sequences</link>
      <description><![CDATA[我想使用深度学习技术来执行比隐马尔可夫模型（浅层模型）更好的推理任务？我想知道什么是最先进的深度学习模型来取代隐马尔可夫模型 (HMM)？该设置是半监督的。训练数据 X(t)、Y(t) 是一个时间序列，具有显著的时间相关性。此外，还有大量未标记的数据，即只有 X(t) 而没有 Y(t)。在阅读了许多论文后，我缩小了以下模型 -&gt; 条件限制玻尔兹曼机（Ilya Sustkever 硕士论文）并使用深度信念网络进行无监督预训练（或使用变分自动编码器进行预训练）。我对这个领域很陌生，想知道这些技术是否已经过时了。]]></description>
      <guid>https://stackoverflow.com/questions/43880116/deep-learning-for-inferences-in-sequences</guid>
      <pubDate>Tue, 09 May 2017 21:25:33 GMT</pubDate>
    </item>
    </channel>
</rss>