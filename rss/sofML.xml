<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 08 Aug 2024 03:18:17 GMT</lastBuildDate>
    <item>
      <title>人工智能技术用于查找两个时间序列之间的相关性/模式/共同趋势</title>
      <link>https://stackoverflow.com/questions/78846294/ai-techniques-to-find-correlation-pattern-common-trend-between-two-time-series</link>
      <description><![CDATA[我有一个想法，使用人工智能技术在两个连续的时间序列数据之间找到有用的信息。结果可以是相关值、共同模式或趋势等，输出结果如 TS1（时间序列 1）数据与 TS2 数据相关，反之亦然。
稍后我想特别指出这种关系究竟发生在哪里，以及它是什么类型的效果。

例如：
输入：过去 5 年的 TS1 和 TS2 数据。[浮点/双精度值]
过程：寻找相关性/模式/共同趋势。[这是我寻求指导的部分。]
输出：TS1 的变化每个月都会对 TS2 产生负面影响。 [输出文本可以由

过程部分的结果组成。]

加载数据、将其传递给模型/系统，并为非技术人员解释结果并不是一项艰巨的任务。对我来说，有趣的部分是如何找到某种关系。
到目前为止，我已经使用了 Person、Spearman 和 Kendall 相关性，并且根据我的要求，它工作得很好。但是，我想了解和使用多种技术，尤其是高级统计和机器学习模型。
由于我是时间序列数据的新手，我对选择正确的路径来实现上述目标的知识有限。所以，有人可以指导我哪些高级技术/模型（静态、机器学习等）适合找到两个连续时间序列数据之间的关系？
提前谢谢您。
祝您有美好的一天！ :)]]></description>
      <guid>https://stackoverflow.com/questions/78846294/ai-techniques-to-find-correlation-pattern-common-trend-between-two-time-series</guid>
      <pubDate>Thu, 08 Aug 2024 02:13:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Sarimax 预测日期？</title>
      <link>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</link>
      <description><![CDATA[我正在使用 Sarimax 进行预测：
# 仅使用 VENDA 过滤数据集
df_int_aux = pd.DataFrame(df_internal[&#39;VENDA&#39;])

# 使用训练数据创建模型
train = round(len(df_int_aux) * 0.85) 
test = len(df_int_aux) - train

model_val = sm.tsa.statespace.SARIMAX(df_int_aux[&quot;VENDA&quot;][:train], order=(0,0,1), seasonal_order=(1, 1, 1, 4), exog=df_internal[&#39;C_EF_VENDA&#39;][:train])

# 拟合模型
model_val_fit = model_val.fit()

# 预测测试数据
validation = model_val_fit.get_forecast(steps=test, exog=df_internal[&#39;C_EF_VENDA&#39;][-test:]) 
validation_mean = validation.predicted_mean

但是，validation_mean 数据集未显示未来日期。它显示的数字索引范围从 101 到 118。数据集有 109 行。我使用前 100 行进行训练，因此第 101 行到第 118 行是模型的预测值。
为什么没有显示预计日期？我该如何解决这个问题？
以下是数据集的示例。可能是因为日期没有遵循特定的频率或模式，所以没有显示日期？
DATE VENDA C_EF_VENDA
2022-01-01 6.004414 12.122044
2022-01-11 10.933905 22.073975
2022-01-18 11.589626 23.397781
2022-01-25 21.005069 42.406200
2022-02-01 8.639416 14.461015
2022-02-08 16.847755 28.200475
2022-02-15 17.289413 28.939740
2022-02-22 16.966222 28.398770
]]></description>
      <guid>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</guid>
      <pubDate>Wed, 07 Aug 2024 21:30:19 GMT</pubDate>
    </item>
    <item>
      <title>数值数据中的异常值检测问题</title>
      <link>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</link>
      <description><![CDATA[我目前正在进行一个数据分析项目，其中我使用 Z 分数来检测数据集数值列中的异常值。但是，我遇到了一个问题，合法的数据点被标记为异常值，我不确定为什么会发生这种情况。
这是我正在做的事情：

缺失值的插补：我使用 sklearn.impute 中的 IterativeImputer 来填充数值列中的缺失值。

异常值检测：我计算每个数值列的 Z 分数，以使用阈值 3 来检测异常值。
例如，我有一条关于埃及古典式摔跤运动员 Yasser Abdel Rahman Sakr 的记录，其属性如下：

体重：120 公斤
身高：180 厘米



尽管这些是合理的测量值，但该记录被标记为我的代码中的异常值。其他记录也出现了此问题。
以下是我的代码的相关部分：
import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer

# 假设“数据”已定义并加载
numeric_cols = data.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
categorical_cols = data.select_dtypes(include=[&#39;object&#39;]).columns

# 在数字列中插入缺失值
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
df_numeric = pd.DataFrame(mice_imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)

# 将插入的数字列与原始分类列合并列
df_MICE = pd.concat([df_numeric, data[categorical_cols]], axis=1)

# 存储异常信息的字典
outliers_info = {}

for col in numeric_cols:
# 计算平均值和标准差
mean = df_MICE[col].mean()
std_dev = df_MICE[col].std()

# 如果 std_dev 为零，则避免除以零
if std_dev == 0:
print(f&quot;列 {col} 的标准差为零。跳过异常值检测。)
继续

# 计算 Z 分数
z_scores = (df_MICE[col] - mean) / std_dev

# 定义异常值阈值
阈值 = 3

# 查找异常值
outliers = df_MICE[np.abs(z_scores) &gt;阈值]

# 将异常值的数量和异常值样本存储在字典中
outliers_info[col] = {
&#39;count&#39;: len(outliers),
&#39;sample&#39;: outliers.head(1) # 一个异常值的样本
}

# 打印每个数值列的异常值数量和样本
for col, info in outliers_info.items():
print(f&#39;Column: {col}&#39;)
print(f&#39;Number of outliers: {info[&quot;count&quot;]}&#39;)
if info[&#39;count&#39;] &gt; 0：
print(&#39;样本异常值：&#39;)
print(info[&#39;sample&#39;])
else:
print(&#39;无异常值。&#39;)
print() # 打印空白行以提高可读性

问题：
尽管是真实且可信的记录，但 Yasser Abdel Rahman Sakr 的体重和身高被标记为异常值。其他记录也会出现此问题。
问题：

什么原因导致合法数据点被标记为异常值？
是否有任何改进或替代方法可以更好地处理此情况下的异常值检测？
我是否应该考虑其他因素或异常值检测方法？
]]></description>
      <guid>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</guid>
      <pubDate>Wed, 07 Aug 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Mediapipe 根据特定面部区域过滤面部标志坐标？</title>
      <link>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</guid>
      <pubDate>Wed, 07 Aug 2024 20:41:07 GMT</pubDate>
    </item>
    <item>
      <title>YOLov8 无法在本地机器上做出正确的预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</link>
      <description><![CDATA[我尝试在本地机器上运行 yolo v8，但做出了错误的预测，即它在图像顶部预测了很多 100% 的 bbox。所有 bbox 都在那里
本地机器结果
但是，如果我在 kaggle/colab 上运行它，它工作正常
vm 结果
vm/local 上的两台机器都是基于 cpu 的，没有使用 gpu。]]></description>
      <guid>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</guid>
      <pubDate>Wed, 07 Aug 2024 18:44:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中使用 knnsearch 设置 k 值</title>
      <link>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</link>
      <description><![CDATA[我有一个代码来对图像进行分类。
training1 = xlsread(&#39;Data Train&#39;);

% 提及训练数据矩阵在 excel 文件中的位置
training = [training1(:,1) training1(:,2) training1(:,3) training1(:,4) training1(:,5) training1(:,6) training1(:,7) training1(:,8) training1(:,9) training1(:,10) training1(:,11) training1(:,12) training1(:,13) training1(:,14) training1(:,15) training1(:,16) training1(:,17) training1(:,18) training1(:,19) training1(:,20) training1(:,21) training1(:,22) training1(:,23) training1(:,24)];

% 提及输入数据变量
Z=[MeanR MeanG MeanB MeanH MeanS MeanV VarRed VarGreen VarBlue VarH VarS VarV RangeR RangeG RangeB RangeH RangeS RangeV sdR sdG sdB sdH sdS sdV];

%执行 knn 分类
result = knnsearch(training,Z);

if (result&gt;=1 &amp;&amp; result&lt;=20)
set(handles.EditBox,&#39;string&#39;,&#39;Raw&#39;);
elseif (result&gt;=21 &amp;&amp; result&lt;=40)
set(handles.EditBox,&#39;string&#39;,&#39;Undercook&#39;);
elseif (result&gt;=41 &amp;&amp; result&lt;=60)
set(handles.EditBox,&#39;string&#39;,&#39;Cook&#39;);
elseif (result&gt;=61 &amp;&amp; result&lt;=80)
set(handles.EditBox,&#39;string&#39;,&#39;Rotten&#39;);
end

knnsearch语法是否只默认k值为1？
如何才能让knnsearch中的k值为5？
当我尝试将其更改为
k = 5;
result = knnsearch(training,Z,&#39;K&#39;,k); 

系统不显示分类结果。]]></description>
      <guid>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</guid>
      <pubDate>Wed, 07 Aug 2024 17:01:04 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中的学习率</title>
      <link>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</link>
      <description><![CDATA[在梯度下降算法中，我根据它们的导数更新B和M值，然后将它们与学习率值相乘，但是当我对L使用相同的值，例如0.0001时，它不能正常工作。减小或增加L值不起作用。作为一种解决方法，我不得不为b和m值设置不同的L值。这是正常的还是有错误？
import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
veri_seti = &quot;study_score_decreasing.csv&quot; #study_score_decreasing.csv #study_score_increasing.csv 
data = pd.read_csv(veri_seti)

# 梯度下降 Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
m_gradient = 0
b_gradient = 0
n = len(points)

for i in range(n):
x = points.iloc[i].study_time
y = points.iloc[i].score

m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
b_gradient += -(2/n) * (y - (m_next * x + b_next))

m = m_next - m_gradient * 0.0001 #(L = 0.0001)
b = b_next - b_gradient * 0.1 #(L = 0.1)

return m, b

# 图形选项 图表
def show_graph(m, b):
plt.scatter(data.study_time, data.score, color=&quot;red&quot;)
x_range = range(int(data.study_time.min()), int(data.study_time.max()) + 1)
plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
plt.xlabel(&#39;学习时间&#39;)
plt.ylabel(&#39;分数&#39;)
plt.title(&#39;学习时间与分数&#39;)
plt.show()
time.sleep(0.001)
print(&quot;=&gt; F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Ana Fonksiyon
def main(m, b, L, epochs):
print(&quot;=&gt; F(X):&quot;, m, &quot;X&quot;, b)

for i in range(epochs):
m, b = gradient_descent(m, b, data, L)
show_graph(m, b)

# 基础说明
main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)

我逐个更新了L值，得到了合乎逻辑的结果，但是用一个共同的L值，为什么解看起来不合逻辑？]]></description>
      <guid>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</guid>
      <pubDate>Wed, 07 Aug 2024 16:59:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 7 个类别训练图像分类器，但我的模型过度拟合，导致模型的准确性在训练过程中表现异常</title>
      <link>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</link>
      <description><![CDATA[我正在为特定汽车发动机部件的 7 种不同模型类型训练图像分类器。每个类别都有 308 张灰度图像，分辨率均为 1014x760。这些图像主要由白色屏幕上的发动机部件组成，每次拍照后都会旋转 60 度，因此数据集由看起来彼此非常相似的图片组成。我想训练我的模型 50 个 epoch，但在第 30 个 epoch 之后，准确率达到 1.0，而验证准确率停留在 0.2 左右。为什么结果这么奇怪？是不是图片太相似了？
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))

model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

以下是通过 tensorboard 表示的图形
我添加了2个model.add(Dropout(0.2))函数，但结果没有太大变化。
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

训练结果反馈退出函数]]></description>
      <guid>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</guid>
      <pubDate>Wed, 07 Aug 2024 15:51:31 GMT</pubDate>
    </item>
    <item>
      <title>构建模拟 SVM 模型的自定义分类器</title>
      <link>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</link>
      <description><![CDATA[我在代码中使用了以下 SVM：
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classes_report, confusion_matrix, f1_score

# 加载数据
data = pd.read_csv(&#39;data.csv&#39;)

# 分离特征 (X) 和目标变量 (y)
X = data.drop(columns=&#39;label&#39;)
y = data[&#39;label&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义小网格搜索的参数网格
param_grid = {
&#39;C&#39;: [0.1, 1, 10],
&#39;gamma&#39;: [&#39;scale&#39;, 0.01, 0.1]
}

# 执行带有交叉验证的网格搜索
grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;), param_grid, cv=3,scoring=&#39;f1_weighted&#39;, verbose=2, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# 来自网格搜索的最佳参数
best_params = grid_search.best_params_
print(f&#39;Best parameters: {best_params}\n&#39;)

# 训练 SVM 模型使用最佳参数
svm_best = SVC(kernel=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=best_params[&#39;gamma&#39;])
svm_best.fit(X_train_scaled, y_train)

# 对测试集进行预测
y_pred_best = svm_best.predict(X_test_scaled)

# 对改进模型的评估
print(&quot;改进的 SVM 模型评估&quot;)
print(confusion_matrix(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))
improved_f1 = f1_score(y_test, y_pred_best, average=&#39;weighted&#39;)
print(f&#39;改进的加权 F1 分数： {improved_f1}\n&#39;)


如您所见，我直接使用来自 sklearn 的 SVM 模型。我如何创建一个名为“分类器”的类，它将执行相同的操作并获得相同的结果？这可能吗？
我尝试创建类并使用每个函数的参数，但结果总是更糟。]]></description>
      <guid>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</guid>
      <pubDate>Wed, 07 Aug 2024 12:39:48 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH, YAML_FILE_PATH)`

我在运行训练脚本时收到此未来错误，我正在努力解决该错误：
FutureWarning：torch.cuda.amp.autocast(args...) 已弃用。
请改用 torch.amp.autocast(&#39;cuda&#39;, args...)。使用 torch.cuda.amp.autocast(amp)
]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow Pipeline 中对大型数据集应用图像增强？</title>
      <link>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</link>
      <description><![CDATA[我有一个图像数据集，每个图像包含一个 1 到 5 个字母的单词。我想使用深度学习对每个图像中组成单词的字符进行分类。这些图像的标签格式如下：
totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar
我正尝试将这些图像加载到 TensorFlow 管道中，以降低由于内存限制而导致的复杂性。下面是我从目录加载和处理图像和标签的代码：
def process_img(file_path):
label = get_label(file_path)
image = tf.io.read_file(file_path)
image = tf.image.decode_png(image, channels=1) 
image = tf.image.convert_image_dtype(image, tf.float32) 
target_shape = [695, 1204]
image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])

# 对标签进行编码
coded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
coded_label.set_shape([5, len(urdu_alphabets)])

return image,coded_label
input_dir = &#39;/kaggle/input/dataset/Data/*&#39;
images_ds = tf.data.Dataset.list_files(input_dir, shuffle=True)

train_count = int(tf.math.round(len(images_ds) * 0.8))
train_ds = images_ds.take(train_count)
test_ds = images_ds.skip(train_count)
train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)

train_ds 如下所示：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;
现在，我想对图像应用简单的增强，例如旋转、剪切、侵蚀和扩张。我最初使用了以下函数：
def augment(image, label):
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode=&#39;nearest&#39;, cval=0.0, interpolation_order=1)
image = tf.image.random_zoom(image, [0.85, 0.85])
image = tf.image.random_shear(image, 0.3)
image = tf.image.random_shift(image, 0.1, 0.1)
return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

但是，tf.image 中的许多函数都已弃用。如何以高效的方式在 TensorFlow 管道中将这些增强应用于图像？
注意：我可以通过不使用 TensorFlow 管道使用 NumPy 数组加载图像来执行这些增强，但我的数据集非常大（110 万张图像），因此我需要一种高效的方法来执行此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</guid>
      <pubDate>Wed, 31 Jul 2024 14:11:01 GMT</pubDate>
    </item>
    <item>
      <title>无法在 python 中安装 lap==0.4.0 库</title>
      <link>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</guid>
      <pubDate>Tue, 13 Jun 2023 09:55:26 GMT</pubDate>
    </item>
    <item>
      <title>Pyspark 中的过采样或 SMOTE</title>
      <link>https://stackoverflow.com/questions/53936850/oversampling-or-smote-in-pyspark</link>
      <description><![CDATA[我有 7 个类，总记录数为 115，我想对这些数据运行随机森林模型。但由于数据不足以获得高精度。所以我想对所有类进行过采样，使多数类本身获得更高的计数，然后少数类获得更高的计数。这在 PySpark 中可行吗？
+---------+-----+
| SubTribe|count|
+---------+-----+
| Chill| 10|
| Cool| 18|
|Adventure| 18|
| Quirk| 13|
| Mystery| 25|
| Party| 18|
|Glamorous| 13|
+---------+-----+
]]></description>
      <guid>https://stackoverflow.com/questions/53936850/oversampling-or-smote-in-pyspark</guid>
      <pubDate>Wed, 26 Dec 2018 20:31:36 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的神经元应该是异步的吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</link>
      <description><![CDATA[我正在设计一个神经网络，并试图确定我是否应该以这样一种方式编写它，即每个神经元都是 Erlang 中的自己的“进程”，或者我是否应该只使用 C++ 并在一个线程中运行一个网络（我仍然会通过在每个网络自己的线程中运行一个实例来使用我的所有核心）。
是否有充分的理由放弃 C++ 的速度而选择 Erlang 提供的异步神经元？]]></description>
      <guid>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</guid>
      <pubDate>Thu, 07 Jul 2016 16:17:43 GMT</pubDate>
    </item>
    </channel>
</rss>