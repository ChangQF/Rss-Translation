<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Jan 2024 21:12:27 GMT</lastBuildDate>
    <item>
      <title>单独更新 TensorFlow 中的指标</title>
      <link>https://stackoverflow.com/questions/77766703/updating-the-metrics-in-tensorflow-separately</link>
      <description><![CDATA[我编写了一个自定义指标：
类 ReidClassificationAccuracy(tf.keras.metrics.Metric)：
    def __init__(self, name=“accuracy_position_distance_thresholding”, with_visibility=False, with_occlusion=False,
                 参数={}, **kwargs):
        super(ReidClassificationAccuracy, self).__init__(name=name, **kwargs)
        self.accuracy = self.add_weight(name=“准确度”, 初始值设定项=“零”)
        self.num_samples = self.add_weight(name=“num_samples”, 初始值设定项=“零”)

    def update_state（自身，pids，cls_score_list，sample_weight =无）：
        总体评分 = 0
        对于 cls_score_list 中的 cls_score：
            总体 cls_score += cls_score
        self.accuracy = self.compute_accuracy(overall_cls_score, pids, [1])[0]
        返回自我准确度

    默认结果（自身）：
        返回 self.accuracy / self.num_samples

    def Reset_states(自身):
        self.accuracy.assign(0.)
        self.num_samples.分配(0.)

    defcompute_accuracy（自身，输出，目标，topk=[1]）：
        &quot;&quot;&quot;&quot;&quot;计算指定的 k 值的 precision@k&quot;&quot;&quot;&quot;
        最大k = 最大(topk)
        批量大小 = 目标大小(0)

        _, pred = 输出.topk(maxk, 1, True, True)
        pred = pred.t()
        正确 = pred.eq(target.view(1, -1).expand_as(pred))

        分辨率=[]
        对于 topk 中的 k：
            Correct_k = Correct[:k].view(-1).float().sum(0, keepdim=True)
            res.append( Correct_k )
        返回资源

现在，由于其输入与其他指标不同，并且不是 y_pred 和 y_trues，因此我想单独更新该指标的状态。目前，我使用以下方法更新指标：
model.compiled_metrics.update_state(y_trues, y_preds) # 更新指标

但是，我无法单独更新 Reid 指标。我尝试将其编写在 for 循环中，但 model.compiled_metrics 不可迭代。]]></description>
      <guid>https://stackoverflow.com/questions/77766703/updating-the-metrics-in-tensorflow-separately</guid>
      <pubDate>Fri, 05 Jan 2024 19:07:02 GMT</pubDate>
    </item>
    <item>
      <title>预测复数（绝对值和相位）的物理问题的线性回归</title>
      <link>https://stackoverflow.com/questions/77766687/linear-regression-for-physical-problem-predicting-complex-number-absolute-and-p</link>
      <description><![CDATA[我正在尝试进行线性回归，它进行一些物理测量，然后预测各种物理值。这些物理值之一是复数，因此我尝试将其分成两部分并分别预测幅度和相位。
我需要将幅度限制在 0 和 1 之间。在我的例子中，唯一可能的幅度值在 0 到 1 之间。允许网络预测超出此范围的值会解锁一系列退化解决方案，因此网络与真实预测相去甚远。
为了限制这一点，我对幅度输出应用了 sigmoid 激活函数。这有时有效。然而，幅度的真实值非常接近零（阶数 10^{-4}，这意味着它们接近 sigmoid 的尾部。这有时会导致损失和梯度在 100 个左右的 epoch 后变为 NaN。是否存在预测复数或限制回归问题输出的更好方法？
我尝试过降低学习率、增加批量大小并使用各种不同的架构，但没有成功。我还尝试在不使用 sigmoid 函数约束输出神经元的情况下解决问题，但这使得问题变得如此退化，以至于网络无法接近正确的解决方案。我还尝试过将幅度变换得更大（即接近 0.5），然后再将其变换回来。
就上下文而言，我的损失函数是
$$L = \frac{1}{n} \sum^n_i (f_{true} - f(a_{pred}, b_{pred}, |c_{pred}|, c_{pred}^{ \phi})^2$$
其中$f_{true})$是我们已知的东西，$a$和$b$是我们试图预测的东西，$c$是我们试图预测的复数。]]></description>
      <guid>https://stackoverflow.com/questions/77766687/linear-regression-for-physical-problem-predicting-complex-number-absolute-and-p</guid>
      <pubDate>Fri, 05 Jan 2024 19:03:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么 cartpole 奖励不收敛</title>
      <link>https://stackoverflow.com/questions/77766359/why-cartpole-reward-is-not-converge</link>
      <description><![CDATA[通过此图像，训练损失和期望值随着时间的推移而收敛，但每个情节的回报没有收敛，即使是伟大的情节。
这是我的训练循环代码：
对于范围内的剧集（500）：
    状态，信息 = env.reset()
    状态 = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
    总奖励 = 0

    对于 count() 中的 t：
        env.render()
        状态 = torch.FloatTensor(状态).to(设备)
        动作 = agent.selectAction(状态,agent.learn_step_counter)
        观察、奖励、完成、_ = env.step(action.item())[:4]

        总奖励+=奖励
        奖励 = torch.tensor([奖励], 设备=设备)
        next_state = torch.tensor(观察, dtype=torch.float32, device=device).unsqueeze(0)
        
        replay_buffer.push（状态，动作，next_state，奖励）
        
        状态 = 下一个状态
        
        代理内存 = replay_buffer
        代理.learn()
        如果完成：
            休息

在此处输入图像描述
有什么方法可以帮助看到收敛吗？]]></description>
      <guid>https://stackoverflow.com/questions/77766359/why-cartpole-reward-is-not-converge</guid>
      <pubDate>Fri, 05 Jan 2024 17:51:33 GMT</pubDate>
    </item>
    <item>
      <title>Python 中级，有志于 ML。朋友建议使用 Java/C++ 进行内存管理，并使用 Web 开发来获得更好的初学者机会。寻求建议[已关闭]</title>
      <link>https://stackoverflow.com/questions/77766281/intermediate-in-python-aspiring-for-ml-friend-suggests-java-c-for-memory-man</link>
      <description><![CDATA[我的 Python 水平处于中级，希望在机器学习 (ML) 领域发展职业生涯。然而，我的一位精通 Web 开发的朋友建议我不要坚持使用 Python。相反，他们建议学习其他语言，如 Java 或 C++，因为它们可以提供对内存管理的更深入的理解。此外，他们建议我考虑转向网络开发，并为初学者提供了更多的范围和机会。对于此事的任何指导或见解，我将不胜感激。
我探索了这两个领域的机会，但发现自己对 ML 比 Web 开发更感兴趣。]]></description>
      <guid>https://stackoverflow.com/questions/77766281/intermediate-in-python-aspiring-for-ml-friend-suggests-java-c-for-memory-man</guid>
      <pubDate>Fri, 05 Jan 2024 17:33:45 GMT</pubDate>
    </item>
    <item>
      <title>AWS ElasticBean CodePipeline 部署一次又一次失败。我缺少什么？</title>
      <link>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</guid>
      <pubDate>Fri, 05 Jan 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>我应该在代码中添加什么或者代码有什么问题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;

    导入操作系统


os.environ[&#39;LOKY_MAX_CPU_COUNT&#39;] = &#39;6&#39;
导入 pandas**强文本** 作为

从 sklearn.preprocessing 导入 StandardScaler


从 sklearn.neighbors 导入 KNeighborsClassifier


从 sklearn.metrics 导入 precision_score


从 sklearn.impute 导入 SimpleImputer
从 sklearn.metrics 导入分类报告


egitim_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\ZSCOREEGITIMDATA.xlsx&#39;)
test_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\ZSCORETESTDATA.xlsx&#39;)


print(&quot;训练数据中的 NaN 值：&quot;)
打印（egitim_data.isnull（）。sum（））


print(&quot;\n测试数据中的 NaN 值：&quot;)
打印（test_data.isnull（）。sum（））


X_train = egitim_data.drop(&#39;标签&#39;, axis=1)
y_train = egitim_data[&#39;标签&#39;]

X_test = test_data.drop(&#39;标签&#39;, axis=1)
y_test = test_data[&#39;标签&#39;]


imputer = SimpleImputer(策略=&#39;均值&#39;)

X_train_scaled = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)


X_test_scaled = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)


定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train_scaled)
X_test_scaled = 缩放器.transform(X_test_scaled)


knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)


y_pred_test = knn_model.predict(X_test_scaled)


准确度测试 = 准确度分数(y_test, y_pred_test)
print(“测试数据的模型准确度：”, precision_test)



分类代表=分类报告（y_test，y_pred_test，zero_division = 1）
print(&quot;分类报告：\n&quot;,classification_rep)


我这样编辑了代码。首先，我有7200条数据，我将它们分为90％的训练数据和10％的测试数据，并创建了两个excel文件。我在 Excel 中使用 Z 分数标准化编辑了这两个数据文件。后来，当我编辑代码时，它给出了以下输出。不过，不应该是46%，而且仍然有错误或缺失，但我找不到它。
&lt;前&gt;&lt;代码&gt;
训练数据中的 NaN 值：
流动持续时间 0
转发 IAT 分钟 0
Bwd IAT 最小值 0
转发 IAT 混合 0
Bwd IAT 最大 0
转发 IAT 平均值 0
Bwd IAT 平均值 0
流量包/秒 0
流字节/秒 0
流量 IAT 最小值 0
流量 IAT 最大 0
流量 IAT 平均值 0
流量 IAT 标准 0
活跃分钟 0
主动平均值 0
活跃最大 0
主动标准 0
空闲分钟 0
空闲平均值 0
空闲最大 0
空闲标准 0
标签0
数据类型：int64

测试数据中的 NaN 值：
流动持续时间 0
转发 IAT 分钟 0
Bwd IAT 最小值 0
转发 IAT 混合 0
Bwd IAT 最大 0
转发 IAT 平均值 0
Bwd IAT 平均值 0
流量包/秒 0
流字节/秒 0
流量 IAT 最小值 0
流量 IAT 最大 0
流量 IAT 平均值 0
流量 IAT 标准 0
活跃分钟 0
主动平均值 0
活跃最大 0
主动标准 0
空闲分钟 0
空闲平均值 0
空闲最大 0
空闲标准 0
标签0
数据类型：int64
测试数据上的模型精度：0.46111111111111114
分类报告：
                  精确召回率 f1-score 支持

音频流 0.61 0.42 0.50 90
       浏览 0.31 0.32 0.31 90
           聊天 0.28 0.34 0.31 90
  文件传输 0.60 0.71 0.65 90
           邮寄 0.55 0.39 0.45 90
            P2P 0.36 0.06 0.10 90
视频流 0.28 0.59 0.38 90
           网络电话 0.96 0.86 0.91 90

       准确度 0.46720
      宏观平均 0.49 0.46 0.45 720
   加权平均 0.49 0.46 0.45 720


进程已完成，退出代码为 0












]]></description>
      <guid>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</guid>
      <pubDate>Fri, 05 Jan 2024 12:46:17 GMT</pubDate>
    </item>
    <item>
      <title>IOPub 数据速率超过 Jupyter Notebook [重复]</title>
      <link>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</link>
      <description><![CDATA[`IOPub` 数据速率超出。

‘Jupyter’服务器将暂时停止发送输出
给客户端以避免崩溃。
要更改此限制，请设置配置变量
`--ServerApp.iopub_data_rate_limit`。

当前值：
ServerApp.iopub_data_rate_limit=1000000.0（字节/秒）
ServerApp.rate_limit_window=3.0（秒）

我尝试更新整个 Jupyter 笔记本，但仍然没有解决问题，我还尝试根据我的数据集更新值，但仍然显示同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</guid>
      <pubDate>Fri, 05 Jan 2024 10:04:06 GMT</pubDate>
    </item>
    <item>
      <title>实时跟踪脚本中 BYTETracker 初始化的问题</title>
      <link>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</link>
      <description><![CDATA[尝试使用 ByteTrack 库初始化实时跟踪脚本中的 BYTETracker 时，我遇到了 TypeError。该错误具体发生在 BYTETracker 类的 __init__ 方法中。
这是我的代码的相关部分：
跟踪器 = [BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument)]

我遇到的错误消息是：
TypeError：+ 不支持的操作数类型：“type”和“float”

我尝试通过创建一个 ByteTrackArgument 实例来解决这个问题，如下所示：
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]

但是，问题仍然存在。值得注意的是，我在脚本中使用 OpenCV 中的 FaceDetectorYN 进行人脸检测。
对于可能导致此错误的原因（尤其是与使用 FaceDetectorYN 结合使用）以及如何解决该错误有任何见解吗？
其他上下文：

我正在使用 ByteTrack 库进行实时跟踪。 链接
错误发生在 BYTETracker 类的 __init__ 方法中。
我将 ByteTrackArgument 的实例传递给 BYTETracker 构造函数。
使用 OpenCV 中的 FaceDetectorYN 执行人脸检测。 链接

这是我的完整代码：
&lt;前&gt;&lt;代码&gt;导入cv2
从 bytetracker 导入 BYTETracker
将 numpy 导入为 np

print(&quot;OpenCV 版本&quot;, cv2.__version__)

类 ByteTrackArgument：
    轨迹阈值 = 0.5
    轨道缓冲区 = 50
    匹配阈值 = 0.8
    纵横比阈值 = 10.0
    最小框面积 = 1.0
    mot20 = 假

MIN_THRESHOLD = 0.5 # 根据需要调整此阈值

# 初始化 ByteTrackArgument
byte_track_argument = ByteTrackArgument()

# 初始化BYTETracker
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]
def start_webcam_tracking():
    cap = cv2.VideoCapture(0) # 使用 0 作为默认网络摄像头，或提供网络摄像头 URL

    如果不是 cap.isOpened():
        print(“错误：无法打开相机。”)
        返回

    而真实：
        ret, 框架 = cap.read()
        如果不转：
            print(“错误：无法从相机读取帧。”)
            休息

        # 人脸检测代码
        检测器 = cv2.FaceDetectorYN.create(r&quot;C:\Users\gratu\live tracker\face_detection_yunet_2023mar.onnx&quot;, &quot;&quot;, (2200, 1200), Score_threshold=MIN_THRESHOLD)
        img_W = int(frame.shape[1])
        img_H = int(frame.shape[0])
        detector.setInputSize((img_W, img_H))

        检测= detector.detect(frame)[1]

        如果检测不是无：
            用于检测中的检测：
                x, y, 宽度, 高度 = 地图(int, 检测[:4])
                cv2.矩形(框架, (x, y), (x + 宽度, y + 高度), (0, 255, 0), 2)

                # 使用面部边界框更新跟踪器
                tracker.update(np.array([[x, y, x + 宽度, y + 高度]]), [frame.shape[0],frame.shape[1]])

        # 从 BYTETracker 获取跟踪结果
        online_targets = tracker.get_online_targets()

        如果 online_targets 不是 None：
            对于 online_targets 中的目标：
                x, y, x2, y2 = target # 根据BYTETracker的输出格式修改这部分
                cv2.矩形(框架, (x, y), (x2, y2), (255, 0, 0), 2)

        cv2.imshow(&#39;具有人脸检测和跟踪功能的网络摄像头&#39;, frame)

        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

    cap.release()
    cv2.destroyAllWindows()

如果 __name__ == “__main__”：
    start_webcam_tracking()

这是我尝试复制的教程，他们使用 yolox 进行人物检测，我尝试对人脸检测做同样的事情
链接]]></description>
      <guid>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</guid>
      <pubDate>Fri, 05 Jan 2024 09:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 正则化 Alpha - 权重还是叶子？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</link>
      <description><![CDATA[我已经阅读了有关 XGBoost 和 LightGBM 的论文以及它们的大部分文档，但无法找到明确的声明表明除了 GOSS &amp; EFB用于更快的学习，基本算法与XGBoost相同。
具体来说，XGBoost 目标函数中唯一的 L1 式正则化是 gamma*T，其中 gamma 是超参数，T 是叶子数量。 LightGBM 有 reg_alpha 参数，根据他们的文档，该参数应用 L1 正则化，该参数是否会惩罚叶子的数量，或者，在更传统的意义上，惩罚每个叶子的贡献的绝对值？
附注参考回归案例，我从未使用过该模型进行分类，因此不知道哪些部分仍然有效。]]></description>
      <guid>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</guid>
      <pubDate>Fri, 05 Jan 2024 08:25:18 GMT</pubDate>
    </item>
    <item>
      <title>我的 tfidf 向量自动编码器对于不同的文本输入产生相同的输出</title>
      <link>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</link>
      <description><![CDATA[我一直在尝试实现一个自动编码器来完成降维任务，输入到最近邻模型中，我意识到所有邻居最终彼此之间的距离为零，我意识到问题源于自动编码器，这对于不同的文本产生了类似的输出。
我已经尝试更改层数、正则化器以及输出层的激活（我使用线性，因为这应该是文本数据的最佳选择）。
我的 desc 和 req 数据帧的形状分别为 (31080, 471494) (31080, 169214)，大部分是稀疏数据，这是我在初始原始文本数据上使用 tfidf 获得的。
类 SparseDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, X, 批量大小):
        自我.X = X
        self.batch_size = 批量大小
        self.n_samples = X.shape[0]

    def __len__(自身):
        返回 int(np.ceil(self.n_samples / self.batch_size))

    def __getitem__(self, idx):
        开始 = idx * self.batch_size
        结束=分钟（开始+ self.batch_size，self.n_samples）
        batch_X = self.X[开始:结束].toarray()
        return batch_X, batch_X # 自动编码器获取与输入和目标相同的数据


组合_vecs_sparse = hstack([df_desc.sparse.to_coo(), df_req.sparse.to_coo()])


# 定义提前停止回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 3，restore_best_weights = True）

# 分割数据
X_train, X_val = train_test_split(combined_vecs_sparse, test_size=0.2, random_state=42)
X_train = X_train.tocsr()
X_val = X_val.tocsr()
before_memory = psutil.Process().memory_info().rss
input_dim = X_train.shape[1] # 特征数量

# 定义自动编码器结构
input_layer = 输入（形状=（input_dim，））

编码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（input_layer）
编码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
编码 = Dense(32,activation=&#39;relu&#39;, kernel_regularizer=l2(0.001))(encoded) # 编码表示
解码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
解码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（解码）
解码=密集（input_dim，激活=&#39;sigmoid&#39;）（解码）

自动编码器=模型（输入层，解码）

# 编译并训练自动编码器
autoencoder.compile（优化器=&#39;adam&#39;，损失=&#39;mse&#39;）


如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</guid>
      <pubDate>Fri, 05 Jan 2024 06:41:35 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默氏症 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。
该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每张图像预测相同的类别（“MCI - 轻度认知障碍”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
使用不同的数据集训练模型两次，但问题仍然存在。我现在已经走进了死胡同，不知道如何解决它。
TFLite 模型代码：
https://colab.research.google.com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=分享
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）

解释器 = tf.lite.Interpreter(model_path=“latest_model.tflite”)
解释器.allocate_tensors()

class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
    图像 = cv2.resize(图像, (150, 150))
    图像 = image.astype(&#39;float32&#39;) / 255.0
    图像 = np.expand_dims(图像, 轴=0)
    返回图像


@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    尝试：
        文件 = request.files[&#39;文件&#39;]
        image_file = Image.open(io.BytesIO(file.read()))
        图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
        如果不是（image.shape[0] &gt;= 150 且 image.shape[1] &gt;= 150 且 image.shape[2] == 3）：
        return jsonify({&quot;error&quot;: &quot;无效的图像形状&quot;})


        图像 = image.astype(&#39;float32&#39;) / 255.0

        预处理图像 = 预处理图像（图像）

        terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)
        解释器.invoke()

        output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])
        Predicted_class_index = np.argmax(output_tensor, axis=1)[0]
        预测类名称 = 类名称[预测类索引]

        结果 = {“预测”：预测类名称，“输出张量”：output_tensor.tolist()}
        返回 jsonify(结果)
    除了异常 e：
       返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

尝试记录输出张量，但这是我从 Flask API 获得的输出。知道输出张量表明偏向于 MCI 类，但如果是这种情况，为什么它在 colab 环境中完美运行，而不是在 Flask API 中运行？
此外，除了使用 Flask API 来将模型与我的 React Native 应用程序集成之外，您还建议我使用其他更好的方法吗？
&lt;前&gt;&lt;代码&gt;{

“输出张量”：[

[

0.0004518234636634588,

0.0004140451201237738,

0.002781340153887868,

0.7277416586875916,

0.2686111330986023

]

],

“预测”：“MCI-轻度认知障碍”

}
]]></description>
      <guid>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Fri, 05 Jan 2024 05:43:52 GMT</pubDate>
    </item>
    <item>
      <title>尽管成本函数收敛到 0，但什么可能导致火车精度下降 [关闭]</title>
      <link>https://stackoverflow.com/questions/77762381/what-could-be-causing-the-decline-in-train-accuracy-despite-the-cost-function-co</link>
      <description><![CDATA[我正在开展一个旨在识别绘画风格的项目。我的模型是提取绘画特征的 Inception 和用于测量绘画风格之间相似度的 Siamese 网络的组合。
但是，我在训练过程中遇到了两个问题。首先，成本函数似乎收敛于 0，其次，训练精度随着时间的推移而下降。
首先，我尝试将批量大小从 1 增加到 5。
其次，我尝试打乱训练数据集，因为模型是根据绘画风格是否匹配来交替学习的。
第三，我尝试将学习率从 0.1 降低到 0.01。
尽管做出了这些努力，问题仍然存在。
我正在寻求深入了解为什么会发生这些问题。

为了让模型能够在匹配样式和非匹配样式之间交替学习，我编写了以下代码。

对于范围 (0,80) 内的 i：
  对于范围 (0,20) 内的 j：
    对于范围 (0,2) 内的 k：
      TRAINING_image.append(图像[200*j+i+k])
      TRAINING_label.append(标签[200*j+i+k])

对于范围 (80, 100) 内的 i：
  对于范围 (0,20) 内的 j：
    对于范围 (0,2) 内的 k：
      TEST_image.append(图像[200*j+i+k])
      TEST_label.append(标签[200*j+i+k])


以下代码用于模型训练。这段代码中的模型指的是异常模型。

for i, (_image1, _label1) in enumerate(train_loader):
    优化器.zero_grad()

    image1 = _image1.to(设备)
    标签1 = _标签1[0]
    矢量1_张量 = 模型(图像1)

    if (i == 0): # 异常情况
      标签2 = 标签1
      矢量2_张量 = 矢量1_张量

    目标向量 = [标签 1 == 标签 2]
    target_tensor = torch.tensor(target_vector).float()
    目标张量 = 目标张量.to(设备)

    成本，预测=损失（向量1_张量，向量2_张量，标签1 ==标签2，阈值）
    成本.向后()
    优化器.step()

    如果预测==（标签1==标签2）：
      正确预测 += 1

    #张量重用以减少计算
    标签2 = 标签1
    矢量2_张量=矢量1_张量.detach()


以下代码是关于损失函数的。我使用Inception模型的avg pool的输出张量进行欧几里德距离测量。

类 ContrastiveLoss(nn.Module):
    def __init__(自身，边距)：
        super(ContrastiveLoss, self).__init__()
        self.margin = 保证金

    defforward（自身，输出1，输出2，标签，阈值）：
        euclidean_distance = nn.function.pairwise_distance(output1, output2, keepdim = True)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                      (标签) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        预测 = euclidean_distance.item() &lt;临界点

        返回loss_对比，预测
]]></description>
      <guid>https://stackoverflow.com/questions/77762381/what-could-be-causing-the-decline-in-train-accuracy-despite-the-cost-function-co</guid>
      <pubDate>Fri, 05 Jan 2024 03:37:00 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递值的形状为 (8631, 28)，索引意味着 (8631, 17)</title>
      <link>https://stackoverflow.com/questions/77750389/valueerror-shape-of-passed-values-is-8631-28-indices-imply-8631-17</link>
      <description><![CDATA[
第 1 步：创建管道
第2步：将管道转换为数据帧
第3步：我正在尝试将管道转换为数据帧，但出现异常。如何解决这个问题
第 4 步：如何解决 ValueError：传递值的形状为 (8631, 28)，索引意味着 (8631, 17) 在管道转换为数据帧之上，

from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
从 sklearn.impute 导入 SimpleImputer
从 sklearn.pipeline 导入管道
从 sklearn.compose 导入 ColumnTransformer

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split

print(&quot;步骤1：导入lib&quot;)
print(&quot;第2步：加载原始数据&quot;)
df = pd.read_csv(“online_shoppers_intention.csv”)

print(&quot;第三步：数据准备&quot;)
X = df.drop([&#39;收入&#39;], axis = 1)
y = df[&#39;收入&#39;]

print(&quot;第四步：数据分割&quot;)
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = .3、random_state = 0)
名称 = X_train.columns.tolist()

numeric_transformer = SimpleImputer(策略 = &#39;常量&#39;)
categorical_transformer = OneHotEncoder(handle_unknown = &#39;忽略&#39;)

numeric_cols = X.select_dtypes(exclude = &quot;object&quot;).columns.values.tolist()
categorical_cols = X.select_dtypes(exclude = [&#39;int&#39;, &#39;float64&#39;, &#39;bool&#39;]).columns.values.tolist()
    
预处理器 = ColumnTransformer(
    变形金刚=[
    (&#39;num&#39;, numeric_transformer, numeric_cols)
    ,(&#39;猫&#39;, categorical_transformer, categorical_cols)
    ],
    余数 = &#39;直通&#39;)

pipeline_preprocessor = Pipeline(steps = [(“预处理器”, 预处理器), (“pandarizer”, FunctionTransformer(lambda x: pd.DataFrame(x, columns = 名称)))]).fit(X_train)
    
X_train_pipe = pipeline_preprocessor.transform(X_train)
X_test_pipe = pipeline_preprocessor.transform(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/77750389/valueerror-shape-of-passed-values-is-8631-28-indices-imply-8631-17</guid>
      <pubDate>Wed, 03 Jan 2024 07:51:34 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：使用 URL 'http://127.0.0.1:8237' 初始化剩余存储时出错：模块 'jwt' 没有属性 'encode'</title>
      <link>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</link>
      <description><![CDATA[第一个命令python run_deployment.py --config deploy成功运行并建议我运行下一个命令 - zenml up
zenml up 生成以下错误。







我正在 YouTube 上关注 Ayush 的 MLOPs 课程。感谢您提前提供的帮助。
我试过了
&lt;前&gt;&lt;代码&gt;1。 pip 安装 jwt
2.pip安装PyJWT
3. pip卸载jwt
4. pip安装jwt==1.3.0
5. pip install --upgrade --force-reinstall PyJWT
6. pip install --upgrade --force-reinstall jwt
]]></description>
      <guid>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</guid>
      <pubDate>Fri, 29 Dec 2023 07:34:39 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习算法的 csv 流</title>
      <link>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</link>
      <description><![CDATA[我有一个很大的 CSV 文件（大约 5GB）。
我试图逐行读取整个文件，并尝试应用最典型的算法（SVM、朴素贝叶斯、线性回归等）。
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
导入 csv

i_f = open(&#39;top2Mmm.csv&#39;, &#39;r&#39; )
reader = csv.reader( i_f, 分隔符 = &#39;;&#39; )
对于读卡器中的行：
print(“斐乐 -&gt;”, 行)

我刚刚成功阅读了 CSV，但我不知道如何获取每一行并构建模型。
我从一个较小的文件开始以加快该过程，但我不知道如何使该过程正常工作。
有什么线索或提示吗？]]></description>
      <guid>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</guid>
      <pubDate>Mon, 29 May 2017 10:23:16 GMT</pubDate>
    </item>
    </channel>
</rss>