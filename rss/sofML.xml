<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 06:18:33 GMT</lastBuildDate>
    <item>
      <title>YOLOv5/SparseML - 无法在给定配方中找到任何修饰符</title>
      <link>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</link>
      <description><![CDATA[我正在尝试使用 SparseML 训练 YOLOv5s 模型。 （我不知道这是否重要，但我正在 Google Colab 中进行培训）。当我运行 train.py 时，出现以下错误：
ValueError：无法在给定配方中找到任何修饰符。修饰符必须在 yaml 键下以列表形式列出，名称中包含“修饰符”。这些键和列表也可以嵌套在用于分阶段食谱的额外键下。

这是我的recipe.yaml：
&lt;前&gt;&lt;代码&gt;---
布局：空
标题：ETS2 车辆检测数据集配方
---

修饰符：
    - !EpochRangeModifier
        开始纪元：0.0
        结束纪元：250.0

    - !SetLearningRateModifier
        开始纪元：5.0
        学习率：0.1

    - !LearningRateModifier
        开始纪元：0.0
        纪元结束：25.0
        lr_class：多步LR
        lr_kwargs：
            伽玛：0.9
            里程碑：[2.0、5.5、10.0]
        初始化lr：0.1

    - !GMPruningModifier
        开始纪元：50.0
        结束纪元：100.0
        更新频率：1.0
        初始化稀疏度：0.05
        最终稀疏度：0.65
        参数：[&#39;blocks.1.conv&#39;]
    
    - !QuantizationModifier
        开始纪元：100.0

    - !TrainableParamsModifier
        参数：[&#39;blocks.1.conv&#39;]

    - !SetWeightDecayModifier
        开始纪元：5.0
        重量衰减：0.0
    
    - !ConstantPruningModifier
        参数：[&#39;blocks.1.conv]

（另外，在 SparseML 和食谱方面，我是一个完全的初学者，所以我很乐意为我的食谱提供建议。）
我不确定我做错了什么，任何帮助/知识都会受到赞赏。感谢您的阅读，祝您早日休息。]]></description>
      <guid>https://stackoverflow.com/questions/77944843/yolov5-sparseml-unable-to-find-any-modifiers-in-given-recipe</guid>
      <pubDate>Tue, 06 Feb 2024 01:41:58 GMT</pubDate>
    </item>
    <item>
      <title>如何塑造二元分类器模型以适应我的输入数据？</title>
      <link>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</link>
      <description><![CDATA[这是我的代码。
&lt;前&gt;&lt;代码&gt;
从 pandas 导入 read_csv
从 keras.preprocessing.image 导入 ImageDataGenerator
将张量流导入为 tf
从tensorflow.keras.models导入模型，顺序
从tensorflow.keras.layers导入Flatten、Dense、Conv2D



train_df = read_csv(“输出/train.csv”)
valid_df = read_csv(“输出/validate.csv”)

train_images = ImageDataGenerator(重新缩放=1./255)
train_generator = train_images.flow_from_dataframe(train_df, x_col=“file_path”, y_col=“on_off_str”,
                                                   class_mode=&#39;二进制&#39;，batch_size=8)

validate_images = ImageDataGenerator（重新缩放=1./255）
validate_generator = validate_images.flow_from_dataframe(valid_df, x_col=“file_path”, y_col=“on_off_str”,
                                                          class_mode=&#39;二进制&#39;，batch_size=8)


模型=顺序（[压平（input_shape =（32,32,3）），
                   密集（128，激活=tf.nn.relu），
                   密集（1，激活=tf.nn.sigmoid）]）

模型.summary()

model.compile(optimizer=tf.optimizers.Adam(),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

历史= model.fit（train_generator，steps_per_epoch = 8，epochs = 15，verbose = 1，
                    验证数据=验证生成器、验证步骤=8）



我正在尝试使用 32x32x3 图像的数据集创建一个简单的二元分类器。
我收到错误：
矩阵大小不兼容：In[0]: [8,196608]，In[1]: [24576,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]

解释器挂起。
如果我将批量大小更改为 1 以使大小匹配
我收到类似的错误，但解释器不再挂起。
矩阵大小不兼容：In[0]: [1,196608]，In[1]: [3072,128]
         [[{{节点顺序/密集/Relu}}]] [操作：__inference_train_function_861]
2024-02-05 19:54:54.132353: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。
         [[{{节点 PyFunc}}]]

如何更改输入或模型的形状？我的图像是 32x32x3。]]></description>
      <guid>https://stackoverflow.com/questions/77944765/how-do-i-shape-my-binary-classifier-model-to-fit-my-input-data</guid>
      <pubDate>Tue, 06 Feb 2024 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>我尝试安装Installing TensorFlow, CUDA, cuDNN with Anaconda for GeForce GTX 1050 [关闭]</title>
      <link>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</link>
      <description><![CDATA[我尝试为 GeForce GTX 1050 安装“使用 Anaconda 安装 TensorFlow、CUDA、cuDNN”。
我使用了以下提到的文章：https://medium.com/@shaikhmuhammad/installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce-gtx-1050-ti-79c1eb94eb7a
我与文章中遵循的步骤的唯一区别是：

我有 nvidia 1050 而不是 1050ti
我在 c/users/lokes 中设置了我的环境，而不是在桌面上

我收到了这个错误，我已将其附加在屏幕截图中，但我无法找到解决该问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</guid>
      <pubDate>Tue, 06 Feb 2024 00:08:48 GMT</pubDate>
    </item>
    <item>
      <title>如何校准 opensearch 异常检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/77943830/how-to-calibrate-opensearch-anomaly-detection</link>
      <description><![CDATA[我正在尝试校准 OpenSearch 异常检测，但有几个问题。
我的大部分流程都以秒为单位记录：COUNT 1 - Sale:February 5, 2024 @ 15:50:49.000。有些时段我们没有任何记录，一般周末的流量很低。
在下面的两个配置中，我遇到了在周末开始（周六和周日）和周一开始时发出警报的问题。因为周六和周日流量下降很多，周一又增加，恢复正常流量。
配置1：

探测器间隔：10m，
窗口延迟：1m，
木瓦尺寸：8。

配置2：

探测器间隔：1m，
窗口延迟：1m，
木瓦尺寸：8。

这种类型的变体可接受的配置是什么？

]]></description>
      <guid>https://stackoverflow.com/questions/77943830/how-to-calibrate-opensearch-anomaly-detection</guid>
      <pubDate>Mon, 05 Feb 2024 20:34:27 GMT</pubDate>
    </item>
    <item>
      <title>我想制作一个职业建议模型[关闭]</title>
      <link>https://stackoverflow.com/questions/77943423/i-want-to-make-a-career-suggestion-model</link>
      <description><![CDATA[有一个数据集，其中包含职位名称和描述。当一个人输入他的技能时，我需要输出他应该做什么类别的工作。我已经使用余弦相似度创建了它。（如果你能告诉我更好的方法，那也会有帮助）现在我需要提供建议，以提高他的技能。
如果他输入统计和Python技能。模型应该可以说是学习数据科学。或者学习数据分析。
你能给我一个关于如何做到这一点的建议吗？非常感谢这种方法。
（发展法学硕士不是一种选择）
我被困在这里了。非常感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/77943423/i-want-to-make-a-career-suggestion-model</guid>
      <pubDate>Mon, 05 Feb 2024 19:11:21 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 Tortoise TTS 模型将英语视频配音为 URDU 语言 [关闭]</title>
      <link>https://stackoverflow.com/questions/77942946/how-to-train-tortoise-tts-model-for-dubing-english-video-to-urdu-language</link>
      <description><![CDATA[我正在做AI配音平台。我想将英语语音视频配音为乌尔都语语音。我已经实现了以下许多模块

从视频中提取音频（完成）
从音频中提取文本（完成）
将英语文本转换为乌尔都语文本（完成）

现在，我想使用这个URDU文本（从输入中提取和转换，如上所述[1,2,3]）和参考英语音频（从TTS 模型中的视频广告在 [1]) 中进行了讨论。
我想要的是 --&gt;该 TTS 模型从参考音频中提取特征和韵律样本，并说出具有完美语音克隆的乌尔都语文本。]]></description>
      <guid>https://stackoverflow.com/questions/77942946/how-to-train-tortoise-tts-model-for-dubing-english-video-to-urdu-language</guid>
      <pubDate>Mon, 05 Feb 2024 17:38:45 GMT</pubDate>
    </item>
    <item>
      <title>如果在多类分类中删除相关嵌入，f1 分数会降低而 AUC 会增加吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</link>
      <description><![CDATA[我在 2 个场景中使用手套嵌入构建了一个神经网络模型。
场景一：
我使用手套嵌入在具有 3725 个唯一标记的 3 个类别的平衡数据集上构建了一个神经网络模型。我正在执行多类分类。
我得到的训练和测试样本 F1 分数分别为 93.2% 和 74.78%。
AUC得分分别为0.815和0.782。
我有 3725 个令牌，其中 25 个令牌在嵌入之间具有相关性 &gt; 0.95。
在场景 B 中，我删除了这些高度相关的标记。
场景 - B：
我再次使用手套嵌入在 3 个类的平衡数据集上构建了一个神经网络模型，这一次只有 3705 个唯一标记。
我得到的训练和测试样本 F1 分数分别为 74.2% 和 60.8%。
AUC得分分别为0.905和0.794。
即从场景 A 到场景 B，我的 f1 分数正在下降，但 AUC 正在增加。
问题：
当减小嵌入矩阵的大小时，文本分类模型中的 F1 分数是否会降低，但 AUC 会增加？
可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</guid>
      <pubDate>Mon, 05 Feb 2024 12:32:57 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以从 PyCaret 中的现有模型进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[摘要
我试图使用 Python 的 PyCaret 库开发一个 ML 模型来分析具有 34 个特征（列）的数据集。我运行 setup() 函数并注意到，由于编码，它将原始数据集扩展至 58 个特征。它还进行了插补和转换。经过这些步骤后，该库将基础数据分为训练集和测试集。
最终根据训练数据集从compare_models()中选择合适的模型。由此，我在测试集上使用 Predict_model() 进行了预测，并对结果感到满意。
我的提供商现在如何向我提供新数据，我想针对新数据运行 Predict_model() ，以便我可以将这些预测返回给我的提供商。为此，我使用了 Predict_model() 函数的“data”参数，但是，我得到的错误低于以下最小可行代码示例。
这个问题旨在了解如何确保我收到的新数据经过适当的预处理，以供在 Predict_model() 的“数据”参数中使用，或者，如果我的整个概念不准确，那么适当的方法应该是什么来满足最终的要求目标是根据开发的原始模型对我收到的新数据进行预测。
带注释的最小可行代码示例
&lt;前&gt;&lt;代码&gt;# 库
导入 pycaret
从 pycaret.regression 导入 *
将 pandas 导入为 pd

# 通过从 CSV 导入创建一个新的 DF
# 这个 DF 有 34 列
baseDf = pd.read_csv(“wave1data.csv”)

# 在 baseDf 上进行设置
# 转换/编码的 DF 结果有 58 列，并在训练/测试数据集之间进行分割
s = setup(baseDf, target = “我的目标功能”, session_id = 64)

# 基本模型对比
最好=比较模型（）

# 预测测试分割
wave1_pred = 预测模型（最佳）

# 此时，我有一个理想的模型，但它基于具有 58 列和多个转换/插补的 DF
# 我尝试使用以下方法对全新的未见过的数据进行预测：
wave2Df = pd.read_csv(“wave2data.csv”)
wave2_pred = Predict_model（最佳，数据= wave2Df）

抛出错误
&lt;块引用&gt;
------------------------------------------------------------ ---------------------------- KeyError Traceback（最近调用
最后）在&lt;细胞系：2&gt;（）
1 # 预测第 2 波
----&gt; 2wave2_pred=predict_model(最佳，数据=wave2Df)
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中
_raise_if_missing(self, key, 索引器, axis_name) 6131 6132 not_found =
列表(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-&gt;第6133章 6134 第6135章
KeyError：“[&#39;Endorsed By&#39;] 不在索引中”

（如果我理解正确的话，wave2Df 数据与“最佳”数据的结构不同，这是有道理的。）
汇总查询

如何克服这些错误并使用我根据新传入数据（在本例中为 wave2Df 的内容）生成的“最佳”模型来运行 Predict_model()？
我是否应该采取完全不同的方式来实现相同的目标？
]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>从多个 csv 文件构建时间序列数据集</title>
      <link>https://stackoverflow.com/questions/77934056/build-a-time-series-dataset-from-multiple-csv-files</link>
      <description><![CDATA[这里是新手👋
我有超过 6K 个不同的 CSV 文件，其中包含 1 分钟时间范围内的股票数据。 csv 文件包含 [timestamp, open, close, high, low] 列。数据范围为9:30-16:00，每分钟有一个数据。
我正在尝试解决两个问题。 1- 将所有这些数据输入模型进行训练 2- 在输入模型之前动态构建时间序列。由于大小，不可能将所有内容都保存在内存中。此外，由于文件大小的原因，以时间序列格式保存文件也是不可能的。
对于每个训练样本，我想构建按天分组的开盘价、收盘价、最低价、最高价的 t-20。这意味着 9:30 不会有 t-20 个步骤，因此在这种情况下，除了 t=0 之外，将用零填充。 9:31 将有 t=0 和 -1，其余为零。 9:50； t0 = 9:50，t-1 = 9:49，t-2 = 9:48... t-19 =9:31，t-20 = 9:30。等等。这将给出形状张量（M [训练示例]、20 [20 个时间步长]、4 [开盘价、收盘价、最低价、最高价或每个步骤]）。
为了解决问题1，我正在考虑使用tf.data.experimental.make_csv_dataset函数。然而，为了解决问题 2，我必须编写一个函数来映射行并构建时间步长。
您将如何实现这一目标？如果这是实现这一目标的最佳且最有效的方法，您将如何构建映射函数？还有什么其他方法可以执行此操作？]]></description>
      <guid>https://stackoverflow.com/questions/77934056/build-a-time-series-dataset-from-multiple-csv-files</guid>
      <pubDate>Sat, 03 Feb 2024 23:03:35 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入需要的模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入引擎

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行这个......已经遵循库安装并看到了
https://anomalib.readthedocs.io/en/latest/markdown/ get_started/anomalib.html
我认为要么是因为引擎已被修改，要么是被库删除了......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>配置 Kaggle 以在两个 T4 GPU 之间进行分布式训练和内存共享</title>
      <link>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</link>
      <description><![CDATA[我正在尝试在 Kaggle 上使用以下命令来训练 Dreambooth：
！加速启动 --num_cpu_threads_per_process=2 “./sdxl_train.py” \
  --pretrained_model_name_or_path=“stabilityai/stable-diffusion-xl-base-1.0” \
  --train_data_dir=“数据集/img” \
  --reg_data_dir=“数据集/reg” \
  --output_dir=“输出” \
  --output_name=“SDXLDreambooth” \
  --save_model_as=“安全张量” \
  --train_batch_size=1 \
  --max_train_steps=8000 \
  --save_every_n_steps=4001 \
  --optimizer_type=“adafactor” \
  --optimizer_argsscale_parameter=Falserelative_step=Falsewarmup_init=False\
  --xformers \
  --lr_scheduler=“constant_with_warmup” \
  --lr_warmup_steps=100 \
  --learning_rate=2.5e-6 \
  --max_grad_norm=0.0 \
  --分辨率=“1024,1024” \
  --save_ precision =“fp16” \
  --save_n_epoch_ratio=1 \
  --max_data_loader_n_workers=1 \
  --persistent_data_loader_workers \
  --mixed_ precision =“fp16” \
  --full_fp16 \
  --logging_dir=&quot;日志&quot; \
  --log_prefix=“最后” \
  --gradient_checkpointing \
  --caption_extension=“.txt” \
  --no_half_vae \
  --缓存潜伏

并添加 --train_text_encoder 会在具有 16 GB VRAM 的 P100 GPU 上出现内存不足错误，即使启用了所有优化。我已经测试过在 Modal 上具有 24 GB VRAM 的 L4 GPU 上运行相同的命令，并且它成功运行最大 VRAM 利用率约为 18 GB。
但是，我注意到 Kaggle 还提供了使用两个 T4 GPU（每个 GPU 具有 16 GB VRAM）的选项，这让我想知道是否可以更改那里的环境配置（包括例如更改脚本， DeepSpeed 配置、加速配置等）以允许在 2 个 GPU 之间共享内存，其总组合内存 (32 GB) 应允许命令运行（需要 18 GB 内存）。
Kaggle 设置的默认行为似乎是在 2 个 GPU 之间复制配置并并行处理训练，因此使用 --train_text_encoder 选项，脚本将需要每个 GPU 18 GB ，导致内存不足错误。
我应该如何配置环境，以便允许两个 GPU 之间共享内存，并避免收到内存不足错误？
&lt;小时/&gt;
编辑：以下是示例笔记本和一些运行的一些链接：

笔记本；
OOM 使用两个 15 GB T4 GPU 运行；
使用 1 个 16 GB P100 GPU 成功运行.

这些都没有启用 --train-text-encoder，因为它会导致 OOM 错误。以下是第一次运行时 T4 GPU 的内存利用率：
]]></description>
      <guid>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</guid>
      <pubDate>Wed, 24 Jan 2024 19:22:44 GMT</pubDate>
    </item>
    <item>
      <title>我不明白为什么我的 k 均值算法不能用于异常检测</title>
      <link>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</link>
      <description><![CDATA[我已经实现了一种聚类算法来检测田纳西州伊士曼过程数据集上的异常，但结果很奇怪，因为它没有标记任何内容。请问我做错了什么？
附上结果混淆矩阵
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.cluster 导入 KMeans
从 sklearn.metrics 导入 precision_score
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从sklearn.metrics导入confusion_matrix

def fit_preprocess(data_path):
    # 从文件中加载数据
    数据 = pd.read_csv(data_path)

    # 为特定列定义延迟映射
    延迟列映射 = {
        “XMEAS(23)”: 6、“XMEAS(24)”: 6、“XMEAS(25)”: 6、“XMEAS(26)”: 6、“XMEAS(27)”: 6、
        “XMEAS(28)”: 6、“XMEAS(29)”: 6、“XMEAS(30)”: 6、“XMEAS(31)”: 6、“XMEAS(32)”: 6、
        “XMEAS（33）”：6，“XMEAS（34）”：6，“XMEAS（35）”：6，“XMEAS（36）”：6，“XMEAS（37）”：15，
        “XMEAS（38）”：15，“XMEAS（39）”：15，“XMEAS（40）”：15，“XMEAS（41）”：15
    }

    # 根据映射调整时间延迟
    Sample_time = 3 # 采样时间（以分钟为单位）
    对于列，delay_columns_mappings.items() 中的延迟：
        shift_steps = 延迟 // 采样时间
        数据[列] = 数据[列].shift(-shift_steps)

    # 填充因移位而出现的 NaN 值
    数据.ffill(inplace=True)

    # 标准化特征，不包括标签列
    feature_columns = [data.columns 中的 col 的 col，如果 col != &#39;label&#39;]
    定标器=标准定标器()
    数据[特征列] = scaler.fit_transform(数据[特征列])

    # 存储并返回预处理参数（每个特征的平均值和标准差）
    预处理参数 = {
        &#39;平均值&#39;：scaler.mean_，
        &#39;std&#39;：scaler.scale_
    }

    返回预处理参数

def load_and_preprocess(data_path, preprocess_params):
    # 加载数据集
    数据 = pd.read_csv(data_path)

    # 分离特征和标签
    feature_columns = [data.columns 中的 col 的 col，如果 col != &#39;label&#39;]
    X = 数据[特征列]
    y = 数据[&#39;标签&#39;]

    定标器=标准定标器()
    scaler.mean_ = preprocess_params[&#39;mean&#39;]
    scaler.scale_ = preprocess_params[&#39;std&#39;]
    X_scaled = 缩放器.transform(X)
    返回 X_scaled, y

def fit_model(X):
    # 训练模型
    模型 = KMeans(n_clusters=2, n_init=10, random_state=0)
    模型.拟合(X)
    返回模型

def 预测（X，模型）：
    # 预测数据点的聚类
    集群 = model.predict(X)

    # 确定异常簇（假设为较小的簇）
    anomaly_cluster = np.argmin(np.bincount(簇))

    # 将异常簇中的数据点标记为异常
    异常 = 集群 == anomaly_cluster

    # 将布尔标志转换为整数（0 表示正常，1 表示异常）
    返回异常.astype(int)


这是 k 均值算法的实现，该脚本尝试标准化数据集并从训练集预测故障并与测试集进行比较。]]></description>
      <guid>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</guid>
      <pubDate>Wed, 03 Jan 2024 19:38:18 GMT</pubDate>
    </item>
    <item>
      <title>预测测试图像时出现错误 - 无法重塑大小数组</title>
      <link>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/67508346/getting-error-while-predicting-a-test-image-cannot-reshape-array-of-size</guid>
      <pubDate>Wed, 12 May 2021 17:25:10 GMT</pubDate>
    </item>
    <item>
      <title>java应用程序中的异常检测</title>
      <link>https://stackoverflow.com/questions/49857024/anomaly-detection-in-java-application</link>
      <description><![CDATA[我正在尝试将异常检测模块集成到现有的java应用程序中，以允许用户从不同的算法和预测模型中进行选择
Egads 库看起来相当乐观，但我不确定它是否符合我的目的，在如果有新数据进来，我应该存储和更新现有模型还是再次传递整个数据。另外，如果我只想预测 15 分钟的时间窗口，那么结果中仅传递 15 分钟的数据肯定不准确。
可能还有其他有用的技术，并且有人可以分享他执行类似任务的经验。不幸的是，找不到任何其他用于此目的的 java 库。]]></description>
      <guid>https://stackoverflow.com/questions/49857024/anomaly-detection-in-java-application</guid>
      <pubDate>Mon, 16 Apr 2018 12:10:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn 进行加权线性回归</title>
      <link>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</link>
      <description><![CDATA[我的数据：
状态 N Var1 Var2
阿拉巴马州 23 54 42
阿拉斯加 4 53 53
亚利桑那州 53 75 65

Var1 和 Var2 是州级别的聚合百分比值。 N 是每个状态的参与者数量。我想在 Python 2.7 中使用 sklearn 考虑 N 作为权重，在 Var1 和 Var2 之间运行线性回归。
总的思路是：
fit(X, y[, 样本权重])

假设数据使用 Pandas 加载到 df 中，并且 N 变为 df[&quot;N&quot;]，我是否只需拟合数据或者我是否需要在命令中将 N 用作 sample_weight 之前以某种方式对其进行处理？
fit(df[&quot;Var1&quot;], df[&quot;Var2&quot;],sample_weight=df[&quot;N&quot;])
]]></description>
      <guid>https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn</guid>
      <pubDate>Sat, 06 Feb 2016 02:58:20 GMT</pubDate>
    </item>
    </channel>
</rss>