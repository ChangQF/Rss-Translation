<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 15 Jul 2024 12:29:42 GMT</lastBuildDate>
    <item>
      <title>如何向 CNN_M_LSTM 模型添加 3 个输入参数？</title>
      <link>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow）中。
能耗和时间戳的大小为 (70082, 2)
Covid 数据集的大小为 (744, 1)
我曾使用 tensorslice 和 zip 将数据打包在一起并对数据集进行窗口化：
这是我打包和窗口化能耗和时间戳数据集以及 covid 数据集的代码：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000

def windowed_dataset(series_energy,series_covid, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
dataset_energy = tf.data.Dataset.from_tensor_slices(series_energy) 
dataset_covid = tf.data.Dataset.from_tensor_slices(series_covid) 
dataset = tf.data.Dataset.zip(dataset_energy,dataset_covid)
dataset = dataset.window(96 + 1, shift=1) #
dataset = dataset.flat_map(lambda window_covid, window_series: tf.data.Dataset.zip((window_covid, window_series)).batch(96 + 1))
dataset = dataset.shuffle(1000)
dataset = dataset.map(lambda window_covid, window_series: (window_covid[:-1], window_series[-1][0])) 
dataset = dataset.padded_batch(128,drop_remainder=True).cache()

返回数据集

对于模型 CNN_M_LSTM，我创建了 2 个输入。这是我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(96, 1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(96, 2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2[:, -1, :])

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate()([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2],outputs=output)
# 定义最终模型
return model_final


我如何拟合我的模型：
model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

错误是模型需要 2 个输入，但收到 1 个输入张量。我曾尝试将 covid 数据集、能量列和时间戳压缩到一个数据集。
我的期望是将 3 个输入输入到我的 CNN_M_LSTM 模型中。我还尝试单独输入数据，我收到数据形状错误，我尝试重新塑造它，但它也不起作用。有没有办法在我的 CNN_M_LSTM 模型中输入 3 个参数？]]></description>
      <guid>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</guid>
      <pubDate>Mon, 15 Jul 2024 11:49:20 GMT</pubDate>
    </item>
    <item>
      <title>线性回归准确度 [关闭]</title>
      <link>https://stackoverflow.com/questions/78749472/linear-regression-accuracy</link>
      <description><![CDATA[我正在尝试仅使用 NumPy 创建线性回归算法。我使用住房数据集编写了此代码。我的成本函数在大约 500 次迭代后收敛，但预测远非准确。问题可能出在哪里？我询问了 ChatGPT，但没有得到有用的答案。我认为该算法很好（请原谅任何糟糕的编码，因为我是新手😬），我怀疑问题可能与过度拟合或类似问题有关。
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def z_score_normalization(x):
mean = np.mean(x)
std = np.std(x)

x = (x - mean)/std
return x,mean,std

def reverse_z_score_normalization(x,mean,std):
return x*std+mean

def compute_prediction(x_i,w,b):
fx = np.dot(x_i,w) + b
return fx

def cost_function(x,y,w,b,m,n,rp):#m：数据数量，n：特征数量，rp：正则化参数（lambda）
total_cost = 0
for i in range(m):
total_cost += (compute_prediction(x[i],w,b) - y[i])**2
total_cost /= 2*m

#返回 total_cost #不带正则化

#正则化
temp_cost = 0
for j in range(n):
temp_cost += (w[j])**2
total_cost += temp_cost*rp/(2*m)

cost_history.append(total_cost)

返回 total_cost

def deriveds_for_gradient_descent(x,y,w,b,m,rp,j):#m：数据数量，n：特征数量，rp：正则化参数（lambda）
dj_dw = 0
dj_db = 0

for i in range(m):
dj_dw += (compute_prediction(x[i],w,b) - y[i])*x[i,j]
dj_db += (compute_prediction(x[i],w,b) - y[i])

dj_dw /= m
dj_db /= m

dj_dw += rp*w[j]/m

return dj_dw,dj_db

def gradient_descent(x,y,w,b,m,n,rp,lr): #m: 数据数量,n: 特征数量,rp: 正则化参数(lambda),lr: 学习率(alpha)
for j in range(n):
dj_dw , dj_db = derived_for_gradient_descent(x,y,w,b,m,rp,j)
w[j] = w[j] - lr*dj_dw
b = b - lr*dj_db

返回 w,b

def raw_training_set_process(raw_x,size):#[7420 4 2 3 &#39;yes&#39; &#39;no&#39; &#39;no&#39; &#39;no&#39; &#39;yes&#39; 2 &#39;yes&#39; &#39;furnished&#39;]

scaled_raw1 = raw_x[:,4:11] #[&#39;yes&#39; &#39;no&#39; &#39;no&#39; &#39;no&#39; &#39;yes&#39; 2 &#39;yes&#39; ] =&gt;否：0，是：1
scaled_raw2 = raw_x[:,-1] #[&#39;带家具&#39;:2, &#39;半带家具&#39;:1,&#39;无家具&#39;:0]

for i in range(size):
for j in scaled_raw1[i]:
if j == &#39;是&#39;: scaled_raw1[i] = 1
elif j == &#39;否&#39; : scaled_raw1[i] = 0 

if scaled_raw2[i] == &#39;带家具&#39; : scaled_raw2[i] = 2 
elif scaled_raw2[i] == &#39;半带家具&#39; : scaled_raw2[i] = 1
else: scaled_raw2[i]=0

raw_x[:,4:11] = scaled_raw1
raw_x[:,-1] = scaled_raw2

返回raw_x

data_set = pd.read_csv(r&#39;path-to-dataset\Housing.csv&#39;)
data_set_in_numpy = data_set.to_numpy() # shape = (545,13)
#print(dataSetInNumpy[0])

y_trainingSet = data_set_in_numpy[:,0]#price
y_trainingSet,y_mean,y_std = z_score_normalization(y_trainingSet)
#print(y_trainingSet[0])
size_of_dataset= len(y_trainingSet) # sizeOfTrainingSet

X_trainingSet = raw_training_set_process(data_set_in_numpy[:,1:],size_of_dataset)#features
X_trainingSet,X_mean,X_std= z_score_normalization(X_trainingSet)
#print(X_trainingSet[0])
number_of_features = len(X_trainingSet[0])

w = np.zeros(number_of_features) #权重
b = 0 #偏差

lr = 0.01 #学习率
rp = 0.01 #正则化参数

cost_history = []

number_of_iterate = 100
for i in range(number_of_iterate):
w,b = gradient_descent(X_trainingSet,y_trainingSet,w,b,size_of_dataset,number_of_features,rp,lr)

cost_history.append(cost_function(X_trainingSet,y_trainingSet,w,b,size_of_dataset,number_of_features,lr))

if (i % 10 == 0):
print(f&quot;迭代次数：{i} w：{np.round(w,2)} b：{b：.2f} &quot;)

numbers = np.arange(len(cost_history))
plt.scatter(numbers,cost_history, marker=&#39;x&#39;, c=&#39;r&#39;)
plt.xlabel(&#39;迭代次数&#39;)
plt.ylabel(&#39;成本&#39;)
plt.show()

prediction = compute_prediction(X_trainingSet[9],w,b)
target = y_trainingSet[9]

print(f&quot;预测：{预测} 目标：{目标}&quot;)

prediction = reverse_z_score_normalization(prediction,X_mean,X_std)
target = reverse_z_score_normalization(target,y_mean,y_std)

print(f&quot;预测：{预测} 目标：{目标}&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/78749472/linear-regression-accuracy</guid>
      <pubDate>Mon, 15 Jul 2024 11:00:58 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证和 MICE 归因 [关闭]</title>
      <link>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</link>
      <description><![CDATA[我正在研究一个二元分类问题，其中有一些缺失数据。我最初的想法是使用 MiceForest。我还使用了分层 k 折技术（数据不平衡）。我还想尽量减少数据泄漏。

我应该何时使用 MiceForest 填补缺失值？针对每个折？还是一开始就填补整个数据集？
我应该使用 SMOTE 来解决每个折中的类别不平衡问题吗？因为我得到了很多误报（当仅使用分层 k 折而没有过度采样时）。

当我对整个数据集进行 mice 填补，用 smote 解决类别不平衡问题，然后进行交叉验证时，我获得了非常好的性能。我觉得这是过度拟合？这是因为数据泄漏吗？（我对这个领域有点陌生）]]></description>
      <guid>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</guid>
      <pubDate>Mon, 15 Jul 2024 06:37:02 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace：Llama-3-8B 合作检查点碎片加载进度在 25% 处停止</title>
      <link>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</link>
      <description><![CDATA[我尝试使用 huggingface 在我的 Colab 笔记本中本地运行 Llama-3-8B 模型。加载模型时，检查点分片在 25% 处停止加载。我不明白问题可能是什么。
from transformers import AutoModelForCausalLM, AutoTokenizer

# 定义模型名称（这是一个占位符，请替换为实际模型名称）
model_name = &quot;meta-llama/Meta-Llama-3-8B&quot;

!huggingface-cli login --token $HF_TOKEN
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 如果模型很大，将其移动到 GPU 可能会有所帮助
model.to(&#39;cuda&#39;)

HF_Token 已定义，出于隐私原因，此处未提及。
提示以下错误：
您的 token 已保存到 /root/.cache/huggingface/token
登录成功
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: 
您的 Colab secrets 中不存在 secret `HF_TOKEN`。
要使用 Hugging Face Hub 进行身份验证，请在设置选项卡 (https://huggingface.co/settings/tokens) 中创建一个令牌，将其设置为 Google Colab 中的机密，然后重新启动会话。
您将能够在所有笔记本中重复使用此机密。
请注意，建议进行身份验证，但仍然可以选择访问公共模型或数据集。
warnings.warn(
词汇表中已添加特殊令牌，请确保对相关的词嵌入进行了微调或训练。
正在加载 检查点 分片：  25%
 1/4 [00:22&lt;01:07, 22.37s/it]
]]></description>
      <guid>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</guid>
      <pubDate>Mon, 15 Jul 2024 05:44:13 GMT</pubDate>
    </item>
    <item>
      <title>如何对视频中对象执行的具体动作进行分类。（不是仅使用一帧，而是使用一组帧）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</link>
      <description><![CDATA[我想创建一个人工智能模型，通过查看帧集合来确定对象行为的结果，而不是通过在家打高尔夫球时查看单个帧来确定高尔夫球是进入还是离开。
我尝试使用 ultralytics，但 ultralytics 按帧对对象进行分类，因此它不符合我的目的。我想知道如何创建一个区分视频中行为分类的模型。
如何制作一个分析帧而不是帧的人工智能模型。]]></description>
      <guid>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</guid>
      <pubDate>Sun, 14 Jul 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：目标 32 超出范围。运行时损失 = 标准（y_pred，y_train）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</link>
      <description><![CDATA[我正在运行一个简单的神经网络，其中包含一些大约 1112 行、23 个输入、2 个隐藏层和 31 个可能输出的 csv 数据。在前向训练之后，在以下代码执行过程中，我收到以下错误消息
在行 loss = criterion(y_pred, y_train)
错误：
-----------------------------------------------------------------------------
IndexError Traceback（最近一次调用最后一次）
&lt;ipython-input-64-47488b841fa2&gt; 在 &lt;cell line: 5&gt;()
8 
9 
---&gt; 10 loss = criterion(y_pred, y_train)
11 
12 #loss.append(loss.detach().numpy())

3 帧
/usr/local/lib/python3.10/dist-packages/torch/nn/ functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
3084 如果 size_average 不为 None 或 reduce 不为 None:
3085 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3086 返回 torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
3087 
3088 

IndexError：目标 32 超出范围。

有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</guid>
      <pubDate>Sun, 14 Jul 2024 19:10:09 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow/Keras `load_img` 错误：“无法导入 PIL.Image”</title>
      <link>https://stackoverflow.com/questions/78746872/tensorflow-keras-load-img-error-could-not-import-pil-image</link>
      <description><![CDATA[尽管下载了枕头模块，我仍然遇到导入错误，是的，尽管我遇到了错误，但我还是卸载了枕头。有什么办法可以修复它吗？https://i.sstatic.net/eAhcTExv.png
我希望包含所有图像的文件夹被导入，这样我就可以通过导入的图像训练数据]]></description>
      <guid>https://stackoverflow.com/questions/78746872/tensorflow-keras-load-img-error-could-not-import-pil-image</guid>
      <pubDate>Sun, 14 Jul 2024 16:20:23 GMT</pubDate>
    </item>
    <item>
      <title>无法将 (Dimension(None)、Dimension(80)) 的元素转换为张量</title>
      <link>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</link>
      <description><![CDATA[我正在尝试阅读 LibRecommender 中有关模型训练过程的教程：https://librecommender.readthedocs.io/en/latest/tutorial.html
我停在了训练模型阶段，代码如下：
model = WideDeep(
task=&quot;ranking&quot;,
data_info=data_info,
embed_size=16,
n_epochs=2,
loss_type=&quot;cross_entropy&quot;,
lr={&quot;wide&quot;: 0.05, &quot;deep&quot;: 7e-4},
batch_size=2048,
use_bn=True,
hidden_​​units=(128, 64, 32),
)

model.fit(
train_data,
neg_sampling=True, # 对训练和评估数据执行负抽样
verbose=2,
shuffle=True,
eval_data=eval_data,
metrics=[&quot;loss&quot;, &quot;roc_auc&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;ndcg&quot;],
)

我收到错误：
TypeError：调用 Flatten.call() 时遇到异常。

无法将 (Dimension(None)、Dimension(80)) 的元素转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF 数据类型。

Flatten.call() 接收的参数：
• 输入=tf.Tensor(shape=(?, 5, 16), dtype=float32)

我不知道为什么会收到此错误？我假设本教程中没有错误，我按照所示按 1:1 执行。
我在 PyCharm 环境中工作并使用 Jupyter 笔记本。]]></description>
      <guid>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</guid>
      <pubDate>Sun, 14 Jul 2024 14:37:06 GMT</pubDate>
    </item>
    <item>
      <title>多线程 TFRecord 写入在 kaggle 笔记本上突然停止[关闭]</title>
      <link>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</link>
      <description><![CDATA[我正在寻求有关在 Kaggle 笔记本中 TFRecord 写入的多线程方面的帮助。我正在研究 VGGFace2 数据集，旨在将图像对转换为 TFRecords，用于训练、验证和测试集，但该过程在单线程上运行速度过慢。
TFRecord 文件包含此配对图像的 protobuf 示例，以表示同一个人和不同的人。
挑战和我尝试过的方法：

单线程处理速度慢：即使是处理数据集的有限子集（每个目录 5 个图像对用于训练，每个目录 2 个图像对用于验证/测试），使用单线程也需要大量时间（可能长达 24 小时）。我已尽可能优化代码，因此我开始探索多线程以提高性能。

多线程问题：当我使用 concurrent.futures.ThreadPoolExecutor 实现多线程时，会话突然停止，没有任何错误消息。此外，所有变量都丢失，需要从头开始完全重新启动。有趣的是，使用单个目录时，多线程可以完美运行（因为我实现的多线程是同时处理不同的目录，所以即使使用多线程池执行器对象，处理单个目录也不再是多线程，而是单线程进程），但即使使用两个目录也会导致与上述相同的问题（VGGFace2 大约有 8631 个目录）。


我的问题：

潜在原因：这些多线程问题背后的原因可能是什么？这是 Kaggle 资源的内存限制吗？

替代方法：其他人是否遇到过类似的挑战？在 Kaggle 环境中，是否有其他方法可以加速 TFRecord 写入？


附加说明：

我正在使用 contextlib.ExitStack 库来打开许多写入器并同时写入。

我在 Kaggle 讨论和 Stack Overflow 上广泛搜索解决方案，但没有找到针对这种情况的具体解决方案。


您对 Kaggle 笔记本中的多线程有什么见解或经验，特别是在处理像 VGGFace2 这样的数据集时？]]></description>
      <guid>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</guid>
      <pubDate>Sun, 14 Jul 2024 11:10:50 GMT</pubDate>
    </item>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我以为你可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>二元分类中的 SHAP 值解释</title>
      <link>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</link>
      <description><![CDATA[我尝试使用每个特征的 SHAP 值来解释我的二元分类模型。我想知道：
正的 SHAP 值是否意味着该特征对预测“1”类的贡献更大，而负的 SHAP 值是否意味着该特征对预测“0”类的贡献更大？
如果我使用绝对 SHAP 值差异来描述特征贡献变化，这个想法是否合理？]]></description>
      <guid>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</guid>
      <pubDate>Fri, 12 Jul 2024 14:25:43 GMT</pubDate>
    </item>
    <item>
      <title>python 中某些函数的贬值：数据框的真值不明确</title>
      <link>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</guid>
      <pubDate>Thu, 11 Jul 2024 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>如何准确计算Doctr ocr中检测到的文本的绝对bbox坐标？</title>
      <link>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</link>
      <description><![CDATA[我一直试图在文档的图片上绘制 bbox，并尝试使用 mindee-doctr 进行 ocr 以查看检测到的文本行。我面临的问题是，我通过乘以相对坐标和页面尺寸计算出的 bbox 的绝对坐标，在原始图像上绘制时都向右上角偏移。有没有办法纠正这个问题？
这是我计算 bbox 的代码：
from doctr.models import ocr_predictor
from doctr.io import DocumentFile

# 使用 docTR 分析图像并获取结果
line_boundaries = []
model = ocr_predictor(pretrained=True) #设置preserve_aspect_ratio=False 或symmetric_pad=False 没有区别。
doc = DocumentFile.from_images(img_path)
result = model(doc)

# 提取每行的边界框坐标
for page in result.pages:
for block in page.blocks:
for line in block.lines:
# 将相对坐标与页面尺寸相乘，得到绝对坐标
x_min, y_min, x_max, y_max = round(line.geometry[0][0] * page.dimensions[0]), round(line.geometry[0][1] * page.dimensions[1]), round(line.geometry[1][0] * page.dimensions[0]), round(line.geometry[1][1] * page.dimensions[1])
line_boundaries.append((x_min, y_min, x_max, y_max))

这是 line_boundaries 的值：
[(531, 148, 1321, 184), (2725, 148, 3061, 177), (526, 254, 3071, 295), (526, 288, 3071, 332), (535, 324, 3071, 363), ... ]
这是我用来绘制方框的函数：
import cv2
from google.colab.patches import cv2_imshow # 代替 cv2.imshow 使用，因为它会导致 collab 崩溃

def draw_rectangles(image_path, line_boundaries):
&quot;&quot;&quot;
使用提供的线边界在图像上绘制矩形。

参数：
image_path：图像文件的路径。
line_boundaries：线边界列表，其中每个边界都是四个点的列表。

返回：
无
&quot;&quot;&quot;

# 加载图像
image = cv2.imread(img_path)

# 遍历线边界并绘制矩形
for bounding in line_boundaries:
#x1, y1, x2, y2 = int(boundary[0][0]), int(boundary[0][1]), int(boundary[2][0]), int(boundary[2][1])
x1, y1, x2, y2 = map(int, bounding) # 将坐标转换为整数
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# 用矩形显示图像
cv2_imshow(image) # 仅使用 cv2_imshow 代替 cv2.imshow 进行协作
cv2.waitKey(0)
cv2.destroyAllWindows()

这是带有在其上绘制的 bboxes。

我尝试过不使用舍入，但没有任何区别，也尝试过只使用预测器，但无济于事。在 ocr_predictor 中设置preserve_aspect_ratio=False 或symmetric_pad=False 也没有区别。]]></description>
      <guid>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</guid>
      <pubDate>Thu, 11 Jul 2024 05:38:22 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 聚类：预测（X）与 fit_predict（X）</title>
      <link>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</link>
      <description><![CDATA[在 scikit-learn 中，一些聚类算法同时具有 predict(X) 和 fit_predict(X) 方法，例如 KMeans 和 MeanShift，而其他算法仅具有后者，例如 SpectralClustering。根据文档：
fit_predict(X[, y]): 对 X 执行聚类并返回聚类标签。
predict(X): 预测 X 中每个样本所属的最接近聚类。

我不太明白这两者之间的区别，在我看来它们似乎是等价的。]]></description>
      <guid>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</guid>
      <pubDate>Mon, 09 May 2016 02:25:29 GMT</pubDate>
    </item>
    </channel>
</rss>