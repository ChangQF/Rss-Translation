<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 20 Jun 2024 03:16:35 GMT</lastBuildDate>
    <item>
      <title>使用 DARTS 对多个时间序列进行全局模型训练导致 NaN 损失</title>
      <link>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</link>
      <description><![CDATA[我尝试使用 DARTS 库在多个时间序列上训练全局模型，但我遇到了训练和验证损失的 NaN 值。在单个时间序列上进行训练时，它工作正常。我总共有 6 个时间序列。
我怀疑一个可能的问题是每个时间序列的开始和结束时间戳都不同。这会导致多个时间戳，其中只有时间序列子集的数据可用。例如，在某些时间戳，6 个时间序列中可能只有 2 个有数据可用。
这是我的代码：
from darts import TimeSeries, Scaler
from darts.models import NBEATSModel

train_series = []
val_series = []
scalers = []

for idx in train_df[&#39;region_city_item_encoded&#39;].unique():
sub_train_df = train_df[train_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)
sub_val_df = val_df[val_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)

cur_time_train_series = TimeSeries.from_dataframe(sub_train_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
train_static_covariates = sub_train_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_time_train_series = cur_time_train_series.with_static_covariates(train_static_covariates)

cur_val_series = TimeSeries.from_dataframe(sub_val_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
val_static_covariates = sub_val_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_val_series = cur_val_series.with_static_covariates(val_static_covariates)

scaler = Scaler()
cur_time_train_series = scaler.fit_transform(cur_time_train_series)
cur_val_series = scaler.transform(cur_val_series)

train_series.append(cur_time_train_series)
val_series.append(cur_val_series)
scalers.append(scaler)

model = NBEATSModel(
input_chunk_length=24,
output_chunk_length=12,
n_epochs=100,
random_state=0
)
model.fit(series=train_series, val_series=val_series)

我尝试了以下方法：

确保使用以下方法填充缺失的日期fill_missing_dates=True。
对每个时间序列分别应用缩放。

训练了 DART 上可用的所有模型

通过添加静态协变量和不使用静态协变量进行训练

]]></description>
      <guid>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</guid>
      <pubDate>Thu, 20 Jun 2024 01:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何评估鸢尾花数据集分类问题的不确定性？</title>
      <link>https://stackoverflow.com/questions/78645152/how-to-estimate-the-uncertainty-of-the-classification-problem-for-the-iris-datas</link>
      <description><![CDATA[对于 dropout 中神经网络的不确定性估计方法，作为深度学习中表示模型不确定性的贝叶斯近似，我想将其应用于鸢尾花数据集。计算回归任务上 T 正向传播的标准差足以作为模型预测的不确定性，但我应该如何获得鸢尾花数据集上模型预测的不确定性？
下面是我的回归任务的代码和结果：
import torch
import numpy as np
import matplotlib.pyplot as plt

# 如果可用，则将设备设置为 GPU，否则设置为 CPU
device = torch.device(&quot;cuda:0&quot;)

# 定义一个带 dropout 的简单神经网络模型
class SimpleModel(torch.nn.Module):
def __init__(self, dropout_rate, decay):
super(SimpleModel, self).__init__()
self.dropout_rate = dropout_rate
self.decay = decay
self.f = torch.nn.Sequential(
torch.nn.Linear(1, 20),
torch.nn.ReLU(),
torch.nn.Dropout(p=self.dropout_rate),
torch.nn.Linear(20, 20),
torch.nn.ReLU(),
torch.nn.Dropout(p=self.dropout_rate),
torch.nn.Linear(20, 1)
)

def forward(self, X): 
return self.f(X)

# 用于估计模型预测中的不确定性的函数
defunctity_estimate(x, model, num_samples, l2):
# 从模型中获取多个预测以估计不确定性
outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) 
y_mean = output.mean(axis=1)
y_variance = output.var(axis=1)
tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)
y_variance += (1. / tau)
y_std = np.sqrt(y_variance)
return y_mean, y_std

# 生成合成数据
N = 200 
min_value = -10
max_value = 10

x_obs = np.linspace(min_value, max_value, N)
noise = np.random.normal(loc = 10, scale = 80, size = N)
y_obs = x_obs**3 + noise

x_test = np.linspace(min_value - 10, max_value + 10, N)
y_test = x_test**3 + noise

# 标准化数据
x_mean, x_std = x_obs.mean(), x_obs.std()
y_mean, y_std = y_obs.mean(), y_obs.std()
x_obs = (x_obs - x_mean) / x_std
y_obs = (y_obs - y_mean) / y_std
x_test = (x_test - x_mean) / x_std
y_test = (y_test - y_mean) / y_std

# 实例化模型、损失函数和优化器
model = SimpleModel(dropout_rate=0.5, decay=1e-6).to(device)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=model.decay)

# 训练模型
for iter in range(20000):
y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))
y_true = torch.Tensor(y_obs).view(-1,1).to(device)
optimizer.zero_grad()
loss = criterion(y_pred, y_true)
loss.backward()
optimizer.step()

if iter % 2000 == 0:
print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(iter, loss.item()))

# 估计模型预测中的不确定性
iters_uncertainty = 200
lengthscale = 0.01
n_std = 2 # 要绘制的标准差数
y_mean, y_std = Certainty_estimate(torch.Tensor(x_test).view(-1,1).to(device), model, iters_uncertainty, lengthscale)

# 绘制结果
plt.figure(figsize=(12,6))
plt.plot(x_obs, y_obs，ls=“none”，marker=“o”，color=“0.1”，alpha=0.8，label=“observed”）
plt.plot（x_test，y_mean，ls=“-”，color=“b”，label=“mean”）
plt.plot（x_test，y_test，ls=&#39;--&#39;，color=&#39;r&#39;，label=&#39;true&#39;）
for i in range(n_std):
plt.fill_between（ 
x_test，
y_mean - y_std * ((i+1.)),
y_mean + y_std * ((i+1.)),
color=“b”，
alpha=0.1
)
plt.legend()
plt.grid()
plt.show()



对于鸢尾花分类问题，我认为就是计算T前向传播的softmax输出的均值和标准差，在均值中找出输出最大值的索引作为预测，在标准差中找出该索引对应的标准差作为不确定性，不知道对不对。]]></description>
      <guid>https://stackoverflow.com/questions/78645152/how-to-estimate-the-uncertainty-of-the-classification-problem-for-the-iris-datas</guid>
      <pubDate>Thu, 20 Jun 2024 01:24:05 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot;变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据中每个结果类别的 n 值是平衡的（即阳性/阴性 toxo）
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

检查缺失值
anyNA(data_hum_mod)
#确保所有变量的方差均不为零或接近零。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类的子采样（这里是负面结果），如何在 R 中实现这一点？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>我在创建与NLP相关的项目时遇到了问题</title>
      <link>https://stackoverflow.com/questions/78644918/i-encountered-a-problem-when-creating-a-project-related-to-nlp</link>
      <description><![CDATA[需要帮助编写和训练神经网络来设置文本样式。输入两个文本：一个样式化示例和一个信息文本。输出是与第一个文本样式化的文本，但其信息含义与第二个文本（摘要）相似，但不包括重复。神经网络必须考虑各种风格特征（表情符号、粗体、斜体、链接、删除线和下划线文本、编号/列表等）。

是否有可能实现这一点？
我尝试使用文本摘要方法制作一个神经元，但后来我陷入困境，不知道该去哪里，我尝试了 T5 模型，但没有得到任何有意义的结果
import re
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained(&#39;t5-base&#39;)
tokenizer = T5Tokenizer.from_pretrained(&#39;t5-base&#39;)

def style_transfer(content_text, style_text):
def preprocess_text(text):
text = re.sub(r&#39;\s+&#39;, &#39; &#39;, text)
return text.strip()

def postprocess_text(text):

text = re.sub(r&#39;\n\s*\n&#39;, &#39;\n&#39;, text)

text = text.strip()
返回文本

content = preprocess_text(content_text)
style = preprocess_text(style_text)

input_text = f&quot;transfer style: {style} content: {content}&quot; 
输入 = tokenizer（输入文本，
return_tensors=&#39;pt&#39;，
max_length=512，
truncation=True，
padding=&#39;max_length&#39;）

输出 = model.generate（输入[&#39;输入ids&#39;]，
注意力掩码=输入[&#39;注意力掩码&#39;]，
max_length=512，
num_beams=5，
early_stopping=True，
top_k=50，
top_p=0​​.95，
)

生成文本 = tokenizer.decode（输出[0]，skip_special_tokens=True）

生成文本 = postprocess_text（生成文本）

返回生成文本

内容文本 = ()
样式文本 = ()

结果 = style_transfer（内容文本，样式文本）
打印（结果）
]]></description>
      <guid>https://stackoverflow.com/questions/78644918/i-encountered-a-problem-when-creating-a-project-related-to-nlp</guid>
      <pubDate>Wed, 19 Jun 2024 22:51:06 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在反向传播后归零</title>
      <link>https://stackoverflow.com/questions/78644685/neural-network-zeroing-out-after-back-propogation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644685/neural-network-zeroing-out-after-back-propogation</guid>
      <pubDate>Wed, 19 Jun 2024 21:19:59 GMT</pubDate>
    </item>
    <item>
      <title>Excel 中的逻辑回归</title>
      <link>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</link>
      <description><![CDATA[我有两个优化模型：
LR-P1：
LR-P1 模型
LR-P2：
LR-P2 模型
我期望两个模型都获得相同的最优值，但我无法计算模型 LR-P1。我进行了所有计算，但 Excel 求解器无法找到最优值。当我将所有系数设为 0.1 时，求解器只会说找到了最优值，但不会更改决策变量。
我的问题是，我进行了所有计算，但 Excel 给出了 NUM 错误，而模型 LR-P2 没有。这是因为 LR-P1 的目标函数太小，以致 Excel 求解器无法对其进行交换，从而导致数值问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</guid>
      <pubDate>Wed, 19 Jun 2024 21:13:58 GMT</pubDate>
    </item>
    <item>
      <title>确定哪些变量对 FDA 贡献最大</title>
      <link>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</guid>
      <pubDate>Wed, 19 Jun 2024 20:46:16 GMT</pubDate>
    </item>
    <item>
      <title>无法重现 PyTorch 模型训练性能</title>
      <link>https://stackoverflow.com/questions/78644377/unable-to-reproduce-pytorch-model-training-performance</link>
      <description><![CDATA[我已经在自定义数据集上为图像分类任务训练了一个 RegNet 模型。那是在 2023 年 8 月。现在我想使用相同的数据集再次训练完全相同的模型。我希望这个新模型能够实现与 2023 年 8 月之前的模型大致相同的性能，因为没有任何变化：

我使用完全相同的 PyTorch 和 Torchvision 版本（1.13 和 0.14）
我使用完全相同的图像数据集进行训练/验证/测试
我使用完全相同的脚本通过 torch 训练模型
我使用与以前完全相同的训练超参数

但是，即使没有任何变化，新训练的模型的表现也明显差于去年的原始模型。 2023 年 8 月的第一个模型的测试准确率达到 0.97，而现在新模型在同一个测试数据集上仅达到 0.94。在训练期间，训练和验证准确率与以前大致相同。
我知道两个模型不会达到完全相同的性能，但 3% 的差异似乎太多了。无论我做什么，我都无法接近去年的 0.97 测试准确率，我得到的只有 0.94。即使一切都完全相同，如所述。即使是带有四个 GPU 的机器和在该机器上运行的 Ubuntu 版本也与 2023 年之前完全相同。
我知道其中涉及一个随机种子，但我怀疑这会导致如此大的 3% 的测试准确率差异。我还知道也许 Nvidia / CUDA 驱动程序已在该机器上更新，当然还有一些依赖项和软件包（例如 numpy）。但这会导致如此巨大的差异吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644377/unable-to-reproduce-pytorch-model-training-performance</guid>
      <pubDate>Wed, 19 Jun 2024 19:45:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么这些简单的线性回归权重梯度 numpy 计算会给出不同的结果？</title>
      <link>https://stackoverflow.com/questions/78644274/why-are-these-simple-linear-regression-weights-gradient-numpy-calculations-givin</link>
      <description><![CDATA[对于权重梯度计算，它们对相同参数给出了不同的结果。
# 定义训练集
X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])
y_train = np.array([460, 232, 178])
b_init = 785.1811367994083
w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])

方法 (1)
def dj_dw(w,x,b,y):
# 示例数量
m = x.shape[0]
dj_dw = (1/m)*(np.dot(np.transpose(np.dot(x,w)+b-y),x))
return dj_dw

方法 (2)
def dj_dw_2(w, x, b, y):
m, n = x.shape
dj_dw = np.zeros(n)
for j in range(n):
for i in range(m):
dj_dw[j] += (1/m) * ((w[j]*x[i][j] + b - y[i]) * (x[i][j]))
return dj_dw

以及结果分别
[-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]
[ 1.59529824e+06 1.73748484e+03 5.72854200e+02 -2.33772157e+04]
]]></description>
      <guid>https://stackoverflow.com/questions/78644274/why-are-these-simple-linear-regression-weights-gradient-numpy-calculations-givin</guid>
      <pubDate>Wed, 19 Jun 2024 19:14:12 GMT</pubDate>
    </item>
    <item>
      <title>如何提取不同图像中相同的 ROI 区域？[关闭]</title>
      <link>https://stackoverflow.com/questions/78644034/how-can-i-extract-roi-region-same-in-different-images</link>
      <description><![CDATA[我是 ROI 提取的新手。我有许多像这样的图像 ROI 区域，它们是由相机从人们的眼睛中捕捉到的。我想确定某人是否患有贫血症。
想使用此 ROI 区域通过 Pearson 相关指数进行数学运算。
我使用开放 CV 和 python。想在所有不同的图像中准确提取这个区域我的 ROI 目标。也不知道如何找到这个 ROI 区域的坐标。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78644034/how-can-i-extract-roi-region-same-in-different-images</guid>
      <pubDate>Wed, 19 Jun 2024 18:05:07 GMT</pubDate>
    </item>
    <item>
      <title>控制 Azure ML 命令源代码的上传位置</title>
      <link>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</link>
      <description><![CDATA[我正在 Azure 机器学习工作室的笔记本中工作，并使用以下代码块通过 命令函数 实例化作业。
来自 azure.ai.ml 导入命令、输入、输出
来自 azure.ai.ml.entities 导入数据
来自 azure.ai.ml.constants 导入 AssetTypes

subscription_id = &quot;&lt;subscription_id&gt;&quot;
resource_group = &quot;&lt;resource_group&gt;&quot;
working = &quot;&lt;workspace&gt;&quot;
storage_account = &quot;&lt;storage_account&gt;&quot;
输入路径 = &quot;&lt;输入路径&gt;&quot;
输出路径 = &quot;&lt;输出路径&gt;&quot;

input_dict = {
&quot;input_data_object&quot;: 输入(
type=AssetTypes.URI_FILE, 
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{input_path}&quot;
)
}

output_dict = {
&quot;output_folder_object&quot;: 输出(
type=AssetTypes.URI_FOLDER,
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{output_path}&quot;,
)
}

job = command(
code=&quot;./src&quot;, 
command=&quot;python 01_read_write_data.py -v --input_data=${{inputs.input_data_object}} --output_folder=${{outputs.output_folder_object}}&quot;,
inputs=input_dict,
outputs=output_dict,
environment=&quot;&lt;asset_env&gt;&quot;,
compute=&quot;&lt;compute_cluster&gt;&quot;,
)

returned_job = ml_client.create_or_update(job)

此操作成功运行，但每次运行时，如果存储在 ./src 目录中的代码发生变化，则会将新副本上传到默认的 blob 存储帐户。我不介意这一点，但每次运行时，代码都会上传到我的 blob 存储帐户根目录下的新容器中。因此，我的默认存储帐户会因容器而变得杂乱无章。我已阅读使用 command() 函数实例化 command 对象的文档，但我没有看到可用于控制 ./src 代码上传位置的参数。有什么方法可以控制吗？]]></description>
      <guid>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</guid>
      <pubDate>Wed, 19 Jun 2024 16:06:28 GMT</pubDate>
    </item>
    <item>
      <title>是否应将多个分类嵌入组合成条件 GAN（cGAN）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</link>
      <description><![CDATA[我正在尝试制作一个条件 GAN (cGAN)，它可以根据标题和视频类别/流派生成 YouTube 缩略图。
它根本不起作用，甚至没有接近，所以我试图回到有关我的架构的基本问题。现在，我所做的是制作两个嵌入向量，一个用于标题，一个用于类别，然后我将它们组合起来并将它们都发送到生成器和鉴别器。
以下是我的代码：
class CategoryTitleEmbeddingNet(nn.Module):
def __init__(self, num_categories:int, category_embedding_dim:int, vocab_size:int, title_embedding_dim:int, title_max_length:int):
&quot;&quot;&quot;
初始化标题/类别嵌入神经网络

参数：
num_categories：唯一类别的总数。
category_embedding_dim：类别嵌入向量的维度。
vocab_size：唯一标记的总数。
title_embedding_dim：标题嵌入向量的维度。
title_max_length：标题中允许的最大标记数。

返回：
无
&quot;&quot;&quot;
super(CategoryTitleEmbeddingNet，self).__init__()
self.category_embedding = nn.Embedding(num_categories，category_embedding_dim)
self.title_embedding = nn.Embedding(vocab_size，title_embedding_dim)

self.fully_connected1 = nn.Linear(category_embedding_dim + title_embedding_dim * title_max_length，256)
self.fully_connected2 = nn.Linear(256，128)
self.fully_connected3 = nn.Linear(128，64)
self.fully_connected4 = nn.Linear(64，1)

def forward(self，category_indices：torch.Tensor，title_indices：torch.Tensor) -&gt; torch.Tensor:

category_embedded = self.category_embedding(category_indices)
title_embedded = self.title_embedding(title_indices)
title_embedded = title_embedded.view(title_embedded.size(0), -1)
category_embedded = category_embedded.view(category_embedded.size(0), -1)

combined_embeddings = torch.cat((category_embedded, title_embedded), dim=1)
combined_output = torch.relu(self.fully_connected1(combined_embeddings))
combined_output = torch.relu(self.fully_connected2(combined_output))
combined_output = torch.relu(self.fully_connected3(combined_output))
final_output = self.fully_connected4(combined_output)
返回final_output

我只是将两个嵌入向量连接成一个。我想知道这样做可以吗？还是我应该分别传递它们？我尝试对此进行一些研究，但这是一个相当小众的问题]]></description>
      <guid>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</guid>
      <pubDate>Tue, 18 Jun 2024 21:04:12 GMT</pubDate>
    </item>
    <item>
      <title>理解 Transformers 的结果，通过梯度下降进行情境学习</title>
      <link>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</link>
      <description><![CDATA[我正在尝试实现这篇论文：
https://arxiv.org/pdf/2212.07677
（这是他们的代码）：
https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
我正在努力匹配他们的实验结果。具体来说，在他们最简单的 GD 模型（单层、单头、无 softmax）上，他们在测试数据上获得了大约 0.20 的恒定低损失。从概念上讲，我不太明白为什么会这样。
据我所知，这个模型只对数据进行了一次梯度下降迭代，那么为什么它会达到如此低的损失？为什么损失在训练步骤中会保持恒定/接近恒定？我们不是在 GD 模型中训练学习率吗？]]></description>
      <guid>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</guid>
      <pubDate>Tue, 18 Jun 2024 20:43:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>sklearn ImportError：无法导入名称 plot_roc_curve</title>
      <link>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</link>
      <description><![CDATA[我尝试按照 sklearn 文档中提供的 示例，绘制带有交叉验证的接收者操作特性 (ROC) 曲线。但是，以下导入在 python2 和 python3 中都给出了 ImportError。
from sklearn.metrics import plot_roc_curve

错误：
回溯（最近一次调用最后一次）：
文件“&lt;stdin&gt;”，第 1 行，在 &lt;module&gt;
ImportError：无法导入名称 plot_roc_curve

python-2.7 sklearn 版本：0.20.2.
python-3.6 sklearn 版本：0.21.3.
我发现以下导入工作正常，但它与 plot_roc_curve 不太一样。
from sklearn.metrics import roc_curve

plot_roc_curve 是否已弃用？有人可以尝试代码并告诉我 sklearn 版本是否有效吗？]]></description>
      <guid>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</guid>
      <pubDate>Thu, 20 Feb 2020 13:44:41 GMT</pubDate>
    </item>
    </channel>
</rss>