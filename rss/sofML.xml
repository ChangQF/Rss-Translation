<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 03 Feb 2024 09:14:06 GMT</lastBuildDate>
    <item>
      <title>用于机器学习的数据集，至少包含 100 列和 10,000 个原始数据</title>
      <link>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</link>
      <description><![CDATA[我想要一个至少包含 100 列和 10000 行的数据集。您不能使用任何编程语言来生成它，因为它将用于机器学习实践
提前感谢您的帮助🙏]]></description>
      <guid>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</guid>
      <pubDate>Sat, 03 Feb 2024 09:03:55 GMT</pubDate>
    </item>
    <item>
      <title>有人可以解释一下在其他层但不在第一层使用 BatchNorm2d 的目的吗</title>
      <link>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</link>
      <description><![CDATA[我正在设计Discriminator类，我在github上看到有人的实现，我无法向自己解释为什么batchnormalization被用在conv2，conv3中，但特别是在第一个卷积层中没有，我为您提供了代码类还有转换函数
类鉴别器（nn.Module）：
    def __init__(self, conv_dim = 32):
        super(鉴别器, self).__init__()

        self.conv_dim = conv_dim
        
        self.conv1 = conv(
            3、conv_dim、4、batch_norm = False
        ）
        self.conv2 = conv(
            转换亮度, 转换亮度 * 2, 4
        ）
        self.conv3 = conv(
            转换亮度 * 2, 转换亮度 * 4, 4
        ）
        self.fc = nn.Linear(
            卷积暗度 * 4 * 4 * 4, 1
        ）
    def 前向（自身，x）：
        leaky_relu = F.leaky_relu
        输出=leaky_relu（
            自转换1(x), 0.2
        ）
        输出=leaky_relu（
            self.conv2(输出), 0.2
        ）
        输出=leaky_relu（
            self.conv3(输出), 0.2
        ）
        输出 = out.view(-1, self.conv_dim * 4 * 4 * 4)
        输出 = self.fc(输出)
        返回

我尝试询问 ChatGPT，但它没有给出一致的答案]]></description>
      <guid>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</guid>
      <pubDate>Sat, 03 Feb 2024 08:35:38 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中使用神经网络进行动物检测</title>
      <link>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</link>
      <description><![CDATA[当我运行最后一部分来训练模型时，我无法检查图像是否有错误。正如你所看到的，我想检测我是否能够训练我的模型来检测动物，例如动物。猫和狗之间。 数据集。
将 pandas 导入为 pd
将 numpy 导入为 np
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D
从tensorflow.keras.layers导入MaxPooling2D
从tensorflow.keras.layers导入Flatten
从tensorflow.keras.layers导入Dense

# 初始化 CNN
分类器=顺序（）

＃卷积
classifier.add(Conv2D(32,(3,3), input_shape = (64, 64, 3), 激活 = &#39;relu&#39;))

# 池化
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 添加第二个卷积层
classifier.add(Conv2D(32, (3,3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 展平
分类器.add(Flatten())

# 全连接
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 1, 激活 = &#39;sigmoid&#39;))

# 编译 CNN
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标= [&#39;准确性&#39;]）

# 将 CNN 拟合到图像上

从 keras.preprocessing.image 导入 ImageDataGenerator
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小 = 32,
                                                类模式 = &#39;分类&#39;)

##### 这里我收到错误
分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)

错误：
]]></description>
      <guid>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</guid>
      <pubDate>Sat, 03 Feb 2024 05:55:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch CCN 勉强训练</title>
      <link>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</guid>
      <pubDate>Sat, 03 Feb 2024 05:53:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么变压器可以接受不同长度的输入？</title>
      <link>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</link>
      <description><![CDATA[我读了“你所需要的就是注意力”论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。并且使用屏蔽注意力来对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？
在我与 Chat-GPT 的第一次对话中，它告诉我我们使用 pad 令牌，而在第二次对话中，我们没有使用。我很困惑，我什至不知道该相信什么。]]></description>
      <guid>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</guid>
      <pubDate>Sat, 03 Feb 2024 04:28:27 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（大约每分钟 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量低于恒定条时，查询将运行并且可以很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
&lt;块引用&gt;
索引错误 - 列表索引超出范围

请注意，查询数量“bar”是发生此错误的时间取决于每个查询的长度，并且如果查询长度在程序执行期间保持相同，则该错误是一致的。例如，在尝试将查询长度增加大约 20% 后，查询长度从大约 330 个查询下降到大约 60 个查询。
&lt;块引用&gt;
文件
“/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，
第 1315 行，文本
返回 self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，该位置的配额应该为 300 个查询/分钟。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。
我写的Gemini Pro API调用函数是：
def gemini_response(message: str) -&gt; &gt;字符串：
    # 初始化顶点AI
    vertexai.init(project=“project-id-0123”, location=“us-central1”)

    # 加载模型
    模型 = GenerativeModel(“gemini-pro”)

    # 查询模型
    响应 = model.generate_content(消息)
    返回响应.文本
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CreateML 只检测到 JSON 文件中的一个类，而实际上我有 2 个类？</title>
      <link>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</link>
      <description><![CDATA[我正在开发一个对象检测机器学习模型，我决定使用 CreateML 来训练它。我希望我的模型能够区分苹果和橙子。
在开始训练之前，我必须提供我的“训练数据”到 CreateML，其中包括我的 JSON 文件（带有我的图像注释）。在我的 JSON 文件中，我希望模型能够检测到 2 个类（苹果和橙子）。
但由于某种原因，CreateML 仅检测类“apple”。 下面是我的 JSON 文件的一小部分的图像，其中包括“apple”的一个注释和“橙色”。
这是我的 JSON 注释的小图片，包括我的类“apple”和和“橙色”
此外，CreateML 表示我对标签“apple”的计数为 37。手动统计后，我发现这是不准确的；实际上，我对“apple”类的计数是 97。 下面是代表这一点的图像。
CreateML 向我展示了我的 JSON 文件计数。它只向我展示了“apple”类，但我也有一个“橙色”类。
最初，我的 JSON 文件中的一些注释具有不同的属性。例如，“坐标”可以是“坐标”。我的部分注释有时有整数值，有时有小数值。我意识到这可能会带来问题，所以我将它们全部更改为整数值。它最终没有成功。
此外，我还进行了一个小实验。因为我所有的“苹果”都是注释位于文件的最顶部，所有“橙色”都位于文件的最顶部。注释在底部，我翻转顺序看看结果。我把所有的“橙色”都放在了注释在顶部。令我惊讶的是，CreateML 仍然告诉我有一个类，但这次它说该类名为“orange！”为什么会出现这种情况？
有人听说过这个吗？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</guid>
      <pubDate>Sat, 03 Feb 2024 01:48:45 GMT</pubDate>
    </item>
    <item>
      <title>尝试在本地运行 falcon-40b 模型。需要帮助，模型没有给出任何输出，并且在 VS Code 上显示退出代码 1</title>
      <link>https://stackoverflow.com/questions/77930495/trying-to-run-falcon-40b-model-locally-need-help-the-model-is-not-giving-any-ou</link>
      <description><![CDATA[我有 Nvidia rtx 3060，并决定在我自己的项目中尝试一下。所以我已将所有模型文件下载到文件夹 ./Model 中，与 app.py 并行。我只是想在继续之前测试一下法学硕士。但是，当我尝试执行代码时，它不会产生任何输出，而且当我将鼠标悬停在 VScode 上执行的命令上时，它会显示 Command Executed now and failed (Exit Code 1)。
测试代码如下：
从变压器导入 AutoModelForCausalLM, AutoTokenizer
进口火炬

# 设置模型目录的路径
model_directory = &quot;./model&quot;;

# 从指定目录初始化分词器和模型
tokenizer = AutoTokenizer.from_pretrained(model_directory)
模型 = AutoModelForCausalLM.from_pretrained(model_directory)

# 确保模型正在使用 GPU
模型 = model.to(“cuda”)

# 定义提示
提示=“”“
&lt;人类&gt;：像我五岁一样解释一下LLMS
&lt;助理&gt;：
”“”

# 生成配置
生成配置={
    &quot;max_length&quot;: 200, # 调整生成token的最大长度
    “温度”: 0.7, # 温度控制随机性
    &quot;top_p&quot;: 0.7, # top_p控制核采样
    &quot;num_return_sequences&quot;: 1, # 要生成的序列数
    &quot;pad_token_id&quot;: tokenizer.eos_token_id, # 填充令牌
    &quot;eos_token_id&quot;: tokenizer.eos_token_id, # 序列结束标记
}

# 对提示进行编码
编码 = tokenizer(提示, return_tensors=“pt”).to(“cuda”)

# 使用编码的提示和生成配置生成响应
with torch.no_grad(): # 禁用梯度计算以节省内存并加快计算速度
    输出 = model.generate(
        input_ids=编码[“input_ids”],
        注意掩码=编码[“注意掩码”]，
        **生成_配置
    ）

# 将生成的 token 解码为文本
generated_text = tokenizer.decode(outputs[0],skip_special_tokens=True)

# 打印生成的文本
打印（生成的文本）

以下是 VScode 屏幕截图：
VScode 显示退出代码
我做错了什么以及如何解决这个问题？
我尝试执行上面给出的代码。我应该收到短信回复，但我什么也没收到。]]></description>
      <guid>https://stackoverflow.com/questions/77930495/trying-to-run-falcon-40b-model-locally-need-help-the-model-is-not-giving-any-ou</guid>
      <pubDate>Sat, 03 Feb 2024 00:38:50 GMT</pubDate>
    </item>
    <item>
      <title>如何修复一层的输出，使其与另一层兼容？</title>
      <link>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</link>
      <description><![CDATA[我的输入是由 21 个氨基酸标签编码的序列。它是一个包含 21 个元素 (1x21) 的数组。我想通过嵌入层然后通过卷积层（等等）将其提供给它。但它不允许我添加卷积层。我明白为什么（输出和输入的尺寸不匹配），但我不知道如何修复它。
这是我的代码：
&lt;前&gt;&lt;代码&gt;embeded_vector_size = 9
最大长度 = 21
vocab_size = len(标签)
模型=顺序（）
model.add（嵌入（vocab_size，embeded_vector_size，input_length = max_length，name =“嵌入”））
model.add(Conv2D(filters=64,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))

我收到此错误：
层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

完整回溯：
ValueError Traceback（最近一次调用最后一次）
[177] 中的单元格，第 7 行
      5 model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length, name=“embedding”))
      6 打印(模型.summary())
----&gt; 7 model.add(Conv2D(filters=3,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))
      8 # 打印(模型.summary())
      9 # 模型.add(MaxPooling2D())

文件 /opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py:253，在assert_input_compatibility(input_spec、inputs、layer_name)中
    第251章
    [252] 第252话规格.min_ndim:
--&gt;第253章
    254 f&#39;层“{layer_name}”的输入{input_index} &#39;
    255、“与层不兼容：”
    [256] 第256话
    257 f”发现 ndim={ndim}。 ”
    258 f“接收到完整形状：{tuple(shape)}”
    第259章）
    260 # 检查数据类型。
    第261章

ValueError：层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</guid>
      <pubDate>Fri, 02 Feb 2024 22:14:15 GMT</pubDate>
    </item>
    <item>
      <title>如何调整我的Python代码来安装最新的tensorflow对象检测包</title>
      <link>https://stackoverflow.com/questions/77929860/how-can-i-adapt-my-python-code-to-install-the-latest-tensorflow-object-detection</link>
      <description><![CDATA[我一直在关注这个并且已经解决了到视频的训练和检测部分。但是，我在安装各种不同的库（包括tensorflow和matplotlib）时遇到了一些问题。
主要错误始终相同：https://github 找到讨论.com/pypa/pip/issues/12330
我尝试了它引导我访问的 github 链接，但该链接没有提到我的问题的任何解决方案。我还尝试浏览不同的论坛并向更有经验的人寻求帮助，但他们也不知道。需要注意的是，只有当我在虚拟环境中时才会出现此错误。当我不在 venv 内时，我能够完美地卸载并重新安装tensorflow。
在线研究后，我发现我在代码中使用的张量流研究模型已被弃用，我应该使用类似于 这个。
我的原始代码如下所示：
# 安装 Tensorflow 对象检测
从 setuptools 导入 find_packages
从 setuptools 导入设置

如果 os.name==&#39;posix&#39;:
    !apt-get 安装 protobuf 编译器
    !cd Tensorflow/models/research &amp;&amp;协议 object_detection/protos/*.proto --python_out=. &amp;&amp; cp object_detection/packages/tf2/setup.py 。 &amp;&amp; python -m pip install .
    
如果 os.name==&#39;nt&#39;:
    url =“https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip”
    wget.download(url)
    !move protoc-3.15.6-win64.zip {paths[&#39;PROTOC_PATH&#39;]}
    !cd {paths[&#39;PROTOC_PATH&#39;]} &amp;&amp; tar -xf protoc-3.15.6-win64.zip
    os.environ[&#39;PATH&#39;] += os.pathsep + os.path.abspath(os.path.join(paths[&#39;PROTOC_PATH&#39;], &#39;bin&#39;))
    !cd Tensorflow/models/blob &amp;&amp;协议 object_detection/protos/*.proto --python_out=. &amp;&amp;复制 object_detection\\packages\\tf2\\setup.py setup.py &amp;&amp; python setup.py build &amp;&amp; python setup.py 安装



    REQUIRED_PACKAGES = [
        # 带有 PY3 的 apache-beam 需要
        &#39;avro-python3&#39;,
        &#39;阿帕奇光束&#39;,
        &#39;枕头&#39;，
        &#39;lxml&#39;,
        &#39;matplotlib&#39;,
        &#39;赛通&#39;,
        &#39;contextlib2&#39;,
        &#39;tf-苗条&#39;,
        &#39;六&#39;，
        &#39;pycocotools&#39;,
        &#39;利维斯&#39;,
        &#39;scipy&#39;,
        &#39;熊猫&#39;,
        &#39;tf-models-official&gt;=2.5.1&#39;,
        &#39;张量流_io&#39;,
        &#39;喀拉斯&#39;,
        &#39;pyparsing==2.4.7&#39;, # TODO(b/204103388)
        &#39;sacrebleu&lt;=2.2.0&#39; # https://github.com/mjpost/sacrebleu/issues/209
    ]

    设置（
        name=&#39;object_detection&#39;,
        版本=&#39;0.1&#39;,
        install_requires=REQUIRED_PACKAGES,
        include_package_data=真，
        包=（
            [p for p in find_packages() if p.startswith(&#39;object_detection&#39;)] +
            find_packages(where=os.path.join(&#39;.&#39;, &#39;slim&#39;))),
        包目录={
            &#39;数据集&#39;: os.path.join(&#39;slim&#39;, &#39;数据集&#39;),
            &#39;网&#39;: os.path.join(&#39;slim&#39;, &#39;网&#39;),
            &#39;预处理&#39;: os.path.join(&#39;slim&#39;, &#39;预处理&#39;),
            &#39;部署&#39;: os.path.join(&#39;slim&#39;, &#39;部署&#39;),
            &#39;脚本&#39;: os.path.join(&#39;slim&#39;, &#39;脚本&#39;),
        },
        description=&#39;Tensorflow 对象检测库&#39;,
        python_requires=&#39;&gt;3.6&#39;,
    ）
    
    !cd Tensorflow/models/research/slim &amp;&amp; pip install -e 。

wget 等模块和其他先决条件能够成功包含。
如何调整此代码以更好地匹配最新的张量流更新？]]></description>
      <guid>https://stackoverflow.com/questions/77929860/how-can-i-adapt-my-python-code-to-install-the-latest-tensorflow-object-detection</guid>
      <pubDate>Fri, 02 Feb 2024 21:06:44 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 Gemini-Pro 制作 LLM 聊天机器人时出现类型错误</title>
      <link>https://stackoverflow.com/questions/77929619/type-error-when-trying-to-make-a-llm-chatbot-using-gemini-pro</link>
      <description><![CDATA[这是代码
导入操作系统

将streamlit导入为st
从 dotenv 导入 load_dotenv
将 google.generativeai 导入为 gen_ai

加载_dotenv()

st.set_page_config(
    page_title=&quot;与 Gemini Pro 聊天&quot;,
    page_icon=&quot;:大脑:&quot;,
    布局=“居中”
）

GOOGLE_API_KEY = os.getenv(“GOOGLE_API_KEY”)
gen_ai.configure(api_key=GOOGLE_API_KEY)
模型 = gen_ai.GenerativeModel(“gemini-pro”)

deftranslate_role_for_streamlit(user_role):
  如果 user_role == “模型”：
    返回“助手”
  别的：
    返回用户角色

如果“聊天会话”是不在 st.session_state 中：
  st.session_state.chat_session = model.start_chat(history=[])
st.title(“CAIE 机器人”)

对于 st.session_state.chat_session.history 中的消息：
  与 st.chat_message(translate_role_for_streamlit(message.role))：
    st.markdown(message.parts[0].text)

user_prompt = st.chat_input(“询问 CAIE 机器人...”)
如果用户提示：
  st.chat_message(“用户”).markdown(user_prompt)
  gemini_response = st.session_state.chat_session.send_message(user_prompt)
  与 st.chat_message(“助理”)：
    st.markdown(gemini_response.text)

!streamlit 运行 main.py

我在线路中遇到错误
如果“chat_session”是不在 st.session_state 中：

我想检查用户是否与机器人进行了活跃的聊天，如果是，那么机器人将保存之前的对话历史记录以供下一次对话使用。会话结束后，历史记录将重置]]></description>
      <guid>https://stackoverflow.com/questions/77929619/type-error-when-trying-to-make-a-llm-chatbot-using-gemini-pro</guid>
      <pubDate>Fri, 02 Feb 2024 20:09:24 GMT</pubDate>
    </item>
    <item>
      <title>epoch 中一批的梯度爆炸和 NaN 损失</title>
      <link>https://stackoverflow.com/questions/77929468/exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</link>
      <description><![CDATA[我正在尝试训练一个机器学习模型，在训练过程中，我在该纪元中的一个批次上不断收到 NaN（无限）损失，而所有其他批次都会产生合理的损失（通过跳过参数更新）该特定批次）。
&lt;前&gt;&lt;代码&gt;3027
losstensor(2.9372, device=&#39;cuda:0&#39;, grad_fn=)
3028
losstensor(2.9796, device=&#39;cuda:0&#39;, grad_fn=)
3029
losstensor(2.9189, device=&#39;cuda:0&#39;, grad_fn=)
3030
losstensor(2.9621, device=&#39;cuda:0&#39;, grad_fn=)
3031
losstensor(3.3539, device=&#39;cuda:0&#39;, grad_fn=)
3032
losstensor(2.8540, device=&#39;cuda:0&#39;, grad_fn=)
3033
losstensor(inf, device=&#39;cuda:0&#39;, grad_fn=) &lt;---------------------------------------- ----
3034
losstensor(3.0785, device=&#39;cuda:0&#39;, grad_fn=)
3035
losstensor(2.9671, device=&#39;cuda:0&#39;, grad_fn=)
3036
losstensor(2.9451，device=&#39;cuda:0&#39;，grad_fn=)
3037
losstensor(3.0042, device=&#39;cuda:0&#39;, grad_fn=)
3038


通过调整不同的学习率，产生 NaN 损失的批次索引保持不变

通过调整批次大小，仍然总会有一个批次产生 NaN 损失，尽管现在它是包含不同数据样本的不同批次索引


一些特定于我的模型的信息：

使用 AdamW 优化器和 CTC 损失训练的变压器

我的数据集样本在分组之前会被打乱


导致此问题的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77929468/exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</guid>
      <pubDate>Fri, 02 Feb 2024 19:36:21 GMT</pubDate>
    </item>
    <item>
      <title>将经过训练的机器学习模型与 React Native 应用程序集成</title>
      <link>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</link>
      <description><![CDATA[我有一个 FYP 项目（Instagram 等社交媒体应用程序），需要我创建一个简单的推荐系统。我已经使用 Python 对余弦相似度数据集进行了训练，但我不知道下一步该做什么。如何将经过训练的机器学习模型集成到 React Native 中，或者是否有更好、更简单的方法来制作推荐系统？
我尝试阅读文档和观看视频。但我似乎仍然无法掌握一些困难的概念。如果您能在训练我的模型后为我提供有关学习内容的说明或步骤，我将不胜感激。或者，如果我必须使用一些库或软件包等。[不确定这是否是适合此查询的论坛]]]></description>
      <guid>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</guid>
      <pubDate>Fri, 30 Dec 2022 13:02:29 GMT</pubDate>
    </item>
    <item>
      <title>重新训练 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</link>
      <description><![CDATA[我有一个使用对象检测 SSD 移动网络训练的张量流模型。
训练现已完成，我导出了模型推理以进行测试。我的问题是，如果我想稍后使用新的图像数据集重新训练模型，我现在应该在这个阶段做什么以使权重渗透到模型中，以便我可以从那时起重新训练它。我知道有一个冻结脚本，我必须使用它吗？ 
谢谢
阿亚德]]></description>
      <guid>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</guid>
      <pubDate>Thu, 11 Oct 2018 22:12:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Minimax 算法和 Alpha Beta 剪枝解决 Tic Tac Toe 4x4 游戏</title>
      <link>https://stackoverflow.com/questions/51427156/how-to-solve-tic-tac-toe-4x4-game-using-minimax-algorithm-and-alpha-beta-pruning</link>
      <description><![CDATA[我使用 Minimax 和 Alpha Beta 剪枝制作了一款 Tic Tac Toe 游戏。我想为 Tic Tac Toe (10x10) 游戏制作一个计算机 AI，但它的游戏树尺寸大得离谱。 
我的代码是这样的，我只需要更改两个变量来更改棋盘大小+连续获胜所需的单元格。
示例：
boardSize = 3 // 这是用于 3x3 井字游戏
boardSize = 4 // 这是用于 4x4 井字游戏
boardSize = 10 // 这是用于 10x10 井字游戏
和
winStreak = 3 // 需要连续出现 3 个方格才能获胜
winStreak = 4 // 需要连续出现 4 个方格才能获胜
希望你能明白。
所以，我改变了制作井字棋的计划，从 10x10 改为 3x3，效果很好。 
然后我更改 boardSize = 4 和 winStreak = 3 使其成为 (4x4) 井字棋游戏。 
现在，我认为 Minimax 结合 Alpha Beta 剪枝足以解决 4x4 问题，但令人惊讶的是，事实并非如此。 
当我（人类）迈出第一步时，极小极大算法会搜索 5-10 分钟，然后浏览器选项卡就会崩溃。它甚至无法迈出第一步。 
如何提高其速度？人们甚至能够使用 Minimax + Alpha Beta 剪枝来解决国际象棋问题，但是我的代码有什么问题吗？ 
我的完整代码大约有 200-300 行，所以我只写伪代码。 
 humanMakeMove();

极小极大(); // Bot 调用 Minimax 函数来采取行动

函数 Minimax(棋盘，玩家) {
if (checkResult() != 0) // 0 = 游戏开始
   返回检查结果（）； // 1 = 赢，0.5 = 平局，-1 = 输

   可用移动 = getAvailableMoves();

   for(i = 0; i &lt; availableMoves.length;i++)
   {
        移动=可用移动[i]；
        从可用移动数组中删除移动（移动）；
        if (玩家==“X”)
            分数 = Minimax(板, &quot;O&quot;);
        别的
            分数 = Minimax(板, &quot;X&quot;);
        板[i] =“-”； // “-”表示空白


        if (树的深度在第一层 &amp;&amp; 分数 == 1)
                返回最大分数； //这里应用了Alpha Beta Pruning，如果我们得到1分，那么我们就不需要再搜索了。


   }
}

我还可以应用哪些优化来使代码运行得更快？]]></description>
      <guid>https://stackoverflow.com/questions/51427156/how-to-solve-tic-tac-toe-4x4-game-using-minimax-algorithm-and-alpha-beta-pruning</guid>
      <pubDate>Thu, 19 Jul 2018 15:57:26 GMT</pubDate>
    </item>
    </channel>
</rss>