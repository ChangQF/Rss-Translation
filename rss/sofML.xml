<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 07 May 2024 12:25:29 GMT</lastBuildDate>
    <item>
      <title>由于导入，在 python 程序中使用 bash 脚本运行 python 代码时出现高延迟</title>
      <link>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</link>
      <description><![CDATA[我有一个 python 应用程序，它使用子进程来运行 bash 脚本。 bash 脚本依次运行一个 python 文件，其中包含一些导入（视网膜面部、深层面部库）。该应用程序需要花费大量时间来运行，因为每次运行子进程时，都需要 20-30 秒来加载/导入视网膜/深脸模块。有没有办法可以加快速度？
注意：更改设置是不可能的，即我无法直接从原始 python 代码调用 python 代码。
我不确定如何解决这个问题，感谢任何帮助。谢谢。
我尝试使用系统模块的缓存版本，但这不起作用。虽然，我不确定如何正确使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</guid>
      <pubDate>Tue, 07 May 2024 12:09:10 GMT</pubDate>
    </item>
    <item>
      <title>模型预测的各种组合产生相似的地面事实</title>
      <link>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</link>
      <description><![CDATA[我有一个模型（3DUnet，回归问题）可以预测值 PD 和 T1，其中 PD 和 T1 是基于输入的 qMRI 输出。根据这些预测，我使用以下公式计算 T1_Weighted_image：Weighted_images = PD (1 - exp(-1 / (T1 + epsilon)))*，其中 epsilon 很小值以防止被零除和 T1=&gt;0 。在训练期间，我用于损失计算的基本事实是 T1_Weighted_groundtruth，但我也有 PD 和 T1 的基本事实值，尽管它们不直接用于损失计算。它们用于确保 PD 和 T1 预测值的正确性。损失是使用 T1_Weighted_predict 和 T1_Weighted_groundtruth 之间的损失函数计算的。
但是，存在各种 PD 或 T1 组合，可以为 T1_Weighted 产生类似的结果。例如，我的模型可能预测 PD 的非常低的值（例如在 CSF 中作为一个明显的例子），而不是预测 T1 的高值（这是正确的答案）。有没有一种方法可以迫使我的模型预测正确的值，或者至少预测（任何）可能的组合？]]></description>
      <guid>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</guid>
      <pubDate>Tue, 07 May 2024 11:19:19 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，让我可以分辨出它无法识别的人是谁？</title>
      <link>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 11:03:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 3.10.0 在 PySpark 中加载 LightGBM 库时出错</title>
      <link>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</link>
      <description><![CDATA[描述：
在 Python 3.10.0 中使用 PySpark 时，我遇到了 LightGBM 回归器和分类器的问题。
环境：
PySpark 版本：3.2.1
Python版本：3.10.0
Py4j版本：0.10.9.5
Spark jar 包：com.microsoft.azure:synapseml_2.12:0.11.0
错误消息：
java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
重现步骤：

在 Python 中将 LightGBM 回归器或分类器与 PySpark DataFrame 结合使用
3.10.0。
遇到上述错误。

所做的尝试：
我按照此处提供的解决方案进行操作，并对已安装的 libomp 进行了符号链接，但问题仍然存在。
详细的错误堆栈：
py4j.protocol.Py4JJavaError：调用 o5147.fit 时发生错误。
E：java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
E 位于 java.base/java.lang.ClassLoader.loadLibrary（来源未知）
E 位于 java.base/java.lang.Runtime.load0（来源未知）
E 位于 java.base/java.lang.System.load（来源未知）
E 位于 com.microsoft.azure.synapse.ml.core.env.NativeLoader.loadLibraryByName(NativeLoader.java:66)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMUtils$.initializeNativeLibrary(LightGBMUtils.scala:33)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train(LightGBMBase.scala:37)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train$(LightGBMBase.scala:36)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0（本机方法）
E 位于 java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/java.lang.reflect.Method.invoke（来源未知）
E 位于 py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E 位于 py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E 位于 py4j.Gateway.invoke(Gateway.java:282)
E 位于 py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E 位于 py4j.commands.CallCommand.execute(CallCommand.java:79)
E 在 py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E 位于 py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E 位于 java.base/java.lang.Thread.run（来源未知）
附加说明：

该问题似乎与在 Python 3.10.0 中加载 LightGBM 库有关
我已检查指定路径中是​​否存在 lib_lightgbm.dylib。
如果您能提供有关解决此问题的任何见解或建议，我们将不胜感激。
]]></description>
      <guid>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</guid>
      <pubDate>Tue, 07 May 2024 10:23:19 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow：INVALID_ARGUMENT：logits 和标签必须具有相同的第一维，得到 logits 形状 [32,2] 和标签形状 [64]</title>
      <link>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</link>
      <description><![CDATA[我正在使用 tensorflow 测试数据集的准确性。您可以在此处找到我的完整代码以及数据集
我仍在学习机器学习，有很多事情让我头疼。例如，为什么在这种情况下，当我使用“sparse_categorical_crossentropy”编译模型时，
# 由于标签不是独热编码，我们使用稀疏分类交叉熵损失
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
optimizer=&quot;sgd&quot;,
metrics=[&quot;accuracy&quot;])

出现错误：
INVALID_ARGUMENT：logits 和标签必须具有相同的第一个维度，logits 形状为 [32,2]，标签形状为 [64]
然而，在花了几个小时查看代码并无望地修复每一行之后。终于成功了！通过更改 loss=&quot;categorical_crossentropy&quot;
# 由于标签不是独热编码，我们使用稀疏分类交叉熵损失
model.compile(loss=&quot;categorical_crossentropy&quot;,
optimizer=&quot;sgd&quot;,
metrics=[&quot;accuracy&quot;])

我现在真的需要一个解释，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</guid>
      <pubDate>Tue, 07 May 2024 09:02:32 GMT</pubDate>
    </item>
    <item>
      <title>安装在谷歌驱动器上的图像数据集上的 DataGradient 如何使用它作为分类图像</title>
      <link>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</link>
      <description><![CDATA[我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
SS]]></description>
      <guid>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</guid>
      <pubDate>Tue, 07 May 2024 08:47:58 GMT</pubDate>
    </item>
    <item>
      <title>如何通过分析声音数据使用深度学习自动对网络音频设备（扬声器设备）进行分组？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78441187/how-to-group-network-audio-devices-speaker-devices-automatically-using-deep-le</link>
      <description><![CDATA[扬声器自动分组意味着对属于同一房间的设备进行分组（返回唯一的扬声器 ID 集列表）。
场景是

建筑物各楼层（室内环境）的不同房间中已安装很少的设备。
这些设备仅使用以太网线供电，没有任何类型的 WiFi 或蓝牙信号。
这些设备可以播放声音，也可以使用内置麦克风进行录音，这些操作可以通过 ssh（在终端中使用扬声器的 id，如 1.1.1.1）或来自扬声器网站的某些 API 来实现控制（某些网站由该扬声器公司提供）。
我们还可以修改声音播放的音量，还可以获取输入（麦克风）的值和声音的采样率、单声道或立体声等通道。
检查下图，我所说的自动组的意思是输出应该类似于
组 1 为 x,y，组 2 为 z，组 3 为 p,q,r，组 4 为 a,b,c,d。


最初，该方法涉及收集一些数据样本并通过提取 MFCC 或梅尔频谱图等声音特征来分析它们。如下获得数据样本，通过运行Python脚本经由终端访问所有扬声器，然后一个扬声器播放声音，例如配置信号或“叮咚”声。而所有其他扬声器（包括正在播放的扬声器）都记录了该声音。
我尝试通过开发基本的暹罗网络来解决这种情况，但不幸的是，我无法实现有效或准确的输出。因此，我正在寻求有关使用深度学习分析声音数据在现实生活中实现此场景的替代方法的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78441187/how-to-group-network-audio-devices-speaker-devices-automatically-using-deep-le</guid>
      <pubDate>Tue, 07 May 2024 08:42:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Huggingface BERT 模型中使用“encoder_hidden_​​states”参数？</title>
      <link>https://stackoverflow.com/questions/78441081/how-to-use-the-encoder-hidden-states-parameter-in-huggingface-bert-model</link>
      <description><![CDATA[我想学习当“is_decoder”为 True 并且“add_cross_attention”为 True 时如何使用 bert。
但它不起作用。
这是我的代码：
tokenizer = BertTokenizer.from_pretrained(&#39;google-bert/bert-base-uncased&#39;)

bertconfig = BertConfig.from_pretrained(&#39;google-bert/bert-base-uncased&#39;, is_decoder=True, add_cross_attention=True)

伯特 = BertLMHeadModel(bertconfig).to(&#39;cuda:0&#39;)

input = tokenizer(&#39;你好，我的狗很可爱。&#39;, return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

cross_tensor = bert.bert.embeddings.forward(inputs[&#39;input_ids&#39;])

输出= bert（**输入，encoder_hidden_​​states = cross_tensor，encoder_attention_mask = torch.ones_like（cross_tensor）.to（&#39;cuda：0&#39;））

错误信息如下：
文件 d:\anaconda3\envs\torch_py38\lib\site-packages\transformers\models\bert\modeling_bert.py:352，在 BertSelfAttention.forward(self,hidden_​​states,attention_mask,head_mask,encoder_hidden_​​states,encoder_attention_mask ，过去的键值，输出注意）
    第349章
    [第 350 章]
    [第 351 章]
--&gt;第352章
    攀上漂亮女局长之后354
    第355章

运行时错误：张量 a (9) 的大小必须与非单一维度 3 处的张量 b (768) 的大小匹配

我尝试更改“encoder_hidden_​​states”张量的形状，但它不起作用。
谁能给我解决这个问题的解决方案，或者在这种情况下使用 bert 的示例。]]></description>
      <guid>https://stackoverflow.com/questions/78441081/how-to-use-the-encoder-hidden-states-parameter-in-huggingface-bert-model</guid>
      <pubDate>Tue, 07 May 2024 08:22:28 GMT</pubDate>
    </item>
    <item>
      <title>Python 推理管道的加速</title>
      <link>https://stackoverflow.com/questions/78440864/speed-up-of-python-inference-pipeline</link>
      <description><![CDATA[我正在尝试构建一个支持 Huggingface 的分布式 LLM 推理平台。实现涉及利用 Python 进行模型处理和利用 Java 与外部系统交互。下面是负责从 Java 程序接收输入文本、通过预先训练的 LLM 进行处理并返回处理后的文本的 Python 代码：
import socket, sys
import threading
from transformers import pipeline

generator = pipeline(&#39;text-generation&#39;, model=&#39;gpt2-large&#39;, device=&quot;cuda&quot;)

def process_input(input_text):
request = generator(input_text, min_length=200)
return request[0][&quot;generated_text&quot;]

def handle_connection(conn):
with conn:
data = conn.recv(10240).decode()
processing_data = process_input(data.strip())
conn.sendall(processed_data.encode())

PORT = int(sys.argv[1])

with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
s.bind((&#39;localhost&#39;, PORT))
s.listen()
while True:
conn, addr = s.accept()
thread = threading.Thread(target=handle_connection, args=(conn,))
thread.start()

如您所见，它接受来自 Java 进程的套接字连接、接收文本、返回请求结果并关闭连接，所有这些都在线程的执行中完成。所有线程之间都有一个共享管道，由于它占用了太多内存空间，因此只创建了 1 次。
在 Java 方面，我有一个类 LLMProcess，它处理创建和与 Python 进程的通信，在每个请求生命周期中使用线程。
LLMProcess process = new LLMProcess();

for (int i = 0; i &lt; 50; i++) {
int index = i;
Thread thread = new Thread(() -&gt; {
System.out.println(&quot;&quot; + index + &quot; : &quot; + process.request(&quot;Sample text&quot;);
System.out.flush();
});
thread.start();
}

但是，当尝试同时执行大量请求时，系统在处理请求时会表现出顺序行为，并显示与线程使用相关的开销，而不是通过 LLM 管道和 GPU 加速有效利用并发处理。
目标是通过最小化线程使用开销并充分利用可用的 GPU 资源来优化此过程。尽管存在 GPU 支持，但在程序执行期间其负载仍然很小，通常不超过 3%。]]></description>
      <guid>https://stackoverflow.com/questions/78440864/speed-up-of-python-inference-pipeline</guid>
      <pubDate>Tue, 07 May 2024 07:38:05 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，在其中我教它不认识的人是谁？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 00:12:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Google Colab Python 代码构建浏览器扩展？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</link>
      <description><![CDATA[亲爱的 StackOverflow，您好，
我目前正在考虑使用 Google Colab 中的代码创建一个浏览器扩展。里面有Python脚本。
我知道，目前无法使用 Python 创建扩展。但还有其他选择吗？
Python 脚本本身大约有 100 行，带有一个训练模型。我认为仅使用 JavaScript 无法做到这一点。
谢谢；]]></description>
      <guid>https://stackoverflow.com/questions/78435672/how-do-i-build-a-browser-extension-using-google-colab-python-code</guid>
      <pubDate>Mon, 06 May 2024 09:19:41 GMT</pubDate>
    </item>
    <item>
      <title>为我的 Npy 数据集定义 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</link>
      <description><![CDATA[我需要帮助为我的数据定义火炬模型。我尝试了各种方法，但似乎没有任何效果。与输入尺寸和形状相关的错误不断出现。我该如何解决这些问题？
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
导入 torch.nn.function 作为 f

# 从 .npy 文件加载数据
data = np.load(“其他py文件/project_files/data/train/data.npy”)
print(&quot;数据形状：&quot;, data.shape) # (401, 701, 255)

数据大小 = 数据.形状[0] * 数据.形状[1] * 数据.形状[2]
print(“数据大小：”, data_size) # 71680755

# 从 .npy 文件加载标签数据
labels = np.load(“其他py文件/project_files/data/train/label.npy”)
print(&quot;标签数据形状:&quot;, labels.shape) # (401, 701, 255)

# 将 numpy 数组转换为 PyTorch 张量
data_tensor = torch.Tensor(数据)
labels_tensor = torch.Tensor(标签)


类 MyModel(nn.Module):
    def __init__(自身):
        超级（MyModel，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def 前向（自身，x）：
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        返回x

模型 = MyModel()
打印（模型）

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

数据集 = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(数据集,batch_size=32,shuffle=True)

纪元数 = 10
对于范围内的纪元（num_epochs）：
    运行损失 = 0.0
    对于 i，enumerate(dataloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        outputs = model(inputs.unsqueeze(1)) # 通道维度
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
        如果我％100==99：
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 100}&quot;)
            运行损失 = 0.0


使用 torch.no_grad()：
    Predictions = model(data_tensor.unsqueeze(1)) # 通道维度

控制台输出：
已连接到 pydev 调试器（版本 223.8836.43）
数据形状：(401, 701, 255)
数据大小：71680755
标签数据形状：(401, 701, 255)
我的模型（
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （池）：MaxPool2d（kernel_size = 2，stride = 2，padding = 0，dilation = 1，ceil_mode = False）
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （fc）：线性（in_features = 71680755，out_features = 2，偏差= True）
）

文件“C:\Users\PC1\PycharmProjects\Project1\newmodel2.py”，第 36 行，向前
    x = x.view(-1, self.fc_input_size)
运行时错误：形状“[-1, 71680755]”对于大小 22579200 的输入无效
python-BaseException
]]></description>
      <guid>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</guid>
      <pubDate>Mon, 06 May 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 GAN 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</link>
      <description><![CDATA[我正在尝试训练 GAN 模型来检测糖尿病视网膜病变图像，但它抛出错误。请帮忙。
图像数据集不为空我已尝试查看它
错误是：-
&lt;前&gt;&lt;代码&gt;纪元 1/50
回溯（最近一次调用最后一次）：
  文件“C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py”，第 65 行，在  中
    分类器.fit（X，Y，batch_size = 32，epochs = 50）
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py”，第 553 行，在 categorical_crossentropy 中
    引发值错误（
ValueError：参数“target”和“output”必须具有相同的形状。收到：target.shape=(无，3)，output.shape=(无，5)

火车模型文件的代码是：
将 numpy 导入为 np
导入imutils
导入系统
导入CV2
导入操作系统
从tensorflow.keras.utils导入到_categorical
从 keras.models 导入 model_from_json
从 keras.layers 导入 MaxPooling2D
从 keras.layers 导入密集、丢弃、激活、扁平化
从 keras.layers 导入 Convolution2D
从 keras.models 导入顺序

图片 = []
图像标签 = []
目录=&#39;数据集&#39;
文件列表 = os.listdir(目录)
索引 = 0
对于 list_of_files 中的文件：
    子文件 = os.listdir(目录+&#39;/&#39;+文件)
    对于子文件中的子项：
        路径=目录+&#39;/&#39;+文件+&#39;/&#39;+子
        img = cv2.imread(路径)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        如果 img 为 None：
          print(&#39;路径错误：&#39;, 路径)
        别的：
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         图像.append(im2arr)
         image_labels.append(文件)
    打印（文件）

X = np.asarray(图像)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;形状 == &quot;+str(X.shape))
print(“形状==”+str(Y.shape))
打印（Y）
X = X.astype(&#39;float32&#39;)
X = X/255

np.save(“model/img_data.txt”,X)
np.save(“model/img_label.txt”,Y)

X = np.load(&#39;model/img_data.txt.npy&#39;)
Y = np.load(&#39;model/img_label.txt.npy&#39;)
打印（Y）
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet 迁移学习代码在这里
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
classifier.add(Convolution2D(32, 3, 3, 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
分类器.add(Flatten())
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 5, 激活 = &#39;softmax&#39;))
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）
分类器.fit（X，Y，batch_size = 32，epochs = 50）

我尝试过更改尺寸，但它不起作用，我无法理解这是版本错误还是代码错误，因此为了解决同样的问题，请提供解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</guid>
      <pubDate>Thu, 02 May 2024 15:39:54 GMT</pubDate>
    </item>
    <item>
      <title>如何根据掩蔽将矩阵相乘并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>用最少层数训练绝对函数的神经网络</title>
      <link>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</link>
      <description><![CDATA[我正在尝试训练神经网络来学习 y = |x|功能。我们知道，绝对函数有两条不同的线在零点处相互连接。所以我尝试使用以下顺序模型：
隐藏层：
2 致密层（激活relu）
输出层：
1 致密层
训练模型后，它只拟合函数的一半边。大多数时候是右手边，有时是左手边。一旦我在隐藏层中再添加 1 层，那么我就用 3 层代替 2 层，它就完全符合该功能了。谁能解释为什么当绝对函数只有一次切割时需要额外的一层？
这是代码：
将 numpy 导入为 np


X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# 重塑数据以适应模型输入
X = X.reshape(-1, 1)
Y = Y.重塑(-1, 1)

将张量流导入为 tf
将张量流导入为 tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 构建模型
模型 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;,metrics=[&#39;mae&#39;])
model.fit(X, Y, epochs=1000)
# 使用模型进行预测
Y_pred = model.predict(X)

# 绘制结果
plt.scatter(X, Y, color=&#39;blue&#39;, label=&#39;实际&#39;)
plt.scatter(X, Y_pred, color=&#39;red&#39;, label=&#39;预测&#39;)
plt.title(&#39;实际与预测&#39;)
plt.xlabel(&#39;X&#39;)
plt.ylabel(&#39;Y&#39;)
plt.图例()
plt.show()

2 个密集层的绘图：

3 个密集层的绘图：
]]></description>
      <guid>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</guid>
      <pubDate>Thu, 11 Apr 2024 15:34:01 GMT</pubDate>
    </item>
    </channel>
</rss>