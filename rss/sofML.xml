<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Feb 2025 01:15:42 GMT</lastBuildDate>
    <item>
      <title>使用模型蒸馏来优化我的模型</title>
      <link>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</link>
      <description><![CDATA[我有一个 NN 模型，用于学习端到端通信系统。它是一个自动编码器，其中编码器充当发射器；它采用 8 位并将其编码为 IQ 值，解码器充当接收器；它采用生成的 IQ 值并将其解码为 8 位。我还有一个通道模型，可以模拟噪声、频率/相位偏移等。
该模型经过训练，具有非常好的误码率 (BER)，但在进行推理时具有高延迟，因此我需要对其进行优化。我正在尝试遵循 pytorch 的知识提炼教程，但到目前为止，我无法让我的学生有效地学习。
我认为我的问题在于我的软损失函数不正确。在原始训练循环中，我使用 BinaryCrossEntropy 损失来对抗模型的预测位概率与真实输入位。从文档中可以看出，K.D 似乎包含了一个额外的损失，即采用学生和父母概率的 KL 散度损失。但是，在运行代码时，我的损失并没有改善。
我感到困惑的是，我的“软损失”应该是什么类型的损失函数，以及它应该获得什么输入类型（logit 或概率）。我尝试了不同的排列（将对数概率输入到 KL Div 中，使用 CrossEntropy 损失而不是 KL，即文档中显示的损失函数），但它们都没有以任何方式提高我的学生模型的性能。
这大致就是我正在使用的代码。它不是完整的代码；我只展示了父自动编码器和 K.D 循环，但这足以表达我的观点。
任何帮助都值得感激。
import torch
import torch.nn as nn
import torch.optim as optim

# 定义编码器
class Encoder(nn.Module):
def __init__(self):
super(Encoder, self).__init__()
self.fc1 = nn.Linear(8, 16) # 扩展特征空间
self.relu = nn.ReLU()
self.fc2 = nn.Linear(16, 10) # 输出 2 个值（IQ 表示）

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x) # 输出原始 IQ 符号
return x

# 定义解码器
class Decoder(nn.Module):
def __init__(self):
super(Decoder, self).__init__()
self.fc1 = nn.Linear(100, 50) # 从 IQ 扩展回来
self.fc2 = nn.Linear(50, 30)
self.fc3 = nn.Linear(30, 16)
self.fc4 = nn.Linear(16, 8) # 输出 8 位恢复序列
self.relu = nn.ReLU()
self.sigmoid = nn.Sigmoid() # 确保输出在 (0,1) 范围内

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x)
x = self.relu(x)
x = self.fc3(x)
x = self.relu(x)
x = self.fc4(x)
x = self.sigmoid() # 解释为概率
return x

# 定义自动编码器 (编码器 -&gt;通道 -&gt; 解码器)
class Autoencoder(nn.Module):
def __init__(self, noise_std=0.1):
super(Autoencoder, self).__init__()
self.encoder = Encoder()
self.decoder = Decoder()

def forward(self, x):
x = self.encoder(x) # 将 8 位编码为 2 个 IQ 符号
x = self.decoder(x) # 解码回 8 位序列
return x

ParentModel = Autoencoder(noise_std=0.1)

# 加载预先训练的权重
load_weights(model, path, optimizer)

def knowledge_distillation(teacher, student, T, epochs, batches, alpha):
ce_loss = nn.BCELoss()
kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)
optimizer = optim.Adam(student.parameters(), lr = 1e-4)

teacher.eval() # 教师设置为评估模式
student.train() # 学生设置为训练模式

for epoch in range(epochs):
input_bits = generate_binary_tensor(8, batches) # 生成 [8, batch] 二进制张量

optimizer.zero_grad()

with torch.no_grad():
teacher_predictions = teacher(input_bits) # 教师前向传递

student_predictions = student(input_bits) # 学生前向传递

# 计算硬损失
hard_loss = ce_loss(student_predictions, input_bits)

# 计算软损失（不确定这部分）
soft_loss = kl_loss(student_predictions, teacher_predictions) * (T**2)

total_loss = alpha*soft_loss + (1-alpha)*hard_loss

total_loss.backward()
optimizer.step()

# 存储 BER

]]></description>
      <guid>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</guid>
      <pubDate>Wed, 05 Feb 2025 00:55:20 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法提高我的 CNN 准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</guid>
      <pubDate>Tue, 04 Feb 2025 20:07:08 GMT</pubDate>
    </item>
    <item>
      <title>函数内部和外部的权重和偏差会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</guid>
      <pubDate>Tue, 04 Feb 2025 19:13:39 GMT</pubDate>
    </item>
    <item>
      <title>如何为特定设备创建机器学习和线性回归模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device</link>
      <description><![CDATA[我有以下数据源模式：
在此处输入图片说明
我将“日期”列编码为三个单独的列：
年、月、日期。
我的未来是
设备 ID；消耗能量；年；月；日
在此处输入图片说明
我想预测给定未来的消耗能量（设备 ID 和日期）。能耗是一个日益增长的设备计数器
我创建了相关矩阵来查看属性之间的相关性，我发现消耗_能耗和设备之间的相关性非常低 (-0.15)。
以下是模型训练的模型指标：
&#39;rmse&#39;: np.float64(0.7648236497453013), 
&#39;r2_score&#39;: 0.41662485203361777, 
&#39;coefficients&#39;: array([[-0.59067425, 0.61791617, 0.04103179, 0.00336239]]), 
&#39;intercept&#39;: array([0.00086466]

当我仅使用一个 device_id 加载数据时，结果更好：
&#39;rmse&#39;: np.float64(0.1045133437744489), 
&#39;r2_score&#39;: 0.9894746517207146, 
&#39;coefficients&#39;: array([[0. , 0.99656364, 0.06202053, 0.00616694]]), 
&#39;intercept&#39;: array([-0.00046585])

我理解基于多个设备的数据构建的模型将产生不同的结果...
我在一个文件中接收包含所有设备的数据。当给定的未来是 device_id 时，如何正确构建此模型？
我应该创建预测方法，该方法首先为给定的 device_id 选择数据，然后为该特定设备创建新的数据框，然后构建模型并进行预测吗？这会耗费时间，我需要根据需求对每个预测进行计算。如果我想保存/选择模型并使用它来提供数据，该如何处理？（我的数据有很多设备）
我不知道是否可以基于具有多个设备的数据构建此模型，并驱动线回归算法将设备 ID 未来视为主要/重要系数因子。我应该使用不同的机器学习模型吗？
我尝试使用 python 和 scikit-learn 为特定设备创建机器学习和线回归模型]]></description>
      <guid>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device</guid>
      <pubDate>Tue, 04 Feb 2025 17:46:48 GMT</pubDate>
    </item>
    <item>
      <title>实现扩散生成模型进行数据增强，但训练损失值太高 [迁移]</title>
      <link>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</guid>
      <pubDate>Tue, 04 Feb 2025 16:56:25 GMT</pubDate>
    </item>
    <item>
      <title>通过计算检查梯度是否会爆炸或消失[关闭]</title>
      <link>https://stackoverflow.com/questions/79412199/check-through-calculations-whether-the-gradients-will-explode-or-vanish</link>
      <description><![CDATA[我正在复习旧考试题目，偶然发现了这个：

考虑一个常规的 MLP（多层感知器）架构，该架构具有 10 个完全连接的层和 ReLU 激活函数。网络的输入是一个大小为 100 的向量，其中每个维度的均值为零，标准差在整个数据集中等于 1。
每个隐藏层有 10000 个神经元，权重从均值为零、方差为 0.01 的正态分布初始化。


以下哪个选项最有可能？
a) 梯度将爆炸 (y)
b) 梯度将消失 (n)
c) 都不是。 (n)

我如何证明这个答案？我想我需要计算一些东西，但我不知道从哪里开始。
输入中的预期值都是零，权重也是如此，所以当一切都为零时很难计算任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/79412199/check-through-calculations-whether-the-gradients-will-explode-or-vanish</guid>
      <pubDate>Tue, 04 Feb 2025 15:26:12 GMT</pubDate>
    </item>
    <item>
      <title>寻求专业培训项目评论数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</link>
      <description><![CDATA[我正在分析专业培训课程的评论，并根据这些评论创建推荐系统。具体来说，我感兴趣的是找到一个包含各种专业发展课程、培训课程和认证的用户评论的数据集。
我搜索过 Kaggle 和政府开放数据门户等常见来源，但没有找到我正在寻找的内容。
我在哪里可以找到这样的数据集？或者我如何使用其他方法来利用现有数据（Coursera / udemy 课程评论）？]]></description>
      <guid>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</guid>
      <pubDate>Tue, 04 Feb 2025 12:51:33 GMT</pubDate>
    </item>
    <item>
      <title>如何计算用户绘制形状识别的最小二乘误差和特征面积误差？</title>
      <link>https://stackoverflow.com/questions/79411692/how-do-you-calculate-the-least-squares-error-and-feature-area-error-for-user-dra</link>
      <description><![CDATA[我想创建一个简单的系统来识别用户在我的大学课程游戏中绘制的预定义形状，以施展不同的法术。每个法术形状只能用一个笔画来绘制。经过大量研究，我发现了 MergeCF 算法（Aaron Wolin，2009）。
它看起来很简单，我能够获得一个初始的“角”数组来分割笔画段。但是，尝试将假阳性角合并到正确角需要使用计算误差拟合值。
这些值是“最小二乘误差”和“特征面积误差”，这些值的计算据称在另一篇关于 Paulson 识别器的论文中（Brandon Paulson，2008）。
阅读这篇论文，它详细说明了使用另一篇论文中的“正交距离平方”方法找到最小二乘误差（Tevfik Metin Sezgin，2002），并且使用另一篇论文中的计算找到特征面积误差（Bo Yu，2003）。
通过找到从一对用户绘制的点到最佳拟合线的每个四边形的面积来找到特征面积。但是，要找到最佳拟合线，您需要使用最小二乘法将线拟合到笔划段。
正交距离平方法要求已经找到最佳拟合线。
特征区域论文没有详细说明如何为直线或圆弧找到最佳拟合线，而正交距离平方论文提到使用“混合拟合”方法，由于缺乏更好的词汇，我无法理解。
我甚至尝试使用 ChatGPT 来了解发生了什么，我并不经常这样做，但经过几次提示后，它只是循环说最小二乘线性回归线是通过计算最小二乘误差找到的，而最小二乘误差是通过计算最小二乘回归线找到的。
如何计算最小二乘误差和特征面积误差？请帮忙。
有关更多信息（不完全相关），我正在使用虚幻引擎 5 蓝图，并且我决定不使用任何神经网络 OCR，例如 Tesseract，因为游戏应该在 Pico Neo 3 Pro VR 耳机上打包和使用，而无需外部命令行程序，这是客户的要求。

Aaron Wolin (2009)：https://www.researchgate.net/publication/220772382_Sort_Merge_Repeat_An_Algorithm_for_Effectively_Finding_Corners_in_Hand-sketched_Strokes
Brandon Paulson (2008)：https://www.researchgate.net/publication/221607733_PaleoSketch_Accurate_primitive_sketch_recognition_and_beautification
Tevfik Metin Sezgin (2002)：https://www.researchgate.net/publication/2496082_Sketch_Based_Interfaces_Early_Processing_for_Sketch_Understanding
Bo Yu (2003): https://dl.acm.org/doi/10.1145/604471.604499
]]></description>
      <guid>https://stackoverflow.com/questions/79411692/how-do-you-calculate-the-least-squares-error-and-feature-area-error-for-user-dra</guid>
      <pubDate>Tue, 04 Feb 2025 12:28:22 GMT</pubDate>
    </item>
    <item>
      <title>huggingface 的图像分割 ONNX 在 ML.Net 中使用时会产生截然不同的结果</title>
      <link>https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use</guid>
      <pubDate>Tue, 04 Feb 2025 09:38:57 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 Llava-v1.5-7b 来检测图像，并从 LM Studio 中提取它的 API。当我要求模型命名时，它将自己称为 Vicuna</title>
      <link>https://stackoverflow.com/questions/79410924/im-trying-to-use-llava-v1-5-7b-to-detect-images-and-im-pulling-its-api-from-l</link>
      <description><![CDATA[为了降低成本，我决定在本地提供 LlaVA 图像。我无法在 google collab 中加载它，因为免费版本提供 12GB 内存，而加载 LlaVA 需要 16GB。
我决定下载 LM studio 并从那里获取 LLaVA，托管服务器托管在我的本地主机端口上。
然后，我将图像转换为 base64 并在我的 VS 代码中调用 api。
# 构建有效载荷。
# 许多实现模仿 OpenAI Chat Completions API，因此我们发送“模型”和“消息”列表。
# 在这里，我们假设您的 Llava 模型接受“图像”字段以及用户的文本。
payload = {
&quot;model&quot;: &quot;llava-v1.5-7b&quot;, # 在此处指定您的模型名称
&quot;messages&quot;: [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: query_text,
&quot;image&quot;:coded_image # 此字段名称可能因您的集成而异
}
]
}

headers = {&quot;Content-Type&quot;: &quot;application/json&quot;}

try:
response = request.post(url, headers=headers, json=payload)
response.raise_for_status()
except request.RequestException as e:
print(f&quot;与 Llava API 通信时出错：{e}&quot;)
return None

try:
return response.json()
except json.JSONDecodeError:
print(&quot;Response is not valid JSON.&quot;)
返回 response.text

当我输入罗纳尔多的图像时，它给出的结果不准确。当我要求它自己命名时，它回答说，

我是一个名为 Vicuna 的语言模型，我接受了大型模型系统组织 (LMSYS) 研究人员的培训。

我构建有效载荷的方式有问题吗？还是问题出在其他地方？
我首先尝试在 LM 工作室界面上传图像，在那里我可以上传罗纳尔多的图像，模型成功识别了人物、活动等。
当我使用端口和 API 标识符在 vs 代码中执行相同操作时，结果与描述一致。]]></description>
      <guid>https://stackoverflow.com/questions/79410924/im-trying-to-use-llava-v1-5-7b-to-detect-images-and-im-pulling-its-api-from-l</guid>
      <pubDate>Tue, 04 Feb 2025 07:49:43 GMT</pubDate>
    </item>
    <item>
      <title>以多模态方式训练潜在扩散模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79410889/training-latent-diffusion-models-in-a-multi-modal-manner</link>
      <description><![CDATA[LDM 论文提到调节块可以包括语义图、文本、图像和表示。是否可以使用图像内容作为模型输入，并通过调节块提供对象类和相应的边界框位置作为训练数据？
这旨在通过提供调节类和边界框，使模型能够在推理过程中在相关区域中生成指定对象。
以上所有内容均基于数据集包含违禁品且具有正确标记的类和边界框的假设。
我正在尝试通过扩散模型生成异常 X 射线包裹图像。]]></description>
      <guid>https://stackoverflow.com/questions/79410889/training-latent-diffusion-models-in-a-multi-modal-manner</guid>
      <pubDate>Tue, 04 Feb 2025 07:33:59 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型在二元分类中仅预测一个类（猫）</title>
      <link>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</link>
      <description><![CDATA[我是 AI 和深度学习的新手，我使用 TensorFlow/Keras 训练了一个二元分类 CNN 来区分猫和狗。然而，在测试数据集上进行评估时，该模型只预测每张图片都是“猫”，尽管数据集包含这两个类别。
这是我的代码：
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import load_model

def normalizer(image, label):
aux = tf.cast(image, dtype=tf.float32)
image_norm = aux/255.0
return image_norm, label

train_data, valid_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/training&#39;,
validation_split=0.1, 
subset=&quot;both&quot;, 
seed=42, 
image_size=(150, 150), 
batch_size=32 
)

test_data = tf.keras.utils.image_dataset_from_directory(
&#39;dataset/test&#39;, 
image_size=(150, 150), 
batch_size=32 
)

train = train_data.map(normalizer)
valid = valid_data.map(normalizer) 
test = test_data.map(normalizer)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),activation=&#39;relu&#39;, input_shape=(150,150,3)))
model.add(MaxPooling2D())

model.add(Conv2D(filters=64, kernel_size=(3,3),激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Conv2D(filters=128, kernel_size=(3,3), 激活=&#39;relu&#39;))
model.add(MaxPooling2D())

model.add(Flatten())

model.add(Dense(units=256, 激活=&#39;relu&#39;))
model.add(Dense(units=1, 激活=&#39;sigmoid&#39;))

model.compile(
optimizer=&#39;adam&#39;,
loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[&#39;accuracy&#39;],
)

print(model.summary())

hist = model.fit(
训练，
batch_size=32， 
epochs=20， 
shuffle=True，
validation_data=valid
)

plt.plot(hist.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_loss&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.title(&#39;训练和验证中的损失&#39;)
plt.show()

如果 hist.history 中有 &#39;accuracy&#39;: 
plt.plot(hist.history[&#39;accuracy&#39;], label=&#39;train&#39;)
plt.plot(hist.history[&#39;val_accuracy&#39;], label=&#39;valid&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.legend()
plt.title(&#39;训练和验证的准确性&#39;)
plt.show()

model.save(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

new_model = load_model(os.path.join(&#39;models&#39;, &#39;test.h5&#39;))

loss, acc = new_model.evaluate(test, batch_size=32)

print(loss)
print(acc)

y_pred = new_model.predict(test) 

y_true = np.concatenate([y.numpy() for x, y in test], axis=0)

matrix = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(
matrix.numpy(), 
annot=True, 
fmt=&#39;d&#39;, 
cmap=&#39;Blues&#39;, 
xticklabels=[&#39;Cat&#39;, &#39;Dog&#39;], 
yticklabels=[&#39;Cat&#39;, &#39;Dog&#39;],
)

plt.ylabel(&#39;True Label&#39;)
plt.xlabel(&#39;Predicted Label&#39;)
plt.title(&#39;Confusion Matrix&#39;)
plt.show()

这是混淆矩阵
混淆矩阵
我怀疑的可能原因

类别不平衡 –&gt; 我的训练数据中猫和狗的数量大致相等，所以我不认为这是原因。
标签问题 –&gt;我检查并确认 y_true 既有 0 也有 1，所以标签应该没问题。
注：该模型在验证中的准确率达到了约 70%
]]></description>
      <guid>https://stackoverflow.com/questions/79409884/tensorflow-model-predicts-only-one-class-cats-in-binary-classification</guid>
      <pubDate>Mon, 03 Feb 2025 20:03:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn 在 KDE 之前正确缩放数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</link>
      <description><![CDATA[我在地理空间数据上使用 sklearn.neighbors.KernelDensity。我注意到带宽估计方法没有考虑数据的空间范围，只考虑样本和特征的数量。 sklearn 文档或 sklearn 教程似乎没有提到应该缩放数据。
因此我的问题是：在拟合 KDE 之前缩放地理空间数据的适当方法是什么？
这是一个最小示例：
import numpy as np
from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=2)

X_uniform = np.random.rand(100, 2)
X_upscaled = X_uniform * 100

for i,X in enumerate([X_uniform, X_upscaled]):
kde = KernelDensity(kernel=&#39;gaussian&#39;, broadband=&quot;scott&quot;).fit(X)
print(f&quot;bw: {kde.bandwidth_}&quot;)

# 创建网格
x_grid = np.linspace(X[:,0].min(), X[:,0].max(), 50)
y_grid = np.linspace(X[:,1].min(), X[:,1].max(), 50)
X,Y = np.meshgrid(x_grid, y_grid)
xy = np.vstack([X.ravel(), Y.ravel()]).T

z = np.exp(kde.score_samples(xy)).reshape(X.shape)
fig.add_trace(go.Contour(z=z, x=x_grid, y=y_grid), row=1, col=i+1)

fig.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79397275/correct-scaling-of-data-before-kde-with-sklearn</guid>
      <pubDate>Wed, 29 Jan 2025 15:33:57 GMT</pubDate>
    </item>
    <item>
      <title>训练数据集应如何分布？</title>
      <link>https://stackoverflow.com/questions/72614571/how-should-a-training-dataset-be-distributed</link>
      <description><![CDATA[我正在构建一个文本转语音模型。我想知道我的训练数据集是否应该“现实地”分布（即与将要使用的数据分布相同），还是应该均匀分布以确保它在各种句子上都能表现良好。]]></description>
      <guid>https://stackoverflow.com/questions/72614571/how-should-a-training-dataset-be-distributed</guid>
      <pubDate>Tue, 14 Jun 2022 09:26:20 GMT</pubDate>
    </item>
    <item>
      <title>银行交易数据集</title>
      <link>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</link>
      <description><![CDATA[我想使用银行交易数据集制作信用卡和借记卡之间的图表。借记和贷记金额，但没有得到正确的数据集。
有人能给我提供同样的数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</guid>
      <pubDate>Sat, 06 Jul 2019 13:09:23 GMT</pubDate>
    </item>
    </channel>
</rss>