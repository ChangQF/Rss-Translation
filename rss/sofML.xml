<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Jan 2024 18:17:58 GMT</lastBuildDate>
    <item>
      <title>图像预处理 |预测范围为 [0.44 - 0.55]</title>
      <link>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</link>
      <description><![CDATA[我正在尝试解决二元分类任务。我有鳄鱼和短吻鳄的数据集。我有一个形状为 (2894, 2) 的矩阵 x，还有一个带有标签 (0 或 1) 的变量 y，形状为 2894。
`
模型 = tf.keras.Sequential([
tf.keras.layers.Dense（256，激活=&#39;relu&#39;），
tf.keras.layers.Dropout(速率=(0.12)),
tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
tf.keras.layers.Dropout(速率=(0.12)),
tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)

])
这是我选择的一个模型。
loss_fn = tf.keras.losses.BinaryCrossentropy()
model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])
但我的预测每次都在 (0.44 - 0.55) 范围内 x_train [[像素, 像素], [像素, 像素], [像素, 像素]] 的示例 另外，我对每个测试和训练变量都使用此方法：洗牌（minmax.fit_transform（np.concatenate（（x_train_aligator，x_train_crocodile），轴= 0）），random_state = 1）`
现在我对 x_train 和 x_test 使用 StandardScaler() 和 MinMaxScaler() 。此外，它们在测试数据的 random_state=1 和 random_state=2 方面也有相似之处
但我不知道为什么我的结果每次都在一个范围内。
我尝试过对数据和随机保存生成进行不同的操作。]]></description>
      <guid>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</guid>
      <pubDate>Fri, 12 Jan 2024 16:42:15 GMT</pubDate>
    </item>
    <item>
      <title>Filter_Value 选择 TDA R</title>
      <link>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</link>
      <description><![CDATA[我对 R 中 TDA 库（GSSTDA、TDAMapper、Mapper 等）的“filter_value”参数有点困惑。
例如，给出 GSSTDA 中的 Mapper 对象：
&lt;前&gt;&lt;代码&gt;映射器(
  完整数据，
  过滤器值，
  间隔数 = 5,
  重叠百分比 = 40,
  distance_type =“cor”，
  clustering_type =“分层”，
  num_bins_when_clustering = 10,
  连接类型=“单一”，
  最优聚类模式=“”，
  na.rm = TRUE
）

参数
过滤值
“对输入矩阵应用滤波函数后获得的向量，即每个包含的样本具有滤波函数值的向量。”

因此，我想问是否有人可以通过一个众所周知的数据（例如 Iris）的示例来描述如何手动选择filter_value？
将不胜感激。
https://search.r-project.org /CRAN/refmans/GSSTDA/html/mapper.html]]></description>
      <guid>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</guid>
      <pubDate>Fri, 12 Jan 2024 15:21:09 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递的 save_path 不是有效的检查点：./param_model/model(8, 100, 3)_75_10_-100_0</title>
      <link>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</link>
      <description><![CDATA[#---生成通道----
通道，set_location_user =generate_channel（params_system，num_samples = 1，
location_user_initial=location_user, Rician_factor=Rician_factor)
y，y_real_tmp =generate_received_pilots_batch（通道，phase_shifts，导频，noise_power_db，Pt = Pt_u）
y_ks_tmp =去相关（y，飞行员）
y_ks = np.concatenate([y_ks_tmp.real, y_ks_tmp.imag], axis=1)
y_real = (y_real_tmp - y_mean) / y_std
y_ks_real = (y_ks - y_ks_mean) / y_ks_std
set_location_user = (set_location_user - location_mean) / (location_std+1e-15)

# ----------------------------------神经网络波束成形------------------------ --------------
model_path = &#39;./param_model/model&#39; + str(params_system) + &#39;_&#39; + str(len_pilot) + &#39;_&#39; + str(
    Rician_factor) + &#39;_&#39; + str(noise_power_db) + &#39;_&#39; + str(input_flag)
#print(“模型路径：”, model_path)

这个粗体文本给出了错误。
使用Python 3.6
如何纠正]]></description>
      <guid>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</guid>
      <pubDate>Fri, 12 Jan 2024 14:30:02 GMT</pubDate>
    </item>
    <item>
      <title>无法对具有 4 个键列和一个索引列的缩放数据执行逆变换</title>
      <link>https://stackoverflow.com/questions/77806320/failure-to-perform-inverse-transform-of-scaled-data-with-4-key-columns-and-an-in</link>
      <description><![CDATA[我正在尝试使用简单的 LSTM 模型来预测太阳能产量，因此，每当我尝试运行代码行时都会遇到问题，它会给我错误
这是我用来预测错误点的完整代码
df[&#39;DateTime&#39;] = df[&#39;Date&#39;].astype(str) + &#39; &#39; + df[&#39;Time&#39;].astype(str)
df = df.drop([&#39;日期&#39;, &#39;时间&#39;], axis=1)
df = df.set_index(&#39;日期时间&#39;)
features = [&#39;温度(°C)&#39;, &#39;风速(m/s)&#39;, &#39;SolarIrrad(W/m2)&#39;]
目标 = &#39;太阳能光伏发电（瓦）&#39;

train_size = int(len(df) * 0.8)
训练，测试 = df.iloc[:train_size], df.iloc[train_size:]

定标器=标准定标器()
scaler.fit(训练[特征])
train_scaled = scaler.transform(train[特征])
test_scaled = scaler.transform(测试[特征])

def create_sequences(数据, target_index, seq_length):
    X、y = []、[]
    对于范围内的 i(len(data)-seq_length)：
        序列=数据[i:(i+seq_length)]
        目标值 = 数据[i+seq_length, 0]
        # 将序列和目标附加到列表中
        X.append(序列)
        y.append(目标值)
    # 将列表转换为 numpy 数组
    X = np.array(X)
    y = np.array(y)
    返回 X, y
序列长度 = 4

X_train, y_train = create_sequences(train_scaled, 目标, seq_length)
X_test, y_test = create_sequences(test_scaled, 目标, seq_length)

模型=顺序（）
model.add(LSTM(单位=50，激活=&#39;relu&#39;，input_shape=(X_train.shape[1]，X_train.shape[2])))
model.add(密集(单位=1))
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;)

Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

历史= model.fit（X_train，y_train，epochs = 50，batch_size = 32，validation_split = 0.2，callbacks = [early_stopping]，verbose = 1）

y_pred = model.predict(X_test)

concatenated_pred = np.concatenate([X_test_last_col, y_pred_last_col], axis=-1)

错误：
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-43-cc4afc30ab89&gt;在&lt;细胞系：6&gt;()
4
5 # 沿最后一个轴连接
6 concatenated_pred = np.concatenate([X_test_last_col, y_pred_last_col], axis=-1)
7
8 # 逆变换
/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py in concatenate(*args, **kwargs)
ValueError：所有输入数组必须具有相同的维数，但索引 0 处的数组有 3 个维度，索引 1 处的数组有 2 个维度

我期待它成功地进行逆变换]]></description>
      <guid>https://stackoverflow.com/questions/77806320/failure-to-perform-inverse-transform-of-scaled-data-with-4-key-columns-and-an-in</guid>
      <pubDate>Fri, 12 Jan 2024 11:43:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在时间序列 LSTM 模型中将数据拆分为训练集、验证集和测试集？</title>
      <link>https://stackoverflow.com/questions/77806128/how-to-split-my-data-into-training-validation-and-testing-sets-in-a-time-series</link>
      <description><![CDATA[我遇到了将数据拆分为训练/验证/测试集的不同策略（嵌套、K-Fold 等），但我不确定该选择哪一种。我想做的是从海面无人机的运动中估计波浪参数。无人机在不同设置下遵循特定的轨迹。
我不知道进行多步骤验证是否与我的案例相关。我想检查我的模型是否过度拟合，是否有可能扩充我的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/77806128/how-to-split-my-data-into-training-validation-and-testing-sets-in-a-time-series</guid>
      <pubDate>Fri, 12 Jan 2024 11:09:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Roberta 计算单词和句子嵌入？</title>
      <link>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</link>
      <description><![CDATA[我正在尝试使用 Roberta 计算单词和句子嵌入，对于单词嵌入，我从 RobertaModel 类中提取最后一个隐藏状态 outputs[0]，但是我不确定这是否是正确的计算方法。
至于句子嵌入，我不知道如何计算它们，这是我尝试过的代码：
从 Transformers 导入 RobertaModel、RobertaTokenizer
进口火炬

模型 = RobertaModel.from_pretrained(&#39;roberta-base&#39;)
tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
Captions = [“示例标题”、“lorem ipsum”、“这只鸟是黄色的，有红色翅膀”、“嗨”、“示例”]

encoded_captions = [tokenizer.encode(caption) 用于字幕中的字幕]

# 用 0 将序列填充到相同的长度
max_len = max(len(seq) 用于编码字幕中的 seq)
padded_captions = [seq + [0] * (max_len - len(seq)) 对于encoded_captions中的seq]

# 转换为批量大小为 5 的 PyTorch 张量
input_ids = torch.tensor(padded_captions)

输出=模型(input_ids)
word_embedding = 输出[0].连续()
句子嵌入 = ??????

如何使用 Roberta 计算单词和句子嵌入？]]></description>
      <guid>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</guid>
      <pubDate>Fri, 12 Jan 2024 10:05:01 GMT</pubDate>
    </item>
    <item>
      <title>使用列表存储的梯度下降导致多变量优化图像处理算法中的除零误差</title>
      <link>https://stackoverflow.com/questions/77805644/gradient-descent-using-list-storage-resulting-in-division-by-zero-error-in-image</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77805644/gradient-descent-using-list-storage-resulting-in-division-by-zero-error-in-image</guid>
      <pubDate>Fri, 12 Jan 2024 09:39:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 tSNE 后在 MNIST 数据集上应用 KMeans [关闭]</title>
      <link>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</link>
      <description><![CDATA[我在 MNIST 数据集上使用 tSNE，并得到了非常好的结果（当我可视化该图时，所有 10 个标签都分离得很好）。
现在，我想对从 tSNE 建模获得的数据应用 KMeans，并再次将其可视化。
不幸的是，这次我得到了非常糟糕的结果 - 集群看起来非常错误。
我知道 t-SNE 空间中的点之间的距离和关系可能不一定保留原始高维空间中存在的结构，当将 KMeans 等聚类算法直接应用于 t 时，这可能会导致误导性的解释。 -SNE嵌入。
还有什么我可以做得更好的吗？
代码示例：
tsne = TSNE(n_components=2, perplexity=15,learning_rate=200, exaggeration=1)
x_train_tsne = tsne.fit(x_train)

kmeans = KMeans(n_clusters=10, n_init=1, init=&#39;kmeans++&#39;)
labels_kmeans = kmeans.fit_predict(x_train)

对于 np.unique(labels_kmeans) 中的数字：
   索引 = (labels_kmeans == 数字)
   plt.scatter(x_train_tsne[索引，0]，x_train_tsne[索引，1]，s=5，alpha=0.8，标签=str(数字))
]]></description>
      <guid>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</guid>
      <pubDate>Fri, 12 Jan 2024 07:35:51 GMT</pubDate>
    </item>
    <item>
      <title>我想通过上传图像来查找“车辆品牌和型号”[关闭]</title>
      <link>https://stackoverflow.com/questions/77804868/i-want-to-find-vehicle-make-and-model-by-uploading-the-image</link>
      <description><![CDATA[我想创建一个Python模型来在上传车辆图像后识别车辆的品牌和型号。
导入deeplake
从张量流导入keras
从tensorflow.keras导入层

加载训练和测试子集
train_dataset = deeplake.load(“hub://activeloop/stanford-cars-train”)
test_dataset = deeplake.load(“hub://activeloop/stanford-cars-test”)

创建 TensorFlow 数据加载器
train_dataloader = train_dataset.tensorflow()
test_dataloader = test_dataset.tensorflow()

定义常量
image_height, image_width, num_channels = (224, 224, 3) # 根据您的数据集进行调整
num_classes = 196 # 数据集中的汽车类别数量

假设您的数据集有一个“images”键
input_layer=layers.Input(shape=(image_height, image_width, num_channels), name=&#39;images&#39;)
x = 层.Conv2D(32, (3, 3), 激活=&#39;relu&#39;)(input_layer)
x = 层数.MaxPooling2D((2, 2))(x)
x = 层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;)(x)
x = 层数.MaxPooling2D((2, 2))(x)
x = 层.Conv2D(128, (3, 3), 激活=&#39;relu&#39;)(x)
x = 层.Flatten()(x)
x = 层.Dense(256, 激活=&#39;relu&#39;)(x)
输出层=层.Dense（num_classes，激活=&#39;softmax&#39;，名称=&#39;car_models&#39;）（x）

模型= keras.Model（输入=输入层，输出=输出层）

编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

指定纪元数和其他训练参数
&lt;前&gt;&lt;代码&gt;num_epochs = 10

训练模型
model.fit（train_dataloader，epochs = num_epochs，validation_data = test_dataloader）

在测试集上评估模型
test_loss, test_accuracy = model.evaluate(test_dataloader)
print(f&#39;测试准确度: {test_accuracy * 100:.2f}%&#39;)

对新图像进行预测
假设您有一个新图像（将“your_image_path”替换为实际路径）
来自tensorflow.keras.preprocessing导入图像
将 numpy 导入为 np

new_image_path = &#39;/content/bmw2.jpg&#39;
img = image.load_img(new_image_path, target_size=(image_height, image_width))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0) # 添加批量维度
img_array /= 255.0 # 标准化像素值

做出预测
预测 = model.predict(img_array)
Predicted_class = np.argmax(预测[0])

将预测的类别映射到实际的汽车品牌和型号（您可能需要从类别索引到品牌和型号的映射）
class_mapping = {} # 定义类映射
Predicted_make_model = class_mapping.get(predicted_class,“未知”)

print(f&#39;给定图像的预测品牌和型号为：{predicted_make_model}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77804868/i-want-to-find-vehicle-make-and-model-by-uploading-the-image</guid>
      <pubDate>Fri, 12 Jan 2024 06:57:25 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行 K 最近邻的问题</title>
      <link>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</link>
      <description><![CDATA[我有一个示例表，我想对其进行 KKNN 分类。变量 V4 是响应，我希望分类器查看新数据点是否将分类为 0 或 1（实际数据有 12 列，第 12 列是响应，但我仍然会简化示例
库(kknn)

数据 &lt;- data.frame(
  V1=c(1.2,2.5,3.1,4.8,5.2),
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1),
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5),
  V4= c(0, 1, 0, 1, 0)
）

现在，我想使用 for 循环通过 LOOCV 构建 kknn 分类。假设 kknn=3
for (i in 1:nrow(data)) {
  train_data &lt;- 数据[-i, 1:3]
  train_data_response &lt;- data.frame(data[-i, 4])
  colnames(train_data_response) &lt;- “响应”
  test_set &lt;- 数据[i, 3]
  模型 &lt;- kknn(公式=train_data_response ~ ., data.frame(train_data),
                data.frame(test_set)，k=3，scale=TRUE)
}

现在我收到以下错误：
model.frame.default(公式，数据=训练)中的错误：
  变量“train_data_response”的类型（列表）无效

有什么办法可以解决这个错误吗？我认为 kknn 接受矩阵或数据帧。我的训练和测试数据确实是数据框，那么什么给出了？
另外，我是否正确执行了 LOOCV？]]></description>
      <guid>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</guid>
      <pubDate>Fri, 12 Jan 2024 04:15:37 GMT</pubDate>
    </item>
    <item>
      <title>初始化 VAE 权重 [关闭]</title>
      <link>https://stackoverflow.com/questions/77804014/initializing-vae-weights</link>
      <description><![CDATA[我正在训练遵循以下整体架构的 VAE：

变压器编码器
Mu/Logvar -&gt;重新参数化-&gt;潜在z
变压器解码器

根据典型的 VAE 设置，Mu 和 Logvar 只是两个前馈网络。然而，当我用标准值（例如权重为 0.5，偏差为 0）初始化它们时，我发现模型的初始 KL 损失巨大 - 例如5,000-20,000+。
当然，这个下降得相当快，但模型仍然花费数百个时期将 KL 损失从 300 降至 &lt;50。
一个“解决方法”我发现将权重初始化为低得多的值，并使用学习率预热。但初始化权重非常小：
def init_weights(self, initrange=0.0001) -&gt; &gt;没有任何：
        self.embedding_layer.weight.data.uniform_(-0.5, 0.5)
        
        nn.init.uniform_(self.fc_mu.weight, -initrange, initrange)
        nn.init.uniform_(self.fc_logvar.weight, -initrange, initrange)
        nn.init.zeros_(self.fc_mu.bias)
        nn.init.zeros_(self.fc_logvar.bias)

这样做的结果是一个更加稳定的 KL 损失（开始时约为 1.5），但我担心它会阻止我的解码器学习有意义的表示。实际的重建损失因此受到巨大影响。
这是 VAE 的已知问题吗？有什么我可以尝试的特定初始化技巧吗？或者也许我应该从正常值开始，让模型训练的时间明显更长？]]></description>
      <guid>https://stackoverflow.com/questions/77804014/initializing-vae-weights</guid>
      <pubDate>Fri, 12 Jan 2024 02:14:48 GMT</pubDate>
    </item>
    <item>
      <title>联邦学习全局聚合后准确率下降</title>
      <link>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</link>
      <description><![CDATA[我正在开展一个联合学习项目。我编写了一段代码来刺激联邦学习的过程。然而，每次迭代进行全局聚合后，全局模型的测试精度会下降很多，并且在接下来的迭代中保持不变。我使用的聚合算法是FedAvg。我尝试将我的代码分成不同的单元来找出问题所在。
对于本地训练，所选客户训练 3 轮。在这个实验中，将选择所有五个客户端进行训练和聚合，我用于本地的模型是从 torchvision 分叉的 vgg16，数据集是 MNIST，并以 i.i.d 方式分割每个客户端： 
for id, net_id in enumerate(selected):
    logging.info(“训练所选设备 %s。” % (str(net_id)))
    结果 = Userlists[net_id].train(hparams[&#39;n_local_epochs&#39;])
    logging.info(&#39;&gt;&gt; 局部模型 %d: 局部精度: %f in round %d\n&#39; % (id, result[&#39;local_test_acc&#39;], step+1))

在本地模型聚合之前，我使用全局服务器的测试数据来测试本地模型的准确性，
tesc，conf = Misc.compute_accuracy(Userlists[2].model，test_dl_global，get_confusion_matrix=True，device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.2478966346153846
tesc，conf = Misc.compute_accuracy（Userlists [3] .model，test_dl_global，get_confusion_matrix = True，device = hparams [&#39;device&#39;]）
打印（测试）
&gt; 0.14413060897435898
tesc,conf=misc.compute_accuracy(Userlists[4].model,test_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.17387820512820512

我使用下面的聚合代码来聚合所选客户端的权重：
&lt;前&gt;&lt;代码&gt;total_sum = 0.0
对于选定的 client_idx：
    Total_sum += 用户列表[client_idx].data_len
    
    
global_para = global_model.state_dict()
client_weights = [torch.tensor( Userlists[client_idx].data_len/total_sum, device=hparams[&#39;device&#39;]) for client_idx in selected]

使用 torch.no_grad()：
    对于顺序，枚举中的 idx（选定）：
        logging.info(f“对于客户端 {idx}”)
        net_para = Userlists[idx].model.state_dict()
        
        如果订单 == 0：
            对于 net_para.keys() 中的键：
                global_para[key] = net_para[key] * client_weights[订单]
        别的：
            对于 net_para.keys() 中的键：
                global_para[key] += net_para[key] * client_weights[订单]


global_model.load_state_dict(global_para)
tesc,conf=misc.compute_accuracy(global_model,train_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])

全局测试精度下降并保持不变
&lt;前&gt;&lt;代码&gt;&gt; 0.11236666666666667

虽然我尝试增加本地训练的纪元，局部准确率提高到 40%，但全局准确率仍然落入与之前相同的值。我的聚合代码中是否有错误的地方？
测试精度应与本地精度保持在同一水平。]]></description>
      <guid>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</guid>
      <pubDate>Thu, 11 Jan 2024 06:30:41 GMT</pubDate>
    </item>
    <item>
      <title>在 Sagemaker 中部署 dolly2 模型进行嵌入，但在调用端点时收到 400 错误</title>
      <link>https://stackoverflow.com/questions/76380739/deployed-dolly2-model-in-sagemaker-for-embeddings-but-receiving-a-400-error-whe</link>
      <description><![CDATA[我已经在 sagemaker 中部署了 dolly2 模型，并且正在尝试创建一些用于嵌入的向量，该代码对于文本生成来说效果很好，但是在更改 inference.py 来处理嵌入之后，我收到以下错误
otocore.errorfactory.ModelError：调用 InvokeEndpoint 操作时发生错误 (ModelError)：从主服务器收到客户端错误 (400)，消息为“{
  “代码”：400，
  “类型”：“InternalServerException”，
  &quot;message&quot;: &quot;(\&quot;您需要定义以下之一 [\u0027audio-classification\u0027, \u0027automatic-speech-recognition\u0027, \u0027feature-extraction\u0027, \u0027text-classification\u0027, \u0027标记分类\u0027、\u0027问答\u0027、\u0027表格问答\u0027、\u0027视觉问答\u0027、\u0027文档问答\u0027、\u0027填充掩码\u0027、\u0027摘要\u0027、\u0027翻译\u0027、\u0027text2文本生成\u0027、\u0027文本生成\u0027、\u0027零样本分类\u0027、\u0027零样本图像分类\u0027、\u0027会话\u0027、\u0027图像-分类\u0027、\u0027图像分割\u0027、\u0027图像到文本\u0027、\u0027对象检测\u0027、\u0027零镜头对象检测\u0027、\u0027深度估计\u0027、\u0027视频分类\ u0027] 作为环境 \u0027HF_TASK\u0027.\&quot;, 403)&quot;
}

下面您还可以看到我用于嵌入的代码
导入json
导入操作系统
导入boto3
从变压器进口管道


def invoke_sagemaker_endpoint():
    # 创建 SageMaker 客户端
    sagemaker_client = boto3.client(“sagemaker-runtime”)

    # 定义端点名称和负载
    endpoint_name = &#39;XXX&#39; # 替换为您的 SageMaker 端点名称
    Payload = {“inputs”：“这是一个大文档。”} # 按照模型的预期更新有效负载格式

    # 将请求发送到 SageMaker 端点
    响应= sagemaker_client.invoke_endpoint（
        端点名称=端点名称，
        ContentType =“应用程序/json”，
        正文=json.dumps(有效负载),
    ）

    # 解析响应并提取嵌入向量
    response_body = response[“Body”].read().decode(“utf-8”)
    response_json = json.loads(response_body)

    如果“嵌入”是在response_json中：
        嵌入 = response_json[“嵌入”]
        embeddings_vector = embeddings[0] # 嵌入作为列表返回
        返回嵌入向量
    别的：
        返回无


如果 __name__ == “__main__”：
    # 将嵌入的 HF_TASK 环境变量设置为“feature-extraction”
    os.environ[&quot;HF_TASK&quot;] = &quot;特征提取&quot;
    # 调用 SageMaker 端点
    embeddings_vector = invoke_sagemaker_endpoint()

    如果嵌入向量：
        打印（嵌入向量）
    别的：
        print(“响应中未发现嵌入。”)



和 inference.py
&lt;前&gt;&lt;代码&gt;
进口火炬
从变压器进口管道

def model_fn(model_dir):
    模型=管道（
        “文本生成”，
        模型=模型目录，
        torch_dtype=torch.bfloat16,
        trust_remote_code=真，
        device_map=“自动”，
        model_kwargs={“load_in_8bit”：True}，
    ）
    分词器=模型.分词器
    embeddings_model = 模型.model

    defgenerate_embeddings（输入）：
        输入 = tokenizer(输入、截断=True、填充=“最长”、return_tensors=“pt”)
        使用 torch.no_grad()：
            输出 = embeddings_model(**输入)
        嵌入=outputs.last_hidden_​​state.mean(dim=1).squeeze(0).tolist()
        返回嵌入

    defretrieve_qa（问题，上下文）：
        输入 = {“问题”：问题，“上下文”：上下文}
        qa_outputs = 模型（问题，上下文）
        返回 qa_outputs

    返回模型、generate_embeddings、retrieve_qa

更改了推理，将 HF 重新部署到 sagemaker，从 api 网关而不是 sagemaker 端点调用]]></description>
      <guid>https://stackoverflow.com/questions/76380739/deployed-dolly2-model-in-sagemaker-for-embeddings-but-receiving-a-400-error-whe</guid>
      <pubDate>Thu, 01 Jun 2023 09:59:24 GMT</pubDate>
    </item>
    <item>
      <title>sklearn LogisticRegressionCV 是否使用最终模型的所有数据</title>
      <link>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</link>
      <description><![CDATA[我想知道sklearn中LogisticRegressionCV的最终模型（即决策边界）是如何计算的。假设我有一些 Xdata 和 ylabels，这样
Xdata # 形状为 (n_samples,n_features)
ylabels # 形状是 (n_samples,)，它是二进制的

现在我跑步
从 sklearn.linear_model 导入 LogisticRegressionCV
clf = LogisticRegressionCV(Cs=[1.0],cv=5)
clf.fit(Xdata,ylabels)

这仅考虑一个正则化参数和 CV 中的 5 个折叠。因此，clf.scores_ 将是一个字典，其中一个键的值是一个形状为 (n_folds,1) 的数组。通过这五次折叠，您可以更好地了解模型的表现。
但是，我对从 clf.coef_ 获得的内容感到困惑（我假设 clf.coef_ 中的参数是 clf.coef_ 中使用的参数&gt;clf.predict）。我认为可能有几个选择：

clf.coef_ 中的参数来自在所有数据上训练模型
clf.coef_ 中的参数来自最佳评分折叠
clf.coef_ 中的参数以某种方式对折叠进行平均。

我想这是一个重复的问题，但在我的一生中，我无法在网上、sklearn 文档或 LogisticRegressionCV 的源代码中找到简单的答案。我发现的一些相关帖子是：

GridSearchCV 最终模型
scikit-learn LogisticRegressionCV：最佳系数
在 sklearn 中使用交叉验证和 AUC-ROC 建立逻辑回归模型
通过交叉验证评估 Logistic 回归
]]></description>
      <guid>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</guid>
      <pubDate>Mon, 13 Aug 2018 21:06:28 GMT</pubDate>
    </item>
    </channel>
</rss>