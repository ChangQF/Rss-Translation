<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 17 Jul 2024 09:18:35 GMT</lastBuildDate>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
有人知道为什么使用嵌入时没有出现此错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我构建我的二元分类项目[关闭]</title>
      <link>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</link>
      <description><![CDATA[我正在开发一个二元分类项目。最初，我得到了一个包含 3290 行和 15 列的真实数据的数据集。然后，我使用 CTGAN 网络生成了包含 100000 行的合成数据集。然后，我将这两个数据集混洗，得到 1 个数据集。我的目标变量高度不平衡（是：23175，否：76825）。我对我的项目有以下问题？

有 7 个分类预测因子，其中有 4 个二元分类列（性别、婚姻状况等），其他是非二元分类变量（省、区等）。我应该使用什么编码技术？

处理这里的数据不平衡问题是否重要？如果重要，我应该使用什么技术来处理这个不平衡问题？

我的数值变量都不是正态分布的。处理这个问题是否重要？如果是，我需要使用哪些技术（例如，如果需要，进行转换）？

我需要标准化或规范化我的数据吗？如果是，为什么？

我应该在这里使用哪些特征选择技术？

我的项目的顺序是什么。请按以下顺序排列。（数据不平衡问题处理/编码分类变量/转换数值数据/标准化或规范化数值数据/特征选择\建模）

最后我可以使用神经网络来实现这一点吗？如果可以，我可以使用哪些 NN 类型


我知道这是一个很长的问题，感谢您花时间和精力回答这些问题。
我期待上述问题的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</guid>
      <pubDate>Wed, 17 Jul 2024 06:57:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 XGBoost 决策树的叶节点上显示预测类标签？</title>
      <link>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</guid>
      <pubDate>Wed, 17 Jul 2024 05:22:10 GMT</pubDate>
    </item>
    <item>
      <title>无法安装 pytorch-forecasting：没有名为“distutils”的模块错误</title>
      <link>https://stackoverflow.com/questions/78757738/unable-to-install-pytorch-forecasting-no-module-named-distutils-error</link>
      <description><![CDATA[当我尝试在 VS Code 终端中运行“pip install pytorch-forecasting”时，我收到错误“没有名为‘distutils’的模块”。
首先，PyTorch 已经安装，并且使用 conda install pytorch-forecasting pytorch&gt;=1.7 -c pytorch -c conda-forge 在 Anaconda 上安装了 pytorch-forecasting。
我已经看到了“pip install setuptools”的解决方案。我在我的终端上成功运行了安装 setuptools 的命令，但它并没有消除“没有名为‘distutils’的模块”错误。
我需要帮助来绕过“没有名为‘distutils’的模块”错误，以便我可以安装 pytorch-forecasting 包。]]></description>
      <guid>https://stackoverflow.com/questions/78757738/unable-to-install-pytorch-forecasting-no-module-named-distutils-error</guid>
      <pubDate>Wed, 17 Jul 2024 05:09:49 GMT</pubDate>
    </item>
    <item>
      <title>如何将多头自注意力输出的形状更改为可以馈送到卷积层的形状？</title>
      <link>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</link>
      <description><![CDATA[我遇到了这样的错误：
MHSA（多头自注意力）的输出如下：
torch.Size([20, 197, 768])


批次大小为 20
序列长度为 197（之前为 196，添加类标记后变为 197）
嵌入维度为 768

我想将其重塑以适应以下格式，以便将其馈送到卷积层：
torch.Size([batch_size, channel, width, height])

我尝试通过使用以下方法添加新维度来实现此目的方法：
torch.unsqueeze(1)
torch.transpose(1, 3)

这成功地允许馈送到卷积层。但是，我不确定这种方法是否正确，如果不正确，请纠正我。
目前，我正在尝试一种不同的方法：
new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)

这导致错误，指出形状对于大小为 (some_number) 的输入无效。这是因为序列长度（197）不是完全平方的，得出的是一个十进制数，而视图函数需要输入一个整数，平方运算在转换为整数后得出 16，但 batch_size * 768 * 16 * 16 不等于 batch_size * 197 * 768，导致错误
我的分析正确吗？我该如何解决这个问题？还有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</guid>
      <pubDate>Wed, 17 Jul 2024 00:24:06 GMT</pubDate>
    </item>
    <item>
      <title>需要什么类型的相机或计算机视觉硬件/软件来跟踪寻找特定运动的人[关闭]</title>
      <link>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</link>
      <description><![CDATA[我需要找到一个摄像头和相应的计算机视觉技术，可以跟踪人类直到做出特定动作，在这种情况下它将触发另一个系统事件。例如，这个摄像头应该检测其框架内的人的存在，如果人坐下，它将告诉触发连接到摄像头系统的 LED 以打开。任何想法或指导都值得赞赏。
我研究过像 arduinos Nicla Vision 这样的计算机视觉系统，但并不认为它有能力检测到这种变化。]]></description>
      <guid>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</guid>
      <pubDate>Tue, 16 Jul 2024 23:02:38 GMT</pubDate>
    </item>
    <item>
      <title>当已知目标值的特征向量时，如何使用监督式机器学习进行时间序列预测？</title>
      <link>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</link>
      <description><![CDATA[我尝试使用 LSTM 预测仪器的连续“偏移”校准值。这些偏移值之前已被证明与用作特征的一对温度值有很好的相关性。这些偏移值显示出周期性，因此为模型选择了 LSTM。但是，我发现的所有 LSTM 示例都使用来自先前数据点序列的特征向量来预测目标。我觉得这可能无法捕捉序列中每个特征向量与其偏移之间的关系。而且由于在预测中只使用了先前数据点序列的特征向量，因此无法有效地利用该温度偏移关系。
为了解决这个问题，我已经将当前数据点的特征向量添加到用于训练模型的向量序列中，当然，也添加到用于预测目标偏移的序列中。
但是，我如何才能对要预测的偏移的特征向量赋予更大的权重，以使模型能够利用温度偏移关系？
谢谢！ 🖖
创建序列的代码如下所示：
def create_sequences(x, y, time_steps = 24):
xs, ys = [], []
for i in range(len(x) - time_steps - 1):
x_temp = x[i:(i + time_steps + 1)] #在目标值之前创建一个 time_steps 序列
xs.append(x_temp)
ys.append(y[i + time_steps]) #目标值是序列之后的值
return np.array(xs), np.array(ys)
]]></description>
      <guid>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</guid>
      <pubDate>Tue, 16 Jul 2024 22:49:31 GMT</pubDate>
    </item>
    <item>
      <title>词到词机器翻译的评估指标/算法</title>
      <link>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</link>
      <description><![CDATA[我正在寻找一种科学合理的方法来评估我的学士论文的单词翻译。在寻找的过程中，我发现大多数方法都是基于 n-gram 精度来进行句子评估。我正在将数据库列和表名从英语翻译成德语。我可以选择包含基本事实，但我更喜欢不包含它的方法。我的翻译和原文也包含缩写。
现在我使用 OpenAIs 最新的嵌入模型，然后计算嵌入之间的距离。但结果很大程度上取决于模型。我研究过使用字符 n-gram 的 ChrF。但如上所述，不需要基本事实的方法会更理想。]]></description>
      <guid>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</guid>
      <pubDate>Tue, 16 Jul 2024 19:42:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 GCP TPU 进行多处理，但 shell 意外死机</title>
      <link>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</link>
      <description><![CDATA[我可以访问美国地区的 32 个可抢占 Cloud TPU v4 芯片，并尝试使用以下 Python 代码运行我的 PyTorch 模型：
import os 
import sys 
import pickle 
import torch_xla.distributed.xla_multiprocessing as xmp 
from transformers 
import AutoTokenizer 
import main_utils as utils 
import multiprocessing as mp

lock = mp.Manager().Lock()

def _mp_fn(i): 

A_tasks, B_tasks, desire_output_lengths, keys = utils.get_total_tasks()

import torch
import torch_xla.core.xla_model as xm
from models_mamba import MambaForCausalLM

DEVICE_NAME = xm.xla_device()
tokenizer = AutoTokenizer.from_pretrained(&quot;~/Mamba-1B/&quot;)
model = MambaForCausalLM.from_pretrained(&quot;~/Mamba-1B/&quot;, torch_dtype=&quot;auto&quot;,
device_map=&quot;auto&quot;, low_cpu_mem_usage=True)
model = model.to_empty(device=&#39;cpu&#39;)
model.apply(lambda module: module.reset_parameters() if hasattr(module, &#39;reset_parameters&#39;) else None)
model = model.to(xm.xla_device())

params_to_save = [&quot;out_proj_y&quot;]

def generate_logits(task, logits_list, desire_output_length):
for i in range(48):
layer_to_save = [i + 1]
input_ids = tokenizer.encode(task, return_tensors=&quot;pt&quot;).to(设备名称)
model.saved_activation(params_to_save, layer_to_save, precision=&#39;r&#39;)
output = model.generate(input_ids, max_length=desired_output_length, no_repeat_ngram_size=2)

saved_dict = model.reset_everything_and_save()
param = saved_dict[f&quot;out_proj_y_{i + 1}&quot;][0, -1, :].to(设备名称)

xm.all_gather(param)
logits = model.get_unembed_for_layer(param, norm=&quot;False&quot;)

xm.all_gather(logits)
logits_list.append(logits.to(&quot;cpu&quot;))

del input_ids、output、saved_dict、param、logits

task_dict = {}
for A_task、B_task、dol、key in zip(A_tasks、B_tasks、desired_output_lengths、keys):
A_logits = []
B_logits = []
generate_logits(A_task、A_logits、dol)
generate_logits(B_task、B_logits、dol)

aux_dict = {&#39;A&#39;: A_logits, &#39;B&#39;: B_logits}
task_dict[key] = aux_dict

device = xm.xla_device()
with lock:
print(f&#39;Process {i}, Device {device}&#39;)

xm.mark_step()
with open(f&#39;~/main_file.pickle&#39;, &#39;wb&#39;) as handle:
pickle.dump(task_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

if name == &quot;main&quot;: 
xmp.spawn(_mp_fn, args=(), nprocs=8, start_method=&#39;fork&#39;)`

要执行此代码，我使用 Cloud Shell 和以下命令（设置 TPU 并在所有 8 个工作器上安装库之后）：
gcloud compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE} --project=${PROJECT_ID} --worker=all --command=&quot;PJRT_DEVICE=TPU python3 ~test.py&quot;
我希望模型在所有 TPU 上执行 _mp_fn() 中定义的任务，按照指定的方式收集和保存 logit，最后将结果存储在 main_file.pickle 中。但是，在 Cloud Shell 中启动命令后，shell 在模型加载期间打印了一些预期的警告，但没有继续进行。相反，过了一段时间，shell 会话意外终止。我不确定如何排除故障？]]></description>
      <guid>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</guid>
      <pubDate>Tue, 16 Jul 2024 17:38:54 GMT</pubDate>
    </item>
    <item>
      <title>我如何访问这些元素？[关闭]</title>
      <link>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</link>
      <description><![CDATA[LIVEKIT_URL= 

LIVEKIT_API_KEY= 

LIVEKIT_API_SECRET= 

DEEPGRAM_API_KEY= 

在此处输入图片描述 来自我的帐户的与项目相关的屏幕截图
页面顶部的 URL 是吗？我无法在 LIVEKIT 上创建 API 密钥。
这是与项目相关的 YouTube 链接。
https://www.youtube.com/watch?v=nvmV0a2geaQ]]></description>
      <guid>https://stackoverflow.com/questions/78755424/how-can-i-access-to-these-elements</guid>
      <pubDate>Tue, 16 Jul 2024 15:20:52 GMT</pubDate>
    </item>
    <item>
      <title>使用张量流的问题（既不包含“saved_model.pb”也不包含“saved_model.pbtxt”）</title>
      <link>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</link>
      <description><![CDATA[我尝试运行代码，但出现错误。
ValueError：尝试加载不兼容/未知类型的模型。&#39;C:\Users\pm23821\AppData\Local\Temp\tfhub_modules\9616fd04ec2360621642ef9455b84f4b668e219e&#39; 既不包含 &#39;saved_model.pb&#39; 也不包含 &#39;saved_model.pbtxt&#39;

这是我第一次使用 tensorflow 和模型，所以我不知道我需要做什么才能运行我的代码。
目标是找到声音之间的差异，当检测到紧急警报器（警察、消防部门和医院）时，它需要看起来像已被识别一样。当听到家用警报器或鹦鹉的声音时，应该看起来它还没有被识别。
# 加载 YAMNet 模型
model = hub.load(&#39;https://tfhub.dev/google/yamnet/1&#39;)

# 加载 YAMNet 中的类名
def load_class_names(csv_path=&#39;yamnet_class_map.csv&#39;):
class_map = pd.read_csv(csv_path, sep=&#39;,&#39;, header=None)
class_names = class_map[1].tolist()
return class_names

class_names = load_class_names()

# 麦克风音频捕获函数
def record_audio(duration=5, fs=16000): # 录制时长为 5 秒
recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=&#39;float32&#39;)
sd.wait()
return recording.flatten()

# 预测音频类的函数
def predict_sound(audio_data):
scores, embeddings, spectrogram = model(audio_data)
prediction = np.argmax(scores, axis=1)[0]
return class_names[prediction]

# 循环主函数
while True:
print(&quot;Escutando...&quot;)
audio_data = record_audio()
audio_data /= np.max(np.abs(audio_data))

predict_class = predict_sound(audio_data)
print(&quot;预测的类：&quot;, predict_class)

if “/m/012n7d”在 Predicted_class 或“/m/03j1ly”中在 Predicted_class 或“/m/03kmc9”中in Predicted_class: print(&quot;Sirene detectorda!&quot;) else: print(&quot;Não é uma Sirene.&quot;) time.sleep(1) # Pausa de 1 segundo antes da próxima gravação ]]></description>
      <guid>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</guid>
      <pubDate>Tue, 16 Jul 2024 14:11:25 GMT</pubDate>
    </item>
    <item>
      <title>尝试在多 GPU 设置上训练机器翻译的 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</link>
      <description><![CDATA[我正在尝试训练机器翻译的变换模型。这是训练函数：
在单个 GPU 上训练时，我得到以下结果：
for src, tgt in train_dataloader:
try:
src = src.to(DEVICE)
tgt = tgt.to(DEVICE)

tgt_input = tgt[:-1, :]

src_mask, tgt_mask, \
src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

logits = model( src, tgt_input,
src_mask, tgt_mask,
src_padding_mask, tgt_padding_mask,
src_padding_mask
)

在Seq2SeqTransformer:
class Seq2SeqTransformer(nn.Module):
def forward( self,
src: Tensor,
trg: Tensor,
src_mask: Tensor,
tgt_mask: Tensor,
src_padding_mask: Tensor,
tgt_padding_mask: Tensor,
memory_key_padding_mask: Tensor ):

src_emb = self.positional_encoding(self.src_tok_emb(src))
tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )
return self.generator(outs)

在此行：
outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )

我明白了：
&gt; sec_emb 的大小为 (31,31) tgt_emb 的大小为 (37,37)

训练运行顺利。
现在，我尝试使用以下设置在具有 8 个 GPU 的计算机上进行训练：
 transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

在调试模式下，我检查此行中的值：
 outs = self.transformer( src_emb, tgt_emb,
src_mask, tgt_mask,
None,
src_padding_mask,
tgt_padding_mask,
memory_key_padding_mask )


我明白了：
&gt; sec_emb 的大小为 (4,31) tgt_emb 的大小为 (5,37)

我认为这是因为事情是并行进行的。
但是，我遇到了此错误消息：
&gt; 文件
&gt; &quot;C:\Projects\MT005\.venv\Lib\site-packages\torch\nn\ functional.py&quot;,
&gt; 第 5382 行，在 multi_head_attention_forward 中
&gt;引发 RuntimeError(f&quot;2D attn_mask 的形状为 {attn_mask.shape}，但应为 {correct_2d_size}。&quot;) RuntimeError:
&gt; 2D attn_mask 的形状为 torch.Size([4, 31])，但应为
&gt; (4, 4)。

有人可以帮助我或提供一些指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</guid>
      <pubDate>Tue, 16 Jul 2024 12:00:27 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False), Dense(64,activation=&#39;relu&#39;), Dense(1) ]) # 模型编译器 model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差 # 模型摘要 model.summary() # 模型模型嵌入 model.fit( X_train, Y_train, epochs=40, batch_size=32,validation_data=(X_test, Y_test)) # 新预测示例预测 = model.predict(X_test) print(prediction)  结果： 模型：“顺序”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 的数量而改善。 这就是问题所在，我不明白发生了什么。
我是深度学习的初学者，所以也许我犯了一个非常简单的错误。 但是我仔细检查了我的数据，X_train 已标准化，我尝试在我的模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。 我真的想尝试找到一种方法来使用深度学习来找到它。 我 d]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOv6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有以下输出，但无法弄清楚如何评估，因为没有 F1 分数 或 混淆矩阵。
平均召回率 (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

平均召回率 (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

平均召回率 (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我训练了 400 个 epoch，这只是输出的一小部分。我也看不到 mAP。
我有这行代码要评估
!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0

有没有办法获得详细的评估指标，例如 F1 分数、混淆矩阵 和 mAP？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>获取“model.fit”keras API 参数的值</title>
      <link>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</link>
      <description><![CDATA[我正在尝试使用自定义回调函数获取 Kera 顺序模型的详细信息。我需要提取 model.fit() API 中设置的参数的所有值，例如 batch_size、epochs、validation_split 等。但我无法在 Keras 的回调中访问它们。您知道如何自动获取这些值吗？
我使用的是 Python 3.10 和 Keras 2.8。]]></description>
      <guid>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</guid>
      <pubDate>Thu, 30 Nov 2023 20:13:45 GMT</pubDate>
    </item>
    </channel>
</rss>