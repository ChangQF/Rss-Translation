<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 21 Jun 2024 15:20:22 GMT</lastBuildDate>
    <item>
      <title>Bindsnet batchsize>1 准确率</title>
      <link>https://stackoverflow.com/questions/78652942/bindsnet-batchsize1-accuracy</link>
      <description><![CDATA[当我尝试增加batchsize时，我的网络的准确率明显低于batchsize=1。我应该怎么做才能提高准确率？
我在bindsnet中使用了DiehlAndCook2015网络。它不适用于batchsize&gt;1吗？]]></description>
      <guid>https://stackoverflow.com/questions/78652942/bindsnet-batchsize1-accuracy</guid>
      <pubDate>Fri, 21 Jun 2024 14:32:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何处理“真实”数据和封闭式方程？</title>
      <link>https://stackoverflow.com/questions/78652914/how-to-deal-with-real-data-and-closed-formed-equation-in-machine-learning</link>
      <description><![CDATA[我的目标是对来自“现实世界”（传感器）的一组数据进行回归分析。
数据采用表格格式。有 6 个独立特征，其值差异很大（需要缩放）。此外，因变量具有很大的可变性（可能需要缩放？）。
当其中一个变量（假设 X1）的绝对值较低时，初始经典训练会显示预测中的弱点。
专家告诉我，在 X1 较低的这个特定区域中，要预测的值（我们称之为 Y）可以通过线性回归来近似。因此，如果所有其他特征都是恒定的，则 X1 和 Y 具有 Y=a * X1 + b 类型的线性依赖关系。
问题是系数“a”和“b”取决于其他特征 a = f(X2,X3,X4,X5)...
请注意，我有一个表，其中列出了其他 5 个特征的几种组合的系数“a”和“b”。
我想将“物理信息”的线性化集成到训练过程中。但我该怎么做呢？我看过物理信息神经网络，但它们只针对 PDE，而不是像我一样针对闭式方程。
对我来说，一个自然的做法是通过方程在这个区域生成假数据。这会被视为物理信息机器学习吗？我看不出添加假数据和添加试图完成方程的损失之间的区别。
非常感谢您的回答，
祝您有美好的一天！]]></description>
      <guid>https://stackoverflow.com/questions/78652914/how-to-deal-with-real-data-and-closed-formed-equation-in-machine-learning</guid>
      <pubDate>Fri, 21 Jun 2024 14:26:46 GMT</pubDate>
    </item>
    <item>
      <title>解释音乐流派分类器准确率低的原因</title>
      <link>https://stackoverflow.com/questions/78652467/explaining-the-poor-accuracy-of-a-music-genre-classifier</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78652467/explaining-the-poor-accuracy-of-a-music-genre-classifier</guid>
      <pubDate>Fri, 21 Jun 2024 12:51:50 GMT</pubDate>
    </item>
    <item>
      <title>聚类实际上会减少数据集中的行数吗？</title>
      <link>https://stackoverflow.com/questions/78652112/does-clustering-actually-reduce-the-number-of-rows-in-a-dataset</link>
      <description><![CDATA[我正在阅读 Luis G. Serrano 的《grokking Machine Learning》一书，看到了这样一句话：
“看起来聚类和降维没什么相似之处，但实际上，它们并没有太大区别。如果我们有一张满是数据的表，每一行对应一个数据点，每一列对应一个特征。因此，我们可以使用聚类来减少数据集中的行数，使用降维来减少列数。”
我对聚类减少行数的说法有疑问。似乎聚类只是对数据进行分组，而不减少其列数。我错了吗？]]></description>
      <guid>https://stackoverflow.com/questions/78652112/does-clustering-actually-reduce-the-number-of-rows-in-a-dataset</guid>
      <pubDate>Fri, 21 Jun 2024 11:30:08 GMT</pubDate>
    </item>
    <item>
      <title>Swin Transformer 中的线性嵌入层是什么？</title>
      <link>https://stackoverflow.com/questions/78651620/what-is-the-linear-embedding-layer-in-swin-transformer</link>
      <description><![CDATA[我的目标是了解 Swin Transformer 中的线性嵌入层实际上是什么
问题是 Swin Transformer 的论文没有解释什么是线性嵌入层。
arXiv：

Swin Transformer

在此原始值特征上应用线性嵌入层以将其投影到任意维度（表示为 C）。

这是论文中对线性嵌入的唯一解释。


PyTorch 文档：

线性
嵌入

YouTube：

嵌入

StackOverflow：

PyTorch 中 Linear 和 Embedding 的区别

老实说，我不知道从哪里开始]]></description>
      <guid>https://stackoverflow.com/questions/78651620/what-is-the-linear-embedding-layer-in-swin-transformer</guid>
      <pubDate>Fri, 21 Jun 2024 09:54:31 GMT</pubDate>
    </item>
    <item>
      <title>如何知道是否有办法改进这个模型，以及如何知道我是否达到了特定模型的限制[关闭]</title>
      <link>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</link>
      <description><![CDATA[我是数据科学领域的新手，所以当我建立模型时，我不确定我是否已经到达终点，而且我也不知道我还能如何改进模型。这是随机森林分类器，我使用 RandomizeSearchCV 作为参数，最终得到了最高的 73%。此外，我还使用了 class_weight 来平衡类别，我缩放了所有内容并清理了数据，正如您将看到的。我不确定这是否是最好的方法，也不知道在制作模型时 73% 是否足够好，也不知道我是否达到了随机森林分类器的极限。
从 matplotlib 导入 pyplot 作为 plt
从 matplotlib.colors 导入 ListedColormap
导入 pandas 作为 pd
导入 numpy 作为 np
从 sklearn.compose 导入 ColumnTransformer
从 sklearn.model_selection 导入 RandomizedSearchCV、train_test_split
从 sklearn.preprocessing 导入 OneHotEncoder、StandardScaler、LabelEncoder、MinMaxScaler、MaxAbsScaler、RobustScaler
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 classes_report、accuracy_score、confusion_matrix
从 imblearn.over_sampling 导入 SMOTE

pd.set_option(&#39;display.max_rows&#39;, None)
pd.set_option(&#39;display.max_columns&#39;, None)
df = pd.read_csv(&#39;pokemon_data.csv&#39;)

duplicates = df.duplicated()
if duplicates.any():
print(df[duplicates], &quot;DUPLICATE&quot;)
else:
print(&quot;NO DUPLICATES&quot;)

class_dist = df[&#39;type1&#39;].value_counts()
print(class_dist)

df.replace(&#39;—&#39;, np.nan, inplace=True)
numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())
# 现在我们填充分类缺失数据列
categorical_cols = df.select_dtypes(include=&#39;object&#39;).columns
df[categorical_cols] = df[categorical_cols].fillna(&#39;Unknown&#39;)

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), [&#39;dexnum&#39;]),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

features = [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;,
&#39;egg_group2&#39;]

X = df[features] # 我们用来预测的变量 
y_type1 = df[&#39;type1&#39;] # 我们预测的内容
y_type2 = df[&#39;type2&#39;] # 我们预测什么

preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), []),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), [&#39;species&#39;, &#39;ability1&#39;, &#39;ability2&#39;, &#39;egg_group1&#39;, &#39;egg_group2&#39;])
])

# Train_test_split 用于将数据集拆分为两个子集，即训练集和测试集
X_train, X_test, y_type1_train, y_type1_test = train_test_split(X, y_type1, test_size=0.4, random_state=42)

X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

param_dist = {
&#39;n_estimators&#39;: [9000],
&#39;max_depth&#39;: [1000],
&#39;min_samples_split&#39;: [2],
&#39;min_samples_leaf&#39;: [1],
&#39;max_features&#39;: [&#39;log2&#39;],
&#39;bootstrap&#39;: [False]
}

rf_type1 = RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=42)
random_search = RandomizedSearchCV(estimator=rf_type1, param_distributions=param_dist, n_iter=50, cv=5, verbose=2, random_state=42, n_jobs=-1)
random_search.fit(X_train, y_type1_train)

# 获取最佳参数
print(&quot;找到最佳参数：&quot;, random_search.best_params_)
print(&quot;最佳准确率： &quot;, random_search.best_score_)

# 使用最佳参数重新训练模型
best_rf = random_search.best_estimator_
y_type1_pred_best = best_rf.predict(X_test)

# 评估模型
print(&quot;具有最佳参数的 Type1 分类报告：&quot;)
print(classification_report(y_type1_test, y_type1_pred_best))
print(&quot;具有最佳参数的 Type1 准确率：&quot;, accuracy_score(y_type1_test, y_type1_pred_best))

]]></description>
      <guid>https://stackoverflow.com/questions/78650661/how-to-know-if-there-is-a-way-to-improve-this-model-and-also-how-should-i-know-i</guid>
      <pubDate>Fri, 21 Jun 2024 06:07:40 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮助我在使用 pytorch 时最大限度地提高 GPU 利用率吗？</title>
      <link>https://stackoverflow.com/questions/78650444/can-anybody-help-me-out-with-maximizing-gpu-utilization-when-using-pytorch</link>
      <description><![CDATA[我目前使用的是带有 7950X CPU 的 RTX 4090。我的目标是在表格数据上运行相对简单的模型时最大限度地提高 GPU 利用率。该模型的参数少于 100 万个，数据形状为 (5,000,000, 120)。当我训练模型时，只有 18% 的 GPU 被利用，完成训练大约需要 3 个小时。
主要问题是，如果我能以某种方式利用 90% 的 GPU，训练时间将显著减少，可能减少到当前时间的五分之一，这将为我节省大量时间。
我尝试了各种解决方案，例如调整批处理大小、增加模型的复杂性以及更改 DataLoader 的 num_workers，但这些都没有起到很好的作用。无论我如何调整，GPU 负载仍然在 10-15% 左右。这真是令人沮丧。
由此，我想到了使用多处理的想法。由于我使用的是单个 GPU，并且单个模型仅使用 18%，因此我仍有空间运行另​​外四个模型。我认为同时运行五个不同的模型可以将 GPU 利用率提高到 100% 左右，从而节省大量时间。但是，当我尝试使用 PyTorch 的多处理时，结果并不理想。
有人能帮我解决这个问题吗，或者我的想法在 PyTorch 中不可行？
我尝试过的方法

将批次大小从 64 增加到 4096、40962、40964
使用 num_workers (2,4,8)
向模型添加更多层
使用 pytorch.multiprocessing
]]></description>
      <guid>https://stackoverflow.com/questions/78650444/can-anybody-help-me-out-with-maximizing-gpu-utilization-when-using-pytorch</guid>
      <pubDate>Fri, 21 Jun 2024 04:50:59 GMT</pubDate>
    </item>
    <item>
      <title>通过几个步骤优化一个过程：如果我们使用一个模型几次才能够计算出损失，那么如何训练它？</title>
      <link>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</link>
      <description><![CDATA[我正在做一个项目，其中有一个包含一定步骤的流程；假设是 3 个步骤。

我们从一个初始化的零矩阵 M0 开始，该矩阵描述了系统的状态，并且必须采取一个行动，其后果是随机的，但受到该行动的强烈影响
我们更新矩阵 (M1)，然后再次采取行动
我们再次更新矩阵 (M2)，采取最后一个行动
现在我们才可以从最后一个矩阵 M3 计算损失，因此我们可以评估我们的策略

我已经建立了一个神经网络，其中状态矩阵是输入，输出是决定采取哪种行动的权重列表。从我（初学者）的理解来看，整个过程有点像循环神经网络，但有额外的步骤。
我用 Python 实现了这个过程，但当我尝试训练模型时，GradientTape() 看起来不太好，因为我的变量从未计算过梯度；我不确定，但我认为损失的随机性使得梯度不可计算（计算权重的损失远非易事，因为权重放在图的边缘，我们对其执行算法）。
我曾想过在没有权重列表的情况下进行强化学习以采取行动，但我不知道奖励的随机性会有多好。此外，我从来没有做过这样的事情，我只有一周的时间来实现一切。动作空间也相当大，因此这种方法存在很多不确定性。
有没有什么常见的做法来解决这类问题？
谢谢！
最好]]></description>
      <guid>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</guid>
      <pubDate>Fri, 21 Jun 2024 00:18:44 GMT</pubDate>
    </item>
    <item>
      <title>如何去除车窗玻璃 bg？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</link>
      <description><![CDATA[我正在使用 rembg 去除背景，但问题是车窗玻璃的背景没有被去除。在这种情况下，如何去除车窗玻璃的背景。有人能帮帮我吗？
--&gt;当前结果图像
--&gt;预期结果图像
output_data = rembg.remove(input_data)
]]></description>
      <guid>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</guid>
      <pubDate>Thu, 20 Jun 2024 23:37:59 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用训练有素的 YOLO-V8 实例分割模型进行预测时将边界框值添加到标签文本文件中？</title>
      <link>https://stackoverflow.com/questions/78649611/how-to-add-the-bounding-box-values-to-the-labels-text-files-during-prediction-wi</link>
      <description><![CDATA[我训练了一个 YOLO-V8 实例分割模型来分割类标签为 0 的对象。我使用 CLI 实例化训练后的模型并根据测试数据进行预测。
!yolo task=segment mode=predict model=&#39;/weights/best.pt&#39; conf=0.25 source=&#39;/test/images&#39; imgsz=1024 save=True save_txt=True save_conf=True

预测后，标签文件将以 .txt 格式存储。这些标签文件包含类索引，后跟多边形坐标，最后是边界框预测的置信度分数。但是，边界框坐标（即 x 中心、y 中心、宽度、高度）不包含在标签文件中。我还想将这些边界框坐标包含到每个标签文件中，因为我想稍后使用这些边界框坐标进行后期处理。示例标签文件内容如下所示：
0 0.21582 0.0898438 0.214844 0.0908203 0.213867 0.0908203 0.210938 0.09375 0.210938 0.0947266 0.203125 0.102539 0.203125 0.103516 0.201172 0.105469 0.200195 0.105469 0.199219 0.106445 0.199219 0.113281 0.200195 0.114258 0.200195 0.115234 0.203125 0.115234 0.204102 0.116211 0.223633 0.116211 0.224609 0.117188 0.227539 0.117188 0.228516 0.118164 0.230469 0.118164 0.231445 0.119141 0.234375 0.119141 0.235352 0.120117 0.248047 0.120117 0.249023 0.121094 0.251953 0.121094 0.25293 0.12207 0.254883 0.0927734 0.260742 0.0917969 0.256836 0.0917969 0.255859 0.0908203 0.233398 0.0908203 0.232422 0.0898438 0.910849

我没有将预测保存到任何“结果”变量中，并且我只在 CLI 中运行预测。]]></description>
      <guid>https://stackoverflow.com/questions/78649611/how-to-add-the-bounding-box-values-to-the-labels-text-files-during-prediction-wi</guid>
      <pubDate>Thu, 20 Jun 2024 21:15:36 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归器输出</title>
      <link>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</link>
      <description><![CDATA[我有一个非常简单的数据集，其中员工年龄和工作年限作为特征，收入作为标签。要求使用各种回归器预测收入水平，我使用了 4 个：决策树 (DT)、随机森林 (RF)、K-最近邻 (KNN) 和线性回归 (LR)。下表给出了 4 个回归器预测的收入水平。
我的问题是：

为什么 DT 预测的收入水平如此严格（几乎等于数据集的实际收入），而其他回归器似乎运行良好？是因为数据太小，DT 无法训练 / 特征数量太少（只有 2 个特征）还是其他原因？
为什么 DT 给出的结果很荒谬，而 RF 却没有？
DT 的 R^2 和 MSE 分别小于和大于 LR，但 DT 似乎仍然产生了更窄的范围。怎么做？



这里不包括任何代码，因为这更像是一个上下文问题。如果需要，可以提供代码。]]></description>
      <guid>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</guid>
      <pubDate>Thu, 20 Jun 2024 20:59:14 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1
我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot; 变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据在每个结果类别（即阳性/阴性 toxo）中的 n 值是平衡的
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

这给出了以下结果：
(训练)
阴性 阳性 
75.75758 24.24242

和
(测试)
阴性 阳性 
54.54545 45.45455

检查缺失值
anyNA(data_hum_mod)
#确保所有变量都不为零或接近零方差。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类别的子采样（这里是负面结果），如何在 R 中实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>确定哪些变量对 FDA 贡献最大 [关闭]</title>
      <link>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</guid>
      <pubDate>Wed, 19 Jun 2024 20:46:16 GMT</pubDate>
    </item>
    <item>
      <title>用 -99999 替换 nan 值有意义吗？</title>
      <link>https://stackoverflow.com/questions/61049335/does-it-make-sense-to-replace-nan-values-by-99999</link>
      <description><![CDATA[如何用值 -99999 替换数据框中的 nan 值？我在这里找到了它，示例 3：https://www.geeksforgeeks.org/python-pandas-dataframe-replace/ 
df.replace(to_replace = np.nan, value =-99999)
也许 -99999 应该仅表示 -infinite，但此操作背后可能有什么意图？有什么想法或猜测吗？:/
我很感激任何建议！]]></description>
      <guid>https://stackoverflow.com/questions/61049335/does-it-make-sense-to-replace-nan-values-by-99999</guid>
      <pubDate>Sun, 05 Apr 2020 20:51:23 GMT</pubDate>
    </item>
    <item>
      <title>如何计算最佳批次大小？</title>
      <link>https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size</link>
      <description><![CDATA[有时我会遇到一个问题：
分配形状为 (1024, 100, 160) 的张量时发生 OOM

例如
分配形状为 (1024, 100, 160) 的张量时发生 OOM

其中 1024 是我的批处理大小，我不知道其余的是什么。如果我减少批处理大小或模型中的神经元数量，它就会运行良好。
是否有一种通用方法可以根据模型和 GPU 内存计算最佳批处理大小，这样程序就不会崩溃？
简而言之：我希望我的模型的批处理大小尽可能大，这将适合我的 GPU 内存并且不会使程序崩溃。]]></description>
      <guid>https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size</guid>
      <pubDate>Mon, 09 Oct 2017 20:25:09 GMT</pubDate>
    </item>
    </channel>
</rss>