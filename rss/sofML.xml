<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Dec 2024 09:19:35 GMT</lastBuildDate>
    <item>
      <title>文本类别预测 [关闭]</title>
      <link>https://stackoverflow.com/questions/79252250/text-category-prediction</link>
      <description><![CDATA[我需要创建某种方法，以便预测文本的类别。
现在我正在使用这样的 PHP-ML：
 $texts = $queriesService-&gt;getTexts();

foreach($texts as $key =&gt; $text){
$string = preg_replace(&#39;/\s+/&#39;, &#39; &#39;, strip_tags($text[&#39;text&#39;]));
$string = str_replace(&#39;&quot;&#39;, &#39;&#39;, $string);
$samples[] = $string;
$labels[] = $text[&#39;category&#39;]; 
}

$tokenize = new WordTokenizer();
$vectorizer = new TokenCountVectorizer($tokenize);

$vectorizer-&gt;fit($samples);
$vectorizer-&gt;transform($samples);

$transformer = new TfIdfTransformer($samples);
$transformer-&gt;transform($samples);

$classifier = new NaiveBayes();
$classifier-&gt;train($samples, $labels);

$testSamples = [
&#39;关于某些产品的示例文本&#39;,
&#39;这是关于糟糕的服务&#39;,
&#39;由于某种原因电子设备无法正常工作&#39;,
];

$vectorizer-&gt;transform($testSamples);
$transformer-&gt;transform($testSamples);

$predictions = $classifier-&gt;predict($testSamples);

这样运行良好，但问题是 - 它占用了大量内存。所讨论的文本各不相同，从 5 个单词到 200 个单词，大约有 100k 个。
即使训练分类器并将其保存到文件 - 文件大小超过 10GB，因此读取它需要大量内存。有没有更好的方法来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79252250/text-category-prediction</guid>
      <pubDate>Wed, 04 Dec 2024 17:55:51 GMT</pubDate>
    </item>
    <item>
      <title>Python 错误：rv_generic.interval() 缺少 1 个必需的位置参数：“confidence”</title>
      <link>https://stackoverflow.com/questions/79250549/python-error-rv-generic-interval-missing-1-required-positional-argument-con</link>
      <description><![CDATA[我一直在尝试运行下面的代码来使用 t 分布计算上限和下限置信区间，但它一直在主题中抛出错误。代码片段如下：
def trans_threshold(Day):
Tran_Cnt=Tran_Cnt_DF[[&#39;Sample&#39;,Day]].dropna()
Tran_Cnt=Tran_Cnt.astype({&#39;Sample&#39;:&#39;str&#39;})
Tran_Cnt.dtypes
#通过 IQR 查找 Materiality 中的异常值
X_Tran = Tran_Cnt.drop(&#39;Sample&#39;, axis=1)
Tran_arr1 = X_Tran.values
#查找第一个四分位数
Tran_q1= np.quantile(Tran_arr1, 0.25)
# 查找第三个四分位数
Tran_q3 = np.quantile(Tran_arr1, 0.75)
# 查找 iqr 区域
Tran_iqr = Tran_q3-Tran_q1
# 查找上限和下限异常值
Tran_upper_bound = Tran_q3+(1.5*Tran_iqr)
Tran_lower_bound = Tran_q1-(1.5*Tran_iqr)
# 删除异常值
Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
# 使用 t 分布确定实质性限值
Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
loc=np.mean(Tran_arr2),
scale=st.sem(Tran_arr2))
return Tran_Threshold_mat

trn_lim_FullFeed_Mon = trans_threshold(Day) 

-------------------------------------------------------------------------------------------
TypeError Traceback (最近一次调用最后一次)
Cell In[106]，第 19 行
17 Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
18 #使用 t 分布计算实质性限值
---&gt; 19 Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
20 loc=np.mean(Tran_arr2),
21 scale=st.sem(Tran_arr2))

TypeError: rv_generic.interval() 缺少 1 个必需的位置参数：&#39;confidence&#39;

问题似乎出在下面的代码片段上。但是，我提供了计算置信区间所需的所有参数，包括自由度，但仍然出现此错误。我哪里做错了，需要做什么？
Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
loc=np.mean(Tran_arr2),
scale=st.sem(Tran_arr2))

此外，Tran_arr2 列表如下所示：
array([12617., 12000., 1123., 537., 8605., 4365., 11292., 12231.,
7640., 9583., 9257., 13864., 14682., 11744., 10501., 8694.,
5327., 10066., 13022., 11092., 7444., 11658., 14920., 12849.,
14681., 5719., 11029., 3814., 14703., 5593., 9772., 8851.,
9551., 15975., 6532., 13827., 8547.])

因此，直到代码块的最后一个使用 t 分布估计置信区间的代码之前，都没有问题。
我使用了以下包：
import pandas as pd
import numpy 作为 np
导入 scipy.stats 作为 st
导入 matplotlib.pyplot 作为 plt
导入 matplotlib.ticker 作为 tkr
导入 matplotlib.scale 作为 mscale
从 matplotlib.ticker 导入 FixedLocator、NullFormatter
pd.options.display.float_format = &#39;{:.0f}&#39;.format
pd.options.mode.chained_assignment = None
]]></description>
      <guid>https://stackoverflow.com/questions/79250549/python-error-rv-generic-interval-missing-1-required-positional-argument-con</guid>
      <pubDate>Wed, 04 Dec 2024 09:32:13 GMT</pubDate>
    </item>
    <item>
      <title>想要构建一个验证码求解器但不知道如何做？[关闭]</title>
      <link>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</link>
      <description><![CDATA[我有一个来自我大学网站的 1000 张带标签的 CAPTCHA 图像数据集，我想训练一个模型，该模型可以在未见过的数据上准确解决类似的 CAPTCHA。CAPTCHA 通常由 6 个字母数字字符（A-Z、0-9）组成。尽管尝试了几种方法，但该模型仍无法实现高精度。
数据集：

样本数量：1000 张带标签的图像。
CAPTCHA 类型：6 个字母数字字符。
图像示例：[附加示例 CAPTCHA 图像]。
我尝试过的方法：
卷积神经网络 (CNN)：

我创建了一个具有多个卷积层和密集层的 CNN 模型。
将输出展平，以为每个字符输入单独的密集层。
在未见过的 CAPTCHA 上实现了较差的泛化。
迁移学习：
使用 MobileNetV2 作为带有自定义头的特征提取器。
调整输入大小（灰度到 RGB 转换）。
由于数据有限，该模型容易过度拟合
我想训练一个模型：

可以高精度处理未见过的 CAPTCHA。
有效地解码字母数字字符序列。

解决此类 CAPTCHA 的最佳方法或架构是什么？我应该使用：

CNN 用于特征提取，然后 LSTM 用于序列解码？
完全卷积架构（例如 CRNN）？
对于有限的数据集，还有其他更好的方法吗？

如果可能，您能否建议一个完整的模型架构、预处理步骤或可能有帮助的损失函数设置？对于这项任务，有没有什么技巧可以增强小数据集？]]></description>
      <guid>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</guid>
      <pubDate>Tue, 03 Dec 2024 18:37:33 GMT</pubDate>
    </item>
    <item>
      <title>convLSTM 教程的输入</title>
      <link>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</link>
      <description><![CDATA[我找到了以下教程“https://medium.com/neuronio-br/uma-introdu%C3%A7%C3%A3o-a-convlstm-c14abf9ea84a”讲授 ConvLSTM 模型。
我创建了一个算法来构建模型输入：
import os
import cv2
import numpy as np
import pandas as pd

# 设置
video_folder = &#39;train&#39; # 包含视频的文件夹的路径
output_folder = &#39;train_npy&#39; # 保存 .npy 文件的文件夹
csv_file = &#39;train.csv&#39; # CSV 文件的路径
frames_per_video = 16 # 每个视频的帧数（时间）
pixels_x, pixels_y = 112, 112 # 帧尺寸

# 使用视频名称和类别加载 CSV
df = pd.read_csv(csv_file) # 列：&#39;video_name&#39;, &#39;tag&#39;

# 根据“tag”列中的唯一值确定的类数
unique_tags = df[&#39;tag&#39;].unique()
num_categories = len(unique_tags) # 根据数据定义类别数量

# 将类别映射到索引的字典
tag_to_index = {tag: idx for idx, tag in enumerate(unique_tags)}

# 如果不存在则创建输出文件夹
os.makedirs(output_folder, exist_ok=True)

# 提取和处理帧的函数
def extract_frames(video_path, num_frames=16, size=(pixels_x, pixels_y)):
cap = cv2.VideoCapture(video_path)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
frame_interval = max(total_frames // num_frames, 1)

frames = []
for i in range(num_frames):
cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)
ret, frame = cap.read()
if not ret:
break
frame = cv2.resize(frame, size)
frames.append(frame)

# 必要时用零帧填充
while len(frames) &lt; num_frames:
frames.append(np.zeros((pixels_x, pixels_y, 3), dtype=np.uint8))

cap.release()
return np.array(frames) # 形状：(frames, pixels_x, pixels_y, 3)

# 循环处理所有视频
for idx, row in df.iterrows():
video_name = row[&#39;video_name&#39;]
class_label = row[&#39;tag&#39;]

# 视频的完整路径
video_path = os.path.join(video_folder, video_name)

# 检查视频是否存在
if not os.path.exists(video_path):
print(f&quot;Video {video_name} not found!&quot;)
continue

# 处理帧并保存为 .npy
scene_data = extract_frames(video_path, frames_per_video)
scene_data = scene_data.transpose(0, 3, 1, 2) # 将顺序改为 (帧、通道、行、列)
np.save(os.path.join(output_folder, f&#39;scene_{idx}.npy&#39;), scene_data)

# 创建并保存类别（独立输出）
category_data = np.zeros((num_categories, 1, frames_per_video, 1))
category_data[tag_to_index[class_label], 0, :, 0] = 1 # 将所有帧的类别标记为 1
np.save(os.path.join(output_folder, f&#39;category_{idx}.npy&#39;), category_data)

print(&quot;处理完成！&quot;)

然而，在训练模型时，我收到以下错误：
ValueError: Arguments target 和 output 必须具有相同的形状。收到：target.shape=(None, 1, 16)，output.shape=(None, 16, 1)
而形状符合教程：
scene_0.npy.shape = (16, 3, 112, 112)
category_0.npy.shape = (5, 1, 16, 1)
为什么目标和输出具有不同的形状？是教程错误还是输入格式有错误？]]></description>
      <guid>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</guid>
      <pubDate>Tue, 03 Dec 2024 18:34:23 GMT</pubDate>
    </item>
    <item>
      <title>未能提高 Tiny VGG CNN 模型的准确率</title>
      <link>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</link>
      <description><![CDATA[设置

在接触了 CNN 理论之后，我试图学习如何在 PyTorch 中编写它的基础知识。
我几乎是一丝不苟地遵循 05. PyTorch Going Modular 来学习如何以脚本方式建模，写下 CNN Explainer 中描述的 CNN 模型 Tiny VGG。

我正在练习的数据，如05. PyTorch Going Modular，是pizza-steak-sushi数据集；加载和转换方式如此处所示。

再次，该模型是这里中的模型。
问题

经过多次尝试设置以下超参数：
NUM_EPOCHS = 20
BATCH_SIZE = 20
HIDDEN_UNITS = 10
LEARNING_RATE = 0.0005

并使用一对损失函数和优化器：
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(tiny_vgg.parameters(), lr=LEARNING_RATE)

我无法提高模型在训练和（大部分）测试数据集中的准确率，目前如下所示：
Epoch：1 | train_loss：1.1018 | train_acc：0.2875 | test_loss：1.0947 | test_acc：0.4500
Epoch：2 | train_loss：1.0974 | train_acc：0.3833 | test_loss：1.0970 | test_acc：0.3125
Epoch：3 | train_loss：1.0960 | train_acc：0.3375 | test_loss：1.0995 | test_acc：0.3125
Epoch：4 | train_loss：1.0895 | train_loss：0.3500 | test_loss：1.1013 | test_acc：0.3125
Epoch：5 | train_loss：1.0626 | train_acc：0.3917 | test_loss：1.0755 | test_acc：0.3250
Epoch：6 | train_loss：1.0566 | train_acc：0.3750 | test_loss：1.0666 | test_acc：0.3375
Epoch：7 | train_loss：1.0105 | train_loss：0.5542 | test_loss：1.0133 | test_acc：0.4208
Epoch：8 | train_loss：0.9488 | train_acc：0.5708 | test_loss：1.0114 | test_acc：0.4792
时期：9 | train_loss：0.8834 | train_acc：0.5625 | test_loss：0.9973 | test_acc：0.4208
时期：10 | train_loss：0.8751 | train_acc：0.5667 | test_loss：1.0350 | test_acc：0.4542
时期：11 | train_loss：0.8098 | train_acc：0.6375 | test_loss：0.9834 | test_acc：0.4458
时期：12 | train_loss：0.8136 | train_acc：0.6333 | test_loss：1.0400 | test_acc：0.3792
时期：13 |训练损失：0.8042 | 训练误差：0.6833 | 测试损失：0.9929 | 测试损失：0.4458
时期：14 | 训练损失：0.7932 | 训练误差：0.6292 | 测试损失：1.0383 | 测试损失：0.4333
时期：15 | 训练损失：0.7796 | 训练损失：0.6458 | 测试损失：1.0025 | 测试损失：0.4833
时期：16 | 训练损失：0.7654 | 训练损失：0.6708 | 测试损失：1.0344 | 测试损失：0.4375
时期：17 | 训练损失：0.7412 | 训练损失：0.6833 | test_loss：1.0358 | test_acc：0.4625
时期：18 | train_loss：0.7168 | train_acc：0.6667 | test_loss：1.0759 | test_acc：0.4125
时期：19 | train_loss：0.7364 | train_acc：0.6500 | test_loss：1.0393 | test_acc：0.4583
时期：20 | train_loss：0.7228 | train_acc：0.7167 | test_loss：1.0826 | test_acc：0.4500


因此，我想知道我是否遗漏了架构方面的某些内容、超参数设置，或者其他内容，例如此类模型缺乏数据等等。再次强调，我对这个主题还很陌生，因此非常感谢任何提示。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</guid>
      <pubDate>Tue, 03 Dec 2024 11:10:18 GMT</pubDate>
    </item>
    <item>
      <title>从 tensorflow 导入 LSTM 时出错</title>
      <link>https://stackoverflow.com/questions/79129501/error-when-importing-lstm-from-tensorflow</link>
      <description><![CDATA[当我尝试从 Tensorflow.keras.layers 导入 LSTM 时，它显示无法解析。当我从 tensorflow.keras.models 导入顺序时也会发生这种情况。
将 tensorflow 导入为 tf
从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Dense, LSTM

我尝试卸载 Tensorflow 并重新安装，但存在同样的问题。我用 Python 版本 3.11 创建了一个新的虚拟环境，但这没有帮助。如果我用 tensorflow.python.keras.models 调用它，它不会显示为未解析，但在运行后会给我一个错误。顺便说一下，Tensorflow 版本 2.18.0。]]></description>
      <guid>https://stackoverflow.com/questions/79129501/error-when-importing-lstm-from-tensorflow</guid>
      <pubDate>Sat, 26 Oct 2024 21:35:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入所需模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入 Engine

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行它......已按照库安装并看到
https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html
我认为这要么是因为引擎已被修改，要么已被库删除......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>处理不平衡问题后，数据严重倾斜，准确率会下降</title>
      <link>https://stackoverflow.com/questions/72715345/accuracy-degrade-with-highly-skewed-data-after-handling-imbalance-problem</link>
      <description><![CDATA[在对数据进行预处理（如缺失值替换和异常值检测）后，我使用 WEKA 随机化和移除百分比过滤器对数据进行分区。我的数据集是一个高度倾斜的数据集，不平衡比率为 6:1，分别对应负类和正类。如果我使用朴素贝叶斯分类器对数据进行分类而不处理类别不平衡问题，我的准确率将达到 83%，召回率为 0.623。但是，如果我使用监督实例 - 重新采样或监督实例 - 扩展子采样过滤器处理类别不平衡（平衡 1:1 后），然后应用朴素贝叶斯分类准确率将下降 77%，召回率为 0.456。
为什么处理类别不平衡率时准确率会下降？]]></description>
      <guid>https://stackoverflow.com/questions/72715345/accuracy-degrade-with-highly-skewed-data-after-handling-imbalance-problem</guid>
      <pubDate>Wed, 22 Jun 2022 12:18:22 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正训练过程中不稳定的损失和准确率？</title>
      <link>https://stackoverflow.com/questions/55894132/how-to-correct-unstable-loss-and-accuracy-during-training</link>
      <description><![CDATA[我正在使用 TensorFlow 中的新 keras API 开发一个小型二元分类项目。该问题是几年前在 Kaggle.com 上发布的希格斯玻色子挑战赛的简化版本。数据集形状为 2000x14，其中每行的前 13 个元素构成输入向量，第 14 个元素是相应的标签。以下是所述数据集的示例：
86.043,52.881,61.231,95.475,0.273,77.169,-0.015,1.856,32.636,202.068, 2.432,-0.419,0.0,0
138.149,69.197,58.607,129.848,0.941,120.276,3.811,1.886,71.435,384.916,2.447,1.408,0.0,1
137.457,3.018,74.670,81.705,5.954,775.772,-8.854,2.625,1.942,157.231,1.193,0.873,0.824,1

我曾尝试根据在线找到的二元分类问题示例来构建各种模型，但在训练模型时遇到了困难。在训练过程中，损失有时会在同一时期内增加，导致学习不稳定。准确率在 70% 左右达到稳定状态。我尝试过改变学习率和其他超参数，但无济于事。相比之下，我已经硬编码了一个完全连接的前馈神经网络，在同样的问题上，它的准确率达到了 80-85% 左右。
这是我当前的模型：
import tensorflow as tf
from tensorflow.python.keras.layers.core import Dense
import numpy as np
import pandas as pd

def normalize(array):
return array/np.linalg.norm(array, ord=2, axis=1, keepdims=True)

x_train = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[:1800, :-1].values
y_train = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[:1800, -1:].values

x_test = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[1800:, :-1].values
y_test = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[1800:, -1:].values

x_train = normalize(x_train)
x_test = normalize(x_test)

model = tf.keras.Sequential()
model.add(Dense(9, input_dim=13,activation=tf.nn.sigmoid)
model.add(Dense(6,activation=tf.nn.sigmoid))
model.add(Dense(1,activation=tf.nn.sigmoid))

model.compile(optimizer=&#39;adam&#39;,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train, epochs=50)
model.evaluate(x_test, y_test)

如前所述，一些 epoch 的起始准确度比结束准确度高，导致学习不稳定。
 32/1800 [................................] - ETA：0s - 损失：0.6830 - 精度：0.5938
1152/1800 [==================&gt;...........] - ETA：0s - 损失：0.6175 - 精度：0.6727
1800/1800 [==============================] - 0s 52us/step - 损失：0.6098 - 精度：0.6861
Epoch 54/250

32/1800 [..............................] - ETA：0s - 损失：0.5195 - 精度：0.8125
1376/1800 [======================&gt;........] - ETA：0s - 损失：0.6224 - 精度：0.6672
1800/1800 [==============================] - 0s 43us/step - 损失：0.6091 - 精度：0.6850
Epoch 55/250

在如此简单的学习中，这些振荡的原因可能是什么模型？

编辑：
我遵循了评论中的一些建议，并相应地修改了模型。现在看起来更像这样：
model = tf.keras.Sequential()
model.add(Dense(250, input_dim=13,activation=tf.nn.relu))
model.add(Dropout(0.4))
model.add(Dense(200,activation=tf.nn.relu))
model.add(Dropout(0.4))
model.add(Dense(100,activation=tf.nn.relu))
model.add(Dropout(0.3))
model.add(Dense(50,activation=tf.nn.relu))
model.add(Dense(1,activation=tf.nn.sigmoid))

model.compile(optimizer=&#39;adadelta&#39;,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/55894132/how-to-correct-unstable-loss-and-accuracy-during-training</guid>
      <pubDate>Sun, 28 Apr 2019 20:02:19 GMT</pubDate>
    </item>
    <item>
      <title>加载 Keras 保存的模型时出现“AttributeError：'str' 对象没有属性 'decode'”</title>
      <link>https://stackoverflow.com/questions/53740577/attributeerror-str-object-has-no-attribute-decode-while-loading-a-keras</link>
      <description><![CDATA[训练后，我使用  保存了 Keras 整个模型和权重

model.save_weights(MODEL_WEIGHTS) 和 model.save(MODEL_NAME)

模型和权重已成功保存，没有错误。
我可以使用 model.load_weights 成功加载权重，它们很好用，但是当我尝试通过 load_model 加载保存的模型时，我收到错误。
文件“C:/Users/Rizwan/model_testing/model_performance.py”，第 46 行，位于 &lt;module&gt;
Model2 = load_model(&#39;nasnet_RS2.h5&#39;,custom_objects={&#39;euc_dist_keras&#39;: euc_dist_keras})
文件“C:\Users\Rizwan\AppData\Roaming\Python\Python36\site-packages\keras\engine\saving.py”，第 419 行，位于 load_model
model = _deserialize_model(f, custom_objects, compile)
文件“C:\Users\Rizwan\AppData\Roaming\Python\Python36\site-packages\keras\engine\saving.py”，第 321 行，位于 _deserialize_model
optimizer_weights_group[&#39;weight_names&#39;]]
文件“C:\Users\Rizwan\AppData\Roaming\Python\Python36\site-packages\keras\engine\saving.py”，第 320 行，位于 &lt;listcomp&gt;
n.decode(&#39;utf8&#39;) for n in
AttributeError: &#39;str&#39; 对象没有属性 &#39;decode&#39;

我从未收到此错误，并且我曾经成功加载任何模型。我使用的是 Keras 2.2.4 和 tensorflow 后端。Python 3.6。
我的训练代码是：
from keras_preprocessing.image import ImageDataGenerator
from keras import backend as K
from keras.models import load_model
from keras.callbacks import ReduceLROnPlateau, TensorBoard, 
ModelCheckpoint,EarlyStopping
import pandas as pd

MODEL_NAME = &quot;nasnet_RS2.h5&quot;
MODEL_WEIGHTS = &quot;nasnet_RS2_weights.h5&quot;
def euc_dist_keras(y_true, y_pred):
return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))
def main():

# 在这里，我们初始化“NASNetMobile”模型类型并自定义最终的
#特征回归层。
# NASNet 是 Google 开发的神经网络架构。
# 该架构专门用于迁移学习，并通过神经架构搜索发现。
# NASNetMobile 是 NASNet 的较小版本。
model = NASNetMobile()
model = Model(model.input, Dense(1,activation=&#39;linear&#39;, kernel_initializer=&#39;normal&#39;)(model.layers[-2].output))

# model = load_model(&#39;current_best.hdf5&#39;, custom_objects={&#39;euc_dist_keras&#39;: euc_dist_keras})

# 该模型将使用“Adam”优化器。
model.compile(&quot;adam&quot;, euc_dist_keras)
lr_callback = ReduceLROnPlateau(monitor=&#39;val_loss&#39;,factor=0.2,patience=5,min_lr=0.003)
# 此回调将模型统计数据记录到 Tensorboard。
tb_callback = TensorBoard()
# 此回调将在每个时期检查最佳模型。
mc_callback = ModelCheckpoint(filepath=&#39;current_best_mem3.h5&#39;, verbose=1, save_best_only=True)
es_callback=EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0, waiting=4, verbose=0, mode=&#39;auto&#39;, baseline=None, restore_best_weights=True)

# 这是训练数据序列。
# 这些是回调。
#callbacks = [lr_callback, tb_callback,mc_callback]
callbacks = [lr_callback, tb_callback,es_callback]

train_pd = pd.read_csv(&quot;./train3.txt&quot;, 分隔符=&quot; &quot;, 名称=[&quot;id&quot;, &quot;label&quot;], index_col=None)

test_pd = pd.read_csv(&quot;./val3.txt&quot;, 分隔符=&quot; &quot;, 名称=[&quot;id&quot;, &quot;label&quot;], index_col=None)

# train_pd = pd.read_csv(&quot;./train2.txt&quot;, 分隔符=&quot; &quot;, 标题=None, index_col=None)
# test_pd = pd.read_csv(&quot;./val2.txt&quot;, 分隔符=&quot; &quot;, 标题=None, index_col=None)
#model.summary()
batch_size=32
datagen = ImageDataGenerator（重新缩放=1. / 255）
train_generator = datagen.flow_from_dataframe（dataframe=train_pd，
directory=&quot;./images&quot;, x_col=&quot;id&quot;, y_col=&quot;label&quot;,
has_ext=True,
class_mode=&quot;other&quot;, target_size=(224, 224),
batch_size=batch_size）
valid_generator = datagen.flow_from_dataframe（dataframe=test_pd，directory=&quot;./images&quot;, x_col=&quot;id&quot;, y_col=&quot;label&quot;,
has_ext=True, class_mode=&quot;other&quot;, target_size=(224, 224),
batch_size=batch_size）

STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size
model.fit_generator(generator=train_generator,
steps_per_epoch=STEP_SIZE_TRAIN,
validation_data=valid_generator,
validation_steps=STEP_SIZE_VALID,
callbacks=callbacks,
epochs=20)

# 我们保存模型。
model.save_weights(MODEL_WEIGHTS)
model.save(MODEL_NAME)
if __name__ == &#39;__main__&#39;:
# 如果程序需要冻结，则在此处使用 freeze_support()
main()
]]></description>
      <guid>https://stackoverflow.com/questions/53740577/attributeerror-str-object-has-no-attribute-decode-while-loading-a-keras</guid>
      <pubDate>Wed, 12 Dec 2018 10:07:13 GMT</pubDate>
    </item>
    <item>
      <title>Weka：不兼容的训练和测试集</title>
      <link>https://stackoverflow.com/questions/50246992/weka-incompatible-training-and-test-sets</link>
      <description><![CDATA[我正在 Weka 上研究一个 分类 问题。我将 arff 文件作为我的训练数据，并从 数据库 中获取我的测试数据。但它们不兼容。在 Weka 工具中，我可以使用 InputMappedClassifier 并解决问题。但我无法在 Java 代码中实现它。请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/50246992/weka-incompatible-training-and-test-sets</guid>
      <pubDate>Wed, 09 May 2018 06:40:00 GMT</pubDate>
    </item>
    <item>
      <title>无法从命令行使用 Weka 进行分类</title>
      <link>https://stackoverflow.com/questions/35697652/unable-to-classify-using-weka-from-command-line</link>
      <description><![CDATA[我正在使用 weka 3.6.13 并尝试使用模型对数据进行分类：
java -cp weka-stable-3.6.13.jar weka.classifiers.Evaluation weka.classifiers.trees.RandomForest -l Parking.model -t Data_features_class_ques-2.arff 

java.lang.Exception：训练和测试集不兼容

虽然当我们使用 GUI 时模型可以工作，通过 Explorer-&gt;Claasify-&gt;Supplied 测试集并加载 arff 文件-&gt;右键单击结果列表并加载模型-&gt;再次右键单击-&gt;在当前数据集上重新评估模型...
任何指示请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/35697652/unable-to-classify-using-weka-from-command-line</guid>
      <pubDate>Mon, 29 Feb 2016 10:50:09 GMT</pubDate>
    </item>
    <item>
      <title>使用亚马逊机器学习提高设备效率[关闭]</title>
      <link>https://stackoverflow.com/questions/30724237/increasing-the-efficiency-of-equipment-using-amazon-machine-learning</link>
      <description><![CDATA[问题描述有点模糊，但我正在寻找方向，因为隐私政策我不能分享确切的细节。所以请帮忙。
我们手头有一个问题，我们需要提高设备的效率，或者换句话说，决定机器应该在多个参数中运行哪些值才能产生最佳输出。
我的问题是，是否可以使用线性回归或多项逻辑回归算法得出这样的数字，如果没有，那么你能否指定哪种算法更合适。此外，你能否指出一些针对此类问题的公共领域研究。
我寻求建议的问题类型是否属于机器学习领域？]]></description>
      <guid>https://stackoverflow.com/questions/30724237/increasing-the-efficiency-of-equipment-using-amazon-machine-learning</guid>
      <pubDate>Tue, 09 Jun 2015 06:12:50 GMT</pubDate>
    </item>
    <item>
      <title>weka 中的数据集不平衡？不起作用</title>
      <link>https://stackoverflow.com/questions/23426582/imbalanced-dataset-in-weka-does-not-work</link>
      <description><![CDATA[我有一个 239 个正数据集和一个 32 个负数据集，因为它是与癌症相关的数据，所以我们只有很少的负数据集。现在，当应用分类时，不平衡的数据集肯定会因为数量巨大而过于偏向正数据集。所以我尝试在 weka 中应用 SMOTE。我也尝试了各种百分比和最近邻居。令我惊讶的是，不是负类增加了几个实例，而是正类进一步增加，使得不平衡的数据集过于偏向。可以做些什么来克服这个问题。如果可用，请给我一些其他方法？
对于初步研究，我们使用了 LIBSVM 和 RBF 作为分类器 ]]></description>
      <guid>https://stackoverflow.com/questions/23426582/imbalanced-dataset-in-weka-does-not-work</guid>
      <pubDate>Fri, 02 May 2014 11:03:34 GMT</pubDate>
    </item>
    </channel>
</rss>