<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 15 Feb 2024 09:16:43 GMT</lastBuildDate>
    <item>
      <title>如何进行“次主题情感”分析？</title>
      <link>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</link>
      <description><![CDATA[我对 ML 和 NLP 还很陌生，因为我还在上大学，我最近得到了这个项目，我必须在其中找到子主题 在一个句子以及它们背后的情感中，我面临的问题是一个句子可以有多个子主题，并且我不明白如何对它们单独进行情感分析，有人可以吗告诉我如何解决这个问题？
有任何帮助吗？
看下面的例子：
“一个轮胎丢失了，因此安装两个轮胎的时间有所延迟。车库的处理方式
真是太棒了。”
如果我们看一下上述评论的次主题情绪，我们会更清楚地了解这些内容是什么
一般都是。
“不正确的轮胎发送阴性” “车库服务积极” “等待时间为负”
数据集：

我能够弄清楚如何在句子中找到子主题，但我无法单独找到每个子主题背后的情感，而只能找到整个句子的情感。
我一直在尝试让它们一起工作，但我不知道该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/77999477/how-do-i-perform-subtheme-sentiments-analysis</guid>
      <pubDate>Thu, 15 Feb 2024 09:03:04 GMT</pubDate>
    </item>
    <item>
      <title>与未知数据的实体匹配</title>
      <link>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</link>
      <description><![CDATA[相似的训练目标数据集对执行效果不好吗？
我将在数据帧上完成实体​​匹配任务。我决定使用基于机器学习的方法并使其通用。在这里，主要问题是我没有任何关于我的特定目标的标记数据集（即我有关于具有属性的人的标记数据集，但我想在关于汽车的数据帧上进行实体匹配）。我应该考虑哪种方法？如果我使用与目标没有相似性的标记数据集来训练模型，它会运行良好吗？如果可能的话，最佳介质是什么？在Google Scholar上，主要使用DITTO和DeepMatcher方法，但这些模型的f1分数是通过类似的训练目标数据集对观察的。]]></description>
      <guid>https://stackoverflow.com/questions/77999291/entity-matching-with-unknown-data</guid>
      <pubDate>Thu, 15 Feb 2024 08:30:49 GMT</pubDate>
    </item>
    <item>
      <title>我有一个 1510X132 功率输入和输出数据的数据集，我必须在数据上应用 DNN 模型来查找错误</title>
      <link>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</link>
      <description><![CDATA[我已经应用了该模型并得到了 mse、mae 错误。我已经预测了数据，现在我必须绘制实际数据与预测数据图，以检查预测数据和实际数据是否彼此相似。我无法编码
实际=pd.DataFrame()
#y=pd.DataFrame()
j=1
对于范围内的 i(0, len(df.columns)-4, 4)：
     #实际[j]=df.iloc[0:1, i+1:i+5]
     实际[j] = df.iloc[0:2, i+1]
     实际[j+1] = df.iloc[0:2, i+2]
     #y[[j, j+1]]=df.iloc[:, i+3:i+5]
    `j+4`

plt.figure(figsize=(10, 6))
plt.plot((实际), label=&#39;实际&#39;)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/77998736/i-have-a-dataset-of-1510x132-power-input-and-output-data-and-i-have-to-apply-dnn</guid>
      <pubDate>Thu, 15 Feb 2024 06:21:44 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中运行 train 方法时将 PNG 图像转换为 np.array</title>
      <link>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77998036/converting-a-png-image-to-a-np-array-while-running-the-train-method-in-a-neural</guid>
      <pubDate>Thu, 15 Feb 2024 01:22:49 GMT</pubDate>
    </item>
    <item>
      <title>集合预测中要缩放什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77997900/what-to-scale-in-ensemble-predictions</link>
      <description><![CDATA[我正在使用堆叠集成方法进行股票预测，但不知道什么和什么不应该扩展。预测训练和测试都是在 x 训练和测试上进行缩放的。我也不知道验证集应该在训练集还是测试集上。
这是代码：
feature_scaler = MinMaxScaler(feature_range=(0, 1))
    Predictions_train_scaled = feature_scaler.fit_transform(predictions_train)
    Predictions_test_scaled = feature_scaler.transform(predictions_test)

    target_scaler = MinMaxScaler(feature_range=(0, 1))
    y_train_shape = y_train.values.reshape(-1, 1)
    y_train_scaled = target_scaler.fit_transform(y_train_shape)
    y_test_shape = y_test.values.reshape(-1, 1)
    y_test_scaled = target_scaler.transform(y_test_shape)


    X_train_meta, y_train_meta_un = train_test_split(predictions_train_scaled,
    y_train_scaled、test_size=0.2、random_state = 1）
    X_val_meta, y_val_meta_un = train_test_split(predictions_test_scaled, y_test_scaled,
    测试大小=0.3，随机状态=1）

    y_train_meta = y_train_meta_un.reshape(-1, 1)
    y_val_meta = y_val_meta_un.reshape(-1, 1)

    #meta_model = 线性回归()
    meta_model = MLPRegressor(hidden_​​layer_sizes=(100, 50)，激活=&#39;relu&#39;，求解器=&#39;adam&#39;，alpha=0.001，random_state=42，max_iter=5000)
    meta_model.fit(X_train_meta, y_train_meta.ravel())

    train_meta_score = meta_model.score(X_train_meta, y_train_meta)
    print(f&quot;元模型训练分数：{train_meta_score}&quot;)
    
    val_score = meta_model.score(X_val_meta, y_val_meta)
    print(f“元模型验证分数：{val_score}”)

    cv_scores = cross_val_score(meta_model, X_train_meta, y_train_meta.ravel(), 评分=&#39;neg_mean_squared_error&#39;, cv=5)
    cv_rmse = np.sqrt(-cv_scores)
    print(f&quot;交叉验证 RMSE: {cv_rmse.mean()}&quot;)

    #predictions_test_scaled = feature_scaler.transform(predictions_test)
    stacked_predictionss = meta_model.predict(predictions_test_scaled)

    stacked_predictions = target_scaler.inverse_transform(stacked_predictionss.reshape(-1, 1))

    返回 stacked_predictions, stacked_predictionss
]]></description>
      <guid>https://stackoverflow.com/questions/77997900/what-to-scale-in-ensemble-predictions</guid>
      <pubDate>Thu, 15 Feb 2024 00:32:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 和 NLP 进行实体活动字段检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/77997504/entity-activity-field-detection-using-python-and-nlp</link>
      <description><![CDATA[我有一个时间有限的项目，从 csv 文件中获取/提取公司名称列表，在 google 上搜索以将每个名称与搜索结果匹配以找到其官方网址，然后提取网址（对于每个公司的数量）并将其存储在数据框中。然后利用 NLP 模型编写另一个脚本来抓取每个网站，并从其 url 中提取地址、员工人数、公司业务类型以及有关公司的其他相关信息，然后将提取的数据存储在更新的数据框中进一步分析。为了保持一致性，该实体流程必须自动化到管道中，以防不时注入新公司名称。我迫切需要帮助来完成这个项目。
我尝试编写一个谷歌搜索脚本来搜索和提取网址，但它没有产生预期的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77997504/entity-activity-field-detection-using-python-and-nlp</guid>
      <pubDate>Wed, 14 Feb 2024 22:24:30 GMT</pubDate>
    </item>
    <item>
      <title>奇怪（？）PySpark 的 OneHotEncoder 行为</title>
      <link>https://stackoverflow.com/questions/77997400/strange-pysparks-onehotencoder-behavior</link>
      <description><![CDATA[我尝试使用 PySpark 的 OneHotEncoder 构建 ML 管道，但我注意到使用“handleInvalid”函数会出现奇怪的行为。 param：当我使用 handleInvalid=“error” 时，我得到 5 个类别，但是当我使用 handleInvalid=“keep” 时，我得到 6 个类别。请参阅下面的示例：
导入操作系统
从 alura_spark.config 导入 configure_spark_env

从 pyspark.sql 导入 SparkSession
从 pyspark.ml.feature 导入 StringIndexer、OneHotEncoder
从 pyspark.ml 导入管道


config_root_dir = os.path.abspath(&#39;.&#39;)
配置_spark_env（root_dir = config_root_dir）

# 启动 SparkSession
火花=（
    SparkSession.builder
    .master(“本地[*]”)
    .appName(“回归模型”)
    .config(“spark.ui.port”,“4050”)
    .getOrCreate()
）
火花

# 创建一个带有分类列的示例 DataFrame
数据=[(“A”,),(“B”,),(“C”,),(“D”,),(“E”,),
        (“A”，)，(“B”，)，(“C”，)，(“D”，)，(“E”，)，
        (“A”,)、(“B”,)、(“C”,)、(“D”,)、(“E”,)]
列 = [“类别”]
df = Spark.createDataFrame(数据, 列)

def apply_pipeline（df，categorical_column，handle_invalid）：
    # StringIndexer 处理新类别
    索引器= StringIndexer(inputCol=categorical_column,outputCol=f“{categorical_column}_index”,handleInvalid=handle_invalid)

    # OneHotEncoder 转换索引类别
    编码器 = OneHotEncoder(inputCols=[f&quot;{categorical_column}_index&quot;], outputCols=[f&quot;{categorical_column}_onehot&quot;], dropLast=True)

    # 管道到链的转换
    管道=管道（阶段=[索引器，编码器]）

    # 拟合并转换 DataFrame
    pipeline_model = pipeline.fit(df)
    df_transformed = pipeline_model.transform(df)

    n_categories = pipeline_model.stages[-1].categorySizes
    打印（&#39;＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃&#39;）
    print(f&quot;n_categories with handleInvalid=&#39;{handle_invalid}&#39; = {n_categories}&quot;)
    df_transformed.show(5)
    打印（&#39;＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃&#39;）

    返回 df_transformed

# 使用handleInvalid=&#39;keep&#39;应用管道
df_transformed_keep = apply_pipeline(df, “类别”, “保留”)

# 应用带有handleInvalid=&#39;error&#39;的管道
df_transformed_error = apply_pipeline(df, “类别”, “错误”)

#spark.stop()

其中 configure_spark_env 只是我用来设置 JAVA_HOME、SPARK_HOME、HADOOP_HOME 和  的函数LD_LIBRARY_PATH 环境变量。
运行上面的代码我得到：
&lt;前&gt;&lt;代码&gt;######################################
n_categories with handleInvalid=&#39;keep&#39; = [6]
+--------+--------------+----------------+
|类别|category_index|category_onehot|
+--------+--------------+----------------+
|一个| 0.0| (5,[0],[1.0])|
|乙| 1.0| (5,[1],[1.0])|
| C| 2.0| (5,[2],[1.0])|
| D| 3.0| (5,[3],[1.0])|
|电子| 4.0| (5,[4],[1.0])|
+--------+--------------+----------------+
只显示前 5 行

######################################
######################################
n_categories with handleInvalid=&#39;error&#39; = [5]
+--------+--------------+----------------+
|类别|category_index|category_onehot|
+--------+--------------+----------------+
|一个| 0.0| (4,[0],[1.0])|
|乙| 1.0| (4,[1],[1.0])|
| C| 2.0| (4,[2],[1.0])|
| D| 3.0| (4,[3],[1.0])|
|电子| 4.0| (4,[],[])|
+--------+--------------+----------------+
只显示前 5 行

######################################

我正在使用以下版本：
&lt;前&gt;&lt;代码&gt;PySpark：3.5.0
JDK：17.0.10
Hadoop：3.3.6
火花：3.5.0
流行操作系统：22.04

由于我在拟合和转换中使用相同的数据，因此我期望使用“keep”或“error”获得 5 个类别。这是我将 sklearn 的 OHE 与 handle_unknow=&#39;ignore&#39; 一起使用时得到的行为。]]></description>
      <guid>https://stackoverflow.com/questions/77997400/strange-pysparks-onehotencoder-behavior</guid>
      <pubDate>Wed, 14 Feb 2024 21:53:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 LoRA 微调 ControlNet？</title>
      <link>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</link>
      <description><![CDATA[我想使用 LoRA 适配器微调 ControlNet。我的数据集由成对图像组成 - 分割图（没有家具的房间，只有墙壁、地板、窗户、门） - RGB 图像（有家具的房间）。我的目标是微调 ControlNet 以根据空房间的分割图生成带家具的房间（我使用空提示）
所以，我不太清楚如何训练 LoRA。我考虑了两个选择

训练 LoRA 进行稳定扩散（仅使用带家具的房间图像作为数据集），然后将此稳定扩散注入 ControlNet

例如，如果我使用这个管道
管道 = StableDiffusionControlNetPipeline.from_pretrained(
    “runwayml/stable-diffusion-v1-5”，controlnet=controlnet，torch_dtype=torch.float16
）

然后将“runwayml/stable-diffusion-v1-5”替换为与我精心调校的 LoRA 一起。这是正确的做法吗？如果我仅微调稳定扩散并且不显示我的分割图，ControlNet 是否能够生成房间？

在图像对上训练 LoRA for ControlNet。
然而，我读到 Huggingface 不支持为 ControlNet 训练 LoRA（所有教程仅展示如何训练稳定扩散）。但我找到了文章
https://fangchuan.github.io/ctrl-room.github.io/&lt; /a&gt;
作者对 ControlNet 进行了微调以生成房间全景图（但文章没有提供技术细节），因此我认为可以直接为 ControlNet 训练 LoRA

那么什么方法来微调 ControlNet 是普遍可以接受的呢？]]></description>
      <guid>https://stackoverflow.com/questions/77997278/how-to-fine-tune-controlnet-using-lora</guid>
      <pubDate>Wed, 14 Feb 2024 21:19:17 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow自定义损失函数在直接调用时有效，但在拟合时给出错误</title>
      <link>https://stackoverflow.com/questions/77997255/tensorflow-custom-loss-function-works-when-called-directly-but-gives-error-when</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77997255/tensorflow-custom-loss-function-works-when-called-directly-but-gives-error-when</guid>
      <pubDate>Wed, 14 Feb 2024 21:14:09 GMT</pubDate>
    </item>
    <item>
      <title>有没有方法可以识别 XGBoost 树适合哪个数据子样本？</title>
      <link>https://stackoverflow.com/questions/77997182/are-there-ways-to-identify-which-subsample-of-data-an-xgboost-tree-was-fitted-on</link>
      <description><![CDATA[我想确定为每棵树的训练随机选择哪些特定的样本行（根据子样本参数）。例如，训练集有 100 行，子采样率为 0.7，我想知道为每棵树的训练选择的确切 70 行。我一直在到处寻找解决方案，但没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/77997182/are-there-ways-to-identify-which-subsample-of-data-an-xgboost-tree-was-fitted-on</guid>
      <pubDate>Wed, 14 Feb 2024 20:58:25 GMT</pubDate>
    </item>
    <item>
      <title>Bagging 和 OOB 分配出现问题</title>
      <link>https://stackoverflow.com/questions/77997095/trouble-with-bagging-and-oob-assignment</link>
      <description><![CDATA[我正在学习 ML 课程，这是我的第一门课程，我是 ML 新手，并且我一直在做一些作业。我对其中一个有关装袋和 OOB 的问题有疑问
我必须填写下面代码中的行。这是 sklearn 的 BaggingRegressor 的简化版本。请注意，sklearn API 未保留。
该算法应该能够在引导数据集上训练同一模型类的不同实例，并为训练集提供 OOB 分数。
模型应作为模型类传递，没有显式参数，也没有括号。
虽然我已经有了 _generate_splits() 函数，但我仍然在努力解决其余的问题。
谁能告诉我这些函数的公式是什么？
将 numpy 导入为 np
`SimplifiedBaggingRegressor 类：
def __init__(self, num_bags, oob=False):
    self.num_bags = num_bags
    self.oob = oob
    
def _generate_splits(self, 数据: np.ndarray):
    &#39;&#39;&#39;
    为每个包生成索引并存储在 self.indices_list 列表中
    &#39;&#39;&#39;
    self.indices_list = []
    数据长度 = len(数据)
    对于范围内的包（self.num_bags）：
        # 这里是你的代码
                  bag_indices = list(范围(0,total_samples))
        self.indices_list.append(bag_indices)
    
def fit(自身、模型构造函数、数据、目标)：
    &#39;&#39;&#39;
    每个包袋都适合模型。
    不带参数（且不带 ()）的模型构造函数被传递给此函数。
    
    例子：
    
    bagging_regressor = SimplifiedBaggingRegressor(num_bags=10, oob=True)
    bagging_regressor.fit(线性回归, X, y)
    &#39;&#39;&#39;
    self.data = 无
    自我目标=无
    self._generate_splits（数据）
    assert len(set(list(map(len, self.indices_list)))) == 1, &#39;所有袋子的长度应该相同！&#39;
    assert list(map(len, self.indices_list))[0] == len(data), &#39;所有包都应包含 `len(data)` 个元素！&#39;
    self.models_list = []
    对于范围内的包（self.num_bags）：
        模型 = model_constructor()
        data_bag, target_bag = # 这里是你的代码
        self.models_list.append(model.fit(data_bag, target_bag)) # 在此存储拟合模型
    如果是 self.oob:
        self.data = 数据
        self.target = 目标
    
def 预测（自身，数据）：
    &#39;&#39;&#39;
    从传递的数据集中获取每个对象的平均预测
    &#39;&#39;&#39;
    # 你的代码在这里

def _get_oob_predictions_from_every_model（自身）：
    &#39;&#39;&#39;
    生成列表的列表，其中列表 i 包含 self.data[i] 对象的预测
    来自所有在训练阶段没有见过这个物体的模型
    &#39;&#39;&#39;
    list_of_predictions_lists = [[] for _ in range(len(self.data))]
    # 这里是你的代码
    
    self.list_of_predictions_lists = np.array(list_of_predictions_lists, dtype=object)

def _get_averged_oob_predictions（自我）：
    &#39;&#39;&#39;
    计算训练集中每个对象的平均预测。
    如果训练阶段的所有包中都使用了对象，则返回 None 而不是预测
    &#39;&#39;&#39;
    self._get_oob_predictions_from_every_model()
    self.oob_predictions = # 这里是你的代码
    
    
def OOB_score(自身):
    &#39;&#39;&#39;
    计算至少有一个预测的所有对象的均方误差
    &#39;&#39;&#39;
    self._get_averged_oob_predictions()
    在此处返回#您的代码`

我试图理解原始 BaggingRegressor 类是如何工作的，但我有点困惑。]]></description>
      <guid>https://stackoverflow.com/questions/77997095/trouble-with-bagging-and-oob-assignment</guid>
      <pubDate>Wed, 14 Feb 2024 20:35:59 GMT</pubDate>
    </item>
    <item>
      <title>UndefinedMetricWarning：精度定义不明确，由于没有预测样本而被设置为 0.0 [重复]</title>
      <link>https://stackoverflow.com/questions/77995496/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-due-to-no</link>
      <description><![CDATA[当我尝试拟合 X 和 Y 训练集时，它会向我发出警告消息：
UndefinedMetricWarning：精度定义不明确，由于没有预测样本而被设置为 0.0。使用“zero_division”参数来控制此行为。 _warn_prf(平均值，修饰符，f“{metric.capitalize()}是”，len(结果))
这导致我的精确率、召回率和 F1 分数为 0。但是添加后
average=&#39;weighted&#39; 和 labels，它产生了可用的分数：
f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;, labels=np.unique(y_pred))

但是我的问题是我的 Y_pred 保持不变，每当我制作 ConfusionMatrix 时，它都会显示有 0 个 True 或 False Positive。有没有办法改变我的 Y_pred，以反映我修正的分数？
如上所述，我修复了分数，但 Y_pred 仍然不变。]]></description>
      <guid>https://stackoverflow.com/questions/77995496/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-due-to-no</guid>
      <pubDate>Wed, 14 Feb 2024 15:37:39 GMT</pubDate>
    </item>
    <item>
      <title>NLP 中的搜索、NER 和关系提取用例</title>
      <link>https://stackoverflow.com/questions/77992256/search-ner-and-relation-extraction-use-case-in-nlp</link>
      <description><![CDATA[我正在尝试对维基百科的文本创建 NER 和摘要。我正在使用维基页面中的文本： https://en.wikipedia.org/wiki/Hyderabad&lt; /a&gt;
下面是我的代码：
from txtai.embeddings import Embeddings
导入系统
将渐变导入为 gr
导入spacy
从字符串导入标点符号


# 创建嵌入模型，由句子转换器和句子转换器支持变形金刚
嵌入=嵌入（{“路径”：“../Lib/site-packages/all-MiniLM-L6-v2”}）

f = open(&#39;海得拉巴.txt&#39;, &#39;r&#39;, 编码=&#39;ISO-8859-1&#39;)
data = f.read().split(“.”)

# 为文本列表建立索引
嵌入.index（数据）

def 搜索（查询）：
    uid = embeddings.search(查询, 1)[0][0]
    返回数据[uid]

def 响应（问题）：
    合并=搜索（问题）
    ans = 代词_coref(合并)
    返回答案

演示 = gr. 接口(
    fn=响应，
    输入=[“文本”]，
    输出=[“文本”]，
）
演示.launch(share=False)

搜索：什么是 Purana pul？
响应：两者之间由许多跨河桥梁连接，其中最古老的是 Purana Pul（“老桥”），建于公元 1578 年。
我想加强对以下内容的响应：旧城和新城由许多横跨穆西河的桥梁连接起来，其中最古老的是 Purana Pul（“旧桥”），建于公元 1578 年.
NER 可以通过 en_core_web_md 模型完成。
如何在响应中将代词替换为适当的名词？]]></description>
      <guid>https://stackoverflow.com/questions/77992256/search-ner-and-relation-extraction-use-case-in-nlp</guid>
      <pubDate>Wed, 14 Feb 2024 05:33:45 GMT</pubDate>
    </item>
    <item>
      <title>三重态损失时间序列</title>
      <link>https://stackoverflow.com/questions/77879087/triplet-loss-time-series</link>
      <description><![CDATA[我正在阅读这篇论文，我想知道三重态损失在一般情况下是如何工作的时间序列、正样本和上下文。
假设我们有一个包含 3 个样本、一个特征和序列长度为 6 的数据集，如下所示：
&lt;前&gt;&lt;代码&gt;y1 = [1,3,4,7,5,2]
y2 = [1,2,3,4,5,6]
y3 = [9,8,4,6,2,1]

如果我遵循给定论文的逻辑，三元组将如下生成：
s_i = 6 # yi 的长度
s_pos = 3 # 随机长度
s_ref = 3+2 # s_pos 和 之间随机s_i

x_ref = [1,2,3,4,5]
x_pos = [2,3,4]

...
x_1_neg = [3,4,7]

现在为了减少损失，我们希望找到给定锚点 x_ref 的 x_pos。
但是，编码器找到正确的对不是很容易吗，因为 x_pos 包含在 x_ref 中，而 x_1_neg 则不包含？仅向编码器提供 x_pos 周围的上下文（例如 [1,...,5]）不是更有意义吗？]]></description>
      <guid>https://stackoverflow.com/questions/77879087/triplet-loss-time-series</guid>
      <pubDate>Thu, 25 Jan 2024 10:13:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中累积大批量的梯度</title>
      <link>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</link>
      <description><![CDATA[我正在使用一个对内存要求很高的 CNN 模型来执行分类任务。
这对我在训练期间可以使用的批量大小造成了很大的限制。
一种解决方案是在训练期间累积梯度，这意味着模型的权重不会在每个批次后更新。相反，相同的权重用于多个批次，而每个批次的梯度会被累积，然后针对单个权重更新操作进行平均。
我正在使用 Tensorflow 后端 Keras，并且我非常确定 Keras 没有现成的函数/方法来实现此目的。
如何为 Keras/tensorflow 模型完成此操作？]]></description>
      <guid>https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras</guid>
      <pubDate>Wed, 20 Mar 2019 19:26:43 GMT</pubDate>
    </item>
    </channel>
</rss>