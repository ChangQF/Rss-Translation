<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Jan 2024 15:14:19 GMT</lastBuildDate>
    <item>
      <title>CNN最后一层的SVR</title>
      <link>https://stackoverflow.com/questions/77794156/svr-on-the-last-layer-of-cnn</link>
      <description><![CDATA[所以我正在尝试创建 CNN + SVR 模型来猜测手稿的制作日期。我正在使用 CNN 的 google 架构，现在如何将 SVR 添加到 CNN 的全连接层来猜测这个日期？
这是我对谷歌架构模型的修改版本：
在此处输入图片描述
嗯，我的第一个想法是提取特征向量，因为最后一层的 CNN 给了我 1x1x1024“图像”最后，但是支持向量回归真的接受特征向量吗？

所以，这里的旁注是我的数据库包含 1300-1600 年时期的拉丁手稿（遗憾的是并不是每年都会给出）。我尝试使用 SVR，因为我的模型应该允许 +-10 年的错误，因为显然很难猜测确切的年份
]]></description>
      <guid>https://stackoverflow.com/questions/77794156/svr-on-the-last-layer-of-cnn</guid>
      <pubDate>Wed, 10 Jan 2024 14:33:07 GMT</pubDate>
    </item>
    <item>
      <title>ValueError: Expected n_neighbors <= n_samples （指定的邻居数量与数据中的样本数量不匹配）</title>
      <link>https://stackoverflow.com/questions/77793613/valueerror-expected-n-neighbors-n-samples-mismatch-between-the-number-of-ne</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77793613/valueerror-expected-n-neighbors-n-samples-mismatch-between-the-number-of-ne</guid>
      <pubDate>Wed, 10 Jan 2024 13:05:17 GMT</pubDate>
    </item>
    <item>
      <title>如何计算两篇论文的loss一起</title>
      <link>https://stackoverflow.com/questions/77793412/how-to-calculate-the-loss-of-two-papers-together</link>
      <description><![CDATA[我的目的是把code2加入到code1中，在paper1的环境下一起计算loss，但是试了很多次都没能计算出结果
我认为关键是处理trainLoder和loss（局部变量和全局变量），如何定义它们？你能帮我解决这个问题吗？
code1：（计算paper1损失的代码）
对于范围内的纪元（args.epochs）：
        t = 时间.time()

        模型.train()
        优化器.zero_grad()
        moc、g1、g2、DGI_loss1、DGI_loss2 = 模型（特征、adj_norm、adj_tensor、drug_nums）
        标签 = adj_label.to_dense().view(-1)

        # BCEloss：监督损失
        BCEloss = torch.mean(loss_function_BCE(moc, labels) * adj_mask)
        BCEloss += torch.mean(loss_function_BCE(g1, labels) * adj_mask)
        BCEloss += torch.mean(loss_function_BCE(g2, labels) * adj_mask)
        打印（BCE损失）

code2:(计算paper2损失的代码)
对于范围内的 e(param.epoch)：
     运行损失 = 0.0 ###
     epo_标签 = []
     epo_分数 = []
     print(&quot;纪元：&quot;, e + 1)
     模型.train()
     开始 = 时间.time()
     对于 i，枚举（trainLoader）中的项目：
         数据，标签=项目
         train_data = data.cuda()
         true_label = label.cuda() ###
         pre_score = 模型(simData, train_data)##*
         train_loss = torch.nn.BCELoss()
         **损失 = train_loss(pre_score, true_label)**
         loss.backward()
         优化器.step()
         优化器.zero_grad()
         running_loss += loss.item() ###
         print(f&quot;批次 {i + 1} 后：loss= {loss:.3f};&quot;, end=&#39;\n&#39;)###

code3：（我试图将它们组合在一起的代码）
train_data = loading_data(参数)

simData = Simdata_pro(参数)
kfolds = 参数.kfold
train_labels = train_data[&#39;train_Labels&#39;]
状态=&#39;有效&#39;
#trainLoader=无
如果状态==&#39;有效&#39;：
    kf = KFold(n_splits=kfolds, shuffle=True, random_state=1)
    train_idx, valid_idx = [], []
    #trainLoader=无
    对于 train_index，kf.split(train_edges) 中的 valid_index：
        train_idx.append(train_index)
        valid_idx.append(valid_index)
    对于范围内的 i（kfolds）：
        Edges_train, Edges_valid = train_edges[train_idx[i]], train_edges[valid_idx[i]]
        labels_train, labels_valid = train_labels[train_idx[i]], train_labels[valid_idx[i]]
        trainEdges = CVEdgeDataset(edges_train, labels_train)
        trainLoader = DataLoader.DataLoader(trainEdges,batch_size=param.batchSize,shuffle=True,num_workers=0)
        # validLoader = DataLoader.DataLoader(validEdges, batch_size=param.batchSize, shuffle=True, num_workers=0)


        对于范围内的纪元（args.epochs）：
            t = 时间.time()

            模型.train()
            优化器.zero_grad()
            moc、g1、g2、DGI_loss1、DGI_loss2 = 模型（特征、adj_norm、adj_tensor、drug_nums）
            标签 = adj_label.to_dense().view(-1)

           ** # BCEloss：监督损失
            BCEloss = torch.mean(loss_function_BCE(moc, labels) * adj_mask)
            BCEloss += torch.mean(loss_function_BCE(g1, labels) * adj_mask)
            BCEloss += torch.mean(loss_function_BCE(g2, labels) * adj_mask)
            打印（&#39;BCEloss=&#39;）
            打印（BCE损失）**

   
            运行损失 = 0.0 ###
            epo_标签 = []
            epo_分数 = []
            print(&quot;纪元：&quot;, 纪元 + 1)
            模型.train()
            开始 = 时间.time()
            #损失=0
         # 新损失=0
            如果 trainLoader 不是 None：
                #损失=0
                for i, item in enumerate(trainLoader): ##i 是索引，item 是具体的值
                    数据，标签=项目
                    train_data = data.cuda()
                    true_label = label.cuda() ###
                    pre_score = 模型(simData, train_data) ##*
                    train_loss = torch.nn.BCELoss()
                   ** 损失 = train_loss(pre_score, true_label)**
                    loss.backward()
                    优化器.step()
                    优化器.zero_grad()
                    running_loss += loss.item() ###
                    print(f&quot;批次 {i + 1} 后：loss= {loss:.3f};&quot;, end=&#39;\n&#39;) ###

                新损失=BCE损失+损失
                打印（&#39;新损失=&#39;）
                打印（新损失）
                打印（&#39;损失&#39;）
                打印（丢失）
    

在哪里定义loss和trainLoder？]]></description>
      <guid>https://stackoverflow.com/questions/77793412/how-to-calculate-the-loss-of-two-papers-together</guid>
      <pubDate>Wed, 10 Jan 2024 12:34:48 GMT</pubDate>
    </item>
    <item>
      <title>带有角度旋转的梯度下降</title>
      <link>https://stackoverflow.com/questions/77793039/gradient-descent-with-angle-rotation</link>
      <description><![CDATA[我正在尝试二次函数的经典梯度下降算法的变体，该算法试图在连续步骤中将其旋转给定角度 $\theta$ 来打破方向的正交性。
这是我的代码：
def gdRotation(A,b,x0,theta,max_iters=1e5,eps=1e-5):
  n_iter = 0
  x_k = x0
  旋转矩阵 = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta),np.cos(theta)]])
  grad_x_k = (A @ x0) + b
  旋转目录 = 旋转矩阵 @ (-grad_x_k)
  
  #确保我们采取下降方向
  d = 旋转目录 如果 np.dot(grad_x_k, 旋转目录) &lt; 0 其他 -grad_x_k
  而 (n_iter &lt;= max_iters) 和 (np.linalg.norm(d) &gt; eps)：
      步骤=(d@d)/(d@A@d)
      x_k += (步骤 * d)
      grad_x_k = (A @ x_k) + b
      旋转目录 = 旋转矩阵 @ (-grad_x_k)
      d = 旋转目录 如果 np.dot(grad_x_k,rotation_dir) &lt; 0 其他 -grad_x_k
      n_iter+=1
  返回 x_k,n_iter

我的问题是该方法不收敛，我可能做错了什么有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77793039/gradient-descent-with-angle-rotation</guid>
      <pubDate>Wed, 10 Jan 2024 11:33:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 解决多类单标签分类问题时，为什么验证准确度会停留在恒定值？</title>
      <link>https://stackoverflow.com/questions/77792680/why-does-the-validation-accuracy-get-stuck-at-a-constant-value-when-using-keras</link>
      <description><![CDATA[我正在尝试使用 Keras 解决多类分类问题。当前网络如下：
模型 = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(128, time_major=False,return_sequences=False, return_state=False))
model.add（密集（64，激活=&#39;relu&#39;））
model.add（密集（32，激活=&#39;relu&#39;））
model.add（密集（4，激活=&#39;softmax&#39;））

model.compile(tf.keras.optimizers.RMSprop(),
              损失=&#39;分类交叉熵&#39;，
              指标=[&#39;准确性&#39;])

准确度值不超过0.3。
我尝试过改变学习率的值，使用更复杂或更简单的网络，将最后一层的激活函数改为sigmoid...
我得到的结果是：
纪元 2/1000
80/80 [================================] - 2s 30ms/步 - 损失：1.3860 - 准确度：0.3675 ​​- val_loss ：1.3859 - val_accuracy：0.3641
纪元 3/1000
80/80 [================================] - 3s 32ms/步 - 损失：1.3858 - 准确度：0.3675 ​​- val_loss ：1.3857 - val_accuracy：0.3641
纪元 4/1000
80/80 [==============================] - 2s 30ms/步 - 损失：1.3856 - 准确度：0.3675 ​​- val_loss ：1.3855 - val_accuracy：0.3641
纪元 5/1000
80/80 [==============================] - 2s 31ms/步 - 损失：1.3854 - 准确度：0.3675 ​​- val_loss ：1.3853 - val_accuracy：0.3641
纪元 6/1000
80/80 [================================] - 2s 30ms/步 - 损失：1.3852 - 准确度：0.3675 ​​- val_loss ：1.3851 - val_accuracy：0.3641
´´´
]]></description>
      <guid>https://stackoverflow.com/questions/77792680/why-does-the-validation-accuracy-get-stuck-at-a-constant-value-when-using-keras</guid>
      <pubDate>Wed, 10 Jan 2024 10:34:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在 BART 模型中使用自定义嵌入？以及如何生成可由 BART 模型使用的位置嵌入？</title>
      <link>https://stackoverflow.com/questions/77792221/how-to-use-custom-embedding-in-bart-model-and-how-to-generate-positinal-embeddi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77792221/how-to-use-custom-embedding-in-bart-model-and-how-to-generate-positinal-embeddi</guid>
      <pubDate>Wed, 10 Jan 2024 09:28:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在AWS EC2中使用NCCL后端设置pytorch DDP？</title>
      <link>https://stackoverflow.com/questions/77792215/how-to-setup-pytorch-ddp-with-nccl-backend-in-aws-ec2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77792215/how-to-setup-pytorch-ddp-with-nccl-backend-in-aws-ec2</guid>
      <pubDate>Wed, 10 Jan 2024 09:27:07 GMT</pubDate>
    </item>
    <item>
      <title>关系分类器评估需要很长时间</title>
      <link>https://stackoverflow.com/questions/77792061/relation-classifier-evaluation-taking-a-long-time</link>
      <description><![CDATA[我有一个深度学习模型，可以预测主体、客体以及它们之间的关系。对于培训，我有这些注释并且可以使用它们。在评估中，我没有这些注释，因此目前我只是对每个图像的所有可能的对象组合进行采样。问题是：图像中可能有 20 个对象，那么我必须测试 400 个关系。乘以批量大小，我的模型必须生成 &gt; 1000 个组合边界框，并将它们在图像上对齐。此操作大大延长了评估时间。培训大约需要 1 小时，而评估最多需要 12 小时。我是否误解了评估抽样，或者我的方法是否错误。目前，我正在分别对所有对象和所有可能的关系进行 Roi 对齐，然后我将连接这些特征以使用分类层。
这是我使用的模型]]></description>
      <guid>https://stackoverflow.com/questions/77792061/relation-classifier-evaluation-taking-a-long-time</guid>
      <pubDate>Wed, 10 Jan 2024 09:01:58 GMT</pubDate>
    </item>
    <item>
      <title>如何进行标题预测？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77791809/how-can-i-perform-title-prediction</link>
      <description><![CDATA[我正在使用 python bert-base-uncased 模型基于句子创建标题。这是我写的代码。我需要根据possible_labels预测标题。有没有什么可能的方法可以根据possible_labels执行标题预测？
加载预训练的 BERT 模型和分词器以进行文本分类
从转换器导入 AutoTokenizer、AutoModelForSequenceClassification
进口火炬
model_name = “bert-base-uncased”；
tokenizer = AutoTokenizer.from_pretrained(model_name)
模型 = AutoModelForSequenceClassification.from_pretrained(model_name)

定义类标签
可能的标签 = [
    “问候”，
    《再见》，
    《表达谢意》，
    “道歉”，
    “请求”，
    “命令或指示”，
    “广告或促销”，
    “新闻更新”，
    “问题”，
    “积极情绪”，
    “负面情绪”，
    “中立声明”，
    # 根据需要添加更多标签
]

输入句子进行分类
text = “正在工作”

# 对输入句子进行分词和编码
编码输入 = 分词器（文本，return_tensors =“pt”）

# 进行预测
输出=模型（**编码输入）

# 获取预测的类别索引
Predicted_class_index = torch.argmax(output.logits, dim=1).item()

# 获取预测标签
预测标签=可能的标签[预测类别索引]

打印结果
print(“输入句子：”, text)
print(&quot;预测标签：&quot;, Predicted_label)

打印所有概率
概率 = torch.nn.function.softmax(output.logits, dim=1).tolist()[0]
对于标签，zip 中的概率（possible_labels，概率）：
    print(f“{label} 的概率：{prob:.4f}”)
]]></description>
      <guid>https://stackoverflow.com/questions/77791809/how-can-i-perform-title-prediction</guid>
      <pubDate>Wed, 10 Jan 2024 08:18:12 GMT</pubDate>
    </item>
    <item>
      <title>我想制作一个程序来读取大型文本文件（例如书籍）并在 python 中输出它们的流派[关闭]</title>
      <link>https://stackoverflow.com/questions/77791719/i-want-to-make-a-program-which-reads-large-text-files-such-as-books-and-outputs</link>
      <description><![CDATA[我认为制作一个读取文本文件（例如书籍）的程序会很有用，并输出其流派。我的研究得出的结论是，我可能需要在大量书籍及其类型上训练我自己的语言模型。这真的是最好的方法吗？如果是，我应该如何在 Python 中做到这一点？
我希望所使用的任何方法都是免费的。以及在 Mac 上工作。]]></description>
      <guid>https://stackoverflow.com/questions/77791719/i-want-to-make-a-program-which-reads-large-text-files-such-as-books-and-outputs</guid>
      <pubDate>Wed, 10 Jan 2024 08:01:59 GMT</pubDate>
    </item>
    <item>
      <title>Handritten 数字识别算法前向传播中的矩阵乘法效率低下</title>
      <link>https://stackoverflow.com/questions/77790906/matrix-multiplication-in-forward-propogation-for-handritten-digit-recog-algo-ine</link>
      <description><![CDATA[我正在用 python 编写手写数字识别神经网络算法，而不使用预先编写的 ML 库。我目前正在尝试实现一个 DenseLayer 类，并在其中实现一个前向传播函数。我当前的功能如下所示。
类 DenseLayer：
  ...
  
  ...
  def for_prop(自身, input_data):
    self.input = input_data

    transpose_weights = self.weights.T
    # matMulComponent = np.matmul(input_data, transpose_weights)
    print(f&quot;转置形状：{transpose_weights.shape} 和输入形状 {input_data.shape}&quot;)
    matMulComponent = input_data.T @ transpose_weights
    打印（len（matMulComponent））

    z = matMulComponent + self.biases.T
    f_wb = self.act_fun(z)
    
    

    self.output = f_wb.reshape(-1, 1)
    print(f&quot;形状结果：{self.output.shape}&quot;)
    返回自身输出

问题是我正在进行大量的重塑和转置以获得结果。这似乎效率不高。
所以我的问题是：

这个实施起来好吗（因此会导致效率低下）
有没有更好的方法来实现这个前向传播函数

这就是我的输入数据数组的样子（我刚刚打印它并采取了 ss）。我供参考的输入数据是一个扁平的 28*28 数组，每个单元格代表一种颜色。我首先对数据进行标准化（z 分数标准化）
输入数据图像
如果有帮助的话，我还截取了第一层的权重格式的屏幕截图。 （请记住，它在 for_prop 函数中使用之前已被转置）。
第一个隐藏层的权重矩阵图片
前向传播似乎确实有效，但这很好：前向传播进度 ]]></description>
      <guid>https://stackoverflow.com/questions/77790906/matrix-multiplication-in-forward-propogation-for-handritten-digit-recog-algo-ine</guid>
      <pubDate>Wed, 10 Jan 2024 04:20:06 GMT</pubDate>
    </item>
    <item>
      <title>NLTK 词形还原器收到错误 BadZipFile：文件不是 zip 文件</title>
      <link>https://stackoverflow.com/questions/77790772/nltk-lemmatizer-received-error-badzipfile-file-is-not-a-zip-file</link>
      <description><![CDATA[我正在尝试使用 NLTK 包中的词形还原器，但出现此错误：
回溯（最近一次调用最后一次）：

  compat_exec 中的文件 /opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/py3compat.py:356
    exec（代码、全局变量、局部变量）

  文件~/Documents/NLP course/exercise.py:21
    print(wn.lemmatize(&#39;平均值&#39;))

  lemmatize 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/stem/wordnet.py:45
    引理 = wn._morphy(word, pos)

  __getattr__ 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:121
    self.__load()

  __load 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:81
    root = nltk.data.find(f&quot;{self.subdir}/{self.__name}&quot;)

  查找文件/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:555
    返回 find(修改后的名称, 路径)

  查找文件/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:542
    返回 ZipFilePathPointer(p, zipentry)

  _decorator 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/compat.py:41
    返回 init_func(*args, **kwargs)

  __init__ 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:394
    zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))

  _decorator 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/compat.py:41
    返回 init_func(*args, **kwargs)

  __init__ 中的文件 /opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:935
    zipfile.ZipFile.__init__(self, 文件名)

  __init__ 中的文件 /opt/anaconda3/lib/python3.8/zipfile.py:1269
    self._RealGetContents()

  _RealGetContents 中的文件 /opt/anaconda3/lib/python3.8/zipfile.py:1336
    raise BadZipFile(“文件不是 zip 文件”)

BadZipFile：文件不是 zip 文件

我的代码如下：
导入字符串
进口重新
从 nltk.stem 导入 WordNetLemmatizer
wn = WordNetLemmatizer()

print(wn.lemmatize(&#39;平均值&#39;))
print(wn.lemmatize(&#39;含义&#39;))

到目前为止已采取措施但仍无法解决问题：

我尝试使用 conda uninstall nltk 卸载 nltk 软件包并重新下载
我还尝试删除 nltk_data 文件并使用 nltk.download() 再次下载该文件。我注意到随后弹出了下载文件的窗​​口，文件的状态为“已过时”，并且我还收到了错误“下载的 zip 文件出错”

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77790772/nltk-lemmatizer-received-error-badzipfile-file-is-not-a-zip-file</guid>
      <pubDate>Wed, 10 Jan 2024 03:19:26 GMT</pubDate>
    </item>
    <item>
      <title>如何将变量的每种组合纳入机器学习建模？</title>
      <link>https://stackoverflow.com/questions/77790711/how-to-get-every-combination-of-vars-into-ml-modeling</link>
      <description><![CDATA[假设我有 var a 和 b。我正在使用 var a 进行聚类，另一个使用 b 进行聚类，另一个使用 a 和 b 进行聚类。我不知道如何用 python 实现这个。
在下面的代码中，我添加了“#HAS TO BE AUTOMATED”我认为需要自动化的地方
示例数据：
    id 年龄 bp sg al 苏 rbc
0 0 48 80 1.020 1 0 1
1 1 7 50 1.020 4 0 1


id：建模中不需要
年龄 bp sg al su ：数字
rbc ：分类

将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从 kmodes 导入 kprototypes

数据集 = pd.read_csv(..)
df=数据集.copy()


#删除不必要的列
df.drop(列=[“id”],inplace=True)

#标准化
columns_to_normalize = [&#39;age&#39;,&#39;bp&#39;,&#39;sg&#39;,&#39;al&#39;,&#39;su&#39;] #必须自动化
df[columns_to_normalize] = df[columns_to_normalize].apply(lambda x: (x - x.mean()) / np.std(x))


#获取值数组
data_array=df.值


#指定数据类型
data_array[:, 0:4] = data_array[:, 0:4].astype(float) #必须自动化
data_array[:, 5] = data_array[:, 5].astype(str) #必须自动化


#创建未经训练的模型
untrained_model = kprototypes.KPrototypes(n_clusters=2,max_iter=20)


#预测集群
集群 = untrained_model.fit_predict(data_array, categorical=[5])

数据集[“聚类标签”]=聚类
print(&quot;聚类后的数据是：&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/77790711/how-to-get-every-combination-of-vars-into-ml-modeling</guid>
      <pubDate>Wed, 10 Jan 2024 03:01:06 GMT</pubDate>
    </item>
    <item>
      <title>我在视觉变压器中有矩形图像数据集。我设置 image_size= (128, 256) 但补丁大小可能是多少？</title>
      <link>https://stackoverflow.com/questions/77788451/i-have-rectangular-image-dataset-in-vision-transformers-i-set-image-size-128</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77788451/i-have-rectangular-image-dataset-in-vision-transformers-i-set-image-size-128</guid>
      <pubDate>Tue, 09 Jan 2024 16:55:52 GMT</pubDate>
    </item>
    <item>
      <title>计算混淆矩阵的精度和召回率</title>
      <link>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</link>
      <description><![CDATA[正如主题所说，我必须计算混淆矩阵的精度和召回率。

我将如何继续使用这个矩阵来计算精度和召回率？]]></description>
      <guid>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</guid>
      <pubDate>Tue, 09 Jan 2024 08:04:36 GMT</pubDate>
    </item>
    </channel>
</rss>