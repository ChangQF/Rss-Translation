<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 21:13:51 GMT</lastBuildDate>
    <item>
      <title>是否应将多个分类嵌入组合成条件 GAN（cGAN）？</title>
      <link>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</link>
      <description><![CDATA[我正在尝试制作一个条件 GAN (cGAN)，它可以根据标题和视频类别/流派生成 YouTube 缩略图。
它根本不起作用，甚至差得很远，所以我试图回到有关我的架构的基本问题。现在，我所做的是制作两个嵌入向量，一个用于标题，一个用于类别，然后我将它们组合起来并将它们都发送到生成器和鉴别器。
我想知道这样做可以吗？还是我应该分别传递它们？我尝试对其进行一些研究，但这是一个相当小众的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</guid>
      <pubDate>Tue, 18 Jun 2024 21:04:12 GMT</pubDate>
    </item>
    <item>
      <title>未构建顺序模型（Tensorflow）</title>
      <link>https://stackoverflow.com/questions/78639634/sequential-model-not-building-tensorflow</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 在 Python 中构建一个用于训练数据的模型，但构建失败。有人发现问题了吗？
我到目前为止已经尝试过了：
def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories):
tf.random.set_seed(200)
model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),
layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

返回模型

model = create_model(NUM_WORDS, EMBEDDING_DIM，32，16，5）

print（model）

每当我print（model）时，它都会显示&lt;Sequential name=sequation,built=False&gt;。]]></description>
      <guid>https://stackoverflow.com/questions/78639634/sequential-model-not-building-tensorflow</guid>
      <pubDate>Tue, 18 Jun 2024 20:58:02 GMT</pubDate>
    </item>
    <item>
      <title>理解 Transformers 的结果，通过梯度下降进行情境学习</title>
      <link>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</link>
      <description><![CDATA[我正在尝试实现这篇论文：
https://arxiv.org/pdf/2212.07677
（这是他们的代码）：
https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
我正在努力匹配他们的实验结果。具体来说，在他们最简单的 GD 模型（单层、单头、无 softmax）上，他们在测试数据上获得了大约 0.20 的恒定低损失。从概念上讲，我不太明白为什么会这样。
据我所知，这个模型只对数据进行了一次梯度下降迭代，那么为什么它会达到如此低的损失？为什么损失在训练步骤中会保持恒定/接近恒定？我们不是在 GD 模型中训练学习率吗？]]></description>
      <guid>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</guid>
      <pubDate>Tue, 18 Jun 2024 20:43:45 GMT</pubDate>
    </item>
    <item>
      <title>如何将设计元素集成到圆形的三个字母的标签中，以便机器学习系统有效地识别方向？</title>
      <link>https://stackoverflow.com/questions/78639183/how-to-integrate-design-elements-into-a-circular-three-letter-tag-for-effective</link>
      <description><![CDATA[我正在开发一个项目，涉及圆形标签的自动方向识别，每个标签都标有三个字母。这些标签可以从任何角度拍摄。我希望融入有助于确定其方向的设计元素，以便机器学习模型进行后续处理。识别系统可能包括图像转换，然后进行机器学习分析。
约束和目标：
标签是圆形的，具有三个字母的代码，并且需要非侵入性且体积小。
设计元素必须清楚地指示方向，才能被计算机视觉系统有效识别。
首选方法包括使用视觉元素、不同的颜色或纹理，因为这些必须易于被系统识别。
具体问题：
我可以将哪些设计元素融入这些三个字母的标签中，以清楚地指示它们的方向以供 ML 处理？
是否有经过验证的计算机视觉和机器学习方法来处理这种方向识别问题？
如果能提供任何实施示例或关于纹理、图案或其他设计特征类型的建议，这些设计特征在类似场景中可以很好地与机器学习应用程序配合使用，我将不胜感激。
感谢您的建议和指导。]]></description>
      <guid>https://stackoverflow.com/questions/78639183/how-to-integrate-design-elements-into-a-circular-three-letter-tag-for-effective</guid>
      <pubDate>Tue, 18 Jun 2024 18:58:45 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“keras.src.backend”没有属性“convert_to_numpy”</title>
      <link>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</link>
      <description><![CDATA[我尝试使用 utoencoder 和 rus 相应的代码，并在使用 tensorflow 和 keras 时遇到问题，在下面的代码中我展示了代码和相应的错误。当我拟合自动编码器模型时，它显示 AttributeError: module &#39;keras.src.backend&#39; 没有属性 &#39;convert_to_numpy&#39;。我无法理解这个错误和相应的解决方案。对于这种情况我该如何解决我的问题？我使用 anaconda3 运行此代码。我使用 tensorflow 版本 2.16.1 和 keras 版本 3.3.3，错误显示在模型、拟合线中。我尝试使用自动编码器消除噪音，在这种情况下我编写了代码。我尝试运行多次但没有成功。我在 genimi 中写入错误。它向我展示了两种方法 1. 升级 tensorflow 和 keras 2. 不要使用 backend.convert_to_numpy，而是使用推荐的方法在较新版本中将张量转换为 NumPy 数组。]]></description>
      <guid>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</guid>
      <pubDate>Tue, 18 Jun 2024 17:28:14 GMT</pubDate>
    </item>
    <item>
      <title>CNN 训练模型预测值超出训练范围</title>
      <link>https://stackoverflow.com/questions/78638666/cnn-trained-model-predicting-values-outside-of-trained-range</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78638666/cnn-trained-model-predicting-values-outside-of-trained-range</guid>
      <pubDate>Tue, 18 Jun 2024 16:37:55 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle GPU 上的训练模型问题 - 只有一个 GPU 正常工作</title>
      <link>https://stackoverflow.com/questions/78638417/issue-with-training-model-on-kaggle-gpu-only-one-gpu-working</link>
      <description><![CDATA[我目前正在尝试使用 GPU 资源在 Kaggle 上训练模型，但似乎只使用了一个 GPU，而不是多个。我使用以下训练代码：
# 步骤 1：安装所需的软件包
#!pip install ultralytics xmltodict albumentations torch torchvision torchaudio

# 步骤 5：训练 YOLO 模型
import os
import torch
from ultralytics import YOLO

# 将 WANDB_MODE 设置为“dryrun”以禁用 WanDB 日志记录
os.environ[&#39;WANDB_MODE&#39;] = &#39;dryrun&#39;

# 为多个 GPU 设置设备
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model = YOLO(&#39;yolov8x.pt&#39;) # 加载预训练的 YOLOv8 模型

# 检查是否有多个 GPU 可用
if torch.cuda.device_count() &gt; 1：
print(f&quot;使用 {torch.cuda.device_count()} GPU&quot;)
model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count()))).to(device)
else：
model = model.to(device)

# 定义训练配置
data_yaml = &quot;&quot;&quot;
train: /../images/train_combined_data
val: /../images/val
test: /../images/test
nc: 1
names: [&#39;Hotspot&#39;]
&quot;&quot;&quot;

with open(&#39;data.yaml&#39;, &#39;w&#39;) as f:
f.write(data_yaml)

# 训练模型
model.train(
data=&#39;data.yaml&#39;,
epochs=50, # 训练 epoch 总数
batch=16, 
imgsz=640, # 训练的目标图像大小
device=&#39;cuda&#39;
)


我查看了 Kaggle 的文档，它应该支持使用多个 GPU 进行训练。我需要在代码中添加一些特定内容来启用多 GPU 训练吗？或者 Kaggle 上是否有我可能遗漏的设置？
如能就此问题提供任何帮助或指导，我将不胜感激。谢谢！
我该如何使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/78638417/issue-with-training-model-on-kaggle-gpu-only-one-gpu-working</guid>
      <pubDate>Tue, 18 Jun 2024 15:40:00 GMT</pubDate>
    </item>
    <item>
      <title>两个单一模型还是一个多类模型？</title>
      <link>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</link>
      <description><![CDATA[我需要预测下一个目标事件 - 购买两个价格类别的汽车（例如，高档和中档）。训练的目标数量大致相同（假设每个价格类别有 10,000 次购买）。我在这里看到两种训练机器学习模型的方法。第一种是多类模型。第二种是两个单独的模型来预测每个细分市场。您认为应该采用哪种方法，为什么？我想以多类模型为基础，您能列出哪些优点和缺点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</guid>
      <pubDate>Tue, 18 Jun 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有特定文档的情况下估计在大型语言模型上运行推理的硬件要求？[关闭]</title>
      <link>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</link>
      <description><![CDATA[我有兴趣在我的计算机上本地运行一个大型语言模型进行推理。我想选择一个可以在线免费访问的模型，但具体的 CPU/RAM 或 GPU/VRAM 要求通常不会在他们的 Hugging Face 或 Azure 页面上提供，而且我在网上其他地方也找不到它们。
当除了上下文长度和参数数量之外没有提供任何要求信息时，我该如何估计给定模型的硬件要求？
例如：我想在我的计算机上运行最大可能的 Phi-3 模型版本，但我在模型页面或网络上找不到任何要求信息。最大的版本之一是这个，上下文长度为 128_000，参数为 14B，我该如何推断硬件要求？]]></description>
      <guid>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</guid>
      <pubDate>Tue, 18 Jun 2024 13:56:25 GMT</pubDate>
    </item>
    <item>
      <title>如何准确检测不同音轨中主节拍和配乐的开始？</title>
      <link>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</link>
      <description><![CDATA[我正在做一个需要编辑配乐的项目。挑战在于检测任何给定配乐的主要节拍和旋律何时得到正确发展。我确信有更好的术语来描述我的目标，但理想情况下，我想跳过“构建”并立即让歌曲从“主要部分”开始。这需要适用于不同类型的各种歌曲，这些歌曲通常具有不同的结构和开始模式，这使得简化流程变得困难。
例如：
https://www.youtube.com/watch?v=P77CNtHrnmI -&gt;我希望我的代码能够识别 0:24 处的开始
https://www.youtube.com/watch?v=OOsPCR8SyRo -&gt; 0:12 处的开始检测
https://www.youtube.com/watch?v=XKiZBlelIzc -&gt; 0:19 处的起始检测
我尝试使用 librosa 分析起始强度并检测节拍，但当前的实现要么检测到歌曲的最开始，要么无法一致地识别节拍何时完全形成。
这是我的方法；
def analyze_and_edit_audio(input_file, output_file):
y, sr = librosa.load(input_file)
tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
beat_times = librosa.frames_to_time(beat_frames, sr=sr)
main_beat_start = beat_times[0]

我对 librosa/audio 编辑经验很少，因此如果您有任何建议，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</guid>
      <pubDate>Tue, 18 Jun 2024 10:35:15 GMT</pubDate>
    </item>
    <item>
      <title>fragment_anything_fast SamAutomaticMaskGenerator 抛出 BackendCompilerFailed-Error</title>
      <link>https://stackoverflow.com/questions/78636270/segment-anything-fast-samautomaticmaskgenerator-throwing-a-backendcompilerfailed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636270/segment-anything-fast-samautomaticmaskgenerator-throwing-a-backendcompilerfailed</guid>
      <pubDate>Tue, 18 Jun 2024 08:29:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 PaddleOCR 进行账单 OCR</title>
      <link>https://stackoverflow.com/questions/78636112/ocr-bill-using-paddleocr</link>
      <description><![CDATA[目前在学习OCR，遇到一个发票问题，需要用OCR从PDF文件中提取表格，但是用PaddleOCR时，有很多文本部分有误或缺失，表格结构和原文件不一样，请给点思路，谢谢
目前在学习PaddleOCR时，遇到很多文本部分有误或缺失，表格结构和原文件不一样，请给点思路和解决方案，谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78636112/ocr-bill-using-paddleocr</guid>
      <pubDate>Tue, 18 Jun 2024 07:48:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有非奇异输出的鉴别器（针对修改后的 WGAN 架构）可能比传统鉴别器表现更好？</title>
      <link>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</link>
      <description><![CDATA[以下是我提出的（修改后的）WGAN 模型的生成器和鉴别器模型架构，用于将降雨数据下采样 4 倍（仅作为测试）。数据集的输入形状是 (8030, 14, 21)，我想要的输出形状是 (8030, 28, 42)。
def build_generator(input_shape):
inputs = Input(shape=input_shape, name=&#39;generator_input&#39;)

# 下采样
downsample_3 = conv_block(inputs, 128, (3, 3), &#39;downsample_3&#39;)
downsample_2 = conv_block(downsample_3, 64, (3, 3), &#39;downsample_2&#39;)
downsample_1 = conv_block(downsample_2, 32, (3, 3), &#39;downsample_1&#39;)

# 瓶颈
bottleneck_0 = conv_block(downsample_1, 16, (3, 3), &#39;bottleneck_0&#39;)
bottleneck_00 = conv_block(bottleneck_0, 16, (3, 3), &#39;bottleneck_00&#39;)

# 上采样
upsample_1 = deconv_block(bottleneck_00, 32, (3, 3), &#39;upsample_1&#39;)
upsample_2 = deconv_block(upsample_1, 64, (3, 3), &#39;upsample_2&#39;)
upsample_3 = deconv_block(upsample_2, 128, (3, 3), &#39;upsample_3&#39;)

# 最终上采样至所需形状
output = Conv2DTranspose(filters=1, kernel_size=(3, 3), kernel_initializer=he_normal(), padding=&#39;same&#39;, 
strides=(2, 2), activated=&#39;relu&#39;, name=&#39;final_upsample_conv&#39;)(upsample_3)

model =模型（输入=输入，输出=输出，名称=&#39;generator&#39;）

返回模型

def build_discriminator（输入形状）：
输入层 = 输入（形状=输入形状，名称=&#39;discriminator_input&#39;）

x = conv_block（输入层，过滤器=16，内核大小=（3，3），名称=&#39;conv1&#39;）#，使用批处理规范=False）
x = conv_block（x，过滤器=32，内核大小=（3，3），名称=&#39;conv2&#39;）#使用批处理规范=False）
x = conv_block（x，过滤器=128，内核大小=（3，3），名称=&#39;conv3&#39;）#使用批处理规范=False）

x = MaxPooling2D（）（x）
x = Dropout（0.25）（x）
输出层 = Dense（1，激活=&#39;线性&#39;）（x）

模型= Model(inputs=input_layer, output=output_layer, name=&#39;discriminator&#39;)

返回模型

根据文献中的 WGAN 实现，我使用 RMSprop 优化器代替 ADAM，学习率为 5e-5，动量为 0.5。此外，我将鉴别器权重剪裁为 -1e-2 和 1e-2 之间。传统上（据我所知），鉴别器模型输出单个值损失，表示它是否能够区分真实输出和虚假输出。在我的情况下，鉴别器输出的形状为 (None, 14, 21, 1)，它似乎效果更好，但我不明白为什么。有人知道为什么会发生这种情况吗？提前致谢！
编辑：这些是 conv_block 和 deconv_block 的架构
def conv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2D(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_conv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
x = Dropout(0.25, name=name+&#39;_dropout&#39;)(x)
return x

def deconv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_deconv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
return x

编辑 2：以下是损失函数
def generator_loss(fake_output, real_output, penalty_weight=10):
&quot;&quot;&quot;
使用 Wasserstein 损失并添加惩罚项的生成器损失函数。

参数：
fake_output (tf.Tensor)：给定生成的图像时，判别器的输出。
real_output (tf.Tensor)：给定真实图像时，判别器的输出。
penalty_weight (float)：惩罚项的权重。

返回：
tf.Tensor：生成器损失。
“” “”
wasserstein_loss = -tf.reduce_mean(fake_output)

# 偏离实际输出的惩罚项
penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_output - real_output))

return wasserstein_loss + penalty

def discriminator_loss(real_output, fake_output):
“” “”
使用 Wasserstein 损失的判别器损失函数。

参数：
real_output (tf.Tensor)：给定真实图像时判别器的输出。
fake_output (tf.Tensor)：给定生成图像时判别器的输出。

返回：
tf.Tensor：判别器损失。
“” “”
return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)
]]></description>
      <guid>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</guid>
      <pubDate>Mon, 17 Jun 2024 09:29:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit-learn 在 Python 中对未标记数据实现层次聚类？</title>
      <link>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</link>
      <description><![CDATA[我正在学习聚类，在尝试查找带有标记数据的数据库时遇到了一些问题，这对我来说是一个限制，因为我发现了非常有趣的未标记数据集。我读过各种无监督聚类技术，并想实现层次聚类。
我将数据加载到 pandas DataFrame 中，对数据进行标准化并应用层次聚类。然后我可视化了树状图，但我不确定如何解释结果或我是否使用了正确的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</guid>
      <pubDate>Sat, 15 Jun 2024 04:03:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么单棵树的随机森林比决策树分类器好得多？</title>
      <link>https://stackoverflow.com/questions/48239242/why-is-random-forest-with-a-single-tree-much-better-than-a-decision-tree-classif</link>
      <description><![CDATA[我使用以下代码将决策树分类器和随机森林分类器应用于我的数据：
def decision_tree(train_X, train_Y, test_X, test_Y):

clf = tree.DecisionTreeClassifier()
clf.fit(train_X, train_Y)

return clf.score(test_X, test_Y)

def random_forest(train_X, train_Y, test_X, test_Y):
clf = RandomForestClassifier(n_estimators=1)
clf = clf.fit(X, Y)

return clf.score(test_X, test_Y)

为什么随机森林分类器的结果好得多（运行 100 次，随机抽取 2/3 的数据用于训练，1/3 用于测试）？
100%|██████████████████████████████████████| 100/100 [00:01&lt;00:00, 73.59it/s]
算法：决策树
最小值：0.3883495145631068
最大值：0.6476190476190476
平均值：0.4861783113770316
中位数：0.48868030937802126
标准差：0.047158171852401135
方差：0.0022238931724605985
100%|█████████████████████████████████████████| 100/100 [00:01&lt;00:00, 85.38it/s]
算法：随机森林
最小值：0.6846846846846847
最大值：0.8653846153846154
平均值：0.7894823428836184
中位数：0.7906101571063208
标准差：0.03231671150915106
方差：0.0010443698427656967

具有一个估计量的随机森林估计量不只是决策树吗？
我做错了什么或误解了这个概念吗？]]></description>
      <guid>https://stackoverflow.com/questions/48239242/why-is-random-forest-with-a-single-tree-much-better-than-a-decision-tree-classif</guid>
      <pubDate>Sat, 13 Jan 2018 11:04:39 GMT</pubDate>
    </item>
    </channel>
</rss>