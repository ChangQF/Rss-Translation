<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 14 May 2024 18:18:11 GMT</lastBuildDate>
    <item>
      <title>带训练数据的基本贝叶斯线性回归</title>
      <link>https://stackoverflow.com/questions/78479716/basic-bayesian-linear-regression-with-training-data</link>
      <description><![CDATA[在此处输入图片描述
我有点困惑，因为我以为我的体重参数是w？不管怎样，我真诚地想要验证我是否走在正确的道路上，或者完全被误导了，因为我的可能性总是得到 0。
# 之前的分发
rv = multivariate_normal([0,0,0], (alpha ** (-1)) * np.eye(3))

#可能性
阿尔法 = 0.7
协方差 = (alpha ** (-1)) * np.eye(len(traininDataY))
w = np.array([0, 2.5, -0.5])
TransposeW= w.reshape(-1, 1)
phi = np.column_stack((np.ones_like(x1train), np.power(x1train, 2), np.power(x2train, 3)))
平均值 = np.dot(xmatrix, w_Transpose)
可能性 = multivariate_normal.pdf(tTrain, np.ravel(mean), Cov2)

编辑：
抱歉，我是新来的，我不知道如何上传图片，以便在我的帖子中可见（如果可能的话）。]]></description>
      <guid>https://stackoverflow.com/questions/78479716/basic-bayesian-linear-regression-with-training-data</guid>
      <pubDate>Tue, 14 May 2024 17:19:05 GMT</pubDate>
    </item>
    <item>
      <title>无法腌制本地对象“main.<locals>.preprocess_train”</title>
      <link>https://stackoverflow.com/questions/78479321/cant-pickle-local-object-main-locals-preprocess-train</link>
      <description><![CDATA[我的目标是训练一个lora模型。我是 python 和 AI 的新手，决定使用这个脚本：  https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py 。
当我运行脚本时出现此错误：
`回溯（最近一次调用最后一次）：
文件“C:\Users\PNP\Desktop\Nappi\script\CreazioneLora.py”，第 977 行，在  中
主要的（）
文件“C:\Users\PNP\Desktop\Nappi\script\CreazioneLora.py”，第 726 行，位于 main 中
对于步骤，批量枚举（train_dataloader）：
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\accelerate\data_loader.py”，第 451 行，位于 __iter__ 中
dataloader_iter = super().__iter__()
                  ^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 439 行，位于 __iter__ 中
返回 self._get_iterator()
       ^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 387 行，在 _get_iterator 中
返回_MultiProcessingDataLoaderIter（自身）
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\PNP\Desktop\Nappi\tirocinio-venv\Lib\site-packages\torch\utils\data\dataloader.py”，第 1040 行，位于 __init__ 中
w.start()
文件“C:\Python311\Lib\multiprocessing\process.py”，第 121 行，在 start`
`self._popen = self._Popen(self)
              ^^^^^^^^^^^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\context.py”，第 224 行，位于 _Popen
返回 _default_context.get_context().Process._Popen(process_obj)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\context.py”，第 336 行，位于 _Popen
返回 Popen(process_obj)
       ^^^^^^^^^^^^^^^^^^^
文件“C:\Python311\Lib\multiprocessing\popen_spawn_win32.py”，第 94 行，位于 __init__ 中
duction.dump(process_obj, to_child)
文件“C:\Python311\Lib\multiprocessing\reduction.py”，第 60 行，转储中
ForkingPickler(文件，协议).dump(obj)
AttributeError：无法腌制本地对象 &#39;main..preprocess_train&#39;`

我通过加速启动和各种输入参数从提示符启动了脚本...可能是什么问题？
我希望它能够根据我作为输入提供的数据库开始训练模型（https:/ /huggingface.co/datasets/TheFusion21/PokemonCards)]]></description>
      <guid>https://stackoverflow.com/questions/78479321/cant-pickle-local-object-main-locals-preprocess-train</guid>
      <pubDate>Tue, 14 May 2024 15:59:52 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 的线性层未训练</title>
      <link>https://stackoverflow.com/questions/78479296/pytorchs-linear-layer-not-training</link>
      <description><![CDATA[我正在尝试实现一个深度嵌入式自组织映射（DESOM），它是一个自动编码器，与可训练的 SOM 一起作为可训练层，我使用线性层实现：
类 SOM(nn.Module)：
    def __init__(
        自身，map_height = 10，map_width = 10，
        Latent_dim = 50，p_norm = 2
    ）：
        超级().__init__()
        
        self.map_height = 地图高度
        self.map_width = map_width
        self.latent_dim = Latent_dim
        self.p_norm = p_norm
        self.som_nodes = self.map_height * self.map_width

        # SOM（扁平化）权重初始化的均匀采样-
        # self.som_wts = torch.distributions.uniform.Uniform(low = - 1 / np.sqrt(latent_space_dim), high = 1 / np.sqrt(latent_space_dim)).sample((m * n, Latent_space_dim))

        # 创建嵌入字典 -
        # self.embedding = nn.Embedding(self.som_nodes, self.latent_dim)
        # self.embedding.weight.data.uniform_(-np.sqrt(1 / self.latent_dim), np.sqrt(1 / self.latent_dim))

        # 使用线性层创建 SOM（无偏差）-
        self.som_wts = nn.Linear（in_features = self.latent_dim，out_features = self.map_height * self.map_width，偏差= False）
        self.som_wts.weight.data.uniform_(-np.sqrt(1 / self.latent_dim), np.sqrt(1 / self.latent_dim))


    def 转发（自身）：
        经过

将 Autoencoder 与 SOM 相结合的整个模型为：
类 DESOM(nn.Module):
    def __init__(
        自我，latent_dim = 50，
        容量= 16，地图高度= 10，
        地图宽度 = 10，p_norm = 2，
    ）：
        超级().__init__()
        self.latent_dim = Latent_dim
        自身容量=容量
        self.map_height = 地图高度
        self.map_width = map_width
        self.p_norm = p_norm

        # tot_train_iterations = num_epochs * len(train_loader)
        # self.decay_vals = list(scheduler(it = step, tot = tot_train_iterations) for step in range(1, tot_train_iterations + 6))
        # self.decay_vals = torch.tensor(np.asarray(decay_vals))

        self.encoder = 编码器(latent_dim = self.latent_dim, 容量 = self.capacity)
        self.decoder = 解码器(latent_dim = self.latent_dim, 容量 = self.capacity)
        self.som = SOM(map_height = self.map_height, map_width = self.map_width, p_norm = self.p_norm)


    def 前向（自身，x）：
        z = self.encoder(x)
        x_recon = self.decoder(z)
        返回 z，x_recon

# 指定 SOM 超参数 -
# m = SOM 高度-
索姆高度 = 20

# n = SOM 宽度-
索姆宽度 = 20

潜在空间暗度 = 50

# 初始化模型-
# 初始化DESOM模型-
模型 = DESOM(
    Latent_dim = Latent_space_dim，容量 = 16，
    地图高度 = 索姆高度，地图宽度 = 索姆宽度，
    p_norm = p_norm
）

model.som.som_wts.weight.shape
# 火炬.Size([400, 50])

# 随机初始化权重-
model.som.som_wts.weight.min().item(), model.som.som_wts.weight.max().item()
＃（-0.14141587913036346，0.14140239357948303）

在train_one_epoch()函数中，训练自动编码器和SOM层的主要代码是：
# 获取潜在代码并重构-
z, x_recon = 模型(x)

优化器.zero_grad()

# 自动编码器重建损失-
recon_loss = F.mse_loss(输入 = x_recon, 目标 = x)

# SOM 训练代码-
l2_dist_z_soms = torch.cdist(x1 = z, x2 = model.som.som_wts.weight, p = p_norm)
Mindist，bmu_indices = torch.min（l2_dist_z_soms，-1）
bmu_locations = 位置[bmu_indices]
squared_l2_norm_dists = torch.square(torch.cdist(x1 = bmu_locations, x2 = 位置, p = p_norm))

# 计算当前迭代/步骤的西格玛 -
全局步骤
curr_sigma_val = sigma_0 * torch.exp(-step / lmbda_val)
步骤 += 1

# 计算高斯地形邻域-
topo_neighb = torch.exp(-squared_l2_norm_dists / ((2 * torch.square(curr_sigma_val)) + 1e-6))

# 计算地形损失-
topo_loss = topo_neighb * squared_l2_norm_dists

# 沿所有 SOM 单位求和并沿批次求平均值 -
topo_loss = topo_loss.sum(1).mean()

# 计算总损失-
总损失 = 侦察损失 + (gamma * topo_loss)
# 伽玛 = 0.001

# 计算梯度与计算损失 -
总损失.backward()
        
# 执行一步梯度下降-
优化器.step()

整个代码可以参考此处。为了简洁起见，我省略了其他部分。
但是训练完成后，当我看到 SOM 层的训练权重时 -
model.som.som_wts.weight.min().item(), model.som.som_wts.weight.max().item()
＃（-0.14141587913036346，0.14140239357948303）

model.som.som_wts.weight.requires_grad
＃ 真的

作为线性层的 SOM 似乎并未经过训练！是什么阻止它接受训练？]]></description>
      <guid>https://stackoverflow.com/questions/78479296/pytorchs-linear-layer-not-training</guid>
      <pubDate>Tue, 14 May 2024 15:56:30 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型签名/水印</title>
      <link>https://stackoverflow.com/questions/78479253/ml-model-signing-watermarking</link>
      <description><![CDATA[我不是机器学习工程师，但我需要研究一项服务是否可以验证另一项服务是否使用特定的机器学习模型进行计算。这个想法是“验证者”服务正在调用“worker”具有输入的服务以及将在工作端处理输入的预期模型。工作人员正在响应模型提出的预测。然后验证器服务必须确认使用了正确的模型。此外，Miner 可以随时重新训练模型。
我在这里有什么选择？是否有任何已知的良好做法？]]></description>
      <guid>https://stackoverflow.com/questions/78479253/ml-model-signing-watermarking</guid>
      <pubDate>Tue, 14 May 2024 15:49:25 GMT</pubDate>
    </item>
    <item>
      <title>调查 TensorFlow 和 PyTorch 性能的差异</title>
      <link>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</guid>
      <pubDate>Tue, 14 May 2024 13:54:26 GMT</pubDate>
    </item>
    <item>
      <title>LinearRegression：即使在管道中有 Simple Imputer 后也会出现 NaN 错误</title>
      <link>https://stackoverflow.com/questions/78478326/linearregression-nan-error-even-after-having-simple-imputer-in-the-pipeline</link>
      <description><![CDATA[我在经典的加州住房数据集上尝试预测房屋价值中位数。因此，数据集包含卧室总数的 NaN 值。我使用 Simple Imputer 将它们替换为中值，但当我继续训练模型时，我仍然得到 NaN 值。现在，输入器使用的策略是中值，因此传统的恒定策略方法不起作用。我检查了数据框，经过预处理后没有任何数据框。但是，当我尝试通过管道预处理并一起运行模型时，我不知道出了什么问题。我复制了一些与 Jupyter 笔记本相关的代码与其余代码的链接，以供参考。任何帮助将不胜感激。谢谢！
https://colab.research.google.com/drive/1gpaI2xJE2tY0gxEAD1oFGUGsBgRIal5q?usp=分享
从 sklearn.cluster 导入 KMeans
从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.metrics.pairwise 导入 rbf_kernel
类 ClusterSimilarity(BaseEstimator, TransformerMixin):
  def __init__(self, n_clusters=10, gamma=1.0,random_state=None):
    self.n_clusters=n_clusters
    self.gamma=gamma
    self.random_state=随机状态
  def fit(self, X,y=无,sample_weight=无):
    self.kmeans_=KMeans(self.n_clusters,random_state=self.random_state,n_init=10)
    self.kmeans_.fit(X,样本权重=样本权重)
    返回自我
  def 变换（自身，X）：
    返回 rbf_kernel(X, self.kmeans_.cluster_centers_,gamma=self.gamma)
  def get_feature_names_out(self,names=None):
    return [f“聚类{i}相似度”对于范围内的 i(self.n_clusters)]

def column_ratio(X):
  返回 X[:,[0]]/X[:,[1]]
defratio_name(function_transformer,feature_names_in):
  返回[&#39;比例&#39;]
defratio_pipeline():
  返回 make_pipeline(SimpleImputer(strategy=&#39;median&#39;,missing_values=pd.NA),FunctionTransformer(column_ratio,feature_names_out=ratio_name),StandardScaler())
log_pipeline=make_pipeline(SimpleImputer(strategy=&#39;median&#39;,missing_values=pd.NA),FunctionTransformer(np.log,feature_names_out=&#39;一对一&#39;),StandardScaler())
cluster_simil=簇相似度(n_clusters=10,gamma=1.,random_state=69)
default_num_pipeline=make_pipeline(SimpleImputer(strategy=&#39;median&#39;,missing_values=pd.NA),StandardScaler())
预处理=ColumnTransformer([(&quot;bedrooms_per_room&quot;,ratio_pipeline(),[&quot;total_bedrooms&quot;,&quot;total_rooms&quot;]),(&quot;rooms_per_house&quot;,ratio_pipeline(),
 [“total_rooms”,“households”]),(“people_per_house”,ratio_pipeline(),[“population”,“households”]),
  (“log”,log_pipeline,[“total_bedrooms”,“total_rooms”,“人口”,“家庭”,“median_venue”]),(“coordinates_ adjustmentments”,cluster_simil,[“纬度”, “经度”]），
   (“cat”,cat_pipeline,make_column_selector(dtype_include=object))],remainder=default_num_pipeline)

从 sklearn.linear_model 导入 LinearRegression
lin_reg=make_pipeline(预处理, LinearRegression())
lin_reg.fit_transform（外壳，外壳标签）

我尝试在管道中使用 SimpleImputer 来删除 NaN 值，但即使在那之后我也收到 NaN 错误。我手动检查了我的管道并对其进行了 NaN 值测试，但数据为空，这意味着不存在 NaN 值，但当我尝试将其用于线性回归模型时，出现了 NaN 错误。除了“None”之外，我还将 Missing_values 用作 pd.NA 和 np.none，并且全部不执行任何操作。]]></description>
      <guid>https://stackoverflow.com/questions/78478326/linearregression-nan-error-even-after-having-simple-imputer-in-the-pipeline</guid>
      <pubDate>Tue, 14 May 2024 13:09:36 GMT</pubDate>
    </item>
    <item>
      <title>Azure 助理 API 文件搜索和响应时间问题</title>
      <link>https://stackoverflow.com/questions/78477560/issues-with-azure-assistant-api-file-search-and-response-times</link>
      <description><![CDATA[我目前正在使用 Azure Assistant API，并面临一些挑战。具体来说，当我尝试添加文件并使用函数调用时，文件搜索功能无法按预期工作。尽管提供了正确的说明，API 经常会回复“我没有该信息”。即使文件中提供了必要的数据。
我已使用 GPT-3.5 Turbo、GPT-4 和 GPT-4 Turbo 对此进行了测试，但该问题在所有型号上仍然存在。此外，与标准 OpenAI Assistant API 相比，响应时间明显更长，在文件搜索、函数调用和响应时间方面表现良好。
我需要有关此问题的进一步支持。]]></description>
      <guid>https://stackoverflow.com/questions/78477560/issues-with-azure-assistant-api-file-search-and-response-times</guid>
      <pubDate>Tue, 14 May 2024 10:52:10 GMT</pubDate>
    </item>
    <item>
      <title>OSError：[model] 似乎没有名为 config.json 的文件</title>
      <link>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</link>
      <description><![CDATA[我想加载一个拥抱模型。 我要加载的模型大约有 15 万次下载所以我不认为模型本身有什么问题。
使用下面的两个加载代码我得到相同的错误：
从变压器导入 AutoModel
AutoModel.from_pretrained(“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”)

还有
从 Transformers 导入 CLIPProcessor、CLIPModel
model_id =“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”
处理器 = CLIPProcessor.from_pretrained(model_id)
模型 = CLIPModel.from_pretrained(model_id)

两者我都得到：
OSError：laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup 似乎没有名为 preprocessor_config.json 的文件。查看“https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/main”以获取可用文件。

任何加载模型的帮助将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</guid>
      <pubDate>Mon, 13 May 2024 19:38:47 GMT</pubDate>
    </item>
    <item>
      <title>在深度训练/验证循环期间使用分层 k 折叠时出现越界错误</title>
      <link>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</guid>
      <pubDate>Mon, 13 May 2024 14:43:39 GMT</pubDate>
    </item>
    <item>
      <title>如何实现每层有多个类的多标签层次分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78468216/how-to-implement-multi-label-hierarchical-classification-with-multiple-classes-a</link>
      <description><![CDATA[我正在开发一个需要多标签层次分类的项目，其中层次结构的每个级别都有多个要预测的类。具体来说，我正在处理一个场景，其中标签遵循​​层次结构，并且在层次结构的每个级别上，都有多个可能的类。
我的项目旨在根据 Instagram 用户的兴趣对他们进行分类。目标是将用户分类为分层兴趣组，其中每个用户可能属于层次结构不同级别的多个类别。
例如，考虑克里斯蒂亚诺·罗纳尔多作为示例用户，他的主要兴趣可能是“体育”和“家人和朋友”作为顶级品类，进一步细化“体育”类分类为“足球”和“手球” （假设）作为同一级别的子类别。
我已经探索了一些现有的分层分类方法，但它们似乎专注于单标签分类或假设每个级别都有一个类别。任何人都可以建议一种方法或为我提供资源来实现每个级别有多个类的多标签层次分类吗？
任何见解、算法或代码示例将不胜感激。
预先感谢您的帮助！
我一直在尝试使用多标签层次分类来根据 Instagram 用户的兴趣对他们进行分类。但是，我的方法遇到了一个限制：它似乎只在层次结构的每一级别对一个类进行分类，这不适合我的用例。]]></description>
      <guid>https://stackoverflow.com/questions/78468216/how-to-implement-multi-label-hierarchical-classification-with-multiple-classes-a</guid>
      <pubDate>Sun, 12 May 2024 14:35:02 GMT</pubDate>
    </item>
    <item>
      <title>不平衡学习管道的哪些部分应用于测试集？</title>
      <link>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</link>
      <description><![CDATA[我创建了一个由 RobustScaler、SMOTE-NC、 组成的 imbalanced-learn Pipeline随机欠采样和随机森林分类器。
RandomSearchCV 用于选择最佳的超参数。
我想在我的测试集上测试最佳估计器。
cv = RepeatedStratifiedKFold(n_splits=5,
                             n_重复=10，
                             随机状态=42
）

缩放器 = RobustScaler(quantile_range=(25.0, 75.0))

smote = SMOTENC（
    分类特征=分类特征，
    采样策略=0.35，
    随机状态=42
）
rus = RandomUnderSampler(sampling_strategy=0.35, random_state=42)

分类器 = RandomForestClassifier(random_state=42)

管道=不平衡_make_pipeline（缩放器，smote，rus，分类器）

random_search = RandomizedSearchCV(
    管道，
    param_distributions=参数，
    评分=scoring_metric，
    简历=简历，
    n_iter=10,
    随机状态=42，
    n_工作=-1，
）

best_model = random_search.fit(X_train, y_train).best_estimator_

y_pred = best_model.predict(X_test)

据我了解，只有缩放（通过X_train获得的设置）和分类器应该应用于测试集。 SMOTE 和 RandomUndersampling 不应应用于 X_test。
这是由 imbalanced-learn 管道保证的还是我必须考虑其他事情？]]></description>
      <guid>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</guid>
      <pubDate>Fri, 10 May 2024 21:28:24 GMT</pubDate>
    </item>
    <item>
      <title>ValueError: matmul: 输入操作数 1 的核心维度 0 不匹配，gufunc 签名为 (n?,k),(k,m?)->(n?,m?)（大小 5 与 3 不同）</title>
      <link>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</link>
      <description><![CDATA[将 numpy 导入为 np
从 numpy.linalg 导入 inv
从 scipy.linalg 导入 pinv

# 定义必要的函数
def create_laplacian_from_adjacency(adj_matrix):
    Degree_matrix = np.diag(adj_matrix.sum(axis=1))

    laplacian_matrix = Degree_matrix - adj_matrix
    
    返回拉普拉斯矩阵

def dirichlet_energy(L, X):
    “”““用于平滑度量化的狄利克雷能量。”“””
    返回 np.trace(X.T @ L @ X)

def update_C(X, X_tilde, L, C, gamma, alpha, lam, J):
    “”“”使用具有主函数近似的梯度下降来更新C。
    p, k = C.shape
    C_old = np.copy(C)
    
    梯度_f = (-2 * gamma * L @ C_old @ inv(C_old.T @ L @ C_old + J) +
                  alpha * (C_old @ X_tilde - X) @ X_tilde.T +
                  2 * L @ C_old @ X_tilde @ X_tilde.T + lam * C_old @ np.ones((k, k)))
    
    # 主要函数优化步骤（简化方法）
    t = 0.01 # 学习率，需要根据实际应用进行调整
    C_new = pinv(C_old - t * 梯度_f)
    C_new = np.maximum(C_new, 0) # 强制非负性
    返回C_new

def update_X(X, L, C, alpha):
    “”“基于更新的C更新X(tilda)。”“”“
    inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C)
    X_tilde_new = inv_matrix @ C.T @ X
    返回 X_tilde_new

def fgc_algorithm(X, L, alpha, gamma, lam, iterations=5):
    “”“”执行特征图粗化算法。“”“”
    p, n = X.形状
    k = 3 # 假设粗化的降维为 3
    C = np.random.rand(p, k)*0.1
    J = np.full((k, k), 1/k)
    X_代字号 = pinv(C)@X

    对于范围内的 i（迭代）：
        C = update_C(X, X_tilde, L, C, gamma, alpha, lam, J)
        X_tilde = update_X(X, L, C, alpha)
        当前能量 = dirichlet_energy(L, X_tilde)
        print(f&quot;迭代 {i}: 狄利克雷能量 = {current_energy}&quot;)

    L_c=C.T@L@C
    返回 C、L_c、X_tilde

＃ 例子：
X = np.random.rand(5, 7) # 10 个节点的随机特征
adj_matrix = np.array([
    [0, 1, 0, 0, 0],
    [1, 0, 1, 1, 1],
    [0, 1, 0, 1, 0],
    [0, 1, 1, 0, 1],
    [1, 1, 0, 1, 0]
]）
L = create_laplacian_from_adjacency(adj_matrix) # 创建样本拉普拉斯矩阵
alpha, gamma, lam = 0.1, 1, 0.5 # 正则化参数

C、L_c、X_tilde = fgc_algorithm(X、L、alpha、gamma、lam)
print(&quot;更新的 C 矩阵：\n&quot;, C)
print(&quot;L_C 矩阵:\n&quot;, L_c)
print(&quot;更新后的特征矩阵 X(tilda):\n&quot;, X_tilde)


我正在尝试实现特色粗化图算法，但每次代码到达第 34 行时：inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C),
出现了上述错误。
我已经尝试检查所有内容，但根据我的说法，矩阵的尺寸是正确的，所以我不太明白为什么会出现这个问题？
如果您碰巧明白这一点，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</guid>
      <pubDate>Fri, 10 May 2024 14:28:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在短时间内建立准确的数据集？</title>
      <link>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</link>
      <description><![CDATA[我们正在开发一款 iOS 应用，让用户可以发送可定制的数字卡片。用户可以从各种卡片模板中进行选择，输入自己的文本，并根据自己的喜好对卡片进行编辑。我们还有一项功能，用户可以提供短信，例如“妈妈生日快乐”，并收到文本的扩展版本，例如“祝我特别的母亲生日快乐！”我爱你，希望你度过愉快的一天。”
我正在研究如何实现这一目标，并计划使用自然语言处理 (NLP) 和 CoreML 创建一个模型。然而，我在为这个特定任务寻找合适的数据集时遇到了问题。因此，我有兴趣构建专门为此目的而定制的准确数据集。但是，我不确定从哪里可以获得必要的数据，或者是否有其他数据源可供快速使用。
如果您有任何见解或替代方法来实现此功能，请分享。]]></description>
      <guid>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</guid>
      <pubDate>Thu, 02 May 2024 09:18:54 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 cv2.dnn.readNetFromCaffe() 的 net.forward() 性能，net.forward 需要更多时间（7 到 10 秒/帧）才能给出结果</title>
      <link>https://stackoverflow.com/questions/54488986/how-to-improve-performance-net-forward-of-cv2-dnn-readnetfromcaffe-net-for</link>
      <description><![CDATA[我使用了net = cv2.dnn.readNetFromCaffe(protoFile, WeightsFile)，然后循环播放实时视频帧以使用net.forward()&lt;来获取每个帧的输出&lt; /代码&gt;.
但是 net.forward() 每帧需要 7 到 10 秒才能给出结果。请帮助我如何提高性能（减少 net.forward() 中的处理时间）。
意思是：从Step1到Step2每帧需要7到10秒。
（下面的代码中提到了Step1和Step2）。

&lt;前&gt;&lt;代码&gt;导入cv2
导入时间
将 numpy 导入为 np

protoFile =“部署.prototxt”
权重文件=“iter_10.caffemodel”

宽度 = 300
高度 = 300

＃ 网络摄像头
上限 = cv2.VideoCapture(0)
hasFrame,frame = cap.read()

net = cv2.dnn.readNetFromCaffe(protoFile,weightsFile)
k = 0
而1：
    k+=1
    t = 时间.time()
    print(&quot;开始时间 = {}&quot;.format(t))
    hasFrame,frame = cap.read()

    如果没有hasFrame：
        cv2.waitKey()
        print(&quot;请稍等====&gt;&quot;)
        休息

    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),
                              (0, 0, 0)，swapRB=False，crop=False)


    net.setInput(inpBlob)

    ＃ 步骤1
    print(&quot;前向 = {}&quot;.format(time.time() - t))

    输出 = net.forward()

    ＃ 第2步
    #每帧花费近 7 到 10 秒
    print(&quot;forward = {}&quot;.format(time.time() - t))
]]></description>
      <guid>https://stackoverflow.com/questions/54488986/how-to-improve-performance-net-forward-of-cv2-dnn-readnetfromcaffe-net-for</guid>
      <pubDate>Sat, 02 Feb 2019 00:56:04 GMT</pubDate>
    </item>
    <item>
      <title>文本分类。 TFIDF 和朴素贝叶斯？ [关闭]</title>
      <link>https://stackoverflow.com/questions/43163959/text-classification-tfidf-and-naive-bayes</link>
      <description><![CDATA[我正在尝试执行文本分类任务，其中有大约 500 条餐厅评论的训练数据，这些评论被标记为 12 个类别。我花了比我应该花的时间来实现 TF.IDF 和余弦相似度来对测试数据进行分类，但只得到了一些非常差的结果（0.4 F-measure）。由于现在时间不在我这边，我需要实施一些更有效且没有陡峭学习曲线的东西。我正在考虑将 TF.IDF 值与朴素贝叶斯结合使用。这听起来合理吗？我知道如果我能够以正确的格式获取数据，我可以使用 Scikit learn 来做到这一点。您还有其他建议我考虑吗？]]></description>
      <guid>https://stackoverflow.com/questions/43163959/text-classification-tfidf-and-naive-bayes</guid>
      <pubDate>Sun, 02 Apr 2017 02:21:23 GMT</pubDate>
    </item>
    </channel>
</rss>