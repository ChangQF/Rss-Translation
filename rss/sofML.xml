<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 14 Dec 2024 21:15:23 GMT</lastBuildDate>
    <item>
      <title>在 Jupyter Notebook 上的机器学习项目中获取“TypeError：ufunc‘isnan’不支持输入类型”</title>
      <link>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types-in-a-machin</link>
      <description><![CDATA[我正在做一个机器学习项目，在 Jupyter Notebook 上预测电动汽车的价格。
我运行这些单元：
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
x[col] = le.transform(x[col]) 
print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5，random_state = 0)

r2_score(y_test，lm.predict(x_test))

从 sklearn.tree 导入 DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train，y_train)
r2_score(y_test，regressor.predict(x_test))

r2_score(y_train，regressor.predict(x_train))

uv = np.nanpercentile(df2[&#39;Base MSRP&#39;]，[99])[0]*2

df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv

df2 = df2[df2[&#39;Model Year&#39;] != &#39;N/&#39;] # 过滤掉包含 &#39;Model Year&#39; 的行&#39;N/&#39;

for col in cols:
df2[col] = df2[col].replace(&#39;N/&#39;, -1)
le.fit(df2[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

le = preprocessing.LabelEncoder()

cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]

for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

我收到此错误：
TypeError回溯（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
1 for col in cols:
2 le.fit(t[col])
----&gt; 3 df2[col] = le.transform(df2[col])
4 print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
136 return np.array([])
137 
--&gt; 138 返回 _encode(y, uniques=self.classes_)
139 
140 def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
185 else:
186 if check_unknown:
--&gt; 187 diff = _check_unknown(values, uniques)
188 if diff:
189 raise ValueError(f&quot;y 包含之前未见过的标签：{str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
259 
260 # 检查 known_values 中的 nans
--&gt; 261 if np.isnan(known_values).any():
262 diff_is_nan = np.isnan(diff)
263 if diff_is_nan.any():

TypeError: ufunc &#39;isnan&#39; 不支持输入类型，并且根据转换规则 &#39;&#39;safe&#39;&#39;，无法将输入安全地强制转换为任何受支持的类型

我尝试了什么？
我尝试使用以下代码：
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)

代码给出了具体的错误。
为了解决这个问题，我尝试使用以下代码来插入缺失值（“N/”）而不是删除它：
for col in cols:
le.fit(t[col].fillna(&#39;Missing&#39;)) # 使用“Missing”插入缺失值
df2[col] = le.transform(df2[col].fillna(&#39;Missing&#39;))
print(le.classes_)

但我仍然收到相同的错误。
这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
以下是数据集的链接：
https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79281350/getting-typeerror-ufunc-isnan-not-supported-for-the-input-types-in-a-machin</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNet V2模型（mobilenet_v2_1.4_224.tflite）的概率总是相同的？</title>
      <link>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:10 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法学习和实现集成兼容的对象检测模型？</title>
      <link>https://stackoverflow.com/questions/79280938/is-there-way-to-learn-and-implement-ensemble-compatible-object-detection-models</link>
      <description><![CDATA[我是机器学习的新手。我知道检测模型和其他东西的基础知识。我研究了一些分类模型以及如何将它们组合起来。但我不确定如何将其用于多类对象检测模型。特别是 Yolov8/11 和其他一些单次模型。是否有任何代码或资源可以帮助我组合对象检测模型？
我尝试过一些资源，例如这个“https://github.com/ancasag/ensembleObjectDetection”。但我认为我需要一些更简单的解释。特别是关于加权组合技术。]]></description>
      <guid>https://stackoverflow.com/questions/79280938/is-there-way-to-learn-and-implement-ensemble-compatible-object-detection-models</guid>
      <pubDate>Sat, 14 Dec 2024 15:53:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Flatten 层与具有动态尺寸的输入一起使用？</title>
      <link>https://stackoverflow.com/questions/79280552/how-to-use-the-flatten-layer-with-an-input-that-has-a-dynamically-sized-dimensio</link>
      <description><![CDATA[我有一个模型，其输入（一批具有形状（高度、宽度、时间）的图像）具有动态大小的维度（时间），该维度仅在运行时确定。但是，Flatten 层需要完全定义的空间维度。代码片段示例：
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# 定义具有未定义维度的输入（无）
input_tensor = Input(shape=(None, 256, 256, None, 13))

# 应用密集层（需要完全定义的形状）
x = Flatten()(input_tensor)
x = Dense(10)(x)

# 构建模型
model = tf.keras.models.Model(inputs=input_tensor, output=x)

model.summary()

这会引发错误：
ValueError：密集层输入的最后一个维度应已定义。未找到。

如何使用 Flatten 而不是 GlobalAveragePooling3D 等替代方案使其工作？我需要保留所有像素级信息。]]></description>
      <guid>https://stackoverflow.com/questions/79280552/how-to-use-the-flatten-layer-with-an-input-that-has-a-dynamically-sized-dimensio</guid>
      <pubDate>Sat, 14 Dec 2024 11:31:35 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林预测 FPL 球员总得分</title>
      <link>https://stackoverflow.com/questions/79280539/predicting-fpl-player-total-points-using-random-forest</link>
      <description><![CDATA[我有一个数据集，其中包含英超联赛（2016-2023 年）大约 100k 个比赛周统计数据。我的目标是预测一名球员在某个比赛周/比赛中将获得多少总分。
我将数据分为训练/测试集，其中训练集包含赛季 &lt; 2022 的统计数据，测试集包含赛季 &gt; 的统计数据2022.
为了说明某位球员的当前状态，我计算了过去 3 个比赛周以下变量的滚动平均值：
进球数、助攻数、零封数、失球数、分钟数、自进球数、扑救数、错失点球数、黄牌数、红牌数、扑救数、影响力、创造力、威胁和 ict_index
然后，我使用这些变量和一些其他变量运行随机森林：
was_home、player_team、opponent_team、opponent_strength、element_type（后卫/中场等）
模型如下所示：
rf &lt;- randomForest(
as.formula(paste(target, &quot;~&quot;, paste(predictors, collapse = &quot; + &quot;))),
data = train,
ntree = 500,
mtry = 7,
nodesize = 10,
significance = TRUE)

这样做我只得到 R^2 约为 57%。所以我的问题是这是否正常，或者我的方法是否出错？我想知道我可以在哪里改进模型，机器学习是否是预测总分的好方法？]]></description>
      <guid>https://stackoverflow.com/questions/79280539/predicting-fpl-player-total-points-using-random-forest</guid>
      <pubDate>Sat, 14 Dec 2024 11:25:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 警告未找到可见的 GPU，正在将设备设置为 CPU</title>
      <link>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</link>
      <description><![CDATA[系统信息

XGBoost 版本：2.1.3
NVIDIA 驱动程序版本：565.57.01
CUDA 版本：12.6（来自 nvcc）和 12.7（来自 nvidia-smi）
GPU：Tesla T4
操作系统：Ubuntu 24.04
Python 版本：3.11.10
torch.cuda.is_available()：True

尽管系统显示 CUDA 和 GPU 可用，但我遇到了来自 XGBoost 的以下警告：

XGBoost 警告：/workspace/src/context.cc:43：未找到可见的 GPU，将设备设置为CPU。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</guid>
      <pubDate>Sat, 14 Dec 2024 09:12:14 GMT</pubDate>
    </item>
    <item>
      <title>结合 RNN 和 FFN [关闭]</title>
      <link>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</link>
      <description><![CDATA[在 FFN 中，我们有一些输入和一些输出，并以此为基础训练模型。在 RNN 中，输入是序列的一段，输出是同一序列的下一个时间步。但是，在我的场景中，我将关节旋转作为输入，将顶点位置作为随时间变化的输出。我不知道如何在 RNN 中处理两个不同的序列（关节旋转和顶点位置）。
我有时间依赖性，并且这两个序列也是相互依赖的。我应该结合使用 FFN 和 RNN 来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</guid>
      <pubDate>Sat, 14 Dec 2024 07:42:00 GMT</pubDate>
    </item>
    <item>
      <title>拟合非线性混合模型 [迁移]</title>
      <link>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</link>
      <description><![CDATA[我试图拟合一个非线性混合模型 (nLMM)，以测试某些生物的丰度是否受到导致丰度显著增加的事件后的采样期的影响。
数据显示了一条重要的曲线，这些生物的丰度在事件发生后激增（事件发生在采样期：-1 和 1 之间），但随后下降。
我试图构建一个非线性混合模型，但我发现理解如何构建模型非常具有挑战性（例如，model &lt;- lmer(abundance ~ samples_period + (1 | rep), data = data）。我非常感谢任何帮助来确定丰度是否受到采样期的影响。
data &lt;- data.frame(
abundant = c(79, 72, 58, 61, 88, 123, 119, 96, 67, 78, 143, 75, 105, 46, 58, 
127, 173, 181, 67, 120, 64, 30, 49, 47, 104, 83, 146, 118, 53, 
98, 223, 257, 255, 292, 354, 133, 129, 140, 27, 55, 68, 148, 
122, 132, 77, 121, 108, 109),
rep = c(&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, &quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, “T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T3”、“T1”、“T2”、“T3”、“
“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“ “T3”, “T1”, “T2”, “T3”, 
“T1”, “T2”, “T3”, “T1”, “T2”, “T3”, “T1”, “T2”, “T3”, “T1”, “T2”, “T3”, “T1”, “T2”, “T3”, “T1”, “T2”, “T3”),
sampling_period_consecutive = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 
6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 
10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 14, 
14, 14, 15, 15, 15, 16, 16, 16),
采样周期 = c(-5, -5, -5, -4, -4, -4, -3, -3, -3, -2, -2, -2, -1, -1, 
-1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 
6, 11, 11, 11, 22, 22, 22, 34, 34, 34, 46, 46, 58, 
58, 58)
)

]]></description>
      <guid>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</guid>
      <pubDate>Fri, 13 Dec 2024 19:32:50 GMT</pubDate>
    </item>
    <item>
      <title>pytorch CNN 是否关心图像大小？</title>
      <link>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</link>
      <description><![CDATA[我最近在玩 CNN，我有如下粘贴的代码。我的问题是，这适用于任何图像大小吗？我不清楚哪个参数或通道（如果有的话）关心图像大小？如果是这样的话，模型如何知道它需要多少个神经元，这不是图像大小的函数吗？
关于预训练模型的相关点 - 如果我使用预训练模型，我是否需要重新格式化我的图像以使其与模型最初训练的图像相同，或者它如何工作？
class CNN(nn.Module):
def __init__(self, num_classes, num_channels=1):
super(CNN, self).__init__()
self.num_classes = num_classes
self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
self.relu1 = nn.ReLU()
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
self.relu2 = nn.ReLU()
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
self.fc = nn.Linear(64*7*7, num_classes)
]]></description>
      <guid>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</guid>
      <pubDate>Fri, 13 Dec 2024 17:24:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 CPU 上使用 llama_cpp 运行 LLaMA 13B 模型会花费过多时间并且产生较差的输出？</title>
      <link>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</guid>
      <pubDate>Fri, 13 Dec 2024 16:45:38 GMT</pubDate>
    </item>
    <item>
      <title>将加速光线行进应用于 NeRF 实现</title>
      <link>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</guid>
      <pubDate>Fri, 13 Dec 2024 15:46:34 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>“发现输入变量的样本数量不一致”我在 train_test_split 过程中做错了什么吗？</title>
      <link>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</link>
      <description><![CDATA[我正在尝试逻辑回归模型，并运行一些测试，但我一直收到此错误。不太确定我和其他人做了什么不同的事情
from sklearn import preprocessing
X = df.iloc[:,:len(df.columns)-1]
y = df.iloc[:,len(df.columns)-1]ere

这是我分离列的方式
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

TTS
logReg = LogisticRegression(n_jobs=-1)
logReg.fit(X_train, y_train)

y_pred = logReg.predict(X_train)

mae = mean_absolute_error(y_test, y_pred)
print(&quot;MAE:&quot; , mae)

ValueError Traceback（最近一次调用最后一次）
Cell In [112]，第 1 行
----&gt; 1 mae = mean_absolute_error(y_test, y_pred)
2 print(&quot;MAE:&quot; , mae)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:196, in mean_absolute_error(y_true, y_pred, sample_weight, multioutput)
141 def mean_absolute_error(
142 y_true, y_pred, *, sample_weight=None, multioutput=&quot;uniform_average&quot;
143 ):
144 &quot;&quot;&quot;平均绝对误差回归损失。
145 
146 更多信息请阅读 :ref:`用户指南 &lt;mean_absolute_error&gt;`。
(...)
194 0.85...
195 “” “”
--&gt; 196 y_type, y_true, y_pred, multioutput = _check_reg_targets(
197 y_true, y_pred, multioutput
198 )
199 check_consistent_length(y_true, y_pred, sample_weight)
200 output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:100，位于 _check_reg_targets(y_true, y_pred, multioutput, dtype)
66 def _check_reg_targets(y_true, y_pred, multioutput, dtype=&quot;numeric&quot;):
67 &quot;&quot;&quot;检查 y_true 和 y_pred 是否属于同一回归任务。
68 
69 参数
(...)
98 正确的关键字。
99 &quot;&quot;&quot;
--&gt; 100 check_consistent_length(y_true, y_pred)
101 y_true = check_array(y_true, Ensure_2d=False, dtype=dtype)
102 y_pred = check_array(y_pred, Ensure_2d=False, dtype=dtype)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py:387，在 check_consistent_length(*arrays) 中
385 uniques = np.unique(lengths)
386 if len(uniques) &gt; 1:
--&gt; 387 raise ValueError(
388 &quot;找到样本数量不一致的输入变量：%r&quot;
389 % [int(l) for l in lengths]
390 )

ValueError: 找到样本数量不一致的输入变量：[25404, 101612]

我以为是我拆分列的方式不对，但这似乎不是问题所在
当测试规模为 50/50 时，它可以工作，但其他测试规模都不起作用]]></description>
      <guid>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</guid>
      <pubDate>Wed, 11 Jan 2023 15:15:49 GMT</pubDate>
    </item>
    <item>
      <title>从头开始训练的 Keras Xception 在历史记录中给出了约 100% 的准确率，但在评估时仅预测 1，准确率为 50%</title>
      <link>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</link>
      <description><![CDATA[我在 keras 上训练 Xception 模型，没有使用预训练权重来解决二元分类问题，结果出现了非常奇怪的行为。历史图显示训练准确率不断增加，直到达到 100%，而验证准确率始终在 50% 左右，因此看起来是过度拟合，但事实并非如此，因为我检查过，即使在训练集上，它也总是预测（接近）1。
这种行为的原因可能是什么？
这是我用来训练的代码。 x_train_xception 已由 keras.applications.xception.preprocess_input 函数预处理。
我使用相同的代码（除了模型创建之外）来训练预训练的 Xception 模型，效果很好
inLayerX = Input((512, 512, 4))
xceptionModel = keras.applications.Xception(include_top = True, weights=None, input_tensor=inLayerX, classes=1, classifier_activation= &#39;sigmoid&#39;)

xceptionModel.compile(loss= &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;])

history = xceptionModel.fit(x_train_xception, y_train, batch_size= batch_size, epochs= epochs, validation_data=(x_val_xception, y_val))

_, accTest = xceptionModel.evaluate(x_test_xception, y_test)
_, accVal = xceptionModel.evaluate(x_val_xception, y_val)
_, accTrain = xceptionModel.evaluate(x_train_xception, y_train)
print(&quot;训练准确率 {:.2%}&quot;.format(accTrain))
print(&quot;验证准确率 {:.2%}&quot;.format(accVal))
print(&quot;测试准确率 {:.2%}&quot;.format(accTest))

输出：
2/2 [================================] - 6s 2s/step - 损失： 1.2063 - 准确率：0.5000
1/1 [==============================] - 4s 4s/步 - 损失：1.2960 - 准确率：0.4667
4/4 [================================] - 5s 1s/步 - 损失：1.2025 - 准确率：0.5083
训练准确率 50.83%
验证准确率 46.67%
测试准确率 50.00%

验证和测试准确率在预期之内，但真正困扰我的是训练准确率，从历史记录来看，我预计训练准确率接近 100%。
模型准确率
模型损失]]></description>
      <guid>https://stackoverflow.com/questions/72930709/keras-xception-trained-from-scratch-give-100-accuracy-in-the-history-but-only</guid>
      <pubDate>Sun, 10 Jul 2022 17:58:52 GMT</pubDate>
    </item>
    <item>
      <title>根据传感器数据进行步态/行走分析</title>
      <link>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</link>
      <description><![CDATA[我组装了一块地毯，里面有 8 个压力传感器。您可以在图片中看到传感器的排列。整个地毯为 80x80 厘米。每个传感器在被按下时都会输出数字信号（0 或 1）。微控制器每 100 毫秒读取一次所有传感器，并输出一个有效载荷字节，其中每个位包含一个三角形的信息。我将所有这些字节存储在一个 100 字节长的数组中。
我需要从这个数组计算步态（方向，用户前进的角度）。用户只是在原地行进，双脚交替抬起和放下。你知道我可以用来做这种分析的任何算法吗？我应该使用机器学习/神经网络吗？语言并不重要，我只需要找出分析这个字节数组的正确方法。谢谢！
]]></description>
      <guid>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</guid>
      <pubDate>Tue, 27 Sep 2016 19:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>