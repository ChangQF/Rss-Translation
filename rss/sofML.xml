<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 06 May 2024 15:14:53 GMT</lastBuildDate>
    <item>
      <title>使用机器学习从图像中提取系统发育树信息</title>
      <link>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</link>
      <description><![CDATA[有多种机器学习模型（Claude、chatGPT 等）可用于从图像中提取机器可读的信息。有没有人见过从已发表的系统发育树图像（互联网上有很多）成功提取 Newick 格式数据（或同等数据）的案例？]]></description>
      <guid>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</guid>
      <pubDate>Mon, 06 May 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>关于用于食谱成分提取的 spaCy NER 模型注释的反馈</title>
      <link>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</link>
      <description><![CDATA[我目前正在训练 spaCy NER 模型，以专门识别和分类食谱中的成分线。目标是从各种食谱中准确提取成分及其数量、单位和制备说明。下面是我如何注释数据的概述：
跨越标签

数量：与单位相关的数字（例如“2”、“3/4”）
成分：实际成分名称（例如“糖”、“牛奶”）
测量单位：测量单位（例如“杯”、“汤匙”）
说明：准备说明（例如“切细丁”、“分开”）

关系标签

quantity_of：将量程标签数量与成分关联起来
action_to：将跨度标签说明与成分关联起来
unit_of：将跨度标签测量单位与数量相关

我提供了两个示例：一个简单的成分系列和一个复杂的成分系列。
简单行
复杂线
我正在寻求有关以下几点的反馈：

注释过度：是否存在太多类别或过于详细的注释，无法有效学习和实际应用？是否应该合并或省略某些类别？
训练数据量：通常建议使用多少带注释的数据来在此类 NER 任务中实现稳健的模型？我想确保该模型在各种配方格式中都是可靠的。

您可以分享的任何见解、经验或资源都会非常有帮助。谢谢！
我尝试过的：
我使用类似的标签设置训练了一个模型，减去关系标签和“测量单位”。跨越标签。我使用来自随机食谱的约 700 个样本对模型进行了训练。结果不佳；该模型很难识别某些方面，特别是在数量及其单位不相邻的情况下（例如 3 瓣大蒜）]]></description>
      <guid>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</guid>
      <pubDate>Mon, 06 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface GPT2 损失计算</title>
      <link>https://stackoverflow.com/questions/78437389/huggingface-gpt2-loss-calculation</link>
      <description><![CDATA[我一直在尝试使用 GPT2 来计算给定句子的损失。原则上我可以通过两种方式实现这一点，一种是将标签传递给模型，另一种是从逻辑。
以 CrossEntropyLoss 作为损失函数 (https:// github.com/huggingface/transformers/blob/391db836ab7ed2ca61c51a7cf1b135b6ab92be58/transformers/modeling_gpt2.py#L539）我可以使用相同的标签来计算它。
但是，当我这样做时，我发现结果不一致。例如：
从变压器导入 AutoModelForCausalLM, AutoTokenizer
进口火炬
导入 torch.nn.function 作为 F


gmodel = AutoModelForCausalLM.from_pretrained(“gpt2”)
tokenizer = AutoTokenizer.from_pretrained(“gpt2”, truncation_side=&#39;right&#39;, pad_side=&#39;right&#39;)
tokenizer.pad_token = tokenizer.eos_token

test1 = &#39;我是第一个测试字符串&#39;
test2 = &#39;我是第二个测试字符串&#39;

使用 torch.no_grad()：
    tokens1 = tokenizer(test1, max_length=1024, padding=True, truncation=True, return_tensors=“pt”)
    tokens2 = tokenizer(test2, max_length=1024, padding=True, truncation=True, return_tensors=“pt”)
    t1 = gmodel(**tokens1, labels=tokens1[“input_ids”])
    t2 = gmodel(**tokens2, labels=tokens2[“input_ids”])
    
打印（t1.损失，t2.损失）
print(F.cross_entropy(t1.logits.view(-1, tokenizer.vocab_size), tokens1[&#39;input_ids&#39;].view(-1),ignore_index=tokenizer.eos_token_id), F.cross_entropy(t2.logits.view( -1, tokenizer.vocab_size), tokens2[&#39;input_ids&#39;].view(-1),ignore_index=tokenizer.eos_token_id))

输出结果为
张量(6.9905) 张量(7.3378)
张量(7.5244) 张量(7.3836)

这不仅不一致（可能是由于模型使用了不同的损失函数），而且它们的顺序发生了变化，因为第一个字符串的损失在第一个实例中较小，但在第二个实例中较大。]]></description>
      <guid>https://stackoverflow.com/questions/78437389/huggingface-gpt2-loss-calculation</guid>
      <pubDate>Mon, 06 May 2024 14:43:10 GMT</pubDate>
    </item>
    <item>
      <title>keras 中的 2 个模型/第二列火车继续第一个模型</title>
      <link>https://stackoverflow.com/questions/78437211/2-models-in-keras-second-trains-continues-on-the-first-model</link>
      <description><![CDATA[我在使用 Keras 时遇到问题，我正在一个接一个地训练两个单独的模型。然而，第二个模型似乎继续使用第一个模型的训练数据或状态。
def create_model():
    图像大小 = 150
    输入形状=（图像大小，图像大小，3）

    pre_trained_model = VGG16（input_shape = input_shape，include_top = False，weights =“imagenet”）
        
    对于 pre_trained_model.layers[:15] 中的层：
        可训练层 = False

    对于 pre_trained_model.layers[15:] 中的层：
        层.可训练= True
        
    last_layer = pre_trained_model.get_layer(&#39;block5_pool&#39;)
    最后的输出=最后的层.输出
        
 …………

    模型 = 模型(pre_trained_model.input, x)

    model.compile(loss=&#39;binary_crossentropy&#39;,
                优化器=优化器.SGD（learning_rate=1e-4，动量=0.9），
                指标=[&#39;acc&#39;])

    模型.summary()
    返回模型

初始化：
&lt;前&gt;&lt;代码&gt;模型 = create_model()

数据生成：
train_datagen = ImageDataGenerator(
    重新缩放=1./255，
    旋转范围=40，
    width_shift_range=0.2，
    height_shift_range=0.2，
    剪切范围=0.2，
    缩放范围=0.2，
    水平翻转=真，）

train_generator = train_datagen.flow_from_directory(
    data_path+“/training_set”，
    目标大小=(150, 150),
    批量大小=5，
    class_mode=&#39;二进制&#39;
）

test_datagen = ImageDataGenerator（重新缩放=1./255）

validation_generator = test_datagen.flow_from_directory(
        data_path+“/test_set”，
        目标大小=(150, 150),
        批量大小=5，
        class_mode=&#39;二进制&#39;）

培训：
历史 = model.fit(
    火车发电机，
    步骤_per_epoch=70, # Anzahl der Batches pro Epoch
    纪元=20，
    验证数据=验证生成器，
    validation_steps=20 # 验证批次的安扎尔
）

第一次训练后，我将所有内容初始化为相同的：
model1 = create_model()

......
history1 = model1.fit(
    火车发电机，
    步骤_per_epoch=70, # Anzahl der Batches pro Epoch
    纪元=20，
    验证数据=验证生成器，
    validation_steps=20 # 验证批次的安扎尔
）

虽然我用
tf.keras.backend.clear_session()

并在单独的函数中实例化每个模型。我处于 acc 为 1.00 的第二个纪元。
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78437211/2-models-in-keras-second-trains-continues-on-the-first-model</guid>
      <pubDate>Mon, 06 May 2024 14:13:34 GMT</pubDate>
    </item>
    <item>
      <title>确定事件的可能原因</title>
      <link>https://stackoverflow.com/questions/78437105/identify-probable-cause-for-an-event</link>
      <description><![CDATA[我正在寻找一些关键字来研究/研究，因为我似乎没有使用正确的关键字......
数据集
我们有一个带有时间戳的事件列表。事件可以是“可能的原因”，也可以是“可能的原因”。 (C) 或“事件” （我）。
问题
查找可能导致事件的可能原因。
虚构因果关系
电压尖峰发生 1 小时后，组件 A 关闭。电压尖峰就是造成这种情况的原因。
问题
我们不知道电压尖峰是导致关机的原因。我们跟踪许多事件，电压尖峰就是其中之一。我们也不知道事件发生后多久，我们只知道事件发生在某个时间。
我们如何将其他事件中的电压尖峰识别为可能的原因？
关键字
我在搜索过程中遇到的一些关键字：自相关、互相关。
但我很挣扎，因为那些似乎主要处理连续数据，而不是离散数据。
有人可以推荐一些合适的关键字进行搜索吗？]]></description>
      <guid>https://stackoverflow.com/questions/78437105/identify-probable-cause-for-an-event</guid>
      <pubDate>Mon, 06 May 2024 13:54:40 GMT</pubDate>
    </item>
    <item>
      <title>如何为 2130 个类别的分类模型配置密集层维度？</title>
      <link>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</link>
      <description><![CDATA[我正在使用 2130 进行汉字分类任务。设置密集层的最佳实践是什么？
&lt;前&gt;&lt;代码&gt;类数 = 2130
SResnetModel = 顺序(
    [
        重新缩放(1./255, input_shape=(224, 224, 3)),
        tflow.keras.applications.ResNet50(
            include_top=假，
            池化=&#39;平均&#39;,
        ),
        展平（），
        密集（2048，激活=&#39;relu&#39;），
        密集（类数，激活 = &#39;softmax&#39;）
    ]
）
SResnetModel.summary()

这是摘要。

我做了一些搜索，发现最好让倒数第二个密集层的尺寸大于类的数量，但我如何配置它。]]></description>
      <guid>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</guid>
      <pubDate>Mon, 06 May 2024 13:33:30 GMT</pubDate>
    </item>
    <item>
      <title>我的独立功能中的字母数字输入 - 错误：ValueError：无法将字符串转换为浮点数：'01232COM002-222'</title>
      <link>https://stackoverflow.com/questions/78436672/alphanumeric-inputs-in-my-independent-feature-error-valueerror-could-not-conv</link>
      <description><![CDATA[我有 2 列，一列是独立的，表示 PART_NO，另一列是相关的，表示数量。
我试图根据特定机器是什么机器来预测其数量。现在的问题是我的 PART_NO 有字母数字字符，例如 01232COM002-222、ABCD/235、GS.612.90-19、123456。
当我使用 RandomForestRegressor 拟合模型时，我得到
错误：无法将字符串转换为浮点数，

我无法用任何内容替换任何斜杠或逗号，因为 PART_NO 是唯一的。
导入 pandas 作为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 GradientBoostingRegressor
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.svm 导入 LinearSVR
从 sklearn. Linear_model 导入 LinearRegression
从sklearn.metrics导入confusion_matrix
从 sklearn.metrics 导入 precision_score
从 sklearn.metrics 导入 f1_score
从 sklearn.metrics 导入分类报告
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.preprocessing 导入 LabelEncoder

文件 = &#39;testData.xlsx&#39;
df = pd.read_excel(文件)
df.head()

print(f&quot;行数 = {df.shape[0]}&quot;)
print(f&quot;列数 = {df.shape[1]}&quot;)

print(&quot;列的数据类型：\n&quot;, df.dtypes)

print(&quot;X 特征和 y 标签&quot;)
X = df[[&#39;PART_NO&#39;]]
y = df[&#39;数量&#39;]
print(&quot;X-特征\n&quot;)
打印(X.head())
打印（“”）
print(&quot;y 标签&quot;)
打印（y.head（））


X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.3、random_state = 42)
print(&quot;训练大小 = &quot;, X_train.shape)
print(&quot;测试尺寸 = &quot;, X_test.shape)
模型 = RandomForestRegressor()
model.fit(X_train, y_train)

我尝试过LabelEncoding、OneHotEncoding，但不起作用，我尝试过字符串替换，但不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78436672/alphanumeric-inputs-in-my-independent-feature-error-valueerror-could-not-conv</guid>
      <pubDate>Mon, 06 May 2024 12:37:58 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列进行需求预测</title>
      <link>https://stackoverflow.com/questions/78436498/demand-forecasting-using-time-series</link>
      <description><![CDATA[有人可以向我提供有关如何使用任何技术预测未来 13 周销售数量的详细解决方案吗？数据集很小，大约76列。数据集如下
日期数量
2022年4月17日 7500
2022年4月24日 5500
2022年5月1日 6000
2022年5月8日 1000
2022年5月15日 31000
2022年5月22日 17500
2022年5月29日 5000
2022年6月5日 10500
2022年6月12日 2994
2022年6月19日 25500
2022年6月26日 20000
2022年7月3日 6500
2022年7月10日 17000
2022年7月17日 22000
2022年7月24日 9000
2022年7月31日 1000
2022年8月7日 9000
2022年8月14日 2500
2022年8月21日 14000
2022年8月28日 13500
2022年9月4日 8500
2022年9月11日 16000
2022年9月18日 15000
2022年9月25日 32000
2022年10月2日 18000
2022年10月9日 7000
2022年10月16日 5000
2022年10月23日 37000
2022年10月30日 8000
2022年11月6日 7000
2022年11月13日 10000
2022年11月20日 20070
2022年11月27日 27000
2022年12月4日 14500
2022年12月11日 2490
2022年12月18日 28500
2022年12月25日 24000
2023年1月1日 10000
2023年1月8日 8500
2023年1月15日 12000
2023年1月22日 28500
2023年2月5日 24000
2023-02-12 13650
2023年2月19日 27000
2023年2月26日 18997
2023-03-05 8500
2023年3月12日 6000
2023-03-19 7000
2023年3月26日 22500
2023年4月2日 13500
2023年4月9日 5000
2023-04-16 6500
2023年4月23日 30500
2023-04-30 13000
2023年5月7日 3000
2023-05-14 8000
2023-05-21 7000
2023年5月28日 22500
2023年6月4日 8500
2023年6月11日 12000
2023年6月18日 14500
2023年6月25日 25500
2023年7月2日 1000
2023年7月9日 22000
2023年7月16日 15500
2023-07-23 14001
2023年8月6日 500
2023年8月13日 2500
2023年8月20日 13500
2023年8月27日 19705
2023年9月3日 5500
2023年9月10日 7000
2023-09-17 8995
2023年9月24日 35999
2023年10月1日 10000

尝试了所有模型，但没有得到准确的结果]]></description>
      <guid>https://stackoverflow.com/questions/78436498/demand-forecasting-using-time-series</guid>
      <pubDate>Mon, 06 May 2024 12:05:18 GMT</pubDate>
    </item>
    <item>
      <title>关于随机梯度下降类的step()方法装饰器的说明</title>
      <link>https://stackoverflow.com/questions/78436387/clarification-about-the-decorator-of-the-step-method-of-the-stochastic-gradien</link>
      <description><![CDATA[在pytorch的SGD类中，step()方法有装饰器_use_grad_for_ Differentiable：
@_use_grad_for_ Differentiable
def 步骤（自我，闭包=无）：
...

通常我希望使用 no_grad 装饰器。
我没有找到有关 _use_grad_for_ Differentiable 的信息，也不知道它的作用。装饰器的源代码可以在文件 torch/optim/optimizer.py 中找到，但它没有文档字符串或有用的注释，并且代码很神秘（对我来说）。为了完成，我将其粘贴到此处：
def _use_grad_for_ Differentiable(func):
 def _use_grad(self, *args, **kwargs):
     导入火炬._dynamo
     prev_grad = torch.is_grad_enabled()
     尝试：
         # 注意下面的图表中断：
         # 我们需要进行图表中断以确保 aot 遵守 no_grad 注释。
         # 这对于性能来说很重要，因为如果没有这个，功能化将产生一个尾声
         # 它更新优化器的变异参数，这对于感应器来说是不可见的，因此，
         # 电感器将为模型中的每个参数进行分配，这很糟糕。
         # 这样，aot 正确地看到这是一个推理图，并且函数化将生成
         # 附加到图表中的尾声，对感应器来说*是*可见的，因此，感应器看到了
         # 步骤到位，能够避免额外的分配。
         # 将来，我们要么 1) 继续向后进行图形中断，所以这个图形中断并不重要
         # 或 2) 有一个完全融合的前向和后向图，默认情况下没有_grad，我们可以删除它
         # 图形中断以允许编译完全融合的 fwd-bwd-optimizer 图形。
         # 请参阅 https://github.com/pytorch/pytorch/issues/104053
         torch.set_grad_enabled(self.defaults[&#39;可微分&#39;])
         torch._dynamo.graph_break()
         ret = func(self, *args, **kwargs)
     最后：
         torch._dynamo.graph_break()
         torch.set_grad_enabled(prev_grad)
     返回 ret
 functools.update_wrapper(_use_grad, func)
 返回_use_grad

我的问题：这个装饰器的作用是什么？为什么不简单地使用 no_grad() 来代替呢？最后，我应该在我的自定义优化器中使用这个装饰器吗？]]></description>
      <guid>https://stackoverflow.com/questions/78436387/clarification-about-the-decorator-of-the-step-method-of-the-stochastic-gradien</guid>
      <pubDate>Mon, 06 May 2024 11:41:58 GMT</pubDate>
    </item>
    <item>
      <title>如何激励强化学习中的有效探索？</title>
      <link>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</link>
      <description><![CDATA[我正在体育馆研究稀疏的山地汽车环境，其中明显的问题是奖励非常稀疏。每一步的奖励是 -1，如果智能体达到目标，则该情节终止。这意味着代理有动力尽快实现目标。
问题是，从我的发现来看，似乎没有一个算法能够始终“幸运”。并在合理的时间内达到目标。
我不想设计特定的奖励，例如动能等。我想要的东西可以推广到不同的环境，理想情况下不需要调整参数。
我认为的解决方案是，探索对我来说需要更有目的性，而不仅仅是随机选择行动。
我研究了神经密度模型（NDM）探索，这看起来很有趣，但我发现它在这个任务中效果不佳，可能是因为状态空间太简单了。
还有其他有前途的技术可以用来使强化学习更加一致吗？尤其是奖励非常稀少？]]></description>
      <guid>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</guid>
      <pubDate>Mon, 06 May 2024 09:47:41 GMT</pubDate>
    </item>
    <item>
      <title>Imblearn 语法无效[重复]</title>
      <link>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</link>
      <description><![CDATA[虽然我已经能够在 jupyter 笔记本中安装 Imblearn ，同时尝试通过以下代码导入管道：
from imblearn.pipeline 导入管道

我收到错误：
文件“C:\Users\ibl165795\AppData\Roaming\Python\Python37\site-
packages\imblearn\utils\_metadata_requests.py”，第 1492 行
    def process_routing(_obj, _method, /, **kwargs):
语法错误：语法无效
]]></description>
      <guid>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</guid>
      <pubDate>Mon, 06 May 2024 09:02:47 GMT</pubDate>
    </item>
    <item>
      <title>为我的 Npy 数据集定义 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</link>
      <description><![CDATA[我需要帮助为我的数据定义火炬模型。我尝试了各种方法，但似乎没有任何效果。与输入尺寸和形状相关的错误不断出现。我该如何解决这些问题？
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
导入 torch.nn.function 作为 f

# 从 .npy 文件加载数据
data = np.load(“其他py文件/project_files/data/train/data.npy”)
print(&quot;数据形状：&quot;, data.shape) # (401, 701, 255)

数据大小 = 数据.形状[0] * 数据.形状[1] * 数据.形状[2]
print(“数据大小：”, data_size) # 71680755

# 从 .npy 文件加载标签数据
labels = np.load(“其他py文件/project_files/data/train/label.npy”)
print(&quot;标签数据形状:&quot;, labels.shape) # (401, 701, 255)

# 将 numpy 数组转换为 PyTorch 张量
data_tensor = torch.Tensor(数据)
labels_tensor = torch.Tensor(标签)


类 MyModel(nn.Module):
    def __init__(自身):
        超级（MyModel，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def 前向（自身，x）：
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        返回x

模型 = MyModel()
打印（模型）

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

数据集 = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(数据集,batch_size=32,shuffle=True)

纪元数 = 10
对于范围内的纪元（num_epochs）：
    运行损失 = 0.0
    对于 i，enumerate(dataloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        outputs = model(inputs.unsqueeze(1)) # 通道维度
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
        如果我％100==99：
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 100}&quot;)
            运行损失 = 0.0


使用 torch.no_grad()：
    Predictions = model(data_tensor.unsqueeze(1)) # 通道维度

控制台输出：
已连接到 pydev 调试器（版本 223.8836.43）
数据形状：(401, 701, 255)
数据大小：71680755
标签数据形状：(401, 701, 255)
我的模型（
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （池）：MaxPool2d（kernel_size = 2，stride = 2，padding = 0，dilation = 1，ceil_mode = False）
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （fc）：线性（in_features = 71680755，out_features = 2，偏差= True）
）

文件“C:\Users\PC1\PycharmProjects\Project1\newmodel2.py”，第 36 行，向前
    x = x.view(-1, self.fc_input_size)
运行时错误：形状“[-1, 71680755]”对于大小 22579200 的输入无效
python-BaseException
]]></description>
      <guid>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</guid>
      <pubDate>Mon, 06 May 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>如何保留机器学习课程中的数学概念？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</link>
      <description><![CDATA[我最近完成了 Deeplearning.ai 在 Coursera 上提供的机器学习专业化以及 ML 和 DS 数学课程。虽然我设法完成了这些课程，但我在保留所教授的数学概念和公式方面面临着挑战。这些课程涵盖了广泛的主题，现在回想起来，我意识到我不记得大部分材料，尤其是详细的数学部分。
我正在寻找有效的策略或工具，以帮助更好地长期掌握和保留这些数学概念。我仍然觉得我还没有完全掌握这些材料。我将不胜感激任何有关学习技巧、资源或任何有助于巩固我对这些概念的理解的具体练习的建议。此外，社区是否有针对适合强化这些知识的后续课程或材料的建议？
我期待一个指导方针。]]></description>
      <guid>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</guid>
      <pubDate>Mon, 06 May 2024 07:31:53 GMT</pubDate>
    </item>
    <item>
      <title>如何将 flax.linen.Module 转换为 torch.nn.Module？</title>
      <link>https://stackoverflow.com/questions/78249695/how-can-i-convert-a-flax-linen-module-to-a-torch-nn-module</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78249695/how-can-i-convert-a-flax-linen-module-to-a-torch-nn-module</guid>
      <pubDate>Sat, 30 Mar 2024 22:24:42 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS() 以 .rda 格式保存。
例如，我将模型训练为：
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)

当我在清除所有环境变量后加载上述模型，并对新数据使用预测函数时：
预测（model_old，newdata = new_data）

我收到以下错误：
eval(Call$data) 中的错误：未找到对象“test”

但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    </channel>
</rss>