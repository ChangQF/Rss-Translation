<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 20 May 2024 09:18:22 GMT</lastBuildDate>
    <item>
      <title>TensorFlow 决策林支持增量学习吗？</title>
      <link>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</link>
      <description><![CDATA[我想预测供应链中的交货时间，其中依赖的特征是材料、供应商、价格、数量等......，这可以通过使用随机森林回归模型来完成，但随机森林回归模型不能支持增量学习，所以我正在寻找其他支持增量学习的模型以及支持GPU的模型。
我尝试过随机森林回归模型和支持向量机，在随机森林回归模型中增量学习是问题，而在 SVM 中处理分类数据是问题。]]></description>
      <guid>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</guid>
      <pubDate>Mon, 20 May 2024 09:08:37 GMT</pubDate>
    </item>
    <item>
      <title>GAN (Pix2Pix) 训练后生成空白白色图像作为输出</title>
      <link>https://stackoverflow.com/questions/78504974/gan-pix2pix-generating-blank-white-image-as-output-after-training</link>
      <description><![CDATA[我训练了一个 Pix2Pix GAN 模型（将卫星图像转换为地图），并想测试该模型的性能。但是，我只能看到空白/白色图像作为输出。我正在使用在训练结束时保存的预训练模型（pix2pix_15000.pth）（即具有所需的生成器和鉴别器损失）。然而，在训练过程中，我可以看到生成的图像与预期的地图（真实标签）相似。
下面是我用来测试模型并检查生成的图像的代码：
input_dim = 3 # 输入通道数
real_dim = 3 # 输出通道数
target_shape = 256 # 图像的目标形状
device = &#39;cpu&#39; # 如果 CUDA 可用，则使用 &#39;cuda&#39;

gen = UNet(input_dim, real_dim).to(设备)
loaded_state = torch.load(“pix2pix_15000.pth”,map_location=torch.device(&#39;cpu&#39;))
gen.load_state_dict(loaded_state[&quot;gen&quot;]) # 调整保存模型的路径
gen.eval() # 将模型设置为评估模式

# 定义转换以预处理输入图像
变换 = 变换.Compose([
    Transforms.Resize((target_shape, target_shape)), # 调整大小以匹配目标形状
    变换.ToTensor(),
    Transforms.Normalize((0.5,), (0.5,)) # 将图像转换为张量
]）

# 加载并预处理图像
image_path = “test/842-sat.jpg” # 卫星图像的路径
图像 = Image.open(image_path).convert(&#39;RGB&#39;)
condition = transform(image).unsqueeze(0) # 添加批量维度
条件最终 = 条件.to(设备)

# 生成输出
使用 torch.no_grad()：
    生成的图像 = gen(条件最终)
    打印（生成的图像）

# 将生成的图像张量转换为PIL图像
to_pil = 变换.ToPILImage()
generated_image = generated_image.squeeze(0).cpu() # 删除batch维度并移至CPU
generated_image_pil = to_pil(生成的图像)

# 显示图像
plt.imshow（生成的图像_pil）
plt.axis(&#39;off&#39;) # 隐藏坐标区
plt.show()

我期望输出是一个地图，对应于卫星图像。请大家提出宝贵的建议，指出我可能错的地方。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78504974/gan-pix2pix-generating-blank-white-image-as-output-after-training</guid>
      <pubDate>Mon, 20 May 2024 06:24:13 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中如何实现数据并行优化器步骤？</title>
      <link>https://stackoverflow.com/questions/78504808/how-is-optimizer-step-implemented-for-data-parallelism-in-pytorch</link>
      <description><![CDATA[我正在尝试在 PyTorch 中从头开始实现数据并行性。为此，我实施了以下步骤：

为每台设备制作模型副本
重新批处理每个模型副本的数据
向前跑动传球
累积梯度并求平均值
优化器采用平均梯度&lt;- 你在这里

我正在尝试找出如何将 pytorch 优化器步骤和手动数据并行性结合起来。目前，我能做到这一点的唯一方法是为每个数据并行模型副本保留优化器的副本 - 这是该代码的简单简化
# 创建优化器，使其步骤更新所有模型的权重
优化器 = [torch.optim.SGD(model.parameters(), lr=0.1) 对于模型中的模型]

# 步进优化器
对于优化器中的优化器：
    优化器.step()

我怀疑这不是人们实现优化器步骤的方式，因为它看起来内存效率不高。在实践中这是如何做到的？有没有一种方法可以将一个优化器附加到多个模型副本？我想了解这样做的正确方法是什么！]]></description>
      <guid>https://stackoverflow.com/questions/78504808/how-is-optimizer-step-implemented-for-data-parallelism-in-pytorch</guid>
      <pubDate>Mon, 20 May 2024 05:32:57 GMT</pubDate>
    </item>
    <item>
      <title>是否可以仅使用线性回归的分类变量来预测连续变量？</title>
      <link>https://stackoverflow.com/questions/78504785/is-it-possible-to-predict-a-continuous-variable-using-only-categorical-variables</link>
      <description><![CDATA[我是机器学习模型的新手，我试图仅使用分类变量来预测连续变量的结果，但在构建模型时，我的模型得分非常低。
这些是我的分类变量。这些数字实际上是每个悬架、车轴、车轮等的类别，我试图预测的是列“MIN”。这是一个数值（轮胎到悬架的间隙）
数据图片
有人可以解释一下如何对这个问题执行线性回归或者我是否需要使用其他类型的模型/方法？在本例中我使用 python 语言。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78504785/is-it-possible-to-predict-a-continuous-variable-using-only-categorical-variables</guid>
      <pubDate>Mon, 20 May 2024 05:25:32 GMT</pubDate>
    </item>
    <item>
      <title>根据钠和蛋白质预测卡路里</title>
      <link>https://stackoverflow.com/questions/78504774/predict-calories-based-on-sodium-and-protein</link>
      <description><![CDATA[我正在做一个与食物推荐相关的团队项目，我的任务是根据蛋白质和钠的输入来预测卡路里。
数据如下：
在此处输入图片描述
我一开始尝试了各种机器学习模型，但当我删除异常值（R 方为 37%）时，准确率很低（XGboost r 方为 25%）。然后我尝试使用神经网络回归并得到以下结果：
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 MinMaxScaler
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense，Dropout
从tensorflow.keras.callbacks导入EarlyStopping
将 matplotlib.pyplot 导入为 plt

# 第 2 步：将数据拆分为特征和目标
X = df_clean[[&#39;蛋白质&#39;, &#39;钠&#39;]]
y = df_clean[&#39;卡路里&#39;]

# 第 3 步：将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 第四步：标准化数据
缩放器 = MinMaxScaler()
X_train = 缩放器.fit_transform(X_train)
X_test = 缩放器.transform(X_test)

# 步骤 5：定义具有附加层和 dropout 的神经网络模型
模型=顺序（）
model.add（密集（128，input_dim = 2，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（64，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（32，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add(Dense(1)) # 回归的输出层

# 第6步：编译模型
model.compile（优化器=&#39;adam&#39;，损失=&#39;mse&#39;，指标=[&#39;mae&#39;]）

# 第 7 步：定义提前停止回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 10，restore_best_weights = True）

# 第 8 步：训练模型
历元 = 20
批量大小 = 32
历史= model.fit（X_train，y_train，epochs = epochs，batch_size = batch_size，validation_data =（X_test，y_test），verbose = 1，callbacks = [early_stopping]）

# 步骤 9：在测试集上评估模型
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
print(f&#39;测试损失: {test_loss:.4f}, 测试 MAE: {test_mae:.4f}&#39;)

# 第10步：对测试集进行预测
y_pred = model.predict(X_test)

# 第 11 步：可视化结果
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=3)
plt.xlabel(&#39;实际卡路里&#39;)
plt.ylabel(&#39;预测卡路里&#39;)
plt.title(&#39;神经网络回归&#39;)
plt.show()

# 第12步：情节训练&amp;验证损失值
plt.figure(figsize=(10, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;训练损失&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证损失&#39;)
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend(loc=&#39;右上&#39;)
plt.show()

在此处输入图片说明
在此处输入图片描述
有人可以建议解决这个问题的方法吗，我是这个领域的新手，所以非常感谢任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78504774/predict-calories-based-on-sodium-and-protein</guid>
      <pubDate>Mon, 20 May 2024 05:21:19 GMT</pubDate>
    </item>
    <item>
      <title>是否有教程可以创建您自己的 PyTorch 模块（线性）、损失（最小二乘）和优化器（梯度下降）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78504763/is-there-a-tutorial-to-create-your-own-pytorch-module-linear-loss-least-squa</link>
      <description><![CDATA[我的目的是为即将到来的九月学年做好准备。
我希望能够编写自己的 PyTorch 模块、损失和优化器。我打算从模块（线性）、损失（最小二乘）和优化器（梯度下降）开始简单。
我的目标是实现：

LeastSquares 损失是 torch.nn.modules.loss._Loss 类的子类。
GradientDescent 优化器，它是 torch.optim.optimizer.Optimizer 类的子类。
Cubic Polynomial 模块，它是 torch.nn.Module 类的子类。

我尝试过的：

阅读通过示例学习 PyTorch
《学习 PyTorch 示例》教授如何定义 autograd 函数。这成为专门的 StackOverflow 问题
其余示例教授如何使用现有的损失函数（例如torch.nn.MSELoss）、优化器（例如torch.optim.SGD）和模块（例如torch.nn.Linear）。


花了一天的时间。我对torch.nn.modules.loss._Loss、torch.optim.optimizer.Optimizer和torch.nn.Module如何感到困惑作品。是否有教程如何创建您自己的 PyTorch 损失、优化器和模块？
下面的代码试图模仿 PyTorch。但是，我不知道 optimizer.zero_grad()、loss.backward() 和 optimizer.step() 如何更新model.parameters()。所以，我把所有东西都放在一类中。
我愿意学习设计模式等。请为我指出正确的方向。
将 numpy 导入为 np
导入数学
将 matplotlib.pyplot 导入为 plt

类模块：pass

三次多项式类（模块）：
  a: np.float64
  b: np.float64
  c: np.float64
  d: np.float64

  学习率：np.float64

  def __init__(self,learning_rate=1e-6) -&gt;;没有任何：
    self.a = np.random.randn()
    self.b = np.random.randn()
    self.c = np.random.randn()
    self.d = np.random.randn()

    自我学习率 = 学习率

  def __call__(self, x: np.ndarray):
    返回 self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
  
  def loss_ln(self, y_pred: np.ndarray, y: np.ndarray) -&gt; np.float64：
    返回 ((y_pred - y) ** 2).sum()
  
  def Zero_grad(self) -&gt;;没有任何：
    # 更新权重后手动将梯度归零
    self.grad_a = 无
    self.grad_b = 无
    self.grad_c = 无
    self.grad_d = 无
  
  def向后(self, y_pred: np.ndarray) -&gt;没有任何：
    # 反向传播计算 a、b、c、d 相对于损失的梯度
    grad_y_pred = 2.0 * (y_pred - y)
    # d/da (y_pred - y)²
    # 2 * (y_pred - y)
    self.grad_a = grad_y_pred.sum()
    
    # d/db (a + (b * x) + (c * x²) + (d * x³) - y)²
    # 2 * (y_pred - y) * x
    self.grad_b = (grad_y_pred * x).sum()

    # d/dc (a + (b * x) + (c * x²) + (d * x³) - y)²
    # 2 * (y_pred - y) * x²
    self.grad_c = (grad_y_pred * x ** 2).sum()

    # d/dd (a + (b * x) + (c * x²) + (d * x³) - y)²
    # 2 * (y_pred - y) * x³
    self.grad_d = (grad_y_pred * x ** 3).sum()

  def 步骤(self) -&gt;没有任何：
    # 使用梯度下降更新权重
    self.a -= self.learning_rate * self.grad_a
    self.b -= self.learning_rate * self.grad_b
    self.c -= self.learning_rate * self.grad_c
    self.d -= self.learning_rate * self.grad_d

dtype = np.float64

# 创建随机输入和输出数据
x = np.linspace(-math.pi, math.pi, 2000, dtype=dtype)
y = np.sin(x)

plt.plot(x, y, &#39;蓝色&#39;)

学习率 = 1e-6
模型=三次多项式（学习率=学习率）

对于范围（2000）内的 t：
  # 前向传递：计算预测 y
  y_pred = 模型(x)

  # 计算并打印损失
  损失 = model.loss_ln(y_pred, y)
  如果 t % 100 == 99：
    打印（t，损失）

  # 在向后传递之前，使用优化器对象将所有
  # 它将更新的变量的梯度（这是可学习的）
  # 模型的权重）。这是因为默认情况下，渐变是
  # 每当 .backward() 时都会在缓冲区中累积（即不被覆盖）
  ＃ 叫做。查看 torch.autograd.backward 的文档以获取更多详细信息。
  model.zero_grad()

  # 向后传递：计算损失相对于模型的梯度
  ＃ 参数
  model.backward(y_pred)

  # 在优化器上调用步骤函数会更新其
  ＃ 参数
  模型.step()


print(f&#39;结果: y = {model.a} + {model.b} x + {model.c} x^2 + {model.d} x^3&#39;)

plt.plot(x, y_pred, &#39;橙色&#39;)

%重置-f
]]></description>
      <guid>https://stackoverflow.com/questions/78504763/is-there-a-tutorial-to-create-your-own-pytorch-module-linear-loss-least-squa</guid>
      <pubDate>Mon, 20 May 2024 05:13:56 GMT</pubDate>
    </item>
    <item>
      <title>我想将分位数回归转换为线性规划问题，我该如何解决这个问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78504275/i-want-to-convert-quantile-regression-into-a-linear-programming-problem-how-can</link>
      <description><![CDATA[我想要分位数回归（转换为线性规划问题，使用sklearn方法解决，但不使用sklearn代码，需要重写，并且不能使用库）代码，klearn是一个软件的界面来解决，有什么解决办法吗？
我尝试使用sklean中的代码，但发现最后是用软件解决的，而且没有解决的代码]]></description>
      <guid>https://stackoverflow.com/questions/78504275/i-want-to-convert-quantile-regression-into-a-linear-programming-problem-how-can</guid>
      <pubDate>Mon, 20 May 2024 00:41:50 GMT</pubDate>
    </item>
    <item>
      <title>主成分分析是否适用于 Hu 矩特征[关闭]</title>
      <link>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</link>
      <description><![CDATA[我正在尝试改进使用 Hu 矩作为特征的图像分类应用程序的结果，但当我应用 pca 时，只有一个特征可以弥补方差的 0.95。
我对原始 Hu 特征执行了以下步骤来查找投影矩阵：

标准化
计算协方差
使用 Eigen 库查找特征向量和值
选择构成方差 0.8 的特征向量并丢弃其余的。
结果是一个 1 到 X 矩阵（只有一个特征覆盖了方差的 0.98）
]]></description>
      <guid>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</guid>
      <pubDate>Sun, 19 May 2024 23:29:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么在多元线性回归中计算梯度时要对矩阵进行转置？</title>
      <link>https://stackoverflow.com/questions/78504139/why-is-the-matrix-transposed-when-calculating-the-gradient-in-a-multiple-linear</link>
      <description><![CDATA[我正在参加在线机器学习课程，在谈论多元线性回归时，他们使用以下函数来计算梯度：
def 渐变(X, Y, w):
   返回 2 * np.matmul(X.T, (预测(X, w) - Y)) / X.shape[0]

X 是包含数据的矩阵，Y 是包含结果的单列矩阵，w 是包含权重的单列矩阵。 Predict(W, w) 返回模型在当前权重下的预测，其格式与 Y 相同。
我熟悉使用函数的偏导数来查找模型与该权重的局部最小值的接近程度的想法，但我无法理解为什么需要对 X 矩阵进行转置表格来进行此计算。我可以计算每个偏导数并将其用作单独的梯度吗？]]></description>
      <guid>https://stackoverflow.com/questions/78504139/why-is-the-matrix-transposed-when-calculating-the-gradient-in-a-multiple-linear</guid>
      <pubDate>Sun, 19 May 2024 23:04:23 GMT</pubDate>
    </item>
    <item>
      <title>没有数据标准化的PCA比标准化后的性能更好，为什么？</title>
      <link>https://stackoverflow.com/questions/78504113/pca-without-data-normalization-performes-better-than-after-normalization-why</link>
      <description><![CDATA[我有这个 Spotify 数据集，其中包含大约 100k 条记录和 28 个混合数值（离散和连续）和二进制的特征，一些数值变量有很多零值。
我想要执行机器学习分类，对 113 种类型进行分类，数量之多。我想要通过强大的主成分分析来改进我的模型。但我很困惑为什么标准化数据（我使用标准缩放器）的准确性比没有标准化时要差得多。
首先，在数据预处理之后，我执行简单的决策树分类器（在超参数调整之后）并获得 43% 的准确率。

然后，我执行主成分分析 (PCA) 以减少特征数量，以便对数据进行聚类以减少类别数量，并更轻松地检测和可视化异常值。

据我所知，在部署PCA之前，我的数据需要标准化（我在这里使用StandardScaler）。然而我发现，如果没有标准化，我的决策树分类器的一维 PCA 的性能会提高到 51%。相反，通过归一化，2 维的准确度降低到只有 5%，这是我需要的。


这怎么可能？如何提高算法性能？
我的代码
&lt;前&gt;&lt;代码&gt;
pca = PCA(n_components=20) # 我在这里尝试了很多值，因为精度随着更多组件的增加而增加
pca.fit(X_train_norm)
X_train_pca = pca.transform(X_train_norm)

＃ 决策树
X_test_pca = pca.transform(X_test_norm)

dt = DecisionTreeClassifier(min_samples_leaf = 3,random_state=42)
dt.fit(X_train_pca, y_train)

y_pred = dt.predict(X_test_pca)

print(&#39;准确率 %s&#39; % precision_score(y_test, y_pred))
print(&#39;F1-score %s&#39; % f1_score(y_test, y_pred, 平均值=无))
打印（分类报告（y_test，y_pred））

pca_all = PCA(n_components=25)
pca_all.fit(X_train_norm)
X_train_pca = pca_all.transform(X_train_norm)
explained_variance = pca_all.explained_variance_ratio_

plt.figure(figsize=(10, 6))
plt.bar（范围（len（explained_variance）），explained_variance，alpha=0.7，align=&#39;center&#39;，
        label=&#39;个体解释方差&#39;)
plt.step(range(len(explained_variance)), np.cumsum(explained_variance), where=&#39;mid&#39;,
         label=&#39;累积解释方差&#39;)
plt.xlabel(&#39;主成分索引&#39;)
plt.ylabel(&#39;解释方差比率&#39;)
plt.legend(loc=&#39;最佳&#39;)
plt.title(&#39;归一化后主成分解释的方差&#39;)
plt.grid()
plt.show()


我附上了标准化后（圆形）和未标准化（肘形）的主成分分析的数据分布。颜色代表流派。还有我的 PCA 的方差可解释性 [pca_without_normalization](https://i.sstatic.net/HaDoNROy.png)pca_with_normalizationpca_explanability_without_normalization]]></description>
      <guid>https://stackoverflow.com/questions/78504113/pca-without-data-normalization-performes-better-than-after-normalization-why</guid>
      <pubDate>Sun, 19 May 2024 22:44:16 GMT</pubDate>
    </item>
    <item>
      <title>定义数字手写识别问题的自动机[关闭]</title>
      <link>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</link>
      <description><![CDATA[大家好，你们能帮我吗？我仍然不知道如何定义我的自动机，也不知道它和手写识别过程之间的关系是什么，基本上我在定义转换、状态和字母方面遇到问题。
这是使用张量流和 MNIST 数据集进行手写数字识别的代码，但如何定义我的自动机？？？
`导入操作系统
导入CV2
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

模型 = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, 激活=tf.nn.softmax))
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;sparse_categorical_crossentropy&#39;,
              指标=[&#39;准确性&#39;])
model.fit(x_train, y_train, epochs=20)
model.save(&#39;手写.model&#39;)

model = tf.keras.models.load_model(&#39;手写.model&#39;)
损失，准确度 = model.evaluate(x_test, y_test)`
打印（丢失）
打印（准确度）

img = cv2.imread(&#39;img.5.png&#39;)[:,:,0]
img = cv2.resize(img, (28, 28))
img = np.invert(np.array([img]))
预测 = model.predict(img)
print(f&quot;该数字可能是 {np.argmax(prediction)}&quot;)
plt.imshow(img[0], cmap=plt.cm.binary)
plt.show()`
]]></description>
      <guid>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</guid>
      <pubDate>Sun, 19 May 2024 22:26:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HuggingFace Transformers Pipeline 在每个提示（如 vLLM）中生成多个文本补全而不触发错误？</title>
      <link>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</link>
      <description><![CDATA[我正在使用 HuggingFace Transformers Pipeline 库为给定的提示生成多个文本补全。我的目标是利用像 GPT-2 这样的模型来生成不同的可能补全，如 vLLM 中的默认值。但是，当我尝试指定 max_length 和 num_return_sequences 等参数时，我遇到了未使用的 model_kwargs 问题。
这是我使用的代码片段：
复制代码
from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline
from typing import List, Dict

def process_prompts(prompts: List[str], model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions: int = 3) -&gt;; List[List[str]]:
device = 0 if model.device.type == &#39;cuda&#39; else -1
text_generator = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer, device=device)
output = []

for prompt in prompts:
try:
results = text_generator(prompt, max_length=50, num_return_sequences=num_completions, num_beams=num_completions)
Completions = [result[&#39;generated_text&#39;] for result in results]
output.append(completions)
except Exception as e:
print(f&quot;Error processing prompt {prompt}: {str(e)}&quot;)

return output

if __name__ == &quot;__main__&quot;:
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
model.to(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

example_prompts = [&quot;Hello, how are you?&quot;]
processing_outputs = process_prompts(example_prompts, model, tokenizer, num_completions=3)
for output in processing_outputs:
print(output)

还有：
 results = text_generator(prompt, max_length=50, num_return_sequences=num_completions)

当我运行此程序时，出现以下错误：
模型未使用以下 `model_kwargs`： [&#39;max_len&#39;]
注意：我知道生成参数中的拼写错误也会触发此警告，但我已经检查并重新检查了参数名称。

和
 raise ValueError(
ValueError：没有波束搜索的贪婪方法不支持不同于 1 的 `num_return_sequences`（得到 4）。

什么可能导致此错误，我该如何修复它以使用模型有效地生成多个完成？
交叉：https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without-triggering-an-error/86297]]></description>
      <guid>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</guid>
      <pubDate>Sun, 12 May 2024 00:06:07 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：LLMChain 的 2 个验证错误</title>
      <link>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</link>
      <description><![CDATA[这是我的完整代码：
!pip install -q Transformers einops 加速 langchain BitsandBytes Sentence_Transformers faiss-cpu pypdf Sentpiece
从 langchain 导入 HuggingFacePipeline
从 Transformer 导入 AutoTokenizer
从 langchain.embeddings 导入 HuggingFaceEmbeddings
从 langchain.document_loaders.csv_loader 导入 CSVLoader
从 langchain.vectorstores 导入 FAISS、Chroma
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate
从 langchain.chains 导入 ConversationalRetrievalChain
从 langchain.chains.question_answering 导入 load_qa_chain
从 langchain.memory 导入 ConversationBufferMemory
进口加速
进口变压器
进口火炬
导入文本换行
loader = CSVLoader(&#39;/kaggle/input/csvdata/chatdata.csv&#39;, 编码=“utf-8”, csv_args={&#39;分隔符&#39;: &#39;,&#39;})
数据 = 加载器.load()

嵌入 = HuggingFaceEmbeddings(model_name=&#39;sentence-transformers/all-MiniLM-L6-v2&#39;,model_kwargs={&#39;device&#39;: &#39;cpu&#39;})

db = FAISS.from_documents(数据，嵌入)


#Mistral 7B 模型 llm

进口火炬
从变压器进口（
    AutoModelForCausalLM，
    自动标记器，
    生成配置，
    文本流媒体,
    管道，
）

MODEL_NAME =“mistralai/Mistral-7B-Instruct-v0.1”

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
模型 = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME、device_map=“自动”、torch_dtype=torch.float16、load_in_8bit=True
）

Generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
Generation_config.max_new_tokens = 1024
Generation_config.温度 = 0.0001
Generation_config.do_sample = True
流光= TextStreamer（标记器，skip_prompt = True，skip_special_tokens = True）


llm = 管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    return_full_text=真，
    Generation_config = Generation_config，
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    流光=流光，
）


def format_prompt（提示，system_prompt =“”）：
    如果 system_prompt.strip():
        return f“[INST] {system_prompt} {prompt} [/INST]”
    return f“[INST] {提示} [/INST]”


SYSTEM_PROMPT = “””
您是一名临床数据科学家和数据分析师，专门从事统计数据分析和报告生成。您的使命是为医疗保健和临床研究提供准确且富有洞察力的数据驱动解决方案。当您做出回应时，请发挥临床数据科学领域经验丰富的数据专业人员所特有的专业知识和精确度。
如果您遇到没有必要信息的问题，请务必不要提供推测性或不准确的答案。
“”“”.strip()

链 = ConversationalRetrievalChain.from_llm(
    嗯，
    chain_type=“东西”，
    检索器=db.as_retriever(),
    return_source_documents=真，
    详细=真，
）

这里我面临错误：
ValidationError：LLMChain 出现 2 个验证错误
勒姆
  预期 Runnable 的实例（type=type_error.任意_type；expected_任意_type=Runnable）
勒姆
  预期 Runnable 的实例（type=type_error.任意_type；expected_任意_type=Runnable）


从 textwrap 导入填充

结果=链（输入（“临床试验平面计ChatBot ---”）
）
打印（填充（结果[“结果”].strip（），宽度= 80））

此 llm 链被编程为使用 llm、矢量数据库和提示与 csv 聊天，我在运行 ConversationalRetrievalChain 时遇到上述错误]]></description>
      <guid>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</guid>
      <pubDate>Thu, 18 Jan 2024 20:20:48 GMT</pubDate>
    </item>
    <item>
      <title>批量归一化适用于小批量吗？</title>
      <link>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</link>
      <description><![CDATA[我使用批量标准化（批量大小为 10）进行人脸检测。
批量归一化适用于如此小的批量大小吗？如果没有，那么我还能用什么来标准化？]]></description>
      <guid>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</guid>
      <pubDate>Tue, 02 Jul 2019 20:38:25 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有 2000 个不同标签的多类标记文本分类问题。使用 LSTM 和 Glove Embedding 进行分类。

目标变量的标签编码器
带有嵌入层的 LSTM 层
误差指标是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder()
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

LSTM 网络如下所示，带有 Glove Embeddings
np.random.seed(种子)
K.clear_session()
模型=顺序（）
model.add(嵌入(max_features, embed_dim, input_length = X_train.shape[1],
         权重=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add（密集（num_classes，激活=&#39;softmax&#39;））
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;sparse_categorical_crossentropy&#39;)
打印（模型.摘要（））

我的错误指标是 F1 分数。我为错误指标构建了以下函数
类指标（回调）：
    def on_train_begin(self, 日志={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_ precisions = []
 
    def on_epoch_end(自我, 纪元, 日志={}):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        _val_f1 = f1_score(val_targ, val_predict)
        _val_recall=recall_score(val_targ, val_predict)
        _val_ precision = precision_score（val_targ，val_predict）
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_ precisions.append(_val_ precision)
        print(&quot;— val_f1: %f — val_ precision: %f — val_recall %f&quot; % (_val_f1, _val_ precision, _val_recall))
        返回
 
指标=指标（）

##模型适合
model.fit（X_train，train_y，validation_data =（X_test，test_y），epochs = 10，batch_size = 64，callbacks = [指标]）

第一个纪元后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

我的代码哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>