<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 09 Feb 2024 18:16:13 GMT</lastBuildDate>
    <item>
      <title>TensorFlow中张量线性组合的非线性最小化</title>
      <link>https://stackoverflow.com/questions/77970069/nonlinear-minimization-of-linear-combination-of-tensors-in-tensorflow</link>
      <description><![CDATA[我想使用 TensorFlow 来最小化具有一定偏移量的 $k$ 2D 张量线性组合的总变化。
导入tensorflow为tf
将tensorflow_probability导入为tfp
将 numpy 导入为 np

k=10
米=100
n=200

b = np.random.rand(m, n)
fitVec = np.random.rand(k, m, n)
x = np.random.rand(k)

linComb = np.tensordot(x, fitVec, 轴=[0,0])+b
linComb.shape = [m, n, 1]#OK我每次都需要这样做吗？
tv = tf.image.total_variation(tf.cast(linComb, dtype=tf.float32))

所以现在我想找到 $x$ 来最小化总变化。由于张量流是一个强大的框架，因此我认为即使我不做任何机器学习的事情也是可能的。到目前为止我有以下内容
def getMinimizer(b, fitVec):
    b0 = 复制.deepcopy(b)
    shape0=b.shape[0]
    形状1=b.形状[1]
    n =fitVec.shape[0]
    b0.shape=[shape0, shape1, 1]
    定义最小化器（x）：
        xscaled = np.ones(n)
        y_tf = tf.constant(b0, dtype=tf.float32)
        对于 np.arange(n) 中的 k：
            vec = xscaled[k] * fitVec[k]
            vec.shape = [shape0, shape1, 1]
            y_tf += x[k]*tf.constant(vec, dtype=tf.float32)
        返回 tf.image.total_variation(y_tf)
    回报最小化

x0=np.ones(k)
x = tf.Variable(x0, dtype=tf.float32)
最小化器 = getMinimizer(b, fitVec)
loss_fn = lambda: 最小化器(x)
损失 = tfp.math.minimize(loss_fn, 优化器 = tf.keras.optimizers.Adam(learning_rate=0.01), num_steps=200)

我不确定我的代码是否按预期工作。]]></description>
      <guid>https://stackoverflow.com/questions/77970069/nonlinear-minimization-of-linear-combination-of-tensors-in-tensorflow</guid>
      <pubDate>Fri, 09 Feb 2024 18:11:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习是解决 Python 人工智能问题的正确方法吗？</title>
      <link>https://stackoverflow.com/questions/77970053/is-machine-learning-a-correct-approach-to-this-ai-problem-in-python</link>
      <description><![CDATA[我正在解决一个问题，我希望代理能够通过地图进行工作。代理必须以尽可能少的动作找到所有硬币，避开潜在的障碍。代理只能看到周围的环境。所以智能体的视野是 3x3，中间是它自己。代理只能上下左右移动。
我附上了一张地图的照片，csv 文件代表了地图。
x = 边界（或障碍物）
。 = 一个空的方块
@ = 目标（假设是一枚硬币）

csv 文件映射
现在我自己找到了一些解决方案，做了很多 if、for、while 等语句，并让它工作得很好，并且代理可以适应它的环境。但我还不满意，所以我想尝试某种形式的机器学习。
有谁知道这个问题的模型或算法吗？基本上，是为了寻找一个特定的目标。我很想得到一些提示和技巧来指导我正确的方向。]]></description>
      <guid>https://stackoverflow.com/questions/77970053/is-machine-learning-a-correct-approach-to-this-ai-problem-in-python</guid>
      <pubDate>Fri, 09 Feb 2024 18:08:07 GMT</pubDate>
    </item>
    <item>
      <title>'{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT] 的 1 减 3 导致的负维度大小</title>
      <link>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</link>
      <description><![CDATA[尝试 model.fit 后：
hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我收到错误：
ValueError Traceback（最近一次调用最后一次）
单元格位于\[132\]，第 1 行
\----\&gt; 1 hist = model.fit（train，epochs = 15，validation_data = val，callbacks = \ [tensorboard_callback \]）#mudei epocas pra 3，antesera 20

文件 \~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\utils\traceback_utils.py :70，在filter_traceback中。\.error_handler(\*args, \*\*kwargs)
67 过滤_tb = \_process_traceback_frames(e.__traceback__)
68 # 要获取完整的堆栈跟踪，请调用：
69 # `tf.debugging.disable_traceback_filtering()`
\---\&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
71 最后：
72 删除filtered_tb

文件 \~\\AppData\\Local\\Temp\__autograph_ generated_filea6t43riq.py:15，位于outer_factory.\.inner_factory.\.tf__train_function(iterator)
13 尝试：
14 do_return =真
\---\&gt; 15 retval_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(迭代器)), 无, fscope)
16 除外：
17 do_return = 假

ValueError：在用户代码中：

    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1338 行，位于训练函数*
        返回step_function（自身，迭代器）
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1322 行，位于步骤函数 **
        输出 = model.distribute_strategy.run(run_step, args=(data,))
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1303 行，位于运行步骤**
        输出 = model.train_step(数据)
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1080 行，位于训练步骤
        y_pred = self(x, 训练=True)
    文件“C：\ Users \ Enenon \ AppData \ Local \ Packages \ PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0 \ LocalCache \ local-packages \ Python310 \ site-packages \ keras \ src \ utils \traceback_utils.py”，第70行，位于错误处理程序
        从 None 引发 e.with_traceback(filtered_tb)
    
    ValueError：调用层“conv2d_26”（类型 Conv2D）时遇到异常。
    
    &#39;{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=“NHWC”, dilations=[1, 1, 1, 1],explicit_paddings=[ 1 减 3 导致的负维度大小], padding=“VALID”, strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_15/Cast,equential_15/conv2d_26/Conv2D/ReadVariableOp)&#39; 输入形状：[64,64,1, 1]、[3,3,1,32]。
    
    调用层“conv2d_26”接收的参数（类型 Conv2D）：
      • 输入=tf.Tensor(形状=(64, 64, 1, 1), dtype=float32)`

我的代码：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv2D(32, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
模型.add(压平())
model.add（密集（64，激活=&#39;relu&#39;））
model.add（密集（3，激活=&#39;sigmoid&#39;））

model.compile(&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我正在使用 64,64,1 数据集。这是将张量从 64,64 修改为 64,64,1 并从张量创建数据集的代码：
x = x.reshape(-1, 64, 64, 1)
标签 = tf.constant(y)
特征 = tf.constant(x)
datax = tf.data.Dataset.from_tensor_slices(特征)
datay = tf.data.Dataset.from_tensor_slices(标签)
数据 = tf.data.Dataset.zip((datax,datay))
data_iterator = data.as_numpy_iterator()
批处理 = data_iterator.next()
]]></description>
      <guid>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</guid>
      <pubDate>Fri, 09 Feb 2024 17:51:53 GMT</pubDate>
    </item>
    <item>
      <title>要使应用程序能够理解加载的非结构化文档（例如，不规则表格）中的信息，我需要了解什么？</title>
      <link>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</link>
      <description><![CDATA[例如
不规则表格
一种能够理解文档中的文本内容并可能将信息分离为键：值的技术。我还是一名大学生，我想在 NLP 领域取得进步，但我找不到从哪里开始。
我应该学习哪些 NLP 技术，路线图应该是什么样的？]]></description>
      <guid>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</guid>
      <pubDate>Fri, 09 Feb 2024 17:36:06 GMT</pubDate>
    </item>
    <item>
      <title>如何创建目标变量[关闭]</title>
      <link>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</link>
      <description><![CDATA[我尝试预测 ncaa 篮球疯狂游行的每个结果。
我有过去 20 场左右锦标赛的历史数据，并且有 team1_score 和 team2_score 等列。我认为创建一个目标变量很容易，只需创建一个列 team1_win 并在 true 时返回 1，否则返回 0。问题是我的数据是经过组织的，因此 team1 始终是获​​胜团队。所以我的目标变量列将只包含 1。对于二元分类来说，这似乎是一个问题。我不确定如何创建目标变量。我是否需要以某种方式重新整理我的数据，以便 team1 并不总是获胜团队？我的目标变量全为1有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</guid>
      <pubDate>Fri, 09 Feb 2024 17:22:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 WSL GPU 支持下运行 Windows Python 代码？</title>
      <link>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</link>
      <description><![CDATA[我尝试在 Windows PS 上的 WSL 2 发行版中使用 TensorFlow。我的问题是，我只能在互联网上找到描述安装过程的页面，但没有人解释如何在 Windows 上使用 GPU 加速从我的 virtual-env 环境运行代码。那么有没有办法在 WSL Distrubition 中远程运行机器学习脚本呢？
我已经成功安装了WSL2，包括miniconda和tensorflow（带有CUDA）。 Tensorflow Feedback 线也发现 GPU 没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</guid>
      <pubDate>Fri, 09 Feb 2024 16:52:00 GMT</pubDate>
    </item>
    <item>
      <title>预测《Madden》的未来统计数据[关闭]</title>
      <link>https://stackoverflow.com/questions/77969495/predicting-future-stats-in-madden</link>
      <description><![CDATA[我有一堆过去 4 年中玩家的《Madden》统计数据以及与之相关的属性（速度、投掷力量等）。
我已经运行了相关性分析，并找出了哪些属性是统计数据的最佳指标，但我想使用 scikit-learn 来看看是否可以预测未来的统计数据。
我希望得到一些反馈，看看我是否以正确的方式这样做，因为我得到的一些答案并不相符。
X = all_years[[&#39;AWARENESS&#39;,&#39;THROW ACC&#39;,&#39;THROW POWER&#39;]]
y = all_years[[&#39;TD&#39;]]

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.4,random_state=100)

X_train = X_train.值
X_test = X_test.值
y_train = y_train.值
y_test = y_test.值

模型=线性回归()
model.fit(X_train,y_train)
FLACCO = pd.DataFrame([[96,94,93]])
林德利 = pd.DataFrame([[90,89,88]])


y_pred = model.predict(X_test)
分数 = model.score(X_test, y_test)

f_predict = model.predict(FLACCO)
l_predict = model.predict(LINDLEY)
打印（f_预测）
打印（l_预测）
]]></description>
      <guid>https://stackoverflow.com/questions/77969495/predicting-future-stats-in-madden</guid>
      <pubDate>Fri, 09 Feb 2024 16:20:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在嵌入词汇中添加新项目？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</link>
      <description><![CDATA[假设您已经训练了一个包含嵌入层的模型。
您的模型表现良好，并且您对嵌入感到满意。
然后，突然，您想在词汇表中添加一个新项目。
换句话说，您想要计算这个新项目的嵌入。
嵌入层基本上是一个查找表，用于将正整数转换为固定大小的密集向量，现在您想要考虑训练期间不存在的新整数。
如何在不从头开始重新训练模型的情况下做到这一点？
重新启动训练冻结除用于嵌入新项目的参数之外的所有参数是否有意义（在进行一些矩阵形状调整之后）？]]></description>
      <guid>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</guid>
      <pubDate>Fri, 09 Feb 2024 15:55:26 GMT</pubDate>
    </item>
    <item>
      <title>Jax 中的训练迭代速度变慢</title>
      <link>https://stackoverflow.com/questions/77969194/training-gets-iteratively-slower-in-jax</link>
      <description><![CDATA[我开始学习 Jax 并尝试实现策略梯度算法。为了这
我编写了以下损失函数。
@jax.jit
def loss_fn(params, obs, key):
    logits = model.apply(params, obs[np.newaxis])
    left_proba = nn.sigmoid(logits)
    action = ~random.bernoulli(key, left_proba, (1,1)) # 0.5 &gt;左概率
    标签 = jnp.array([[1.]]) - 操作
    返回 optax.sigmoid_binary_cross_entropy(logits, labels).mean(), 操作


我可以在 CartPole 上成功训练一个小型神经网络，我在循环中的某个地方调用这个函数。问题是每次梯度更新训练都会变慢。一开始我每秒执行的迭代次数不止一次，最后一次迭代需要 20 秒以上。我已将问题范围缩小到此函数，因为如果我将 random.bernoulli 调用替换为常量 0.5（如评论中所示），训练不会随着时间的推移而减慢。
我认为可能在循环中调用random.bernoulli会导致很大的开销，因此我预先计算了一个大的随机数数组，然后将数字传递给函数，如下所示：&lt; /p&gt;
uniform_probs = random.uniform(subkey, (n_iterations, n_episodes_per_update, n_max_steps))

@jax.jit
def loss_fn（参数，obs，uniform_prob）：
    logits = model.apply(params, obs[jnp.newaxis])
    left_proba = nn.sigmoid(logits)
    行动 = uni_prob &gt;左概率
    标签 = jnp.array([[1.]]) - 操作
    返回 optax.sigmoid_binary_cross_entropy(logits, labels).mean(), 操作

然后我会这样称呼它：loss_fn(params, obs,uniform_probs[0,0,0]) （正确的索引取决于我在训练循环中的位置）。然而，这根本不会改变训练时间，它仍然会随着时间的推移而减慢，我真的不明白与使用常量 0.5 相比有什么区别。有什么指示或帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969194/training-gets-iteratively-slower-in-jax</guid>
      <pubDate>Fri, 09 Feb 2024 15:29:37 GMT</pubDate>
    </item>
    <item>
      <title>“AdaBoostClassifier”对象没有属性“estimator_”</title>
      <link>https://stackoverflow.com/questions/77963903/adaboostclassifier-object-has-no-attribute-estimator</link>
      <description><![CDATA[我正在尝试对 adaboost 算法的每一轮进行计时（构建每棵附加树需要多长时间）。我 conda 安装了 scikit-learn 1.4.0（因为在他们的网站上说是这个版本）以及运行代码的所有其他要求。
这是我的代码：
Y, z = parse.getHARData() #返回我的特征 Y 和标签 z

Z_train, Z_test, j_train, j_test = train_test_split(Y, z, test_size=0.30, shuffle=True)

b_estimator = DecisionTreeClassifier(max_深度=深度)

ada = AdaBoostClassifier(估计器=b_估计器，n_估计器=NUMTREES)

经过时间 = []

对于范围内的阶段（NUMTREES）：start_time = time.time()

    # 访问并拟合当前的基本估计器
    base_estimator = ada._make_estimator(append=True, random_state=42)
    base_estimator.fit(Z_train, j_train)
    
    elapsed_time = time.time() - 开始时间
    elapsed_times.append(elapsed_time)

我期望这会启动计时器，使用森林之前的信息种植一棵树，将该树添加到集合中，停止时间，并将经过的时间附加到 elapsed_times 中。
相反，它返回此错误：
AttributeError Traceback（最近一次调用最后一次）
第 7 行 [9] 中的单元格
      4 开始时间 = time.time()
      6 # 访问并拟合当前的基本估计器
----&gt; 7 base_estimatr = ada._make_estimator(append=True, random_state=42)
      8 base_estimatr.fit(Z_train, j_train)
     10 经过时间 = time.time() - 开始时间

文件〜/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/ensemble/_base.py:141，在BaseEnsemble._make_estimator（self，append，random_state）中
    135 def _make_estimator（自我，附加=真，随机状态=无）：
    136 &quot;&quot;&quot;制作并配置`estimator_`属性的副本。
    137
    138 警告：此方法应用于正确实例化新的
    139 名次级估算员。
    第140章 140
--&gt; 141 估计器 = 克隆（self.estimator_）
    142 estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})
    144如果random_state不是None：

AttributeError：“AdaBoostClassifier”对象没有属性“estimator_”
]]></description>
      <guid>https://stackoverflow.com/questions/77963903/adaboostclassifier-object-has-no-attribute-estimator</guid>
      <pubDate>Thu, 08 Feb 2024 18:08:39 GMT</pubDate>
    </item>
    <item>
      <title>带有 ball_tree 和度量半正矢的 DBSCAN</title>
      <link>https://stackoverflow.com/questions/77944949/dbscan-with-ball-tree-and-metric-haversine</link>
      <description><![CDATA[我有来自一本使用欧几里得度量的期刊的类代码：
 类 ST_DBSCAN():
        ”“”
        执行 ST_DBSCAN 聚类的类
        参数
        ----------
        eps1：浮点数，默认=0.5
            之间的空间密度阈值（最大空间距离）
            有两点被认为是相关的。
        eps2：浮点数，默认=10
            两个之间的时间阈值（最大时间距离）
            被认为相关的点。
        min_samples ：整数，默认=5
            一个核心点所需的样本数量。
        度量：字符串默认=&#39;euclidean&#39;
            使用的距离度量 - 更多选项是
            ‘braycurtis’，‘堪培拉’，‘切比雪夫’，‘cityblock’，‘相关性’，
            ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’,
            ‘kulsinski’，‘mahalanobis’，‘匹配’，‘rogerstanimoto’，‘sqeuclidean’，
            ‘russellrao’、‘seuclidean’、‘sokalmichener’、‘sokalsneath’、‘yule’。
        n_jobs ：int 或 None，默认=-1
            启动进程数 -1 表示使用所有处理器
        属性
        ----------
        标签：数组，形状= [n_samples]
            数据的聚类标签 - 噪声定义为 -1
        参考
        ----------
        Ester, M.、H. P. Kriegel、J. Sander 和 X. Xu，《基于密度的方法》
        在带有噪声的大型空间数据库中发现簇的算法”。
        见：第二届国际知识发现会议论文集
        和数据挖掘，俄勒冈州波特兰，AAAI Press，第 226-231 页。 1996年
    ”“”
        
    
        def __init__(自我,
                     eps1=0.5，
                     每股收益2=10,
                     最小样本=5，
                     度量=&#39;欧几里得&#39;,
                     n_jobs=-1):
            self.eps1 = eps1
            自我.eps2 = eps2
            self.min_samples = min_samples
            self.metric = 公制
            self.n_jobs = n_jobs
    
        def fit(自身, X):
            
            应用ST DBSCAN算法
            ----------
            X ：二维 numpy 数组
                数组的第一个元素应该是时间
                属性为浮点数。数组中的以下位置是
                被视为空间坐标。该结构应如下所示 [[time_step1, x, y], [time_step2, x, y]..]
                例如二维数据集：
                数组([[0,0.45,0.43],
                [0,0.54,0.34],...])
            退货
            --------
            自己
            
            #检查输入是否正确
            X = 检查数组(X)
    
            如果不是 self.eps1 &gt; 0.0 或不是 self.eps2 &gt; 0.0 或不是 self.min_samples &gt; 0.0：
                raise ValueError(&#39;eps1, eps2, minPts 必须为正&#39;)
    
            n, m = X.形状
    
            
            # 使用二次内存消耗进行计算
    
            # 计算“时间”属性和空间属性的平方形式欧几里德距离矩阵
            time_dist = pdist(X[:, 0].reshape(n, 1), metric=self.metric)
            euc_dist = pdist(X[:, 1:], metric=self.metric)
    
            # 使用 time_dist 过滤 euc_dist 矩阵
            dist = np.where(time_dist &lt;= self.eps2, euc_dist, 2 * self.eps1)
    
            db = DBSCAN(eps=self.eps1,
                            min_samples=self.min_samples,
                            指标=&#39;预先计算&#39;）
            db.fit(正方形(距离))
    
            self.labels = db.labels_
    
           
    
        返回自我

这是在欧几里得度量中实现的，但是当应用于地理位置时，转换为笛卡尔坐标会出现问题。如何更改该类以处理具有度量正弦值的球树实现。]]></description>
      <guid>https://stackoverflow.com/questions/77944949/dbscan-with-ball-tree-and-metric-haversine</guid>
      <pubDate>Tue, 06 Feb 2024 02:25:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4 [关闭]</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中分析 RNN、CNN、NN 结果</title>
      <link>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</link>
      <description><![CDATA[我有大量标记数据集。每行包含 863 标记化单词。我正在尝试验证哪种类型的 NN 最适合分析此类数据集。
我准备了3个模型：
美国有线电视新闻网：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32, input_length=863),
        tf.keras.layers.Conv1D(32, 5, 激活=&#39;relu&#39;,kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01)),
        tf.keras.layers.GlobalMaxPooling1D(),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

简单平面神经网络：
模型 = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
]）

和 RNN：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
        tf.keras.layers.LSTM（32，激活=&#39;relu&#39;，return_sequences=True），
        tf.keras.layers.LSTM(32, 激活=&#39;relu&#39;, ),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

CNN 和 NN 给出了有希望的结果，accu 率约为 98%（可能过度拟合），而 RNN 的accu 率仅为 65% 左右。值得一提的是，RNN 的 epoch 至少在 10 分钟左右，而 CNN 和 NN 只有 1 分钟。
如何让 RNN 表现更好？]]></description>
      <guid>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</guid>
      <pubDate>Tue, 28 Apr 2020 19:50:48 GMT</pubDate>
    </item>
    <item>
      <title>对 Dataframe 中的某些列进行估算</title>
      <link>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</link>
      <description><![CDATA[我正在尝试在名为“年龄”的单个列上使用Imputer来替换缺失值。但是，我收到错误：“预期是二维数组，却得到了一维数组：”
以下是我的代码
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 Imputer

数据集 = pd.read_csv(“titanic_train.csv”)

dataset.drop(&#39;小屋&#39;, axis=1, inplace=True)
x = dataset.drop(&#39;幸存&#39;, axis=1)
y = 数据集[&#39;幸存&#39;]

imputer = Imputer（missing_values =“nan”，策略=“平均值”，轴= 1）
imputer = imputer.fit(x[&#39;年龄&#39;])
x[&#39;年龄&#39;] = imputer.transform(x[&#39;年龄&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</guid>
      <pubDate>Tue, 18 Sep 2018 10:44:49 GMT</pubDate>
    </item>
    <item>
      <title>网格上的 CNN 回归 - 卷积神经网络的局限性？</title>
      <link>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</link>
      <description><![CDATA[我正在使用 CNN 解决（与高能物理相关的）问题。
为了理解这个问题，让我们考虑一下这里的这些示例。
左侧是 CNN 的输入，右侧是所需的输出。因此网络应该对输入进行聚类。这种聚类背后的实际算法（即我们如何获得所需的训练输出）非常复杂，我们希望 CNN 能够学习这一点。
我尝试过不同的 CNN 架构，例如类似于 U-net 架构的架构 (https:// arxiv.org/abs/1505.04597），还有各种卷积层的串联等。
输出总是非常相似（对于所有架构）。
在这里您可以看到一些 CNN 预测。
原则上，网络表现得相当好，但正如您所看到的，在大多数情况下，CNN 输出由几个直接相邻的填充像素组成，这在真实情况下永远不会（！）发生。&lt; /p&gt;
我一直在所有网络中使用均方误差作为损失函数。
您对如何避免这一问题并提高网络性能有什么建议吗？
或者这是 CNN 的一般限制，并且在实践中不可能使用 CNN 解决这样的问题？]]></description>
      <guid>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</guid>
      <pubDate>Mon, 05 Mar 2018 12:05:42 GMT</pubDate>
    </item>
    </channel>
</rss>