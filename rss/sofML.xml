<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 27 Jun 2024 15:17:13 GMT</lastBuildDate>
    <item>
      <title>Oracle 机器学习（OML）df_datetime 给出“未选择任何列”错误</title>
      <link>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</link>
      <description><![CDATA[如果有人能帮忙，我遇到了一些编码问题！我试图从 oml.Dataframe df 获取 Datetime 类型。我试过这个代码：
 &#39;&#39;&#39;
df = oml.sync(query=QUERY)
df_datetime = df.select_types(include=[&#39;oml.Datetime&#39;])
&#39;&#39;&#39;

但我收到一条错误消息，提示未选择任何列。我是否错误地使用了此功能？
我找到了一种解决方法，使用
&#39;&#39;&#39;
df = oml.sync(query=QUERY)
df_datetime = []
for col, dtype in df.dtypes.items():
if dtype.__name__ == &#39;Datetime&#39;:
df_datetime.append(col)
&#39;&#39;&#39;

这确实返回了 Datetime 对象，所以我知道它们存在。如果有人可以向我解释我做错了什么，我更愿意使用 select_types 方法。]]></description>
      <guid>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</guid>
      <pubDate>Thu, 27 Jun 2024 14:52:10 GMT</pubDate>
    </item>
    <item>
      <title>软件开发和数据科学之间的主要区别是什么？</title>
      <link>https://stackoverflow.com/questions/78678325/what-are-the-key-differences-between-software-development-and-data-science</link>
      <description><![CDATA[我想了解软件开发和数据科学之间的主要区别。虽然这两个领域都涉及编程和解决问题，但它们似乎专注于不同的领域并需要不同的技能组合。
在软件开发中，主要关注的是设计、编码、测试和维护软件应用程序。它通常需要了解软件工程原理、系统设计，并熟练掌握 Java、C++ 或 Python 等语言。
另一方面，数据科学涉及使用统计分析、机器学习和数据可视化从数据中提取见解。数据科学家通常处理大型数据集，使用 R 或 Python 等工具，并且需要强大的统计背景和领域知识。
有人可以详细说明每个领域的具体角色、所需技能和典型项目吗？它们如何重叠，每个领域的职业前景如何？]]></description>
      <guid>https://stackoverflow.com/questions/78678325/what-are-the-key-differences-between-software-development-and-data-science</guid>
      <pubDate>Thu, 27 Jun 2024 14:38:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在现场生成数据时实现神经网络的训练？</title>
      <link>https://stackoverflow.com/questions/78677993/how-do-i-implement-training-a-neural-network-when-generating-the-data-on-spot</link>
      <description><![CDATA[我想构建一个物理模拟的替代模型。这样我就可以自己生成数据了。数据本身非常大，因此生成一些数据样本（例如一批 124 个）然后立即将其用于训练是有意义的，而不是生成数百万个样本，因为我无法保存这样的数据集。通过这种方式，我可以生成一个批次，临时保存它/将其发送到等待数据的神经网络，然后在训练后丢弃它。
我通常使用 pytorch 实现我的模型，我也熟悉 torch.utils.data.DataLoader，它可以将批次提取到 RAM 中，但它需要目录结构和文件 ID，即整个数据集需要保存在某个地方。]]></description>
      <guid>https://stackoverflow.com/questions/78677993/how-do-i-implement-training-a-neural-network-when-generating-the-data-on-spot</guid>
      <pubDate>Thu, 27 Jun 2024 13:31:24 GMT</pubDate>
    </item>
    <item>
      <title>React Native 中的图像背景去除器</title>
      <link>https://stackoverflow.com/questions/78677677/image-background-remover-in-react-native</link>
      <description><![CDATA[需要一个像remove.bg这样的图像背景去除器。如果有任何库建议或者我必须使用机器学习概念。
我试过这个库react-native-background-remover，但这个库只能去除人体背景。]]></description>
      <guid>https://stackoverflow.com/questions/78677677/image-background-remover-in-react-native</guid>
      <pubDate>Thu, 27 Jun 2024 12:31:01 GMT</pubDate>
    </item>
    <item>
      <title>Flutter 中的 TensorFlowInferenceInterface</title>
      <link>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</link>
      <description><![CDATA[我在 Android 中有一个对象检测代码，我想将其转换为 Flutter 以便也用于 IOS
public static Classifier create(
AssetManager assetManager,
String modelFilename,
String[] labels,
int inputSize,
int imageMean,
float imageStd,
String inputName,
String outputName) {
final TensorFlowImageClassifier c = new TensorFlowImageClassifier();
c.inputName = inputName;
c.outputName = outputName;

// 将标签名称读入内存。
Collections.addAll(c.labels, labels);

c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);

// 输出的形状为 [N, NUM_CLASSES]，其中 N 是批次大小。
final Operation operation = c.inferenceInterface.graphOperation(outputName);
final int numClasses = (int) operation.output(0).shape().size(1);

// 理想情况下，可以从输入操作的形状中检索 inputSize。唉，
// 通常使用的 graphdef 中输入的占位符节点未指定形状，因此它
// 必须作为参数传入。
c.inputSize = inputSize;
c.imageMean = imageMean;
c.imageStd = imageStd;

// 预分配缓冲区。
c.outputNames = new String[]{outputName};
c.intValues = new int[inputSize * inputSize];
c.floatValues = new float[inputSize * inputSize * 3];
c.outputs = new float[numClasses];

return c;
}

调用：
create(
getAssets(),
&quot;file:///android_asset/xxx&quot;,
getResources().getStringArray(R.array.yyy),
INPUT_SIZE,
128,
128,
&quot;input&quot;,
&quot;InceptionV3/Predictions/Reshape_1&quot;)

org.tensorflow.contrib.android.TensorFlowInferenceInterface
在 Flutter 中找不到这个类，我应该用什么来替换它？]]></description>
      <guid>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</guid>
      <pubDate>Thu, 27 Jun 2024 12:06:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 Jax 的 vmap 函数在 LinkedLists 上实现矢量化函数</title>
      <link>https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function</link>
      <description><![CDATA[尝试使用 Jax 实现算法的矢量化版本（来自计算几何）。我使用 LinkedList 制作了最小工作示例来特别表达我的查询（否则我将使用 DCEL）。
这个想法是，这个矢量化算法将检查 DCEL 上的某些标准。为了简单起见，我用一个简单的求和算法代替了这个“标准检查程序”。

import jax
from jax import vmap
import jax.numpy as jnp

class Node: 

# 构造函数初始化节点对象 
def __init__(self, data): 
self.data = data 
self.next = None

class LinkedList: 

def __init__(self): 
self.head = None

def push(self, new_data): 
new_node = Node(new_data) 
new_node.next = self.head 
self.head = new_node 

def printList(self): 
temp = self.head 
while(temp): 
print (temp.data,end=&quot; &quot;) 
temp = temp.next

def summate(list) :
prev = None
current = list.head
sum = 0
while(current is not None): 
sum += current.data
next = current.next
current = next
return sum

list1 = LinkedList() 
list1.push(20) 
list1.push(4) 
list1.push(15) 
list1.push(85) 

list2 = LinkedList() 
list2.push(19)
list2.push(13)
list2.push(2)
list2.push(13)

#list(map(summate, ([list1, list2])))

vmap(summate)(jnp.array([list1, list2]))


我收到以下错误。
 TypeError: Value &#39;&lt;__main__.LinkedList object at 0x1193799d0&gt;&#39; dtype 为 object 的数组类型不是有效的 JAX 数组类型。JAX 仅支持数字类型的数组。
目标是，如果我有一组 10,000 个 Linkedlist，我应该能够以矢量化方式在每个 LinkedList 上应用此 summate 函数。我已经在基本 Python 中实现了我想要的功能，但我想在 Jax 中执行此操作，因为有一个更大的概率函数，我将使用此子过程（它是马尔可夫链）。
可能我完全无法在 Jax 上处理此类数据结构，因为错误表明仅支持数字类型。我可以使用 pytrees 以某种方式缓解此限制吗？
建议我使用 jnp 中的简单列表会很诱人，但我只是使用 Linkedlist 作为简单（st）数据结构的示例。如前所述，我实际上是在 DCEL 上工作。
PS：Linkedlist 代码取自 GeeksForGeeks，因为我想快速想出一个最小工作示例。]]></description>
      <guid>https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function</guid>
      <pubDate>Thu, 27 Jun 2024 10:39:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML.Net 构建并训练图像到 C# 对象 ML 模型</title>
      <link>https://stackoverflow.com/questions/78677104/build-and-train-an-image-to-c-sharp-object-ml-model-using-ml-net</link>
      <description><![CDATA[我希望构建/训练一个 ML 模型，该模型可以扫描试卷，获取问题列表、问题图像（如果有）和问题选项（如果有），然后将其作为 c# 对象返回作为输出。
我对 C# 有广泛的了解，但到目前为止我见过的所有 Ml.Net 课程都是文本和标签。
请为我指明正确的方向
我尝试过使用 Azure congitive，但我得到的只是对象和面部识别。
我希望能够获取问题列表及其问题图像和/或问题选项]]></description>
      <guid>https://stackoverflow.com/questions/78677104/build-and-train-an-image-to-c-sharp-object-ml-model-using-ml-net</guid>
      <pubDate>Thu, 27 Jun 2024 10:35:46 GMT</pubDate>
    </item>
    <item>
      <title>寻求从相册页面高精度提取图像的建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78676858/seeking-advice-for-extracting-images-from-album-pages-with-high-precision</link>
      <description><![CDATA[我目前正在开展一个项目，需要以极高的精度从扫描的相册页面中提取单个图像。每次扫描的分辨率为 5792x5792。
在此处输入图像描述
这是我当前的工作流程：
使用 YOLOv9 对象检测（训练尺寸为 640）检测和裁剪单个图像。
使用 YOLOv8 分割（训练尺寸为 640）从裁剪后的图像中提取图像。
在此处输入图像说明
虽然结果不错，但我的目标是获得高度准确的蒙版，并且面临一些问题：
蒙版中缺少图像的某些部分。
蒙版中包含部分背景。
蒙版不平滑且显得不连贯。
在此处输入图像说明
我也尝试过使用 Mask RCNN，但与 YOLOv8 和 YOLOv9 分割相比，结果略差。此外，我尝试了掩模细化技术，但最终却降低了掩模质量。
我正在寻找有关任何预处理、后处理或​​可能不同的方法的建议，以帮助我以非常高的精度实现我的目标。
任何建议或推荐都将不胜感激！
提前谢谢您！
我也尝试过使用 Mask RCNN，但与 YOLOv8 和 YOLOv9 分割相比，结果略差。此外，我尝试了掩模细化技术，但最终却降低了掩模质量。
我正在寻找有关任何预处理、后处理或​​可能不同的方法的建议，以帮助我以非常高的精度实现我的目标。
任何建议或推荐都将不胜感激！
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78676858/seeking-advice-for-extracting-images-from-album-pages-with-high-precision</guid>
      <pubDate>Thu, 27 Jun 2024 09:52:03 GMT</pubDate>
    </item>
    <item>
      <title>在将 CNN 作为图像分割的训练模型时如何修复错误？</title>
      <link>https://stackoverflow.com/questions/78676233/how-fix-errors-when-make-a-training-model-as-cnn-for-image-segmentation</link>
      <description><![CDATA[我有一个项目需要通过实例分割提取图像中的 ROI 区域。
我选择 YOLO5 来完成这个任务，因为我有 spyder (3.11)。首先，为了练习，我制作了一个包含 5 张图片的训练模型，这个预先训练好的 model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;) 成功制作成功。
import torch
from PIL import Image
import os

# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤3：在训练数据上拟合模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = os.path.join(dataset_path, &#39;yolov5s.pt&#39;)
torch.save(model.state_dict(), save_path)`

但之后我使用 50 张图像制作了一个良好的训练模型，并将它们标记为 eye_number.png，然后再次使用 yolov5s 作为预训练模型。但我的挑战是由于以下错误而出现的。

异常：“model”。缓存可能已过期，请尝试 force_reload=True 或参阅 https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading 寻求帮助。&quot;

这是我第二次尝试制作训练模型
 import torch
from PIL import Image
import os

# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;,
&#39;eye_6.png&#39;, &#39;eye_7.png&#39;, &#39;eye_8.png&#39;, &#39;eye_9.png&#39;, &#39;eye_10.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤 3：根据训练数据拟合模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = &#39;D:/yolov5s_webcam.pt&#39;
torch.save(model.state_dict(), save_path)`

我尝试更改保存模型的直接方式，但没有帮助。
我该如何修复此错误？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78676233/how-fix-errors-when-make-a-training-model-as-cnn-for-image-segmentation</guid>
      <pubDate>Thu, 27 Jun 2024 07:53:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在自定义 Hugginface 服务器上设置和运行视频模型？</title>
      <link>https://stackoverflow.com/questions/78675537/how-to-set-up-and-run-a-video-model-on-a-custom-hugginface-server</link>
      <description><![CDATA[我正在开展一个涉及自定义视频模型的项目，我需要在我的服务器上设置并运行它。该模型需要特定的配置文件，例如 config.json，以及 pytorch_model.bin 和 pytorch_model.safetensors 等格式的权重。
我的模型文件具有以下目录结构：
/model_directory
──coder
│ ── config.json
│ └── pytorch_model.safetensors
── controlnet
│ ── config.json
│ └── pytorch_model.safetensors
──tention
└──tention.ckpt


有人可以提供详细示例或指南来说明如何：
正确加载这些模型及其配置。
我目前在 huggingface 上收到此错误。
OSError：/repository 似乎没有名为 config.json 的文件。查看“https://huggingface.co//repository/None”以获取可用文件。

设置推理管道以运行模型。
处理潜在问题，例如缺少配置文件或路径不正确。
任何帮助或示例都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78675537/how-to-set-up-and-run-a-video-model-on-a-custom-hugginface-server</guid>
      <pubDate>Thu, 27 Jun 2024 04:31:28 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>选择 OptunaSearchCV 管道参数的问题</title>
      <link>https://stackoverflow.com/questions/78672059/problems-with-selecting-optunasearchcv-pipeline-parameters</link>
      <description><![CDATA[我有一个代码，用于迭代模型本身和整个管道的超参数
preprocessor = ColumnTransformer(
[
(&#39;OneHotEncoder&#39;, OneHotEncoder(drop=&#39;if_binary&#39;, sparse_output=False), binary_cols),
(&#39;CatBoostEncoder&#39;, CatBoostEncoder(random_state=RANDOM_STATE), non_binary_cat_cols),
(&#39;StandardScaler&#39;, StandardScaler(), num_cols)
],
verbose_feature_names_out=False,
remainder=&#39;drop&#39;
)

pipe_final = ImbPipeline([
(&#39;preprocessor&#39;, preprocessor),
(&#39;target_imbalance&#39;, ADASYN()),
(&#39;selection&#39;, PCA()),
(&#39;models&#39;, CatBoostClassifier(random_state=RANDOM_STATE))
])

# CatBoostClassifier 的参数
param_grid = {
&#39;models__iterations&#39;: [1000, 2000, 3000],
&#39;models__class_weights&#39;: [&#39;Balanced&#39;, None],
&#39;target_imbalance&#39;: [ADASYN(random_state=RANDOM_STATE), SMOTETomek(random_state=RANDOM_STATE),
SMOTE(random_state=RANDOM_STATE, k_neighbors=10), &#39;passthrough&#39;],
&#39;preprocessor__StandardScaler&#39;: [StandardScaler(), RobustScaler(), MinMaxScaler(),
PowerTransformer(), QuantileTransformer(),
Normalizer()，PolynomialFeatures（degree=2，include_bias=False），&#39;passthrough&#39;]，
&#39;selection&#39;：[PCA（random_state=RANDOM_STATE，n_components=“mle”，svd_solver=“full”），
SelectKBest（mutual_info_classif，k=40），
SelectKBest（f_classif，k=40），
SelectKBest（chi2，k=40），
SelectPercentile（mutual_info_classif，百分位数=10），
SelectPercentile（f_classif，百分位数=10），
SelectFromModel（CatBoostClassifier（random_state=RANDOM_STATE）），
SelectFromModel（LogisticRegression（random_state=RANDOM_STATE）），
SelectFromModel（RandomForestClassifier（random_state=RANDOM_STATE）），
&#39;passthrough&#39;],
}

gs = GridSearchCV(
pipe_final, 
param_grid, 
cv=5, 
scoring=&#39;roc_auc&#39;, 
n_jobs=-1
)

# Запускаем поиск гиперпараметров
gs.fit(X, y_enc)

这需要很长时间才能完成，我想加快速度。为此，我想使用 OptunaSearchCV。我是否理解正确，使用 OptunaSearchCV 我可以枚举模型本身的超参数，但不能枚举整个管道，因为没有可以设置 SelectKBest(f_classif, k=40)、RobustScaler() 等的分布？
抱歉，如果我的措辞在某些地方不准确，我使用谷歌翻译，因为......我不是母语人士]]></description>
      <guid>https://stackoverflow.com/questions/78672059/problems-with-selecting-optunasearchcv-pipeline-parameters</guid>
      <pubDate>Wed, 26 Jun 2024 11:14:26 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 ElasticNetCV：获取具有相应 MSE 的超参数完整网格？</title>
      <link>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</link>
      <description><![CDATA[我在 Python 中用三个分割拟合了一个 ElasticNetCV：
import numpy as np
from sklearn.linear_model import LinearRegression

#样本数据：
num_samples = 100 # 样本数量
num_features = 1000 # 特征数量
X = np.random.rand(num_samples, num_features)
Y = np.random.rand(num_samples)

#模型
l1_ratios = np.arange(0.1, 1.1, 0.1)
tscv=TimeSeriesSplit(max_train_size=None, n_splits=3)
regr = ElasticNetCV(cv=tscv.split(X), random_state=42,l1_ratio=l1_ratios)
regr.fit(X,Y)

现在我想获取超参数组合的整个网格以及相应的 MSE 作为数据框，我尝试了以下方法。但是，问题在于，生成的数据框显示的超参数组合的 MSE 并未显示为 ElasticNetCV 对象中的最小值，该对象可以通过 regr.alpha_ 和 regr.l1_ratio_ 获得
:
mse_path = regr.mse_path_
alpha_path = regr.alphas_

# 重塑 mse_path，使 l1_ratios、n_alphas、cross_validation_step 作为单独的列
mse_values = mse_path.flatten()
alpha_values = alpha_path.flatten()
l1_values=np.tile(l1_ratios ,int(alpha_values.shape[0]/l1_ratios.shape[0]))
repeated_l1_ratios = np.repeat(l1_ratios, 100)

# mse维度为 (11, 100, 3)
array_3d = mse

# 将 3D 数组展平为 2D 数组
# 每个形状为 (100, 3) 的子数组都将成为新 2D 数组中的一行
array_2d = array_3d.reshape(-1, 3)

# 从 2D 数组创建 DataFrame
df = pd.DataFrame(array_2d, columns=[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;])

df[&#39;alpha_values&#39;] = alpha_values
df[&#39;l1_values&#39;] = duplicate_l1_ratios

以下结果导致超参数组合不是真实的组合。因此，在组合 MSE 和超参数值时，会出现问题：
# 计算三个分割中每行的最小 MSE
df[&#39;Min MSE&#39;] = df[[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;]].min(axis=1)

# 确定总体最小 MSE 的行
min_mse_row_index = df[&#39;Min MSE&#39;].idxmin()

# 检索最小 MSE 的行
min_mse_row = df.loc[min_mse_row_index]

print(&quot;所有分割中 MSE 最小的行：&quot;)
print(min_mse_row)
]]></description>
      <guid>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</guid>
      <pubDate>Wed, 26 Jun 2024 08:49:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 Cord-V2 数据集微调 LayoutLmv3</title>
      <link>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</link>
      <description><![CDATA[我正在使用 CORD-v2 数据集对 LayoutLMv3 进行微调。我在数据预处理部分遇到了困难，特别是如何从图像中正确提取总量 (TTC)。我在网上找到的示例似乎使用了较旧的 CORD 数据集，该数据集的格式不同。新的 CORD-v2 数据集仅包含图像和地面实况标签。
如何解决这个问题？
我尝试过 YouTube 和 Hugging Face 中的示例，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</guid>
      <pubDate>Tue, 11 Jun 2024 09:22:25 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 中 model.eval() 起什么作用？</title>
      <link>https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch</link>
      <description><![CDATA[我什么时候应该使用 .eval()？我理解它应该允许我“评估我的模型”。我如何关闭它进行训练？
使用 .eval() 的示例训练代码。]]></description>
      <guid>https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch</guid>
      <pubDate>Sat, 01 Feb 2020 15:58:15 GMT</pubDate>
    </item>
    </channel>
</rss>