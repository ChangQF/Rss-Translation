<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 03 Apr 2024 12:24:11 GMT</lastBuildDate>
    <item>
      <title>线性回归的 cros_val_score 问题</title>
      <link>https://stackoverflow.com/questions/78267134/problem-with-cros-val-score-with-linear-regression</link>
      <description><![CDATA[我有这段代码，当我计算 cross_val_score 时，我不明白错误在哪里。
您可以在最后找到代码。
当我将 X 和 Y 插入 cross_val_score 时，我得到此输出= [-1.04310278e+25 -9.02663688e-01 -5.58849445e-01 -4.67666181e-01
-5.31695826e-01 -5.38521348e-01 -4.88145909e-01 -9.43066823e-01
-8.05226210e-01 -1.08907648e+00]
如果我插入 x_train 和 y_train （即使在将它们分割为 test_size=0.01 后，因此与 X 和 Y 非常相似），我得到的 cross_val_score 的结果为：
[-0.71077296 -0.55292557 -0.61305045 -0.56387715 -0.69411482 -0.6876596
-0.58946027 -0.65684074 -0.70967388 -0.887881]
它们的结果截然不同，我不明白问题出在哪里。
X 的形状为 (18994, 12)
Y (18994, 2)，如果 x_train 的测试大小为 0.2，则我们有 (15195, 12) 和 y_train (15195, 2)。
谁能帮我理解我哪里出了问题？
导入请求
将 pandas 导入为 pd
导入 csv
导入地理数据
将plotly.express导入为px
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
将seaborn导入为sns
从 sklearn.model_selection 导入 train_test_split
将日期时间导入为 dt
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.metrics 导入mean_squared_error, r2_score
从 sklearn.multioutput 导入 MultiOutputRegressor
从 sklearn.model_selection 导入 cross_val_score、cross_val_predict
模型=线性回归()
mulregressor = MultiOutputRegressor（模型）
data=pd.read_excel(r&quot;C:\Users\...\file.xlsx&quot;)
def 使用标准化（数据）：
    尺度标准=标准缩放器()
    数据=scaleStandard.fit_transform（数据）
    data=pd.DataFrame(data, columns=[“卧室”,“浴室”,“平方英尺”,“建造年份”,“价格”,
                                “listedDate”、“removedDate”、“daysOnMarket”、“latitude”、
                               “经度”、“公寓”、“多户住宅”、“单户住宅”、“联排别墅”、“浴室比率”])
    返回数据

datastand=使用标准化（数据）
X=datastand.drop([“daysOnMarket”], axis=1)
X=X.drop([&quot;removedDate&quot;], axis=1)
X=X.drop([“价格”], axis=1).values
Y=datastand[[&quot;daysOnMarket&quot;,&quot;price&quot;]].values

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)
k_fold_acc= cross_val_score(mulregressor, x_train,y_train,cv=10,scoring=“neg_mean_squared_error”)
打印（k_fold_acc）
]]></description>
      <guid>https://stackoverflow.com/questions/78267134/problem-with-cros-val-score-with-linear-regression</guid>
      <pubDate>Wed, 03 Apr 2024 10:53:41 GMT</pubDate>
    </item>
    <item>
      <title>机器学习：为什么我的混淆矩阵是这样的？当我尝试了所有测试并且得到了合理的分数时</title>
      <link>https://stackoverflow.com/questions/78266973/machine-learning-why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tes</link>
      <description><![CDATA[我正在使用 scklearn 的随机森林分类，除了混淆矩阵之外，我在所有方面都得到了不错的结果，这里是代码和结果
训练和测试的标签分布
火车组的大小
模型
训练模型的分数
这是问题
这不是我所期望的，特别是因为训练量仅为训练数据集中 677k 的训练量的 1/3，但在混淆矩阵中它只处理所有标签 0。 
模型：
导入时间
# 记录开始时间
开始时间 = 时间.time()

# 随机森林分类器
rf = 随机森林分类器()

# 定义参数网格
rf_param_grid = {&#39;n_estimators&#39;：[45]，&#39;标准&#39;：[&#39;熵&#39;]，&#39;max_深度&#39;：[30]}

# 网格搜索
rf_cv = GridSearchCV(rf, rf_param_grid, cv=7)
rf_cv.fit(X_train, y_train)

# 记录结束时间
结束时间 = time.time()

# 计算经过的时间
经过时间 = 结束时间 - 开始时间

# 打印结果
print(&quot;最佳成绩：&quot;, rf_cv.best_score_)
print(&quot;最佳参数：&quot;, rf_cv.best_params_)
print(&quot;经过时间:&quot;, elapsed_time, &quot;秒&quot;)

我在这里的每堂课都取得了超过 98% 的好成绩：
# 对训练数据进行预测
y_train_pred = rf_cv.predict(X_train)

# 计算准确率
准确度=准确度_得分（y_train，y_train_pred）

# 计算每个类别的准确率、召回率和 F1 分数
精度 = precision_score(y_train, y_train_pred, 平均值=无)
召回率=召回率（y_train，y_train_pred，平均值=无）
f1 = f1_score(y_train, y_train_pred, 平均值=无)

# 计算宏观平均精度、召回率和 F1 分数
Macro_ precision = precision_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)
宏召回 = 召回分数(y_train, y_train_pred, 平均值=&#39;宏&#39;)
Macro_f1 = f1_score(y_train, y_train_pred, 平均值=&#39;宏&#39;)

# 打印评估指标
print(“准确度：”, 准确度)
print(&quot;精度（0、1、2 类）：&quot;, precision)
print(“召回（0、1、2 类）：”，召回）
print(&quot;F1-分数（0、1、2 类）：&quot;, f1)
print(&quot;宏观平均精度：&quot;, macro_ precision)
print(&quot;宏观平均召回率：&quot;, Macro_recall)
print(“宏观平均 F1 分数：”, Macro_f1)

混淆矩阵，其中不显示除 0 类之外的所有标签
# 生成混淆矩阵
conf_matrix = fusion_matrix(y_train, y_train_pred)

# 定义类标签
class_labels = [&#39;类别 0&#39;,&#39;类别 1&#39;,&#39;类别 2&#39;]

# 使用类标签可视化混淆矩阵
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt=“d”, cmap=“蓝调”, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel(&#39;预测标签&#39;)
plt.ylabel(&#39;真实标签&#39;)
plt.title(&#39;混淆矩阵&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78266973/machine-learning-why-is-my-confusion-matrix-like-this-when-i-tried-all-the-tes</guid>
      <pubDate>Wed, 03 Apr 2024 10:21:59 GMT</pubDate>
    </item>
    <item>
      <title>我如何提取边界框的 x 和 y 坐标</title>
      <link>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</link>
      <description><![CDATA[我的代码定义了一个 ROS 节点，用于使用 YOLO 算法在图像中进行对象检测，特别是使用从指定文件路径加载的 YOLO 模型。该节点订阅了广播图像的 ROS 主题 (/camera/color/image_raw)。收到图像后，会触发回调函数，使用 CvBridge 将 ROS 图像消息转换为 OpenCV 图像格式。然后该图像由 YOLO 模型处理以检测对象。结果（包括检测到的对象框）将打印到控制台。为了防止连续处理和重新处理图像，在处理第一个图像后设置一个标志（image_processed）。此外，处理后的图像将保存到指定的文件夹（临时）。该节点保持活动状态，侦听新图像，直到手动关闭。
#!/usr/bin/python3
导入罗斯比
从sensor_msgs.msg导入图像
从 cv_bridge 导入 CvBridge
导入CV2
导入操作系统
从 ultralytics 导入 YOLO

类节点（对象）：
    def __init__(自身):
        self.yolo_model = YOLO(&#39;/home/user/catkin_ws/src/run_folder/content/runs/obb/train/weights/best.pt&#39;)
        self.br = CvBridge()
        self.image_processed = False # 指示图像是否已处理的标志

        # 订阅发布图像的ROS主题
        rospy.Subscriber(“/camera/color/image_raw”, Image, self.callback)

    def 回调（自身，消息）：
        if not self.image_processed: # 仅当尚未处理图像时才处理
            image = self.br.imgmsg_to_cv2(msg) # 将ROS图像消息转换为OpenCV图像

            # 使用 YOLO 进行物体检测
            结果= self.yolo_model.predict（图像，显示= True）
            对于结果 [0] 中的 r：
                打印（“---------------------------”）
                打印（r.boxes）

            # 第一次处理后保存图像
            self.save_image(图像, &#39;临时&#39;)
            self.image_processed = True # 将标志设置为 True 以避免重新处理

    def save_image(自身, 图像, 文件夹名称):
        如果不是 os.path.exists(folder_name):
            os.makedirs(文件夹名称)
        
        file_path = os.path.join(folder_name, &#39;image.jpg&#39;)
        cv2.imwrite（文件路径，图像）
        print(f“图像保存在{file_path}”)

如果 __name__ == &#39;__main__&#39;:
    rospy.init_node（“image_processor_with_yolo”，匿名= True）
    节点 = 节点()
    rospy.spin() # 保持节点运行直到关闭


当我尝试提取边界框时，我得到“无”在输出中。]]></description>
      <guid>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</guid>
      <pubDate>Wed, 03 Apr 2024 10:11:55 GMT</pubDate>
    </item>
    <item>
      <title>Torchserve 工作流程因超过 14 个 python 异步请求而卡住</title>
      <link>https://stackoverflow.com/questions/78266554/torchserve-workflow-is-stuck-for-more-than-14-python-async-requests</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78266554/torchserve-workflow-is-stuck-for-more-than-14-python-async-requests</guid>
      <pubDate>Wed, 03 Apr 2024 09:12:56 GMT</pubDate>
    </item>
    <item>
      <title>服务模型时 MLFlow 抛出错误：配置的跟踪 uri 方案：“文件”对于与代理 mlflow-artifact 方案一起使用无效</title>
      <link>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</link>
      <description><![CDATA[我正在尝试使用 MLFlow CLI 在本地提供模型作为 REST 端点。 MLflow版本是2.11.3。以下是我正在使用的论点。
mlflow 模型服务 -m “runs:/7782c4a700cc49c1a04f9ce608d90358/knnmodel” --env-manager本地--端口5000

以下是例外情况：
回溯（最近一次调用最后一次）：
  文件“/home/vagrant/.local/bin/mlflow”，第 8 行，在  中。
    sys.exit(cli())
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1157 行，在 __call__ 中
    返回 self.main(*args, **kwargs)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1078 行，在 main 中
    rv = self.invoke(ctx)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1688 行，在调用中
    返回_process_result(sub_ctx.command.invoke(sub_ctx))
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1688 行，在调用中
    返回_process_result(sub_ctx.command.invoke(sub_ctx))
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1434 行，在调用中
    返回 ctx.invoke(self.callback, **ctx.params)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 783 行，在调用中
    返回 __callback(*args, **kwargs)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/cli.py”，第 104 行，在服务中
    返回 get_flavor_backend(
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/flavor_backend_registry.py”，第 44 行，位于 get_flavor_backend
    local_path = _download_artifact_from_uri(
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py”，第 105 行，位于 _download_artifact_from_uri
    返回 get_artifact_repository(artifact_uri=root_uri).download_artifacts(
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py”，第 124 行，位于 get_artifact_repository
    返回 _artifact_repository_registry.get_artifact_repository(artifact_uri)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py”，第 77 行，位于 get_artifact_repository
    返回存储库（artifact_uri）
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py”，第 27 行，位于 __init__ 中
    self.repo = get_artifact_repository(uri)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py”，第 124 行，位于 get_artifact_repository
    返回 _artifact_repository_registry.get_artifact_repository(artifact_uri)
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py”，第 77 行，位于 get_artifact_repository
    返回存储库（artifact_uri）
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py”，第 45 行，位于 __init__ 中
    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py”，第59行，在resolve_uri中
    _validate_uri_scheme（track_parse.scheme）
  文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py”，第 35 行，位于 _validate_uri_scheme 中
    引发 MlflowException(
mlflow.exceptions.MlflowException：配置的跟踪 uri 方案：“文件”对于与代理 mlflow-artifact 方案一起使用无效。允许的跟踪方案为：{&#39;https&#39;, &#39;http&#39;}

似乎无法从 MLflow Web 服务器访问 MLflow 工件。有解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</guid>
      <pubDate>Wed, 03 Apr 2024 09:02:38 GMT</pubDate>
    </item>
    <item>
      <title>客户与业务对话的短信分类</title>
      <link>https://stackoverflow.com/questions/78266176/text-message-classification-for-customer-business-conversations</link>
      <description><![CDATA[我想将客户的新消息分为 +-3 类。在某些情况下，企业是一家药房，消息通常是订单，但有时是一般性问题。我想将消息分为 A. 一般问题、B. 需要药剂师和 C. 不需要药剂师的订单。这将确保药剂师能够专注于需要的信息，从而提高业务效率。
我想知道我是否可以使用现有的预训练模型以及如何有效地做到这一点，或者我是否必须在旧消息上训练我自己的特定模型，如果可以的话我如何标记消息快速进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/78266176/text-message-classification-for-customer-business-conversations</guid>
      <pubDate>Wed, 03 Apr 2024 08:04:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PYMC 定义自定义似然函数？</title>
      <link>https://stackoverflow.com/questions/78266093/how-can-i-define-a-custom-likelihood-function-using-pymc</link>
      <description><![CDATA[使用 emcee 包执行 MCMC 时，我可以简单地定义自定义似然函数和后验函数，并将它们放入 emcee.EnsembleSampler 中。整个代码：
def log_likelihood(theta):
    teff_k、logg_k、feh_k、CFE、NFE、OFE、MGFE、ALFE、SIFE、CAFE、SCFE、TIFE、VFE、CRFE、COFE、NIFE = θ
    VMIC、HE_ABUN、LIFE、NAFE、SFE、KFE、MNFE、CUFE、ZNFE、YFE、SRFE、BAFE、NDFE、LAFE、EUFE、ZRFE、PRFE、CEFE、SMFE、AUFE、THFE、UFE = 其他参数
    input_para = [teff_k, logg_k, feh_k, VMIC, HE_ABUN, LIFE,
                  CFE、NFE、OFE、NAFE、MGFE、ALFE、
                  SIFE、SFE、KFE、CAFE、SCFE、TIFE、
                  VFE、CRFE、MNFE、COFE、NIFE、CUFE、
                  ZNFE、YFE、SRFE、BAFE、NDFE、LAFE、
                  EUFE、ZRFE、PRFE、CEFE、SMFE、AUFE、
                  THFE、UFE]
    input_para = np.array(input_para).T
    input_para = torch.tensor(input_para, dtype=torch.float64).view(1, -1)
    # 使用 Transformer 模型预测并计算 chi_square
    model1 = 变压器（num_layers、d_model、num_heads、d_ff、dropout、input_p_dim、output_s_dim）
    Predict_flux_7718 = 模型1(input_para)
    chi_squared = np.sum(residn ** 2 / yerrn ** 2 + np.log(yerrn ** 2))
    返回 -0.5 * 卡方

……
def log_probability(theta):
    lp = log_prior(theta)
    如果不是 np.isfinite(lp):
        返回-np.inf
    返回 lp + log_likelihood(theta)

……
 采样器 = emcee.EnsembleSampler(nwalkers, ndim, log_probability,
                                    池=池，
                                    移动=[(emcee.moves.DEMove(), 0.01),(emcee.moves.DESnookerMove(), 0.01)])

但是如果我想将 emcee 更改为 pymc 包，我该如何定义这个似然函数？例如  Likelihood = pm.Potential(&#39;likelihood&#39;, log_likelihood(theta))?]]></description>
      <guid>https://stackoverflow.com/questions/78266093/how-can-i-define-a-custom-likelihood-function-using-pymc</guid>
      <pubDate>Wed, 03 Apr 2024 07:50:36 GMT</pubDate>
    </item>
    <item>
      <title>图像超分辨率深度学习模型的指导</title>
      <link>https://stackoverflow.com/questions/78265945/guidance-with-image-super-resolution-deep-learning-models</link>
      <description><![CDATA[我正在寻找可以在嵌入式平台（Nvidia Jetson 类型）上实时运行的超分辨率模型，同时获得尽可能高的输出图像质量。
我一直在寻找单图像（SISR）和多帧（MFSR）选项。我主要看到四类模型：仅关注空间超分辨率（SISR）的模型、利用时间相关性（MFSR）的模型、尝试提取图像高频部分的模型以及基于视觉变换器的模型。然而，作为该领域的专家，我很快发现存在大量可能的模型，其中许多模型可能不适合嵌入式设备。我想就以下问题寻求一些指导：

首先，您会推荐一个特定的模型系列来实现我的目标吗？
您有特定的型号吗？

提前谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78265945/guidance-with-image-super-resolution-deep-learning-models</guid>
      <pubDate>Wed, 03 Apr 2024 07:26:45 GMT</pubDate>
    </item>
    <item>
      <title>使用非序列数据时，LSTM 给出更概括的结果，准确率为 89%，而使用序列数据时准确率为 64%</title>
      <link>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</link>
      <description><![CDATA[我正在研究时间序列分类。我使用了下面给出的两个预处理步骤

时间序列数据集 --&gt;切片时间序列 ---&gt;训练-验证-测试-分割 --&gt;模型训练
准确度——90%

时间序列数据集 --&gt;训练-验证-测试-分割 --&gt;独立切片训练/测试/验证 --&gt;模型训练
准确度 -- 64%


我目前正在努力为我的项目获取顺序概率分布，并已进入第二步。但是，我得到的准确度结果低于预期，甚至低于第一步。我已经调整了超参数并解决了类别不平衡的问题以消除偏差，但准确率没有提高到超过 64%。谁能提供一些关于为什么会发生这种情况的见解？
对于训练验证测试拆分，我使用 sklearn
x_main, x_test, y_main, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y,shuffle=True)
x_train, x_val, y_train, y_val = train_test_split(x_main, y_main, test_size= 0.1, random_state=42,stratify=y_main,shuffle=True)
至少我预计第二步的结果接近 80-85%]]></description>
      <guid>https://stackoverflow.com/questions/78265338/lstm-giving-more-generalize-result-with-accuracy-of-89-when-using-non-sequentia</guid>
      <pubDate>Wed, 03 Apr 2024 05:05:10 GMT</pubDate>
    </item>
    <item>
      <title>Autograd 返回无</title>
      <link>https://stackoverflow.com/questions/78263388/autograd-returning-none</link>
      <description><![CDATA[我正在尝试创建一个收缩自动编码器，我在几篇论文中读到，其主要思想是使用编码器输出相对于其输入的雅可比行列式的范数。
换句话说，我试图在使用原始输入的同时获取编码器输出的梯度。
到目前为止，我有这样的事情：
defcompute_model_loss（self，predX，X，latent_X）：
    梯度= torch.autograd.grad（输出= Latent_X，输入= X，grad_outputs = torch.ones_like（latent_X），
                                    create_graph = True，allow_unused = True）[0]
    
    print(渐变) # 无！
    frobenius_norm = torch.mean(torch.norm(梯度, p = &#39;fro&#39;, dim = (1,2)))
    
    Contractive_penalty = self.args[&#39;lambda&#39;] * frobenius_norm
    总损失 += 收缩惩罚

计算第一行中的梯度时会出现问题。由于某种原因，它返回 None，所以我首先尝试的是查看数据是什么样的。
输入数据X：(32 x 2866)
张量([[0.4663, 0.3859, 0.6573, ..., 0.7819, 0.0822, 0.3332],
        [0.4204, 0.8448, 0.6168, ..., 0.2698, 0.3503, 0.3372],
        [0.6329, 0.4084, 0.7437, ..., 0.3490, 0.4902, 0.8333],
        ...,
        [0.3004, 0.6908, 0.7698, ..., 0.8115, 0.9253, 0.1996],
        [0.6895, 0.6812, 0.4595, ..., 0.8959, 0.6600, 0.5660],
        [0.5647, 0.2448, 0.5046, ..., 0.6494, 0.4483, 0.5269]],
       device=&#39;cuda:0&#39;, grad_fn=)

编码器的输出：(32 x 32)
张量([[0.3837, 0.3975, 0.5000, ..., 0.6480, 0.9503, 0.8660],
        [0.4182, 0.5000, 0.8683, ..., 0.4916, 0.7044, 0.5293],
        [0.5000, 0.7034, 0.5588, ..., 0.3750, 0.5000, 0.5000],
        ...,
        [0.4598, 0.4478, 0.9167, ..., 0.8179, 0.7026, 0.5000],
        [0.5000, 0.5370, 0.4786, ..., 0.4529, 0.3132, 0.4245],
        [0.4134, 0.5000, 0.4898, ..., 0.4799, 0.5000, 0.7334]],
       device=&#39;cuda:0&#39;, grad_fn=)

所以这两个张量都有一个计算图...所有这些张量也将 requires_grad_ 设置为 True。
关于自动编码器架构，它主要是一个又一个的线性层，我的前向函数如下所示：
defforward(self, x)：
    编码 = self.encoder(x)
    解码 = self.decoder(编码)
    返回解码后的、编码后的

至于训练部分：
x_batch = torch.tensor(tr_model.X_train[b]).to(self.device)

x_batch.requires_grad_(True)
x_batch.retain_grad()

# h 是编码器的输出
x_pred_batch, h = tr_model.model.forward(x_batch)

# 这里我们计算收缩损失
损失 = tr_model.compute_model_loss(x_pred_batch, x_batch, h)

如果有帮助，当我使用普通自动编码器执行 loss.backward() 时，一切正常。 （虽然我在那里不使用编码器的输出）
非常感谢您阅读本文！！
编辑::
编码器的架构：
 self.encoder = torch.nn.Sequential(
            自定义块（input_dim，3000），
            自定义块(3000, 2500),
            自定义块(2500, 2000),
            自定义块(2000, 1500),
            自定义块(1500, 1200),
            自定义块(1200, 1000),
            自定义块(1000, 800),
            自定义块(800, 600),
            自定义块(600, 400),
            自定义块(400, 300),
            自定义块(300, 150),
            自定义块(150, 100),
            自定义块(100, 50),
            自定义块（50，L），
            torch.nn.Sigmoid()
        ）

其中 custom_block 是：
def custom_block（input_dim，output_dim，dropout_rate = 0.4）：
    返回 torch.nn.Sequential(
        torch.nn.Linear(input_dim, output_dim),
        torch.nn.BatchNorm1d(output_dim),
        torch.nn.PReLU(),
        torch.nn.Dropout(dropout_rate)
    ）
]]></description>
      <guid>https://stackoverflow.com/questions/78263388/autograd-returning-none</guid>
      <pubDate>Tue, 02 Apr 2024 18:36:19 GMT</pubDate>
    </item>
    <item>
      <title>如何修复此错误没有名为“llama_index.llms.llama_cpp”的模块</title>
      <link>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</link>
      <description><![CDATA[我尝试将 mixtral-8x7b 与我自己的数据一起使用，但没有成功。这是我的代码
导入火炬
从 llama_index.llms.llama_cpp 导入 LlamaCPP
从 llama_index.llms.llama_cpp.llama_utils 导入 messages_to_prompt、completion_to_prompt
llm = 骆驼CPP(
    model_url=None, # 我们将在本地加载。
    model_path=&#39;./Models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#39;, # 4 位模型
    温度=0.1，
    max_new_tokens=1024, # 增加以支持更长的响应
    context_window=8192, # Mistral7B 有一个 8K 上下文窗口
    生成_kwargs={},
    # 至少设置为 1 才能使用 GPU
    model_kwargs={“n_gpu_layers”: 40}, # 40 对于 RTX 3090 来说是一个很好的层数，如果您的 VRAM 少于 24GB，您可能需要减少层数
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    详细=真
）

这给出了错误“没有名为“llama_index.llms.llama_cpp”的模块”。
我已经安装了 llama_index，使用了我的 MAC Mini 以及 Google Colab 的 GPU
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</guid>
      <pubDate>Tue, 02 Apr 2024 17:10:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 openai 和 langchain 将已创建的 chromadb 集合与法学硕士一起使用？</title>
      <link>https://stackoverflow.com/questions/77642444/how-can-you-use-an-already-created-chromadb-collection-with-a-llm-using-openai-a</link>
      <description><![CDATA[我已经使用其文档和元数据创建了 chromadb 集合。
问题是当我想使用 langchain 创建 llm 并传递此 chromadb 集合以用作知识库时。
langchain_chroma = 色度(
客户端=持久客户端，
集合名称=集合.名称,
embedding_function = openai_ef，
）

llm_model =“gtp35turbo-最新”

llm = AzureChatOpenAI(
   api_key=openai_api_key,
   api_version=openai_api_version,
   azure_endpoint=openai_api_base,
   模型=llm_模型）

qa_chain = RetrievalQA.from_chain_type(
   嗯，
   检索器=langchain_chroma.as_retriever(),
   chain_type=&quot;精炼&quot;
）

当我想跑步时：
qa_chain.run(“对象检测问题需要多少数据科学家”)

我收到此错误：
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-81-3cdb65aeb43e&gt;在&lt;细胞系：1&gt;()
----&gt; 1 qa.run(“对象检测问题需要多少数据科学家”)

9帧
/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py 中相似性_search_with_score(self, query, k, filter, where_document, **kwargs)
    第430章）
    第431章：
--&gt;第432章
    第433章
    第434章

AttributeError：“OpenAIEmbeddingFunction”对象没有属性“embed_query”

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77642444/how-can-you-use-an-already-created-chromadb-collection-with-a-llm-using-openai-a</guid>
      <pubDate>Mon, 11 Dec 2023 21:17:23 GMT</pubDate>
    </item>
    <item>
      <title>打印函数在 Keras/Tensorflow 中的调用函数内不打印任何内容</title>
      <link>https://stackoverflow.com/questions/72176003/print-function-is-printing-nothing-inside-call-function-in-keras-tensorflow</link>
      <description><![CDATA[我想使用 print 命令打印下面的调用函数中的一些对象，但当代码成功运行时它不会打印任何内容。我正在阅读 (THIS) keras 调试教程，但我仍然很困惑为什么它不打印任何东西。
&lt;前&gt;&lt;代码&gt;#Hyperparams
学习率 = 0.001
权重衰减 = 0.0001
批量大小 = 100
纪元数 = 1

image_size = 72 # 我们将输入图像调整为这个大小
patch_size = 6 # 从输入图像中提取的补丁的大小
num_patches = (image_size // patch_size) ** 2
投影暗度 = 64
头数 = 4
变压器单位 = [
投影_dim * 2，
投影_暗淡，
] # 变压器层的大小
变压器层数 = 8
mlp_head_units = [2048, 1024]

我想打印下面的调用函数中的（位置和编码）。为此，我使用了打印，但它不起作用。而此处，他们就这样做了。
类 PatchEncoder(layers.Layer):
  def __init__(自身、num_patches、projection_dim、position_embedding):
  超级.__init__()
  self.num_patches = num_patches
  self.projection=layers.Dense(units=projection_dim)
  self.position_embedding = 层.Embedding(
    input_dim=num_patches，output_dim=projection_dim
  ）

  def 调用（自身，补丁）：
  位置 = tf.range(start=0, limit=self.num_patches, delta=1)
  编码 = self.projection(patch) + self.position_embedding(positions)
  print(&quot;编码形状为：&quot;,encoded.shape)
  print(&quot;pos.shape 是：&quot;, Positions.shape)
  返回编码
]]></description>
      <guid>https://stackoverflow.com/questions/72176003/print-function-is-printing-nothing-inside-call-function-in-keras-tensorflow</guid>
      <pubDate>Mon, 09 May 2022 17:27:22 GMT</pubDate>
    </item>
    <item>
      <title>pytorch .stack .squeeze后的最终形状</title>
      <link>https://stackoverflow.com/questions/51851966/pytorch-stack-final-shape-after-squeeze</link>
      <description><![CDATA[我有一个 200 列 x 2500 行的 pandas 数据框，我将其转换为张量 
张量 = torch.tensor(df.values)
张量.size() =&gt; ([2500,200])

我将其分块并枚举

&lt;前&gt;&lt;代码&gt;列表=[]
对于 i，枚举中的块（tensor.chunk（100，dim = 0））
    chunk.size =&gt;([25,200])
    输出=隐藏层（块）
    输出.size() =&gt; ([25,1])
    列表+=输出

块被输入到一些层并作为 1 个特征张量输出。所以现在我有一个 100 个张量的列表，每个张量有 25 个 1、100x25x1 的块
所以我
stacked = torch.stack(list, 1).squeeze(2)
stacked.size()=([25,100])

我已经尝试过堆叠和挤压，但我似乎无法回到我想要的 ([2500,1]) 。我错过了什么吗？如果您能快速帮助我了解堆叠和挤压的作用以及为什么它对我不起作用，我将永远感激您！谢谢]]></description>
      <guid>https://stackoverflow.com/questions/51851966/pytorch-stack-final-shape-after-squeeze</guid>
      <pubDate>Wed, 15 Aug 2018 02:01:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：如何解决 RuntimeError：就地操作只能用于不与任何其他变量共享存储的变量</title>
      <link>https://stackoverflow.com/questions/45693586/pytorch-how-to-get-around-the-runtimeerror-in-place-operations-can-be-only-use</link>
      <description><![CDATA[使用 PyTorch，我在使用两个变量进行操作时遇到问题：
sub_patch : [torch.FloatTensor 大小 9x9x32]

pred_pa​​tch : [torch.FloatTensor 大小 5x5x32]

sub_patch是torch.zeros创建的变量
pred_pa​​tch 是一个变量，我使用嵌套 for 循环对其中的 25 个节点进行索引，并与其相应的大小为 [5,5,32] 的唯一过滤器 (sub_filt_patch) 相乘。结果被添加到 sub_patch 中的相应位置。
这是我的一段代码：
对于范围内的 i(filter_sz)：
    对于范围内的j（filter_sz）：

        # 从滤波器张量中索引正确的滤波器
        sub_filt_col = (patch_col + j) * filter_sz
        sub_filt_row = (patch_row + i) * filter_sz

        sub_filt_patch = sub_filt[sub_filt_row:(sub_filt_row + filter_sz), sub_filt_col:(sub_filt_col+filter_sz), :]

        # 将过滤器和 pred_pa​​tch 相乘并求和到子补丁上
        sub_patch[i:(i + filter_sz), j:(j + filter_sz), :] += (sub_filt_patch * pred_pa​​tch[i,j]).sum(dim=3)

我从这段代码的底行得到的错误是
运行时错误：就地操作只能用于不与任何其他变量共享存储的变量，但检测到有 2 个对象共享它

我明白为什么会发生这种情况，因为 sub_patch 是一个变量，而 pred_pa​​tch 也是一个变量，但是我怎样才能解决这个错误呢？任何帮助将不胜感激！
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/45693586/pytorch-how-to-get-around-the-runtimeerror-in-place-operations-can-be-only-use</guid>
      <pubDate>Tue, 15 Aug 2017 13:12:39 GMT</pubDate>
    </item>
    </channel>
</rss>