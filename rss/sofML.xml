<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Apr 2024 21:13:11 GMT</lastBuildDate>
    <item>
      <title>为什么我的 cGAN 生成的似乎只是噪声？</title>
      <link>https://stackoverflow.com/questions/78289217/why-is-my-cgan-generating-what-seems-to-be-just-noise</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78289217/why-is-my-cgan-generating-what-seems-to-be-just-noise</guid>
      <pubDate>Sun, 07 Apr 2024 20:36:07 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 预测的问题</title>
      <link>https://stackoverflow.com/questions/78288928/problems-with-tensorflow-forcasting</link>
      <description><![CDATA[我正在创建一个机器学习预测模型来预测苏格兰的风能和太阳能。当通过预测模型运行它时，它会产生大约 0.00004 的损失，所以我假设它能够很好地预测这些数据。但这就是产生的
编辑：
对于上下文，使用的数据框是苏格兰某个地点平均风速的实际值。它采用 M/S 格式，并已标准化以用于预测算法。
带有预测的图表
显然这是不对的，我不知道如何解决它。
这是我使用的代码
df = pd.DataFrame(data=wind_data[0], columns=[&#39;mean_wind_speed&#39;])

df.fillna(方法=&#39;bfill&#39;, inplace=True)

输入形状 = (365*24*5, 1)
输出形状 = (30*24, 1)

def 准备数据(df):
    n = len(df)
    train_df = df[0:int(n * 0.6)]
    val_df = df[int(n * 0.6):int(n * 0.8)]
    test_df = df[int(n * 0.8):]

    train_std = train_df.std().values
    train_mean = train_df.mean().values

    train_df = (train_df - train_mean) / train_std
    val_df = (val_df - train_mean) / train_std
    test_df = (test_df - train_mean) / train_std

    返回train_df、val_df、test_df

train_df、val_df、test_df = 准备数据(df)

def create_lstm_model(input_shape,learning_rate=0.001):
    优化器 = Adam(学习率=学习率)
    模型=顺序（[
        LSTM（64，输入形状=输入形状），
        密集(1)
    ]）
    model.compile（优化器=优化器，损失=&#39;mse&#39;，指标=[&#39;mae&#39;]）
    返回模型

自定义学习率 = 0.001

lstm_model = create_lstm_model(input_shape=(input_shape[0], input_shape[1]),learning_rate=custom_learning_rate)
历史= lstm_model.fit（train_df，train_df，validation_data =（val_df，val_df），epochs = 10）

def recursive_forecast（模型，初始输入，预测步骤）：
    预测=[]
    当前输入 = 初始输入
    对于 _ 在范围内（forecast_steps）：
        # 预测下一个时间步
        next_prediction = model.predict(current_input[np.newaxis, :, :])
        # 将预测追加到预测中
        预测.append(next_prediction)
        # 通过删除第一个时间步并附加预测来更新当前输入
        current_input = np.concatenate((current_input[1:], next_prediction), axis=0)
    返回 np.array(预测).reshape(-1, 1)

initial_input = test_df.iloc[0:input_shape[0]] # 使用最后一个 &#39;input_shape[0]&#39; 数据点作为初始输入
Forecast_steps = output_shape[0] # 预测的时间步数
预测= recursive_forecast（lstm_model，initial_input.values，forecast_steps）

plt.plot(np.arange(len(test_df)), test_df, label=&#39;实际&#39;, color=&#39;蓝色&#39;)
plt.plot(np.arange(len(test_df), len(test_df) + len(预测)), 预测, label=&#39;预测&#39;, color=&#39;红色&#39;)
plt.xlabel(&#39;时间步长&#39;)
plt.ylabel(&#39;风速&#39;)
plt.title(&#39;预测&#39;)
plt.网格（真）
plt.图例()
plt.show()

任何帮助将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78288928/problems-with-tensorflow-forcasting</guid>
      <pubDate>Sun, 07 Apr 2024 18:45:26 GMT</pubDate>
    </item>
    <item>
      <title>关闭选项卡后如何保持 Paperspace 的渐变笔记本运行</title>
      <link>https://stackoverflow.com/questions/78288786/how-do-i-keep-paperspaces-gradients-notebooks-running-after-i-close-my-tabs</link>
      <description><![CDATA[我正在纸空间的渐变上运行笔记本。当我预订一台机器 4 小时并开始在其中运行 jupyter 笔记本时，如果我关闭浏览器，执行就会停止。我怎样才能改变这种行为？我在 google collab pro 上没有遇到这个问题
我尝试了免费和付费 GPU 机器，并尝试升级到 Pro 帐户]]></description>
      <guid>https://stackoverflow.com/questions/78288786/how-do-i-keep-paperspaces-gradients-notebooks-running-after-i-close-my-tabs</guid>
      <pubDate>Sun, 07 Apr 2024 17:58:26 GMT</pubDate>
    </item>
    <item>
      <title>澄清：模型评估 - Tran 和 Val 损失</title>
      <link>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</link>
      <description><![CDATA[我不确定我的模型是否表现良好。我的理解是，如果我的模型在训练和验证损失之间进行调整，也表明没有欠拟合或过拟合。然而，我的教授不同意，并说我的模型没有经过充分的训练，这表明图表本身是不正确的。
我不确定问题出在哪里或者到底出了什么问题。有人可以提供一些指导或帮助吗？
我的模型的训练和 val 损失图
我通过在嵌入层中添加一些噪声来训练模型。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</guid>
      <pubDate>Sun, 07 Apr 2024 17:35:17 GMT</pubDate>
    </item>
    <item>
      <title>如何创建 CNN-LSTM 架构？</title>
      <link>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</link>
      <description><![CDATA[我尝试创建混合 CNN 和 LSTM 模型。我遇到了与架构形状相关的问题。这导致epoch无法跑完数据200次。
我的数据大小是（96,2）
错误：
纪元 1/200
    178/未知 9s 34ms/步 - 损耗：1.2366 - mse：5.4560
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
第 4 行 [40] 中的单元格
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs = 200，batch_size = 128）

无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[78,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_23678]

CNN-LSTM模型：
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn


编译模型
def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()

model_create.fit（train_dataset，epochs = 200，batch_size = 128）


我曾尝试在 flatten() 函数之前添加 reshape 来改变形状。我还减小了批量大小和纪元大小。这些都不起作用。如何将我的模型与 train_data 相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 16:34:57 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降：缩减特征集的运行时间比原始特征集更长</title>
      <link>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</link>
      <description><![CDATA[我尝试用 Python 实现梯度下降算法来解决机器学习问题。我正在使用的数据集已经过预处理，并且在比较两个数据集（一个具有原始特征，另一个数据集使用奇异值分解（SVD）减少了前一组的维度）时，我在运行时观察到了意外的行为。我始终如一观察到，与减少的数据集相比，较大的原始数据集的梯度下降算法的运行时间较低，这与我的预期相反。鉴于数据集较小，缩减后的数据集的运行时间是否应该更短？我试图理解为什么会发生这种情况。
以下是相关代码片段：
导入时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

def h_theta(X1, theta1):
    # 假设函数的实现
    返回 np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
    # 成本函数的实现
    返回 np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, θ):
    # 梯度计算
    梯度 = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
    返回梯度

def 梯度下降(X1, y1):
    theta_initial = np.zeros(X1.shape[1]) # 用零初始化 theta
    迭代次数 = 1000
    学习率 = [0.1, 0.01, 0.001]
    成本迭代 = []
    θ值 = []
    开始 = 时间.time()
    对于 Learning_rates 中的 alpha：
        theta = theta_initial.copy()
        成本历史 = []
        对于范围内的 i(num_iterations)：
            梯度 = grad(X1, y1, θ)
            theta = theta - np.dot(alpha, 梯度)
            成本 = j_theta(X1, y1, θ)
            cost_history.append(成本)
        cost_iterations.append(cost_history)
        theta_values.append(theta)
    结束 = 时间.time()
    print(f&quot;所用时间：{end - start} 秒&quot;)
    图, axs = plt.subplots(len(learning_rates), Figsize=(8, 15))
    对于 i，枚举中的 alpha（学习率）：
        axs[i].plot(范围(num_iterations), cost_iterations[i], label=f&#39;alpha = {alpha}&#39;)
        axs[i].set_title(f&#39;学习率：{alpha}&#39;)
        axs[i].set_ylabel(&#39;成本 J&#39;)
        axs[i].set_xlabel(&#39;迭代次数&#39;)
        axs[i].legend()
    plt.tight_layout()
    plt.show()

# 使用 SVD 将 X 减少到 3 个特征（列）的代码：
# 对 X 进行奇异值分解并将其减少到 3 列
U、S、Vt = np.linalg.svd(X_归一化)
# 将 X 减少到 3 列
X_reduced = np.dot(X_normalized, Vt[:3].T)

# 打印 X_reduced 的前 5 行
print(&quot;X_reduced 的前 5 行：&quot;)
# 标准化 X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;减少和归一化后 X 的均值和标准差：\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# 打印缩小后的 X 的形状以确认它只有 3 个特征
print(&quot;X_reduced 的形状：&quot;, X_reduced.shape)

# 将截距列添加到 X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))


# 用法示例
# X_normalized_with_intercept 和 y_normalized 表示原始数据集
# X_reduced_with_intercept 和 y_normalized 表示缩减后的数据集

# 对原始数据集进行梯度下降
梯度下降（X_normalized_with_intercept，y_normalized）

# 对缩减后的数据集执行梯度下降
梯度下降（X_reduced_with_intercept，y_归一化）

在我的梯度下降实现中，与完整数据集相比，什么可能导致缩减数据集始终具有更长的运行时间？任何有关故障排除的见解或建议将不胜感激。
我尝试重写和审查我的实现，但似乎对于大多数学习率和迭代次数的增加，较大功能集的运行时间低于其 SVD 子集。]]></description>
      <guid>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</guid>
      <pubDate>Sun, 07 Apr 2024 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>内核形状必须与输入具有相同的长度，但接收形状为 (3, 3, (None, 7, 7, 512), 64) 的内核和形状为 [(None, 7, 7, 512)] 的输入</title>
      <link>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</guid>
      <pubDate>Sun, 07 Apr 2024 12:27:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行高效的成对采样</title>
      <link>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</link>
      <description><![CDATA[我有一个包含大约 150k 图像和 150k 音频的数据集。每个图像，都有相应的音频。我希望我的网络能够学习使用图像数据将一个音频映射到另一个音频。
当我创建这些源目标对时，我的目标是从训练数据的子集中采样的，并且还必须满足特定的阈值标准。然后创建源目标对进行训练。
我现在这样做的方式是导出目标采样的子集并在自定义数据集的 getitem() 中应用阈值。这花费了太长的时间并在训练过程中造成了瓶颈。
我不确定如何解决这个问题并加快训练速度。我该如何解决这个问题。
def _getitem_(idx):
   sourceimg、sourceaudio = self.data[idx]
   Target_dataset = create_subset(sourceimg, sourceaudio)
   迭代次数 = 0
   虽然正确：
     targetimg, targetaudio = np.random(Target_dataset)
     如果 cal_value(target_img) &gt;= 阈值：
       返回sourceimg、sourceaudio、targetimg、targetaudio
     迭代+=1
     如果迭代次数 &gt;= 10：
       _getitem_(random.randint(0, len(self.dataset))

我在训练期间尝试过这样做。但就我拥有的数据量而言，这根本没有效率。如何在不增加数据加载器中的批处理大小或 num_worker 的情况下加快训练速度]]></description>
      <guid>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</guid>
      <pubDate>Sun, 07 Apr 2024 12:16:21 GMT</pubDate>
    </item>
    <item>
      <title>如何结合CNN-LSTM架构？</title>
      <link>https://stackoverflow.com/questions/78287568/how-to-combine-cnn-lstm-architecture</link>
      <description><![CDATA[CNN和LSTM的架构代码
我想创建 CNN-LSTM 。我在 keras 中遇到了 Flatten() 问题，形状不匹配导致模型无法运行所有数据。
数据形状 = (96,2)
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn

def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()


model_create.fit（train_dataset，epochs=5，batch_size=128）

错误：
&lt;前&gt;&lt;代码&gt;
纪元 1/5
    178/未知 9s 40ms/步 - 损耗：3.5391 - mse：18.2760
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
[213] 单元格，第 4 行
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs=5，batch_size=128）
      5 #其他：
      6 #model.fit(train_dataset,epochs =200,batch_size = 512)
无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[85,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_64945]


为了修复 Flatten()，我尝试使用 Reshape()，但是模型拟合无法运行 5 个时期。如何修复形状以使模型拟合运行 5 个时期？]]></description>
      <guid>https://stackoverflow.com/questions/78287568/how-to-combine-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 11:06:09 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV 新手，我如何安装/构建 opencv_traincascade</title>
      <link>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</link>
      <description><![CDATA[所以我一直致力于机器学习项目，并且需要使用 opencv_traincascade 训练自定义数据集。但每当我尝试安装它时，它就永远无法工作。我还有其他东西，比如 opencv_annotation 和其他东西可以工作，但是 traincascade 或 event createsamples 不起作用。我必须手动构建这些吗？
我下载了mingw-gcc、cmake，在网上找不到可行的解决方案。顺便说一句，我有 opencv 4.9.0，手动安装在 anaconda 和我的 C: 驱动器中。我也尝试过寻找一些第三方，他们安装了整个 opencv 并且可以复制，但没有运气。任何帮助将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</guid>
      <pubDate>Sun, 07 Apr 2024 03:46:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络对不同输入的相同预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</link>
      <description><![CDATA[我正在尝试在 Matlab 中构建一个神经网络，而不使用深度学习工具箱，其中一个隐藏层可以预测图像显示的是脑肿瘤还是健康的大脑。我使用的数据库包含 4000 张图像（2000 张脑肿瘤图像和 2000 张健康大脑图像）。
我面临的问题是准确率为 50%，并且每张图像的预测都是相同的。结果，混淆矩阵的一列始终为 0。我尝试更改学习率，尝试更改隐藏层上的神经元数量，但没有任何改变输出。我使用的隐藏层和输出层的激活函数都是 sigmoid，并且使用的优化算法是梯度下降。]]></description>
      <guid>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</guid>
      <pubDate>Sat, 06 Apr 2024 15:52:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle Notebook 中降级 Tensorflow 版本时遇到问题；我应该怎么办</title>
      <link>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</link>
      <description><![CDATA[我在tensorflow versino 2.11.0中编写了一个代码，但是最近我的代码无法运行，发现当前的tensorflow版本2.15.0是主要问题，所以我使用代码降级了我的版本！pip install tensorflow- GPU==2.11.0
但是我的笔记本确实找到了任何 GPU，尽管我像以前一样在我的 Kaggle 笔记本中启用了 GPU P100 加速器。我还在代码中检查 GPU。
导入tensorflow为tf

如果 tf.test.gpu_device_name():

print(&#39;默认 GPU 设备：{}&#39;.format(tf.test.gpu_device_name()))

别的：

print(&quot;请安装GPU版本的TF&quot;)

得到了
&lt;前&gt;&lt;代码&gt;
    请安装GPU版本的TF


请在这方面帮助我。我的项目截止日期非常接近
在 Kaggle 笔记本中降级 Tensorflow 版本时遇到问题。]]></description>
      <guid>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</guid>
      <pubDate>Fri, 05 Apr 2024 10:28:53 GMT</pubDate>
    </item>
    <item>
      <title>ML ColumnTransformer OneHotEncoder</title>
      <link>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</link>
      <description><![CDATA[当在数据帧的第一列中转换分类数据时，我发现 ColumnTransformer 与 OneHotEncoder 出现奇怪的行为。当我向 csv 文件添加一行时，就会发生此行为。
初始数据为：
标题、每日总收入、影院、DayInYear
AC汀巴黎,307,5,257
给莫莫的一封信，307,5,257
生命的另一天,307,5,257
批准收养，307,5,257
四月与非凡的世界, 307,5,257
美女,307,5,257
鸟男孩被遗忘的孩子，307,5,257
奇科丽塔,307,5,257

运行代码时
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd

数据集 = pd.read_csv(&#39;../data/GKIDS_DayNum_test_names.csv&#39;)
数据集[&#39;标题&#39;].str.strip()
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.preprocessing 导入 OneHotEncoder

title_column_index = dataset.columns.get_loc(&#39;标题&#39;)
print(&#39;标题索引：&#39;, title_column_index)
ct = ColumnTransformer(transformers=[(&#39;编码器&#39;, OneHotEncoder(), [title_column_index])], 剩余=&#39;passthrough&#39;)
X_Encoded = np.array(ct.fit_transform(X))
打印（X_编码）

结果是正确的：
&lt;前&gt;&lt;代码&gt;[[1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 307 5]]

但是，当我添加附加行时：BlueGiant,307,5,257
到文件并重新运行代码我得到奇怪的输出：
&lt;前&gt;&lt;代码&gt; (0, 0) 1.0
  (0, 9) 307.0
  (0, 10) 5.0
  (1, 1) 1.0
  (1, 9) 307.0
  (1, 10) 5.0
  (2, 2) 1.0
  (2, 9) 307.0
  (2, 10) 5.0
  (3, 3) 1.0
  (3, 9) 307.0
  (3, 10) 5.0
  (4, 4) 1.0
  (4, 9) 307.0
  (4, 10) 5.0
  (5, 5) 1.0
  (5, 9) 307.0
  (5, 10) 5.0
  (6, 6) 1.0
  (6, 9) 307.0
  (6, 10) 5.0
  (7, 8) 1.0
  (7, 9) 307.0
  (7, 10) 5.0
  (8, 7) 1.0
  (8, 9) 307.0
  (8, 10) 5.0

我不明白为什么会这样。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</guid>
      <pubDate>Thu, 04 Apr 2024 15:25:57 GMT</pubDate>
    </item>
    <item>
      <title>Yolo v9 保存每个纪元和损失</title>
      <link>https://stackoverflow.com/questions/78232885/yolo-v9-saving-each-epoch-and-loss</link>
      <description><![CDATA[我有这段代码可以在自定义数据集上训练 yolov9 模型。但由于我只有 T4 GPU 并且我的数据集很大，所以它只训练了 3 个 epoch，然后就停止了。我想单独训练每个纪元并保存它，它是损失。我该怎么做？
这是我正在使用的代码：
%cd /content/my_drive/MyDrive/yolov9/yolov9

!python train.py \
--batch 16 --epochs 25 --img 640 --min-items 0 --close-mosaic 15 \
--data /content/my_drive/MyDrive/yolov9/yolov9/data.yaml \
--weights /content/my_drive/MyDrive/yolov9/yolov9/gelan-c.pt \
--cfg 模型/检测/gelan-c.yaml \
--hyp hyp.scratch-high.yaml
]]></description>
      <guid>https://stackoverflow.com/questions/78232885/yolo-v9-saving-each-epoch-and-loss</guid>
      <pubDate>Wed, 27 Mar 2024 15:36:49 GMT</pubDate>
    </item>
    <item>
      <title>我在 Gradio 部署后收到输出错误</title>
      <link>https://stackoverflow.com/questions/78058612/i-am-getting-output-error-after-gradio-deployment</link>
      <description><![CDATA[我正在使用 Gradio 部署机器学习模型，在 Gradio 上部署后，我在输入和输出显示错误后收到错误
此输出的代码是
导入gradio为gr
将 numpy 导入为 np
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入 StandardScaler、MinMaxScaler

# 示例数据（将其替换为您的实际数据）
X_train = np.array([[230.1, 37.8, 69.2],
                    [44.5、39.3、45.1]、
                    [17.2、45.9、69.3]、
                    [151.5、41.3、58.5]、
                    [180.8, 10.8, 58.4]])
y_train = np.array([22.1, 10.4, 9.3, 18.5, 12.9])

# 初始化并训练您的线性回归模型
缩放器 = MinMaxScaler()
缩放器.fit(X_train)
X_train_scale = 缩放器.transform(X_train)

lm = 线性回归()
lm.fit(X_train_scale, y_train)

# 定义预测函数
def Predict_sales(电视、广播、报纸):
    # 缩放输入特征
    input_features = scaler.transform([[电视、广播、报纸]])
    # 预测销量
    预测 = lm.predict(input_features)
    返回预测[0]

# 创建渐变界面
tv_input = gr.Number(标签=“电视”)
radio_input = gr.Number(label=&quot;Radio&quot;)
newspaper_input = gr.Number(label=“报纸”)
output_text = gr.Textbox(label=&quot;预测销售额&quot;)

gr.Interface(fn=predict_sales,
             输入=[电视输入、广播输入、报纸输入]、
             输出=输出文本，
             title=&quot;销售预测&quot;,
             description=&quot;输入广告费用以预测销售额&quot;,
            debug=True,enable_queue=True).launch()

]

如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78058612/i-am-getting-output-error-after-gradio-deployment</guid>
      <pubDate>Mon, 26 Feb 2024 04:17:40 GMT</pubDate>
    </item>
    </channel>
</rss>