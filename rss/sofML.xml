<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 27 Dec 2024 01:15:47 GMT</lastBuildDate>
    <item>
      <title>更改 YOLO 片段预测图像中的片段颜色</title>
      <link>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</link>
      <description><![CDATA[我正在开展一个对象检测项目，在一组图像中检测三种类型的对象。此图像显示了 yolo 片段模型预测模型示例 图像。
我遇到的问题是检测到的符号是白色的。这种颜色对某些人来说可能可见，但对其他人来说可能不可见。有没有办法改变这种颜色？我对 Python 很陌生。我已经预测了整个集合的图像（总计超过 100,000 张图像）。
我使用使用 LabelStudio 标记感兴趣的符号的图像训练了 YOLOv8 模型。在标记过程中，我使用了红色、蓝色和绿色等颜色。但不知何故，在获得最终的 YOLOv8 模型 (best.pt) 并运行预测后，检测到的符号的颜色与最初使用的颜色不同。我有以下问题：

如何更改检测到的物体的白色？

有没有办法确保在 labelstudio 上标记时使用的颜色保留在预测图像中？


import cv2
import numpy as np

# 定义一个函数来替换完整的矩形白色边界框的颜色
def replace_white_rectangles_with_green(image_path, output_path):
# 读取图像
image = cv2.imread(image_path, cv2.IMREAD_COLOR)

# 将图像转换为灰度以进行轮廓检测
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# 对灰度图像进行阈值处理以创建白色区域的二元掩码
_, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)

# 查找轮廓以检测形状
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for contour in contours:
# 近似轮廓以检查其是否形成矩形
epsilon = 0.02 * cv2.arcLength(contour, True)
approx = cv2.approxPolyDP(contour, epsilon, True)

# 检查轮廓是否有四个边（矩形或正方形）
if len(approx) == 4:
# 验证形状是否凸（以确认它是矩形/正方形）
if cv2.isContourConvex(approx):
# 将边界框的白色替换为亮绿色
cv2.drawContours(image, [contour], -1, (0, 255, 0), thicken=cv2.FILLED)

# 保存修改后的图像
cv2.imwrite(output_path, image)

# 输入和输出图像的路径
input_image_path = &quot;path_to_your_image.jpg&quot; # 替换为输入图像路径
output_image_path = &quot;path_to_save_modified_image.jpg&quot; # 替换为所需的输出路径

# 将函数应用于图像
replace_white_rectangles_with_green(input_image_path, output_image_path)

print(&quot;图像已处理，白色矩形边界框被绿色替换。&quot;)

我尝试了上述代码，但不知何故它只会随机产生绿色框或在原本为白色的原始背景上产生绿色框。]]></description>
      <guid>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</guid>
      <pubDate>Thu, 26 Dec 2024 16:15:02 GMT</pubDate>
    </item>
    <item>
      <title>对 ML 输入数据进行标准化和规范化都可以得到最佳结果，为什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79309671/both-standardizing-and-normalizing-my-input-data-for-ml-gives-the-best-results</link>
      <description><![CDATA[当我将输入数据的标准化和规范化结合用于混合 ANN 模型时，它会产生最佳结果。
但我找不到任何地方，为什么。我基于一篇论文的方法，但他们也没有证明他们的做法是合理的。
有人知道为什么吗？
与同时标准化和规范化我的输入数据相比，对我的输入数据进行标准化后，R2 小于 0.71，RMSE 更高，结果更不稳定。]]></description>
      <guid>https://stackoverflow.com/questions/79309671/both-standardizing-and-normalizing-my-input-data-for-ml-gives-the-best-results</guid>
      <pubDate>Thu, 26 Dec 2024 14:04:57 GMT</pubDate>
    </item>
    <item>
      <title>如何利用代表性模式原理提高灰度纹理分割的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</guid>
      <pubDate>Thu, 26 Dec 2024 13:48:07 GMT</pubDate>
    </item>
    <item>
      <title>需要 chromadb 和 transformers 一起使用，但要求有冲突，因为 chromadb 需要 0.20 版本的 tokenizers，而后者需要 0.21 版本</title>
      <link>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</link>
      <description><![CDATA[我必须在一个项目中同时使用 chromadb 和 transformers，但 chromadb 需要 &lt;=0.20.3 版本的 tokenizers，而 transformers 需要 &gt;=0.21 版本的 tokenizers，并且与 chromadb 兼容的旧版本 transformers 需要 rust 编译器，因此这也不是一种选择。
我尝试升级 transformers、tokenizers，也尝试降级 transformers，但都不起作用，而对于所有这些，我都在使用虚拟环境。]]></description>
      <guid>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</guid>
      <pubDate>Thu, 26 Dec 2024 11:03:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在验证数据上获得相同的准确度？</title>
      <link>https://stackoverflow.com/questions/79308661/why-am-i-getting-the-same-accuracy-on-validation-data</link>
      <description><![CDATA[我在验证数据上获得了相同的准确率，而训练数据的准确率在每个时期变化不大。
训练数据包含 19670 张图像（14445：0 类，5225：1 类）。验证数据包含 4918 张图像（3612：0 类，1306：1 类）。
由于类别不平衡，我应用了计算类别权重，因此少数类别的惩罚更高
但是，验证数据的准确率相同，并且每个时期的损失变化不大。
我对所有训练数据应用了数据增强。此外，我使用的是 VGG16，解冻了最后 5 层，并在网络中添加了一些密集层。我改变了 learning_rate 值，但没有得到任何显着的改进，结果仍然相同。它在训练和验证数据的准确率上遵循相同的模式，没有任何改进，值重复。
这是代码：
类权重：
来自 sklearn.utils.class_weight import compute_class_weight
pesos=compute_class_weight(&quot;balanced&quot;,classes=np.unique(labels),y=labels) #这些比索与每个类相对应

#创建了一个字典，在进入时刻将参数作为 class_weight 传递给它
pesos_clases={i: pesos[i] for i in range(len(pesos))}

神经网络：
来自tensorflow.keras.applications import vgg16
VGG16=vgg16.VGG16(
weights=&quot;imagenet&quot;, 
include_top=False, 
input_shape=(224,224,3)
)

VGG16.trainable=True #Entrenable

#解冻最后 5 层
for layer in VGG16.layers[:-5]:
layer.trainable=False 

from tensorflow.keras import layer
from tensorflow import keras
x=VGG16.output
x=layers.GlobalAveragePooling2D()(x)

x=layers.Dense(1000,activation=&quot;relu&quot;)(x) #1000 个神经元
x=layers.Dropout(0.3)(x) #30% 的神经元‘去活性’

输出=layers.Dense(1,activation=“sigmoid”)(x) #la capa de salida：1个神经元
modelo=keras.Model(VGG16.inputs,output) #Creo el modelo

#Elijo el optimizador
从tensorflow.keras.optimizers导入Adam
optimizador=Adam(learning_rate=0.001) #Esto cambiardependiendo del 训练表现 --&gt; AJUSTAR (0.001 初始)

#编译步骤
模型.编译(
    优化器=优化器，#Optimizador
    损失=“binary_crossentropy”，#clasificación binaria
    指标=[“准确度”]
）

#模特表演
历史=模型.fit(
    火车发电机，
    纪元=20，
    回调=[ES],
    验证数据=val_generator，
    class_weight=pesos_clases #指定比索对应的处罚类别，主要是针对少数派的惩罚模型
）

但我得到了这些结果：
结果
我想知道为什么会发生这种情况。
我改变了超参数，例如学习率、神经元数量，并且对少数类应用了 class_weight，但没有得到任何显著的改善]]></description>
      <guid>https://stackoverflow.com/questions/79308661/why-am-i-getting-the-same-accuracy-on-validation-data</guid>
      <pubDate>Thu, 26 Dec 2024 04:26:53 GMT</pubDate>
    </item>
    <item>
      <title>在 MNIST 数据集上使用 Gumbel softmax 进行 VAE</title>
      <link>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</link>
      <description><![CDATA[kl 损失变为 0 可能是什么问题？重建损失很小，但每幅图像都相同，并且不代表任何数字。
这是我使用的编码器/解码器架构，我认为该架构足够复杂，可以有效地捕获和创建新的数字。
import tensorflow as tf
from keras import layer
import gumbel_softmax

class VariationalAutoEncoder(tf.keras.Model):
def __init__(self, latent_dim, categorical_dim):
super(VariationalAutoEncoder, self).__init__()
self.latent_dim = latent_dim
self.categorical_dim = categorical_dim
self.z_dim = latent_dim * categorical_dim

self.encoder = tf.keras.Sequential([
layer.InputLayer(input_shape=(28, 28, 1)),
层。Conv2D（16，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。Conv2D（32，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。Conv2D（64，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。BatchNormalization（），
层。Flatten（），
层。Dense（128，激活=&#39;relu&#39;），
层。Dense（self.z_dim），
层。Reshape（（latent_dim，categorical_dim）），
]）

self.decoder = tf.keras.Sequential（[
层。InputLayer（input_shape=（latent_dim，categorical_dim）），
层。Flatten（），
层。Dense（7 * 7 * 64，激活=&#39;relu&#39;），
layers.Reshape（（7，7，64）），
layers.Conv2DTranspose（64，（3，3），激活=&#39;relu&#39;，strides= 2，padding=&#39;same&#39;），
layers.Conv2DTranspose（32，（3，3），激活=&#39;relu&#39;，strides = 2，padding=&#39;same&#39;），
layers.Conv2DTranspose（1，（3，3），激活=&#39;sigmoid&#39;，padding=&#39;same&#39;），
]）

def call（self，x，temperature，hard）：
logits = self.encoder（x）
z = gumbel_softmax.gumbel_softmax（logits，temperature，hard=hard）
reconstructed = self.decoder（z）
return tf.reshape（reconstructed，（reconstructed.shape[0]，28，28））， tf.nn.softmax(logits, axis=-1)


import tensorflow as tf

def sample_gumbel(shape, eps=1e-15): 
&quot;&quot;&quot;从 Gumbel(0, 1) 分布中采样。&quot;&quot;&quot;
U = tf.random.uniform(shape, minval=0, maxval=1)
return -tf.math.log(-tf.math.log(U + eps) + eps)

def gumbel_softmax_sample(logits,temperature): 
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样。&quot;&quot;&quot;
y = logits + sample_gumbel(tf.shape(logits))
return tf.nn.softmax(y /temperature)

def gumbel_softmax(logits,temperature,hard=False):
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样并可选择离散化。

参数：
logits：[batch_size, n_class] 未归一化的对数概率
temperature：非负标量
hard：如果为 True，则取 argmax，但对软样本 y 进行区分

返回：
[batch_size, n_class] 来自 Gumbel-Softmax 分布的样本。
如果 hard=True，则返回的样本将是独热样本，否则它将是跨类别总和为 1 的概率分布。
&quot;&quot;&quot;
y = gumbel_softmax_sample(logits,temperature) 
if hard: 
y_hard = tf.one_hot(tf.argmax(y, axis=-1), tf.shape(logits)[-1])
y = tf.stop_gradient(y_hard - y) + y 

return y


以下是我使用的参数，我尝试了很多次，但没有成功。
LR_RATE = 5e-3

BATCH_SIZE=64
NUM_ITERS=900
tau0 = 1
ANNEAL_RATE=5e-5
MIN_TEMP=0.1
EPOCHS = 30

CATEGORICAL_DIM = 10
LATENT_DIM = 32

我计算了 kl 损失和重建损失的总和batch_size
def compute_loss(model, x,temperature,hard,beta):
重建，logits = model(x,temperature =temperature,hard =hard)

mse = keras.losses.BinaryCrossentropy(reduction=&quot;sum_over_batch_size&quot;)

rebuilding_loss = mse(x,reconstructed)
kl_loss = tf.reduce_mean(
tf.reduce_sum(
logits * (tf.math.log(logits + 1e-8) - tf.math.log(1.0 / config.CATEGORICAL_DIM)),
axis=[1, 2]
)
)
total_loss = rebuilding_loss + beta * kl_loss
return total_loss, rebuilding_loss, beta * kl_loss, reconstructed
]]></description>
      <guid>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</guid>
      <pubDate>Wed, 25 Dec 2024 16:28:11 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些“领域特定”功能来预测糖尿病？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</link>
      <description><![CDATA[我需要在 MLOps 周期中为经典糖尿病 ML 数据集添加“领域特定”特征和特征工程的 Python 代码实现？
我正在向高中生教授机器学习。因此，我们使用经典 ML 糖尿病数据集。我需要使用 pandas、numpy 和 scikit-learn 使其保持简单。
我目前所做的工作：
我的特征工程实践想法是对性别进行分类、根据出生日期计算年龄（我已将日期添加到原始数据集）并根据 年龄 * BMI 计算风险百分比，但这些想法更侧重于从现有特征和特征交互中得出新变量。我真的需要一个想法/代码来实际演示“创建特定于域的功能”：
我拥有的代码需要基于特定于域的示例进行构建：
import pandas as pd
#数据以 CSV 格式导入，学生对此进行了一些基本的处理
data_frame = pd.read_csv(&quot;2.2.1.wrangled_data.csv&quot;)
data_frame[&#39;SEX&#39;] = data_frame[&#39;SEX&#39;].apply(lambda gender: -1 if gender.lower() == &#39;male&#39; else 1 if gender.lower() == &#39;female&#39; else None)
data_frame[&#39;Age&#39;] = ((data_frame[&#39;DoTest&#39;] - data_frame[&#39;DoB&#39;]).dt.days / 365.25).round().astype(int)
data_frame[&#39;Risk&#39;] = data_frame[&#39;BMI&#39;] * data_frame[&#39;Age&#39;]
data_frame[&#39;RiskPercentage&#39;] = ((data_frame[&#39;Risk&#39;] / data_frame[&#39;Risk&#39;].max()) * 100).round(2)

有任何关于特定领域示例的帮助想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</guid>
      <pubDate>Wed, 25 Dec 2024 10:36:46 GMT</pubDate>
    </item>
    <item>
      <title>分离图像内的盲文字符</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[我正在做一个将盲文转换为文本的项目。我已经编写了从图像中识别盲文点的代码，但我不知道如何将盲文分割成单元格。
这部分是识别图像中的斑点（较小的低质量图像目前不起作用）
import cv2
import numpy as np
from sklearn.cluster import KMeans

# 加载图像
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 设置 SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# 按区域过滤（斑点大小）
params.filterByArea = True
params.minArea = 100 # 根据点大小进行调整
params.maxArea = 1000

# 按圆度过滤
params.filterByCircularity = True
params.minCircularity = 0.9 # 调整点的形状

# 按凸度过滤
params.filterByConvexity = False
params.minConvexity = 0.7

# 按惯性过滤（圆度）
params.filterByInertia = True
params.minInertiaRatio = 0.95

# 使用参数创建检测器
detector = cv2.SimpleBlobDetector_create(params)

# 检测斑点
keypoints = detector.detect(image)

# 将检测到的斑点绘制为红色圆圈
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;输出图像&quot;)
cv2.imshow(&quot;输出图像&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;检测到的斑点数量：{len(keypoints)}&quot;)

以下代码将 blob 的坐标放在图形上（认为这种方式可能更容易操作）
#将图像转换为图形

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob 的坐标
rounded_coords = np.round(blob_coords).astype(int) #四舍五入的坐标

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# 基于邻近度的分组
# 如果 X 距离小于最小距离
# 如果 Y 距离小于最小距离
# 存储 X 和 Y 坐标

# 计算最小 x 和 y差异（尝试基于接近度）
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # 单元格宽度阈值
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # 单元格高度阈值
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# 绘图
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

尝试通过接近度将它们分开（位于我尝试将距离很近的物体分组到一起（我将距离很近的物体分组到一起），但我无法理解其中的逻辑。我也尝试了组聚类 (Kmeans)，但它不是很准确，并且不适用于具有不同字符数的图像，因为它需要不断知道要形成多少个簇。
# 尝试 kmeans 聚类方法
# kmeans 不起作用（无法从图像中找出簇的数量）
# 如果可以找出 nclusters，则可以工作

导入数学
从 sklearn.cluster 导入 KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # 提取 blob 的 (x, y) 位置
rounded_coords = np.round(blob_coords).astype(int) # 为简单起见，对坐标进行四舍五入

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点

ax.invert_yaxis() # 反转 Y 轴以获得类似图像的坐标
plt.title(&quot;盲文单元检测&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络进行分类</title>
      <link>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</link>
      <description><![CDATA[我正在使用 GNN 开展欺诈检测项目。我的图表以银行代码（SWIFT BIC 代码）作为节点，边表示交易。
以下是我的张量的形状：

节点特征张量形状：torch.Size([210, 6])
边缘特征张量形状：torch.Size([200, 4])
邻接矩阵张量形状：torch.Size([210, 210])
标签张量形状：torch.Size([200, 1])

我尝试了很多次，但目前正在遵循本教程：https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html
以下是我的 GNN 代码：
class GCNLayer(nn.Module):

def __init__(self, c_in, c_out):
super().__init__()
self.projection = nn.Linear(c_in, c_out)

def forward(self, node_feats, adj_matrix):
# Num neighbours = 传入边的数量
num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
node_feats = self.projection(node_feats)
print(&quot;node_feats &quot;,node_feats)
node_feats = torch.bmm(adj_matrix, node_feats)
node_feats = node_feats / num_neighbours
返回node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

使用 torch.no_grad():
out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;邻接矩阵&quot;, adjacency_matrix_tensor)
print(&quot;输入特征&quot;, node_features_tensor)
print(&quot;输出特征&quot;, out_feats)

但无论我怎么尝试，乘法过程中总是会出现维度错误：“

RuntimeError：mat1 和 mat2 形状无法相乘（210x6 和 2x2）。

我知道我们正在尝试将 node_Features_tensor (210,6) 与 adjacency_matrix_tensor (210,210) 相乘，但我已经为此困扰了好几天！
我尝试了 GNN/GCN 的多种实现。我希望能够训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</guid>
      <pubDate>Tue, 12 Mar 2024 09:08:05 GMT</pubDate>
    </item>
    <item>
      <title>特征名称应与 fit 期间传递的特征名称相匹配</title>
      <link>https://stackoverflow.com/questions/77748547/the-feature-names-should-match-those-that-were-passed-during-fit</link>
      <description><![CDATA[我尝试在使用 sklearn 线性回归创建模型后计算 r 平方值。
我只是

导入 csv 数据集
过滤有趣的列
在训练和测试中拆分数据集
创建模型
对测试进行预测
计算 r 平方以查看模型与测试数据集的拟合程度

数据集取自https://www.kaggle.com/datasets/jeremylarcher/american-house-prices-and-demographics-of-top-cities
代码如下
&#39;&#39;&#39; 让我们验证价格和浴室床位数量之间是否存在相关性&#39;&#39;&#39;

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df = pd.read_csv(&#39;data/American_Housing_Data_20231209.csv&#39;)

df_interesting_columns = df[[&#39;Beds&#39;, &#39;Baths&#39;, &#39;Price&#39;]]

independent_variables = df_interesting_columns[[&#39;Beds&#39;, &#39;Baths&#39;]]
dependent_variable = df_interesting_columns[[&#39;Price&#39;]]

X_train, X_test, y_train, y_test = train_test_split(independent_variables,dependent_variable, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

prediction = model.predict(X_test)

print(model.score(y_test, prediction))

但我得到了错误
ValueError：特征名称应与拟合期间传递的特征名称相匹配。
拟合时未看到的特征名称：

价格
拟合时看到的特征名称，但现在缺失：
浴室
床

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/77748547/the-feature-names-should-match-those-that-were-passed-during-fit</guid>
      <pubDate>Tue, 02 Jan 2024 21:01:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>选择 CNN 中的步幅和过滤器数量（Keras）</title>
      <link>https://stackoverflow.com/questions/46065445/selecting-number-of-strides-and-filters-in-cnn-keras</link>
      <description><![CDATA[我正在使用 keras 构建一个用于信号分类的 cnn 模型。在 keras 中，调整超参数和选择步长数以及滤波器数量的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/46065445/selecting-number-of-strides-and-filters-in-cnn-keras</guid>
      <pubDate>Wed, 06 Sep 2017 01:26:02 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 交叉验证过度拟合或欠拟合</title>
      <link>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</link>
      <description><![CDATA[我正在使用 scikit-learn cross_validation 并获得例如 0.82 平均分数 (r2_scorer)。
我如何知道使用 scikit-learn 函数时是否存在过度拟合或欠拟合？]]></description>
      <guid>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</guid>
      <pubDate>Tue, 03 Dec 2013 17:25:03 GMT</pubDate>
    </item>
    </channel>
</rss>