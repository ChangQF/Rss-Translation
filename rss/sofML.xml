<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 23 Jan 2025 03:18:44 GMT</lastBuildDate>
    <item>
      <title>如何在固定的BBOX中将YOLOv8model与Deepsort连接起来？</title>
      <link>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</link>
      <description><![CDATA[我正在生成一个可以检测摩托车和汽车的模型来提取各自的信息。
但在将 YOLOv8 模型（这是我自定义的模型）与 Deepsort 算法连接的过程中，我发现了几个问题。

起初，自定义模型（YOLOv8）可以检测到每辆车，提取的视频显示完美的边界框
与 Deepsort 连接后，它漏掉了几辆车，提取的视频有错误的边界框（它们太大，不适合每辆车）
我在 YOLOv8 2 Deepsort 之间找不到错误的结果。

请帮帮我
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort

# 初始化 YOLO 模型
model_path = &quot;/content/drive/MyDrive/Capstone/best_motorcycle_detector_NIGHT8.pt&quot;
model = YOLO(model_path)
model.to(&#39;cuda&#39;) # 使用 GPU

# 初始化 DeepSORT
tracker = DeepSort(max_age=200, n_init=1, nn_budget=200)

# 帮助程序将 YOLO 结果转换为 DeepSORT 格式
def yolo_to_deepsort(yolo_results, target_classes):
detections = []
for det in yolo_results[0].boxes:
x1, y1, x2, y2 = map(float, det.xyxy[0].cpu().numpy())
confidence = float(det.conf.cpu().numpy().item())
class_id = int(det.cls.cpu().numpy())
if class_id in target_classes:
detections.append([(x1, y1, x2, y2),置信度])
返回检测

# 主处理循环
video_path = &quot;/content/drive/MyDrive/Capstone/11.15 1200-1400/1320-1400.mp4&quot;
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = &quot;/content/drive/MyDrive/Capstone/Results/processed_video.avi&quot;
video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), fps, (frame_width, frame_height))

target_classes = [2, 3] # 汽车 (2)、摩托车 (3)

while cap.isOpened():
ret, frame = cap.read()
if not ret:
break

# 运行 YOLO 模型
results = model(frame, conf=0.3)

# 将 YOLO 结果转换为 DeepSORT 格式
detections = yolo_to_deepsort(results, target_classes)

# 更新跟踪器
tracks = tracker.update_tracks(detections, frame=frame)

# 绘制边界框
for track in tracks:
if not track.is_confirmed():
continue
x1, y1, x2, y2 = map(int, track.to_tlbr())
track_id = track.track_id
label = f&quot;ID {track_id}&quot;
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 保存帧
video_writer.write(frame)

cap.release()
video_writer.release()


找出 YOLO 中的协调性
Deepsort 的输入和输出
与其他算法结合，但 Deepsort 更适合我的视频
]]></description>
      <guid>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</guid>
      <pubDate>Wed, 22 Jan 2025 14:30:52 GMT</pubDate>
    </item>
    <item>
      <title>对于 pytorch 模型 (Yolov3) 来说，“前向/后向通道大小”太大</title>
      <link>https://stackoverflow.com/questions/79378022/the-forward-backward-passage-size-is-too-large-for-the-pytorch-model-yolov3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79378022/the-forward-backward-passage-size-is-too-large-for-the-pytorch-model-yolov3</guid>
      <pubDate>Wed, 22 Jan 2025 14:07:39 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 GNN-LSTM 预测尼日利亚某州未来确诊的脑膜炎病例数</title>
      <link>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</guid>
      <pubDate>Wed, 22 Jan 2025 11:15:34 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB 神经网络预测收敛到 1</title>
      <link>https://stackoverflow.com/questions/79376474/matlab-neural-network-predictions-converging-to-1</link>
      <description><![CDATA[我正在尝试训练神经网络但遇到了障碍。我有一个简化版的我面临的问题：
设置：
NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options);

X_data 大小为 25455x3，其中 3 列中的每一列都经过了归一化（平均值 = 0 和标准差 = 1）
Y_data 大小为 25455x1，值为 1 或 0（二元分类）（sum(Y_data)= 11541(~45%))
NN 是从以下代码中新生成的 dlnetwork：

 featureInputLayer(3, &quot;Name&quot;, &quot;InputLayer&quot;)
fullyConnectedLayer(32, &quot;Name&quot;, &quot;HiddenLayer1&quot;, &quot;WeightsInitializer&quot;, &quot;he&quot;) 
reluLayer(&quot;Name&quot;, &quot;ReLU&quot;)
dropoutLayer(0.2, &quot;Name&quot;, &quot;Dropout&quot;) 
fullyConnectedLayer(1, &quot;Name&quot;, &quot;OutputLayer&quot;, &quot;WeightsInitializer&quot;, &quot;glorot&quot;)
sigmoidLayer(&quot;Name&quot;, &quot;SigmoidOutput&quot;) 
];
NN = dlnetwork(NN);


选项：

选项 = trainingOptions(&quot;adam&quot;, ...
LearnRateSchedule = &quot;piecewise&quot;, ...
LearnRateDropFactor = 0.2, ...
LearnRateDropPeriod = 5, ...
MaxEpochs = 1, ... 
MiniBatchSize = 128, ...
ExecutionEnvironment = &quot;cpu&quot;, ...
Plots = &quot;none&quot;);

问题：
当我运行单行 NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options); 时，模型运行 194 次迭代，最终 trainingloss 为 0.044545，但模型在类似测试数据上的测试准确率仅为 ~45%，但更令人担忧的是，使用类似数据生成的预测非常偏向 1。事实上，最低预测是 0.6876，平均值是 0.9030。这是一个巨大的飞跃，对我来说毫无意义。我希望模型的预测保持大约 0.5 的正常值（暂时不考虑任何学习）或者可能稍微少一点以匹配略低的 1 个标签数量。为什么会发生这种情况，我该如何解决？
注意：我真正做的是运行一个循环，该循环运行我上面提到的代码行。我描述的问题只是循环的第一次迭代；随着循环的继续，偏差变得更加极端，直到所有预测都只有 1.00000。每次循环中的数据都不同，但它非常相似，并且其中存在应该可以学习的趋势（从其他类型的回归建模中发现）。]]></description>
      <guid>https://stackoverflow.com/questions/79376474/matlab-neural-network-predictions-converging-to-1</guid>
      <pubDate>Wed, 22 Jan 2025 03:36:04 GMT</pubDate>
    </item>
    <item>
      <title>解释 TensorFlow 决策森林中的变量重要性方法</title>
      <link>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests</guid>
      <pubDate>Wed, 22 Jan 2025 03:01:46 GMT</pubDate>
    </item>
    <item>
      <title>机器学习检查过度拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/79376198/machine-learning-checking-for-overfitting</link>
      <description><![CDATA[我训练了一个机器学习模型，但不确定它是否过度拟合。使用训练集进行预测时的准确率、精确率、召回率和 f1 分数均为 1.0，而对于测试集，所有分数均为 ~0.9。我知道当它不能很好地概括测试集时就会发生过度拟合，但我的测试集结果相当高。我感到困惑的是，训练集是完美的。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79376198/machine-learning-checking-for-overfitting</guid>
      <pubDate>Tue, 21 Jan 2025 23:37:48 GMT</pubDate>
    </item>
    <item>
      <title>训练 Hugging Face Transformer 期间 GPU 利用率几乎始终为 0</title>
      <link>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</link>
      <description><![CDATA[我正在使用我的发票数据对 Donut Cord-v2 模型进行微调，该发票数据在预处理并作为数据集保存在磁盘上时大小约为 360 GB。我几乎完全按照这个笔记本进行操作，只是我有 6 个训练周期而不是 3 个。
我在单个 Nvidia H100 SXM GPU / Intel Xeon® Gold 6448Y / 128 GB RAM 上进行训练。
每当我开始训练并使用 htop 和 nvidia-smi 检查 CPU 和 GPU 利用率时，我都会看到 CPU 的利用率为 10-12%，由 python 使用，GPU 内存几乎一直被占用 90%，但 GPU 利用率几乎始终为 0。如果我不断刷新 nvidia-smi 的输出，则每 10-12 秒一次，利用率将跳转到100% 然后立即回到 0。我不禁感觉到我的 CPU 和 GPU 之间存在瓶颈，CPU 尝试不断处理数据并将其发送到 GPU，GPU 处理速度非常快，并且只是闲置，等待来自 CPU 的下一批。我从磁盘加载已经预处理的数据集，如下所示：
from datasets import load_from_disk
processed_dataset = load_from_disk(r&quot;/dataset/dataset_final&quot;)

我的处理器配置如下：
from transformers import DonutProcessor

new_special_tokens = [] # 将添加到 tokenizer 的新 token
task_start_token = &quot;&lt;s&gt;&quot; # 任务 token 的启动
eos_token = &quot;&lt;/s&gt;&quot; # tokenizer 的 eos token

processor = DonutProcessor.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 向 tokenizer 添加新的特殊 token
processor.tokenizer.add_special_tokens({&quot;additional_special_tokens&quot;: new_special_tokens + [task_start_token] + [eos_token]})

# 我们更新了一些与预训练不同的设置；即图像的大小 + 无需旋转
processor.feature_extractor.size = [1200,1553] # 应为 (宽度, 高度)
processor.feature_extractor.do_align_long_axis = False

我的模型配置是：
import torch
from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig

#print(torch.cuda.is_available())

# 从 huggingface.co 加载模型
model = VisionEncoderDecoderModel.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 调整嵌入层的大小以匹配词汇表大小
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f&quot;新嵌入大小： {new_emb}&quot;)
# 调整我们的图像大小和输出序列长度
model.config.encoder.image_size = process.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[&quot;train&quot;][&quot;labels&quot;], key=len))

# 添加解码器启动的任务令牌
model.config.pad_token_id = process.tokenizer.pad_token_id
model.config.decoder_start_token_id = process.tokenizer.convert_tokens_to_ids([&#39;&lt;s&gt;&#39;])[0]

我的训练代码是：
import gc
gc.collect()

torch.cuda.empty_cache()

from transformers import Seq2SeqTrainingArguments，Seq2SeqTrainer

导入日志记录
logging.basicConfig(level=logging.INFO)

# 训练参数
training_args = Seq2SeqTrainingArguments(
output_dir=r&quot;/trained&quot;, # 指定本地目录保存模型
num_train_epochs=6,
learning_rate=2e-5,
per_device_train_batch_size=8,
weight_decay=0.01,
fp16=True,
logs_steps=50,
save_total_limit=2,
evaluation_strategy=&quot;no&quot;,
save_strategy=&quot;epoch&quot;,
predict_with_generate=True,
report_to=&quot;none&quot;,
# 禁用推送到集线器
push_to_hub=False

)

# 创建训练器
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=processed_dataset[&quot;train&quot;],
)

# 开始训练
trainer.train()

使用 360 GB 数据集完成 6 个 epoch 的训练预计需要 54 小时。当我在装有 Intel i9 11900KF / RTX 3050 的 PC 上运行完全相同的代码时，我发现 GPU 利用率一直保持在 100%。我的代码中是否存在瓶颈？为什么 CPU 会继续处理已经预处理的数据集？ Cuda 12.6
编辑：
由于我的 RAM 和 CPU 核心数量允许，将 Seq2SeqTrainer 的 dataloader_num_workers 参数更改为 &gt;0 值是否有意义？（并且 CPU 利用率最高为 10-12%）]]></description>
      <guid>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</guid>
      <pubDate>Tue, 21 Jan 2025 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>微调后的IP-Adapter模型未能取得有效效果</title>
      <link>https://stackoverflow.com/questions/79373102/the-finetuned-ip-adapter-model-fails-to-achieve-effective-results</link>
      <description><![CDATA[微调环境基于开源及其说明：
https://github.com/tencent-ailab/IP-Adapter
我使用图像+提示对作为训练数据，训练了一个用于微调的IP-Adapter模型。但是，经过微调的模型并没有反映出根据提示得出的预期结果。
训练图像和 .json 提示文件的 2 个失败案例：

白天图像的相同提示（17 张图像）：
例如，&quot;image_file&quot;: &quot;day_1.jpg&quot;, &quot;text&quot;: &quot;白天有阳光的城市。&quot;
夜间图像的相同提示（17 张图像）：
例如，&quot;image_file&quot;: &quot;night_1.jpg&quot;, &quot;text&quot;: &quot;夜晚有光的城市。没有阳光。&quot;
推理结果：训练后，输入白天图像，提示如 &quot;白天有阳光的城市。&quot;为了进行推理，生成的输出图像仍然是白天图像，而不是夜间图像。

针对数千张图像（1500 张中性面部图像和 2500 张悲伤面部图像）进行训练：
4 种不同的提示，用于表示中性面部表情的中性图像。
例如，&quot;image_file&quot;: &quot;neu_1.jpg&quot;, &quot;text&quot;: &quot;此人的面部表情为中性。&quot;
4 种不同的提示，用于表示悲伤面部表情的悲伤图像。
例如，&quot;image_file&quot;: &quot;sad_1.jpg&quot;, &quot;text&quot;: &quot;这个人正露出悲伤的表情。&quot;


推理结果：训练后，应用带有提示的中性图像，例如 &quot;一个人很伤心。&quot;，或 &quot;这个人正露出悲伤的表情。&quot; ，生成的输出人脸图像仍然很中性，一点也不悲伤。
微调步骤：

修改tutorial_train_plus.py中的代码：
将accelerator.save_state(save_path)
替换为accelerator.save_state(save_path, safe_serialization=False)

运行以下脚本进行微调：
accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; 
tutorial_train_plus.py 
--pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; 
--pretrained_ip_adapter_path=“models/ip-adapter-plus_sd15.bin”
--image_encoder_path=“models/image_encoder/”
--data_json_file=“assets/prompt_image.json”
--data_root_path=“assets/images/train/”
--mixed_precision=“fp16”
--resolution=512
--train_batch_size=8
--dataloader_num_workers=4
--learning_rate=1e-04
--weight_decay=0.01
--output_dir=“out_model” 
--num_train_epochs=300
--save_steps=200

参考readme中的说明，将pytorch.bin转换为ip-adapter.bin。
在推理文件
ip_adapter-plus_demo.py中，
修改原始模型：ip_ckpt = &quot;models/ip-adapter-plus_sd15.bin&quot;


对训练好的模型ip_ckpt = &quot;models/ip-adapter.bin&quot;

运行python3 ip_adapter-plus_demo.py进行推理

上面的过程有什么问题吗，或者问题出在输入图像或提示上？
在我的例子中，通常至少需要多少个数据集对（一个图像和一个提示）才能获得有效的结果？]]></description>
      <guid>https://stackoverflow.com/questions/79373102/the-finetuned-ip-adapter-model-fails-to-achieve-effective-results</guid>
      <pubDate>Tue, 21 Jan 2025 03:08:09 GMT</pubDate>
    </item>
    <item>
      <title>React Native CLI 项目中的 YOLO 对象检测模型</title>
      <link>https://stackoverflow.com/questions/79372324/yolo-object-detection-model-in-react-native-cli-project</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79372324/yolo-object-detection-model-in-react-native-cli-project</guid>
      <pubDate>Mon, 20 Jan 2025 18:24:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>我正在进行 Yolov8 模型训练，但准确率只有 70%</title>
      <link>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</link>
      <description><![CDATA[我目前正在为我的项目训练 YOLOv8 模型。目标是训练该模型从发送给聊天机器人的照片中识别产品的股票代码。
我有 1,522 个股票代码，每个股票代码大约有 10-15 张照片。虽然照片数量有点少，但我们的客户通常会向我们发送训练中使用的相同照片。例如，他们可能会从我们的 Instagram 个人资料中截取屏幕截图并将其发送给我们。
我正在训练模型，但它的准确率只有 70% 左右。我相信问题可能与超参数有关，但我对它们不太熟悉。我非常感谢您提供的任何建议或建议。
hyp.yaml
# 学习率和动量 Ayarları
lr0: 0.01 # 关闭状态
lrf: 0.01 # Final öğrenme oranı (lr0 ile çarpılır)
动量：0.9#SGD动量
Weight_decay: 0.0005 # L2 正则化（权重衰减）
Warmup_epochs: 2.0 # Isınma epoch sayısı
Warmup_momentum: 0.8 # Isınma süresince başlangıç Momentumu
Warmup_bias_lr: 0.1 # 是否存在偏差
#Kayip Fonksiyonu （损失函数）Ayarları
box: 0.05 # Box kaybı kazancı (GIoU/DIoU/CIoU)
cls: 0.5 # Sınıf kaybı kzancı
iou: 0.2 # IoU eşiği (标签 için)
kobj: 1.0 # 不可以
# 增强 Ayarları (Veri artırma)
hsv_h: 0.005 # Görüntü HSV-Hue artırma（分数）- Çok küçük değişiklikler
hsv_s: 0.1 # Görüntü HSV-Saturation artırma（分数） - Çok küçük değişiklikler
hsv_v: 0.1 # Görüntü HSV-Value artırma（分数） - Çok küçük değişiklikler
度数：0.0 # Görüntü döndürme (+/- derece)
翻译：0.1 # Görüntü kaydırma (+/- 分数)a
比例：0.5 # Görüntü ölçekleme (+/- kazanç)
剪切力：0.0 # Görüntü kaydırma (+/- derece)
透视图：0.0 # Görüntü perspektifi（+/- 分数），0-0.001 arası
Flipud: 0.0 # Görüntüyü yukarıdan aşağıya çevirme (olasılık)
Fliplr: 0.5 # Görüntüyü sağdan sola çevirme （奥拉斯利克）
马赛克: 0.0 # 马赛克艺术 (olasılık) - Bu durumda kapalı
mixup: 0.0 # Mixup artırma (olasılık) - Bu durumda kapalı
copy_paste: 0.0 # 复制粘贴艺术 (olasılık) - Bu durumda kapalı

火车.py =
从 ultralytics 导入 YOLO
导入万数据库
从 wandb.integration.ultralytics 导入 add_wandb_callback
# WandB oturumunu başlatın
如果 __name__ == “__main__”：
    wandb.login()
    wandb.init(project=&quot;ultralytics&quot;, job_type=&quot;training&quot;)
    # 模型尤克莱因
    模型 = YOLO(&#39;yolov8n.pt&#39;)
    # WandB 回调&#39;ini ekleyin
    add_wandb_callback（模型，enable_model_checkpointing = True）
    # 模型 eğitin
    模型.火车(
        data=&#39;y.yaml&#39;, # Veri kümesi yapılandırma dosyası
        epochs=100, # Eğitim epoch sayısı
        batch=16, # 批量博语图
        project=&#39;my_project&#39;, # Proje adı (varsayılan: 运行/训练)
        name=&#39;exp&#39;, # 名称 (varsayılan: exp)
        cfg=&#39;hyp.yaml&#39; # 超参数 ayarları
    ）
    #WandB oturumunu sonlandırın
    wandb.finish()
]]></description>
      <guid>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</guid>
      <pubDate>Tue, 27 Aug 2024 13:16:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 OvO 和 OvR 返回相同的结果？</title>
      <link>https://stackoverflow.com/questions/70981876/why-does-ovo-and-ovr-return-the-same-result</link>
      <description><![CDATA[我正在使用 scikit-learn 的 roc_auc_score() 函数来解决多类分类问题。对于具有三个标签（和另一个数据集）的鸢尾花，当我使用一对一和一对其余时，我得到完全相同的输出。有人知道为什么会这样吗？这是我的代码：
从 sklearn 导入数据集
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
从 sklearn.tree 导入 DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)
clf = DecisionTreeClassifier(random_state=0)
clf = clf.fit(X_train, y_train)
y_pred = clf.predict_proba(X_test)
auc_ovr = roc_auc_score(y_test, y_pred, average=&#39;macro&#39;, multi_class=&#39;ovr&#39;)
auc_ovo = roc_auc_score(y_test, y_pred, average=&#39;macro&#39;, multi_class=&#39;ovo&#39;)
print(f&#39;OVR: {auc_ovr}, OVO: {auc_ovo}&#39;)


最后一行的输出是：
OVR: 0.9833333333333334, OVO: 0.9833333333333334
]]></description>
      <guid>https://stackoverflow.com/questions/70981876/why-does-ovo-and-ovr-return-the-same-result</guid>
      <pubDate>Fri, 04 Feb 2022 05:35:38 GMT</pubDate>
    </item>
    <item>
      <title>如何暂时禁用 MLFlow？</title>
      <link>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</link>
      <description><![CDATA[是否可以暂时禁用 MLFlow 以调试代码或添加新功能？如果不禁用，它会保存大量实际上无用或未完成的执行。
或者最好的策略是使用不调用 mlflow.start_run() 的类似代码？]]></description>
      <guid>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</guid>
      <pubDate>Tue, 07 Apr 2020 20:14:15 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow keras 序列模型 - 如何仅预测最后一步的输出</title>
      <link>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</link>
      <description><![CDATA[我正在使用 tf.keras 库开发一个序列模型。
假设我有 5 个时间步骤，每个时间步骤本质上都有一个输出。
每个时间步骤中的特征数量为 - 比如 - 4。我正在研究分类问题，输出可以是 0、1 或 2 之一。
例如，以下是一个训练示例。
步骤 1 输入：`[0, 5, 4, 5]` 和输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 和输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 和输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 和输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 和输出 = `2`

在训练我的模型时，我希望以这样的方式训练它：
在步骤 1 中，如果您的输入是 [0, 5, 4, 5]，则此时间步的输出为 0。

在步骤 2 中，如果您的输入是 [1, 2, 2, 7]，则此时间步的输出为 1。
以此类推....
但在后期制作中，我只希望我的模型能够估计最后一个时间步的输出。例如：
步骤 1 输入：`[0, 5, 4, 5]` 且输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 且输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 且输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 且输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 且输出 = **`?`**

基于此，在训练我的模型时，我对应该如何构建和训练我的模型有点困惑？由于我只对最后一步的输出感兴趣，但仍希望在训练阶段通过提供最后一步之前的先前步骤的输出来帮助我的模型，我想知道我应该如何构建模型？
如果我提供所有时间步的输出作为预期输入，据我所知，损失/成本是基于此计算的。例如，如果第 3 个时间步的输出计算错误，成本将增加。这可能是预料之中的，但对我来说重要的是最后一步的输出，我主要感兴趣的是在最后一步做出正确的预测。在这种情况下，如何使用 tf.keras 构建我的模型？
（或者，如果我仍然需要以某种方式训练我的模型，以便它尝试分别估计每个时间步的输出。最后，我仍然希望仅基于最后一步的输出来计算准确度。）]]></description>
      <guid>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</guid>
      <pubDate>Tue, 18 Dec 2018 01:24:53 GMT</pubDate>
    </item>
    <item>
      <title>KD树最近邻搜索如何工作？</title>
      <link>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</link>
      <description><![CDATA[我正在查看 KD 树的 Wikipedia 页面。例如，我用 Python 实现了列出的构建 kd 树的算法。
但是，使用 KD 树进行 KNN 搜索的算法会切换语言，并且并不完全清楚。英语解释开始有意义，但其中的部分（例如他们“展开递归”以检查其他叶节点的区域）对我来说真的没有任何意义。
这是如何工作的，以及如何在 Python 中使用 KD 树进行 KNN 搜索？这并不是一个“给我发代码！”类型的问题，我也不指望这样。请简单解释一下 :)]]></description>
      <guid>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</guid>
      <pubDate>Sat, 11 Dec 2010 19:14:25 GMT</pubDate>
    </item>
    </channel>
</rss>