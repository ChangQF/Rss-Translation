<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Jun 2024 03:17:35 GMT</lastBuildDate>
    <item>
      <title>PySpark：具有 min_frequency 的 StringIndexer，类似 scikit-learn 的 OrdinalEncoder</title>
      <link>https://stackoverflow.com/questions/78613394/pyspark-stringindexer-with-min-frequency-like-in-scikit-learns-ordinalencoder</link>
      <description><![CDATA[我正在 PySpark 中构建一个机器学习管道，其中 StringIndexer 是其中一个阶段。问题是有些类别非常小，所以我希望将它们映射到同一个标签。使用 scikit-learn 中的 OrdinalEncoder 可以实现这一点。
我想我正在寻找一种方法来扩展 StringIndexer 类，但我无论如何也想不出如何做到这一点。我想我必须覆盖 fit（或 _fit）方法，但我甚至无法在源代码中找到它。欢迎提出其他建议。
这里有一个小例子：
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer

spark = SparkSession.builder.getOrCreate()

data = [
(&quot;a&quot;,),
(&quot;b&quot;,),
(&quot;a&quot;,),
(&quot;b&quot;,),
(&quot;c&quot;,),
(&quot;b&quot;,),
(&quot;b&quot;,),
(&quot;d&quot;,),
(&quot;d&quot;,),
(&quot;d&quot;,),
(&quot;e&quot;,)
]

data = spark.createDataFrame(data, [&quot;category&quot;])

indexer = StringIndexer(inputCol=&#39;category&#39;, outputCol=&#39;label&#39;)
categories_and_labels = indexer.fit(data).transform(data)


以上代码给出以下结果：
+--------+-----+
|category|label|
+--------+-----+
| a| 2.0|
| b| 0.0|
| a| 2.0|
| b| 0.0|
| c| 3.0|
| b| 0.0|
| b| 0.0|
| d| 1.0|
| d| 1.0|
| d| 1.0|
| e| 4.0|
+--------+-----+

我想要一个带有参数 minFrequency 的自定义类，我可以像下面这样使用：
indexer = CustomStringIndexer(inputCol=&#39;category&#39;, outputCol=&#39;label&#39;, minFrequency=3)
categories_and_labels = indexer.fit(data).transform(data)

预期结果将是
+--------+-----+
|category|label|
+--------+-----+
| a| 2.0|
| b| 0.0|
| a| 2.0|
| b| 0.0|
| c| 2.0|
| b| 0.0|
| b| 0.0|
| d| 1.0|
| d| 1.0|
| d| 1.0|
| e| 2.0|
+--------+-----+

我使用的是 Spark 版本 3.5]]></description>
      <guid>https://stackoverflow.com/questions/78613394/pyspark-stringindexer-with-min-frequency-like-in-scikit-learns-ordinalencoder</guid>
      <pubDate>Wed, 12 Jun 2024 14:43:04 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护 [关闭]</title>
      <link>https://stackoverflow.com/questions/78612921/predictive-maintenance</link>
      <description><![CDATA[我从事预测性维护工作，其中有每小时的传感器数据，其中包含振动、压力等列。要求是预测机器何时会出现故障。
以下是数据示例：



ID
压力
振动
故障状态
时间




1
75.53
450.65
0
2023-03-01 00:30:00


2
143.54
543.40
1
2023-03-01 01:30:00



有了这些数据，我们需要预测接下来 10 个时间步骤的故障状态。
我不确定是否要使用时间序列分类模型或常规分类模型来解决这个问题。您能建议最好的方法吗？
我尝试使用 DES-SVM 方法双指数平滑和 SVM 模型来处理数据，但预测非常差。之后我做了一些特征工程，并浏览了一些 kaggle 笔记本以进一步了解，但解决方案并不令人满意。因为我需要预测接下来的 n 步，并告诉用户该机器可能在接下来的 12 小时内出现故障。但这种模型只能在时间序列中使用，但我不知道如何在时间序列中使用基于预测的分类模型]]></description>
      <guid>https://stackoverflow.com/questions/78612921/predictive-maintenance</guid>
      <pubDate>Wed, 12 Jun 2024 13:17:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么模型训练准确率在提高，但验证准确率却上不去，而是卡住了？</title>
      <link>https://stackoverflow.com/questions/78612882/why-model-training-accuracy-is-increasing-but-validation-accuracy-is-not-increa</link>
      <description><![CDATA[我必须预测乌尔都语文本中的积极、中性和消极情绪。它有 30k 个样本
样本数据集
训练样本 = 24k，而验证样本 = 6k
我正在使用 bilstm 训练模型，但训练准确率正在提高，而验证却停滞不前。我尝试将批量大小从（2 到 256）更改为学习率从 0.1 更改为 1e-11，优化器也在变化，我使用了 Adam、SGD、RMSProp 和 Adadelta。我已经使用 word2vec 构建了可训练和不可训练的嵌入，甚至验证数据和 tran 数据也分成两半，更改层和更改单元，但没有任何改进
import keras.backend as K
def get_f1(y_true, y_pred): # 取自旧 keras 源代码
true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
precision = true_positives / (predicted_positives + K.epsilon())
recall = true_positives / (possible_positives + K.epsilon())
f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
return f1_val

def bilstm(embedding_layer):
#定义神经网络
model = Sequential()
model.add(embedding_layer)
model.add(Bidirectional(LSTM(units=128, return_sequences = True)))
model.add(Bidirectional(LSTM(units=64)))
model.add(Dense(3,activation=&#39;softmax&#39;))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[get_f1,&#39;accuracy&#39;])
return model

model = bilstm(embedding_layer)

learning_rate_reduction = ReduceLROnPlateau(monitor=&#39;val_accuracy&#39;,patience = 2,verbose=1,factor=0.5, min_lr=0.00001)
model.fit(train_seq, train_label, epochs=10, batch_size=8, validation_data=(val_seq, val_label))

Epoch 1
1511/1511 [==============================] - 601s 393ms/step - 损失：1.0787 - get_f1：0.0626 - 准确率：0.3957 - val_loss：1.0402 - val_get_f1：0.1804 - val_accuracy：0.4514
Epoch 2
1511/1511 [================================] - 615s 407ms/步 - 损失：0.7377 - get_f1：0.6348 - 准确度：0.6760 - val_loss：1.1938 - val_get_f1：0.3784 - val_accuracy：0.4509
Epoch 3
1511/1511 [=============================] - 608s 402ms/步 - 损失：0.3419 - get_f1：0.8503 - 准确度：0.8559 - val_loss：1.5797 - val_get_f1：0.4148 - val_accuracy：0.4448
Epoch 4
1511/1511 [===============================] - 612s 405ms/步 - 损失：0.2141 - get_f1：0.9074 - 准确度：0.9084 - val_loss：2.2244 - val_get_f1：0.4319 - val_accuracy：0.4459
Epoch 5
1511/1511 [===============================] - 609s 403ms/步 - 损失：0.1548 - get_f1：0.9357 - 准确度：0.9368 - val_loss：2.5604 - val_get_f1：0.4302 - val_accuracy：0.4391

]]></description>
      <guid>https://stackoverflow.com/questions/78612882/why-model-training-accuracy-is-increasing-but-validation-accuracy-is-not-increa</guid>
      <pubDate>Wed, 12 Jun 2024 13:09:39 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统 - 奇异值分解 (SVD) 提供随机结果</title>
      <link>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</link>
      <description><![CDATA[有时，输出的质量仅通过目测来评估。查看下面提供的示例，很明显预期的用户评分为 2 和 3（请参考空数据单元格）。
这些示例经过简化 - 我使用了更大的数据集，但仍然获得了相同的结果。
表 1

用户 电影 1 电影 2 电影 3

1 - 3 4

2 2 3 4

3 2 3 4

表 2

用户 电影 1 电影 2 电影 3

1 - 3 3

2 3 3 3

3 3 3 3

ALS 模型提供了这些评分，但 SVD 模型却惨遭失败。有人知道这是为什么吗？为什么我们会得到 2 和 3 以外的结果？]]></description>
      <guid>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</guid>
      <pubDate>Wed, 12 Jun 2024 10:21:28 GMT</pubDate>
    </item>
    <item>
      <title>在实践代码中，它说 keras 层无效</title>
      <link>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</guid>
      <pubDate>Wed, 12 Jun 2024 07:26:52 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型的输入</title>
      <link>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</link>
      <description><![CDATA[我训练了一个基于 bert 的模型，我已经研究了很长时间了。训练后，我在模型目录中得到了几个文件 - pytorch_model.bin、training_args.bin、merges.txt、vocab.json。现在我想通过向模型提供输入并检查其输出来测试模型。但我不知道该怎么做。
我试着在网上查找，有人建议我使用 Gradio。]]></description>
      <guid>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</guid>
      <pubDate>Wed, 12 Jun 2024 06:27:31 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 对象检测-数据集格式[关闭]</title>
      <link>https://stackoverflow.com/questions/78610630/tensorflow-object-detection-dataset-format</link>
      <description><![CDATA[我正在使用深度学习进行实时对象检测任务。哪种类型的 TensorFlow 数据集格式是强烈推荐的？我看到 TfRecord 格式和 tf.data.Dataset 格式使用最多。哪一个最好用？我的注释采用 PASCAL VOC（xml）格式。]]></description>
      <guid>https://stackoverflow.com/questions/78610630/tensorflow-object-detection-dataset-format</guid>
      <pubDate>Wed, 12 Jun 2024 04:26:15 GMT</pubDate>
    </item>
    <item>
      <title>我应该调整哪些超参数来提高准确性？</title>
      <link>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</link>
      <description><![CDATA[我想知道如何在多标签分类问题中提高准确率并降低损失。
如果你查看 sklearn 参考，你会发现在多类和多输出算法中提到了多标签，我现在正在测试它。
（https://scikit-learn.org/stable/modules/multiclass.html）
样本数据使用sklearn.datasets中的make_multilabel_classification有10个特征，通过修改n_classes创建一个数据集。
当multilabel有两个类时，似乎准确率和损失都比较令人满意。
from numpy import mean
from numpy import std
from sklearn.datasets import make_multilabel_classification
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, hamming_loss

# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=2, random_state=1)

# 总结数据集形状
print(X.shape, y.shape)
# 总结前几个示例
for i in range(10):
print(X[i], y[i])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
print(scaler.mean_)
print(scaler.var_)

x_train_std = scaler.transform(X_train)
x_test_std = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std, y_train)

pred = knn.predict(x_test_std)

print(accuracy_score(y_test, pred))
print(hamming_loss(y_test, pred))

accuracy_score: 0.8345, hamming_loss: 0.08875
但是，随着类别数超过3，准确率得分逐渐下降，损失增加。
# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=3, random_state=1)

n_classes= 3 --&gt; accuracy_score: 0.772, hamming_loss: 0.116
n_classes= 4 --&gt; accuracy_score: 0.4875, hamming_loss: 0.194125
使用 RandomForestClassifier 算法和 MLPClassifier 算法时（如参考中所示）或使用 ClassifierChain(estimator=SVC) 使用不支持多标签分类的算法时，情况也类似。
我应该调整哪些超参数来提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</guid>
      <pubDate>Wed, 12 Jun 2024 03:24:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 中的标记坐标叠加到 JPG 图像上以进行 CNN 训练？[关闭]</title>
      <link>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-onto-jpg-images-for-cnn-training</link>
      <description><![CDATA[我正在开展一个计算机视觉项目，该项目涉及检测和分割 MRI 扫描中的骨折。作为该项目的一部分，我让专家直接在图像上标记骨折区域。此过程会生成一个 JSON 文件，其中包含以下信息：

标记区域的坐标
标记区域的名称
标记图像的名称

我面临的挑战是将这些坐标从 JSON 文件转移到相应的 JPG 图像上，以准备进行 CNN 训练。
以下是我的 JSON 文件结构示例：
&quot;item&quot;: {
&quot;name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;team&quot;: {
&quot;name&quot;: &quot;Mask&quot;,
&quot;slug&quot;: &quot;mask&quot;
&quot;file_name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;annotations&quot;: [
{
&quot;bounding_box&quot;: {
&quot;h&quot;: 142.16649999999993,
&quot;w&quot;: 124.14549999999997,
&quot;x&quot;: 679.8006,
&quot;y&quot;: 425.7789
},
&quot;name&quot;: &quot;Broken&quot;,
&quot;polygon&quot;: {
&quot;paths&quot;: [
[
{
&quot;x&quot;: 695.1519,
&quot;y&quot;: 567.9454
},
{
&quot;x&quot;: 679.8006,
&quot;y&quot;: 530.5683
},


到目前为止，我已经设法从 JSON 文件中提取了必要的坐标。但是，我很难将这些坐标叠加到 JPG 图像上以生成 CNN 的训练数据。
我的问题：

如何准确地将 JSON 文件中的坐标叠加到相应的 JPG 图像上？
是否有任何推荐的 Python 库或方法专门适合此任务？
]]></description>
      <guid>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-onto-jpg-images-for-cnn-training</guid>
      <pubDate>Tue, 11 Jun 2024 09:28:24 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
替代方法：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
Sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
Sum(i) = (1/T)*sum((y_i&#39;-y_i_target).^2);
end
[minval, minidx] = min(Sum);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>神经网络预测变成直线</title>
      <link>https://stackoverflow.com/questions/72775077/neural-network-prediction-becomes-a-straight-line</link>
      <description><![CDATA[我实现了一个两层神经网络（根据 Kolmogorov-Arnold 定理，这足以表示 n 个变量的任何非线性函数）来预测时间序列。然而，在神经网络的末端，收到的预测的波动性下降到几乎为零，并变成一条直线（我附上了预测屏幕和神经网络的源代码）。我增加了隐藏层中的神经元数量、时期数、训练样本的大小、学习率，改变了训练样本数据的规范化范围，改变了初始权重的范围。但都无济于事。训练样本的大小为 336 个示例，训练方法是误差的反向传播，规范化方法是极小极大。此外，当使用双曲正切作为激活函数时，情况有所改善，但图形看起来也很奇怪。ReLU 输出“直接预测”。有人对这个问题有什么想法吗？
import random
import sys
import numpy
import math

eta=0.0001 #学习率
n=200 #训练周期数。还有 500、1000、5000
inp=30 #输入层大小
m=60 #隐藏层大小
y=0 #输出信号
t=0 #目标信号
e=0 #错误
d_y=0 #最后一个神经元的局部梯度
err=0 #计算输出神经元的网络误差
err_av=0 #平均网络误差
path=&#39;dataTrain.txt&#39; #训练样本
path2=&#39;dataLaunch.txt&#39; #启动预测
day = 365 #预测天数
...

其余内容在网站上：https://ideone.com/vV2QW6
屏幕截图（激活函数 - sigmoid）：https://ibb.co/GHrTGLr
屏幕截图（激活函数 - 双曲正切）：https://ibb.co/WHFX3Sc
感谢关注。]]></description>
      <guid>https://stackoverflow.com/questions/72775077/neural-network-prediction-becomes-a-straight-line</guid>
      <pubDate>Mon, 27 Jun 2022 15:59:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将生成的数据转换为 Pandas 数据框</title>
      <link>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</link>
      <description><![CDATA[from sklearn.datasets import make_classification
df = make_classification(n_samples=10000, n_features=9, n_classes=1, random_state = 18,
class_sep=2, n_informative=4)

创建数据后。它是元组，将元组转换为 pandas 数据框后
 df = pd.DataFrame(data, columns=[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;])

所以我得到了 9 个特征（列），但是当我尝试插入 9 个列时，它显示。

ValueError：传递值的形状为 (2, 1)，索引暗示 (2, 9)

基本上我想生成数据并将其转换为 pandas 数据框，但无法获取它。
错误是：]]></description>
      <guid>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</guid>
      <pubDate>Mon, 26 Apr 2021 11:54:24 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 模型输入形状</title>
      <link>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</link>
      <description><![CDATA[我加载了一个自定义 PyTorch 模型，我想找出它的输入形状。类似这样的内容：
model.input_shape

是否可以获取此信息？

更新： print() 和 summary() 不显示此模型的输入形状，因此它们不是我要找的。]]></description>
      <guid>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</guid>
      <pubDate>Fri, 05 Mar 2021 07:59:52 GMT</pubDate>
    </item>
    </channel>
</rss>