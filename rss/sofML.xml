<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Feb 2025 09:18:29 GMT</lastBuildDate>
    <item>
      <title>如何通过 Keras、Tensorflow 实现重现性？</title>
      <link>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</link>
      <description><![CDATA[每次运行以下代码时，我获得的准确率和损失都不一样。我按照之前帖子中的说明操作，但无法解决。问题可能出在哪里？
import os
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = &quot;1&quot;
os.environ[&quot;TF_CUDNN_DETERMINISTIC&quot;] = &quot;1&quot;
...
...

SEED=65
tf.keras.utils.set_random_seed(SEED) 
tf.config.experimental.enable_op_determinism()
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

训练，测试，训练目标，测试目标 = train_test_split((df.loc[:,&quot;input_alarm_1&quot;:&quot;input_alarm_&quot;+str(num_backtracking_events)]), df.loc[:,&quot;output_failure&quot;], test_size=0.25, random_state=SEED)

训练 = np.asarray(training).astype(&#39;float32&#39;)
训练目标 = np.asarray(trainingtarget).astype(&#39;float32&#39;)
test = np.asarray(test).astype(&#39;float32&#39;)
testtarget = np.asarray(testtarget).astype(&#39;float32&#39;)

initializer = tf.keras.initializers.GlorotUniform(seed=SEED)

model = keras.Sequential(
[
layer.Dense(600, 激活=&quot;relu&quot;, input_shape=(num_backtracking_events,), kernel_initializer=initializer),
layer.Dense(300, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(100, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(1, 激活=&quot;sigmoid&quot;, kernel_initializer=initializer),
#dropout?
]
)

#编译模型
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

#fit
history = model.fit(training, trainingtarget, batch_size=10, epochs=50, validation_split=0.1)

# 评估 keras 模型
test_loss, test_acc = model.evaluate(test, testtarget)
print(&#39;Accuracy: %.2f&#39; % (test_acc*100))
print(&#39;Loss: %.2f&#39; % (test_loss*100))
]]></description>
      <guid>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</guid>
      <pubDate>Wed, 05 Feb 2025 08:09:02 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>使用模型蒸馏来优化我的模型</title>
      <link>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</link>
      <description><![CDATA[我有一个 NN 模型，用于学习端到端通信系统。它是一个自动编码器，其中编码器充当发射器；它采用 8 位并将其编码为 IQ 值，解码器充当接收器；它采用生成的 IQ 值并将其解码为 8 位。我还有一个通道模型，可以模拟噪声、频率/相位偏移等。
该模型经过训练，具有非常好的误码率 (BER)，但在进行推理时具有高延迟，因此我需要对其进行优化。我正在尝试遵循 pytorch 的知识提炼教程，但到目前为止，我无法让我的学生有效地学习。
我认为我的问题在于我的软损失函数不正确。在原始训练循环中，我使用 BinaryCrossEntropy 损失来对抗模型的预测位概率与真实输入位。从文档中可以看出，K.D 似乎包含了一个额外的损失，即采用学生和父母概率的 KL 散度损失。但是，在运行代码时，我的损失并没有改善。
我感到困惑的是，我的“软损失”应该是什么类型的损失函数，以及它应该获得什么输入类型（logit 或概率）。我尝试了不同的排列（将对数概率输入到 KL Div 中，使用 CrossEntropy 损失而不是 KL，即文档中显示的损失函数），但它们都没有以任何方式提高我的学生模型的性能。
这大致就是我正在使用的代码。它不是完整的代码；我只展示了父自动编码器和 K.D 循环，但这足以表达我的观点。
任何帮助都值得感激。
import torch
import torch.nn as nn
import torch.optim as optim

# 定义编码器
class Encoder(nn.Module):
def __init__(self):
super(Encoder, self).__init__()
self.fc1 = nn.Linear(8, 16) # 扩展特征空间
self.relu = nn.ReLU()
self.fc2 = nn.Linear(16, 10) # 输出 2 个值（IQ 表示）

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x) # 输出原始 IQ 符号
return x

# 定义解码器
class Decoder(nn.Module):
def __init__(self):
super(Decoder, self).__init__()
self.fc1 = nn.Linear(100, 50) # 从 IQ 扩展回来
self.fc2 = nn.Linear(50, 30)
self.fc3 = nn.Linear(30, 16)
self.fc4 = nn.Linear(16, 8) # 输出 8 位恢复序列
self.relu = nn.ReLU()
self.sigmoid = nn.Sigmoid() # 确保输出在 (0,1) 范围内

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x)
x = self.relu(x)
x = self.fc3(x)
x = self.relu(x)
x = self.fc4(x)
x = self.sigmoid() # 解释为概率
return x

# 定义自动编码器 (编码器 -&gt;通道 -&gt; 解码器)
class Autoencoder(nn.Module):
def __init__(self, noise_std=0.1):
super(Autoencoder, self).__init__()
self.encoder = Encoder()
self.decoder = Decoder()

def forward(self, x):
x = self.encoder(x) # 将 8 位编码为 2 个 IQ 符号
x = self.decoder(x) # 解码回 8 位序列
return x

ParentModel = Autoencoder(noise_std=0.1)

# 加载预先训练的权重
load_weights(model, path, optimizer)

def knowledge_distillation(teacher, student, T, epochs, batches, alpha):
ce_loss = nn.BCELoss()
kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)
optimizer = optim.Adam(student.parameters(), lr = 1e-4)

teacher.eval() # 教师设置为评估模式
student.train() # 学生设置为训练模式

for epoch in range(epochs):
input_bits = generate_binary_tensor(8, batches) # 生成 [8, batch] 二进制张量

optimizer.zero_grad()

with torch.no_grad():
teacher_predictions = teacher(input_bits) # 教师前向传递

student_predictions = student(input_bits) # 学生前向传递

# 计算硬损失
hard_loss = ce_loss(student_predictions, input_bits)

# 计算软损失（不确定这部分）
soft_loss = kl_loss(student_predictions, teacher_predictions) * (T**2)

total_loss = alpha*soft_loss + (1-alpha)*hard_loss

total_loss.backward()
optimizer.step()

# 存储 BER

]]></description>
      <guid>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</guid>
      <pubDate>Wed, 05 Feb 2025 00:55:20 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法提高我的 CNN 准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</guid>
      <pubDate>Tue, 04 Feb 2025 20:07:08 GMT</pubDate>
    </item>
    <item>
      <title>函数内部和外部的权重和偏差会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412793/diferent-results-for-weights-and-biases-in-or-out-the-function</guid>
      <pubDate>Tue, 04 Feb 2025 19:13:39 GMT</pubDate>
    </item>
    <item>
      <title>如何为特定设备创建机器学习和线性回归模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device</link>
      <description><![CDATA[我有以下数据源模式：
在此处输入图片说明
我将“日期”列编码为三个单独的列：
年、月、日期。
我的未来是
设备 ID；消耗能量；年；月；日
在此处输入图片说明
我想预测给定未来的消耗能量（设备 ID 和日期）。能耗是一个日益增长的设备计数器
我创建了相关矩阵来查看属性之间的相关性，我发现消耗_能耗和设备之间的相关性非常低 (-0.15)。
以下是模型训练的模型指标：
&#39;rmse&#39;: np.float64(0.7648236497453013), 
&#39;r2_score&#39;: 0.41662485203361777, 
&#39;coefficients&#39;: array([[-0.59067425, 0.61791617, 0.04103179, 0.00336239]]), 
&#39;intercept&#39;: array([0.00086466]

当我仅使用一个 device_id 加载数据时，结果更好：
&#39;rmse&#39;: np.float64(0.1045133437744489), 
&#39;r2_score&#39;: 0.9894746517207146, 
&#39;coefficients&#39;: array([[0. , 0.99656364, 0.06202053, 0.00616694]]), 
&#39;intercept&#39;: array([-0.00046585])

我理解基于多个设备的数据构建的模型将产生不同的结果...
我在一个文件中接收包含所有设备的数据。当给定的未来是 device_id 时，如何正确构建此模型？
我应该创建预测方法，该方法首先为给定的 device_id 选择数据，然后为该特定设备创建新的数据框，然后构建模型并进行预测吗？这会耗费时间，我需要根据需求对每个预测进行计算。如果我想保存/选择模型并使用它来提供数据，该如何处理？（我的数据有很多设备）
我不知道是否可以基于具有多个设备的数据构建此模型，并驱动线回归算法将设备 ID 未来视为主要/重要系数因子。我应该使用不同的机器学习模型吗？
我尝试使用 python 和 scikit-learn 为特定设备创建机器学习和线回归模型]]></description>
      <guid>https://stackoverflow.com/questions/79412575/how-create-machine-learning-and-line-regresion-model-for-specific-device</guid>
      <pubDate>Tue, 04 Feb 2025 17:46:48 GMT</pubDate>
    </item>
    <item>
      <title>实现扩散生成模型进行数据增强，但训练损失值太高 [迁移]</title>
      <link>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412455/implementing-a-diffusion-generative-model-for-data-augmentation-but-training-los</guid>
      <pubDate>Tue, 04 Feb 2025 16:56:25 GMT</pubDate>
    </item>
    <item>
      <title>寻求专业培训项目评论数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</link>
      <description><![CDATA[我正在分析专业培训课程的评论，并根据这些评论创建推荐系统。具体来说，我感兴趣的是找到一个包含各种专业发展课程、培训课程和认证的用户评论的数据集。
我搜索过 Kaggle 和政府开放数据门户等常见来源，但没有找到我正在寻找的内容。
我在哪里可以找到这样的数据集？或者我如何使用其他方法来利用现有数据（Coursera / udemy 课程评论）？]]></description>
      <guid>https://stackoverflow.com/questions/79411761/seeking-dataset-for-reviews-of-professional-training-programs</guid>
      <pubDate>Tue, 04 Feb 2025 12:51:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 2048 游戏的训练对我来说效果不佳？[关闭]</title>
      <link>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</guid>
      <pubDate>Tue, 04 Feb 2025 10:28:14 GMT</pubDate>
    </item>
    <item>
      <title>训练数据集应如何分布？[关闭]</title>
      <link>https://stackoverflow.com/questions/72614571/how-should-a-training-dataset-be-distributed</link>
      <description><![CDATA[我正在构建一个文本转语音模型。我想知道我的训练数据集是否应该“现实地”分布（即与将要使用的数据分布相同），还是应该均匀分布以确保它在各种句子上都能表现良好。]]></description>
      <guid>https://stackoverflow.com/questions/72614571/how-should-a-training-dataset-be-distributed</guid>
      <pubDate>Tue, 14 Jun 2022 09:26:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的分片是什么以及如何在 Tensorflow 中进行分片？</title>
      <link>https://stackoverflow.com/questions/68981874/what-is-sharding-in-machine-learning-and-how-to-do-sharding-in-tensorflow</link>
      <description><![CDATA[在机器学习的背景下，分片具体是什么（这里提出了一个更通用的古怪问题），以及它在 Tensorflow 中是如何实现的？
在谈论机器学习中的数据管道时，什么是分片，为什么我们需要分片？]]></description>
      <guid>https://stackoverflow.com/questions/68981874/what-is-sharding-in-machine-learning-and-how-to-do-sharding-in-tensorflow</guid>
      <pubDate>Mon, 30 Aug 2021 09:35:08 GMT</pubDate>
    </item>
    <item>
      <title>银行交易数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</link>
      <description><![CDATA[我想使用银行交易数据集制作信用卡和借记卡之间的图表。借记和贷记金额，但没有得到正确的数据集。
有人能给我提供同样的数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</guid>
      <pubDate>Sat, 06 Jul 2019 13:09:23 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护 - 如何将目标函数的贝叶斯优化与梯度下降的逻辑回归结合起来使用？</title>
      <link>https://stackoverflow.com/questions/48619800/predictive-maintenance-how-to-use-bayesian-optimization-with-objective-functio</link>
      <description><![CDATA[我试图重现 arimo.com 中显示的问题

这是一个如何为硬盘故障构建预防性维护机器学习模型的示例。我真正不明白的部分是如何将贝叶斯优化与自定义目标函数和逻辑回归与梯度下降结合使用。要优化的超参数是什么？问题的流程是什么？

正如我们之前的帖子所述，贝叶斯优化 [6] 用于
找到最佳超参数值。超参数调整中要优化的目标函数是在验证集上测量的以下分数：
S = alpha * fnr + (1 – alpha) * fpr
其中 fpr 和 fnr 是在验证集上获得的假阳性和假阴性率。我们的目标是保持假阳性率较低，因此我们使用 alpha = 0.2。由于验证集高度不平衡，我们发现标准分数（如准确率、F1 分数等）效果不佳。事实上，使用这个自定义分数对于模型获得良好的性能至关重要。
请注意，我们仅在运行贝叶斯优化时使用上述分数。为了训练逻辑回归模型，我们使用梯度下降法和通常的岭损失函数。

特征选择之前的数据框：
index date serial_number model capacity_bytes Failure 读取错误率 重新分配扇区数 通电时间 (POH) 温度 当前待处理扇区数 age yet_temperature yet_age yet_reallocated_sectors_count yet_read_error_rate yet_current_pending_sector_count yet_power_on_hours yet_failure
0 77947 2013-04-11 MJ0331YNG69A0A Hitachi HDS5C3030ALA630 3000592982016 0 0 0 4909 29 0 36348284.0 29.0 20799895.0 0.0 0.0 0.0 4885.0 0.0
1 79327 2013-04-11 MJ1311YNG7EWXA 日立 HDS5C3030ALA630 3000592982016 0 0 0 8831 24 0 36829839.0 24.0 21280074.0 0.0 0.0 0.0 8807.0 0.0
2 79592 2013-04-11 MJ1311YNG2ZD9A 日立 HDS5C3030ALA630 3000592982016 0 0 0 13732 26 0 36924206.0 26.0 21374176.0 0.0 0.0 0.0 13708.0 0.0
3 80715 2013-04-11 MJ1311YNG2ZDBA 日立 HDS5C3030ALA630 3000592982016 0 0 0 12745 27 0 37313742.0 27.0 21762591.0 0.0 0.0 0.0 12721.0 0.0
4 79958 2013-04-11 MJ1323YNG1EK0C 日立 HDS5C3030ALA630 3000592982016 0 524289 0 13922 27 0 37050016.0 27.0 21499620.0 0.0 0.0 0.0 13898.0 0.0
]]></description>
      <guid>https://stackoverflow.com/questions/48619800/predictive-maintenance-how-to-use-bayesian-optimization-with-objective-functio</guid>
      <pubDate>Mon, 05 Feb 2018 09:55:51 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 的超参数调整</title>
      <link>https://stackoverflow.com/questions/44181511/hyperparameter-tune-for-tensorflow</link>
      <description><![CDATA[我正在寻找一个用于直接在 Tensorflow（而不是 Keras 或 Tflearn）中编写的代码的超参数调整包。您能给我一些建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/44181511/hyperparameter-tune-for-tensorflow</guid>
      <pubDate>Thu, 25 May 2017 13:13:53 GMT</pubDate>
    </item>
    <item>
      <title>神经网络是一种懒惰的学习方法还是积极学习的方法？[关闭]</title>
      <link>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</link>
      <description><![CDATA[神经网络是一种懒惰的还是积极学习的方法？不同的网页有不同的说法，所以我想得到一个可靠的答案，并有好的文献来支持它。最明显的书是米切尔著名的《机器学习》一书，但浏览整本书我找不到答案。谢谢 :)。]]></description>
      <guid>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</guid>
      <pubDate>Thu, 21 Apr 2011 21:16:25 GMT</pubDate>
    </item>
    </channel>
</rss>