<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 15:12:21 GMT</lastBuildDate>
    <item>
      <title>ValueError：无法解释指标标识符：使用 Keras 和 Scikeras 造成损失</title>
      <link>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</guid>
      <pubDate>Sat, 16 Mar 2024 14:19:04 GMT</pubDate>
    </item>
    <item>
      <title>代码有问题还是我的数据有问题？</title>
      <link>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</guid>
      <pubDate>Sat, 16 Mar 2024 10:00:18 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中，如何导出-2*wx</title>
      <link>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</link>
      <description><![CDATA[【梯度下降算法部分】
(https://i.stack.imgur.com/iTBz6.png)&lt; /p&gt;
this.updateWeights = function() {
 
  让wx；
  让w_deriv = 0;
  让 b_deriv = 0;

  for (让 i = 0; i &lt; this.points; i++) {
    wx = this.yArr[i] - (this.weight * this.xArr[i] + this.bias);
    w_deriv += -2 * wx * this.xArr[i];
    b_deriv += -2 * wx;
  }
  
  this.weight -= (w_deriv / this.points) * this.learnc;
  this.bias -= (b_deriv / this.points) * this.learnc;
}
            

请解释一下这部分！！
-2 * wx * this.xArr[i]

这部分是诱导出来的......？
如何通过数学公式归纳...]]></description>
      <guid>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</guid>
      <pubDate>Sat, 16 Mar 2024 09:41:21 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的超参数调整和模型评估</title>
      <link>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</link>
      <description><![CDATA[我对机器学习很陌生，对如何正确使用超参数调整和模型评估感到有点困惑。
超参数调整应该在整个数据集上进行还是仅在训练集上进行？正确的操作顺序是什么？
您能否检查我的代码并建议我考虑该问题的最佳实践？
在这里，我首先对整个数据集使用超参数调整，然后仅在训练集上评估模型性能。这是对的吗？不会导致数据泄露吗？
超参数调优
numeric_features = X.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns
categorical_features = X.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).columns

预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, StandardScaler(), numeric_features),
        (&#39;猫&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), categorical_features)
    ]
）

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alpha = np.arange(0, 1.1, 0.1),
                     随机状态=818，
                     职位数 = -1)

模型= make_pipeline（预处理器，en_cv）
模型.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

模型评估：
ElasticNet = make_pipeline(预处理器, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = 均方误差(y_test, y_pred)

打印（r2，MSE）

提前致谢，祝您有美好的一天！
实际上，这段代码在包含约 80000 个观察值和约 150 列的数据集上运行大约需要 18 分钟。这是否足够？]]></description>
      <guid>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</guid>
      <pubDate>Sat, 16 Mar 2024 09:27:26 GMT</pubDate>
    </item>
    <item>
      <title>我在梯度提升模型中收到此错误“AttributeError：'HalfSquaredError'对象没有属性'get_init_raw_predictions'”</title>
      <link>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</guid>
      <pubDate>Sat, 16 Mar 2024 08:56:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 opencv / python / 任何 AI/ML 模型检测并提取二值图像中的四边形的四个点</title>
      <link>https://stackoverflow.com/questions/78170927/detect-and-extract-four-points-of-quadrilaterals-in-an-binary-image-using-opencv</link>
      <description><![CDATA[我有这样的二进制分段蒙版图像

我想从这样的图像中检测四边形的四个坐标

output object = [ ( (x11,y11), (x12,y12), (x13,y13), (x14,y14) ) , # 第一个四边形
                  ( (x21,y21), (x22,y22), (x23,y23), (x24,y24) ) ] # 第二个四边形

我尝试过使用边缘检测技术的文档扫描方法，但无法找到四边形的四个点。
我可以通过此代码获取角点的坐标，但无法从中创建四边形。
导入imutils
导入CV2
将 numpy 导入为 np

图像 = cv2.imread(&#39;../masks\cpu-b-model\segmentation4.png&#39;,0)

cnts = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
cnts = imutils.grab_contours(cnts)
c = max(cnts, key=cv2.contourArea)
输出=图像.copy()
cv2.drawContours(输出, [c], -1, (0, 255, 0), 3)
(x, y, w, h) = cv2.boundingRect(c)
text = &quot;原始, num_pts={}&quot;.format(len(c))
# cv2.putText(输出, 文本, (x, y - 15), cv2.FONT_HERSHEY_SIMPLEX,0.9, (0, 255, 0), 2)
cv2.imshow(“原始轮廓”, 输出)
cv2.waitKey(0)
cv2.destroyAllWindows()
打印（文本）

对于 np.linspace(0.05, 0.1, 10) 中的 eps：
    peri = cv2.arcLength(c, True)
    近似 = cv2.approxPolyDP(c, eps * peri, True)
    打印（大约）
    输出=图像.copy()
    cv2.drawContours(输出, [大约], -1, (0, 255, 0), 3)
    文本 =“eps={:.4f}, num_pts={}”.format(eps, len(大约))
    cv2.putText（输出，文本，（x，y - 15），cv2.FONT_HERSHEY_SIMPLEX，
        0.9, (0, 255, 0), 2)
    print(“[INFO] {}”.format(text))
    cv2.imshow(“近似轮廓”, 输出)
    cv2.waitKey(0)
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78170927/detect-and-extract-four-points-of-quadrilaterals-in-an-binary-image-using-opencv</guid>
      <pubDate>Sat, 16 Mar 2024 07:30:32 GMT</pubDate>
    </item>
    <item>
      <title>删除点云中的底层</title>
      <link>https://stackoverflow.com/questions/78170818/remove-base-floor-in-point-cloud</link>
      <description><![CDATA[我有沙堆 3d 点云的 .obj 文件。它包含底层点和沙堆点。我需要移除底层点并仅过滤沙堆。有相关的 Matlab 或 Python 代码吗？
我尝试了一些使用 python 的代码。但它不正确。]]></description>
      <guid>https://stackoverflow.com/questions/78170818/remove-base-floor-in-point-cloud</guid>
      <pubDate>Sat, 16 Mar 2024 06:31:22 GMT</pubDate>
    </item>
    <item>
      <title>实时检测在YOLOv8中播放音频文件</title>
      <link>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</link>
      <description><![CDATA[我正在 YOLOv8 项目中工作，以检测困倦并在检测到困倦时播放警报音频文件。我面临的问题是我无法实时播放音频，因为我的检测首先存储在结果中。一旦我关闭检测窗口，它就会访问结果中存储的数据并连续播放音频。我该如何解决这个问题？
导入操作系统
从 ultralytics 导入 YOLO

进口火炬
导入 matplotlib
将 numpy 导入为 np
导入CV2
导入pygame

pygame.init()
sound_to_play = pygame.mixer.Sound(r&#39;D:\ML\同步警惕驱动程序\alarm.wav&#39;)
sound_to_play.play()

模型 = YOLO(r&#39;C:\Users\HP\Downloads\last.pt&#39;)

上限 = cv2.VideoCapture(0)
而真实：
    ret, 框架 = cap.read()

    结果 = model.predict(source=“0”,show=True)
    对于结果中的 r：
        如果 len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            打印（d类）
            如果 dclass==2.0:
              sound_to_play.play()
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

pygame.quit()
cap.release()
cv2.destroyAllWindows()

&lt;块引用&gt;
问题是我的代码首先进行检测并将其存储在结果中，然后进入 for 循环
预期输出是它同时检测并检查类值
]]></description>
      <guid>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</guid>
      <pubDate>Sat, 16 Mar 2024 06:28:04 GMT</pubDate>
    </item>
    <item>
      <title>XGBRegressor 树中叶子值的求和与预测不匹配</title>
      <link>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</link>
      <description><![CDATA[据我了解，XGBoost 模型（在本例中为 XGBRegressor）的最终预测是通过对预测叶子的值求和来获得的 [1] [2]。然而，我未能匹配对值求和的预测。这是 MRE：
导入json
从集合导入双端队列

将 numpy 导入为 np
从 sklearn.datasets 导入 load_diabetes
从 sklearn.model_selection 导入 train_test_split
将 xgboost 导入为 xgb


def leafs_vector(树):
    “”“”返回每棵树的节点向量，只有叶子有 0 个不同“”“”

    堆栈 = 双端队列([树])

    而堆栈：
        节点 = stack.popleft()
        如果“叶”是在节点中：
            产量节点[“叶子”]
        别的：
            产量 0
            对于节点 [“children”] 中的子节点：
                堆栈.追加（子）


# 加载糖尿病数据集
糖尿病 = load_diabetes()
X, y = 糖尿病.数据, 糖尿病.目标

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 XGBoost 回归模型
xg_reg = xgb.XGBRegressor(目标=&#39;reg:squarederror&#39;,
                          最大深度=5，
                          n_估计器=10)

# 训练模型
xg_reg.fit(X_train, y_train)

# 计算原始预测
y_pred = xg_reg.predict(X_test)

# 获取每个预测叶子的索引
Predicted_leafs_indices = xg_reg.get_booster().predict(xgb.DMatrix(X_test), pred_leaf=True).astype(np.int32)

# 获取树木
树 = xg_reg.get_booster().get_dump(dump_format=“json”)
trees = [json.loads(tree) 用于树中的树]

# 获取节点向量（按节点 ID 排序）
leafs = [树中树的列表(leafs_vector(tree))]

l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)))

断言 np.allclose(np.array(l_pred), y_pred, atol=0.5) # 失败

我还尝试添加 base_score 的默认值 (0.5)（如 这里）到总和，但它也不起作用。
&lt;前&gt;&lt;代码&gt;l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)) + 0.5)
]]></description>
      <guid>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</guid>
      <pubDate>Fri, 15 Mar 2024 21:07:25 GMT</pubDate>
    </item>
    <item>
      <title>将 fit_resamples 与自定义分割数据一起使用？</title>
      <link>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</link>
      <description><![CDATA[我有一个自定义函数，可以根据各种标准和规则将数据分成训练集和测试集。我想在 tidymodels 工作流程中与 fit_resamples 一起使用此函数。但是，当我可以使我的列表看起来像用 vfold_cv 制作的列表时，它似乎不起作用。我正在使用的示例代码：
data(ames, package = “modeldata”)

split_data &lt;- 函数(df, n) {
  set.seed(123) # 为了重现性
  df$id &lt;- seq.int(nrow(df))
  list_of_splits &lt;- list()
  
  for(i in 1:n) {
    train_index &lt;- 样本(df$id, size=ceiling(nrow(df)*.8))
    train_set &lt;- df[train_index,]
    test_set &lt;- df[-train_index,]
    list_of_splits[[i]] &lt;- list(train_set = train_set, test_set = test_set)
  }
  
  返回（分割列表）
}

分割 &lt;- split_data(ames, 5)

重新采样 &lt;- map(splits, ~rsample::make_splits(
  x = .$train_set |&gt;选择(colnames(.$test_set)),
  评估=.$test_set
））

名称（重新采样）&lt;-paste0（“折叠”，seq_along（重新采样））

重新采样 &lt;- tibble::tibble(splits = 重新采样,
                            id = 名称（重新采样））

lm_model &lt;-
  Linear_reg() %&gt;%
  set_engine(“lm”)

lm_wflow &lt;-
  工作流程() %&gt;%
  add_model(lm_model) %&gt;%
  add_formula(Sale_Price ~ 经度 + 纬度)

res &lt;- lm_wflow %&gt;%
  fit_resamples（重新采样=重新采样）

运行最后一行后返回的错误是：
`check_rset()` 中出现错误：
！ “resamples”参数应该是一个“rset”对象，例如由“vfold_cv()”或其他“rsample”函数生成的类型。

如果我尝试强制该类“rset” class(resamples) &lt;- “rset”，列表看起来不再正确，我得到了相同的错误。
使用自定义交叉折叠数据集的正确方法是什么？
注意 - 附加问题：在上面的示例代码中，测试集和训练集的大小在折叠中是一致的。在我的实际数据中，这会略有不同 - 这有关系吗？]]></description>
      <guid>https://stackoverflow.com/questions/78167178/use-fit-resamples-with-custom-split-data</guid>
      <pubDate>Fri, 15 Mar 2024 13:11:15 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中线性回归的内部工作原理</title>
      <link>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</link>
      <description><![CDATA[我试图了解 Scikit-learn 中线性回归模型的内部工作原理。
这是我的数据集

这是我执行 one-hot-encoding 后的数据集。

这是执行线性回归后的系数和截距值。

销售价格是从属列，其余列是特征。
这些是在这种情况下工作正常的预测值。

我注意到系数的数量比特征的数量多 1。这就是我生成特征矩阵的方式：
feature_matrix = dataFrame.drop([&#39;售价($)&#39;], axis = &#39;列&#39;).to_numpy()

# 要添加为列的数组
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# 使用append()方法将列添加到数组
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1) # axis = 1表示列，0表示行

结果

我想知道的是 Scikit-learn 如何使用这些系数和截距来预测值。
这是我尝试过的。
我还注意到，通过进行此计算得到的值实际上等于每种情况下的里程数。但这不是这里的依赖功能。那么这是怎么回事？]]></description>
      <guid>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</guid>
      <pubDate>Fri, 15 Mar 2024 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>使用convert_marian_to_pytorch.py​​脚本转换opus mt模型后出现问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</link>
      <description><![CDATA[利用convert_marian_pytorch.py​​ 脚本将 OPUS 转换模型转换为与 Hugging Face Transformers 兼容的格式后，我将生成的模型上传到 Hugging Face 的模型中心 此处。然而，经过测试，模型的翻译输出似乎是一堆随机单词，缺乏连贯性。
如何解决此问题或探索替代方法以直接在 Python 中使用 Marian 模型，而不依赖 OPUS cat mt 引擎。]]></description>
      <guid>https://stackoverflow.com/questions/78163296/problem-after-using-convert-marian-to-pytorch-py-script-to-convert-opus-mt-model</guid>
      <pubDate>Thu, 14 Mar 2024 20:30:25 GMT</pubDate>
    </item>
    <item>
      <title>多类不平衡数据集预处理问题</title>
      <link>https://stackoverflow.com/questions/78148016/multiclass-imbalance-dataset-preprocessing-problem</link>
      <description><![CDATA[在预处理我面临的数据集时
ValueError：目标是多类，但平均值=&#39;二进制&#39;。请选择其他
平均设置，[无、&#39;微观&#39;、&#39;宏观&#39;、&#39;加权&#39;]之一。

如何解决？
我会请专家去检查一下我在那里犯了什么错误。
我尝试使用迄今为止学到的知识进行预处理部分。]]></description>
      <guid>https://stackoverflow.com/questions/78148016/multiclass-imbalance-dataset-preprocessing-problem</guid>
      <pubDate>Tue, 12 Mar 2024 14:42:42 GMT</pubDate>
    </item>
    <item>
      <title>scikeras.wrappers.KerasClassifier 返回 ValueError：无法解释指标标识符：loss</title>
      <link>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</link>
      <description><![CDATA[我正在研究 KerasClassifier，因为我想将其插入 scikit-learn 管道中，但我收到了前面提到的 ValueError。
以下代码应该能够重现我遇到的错误：
从 sklearn.model_selection 导入 KFold，cross_val_score
从 sklearn.preprocessing 导入 StandardScaler
从 scikeras.wrappers 导入 KerasClassifier
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从 sklearn.datasets 导入 load_iris
将 numpy 导入为 np

数据 = load_iris()
X = 数据.数据
y = 数据.目标

def create_model():
    模型=顺序（）
    model.add（密集（8，input_dim = 4，激活=&#39;relu&#39;））
    model.add（密集（3，激活=&#39;softmax&#39;））
    model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
                  优化器=&#39;亚当&#39;,
                  指标=[&#39;准确性&#39;])
    返回模型

clf = KerasClassifier(build_fn=create_model,
                      纪元=100，
                      批量大小=10，
                      详细=1)

管道=管道（[
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;clf&#39;，clf）
]）

kf = KFold(n_splits=5, shuffle=True, random_state=42)
结果= cross_val_score（管道，X，y，cv = kf）
print(&quot;交叉验证准确度：&quot;, np.mean(结果))

似乎我的模型正在随着纪元的运行而被编译。但是，之后我收到错误：
ValueError：无法解释指标标识符：丢失

tensorflow 和 scikeras 库的版本是：
scikeras==0.12.0
张量流==2.15.0
]]></description>
      <guid>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</guid>
      <pubDate>Fri, 01 Mar 2024 17:03:39 GMT</pubDate>
    </item>
    <item>
      <title>与 joblib 并行训练 sklearn 模型会阻碍该过程</title>
      <link>https://stackoverflow.com/questions/47551850/training-sklearn-models-in-parallel-with-joblib-blocks-the-process</link>
      <description><![CDATA[根据这个答案中的建议，我尝试使用 joblib并行训练多个 scikit-learn 模型。
导入joblib
导入numpy
从sklearn导入树，线性模型

分类器参数 = {
                “决策树”：(tree.DecisionTreeClassifier, {}),&#39;&#39;
                “逻辑回归”：（线性模型.LogisticRegression，{}）
}


XTrain = numpy.array([[1,2,3],[4,5,6]])
yTrain = numpy.array([0, 1])

def trainModel(名称, clazz, params, XTrain, yTrain):
    print(&quot;训练&quot;, 姓名)
    模型 = clazz(**参数)
    model.fit(XTrain, yTrain)
    返回模型


joblib.Parallel(n_jobs=4)(joblib.delayed(trainModel)(name, clazz, params, XTrain, yTrain) for (name, (clazz, params)) in classifierParams.items())

但是，在不使用 CPU 的情况下，对最后一行的调用需要很长时间，事实上它似乎只是阻塞并且从不返回任何内容。我的错误是什么？ 
在 XTrain 中使用极少量数据进行的测试表明，跨多个进程复制 numpy 数组并不是延迟的原因。]]></description>
      <guid>https://stackoverflow.com/questions/47551850/training-sklearn-models-in-parallel-with-joblib-blocks-the-process</guid>
      <pubDate>Wed, 29 Nov 2017 11:34:35 GMT</pubDate>
    </item>
    </channel>
</rss>