<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 02 Apr 2024 12:25:38 GMT</lastBuildDate>
    <item>
      <title>MNIST 中 0 的误报太多：使用 Logistic 回归进行多类分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/78260773/way-too-many-false-positives-for-0-in-mnist-multiclass-classification-using-log</link>
      <description><![CDATA[我编写了一个基本上使用 MNIST 数据集的代码，并使用逻辑回归对 0 到 9 的数字进行分类。我使用了多个逻辑回归单元并单独训练它们，保存它们的权重，然后找到每个逻辑单元的预测标签。然后我简单地取给出最大值的逻辑单元，并将与该逻辑单元对应的数字视为预测数字
但是当我看到混淆矩阵时，它在第一列中显示了许多误报，即大多数逻辑单元进行了许多错误分类，认为它们为零。这是我获得的混淆矩阵（附为照片）：
混淆矩阵
我不确定我在这里做错了什么。我将在此处留下笔记本的链接：
https://colab.research.google.com/drive/1rDCW4Tpf4AMLzjxYjTrqv4c1_e0oOY75?usp=sharing
我尝试使用 MNIST 数据集实现逻辑回归并执行多类分类。
我在 Kaggle 中浏览了一下，发现了类似的东西，但他们的混淆矩阵结果是这样的：
链接到在 MNIST 分类器上使用逻辑回归实现的多类分类：
https://www.kaggle.com/code/hamzaboulahia/logistic-regression-mnist-classification]]></description>
      <guid>https://stackoverflow.com/questions/78260773/way-too-many-false-positives-for-0-in-mnist-multiclass-classification-using-log</guid>
      <pubDate>Tue, 02 Apr 2024 10:57:45 GMT</pubDate>
    </item>
    <item>
      <title>如何有效地使用神经网络进行回归？</title>
      <link>https://stackoverflow.com/questions/78260668/how-can-i-effectively-use-a-neural-net-for-regression</link>
      <description><![CDATA[我有训练数据和相应的目标数据来训练。我的数据的形状是 [1,272] - 只是 272 个值的数组。我有少量数据可供使用，但我相信这是可行的。
我的目标是捕获第一个数组中的每个值与目标数组中相同索引处的值之间的关系。
例如：使用 X: [1,5,3] Y: [2,10,6] 训练的网络将预测输入：[5, 10, 20]，预测 [10,20,40] 
在我的例子中，这些值将具有非线性关系
我的最终目标是从每个值的直接邻居中获取一些影响，为此我尝试了 conv1d 层并向后移动。
当我确实有一个模型训练并实现相对较低的损失时，当我尝试使用新的、未见过的数组进行预测时，无论输入如何，结果通常都非常接近。
我假设我在这里尝试的东西有一个名称，也许还有一条不同的路线（不是神经网络）。即使是一些正确的术语也会对我有很大帮助 - 即使谷歌也不知所措。
我正在使用tensorflowjs。我尝试过：
尝试1
仅使用 2 个密集层，均具有 272 个单元，其中一个使用 tf.eye(272) 进行权重，第二个我让权重自然初始化。尝试使用可训练和不可训练的第一层。目标是迫使模型在此处找到相应值之间的某种关系，而不受数组中远处值的影响
结果 1
可训练密集层的权重显然会受到各个方面的影响，以实现最低的损失。
尝试2
使用 conv1d 层（填充：“相同”、kernelSize：1、stride：1、filters：272），后跟不可训练的密集层，如前所述。
结果 2
无论配置/激活功能如何，损失都不会下降。
在这些架构中，我尝试了激活函数的所有组合，并将一层或另一层设置为不可训练，useBias：true/false等。尽管我感觉我在这里走错了路。]]></description>
      <guid>https://stackoverflow.com/questions/78260668/how-can-i-effectively-use-a-neural-net-for-regression</guid>
      <pubDate>Tue, 02 Apr 2024 10:36:31 GMT</pubDate>
    </item>
    <item>
      <title>(Pytorch) mat1 和 mat2 形状不能相乘（212992x13 和 1280x3）</title>
      <link>https://stackoverflow.com/questions/78259907/pytorch-mat1-and-mat2-shapes-cannot-be-multiplied-212992x13-and-1280x3</link>
      <description><![CDATA[我正在尝试使用自定义数据集在 Pytorch 预训练模型上进行迁移学习。
目前，我收到错误如下
mat1 和 mat2 形状不能相乘（212992x13 和 1280x3）
在训练自定义模型期间。
当我尝试使用高效网络时，下面的代码可以工作并且训练成功，但是当我使用像挤压网络这样的模型时，我收到错误
作品：
权重 = torchvision.models.EfficientNet_B0_Weights.DEFAULT
模型 = torchvision.models.efficientnet_b0(weights=weights).to(device)

不起作用：
权重 = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
模型 = torchvision.models.squeezenet1_0(权重=权重).to(设备)

火车：
auto_transforms = Weights.transforms()
train_dataloader，test_dataloader，class_names = data_setup.create_dataloaders（train_dir = train_dir，test_dir = test_dir，transform = auto_transforms，batch_size = 32）

对于 model.features.parameters() 中的参数：
    param.requires_grad = False #冻结层

火炬.manual_seed(42)
输出形状 = len(类名)
model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    torch.nn.Linear(in_features=1280,
                    输出特征=输出形状，
                    偏差=True)).to(设备)

loss_fn = nn.CrossEntropyLoss()
优化器 = torch.optim.Adam(model.parameters(), lr=0.001)

#火车期间出错
结果= engine.train(model=model,train_dataloader=train_dataloader,test_dataloader=test_dataloader,optimizer=optimizer,loss_fn=loss_fn,epochs=100,device=device)

训练图像尺寸为512x512
为了确保这不是转换问题，我使用了自动转换，但问题仍然存在。
虽然存在类似的主题mat1和mat2形状不能相乘（128x4 和 128x64），它完全基于创建新的顺序模型，而我正在尝试在预训练模型上使用迁移学习。]]></description>
      <guid>https://stackoverflow.com/questions/78259907/pytorch-mat1-and-mat2-shapes-cannot-be-multiplied-212992x13-and-1280x3</guid>
      <pubDate>Tue, 02 Apr 2024 08:24:45 GMT</pubDate>
    </item>
    <item>
      <title>如何从图像中预测字符？</title>
      <link>https://stackoverflow.com/questions/78259157/how-to-predict-characters-from-an-image</link>
      <description><![CDATA[要求：从图像中读取或预测字符。
先决条件：我在 python 中使用 pytesseract 和 opencv 从 imagesv 读取文本，但我看到了一个问题，如下所述。
问题：

很少有图像的文本被水平切片（如附件所示）。


当我运行 python 函数从附加图像中读取文本时，它会读取
不正确的文字。从图像中读取文本的示例代码片段如下。


实际输出： 对于附加图像，而不是 UnAuthorizedInteger，它读取为 InAutharizedintanear
 def extract_text_from_image(image_path):
       图像 = Image.open(图像路径)
       文本 = pytesseract.image_to_string(图像)
       返回文本

那么，当图像如上面所附的那样进行切片时，我们可以使用任何深度学习模型来预测图像中的文本吗？如果是，请告诉我要应用的策略。]]></description>
      <guid>https://stackoverflow.com/questions/78259157/how-to-predict-characters-from-an-image</guid>
      <pubDate>Tue, 02 Apr 2024 05:42:07 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试计算 PGN 的最终距离时遇到问题，IndexError：索引 695 超出尺寸 695 的维度 1 的范围</title>
      <link>https://stackoverflow.com/questions/78259116/i-am-facing-problem-while-trying-to-calculate-final-dist-for-pgn-indexerror-in</link>
      <description><![CDATA[我从代码中得到的错误是
回溯（最近一次调用）：
  文件“/Users/sagar/PycharmProjects/TouchFYP/V2_main.py”，第 201 行，在  中
    火车（模型，train_loader，val_loader，优化器，标准，调度器，设备，5，）
  文件“/Users/sagar/PycharmProjects/TouchFYP/V2_main.py”，第 167 行，列车中
    损失，_ = 模型（文章，ext_enc_inp，摘要[:, :-1]，摘要[:, 1:]，teacher_forcing_ratio=0.5）
  文件“/Users/sagar/PycharmProjects/TouchFYP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1511 行，位于 _wrapped_call_impl 中
    返回 self._call_impl(*args, **kwargs)
  文件“/Users/sagar/PycharmProjects/TouchFYP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1520 行，位于 _call_impl 中
    返回forward_call(*args, **kwargs)
  文件“/Users/sagar/PycharmProjects/TouchFYP/my_V2.py”，第 283 行，向前
    Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov,
  文件“/Users/sagar/PycharmProjects/TouchFYP/my_V2.py”，第 359 行，位于 get_final_distribution
    attn_dist_extended[b, vocab_idx] +=attention_weighted[b, idx]
IndexError：索引 695 超出尺寸 695 的维度 1 的范围

这就是我的代码：
 defforward(self, enc_input, enc_input_ext, dec_input, target=None, Teacher_forcing_ratio=0.5):
        enc_output, enc_hidden = self.encoder(enc_input)

        dec_hidden = enc_hidden
        batch_size, seq_len = dec_input.size()
        输出 = torch.zeros(batch_size, seq_len, self.vocab_size).to(
            enc_input.device) # 相应调整 self.vocab_size

        如果自我训练：
            损失=0
            for t in range(seq_len - 1): # 假设 dec_input 包含 ; &lt;eos&gt;
                dec_input_t = dec_input[:, t].unsqueeze(1)
                true_output = 目标[:, t + 1]
                p_vocab，dec_hidden，p_gen = self.decoder（dec_input_t，dec_hidden，enc_output）
                context_vector, attn = self.attention(dec_hidden, enc_output)
                Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov) # 简化

                # Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov) # 简化

                损失+= f.cross_entropy(final_dist, true_output,ignore_index=self.pad_token_id)
            回波损耗 / (seq_len - 1)
        别的：
            生成的令牌 = []
            dec_input_t = torch.tensor([[self.sos_token_id]] * batch_size,
                                       device=enc_input.device) # 以  开头代币
            对于范围内的 t(self.max_length)：
                p_vocab，dec_hidden，p_gen = self.decoder（dec_input_t，dec_hidden，enc_output）
                context_vector, attn = self.attention(dec_hidden, enc_output)
                Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab,attn, self.max_oov) # 简化

                topi = Final_dist.max(1)[1]
                generated_tokens.append(topi.squeeze().detach().cpu().numpy())

                if (topi == self.sos_token_id).all():
                    休息
                dec_input_t = topi.unsqueeze(1)

            返回 generated_tokens

    def get_final_distribution（自我，x，p_gen，p_vocab，attention_weights，max_oov）：
        批量大小 = x.size(0)
        # 裁剪概率以避免损失计算中的 log(0)
        p_gen = torch.clamp(p_gen, 0.001, 0.999)
        p_vocab_weighted = p_gen * p_vocab
        注意力权重 = (1 - p_gen) * 注意力权重

        扩展名 = torch.zeros((batch_size, max_oov)).float().to(x.device)
        p_vocab_extended = torch.cat([p_vocab_weighted, 扩展名], 暗淡=1)
        # print(&quot;得到最终分布的x值&quot;,x.shape)
        # print(&quot;x 中的最大索引:&quot;, x.max().item())
        # print(&quot;p_vocab_extend 的第二个维度：&quot;, p_vocab_extend.size(1))
        # print(&quot;得到最终分布的attention_weighted&quot;, attention_weighted.shape)
        # print(&quot;来自 x 的样本值：&quot;, x[0, :10])
        # print(&quot;attention_weighted 的样本值:&quot;, Attention_weighted[0, :10])

        Final_distribution = p_vocab_extended.scatter_add_(1, x, 注意权重)

        返回最终分布
]]></description>
      <guid>https://stackoverflow.com/questions/78259116/i-am-facing-problem-while-trying-to-calculate-final-dist-for-pgn-indexerror-in</guid>
      <pubDate>Tue, 02 Apr 2024 05:29:10 GMT</pubDate>
    </item>
    <item>
      <title>如何定义（多个）向量以输入到输出已知数据的（多个）向量的黑匣子</title>
      <link>https://stackoverflow.com/questions/78258904/how-to-define-multiple-vectors-for-input-to-a-black-box-that-outputs-multiple</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78258904/how-to-define-multiple-vectors-for-input-to-a-black-box-that-outputs-multiple</guid>
      <pubDate>Tue, 02 Apr 2024 04:12:08 GMT</pubDate>
    </item>
    <item>
      <title>将数据集从 LabelImg XML 转换为 YOLO txt 格式时出现问题：某些边界框的长度和宽度反转</title>
      <link>https://stackoverflow.com/questions/78258685/problem-with-converting-a-dataset-from-labelimg-xml-to-yolo-txt-format-inversio</link>
      <description><![CDATA[将数据集从 LabelImg XML (ImageNet) 格式转换为 YOLO txt 格式时，某些边界框的宽度和高度颠倒（即应该是宽度是高度，应该是高度是宽度）。
为了转换数据，我使用以下函数
def get_classes():
    &quot;&quot;&quot;&quot;从 txt 文件中读取类列表&quot;&quot;&quot;&quot;
    类=[]
    将 open(&#39;classes.txt&#39;) 作为 f：
        行= f.readlines()
        对于行中的 i：
            电流=我
            当前 = current.replace(&#39;\n&#39;, &#39;&#39;)
            类.append（当前）
    返回类

def 转换（大小，盒子）：
    “”““转换单个 robndbox”“”
    dw = 1./（大小[0]）
    dh = 1./（大小[1]）
    x = 框[0] * dw
    y = 框[1] * dh
    w = 框[2] * dw
    h = 框[3] * dh
    返回 x、y、w、h


def Convert_annotation(annotation_path):
    》》》》将XML标注文件转换为YOLO txt格式的函数》》》》
    in_file = open(annotation_path, 编码=&#39;UTF-8&#39;)
    out_file = open(&#39;./labels/&#39; + os.path.basename(annotation_path)[:-4] + &#39;.txt&#39;, &#39;w&#39;)
    树 = ET.parse(in_file)
    根 = 树.getroot()
    大小 = root.find(&#39;大小&#39;)
    类 = get_classes()
    w = int(size.find(&#39;宽度&#39;).text)
    h = int(size.find(&#39;高度&#39;).text)
    对于 root.iter(&#39;object&#39;) 中的 obj：
        困难 = obj.find(&#39;困难&#39;).text
        cls = obj.find(&#39;名称&#39;).text
        如果 cls 不在类中或 int(difficult) == 1：
            继续
        cls_id = 类.index(cls)
        xmlbox = obj.find(&#39;robndbox&#39;)
        b = (float(xmlbox.find(&#39;cx&#39;).text), float(xmlbox.find(&#39;cy&#39;).text), float(xmlbox.find(&#39;w&#39;).text),
             浮动（xmlbox.find（&#39;h&#39;）.text））
        theta = float(xmlbox.find(&#39;角度&#39;).text)
        θ -= 1.5
        b1、b2、b3、b4 = b
        如果b3＜ b4：
            b = (b1, b2, b4, b3)
            θ = int(((θ * 180 / math.pi) + 90) % 180)
        别的：
            θ = int(θ * 180 / math.pi)
        bb = 转换((w, h), b)
        #print(f“{theta} - {cls}”)
        out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &quot; &quot; + str(theta) + &#39;\n&#39;)


我期望得到的结果：
labelImg2 带注释的图像单选组件 xml
我实际上得到了什么
在此处输入图像描述
图中，terminal、transistor.bjt.pnp、resistor.adjustable 等元素和一些电阻器具有倒置的边界框。
例如，其中一些元素（晶体管和可调电阻）在 xml 中具有以下标记
&lt;前&gt;&lt;代码&gt;&lt;对象&gt;;
    &lt;名称&gt;transistor.bjt.pnp
    &lt;姿势&gt;未指定&lt;/姿势&gt;
    &lt;截断&gt;0
    &lt;困难&gt;0&lt;/困难&gt;
    &lt;Robndbox&gt;
        &lt;cx&gt;1299.0&lt;/cx&gt;
        &lt;cy&gt;375.5&lt;/cy&gt;
        &lt;w&gt;123.0&lt;/w&gt;
        &lt;h&gt;106.0&lt;/h&gt;
        &lt;角度&gt;1.570796
    &lt;/robndbox&gt;
    &lt;额外/&gt;
&lt;/对象&gt;
&lt;对象&gt;
    &lt;名称&gt;可调电阻
    &lt;姿势&gt;未指定&lt;/姿势&gt;
    &lt;截断&gt;0
    &lt;困难&gt;0&lt;/困难&gt;
    &lt;Robndbox&gt;
        &lt;cx&gt;658.0&lt;/cx&gt;
        &lt;cy&gt;479.0&lt;/cy&gt;
        &lt;w&gt;122.0&lt;/w&gt;
        &lt;h&gt;96.0&lt;/h&gt;
        &lt;角度&gt;1.55226
    &lt;/robndbox&gt;
    &lt;额外/&gt;
&lt;/对象&gt;

我该如何解决这个问题？预先感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78258685/problem-with-converting-a-dataset-from-labelimg-xml-to-yolo-txt-format-inversio</guid>
      <pubDate>Tue, 02 Apr 2024 02:37:25 GMT</pubDate>
    </item>
    <item>
      <title>令人惊讶但令人困惑的机器学习结果</title>
      <link>https://stackoverflow.com/questions/78258533/surprising-but-confusing-ml-results</link>
      <description><![CDATA[有人可以帮我理解以下问题吗？
我正在使用监督机器学习技术进行异常检测。我对结果感到惊讶和厌倦，对于一个数据集来说，它非常出色，而对于其他数据集来说，这些结果更糟。
问题是，“假阳性率为 100%，同时真阳性率为 99%。”据我所知，这种情况不应该发生，否则情况应该是这样，即如果 TPR 为 100%，则 FPR 应该为 0%。你怎么说？获得这样的结果的可能原因是什么？
为了更好地理解，请附上屏幕截图。
我尝试针对不同的数据集使用 0-4 范围内的不同值来调整我的模型。该模型在一种情况下显示出令人鼓舞的结果，但在另一种数据集上则显示出令人沮丧的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78258533/surprising-but-confusing-ml-results</guid>
      <pubDate>Tue, 02 Apr 2024 01:26:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 pydotplus 可视化决策树</title>
      <link>https://stackoverflow.com/questions/78257646/visualization-a-decision-trees-with-pydotplus</link>
      <description><![CDATA[在决策树项目中
我写了这段代码，但是出现了具体的错误，我咨询了AI，问题没有解决，你可以看看问题出在哪里。
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从sklearn导入预处理
df = pd.read_csv(&#39;E:\mostafa文件夹\mashin学习\S5recommendersystems\Files\drug200.csv&#39;)
df.head()
df.describe()
df.列
x = df[[&#39;年龄&#39;, &#39;性别&#39;, &#39;血压&#39;, &#39;胆固醇&#39;, &#39;Na_to_K&#39;]].values
X
y = df[[&#39;药物&#39;]].值
y
从sklearn导入预处理
le_sex = 预处理.LabelEncoder()
le_sex.fit([&#39;F&#39;,&#39;M&#39;])
x[:,1] = le_sex.transform(x[:,1])


le_BP = 预处理.LabelEncoder()
le_BP.fit([ &#39;低&#39;, &#39;正常&#39;, &#39;高&#39;])
x[:,2] = le_BP.transform(x[:,2])


le_Chol = 预处理.LabelEncoder()
le_Chol.fit([ &#39;正常&#39;, &#39;高&#39;])
x[:,3] = le_Chol.transform(x[:,3])

x[0:4]
从 sklearn.model_selection 导入 train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)

print(&#39;火车形状：&#39;,x_train.shape,y_train.shape)
print(&#39;测试形状：&#39;,x_test.shape,y_test.shape)
从 sklearn.tree 导入 DecisionTreeClassifier

树= DecisionTreeClassifier（标准=“熵”，max_深度= 6）

树.fit(x_train,y_train)
pretree = tree.predict(x_test)

打印（前树[0:5]）
打印（y_测试[0:5]）
从 sklearn 导入指标
print(&quot;DecisionTrees 的准确率：&quot;,metrics.accuracy_score(y_test, pretree))
从 io 导入 StringIO
导入 pydotplus
将 matplotlib.image 导入为 mpimg
从 sklearn.tree 导入 DecisionTreeClassifier
从sklearn.tree导入export_graphviz
%matplotlib 内联
点数据 = StringIO()
特征名称 = df.columns
文件名 = &#39;mosi.png&#39;

out = export_graphviz(树,feature_names = featuresnames)
从sklearn.tree导入export_graphviz

点数据 = StringIO()
文件名 = &#39;ee.png&#39;
#featurenames = df.columns[0:5]

out = export_graphviz(tree, feature_names=df.columns[0:5], out_file=dot_data, class_names= np.unique(y_train),filled=True,special_characters=True,rotate=False)

图 = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png(文件名)
img = mpimg.imread(文件名)
plt.figure(figsize=(100, 200))
plt.imshow(img, 插值=&#39;最近&#39;)


这个问题取自这一行
out = export_graphviz(tree, feature_names=df.columns[0:5], out_file=dot_data, class_names= np.unique(y_train),filled=True,special_characters=True,rotate=False)
&lt;来自“C:\\Users\\PARDAZESHGARA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn”的模块“sklearn.tree” \\tree\\__init__.py&#39;&gt;不是估计器实例。

大家帮帮我！！！]]></description>
      <guid>https://stackoverflow.com/questions/78257646/visualization-a-decision-trees-with-pydotplus</guid>
      <pubDate>Mon, 01 Apr 2024 20:27:09 GMT</pubDate>
    </item>
    <item>
      <title>无法解释指标标识符 - scikeras.wrappers.KerasRegressor</title>
      <link>https://stackoverflow.com/questions/78257033/metric-identfier-cannot-be-interpreted-scikeras-wrappers-kerasregressor</link>
      <description><![CDATA[我正在尝试使用 scikeras.wrappers.KerasRegressor 调整超参数，但遇到了如下问题：
代码：
 # 定义一个函数来创建 lstm_model 的实例
def create_lstm_model():
    
    模型=顺序（[
            LSTM(5, input_shape = (Xtrain.shape[1], Xtrain.shape[2]), dropout = 0.1, 激活 = &#39;tanh&#39;, return_sequences = True),
            LSTM(10, dropout = 0.05, 激活 = &#39;tanh&#39;),
            密集（5，激活=&#39;relu&#39;），
            密集(1)
        ]）
    model.compile(优化器 = tf.keras.optimizers.Adam(), 损失 = tf.keras.losses.MeanSquaredError(), 指标 = [keras.metrics.MeanSquaredError()])
    
    返回模型

#为网络创建sklearn模型
模型 = KerasRegressor(build_fn = create_lstm_model, verbose = 1)

#参数网格
批次 = [16, 32]
历元 = [3, 4]

param_grid = dict(batch_size = 批次, epochs = epochs)

网格 = GridSearchCV(估计器 = 模型,
                    参数网格 = 参数网​​格,
                    简历 = 3)
grid.fit（Xtrain，ytrain，validation_data =（Xvalidation，yvalidation））

错误：
&lt;前&gt;&lt;代码&gt; fn_or_cls = keras_metric_get(公制)
  文件“/home/aaa/Desktop/aaa/aaa/2024-gold-price-prediction-with-lstm-model/.venv/lib/python3.10/site-packages/keras/src/metrics/__init__.py” ;，第 204 行，在 get 中
    raise ValueError(f“无法解释指标标识符：{identifier}”)
ValueError：无法解释指标标识符：损失

我从更复杂的代码开始，并在故障排除期间将其简化到最低限度。我什至试图从模型函数中删除指标。
您对我的代码有什么问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78257033/metric-identfier-cannot-be-interpreted-scikeras-wrappers-kerasregressor</guid>
      <pubDate>Mon, 01 Apr 2024 18:02:32 GMT</pubDate>
    </item>
    <item>
      <title>视觉变压器：运行时错误：mat1和mat2形状不能相乘（32x1000和768x32）[关闭]</title>
      <link>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</link>
      <description><![CDATA[我正在尝试对视觉变换器模型进行回归，但无法用回归层替换最后一层分类
类RegressionViT(nn.Module)：
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(预训练=预训练)
        # 从 vit_b_16 访问实际输出特征尺寸
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def 前向（自身，x）：
        x = self.vit_b_16(x)
        x = self.regressor(x)
        返回x


＃ 模型
模型 = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型.to（设备）

criteria = nn.MSELoss() # 使用适当的损失函数进行回归
优化器 = optim.Adam(model.parameters(), lr=0.0001)


当我尝试初始化模型时收到此错误
运行时错误：mat1 和 mat2 形状无法相乘（32x1000 和 768x32）

问题是回归层和vit_b_16模型层之间不匹配，解决此问题的正确方法是什么]]></description>
      <guid>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</guid>
      <pubDate>Mon, 01 Apr 2024 06:33:32 GMT</pubDate>
    </item>
    <item>
      <title>python中的批量梯度下降算法返回巨大的值</title>
      <link>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</link>
      <description><![CDATA[我正在尝试在 python 中实现批量梯度下降算法，该算法将训练集、学习率和迭代次数作为输入参数，并返回权重。然而，当我运行它时，在几次迭代内，参数的值呈指数级增长，最终返回“nan”。
&lt;前&gt;&lt;代码&gt;x = [[2104] [1600] [2400] [1416] [3000] [1985] [1534] [1427] [1380] [1494] [1940] [2000] [1890] [4478 ] [1268] [2300] [1320] [1236] [2609] [3031] [1767] [1888] [1604] [1962] [3890] [1100] [1458] [2526] [2200] [2637] [ 1839][1000][2040][3137][1811][1437][1239][2132][4215][2162][1664][2238][2567][1200][852][1852][1203] ]

y = [399900 329900 369000 232000 539900 299900 314900 198999 212000 242500 239999 347000 329999 699900 259900 449900 299900 1 99900 499998 599000 252900 255000 242900 259900 573900 249900 464500 469000 475000 299900 349900 169900 314900 579900 285900 249900 229900 345000 549000 287000 368500 329900 314000 299000 179900 299900 239500 ]

a = 0.01

迭代次数 = 100

def BGD ( x, y, a, num_iter):
    m = len(x) #样本数
    n = x.shape[1] #特征数量
    p = np.zeros(n)
    b = 0
    对于 _ 在范围内（num_iter）：
        sum_p = np.zeros(n)
        总和 = 0
        对于范围 (m) 内的 i：
            sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
            sum_b = sum_b + (((np.dot(p,x[i])+b) - y[i]))
        p = p - (a * (1/m) * sum_p)
        b = b - (a * (1/m) * sum_b)
    返回 p、b

p, b = BGD(x, y, 0.01, 100)
打印（页）
打印(b)

我得到以下信息：
RuntimeWarning: add 中遇到溢出
  sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
RuntimeWarning：减法中遇到无效值
  p = p - (a * (1/m) * sum_p)
[南]
南
]]></description>
      <guid>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</guid>
      <pubDate>Sat, 30 Mar 2024 14:00:37 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 自定义和默认目标和评估函数</title>
      <link>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</link>
      <description><![CDATA[我正在训练 BDT 以进行信号/背景的二元分类（我从事粒子物理学工作）。我的模型（用 python 实现）如下所示：
导入 xgboost 为 xgb
train = xgb.DMatrix(data=train_df[特征],label=train_df[“标签”],
                    缺失=“inf”，feature_names=特征，权重=(np.array(train_df[&#39;label&#39;].array)*-0.99+1))
测试= xgb.DMatrix(数据=test_df[特征],标签=test_df[“标签”],
                   缺失=“inf”，feature_names=特征，权重=(np.array(test_df[&#39;label&#39;].array) *-0.99+1))

参数 = {}


# 助推器参数
param[&#39;eta&#39;] = 0.1 # 学习率
param[&#39;max_depth&#39;] = 10 # 树的最大深度
param[&#39;subsample&#39;] = 0.5 # 训练树的事件分数
param[&#39;colsample_bytree&#39;] = 0.5 # 训练树的特征分数

# 学习任务参数
param[&#39;objective&#39;] = &#39;binary:logistic&#39; # 目标函数
param[&#39;eval_metric&#39;] = &#39;error&#39; # 交叉验证的评估指标
param = list(param.items()) + [(&#39;eval_metric&#39;, &#39;logloss&#39;)] + [(&#39;eval_metric&#39;, &#39;rmse&#39;)]


num_trees = 50 # 要制作的树的数量
booster = xgb.train(参数,train,num_boost_round=num_trees)

该模型表现良好，但我想稍微修改一下成本/损失函数，以便误报比真报受到更多惩罚。我正在寻找背景事件尽可能少的选择，即使这意味着牺牲信号的很大一部分。
根据自定义目标和评估函数的文档，我成功添加教程案例。
哪些是标准目标和评估函数（或者我可能已经在模型中实现的函数）？
由于模型已经表现良好，我只想为已经实现的功能添加一个额外的术语。
作为参考，我正在使用的软件包版本：
python 版本：3.10.12（主要，2023 年 11 月 20 日，15:14:05）[GCC 11.4.0]
XGBoost版本：2.0.2
熊猫版本：2.1.4
Numpy 版本：1.26.2
]]></description>
      <guid>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</guid>
      <pubDate>Tue, 05 Mar 2024 18:53:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PyTorch Lightning 模块不保存记录的 val 损失？模型检查点错误</title>
      <link>https://stackoverflow.com/questions/75791020/why-doesnt-pytorch-lightning-module-save-logged-val-loss-modelcheckpoint-error</link>
      <description><![CDATA[我正在 Kaggle 上运行基于 LSTM 的模型训练。我为此使用 Pytorch Lightning 和 wandb 记录器。
这是我模型的类：
类模型（pl.LightningModule）：
    def __init__(
        自己，
        输入大小：整数，
        隐藏大小：int，
        双向：bool = False，
        lstm_layers：int = 1，
        lstm_dropout：浮动= 0.4，
        fc_dropout：浮动= 0.4，
        lr：浮动= 0.01，
        lr_scheduler_patience：int = 2，
    ）：
        超级().__init__()
        self.lr = lr
        self.save_hyperparameters()

        # LSTM
        self.encoder_lstm = nn.LSTM(
            输入大小=输入大小，
            隐藏大小=隐藏大小，
            num_layers=lstm_layers,
            双向=双向，
            dropout=lstm_dropout 如果 lstm_layers &gt; 1 否则 0,
            批处理第一=真，
        ）

        # 全连接
        num_directions = 2 如果是双向则为 1
        self.fc = nn.Sequential(
            nn.线性(
                隐藏大小 * 数字方向，隐藏大小 * 数字方向 * 2
            ),
            ReLU(),
            nn.Dropout(fc_dropout),
            nn.Linear(hidden_​​size * num_directions * 2, input_size),
        ）

        self.loss_function = nn.MSELoss()

    def 配置_优化器（自身）：
        优化器 = torch.optim.AdamW(self.parameters(), lr=self.lr)
        返回 {
            “优化器”：优化器，
            “lr_scheduler”：{
                “调度程序”：torch.optim.lr_scheduler.ReduceLROnPlateau(
                    优化器，patience=self.hparams.lr_scheduler_patience
                ),
                “监视器”：“val_loss”，
            },
        }

    defforward(self, x, prev_state):
        ...

    def Training_step（自身，批次，batch_idx）：
        损失，_ = self._step(batch)

        self.log(“train_loss”, 损失)
        回波损耗

    defvalidation_step(self,batch,batch_idx):
        损失，嵌入= self._step(batch)

        self.log(“val_loss”, 损失)
        
        返回 {
            &#39;val_loss&#39;：损失，
            &#39;preds&#39;: embeddings # 这由我的自定义回调消耗
        }

    def test_step（自身，批次，batch_idx）：
        损失，_ = self._step(batch)

        self.log(“test_loss”, 损失)

这就是我使用它的方式：
&lt;前&gt;&lt;代码&gt;模型 = 模型(
    双向=假，
    lstm_layers=1,
    lstm_dropout=0.4，
    fc_dropout=0.4,
    lr=0.01，
    lr_scheduler_patience=2
）

...

checkpoint_callback = ModelCheckpoint(
    监视器=“val_loss”，
    every_n_train_steps=100，
    详细=真
）

培训师 = pl.Trainer(
    加速器=&#39;gpu&#39;,
    精度=16，
    最大纪元=100，
    回调=[early_stopping、checkpoint_callback、lr_monitor、custom_callback]、
    log_every_n_steps=50,
    记录器=wandb_logger，
    auto_lr_find=真，
）

trainer.tune(模型, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)

trainer.fit(模型, train_dataloader, val_dataloader)

当我不运行 trainer.tune(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader) trainer.fit 时效果很好，但是当我运行 &#39;trainer.tune &#39; 我收到这样的 ModelCheckpoint 错误：
MisconfigurationException：`ModelCheckpoint(monitor=&#39;val_loss&#39;)`无法在返回的指标中找到受监控的键：[&#39;train_loss&#39;, &#39;epoch&#39;, &#39;step&#39;]。提示：您是否在“LightningModule”中调用了“log(&#39;val_loss&#39;, value)”？


因此，即使我记录了 val_loss 它也不会被保存。在 Trainer 对象上，我设置了 log_every_n_steps=50 ，在 ModelCheckpoint 上，我设置了 every_n_train_steps=100 ，因此看起来在 ModelCheckpoint 启动时应该记录“val_loss”。 
我在 validation_step 中打印了 val loss，它是在 ModelCheckpoint 运行之前计算的。我还在自定义回调中定义了一个 on_train_batch_end 函数来查看保存的训练器指标。事实证明，val 损失实际上缺失了。]]></description>
      <guid>https://stackoverflow.com/questions/75791020/why-doesnt-pytorch-lightning-module-save-logged-val-loss-modelcheckpoint-error</guid>
      <pubDate>Mon, 20 Mar 2023 13:43:02 GMT</pubDate>
    </item>
    <item>
      <title>朴素贝叶斯分类准确率为 100%</title>
      <link>https://stackoverflow.com/questions/72619634/accuracy-in-naive-bayes-classification-is-100</link>
      <description><![CDATA[我有一个分类问题，我想对 A、B 和 C 类进行分类。我尝试使用朴素贝叶斯分类器，准确率是 100%，我真的怀疑这不是真的。我有大约350个的小数据集，其中A类是140个，B类是140个，其余的是C类。这是我使用的代码。有人可以给我一些建议吗？
导入sklearn
从 sklearn.metrics 导入 precision_score
X = feature_data_frame.values
y = 标签数据
导入 sklearn.preprocessing 作为预处理
从 sklearn.model_selection 导入 train_test_split
从 sklearn.naive_bayes 导入 GaussianNB、MultinomialNB
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.10)
gnb = GaussianNB()
y_pred = gnb.fit(x_train, y_train).predict(x_test)
准确度=准确度_得分(y_test, y_pred)

提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/72619634/accuracy-in-naive-bayes-classification-is-100</guid>
      <pubDate>Tue, 14 Jun 2022 15:27:33 GMT</pubDate>
    </item>
    </channel>
</rss>