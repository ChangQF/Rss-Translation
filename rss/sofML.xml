<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 11 Jul 2024 18:19:58 GMT</lastBuildDate>
    <item>
      <title>Keras 调谐器引发 RuntimeError</title>
      <link>https://stackoverflow.com/questions/78737115/keras-tuner-raising-runtimeerror</link>
      <description><![CDATA[每当我想使用 keras tuner 调整超参数时，它都会引发异常，提示 RuntimeError：连续失败次数超过 3 的限制。有人能帮我解决这个问题吗？
我尝试使用 keras_tuner 调整神经网络中的超参数。我的期望是获得超参数值的最佳组合。但是当我想运行 keras_tuner 超参数对象的 search 方法时，会发生 RuntimeError。]]></description>
      <guid>https://stackoverflow.com/questions/78737115/keras-tuner-raising-runtimeerror</guid>
      <pubDate>Thu, 11 Jul 2024 18:09:25 GMT</pubDate>
    </item>
    <item>
      <title>哪种模型架构最适合解决我的问题？</title>
      <link>https://stackoverflow.com/questions/78736820/what-model-architecture-would-suite-my-problem-the-best</link>
      <description><![CDATA[我正在寻找关于哪种模型架构最适合解决以下问题的建议。
我有一些复杂的动画，每个动画都由 70 个随时间变化的变量描述。它们的长度可以从一分钟到很多分钟不等。如果需要，我可以以 1 到 60 之间的任何 fps 对它们进行采样以获取训练数据。
我想尝试在其中一些上训练模型，以查看是否可以根据较短的输入动画序列生成类似的模型。
我研究过用于多变量时间序列预测的模型，例如 huggingface 上的 autoformer 和 informer。但我的训练数据不是每年/每月/季节性的，所以我不确定这些复杂模型是否最合适。如果我理解正确的话，它们的输出也是一个分布，我可能不需要它。如果可能的话，我想从一个简单的开始。]]></description>
      <guid>https://stackoverflow.com/questions/78736820/what-model-architecture-would-suite-my-problem-the-best</guid>
      <pubDate>Thu, 11 Jul 2024 16:57:53 GMT</pubDate>
    </item>
    <item>
      <title>无法检测/删除图像数据集中两个位置不同的水印</title>
      <link>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</link>
      <description><![CDATA[我在从一组图片中删除水印时遇到了问题。这些水印彼此靠近，但又有所不同（见下文）。其中一个水印是红色方块，里面有白色文字。另一个是半透明的灰色句子。目的是处理图像以用于机器学习。
图像
解决问题的尝试：
由于水印在图像数据集中的位置各不相同，我尝试了以下操作：

复制图像并将其转换为 HSV 颜色空间
为感兴趣的区域选择一系列下限值和上限值（在分割图像并为每个通道构建直方图后选择这些值）
使用 cv2.inRange() 函数构建蒙版
使用蒙版在原始图像中修复水印

对于红色方块，前三个步骤完美无缺。但第三步只是有点奏效。水印比以前明显少了，但仍然很明显。对于文本，我无法在第 3 步中获得足够好的蒙版 - 它的颜色/像素强度与周围区域和文本本身太接近了。
这看起来更像是一个机器学习问题，这很好，但我想事先用尽其他选择。关于如何使用机器学习或算法方法解决这个问题有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</guid>
      <pubDate>Thu, 11 Jul 2024 16:54:13 GMT</pubDate>
    </item>
    <item>
      <title>在交互式笔记本中加载 Azure ML Studio 中已注册的模型</title>
      <link>https://stackoverflow.com/questions/78736775/load-a-registered-model-in-azure-ml-studio-in-an-interactive-notebook</link>
      <description><![CDATA[我正在使用 Azure 机器学习工作室，我的默认数据存储（blob 存储）中存储了一个 sklearn mlflow 模型，然后我将其注册为模型资产。在将其部署为批处理端点之前，如何将此模型加载到交互式笔记本中以执行一些快速模型推理和测试。
我看到了一篇链接为此处的帖子，建议在本地下载模型工件，但我不需要这样做。我应该能够直接从数据存储或注册的资产加载模型，而无需在多个位置复制模型。我尝试了以下操作，但没有成功。
从已注册的模型资产读取
import mlflow
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Model

ml_client = MLClient(DefaultAzureCredential(), &quot;&lt;subscription_id&gt;&quot;, &quot;&lt;resource_group&gt;&quot;, &quot;&lt;workspace_id&gt;&quot;)

model = ml_client.models.get(&quot;&lt;model_name&gt;&quot;, version=&quot;1&quot;)
loaded_model = mlflow.sklearn.load_model(model.id)

&gt;&gt;&gt; OSError：没有这样的文件或目录：...

从数据存储中读取
import mlflow

model_path = &quot;&lt;datastore_uri_to_model_folder&gt;&quot;
loaded_model = mlflow.sklearn.load_model(model_path)

&gt;&gt;&gt; DeserializationError：无法反序列化内容类型：text/html
]]></description>
      <guid>https://stackoverflow.com/questions/78736775/load-a-registered-model-in-azure-ml-studio-in-an-interactive-notebook</guid>
      <pubDate>Thu, 11 Jul 2024 16:45:51 GMT</pubDate>
    </item>
    <item>
      <title>如何针对简单的 ML 模型对来自 EE 的卫星数据进行标准化/预处理？</title>
      <link>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</link>
      <description><![CDATA[我对 Earth Engine/QGIS 还不太熟悉（没有 ArcGIS 许可证），我想使用简单的 ML 模型，利用卫星 VCD、NDVI 和气象数据估算地面 O3。
我对 GIS/地理空间数据处理领域非常迷茫，所以我尽我所能，疯狂地谷歌搜索并阅读了一些文章，以解释我的理由。
我想使用的数据如下：
EE 数据集：

Daymet V4 每日气候变量（https://developers.google.com/earth-engine/datasets/catalog/NASA_ORNL_DAYMET_V4#bands)
MOD13A2 NDVI 产品 (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A2)
Sentinel-5P (TROPOMI) O3 VCD 数据 (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_O3#bands)

标签：- 来自 EPA 的地面 O3 数据（https://epa.maps.arcgis.com/apps/webappviewer/index.html?id=5f239fd3e72f424f98ef3d5def547eb5&amp;extent=-146.2334,13.1913,-46.3896,56.5319），转到右上角的“选择图层”图标并选择 O3 活动/非活动，然后将鼠标悬停在任意点上

似乎这个 EPA 数据可以直接导出为 CSV，包括经度、纬度和臭氧测量值。

假设——我需要从不同来源获取的数据具有相同的空间/时间分辨率，以便使用一些简单的机器学习算法（RF/线性回归）。
在我设想的数据集中，每“行”数据将是给定像素在给定日期的气象变量值、NDVI 和 VCD 值，我可以对其执行基本的 RF/回归（使用 EE 或 Python）。在我看来，要使它发挥作用，所有数据集都需要就“像素”是什么达成一致，并且成为/成为每日时间分辨率（Daymet 和 TROPOMI 已经是每日的，我假设我可以取最接近的 16 天 NDVI 值）。
基于这个假设，我想让所有数据都具有相同的空间分辨率，因此我正在尝试弄清楚如何“重新投影” TROPOMI 数据（当前分辨率为 1111.3km）转换为 1km 分辨率（据我所知，这是所有 Daymet 数据和 MODIS 数据的分辨率）。 *我不知道如何使来自 EPA 数据的“最近像素”（来自点数据而非栅格数据）匹配，以便将其用作数据标签，但这似乎是一个更常见的问题，因此在整理完其余部分后，我将四处寻找如何修复该问题。
因此，我想在这里完成的主要操作是标准化空间分辨率：将 TROPOMI 数据转换为 1km 像素或将 Daymet/MODIS 数据转换为 1.113km 像素。
我尝试在 Earth Engine 中可视化所有三个输入数据集（为 Daymet 选择最高温度），像素似乎根本没有对齐。我已在此处附上每个图层的屏幕截图：ndvi 像素、daymet 像素和 tropomi 像素
TROPOMI 数据似乎给出了某种奇怪的模糊像素，Daymet 数据是规则的方形像素但倾斜，而 NDVI 数据是平行四边形。我隐约觉得这与不同的“投影”/“CRS”有关设置，但我对这两者都不太了解，并且不确定如何继续我认为我需要做的重新缩放。
脚本链接：https://code.earthengine.google.com/6fafaccf040e206e97a32e795611d7e4]]></description>
      <guid>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</guid>
      <pubDate>Thu, 11 Jul 2024 16:43:51 GMT</pubDate>
    </item>
    <item>
      <title>未识别的 TensorFlow 回溯导致 ResourceExhaustedError</title>
      <link>https://stackoverflow.com/questions/78736732/unidentified-tensorflow-retracing-leading-to-resourceexhaustederror</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78736732/unidentified-tensorflow-retracing-leading-to-resourceexhaustederror</guid>
      <pubDate>Thu, 11 Jul 2024 16:34:38 GMT</pubDate>
    </item>
    <item>
      <title>随机森林如何处理缺失值？</title>
      <link>https://stackoverflow.com/questions/78736714/how-random-forest-handle-missing-value</link>
      <description><![CDATA[scikit-learn 中的 随机森林回归器 使用什么技术来处理缺失值？
首先，我认为随机森林回归器能够在训练和生产过程中原生处理缺失值，而不会损失任何准确性（sklearn doc、sklearn changelog 1.4、堆栈问题）。因此，我使用 scikit-learn 中的 RandomForestResgressor 处理具有大量缺失值的数据（见下图）。
它正在工作，但现在，经过一些研究，我发现随机森林模型本身并不能处理缺失值，但库使用了各种技术，例如 imputation/knn/Miss Forest/等。处理缺失值。
我想知道 scikit-learn 在其 RandomForestRegressor 实现中如何处理缺失值，以了解是否有大量缺失值是可以的，并且不会降低准确性，或者是否最好在上游清理大部分缺失值（删除具有大量缺失值的观察值和特征）。

我的示例：
每列都是一个特征，每行都是一个观察值。例如，我们看到右侧的 4 个特征仅出现在少数观察值中。根据随机森林处理缺失值的方式，在训练模型之前删除这些特征可能是更好的选择。
]]></description>
      <guid>https://stackoverflow.com/questions/78736714/how-random-forest-handle-missing-value</guid>
      <pubDate>Thu, 11 Jul 2024 16:29:49 GMT</pubDate>
    </item>
    <item>
      <title>如何运行我们的研究机构解析器</title>
      <link>https://stackoverflow.com/questions/78736452/how-to-run-ourresearch-institution-parser</link>
      <description><![CDATA[我尝试在本地运行 ourresearch 的 ML 模型 https://github.com/ourresearch/openalex-institution-parsing/tree/main/V2 来处理从属关系字符串。我已经从 aws 下载了他们所说的必需文件。
我花了很多时间试图弄清楚，但我一点也不聪明。我期待某种入口点，但我似乎找不到，或者我如何才能运行它。]]></description>
      <guid>https://stackoverflow.com/questions/78736452/how-to-run-ourresearch-institution-parser</guid>
      <pubDate>Thu, 11 Jul 2024 15:29:44 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习 Python 预测价格</title>
      <link>https://stackoverflow.com/questions/78736432/predict-a-price-with-machine-learning-python</link>
      <description><![CDATA[我正在开始机器学习。我想估算某篇文章在某个日期的价格。例如，如果我知道 07/01/24 和 07/11/24 之间的价格，那么 07/12/24 的价格是多少？
我尝试使用线性或多项式回归、RandomForestRegresor 和 GradientBoostingRegresor。这些模型对提供的数据返回非常准确的结果，但当我尝试在此范围之外使用它时，结果要么与最后一个值相同，要么是不可能得到的结果（有时我会得到负价格）。
有人知道怎么做吗？我应该使用其他方法吗？
例如，我尝试对 1992 年 1 月至 2024 年 5 月之间的汽油价格进行 GradientBoosting：
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor

data = pd.read_csv(&quot;essence2.csv&quot;, sep=&quot;;&quot;).iloc[::-1]

x = data[&quot;date&quot;].values.reshape(-1, 1)
y = data[&quot;price&quot;].values

x_train, x_test, y_train, y_test = train_test_split(
x,
y,
test_size=0.01,
random_state=2,
)

model = GradientBoostingRegressor(
n_estimators=200,
learning_rate=1,
random_state=2,
)
model.fit(x_train, y_train)

y_predictions = np.round(model.predict(x), 4)
residues = y_predictions - y

fig, ax = plt.subplots(2)

ax[0].plot(x, y, color=&quot;blue&quot;, label=&quot;Prix réel&quot;)
ax[0].plot(x, y_predictions, color=&quot;orange&quot;, linestyle=&quot;--&quot;, label=&quot;Gradient Boosting Regressor&quot;)
ax[0].xaxis.set_major_locator(MaxNLocator(8))
ax[0].tick_params(axis=&quot;x&quot;, rotation=45)
ax[0].legend()

ax[1].plot(x, residuals, color=&quot;orange&quot;, label=&quot;Résidues&quot;)
ax[1].xaxis.set_major_locator(MaxNLocator(8))
ax[1].tick_params(axis=&quot;x&quot;, rotation=45)
ax[1].legend()

plt.tight_layout()
plt.show()


精华预测
如何预测价格2024 年 6 月？]]></description>
      <guid>https://stackoverflow.com/questions/78736432/predict-a-price-with-machine-learning-python</guid>
      <pubDate>Thu, 11 Jul 2024 15:25:21 GMT</pubDate>
    </item>
    <item>
      <title>分割任何模型（SAM）如何使用多个框及其对应点来预测torch？</title>
      <link>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</link>
      <description><![CDATA[我目前正在尝试 Segment Anything 模型 (SAM)，我的问题需要多个框及其对应的点，以便在框内明确。例如，box1 = [#, #, # ,#]，其点为 [x,y]，类为 [0 或 1]，然后在单个图像中包含多个这样的点。
我仅使用多个边界框就可以做到这一点，但我想在每个框中包含点。
这是我当前的代码，我不再确定它为什么会给我一个错误：
RuntimeError：除了维度 1 之外，张量的大小必须匹配。预期大小为 1，但列表中的张量编号 1 的大小为 3。

import numpy as np
import torch
import matplotlib.pyplot as plt

point = np.array([[330, 370]])
label = np.array([1])

input_point = torch.tensor(point, device=predictor.device)
input_point = input_point.unsqueeze(0)
transformed_point = predictor.transform.apply_coords_torch(input_point, image.shape[:2])

input_label = torch.tensor(label, device=predictor.device)
input_label = input_label.unsqueeze(0)

#yxyx-xyxy
filtered_rois_xyxy = transform_yxyx_to_xyxy(filtered_rois)
input_boxes = torch.tensor(filtered_rois_xyxy, device=predictor.device)
transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2]) 

masks,_,_ = predictor.predict_torch(
boxes=transformed_boxes,
point_coords=transformed_point,
point_labels=input_label,
multimask_output=False
)

masks.shape

plt.figure(figsize=(10, 10))
plt.imshow(image)
for mask in mask:
show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
for box in input_boxes:
show_box(box.cpu().numpy(), plt.gca())
plt.axis(&#39;off&#39;)
plt.show()

为了调试目的，这是每个输入的打印：

print(input_boxes)
print(input_point)
print(input_label)

tensor([[330, 370, 495, 634],
[401, 168, 586, 425],
[ 1, 0, 157, 210]], dtype=torch.int32)
tensor([[[330, 370]]])
张量([[1]])
]]></description>
      <guid>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</guid>
      <pubDate>Thu, 11 Jul 2024 14:49:27 GMT</pubDate>
    </item>
    <item>
      <title>Flask 后端的部署</title>
      <link>https://stackoverflow.com/questions/78735527/deployment-of-flask-backend</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78735527/deployment-of-flask-backend</guid>
      <pubDate>Thu, 11 Jul 2024 12:29:34 GMT</pubDate>
    </item>
    <item>
      <title>x86 mulss 结果随时间而不同[关闭]</title>
      <link>https://stackoverflow.com/questions/78735265/x86-mulss-result-is-diffrent-over-time</link>
      <description><![CDATA[我正在尝试制作 C++ 神经网络库，我注意到，有时具有相同输入和相同参数值的模型会毫无原因地大幅改变输出。我不知道为什么会发生这种情况，我确信我没有犯任何错误。看起来我的 fpu 已经耗尽并损失了一点精度，这导致较大网络（例如 10m 个参数）中的输出非常不同。
编辑：
经过测试，我发现使用 -O3 标志会导致错误的二进制文件，而 -Oz 会导致有效的二进制文件。有什么想法吗？它仍然使用 mulss...
编辑 2：经过进一步测试，-Oz 只会延迟一段时间的缺陷。
MRE？我无法提供 MRE，因为简单的 10 ^ 7 乘法 100 次返回结果 0 的总差异增量...
我只能提供整个库的“MRE”。如果有错误，我会查看
network.cpp -&gt;

void nn::network::eval::forward(evaluator *evaluator)
void forward_layer(int l1_size, int l2_size, float* l1_values, float* l2_values, float* weights, float* biases, nn::network::modules::activation::activation_func func)
void nn::network::connect(network *network, float dist_max, float dist_min)

我创建为 MRE 的 Repo（我知道它不像想要的那么小）：https://github.com/ZDibLO/libnn
示例：
输入：-0.729, 0.670, 0.937, -0.557, -0.383, 0.094, -0.623, 0.985, 0.992, 0.935
通过：0
输出：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
...
通过：4
输出：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
通过：5
出局：-0.494, -0.852, 0.961, 0.747, 0.826, 0.685, -0.495, 0.662, 0.810, -0.453
...
通过：8
出局：-0.494, -0.852, 0.961, 0.747, 0.826, 0.685, -0.495, 0.662, 0.810, -0.453
通过：9
出局：-0.521, -0.964, 0.973, 0.012, 0.991, 0.818, 0.752, 0.896, 0.995, 0.779
...
通过：16
出局：-0.521, -0.964, 0.973, 0.012, 0.991, 0.818, 0.752, 0.896, 0.995, 0.779
通过：17
出局：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
...
通过：99
出局：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901，-0.981
delta：22.8341
总计：3786ms
平均值：37ms
网络：params：10091120（10.0m）权重：9991100，偏差：100020

Delta：与上次输出相比，总输出变化
请帮忙，因为我无法继续处理未定义的行为。
CPU：i7 8550u / 操作系统：linux | Arch Linux
我将很快在两台使用相同型号的不同 Windows 机器上进行测试。
我尝试了不同的模型，调整了 sse 标志（之前它返回 NaN），并且我在寻找其他错误，但找不到任何错误。]]></description>
      <guid>https://stackoverflow.com/questions/78735265/x86-mulss-result-is-diffrent-over-time</guid>
      <pubDate>Thu, 11 Jul 2024 11:31:13 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Linux 远程服务器中运行计算机视觉 Github 存储库 [关闭]</title>
      <link>https://stackoverflow.com/questions/78733479/how-to-run-computer-vision-github-repositories-in-linux-remote-server</link>
      <description><![CDATA[我是 ml/dl 的新学习者。我想在 Linux 远程服务器上运行计算机视觉 github 存储库。请帮帮我。我不知道如何运行任何计算机视觉 github 存储库。在我 gitclone 它之后，我该如何运行它。此外，我必须在大多数 github 存储库中手动下载数据集。如果数据集太大（例如 80 GB），我该如何在远程服务器中下载它。请帮帮我。
大家好，我是 ml/dl 的新学习者。我想在 Linux 远程服务器上运行计算机视觉 github 存储库。请帮帮我。我不知道如何运行任何计算机视觉 github 存储库。在我 gitclone 它之后，我该如何运行它。此外，我必须在大多数 github 存储库中手动下载数据集。如果数据集太大（例如 80 GB），我该如何在远程服务器中下载它。请帮帮我。如果有人能给我一个大纲，那将非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78733479/how-to-run-computer-vision-github-repositories-in-linux-remote-server</guid>
      <pubDate>Thu, 11 Jul 2024 03:52:26 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft ML：预测代码始终预测 0，但在“评估”选项卡中尝试时，它始终有效（非 0 的值）</title>
      <link>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</link>
      <description><![CDATA[系统信息：

模型构建器版本 17.15.0.2337001
Visual Studio 版本 2022

我创建了一个机器学习模型（值预测），一切都很好

通过这张图片，你可以看到模型正常工作
但是当我使用模型构建器的代码时：
使用 ConsoleApp2;

//加载样本数据
var sampleData = new MLModel1.ModelInput()
{
Device = 1F,
Temperature = 20F,
Weather = 1F,
Time = 2F,
};

//加载模型并预测输出
var result = MLModel1.Predict(sampleData);
Console.WriteLine(result.Predict);

输出始终为 0 或预测始终为 0：
]]></description>
      <guid>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</guid>
      <pubDate>Wed, 10 Jul 2024 14:28:45 GMT</pubDate>
    </item>
    </channel>
</rss>