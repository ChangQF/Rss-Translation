<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 24 Feb 2024 09:11:34 GMT</lastBuildDate>
    <item>
      <title>[D] 本科生的机器学习实习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/</link>
      <description><![CDATA[所以我目前是大学四年级学生，学习软件工程。期望获得 ML 实习生的职位是否现实？  对于那些能够做到这一点的人，您的投资组合中有哪些类型的项目？我已经学习了课程并创建了一些小项目，但到目前为止还没有什么重大项目，这些会帮助我被聘用吗？ 此外，如果有人正在寻找 ML/DS 实习生，我可以向您发送我的简历，哈哈。我真的很想踏入这门并开始积累经验。毕业后我将攻读硕士学位。   由   提交 /u/Critical-Strategy914    reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/</guid>
      <pubDate>Sat, 24 Feb 2024 08:36:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与变形金刚相比，曼巴的根本缺点是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</link>
      <description><![CDATA[你好！ 我思考这个问题有一段时间了。澄清一下，我并不是指“它尚未经过广泛测试”之类的方面。 “它的可扩展性是不确定的，”或“缺乏工业基础设施。”相反，我有兴趣了解 Transformer 和 Mamba 架构之间的核心差异，特别是这些差异如何使 Mamba 与 Transformers 相比处于劣势。 致以诚挚的问候！    由   提交 /u/Alarmed-Profile5736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</guid>
      <pubDate>Sat, 24 Feb 2024 07:11:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 数据集要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aynuye/r_dataset_requirements/</link>
      <description><![CDATA[需要帮助找到学术项目的 OMOP（观察医学结果合作伙伴关系）免费数据集。   由   提交/u/Ok-Art5200  /u/Ok-Art5200 reddit.com/r/MachineLearning/comments/1aynuye/r_dataset_requirements/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aynuye/r_dataset_requirements/</guid>
      <pubDate>Sat, 24 Feb 2024 06:34:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用基于视觉的 ML 工程进行仓库中的箱子检测（Yolov8、FastAPI、Docker）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayiwdw/p_box_detection_in_warehouse_using_vision_based/</link>
      <description><![CDATA[      我很高兴展示我最近开发的第一个用于图像对象检测的端到端 ML 工程项目，该项目利用 MLOps 原理来简化部署流程并确保可扩展性和可靠性。  该项目重点关注仓库传送带中的盒装容器检测。 以下是关键组件和方法的简要概述： 🔍 数据准备和模型训练 - 我首先从 Roboflow 下载数据集，并在使用迁移学习训练集。通过严格的验证和测试，该模型的准确度（平均精度或 mAP）超过 90%。 🛠️ 使用 FAST API 进行软件工程 - 接下来，我转向软件工程方面，我开发了一个 FastAPI Web 应用程序，为最终用户提供经过训练的模型。该应用程序包括一个用户友好的界面，用于上传传送带图像并接收带标签框的预测。下面附有 Web UI 图像。 📦 使用 Docker 容器化进行部署 - 为了方便部署和可访问性，我使用 Docker 对 FastAPI 应用程序进行了容器化。这允许最终用户无缝安装和运行应用程序，而无需担心依赖项或环境设置。 有关如何开发此项目的更多详细信息，我强烈建议您查看下面的流程图： https://preview.redd.it/4wjliokmzfkc1 .png?width=2048&amp;format=png&amp;auto=webp&amp;s=3fc7c2f22db0ff60a4978ea84aa7d1688c8cdf77 下一步，我将致力于添加一些单元和集成测试，以及实施监控以及自动反馈机制来触发模型训练，从而避免数据和模型漂移。  Docker 映像，包含有关如何运行对象检测 Web 应用程序的说明 - https://hub.docker .com/r/agpsuai23/box_detection_image  此项目的代码存储库 - https://github.com/abhijeetgupta23/Box-Detection-in-Warehouse-using-Vision-Based-ML-Engineering   由   提交/u/Snoo_72181   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayiwdw/p_box_detection_in_warehouse_using_vision_based/</guid>
      <pubDate>Sat, 24 Feb 2024 02:13:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 编写 ML 软件时 - 如何使用 TDD？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</link>
      <description><![CDATA[请告诉我是否有更好的子程序。 测试驱动开发。 &lt; p&gt;我已经在 ML 软件上工作了一段时间，我觉得我突然想要更好地遵循 TDD 并尝试将其应用到更细致的 ML 用例中。 有一件事多年来我注意到需求和设计对于我们的工作来说可能是模糊的——我所做的很多事情至少从最简单的设计开始，然后我们依靠迭代和强大的评估框架来证明某些改进是否合理。已实施（如果不能提高性能，为什么要实施任何东西）。在这些类型的原型设计场景中，TDD 可能会成为一个巨大的时间杀手，并且在您确定设计之前有点无用。 不过，当需求明确时，TDD 非常好，所以我正在努力变得更好将其纳入我的武器库中。 您对 TDD 有何看法以及如何/何时使用它？   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</guid>
      <pubDate>Sat, 24 Feb 2024 02:05:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 关于 MoE 和 Mamba 实施的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</link>
      <description><![CDATA[大家好， 我正在深入研究我的硕士论文，需要一些指导。我工作的核心是线性化一个充满记忆效应的复杂函数。虽然 Transformer 架构已在文献中进行了探讨，但我正在考虑采用 Mamba 架构的全新角度，或通过 MoE（专家混合）方法为 Transformer 增添趣味。 Moe-Mamba 也在讨论中。 问题是：这是我第一次真正使用这些架构，所以我真的不知道从哪里开始才能真正实现它们代码。 我应该在哪里更多地了解这些架构？您还可以为这些架构建议一些代码实现（我认为还没有库）吗？ PS：我知道我仍然需要研究很多关于这些主题的内容，所以不要评判我的愚蠢有问题请教，所以才来请教，我想学习！ :)   由   提交/u/PaleAle34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</guid>
      <pubDate>Fri, 23 Feb 2024 22:13:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代降维</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</link>
      <description><![CDATA[大家好， 我熟悉更经典的降维技术，如 SVD、PCA 和因子分析。但是，有没有一些现代技术或者人们多年来学到的一些技巧可以分享。对于上下文来说，这适用于表格数据。谢谢！   由   提交 /u/MuscleML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</guid>
      <pubDate>Fri, 23 Feb 2024 20:08:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] Beyond A*：通过 Search Dynamics Bootstrapping 使用 Transformers 进行更好的规划 - Meta 2024 - Searchformer - 显着优于直接预测最佳计划的基线，模型大小小 5-10 倍，训练数据集小 10 倍！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay5zka/r_beyond_a_better_planning_with_transformers_via/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.14083  摘要：  虽然 Transformers 在各种应用程序设置中取得了巨大进步，但此类架构仍然存在在解决复杂的决策任务方面落后于传统的符号规划器。在这项工作中，我们演示了如何训练 Transformer 来解决复杂的规划任务，并提出了 Searchformer，这是一种 Transformer 模型，可以在 93.7% 的时间内以最佳方式解决以前未见过的推箱子谜题，同时比标准 A 减少多达 26.8% 的搜索步骤Searchformer 是一个编码器-解码器 Transformer 模型，经过训练可以预测 A* 的搜索动态。然后通过专家迭代对该模型进行微调，以执行比 A* 搜索更少的搜索步骤，同时仍然生成最佳计划。在我们的训练方法中，A* 的搜索动态表示为令牌序列，概述了在符号规划期间将任务状态添加到搜索树中和从搜索树中删除时的情况。在我们对迷宫导航的消融研究中，我们发现Searchformer 的性能显着优于使用小 5-10 倍的模型大小和小 10 倍的训练数据集直接预测最佳计划的基线。我们还演示了 Searchformer 如何可以扩展到更大、更复杂的决策任务（例如推箱子），并提高已解决任务的百分比并缩短搜索动态。   https:/ /preview.redd.it/fhn5bsklbdkc1.jpg?width=1028&amp;format=pjpg&amp;auto=webp&amp;s=bbb8d726ba74d046023a6c6827249fa602c6eff1 https://preview.redd.it/n5a54uklbdkc1.jpg?width=521&amp;format=pjpg&amp;auto=webp&amp;放大器;s =619d31cf68977f98213422566f7c075aa1a2007b https ://preview.redd.it/ztmf8rklbdkc1.jpg?width=1144&amp;format=pjpg&amp;auto=webp&amp;s=700c9cf543b09b85b07d296a314d0ef6b451c1d0 https://preview.redd.it/poragwklbdkc1.jpg?width=936&amp;format=pjpg&amp;auto=webp&amp; ;s=18435580f179a63d72305aa1d9c4511f1ecf70c5   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay5zka/r_beyond_a_better_planning_with_transformers_via/</guid>
      <pubDate>Fri, 23 Feb 2024 17:17:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] OpenCodeInterpreter：将代码生成与执行和优化集成 - 2024 - HumanEval 为 92.7！ GPT-4 CodeInterpreter只有88.0！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay569a/r_opencodeinterpreter_integrating_code_generation/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.14658  Github：https://opencodeinterpreter.github .io/  摘要：  大型语言模型的引入显着改进了代码生成。然而，开源模型通常缺乏 GPT-4 代码解释器等高级系统的执行能力和迭代细化。为了解决这个问题，我们引入了OpenCodeInterpreter，这是一个开源代码系统系列，旨在生成、执行和迭代优化代码。由具有 68K 多轮交互的数据集 Code-Feedback、OpenCodeInterpreter 提供支持集成执行和人工反馈以实现动态代码细化。我们跨关键基准测试（例如 HumanEval、MBPP 及其 EvalPlus 的增强版本）对 OpenCodeInterpreter 进行了全面评估，揭示了其卓越的性能。值得注意的是，OpenCodeInterpreter-33B 在 HumanEval 和 MBPP 的平均（及以上版本）上实现了 83.2 (76.4) 的准确度，与 GPT-4 的 84.2 (76.2) 相媲美，并通过 GPT 的综合人类反馈进一步提升至 91.6 (84.6)。 4. OpenCodeInterpreter 缩小了开源代码生成模型与 GPT-4 代码解释器等专有系统之间的差距。   https://preview.redd.it/56p1vhv26dkc1.jpg?width=752&amp;format=pjpg&amp;auto =webp&amp; ;s=f1f47a4d25a05ff4a41e46eadce82ca51c1784cb   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay569a/r_opencodeinterpreter_integrating_code_generation/</guid>
      <pubDate>Fri, 23 Feb 2024 16:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 情节曲折</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</link>
      <description><![CDATA[看到一些 ICLR 结果似乎让社区感到惊讶：  Mamba ➡️ 拒绝 V-JEPA ➡️拒绝 MetaGPT ➡️按照讨论接受（口头）这里  还有哪些接受/拒绝引起了一些人的注意？   由   提交 /u/hzmehrdad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</guid>
      <pubDate>Fri, 23 Feb 2024 16:37:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongRoPE：将 LLM 上下文窗口扩展至超过 200 万代币 - Microsoft 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay4mbu/r_longrope_extending_llm_context_window_beyond_2/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2402.13753  摘要：  大上下文窗口是大型语言模型（LLM）中的一个理想功能。然而，由于微调成本高、长文本稀缺以及新标记位置引入的灾难性值，当前的扩展上下文窗口仅限于大约 128k 个标记。本文介绍了 LongRoPE，它首次将预训练的 LLM 的上下文窗口扩展到令人印象深刻的 2048k 令牌，在 256k 训练长度内仅需要 1k 微调步骤，同时保持原始性能短上下文窗口。这是通过三个关键创新实现的：（i）我们通过有效的搜索识别和利用位置插值中的两种形式的非均匀性，为微调提供更好的初始化，并在非微调场景中实现 8 倍扩展； (ii) 我们引入了一种渐进扩展策略，首先微调 256k 长度的 LLM，然后对微调的扩展 LLM 进行第二次位置插值，以实现 2048k 上下文窗口； (iii) 我们在 8k 长度上重新调整 LongRoPE 以恢复短上下文窗口性能。在 LLaMA2 和 Mistral 上进行的各种任务的广泛实验证明了我们方法的有效性。 通过 LongRoPE 扩展的模型保留了原始架构，并对位置嵌入进行了少量修改，并且可以重用大多数预先存在的优化。   https://preview.redd.it/siuxi9gf2dkc1.jpg?width=1109&amp;format=pjpg&amp;放大器;auto=webp&amp;s=c21f2879f3bdafafb1e9f0a97ca303b90c96d18f https://preview.redd.it/vtpxmcgf2dkc1.jpg?width=1188&amp;format=pjpg&amp;auto=webp&amp;s=9837d7ed191130bbf853c7455f06cb16cd70 4756 https://preview.redd.it/uapbcbgf2dkc1.jpg?width=1115&amp;format =pjpg&amp;auto=webp&amp;s=aa2f5f41dc0a32968c45144dfea21e1541d089c2   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay4mbu/r_longrope_extending_llm_context_window_beyond_2/</guid>
      <pubDate>Fri, 23 Feb 2024 16:23:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴：简单的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay49tp/d_mamba_the_easy_way/</link>
      <description><![CDATA[Mamba 看起来像是一个令人兴奋的新语言模型架构，我花了一段时间才完全理解这篇论文！该模型采用了很多棘手的概念（S4、GPU 内存、并行扫描等），因此我写了一篇博文，介绍我对 Mamba 伟大思想和贡献的理解，着眼于使其尽可能适合初学者. 链接：https://jackcook.com/2024/02/23/mamba。 html 我希望这对您有所帮助，并且我很乐意讨论任何其他问题或澄清点。让我知道你的想法！   由   提交 /u/jackcook   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay49tp/d_mamba_the_easy_way/</guid>
      <pubDate>Fri, 23 Feb 2024 16:09:49 GMT</pubDate>
    </item>
    <item>
      <title>[R]“生成模型：他们知道什么？他们知道事情吗？让我们找出答案！”。论文引用：“我们的研究结果表明，我们研究的所有类型的生成模型都包含有关场景内在因素（法线、深度、反照率和阴影）的丰富信息，这些信息可以使用 LoRA 轻松提取。”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</guid>
      <pubDate>Fri, 23 Feb 2024 14:51:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大家对 Mamba 被 ICLR 拒绝感到惊讶？我错过了什么吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1axsxeo/d_why_is_everybody_surprised_that_mamba_got/</link>
      <description><![CDATA[我也不只是想逆势而行。我不断在 Reddit、工作中、不同的在线论坛等上听到这个消息。当我第一次听到这个消息时我也很惊讶，但读完这篇论文后我并不特别惊讶。他们的硬件调整很有趣，但除此之外，这似乎是对之前论文的简单改编。基准实验并不像我最初认为的那么广泛，因为每个人都在谈论它有多么革命性。阅读这篇论文给我留下了很多问题，比如“X 任务或 Y 基准测试的性能怎么样？”我并不是想羞辱作者，但它并不真的感觉像一个“传统”的。机器学习领域的论文也有。 已经发布了很多并不完全适合会议出版物的优秀论文，我认为这不仅仅是因为某件事被讨论得很多在 Twitter 或 LinkedIn 上，这意味着它值得在某个场所发布。我真的想知道我是否低估了它，因为我没有正确理解它并且愿意接受任何意见。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1axsxeo/d_why_is_everybody_surprised_that_mamba_got/</guid>
      <pubDate>Fri, 23 Feb 2024 05:40:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>