<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 09 Dec 2023 15:12:21 GMT</lastBuildDate>
    <item>
      <title>[P] llm_microlibs：在预算限制下以分布式模式运行模型的构建块</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</guid>
      <pubDate>Sat, 09 Dec 2023 14:47:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 洞察深度学习程序员的现实生活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</link>
      <description><![CDATA[大家好！ 我对深度学习程序员的日常工作感到好奇。这个领域有很多讨论，但我想了解这份工作的真正含义。它是否主要是重复的，围绕选择预构建模型和调整参数？或者，它是否涉及更多复杂性，例如从头开始创建自己的算法和模型？ 我特别有兴趣听取那些在该领域拥有第一手经验的人的意见。你的大部分时间是如何度过的？您面临的常见任务或挑战是什么？而且，您的工作中有多少涉及创新与日常任务？ 任何见解或个人经验将不胜感激。提前致谢！   由   提交/u/Maleficent_Average39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</guid>
      <pubDate>Sat, 09 Dec 2023 14:30:41 GMT</pubDate>
    </item>
    <item>
      <title>[N] 2 篇优秀 NeurIPS 论文的完整报告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eejuc/n_fullreport_of_2_great_neurips_papers/</link>
      <description><![CDATA[      这里有 2 篇优秀的 NeurIPS 接受论文，位于第 2 页和第 32 页： https://www.rsipvision.com/ComputerVisionNews-2023December/2/ 尽情享受！ https://preview.redd.it/x4xk78kv2a5c1.png?width= 970&amp;format=png&amp;auto=webp&amp;s=79bb0f481765d86e54d218bb6f601d906b5f42de   由   提交/u/Gletta  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eejuc/n_fullreport_of_2_great_neurips_papers/</guid>
      <pubDate>Sat, 09 Dec 2023 14:18:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您使用哪个框架在图像分类模型上应用训练后量化？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eefeh/d_which_framework_do_you_use_for_applying/</link>
      <description><![CDATA[嗨，我只想在标准骨干网（例如efficientnet、mobilenet等）上应用没有校准数据集的PTQ，并报告量化模型大小、fps等。但实现非常复杂，并且大多数都使用不同的量化管道。您对这个问题有什么建议吗？   由   提交/u/m-pektas  /u/m-pektas  reddit.com/r/MachineLearning/comments/18eefeh/d_which_framework_do_you_use_for_applying/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eefeh/d_which_framework_do_you_use_for_applying/</guid>
      <pubDate>Sat, 09 Dec 2023 14:12:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我构建了一个比较云 GPU 的工具。我应该如何改进呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</link>
      <description><![CDATA[    /u/Egor_S   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</guid>
      <pubDate>Sat, 09 Dec 2023 13:20:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2023年11月研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ed48k/p_research_papers_in_november_2023/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ed48k/p_research_papers_in_november_2023/</guid>
      <pubDate>Sat, 09 Dec 2023 13:00:23 GMT</pubDate>
    </item>
    <item>
      <title>【新闻】GitHub 连续 3 天成为全球热门：SuperDuperDB，一个将 AI 与主流数据库集成的框架（让它们变得超级超级）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</link>
      <description><![CDATA[它可以轻松地将人工智能构建到您的应用程序中，而无需复杂的管道，并使您的数据库变得智能化（包括矢量搜索），一定要检查一下：&lt; a href=&quot;https://github.com/SuperDuperDB/superduperdb&quot;&gt;https://github.com/SuperDuperDB/superduperdb  &amp;# 32；由   提交 /u/escalize   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</guid>
      <pubDate>Sat, 09 Dec 2023 11:58:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 训练模型时使用的是哪个 NVIDIA A100 版本？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eb7b6/r_which_nvidia_a100_version_was_used_for_training/</link>
      <description><![CDATA[在这篇文章中，他们提到了 NVIDIA A100 进行训练，但我不确定是 40GB 还是 80GB 版本。他们添加了“记忆”是32GB，但也许只是电脑RAM。有什么线索吗？ 你会如何将其与更便宜的 GPU 相匹配？ https ://github.com/sony/hFT-Transformer https://arxiv.org /pdf/2307.04305.pdf   由   提交/u/alf_Lafleur  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eb7b6/r_which_nvidia_a100_version_was_used_for_training/</guid>
      <pubDate>Sat, 09 Dec 2023 10:58:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于文档布局分析的 Vision Grid Transformers</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e8wub/r_vision_grid_transformers_for_document_layout/</link>
      <description><![CDATA[阿里巴巴研究院最近（2023 年 10 月）发布了文档布局分析的新模型，为文档布局分析任务树立了新的基准。  简介 - 为了充分利用多模态信息并利用预训练技术来学习 DLA 的更好表示，在本文中，我们提出了 VGT，一种双流 Vision Grid Transformer，其中 Grid Transformer ( GiT）被提出并预训练用于 2D 令牌级和段级语义理解 https://arxiv.org/abs /2308.14978 ​ 对 LLM 使用的影响 - VGT 可以将页面分解为不同的部分（标题、副标题、标题等），然后可以进行 OCRed 并传递给 RAG 的法学硕士。 通过个人使用 VGT，似乎即使是视觉丰富的文档也可以通过很少的后处理轻松解析。 &lt; !-- SC_ON --&gt;  由   提交 /u/GustaMusto   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e8wub/r_vision_grid_transformers_for_document_layout/</guid>
      <pubDate>Sat, 09 Dec 2023 08:16:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们如何知道现在“最好的模型”是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</link>
      <description><![CDATA[出于某种原因，每个人都在疯狂炒作“MistralAI”发布新模型。人们会访问一个通用网站来比较哪些型号是目前的“领先型号”吗？人们是否会寻找特定的统计数据来确定哪些模型最好？   由   提交 /u/stuck-in-an-ide   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</guid>
      <pubDate>Sat, 09 Dec 2023 06:43:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAAI 会议决定出炉！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e6cpk/d_aaai_conference_decisions_out/</link>
      <description><![CDATA[没有给我发送电子邮件通知，但已在 CMT 中显示:)  &amp;# 32；由   提交/u/benthe human_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e6cpk/d_aaai_conference_decisions_out/</guid>
      <pubDate>Sat, 09 Dec 2023 05:23:19 GMT</pubDate>
    </item>
    <item>
      <title>[D]组讨论 OpenAI 的零样本图像分类基础 CLIP 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dzol3/group_discussion_on_openais_foundational_clip/</link>
      <description><![CDATA[大家好，今天我们就 OpenAI 的 2021 年 CLIP 论文进行了热烈的小组讨论。 每个星期五我们都在回顾当今机器学习中使用的许多最先进技术的基础知识。希望每周都能学到一点东西，并发现可以应用到自己工作中的模式。我觉得在阅读这篇论文之前总是有一些我没有完全理解的信息，所以我发现它很有帮助。 尽管截至本周这还不是开创性的研究，但我认为它很好退后一步，回顾一下基础知识，并跟上最新和最好的内容。 如果有人觉得有帮助的话，可以在此处发布注释和视频回顾： https://blog.oxen.ai/arxiv-dives-zero-shot-image -classification-with-clip/ 也希望有人能在周五参加我们的直播或推荐论文！我们有一个由 400 多名工程师和研究人员组成的非常稳定且有趣的团队。 ​  &amp;# 32；由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dzol3/group_discussion_on_openais_foundational_clip/</guid>
      <pubDate>Fri, 08 Dec 2023 23:23:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] QuIP#：SOTA 2 位法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dwb4t/r_quip_sota_2_bit_llms/</link>
      <description><![CDATA[我们很高兴推出 QuIP#，这是一种新的 SOTA LLM 量化方法，它使用 QuIP（论文）和 QuIP 的不相干处理。格来实现具有接近 fp16 性能的 2 位 LLM！现在您可以在 24G GPU 上运行 LLaMA 2 70B，无需卸载！ QuIP# 碾压了所有公开可用的语言建模和 2 位 PTQ 方法。零射击任务，同时概念上干净简单。我们已经发布了量化的 LLaMA、Mistral 和 OpenHermes 模型，以及完整的代码库，位于 https://github.com/Cornell- RelaxML/quip-sharp  有关 QuIP# 工作原理的更多信息，请参见此处 https:/ /cornell-relaxml.github.io/quip-sharp/. ​   由   提交/u/tsengalb99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dwb4t/r_quip_sota_2_bit_llms/</guid>
      <pubDate>Fri, 08 Dec 2023 20:49:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于共谋圈的真诚讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</link>
      <description><![CDATA[亲爱的 NeurIPS 同胞拒绝。当你的深度学习、强化学习、图神经网络和深度学习理论被人们飞到新奥尔良时，你意识到自己被抛在了后面。 ​ I邀请您加入我的小组治疗讨论，我们今天的主题是共谋环。 ​ 我想……第一个问题是它们是否真的存在？它们在机器学习学术界的渗透程度如何？作为一个为发表第一篇论文而奋斗多年的人，我的轶事证据表明，机器学习更多的是关于鼓手的节奏，而鼓手肯定是深度学习的粉丝。 ​ 作为一个仍在努力发表另一篇论文的人，我的轶事观察是，在过去几年里，鼓声变得更加激烈。 ​ 作为一个与同样被边缘化的其他人进行过很多很多对话的人，我们的轶事数据池并不完全是一个数据集，而是一种过滤，它不是独立同分布的，但肯定表明积极主动获取深度学习引用对我们的职业生涯来说是更好的选择。 ​ 作为目前正在审稿 ICLR/AAAI/AISTATS 的人。我的轶事证据是审稿人协调是通过秘密握手、关键词、引文、参考文献列表、主题、arxiv 预印本和shibboleths 进行的。 ​ 我希望你能找到作为一个从内向外看或从外向内看的人，勇敢地分享你的经历。 ​ 作为希望的灯塔，我提醒你阅读迈克尔·乔丹的革命尚未发生。&lt; /p&gt; ​ 作为最后一个需要思考的问题。深度学习合谋圈已经崩溃了吗？还会进一步崩溃吗？   由   提交 /u/Terrible_Button_1763   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</guid>
      <pubDate>Fri, 08 Dec 2023 18:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>