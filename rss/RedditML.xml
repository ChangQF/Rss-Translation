<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 08 Jan 2024 18:17:52 GMT</lastBuildDate>
    <item>
      <title>[P] 我构建了 marimo——一个开源的反应式 Python 笔记本，它存储为 .py 文件，可以作为脚本执行，并且可以作为应用程序部署。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191rdwq/p_i_built_marimo_an_opensource_reactive_python/</link>
      <description><![CDATA[嗨！我想分享 marimo，一个用于 Python 的开源反应式笔记本。它旨在解决 Jupyter 笔记本的许多众所周知的问题，同时为您提供新功能：marimo 笔记本可重复（无隐藏状态）、git 友好（存储为 Python 文件）、可作为 Python 脚本执行以及可部署为 Web 应用程序。 GitHub 存储库： https://github.com/marimo-team/marimo 在 marimo 中，您的笔记本代码、输出和程序状态保证是一致的。运行单元格并通过自动运行引用其变量的单元格来做出反应。删除一个单元格，marimo 就会从程序内存中清除其变量，从而消除隐藏状态。如果您担心意外触发昂贵的计算，您可以禁用特定单元格的自动运行。 marimo 还附带 UI 元素，例如滑块、数据帧转换器以及自动与 Python 同步的交互式绘图。与元素交互，使用该元素的单元格会自动以其最新值重新运行。反应性使这些 UI 元素比 Jupyter 小部件更有用，更不用说更易于使用。 我选择开发 marimo，因为我相信 ML 社区应该有一个更好的编程环境来进行研究和交流。我看到很多研究都是从 Jupyter 笔记本开始的（我自己的大部分也是这样）。由于 Jupyter 笔记本固有的缺陷，我还看到许多相同的研究无法重现或因隐藏的错误而减慢速度。 我坚信，我们的工作质量取决于我们的工作质量我们使用的工具塑造了我们的思维方式——更好的工具，更好的思维。 2017 年至 2018 年，我在 Google Brain 担任软件工程师，当时 TensorFlow 正在过渡到 TensorFlow 2，而 JAX 还处于早期阶段。我亲眼目睹了 PyTorch 和 JAX 为我们的社区带来的生产力的提高，后来当我在斯坦福大学与 Stephen Boyd 一起攻读博士学位时，我也亲眼目睹了我自己的研究。我们对 marimo 的目标是通过新的编程环境做一些类似的事情。 marimo 的开发经过了科学家和工程师的密切投入，并受到了包括 Pluto.jl 和 Streamlit 在内的许多工具的启发。只有我们两个人在研究它——我们最近将其开源，因为我们认为它已经准备好供更广泛的使用。请尝试一下（pip install marimo &amp;&amp; marimo 教程简介）。我们非常希望您能提供任何反馈！   由   提交 /u/akshayka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191rdwq/p_i_built_marimo_an_opensource_reactive_python/</guid>
      <pubDate>Mon, 08 Jan 2024 18:00:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 128GB RAM 的 MacBook Pro M3 Max 适合机器学习吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191r9qj/d_is_the_macbook_pro_m3_max_with_128gb_of_ram/</link>
      <description><![CDATA[成本不是一个因素。我正在考虑购买便携式 Apple 机器，因为我希望使用同一台计算机进行 macOS 和 iOS 开发。 这台笔记本电脑可以处理机器学习工作负载吗？我主要研究计算机视觉模型：深度学习、少量镜头学习。我还计划建立计算机试听和自然语言处理的模型。如果我购买最高规格，我希望这台电脑至少能使用几年。 对于我的情况，您会推荐什么？   由   提交/u/ordinarytier  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191r9qj/d_is_the_macbook_pro_m3_max_with_128gb_of_ram/</guid>
      <pubDate>Mon, 08 Jan 2024 17:56:14 GMT</pubDate>
    </item>
    <item>
      <title>[P]Retri-evals：检索评估管道</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191qvst/pretrievals_retrieval_evaluation_pipelines/</link>
      <description><![CDATA[大家好， 我们一直致力于为法学硕士构建检索管道，与许多其他人一样，我们质疑如何改变我们的管道（例如分块、清理）会影响整体结果。  我们还面临着根据哪些数据进行评估的问题。学术上使用MTEB，但使用我们自己的数据会更可靠。 Retri-evals 希望能够解决这些问题。我们提取了 MTEB 抽象，使我们能够针对开源数据集进行评估，并且我们将开源用于从生产数据自动生成评估数据集的代码。 我很想听听您的意见想法！我们希望通过工具来补充该领域的现有解决方案，使其更容易投入生产。 https ://github.com/DeployQL/retri-evals   由   提交 /u/mtbarta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191qvst/pretrievals_retrieval_evaluation_pipelines/</guid>
      <pubDate>Mon, 08 Jan 2024 17:41:02 GMT</pubDate>
    </item>
    <item>
      <title>我有资格攻读机器学习博士学位吗？ [d]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191pt08/do_i_have_the_qualifications_to_pursue_a_phd_in/</link>
      <description><![CDATA[我最近从美国一所顶级学校完成了计算机科学本科课程，但不一定是最华丽的计算机科学项目。我的成绩很好，但并不令人难以置信（3.6）。我目前在一家初创公司担任数据科学家，但我开始面临四分之一人生危机，并且完全没有成就感。 深思熟虑过我一生想要做什么在过去的几个月里，我对攻读博士学位产生了兴趣，希望能够进行人工智能方面的研究。由于在本科期间没有做过任何研究，我有点冒名顶替综合症，在如此快速发展和竞争激烈的领域，担心我在申请时不会被视为强有力的候选人。我喜欢阅读并尝试实施研究论文，但有时我无法理解一些高级数学。我一直是一名数学成绩很好的学生，但本科时不需要学习大量的高水平数学课程，而且我也想知道这是否会成为一个问题。 有谁了解招生的竞争程度吗？美国顶尖学校是否有此类课程？我不知道有人追求类似的路线，并且考虑到我不存在的研究经验，我担心申请只是浪费时间。谢谢！   由   提交 /u/Wheelerdealer75205   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191pt08/do_i_have_the_qualifications_to_pursue_a_phd_in/</guid>
      <pubDate>Mon, 08 Jan 2024 16:57:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 采访里奇·萨顿</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191oujg/d_interview_with_rich_sutton/</link>
      <description><![CDATA[一个多月前，我向这位订阅者询问了一些问题，以询问 Rich Sutton (此处），截至今天，完整采访内容可在 https:/ /youtu.be/4feeUJnrrYg！ Rich 有一些独特的想法 - 或者正如他喜欢说的 - 它是什么过时了，但我很好奇听听其他人之后的想法提出其中一些想法。 大纲： 0:00 - 简介 1:33 - 采访开始 2:04 - OpenMind 研究院4:32 - 人工智能的历史7:13 - 扩展容易吗？10:49 - 反向传播和反向传播的问题陈述21:22 - 狭隘视野的咆哮23:43 - 令人兴奋的新事物 32:00 - 记忆 35:34 - 提出想法 43 :47 - STOMP45:30 - Keen Technologies50:39 - 人类的下一阶段和未来情绪1:06:25 - 外星人工智能1:08:00 - 不同的研究方法1:21:30 - 里奇的建议1:26:00 - RL 牛肉1:27:07 - 将所有内容整合在一起    由   提交/u/ejmejm1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191oujg/d_interview_with_rich_sutton/</guid>
      <pubDate>Mon, 08 Jan 2024 16:17:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人脑 FLOPs 估计，是否比我们想象的要低？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191ol1n/d_human_brain_flops_estimate_is_it_lower_than_we/</link>
      <description><![CDATA[这篇文章旨在提供对人脑的深入了解，以便更容易将其与人工神经网络进行比较。 对我即将要说的大部分内容持保留态度，我很容易就会被一个数量级所影响，或者错过一些东西。  Ray Kurzweils 估计。 1011 个神经元。每个神经元有 1000 个突触连接。每秒 100 个峰值。  每秒计算 1011 × 1000 × 100=1016 次。 引用奇点临近：“考虑到人脑逆向工程的早期阶段，我将使用更保守的数字 1016 CPS”。  我自己的计算。自 2005 年以来情况似乎发生了变化，现在维基百科说每个神经元有 7000 个突触 https://en.m.wikipedia.org/wiki /Neuron  神经元放电速度平均为 0.1 到 2 赫兹。 https://aiimpacts.org/rate-of-neuron-firing/ #:~:text=Assorted%20estimates- 我将使用 1/s 作为尖峰频率。大脑也更明确，有 86,000,000,000 个神经元。 8,6×1010 × 7000 × 1 = 6×1014。 6×10 14 FLOP（每个突触一次 FLOP）。  峰值能量需求。神经元的每次激活都需要一定量的能量，该能量似乎为 2.468 × 10−7 J https://link.springer.com/article/10.1007/s11571-018-9503-3  所以从这里开始，其他一切都可以被弄清楚。尖峰能量 = 2.468 × 10−7 J 24 小时内大脑能量消耗 = 1,673,600 焦耳 24 小时内的秒数 = 86400。每个神经元有 7000 个突触。 1,673,600÷(2.468 × 10 −7) J = 6,782×1012。 6,782×1012 ÷ 86400 = 78,486,103。 (78,每秒 500 万次峰值）。 78,486,103 × 7000 = 5.49×1010 FLOP 或 549 gigaFLOPs 如果 3 正确，则意味着高端手机的 GPU 计算量比人脑的计算量还要多（三星 s23，fp32 时为 3,681 TFLOP。大脑一天平均为 0,549 TFLOP）。 这不是比较事物的好方法，因为大脑是一台大规模并行计算机，内存基本上存在于结构中。  那么需要多少“内存”呢？我们谈论的是大脑吗？我们有： 86,000,000,000 个神经元。每个神经元有 7000 个突触。每个突触 5 位。 https://www.cnsnevada.com/what-is-the-memory-capacity-of-a- human-brain/#:~:text=Neurons%20are%20the%20cells%20which 86,000,000,000 × 7000 × 5 = 3×1015 位或 3.76×1014 字节。祝你好运，在手机上安装 376 TB RAM。 但是每秒 78,500,000 个峰值真的足以让大脑处理所有事情吗？让我们看看眼睛。 每只眼睛的总分辨率为 8 兆像素。 https://m.youtube.com/watch?v=4I5Q3UXkGd0&amp;pp=ygUednNhdWNlIHJlc29sdXRpb 24gb2YgaHVtYW4gZXll&lt; /p&gt; 通过视神经发送的信息大约只有 10,000,000 位/秒 https://www.eurekalert。 org/news-releases/468943 （只有最相关的信息通过视神经发送，因为大脑希望不惜一切代价节省电量）。因此，我们的双眼每秒有 20,000,000 个尖峰，这是 7850 万个尖峰的 25.5%。 7850 万个尖峰并不是一个硬性的性能上限，它只是一天的平均值，而大脑是根据需要主动调节脑电波频率。 您认为哪种情况更有可能？ 1. 2. 或 3.   由   提交 /u/SpaceXRaptor42   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191ol1n/d_human_brain_flops_estimate_is_it_lower_than_we/</guid>
      <pubDate>Mon, 08 Jan 2024 16:05:56 GMT</pubDate>
    </item>
    <item>
      <title>低延迟计算机视觉推理服务器 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191oiq2/low_latency_computer_vision_inference_server_p/</link>
      <description><![CDATA[我正在尝试部署计算机视觉模型来对实时视频源 (30fps) 运行预测。我的想法是在 Docker 容器中创建一个“服务器”应用程序，该应用程序将在容器启动时加载模型，然后侦听运行预测的请求。这些请求将来自同一台机器上的另一个进程（从多个摄像机获取帧）。我遇到的问题是，由于序列化，从一个进程到 Docker 化服务器的图像通信速度太慢。我的问题是：有没有办法减少这种设置的延迟？这是我的想法：  在运行模型的 Docker 应用程序中安装相机：不幸的是，由于其他设计限制，这是不可能的。 使用卷绑定并通过磁盘 I/O：太慢。 运行一个简单的 HTTP 服务器：序列化 numpy 图像需要太长时间。 使用消息代理：我尝试了 RabbitMQ 和 Kafka，但是序列化问题仍然存在。  是否有一个我没有考虑过的选项，或者这不是使用 Docker 的正确位置？   由   提交 /u/xlext   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191oiq2/low_latency_computer_vision_inference_server_p/</guid>
      <pubDate>Mon, 08 Jan 2024 16:03:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研讨会</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191nawt/d_workshops/</link>
      <description><![CDATA[我正在考虑在一个月内向 ICLR 研讨会提交论文，但我想知道顶级会议研讨会的接受率通常是多少。 我在此子中找到的只是 7 年前的帖子。   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191nawt/d_workshops/</guid>
      <pubDate>Mon, 08 Jan 2024 15:09:03 GMT</pubDate>
    </item>
    <item>
      <title>温湿度传感器故障/失效预测 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191m8i3/temperature_and_humidity_sensor_faultfailure/</link>
      <description><![CDATA[我拥有气象站 (Vaisala HMP155) 中使用的特定品牌传感器的 5 年温度和湿度读数数据集。每个数据点对应 10 分钟的观察。所以每个数据点有 2 列。我认为大约有 350-400k 的数据点或行。  存在诸如 999 之类的不稳定读数和明显不准确的负值。当他们看到这些读数时，就该去检查传感器并进行故障排除。  我如何利用这些数据来制定一种算法来检测这些故障，然后在实际再次发生故障之前预测或警告是否出现问题？比如寻找早期迹象...  我想制作某种警报系统，这样一旦传感器出现故障或出现故障，就不必进行维护。如果数据或模式有问题，他们就会收到通知...   由   提交 /u/Funny_Shoe1772   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191m8i3/temperature_and_humidity_sensor_faultfailure/</guid>
      <pubDate>Mon, 08 Jan 2024 14:18:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何猜测梯度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191lu3v/r_how_to_guess_a_gradient/</link>
      <description><![CDATA[      奇怪的是，你在不知道目标函数的情况下就知道梯度在哪里。 论文：https://arxiv.org/abs/2312.04709 摘要  关于梯度你能说多少无需计算损失或不知道标签的神经网络？这听起来可能是一个奇怪的问题：答案肯定是“很少”。然而，在本文中，我们表明梯度比之前想象的更加结构化。梯度位于可预测的低维子空间中，该子空间取决于网络架构和传入特征。利用这种结构可以显着改进基于方向导数的无梯度优化方案，该方案一直难以扩展到在玩具数据集上训练的小型网络之外。我们研究如何缩小计算精确梯度的方法和使用方向导数的方法之间优化性能的差距。此外，我们强调了克服精确梯度优化和猜测梯度之间巨大差距的新挑战。  https://preview.redd.it/l7tm982c28bc1.png?width=1962&amp;format=png&amp;auto=webp&amp;s=94d237353bc53ee b21489f6adeeaa8e43043f44a ​   由   提交/u/That_Violinist_18   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191lu3v/r_how_to_guess_a_gradient/</guid>
      <pubDate>Mon, 08 Jan 2024 14:00:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 是否存在仅适用于比较结果的贝叶斯优化的等效项？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191juun/p_is_there_an_equivalent_of_bayesian_optimization/</link>
      <description><![CDATA[大家好，我正在解决一个问题，我需要找到最佳参数集（其中 10 个）来优化一个非常昂贵的目标功能。通常，我会使用贝叶斯优化，但在这种特定情况下，我无法访问实际的目标函数，我唯一可以计算的是该函数在特定参数集 A 或 B 下是否更高。我不知道函数的实际值，也不知道它的导数。我所能做的就是比较两组参数，并判断哪一组产生的函数值较低。 关于我可以使用什么来找到最佳参数来优化该函数，有什么建议吗？&lt; /p&gt;   由   提交/u/ale152  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191juun/p_is_there_an_equivalent_of_bayesian_optimization/</guid>
      <pubDate>Mon, 08 Jan 2024 12:12:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 3090 与新 40 系列同等产品</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191isia/d_3090_vs_the_new_40_series_equivalent/</link>
      <description><![CDATA[我发现了一些 3090（新）的优惠：  MSI（1260 美元） PALIT（965 美元） PALIT OC（900 美元）  我想知道 40 系列的较低型号（主要是 4070 和 4070 TI，因为 4080 远远超出了我的预算（需要升级电源）对于游戏/AI 与缺乏 V-RAM 来说是值得的  请注意卡的可用性和选择就我而言有限，另外，我的电源必须更换，因为它只是650W金牌（也开放电源升级建议）。 谢谢   由   提交 /u/myselfitself   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191isia/d_3090_vs_the_new_40_series_equivalent/</guid>
      <pubDate>Mon, 08 Jan 2024 11:06:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] Infinite-LLM：使用 DistAttention 和分布式 KVCache 实现长上下文的高效 LLM 服务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191iqxj/r_infinitellm_efficient_llm_service_for_long/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.02669 摘要：  大型语言模型（LLM）的快速增长已经成为基于云的法学硕士服务的增长的驱动力，这些服务现在是推进人工智能应用程序不可或缺的一部分。然而，LLM服务的动态自回归性质，以及支持超长上下文长度的需要，要求灵活分配和释放大量资源。这给设计基于云的LLM服务系统带来了相当大的挑战，低效的管理可能导致性能下降或资源浪费。为了应对这些挑战，本文引入了DistAttention，这是一种新颖的分布式注意力算法，它将KV Cache分割成更小的、可管理的单元，从而实现注意力模块的分布式处理和存储。基于此，我们提出了DistKV-LLM，这是一种分布式LLM服务系统，可以动态管理KV缓存并有效地编排跨数据中心的所有可访问的GPU和CPU内存。这确保了云上的高性能法学硕士服务，可适应广泛的上下文长度。在具有 32 个 NVIDIA A100 GPU（配置为 2 到 32 个实例）的云环境中进行验证，我们的系统表现出 1.03-2.4 倍的端到端吞吐量改进，并且支持的上下文长度比当前状态长 2-19 倍-art LLM 服务系统，通过对上下文长度高达 1,900K 的 18 个数据集的广泛测试证明。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191iqxj/r_infinitellm_efficient_llm_service_for_long/</guid>
      <pubDate>Mon, 08 Jan 2024 11:03:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么在机器学习中几乎所有的概率推导都如此难以遵循？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190to69/d_why_are_almost_all_probabilistic_derivations_so/</link>
      <description><![CDATA[      我认为自己非常擅长数学，甚至还教过大学生，活跃于ML 等领域。 然而，我发现大多数（如果不是全部）涉及 ML 中远程概率问题的论文都得到了残酷的解释。 最近，我决定真的了解 OG [DDPM](https://arxiv.org/pdf/2006.11239.pdf) 论文。&lt; /p&gt; 这是推导的一部分，他们……以某种方式……插入了 KLD。我完全不清楚这个跳跃是如何进行的。是的，我看过 KLD 的定义，是的，我用谷歌搜索过，但每个人似乎都相信这一点。 ChatGPT 说“存在未显示的隐藏期望”。 https://preview.redd.it/glvvzcc351bc1.png?width=2014&amp;format=png&amp;auto=webp&amp;s=d4c95a5716c0b8113e9a3346b8f99e3c5 a3db919 有人知道吗？  ​ 更新：感谢大家的评论，我这里的结论是DDPM论文有一个错误，即上面的图像。  错误是因为它们显示外部期望没有被用完，而实际上它已经被用尽了。  我在 Calvin 的论文此处中找到了正确的推导过程。这是图像： ​ https://preview.redd.it/54o6592vj2bc1.png?width=2370&amp;format=png&amp;auto=webp&amp;s=78d089d3d5c183f286bac15d3e6 d38ed5fa4e37e 上面是正确的，而DDPM论文是错误的。  ​   由   提交 /u/Ayakalam   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190to69/d_why_are_almost_all_probabilistic_derivations_so/</guid>
      <pubDate>Sun, 07 Jan 2024 14:46:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>