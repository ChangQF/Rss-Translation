<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 15 Feb 2024 18:17:09 GMT</lastBuildDate>
    <item>
      <title>对预建ML工作站的看法 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arlupt/opinion_on_prebuild_ml_workstation_p/</link>
      <description><![CDATA[您好，我正计划在欧洲购买一个 ML 工作站。我找到了一个预构建工作站，它并不是主要专注于 ML，但考虑到 aime、lambda lab 和 co 的价格，它似乎相当便宜。类似的构建。 考虑到 8.989 欧元的价格（20% 增值税和运费），您对此构建有何看法？ ​ 底盘 分形设计 - Meshify 2 XL |玻璃窗 CPU（处理器） AMD Ryzen Threadripper PRO 5955WX，16x 4.0GHz，64MB L3 缓存 主板 华硕 Pro WS WRX8OE-SAGE SE WIFI II | AMD WRX80 显卡 2x NVIDIA GeForce RTX 4090 24GB |技嘉游戏超频 内存 128GB DIMM DDR4-3200 CL22 ECC | 8x 16GB SSD (M.2 / PCIE) 2TB 西部数据黑色 SN850X |读取速度高达 7,300 MB/s 电源装置 1500W - Corsair HXi Platinum 2023 系列 |完全 CPU冷却器 安静！黑岩专业TR4 | 135mm+ 120mm PWM风扇 机箱风扇 7x 120mm Noctua NF-A12×25 |黑色，PWM ​ 总金额 €8.989,- ​ 如果稍微玩一下包含的部分，所以如果您认为某些内容没有意义，请告诉我。   由   提交/u/Striking_Way_3205   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arlupt/opinion_on_prebuild_ml_workstation_p/</guid>
      <pubDate>Thu, 15 Feb 2024 18:07:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用不同的 GPU 模型来训练神经网络是否有价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arln8e/d_is_there_value_to_using_different_gpu_models_to/</link>
      <description><![CDATA[嗨，一位 ML 菜鸟。就异构计算而言，在不同型号的 GPU（例如 3090 和 4060 一起）上训练同一神经网络的不同部分是否有好处？还是只使用多个相同型号的 GPU（例如 3x 3090s）更好？   由   提交 /u/FellowOInfiniteJest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arln8e/d_is_there_value_to_using_different_gpu_models_to/</guid>
      <pubDate>Thu, 15 Feb 2024 17:59:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于投影的迭代方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arlco0/d_projectionbased_iterative_methods/</link>
      <description><![CDATA[      您好！我试图理解基于投影的迭代方法背后​​的理论。如果有人能向我解释如何将以下命题与该图联系起来，我将不胜感激。您还能为该图提出一个标题吗？  ​ ​ https://preview.redd.it/motenqf5esic1.png?width=480&amp;format=png&amp;auto=webp&amp;s=541a8d097763759126ee cd9a5b82c9d35150d551   由   提交 /u/ItsGauss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arlco0/d_projectionbased_iterative_methods/</guid>
      <pubDate>Thu, 15 Feb 2024 17:47:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] BERT 与 ChatGPT 比较（文本分类和情感分析）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</link>
      <description><![CDATA[有人研究过微调特定 BERT（或任何其他类似模型）与 ChatGPT（微调与否）之间的情感比较分析和文本分类？ 我很想知道它们在性能、成本、维护等方面的比较。   由   提交 /u/Grinbald   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</guid>
      <pubDate>Thu, 15 Feb 2024 17:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于机器学习研究科学家角色的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arkj9h/d_question_on_machine_learning_research_scientist/</link>
      <description><![CDATA[大家好， 我是一名国际硕士。正在考虑攻读博士学位的美国学生程序。在过去的两年里，我主要从事视觉语言模型和多模态学习方面的研究。与此同时，我收到了一家知名 IT 公司的软件工程师职位邀请。尽管如此，我的热情还是偏向于研究，并渴望在未来担任工业研究科学家职位。 鉴于我在研究或应用科学家角色中的直接联系有限，我正在寻求对当前工作的见解市场。谁能分享一下研究科学家（Google、Deepmind、Meta 等）的招聘情况如何？ P.S.我曾尝试在多个 Reddit 子版块中提出这个问题，但到目前为止还没有得到回复。    由   提交 /u/shubhamprshr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arkj9h/d_question_on_machine_learning_research_scientist/</guid>
      <pubDate>Thu, 15 Feb 2024 17:13:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1M/10M token上下文窗口怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</link>
      <description><![CDATA[是否会启动社区头脑风暴主题？ - 人们是否认为 RingAttention 可以充分扩展？参见https://largeworldmodel.github.io - 它是用 1M 还是 10Mn 令牌窗口进行训练的，这对我来说似乎不清楚？他们是否以某种方式进行概括？ - 存在哪些数据集可以训练 10M 文本标记窗口？ - 在这么长的背景下你如何做 RLHF？ 1M 文本 ~ 4M 字符 ~ 272k 秒阅读时间（根据 Google 假设 68 毫秒/字符）~ 阅读一个示例需要 75 小时？ 编辑：当然 lucidrains 已经在着手实施 RingAttention！ (https://github.com/lucidrains/ring-attention-pytorch)   由   提交 /u/gggerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</guid>
      <pubDate>Thu, 15 Feb 2024 16:13:29 GMT</pubDate>
    </item>
    <item>
      <title>如何实现语言模型中拼写错误的鲁棒性？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj0a1/how_to_achieve_robustness_to_spelling_mistakes_in/</link>
      <description><![CDATA[据我们所知，ChatGPT 和类似的 LLM 对拼写错误非常稳健。例如，当我写“buter”时，他们会理解我的意思可能是“黄油”即使在有限的上下文场景中。 我在“干净”的语料库上预训练了 BERT 模型。就其本身而言，它在许多任务上都能很好地工作，但在包含拼写错误的嘈杂文本的示例中，性能显着下降。因此，我一直在寻找缓解这种情况的方法，并发现了一些带有拼写纠正管道的旧技术，这对我来说似乎并不有趣。在其他一些情况下，建议随机或使用预定义的字典增加语料库的噪声（在我看来，这不是很好）。然后可以选择混合干净和不干净的语料库来创建更加多样化的预训练数据。我认为这就是要走的路。 因此，如果有人能给我指出任何有关这方面的分析/比较/已发表的工作，那就太好了。或者是否有人能够解释为什么 GPT 擅长处理嘈杂的输入。   由   提交 /u/DunderSunder   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj0a1/how_to_achieve_robustness_to_spelling_mistakes_in/</guid>
      <pubDate>Thu, 15 Feb 2024 16:10:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 衡量将法学硕士纳入工作流程之前和之后的软件工程生产力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</link>
      <description><![CDATA[我在一家软件工程公司（外包）工作，我们的管理层希望衡量大型语言模型对日常工程工作（包括软件工程、数据工程、质量保证等）。最终目标是获得一些原始指标（例如“使用 LLM 时 X 团队的表现提高了 30%”）以呈现给客户，旨在证明我们优于不使用 LLM 的竞争对手。 我的观点是，准确衡量这种影响具有挑战性，因为 LLM 的表现可能会因任务环境的不同而有很大差异（例如，为网站开发简单的注册表单与为 IBM 大型机编写代码）。此外，这最终取决于执行工作的个人（例如，在使用法学硕士作为支持工具时，A 和 B 可能会在同一任务上花费不同的时间）。 我在这里合理吗？ ？有没有什么方法可以准确衡量这些影响？我试图找到关于这个主题的研究论文，但大多数都侧重于综合 LLM 测试，将个人 LLM 表现与其他 LLM 进行比较。 编辑：发现 github copilot 研究博客指出生产力提高了 55%： https://github.blog/2022 -09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/   由   提交 /u/GottaPerformMiracles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</guid>
      <pubDate>Thu, 15 Feb 2024 15:52:36 GMT</pubDate>
    </item>
    <item>
      <title>[N] Gemini 1.5，具有 1M 上下文长度令牌的 MoE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</link>
      <description><![CDATA[https://blog.google/technology/ai/google-gemini-next- Generation-model-february-2024/  &amp;# 32；由   提交/u/Electronic-Author-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</guid>
      <pubDate>Thu, 15 Feb 2024 15:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我应该水冷我的 ML/CFD 设备吗？从长远来看它是否更便宜？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1argyb4/p_should_i_water_cool_my_mlcfd_rig_and_is_it/</link>
      <description><![CDATA[构建我的第一个 ML/CFD 装备（在更有经验的人的帮助下），我知道水冷 PC 的初始成本和维护是更高，但从长远来看，它会通过降低能源费用和减少 GPU 的烹饪次数来节省我的钱吗？我将基本上 24/7 365 运行模拟和训练。为了添加更多上下文，我希望 GPU 在大约 2-3 年内保持稳定的性能，并希望在该时间范围内优化我的成本节省。 还有人尝试过技术含量较低的解决方案，例如将电脑放入金属盒中，然后将其浸入大水体（游泳池、大垃圾桶）中进行被动冷却。我知道微软有水下数据中心，我想知道小规模的数据中心是否有效（假设所有东西都防水）。 我一直无法在网上找到好的资源。这样做的经济效益，因此任何建议都值得赞赏。   由   提交 /u/FellowOInfiniteJest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1argyb4/p_should_i_water_cool_my_mlcfd_rig_and_is_it/</guid>
      <pubDate>Thu, 15 Feb 2024 14:42:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] UI 的多模式点击模型：PTA-Text</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ardr12/d_p_a_multimodal_click_model_for_ui_ptatext/</link>
      <description><![CDATA[HuggingFace 演示： https://huggingface.co/spaces/AskUI/pta-text-v0.1 模型检查点： https://huggingface.co/AskUI/pta-text-0.1  您好！ 我想与您分享我最近开发的一个项目。我的灵感来自于一个问题：UI 通常是结构化的，不像现实世界的图像那样嘈杂，但为什么人们使用像 LLM/VLM 这样的重型智能模型？当然，大型法学硕士/VLM 有利于规划，但在本地化方面遇到困难。因此，我构建了一个小型多模式，它可以获取用户屏幕截图并执行单击命令。目前，我**仅在文本上进行**作为原型设计阶段。 当然，有一些很好的企业解决方案，例如 Copilot、Adept ACT-1、AutoGPT 等正在尝试实现这但我的只是它的较小版本。 期待听到您的意见！ 注意事项：  仅接受 1920x1080 尺寸屏幕截图的训练。因此，在该尺寸上表现良好，但在其他宽高比上效果还不错。 我添加了位置说明符来帮助定位。例如，我们可以输入“单击文本“通知””在屏幕的右上角”等 当文本出现在多个位置时，我们甚至无法使用位置说明符缩小范围，会出现一些问题。    由   提交/u/Outlandish_MurMan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ardr12/d_p_a_multimodal_click_model_for_ui_ptatext/</guid>
      <pubDate>Thu, 15 Feb 2024 11:52:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 变压器中数字特征的位置编码。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arc4di/d_positional_encodings_for_numerical_features_in/</link>
      <description><![CDATA[嗨！ 我正在尝试使用特征序列（这些是描述太阳活动区域的磁场特征， （因此每个特征对应不同的特征）来预测该区域是否会在未来 12 小时内产生耀斑。现在，我最近开始研究变压器架构，并了解到，为了使这些模型能够理解数据的顺序性质（或者我应该说，学习它），需要包含位置编码。然而，我有点困惑它们对于这种类型的数据有多大用处。 我理解 NLP 中出现的位置编码的想法，因此您可以将其应用于词嵌入。在这种情况下，如果您将单词嵌入作为标记（这是固定的，因此每个单词将始终是相同的嵌入），我可以理解模型可能能够在某种程度上记住嵌入是什么，然后提取位置信息从编码。然而，当涉及到顺序数值数据时，我担心这些编码可能没有那么有用。模型如何知道区分编码和实际值？除此之外，由于数据被归一化为 0 和 std 1，嵌入（例如通常的正弦曲线）不会淹没数据的真实值吗？ 我猜这一切都是这是因为，当我从模型中取出位置编码时，性能基本保持不变，因此它似乎没有使用与序列顺序相关的任何信息来进行预测。我想知道这是否是因为它确实对这项任务没有帮助，或者是因为我处理这件事的方式没有帮助。也许还存在一个问题，因为我的数据中有间隙（用 0 填充），并且到目前为止我没有使用屏蔽。也许添加掩蔽后会有一些更明显的影响，但我想这不会很大，因为我看不到在以下时间段内输出的样子（耀斑概率与时间）有很多变化当我取出编码时没有间隙。 非常感谢对此的任何见解！   由   提交 /u/Calcirium   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arc4di/d_positional_encodings_for_numerical_features_in/</guid>
      <pubDate>Thu, 15 Feb 2024 10:01:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用小数据集进行验证</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arakd6/d_validation_with_small_datasets/</link>
      <description><![CDATA[我所在的领域的数据集通常很小（100-10000 个样本）且具有层次结构（取自 10-50 个参与者）。这意味着，为了在足够大的测试集上评估数据，而不仅仅是少数参与者，我们需要使用交叉验证。到目前为止一切顺利。 但是，这仍然没有解决验证问题。有几种可能的方法可以进行验证：  跳过验证。这似乎是我所在领域的首选方法。我认为这是非常错误的，而且我发现它可以高估准确度 5%（包含 5000 个样本的数据集），甚至高达 20%（100 个样本）。 将训练数据一次分割为训练和验证集用于对每个测试折叠进行验证。这样做的缺点是验证集最终很小（比测试集小得多），并且如果不小心的话，训练验证分割可能是任意的。 完全嵌套交叉验证。这似乎是正确验证超参数配置的最佳方法，因为它几乎使用整个数据集进行验证。我在我的领域还没有遇到过一篇使用嵌套交叉验证（正确）的论文。我相信主要问题非常明显：如果一个人使用 10 倍嵌套交叉验证训练一个神经网络模型 100 个时期，并尝试优化 5 个二进制超参数，那么最终已经有大约 100 * 10^2 * 2 ^5 = 320,000 个纪元。如果一个 epoch 需要 10 秒，这已经相当于一个多月的计算时间，而且我们仍然只验证了很少的超参数配置。  我可以看到以下解决方案：   p&gt;  接受计算需要这么长时间（并希望评审者不要要求我们重复实验）。 找到尽可能限制超参数配置的方法。  li&gt; 改用 5 重嵌套交叉验证。 减小验证集的大小（方法 2）。 承认并停止将神经网络拟合得较小数据集。  您对此有何看法？您更喜欢哪些选项？您还有其他解决方案吗？   由   提交/u/philosophicalmachine  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arakd6/d_validation_with_small_datasets/</guid>
      <pubDate>Thu, 15 Feb 2024 08:07:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] Whisper Large v3 基准：在消费类 GPU 上以 5110 美元（每美元 11,736 分钟）转录 100 万小时 - 后续</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ar08br/p_whisper_large_v3_benchmark_1_million_hours/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ar08br/p_whisper_large_v3_benchmark_1_million_hours/</guid>
      <pubDate>Wed, 14 Feb 2024 22:50:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>