<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 28 Feb 2024 06:17:34 GMT</lastBuildDate>
    <item>
      <title>[D] CUDA 替代方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1wy6l/d_cuda_alternative/</link>
      <description><![CDATA[随着 ChatGPT 和 LLM 革命的出现，由于 Nvidia H100 正在成为大型科技公司的主要支出，您认为我们会获得可行的 CUDA 替代方案吗？我猜现在大型科技公司更有动力投资非 CUDA GPU 编程框架？   由   提交 /u/Mohan-Das   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1wy6l/d_cuda_alternative/</guid>
      <pubDate>Wed, 28 Feb 2024 04:21:00 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 在多个数据集上进行训练时，批次内组成的重要性与每个数据点出现的次数的关系</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1ppjd/discussion_importance_of_intrabatch_composition/</link>
      <description><![CDATA[[讨论] 我正在尝试使用不同质量的数据集、受监督的开源数据（范围较窄，但可能是干净的数据）来训练语音识别模型，带有耳语的伪标记数据，以及从 YT（人类上传的字幕）中抓取的弱监督数据。数据集的大小各不相同，我试图通过调整每个批次对应于每个数据集的百分比来解决这一问题。  我没有足够的计算能力来运行大型超参数扫描，并且正在寻找该领域的现有研究或来自其他经验的直觉。    由   提交 /u/ResponsibleHouse7436   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1ppjd/discussion_importance_of_intrabatch_composition/</guid>
      <pubDate>Tue, 27 Feb 2024 22:51:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用超过 10,000 个 GPU 进行 LLM 训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1m0sy/r_llm_training_with_10000_gpus/</link>
      <description><![CDATA[LLM 训练超过 10,000 个 GPU！ https://arxiv.org/abs/2402.15627 想法??   由   提交 /u/CathieVictoriaWood   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1m0sy/r_llm_training_with_10000_gpus/</guid>
      <pubDate>Tue, 27 Feb 2024 20:26:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 深度表示和 K 均值聚类的自动编码器参数调整</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1ky8g/d_tuning_parameters_of_autoencoder_for_deep/</link>
      <description><![CDATA[大家好。我使用深度表示和 K 均值聚类来查找未标记的单粒子轨迹数据中的结构。我从一系列浅层特征开始，使用深度稀疏自动编码器来获取深层特征，将它们用作 K 均值聚类的输入并评估 Davies-Bouldin 索引。 现在我想调整自动编码器的参数（稀疏目标、权重和学习率）。  创建一个参数网格，并为每个组合评估聚类性能并选择最终最小化 DBI 的参数是否有意义？ 还有其他建议吗？ &gt;   由   提交/u/Naive-Bee4750  /u/Naive-Bee4750 reddit.com/r/MachineLearning/comments/1b1ky8g/d_tuning_parameters_of_autoencoder_for_deep/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1ky8g/d_tuning_parameters_of_autoencoder_for_deep/</guid>
      <pubDate>Tue, 27 Feb 2024 19:43:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为从事跨学科人工智能研究的研究人员（博士后/博士）下一步提供建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1jp42/d_advice_for_the_next_step_of_researchers/</link>
      <description><![CDATA[我不确定这是否是一个常见问题。 我是一名博士后，在欧洲从事与人工智能相关的项目，没有论文顶级人工智能会议。我主要专注于将人工智能应用于特定领域的数据集。 然而，做博士后是一项不稳定的工作，我意识到我可能不喜欢做研究，可能是因为缺乏研究的兴趣和热情。  p&gt; 除了继续做博士后或应用 PI（可能非常具有挑战性）。工业是我的情况的选择吗？显然，我无法与“真正的人工智能”竞争。研究人员进入大型科技公司。 小型科技公司的 MLE 或 DS 工作怎么样？另外如果没有应届毕业生，我找这些工作会不会更难？ 如果可以的话，我应该准备什么？ Leetcode什么的。如果不是，有什么选择？ 谢谢。   由   提交/u/CurrentExamination49   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1jp42/d_advice_for_the_next_step_of_researchers/</guid>
      <pubDate>Tue, 27 Feb 2024 18:53:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 捕获 SVR 模型的预测不确定性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1j8yl/d_capturing_forecast_uncertainty_for_a_svr_model/</link>
      <description><![CDATA[有人对量化 SVR 模型的预测不确定性有任何建议吗？ 在参数模型中，例如基本 OLS - 预测区间解释参数和观测的不确定性是相当简单的。与神经网络类似 - 不确定性可以通过添加 dropout 层和对预测分布进行建模来量化 - 但是我不完全确定如何最好地使用 SVR 来做到这一点！ 任何想法都非常受欢迎   由   提交/u/LDM-88  /u/LDM-88 reddit.com/r/MachineLearning/comments/1b1j8yl/d_capturing_forecast_uncertainty_for_a_svr_model/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1j8yl/d_capturing_forecast_uncertainty_for_a_svr_model/</guid>
      <pubDate>Tue, 27 Feb 2024 18:35:20 GMT</pubDate>
    </item>
    <item>
      <title>[D]最近与凸优化相关的文献？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1j1yd/drecent_literature_related_to_convex_optimization/</link>
      <description><![CDATA[大家好，我在凸优化课程中，该课程的一个关键组成部分是一个项目，在该项目中我们将凸优化传递回我们的区域学习，对我来说就是深度学习。显然，如果取得重大进展，这也可以转化为研究想法。  无论如何，我正在寻找有关我可以探索的最近论文/有趣项目的方向/建议。我确实希望我的结果能够呈现出一定程度的新颖性！提前致谢    由   提交 /u/Quiet_Cantaloupe_752   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1j1yd/drecent_literature_related_to_convex_optimization/</guid>
      <pubDate>Tue, 27 Feb 2024 18:27:27 GMT</pubDate>
    </item>
    <item>
      <title>你会用什么技术来解决这个难题（自由流动）？更多信息见评论。 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1it95/with_what_technique_would_you_tackle_this_puzzle/</link>
      <description><![CDATA[      ​  https://preview.redd.it/7nt5nhtd66lc1.png？ width=497&amp;format=png&amp;auto=webp&amp;s=0cbf2001eb6add8107e2d99be76e185c533d9249   由   提交 /u/IntroDucktory_Clause   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1it95/with_what_technique_would_you_tackle_this_puzzle/</guid>
      <pubDate>Tue, 27 Feb 2024 18:17:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从某种意义上说，“在分布之外泛化”的想法不是不可能的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1he3r/d_isnt_the_idea_of_generalizing_outside_of_the/</link>
      <description><![CDATA[嘿，我并不是专门从事机器学习的人，但我是一名开发人员，并且对此有 8 年的业余爱好者研究。 &lt; p&gt;在分布之外进行泛化的想法对我来说一直很不寻常。当然，如果您学习英语，那么您现在在某种意义上就会知道如何编码，因为它是英语的。在这种情况下，该模型的泛化程度远没有我们希望的那么高，因为编码所需的基础知识几乎不是逻辑，而是读写。 在同样的意义上，人们可以想象一种颜色是不同颜色的混合。但想象一种全新的颜色并尝试一下，实际上是不可能的。在这种情况下，我们对分布之外的概括的定义并不在分布之外，只是分布比我们想象的（或可以量化的）要大 与想象你从未听过的声音是一样的。再一次，你可以想象你听到过的其他声音的融合，也许你想象的这种新声音确实是你在现实中从未听过的东西，但你还没有推广到频谱的其余部分。我无法想象 20khz 以上的声音听起来是什么样子，因为我完全没有关于它可能是什么的基本事实，就像我无法客观地想象 X 射线是什么样子，因为我受限于我的能力，只能看到可见光。   由   提交 /u/EveningPainting5852   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1he3r/d_isnt_the_idea_of_generalizing_outside_of_the/</guid>
      <pubDate>Tue, 27 Feb 2024 17:21:15 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] SOTA计算机视觉模型中如何处理尺度等方差？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b1ftd5/discussion_how_is_scale_equivariance_handled_in/</link>
      <description><![CDATA[我正在阅读 Aaron Courville、Ian Goodfellow 和 Yoshua Bengio 写的《深度学习》一书，他们提到了共享权重时卷积平移的自然等变性。因此，我想知道当前处理尺度变化的技术。 您认为权重共享和内核调整大小是否足够强大，足以解决这个问题？   由   提交/u/SufficientAd542   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b1ftd5/discussion_how_is_scale_equivariance_handled_in/</guid>
      <pubDate>Tue, 27 Feb 2024 16:19:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 会议论文是双盲的。为了什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b18is0/d_conference_paper_is_doubleblinded_for_what/</link>
      <description><![CDATA[我对提交给会议的论文必须双盲的要求感到非常困惑，但它们却可以预印有作者姓名和隶属关系，并且可以同时作为同一篇论文共享。人们甚至在接受/发表之前在 Twitter、LinkedIn、Reddit 等 SNS 上宣传他们的论文。感觉就像我在看 Instagram 版的学术界。如何确保广告而不是报纸本身不会影响决策过程？   由   提交 /u/Dry_Cheesecake_8311   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b18is0/d_conference_paper_is_doubleblinded_for_what/</guid>
      <pubDate>Tue, 27 Feb 2024 10:17:05 GMT</pubDate>
    </item>
    <item>
      <title>[N] 在 MWC 的技嘉展位上看到了这些……想象一下你可以用这些做什么！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b17igl/n_saw_these_at_the_gigabyte_booth_at_mwcimagine/</link>
      <description><![CDATA[       由   提交 /u/BubblyMcnutty   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b17igl/n_saw_these_at_the_gigabyte_booth_at_mwcimagine/</guid>
      <pubDate>Tue, 27 Feb 2024 09:05:38 GMT</pubDate>
    </item>
    <item>
      <title>对于新晋研究科学家来说，该行业不会“复苏”[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</link>
      <description><![CDATA[      今天的热门话题问：“科技行业还没有复苏吗？我有那么糟糕吗？” 让我做出一个大胆的预测（我希望我是错的，但我不认为我是错的）：这个行业不会“ “恢复”对于新晋研究科学家： 您的机器学习论文数量呈指数级增长，反映出博士生和博士后数量呈指数级增长： ​ &lt; p&gt;https://preview.redd.it/viv6l1gnkykc1。 png?width=899&amp;format=png&amp;auto=webp&amp;s=04e227dede42f7d46d1941fc268bb7ea0a409a04 ...毕业并开始竞争大致固定数量的井- 支付行业研究职位。这些职位的数量可能会季节性增加或减少，但长期趋势是他们的就业前景将变得越来越差，而这种指数趋势仍在持续。 ​  div&gt;  由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</guid>
      <pubDate>Mon, 26 Feb 2024 17:24:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是科技行业还没复苏还是我太差了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</link>
      <description><![CDATA[我是欧洲顶尖大学的一名应届博士毕业生，正在研究 ML/CV 中的一些热门主题，已发表 8 - 20 篇论文，其中大部分是我的第一作者。这些论文已累计被引用1000-3000次。 （使用新帐户和广泛的范围来保持匿名） 尽管我认为自己是一个相当有实力的候选人，但我在最近的求职过程中遇到了重大挑战。我主要瞄准研究科学家职位，希望从事开放式研究。我已经联系了欧洲、中东和非洲地区的许多高级机器学习研究人员，虽然有些人表达了兴趣，但不幸的是，由于各种原因（例如人员有限或招聘经理没有更新信息），没有一个机会成为现实。 我主要针对大型科技公司以及一些最近流行的机器学习初创公司。不幸的是，我的大部分申请都被拒绝了，而且常常没有面试的机会。 （我只接受过一家大型科技公司的一次面试，然后就被拒绝了。） 特别是，尽管有朋友的推荐，我还是立即遭到了 Meta 的研究科学家职位拒绝（几天之内）。我现在只是非常困惑和不安，不知道出了什么问题，我是否被这些公司列入了黑名单？但我不记得我树敌过。我希望就下一步可以做什么寻求一些建议......   由   提交/u/Holiday_Safe_5620   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</guid>
      <pubDate>Mon, 26 Feb 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>