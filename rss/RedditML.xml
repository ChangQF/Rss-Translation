<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 25 Dec 2024 09:16:42 GMT</lastBuildDate>
    <item>
      <title>[D] 在 Byte Latent Transformer 中，解码后的块边界是如何确定的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/</link>
      <description><![CDATA[在 Meta 最近的论文 Byte Latent Transformer 中，我了解到本地编码器模型使用 patch 分割方法（例如基于熵的方法）首先切割 patch，然后对于每个 patch，交叉注意力将关注该批次中的字节（因为 patch 边界已经确定）。但是，在这种情况下解码如何工作？是不是在解码每个字节时，都假设它在最新的 patch 中，如果检测到新的输出字节是新的 patch 边界（例如使用基于熵的方法），它会切割一个新的 patch，未来的字节现在属于这个 patch？如果是这样的话，每个输出 patch 的起始字节是否可以使用前一个 patch 有效地解码？还是当找到新的边界时，这个字节被丢弃，开始一个新的 patch，并使用这个新的 patch 再次解码它的起始字节？我不确定作者是否在论文中明确提到了这一点。    提交人    /u/TommyX12   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/</guid>
      <pubDate>Tue, 24 Dec 2024 17:19:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] OREO：用于大型语言模型中多步推理的离线强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hlglku/r_oreo_offline_rl_for_multistep_reasoning_in/</link>
      <description><![CDATA[本文介绍了 OREO，一种新颖的离线 RL 方法，它将策略学习与价值评估相结合，以改进 LLM 多步推理。关键创新是使用软贝尔曼方程和偏好优化来更好地在推理步骤之间分配信用。 主要技术要点： - 通过偏好学习和价值函数估计实现离线 RL - 使用软贝尔曼方程学习最佳行为 - 同时训练策略和价值函数 - 与现有的 DPO（直接偏好优化）方法集成 - 在 GSM8K、MATH 和 ALFWorld 基准上进行测试 结果： - 在 GSM8K 数学推理任务上的表现优于基线方法 - 在 MATH 基准问题上表现出改进的性能 - 在 ALFWorld 环境中展示更好的推理能力 - 在推理步骤中实现更有效的信用分配 - 减少推理期间的计算开销 我认为这项工作解决了让 LLM 执行复杂推理的一个根本挑战。通过更好地了解哪些步骤对成功的结果贡献最大，我们可以为需要精确逻辑思维的任务训练更有能力的系统。该方法对于自动定理证明、机器人规划和其他需要结构化多步骤推理的领域的应用可能特别有价值。 我特别感兴趣的是，这种方法如何扩展到更开放的推理任务，其中“正确”的步骤顺序并不像数学问题那样明确定义。推理过程中的计算效率也值得注意，因为它表明了实际的可部署性。 TLDR：新的离线 RL 方法结合了策略学习和价值评估，通过更好地理解哪些步骤对成功结果最重要来改进 LLM 推理。 完整摘要在这里。论文这里。   由    /u/Successful-Western27  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hlglku/r_oreo_offline_rl_for_multistep_reasoning_in/</guid>
      <pubDate>Tue, 24 Dec 2024 16:07:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我制作了一个 TikTok Brain Rot 视频生成器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hlgdyw/p_i_made_a_tiktok_brain_rot_video_generator/</link>
      <description><![CDATA[我制作了一个简单的脑腐生成器，可以根据单个 Reddit URL 生成视频。 Tldr：事实证明，制作它并不容易。 简单地说，让这个超级困难的主要思想是文本和音频之间的对齐，又称强制对齐。因此，在这个项目中，Wav2vec2 用于音频提取。然后，它使用来自音频的逐帧标签概率，创建一个网格矩阵，该网格矩阵表示在使用网格矩阵（回溯算法）中最可能路径之前每次对齐标签的概率。 如果没有我所关注和学习的 Motu Hira 关于强制对齐的教程，这真的不可能完成。请注意，这其中的数学运算相当繁重： https://pytorch.org/audio/main/tutorials/forced_alignment_tutorial.html 示例： https://www.youtube.com/shorts/CRhbay8YvBg 这是 github repo：（如果您对此感兴趣，请为该 repo 加注星标🙏） https://github.com/harvestingmoon/OBrainRot?tab=readme-ov-file 一如既往地欢迎任何建议：）    由   提交  /u/notrealDirect   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hlgdyw/p_i_made_a_tiktok_brain_rot_video_generator/</guid>
      <pubDate>Tue, 24 Dec 2024 15:57:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么不平衡的数据增强没有明确的定义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hlf6ye/d_why_is_data_augmentation_for_imbalances_not/</link>
      <description><![CDATA[好的，我们知道我们可以在预处理期间增强数据并保存该数据，生成具有方差的新样本，同时增加样本量并解决类别不平衡问题 我们还知道，使用原始数据集，您可以通过转换管道应用转换，这意味着在应用转换时，您的模型在每个时期都会看到不同版本的图像。但是，如果数据集不平衡，它仍然保持不变，因为模型仍然看到更多的多数类，但是每个样本都会提供方差，从而增加了普遍性。据我们所知，转换管道中的数据增强不会改变数据集大小。 因此，解决不平衡的最佳做法是什么？是否可以通过增强来增加数据集而不是使用转换管道？因为在预处理阶段和训练期间进行增强可能会过度增强你的图像，并可能改变实际问题的定义。 - 一些背景信息我有 3700 张眼底图像，并计划使用一些深度 CNN 架构    提交人    /u/amulli21   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hlf6ye/d_why_is_data_augmentation_for_imbalances_not/</guid>
      <pubDate>Tue, 24 Dec 2024 14:57:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们可以不要再在标题中使用“这就是我们所需要的”吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hlbtrs/d_can_we_please_stop_using_is_all_we_need_in/</link>
      <description><![CDATA[正如标题所示。我们需要停止或减少在论文标题中使用“......就是我们所需要的”。它慢慢变得有点荒谬。大多数时候它没有实际的科学价值。它已经成为一种为了吸引注意力而吸引注意力的不良做法。    提交人    /u/H4RZ3RK4S3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hlbtrs/d_can_we_please_stop_using_is_all_we_need_in/</guid>
      <pubDate>Tue, 24 Dec 2024 11:37:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 血脑屏障通透性预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hl93ke/d_blood_brain_barrier_permeability_prediction/</link>
      <description><![CDATA[我想知道哪种机器学习方法在血脑屏障通透性预测方面处于领先地位。据我所知，没有排行榜或基准，寻找论文让我找到了一篇 2020 年的论文，有 60 次引用，这并没有激发太大的信心。谢谢！    提交人    /u/Blutorangensaft   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hl93ke/d_blood_brain_barrier_permeability_prediction/</guid>
      <pubDate>Tue, 24 Dec 2024 08:12:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 任意深度神经网络的表征能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hl5918/r_representation_power_of_arbitrary_depth_neural/</link>
      <description><![CDATA[是否有任何定理讨论具有固定隐藏层大小但任意深度的神经网络的表示能力？ 我对以下情况特别感兴趣： 假设我正在使用神经网络构建一个向量值函数 f，将标量 t 映射到 2 维向量 v。f：t-&gt; v。 并且这仅使用大小为 2 的隐藏层完成。 我想知道是否有任何定理可以保证任何上述形式的函数 f 都可以被神经网络近似，只要它具有足够的深度。    提交人    /u/atharvaaalok1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hl5918/r_representation_power_of_arbitrary_depth_neural/</guid>
      <pubDate>Tue, 24 Dec 2024 03:58:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用基础模型自动搜索人工生命</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hl4o42/r_automating_the_search_for_artificial_life_with/</link>
      <description><![CDATA[很高兴在假期前发布这项新作品，使用基础模型自动搜索人工生命！ 博客：https://sakana.ai/asal/ 论文：https://arxiv.org/abs/2412.17799 论文的网站版本：https://pub.sakana.ai/asal/ GitHub：https://github.com/SakanaAI/asal 摘要 随着最近诺贝尔奖因蛋白质发现领域的重大进展而颁发，用于探索大型组合空间的基础模型 (FM) 有望彻底改变许多科学领域。人工生命 (ALife) 尚未集成 FM，因此为该领域提供了一个重大机会，以减轻主要依靠手动设计和反复试验来发现逼真模拟配置的历史负担。本文首次介绍了使用视觉语言 FM 成功实现这一机会的方法。所提出的方法称为自动搜索人工生命 (ASAL)，(1) 找到产生目标现象的模拟，(2) 发现产生时间开放式新颖性的模拟，以及 (3) 照亮整个有趣多样的模拟空间。由于 FM 的通用性，ASAL 可有效适用于各种 ALife 基质，包括 Boids、粒子生命、生命游戏、Lenia 和神经细胞自动机。突出这项技术潜力的主要成果是发现了以前从未见过的 Lenia 和 Boids 生命形式，以及像康威生命游戏一样开放式的细胞自动机。此外，使用 FM 可以以人性化的方式量化以前定性的现象。这种新范式有望加速 ALife 研究，超越仅靠人类智慧无法实现的范围。    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hl4o42/r_automating_the_search_for_artificial_life_with/</guid>
      <pubDate>Tue, 24 Dec 2024 03:23:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 上下文反向传播循环：通过自上而下的迭代反馈增强深度推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hl435x/r_contextual_backpropagation_loops_amplifying/</link>
      <description><![CDATA[想象一下，你正努力在浓雾中辨认一个人影：首先，你猜测一下——也许是朋友——然后当你注意到它的身高或步态不太匹配时，重新检查你的假设。这种假设和改进的迭代过程捕捉到了人类如何不断依靠背景来加深理解。我的新方法，上下文反向传播循环 (CBL)，通过将模型的最佳猜测推回到较早的层，根据高级线索改进不确定的特征，反映了这种现实世界的动态。因此，CBL 使神经网络能够反复将“看到”的内容与“想到”的内容进行对齐，最终形成一种更稳健、更受情境驱动的学习形式。 https://arxiv.org/abs/2412.17737 编辑：谢谢大家。将添加 FLOP 计数、不动点定理的讨论、当 h 的数量增加时会发生什么、变压器比较     提交人    /u/jacobfa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hl435x/r_contextual_backpropagation_loops_amplifying/</guid>
      <pubDate>Tue, 24 Dec 2024 02:50:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 关于LLM基准测试工具的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hl2epr/p_advice_on_llm_benchmarking_tool/</link>
      <description><![CDATA[      我正在研究一个个性化 LLM（性能）基准测试工具，希望得到您的建议。这个想法是让人们根据自己的设置来评估 AI 提供商和模型 - 使用他们的 API 密钥、他们所处的任何层级、使用他们的请求结构、模型配置等。目标是拥有与实际使用更相关的基准，而不仅仅是依靠通用统计数据。 例如，您如何知道是否应该在 Groq、Bedrock 还是其他提供商上运行 LLama3？我自己的 OpenAI GPT-4o 是否真的像他们宣传的那样表现？我的 Claude 或 GPT 响应更快吗？哪种模型最适合我的用例？ 您还想添加什么？这些是我们正在考虑的一些事情。我想扩展这个列表，并获得关于总体方向的反馈。要添加的内容：  允许长期运行的基准测试显示 AI 提供商一天中的时间/一周中某天的性能变化。也许可以通过显示性能差异的热图来实现 定期进行计划的基准测试，以标记您设置的特定性能障碍是否被突破 并发性能比较 社区共享/编辑基准 ...（请帮我添加）  欢迎任何反馈 示例图 更多上下文请访问 vm-x.ai/benchmarks（仅用于上下文，不用于推广）    由    /u/campoblanco 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hl2epr/p_advice_on_llm_benchmarking_tool/</guid>
      <pubDate>Tue, 24 Dec 2024 01:16:54 GMT</pubDate>
    </item>
    <item>
      <title>傅里叶神经算子输入/输出维度 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hksw79/fourier_neural_operator_inputoutput_dimension_r/</link>
      <description><![CDATA[大家好， 首先我要声明，我并不是 ML 专家，我是一名计算化学家，在研究中使用 ML，主要是在特定领域的数据上对已知模型进行再训练。话虽如此，我对使用傅立叶神经算子 (FNO) 架构来解决输入和输出维度不同但都经过网格离散化的问题很感兴趣。理想情况下，我的输入是具有不同分辨率的 3D 网格（即，可以是 16x16x16 或 90x90x90），而我的输出是具有相对粗糙分辨率的 1D，但我也希望能够进行这种更改。输入 3D 网格是实空间中不同点的值，输出 1D 网格是能量网格上的强度值。这两个网格的分辨率都是任意的，这就是我想要使用 FNO 的原因。在任一网格上，零样本超分辨率也具有很大的实用性。我的想法如下：  我不完全理解这种分辨率变化是否能在普通的 FNO 架构中轻松实现，因为我见过的示例总是预测相同的输入和输出网格形状，但它们显然可以在训练和测试之间改变分辨率。 我可以想象有一个架构：  输入网格上的 FNO --&gt; 线性层改变维度形状 --&gt;输出网格上的另一个 FNO，但我认为这会破坏进行超分辨率的可能性，因为内部线性层的形状将使得无法改变输入和输出离散化分辨率？  我可以通过连接每个维度将我的 3D 网格转换为 1D 网格吗（确保保持一定程度的绝对位置测量 - 我以前见过一个热编码网格位置做这样的事情），那么我只需要输入和输出分辨率不同，而不是数据的实际形状？我不确定这是否比上述任何一个都更容易，或者在某种程度上更糟。   我非常感谢任何意见，请随时指出我明显遗漏的任何事情，因为我是这个领域的新手。     提交人    /u/TheWill_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hksw79/fourier_neural_operator_inputoutput_dimension_r/</guid>
      <pubDate>Mon, 23 Dec 2024 17:35:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hknz26/d_fine_tuning_large_language_models/</link>
      <description><![CDATA[这些文章探讨了参数高效微调背后的想法，展示了在多层感知器 (MLP) 上实现低秩自适应 (LoRA)。然后还解释如何用更少的参数来实现有效学习（内在维度）以及如何使用技术（随机子空间训练）来针对给定任务进行测量。 1.探索 LoRA — 第 1 部分：参数高效微调和 LoRA 背后的想法  探索 LoRA — 第 2 部分：通过在 MLP 上的实施分析 LoRA 内在维度第 1 部分：大型模型中的学习如何由一些参数驱动及其对微调的影响 内在维度第 2 部分：通过随机子空间训练测量模型的真实复杂性     提交人    /u/l1cache   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hknz26/d_fine_tuning_large_language_models/</guid>
      <pubDate>Mon, 23 Dec 2024 13:41:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们是否将其他增强技术应用于过采样数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hkl07r/d_do_we_apply_other_augmentation_techniques_to/</link>
      <description><![CDATA[假设您的数据集中多数类别相对于少数类别的流行程度相当高（与其余类别相比，多数类别占数据集的 48%）。 如果我们在一个类别中有 5000 张图像，并且我们对数据进行过采样，使少数类别现在与多数类别（5000 张图像）匹配，然后应用增强技术，如随机翻转等。这不会使数据集大幅增加吗？因为我们会通过过采样创建重复项，然后通过其他增强技术创建新样本？ 或者我可能是错的，我只是对我们是否进行过采样并应用其他增强技术或增强是否足够感到困惑    提交人    /u/amulli21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hkl07r/d_do_we_apply_other_augmentation_techniques_to/</guid>
      <pubDate>Mon, 23 Dec 2024 10:28:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hjq0bm/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hjq0bm/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 22 Dec 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>