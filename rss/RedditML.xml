<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 04 Mar 2024 12:27:41 GMT</lastBuildDate>
    <item>
      <title>[D] 现代数据堆栈中的“现代”是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b69m3w/d_whats_modern_in_the_modern_data_stack/</link>
      <description><![CDATA[2024年行业场景、社区评论、平台标准等等！ 𝐖𝐡𝐚𝐭 𝐭𝐡𝐢𝐬 𝐩𝐢𝐞𝐜𝐞 𝐞𝐧𝐭𝐚𝐢𝐥 𝐬： &lt; ul&gt; 现代数据堆栈的当前状态 深入了解细节： - 结构 - 堆栈的元素 - 之前和之后概览  𝐑𝐞𝐚𝐝 𝐭𝐡𝐞 𝐜𝐨𝐦𝐩𝐥𝐞𝐭𝐞 𝐚𝐫𝐭𝐢𝐜𝐥𝐞 𝐡𝐞𝐫𝐞： https://moderndata101.substack.com/p/whats-modern-in-the-modern-data-stack  &amp;# 32；由   提交/u/growth_man  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b69m3w/d_whats_modern_in_the_modern_data_stack/</guid>
      <pubDate>Mon, 04 Mar 2024 12:21:43 GMT</pubDate>
    </item>
    <item>
      <title>[D]实施正确性措施</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b68qsg/d_implementation_correctness_measures/</link>
      <description><![CDATA[所以我所说的问题是当你以不同的方式实现相同的算法时（比如你移植到gpu或框架等）并且你想确保你做对了。 我当前用作健全性检查的方法是 D(x,y)= (x-y)2&lt; /sup&gt;/(x2+y2 ) 我只是假设，如果张量很大，我不会击中所有零。你可以添加一个早期存在或一个 epsilon 来保证正确性。  如果我看到 D 在许多输入中始终小于 0.05，我会在实现中感到安全。 这种情况常见吗？你有自己的吗？我打赌它有一个名字。   由   提交/u/rejectedlesbian  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b68qsg/d_implementation_correctness_measures/</guid>
      <pubDate>Mon, 04 Mar 2024 11:32:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] ASD（自闭症谱系障碍）感觉过载检测传感器参数数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b67x5n/r_asd_autism_spectrum_disorder_sensory_overload/</link>
      <description><![CDATA[是否有关于传感器参数的可用数据集，用于检测 ASD 患者的感觉超负荷？    由   提交 /u/emcay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b67x5n/r_asd_autism_spectrum_disorder_sensory_overload/</guid>
      <pubDate>Mon, 04 Mar 2024 10:41:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当前最好的分类架构/模型是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b67qhq/d_what_are_the_current_best_classification/</link>
      <description><![CDATA[我正在考虑升级旧的分类模型，该模型使用经过微调的 BERT 进行分类，但我很难找到一个好的列表当前最好的分类模型。优选地，模型可以在本地进行训练。如果有人目前正​​在使用分类模型并且可以分享一些见解，那就太好了！   由   提交 /u/Dustwellow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b67qhq/d_what_are_the_current_best_classification/</guid>
      <pubDate>Mon, 04 Mar 2024 10:28:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年从零到英雄要实施哪些论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/</link>
      <description><![CDATA[标题的灵感来自于 Karpathy 在其课程中自下而上的方法神经网络从零到英雄。与此类似，但对于研究论文来说，从早期论文（可能最多 10-15 年前）“自下而上”到今天的论文 - 熟悉 DL 的人（至少读一本 DL 书）应该做什么，但相对而言实施经验不足？这样做的目标是更深入地了解 DL 研究，并达到一个可以轻松阅读、理解和实现当今的 DL 论文的水平，甚至可能自己提出新颖的想法。 我主要考虑的领域是 CV 和 NLP，但它也可能是更一般的东西，例如学习率、损失函数、激活函数、正则化、优化等。  有谁知道可以的论文/论文为此目的是否可以很好地实施，或者您是否有任何想要分享的列表？感谢任何帮助！   由   提交/u/total-expectation  /u/total-expectation  reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/</guid>
      <pubDate>Mon, 04 Mar 2024 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有活跃且良好的 ML Discord 服务器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b61iph/d_any_active_and_good_discord_servers_for_ml/</link>
      <description><![CDATA[嗨，我正在寻找更大规模的 ML/数据科学 Discord 服务器来寻找团队（很像游戏开发团队）或与以下内容相关的讨论：话题。有什么建议么？    由   提交 /u/AloofWasTaken   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b61iph/d_any_active_and_good_discord_servers_for_ml/</guid>
      <pubDate>Mon, 04 Mar 2024 04:09:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML 不认真？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b610zm/d_ml_being_unserious/</link>
      <description><![CDATA[我正在阅读三元论文，它来了对我来说，虽然大多数 Arxiv 论文的名称都是“探索自动语音识别系统的基于神经的声学模型中编码的信息”之类的内容，但三元论文却被称为有点不严肃的“1 位法学硕士时代” ：所有大型语言模型均为 1.58 位”。  机器学习中还发生了一些其他不严重的事情，例如“注意力就是你所需要的”论文和我最喜欢的论文名称“Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling”，人们将他们的技术命名为 LASER 和 DRμGS，人们将他们的 LLM 命名为无毒百吉饼、海豚和其他随机的东西 在目前的缓慢时期，您对于 ML 不认真的时期有什么好的建议吗？   由   提交/u/adumdumonreddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b610zm/d_ml_being_unserious/</guid>
      <pubDate>Mon, 04 Mar 2024 03:43:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于在大型科技/对冲基金从事 ML 工作的人员，您认为 alpha 主要来自噪音的百分比是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5zs3p/d_for_people_working_on_ml_at_big_techhedge_funds/</link>
      <description><![CDATA[我还没有对此下定决心，但我的一些同事坚信大多数人并没有使用 ML 获得真正有意义的成果，但正在追寻内部出版偏见。好奇其他公司的人怎么想。   由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5zs3p/d_for_people_working_on_ml_at_big_techhedge_funds/</guid>
      <pubDate>Mon, 04 Mar 2024 02:41:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 在搜索/推荐排名方面有多成功？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5zkhm/d_how_successful_has_rag_been_in/</link>
      <description><![CDATA[有一些努力将检索/排名问题转化为文档 ID 生成问题，例如 2 年前来自 google 的可区分搜索索引论文和微软。我并不完全确定复制这些系统是否值得，尽管它们相当大胆地宣称成功。此外，鉴于 RAG 在聊天机器人中非常有用，是否在搜索/推荐的背景下公开或“启动”了任何可信的重大工作？   由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5zkhm/d_how_successful_has_rag_been_in/</guid>
      <pubDate>Mon, 04 Mar 2024 02:31:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Mamba 论文似乎远离了与 RNN 的明显联系（la Schmidhuber 等人）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5zhrj/d_why_does_the_mamba_paper_seem_to_distance/</link>
      <description><![CDATA[也许我错过了一些东西，但与 RNN 的相似之处很难忽视。然而这篇论文似乎有意提及 RNN。是因为卷积优化使其比顺序 RNN 高效得多吗？或者说 Schmidhuber 是否因为之前过于热心的自我归因而被认为是 ML 社区的弃儿？    由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5zhrj/d_why_does_the_mamba_paper_seem_to_distance/</guid>
      <pubDate>Mon, 04 Mar 2024 02:28:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] 观看 Andrej Karpathy 的视频后手动进行反向传播的笔记</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5zets/p_my_notes_for_doing_back_propagation_manually/</link>
      <description><![CDATA[大家好，我写了一篇关于 Andrej Karpathy 视频的文章：[构建 makemore 第 4 部分：成为反向传播忍者](https://www.youtube.com/watch?v=q8SA3rM6ckI&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAU hRvKZ&amp;索引= 5&amp;t=5798s)。 我花了2天时间才完成代码和注释。我希望它能有所帮助:) ​ 链接：https://lyk-love.cn/2024/03/04/backpropagate%20manually/   由   提交/u/AdministrativeCar545   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5zets/p_my_notes_for_doing_back_propagation_manually/</guid>
      <pubDate>Mon, 04 Mar 2024 02:24:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与传统的 MLP 相比，DCN 有用吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5va25/d_how_useful_is_dcn_compared_to_good_old_mlp/</link>
      <description><![CDATA[在工业大规模搜索/推荐环境中，与更直接的 MLP 相比，DCN 似乎仍然很受欢迎。这个想法或多或少有点像 ResNet，原始的原始输入不断出现在每一层。但是原始输入的层输出的逐元素乘积真的会增加任何价值吗？如果是这样，为什么变压器架构不采用它？   由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5va25/d_how_useful_is_dcn_compared_to_good_old_mlp/</guid>
      <pubDate>Sun, 03 Mar 2024 23:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人形机器人跳舞、高五、挥手、拥抱</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5psws/r_humanoid_robot_dancing_high_five_waving_hugging/</link>
      <description><![CDATA[   /u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5psws/r_humanoid_robot_dancing_high_five_waving_hugging/</guid>
      <pubDate>Sun, 03 Mar 2024 19:37:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] LaVague：开源 Text2Action AI 管道，将自然语言转换为 Selenium 代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/</link>
      <description><![CDATA[      🌊 发布#LaVague，完全开源的 AI 管道，用于转换自然语言到浏览器操作中！ 在不到 150 行代码（具有本地嵌入的 RAG + Hugging Face API 上的 Mixtral）中，它根据用户查询生成 Selenium 代码。在此 GIF 中，您可以看到它按照用户说明命令浏览器浏览 HF 网站！ https:// i.redd.it/vf91c2if25mc1.gif 在 Colab 上尝试：https://colab.research.google.com/github/dhuynh95/LaVague/blob/main/LaVague.ipynb GitHub：https://github.com/dhuynh95/LaVague 非常令人兴奋的是如何创建一个可以执行操作的人工智能助手对于我们来说，例如登录政府帐户、填写表格或提取个人信息！ 周末使用开源工具进行黑客攻击非常有趣，从 Hugging Face 本地嵌入变压器 + HF Pro API，通过 Mistral Mixtral 模型，使用 Llama Index 进行 RAG！ 一些挑战：为了让它在 Colab 上运行，以应对 GPU 较差的，我不得不求助于 HF Inference API Mixtral，因为它是唯一足够好的模型（gemma-7b 没有成功并且拒绝生成代码）。 因为我使用了现成的模型，所以我必须用很少的时间来提高性能通过射击学习和思想链，该模型成功生成了适当的代码！ 潜在的后续步骤：微调 2b 或 7b 模型以完全在本地运行，以便每个人都可以从透明且可定制的 AI 代理中受益为我们采取行动 我可能很难完全发展这个项目，所以我愿意贡献:)  &amp;# 32；由   提交 /u/Separate-Still3770    reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/</guid>
      <pubDate>Sun, 03 Mar 2024 15:38:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>