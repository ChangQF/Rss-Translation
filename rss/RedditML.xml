<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 21 Jun 2024 09:15:03 GMT</lastBuildDate>
    <item>
      <title>[D] FP8 现状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkz9k8/d_fp8_current_state/</link>
      <description><![CDATA[我记得当时对 fp8 训练有一些炒作，但问题是它并没有真正得到支持。我最近检查了一下，似乎仍然没有得到太多支持，尽管 rtf 40 系列以及 h100 都支持 fp8。我只是想知道发生了什么，是不是太不稳定了？pytorch 根本不在乎吗？考虑到现代硬件支持它，这对我来说似乎是一个谜     提交人    /u/ClumsyClassifier   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkz9k8/d_fp8_current_state/</guid>
      <pubDate>Fri, 21 Jun 2024 08:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[D]Open AI JSON模式实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkxya3/d_open_ai_json_mode_implementation/</link>
      <description><![CDATA[如何在 llm 端实现函数调用或 JSON 模式？我想一定有一个 JSON 验证器和分类器。任何想法都值得感激。    提交人    /u/WrapKey69   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkxya3/d_open_ai_json_mode_implementation/</guid>
      <pubDate>Fri, 21 Jun 2024 07:08:25 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 基于 LLM 的 Python 文档，不会触及你的原始代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkxld2/project_llm_based_python_docs_that_never_touches/</link>
      <description><![CDATA[文档编写繁琐且耗时。我认为 LLM 可能是答案，但它们往往会产生幻觉，发明函数或曲解代码。当您尝试记录真实、有效的代码时，这并不理想 所以我创建了 lmdocs。它可以：  从导入的库中引用文档 保证您的原始代码不变 使用 OpenAI 和本地 LLM  我很乐意从其他开发人员那里获得一些反馈。如果您有兴趣，可以在这里查看：https://github.com/MananSoni42/lmdocs 它是开源的，所以请随意贡献或让我知道您的想法。     由    /u/ford_prefect_9931 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkxld2/project_llm_based_python_docs_that_never_touches/</guid>
      <pubDate>Fri, 21 Jun 2024 06:44:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM 中的位置嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkpurw/d_positional_embeddings_in_llms/</link>
      <description><![CDATA[大家好，众所周知，我们使用位置嵌入为 Transformer 提供序列信息。 从我在代码和论文中看到的内容来看，我们引入它的方式取决于所使用的位置嵌入策略。 例如，1. RoPE：将其添加到每个注意层的查询向量中 2. Alibi：将其添加到每层的注意掩码中 3. 可学习的位置嵌入：添加或连接到 token 嵌入 为什么会有这样的差异？具体来说，为什么在每个注意层都引入了 #1 和 #2，而只在第一层引入了 #3？ 谢谢！    提交人    /u/gokstudio   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkpurw/d_positional_embeddings_in_llms/</guid>
      <pubDate>Thu, 20 Jun 2024 23:35:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 好的 TTS 服务??</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkn6hf/d_good_tts_services/</link>
      <description><![CDATA[该死，ElevenLabs 太贵了！我一直在使用 eleven_monolingual_v1，虽然我喜欢它的质量，但价格对我来说太高了。我正在寻找更便宜、听起来不像机器人的 TTS 替代品。我不需要语音克隆或多语言功能。如果它稍差一点，我也可以接受。 将通过 API 调用进行流式传输。有什么建议吗？    提交人    /u/Quiet_Head_404   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkn6hf/d_good_tts_services/</guid>
      <pubDate>Thu, 20 Jun 2024 21:34:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 NeRF 将视频转换为 VR 体验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkmfei/p_using_nerfs_to_convert_videos_to_vr_experiences/</link>
      <description><![CDATA[大家好，我和一些朋友本周末将参加伯克利 AI Hackathon，我们对我们的项目有一个疯狂的想法。我们想使用 AI 将场景视频转换为 VR 体验。理想情况下，这种体验将是“可行走的”，因为我们会将场景加载到 Unity 中，并将场景加载到 VR 耳机上，并允许用户四处走动。我的背景是 NLP，所以我不知道这个项目的可行性。显然，我们可以尝试一些不那么雄心勃勃的变体，例如只为视频添加深度以使其与 Vision Pro 配合使用。我很想听听大家对这个项目的看法；如果有人可以向我发送资源以便我快速学习 NeRF，那就太好了。最近的论文会很棒，任何公共在线课程都会更好。 提前谢谢！    提交人    /u/ekolasky   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkmfei/p_using_nerfs_to_convert_videos_to_vr_experiences/</guid>
      <pubDate>Thu, 20 Jun 2024 21:03:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] PixelProse 16M 密集图像字幕数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkkf9k/p_pixelprose_16m_dense_image_captions_dataset/</link>
      <description><![CDATA[      大家好， 希望大家一切顺利。我们想在这里介绍我们小组的一个新项目。希望你喜欢它。 我们使用密集字幕刷新 CC12M、RedCaps 和 CommonPool，以使用 Gemini-1.0 Pro Vision 生成一个新的 16M 数据集，称为 PixelProse，其中包含超过 16M 对图像和密集字幕。希望它对您的项目有用。  arXiv：https://arxiv.org/abs/2406.10328 huggingface repo：https://huggingface.co/datasets/tomg-group-umd/pixelprose  简介图：来自PixelProse。具体短语以绿色突出显示，负面描述以紫色下划线。    提交人    /u/pidoyu   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkkf9k/p_pixelprose_16m_dense_image_captions_dataset/</guid>
      <pubDate>Thu, 20 Jun 2024 19:37:26 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 寻找使用视频中的人体姿势关键点制作 3D 头像动画的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dke0iv/project_looking_for_resources_on_animating_3d/</link>
      <description><![CDATA[大家好， 我正在做一个项目，我想使用视频中的人体姿势关键点来为 3D 头像制作动画。目标是创建一个模仿儿童舞步的屏幕。我有一些姿势估计的经验，我在 GitHub 上遇到了一个与此相关的项目，名为 DigiHuman。 我正在寻找类似的项目或资源来帮助我入门。这里有人以前做过类似的事情吗？或者你知道任何有用的工具、库或教程吗？任何建议或指示都将不胜感激！ 提前致谢！    提交人    /u/rajanghire534   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dke0iv/project_looking_for_resources_on_animating_3d/</guid>
      <pubDate>Thu, 20 Jun 2024 15:10:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] RLHF 的启动代码库？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkcx6q/r_starter_code_repos_for_rlhf/</link>
      <description><![CDATA[大家好， 我即将开始 LLM 研究，特别是 RLHF。我正在寻找可以作为起点的开放课程库。我发现了以下内容： 1) https://github.com/OpenLLMAI/OpenRLHF 2) https://github.com/huggingface/trl 3) https://github.com/CarperAI/trlx 所有这些似乎都与 transformers 库兼容，而该库又支持完整的开源（代码+数据，而不仅仅是权重）模型，例如 Pythia。所有这些似乎都得到了相当程度的更新。1) 和 3) 支持分布式训练。您会推荐哪一个？还有其他建议吗？ 抱歉，我的问题可能有些幼稚。我是 LLM 新手 :)    提交人    /u/South-Conference-395   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkcx6q/r_starter_code_repos_for_rlhf/</guid>
      <pubDate>Thu, 20 Jun 2024 14:24:42 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 时间序列数据异常检测算法方案思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkc8lu/project_thoughts_on_algorithm_plan_for_anomaly/</link>
      <description><![CDATA[关于时间序列数据中异常检测算法计划的想法 大家好， 我正在研究检测时间序列数据中的峰值，特别是地面磁日数据中的文化遗迹。手动操作需要比较两个或三个地面站，并评估峰值是否同时出现在两个地面站、只出现在一个地面站还是在两个地面站之间移动等，以确定它们是否是文化遗迹。 我想自动执行此任务，因为像显式算法计算（例如，带阈值的滑动窗口）这样的方法太过粗暴。好消息是，我们有超过 15 个项目的原始数据和校正数据（训练数据）。每个项目包括 100 天的地面昼夜数据，每天有 2-3 个地面站。 我已经编译了训练数据，现在正在探索模型选项，非常希望您能提供帮助！ 简而言之：  使用 LSTM 模型： 我的想法是这种算法适合异常检测 它足够灵活，可以处理可变特征，即不同数量的地面站。  实现双流 LSTM 模型： 通过各自的 LSTM 层处理每个地面站。 连接来自 LSTM 层的输出。 使用密集层对组合输出进行分类。  处理不平衡数据： 数据集高度倾斜，其中 99.5% 的标签为 0（正常），只有 0.5% 为 1（异常）。 使用类权重或 SMOTE 技术来平衡数据集。   用于模型训练：  批处理输入数据： 每次数据都有 ~90,000 个点（频率：每秒 10 个数据点），因此批处理在这里是个好主意。  通过 LSTM 层处理： 每个地面站的数据都经过其各自的 LSTM 层。  连接输出： 组合来自 LSTM 层的输出。  使用密集层进行分类：  密集层使用组合输出对每个地面站的数据进行分类。   期待对此方法的任何见解或建议！    提交人    /u/Imarami21   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkc8lu/project_thoughts_on_algorithm_plan_for_anomaly/</guid>
      <pubDate>Thu, 20 Jun 2024 13:55:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我收到 ICML 研讨会的接受推荐后，应该回复审稿人吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dk9jmw/r_should_i_respond_to_reviewers_after_i_got_an/</link>
      <description><![CDATA[我收到了三份评审意见和一份领域主席元评审意见，建议接受 ICLR 研讨会。该论文也将在 PMLR 上发表。 我想知道我是否应该与 OpenReview 的审稿人讨论。由于有“反驳期”，我已经在其他会议上这样做了，但这次提交没有这样的时间。因此感觉讨论部分是不必要的，特别是在它已经被领域主席接受之后。 不过我认为回答他们的问题当然是件好事。我应该花时间在这上面吗？    提交人    /u/howtorewriteaname   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dk9jmw/r_should_i_respond_to_reviewers_after_i_got_an/</guid>
      <pubDate>Thu, 20 Jun 2024 11:42:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要帮助来制定有效的策略来处理机器学习中的不平衡数据集吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dk9a2s/d_need_help_in_effective_strategies_for_handling/</link>
      <description><![CDATA[大家好！ 我正在做一个机器学习项目，正在努力解决不平衡的数据集问题。除了通常的重采样技术之外，你还用过哪些有效的方法来处理这个问题？任何算法级的方法或最近的研究见解都会非常有帮助。提前谢谢了！    提交人    /u/llumo-ai   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dk9a2s/d_need_help_in_effective_strategies_for_handling/</guid>
      <pubDate>Thu, 20 Jun 2024 11:26:43 GMT</pubDate>
    </item>
    <item>
      <title>[N] Ilya Sutskever 和朋友们创立了 Safe Superintelligence Inc.</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1djrs3n/n_ilya_sutskever_and_friends_launch_safe/</link>
      <description><![CDATA[该公司在帕洛阿尔托和特拉维夫设有办事处，将只关注构建 ASI。没有产品周期。 https://ssi.inc    提交人    /u/we_are_mammals   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1djrs3n/n_ilya_sutskever_and_friends_launch_safe/</guid>
      <pubDate>Wed, 19 Jun 2024 19:29:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] [D] 更新于：嗨，我是一名高级机器学习工程师，正在寻找伙伴来一起创造很酷的东西！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1djmpwx/p_d_updated_on_hi_im_a_senior_machine_learning/</link>
      <description><![CDATA[旧帖子：https://www.reddit.com/r/MachineLearning/comments/1dj8pg6/p_d_hi_im_a_senior_machine_learning_engineer/ 哇，我没想到这个帖子会引起这么多的关注！ 我为那些认真对待项目、Kaggle 竞赛或 Leetcode 挑战的人创建了一个 Google 表单。请花一点时间填写。 表格：https://forms.gle/k3jzCfNJy3rgz4ec6 工作方式如下：  我将根据您的目标和专业知识创建小组。 每个小组都会有一个团队负责人来协助进展，并给予我支持。 将所有人组织成团队需要一些时间，所以请耐心等待。我会尽快与您联系。  谢谢！    提交人    /u/Rude-Eye3588   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1djmpwx/p_d_updated_on_hi_im_a_senior_machine_learning/</guid>
      <pubDate>Wed, 19 Jun 2024 15:59:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>