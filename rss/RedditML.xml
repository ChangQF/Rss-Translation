<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 08 Feb 2024 18:16:44 GMT</lastBuildDate>
    <item>
      <title>[R] 长更适合对齐：用于指令微调的简单但难以击败的基线</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am1v5f/r_long_is_more_for_alignment_a_simple_but/</link>
      <description><![CDATA[标题：对齐越长：一个简单但难以击败的指令微调基线 论文：https://arxiv.org/abs/2402.04833 摘要：法学硕士的指令微调需要高质量的数据，这是一个共识，但高质量的数据是什么？ LIMA (NeurIPS 2023) 和 AlpaGasus (ICLR 2024) 是选择此类高质量示例的最先进方法，可以通过手动管理或使用 GPT-3.5-Turbo 作为质量评分器。我们证明，从标准数据集中选择具有最长响应的 1,000 条指令的极其简单的基线可以始终优于根据 GPT-4 和 PaLM-2 作为判断的这些复杂方法，同时在测试事实知识的 OpenLLM 基准上保持竞争力。我们用几个最先进的 Llama-2-7B、Llama-2-13B 和 Mistral-7B）和数据集（Alpaca-52k 和 Evol-Instruct-70k）演示了这一点。此外，对如此长的指令进行轻量级细化可以进一步提高微调后的 LLM 的能力，并使我们能够在 AlpacaEval 2.0 上获得排名第二的基于 Llama-2-7B 的模型，同时仅训练 1,000 个示例和没有额外的偏好数据。我们还对我们的模型进行了彻底的分析，以确保其性能的增强不仅仅是由于 GPT-4 偏好较长的响应，从而排除了任何人为改进。总之，我们的研究结果表明，对最长指令的微调应该成为任何指令微调研究的默认基线。   由   提交 /u/m_andriushchenko   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am1v5f/r_long_is_more_for_alignment_a_simple_but/</guid>
      <pubDate>Thu, 08 Feb 2024 18:01:47 GMT</pubDate>
    </item>
    <item>
      <title>想要建立一个人工智能项目寻求合作或建议 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am16xi/want_to_build_an_ai_project_seeking_for_collab_or/</link>
      <description><![CDATA[想要构建一个我的人工智能版本 想要构建一个我的人工智能版本...就像一个智能代理我的个性和与我相似的思维过程，这样我就可以将该代理放入情境中，看看它如何反应...... 这个想法现在是否太牵强了？我想把它作为我的最后一年的项目，我正在寻找资源来完成它，但我完全迷失了。 非常感谢专家或好心人的帮助，甚至我们可以合作完成这个小项目。  p&gt;   由   提交 /u/Raz--8   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am16xi/want_to_build_an_ai_project_seeking_for_collab_or/</guid>
      <pubDate>Thu, 08 Feb 2024 17:33:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是什么让 PPO 强化学习不仅仅是拥有一个花哨的损失函数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</link>
      <description><![CDATA[我正在考虑使用 RLHF 训练扩散模型，并且正在查看这篇论文 kvablack/ddpo-pytorch：用于微调扩散模型的 DDPO，在具有 LoRA 支持的 PyTorch 中实现（github.com），但代码本身似乎只是反向传播基于unet的一个奇特的（乍一看是可微分的！）损失函数。强化学习与普通模型训练有何区别？两者相同吗？这只是术语问题吗？  在此处复制相关代码？ for i, example in tqdm( list(enumerate(samples_batched)), desc=f&quot;Epoch {epoch}.{inner_epoch} :training&quot;,position=0,disable=notaccelerator.is_local_main_process, ): if config.train.cfg: # 将否定提示连接到示例提示以避免两次前向传递 embeds = torch.cat( [train_neg_prompt_embeds, Sample[&quot;prompt_embeds&quot;) ;]] ) else: embeds = sample[“prompt_embeds”] for j in tqdm( range(num_train_timesteps), desc=“Timestep”,position=1, left=False,disable=not Accelerator.is_local_main_process, ): with Accumulate(unet): with autocast(): if config.train.cfg:noise_pred =unet( torch.cat([sample[&quot;latents&quot;][:, j]] * 2), torch.cat([样本[“时间步长”][:, j]] * 2), 嵌入, ).sample Noise_pred_uncond, Noise_pred_text = Noise_pred.chunk(2) Noise_pred = ( Noise_pred_uncond + config.sample.guidance_scale * (noise_pred_text - Noise_pred_uncond) ) else :noise_pred =unet(sample[“latents”][:, j],sample[“timesteps”][:,j],embeds,).sample#计算当前模型下给定潜伏的next_lateents的对数概率_ , log_prob = ddim_step_with_logprob( pipeline.scheduler,noise_pred,sample[“timesteps”][:, j],sample[“latents”][:,j], eta=config.sample.eta, prev_sample=sample[&quot; ;next_latents&quot;][:, j], ) # ppo 逻辑优点 = torch.clamp(sample[&quot;advantages&quot;], -config.train.adv_clip_max, config.train.adv_clip_max, )ratio = torch.exp(log_prob -样本[“log_probs”][:, j]) unclipped_loss = -advantages * 比率 Clipped_loss = -advantages * torch.clamp(ratio, 1.0 - config.train.clip_range, 1.0 + config.train.clip_range, ) loss = torch .mean(torch.maximum(unclipped_loss, Clipped_loss)) # 调试值 # John Schulman 说 (ratio - 1) - log(ratio) 是一个更好的 # 估计器，但大多数现有代码都使用它，所以... # http:// /joschu.net/blog/kl-approx.html 信息[“approx_kl”].append( 0.5 * torch.mean((log_prob - 样本[“log_probs”][:, j]) ** 2) ) 信息[“clipfrac”].append( torch.mean( ( torch.abs(ratio - 1.0) &gt; config.train.clip_range ).float() ) ) info[&quot;loss&quot;].append(loss) # 向后传递 Accelerator.backward(loss) if Accelerator.sync_gradients: Accelerator.clip_grad_norm_(unet.parameters(), config. train.max_grad_norm ) optimizationr.step() optimizationr.zero_grad()    由   提交 /u/ExaminationNo8522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</guid>
      <pubDate>Thu, 08 Feb 2024 16:49:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 神经常微分方程的替代品？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/</link>
      <description><![CDATA[https://arxiv.org/abs/2401.01836 我看到这篇论文，其中 NODEC 是为了实现未知动力系统的最优控制而实现的，我想知道我们还可以使用哪些其他方法来解决类似的问题。我知道强化学习，但我正在寻找更有效的数据。表征学习或模仿学习可能吗？或者有其他方法可以改善结果？   由   提交/u/Striking-Cricket788  /u/Striking-Cricket788  reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/</guid>
      <pubDate>Thu, 08 Feb 2024 15:57:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离开我的胸膛。我正在攻读机器学习博士学位，但我是个失败者。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</link>
      <description><![CDATA[我的机器学习博士学位已经过半了。 我很幸运，进入了一个很好的项目，尤其是在一个好的项目中。实验室的学生都是超级明星，毕业后会找到很好的工作。我不是他们中的一员。我有一本蹩脚的、技术性不高的出版物，我正在努力寻找一个在我的能力范围内可以解决的新问题。我已经很努力了。我在本科生和硕士期间一直在做研究，尽我所能 - 做项目、阅读论文、学习机器学习和数学课程、为教授撰写资助...... 事实是，我可以达不到产生新想法的水平。无论我多么努力，这都不是我的事。我想为什么。我开始怀疑 STEM 是否一开始就不是我的菜。我环顾四周，发现有些人的大脑只是“理解”了这一点。事情变得更容易。对我来说，这需要额外的努力和额外的时间。在本科期间，我可以更加努力、更长时间地学习。嗯，不是为了博士学位。尤其是在这个快节奏、拥挤的领域，我需要吸收新东西并快速发布。 我是一个冒名顶替者，这不是一种综合症。我快被抓了其他人都获得了多个实习机会等等。我到处都被拒绝。看来现在他们知道了。他们知道我没用。我想对我的顾问说这些，但他是如此的天才，以至于他无法理解普通人的想法。我所有的高级实验室伙伴都是全职工作人员，所以实际上我现在是实验室中最资深的。   由   提交 /u/rsfhuose   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</guid>
      <pubDate>Thu, 08 Feb 2024 15:10:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 鲁棒强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alxn1f/r_robust_reinforcement_learning/</link>
      <description><![CDATA[深度强化学习中对抗性观点的精选阅读列表。  https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alxn1f/r_robust_reinforcement_learning/</guid>
      <pubDate>Thu, 08 Feb 2024 15:00:53 GMT</pubDate>
    </item>
    <item>
      <title>[2402.04882] LMUFormer：具有勒让德内存单元的低复杂性但功能强大的尖峰模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alvgzr/240204882_lmuformer_low_complexity_yet_powerful/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alvgzr/240204882_lmuformer_low_complexity_yet_powerful/</guid>
      <pubDate>Thu, 08 Feb 2024 13:16:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] Bard 现在是 Gemini（免费试用 Gemini Ultra）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alvblw/d_bard_is_now_gemini_free_trial_to_gemini_ultra/</link>
      <description><![CDATA[     &lt; /td&gt; https://preview.redd.it/6tguxx3v1dhc1.png?width=1178&amp;format=png&amp;auto=webp&amp;s=4f1b7b9a25d3a9b2738ad945e8 beb1347cb93447 💻 今天，Google 推出了 Gemini Advanced——一种新的体验，让您可以访问 Ultra 1.0，这是他们最大、最强大的最先进的人工智能模型。在第三方评估者的盲目评估中，与领先的替代方案相比，带有 Ultra 1.0 的 Gemini Advanced 现在是最受欢迎的聊天机器人。 🚀  借助 Google 的 Ultra 1.0 模型，Gemini Advanced 能够更有效地执行高度复杂的任务，例如编码、逻辑推理、遵循细致入微的指令以及在创意项目上进行协作。 🤖 Gemini Advanced 不仅可以让用户进行更长、更详细的对话；它还可以更好地理解之前提示的上下文。例如：  💡 Gemini Advanced 可以成为您的个人导师 - 根据您的学习风格创建分步说明、示例测验或来回讨论。  💻 它可以帮助您实现更高级的编码场景，充当想法的共鸣板并帮助您评估不同的编码方法。  🎨 它可以帮助数字创作者通过生成新鲜内容、分析最新趋势以及集思广益来扩大受众群体的改进方法，从想法到创作。  Gemini Advanced 的第一个版本反映了 Google 目前在人工智能推理方面的进步，并将继续改进。随着他们添加新的专有功能，Gemini Advanced 用户将能够访问扩展的多模式功能、更多的交互式编码功能、更深入的数据分析功能等等。 Gemini Advanced 目前已在 150 多个国家和地区提供英语版本，并且随着时间的推移，Google 会将其扩展到更多语言。 🌍  Gemini Advanced 作为 Google 全新 Google One AI Premium 计划的一部分提供，价格为 19.99 美元/月，从两个月免费试用开始。该计划为用户提供了 Google AI 的最佳功能及其最新进展，以及现有 Google One Premium 计划的所有优势，例如 2TB 存储空间。此外，AI Premium 订阅者很快就能在 Gmail、文档、幻灯片、表格等中使用 Gemini（以前称为 Duet AI）。 📈  Google 继续采取大胆且负责任的方式将这项技术推向世界。而且，为了缓解不安全内容或偏见等问题，他们根据人工智能原则将安全性融入到产品中。在推出 Gemini Advanced 之前，他们进行了广泛的信任和安全检查，包括外部红队检查。他们根据人类反馈，使用微调和强化学习进一步完善了底层模型。 🛡️  Google 听说用户希望以更简单的方式在手机上访问 Gemini。因此，今天他们开始通过 Android 上的新应用程序和 iOS 上的 Google 应用程序为 Gemini 和 Gemini Advanced 推出新的移动体验。 📱    由   提交/u/Kakachia777   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alvblw/d_bard_is_now_gemini_free_trial_to_gemini_ultra/</guid>
      <pubDate>Thu, 08 Feb 2024 13:08:35 GMT</pubDate>
    </item>
    <item>
      <title>《[D]》对Loss函数的质疑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alraes/d_doubt_with_loss_function/</link>
      <description><![CDATA[我有一个多类问题，其中标签向量可以像 [ 0, 1, 0, 0, 1] ，就像在多个类中一样同时是真实的。我应该使用什么样的损失函数，就像在 pytorch Crossentropy 中一样，你只能传递 1 个正确的列作为目标。 我想到的第一件事是将目标向量转换为概率分布，如 [0 , 0.5, 0, 0, 0.5] 并应用 KLDivergence 损失。这个练习或者有什么方法可以将多重正确的标签传递到 pytorch 交叉熵   由   提交/u/Severe_Difficulty_32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alraes/d_doubt_with_loss_function/</guid>
      <pubDate>Thu, 08 Feb 2024 08:45:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 宗师级国际象棋无需搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</guid>
      <pubDate>Thu, 08 Feb 2024 08:23:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于时间序列预测的纯解码器基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alqz7h/r_a_decoderonly_foundation_model_for_timeseries/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alqz7h/r_a_decoderonly_foundation_model_for_timeseries/</guid>
      <pubDate>Thu, 08 Feb 2024 08:22:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</link>
      <description><![CDATA[我读过很多东西说“除非你绝对确定你想读博士”，但我不确定。我的动机主要是为了打开更多的工业大门。我想成为一名研究科学家/机器学习工程师/数据科学，甚至是量化角色，但如果没有高级学位，这些角色越来越难获得。  我应该提一下，我的本科学位不是计算机科学，尽管我有机器学习方面的研究经验，并且选修了一些数学/统计课程（线性算法、统计、概率、计算）。我的问题是： 1) 攻读 4-6 年机器学习博士学位的机会成本（而不是从数据分析等较低入门职位开始并从那里向上工作）是否值得开设更多工业门？  2）我在没有博士学位的情况下获得研究科学家职位的可能性有多大？ 3）我可能还想最终转向初创公司（可能是在工业界之后或立即）。在这种情况下，博士学位并没有多大帮助。但是考虑到一切（事实上我不确定自己会做什么和想要什么），博士学位可以成为弄清楚我想要什么的途径吗？ 我觉得博士学位不值得它仅适用于 DS/MLE 甚至 Quant 角色。更不用说研究科学家了，对于如今的单身汉来说，我什至获得这些职位之一的机会都很难。我申请了大量的数据科学工作，但没有得到回应——这就是为什么我认为博士学位可能最终会引起一些关注。   由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</guid>
      <pubDate>Thu, 08 Feb 2024 05:06:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于音乐生成，我们需要比其他格式更关注 MIDI。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alek80/d_for_music_generation_we_need_to_focus_on_midi/</link>
      <description><![CDATA[嗯，我想每个有音乐创作和制作经验的人都会同意这个标题，对吧？  我以制作音乐为生已经快两年了，我可以处理非常糟糕的 MIDI 文件、非常蹩脚的录音、嘈杂的环境、额外的乐器和类似的事情。所以我学会了如何隔离不同的声音、对其进行均衡、消除噪音并将其投入生产。  这些天，我正在研究生成模型和网站，并发现了一个常见问题。他们输出一个 wav/mp3 文件，其中所有声音都绑定在一起。我知道这对于想要尝试新事物的爱好者来说是有好处的。我也知道，对于一位想在 Instagram 视频上使用音乐而不要求版权的奶奶来说，这是最好的选择，但让我们从音乐家的角度来看这一点。  从音乐家/音频制作人的角度来看，如果这些工具能够为他们提供“可编辑”的功能，那么它们将会非常有帮助。材料。看，我喜欢微调 SD 模型，甚至经营一家为个人和企业提供 SD（一般图像生成）服务的企业，他们向我询问有关 SVG 的问题。为什么？因为 SD 输出很棒，但我们必须从头开始编写模型才能利用 SVG 生成的强大功能，因为 SVG 图像更容易操作和编辑。  音乐生成也是如此。我喜欢 Suno、riffusion 或 MusicGen 给我的 wav 文件。它们很棒（作为由机器从文本组成的东西），但在生产中，它们可能毫无用处。  例如，当您向 MusicGen 请求钢琴独奏时，它很可能是一种听起来像是演奏者用头撞琴键的声音。当你要求失真吉他时，如果你不明确旋律或其他什么，它听起来就像一元店的吹风机！  许多不必要的声音是第一个问题，如果它们超过某个阈值，那么消除它们可能是不可能的。接下来的事情是这些模型没有为我们提供有关赛道的有用信息。好吧，它们只是为了给我们提供该死的波形，这很好，但我的耳朵确实不好，想知道这个音乐的音阶。 那么 MIDI 输出可以解决这个问题吗？是的，它确实。虽然我猜 MIDI 模型将具有非常直接的输出（例如完美的八分音符）并且无法遵循额外的音阶和内容，但您可以轻松地在 DAW 中修复这些问题并将您想要的 soundfont/vst/kontakt 库应用于它们，享受你的音乐。  我还认为，如果我们编写代码将生成的 midi 转换为 wav/mp3 并将其提供给用户，那就太好了！  我对音乐制作方面的事情抱怨了很多，但我也搜索了 MIDI 生成器模型，发现在 HF 和其他模型集线器上也有一些努力。我还没有测试过它们，但我想制作一个新模型并不难，因为 midi 只是音乐代码，我们可以通过微调其中一个文本生成器模型来轻松拥有一个代码生成器。  你觉得怎么样？   由   提交/u/Haghiri75  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alek80/d_for_music_generation_we_need_to_focus_on_midi/</guid>
      <pubDate>Wed, 07 Feb 2024 21:43:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否还有其他人觉得整个员工队伍都因为对 ML 职业提供和期望的不切实际的期望而误入歧途？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1al0fny/d_does_anyone_else_feel_like_theres_an_entire/</link>
      <description><![CDATA[例如，查看此推文，我看到我的网络中的一位（非机器学习）软件工程师分享了该推文：  https://x.com/pwang/status/1753445897583653139?s=20 &lt; p&gt;（对于那些不想点击的人，它得到了一些相当大的积极吸引力，并说“当人类创造 AGI 时，它将被命名为 Untitled14.ipynb”） 我已经最近，我们不得不与一些人合作，他们认为他们可以将一些混乱的数据处理代码从笔记本复制并粘贴到 cronjob 中，并将其称为生产 ML 系统，之后我们不得不处理许多令人沮丧的交互。还有一些人认为，谈论他们从社交媒体上获得的最新前沿研究论文可以很好地替代了解如何很好地实施核心基础知识。 我觉得这些人中的许多人都会如果他们在职业生涯开始时得到适当的支持和建议，这样他们就知道应该投入时间来发展哪些技能，以成为一名决策科学家、研究员或 MLE（或者可能以上都不是，并鼓励他们从事某些事情），那很好。否则他们更擅长）。但相反，他们被告知，他们可以通过成为“介于两者之间的东西”来增加价值——这实际上往往是一种旁观者；不太擅长软件工程、数学，也不欣赏成为该领域研究人员所需的时间和奉献精神（甚至不了解研究人员的贡献）。 我觉得这个行业正在慢慢醒来事实上，这些人只能做出有限的贡献，到那时，很多人将失业或被迫做出不令人满意的选择。这让我感到难过，因为造成这种情况的责任实际上在于那些让他们误入歧途的影响者和那些未能为他们提供所需支持和指导的非技术经理。   由   提交/u/capguard  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1al0fny/d_does_anyone_else_feel_like_theres_an_entire/</guid>
      <pubDate>Wed, 07 Feb 2024 11:02:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>