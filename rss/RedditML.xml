<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 23 May 2024 21:13:09 GMT</lastBuildDate>
    <item>
      <title>[D] 从 HuggingFace 寻找具体文章</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cz3szy/d_looking_for_specific_article_from_huggingface/</link>
      <description><![CDATA[大家好，这可能有点盲目，但事实是这样的： 我看到有人来自 HuggingFace要么分享/喜欢关于极端分类问题的方法（使用 HuggingFace）的帖子，例如在几个镜头设置中，数百甚至数千个课程。我当时没有保存它，而且我再也找不到它了。我尝试过 LinkedIn 搜索，但查找帖子的效果很差。谷歌、HuggingFace 网站、Perplexity 都没有用。这对任何人来说都敲响了警钟吗？    由   提交/u/Medical_Initial  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cz3szy/d_looking_for_specific_article_from_huggingface/</guid>
      <pubDate>Thu, 23 May 2024 21:10:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 光栅到图形</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cz3o5m/r_raster_to_graphics/</link>
      <description><![CDATA[我想调整 vtracer 的超参数。会产生好的结果吗？如果不是，我可以采用哪些其他技术或者我应该遵循什么方法？    由   提交/u/Worldly-Inflation-92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cz3o5m/r_raster_to_graphics/</guid>
      <pubDate>Thu, 23 May 2024 21:04:27 GMT</pubDate>
    </item>
    <item>
      <title>网约车补偿策略[讨论][D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cz376z/compensation_strategy_for_ride_hailing_discussion/</link>
      <description><![CDATA[大家好 我正在研究概念验证，以开发出租车司机薪酬的数学模型。主要目标是增强客户体验。 我提出了如下基本模型。  C=B+R+P+S+T B 为基本费率：B=费率×距离或费率×小时 R 为评级奖金：R=base_bonus×(average_ rating−threshold_ rating) 评级高于某个阈值 P 是高峰时间激励：P=peak_rate×hours_worked_during_peak S 是安全奖金： S=safety_bonus 如果在规定的时间内没有发生事故或罚单 T 是额外服务的小费和其他奖金（如行李帮助等） 此外，我们必须计划该模型的测试策略。 任何有关上述模型的反馈都将受到高度赞赏。  您认为此模型有什么风险？   由   提交 /u/Puzzleheaded_Text780   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cz376z/compensation_strategy_for_ride_hailing_discussion/</guid>
      <pubDate>Thu, 23 May 2024 20:44:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] 介绍 SSAMBA：自监督音频曼巴！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cz1yoa/r_introducing_ssamba_the_selfsupervised_audio/</link>
      <description><![CDATA[嘿 Reddit， 厌倦了变形金刚？你真的需要关注吗？认识 SSAMBA（自我监督音频曼巴）！ 🐍✨  这个无需注意、纯粹基于状态空间模型 (SSM) 的自我监督奇迹不只是嘶嘶声，而是咆哮声！在说话人识别、关键字识别和音频分类等任务上，SSAMBA 比基于 Transformer 的同类产品 (SSAST) 实现了更好或相似的性能。但更重要的是：它的 GPU 内存效率更高，推理速度更快，尤其是在音频长度较长的情况下。好奇吗？请在此处查看完整论文：arXiv 上的 SSAMBA  感谢您的收听！    由   提交 /u/attentionisallyounee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cz1yoa/r_introducing_ssamba_the_selfsupervised_audio/</guid>
      <pubDate>Thu, 23 May 2024 19:53:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] Paperswithcode相关？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cz1t4x/d_paperswithcode_relevant/</link>
      <description><![CDATA[对我来说，paperswithcode 与跟踪 ML 进展的相关性降低了。 但这很难说，在我的领域（表格 ML/DL）没有太多既定的学术基准（还不需要像带有代码的论文之类的东西） 在 NLP 和基础模型空间中，hf 空间中的排行榜已成为一种现象（主要是在 NLP 中） ）。 总体而言，paperswithcode 感觉维护较少且不太有用。 您经常使用paperswithcode 吗？你用它来做什么？它在您的什么领域有用？   由   提交/u/_puhsu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cz1t4x/d_paperswithcode_relevant/</guid>
      <pubDate>Thu, 23 May 2024 19:46:48 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 如果你可以选择 3 篇关于视频/图像生成模型的论文，你会选择哪一篇？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyy7v9/dr_if_you_could_pick_3_papers_about_videoimage/</link>
      <description><![CDATA[我正在攻读硕士学位，我选择做一个视频生成项目。我读过一些关于图像和视频合成的论文：  VQGAN 稳定扩散 Imagen  我还挑选了 3 篇视频生成论文：  Video-LDM 针对多视图生成微调的稳定视频扩散 (SVD-MV) Text2Video-Zero  我还阅读了一些调查论文，这些就是我选择讨论的模型。 我很难选择一篇按逻辑顺序排列的论文，所以我首先解释一下这 3 篇图像生成论文，视频生成论文应该遵循图像合成论文中提到的相同策略。 我可以请您建议写不同的论文吗？我仍然可以将所有论文改成其他内容。 主要是最近的内容（2020-2024 都可以）并且会产生重大影响。我知道例如 VQGAN 是一种流行的基础模型，论文中使用的技术和策略在今天仍然有意义。 然而，Imagen（由谷歌开发）不是开源的，我更喜欢开源代码的论文。这就是我避开 OpenAI 论文的原因。 我还读到，在视频生成中，选择扩散而不是 GAN 是因为它在质量和训练方面都有更好的结果。然而，扩散的计算成本更高。 例如，Video-LDM 基于稳定扩散，所以对我来说，它是值得讨论的好论文。    提交人    /u/ShlomiRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyy7v9/dr_if_you_could_pick_3_papers_about_videoimage/</guid>
      <pubDate>Thu, 23 May 2024 17:18:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变分推理：反向 KL 与正向 KL</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/</link>
      <description><![CDATA[大家好， 我正在研究变分推理方法，主要是在 BNN 的背景下。使用反向（独占）KL 作为变分目标是常见的方法，尽管最近我偶然发现了一些使用前向（包含）KL 作为目标的有趣作品，例如 [1][2][3]。此外，在 GP 的 VI 背景下，两种散度度量均已使用，请参阅 [4]。 虽然我熟悉反向 KL 目标之间众所周知的差异，但它是“模式-”寻求”而正向 KL 是“模式覆盖”，我看到其中一些作品对这些 VI 目标的下游差异提出了主张，例如（此处解释）“反向 KL 低估了预测方差” [4]和“前向 KL 对于受益于保守不确定性量化的应用很有用” [3]. 我有兴趣在 VI 的背景下理解这些下游差异，但还没有找到任何从理论上而不是从经验上解释这些主张的著作。任何人都可以为我指出正确的方向或尝试解释这一点？ 干杯 [1] Naesseth、Christian、Fredrik Lindsten 和 David Blei。 “马尔可夫分数攀爬：KL (p|| q) 的变分推理。” 神经信息处理系统的进展 33 (2020): 15499-15510。 [2] Zhang, L., Blei, D. M., &amp;奈塞斯，C.A.（2022）。传输分数攀登：使用前向 KL 和自适应神经传输进行变分推理。 arXiv 预印本 arXiv:2202.01841。 [3] McNamara, D., Loper, J., &amp; Regier, J.（2024 年 4 月）。用于摊余变分推理中的包容性 KL 最小化的顺序蒙特卡罗。 人工智能与统计国际会议（第 4312-4320 页）。 PMLR。 [4] Bauer, M.、Van der Wilk, M.、&amp;拉斯穆森，C.E.（2016）。了解概率稀疏高斯过程近似。 神经信息处理系统的进展，29。   由   提交/u/DriftingClient  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/</guid>
      <pubDate>Thu, 23 May 2024 14:36:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] Phi-3 模型并排比较。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/</link>
      <description><![CDATA[      https://preview.redd.it/8l04pnfhq62d1.png?width=661&amp;format=png&amp;auto=webp&amp;s=7fe616ca8cd7da97407 0c86b6b47ffab3ab545e5   https://preview.redd.it/hr7fr1uiq62d1.png?width=688&amp;format=png&amp;auto=webp&amp;s=bd3de359bfe4c1ed82d092be92ae38c246bdfda2    https://preview.redd.it/v6k3v39kq62d1.png？ width=450&amp;format=png&amp;auto=webp&amp;s=c0abb0e397a498ef7ccfb35b1b1cb598198f66ad 对于任何想要在一个地方比较 Phi-3 基准的人。 有趣的比较：ANLI、Hellaswag、MedQA、TriviaQA、语言理解、事实知识和稳健性。 注意：Phi-3 迷你模型表的标签顺序不同。   由   提交/u/dark_surfer  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/</guid>
      <pubDate>Thu, 23 May 2024 14:19:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在您自己的云（Azure/AWS/GCP）上部署机器学习模型时，您面临的最大挑战是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyrr8c/d_whats_the_biggest_challenge_you_face_when/</link>
      <description><![CDATA[您好，这是一篇市场研究文章，旨在了解人们在自己的云 (AWS) 上的生产环境中部署开源或自定义 ML 模型时所面临的挑战/Azure/GCP）。 选项：  部署复杂性（K8S、Knative、Ray 等） 根据用户需求自动扩展 缺乏 GPU 可用性（竞价型实例、配额限制） 设置 CI/CD     ;由   提交/u/Capital_Ad1552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyrr8c/d_whats_the_biggest_challenge_you_face_when/</guid>
      <pubDate>Thu, 23 May 2024 12:38:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] ML 数据几何</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/</link>
      <description><![CDATA[        由   提交/u/Late-Yak9284  /u/Late-Yak9284 reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/</guid>
      <pubDate>Thu, 23 May 2024 09:56:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReproModel：开源机器学习研究工具箱。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</link>
      <description><![CDATA[嗨，我是计算机科学博士，我刚刚开发出了我认为机器学习研究的巨大飞跃。我开源了该应用程序供大家查看，欢迎所有反馈和贡献。 ReproModel 是一个无代码工具箱，使科学家和研究人员能够有效地测试和重现 ML 模型。很大一部分研究时间都浪费在测试现有论文中的模型上。要复制或测试结果，您必须查看提供的代码，并模仿所有配置文件和实验条件，即数据加载器、预处理、优化器等。 工具箱将所有这些都拿走了通过从现有论文中获取配置文件（即将推出），直接加载模型，并通过简单的复选框和下拉菜单在数据上测试它们。当然，定制是可能的并且受到鼓励。 您可以在此处找到存储库。当然，还需要做更多的工作，但我正在逐步实现这一点，以确保代码未来的兼容性和可重用性。 https://github.com/ReproModel/repromodel 感谢您的时间、评论和支持！   由   提交 /u/MintOwlTech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</guid>
      <pubDate>Thu, 23 May 2024 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>[项目] YOLOv8量化项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</link>
      <description><![CDATA[我在 Jetson Orin Nano 中量化了 YOLOv8。我用TensorRT（FP16，INT8）导出它并比较性能。基于YOLOv8s，基础模型的mAP50-95为44.7，推理速度为33.1 ms。使用TensorRT（FP16）导出的模型显示mAP50-95为44.7，推理速度为11.4 ms。使用TensorRT（INT8）导出的模型显示mAP50-95为41.2，推理速度为8.2 ms。 mAP50-95略有损失，但推理速度大幅下降。用TensorRT（INT8）导出校准存在问题，但通过增加校准数据将mAP50-95的损失降到最低。我使用 YOLOv8 以及 YOLOv8s 的所有基本模型进行了测试。 https://github.com/ the0807/YOLOv8-ONNX-TensorRT   由   提交/u/Loud-Insect9247   reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</guid>
      <pubDate>Thu, 23 May 2024 06:39:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] Geoff Hinton 目前对反向传播作为大脑学习机制的看法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</link>
      <description><![CDATA[我大约 4 年前观看了他的一次演讲，他在演讲中驳斥了所有反对反向传播作为大脑学习机制的观点。但我记得最近在一个播客上听到过他（现在找不到了），其中他对反向传播持怀疑态度，并且似乎暗示赫布学习更为重要。我很想知道他目前的信念以及原因。他最近一次讨论这个问题的采访或讲座是什么？ /u/geoffhinton 编辑：这是我提到的讲座，名为“大脑会进行反向传播吗？”   由   提交/u/guesswho135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</guid>
      <pubDate>Thu, 23 May 2024 01:58:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI 代理：太早、太昂贵、太不可靠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</guid>
      <pubDate>Wed, 22 May 2024 14:27:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>