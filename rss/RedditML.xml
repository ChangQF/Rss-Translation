<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 11 Feb 2025 15:19:07 GMT</lastBuildDate>
    <item>
      <title>[D] 14B 型号、168GB GPU，每秒仅有 4 个令牌？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in0gwo/d_14b_model_168gb_gpu_and_only_4_tokenssec/</link>
      <description><![CDATA[我在 **7 台机器（每台配备 24GB VRAM，总共 168GB）上运行 DeepSeek-R1-Distill-Qwen-14B 时遇到了性能问题 模型：DeepSeek-R1-Distill-Qwen-14B（14B 参数）  硬件：AWS g6.4xlarge - 7X GPU：7 台机器，每台配备 24GB GPU（总共 168GB VRAM）💪 推理引擎：vLLM 多节点/多 GPU 框架：Ray 精度：测试 FP32 和 FP16  我使用 Ray 进行多节点多 GPU 编排，并使用 vLLM 作为推理引擎。我的速度如下： FP32 → 4.5 个令牌/秒 FP16 → 8.8 个令牌/秒 对于 168GB GPU 集群 上的 14B 模型，这感觉太慢了。我原本期望有更好的性能，但有些东西成为了系统的瓶颈。 我使用的命令 python -m vllm.entrypoints.openai.api\_server \--model /home/ubuntu/DeepSeek-R1-Distill-Qwen-14B \--enable-reasoning \--reasoning-parser deepseek\_r1 \--dtype float16 \--host [0.0.0.0](http://0.0.0.0) \--port 8000 \--gpu\_memory-utilization 0.98 \--tensor-parallel-size 1 \--pipeline-parallel-size 7  我注意到的事情 尽管我已经使用了 98% 的 GPU，但所有 GPU 都没有得到充分利用。 如果您使用过多节点 vLLM 设置，我很想听听您是如何优化性能的。有什么帮助吗？ **我遗漏了什么？**a    提交人    /u/shrijayan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in0gwo/d_14b_model_168gb_gpu_and_only_4_tokenssec/</guid>
      <pubDate>Tue, 11 Feb 2025 14:59:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] GAN 和扩散模型的优化技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imy4wn/doptimization_techniques_for_gans_and_diffusion/</link>
      <description><![CDATA[我正在使用开源 GAN 和扩散模型，但问题是对于我的用例模型来说，它具有很高的推理时间  那么有什么技术可以减少它吗？    提交人    /u/jiraiya1729   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imy4wn/doptimization_techniques_for_gans_and_diffusion/</guid>
      <pubDate>Tue, 11 Feb 2025 13:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 迅速压缩</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imx6bm/d_prompt_compression/</link>
      <description><![CDATA[我有一个相当大的提示，我在其中列出了我想在段落中查找的内容。例如，“以下文本是否包含对数学、统计学、生物学……&lt;段落&gt;&gt; 的引用”。我希望它只输出它能够找到的关键字列表。 问题是，鉴于我希望找到的关键字数量很大，是否可以用两个可学习标记之一替换整个列表？从 dreambooth 获得了这个可学习标记的想法。 很想听听你的想法。如果这已经在论文中完成了就更好了    提交人    /u/themathstudent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imx6bm/d_prompt_compression/</guid>
      <pubDate>Tue, 11 Feb 2025 12:12:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调能赚大钱——怎么做到的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imwnnp/d_finetuning_is_making_big_moneyhow/</link>
      <description><![CDATA[嘿！ 自从我担任计算机视觉研究员以来，我一直在研究 LLM 行业。 与计算机视觉任务不同，似乎许多公司（尤其是初创公司）都依赖于基于 API 的服务，例如 GPT、Claude 和 Gemini，而不是像 Llama 或 Mistral 这样的自托管模型。我还在这个 subreddit 中看到了很多讨论微调的帖子。 这让我很好奇！据报道，Together AI 的 ARR 已达到 1 亿美元以上，令我惊讶的是，微调似乎是其主要的收入驱动因素之一。微调是如何为如此高的收入数字做出贡献的？公司是否在大力投资它以获得更好的性能、数据隐私或成本节省？ 那么，为什么要微调模型而不是使用 API（GPT、Claude 等）？我真的想知道。  很想听听您的想法 - 提前谢谢！    提交人    /u/Vivid-Entertainer752   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imwnnp/d_finetuning_is_making_big_moneyhow/</guid>
      <pubDate>Tue, 11 Feb 2025 11:41:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 循环潜在推理：在不生成标记的情况下扩展语言模型中的测试时间计算</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imwkns/r_recurrent_latent_reasoning_scaling_testtime/</link>
      <description><![CDATA[我发现这篇论文的主要贡献是重新思考我们如何通过连续循环处理而不是离散层来扩展推理过程中的计算。作者建议将模型深度视为一个连续参数，可以在推理时动态调整。 主要技术要点： - 引入“循环深度” - 允许信息多次循环通过组件 - 将深度建模为连续参数而不是离散层 - 使用微分方程的原理来创建平滑的信息流 - 根据任务复杂性实现自适应计算 主要结果： - 匹配较大模型的性能，同时减少 30-40% 的计算量 - 与传统架构相比，显示出更稳定的训练动态 - 展示了跨处理步骤的改进的信息保留 - 通过增加推理迭代实现了一致的性能扩展 我认为这种方法可以帮助解决我们扩展语言模型的一些基本效率低下问题。我们可以通过更智能的处理更好地利用现有参数，而不是简单地扩大模型。对深度的连续处理还为在部署期间平衡计算与性能提供了更大的灵活性。 我认为最大的挑战将是在实践中有效地实现这一点，特别是对于并行处理。与传统的前馈架构相比，循环性质增加了复杂性。然而，计算节省可能使它对许多应用程序来说是值得的。 TLDR：本文建议将神经网络深度视为连续的而不是离散的，使用循环处理在推理过程中更有效地扩展计算。显示出有希望的结果，在保持性能的同时减少了 30-40% 的计算量。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imwkns/r_recurrent_latent_reasoning_scaling_testtime/</guid>
      <pubDate>Tue, 11 Feb 2025 11:35:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 项目 A：患者安全与学习的道德 AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imr4yx/p_project_a_ethical_ai_for_patient_safety_learning/</link>
      <description><![CDATA[作为一名拥有医院实践经验的实习护士，我见证了技术在哪些方面能够产生真正的影响，以及在哪些方面无法满足患者和医护人员的需求。医院目前面临的最大问题之一是患者跌倒：这个问题每年造成数十亿美元的损失，延长住院时间，并增加本已不堪重负的护士的工作量。虽然存在跌倒预防策略，但大多数策略仅依靠人工观察和人工干预，这在高压力环境下并不总是可行的。 我正在开展一项非营利性计划，旨在开发一种可穿戴贴片，用于跟踪患者运动、预测跌倒风险并监测实时生命体征，包括心率 (HR)、呼吸频率 (RR)、皮肤温度、血氧饱和度 (SpO₂)（如果可能）和心电图监测。该系统将使用人工智能驱动的分析在跌倒发生之前提供预警，为护士提供主动工具来预防患者受伤并减轻员工负担。 这不是另一家专注于利润的人工智能驱动的初创公司，而是一项旨在将患者、护士和道德人工智能放在首位的非营利计划。我们的人工智能不会利用患者数据，不会取代医护人员，也不会危及安全。相反，我们正在构建一个可扩展、负责任的系统，该系统与医院工作流程相结合，使医疗保健更加安全。 现在，我一个人在做这件事，但我需要人工智能/机器学习工程师、生物医学工程师、软件工程师和人工智能伦理专家来实现它。虽然我还没有资金，但我知道，一旦我们有了一个可行的原型，获得合适的资金将容易得多。如果这个系统在一家医院取得成功，它可以扩展到全球的医疗保健系统，防止数千次跌倒，为医院节省数十亿美元，并减少护士的倦怠。 除了医疗保健之外，我相信这种道德人工智能方法还可以改善现代教育。如果我们成功为医院创建负责任的人工智能，我们可以将同样的理念应用于支持学生和教师的教育系统，而不会取代人类的学习。 如果您对道德人工智能充满热情，并希望在医疗保健领域做出真正的改变，让我们一起创造伟大的东西。在下面给我发消息或评论，我很乐意合作。    提交人    /u/FatCockroachTheFirst   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imr4yx/p_project_a_ethical_ai_for_patient_safety_learning/</guid>
      <pubDate>Tue, 11 Feb 2025 05:12:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用潜在推理扩展测试时间计算：一种循环深度方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imr031/r_scaling_up_testtime_compute_with_latent/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imr031/r_scaling_up_testtime_compute_with_latent/</guid>
      <pubDate>Tue, 11 Feb 2025 05:04:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我对知识蒸馏的实验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imodbb/p_my_experiments_with_knowledge_distillation/</link>
      <description><![CDATA[嗨 r/MachineLearning 社区！ 我对知识提炼进行了几次实验，想分享我的发现。以下是比较教师、学生、微调和提炼模型性能的结果片段：   数据集 Qwen2 模型系列 MMLU（推理） GSM8k（数学） WikiSQL（编码）            1 预训练 - 7B 0.598 0.724 0.536   2 预训练 - 1.5B 0.486 0.431 0.518   3 微调 - 1.5B 0.494 0.441 0.849   4 Distilled - 1.5B, Logits Distillation 0.531 0.489 0.862   5 Distilled - 1.5B, Layers Distillation 0.527 0.481 0.841   如需详细分析，您可以阅读本报告。 我还创建了一个开源库以促进其采用。您可以在此处尝试。 我的结论：当目标数据集上的较大模型和较小模型之间存在很大差距时，优先选择蒸馏而不是微调。在这种情况下，蒸馏可以有效地传递知识，从而比单独的标准微调产生更好的性能。 附注：本博客文章对蒸馏进行了高级介绍。 让我知道你的想法！    提交人    /u/darkItachi94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imodbb/p_my_experiments_with_knowledge_distillation/</guid>
      <pubDate>Tue, 11 Feb 2025 02:44:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 mHuBERT 模型追踪到 jit</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imkdnf/p_tracing_mhubert_model_into_a_jit/</link>
      <description><![CDATA[嗨， 我将 mHuBERT 模型追踪到 jit 中，因此很容易从语音中提取离散的“语义”标记。在此过程中，我偶然发现了一些意想不到的事情，也学到了一些关于 FAISS 聚类库的知识。我决定将其包装成一篇文章以防万一。 如果您需要离散语音标记，请随意使用此处的跟踪模型：https://huggingface.co/balacoon/mhubert 您可以在博客文章中了解有关该过程的更多信息：https://balacoon.com/blog/mhubert_tracing/（包含对跟踪和测试笔记本的引用） 来自 hubert 或 wav2vec 的离散标记通常用作多模态 LLM 的音频输入。希望你会发现这很方便    提交人    /u/clementruhm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imkdnf/p_tracing_mhubert_model_into_a_jit/</guid>
      <pubDate>Mon, 10 Feb 2025 23:24:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 预训练对 LLM 中的 RL 的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imhxzh/d_pretrainings_effect_on_rl_in_llms/</link>
      <description><![CDATA[是否有人知道任何研究显示不同的预训练和 RL 计算预算之间的动态和相互作用以及对最终模型智能的影响？例如，固定 RL 预算，各种预训练模型大小如何响应 RL？我的直觉是会有一些指数曲线，但我认为我没有看到任何显示这一点的图表。    提交人    /u/H2O3N4   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imhxzh/d_pretrainings_effect_on_rl_in_llms/</guid>
      <pubDate>Mon, 10 Feb 2025 21:41:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 扩展研讨会论文的常见做法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imaq6g/r_common_practice_when_extending_a_workshop/</link>
      <description><![CDATA[因此，我之前曾被 ICML 研讨会接受过一篇论文。现在，我基本上有相同的论文（问题陈述等），但我提出了一种不同的损失，基本上可以让我获得我在研讨会论文中可以获得的所有内容，但效果更好，而且 - 重要的是 - 让我可以将该方法应用于其他数据集和数据类型（例如 3D）而不仅仅是 MNIST（这是我的研讨会论文）。 我想尽快将其提交给会议。我应该怎么做？在 arxiv 中创建一个具有不同标题的新预印本？还是只是用这个版本更新预印本？研讨会论文已经发表。 我很怀疑，因为整体结构与以前相同。改变的是一些关键的数学知识，以及额外的实验和更好的结果。    提交人    /u/howtorewriteaname   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imaq6g/r_common_practice_when_extending_a_workshop/</guid>
      <pubDate>Mon, 10 Feb 2025 16:52:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] KL 散度是 LLM 后训练 RL 的主要奖励吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im7bsb/d_kl_divergence_as_a_primary_reward_in_llm/</link>
      <description><![CDATA[假设我们预训练了一个 LLM。如果我们使用该预训练的 LLM 生成一个序列，我们无法获得具有最佳 KL 散度的序列。这就是之前出现波束搜索的原因。那么如果我们执行 RL，其中纯 KL 散度是奖励模型，会怎么样？生成的模型将生成比预训练的 LLM 具有低得多的整体 KL 散度的序列。会发生什么？模型会“更连贯”吗？ 我想听听大家对此的看法，因为这似乎是一个似乎可以得出简单答案的思想实验，但序列的 KL 散度是一个目标，如果没有非线性优化 (RL)，实际上很难解决。是的，我们直接知道 token 概率，但要知道预训练模型“偏好”的序列的累积概率就困难得多。这感觉像是一个不对称优化问题（易于评估，但难以解决），我想知道它是否会产生任何有意义的结果。 我的实现想法是只使用 GRPO 进行 RL。但你们觉得呢？    提交人    /u/RiceCake1539   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im7bsb/d_kl_divergence_as_a_primary_reward_in_llm/</guid>
      <pubDate>Mon, 10 Feb 2025 14:27:55 GMT</pubDate>
    </item>
    <item>
      <title>深度学习笔记本电脑 PhD [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</link>
      <description><![CDATA[嗨， 我有 2,000 英镑，需要在 3 月之前用这笔钱买一台笔记本电脑（否则我会失去资金）用于攻读应用数学博士学位，这涉及大量深度学习。我所做的大部分工作可能都会在云端进行，但考虑到我有这个预算，我最好买一台最好的笔记本电脑，以防我需要离线运行某些东西。 请问可以给我一些购买建议吗？我不想买 Mac，但所有的选择让我有点困惑。我知道新的 GPU（nvidia 5000 系列）刚刚发布，并且已经宣布推出采用 Lunar Lake / Snapperagon CPU 的新款笔记本电脑。 我不确定我是否应该购买具有不错 GPU 的产品，或者只是购买像 lenove carbon x1 这样的轻薄超极本。 谢谢你的帮助！ **编辑： 我可以通过我的大学访问 HPC，但在使用它之前，我宁愿确保我的项目能够在我自己创建的玩具数据集或 MNIST、CFAR 等上运行。因此，除了推理之外，这意味着我可能会在我的笔记本电脑上进行一些轻度训练（老实说，这也可能在云端）。所以问题是我是否应该选择会耗尽电池并增加体积的 GPU，还是选择轻薄的 GPU。 我一直使用 Windows，因为我不喜欢软件，所以这不是什么问题。虽然我从未更新到 Windows 11，因为担心出现错误。 我有一台几年前用 rx 5600 xt 组装的台式电脑 - 我认为现在这已经非常过时了。但这意味着我不会将笔记本电脑停靠在上面，因为我已经有一台台式电脑了。    提交人    /u/Bloch2001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</guid>
      <pubDate>Mon, 10 Feb 2025 11:37:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>