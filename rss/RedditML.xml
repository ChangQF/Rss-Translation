<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 10 Dec 2024 21:16:46 GMT</lastBuildDate>
    <item>
      <title>[D] 逆神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/</link>
      <description><![CDATA[大家好，我想问一下你们是否知道当前最好的监督逆神经网络是什么？我知道 GAN、VAE 和条件 VAE。 基本上，我的目标是确定满足输出值的多元函数的输入值，例如，找到 x=[x1,...,xN] 使得 0.2=f(x)。 我的主要专业是工程学（不是机器学习），我对该领域的了解非常有限。但是，我很擅长阅读你建议的任何研究论文。 谢谢大家， 编辑：抱歉，这个例子令人困惑。f 不是多元 pdf 而是一个“多元函数”f : R^N -&gt; R。具体来说，f 是一个“单变量” pdf 具有 N-1 个参数。    提交人    /u/zonanaika   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/</guid>
      <pubDate>Tue, 10 Dec 2024 20:34:05 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 在 Google Cloud Vortex AI 上测试已部署的 TensorFlow 模型时收到“无法将 TF 方言降低为 CoreRT 方言”错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hba31i/discussion_received_a_failed_to_lower_tf_dialect/</link>
      <description><![CDATA[嗨， 有人知道我该如何解决这个错误吗？ 这是我所做的： 我使用 TensorFlow 在本地训练和评估了我的模型并保存了它。之后，我将模型部署到 Google Cloud 并创建了一个端点进行测试。但是，当我尝试通过端点测试模型时，收到以下错误：  &quot;error&quot;: &quot;failed to lower TF Dialect to CoreRT Dialect.:0: error: loc(fused[&quot;Reshape:&quot;, &quot;function_1_1/function_1/flatten_1/Reshape@__inference_serving_default_12233&quot;, &quot;StatefulPartitionedCall/StatefulPartitionedCall/function_1_1/function_1/flatten_1/Reshape&quot;]): &#39;tf.Reshape&#39; op 需要 &#39;shape&#39; 最多有一个动态维度，但在索引 0 和 1\n 处有多个动态维度：0：注意：loc(fused[&quot;Reshape:&quot;, &quot; functional_1_1 / functional_1 / flatten_1 / Reshape @ __inference_serving_default_12233 &quot;, &quot; StatefulPartitionedCall / StatefulPartitionedCall / functional_1_1 / functional_1 / flatten_1 / Reshape &quot;]): 查看当前操作：％78 = &quot; tf.Reshape &quot; (％77，％57) {device = &quot; / job：localhost / replica：0 / task：0 / device：CPU：0 &quot;} ：（tensor &lt; * xf32 &gt;，tensor &lt; 2xi32 &gt;）-&gt;张量&lt;*xf32&gt;\n&quot;  这是我的模型摘要： 层（类型）输出形状参数＃=========================================================================== input_image（InputLayer）[（None，160，160，3）] 0 conv2d（Conv2D）（None，160，160，32）896 max_pooling2d（MaxPooling2D（None，80，80，32）0）conv2d_1（Conv2D）（None，80，80，64） 18496 max_pooling2d_1 (MaxPooling (None, 40, 40, 64) 0 2D) conv2d_2 (Conv2D) (None, 40, 40, 128) 73856 max_pooling2d_2 (MaxPooling (None, 20, 20, 128) 0 2D) conv2d_3 (Conv2D) (None, 20, 20, 256) 295168 max_pooling2d_3 (MaxPooling (None, 10, 10, 256) 0 2D) flatten (Flatten) (None, 25600) 0 dropout (Dropout) (None, 25600) 0 density (Dense) (None, 256) 6553856 dropout_1（Dropout）（无，256） 0 density_1（Dense）（无，128） 32896 dropout_2（Dropout）（无，128） 0 density_2（Dense）（无，5） 645  提前感谢你的帮助 我还检查了测试输入图像的形状，它具有与训练和评估图像相同的形状 160*160。注意：测试图像是编码的 base64 图像 json 文件。 一开始我也怀疑这可能是因为谷歌云（2.9）和我用于创建模型本身的版本（2.18）之间的框架版本差异，但我也尝试了 2.9，但仍然有同样的问题。    提交人    /u/is_albus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hba31i/discussion_received_a_failed_to_lower_tf_dialect/</guid>
      <pubDate>Tue, 10 Dec 2024 19:32:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] Articulate Anything：从任何输入方式自动生成可交互的 3D 资产</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/</link>
      <description><![CDATA[📦 Frontier AI 能否将任何物理对象从任何输入模式转换为也可以移动的高质量数字孪生？很高兴分享我们的工作 Articulate-Anything，探索大型视觉语言模型 (VLM) 如何弥合物理世界和数字世界之间的差距。 Articulate-Anything 🐵 是一种最先进的方法，可以从任何输入模式（包括文本、图像或视频）自动创建可交互的 3D 资产。  网站：articulate-anything.github.io 论文：https://arxiv.org/abs/2410.13882 代码：https://github.com/vlongle/articulate-anything 请参阅我的推特帖子：https://x.com/int64_le/status/1866519866934714623 深入研究该方法    由    /u/Prudent_Fly_1004  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/</guid>
      <pubDate>Tue, 10 Dec 2024 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从失业到 Lisp：在青少年的深度学习编译器上运行 GPT-2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/</link>
      <description><![CDATA[几个月前，我失业了，不知道下一步该做什么。我想从系统的角度更多地了解深度学习。在学习了吴恩达的监督学习课程后，我渴望更多地了解像 Pytorch 或 Tinygrad 这样的深度学习框架（或深度学习编译器）。 我开始研究 Tinygrad，从我在网上找到的教程中学习，我发现它很有趣，因为它是一个真正的编译器，它采用传统的 Python 代码并将其转换为抽象语法树，然后将其解析为 UOps 和 ScheduleItems，最终拥有一个代码生成层。虽然设计很有趣，但代码很难读。 就在那时，我偶然发现了一些完全出乎意料的东西，一个基于 Common Lisp 构建的深度学习编译器，由一名 18 岁的日本年轻人在他的间隔年期间维护。目前我们已经完成了一件伟大的事情，它可以运行 gpt2！ 目前，它只是生成 C 内核，但未来我们希望支持 cuda codegen 以及许多其他功能，并作为任何想要使用 Common Lisp 进行深度学习编译器工作的人的学习工具。 这是一个开源项目，欢迎任何人做出贡献！ https://github.com/hikettei/Caten    提交人    /u/yCuboy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/</guid>
      <pubDate>Tue, 10 Dec 2024 18:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么阻止您使用基础模型进行时间序列预测？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/</link>
      <description><![CDATA[我一直在尝试基础模型，例如 Sulie、Granite TTM 和 Amazon Chronos，每个模型都有自己的优势。真正令人着迷的是，使用零样本方法可以更快地获得准确的预测。然而，尽管这些模型改进了预测，但与更易于解释的 ARIMA 等更传统的方法相比，可解释性仍然是一个重大挑战。 我很好奇 - 您认为可解释性是决定性因素吗，或者还有其他原因导致基础预测模型没有得到更广泛的采用？很想听听您在使用这些模型时遇到的最大障碍或挑战是什么。    提交人    /u/Queasy_Emphasis_5441   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/</guid>
      <pubDate>Tue, 10 Dec 2024 18:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于供暖系统机器学习的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/</link>
      <description><![CDATA[您好，在传统的加热过程中，您可以控制何时开始加热以及何时使用目标值停止加热。一旦达到目标值，它将停止并一遍又一遍地启动整个过程，同时还会控制一些泵阀（有多少水需要循环）。 在过去 5 年（每分钟），我已经在时间序列数据库中捕获了几个室温、所有启动/停止以及加热自动化软件所做的泵阀调整。 我正在尝试创建一个具有恒定目标值的模型，例如 23°C。该模型的输入是加热系统状态（开/关）、泵阀位置和当前室温。输出应该是如何设置加热系统状态并调整阀门位置以达到并保持室温的目标值。在最好的情况下，它应该完全取代加热自动化软件。其他的就是仅仅建议或监督这个过程。 我想到了一些问题，我不知道该如何处理：  这个过程很慢，一旦开始加热，由于温度变化缓慢，1 小时后才能看到结果。那么对行动的评估必须延迟吗？ 我捕获的数据包含历史温度，但我认为它可能有缺陷。数据中的温度已经受到现有加热系统的影响。我没有温度数据显示在没有任何加热系统的情况下室温是如何变化的。这是学习的问题吗？我需要创建合成数据吗？ 训练一个模型来输出“开始加热/停止加热”（将其余部分留给传统的加热自动化）还是控制加热状态和泵阀本身会更好？ 什么是最好的机器学习技术，例如无监督，强化学习？     提交人    /u/QuickYogurt2037   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/</guid>
      <pubDate>Tue, 10 Dec 2024 16:15:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 这个数据集到底有多难？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/</link>
      <description><![CDATA[新论文提醒！ 分类自动编码器测量分类难度并检测标签错误 我们倾向于认为训练分类器的挑战是由超参数调整或模型创新来处理的，但数据及其嵌入中存在丰富的固有信号。了解机器学习问题的难度一直很难。现在不再如此。 现在，您可以计算分类数据集的难度，而无需训练分类器，并且每个类别只需要 100 个标签。而且，这个难度估计与数据集大小出奇地无关。 传统上，数据集难度评估方法耗时和/或计算密集型，通常需要训练一个或多个大型下游模型。更重要的是，如果你在数据集上训练具有特定架构的模型并实现特定的准确度，则无法确定你的架构是否完全适合手头的任务 - 可能是一组不同的归纳偏差会导致模型更轻松地学习数据中的模式。 我们的方法为每个类训练一个轻量级自动编码器，并使用重建误差的比率来估计分类难度。在 100k 样本数据集上运行此数据集难度估计方法只需几分钟，并且不需要调整或自定义处理即可在新数据集上运行！ 效果如何？我们对 19 个常见的视觉数据集进行了系统研究，将我们的方法估计的难度与 SOTA 分类准确度进行了比较。除了一个异常值外，相关性为 0.78。它甚至适用于医疗数据集！ 论文链接：https://arxiv.org/abs/2412.02596 GitHub Repo Linked in Arxiv pdf    提交人    /u/ProfJasonCorso   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/</guid>
      <pubDate>Tue, 10 Dec 2024 16:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] 理解 Transformer 在图形搜索中的局限性：学习和扩展行为的机制分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/</link>
      <description><![CDATA[本文通过研究 transformer 如何处理图连通性问题，解决了关于 transformer 学习搜索算法的能力的一个基本问题。作者开发了一种新颖的解释方法来分析 Transformer 如何逐层处理搜索操作。 关键技术要点：- 使用图可达性作为测试用例，具有可控的复杂性和无限的训练数据- 开发解释技术来了解 Transformer 层如何计算可达顶点集- 发现 Transformer 学习随着深度呈指数级扩展搜索边界- 展示了基于图大小的明显扩展限制- 表明上下文学习（思路链）无法克服这些限制 主要结果：- 小型 Transformer 在经过适当训练后可以学习基本搜索- 每一层计算先前可达顶点及其邻居的并集- 随着图大小的增加，性能急剧下降- 添加参数并不能解决扩展问题- 模型在处理超出其训练分布的图时遇到困难 我认为这项工作揭示了 Transformer 中重要的架构限制，我们需要为需要搜索功能的应用程序解决这些限制。缩放行为表明，对于更大的搜索空间，而不仅仅是更大的模型，我们可能需要从根本上不同的方法。 我认为他们开发的解释方法对于理解 Transformer 如何处理除图形之外的其他类型的结构化数据很有价值。关于扩展限制的明确经验结果应该为涉及搜索类计算的应用程序的架构选择提供参考。 TLDR：Transformer 可以学习基本的图形搜索操作，但在扩展方面面临根本限制。添加更多参数无济于事，这表明我们需要新的方法来解决复杂的搜索问题。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/</guid>
      <pubDate>Tue, 10 Dec 2024 13:37:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 模型出处：您如何追踪您的 ML 模型谱系？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/</link>
      <description><![CDATA[嘿 r/MachineLearning，我很好奇这个社区的人们是如何处理模型出处的 - 在整个生命周期中跟踪机器学习模型的谱系和演变的实践。  您目前是否使用任何工具或方法来跟踪您的 ML 模型的出处？ 如果是，您使用什么解决方案？它们是定制的还是现成的？ 如果不是，您是否认为您的工作需要这样的工具？ 您认为模型出处解决方案中的哪些功能必不可少？     提交人    /u/crtahlin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/</guid>
      <pubDate>Tue, 10 Dec 2024 12:29:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] The Well：机器学习的大规模多样化物理模拟集合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/</link>
      <description><![CDATA[数据集：https://github.com/PolymathicAI/the_well 论文：https://arxiv.org/pdf/2412.00568 摘要：  基于机器学习的替代模型为研究人员提供了加速基于模拟的工作流程的强大工具。然而，由于该领域的标准数据集通常涵盖一小部分物理行为，因此很难评估新方法的有效性。为了解决这一差距，我们引入了 Well：一个包含各种时空物理系统的数值模拟的大规模数据集集合。 The Well 汇聚了领域专家和数值软件开发人员的智慧，提供了 16 个数据集的 15TB 数据，涵盖生物系统、流体动力学、声散射以及河外流体或超新星爆炸的磁流体动力学模拟等不同领域。这些数据集可以单独使用，也可以作为更广泛的基准套件的一部分使用。为了方便使用 The Well，我们提供了一个统一的 PyTorch 界面来训练和评估模型。我们通过引入示例基线来演示该库的功能，这些示例基线突出了 The Well 复杂动态所带来的新挑战。代码和数据可在此 https URL 上找到。     提交人    /u/StartledWatermelon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/</guid>
      <pubDate>Tue, 10 Dec 2024 10:49:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你是如何跟上文学发展的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/</link>
      <description><![CDATA[标题基本就是这个意思。你用什么工具/策略来跟上文献？ 编辑：为了便于理解，我是一名一年级博士生，我指的是特定“利基”领域的文献（如果除了极少数例外，你可以在 ML 中将任何东西称为利基的话）    提交人    /u/Rickmaster7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/</guid>
      <pubDate>Tue, 10 Dec 2024 03:20:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何管理和追踪庞大且不断演变的图像数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/</link>
      <description><![CDATA[我想知道人们如何管理大型内部数据集的生命周期？比如说 &gt;1TB 和 100k 个文件。 在我的新角色中，我们有多个生产模型，这些模型是从内部数据集训练出来的，大小从几千到几十万张图像不等。我们还有大量新数据进入，每天超过 100 万张图像，因此我们不断挖掘这些数据并发送新的部分进行注释。 到目前为止，团队基本上都是自己管理这个问题，结果是可以预测的。在某些情况下，我们无法将我们的生产模型与任何特定数据关联起来。我们的一些核心数据集仅存在于人们的主目录中，很容易被一个错误的命令抹去。对于一个幸好被淘汰的模型，已知训练代码和原始训练数据都丢失了。 该组织的部分成员采用了 DVC，这似乎很不错，直到文件数量或整体大小变大。一方面，有些人将整个数据集塞进几个档案中并跟踪它们。这最大限度地减少了哈希带来的挫败感，但当只有几个文件更新时会占用大量存储空间。另一方面，有些人跟踪每个文件，这样可以单独更新文件，但签入和签出非常麻烦。其他人则将这两种方法的差异分开，以分层方式跟踪数据集的块作为档案。 那么你的组织是如何管理这一点的？在处理这些庞大且不断发展的数据集时，什么有效，什么无效？    提交人    /u/SirPitchalot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/</guid>
      <pubDate>Tue, 10 Dec 2024 00:09:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人成功训练过具有模型并行性的 LLM 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/</link>
      <description><![CDATA[您好， 我正在为我的硕士论文研究微调 Llama-3.1。不幸的是，我目前的情况不允许使用高内存 GPU，例如 A100。相反，我可以使用具有多个低内存 GPU 的设置，例如 4×3090 或 8×V100。 因此，我需要实现模型并行性来训练我的模型，因为它不适合单个 GPU。但是，我注意到大多数框架主要关注数据并行性，这并不能满足我的需求。 有没有人通过将模型拆分到多个 GPU 上来成功训练模型？如果有，您能推荐我应该探索的框架或方法吗？我特别想寻找完整的培训，尽管我有兴趣听听是否有人使用 LoRA 管理过这个。 此外，如果有更适合此类问题的 subreddit，请引导我到那里。 谢谢！    提交人    /u/anilozlu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/</guid>
      <pubDate>Mon, 09 Dec 2024 15:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 08 Dec 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>