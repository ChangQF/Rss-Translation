<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 04 Feb 2025 09:17:19 GMT</lastBuildDate>
    <item>
      <title>[D] 使用多模态对比损失与微调 LLaVa 结合 ViT 和 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihdk1s/d_combining_a_vit_and_llm_using_multimodal/</link>
      <description><![CDATA[我有一个非常擅长对医学图像进行分类的 ViT，我想将它用于 VLM，以根据图像 + 患者临床信息输出报告。  我的想法是，我可以以某种方式将 ViT 与 llama3 或其他具有医学知识的 LLM 结合起来，就像我假设 LLaVa 或 CLIP 使用多模态对比损失或线性投影所做的那样。这可能更适合添加医学知识，但我的数据集没有全文报告。我只有带有简短文本标题的图像。 但是，我也可以微调 LLaVa 或其他 VLM。我不确定这是否会使 VLM 拥有足够的医学知识，但我认为它会更能够遵循指示（即 VQA）。 有什么好方法可以将真正优秀的医学 ViT 与 LLM 结合起来制作 VLM？或者将 ViT 和 LLM 结合起来不是一个好的选择？    提交人    /u/Amazydayzee   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihdk1s/d_combining_a_vit_and_llm_using_multimodal/</guid>
      <pubDate>Tue, 04 Feb 2025 08:59:51 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 跳跃推理曲线？追踪 GPT-[n] 和 o-[n] 模型在多模态拼图上的推理性能演变</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihd6vl/research_the_jumping_reasoning_curve_tracking_the/</link>
      <description><![CDATA[      o1 比 GPT4o 有所改进，但在简单的抽象推理方面仍然存在很大困难。 o1 的改进几乎是 GPT-4o 计算成本的 750 倍。 https://preview.redd.it/e2nzbrf923he1.png?width=1172&amp;format=png&amp;auto=webp&amp;s=d46bd36ee6d33acc6b828b7f84b833899e6d82ce 无法理解简单模式 https://preview.redd.it/f0lmamae23he1.png?width=2492&amp;format=png&amp;auto=webp&amp;s=70d97e9e8311ca9fb300215400ca23146274e3ab 感知仍然是 o1 的主要瓶颈： https://preview.redd.it/y5ctbbeg23he1.png?width=2380&amp;format=png&amp;auto=webp&amp;s=fd9ea081d188be8d5617b469865deee37d5d7efb 更多详情：https://arxiv.org/abs/2502.01081   由    /u/sgpfc  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihd6vl/research_the_jumping_reasoning_curve_tracking_the/</guid>
      <pubDate>Tue, 04 Feb 2025 08:31:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 联邦学习讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihcyo4/d_discussion_on_federated_learning/</link>
      <description><![CDATA[最近几天一直对联邦学习框架很感兴趣，我一直在为其开发一个 POC 模型，以实现分散式学习。 我想知道其他人的想法，我在这方面确实没有太多专业知识，但我发现使用分散式学习进行无监督学习的概念相当令人着迷。 如果我要开发这样一个框架，会对它有什么期望？    提交人    /u/Critical_Pipe1134   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihcyo4/d_discussion_on_federated_learning/</guid>
      <pubDate>Tue, 04 Feb 2025 08:13:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] llms 是如何微调的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihcgf1/d_how_are_llms_fine_tuned/</link>
      <description><![CDATA[我听说有人对 LLM 进行微调以使其变得更好，该怎么做呢？    提交人    /u/getmesometho   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihcgf1/d_how_are_llms_fine_tuned/</guid>
      <pubDate>Tue, 04 Feb 2025 07:35:45 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何创建一个接受 Restful Apis 项目文件的软件，使用 ML 模型自动生成 OpenAPI 格式的文档</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihc4ap/p_how_to_create_a_software_that_accept_project/</link>
      <description><![CDATA[大家好，我正攻读计算机科学学位的最后一年，对于我的最终项目，我提出了一个解决方案，该解决方案接受使用 Spring Boot 或 Node + Express 制作的 Restful API 的项目文件并对其进行分析，然后自动为其生成 OpenAPI 格式的文档。我知道已经有像 SwaggerUI 这样的东西可以做到这一点，我希望我的解决方案是一个更好的选择.. 文档应包含基本详细信息，例如请求类型、输入数据、输出数据、格式等。 因此，我确定项目有两个部分， 首先，我必须分析代码：（模型、视图、控制器） 然后使用该信息来生成文档。 我以前没有使用过 ML 概念，因此我不知道我的起点在哪里。 请帮我找到某种路线图或关于如何完成我的项目的建议。    提交人    /u/NevaDeS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihc4ap/p_how_to_create_a_software_that_accept_project/</guid>
      <pubDate>Tue, 04 Feb 2025 07:10:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 我的论文需要对 ML 和 AI 相关项目的建议。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihc1cr/d_p_need_suggestion_for_ml_and_ai_related/</link>
      <description><![CDATA[我是计算机科学专业四年级学生，需要准备我的毕业论文。我需要给一位导师留下深刻印象，他是我的论文团队主管，与 AI 有关。 我有一些经验，因为我已经完成了 NLP 和模式识别课程，还使用 ​​Python 中的机器学习完成了一个完整的 AI 和 NLP 项目。 您能否建议我一些 AI 项目，我可以做这些项目来给我的导师留下深刻印象，让他相信我有能力轻松完成论文并完成论文答辩？我只需要给他留下深刻印象，让他在接下来的两个学期担任我的导师。谢谢。    提交人    /u/FixNervous5345   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihc1cr/d_p_need_suggestion_for_ml_and_ai_related/</guid>
      <pubDate>Tue, 04 Feb 2025 07:04:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 HuggingFace 问题的 BERT 嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ih2ywl/d_bert_embeddings_using_huggingface_questions/</link>
      <description><![CDATA[我正在尝试查找带有操作码的反汇编文件的 BERT 嵌入。反汇编文件的示例： add move sub ...（等等） 该文件将包含几行操作码。我的目标是找到一个代表整个文件的嵌入向量（用于下游任务，例如分类/聚类）。 对于 BERT，主要有两个部分：标记器和实际的 BERT 模型。我很困惑 512 的上下文大小是用于标记器还是实际模型。我问这个问题的原因是，我可以将所有操作码提供给标记器（可能有数千个操作码），然后将它们分成块（如果需要，可以有一些重叠），然后将每个块提供给 BERT 模型以找到该块的嵌入*吗？或者我应该先将操作码分成块，然后再对它们进行标记？ 这是我目前拥有的代码：```py def tokenize_and_chunk(opcodes, tokenizer, max_length=512, override_percent=0.1): &quot;&quot;&quot; 先将所有操作码标记为子字，然后分成有重叠的块 Args: opcodes (list): 操作码字符串列表 tokenizer: Hugging Face tokenizer max_length (int): 最大序列长度 override_percent (float): 块之间的重叠百分比 返回: BatchEncoding: 包含 input_ids、attention_mask 等。&quot;&quot;&quot; # 使用列表推导将所有操作码标记为子词 all_tokens = [token for opcode in opcodes for token in tokenizer.tokenize(opcode)] # 计算分块参数 chunk_size = max_length - 2 # 考虑[CLS] 和 [SEP] step = max(1, int(chunk_size * (1 - override_percent))) # 使用海象运算符生成重叠块 token_chunks = [] start_idx = 0 while (current_chunk := all_tokens[start_idx:start_idx + chunk_size]): token_chunks.append(current_chunk) start_idx += step # 将标记块转换为模型输入 return tokenizer( token_chunks, is_split_into_words=True, padding=&#39;max_length&#39;, truncation=True, max_length=max_length, return_tensors=&#39;pt&#39;, add_special_tokens=True )  def generate_malware_embeddings(model_name=&#39;bert-base-uncased&#39;, override_percent=0.1): &quot;&quot;&quot; 使用 BERT 和重叠标记块生成嵌入 &quot;&quot;&quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModel.from_pretrained(model_name).eval() embeddings = {} malware_dir = MALWARE_DIR / &#39;winwebsec&#39; for filepath in malware_dir.glob(&#39;*.txt&#39;): # 使用海象运算符读取操作码 with open(filepath, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: opcodes = [l for line in f if (l := line.strip())] # 标记并分块重叠coded_chunks = tokenize_and_chunk( opcodes=opcodes, tokenizer=tokenizer, max_length=MAX_LENGTH, override_percent=overlap_percent ) # 使用推理模式批量处理所有块 with torch.inference_mode(): output = model(**encoded_chunks) # 计算有效的标记掩码 input_ids =coded_chunks[&#39;input_ids&#39;] valid_mask = ( (input_ids != tokenizer.cls_token_id) &amp; (input_ids != tokenizer.sep_token_id) &amp; (input_ids != tokenizer.pad_token_id) ) # 处理每个块的嵌入 chunk_embeddings = [outputs.last_hidden_​​state[i][mask].mean(dim=0).cpu().numpy() for i, mask in enumerate(valid_mask) if mask.any() ] # 跨块取平均值（无规范化） file_embedding = np.mean(chunk_embeddings, axis=0) if chunk_embeddings \ else np.zeros(model.config.hidden_​​size) embeddings[filepath.name] = file_embedding return embeddings  ``` 如您所见，代码首先对操作码调用 tokenize()，将它们拆分成块（有重叠），然后对所有带有 is_split_into_words=True 标志的块调用标记器的 __call__ 函数。这是正确的方法吗？这会对操作码进行两次标记吗？ * 另外，我的目标是找到整个文件的嵌入。为此，我计划取所有块的平均嵌入。但对于每个块，我应该取每个标记的平均嵌入吗？或者只取 [CLS] 标记的嵌入？    提交人    /u/_AnonymousSloth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ih2ywl/d_bert_embeddings_using_huggingface_questions/</guid>
      <pubDate>Mon, 03 Feb 2025 23:07:46 GMT</pubDate>
    </item>
    <item>
      <title>改变旧记忆或过去对话的标记方法是否有助于增加 LLM 的上下文长度？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1igxiqh/would_changing_the_tokenization_method_for_older/</link>
      <description><![CDATA[所以我在考虑标记器并阅读了一些相关资料。我主要想找到一个问题的答案，即 LLM 是否可以同时使用多种不同的标记方法。例如，同时使用单词和子单词标记。或者将单词转换为“词性”，并将其与标记信息一起输入到 LLM 中。无论如何，在此过程中，一个问题突然出现在我的脑海中。能否通过使用更高级别的标记方法以某种方式模拟旧记忆？例如单词级别标记与子单词（或相反）。我假设准确性或能力会相应改变，但大概会影响回忆或上下文长度，对吧？     提交人    /u/TheRealBobbyJones   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1igxiqh/would_changing_the_tokenization_method_for_older/</guid>
      <pubDate>Mon, 03 Feb 2025 19:26:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 通过加权和抽样实现标签平衡</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1igt9xk/d_label_balancing_with_weighting_and_sampling/</link>
      <description><![CDATA[我有一个非常不平衡的数据集，其中最频繁的标签比最不频繁的标签大约高出 400 倍。因此，我在训练中使用加权方法来消除模型的偏差（一个数据点的单独损失是 actual_loss_of_the_datapoint*1/frequency_of_label）。 我注意到我的模型性能似乎仍然偏向更频繁的标签。因此，我想知道我当前的加权方法是否太弱，我应该改用采样方法（上采样/下采样）。在消除模型偏差方面，加权损失是否比上采样/下采样效果更差？（执行 actual_loss_of_the_datapoint*1/frequency_of_label 可能不等同于对我的所有数据进行上采样，对吧？）    提交人    /u/EndangeredCephalopod   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1igt9xk/d_label_balancing_with_weighting_and_sampling/</guid>
      <pubDate>Mon, 03 Feb 2025 16:36:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用算法蒸馏扩展跨域动作模型的上下文强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1igl19m/r_scaling_incontext_reinforcement_learning_with/</link>
      <description><![CDATA[我刚刚阅读了这篇关于动作建模的新论文，其中介绍了一种将上下文 RL 与连续噪声蒸馏相结合的有趣方法。关键技术贡献是使用基于 Transformer 的架构，该架构通过两阶段过程学习动作表示：初始特征提取与噪声蒸馏，然后通过 RL 进行上下文细化。 主要技术组件和结果：  连续噪声蒸馏：一种在模型训练期间从视频数据中滤除不相关特征的新技术 上下文内动作学习：使用 Transformer 注意机制来捕获动作序列中的时间关系 结果：与以前的方法相比，动作识别准确率提高了 27%，训练速度提高了 35% 跨领域评估：在涵盖机器人、人类动作和游戏环境的新数据集上进行了测试  实现细节：- 多层注意架构，具有针对动作理解不同方面的专门层 - 结合监督学习和 RL 微调的两阶段训练过程 - 自定义损失函数平衡特征提取和时间连贯性- 与现有视觉变换器主干集成 我认为这种方法对于实时动作理解至关重要的机器人应用特别有用。更快的训练时间和更高的准确性使其可以实际部署在生产系统中。跨域性能表明它可能很好地推广到新任务。 但是，我认为计算要求可能会限制立即广泛采用。本文指出训练期间 GPU 内存使用率很高。在将其用于安全关键型应用之前，还需要解决复杂动作序列的性能下降问题。 TLDR：使用上下文 RL 和噪声蒸馏的新动作建模方法实现了 27% 的更好准确度和 35% 的更快训练速度，在机器人和自动化系统中具有潜在的应用。 完整摘要在这里。论文此处。    由    /u/Successful-Western27 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1igl19m/r_scaling_incontext_reinforcement_learning_with/</guid>
      <pubDate>Mon, 03 Feb 2025 09:07:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 使用不准确的标签进行训练时测量模型性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1igi8nu/d_p_measuring_model_performance_when_training/</link>
      <description><![CDATA[我正在一个没有标记数据的领域工作。例如，数据将是寻找异常使用情况的网络流量模式。由于没有基本事实数据，我们经常依赖一组启发式方法，这些方法可以帮助我们根据数据点中特定特征违反的基于启发式规则阈值的数量获得每个数据点的分数。 我想要构建的解决方案的目标是识别启发式规则被充分触发的类似模式，同时也捕获启发式规则不足以将数据点识别为异常但表现出较新的异常模式的案例数据点。 我遇到的问题是 - 我如何衡量这个模型的性能。目前，违反任何单一启发式的数据点都被认为是坏的（即异常）。但是经典的机器学习模型评估说我需要每个数据点的“是/否”，我目前正在使用 F 分数来检查模型性能。这种衡量模型性能的方式是，如果没有触发任何基于启发式的规则，则会惩罚将数据点归类为异常的模型。 这个问题可以用不同的方式来表达 -  如何向我的模型添加较新的基于启发式的规则，以使其不会过时/陈旧 如何衡量此模型的性能，以便识别较新的模式不会受到惩罚。  那么我该如何处理呢？我不需要确切的答案 - 我觉得这应该是一个定义明确且经过探索的问题空间，如果有人可以建议我应该搜索哪些术语才能找到该空间的正确材料/论文，那将非常有用。    提交人    /u/PsychologicalRide127   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1igi8nu/d_p_measuring_model_performance_when_training/</guid>
      <pubDate>Mon, 03 Feb 2025 05:46:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究人员使用哪些软件工具来制作这样的神经网络架构？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ig6k3l/d_which_software_tools_do_researchers_use_to_make/</link>
      <description><![CDATA[        提交人    /u/SimpleObvious4048   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ig6k3l/d_which_software_tools_do_researchers_use_to_make/</guid>
      <pubDate>Sun, 02 Feb 2025 20:19:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] [P] 使用大型概念模型研究 KV 缓存压缩</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ig0z7h/r_p_investigating_kv_cache_compression_using/</link>
      <description><![CDATA[大家好，假期期间我阅读了 Meta 的论文，其中介绍了大型概念模型，并认为这可能是压缩 KV 缓存的有效方法。我在 TPU v4-32s 上的 Jax 中实现并训练了一个 LCM 架构，以探索其在 KV 缓存压缩方面的潜力。完整实现和详细结果可在此处获得。 主要发现：虽然理论上很有希望，但基础 LCM 架构表现出明显的性能下降。我怀疑以下原因会导致这种退化：  序列打包损害了概念嵌入语义，阻碍了有效的注意 联合编码器-解码器训练将计算浪费在概念形成上，而不是利用预训练的知识 由于 LCM 在 seq_len/concept_size 示例上进行训练，而标准 Transformer 中的 seq_len 则不然，因此有效训练减少  值得探索的潜在改进：  禁用序列打包 利用预训练的编码器/解码器（SONAR/T5） 研究有/无联合训练的基于扩散的 LCM  但是，考虑到基本的数据效率问题，替代的 KV 缓存压缩方法可能更有前景。 上面的链接中有实现细节和完整分析。欢迎讨论和反馈。     由    /u/clankur 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ig0z7h/r_p_investigating_kv_cache_compression_using/</guid>
      <pubDate>Sun, 02 Feb 2025 16:27:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Feb 2025 03:15:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>