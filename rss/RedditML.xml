<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sun, 05 Jan 2025 15:14:43 GMT</lastBuildDate>
    <item>
      <title>[D] 随机 SVD/PCA 用于有效注意力机制——有潜力吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hu168k/d_randomised_svdpca_for_efficient_attention/</link>
      <description><![CDATA[这个想法在我脑子里已经萦绕了一段时间，很想听听大家对它是否有潜力的意见——有太多值得关注的效率改进提案，我已经记不清哪些已经尝试过，哪些还没有尝试过！ 该过程的效果如下：  首先像平常一样计算键和查询 然后，对查询进行随机 PCA，以确定查询空间中最大的 D 个组成部分。  对于 D 个最大组件中的每一个，保留与该组件最匹配的密钥向量 定期关注这些密钥。  鉴于长度为 N 的序列的典型注意力具有复杂度 O(N^2)，而随机 PCA 为 O(D^2)，因此这里可能会节省相当大的推理时间。 我看不到任何现有的研究表明这是否有用。LoRA 和 Linformers 接近，因为它们也使用低秩近似，但我认为我提出的建议是独一无二的。有什么见解吗？    提交人    /u/enjeyw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hu168k/d_randomised_svdpca_for_efficient_attention/</guid>
      <pubDate>Sun, 05 Jan 2025 07:55:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人类智能存在于大数据领域，还是小数据领域？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</link>
      <description><![CDATA[当今前沿的 LLM 拥有数万亿个参数，并在 500 万亿个 token 上进行训练。 人类大脑拥有 860 亿个神经元和 100 万亿个突触。 任何人消耗的文本信息量都比 LLM 所训练的少几个数量级。但是，人眼以大约 10Mbps 的速率捕获视觉信息。加上听觉、触觉、平衡感、嗅觉等其他感官，人类儿童在生命的最初几年消耗的信息量比任何 LLM 所见过的都要多。 这似乎表明人类智能需要大数据。 但是那些从出生就失明的人怎么办？先天性聋盲（没有记录在案的病例）怎么办？    提交人    /u/Gear5th   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</guid>
      <pubDate>Sun, 05 Jan 2025 06:07:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 无法规划，但可以在 LLM-Modulo 框架中帮助规划</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hty2jr/r_llms_cant_plan_but_can_help_planning_in/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hty2jr/r_llms_cant_plan_but_can_help_planning_in/</guid>
      <pubDate>Sun, 05 Jan 2025 04:57:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 巴洛双胞胎如何避免因仿射变换而不同的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htpuuv/r_how_barlow_twins_avoid_embeddings_that_differ/</link>
      <description><![CDATA[我正在阅读 Barlow Twins (BT) 论文，但就是不明白它如何避免以下情况。 当互相关矩阵等于单位矩阵时，BT 损失最小化。实现这一点的必要条件是对角线元素 C_ii 为 1。这可以通过 2 种不同的方式实现。对于每个 x：  zA=zB zA=a⋅zB+b  其中 zA 和 zB 是同一输入 x 的不同增强的嵌入。换句话说，嵌入可以不同，但​​这种差异被掩盖了：corr(X,aX+b)=corr(X,X)=1。 直观地说，如果我们的目标是学习对扭曲不变的表示，那么应该避免第二种解决方案。有什么想法可以驱动网络避免这种情况吗？   由    /u/Seiko-Senpai  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htpuuv/r_how_barlow_twins_avoid_embeddings_that_differ/</guid>
      <pubDate>Sat, 04 Jan 2025 22:10:28 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 寻找深度学习模型失败的输入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htp9tv/project_finding_inputs_where_deep_learning_models/</link>
      <description><![CDATA[大家好！上个月在 NeurIPS（机器学习会议）上，我读到了一篇有趣的论文“人类在算法预测中的专业知识”，该论文描述了一个框架，用于确定机器学习模型在哪些方面的表现优于人类专家。我发现作者的工作非常有趣。下面，我将进一步探讨他们的框架，并将其扩展到多类分类。我的结果非常令人惊讶，表明一组现代模型架构在 CIFAR-10 中处理狗和猫时遇到了麻烦。 GitHub 链接：https://github.com/sunildkumar/model_indistinguishability 论文链接：https://arxiv.org/abs/2402.00793    提交人    /u/dragseon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htp9tv/project_finding_inputs_where_deep_learning_models/</guid>
      <pubDate>Sat, 04 Jan 2025 21:44:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 受邀演讲会公开吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htohxj/d_will_neurips_invited_talks_be_made_public/</link>
      <description><![CDATA[大家好， 2024 年 neurips 尚未向公众开放受邀演讲，未注册者也可以访问： https://neurips.cc/virtual/2024/eventlistwithbios/invited%20talk 参加了上届 neurips 的人：你们可以在线访问演讲吗？如果可以，这是否意味着今年的演讲不会公开？ 2023年、2022年公开： https://neurips.cc/virtual/2023/eventlistwithbios/invited%20talk https://neurips.cc/virtual/2022/events/Invited%20Talk 谢谢！    提交人    /u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htohxj/d_will_neurips_invited_talks_be_made_public/</guid>
      <pubDate>Sat, 04 Jan 2025 21:09:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我建立了一个庞大的数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htocnj/r_ive_built_a_big_ass_dataset/</link>
      <description><![CDATA[我清理/处理并合并了大量患者信息数据集，每个数据集都会询问患者关于他们自己的各种问题。我还知道他们是否患有这种疾病。我有他们 10 年前对所有问题的回答，以及他们现在或最近的回答，以及他们现在和 10 年前的疾病状况。我找不到任何论文做过如此大规模的研究，我感觉自己坐在一袋钻石上，但我不知道如何打开它。您认为最好的方法是什么？如何最大限度地利用它？我知道很多都是关于我的最终目标是什么，但我真的想知道其他人会先做什么！ （我有 2500 名患者和 27 个数据集，其中包含最早记录和最新记录。因此有 366 个特征，每个特征一个最新，一个最早，大约有 200 万个细胞。）想知道您的想法    提交人    /u/Disastrous_Ad9821   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htocnj/r_ive_built_a_big_ass_dataset/</guid>
      <pubDate>Sat, 04 Jan 2025 21:03:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何在运动规划（机器人）中考虑防撞？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hto1rl/r_how_to_consider_collisionavoidance_in_motion/</link>
      <description><![CDATA[大家好， 我正在启动一个研究项目，重点是设计一个 ML 模型，用于使用协作机器人 (cobot) 进行自动精加工任务（例如抛光、去毛刺、磨削）中的运动规划。 该模型将接受以下输入：  工作单元、工件、工具和机器人的 CAD 近似值 刀具路径 碰撞矩阵  所需输出有两个方面：  工件的最佳位置 机器人的运动轨迹  我可用的训练数据有限，但我不确定选择哪种 ML 模型来确保有效集成防撞功能。我正在考虑的一个选项是在已经考虑防撞和机器人运动学的输出上训练模型。但是，我不完全确定如何实施这种方法，或者它是否是最有效的方法。 有人对我如何解决这个问题有什么想法吗？或者，您是否知道任何探讨类似主题的文章或资源？ 提前感谢您的见解！    提交人    /u/BathroomEast3868   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hto1rl/r_how_to_consider_collisionavoidance_in_motion/</guid>
      <pubDate>Sat, 04 Jan 2025 20:50:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 实现 StyleGAN2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htm15z/p_implementing_the_stylegan2/</link>
      <description><![CDATA[[P] 大家好，我最近一直在写一个名为“StyleGAN2 之路”的博客系列，现在我终于开始写 StyleGAN2 了。我在这里写了一篇文章：https://ym2132.github.io/StyleGAN2 我的目标是讲解论文、代码和训练过程。希望您觉得它有用，如果您能提供任何反馈，我将不胜感激 :)    提交人    /u/throwaway16362718383   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htm15z/p_implementing_the_stylegan2/</guid>
      <pubDate>Sat, 04 Jan 2025 19:22:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 思想证明：神经符号程序合成可实现稳健且可解释的推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htkg5l/r_proof_of_thought_neurosymbolic_program/</link>
      <description><![CDATA[大型语言模型 (LLM) 彻底改变了自然语言处理，但它们在推理不一致方面仍存在困难，尤其是在新领域和复杂逻辑序列中。这项研究引入了思想证明，这是一个增强 LLM 输出可靠性和透明度的框架。我们的方法将 LLM 生成的想法与形式逻辑验证联系起来，使用自定义解释器将 LLM 输出转换为一阶逻辑构造以供定理证明器审查。我们方法的核心是基于 JSON 的中间领域特定语言，它在设计上平衡了精确的逻辑结构和直观的人类概念。这种混合表示既可以进行严格的验证，又可以方便人类理解 LLM 推理过程。主要贡献包括具有排序管理的强大类型系统，以增强逻辑完整性，明确表示规则以明确区分事实和推理知识，以及灵活的架构，可轻松扩展到各种特定领域的应用程序。我们通过对 StrategyQA 和一项新颖的多模态推理任务进行基准测试，证明了思想证明的有效性，并在开放式场景中表现出了更好的性能。通过提供可验证和可解释的结果，我们的技术满足了人工智能系统问责制的关键需求，并为高风险领域的人在环监督奠定了基础。 Arxiv 论文    提交人    /u/North-Ad-9741   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htkg5l/r_proof_of_thought_neurosymbolic_program/</guid>
      <pubDate>Sat, 04 Jan 2025 18:13:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2024 年值得关注的人工智能研究论文（第一部分）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htedss/p_noteworthy_ai_research_papers_of_2024_part_one/</link>
      <description><![CDATA[        由    /u/seraschka 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htedss/p_noteworthy_ai_research_papers_of_2024_part_one/</guid>
      <pubDate>Sat, 04 Jan 2025 13:28:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我为 TensorFlow 和 Keras 编写了优化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htch73/p_i_wrote_optimizers_for_tensorflow_and_keras/</link>
      <description><![CDATA[大家好，我为 TensorFlow 和 Keras 编写了优化器，它们的使用方式与 Keras 优化器相同。 https://github.com/NoteDance/optimizers    提交人    /u/NoteDancing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htch73/p_i_wrote_optimizers_for_tensorflow_and_keras/</guid>
      <pubDate>Sat, 04 Jan 2025 11:25:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果你不断要求 LLM 写出更好的代码，他们能写出更好的代码吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ht2m3y/d_can_llms_write_better_code_if_you_keep_asking/</link>
      <description><![CDATA[https://minimaxir.com/2025/01/write-better-code/ 这是一个理论实验，其结果有趣。简而言之，答案是肯定的，这取决于您对“更好”的定义。    提交人    /u/minimaxir   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ht2m3y/d_can_llms_write_better_code_if_you_keep_asking/</guid>
      <pubDate>Sat, 04 Jan 2025 01:12:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>