<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 13 Feb 2024 15:12:20 GMT</lastBuildDate>
    <item>
      <title>[R] [P] 通过贝叶斯优化将 LLM 评估速度提高 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/</link>
      <description><![CDATA[最近我一直致力于通过使用贝叶斯优化来选择合理的子集来快速进行 LLM 评估。 贝叶斯优化是使用它是因为它有利于探索/利用昂贵的黑匣子（释义，LLM）。 项目链接&lt; /p&gt; 我很想听听您对此的想法和建议！   由   提交 /u/b06901038g   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/</guid>
      <pubDate>Tue, 13 Feb 2024 14:51:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 为什么目标检测模型的对手看起来与图像分类器不同</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/</link>
      <description><![CDATA[大家好。我只是想看看图像分类器上的对抗性攻击的行为，并决定也尝试使用对象检测器。我注意到对这些模型的无针对性的对抗性攻击产生了一些感兴趣的面具。图像分类器生成了通常预期的流行噪声掩模，但相同条件下的对象检测器生成了与所讨论的对象非常相似的掩模。这背后的原因是什么？感谢您的帮助！ 第一张图片是对象检测器的对抗性掩模，第二张图片是图像分类器的对抗性掩模，最后一张图片是原始图片。    由   提交/u/tatteredsky  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/</guid>
      <pubDate>Tue, 13 Feb 2024 14:48:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] End Sem 项目的想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apv0zl/p_ideas_for_end_sem_project/</link>
      <description><![CDATA[需要对 Gen AI 或任何其他可以使用法学硕士的领域的 End Sem 项目有帮助的想法。   由   提交/u/Optimal_Land5276   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apv0zl/p_ideas_for_end_sem_project/</guid>
      <pubDate>Tue, 13 Feb 2024 14:41:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何用多个数据集训练 LSTM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apu9u7/p_how_to_train_lstm_with_multiple_datasets/</link>
      <description><![CDATA[我正在尝试训练 LSTM 以解决时间序列预测的流体动力学问题。我有 4 个不同的数据集，每个数据集都有 2400 个时间步长和 6 个特征。我想训练 lstm 使其能够学习所有 4 个数据集的共同动态（因为它们在相同参数下运行本质上是不同的）。有人可以建议如何解决这个问题吗？ 我正在使用 keras，并且已经能够使用单个数据集成功地训练和预测 seq2seq 类型预测。   由   提交/u/Born-Plankton2373  /u/Born-Plankton2373 reddit.com/r/MachineLearning/comments/1apu9u7/p_how_to_train_lstm_with_multiple_datasets/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apu9u7/p_how_to_train_lstm_with_multiple_datasets/</guid>
      <pubDate>Tue, 13 Feb 2024 14:06:03 GMT</pubDate>
    </item>
    <item>
      <title>[2402.07901] FAST：加速 Transformer 的可分解注意力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/</guid>
      <pubDate>Tue, 13 Feb 2024 12:00:51 GMT</pubDate>
    </item>
    <item>
      <title>[R]与向量相比，PFAS 可能是更好的单词嵌入空间，甚至可能更高的计算效率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/</link>
      <description><![CDATA[https://huggingface.co/blog/TuringsSolutions /pfafresearch 所以这个博客引起了大多数人的注意，但基本上与向量相比，它捕获了更多的单词信息，实际上，当放大时，它可能会比向量更好。与向量相比，它具有更多的维度。其他几何表示形式也正在研究中，例如三角形空间、正方形空间等，以找到单词关系的最佳几何编码，与向量相比，这种编码可能更好地建模信息。它在某些基准测试中的得分略有提高，因此作为概念证明，它似乎确实有效。从本质上讲，它的工作原理是将单词转换为分形样式的数字嵌入，以捕获更丰富的信息。 https://arxiv。 org/abs/2402.06184 具有讽刺意味的是，最近训练也被可视化为分形，因此 word2fractal 风格的嵌入过程也被理论化也就不足为奇了。    由   提交/u/TheCrazyAcademic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/</guid>
      <pubDate>Tue, 13 Feb 2024 05:13:48 GMT</pubDate>
    </item>
    <item>
      <title>[2402.07839]通过最佳传输实现元剪枝</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apl6im/240207839_towards_metapruning_via_optimal/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apl6im/240207839_towards_metapruning_via_optimal/</guid>
      <pubDate>Tue, 13 Feb 2024 04:49:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] 您将如何训练序列结束预测模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apk6mc/p_how_would_you_train_a_endofsequence_prediction/</link>
      <description><![CDATA[所以我试图让模型预测序列结尾，例如“你今天好吗？”我出去散步了，感觉棒极了！”，目标是预测“棒极了！”是序列中正确的最后一个单词。我现在正在通过进行二元分类来接近它，负样本是正样本的子序列，因此在这种情况下负样本将是[“怎么样”，“怎么样”，“你怎么样”，.. .， &quot;今天怎么样？我出去散步，感觉很不错”]。我认为不幸的是，这使得总数据中 3% 是正样本，97% 是负样本，因此模型总是被激励去预测负样本，以达到 97%。有哪些方法可以解决这个问题？或者有没有更好的方法来表达这个问题来避免这种情况？你们在解决这样的问题时有什么办法吗？   由   提交 /u/DolantheMFWizard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apk6mc/p_how_would_you_train_a_endofsequence_prediction/</guid>
      <pubDate>Tue, 13 Feb 2024 03:57:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] 跟踪医疗保健领域会议截止日期</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/</link>
      <description><![CDATA[        由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/</guid>
      <pubDate>Tue, 13 Feb 2024 01:14:39 GMT</pubDate>
    </item>
    <item>
      <title>您的 RAG 设置中有什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/</link>
      <description><![CDATA[您在 RAG 中使用哪些框架和库？  我最好奇LangChain是否像以前一样受欢迎？ 这是我的高级版本：   langchain使用OpenAI用于创建嵌入 Pinecone 用于存储嵌入 langchain 用于加载文档分割器和字符分割器以进行分块 Mongo 用于对话内存  ​   由   提交 /u/EnvironmentalDepth62   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/</guid>
      <pubDate>Mon, 12 Feb 2024 22:14:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 RMSNorm 在 Transformer 中比 LayerNorm 更快很重要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/</link>
      <description><![CDATA[最近发布的 LLM 中很大一部分都使用 RMSNorm 而不是 LayerNorm。 原始 RMSNorm 论文 (https://arxiv.org/pdf/1910.07467.pdf），我见过的大多数参考文献都认为 RMSNorm 比 LayerNorm 更好，因为它是计算效率更高。 但是，LayerNorm 只占整体计算的一小部分，因此我不清楚为什么加速会有很大帮助。渐进地，LayerNorm 为 O(d_model)，而像 MLP 这样的组件为 O(d_model2 )，或者注意力为 O(d_model*seq_len + d_model2 ）。 是否只是 LayerNorm 的平均居中部分没有那么有用，因此 RMSNorm 可以在不影响表现力的情况下为您带来轻微的效率提升？或者 RMSNorm 还有其他我没有看到的好处吗？   由   提交/u/kei147  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/</guid>
      <pubDate>Mon, 12 Feb 2024 21:09:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 斯坦福大学 Tri Dao 专访：论 FlashAttention 和稀疏性、量化和高效推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/</link>
      <description><![CDATA[Imbue 的一般智能播客的新一集与 Tri Dao 合作，FlashAttention 和Together AI 首席科学家。 涵盖的一些主题：  对循环连接进行逆向押注，而不是注意力 使用数据增强将知识编码为模型 设计利用硬件的算法  听对话：  Spotify 苹果播客 袖珍广播 摘要和参考论文  &lt;!-- SC_ON - -&gt;  由   提交 /u/thejashGI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/</guid>
      <pubDate>Mon, 12 Feb 2024 20:20:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 神经网络可训练性的边界是分形的 - Jascha Sohl-Dickstein</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/</link>
      <description><![CDATA[值得一读，只是为了简洁的可视化。 https://sohl-dickstein.github.io/2024/02/12/fractal.html https://arxiv.org/abs/2402.06184   由   提交 /u/currentscurrents   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/</guid>
      <pubDate>Mon, 12 Feb 2024 19:50:07 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] KV Cache 巨大并且成为 LLM 推理的瓶颈。我们以免微调 + 即插即用的方式将它们量化为 2 位。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/</link>
      <description><![CDATA[众所周知，批量推理是高效 LLM 服务的常见做法（这是 ChatGPT 等服务出现初始延迟的主要原因之一）。这种批处理实践的动机是，推理延迟主要受到模型加载的 I/O 成本的限制，而不是实际计算的限制，其中以批处理方式服务多个请求增加了可容忍的延迟增加，同时大大节省了每个令牌的成本。然而，批量推理（或长上下文任务，或两者）的一个问题是需要大量的 KV 缓存。正如 Jeff Dean 的之前的论文中所示：具有 bs=512 和  的 500B+ 模型seqlen=2048总 KV 缓存约为 3TB - 这是模型权重的 3 倍，并带来另一个 I/O 挑战，因为 GPU 需要将整个 KV 缓存加载到内存中在下一代代币中，计算核心再次大部分处于空闲状态。 当然，人们已经进行了各种尝试来减少 KV 缓存的大小。有些人通过使用逐出策略来丢弃不重要的令牌（例如，StremingLLM 和 H2O）；有些应用系统级优化，例如分页或卸载（例如，vLLM和FlexGen）。然而，对普通 KV 缓存量化的探索——据说可以带来直接的效率增益，同时与所有上述方法兼容——只看到了有限的性能保留。 我们探索了 KV 缓存量化的任务并发现关键挑战是密钥缓存中存在通道方面的异常值（通道=令牌d维度的某个索引）； 我们注意到这本身就是一个有趣的观察，因为这种模式在值缓存中不存在。沿着这个通道维度直接量化具有挑战性，因为新令牌以流式方式到达，这意味着我们永远不会知道下一个标记是否将包含异常值（或其范围）。考虑到这一点，我们提出了🥝KIVI，我们在一个小缓冲区的帮助下对键缓存进行每通道量化，对值缓存进行每令牌量化在 FP16 中。 我们的方法通过以 2 位量化的 KV 缓存实现了可接受的性能下降（针对 LM-Eval 和 LongBench 等实际任务进行评估时，平均准确率下降 &lt;1%）。这使得我们评估的 Llama/Mistral/Falcon 模型的峰值内存减少了 2.6 倍，同时批量大小增加了 4 倍，从而提高了 2.35 倍 - 3.47 倍的吞吐量。 🥝 KIVI：调整 - KV Cache 的免费非对称 2 位量化 📰 论文：https://arxiv.org/abs/2402.02750 😼 代码：https://github.com/jy-yuan/KIVI 📈快速浏览 主要结果   由   提交/u/choHZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/</guid>
      <pubDate>Mon, 12 Feb 2024 16:00:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>