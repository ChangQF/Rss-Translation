<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 30 Mar 2024 06:15:54 GMT</lastBuildDate>
    <item>
      <title>[R] 图像网格值得制作视频：使用 VLM 进行零样本视频问答</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bra4xc/r_an_image_grid_can_be_worth_a_video_zeroshot/</link>
      <description><![CDATA[       由   提交/u/dippatel21   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bra4xc/r_an_image_grid_can_be_worth_a_video_zeroshot/</guid>
      <pubDate>Sat, 30 Mar 2024 05:28:15 GMT</pubDate>
    </item>
    <item>
      <title>[N] Stability AI 创始人如何让他的价值数十亿美元的初创公司陷入困境</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br9vxr/n_how_stability_ais_founder_tanked_his/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br9vxr/n_how_stability_ais_founder_tanked_his/</guid>
      <pubDate>Sat, 30 Mar 2024 05:13:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] BLADE：使用小型特定领域模型增强黑盒大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br9e3r/r_blade_enhancing_blackbox_large_language_models/</link>
      <description><![CDATA[       由   提交/u/dippatel21   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br9e3r/r_blade_enhancing_blackbox_large_language_models/</guid>
      <pubDate>Sat, 30 Mar 2024 04:45:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变量多元关系与统一</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br97o6/r_variable_polyadic_relations_and_unification/</link>
      <description><![CDATA[我的论文需要有关可变多元关系和统一的资源。我在统一中发现了一点，但似乎 VP 关系在研究中并不常见。任何帮助，将不胜感激。    由   提交/u/IsDeathTheStart  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br97o6/r_variable_polyadic_relations_and_unification/</guid>
      <pubDate>Sat, 30 Mar 2024 04:35:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]：用新的 LongNet 替换 LlamaDecoderLayer 类拥抱 Face</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br8k70/dreplacing_the_llamadecoderlayer_class_hugging/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br8k70/dreplacing_the_llamadecoderlayer_class_hugging/</guid>
      <pubDate>Sat, 30 Mar 2024 03:59:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 优化移动设备的CCN模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br6tc5/r_optimising_ccn_models_for_mobile_devices/</link>
      <description><![CDATA[我正在训练 4 个具有多类分类的 CCN 模型，并且想知道可以进行哪些优化以使其在移动设备上运行得更快.   由   提交 /u/susanmylife   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br6tc5/r_optimising_ccn_models_for_mobile_devices/</guid>
      <pubDate>Sat, 30 Mar 2024 02:29:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么扩散模型不会过度拟合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1br15cv/d_why_dont_diffusion_models_overfit/</link>
      <description><![CDATA[DM 学习将噪声映射到数据分布。我很难理解为什么在足够的训练时期和足够大/足够强大的模型的限制下，它们不会（过度）拟合经验数据分布，在这种情况下样本将只是训练的副本数据集。 VAE 中没有像 KL 项这样的东西可以规范化事物。    由   提交/u/daking999  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1br15cv/d_why_dont_diffusion_models_overfit/</guid>
      <pubDate>Fri, 29 Mar 2024 22:17:47 GMT</pubDate>
    </item>
    <item>
      <title>关于DDIM论文的问题[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqzb7y/question_about_ddim_paper_d/</link>
      <description><![CDATA[        由   提交 /u/Aggressive-Plate6873    reddit.com/r/MachineLearning/comments/1bqzb7y/question_about_ddim_paper_d/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqzb7y/question_about_ddim_paper_d/</guid>
      <pubDate>Fri, 29 Mar 2024 21:05:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 部署嵌入模型的最佳方式？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqyvu3/d_best_way_to_deploy_embedding_models/</link>
      <description><![CDATA[TL;DR：部署和使用嵌入模型的最佳实践是什么？ 有 69 个用于部署 LLM 的包（与您聊天的人）每周都会有更多内容出现。  但是，我还没有看到任何包或框架可以在生产中有效地服务像 gte-large 这样的嵌入模型。    由   提交/u/Amgadoz  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqyvu3/d_best_way_to_deploy_embedding_models/</guid>
      <pubDate>Fri, 29 Mar 2024 20:41:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] JavaScript 中的教育深度学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqwhmj/p_educational_deep_learning_in_javascript/</link>
      <description><![CDATA[学习机器学习，我一直对 PyTorch 及其自动反向传播感兴趣。  在这个项目中，我尝试重新实现 PyTorch 的大部分内容 在 JavaScript 中。它以文档齐全、单元测试且可解释的方式从头开始实现，因此它可以帮助 JavaScript 学习者进入机器学习！此外，JavaScript PyTorch 可以更好地与 Web 浏览器集成，从而可以创建很酷的 Web 演示&lt; /a&gt;. 希望您喜欢！  GitHub 存储库此处！   由   提交 /u/suspicious_beam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqwhmj/p_educational_deep_learning_in_javascript/</guid>
      <pubDate>Fri, 29 Mar 2024 18:30:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 液体网络、神经常微分方程/偏微分方程和基于文本的扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqw0yl/d_liquid_nets_neural_odespdes_and_textbased/</link>
      <description><![CDATA[作为一名全栈工程师，我一直在转向 AI/ML，但我对某些事情有一些疑问。 我最近在自然语言处理和文本生成领域发现了一些有趣的进展。具体来说，我从麻省理工学院学习了液体神经网络（LNN），以及扩散模型在文本生成中的应用。 液体神经网络将神经网络的隐藏状态表示为时间的连续函数，根据微分方程演化。由于它们具有即时学习和适应的能力，它们在机器人和控制系统方面显示出了前景。它的创建者来自麻省理工学院，创建了一家名为 Liquid AI 的衍生初创公司，他们的博客文章讨论了他们开发新一代人工智能基础模型并超越 Transformer 的计划。 关于“超越生成式预训练变形金刚”的 Liquid AI 博客文章 另一方面，在图像生成方面取得成功的扩散模型现在也被应用于文本生成。 文本扩散 - 关于分数熵离散扩散的推文 看起来液体神经网络集成了神经常微分方程，类似于传统的扩散模型，但我有点对这一切感到困惑。我已经浏览过他们的液体结构状态空间模型，我猜它更类似于 Mamba，但是以某种方式将液体/神经 ODE 方面的内容实现到其中（我在尝试理解代码库时有点迷失）。 根据他们的 YouTube 视频，看起来液体神经网络主要应用于机器人技术，但这似乎在该特定领域是一件相当大的事情。我想知道这是否就是最近特斯拉自动驾驶能力实现巨大飞跃的原因。 我很想听听您对这些方法的想法和见解：  您认为液体神经网络可以有效地应用于文本生成任务，从而有可能取代或补充类似 GPT 的架构吗？ 您对扩散模型在文本生成中的应用有何看法？您认为这种方法有潜力推动生成语言模型的发展吗？ 考虑到顺序，您在将这些技术应用于文本生成时是否会遇到任何特定的挑战或限制？文本的本质以及捕获远程依赖关系的需要？ 您是否在生成语言模型领域遇到过其他令人兴奋或值得讨论的最新进展或有前景的方法？ 这是否可能会带来困扰传统 GPT 的上下文窗口的突破？  我一直在尝试深入学习 ODE/PDE，但我可能是错的，但是看起来它可能有一些潜力可以根据输入的复杂性动态调整计算要求。我想知道这是否可能与人类的系统 1 和系统 2 思维并行。 我在这个 Reddit 子版块上搜索了一些关于液体神经网络和神经 ODE/PDE 的帖子，但没有找到关于文本生成我真的看到了很多。 我渴望从这个社区的集体知识和观点中学习。任何与这些主题相关的见解、论文或资源将不胜感激。 提前感谢您的投入！    ;由   提交/u/Squidster777  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqw0yl/d_liquid_nets_neural_odespdes_and_textbased/</guid>
      <pubDate>Fri, 29 Mar 2024 18:11:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] Pytorch FSDP 是管道并行吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqsq3w/d_pytorch_fsdp_is_pipeline_parallelism_right/</link>
      <description><![CDATA[编辑：要明确的是，我对 FSDP 如何处理对于一个 GPU 来说太大的模型感兴趣（使用数据并行性这一事实并不重要）我想讨论的内容）。 在阅读FSDP论文后我得到了这个理解：   使用参数分片执行计算并相应地传达激活信息。 [...] 通过在计算之前按需传递参数来执行与本地训练相同的计算。由于参数通信对先前的计算没有任何数据依赖性，因此它们可以与在同一前向或后向传递中执行的先前计算重叠。然而，这种方法要求按需通信的参数能够完全具体化，并且能够适合单个GPU设备的内存。  FSDP属于第二类通信参数  但我很困惑为什么从来没有在任何地方明确提及这一点（博客文章、文档、论文）？   由   提交 /u/fasttosmile   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqsq3w/d_pytorch_fsdp_is_pipeline_parallelism_right/</guid>
      <pubDate>Fri, 29 Mar 2024 15:56:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 魔法背后的技术：OpenAI SORA 的工作原理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqmn86/d_the_tech_behind_the_magic_how_openai_sora_works/</link>
      <description><![CDATA[大家好，我对 OpenAI SORA 进行了深入研究，很高兴与大家分享。 在在这段视频中，我解释了图像和视频生成人工智能技术的演变，并深入探讨了 OpenAI SORA 的训练方式、其功能以及对社会的影响的细节。 享受：https://youtu.be/IqZXkBjKb2E?si=aaF8MUyqc5bhZ6G9   由   提交/u/johnolafenwa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqmn86/d_the_tech_behind_the_magic_how_openai_sora_works/</guid>
      <pubDate>Fri, 29 Mar 2024 11:11:51 GMT</pubDate>
    </item>
    <item>
      <title>[P] Jamba：首款基于 Mamba 的生产级模型，提供一流的质量和性能。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</link>
      <description><![CDATA[帖子：https://www.ai21 .com/blog/announcing-jamba  我们很高兴宣布 Jamba，世界上第一个基于 Mamba 的生产级模型。通过使用传统 Transformer 架构的元素增强 Mamba 结构化状态空间模型 (SSM) 技术，Jamba 弥补了纯SSM模型。它提供了 256K 上下文窗口，已经在吞吐量和效率方面展现了显着的进步——这只是这种创新混合架构的开始。值得注意的是，Jamba 在各种基准测试中都优于或匹配同尺寸级别的其他最先进型号。  ​ &lt; !-- SC_ON --&gt;  由   提交 /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</guid>
      <pubDate>Fri, 29 Mar 2024 03:39:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>