<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 24 Jan 2024 06:18:29 GMT</lastBuildDate>
    <item>
      <title>[R] 推荐硬件规格</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ea9xz/r_recommend_hardware_specs/</link>
      <description><![CDATA[我正在对尺寸为 3840x2160 的 RGB 图像进行自定义训练 YOLOv8 模型。我一开始使用 YOLOv8 的默认图像大小 640x640，但结果还有很多不足之处（想象一下块状像素化图像）。因此，我决定将训练分辨率提高到 1024x576，这次图像质量得到了改善，但代价是训练期间 GPU 使用量约为 15 GB（为此使用了 google colab T4）。现在，我正在考虑对原始图像大小进行训练，但这需要大量内存。 我知道有很多因素会影响训练期间的内存使用情况，但非常粗略的最小二乘近似表明以下 GPU 使用情况用于在 3840x2160 图像尺寸上进行训练： &gt;线性拟合：66.62 GB &gt;二次拟合：176.33 GB &gt;立方拟合：87.87 GB 这里有人有训练这么大图像的经验吗？如果是这样，您最终使用了多少计算资源？ 鉴于此背景，我正在向该社区寻求硬件建议。具体来说，我希望就我应该考虑的这个新工作站的规格提供建议。这是针对我的实验室的，所以我认为 $$ 不应该成为问题。 谢谢   由   提交/u/Black_Beard53  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ea9xz/r_recommend_hardware_specs/</guid>
      <pubDate>Wed, 24 Jan 2024 06:04:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2024审稿人评分及反驳讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8xyw/d_discussion_on_cvpr_2024_reviewer_scores_and/</link>
      <description><![CDATA[嘿 CVPR 爱好者！ 随着 CVPR 2024 审稿人的分数出来，我认为开放一下会很好有关审查过程和反驳的讨论和问题的线索。分享您的经验、见解，让我们继续讨论！ 首先，这是我的分数和置信度：  论文 1：4 (4)  论文 2：4 (3) 论文 3：2 (5)  其他人都怎么样？请随意分享您的分数、提出问题或寻求建议。让我们一起解决这个问题，充分利用反驳过程！ 🚀 #CVPR2024   由   提交/u/darkknight-6  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8xyw/d_discussion_on_cvpr_2024_reviewer_scores_and/</guid>
      <pubDate>Wed, 24 Jan 2024 04:47:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么时候在 TPU 上训练有意义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</link>
      <description><![CDATA[我花了几周时间将 torch 模型训练脚本移植到 PyTorch/XLA 并在 TPU v3 和 v4 上进行测试。从纯粹的训练速度和成本效率的角度来看，我将结果与 GCP 中的 a2/g2 机器上的训练进行了比较。我很惊讶移植代码有多困难，以及 TPU 上的训练有多慢且成本低效。 Dev UX 让人想起使用 TensorFlow（从最坏的意义上来说）。东西通常不能开箱即用，很难调试，因为所有东西都是编译的，而且张量是惰性的。整个事情非常不透明，不清楚发生了什么。没有您期望拥有的基本工具，例如如果不进行分析就无法检查 TPU 利用率。 更令人惊讶的是，训练速度比使用同等价格的 GPU 时慢得多。例如，与 g2-standard-96（8xL4 GPU）上的训练相比，TPU v3-8 上的训练速度大约慢 2 倍，而成本却大致相同。 TPU v4-8 价格更高，但仍然比 g2-standard-96 慢。我的模型或多或少是一个简单的密集网络，它来自推荐领域。未移植的 pytorch 代码使用 DDP。数据加载器经过高度优化并具有基准测试，我确信这不是瓶颈。 XLA 指标没有显示任何危险信号。 此时，我想知道为此投入更多精力是否有意义。非 Google 人员是否真的使用 TPU 进行大规模训练？是不是 Torch/XLA 还没有准备好迎接黄金时段，只是 TPU 最适合与 TF 或 JAX 一起使用？ TPU 是否有特定的用例？   由   提交 /u/Puzzleheaded-Stand79    reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</guid>
      <pubDate>Wed, 24 Jan 2024 04:15:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 测试基于 LLM 的应用程序很困难。你怎么处理这个问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</link>
      <description><![CDATA[让我知道你是如何处理这个问题的。非常感谢您的评论！   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</guid>
      <pubDate>Wed, 24 Jan 2024 03:17:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们不能使用合成数据来帮助创建用于放射图像分析训练的更清晰的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</link>
      <description><![CDATA[这是否比创建合成数据来训练 LLM 更难，类似于 AMIE 在最近的论文中所做的：https://blog.research.google/2024/01/amie-research-ai-system- for-diagnostic_12.html   由   提交/u/derpgod123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</guid>
      <pubDate>Wed, 24 Jan 2024 01:27:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻求研究合作者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/</link>
      <description><![CDATA[大家好！我正在寻找一些对 ML/AI 研究（主要是计算机视觉）感兴趣并希望在顶级会议上发表文章的合作者。任何也在寻找合作者的人，请随时私信我，我会分享更多细节。谢谢！   由   提交 /u/Zealousideal-Song744    reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/</guid>
      <pubDate>Wed, 24 Jan 2024 00:47:56 GMT</pubDate>
    </item>
    <item>
      <title>[D]天真的问题。在梯度下降中，为什么我们要将增量添加到权重中？为什么不乘以它呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e3xvk/d_naive_question_in_gradient_descent_why_are_we/</link>
      <description><![CDATA[为什么不是乘法，因为两个操作都可以改变值（尽管乘法会极大地改变它），这正是我们想要的？ new_weights = old_weights * delta   由   提交 /u/GullibleTrust5682   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e3xvk/d_naive_question_in_gradient_descent_why_are_we/</guid>
      <pubDate>Wed, 24 Jan 2024 00:38:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您关注哪些博客/YT 频道？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e17ht/d_what_blogsyt_channels_do_you_follow/</link>
      <description><![CDATA[我真的想确保我了解最新的方法和论文。我不想被它们淹没，但也许每周一次我想看看本周最重要的论文是什么。特别是在法学硕士和强化学习领域。我以前只是关注 OpenAI 和 Deepmind 来做这些事情，但我确信还有更多，自从 LLM 出现以来，强化学习并没有得到那么多的喜爱，所以我也想关注这一点。 &lt; p&gt;感谢您提前提出的建议！   由   提交/u/Intelligent_Rough_21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e17ht/d_what_blogsyt_channels_do_you_follow/</guid>
      <pubDate>Tue, 23 Jan 2024 22:36:58 GMT</pubDate>
    </item>
    <item>
      <title>[D]本科生：你如何应对出版业固有的不可预测性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dzrm4/d_undergrad_how_do_you_deal_with_the_inherent/</link>
      <description><![CDATA[作为一名本科生，我在实验室工作了 2.5 年（我知道这是相当标准的，但对于本科生来说，这对我来说是一段非常长的时间）一般只有4年）。  我们向 CVPR 提交了申请，今天收到了我们的评论 - 一项弱接受，一项勉强接受，一项弱拒绝 - 总体而言非常边缘。通过反驳，我们也许能够让其中一些足以被接受，但我们也可能不会。  我的问题是——您如何应对出版中的极端不确定性？我很难接受这样一个事实：我们花费了 2.5 年的时间所取得的成果可能不会见到曙光。    由   提交 /u/YodelingVeterinarian   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dzrm4/d_undergrad_how_do_you_deal_with_the_inherent/</guid>
      <pubDate>Tue, 23 Jan 2024 21:37:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2024 评论已出！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dydvm/d_cvpr_2024_reviews_are_out/</link>
      <description><![CDATA[你们都好吗？ 第一次提交，看到我的分数后会再次尝试:/   由   提交/u/V1bicycle  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dydvm/d_cvpr_2024_reviews_are_out/</guid>
      <pubDate>Tue, 23 Jan 2024 20:40:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究人员使用哪些工具在论文中创建出色的图像和流程图？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</link>
      <description><![CDATA[实际上，我想知道优秀的研究论文中的模型架构图有多酷，其中包含清晰的流程流程图和模型架构的出色可视化。目前我使用draw.io，但很好奇使用什么工具？我的意思是他们使用 Figma、Adobe 等专业工具吗？   由   提交 /u/MysticShadow427   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</guid>
      <pubDate>Tue, 23 Jan 2024 18:17:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有这些 AI 服务如何能够负担每月 5/10/20 美元的费用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</link>
      <description><![CDATA[各种人工智能服务（从语音识别到 OCR 和艺术生成）如何嵌入新数据，以如此低的成本提供其功能？使用 GPT-4 API 之类的东西很快就会花费 10 美元，这对于其他模型来说也是类似的。即使在本地运行 LLaMA 2 这样的东西也会产生巨大的成本。我很好奇这些服务在运营这些大型模型时采用的经济策略来维持较低的月费。   由   提交 /u/Numerous_Bed9323   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:00 GMT</pubDate>
    </item>
    <item>
      <title>[N] Meta 开源了经过 450 万小时预训练的 wav2vec2 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/</link>
      <description><![CDATA[一个月前，Meta AI 发布了 W2V-Bert，这是其 Seamless 模型的构建模块之一。  它已经过 450 万小时的未标记音频数据的预训练，涵盖超过 143 种语言。  优点：  实现低资源微调 比 Whisper 更快、更轻 MIT 许可证 可以针对其他音频任务进行微调  缺点：  基于 CTC，因此适用于标准化转录 &lt; li&gt;使用前需要微调  资源：  原始仓库：https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file#whats-new 变形金刚文档：https://huggingface.co/docs/transformers/main/en/model_doc/wav2vec2 -bert 蒙古语博客文章的 ASR 微调：https:// Huggingface.co/blog/fine-tune-w2v2-bert    由   提交 /u/Sufficient-Tennis189   /u/Sufficient-Tennis189  reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/</guid>
      <pubDate>Tue, 23 Jan 2024 14:37:15 GMT</pubDate>
    </item>
    <item>
      <title>[N]ICLR2024的学习理论家，我感同身受！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dnolo/n_learning_theorists_of_iclr2024_i_feel_you/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dnolo/n_learning_theorists_of_iclr2024_i_feel_you/</guid>
      <pubDate>Tue, 23 Jan 2024 12:49:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>