<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Wed, 05 Mar 2025 03:26:45 GMT</lastBuildDate>
    <item>
      <title>[D] LLM量化建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3pyv1/d_llm_quantization_advice/</link>
      <description><![CDATA[在老实说，这是令人着迷和压倒性的混合。我得到了减少基础模型的规模，更快地推理，丢失精度，所有这些好东西，但我想了解更多。 如果您在对您有所帮助之前曾经经历过？任何更改游戏的论文，博客文章，存储库，代码教程或艰苦学习的课程？我想从“哦，我有点明白它”到真正知道我在做什么。 很想听听任何在这条路上有效的人，没有什么工作，没有什么，您希望您早些时候知道！ 欣赏它！    &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/Professionfox8649     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3pyv1/d_llm_quantization_advice/</guid>
      <pubDate>Wed, 05 Mar 2025 00:17:01 GMT</pubDate>
    </item>
    <item>
      <title>[D]混淆矩阵。困惑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/</link>
      <description><![CDATA[       &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/wide-chef-7011     [link]   ＆＃32;   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/</guid>
      <pubDate>Tue, 04 Mar 2025 19:48:39 GMT</pubDate>
    </item>
    <item>
      <title>[d]寻找有关长期AI记忆和上下文保留的见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在跟踪AI模型如何保留上下文，不仅仅是基于会话的内存，而且更接近自适应智能，这些智能会记住和完善其理解的理解。   大多数当前模型似乎都可以限制在互动的情况下，而不是在互动的情况下进行互动，但如果互动，则互动的互动，如果iif if if if if if if if if if if if if if if if if if if if if if if if if if if if a if if if if if i，则陷阱？ 好奇是否有人从事此工作，还是在增强学习和变形金刚进行的除外的有前途的方法。人们如何考虑将AI内存发展为更有机的东西？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/</guid>
      <pubDate>Tue, 04 Mar 2025 19:29:10 GMT</pubDate>
    </item>
    <item>
      <title>[d]我应该从这种类型的问题中期待什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3fslw/d_what_should_i_expect_from_this_type_of_question/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   数据访谈：访谈探讨您有关如何消耗和组织数据的思维过程，并在各种约束中验证您的方法。可能会要求您设计一个数据体系结构来解决特定问题，并编写代码以将一些给定的输入转换为有用的输出。可能会要求您考虑等问题，例如延迟，最终一致性和竭尽全力。我们使用Scala和Java开发数据管道，但使用各种工具来移动数据。在整个过程中，您应该期望与各种工具和技术讨论您的个人经验。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/horror_weakness_6996      [links]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3fslw/d_what_should_i_expect_from_this_type_of_question/</guid>
      <pubDate>Tue, 04 Mar 2025 17:11:14 GMT</pubDate>
    </item>
    <item>
      <title>[r] readerlm-v2：使用1.5b参数语言模型有效的HTML-TO-MARKDOWN转换</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3em5l/r_readerlmv2_efficient_htmltomarkdown_conversion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在研究一种专业的LM方法，该方法演示了目标优化如何比特定任务胜过更大的模型。 ReaderLM-V2是一种小型语言模型（3-7B参数），它以显着效率将HTML转换为Markdown和JSON。 此处的技术方法非常聪明：   使用较大模型的同步数据生成的同步数据生成较大的模型来创建高级培训 li&gt; li-li&gt; li&gt; li&gt; li&gt;      代币化处理HTML标签和结构有效地  利用块蒸馏技术来维持长期文档的上下文 仅专注于HTML理解，而不是一般能力    ，结果表明了更大的优势，结果显示了大量的模型： 保持对复杂文档的更好结构理解 处理嵌套的要素，表格和格式化更准确地 使用的计算资源   在文档中提供了更好的构建 在在许多任务中做得非常好而不是平庸的表现的模型。综合数据生成方法还解决了配对训练数据的专用域中的一个常见问题。 我认为，这种方法可以应用于其他专业文档处理任务，其中结构与内容一样重要。特别有趣的是，当针对特定领域进行适当优化时，较小的模型可以胜过较大的模型。  tldr：readerlm-v2是一种小但专业的语言模型，它通过使用合成训练数据和专业的建筑和专业的建筑和专业的模型，将HTML转换为Markdown/JSON比更大的Markdown/JSON更有效。它证明了目标优化可以胜过原始参数计数。  完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3em5l/r_readerlmv2_efficient_htmltomarkdown_conversion/</guid>
      <pubDate>Tue, 04 Mar 2025 16:23:55 GMT</pubDate>
    </item>
    <item>
      <title>[P]白血病的分类和检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您当前正在尝试开发一个由白血病数据集训练的模型，当我给出测试图像时，它需要给出输出为是（包含白血病）。问题是我想要一个统一的文件夹来用于所有培训图像。我找不到它，如果某人具有此类数据集的链接，请您分享。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/i_love_69_position     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/</guid>
      <pubDate>Tue, 04 Mar 2025 15:32:03 GMT</pubDate>
    </item>
    <item>
      <title>[r]当LLM对其答案感到担忧时 - 何时是合理的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3alha/r_when_an_llm_is_apprehensive_about_its_answers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  不确定性估计对于评估大型语言模型（LLMS）至关重要，尤其是在不正确答案导致重大后果的高风险域中。许多方法考虑了这个问题，同时着眼于特定类型的不确定性，而忽略了他人。我们调查了哪些估计值，特别是令牌的熵和模型法官（MASJ），将用于针对不同问题主题的多项选择提问任务。我们的实验考虑了三个LLM：PHI-4，Mistral和Qwen，它们的大小从1.5B到72B和14个主题。尽管MASJ的性能与随机误差预测指标相似，但响应熵预测了知识依赖性域中的模型误差，并作为问题难度的有效指标：对于生物学RocAUC为0.73。这种相关因素依赖性域而消失：对于数学问题，ROC-AUC为0.55。更重要的是，我们发现熵措施需要推理金额。因此，与数据不确定性相关的熵应集成到不确定性估计框架中，而MASJ需要改进。此外，现有的MMLU-PRO样品是有偏见的，应平衡不同子域的所需推理，以提供对LLMS性能的更公平评估。    https://huggingface.co/papers/2503.01688  href =“ https://arxiv.org/abs/2503.01688”&gt; https://arxiv.org/abs/2503.01688      &lt;！ -  sc_on- sc_on-提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j3alha/r_when_an_an_lm_is_is_appreans_appreans_about_about_its_istswers/”&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3alha/r_when_an_llm_is_apprehensive_about_its_answers/</guid>
      <pubDate>Tue, 04 Mar 2025 13:20:10 GMT</pubDate>
    </item>
    <item>
      <title>[r]加强学习在稀疏的奖励，国家陷阱和缺乏终端状态的斗争中 - 我们可以改善它吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j39u8q/r_reinforcement_learning_struggles_with_sparse/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  稀疏奖励，状态陷阱和终端状态的缺失挑战常见的RL方法。 伞增强学习引入了代理的连续合奏和A 连续时间政策优化框架，提高了这些设置的探索和效率。      paper：    https://doi.org/10.1016/j.cnsns.2024.108583  href =“ https://github.com/enuzhin/ur/”&gt; https://github.com/enuzhin/ur/    结果：  https://paperswithcode.com/paper/paper/umbrella-reinforecement-lecrespection-learnning      效率低下。  连续时间优化提高了潜在效率的增长。    登录奖励奖励功能确保探索-Exploitic平衡。        问题。   这可以解决现实世界问题吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/egornuzhin     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j39u8q/r_reinforcement_learning_struggles_with_sparse/</guid>
      <pubDate>Tue, 04 Mar 2025 12:39:02 GMT</pubDate>
    </item>
    <item>
      <title>[d]清除时间序列中清除的简历的好处？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/</link>
      <description><![CDATA[      Hello, In a context of time series prediction, I have troubles to grasp the actual benefits of Purged CV versus regular time split CV. Formulated differently, why is there a risk of data leakage when time split CV is applied? As a reminder for下面的每个人都是常规时间拆分简历的工作方式（源 在这里，清除的人如何工作（相同欢迎任何见解，谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bilbilious-pomelo-700      [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/</guid>
      <pubDate>Tue, 04 Mar 2025 11:53:01 GMT</pubDate>
    </item>
    <item>
      <title>[d] ICLR 2025首次计时器？分享您接受的东西</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，所以我的第一篇论文被ICLR接受。等不及要去新加坡了！我认为这可能是一个很好的机会，可以看到该社区研究人员接受的一些作品。  对我来说，我加入了一个从事仿生的物理学家的实验室。他对航班机制特别感兴趣，并且在面向飞行的工程周围有许多项目。一些学生专注于老鹰队以及他们如何飞翔的热风，而另一些学生（例如我）专注于机器人机制，类似于蜂鸟和苍蝇。  长话短说，我们在拍打机翼周围开发了一个测量系统，跟踪其运动和系统中的空气动态力。然后，我们提出了一个问题：要获得所需的预定义空气动力的输入翼电影应该是什么。  方法有一个多元时间序列，重点是傅立叶空间。我们提出了一个在频域中确实表示的体系结构，并专门针对这些类型的任务任务量身定制，我们将其定义为逆映射。尽管我们没有证明可以应用逆映射的其他领域，但我们确实提供了一些可以进行未来研究的例子。  我们开放了数据集以及我们开发的框架（您可以在Github上查看它，Repo的名称是AdpativeSpectRumlayer）。 如果您是我的第一次，我很想听听您的故事     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/upaster-ability-774     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j35gpt/d_iclr_2025_2025_first_timers_here_here_here_share_share_hare_hare_hare_got_you/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/</guid>
      <pubDate>Tue, 04 Mar 2025 07:28:27 GMT</pubDate>
    </item>
    <item>
      <title>[r]谨慎的优化者：通过一行代码改进培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j33lm7/r_cautious_optimizers_improving_training_with_one/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这是一个令人惊讶的简单调整。在大多数现代深度学习优化器中，通常根据梯度的运行差异，以某种形式的动量和/或学习率缩放来计算模型权重的更新。这意味着“瞬时”从特定的向后通行证的梯度实际上可能指向更新的方向。 作者提出了一个简单的更改：他们建议忽略优化器的任何更新，而优化器的任何更新与最新后退的相反符号。换句话说，他们建议仅应用与当前梯度保持一致的更新，从而使更新更稳定，并且与最新数据一致。他们发现，这种小的调整可以大大加快训练的速度。 这是一个有趣的想法，虽然我很好奇它是如何发挥作用的，但我将等待独立的复制，然后才完全相信。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ahmedmastafa16     &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j33lm7/r_cautious_optimizers_improving_improving_training_with_with_one/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j33lm7/r_cautious_optimizers_improving_training_with_one/</guid>
      <pubDate>Tue, 04 Mar 2025 05:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[R]具有非高斯可能性的高斯过程的集成梯度归因</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j337qu/r_integrated_gradient_attribution_for_gaussian/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi reddit， 我一直在做这项兼职，很喜欢一些反馈 - 无需退缩，请随时告诉我，如果您认为这应该为crackpot Science标记：  paper： https://arxiv.org/pdf/2205.12797     代码： https://github.com/sarems/sarems/sarems/iggp    这个想法是应用 稀疏变异高斯过程具有非高斯的可能性/观察值。我在可能的情况下衍生了封闭形式的公式，并使用了taylor近似 /高斯 -   - 高正交正交正交（定理1）。  此外，当使用高斯过程模型时，我正在研究集成梯度的完整性属性（属性的总和=模型输出的差异），而不是原始工作（Theorem 2）。提交由＆＃32; /u/sarems     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j337qu/r_integrated_gradient_attribution_for_gaussian/</guid>
      <pubDate>Tue, 04 Mar 2025 04:59:10 GMT</pubDate>
    </item>
    <item>
      <title>[P]建议或有关如何创建指令数据集的指导</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j320ns/p_advice_or_guidance_on_how_to_create_an/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我有一个糖尿病友好型食谱数据集，其中包括标题，描述，准备时间，烹饪时间，服务时间，服务，分步说明，标签，标签，营养事实，营养事实和配料列表等领域。我希望将其转变为指令 - 格式数据集（即{指令，输入，输出}三倍）来训练或微调大型语言模型 我对教学调整有点陌生，任何建议，经验，因此，您可以分享的任何建议，您可以提前感谢您的DETAS！ href =“ https://huggingface.co/datasetsets/elizah521/diabetes_recipes/tree/main/main”&gt; https://huggingface.co/datasets/elizah521/diabetes_recipes_recipes_recipes/tree/main/main/main/main/main  /u/u/forade-type-1514     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j320ns/p_advice_or_guidance_on_how_to_create_an/</guid>
      <pubDate>Tue, 04 Mar 2025 03:54:35 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j1hc0o/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Mar 2025 03:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>