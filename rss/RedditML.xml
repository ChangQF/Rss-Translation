<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 14 Jun 2024 12:27:19 GMT</lastBuildDate>
    <item>
      <title>[D] 讨论苹果在 iPhone 15 Pro 上部署 30 亿参数 AI 模型——他们是如何做到的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</link>
      <description><![CDATA[大家好， 所以，我一直在本地运行 Phi-3 mini，老实说，它还不错。尽管在模型文件中进行了所有调整和结构化提示，但这是正常的，尤其是考虑到典型 GPU 设置上的滞后响应时间。我最近在检查 Apple 最近的设备模型，他们在 iPhone 15 Pro 上运行了一个近 30 亿参数的 AI 模型！ 这是 AI 在移动设备上实现可能性的进步。他们想出了一些技巧来实现这一点，我只是想和大家讨论一下：  优化的注意力机制：Apple 通过使用分组查询注意力机制显着降低了计算开销。此方法可批量处理查询，从而减少必要的计算。 共享词汇嵌入：老实说，我对此没有太多了解 - 我需要更多地了解它 量化技术：对模型权重采用 2 位和 4 位量化混合，有效降低了内存占用和功耗。 高效的内存管理：动态加载小型、特定于任务的适配器，可以加载到基础模型中以专门化其功能，而无需重新训练核心参数。这些适配器重量轻，仅在需要时使用，在内存使用方面灵活且高效。 高效的键值 (KV) 缓存更新：即使我都不知道它是如何工作的 功耗和延迟分析工具：他们使用 Talaria 等工具实时分析和优化模型的功耗和延迟。这样一来，他们就可以在性能、功耗和速度之间做出权衡，定制比特率选择，以在不同条件下实现最佳运行。：Talaria 演示视频 通过适配器进行模型专业化：无需重新训练整个模型，只需针对不同任务训练特定的适配器层，即可保持高性能，而无需重新训练整个模型的开销。 Apple 的适配器让 AI 可以随时切换档位以执行不同的任务，同时保持轻便和快速。  有关更详细的见解，请查看 Apple 的官方文档：介绍 Apple Foundation Models 讨论要点：  在移动设备上部署如此庞大的模型的可行性如何？ 这些技术对未来的移动应用有何影响？ 这些策略与典型的桌面 GPU 环境（例如我使用 Phi-3 mini 的体验）中使用的策略相比如何？     提交人    /u/BriefAd4761   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</guid>
      <pubDate>Fri, 14 Jun 2024 11:50:36 GMT</pubDate>
    </item>
    <item>
      <title>Pecan Street 数据需求响应“[D]”，“[R]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfn5wt/pecan_street_data_demand_response_d_r/</link>
      <description><![CDATA[嗨。这里有人用过 Pecan Street Data 吗？它用于需求响应程序，包含大量用户电力需求数据。我只能从我的大学网站访问有限量的数据。但我想要至少 2 年（2015 年和 2016 年）的数据。有人能在这方面提供帮助吗？非常感谢    提交人    /u/muqz_786   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfn5wt/pecan_street_data_demand_response_d_r/</guid>
      <pubDate>Fri, 14 Jun 2024 09:55:08 GMT</pubDate>
    </item>
    <item>
      <title>[D]通过数据融合提高天气预报准确性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfn4c6/denhancing_weather_forecast_accuracy_through_data/</link>
      <description><![CDATA[我有四组不同的天气预报数据，其中包括时间、城市 ID、太阳辐照度、温度、湿度、风速、降雨量、云量和气压等类似字段。时间戳的格式为“2024-06-14 12:30:00”，间隔为 15 分钟。共有 20 个城市，每个城市之间的距离约为 100 公里。此外，我还有一组实际天气数据，其字段与预报相同。问题是所有天气预报都有些不准确，所以我想使用数据融合来获得更精确的天气预报。我将这种融合定义为回归问题，使用预报数据来拟合实际天气数据。例如，为了获得准确的太阳辐照度预报，我会使用预报数据拟合实际辐照度，然后将该模型应用于未来的预测，旨在使模型的预测比任何单个预测更接近实际辐照度。我使用 LightGBM 为这个回归开发了一个模型，并在测试集上使用 RMSE 指标对其进行了评估，发现 RMSE 确实低于任何单个预测。然而，在某些情况下，比如在太阳辐照度波动较大的阴天，该模型的表现就很一般。所以，我的问题是，这种方法的上限是什么，有没有更好的方法？    submitted by    /u/Rich-Effect2152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfn4c6/denhancing_weather_forecast_accuracy_through_data/</guid>
      <pubDate>Fri, 14 Jun 2024 09:51:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 探索大规模全模态预训练的极限</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfm06d/r_explore_the_limits_of_omnimodal_pretraining_at/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfm06d/r_explore_the_limits_of_omnimodal_pretraining_at/</guid>
      <pubDate>Fri, 14 Jun 2024 08:30:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 多元时间序列</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfkwob/p_multivariate_time_series/</link>
      <description><![CDATA[多元时间序列 关于提高多元时间序列预测模型性能的建议 大家好， 我去年毕业后最近加入了一家公司，一直在从事一个多元时间序列预测项目。 为了说明，让我们考虑一个类似的例子：预测信用卡交易中的逾期付款次数。目标是开发一个 t+1 模型，根据收集到的特征预测逾期付款次数。 这是我遵循的方法： 1. 数据聚合：我按天聚合了所有特征。2. 初始模型：我从使用线性回归的基本模型开始。3. 高级模型：然后我实现了 XGBoost 模型和无状态 RNN 模型。两者都比基准线性回归模型有所改进。 XgBoost 模型的表现略胜于 rnn 模型  然而，尽管做出了这些努力，但我在性能改进方面还是达到了瓶颈。根据训练和验证损失图，没有过度拟合的证据。 以下是我尝试的其他技术： • Dropout 和残差层：实现以防止过度拟合并改进学习。• 自定义损失函数：旨在更好地捕获预测目标。• LSTM 层：捕获时间序列数据中更长的依赖关系。 尽管做出了这些努力，但我没有看到模型性能有任何进一步的改进。 就背景而言，我的数据集包含 10 个国家/地区的 300 天的数据。 任何关于如何进一步提高模型性能的建议或见解都将不胜感激！    提交人    /u/SoftwareArt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfkwob/p_multivariate_time_series/</guid>
      <pubDate>Fri, 14 Jun 2024 07:09:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] Lamini.AI 推出记忆调节功能：法学硕士准确率达 95%，幻觉减少 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</link>
      <description><![CDATA[https://www.lamini.ai/blog/lamini-memory-tuning  Lamini Memory Tuning 是一种将事实嵌入 LLM 的新方法，可提高事实准确性并将幻觉减少到以前无法实现的水平 - 对于一位财富 500 强客户来说，Lamini Memory Tuning 的准确率达到了 95%，而其他方法的准确率仅为 50%。幻觉从 50% 减少到 5%。 Lamini Memory Tuning 是一项研究突破，它克服了 AI 世界中一个看似矛盾的现象：实现精确的事实准确性（即没有幻觉），同时坚持使 LLM 变得有价值的泛化能力。 该方法需要在任何开源 LLM（如 Llama 3 或 Mistral 3）之上使用精确的事实调整数百万个专家适配器（例如 LoRA）。如果目标是准确无误地获取罗马帝国的事实，Lamini Memory Tuning 会创建关于凯撒、渡槽、军团和您提供的任何其他事实的专家。受信息检索的启发，该模型在推理时仅从索引中检索最相关的专家 - 而不是所有模型权重 - 因此延迟和成本显着降低。高精度、高速度、低成本：使用 Lamini Memory Tuning，您无需选择。  研究论文：https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf    提交人    /u/we_are_mammals   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</guid>
      <pubDate>Fri, 14 Jun 2024 02:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI 的训练范式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfdlk4/d_openais_training_paradigm/</link>
      <description><![CDATA[有人找到关于 OpenAI 模型开发方法的好文献吗？ 我想知道： - 他们的数据开发流程是什么样的。 - 他们进行什么样的手动/合成标记。 - 他们特定的变压器架构和损失函数。 - 他们如何在训练工作流程中实现 RLHF - 任何相关的 Python 代码都可以启动哈哈    提交人    /u/MGeeeeeezy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfdlk4/d_openais_training_paradigm/</guid>
      <pubDate>Fri, 14 Jun 2024 00:04:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您觉得 NoPE 怎么样（至少在小型机型上）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfay95/d_what_do_you_think_of_nope_on_small_models_at/</link>
      <description><![CDATA[      嗨！ 我尝试了 Karpathy 的“让我们重现 GPT-2 (124M)”（和 repo）代码，但使用了 NoPE，只是删除了位置编码。 编辑：NoPE 代表无位置编码，我不知道它以前是否使用过，但我第一次看到它是在论文“位置编码对 Transformers 中长度泛化的影响”中，该论文认为没有必要添加位置编码，网络将学习它们。他们还认为它的表现优于他们。 EDIT2：不幸的是，根据我的经验，我发现 ALiBi 比 NoPE 表现更好，它对所有序列长度的困惑度值都与 NoPE 的最低困惑度相同（在训练序列长度为 1024 时） 从我得到的结果来看，它还不错，但我认为它的泛化能力有点夸大了。 我得到了以下数据： https://preview.redd.it/clq8zwl2ve6d1.png?width=1480&amp;format=png&amp;auto=webp&amp;s=c8c846df1ada58c0bed156e24595478087e77b75 https://preview.redd.it/0hvgd273ve6d1.png?width=1498&amp;format=png&amp;auto=webp&amp;s=f111ef399c34f7c01530064cb3b87d71ac2db8a8 它使用相同的硬件、训练和验证集、训练配置等。不幸的是，我无法与学习到 PE 的模型进行比较（这些实验对我来说成本太高了）。 这些图有很多问题：  颜色不一样两幅图中的图例相同。我无法纠正我的实例已经关闭的情况 :( 序列长度的跳跃太大（总是翻倍）。 在训练过程中大约 15xxx 步发生了一些事情。当我回来时，我发现标准太高（没有记录/绘制），即使在最后一步也是如此，因此很难从中恢复。我不确定稳定阶段的标准，但我猜它下降到了 1 以下。  我缺少最重要的一点，即与学习到的 PE 模型进行公平比较，但我已经在使用 ALiBi 和 FIRE 进行两次实验，这变得太昂贵了。NoPE 在吞吐量方面没有真正的收益（至少在这种情况下），所以我们可以观察到的唯一收益可能是有点概括。 你们觉得怎么样？有人对此有更多经验吗？    提交人   /u/ReinforcedKnowledge   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfay95/d_what_do_you_think_of_nope_on_small_models_at/</guid>
      <pubDate>Thu, 13 Jun 2024 22:00:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 FIRE（位置编码方法）及其实现的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfae0z/d_questions_about_fire_the_positional_encoding/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfae0z/d_questions_about_fire_the_positional_encoding/</guid>
      <pubDate>Thu, 13 Jun 2024 21:34:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 处理大规模特征。例如从 -1e2 到 1e4</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df4478/d_dealing_with_features_having_large_scale_eg/</link>
      <description><![CDATA[如果特征的范围很大，你会如何处理？一种选择是对数尺度，但如果特征是负值怎么办？ 你们是如何处理深度学习模型中的此类数据的？ 编辑：为了清楚起见，这些特征的范围很广。例如 -1e2 到 +1e4。 此外，如果输入特征和学习特征中存在如此宽的范围，你会如何处理？ 您是否成功地对案例使用了最小最大或标准化？    提交人    /u/rmm_philosopher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df4478/d_dealing_with_features_having_large_scale_eg/</guid>
      <pubDate>Thu, 13 Jun 2024 17:08:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 wandb.ai 的经验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df3y27/d_experiences_with_wandbai/</link>
      <description><![CDATA[我个人在 ML 研究中经常使用 wandb.ai (权重和偏差)，发现它很棒，但缺少一些功能。 因此，我目前正在构建一个像 wandb 这样的 MLOps 平台，但更适合团队合作。 我对社区的问题是：  你不喜欢 wandb 的哪些方面？ 你希望它有什么功能吗？  我非常感谢任何见解！我想构建一个真正有益且比 wandb 更有价值的东西。请告诉我任何事情，随时给我发私信！ 我的个人观点：wandb 很棒，但很难作为一个团队一起工作，尤其是在远程工作时。我们发现自己经常使用 slack + notion 来跟踪事情，但情况已经失控了。 此外，没有办法“采取行动”。即您看到一些结果，并决定要为其制作 jira 票证，则必须手动完成。 对其他人来说，这还不是一个足够大的问题吗？只是好奇    提交人    /u/Sriyakee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df3y27/d_experiences_with_wandbai/</guid>
      <pubDate>Thu, 13 Jun 2024 17:01:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenMetricLearning 3.0 统一支持图片和文字！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</link>
      <description><![CDATA[      大家好！ 我想分享 OpenMetricLearning 3.0 的发布！  OML — 是一个用于表示学习和检索的库，其中包含大量模型、损失、矿工、采样器、指标和其他有用的东西，如 DDP、与 PyTorchLightning 和 PyTorch Metric Learning 的集成、不同的实验跟踪器等。   有什么新内容？ * 我们已经添加了文本支持，现在我们正在添加音频！（用户不仅已经将 OML 用于图像，而且现在我们还提供开箱即用的支持、测试和示例。） * 代码统一适用于图像、文本，并且适用于声音！我邀请您查看图像和文本的并排比较。 * 检索部分已分离，可用于模型验证和推理，并进行以下重新排名或其他后期处理。 * 库的功能已在一个地方描述，以便于导航，并且我们总体上改进了文档和示例。 * 一些计算，特别是与内存相关的计算，已经进行了优化。 我们欢迎潜在的贡献者： * 代码变得更加模块化，因此入门门槛降低了 - 您可以获取单独的代码并继续进行它。 * 我们还用我们的问题/任务更新了board。 您在GitHub上的⭐️极大地帮助了我们进一步发展！ OML    提交人    /u/Zestyclose-Check-751   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</guid>
      <pubDate>Thu, 13 Jun 2024 09:06:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 微软开源 Recall AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</link>
      <description><![CDATA[我创建了一个开源的 Microsoft Recall AI 替代品。 这会记录您屏幕上的所有内容，并可以使用自然语言搜索。但与 Microsoft 的实现不同，这不是隐私噩梦，现在就可以使用。并带有实时加密 这是一个新的启动项目，需要贡献，因此请希望转到 github repo 并给它一个星星 https://github.com/VedankPurohit/LiveRecall 它是完全本地的，您可以查看代码。而且所有内容始终是加密的，这与 Microsoft 的含义不同，当您登录时，图像会被解密并可能被盗    提交人    /u/Vedank_purohit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</guid>
      <pubDate>Thu, 13 Jun 2024 05:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 能否发明更好的方法来培养 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</link>
      <description><![CDATA[新博客文章和论文： https://sakana.ai/llm-squared/ https://arxiv.org/abs/2406.08414 发现用于大型语言模型的偏好优化算法 摘要 离线偏好优化是增强和控制大型语言模型 (LLM) 输出质量的关键方法。通常，偏好优化被视为使用手工制作的凸损失函数的离线监督学习任务。虽然这些方法基于理论见解，但它们本质上受到人类创造力的限制，因此可能的损失函数的大量搜索空间仍未得到探索。我们通过执行 LLM 驱动的目标发现来解决这个问题，以自动发现新的最先进的偏好优化算法，而无需（专家）人工干预。具体而言，我们迭代地提示 LLM 根据先前评估的性能指标提出和实施新的偏好优化损失函数。此过程导致发现以前未知且性能良好的偏好优化算法。其中表现最好的我们称之为发现偏好优化 (DiscoPOP)，这是一种自适应地混合逻辑和指数损失的新算法。实验证明了 DiscoPOP 的最先进的性能及其成功转移到保留任务。    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</guid>
      <pubDate>Thu, 13 Jun 2024 02:18:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>