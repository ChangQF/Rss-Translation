<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 21 Jan 2024 09:12:44 GMT</lastBuildDate>
    <item>
      <title>[讨论] 是否可以在一台电脑中同时使用 Rtx 4070 12gb 和 Rtx 3060 12gb，用于 LLM 和其他可能受益于此配置的应用程序？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bzg00/discussion_is_it_possible_to_use_a_rtx_4070_12gb/</link>
      <description><![CDATA[我买不起 24GB 显卡。 Rtx 4070 用于主要游戏，Rtx 3060 将与 4070 一起用于需要高 vram 的 LLM 和搅拌机等其他应用。    由   提交 /u/GodCREATOR333   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bzg00/discussion_is_it_possible_to_use_a_rtx_4070_12gb/</guid>
      <pubDate>Sun, 21 Jan 2024 09:10:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有任何动手/实用的 ML YouTube 频道？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/</link>
      <description><![CDATA[我一直在寻找实用的深度学习或机器学习论文实现或 YouTube 频道上的实践。您有推荐的频道吗？   由   提交/u/Agitated-Ad809  /u/Agitated-Ad809 reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/</guid>
      <pubDate>Sun, 21 Jan 2024 08:52:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 多重研讨会</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bydva/d_multiple_workshop/</link>
      <description><![CDATA[如果我被多个研讨会录取，我是否必须退出一个研讨会？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bydva/d_multiple_workshop/</guid>
      <pubDate>Sun, 21 Jan 2024 07:58:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 生成并预览 3D 骨骼动画 (Momask)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bxusa/p_generate_preview_3d_skeletal_animations_momask/</link>
      <description><![CDATA[ 由   提交/u/nmfisher  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bxusa/p_generate_preview_3d_skeletal_animations_momask/</guid>
      <pubDate>Sun, 21 Jan 2024 07:22:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自我奖励语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bqy3b/r_selfrewarding_language_models/</link>
      <description><![CDATA[摘要： 我们认为，为了实现超人智能体，未来的模型需要超人反馈才能提供足够的训练信号。目前的方法通常根据人类偏好来训练奖励模型，这可能会受到人类表现水平的瓶颈，其次这些单独的冻结奖励模型无法在 LLM 训练期间学习改进。在这项工作中，我们研究自我奖励语言模型，其中语言模型本身通过法学硕士作为法官来使用，提示在训练期间提供自己的奖励。我们表明，在迭代 DPO 培训期间，不仅提高了指令遵循能力，而且还提高了为自身提供高质量奖励的能力。在我们的方法的三个迭代中对 Llama 2 70B 进行微调，产生的模型优于 AlpacaEval 2.0 排行榜上的许多现有系统，包括 Claude 2、Gemini Pro 和 GPT-4 0613。虽然只是初步研究，但这项工作打开了大门模型在两个轴上不断改进的可能性。 https://arxiv.org/abs/2401.10020&lt; /a&gt;    由   提交 /u/rlresearcher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bqy3b/r_selfrewarding_language_models/</guid>
      <pubDate>Sun, 21 Jan 2024 00:54:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] PriomptiPy - 用于预算代币并动态呈现法学硕士提示的 python 库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bkzax/p_priomptipy_a_python_library_to_budget_tokens/</link>
      <description><![CDATA[       由   提交/u/tg1482  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bkzax/p_priomptipy_a_python_library_to_budget_tokens/</guid>
      <pubDate>Sat, 20 Jan 2024 20:25:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在哪里可以找到新的或鼓舞人心的机器学习项目或方法来学习？不一定是最前沿的机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bkmiz/d_where_to_find_new_or_inspiring_ml_projects_or/</link>
      <description><![CDATA[您好， 我是一名 ML 工程师，和我们所有人一样，我会尽力跟上 ML 的步伐/AI，不仅是 SOTA，还包括其他从业者使用的不同方法或技术。  这里有很多关于如何及时了解研究的讨论（Youtube 频道、播客、时事通讯、科学期刊……凡是你能想到的），但我觉得这些往往是关于通过需要巨大 GPU 进行训练的大型模型来解决复杂问题。这很好，可以从中学到很多东西，但根据我的经验，这些不是我们在工作或副项目中面临的问题。或者至少这不是我想了解更多的内容。  我正在尝试寻找资源，了解其他人如何解决中型项目，或者如何解决一路上遇到的障碍。我的意思是你想出的那些小技巧会带来很大的不同——比如必须以不同的方式预处理数据（比如将星期几添加到特征中，或者使用不同的嵌入或以不同的方式标准化），改变指标，用特定的方式做dropout，从RNN切换到LSTM……这是你在工作中从资深同事那里学到的东西（如果幸运的话最多1-2人），所以一定有更好的方法。  到目前为止，我最好的资源是 Kaggle，我真的很喜欢看到其他人的数据处理和建模方法。你们还有其他使用的吗？ 感谢所有评论。谢谢你！    由   提交 /u/grokland   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bkmiz/d_where_to_find_new_or_inspiring_ml_projects_or/</guid>
      <pubDate>Sat, 20 Jan 2024 20:10:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型语言模型中的涌现能力只是上下文学习吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bkcqz/r_are_emergent_abilities_in_large_language_models/</link>
      <description><![CDATA[论文。我不隶属于作者。 摘要：  大型语言模型表现出了突现能力，在未经过明确训练的各种任务中表现出了卓越的性能，包括那些需要复杂推理能力的人。这种能力的出现对 NLP 的未来研究方向具有深远的影响，特别是随着此类模型的部署变得更加普遍。然而，一个关键的挑战是，对这些能力的评估常常与通过替代提示技术（例如情境学习和指令遵循）在模型中出现的能力相混淆，这些能力也会随着模型规模的扩大而出现。在这项研究中，我们首次对这些新兴能力进行全面检查，同时考虑了可能影响模型评估的各种潜在偏差因素。我们对 18 个模型进行了严格的测试，参数范围从 6000 万到 1750 亿个参数，涉及 22 项综合任务。通过 1,000 多个实验，我们提供了令人信服的证据，证明涌现能力主要归因于情境学习。我们没有发现推理能力出现的证据，因此为驱动所观察到的能力的潜在机制提供了有价值的见解，从而减轻了对其使用的安全担忧。  作者讨论了这项工作此处。  然而，我们的研究提供了不同的视角，通过以下方式解决这些问题揭示了法学硕士的新兴能力，除了语言能力之外，并不像以前认为的那样本质上是不可控制或不可预测的。相反，我们的新颖理论将它们归因于法学硕士基于几个例子完成任务的能力的表现，这种能力被称为“情境学习”（ICL）。我们证明，ICL、记忆和语言能力（语言能力）的出现相结合可以解释法学硕士所表现出的能力和局限性，从而表明法学硕士缺乏新兴推理能力。   该作品的一位作者在此视频。 这项工作在这篇 Reddit 帖子中进行了讨论（ 280 多条评论）。该作品的一位作者在那里发表了评论，包括该作品的摘要。 这里是你/H_TayyarMadabushi 的 Reddit 评论，在撰写本文时完全是关于该作品的。 该作品在 这篇博文（并非由该作品的任何作者撰写）。   由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bkcqz/r_are_emergent_abilities_in_large_language_models/</guid>
      <pubDate>Sat, 20 Jan 2024 19:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 记忆构建和巩固的生成模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bi4bc/r_a_generative_model_of_memory_construction_and/</link>
      <description><![CDATA[论文：https://www.nature.com/articles/s41562-023-01799-z 预印本版本：https://www.biorxiv.org/content/10.1101/2023.01.19.524711 &lt;强&gt;代码：https://github.com/ellie-as/generative-memory&lt; /p&gt; 摘要：  情景记忆被（重新）构建，与想象力共享神经基质，将独特的特征与基于模式的预测相结合并显示模式基于的扭曲随着整合而增加。在这里，我们提出了一个计算模型，其中海马重放（来自自联想网络）训练生成模型（变分自动编码器），以通过海马结构从内嗅、内侧前额叶和前外侧颞叶皮质中的潜在变量表征（重新）创建感觉体验。模拟显示了记忆年龄和海马病变的影响，与之前的模型一致，但也提供了语义记忆、想象力、情景未来思维、关系推理和基于图式的扭曲（包括边界扩展）的机制。该模型解释了如何通过有效结合海马和新皮质系统来存储和重建记忆的独特感觉和可预测概念元素，优化有限海马存储对新的和不寻常信息的使用。总的来说，我们相信海马体重放训练生成模型提供了记忆构建、想象力和巩固的全面说明。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bi4bc/r_a_generative_model_of_memory_construction_and/</guid>
      <pubDate>Sat, 20 Jan 2024 18:22:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bhztz/r_reinforcement_learning/</link>
      <description><![CDATA[深度强化学习泛化分析调查 https://twitter.com/EzgiKorkmazAI/status/1744434469107335628 摘要： 强化学习研究通过利用深度学习获得了巨大的成功和关注。神经网络解决高维状态或动作空间中的问题。虽然深度强化学习策略目前被部署在从医疗应用到自动驾驶汽车的许多不同领域，但该领域仍然存在一些关于深度强化学习策略的泛化能力的问题。在本文中，我们将概述深度强化学习策略遇到限制其鲁棒性和泛化能力的过拟合问题的根本原因。此外，我们将形式化和统一不同的解决方案以提高泛化性，并克服状态-动作价值函数的过度拟合。我们相信我们的研究可以为当前深度强化学习的进展提供紧凑、系统的统一分析，并有助于构建具有更高泛化能力的鲁棒深度神经策略。  &amp; #32；由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bhztz/r_reinforcement_learning/</guid>
      <pubDate>Sat, 20 Jan 2024 18:16:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] Vision Mamba：利用双向状态空间模型进行高效视觉表示学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bgoug/r_vision_mamba_efficient_visual_representation/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2401.09417 代码和模型：https://github.com/hustvl/Vim 摘要：  最近状态空间具有高效硬件感知设计的模型（SSM），即 Mamba，在长序列建模方面表现出了巨大的潜力。纯粹基于 SSM 构建高效且通用的视觉主干是一个有吸引力的方向。然而，由于视觉数据的位置敏感性以及视觉理解的全局上下文的要求，表示视觉数据对于 SSM 来说是一个挑战。在本文中，我们证明视觉表示学习对自注意力的依赖是不必要的，并提出了一种具有双向 Mamba 块（Vim）的新通用视觉主干，它用位置嵌入来标记图像序列并用双向状态空间模型压缩视觉表示。在 ImageNet 分类、​​COCO 对象检测和 ADE20k 语义分割任务上，与 DeiT 等成熟的视觉转换器相比，Vim 实现了更高的性能，同时还展示了显着改进的计算和性能。记忆效率。例如，在对分辨率为 1248×1248 的图像进行批量推理提取特征时，Vim 比 DeiT 快 2.8 倍，并节省 86.8% 的 GPU 内存。结果表明 Vim 能够克服计算和计算问题。它克服了对高分辨率图像执行 Transformer 式理解的内存限制，并且具有成为视觉基础模型的下一代骨干的巨大潜力。代码可在 此 https URL 获取。  https://preview.redd.it/gf2b6teuomdc1.png?width=2880&amp;format=png&amp;auto =webp&amp;s=3aece9b012541f8aa20dcee50eedb68bd9bed7c6   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bgoug/r_vision_mamba_efficient_visual_representation/</guid>
      <pubDate>Sat, 20 Jan 2024 17:19:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 AlphaCodium 生成代码：从即时工程到流程工程（建议的方法将基准测试的准确度从 19% 提高到 44%）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bfkz2/r_code_generation_with_alphacodium_from_prompt/</link>
      <description><![CDATA[研究（至少对我来说，痛苦的个人经历）表明，在应对复杂的编码挑战时，仅靠即时工程具有固有的局限性。 &lt;在 arXiv 上发表的一篇论文中，一项新研究的作者提出了一种名为 AlphaCodium 的新颖迭代方法，该方法专注于针对测试用例重复生成、执行和调试代码。这个具体的反馈循环允许法学硕士“学习”通过迭代培养关键编程技能。 在竞争性编程基准 CodeContests 上进行评估时，AlphaCodium 将 GPT-4 的代码生成准确性从 19% 提高到了 44%。它还超越了之前发布的方法，例如 AlphaCode，同时通过避免暴力生成，使用的模型查询数量减少了 10,000 倍。 AlphaCodium 采用的原则是：  测试驱动开发提供目标适应度函数 模块化编码 扩大测试覆盖范围揭示普遍性差距 锚定已知测试以防止回归  &lt;研究人员认为，与将模型视为通用文本生成器相比，这些软件工程实践更适合代码生成。虽然需要更多的实验，但 AlphaCodium 演示的测试调试循环可能会指向更强大的人工智能编程技术。 完整摘要位于此处。论文位于此处。仓库位于此处。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/19bfkz2/r_code_ Generation_with_alphacodium_from_prompt/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bfkz2/r_code_generation_with_alphacodium_from_prompt/</guid>
      <pubDate>Sat, 20 Jan 2024 16:30:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] The Manga Whisperer：自动生成漫画转录</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bd8ua/r_the_manga_whisperer_automatically_generating/</link>
      <description><![CDATA[     &lt; td&gt; 论文：http://arxiv.org/abs /2401.10224 Github：https://github.com/ragavsachdeva/magi 自己尝试一下：https://huggingface.co/spaces/ragavsachdeva/the-manga- Whisper/ TLDR：给定高分辨率漫画页面作为输入，Magi（我们的模型）可以 (i) 检测面板、字符、文本块，(ii) 群集字符（无需制作任何内容） (iii) 将文本块与其说话者进行匹配，(iv) 执行 OCR，(v) 生成谁说了什么以及何时说的文字记录（通过按阅读顺序对面板和文本框进行排序） ）。请参阅下图的示例。 想分享我过去几个月一直在做的事情，希望其他人觉得它有用:) 我对模型检测和聚类角色的能力特别满意（尽管由于遮挡导致视点和部分可见性发生极大变化）。由于模型无法“读取”文本，因此文本与说话者的匹配还有改进的空间。对话（它只是试图在视觉上匹配它们）。我正在努力让它变得更好。 这里有一个预告片： 预测的面板为绿色，文本块为红色，字符为蓝色。预测的字符身份关联由连接字符框中心的线显示。未显示文本到说话者关联，但提供了生成的文字记录。 我很想知道是否有人使用此模型进行很酷的个人或研究项目。一个有趣的用例是使用 Magi 来抓取并自动注释大规模漫画数据集来训练漫画扩散模型。   由   提交 /u/ragavsachdeva   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bd8ua/r_the_manga_whisperer_automatically_generating/</guid>
      <pubDate>Sat, 20 Jan 2024 14:44:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 鲜为人知的研究领域 ML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b7i8s/d_lesser_known_research_areas_ml/</link>
      <description><![CDATA[[D] 您对机器学习中哪些鲜为人知或较少探索的领域感兴趣？ （布罗德，不是高度专业化的想法或主题）我正在寻找一些领域，以便我可以研究和发现它们。   由   提交/u/mango-clay  /u/mango-clay  reddit.com/r/MachineLearning/comments/19b7i8s/d_lesser_known_research_areas_ml/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b7i8s/d_lesser_known_research_areas_ml/</guid>
      <pubDate>Sat, 20 Jan 2024 08:50:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>