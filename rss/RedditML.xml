<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 05 Dec 2024 21:16:37 GMT</lastBuildDate>
    <item>
      <title>我有一个问题[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7jpca/i_have_a_question_r/</link>
      <description><![CDATA[有没有什么方法可以让我在 geometry dash 中成为机器学习 AI？我真的不知道如何编写代码和 AI。我试过很多次，但都没有成功。如果有人能帮我这个忙，那就太好了！（顺便说一句，没人必须这样做）    提交人    /u/ghostguyhasyoutube   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7jpca/i_have_a_question_r/</guid>
      <pubDate>Thu, 05 Dec 2024 21:00:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 陷入人工智能地狱：法学硕士毕业后该做什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</guid>
      <pubDate>Thu, 05 Dec 2024 20:49:57 GMT</pubDate>
    </item>
    <item>
      <title>图像生成模型评估挑战赛（香肠、KOALA、PixArt-α）[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/</guid>
      <pubDate>Thu, 05 Dec 2024 19:26:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我是一名语言学学生，需要评估图像分类模型的准确性和效率。但我不知道如何解释这些图表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7g0g4/p_i_am_a_linguistic_student_who_need_to_evaluate/</link>
      <description><![CDATA[        提交人    /u/No_Art8744   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7g0g4/p_i_am_a_linguistic_student_who_need_to_evaluate/</guid>
      <pubDate>Thu, 05 Dec 2024 18:27:14 GMT</pubDate>
    </item>
    <item>
      <title>U-Net 与 Attention U-Net [研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/</link>
      <description><![CDATA[大家好， 我是年轻的研究员，正在研究内部数据集，为一个有趣的用例构建基础模型。但我有论文要完成，这只是我目前研究的尾声。 对于我的论文，我们决定设置一个小节来比较在 U-Net 中使用注意力模块时我的分割结果有何不同。我参考了一些关于其工作原理以及如何实现的论文。 结果很有希望（att unet 优于 unets，这并不奇怪），但我看到了一个令人担忧的对立点，即注意力 Unet 比 unet 具有更多的参数。有没有办法进行这项研究，比较有注意力和没有注意力的结果？并且没有其他额外因素影响结果（层、参数等）。 在这种情况下进行消融研究是否有意义？我还没有看到任何其他论文使用这项研究来比较类似的用例。 任何我可以浏览的论文，都欢迎提出建议和提示。    提交人    /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/</guid>
      <pubDate>Thu, 05 Dec 2024 16:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 给新机器学习导师的建议：什么项目能让我的学生找到工作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/</link>
      <description><![CDATA[我刚刚开始以机器学习导师的身份授课。我拥有计算机科学硕士学位和两年的专业经验。但我从未担任过导师（至少在这个领域没有）。今天，我正在上第一堂机器学习课，只是用幻灯片讲解基础知识，这时我的学生停下来告诉我，幻灯片对她来说太基础了。 她想多谈谈她可以做的哪些项目可以吸引雇主并获得实习机会。她直截了当地问我她应该做什么样的项目。说实话，我并不完全确定，我脑海中闪现的是训练模型来识别 MNIST 数字或其他适合（相对）新手的简单项目。但这些真的能帮助她获得实习机会吗？我对自己表示怀疑，所以我反问她，她对哪些与机器学习有关的事情充满热情，并会激励她努力工作？她回答说她可以做任何与机器学习相关的事情，她只想做能赚钱并得到公司认可的事情。 所以基本上我觉得我因为没有一个好的答案而作为一个导师失败了，如果这种情况再次发生，我希望准备好答案。你们觉得怎么样？新手或不那么新手的学生可以承担哪些项目，使他们更容易找到工作和实习？ 既然我们谈到这个，你认为我应该准备哪些东西才能成为一名更好的导师。你希望你的机器学习导师为你准备什么样的东西？你想要关于关键概念的幻灯片课程吗？有练习的 Jupyter 笔记本？还有别的吗？老实说，我不确定我应该做些什么来为这些课程做准备。    提交人    /u/Seijiteki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/</guid>
      <pubDate>Thu, 05 Dec 2024 15:16:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] ReVersion：从图像中学习关系提示以进行受控扩散生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/</link>
      <description><![CDATA[ReVersion 引入了一种使用扩散模型学习和传输视觉关系的新方法。它不是仅仅关注物体的外观，而是通过关系提示和专门的采样技术学习物体如何相互作用。 关键技术方面： - 使用冻结的预训练文本到图像扩散模型作为基础 - 通过对比学习实现关系引导，将提示引导至关系丰富的潜在空间 - 采用关系焦点采样来强调高级交互而不是低级细节 - 创建捕捉物体之间空间和交互关系的关系提示 - 引入用于评估关系反转方法的新基准数据集 结果： - 在保留物体关系的同时优于现有方法，同时允许外观灵活性 - 在“在……之上”、“旁边”、“里面”等空间关系上表现出色- 成功地将学习到的关系转移到新的对象对 - 在不同风格和环境中保持关系一致性 我认为这种方法对于改进需要处理具有多个交互对象的复杂场景的自动图像生成系统特别有价值。学习和转移关系的能力，而不仅仅是外观，可以帮助弥合当前图像生成能力与人类对物体在空间中相互作用方式的理解之间的差距。 我认为关系焦点采样技术的应用范围不仅限于关系学习——它可能在我们需要在扩散模型中强调高级特征而不是低级细节的任何地方都很有用。 TLDR：新方法使用扩散模型从图像中学习视觉关系，引入关系引导和关系焦点技术，在空间关系保存和转移方面显示出很强的效果。 完整摘要在这里。论文此处。    由    /u/Successful-Western27 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/</guid>
      <pubDate>Thu, 05 Dec 2024 14:30:05 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 无符号整数表示为向量，重点是外推</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/</link>
      <description><![CDATA[大家好， 我正在研究一个回归任务，该任务将基于 Transformer 的架构应用于基于网格的结构。想象一下迷宫之类的东西，其目标是预测到目标的距离。每个输入标记都包含分类特征以及 x/y 坐标。这个想法是在小网格上进行训练，然后推广到更大的网格。 这是我目前用于坐标和标记嵌入的方法： x_emb = self.w_x.weight * x # 形状：bs，sequence len，1，d y_emb = self.w_y.weight * y # 形状：bs，sequence len，1，d cat_emb = self._categ(categ) sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # 形状：bs，sequence len，num_cat，d sequence_emb = serial_emb.view(bs, seq_len, -1) transformer_inputs = self._linear(sequence_emb) 换句话说，x/y 坐标嵌入是缩放的可学习向量。但是，这种方法的泛化能力有限。我怀疑改进坐标表示至关重要。 不幸的是，这个基于 token 的结构是该任务所必需的，所以我需要专注于制作一个智能 token 表示。我故意避免减去嵌入来计算相对距离，因为核心目标是让模型自己学习这些距离。 到目前为止，我已经尝试过以下方法： 我还尝试过以下方法：  位置编码而不是缩放向量 对数缩放向量 指数缩放向量  有人知道在这种情况下用于数值表示的有趣工作或技术吗？任何建议都将不胜感激！ 如果您发现有关基于大小和标记的 Transformer 外推的有趣论文，我很乐意从中汲取任何灵感。    提交人    /u/mbus123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/</guid>
      <pubDate>Thu, 05 Dec 2024 10:33:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 除了模型性能指标的变化之外的数据漂移检测方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/</link>
      <description><![CDATA[大家好， 正如标题所暗示的，我一直依靠（接近）实时监控模型性能指标来查看我的用例中是否发生了数据漂移。 我想知道您是否知道其他更复杂/更高级的方法来检测数据漂移。很想听听任何类型的方法，无论它们针对的是协变量/特征漂移、目标/标签漂移还是概念漂移的检测。 如果您可以分享任何 Python 或 R 实现来执行上述数据漂移检查就更好了。 提前谢谢！    提交人    /u/YsrYsl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/</guid>
      <pubDate>Thu, 05 Dec 2024 01:02:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每日论文讨论 - FlashAttention 3</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/</link>
      <description><![CDATA[      作为 Yannic Kilcher discord 服务器上每日论文讨论的一部分，我将自愿领导对 FlashAttention-3 🧮 🔍 的分析 📜 FlashAttention-3：具有异步和低精度的快速准确注意力 🌐 https://arxiv.org/abs/2407.08608 🕰 2024 年 12 月 5 日星期四 01:30 AM UTC // 2024 年 12 月 5 日星期四 7.00 AM IST // 2024 年 12 月 4 日星期三 5:30 PM PT FlashAttention-3 引入了三个巧妙的想法来提升 Hopper GPU 的性能 - 1️⃣ 生产者-消费者异步：这种技术将任务分为不同的部分。例如，如果我们有 2 个 warpgroup（标记为 1 和 2 - 每个 warpgroup 都是一组 4 个 warp），我们可以使用同步屏障 (bar.sync)，以便 warpgroup 1 首先执行其 GEMM（例如，一次迭代的 GEMM1 和下一次迭代的 GEMM0），然后 warpgroup 2 执行其 GEMM，而 warpgroup 1 执行其 softmax，依此类推。通过这样做，它可以更好地利用 GPU 资源并隐藏可能降低性能的延迟。 2️⃣ 隐藏 Softmax 操作：FlashAttention-3 通过将较慢的 softmax 计算与较快的矩阵乘法（GEMM）重叠来提高效率。它不是等待 Softmax 完成后再开始下一步计算，而是并行处理它们，从而加快整个过程。 3️⃣ 硬件加速低精度计算：这种方法使用高级 GPU 功能以较低的精度（FP8）执行计算，速度更快且占用更少的内存。 FlashAttention-3 对其算法进行了调整，以有效处理这些低精度计算，在保持准确性的同时将处理速度提高了近一倍。 https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a    提交人    /u/CATALUNA84   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/</guid>
      <pubDate>Wed, 04 Dec 2024 20:03:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] ICLERB：一种评估嵌入和重排器以进行上下文学习的更好方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</link>
      <description><![CDATA[当前的嵌入基准（如 MTEB 和 BEIR）包括多个数据集和任务，但基本上基于相关性注释（如文本相似性）。这些对于为大多数搜索/检索用例选择最佳嵌入非常有用。如今，许多人使用这些嵌入来检索项目以进行上下文学习（例如文档 RAG 或小样本学习），以使 LLM 适应特定任务。然而，他们仍在使用 MTEB 来选择最佳嵌入，即使该基准上的性能并不一定意味着他们下游 LLM 任务的更好性能（毕竟 MTEB 是在 2021 年问世的）。 在我们的最新论文中，我们提出了一个新的评估框架和基准，称为 ICLERB。该基准测试通过使用直接偏好优化 (DPO) 作为相关性指标来挑战传统方法，以反映嵌入和重新排序器与 LLM 一起用于上下文学习时的实际效用。 https://arxiv.org/pdf/2411.18947 主要亮点： - 嵌入优于重新排序器：我们发现更简单的嵌入模型优于来自 Cohere、NVIDIA 和 VoyageAI 的高容量重新排序器。 - 大小不是一切：在三个 Snowflake 嵌入中，最小的模型（33M 个参数）优于较大的模型（109M 和 334M）。 - 重新思考训练和评估目标：这些发现表明，训练和评估更大的检索仅基于文本相似性的模型可能会适得其反。 有趣的是，某些模型（如 BGE）的性能对所使用的数据集或 LLM 非常敏感，而其他模型（如 NV）则更稳定。我们计划继续向基准添加更多数据集和 LLM，以扩大其范围。 在我们努力改进 ICLERB 的过程中，很想听听您的想法和反馈！您是否希望包含其他检索模型、LLM 或数据集？    提交人    /u/Crossing_Minds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</guid>
      <pubDate>Wed, 04 Dec 2024 19:05:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 二元适应度优化。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/</link>
      <description><![CDATA[您是否知道任何论文或哪个领域可以解决以下问题：您有一个需要优化的函数 f(x)，但您正在优化的成本/适应度是二进制的。我正在做一个关于这个的项目，我不确定这个领域是否有研究。 非常感谢 &lt;3    提交人    /u/pamintandrei   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/</guid>
      <pubDate>Wed, 04 Dec 2024 18:30:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在GNN中定制注意力机制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</link>
      <description><![CDATA[我正在寻找一些基础代码或算法，以便在处理节点预测任务的图表时创建新的注意机制。我看到恒星图中有一些文档，但我想知道是否还有其他有用的材料。谢谢！！！    提交人    /u/Whole_Hat_4852   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</guid>
      <pubDate>Wed, 04 Dec 2024 14:56:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 01 Dec 2024 16:00:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>