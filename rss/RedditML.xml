<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 09 Feb 2024 03:13:33 GMT</lastBuildDate>
    <item>
      <title>[R] 在同一系统中同时使用 Intel Arc 和 RTX 卡</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amdw8f/r_using_both_an_intel_arc_and_rtx_card_in_the/</link>
      <description><![CDATA[嘿，我想知道是否有人知道我是否可以在我的机器学习装备中同时使用 Intel Arc 和 Nvidia RTX 卡。我的朋友在 Intel 工作，给了我他的两张 Arc 770 卡。我目前有一个机器学习设备，配备 Ryzen、128 GB DDR4 和 RTX 4080。我使用 PyTorch 主要是想知道是否可以同时使用所有 3 个卡来训练模型。   由   提交/u/Helpful_Pickle1246   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amdw8f/r_using_both_an_intel_arc_and_rtx_card_in_the/</guid>
      <pubDate>Fri, 09 Feb 2024 02:54:55 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 当给定论文或主题发布任何新代码时，您会收到提醒！希望得到您的反馈:)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amdgun/project_get_alerted_when_any_new_code_is_released/</link>
      <description><![CDATA[      刚刚为这个社区构建了一些东西，我想我会亲自分享🙂希​​望得到您的反馈:)您可以在任何时候收到提醒针对给定论文或主题发布了新代码！您可以在浏览互联网时选择任何论文或主题（Google、Scholar、Arxiv、IEEE 等） 只需安装代码查找器扩展程序（Chrome：https://chromewebstore.google.com/detail/ai-code-finder-for-papers/aikkeehnlfpamidigaffhfmgbkdeheil | Firefox：&lt; a href=&quot;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/&quot;&gt;https://addons.mozilla.org/en-US/firefox/addon/code- finder-catalyzex/ | 边缘：https://microsoftedge.microsoft .com/addons/detail/get-papers-with-code-ever/mflbgfojghoglejmalekheopgadjmlkm），单击浏览时遇到的任何铃声/警报图标，然后按照屏幕上的后续步骤操作 🙂  &lt; p&gt;此外，通过提醒  将您感兴趣的领域的最新进展直接发送到您的收件箱。 作者的最新作品：成为第一个了解作者的人发布新论文。  ​ https://preview.redd.it/8l5pl2cd1hh​​c1.png?width=3074&amp;format=png&amp;auto=webp&amp;s=1c9263dc0ec9d8fcc726 67d1d2b912fdb147c081&lt; /p&gt; https://preview.redd。 it/6muoo2cd1hh​​c1.png?width=1848&amp;format=png&amp;auto=webp&amp;s=5f23f464c214a9dbc70db41276f95fda3238fa47 https://preview.redd.it/fk6rf3cd1hh​​c1.png?width=1890&amp;format=png&amp;auto=webp&amp;s=7142b62c52 88db0d7a7863480b2f262836d2758b   由   提交/u/fullerhouse570  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amdgun/project_get_alerted_when_any_new_code_is_released/</guid>
      <pubDate>Fri, 09 Feb 2024 02:33:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 单图像 3D 场景重建的当前技术水平？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amdaw6/d_current_state_of_the_art_for_single_image_3d/</link>
      <description><![CDATA[我知道的大多数 Nerf 论文都需要多个输入视图，我发现的主要论文是 Single view nerf with Depth Teacher。  想知道是否有人有其他论文或技术可以分享/讨论他们最成功的部分？   由   提交/u/AbjectDrink3276  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amdaw6/d_current_state_of_the_art_for_single_image_3d/</guid>
      <pubDate>Fri, 09 Feb 2024 02:25:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从训练有素的神经状态中获取手工技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amcyr4/d_eriving_handcrafted_techniques_from_trained/</link>
      <description><![CDATA[神经网络学习近似一些极其复杂的函数。例如，假设有一个用于神经图像压缩的网络。该模型学习了一个函数，以便在该任务中表现良好。  压缩“程序”它清楚地表明在我的机器上可以工作，尽管需要额外的步骤。因此，我们可以制作该学习压缩“技术”的等效手工版本。 我想知道是否有研究从对复杂函数进行建模的学习模型状态中导出经典手工等效项。这种手工制作的（更简单的）等价物将比它们臃肿的神经表兄弟更有效。当我们试图理解从神经配置到“重写”人工智能的过程时，这个话题是否属于“可解释的人工智能”领域？它是一种手工方法吗？   由   提交 /u/WhereIsMyButtplug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amcyr4/d_eriving_handcrafted_techniques_from_trained/</guid>
      <pubDate>Fri, 09 Feb 2024 02:08:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博士学位对于在 FAANG 公司从事机器学习工作有多重要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amc3eu/d_how_important_is_a_phd_for_working_in_ml_at/</link>
      <description><![CDATA[大家好， 我是一名来自欧盟的计算机工程专业毕业生，目前正在攻读数据科学硕士学位。我对 ML 的研究和开发充满热情，并且对 LLM 模型有一些经验。我喜欢做项目和写报告，我认为它们比考试更有意义。 我的目标是在 FAANG 公司之一从事 ML 工作，但我不确定是否需要这样做为此攻读博士学位。我的一些导师在 FAANG 公司工作，但不在机器学习领域工作，他们告诉我，博士学位对于进入那里的机器学习职位至关重要。然而，我也看到一些没有博士学位或拥有不同领域博士学位的人在 FAANG 公司找到了 ML 工作。 所以，我想知道 ML 社区对此有何看法。博士学位对于在 FAANG 公司从事机器学习工作有多重要？攻读博士学位与直接进入工业界相比，有哪些优点和缺点？  我非常感谢您的任何见解或建议。  感谢您的阅读。   由   提交 /u/aedlearndl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amc3eu/d_how_important_is_a_phd_for_working_in_ml_at/</guid>
      <pubDate>Fri, 09 Feb 2024 01:26:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 具有累积金额的 Mamba</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amb3xu/d_mamba_with_cumulative_sums/</link>
      <description><![CDATA[Mamba 是一个带有数据的状态空间模型相关系数。它最初是通过关联扫描进行训练的，目前是pytorch 不直接支持，因此作者为其编写了自定义 cuda 内核（这具有内核融合的额外好处）。为了简化这一点，有人在一个文件中编写了 最小版本的 mamba，其中关联扫描操作被 for 循环取代，为了实现的简单性而牺牲了效率。 不过，我认为有一种方法可以在纯pytorch中实现mamba，并且不会损失太多效率，那就是使用累积和 pytorch 有效支持。此实现封装在我对最小 mamba 存储库的相当简单的 commit 中，它提供了大约 14 倍的加速最小的 for 循环实现（代码较少）。还通过与 for 循环实现的输出进行比较来验证其正确性。 高级思想基本上是“分解”循环。将原始并行扫描降低到两个累加和的比率，同时保留关联扫描相同的时间复杂度 O(n) 和并行效率 O(logn)。  &amp; #32；由   提交/u/dna961010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amb3xu/d_mamba_with_cumulative_sums/</guid>
      <pubDate>Fri, 09 Feb 2024 00:39:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Gemini 和 LlamaIndex 的多模式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am7kda/d_multimodal_using_gemini_and_llamaindex/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am7kda/d_multimodal_using_gemini_and_llamaindex/</guid>
      <pubDate>Thu, 08 Feb 2024 22:01:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] MoCo的动力动力不成立？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am7bc6/d_moco_motivation_for_momentum_does_not_hold/</link>
      <description><![CDATA[MoCo 论文比较了端到端预训练后的性能（查询和密钥都来自在线编码器），经过适当的 MoCo 预训练后，并带有存储库（只有查询来自在线编码器，较旧的查询来自较旧的编码器）。端到端训练提供了与 MoCo 相同的精度，作为队列中键数量/批量大小的函数，但批量大小在常见硬件上不能增长得那么大。记忆库训练的准确性较低，作者说这证实了他们的假设，即旧密钥与查询和新密钥不一致，但我不明白！与动量编码器一样，精度随着队列长度的相同类型的函数而增长，并且键越多（并且越旧并且据说一致性越差），精度就越高，但仅低 2%。随着队列的增长，它必须变得更糟，或者回报按比例减少，才能证实作者的假设，对吧？在这种情况下，势头并不令人信服，而在 DINO 中，它有不同的解释和目的，听起来不错（而且性能显然更好）   由   提交/u/reverendCappuccino   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am7bc6/d_moco_motivation_for_momentum_does_not_hold/</guid>
      <pubDate>Thu, 08 Feb 2024 21:50:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我找不到关于美术绘画分类的少样本学习的论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am64k4/d_why_cant_i_find_any_papers_on_fewshot_learning/</link>
      <description><![CDATA[也许我的（重新）搜索能力不是最好的，但我找不到任何关于美术绘画分类的论文问题。是因为这样做没有意义吗？我错过了什么吗？我刚刚开始研究小样本学习，据我了解，这似乎是使用它的一个很好的案例场景。    由   提交 /u/hellounderweinewr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am64k4/d_why_cant_i_find_any_papers_on_fewshot_learning/</guid>
      <pubDate>Thu, 08 Feb 2024 21:00:29 GMT</pubDate>
    </item>
    <item>
      <title>[D]对以反射（自我调整）为动力的LLM代理人系列作品的关注</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/</link>
      <description><![CDATA[我们在基于 LLM 的代理中看到了大量的作品，它们可以在 Web 应用程序上执行任务，例如 webshop、webarena、agentbench等...&lt; /p&gt; 此外，我们还可以找到以下关于基于反射的智能体的工作，该智能体从与环境的交互中获取先前试验的反馈和错误。典型的工作是Reflexion: Language Agents with Verbal Reinforcement Learning 在每次试验中，代理，或者说，llm，消化提示，其中不仅包含当前试验的历史记录，还包含还有之前试验的系统信息或反馈或错误消息。反馈可以来自系统设置，也可以来自另一个更强大的LLM，可以充当超级法官来提供反馈。 无论如何，我不认为这是RL，因为代理没有学习过程，但有一个提示。 我最关心的是这个标签是否泄漏？智能体从环境中获得反馈，并通过更多的尝试，当然，智能体应该对最终答案有更清晰的方法。那么有什么意义呢？ 我看到一篇文章与我有同样的担忧：noahshinn/reflexion: [ NeurIPS 2023] 反射：具有言语强化学习的语言智能体 (github.com) ​ 想听听您从学术和工业角度的看法. ​ ​ ​ ​ &lt; /div&gt;  由   提交/u/yanancc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/</guid>
      <pubDate>Thu, 08 Feb 2024 19:11:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是什么让 PPO 强化学习不仅仅是拥有一个花哨的损失函数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</link>
      <description><![CDATA[我正在考虑使用 RLHF 训练扩散模型，并且正在查看这篇论文 kvablack/ddpo-pytorch：用于微调扩散模型的 DDPO，在具有 LoRA 支持的 PyTorch 中实现（github.com），但代码本身似乎只是反向传播基于unet的一个奇特的（乍一看是可微分的！）损失函数。强化学习与普通模型训练有何区别？两者相同吗？这只是术语问题吗？  在此处复制相关代码？ for i, example in tqdm( list(enumerate(samples_batched)), desc=f&quot;Epoch {epoch}.{inner_epoch} :training&quot;,position=0,disable=notaccelerator.is_local_main_process, ): if config.train.cfg: # 将否定提示连接到示例提示以避免两次前向传递 embeds = torch.cat( [train_neg_prompt_embeds, Sample[&quot;prompt_embeds&quot;) ;]] ) else: embeds = sample[“prompt_embeds”] for j in tqdm( range(num_train_timesteps), desc=“Timestep”,position=1, left=False,disable=not Accelerator.is_local_main_process, ): with Accumulate(unet): with autocast(): if config.train.cfg:noise_pred =unet( torch.cat([sample[&quot;latents&quot;][:, j]] * 2), torch.cat([样本[“时间步长”][:, j]] * 2), 嵌入, ).sample Noise_pred_uncond, Noise_pred_text = Noise_pred.chunk(2) Noise_pred = ( Noise_pred_uncond + config.sample.guidance_scale * (noise_pred_text - Noise_pred_uncond) ) else :noise_pred =unet(sample[“latents”][:, j],sample[“timesteps”][:,j],embeds,).sample#计算当前模型下给定潜伏的next_lateents的对数概率_ , log_prob = ddim_step_with_logprob( pipeline.scheduler,noise_pred,sample[“timesteps”][:, j],sample[“latents”][:,j], eta=config.sample.eta, prev_sample=sample[&quot; ;next_latents&quot;][:, j], ) # ppo 逻辑优点 = torch.clamp(sample[&quot;advantages&quot;], -config.train.adv_clip_max, config.train.adv_clip_max, )ratio = torch.exp(log_prob -样本[“log_probs”][:, j]) unclipped_loss = -advantages * 比率 Clipped_loss = -advantages * torch.clamp(ratio, 1.0 - config.train.clip_range, 1.0 + config.train.clip_range, ) loss = torch .mean(torch.maximum(unclipped_loss, Clipped_loss)) # 调试值 # John Schulman 说 (ratio - 1) - log(ratio) 是一个更好的 # 估计器，但大多数现有代码都使用它，所以... # http:// /joschu.net/blog/kl-approx.html 信息[“approx_kl”].append( 0.5 * torch.mean((log_prob - 样本[“log_probs”][:, j]) ** 2) ) 信息[“clipfrac”].append( torch.mean( ( torch.abs(ratio - 1.0) &gt; config.train.clip_range ).float() ) ) info[&quot;loss&quot;].append(loss) # 向后传递 Accelerator.backward(loss) if Accelerator.sync_gradients: Accelerator.clip_grad_norm_(unet.parameters(), config. train.max_grad_norm ) optimizationr.step() optimizationr.zero_grad()    由   提交 /u/ExaminationNo8522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</guid>
      <pubDate>Thu, 08 Feb 2024 16:49:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离开我的胸膛。我正在攻读机器学习博士学位，但我是个失败者。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</link>
      <description><![CDATA[我的机器学习博士学位已经过半了。 我很幸运，进入了一个很好的项目，尤其是在一个好的项目中。实验室的学生都是超级明星，毕业后会找到很好的工作。我不是他们中的一员。我有一本蹩脚的、技术性不高的出版物，我正在努力寻找一个在我的能力范围内可以解决的新问题。我已经很努力了。我在本科生和硕士期间一直在做研究，尽我所能 - 做项目、阅读论文、学习机器学习和数学课程、为教授撰写资助...... 事实是，我可以达不到产生新想法的水平。无论我多么努力，这都不是我的事。我想为什么。我开始怀疑 STEM 是否一开始就不是我的菜。我环顾四周，发现有些人的大脑只是“理解”了这一点。事情变得更容易。对我来说，这需要额外的努力和额外的时间。在本科期间，我可以更加努力、更长时间地学习。嗯，不是为了博士学位。尤其是在这个快节奏、拥挤的领域，我需要吸收新东西并快速发布。 我是一个冒名顶替者，这不是一种综合症。我快被抓了其他人都获得了多个实习机会等等。我到处都被拒绝。看来现在他们知道了。他们知道我没用。我想对我的顾问说这些，但他是如此的天才，以至于他无法理解普通人的想法。我所有的高级实验室伙伴都是全职工作人员，所以实际上我现在是实验室中最资深的。   由   提交 /u/rsfhuose   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</guid>
      <pubDate>Thu, 08 Feb 2024 15:10:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 宗师级国际象棋无需搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</guid>
      <pubDate>Thu, 08 Feb 2024 08:23:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</link>
      <description><![CDATA[我读过很多东西说“除非你绝对确定你想读博士”，但我不确定。我的动机主要是为了打开更多的工业大门。我想成为一名研究科学家/机器学习工程师/数据科学，甚至是量化角色，但如果没有高级学位，这些角色越来越难获得。  我应该提一下，我的本科学位不是计算机科学，尽管我有机器学习方面的研究经验，并且选修了一些数学/统计课程（线性算法、统计、概率、计算）。我的问题是： 1) 攻读 4-6 年机器学习博士学位的机会成本（而不是从数据分析等较低入门职位开始并从那里向上工作）是否值得开设更多工业门？  2）我在没有博士学位的情况下获得研究科学家职位的可能性有多大？ 3）我可能还想最终转向初创公司（可能是在工业界之后或立即）。在这种情况下，博士学位并没有多大帮助。但是考虑到一切（事实上我不确定自己会做什么和想要什么），博士学位可以成为弄清楚我想要什么的途径吗？ 我觉得博士学位不值得它仅适用于 DS/MLE 甚至 Quant 角色。更不用说研究科学家了，对于如今的单身汉来说，我什至获得这些职位之一的机会都很难。我申请了大量的数据科学工作，但没有得到回应——这就是为什么我认为博士学位可能最终会引起一些关注。   由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</guid>
      <pubDate>Thu, 08 Feb 2024 05:06:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>