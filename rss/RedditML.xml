<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 19 May 2024 09:14:36 GMT</lastBuildDate>
    <item>
      <title>[D] 如何在Word2Vec CBOW方法中获得词嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cviv1u/d_how_to_get_word_embedding_in_word2vec_cbow/</link>
      <description><![CDATA[我正在尝试使用 PyTorch 实现 CBOW 算法。我知道隐藏层是目标单词的嵌入，它的维度等于我想要嵌入的维度。我很难理解何时获得嵌入。 在反向传播之后，我是否再次需要前向传递才能获得正确的隐藏层输出，或者是其他什么？另外，如果我有任何错误，请纠正我。 以下是 CBOW 类实现。 class CBOW (模块): def __init__(self, in_channel: int, out_channel : int, winSize : int): super().__init__( ) self.N = in_channel self.V = out_channel self.lin1 = 线性(in_features= self.N, out_features= self.V) self.lin2= 线性(in_features=self.V, out_features= self.N) self.softmax = Softmax(dim=1) def front(self,input : torch.Tensor): assert len(input.shape) == 2, “收到的输入尺寸不正确”断言 input.shape[1] == self.N,“单词特征向量不匹配”输入= self.lin1（输入）嵌入= torch.mean（输入，dim = 0，keepdim = True）输出= self.lin2（嵌入）返回self.softmax（out）def向后（自我，预测：torch.Tensor，目标：torch.Tensor）：断言prediction.shape == target.shape，f“输入形状不匹配\n预测形状：{prediction.shape}\n目标形状：{target.shape}” loss_fn = MSELoss() loss = loss_fn(prediction, target) loss.backward() 返回损失    由   提交 /u/Harshtherocking   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cviv1u/d_how_to_get_word_embedding_in_word2vec_cbow/</guid>
      <pubDate>Sun, 19 May 2024 07:35:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 柯尔莫哥洛夫-阿诺德网络 (KAN) 解释：MLP 的优越替代方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvgn92/r_kolmogorovarnold_networks_kans_explained_a/</link>
      <description><![CDATA[ 由   提交 /u/mehulgupta7991   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvgn92/r_kolmogorovarnold_networks_kans_explained_a/</guid>
      <pubDate>Sun, 19 May 2024 05:09:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 回顾我的 ML 之旅：135 门 Coursera 课程、35 门 Udemy 课程和 32 门 Udacity 课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvg6xp/d_looking_back_at_my_ml_journey_135_coursera/</link>
      <description><![CDATA[      每次我回顾我的ML 之旅，我只意识到电子学习平台的力量。首先，我将感谢 Coursera、Udemy、Udacity 平台，我想花点时间回顾一下过去 6 年我的机器学习 (ML) 之旅。这是学习、挑战和成就的旋风，我认为分享我的经验可能会激励或帮助走在类似道路上的其他人。 我的旅程的开始 我在学士学位时学习了电子和电信。我是一个数学迷，这是我在参加吴恩达爵士的课程后选择机器学习的主要原因。和你们中的许多人一样，我一开始并不是对机器如何从数据中学习感到好奇😂。我开始是因为 ML 里面的数学。我有一些基本的编程知识，但 ML 的世界对我来说是全新的。我决定一头扎进去，Coursera 是我的第一站。 Coursera：135 门课程 正如我之前所说，我从吴恩达 (Andrew Ng) 的著名“机器学习”课程开始。这门课程为我打下了坚实的基础，从那以后，我探索了无数其他课程。以下是一些脱颖而出的课程： 吴恩达 (Andrew Ng) 的深度学习专项课程：这一系列课程帮助我掌握了神经网络和深度学习的复杂性。 应用数据科学与 Python 专项课程：提供了一种将 ML 技术应用于实际问题的实践方法。 机器学习数学：加强了我对 ML 算法数学基础的理解。Coursera 上的结构化学习路径和高质量内容让我保持参与并不断接受挑战。 还有更多专项课程，如“医学人工智能”、“强化学习专项课程”等。我一个人就完成了 50 多门 DL.AI 课程 Udemy：35 门课程 向 Lazy Programmer 致敬。地球上最好的 ML 导师🌏​​。毫无疑问。 Udacity：32 门课程 最后，我转向 Udacity 的纳米学位课程，这些课程更具沉浸感和基于项目的特点。结构化的课程和以行业为中心的项目帮助我将所学知识应用到实际环境中。我完成的值得注意的纳米学位包括： 机器学习工程师纳米学位英特尔物联网和边缘人工智能纳米学位 AWS 机器学习纳米学位回首过去，这段旅程紧张但回报丰厚。以下是一些关键要点： 一致性是关键：定期、专门的学习时间至关重要。随着时间的推移，小而持续的努力会累积起来。 实践经验：从事项目和解决现实问题是无价的。这是理论知识与实际应用相遇的地方。 社区和网络：参与在线社区、参加网络研讨会和参与论坛帮助我保持动力和联系。 再次为我的导师和所有这些平台致以崇高的敬意。现在我是一名高级数据科学家。也在我国的一些学院讲课。也是斯里兰卡🇱🇰唯一的人工智能导师。    提交人    /u/1zuu   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvg6xp/d_looking_back_at_my_ml_journey_135_coursera/</guid>
      <pubDate>Sun, 19 May 2024 04:41:56 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分布式系统的交叉点 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvfrd9/intersection_of_ml_distributed_systems_d/</link>
      <description><![CDATA[分布式系统和机器学习交叉领域存在哪些现有问题？ 我在这两个领域都有不错的背景，并且我想要从事利用分布式计算来解决机器学习问题的项目。有哪些值得一看的好资源？或者如何开始？   由   提交/u/tcuser12  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvfrd9/intersection_of_ml_distributed_systems_d/</guid>
      <pubDate>Sun, 19 May 2024 04:16:12 GMT</pubDate>
    </item>
    <item>
      <title>[D]为什么我们没有在论文中看到零样本的 Truthfulqa 性能？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvf2jf/dwhy_dont_we_see_zero_shot_truthfulqa_performance/</link>
      <description><![CDATA[我的直觉是它是最重要的指标之一，但我们通常会看到多次拍摄的性能。就像 phi3 论文中报告的 10 次射击性能一样。   由   提交 /u/Bytesfortruth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvf2jf/dwhy_dont_we_see_zero_shot_truthfulqa_performance/</guid>
      <pubDate>Sun, 19 May 2024 03:36:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否可以用高光谱卫星图像训练 ViTMAE？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</link>
      <description><![CDATA[我正在尝试训练 ViTMAE 编码器来学习一些高光谱卫星图像的表示。图像采用 TIFF 格式，并且有许多波段 (224)。是否可以使用如此大量的输入频段来训练 ViTMAE？知道我应该怎么做吗？   由   提交/u/Robur_131  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</guid>
      <pubDate>Sat, 18 May 2024 23:42:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴收敛速度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</link>
      <description><![CDATA[我正在使用不平衡的数据集训练 mamba 进行顺序标记任务，我有近 800k 的训练示例。在一个时代之后，少数族裔班级的表现非常糟糕，接近于零。我试图过度拟合一批，但无法实现这一目标。我也尝试过减肥。我想知道这是否正常？曼巴舞是不是从一开始就是这样，然后开始收敛的？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</guid>
      <pubDate>Sat, 18 May 2024 20:37:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] Grounding DINO 1.5 发布：最强大的开集检测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</guid>
      <pubDate>Sat, 18 May 2024 16:05:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基础时间序列模型被高估了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</link>
      <description><![CDATA[我一直在探索基础时间序列模型，如 TimeGPT、Moirai、Chronos 等，并想知道它们是否真的具有强大的样本潜力 -高效的预测，或者他们只是借用 NLP 基础模型的炒作并将其引入时间序列领域。 我可以理解为什么它们可能会起作用，例如，在需求预测中，它是关于但它们能否处理任意时间序列数据，如环境监测、金融市场或生物医学信号，这些数据具有不规则模式和非平稳数据？ 它们的概括能力是否被高估了？    由   提交 /u/KoOBaALT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</guid>
      <pubDate>Sat, 18 May 2024 16:00:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Llamas 用英语工作吗？论多语言 Transformer 的潜在语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10588 代码：https://github.com/epfl-dlab/llm-latent-language 数据集：https://huggingface.co/datasets/wendlerc/llm-latent-language Colab 链接：  (1) https://colab .research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk_QPHBZb7K?usp=sharing (2) https://colab.research.google.com/drive/1EhCk3_CZ_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing 摘要：  我们询问在不平衡的、以英语为主的语料库上训练的多语言语言模型是否使用英语作为内部枢轴语言——这个问题对于理解语言模型如何发挥作用至关重要以及语言偏见的根源。我们的研究重点关注 Llama-2 系列变压器模型，使用精心构建的非英语提示和独特的正确单标记延续。从一层到另一层，变压器逐渐将最终提示标记的输入嵌入映射到计算下一个标记概率的输出嵌入。通过高维空间跟踪中间嵌入揭示了三个不同的阶段，其中中间嵌入（1）从远离输出令牌嵌入的地方开始； (2) 已经允许在中间层中解码语义上正确的下一个标记，但给予其英语版本比输入语言版本更高的概率； (3) 最后进入嵌入空间的输入语言特定区域。我们将这些结果转化为概念模型，其中三个阶段分别在“输入空间”、“概念空间”和“输出空间”中运行。至关重要的是，我们的证据表明抽象的“概念空间”是存在的。比其他语言更接近英语，这可能会对多语言语言模型的偏见产生重要影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</guid>
      <pubDate>Sat, 18 May 2024 15:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 鲁棒智能体学习因果世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10877 摘要：  长期以来，人们一直假设因果推理在强大而通用的智能。然而，尚不清楚智能体是否必须学习因果模型才能推广到新领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布变化下满足后悔界限的智能体都必须学习数据生成过程的近似因果模型，该模型收敛到最佳智能体的真实因果模型。我们讨论了这一结果对包括迁移学习和因果推理在内的多个研究领域的影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</guid>
      <pubDate>Sat, 18 May 2024 15:06:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 命名实体识别库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</link>
      <description><![CDATA[大家好，我需要决定使用哪个库进行命名实体识别。我使用过 spaCy，它运行良好，但我需要一个允许我对实体和子实体进行分类的库。有人做过类似的事情吗？我的意思是，同一个词可以是多个实体。 spaCy 提供了 SpanCat 管道，理论上可以实现这一点，但我在创建训练语料库时遇到了麻烦。我认为这是因为他们希望你购买像 Prodigy 这样的注释文本框架。   由   提交/u/Original_Ad8019   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</guid>
      <pubDate>Sat, 18 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>[N] ICML 2024 离散运算可微分研讨会 🤖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</link>
      <description><![CDATA[大家好！ 今年我们将在 ICML 组织可微分几乎所有内容研讨会。&lt; /p&gt; 许多离散操作，例如排序、topk、最短路径、聚类（等等）几乎到处都有零梯度，因此不适合现代基于梯度的学习框架（例如深度学习）。本次研讨会将涵盖旨在解决此类问题的研究主题！ https:// Differentiable.xyz/ 我们鼓励任何从事相关主题工作的人提交他们的作品。即使您没有提交，也请务必参加 ICML 的研讨会，观看即将举行的一些激动人心的演讲！ 我在下面附上了研讨会的完整摘要！祝你当前的工作一切顺利，L :) 梯度和导数是机器学习不可或缺的一部分，因为它们支持基于梯度的优化。然而，在许多实际应用中，模型依赖于实现离散决策的算法组件，或者依赖于离散的中间表示和结构。这些离散步骤本质上是不可微分的，因此破坏了梯度流。要使用基于梯度的方法来学习此类模型的参数，需要将这些不可微分的组件变成可微分的。这可以通过仔细考虑来完成，特别是使用平滑或松弛来为这些组件提出可微的代理。随着模块化深度学习框架的出现，这些想法在机器学习的许多领域变得比以往任何时候都更加流行，在短时间内生成了大量“可微分的一切”，影响了渲染、排序和排名等各种主题，凸优化器、最短路径、动态规划、物理模拟、神经网络架构搜索、top-k、图算法、弱监督学习和自监督学习等等。 本次研讨会将为任何可区分的事物提供一个论坛，汇聚学术界和行业研究人员，突出挑战和发展，提供统一的想法，讨论实际的实施选择并探索未来的方向。   由   提交/u/machine_learning_res   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</guid>
      <pubDate>Sat, 18 May 2024 13:22:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPT-Burn：纯 Rust 中简单简洁的 GPT 实现 🔥</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</link>
      <description><![CDATA[       由   提交/u/ProfessionalDrummer7   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</guid>
      <pubDate>Sat, 18 May 2024 08:25:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>