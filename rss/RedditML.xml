<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 23 Dec 2023 00:57:29 GMT</lastBuildDate>
    <item>
      <title>[R] 利用启动攻击绕过开源法学硕士的安全培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oqz32/r_bypassing_the_safety_training_of_opensource/</link>
      <description><![CDATA[我是这篇短文的作者之一；欢迎提供反馈并非常感谢！ 论文：https://arxiv.org/abs /2312.12321 项目页面：https://llmpriming.focallab.org 代码+数据：https:// github.com/uiuc-focal-lab/llm-priming-attacks 摘要  随着最近人气的飙升的法学硕士对法学硕士安全培训的需求不断增加。在本文中，我们研究了 SOTA 开源 LLM 在简单的、无优化的攻击（我们称为启动攻击）下的脆弱性，这些攻击很容易执行并有效地绕过安全训练的对齐。根据 Llama Guard 的测量，我们提出的攻击将有害行为的攻击成功率提高了 3.3 倍。     ;由   提交 /u/Dapper_Fudge6647   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oqz32/r_bypassing_the_safety_training_of_opensource/</guid>
      <pubDate>Fri, 22 Dec 2023 22:41:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么张量程序没有像神经切线核那样受到同样的关注？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</link>
      <description><![CDATA[神经正切核经常在理论论文中被引用，作为推广线性函数逼近器证明的基础。然而，它们有几个缺点，即它们不包含特征学习的概念。张量程序应该可以解决这个问题，但我认为我从未在理论论文中看到过它们被引用。人们是否怀疑结果或认为结果缺乏严谨性？结果是否被认为不太有用？或者它们只是因为数学上更复杂且更难学习而使用较少？  我问这个问题的部分原因是我想知道它是否值得花精力去阅读和理解整个论文系列，或者这项工作是否经过深思熟虑。    由   提交 /u/OptimizedGarbage   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</guid>
      <pubDate>Fri, 22 Dec 2023 22:03:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenMetricLearning 2.0 发布！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18onq8k/p_openmetriclearning_20_is_released/</link>
      <description><![CDATA[您好！  我想介绍 OpenMetricLearning 2.0 的发布！库用于训练将数据表示为向量的深度学习模型。此外，我们还有一个预训练图像模型、DDP 支持、大量示例和文档。  该版本有哪些新功能？  迁移到 PyTorch 2.0（很容易）&amp; Lightning 2.0（很痛苦） 减少了通过 pip 安装的依赖项数量 对所有当前版本的 Python 提供了稳定支持：现在 CI/CD 对所有内容都运行测试 - &lt; strong&gt;3.8、3.9、3.10、3.11 修复了一些烦人的小错误，整理了文档，并简化了公共数据集上管道的启动（例如 InShop、 斯坦福在线产品、CAR、CUB） 对于重新识别：添加了在以下情况下更正确地处理同一对象的一系列照片的功能：计算指标  我们希望所有这些改变能让OML使用起来更加方便。非常欢迎您在GitHub上的⭐！ ​    由   提交 /u/Zestyclose-Check-751   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18onq8k/p_openmetriclearning_20_is_released/</guid>
      <pubDate>Fri, 22 Dec 2023 20:09:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究论文的升级何时会成为新论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</link>
      <description><![CDATA[嘿，我是研究新手，过去 6 年来一直致力于实现一篇论文并根据我们在推荐系统领域的用例进行定制几个月。我按照 1. 在损失函数中使用不同的运算符（余弦相似度而不是点积）进行了一些更改。 2. 使用不同类型的数据。这有点难以解释，我使用元数据相似性而不是论文中一对共现的逐点互指数（PMI）。 3. 不同的数据预处理方式。 结果确实有所改善，但我不确定这是否只是一些修改，或者它本身就是一篇论文。任何人都可以阐明新想法与小修改的一些迹象吗？   由   提交/u/Abs0lut_Jeer0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</guid>
      <pubDate>Fri, 22 Dec 2023 19:49:11 GMT</pubDate>
    </item>
    <item>
      <title>[N] 使用 Ollama 在几秒钟内在本地运行 Mixtral LLM！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/</link>
      <description><![CDATA[嘿， 人工智能最近变得很疯狂，事情变化得非常快。我制作了一个涵盖新发布的 Mixtral AI 的视频，详细介绍了它的工作原理以及如何在本地运行它。我还介绍了 Microsoft 的 Phi LLM 以及未经审查的 Mixtral 版本 (Dolphin-Mixtral)，请查看！ https:/ /youtu.be/ILfmdKMa2Lo 说实话，这些事情的进展速度太疯狂了！看起来我们几个月前刚刚推出了 Mistral，而现在我们有了 mixtral，它在各个方面都彻底摧毁了它，而所有这些技术都可以在本地运行，确保我们的隐私绝对免费！ 请告诉我您的想法，或者如果您对其他视频有任何疑问/请求， 干杯  &amp;# 32；由   提交/u/dev-spot  /u/dev-spot  reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/</guid>
      <pubDate>Fri, 22 Dec 2023 19:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] Paint3D：使用无光照纹理扩散模型绘制任何 3D 内容</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18omq08/r_paint3d_paint_anything_3d_with_lightingless/</link>
      <description><![CDATA[高质量纹理贴图对于创建逼真的 3D 渲染至关重要。然而，对复杂的 3D 资源进行纹理化需要大量的手动绘画才能正确覆盖 UV 空间。这使得纹理成为 3D 内容创建中最耗费人力的部分之一。 一篇新论文提出了 Paint3D，这是一种人工智能模型，可以使用文本或图像调节自动创建 2K UV 纹理图。  p&gt; 关键在于其新颖的从粗到精的框架，它首先从模型中捕获多视图图像，然后细化 UV 空间中的纹理。这种方法避免了纹理中预烘焙光照的常见缺陷，保持了它们对图形管道中各种光照条件的适应性。 定量测量和用户研究表明 Paint3D 优于现有方法，提供更真实和完整的纹理。  从质量上来说，项目网站上包含的示例看起来也有所改进。但它并非没有局限性，处理某些材质属性（例如光泽度）仍然具有挑战性。 TLDR：Paint3D 可以自动为 3D 模型创建高分辨率纹理，这可以显着减少建模者和设计师的工作量以前受到纹理映射 AI 工具质量的限制。 完整摘要位于此处。论文位于此处。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/18omq08/r_paint3d_paint_anything_3d_with_lightingless/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18omq08/r_paint3d_paint_anything_3d_with_lightingless/</guid>
      <pubDate>Fri, 22 Dec 2023 19:21:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对齐马蹄形</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ojxf8/d_alignment_horseshoe/</link>
      <description><![CDATA[       由   提交/u/31162123  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ojxf8/d_alignment_horseshoe/</guid>
      <pubDate>Fri, 22 Dec 2023 17:17:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] 培训本地法学硕士将文本翻译成代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/</link>
      <description><![CDATA[我正在开发一个项目，该项目将获取 PDF 并将其转换为代码。我不能太具体地介绍我正在使用的 PDFS。但是有人对我应该使用/微调什么模型有任何建议吗？我可以根据现有 PDF 和现有代码创建大型数据集。但我不确定“如何” （模型/训练/微调等） 理想的解决方案是我可以在工作中使用的模型，因此任何我可以独立编程/训练的东西都会很棒。 任何建议将不胜感激，谢谢！   由   提交/u/slb1357  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/</guid>
      <pubDate>Fri, 22 Dec 2023 16:43:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] Perseus：消除大型模型训练中的能量膨胀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.06902 项目页面：https:// /ml.energy/zeus/perseus/ 积分： https://ml.energy/zeus/perseus/integrating/ 摘要：  在大量数据上训练大型人工智能模型GPU 消耗大量能源。我们观察到，并非训练期间消耗的所有能量都会直接贡献于端到端训练吞吐量，并且可以在不减慢训练速度的情况下消除很大一部分能量，我们将其称为能量膨胀。在这项工作中，我们确定了大型模型训练中两个独立的能量膨胀来源：内在和外在，并提出Perseus，一个统一的优化框架减轻两者。珀尔修斯获得“迭代时间-能量”任何大型模型训练作业的帕累托前沿都使用基于图切割的高效迭代算法，并在一段时间内安排其前向和后向计算的能耗，以消除内在和外在的能量膨胀。对 GPT-3 和 Bloom 等大型模型的评估表明，Perseus 将大型模型训练的能耗降低了高达 30%，实现了以前无法实现的节省。  &lt;!-- SC_ON - -&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</guid>
      <pubDate>Fri, 22 Dec 2023 12:59:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士如何知道每个子标记中存在的字符？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</link>
      <description><![CDATA[例如，当我向 chatGPT 询问该句子中的字母数量时： 但是，我想知道这些是如何实现的LLM 可以执行计数，因为知道分词器正在执行子标记级别而不是字符级别（这意味着每个子标记“可能”不知道它具有的字符）。）。 答案是 15这是正确的 ​ 但是，我想知道这些 LLM 如何执行计数，因为知道标记器正在执行子标记级别而不是字符级别（意味着每个子标记都是“也许”不知道它有哪些字符”）。 ​ ​ &lt;!-- SC_ON - -&gt;  由   提交/u/kekkimo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</guid>
      <pubDate>Fri, 22 Dec 2023 12:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] 苹果研究人员推出 DeepPCR：一种新颖的机器学习算法，可并行化典型的顺序操作，以加速神经网络的推理和训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</link>
      <description><![CDATA[”论文通过各种应用展示了 DeepPCR 的有效性。它在多层感知器中实现了前向传递高达 30 倍的加速和后向传递高达 200 倍的加速。此外，该算法还应用于深度 ResNet 架构的并行训练和扩散模型的生成，从而使训练速度提高 7 倍，生成速度提高 11 倍。” 论文：https://arxiv.org/pdf/2309.16318.pdf 研究页面：https://machinelearning.apple.com/research/deepcr https://twitter.com/i/status/1735876638947348656   由   提交/u/paryska99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</guid>
      <pubDate>Fri, 22 Dec 2023 12:19:33 GMT</pubDate>
    </item>
    <item>
      <title>[D]什么时候应该和不应该平衡不平衡的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</link>
      <description><![CDATA[我似乎总是在这个问题上得到相互矛盾的答案，想法？    ;由   提交/u/Throwawayforgainz99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</guid>
      <pubDate>Fri, 22 Dec 2023 11:26:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我尝试教 Mistral 7B 一门新语言（巽他语），它成功了！ （有点）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</guid>
      <pubDate>Fri, 22 Dec 2023 10:54:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有哪些便宜又好的设备可以用于 CUDA 训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</link>
      <description><![CDATA[在过去的四年里，我一直在大学服务器上训练我的所有模型，但是，因为我即将毕业 - 这根本行不通不再… 我一直在使用 Mac，但是想到毕业后无法运行 CUDA 就很难受了，哈哈 - 考虑设置尽可能便宜的计算机来训练 AI，但是，我当涉及到 Windows / Linux 计算机时，不知道从哪里开始 - 并且也不太了解所有 GPU 之间的区别 你们知道有什么便宜但性能好的 AI 计算机吗 -如果他们可以购买二手的，那就更好了！   由   提交/u/Middle_Stomach_6681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</guid>
      <pubDate>Fri, 22 Dec 2023 07:22:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>