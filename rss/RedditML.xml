<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 07 Jan 2024 18:15:49 GMT</lastBuildDate>
    <item>
      <title>[R] GANs N' Roses：稳定、可控、多样化的图像到图像转换（也适用于视频！）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190yd81/r_gans_n_roses_stable_controllable_diverse_image/</link>
      <description><![CDATA[       由   提交/u/More_Perspective_626   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190yd81/r_gans_n_roses_stable_controllable_diverse_image/</guid>
      <pubDate>Sun, 07 Jan 2024 18:13:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 更快地阅读机器学习论文的方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190vo6n/d_faster_way_to_read_ml_papers/</link>
      <description><![CDATA[似乎我在试图走捷径，但我想首先知道我发现的一篇论文是否确实提供了关于如何解决我的问题的见解手头的 ML 问题，只有在那之后我才会阅读详细信息。 任何提示将不胜感激   由   提交/u/Snoo_72181   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190vo6n/d_faster_way_to_read_ml_papers/</guid>
      <pubDate>Sun, 07 Jan 2024 16:18:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么在机器学习中几乎所有的概率推导都如此难以遵循？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190to69/d_why_are_almost_all_probabilistic_derivations_so/</link>
      <description><![CDATA[      我认为自己非常擅长数学，甚至还教过大学生，活跃于ML 等领域。 然而，我发现大多数（如果不是全部）涉及 ML 中远程概率问题的论文都得到了残酷的解释。 最近，我决定真的了解 OG [DDPM](https://arxiv.org/pdf/2006.11239.pdf) 论文。&lt; /p&gt; 这是推导的一部分，他们……以某种方式……插入了 KLD。我完全不清楚这个跳跃是如何进行的。是的，我看过 KLD 的定义，是的，我用谷歌搜索过，但每个人似乎都相信这一点。 ChatGPT 说“存在未显示的隐藏期望”。 https://preview.redd.it/glvvzcc351bc1.png?width=2014&amp;format=png&amp;auto=webp&amp;s=d4c95a5716c0b8113e9a3346b8f99e3c5 a3db919 有人知道吗？  ​   由   提交 /u/Ayakalam   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190to69/d_why_are_almost_all_probabilistic_derivations_so/</guid>
      <pubDate>Sun, 07 Jan 2024 14:46:58 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]我可以使用LORA/QLORA来微调BERT吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190rw8w/discussion_can_i_use_loraqlora_to_finetune_bert/</link>
      <description><![CDATA[BERT，从技术上讲也是一个法学硕士，传统上是通过在特定领域数据集上屏蔽单词来进行微调/领域适应的。但我也可以将 qlora 与基于 BERT 的模型结合使用，以实现更高效的微调吗？   由   提交/u/Electronic-Letter592   reddit.com/r/MachineLearning/comments/190rw8w/discussion_can_i_use_loraqlora_to_finetune_bert/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190rw8w/discussion_can_i_use_loraqlora_to_finetune_bert/</guid>
      <pubDate>Sun, 07 Jan 2024 13:14:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] 深度学习和强化学习的库。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190qggj/p_a_library_for_deep_learning_and_reinforcement/</link>
      <description><![CDATA[   大家好，我写了一个基于multiprocessing模块实现并行训练的机器学习库。我还没有做足够的测试。有人有兴趣测试它的并行训练性能吗？   由   提交 /u/NoteDance   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190qggj/p_a_library_for_deep_learning_and_reinforcement/</guid>
      <pubDate>Sun, 07 Jan 2024 11:46:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 那么，曼巴大战变形金刚……炒作是真的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/</link>
      <description><![CDATA[听到了有关序列建模模块新成员 Mamba 的所有讨论。据说它速度更快，可以更好地处理更长的序列，甚至在某些任务上优于 Transformer。但它真的是王位窃取者还是只是昙花一现？ 我的看法： 优点：Mamba 拥有高效的内存使用、随序列长度线性扩展以及令人印象深刻的性能语言和 DNA 建模。另外，它放弃了注意力机制，可能为更快的推理铺平道路。  弱点：仍处于早期阶段，因此 Mamba 在不同任务中的长期稳定性和表现仍有待观察。虽然它不需要关注，但它的状态空间方法对于某些人来说可能更难以掌握。  对于人工智能爱好者来说，曼巴只是下一个闪亮的玩具，还是序列建模中真正的范式转变？它会推翻强大的变形金刚，还是作为一种专门的工具并存？让我们听听您的想法！ https://arxiv.org/abs/2312.00752 &lt; /div&gt;  由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/</guid>
      <pubDate>Sun, 07 Jan 2024 11:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 评估 NLG LLM 的逻辑一致性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190ph03/d_assessing_logical_coherence_of_an_nlg_llm/</link>
      <description><![CDATA[标题几乎说明了一切，非常感谢任何已评估 LLM 逻辑一致性的参考资料。   由   提交/u/Plus_Tough_7497   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190ph03/d_assessing_logical_coherence_of_an_nlg_llm/</guid>
      <pubDate>Sun, 07 Jan 2024 10:39:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能与人工智能对话的悖论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190pedy/d_the_paradox_of_ai_to_ai_conversations/</link>
      <description><![CDATA[   /u/justnews_app   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190pedy/d_the_paradox_of_ai_to_ai_conversations/</guid>
      <pubDate>Sun, 07 Jan 2024 10:33:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] VCoder：用于多模态大语言模型的多功能视觉编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190mlil/r_vcoder_versatile_vision_encoders_for_multimodal/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.14233 代码：https:// /github.com/SHI-Labs/VCoder 数据集：https://huggingface.co/datasets/shi-labs/COST 项目页面：https://praeclarumjj3.github.io/vcoder/ 拥抱脸部空间：https://huggingface.co/spaces/shi-labs/VCoder 视频：https://www.youtube.com/watch?v=go493IGgVWo 摘要:  人类拥有非凡的视觉感知能力，能够看到并理解所看到的事物，帮助他们理解视觉世界，进而理解推理。多模态大语言模型（MLLM）最近在视觉语言任务上取得了令人印象深刻的性能，从视觉问答和图像字幕到视觉推理和图像生成。然而，当提示识别或计数（感知）给定图像中的实体时，现有的 MLLM 系统会失败。为了开发用于感知和推理的精确 MLLM 系统，我们建议使用多功能视觉编码器 (VCoder) 作为多模态 LLM 的感知眼睛。我们向 VCoder 提供感知模式，例如分割或深度图，从而提高 MLLM 的感知能力。其次，我们利用 COCO 的图像和现成的视觉感知模型的输出来创建 COCO 分割文本 (COST) 数据集，用于在对象感知任务上训练和评估 MLLM。第三，我们引入了评估 MLLM 在 COST 数据集上的物体感知能力的指标。最后，我们提供了大量的实验证据，证明 VCoder 相对于现有的多模态 LLM（包括 GPT-4V）改进了对象级感知技能。我们开源数据集、代码和模型以促进研究。我们在 此 https URL    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190mlil/r_vcoder_versatile_vision_encoders_for_multimodal/</guid>
      <pubDate>Sun, 07 Jan 2024 07:18:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无监督通用图像分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190md55/r_unsupervised_universal_image_segmentation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.17243 代码：https://github .com/u2seg/U2Seg 项目页面：https://u2seg。 github.io/ 摘要：  已经提出了几种无监督图像分割方法，消除了密集手动的需要 -带注释的分割掩码；当前模型分别处理语义分割（例如，STEGO）或与类无关的实例分割（例如，CutLER），但不能同时处理两者（即全景分割）。我们提出了一种无监督通用分割模型（U2Seg），擅长使用新颖的统一框架执行各种图像分割任务（实例、语义和全景）。 U2Seg 通过利用自监督模型和聚类来为这些分割任务生成伪语义标签；每个簇代表像素的不同语义和/或实例成员资格。然后，我们在这些伪语义标签上对模型进行自我训练，与针对每个任务量身定制的专用方法相比，获得了显着的性能提升：与无监督的 CutLER 相比，APbox 提升了 +2.6 COCO 上的实例分割和 COCOStuff 上无监督语义分割的 +7.0 PixelAcc 增加（与 STEGO 相比）。此外，我们的方法为无监督全景分割建立了一个新的基线，这是以前从未探索过的。 U2Seg 也是用于少镜头分割的强大预训练模型，在低数据状态（例如只有 1% COCO 标签）上训练时，比 CutLER 多出 +5.0 APmask 。我们希望我们简单而有效的方法能够激发更多关于无监督通用图像分割的研究。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190md55/r_unsupervised_universal_image_segmentation/</guid>
      <pubDate>Sun, 07 Jan 2024 07:03:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我如何获得 AI/ML 领域的经验来担任研究职位</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190l41n/d_how_can_i_gain_experience_in_aiml_for_research/</link>
      <description><![CDATA[您好，我是一名计算机科学专业的新生，对 AI/ML 研究非常感兴趣，尤其是强化学习。我想向教授寻求研究机会，但我没有太多经验可以展示。我已经完成了一些在线课程，阅读了教科书等，但除了我完成了一些编码作业作为其中的一部分之外，我没有什么可以展示的。您对我可以做些什么来获得强化学习经验有什么建议吗？我可以向教授展示这些经验，以证明我已经准备好在他们的实验室进行研究？我一直在考虑从头开始实现一些论文和/或做一些涉及机器学习的副项目。这是一个好的起点吗？   由   提交/u/meemaowie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190l41n/d_how_can_i_gain_experience_in_aiml_for_research/</guid>
      <pubDate>Sun, 07 Jan 2024 05:48:36 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] 去噪自动编码器过时了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190ggrg/rp_are_denoising_autoencoders_out_of_style/</link>
      <description><![CDATA[分数匹配模型，特别是其去噪分数匹配实现现在非常热门。然而，几乎所有这些都是某种形式的大型随机降噪器。我想知道为什么去噪自动编码器没有对它们进行太多的研究，考虑到两者在理论上和功能上都很相似（[1]中导出的去噪分数匹配论文明确地建立了两者之间的联系）。 &lt;此外，自动编码器比 U-Net 对应物灵活得多，因为它们可用于低维潜变量建模（例如 VAE）。我知道有几篇论文将去噪自动编码器与变分自动编码器 [2] 和对抗性自动编码器 [3] 相结合，在我看来，这是一个不错的开始。  在我自己的研究中，我发现它们本身在概率建模方面具有巨大的潜力。 ​ 参考文献 [1] 帕斯卡·文森特。分数匹配和去噪自动编码器之间的联系。神经计算，2011。 [2] Antonia Creswell、Kai Arulkumaran、Anil Anthony Bharath。使用马尔可夫链改进生成自动编码器的采样。 arXiv，2016。 [3] Antonia Creswell，Anil Anthony Bharath。去噪对抗性自动编码器。 arXiv，2017。   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190ggrg/rp_are_denoising_autoencoders_out_of_style/</guid>
      <pubDate>Sun, 07 Jan 2024 01:47:29 GMT</pubDate>
    </item>
    <item>
      <title>[D]我们的大脑如何防止过度拟合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</link>
      <description><![CDATA[老实说，这个问题引发了一系列其他问题，老实说，这很有趣，我们阻止这种情况发生的机制是什么？ 梦想只是生成数据增强，以便我们防止过度拟合吗？ 如果我们进一步将过度拟合拟人化，患有学者综合症的人会过度拟合吗？ （因为他们在狭窄的任务上表现出色，但在泛化方面有其他障碍。尽管他们仍然有梦想） 为什么我们不记忆，而是学习？    由   提交 /u/BlupHox   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</guid>
      <pubDate>Sat, 06 Jan 2024 22:33:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用开源模型进行长代理树搜索取得令人难以置信的结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</link>
      <description><![CDATA[你好， 我看到 GPT-4 的长代理树搜索以 94.4% 的 pass@1 领先于 HumanEval现在几周了。 https://paperswithcode.com/sota/code- Generation-on- humaneval &lt; p&gt;​ 原始论文的作者在他们的官方 github 存储库。我必须更改一些代码才能使用 CodeLlama-7b 进行尝试，并使用 pass@1 进行人类评估，仅 2 次最大迭代即可将 HumanEval 得分从 37% 提高到大约 70%。 这是一些令人难以置信的结果在我看来，因为这个分数比只有 7b 模型的 GPT-3.5 更高。我认为必须进行更多测试，但令我惊讶的是人们没有更多地谈论这一点。   由   提交/u/ArtZab  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</guid>
      <pubDate>Sat, 06 Jan 2024 16:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>