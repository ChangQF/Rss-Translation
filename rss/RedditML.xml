<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Wed, 21 Aug 2024 15:15:16 GMT</lastBuildDate>
    <item>
      <title>[D] Beta9：开放无服务器 GPU 云</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1exq4k9/d_beta9_open_serverless_gpu_cloud/</link>
      <description><![CDATA[嗨 r/MachineLearning， TL;DR：Beta9 是一个开源框架，可让开发人员轻松地在云 GPU 上运行无服务器功能。 Git Repo： https://github.com/beam-cloud/beta9 我是 Eli，一年前，我们在这个 subreddit 上推出了 Beam。我们决定将其开源。  如果您还没有听说过 Beam，它是一个用于 GPU 工作负载的无服务器平台。它为将代码部署到云上提供了 Python 优先的开发人员体验，并且它可能是将 ML 模型部署到 GPU 上的最快方法。  您可以使用它做以下几件事：  微调 LLM 部署无服务器推理 API 将工作负载自动扩展到数百个 GPU  有几个技术组件使这种体验成为可能：自定义 Runc 容器运行时、图像缓存、全局分布式存储和工作负载调度程序。这些都包含在开源项目中。 为什么要开源？  Beam 是一种在云上运行代码的新方法，我们构建了不同的抽象来实现它（Web 端点、任务队列、存储卷等等）。通过开源，我们邀请您为框架贡献自己的云抽象。想要将 S3 存储桶安装到您的无服务器容器中吗？您可以为此构建一个抽象！如何在云 GPU 容器上公开 TCP 端口？向 repo 发送 PR 并疯狂地进行操作。 您可以插入自己的硬件提供商。想要在 Lambda Labs 上使用 A6000？在您的家庭实验室中使用 3090 集群怎么样？是的，您可以连接它们。如果您有 AWS 或 GCP 积分，您也可以使用它们。   我们认为，让云变得更简单是一个很大的机会，但这是我们无法独自完成的努力。如果您对这个愿景感到兴奋，我们很高兴您尝试一下这个项目。 以下是我们的快速链接：  Repo： https://github.com/beam-cloud/beta9 带有示例应用程序和教程的 Github： https://github.com/beam-cloud/examples 文档： https://docs.beam.cloud/  让我知道您的想法，以及您希望我们将来构建什么！   由    /u/velobro  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1exq4k9/d_beta9_open_serverless_gpu_cloud/</guid>
      <pubDate>Wed, 21 Aug 2024 13:49:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用来自 LLM 的合成数据来训练/微调其他模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1expxo4/d_using_synthetic_data_from_llms_to_trainfinetune/</link>
      <description><![CDATA[嗨，这似乎是一个简单的问题，但我找不到好的材料和建议。 我想用 LLM 生成合成数据来试验句子转换器的微调。  你知道有什么好的做法、教程或框架可以使合成数据尽可能有效吗？ 除了 Llama-3 系列之外，你是否知道有哪些好的 LLM 允许（或至少不禁止）这样使用它们的输出？     提交人    /u/BenXavier   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1expxo4/d_using_synthetic_data_from_llms_to_trainfinetune/</guid>
      <pubDate>Wed, 21 Aug 2024 13:40:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 针对文本到 SQL 的微调 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1exnpo0/d_finetuning_llm_for_texttosql/</link>
      <description><![CDATA[大家好， 我目前正在使用 Mistral v2 Instruct 模型开展一个文本到 SQL 项目。但是，我遇到了一个问题，即该模型比以前更容易产生幻觉。下面是我当前的配置 训练参数 training_args = TrainingArguments( output_dir=&quot;models-sql&quot;, per_device_train_batch_size=4, gradient_accumulation_steps=4, learning_rate=2e-5, logs_steps=100, num_train_epochs=1, report_to=None, save_steps=200, #save_total_limit=2, evaluation_strategy=&quot;steps&quot;, eval_steps=50, do_eval=True, greater_is_better=False, load_best_model_at_end=True, auto_find_batch_size=True, optim=&quot;paged_adamw_8bit&quot;, warmup_ratio=0.03, lr_scheduler_type=&quot;linear&quot;, #余弦，待重新启动gradient_checkpointing=True，gradient_checkpointing_kwargs={&quot;use_reentrant&quot;: True},) LoRA 配置 peft_config = LoraConfig( r=64, lora_alpha=32, lora_dropout=0.1, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot;, target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;, &quot;ffn&quot;, &quot;lm_head&quot;],) 训练器设置 trainer = SFTTrainer( model=model, tokenizer=tokenizer, args=training_args, peft_config=peft_config, max_seq_length=5250, train_dataset=train_dataset, eval_dataset=eval_dataset, dataset_text_field=&quot;text&quot;,packing=True, neftune_noise_alpha=5, #callbacks=[early_stop], ) 其他设置 Epochs = 10 保存策略 = checkpoint 随机状态 = 1807 预热步骤 = 500    submitted by    /u/Accomplished-Clock56   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1exnpo0/d_finetuning_llm_for_texttosql/</guid>
      <pubDate>Wed, 21 Aug 2024 11:58:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 解决 ctf 的 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1exmvqp/d_llm_for_solving_ctf/</link>
      <description><![CDATA[我们能否训练或微调 LLM 来解决 ctf（网络安全挑战）？通过使用所有 hack the box 模块机器的数据集对其进行训练，尝试 hack me、pico ctf 和所有其他可能的平台。然后它会解决 ctf 吗？这可能吗？    提交人    /u/EstablishmentDry6444   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1exmvqp/d_llm_for_solving_ctf/</guid>
      <pubDate>Wed, 21 Aug 2024 11:12:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] HiRED：在资源受限的环境中，通过注意力引导标记删除实现高分辨率视觉语言模型的有效推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1exhg0f/r_hired_attentionguided_token_dropping_for/</link>
      <description><![CDATA[带有 LLaVA-Next 的 HiRED 使用固定且较少的标记数量（即 20%），但仍保留了大部分准确性。它根据图像分区的视觉内容在图像分区之间策略性地分配固定标记预算，并从每个分区中选择最重要的标记子集。有什么想法吗？    提交人    /u/E-fazz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1exhg0f/r_hired_attentionguided_token_dropping_for/</guid>
      <pubDate>Wed, 21 Aug 2024 05:17:29 GMT</pubDate>
    </item>
    <item>
      <title>根据组学数据预测表型 [R] [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1exczzr/predicting_phenotypes_from_omics_data_r_p/</link>
      <description><![CDATA[大家好， 我正在一个新领域开始新的研究工作，PI 对使用深度学习从各种组学数据中预测表型变化很感兴趣。我想知道您是否推荐我查看该领域中一些有趣的应用论文？显然我可以自己谷歌一下并找到一些东西，但不确定是否有任何开创性的、可以说是“必读”的内容。非常感谢！    提交人    /u/Ok-Administration894   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1exczzr/predicting_phenotypes_from_omics_data_r_p/</guid>
      <pubDate>Wed, 21 Aug 2024 01:28:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 加入我们的全球论文阅读小组，深入探讨“像图表一样规划 (PLaG)” - 与作者 Fangru Lin 一起增强异步计划推理中的 LLM | ICML 2024！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ex6dl6/r_join_our_global_paper_reading_group_for_a_deep/</link>
      <description><![CDATA[大家好， 我们很高兴邀请您参加即将举行的会议，该会议由我们的全球论文阅读小组Computer Vision Talks主办，我们邀请作者讨论他们的研究！这次，我们将探讨一个在 AI 社区中越来越受关注的有趣话题：“像图表一样规划 (PLaG)：增强 LLM 在异步规划推理中的能力”。 🚀🧠 论文链接：https://arxiv.org/pdf/2402.02805 活动详情：  日期：2024 年 8 月 24 日星期六 时间：美国东部时间上午 10:00 加入： Zoom 注册链接  关于什么？  “像图一样规划 (PLaG)” 方法引入了一种创新方法，通过将任务分解为形成执行图的子任务来改进大型语言模型 (LLM)。此方法允许并行和顺序执行任务，从而显着提高 LLM 性能而无需进行微调。我们的演讲者是来自牛津大学的 DPhil NLP 学生 Fangru Lin，他将指导我们如何将 PLaG 应用于 GPT-4 和 LLaMA-2 等模型，即使任务复杂性增加也能获得最先进的结果。 为什么要参加？ 本次会议非常适合对高级提示工程、LLM 或解决复杂推理任务感兴趣的任何人。无论您是刚接触 PLaG 方法还是一直在关注其发展，本次演讲都将为您提供宝贵的见解和实际应用。 关于我们的演讲者： Fangru Lin 是牛津大学的 DPhil NLP 学生，也是克拉伦登学者。她的研究已经在 AI 领域引发了热烈的讨论，我们很高兴她能与我们分享她的见解。 加入我们并提供支持： 作为一个全球论文阅读小组，我们的目标是将来自世界各地的 AI 爱好者和专业人士聚集在一起。如果您对此主题感兴趣，请考虑加入我们的谈话，并在 LinkedIn 和 YouTube 上与我们的帖子互动，以帮助传播信息：  LinkedIn： 计算机视觉谈话帖子 YouTube： 计算机视觉谈话频道  我们很高兴您能参与我们的讨论并听取您的想法！    提交人    /u/ShambhaviCodes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ex6dl6/r_join_our_global_paper_reading_group_for_a_deep/</guid>
      <pubDate>Tue, 20 Aug 2024 20:39:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找使用 Mamba 的句子嵌入器（句子转换器的替代品）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ex45q7/d_looking_for_sentence_embeder_that_uses_mamba/</link>
      <description><![CDATA[大家好， 我正在开展一个涉及生成句子嵌入的项目，我一直在使用 sentence-transformers all-MiniLM-L12-v2 。这很棒，但我很好奇是否还有其他使用 mamba ssm 而不是 transformer 的好选择。 理想情况下，我正在寻找这样的东西：  为不同的任务提供各种预训练模型（例如，语义相似性、聚类等） 相对易于使用且具有良好的文档 提供不错的性能  任何建议都将不胜感激！ 提前致谢！    提交人    /u/Remote-Beautiful4399   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ex45q7/d_looking_for_sentence_embeder_that_uses_mamba/</guid>
      <pubDate>Tue, 20 Aug 2024 19:10:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 持续模型训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ex3y5h/d_continuous_model_training/</link>
      <description><![CDATA[在哪些用例中，您必须在出现任何小的漂移时不断从数据中学习？或者，无需等待漂移，当新一批数据到达时，您会不断训练模型，以便始终从模型中获得最佳准确度。换句话说，您必须不断训练模型以从数据中提取最佳预测能力？    提交人    /u/mutlu_simsek   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ex3y5h/d_continuous_model_training/</guid>
      <pubDate>Tue, 20 Aug 2024 19:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在哪里可以找到数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ex0cvo/p_where_to_find_the_dataset/</link>
      <description><![CDATA[大家好， 我正在为即将举行的黑客马拉松制定问题陈述，该问题陈述涉及使用卷积神经网络 (CNN) 根据雷达微多普勒频谱图像对无人机和鸟类进行分类。 目标是开发一种可以使用这些雷达特征准确区分无人机和鸟类的模型。这在空域监控和安全方面具有重要应用。 我找到了一篇关于它的研究文章。但我找不到与其相关的数据集。 任何有助于找到合适数据集的帮助都将不胜感激！    提交人    /u/No_Technology615   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ex0cvo/p_where_to_find_the_dataset/</guid>
      <pubDate>Tue, 20 Aug 2024 16:40:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 代码提交 – 审核奖励</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewqh0a/d_code_submission_bonus_for_review/</link>
      <description><![CDATA[对于 AAAI-25，在提交论文时上传代码是否会提供任何审查奖励？还是在接受后上传代码就足够了？只是想判断在审查之前是否值得付出额外的努力。    提交人    /u/Ok_Butterfly7408   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewqh0a/d_code_submission_bonus_for_review/</guid>
      <pubDate>Tue, 20 Aug 2024 08:39:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们是否已经不再使用“微调”而改用“监督微调”了？或者是否存在其他微调范式方法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewezs4/d_have_people_stopped_saying_fine_tuning_in_place/</link>
      <description><![CDATA[我一直认为微调意味着我们可以进行监督。但如今，人们似乎都说“SFT”。很好奇历史是怎样的。    由   提交  /u/Seankala   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewezs4/d_have_people_stopped_saying_fine_tuning_in_place/</guid>
      <pubDate>Mon, 19 Aug 2024 22:20:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] 了解 Transformers 和 LLM 的图解书</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/</link>
      <description><![CDATA[      我在这个 subreddit 上看到过好几个人对 Transformers 和 LLM 内部工作原理的长篇解释感兴趣的例子。 这是我和我双胞胎兄弟在过去三年半里一直试图填补的空白。上周，我们发表了“超级学习指南：Transformers &amp;大型语言模型”，这是一本 250 页的书，包含 600 多幅插图，面向对该领域有浓厚兴趣的视觉学习者。 本书深入介绍了以下主题：  基础：神经网络入门以及用于训练和评估的重要深度学习概念。 嵌入：标记化算法、词嵌入（word2vec）和句子嵌入（RNN、LSTM、GRU）。 Transformers：其自注意力机制背后的动机、编码器-解码器架构的详细概述以及相关变体（如 BERT、GPT 和 T5），以及如何加速计算的技巧和窍门。 大型语言模型：调整基于 Transformer 的模型的主要技术，例如即时工程、（参数高效的）微调和偏好调整。 应用：最常见的问题，包括情感提取、机器翻译、检索增强生成等等。  （如果您想知道：此内容与我们 5-6 年前在此 subreddit 上分享的斯坦福插图学习指南的氛围相同，关于 CS 229：机器学习、CS 230：深度学习 和 CS 221：人工智能) 学习愉快！ https://preview.redd.it/n6zraaltemjd1.jpg?width=1905&amp;format=pjpg&amp;auto=webp&amp;s=1110f750df0d8a60d5fdf1d4967b41e1b5617efe    提交人    /u/shervinea   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/</guid>
      <pubDate>Mon, 19 Aug 2024 13:14:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 18 Aug 2024 02:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>