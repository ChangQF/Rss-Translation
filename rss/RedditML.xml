<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 29 Nov 2023 12:25:49 GMT</lastBuildDate>
    <item>
      <title>[R] 生成带有支撑线的多维簇</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186nkvk/r_generating_multidimensional_clusters_with/</link>
      <description><![CDATA[      https ://arxiv.org/abs/2301.10327  Clugen 是一个用于合成数据生成的模块化程序，能够使用任意分布创建由线段支持的多维集群。 Clugen 是开源的，经过全面的单元测试和记录，可用于 Python、R、Julia 和 MATLAB/Octave 生态系统。 ​ 各种 3D 示例。 &lt; !-- SC_ON --&gt;  由   提交 /u/FakenMC   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186nkvk/r_generating_multidimensional_clusters_with/</guid>
      <pubDate>Wed, 29 Nov 2023 11:47:48 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 端到端导航</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ncoy/discussion_endtoend_navigation/</link>
      <description><![CDATA[我对进行端到端导航感兴趣。谁能推荐任何概述视觉或基于激光雷达的端到端导航的最佳实践和潜在陷阱的调查或评论论文？   由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ncoy/discussion_endtoend_navigation/</guid>
      <pubDate>Wed, 29 Nov 2023 11:33:18 GMT</pubDate>
    </item>
    <item>
      <title>GPT API 因无效日期而崩溃 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186mqrs/gpt_api_meltdown_from_invalid_dates_p/</link>
      <description><![CDATA[      &amp;# 32；由   提交 /u/robleregal   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186mqrs/gpt_api_meltdown_from_invalid_dates_p/</guid>
      <pubDate>Wed, 29 Nov 2023 10:54:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 他们说：“这不仅仅是记住训练数据”：从（生产）语言模型中可扩展地提取训练数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</link>
      <description><![CDATA[      &amp;# 32；由   提交/u/wojcech  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</guid>
      <pubDate>Wed, 29 Nov 2023 10:45:56 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 后续问题检测和改写。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186mflk/discussion_followup_question_detection_and/</link>
      <description><![CDATA[我一直在考虑部署更小的 ML 模型来处理后续检测和改写，因为这将使我的对话管道更快，并希望能够消除了我在使用开源法学硕士时遇到的一些不一致问题。  我把这项任务搁置了一段时间，因为我专注于训练更大的模型，但意识到我已经到了重新措辞问题甚至后续问​​题检测都出现问题的地步，并且开始成为“瓶颈”在我的管道中。  我在互联网上搜索了一下这两个内容，发现了一些讨论一些数据集和方法的论文，但没有一篇论文验证了我想要的上下文。即使使用他们讨论的模型或数据集，我也无法达到预期的结果。 （请注意，大多数论文和数据集都是为了改写而不是检测） 我正在考虑使用某种分类模型来检测后续问题（主要是 BERT）并使用 BART、GPT2或 T5 进行改写。  我想要解决的一个小（且简单）示例如下： - 我们在德国有多少天休假？ - 法国呢？ ==&gt;法国有多少天假期？  您以前有处理此类任务的经验吗？我检查了 CANARD 数据集并测试了在该数据集上训练的 T5 模型，但它的性能很差。任何用于处理此问题的数据集、模型或解决方案都会很棒，因为它一直是对话管道中的一个难题。   由   提交/u/cedar_mountain_sea28   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186mflk/discussion_followup_question_detection_and/</guid>
      <pubDate>Wed, 29 Nov 2023 10:33:24 GMT</pubDate>
    </item>
    <item>
      <title>[D]“知识蒸馏”或“师生法”的学生培养目标</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186lrc7/d_student_training_objective_in_knowledge/</link>
      <description><![CDATA[我见过 3 种目标。哪个最受欢迎？哪个最有用？  匹配老师的一些隐藏状态 似乎来自原始知识蒸馏论文，但由于其复杂性和模型结构，我从未在实践中使用过它限制  匹配老师的输出logits（可能有温度） 匹配老师的预测标签 我喜欢这个因为： 1. 完全不受模型结构的限制； 2.学生培养中无需额外的目标代码； 3. 我可以将教师输出保存为普通数据集，以便进一步探索     由   提交 /u/txhwind   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186lrc7/d_student_training_objective_in_knowledge/</guid>
      <pubDate>Wed, 29 Nov 2023 09:46:08 GMT</pubDate>
    </item>
    <item>
      <title>MeshGPT：使用仅解码器变压器生成三角形网格 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ktla/meshgpt_generating_triangle_meshes_with/</link>
      <description><![CDATA[       由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ktla/meshgpt_generating_triangle_meshes_with/</guid>
      <pubDate>Wed, 29 Nov 2023 08:36:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用侵犯版权但不受传统版权保护的音频进行培训是否合法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186kq06/d_is_it_legal_to_train_on_audio_that_is_copyright/</link>
      <description><![CDATA[我想根据音乐家录制的流行歌曲并因此拥有的钢琴封面来训练一个模型，但由于侵犯版权，他们仍然付费这些歌曲的作者的版税。很好奇这是否合法——我认为不合法，因为在这种情况下，这些录音并没有被货币化，而是被输入到一个模型中，并且这些录音仍然由授权给我的音乐家 100% 拥有。   由   提交 /u/SuperwhizAJ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186kq06/d_is_it_legal_to_train_on_audio_that_is_copyright/</guid>
      <pubDate>Wed, 29 Nov 2023 08:29:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] 一段从头开始讲述人工智能历史的视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186hq4s/p_a_video_covering_the_history_of_artificial/</link>
      <description><![CDATA[我在这 2.5 小时的“完整”活动中度过了一段愉快的时光。人工智能的历史。 https://www.youtube.com/watch?v=k72eqhhlfbU&lt; /a&gt; 由于作者显然不是机器学习研究员，而是天体物理学家，我认为这是对这个主题的相当彻底的处理（如果有偏见和不敬的话）。非常适合这么小的 YouTube 频道。   由   提交/u/larenspear  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186hq4s/p_a_video_covering_the_history_of_artificial/</guid>
      <pubDate>Wed, 29 Nov 2023 05:17:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果运行时间或 GPU 内存使用没有显着减少，那么参数高效微调的动机是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</link>
      <description><![CDATA[我一直在尝试诸如提示调整和 LoRA 之类的方法，这些方法的参数效率很高，因为它们只微调很小的一部分（即是所有参数的&lt;1%）。 但是对于这两种方法，您必须在反向传播期间缓存中间梯度，这意味着您在微调期间（或在由于无需存储冻结层的优化器状态，​​因此节省了大部分 GPU 内存）。例如，我已经让 LoRA 将我的自定义模型的 GPU 内存占用量从 8.5GB 减少到了 8.5GB。 8.1GB，非常小。微调时间减少也并不是真正的主要优势，每批微调同一模型减少了 20 毫秒，从 210 毫秒减少到 190 毫秒。 这引出了一个问题 - 实际原因是什么？参数高效微调（例如，带有 1.6k+ 引用的提示调整）的流行，如果它不能真正节省 GPU 内存和训练时间？ 我可以看到两个可能的原因（但我不太相信他们真的解释了围绕参数高效微调的“炒作”）：  下游任务的微调模型检查点显着减少。例如，在提示调整中，我们只需要在硬盘/SSD 上保存经过训练的微小软提示（〜很少兆字节），而不是整个更改后的模型权重（〜很多很多 GB）。  但从实际角度来看，我觉得大多数人都缺乏计算能力（例如 GPU 内存），而不是硬盘空间。换句话说，训练时间和 GPU 内存消耗似乎比节省检查点存储空间更相关。  第二个是域转移的鲁棒性（因为我们是保留大部分原始模型的权重，而不是破坏性地重新学习它们），这一点在提示调整论文中提到过，但在 LoRA 论文中却没有提及太多。  我可以认为这是一个可能的原因，但是在分布外设置中的提示调优论文中的性能增益充其量是微乎其微的，并且 LoRA 没有提到域转换。   （编辑 - 我还想知道是否还缺少其他东西来减少 GPU 内存和运行时间？我听说 QLoRA 增加了 4- LoRA 之上模型的位量化，所以也许这是解决 LoRA 内存效率问题的一种方法。但我不知道是否有什么可以减少内存占用以进行快速调整？）   由   提交/u/patricky168  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</guid>
      <pubDate>Wed, 29 Nov 2023 01:06:13 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]知识蒸馏定义不好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18681i4/discussion_knowledge_distillation_is_badly_defined/</link>
      <description><![CDATA[或者不那么挑衅地说，知识蒸馏是一系列没有明确目标的技术。 在我看来，在文献中通常考虑两种类型的知识蒸馏目标：  让学生模型向教师模型学习，以在教师训练数据集上重现教师的答案（非常重要） 让学生模型向教师模型学习，以重现教师在另一个下游数据集上预测的答案，因此教师和学生都不会“看到” ;之前的那些数据点  但是，我可以想象另一个我认为合理但似乎没有进行太多探索的目标：  拥有学生模型复制教师的输出到处，也就是说，训练学生对任何数据点给出与教师相同的答案。这可以通过将损失函数设置为教师和学生预测之间均方差的 x 积分来实现。我真的不知道如何解决这个特定问题，也许使用重要性采样？  如果有人知道尝试（3）的论文，请发布链接。  如果有人知道尝试（3）的论文，请发布链接。 p&gt; 讨论 知识蒸馏可能具有这三个目标之一（甚至可能是另一个目标）。根据实际用于训练学生的目标，结果应该会有很大差异。 教师在 A 组上进行训练，学生在 A 组上进行训练 在 (1) 的情况下，考虑到学生通常较小或具有更简单的架构，学生不会超越教师也就不足为奇了。如果学生在与老师相同的测试集上进行评估，那么学生就无法在他的领域击败老师。然而，学生可能完全无法匹配教师对分布外 (OOD) 数据的预测，因为它从未接受过 OOD 样本的训练。在这些情况下，我不确定学生是否会超越老师。 老师在 A 组上训练，学生在 B 组上训练 在（2）的情况下，考虑到新的下游分布通常与训练分布有很大不同，学生表现优于老师我不会感到惊讶，因此老师永远没有机会从这个新数据集中学习，但是学生会同时获得先前的信息（通过老师）和新的信息（通过输入，成为梯度，成为参数更新）。我什至想说，在这种情况下，一个体型相似的学生会被老师殴打，这将是令人惊讶的。 这就是为什么我认为如果我们在被打之前承认这种潜在影响会更好。令人惊讶的是，学生模型的表现优于教师。这在法学硕士接受其他法学硕士生成的数据训练的时代似乎尤为重要。在我看来，人们高估了蒸馏的效果，而实际上它可能只是迁移学习。性能的提高将归因于额外数据的使用，或者生成数据的选择机制，而不是蒸馏行为。 教师在 A 组上接受培训，学生在 A 组上接受培训整个输入域 对于（3），我想这实际上是知识蒸馏的最真实形式，因为目标实际上是在整个域上复制（非常复杂的）函数而不是只是一小部分。我想这就是所谓的“模型压缩”。在文献中，但我从未见过这样的问题。这种方法对于分布外的样本应该是稳健的，因为学生应该做出与老师相同的预测。换句话说，如果老师对 OOD 数据稳健，那么学生也应该如此。 我希望学生在任何给定的测试集上都比老师表现得更好一些，因为学生应该“顺利”出”教师做出的任何异常（伪影）预测并创建一种正则化形式。不过我可能是错的。 请随意发表您对此的想法。   由   提交 /u/Cosmolithe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18681i4/discussion_knowledge_distillation_is_badly_defined/</guid>
      <pubDate>Tue, 28 Nov 2023 21:58:04 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]您发现自己每周在工作的哪一部分上浪费时间最多？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/</link>
      <description><![CDATA[不询问拖延症、实际工作职责。对我来说，它必须处理电子表格和演示文稿。您的工作流程中存在哪些瓶颈？   由   提交/u/zero-true  /u/zero-true reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/</guid>
      <pubDate>Tue, 28 Nov 2023 18:52:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 具有 2d 旋转嵌入的交叉轴变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18607oa/r_crossaxis_transformer_with_2d_rotary_embeddings/</link>
      <description><![CDATA[ 由   提交/u/lilyerickson  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18607oa/r_crossaxis_transformer_with_2d_rotary_embeddings/</guid>
      <pubDate>Tue, 28 Nov 2023 16:34:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2023机构排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/185pdax/d_neurips_2023_institutions_ranking/</link>
      <description><![CDATA[       由   提交/u/Roland31415   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/185pdax/d_neurips_2023_institutions_ranking/</guid>
      <pubDate>Tue, 28 Nov 2023 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>