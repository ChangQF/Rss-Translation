<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 29 Nov 2023 21:12:12 GMT</lastBuildDate>
    <item>
      <title>[D] 推荐给传统统计学家的认证/课程作业</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ynti/d_certificationcoursework_recommended_for/</link>
      <description><![CDATA[我在一家大型制药公司担任统计学家。我们部门充满了中高级统计人员，他们的大部分职业生涯都从事更传统的统计工作。每个人都至少拥有统计学硕士学位和 5 年经验。人们几乎完全使用 SAS 进行编程，但少数人使用 R。我们通常使用 GLM，制定抽样计划并负责规划和分析 DoE。  我们部门找到了额外的培训资金，如果我们在年底之前使用的话，我们就可以获得这些资金。我正在寻找推荐的培训计划/认证/课程，这对业内认可的这群人来说是有好处的。我正在寻找能够为现代机器学习技术提供适当细节水平的东西，因为世界似乎正在​​朝这个方向发展。显然，我们在软件方面可能有点落后，但可以处理数学部分。我希望在新技术的对话和实践之间取得成果，具体取决于统计学家愿意投入课程的工作水平。   由   提交/u/flapjaxrfun   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ynti/d_certificationcoursework_recommended_for/</guid>
      <pubDate>Wed, 29 Nov 2023 20:04:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 位置描述对于变压器算法很重要</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186y5ve/r_positional_description_matters_for_transformer/</link>
      <description><![CDATA[一篇新论文专注于提高 LLM 的算术技能，主要是类似 GPT 的模型。法学硕士在算术方面面临三个主要挑战：  复杂计算：法学硕士很难处理复杂的算术，特别是涉及大量数字，导致内部中间步骤遇到困难。  长度限制：法学硕士仅限于处理其训练数据范围内的数字，从而限制了实用性。 与语言的集成：  strong&gt; 由于表面格式的差异，合并算术和自然语言数据会遇到障碍，导致位置相关的表示发生冲突。  为了解决这些挑战，文章介绍了增强乘法的技术：&lt; /p&gt;  填充：数字因子被填充到固定的 15 位长度，确保一致性和位置不变性。 重新排序： 乘积中的数字顺序被颠倒，以与乘法的自然级数保持一致。  结果令人印象深刻。在测试中，他们的方法在计算最多 12 位数字的乘积时达到了 99% 的准确率。相比之下，简单地要求 GPT-4 将两个 4 位数字相乘的准确度不到 1%。 为了克服长度限制，本文探讨了数据格式和位置编码，包括随机间距和替代编码编码。这些创新使法学硕士能够推广加法来处理额外的数字。 本文还通过随机化格式和使用替代位置编码来解决算术和语言数据的集成，从而实现有效的数据集成。 TLDR：正如论文标题所述，位置描述对于变换器算术很重要。 完整摘要位于此处。论文这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/186y5ve/r_positional_description_matters_for_transformer/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186y5ve/r_positional_description_matters_for_transformer/</guid>
      <pubDate>Wed, 29 Nov 2023 19:43:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] MMMU：专家 AGI 的大规模多学科多模态理解和推理基准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186y58x/r_mmmu_a_massive_multidiscipline_multimodal/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2311.16502  博客：https://mmmu-benchmark.github.io/  摘要：  我们介绍 MMMU ：一个新的基准，旨在评估需要大学水平学科知识和深思熟虑推理的大规模多学科任务的多模态模型。 MMMU 包含 11,500 个从大学考试、测验和教科书中精心收集的多模态问题，涵盖六个核心学科：设计、商业、科学、健康与医学、人文与科学社会科学、技术与科学工程。这些问题涵盖 30 个主题和 183 个子领域，包括 30 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。与现有基准不同，MMMU 专注于利用特定领域知识进行高级感知和推理，挑战模型来执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有的 GPT-4V(ision) 的评估凸显了 MMMU 带来的巨大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确率，这表明还有很大的改进空间。我们相信 MMMU 将刺激社区构建下一代多模式基础模型，以实现专家通用人工智能。  https://preview.redd.it/0k2e074​​fbc3c1.jpg?width=1663&amp;format=pjpg&amp;auto=webp&amp;s=03c5b80bbab 288919bff3c7838f65ecb79cf8174 https://preview。 redd.it/g646d94fbc3c1.jpg?width=1475&amp;format=pjpg&amp;auto=webp&amp;s=65f5fa706e8295d4f296186ab7f082deb5968a6d https://preview.redd.it/ueftb64fbc3c1.jpg?width=1665&amp;format=pjpg&amp;auto=webp&amp;s=e3a945 765372dc3fc8ae2cb387a5c48b7c5d215b&lt; /a&gt; https:// Preview.redd.it/e10k3a4fbc3c1.jpg?width=1667&amp;format=pjpg&amp;auto=webp&amp;s=cbb71e4030cdb6067dae4fe86baaf8403ff99ec8   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186y58x/r_mmmu_a_massive_multidiscipline_multimodal/</guid>
      <pubDate>Wed, 29 Nov 2023 19:42:21 GMT</pubDate>
    </item>
    <item>
      <title>对抗性扩散蒸馏</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186xudj/adversarial_diffusion_distillation/</link>
      <description><![CDATA[   /u/KarlKani44  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186xudj/adversarial_diffusion_distillation/</guid>
      <pubDate>Wed, 29 Nov 2023 19:29:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过深度学习发现了数百万种新材料</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</link>
      <description><![CDATA[帖子：https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/ 论文：https://www.nature.com/articles/s41586-023-06735-9 摘要： 新型功能材料实现了从清洁能源到信息处理的技术应用的根本性突破。从微芯片到电池和光伏发电，无机晶体的发现一直受到昂贵的试错方法的瓶颈。与此同时，随着数据和计算的增加，语言、视觉和生物学的深度学习模型展示了新兴的预测能力。在这里，我们展示了大规模训练的图网络可以达到前所未有的泛化水平，从而将材料发现的效率提高一个数量级。在持续研究中发现的 48,000 个稳定晶体的基础上，效率的提高使得能够在当前凸包下方发现 220 万个结构，其中许多结构逃过了人类之前的化学直觉。我们的工作代表了人类已知的稳定材料的数量级扩展。最终凸包上的稳定发现将可用于筛选技术应用，正如我们对层状材料和固体电解质候选物的演示一样。在稳定结构中，有 736 个已通过独立实验实现。数以亿计的第一原理计算的规模和多样性也解锁了下游应用的建模能力，特别是导致高度准确和强大的学习原子间势，可用于凝聚相分子动力学模拟和高保真零-离子电导率的射击预测。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</guid>
      <pubDate>Wed, 29 Nov 2023 18:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPU 基准测试：23 个消费级 GPU 上的稳定 Diffusion v1.5（生成 460,000 个精美的二维码）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186v2ve/p_gpu_benchmark_stable_diffusion_v15_on_23/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186v2ve/p_gpu_benchmark_stable_diffusion_v15_on_23/</guid>
      <pubDate>Wed, 29 Nov 2023 17:33:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为我的项目绘制基本事实的注释应用程序推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186tsfy/d_annotation_apps_recommendation_to_draw_ground/</link>
      <description><![CDATA[     &lt; /td&gt; 我目前正在为我的特征工程课程做一个项目，该项目的目的是检测田地边界或道路。因此，对于基本事实，我使用了 Matlab 中的图像分割，它具有各种工具，例如洪水填充等，可以将田野填充为黑色，将道路设置为白色。正如您在第二张图片中看到的那样，这是我绘制的地面，它有很多噪点。我们的教授建议我们使用 iPad 来绘制真实图像。任何人都可以为我推荐绘制相同精确事实但没有噪音的应用程序   由   提交 /u/Certain-Seesaw-2274   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186tsfy/d_annotation_apps_recommendation_to_draw_ground/</guid>
      <pubDate>Wed, 29 Nov 2023 16:41:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获取参考图像并生成主题的附加图像的模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186t4k8/d_model_that_takes_a_reference_image_and/</link>
      <description><![CDATA[嗨，我正在寻找一篇我记得看过但忘记保存的论文。本文讨论了获取单个输入图像，例如与一个人，然后在其他环境中生成同一个人的其他图像。我依稀记得一个演示图像，以一幅女性的画作为参考图像，然后将该女性修复到现有的宇航员图像中。 我做了一些研究，break-a-scene 与我所说的最接近。但是，break-a-scene 讨论的是从源图像中提取多个标记，我认为我正在寻找的论文只讨论了一个主题。而且演示图像不存在。 以下是我发现的一堆论文，它们做了类似的事情，但我也不认为是我所看到的： https://realfill.github.io/ https://github.com/garibida/cross-image-attention https://github.com/SUDO-AI-3D/zero123plus https://omriavrahami.com/the-chosen-one/ https ://omriavrahami.com/break-a-scene/ https://github.com/腾讯ARC/CustomNet https://damo-vilab.github.io/AnyDoor -Page/ https://github.com/OPPO-Mente-实验室/主题扩散 Lmk，如果您知道处理此或相关任务的任何其他论文，谢谢！   由   提交 /u/synapticpaint   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186t4k8/d_model_that_takes_a_reference_image_and/</guid>
      <pubDate>Wed, 29 Nov 2023 16:13:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] Rankitect：在元规模上与世界级工程师对战的架构搜索排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186s7ps/r_rankitect_ranking_architecture_search_battling/</link>
      <description><![CDATA[   论文&lt; /strong&gt;: https://arxiv.org/abs/2311.08430 摘要:   神经架构搜索（NAS）已经证明了它在计算机视觉方面的功效和排名系统的潜力。然而，之前的工作主要集中在学术问题上，这些问题是在良好控制的固定基线下进行小规模评估的。在行业系统中，例如 Meta 中的排名系统，尚不清楚文献中的 NAS 算法是否能够超越生产基线，因为：（1）规模 - Meta 排名系统为数十亿用户服务，（2）强大的基线 - 基线是生产自深度学习兴起以来，多年来数百到数千名世界级工程师优化了模型，(3) 动态基线 - 工程师可能在 NAS 搜索过程中建立了新的、更强的基线，(4) 效率 - 搜索管道必须产生结果快速与生产生命周期保持一致。在本文中，我们介绍了 Rankitect，一个用于 Meta 排名系统的 NAS 软件框架。 Rankitect 寻求通过从头开始构建低级构建块来构建全新的架构。 Rankitect 实现并改进了最先进 (SOTA) NAS 方法，以在同一搜索空间下进行全面、公平的比较，包括基于采样的 NAS、一次性 NAS 和可微分 NAS (DNAS)。我们通过与 Meta 上的多个生产排名模型进行比较来评估 Rankitect。我们发现 Rankitect 可以从头开始发现新模型，实现归一化熵损失和 FLOP 之间的竞争性权衡。当利用工程师设计的搜索空间时，Rankitect 可以生成比工程师更好的模型，实现积极的离线评估和 Meta 规模的在线 A/B 测试。  https://preview.redd.it/n1l2fhle3b3c1.png?width=1360&amp;format=png&amp; ＆汽车=webp&amp;s=772bbd4885bd87fb14461d76f8aca804b32fe1b8   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186s7ps/r_rankitect_ranking_architecture_search_battling/</guid>
      <pubDate>Wed, 29 Nov 2023 15:35:28 GMT</pubDate>
    </item>
    <item>
      <title>机械地分析微调对程序定义的任务的影响（以及为什么微调很容易撤消）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186qh5x/mechanistically_analyzing_the_effects_of/</link>
      <description><![CDATA[       由   提交/u/hazardoussouth  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186qh5x/mechanistically_analyzing_the_effects_of/</guid>
      <pubDate>Wed, 29 Nov 2023 14:20:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] 他们说：“这不仅仅是记住训练数据”：从（生产）语言模型中可扩展地提取训练数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</link>
      <description><![CDATA[      &amp;# 32；由   提交/u/wojcech  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</guid>
      <pubDate>Wed, 29 Nov 2023 10:45:56 GMT</pubDate>
    </item>
    <item>
      <title>MeshGPT：使用仅解码器变压器生成三角形网格 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ktla/meshgpt_generating_triangle_meshes_with/</link>
      <description><![CDATA[       由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ktla/meshgpt_generating_triangle_meshes_with/</guid>
      <pubDate>Wed, 29 Nov 2023 08:36:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用侵犯版权但不受传统版权保护的音频进行培训是否合法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186kq06/d_is_it_legal_to_train_on_audio_that_is_copyright/</link>
      <description><![CDATA[我想根据音乐家录制的流行歌曲并因此拥有的钢琴封面来训练一个模型，但由于侵犯版权，他们仍然付费这些歌曲的作者的版税。很好奇这是否合法——我认为不合法，因为在这种情况下，这些录音并没有被货币化，而是被输入到一个模型中，并且这些录音仍然由授权给我的音乐家 100% 拥有。   由   提交 /u/SuperwhizAJ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186kq06/d_is_it_legal_to_train_on_audio_that_is_copyright/</guid>
      <pubDate>Wed, 29 Nov 2023 08:29:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果运行时间或 GPU 内存使用没有显着减少，那么参数高效微调的动机是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</link>
      <description><![CDATA[我一直在尝试诸如提示调整和 LoRA 之类的方法，这些方法的参数效率很高，因为它们只微调很小的一部分（即是所有参数的&lt;1%）。 但是对于这两种方法，您必须在反向传播期间缓存中间梯度，这意味着您在微调期间（或在由于无需存储冻结层的优化器状态，​​因此节省了大部分 GPU 内存）。例如，我已经让 LoRA 将我的自定义模型的 GPU 内存占用量从 8.5GB 减少到了 8.5GB。 8.1GB，非常小。微调时间减少也并不是真正的主要优势，每批微调同一模型减少了 20 毫秒，从 210 毫秒减少到 190 毫秒。 这引出了一个问题 - 实际原因是什么？参数高效微调（例如，带有 1.6k+ 引用的提示调整）的流行，如果它不能真正节省 GPU 内存和训练时间？ 我可以看到两个可能的原因（但我不太相信他们真的解释了围绕参数高效微调的“炒作”）：  下游任务的微调模型检查点显着减少。例如，在提示调整中，我们只需要在硬盘/SSD 上保存经过训练的微小软提示（〜很少兆字节），而不是整个更改后的模型权重（〜很多很多 GB）。  但从实际角度来看，我觉得大多数人都缺乏计算能力（例如 GPU 内存），而不是硬盘空间。换句话说，训练时间和 GPU 内存消耗似乎比节省检查点存储空间更相关。  第二个是域转移的鲁棒性（因为我们是保留大部分原始模型的权重，而不是破坏性地重新学习它们），这一点在提示调整论文中提到过，但在 LoRA 论文中却没有提及太多。  我可以认为这是一个可能的原因，但是在分布外设置中的提示调优论文中的性能增益充其量是微乎其微的，并且 LoRA 没有提到域转换。   （编辑 - 我还想知道是否还缺少其他东西来减少 GPU 内存和运行时间？我听说 QLoRA 增加了 4- LoRA 之上模型的位量化，所以也许这是解决 LoRA 内存效率问题的一种方法。但我不知道是否有什么可以减少内存占用以进行快速调整？）   由   提交/u/patricky168  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</guid>
      <pubDate>Wed, 29 Nov 2023 01:06:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>