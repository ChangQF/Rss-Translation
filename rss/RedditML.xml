<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 09 Apr 2024 00:58:18 GMT</lastBuildDate>
    <item>
      <title>[D] LightGBM 算法问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</link>
      <description><![CDATA[GBDT 算法帮助 嘿，我知道创建传统决策树时需要某种损失函数，例如均方误差 - 你扫描预测空间并找到最小化 MSE 的分割，这就是递归二元分割所实现的，直到达到某种停止标准。我也明白，如果要进行分割，您可以使用该区域中所有预测的平均值找到每个区域的新 MSE，这就是您如何确定是否进行分割的方法。 我目前正在学习提升，现在明白这个过程是相似的，但我们现在基于残差构建一棵树。过程完全相同吗？ 我一直在观看一些 statquest，这是 boosting 的通用算法 https://i.imgur.com/DudpZ5S.png 我正在努力理解 B) 和 C) 之间的区别。在 B 中，我们通过基于一些损失函数（如 MSE）进行递归二元分割来将回归树拟合到残差值（r_i_m）？但是对于 C 部分，我们在节点（称为 gamma）计算这些残差，这也最小化了损失函数？这不是我们在 B 部分所做的吗，因为我们取每个分割点残差的平均值，看看它是否最小化了我们的损失函数。 正如你所知，有点困惑，谢谢- 感谢任何帮助！   由   提交 /u/PencilSpanker   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</guid>
      <pubDate>Mon, 08 Apr 2024 23:05:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的研究技术堆栈是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</link>
      <description><![CDATA[我计划进行文本+音频的大型多模态训练（1B 参数）。截至目前，我正在考虑使用 pytorch、deepspeed、wandb。对于分布式大型模型训练，您有什么建议以及一般使用什么？ 您使用 Hughginface 吗？我觉得它有点太包裹了，以至于接触到裸露的主干会变得混乱，但还没有进行适当的尝试。对于现成的模型和自定义数据集训练，这听起来确实很有用，但研究需要的不仅仅是这些。那么，您在研究方面的经验如何，您需要灵活地改变模型？一般来说，您在研究方面的技术堆栈是什么？   由   提交/u/gokulPRO  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</guid>
      <pubDate>Mon, 08 Apr 2024 20:38:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] LegalKit 检索，使用标量 (int8) 的二分搜索通过法国法律代码重新评分</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz673f/p_legalkit_retrieval_a_binary_search_with_scalar/</link>
      <description><![CDATA[      此空间展示了 Louis Brulé Naudet 的 tsdae-lemone-mbert-base 模型，这是一个句子嵌入模型基于 BERT，使用基于 Transformer 的序列去噪自动编码器进行无监督句子嵌入学习，其目标只有一个：法国法律领域适应。 这一过程旨在提高内存效率和速度，二进制索引为小到足以容纳内存，并且 int8 索引作为视图加载以节省内存。此外，二进制索引的搜索速度比 float32 索引快得多（高达 32 倍），而重新评分也非常高效。 链接到 🤗 Space ：https://huggingface.co/spaces/louisbrulenaudet/legalkit-retrieval LegalKit 检索缩略图。  &amp; #32；由   提交/u/louisbrulenaudet  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz673f/p_legalkit_retrieval_a_binary_search_with_scalar/</guid>
      <pubDate>Mon, 08 Apr 2024 19:02:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM比较和参数调优的OSS工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</link>
      <description><![CDATA[      我最初将此工具编写为 CLI 应用程序，用于使用网格搜索测试推理参数的组合（因此名称 Ollama 网格搜索)。 https://preview.redd.it/vbgh6vbhpac1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=16687c2e9077348 c5e082ef85c80820c46c83ef9 以下是它的一些功能： ​  自动从本地或远程 Ollama 服务器获取模型； 迭代不同的模型和参数以生成推理； 同时对不同模型进行 A/B 测试提示；  进行同步推理调用以避免向服务器发送垃圾邮件； 可选择输出推理参数和响应元数据（推理时间、令牌和令牌）；&lt; /li&gt; 重新获取单个推理调用； 可以按名称过滤模型选择； 列出可以以 JSON 格式下载的实验格式； 可配置的推理超时； 可以在设置中定义自定义默认参数和系统提示   ​ 大多数主要平台的源代码和（未签名）版本可在以下位置获取： https://github.com/dezoito/ollama-grid-search  它仍在进行中......我计划添加更多功能，但我我正在利用业余时间做这件事。  希望这对使用开源模型的人们有用   由   提交 /u/grudev   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</guid>
      <pubDate>Mon, 08 Apr 2024 18:04:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 促进离策略学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</link>
      <description><![CDATA[        由   提交 /u/ggyshay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</guid>
      <pubDate>Mon, 08 Apr 2024 16:29:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 研讨会 vs. ECAI vs. BMVC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz0b6a/d_cvpr_workshop_vs_ecai_vs_bmvc/</link>
      <description><![CDATA[我最近在 CVPR 研讨会上接受了我的论文。然而，我的顾问建议撤回论文并将其提交给 ECAI 可能是更好的选择。作为一名计划在即将到来的秋天申请博士学位课程的理学硕士学生，我试图了解哪种选择可以更丰富我的简历。 在这种情况下，BMVC 相比如何，特别是考虑到我的工作重点是可解释性在视觉变形金刚中？ （更适合简历密集的会议）   由   提交/u/whats-a-monad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz0b6a/d_cvpr_workshop_vs_ecai_vs_bmvc/</guid>
      <pubDate>Mon, 08 Apr 2024 15:10:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于那些独自发表文章的人来说，你们的经历是怎样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</link>
      <description><![CDATA[这些天我有一些空闲时间，一直在努力赶上我所在领域的研究。我想真正重新审视我在硕士期间正在研究的一个主题，但始终无法发表论文。问题是，我不确定作为唯一作者，如果没有任何真正的资源访问权限，这是否可行。 朋友和熟人告诉我这是可能的，但极其困难。很好奇其他成功做到这一点的人是怎么想的。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</guid>
      <pubDate>Mon, 08 Apr 2024 14:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] DBRX 是专门为企业设计的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</link>
      <description><![CDATA[我了解到 Databricks 推出了名为 DBRX 的新通用 LLM。我很好奇它与其他法学硕士有何不同（除了开源之外）？ 它是专门为企业还是供公众使用而设计的，例如 chatgpt？或者它是否像“企业聊天”？他们可以在哪里微调开源模型以满足他们的需求？   由   提交/u/Ok_Moment4946   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</guid>
      <pubDate>Mon, 08 Apr 2024 13:22:40 GMT</pubDate>
    </item>
    <item>
      <title>“clip-vit-large-patch14”如何将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 代币，但有 [SOT] 和 [EOT] 代币。 [研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</link>
      <description><![CDATA[大家好， 我有以下问题： “clip-vit-large-patch14”如何;将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 标记，但有 [SOT] 和 [EOT] 标记。 当我使用 CLIP 文本编码器并提取 pooler_output 时……这个向量到底是如何创建的？ [SOT] 代币是否用作 [CLS] 代币？或者是否进行了池化操作？ [研究] 此致， Tom  &amp; #32；由   提交/u/tommilyjonesOG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</guid>
      <pubDate>Mon, 08 Apr 2024 13:09:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 手写文本到文本项目 - 寻求提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</link>
      <description><![CDATA[大家好，我有一个朋友，他有自己的生意，他经常告诉我他手动抄写帐单浪费了多少时间从手写纸滑到他电脑上的文档。我不是专业人士，我仍在学习，但我想知道是否可以使用已经训练好的模型应用一些迁移学习，然后使用新数据  有谁知道是否有一些CNN已经用于此类项目？ 使用什么样的激活函数？感谢您的关注，并提前感谢您的回复:)    由   提交/u/December92_yt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</guid>
      <pubDate>Mon, 08 Apr 2024 10:46:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 确保加拿大的人工智能优势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</guid>
      <pubDate>Mon, 08 Apr 2024 09:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于高性能语言技术的新的海量多语言数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.14009 项目页面：https://hplt -project.org/ 数据集：https:// /hplt-project.org/datasets/v1.2 GitHub： https://github.com/hplt-project 摘要：  我们介绍HPLT（高性能语言技术）语言资源，一个新的大规模多语言数据集，包括从 CommonCrawl 中提取的单语和双语语料库以及从互联网档案馆中提取的以前未使用的网络爬虫。我们描述了大型语料库的数据采集、管理和处理方法，这些方法依赖于开源软件工具和高性能计算。我们的单语集合侧重于中低资源语言，涵盖 75 种语言，并在文档级别删除了总共约 5.6 万亿个单词标记。我们以英语为中心的平行语料库源自其单语对应语料库，涵盖 18 个语言对和超过 9600 万个对齐句子对，以及大约 14 亿个英语标记。 HPLT 语言资源是迄今为止发布的最大的开放文本语料库之一，为语言建模和机器翻译培训提供了丰富的资源。我们公开发布了这项工作中使用的语料库、软件和工具。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</guid>
      <pubDate>Mon, 08 Apr 2024 08:56:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么人们在 Arxiv 上上传他们的作品，而不是提交会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</link>
      <description><![CDATA[纯粹是我的好奇心。 我知道这种情况，当一篇论文在会议或期刊上被接受，并且可以上传时他们在 Arxiv 上的工作。但我的问题是，有些作品只在Arxiv上上传。这是否意味着作者不想在会议上提交自己的作品，而是想发布自己的作品？或者，发布后他们还有其他提交计划吗？ 我这么问是因为我最近的工作在一次会议上被拒绝了，但我不想再深究了。人们是否也喜欢在 Arxiv 上上传废弃作品的同样情况？   由   提交/u/Shot-Button-9010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</guid>
      <pubDate>Sun, 07 Apr 2024 17:31:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们知道Gemini 1.5是如何实现10M上下文窗口的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</link>
      <description><![CDATA[我们知道 Gemini 1.5 是如何实现 1.5M 上下文窗口的吗？随着注意力窗口的扩大，计算量不会呈二次方增长吗？    由   提交/u/papaswamp91  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</guid>
      <pubDate>Sun, 07 Apr 2024 16:21:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>