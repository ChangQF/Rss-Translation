<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 08 Jul 2024 06:21:57 GMT</lastBuildDate>
    <item>
      <title>[D] 机器学习和人工智能相关任务中最常用的词之一是？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dy0phc/d_one_of_the_most_used_words_in_machine_learning/</link>
      <description><![CDATA[尤其是在播客中 ---&gt; &quot;显然&quot; 在&quot;机器学习&quot; 和 &quot;人工智能&quot; 期待有趣的观察！    提交人    /u/Worth-Card9034   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dy0phc/d_one_of_the_most_used_words_in_machine_learning/</guid>
      <pubDate>Mon, 08 Jul 2024 05:07:01 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 神经解码 - 将歌曲聆听的脑电图数据映射到相应的音频文件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</link>
      <description><![CDATA[大家好， 我有一个数据集，其中包含参与者听一组 10 首歌曲的预处理 EEG 数据（脑电图时间序列数据）。我的目标是构建一个回归模型，将 EEG 数据作为输入映射回原始音频文件作为输出，以尝试重建参与者正在听的歌曲。这是一个利用 CNN 执行此操作的示例：https://arxiv.org/abs/2207.13845 以下是我遇到的一些障碍：  在链接的论文中，EEG 数据和目标音频文件被切成 1 秒长的片段，并在这些片段上训练 CNN。一个可能的问题是音频和 EEG 数据之间的延迟 - 大脑对刺激做出反应需要不可忽略的时间（大约 100 毫秒）。这会使 1 秒分段成为一种有问题的方法吗？我是否应该在模型中明确考虑这种延迟？或者模型应该在训练中“自行”考虑这个问题？是否有可以避免将数据划分为任意段的模型？ 我遇到的部分困难是我缺乏处理时间序列数据的经验，不知道哪些模型合适。CNN 是捕获短期和长期时间依赖性的理想选择吗？或者基于循环网络或变压器的架构是否更合适？我的直觉（可能完全错误）告诉我 RNN 更适合捕获短期依赖性，而变压器更适合捕获整个输入的依赖性。 降维会在这里有所帮助吗？输入数据是 64 通道 EEG 数据，采样率为 1024 Hz，平均歌曲长度约为 200 秒。这意味着十个输入数据中的每一个的大小约为 ~64 x 200K。我正在考虑使用 ICA，因为 EEG 数据可能很棘手且嘈杂（电极在微伏级上敏感），并且 ICA 通常与 EEG 数据一起使用以消除伪影。  任何建议或想法都将不胜感激。另外，我应该指出，我这样做既是为了研究也是为了学习目的 - 我目前不担心可扩展性/效率，我愿意使用现有框架或从头开始开发模型。 提前谢谢您。    提交人    /u/dusmansen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</guid>
      <pubDate>Mon, 08 Jul 2024 01:49:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 尝试梯度下降。为什么不使用 Nelder-Mead？长篇。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxw5ge/r_experimenting_with_gradient_descent_why_not/</link>
      <description><![CDATA[关于 GD 有很多视频和网页。但是，几乎所有示例都过于简单。我挖掘了一些 2005 年的数据，这些数据只有五个参数需要确定，但 GD 却取得了成功。似乎 GD 总是在遇到障碍物时反弹。它也非常依赖于步长。需要对步长进行太多猜测。我想知道是什么让这变得如此困难，因为其他方法（如 Nelder-Mead）可以在几秒钟内找到最小值。为了回答我的问题，我打印出了每次迭代的 MSE 和五个参数或 MSE 和五个参数的变化。 使用真实数据时没有公式可以区分。我必须通过在每个方向上采取小步并除以增量的差值 2 来估计每个参数的导数。为了得到中心差分斜率，我必须对每个参数在正方向和负方向上采取一小步，然后除以两倍的步长。对于真实数据，导数可能会很嘈杂。每个小步骤都需要评估成本函数，因此需要 10 次评估才能计算梯度。这很耗时。我发现，与 YouTube 和其他常见示例不同，“地形”不像碗状，而是像大峡谷，有各种曲折。这意味着我不能使用大步，因为 GD 经常会在 5 个维度中遇到“峡谷壁”。然后我尝试了许多其他增强版的 GD，它们都不太好用，尽管有些会在一小时左右接近。GD-Adam GD-RMSprop GD-delta GD-AdaGrad 表现不佳。但是，我确实找到了一个新的附加代码，它使所有这些代码，尤其是简单的 GD 工作得更好。诀窍是迈出一步。评估它，如果它更好，那么就朝同一方向再迈一步，直到 MSE 不再变好，然后重复循环以获得新的梯度。这样做的好处是，计算“撞”在“墙壁”上的梯度所浪费的时间更少。此外，步长可以小一点，以导航“地形”中曲折的部分，但当找到一个好的方向时，能够朝一个方向迈出多步就像迈出更大的一步。 直观地讲，它的工作原理是这样的。起点是峡谷中某个有溪流的地方。溪流是当前的低点。然而，它蜿蜒曲折，所以任何一步都会撞到溪流的岸边，甚至可能是峡谷的墙壁。最好不要完全沿着溪流走，因为有很多曲折。最好尽可能在一个方向上“飞越溪流”，在“峡谷壁”之间飞越。这样可以避免大量计算梯度，因为如果有大量数据，则必须计算大量梯度，这可能非常耗时。 现在，如果您拥有像碗一样的漂亮地形，所有这些都没有区别。您在 YouTube 或网站上看到的任何方法都可以使用，但我从未见过一种方法可以处理真实数据，而且地形很可能不是像碗一样的结构。此外，找到具有 10 个点的导数在时间上非常昂贵。 这就是为什么我想知道为什么不使用 Nelder-Mead，除非它要高效，否则它会占用大量内存，但内存很便宜。 我看过很多关于不同形式梯度下降的 YouTube 视频。我相信大多数人教的都是他们所学的东西，并没有真正处理过真实数据。 接下来是找到一些真实的训练数据来识别 8x8 网格中的字符，但即使是 16x16 点阵也很好。我知道我正在重新发明轮子，但我想尝试我的新附加代码和 Nelder-Mead 来最小化成本函数 GD #gradient-descent, #Nelder-Mead, #Adam #AdaGrad, #Delta, #RMSprop    提交人    /u/pnachtwey   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxw5ge/r_experimenting_with_gradient_descent_why_not/</guid>
      <pubDate>Mon, 08 Jul 2024 01:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[项目] minigrad - andrej karpathy 用 Go 语言实现的 micrograd</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</link>
      <description><![CDATA[这个周末，我想学习更多关于 ML 的知识，并继续从事我的 ML 项目。所以我用 golang（重新）构建了 karpathy 的 micrograd。也学习了更多关于反向传播的知识。很想知道你的。请贡献并在 GitHub repo 上留下一颗星。 github - https://github.com/0verread/minigrad    提交人    /u/ElegantGoose9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</guid>
      <pubDate>Mon, 08 Jul 2024 00:51:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] Open-TeleVision：具有沉浸式主动视觉反馈的远程操作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</link>
      <description><![CDATA[        提交人    /u/XiaolongWang   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</guid>
      <pubDate>Sun, 07 Jul 2024 23:09:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你们都用什么进行大规模训练？普通的 pytorch 还是使用像 HF Accelerate 这样的库。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</link>
      <description><![CDATA[我很快就要为研究论文训练一个大型集群多机器。好奇你们都为大规模训练做了什么，是坚持我所知道的 pytorch（FSDP、DDP、TP、MP 等...）和 slurm 更好，还是值得学习像 HF accelerate 这样的东西进行大规模训练？    提交人    /u/I_will_delete_myself   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</guid>
      <pubDate>Sun, 07 Jul 2024 22:46:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关系数据中的 AI/ML 应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxm0hz/d_aiml_applications_in_relational_data/</link>
      <description><![CDATA[我有一个包含大量表和列的数据库，并且它们之间的关系是已知的。 根据对数据模型的理解，我们实施了多项基于规则的数据质量检测。 但是，使用机器学习可以构建哪些可能的用例？关系数据中的可能应用程序有哪些。 或者，如果事先知道逻辑和规则，基于规则的检查是否是最好的。 我想学习和阅读有关关系数据中 AI/ML 用例的更多信息。如果有人可以指出有关用例以及如何实现用例的正确文章/方向，那将会很有帮助。 谢谢    提交人    /u/rekonist-app   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxm0hz/d_aiml_applications_in_relational_data/</guid>
      <pubDate>Sun, 07 Jul 2024 17:34:50 GMT</pubDate>
    </item>
    <item>
      <title>[N] PyTorch 官方纪录片回顾了它的过去和未来</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxlqg2/n_official_pytorch_documentary_revisits_its_past/</link>
      <description><![CDATA[        由    /u/gadgetygirl 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxlqg2/n_official_pytorch_documentary_revisits_its_past/</guid>
      <pubDate>Sun, 07 Jul 2024 17:23:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 添加多个 GPU 是否会同时线性增加 FLOP 速率和 VRAM 内存带宽？如果不是，随着利用率的增加，利用率是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxlll2/d_does_adding_multiple_gpus_increase_both_the/</link>
      <description><![CDATA[我正在尝试优化我的 LLM 服务器的 AWS 消耗，我很好奇在 TTFT、TPOT 和每小时总成本方面，拥有 4X 或 8X AMD Radeon Pro V520 意味着什么。对于我的指标估计，我遵循 https://www.jinghong-chen.net/estimate-vram-usage-in-llm-inference/。但是，我想了解是否可以通过堆叠更多 GPU 来线性增加 FLOPS 速率和 VRAM 带宽。谢谢。  我的理解是存在某种模型带宽和模型 flops 利用率。我已经看到 8X H100-80gb 有 25% 的 MBU。我希望我可以更好地理解如何在不消耗太多现金的情况下估算这些曲线。 PD：此外，我希望我可以更深入地了解该主题，如果您能给我指出一些有关该主题的入门知识，我将不胜感激。    提交人    /u/automated_msp   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxlll2/d_does_adding_multiple_gpus_increase_both_the/</guid>
      <pubDate>Sun, 07 Jul 2024 17:17:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如果提供微调 API，则可以通用地越狱 LLM 的安全输入和输出</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxiqhh/r_a_universal_way_to_jailbreak_llms_safety_inputs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxiqhh/r_a_universal_way_to_jailbreak_llms_safety_inputs/</guid>
      <pubDate>Sun, 07 Jul 2024 15:11:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Llama 创建 DPO 数据集：最佳实践？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxh210/d_creating_a_dpo_dataset_using_llama_best/</link>
      <description><![CDATA[我要创建一个合成 DPO 数据集来微调 Llama-3-8b。我是否可以只将 Llama-3-70 的响应用作“已接受”而将 Llama-3-8b 的响应用作“已拒绝”？或者更好的方法是从 Llama-3-8b 中抽取两个响应，并将其中一个选为已接受，另一个选为已拒绝。    提交者    /u/AdKind316   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxh210/d_creating_a_dpo_dataset_using_llama_best/</guid>
      <pubDate>Sun, 07 Jul 2024 13:54:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReproModel：开源 ML 研究工具箱更新！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxgt56/p_repromodel_open_source_ml_research_toolbox/</link>
      <description><![CDATA[大家好，我很高兴与大家分享 ReproModel 的最新动态，这是一个开源工具箱，旨在简化机器学习模型的测试和复制。 我和你们中的许多人一样，在对模型进行基准测试和比较方面遇到了很多困难，从缺少代码到不透明的实验参数减慢了进程。我决定自己动手，在我的工作场所创建了一个迷你工具箱来简化这个过程。 目标是减少复制实验所花费的时间和精力，使研究人员能够专注于创新而不是设置。 我知道这项任务并不容易，不久前我联系了社区，并获得了巨大的帮助。在团队的努力下，我们现在已经在已经实现的功能中添加了代码提取器、AI 实验描述生成器和自定义脚本编辑器。 该项目是开源的，如果您喜欢我们正在构建的内容，欢迎与我们分享您的想法、做出贡献或留下一颗星 :) 您可以在此处找到存储库：https://github.com/ReproModel/repromodel 感谢您的时间，请随时在下面留下任何意见或建议！    提交人    /u/MintOwlTech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxgt56/p_repromodel_open_source_ml_research_toolbox/</guid>
      <pubDate>Sun, 07 Jul 2024 13:42:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在深度模型上进行“深度工作”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</link>
      <description><![CDATA[大家好， 我最大的生产力挑战之一是等待深度学习训练循环、标记化或处理循环运行时的停机时间。对于较短的循环，这些循环可能需要 5 分钟到一个小时的时间，在此期间，我常常发现自己不知道该做什么。 开始一项新任务很困难，因为不断的上下文切换会打乱我的工作流程和注意力。 我以前在大学里遵循深度工作方法，这确实有助于控制我的注意力缺陷多动障碍。我白天不使用手机或社交媒体，一次只“专注于”一项任务。 现在，我觉得这几乎是不可能的。我“被迫”休息这些小憩，不断在任务之间切换，这非常具有挑战性。 您对如何充分利用这些间隔有什么建议吗？你会为这些时间段保留特定任务吗？ 即使从专注编码切换到阅读论文，如果只花 10 分钟左右的时间，也会非常困难。 有人解决过这些问题吗，还是只有我？ 谢谢。    提交人    /u/Magnospm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</guid>
      <pubDate>Sun, 07 Jul 2024 13:02:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于 Mamba 的语言模型实证研究（8B Mamba-2-Hybrid 在 3.5T token 数据上）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx9ggp/r_an_empirical_study_of_mambabased_language/</link>
      <description><![CDATA[链接：http://arxiv.org/abs/2406.07887  选择性状态空间模型 (SSM)（如 Mamba）克服了 Transformers 的一些缺点，例如序列长度的二次计算复杂度和键值缓存的大量推理时间内存要求。此外，最近的研究表明，SSM 可以匹配或超越 Transformers 的语言建模能力，使其成为一种有吸引力的替代方案。然而，在受控设置（例如相同的数据）中，迄今为止的研究仅展示了将 SSM 与 Transformers 进行比较的小规模实验。为了了解这些架构在更大规模上的优势和劣势，我们直接比较了在多达 3.5T 个 token 的相同数据集上训练的 8B 参数 Mamba、Mamba-2 和 Transformer 模型。我们还将这些模型与由 43% Mamba-2、7% 注意力和 50% MLP 层 (Mamba-2-Hybrid) 组成的混合架构进行了比较。使用一组不同的任务，我们回答了 Mamba 模型是否可以在更大的训练预算下与 Transformers 匹敌的问题。我们的结果表明，虽然纯 SSM 在许多任务上与 Transformers 匹敌或超过 Transformers，但它们在需要强大复制或上下文学习能力（例如 5-shot MMLU、电话簿）或长上下文推理的任务上落后于 Transformers。相比之下，我们发现 8B Mamba-2-Hybrid 在我们评估的所有 12 个标准任务上都超过了 8B Transformer（平均 +2.65 分），并且预计在推理时生成 token 时速度最高可提高 8 倍。为了验证长上下文能力，我们提供了额外的实验，评估 Mamba-2-Hybrid 和 Transformer 的变体，以支持 16K、32K 和 128K 序列。在另外 23 个长上下文任务中，混合模型继续接近或平均超过 Transformer。为了进一步研究，我们发布了检查点以及用于训练我们模型的代码，作为 NVIDIA Megatron-LM 项目的一部分。     submitted by    /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx9ggp/r_an_empirical_study_of_mambabased_language/</guid>
      <pubDate>Sun, 07 Jul 2024 05:52:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 07 Jul 2024 02:15:10 GMT</pubDate>
    </item>
    </channel>
</rss>