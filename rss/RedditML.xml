<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 11 Dec 2023 18:17:30 GMT</lastBuildDate>
    <item>
      <title>[R] 如何阅读和理解Einops表达式？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18g01bv/r_how_to_read_and_understand_einops_expressions/</link>
      <description><![CDATA[这是我目前对 einops 的了解：  einops 代表爱因斯坦启发的运算符号。 符号大致受到爱因斯坦求和（特别是 numpy.einsum 运算）的启发。 h = 高度，w = 宽度，c = 通道（颜色），b = 批次 左侧是输入形状。左侧是输出形状。 括号中的字母相乘。 einops.rearrange 包括转置（轴排列）、重塑（视图）、挤压、取消挤压、堆叠等功能，连接和其他操作。  我不明白：  所有操作元素是什么？我上面遗漏了什么吗？  我如何阅读正在进行的操作？ （即我如何知道图像将被压缩或分割成不同的图像？） 操作中顺序重要吗？ （即‘wh c -&gt; (w h) c’与‘h w c -&gt; (h w) c’不同吗？） 为什么有些元素出现在运算中而另一些则没有？ （即“h w c -&gt; (h w) c”与“h w -&gt; (h w)”不同吗？）  我试图理解的 einops.rearrange 操作示例：   &#39;b f h w c -&gt; (b f) c h w’ ‘(b f) e -&gt; b f e’ ‘br r -&gt; br ()’ ‘b s e -&gt; (b s) e’ ‘b s -&gt; (b s)&#39;  之前的研究参考：  https: //einops.rocks/api/rearrange/ https://openreview.net/pdf ?id=oapKSVM2bcj https://youtu.be/ll1BlfYd4mU?si=BmCVibyEifrZhiXC https://youtu.be/xGy75Pjsqzo?si=GiaxqN4vSX9_uTtL    由   提交 /u/Joe_The_Armadillo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18g01bv/r_how_to_read_and_understand_einops_expressions/</guid>
      <pubDate>Mon, 11 Dec 2023 17:59:32 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 在虚构宇宙中训练的无提示聊天机器人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fxki4/discussion_a_promptless_chatbot_trained_on_a/</link>
      <description><![CDATA[我来这里是为了分享一个我一直在思考的概念，它是人工智能、创意故事讲述和同人文化的交叉点。您的见解对于塑造这个想法将是非常宝贵的。 概念： 想象一个“无提示”聊天机器人，它不仅仅是反应性的（就像当前的人工智能一样）模型），但可以根据其训练发起对话，并且具有创造性的转变。这种人工智能仅在漫威或 DC 等特定的虚构宇宙中进行训练，并且它的运作相信这个宇宙就是它的真实现实。目标是让它生成提示并参与深深植根于该宇宙的传说、角色和故事情节的对话。 为什么它如此吸引人： 创意探索：这个概念提供了一种与这些深受喜爱的虚构世界互动并为其做出贡献的新方式。 有目的的同人小说：有可能通过激励同人小说作为增强人工智能学习的一种手段，可能为作家带来新的创作机会。 技术创新：对于开发者来说，这提出了将人工智能推向叙事理解和创造力的未知领域的挑战。 技术方面： 聊天机器人将使用先进的 NLP 技术，并接受来自虚构宇宙的综合数据集的训练，包括漫画、电影和人物传记。 提示人工智能发起场景或讨论的无交互模型。 想要的想法： 您对这个想法的初步印象是什么？ 对于这些虚构宇宙的粉丝和作家来说，这听起来有多吸引人？   由   提交/u/sjseto0519   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fxki4/discussion_a_promptless_chatbot_trained_on_a/</guid>
      <pubDate>Mon, 11 Dec 2023 15:49:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] openai失败时的备份</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fwme5/d_backup_if_openai_fails/</link>
      <description><![CDATA[嗨！我们正在为我的公司实施一个使用 chatgpt 的机器人，但我们对 openai 停机故障有一些担忧。您是否有一些可以在生产中实施的备份解决方案，以防发生故障（模型切换）？知道模型是经过微调的，因为我们正在使用嵌入（使用 gpt 3.5 创建）进行信息检索   由   提交 /u/New_Detective_1363   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fwme5/d_backup_if_openai_fails/</guid>
      <pubDate>Mon, 11 Dec 2023 15:07:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 指针网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fw6cf/d_pointer_network/</link>
      <description><![CDATA[嗨， 我正在尝试在 pytorch 中实现指针网络。 ``` RNN 在每个时间步 i 被馈送 P_i，直到到达输入序列的末尾，此时一个特殊符号 ⇒ 将被输入到模型中。 模型然后切换到生成模式，直到网络遇到特殊符号⇐，它代表输出序列的终止。 ``` 如何指向⇐符号？用于注意力的编码器 LSTM 输出将等于输入序列的长度，并且这个特殊符号不是编码过程的一部分。论文没有强调这个令牌是否也是编码器初始化的一部分。 有人可以帮忙吗？   由   提交/u/Affectionate478  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fw6cf/d_pointer_network/</guid>
      <pubDate>Mon, 11 Dec 2023 14:47:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] LagrangeBench：拉格朗日流体力学基准测试套件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fu897/p_lagrangebench_a_lagrangian_fluid_mechanics/</link>
      <description><![CDATA[Github：github.com/tumaer/lagrangebench arXiv：arxiv.org/abs/2309.16342  作者：Artur Toshev、Gianluca Galletti 等人。  这是什么？ LagrangeBench是一个针对CFD粒子问题的机器学习基准测试库，基于&lt;强&gt;JAX。它旨在评估和开发具有挑战性的物理问题的学习粒子模型（例如图神经网络）。据我们所知，这是针对这组特定问题的第一个基准。我们的工作受到 PDEBench 和 PDEArena，我们建议将其作为拉格朗日的替代方案。 核心贡献  7 个新的 2D/3D 粒子流体数据集，基于已建立的 CFD 问题，并使用我们自己的平滑粒子流体动力学 (SPH) 求解器生成，该求解器也是用 JAX 编写的。 添加了三种不同的邻居搜索实现，以处理更大的问题系统或可变粒子计数。 JAX 重新实现各种图神经网络：GNS、SEGNN、EGNN、PaiNN。 训练策略  strong&gt;，包括随机游走噪声[A Sanchez-Gonzalez et al.]和前推损失[J Brandstetter et al.].  摘要  机器学习已成功应用各种科学应用中基于网格的 PDE 建模。然而，基于拉格朗日粒子离散化的学习偏微分方程求解器（这是解决自由表面或复杂物理问题的首选方法）在很大程度上仍未得到探索。我们推出了 LagrangeBench，这是第一个针对拉格朗日粒子问题的基准测试套件，重点关注时间粗粒度。具体来说，我们的贡献是：(a) 使用平滑粒子流体动力学 (SPH) 方法生成的七个新流体力学数据集（四个 2D 和三个 3D），包括泰勒-格林涡流、盖驱动腔、反向泊肃叶流、和溃坝，其中每个都包括不同的物理原理，如实墙相互作用或自由表面，(b) 高效的基于 JAX 的 API，具有各种最新的训练策略和三个邻居搜索例程，以及 (c) 已建立的图神经网络 (GNN) 的 JAX 实现）像 GNS 和 SEGNN 一样具有基线结果。最后，为了衡量学习代理的性能，我们超越了既定的位置误差，并引入了粒子分布的动能 MSE 和 Sinkhorn 距离等物理指标。    由   提交/u/ggalletti99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fu897/p_lagrangebench_a_lagrangian_fluid_mechanics/</guid>
      <pubDate>Mon, 11 Dec 2023 13:11:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你能在有限使用的数据集上训练模型并使其使用 apache 许可证开源吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ftvv0/d_can_you_train_a_train_a_model_on_a_limiteduse/</link>
      <description><![CDATA[我一直在寻找可在我的项目中使用的公共数据集，这些数据集将用于商业用途。在本主题中，作者通常不允许商业用途。然而，我发现了一个带有 apache 许可证的开源模型，该模型是在这个有限使用的数据集上进行训练的。这个模型可以用于商业用途吗？   由   提交/u/Horror_Panda920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ftvv0/d_can_you_train_a_train_a_model_on_a_limiteduse/</guid>
      <pubDate>Mon, 11 Dec 2023 12:53:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] SparQ Attention：带宽高效的 LLM 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ftsaq/r_sparq_attention_bandwidthefficient_llm_inference/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.04985 摘要：  生成式大语言模型（LLM）开辟了许多新颖的可能性，但由于其巨大的计算要求，它们的普遍使用仍然具有挑战性。一些最有用的应用程序需要一次处理大量样本并使用长上下文，这两者都会显着增加模型的内存通信负载。我们引入了SparQ Attention，这是一种通过选择性获取缓存历史记录来减少注意力块内的内存带宽需求，从而提高 LLM 推理吞吐量的技术。我们提出的技术可以在推理过程中直接应用于现成的法学硕士，无需对预训练设置进行任何修改或进行额外的微调。我们通过在各种下游任务上评估 Llama 2 和 Pythia 模型，展示了 SparQ Attention 如何将注意力内存带宽要求降低多达八倍，而不会损失任何准确性。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ftsaq/r_sparq_attention_bandwidthefficient_llm_inference/</guid>
      <pubDate>Mon, 11 Dec 2023 12:47:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 多级聚合数据集的最佳时间序列模型是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fsa6x/d_what_is_best_time_series_model_for_dataset_with/</link>
      <description><![CDATA[我有一个根据位置、产品类型和日期聚合的数据集。如果对位置和产品类型的每种组合使用不同的模型，或者仅使用具有时间特征的回归模型，会更好吗？还建议哪种模型最适合此类数据集？   由   提交 /u/oakvard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fsa6x/d_what_is_best_time_series_model_for_dataset_with/</guid>
      <pubDate>Mon, 11 Dec 2023 11:17:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 批量大小对训练损失有什么影响？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fs4ik/d_what_is_the_effect_of_batch_size_on_training/</link>
      <description><![CDATA[我一直在尝试批量大小，我发现在训练集上使用较大的批量大小与较小的批量大小相比，损失较低。我知道这样一个事实，即批量大小越小，权重更新就越多，但是我无法理解在相同的情况下损失有何不同。训练纪元数。   由   提交 /u/Melodic_Stomach_2704   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fs4ik/d_what_is_the_effect_of_batch_size_on_training/</guid>
      <pubDate>Mon, 11 Dec 2023 11:06:39 GMT</pubDate>
    </item>
    <item>
      <title>如何只下载 ImageNet 数据库中的动物？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fr53n/how_to_download_just_the_animals_in_the_imagenet/</link>
      <description><![CDATA[还有动物及其各自标签的地图？有没有一种有效的方法可以做到这一点。   由   提交/u/emperor_xyz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fr53n/how_to_download_just_the_animals_in_the_imagenet/</guid>
      <pubDate>Mon, 11 Dec 2023 10:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] MMOCR 与 TrOCR</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fqvhs/d_mmocr_vs_trocr/</link>
      <description><![CDATA[向大家问好。 在我们当前的任务中，我们正在使用 MMOCR，它提供了很高的准确性，但是每个新的预测数据示例需要 1.4 秒。对于我们的任务来说，这是一个相当慢的结果，我们正在寻找更快的模型。 现在我们正在寻找任何可能的替代方案，我注意到 TrOCR（基于 Transformer 的 OCR）提供了很好的效果结果。但我不知道需要多少时间，是否会更准确。 我正在寻找对我提到的模型（或其他结果更好的模型）有经验的人，他可以帮我弄清楚。 而且，我还有另一个问题：以下哪种方法在多行文本块标记的预测时间方面效果更快：逐行或整个阻止？   由   提交 /u/thattallsoldier   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fqvhs/d_mmocr_vs_trocr/</guid>
      <pubDate>Mon, 11 Dec 2023 09:42:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] Google Gemini 是真货还是宣传噱头？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18fbc04/d_is_google_gemini_the_real_deal_or_a_publicity/</link>
      <description><![CDATA[我关注法学硕士和多模式模型的演变已经有一段时间了，我对 Google 推出 Gemini 感到非常兴奋。说实话，我不确定双子座是否还在那里。听起来好像它被宣传得比实际情况要多得多。  https ://www.cnbc.com/2023/12/08/google-faces-controversy-over-edited-gemini-ai-demo-video.html   由   提交 /u/Dry_Cattle9399   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18fbc04/d_is_google_gemini_the_real_deal_or_a_publicity/</guid>
      <pubDate>Sun, 10 Dec 2023 19:34:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我对 Llama 进行了微调，为我的代码库生成系统图</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f6phk/p_i_finetuned_llama_to_generate_system_diagrams/</link>
      <description><![CDATA[       由   提交/u/jsonathan  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f6phk/p_i_finetuned_llama_to_generate_system_diagrams/</guid>
      <pubDate>Sun, 10 Dec 2023 16:03:27 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 归一化流（如果有）相对于 GAN 和 VAE 的优势？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</link>
      <description><![CDATA[鉴于这最后一篇文章，我认为对这个主题进行更新的讨论可能会很有用。即，与其他方法（例如 GAN、VAE、带有 SDE 的贝叶斯神经网络）相比，标准化流有哪些实际优势今天？ 我对这个主题也很陌生。如果问题太幼稚请见谅。   由   提交/u/WorldML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</guid>
      <pubDate>Sun, 10 Dec 2023 12:29:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>