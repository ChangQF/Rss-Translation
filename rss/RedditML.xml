<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 23 Mar 2024 12:24:33 GMT</lastBuildDate>
    <item>
      <title>[D] 十亿还是比特？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blqqd2/d_billions_or_bits/</link>
      <description><![CDATA[ 由   提交 /u/neptunography   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blqqd2/d_billions_or_bits/</guid>
      <pubDate>Sat, 23 Mar 2024 12:12:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 考虑到目标的分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blqj0w/r_classification_with_goal_in_mind/</link>
      <description><![CDATA[假设我有一个分类问题和不对称错误分类成本。我可以简单地比较两个模型并根据样本外的预期利润值设置拒绝率吗？   由   提交/u/gardas603  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blqj0w/r_classification_with_goal_in_mind/</guid>
      <pubDate>Sat, 23 Mar 2024 12:01:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] DenseFormer：通过深度加权平均增强 Transformer 中的信息流</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blp62y/r_denseformer_enhancing_information_flow_in/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.02622 代码：https://github .com/epfml/DenseFormer 摘要：  Vaswani 等人的变压器架构。 （2017）现在在从自然语言处理到语音处理和图像理解的各个应用领域中无处不在。我们提出了DenseFormer，这是对标准架构的简单修改，可以在不增加模型大小的情况下提高模型的复杂度——为 100B 参数范围内的大型模型添加数千个参数。我们的方法依赖于每个转换器块之后的附加平均步骤，该步骤计算当前和过去表示的加权平均值 - 我们将此操作称为深度加权平均（DWA）。学习到的 DWA 权重表现出连贯的信息流模式，揭示了对来自远处层的激活的强大且结构化的重用。实验表明，DenseFormer 的数据效率更高，达到了与更深的 Transformer 模型相同的困惑度，并且对于相同的困惑度，这些新模型在内存效率和推理时间方面优于 Transformer 基线。  &lt; /div&gt;  由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blp62y/r_denseformer_enhancing_information_flow_in/</guid>
      <pubDate>Sat, 23 Mar 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mora：通过多代理框架实现通用视频生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.13248 GitHub：https:// /github.com/lichao-sun/Mora 摘要：  Sora是第一个大规模通才视频生成的模式引起了社会的广泛关注。自 OpenAI 于 2024 年 2 月推出以来，没有其他视频生成模型能够与 Sora 的性能或支持广泛视频生成任务的能力相媲美。此外，只有少数完全发布的视频生成模型，其中大多数是闭源的。为了解决这一差距，本文提出了一种新的多智能体框架Mora，它结合了多个先进的视觉人工智能智能体来复制Sora演示的通用视频生成。特别是，Mora 可以利用多个视觉代理并在各种任务中成功模仿 Sora 的视频生成功能，例如（1）文本到视频生成，（2）文本条件图像到视频生成，（3）扩展生成视频、(4) 视频到视频编辑、(5) 连接视频和 (6) 模拟数字世界。我们大量的实验结果表明，Mora 在各种任务中都取得了与 Sora 相近的性能。但综合来看，我们的工作与Sora之间存在着明显的绩效差距。总而言之，我们希望这个项目能够通过协作人工智能代理来指导视频生成的未来轨迹。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/</guid>
      <pubDate>Sat, 23 Mar 2024 10:19:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我听了 Sam Altman 最近在 Lex Fridman 进行的 2 小时采访 - 以下是我们都应该知道的关键要点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</link>
      <description><![CDATA[Altman 本周在 Lex Fridman 播客上接受采访。这是一次相当长的采访，所以我想分享一下我听他讲话时记下的 10 个关键要点。你可以支持他，也可以反对他——但你不能否认他是即将到来的人工智能常态中最重要的核心人物之一！希望这对感兴趣的人来说是有洞察力的:) -- 1。创造有价值数据的人应该因使用这些数据而获得某种补偿💸 剩下的问题是实现它的经济模型。一个很好的类比是音乐从 CD 到 Napster 再到 Spotify 的转变。或者从电影到 YouTube 视频的转变。未来数据是否存在类似的经济模型？ ⏳ 40:16 -- 2. 比尔·盖茨无法想象我们有一天会在计算机中需要千兆字节的内存💾 同样，我们可以&#39;今天，我们无法想象法学硕士如何或为何需要数十亿的上下文长度，但这仍然可能发生。 （对于上下文：具有 10 亿上下文长度的法学硕士意味着它可以处理和理解每个查询的约 200 万个文档页） ⏳ 51:13 --  “我想赋予 ChatGPT 保留记忆的能力”📝  想象一下，一个模型会逐渐了解您并随着时间的推移对您变得更加有用。这很可能是上面强调的十亿上下文长度 LLM 的一个用例。 ⏳ 55:33 --  计算将成为未来的货币。 💲  Sam 相信它将成为世界上最珍贵的商品。 ⏳ 1:09:55 --  核聚变将解决“能源问题”⚛  由于未来世界需要大量的计算，我们将需要大量的能源来为一切提供动力。 Sam 相信核聚变是解决这个问题的最佳方法。 ⏳ 1:11:29 --  Q- star 可能存在（但我们不会谈论这个）⭐  Lex 当然询问了 Q-star，但 Sam 并没有否认它的存在- 只是说“我们还没有准备好谈论这个”。 ⏳ 1:02:36 --  程序员不会过时👩🏻‍💻  但它可能会与现在的编程方式有所不同。不管怎样，萨姆认为没有人真正进行纯粹的编码——因为大多数程序员使用预先存在的软件包/技术/软件。利用 LLM 协助编码的方式与此类似。 ⏳ 1:29:50 --  超越 Google很无聊🔍  OpenAI 不想做一个更好的搜索引擎；以这种方式思考低估了他们在人工智能方面的工作。 ⏳ 1:17:37 --  “ ChatGPT 中不会有广告！” （最好）🚩  Sam 对广告有偏见 - 这就是为什么目前 ChatGPT 的商业模式是通过付费进行的。在某种程度上，我觉得这令人放心 - 因为当你引入广告时，你的“真正的客户”现在就变成了广告商，而不是实际的用户（现在变成了产品）。 ⏳ 1:20： 15 --  我们不再谈论 AGI（让我们称之为别的东西吧）🧠 &lt; /ol&gt; 人们对于 AGI 是什么有不同的定义，因此 Sam 主张更多地谈论具体的功能，而不是把 AGI 作为一个通用术语。不过，根据他的定义，AGI 是一个无需人类干预即可推进科学发现的系统。 ⏳ 1:32:33    ;由   提交 /u/SwimIndependent6688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</guid>
      <pubDate>Sat, 23 Mar 2024 09:15:03 GMT</pubDate>
    </item>
    <item>
      <title>[R]学习最大公约数：解释变压器预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blk4g1/r_learning_the_greatest_common_divisor_explaining/</link>
      <description><![CDATA[ 由   提交/u/Chaoses_Ib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blk4g1/r_learning_the_greatest_common_divisor_explaining/</guid>
      <pubDate>Sat, 23 Mar 2024 04:55:33 GMT</pubDate>
    </item>
    <item>
      <title>[N] Stability AI 创始人 Emad Mostaque 计划辞去首席执行官职务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</link>
      <description><![CDATA[https://www.forbes.com/sites/kenrickcai/2024/03/22/stability-ai-Founder-emad-mostaque- plan-to-resign-as-ceo-sources-say/ 官方公告：https: //stability.ai/news/stabilityai-announcement 无付费专区，福布斯：  尽管如此，莫斯塔克还是向公众展现了勇敢的一面。 “我们的目标是今年实现正现金流，”他二月份在 Reddit 上写道。据一位知情人士透露，即使在会议上，他也将计划中的辞职描述为一次成功使命的顶峰。  首先是 Inflection AI，现在是 Stability AI？你有什么想法？   由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</guid>
      <pubDate>Sat, 23 Mar 2024 03:49:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 MLLM 愿景的讨论？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blirbx/d_discussion_about_mllm_vision/</link>
      <description><![CDATA[您认为为什么像 gpt-v 这样的 MLLM 缺乏描述图像中对象之间的空间关系的能力，就像它可以看到和描述对象一样，但无法关联到物体的确切位置？有什么办法可以克服这个问题吗？   由   提交 /u/Moodrammer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blirbx/d_discussion_about_mllm_vision/</guid>
      <pubDate>Sat, 23 Mar 2024 03:39:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 开放式居住</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blimk2/d_openai_residency_oa/</link>
      <description><![CDATA[刚刚获得 OpenAI 人工智能驻场黑客等级。总共 2 小时 15 分，有 3 道简答数学题和 1 道编码题。 它说数学涉及线性代数、概率和统计。我期待中等硬度的 LC 等效编码。不太确定如何准备。我的朋友建议我看看量化准备书籍/指南，例如 Jane Street 博客，因为这似乎是一个数学繁重的面试。 以前做过住院医师 OA 的人吗？同时也希望听取曾在 OpenAI 或其他 AI 实习机构（例如 Uber/Meta 等）面试过的人的意见。   由   提交 /u/Careless-Cow-5683   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blimk2/d_openai_residency_oa/</guid>
      <pubDate>Sat, 23 Mar 2024 03:32:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] 为文本语料库提供主题相似度排序的最佳方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blhdq0/p_best_way_to_give_corpuses_of_text_a_topical/</link>
      <description><![CDATA[我有很多文本段落。给定一个段落，我想仅根据主题返回 N 个最相似的段落。这些段落也是矢量化的，但是当我在矢量数据库中进行相似性搜索时，我感觉结果并不理想。我纯粹希望它根据“锻炼”、“跑步”、“编码”、“悲伤”等实质性主题进行比较。理想情况下，随着数据集的增长，粒度会越来越细。 我的一个想法是让法学硕士提取大约 10 个最实质性的主题，然后通过词网之类的东西计算相似度。也许这种方法更能实现我的目标。 也许我没有给予余弦相似度向量搜索足够的信任，但我想了解您对此事的一些想法！ &lt; /div&gt;  由   提交/u/Calm_Ad_343   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blhdq0/p_best_way_to_give_corpuses_of_text_a_topical/</guid>
      <pubDate>Sat, 23 Mar 2024 02:29:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 进行机器学习面试后感到精疲力尽</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</link>
      <description><![CDATA[在过去的两个月里，我一直在面试机器学习工程师和相关职位，从大型科技公司到小型初创公司。采访的风格多种多样，而且似乎无处不在。即使面试了10家不同的公司，后来又面试了30多次，我都没有成功。我要么被他们迷住了，要么被拒绝了。 我接受过的一些面试是：  Leetcode 风格的编码问题。 从头开始实现 SVM 等机器学习算法或反向传播或卷积等算法的某些组件。 深入了解与编程语言相关的问题，例如有关 Python GIL 或 C++ 指针的问题。 与 OOP 相关的理论和实现问题。 典型的 SWE 风格系统设计面试，例如设计 Instagram 机器学习系统设计面试，例如设计推荐系统。 机器学习理论问题，例如什么是铰链损失或解释逻辑回归或何时可以使用 KL 散度。 深度学习理论问题，例如 SGD 和 Adam 之间的区别是什么、神经网络中的量化是什么、如何量化你能加快深度学习模型的推理速度吗？ 计算机视觉理论问题，例如 YOLO 和 FasterRCNN 之间有什么区别、什么损失函数可用于图像分割或解释对极几何。 自然语言处理理论问题，例如 Transformer 为何比 RNN 更好、BERT 中的双向性是什么，或者词干提取和词形还原之间有什么区别。 之前的工作、之前的研究论文、之前的项目相关问题. 带回家的作业也无处不在，从构建基于时间序列的模型到部署分类模型作为与公司面临的相关问题的端点。 与工具相关的问题，例如 Docker、Kubernetes、AWS 等。 行为轮面试 数学、统计和基于概率的面试，例如贝叶斯定理或伯努利分布或矩阵的等级是什么或区分某些东西。  我确信我还缺少其他风格的采访。我的记忆力不太好，所以也许我容易忘记我所学的东西，因此觉得这些采访很困难。我想知道人们是如何准备这些采访的。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</guid>
      <pubDate>Sat, 23 Mar 2024 00:26:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 哪些令人尴尬的并行工作负载需要 GPU？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</link>
      <description><![CDATA[大家好， 我正在研究垂直 GPU 集群，并寻找一些可以运行的用例我正在构建的集群。我从法学硕士批量推理开始，但很想听听你的想法。唯一的要求是他们不使用机器间通信。    由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</guid>
      <pubDate>Fri, 22 Mar 2024 22:06:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有“完整”LLMOps 的资源或存储库吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bkyr87/d_any_resources_or_repos_of_complete_llmops/</link>
      <description><![CDATA[您好， 因此，我的团队希望探索在生产中应用法学硕士，但我们根本不知道从哪里开始堆栈的术语。您能否推荐一些资源，以便我能够为 LLMOps 的外观打下坚实的基础？谢谢！   由   提交 /u/TheCockatoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bkyr87/d_any_resources_or_repos_of_complete_llmops/</guid>
      <pubDate>Fri, 22 Mar 2024 13:04:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何用更少的 GPU 内存训练神经网络：可逆残差网络回顾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bkv29x/r_how_to_train_a_neural_network_with_less_gpu/</link>
      <description><![CDATA[探索可逆残差网络的有趣方法。 OpenCV.ai 团队的新文章回顾了一种减少 GPU 内存需求的方法在神经网络训练期间。您将发现可逆残差网络在神经网络训练期间如何节省 GPU 内存。该技术在“可逆残差网络：无需存储激活的反向传播”中详细描述。通过不存储反向传播的激活，可以有效地训练更大的模型。了解其在降低硬件要求方面的应用，同时保持 CIFAR 和 ImageNet 分类等任务的准确性。   由   提交/u/Human_Statistician48   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bkv29x/r_how_to_train_a_neural_network_with_less_gpu/</guid>
      <pubDate>Fri, 22 Mar 2024 09:21:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>