<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 10 Mar 2024 12:20:59 GMT</lastBuildDate>
    <item>
      <title>[P]用LangChain教LLM像你一样写作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb8jqk/p_using_langchain_to_teach_an_llm_to_write_like/</link>
      <description><![CDATA[       由   提交/u/phicreative1997  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb8jqk/p_using_langchain_to_teach_an_llm_to_write_like/</guid>
      <pubDate>Sun, 10 Mar 2024 11:48:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 进入未知：自学习大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb8bxo/r_into_the_unknown_selflearning_large_language/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09147 代码：https://github.com/teddy-f-47/self-learning-llm-public 摘要：  我们解决自学LLM的主要问题：学什么的问题。我们提出了一个自学法学硕士框架，使法学硕士能够通过自我评估自己的幻觉来独立学习以前未知的知识。利用幻觉评分，我们引入了未知点（PiUs）的新概念，以及用于自动 PiU 识别的一种外在方法和三种内在方法。它有助于创建一个自学循环，专门关注“未知点”中的知识差距，从而降低幻觉分数。我们还制定了衡量法学硕士自学能力的评估指标。我们的实验表明，经过微调或对齐的 7B-Mistral 模型具有相当好的自学习能力。我们的自学理念使法学硕士更新更加高效，并为知识交流开辟了新的视角。它还可能增加公众对人工智能的信任。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb8bxo/r_into_the_unknown_selflearning_large_language/</guid>
      <pubDate>Sun, 10 Mar 2024 11:35:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人类课程效应随着神经网络中的情境学习而出现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb8at2/r_human_curriculum_effects_emerge_with_incontext/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.08674 摘要：  人类学习对规则式结构和课程很敏感用于训练的示例。在受简洁规则约束的任务中，当相关示例在试验中被阻止时，学习会更加稳健，但在没有此类规则的情况下，交错会更有效。迄今为止，还没有神经模型能够同时捕获这些看似矛盾的效应。在这里，我们表明，这种相同的权衡自发地出现在“上下文学习”中。 （ICL）既适用于元学习训练的神经网络，也适用于大型语言模型（LLM）。 ICL 是“在上下文中”学习新任务的能力。 - 没有重量变化 - 通过激活动力学中实现的内循环算法。预训练的 LLM 和元学习 Transformer 的实验表明，ICL 在涉及类规则结构的任务中表现出人类所表现出的阻塞优势，相反，并发权重学习再现了人类在缺乏此类结构的任务中观察到的交错优势。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb8at2/r_human_curriculum_effects_emerge_with_incontext/</guid>
      <pubDate>Sun, 10 Mar 2024 11:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要获取数据集方面的帮助 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb73ko/need_help_with_obtaining_a_dataset_please_d/</link>
      <description><![CDATA[大家好，我正在尝试从 NIST 获取 FRVT 1:N 数据集，但不知道如何获取它。它有一个可以提交结果的表单，但我找不到可以请求数据集的位置。如果我遗漏了什么，你们可以告诉我吗？我的项目/研究需要数据集。谢谢 这是官方链接：https://pages.nist.gov/frvt/html /frvt1N.html 这是我第一次使用这个 Reddit 子版块，如果我表现得像个菜鸟，那么抱歉  &amp;# 32；由   提交/u/roct07  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb73ko/need_help_with_obtaining_a_dataset_please_d/</guid>
      <pubDate>Sun, 10 Mar 2024 10:17:11 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 尝试生成用于注视预测的合成数据集，为什么模型难以学习数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb6z05/discussion_experimenting_with_generating_a/</link>
      <description><![CDATA[我已经设置了一个虚幻引擎 5 项目，该项目具有超人类视角，可以查看虚拟屏幕上的随机点，然后通过虚拟网络摄像头捕获照片我已经设置好了，然后保存。下面是一个大约 6.5K 图像的简单数据集（我说简单是因为我冻结了以前的很多变量，这里唯一改变图像的东西是照明、角色的位置、屏幕坐标和一些轻微的面部表情）运动我通过随机移动角色的嘴骨组合在一起）。 但出于某种原因，从头开始对数据进行训练的模型很难学习这些特征，并且训练和损失略有下降，但总体保持不变非常停滞... 这很奇怪，因为我已经（在某种程度上）成功地在我用网络摄像头制作的数据集上训练了注视预测模型，我试图用这些数据进行模仿。我成功地在具有 4K 和 6K 样本的相同模型架构上从头开始训练这些模型（最终有效 mse 损失约为 0.008（训练约为记忆中的 1/2），这仍然比我想要的要高，但是比合成模型好得多，合成模型通常在训练和有效方面都停滞在 0.08 左右）。 我尝试冻结除眼球运动和照明之外的所有变量，模型立即学习，训练损失降至 ~ 0.001。 我真的只是想了解可能导致问题的原因，对我来说非常奇怪的是，相同的模型具有完全相同的数据操作（使用一些 fastai 将大小调整为 (240, 320)图像增强）可以很好地学习具有更多变量的真实数据（例如，我在这里冻结的头部旋转，姿势变化，甚至衣服变化），但完全无法从这个更简单的数据集中学习：https://huggingface.co/datasets/goatman/meta human-gaze-prediction 抱歉这么久帖子，任何见解都会令人惊叹！   由   提交/u/Goatman117  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb6z05/discussion_experimenting_with_generating_a/</guid>
      <pubDate>Sun, 10 Mar 2024 10:08:51 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我使用人工智能代理来淡化新闻的耸人听闻</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb60g8/p_i_use_ai_agents_to_desensationalize_the_news/</link>
      <description><![CDATA[在当今世界，吸引人的标题和文章常常会分散读者对事实和相关信息的注意力。 Simply News 试图消除争论并提供有关实际发生情况的简单每日更新。通过协调多个人工智能代理，Simply News 处理耸人听闻的新闻文章，并将其转化为每天涵盖许多不同主题的有凝聚力的、以新闻为中心的播客。每个代理负责此过程的不同部分。例如，我们有执行以下功能的代理： 排序器：扫描大量新闻源，并根据与播客类别的相关性和重要性过滤文章。  投手：考虑到文章中呈现的叙述角度，为每篇排序的文章制作引人注目的投稿。 法官&lt; /strong&gt;：评估提案并就应涵盖哪些内容做出编辑决定。 脚本编写者：为所选文章起草引人入胜的脚本由法官决定，确保听力的清晰度和准确性。 我们的人工智能被引导选择与播客类别最相关的新闻文章。将人类从这个循环中移除意味着明确的偏见不会影响到报道内容的决定。 人工智能决策也更容易审计，这种透明度是人工智能能够成为强大的关键原因用于消除新闻中的偏见和耸人听闻的工具。 您可以在此处收听。 https://www.simplynews.ai/    ;由   提交 /u/sapientais   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb60g8/p_i_use_ai_agents_to_desensationalize_the_news/</guid>
      <pubDate>Sun, 10 Mar 2024 09:04:58 GMT</pubDate>
    </item>
    <item>
      <title>GAN 仍然有意义吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</link>
      <description><![CDATA[[D] 随着 Difussion 模型的不断兴起，我很好奇 GAN 是否会卷土重来？有什么想法吗？   由   提交 /u/Superb-Assignment-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</guid>
      <pubDate>Sun, 10 Mar 2024 06:00:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于 ML 的直觉与同行不同是否常见？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb2b9r/d_is_it_common_to_have_intuitions_different_from/</link>
      <description><![CDATA[我注意到我和我的同事对于 ML 的某些事情有不同的直觉。例如，以标记方式标记事物。 对于一个用例，我们想要检测距离摄像头最近的车辆。我认为我们应该在后处理步骤中处理这个问题，并让模型通过标记图像中所有可见的车辆来检测所有车辆，因为它们具有相似的特征。我的直觉是，仅标记距离摄像机最近的车辆会导致模型学习非常具体的特征，从而可能导致过度拟合，因为它会尝试区分距离摄像机较远和最近的车辆，尽管它们共享通用特征使它们成为车辆的特征。我的同事认为，只有最接近的车辆才应该被标记，因为这才是他们有兴趣检测的东西。 同样，我们在是否应该标记部分可见（&lt;30% 可见）的物体上也存在分歧。 。我认为它们应该是这样，因为如果你观察训练期间发生的增强，就会发现由于某些增强（例如缩放和马赛克）而不可避免地会出现部分可见的对象。因此，避免在原始图像中标记它们只会导致训练期间的不一致，其中一些部分对象被标记，而另一些则没有。他们不这么认为。 TLDR;对 ML 的直觉和理解存在差异是典型的吗？   由   提交 /u/notEVOLVED   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb2b9r/d_is_it_common_to_have_intuitions_different_from/</guid>
      <pubDate>Sun, 10 Mar 2024 05:12:16 GMT</pubDate>
    </item>
    <item>
      <title>[N] 现在有一种可行的方法来执行时间序列数据增强</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bawmp3/n_there_is_now_a_viable_way_to_perform_time/</link>
      <description><![CDATA[ 由   提交/u/anonymousTestPoster  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bawmp3/n_there_is_now_a_viable_way_to_perform_time/</guid>
      <pubDate>Sun, 10 Mar 2024 00:18:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 超越语言模型：字节模型是数字世界模拟器 - 微软亚洲研究院 2024 - bGPT - 模拟 CPU 行为的卓越能力，执行各种操作的准确度超过 99.99%！可以帮助解决标记化问题！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baqjkn/r_beyond_language_models_byte_models_are_digital/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.19155  带有代码和权重的纸页：https://byte-gpt.github.io/  摘要：  传统深度学习经常忽视字节，数字世界的基本单位，所有形式的信息和操作都以二进制格式进行编码和操作。受到自然语言处理中下一个标记预测成功的启发，我们引入了bGPT，这是一个具有下一个字节预测的模型来模拟数字世界。 bGPT 与各种模式（包括文本、音频和图像）的性能专业模型相匹配，并为预测、模拟和诊断算法或硬件行为提供了新的可能性。它几乎完美地复制了符号音乐数据的转换过程，在将ABC记谱转换为MIDI格式时实现了每字节0.0011位的低错误率。此外，bGPT 在模拟 CPU 行为方面表现出卓越的能力，在执行各种操作时准确率超过 99.99%。利用下一个字节预测，bGPT 等模型可以直接从大量二进制数据中学习，有效地模拟复杂的数据。数字世界的模式。   https:/ /preview.redd.it/u0h8rs651dnc1.jpg?width=1836&amp;format=pjpg&amp;auto=webp&amp;s=6f7ae48560280a7d15f7095bb8915b8db50ba9ef https://preview.redd.it/e2fnnt651dnc1.jpg?width=899&amp;format=pjpg&amp;auto=webp&amp; s =d637e2f08e70c7caa3d227dc9f8bd26ec5921360 来源：安德烈·卡帕蒂 https://youtu.be/zduSFxRajkE?si=Z3AFwwhth3j7raSv    由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baqjkn/r_beyond_language_models_byte_models_are_digital/</guid>
      <pubDate>Sat, 09 Mar 2024 19:56:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 法学硕士在预测神经科学实验结果方面超越人类专家（81% vs 63%）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_human_experts_in_predicting/</link>
      <description><![CDATA[一项新研究表明，法学硕士可以比人类专家更准确地预测哪些神经科学实验可能会产生积极的结果。研究人员使用只有 70 亿个参数的 GPT-3.5 类模型，发现根据神经科学文献对其进行微调可以进一步提高性能。 我认为实验设计很有趣。法学硕士收到了两个版本的摘要，其结果显着不同，我们被要求预测哪个更有可能是真正的摘要，本质上是预测哪个结果更有可能。它们比人类高出约 18%。 其他亮点：  对神经科学文献的微调提高了性能 模型的准确率比人类高出 81.4%。人类专家为 63.4% 在所有测试的神经科学子领域都成立 即使较小的 7B 参数模型也能与较大的模型相媲美 微调的“BrainGPT”模型的准确度比基准提高了 3%  其意义重大 - 人工智能可以帮助研究人员优先考虑最有希望的实验，加速科学发现并减少浪费的努力。它可能会在理解大脑和开发神经系统疾病治疗方面带来突破。 但是，该研究仅关注神经科学，测试集有限。需要更多的研究来看看这些发现是否可以推广到其他科学领域。虽然人工智能可以帮助识别有前途的实验，但它无法取代人类研究人员的创造力和批判性思维。  全文在此。我还写了 更详细的分析在这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_ human_experts_in_predicting/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_human_experts_in_predicting/</guid>
      <pubDate>Sat, 09 Mar 2024 19:37:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 中实现多头注意力的 5 种不同方式的速度比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</guid>
      <pubDate>Sat, 09 Mar 2024 15:12:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习 CUDA/C++ 有多大价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</link>
      <description><![CDATA[目前每个人都在努力使 AI 实现快速/高效（因为效率更高 -&gt; 计算上花费的资金更少）。 例如，Flash Attention 2是在CUDA中实现的。 Llama.cpp 是 C++ PyTorch 够用吗？或者在这个市场上学习 CUDA/C++ 是否有优势，特别是对于法学硕士？ 如果 CUDA 在某些情况下有用，那么这些情况是什么？   由   提交/u/joelthomas-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</guid>
      <pubDate>Sat, 09 Mar 2024 09:44:20 GMT</pubDate>
    </item>
    <item>
      <title>[N] 矩阵乘法突破可能带来更快、更高效的人工智能模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</link>
      <description><![CDATA[   “计算机科学家发现了一个报告称，通过消除以前未知的低效率，比以往更快地乘以大型矩阵的新方法 广达杂志。这最终可能会加速人工智能模型，例如ChatGPT，它严重依赖矩阵乘法来运行。据报道，最近两篇论文中提出的研究结果使矩阵乘法效率实现了十多年来的最大改进。 ...图形处理单元 (GPU) 擅长处理矩阵乘法任务，因为它们能够同时处理许多计算。他们将大型矩阵问题分解为更小的部分，并使用算法同时解决它们。完善该算法一直是过去一个世纪（甚至在计算机出现之前）矩阵乘法效率突破的关键。 2022 年 10 月，我们涵盖了Google DeepMind AI 模型发现的一项名为 AlphaTensor 的新技术，专注于针对特定矩阵大小（例如 4x4 矩阵）的实用算法改进。 相比之下， 新研究，由清华大学的段燃和周仁飞、加州大学伯克利分校的吴洪勋以及 Virginia Vassilevska 进行麻省理工学院的 Williams、Yinzhan Xu 和 Zixuan Xu（在第二篇论文中）寻求通过降低复杂性指数 ω 来实现理论增强，从而在所有大小的矩阵上获得广泛的效率增益。这项新技术并没有像 AlphaTensor 那样寻找直接、实用的解决方案，而是解决了基础性的改进，可以在更广泛的范围内提高矩阵乘法的效率。  ... 两个 n×n 矩阵相乘的传统方法需要 n3 次单独的乘法。然而，这项新技术改进了“激光方法”。由 Volker Strassen 于 1986 年提出，减小了指数的上限（表示为前面提到的 ω），使其更接近到理想值 2，这表示理论上所需的最小操作数。” ​ https://preview.redd.it/a49r1ajv59nc1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp; s =cf315793e6784ef9e62d48e00ebf0f3809070f6c https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/&lt; /strong&gt;   由   提交/u/Secure-Technology-78   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</guid>
      <pubDate>Sat, 09 Mar 2024 06:28:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>