<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Wed, 19 Feb 2025 09:18:03 GMT</lastBuildDate>
    <item>
      <title>[P]改进化学风险预测的机器学习模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1it0j30/p_improving_machine_learning_model_for_chemical/</link>
      <description><![CDATA[在-Project/Tree/Tree/配方风险 - 分析仪“&gt; shabnailmi/data-Science-group-project在食谱风险 - 雅分析仪上  我正在从事机器学习项目旨在预测化学组合的风险水平，特别是侧重于危险化学物质，爆炸性前体和有毒物质。我们的项目全面的化学风险预测模型（CCRPM）旨在帮助监管机构评估化学进口，购买和食谱的潜在危险。  我们已经使用 〜1000个化学食谱（合成）的数据集训练了我们的模型（低，中，高）。但是，在使用新数据测试模型时，我们面临准确性问题。一些关键问题包括：  该模型有时预测无害组合的高风险（例如，水 +水）。   特征工程挑战  - 有效地编码化学名称和数量。  数据集不平衡的数据集  - 大多数化学组合都是高风险，导致偏见。  处理新的/看不见的化学物质  - 如果输入了新的化学组合，该模型努力评估其风险。   🔹我们当前的方法：   模型：随机森林＆amp; XGBOOST（也测试了LSTM，但结果不是很好）。  预处理：一式编码化学名称，缩放量和特征选择。   🛠️我们尝试了什么：   smote用于平衡数据集（有所帮助但仍然需要改进）。  tf-idf＆amp;基于文本的化学名称的嵌入（不确定这是否是理想的）。 使用GridSearchCV （增量改进）进行超参数调整。 🔹我们需要以下帮助：   编码化学名称 + ML模型数量的最佳方法？   如何处理不在培训数据中的看不见的化学物质？   是否有适合这种类型的分类问题的更好的ML模型？   有任何提高概括和准确性的技术？   如果有人有使用化学安全数据集，基于NLP的ML模型或分类问题的经验 ，我们爱您的输入！任何帮助，建议或研究论文将不胜感激！ 🙏  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1it0j30/p_improving_machine_machine_learning_model_model_for_chemical/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1it0j30/p_improving_machine_learning_model_for_chemical/</guid>
      <pubDate>Wed, 19 Feb 2025 08:14:07 GMT</pubDate>
    </item>
    <item>
      <title>[d]在深度学习备忘单上应该找到哪些常见的实施技巧或陷阱？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iszjp1/d_what_are_the_common_implementation_tips_or/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在谈论事物的工程方面。假设您有一个想法要实现。由于深度学习仍然不是一门精确的科学学科，因此在试用和实施错误时，很容易开枪射击自己，并且错误地说服您的想法不值得。 ，因此从实施中开始透视图，某人在使用深度学习模型时绝对或不做什么？ 例如还可以随意发布链接到您在此上下文中真正有用的任何内容。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iszjp1/d_what_are_are_the_common_implementation_tips_or/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iszjp1/d_what_are_the_common_implementation_tips_or/</guid>
      <pubDate>Wed, 19 Feb 2025 07:04:18 GMT</pubDate>
    </item>
    <item>
      <title>[d]相同的培训代码给出了不同的输出</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isxkgf/d_same_training_code_gives_different_output/</link>
      <description><![CDATA[     我有一个当我尝试在Colab中运行时，一个文件冗长的代码开始给出    但是，当我将代码更改为分裂为文件的模块化类型时，输出就像    我手动检查了代码没有更改的每行代码。  我认为我给您的信息可能不足，但让我知道您是否需要一些其他信息。  proble围绕 &lt; &lt; ，只有一个人认为我给您的信息可能不足。 P&gt;第一个代码给出这样的输出   模块化代码给出     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/shrijayan     [link]    [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isxkgf/d_same_training_code_gives_different_output/</guid>
      <pubDate>Wed, 19 Feb 2025 05:02:20 GMT</pubDate>
    </item>
    <item>
      <title>[r]大语言模型中深度的诅咒：我们朝错误的方向扩展吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isumx1/r_the_curse_of_depth_in_large_language_models_are/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  &#39;depth的诅咒;论文强调了LLM缩放中的基本缺陷，超过一定深度，其他层几乎没有任何贡献。 ln）导致输出方差在深层中爆炸。 结果？深层失去有效的学习能力，实质上是身份功能。 这意味着我们正在训练比必要的更深的模型，浪费了对没有有意义提高性能的层。   如果这是真的，它从根本上挑战了LLM开发中的“较大是更好的”假设。 对模型缩放的影响 效率 如果深层的回报降低，则： 我们是否过度建造llms？  如果深层没有有意义的贡献，则模型像GPT-4一样，DeepSeek和Mistral可以显着优化而不会失去性能。 这与经验结果保持一致，表明修剪模型保持竞争力性能。  分层缩放尺度修复 - 一个简单的解决方案？  本文提出了分层缩放以控制梯度方差并提高训练效率。  &lt; li&gt;这可以防止更深的层变得统计。  我们是否应该扩大宽度而不是深度？贡献，然后也许缩放宽度（例如，专家的混合物）是更有效的方向。 变压器缩放定律可能需要修订以解释此瓶颈。  表明，当前的LLM可能在达到理论参数缩放限制之前很久就遇到了建筑效率低下的效率。 这对新兴行为＆amp;这意味着什么意味着什么。 ai对齐 这也引起了有关出现紧急特性的深刻问题。 如果深层在功能上是多余的，则：  实际上是智能的地方成型？如果早期和中期正在从事所有实际工作，那么出现可能是梯度稳定性的函数，而不仅仅是扩展。 为什么LLMS显示出意外的强化替代？某些中层层是否正在形成持久的结构，即使更深的层变得不活跃？  如果深模型只是在没有有意义的收益的情况下膨胀参数计数，那么AI ISN的未来，则&#39;t更大，更聪明。 更大的问题：我们是否朝错误的方向进行缩放？ 本文表明我们将深度缩放缩放为提高AI功能的默认方法。  如果深层未充分利用深层，我们是否应该优先考虑建筑精炼而不是原始规模？ 这对于有效的微调，修剪意味着什么策略和下一代变压器体系结构？ 这可以解释某些新兴行为，因为中层层扮演意想不到的角色？  “更大模型=更好模型”的想法;已经驾驶了AI多年。但是，如果本文坚持下去，我们可能正处于更深入的模型正在积极浪费资源的时刻。 最终思考：这改变了有关缩放的一切从根本上讲，我们已经过期要转变AI架构。  您怎么看？ AI的研究是否应该从深度扩展和专注于更好的结构化体系结构？ 这能否导致新模型胜过较少参数的新模型？   听到别人的想法，这是后级时代的开始吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://www.reddit.com/r/machinelearning/comments/1isumx1/r_the_curse_of_depth_in_in_large_langue_models_models_are/”&gt; [links]   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1isumx1/r_the_curse_curse_of_depth_in_in_in_large_langue_models_models_models_models_are/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isumx1/r_the_curse_of_depth_in_large_language_models_are/</guid>
      <pubDate>Wed, 19 Feb 2025 02:29:59 GMT</pubDate>
    </item>
    <item>
      <title>[r] LLMS中深度的诅咒：为什么深层效果降低？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isu1nn/r_the_curse_of_depth_in_llms_why_are_deep_layers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  最近的研究揭示了现代大语言模型中意外的问题，较深的层没有提高自己的体重。 最近的一篇论文＆quot&#39;大语模型中的深度诅咒“ &lt; /em&gt;”突出了一个关键问题：  -  LLM中的深层对学习的贡献明显少于早期。&lt; - 这些层中的许多层都可以在没有严重的性能损失的情况下进行修剪，从而提出了有关训练效率的问题。  - 罪魁祸首？前层归一化（PRE-LN）会导致输出方差在更深的层中爆炸，从而使它们的作用几乎像身份功能。  - 一个简单的修复？控制这一差异并提高训练效率的分层缩放。 这对LLM体系结构，培训效率和缩放定律具有重大影响。如果诸如Llama，Mistral和DeepSeek之类的模型中的一半层没有有效贡献，我们要处理多少计算浪费？&lt; /p&gt; 关键问题进行讨论：1️）我们应该重新思考）深层培训策略以提高效率吗？2️）这会影响到变压器体系结构中更深入=更好的假设吗？3️）本文的见解可以帮助您LLM压缩，微调或蒸馏技术？ 纸链接： arxiv preprint：25057955V1   p&gt; 让我们讨论一下 - 您对深度诅咒的想法是什么？  &lt;！ - sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://www.reddit.com/r/machinelearning/comments/1isu1nn/r_the_curse_curse_of_depth_in_in_in_in_llms_why_are_are_are_deep_layers/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1isu1nn/r_the_curse_curse_of_depth_in_in_in_in_in_llss_why_are_are_are_are_are_deep_deep_deep_layers/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isu1nn/r_the_curse_of_depth_in_llms_why_are_deep_layers/</guid>
      <pubDate>Wed, 19 Feb 2025 02:02:05 GMT</pubDate>
    </item>
    <item>
      <title>[D]自动驾驶汽车，机器学习实习，请学习指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1istv6m/d_autonomous_vehicle_machine_learning_internship/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，所以我下周进行了第二轮ML技术讨论采访，并以机器学习实习职位的款项（它适用于机器人，Comp SCI等的硕士学位学生。机器人手臂操纵的强化学习是一个经典的计算机视觉项目，用于视觉探光和实习，专注于机器人导航和感知（不是ML） 我非常了解我的项目，这很好。 但是，对于即将到来的采访，我会从几种资源中练习ML概念，并观看Turing在YouTube上的模拟访谈并理解这些答案。我还应该深入研究吗？由于它是一家自动驾驶公司，因此它将与Lidar和Cameras Ofcourse一起使用ML，因此在此方面有任何资源吗？ 也第三轮也是一次现场编码面试，也很害怕...我想尽可能多地leetcode？ 谢谢您的阅读！如果您还有其他建议可以给我  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/arboyxx     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1istv6m/d_autonomous_vehicle_machine_learning_internship/</guid>
      <pubDate>Wed, 19 Feb 2025 01:54:00 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于DDPM的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isn9au/d_question_about_ddpm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图将我的大脑缠绕在我所读的东西上，但是很难做到。 为简单起见，让我们来想象一下，将DDPM模型进行了参数化，使其直接输出估计的干净图像。例如，x（x  t，t）= hat {x} _t。现在，想象一下我们的X（）网络是最佳的。给定DDPM目标，这意味着输出将为E [X_0 | X_T]。我试图了解具有此完美的Denoiser如何使参数化的反向后p（x  {t-1} | x  t）等于真实的反向后p（x  {t-） 1} | x_0，x_t）。我一直在试图得出这种平等，但我似乎无法弄清楚。我看到很多论文提出了主张，但没有人解释过。这很简单，我很愚蠢？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1isn9au/d_question_about_ddpm/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isn9au/d_question_about_ddpm/</guid>
      <pubDate>Tue, 18 Feb 2025 20:49:03 GMT</pubDate>
    </item>
    <item>
      <title>[D]用于培训基础模型的游戏引擎</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isn1oz/d_game_engines_for_training_foundational_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我认为对游戏发动机的模拟训练AI对于解锁下一个智能级别将非常重要。原因如下：  视频中可用的数据比Internet文本中的可用数据多得多。  AI需要了解物理学 - 比可重复的，无限的 - 触发产生更好的方法游戏环境 当然，它们不会准确地建模物理，但是您可以想象一个基础模型首先接受了80％模拟轨迹训练（因为样品价格便宜），而20％真实轨迹。  因此，我正在考虑ho积团结库存来骑这一波。我能想到的一些对立面    统一股票由于其他原因而波动，例如：不良管理。    AI公司制造自己的AI模拟引擎，以更准确地反映现实世界的物理学 - ＆GT; Unity认为没有上升空间。   每个人都想到什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/complect-media-8074      [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isn1oz/d_game_engines_for_training_foundational_models/</guid>
      <pubDate>Tue, 18 Feb 2025 20:40:18 GMT</pubDate>
    </item>
    <item>
      <title>[d] [p]图像/txt-to-json模型建议。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isk800/dp_imagetxttojson_model_recommendation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我需要一些建议我构建的项目的建议，该项目使用AI从屏幕截图或文本字符串中推断出交易。目前，我正在使用两种型号：    vision_model ：llama3.2-vision：11b-Instruct-q4_k_m     text_model ：llama3.2：3b-instruct-q6_k   这些模型是通过我的桌面上的ollama api托管的GTX 2080 Super GPU（8GB VRAM）。但是，我想最终将Ollama搬到我的Intel Nuc，这没有GPU。我也很高兴听到有关CPU兼容模型的建议吗？    我的问题面向：   日期精度：偶尔会误解交易日期。   交易检测：处理时：具有多个交易的屏幕截图（7-8），这些模型通常仅检测1-3个交易，无论是从文本还是图像中。  我正在寻找的东西for：   模型建议：在图像到json或文本到json任务中出色的模型的建议，特别是用于准确提取交易详细信息。  优化提示：有关优化模型以有效运行仅在CPU的设置上的建议。   替代方法：任何其他方法或可以提高我应用程序中交易检测准确性和可靠性的工具。  我感谢您提供的任何见解或建议！ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/conte95     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isk800/dp_imagetxttojson_model_recommendation/</guid>
      <pubDate>Tue, 18 Feb 2025 18:48:31 GMT</pubDate>
    </item>
    <item>
      <title>[r]大语言模型中深度的诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isdopn/r_the_curse_of_depth_in_large_language_models/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isdopn/r_the_curse_of_depth_in_large_language_models/</guid>
      <pubDate>Tue, 18 Feb 2025 14:18:32 GMT</pubDate>
    </item>
    <item>
      <title>[R]评估现实世界软件工程任务的LLM：一项耗资100美元的基准研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1isbo6t/r_evaluating_llms_on_realworld_software/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  旨在评估现实世界软件工程任务上LLM的新基准测试，直接从附上的“实际美元”值中拉出。该方法涉及收集1,400多个任务，从$ 50- $ 32,000的支出，创建标准化的评估环境以及测试编码能力和工程管理决策。 关键技术要点： - 通过单位测试，专家测试验证任务验证和与人类解决方案的比较 - 评估使用Docker容器来确保测试环境 - 包括直接编码任务和高级工程管理决策 - 任务涵盖Web开发，移动应用程序，数据处理和系统体系结构 - 总任务价值超过100万美元的实际自由付款 我认为，此基准是我们评估LLM的重要转变世界应用。通过将绩效直接与经济价值联系起来，我们可以更好地了解当前能力和实用程序之间的差距。较低的成功率表明，在LLM可以可靠地处理专业软件工程任务之前，我们需要取得重大进展。 我认为，包括管理级别的决策尤其有价值，因为它可以测试技术理解和战略思维。这可以有助于指导开发更完整的工程辅助系统。  tldr：新的基准测试在实际$ 1M+的UPWORK编程任务上进行LLMS。当前模型在很大程度上挣扎，仅完成约10％的编码任务和约20％的管理决策。  完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1isbo6t/r_evaluating_llms_on_realworld_software/</guid>
      <pubDate>Tue, 18 Feb 2025 12:35:00 GMT</pubDate>
    </item>
    <item>
      <title>[r]本地稀疏注意：硬件一致且本地可训练的稀疏注意力（由Liang Wenfeng提交 -  DeepSeek）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1is9ufs/r_native_sparse_attention_hardwarealigned_and/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  本机稀疏注意：硬件与硬件和本地训练的稀疏注意 jingyang yuan ，Huazuo Gao，Damai Dai，Junyu Luo，Liang Zhao，Zhengyan Zhang，Zhenda Xie，Y。X。 Wang，Zhiping Xiao，Yuqing Wang，Chong Ruan，Ming Zhang，Wenfeng Liang，Wangding Zeng  长篇小说建模对于下一代语言模型至关重要计算挑战。稀疏的注意力为提高效率的方向提供了有希望的方向，同时保持模型功能。我们提出了NSA，这是一种本地可训练的稀疏注意机制，将算法创新与硬件一致的优化相结合，以实现有效的长篇文化建模。 NSA采用了动态的分层稀疏策略，将粗粒的令牌压缩与精细的令牌选择相结合，以保持全球环境意识和局部精度。我们的方法通过两个关键创新进行了稀疏注意设计：（1）我们通过算术强度平衡算法设计实现了实质性的加速，并对现代硬件进行了优化。 （2）我们启用端到端培训，在不牺牲模型性能的情况下减少预处理的计算。如图1所示，实验表明，使用NSA预测的模型维持或超过了一般基准，长篇下说任务和基于指导的推理的全部注意力模型。同时，NSA在对解码，正向传播和向后传播的64k长度序列上的全面关注方面实现了实质性加速，从而在整个模型生命周期中验证了其效率。&lt; /em&gt;  arxiv：2502.11089 [cs.cl]：&lt; /em&gt; 一个href =“ https://arxiv.org/abs/2502.11089”&gt; https://arxiv.org/abs/2502.11089       &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nunki08     [link]        [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1is9ufs/r_native_sparse_attention_hardwarealigned_and/</guid>
      <pubDate>Tue, 18 Feb 2025 10:39:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] Finetuning Modernbert正在服用3小时（2个时代）和35 gigs的VRAM。正常吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1is0q1a/d_finetuning_modernbert_is_taking_3hrs_2_epochs/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以其他详细信息... 我正在使用a6000 48gb vram，8vcpu，8vcpu，45 gb ram。我的数据集是NewsArticle文本和标签的9K样本。 我正在使用的模型是“ answerdotai/sodernbert-base”。上下文长度为8192。 最初，当我尝试使用32或16的批处理进行捕获时，我一直在遇到OOM错误。然后，我看到了设置4个或更少的批处理。唯一的训练开始。即使训练一个时代也要花费1H 31分钟。这是正常的吗？这是我第一次迎接模型参考或过去的经验。当我将批处理大小设置为32或16时，我没想到会看到一个45MB CSV文件会填充整个VRAM。是Pytorch错误还是??? &lt; /p&gt; 编辑 - IM使用的数据集是“ valurank/politainbias_allsides_txt”的截断版本。大约有19k数据样本。我正在使用其中的一个子集 - 大约9K样品。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solaris12     &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1is0q1a/d_finetuning_modernning_modernbert_is_taking_3hrs_2_2_epochs/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1is0q1a/d_finetuning_modernbert_is_taking_3hrs_2_epochs/</guid>
      <pubDate>Tue, 18 Feb 2025 01:22:30 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客等对于产品和服务。 请不要发布链接缩短器，链接聚合器网站或自动订阅链接。    任何滥用信托的滥用都会领导禁止。 鼓励其他人创建新帖子，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。   元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们通过不垃圾邮件主题来促进自己的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iqiy4x/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 16 Feb 2025 03:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1ilhw29/d_simple_questions_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    </channel>
</rss>