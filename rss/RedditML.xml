<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 02 May 2024 06:18:50 GMT</lastBuildDate>
    <item>
      <title>[D] 最适合的会议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/</link>
      <description><![CDATA[我的 icml 提交被拒绝，分数为 6655，我的心碎了，有哪些高接受率的会议我可以重新提交它？我只是想把它放进去然后继续前进。    由   提交/u/One-Blueberry4699   /u/One-Blueberry4699 reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/</guid>
      <pubDate>Thu, 02 May 2024 06:05:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要示例训练 Pytorch 项目进行测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci7ioc/d_need_sample_training_pytorch_projects_for/</link>
      <description><![CDATA[我想执行一些训练 Pytorch 项目来测试我们在 GPU 上运行的技术堆栈。到目前为止，我一直在运行 Huggingface 的模型，进展顺利。但是，现在我需要执行培训的项目。这意味着具有数据集和训练代码的项目。我不是一名机器学习工程师，所以我无法从头开始创建一个，但我可以理解代码并进行一些小的更改以使其运行。如果有使用数据训练 LLM 或 BERT 的公共项目，您有什么建议吗？这些对于我的测试来说非常有用。我的测试是在 80GB 的 H100 GPU 上进行的，因此我可以运行大型模型训练。   由   提交/u/Chachachaudhary123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci7ioc/d_need_sample_training_pytorch_projects_for/</guid>
      <pubDate>Thu, 02 May 2024 05:12:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 商业环境中聊天机器人管道的现状？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci5x4l/d_current_state_of_chatbot_pipelines_in/</link>
      <description><![CDATA[大家好，我目前的任务是研究管道，为我的大学构建本地自定义聊天机器人。我一直在阅读 RAG、Rasa、Dialogflow 等方法以及 LangChain、Ragflow、KRAGEN 等特定管道，并获得了一些测试结果。然而，我想了解哪些管道和方法对于构建聊天机器人最有效，特别是在商业环境中。我非常感谢您提供的所有信息！   由   提交 /u/ghosthunterk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci5x4l/d_current_state_of_chatbot_pipelines_in/</guid>
      <pubDate>Thu, 02 May 2024 03:41:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 免训练图神经网络和标签作为特征的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci1qqr/r_trainingfree_graph_neural_networks_and_the/</link>
      <description><![CDATA[ 由   提交/u/joisino  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci1qqr/r_trainingfree_graph_neural_networks_and_the/</guid>
      <pubDate>Thu, 02 May 2024 00:14:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] ollama 安装模型路径</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chzhb7/r_ollama_installed_models_path/</link>
      <description><![CDATA[你好， 我一直在尝试找到我用 ollama 安装的模型的确切路径，但没有。正如您在下面的代码中看到的那样，这似乎是常见问题解答中所说的。我试图对其进行多语言测试，发现该模型是不可能的。你能给我看一下吗？ luispoveda93@LUIS-PC:/usr/share/ollama/.ollama/models$ ls -la 总计 16 drwxr-xr-x 4 ollama ollama 4096 5 月 1 日 00:03 。 drwxr-xr-x 3 ollama ollama 4096 Apr 30 23:56 .. drwxr-xr-x 2 ollama ollama 4096 May 1 00:03 blobs drwxr-xr-x 3 ollama ollama 4096 May 1 00:03 清单 luispoveda93@LUIS- PC：/usr/share/ollama/.ollama/models$ ollama list NAME ID SIZE MODIFIED milstra:latest 61e88e884507 4.1 GB 43 分钟前 nomic-embed-text:latest 0a109f422b47 274 MB 24 小时前    由   提交/u/povedaaqui  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chzhb7/r_ollama_installed_models_path/</guid>
      <pubDate>Wed, 01 May 2024 22:35:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] Pytorch 的现代最佳编码实践（用于研究）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/</link>
      <description><![CDATA[大家好，我从 2019 年开始使用 Pytorch，在那段时间它发生了很大的变化（尤其是自从 Huggingface 以来）。  您有推荐的现代指南/style-docs/example-repos 吗？例如，命名张量是一种好的/常见的做法吗？推荐使用 Pytorch Lightning 吗？目前最好的配置管理工具是什么？您多久使用一次 torch.script 或 torch.compile？   由   提交 /u/SirBlobfish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/</guid>
      <pubDate>Wed, 01 May 2024 21:24:42 GMT</pubDate>
    </item>
    <item>
      <title>[D]解决预测与目标值中残留模式的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chvykz/dmethod_to_address_residual_patterns_in_forecast/</link>
      <description><![CDATA[      大家好，我有一个问题，当预测之间存在残差时，如何提高 XGBoost 模型的性能值和实际目标值显示出一种模式。在我上传的图表中，每个点代表实际与预测的对。我还添加了一条对角线，表示预测值和实际值之间的完美匹配。  我可以探索哪些策略来减少这种残差模式并提高预测与实际值之间的一致性？  我应该考虑更改模型类型，还是是否有其他方法（例如特征工程或统计调整）可以解决此问题？ 有关模型诊断或评估技术的任何建议可能有助于进一步完善我的模型吗？  https://preview.redd.it/shj4g7m9hvxc1.png ?width=846&amp;format=png&amp;auto=webp&amp;s=b8e658c9ee7b8ce7612e3cd1f199a1682e056dca   由   提交 /u/tipoviento   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chvykz/dmethod_to_address_residual_patterns_in_forecast/</guid>
      <pubDate>Wed, 01 May 2024 20:12:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 模型崩溃不可避免吗？通过积累真实数据和合成数据来打破递归魔咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chu565/r_is_model_collapse_inevitable_breaking_the_curse/</link>
      <description><![CDATA[ 由   提交/u/RSchaeffer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chu565/r_is_model_collapse_inevitable_breaking_the_curse/</guid>
      <pubDate>Wed, 01 May 2024 18:58:20 GMT</pubDate>
    </item>
    <item>
      <title>[P]我转载了Anthropic最近的可解释性研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/</link>
      <description><![CDATA[当能力研究发展得像目前一样快时，并没有多少人关注 LLM 可解释性研究，但可解释性确实很重要，在我看来意见，确实有趣又令人兴奋！近几个月来，Anthropic 取得了很多突破，其中最大的突破是“迈向单义性”。基本思想是，他们找到了一种训练稀疏自动编码器以基于变压器激活生成可解释特征的方法。这使我们能够在推理过程中查看语言模型的激活，并了解模型的哪些部分最负责预测每个下一个标记。对我来说真正突出的一点是，他们训练的自动编码器实际上非常小，并且不需要大量计算即可工作。这让我产生了尝试通过在我的 M3 Macbook 上训练模型来复制该研究的想法。经过大量阅读和实验，我得到了相当不错的结果！我在我的博客上写了一篇更深入的文章：  https://jakeward.substack.com/p/monosemanticity-at-home-my-attempt 我现在也在使用这项技术开展一些后续项目作为可以在 Colab 笔记本中运行以使其更易于访问的最小实现。如果您阅读我的博客，我很乐意听到任何反馈！   由   提交 /u/neverboosh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/</guid>
      <pubDate>Wed, 01 May 2024 17:51:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] KAN：柯尔莫哥洛夫-阿诺德网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2404.19756 代码：https://github.com/KindXiaoming/pykan 快速介绍：https://kindxiaoming.github.io/pykan/intro.html 文档：https://kindxiaoming.github.io/pykan/ 摘要：  受 Kolmogorov-Arnold 表示定理的启发，我们提出Kolmogorov-Arnold 网络 (KAN) 作为多层感知器 (MLP) 的有前途的替代方案。虽然 MLP 在节点（“神经元”）上具有固定的激活函数，但 KAN 在边（“权重”）上具有可学习的激活函数。KAN 根本没有线性权重 - 每个权重参数都被参数化为样条函数的单变量函数取代。我们表明，这种看似简单的变化使 KAN 在准确性和可解释性方面优于 MLP。就准确性而言，在数据拟合和 PDE 求解方面，小得多的 KAN 可以实现与大得多的 MLP 相当或更好的准确性。从理论上和经验上讲，KAN 拥有比 MLP 更快的神经缩放定律。就可解释性而言，KAN 可以直观地可视化，并且可以轻松地与人类用户交互。通过数学和物理学中的两个例子，KAN 被证明是帮助科学家（重新）发现数学和物理定律的有用合作者。总之，KAN 是 MLP 的有前途的替代品，为进一步改进当今严重依赖 MLP 的深度学习模型提供了机会。  https://preview.redd.it/r7vjmp31juxc1.png?width=2326&amp;format=png&amp;auto=webp&amp;s=a2c722cf733510194659b9aaec24269a7f9e5d47   由    /u/SeawaterFlows  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/</guid>
      <pubDate>Wed, 01 May 2024 17:03:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找最近的研究/论文/文章，该研究表明具有与 ViT 相似数量参数的替代模型表现同样好，表明特定模型没有什么特别之处。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chr750/d_looking_for_a_recent_studypaperarticle_that/</link>
      <description><![CDATA[标题基本上，这是我最近读到的对话，现在正在寻找来源。其中还提到了一篇具体论文。得出的结论是，我们可能已经达到了统计模型所能做的极限，并且模型本身没有什么特别的——只有输入的数据很重要。任何指示将不胜感激，谢谢！    由   提交 /u/SunraysInTheStorm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chr750/d_looking_for_a_recent_studypaperarticle_that/</guid>
      <pubDate>Wed, 01 May 2024 17:00:04 GMT</pubDate>
    </item>
    <item>
      <title>[2404.10667] VASA-1：实时生成逼真的音频驱动的说话面孔</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chpv6d/240410667_vasa1_lifelike_audiodriven_talking/</link>
      <description><![CDATA[ 由   提交 /u/shadowylurking   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chpv6d/240410667_vasa1_lifelike_audiodriven_talking/</guid>
      <pubDate>Wed, 01 May 2024 16:05:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] TensorDock — GPU 云市场，H100 每小时 2.49 美元起</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/</link>
      <description><![CDATA[大家好！我是来自 TensorDock 的 Jonathan，我们正在构建一个云 GPU 市场。我们希望让 GPU 真正变得价格实惠且易于使用。 我曾经在中学时在自托管服务器上启动了网络托管服务。但构建服务器与销售云不同。有很多开源软件可以管理你的家庭实验室的副业项目，但没有任何东西可以将其商业化。 大型云提供商收取高得离谱的价格 - 如此之高以至于他们经常可以偿还他们的硬件在 6 个月内，全天候 24 小时使用。 我们正在构建允许任何人成为云的软件。我们希望达到这样一个目标：任何[插入容量过剩的公司、数据中心、云提供商]都可以在我们的节点上安装我们的软件并赚钱。他们可能不会在 6 个月内偿还硬件费用，但他们不需要做繁重的工作 - 我们处理支持、软件、付款等。 反过来，您可以访问真正独立的云：来自世界各地的供应商提供的 GPU，这些供应商在价格和可靠性方面相互竞争。 到目前为止，我们已经采用了相当多的 GPU，其中包括200 个 NVIDIA H100 SXM，售价仅为 2.49 美元/小时。但我们还有 A100 80G 起价为 1.63 美元/小时，A6000 起价为 0.47 美元/小时，A4000 起价为 0.13 美元/小时，等等。因为我们是一个真正的市场，所以价格会随着供需而波动。 所有这些都可以在纯 Ubuntu 22.04 中使用，或者预装流行的 ML 软件包 - CUDA、PyTorch、TensorFlow 等，并且所有这些都由托管我们已经仔细审查过的矿场、数据中心或企业网络。 如果您正在为下一个项目寻找托管服务，请尝试一下！很高兴提供测试积分，请发送电子邮件至 [jonathan@tensordock.com](mailto:jonathan@tensordock.com）。如果您最终决定尝试我们，请在下面提供反馈[或直接提供！]:) ​ 部署 GPU 虚拟机：https://dashboard.tensordock.com/deploy 仅 CPU 虚拟机：https://dashboard.tensordock.com/deploy_cpu 申请成为主机： https://tensordock.com/host   由   提交/u/jonathan-lei   reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/</guid>
      <pubDate>Wed, 01 May 2024 10:31:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 2024 决策主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chfqca/d_icml_2024_decision_thread/</link>
      <description><![CDATA[ICML 2024 论文录取结果应该在 24 小时左右发布。我想我可以创建这个主题让我们讨论与之相关的任何事情。 每年的评论中都会有一些噪音。别忘了，即使你的论文可能会被拒绝，但这并不意味着它不是有价值的工作。祝大家好运！    提交人    /u/hugotothechillz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chfqca/d_icml_2024_decision_thread/</guid>
      <pubDate>Wed, 01 May 2024 07:01:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>