<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 07 Jun 2024 01:05:21 GMT</lastBuildDate>
    <item>
      <title>[R] NLP 与 LLM；有哪些主题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9y0wu/r_nlp_with_llms_what_topics/</link>
      <description><![CDATA[我们正在开发一门课程，向小型技术社区的学生教授 NLP 和 LLM。当今市场上哪些主题最相关？我想知道专家目前在这个领域强调什么。    提交人    /u/xyz__999   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9y0wu/r_nlp_with_llms_what_topics/</guid>
      <pubDate>Fri, 07 Jun 2024 00:47:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] PyTorch Vs....为什么仍然是 Tensorflow？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9w79p/d_pytorch_vs_why_still_tensorflow/</link>
      <description><![CDATA[经过长时间的中断，我重新回到了机器学习领域。在与朋友交谈并做了一些研究（例如，2024 年 Tensorflow 与 PyTorch 的快速投票）后，我觉得 TensorFlow 可能不是恢复速度的最佳库。 现在，我对这篇文章的问题是：如果 TensorFlow 已经失宠到这种程度，人们建议不要使用它，那么为什么谷歌搜索“PyTorch vs.”仍然会带来大量将 PyTorch 与 TensorFlow 进行比较的文章和网站吗？ 在设置 PyTorch 环境之前，我应该考虑没有像样的 PyTorch 竞争者吗？ 期待您的见解！    提交人    /u/Tolure   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9w79p/d_pytorch_vs_why_still_tensorflow/</guid>
      <pubDate>Thu, 06 Jun 2024 23:19:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 BERT 模型检测有意混淆的文本中的毒性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9rcnp/d_using_bert_models_to_detect_toxicity_in_texts/</link>
      <description><![CDATA[大家好， 我一直在使用 BERT 模型来检测文本中的毒性，我遇到了一个有趣的困境，我想与大家讨论一下。 困境：当用户故意以掩盖其真实意图的方式书写时，例如使用 leet 说话（l33t sp34k）或故意拼写错误，准确检测有毒内容就会变得具有挑战性。这种文本操纵可能涉及：  Leet talk：用数字或符号替换字母（例如，用“h3ll0”替换“hello”）。 过多的元音或辅音：故意重复字母（例如，“heeeeeey”或“byeeee”）。 拼写错误或语音拼写：故意拼错单词（例如，用“toxxic”代替“toxic”）。  我的方法：为了解决这个问题，我建议使用双模型系统：  混淆检测模型：该模型首先识别文本是否包含任何形式的故意混淆，例如 leet talk 或其他类型的操作，并尝试规范化文本。 规范化和毒性检测模型：检测到混淆后，该模型将尝试将混淆的文本转换为标准语言。 规范化后，文本将通过传统的毒性检测模型。  但最近一位同事向我提出了一个选择，那就是只训练一个 BERT 模型，使用所有毒性示例，并通过以编程方式将我的毒性数据集转换为 leet 语言或其他混淆来进行数据增强。 使用侮辱数据集，并将毒性数据集中存在的侮辱转换为 leet 语言和其他技术。 对我来说这似乎也是一种有效的方法。 哪种选择似乎最有意义？ 还有其他方法可以与这些方法竞争吗？   由    /u/GiRLaZo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9rcnp/d_using_bert_models_to_detect_toxicity_in_texts/</guid>
      <pubDate>Thu, 06 Jun 2024 19:54:59 GMT</pubDate>
    </item>
    <item>
      <title>筛选数据非常耗时[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9plun/sifting_through_data_is_timeconsuming_d/</link>
      <description><![CDATA[我是一名数据科学家，我花了很多时间查看我的数据，仔细检查它，寻找错误中的模式，以尝试提高模型性能。这对我来说真的很痛苦。大家有同感吗？你用什么策略来完成这项任务？ 我所做的是查看数据，找到一个示例模式，然后构建特征：查看特定数据片段有多少错误。    提交人    /u/Smooth-Use-2596   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9plun/sifting_through_data_is_timeconsuming_d/</guid>
      <pubDate>Thu, 06 Jun 2024 18:43:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 您是 NeurIPS'24 的审稿人吗？请阅读此内容</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9o8tn/r_are_you_a_reviewer_for_neurips24_please_read/</link>
      <description><![CDATA[你好！ 我目前担任 NeurIPS&#39;24 的区域主席 (AC)。提交的论文数量非常多，为这些论文分配合格的审稿人非常困难。 为什么很难，你可能会问。从高层次上讲，这是因为我们作为 AC，没有足够的信息来判断一篇论文是否分配给了足够数量（至少 3 个）的合格审稿人（即，能够对论文进行信息性评估的个人）。事实上，作为 AC，我们只能使用以下标准来决定是否为任何给定的论文分配审稿人：(i) 他们的出价；(ii) “亲和力”得分；(iii) 他们的个人 OpenReview 个人资料。但是  在注册成为审稿人的人中，只有一小部分对论文进行了投标。举个例子，在我堆积如山的论文中，有 30% 没有审稿人对其进行投标；实际上，大多数论文只有 3-4 个出价（不一定是“积极的”）。 当没有出价时，下一个指标是“亲和力”分数。但是，这个指标是以自动方式计算的，效果不佳（此外，一个人可能是某个领域的专家，但他们可能不愿意审阅某篇论文，例如，由于个人偏见）。 我们可以使用的最后一个指标是审稿人的“背景”，但这需要我们（即 AC）手动检查每个审稿人的 OpenReview 个人资料——这很耗时。更糟糕的是，今年的 NeurIPS 有大量审稿人是本科生或硕士生，他们的 OpenReview 简介完全为空。  鉴于上述情况，我写这篇文章是为了请求您的合作。如果您是 NeurIPS 的审稿人，请确保您的 OpenReview 简介是最新的。如果您是本科生/硕士生，请附上一个网页链接，该链接可以显示您是否具有审稿专业知识，或者您是否与一些“专家研究人员”在实验室工作（他们可能会通过提供审稿技巧来帮助您）。这同样适用于博士生或博士后：确保 OpenReview 上提供的信息反映了您的专业知识和偏好。 底线：您已同意担任（可以说是顶级的）顶级 ML 会议的审稿人。 请认真对待这项职责。如果您被分配到正确的论文，您将能够提供更多有用的评审，评审过程也会更顺畅。有用的评审对作者和 AC 都很有用。通过做得好，您甚至可能会获得“顶级评审员”的认可。    提交人    /u/hihey54   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9o8tn/r_are_you_a_reviewer_for_neurips24_please_read/</guid>
      <pubDate>Thu, 06 Jun 2024 17:46:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 给学生阅读人工智能研究论文的技巧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9k7rq/d_tips_for_getting_used_to_reading_ai_research/</link>
      <description><![CDATA[大家好， 我目前正在攻读五年制计算机科学课程的第二年，我计划明年深入研究人工智能领域。一段时间以来，我一直对人工智能充满热情，我渴望超越一般人的期望，深化我的知识。我目前的目标是三年内毕业，成为一名对该领域有深刻掌握的精英 MLE。 在正式学习人工智能之前，我咨询了一位大学教授，了解如何提高自己并深入研究人工智能，他强烈建议我开始阅读人工智能研究论文，以便在接下来的几年里适应它。然而，我发现，对于像我这样的相对新手学生来说，深入研究这些论文可能会相当艰巨（我只具备线性代数、概率和统计学的基本知识，以及对机器学习算法工作原理的一些了解）。因此，我想知道您是否可以分享任何技巧或策略来帮助我习惯这种阅读类型并充分利用它。 如果您能分享任何建议，资源或个人经验，我们将不胜感激。 提前感谢您的帮助    提交人    /u/ratybox_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9k7rq/d_tips_for_getting_used_to_reading_ai_research/</guid>
      <pubDate>Thu, 06 Jun 2024 14:58:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在这种情况下，多标签分类是最好的方法吗？我和我的经理似乎陷入了困境。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9jxt0/d_is_multilabel_classification_the_best_approach/</link>
      <description><![CDATA[我的公司处理来自智能阀门设备的设备遥测数据。它基本上是来自不同传感器和一些模拟组件的大量数字数据，例如电流和电压。无论是就特征还是样本而言，数据都非常庞大。我的经理基本上想使用机器学习来识别设备中的故障，该设备由多个子组件组成，因此一个子组件中可能存在多个故障，或者一个数据集中不同子组件中可能存在多个故障。 我之前曾使用 XGBoost 和随机森林分类器来识别单个问题，例如仅开关故障，因为那是当时的重点。我也使用 LSTM 处理过一些故障。现在有了这个新目标，基本上要用 ML 检测所有故障，我的经理希望我为所有故障单独标记和训练模型。因此，如果有 50 种类型的故障，就会有 50 个模型。  我觉得这非常低效，难以维护，除此之外，我认为它忽略了一个数据集可能有两个或三个故障的事实，并且在针对一个故障进行训练时，将所有其他故障标记为与正常数据相同的标签可能会在某种程度上产生误导。  我想使用多标签分类器。这里的约束是模型选择，因为我认为很难为某些模型编写多标签包装器，因此我们不得不依赖随机森林来发现所有故障。  你们能否就如何为此类数据选择方法和模型给出建议/提示/正反两方的论据/一般意见？ 您认为是否有可能只为 1 TB 的数据建立一个模型并通过它对所有故障进行分类？或者最好遵循我经理的方法，我觉得这违背了机器学习“学习”部分的目的。     由    /u/General_Working_3531 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9jxt0/d_is_multilabel_classification_the_best_approach/</guid>
      <pubDate>Thu, 06 Jun 2024 14:46:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 针对大型语言模型的有影响力的训练数据检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9hqu0/d_tokenwise_influential_training_data_retrieval/</link>
      <description><![CDATA[论文《面向大型语言模型的 Token 式影响力训练数据检索》介绍了 RapidIn，这是一个旨在估计训练数据对大型语言模型 (LLM) 影响力的框架。 RapidIn 通过将训练数据的梯度向量压缩为称为 RapidGrad 的低维表示，实现了超过 200,000 倍的压缩率，解决了此类大型模型固有的可扩展性和效率问题。这些压缩的梯度被缓存以实现高效检索，从而使 RapidIn 能够在几分钟内估计整个训练数据集的影响力。 该框架支持多 GPU 并行化，以进一步加速缓存和检索过程。实证结果证明了 RapidIn 的有效性和效率，使其成为 LLM 背景下的机器学习、可解释性和数据清理等任务的宝贵工具。 我刚刚写了一篇关于它的 Medium 文章，🤗 让我知道你的想法 https://pub.towardsai.net/token-wise-influential-training-data-retrieval-for-large-language-models-7476037d6fc2     提交人    /u/rezayazdanfar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9hqu0/d_tokenwise_influential_training_data_retrieval/</guid>
      <pubDate>Thu, 06 Jun 2024 13:07:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] CNN 在 1 个 epoch 内能榨出多少能量？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9hloz/d_how_much_juice_can_be_squeezed_out_of_a_cnn_in/</link>
      <description><![CDATA[      嘿嘿！ 昨天做了一个小实验。采用 CIFAR-10 数据集并尝试模型架构，使用模拟退火对其进行优化。 设置合理的搜索空间（卷积层、密集层、内核大小等具有一系列值），然后使用模拟退火找到最佳区域。我们仅对模型进行了一次训练，并使用验证准确率作为目标函数。 之后，我们采用表现最佳的模型并对其进行了 25 次训练，将结果与随机架构设计进行比较。 下图显示得更好，但与随机选择相比，我们看到性能提高了约 10%。必须承认，计算工作量相当大。没什么疯狂的，但完整的细节在这里。 尽管这是一个超级简单的测试，并且模拟退火并不是那么好，但我想说它再次证明了采用系统的方法来设计架构的优点多于缺点。有什么想法吗？ https://preview.redd.it/ox3bd5d09y4d1.png?width=973&amp;format=png&amp;auto=webp&amp;s=e508112539bf39cee331a79f6d5958ea23ade5f3    提交人    /u/AccomplishedPace6024   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9hloz/d_how_much_juice_can_be_squeezed_out_of_a_cnn_in/</guid>
      <pubDate>Thu, 06 Jun 2024 13:01:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 参与资助决定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9h8z3/d_icml_participation_grant_decisions/</link>
      <description><![CDATA[大家好， ICML 旅行补助金决定今天早些时候已经出炉。我创建这个主题是为了了解他们在选择过程中的标准。是否有人/学生真正被选中？看到我甚至没有获得注册费减免，这真是令人失望，而这在过去我参加的其他会议（neurips/iclr）中是默认的。 分享您的想法和经验将是无价的。 谢谢大家！ 请点赞以帮助传播帖子并在过程中获得更多透明度（至少在这里）。    提交人    /u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9h8z3/d_icml_participation_grant_decisions/</guid>
      <pubDate>Thu, 06 Jun 2024 12:42:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 CPU 上的 LLM 嵌入实现闪电般的文本分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9gmqz/p_lightningfast_text_classification_with_llm/</link>
      <description><![CDATA[      https://preview.redd.it/zzybbetrzx4d1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=81f545ea1ea5e752e39d71d1a0997d96a1bb39cb 我很高兴介绍fastc，这是一个不起眼的 Python 库，旨在使文本分类高效而直接，尤其是在 CPU 环境中。无论您从事的是情绪分析、垃圾邮件检测还是其他文本分类任务，fastc 都面向小型模型，避免了微调，非常适合资源受限的环境。尽管方法简单，但性能却相当不错。 主要特点  专注于 CPU 执行：使用 deepset/tinyroberta-6l-768d 等高效模型生成嵌入。 余弦相似度分类：无需微调，而是使用类嵌入质心和文本嵌入之间的余弦相似度对文本进行分类。 高效的多分类器执行：在使用同一模型进行嵌入时，无需额外开销即可运行多个分类器。 使用 HuggingFace 轻松导出和加载：可以轻松地将模型导出到 HuggingFace 并从中加载。与微调不同，只需要在内存中加载一个嵌入模型即可为任意数量的分类器提供服务。  https://github.com/EveripediaNetwork/fastc    提交人    /u/brunneis   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9gmqz/p_lightningfast_text_classification_with_llm/</guid>
      <pubDate>Thu, 06 Jun 2024 12:09:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] 可扩展的无 MatMul 语言建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9fkkn/r_scalable_matmulfree_language_modeling/</link>
      <description><![CDATA[Arxiv link – 可扩展的无 MatMul 语言建模  [...] 在这项工作中，我们表明，MatMul 操作可以完全从 LLM 中消除，同时在十亿参数规模下保持强劲性能。我们的实验表明，我们提出的无 MatMul 模型实现了与最先进的 Transformers 相当的性能，后者在推理期间需要更多的内存，规模至少达到 2.7B 参数。我们研究了缩放规律，发现我们的无 MatMul 模型和全精度 Transformers 之间的性能差距随着模型尺寸的增加而缩小。我们还提供了此模型的 GPU 高效实现，与未优化的基线相比，在训练期间可将内存使用量降低高达 61%。通过在推理过程中使用优化的内核，与未优化的模型相比，我们的模型的内存消耗可以减少 10 倍以上。为了正确量化我们架构的效率，我们在 FPGA 上构建了一个自定义硬件解决方案，该解决方案利用了 GPU 无法实现的轻量级操作。     提交人    /u/PantsuWitch   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9fkkn/r_scalable_matmulfree_language_modeling/</guid>
      <pubDate>Thu, 06 Jun 2024 11:08:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有哪些不错的应用 ML/DS 会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9ef1x/d_what_are_some_good_applied_ml_ds_conferences/</link>
      <description><![CDATA[寻找可提交行业论文的会议。    提交人    /u/LeFlame420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9ef1x/d_what_are_some_good_applied_ml_ds_conferences/</guid>
      <pubDate>Thu, 06 Jun 2024 09:52:14 GMT</pubDate>
    </item>
    <item>
      <title>ML 研究员，博士生日常工作 - 需要建议！[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d9djq5/ml_researcher_phd_routine_advice_needed_d/</link>
      <description><![CDATA[大家好， 我刚完成博士学位的第一年，正在尝试找出一个健康的研究常规。我正在研究计算机视觉。目前，我的日常工作安排如下：  3 小时：制定研究假设（想法和实验） 1.5 小时：查看与我的研究相关的文献 3 小时：巩固我的数学背景和其他基础知识。  虽然我打算坚持这个常规，但我想知道这是否是进行研究的最有效方法，尤其是对于一年级学生而言。这是我担心的：  与实际研究开发相比，我是否在背景知识（数学和读写能力）上花费了太多时间？ 我应该调整这些活动之间的平衡吗？  很想听听更有经验的研究人员的建议！任何关于构建健康的博士一年级日常活动的提示都将不胜感激。    提交人    /u/Same_Half3758   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d9djq5/ml_researcher_phd_routine_advice_needed_d/</guid>
      <pubDate>Thu, 06 Jun 2024 08:46:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>