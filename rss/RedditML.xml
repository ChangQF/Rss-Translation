<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 05 Jun 2024 18:18:40 GMT</lastBuildDate>
    <item>
      <title>[D] 请求就 Equinox Architecture 攻读法学硕士进行合作（我需要您的帮助！！！）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8wp5d/d_request_for_collaboration_on_equinox/</link>
      <description><![CDATA[我认为开发了一种像 Transformer 这样的技术，它的时间复杂度为 O(2*n)，瘫痪时的时间复杂度为 O(log(n))。我通过构建一些小型 LLM 对其进行了测试，其在 PTB 数据集上的准确率为 19% 到 30%，并且有训练过的 PTB 数据集，当模型过度拟合时，它在 PTB 数据集训练上的准确率达到 90%。我想知道这项技术在未来是否有潜力。我已经给研究人员写了一封电子邮件，你可以阅读它以了解更多信息，如果你是 AI 或 LLM 领域的研究人员，请联系我。我真的需要你的帮助。 这是电子邮件： 尊敬的先生/女士， 希望您收到这封信时一切安好。我写信是为了介绍我开发的一种名为“Equinox”的技术，我相信它在语言模型和 Transformer 领域有潜在的应用。我正在寻求您的专业知识和合作，以探索其功能。 Equinox 旨在执行时间复杂度为 O(2n) 的转换器任务，并行化后可实现 O(log(n))。其空间复杂度为 O(2n)。该架构在参数和训练方法方面提供了灵活性。值得注意的是，它允许在有或没有预测下一个标记的情况下进行训练。 在我的实验中，我使用 PTB 数据集和 Equinox 架构训练了几个小型语言模型 (LLM)，具体来说是没有预测下一个标记。结果如下：   Token预测 训练集 测试集 验证集    第3个Token 27.00% 28.18% 26.36%   第5个Token 25.25% 26.18% 24.43%   第9个标记 18.15% 19.06% 17.43%   注意：只有网络的解码器部分接受了预测下一个标记的训练。 正如预期的那样，准确度随着标记预测距离的增加而降低，因为 Equinox 部分没有为此目的进行训练。我相信在训练预测下一个 token 时可以缓解此问题。 Equinox 和解码器的参数数量如下：   Token 预测 Equinox 的参数数量 解码器的参数数量    第 3 个 Token 1,180,416 40,492,801   第 5 个 Token 2,360,832 40,492,801   第 9 个代币 3,541,248 40,492,801   Equinox 中的参数数量范围可以从最小 2*编码维度^2 到可能无限大，从而提供可扩展性。 目前，Equinox 的实现类似于单头转换器，没有任何 MLP 转换，只运行一次。我相信这种架构具有巨大的潜力，凭借您的专业知识，我们可以进一步探索和完善其功能。 我很感激有机会更详细地讨论这项技术并探索潜在的合作。您的见解和贡献对于实现 Equinox 的全部价值将非常宝贵。 感谢您考虑此提议。我期待您的回复。 注意：这封邮件是由 chat-gpt 重写的，邮件感觉我不需要你的帮助，我可以一个人完成，但实际上，我真的需要你的帮助。我愿意分享代码和有关架构的任何其他信息。equinox 的参数可以是一个常数 此致敬意， Dakshish Singh    提交人    /u/Conscious-Gazelle-91   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8wp5d/d_request_for_collaboration_on_equinox/</guid>
      <pubDate>Wed, 05 Jun 2024 18:12:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从时间序列数据中推导规则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8u59u/p_rule_derivation_from_time_series_data/</link>
      <description><![CDATA[大家好。我有表格时间序列数据 [行是感兴趣的区域，列是时间间隔，每个单元格包含当时的 ROI 值]。需要知道是否有任何方法/字段可以使用 ML 派生规则。（实现相同目的的替代方案也可以）。我所说的规则是指（转换、/连接建立/断开、同步、激活） 如果需要更多详细信息，请告诉我。谢谢。    提交人    /u/sagax8   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8u59u/p_rule_derivation_from_time_series_data/</guid>
      <pubDate>Wed, 05 Jun 2024 16:27:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 医生手写数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8tgu9/d_doctor_handwritten_datase/</link>
      <description><![CDATA[大家好， 正在寻找医生手写笔记的数据集，用于手写识别项目。有什么线索吗？ 谢谢！    提交人    /u/Dependent-Ad914   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8tgu9/d_doctor_handwritten_datase/</guid>
      <pubDate>Wed, 05 Jun 2024 15:59:13 GMT</pubDate>
    </item>
    <item>
      <title>[P]评估 RAG 的开源 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8teo3/pevaluating_open_source_llm_for_rag/</link>
      <description><![CDATA[我一直在研究 RAG 和 LLM，并且开发了一些我认为您会觉得有用的东西：BeyondLLM。 这是我创建的一个工具，用于简化构建高级 AI 应用程序的过程。只需几行代码，您就可以深入了解检索增强生成和大型语言模型。而且，它是开源的！ 现在，这是最新更新：BeyondLLM 现在包含其他功能：  微调嵌入：自定义模型的嵌入以提高性能。 可观察性：轻松监控模型的性能。 Groq LLM：为低延迟应用程序体验更快的推理时间。  如果您有兴趣，可以在 GitHub 上查看 BeyondLLM：BeyondLLM GitHub    提交人    /u/trj_flash75   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8teo3/pevaluating_open_source_llm_for_rag/</guid>
      <pubDate>Wed, 05 Jun 2024 15:56:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据湖（房子）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8syrm/d_data_lakehouses/</link>
      <description><![CDATA[嗨！我叫 Alina，是 Qbeast 的产品营销经理。 我们正试图更好地了解人们在管理数据时面临的挑战，无论是在数据湖还是数据湖屋中。我们很乐意听听您在数据存储方法方面的经验。 如果您能花几分钟时间填写此调查问卷，我们将不胜感激。调查链接：https://forms.gle/DJ5N3zcfWLxYUJmF8 如果您有更多关于湖泊（房屋）的信息要分享，我很乐意与您聊天。非常感谢！    提交人    /u/alinagrebenkina   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8syrm/d_data_lakehouses/</guid>
      <pubDate>Wed, 05 Jun 2024 15:38:07 GMT</pubDate>
    </item>
    <item>
      <title>[P]OpenAGI：法学硕士的自主代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8svhx/popenagi_autonomous_agents_for_llms/</link>
      <description><![CDATA[我一直想创建像人类一样的代理。虽然 LLM 擅长收集信息，但我希望代理能够独立规划、推理和行动。 所以我创建了 OpenAGI。 OpenAGI 可帮助您为教育、金融、医疗保健等领域的各种任务构建自主代理。它是开源的，旨在让代理随着时间的推移而学习和改进。 GitHub：OpenAGI GitHub    提交人    /u/trj_flash75   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8svhx/popenagi_autonomous_agents_for_llms/</guid>
      <pubDate>Wed, 05 Jun 2024 15:34:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与去噪模型（DDPM）相比，变分扩散模型（VDM）是否仍在使用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8sa60/d_are_variational_diffusion_models_vdm_still_used/</link>
      <description><![CDATA[如果我理解正确的话，VDM 和 DDPM 之间的主要区别在于 VDM 尝试预测每个 x_t 处的完整噪声，而 DDPM 尝试预测从 x_t-1 到 x_t 的步进噪声。我以本文中的 VDM 推导为基础：https://arxiv.org/abs/2208.11970。 VDM 还在任何地方使用吗？我发现几乎所有知名的图像生成模型都使用 DDPM。即使是试图学习单步扩散的回流方法似乎也是从训练过的 DDPM 开始的。    提交人    /u/WhatIsThis_WhereAmI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8sa60/d_are_variational_diffusion_models_vdm_still_used/</guid>
      <pubDate>Wed, 05 Jun 2024 15:09:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 深度学习的局限性：从复杂性理论的角度进行序列建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8s0yp/r_limits_of_deep_learning_sequence_modeling/</link>
      <description><![CDATA[论文链接：https://arxiv.org/abs/2405.16674 X 线程：https://x.com/NikolaZubic5/status/1797567892646470137    提交人    /u/NikolaZubic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8s0yp/r_limits_of_deep_learning_sequence_modeling/</guid>
      <pubDate>Wed, 05 Jun 2024 14:59:01 GMT</pubDate>
    </item>
    <item>
      <title>[P] 机器学习 / 人工智能研究的存储库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8qt8q/p_a_repository_for_mlai_research/</link>
      <description><![CDATA[因为我经常看到有趣的论文、新闻和其他资源，而且记忆力像金鱼一样好，所以我决定将它们保存在 GitHub 存储库中。 有几份新闻通讯、LinkedIn、Twitter、subreddits 提供有关 ML 的更新。对我来说，它太分散了，有时要找到我读过的文章成了一场噩梦。在过去的几年里，我试图收集各种新闻和文章（我想保存下来以后再读的）。我按周划分它们，你可以在这里找到它们（如果你认为这有用的话）： https://github.com/SalvatoreRa/ML-news-of-the-week  总的来说，我想提出一个问题：你认为数据科学家跟踪趋势的最佳来源是什么？新文章？ 在我看来，存在一种信息过载，在阅读一篇真正有趣的文章之前，我必须阅读几篇文章，在我看来，大多数文章只介绍了增量研究。尤其是今天，许多文章只是已经描述或提出的内容的小变化。例如，快速工程就是一个例子，您可以查看几个方法系列，然后查看来自 CoT 的数百种变体等等。今天，在我看来，RAG 中也发生了同样的事情：Twitter 或 LinkedIn 上的一篇文章指出一种方法是 SOTA，然后阅读这篇文章有一种似曾相识的感觉    提交人    /u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8qt8q/p_a_repository_for_mlai_research/</guid>
      <pubDate>Wed, 05 Jun 2024 14:06:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于生物组织图像合成的检索增强扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8qit5/p_retrieval_augmented_diffusion_for_biological/</link>
      <description><![CDATA[这是一个将 RAG 与 Diffusion 相结合进行生物组织图像合成的探索性项目。这是一个有趣的学习经历，我想分享它，以防其他人觉得有用。但它仍然需要改进，我将尝试整合一个经过微调的模型，而不是从头开始训练。 链接：https://github.com/lnairGT/Diffusion-with-RAG    提交人    /u/IllustriousSir_007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8qit5/p_retrieval_augmented_diffusion_for_biological/</guid>
      <pubDate>Wed, 05 Jun 2024 13:54:44 GMT</pubDate>
    </item>
    <item>
      <title>生物技术中的人工智能。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8pxsw/ai_in_biotech_d/</link>
      <description><![CDATA[想知道在生物技术领域，AI 是营销热词还是有实际应用。 我是 ML 工程师，根本不是生物学家。我在生物技术领域发现了几种 AI 解决方案，但它们似乎都没有真正带来价值，而更像是一种趋势。我希望我错了，但找不到好的证据。 我发现的解决方案是 Material gen、Microsoft；Alpha Fold、DeepMind；EvBio。    提交人    /u/IIISergeyIII   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8pxsw/ai_in_biotech_d/</guid>
      <pubDate>Wed, 05 Jun 2024 13:28:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于生成推荐的万亿参数序列传感器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8o2sz/r_trillionparameter_sequential_transducers_for/</link>
      <description><![CDATA[Meta 的研究人员最近发表了一篇开创性的论文，将 ChatGPT 背后的技术与推荐系统相结合。他们表明，他们可以将这些模型扩展到 1.5 万亿个参数，并在生产 A/B 测试中将顶线指标提高了 12.4%。 我们在本文中深入探讨细节：https://www.shaped.ai/blog/is-this-the-chatgpt-moment-for-recommendation-systems    提交人    /u/skeltzyboiii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8o2sz/r_trillionparameter_sequential_transducers_for/</guid>
      <pubDate>Wed, 05 Jun 2024 11:55:46 GMT</pubDate>
    </item>
    <item>
      <title>[N] 加入我们即将举行的网络研讨会：从云到芯片：将 LLM 引入边缘设备。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8k9t0/n_join_our_upcoming_webinar_from_cloud_to_chip/</link>
      <description><![CDATA[从云到边缘的转变代表了大型语言模型 (LLM) 部署的范式转变。通过利用硬件和软件方面的进步、优化模型和解决安全问题，我们可以充分利用边缘设备上 LLM 的潜力。欢迎于 6 月 26 日下午 3 点 (GMT+2) 加入我们，探索这些突破性的发展，并从行业专家那里获得宝贵的见解。 在此处查找有关网络研讨会的更多信息并注册：https://www.embedl.com/events/webinar-from-cloud-to-chip-bringing-llms-to-edge-devices    提交人    /u/Embedl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8k9t0/n_join_our_upcoming_webinar_from_cloud_to_chip/</guid>
      <pubDate>Wed, 05 Jun 2024 07:43:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] mamba.np：Mamba 的纯 NumPy 实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</link>
      <description><![CDATA[      mamba.np 受到一些很棒的项目的启发，我用纯 Numpy 从头实现了 Mamba。代码的目标是简单、可读、轻量，因为它可以在本地 CPU 上运行。 https://github.com/idoh/mamba.np 希望您觉得它有用 :)    提交人    /u/id0h   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</guid>
      <pubDate>Tue, 04 Jun 2024 16:02:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>