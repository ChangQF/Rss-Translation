<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 16 Jan 2024 03:15:46 GMT</lastBuildDate>
    <item>
      <title>[D] 深度学习最好的进阶书籍？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/</link>
      <description><![CDATA[我遇到的几乎所有书籍都是从头开始写的。他们是否有深入探讨主题的地方？   由   提交/u/toxfu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/</guid>
      <pubDate>Tue, 16 Jan 2024 02:12:50 GMT</pubDate>
    </item>
    <item>
      <title>[p] 深度调谐器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197ogx3/p_deeptuner/</link>
      <description><![CDATA[我正在开发一个吉他调音器，它能够在嘈杂的环境中拾取吉他音符并识别它们的频率。我的目标是为空弦吉他弦的音频添加背景噪音。最初我会添加白噪声，然后尝试使用其他乐器和人说话进行训练。然后，该模型将重建孤立吉他的音频。 我的架构目前涉及自动编码器，但我一直在尝试寻找有关可以隔离特定音频（单个扬声器、乐器标识符）的音频模型的更新论文）。 我正在寻找研究论文推荐以及音乐数据集。 （Nsynth 对于吉他来说大多是垃圾）   由   提交/u/Perfect_Natural_2540   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197ogx3/p_deeptuner/</guid>
      <pubDate>Tue, 16 Jan 2024 00:08:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 超参数搜索时将权重衰减值除以学习率的原因是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197mqpg/d_what_is_the_reason_for_dividing_weight_decay/</link>
      <description><![CDATA[      你好， 我现在至少看到了两个高-级别的机器学习论文，人们在进行超参数搜索时将权重衰减除以学习率。我很好奇它背后的想法是什么？ 我唯一的想法是，这是可以做到的，因为权重衰减项在梯度优化步骤中乘以学习率（方程 1），所以通过除以权重衰减值，我们可以确保优化步骤中的权重衰减项不依赖于我们的学习率。 公式 1  以下是论文：  视觉表示对比学习的简单框架 预训练你的损失：利用信息丰富的先验进行简单的贝叶斯迁移学习  纸张1 ​ 论文2 谢谢。   由   提交/u/Significant_Chip_269   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197mqpg/d_what_is_the_reason_for_dividing_weight_decay/</guid>
      <pubDate>Mon, 15 Jan 2024 22:55:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对强化学习的真实体验是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197jp2b/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[根据我个人的经验，SOTA RL 算法根本不起作用。我尝试强化学习已有 5 年多了。我记得当 Alpha Go 击败世界著名围棋棋手 Lee Sedol 时，每个人都认为 RL 会席卷 ML 社区。然而，除了玩具问题之外，我个人从未找到过 RL 的实际用例。 您对此有何体验？除了广告推荐系统和 RLHF 之外，RL 是否还有合法的用例？或者，这都是炒作吗？ 编辑：由于我的评论被否决，这里是我的文章的链接更好地描述了我的立场。 这并不是说我不理解强化学习。我发布了我的开源代码和就此写了一篇论文。 事实是它非常难以理解。其他深度学习算法，如 CNN（包括 ResNet）、RNN（包括 GRU 和 LSTM）、Transformers 和 GAN 并不难理解。这些算法可以工作，并且在实验室外有实用用例。 传统的 SOTA RL 算法（如 PPO、DDPG 和 TD3）非常困难。即使解决一个玩具问题，你也需要做大量的研究。相比之下，决策转换器是任何人都可以实现的东西，而且它似乎匹配或超越了 SOTA。您不需要两个网络相互竞争。您不必经历地狱般的事情来调试您的网络。它只是自然地以自回归的方式学习最好的一组动作。 我也不是故意显得傲慢或暗示强化学习不值得学习。我只是还没有看到任何现实世界中的实际用例。我只是想开始讨论，而不是声称我什么都知道。 编辑 2：令人震惊的是，有很多人因为我没有完全理解 RL 而称我为白痴。你们太自在了称呼不同意的人的名字。新闻快讯，并不是每个人都拥有机器学习博士学位。我的本科学位是生物学。我自学了高级数学来理解机器学习。我对这个领域充满热情；我在 RL 方面的经历非常令人失望。 有趣的是，很少有人反驳我的实际观点。总结一下：  缺乏实际应用 极其复杂且 99% 的人无法使用 比 CNN、RNN 和 GAN 等传统深度学习算法困难得多 样本效率低下且不稳定 难以调试 更好的替代方案，例如决策转换器&lt; /li&gt;  这些难道不是合理的批评吗？本子的目的不是要讨论与机器学习相关的问题吗？ 致少数没有称我为白痴的评论者……谢谢！请记住，你不需要付出任何代价就能变得友善！ 最终编辑：很多人似乎都认为 RL 被过度炒作了。不幸的是，这些评论被否决了。澄清一些事情：  我们在强化学习上投入了大量资金。我们从这项投资中得到的只是一个可以在（某些）视频游戏中超越人类的机器人。 AlphaFold 没有使用任何强化学习。 SpaceX 也没有。  我承认它对机器人技术很有用，但仍然认为它在实验室之外的用例极其有限。  如果您无意中发现了这条线索并对 RL 替代方案感到好奇，请查看决策转换器。它可以用于任何可以使用传统强化学习算法的情况。   由   提交 /u/Starks-Technology   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197jp2b/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 20:56:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练基于嵌入而不是令牌进行转换。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197h7em/d_training_transforms_on_embeddings_rather_than/</link>
      <description><![CDATA[在训练 LLM 来预测嵌入向量而不是标记序列方面做了哪些工作？例如，将为文本中的每个句子（或短语）创建嵌入，然后法学硕士对此进行训练。   由   提交/u/redv  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197h7em/d_training_transforms_on_embeddings_rather_than/</guid>
      <pubDate>Mon, 15 Jan 2024 19:19:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 公司邀请我在工程会议上担任有关 AI/ML 主题的演讲者，但我缺乏经验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197gpl3/d_company_invited_me_to_become_a_speaker_about_an/</link>
      <description><![CDATA[我是一名 Linux 高级嵌入式和系统开发人员，去年出于好奇我决定学习 AI / ML 主题，并且我已完成Andrew Ng 的机器学习和深度学习专业。然后，为了了解更多信息，我在创建内部 AI/ML 社区时加入了该社区并成为创始成员，但这一切都是最近的事，除了帮助建立社区之外，我没有做任何事情。 &lt;由于这个原因，上级注意到了我并邀请我成为一名演讲者，我对此很感激，但我的经验几乎为零，所以我在礼貌地拒绝或可能将其重点放在工程师之间犹豫不决，因为例如，关于人工智能有许多神话可以被揭开，或者为那些对此一无所知的人解释基本概念。 我想知道你的想法，以及你对可能的主题有什么建议吗？有趣。 谢谢。   由   提交 /u/bulletinyourfnhead   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197gpl3/d_company_invited_me_to_become_a_speaker_about_an/</guid>
      <pubDate>Mon, 15 Jan 2024 19:00:47 GMT</pubDate>
    </item>
    <item>
      <title>NLP 应用程序的降维被遗忘了……？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197glkm/dimensionality_reduction_for_nlp_applications/</link>
      <description><![CDATA[好吧，几年前我写论文时，我写了整整一节关于为什么降维（DR）很重要，我记得通过说明距离概念在高维度中失去重要性的东西来论证这一点。我还记得我用于聚类的一些方法在较低维度上效果更好，因此，DR 不仅仅是为了可视化目的（我觉得这是我最常使用的），而是建模中的必要步骤，不过，这些天，我读了很多关于人们使用 LLM 并将极高维嵌入到模型、聚类方法等中的文章，甚至没有提到 DR。那是怎么回事？我记得（尽管模糊）对维度如何影响常见距离度量进行了一些测试，基本上，余弦相似度在比 BERT、Mistral、openAI 或其他任何输出少得多的维度上不再有意义。我在这里误解了什么吗？难道DR真的没那么重要吗，人们是不是低估了它的价值，他们不在乎，难道他们不知道吗……？预先感谢大家，希望有人能为我阐明这一点:)   由   提交/u/_donau_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197glkm/dimensionality_reduction_for_nlp_applications/</guid>
      <pubDate>Mon, 15 Jan 2024 18:56:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 相对位置嵌入以及相对于绝对位置编码的优势是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197euq9/d_relative_positional_embedding_and_whats_the/</link>
      <description><![CDATA[所以我只是阅读了绝对位置编码，然后阅读了相对位置嵌入。 我所能理解的是它是如何完成的与每个词相关。但我真的想不出相对于绝对优势的优势，因为“注意力就是你所需要的一切”。还指出“我们选择这个函数是因为我们假设它可以让模型轻松学习参加相对位置......”那么相对位置编码有什么优势？  有人可以解释一下以下几点吗：  我们选择正弦版本是因为它可以允许模型推断出比训练期间遇到的序列长度更长的序列长度。 （如何？） 使用绝对位置信息必然意味着模型可以处理的标记数量有限    由   提交/u/karun_kodes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197euq9/d_relative_positional_embedding_and_whats_the/</guid>
      <pubDate>Mon, 15 Jan 2024 17:49:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] Draw2Img：在画布上绘图，立即创建令人惊叹的图形和图像</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197ci39/p_draw2img_draw_on_canvas_to_instantly_create/</link>
      <description><![CDATA[这是一个开源 Web UI，用于通过 SDXL-Turbo 生成交互式文本引导图像到图像，后端是多线程 HTTP + Websocket用 Python 编写的服务器。 如果满足以下条件，您可能会对这个项目感兴趣：  您或朋友/家人/孩子有兴趣学习生成艺术的基础知识，但没有时间/耐心/技能来a1111/comfy/etc 你几乎没有艺术技能（或者可能很多！），并且只是想要以最少的精力和时间为您的网站或项目创建美观的自定义图形 您想要快速创建创造性地迭代 512x512 基础图像，作为更高级工作流程（例如升级、扩散等）的第一步  GitHub 链接：https://github.com/GradientSurfer/Draw2Img   由   提交 /u/GradientSurfer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197ci39/p_draw2img_draw_on_canvas_to_instantly_create/</guid>
      <pubDate>Mon, 15 Jan 2024 16:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 扩散模型的潜在分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19792fl/d_latent_distributions_of_diffusion_model/</link>
      <description><![CDATA[        由   提交/u/sushilkhadakaanon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19792fl/d_latent_distributions_of_diffusion_model/</guid>
      <pubDate>Mon, 15 Jan 2024 13:45:19 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 PGVector 的 2048 维减少到 2000 维</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19785i7/p_reducing_2048_dimensions_to_2000_dimensions_for/</link>
      <description><![CDATA[ML 的朋友们好， 我正在开发一个使用 PGVector 进行高效相似性搜索的项目，我使用特征向量我从 EfficientNet B5 获得，输出为 2048d。问题是我需要根据向量对表进行索引，否则，会出现典型的数据库硬件问题（RAM 不足）。然而，PGVector 提供的方法有一个限制，向量最多可以是 2000d。我发现的一种解决方案是PCA，但我有相当多的数据，所以在测试之前，我想得到一些意见和建议。这里有人尝试过 PCA 进行降维以进行相似性搜索，主要是针对 L2 和余弦，如果是这样，结果如何？   由   提交 /u/TutubanaS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19785i7/p_reducing_2048_dimensions_to_2000_dimensions_for/</guid>
      <pubDate>Mon, 15 Jan 2024 12:59:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] EACL 2024 决定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1975kvn/d_eacl_2024_decisions/</link>
      <description><![CDATA[致力于 EACL 2024 的人的决定将于今天（2024 年 1 月 15 日）公布！您的期望是什么？    由   提交 /u/OraclePred   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1975kvn/d_eacl_2024_decisions/</guid>
      <pubDate>Mon, 15 Jan 2024 10:23:06 GMT</pubDate>
    </item>
    <item>
      <title>[R]“从生成式人工智能到值得信赖的人工智能：法学硕士可以从 Cyc 学到什么”（2023 年）——Doug Lenat 去世前的最后一篇论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1974yoo/r_getting_from_generative_ai_to_trustworthy_ai/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2308.04445 博客文章：https://garymarcus.substack.com/p/doug-lenat-1950-2023 相关 Doug Lenat 演讲： 2022 年：https://www.youtube.com/watch?v=VjkbmLjwXO8 2019 年：https://www.youtube.com/watch?v=v2rK40bNrrY 摘要：  生成式人工智能是当前最流行的人工智能方法，由大型语言模型 (LLM) 组成，这些模型经过训练可产生合理的输出，但不一定正确。尽管他们的能力往往令人难以置信，但他们缺乏推理能力，导致法学硕士不太值得完全信任。此外，他们的结果往往是不可预测和无法解释的。我们列出了未来人工智能的 16 个需求，并讨论了人工智能的替代方法，该方法理论上可以解决与当前方法相关的许多限制：用精选的片段进行人工智能教育显性知识和经验规则，使推理引擎能够自动推断出所有这些知识的逻辑蕴涵。即使以这种方式产生的长论证也可以是值得信赖和可解释的，因为完整的一步一步的推理总是可用的，并且对于每一步，所使用的知识的来源都可以记录和审计。然而，有一个问题：如果逻辑语言的表达能力足以完全表达我们用英语所说的任何内容的含义，那么推理引擎的运行速度就会太慢。这就是为什么符号人工智能系统通常会选择一些快速但表达能力较差的逻辑，例如知识图。我们描述了一个人工智能系统 Cyc 如何开发出方法来克服这种权衡，并能够实时进行高阶逻辑推理。我们建议任何值得信赖的通用人工智能都需要混合这些方法，即法学硕士方法和更正式的方法，并为实现这一梦想奠定了道路。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1974yoo/r_getting_from_generative_ai_to_trustworthy_ai/</guid>
      <pubDate>Mon, 15 Jan 2024 09:41:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2024 决定将于今天公布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196uyub/d_iclr_2024_decisions_are_coming_out_today/</link>
      <description><![CDATA[我们很快就会在接下来的几个小时内知道结果。请随意宣传您已被接受的项目，并对您被拒绝的项目进行咆哮。   由   提交/u/deschaussures147  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196uyub/d_iclr_2024_decisions_are_coming_out_today/</guid>
      <pubDate>Mon, 15 Jan 2024 00:25:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>