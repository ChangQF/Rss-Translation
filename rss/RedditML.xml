<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 25 Feb 2024 21:12:21 GMT</lastBuildDate>
    <item>
      <title>[P] 机器学习项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azyic0/p_machine_learning_project/</link>
      <description><![CDATA[作为 PNU 的一名专注于数据科学的学生， 我们很高兴邀请您参加我们的机器学习项目以开发电影推荐系统为中心  我们恳请您填写表格参与其中，为我们提供宝贵的见解来完善和优化我们的系统。  https://forms.gle/hQddBe9FxpFtxbrt5 感谢您的时间和合作.   由   提交 /u/its-odd   /u/its-odd  reddit.com/r/MachineLearning/comments/1azyic0/p_machine_learning_project/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azyic0/p_machine_learning_project/</guid>
      <pubDate>Sun, 25 Feb 2024 20:49:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 关于 Mamba 的更多思考：使用 VisionMamba 从缩略图预测 Youtube 视频的观看次数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azxgjy/p_more_thoughts_on_mamba_predicting_the_view/</link>
      <description><![CDATA[为了好玩，我写了一篇关于我的项目的文章，该项目使用使用 Mamba 的 Vision Transformer 变体从视频缩略图预测视频的观看次数而是作为骨干。这是一个非常好用的模型。它的内存轻意味着我可以像 GPU 匮乏的人一样使用它，这太棒了。这是系列文章的第一篇，我计划将来扩大数据集，合并文本嵌入，然后最终将其与 RLHF 一起使用来微调稳定的扩散模型。 https:// /open.substack.com/pub/2084/p/2084-can-you-predict-the-view-count?r=brh1e&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true   由   提交 /u/ExaminationNo8522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azxgjy/p_more_thoughts_on_mamba_predicting_the_view/</guid>
      <pubDate>Sun, 25 Feb 2024 20:07:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您可以推荐哪些机器学习模型监控工具？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azx13e/d_what_machine_learning_model_monitoring_tools/</link>
      <description><![CDATA[我们的团队希望在生产中的解决方案中添加模型监控。 我们的模型主要处理表格数据。我们非常喜欢免费的解决方案。 欢迎并赞赏任何建议。   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1azx13e/d_what_machine_learning_model_monitoring_tools/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azx13e/d_what_machine_learning_model_monitoring_tools/</guid>
      <pubDate>Sun, 25 Feb 2024 19:50:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推理速度差异较大</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azwsbp/d_large_difference_in_inference_speed/</link>
      <description><![CDATA[我正在训练用于对象检测的 yolov5 模型。我使用稀疏 ml 对其进行修剪和量化，然后将其导出为 onnx 格式。 （图像大小 640，批量大小 16） 在使用 CPU（和 ryzen 5 5600、16GB RAM）的笔记本电脑上推断时，每个图像速度大约需要 20 毫秒。 现在什么时候我在树莓派 5（A76，8gb 内存）中推断同样的事情，每个图像的推理速度仅为 220 毫秒 为什么推理速度有如此大的差异。我知道 Pi 模块的 cpu 可能较慢，但相差 10 倍??? 我在它们两个中安装了相同的库。是否需要在树莓派中手动配置onnx运行时以提高推理速度？   由   提交 /u/Melodic_Draw6781   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azwsbp/d_large_difference_in_inference_speed/</guid>
      <pubDate>Sun, 25 Feb 2024 19:40:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] #14 使用 Python 和 Tensorflow 的神经网络中的激活函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azw9kz/d_14_activation_functions_in_neural_networks_with/</link>
      <description><![CDATA[       由   提交/u/datonsx  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azw9kz/d_14_activation_functions_in_neural_networks_with/</guid>
      <pubDate>Sun, 25 Feb 2024 19:19:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 唤醒词检测的当前 SOTA 是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azvw5g/d_what_is_the_current_sota_for_wake_word_detection/</link>
      <description><![CDATA[大家好 Tldr：如何准确地进行唤醒词检测？  我正在开展一个项目，我需要进行唤醒词检测。当前的实现使用 STT 模型来转录音频，然后使用 if 语句检查唤醒词是否在音频中。  这是一个临时实现。我假设有更好的方法，所以我想听听对此有更多了解的人的建议。    由   提交/u/Amgadoz  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azvw5g/d_what_is_the_current_sota_for_wake_word_detection/</guid>
      <pubDate>Sun, 25 Feb 2024 19:04:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 硕士论文课堂增量学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azuuqk/d_classincremental_learning_for_masters_thesis/</link>
      <description><![CDATA[大家好， 我写这篇文章是因为我已经开始写硕士论文了。我选择的主题是计算机视觉背景下的课堂增量学习。主要目标是修改针对特定数据集的现有方法之一，以实现比“基本”数据集更高的准确性。方法。鉴于时间限制，这是我认为最好尝试的方法。 如果您有任何建议（关于我应该尝试修改哪些课堂增量学习方法以及如何修改），我将不胜感激以及任何其他建议。 几天后，我将与我的协调员会面讨论这些想法。   由   提交/u/alterednitrogen  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azuuqk/d_classincremental_learning_for_masters_thesis/</guid>
      <pubDate>Sun, 25 Feb 2024 18:23:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 节奏...</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azr45o/d_the_pace/</link>
      <description><![CDATA[这个领域的发展速度令人难以置信。我刚刚发现两周前上传到 arXiv 的一篇论文与我几天前完成的工作非常相似。现在我的纸只是卫生纸，我感到非常沮丧。   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azr45o/d_the_pace/</guid>
      <pubDate>Sun, 25 Feb 2024 15:53:23 GMT</pubDate>
    </item>
    <item>
      <title>[N]Magika 简介：强大的文件类型检测库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azp35r/nintroducing_magika_a_powerful_file_type/</link>
      <description><![CDATA[      Magika 是 Google 开发的文件类型检测库，已获得关注。我们创建了一个网站，您可以在其中轻松试用 Magika。请随意尝试一下！ https://9revolution9.com/tools/security/file_scanner/  https:// /preview.redd.it/u5cuqvfyqqkc1.png?width=2094&amp;format=png&amp;auto=webp&amp;s=d16e51115134e3943cc6027cc0a9191ba835c38f ​ &lt; !-- SC_ON --&gt;  由   提交/u/glassonion999  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azp35r/nintroducing_magika_a_powerful_file_type/</guid>
      <pubDate>Sun, 25 Feb 2024 14:24:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我编写了一个用于调试 Triton 代码的小工具。有人感兴趣吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azmuf7/d_i_wrote_a_small_tool_for_debugging_triton_code/</link>
      <description><![CDATA[嘿，我正在编写 Triton 内核，据我所知，调试代码的唯一方法是使用 tl.device_print，它仅适用于张量数据（没有适合您的形状）并阻塞输出。因此，我编写了一个小工具来仅使用 torch 来运行内核，而无需更改代码。唯一的变化是减少启动网格大小并将内核包装器更改为调试包装器。下面是一个简单内核的示例： import torch import triton # import triton.language as tl import Tests.Triton.triton_debug_module as tl # @triton.jit @tl.debug def add_kernel(x_ptr , y_ptr, output_ptr, n_elem, BLOCK_SIZE: tl.constexpr): pid = tl.program_id(axis=0) block_start = BLOCK_SIZE * pid 偏移量 = block_start + tl.arange(0, BLOCK_SIZE) mask = 偏移量 &lt; n_elem x = tl.load(x_ptr + 偏移量, mask=mask) y = tl.load(y_ptr + 偏移量, mask=mask) 输出 = x+y tl.store(output_ptr + 偏移量, 输出, mask=mask) def add （x：torch.Tensor，y：torch.Tensor）：输出= torch.empty_like（x）断言x.is_cuda和y.is_cuda和output.is_cuda n_elem =输出.numel（）网格= lambda元：（triton.cdiv (n_elem, meta[&#39;BLOCK_SIZE&#39;]), ) add_kernel[grid](x, y, output, n_elem, BLOCK_SIZE=64) 返回输出  此代码将使用 torch 后端执行，并且您可以以正常方式查看每个张量形状和值。代码中唯一的变化是注释掉 triton.language 和 triton.jit 的导入，导入调试模块并在 tl.debug 中包装内核。 我还实现了内存读写的自动可视化（相同的东西，但是将内核包装在 tl.debug_vis 中，无需其他更改）。以下是 flashattention2 转发内核的示例： ​ Attn Fwd 那么，这个工具可能对某人有用吗？它仍然有点不完整，因为我还没有包装所有的triton函数，而且还有外部cuda函数，我只实现了tl.math.exp2 Floor sqrt和log2。那么，我应该开源吗？我应该发布 arxiv 内容还是将其提交给某个研讨会？   由   提交/u/clueless_scientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azmuf7/d_i_wrote_a_small_tool_for_debugging_triton_code/</guid>
      <pubDate>Sun, 25 Feb 2024 12:31:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 受自然启发的本地传播</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azlava/r_natureinspired_local_propagation/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2402.05959 OpenReview：https:// /openreview.net/forum?id=uCMxeZCp2T 摘要：  机器学习领域取得的惊人成果，包括生成人工智能的最新进展依赖于大数据收集。相反，自然界中的智能过程不需要此类收集，而只需在线处理环境信息即可。特别是，自然学习过程依赖于数据表示和学习以尊重时空局部性的方式交织在一起的机制。本文表明，这种特征源于前算法的学习观，这种学习观受到理论物理学相关研究的启发。我们表明，当传播速度达到无穷大时，所导出的“学习定律”的算法解释采用哈密顿方程的结构，可简化为反向传播。这为基于完整在线信息处理的机器学习研究打开了大门，这些研究基于用所提出的时空局部算法替换反向传播。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azlava/r_natureinspired_local_propagation/</guid>
      <pubDate>Sun, 25 Feb 2024 10:58:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 潜在空间理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azk18e/d_theory_for_latent_space/</link>
      <description><![CDATA[大家好，我想知道是否有人有一些关于“潜在空间理论”的论文/参考文献可以分享。在神经网络中。我试图说得更清楚：现在我们在许多应用中看到神经网络在其隐藏层中找到了学习的相关模式。以 CNN 为例，以及我们检查网络学习的过滤器的方式。或者考虑一个“更简单”的方法。 word2vec：同样，我们可以通过绘制嵌入的投影来研究每个单词的特定情况下会发生什么。然而，这些都是启发式的，取决于具体情况。有人知道一个更通用的理论来描述这些隐藏层中发生的事情吗？抱歉，如果这个问题很幼稚，我来自物理学，我喜欢认为某个地方有一些原理:)   由   提交/u/PiMas88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azk18e/d_theory_for_latent_space/</guid>
      <pubDate>Sun, 25 Feb 2024 09:35:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 深度神经网络中的梯度下降是隐式 Ricci 流吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azjaw7/d_is_gradient_descent_in_deep_neural_networks_an/</link>
      <description><![CDATA[深度神经网络在流形假设下工作，即训练数据位于较低维流形上。特别是，最后一层是线性层，为了正常工作，这必须意味着目标流形曲率必须约为常数，因为线性子空间需要一致的曲率来逼近目标流形（在回归的情况下） ）或将其分开（在分类的情况下）。也就是说，梯度下降生成的流形序列必须在其度量张量的意义上收敛到恒定曲率的流形。里奇流也这样做。我的比较正确吗？   由   提交 /u/Flankierengeschichte   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azjaw7/d_is_gradient_descent_in_deep_neural_networks_an/</guid>
      <pubDate>Sun, 25 Feb 2024 08:46:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 除了检索增强生成（RAG）之外，还有哪些使用法学硕士构建的其他范式和框架？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</link>
      <description><![CDATA[自 InstructGPT 及其大众市场应用程序 ChatGPT 上市以来，已经一年多了，并引发了我们今天看到的围绕法学硕士的兴趣风暴。  除了研究和学术兴趣之外，行业（初创企业、大型企业等）也出于商业原因尝试构建由法学硕士支持的新产品和/或功能。 然而，到目前为止，我所看到的大部分内容要么是围绕 LLM 的薄包装应用程序，要么是 RAG 的某些变体。  除了检索增强生成（RAG）之外，还有哪些使用法学硕士进行构建的其他范式和框架？   由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</guid>
      <pubDate>Sun, 25 Feb 2024 04:27:51 GMT</pubDate>
    </item>
    </channel>
</rss>