<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 17 Jan 2024 03:15:36 GMT</lastBuildDate>
    <item>
      <title>[D] 离线批量服务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/198k375/d_offline_batch_serving/</link>
      <description><![CDATA[我对如何为离线批量预测提供机器学习模型感到困惑。 这是我想做的 -创建一个预定的管道（例如 Airflow、Kubeflow 等）来生成特征，然后从某个对象存储（例如 s3）加载经过训练的模型，生成预测，最后将它们保存到数据仓库中以供使用。这对我来说最有意义。 但是，一些资源似乎建议将模型部署为端点，即使对于批量用例也是如此。值得注意的是，这是 Chip Huyen 的《设计机器学习系统》中推荐的架构。 对此有什么想法吗？我错过了什么吗？   由   提交/u/Appropriate_Cut_6126   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/198k375/d_offline_batch_serving/</guid>
      <pubDate>Wed, 17 Jan 2024 01:25:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 监督（可能）集群或多代理旅行商人问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/198jyq7/p_supervised_maybe_clustering_or_multiagent/</link>
      <description><![CDATA[大家好。我在为地图点组分配标签时遇到问题。我们可以将它们视为我们应该访问的点。我们还有一些球探会访问这些地点。我们应该用最少的时间来做这件事。这是一种旅行商人问题，但涉及多个商人。 我的想法是确定每个点的权重作为到其他点的距离，然后应用带有 N 个聚类的加权 K 均值。之后我们就可以解决个别优化问题 但我对这种方法有一些担忧。我们有标签，这些标签是如何由人类分配的，但我们根本不使用它。由于每次侦察的数量不同，我们无法解决分类问题。此外，该算法不能将点标记为“噪声”（如 HDBSCAN 中的点），这并不重要，但对于距离其他点非常远的点可能很有用 关于如何解决它有什么考虑吗？    由   提交/u/Jor_ez   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/198jyq7/p_supervised_maybe_clustering_or_multiagent/</guid>
      <pubDate>Wed, 17 Jan 2024 01:19:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在组织中推出推荐模型的项目模板/步骤</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/198jh0u/d_project_templatesteps_for_rolling_out_a/</link>
      <description><![CDATA[我对构建内容推荐模型有些满意。我不太确定如何确定项目范围以及将项目推出到我们的应用程序中所涉及的更广泛的步骤。 是否有关于项目管理此类功能的蓝图？例如。目标、特征探索、模型选择、训练、测试、“将模型实施到生产中的步骤”？   由   提交/u/back-off-warchild  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/198jh0u/d_project_templatesteps_for_rolling_out_a/</guid>
      <pubDate>Wed, 17 Jan 2024 00:57:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 给我你最好的！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/198higd/d_give_me_your_best/</link>
      <description><![CDATA[大家好！ 我正在尝试阐述我在深度学习和神经网络方面的知识（理论和实践）。&lt; /p&gt; 你能留下你知道的最好的科学论文、课程、项目等吗？非常感谢！   由   提交/u/Binibini000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/198higd/d_give_me_your_best/</guid>
      <pubDate>Tue, 16 Jan 2024 23:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[P]从头开始的小型潜伏扩散变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/198eiv1/p_small_latent_diffusion_transformer_from_scratch/</link>
      <description><![CDATA[      我训练了一个相对简单的基于 Transformer 的扩散模型来生成 256 x 256 图像从头开始。这是仓库： https://github.com/apapiu/transformer_latent_diffusion/tree/main - 代码应该希望它相当容易理解并且独立。 以下是在 1A100 从头开始​​训练大约 30 小时后的一些示例： 根据各种提示生成图像 该模型基于 DiT /Pixart-alpha 架构，但进行了各种修改和简化。我还在噪声表方面做出了一些有问题的决定，但似乎工作正常。 该模型是 100MM 参数，因此应该很容易对其进行实验。我欢迎任何反馈，也欢迎合作，所以请联系我们！希望这对想要尝试扩散模型/变压器但“GPU 较差”的人们有所帮助。 :) 该存储库还链接到一个 colab，您可以在其中使用自己的输入 - 请随意尝试。 ​    由   提交 /u/spring_m   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/198eiv1/p_small_latent_diffusion_transformer_from_scratch/</guid>
      <pubDate>Tue, 16 Jan 2024 21:29:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 时间序列数据上的 MAMBA 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1989o55/d_mamba_models_on_time_series_data/</link>
      <description><![CDATA[大家好，我最近对 ​​MAMBA 架构很感兴趣，特别是因为它采用线性时间无关数学模型作为一种内存。我热衷于将其应用于时间序列分类或回归任务，但我在网上找到的大多数信息都集中在它在语言建模中的使用。尽管我尝试在时间序列数据集上训练这些模型，但它们似乎没有学到任何东西。我想知道你们中是否有人遇到过在时间序列数据上成功训练 MAMBA 模型的例子，以便找出我做错了什么。提前致谢！   由   提交 /u/jumpyAlucard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1989o55/d_mamba_models_on_time_series_data/</guid>
      <pubDate>Tue, 16 Jan 2024 18:16:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您如何处理雇主提出的不合理要求以及对机器学习不切实际的期望？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19882l2/d_how_do_you_deal_with_unreasonable_request_from/</link>
      <description><![CDATA[几个月前，我接受了一个职位，通过为社会科学研究项目训练机器学习模型来支持该项目。该项目涉及使用团队（由多名实习生、研究生、博士后和教授组成）花费数年时间并付出疯狂努力编制的数据集。然而，问题是他们没有事先咨询任何真正了解机器学习的人。对于非常复杂的任务来说，他们的数据集太小（只有大约 200 行）。更糟糕的是，大多数变量的预测价值微乎其微，而用于推导这些变量的方法虽然非常耗费人力，却引发了人们对其有效性的担忧。 该项目的 MO 绝对令人困惑：通过巨大的数据积累了数千个预测变量。努力和人力，期待完美的结果。任何模型如何用如此小的数据集估计如此多的参数却被忽视了。项目负责人似乎对 ML 有着某种神奇的理解，这可能是受到其在特定领域频繁误用的影响。这个项目的灵感尤其来自于一篇研究论文，我几乎可以保证该论文在其验证集上过拟合。 所有这些都让我处于尴尬的境地，作为新人，我需要告知这一点一个由经验丰富的博士后和教授组成的团队，全部来自社会科学背景，没有定量专业知识，他们多年的工作产生了一个完全不适合他们的目标的数据集，并且他们所建立的现有文献都是错误的，因为他们显然没有不知道什么是测试集以及何时使用它。我也不能告诉他们只扩展数据集，因为达到 200 行已经花费了数年时间。 我必须承认我对这次谈话有点紧张。 ​ 我怀疑对 ML 功能抱有不切实际的期望是一种常见的经历。其他人如何处理这个问题？如果他们坚持不管，你会直白地告诉他们这行不通，然后到别处找工作吗？如果是这样，这些交互通常如何进行？   由   提交 /u/Excusemyvanity   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19882l2/d_how_do_you_deal_with_unreasonable_request_from/</guid>
      <pubDate>Tue, 16 Jan 2024 17:13:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] APAR：法学硕士可以进行自动并行自动回归解码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1987em3/r_apar_llms_can_do_autoparallel_autoregressive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.06761 摘要：  大型语言模型（LLM）的大规模采用需要高效部署策略。然而，自回归解码过程是大多数法学硕士生成文本的基础，它对实现高效服务提出了挑战。在这项工作中，我们介绍了一种并行自回归生成方法。通过对包含层次结构的通用领域数据进行指令调整，我们使法学硕士能够独立规划其生成过程并执行自动并行自回归（APAR）生成，从而显着减少生成步骤的数量。单独使用 APAR 即可实现高达 2 倍的加速，与推测解码结合使用时，加速可高达 4 倍。此外，APAR 减少了生成过程中的键值缓存消耗和注意力计算。与最先进的服务框架相比，这使得高吞吐量场景中的吞吐量提高了 20-70%，延迟降低了 20-35%。     由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1987em3/r_apar_llms_can_do_autoparallel_autoregressive/</guid>
      <pubDate>Tue, 16 Jan 2024 16:46:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1986k0x/r_transformers_are_multistate_rnns/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.06104 代码：https ://github.com/schwartz-lab-NLP/TOVA 摘要：  Transformers 被认为在概念上与到上一代最先进的 NLP 模型 - 循环神经网络 (RNN)。在这项工作中，我们证明了仅解码器 Transformer 实际上可以被概念化为无限多状态 RNN——一种具有无限隐藏状态大小的 RNN 变体。我们进一步证明，通过固定隐藏状态的大小，预训练的 Transformer 可以转换为有限多状态 RNN。我们观察到一些现有的转换器缓存压缩技术可以被构建为这样的转换策略，并引入了一种新的策略，TOVA，它比这些策略更简单。我们对多个远程任务进行的实验表明，TOVA 优于所有其他基线策略，同时几乎与完整（无限）模型相当，并且在某些情况下仅使用原始缓存大小的 1/8。我们的结果表明，变压器解码器 LLM 在实践中通常表现为 RNN。他们还提出了缓解最痛苦的计算瓶颈之一——缓存大小的选项。我们在 此 https URL 公开发布我们的代码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1986k0x/r_transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 16:12:24 GMT</pubDate>
    </item>
    <item>
      <title>[P] 📢 使用 Fondant 自动 RAG 优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19826qq/p_automated_rag_optimization_with_fondant/</link>
      <description><![CDATA[大家好， 我们通过名为“检索增强生成”（RAG）的自定义数据处理框架分享了一些关于检索增强生成（RAG）的实用见解我们最新博文中的软糖。 微调 RAG 是一项复杂的任务，需要大量时间和精力。我们构建了一个示例管道，用于索引自定义知识库（PDF、Huggingface 数据集等）、处理数据（嵌入、分块等）并评估结果。我们集成了不同的参数搜索技术来选择最佳配置，从而为您的 RAG 系统带来最佳结果。 为了构建管道，我们利用了 Fondant，这是一个开源框架数据处理框架，可以简化流程通过提供可重用组件来构建数据管道。它具有一系列功能，可以轻松开发和扩展管道，例如本地测试、内置云兼容性、缓存等。 查看以下资源：  📖 阅读博客文章 - https://medium .com/fondant-blog/lets-tune-rag-pipelines-with-fondant-902f7215e540 🔗 Medium 上的翻糖博客 - https://medium.com/fondant-blog 📂 RAG GitHub 存储库 - https://github.com/ml6team/fondant-usecase-RAG 📂 Fondant GitHub存储库 - https://github.com/ml6team/fondant 让我们知道什么如果您有任何问题或反馈，请随时在 Discord 上或在下面的评论中与我们联系。   由   提交 /u/East_Dragonfruit7277   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19826qq/p_automated_rag_optimization_with_fondant/</guid>
      <pubDate>Tue, 16 Jan 2024 12:50:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] iPhone 文本检测的有趣现象</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197zbzs/d_interesting_occurrence_about_text_detection_of/</link>
      <description><![CDATA[      我点击该图像几次，它检测到第二只狗是单词“dog”用中文写的。我不认为这是有原因的，但如果有人有任何想法，我很乐意听到他们。   由   提交/u/Ok_Care_886   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197zbzs/d_interesting_occurrence_about_text_detection_of/</guid>
      <pubDate>Tue, 16 Jan 2024 09:57:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于直接偏好优化（DPO）方程的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197yl66/d_question_about_direct_preference_optimization/</link>
      <description><![CDATA[      这是（直接偏好优化）DPO 的损失： https://preview.redd.it/6ubjn8ekprcc1.png?width=1324&amp;format=png&amp; ;auto=webp&amp;s=c932f5c030c2fb6b5f0f136934b047bc364d1dcc 我不明白 pi\_ref 的除法（对于 y\_w 和 y\_l）。我知道目的是微调后的模型不会偏离参考模型太远，但只要从数学上看 - 为什么 pi\_ref(y\_w|x) 应该接近 pi\_theta(y\_w |x)? 至少对于 y\_w 来说，损失似乎会受益于 pi\_ref(y\_w|x) 尽可能接近 0，因为我们希望最大化左侧部分 我错过了什么？ 谢谢。   由   提交/u/erap129  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197yl66/d_question_about_direct_preference_optimization/</guid>
      <pubDate>Tue, 16 Jan 2024 09:05:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 深度学习最好的进阶书籍？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/</link>
      <description><![CDATA[我遇到的几乎所有书籍都是从头开始写的。他们是否有深入探讨主题的地方？   由   提交/u/toxfu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/</guid>
      <pubDate>Tue, 16 Jan 2024 02:12:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对强化学习的真实体验是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/197jp2b/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[根据我个人的经验，SOTA RL 算法根本不起作用。我尝试强化学习已有 5 年多了。我记得当 Alpha Go 击败世界著名围棋棋手 Lee Sedol 时，每个人都认为 RL 会席卷 ML 社区。然而，除了玩具问题之外，我个人从未找到过 RL 的实际用例。 您对此有何体验？除了广告推荐系统和 RLHF 之外，RL 是否还有合法的用例？或者，这都是炒作吗？ 编辑：由于我的评论被否决，这里是我的文章的链接可以更好地描述我的立场。 这并不是说我不理解强化学习。我发布了我的开源代码和就此写了一篇论文。 事实上，它非常难以理解。其他深度学习算法，如 CNN（包括 ResNet）、RNN（包括 GRU 和 LSTM）、Transformers 和 GAN 并不难理解。这些算法可以工作，并且在实验室外有实用用例。 传统的 SOTA RL 算法（如 PPO、DDPG 和 TD3）非常困难。即使解决一个玩具问题，你也需要做大量的研究。相比之下，决策转换器是任何人都可以实现的东西，而且它似乎匹配或超越了 SOTA。您不需要两个网络相互竞争。您不必经历地狱般的调试网络。它只是自然地以自回归的方式学习最好的一组动作。 我也不是故意显得傲慢或暗示强化学习不值得学习。我只是还没有看到它在现实世界中的任何实际用例。我只是想开始讨论，而不是声称我什么都知道。 编辑 2：令人震惊的是，有很多人因为我没有完全理解 RL 而称我为白痴。你们太自在了称呼不同意的人的名字。新闻快讯，并不是每个人都拥有机器学习博士学位。我的本科学位是生物学。我自学了高级数学来理解机器学习。我对这个领域充满热情；我在强化学习方面的经历非常令人失望。 有趣的是，很少有人反驳我的实际观点。总结一下：  缺乏实际应用 极其复杂且 99% 的人无法使用 比 CNN、RNN 和 GAN 等传统深度学习算法困难得多 样本效率低下且不稳定 难以调试 更好的替代方案，例如决策转换器&lt; /li&gt;  这些难道不是合理的批评吗？本子的目的不是要讨论与机器学习相关的问题吗？ 致少数没有称我为白痴的评论者……谢谢！请记住，你不需要付出任何代价就能变得友善！ 编辑 3：很多人似乎都认为 RL 被过度炒作了。不幸的是，这些评论被否决了。澄清一些事情：  我们在强化学习上投入了大量资金。我们从这项投资中得到的只是一个可以在（某些）视频游戏中超越人类的机器人。 AlphaFold 没有使用任何强化学习。 SpaceX 也没有。  我承认它对机器人技术很有用，但仍然认为它在实验室之外的用例极其有限。  如果您无意中发现了这条线索并对 RL 替代方案感到好奇，请查看决策转换器。它可以用于任何可以使用传统强化学习算法的情况。 最终编辑：对于那些最近做出贡献的人，感谢你们深思熟虑的讨论！据我所知，像 Dreamer 和 IRIS 这样基于模型的模型可能会有未来。但每个真正使用过像 DDPG 这样的无模型模型的人都一致认为它们很糟糕而且不起作用。   由   提交 /u/Starks-Technology   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/197jp2b/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 20:56:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>