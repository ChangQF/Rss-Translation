<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 22 Feb 2024 06:17:49 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么研究人员很少发布训练代码？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awy07v/d_why_do_researchers_so_rarely_release_training/</link>
      <description><![CDATA[我现在正在查看针对各种 MoE 模型的 3 篇不同论文。这三个都发布了模型权重和推理代码，但都没有发布训练代码。  当我们期望现在大多数论文都有代码及其实现时，为什么这种情况如此普遍和被接受？   由   提交/u/hazard02  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awy07v/d_why_do_researchers_so_rarely_release_training/</guid>
      <pubDate>Thu, 22 Feb 2024 04:59:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将向量二进制掩码转换为矩阵</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awxakt/d_converting_vector_binary_mask_to_matrix/</link>
      <description><![CDATA[我正在研究二进制图像分割，我有图像及其二进制掩码，在预处理期间我已使用 tf.kera.utils 将 Maks 转换为矩阵.to_categorical（掩码，2）。它可以帮助我的模型（unet 模型）表现更好吗？我应该做什么？   由   提交/u/NailaBaghir   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awxakt/d_converting_vector_binary_mask_to_matrix/</guid>
      <pubDate>Thu, 22 Feb 2024 04:20:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 测量数据集的复杂性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awwr4d/d_measuring_complexity_of_datasets/</link>
      <description><![CDATA[我正在启动一个项目，我们的目标是证明我所在领域的许多数据集比更通俗的 ML 数据集（UCI、图像、 ETC。）。我们实验室的多项结果表明，某些算法在我们的领域中运行得非常好，而在外部则不太好，反之亦然——一些在更通用的 ML 数据集中运行良好的算法效果不佳，或者计算量更大与我所在领域的方法获得相同结果的成本很高。 作为第一步，我们已经证明许多数据集的损失函数的平滑度远低于标准 ML 数据集。我还想探索其他方法来提供更全面的观点。因此，我想知道以下数学工具是否可以共同用于解释数据集的复杂程度：  损失的预期平滑度 损失的预期 Lipschitz 常数 Mapper 算法（TDA 中）或 Betti 曲线的结果 损失的强凸性（强凸性可以证明等于损失的锐度，并且平坦的最小值已被证明可以更好地概括）  这些有意义吗？据我所知，它们单独只显示了图片的一部分，但它们一起应该提供数据复杂性的更全面的视图。   由   提交 /u/FlyingQuokka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awwr4d/d_measuring_complexity_of_datasets/</guid>
      <pubDate>Thu, 22 Feb 2024 03:51:47 GMT</pubDate>
    </item>
    <item>
      <title>有谁知道一个好的社区支持的人工智能语音生成器？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awvydl/does_anyone_know_a_good_community_supported_ai/</link>
      <description><![CDATA[寻找包含大量社区生成的虚构声音的内容，有点像 uberduck.ai 在取消所有社区内容之前的情况。   由   提交 /u/ProBoyGaming521   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awvydl/does_anyone_know_a_good_community_supported_ai/</guid>
      <pubDate>Thu, 22 Feb 2024 03:12:04 GMT</pubDate>
    </item>
    <item>
      <title>[P]txtai 7.0发布：增加对图搜索、高级图遍历和图RAG的支持</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awv8u0/p_txtai_70_released_adds_support_for_graph_search/</link>
      <description><![CDATA[txtai 是一个用于语义搜索、LLM 编排和语言模型工作流程的一体化嵌入数据库。 此主要版本介绍了下一代语义图。它增加了对图搜索、高级图遍历和图 RAG 的支持。它还为 API 添加了二进制支持、索引格式改进和训练 LoRA/QLoRA 模型。 请参阅下文了解更多信息。 GitHub：https://github.com/neuml/txtai 发行说明：https://github.com/neuml/txtai/releases/tag/v7.0.0 文章：https://medium.com/neuml/whats-new-in-txtai-7-0-855ad6a55440    由   提交/u/davidmezzetti   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awv8u0/p_txtai_70_released_adds_support_for_graph_search/</guid>
      <pubDate>Thu, 22 Feb 2024 02:38:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你如何保持自己的最新状态？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awq5d2/d_how_do_you_keep_yourself_updated/</link>
      <description><![CDATA[我想阅读我感兴趣的领域的最新研究成果，但我不知道该去哪里寻找。那么，你该怎么做呢？欢迎任何建议，谢谢！   由   提交 /u/zero_redditer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awq5d2/d_how_do_you_keep_yourself_updated/</guid>
      <pubDate>Wed, 21 Feb 2024 22:55:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将模型集成到现有生产系统中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awoqex/d_integrating_models_into_existing_production/</link>
      <description><![CDATA[我是一名软件工程师，第一次在 MLOP 团队工作。 我们有一位数据科学家，负责构建模型并将其部署到 http 端点上的 Databricks。我们的工程团队所做的就是构建与他的模型集成的服务。数据流基本上是：  发生异步事件，触发我们的一个 lambda。 lambda 摄取事件并从另一个系统 (S3) 获取一些相关信息. lambda 将聚合的特征发送到 Databricks 端点以进行推理。 lambda 将预测存储在 postgres 中，并将包含预测的事件发送到另一个团队的系统以显示给用户。  在这个团队工作时，我观察到了一些事情：  我们的数据科学家通常准备好部署 6 个月的 ML 模型在我们能够将其用作组织之前（一般来说，工程非常耗时）。 我们一遍又一遍地做同样的工作，使用预测日志数据库进行异步处理。  我想知道：  我们正在构建的架构有多常见？ 您将 ML 模型集成到生产中的经验是什么？系统？ 您的具体集成用例是什么？    由   提交/u/Turbulent_Cat_5827   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awoqex/d_integrating_models_into_existing_production/</guid>
      <pubDate>Wed, 21 Feb 2024 22:00:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有人注意到 FP32 和 FP16 型号之间惊人的巨大差异吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awn2t5/d_anyone_else_notice_a_surprisingly_big/</link>
      <description><![CDATA[在我工作的公司，我们运行图像分类服务。该模型相当简单：我们只有一个预先训练的图像编码器主干模型，我们用自己的数据进一步对其进行了预训练，并且我们为每个“任务”都有一个完全连接的层。 “任务”是一个“任务”。可能类似于对狗的图像进行分类或对图像的颜色进行分类。 我最近更改了我的代码以使用 HuggingFace 的 Accelerate 模块而不是 PyTorch 的本机 DDP，并且还使用混合精度训练来训练我的模型将权重存储在 FP16 而不是 FP32 中。我将主干层和全连接层冻结用于原始任务，只想训练新的全连接层。然而，问题是我注意到冻结分支做出的预测存在显着差异。 在调查了所有可能出错的地方，甚至手动检查 IPython 终端中的权重值后，我注意到主要区别在于之前模型的权重存储在 FP32 中，而我新训练的模型权重存储在 FP16 中。 我听说过 FP16 本质上是速度和性能的权衡的故事，但我从来不知道这一点那就完全不同了。好奇是否有其他人经历过类似的情况。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awn2t5/d_anyone_else_notice_a_surprisingly_big/</guid>
      <pubDate>Wed, 21 Feb 2024 20:54:41 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 您的 ML 技术堆栈是什么样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awmnt7/dr_what_does_your_ml_tech_stack_look_like/</link>
      <description><![CDATA[有许多用于深度学习模型训练和推理的库。  你的训练技术堆栈是什么样的？ 例如，我大量使用huggingface生态系统库，很少需要导入这些库或普通旧火炬之外的东西。    由   提交/u/iordanis_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awmnt7/dr_what_does_your_ml_tech_stack_look_like/</guid>
      <pubDate>Wed, 21 Feb 2024 20:37:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 《Scalable Diffusion Models with Transformers》一书的两位作者之一关于 OpenAI Sora 的 Twitter/X 帖子：“这是我对 Sora 技术报告的看法，其中包含大量可能完全错误的猜测。[。 ..]”。这项工作的另一位作者是 OpenAI 的 Sora。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awmces/d_twitterx_thread_about_openais_sora_from_one_of/</link>
      <description><![CDATA[展开的 Twitter/X 线程。 线程中的第一条推文，我通过Yann LeCun 的这条推文。  这是我对 Sora 技术报告的看法，其中有大量可能完全错误的猜测。首先，非常感谢团队分享有用的见解和设计决策 - Sora 令人难以置信，并将改变视频生成社区。  到目前为止我们学到了什么： - 架构：Sora 是基于我们的扩散变压器 (DiT) 模型（在 ICCV 2023 中发布）构建的 - 它是具有变压器骨干的扩散模型，简而言之： DiT = [VAE 编码器 + ViT + DDPM + VAE 解码器]。  根据报告，似乎没有太多额外的花哨的东西。  [...]  可扩展使用变压器的扩散模型。 Sora 技术报告. 来自该作品另一位作者的推文：  索拉来了！它是一个扩散变压器，可以生成长达一分钟的 1080p 视频，具有良好的连贯性和质量。 @ /_tim_brooks 和我在 @ /openai 已经为此工作了一年，我们对通过模拟一切来追求 AGI 感到兴奋！ http://openai.com/sora  相关帖子：[D] OpenAI Sora Video Gen - 怎么样？ &lt;!-- SC_ON - -&gt;  由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awmces/d_twitterx_thread_about_openais_sora_from_one_of/</guid>
      <pubDate>Wed, 21 Feb 2024 20:24:46 GMT</pubDate>
    </item>
    <item>
      <title>[N] 语言处理单元 (LPU) 使 LLM 的推理速度提高 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</link>
      <description><![CDATA[本周，一家鲜为人知的公司 Groq 展示了运行 Llama-2 等开源 LLM 的前所未有的速度（ 700 亿个参数），每秒超过 100 个令牌，而 Mixtral 在 Groq 的语言处理单元 (LPU) 上每用户每秒近 500 个令牌。 对于比较：   “根据 Groq 的说法，在类似的测试中，在典型的基于 GPU 的计算系统上，ChatGPT 的加载速度为每秒 40-50 个令牌，而 Bard 的加载速度为每秒 70 个令牌。 每个用户每秒 100 个令牌的上下文，展示了在 Groq 语言处理上以每秒超过 100 个令牌的速度运行开源 LLM 的前所未有的速度，例如 Llama-2（700 亿个参数），以及每秒每用户近 500 个令牌的 Mixtral单位（LPU）。  那么：LPU是什么，它是如何工作的，Groq在哪里（这么不幸的名字，考虑到马斯克的Grok都结束了）媒体）来自哪里？ https://www.turingpost.com/p/fod41    由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</guid>
      <pubDate>Wed, 21 Feb 2024 18:32:13 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 研究人员（硕士、博士）如何实现复杂模型？他们是神吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</link>
      <description><![CDATA[我现在正在做我的论文。我很好地掌握了大多数 ML 模型（RNN、CNN、LSTM、Transformers、GPT、CNN、GAN、LDM、VAE、自动编码器等）的高级细节。当然，我绝不是专家，但我能够学习我需要的东西。 但是当真正使用它们，并在代码中实现它们并训练它们时，这就变成了地狱。对于更简单的模型，还好，但是对于更复杂的模型，网上没有教程，他们只是说“使用现有模型”。 世界各地的研究人员如何实现复杂的模型？例如，扩散模型、LDM 或修改后的 LLM，如 Transformer 或 GPT？ 或者它们如何更改现有模型，并使用不同的技术，例如添加编码器进行调节？  &gt;就像，研究和理解基础知识很好，但实际实施起来却非常困难。他们是如何做到如此优雅的？一些调查研究论文包括多种模型的使用和比较。他们是如何做到的？   由   提交 /u/ShlomiRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</guid>
      <pubDate>Wed, 21 Feb 2024 15:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[新闻]Google发布全新开放的LLM模型：Gemma模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</link>
      <description><![CDATA[明显比 llama7 和 13 更好（但不与 Mistra7b 进行基准测试）： https://blog.google/technology/developers/gemma-open-models/   由   提交 /u/edienemis   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</guid>
      <pubDate>Wed, 21 Feb 2024 13:28:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果一篇论文没有可用的开源代码，您是否可以为了乐趣/练习而实现该代码，并将其发布在您自己的 Github 上并附上适当的引用，并注明所有功劳均归作者所有？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awaeo0/d_if_a_paper_has_no_open_source_code_available/</link>
      <description><![CDATA[大家好， 我在发表于 的一篇论文中找到了物理数值计算的 ML 实现的描述具有 CC4 许可证的 arXiv（除了 arXiv 之外，它还发表在期刊上，但我只查看 arXiv 版本）： 您可以自由地：共享 — 复制并重新分发以下内容用于任何目的的任何媒体或格式，甚至是商业目的。改编——为任何目的（甚至商业目的）重新混合、转换和构建材料。只要您遵守许可条款，许可方就不能撤销这些自由。根据以下条款： 归属 - 您必须给出适当的信用，提供许可证的链接，并注明是否进行了更改。您可以以任何合理的方式这样做，但不得以任何暗示许可方认可您或您的使用的方式。相同方式共享 — 如果您对材料进行重新混合、转换或构建，则必须在与原始材料相同的许可下分发您的贡献。无额外限制 — 您不得应用法律条款或技术措施来合法限制他人执行许可证允许的任何操作。  据我所知，该论文没有开源代码。 如果我尝试编写自己的实现，从学术行为角度来看是否有任何问题？论文中的 ML 内容并将其发布到我自己的 Github 上？当然，我会引用这篇论文，并说这个项目中的所有内容都是基于该论文。 我真的不想联系作者来要求诚实。我想知道无论是否联系作者都可以这样做。想象一下，每次您想做类似的事情时都必须这样做（例如，练习实现“注意力就是您所需要的一切”）。 非常感谢！   由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awaeo0/d_if_a_paper_has_no_open_source_code_available/</guid>
      <pubDate>Wed, 21 Feb 2024 12:02:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>