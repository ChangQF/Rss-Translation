<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 09 Apr 2024 03:16:22 GMT</lastBuildDate>
    <item>
      <title>[D] 就 RAG 研究而言，为什么似乎很多人没有致力于猎犬的研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</link>
      <description><![CDATA[我是几年前进行 NLP 研究的人，后来停止并加入了行业，最近试图重新掌握事物。我对 RAG 相关的工作很感兴趣，并开始阅读一些论文。 我的理解是，对于 RAG，你有检索器和生成器。对于生成器来说，使用各种 LLM 似乎是标准的，但检索器似乎也设置为使用 BM25 或最初使用的 DPR 之类的东西。我认为 RAG 的性能将在很大程度上依赖于检索器，但我也有点惊讶地发现似乎没有在这方面进行大量研究。 我只是错误并且没有看向正确的方向？或者说，检索器似乎没有得到那么多关注是有什么原因吗？ 想想看，我并没有真正看到编码器模型总体上做了很多工作。    由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</guid>
      <pubDate>Tue, 09 Apr 2024 01:38:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 高效扩散模型中缺失的 U</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</link>
      <description><![CDATA[一篇新论文提出用利用神经常微分方程的连续 U-Net 取代扩散模型中的标准离散 U-Net 架构。这种重新表述可以对去噪过程进行连续建模，从而显着提高效率：  推理速度提高 80% 模型参数减少 75% 70% 保持或提高图像质量  关键技术贡献：  动态神经 ODE 模块建模潜在表示演化使用二阶微分方程 自适应时间嵌入来调节扩散时间步长的动力学 高效的 ODE 求解器和常量内存伴随方法，可实现更快、内存效率更高的训练 &lt; /ul&gt; 作者展示了这些在图像超分辨率和去噪任务上的改进，并通过详细的数学分析解释了为什么连续公式会导致更快的收敛和更有效的采样。 潜在影响： p&gt;  使扩散模型适用于更广泛的应用（实时工具、资源受限设备） 在深度学习、微分方程、动力学的交叉领域开辟新的研究方向系统  存在一些局限性：(1) ODE 求解器和伴随方法增加了复杂性，(2) 我认为即使进行了改进，扩散模型仍然可能需要大量计算。 &lt; p&gt;完整摘要此处。 Arxiv 此处。 TL;DR：新论文建议替换离散 U-使用神经 ODE 的连续 U-Net 扩散模型中的网络，可将推理速度提高 80%、参数减少 75%、FLOP 减少 70%，同时保持或提高图像质量。主要影响：更高效、更容易理解的生成模型、连续时间深度学习的新研究方向。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</guid>
      <pubDate>Tue, 09 Apr 2024 01:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] LightGBM 算法问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</link>
      <description><![CDATA[GBDT 算法帮助 嘿，我知道创建传统决策树时需要某种损失函数，例如均方误差 - 你扫描预测空间并找到最小化 MSE 的分割，这就是递归二元分割所实现的，直到达到某种停止标准。我也明白，如果要进行分割，您可以使用该区域中所有预测的平均值找到每个区域的新 MSE，这就是您如何确定是否进行分割的方法。 我目前正在学习提升，现在明白这个过程是相似的，但我们现在基于残差构建一棵树。过程完全相同吗？ 我一直在观看一些 statquest，这是 boosting 的通用算法 https://i.imgur.com/DudpZ5S.png 我正在努力理解 B) 和 C) 之间的区别。在 B 中，我们通过基于一些损失函数（如 MSE）进行递归二元分割来将回归树拟合到残差值（r_i_m）？但是对于 C 部分，我们在节点（称为 gamma）计算这些残差，这也最小化了损失函数？这不是我们在 B 部分所做的吗，因为我们取每个分割点残差的平均值，看看它是否最小化了我们的损失函数。 正如你所知，有点困惑，谢谢- 感谢任何帮助！   由   提交 /u/PencilSpanker   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</guid>
      <pubDate>Mon, 08 Apr 2024 23:05:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的研究技术堆栈是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</link>
      <description><![CDATA[我计划进行文本+音频的大型多模态训练（1B 参数）。截至目前，我正在考虑使用 pytorch、deepspeed、wandb。对于分布式大型模型训练，您有什么建议以及一般使用什么？ 您使用 Hughginface 吗？我觉得它有点太包裹了，以至于接触到裸露的主干会变得混乱，但还没有进行适当的尝试。对于现成的模型和自定义数据集训练，这听起来确实很有用，但研究需要的不仅仅是这些。那么，您在研究方面的经验如何，您需要灵活地改变模型？一般来说，您在研究方面的技术堆栈是什么？   由   提交/u/gokulPRO  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</guid>
      <pubDate>Mon, 08 Apr 2024 20:38:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] LegalKit 检索，使用标量 (int8) 的二分搜索通过法国法律代码重新评分</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz673f/p_legalkit_retrieval_a_binary_search_with_scalar/</link>
      <description><![CDATA[      此空间展示了 Louis Brulé Naudet 的 tsdae-lemone-mbert-base 模型，这是一个句子嵌入模型基于 BERT，使用基于 Transformer 的序列去噪自动编码器进行无监督句子嵌入学习，其目标只有一个：法国法律领域适应。 这一过程旨在提高内存效率和速度，二进制索引为小到足以容纳内存，并且 int8 索引作为视图加载以节省内存。此外，二进制索引的搜索速度比 float32 索引快得多（高达 32 倍），而重新评分也非常高效。 链接到 🤗 Space ：https://huggingface.co/spaces/louisbrulenaudet/legalkit-retrieval LegalKit 检索缩略图。  &amp; #32；由   提交/u/louisbrulenaudet  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz673f/p_legalkit_retrieval_a_binary_search_with_scalar/</guid>
      <pubDate>Mon, 08 Apr 2024 19:02:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM比较和参数调优的OSS工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</link>
      <description><![CDATA[      我最初将此工具编写为 CLI 应用程序，用于使用网格搜索测试推理参数的组合（因此名称 Ollama 网格搜索)。 https://preview.redd.it/vbgh6vbhpac1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=16687c2e9077348 c5e082ef85c80820c46c83ef9 以下是它的一些功能： ​  自动从本地或远程 Ollama 服务器获取模型； 迭代不同的模型和参数以生成推理； 同时对不同模型进行 A/B 测试提示；  进行同步推理调用以避免向服务器发送垃圾邮件； 可选择输出推理参数和响应元数据（推理时间、令牌和令牌）；&lt; /li&gt; 重新获取单个推理调用； 可以按名称过滤模型选择； 列出可以以 JSON 格式下载的实验格式； 可配置的推理超时； 可以在设置中定义自定义默认参数和系统提示   ​ 大多数主要平台的源代码和（未签名）版本可在以下位置获取： https://github.com/dezoito/ollama-grid-search  它仍在进行中......我计划添加更多功能，但我我正在利用业余时间做这件事。  希望这对使用开源模型的人们有用   由   提交 /u/grudev   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</guid>
      <pubDate>Mon, 08 Apr 2024 18:04:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 促进离策略学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</link>
      <description><![CDATA[        由   提交 /u/ggyshay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</guid>
      <pubDate>Mon, 08 Apr 2024 16:29:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于那些独自发表文章的人来说，你们的经历是怎样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</link>
      <description><![CDATA[这些天我有一些空闲时间，一直在努力赶上我所在领域的研究。我想真正重新审视我在硕士期间正在研究的一个主题，但始终无法发表论文。问题是，我不确定作为唯一作者，如果没有任何真正的资源访问权限，这是否可行。 朋友和熟人告诉我这是可能的，但极其困难。很好奇其他成功做到这一点的人是怎么想的。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</guid>
      <pubDate>Mon, 08 Apr 2024 14:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] DBRX 是专门为企业设计的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</link>
      <description><![CDATA[我了解到 Databricks 推出了名为 DBRX 的新通用 LLM。我很好奇它与其他法学硕士有何不同（除了开源之外）？ 它是专门为企业还是供公众使用而设计的，例如 chatgpt？或者它是否像“企业聊天”？他们可以在哪里微调开源模型以满足他们的需求？   由   提交/u/Ok_Moment4946   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</guid>
      <pubDate>Mon, 08 Apr 2024 13:22:40 GMT</pubDate>
    </item>
    <item>
      <title>“clip-vit-large-patch14”如何将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 代币，但有 [SOT] 和 [EOT] 代币。 [研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</link>
      <description><![CDATA[大家好， 我有以下问题： “clip-vit-large-patch14”如何;将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 标记，但有 [SOT] 和 [EOT] 标记。 当我使用 CLIP 文本编码器并提取 pooler_output 时……这个向量到底是如何创建的？ [SOT] 代币是否用作 [CLS] 代币？或者是否进行了池化操作？ [研究] 此致， Tom  &amp; #32；由   提交/u/tommilyjonesOG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</guid>
      <pubDate>Mon, 08 Apr 2024 13:09:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 手写文本到文本项目 - 寻求提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</link>
      <description><![CDATA[大家好，我有一个朋友，他有自己的生意，他经常告诉我他手动抄写帐单浪费了多少时间从手写纸滑到他电脑上的文档。我不是专业人士，我仍在学习，但我想知道是否可以使用已经训练好的模型应用一些迁移学习，然后使用新数据  有谁知道是否有一些CNN已经用于此类项目？ 使用什么样的激活函数？感谢您的关注，并提前感谢您的回复:)    由   提交/u/December92_yt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</guid>
      <pubDate>Mon, 08 Apr 2024 10:46:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 确保加拿大的人工智能优势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</guid>
      <pubDate>Mon, 08 Apr 2024 09:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于高性能语言技术的新的海量多语言数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.14009 项目页面：https://hplt -project.org/ 数据集：https:// /hplt-project.org/datasets/v1.2 GitHub： https://github.com/hplt-project 摘要：  我们介绍HPLT（高性能语言技术）语言资源，一个新的大规模多语言数据集，包括从 CommonCrawl 中提取的单语和双语语料库以及从互联网档案馆中提取的以前未使用的网络爬虫。我们描述了大型语料库的数据采集、管理和处理方法，这些方法依赖于开源软件工具和高性能计算。我们的单语集合侧重于中低资源语言，涵盖 75 种语言，并在文档级别删除了总共约 5.6 万亿个单词标记。我们以英语为中心的平行语料库源自其单语对应语料库，涵盖 18 个语言对和超过 9600 万个对齐句子对，以及大约 14 亿个英语标记。 HPLT 语言资源是迄今为止发布的最大的开放文本语料库之一，为语言建模和机器翻译培训提供了丰富的资源。我们公开发布了这项工作中使用的语料库、软件和工具。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</guid>
      <pubDate>Mon, 08 Apr 2024 08:56:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们知道Gemini 1.5是如何实现10M上下文窗口的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</link>
      <description><![CDATA[我们知道 Gemini 1.5 是如何实现 1.5M 上下文窗口的吗？随着注意力窗口的扩大，计算量不会呈二次方增长吗？    由   提交/u/papaswamp91  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</guid>
      <pubDate>Sun, 07 Apr 2024 16:21:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>