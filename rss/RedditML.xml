<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Thu, 01 Aug 2024 12:29:11 GMT</lastBuildDate>
    <item>
      <title>[R] 长序列模型对长序列的建模效果如何？比较架构归纳偏差对长语境能力的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ehej2y/r_how_well_can_a_long_sequence_model_model_long/</link>
      <description><![CDATA[      TL;DR 小语言模型举步维艰无论架构如何，都可以处理长上下文。 论文： https://arxiv.org/pdf/2407.08112 摘要：  长序列在现实世界场景中大量出现，因此正确建模它们会打开许多​​下游用例。然而，深度神经网络经常因为各种原因而难以处理这些问题。系统工程和模型设计方面的最新进展使得模型的扩展成为可能，这些模型据称可以支持扩展的上下文长度。特别是，状态空间和线性递归神经网络模型系列理论上可以延伸到无限的序列长度。然而，这是否好得令人难以置信？我们进行了评估，以表明虽然这种说法在理论上可能是合理的，但仍然存在经验观察到的巨大实际差距。具体而言，循环模型在与具有注意机制的长上下文 LLM 相同的设置下仍然会受到影响。我们进一步表明，不同的归纳偏差具有不一致的外推能力，这凸显了进一步研究此类范式的必要性，并调查为什么长上下文模型似乎无法按照预期的方式运行。  实证结果： M2A = Mamba2Attn，TPP = Transformer++，RG &amp; RG-IT = Recurrent Gemma（/指令调整），SL &amp; SL-IT= ShearedLLaMa（/指令调整）。每个模型有2.7B到3B个参数。 https://preview.redd.it/d9pth8skg1gd1.png?width=759&amp;format=png&amp;auto=webp&amp;s=3df65d368ec3f0fbec93a866bb42cb6f83227c5b https://preview.redd.it/djez9l6ph1gd1.png?width=923&amp;format=png&amp;auto=webp&amp;s=c7a7e12f7c42bfba23372de379b63036c4db9800 https://preview.redd.it/fveuwywph1gd1.png?width=927&amp;format=png&amp;auto=webp&amp;s=1e0e7fa1c03099c09bccd4bed164f8bea13d5807 编辑：修复格式，添加模型尺寸信息。    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ehej2y/r_how_well_can_a_long_sequence_model_model_long/</guid>
      <pubDate>Thu, 01 Aug 2024 11:41:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何正确对 DTLN（双信号变换 LSTM 网络）进行 TFLite 完全整数量化？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ehe9or/d_how_to_correctly_do_tflite_fully_integer/</link>
      <description><![CDATA[      我试图像 breizhn/DTLN 一样进行 TFLite 训练后量化练习。 作者进行了默认量化（float32）并将其分为 TFlite 模型的 2 个阶段，因为 TF2.3 不能很好地支持复值。 然后我尝试使用 TF2.15 并从“DTLN/pretrained_model/dtln_saved_model”加载他保存的模型并尝试进行全整数量化。错误显示如下，我不确定如何修复。 3 个节点已委托，需要 Flex ops 吗？ 在寻求帮助的过程中，我发现有人做了这项工作并分享了 https://github.com/nyadla-sys 、TFLite 模型。 模型图更加简洁与原始的 2 阶段 TFLite 模型进行比较，而作者 nyadla-sys 没有谈论如何做到这一点或分享 python 脚本。 我想知道他是怎么做到的，我不确定这项工作是否需要其他技能，因为我只知道基本的转换步骤。 希望有人可以在这里帮忙，谢谢。    提交人    /u/Ok_Box_6059   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ehe9or/d_how_to_correctly_do_tflite_fully_integer/</guid>
      <pubDate>Thu, 01 Aug 2024 11:26:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 计算机视觉领域最需要的数据是什么？我会去收集并开源它们</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ehanya/d_what_is_the_most_in_demand_data_in_computer/</link>
      <description><![CDATA[大家好，我想做一些数据收集项目。我想知道目前需要什么类型的图像或视频数据。所以在评论中列出一些你最想要的数据，我会免费收集并开源获得最多点赞的数据    提交人    /u/Limp-Run-9075   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ehanya/d_what_is_the_most_in_demand_data_in_computer/</guid>
      <pubDate>Thu, 01 Aug 2024 07:30:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习论坛</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh8d35/d_forum_for_machine_learning/</link>
      <description><![CDATA[大家好， 我想知道是否有一个论坛或在线社区（当然除了这个 subreddit 之外），人们可以在那里讨论机器学习、分享他们的观点/资源等。我一直没能找到，我觉得我们应该有类似的东西。类似老式论坛页面风格的东西。或者它在 2024 年不再是一个流行的平台了？ 你怎么看？    提交人    /u/satori_paper   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh8d35/d_forum_for_machine_learning/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有人觉得 LLM 没什么意思吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/</link>
      <description><![CDATA[我不是 ML 研究员。当我想到很酷的 ML 研究时，我想到的是 OpenAI Five 或 AlphaFold 之类的东西。如今，人们热议的是 LLM 和扩展转换器，虽然该领域确实有一些研究和优化要做，但它对我来说并不像其他领域那么有趣。对我来说，ML 的有趣部分是为您的用例端到端训练模型，但如今的 SOTA LLM 可以用于处理许多用例。好的数据 + 大量的计算 = 不错的模型。就这样？ 如果我可以用一小部分计算来训练这些模型，我可能会更感兴趣，但这样做是不合理的。那些没有计算能力的人只能进行微调或快速工程，而我内心的 SWE 发现这很无聊。这个领域的大多数人真的把精力投入到下一个标记预测器中了吗？ 显然，LLM 具有颠覆性，并且已经发生了很大的变化，但从研究的角度来看，它们对我来说并不有趣。还有人有这种感觉吗？对于那些因为与 LLM 无关的东西而被该领域吸引的人，你对此有何感想？你是否希望 LLM 的炒作会逐渐消退，以便焦点可以转移到其他研究上？那些在当前趋势之外进行研究的人：你如何处理所有的噪音？    提交人    /u/leetcodeoverlord   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/</guid>
      <pubDate>Thu, 01 Aug 2024 01:40:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] Google 的 Gemma-2-2B 与 Microsoft Phi-3：医疗保健领域小型语言模型的比较分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh3clp/d_googles_gemma22b_vs_microsoft_phi3_a/</link>
      <description><![CDATA[      探索 Google 的 Gemma-2-2b-it 和 Microsoft 的 Phi-3-4k 模型在医疗领域的表现。 （未经微调） Google 的 Gemma-2-2b-it 平均表现出色，得分为 59.21%，而 Microsoft 的 Phi-3-4k 以 68.93% 领先。 将很快为医疗领域评估更多小型模型 源帖子 https://preview.redd.it/lfh5gvm44zfd1.png?width=2779&amp;format=png&amp;auto=webp&amp;s=901da925caa150ba7a1ca31bf28f9cd5824280a0    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh3clp/d_googles_gemma22b_vs_microsoft_phi3_a/</guid>
      <pubDate>Thu, 01 Aug 2024 00:40:59 GMT</pubDate>
    </item>
    <item>
      <title>EC2 上的大型文件大数据集 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh37ao/large_dataset_of_large_files_on_ec2_d/</link>
      <description><![CDATA[因此，我们正在努力训练一个相对较小的 CNN 和一些其他项目，这些项目是大规模大文件数据集上的。 数据集：16TB 的 500mb 卫星图像 我目前有一个 g5 EC2 实例（4 个 gpu），带有一个 16TB EBS io1 驱动器，其中已加载数据。使用简单的数据加载器和数据集对象，我们在单磁盘读取速度上遇到了瓶颈。 直接从 S3 读取更好还是使用其他方法来加快读取速度更好？    提交人    /u/SuperbMonk4403   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh37ao/large_dataset_of_large_files_on_ec2_d/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:38 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 小型数据集的机器学习 (n<50)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh25qt/discussion_ml_with_small_datasets_n50/</link>
      <description><![CDATA[我正在开展一个机器学习项目，涉及 4 个特征和一个将这 4 个特征相加的附加特征。有 5 个目标变量，所有特征和目标都是整数。 目前，我只有 1 行真实数据，因为获取更多数据非常麻烦。为了解决这个问题，我使用约束、关系逻辑和随机数生成器创建了 100 个模拟数据样本。数据中的关系相对线性（一个特征的度为 2，其他特征的度为 1）。由于范围较大（2000-3000），某些目标的均方误差 (MSE) 相当高（200-700），而对于其他目标，由于范围较小（5-20​​），均方误差较小（9-15）。 我尝试了各种模型：多元线性回归、岭回归、梯度提升和 MLP 回归。基于最低 MSE 的最佳结果是，对于 5 个目标中的 4 个，使用多项式交互的岭回归（一个特征的度为 2，其余特征的度为 4），对于剩余的目标，使用梯度提升。 但是，由于数据集较小，我在超参数调整方面遇到了瓶颈。对于如此有限的数据，更高级的方法是不可行的。当我们最终获得真实数据时，我们可能会有大约 40 个样本。 我的问题是：是否有可能使用如此小的数据集创建准确的 ML 模型？    提交人    /u/CashCrane   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh25qt/discussion_ml_with_small_datasets_n50/</guid>
      <pubDate>Wed, 31 Jul 2024 23:45:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 注释词汇表（可能）就是您所需要的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</link>
      <description><![CDATA[论文链接：https://www.biorxiv.org/content/10.1101/2024.07.30.605924v1 摘要：  蛋白质语言模型 (pLM) 彻底改变了蛋白质系统的计算建模，构建了以结构特征为中心的数值嵌入。为了增强蛋白质嵌入中可用的生化相关属性的广度，我们设计了注释词汇表，这是一种由结构化本体定义的蛋白质属性的转换器可读语言。我们从头开始训练注释变换器 (AT)，以恢复掩蔽的蛋白质属性输入，而无需参考氨基酸序列，仅基于蛋白质描述构建新的数值特征空间。我们在各种模型架构中利用 AT 表示，用于蛋白质表示和生成。为了展示注释词汇集成的优点，我们进行了 515 次不同的下游实验。使用新颖的损失函数和仅 3 美元的商业计算，我们的首要表示模型 CAMP 为 15 个常见数据集中的 5 个生成了最先进的嵌入，其余数据集上的性能也具有竞争力；凸显了使用注释词汇进行潜在空间管理的计算效率。为了标准化从头生成的蛋白质序列的比较，我们提出了一种新的基于序列比对的分数，它比传统的语言建模指标更灵活、更具生物学相关性。我们的生成模型 GSM 使用类似 BERT 的生成方案从仅注释提示中生成高比对分数。特别值得注意的是，许多 GSM 幻觉返回具有统计意义的 BLAST 命中，其中富集分析显示与注释提示匹配的属性 - 即使基本事实与整个训练集的序列同一性较低。总体而言，注释词汇工具箱提供了一种有前途的途径，可以用本体和知识图谱的成员取代传统的标记，增强特定领域的转换器模型。注释词汇对蛋白质的简洁、准确和有效的描述为构建蛋白质的数值表示以进行蛋白质注释和设计提供了一种新颖的方法。  我们很自豪地宣布发布我们的最新作品！请阅读、分享并提出任何问题！    提交人    /u/TeamArrow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</guid>
      <pubDate>Wed, 31 Jul 2024 17:47:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 无限上下文长度真的可能吗？：“Unlimiformer”作者周五讨论 NeurIPS 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</link>
      <description><![CDATA[无限上下文长度真的可能吗？代价是什么？ 2023 年 NeurIPS 论文 Unlimiformer 的作者 Amanda Bertsch 将在本周五的 Oxen.ai 论文俱乐部中描述该架构并回答问题。  Oxen 首席执行官兼 Plain Speak 大师 Greg Schoeninger u/FallMindless3563 将帮助解释该概念并将其与我们审阅过的其他论文联系起来。 致电：https://oxen.ai/community  声称使无限上下文长度成为可能的技巧：将交叉注意力计算卸载到 K-最近邻 (K-NN) 索引。  我在这里发了一条推文，其中某人制作了巧妙的 K-NN 动画：https://x.com/mustafarrag/status/1817647917059944474 论文：https://arxiv.org/abs/2305.01625 Greg，我将用我的前 5 个问题来回答。到目前为止，我只阅读了摘要。    提交人    /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</guid>
      <pubDate>Wed, 31 Jul 2024 15:47:52 GMT</pubDate>
    </item>
    <item>
      <title>[N] Finegrain 的对象橡皮擦演示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</link>
      <description><![CDATA[      对象橡皮擦 Finegrain 刚刚在 Huggingface 上发布了一个对象橡皮擦的演示。该模型可以删除任何对象或图像，并删除来自对象的任何效果（阴影、反射、光线）。与我们迄今为止拥有的其他橡皮擦相比，结果令人印象深刻；你怎么看？ 演示链接：https://huggingface.co/spaces/finegrain/finegrain-object-eraser    由   提交  /u/nota-Reddit   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</guid>
      <pubDate>Wed, 31 Jul 2024 15:20:10 GMT</pubDate>
    </item>
    <item>
      <title>为大型语言模型提供更好内存的框架 - 免费且开源。[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</link>
      <description><![CDATA[Github - https://github.com/chisasaw/redcache-ai 使用的 SDK [scikit-learn、numpy 和 openai] 什么？ 在构建聊天应用程序时，我找不到经济高效、可扩展且价格合理的内存层。这导致了 redcache-ai。它是一个提供语义搜索、存储和检索增强生成 (RAG) 的 Python 包。 用例？ 如果开发人员想要构建使用文档摘要和/或语义搜索的桌面应用程序，开发人员可以使用 redcache-ai 和选择的大型语言模型提供程序。聊天应用程序存储用户会话也是如此。 优点？  易于使用。只需像安装 Python 包一样安装它，即“pip install redcache-ai”。 提供存储可扩展性。将您的记忆存储到磁盘、sqlite 或您选择的数据库中。  请提供反馈并提出问题。也可以随时为项目做出贡献。    提交人    /u/hack_knight   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</guid>
      <pubDate>Wed, 31 Jul 2024 13:02:41 GMT</pubDate>
    </item>
    <item>
      <title>关于使用知识图谱的神经符号人工智能的调查论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</link>
      <description><![CDATA[  由    /u/joestomopolous  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</guid>
      <pubDate>Wed, 31 Jul 2024 11:06:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    </channel>
</rss>