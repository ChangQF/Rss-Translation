<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 01 Oct 2024 15:18:07 GMT</lastBuildDate>
    <item>
      <title>[D] ECCV 应用程序可让你浏览论文并查找相关工件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftq8rj/d_eccv_app_that_lets_you_browse_papers_and_find/</link>
      <description><![CDATA[有一个应用程序可以浏览论文、按受欢迎程度排名、过滤开放模型、数据集和演示  huggingface.co/spaces/ECCV/ECCV2024-papers     提交人    /u/Illustrious_Row_9971   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftq8rj/d_eccv_app_that_lets_you_browse_papers_and_find/</guid>
      <pubDate>Tue, 01 Oct 2024 15:01:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Python dash 和 Matplotlib 的类似彭博的交易终端</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftp6qy/p_bloomberg_like_trading_terminal_using_python/</link>
      <description><![CDATA[      我目前正在编写一个交易终端的代码。这类似于彭博终端，用户可以从中获得有关股票的所有见解。我在这个项目中使用了 Numpy、Pandas、Matplotlib、Dash、Plotily、Sklearn 等 Python 库。 这只是我工作的一小部分，在启动应用程序之前，我将添加大量见解和功能。我还使用了 yfinance 库，因此所有财务数据均来自雅虎财经。我很想知道我可以添加哪些功能使其脱颖而出。 我很想知道我可以添加什么功能使其变得更好。 https://preview.redd.it/gx4w5p8il5sd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=65ca2710971261f4988eb5d5136c5967d7af8c77 https://preview.redd.it/b2wk6j8il5sd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=a4627be7ee9b90947df65105938b136f77a5d540 https://preview.redd.it/fdbohi8il5sd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4e45296a4c2bf6a7555ac275b4c22dd5e8eba858    提交人    /u/Muda_ahmedi   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftp6qy/p_bloomberg_like_trading_terminal_using_python/</guid>
      <pubDate>Tue, 01 Oct 2024 14:16:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] weka 内存不足</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftolst/p_weka_out_of_memory/</link>
      <description><![CDATA[大家好，我第一次使用 weka 完成一个关于文本分类的作业，我一直遇到这个问题（内存不足（堆上剩余少于 50MB）。请加载较小的数据集或使用较大的堆大小。- 初始堆大小：128MB - 当前使用的内存（堆）：1998.5MB - 可用的最大内存（堆）：2048MB 注意：可以使用 -Xmx 选项指定 Java 堆大小。例如，要使用 128MB 作为堆大小，命令行如下所示：java -Xmx128m -classpath ... 这在 SimpleCLI 中不起作用，上面的 java 命令是指启动 Weka 的命令。有关更多信息，请参阅网络上的 Weka FAQ。）有人知道如何修复它吗？:(     提交   /u/Silver-Falcon6746   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftolst/p_weka_out_of_memory/</guid>
      <pubDate>Tue, 01 Oct 2024 13:51:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何从数据到部署：云 ML 平台还是开源工具？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftng8j/d_how_do_you_go_from_data_to_deployment_cloud_ml/</link>
      <description><![CDATA[我正在尝试使用各种工具来完成我的 ML 项目，开源工具和商业工具都很棒，但感觉我需要 10 多种工具才能拥有完整的管道。我正在尝试创建一个工作流，以便我可以轻松地从数据转到部署。MLOps 工具有很多，但其中很多只是帮助您进行实验跟踪，但 ML 生命周期还有更多功能。所以我一直在考虑转向 AWS Sagemaker、Azure ML、Google Vertex AI 等云解决方案。 乍一看，有些似乎有点笨重，协作体验不佳，一旦选择了一个，显然缺乏灵活性，所以我想了解一下人们对这些工具的体验如何？ 更具体地说，从数据到部署以及随着数据的发展持续维护 ML 生命周期有多容易。  这些工具有用吗？还是我应该使用开源工具打包自己的解决方案？你们面临哪些挑战？    提交人    /u/Lumiere-Celeste   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftng8j/d_how_do_you_go_from_data_to_deployment_cloud_ml/</guid>
      <pubDate>Tue, 01 Oct 2024 12:58:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 对基于 Gemma 的模型进行微调（LoRA）并获得较高的训练和验证损失值</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftgxx3/p_finetuning_lora_a_gemmabased_model_and_getting/</link>
      <description><![CDATA[您好！我是一名计算机科学专业的学生，​​正在尝试微调（LoRA）基于 Gemma 7b 的模型以完成我的论文。但是，我不断获得较高的训练和验证损失值。我尝试了不同的学习率、批量大小、lora 等级、lora alpha 和 lora dropout，但损失值仍然很高。 我也尝试使用不同的数据整理器。使用 DataCollat​​orForLanguageModeling，我的损失值低至 ~4.XX。使用 DataCollat​​orForTokenClassification，它开始时非常高，大约为 18-20，有时更高。DataCollat​​orWithPadding 对我来说不起作用，它给了我这个错误： ValueError：预期输入 batch_size (304) 与目标 batch_size (64) 匹配。  这是我的训练师  training_args = TrainingArguments( output_dir=&quot;./training&quot;, remove_unused_columns=True, per_device_train_batch_size=params[&#39;batch_size&#39;], gradient_checkpointing=True, gradient_accumulation_steps=4, max_steps=500, learning_rate=params[&#39;learning_rate&#39;], logsing_steps=10, fp16=True, optim=&quot;adamw_hf&quot;, save_strategy=&quot;steps&quot;, save_steps=50, evaluation_strategy=&quot;steps&quot;, eval_steps=5, do_eval=True, label_names = [&quot;input_ids&quot;, &quot;labels&quot;, &quot;attention_mask&quot;], report_to = &quot;none&quot;, ) trainer = Trainer( model=model, train_dataset=tokenized_dataset[&#39;train&#39;], eval_dataset=tokenized_dataset[&#39;validation&#39;], tokenizer=tokenizer, data_collat​​or=data_collat​​or, args=training_args, )  我的数据集如下所示  text,absent,dengue,health,mosquito,sick 不是生病的好时机。,0,0,1,0,1 NUNG NA DENGUE AKO [LINK],0,1,1,0,1 是发烧还是天气原因,0,0,1,0,1 上帝帮助病人？,0,0,1,0,1 &quot;产妇观察。 [HASHTAG] [HASHTAG] [HASHTAG] @ 西利曼大学医学中心基金会，Inc . [LINK]&quot;,0,0,1,0,0 ? @ St .特蕾莎医院 [LINK],0,0,1,0,0  Tokenized: {&#39;text&#39;: &#39;现在不是生病的好时机&#39;, &#39;input_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1665, 476, 1426, 1069, 577, 947, 11666], &#39;attention_mask&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], &#39;labels&#39;: [0, 0, 1, 0, 1]}  格式化程序： import re from datasets import DatasetDict max_length = 20 def clean_text(text): # 删除 URL text = re.sub(r&quot;\[LINK\]&quot;, &quot;&lt;URL&gt;&quot;, text) # 删除标签和提及 text = re.sub(r&quot;@[A-Za-z0-9_]+&quot;, &quot;\[MENTION\]&quot;, text) text = re.sub(r&quot;#\w+&quot;, &quot;\[HASHTAG\]&quot;, text) # 将文本小写 text = text.lower() # 删除特殊字符和多余空格 text = re.sub(r&quot;[^a-zA-Z0-9\s&lt;&gt;\&#39;]&quot;, &quot;&quot;, text) text = re.sub(r&quot;\s+&quot;, &quot; &quot;, text).strip() return text # 对文本列应用清理 dataset[&#39;train&#39;] = dataset[&#39;train&#39;].map(lambda x: {&#39;text&#39;: clean_text(x[&#39;text&#39;])}) def tokenize_function(examples): # 对文本进行标记 tokenized_text = tokenizer( examples[&#39;text&#39;], padding=&quot;max_length&quot;, truncation=True, max_length=max_length ) # 创建标签列表列表 labels = [ [examples[&#39;absent&#39;][i], examples[&#39;dengue&#39;][i], examples[&#39;health&#39;][i], examples[&#39;mosquito&#39;][i], examples[&#39;sick&#39;][i]] for i in range(len(examples[&#39;text&#39;])) ] tokenized_text[&#39;labels&#39;] = labels return tokenized_text # 对数据集应用标记化 tokenized_dataset = dataset.map(tokenize_function, batched=True) # 删除原有的标签列 tokenized_dataset = tokenized_dataset.remove_columns([&#39;absent&#39;, &#39;dengue&#39;, &#39;health&#39;, &#39;mosquito&#39;, &#39;sick&#39;]) # 打印出一个标记化的示例 print(tokenized_dataset[&#39;train&#39;][0])     submitted by    /u/jobiskits   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftgxx3/p_finetuning_lora_a_gemmabased_model_and_getting/</guid>
      <pubDate>Tue, 01 Oct 2024 05:44:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 可扩展的机器学习管道，专注于训练基础设施</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftg3ly/d_scalable_ml_pipelines_focusing_training_infra/</link>
      <description><![CDATA[大家好，我目前正在尝试更多地了解 ML 系统设计，重点是基础模型的训练基础设施，我发现研究这个主题很困难。 有没有什么好的资源有人熟悉可能会有帮助？    提交人    /u/adi214   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftg3ly/d_scalable_ml_pipelines_focusing_training_infra/</guid>
      <pubDate>Tue, 01 Oct 2024 04:51:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 考古学或古代历史中的机器学习潜力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdw6n/d_machine_learning_potential_in_archeology_or/</link>
      <description><![CDATA[目前在 T10 攻读数学计算机科学专业的本科生，这可能是一个愚蠢的问题，但有没有人想过或对在埃及学或考古学等历史领域使用人工智能有过任何想法？破译象形文字，使用深度学习寻找陵墓或文物的位置，组装破碎或毁坏的古代文物或陶器和绘画等物品。只是想知道是否有人对这个主题有任何想法，以及它是否具有现实潜力（或没有）。我认为最大的困难是缺乏好的数据。    提交人    /u/hmbhack   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdw6n/d_machine_learning_potential_in_archeology_or/</guid>
      <pubDate>Tue, 01 Oct 2024 02:47:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们如何构建对话式检索增强生成应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/</link>
      <description><![CDATA[我已阅读了各种资源，例如： - https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/ - https://python.langchain.com/docs/tutorials/qa_chat_history/ - https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/ - https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/ - https://huggingface.co/datasets/nvidia/ChatRAG-Bench  但这些感觉过于简单，因为它们没有解决以下复杂性： 1）何时检索与立即响应以减少延迟 2）依靠先前在对话中检索到的现有上下文，而不是在当前回合再次检索 3）在检索到的信息和过去的对话历史之间划分 LLM 上下文。 我相信一些团队已经有了很好的系统，将不胜感激指针！    由    /u/iidealized 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/</guid>
      <pubDate>Tue, 01 Oct 2024 02:17:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] SynthPAI：用于个人属性推断的合成数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftb4hk/r_synthpai_a_synthetic_dataset_for_personal/</link>
      <description><![CDATA[[被 NeurIPS&#39;24 D&amp;B 接受] TL;DR：我们为 Reddit 构建了一个 LLM 代理模拟框架，以生成合成数据来推进基于推理的隐私研究。 预印本：https://arxiv.org/abs/2406.07217 Github：https://github.com/eth-sri/SynthPAI 在我们的最新研究论文中，我们介绍了 SynthPAI - 一个合成数据集，它为评估基于 LLM 的个人属性推理的新基准奠定了基础。我们为 Reddit 构建了一个 LLM 代理模拟框架，以生成合成数据来推进基于推理的隐私研究。然后，我们评估了 18 个最先进的 LLM，评估它们从合成人物撰写的文本集合中预测特定私人属性（如年龄、性别、出身、工作等）的能力。使用这个框架，我们构建了一个 PAI（私人属性推断）数据集，该数据集包含 7800 多条评论和 300 个合成配置文件以及人工验证的属性标签。我们表明，我们的评论表现出高保真度（人类无法将它们与真实评论区分开来）和 PAI 研究的信息性，使我们能够在各种实验中得出与真实世界数据相同的定性结论。     提交人    /u/equin_x   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftb4hk/r_synthpai_a_synthetic_dataset_for_personal/</guid>
      <pubDate>Tue, 01 Oct 2024 00:27:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] PyTorch 原生架构优化：torchao</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftaw4e/d_pytorch_native_architecture_optimization_torchao/</link>
      <description><![CDATA[https://pytorch.org/blog/pytorch-native-architecture-optimization/    由   提交  /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftaw4e/d_pytorch_native_architecture_optimization_torchao/</guid>
      <pubDate>Tue, 01 Oct 2024 00:16:04 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 为AI模型量身定制的无损压缩库 - 将Llama3.2的传输时间缩短33%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/</link>
      <description><![CDATA[如果您希望减少 Hugging Face 的下载时间并帮助减少其服务器负载—（Clem Delangue 提到 HF 每天处理高达 6PB 的数据！） —&gt;您可能会发现 ZipNN 很有用。 ZipNN 是一个开源 Python 库，可在 MIT 许可下使用，专门用于压缩 AI 模型而不会损失准确性（类似于 Zip，但针对神经网络进行了定制）。 它使用无损压缩将模型大小减少 33%，节省了三分之一的下载时间。 ZipNN 有一个 HF 插件，因此您只需添加一行代码即可。 在这里查看： https://github.com/zipnn/zipnn Hugging Face 上已经有几个带有 ZipNN 的压缩模型，如果您有兴趣，可以直接上传更多。 最新的是 Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed 看看这个 Kaggle 笔记本： 对于您可以在此 Kaggle 笔记本中找到 Llama-3.2 的实际示例： https://www.kaggle.com/code/royleibovitz/huggingface-llama-3-2-example ZipNN repo 中提供了更多示例： https://github.com/zipnn/zipnn/tree/main/examples    提交人    /u/Candid_Raccoon2102   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/</guid>
      <pubDate>Mon, 30 Sep 2024 18:32:24 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 有哪些关于机器学习、深度学习或 NLP 的博客？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/</link>
      <description><![CDATA[您可以分享它们吗    由   提交  /u/Internal_Complaint64   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/</guid>
      <pubDate>Mon, 30 Sep 2024 13:12:20 GMT</pubDate>
    </item>
    <item>
      <title>[N] 强化学习速查表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fssp75/n_reinforcement_learning_cheat_sheet/</link>
      <description><![CDATA[大家好！ 我刚刚在 Medium 上发表了我的第一篇文章，还创建了一个强化学习备忘单。🎉 我很乐意听到您的反馈、建议或任何关于如何改进它们的想法！ 请随时查看它们，并提前感谢您的支持！😊 https://medium.com/@ruipcf/reinforcement-learning-cheat-sheet-39bdecb8b5b4    提交人    /u/Prudent_Nose921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fssp75/n_reinforcement_learning_cheat_sheet/</guid>
      <pubDate>Mon, 30 Sep 2024 10:56:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fru46i/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fru46i/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 29 Sep 2024 02:15:10 GMT</pubDate>
    </item>
    </channel>
</rss>