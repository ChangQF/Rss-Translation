<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 05 Feb 2024 21:12:35 GMT</lastBuildDate>
    <item>
      <title>维苏威挑战奖已颁发！ [N]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajqtfd/the_vesuvius_challenge_prize_has_been_awarded_n/</link>
      <description><![CDATA[https://scrollprize.org/grandprize 看起来这个项目已经死在水中了，读取到的字母为零，直到一个非常偶然的发现“裂开的泥土”出现了。一个人用自己的视觉系统盯着扫描图像进行模式识别。这导致其他人看到数据中的墨迹、对其进行标记、训练 ML 模型、使用它们查找更多字母等等。   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajqtfd/the_vesuvius_challenge_prize_has_been_awarded_n/</guid>
      <pubDate>Mon, 05 Feb 2024 20:35:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从相邻领域（生物信息学）过渡到机器学习的最佳策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajqeyx/d_best_strategies_for_transitioning_into_ml_from/</link>
      <description><![CDATA[我是一名生物信息科学家，拥有硕士学位和大约 3 年的专业经验。目前，我的工作包括软件开发、临时数据分析，我大约有 1/4 的时间用于开发 ML 算法。我从研究生院起就对机器学习充满热情，但直到最近才决定转入一份以机器学习为主要任务的工作。我的前两份工作很高兴能在我的领域取得出色的工作。现在，我感觉更加自信，我想专注于我热衷的事情。 我想知道是否有人对我的第一份工作（主要是 ML/DL 竞争职位）有什么建议？我在目前的职位上领导了一些 ML/DL 算法的开发，拥有一些不错的经验。我还在研究生院进行了一个为期一年的项目，旨在使用机器学习来预测 RNA 活性，但该项目失败了，也没有发表任何文章。我还有一个令人印象深刻的中等水平的个人项目，它使用深度学习来解决我所在领域的问题。 我计划将我的机器学习算法经验作为我简历中每个工作/学位的首要项目。我还能做些什么来帮助我过渡到这个新的专业角色？我应该根据我目前的资格集中精力获得第一个职位吗？我应该用多个 ML/DL 项目来完善我的 github 吗？我应该寻求特定的机器学习认证吗？ 从更普遍的过去经验中，什么最有可能有助于获得有竞争力的机器学习/深度学习职位？    由   提交 /u/FutureDNAchemist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajqeyx/d_best_strategies_for_transitioning_into_ml_from/</guid>
      <pubDate>Mon, 05 Feb 2024 20:19:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 不知道这是否可能，机器学习能否在谷歌地图/街景上发现特定的建筑类型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajptr9/d_no_clue_if_this_is_possible_can_machine/</link>
      <description><![CDATA[我花了相当多的时间梳理谷歌地图和地图。寻找特定类型建筑的街景。有没有办法训练机器在指定的地理区域为我梳理谷歌街道并标记符合我标准的每座建筑物？ 这是一个非常简单的结构，我认为不会那么难教机器识别，但我对这个领域非常陌生，所以我真的不知道我在说什么。只是向更有经验的人寻求一些指导来解决我的问题。 我假设这将是某种自定义解决方案。现在在网上找不到任何可以实现我正在寻找的东西。有什么想法吗？   由   提交 /u/DetectiveDanStark   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajptr9/d_no_clue_if_this_is_possible_can_machine/</guid>
      <pubDate>Mon, 05 Feb 2024 19:55:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 神经网络训练过程中自动正则化超参数调整的研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajptak/d_research_on_automatic_regularization/</link>
      <description><![CDATA[我很好奇是否有人知道在神经网络训练期间自动调整正则化超参数的研究（例如在单次运行期间而不是执行网格搜索） ）？通过监视验证集上的损失函数似乎这是可行的，但我似乎找不到正确的关键字来进行正确的文献搜索。 我想出了一个启发式方法该技术涉及检查验证损失是否会根据预测熵的 epsilon 减少而增加/减少。然后在每个时期相应地增加/减少正则化超参数的一小部分。这似乎在 FashionMNIST 等基本数据集上效果很好。类似的东西已经存在了吗？   由   提交 /u/TheFlyingDrildo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajptak/d_research_on_automatic_regularization/</guid>
      <pubDate>Mon, 05 Feb 2024 19:55:23 GMT</pubDate>
    </item>
    <item>
      <title>从开普敦到迦太基：这部纪录片讲述了一支由全非洲女性领导的人工智能研究团队奋力克服困难，以及她们将非洲人工智能推向世界的不可思议的旅程。 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajkh13/cape_to_carthage_documentary_about_an_all_african/</link>
      <description><![CDATA[在人工智能领域，非洲素有“失踪大陆”的美誉。跟随一支由女性领导的全非洲研究团队，与科技巨头和顶尖大学竞争顶级国际人工智能研究会议 NeurIPS 的席位，以改变历史。 观看 30 场比赛分钟纪录片此处。   由   提交/u/BioGeek  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajkh13/cape_to_carthage_documentary_about_an_all_african/</guid>
      <pubDate>Mon, 05 Feb 2024 16:24:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] AMD 软件成熟度 vs Nvidia</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajk7q2/d_amd_software_maturity_vs_nvidia/</link>
      <description><![CDATA[我正在考虑使用 AMD MI210/MI250/MI300X 作为 A100/H100/H200 的替代品。有什么轶事或数据可以说明 AMD 软件的成熟程度吗？ Raja Koduri 最近表示，虽然 Nvidia 支持开箱即用的 80% 的 Huggingface 模型，但 AMD（或任何其他供应商）仅支持开箱即用的 5%。 AMD 声称支持整个变形金刚库，但当我与 AMD 工程师交谈时，他告诉我“细节决定成败……”我很想知道是否有人有直接经验或数据支持这两种说法 Databricks 测试：Databricks vLLM 端口：嵌入式LLM George Hotz 抱怨 AMD 很糟糕：用户体验问题   由   提交 /u/wombatscientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajk7q2/d_amd_software_maturity_vs_nvidia/</guid>
      <pubDate>Mon, 05 Feb 2024 16:13:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何优化 PyTorch ML 成本和性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajk3ke/d_how_to_optimize_pytorch_ml_cost_and_performance/</link>
      <description><![CDATA[https://cuno.io/blog/optimizing-pytorch-machine-learning-cost-and-performance-using-cunofs/ 经常有很多关于本小节介绍如何优化 Pytorch 的 ML 成本/性能算法。我认为这篇文章对于 Pytorch 用户来说是一个好的开始（无论您是否有经验）   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1ajk3ke/d_how_to_optimize_pytorch_ml_cost_and_performance/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajk3ke/d_how_to_optimize_pytorch_ml_cost_and_performance/</guid>
      <pubDate>Mon, 05 Feb 2024 16:08:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微软研究院的 EvoPrompt – 进化算法与即时工程的结合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aji7np/d_microsoft_researchs_evoprompt_evolutionary/</link>
      <description><![CDATA[      在此处访问全文 我在浏览 LinkedIn 时发现了这篇来自微软、清华大学和西北大学的新颖的预印本论文。他们的论文标题为连接大型语言模型与进化算法产生强大的提示优化器。  在本文中，研究人员表明，一种模仿进化算法的极其简单的算法具有执行自动化提示工程的潜力。这种方法具有可扩展性，易于实现，并且明显优于手动提示工程。 虽然本文讨论了两种不同的进化算法：遗传算法和差分进化，但结果并不&lt;距离那么远。另外，我喜欢遗传算法，因为它们更类似于自然选择。下图总结了 GA 方法： LLM 实现的遗传算法 与常规遗传算法一样，有 5 个步骤：初始化、选择、交叉、变异和评估。 初始化：我们向人们提供了我们所知道的一系列不错的提示，并可能使用 GPT-3.5 生成提示。 选择：使用轮盘赌法，选择两个个体作为父母 交叉：使用上述说明，父母交配形成一个新的孩子 变异 变异 strong&gt;：使用上面的说明，孩子会经历突变 评估：我们针对特定提示对模型的性能进行评分。 重复此过程直到种群规模增加一倍，然后对种群进行排序并剔除回原始大小。 作为生物学专业的人，我喜欢看到遗传算法的实际应用。这些算法优雅、稳健且美观，模仿现实生活中的自然选择过程。这绝对是我最喜欢的人工智能算法，我很高兴看到它未来的工作方向。 我向你们提出的问题是，你们对这种方法有何看法？我已经发表了几篇关于自动提示工程的文章，对我来说，这篇文章验证了这种方法是可能的。然而，本文缺乏的一件事是评估过程如何运作的具体细节，特别是对于更复杂的现实世界提示。 我非常想了解你们的观点！而且，如果您正在寻找易于理解的摘要，我在这里详细讨论这篇论文。   由   提交 /u/Starks-Technology   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aji7np/d_microsoft_researchs_evoprompt_evolutionary/</guid>
      <pubDate>Mon, 05 Feb 2024 14:47:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有更大的团队放弃 wandb？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajg85m/d_any_larger_teams_switching_away_from_wandb/</link>
      <description><![CDATA[在大约六个月的时间里，我的团队遇到了运行失败、奇怪的用户体验问题和普遍存在错误的行为等问题。与其他公司的朋友交谈，似乎我们并不是唯一的人。 我们正在考虑更换，这已经够不便的了，但我的总体印象是没有太多选择除了 wandb 之外的更大团队（我们规模不大，但我们正在成长）。大多数开源解决方案看起来都很简单，我们团队中没有人有与任何其他供应商合作的丰富经验。所以，我希望这里的一些人能够插话。有没有人加入过从 wandb 转型的团队，如果是的话，你们最终运行了什么？ 编辑： &lt; p&gt;谢谢您的推荐！在探索了其中的一些之后，我真的很喜欢 Comet。对于我们团队来说，这似乎是最简单的转变。一些开源选项看起来很酷，但比大型团队更适合单独的研究人员。我还将尝试本周这里提到的其他几个供应商。海王星的定价尤其有趣。我感谢所有的帮助！   由   提交 /u/FreeKingBoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajg85m/d_any_larger_teams_switching_away_from_wandb/</guid>
      <pubDate>Mon, 05 Feb 2024 13:15:07 GMT</pubDate>
    </item>
    <item>
      <title>寻求指导：选择低计算能力的机器学习研究主题进行会议提交[研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajftgq/seeking_guidance_choosing_a_lowcomputational/</link>
      <description><![CDATA[你好，机器学习科学家，[研究] 我正在寻找一篇机器学习领域的研究论文，目标是一年内将其提交给信誉良好的会议。虽然我对机器学习和深度学习的基础知识有深入的了解，但我受到可用计算资源的限制；我将使用我的笔记本电脑进行研究。鉴于这一限制，您能否推荐一个无需大量计算能力即可探索的机器学习研究领域？ 谢谢  &amp; #32；由   提交 /u/Significant-Raise-61   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajftgq/seeking_guidance_choosing_a_lowcomputational/</guid>
      <pubDate>Mon, 05 Feb 2024 12:54:17 GMT</pubDate>
    </item>
    <item>
      <title>具有高效编译的最佳 GPU 张量抽象库？ （Triton、Halide、张量推导式、TorchScript 等）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ajeago/best_gpu_tensor_abstraction_libraries_with/</link>
      <description><![CDATA[显然 CUDA 可用于低级 GPU 编程，但需要花费大量时间进行编程。然后您可以使用像 Pytorch 这样的库来实现高级级操作，但尝试做复杂的事情可能会非常慢。 然后还有一些有趣的语言尝试在抽象级别上插入 CUDA 之上 - Triton 和 Halide。 还有适合张量约简的爱因斯坦符号风格的库。张量推导式是一种使用遗传算法来调整 GPU 函数的方法。我真的很喜欢张量理解的编程模型，但它似乎不再处于积极开发状态。 Einops 看起来很相似，但看起来不太复杂，而且似乎没有那么先进的优化器（？）。还有 opt-einsum，它看起来优化程度较低，因为它似乎无法编译为单个 CUDA 内核。 然后是 numba，它专注于更命令式的风格，但更受限制它如何将操作映射到 GPU。我认为 TorchScript 是类似的？ 有什么我遗漏的吗？ 如果有的话，您会使用哪些？ 是否有积极维护相当于张量理解，提供相同的功能集和优化？ 感谢您的时间。   由   提交 /u/HumanSpinach2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ajeago/best_gpu_tensor_abstraction_libraries_with/</guid>
      <pubDate>Mon, 05 Feb 2024 11:25:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过 Lipschitz 正则化实现大规模零样本机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aje94r/r_zeroshot_machine_unlearning_at_scale_via/</link>
      <description><![CDATA[ 由   提交/u/JustAddMoreLayers   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aje94r/r_zeroshot_machine_unlearning_at_scale_via/</guid>
      <pubDate>Mon, 05 Feb 2024 11:22:35 GMT</pubDate>
    </item>
    <item>
      <title>跟着我重复：Transformers 在复制方面比状态空间模型更好 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aj9swv/repeat_after_me_transformers_are_better_than/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.01032 摘要： Transformers 是序列建模的主要架构，但人们对使用固定大小潜在状态的模型越来越感兴趣不依赖于序列长度，我们将其称为“广义状态空间模型”。 （GSSM）。在本文中，我们表明，虽然 GSSM 在推理时间效率方面很有前景，但与需要从输入上下文复制的任务上的 Transformer 模型相比，它们是有限的。我们从字符串复制这一简单任务的理论分析开始，并证明两层 Transformer 可以复制指数长度的字符串，而 GSSM 从根本上受到其固定大小潜在状态的限制。根据经验，我们发现 Transformer 在需要复制上下文的合成任务的效率和泛化方面优于 GSSM。最后，我们评估了预训练的大型语言模型，发现 Transformer 模型在从上下文复制和检索信息方面显着优于状态空间模型。综上所述，这些结果表明 Transformer 和 GSSM 在实际感兴趣的任务上存在根本差距。   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aj9swv/repeat_after_me_transformers_are_better_than/</guid>
      <pubDate>Mon, 05 Feb 2024 06:16:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] Chess-GPT，比 GPT-4 小 1000 倍，可以下 1500 Elo 国际象棋。我们可以直观地看到它的内部棋盘状态，并且它可以准确地估计游戏中玩家的 Elo 等级。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/</link>
      <description><![CDATA[   gpt-3.5-turbo-instruct 的 Elo 等级为 1800，国际象棋看起来很神奇。但事实并非如此！一个参数小 100-1000 倍的法学硕士将在进行数百万局国际象棋比赛后学会如何玩 ELO 1500。 该模型仅经过训练来预测 PGN 字符串中的下一个字符 (1.e4 e5 2.Nf3 ...）并且从未明确给出棋盘状态或国际象棋规则。尽管如此，为了更好地预测下一个角色，它学习计算游戏中任何时刻的棋盘状态，并学习一系列不同的规则，包括将、将死、易位、过路、升级、固定棋子此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 Elo 评级。 我们可以可视化模型的内部棋盘状态，因为它是预测下一个字符。例如，在此热图中，左侧是真实白色棋子位置，中间是二进制探针输出，右侧是探针置信度梯度。我们可以看到该模型非常有信心，没有白棋子位于任一后排。 ​ https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s= 003fe39d8a9bce2cc3271c4c9232c00e4d886aa6 此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 ELO 评级。更多信息请参阅这篇文章： https:/ /adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html 代码在这里：https://github.com/adamkarvonen/chess_llm_interpretability   由   提交 /u/seraine   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/</guid>
      <pubDate>Sun, 04 Feb 2024 17:06:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>