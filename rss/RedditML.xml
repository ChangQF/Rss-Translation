<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 24 Jan 2024 15:14:44 GMT</lastBuildDate>
    <item>
      <title>[D] DDIM 反转 - 真实图像的反转潜伏期如何“​​高斯”？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ej9jz/d_ddim_inversion_how_gaussian_are_the_inverted/</link>
      <description><![CDATA[我遇到过几篇论文，它们使用确定性反演来查找潜伏，该潜伏（连同提示）可以使用稳定扩散再现真实图像。在“提示到提示”中，赫兹等人。请注意以下几点：  但是，在许多其他情况下，反演不够准确，如图 1 所示。 11. 这部分是由于失真可编辑性权衡 [43]，我们认识到减少无分类器指导 [18] 参数（即减少即时影响）可以改善重建，但限制了我们执行重大任务的能力  我在其他论文中看到过类似的说法，这是由于反转潜伏不属于生成模型通常从中采样的标准高斯空间它的初始噪声潜伏。我想知道是否有人知道对此进行深入研究的任何著作？量化反向潜伏偏离预期高斯分布的最佳方法是什么？是否有某些图像在 SD 的学习分布下不太可能出现，并且反转它们会导致潜在的高斯分布更小？预先感谢您的任何建议和指示！ ​   由   提交 /u/35mmpy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ej9jz/d_ddim_inversion_how_gaussian_are_the_inverted/</guid>
      <pubDate>Wed, 24 Jan 2024 15:10:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] Finetune 提速 TinyLlama 387%、DPO 提速 188%、LLM 推理提速 2 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eiwe8/p_finetune_387_faster_tinyllama_188_faster_dpo_2x/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eiwe8/p_finetune_387_faster_tinyllama_188_faster_dpo_2x/</guid>
      <pubDate>Wed, 24 Jan 2024 14:54:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习研究主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eivj7/d_topics_for_ml_research/</link>
      <description><![CDATA[您能推荐一些与 AI/ML 相关的论文主题吗？基于该研究，我想将该功能实现到软件中。我更喜欢移动（Android）开发。由于这是我最后一年项目的一部分，我大约有 4 个月的时间。谢谢   由   提交 /u/ToxikSunaan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eivj7/d_topics_for_ml_research/</guid>
      <pubDate>Wed, 24 Jan 2024 14:53:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在基于 Transformer 的 LLM（例如 GPT2 和 LLAMA2）中如何处理令牌嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19efvii/d_how_are_tokenembeddings_processed_in/</link>
      <description><![CDATA[我一直认为，简单的前向传递的上下文宽度为 1600，填充了最多 1600 个标记，并在输出处生成一个标记。考虑到自动回归特性，然后新的令牌将附加到之前的令牌上，并且正在发生下一次前向传递。 我目前正在研究一些内部结构，并开始将 GTP2-XL 的激活视为以及使用 Transformer Lens 库的 LLAMA2。在那里我第一次接触了令牌嵌入。如果我向 LLM 输入三个标记，它会在两层之间缓存一个大小为 4x1600 的张量（将“开始”标记添加到输入标记中）。 我在理解模型如何运行使用所有这些向量进行前向传播。所以我的问题是：  是否在每个输入标记（+起始标记）的每次前向传递中计算所有标记 Emebedding？ 线性中的权重是多少？层像？我认为这应该是一个大小为 1600x1600 的矩阵，因此计算结果为 4x1600 * 1600x1600，并再次导致 4x1600 矩阵 如果是这样：是否涉及某种权重共享？ 1600x1600 导致每层约 250 万个权重，对于 GPT2，变压器的一个线性层部分总共有 1.228 亿个权重，对于我来说，考虑 1.5B 个参数的总数，这似乎是合理的。 会发生什么最终所有这些令牌嵌入？我认为它只需要一个一维向量来计算 softmax 并决定输出哪个标记。 transformer 论文提到它在 LLM 开始时向嵌入向量添加了一些位置编码。它们通常是如何计算的？  ​ 非常感谢您的建议。   由   提交/u/bineda  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19efvii/d_how_are_tokenembeddings_processed_in/</guid>
      <pubDate>Wed, 24 Jan 2024 12:22:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 视觉模型的方便比较图表：何时使用什么！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</link>
      <description><![CDATA[       由   提交/u/Instantinopaul   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 幻视曼巴再次出击！变形金刚王座正在崩溃吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</link>
      <description><![CDATA[还记得震撼 NLP 的状态空间模型 Mamba 吗？好吧，抓住你的像素，因为它们现在也在计算机视觉领域碾压它！ 他们的新模型 Vision Mamba 抛弃了自我关注热潮，并依赖于状态空间魔法。结果？性能与顶级视觉变压器 (DeiT) 相当，但效率更高！ 这可能会改变游戏规则，伙计们。我们正在谈论更快、更轻的型号，它们可以在您祖母的笔记本电脑上运行，但仍然像鹰一样看得见。 有什么想法吗？我很高兴看到变形金刚领域出现一些竞争。我们可以期待在这个新架构上推出 chatgpt v2 吗？道歉！可能听起来很疯狂，而且评论还为时过早。 查看论文：https: //paperswithcode.com/paper/vision-mamba-efficient-visual-representation   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</guid>
      <pubDate>Wed, 24 Jan 2024 11:06:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] InternLM-Math：SOTA 开源数学推理法学硕士。求解器、证明者、验证者、增强器。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ee2ku/p_internlmmath_sota_opensourced_math_reasoning/</link>
      <description><![CDATA[   上海人工智能实验室推出新的SOTA数学法学硕士，具有7B和20B规模的开放式来源。 Github：https://github.com/InternLM/InternLM-Math Huggingface：https://huggingface.co/internlm/internlm2-math-7b 演示：https://huggingface.co/spaces/internlm/internlm2-math -7b ​ https://preview.redd.it/4emyeapn7dec1.png?width=1224&amp;format=png&amp;auto=webp&amp;s=6a79ba3e4b98f48befed91eded1cf286b9fca137 特点：  7B 和 20B 中文和英语数学 LM 的性能优于 ChatGPT。 InternLM2-Math 继续从 InternLM2-Base 进行预训练，具有约 100B 高质量数学-相关代币和 SFT 以及约 200 万双语数学监督数据。我们应用最小哈希和精确数字匹配来消除可能的测试集泄漏。 添加 Lean 作为数学问题解决和数学定理证明的支持语言。我们正在探索将 Lean 3 与InternLM-Math 用于可验证的数学推理。 InternLM-Math 可以为 GSM8K 等简单的数学推理任务生成 Lean 代码，或者提供基于 Lean 状态的可能证明策略。 也可以视为奖励模型，支持结果/过程/精益奖励模型。我们用各种类型的奖励建模数据来监督InternLM2-Math，使InternLM2-Math也可以验证思维链过程。我们还添加了将思维链过程转换为 Lean 3 代码的功能。 数学 LM 增强助手和代码解释器。 InternLM2-Math 可以帮助增强数学推理问题并使用代码解释器解决它们，这使您可以更快地生成综合数据！  性能： https://preview.redd.it/ttzsd4408dec1.png?width=1175&amp;format =png&amp;auto=webp&amp;s=8894552a848130a8240a2e135a6b78d0841311d4   由   提交/u/OpenMMLab  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ee2ku/p_internlmmath_sota_opensourced_math_reasoning/</guid>
      <pubDate>Wed, 24 Jan 2024 10:29:53 GMT</pubDate>
    </item>
    <item>
      <title>[项目] BELT（较长文本的 BERT）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19edzov/project_belt_bert_for_longer_texts/</link>
      <description><![CDATA[我们创建了 BELT（BERT For Longer Texts）——一个 Python 包，允许对长度超过 512 个 token 的文本使用类似 BERT 的模型。该方法是 Jacob Devlin 提出的想法的实现，Jacob Devlin 是 评论。您可以在 Medium 上我刚刚发表的两篇文章中阅读有关它的更多详细信息： 第一部分是应用 BERT 分类器的概述： 第 1 部分 第二部分深入介绍我们训练 BELT 模型的方法。 第 2 部分 该存储库已开源： Repo 我知道你在想什么：“等等，bucko，这不是什么新鲜事。每个人都知道有像 BigBird 或 Longformer 这样的模型可以处理更长的文本”。对此我的回答是：“我知道，伙计，但是 BigBird 和 Longformer 不是修改过的 BERT。它们是具有不同架构的模型。因此，它们需要从头开始预训练或下载。 BELT修改模型微调。这带来了 BELT 方法的主要优点 - 它使用任何预先训练的 BERT 或 RoBERTa 模型。快速查看 HuggingFace Hub 可以确认，BERT 的资源比 Longformer 多大约 100 倍。找到适合特定任务或语言的可能会更容易。”享受吧！   由   提交/u/MBrzozowskiML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19edzov/project_belt_bert_for_longer_texts/</guid>
      <pubDate>Wed, 24 Jan 2024 10:24:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你们都使用什么类型的服务器进行机器学习/人工智能训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ec5le/d_what_type_of_servers_are_you_all_using_for/</link>
      <description><![CDATA[我在校园数据中心工作，希望从社区获得一些见解。我们最近获得了一些预算，我们的使命宣言是扩大和扩展更多与人工智能相关的项目。我自己不是最终用户，所以如果我对实际细节的掌握有点不确定，请原谅我，但通常我们希望为想要从事机器学习项目、NLP 和 LLM、AI 的教师和学生提供服务器训练等。 我在 Google 上搜索了一下，显然我很高兴看到 Nvidia 和 AMD 推出的所有新 AI 处理器。我指的是前者的 H100 系列和 Grace Hopper，后者的 Instinct MI300。我们最近经常合作的服务器供应商技嘉（Gigabyte）拥有适用于所有这些最新芯片的服务器（如果有人好奇，那就是G593-ZD2 配备 8-GPU HGX H100 模块，H223-V10 与 GH200，G383-R80 与 MI300A 和 G593-ZX1 与 MI300X），但正如你可以想象的那样，它们花费了相当多的钱。我们正在考虑选择具有大量 L40S GPU 的产品。但令我苦恼的是，我们不能花一点钱为我们的大学购买一台闪亮的超级计算机。 所以我想知道是否有人可以分享您用于机器学习的内容以及您的想法在这些新的人工智能芯片上。谁知道呢，但也许我会找到一些灵感带到我们的下一次采购会议上。非常感谢！   由   提交 /u/manwhoholdtheworld   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ec5le/d_what_type_of_servers_are_you_all_using_for/</guid>
      <pubDate>Wed, 24 Jan 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么时候在 TPU 上训练有意义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</link>
      <description><![CDATA[我花了几周时间将 torch 模型训练脚本移植到 PyTorch/XLA 并在 TPU v3 和 v4 上进行测试。从纯粹的训练速度和成本效率的角度来看，我将结果与 GCP 中的 a2/g2 机器上的训练进行了比较。我很惊讶移植代码有多困难，以及 TPU 上的训练有多慢且成本低效。 Dev UX 让人想起使用 TensorFlow（从最坏的意义上来说）。东西通常不能开箱即用，很难调试，因为所有东西都是编译的，而且张量是惰性的。整个事情非常不透明，不清楚发生了什么。没有您期望拥有的基本工具，例如如果不进行分析就无法检查 TPU 利用率。 更令人惊讶的是，训练速度比使用同等价格的 GPU 时慢得多。例如，与 g2-standard-96（8xL4 GPU）上的训练相比，TPU v3-8 上的训练速度大约慢 2 倍，而成本却大致相同。 TPU v4-8 价格更高，但仍然比 g2-standard-96 慢。我的模型或多或少是一个简单的密集网络，它来自推荐领域。未移植的 pytorch 代码使用 DDP。数据加载器经过高度优化并具有基准测试，我确信这不是瓶颈。 XLA 指标没有显示任何危险信号。 此时，我想知道为此投入更多精力是否有意义。非 Google 人员是否真的使用 TPU 进行大规模训练？是不是 Torch/XLA 还没有准备好迎接黄金时段，只是 TPU 最适合与 TF 或 JAX 一起使用？ TPU 是否有特定的用例？   由   提交 /u/Puzzleheaded-Stand79    reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</guid>
      <pubDate>Wed, 24 Jan 2024 04:15:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 测试基于 LLM 的应用程序很困难。你怎么处理这个问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</link>
      <description><![CDATA[让我知道你是如何处理这个问题的。非常感谢您的评论！   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</guid>
      <pubDate>Wed, 24 Jan 2024 03:17:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们不能使用合成数据来帮助创建用于放射图像分析训练的更清晰的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</link>
      <description><![CDATA[这是否比创建合成数据来训练 LLM 更难，类似于 AMIE 在最近的论文中所做的：https://blog.research.google/2024/01/amie-research-ai-system- for-diagnostic_12.html   由   提交/u/derpgod123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</guid>
      <pubDate>Wed, 24 Jan 2024 01:27:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究人员使用哪些工具在论文中创建出色的图像和流程图？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</link>
      <description><![CDATA[实际上，我想知道优秀的研究论文中的模型架构图有多酷，其中包含清晰的流程流程图和模型架构的出色可视化。目前我使用draw.io，但很好奇使用什么工具？我的意思是他们使用 Figma、Adobe 等专业工具吗？   由   提交 /u/MysticShadow427   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</guid>
      <pubDate>Tue, 23 Jan 2024 18:17:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有这些 AI 服务如何能够负担每月 5/10/20 美元的费用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</link>
      <description><![CDATA[各种人工智能服务（从语音识别到 OCR 和艺术生成）如何嵌入新数据，以如此低的成本提供其功能？使用 GPT-4 API 之类的东西很快就会花费 10 美元，这对于其他模型来说也是类似的。即使在本地运行 LLaMA 2 这样的东西也会产生巨大的成本。我很好奇这些服务在运营这些大型模型时采用的经济策略来维持较低的月费。   由   提交 /u/Numerous_Bed9323   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>