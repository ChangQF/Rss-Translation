<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 26 Jun 2024 01:04:09 GMT</lastBuildDate>
    <item>
      <title>[R] Transformer 推理能力有多强？局部屏障和归纳便笺本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dollqe/r_how_far_can_transformers_reason_the_locality/</link>
      <description><![CDATA[  由    /u/marojejian  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dollqe/r_how_far_can_transformers_reason_the_locality/</guid>
      <pubDate>Wed, 26 Jun 2024 00:39:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用模板语言增强法学硕士 (LLM) 的手动文本训练数据生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dok8fb/pamplifying_manual_text_training_data_generation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dok8fb/pamplifying_manual_text_training_data_generation/</guid>
      <pubDate>Tue, 25 Jun 2024 23:34:06 GMT</pubDate>
    </item>
    <item>
      <title>[N] Karpathy 已开始推出新系列“LLM101n”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1doj9kv/n_karpathy_has_begun_a_new_series_llm101n/</link>
      <description><![CDATA[https://github.com/karpathy/LLM101n    由   提交  /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1doj9kv/n_karpathy_has_begun_a_new_series_llm101n/</guid>
      <pubDate>Tue, 25 Jun 2024 22:50:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尝试根据 PII 数据对 Deberta-v3 进行微调。是否可以将 mbert“spans”转换为 SentencePiece 可以使用的内容？详情请参阅帖子。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1doft4d/d_attempting_to_finetune_debertav3_on_pii_data_is/</link>
      <description><![CDATA[嗨， 我正在尝试在此处的 ai4privacy 300k 数据集上微调 Deberta-v3：https://huggingface.co/datasets/ai4privacy/pii-masking-300k。特别是，我正在尝试对其进行微调以进行标记分类，但不一定用于掩码。 幸运的是，数据集有 span_label 和 privacy_mask，但我意识到它们很难与 Deberta-v3 使用/想要的 SentencePiece 之类的东西一起使用。也就是说，span_label 和 privacy_mask 具有 [start_position, end_position, label]，这与 Distilbert 和 WordPiece 配合得很好，但 WordPiece 和 SentencePiece 之间似乎没有 1:1 的转换。 最后，我试图构建的是发送到 Deberta-v3 进行微调的示例，这些示例由 input_ids 和 labels（如 B-FIRSTNAME、B-LASTNAME 等）组成，与 SentencePiece 一致，但源自 WordPiece。 这可能吗？我是不是做错事了？ 谢谢！    提交人    /u/IThrowShoes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1doft4d/d_attempting_to_finetune_debertav3_on_pii_data_is/</guid>
      <pubDate>Tue, 25 Jun 2024 20:23:45 GMT</pubDate>
    </item>
    <item>
      <title>[P] AI Code Heist：一款探索 LLM 漏洞的互动游戏</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dofie5/p_ai_code_heist_an_interactive_game_to_explore/</link>
      <description><![CDATA[我很高兴推出 AI Code Heist，这是一款互动游戏，旨在帮助开发人员了解和利用大型语言模型 (LLM) 的漏洞。随着 LLM 的日益普及，认识到如何操纵这些强大的工具来引发不必要的响应至关重要。 在 AI Code Heist 中，您将与一个名为 Sphinx 的聊天机器人互动，它会隐藏密码。您的目标是使用即时工程和即时注入技术让 Sphinx 揭示隐藏的密码。该游戏提供了一种实用且引人入胜的方法来学习 LLM 的复杂性及其潜在的弱点。 查看 GitHub repo 以了解更多信息并在本地运行游戏：AI Code Heist GitHub Repo 快乐黑客！    提交人    /u/theKeySpammer   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dofie5/p_ai_code_heist_an_interactive_game_to_explore/</guid>
      <pubDate>Tue, 25 Jun 2024 20:11:39 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的总 epoch 数对同一 epoch 进行不同的结果 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dodfqp/different_results_for_the_same_epoch_using/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dodfqp/different_results_for_the_same_epoch_using/</guid>
      <pubDate>Tue, 25 Jun 2024 18:44:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习笔记本电脑 - GPU</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dod55h/d_laptop_for_ml_gpu/</link>
      <description><![CDATA[大家好， 我想就一个在这里出现过多次的话题寻求一些建议，但我希望获得一些新的观点。 我是一名机器学习工程师，主要从事计算机视觉任务。我的大部分繁重工作都是在我的工作桌面、训练集群或其他远程机器上远程完成的。但是，我发现在本地运行原型或处理个人项目很方便，而不必总是连接到远程机器。 我目前正在考虑购买一台新的 ThinkPad，并试图在性能和便携性之间取得平衡。具体来说，我很好奇其他 ML 工程师使用的 GPU 设置类型：  您是喜欢至少拥有一些专用 GPU，还是始终依赖远程计算（或者使用集成 GPU 进行本地计算）？ 如果您更喜欢为笔记本电脑使用专用 GPU，那么它的功能有多强大？  到目前为止，我的结论是，除非我预算有限（尤其是因为我做其他使用 GPU 的事情，例如查看点云），否则最好使用一些专用 GPU，但中档 GPU 就足够了。 目前，我正在使用配备 RTX 2060 Max-Q 的 Dell XPS 17 9700。这是一台坚固的机器，但有点重，而且可能会变得很响很热。我正在考虑功能较弱的 GPU（或根本没有）是否足够。我没有预算限制，但我有点喜欢真正的“ThinkPad 体验”，即购买便宜的二手型号，同时仍拥有出色、可靠的机器。 任何见解或建议都将不胜感激！    提交人    /u/Mateo-dev   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dod55h/d_laptop_for_ml_gpu/</guid>
      <pubDate>Tue, 25 Jun 2024 18:32:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 码本崩溃</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1docuy4/d_codebook_collapse/</link>
      <description><![CDATA[我正在训练一个模型来学习一个用于量化编码器输出的码本，类似于 VQ-VAE 中使用的方法。我的目标是通过使用最接近的码字索引来表示编码器嵌入，从而对其进行标记。但是，我遇到了一个问题，即码字彼此非常相似，这使得稳健的标记变得困难。 有没有办法确保模型学习不同的码字？ 此外，我没有像在 VQ-VAE 中那样重建输入。相反，我使用作为量化嵌入函数的损失函数来训练模型。    提交人    /u/as13ms046   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1docuy4/d_codebook_collapse/</guid>
      <pubDate>Tue, 25 Jun 2024 18:20:53 GMT</pubDate>
    </item>
    <item>
      <title>根据历史预测事件时间的最佳技术[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1do9ur6/best_technique_to_predict_the_timing_of_an_event/</link>
      <description><![CDATA[我有一份历史数据，记录了一天中不同时间点的多次发货。发货可能会根据当月日期提前或延迟。例如，发货可能会在当月第一个工作日提前发货，而在月底则明显延迟。 我有过去 6 个月的数据，哪个模型最好，以便我可以使用过去的数据预测当天的发货时间。我也希望根据当前日期定期对模型进行反馈。[D]    提交人    /u/Dhinakharan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1do9ur6/best_technique_to_predict_the_timing_of_an_event/</guid>
      <pubDate>Tue, 25 Jun 2024 16:14:09 GMT</pubDate>
    </item>
    <item>
      <title>[N] ESM3：用语言模型模拟 5 亿年的进化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1do91g9/n_esm3_simulating_500_million_years_of_evolution/</link>
      <description><![CDATA[博客文章：https://www.evolutionaryscale.ai/blog/esm3-release 预印本（待批准）：https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf 摘要：  超过三十亿年的进化产生了编码到天然蛋白质空间中的生物学图像。在这里，我们展示了在进化产生的标记上训练的语言模型可以充当进化模拟器，以生成远离已知蛋白质的功能性蛋白质。我们提出了 ESM3，这是一种前沿的多模态生成语言模型，可以推理蛋白质的序列、结构和功能。ESM3 可以遵循结合其模态的复杂提示，并且对生物比对反应灵敏。我们已经促使 ESM3 生成具有思维链的荧光蛋白。在我们合成的几代中，我们发现了一种明亮的荧光蛋白，与已知的荧光蛋白相距很远（58% 相同）。同样遥远的天然荧光蛋白被五亿多年的进化所隔开  EvolutionaryScale 在剥离 Meta 后首次发布了重大版本。  权重和代码已发布，但有重大警告 来自 HuggingFace： https://www.evolutionaryscale.ai/legal/community-license-agreement 总体情况： EvolutionaryScale AI 模型仅根据本社区许可协议供个人或非商业组织用于非商业用途。您不得将 EvolutionaryScale AI 模型或 EvolutionaryScale AI 模型的任何衍生作品或其输出用于： a. 与任何商业活动有关，例如 b. 开发任何产品或服务，例如在 API 后面托管 AI 模型；或 c. 与药物开发有关；或 d.而不归属于 EvolutionaryScale 和本社区许可协议；或者 例如，训练任何其他大型语言模型、任何用于蛋白质表示学习或蛋白质生成的技术或任何其他类似于 EvolutionaryScale 的 AI 模型的 AI 驱动的第三方模型，即使用于非商业用途。 您可以根据社区许可协议发布、共享和调整 EvolutionaryScale AI 模型及其输出用于非商业目的    提交人    /u/TeamArrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1do91g9/n_esm3_simulating_500_million_years_of_evolution/</guid>
      <pubDate>Tue, 25 Jun 2024 15:40:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR与AISTATS之间的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1do796a/d_difference_between_iclr_and_aistats/</link>
      <description><![CDATA[这里有一个重复的问题，但我想再次提出这个话题，因为九月/十月的截止日期即将到来，现在情况可能已经发生了变化。 三大 ML 会议是 ICML/NeurIPS/ICLR，它们将一年分为 3 个截止日期。然而，AISTATS 也享有良好的声誉。 ICLR 和 AISTATS 的截止日期非常接近，因此许多人不得不决定将自己的工作提交给哪个。 由于深度学习 (DL) 的流行，ICLR 迅速崛起，但现在人们似乎将其与 ICML/NeurIPS 等同对待，那里似乎有相当多的非 DL 和理论 ML 论文。 问题：对于纯经验性的 DL 论文，将它们提交给 ICLR 似乎是理所当然的。那么 (1) 具有更多理论结果的 ML 论文，或 (2) 没有 DL（例如统计 ML）的 ML 论文呢？ 将这些作品提交给 ICLR 和 AISTATS 的利弊是什么？ 需要考虑的一些方面：  对于这些类型的工作，AISTATS 是否会降低声望或受到 ML 社区的较少关注？ 向 ICLR 提交理论作品的体验如何？（例如，那里的审稿人会要求进行许多实验吗？） 行业/学术界对 ICLR 是否更重要，或者对 ICLR 和 AISTATS 的待遇相同（对于更多理论性作品）？  免责声明：请不要再说“作品本身比出版地点更重要”，这显然是正确的，但并没有太多的信息量。    提交人    /u/zy415   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1do796a/d_difference_between_iclr_and_aistats/</guid>
      <pubDate>Tue, 25 Jun 2024 14:24:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 LlamaIndex 索引进行 OS 海量文档分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1do4ujr/p_os_mass_document_analytics_with_llamaindex_index/</link>
      <description><![CDATA[      大家好，我与大家分享我的最新开源项目，用于在大量文档中进行海量数据提取和问答。您可以将目标数据模式定义为 pydantic 模型或 python 基元。布局元素和人工注释会自动嵌入并作为 LlamaIndex VectorStore 进行访问。如果您编写自定义 LlamaIndex 问答管道，它们将显示在前端并可应用于语料库。 我已经在 OpenContracts 上工作多年了。虽然它最初是一种标记和注释文档的工具，但由于 LLM 和矢量数据库的最新进展，我发布了一个新版本，其中包含许多很酷的功能，可以使用 LLM、矢量搜索和 AI 代理。它基于 Django，随着时间的推移，Django 变得越来越强大，这让我感到惊讶！ 主要功能：  管理文档 - 管理文档集合 布局解析器 - 自动从 PDF 中提取布局特征 自动矢量嵌入 - 为上传的 PDF 和提取的布局块生成 可插入式微服务分析器架构 - 让您分析文档并自动对其进行注释 人工注释界面 - 手动注释文档，包括多页注释。 LlamaIndex 集成 - 使用我们的矢量存储（由 pgvector 提供支持）和任何手动或自动注释的功能让 LLM 智能地回答问题。 数据提取 - 使用复杂的 LLM 支持的查询在数百个文档中提出多个问题行为。我们的示例实现使用 LlamaIndex + Marvin。 自定义数据提取 - 自定义数据提取管道可用于前端批量查询文档。  查看 repo 或文档！    提交人    /u/TallTahawus   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1do4ujr/p_os_mass_document_analytics_with_llamaindex_index/</guid>
      <pubDate>Tue, 25 Jun 2024 12:31:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 需要帮助理解这篇论文。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1do44r5/r_need_help_in_understanding_this_paper/</link>
      <description><![CDATA[我正在尝试理解论文“Instacart 的基于嵌入的杂货搜索模型”。我理解了预热和级联数据集的原因。我不明白共享编码器和双塔模型之间的区别。假设我有正的 &lt;查询，产品&gt; 匹配。它是如何训练并与负匹配进行比较的。有人可以建议如何重新创建这篇论文吗？ 处理 img i47q4rpxan8d1...    提交人    /u/Legitimate_Celery_69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1do44r5/r_need_help_in_understanding_this_paper/</guid>
      <pubDate>Tue, 25 Jun 2024 11:52:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如今，亚马逊和沃尔玛等电子商务公司如何产生互补推荐（或经常一起购买）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dnwfis/d_how_do_ecomm_companies_like_amazon_and_walmart/</link>
      <description><![CDATA[我最近开始研究推荐系统。我知道过去是使用一些统计算法（如 FP-Growth 和 Prod2Vec）来实现的。    提交人    /u/Abs0lute_Jeer0   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dnwfis/d_how_do_ecomm_companies_like_amazon_and_walmart/</guid>
      <pubDate>Tue, 25 Jun 2024 03:30:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>