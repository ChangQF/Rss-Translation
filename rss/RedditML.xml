<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 13 Feb 2024 03:15:01 GMT</lastBuildDate>
    <item>
      <title>[D] 正确准备数据集，设置 EfficientNetV2B0 模型以使用 Tensorflow 在自定义数据集上进行训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apiml5/d_properly_preparing_dataset_setting_up/</link>
      <description><![CDATA[我还是机器学习领域的新手。我想问我的代码有什么问题，结果模型没有正确分类苹果叶病。我在 4 个类的 7k 图像数据集上进行了训练，每个类都有大约 1.8k 的图像。每个类的总图像不相等，这会影响训练结果吗？还是我下面的代码有问题？ ```python from google.colab importdrivedrive.mount(&#39;/content/gdrive&#39;)import zipfile zip_ref = zipfile .ZipFile(&#39;/content/gdrive/MyDrive/dataset/data9k.zip&#39;, &#39;r&#39;) zip_ref.extractall(&quot;/content/dataset&quot;) zip_ref.close() 导入tensorflow为tf从tensorflow.keras.applications.imagenet_utils导入preprocess_input导入matplotlib.pyplot作为plt train_dataset = tf.keras.utils.image_dataset_from_directory(&#39;/content/dataset/datasets/train&#39;,batch_size=10,image_size= (224, 224), labels=&#39;inferred&#39;, label_mode=&#39;categorical&#39; ) validation_dataset = tf.keras.utils.image_dataset_from_directory( &#39;/content/dataset/datasets/test&#39;, batch_size=10, image_size=(224, 224), labels=&#39;推断&#39;, label_mode=&#39;分类&#39; ) val_batches = tf.data.experimental.cardinality(validation_dataset) test_dataset =validation_dataset.take(val_batches // 5) valid_dataset = valid_dataset.skip(val_batches // 5) AUTOTUNE = tf.data.AUTOTUNE train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE) validation_dataset = valid_dataset.prefetch(buffer_size=AUTOTUNE) test_dataset = test_dataset.prefetch (buffer_size=AUTOTUNE) data_augmentation = tf.keras.Sequential( [tf.keras.layers.RandomFlip(&#39;horizo​​ntal&#39;), tf.keras.layers.RandomRotation(0.2)] )  模型 = tf.keras.applications.efficientnet_v2.EfficientNetV2B0( include_top=False, Weights=None, input_tensor=None, input_shape=(224, 224, 3), pooling=&#39;avg&#39;, include_preprocessing=True ) model.trainable=False prediction_layer = tf.keras.layers.Dense(4,activation=&#39;softmax&#39;) inputs = tf.keras.Input(shape =(224, 224, 3)) x = data_augmentation(inputs) x = preprocess_input(x) x = model(x) x = tf.keras.layers.Dropout （0.2）（x）输出=预测层（x）模型= tf.keras.Model（输入，输出） model.compile（optimizer=tf.keras.optimizers.Adam（learning_rate=1e-4） ），loss=tf.keras.losses.CategoricalCrossentropy（），metrics=[&#39;accuracy&#39;]） model.fit（train_dataset，validation_data=validation_dataset，epochs=10） 从tensorflow.keras.preprocessing导入numpy作为np导入图像 img_path = &#39;gdrive/MyDrive/dataset/rust.jpg&#39; img = image.load_img(img_path, target_size=(224, 224)) img_array = image.img_to_array(img) img_array = np.expand_dims(img_array, axis=0) 预测 = model.predict(img_array) class_names = [&#39;apple_scab&#39;, &#39;black_rot&#39; , &#39;cedar_apple_rust&#39;, &#39;healthy&#39;] predicted_index = np.argmax(predictions[0]) label = class_names[predicted_index] print(&quot;预测标签：&quot;, label) ```   由   提交 /u/Mammoth-Baker5144   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apiml5/d_properly_preparing_dataset_setting_up/</guid>
      <pubDate>Tue, 13 Feb 2024 02:41:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 跟踪医疗保健领域会议截止日期</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/</link>
      <description><![CDATA[        由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/</guid>
      <pubDate>Tue, 13 Feb 2024 01:14:39 GMT</pubDate>
    </item>
    <item>
      <title>[R]“预训练语言模型的监督对比学习”的实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apf8pr/r_implementation_of_supervised_contrastive/</link>
      <description><![CDATA[谁能帮我实现这篇论文：https: //arxiv.org/abs/2011.01403 我找不到这篇论文的任何官方实现。我找到的一个 GitHub 存储库并未产生论文中提到的预期结果。   由   提交 /u/Awkward_Grab_6189   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apf8pr/r_implementation_of_supervised_contrastive/</guid>
      <pubDate>Tue, 13 Feb 2024 00:02:37 GMT</pubDate>
    </item>
    <item>
      <title>您的 RAG 设置中有什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/</link>
      <description><![CDATA[您在 RAG 中使用哪些框架和库？  我最好奇LangChain是否像以前一样受欢迎？ 这是我的高级版本：   langchain使用OpenAI用于创建嵌入 Pinecone 用于存储嵌入 langchain 用于加载文档分割器和字符分割器以进行分块 Mongo 用于对话内存  ​   由   提交 /u/EnvironmentalDepth62   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/</guid>
      <pubDate>Mon, 12 Feb 2024 22:14:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 马尔可夫注意力：通过马尔可夫链对 Transformers 进行原理分析的框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apc855/r_attention_with_markov_a_framework_for/</link>
      <description><![CDATA[大家好，我正在分享我们最近通过马尔可夫链分析变压器的工作。特别是，我们设计了一个框架，可以对这些模型进行系统的理论和实证分析。论文在这里：https://arxiv.org/abs/2402.04161 期待您的建设性意见反馈和意见！ :)   由   提交 /u/pikachuchameleon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apc855/r_attention_with_markov_a_framework_for/</guid>
      <pubDate>Mon, 12 Feb 2024 21:55:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 RMSNorm 在 Transformer 中比 LayerNorm 更快很重要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/</link>
      <description><![CDATA[最近发布的 LLM 中很大一部分都使用 RMSNorm 而不是 LayerNorm。 原始 RMSNorm 论文 (https://arxiv.org/pdf/1910.07467.pdf），我见过的大多数参考文献都认为 RMSNorm 比 LayerNorm 更好，因为它是计算效率更高。 但是，LayerNorm 只占整体计算的一小部分，因此我不清楚为什么加速会有很大帮助。渐进地，LayerNorm 为 O(d_model)，而像 MLP 这样的组件为 O(d_model2 )，或者注意力为 O(d_model*seq_len + d_model2 ）。 是否只是 LayerNorm 的平均居中部分没有那么有用，因此 RMSNorm 可以在不影响表现力的情况下为您带来轻微的效率提升？或者 RMSNorm 还有其他我没有看到的好处吗？   由   提交/u/kei147  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/</guid>
      <pubDate>Mon, 12 Feb 2024 21:09:53 GMT</pubDate>
    </item>
    <item>
      <title>RVC - 如何增加合并声音的声音“范围”？ [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1apayay/rvc_how_to_increase_vocal_range_on_merged_voices_p/</link>
      <description><![CDATA[大家好，我正在创建一些声音的自定义合并，以获得我正在寻找的独特声音，但我的所有迭代都存在一个问题是无法唱出高音。例如，如果我发出高音调的“eeeeeeeeeeeee”它噼啪作响，然后就断了。笑声也一样——听起来很刺耳，更像是麦克风出了问题。 我需要找到一个范围更好的型号吗？有没有办法合并多个模型并包含一个具有更好范围的模型而不破坏现有的声音？我确信这很复杂，但我是这个场景的新手，所以寻求一些指导。谢谢！   由   提交 /u/doomdragon6   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1apayay/rvc_how_to_increase_vocal_range_on_merged_voices_p/</guid>
      <pubDate>Mon, 12 Feb 2024 21:03:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 斯坦福大学 Tri Dao 专访：论 FlashAttention 和稀疏性、量化和高效推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/</link>
      <description><![CDATA[Imbue 的一般智能播客的新一集与 Tri Dao 合作，FlashAttention 和Together AI 首席科学家。 涵盖的一些主题：  对循环连接进行逆向押注，而不是注意力 使用数据增强将知识编码为模型 设计利用硬件的算法  听对话：  Spotify 苹果播客 袖珍广播 摘要和参考论文  &lt;!-- SC_ON - -&gt;  由   提交 /u/thejashGI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/</guid>
      <pubDate>Mon, 12 Feb 2024 20:20:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 生成分割模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap99c1/d_generative_segmentation_model/</link>
      <description><![CDATA[我目前正在开发一个用于分割图像的系统。在图像中我有大的物品和小的物品。现在我为大件物品制作了一些标签，并且可以很好地检测它们。有时我的模型也会检测到较小的。现在我想更深入地找到更小的。由于标记需要很长时间，我问自己是否有一种方法可以使用我的模型来预测和找到较小的项目？目前我正在使用 UNet，但我愿意切换架构。 我想到的是改变损失函数，以允许在训练期间出现一些小“错误”，以鼓励学习较小的物品。或者也许变压器方法可以在这里工作。 已经存在这样的东西了吗？ 始终开放想法、论文和讨论:) &lt; !-- SC_ON --&gt;  由   提交/u/Schrifti   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap99c1/d_generative_segmentation_model/</guid>
      <pubDate>Mon, 12 Feb 2024 19:57:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] 神经网络可训练性的边界是分形的 - Jascha Sohl-Dickstein</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/</link>
      <description><![CDATA[值得一读，只是为了简洁的可视化。 https://sohl-dickstein.github.io/2024/02/12/fractal.html https://arxiv.org/abs/2402.06184   由   提交 /u/currentscurrents   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/</guid>
      <pubDate>Mon, 12 Feb 2024 19:50:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 4xGPU 1U 服务器构建 ML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap7upj/d_ml_build_using_4xgpu_1u_server/</link>
      <description><![CDATA[我有一个1028 服务器设计为托管最多 4 个 GPU。它是根据 Pascal 时代设计的。 所以现在我正在阅读这个 Reddit 子版块，了解 P40s、P100s 和 3060s。  我认为我的观点是对的，对于娱乐和学习的训练来说，4 倍较小的 GPU 比 1 倍 3090 更好。例如，配置 4x P40 意味着 96 GB 的 VRAM。当然速度较慢，但​​主要是我希望不受模型大小的限制。  我希望主要通过 Jupyter 书籍进行工作，因为这就是我正在学习的内容。 我目前正在参加有关 TensorFlow 和 keras 的 Udemy 课程。并且在我的 3060 桌面上运行了 Automatics SD、PrivateGPT 等。 我当然想设置 LLM 等，但最终我当然想接受培训。 我没有立即去的预算对于 multi 3090。 我现在想先向每个 GPU 发送低于 200 美元的价格。  那么对于这样的多 GPU 构建，您会构建什么 GPU 呢？用 12GB 3060 或 24GB P40 填充它？  构建的其余部分如下： sys-1028gq-trt 2x 2690 v3s 64 GB DDR4@2133 MHz 双冗余 1000W PSU（如果连接到 220V，则为 2000W） 我还有一台配备 Ryzen 3600、3060 12GB 和 16 GB RAM 的台式机。    由   提交/u/Specialist_Chef_5491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap7upj/d_ml_build_using_4xgpu_1u_server/</guid>
      <pubDate>Mon, 12 Feb 2024 19:02:06 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] KV Cache 巨大并且成为 LLM 推理的瓶颈。我们以免微调 + 即插即用的方式将它们量化为 2 位。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/</link>
      <description><![CDATA[众所周知，批量推理是高效 LLM 服务的常见做法（这是 ChatGPT 等服务出现初始延迟的主要原因之一）。这种批处理实践的动机是，推理延迟主要受到模型加载的 I/O 成本的限制，而不是实际计算的限制，其中以批处理方式服务多个请求增加了可容忍的延迟增加，同时大大节省了每个令牌的成本。然而，批量推理（或长上下文任务，或两者）的一个问题是需要大量的 KV 缓存。正如 Jeff Dean 的之前的论文中所示：具有 bs=512 和  的 500B+ 模型seqlen=2048总 KV 缓存约为 3TB - 这是模型权重的 3 倍，并带来另一个 I/O 挑战，因为 GPU 需要将整个 KV 缓存加载到内存中在下一代代币中，计算核心再次大部分处于空闲状态。 当然，人们已经进行了各种尝试来减少 KV 缓存的大小。有些人通过使用逐出策略来丢弃不重要的令牌（例如，StremingLLM 和 H2O）；有些应用系统级优化，例如分页或卸载（例如，vLLM和FlexGen）。然而，对普通 KV 缓存量化的探索——据说可以带来直接的效率增益，同时与所有上述方法兼容——只看到了有限的性能保留。 我们探索了 KV 缓存量化的任务并发现关键挑战是密钥缓存中存在通道方面的异常值（通道=令牌d维度的某个索引）； 我们注意到这本身就是一个有趣的观察，因为这种模式在值缓存中不存在。沿着这个通道维度直接量化具有挑战性，因为新令牌以流式方式到达，这意味着我们永远不会知道下一个标记是否将包含异常值（或其范围）。考虑到这一点，我们提出了🥝KIVI，我们在一个小缓冲区的帮助下对键缓存进行每通道量化，对值缓存进行每令牌量化在 FP16 中。 我们的方法通过以 2 位量化的 KV 缓存实现了可接受的性能下降（针对 LM-Eval 和 LongBench 等实际任务进行评估时，平均准确率下降 &lt;1%）。这使得我们评估的 Llama/Mistral/Falcon 模型的峰值内存减少了 2.6 倍，同时批量大小增加了 4 倍，从而提高了 2.35 倍 - 3.47 倍的吞吐量。 🥝 KIVI：调整 - KV Cache 的免费非对称 2 位量化 📰 论文：https://arxiv.org/abs/2402.02750 😼 代码：https://github.com/jy-yuan/KIVI 📈快速浏览 主要结果   由   提交/u/choHZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/</guid>
      <pubDate>Mon, 12 Feb 2024 16:00:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] 征求对 Graphbook 的诚实反馈，Graphbook 是一个交互式计算平台，可以直观地拆箱和编辑变压器模型以进行应用研究。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ap24gt/p_soliciting_honest_feedback_on_graphbook_an/</link>
      <description><![CDATA[      ​ https://preview.redd.it/g4oh19wy56ic1.png?width=5096&amp;format=png&amp; ;auto=webp&amp;s=fd1fa1350f86281ba56c6daa1609b410f5120c71 项目 来源： 我和我的同事都是 MLE/应用研究人员，并且一直对尝试在生产 NLP 用例中排除故障和定制转换器感到恼火。这是从 BERT 在 Tensorflow1 上出现时开始的，当时你根本无法真正单步调试模型。澄清一下，当然，纯粹清理数据需要花费大量精力，但我们发现，通过深入研究模型架构，我们也可以更好地理解和解决问题。是的，TF1 是 5-6 年前的事，现在有了急切的执行和 PyTorch，这使得逐层查看数据变得更加容易，但共识似乎仍然很像做“手术”来进行编辑，这是随着新研究的扩展变得越来越重要。 另请注意，我们听说研究人员使用 Netron（25K+ 星github）来直观地调试事物，但抱怨它对于变压器架构来说非常有限，因为它是扁平的和只读的。 高级描述： 我们的项目 Graphbook 采用模型架构的可视化层次结构，并将其转换为交互式执行的拖放图（即图形）并让您查看每个变量的数据快照。它是一个 DAG，可让您直观地编排从模型输入到输出的数据流。每个操作要么是原始操作（通过 CUDA C++ 编程直接在 GPU 上运行），要么是复合操作，因此分解为子图。因此，这里的一些主要好处是，您无需迭代调试器即可查看每个变量的所有数据，并且当您进行更改时，它像 Jupyter Notebook 一样是交互式的（因此不会从头开始重新运行）。另一个主要好处是，您可以真正精细地钻取每个模型权重的读写，以便您可以观察每个权重的更新方式。除了模型设计器部分之外，Graphbook 还允许您训练和部署图表本身，因此 Graphbook 是模型设计器部分之上的 MLOps 工具。 youtube.com/watch?v=h-S4MdVn0XI&quot;&gt;短视频“商业广告”，其中提供了更多详细信息。 当前状态： &lt;该平台正处于“免费增值”版本的最后阶段，之后将是试用+订阅。我们正在发布预先训练的变压器图。我们现在能够相当快速地将模型从 PyTorch 和 HuggingFace 转移到 Graphbook。到目前为止，我们已经有了 BERT、GPT2、Flan-T5 和 Llama2，接下来我们正在开发 Mistral 和 Phi-2。 最终，我正在寻求有关我们的内容的反馈和原始意见。正在做。我们在贸易展（MLOps World、NYC AI Summit，以及将在 Nvidia GTC 和 ODSC Boston）上与人们交谈时听到了各种各样的反应，但到目前为止还没有真正有机会联系到我认为 /machinelearning 可能拥有一个更大的专家和应用研究人员小组。抱歉，如果这篇文章过于促销。   由   提交/u/graphbook  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ap24gt/p_soliciting_honest_feedback_on_graphbook_an/</guid>
      <pubDate>Mon, 12 Feb 2024 15:08:47 GMT</pubDate>
    </item>
    <item>
      <title>[D]架构超参数优化策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aow8ri/d_architecture_hyperparameter_optimisation/</link>
      <description><![CDATA[我想知道是否值得对模型架构进行广泛的超参数调整。学习率调整通常会带来回报，因为这对收敛和整体性能有很大影响，但是在调整架构（num_layers、num_heads、dropout 等）时，我发现如果你保持在某个最佳范围内，实际性能差异是边缘。难道我做错了什么？您对此有什么经验？   由   提交 /u/Primary-Wasabi292    reddit.com/r/MachineLearning/comments/1aow8ri/d_architecture_hyperparameter_optimization/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aow8ri/d_architecture_hyperparameter_optimisation/</guid>
      <pubDate>Mon, 12 Feb 2024 09:35:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>