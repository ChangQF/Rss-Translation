<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 09 May 2024 18:17:24 GMT</lastBuildDate>
    <item>
      <title>[P] 合适的神经网络架构推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1co3cdx/p_suitable_neural_network_architecture/</link>
      <description><![CDATA[      我正在开发一个需要神经网络的项目  输入：过程参数：P1，P2，P 3（范围10s， 100s) 输出：O1，O2，图像（100x200 数组 [范围 0:90]）  有人可以推荐最适合此场景的神经网络架构吗？ （如图所示）。我之前使用 MLP 进行回归，使用 CNN 计算图像结构的大小。 （我的背景是机械工程。） https://preview.redd.it/afmzw17ivfzc1.jpg?width=1046&amp;format=pjpg&amp;auto=webp&amp;s=297c14ae02b4a155c527aad3bb54c8b1040d9662   由   提交 /u/mrpacetv   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1co3cdx/p_suitable_neural_network_architecture/</guid>
      <pubDate>Thu, 09 May 2024 17:52:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]“特征”一词从何而来？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1co2ye4/d_where_does_the_term_feature_come_from/</link>
      <description><![CDATA[也许是一个愚蠢的琐事问题，但我无法弄清楚。 ML 将特征称为特征，统计将特征称为预测变量，数学将特征称为特征变量，工程也将特征变量称为特征。 我知道它们是什么，但为什么我们称它们为特征？有谁知道起源故事吗？   由   提交 /u/FirefoxMetzger   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1co2ye4/d_where_does_the_term_feature_come_from/</guid>
      <pubDate>Thu, 09 May 2024 17:35:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] 通过闭路电视录像监控员工/员工的生产力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1co2qan/p_staffemployee_productivity_monitoring_with_cctv/</link>
      <description><![CDATA[大家好，任何形式的见解和信息都值得赞赏。我想做一个项目 - “通过闭路电视录像监控员工/员工的生产力”。但我对此很陌生，不知道如何有效地解决这个问题。让我简单介绍一下问题，  首先，我需要检测员工的特定动作以标记为工作和不工作，首先像在办公桌上工作这样的简单动作目前就可以了，我需要用动作训练模型 然后我想改变动作来检测不同员工的多项任务。  谢谢   由   提交/u/ahar_AIM   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1co2qan/p_staffemployee_productivity_monitoring_with_cctv/</guid>
      <pubDate>Thu, 09 May 2024 17:25:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] QServe：W4A8KV4 量化和系统协同设计，实现高效的 LLM 服务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1co2k7i/r_qserve_w4a8kv4_quantization_and_system_codesign/</link>
      <description><![CDATA[📚 研究论文：http://arxiv.org/abs/2405.04532v1 🤔 原因：现有的 INT4 量化技术无法在基于云的大批量语言模型服务中实现性能提升，因为 GPU 的运行时开销很大。 💻 怎么做？：研究论文提出了一种新的量化算法 QoQ，代表 quattuor-octo-quattuor，使用 4 位权重、8 位激活和 4 位 KV 缓存。该算法在 QServe 推理库中实现，旨在通过引入渐进量化来减少 GPU 上的反量化开销。 **此外，研究论文引入了 SmoothAttention 来减轻由 4 位 KV 量化引起的准确度下降。QServe 还执行计算感知权重重新排序，并利用寄存器级并行性来减少反量化延迟。最后，QServe 利用融合注意力内存绑定来进一步提高性能。 🦾 性能提升：与现有技术相比，研究论文实现了显着的性能改进。QServe 在 A100 上将 Llama-3-8B 的最大可实现服务吞吐量提高了 1.2 倍，在 L40S 上提高了 1.4 倍；在 A100 上将 Qwen1.5-72B 的最大可实现服务吞吐量提高了 2.4 倍。    提交人    /u/dippatel21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1co2k7i/r_qserve_w4a8kv4_quantization_and_system_codesign/</guid>
      <pubDate>Thu, 09 May 2024 17:18:08 GMT</pubDate>
    </item>
    <item>
      <title>[D]零售公司流失分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1co2bsp/d_churn_analysis_on_retail_company/</link>
      <description><![CDATA[回到基础： 我正在为一家想要启动客户流失分析项目的公司分析购买数据。这是一个基本的机器学习问题，你会说这是一个非常琐碎的分类。但它在数据方面存在很多问题，特别是：该公司是一家连锁超市，识别哪个客户是流失者极其困难。 目前采用的方法是定义一个时间范围并计算自上次收到以来的天数。通过这种研究模式，我们验证了在 2023 年每个双月的示例样本中，最后一次收货与双月结束之间的平均天数为 4 周！因此，很难说谁是流失者，需要花费多长时间？ 您是否曾遇到过零售客户这样的问题？您有什么建议吗？ 谢谢   由   提交 /u/suicidebootstrap   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1co2bsp/d_churn_analysis_on_retail_company/</guid>
      <pubDate>Thu, 09 May 2024 17:08:02 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 如何查找实例分割模型动物园存储库？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnzdh9/project_how_to_find_instance_segmentation_model/</link>
      <description><![CDATA[我正在开发一个使用 TensorFlow 进行实例分割的项目。教授告诉我找到 github 存储库，这些存储库是用于实例分割的模型动物园。它应该与 TensorFlow 一起使用，并且应该有预训练的模型。问题是我找不到动物园模型，而是单个模型。 如何找到用于实例分割的模型动物园并且与 TensorFlow 兼容的 github 存储库？ 除了链接和资源之外，我们非常感谢任何进一步的意见和建议。谢谢 到目前为止我尝试过的事情：  Google 搜索“instanceegmentation github”。 在 github 搜索中搜索“instanceegmentation”吧。 询问 ChatGpt 和 Gemini 是否可以为我找到任何存储库。我可以找到诸如 PaddlePaddle、supervision 或 AdelaiDet 等框架，但它们与 Tensorflow 不兼容。它们是相当独立的框架。我还可以找到作为实例分割模型动物园的存储库，但与 PyTorch 兼容。教授告诉我使用 TensorFlow，而不是 PyTorch。  到目前为止，我已经浏览了大约 50 到 60 个存储库。   由   提交/u/Complex_Tomatillo786   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnzdh9/project_how_to_find_instance_segmentation_model/</guid>
      <pubDate>Thu, 09 May 2024 15:01:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlphaMath Almost Zero：无需流程的流程监督</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnu9mx/r_alphamath_almost_zero_process_supervision/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.03553 代码：https ://github.com/MARIO-Math-Reasoning/Super_MARIO 模型：https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B 摘要： &lt; blockquote&gt; 大型语言模型 (LLM) 的最新进展极大地增强了他们的数学推理能力。然而，这些模型仍然难以解决需要多个推理步骤的复杂问题，经常导致逻辑或数值错误。虽然数字错误很大程度上可以通过集成代码解释器来解决，但识别中间步骤中的逻辑错误更具挑战性。此外，手动注释这些培训步骤不仅成本高昂，而且需要专业知识。在本研究中，我们引入了一种创新方法，通过利用蒙特卡罗树搜索（MCTS）框架自动生成过程监督和评估信号，从而消除了手动注释的需要。本质上，当法学硕士经过良好的预训练时，只需要数学问题及其最终答案来生成我们的训练数据，而不需要解决方案。我们继续训练一个阶梯级价值模型，旨在改进法学硕士在数学领域的推理过程。我们的实验表明，使用由 MCTS 增强的法学硕士自动生成的解决方案可以显着提高模型处理复杂数学推理任务的能力。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnu9mx/r_alphamath_almost_zero_process_supervision/</guid>
      <pubDate>Thu, 09 May 2024 10:48:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] ECCV 2024回顾讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cntiks/d_eccv_2024_review_discussion/</link>
      <description><![CDATA[我认为，与其他会议一样，我们可能会为提交给 ECCV 的人们进行讨论，因为评论将在 10 小时内发布（晚上 10 点（欧洲中部夏令时间）。这是我第一次在任何地方提交，说实话我很紧张。   由   提交/u/mr_birrd  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cntiks/d_eccv_2024_review_discussion/</guid>
      <pubDate>Thu, 09 May 2024 10:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于三年级博士生来说，开始提交 TPAMI 是一个好主意吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnrv30/d_is_it_a_good_idea_for_a_3rd_year_phd_student_to/</link>
      <description><![CDATA[我的简历中最近接受了一篇论文（由 ICML），根据我的导师的说法，可以通过额外的扎实工作提交给 trans。我确实有一些学术野心，想在 TPAMI 等顶级跨性别杂志上发表文章。然而，我听到很多同行抱怨，提交给跨性别者可能需要 6 到 12 个月的时间，而且结果没有保证。由于我正处于博士学位的第三年末，这可能相当危险，因为它可能会导致不必要的推迟毕业。那么我应该继续专注于会议吗？提前致谢。   由   提交/u/INeedPapers_TTT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnrv30/d_is_it_a_good_idea_for_a_3rd_year_phd_student_to/</guid>
      <pubDate>Thu, 09 May 2024 08:02:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 VQ-VAE 进行 SSL？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnr7ax/d_use_vqvaes_for_ssl/</link>
      <description><![CDATA[VQ-VAE 已成功用于将图像转换为扩散模型 (LDM) 的代表性潜在空间。然而，对于自监督学习，我找不到人们大量使用它们来创建嵌入，该嵌入稍后可以用作下游模型的输入来预测例如图像类。 你知道为什么吗？是？直觉上，我认为 VQ-VAE 也应该产生相当好的嵌入。    由   提交/u/That_Phone6702   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnr7ax/d_use_vqvaes_for_ssl/</guid>
      <pubDate>Thu, 09 May 2024 07:15:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 多元时间序列的矩阵轮廓与深度学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnpo6n/d_matrix_profile_vs_deep_learning_for/</link>
      <description><![CDATA[大家好， 所以我阅读了大量的方法，特别是关于多元时间序列和实时研究的方法人类活动识别（HAR）。不过，我最近偶然发现了 Eamonn Keogh 在 Matrix Profiles 方面令人惊叹且全面的工作，最终陷入了困境。  但是出于好奇，在多元时间序列和实时数据流的背景下，矩阵配置文件与深度学习方法（例如 MLP、LSTM 等）相比如何？  我很想听听其他人的观点！   由   提交 /u/peachjpg111   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnpo6n/d_matrix_profile_vs_deep_learning_for/</guid>
      <pubDate>Thu, 09 May 2024 05:31:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 审稿人你们都需要停止那么懒惰的狗。为什么审稿人做事这么懒？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnn54n/d_reviewers_you_all_need_to_stop_being_so_lazy/</link>
      <description><![CDATA[我提交了一篇论文。 被会议接受。 收到来自某个随机家伙的电子邮件_插入_大学_。发送给主席和会议负责人。 指责我抄袭，并说发表论文的匹配度为 92%... 检查交叉引用。标题、作者（我和导师）、数据、结论，几乎整篇论文都被突出显示。 只有来源说 Arkiv。我偶然在那里有我的预印本。我按照他们的政策预印本并张贴了通知。 现在，这是非常愚蠢的。我做了很多尽职调查，如果它与作者匹配，它必须引用我的预印本。 为什么审稿人如此懒惰，可以采取如此激烈的行动，而不是仅仅向作者询问有关这些的问题？我真的不理解其中一些人。对于处理这些情况您有什么建议吗？   由   提交 /u/I_will_delete_myself   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnn54n/d_reviewers_you_all_need_to_stop_being_so_lazy/</guid>
      <pubDate>Thu, 09 May 2024 03:03:45 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 一致性 LLM：将 LLM 转换为并行解码器可将推理速度提高 3.5 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/</link>
      <description><![CDATA[      大家好！我们在这里分享我们的最新工作：一致性大语言模型（CLLM），这是一个新的模型系列，能够通过有效地并行解码𝑛令牌来减少推理延迟。您的 LLM 服务/本地部署的新朋友，推理速度更快！ 🔥 请查看我们的博客文章以获取 3.1 倍加速的演示： https://hao -ai-lab.github.io/blogs/cllm/ 与现有的快速解码技术相比，CLLM 实现快速并行解码无需：  草稿模型 架构修改/辅助模型组件  这为 CLLM 带来了许多优势：  CLLM 不必处理获取“良好”草稿模型以及在单个系统中管理两个不同模型的复杂性。 CLLM 与目标 LLM 共享相同的架构，无需额外的工程设计将该技术应用于不同模型时的努力。 CLLM 可以与其他技术无缝集成，以实现高效的 LLM 推理（例如 Lookahead 解码），从而实现更显着的加速。   CLLM 使用的这种解码方法称为 Jacobi 解码，与传统的自回归解码相比，它提高了推理效率。 CLLM 的训练目标是通过尽可能少的步骤将任何随机初始化的 𝑛 令牌序列映射到与 AR 解码相同的结果，从而执行高效的 Jacobi 解码。 实验结果证明了 CLLM 的有效性，显示各种任务的生成速度提高了 2.4 倍到 3.4 倍。 与 Medusa2 相比，CLLM 实现了相当或更好的性能，但**不需要额外的参数或树式验证** CLLM 训练目标可视化 请参阅 &lt; a href=&quot;http://arxiv.org/abs/2403.00835&quot;&gt;我们的论文了解更多详细信息。欢迎尝试我们的代码库和 CLLM检查点！ 如果您觉得我们的工作有趣，请订阅、点赞或转发，谢谢！了解更多信息并在 Twitter 上与我们互动： https://x.com/haoailab/status/1788269848788869299&lt; /a&gt;   由   提交 /u/No_Yogurtcloset_7050   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/</guid>
      <pubDate>Wed, 08 May 2024 21:15:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer 如何在单次梯度更新后记住事实？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/</link>
      <description><![CDATA[我想我首先希望引用该事实，或者让有人告诉我这是我编造的。但民间知识是，针对单个时期训练的 Transformer 可以回忆起仅在训练数据集中出现过一次的事实。这意味着一次更新足以修改权重以产生正确的输出（不会灾难性地忘记其他事实）。 这对我来说真的很惊讶。我认为一次大到足以大幅修改输出的单个更新将具有相当大的破坏性，并且考虑到损失情况的非单调性，可能只是无法达到您想要的效果。对于如何/为什么会发生这种情况，是否有一个很好的答案，如果有的话，任何人都可以提供调查此问题的研究链接吗？它是大型模型（如 NTK）的特征，还是 Transformer 架构的特征，还是其他什么？请注意，我不是在询问上下文学习，而是在询问单个梯度步骤的变化。   由   提交 /u/asdfwaevc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/</guid>
      <pubDate>Wed, 08 May 2024 20:14:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>