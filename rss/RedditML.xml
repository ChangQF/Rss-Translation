<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 19 Apr 2024 06:18:19 GMT</lastBuildDate>
    <item>
      <title>[D] neurips审稿人邀请邮件今年发出了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7p5ic/d_is_neurips_reviewer_invitation_email_out_this/</link>
      <description><![CDATA[用于在每年的这个时候接收邀请。也许我被遗忘了。   由   提交 /u/noname139713   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7p5ic/d_is_neurips_reviewer_invitation_email_out_this/</guid>
      <pubDate>Fri, 19 Apr 2024 05:35:51 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的概率[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7ocoq/probability_for_machine_learning_d/</link>
      <description><![CDATA[我是一名刚毕业的工程专业毕业生，正在从传统的软件工程岗位转换到专注于 ML/AI 的岗位。我在本科时学过概率入门课程，但最近的发展（例如扩散模型），甚至一些相对较旧的模型（例如 VAE 或 GAN）都需要对概率论有深入的了解。当我阅读这些模型时，我发现与概率相关的数学/概念很难理解。关于如何弥合知识差距有什么建议吗？    由   提交 /u/AffectionateCoyote86   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7ocoq/probability_for_machine_learning_d/</guid>
      <pubDate>Fri, 19 Apr 2024 04:47:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当我只有一组 PDF 文档时，如何评估 RAG - 检索和生成？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7oa6k/d_how_to_evaluate_rag_both_retrieval_and/</link>
      <description><![CDATA[假设我有 1000 个 PDF 文档，用作 RAG 管道的输入。 我想评估RAG 管道，以便我可以测量： - 哪些嵌入模型更适合我的数据？ - 哪些重新排序器有效并且需要它们？ - 哪些法学硕士给出了最真实和连贯的答案？ 我如何评估管道的这些步骤？ 根据我的研究，我发现大多数框架都需要标签来进行检索和世代评价。我如何使用法学硕士创建这些数据？还有其他技术吗？ 我发现的一些东西： 对于检索：使用 LLM 生成用于检索的综合排名标签。 我应该使用哪个法学硕士？我应该遵循哪些最佳实践？我可以查看任何代码吗？ 对于生成的文本： - 为每一代生成如上所述的合成标签。 - 使用法学硕士作为法官，根据所获得的背景和提出的问题对每一代进行评分。您会推荐哪些法学硕士？ 哪些技术对你们有用？   由   提交 /u/awinml1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7oa6k/d_how_to_evaluate_rag_both_retrieval_and/</guid>
      <pubDate>Fri, 19 Apr 2024 04:43:11 GMT</pubDate>
    </item>
    <item>
      <title>[项目] RL项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7o6eg/project_rl_project/</link>
      <description><![CDATA[大家好。 我想为一个班级项目构建我的这个想法，并且我需要其他人的一些意见。  我想构建一个可以玩漂移猎人游戏的人工智能算法（https://drift- Hunters.co/drift-hunters-games）。我想我必须构建一些强化学习程序，尽管我不确定如何组织状态表示和输入数据。我还想象我需要连续一段时间记录我的屏幕来收集数据。 我选择这个游戏是因为它有三个非常基本的命令（左转、右转和开车）前进），游戏的目的（永无止境）是最大化漂移分数。 任何想法都非常感激。如果您还需要更多信息，请告诉我。 谢谢大家。   由   提交/u/Valuable-Wishbone276   /u/Valuable-Wishbone276 reddit.com/r/MachineLearning/comments/1c7o6eg/project_rl_project/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7o6eg/project_rl_project/</guid>
      <pubDate>Fri, 19 Apr 2024 04:36:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 统一信息检索中的偏见和不公平：大型语言模型的挑战和机遇调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7o4xt/r_unifying_bias_and_unfairness_in_information/</link>
      <description><![CDATA[      PDF: https://arxiv.org/abs/2404.11457 GitHub：https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey ​ ; 摘要：随着大型语言模型（LLM）的快速发展，信息检索（IR）系统，例如搜索引擎和推荐系统，已经发生了重大的范式转变。这种演变在预示着新机遇的同时，也带来了新的挑战，特别是在偏见和不公平方面，这可能会威胁信息生态系统。在本文中，我们对法学硕士整合时IR系统中出现的紧迫偏见和不公平问题的现有研究进行了全面调查。我们首先将偏见和不公平问题统一为分布不匹配问题，为通过分布调整对各种缓解策略进行分类提供了基础。随后，我们系统地深入研究了法学硕士融入IR系统的三个关键阶段所产生的具体偏见和不公平问题：数据收集、模型开发和结果评估。在此过程中，我们仔细回顾和分析了最近的文献，重点关注与这些问题相关的定义、特征和相应的缓解策略。最后，我们确定并强调了未来工作中的一些悬而未决的问题和挑战，旨在激励投资者关系领域及其他领域的研究人员和利益相关者更好地理解和减轻法学硕士时代投资者关系的偏见和不公平问题。  ​ https://preview.redd.it /d48pt3sw6dvc1.png?width=1126&amp;format=png&amp;auto=webp&amp;s=2343460399473bde3f5e37c0bbcfdc88ffc81efb   由   提交/u/KID_2_2  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7o4xt/r_unifying_bias_and_unfairness_in_information/</guid>
      <pubDate>Fri, 19 Apr 2024 04:34:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过用旧方法提取大型语言模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7izc4/d_has_anyone_tried_distilling_large_language/</link>
      <description><![CDATA[因此，如今，每个人都在将从大型语言模型收集的基本原理提炼到另一个相对较小的模型中。然而，我记得从前我们在进行蒸馏时训练了小型网络以匹配大型网络的逻辑。这是忘记/尝试过并且今天不起作用吗？   由   提交 /u/miladink   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7izc4/d_has_anyone_tried_distilling_large_language/</guid>
      <pubDate>Fri, 19 Apr 2024 00:11:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 医疗领域基准上的 Llama-3（7B 和 70B）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c7b35q/d_llama3_7b_and_70b_on_a_medical_domain_benchmark/</link>
      <description><![CDATA[      Llama-3 正在人工智能社区掀起波澜。我很好奇它在医学领域的表现如何，以下是 Llama-3（7B 和 70B）在由 9 个不同数据集组成的医学领域基准上的评估结果 https://preview.redd.it/sdwx5tglxbvc1.png?width=1464&amp;format= PNG&amp; ;auto=webp&amp;s=d32585a69244d44c83e2b1e8a85301a7a8676ea2 我会进行微调、评估和释放 Llama-3 和在接下来的几天里，我们将在不同的医疗和法律基准上获得不同的法学硕士。请关注此处的更新：https://twitter.com/aadityaura https://preview.redd.it/9egbcayv9avc1.png?width=1344&amp;format=png&amp;auto =webp&amp;s=436a972421d5568e1a544962b8cfd1c7b14efe04   由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c7b35q/d_llama3_7b_and_70b_on_a_medical_domain_benchmark/</guid>
      <pubDate>Thu, 18 Apr 2024 18:45:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自信地展示您的工作：调整曲线的置信带</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c783my/r_show_your_work_with_confidence_confidence_bands/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2311.09480  推文：https://x.com/NickLourie/status/1770077925779337563  代码：https://github.com/nicholaslourie/opda 文档：https://nicholaslourie.github.io/opda/tutorial/usage.html 摘要：  超参数的选择极大地影响自然语言处理的性能。通常，很难判断一种方法是否优于另一种方法，或者只是更好地调整。调优曲线通过考虑调优工作来解决这种歧义。具体来说，他们将验证性能绘制为迄今为止尝试的超参数选择数量的函数。虽然这些曲线存在多种估计器，但通常使用点估计，我们发现点估计会默默地失败，并且当给出的数据太少时会给出矛盾的结果。除了点估计之外，置信带对于严格建立不同方法之间的关系也是必要的。我们提出了第一种为调谐曲线构建有效置信带的方法。这些频带是精确的、同步的且无分布的，因此它们为比较方法提供了坚实的基础。实证分析表明，虽然作为基线的引导置信带未能接近其目标置信度，但我们的置信带却完全达到了目标。我们通过消融验证我们的设计，分析样本量的影响，并提供将模型与我们的方法进行比较的指导。为了促进未来工作中的自信比较，我们发布了 opda：一个易于使用的库，您可以使用 pip 安装它。    由   提交/u/nicholaslourie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c783my/r_show_your_work_with_confidence_confidence_bands/</guid>
      <pubDate>Thu, 18 Apr 2024 16:46:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] InternVL v1.5开源，OpenCompass多模态基准测试排名第一</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c780cd/r_internvl_v15_open_sourced_ranking_first_in/</link>
      <description><![CDATA[      https://internvl.opengvlab.com/ 模型下载： https://huggingface.co/collections/OpenGVLab/internvl-65b92d6be81c86166ca0dde4 OpenCompass： https://rank.opencompass.org.cn 一些示例： https:// Preview.redd.it/vtwjml3qm9vc1.png?width=2508&amp;format=png&amp;auto=webp&amp;s=e32c044d4bc60ef28baf64dccdcb5fe9b10dfc61 https://preview.redd.it/p51vt3xpn9vc1.png?width=2609&amp;format=png&amp;auto=webp&amp;s = 73907e5ffb4d9b9bd4250cbce53e3bd29dedabf1   由   提交 /u/flyforlight   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c780cd/r_internvl_v15_open_sourced_ranking_first_in/</guid>
      <pubDate>Thu, 18 Apr 2024 16:42:45 GMT</pubDate>
    </item>
    <item>
      <title>[N] Meta 发布 Llama 3</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c77f0m/n_meta_releases_llama_3/</link>
      <description><![CDATA[      https://llama.meta.com/llama3 /  ​ ​ /u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c77f0m/n_meta_releases_llama_3/</guid>
      <pubDate>Thu, 18 Apr 2024 16:18:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 压缩线性代表智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c76tpe/r_compression_represents_intelligence_linearly/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.09937 代码：https://github.com/hkust-nlp/llm-compression-intelligence 数据集：https://huggingface.co/datasets/hkust-nlp/llm-compression 摘要：&lt; /p&gt;  人们相信，学习良好的压缩会带来智慧。最近，语言建模已被证明等同于压缩，这为大型语言模型（LLM）的成功提供了令人信服的理由：更高级语言模型的开发本质上是增强压缩，从而促进智能。尽管讨论如此吸引人，但关于压缩和智能之间相互作用的实证证据却很少。在这项工作中，我们在法学硕士的背景下研究了它们的关系，将法学硕士视为数据压缩器。考虑到“智力”的抽象概念，我们采用平均下游基准分数作为替代，特别针对与知识和常识、编码和数学推理相关的智力。我们的研究涵盖 12 个基准，汇集了来自不同组织的 30 名公共法学硕士。值得注意的是，我们发现法学硕士的智力（通过平均基准分数反映出来）几乎与他们压缩外部文本语料库的能力线性相关。这些结果提供了具体的证据，支持这样的观点：卓越的压缩能力意味着更高的智力。此外，我们的研究结果表明，压缩效率作为源自原始文本语料库的无监督指标，可以作为与模型功能线性相关的可靠评估指标。我们开源我们的压缩数据集以及数据收集管道，以方便未来的研究人员正确评估压缩。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c76tpe/r_compression_represents_intelligence_linearly/</guid>
      <pubDate>Thu, 18 Apr 2024 15:54:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 产品评估是讨论最多的话题之一</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c75q78/d_product_evaluations_is_one_of_the_most/</link>
      <description><![CDATA[我们是一家人工智能咨询公司，这种情况一次又一次地发生在我们身上...... 我们开始一个新的法学硕士项目客户。 他们的工程师很快就能完成 80%。 他们有很多边缘情况，希望我们完成剩余的 20%。  &gt;我们向他们询问有关评估的信息。 当然他们没有。 我们创建评估框架，迭代改进管道，瞧。  工作完成，每个人都很高兴。 我认真地认为，根据我们的观察，最好的人工智能产品团队将是那些在评估上花费大量时间的团队。它很无聊，很重复，但它区分了令人惊叹的人工智能产品和表现不佳的产品。   由   提交 /u/BootstrapGuy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c75q78/d_product_evaluations_is_one_of_the_most/</guid>
      <pubDate>Thu, 18 Apr 2024 15:10:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 100+标签文本分类问题。 “通常”的方法是什么？变形金刚？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c748uc/d_100_labels_textclassification_problem_whats_the/</link>
      <description><![CDATA[每个文本不超过15个单词，并且类别高度不平衡。但它们都至少有 30 个左右的实例。 我成功地处理了具有相同性质的数据，但具有大约 15 个标签以及梯度增强模型的集合。  在深入测试一堆模型之前，我想知道是否有一些策略可以解决像这样的高维问题。  有些问题是无法解决的，让我们面对现实吧。但是你们会尝试什么？   由   提交/u/Melodic_Reality_646   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c748uc/d_100_labels_textclassification_problem_whats_the/</guid>
      <pubDate>Thu, 18 Apr 2024 14:07:23 GMT</pubDate>
    </item>
    <item>
      <title>[N] 美联储任命“人工智能末日者”来管理美国人工智能安全研究所</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</link>
      <description><![CDATA[https://arstechnica.com/tech-policy/2024/04/feds-appoint-ai-doomer-to-run-us-ai-safety-institute/ 文章简介： 被任命为 AI 安全负责人的是 Paul Christiano，他是前 OpenAI 研究员，开创了一种名为基于人类反馈的强化学习的基础 AI 安全技术（ RLHF），但也因预测“人工智能发展有 50% 的机会以‘厄运’而告终”而闻名。尽管克里斯蒂安诺的研究背景令人印象深刻，但一些人担心，任命所谓的“人工智能厄运者”会带来灾难。 NIST 可能冒着鼓励非科学思维的风险，许多批评家认为这些思维纯粹是猜测。   由   提交/u/bregav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</guid>
      <pubDate>Wed, 17 Apr 2024 22:49:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>