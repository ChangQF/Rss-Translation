<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 12 Oct 2024 06:21:05 GMT</lastBuildDate>
    <item>
      <title>核技巧（RKHS）应用于逻辑：语义空间框架中的逻辑属性和量词</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1sfz2/the_kernel_trick_rkhs_applied_to_logic_logical/</link>
      <description><![CDATA[        由    /u/musescore1983   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1sfz2/the_kernel_trick_rkhs_applied_to_logic_logical/</guid>
      <pubDate>Sat, 12 Oct 2024 04:33:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAAI 2025 第一阶段决议泄露？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1plva/d_aaai_2025_phase_1_decision_leak/</link>
      <description><![CDATA[是否有人检查过 AAAI 提交的修订部分并注意到该论文已移至文件夹“Rejected_Submission”。它应该在 Venueid 标签下可见。我从推特帖子中了解到这一点： https://x.com/balabala5201314/status/1843907285367828606    提交人    /u/Wise_Witness_6116   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1plva/d_aaai_2025_phase_1_decision_leak/</guid>
      <pubDate>Sat, 12 Oct 2024 01:43:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么看起来谷歌的 TPU 对 nVidia 的 GPU 不构成威胁？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/</link>
      <description><![CDATA[尽管谷歌在其许多内部 AI 工作中使用了 TPU，但似乎并没有像 nVidia 的 GPU 那样推动其收入增长。这是为什么？为什么拥有自己的 AI 设计的处理器没有像 nVidia 那样对他们有所帮助，为什么所有其他专注于 AI 的公司似乎仍然只想在 nVidia 芯片上运行他们的软件……即使他们使用的是谷歌数据中心？    提交人    /u/kugelblitz_100   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/</guid>
      <pubDate>Sat, 12 Oct 2024 00:44:55 GMT</pubDate>
    </item>
    <item>
      <title>[N] Kaido Orav 和 Byron Knoll 的 fx2-cmix 赢得 7950 欧元的 Hutter 奖！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1l725/n_kaido_orav_and_byron_knolls_fx2cmix_wins_7950/</link>
      <description><![CDATA[Kaido Orav 和 Byron Knoll 凭借其&quot;fx2-cmix&quot; 参赛作品在 人类知识无损压缩 Hutter 奖 上提高了 1.59%。由于 Hutter 奖将参赛者限制为单一&quot;通用处理器，并使用&quot;https://www.youtube.com/watch?v=AKMuA_TVz3A&quot;&gt;最&quot;通用的损失函数，因此所需的算法进步&quot;通常适用，无论业界的&quot;硬件彩票&quot; 或损失函数妥协如何。在这方面，它为机器学习的科学进步提供了独特且低风险的激励。  与之前的 Hutter 奖获奖算法相比，fx2-cmix 的一些算法进步如下：  混合器和预测器：当错误低于某个阈值时，混合器现在会跳过权重更新，从而提高处理速度。  单次传递 维基百科转换：此更新通过将转换过程从以前的多步骤方法简化为单次传递，减少了处理维基百科等大型数据集所需的时间和磁盘使用量，从而显著加快了预处理阶段。  新的词干和上下文方法：利用自然语言处理技术（如词干过程中的新词类型）来创建更紧凑、更相关的词流。这不仅提高了训练数据的质量，而且还增强了压缩能力，降低了存储要求。  高效的文章排序：通过将整个文章嵌入到大向量中并使用 t-SNE 将其减少到单个维度，可以快速重新排序整个语料库以进一步加快训练速度。  有关进展的详细描述以及 Jupyter 笔记本和其他文档可在 fx2-cmix README 中找到。     提交人    /u/jabowery   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1l725/n_kaido_orav_and_byron_knolls_fx2cmix_wins_7950/</guid>
      <pubDate>Fri, 11 Oct 2024 21:56:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 复合学习单元：超越参数更新的广义学习，将 LLM 转变为自适应推理机</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1gtns/r_composite_learning_units_generalized_learning/</link>
      <description><![CDATA[  由    /u/jalabulajangs  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1gtns/r_composite_learning_units_generalized_learning/</guid>
      <pubDate>Fri, 11 Oct 2024 18:37:47 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] ODE/PDE 或数学问题的 ML 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g1e3vt/discussion_ml_models_for_odepde_or_math_problems/</link>
      <description><![CDATA[我正在寻找与用于解决 ODE/PDE（常微分方程和偏微分方程）的 ML/DL 模型相关的论文/材料（如果有的话）。此外，一般解决任何与数学相关的问题，如数学奥林匹克。我只是想研究这些，因为我来自数学背景，觉得这些领域真的很有前途。 谢谢。    提交人    /u/optimization_ml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g1e3vt/discussion_ml_models_for_odepde_or_math_problems/</guid>
      <pubDate>Fri, 11 Oct 2024 16:38:30 GMT</pubDate>
    </item>
    <item>
      <title>MLE-bench：在机器学习工程中评估机器学习代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g18dq4/mlebench_evaluating_machine_learning_agents_on/</link>
      <description><![CDATA[  由    /u/MTGTraner  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g18dq4/mlebench_evaluating_machine_learning_agents_on/</guid>
      <pubDate>Fri, 11 Oct 2024 12:16:30 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 模拟数据训练模型与实验数据训练模型之间的迁移学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/</link>
      <description><![CDATA[      您好， 我正在为我的硕士论文研究一个替代模型，该模型可以预测地震工程中高保真模型的参数。 我试图解决的问题是那些高保真度模型依赖于我们没有实际测量方法的参数，这些参数是根据经验法则和/或工程师的直觉设置的。这导致模拟和实验之间存在很大差异。 我们进行的实验在经济和时间方面都很昂贵，而且往往具有破坏性。因此，我们希望能够在破坏性阶段之前使用这些实验的初步结果来调整高保真模型。 因此，我计划的工作流程如下：  基于模拟数据构建基础模型，可能来自多个不同的高保真模型 使用来自特定实验的实验数据微调此基础模型。  我目前正在研究多种模型架构，但发现很少有有希望的... 预期工作流程 我的问题是： 你有什么建议吗？技术和模型方面，是否可以根据数据集非常有限的物理系统对模型进行良好的微调？ 任何其他建议都乐意接受！ 抱歉，如果这篇文章看起来有点幼稚，那么这个主题超出了我在大学学习的 ML 课程的范围。 非常感谢！    提交人    /u/DurandilAxe   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/</guid>
      <pubDate>Fri, 11 Oct 2024 11:20:03 GMT</pubDate>
    </item>
    <item>
      <title>[r][d] BigBench 的 SOTA 是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/</link>
      <description><![CDATA[BIgBench 在 2 年前引起轰动 (https://github.com/google/BIG-bench)。 但排行榜几乎没有更新。https://paperswithcode.com/sota/machine-learning-on-big-bench 有人知道 bigbench 的 SOTA 是什么吗？我见过这个：https://www.reddit.com/r/singularity/comments/1akz9u8/selfdiscover_google_deepmind_large_language/ 例如。但不是最近的论文。谢谢！    提交人    /u/sunchipsster   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/</guid>
      <pubDate>Fri, 11 Oct 2024 10:04:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 语音生物识别技术可行吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/</link>
      <description><![CDATA[我正在与一家处理大量音频数据的公司合作。所有数据都是单说话人，99% 的数据时长在 1 到 10 分钟之间。我们希望按说话人对这些数据进行排序，但无法通过人工标记处理负载。 我一直在阅读有关说话人分类和 SpeechBrain 和 PyannoteAudio 等库的文章，但我不确定这些是否适合我的用例。我不关心如何区分说话者，我关心的是高精度可聚类嵌入。 我粗略地浏览了一下文献，发现了以下内容：  深度说话者：端到端神经说话者嵌入系统 (2017) [575 引用] 用于短时说话者验证的深度说话者嵌入 (2017) [166 引用] VoxCeleb2：深度说话者识别 (2018) [2551 引用] Voxceleb：野外大规模说话人验证 (2020 年，可能是上述的官方印刷版) [722 引用] 基于深度学习的说话人识别：概述 (2021 年) [398 引用]  我是这个领域的新手，但直观地看，这似乎是一个可以解决的问题。我是否遗漏了什么显而易见的东西？在这个领域工作的人可以解释一下什么是 SOTA 吗？    提交人    /u/FPGA_Superstar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/</guid>
      <pubDate>Fri, 11 Oct 2024 10:03:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 差动变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/</link>
      <description><![CDATA[      论文 摘要  Transformer 倾向于将注意力过度分配到不相关的上下文中。在本研究中，我们引入了 Diff Transformer，它可以在消除噪音的同时放大对相关上下文的注意力。具体来说，差分注意力机制将注意力分数计算为两个单独的 softmax 注意力图之间的差值。减法可以消除噪音，促进稀疏注意力模式的出现。[...] [...] 它在实际应用中具有显着优势，例如长上下文建模、关键信息检索、幻觉缓解、上下文学习和减少激活异常值。[...]     提交人    /u/fliiiiiiip   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/</guid>
      <pubDate>Fri, 11 Oct 2024 06:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习造福人类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/</link>
      <description><![CDATA[您好，在日常公司工作中，我经历了一种存在危机。我觉得我所有的知识都浪费了，因为它们没有用来帮助我身边的更多人，我想知道我们如何才能利用我们的技能让机器学习更适用于小型企业。 您是否曾在小型企业工作过，并应用过一些机器学习或至少某种程度的数据工程来帮助他们简化流程？我们如何才能真正利用技术帮助改善小型企业？ 您是否有一些关于此类问题的文章或书籍，阅读它们并了解您的所有观点会很不错！    提交人    /u/ALESS885   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/</guid>
      <pubDate>Fri, 11 Oct 2024 02:00:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] nGPT：在超球面上进行表征学习的正则化 Transformer</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2410.01131 摘要：  我们提出了一种新颖的神经网络架构，即在超球面上进行表征学习的规范化 Transformer (nGPT)。在 nGPT 中，形成嵌入、MLP、注意矩阵和隐藏状态的所有向量都是单位范数规范化的。输入的 token 流在超球面的表面上传播，每一层都会向目标输出预测贡献一个位移。这些位移由 MLP 和注意块定义，它们的向量分量也位于同一个超球面上。实验表明，nGPT 的学习速度更快，可将实现相同准确度所需的训练步骤数减少 4 到 20 倍，具体取决于序列长度。  亮点：  我们的主要贡献如下：  超球面上的网络参数优化 我们建议将形成网络矩阵嵌入维度的所有向量归一化为位于单位范数超球面上。这使我们能够将矩阵向量乘法视为表示余弦相似度在 [-1,1] 内的点积。归一化使得权重衰减变得不必要。  归一化 Transformer 作为超球面上的可变度量优化器 归一化 Transformer 本身在超球面上执行多步优化（每层两步），其中注意力和 MLP 更新的每一步都由特征学习率（可学习可变度量矩阵的对角线元素）控制。对​​于输入序列中的每个标记 t_i，归一化 Transformer 的优化路径从超球面上与其输入嵌入向量相对应的点开始，并移动到超球面上最能预测下一个标记 t_i+1 的嵌入向量的点。 更快的收敛 我们证明，归一化 Transformer 将实现相同准确度所需的训练步骤数减少了 4 到 20 倍。 视觉亮点： https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&amp;format=png&amp;auto=webp&amp;s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df 不确定 20k 和 200k 预算之间的区别；可能绘制了使用不同初始学习率的运行的最佳结果 https://preview.redd.it/waof2llr7ytd1.png?width=1337&amp;format=png&amp;auto=webp&amp;s=3f82cee29c5fe753e219edf55ab16460fcf9a11a https://preview.redd.it/a5vburms7ytd1.png?width=859&amp;format=png&amp;auto=webp&amp;s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</guid>
      <pubDate>Thu, 10 Oct 2024 15:37:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>