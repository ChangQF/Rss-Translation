<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 09 Jan 2024 03:14:46 GMT</lastBuildDate>
    <item>
      <title>[D] 缓存教师模型的预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1922hly/d_caching_predictions_from_a_teacher_model/</link>
      <description><![CDATA[你好。我正在通过知识蒸馏来训练模型，我不想在每个时期都在相同的数据上对教师模型进行前向传递。有没有一种有效的方法将预测缓存到磁盘？   由   提交 /u/notEVOLVED   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1922hly/d_caching_predictions_from_a_teacher_model/</guid>
      <pubDate>Tue, 09 Jan 2024 01:39:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] WikiChat：通过维基百科上的少发基础来阻止大型语言模型聊天机器人的幻觉 - 在与人类用户就最新主题进行的对话中实现了 97.9% 的事实准确性，比 GPT-4 好 55.0%！ - 斯坦福大学 2023</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2305.14292v2  Github：https://github.com/stanford-oval/WikiChat  摘要：  本文提出了第一个基于 LLM 的少样本聊天机器人几乎从不产生幻觉，并且具有高会话性和低延迟。 WikiChat 以英语维基百科为基础，这是最大的精选自由文本语料库。  WikiChat 生成法学硕士的回复，仅保留有根据的事实，并将其与从语料库中检索到的其他信息相结合，形成事实且引人入胜的回复。 我们将基于 GPT-4 的 WikiChat 提炼为质量损失最小的 7B 参数 LLaMA 模型，以显着改善其延迟、成本和隐私，并促进研究和部署。  使用一种新颖的人类和法学硕士混合评估方法，我们证明我们最好的系统在模拟对话中达到了 97.3% 的事实准确性。它显着优于所有基于检索和基于 LLM 的基线，与 GPT-4 相比，在头部、尾部和近期知识方面分别提高了 3.9%、38.6% 和 51.0%。与之前最先进的基于检索的聊天机器人相比，WikiChat 的信息量和吸引力也显着提高，就像法学硕士一样。  WikiChat 在与人类用户就近期话题进行的对话中实现了 97.9% 的事实准确性，比 GPT-4 高出 55.0%，同时获得了更高的用户评分和更有利的评论。   https:/ /preview.redd.it/9mhpdh300bbc1.jpg?width=1225&amp;format=pjpg&amp;auto=webp&amp;s=cb64b717e920d7bf727782f7c803500ae838d6ef https://preview.redd.it/5dxesl200bbc1.jpg?width=862&amp;format=pjpg&amp;auto=webp&amp;s =b6de0cda980eec3cf3484ff1f9cd6dc1acf13505 https ://preview.redd.it/j387vl200bbc1.jpg?width=914&amp;format=pjpg&amp;auto=webp&amp;s=736fb922c1f98f4c7b132f1c153f4653a8b85441 https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&amp;format=pjpg&amp;auto=webp&amp; ;s=95b40a9cf67d7f3729dae85878db67a262cc5201   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/</guid>
      <pubDate>Tue, 09 Jan 2024 00:07:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 scikit-learn 寻求有关 LocalOutlierFactor 的 n_neighbors 优化初始化的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191y9lg/d_seeking_advice_on_optimal_initialization_of_n/</link>
      <description><![CDATA[您好r/machinelearning社区，&lt; /p&gt; 我正在开发一个项目，使用 scikit-learn 中的 LocalOutlierFactor 模型进行异常检测。我想知道为 n_neighbors 参数选择初始值的最佳实践。 感谢您的帮助   由   提交 /u/battlefieldanalytica   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191y9lg/d_seeking_advice_on_optimal_initialization_of_n/</guid>
      <pubDate>Mon, 08 Jan 2024 22:35:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在Python中选择pdf处理包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191uyuq/d_choosing_a_pdf_processing_package_in_python/</link>
      <description><![CDATA[我正在使用深度学习来理解文档，我必须处理大量 PDF 文档。我对 python 中的各种 pdf 处理包做了一些研究。以下是一些使用 Python 处理 pdf 的常用软件包。然而，我曾经对使用哪个包来执行不同的任务（例如合并 pdf、裁剪 pdf 和从 pdf 中提取文本）感到困惑。还有一个工具可以将扫描的 pdf 转换为可搜索的 PDF，这是我在研究之前不知道的。  PyPDF ：主要是 pdf 转换 Pdfminer.six：PDF 提取，包括布局信息 PdfPlumber ：在 PDFminer 之上添加表格提取功能 PyMuPDF ：最快的 PDF 处理，很多功能，包括 pdf 转换和文本提取、表格提取等&lt; /li&gt; OCRmyPDF ：转换扫描的 pdf到可搜索的 pdf  我还尝试在此博客中详细介绍该主题https://pythonify.com/blogs/pdf-packages-comparison-all-you-need-to-know 快乐的机器学习： )   由   提交/u/RelevantRevolution86  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191uyuq/d_choosing_a_pdf_processing_package_in_python/</guid>
      <pubDate>Mon, 08 Jan 2024 20:23:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] NeuralRad：第一个免费使用器官和肿瘤分割云</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191sfl4/p_neuralrad_first_free_to_use_organ_and_tumor/</link>
      <description><![CDATA[       通过与国际原子能机构 (IAEA) 的合作，我们了解到大多数第三世界国家的医院不具备技术和相应的基础设施，无法为医生、神经外科医生和医学物理学家提供易于使用的解决方案，以使用人工智能轻松快速地勾勒出危险器官 (OAR) 或肿瘤的轮廓在患者治疗工作流程中。我们决定致力于此并对该领域产生影响。  经过两年的努力，我们想推出 service.neuralrad.com，第一个免费使用全身器官-任何人都可以使用的风险 (OAR) 和肿瘤分割云平台。  我们使用一系列高性能 GPU 服务器（其中大多数是 Nvidia Geforce 4090 和 3090）构建此云平台，并且在任何特定时间动态分配超过100G的GPU内存，用于基于深度学习的快速分割推理。通过这项服务，我们希望帮助医学物理学家和医生解决放射治疗工作流程中棘手的病变和 OAR 分割问题。 p.s.该平台已被 IAEA 选择用于 IAEA 2023 年医学物理人工智能研讨会计划。 （https://www.iaea.org/events/evt2304232&lt;强&gt;)  免责声明：NeuralRad 云服务目前尚未获得 FDA 批准。我们建议将此服务用于研究和学习目的。 dicom 文件的所有患者信息都会在浏览器（客户端）端自动匿名，并且只有匿名的 dicom 数据才会发送到 NeuralRad 云服务器进行分段推理。  祝新年快乐！  新年快乐！ p&gt; ​ https://preview.redd.it/t5c9755dh9bc1.png?width=1746&amp;format=png&amp;auto=webp&amp;s=598b1337bcca5c8c70113003ed6679fb8b7fa78b https://preview.redd.it/ndji155dh9bc1.jpg?width=1853&amp;format=pjpg&amp; auto=webp&amp;s=53de4bdd072ed3c46229a34dcc7731425a8f1021 （注意：我们需要登录以避免滥用我们的 GPU 服务器阵列。它是免费使用的，只需注册一个帐户，我们将手动批准。）（平台演示视频可在此处观看：https://www.youtube.com/watch?v=UX_CIUcJ1uE）    由   提交 /u/coolwulf   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191sfl4/p_neuralrad_first_free_to_use_organ_and_tumor/</guid>
      <pubDate>Mon, 08 Jan 2024 18:42:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] RTX 4500 vs A5000基准，A5000更强？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191s6ii/r_rtx_4500_vs_a5000_benchmark_a5000_stronger/</link>
      <description><![CDATA[      查看基准测试结果，取决于网络/任务，但我觉得 A5000 是更强。 https://preview .redd.it/hyoe7vfif9bc1.png?width=1774&amp;format=png&amp;auto=webp&amp;s=f4ef7df9072991fd477d5afe703a4e627622e51f   由   提交 /u/oren_a   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191s6ii/r_rtx_4500_vs_a5000_benchmark_a5000_stronger/</guid>
      <pubDate>Mon, 08 Jan 2024 18:32:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我构建了 marimo——一个开源的反应式 Python 笔记本，它存储为 .py 文件，可以作为脚本执行，并且可以作为应用程序部署。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191rdwq/p_i_built_marimo_an_opensource_reactive_python/</link>
      <description><![CDATA[嗨！我想分享 marimo，一个用于 Python 的开源反应式笔记本。它旨在解决 Jupyter 笔记本的许多众所周知的问题，同时为您提供新功能：marimo 笔记本可重复（无隐藏状态）、git 友好（存储为 Python 文件）、可作为 Python 脚本执行以及可部署为 Web 应用程序。 GitHub 存储库： https://github.com/marimo-team/marimo 在 marimo 中，您的笔记本代码、输出和程序状态保证是一致的。运行单元格并通过自动运行引用其变量的单元格来做出反应。删除一个单元格，marimo 就会从程序内存中清除其变量，从而消除隐藏状态。如果您担心意外触发昂贵的计算，您可以禁用特定单元格的自动运行。 marimo 还附带 UI 元素，例如滑块、数据帧转换器以及自动与 Python 同步的交互式绘图。与元素交互，使用该元素的单元格会自动以其最新值重新运行。反应性使这些 UI 元素比 Jupyter 小部件更有用，更不用说更易于使用。 我选择开发 marimo，因为我相信 ML 社区应该有一个更好的编程环境来进行研究和交流。我看到很多研究都是从 Jupyter 笔记本开始的（我自己的大部分也是这样）。由于 Jupyter 笔记本固有的缺陷，我还看到许多相同的研究无法重现或因隐藏的错误而减慢速度。 我坚信，我们的工作质量取决于我们的工作质量我们使用的工具塑造了我们的思维方式——更好的工具，更好的思维。 2017 年至 2018 年，我在 Google Brain 担任软件工程师，当时 TensorFlow 正在过渡到 TensorFlow 2，而 JAX 还处于早期阶段。我亲眼目睹了 PyTorch 和 JAX 为我们的社区带来的生产力的提高，后来当我在斯坦福大学与 Stephen Boyd 一起攻读博士学位时，我也亲眼目睹了我自己的研究。我们对 marimo 的目标是通过新的编程环境做一些类似的事情。 marimo 的开发经过了科学家和工程师的密切投入，并受到了包括 Pluto.jl 和 Streamlit 在内的许多工具的启发。只有我们两个人在研究它——我们最近将其开源，因为我们认为它已经准备好供更广泛的使用。请尝试一下（pip install marimo &amp;&amp; marimo 教程简介）。我们非常希望您能提供任何反馈！   由   提交 /u/akshayka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191rdwq/p_i_built_marimo_an_opensource_reactive_python/</guid>
      <pubDate>Mon, 08 Jan 2024 18:00:41 GMT</pubDate>
    </item>
    <item>
      <title>[P]Retri-evals：检索评估管道</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191qvst/pretrievals_retrieval_evaluation_pipelines/</link>
      <description><![CDATA[大家好， 我们一直致力于为法学硕士构建检索管道，与许多其他人一样，我们质疑如何改变我们的管道（例如分块、清理）会影响整体结果。  我们还面临着根据哪些数据进行评估的问题。学术上使用MTEB，但使用我们自己的数据会更可靠。 Retri-evals 希望能够解决这些问题。我们提取了 MTEB 抽象，让我们可以针对开源数据集进行评估，并且我们将开源用于从生产数据自动生成评估数据集的代码。 我很想听听您的意见想法！我们希望通过工具来补充该领域的现有解决方案，使其更容易投入生产。 https ://github.com/DeployQL/retri-evals   由   提交 /u/mtbarta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191qvst/pretrievals_retrieval_evaluation_pipelines/</guid>
      <pubDate>Mon, 08 Jan 2024 17:41:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 采访里奇·萨顿</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191oujg/d_interview_with_rich_sutton/</link>
      <description><![CDATA[一个多月前，我向这位订阅者询问了一些问题，以询问 Rich Sutton (此处），截至今天，完整采访内容可在 https:/ /youtu.be/4feeUJnrrYg！ Rich 有一些独特的想法 - 或者正如他喜欢说的 - 它是什么过时了，但我很想听听其他人之后的想法提出其中一些想法。 大纲： 0:00 - 简介 1:33 - 采访开始 2:04 - OpenMind 研究院4:32 - 人工智能的历史7:13 - 扩展容易吗？10:49 - 反向传播和反向传播的问题陈述21:22 - 狭隘视野的咆哮23:43 - 令人兴奋的新事物 32:00 - 记忆 35:34 - 提出想法 43 :47 - STOMP45:30 - Keen Technologies50:39 - 人类的下一阶段和未来情绪1:06:25 - 外星人工智能1:08:00 - 不同的研究方法1:21:30 - 里奇的建议1:26:00 - RL 牛肉1:27:07 - 将所有内容整合在一起    由   提交/u/ejmejm1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191oujg/d_interview_with_rich_sutton/</guid>
      <pubDate>Mon, 08 Jan 2024 16:17:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人脑 FLOPs 估计，是否比我们想象的要低？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191ol1n/d_human_brain_flops_estimate_is_it_lower_than_we/</link>
      <description><![CDATA[这篇文章旨在提供对人脑的深入了解，以便更容易将其与人工神经网络进行比较。 对我即将要说的大部分内容持保留态度，我很容易就会被一个数量级所影响，或者错过一些东西。  Ray Kurzweils 估计。 1011 个神经元。每个神经元有 1000 个突触连接。每秒 100 个峰值。  每秒计算 1011 × 1000 × 100=1016 次。 引用奇点临近：“考虑到人脑逆向工程的早期阶段，我将使用更保守的数字 1016 CPS”。  我自己的计算。自 2005 年以来情况似乎发生了变化，现在维基百科说每个神经元有 7000 个突触 https://en.m.wikipedia.org/wiki /Neuron  神经元放电速度平均为 0.1 到 2 赫兹。 https://aiimpacts.org/rate-of-neuron-firing/ #:~:text=Assorted%20estimates- 我将使用 1/s 作为尖峰频率。大脑也更明确，有 86,000,000,000 个神经元。 8,6×1010 × 7000 × 1 = 6×1014。 6×10 14 FLOP（每个突触一次 FLOP）。  峰值能量需求。神经元的每次激活都需要一定量的能量，该能量似乎为 2.468 × 10−7 J https://link.springer.com/article/10.1007/s11571-018-9503-3  所以从这里开始，其他一切都可以被弄清楚。尖峰能量 = 2.468 × 10−7 J 24 小时内大脑能量消耗 = 1,673,600 焦耳 24 小时内的秒数 = 86400。每个神经元有 7000 个突触。 1,673,600÷(2.468 × 10 −7) J = 6,782×1012。 6,782×1012 ÷ 86400 = 78,486,103。 (78,每秒 500 万次峰值）。 78,486,103 × 7000 = 5.49×1010 FLOP 或 549 gigaFLOPs 如果 3 正确，则意味着高端手机的 GPU 计算量比人脑的计算量还要多（三星 s23，fp32 时为 3,681 TFLOP。大脑一天平均为 0,549 TFLOP）。 这不是比较事物的好方法，因为大脑是一台大规模并行计算机，内存基本上存在于结构中。  那么需要多少“内存”呢？我们谈论的是大脑吗？我们有： 86,000,000,000 个神经元。每个神经元有 7000 个突触。每个突触 5 位。 https://www.cnsnevada.com/what-is-the-memory-capacity-of-a- human-brain/#:~:text=Neurons%20are%20the%20cells%20which 86,000,000,000 × 7000 × 5 = 3×1015 位或 3.76×1014 字节。祝你好运，在手机上安装 376 TB RAM。 但是每秒 78,500,000 个峰值真的足以让大脑处理所有事情吗？让我们看看眼睛。 每只眼睛的总分辨率为 8 兆像素。 https://m.youtube.com/watch?v=4I5Q3UXkGd0&amp;pp=ygUednNhdWNlIHJlc29sdXRpb 24gb2YgaHVtYW4gZXll&lt; /p&gt; 通过视神经发送的信息大约只有 10,000,000 位/秒 https://www.eurekalert。 org/news-releases/468943 （只有最相关的信息通过视神经发送，因为大脑希望不惜一切代价节省电量）。因此，我们的双眼每秒有 20,000,000 个尖峰，这是 7850 万个尖峰的 25.5%。 7850 万个尖峰并不是硬性的性能上限，它只是一天中的平均值，而大脑是根据需要主动调节脑电波频率。 您认为哪种情况更有可能？ 1. 2. 或 3.   由   提交 /u/SpaceXRaptor42   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191ol1n/d_human_brain_flops_estimate_is_it_lower_than_we/</guid>
      <pubDate>Mon, 08 Jan 2024 16:05:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何猜测梯度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191lu3v/r_how_to_guess_a_gradient/</link>
      <description><![CDATA[      奇怪的是，你在不知道目标函数的情况下就知道梯度在哪里。 论文：https://arxiv.org/abs/2312.04709 摘要  关于梯度你能说多少无需计算损失或不知道标签的神经网络？这听起来可能是一个奇怪的问题：答案肯定是“很少”。然而，在本文中，我们表明梯度比之前想象的更加结构化。梯度位于可预测的低维子空间中，该子空间取决于网络架构和传入特征。利用这种结构可以显着改进基于方向导数的无梯度优化方案，该方案一直难以扩展到在玩具数据集上训练的小型网络之外。我们研究如何缩小计算精确梯度的方法和使用方向导数的方法之间优化性能的差距。此外，我们强调了克服精确梯度优化和猜测梯度之间巨大差距的新挑战。  https://preview.redd.it/l7tm982c28bc1.png?width=1962&amp;format=png&amp;auto=webp&amp;s=94d237353bc53ee b21489f6adeeaa8e43043f44a ​   由   提交/u/That_Violinist_18   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191lu3v/r_how_to_guess_a_gradient/</guid>
      <pubDate>Mon, 08 Jan 2024 14:00:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 是否存在仅适用于比较结果的贝叶斯优化的等效项？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191juun/p_is_there_an_equivalent_of_bayesian_optimization/</link>
      <description><![CDATA[大家好，我正在解决一个问题，我需要找到最佳参数集（其中 10 个）来优化一个非常昂贵的目标功能。通常，我会使用贝叶斯优化，但在这种特定情况下，我无法访问实际的目标函数，我唯一可以计算的是该函数在特定参数集 A 或 B 下是否更高。我不知道函数的实际值，也不知道它的导数。我所能做的就是比较两组参数，并判断哪一组产生的函数值较低。 关于我可以使用什么来找到最佳参数来优化该函数，有什么建议吗？&lt; /p&gt;   由   提交/u/ale152  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191juun/p_is_there_an_equivalent_of_bayesian_optimization/</guid>
      <pubDate>Mon, 08 Jan 2024 12:12:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 3090 与新 40 系列同等产品</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191isia/d_3090_vs_the_new_40_series_equivalent/</link>
      <description><![CDATA[我发现了一些 3090（新）的优惠：  MSI（1260 美元） PALIT（965 美元） PALIT OC（900 美元）  我想知道 40 系列的较低型号（主要是 4070 和 4070 TI，因为 4080 远远超出了我的预算（需要升级电源）对于游戏/AI 与缺乏 V-RAM 来说是值得的  请注意卡的可用性和选择就我而言是有限，另外，我的电源必须更换，因为它只是650W金牌（也开放电源升级建议）。 谢谢   由   提交 /u/myselfitself   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191isia/d_3090_vs_the_new_40_series_equivalent/</guid>
      <pubDate>Mon, 08 Jan 2024 11:06:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] Infinite-LLM：使用 DistAttention 和分布式 KVCache 实现长上下文的高效 LLM 服务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/191iqxj/r_infinitellm_efficient_llm_service_for_long/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.02669 摘要：  大型语言模型（LLM）的快速增长已经成为基于云的法学硕士服务的增长的驱动力，这些服务现在是推进人工智能应用程序不可或缺的一部分。然而，LLM服务的动态自回归性质，以及支持超长上下文长度的需要，要求灵活分配和释放大量资源。这给设计基于云的LLM服务系统带来了相当大的挑战，低效的管理可能导致性能下降或资源浪费。为了应对这些挑战，本文引入了一种新颖的分布式注意力算法DistAttention，它将KV Cache分割成更小的、可管理的单元，从而实现注意力模块的分布式处理和存储。基于此，我们提出了DistKV-LLM，这是一种分布式LLM服务系统，可以动态管理KV缓存并有效地编排跨数据中心的所有可访问的GPU和CPU内存。这确保了云上的高性能法学硕士服务，可适应广泛的上下文长度。在具有 32 个 NVIDIA A100 GPU（配置为 2 到 32 个实例）的云环境中进行验证，我们的系统表现出 1.03-2.4 倍的端到端吞吐量改进，并且支持的上下文长度比当前状态长 2-19 倍-art LLM 服务系统，通过对上下文长度高达 1,900K 的 18 个数据集的广泛测试证明。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/191iqxj/r_infinitellm_efficient_llm_service_for_long/</guid>
      <pubDate>Mon, 08 Jan 2024 11:03:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>