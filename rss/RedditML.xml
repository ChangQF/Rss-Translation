<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 07 Jan 2024 03:14:14 GMT</lastBuildDate>
    <item>
      <title>计算机化视觉 - 帮助在浏览器中实现 XML 数据集？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190i0a6/computerized_vision_help_implementing_xml_dataset/</link>
      <description><![CDATA[我有一个带注释的 XML 数据集，我计划在浏览器中使用它，我希望能够在 Web 元素通过以下方式识别特定对象时单击它：手动编程或使用特定框架。 Pyautogui 等框架以其基于 X、Y 坐标和屏幕截图等进行点击的能力而闻名，但我希望通过训练自己的数据集来获得更高的稳定性。我无法弄清楚如何在这个方程中使用我的数据集，希望听到所有的想法和选项。   由   提交/u/Punkrocker410   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190i0a6/computerized_vision_help_implementing_xml_dataset/</guid>
      <pubDate>Sun, 07 Jan 2024 03:05:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] Mamba 和 S4 解释：架构、并行扫描、内核融合、循环/卷积公式、第一原理的数学推导、HiPPO 理论直观解释、数学直观解释</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190gs4y/p_mamba_and_s4_explained_architecture_parallel/</link>
      <description><![CDATA[   /u/hkproj_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190gs4y/p_mamba_and_s4_explained_architecture_parallel/</guid>
      <pubDate>Sun, 07 Jan 2024 02:03:24 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] 去噪自动编码器过时了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190ggrg/rp_are_denoising_autoencoders_out_of_style/</link>
      <description><![CDATA[分数匹配模型，特别是其去噪分数匹配实现现在非常热门。然而，几乎所有这些都是某种形式的大型随机降噪器。我想知道为什么去噪自动编码器没有对它们进行太多的研究，考虑到两者在理论上和功能上都很相似（[1]中导出的去噪分数匹配论文明确地建立了两者之间的联系）。 &lt;此外，自动编码器比 U-Net 对应物灵活得多，因为它们可用于低维潜变量建模（例如 VAE）。我知道有几篇论文将去噪自动编码器与变分自动编码器 [2] 和对抗性自动编码器 [3] 相结合，在我看来，这是一个不错的开始。  在我自己的研究中，我发现它们本身在概率建模方面具有巨大的潜力。 ​ 参考文献 [1] 帕斯卡·文森特。分数匹配和去噪自动编码器之间的联系。神经计算，2011。 [2] Antonia Creswell、Kai Arulkumaran、Anil Anthony Bharath。使用马尔可夫链改进生成自动编码器的采样。 arXiv，2016。 [3] Antonia Creswell，Anil Anthony Bharath。去噪对抗性自动编码器。 arXiv，2017。   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190ggrg/rp_are_denoising_autoencoders_out_of_style/</guid>
      <pubDate>Sun, 07 Jan 2024 01:47:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大规模标记文本数据的最佳注释平台</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190g5td/d_best_annotation_platforms_for_labeling_text/</link>
      <description><![CDATA[我们正在标记文本数据以评估/微调一些大型语言模型 (LLM)。我们拥有一支内部注释者和合作者团队。我们正在寻找一个注释平台，理想情况下不限制可以参与的注释者的数量，具有良好的角色管理和监控标签和注释者的方式，能够灵活地定义标签模式，并且具有用户友好的用户界面。&lt; /p&gt; 我们已经尝试过 Scale AI&#39;s Studio 和 Label Studio，但它们都有各自的局限性，并且根据它们的商业模式和计算标记单元的方式，注释的大规模成本可能会很高（Label Studio 确实有免费的社区版本，但也有没有角色管理，因此大规模注释者可能会使标签管理变得困难。）我们还考虑了 Amazon Mechanical Turk，但一个挑战可能是注释者首先需要有一个工作 ID 才能加入注释任务（如果我是这样，请纠正我）错误）。 有人对大规模文本数据的可靠注释平台有任何建议吗？我们是否缺少任何明显的平台或工具？该工具不一定是免费的，但在缩放注释时，可靠性和支持非常重要，并且成本合理。   由   提交/u/pedhoss  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190g5td/d_best_annotation_platforms_for_labeling_text/</guid>
      <pubDate>Sun, 07 Jan 2024 01:32:46 GMT</pubDate>
    </item>
    <item>
      <title>主动学习[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190fyt6/active_learning_d/</link>
      <description><![CDATA[有人知道一些可以开始主动学习的好文献/资源吗？我有统计学背景，由于实验设计/实验设计而对这个领域感兴趣。优化设计和主动学习领域之间有很多联系，因此想知道这个领域的人推荐阅读哪些内容。   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/190fyt6/active_learning_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190fyt6/active_learning_d/</guid>
      <pubDate>Sun, 07 Jan 2024 01:23:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] HPC 推理优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190fwpn/d_inference_optimization_with_hpc/</link>
      <description><![CDATA[大家好， 我有一项关于模型 Llama 推理优化的任务。我必须创建一个框架来优化推理。  我已经在 Huggingface 论坛中询问过，您知道还有其他论坛可以解决有关机器学习/人工智能模型的问题吗？ 此外，如果您对此主题有了解，请 lmk，我也会在这里发布问题。 我只是不确定它是否是正确的论坛 感谢您的帮助：p &lt; !-- SC_ON --&gt;  由   提交/u/Few-Letter312  /u/Few-Letter312 reddit.com/r/MachineLearning/comments/190fwpn/d_inference_optimization_with_hpc/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190fwpn/d_inference_optimization_with_hpc/</guid>
      <pubDate>Sun, 07 Jan 2024 01:20:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关系抽取</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190dzwu/d_relation_extraction/</link>
      <description><![CDATA[我正在尝试使用 Hugging Face 中的 REBEL 模型进行关系提取。它通过三元组线性化输出关系三元组。它是在 REBEL 数据集上进行训练的，该数据集本质上是维基百科数据。我有自由格式的文本，我想从中生成关系三元组。那么，如何从该文本创建一个数据集，以便与 REBEL 数据集紧密结合？我想在自由格式文本上微调模型。  REBEL 模型：https://huggingface.co/Babelscape/rebel-large REBEL 数据集：https://huggingface.co/datasets/Babelscape/rebel-dataset  如果您认为还有任何其他值得尝试进行关系提取的 ML 模型，我们将非常感谢您提供的信息。 :) 谢谢！   由   提交 /u/RajHalifax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190dzwu/d_relation_extraction/</guid>
      <pubDate>Sat, 06 Jan 2024 23:52:09 GMT</pubDate>
    </item>
    <item>
      <title>[D]我们的大脑如何防止过度拟合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</link>
      <description><![CDATA[老实说，这个问题引发了一系列其他问题，老实说，这很有趣，我们阻止这种情况发生的机制是什么？ 梦想只是生成数据增强，以便我们防止过度拟合吗？ 如果我们进一步将过度拟合拟人化，患有学者综合症的人会过度拟合吗？ （因为他们在狭窄的任务上表现出色，但在泛化方面有其他障碍。尽管他们仍然有梦想） 为什么我们不记忆，而是学习？    由   提交 /u/BlupHox   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</guid>
      <pubDate>Sat, 06 Jan 2024 22:33:40 GMT</pubDate>
    </item>
    <item>
      <title>[D]试图理解专有硬件制造商将重组行业并导致OpenAI企业价值下降的论点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1905izt/d_trying_to_understand_the_argument_that/</link>
      <description><![CDATA[一些硅谷声音的观点之一是，有两个主要因素会导致专有/闭源模型构建者泄漏价值：(1) 延迟当前的所有工具都使得构建生产质量的代码变得不可行——API 应该需要 30-50 毫秒而不是 30-50 秒。 (2) 如果您尝试构建某些东西，这些平台上 100 万个代币的成本在经济上是不可能的。  争论的焦点是，云服务将会为用户带来毫秒级的延迟，并且 100 万个代币的价格约为 10-20 美分，并且他们需要构建自己的定制硬件来做到这一点。  讨论这个问题的人不是机器学习工程师/研究人员。发生这样的事情的可行性是什么？除了实际制造能够将成本降低几个数量级的硬件之外，这种观点还面临哪些挑战？   由   提交 /u/SloppyDrunkCarrot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1905izt/d_trying_to_understand_the_argument_that/</guid>
      <pubDate>Sat, 06 Jan 2024 17:49:04 GMT</pubDate>
    </item>
    <item>
      <title>单变量异常检测[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1904tl7/univariate_anomaly_detection_d/</link>
      <description><![CDATA[嗨！ 我面临着一个看似“简单”的问题，但我一直在努力解决它现在处于异常/异常值检测领域。 我有一个包含大约 60K 数据点的数据集。每个数据点都是一个组的一部分（~1500 个组；最小组大小为 15），并且具有长度参数。 任务是在组内执行异常检测，即，如果数据点具有与组中其他数据点相比，长度不规则，将其标记为异常。 我对数据使用 log2 转换，转换后，大多数组 (75%)基于 Shapiro-Wilks 检验呈正态分布。 作为第一个解决方案，我尝试了 std 与均值的经典距离，其中如果长度大于mean+3*std，则这是一个异常。 此解决方案有两个问题： 在具有大量数据点的组中，其中绝大多数数据点具有相同或非常相似的长度， std 非常小，因此使阈值非常小，并导致对数据点发出警报，我不认为这是异常。 这种方法导致了相对较高的检测（约 250 个异常），我的目标是仅对所有组中数据中的一小部分最极端的异常发出警报。 当我尝试提高阈值（例如提高到 4std）时，我遇到了另一个问题，我错过了这个问题组中的异常，其中一个数据点与其他数据点相比具有非常大的长度，这导致高标准差，从而使极端数据点与平均距离相比具有“低”标准差。 我很感激有关该主题的任何帮助或想法。谢谢！   由   提交/u/thk_ML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1904tl7/univariate_anomaly_detection_d/</guid>
      <pubDate>Sat, 06 Jan 2024 17:19:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用开源模型进行长代理树搜索取得令人难以置信的结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</link>
      <description><![CDATA[你好， 我看到 GPT-4 的长代理树搜索以 94.4% 的 pass@1 领先于 HumanEval现在几周了。 https://paperswithcode.com/sota/code- Generation-on- humaneval &lt; p&gt;​ 原始论文的作者在他们的官方 github 存储库。我必须更改一些代码才能使用 CodeLlama-7b 进行尝试，并使用 pass@1 进行人类评估，仅 2 次最大迭代即可将 HumanEval 得分从 37% 提高到大约 70%。 这是一些令人难以置信的结果在我看来，因为这个分数比只有 7b 模型的 GPT-3.5 更高。我认为必须进行更多测试，但令我惊讶的是人们没有更多地谈论这一点。   由   提交/u/ArtZab  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</guid>
      <pubDate>Sat, 06 Jan 2024 16:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变形金刚的思想链表现力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/</link>
      <description><![CDATA[论文。我不隶属于作者。 摘要：  最近的理论工作已经发现了令人惊讶的简单推理问题，例如检查图中的两个节点是否连接或模拟有限状态机，标准变压器在读取输入后立即回答，这证明是无法解决的。然而，在实践中，变形金刚的推理可以通过允许他们使用“思维链”来改进。或“暂存器”，即，在回答之前生成中间标记序列并对其进行调节。受此启发，我们问：这种中间生成是否从根本上扩展了仅解码器变压器的计算能力？我们证明答案是肯定的，但增加的数量很大程度上取决于中间代的数量。例如，我们发现具有对数数量的解码步骤（相对于输入长度）的 Transformer 解码器仅略微突破了标准 Transformer 的限制，而线性数量的解码步骤则增加了明显的新能力（在标准复杂性猜想下）：所有常规语言。我们的结果还表明，线性步骤使变压器解码器保持在上下文相关语言内，而多项式步骤使它们能够准确识别多项式时间可解决问题的类别——这是根据标准复杂性类别对变压器类型进行的第一个准确表征。总之，我们的结果提供了一个细致入微的框架，用于理解变压器的思想链或暂存器的长度如何影响其推理能力。    由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/</guid>
      <pubDate>Sat, 06 Jan 2024 07:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习有什么有趣的数学理论吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zo7or/d_is_there_any_interesting_mathematical_theory_of/</link>
      <description><![CDATA[大家好！我的问题在标题中，这是一些背景信息。我的背景可以描述为“主修理论计算机科学（非常强调‘理论’这个词，想想计算复杂性理论），辅修数学”。 几年前，我参加了入门课程机器学习课程......非常沮丧和失望。  没有解释任何东西应该如何或为什么工作，相反有很多不令人信服的猜测，比如“如果你添加一个卷积层，然后它将学习简单的几何形状，因此后面的层将有更多的结构可以使用”或者“我们可以在 RNN 中使用额外的输入，并以某种方式组合三个输入，这样新的输入将起到‘长期记忆’的作用”。我没想到数学逻辑或编程语言理论的严谨程度，但其他科学，例如经济学，至少可以用一些简化的模型来解释他们正在研究的现象。我们在机器学习中没有类似的东西吗？ 在课程中，我们直接跳到一些相当复杂的问题，例如区分猫的图片和狗的图片。我怀疑是否有人能够对这两类图片给出一个很好的定义。虽然这让神经网络能够解决问题变得更加令人印象深刻，但我看不出我们可以从中学到什么关于神经网络如何做到这一点的信息。训练神经网络区分蓝色和绿色、正方形和圆形等，然后尝试使用结果来分析神经网络如何学习不是更好吗？  后来我开了一个很少有机器学习教科书。  我真的很喜欢有关 PAC 学习的部分，总体来说统计学习也很适合我。 有关神经网络和尤其是深度学习，几乎与我在课程中听到的关于仪式舞蹈如何导致降雨的炼金术级别的推测相同。  所以我试图在 arXiv 上找到一些现代结果。   大多数关于机器学习的论文都将更难以推理的模型应用于更难以理解的问题，这让我感到非常失望。 有一些关于神经网络是通用逼近器的结果，即使不多也很好。 我还遇到过（在写硕士论文时）一两篇关于感知器和电路计算复杂性的论文阈值函数。令人遗憾的是，似乎几乎没有关于这个主题的当代研究。  ​ 我的“咆哮”到此结束。 ，我希望这能够澄清“机器学习数学”的类型。我正在寻找。请注意，我确实理解使用现代模型需要大量的知识和专业知识，我并不是想贬低你所做的工作，我只是对找到这些模型如何工作的理论解释有多么困难感到沮丧，考虑到机器学习的普及。 我真的很感激任何想法或建议！ 此外，英语不是我的母语，所以我很抱歉任何拼写错误、不正确的语法或尴尬的句子。 ​ UPD：看到这篇文章刚刚。我真的很想看到更多类似的作品。   由   提交/u/a_broken_coffee_cup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zo7or/d_is_there_any_interesting_mathematical_theory_of/</guid>
      <pubDate>Sat, 06 Jan 2024 01:50:11 GMT</pubDate>
    </item>
    <item>
      <title>基于变压器的法学硕士不是一般学习者：通用电路视角 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</link>
      <description><![CDATA[https://openreview.net/forum?id =tGM7rOmJzV  （法学硕士）的巨大成功引发了人工智能界研究重点的显着转变。这些令人印象深刻的实证成就激发了人们对法学硕士是“通用人工智能（AGI）的火花”的期望。然而，一些评估结果也呈现了法学硕士失败的令人困惑的例子，包括一些看似微不足道的任务。例如，GPT-4 能够解决一些 IMO 中对研究生来说可能具有挑战性的数学问题，而在某些情况下它可能会在小学水平的算术问题上出错。 ...  我们的理论结果表明 T-LLM 无法成为通用学习者。然而，T-LLM 在各种任务中取得了巨大的经验成功。我们对这种不一致现象提供了一个可能的解释：虽然 T-LLM 不是一般学习者，但他们可以通过记忆大量实例来部分解决复杂的任务，从而导致人们产生一种错觉，认为 T-LLM 具有真正解决这些任务问题的能力。     由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</guid>
      <pubDate>Fri, 05 Jan 2024 21:39:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>