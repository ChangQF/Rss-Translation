<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 05 Dec 2023 15:15:01 GMT</lastBuildDate>
    <item>
      <title>[R] 加州大学伯克利分校的论文“Sequential Modeling Enables Scalable Learning for Large Vision Models”有一条奇怪的缩放曲线。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bdcu7/r_sequential_modeling_enables_scalable_learning/</link>
      <description><![CDATA[   看到这篇论文“顺序建模实现大视觉模型的可扩展学习” （https://arxiv.org/abs/2312.00785）其中的数字看起来有点奇怪。对于不同的模型尺寸，线条看起来相同。  不同的运行或不同尺寸的大型模型通常是相同的吗？ https:/ /twitter.com/JitendraMalikCV/status/1731553367217070413  ​ 取自 https://arxiv.org/abs/2312.00785 中的图 3 这是完整的图3图 来自https:// arxiv.org/abs/2312.00785   由   提交/u/rantana  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bdcu7/r_sequential_modeling_enables_scalable_learning/</guid>
      <pubDate>Tue, 05 Dec 2023 14:37:08 GMT</pubDate>
    </item>
    <item>
      <title>Apple ML 给出奇怪的结果......需要帮助 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bdbx8/apple_ml_giving_weird_results_need_help_p/</link>
      <description><![CDATA[大家好，我想简单介绍一下我要创建的内容： 我的目标是创建一个模型使用 WLASL 数据集将手语转换为文本。现在，从一开始就从kaggle下载这个模型，虽然数据集看起来相当全面，但每个类别的视频数量从5到13个不等，这显然需要训练的内容相当少。我决定尝试 Apple Create ML，而不是像 Tensorflow 或更复杂的深度学习框架，因为这样会简单得多。由于数据集在每个类别的视频方面非常有限，因此我在“手部动作分类器”中使用了所有 6 个数据增强。 （水平翻转、旋转、平移、缩放、插帧、丢帧）。虽然我知道这无法保存模型，但它肯定会大大提高准确性。请注意，我没有使用数据集中的所有 2000 个类（单词），而是仅使用了 300 个类的子集。我获得了 16% 的验证准确率，以及 90% 的所有增强训练准确率，因此我的模型显然过度拟合。所以我对 25 个类进行了同样的尝试，这次我获得了 42% 的验证准确率，以及 100% 的训练准确率。再次，过度拟合。我转到实时预览，几乎我尝试的每个迹象都被预测为错误。 现在，我决定使用“模型源”来进行预测。在侧边栏中。我不太确定它们的用途，但这是我尝试过的： 我将数据子集分成 2 个单独的模型源（16 个类，但数量仍然很高），并得到 83分别为 % 验证准确度和 90% 验证准确度。这两个模型源都使用所有数据增强。我的模型显然过度拟合，在两个来源中都有 100% 的训练准确度，但将其分成两个模型显然提高了我的准确度，当我在“实时预览”中测试这一点时，我自己做的每个 ASL 标志都能够以超过 90% 的置信度准确猜测每个单词。 所以我的问题是，即使我的数据有限（虽然增强确实增加了很多，但显然性能差异不应该这么大），我的模型为何表现得这么好？此外，将一个模型拆分为单独的模型源是否可行？我不确定“模型来源”有什么用？甚至是，所以我尝试了这个，不知怎的，我得到了更好的结果。如果可行，我如何将它们实现到一个快速应用程序中。我现在有点困惑，所以希望有人能告诉我发生了什么事。如果这不是一个可行的解决方案，有人可以提供另一个解决方案来说明我如何使用这个数据集吗？事先了解它会非常有帮助，但即使你不知道，你能帮助我吗？ 非常感谢大家:) PS：这里是链接它 -： Kaggle 链接：https://www.kaggle.com/datasets/risangbaskoro /wlasl-processed 原始论文github页面：https://github.com/dxli94/WLASL&lt; /a&gt; 抱歉这么长的消息。如果您需要任何图像来获得更多见解或更好的知识以提供更好的帮助，我很乐意提供它们。 再次非常感谢您 &lt;!-- SC_ON - -&gt;  由   提交/u/_lion_08   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bdbx8/apple_ml_giving_weird_results_need_help_p/</guid>
      <pubDate>Tue, 05 Dec 2023 14:35:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 顺序建模支持大型视觉模型的可扩展学习。 Transformer 接受“视觉句子”（1.64B 图像、420B 图像标记）的训练。同一模型可以执行修复、旋转、光照、语义分割、边缘检测、姿势估计等</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bd348/r_sequential_modeling_enables_scalable_learning/</link>
      <description><![CDATA[博客 - https://yutongbai.com/lvm.html  论文 - https://arxiv.org/abs/2312.00785   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bd348/r_sequential_modeling_enables_scalable_learning/</guid>
      <pubDate>Tue, 05 Dec 2023 14:24:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用交叉注意力的多模态编码器-解码器模型的最新进展是什么？我见过的大多数研究都只使用了 self-attention</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bd0lw/d_what_is_the_latest_with_multimodal/</link>
      <description><![CDATA[我见过的大多数多模态模型都使用编码器来嵌入图像/视频，然后使用自注意力将它们发送到仅 llm 的解码器。  有没有使用交叉注意力代替编码器-解码器模型的研究？   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bd0lw/d_what_is_the_latest_with_multimodal/</guid>
      <pubDate>Tue, 05 Dec 2023 14:21:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Paved2Paradise：通过考虑现实世界的经济高效且可扩展的 LiDAR 模拟</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bbklt/r_paved2paradise_costeffective_and_scalable_lidar/</link>
      <description><![CDATA[   /u/michaelaalcorn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bbklt/r_paved2paradise_costeffective_and_scalable_lidar/</guid>
      <pubDate>Tue, 05 Dec 2023 13:06:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] *ACL 会议的声望差异</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bb9zg/d_difference_in_prestige_for_acl_conferences/</link>
      <description><![CDATA[这里是紧张的本科生（大三）。我的第一作者 10 月 ACL 滚动评审提交得到了好评。今年所有 *ACL 会议都将通过 ARR 进行。我最初的计划是提交给 EACL，但我注意到我的审稿分数高于 ACL 2023 接受论文的中位数，所以我想知道是否值得等待 NAACL，或者 ACL 尝试获得接受 从本质上讲，对于申请博士学位的学生来说，各种 *ACL 会议之间的声望是否存在显着差异。我意识到 ACL 显然是旗舰会议，但阅读周围似乎 NAACL 和 ACL 之间的引用/声望之间的差距非常小，但 NAACL 和 EACL 稍大一些 - 这些差异重要吗，特别是在博士申请的情况下？当大多数人说顶级计算机科学项目的顶级会议上有 3 篇以上第一作者论文时，EACL 和 NAACL 是否属于此类？ 我认为我现在的主要决定是在 EACL 和 NAACL 之间，因为 ACL 似乎太晚了，并且然后我想完全专注于其他论文（即使它被拒绝，我也没有太多时间在 12 月博士截止日期之前及时接受它）。我目前正在美国学习也计划在美国攻读博士学位 - 不确定这是否会以任何方式影响 NAACL 与 EACL 的人际网络 - 我认为每个会议中都有来自各自地区的更多教授？ 非常感谢任何建议，并且希望这也能帮助其他学生，因为我确信我不是第一个遇到这种困境的人。   由   提交/u/AltAccountV24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bb9zg/d_difference_in_prestige_for_acl_conferences/</guid>
      <pubDate>Tue, 05 Dec 2023 12:50:14 GMT</pubDate>
    </item>
    <item>
      <title>[D]寻找课程推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18baqlb/d_looking_for_course_recommendations/</link>
      <description><![CDATA[我正在尝试拼凑人工智能和离散数学的双硕士学位，并且想知道是否有人有一些课程推荐。以下是我目前计划学习的课程列表。 数学课程： - 现代密码学 (8) - 信息论 (6) ) - 机器学习理论（8） - 图多项式与算法（6） - 因果关系（8） -图对称性和组合设计 (8) - 密码学精选领域 (8) AI 课程： - 机器学习 1 (6) - 深度学习 1 (6) - 计算机视觉 1 (6) - 人工智能的公平性、问责性、保密性和透明度 (6) &lt; p&gt;- 自然语言处理 1 (6) - 知识表示与推理 (6) - 信息检索 1 (6) - 深度学习 2 (6) - 博弈论(6) - 计算社会选择(6) - 算法博弈论(6)   由   提交 /u/Cocorow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18baqlb/d_looking_for_course_recommendations/</guid>
      <pubDate>Tue, 05 Dec 2023 12:17:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] StableSSM：通过稳定的重参数化缓解状态空间模型中的内存诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18b8nn4/r_stablessm_alleviating_the_curse_of_memory_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2311.14495 OpenReview：https:// /openreview.net/forum?id=BwG8hwohU4 摘要：  在本文中，我们研究了长期从参数化的角度来看状态空间模型（SSM）的记忆学习能力。我们证明，没有任何重新参数化的状态空间模型表现出与传统 RNN 类似的内存限制：可以通过状态空间模型稳定近似的目标关系必须具有指数衰减内存。我们的分析确定了这种“记忆诅咒”。由于循环权重收敛到稳定边界，这表明重新参数化技术可能是有效的。为此，我们引入了一类 SSM 重新参数化技术，可以有效解除其内存限制。除了提高逼近能力之外，我们进一步说明重新参数化方案的原则性选择还可以增强优化稳定性。我们使用合成数据集和语言模型验证了我们的发现。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18b8nn4/r_stablessm_alleviating_the_curse_of_memory_in/</guid>
      <pubDate>Tue, 05 Dec 2023 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 仍在进行研究的行业实验室</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18b4xju/d_industry_labs_that_still_do_research/</link>
      <description><![CDATA[我想知道是否还有研究实验室仍在进行探索性研究。我观察到的大多数地方都在朝着更少的探索性工作和更多的具体议程研究方向发展。我认为这阻碍了有趣/探索性的研究，而这通常会带来新的发现。我在攻读博士学位期间进行了此类研究，现在我被困在一个大型研发实验室中，该实验室的任务是研究特定的事情。您会推荐我看哪些实验室，并且仍然可以在某种程度上进行探索？   由   提交/u/oa97z  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18b4xju/d_industry_labs_that_still_do_research/</guid>
      <pubDate>Tue, 05 Dec 2023 05:35:54 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 教科书的文本到语音生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18b3g3g/discussion_text_to_voice_generation_for_textbooks/</link>
      <description><![CDATA[我正在听 lex 播客，了解我研究的一些内容，想问一下，是否有任何听起来足够自然的本地文本到语音模型？ 我非常想用它把一本书的文本部分变成音频，这样我就可以在阅读时收听它。我使用 edge 的 tts  进行语音，将一段文字放到剪贴板和 edge-tts 中以收听文本，但它造成两个问题：1.需要网络连接并且打开书2.只能逐段进行，很容易出错，或者有时用多了之后就无法转换全文。 &lt; p&gt;这个想法是将一本书的章节转换成音频文件并传输，以便我可以在手机上即时收听。 离线模型的状态如何？他们有能力输出良好的声音（或者甚至能够从他们的讲座中提供导师的声音并对其进行训练）？   由   提交/u/sweetchocolotepie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18b3g3g/discussion_text_to_voice_generation_for_textbooks/</guid>
      <pubDate>Tue, 05 Dec 2023 04:12:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于时间序列预测的 Transformer</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ax51t/d_transformers_for_time_series_forecasting/</link>
      <description><![CDATA[有一些新兴的 Transformer 模型专为预测时间序列值而设计，例如 Informer 和 Temporal Fusion Transformer。您对这个话题有何看法？你认为他们能忍受 RNN 吗？   由   提交 /u/MrGolran   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ax51t/d_transformers_for_time_series_forecasting/</guid>
      <pubDate>Mon, 04 Dec 2023 23:07:34 GMT</pubDate>
    </item>
    <item>
      <title>Mamba：具有选择性状态空间的线性时间序列建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aq0k5/mamba_lineartime_sequence_modeling_with_selective/</link>
      <description><![CDATA[       由   提交/u/Jean-Porte  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aq0k5/mamba_lineartime_sequence_modeling_with_selective/</guid>
      <pubDate>Mon, 04 Dec 2023 18:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种架构可以替代变压器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</link>
      <description><![CDATA[我最近读到了这个观点，讨论变压器可以被更换。  https://towardsdatascience.com/a-requiem-for-the-transformer -297e6f14e189 总的来说，过去几个月发表的文章显示了 Transformer 的局限性： https://arxiv.org/abs/2203.15556 https:/ /arxiv.org/abs/2304.15004  在计算机视觉中，具有相同预算的ConvNet似乎具有相似的性能： https://arxiv.org/abs/2310.19909  https://arxiv.org/abs/2310.16764  DeepMind 表明 Transformer 无法泛化到训练集分布之外： https://arxiv.org/abs/2311.00871 液体神经网络、鬣狗、尖峰神经网络等模型显示出活跃的搜索对于新架构： https://arxiv.org/pdf/2006.04439.pdf&lt; /p&gt; https://www.together.ai/blog/monarch-mixer  并不是说 Transformer 很快就会被取代，但我现在想知道下一个主导架构可能是什么？  所提出的架构似乎都不比变压器具有竞争优势   由   提交/u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</guid>
      <pubDate>Mon, 04 Dec 2023 17:44:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年机器学习研究热点是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18am0tx/d_whats_hot_for_machine_learning_research_in_2024/</link>
      <description><![CDATA[ML 中或与 ML 相关的哪些子领域/方法、应用领域预计将在 2024 年获得广泛关注（双关语无意）？&lt; /p&gt; PS：请不要回避提出您可能认为或知道的任何可能成为机器学习研究热门主题的内容，很可能您所知道的内容对于我们这里的许多人来说可能是未知的:)   由   提交 /u/ureepamuree   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18am0tx/d_whats_hot_for_machine_learning_research_in_2024/</guid>
      <pubDate>Mon, 04 Dec 2023 15:01:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>