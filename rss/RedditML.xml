<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 28 May 2024 15:13:57 GMT</lastBuildDate>
    <item>
      <title>[D] 2024 年微调还值得吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2l9lg/d_is_finetuning_still_worth_it_in_2024/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2l9lg/d_is_finetuning_still_worth_it_in_2024/</guid>
      <pubDate>Tue, 28 May 2024 14:47:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 帮助学校RAG项目选择LLM模型和嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2kojo/p_help_to_choose_llm_model_and_embeddings_for/</link>
      <description><![CDATA[您好， 在学校内，我们正在制定一个项目，根据法语文档创建 RAG。 我们必须选择一个具有很少参数（如 7B 和嵌入）的法学硕士，并通过文献研究证明其合理性。 我承认我不知道从哪里开始。它到处都有基准，来自各地的模型，我需要您对要查看哪些测量值的意见，以及如果您对嵌入和模型有想法，我可以查看。 谢谢。    由   提交 /u/Thamelia   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2kojo/p_help_to_choose_llm_model_and_embeddings_for/</guid>
      <pubDate>Tue, 28 May 2024 14:22:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入矩阵和最终的 pre-softmax 矩阵是否应该在 Transformer 中共享？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</link>
      <description><![CDATA[大家好， 在比较各种 LLM 时，我们可以看到，在采用 softmax 获得预测的 token 概率之前，其中一些 LLM 对 token 嵌入和转换矩阵使用相同的矩阵。我发现这篇 2016 年的论文使用输出嵌入改进语言模型表明这种方法更优越，注意力就是您所需要的论文也引用了它并进行了这种权重共享。GPT2 和 Gemma 等其他模型也是如此。 这让我想知道为什么 LLaMa 模型不进行这种权重共享。就模型容量而言，在那里使用单独的矩阵是否值得？像 Gemma 这样的模型是否必须使用权重共享，因为它们使用了庞大的词汇量？我对这里的权衡很感兴趣，如果有的话，目前对这个主题的共识是什么。    提交人    /u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</guid>
      <pubDate>Tue, 28 May 2024 12:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] ~300 条新闻，可快速了解当前生成式 AI 的最新动态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2i2id/p_300_news_to_quickly_get_uptodate_with_the/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2i2id/p_300_news_to_quickly_get_uptodate_with_the/</guid>
      <pubDate>Tue, 28 May 2024 12:18:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] H5 到 TFLite 转换中 TransposeConv 的奇怪维度。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</link>
      <description><![CDATA[      我尝试在 https://medium.com/analytics-vidhya/noise-suppression-using-deep-learning-6ead8c8a1839，这是full Conv1D SEGAN 模型。 然后我完成训练并得到 H5 模型。然后我尝试转换为具有 Full Integer INT8 量化的 TFLite 模型。 （原始示例没有这样做全整数量化，仅设置为“默认”。）量化代码如下。 defrepresentative_data_gen(): for input_value, _ in test_dataset.take(100): yield [input_value] model = load_model(&#39; NS_SEGAN_localTrained.h5&#39;) model.summary() score = model.evaluate(test_dataset)  tflite_model = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model.optimizations = [tf.lite.Optimize.DEFAULT]&lt; /code&gt; tflite_model.representative_dataset =representative_data_gen tflite_model.target_spec.supported_ops = [  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS, # 启用 TensorFlow 操作。 &lt; code&gt;tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # 同时使用选择操作和内置函数 tflite_model.inference_input_type = tf.int8 tflite_model.inference_output_type = tf.int8 tflite_model_quant_INT8 = tflite_model.convert() with open(&#39; NS_SEGAN_localTrained_quant_2.tflite&#39;, &#39;wb&#39;) as f: f.write(tflite_model_quant_INT8) 然后看起来很奇怪，只有第一个“TransposeConv”运算符获取正常维度，其他运算符的输出维度为[1,1,1,1]。 第一个“TransposeConv”具有正常尺寸。 其他 6 个 &#39;TransposeConv&#39; 在 TFLite 转换后得到 [1,1,1,1]。 型号链接 H5 型号  TFLite（完整 INT8 量化） 我有点怀疑这是正确的，而另一方面，它是由 TFLite API 转换的，这让我认为它应该是正确的。有人专家告诉我它不应该是[1,1,1,1]，但没有解释或建议。 我不知道如何确认这是否正确。如果[1,1,1,1]在这种情况下是合理的？此外，如果是错误的，为什么会发生这种情况以及如何解决它？如果有人有想法或建议，请建议或指导非常感谢。   由   提交/u/Ok_Box_6059   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</guid>
      <pubDate>Tue, 28 May 2024 11:54:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在 pytorch 模型上运行并发推理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</link>
      <description><![CDATA[大家好， 我有几个用于验证图像的 pytorch 模型，我想将它们部署到端点。我正在使用快速 api 作为 API 包装器，我将介绍迄今为止的开发过程： 之前，我正在运行一个简单的 OOTB 推理，如下所示： model = Model() @app.post(&#39;/model/validate/&#39;): pred = model.forward(img) return {&#39;pred&#39;:pred}  这种方法的问题是它无法处理并发流量，因此请求会排队，并且推理会一次发生 1 个请求，这是我想避免的。 我当前的实现如下：它复制模型对象，并分拆一个新线程来处理特定图像。有点像这样： model = Model() def validate(model, img): pred = model.forward(img) return pred @app.post(&#39;/model/validate/&#39;): model_obj = copy.deepcopy(model) loop = asyncio.get_event_loop() pred = await loop.run_in_executor(validate, model_obj, img) return {&#39;pred&#39; : pred}  这种方法会复制模型对象并对对象副本进行推断，这样我就能够处理并发请求。 我的问题是，有没有另一种更优化的方法可以实现 pytorch 模型并发，或者这是一种有效的做事方式？ TLDR：使用模型对象的副本创建新线程以实现并发，还有其他方法可以实现并发吗？    提交人    /u/comical_cow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 07:33:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 泊松变分自动编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</link>
      <description><![CDATA[预印本：https://arxiv.org/abs/2405.14473 X 线程摘要：https://x.com/hadivafaii/status/1794467115510227442    提交人    /u/vafaii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</guid>
      <pubDate>Tue, 28 May 2024 04:56:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] SAR 和光学图像的多模态图像分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d23awr/d_multimodal_image_classification_for_sar_and/</link>
      <description><![CDATA[你好！ 在过去的几周里，我一直在尝试使用 SAR 和光学图像，但由于代码中的错误，我无法对其进行训练。 我觉得我在理论和实践方面都缺乏有关该主题的大量信息，其中我可以学习编写这样的模型吗？ 提前致谢！   由   提交 /u/Icy_Dependent9199   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d23awr/d_multimodal_image_classification_for_sar_and/</guid>
      <pubDate>Mon, 27 May 2024 21:49:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] DARWIN - 开源 Devin 替代品带着更新回来了</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d219rl/p_darwin_opensourced_devin_alternative_is_back/</link>
      <description><![CDATA[达尔文带着另一次更新回来了🦾。 那么，本周有什么新内容？本周我们强调了提高 DARWIN 理解现有项目的能力，这些项目是在没有 DARWIN 帮助的情况下编写的，并且脱离了上下文。由于上下文长度是一个挑战，DARWIN 有效地映射了存储库结构并提取了保留足够上下文的类和函数签名。 除此之外，我们还收到了大量要求在更安全的环境中运行 DARWIN 的请求，因此我们有发布了前端和后端的 docker，您可以从存储库或 docker hub 下载它们。 观看我们的视频教程，见证 DARWIN 的实际功能： 📹 视频 1：观看 DARWIN在此处训练机器学习模型：Darwin ML Training 以防万一您错过了我们上一版本中的 DARWIN，DARWIN 是一位由您指挥的人工智能软件实习生。它具有帮助您构建和部署代码的功能。通过访问互联网，达尔文依靠更新的知识来编写代码并执行它们。如果万一遇到错误，DARWIN 会尝试通过访问讨论和论坛来解决它。更好的是它是开源的。 访问达尔文 欢迎加入我们，我们将揭开达尔文的全部潜力。在评论中分享您的反馈、想法或您希望 DARWIN 下一步做什么，或前往 DARWIN 存储库。我们还在建立一个 Discord 社区，我们很高兴在那里见到您。   由   提交 /u/Curious-Swim1266    reddit.com/r/MachineLearning/comments/1d219rl/p_darwin_opensourced_devin_alternative_is_back/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d219rl/p_darwin_opensourced_devin_alternative_is_back/</guid>
      <pubDate>Mon, 27 May 2024 20:20:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] MusicGPT – 一个使用本地法学硕士生成音乐的开源应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</link>
      <description><![CDATA[大家好！ 想分享一下我过去几个月一直在做的最新副业。这是一个运行本地音乐生成模型的终端应用程序，目前只有 MusicGen by Meta 可用。 https://github.com/gabotechs/MusicGPT 它适用于 Windows、Linux 和MacOS 无需安装 Python 或任何重型机器学习框架。相反，它完全用 Rust 编写，使用 ONNX 运行时以高性能方式在本地运行 LM，甚至使用 GPU 等硬件加速器。 该应用程序的工作原理如下：  它接受用户的自然语言提示 根据提示生成音乐样本 编码生成的样本转换为 .wav 格式并在设备上播放  此外，它还提供了一个 UI，允许在类似聊天的 Web 应用程序中与 AI 模型进行交互，存储聊天历史记录和设备上生成的音乐。 该项目的愿景是最终能够实时生成无限的音乐流，例如，在编码时收听始终新的 LoFi 歌曲的无限流，但还没有完全实现...... 这是一个有趣的旅程，在 Rust 的受限环境中建立基于 Transformer 的模型并运行，没有 PyTorch 或 TensorFlow，希望你喜欢它！ &lt; /div&gt;  由   提交 /u/GabrielMusat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</guid>
      <pubDate>Mon, 27 May 2024 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用图神经网络对列车延误进行多步并行预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1uawc/d_multistep_parallel_prediction_for_train_delays/</link>
      <description><![CDATA[    &lt; /a&gt;  大家好， 我目前正在做一个涉及使用的项目用于预测火车延误的图神经网络（GNN）。目标是对网络中的每个列车执行多步并行预测。主要挑战之一是考虑问题的时空维度以及整个铁路上列车之间的相互作用（参见图中网络的样子）。该数据集由给定时间的网络状态（包含当前列车）的快照组成 https://preview.redd.it/nywjb71fnz2d1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f141d0e1badf1e5ec 7397972f6c1bd8051fc09f9 到目前为止，我已经提出了两种不成熟的方法，希望得到任何建议、评论或替代方案： 方法 1：异构图模型 在这种方法中，我构建了一个具有两种类型节点的异构图：车站和火车。这导致了三种类型的关系：火车站-车站、火车-火车和车站-车站。预测任务本质上是边缘预测，特别是火车站边缘。对于给定的火车，该模型旨在预测其与 n_following 车站的链接（延误）。 方法 2：同质图模型 我的第二个该方法涉及使用同质图，其中每个节点代表铁路网络中的一个重要点 (RP)，例如车站、交叉点等。每个节点都具有 RP 类型、地理位置和时间序列等特征代表火车在该 RP 随时间的延误情况的特征。该图中的边代表两个 RP 之间的直接行驶路径，具有平均行驶时间和每天列车数量等特征。 GNN 的输出是接下来几个时间步中每个 RP 的预测延迟。 在现有文献中，模型要么是针对一列火车的预测，要么铁路是由一条线路组成的这不是我想要的。  如果您有任何见解或建议，我将不胜感激。提前致谢！   由   提交/u/OtherDepartment8085  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1uawc/d_multistep_parallel_prediction_for_train_delays/</guid>
      <pubDate>Mon, 27 May 2024 15:35:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：特征选择的首选方法？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/</link>
      <description><![CDATA[方法 1 - 形状：删除平均绝对形状值低于特定值的特征 方法 2 - 特征重要性：删除特征特征重要性值低于特定值 方法 3 - R 平方：从模型中单独删除每个特征，并计算每个单独模型的 R2 分数。应删除不会显着增加 R2 分数的特征 方法 4 - 保留所有特征并让 XGBoost 对其进行排序 您对这些特征的相对功效有何看法方法以及您喜欢使用的任何其他方法，特别是 XGBoost？   由   提交 /u/Gef_1_Man_Army   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/</guid>
      <pubDate>Mon, 27 May 2024 15:23:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离散多令牌方法与传递多模式输出和机器人控制隐藏状态的单令牌方法相比如何？行业整体趋势如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1rmjl/d_how_do_discrete_multitoken_approaches_compare/</link>
      <description><![CDATA[对于连续输出，我见过使用两种不同的方法 1：模型给出了离散标记的词汇表，并可以组装它们的列表，并将其传递给解码器 2：模型为每个输出模态赋予一个标记（例如，图像标记），并且在选择时将隐藏状态传递给解码器 这两种方法的最新趋势是什么？ 2 看起来会快很多，因为你只需要 1 个令牌，但这对损失函数有何影响，因为有时你会遇到回归问题（图像生成），有时你会遇到分类问题？    由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1rmjl/d_how_do_discrete_multitoken_approaches_compare/</guid>
      <pubDate>Mon, 27 May 2024 13:33:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 分形网络曾经被扩展过吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/</link>
      <description><![CDATA[我一直在阅读《FractalNet：超深度神经网络》没有残差的网络”，我想知道 FractalNet 背后的方法是否在其他文章中得到了改进。   由   提交/u/research_pie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/</guid>
      <pubDate>Mon, 27 May 2024 13:28:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>