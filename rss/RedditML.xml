<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sun, 02 Jun 2024 15:13:24 GMT</lastBuildDate>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[D]埃及阿拉伯方言自动语音识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6etb2/d_egyptian_arabic_dialect_automatic_speech/</link>
      <description><![CDATA[大家好，我有一个竞赛，开发阿拉伯埃及方言的自动语音识别，我们有一个大约 8 GB 的大数据集，有人可以推荐我一个合适的配方、教程或框架以及良好的文档吗。  请注意：不允许对 wav2vec 等预训练模型进行微调，训练必须从头开始     提交人    /u/Unfair-Training-6310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6etb2/d_egyptian_arabic_dialect_automatic_speech/</guid>
      <pubDate>Sun, 02 Jun 2024 14:41:51 GMT</pubDate>
    </item>
    <item>
      <title>如果 LLM 是基于 token 的自回归模型，那么它们如何生成图像？（Transformers + VQVAE）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</link>
      <description><![CDATA[      分享我 YT 频道的一段视频，讨论某些多模态 LLM（如 Gemini）如何将图像生成为一系列可学习的图像标记。    提交人    /u/AvvYaa   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</guid>
      <pubDate>Sun, 02 Jun 2024 14:33:16 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 人们是否有兴趣使用两个通过 NVLINK 连接的 RTX A6000 来创建中端 GPU 装备？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6ct6w/discussion_are_people_interested_in_creating_a/</link>
      <description><![CDATA[https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-a6000/ 这将提供 96 GB 的内存大小 由于成本原因，我想利用 NVIDIA 的高等教育和研究资助计划 https://developer.nvidia.com/higher-education-and-research    提交人    /u/Flintstone9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6ct6w/discussion_are_people_interested_in_creating_a/</guid>
      <pubDate>Sun, 02 Jun 2024 13:01:27 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Tangles：Diestel 在书中宣布了一种新的数学 ML 工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</link>
      <description><![CDATA[      大家好，我想分享一本社区可能会感兴趣的新书！ 图论学家 Diestel 写了一本面向 ML 社区（及其他人）的书：  缠结：经验科学中人工智能的结构化方法 Reinhard Diestel，剑桥大学出版社 2024  ----- 出版商简介： 缠结提供了一种在不精确数据中识别结构的精确方法。通过将经常一起出现的特质分组，它们不仅可以揭示事物的集群，还可以揭示其特质的类型：政治观点、文本、健康状况或蛋白质的类型。缠结为人工智能提供了一种新的结构化方法，可以帮助我们理解、分类和预测复杂现象。 这已成为可能，这是由于缠结的数学理论最近被公理化，这使得缠结的应用范围远远超出了图论的起源：从数据科学和机器学习中的聚类到预测经济学中的客户行为；从 DNA 测序和药物开发到文本和图像分析。 这是首次探索此类应用。假设只具备本科数学基础知识，那么缠结理论及其潜在含义将对科学家、计算机科学家和社会科学家开放。 ----- 电子书以及包括教程在内的开源软件可在 tangles-book.com 上找到。 注意：这是一本“外展”书，主要不是关于缠结理论，而是关于以多种意想不到的方式和领域应用缠结。图中的缠结在 Diestel 的《图论》第 5 版中介绍。 目录和数据科学家简介（Ch.1.2）可从 tangles-book.com/book/details/ 和 arXiv:2006.01830 获得。第 6 章和第 14 章介绍了一种基于缠结的新软聚类方法，与传统方法截然不同。第 7-9 章涵盖了第 14 章所需的理论。 tangles-book.com 的软件部分表示，他们邀请在具体项目上进行合作，以及为他们的 GitHub 软件库做出贡献。  https://preview.redd.it/ysj91dw2o54d1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2 ​ ​    提交人    /u/Prestigious_Ship_238   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</guid>
      <pubDate>Sun, 02 Jun 2024 12:56:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] EMNLP 匿名政策</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6c5ox/d_emnlp_anonymity_policy/</link>
      <description><![CDATA[今年 1 月，ACL 根据工作组报告的建议更新了其匿名政策，该报告指出：  我们强调，提交和审查应保持双盲，提交的论文应完全匿名。认识到其中涉及许多权衡，在进行社区范围的调查并考虑了多种选择（列在本文档末尾附近的“考虑的提案”部分中）后，我们建议如下：(a) 更改 ACL 政策，现在允许随时进行匿名和非匿名预印，以便可以不受限制地进行有关工作的技术对话。 (b) 明确表示允许在所有媒体（包括社交媒体）上讨论未发表的作品，但不鼓励（但不禁止）进行广泛的宣传和公关。  这可能是一个显而易见的问题，但我以前从未提交过 ACL 会议，也不想搞砸它：这是否意味着我们可以在提交之前、期间或之后的任何时间将非匿名预印本发布到 Arxiv，而没有被拒绝的风险？    提交人    /u/monkeyofscience   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6c5ox/d_emnlp_anonymity_policy/</guid>
      <pubDate>Sun, 02 Jun 2024 12:25:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] PU 学习综述</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6b1z4/d_a_survey_of_pu_learning/</link>
      <description><![CDATA[      主要 html 版本 sample1 paper1 paper2    提交人    /u/Acceptable-Worry-493   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6b1z4/d_a_survey_of_pu_learning/</guid>
      <pubDate>Sun, 02 Jun 2024 11:17:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] FineWeb 技术报告：大规模挖掘网络上最精细的文本数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</link>
      <description><![CDATA[FineWeb 15 万亿公开发布的网络规模数据集背后的团队刚刚发表了一篇关于创建高质量网络规模数据集的科学的详尽博客文章，详细介绍了 FineWeb 的步骤和学习成果，以一种 distill.pub 交互式文章/博客的方式。 他们还发布了 FineWeb-Edu，这是 Common Crawl 的一个过滤子集，拥有 1.3T 令牌，专注于教育内容非常丰富的网页，在知识和推理密集型基准测试（如 MMLU、ARC 和 OpenBookQA）上，其表现似乎优于所有公开发布的网络规模数据集 有趣的阅读：https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1     由    /u/Thomjazz 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</guid>
      <pubDate>Sun, 02 Jun 2024 08:17:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 合并竞赛：通过合并高效构建 LLM（NeurIPS 2024 挑战赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d67ydm/r_llm_merging_competition_building_llms/</link>
      <description><![CDATA[NeurIPS 2024 挑战赛网站：https://llm-merging.github.io/ Discord：https://discord.gg/dPBHEVnV LLM 合并竞赛：通过合并高效构建 LLM NeurIPS 2024 挑战赛 目标和重点 从头开始训练高性能大型语言模型 (LLM) 是一项众所周知的昂贵且困难的任务，仅计算费用就高达数亿美元。然而，这些预先训练过的 LLM 可以通过微调以低成本轻松适应新任务，从而产生大量适合特定用例的模型。最近的研究表明，专门的微调模型可以快速合并以结合各种功能并推广到新技能。 开始 比赛将为参赛者提供已经在特定任务数据集上训练过的专家模型列表。所有这些模型都将在 Hugging Face Model Hub 上公开提供，并具有允许将其用于研究目的的许可证。这些模型可以是完全微调的模型，也可以是通过参数高效的微调方法（例如 LoRA）获得的模型。此列表中的模型需要满足以下标准：（1）模型大小 &lt;= 8B 参数，（2）模型具有与研究用途兼容的许可证（例如 MIT、Apache 2 等）。 可以在此处找到具有端到端提交流程的入门套件：https://github.com/llm-merging/LLM-Merging    提交人    /u/hardmaru   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d67ydm/r_llm_merging_competition_building_llms/</guid>
      <pubDate>Sun, 02 Jun 2024 07:33:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多智能体强化学习中 IQL 与 VDN 的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d67d7j/r_different_between_iql_and_vdn_for_multi_agent/</link>
      <description><![CDATA[      大家好， 我正在研究强化学习中的合作多智能体。我已经使用 DQN（深度 Q 网络）为 PettingZoo 中 Cooperative Pong 环境中的每个代理实现了独立 Q 学习。现在我正在阅读有关 VDN （值分解网络）的内容，我有一个问题。 在本文中，他们说连接值 Q 函数是每个代理的子 Q 函数的总和。这个想法是最大化每个代理的局部 Q 函数，这相当于最大化全局 Q 函数。但 IQL 也是最大化每个代理的局部 Q 函数。那么这两者之间有什么不同？ https://preview.redd.it/e3gvpcotv34d1.png?width=626&amp;format=png&amp;auto=webp&amp;s=07672f79fee8fbaa660abb0bcfb28c24de01ba23    提交人    /u/Civil_Statement_9331   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d67d7j/r_different_between_iql_and_vdn_for_multi_agent/</guid>
      <pubDate>Sun, 02 Jun 2024 06:53:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 师生培训策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65y1h/d_teacher_student_training_strategy/</link>
      <description><![CDATA[我计划使用 LLM（例如 llama3）通过提示提取训练数据，然后使用带有 CLS 令牌的较小模型进行自定义训练，以尝试匹配 LLM 的准确性。假设我可以在 1M+ 数据上运行提示（尽管我怀疑我不需要那么多）。 提示：以下句子包含苹果还是橘子：示例：  &quot;&lt;prompt&gt; 苹果，橘子&quot; -&gt; 苹果，橘子 &quot;&lt;prompt&gt; 苹果，橘子&quot; -&gt; 苹果，橘子 &quot;&lt;prompt&gt; 苹果，没有橘子&quot; -&gt;苹果  所以我的问题是：  我见过的最后一个 CLS 类型 LLM 是微软的 xtremedistil。这些模型还在使用吗？如果是，最新 + 最好的是什么？ 使用句子转换器并进行分类会更好吗？ 在我的学生模型训练集中，我将删除上面的提示，这种方法有风险吗？  在我看来，从长远来看，这些模型更小，成本也更低。非常感谢大家的普遍想法。    提交人    /u/themathstudent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65y1h/d_teacher_student_training_strategy/</guid>
      <pubDate>Sun, 02 Jun 2024 05:16:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您在现实世界中使用 LLM 的案例有哪些？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</link>
      <description><![CDATA[我认为我们应该分享更多 LLM 的生产用例，而不仅仅是理论上的最佳实践。 您可以分享您在生产中看到/构建的用例吗？它应包括以下详细信息：  它解决的问题 实现细节（模型、基础设施等） 它产生的业务影响     提交人    /u/madredditscientist   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</guid>
      <pubDate>Sun, 02 Jun 2024 05:12:14 GMT</pubDate>
    </item>
    <item>
      <title>为开源模型实现“扩展单义性：从 Claude 3 Sonnet 中提取可解释的特征”论文。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</link>
      <description><![CDATA[我最近偶然发现了一篇有趣的论文，题为“扩展单义性：从 Claude 3 Sonnet 中提取可解释特征”，该论文探讨了如何使用稀疏自动编码器从大型语言模型的激活中提取可解释特征。该方法似乎有望深入了解模型的内部表示和行为。 这让我开始思考为开源语言模型实施类似的可解释性技术的可行性。我们能否在不进行大量微调的情况下控制 LLM 及其行为。 我想联系这个社区讨论一些事情：  是否有人已经在开源语言模型上实施或试验过类似的可解释性技术？我们可以制作类似于金门克劳德的东西吗？ 您认为调整和扩展这些技术以与 Llama、phi、mistral 等一起使用是否可行？与 sonnet 相比，它们的参数大小要小得多。 我有兴趣与其他对这个研究领域充满热情的人合作。如果您正在研究开源模型的可解释性或对新方法有想法，我很高兴与您合作并进一步探索。我们可以合作实施技术、共享资源或集思广益新想法。  如果您有兴趣合作或有任何想法要分享，请随时分享。    提交人    /u/No-Point1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</guid>
      <pubDate>Sun, 02 Jun 2024 03:50:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] CoPE：上下文位置编码：学习计算重要的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</link>
      <description><![CDATA[  由    /u/fasttosmile  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</guid>
      <pubDate>Sat, 01 Jun 2024 19:05:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mojo 值得吗，或者你愿意为 ML 学习哪种第二语言？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/</link>
      <description><![CDATA[基本上就是标题。我非常精通 Python（正如预期的那样），但除此之外，我对 JavaScript 和 C++ 的了解非常有限。我想学习一种更“低级”的第二种语言，可以更好地利用硬件功能。我的目标不是重写 Pytorch 或完全替换 Python（尽管 将推理移植到 Mojo 可能有意义），而是为性能关键用例提供替代方案。 从今天的情况来看，答案显然是 C++。然而，Rust 越来越受欢迎，除了陡峭的学习曲线外，人们开始在许多方面将其置于 C++ 之上。在这两种情况下，语法和语言都与 Python 不太接近，这使得它们很难学习。 Mojo 在这方面似乎要好得多，既提供了语法类似于 Rust 的低级功能（至少对于像我这样的门外汉来说），又可以用作奇怪的 Python 风格。它甚至允许直接导入 Python 库。这对于这种缺乏大型社区和各种库的年轻语言非常有帮助。尽管如此，该语言仍然很年轻，而且很容易发生变化，所以我不确定是否应该投资。 那么，对于上述用例，您认为最好的“第二种”语言是什么？有使用 Mojo 的经验吗？您是如何学习它或资源有限的任何其他语言的。如果我使用 Mojo，我打算通读文档并解决去年使用它的代码出现的问题。    提交人    /u/canbooo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/</guid>
      <pubDate>Sat, 01 Jun 2024 11:15:04 GMT</pubDate>
    </item>
    </channel>
</rss>