<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 07 Sep 2024 18:19:11 GMT</lastBuildDate>
    <item>
      <title>我尝试编写自己的 YOLO 模型来检测足球运动员 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbc185/i_tried_to_code_my_own_yolo_model_to_detect/</link>
      <description><![CDATA[       由    /u/AvvYaa  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbc185/i_tried_to_code_my_own_yolo_model_to_detect/</guid>
      <pubDate>Sat, 07 Sep 2024 17:15:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用长距离深度学习对硬件加密进行广义功率攻击</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbbbb3/r_generalized_power_attacks_against_hardware/</link>
      <description><![CDATA[      https://preview.redd.it/8x0oesms1fnd1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=3d3d331ddb4ebf23eface0d49b312b28cd5e2fcd 星期六快乐 我很高兴地宣布，经过 3 年的研发，我们终于发布了 GPAM，我们的通用模型电源侧信道攻击模型：  幻灯片&amp; 论文：https://elie.net/publication/generalized-power-attacks-against-crypto-hardware-using-long-range-deep-learning 代码 &amp;数据集：https://github.com/google/scaaml/tree/main/papers/datasets/ECC/GPAM  与以前的方法相比，GPAM 代表了一代人的飞跃，因为它能够攻击多种算法（AES、ECC）和对策，而无需人工干预并且无需预处理输入跟踪。它确实需要一些自动超调优，因此：每次攻击约 700 GPU/h。    提交人    /u/ebursztein   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbbbb3/r_generalized_power_attacks_against_hardware/</guid>
      <pubDate>Sat, 07 Sep 2024 16:44:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] Adam 优化器导致 Transformer 语言模型中出现特权基础</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbavdv/r_adam_optimizer_causes_privileged_basis_in/</link>
      <description><![CDATA[        由    /u/rrenaud  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbavdv/r_adam_optimizer_causes_privileged_basis_in/</guid>
      <pubDate>Sat, 07 Sep 2024 16:26:05 GMT</pubDate>
    </item>
    <item>
      <title>学习 ViT 中的局部表示“[D]”“[R]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb9su4/learning_local_representations_in_vit_d_r/</link>
      <description><![CDATA[我正在阅读这篇题为“视觉变换器是否像卷积神经网络一样看待问题？”的论文，我有一个大问题。作者说，在较早的层中，注意力头会混合关注局部和全局，只有在对大型数据集（JFT）进行预训练时才会如此，而在小型数据集（ImageNet）上进行预训练时，它很难关注局部。我的问题是，为什么 ViT 很难关注在本地关注的自我补丁？    提交人    /u/Dapper-Edge2661   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb9su4/learning_local_representations_in_vit_d_r/</guid>
      <pubDate>Sat, 07 Sep 2024 15:40:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] NviWatch 是一款用于监控 Nvidia GPU 的 rust tui</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb55rf/p_nviwatch_a_rust_tui_for_monitoring_nvidia_gpus/</link>
      <description><![CDATA[      想分享，因为这可以帮助您进行 GPU 监控。✅ 专注于 GPU 进程✅ 多种视图模式✅ 用 rust 编写的轻量级✅ 直接使用 NVML    提交人    /u/msminhas93   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb55rf/p_nviwatch_a_rust_tui_for_monitoring_nvidia_gpus/</guid>
      <pubDate>Sat, 07 Sep 2024 11:53:24 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 学习更长序列的位置嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb4zls/discussion_learned_positional_embeddings_for/</link>
      <description><![CDATA[因此，我重新阅读了 transformer 论文，让我印象深刻的一件事是作者还使用了学习到的位置嵌入。Karpathy 的 nanoGPT 实现使用了学习到的位置嵌入，我想知道这些嵌入如何扩展到更长的序列？ 从直觉上讲，如果模型从未见过超过 max_length 的标记，它将无法生成有意义的东西。那么 OpenAI 的 GPT（假设他们仍然使用学习到的 PE）如何扩展到超过 2k 的上下文长度？    提交人    /u/tororo-in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb4zls/discussion_learned_positional_embeddings_for/</guid>
      <pubDate>Sat, 07 Sep 2024 11:43:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] Alex Kim 不为人知的故事 – 使用大型语言模型进行财务报表分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb43l0/d_the_untold_story_of_alex_kim_financial/</link>
      <description><![CDATA[        由    /u/NextgenAITrading 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb43l0/d_the_untold_story_of_alex_kim_financial/</guid>
      <pubDate>Sat, 07 Sep 2024 10:46:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 审查并为我的聊天机器人提出想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb2mwl/p_review_and_suggest_ideas_for_my_chatbot/</link>
      <description><![CDATA[好的，我目前正在尝试使用以下技术细节构建支持聊天机器人 1. 用于 Web 服务器的 FastAPI（需要使其更快）2. Qdrant 作为矢量数据库（发现它是 Chromadb、Elastic Search 和 Milvus 中最快的）3. MongoDB 用于存储所有数据和反馈。4. 语义分块，最大令牌限制为 512。5. granite-13b-chat-v2 作为 LLM（我知道它不好，但我可用的选项有限）6. 数据是结构化的和非结构化的。考虑将 GraphRAG 纳入当前架构。7. 多个数据源存储在多个矢量数据库集合中，因为我已经实现了访问控制。8. 目前使用 mongoengine 作为 ORM。如果您知道更好的方法，请提出建议。 9. 目前使用 all-miniLM-l6-v2 作为向量嵌入，但计划使用 stella_en_400M_v5。10. 使用余弦相似度检索文档。11. 使用 BLEU、F1 和 BERT 分数根据黄金答案进行自动评估。12. 使用 top_k 作为 3。13. 目前使用基本问答提示，但想要改进它。有什么建议吗？也听说过自动提示评估。14. 目前所有东西都使用自定义代码。希望使用 Llamaindex 或 Langchain 来实现这一点。15. 现在我没有使用任何 AI 代理，但我想知道你的意见。16. 这是一个简单的 RAG 框架，我正在努力改进它。17. 我还没有包括重新排名器，但我也计划这样做。 我想我提到了我在项目中使用的几乎所有东西。所以请分享你对它的建议、评论和评价。谢谢！！    由   提交  /u/QaeiouX   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb2mwl/p_review_and_suggest_ideas_for_my_chatbot/</guid>
      <pubDate>Sat, 07 Sep 2024 08:57:04 GMT</pubDate>
    </item>
    <item>
      <title>[N] Arxiv 将于周二举办多模式“声音化”无障碍会议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb28a3/n_arxiv_is_having_a_multimodal_sonification/</link>
      <description><![CDATA[  由    /u/Competitive_Travel16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb28a3/n_arxiv_is_having_a_multimodal_sonification/</guid>
      <pubDate>Sat, 07 Sep 2024 08:26:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 评估大型语言模型在保护秘密/隐藏信息方面的有效性的工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb0ami/p_tool_for_assessing_the_effectiveness_of_large/</link>
      <description><![CDATA[https://github.com/user1342/Would-You-Kindly    由   提交  /u/OppositeMonday   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb0ami/p_tool_for_assessing_the_effectiveness_of_large/</guid>
      <pubDate>Sat, 07 Sep 2024 06:03:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种 LLM 模型最适合对文本到 SQL 进行微调？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fawirs/d_which_llm_model_is_best_suited_for_finetuning/</link>
      <description><![CDATA[我正在开展一个金融数据分析项目，重点是文本到数据的可视化。第一步是根据输入文本生成相关的 SQL 查询。我使用 Mistral 7B 模型来完成这项任务。但是，在使用 Google Colab 中的数据集对其进行训练时，我总是遇到内存不足错误。我尝试了各种配置，例如调整批处理大小和标记长度，但每次仍然显示 CUDA 内存不足错误。我使用了不同类型的硬件加速器，但问题仍然存在。有人对我使用的模型是否太大或是否有我应该考虑的替代方案有什么建议吗？    提交人    /u/More_Lawfulness_6862   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fawirs/d_which_llm_model_is_best_suited_for_finetuning/</guid>
      <pubDate>Sat, 07 Sep 2024 02:19:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 检索增强生成 vs 长上下文 LLM，我们确定后者会取代前者吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fabu65/d_retrievalaugmented_generation_vs_longcontext/</link>
      <description><![CDATA[我认为这个问题已经争论了很长时间。但最近有两篇关于这个问题的有趣文章，我想以此作为讨论 RAG 与长上下文 LLM 的起点。 总之，如果我们可以将所有内容都放在提示中，我们就不需要进行检索。但是，我真的怀疑我们能否拥有一个能够覆盖任何组织拥有的大量数据的上下文长度的模型（并且没有可怕的计算成本）。 无论如何，有令人难以信服的报告称 LC-LLM 在 QA 中效果更好（至少到目前为止，我还没有读过一篇文章让我相信 LC-LLM 比 RAG 效果更好）。  有两篇文章讨论了噪声对 LLM 和 RAG 的影响：  第一篇文章指出噪声会影响 LLM 的性能，并竭尽全力对此进行描述。https://arxiv.org/abs/2408.13533 第二篇文章比较了 RAG 和 LC-LLM，并表明通过增加上下文的大小，我们会出现峰值（我们添加相关块），然后性能会下降，因为 LLM 更难找到正确的信息。 https://arxiv.org/abs/2409.01666  我认为我们最终会保留 RAG 的原因或多或少是 LLM 是复杂的神经网络，因此也是模式识别机器。最终，优化信噪比是机器学习中最常见（有时也很困难）的任务之一。当我们开始过度增加这种噪音时，最终模型必然会开始发现噪音并偏离重要信息（此外，LLM 的参数记忆和上下文之间也存在微妙的相互作用，我们仍然不知道为什么有时会忽略上下文） 其次，我个人认为，还有一个结构性原因。自注意力机制会寻求相关关系，而在上下文长度增加的情况下，我们倾向于维数灾难，最终会加剧虚假关系。 我想和您讨论一下您认为 RAG 不会被取代的原因，或者您是否认为 LC-LLM 最终会取代它？在第二种情况下，它如何解决大量上下文无关数据的问题？    submitted by    /u/NoIdeaAbaout   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fabu65/d_retrievalaugmented_generation_vs_longcontext/</guid>
      <pubDate>Fri, 06 Sep 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 CUDA 比 ROCm 快这么多？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/</link>
      <description><![CDATA[通常人们会回答“因为 NVIDIA 有更多的时间和金钱”。但是，为什么 AMD 无法赶上？究竟是什么让优化 ROCm 如此困难？ 如果您可以指出一些资源，或者您的回答尽可能详细地说明特定内核和结构的实现以及如何从 Triton 或 XLA 精确地进行和优化 CUDA 调用，那将会很有帮助。谢谢 :)    提交人    /u/evilevidenz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/</guid>
      <pubDate>Fri, 06 Sep 2024 06:52:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f63rhf/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f63rhf/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 01 Sep 2024 02:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>