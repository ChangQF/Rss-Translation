<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 28 Jan 2024 09:13:17 GMT</lastBuildDate>
    <item>
      <title>需要 NLP 项目帮助！！ [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acy8bb/nlp_project_help_needed_p/</link>
      <description><![CDATA[我目前正在从事一个 NLP 项目，即文本摘要。我无法获得正确的数据集......那么，我应该选择预训练模型吗？  我被困住了。任何网络参考也会有所帮助。提前致谢！！   由   提交 /u/Ok_Freedom__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acy8bb/nlp_project_help_needed_p/</guid>
      <pubDate>Sun, 28 Jan 2024 08:40:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 200k 向量 (30GB) 语义搜索的最佳实践 嵌入的价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acxvd7/d_best_practices_for_semantic_search_on_200k/</link>
      <description><![CDATA[嗨，我已将一些特定于域的名称向量转换为嵌入，数据集大小为 200k 字。所有嵌入都是使用 OpenAI 的嵌入模型 3 生成的（每个嵌入 3072 个暗淡）。现在我计划实现语义搜索相似度。给定一个域关键字，我想找到前 5 个最相似的匹配项。嵌入所有 280k 个单词后，包含嵌入的 JSON 文件的大小约为 30GB。 我是这个领域的新手，正在评估最佳选项。  我应该这样做吗？使用像 Pinecone 或 Typsense 这样的云矢量数据库，还是在 DigitalOcean 上本地托管？ 如果我选择像 Typsense 这样的云选项，我需要什么配置（RAM 等）才能实现 280k 嵌入（30GB）尺寸）？大概要花多少钱？  过去几天我一直很困惑，找不到有用的资源。我们将非常感谢您提供的任何帮助或建议。   由   提交/u/stoicbats_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acxvd7/d_best_practices_for_semantic_search_on_200k/</guid>
      <pubDate>Sun, 28 Jan 2024 08:15:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2024 反驳</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acxpdq/d_cvpr_2024_rebuttals/</link>
      <description><![CDATA[许多人，尤其是首次提交者，在寻找资源来制定有效反驳方面面临着挑战。我在之前的互动中遇到过一篇关于这个主题的有用帖子，事实证明它是有益的。您可以在这篇文章中找到有关撰写反驳的深入指导，特别是对于首次投稿者而言：我们如何撰写反驳。感谢作者！ 不幸的是，出于隐私考虑，我无法分享我个人的反驳。不过，我鼓励每个人通过分享任何公开的信息来做出贡献，这些信息可以帮助其他人磨练反驳写作技巧   由   提交/u/darkknight-6  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acxpdq/d_cvpr_2024_rebuttals/</guid>
      <pubDate>Sun, 28 Jan 2024 08:04:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能对移动应用程序开发的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acx2p0/d_impact_of_ai_on_mobile_app_development/</link>
      <description><![CDATA[我是一名移动应用开发人员，主要是 Android。我很好奇最近对人工智能的关注如何能够通过更好、更智能的框架改善应用程序开发体验或提高应用程序性能。 有人知道这些领域的最新进展吗？    由   提交 /u/oldbell_newbell   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acx2p0/d_impact_of_ai_on_mobile_app_development/</guid>
      <pubDate>Sun, 28 Jan 2024 07:21:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我有一个关于强化学习中时间相关性问题的问题。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acv7fu/d_i_have_a_question_about_the_issue_of_temporal/</link>
      <description><![CDATA[我一直在学习强化学习，并试图了解样本之间时间相关性的影响。我知道这会使学习不稳定，但我不清楚为什么。是不是因为只计算某些情况下的梯度，导致学习出现偏差和不稳定？ 我的另一个问题是，在我正在阅读的 RL 书的 PG 部分中，它说对于使用return（REINFORCE）的策略梯度方法，样本之间的相关性不是问题，因为更新是使用return完成的，这是总奖励，所以没有必要使用重播缓冲区，对吗？ 我知道A2C算法是一种在线学习方法，它使用Q函数而不是返回来更新每一步，但这是否会导致样本之间的相关性问题？如果是的话，那么使用 return 的 REINFORCE 是否具有离线 RL 的特征？   由   提交/u/DRLC_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acv7fu/d_i_have_a_question_about_the_issue_of_temporal/</guid>
      <pubDate>Sun, 28 Jan 2024 05:26:53 GMT</pubDate>
    </item>
    <item>
      <title>[R]CV问题：“你见过这个物体吗？”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acufln/rcv_question_have_you_seen_this_object/</link>
      <description><![CDATA[假设您有 5 个苹果 a、b、c、d、e。您正在向摄像机一一展示这些苹果，从a到e。您的对象检测模型会检测到它们，绘制它们的边界框并给它们添加标签“apple”。 但是是否可以在其中显示“apple a”？再次让模型思考“是的，我以前见过这个苹果，它的‘苹果 a’”并给苹果“apple a”标签，而不仅仅是“apple”？   由   提交/u/OkRestaurant9285   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acufln/rcv_question_have_you_seen_this_object/</guid>
      <pubDate>Sun, 28 Jan 2024 04:42:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 变换++</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acsunw/d_transformer/</link>
      <description><![CDATA[大家好，我发现了这篇有趣的论文，但我无法找到它的任何实现。有谁知道在哪里可以找到 Transformer++ 的实现/示例代码？谢谢顺便说一句：D 这是论文的链接：https://arxiv.org/abs/2003.04974   由   提交 /u/Ok-Literature5484    reddit.com/r/MachineLearning/comments/1acsunw/d_transformer/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acsunw/d_transformer/</guid>
      <pubDate>Sun, 28 Jan 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们一直称“生成”模型为“生成”模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acsq74/d_why_do_we_keep_calling_generation_models/</link>
      <description><![CDATA[我认为生成模型模拟了联合概率分布，而判别模型模拟了条件概率。 当我们执行文本或图像时一代，我们不是为模型提供某种输入来进行调节吗？难道这些不应该被称为“一代模型”吗？因为它们本质上具有歧视性，但正在执行生成任务？   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acsq74/d_why_do_we_keep_calling_generation_models/</guid>
      <pubDate>Sun, 28 Jan 2024 03:10:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 变分自动编码器已经 10 岁了</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acqgn9/d_the_variational_autoencoder_is_now_10_years_old/</link>
      <description><![CDATA[我感觉自己老了哈哈。  不过，严肃地说，作为深度生成建模的实用选择，它似乎经受住了时间的考验。相比之下，GAN 研究似乎已经变得停滞不前，流、基于能量的模型和基于扩散/基于分数的模型正在被纳入 VAE 中，以实现更具表现力的先验。我坚信 VAE 在未来很长一段时间内仍然有用。 只是一个想法。   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acqgn9/d_the_variational_autoencoder_is_now_10_years_old/</guid>
      <pubDate>Sun, 28 Jan 2024 01:14:35 GMT</pubDate>
    </item>
    <item>
      <title>[N][P] 各种国际象棋语言模型新闻，包括发布据称 Elo 高达 1500 的开源语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aclto7/np_various_chess_language_model_news_including/</link>
      <description><![CDATA[国际象棋语言模型新闻： a) Chess-GPT：开源语言模型，据称 Elo 高达 1500。其中包括一些神经网络可解释性材料。开发者 - u/seraine - 创建了有关此 此处和此处 &gt;. b) 博客文章 揭穿棋盘：用 GPT 对抗国际象棋引擎来估计 Elo 评级并评估合法移动能力：一位计算机科学教授对 4 种语言模型进行的国际象棋测试。测试的性能最好的语言模型是 gpt-3.5-turbo-instruct，估计 Elo 为 1750 +/- 50，非法移动尝试率约为千分之一。 我之前在本子中发表的关于 gpt-3.5-turbo-instruct 下棋的文章。。 p&gt; c) Subreddit r/LLMChess 最近创建。   由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aclto7/np_various_chess_language_model_news_including/</guid>
      <pubDate>Sat, 27 Jan 2024 21:44:07 GMT</pubDate>
    </item>
    <item>
      <title>您如何协调人工智能的巅峰炒作与艰难的人工智能就业市场？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjty3/how_do_you_reconcile_peak_hype_in_ai_with_a_tough/</link>
      <description><![CDATA[ DeepMind 联合创始人表示“我们已经在人工智能革命中达到了炒作的顶峰”。1&lt; /li&gt; 此外，美国的失业率处于历史低位，为 3.7%。 然而，我不断听说人工智能研究人员和从业者的就业市场目前非常艰难。&lt; /li&gt;  如何协调这三者？  1 https://www.youtube.com/watch?v=Go_6UldZL50   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjty3/how_do_you_reconcile_peak_hype_in_ai_with_a_tough/</guid>
      <pubDate>Sat, 27 Jan 2024 20:14:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] DeepSeek-Coder：当大语言模型遇上编程——代码智能的崛起 - DeepSeek-AI 2024 - 超越GPT-3.5和Codex的SOTA开源编码模型，同时在研究和商业用途上不受限制！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjpp1/r_deepseekcoder_when_the_large_language_model/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2401.14196  Github：https： //github.com/deepseek-ai/DeepSeek-Coder  模型：https://huggingface。 co/deepseek-ai  摘要：  大型语言模型的快速发展彻底改变了软件开发中的代码智能。然而，闭源模型的主导地位限制了广泛的研究和开发。为了解决这个问题，我们推出了 DeepSeek-Coder 系列，这是一系列大小从 1.3B 到 33B 的开源代码模型，在 2 万亿个代币上从头开始训练。这些模型在高质量的项目级代码语料库上进行了预训练，并采用 16K 窗口的填空任务来增强代码生成和填充。我们的广泛评估表明，DeepSeek-Coder 不仅在多个基准测试中实现了开源代码模型中最先进的性能，而且还超越了 Codex 和 GPT-3.5 等现有的闭源模型。此外，DeepSeek-Coder 模型享有宽松的许可证，允许研究和不受限制的商业用途。   https://preview.redd.it/adspck4uh1fc1.jpg?width=1505&amp;format=pjpg&amp;auto=webp&amp;s = 94970f9bd5db45bf4be9f206355c8f2a4545dcc3 https： //preview.redd.it/7cm8hk4uh1fc1.jpg?width=1659&amp;format=pjpg&amp;auto=webp&amp;s=cba202f43a220492209b1ece030f7a76b080212a https://preview.redd.it/8jobgk4uh1fc1.jpg?width=1535&amp;format=pjpg&amp;auto =webp&amp; s=62065c3855e5abf329f3df46414e5c50fd293b66  https://preview.redd.it/mtoq8n4uh1fc1.jpg?width=1524&amp;format=pjpg&amp;auto=webp&amp;s=96130d9578a11f21d03a0bd6755e6a2c0034b4c5 https://preview.redd.it/tc032n4uh1fc1.jpg?width=1698&amp;format=p jpg&amp;自动= webp&amp;s=f29bd294ec63257ad2f7c1b3725657f53d955de2   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjpp1/r_deepseekcoder_when_the_large_language_model/</guid>
      <pubDate>Sat, 27 Jan 2024 20:09:44 GMT</pubDate>
    </item>
    <item>
      <title>[D]“特征稀释”是深度神经网络中公认的现象以及如何应对它</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/</link>
      <description><![CDATA[我一直在努力应对与数据集成和多模式神经网络相关的挑战，我希望得到您的见解。场景如下：我有一个包含多种类型特征的特征矩阵，包括 0 到 1 范围内的 5 个连续变量。此外，我将一个 1024 维的嵌入向量连接到同一个特征矩阵中，其中嵌入值为也是连续的。 我担心的是高维嵌入特征的存在是否会削弱原始 5 个连续变量的效果或重要性。这是一种公认​​的现象吗？如果是，如何解决或对抗这种潜在的稀释效应？ 我很欣赏有关此主题的相关文献的任何指导或参考。预先感谢您的专业知识！ P.S.阅读一些评论后，一些额外的背景信息：一般来说，该模型应该能够仅使用 5 个特征就能够表现良好。我已经确认了这一点。嵌入的作用是提供上下文信息，使预测从“总体良好”变为“在特定上下文中良好”。我知道这可能有点模糊，但在不深入了解我的建模任务的细节的情况下，这是我能做的最好的事情。    由   提交 /u/Primary-Wasabi292    reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/</guid>
      <pubDate>Sat, 27 Jan 2024 17:26:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我的兴趣与典型机器学习工程师的日常职责有交叉吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acasly/d_do_my_interests_intersect_with_the_day_to_day/</link>
      <description><![CDATA[这似乎是一个相当广泛的立场，所以我试图弄清楚我的热情和 ML 工程师在他们的实际工作中是否有重叠。日复一日。 我的兴趣：  性能至关重要的低级编程。用于快速操作的 GPU 和 SIMD 对 DL 有一定的热情，但不是太多，因为它对我来说显得太黑箱 我对在消费硬件 GGML 上运行模型非常着迷， LLama.cpp 对经典算法充满热情  我也擅长数学，并且希望解决一些问题。在 React 和 SaaS 上工作通常不会涉及太多这些。   由   提交 /u/ThrowayGigachad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acasly/d_do_my_interests_intersect_with_the_day_to_day/</guid>
      <pubDate>Sat, 27 Jan 2024 13:26:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>