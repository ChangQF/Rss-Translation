<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 20 Mar 2024 18:16:02 GMT</lastBuildDate>
    <item>
      <title>[D] 为标准搜索栏重用文档向量，好不好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjjojw/d_reuse_documents_vectors_for_a_standard_search/</link>
      <description><![CDATA[嗨， 我嵌入 JSON 对象，然后为虚拟助手执行 RAG 并获取 N 个最相关的对象放入其中上下文，以便法学硕士可以确定它是否相关并使用该信息。因此，即使 N top 不是完美匹配，LLM 也足够聪明，不会提及它们。 重点是，我考虑过重新使用向量进行目录搜索，因为几乎所有内容都已经设置完毕（尝试，所以也许这是一个坏主意），所以它可能会返回 XX 项。我在匹配分数（距离）上设置了一个固定阈值，以确保没有列出奇怪的东西（我在多次尝试后调整了值），但根据用户输入（有多少个单词......），阈值有时会削减项目这应该是列表的一部分，有时它会让所有不好的项目通过。 这个用例可行吗？我想知道你们中的一些人是否尝试过弹性阈值？ 假设距离有两种情况：  对于查询“A”，[0.1 , 0.2, 0.9] 作为阈值为 0.3 的距离：我得到了预期的 2 件事 对于查询“B”，[0.1, 0.28, 0.29, 0.3]，阈值仍然为 0.3 ：不幸的是，由于查询的变化，这次阈值不好，所有“0.28、0.29、0.3”都只是垃圾。  是否有任何已知的技术可以说“if”存在显着差距”在距离之间，阈值可能应该固定在那里？ 抱歉，如果不清楚。无论如何，我仍然可以对原始文档内容使用基本的数据库搜索（但认为它比数据库向量更强大......）。 谢谢，   由   提交/u/sneko7  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjjojw/d_reuse_documents_vectors_for_a_standard_search/</guid>
      <pubDate>Wed, 20 Mar 2024 17:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何思考像 BERT 和 Transformers 这样的 AI 突破性想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjjdbz/d_how_to_think_of_groundbreaking_ideas_in_ai_like/</link>
      <description><![CDATA[大家好！我是一名研究生人工智能研究员，我真的很钦佩我们领域的伟大想法，比如 BERT 如何使用掩码或 Transformer 如何工作。这些想法极大地改变了我们用人工智能可以做什么。 我很好奇，人们是如何想出这么大的想法的？是通过解决很多小问题，还是这些想法只是你突然想到的？您如何研究这些想法，使其取得重大突破？ 我非常感谢您提出并致力于人工智能领域重大想法的任何故事或技巧。感谢您分享您的想法！   由   提交 /u/Few-Pomegranate4369    reddit.com/r/MachineLearning/comments/1bjjdbz/d_how_to_think_of_groundwriting_ideas_in_ai_like/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjjdbz/d_how_to_think_of_groundbreaking_ideas_in_ai_like/</guid>
      <pubDate>Wed, 20 Mar 2024 17:39:19 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 模型能让人像 Snapchat 一样快速变老？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjj80n/research_model_to_make_someone_older_as_fast_as/</link>
      <description><![CDATA[大家好，我想知道您是否知道任何具有 API 的模型或服务，以便我可以发送图像并快速使人变老   由   提交/u/dbooh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjj80n/research_model_to_make_someone_older_as_fast_as/</guid>
      <pubDate>Wed, 20 Mar 2024 17:33:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia 最新的 Blackwell GPU 将减少多少训练和推理时间/价格？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</link>
      <description><![CDATA[显然训练 Llama-2 花费了约 500 万美元。您认为这些新 GPU 会降低类似模型的训练/推理成本多少？我很好奇，因为我很可能很快就会购买苹果产品。   由   提交 /u/dittospin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</guid>
      <pubDate>Wed, 20 Mar 2024 16:34:13 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 不到 300 行的 Python + pytorch 从头开始​​稀疏混合专家语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</link>
      <description><![CDATA[大家好，我实现了专家语言模型的稀疏混合（基本上是 Mixtral、Grok-1 和据称的 GPT-4 中使用的微型版本） ）在纯 pytorch 中从头开始，并用小莎士比亚对其进行训练。这主要基于 Andrej Karpathy 的 makemore（仅自回归字符级解码器变压器模型）。我的目标是让它成为一个可破解的实现，人们可以用它来理解它是如何真正工作和改进的。我预计全年会出现越来越多的此类模型。 存储库位于：https://github。 com/AviSoori1x/makeMoE 我几个月前创建了这个并在 Localllama 上分享，但我想我也应该在这里分享，因为我做了一些更新，例如添加专家能力和整合整个实现少于 300 行可读的 python + pytorch。逐步完成此操作的博客位于：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch 。希望这对您有所帮助！   由   提交/u/avi1x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</guid>
      <pubDate>Wed, 20 Mar 2024 15:20:46 GMT</pubDate>
    </item>
    <item>
      <title>[D]寻找LLM API（openrouter、openai et simila）的GUI，带有插件和RAG支持。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/</link>
      <description><![CDATA[大家好， 我正在寻找一个支持 OpenAI 的 ChatGPT API（或像 OpenRouter 那样兼容）的用户友好型 GUI并允许插件和 RAG。我使用开源模型（mixtral、Hermes 34b）和私有模型，例如 claude-opus 和 perplexity-sonnet 这里有人有一些建议吗？  提前感谢您的帮助！ 编辑：我想知道是否存在任何平台可以提供修改聊天历史记录或基于聊天事件运行代码的功能。   由   提交 /u/Distinct-Target7503   /u/Distinct-Target7503 reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/</guid>
      <pubDate>Wed, 20 Mar 2024 13:54:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为 ICML 2024 提交了 0 条评论 - 还有其他人面临这个问题吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjdyax/d_0_reviews_submitted_for_icml_2024_someone_else/</link>
      <description><![CDATA[大家好，我向 ICML 2024 投了一篇论文，作者反驳阶段应该明天开始，但是到今天好像还没有审稿人提交审稿然而。有人面临类似的问题吗？如果没人评论怎么办？   由   提交 /u/Tigmib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjdyax/d_0_reviews_submitted_for_icml_2024_someone_else/</guid>
      <pubDate>Wed, 20 Mar 2024 13:52:06 GMT</pubDate>
    </item>
    <item>
      <title>微调多个LLM [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjdfgb/fine_tune_multiple_llm_d/</link>
      <description><![CDATA[是否有任何框架可以微调不同的 llm （如 llama2、mxtral、titan）并获取测试数据的输出。前提是我有训练和测试数据。  基本上是一些可以帮助我快速微调多个模型的框架   由   提交/u/IntentionNo5258  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjdfgb/fine_tune_multiple_llm_d/</guid>
      <pubDate>Wed, 20 Mar 2024 13:27:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种矢量化器最适合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjc1kr/d_which_vectorizer_fits_the_best/</link>
      <description><![CDATA[我有一个荷兰语文本数据集。我试图使用这些元素的描述将文本与分类元素对齐。我想利用余弦相似度。在这种情况下，哪种模型是嵌入文本的最佳模型？我正在考虑来自 SentenceTransformers 的 paraphrase-multilingual-mpnet-base-v2它是多语言的，其性能令人印象深刻。 此外，我正在处理将超过输入中标记的最大数量的文本，在这种情况下避免截断的最佳实践是什么？    由   提交/u/purpleoptimum  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjc1kr/d_which_vectorizer_fits_the_best/</guid>
      <pubDate>Wed, 20 Mar 2024 12:17:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么学术论文的可读性一直不好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</link>
      <description><![CDATA[  其中一些期望是不可避免的。论文绝对必须假设该领域特有的广泛背景知识和词汇——包括每篇论文对该领域的基本介绍都是多余的。有时论文假设了太多背景，但作为作者或读者很难判断，尤其是因为人们知道的事情会随着时间和不同背景而变化。 参考 在过去的3年里，为了拓宽我对ML领域的理解，我读了很多论文，但没有考虑过专注于某个领域特定领域的特定问题。   这使我能够理解学术写作的结构、机器学习的基本概念以及与以前的知识相比相对宝贵的见解。过去，但当我遇到新论文时，有些论文仍然很冗长，由于学术论文性质的可读性而让我烦恼。 这强烈要求我浏览一些内容，这对纯粹的学术论文是有害的。与非学术书籍相比，专注于阅读文档（我并不认为所有书籍都像个人畅销书一样出色，但我想强调学术和非学术写作之间的结构差异。） 问题是，为什么这个约定不能顺利地改变到更好的方向，对现有的和新的研究人员有帮助？   由   提交/u/Mundane_Definition_8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</guid>
      <pubDate>Wed, 20 Mar 2024 09:04:32 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 为什么要增加 CNN 层的通道大小？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</link>
      <description><![CDATA[在我看到的大多数 CNN 架构中，他们将通道数从 1 或 3 增加到 16、32 甚至 64。难道不是减少通道数吗？维数？   由   提交 /u/Radiant_Walrus3007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</guid>
      <pubDate>Wed, 20 Mar 2024 05:53:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最近的“LLM工程师”没有NLP背景的情况常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</link>
      <description><![CDATA[过去几周，我参加了一些聚会和社交活动，在那里我遇到了很多声称他们“与法学硕士合作”的人。我个人对它们没有太多经验，并且对更“经典”的领域进行了研究。 NLP（ELMo 和 BERT 在我做研究时是重大公告），现在主要作为一名工程师在业界工作。 我经常注意到，当我尝试谈论 LLM 研究模式或应用程序和那些我称之为经典方法的人通常似乎不知道我在说什么。 我不是在谈论研究人员，显然如果你正在与法学硕士进行实际研究，我假设您已经在该领域工作了一段时间。如今，LLM 和 NLP 似乎被分开对待。好奇其他人的想法。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</guid>
      <pubDate>Wed, 20 Mar 2024 00:59:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 在每层使用相同维度的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</link>
      <description><![CDATA[我的直觉是，随着我们在层中移动，令牌会逐渐丰富，但这意味着我们需要在每个令牌中存储更少的信息前面的层比后面的层要多。 从（相对）低维嵌入开始，然后将它们投影或扩展到更高的维度，直到它们达到最终大小，这不是有意义吗？    由   提交/u/timtom85  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</guid>
      <pubDate>Tue, 19 Mar 2024 19:35:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我如何在 Google Gemma 6T 代币模型中发现 8 个错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</guid>
      <pubDate>Tue, 19 Mar 2024 17:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>