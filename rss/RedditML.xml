<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Thu, 18 Jul 2024 21:13:51 GMT</lastBuildDate>
    <item>
      <title>[D] 生成高维（>100k 列）、稀疏合成数据的策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6k3m9/d_strategies_for_generating_highdimensional_100k/</link>
      <description><![CDATA[我正在尝试实现一篇论文，该论文提出了对估计算法的修改，以便挖掘稀疏、高维数据集。 这篇论文很旧，我无法找到作者使用的所提到的真实世界数据集。他们确实使用某些方法生成了合成数据集，但我确信在过去的 20 年里，高维合成数据生成的最新技术已经得到了改善。 我研究过 scikit-learn 的 `make_classification()` 方法和其他一些深度学习方法，但这些方法似乎都难以生成高维数据。 是否有可以以内存高效的方式生成稀疏、高维合成数据集的方法？ 我意识到这是一个相当模糊的问题，很乐意澄清我的用例。我甚至愿意将我的问题修改为可能更实用的内容。这是我第一次尝试实现这样的事情，所以我不知道会发生什么。    提交人    /u/SnooApples8349   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6k3m9/d_strategies_for_generating_highdimensional_100k/</guid>
      <pubDate>Thu, 18 Jul 2024 19:39:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 预测性维护的机器学习——最佳实践？过时的方法？该做什么，不该做什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6ix0q/d_ml_for_predictive_maintenance_best_practices/</link>
      <description><![CDATA[我在很多搜索中都看到了“相似性学习”，但我想我会在这里问-- 基本上，是否有人使用传感器数据来完成预测设备故障的任务。寻求以下方面的建议：  异常检测和故障预测的最佳模型 处理来自多个传感器的时间序列数据 估计剩余使用寿命 结合不同 ML 技术的有效方法  有类似项目的经验吗？什么方法有效？    提交人    /u/LyPreto   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6ix0q/d_ml_for_predictive_maintenance_best_practices/</guid>
      <pubDate>Thu, 18 Jul 2024 18:51:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] NER 的最佳方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6i9dh/d_best_approach_to_ner/</link>
      <description><![CDATA[我认为自己在这个领域有点菜鸟，所以我希望从这篇文章中学习和理解得更好。  我编写了一个遍历文件系统并将文件传递给 Apache tika 的 Python 应用程序，Apache tika 将其支持的扩展中的内容传递给最终会找到 NER 的函数，该函数基本上用于识别和分类文件内容。 我使用了 spacey，但发现其标记实体输出的准确性不如我希望的那样准确，我理解可以自定义训练模型以提高准确性，但如果该工具要在不同的文件系统和不同的结构化用户数据上使用它，则可能意味着重新训练模型以适应每个用例。  然后，我尝试使用来自 ollama 的 LLM 和一个 Python 库向 ollama 服务器发出请求，我尝试了几个模型，mistral 和 orca-mini 和设计一个提示以在传递的内容中查找特定的 NER，并输出找到的实体的响应，这样我就可以得到与 spacy 类似的结果。  我发现准确度要好得多，但完成所需的时间令人震惊。 例如：我使用 Faker 在 pdf、CSV、RTF、DOCX 等中创建随机数据……用于测试目的，spacy 能够在几秒钟内处理 16 个 20 KB 的文件，上述 LLM 方法使用 mistral 花费 26 分钟，在 orca mini 上花费 15 分钟。  我理解 LLM 模型要大得多，需要更多的计算，而运行 spacy 和 NER 任务可能超出了它们的能力，但我正在努力寻找平衡。一种具有实际性能输出的通用 NER 方法。 还有其他项目或经过训练的模型可能适合这项任务吗？还是我做错了？    提交人    /u/Hungry-Jackfruit-265   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6i9dh/d_best_approach_to_ner/</guid>
      <pubDate>Thu, 18 Jul 2024 18:25:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我正在为几个任务训练视觉模型，然后我决定通过视觉提示将基础模型“串联起来”。这是纯粹的炒作吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6h0e8/d_i_was_training_vision_models_for_several_tasks/</link>
      <description><![CDATA[(视觉)提示和基础模型真的可以用于工业级应用吗？ 我正在“连接”基础模型（用于视觉），并慢慢意识到我实际上正在做视觉提示（在它们之间）。但是，除了这个关于从系统角度进行视觉提示的相当有趣的观点外，我没有找到太多关于这方面的信息。 有人使用或遵循过这条路线吗？    提交人    /u/btcmx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6h0e8/d_i_was_training_vision_models_for_several_tasks/</guid>
      <pubDate>Thu, 18 Jul 2024 17:27:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] ML 系统设计：450 个值得学习的案例研究（Airtable 数据库）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6gdsk/p_ml_system_design_450_case_studies_to_learn_from/</link>
      <description><![CDATA[大家好！想分享来自 100 多家公司的 450 个 ML 用例的数据库链接，这些用例详细介绍了 ML 和 LLM 系统设计。您可以按行业或 ML 用例进行筛选。 如果这里有人着手设计 ML 系统，我希望你会发现它很有用！ 数据库链接：https://www.evidentlyai.com/ml-system-design  免责声明：我是 Evidently 背后的团队成员，这是一个开源 ML 和 LLM 可观察性框架。我们整理了这个数据库。    提交人    /u/dmalyugina   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6gdsk/p_ml_system_design_450_case_studies_to_learn_from/</guid>
      <pubDate>Thu, 18 Jul 2024 17:01:17 GMT</pubDate>
    </item>
    <item>
      <title>[N] Fish Speech 1.3 更新：增强稳定性、情感和语音克隆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6g122/n_fish_speech_13_update_enhanced_stability/</link>
      <description><![CDATA[我们很高兴地宣布，Fish Speech 1.3 现在提供了增强的稳定性和情感，并且只需 10 秒 的音频提示即可克隆任何人的声音！作为开源社区的坚定倡导者，我们今天开源了 Fish Speech 1.2 SFT，并引入了自动重新排名系统。敬请期待，因为我们很快就会开源 Fish Speech 1.3！我们期待收到您的反馈。 Playground（DEMO）：http://fish.audio GitHub：fishaudio/fish-speech    提交人    /u/lengyue233   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6g122/n_fish_speech_13_update_enhanced_stability/</guid>
      <pubDate>Thu, 18 Jul 2024 16:46:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 训练 LLM 引用预训练数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e6fxgj/r_training_llms_to_cite_the_pretraining_data/</link>
      <description><![CDATA[我们的工作被 COLM 接受，并认为值得在此分享： &quot;源感知训练实现语言模型中的知识归因&quot; TL;DR: 通常，LLM 在训练期间会学习很多东西，但不记得从哪里学到的。本文是关于教 LLM 从预训练数据中引用他们的知识来源。这可以使模型更透明、更容易理解和更可靠。我们提出了一个两步过程：1) 使用文档 ID 注入进行预训练和 2) 指令调整。第一阶段教模型将知识片段链接到特定的预训练文档。第二阶段教模型如何在生成答案时引用这些文档。 🔗 论文：https://arxiv.org/abs/2404.01019 代码：https://github.com/mukhal/intrinsic-source-citation    提交人    /u/moyle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e6fxgj/r_training_llms_to_cite_the_pretraining_data/</guid>
      <pubDate>Thu, 18 Jul 2024 16:43:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代理工作流程的人为干预</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e68up3/r_human_intervention_for_agentic_workflow/</link>
      <description><![CDATA[我最近探索了 OpenAGI 框架的一个新增功能：任务规划期间的人为干预。此功能允许自主 AI 代理从人类那里征求澄清或附加信息，从而提高任务准确性和效率。 我已将其集成到我的工作流程中，用于反馈和报告生成，结果令人印象深刻。与 crewAI 不同，任务规划准确性有时会下降，OpenAGI 的方法被证明更可靠。 我很想听听你的经验。 在 GitHub 上探索它：GitHub - github.com/aiplanethub/openagi    提交人    /u/trj_flash75   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e68up3/r_human_intervention_for_agentic_workflow/</guid>
      <pubDate>Thu, 18 Jul 2024 11:14:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 3090 上进行训练时出现异常行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e650gq/d_strange_behaviour_with_training_on_3090/</link>
      <description><![CDATA[我正在训练源分离模型，更具体地说，我正在微调一个模型。来自此 repo  问题是这样的：我尝试微调的原始检查点的 SDR 值约为 11。当我开始微调时（尽管批次大小只有 1，而原始批次大小为 16），第一个 epoch 完成后，我的 SDR 值为 0.0016 左右。然后下一个 epoch 大约是 3，然后下一个是 6，然后它逐渐上升到 6-7。 这不应该发生 - 从第一个 epoch 开始，SDR 就应该接近原始值（11）。 我为什么这么认为？该 repo 还允许您在没有训练的情况下验证 SDR 的检查点 - 当我再次验证 0.0016 SDR 检查点时，它告诉我 SDR 实际上大约是 11，就像它应该的那样。但在训练期间，它要小得多。 作者告诉我这可能是 Pytorch 的问题，但即使在最新版本上，问题仍然存在。不管怎样，我已经在云 A6000/A100/H100 上进行了完全相同的训练，问题不存在。从第一个 epoch 开始，SDR 值就完全正常了。 这只是 3090 不够用还是某处有错误？所有其他损失值也在正常范围内。    提交人    /u/lucellent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e650gq/d_strange_behaviour_with_training_on_3090/</guid>
      <pubDate>Thu, 18 Jul 2024 06:53:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] Spider2-V：多模式代理距离实现数据科学和工程工作流程自动化还有多远？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e5qt1r/r_spider2v_how_far_are_multimodal_agents_from/</link>
      <description><![CDATA[多模式 AI 代理的新基准，专注于现实世界的 Dara 工程任务。 项目页面：链接，论文：链接，代码：链接。  TLDR：自主 LLM 代理无法取代数据工程师……目前如此。但至少我们可以跟踪进度 🫡 概述： 随着 AI 技术变得越来越先进，我们需要越来越复杂的基准来评估系统的质量和衡量进度。出现了一个独特的基准分支，专注于使用专业工具/应用程序和网站（参见WorkArena、WebArena、OSWorld）。 在 Spider2-V 项目中，正在创建一个基准来评估数据工程中的 AI 代理。它包含 494 个任务，涵盖整个工作周期：  数据仓库（Snowflake、BigQuery 等工具） 数据提取（例如 Airbyte） 数据转换（例如 dbt） 数据可视化（例如 Superset、Metabase） 数据编排（例如 Airflow、Dagster）  （以及心爱的 Excel 文件，因为谁能没有它们？） 如果您有数据工程经验，您就会明白这是一个庞大的集合，尽管它没有涵盖您可能遇到的所有解决方案。 准备每个任务平均需要 4 个小时，因此它们非常原子化，不需要很长的视野思考。任务分为三个难度等级：  简单（20%，不超过 5 步即可解决） 中等（63%，6-15 步） 困难（17%，16-40 步）  所有任务均基于 DE/DS 教程，由人工标注员从网络上获取。可以说它们代表了真实的用例。简单任务示例：  将当前 Google Drive 文件夹下的数据加载到打开的 BigQuery 数据集的新表 “data1” 中  或者中等难度的任务：  从 GitHub 安装 dbt-cloud-cli，并将二进制文件解压到与 dbt 项目 “analytics” 相同的文件夹中  为了解决任务，LLM 代理可以访问 IDE 和浏览器（已设置账户）。模型使用 pyautogui 生成 Python 代码以与虚拟机的 UI 交互，然后执行代码，并逐步重复该过程。  猜猜 GPT-4 完成了多少任务？ 只有 14%！这个数字似乎很低，但可以突出显示更成功的集群——40% 的简单任务和 25% 的数据可视化任务都得到了解决。 除了专有模型外，还测试了开放模型 (LLAMA 3 70B、Mixtral 8x7B)，但由于它们不是多模态的并且不接受图像作为输入，因此仅向它们显示了屏幕的文本描述。这大大降低了它们的指标——它们只解决了一小部分任务。然而，我们热切期待 LLAMA-3 405B，据传它是多模态的，将于 7 月 23 日发布。  我非常渴望看到 GPT-5 发布时发布的基准指标——然后我们拭目以待！押注下一代模型将解决多少百分比的任务！     由    /u/stalkermustang 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e5qt1r/r_spider2v_how_far_are_multimodal_agents_from/</guid>
      <pubDate>Wed, 17 Jul 2024 19:20:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于稳定扩散中潜伏层的维度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e5qszw/d_about_the_dimensions_of_latents_in_stable/</link>
      <description><![CDATA[您好，我对这个问题已经思考了一段时间了，希望您能解答我的疑惑。 鉴于潜在（稳定）扩散中的自动编码器经过训练可以产生与输入图像感知相似的潜在数据，作者选择 4x64x64 作为潜在数据的尺寸似乎很奇怪；为什么要添加通道？ 选择 3x64x64 会更合理，因为可以说自动编码器将学习将输入图像的每个通道映射到潜在数据中的通道，从而尽可能在潜在空间中保留感知相似性。 所以我想讨论的主题是：为什么选择 4 个通道的潜在数据？    提交人    /u/HumbBest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e5qszw/d_about_the_dimensions_of_latents_in_stable/</guid>
      <pubDate>Wed, 17 Jul 2024 19:20:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] 匹配医学图像中的分割区域</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e5i610/p_matching_segment_areas_in_medical_images/</link>
      <description><![CDATA[      https://preview.redd.it/2p5lksh1z2dd1.png?width=597&amp;format=png&amp;auto=webp&amp;s=1995475c783500ab58e9564e140b8debdf7dc8f3 参考附图，我正在努力构建一个深度学习网络，该网络能够找出左图中哪个分割区域是与右侧区域 1（红色数字）匹配的身体部分。有人可以分享指向解决此挑战的地方的指针吗，或者无论如何问题的名称是什么，以便我可以搜索论文和代码？ 提前致谢，我也愿意合作，这是为了在心脏病的背景下解释人工智能。     提交人    /u/sladebrigade   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e5i610/p_matching_segment_areas_in_medical_images/</guid>
      <pubDate>Wed, 17 Jul 2024 13:26:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 《ReFT：语言模型的表征微调》作者，本周五将在 Oxen.ai 论文俱乐部发表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e5h1m8/d_author_of_reft_representation_finetuning_for/</link>
      <description><![CDATA[Arxiv 论文第一作者 Zhengxuan Wu 将与 Greg Schoeninger 一起参加本周五的 Oxen.AI 论文俱乐部，解释编辑表示如何比参数高效微调 (PEFT) 方法更好。https://lu.ma/oxen ReFT：语言模型的表示微调。 Greg，仅阅读摘要就有 3 个问题和 1 条评论。  “表示”到底是什么意思？即，本文所指的表示捕获了神经网络的哪一部分？ “任务特定干预”中的干预是什么意思？我以前没有听说过预训练或微调这个术语。 就 API 而言，本文的要点是否类似于这样：&quot;我们不会通过提高可嗅探输入和输出的保真度（深度）或广度来改进 RESTful API，而是通过直接更改所有现有输入和输出的代码核心来改进 API？&quot;  评论）摘要让这篇论文听起来很巫术。希望测试是苹果 :: 苹果。 期待您在星期五揭开神秘面纱，Greg。非常酷，论文的第一作者能加入进来，帮助解释和回答问题。 详情： https://lu.ma/oxen 7 月 19 日星期五，太平洋时间上午 10:00，东部时间下午 1:00，Zoom 上 论文：https://arxiv.org/pdf/2404.03592 感谢：感谢 Greg、u/FallMindless3563、Scott Howard u/sthoward 和 Oxen团队为我提供了一个 Easy 按钮并与社区分享您的知识，同时提供了很酷的工具来在 oxen.ai 上管理数据集。    提交人    /u/ReluOrTanh   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e5h1m8/d_author_of_reft_representation_finetuning_for/</guid>
      <pubDate>Wed, 17 Jul 2024 12:32:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 租用 GPU 的最佳地点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e562n1/d_best_places_to_rent_gpus_from/</link>
      <description><![CDATA[大家好， 我希望能够灵活地按需租用 GPU，而且当然不必支付很多费用。我一直在关注一些公司，例如 brev.Dev、runpod 和 fluidstack。我想知道你们是否使用其中任何一个或其他东西来运行工作负载     提交人    /u/OGbeeper99   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e562n1/d_best_places_to_rent_gpus_from/</guid>
      <pubDate>Wed, 17 Jul 2024 01:42:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e34cwr/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e34cwr/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jul 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>