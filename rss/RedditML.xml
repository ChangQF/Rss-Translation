<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 22 Dec 2023 15:13:13 GMT</lastBuildDate>
    <item>
      <title>[D] 使用SparkFM进行推荐系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ofzta/d_using_sparkfm_for_recommendation_system/</link>
      <description><![CDATA[我有一个使用分解机的项目（基于 lightFM&lt; /a&gt;) 用于生成产品推荐。由于我正在处理大数据，因此我使用 Spark 来进行高效的数据操作。由于 LightFM 与 Spark 完全无关，因此我必须先将 Spark 数据帧转换为 numpy，然后再将其输入 LightFM，这非常耗时。因此，我正在考虑切换到 Spark FM，但注意到 spark.ml.recommendation 不包含 FM 模型，而是 Spark 有一个 FMClassifier 和  FMRegressor 用于分类和回归任务。 我想知道是否可以使用这两个模型中的任何一个来构建推荐系统，其他人有类似的经验吗？是否值得进行切换，还是应该坚持使用 LightFM？   由   提交/u/Modruc  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ofzta/d_using_sparkfm_for_recommendation_system/</guid>
      <pubDate>Fri, 22 Dec 2023 14:20:27 GMT</pubDate>
    </item>
    <item>
      <title>目前最先进的人脸识别模型？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oevls/current_state_of_the_art_face_recognition_model_d/</link>
      <description><![CDATA[我目前正在进行一个项目，需要我在树莓派等边缘平台上实现实时人脸识别软件。我们目前正在尝试在 Windows 上测试多个人脸识别模型，以测试准确性。项目要求精度尽可能的好，可能的话100%。对于这个用例，您会推荐哪些模型，即实时性要求最低，精度最高。您还能推荐一些吗资源相同。    由   提交 /u/Shaurya_Saxena   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oevls/current_state_of_the_art_face_recognition_model_d/</guid>
      <pubDate>Fri, 22 Dec 2023 13:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformer 作为基于 NMDAR 启发的非线性的海马记忆巩固模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oellc/r_transformer_as_a_hippocampal_memory/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=vKpVJxplmB 代码：补充材料中包含的 PyTorch 实现代码。 摘要:  海马体在学习、记忆和空间表征等依赖于 NMDA 受体 (NMDAR) 的过程中发挥着关键作用。受最近将深度学习模型与海马体进行比较的研究结果的启发，我们提出了一种模仿 NMDAR 动力学的新非线性激活函数。类似 NMDAR 的非线性在变压器中将短期工作记忆转变为长期参考记忆方面具有有益的作用，从而增强了类似于哺乳动物大脑中的记忆巩固的过程。我们设计了一个评估这两种记忆功能的导航任务，并表明操纵激活功能（即模仿 NMDAR 的 Mg2+-门控）会破坏长期记忆过程。我们的实验表明，类细胞函数和参考存储器驻留在变压器的前馈网络层中，并且非线性驱动这些过程。我们讨论了类似 NMDAR 的非线性在变压器架构和海马空间表示之间建立这种惊人相似性方面的作用。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oellc/r_transformer_as_a_hippocampal_memory/</guid>
      <pubDate>Fri, 22 Dec 2023 13:11:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] Perseus：消除大型模型训练中的能量膨胀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.06902 项目页面：https:// /ml.energy/zeus/perseus/ 积分： https://ml.energy/zeus/perseus/integrating/ 摘要：  在大量数据上训练大型人工智能模型GPU 消耗大量能源。我们观察到，并非训练期间消耗的所有能量都会直接贡献于端到端训练吞吐量，并且可以在不减慢训练速度的情况下消除很大一部分能量，我们将其称为能量膨胀。在这项工作中，我们确定了大型模型训练中两个独立的能量膨胀来源：内在和外在，并提出Perseus，一个统一的优化框架减轻两者。珀尔修斯获得“迭代时间-能量”任何大型模型训练作业的帕累托前沿都使用基于迭代图切割的算法，并在一段时间内安排其前向和后向计算的能耗，以消除内在和外在的能量膨胀。对 GPT-3 和 Bloom 等大型模型的评估表明，Perseus 将大型模型训练的能耗降低了高达 30%，实现了以前无法实现的节省。  &lt;!-- SC_ON - -&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</guid>
      <pubDate>Fri, 22 Dec 2023 12:59:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士如何知道每个子标记中存在的字符？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</link>
      <description><![CDATA[例如，当我向 chatGPT 询问该句子中的字母数量时： 但是，我想知道这些是如何实现的LLM 可以执行计数，因为知道分词器正在执行子标记级别而不是字符级别（这意味着每个子标记“可能”不知道它具有的字符”）。）。 答案是 15这是正确的 ​ 但是，我想知道这些 LLM 如何执行计数，因为知道标记器正在执行子标记级别而不是字符级别（意味着每个子标记都是“也许”不知道它有哪些字符”）。 ​ ​ &lt;!-- SC_ON - -&gt;  由   提交/u/kekkimo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</guid>
      <pubDate>Fri, 22 Dec 2023 12:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] 苹果研究人员推出 DeepPCR：一种新颖的机器学习算法，可并行化典型的顺序操作，以加速神经网络的推理和训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</link>
      <description><![CDATA[”论文通过各种应用展示了 DeepPCR 的有效性。它在多层感知器中实现了前向传递高达 30 倍的加速和后向传递高达 200 倍的加速。此外，该算法还应用于深度 ResNet 架构的并行训练和扩散模型的生成，从而使训练速度提高 7 倍，生成速度提高 11 倍。” 论文：https://arxiv.org/pdf/2309.16318.pdf 研究页面：https://machinelearning.apple.com/research/deepcr https://twitter.com/i/status/1735876638947348656   由   提交/u/paryska99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</guid>
      <pubDate>Fri, 22 Dec 2023 12:19:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 交互式语音机器人？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odkqm/d_interactive_voice_robot/</link>
      <description><![CDATA[我们可以使用 openAI API 或任何其他平台 API 构建交互式语音机器人玩具吗？或者我们如何以其他可能的方式构建它？   由   提交/u/richierich1008   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odkqm/d_interactive_voice_robot/</guid>
      <pubDate>Fri, 22 Dec 2023 12:14:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在第一个 epoch 中训练损失增加</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odi68/d_increase_in_training_loss_while_in_the_first/</link>
      <description><![CDATA[      我正在训练一个使用感知器重采样器的字幕模型，就像 Flamingo 中的那样，唯一的区别是我不使用感知器重采样器不要将学习到的查询参数连接到键和查询以进行交叉关注。  一切都很顺利，直到我达到了数据集的大约 50%（第一个时期），这一切发生了。我还在测试其他架构，例如根本不使用感知器重采样器，而只使用从视觉编码器获得的所有功能，并且我在大约 50% 的数据集上也得到了类似的结果，但不是在完全相同的迭代中（我使用的是种子，所以如果是数据问题，它应该完全相同，对吧？）。其他架构已经达到 8000 次迭代标记，并且训练损失没有像这样的任何增加......为什么会发生这种情况？ 训练损失约为 50%我的第一个纪元    由   提交 /u/AromaticCantaloupe19   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odi68/d_increase_in_training_loss_while_in_the_first/</guid>
      <pubDate>Fri, 22 Dec 2023 12:09:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ocwib/d_advice_for_learning/</link>
      <description><![CDATA[大家好，我对 ML 世界还很陌生。开始到现在已经有大约5个月了，一开始我感觉自己在进步，但现在我感到有点失落。我也觉得自己学得很慢。就像有时我被一个问题困了几个小时才意识到解决方案很简单。我不知道是否每个人都是这样，但我对自己取得的进步并不满意。 5个月内我应该进步多少？  有很多我不知道的事情，但我什至不知道我不知道什么 因此，您自己的旅程的 ML/ 路线图将不胜感激。 .  目前正在学习情感分析，重点研究NLP atm   由   提交 /u/Ok-Neighborhood-7690   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ocwib/d_advice_for_learning/</guid>
      <pubDate>Fri, 22 Dec 2023 11:32:03 GMT</pubDate>
    </item>
    <item>
      <title>[D]什么时候应该和不应该平衡不平衡的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</link>
      <description><![CDATA[我似乎总是在这个问题上得到相互矛盾的答案，想法？    ;由   提交 /u/Throwawayforgainz99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</guid>
      <pubDate>Fri, 22 Dec 2023 11:26:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我尝试教 Mistral 7B 一门新语言（巽他语），它成功了！ （有点）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</guid>
      <pubDate>Fri, 22 Dec 2023 10:54:20 GMT</pubDate>
    </item>
    <item>
      <title>[D]如何训练具有无限负类示例的二元分类器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ob5az/dhow_to_train_a_binary_classifier_with_infinite/</link>
      <description><![CDATA[假设我想训练一个仅猫的分类器，当给定猫的图像时输出高概率，而当给定其他任何东西时输出低或零概率。现在我可以收集几百张猫图像作为正类，但我的负类样本实际上是无限的。我该如何训练这样的分类器？这种培训范式的正式名称是什么？请建议我应该探索哪些方向。   由   提交/u/dopekid22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ob5az/dhow_to_train_a_binary_classifier_with_infinite/</guid>
      <pubDate>Fri, 22 Dec 2023 09:36:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有哪些便宜又好的设备可以用于 CUDA 训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</link>
      <description><![CDATA[在过去的四年里，我一直在大学服务器上训练我的所有模型，但是，因为我即将毕业 - 这根本行不通不再… 我一直在使用 Mac，但是想到毕业后无法运行 CUDA 就很难受了，哈哈 - 考虑设置尽可能便宜的计算机来训练 AI，但是，我当涉及到 Windows / Linux 计算机时，不知道从哪里开始 - 并且也不太了解所有 GPU 之间的区别 你们知道有什么便宜但性能好的 AI 计算机吗 -如果他们可以购买二手的，那就更好了！   由   提交/u/Middle_Stomach_6681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</guid>
      <pubDate>Fri, 22 Dec 2023 07:22:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 深入探讨 MMLU（“你比 LLM 更聪明吗？”）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ntia7/d_deep_dive_into_the_mmlu_are_you_smarter_than_an/</link>
      <description><![CDATA[在 MMLU 周围的所有喧嚣之后（例如 我的文章）我想我会制作一个界面来看看人类如何做，甚至是中间的LLM。它的名字叫你比法学硕士聪明吗？ &lt; p&gt;它向您提出 MMLU 的随机问题，并将您的答案与 LLM 的答案进行比较。单击“这是什么”按钮位于底部，了解有关其工作原理的更多详细信息。 感谢反馈！   由   提交/u/brokensegue  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ntia7/d_deep_dive_into_the_mmlu_are_you_smarter_than_an/</guid>
      <pubDate>Thu, 21 Dec 2023 18:21:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>