<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 01 Dec 2023 15:14:06 GMT</lastBuildDate>
    <item>
      <title>[D] 尝试改善扫描照片的最佳项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188dteo/d_best_projects_to_try_for_improving_scanned/</link>
      <description><![CDATA[我有大约 1000 张扫描照片。大部分年龄在20-30岁左右，所以状况相当好。一对夫妇年龄要大得多。 我想尽可能地进行批处理/自动化，然后手动检查原始与已处理的数据以挑选出我需要手动处理的内容。 &lt; p&gt;这些是我想要解决的问题（按集合中的流行顺序排列）：  相纸纹理（即哑光）使其看起来像是扫描件而不是照片&lt; /li&gt; 调整亮度/对比度/颜色等 照片模糊或失焦 照片损坏（撕裂、起皱等） &lt; li&gt;照片真的很旧（有污渍）  我想尽可能地进行批处理。不太确定如何执行#1，#2 可以通过很多软件通过其“自动调整”功能来完成。设置。我在想也许是伊尔凡维尤？或者Photoshop？ 不确定还有哪些其他工具。 有一些机器学习或神经网络模型可以修复#3-5，也许还有#1-2。然而，很多人都专注于脸部，有些走得太远，基本上取代了脸部，所以看起来超级假。只要浏览一下 gitlab 就有很多选择。我测试过的座右铭是 https://github.com/microsoft/Bringing-Old -Photos-Back-to-Life 然后就是所有随机的“AI”网站。理想情况下，我希望将超级老照片恢复到第一次拍摄时的样子。我想将不太旧的照片（即 90 年代的照片）恢复到今天拍摄的样子。至少这是目标。 有比较的地方吗？ 此外，同样，我很好奇是否有任何工具可以很好地用于手机拍摄的一般照片“优化”他们。例如，我通常通过噪声过滤器来运行它们，以使它们看起来更清晰。 ​ 总而言之，我知道有很多项目在那里。我很好奇是否有比较，这样我就可以了解其中的差异或当前最好的，而不必尝试设置每个来进行比较。 我将不胜感激建议。   由   提交/u/eng33  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188dteo/d_best_projects_to_try_for_improving_scanned/</guid>
      <pubDate>Fri, 01 Dec 2023 14:54:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一些作者是否认真地添加了比需要的更多的数学知识，以使论文“看起来”更具开创性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</link>
      <description><![CDATA[我最近注意到一种趋势，即作者在某些情况下添加了超出所需的形式主义（例如，图表/图像就可以很好地完成工作）。  这是否是为了使论文看起来更好而添加了超出所需的数学内容，或者可能只是受到出版商的限制（无论论文必须坚持什么格式才能发表）？ &gt;   由   提交 /u/Inquation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</guid>
      <pubDate>Fri, 01 Dec 2023 14:29:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 选择非负矩阵分解的分量数量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</link>
      <description><![CDATA[我有几个无向加权网络的估计，即加权邻接矩阵。我使用 NMF 来识别子网/组件。我知道有多种方法可以得出要使用的组件数量，从简单的方法（例如膝图）到更复杂的方法（例如 使用随机删除的数据点进行交叉验证。我已经对我的网络集应用了多种方法，他们建议使用 5-8 个组件。我对两个子网应该是什么样子有一个先验的想法。使用 5-8 个组件似乎可以创建预期两个子网的子网。或者其中两个组件代表预期的子网络，而其余组件看起来与这些组件非常相似。如果我将组件的数量减少到两个，那么我就获得了两个预期的子网。  简单地使用两个组件是否合法，因为它们符合我的期望？ 是有合并组件的方法，例如将子子网合并到预期的子网中？例如。找到与预期网络的相关性最大化的子网络的加性组合？ 是否有方法强制执行“更大”的网络？网络？  我很欣赏你对此的想法！   由   提交 /u/dizzledk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</guid>
      <pubDate>Fri, 01 Dec 2023 14:20:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据科学书籍推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188bind/d_data_science_book_recommendations/</link>
      <description><![CDATA[我拥有计算机科学学位以及一些数学和理论统计知识，成为一名专业编码员已超过 10 年。我并没有真正使用太多统计知识，所以我只是粗略地了解一下表面。 现在，我希望深入研究机器学习。我并不是在寻找一条容易的道路，我的主要目的不是快速找到一份工作，我这样做只是为了个人成长。在深入研究机器学习之前，我想加强我在数据科学方面的基础，甚至可能专注于数据挖掘。 有这方面的书籍推荐吗？我不需要编程书籍，因为我已经了解一些编程语言，包括 Python。我看到了 Pang-Ning Tan 等人的“数据挖掘导论”。这是一个好的起点吗？我发现的另一本书是 Gareth James 的“统计学习简介”。   由   提交 /u/lp_kalubec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188bind/d_data_science_book_recommendations/</guid>
      <pubDate>Fri, 01 Dec 2023 13:11:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 pytorch 中冻结 resnet18 层中的特定参数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188acll/d_freeze_particular_parameters_in_a_layer_of_a/</link>
      <description><![CDATA[嘿朋友们！在冻结神经网络的层中的特定参数之前，是否有人编写过任何代码？  ​ 我的意思并不是冻结神经网络的特定层，我可以成功做到这一点（例如：  for param in resnet18.fc.parameters(): param.requires_grad = True  ​ 我的意思是冻结特定参数内 一层 - 通过应用某种遮罩或其他方式。 ​ 非常感谢！:)   由   提交 /u/Cultural-Average3959    reddit.com/r/MachineLearning/comments/188acll/d_freeze_pspecial_parameters_in_a_layer_of_a/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188acll/d_freeze_particular_parameters_in_a_layer_of_a/</guid>
      <pubDate>Fri, 01 Dec 2023 12:08:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] NLP项目-需要体育比赛成绩单</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1880a0p/p_nlp_project_in_need_of_sports_games_transcripts/</link>
      <description><![CDATA[大家好 - 我是一名大学生，希望使用体育比赛的实况转播和彩色解说来构建用于自然语言处理的文字语言模型- 有谁知道电视/广播中是否存在此类内容的文字记录。谢谢！   由   提交 /u/FantasticPotato2470   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1880a0p/p_nlp_project_in_need_of_sports_games_transcripts/</guid>
      <pubDate>Fri, 01 Dec 2023 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 修改后的 Tsetlin Machine 在 7950X3D 上的实现性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187vrpg/p_modified_tsetlin_machine_implementation/</link>
      <description><![CDATA[      嘿。 我在过去 1.5 年里一直致力于我的宠物项目，取得了一些令人印象深刻的成果。 在 Ryzen 7950X3D CPU 上使用没有卷积的一个平坦层的 MNIST 推理性能：每秒 4600 万次预测，吞吐量：25 GB/s，准确度：98.05%。 AGI 实现了。老实说，ACI（人工智能集体智能）。 在 MNIST 性能上改进的 Tsetlin 机器   由   提交 /u/ArtemHnilov   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187vrpg/p_modified_tsetlin_machine_implementation/</guid>
      <pubDate>Thu, 30 Nov 2023 22:54:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找类似的教育内容</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187u8fa/d_looking_for_similar_educational_content_like/</link>
      <description><![CDATA[https://youtu.be/p1bfK8ZJgkE?si =439ofCFcaFX1EBie（Krish Naik - 带部署的端到端深度学习项目） 我是一名“全栈”人员具有几年经验的数据科学家，我真的很喜欢这种类型的内容，它们涵盖了具有高水平代码和实现质量的端到端项目（即模块化和可重用代码、装饰器、数据类、pydantic、mlops、 dvc、ci/cd、部署等） 是否有任何您会推荐的类似课程、视频、创作者、网站或类似内容，涵盖具有同等技能水平的类似内容？我在目前的工作中已经实现了这样的端到端项目，但我希望了解更多并在涉及 python 开发、机器学习应用程序等方面遵循最佳实践。    由   提交 /u/Fendrbud   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187u8fa/d_looking_for_similar_educational_content_like/</guid>
      <pubDate>Thu, 30 Nov 2023 21:50:15 GMT</pubDate>
    </item>
    <item>
      <title>YUAN-2.0-102B，带代码和重量。 ChatGPT 和 GPT-4 在各种基准上的得分 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187spj3/yuan20102b_with_code_and_weights_scores_between/</link>
      <description><![CDATA[       由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187spj3/yuan20102b_with_code_and_weights_scores_between/</guid>
      <pubDate>Thu, 30 Nov 2023 20:47:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[Rich 是强化学习书籍&lt;的作者&lt; /a&gt;，最近，他与一些同事创立了 OpenMind 研究所。 ​ 面试时间为 1 周。我有 RL 背景，并且已经对问题和主题有了一些想法，但我也想在艾伯塔省 RL 泡沫之外寻找问题。技术问题是最好的，尽管我对任何事情都持开放态度。谢谢！ ​ 采访发布几周后，我将在此帖子中发布更新。   由   提交/u/ejmejm1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 17:00:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我们将 Netron 集成到 GitHub 中以可视化模型架构</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187l3jp/p_we_integrated_netron_into_github_for/</link>
      <description><![CDATA[我们喜欢 Lutz Roeder 创建的 Netron 库：https:/ /github.com/lutzroeder/netron 我们想要探索能够为 GitHub 中托管的 ML 模型渲染 Netron 可视化的感觉，因此我们构建了 Netron 集成：https://about.xethub.com/blog/visualizing-ml-models-github-netron  Netron 专注于一次查看 1 个模型文件，但我们还在拉取请求中合并了前后模型可视化： https://assets-global.website-files.com/6474aea6101c81b742144dd2/65689c07b7f2b060a92 5097c_github_pr2.png   由   提交 /u/semicausal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187l3jp/p_we_integrated_netron_into_github_for/</guid>
      <pubDate>Thu, 30 Nov 2023 15:25:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAMAS 2024 评论已出炉！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187ilt5/d_aamas_2024_reviews_are_out/</link>
      <description><![CDATA[我没有看到讨论帖子，所以我想我应该制作这个。   由   提交 /u/LessPoliticalAccount   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187ilt5/d_aamas_2024_reviews_are_out/</guid>
      <pubDate>Thu, 30 Nov 2023 13:32:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于序列建模的分层门控循环神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187iaw6/r_hierarchically_gated_recurrent_neural_network/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.04823 代码：https://github.com/OpenNLPLab/HGRN 模型：https://huggingface.co/OpenNLPLab 摘要：  变形金刚已经超越RNN 因其并行训练和长期依赖建模的卓越能力而广受欢迎。最近，人们对使用线性 RNN 进行高效序列建模重新产生了兴趣。这些线性 RNN 通常在线性递归层的输出中采用门控机制，而忽略了在递归中使用遗忘门的重要性。在本文中，我们提出了一种门控线性 RNN 模型，称为分层门控循环神经网络（HGRN），其中包括由可学习值下限的遗忘门。当向上移动层时，下界单调增加。这允许上层对长期依赖关系进行建模，而下层对更多本地的短期依赖关系进行建模。语言建模、图像分类和远程竞技场基准的实验展示了我们提出的模型的效率和有效性。源代码可在 此 https URL 处获取。  https://preview.redd.it/thph9bpmjh3c1.png?width=965&amp;format=png&amp; ;auto=webp&amp;s=8e4871cd280ef7e5b771b463435d47da11dca52d   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187iaw6/r_hierarchically_gated_recurrent_neural_network/</guid>
      <pubDate>Thu, 30 Nov 2023 13:16:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过深度学习发现了数百万种新材料</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</link>
      <description><![CDATA[帖子：https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/ 论文：https://www.nature.com/articles/s41586-023-06735-9 摘要： 新型功能材料实现了从清洁能源到信息处理的技术应用的根本性突破。从微芯片到电池和光伏发电，无机晶体的发现一直受到昂贵的试错方法的瓶颈。与此同时，随着数据和计算的增加，语言、视觉和生物学的深度学习模型展示了新兴的预测能力。在这里，我们展示了大规模训练的图网络可以达到前所未有的泛化水平，从而将材料发现的效率提高一个数量级。在持续研究中发现的 48,000 个稳定晶体的基础上，效率的提高使得能够在当前凸包下方发现 220 万个结构，其中许多结构逃过了人类之前的化学直觉。我们的工作代表了人类已知的稳定材料的数量级扩展。最终凸包上的稳定发现将可用于筛选技术应用，正如我们对层状材料和固体电解质候选物的演示一样。在稳定结构中，有 736 个已通过独立实验实现。数以亿计的第一原理计算的规模和多样性也解锁了下游应用的建模能力，特别是导致高度准确和强大的学习原子间势，可用于凝聚相分子动力学模拟和高保真零-离子电导率的射击预测。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</guid>
      <pubDate>Wed, 29 Nov 2023 18:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>