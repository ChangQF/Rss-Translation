<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 09 Mar 2024 18:14:57 GMT</lastBuildDate>
    <item>
      <title>[D] 使用 GAN/Diffusion 模拟整形手术结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1banrhx/d_simulating_plastic_surgery_outcomes_using/</link>
      <description><![CDATA[我想训练一个模型来帮助用户展示整形手术的结果。  我想在这个 Reddit 上获得专家和经验丰富的人的意见和指导，了解针对此类任务开发人工智能的最佳方法。主义的。  我有一个大约 1500 个之前和之后的自定义数据集。图像后进行训练。我也可以投资获得更大的数据集。 我想在这个 reddit 上获得专家和经验丰富的人的意见和指导，了解针对此类任务开发人工智能的最佳方法。    由   提交/u/tjain73  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1banrhx/d_simulating_plastic_surgery_outcomes_using/</guid>
      <pubDate>Sat, 09 Mar 2024 17:58:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 语音到语音/语音传输工具问题。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ban1ex/d_voice_to_voice_voice_transfer_tools_question/</link>
      <description><![CDATA[TL;DR：获取语音并对其应用不同声音的好方法是什么？ 我的爱好项目现在要了解 ML/AI，就是尝试建立一个（天真的）漫画到动画的完全自动化管道。 我解析/理解了页面/对话框的工作（GPT4-V/ llava 是很棒的工具），我负责音频部分。 我需要能够以不同的声音读取不同的语音气泡。 同样，我制作了以下示例来自公共领域数据的声音。 此外，我已经让 bark 读取气泡中的文本。 问题是，bark 不允许我提供零的语音样本- 声音的镜头复制。它只是生成随机声音：  « Bark 尝试匹配给定预设的音调、音高、情感和韵律，但目前不支持自定义语音克隆。该模型还尝试保留音乐、环境噪音等。 » -- https://github.com/suno-ai/bark&lt; /a&gt;  所以我现在想做的是获取树皮生成的音频/语音，并改变这些样本的声音，同时保持它的文字/语气/个性。 我看了很多（已经好几个星期了），我发现唯一有效的是https://www.resemble.ai/。我可以给它发出树皮生成的音频，并让它改变声音。这样可行。但它是闭源且付费的，因此它只能作为演示，表明我想做的事情可以完成。 但我一直无法找到可以满足我需要的 GitHub 项目。 &gt; 我发现基于语音样本进行 TTS 的项目（迄今为止最好的是 alltalk），效果很好。  但这不是我需要的，我需要语音到语音，而不是文本到语音。 我在 GitHub 上尝试了六个项目，大多数我都没有尝试过。甚至无法运行并且明显被遗弃（或者太神秘/不适合公众），我能够运行的少数几个只是不能完成这项工作。 所以我正在寻找在社区中寻求有关如何实现此目标的任何想法的帮助。 你知道有什么方法可以做到这一点吗？ 我会这样做吗？都错了吗？有什么替代方法可以完成此任务？ 任何指示或想法将非常感激，这是我第一次在这个项目中真正陷入困境。 谢谢您的智慧非常超前。   由   提交/u/arthurwolf  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ban1ex/d_voice_to_voice_voice_transfer_tools_question/</guid>
      <pubDate>Sat, 09 Mar 2024 17:28:33 GMT</pubDate>
    </item>
    <item>
      <title>[P]只有聪明的法学硕士才能读懂好论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bamgbm/p_only_smart_llms_can_understand_good_essays/</link>
      <description><![CDATA[       我分析了几位法学硕士以及他们“理解”法律的程度。大学论文：对于所有法学硕士来说，糟糕的论文都很难理解。但对于聪明的法学硕士来说，好的论文是有意义的。这可能是衡量法学硕士推理能力的一种方法。  https://preview.redd .it/0o1g8dvtacnc1.png?width=2100&amp;format=png&amp;auto=webp&amp;s=0010d7fc2941281499343d2e4c3aed69708b2ddb 更多绘图、数据、代码：https://muxamilian.github.io/essay-entropy-llms/  &amp; #32；由   提交/u/muxamilian  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bamgbm/p_only_smart_llms_can_understand_good_essays/</guid>
      <pubDate>Sat, 09 Mar 2024 17:04:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 中实现多头注意力的 5 种不同方式的速度比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</guid>
      <pubDate>Sat, 09 Mar 2024 15:12:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用多少合成数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bajiz5/d_how_much_synthetic_data_to_use/</link>
      <description><![CDATA[我正在微调 LLM，使其以特定方式响应，为此我需要使用合成数据，因为我不我自己有很多真实数据，那么应该使用多少合成数据。  使用的数据量是否取决于模型的大小或任何其他因素？   由   提交/u/Medium_Alternative50   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bajiz5/d_how_much_synthetic_data_to_use/</guid>
      <pubDate>Sat, 09 Mar 2024 14:55:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] PromptKD：视觉语言模型的无监督即时蒸馏，即时学习中的 SOTA。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bahtd6/r_promptkd_unsupervised_prompt_distillation_for/</link>
      <description><![CDATA[      纸张： https://arxiv.org/abs/2403.02781 项目页面： https://zhengli97.github.io/PromptKD/ Github： https://github.com/zhengli97/PromptKD ​ https://preview.redd.it/bop5wm9f8bnc1.png?width=1330&amp;format=png&amp; ;auto=webp&amp;s=ad50156e81d6f9248c597ca239596f28a9f5d7cb 亮点： (1)。一种新颖的视觉语言模型两阶段无监督即时蒸馏框架。 (2)。重用高质量的教师文本特征，而不是训练学生自己的文本编码器。 (3)。使用老师提供的软标签对大量未标记的域图像进行蒸馏。 (4)。 PromptKD 在 11 个不同的识别数据集上优于所有现有的即时学习方法。 摘要： 在本文中，我们介绍了一种无监督域即时蒸馏框架，旨在通过使用未标记的域图像进行提示驱动的模仿，将较大教师模型的知识转移到轻量级目标模型。 具体来说，我们的框架由两个不同的阶段组成。在初始阶段，我们使用域（少样本）标签预训练大型 CLIP 教师模型。预训练后，我们利用 CLIP 独特的解耦模态特征，通过教师文本编码器预计算文本特征并将其存储为类向量一次。 在后续阶段，存储的类向量在教师和学生图像编码器之间共享，以计算预测的 logits。此外，我们通过 KL 散度损失来对齐教师和学生模型的对数，鼓励学生图像编码器通过可学习的提示生成与教师相似的概率分布。 所提出的提示蒸馏过程消除了对标记数据的依赖，使算法能够利用域内大量未标记的图像。最后，利用训练有素的学生图像编码器和预存储的文本特征（类向量）进行推理。 据我们所知，我们是第一个 (1) 执行无监督域的人-针对CLIP的特定提示驱动的知识蒸馏，(2)建立实用的文本特征预存储机制作为教师和学生之间共享的类向量。在11个识别数据集上的大量实验证明了我们方法的有效性。  图 1 我们的概述提示KD框架。 (a) 我们首先使用带标签的训练图像预训练一个大型 CLIP 教师模型。 (b) 重用现有的更高质量的教师文本特征进行无监督的快速蒸馏。 (c) 利用训练有素的学生和预先存储的教师文本特征进行最终推理。 实验结果： 基础-to-novel实验： 表1. 与现有最先进的从基础到新颖的概括方法进行比较。  跨数据集实验： 表 2. PromptKD 与现有先进方法在跨数据集基准评估上的比较。 ​ &lt; !-- SC_ON --&gt;  由   提交 /u/zhengli_nku   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bahtd6/r_promptkd_unsupervised_prompt_distillation_for/</guid>
      <pubDate>Sat, 09 Mar 2024 13:32:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] 分数 GPU 容器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baho3l/p_fractional_gpu_containers/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交 /u/Thick-Taste-9985   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baho3l/p_fractional_gpu_containers/</guid>
      <pubDate>Sat, 09 Mar 2024 13:24:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练了超过 120 个风格迁移 MLModel - 它们位于 GitHub 上</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baf5if/p_trained_over_120_style_transfer_mlmodels_here/</link>
      <description><![CDATA[       由   提交 /u/VysokoAnime   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baf5if/p_trained_over_120_style_transfer_mlmodels_here/</guid>
      <pubDate>Sat, 09 Mar 2024 10:54:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 具有广义持续学习的可扩展语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baepox/r_scalable_language_model_with_generalized/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=mz8owj4DXu 摘要：  持续学习变得越来越重要，因为它促进了语言模型中可扩展知识和技能的获取和完善。然而，现有方法通常在现实场景中遇到严格的限制和挑战，例如依赖经验回放、优化约束和推理任务 ID。在这项研究中，我们引入了可扩展语言模型（SLM）来在更具挑战性和通用性的环境中克服这些限制，代表着持续学习实际应用的重大进步。具体来说，我们提出了与动态任务相关知识检索（DTKR）集成的联合自适应重新参数化（JARe） ），以实现基于特定下游任务的语言模型的自适应调整。这种方法利用向量空间内的任务分布，旨在实现平稳且轻松的持续学习过程。我们的方法在不同的骨干网和基准上展示了最先进的性能，在全套和少量场景中实现有效的持续学习，并且遗忘最少。此外，虽然之前的研究主要集中在分类等单一任务类型，但我们的研究超越了大型语言模型，即 LLaMA-2，探索了跨不同领域和任务类型的影响，例如单一语言模型可以适当地扩展到更广泛的应用程序。代码和模型将向公众发布。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baepox/r_scalable_language_model_with_generalized/</guid>
      <pubDate>Sat, 09 Mar 2024 10:24:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习 CUDA/C++ 有多大价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</link>
      <description><![CDATA[目前每个人都在努力使 AI 实现快速/高效（因为效率更高 -&gt; 计算上花费的资金更少）。 例如，Flash Attention 2是在CUDA中实现的。 Llama.cpp 是 C++ PyTorch 够用吗？或者在这个市场上学习 CUDA/C++ 是否有优势，特别是对于法学硕士？ 如果 CUDA 在某些情况下有用，那么这些情况是什么？   由   提交/u/joelthomas-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</guid>
      <pubDate>Sat, 09 Mar 2024 09:44:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemma-7b：本地 RTX 3090 上的推理速度太慢</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1badye0/d_gemma7b_inference_is_way_too_slow_on_local_rtx/</link>
      <description><![CDATA[只是一个简单的测试，如下所示 from Transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(&quot;google /gemma-7b&quot;) model = AutoModelForCausalLM.from_pretrained(&quot;google/gemma-7b&quot;, device_map=&quot;auto&quot;) Mission_text = &quot;用 Java 写一个简单的程序&quot;; Mission_ids = tokenizer(mission_text, return_tensors=“pt”).to(“cuda”) Mission_outputs = model.generate(input_ids=mission_ids[“input_ids”], max_new_tokens=500) print(tokenizer.decode(mission_outputs[0]) ]))  和输出 用 Java 写一个简单的程序，它将接受一个字符串并返回元音的数量字符串。答案：步骤 1/4 1. 我们需要创建一个以字符串作为输入的方法。步骤 2/4 2. 我们需要遍历字符串并检查每个字符是否是元音。步骤3/4 3.如果一个字符是元音，我们需要增加一个计数器。步骤 4/4 4. 最后，我们需要返回计数器作为字符串中元音的数量。代码如下： public static int countVowels(String str) { int count = 0; for (int i = 0; i &lt; str.length(); i++) { char ch = str.charAt(i); if (ch == &#39;a&#39; || ch == &#39;e&#39; || ch == &#39;i&#39; || ch == &#39;o&#39; || ch == &#39;u&#39;) { count++;返回计数；为了测试程序，我们可以使用字符串调用该方法并打印结果： public static void main(String[] args) { String str = &quot;Hello World&quot;; int count = countVowels(str); System.out.println(&quot;&quot; + str + &quot; 中的元音数量为 &quot; + count);输出：Hello World 中的元音数量为 3&lt;eos&gt;  我认为在我的本地计算机上使用 RTX 3090 生成上面的文本大约需要 10 分钟，是不是太慢了？ Windows 10，pytorch 2.1。 2+cu121，CUDA 12.3，VRAM消耗约22GB，推理时GPU负载约60-70%   由   提交/u/tunggad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1badye0/d_gemma7b_inference_is_way_too_slow_on_local_rtx/</guid>
      <pubDate>Sat, 09 Mar 2024 09:32:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 R/Python 进行机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</link>
      <description><![CDATA[我是应用统计专业的学生。正因为如此，我们通过 R 学习统计机器学习。然而，当我遇到机器学习时，大多数时候，人们通常更多地谈论使用 Python。所以我不知道有哪个领域/行业实际使用 R 的 ML 来代替？如果 R 在 ML 中非常不受欢迎，你认为公司会因为我只懂 R 而选择我吗？ （如果是这样的话我可能会尝试学习Python，但我想我不会很强）。干杯伙计们！    由   提交 /u/StrangerOnTheRoad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</guid>
      <pubDate>Sat, 09 Mar 2024 08:19:19 GMT</pubDate>
    </item>
    <item>
      <title>[N] 矩阵乘法突破可能带来更快、更高效的人工智能模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</link>
      <description><![CDATA[   “计算机科学家发现了一个报告称，通过消除以前未知的低效率，比以往更快地乘以大型矩阵的新方法 广达杂志。这最终可能会加速人工智能模型，例如ChatGPT，它严重依赖矩阵乘法来运行。据报道，最近两篇论文中提出的研究结果使矩阵乘法效率实现了十多年来的最大改进。 ...图形处理单元 (GPU) 擅长处理矩阵乘法任务，因为它们能够同时处理许多计算。他们将大型矩阵问题分解为更小的部分，并使用算法同时解决它们。完善该算法一直是过去一个世纪（甚至在计算机出现之前）矩阵乘法效率突破的关键。 2022 年 10 月，我们涵盖了Google DeepMind AI 模型发现的一项名为 AlphaTensor 的新技术，专注于针对特定矩阵大小（例如 4x4 矩阵）的实用算法改进。 相比之下， 新研究，由清华大学的段燃和周仁飞、加州大学伯克利分校的吴洪勋以及 Virginia Vassilevska 进行麻省理工学院的 Williams、Yinzhan Xu 和 Zixuan Xu（在第二篇论文中）寻求通过降低复杂性指数 ω 来实现理论增强，从而在所有大小的矩阵上获得广泛的效率增益。这项新技术并没有像 AlphaTensor 那样寻找直接、实用的解决方案，而是解决了基础性的改进，可以在更广泛的范围内提高矩阵乘法的效率。  ... 两个 n×n 矩阵相乘的传统方法需要 n3 次单独的乘法。然而，这项新技术改进了“激光方法”。由 Volker Strassen 于 1986 年提出，减小了指数的上限（表示为前面提到的 ω），使其更接近到理想值 2，这表示理论上所需的最小操作数。” ​ https://preview.redd.it/a49r1ajv59nc1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp; s =cf315793e6784ef9e62d48e00ebf0f3809070f6c https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/&lt; /strong&gt;   由   提交/u/Secure-Technology-78   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</guid>
      <pubDate>Sat, 09 Mar 2024 06:28:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 不可能有真正的苏格兰口语系统（模仿）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</link>
      <description><![CDATA[       由   提交 /u/TobyWasBestSpiderMan   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</guid>
      <pubDate>Fri, 08 Mar 2024 14:33:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>