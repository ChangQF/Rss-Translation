<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 04 Dec 2023 12:25:36 GMT</lastBuildDate>
    <item>
      <title>[D] 有哪些工具可以用于观察和监控生产中的 AI 模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aicst/d_what_are_some_tools_for_observing_and/</link>
      <description><![CDATA[有许多机器学习监控和可观察性工具，但我们是否有为生产中的 AI 模型构建用于观察和监控的特定工具？或者任何开源库？   由   提交/u/Aromatic_Ad9700   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aicst/d_what_are_some_tools_for_observing_and/</guid>
      <pubDate>Mon, 04 Dec 2023 11:35:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 计算机科学硕士还是统计学硕士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18afzr3/d_ms_in_cs_or_statistics/</link>
      <description><![CDATA[一些背景知识。我今年毕业，获得计算机科学学士学位。我有一份 MLOps/云工程师的工作，但不幸的是我被解雇了。我现在的目标是成为一名 MLE 或一名数据科学家（不一定是研究员，尽管那很酷）。我的问题是，在这方面，计算机科学硕士和统计硕士哪个更有用？我听说做 ML/DS 特定程序可以成为“摇钱树”。而且有点肤浅，所以我宁愿做一些更基础的事情，给我更多的选择。如果你们有任何意见，我将不胜感激！提前致谢。   由   提交/u/Ultra_Amp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18afzr3/d_ms_in_cs_or_statistics/</guid>
      <pubDate>Mon, 04 Dec 2023 08:37:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调基础模型时的图像和图像标题策略？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18acct2/d_strategy_for_images_and_image_captions_when/</link>
      <description><![CDATA[我想微调文本到图像模型以了解与我一起工作的人的某些面孔。我做了一些实验，我可以得到一些让人想起一个人的图像，但实际上看起来并不像他们。我还需要在提示中提供比我预期更多的内容。 例如，有一个人是一个留着小胡子、戴着眼镜的大个子。我使用他的几张图像进行了微调，标题是他在训练数据集中的真实姓名。 当我生成以他的名字为主题的图像时，没有一张脸上有胡子或眼镜。如果我提示“留着小胡子、戴眼镜的马克·史密斯正在做 xyz”它看起来确实有点让人想起他，但仍然不太正确。 我应该采取什么策略来改进这一点？我需要更多他的照片吗？我是否应该将他的名字（或类似的名字）散列到通用标题中，以确保模型中的其他权重不会干扰？还有其他想法吗？ 我意识到我可以尝试，但不断微调的成本非常高，而且我不想多次走错方向。   由   提交/u/coinclink  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18acct2/d_strategy_for_images_and_image_captions_when/</guid>
      <pubDate>Mon, 04 Dec 2023 04:32:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 没有 Autograd 的教育变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aaxfj/p_educational_transformer_without_autograd/</link>
      <description><![CDATA[我学习 NLP 一段时间了，总是发现很难找到具有显式前向和反向传播的 Transformer 的完整实现。这个项目是我在学习如何更好地理解 Transformer 优化的同时尝试构建它的项目。 它有我所能做到的详细记录，并且可以通过编辑config.py 文件并运行 run.py 脚本。获取代码： git clone https://github.com/eduardoleao052/Transformer-from-scratch.git  我成功生成了一些在儒勒凡尔纳和莎士比亚的作品上训练这个模型的文本非常好。希望您喜欢！ GitHub 存储库此处！   由   提交 /u/suspicious_beam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aaxfj/p_educational_transformer_without_autograd/</guid>
      <pubDate>Mon, 04 Dec 2023 03:12:36 GMT</pubDate>
    </item>
    <item>
      <title>[P][D] A100 在文本生成的小批量大小下比预期慢得多</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aa0cq/pd_a100_is_much_slower_than_expected_at_low_batch/</link>
      <description><![CDATA[我正在开发一个项目，从 1.2B 参数全精度 LLM (5gb) 生成文本 不幸的是我&#39;我可以用来部署这个模型的基础设施有限。不支持批量推理。我拥有的基础设施允许我在单个 A100 上部署模型的副本，每个进程 1 个，最多支持 9 个进程（这些称为“副本”）。我知道这没有什么意义，因为我的模型受内存限制，并且每个进程都会争夺内存带宽以读取相同的权重，但我现在无法更改这一点。 我的平均输入和每个输出令牌大约为 1000 个。我估计使用全精度时每个令牌的 kv 缓存大约为 400kB。  我使用如上所述的各种“副本”对模型的延迟进行了基准测试。我想将其与 A100 的理论性能进行比较。对于我的用例，第一个令牌的时间可以忽略不计（&lt;200ms），并且生成受内存限制。 我发现，如果有 5 个或更多副本，数学计算结果是，我的模型大致与我预计。例如，对于 1000 个输出令牌、6 个副本，就像我使用来自 30GB 模型 + 5GB 的 kv 缓存的一批 6 个请求来生成。内存带宽约为 1-1.3tbps，相当于每个请求约 30 秒，这与我所看到的相差不远。其他副本编号 5、7、8 和 9 也是如此。 但是，当我使用单个副本运行时，我预计生成平均会徘徊在 5-6 标记附近。相反，我看到 &gt; 20多岁。在这个数字开始有意义之前，我需要再添加 4 个副本。看起来该模型占用的内存太少，无法分配整个内存带宽。 有谁知道这种额外的延迟可能来自哪里？ A100 内存带宽模型是否必须达到一定的已用内存量才能达到可用内存带宽？   由   提交/u/currytrash97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aa0cq/pd_a100_is_much_slower_than_expected_at_low_batch/</guid>
      <pubDate>Mon, 04 Dec 2023 02:23:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用嵌入生成主题以进行主题建模的包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a9qvv/d_package_that_uses_embeddings_to_generate_topics/</link>
      <description><![CDATA[我在 Python 中使用过 Bertopic 和 Top2Vec，但我想知道 R 中是否有类似的东西可以使用预训练模型来生成主题？如果没有，您认为投入时间构建这样的东西对社区有用吗？   由   提交 /u/NewerResearcher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a9qvv/d_package_that_uses_embeddings_to_generate_topics/</guid>
      <pubDate>Mon, 04 Dec 2023 02:09:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于分段任意模型的所有有趣的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a9agl/d_everything_interesting_about_the_segment/</link>
      <description><![CDATA[      分享我的 ML YouTube 频道中有关 Meta 的 Segment Anything 模型的视频，如何实现它的工作原理、训练方式以及部署方式。为感兴趣的人留下链接！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a9agl/d_everything_interesting_about_the_segment/</guid>
      <pubDate>Mon, 04 Dec 2023 01:45:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我正在训练使用 jais 模型嵌入。有什么想法吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a93u1/d_im_training_to_use_jais_model_embedding_any/</link>
      <description><![CDATA[我正在训练使用 jais 模型嵌入，我很想知道以前是否有人使用过它 &lt; !-- SC_ON --&gt;  由   提交/u/ahsaor8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a93u1/d_im_training_to_use_jais_model_embedding_any/</guid>
      <pubDate>Mon, 04 Dec 2023 01:36:05 GMT</pubDate>
    </item>
    <item>
      <title>[R]大型变压器模型推理优化（Lilian Weng，2023）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a64mb/r_large_transformer_model_inference_optimization/</link>
      <description><![CDATA[ 由   提交/u/niplav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a64mb/r_large_transformer_model_inference_optimization/</guid>
      <pubDate>Sun, 03 Dec 2023 23:08:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 您在研究中引用了 Arxiv 预印本吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a5hd7/r_do_you_cite_arxiv_preprints_in_your_research/</link>
      <description><![CDATA[机器学习研究人员大家好！ 一些背景信息： 我正在写一篇调查论文（调查+拟议的研究方向）。它一直在不断发展，包括新的发展。没什么新奇的。我远离学术界，在一家初创公司担任科学家。 现实是：由于我的日程安排，我根本无法处理审核过程。同时，我真的希望这篇论文能够被阅读/使用/引用。我会将其上传到 arxiv 中。 我的问题是：在 ML 研究中，您多久会考虑将 arxiv 作为一个很好的参考？由于机器学习研究的超快性质，我的猜测是它与其他领域有点不同。 （我的博士学位是应用数学，我们经常引用 arxiv 预印本）。 （我的博士学位是应用数学，我们经常引用 arxiv 预印本）。 &gt;   由   提交 /u/tanweer_m   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a5hd7/r_do_you_cite_arxiv_preprints_in_your_research/</guid>
      <pubDate>Sun, 03 Dec 2023 22:38:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 论文分析 - 从（生产）语言模型中可扩展地提取训练数据（视频演练）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189y2cv/d_paper_analysis_scalable_extraction_of_training/</link>
      <description><![CDATA[https://youtu.be/KwpeuqT69fw 研究人员只需要求 ChatGPT 多次重复一个单词，就可以从 ChatGPT 中获取大量训练数据，这会导致模型出现分歧并开始吐出记忆的文本。 为什么会出现这种情况？这些模型真正逐字记住了多少训练数据？ ​ 概要： 0:00 - 简介 8:05 - 可提取与可发现的记忆 14:00 - 模型泄漏的数据比之前想象的更多 20:25 - 某些数据是可提取的，但不可发现 &gt; 25:30 - 从封闭模型中提取数据 30:45 - 诗诗诗 37:50 - 定量隶属度测试 40: 30 - 进一步探索 ChatGPT 漏洞 47:00 - 结论 ​ 论文：https://arxiv.org/abs/2311.17035   由   提交/u/ykilcher  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189y2cv/d_paper_analysis_scalable_extraction_of_training/</guid>
      <pubDate>Sun, 03 Dec 2023 17:13:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 在规划中结合空间和时间抽象以实现更好的泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189qtnk/r_combining_spatial_and_temporal_abstraction_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.00229 OpenReview：https:// /openreview.net/forum?id=eo9dHwtTFt 代码：  https://github.com/mila-iqia/skipper 博客文章：http://mingde.world/combining-spatial-and-temporal-abstraction-in-planning/ 摘要:  受人类意识规划的启发，我们提出了Skipper，这是一种基于模型的强化学习代理，它利用空间和时间抽象来概括所学技能新颖的情况。它自动将手头的任务分解为更小规模、更易于管理的子任务，从而实现稀疏决策并将其计算集中在环境的相关部分。这依赖于表示为有向图的高级代理问题的定义，其中使用事后知识端到端地学习顶点和边。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法预计会有所帮助的地方。与现有最先进的分层规划方法相比，以泛化为中心的实验验证了 Skipper 在零样本泛化方面的显着优势。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189qtnk/r_combining_spatial_and_temporal_abstraction_in/</guid>
      <pubDate>Sun, 03 Dec 2023 10:28:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] TSMixer：用于预测的时间序列混合器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189nx1e/p_tsmixer_time_series_mixer_for_forecasting/</link>
      <description><![CDATA[github 链接 TSMixer 是一个基于 PyTorch 的 TSMixer 架构实现，如 TSMixer 论文所述。它利用混合器层来处理时间序列数据，为标准和扩展预测任务提供强大的方法。   由   提交/u/Yossarian_1234   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189nx1e/p_tsmixer_time_series_mixer_for_forecasting/</guid>
      <pubDate>Sun, 03 Dec 2023 06:56:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​的扩散模型 | DDPM PyTorch 实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189lqlh/p_diffusion_models_from_scratch_ddpm_pytorch/</link>
      <description><![CDATA[       由   提交/u/tusharkumar91   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189lqlh/p_diffusion_models_from_scratch_ddpm_pytorch/</guid>
      <pubDate>Sun, 03 Dec 2023 04:38:35 GMT</pubDate>
    </item>
    </channel>
</rss>