<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 05 Dec 2023 03:15:35 GMT</lastBuildDate>
    <item>
      <title>[D] 用于时间序列预测的 Transformer</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ax51t/d_transformers_for_time_series_forecasting/</link>
      <description><![CDATA[有一些新兴的 Transformer 模型专为预测时间序列值而设计，例如 Informer 和 Temporal Fusion Transformer。您对这个话题有何看法？你认为他们能忍受 RNN 吗？   由   提交 /u/MrGolran   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ax51t/d_transformers_for_time_series_forecasting/</guid>
      <pubDate>Mon, 04 Dec 2023 23:07:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] XGBoost特征选择</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18atkzs/d_xgboost_feature_selection/</link>
      <description><![CDATA[当前对深度和学习率进行有根据的猜测，添加几个白噪声变量，并删除特征重要性图中较低的所有变量。 有更好的方法吗？ 我看到其他人提到在删除白噪声以下的特征之前将树深度减小到 1。另一种方法是使用 shap 值并查看 pdp 图。   由   提交/u/fuzzy_plums  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18atkzs/d_xgboost_feature_selection/</guid>
      <pubDate>Mon, 04 Dec 2023 20:33:50 GMT</pubDate>
    </item>
    <item>
      <title>Mamba：具有选择性状态空间的线性时间序列建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aq0k5/mamba_lineartime_sequence_modeling_with_selective/</link>
      <description><![CDATA[       由   提交/u/Jean-Porte  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aq0k5/mamba_lineartime_sequence_modeling_with_selective/</guid>
      <pubDate>Mon, 04 Dec 2023 18:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种架构可以替代变压器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</link>
      <description><![CDATA[我最近读到了这个观点，讨论变压器可以被更换。  https://towardsdatascience.com/a-requiem-for-the-transformer -297e6f14e189 总的来说，过去几个月发表的文章显示了 Transformer 的局限性： https://arxiv.org/abs/2203.15556 https:/ /arxiv.org/abs/2304.15004  在计算机视觉中，具有相同预算的ConvNet似乎具有相似的性能： https://arxiv.org/abs/2310.19909  https://arxiv.org/abs/2310.16764  DeepMind 表明 Transformer 无法泛化到训练集分布之外： https://arxiv.org/abs/2311.00871 液体神经网络、鬣狗、尖峰神经网络等模型显示出活跃的搜索对于新架构： https://arxiv.org/pdf/2006.04439.pdf&lt; /p&gt; https://www.together.ai/blog/monarch-mixer  并不是说 Transformer 很快就会被取代，但我现在想知道下一个主导架构可能是什么？  所提出的架构似乎都不比变压器具有竞争优势   由   提交/u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</guid>
      <pubDate>Mon, 04 Dec 2023 17:44:29 GMT</pubDate>
    </item>
    <item>
      <title>从头开始学习 ML 概念的新资源（唯一要求是 Python）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18apjbp/new_resource_for_learning_ml_concepts_from/</link>
      <description><![CDATA[结构化学习资源最适合我们大多数人，您可能对 ChatGPT 在最低级别的实际工作原理以及如何对其进行编码感兴趣 不幸的是，许多人工智能/机器学习资源包含太多数学知识，要么令人困惑，要么只是拖着乏味的数学证明。我创建了 YouTube 频道 GPT 和 Chill，专注于您在 ML 副项目或进入 ML 工程时实际需要了解的内容. 到目前为止，我们已经从头开始介绍了许多 ML 和神经网络概念，播放列表/系列中的下一个视频将介绍自注意力和从头开始编码 GPT。如果你对此感兴趣，我建议你先浏览一下现有的播放列表！ 你所需要的只是 Python 和基本的微积分知识（x^2 的导数是什么以及类似的东西）  p&gt; 我还将制作一个类似的播放列表/系列，介绍自动驾驶模型的工作原理和Deepfakes 是如何从头开始用代码生成的。我还将添加一个平台，您可以在其中尝试编写 ML 概念并针对基本测试用例进行运行。 如果您还想观看视频，请留下评论！希望回馈这个社区，因为它在我进入数据科学的过程中教会了我无数的教训。   由   提交 /u/GPTandChill   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18apjbp/new_resource_for_learning_ml_concepts_from/</guid>
      <pubDate>Mon, 04 Dec 2023 17:42:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 损失权重 - 理论保证？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ap1lk/r_loss_weighting_theoretical_guarantees/</link>
      <description><![CDATA[      对于由加权损失组成的损失函数的模型训练： &lt; p&gt;​ https ://preview.redd.it/j04h4v0sab4c1.png?width=153&amp;format=png&amp;auto=webp&amp;s=21677d2520375d500b904bb3ae30d403c9941d7c 我想知道关于模型可以说些什么基于损失 ℒ_i 的 ℒ 损失收敛，或者可能是分别收敛于 ℒ_i 损失的模型。例如，如果我对模型 m_i 有一些收敛到损失 ℒ_i 的保证/属性，如果其中一些保证属性转换到收敛于 ℒ 的模型 m。 非常感谢有关此问题的理论论文的链接，甚至是关键字来帮助我搜索此类论文。 &lt; p&gt;提前非常感谢您的帮助/指导！   由   提交 /u/progmayo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ap1lk/r_loss_weighting_theoretical_guarantees/</guid>
      <pubDate>Mon, 04 Dec 2023 17:20:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何使用 PyTorch 分布式训练神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aouo2/d_how_to_distributedly_train_nns_using_pytorch/</link>
      <description><![CDATA[https:/ /twitter.com/rebooted101_py/status/1731671501378605263?t=xZNUl8nPHSQoHcz6j58asQ&amp;s=19   由   提交/u/palashsharma15   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aouo2/d_how_to_distributedly_train_nns_using_pytorch/</guid>
      <pubDate>Mon, 04 Dec 2023 17:11:43 GMT</pubDate>
    </item>
    <item>
      <title>[D]从聊天模型到代理模型+避免提示注入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18an6mb/d_from_chat_models_to_agent_models_avoiding/</link>
      <description><![CDATA[我最近在考虑LLM代理，我有两个想法对我来说很自然，尽管我没有看到很多关于LLM代理的讨论他们。因此，我决定与您分享。 首先，我看到一些人多次指出的一个重要想法是，LLM 输出不应该被视为“不”。与其说是“声音”，不如说是“思想”。 ChatGPT 的冗长是有原因的，特别是在处理数学或代码时。我们不应该试图让法学硕士变得不那么冗长，因为这意味着让他们“少思考”关于他们的回应。实际上，代理越详细越好，这样我们就可以检查其结果背后的推理。 因此，我们真正需要的是使模型尽可能详细，然后隐藏代理“想法”例如，ChatGPT 也以同样的方式隐藏代码解释器的使用。然后，我们给它一个“谈话”每当它真正想要向用户发送消息时，它就可以使用该操作。 这样，即使我们给它一个多项选择问题并要求它仅回复正确答案，它也会首先分析每个选项，然后使用“谈话”行动在最后。我觉得 ChatGPT 已经在朝这个方向发展了。 我想到的第二件事是，代理绝对需要知道他们自己的“想法”和“想法”之间的区别。以及他们从环境中获得的输入。这对于避免提示注入并确保安全至关重要。 我提出的解决方案是对输入和“想法”使用完全不同的令牌。模型的。因此，即使两个数据源的人类可读文本相同，我们也会将它们编码为不同的标记集。这类似于使用不同的语言(例如英语和中文)，但是“思想”中所说的语言是不同的。输入部分永远不允许使用模型的名称（例如 English v2）。我不确定这是否能解决 100% 的提示注入问题，但它肯定会有很大帮助。 这种方法的唯一问题是模型也需要学习这种新语言，并且它需要与任何其他语言一样多的训练数据。解决方案的一部分是复制数据集并将文本转换为常规文本和“特殊”文本。令牌，但还需要一些更多的技巧来教模型在这些语言之间进行翻译。 对这些想法有什么想法吗？   由   提交/u/DMKAI98  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18an6mb/d_from_chat_models_to_agent_models_avoiding/</guid>
      <pubDate>Mon, 04 Dec 2023 15:57:04 GMT</pubDate>
    </item>
    <item>
      <title>我是新来的，但这似乎太好了，无法不分享。对此大家有何看法呢？ 4M：大规模多模态掩模建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18am683/i_am_new_here_but_this_seemed_too_good_not_to/</link>
      <description><![CDATA[ 由   提交 /u/Dullydude   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18am683/i_am_new_here_but_this_seemed_too_good_not_to/</guid>
      <pubDate>Mon, 04 Dec 2023 15:08:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年机器学习研究热点是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18am0tx/d_whats_hot_for_machine_learning_research_in_2024/</link>
      <description><![CDATA[ML 中或与 ML 相关的哪些子领域/方法、应用领域预计将在 2024 年获得广泛关注（双关语无意）？&lt; /p&gt; PS：请不要回避提出您可能认为或知道的任何可能成为机器学习研究热门主题的内容，很可能您所知道的内容对于我们这里的许多人来说可能是未知的:)   由   提交 /u/ureepamuree   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18am0tx/d_whats_hot_for_machine_learning_research_in_2024/</guid>
      <pubDate>Mon, 04 Dec 2023 15:01:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] 即将推出的无隐藏状态的反应式 Python+SQL 笔记本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18akbc6/p_an_up_and_coming_reactive_pythonsql_notebook/</link>
      <description><![CDATA[    &lt; /a&gt;  该项目仍处于早期阶段，因此请让我们知道您的反馈！以下是快速预览： 零真演示 如果您想查看我们的 github：https://github.com/Zero-True/zero-true   由   提交/u/zero-true  /u/zero-true reddit.com/r/MachineLearning/comments/18akbc6/p_an_up_and_coming_reactive_pythonsql_notebook/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18akbc6/p_an_up_and_coming_reactive_pythonsql_notebook/</guid>
      <pubDate>Mon, 04 Dec 2023 13:34:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 没有 Autograd 的教育变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18aaxfj/p_educational_transformer_without_autograd/</link>
      <description><![CDATA[我学习 NLP 一段时间了，总是发现很难找到具有显式前向和反向传播的 Transformer 的完整实现。这个项目是我在学习如何更好地理解 Transformers 中的优化的同时构建它的尝试。 它有尽可能详细的文档记录，并且可以通过编辑config.py 文件并运行 run.py 脚本。获取代码： git clone https://github.com/eduardoleao052/Transformer-from-scratch.git  我成功生成了一些在儒勒凡尔纳和莎士比亚的作品上训练这个模型的文本非常好。希望您喜欢！ GitHub 存储库此处！   由   提交 /u/suspicious_beam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18aaxfj/p_educational_transformer_without_autograd/</guid>
      <pubDate>Mon, 04 Dec 2023 03:12:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于分段任意模型的所有有趣的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a9agl/d_everything_interesting_about_the_segment/</link>
      <description><![CDATA[      分享我的 ML YouTube 频道中关于 Meta 的 Segment Anything 模型的视频，如何实现它的工作原理、训练方式以及部署方式。为感兴趣的人留下链接！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a9agl/d_everything_interesting_about_the_segment/</guid>
      <pubDate>Mon, 04 Dec 2023 01:45:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 您在研究中引用了 Arxiv 预印本吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a5hd7/r_do_you_cite_arxiv_preprints_in_your_research/</link>
      <description><![CDATA[机器学习研究人员大家好！ 一些背景信息： 我正在写一篇调查论文（调查+拟议的研究方向）。它一直在不断发展，包括新的发展。没什么新奇的。我远离学术界，在一家初创公司担任科学家。 现实是：由于我的日程安排，我根本无法处理审核过程。同时，我真的希望这篇论文能够被阅读/使用/引用。我会将其上传到 arxiv 中。 我的问题是：在 ML 研究中，您多久会考虑将 arxiv 作为一个很好的参考？由于机器学习研究的超快性质，我的猜测是它与其他领域有点不同。 （我的博士学位是应用数学，我们经常引用 arxiv 预印本）。 （我的博士学位是应用数学，我们经常引用 arxiv 预印本）。 &gt;   由   提交 /u/tanweer_m   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a5hd7/r_do_you_cite_arxiv_preprints_in_your_research/</guid>
      <pubDate>Sun, 03 Dec 2023 22:38:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>