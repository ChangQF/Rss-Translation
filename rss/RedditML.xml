<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 28 Oct 2024 15:18:25 GMT</lastBuildDate>
    <item>
      <title>[D] 确保 Runpod 的 Whisper API 中对长时间运行的转录作业的同步响应</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdym7u/d_ensuring_synchronous_responses_in_whisper_api/</link>
      <description><![CDATA[我们目前正在使用由 runpod 托管的 Whisper API，并且注意到如果音频转文本作业在 60 秒内完成，则以下代码运行时不会出现问题：  def generate_transcript(signed_url: str) -&gt; str: url = &quot;https://api.runpod.ai/v2/XXX/runsync&quot; headers = { &quot;accept&quot;: &quot;application/json&quot;, &quot;authorization&quot;: &quot;XXX&quot;, &quot;content-type&quot;: &quot;application/json&quot; } # 定义要在请求中发送的数据 data = { &quot;input&quot;: { &quot;audio&quot;:signed_url, &quot;model&quot;: &quot;large-v2&quot; } } # 发送 POST 请求 response = request.post(url, headers=headers, json=data) response_data = response.json() # 提取转录字段 transcription = response_data.get(&quot;output&quot;, {}).get(&quot;transcription&quot;, None) return transcription  但是，如果代码花费的时间超过 60 秒，我们会收到一个 JSON 响应，表明作业状态仍为 IN_PROGRESS：  {&quot;delayTime&quot;:663,&quot;id&quot;:&quot;sync-58330ff2-6c74-469c-892b-e5ed2f09e1a6-u1&quot;,&quot;status&quot;:&quot;IN_PROGRESS&quot;,&quot;workerId&quot;:&quot;8olee7j959gl7n&quot;}  为了处理这个问题，我们可能需要实现一个轮询机制，每 N 秒检查一次作业状态，直到完成。这种方法会使我们的代码复杂化，因为我们更喜欢一个解决方案，即 requests.post 仅在转录完全处理后返回。 是否可以将 API 配置为在所有情况下同步运行，即使处理可能需要几分钟？这将通过消除定期轮询的需要来简化我们的代码。    提交人    /u/yccheok   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdym7u/d_ensuring_synchronous_responses_in_whisper_api/</guid>
      <pubDate>Mon, 28 Oct 2024 10:20:40 GMT</pubDate>
    </item>
    <item>
      <title>[D]最终在 NuerIPS-24 上发表了一张海报</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/</link>
      <description><![CDATA[我今年通过期刊轨道 (MLRC) 在 NuerIPS 上发表了一张海报，以及主要的会议论文。我没想到会发生这种情况，所以我之前没有计划/研究过费用/资金。我已经安排好了签证和会议注册，但对 Nuerips 的进一步进展和如何资助它一无所知（我是本科三年级学生）。如果你以前已经参加过 NeurIPS，请倾诉你的想法和经验。    提交人    /u/whit3whistl3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/</guid>
      <pubDate>Mon, 28 Oct 2024 08:50:47 GMT</pubDate>
    </item>
    <item>
      <title>[N] 有任何肺癌检测模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdnra7/n_any_models_lung_cancer_detection/</link>
      <description><![CDATA[我是一名医学生，正在探索 AI 在资源有限的医院（通过 CT 图像）改善肺癌诊断的潜力。AI 的经济实惠使其成为一种很有前途的工具，但我面临着为这一特定应用寻找合适的预训练模型或开源资源的挑战。我有点避免使用商业模型，因为研究重点是低资源设置。虽然像 GPT 这样的大型语言模型很有价值，但我知道它们在直接分析医学图像方面的局限性。所以有什么建议吗？任何东西都会真正帮助我，谢谢！    提交人    /u/Krank910   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdnra7/n_any_models_lung_cancer_detection/</guid>
      <pubDate>Sun, 27 Oct 2024 23:01:47 GMT</pubDate>
    </item>
    <item>
      <title>基于时间的课程学习 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdn8hr/time_based_curriculum_learning_discussion/</link>
      <description><![CDATA[是否有人探索过 LLM 课程学习的变体，其中 1) 按时间顺序提供模型训练的信息，或 2) 明确指定/学习训练数据源的生产日期。 扩展 1）对于 LLM，这可能意味着首先对 2010 年的维基百科文章进行训练 -&gt; 对 2011 年的维基百科文章进行训练。 扩展 2）在这种情况下，它可以表示所有文本中的特定标记，用于训练编码按时间顺序排列信息的语言模型。类似于 [CLS] 标记。另一个示例可能是具有额外损失的子网/超网络，训练使得训练数据的按时间顺序排列的创建日期必须可以从文本中预测出来。 具体来说，我想激发人们对这些修改的实用性及其潜在好处的讨论。一些问题包括，为了能够估计输入文本的时间顺序，输入文本必须足够长且完整。这可能意味着在预训练中使用的传统截断策略将不可行。    提交人    /u/Envoy-Insc   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdn8hr/time_based_curriculum_learning_discussion/</guid>
      <pubDate>Sun, 27 Oct 2024 22:36:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 揭秘分布式检查点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdkdka/d_demystifying_distributed_checkpointing/</link>
      <description><![CDATA[        提交人    /u/joygao   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdkdka/d_demystifying_distributed_checkpointing/</guid>
      <pubDate>Sun, 27 Oct 2024 20:25:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] 需要 Arxiv cs.AI 上的认可</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdje64/r_need_endorsement_on_arxiv_csai/</link>
      <description><![CDATA[我是一名独立研究员，我的论文已经在 IEEE explorer 上发表，我希望将其上传到 arxiv，我需要 CS.AI 的认可&gt; 认可代码：PM3P4K https://arxiv.org/auth/endorse?x=PM3P4K    提交人    /u/benxben13   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdje64/r_need_endorsement_on_arxiv_csai/</guid>
      <pubDate>Sun, 27 Oct 2024 19:42:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与 Leland McInnes 的最新访谈：UMAP、HDBSCAN 和数据几何 | 从机器学习中学习 #10</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdimqa/d_new_interview_with_leland_mcinnes_umap_hdbscan/</link>
      <description><![CDATA[      本期《从机器学习中学习》与 Leland McInnes 一起探索纯数学与现代数据科学的交集，Leland McInnes 是无监督学习工具生态系统背后的思想者，包括 UMAP、HDBSCAN、PyNN Descent 和 DataMapPlot。作为 Tutte 数学和计算研究所的研究员，McInnes 从根本上改变了我们处理和理解复杂数据的方式。 抵制追逐炒作的冲动，寻求真正的理解并真正有所作为。    提交人    /u/NLPnerd   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdimqa/d_new_interview_with_leland_mcinnes_umap_hdbscan/</guid>
      <pubDate>Sun, 27 Oct 2024 19:09:02 GMT</pubDate>
    </item>
    <item>
      <title>特征工程与交叉验证 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdbgzt/cross_validation_with_feature_engineering_d/</link>
      <description><![CDATA[您可以使用交叉验证来通知特征的增加/减少吗？还是仅用于超参数调整？如果两者兼而有之，您通常会使用交叉验证来选择特征，然后冻结特征并再次运行交叉验证来调整超参数吗？试图了解结合两者的迭代过程会是什么样子。     提交人    /u/Secret_Valuable_Yes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdbgzt/cross_validation_with_feature_engineering_d/</guid>
      <pubDate>Sun, 27 Oct 2024 13:54:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求有关提高以单一特征为主的 Instacart 购物篮分析模型的稳健性的建议。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd9q9f/d_seeking_advice_on_improving_robustness_of/</link>
      <description><![CDATA[大家好， 我正在 Kaggle 上开展 Instacart 购物篮分析项目，专注于预测重复购买。我设计了一个名为 num_of_ord_purch_p_prod 的功能，表示特定用户购买特定产品的次数。虽然此功能具有很高的预测性，但它在各个模型中都占据了功能重要性的主导地位，这引发了对潜在过度依赖和模型稳健性的担忧。有关更多详细信息，请参阅我的特征工程笔记本的链接：笔记本。 项目详细信息：  类别平衡：目标类别（重新排序状态）是平衡的。 评估方法：我在两个单独的测试集上测试了模型，每个测试集仅包含最新的订单，以进行更多基于时间的验证。 模型性能： 我的最佳 LightGBM 模型实现了 0.85 的 AUC，与第二佳特征相比，num_of_ord_purch_p_prod 的重要性提高了 1000 倍。 我的最佳 XGBoost模型实现了 0.72 的 AUC，与第二佳特征相比，同一特征的重要性提高了 20 倍。  特征重要性：SHAP 分析证实了 num_of_ord_purch_p_prod 的重要性，即使在应用正则化技术之后也是如此。在 XGBoost 中，正则化降低了其主导性，但也降低了 AUC。  使用中的功能：除了 num_of_ord_purch_p_prod 之外，我还包含了以下功能：  frequency_of_reorder（用户重新订购产品的频率） product_mean_of_position（用户订单中的平均产品位置） prob_of_being_reordered（基于过去购买的重新订购概率） count_ord_no_prev_purchased_items（每个订单中的新商品数量） 每周每天的产品订单分布数量等。  征求建议：鉴于单一特征占主导地位，我正在寻找建议，以增强模型的稳健性和泛化能力，同时降低 auc（&gt; 0.80）。    提交人    /u/ds_reddit1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd9q9f/d_seeking_advice_on_improving_robustness_of/</guid>
      <pubDate>Sun, 27 Oct 2024 12:21:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 有人有 wikitext-2-v1.zip 数据集文件或其他链接可以下载吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd36p9/r_does_anyone_have_wikitext2v1zip_dataset_file_or/</link>
      <description><![CDATA[大家好， 我正在尝试重现一个使用 wikitext-2 数据集的旧实验，并且它依赖于 torchtext 来导入它。但是，下载数据集的链接似乎不再有效。以下是损坏的链接： https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip 以下是相关的 torchtext 源代码，供参考： https://pytorch.org/text/0.12.0/_modules/torchtext/datasets/wikitext2.html 有人知道更新的链接或获取此数据集的解决方法吗？谢谢！    提交人    /u/reddo-lumen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd36p9/r_does_anyone_have_wikitext2v1zip_dataset_file_or/</guid>
      <pubDate>Sun, 27 Oct 2024 04:35:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 27 Oct 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 交叉验证后在完整数据集上进行训练？语义分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gct22r/d_train_on_full_dataset_after_crossvalidation/</link>
      <description><![CDATA[我目前正在进行燕麦叶病症状的语义分割项目。数据集很小，只有 16 张图像。由于时间限制，我无法扩展它。 我目前正在训练 3 个模型、3 个主干和 3 个损失 - 使用 5 倍交叉验证和网格搜索。 完成后，我计划对每个图像的几个不同级别的增强进行交叉验证。 我的问题是： 一旦我确定了最佳模型、主干、损失和增强组合，我可以在如此小的整个数据集上进行训练吗？如果我能做到这一点，我怎么知道何时停止训练以防止过度拟合但仍然充分学习数据？ 到目前为止，我附上了一些结果的图片。 https://preview.redd.it/sx394c58l5xd1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=3cefbf5c84bf3fbf48936c47810c4e3039dcb410 感谢您提供的任何帮助提供！    由   提交  /u/Entire_Commission169   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gct22r/d_train_on_full_dataset_after_crossvalidation/</guid>
      <pubDate>Sat, 26 Oct 2024 19:38:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用神经网络进行形状限制回归</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/</link>
      <description><![CDATA[前段时间在工作中，我们必须强制我们的模型学习一个特征的递增函数。例如，作为出价函数的中标概率应该增加。最近，我偶然发现了一篇关于使用形状限制函数进行回归的论文 https://arxiv.org/abs/2209.04476，我想通过实际的代码来训练这样的模型，让它更具体一些。 因此，我写了一篇博文：https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html 还有一个带有随附代码的笔记本：https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape_constrained_models.ipynb 我以前经常做广告。所以这种模型在这个行业似乎很有用——根据出价预测赢得广告拍卖的概率。我希望它在其他地方也有用。 所以我希望你会喜欢它！这是一个很大的“数学”，但你知道，它不可能是别的。    提交人    /u/alexsht1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/</guid>
      <pubDate>Sat, 26 Oct 2024 16:58:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 任何设备上的实时角色动画</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/</link>
      <description><![CDATA[      我最近看到了阿里巴巴的这篇论文MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling，真的很有趣。浏览完论文后，我想，“嘿，可以使用一些开源工具复制这个工作流程！”我设法创建了一个可行的系统，该系统可以在设备上以 ~10fps 的速度实时运行，请注意，这是在一台配备 8 GB RAM 和 4 GB VRAM 的土豆笔记本电脑上运行的。 原始视频 重建视频 当前工作流程如下所示 -&gt; 1. 我使用 Tracking4All 创建了一个 Unity 应用程序，它可以从网络摄像头获取输入并使用 Mediapipe 生成动画姿势。 2. 接下来，我将这些生成的图像发送到 Python 服务器，该服务器接收原始帧、动画角色以及来自 Mediapipe 姿势的人物面具。 3. 最终使用 MI-GAN，我能够实时移除人物。 这个项目目前存在一些缺陷 1. MI-GAN 模型虽然速度很快，但却是主要的瓶颈。我尝试了 OpenCV 中提供的其他算法，但它们更糟糕、更慢（~1fps）。 2. 角色大小调整并不总是准确的，尽管可以在 Unity 中轻松调整。 3. 遮挡问题仍然是一个挑战。 此外，值得注意的是，Tracking4All 软件包需要许可证，这可能会限制可访问性。 是否有任何算法可以在各种设备（移动设备、Windows、Mac 和 Linux）上实时执行修复？ 该项目的目标是创建任何人都可以在任何设备上运行的端到端工作流程。这在 AR 和 VFX 中有许多应用程序！您对此有何看法？我接下来应该实现什么？   由    /u/Jazzlike-Shake4595  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/</guid>
      <pubDate>Sat, 26 Oct 2024 15:49:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>