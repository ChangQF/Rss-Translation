<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Tue, 26 Mar 2024 21:12:57 GMT</lastBuildDate>
    <item>
      <title>[P] 多变量目标模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bohb7d/p_model_with_multi_variable_target/</link>
      <description><![CDATA[大家好。我需要训练一个模型，其中目标是概率向量，即 [0.2, 0.4。 0.1, 0.3] 使得其分量之和为 1。我正在考虑使用具有交叉熵损失的分类器，但我不确定这是正确的解决方案，因为此类分类器通常针对 [0,0 ,1,0...] 即 - 只有一个分量等于 1，然后可以证明分类器实际上学习生成分类器最佳候选上具有最高值的分布。我需要类似但不一样的东西，即具有 kl 散度损失函数或类似“距离”的模型概率向量之间，有什么想法或参考吗？   由   提交 /u/notSheOrThemOrIt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bohb7d/p_model_with_multi_variable_target/</guid>
      <pubDate>Tue, 26 Mar 2024 19:55:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Llama-2 处理大型文档的最佳方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bog35u/d_best_way_to_process_large_documents_with_llama2/</link>
      <description><![CDATA[大家好， 我正在开发一个项目，需要处理有时超过 100 页的文档。我正在使用 OCR 来抓取文本，并打算向法学硕士提供删除某些个人身份信息的指示。  有时，第 15 页（例如）上的信息会提供有关需要从第 86 页（例如）中删除哪些信息的线索。我必须保留上下文，因此所有 100 个文档必须以某种方式连接在一起。被视为 PII 的事物可能像“祖母”或“接待员”一样无辜，它仅依赖于符合 PII 资格的上下文（情况各不相同）。  我认为处理它的最佳方法如下：  创建一个训练数据集，其中我有多个单页文档文本作为输入，以及重要信息（有助于将该页面与其他页面联系起来的信息）作为输出。 在此数据集上训练 Llama-2 以创建“全局上下文创建者”LLM 。  在由单个页面 + 全局上下文组成的数据集上训练单独的 Llama-2 作为输入，并删除相关 PII 的页面文本作为输出。  对于文档中的每个页面，发送全局上下文 + 提示（这将是要删除哪个 PII 的说明），并接收删除 PII 的输出。   这是解决最大代币问题的唯一方法。还有其他人有什么想法吗？   由   提交 /u/HideousOstrich   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bog35u/d_best_way_to_process_large_documents_with_llama2/</guid>
      <pubDate>Tue, 26 Mar 2024 19:07:02 GMT</pubDate>
    </item>
    <item>
      <title>语言模型的时间理解[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bofewh/temporal_understanding_of_language_models_d/</link>
      <description><![CDATA[有什么好的资源吗？   由   提交/u/One_Definition_8975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bofewh/temporal_understanding_of_language_models_d/</guid>
      <pubDate>Tue, 26 Mar 2024 18:40:08 GMT</pubDate>
    </item>
    <item>
      <title>ACL 2024 评论 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1boea3w/acl_2024_reviews_discussion/</link>
      <description><![CDATA[ACL 2024（ARR 2 月）审核的讨论主题。 我的稳健性得分为 3、3、4。你们呢？   由   提交/u/EDEN1998  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1boea3w/acl_2024_reviews_discussion/</guid>
      <pubDate>Tue, 26 Mar 2024 17:55:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 单智能体或多智能体强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bod414/d_single_agent_or_multiagent_reinforcement/</link>
      <description><![CDATA[社区您好， 我目前正在研究代理可以具有不同输出结构的情况。此外，我利用图神经网络来描述状态，其中包含许多类别，这些类别的数量在代理之间可能有所不同。考虑到这种情况，什么配置最合适？  在多代理设置中，在我们的例子中，代理通常不会同时请求操作。在决策点，我们可能有一个或多个代理寻求行动，而其他代理则很忙。在这种情况下，批评者网络应该接收什么输入？ 提前谢谢   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bod414/d_single_agent_or_multiagent_reinforcement/</guid>
      <pubDate>Tue, 26 Mar 2024 17:08:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 代理操作系统 - 罗格斯大学 2024 - AIOS</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bod1lq/r_llm_agent_operating_system_rutgers_university/</link>
      <description><![CDATA[     &lt; /td&gt; 论文：https://arxiv .org/abs/2403.16971 Github：https://github.com/agiresearch/AIOS 摘要：  基于大语言模型 (LLM) 的智能代理的集成和部署充满了挑战，这些挑战影响了其效率和功效。这些问题包括代理通过 LLM 请求的次优调度和资源分配、代理与 LLM 之间交互期间维护上下文的困难，以及集成具有不同功能和专业的异构代理所固有的复杂性。 代理数量和复杂性的快速增加进一步加剧了这些问题，通常会导致资源瓶颈和次优利用。受这些挑战的启发，本文提出了AIOS，一种LLM代理操作系统，它将大型语言模型嵌入到操作系统（OS）中。具体来说，AIOS旨在优化资源分配，促进跨代理的上下文切换，实现代理的并发执行，为代理提供工具服务，并维护代理的访问控制。我们提出了这样一个操作的架构系统，概述其旨在解决的核心挑战，并提供 AIOS 的基本设计和实施。我们对多个代理并发执行的实验证明了我们的 AIOS 模块的可靠性和效率。通过这一点，我们的目标不仅是提高LLM代理的性能和效率，而且是未来更好地开发和部署AIOS生态系统的先锋。   https:/ /preview.redd.it/o6uqo550npqc1.jpg?width=1329&amp;format=pjpg&amp;auto=webp&amp;s=a63e616a0e6bd4a29bc898544325275f0f65c7a2 https://preview.redd.it/onsep550npqc1.jpg?width=601&amp;format=pjpg&amp;auto=webp&amp;s =d5463b3f0ab98e61d7281895f9d38bed754da93d   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bod1lq/r_llm_agent_operating_system_rutgers_university/</guid>
      <pubDate>Tue, 26 Mar 2024 17:05:57 GMT</pubDate>
    </item>
    <item>
      <title>现代信息检索的一些优秀讲座视频[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1boc7jr/some_good_lecture_videos_on_modern_information/</link>
      <description><![CDATA[有没有一些涉及现代方面的信息检索好的讲座视频。   由   提交/u/One_Definition_8975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1boc7jr/some_good_lecture_videos_on_modern_information/</guid>
      <pubDate>Tue, 26 Mar 2024 16:32:11 GMT</pubDate>
    </item>
    <item>
      <title>有人使用 Google 的 GPUDirect TCPX 吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1boc1ag/anyone_using_googles_gpudirect_tcpx_d/</link>
      <description><![CDATA[https://cloud.google .com/compute/docs/gpus/gpudirect 有人使用这个吗？您的工作流程是如何适应这种情况的？有困难吗？   由   提交/u/Head-Technology-5029   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1boc1ag/anyone_using_googles_gpudirect_tcpx_d/</guid>
      <pubDate>Tue, 26 Mar 2024 16:25:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 零均值泄漏 ReLu</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bo8idx/r_zero_mean_leaky_relu/</link>
      <description><![CDATA[         ;由   提交 /u/1nyouendo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bo8idx/r_zero_mean_leaky_relu/</guid>
      <pubDate>Tue, 26 Mar 2024 13:55:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大型科技公司中典型的“研究科学家”的职责是什么样的？是什么决定了您是否能够继续以这个职位发表论文（获得博士学位后继续从事）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bnz8nt/d_what_do_the_responsibilities_of_a_typical/</link>
      <description><![CDATA[我正在面试一家大型科技公司的此类职位。我通过一则写着“机器学习研究科学家”的广告进行了申请。我很乐意继续进行研究并发表论文。我在这家公司见过很多研究科学家，他们每年都会发表相当多的论文。但有些事情告诉我，并不是每个“研究科学家”都会处于这样的位置。那么什么决定了我将获得的工作类型/责任/自由？具体来说，是什么决定了我是否可以自由地进行研究和发表，而不是做公司内部的事情？   由   提交/u/Yalkim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bnz8nt/d_what_do_the_responsibilities_of_a_typical/</guid>
      <pubDate>Tue, 26 Mar 2024 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我的二元向量搜索比你的 FP32 向量更好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bnxum3/d_my_binary_vector_search_is_better_than_your/</link>
      <description><![CDATA[https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors 向量领域内搜索中出现了一个有趣的发展：二元向量搜索。这种方法有望将内存消耗显着减少 30 倍，从而解决长期存在的内存消耗问题。然而，引发争论的一个关键方面是它对准确性的影响。 我们相信，使用二元向量搜索以及特定的优化技术可以保持类似的准确性。为了阐明这个主题，我们展示了一系列实验，以证明这种方法的效果和含义。 ​ 您以前尝试过二元向量吗？    由   提交/u/gaocegege  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bnxum3/d_my_binary_vector_search_is_better_than_your/</guid>
      <pubDate>Tue, 26 Mar 2024 03:23:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 就业后如何继续学习并与时俱进</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bnvv8e/d_how_to_keep_studying_and_up_to_date_after_being/</link>
      <description><![CDATA[对于正在全职工作的你们来说：你们还在业余时间学习吗？如果没有，您如何跟上这些框架以及每天出现的新事物？ 除了每天全职工作 9 小时之外，我还有其他义务（每周治疗、英语课程、硕士课程）课程、健身房、社交生活等），因此，我正在努力继续学习。有些东西是我想学习的，但在我的工作项目中没有机会学习（例如高级 NLP 和 MLOps），或者我在面试时必须记住的基础知识。我是这个领域的新手（只有 3 岁），欢迎任何建议。    由   提交/u/deadknxght   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bnvv8e/d_how_to_keep_studying_and_up_to_date_after_being/</guid>
      <pubDate>Tue, 26 Mar 2024 01:49:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 近期 AI 会议同行评论中有多达 17% 由 ChatGPT 撰写</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/</link>
      <description><![CDATA[一项新研究发现，2023-2024 年顶级 AI 会议的同行评审中很大一部分可能包含来自 ChatGPT 等模型的大量 AI 生成内容. 使用一种新颖的统计技术，研究人员估计了人工智能在大量文档中生成的文本的百分比。通过分析同行评审，他们发现：  10.6% 的 ICLR 2024 评审包含重要的人工智能内容 9.1%（NeurIPS 2023） 6.5%（CoRL） 2023 EMNLP 2023 为 16.9%  相比之下，2022 年及更早的 ChatGPT 前评论中只有 1-2% 被标记为具有重大 AI 贡献。&lt; /p&gt; 一些关键发现：  以人工智能为主的评论往往会在接近截止日期的情况下进行 人工智能风格评论中的学术引用较少 &gt; 带有人工智能色彩的审稿人较少参与作者讨论 人工智能内容使审稿在语义上更加同质 较低的审稿人信心与较高的人工智能估计相关   ol&gt; 我认为，这项研究针对学术界在研究中负责任地使用人工智能的积极政策制定提出了一些问题。人工智能可能会通过这些“影子”来侵蚀同行评审的质量和完整性。影响。开放性问题包括：  是否应该披露人工智能在同行评审中的协助？ 尽管面临人工智能的诱惑，我们应该如何激励良好实践？ 我们能否保留人工智能同质化下的智力多样性？ 我们是否应该重新考虑人类/人工智能混合知识工作的信用？  总的来说，这是对人工智能在基础上快速增长的卷须的有趣的实证一瞥科学的质量控制！我认为测量某些人工智能词语“滴答”频率的方法是可行的。很有道理（例如，GPT4 使用的一些形容词就很清楚）。  我很好奇阅读有关此的评论！我有一个这里提供了更详细的摘要作为如果您有兴趣，那么原始论文就在这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/</guid>
      <pubDate>Mon, 25 Mar 2024 23:36:06 GMT</pubDate>
    </item>
    <item>
      <title>【讨论】论“轨迹一致性蒸馏”的抄袭行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bnmus0/discussion_on_plagiarism_of_trajectory/</link>
      <description><![CDATA[   我们遗憾地发现了我们的一致性轨迹模型（CTM、ICLR24） 被抄袭 轨迹一致性蒸馏（TCD）！令人难以置信——他们不仅窃取了我们关于轨迹一致性的想法，而且还犯下了“逐字剽窃”的罪名，逐字逐句地复制我们的证明！我们试图与 TCD 作者解决这个抄袭问题，但谈话令人失望，问题仍然没有解决。您可以在此处查看有关抄袭的更多信息。  ​ 逐字抄袭通过 TCD 对抗 CTM  TCD 针对 CTM 的抄袭行为列表   由   提交/u/Embarrassed_Aerie387  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bnmus0/discussion_on_plagiarism_of_trajectory/</guid>
      <pubDate>Mon, 25 Mar 2024 19:42:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>