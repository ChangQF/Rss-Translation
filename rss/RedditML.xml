<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Sun, 25 Aug 2024 21:13:39 GMT</lastBuildDate>
    <item>
      <title>[D] 寻找可以指导我选择教授和博士研究课题的人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f13nm3/d_looking_for_persons_who_can_mentor_me_for/</link>
      <description><![CDATA[我想攻读机器学习博士学位，但我认为我在这方面知识很少，我想研究诸如将概率图模型与深度学习相结合（尝试在深度学习中添加因果关系）之类的主题，或者诸如视觉问答或因果表示学习之类的主题，人们正在试图弄清楚系统 2 机器学习，但我不知道他们试图在哪些特定任务上构建此类模型，此类模型是否可以扩展到解决数学问题，编码，医学诊断等任务。 这里有人可以成为我的导师，解疑答惑，根据他们的研究经验提供指导吗？    提交人    /u/binny_sarita   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f13nm3/d_looking_for_persons_who_can_mentor_me_for/</guid>
      <pubDate>Sun, 25 Aug 2024 18:46:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 比利时博士学位</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f1253q/d_phd_in_belgium/</link>
      <description><![CDATA[大家好， 我是一名印度学生，目前正在巴塞罗那联邦大学 (UPF) 攻读智能交互系统硕士学位。 我想知道比利时是否有提供 ML 博士学位的好学校。我对 RL 很感兴趣，但即使是非 RL 方向的博士学位也会很棒。 任何帮助都将不胜感激！    提交人    /u/FlyTrain1011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f1253q/d_phd_in_belgium/</guid>
      <pubDate>Sun, 25 Aug 2024 17:42:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 检测和分类软件漏洞</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f117b1/p_detecting_and_classifying_software/</link>
      <description><![CDATA[大家好，我一直想做一个项目，想听听大家的想法。目标是从反汇编的可执行文件中对漏洞进行分类。我的计划是同时使用顺序信息（指令、助记符、操作数）和控制流图（基于分支）。 我选择了一些常见的漏洞，例如缓冲区溢出、整数溢出和内存泄漏，并希望将它们分类为包含这些漏洞之一或不易受攻击。Juliet C/C++ 数据集似乎是这项任务的不错选择，因为它包含以不同风格编写的约 60k 个易受攻击和不易受攻击的源代码测试用例。这些可以使用不同的编译器选项进行编译，以生成略有不同的反汇编，从而构建更大的词汇表。 我很想听听你对这个想法的看法，以及在开始之前我应该​​考虑哪些主要障碍。    提交人    /u/SgtPepper8903   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f117b1/p_detecting_and_classifying_software/</guid>
      <pubDate>Sun, 25 Aug 2024 17:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 处理包含大量 NaN 的大型表格数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0zq8l/p_dealing_with_large_tabular_dataset_with_lot_of/</link>
      <description><![CDATA[现在我正在处理 400k 行分类表格数据集，其中包含如此多的 NaN，因此如果我执行 dropna()，它只会剩下 4 行。我现在正在进行 KNN 插补，但它花费了大量时间（在我撰写这篇文章时它还没有完成）。我的问题是，如何处理大型数据集的插补？我必须对数据集进行抽样还是其他什么？    提交人    /u/Fun_Ambition_5186   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0zq8l/p_dealing_with_large_tabular_dataset_with_lot_of/</guid>
      <pubDate>Sun, 25 Aug 2024 16:01:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 根据应用交互的视频片段生成 Gherkin 场景</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0z19p/p_generating_gherkin_scenarios_from_video_footage/</link>
      <description><![CDATA[大家好，我正在做一个项目，需要根据视频输入生成 Gherkin 场景。本质上，我想分析应用程序交互的视频片段（如光标移动、文本输入和按钮点击）并自动为 BDD 框架创建 Gherkin 测试用例。有人做过类似的事情吗？或者对如何处理这个问题有什么建议吗？你会推荐什么工具或技术？    提交人    /u/lonylegend   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0z19p/p_generating_gherkin_scenarios_from_video_footage/</guid>
      <pubDate>Sun, 25 Aug 2024 15:31:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 编码还是不编码？探索编码在预训练中的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0yh4v/r_to_code_or_not_to_code_exploring_impact_of_code/</link>
      <description><![CDATA[  由    /u/RobbinDeBank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0yh4v/r_to_code_or_not_to_code_exploring_impact_of_code/</guid>
      <pubDate>Sun, 25 Aug 2024 15:07:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Aug 2024 15:00:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GNN 进行量子机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0xys8/r_quantum_machine_learning_with_gnns/</link>
      <description><![CDATA[我刚刚写了一篇关于“使用 GNN 的量子机器学习”的研究论文。你们能帮我审阅一下吗？ https://drive.google.com/file/d/12U-owjYFgV0jrS4vThrg8DXHJF5baO7J/view?usp=drivesdk    提交人    /u/Anonymous_Life17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0xys8/r_quantum_machine_learning_with_gnns/</guid>
      <pubDate>Sun, 25 Aug 2024 14:44:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何分析梯度直方图并调试我的深度学习模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0x5jd/p_how_to_analyze_gradient_histograms_and_debug_my/</link>
      <description><![CDATA[我正在尝试训练一个模型来预测给定的声音和给定的音素相同（输出 1）或不相同（输出 0）的概率。 但损失并没有从 0.6 减少，因为随机概率为 0.5，随机预测的 BCE 损失为 0.69。 我在训练时使用权重和偏差来绘制模型的参数和梯度 这是报告：https://api.wandb.ai/links/svar-svar/61n2waag gradients_max 在正常范围内，但梯度直方图的范围是 1000，最大值怎么会小于 1，而直方图又有这样的分布。请帮助我理解如何解释这些直方图 class MLPLayer( nn.Module ): def __init__( self , hidden_​​dim1 , hidden_​​dim2 , hidden_​​dim3 , embedding_dim ) : super( MLPLayer , self ).__init__() self.layer_1 = nn.Linear( embedding_dim , hidden_​​dim1 ) self.relu_fn = nn.ReLU() self.dropout_layer_1 = nn.Dropout( p = 0.15 ) self.layer_2 = nn.Linear( hidden_​​dim1 , hidden_​​dim2 ) self.linear_3 = nn.Linear（hidden_​​dim2，hidden_​​dim3） self.linear_4 = nn.Linear（hidden_​​dim3，1） self.dropout_layer_2 = nn.Dropout（p = 0.15） def forward（self，x）： out1 = self.relu_fn（self.layer_1（x）） out1 = self.dropout_layer_1（out1） out1 = self.relu_fn（self.layer_2（out1）） out1 = self.dropout_layer_2（self.relu_fn（self.linear_3（out1 out2 = self.linear_4 ( out1 ) out = torch.sigmoid( out2 ) return out class PhonemeEmbedding(nn.Module): def __init__(self, embedding_dim=24, max_len=1000, device=&#39;cpu&#39;): super(PhonemeEmbedding, self).__init__() self.max_len = max_len self.embedding_dim = embedding_dim self.device = device self.embedding_layer = nn.Embedding（num_embeddings=self.max_len，embedding_dim=self.embedding_dim） def forward（self，x）： 返回self.embedding_layer（x） dta_model = CustomWav2Vec2ForCTC（base_model_name=&quot;facebook/wav2vec2-base-960h&quot;） dta_model.load_state_dict(torch.load(&quot;/kaggle/input/lates-model/model_dta_model_epoch_7.pth&quot; , map_location = &quot;cuda&quot;)) 这是主要架构，我将音频通过 Wav2Vec2，并为所有音素嵌入音素，将这两个音素连接起来，然后在上面的 mlp 中对它们进行处理。 我注意到的另一件事是，嵌入层在整个训练过程中保持高斯分布，如何防止这种情况并让它学习音素声音的概念。    提交人    /u/Agreeable_Ad_1085   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0x5jd/p_how_to_analyze_gradient_histograms_and_debug_my/</guid>
      <pubDate>Sun, 25 Aug 2024 14:08:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] Jamba-1.5：大规模混合 Transformer-Mamba 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wvnz/r_jamba15_hybrid_transformermamba_models_at_scale/</link>
      <description><![CDATA[      TL;DR: 大型（高达 94B/398B 活跃/总参数）混合开放权重模型，高达 256k 上下文 论文： https://arxiv.org/pdf/2408.12570 博客： https://www.ai21.com/blog/announcing-jamba-model-family 摘要：  我们提出了 Jamba-1.5，这是基于我们的 Jamba 架构的新型指令调整大型语言模型。Jamba 是 Transformer-Mamba 混合专家架构，可在上下文长度上提供高吞吐量和低内存使用率，同时保持与 Transformer 模型相同或更好的质量。我们发布了两种模型大小：Jamba-1.5-Large，具有 94B 个活动参数，以及 Jamba-1.5-Mini，具有 12B 个活动参数。这两种模型都针对各种对话和指令遵循功能进行了微调，并且具有 256K 个标记的有效上下文长度，这是开放权重模型中最大的。为了支持经济高效的推理，我们引入了 ExpertsInt8，这是一种新颖的量化技术，允许在具有 8 个 80GB GPU 的机器上安装 Jamba-1.5-Large，同时处理 256K 个标记上下文而不会损失质量。在一系列学术和聊天机器人基准测试中，Jamba-1.5 模型取得了出色的结果，同时提供了高吞吐量，并且在长上下文基准测试中优于其他开放权重模型。两种尺寸的模型权重均在 Jamba 开放模型许可下公开提供，我们将 ExpertsInt8 作为开源发布。  视觉亮点： 在混合架构中，Mamba-1 块的表现优于 Mamba-2 块。当模型具有注意力时，Mamba-2 处理细节可能是多余的 https://preview.redd.it/onrdw742ftkd1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=dff7d4dd10ccc0b8d1481bab7de084cb2ac1b586 https://preview.redd.it/nm79gn64ftkd1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=650327eba37add3af6fd629371b98010789f10a2 https://preview.redd.it/yd3l7k46ftkd1.png?width=1115&amp;format=png&amp;auto=webp&amp;s=bce413b0f95ec2cb786cb388589cbe22c06d5ca9 https://preview.redd.it/qe4choo7ftkd1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=eca7f428af74099d15dd1ebd7db1cdbe7d6d721e https://preview.redd.it/vy26ido9ftkd1.png?width=1109&amp;format=png&amp;auto=webp&amp;s=b6b2ee51a650c2910a01c465a810e33ad7880ebb 无限长凳 https://preview.redd.it/61dfzmugftkd1.png?width=1147&amp;format=png&amp;auto=webp&amp;s=c53aa8a8d43b49aeb81871db9f25227874cc34ad 下载：https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251    由   提交  /u/StartledWatermelon   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wvnz/r_jamba15_hybrid_transformermamba_models_at_scale/</guid>
      <pubDate>Sun, 25 Aug 2024 13:55:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习到底发生了什么？一些最小模型 (Stephen Wolfram)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wj6s/r_whats_really_going_on_in_machine_learning_some/</link>
      <description><![CDATA[Stephen Wolfram 最近发表了一篇博客文章，其中提出了一些关于离散神经网络的有趣观点，从自动机的角度来看待训练： https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wj6s/r_whats_really_going_on_in_machine_learning_some/</guid>
      <pubDate>Sun, 25 Aug 2024 13:38:16 GMT</pubDate>
    </item>
    <item>
      <title>有人在机器学习中使用合成数据吗？它对你的项目有何影响？[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wbfy/anyone_actually_using_synthetic_data_in_ml_how/</link>
      <description><![CDATA[我很好奇您在机器学习项目中实际使用的合成数据的实际应用。 它是否真正增强了您的流程或结果？您在使用它时面临的最大挑战是什么？ 我很想听听您的经历——好的和坏的。    提交人    /u/Value-Forsaken   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wbfy/anyone_actually_using_synthetic_data_in_ml_how/</guid>
      <pubDate>Sun, 25 Aug 2024 13:27:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] 生产中的机器学习：从数据科学家到机器学习工程师</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</link>
      <description><![CDATA[我很高兴与大家分享我整理的一门课程：生产中的机器学习：从数据科学家到机器学习工程师。本课程旨在帮助您从 Jupyter 笔记本中获取任何 ML 模型并将其转变为可用于生产的微服务。 本课程涵盖以下内容：  将您的 Jupyter 代码构建为生产级代码库 管理数据库层 参数化、日志记录和最新的干净代码实践 使用 GitHub 设置 CI/CD 管道 为您的模型开发 API 容器化您的应用程序并使用 Docker 进行部署  我很乐意收到您对本课程的反馈。这是免费访问的优惠券代码：FREETOLEARN24。您的见解将帮助我改进和完善内容。如果您喜欢本课程，我希望您留下好评，以便其他人也可以找到这门课程。谢谢，祝您学习愉快！    提交人    /u/5x12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</guid>
      <pubDate>Sat, 24 Aug 2024 20:54:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] Liger Kernel：一行代码使 LLM 培训速度提高 20%，内存减少 60%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</link>
      <description><![CDATA[        提交人    /u/Icy-World-8359   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</guid>
      <pubDate>Sat, 24 Aug 2024 15:39:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>