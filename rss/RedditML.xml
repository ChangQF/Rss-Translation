<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Mon, 12 Aug 2024 15:17:17 GMT</lastBuildDate>
    <item>
      <title>[D] 建立AI团队</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eqfgjh/d_building_ai_team/</link>
      <description><![CDATA[大家好，我想你们中的一些人已经意识到，开发 AI 代理是许多 B2C 或 B2B 组织的未来。我正在做一个与 AI 代理相关的项目。我正在组建一个团队和一家公司，我想和一些人合作。有没有人感兴趣并且有时间做这件事？    提交人    /u/ResponsibleCandle585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eqfgjh/d_building_ai_team/</guid>
      <pubDate>Mon, 12 Aug 2024 14:39:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 1.5-Pints 技术报告：几天内完成预训练，而不是几个月——您的语言模型依靠优质数据蓬勃发展 (2408.03506)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eqdbro/r_15pints_technical_report_pretraining_in_days/</link>
      <description><![CDATA[  由    /u/mouse0_0  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eqdbro/r_15pints_technical_report_pretraining_in_days/</guid>
      <pubDate>Mon, 12 Aug 2024 13:10:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 提示与偏见</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eqc5nt/r_prompt_and_prejudice/</link>
      <description><![CDATA[  由    /u/Cioni  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eqc5nt/r_prompt_and_prejudice/</guid>
      <pubDate>Mon, 12 Aug 2024 12:16:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 文本到语音模型如何产生情感？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eqaczc/d_how_do_text_to_speech_models_generate_emotions/</link>
      <description><![CDATA[他们在语音转语音模型中提取了什么样的特征，可以帮助他们模仿与原始说话者相同的情感。这在文本转语音模型中是如何工作的，因为在这样的模型中，没有这样的特征直接传递给模型。这都是随机的吗？我想知道这一点，特别是对于商业产品。任何关于这方面的研究论文也将受到高度赞赏。    提交人    /u/Just_Difficulty9836   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eqaczc/d_how_do_text_to_speech_models_generate_emotions/</guid>
      <pubDate>Mon, 12 Aug 2024 10:40:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何处理时间序列数据中事件发生和标签报告之间的时间延迟？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq9mqx/d_how_to_handle_time_delays_between_event/</link>
      <description><![CDATA[大家好， 我正在开展一个项目，旨在使用租赁汽车的传感器数据实时检测车辆盗窃。我的数据集包括这些车辆的带有时间戳的传感器读数，我还有一个文件，其中显示过去哪些车辆被报告被盗。 我认为某些驾驶行为与盗窃密切相关（例如，高速、长时间驾驶、跨境驾驶等）。 我面临的挑战是盗窃报告通常是在实际盗窃发生几天后提交的，这导致传感器数据的时间戳与盗窃的实际时间不一致。这种延迟使得很难准确地将传感器数据与盗窃的发生联系起来，我担心这可能会影响我的模型实时检测盗窃的能力。 最初，我考虑使用自动编码器或隔离森林进行无监督异常检测，但这种方法可能不起作用，原因有二：  盗窃并非极为罕见 由于数据来自租赁汽车，因此由于每辆车的驾驶员各不相同，因此不存在“正常”驾驶行为。  以前有人处理过类似的问题吗？如果您能就预处理技术或模型策略提出任何有助于缓解此问题的建议，我将不胜感激。 提前感谢您的见解！    提交人    /u/Laippe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq9mqx/d_how_to_handle_time_delays_between_event/</guid>
      <pubDate>Mon, 12 Aug 2024 09:54:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 有没有关于新的 Flux 图像生成模型的论文/技术报告？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq9det/r_is_there_any_papertechnical_report_about_the/</link>
      <description><![CDATA[我发现 X 和 LinkedIn 被 Black Forest Labs 发布的新型 Flux 模型产生的一些令人印象深刻的结果所取代。 有没有关于这些模型的技术报告或论文？我似乎找不到任何东西。如果没有，有没有相关论文可以指出所使用的方法？（我想 Rombach 最近的工作可能是一个很好的起点）    提交人    /u/ats678   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq9det/r_is_there_any_papertechnical_report_about_the/</guid>
      <pubDate>Mon, 12 Aug 2024 09:36:37 GMT</pubDate>
    </item>
    <item>
      <title>[P]如何检测数字页面上的图形？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq96bk/phow_to_detect_graphs_on_digital_pages/</link>
      <description><![CDATA[我已经制作了一个检测表格的模型，作为它的扩展，我想制作一个可以检测打印页面上的图表的模型。我不是在寻找确切的想法，这更像是一个从哪里开始的问题。任何预先存在的模型也将受到赞赏以提供灵感。     提交人    /u/SnooAdvice1157   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq96bk/phow_to_detect_graphs_on_digital_pages/</guid>
      <pubDate>Mon, 12 Aug 2024 09:23:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 新的 Llama 缩放定律？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq95ga/d_new_llama_scaling_laws/</link>
      <description><![CDATA[&quot;在开发 Llama 3 的过程中，我们对扩展行为进行了一些新的观察。例如，虽然 8B 参数模型的 Chinchilla 最佳训练计算量对应于 ~200B 个 token，但我们发现即使在使用两个数量级以上的数据训练模型后，模型性能仍会继续提高。在我们使用高达 15T 的 token 进行训练后，我们的 8B 和 70B 参数模型都继续以对数线性方式提高。较大的模型可以用较少的训练计算来匹配这些较小模型的性能，但较小的模型通常是首选，因为它们在推理过程中效率更高。&quot; 这是摘自Meta Llama 3 介绍：迄今为止最强大的公开可用 LLM 您认为 Llama 刚刚引入了新的扩展定律吗？ Chinchilla 之前为什么没发现这一点？模型大小与 token 数量的比例有没有新的数字？    提交人    /u/akashkash   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq95ga/d_new_llama_scaling_laws/</guid>
      <pubDate>Mon, 12 Aug 2024 09:21:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformers 是通用的上下文学习器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq5bkz/r_transformers_are_universal_incontext_learners/</link>
      <description><![CDATA[这项工作探索了 Transformer 的表现力，重点关注它们处理任意数量上下文标记的能力。值得注意的是，单个 Transformer 可以处理具有固定嵌入维度和恒定数量头的无限数量的标记。注意层之间 MLP 层的使用也受到严格监管    提交人    /u/AhmedMostafa16   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq5bkz/r_transformers_are_universal_incontext_learners/</guid>
      <pubDate>Mon, 12 Aug 2024 05:06:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用断路器提高一致性和稳健性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq4c77/r_improving_alignment_and_robustness_with_circuit/</link>
      <description><![CDATA[  由    /u/AhmedMostafa16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq4c77/r_improving_alignment_and_robustness_with_circuit/</guid>
      <pubDate>Mon, 12 Aug 2024 04:10:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多智能体模仿学习：价值很容易，遗憾很难</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq4aik/r_multiagent_imitation_learning_value_is_easy/</link>
      <description><![CDATA[  由    /u/AhmedMostafa16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq4aik/r_multiagent_imitation_learning_value_is_easy/</guid>
      <pubDate>Mon, 12 Aug 2024 04:07:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于撰写基准论文的优点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq22hg/d_pros_about_writing_a_benchmark_paper/</link>
      <description><![CDATA[作为一个从未写过论文提出基准的人，我只能想象写一篇论文会有什么见解/收获。也许发现模型的底层性能就是其中之一。对于那些写过类似文章的人，你觉得你的经验有价值吗？你会推荐吗？    提交人    /u/Haunting_Air3071   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq22hg/d_pros_about_writing_a_benchmark_paper/</guid>
      <pubDate>Mon, 12 Aug 2024 02:08:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研讨会：可扩展的无 MatMul 语言建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epz1zr/r_workshop_scalable_matmulfree_language_modeling/</link>
      <description><![CDATA[摘要  矩阵乘法 (MatMul) 通常占据大型语言模型 (LLM) 总体计算成本的主导地位。随着 LLM 扩展到更大的嵌入维度和上下文长度，此成本只会增加。在这项工作中，我们表明 MatMul 操作可以完全从 LLM 中消除，同时在十亿参数规模下保持强劲性能。我们的实验表明，我们提出的无 MatMul 模型实现了与最先进的 Transformers 相当的性能，后者在推理期间需要更多的内存，规模至少达到 2.7B 参数。我们研究了缩放规律，发现我们的无 MatMul 模型和全精度 Transformers 之间的性能差距随着模型尺寸的增加而缩小。我们还提供了此模型的 GPU 高效实现，与未优化的基线相比，在训练期间可将内存使用量降低高达 61%。通过在推理过程中使用优化的内核，与未优化的模型相比，我们的模型的内存消耗可以减少 10 倍以上。为了正确量化我们架构的效率，我们在 FPGA 上构建了一个自定义硬件解决方案，该解决方案利用了 GPU 无法处理的轻量级操作。我们以 13W 的功耗处理了十亿参数规模的模型，超出了人类可读的吞吐量，使 LLM 更接近类似大脑的效率。这项工作不仅展示了 LLM 在保持有效运行的情况下可以剥离到何种程度，而且还指出了未来加速器在处理下一代轻量级 LLM 时应该优化的操作类型。  论文： 链接到 arXiv 仓库： 链接到 GitHub 研讨会 大家好，我们将与最近的论文“可扩展的无 MatMul 语言建模”的第一作者一起举办一个免费的在线研讨会！Ridger Zhu 将介绍一些关于论文和研究领域的幕后花絮，以及一些关于实施细节的说明。我们希望将人数控制在较小规模，以便 Ridger 有机会回答每个人的问题，因此将按照先到先得的原则进行，容量有限。 时间： 8 月 15 日下午 6 点（太平洋夏令时间） 地点： 在线（通过 Zoom） 加入候补名单： 在此处注册（我们将在临近日期时发送包含会议详细信息的电子邮件）    提交人    /u/Dankiest_Of_Memes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epz1zr/r_workshop_scalable_matmulfree_language_modeling/</guid>
      <pubDate>Sun, 11 Aug 2024 23:43:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持有效，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢所有人在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Aug 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>