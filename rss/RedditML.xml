<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 09 Feb 2024 00:56:12 GMT</lastBuildDate>
    <item>
      <title>[D] 具有累积金额的 Mamba</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amb3xu/d_mamba_with_cumulative_sums/</link>
      <description><![CDATA[Mamba 是一个带有数据的状态空间模型相关系数。它最初是通过关联扫描进行训练的，目前是pytorch 不直接支持，因此作者为其编写了自定义 cuda 内核（这具有内核融合的额外好处）。为了简化这一点，有人在一个文件中编写了 最小版本的 mamba，其中关联扫描操作被 for 循环取代，为了实现的简单性而牺牲了效率。 不过，我认为有一种方法可以在纯pytorch中实现mamba，并且不会损失太多效率，那就是使用累积和 pytorch 有效支持。此实现封装在我对最小 mamba 存储库的相当简单的 commit 中，它提供了大约 14 倍的加速最小的 for 循环实现（代码较少）。还通过与 for 循环实现的输出进行比较来验证其正确性。 高级思想基本上是“分解”循环。将原始并行扫描降低到两个累加和的比率，同时保留关联扫描相同的时间复杂度 O(n) 和并行效率 O(logn)。  &amp; #32；由   提交/u/dna961010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amb3xu/d_mamba_with_cumulative_sums/</guid>
      <pubDate>Fri, 09 Feb 2024 00:39:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] PC组件思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amatjs/d_pc_component_thoughts/</link>
      <description><![CDATA[所以我想为 ML 构建一台电脑，而且我真的对 RL 更感兴趣 我得到了一个 7900gre 所以 gpu 是尽管我知道 NVidia 在这个市场上表现出色，但我有点希望 AMD 能够加强 ROCm 或他们拥有的其他一些开放魔法，这样这款 GPU 至少是不错的（如果我可以与任何 NVidia GPU 进行比较，它会太棒了，这样我就知道我对它的立场了，比如它比 3060 更好吗？考虑到所有因素） 接下来是 ram 和 CPU，我想知道 cpu 数量、线程数量、cpu 是多少时钟，缓存大小对 RL 很重要，我正在研究 7600x（如果它真的不重要）/ 7700（如果它有点重要），或者 7900x（如果它真的很重要），也正在研究 32 GB 内存或 64 GB，如果这真的很重要（速度重要吗？） 我知道这篇文章到处都是，但我不知道如何构建它，我有很多问题，总体来说 cpu/ram 影响有多大ML/RL 应用？   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amatjs/d_pc_component_thoughts/</guid>
      <pubDate>Fri, 09 Feb 2024 00:25:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR/ICCV/ECCV 研讨会或 Q1 期刊？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ama7n7/d_cvpriccveccv_workshops_or_q1_journals/</link>
      <description><![CDATA[您好，我有一篇论文，想发表它，诉讼日期已经过去，我认为该论文也可能不会在诉讼中被接受，因为结果适用于小型数据集。那么是发表在q1期刊上好还是发表在顶级会议的workshop上好呢？而且workshop很多，如何知道好不好呢？这篇论文非常好，很容易被 Q1 期刊接受，但我已经在 Q1 期刊上发表过论文，所以我需要知道哪个更适合我发表？   由   提交 /u/Professional_Mud4298   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ama7n7/d_cvpriccveccv_workshops_or_q1_journals/</guid>
      <pubDate>Thu, 08 Feb 2024 23:57:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找人工智能故事！征集算法让你惊讶的轶事</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ama474/d_seeking_ai_stories_call_for_anecdotes_of_ways/</link>
      <description><![CDATA[亲爱的同事们， 曾经遇到过人工智能巧妙地战胜了你的实验设计，或者揭示了你的奖励函数中意想不到的缺陷吗？&lt; /p&gt; 我们（Aaron Dharna、Joel Lehman、Victoria Krakovna 和 Jeff Clune）正在撰写一篇关于人工智能如何找到让我们惊喜的方法的论文。 我们正在收集此类故事来扩展我们之前的作品数字进化的惊人创造力：https://arxiv.org/abs/1803.03453到深度学习环境，强调人工智能安全的重要性和我们工作的不可预测性。 我们的目标是尽可能多地记录有关人工智能（任何类型，包括强化学习、机器学习等）的真实轶事，使其令人惊讶创作者和用户。因此，您的经历对于这项工作至关重要。 我们希望您能够帮助对这些有趣且有时不祥的轶事进行明确的描述，以便我们可以通过提交和/或传播信息来为人工智能安全讨论提供信息征集轶事的内容。 通过贡献，您将帮助我们的社区内外加深理解。请将您的轶事在 2024 年 3 月 1 日之前发送至 aifindsaway@gmail.com。 请随时分享以下电话宽：https://docs.google.com/document/d/1BhRWzkIYRUDjU5zon-ILXINPL4VqZp2JZXNsTjekBPk /edit?usp=sharing 让我们用集体研究冒险的见解来共同照亮前进的道路。 干杯 tl;dr 请提交（至aifindsaway@gmail.com）您所知道的有关人工智能以令其创造者惊讶的方式行事的任何故事，尤其是如果它可以被视为不安全（例如破解奖励函数、在环境或实验设计中发现漏洞、目标错误概括等）。   由   提交/u/aadharna  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ama474/d_seeking_ai_stories_call_for_anecdotes_of_ways/</guid>
      <pubDate>Thu, 08 Feb 2024 23:52:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获得人工智能/机器人硕士学位后从软件工程转向机器学习工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am88no/d_transitioning_from_software_engineering_to_ml/</link>
      <description><![CDATA[抱歉，如果这偏离了主题，但我确实需要一些建议。要获得 ML 工程师的工作，我的简历中需要的内容是今年 6 月刚毕业的人工智能/机器人硕士，具有 1.5-2 年的软件工程经验。除了我在这些课程上完成的课程和项目以及我目前正在研究的硕士项目之外，我之前没有任何机器学习经验。改进我的简历的最佳方法是什么，或者你做了什么帮助你进入机器学习工作的事情。我对此一无所知，因为我不认识任何从事机器学习工作的人，我可以向他们寻求建议。非常感谢任何帮助。   由   提交 /u/Theme_Spiritual   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am88no/d_transitioning_from_software_engineering_to_ml/</guid>
      <pubDate>Thu, 08 Feb 2024 22:30:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Gemini 和 LlamaIndex 的多模式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am7kda/d_multimodal_using_gemini_and_llamaindex/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am7kda/d_multimodal_using_gemini_and_llamaindex/</guid>
      <pubDate>Thu, 08 Feb 2024 22:01:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] MoCo的动力动力不成立？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am7bc6/d_moco_motivation_for_momentum_does_not_hold/</link>
      <description><![CDATA[MoCo 论文比较了端到端预训练后的性能（查询和密钥都来自在线编码器），经过适当的 MoCo 预训练后，并带有存储库（只有查询来自在线编码器，较旧的查询来自较旧的编码器）。端到端训练提供了与 MoCo 相同的精度，作为队列中键数量/批量大小的函数，但批量大小在常见硬件上不能增长得那么大。记忆库训练的准确性较低，作者说这证实了他们的假设，即旧密钥与查询和新密钥不一致，但我不明白！与动量编码器一样，精度随着队列长度的相同类型的函数而增长，并且键越多（并且越旧并且据说一致性越差），精度就越高，但仅低 2%。随着队列的增长，它必须变得更糟，或者回报按比例减少，才能证实作者的假设，对吧？在这种情况下，势头并不令人信服，而在 DINO 中，它有不同的解释和目的，听起来不错（而且性能显然更好）   由   提交/u/reverendCappuccino   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am7bc6/d_moco_motivation_for_momentum_does_not_hold/</guid>
      <pubDate>Thu, 08 Feb 2024 21:50:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]对以反射（自我调整）为动力的LLM代理人系列作品的关注</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/</link>
      <description><![CDATA[我们在基于 LLM 的代理中看到了大量的作品，它们可以在 Web 应用程序上执行任务，例如 webshop、webarena、agentbench等...&lt; /p&gt; 此外，我们还可以找到以下关于基于反射的智能体的工作，该智能体从与环境的交互中获取先前试验的反馈和错误。典型的工作是Reflexion: Language Agents with Verbal Reinforcement Learning 在每次试验中，代理，或者说，llm，消化提示，其中不仅包含当前试验的历史记录，还包含还有之前试验的系统信息或反馈或错误消息。反馈可以来自系统设置，也可以来自另一个更强大的LLM，可以充当超级法官来提供反馈。 无论如何，我不认为这是RL，因为代理没有学习过程，但有一个提示。 我最关心的是这个标签是否泄漏？智能体从环境中获得反馈，并通过更多的尝试，当然，智能体应该对最终答案有更清晰的方法。那么有什么意义呢？ 我看到一篇文章与我有同样的担忧：noahshinn/reflexion: [ NeurIPS 2023] 反射：具有言语强化学习的语言智能体 (github.com) ​ 想听听您从学术和工业角度的看法. ​ ​ ​ ​ &lt; /div&gt;  由   提交/u/yanancc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/</guid>
      <pubDate>Thu, 08 Feb 2024 19:11:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是什么让 PPO 强化学习不仅仅是拥有一个花哨的损失函数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</link>
      <description><![CDATA[我正在考虑使用 RLHF 训练扩散模型，并且正在查看这篇论文 kvablack/ddpo-pytorch：用于微调扩散模型的 DDPO，在具有 LoRA 支持的 PyTorch 中实现（github.com），但代码本身似乎只是反向传播基于unet的一个奇特的（乍一看是可微分的！）损失函数。强化学习与普通模型训练有何区别？两者相同吗？这只是术语问题吗？  在此处复制相关代码？ for i, example in tqdm( list(enumerate(samples_batched)), desc=f&quot;Epoch {epoch}.{inner_epoch} :training&quot;,position=0,disable=notaccelerator.is_local_main_process, ): if config.train.cfg: # 将否定提示连接到示例提示以避免两次前向传递 embeds = torch.cat( [train_neg_prompt_embeds, Sample[&quot;prompt_embeds&quot;) ;]] ) else: embeds = sample[“prompt_embeds”] for j in tqdm( range(num_train_timesteps), desc=“Timestep”,position=1, left=False,disable=not Accelerator.is_local_main_process, ): with Accumulate(unet): with autocast(): if config.train.cfg:noise_pred =unet( torch.cat([sample[&quot;latents&quot;][:, j]] * 2), torch.cat([样本[“时间步长”][:, j]] * 2), 嵌入, ).sample Noise_pred_uncond, Noise_pred_text = Noise_pred.chunk(2) Noise_pred = ( Noise_pred_uncond + config.sample.guidance_scale * (noise_pred_text - Noise_pred_uncond) ) else :noise_pred =unet(sample[“latents”][:, j],sample[“timesteps”][:,j],embeds,).sample#计算当前模型下给定潜伏的next_lateents的对数概率_ , log_prob = ddim_step_with_logprob( pipeline.scheduler,noise_pred,sample[“timesteps”][:, j],sample[“latents”][:,j], eta=config.sample.eta, prev_sample=sample[&quot; ;next_latents&quot;][:, j], ) # ppo 逻辑优点 = torch.clamp(sample[&quot;advantages&quot;], -config.train.adv_clip_max, config.train.adv_clip_max, )ratio = torch.exp(log_prob -样本[“log_probs”][:, j]) unclipped_loss = -advantages * 比率 Clipped_loss = -advantages * torch.clamp(ratio, 1.0 - config.train.clip_range, 1.0 + config.train.clip_range, ) loss = torch .mean(torch.maximum(unclipped_loss, Clipped_loss)) # 调试值 # John Schulman 说 (ratio - 1) - log(ratio) 是一个更好的 # 估计器，但大多数现有代码都使用它，所以... # http:// /joschu.net/blog/kl-approx.html 信息[“approx_kl”].append( 0.5 * torch.mean((log_prob - 样本[“log_probs”][:, j]) ** 2) ) 信息[“clipfrac”].append( torch.mean( ( torch.abs(ratio - 1.0) &gt; config.train.clip_range ).float() ) ) info[&quot;loss&quot;].append(loss) # 向后传递 Accelerator.backward(loss) if Accelerator.sync_gradients: Accelerator.clip_grad_norm_(unet.parameters(), config. train.max_grad_norm ) optimizationr.step() optimizationr.zero_grad()    由   提交 /u/ExaminationNo8522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1am04c9/d_what_makes_ppo_reinforcement_learning_and_not/</guid>
      <pubDate>Thu, 08 Feb 2024 16:49:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 神经常微分方程的替代品？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/</link>
      <description><![CDATA[https://arxiv.org/abs/2401.01836 我看到这篇论文，其中 NODEC 是为了实现未知动力系统的最优控制而实现的，我想知道我们还可以使用哪些其他方法来解决类似的问题。我知道强化学习，但我正在寻找更有效的数据。表征学习或模仿学习可能吗？或者有其他方法可以改善结果？   由   提交/u/Striking-Cricket788  /u/Striking-Cricket788  reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alywkc/d_alternate_of_neural_ode/</guid>
      <pubDate>Thu, 08 Feb 2024 15:57:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离开我的胸膛。我正在攻读机器学习博士学位，但我是个失败者。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</link>
      <description><![CDATA[我的机器学习博士学位已经过半了。 我很幸运，进入了一个很好的项目，尤其是在一个好的项目中。实验室的学生都是超级明星，毕业后会找到很好的工作。我不是他们中的一员。我有一本蹩脚的、技术性不高的出版物，我正在努力寻找一个在我的能力范围内可以解决的新问题。我已经很努力了。我在本科生和硕士期间一直在做研究，尽我所能 - 做项目、阅读论文、学习机器学习和数学课程、为教授撰写资助...... 事实是，我可以达不到产生新想法的水平。无论我多么努力，这都不是我的事。我想为什么。我开始怀疑 STEM 是否一开始就不是我的菜。我环顾四周，发现有些人的大脑只是“理解”了这一点。事情变得更容易。对我来说，这需要额外的努力和额外的时间。在本科期间，我可以更加努力、更长时间地学习。嗯，不是为了博士学位。尤其是在这个快节奏、拥挤的领域，我需要吸收新东西并快速发布。 我是一个冒名顶替者，这不是一种综合症。我快被抓了其他人都获得了多个实习机会等等。我到处都被拒绝。看来现在他们知道了。他们知道我没用。我想对我的顾问说这些，但他是如此的天才，以至于他无法理解普通人的想法。我所有的高级实验室伙伴都是全职工作人员，所以实际上我现在是实验室中最资深的。   由   提交 /u/rsfhuose   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alxv3l/d_off_my_chest_im_doing_phd_in_ml_and_im_a_failure/</guid>
      <pubDate>Thu, 08 Feb 2024 15:10:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] Bard 现在是 Gemini（免费试用 Gemini Ultra）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alvblw/d_bard_is_now_gemini_free_trial_to_gemini_ultra/</link>
      <description><![CDATA[     &lt; /td&gt; https://preview.redd.it/6tguxx3v1dhc1.png?width=1178&amp;format=png&amp;auto=webp&amp;s=4f1b7b9a25d3a9b2738ad945e8 beb1347cb93447 💻 今天，Google 推出了 Gemini Advanced——一种新的体验，让您可以访问 Ultra 1.0，这是他们最大、最强大的最先进的人工智能模型。在第三方评估者的盲目评估中，与领先的替代方案相比，带有 Ultra 1.0 的 Gemini Advanced 现在是最受欢迎的聊天机器人。 🚀  借助 Google 的 Ultra 1.0 模型，Gemini Advanced 能够更有效地执行高度复杂的任务，例如编码、逻辑推理、遵循细致入微的指令以及在创意项目上进行协作。 🤖 Gemini Advanced 不仅可以让用户进行更长、更详细的对话；它还可以更好地理解之前提示的上下文。例如：  💡 Gemini Advanced 可以成为您的个人导师 - 根据您的学习风格创建分步说明、示例测验或来回讨论。  💻 它可以帮助您实现更高级的编码场景，充当想法的共鸣板并帮助您评估不同的编码方法。  🎨 它可以帮助数字创作者通过生成新鲜内容、分析最新趋势以及集思广益来扩大受众群体的改进方法，从想法到创作。  Gemini Advanced 的第一个版本反映了 Google 目前在人工智能推理方面的进步，并将继续改进。随着他们添加新的专有功能，Gemini Advanced 用户将能够访问扩展的多模式功能、更多的交互式编码功能、更深入的数据分析功能等等。 Gemini Advanced 目前已在 150 多个国家和地区提供英语版本，并且随着时间的推移，Google 会将其扩展到更多语言。 🌍  Gemini Advanced 作为 Google 全新 Google One AI Premium 计划的一部分提供，价格为 19.99 美元/月，从两个月免费试用开始。该计划为用户提供了 Google AI 的最佳功能及其最新进展，以及现有 Google One Premium 计划的所有优势，例如 2TB 存储空间。此外，AI Premium 订阅者很快就能在 Gmail、文档、幻灯片、表格等中使用 Gemini（以前称为 Duet AI）。 📈  Google 继续采取大胆且负责任的方式将这项技术推向世界。而且，为了缓解不安全内容或偏见等问题，他们根据人工智能原则将安全性融入到产品中。在推出 Gemini Advanced 之前，他们进行了广泛的信任和安全检查，包括外部红队检查。他们根据人类反馈，使用微调和强化学习进一步完善了底层模型。 🛡️  Google 听说用户希望以更简单的方式在手机上访问 Gemini。因此，今天他们开始通过 Android 上的新应用程序和 iOS 上的 Google 应用程序为 Gemini 和 Gemini Advanced 推出新的移动体验。 📱    由   提交/u/Kakachia777   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alvblw/d_bard_is_now_gemini_free_trial_to_gemini_ultra/</guid>
      <pubDate>Thu, 08 Feb 2024 13:08:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 宗师级国际象棋无需搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alqzlf/r_grandmasterlevel_chess_without_search/</guid>
      <pubDate>Thu, 08 Feb 2024 08:23:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</link>
      <description><![CDATA[我读过很多东西说“除非你绝对确定你想读博士”，但我不确定。我的动机主要是为了打开更多的工业大门。我想成为一名研究科学家/机器学习工程师/数据科学，甚至是量化角色，但如果没有高级学位，这些角色越来越难获得。  我应该提一下，我的本科学位不是计算机科学，尽管我有机器学习方面的研究经验，并且选修了一些数学/统计课程（线性算法、统计、概率、计算）。我的问题是： 1) 攻读 4-6 年机器学习博士学位的机会成本（而不是从数据分析等较低入门职位开始并从那里向上工作）是否值得开设更多工业门？  2）我在没有博士学位的情况下获得研究科学家职位的可能性有多大？ 3）我可能还想最终转向初创公司（可能是在工业界之后或立即）。在这种情况下，博士学位并没有多大帮助。但是考虑到一切（事实上我不确定自己会做什么和想要什么），博士学位可以成为弄清楚我想要什么的途径吗？ 我觉得博士学位不值得它仅适用于 DS/MLE 甚至 Quant 角色。更不用说研究科学家了，对于如今的单身汉来说，我什至获得这些职位之一的机会都很难。我申请了大量的数据科学工作，但没有得到回应——这就是为什么我认为博士学位可能最终会引起一些关注。   由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1alny9s/d_phd/</guid>
      <pubDate>Thu, 08 Feb 2024 05:06:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>