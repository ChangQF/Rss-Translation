<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 26 Oct 2024 06:21:39 GMT</lastBuildDate>
    <item>
      <title>[D] 有人知道 Eleven 实验室是如何设计提示音的吗？我想根据提示音生成新的声音，而不是声音克隆。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gcej8u/d_do_anyone_know_how_eleven_labs_is_designing_the/</link>
      <description><![CDATA[        由    /u/usama__01 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gcej8u/d_do_anyone_know_how_eleven_labs_is_designing_the/</guid>
      <pubDate>Sat, 26 Oct 2024 06:02:26 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 开源视频索引/标签/标签生成工具。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/</link>
      <description><![CDATA[伙计们，我正在寻找一个开源工具或任何可以帮助我生成视频标签的 repo，以便对多个视频进行分类并进行进一步分析。 我想要的等价物是 Azure AI clvideo inxer，但如果有这样的开源工具，它将解决问题。    提交人    /u/jokingwizard   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/</guid>
      <pubDate>Sat, 26 Oct 2024 04:18:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 可访问的预训练视频嵌入模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/</link>
      <description><![CDATA[我在这个领域还比较陌生，所以如果我遗漏了一些信息或者有更好的地方可以发布这篇文章，我深表歉意。我一直在致力于一个项目，试图为长视频实现语义搜索和 RAG，并试图了解视频嵌入模型的当前状态。 我的问题是，我发现视频嵌入解决方案比文本或图像嵌入模型更难实现。OpenAI 或 Bedrock 上似乎没有任何可用的东西。有一些开源模型，但种类比其他嵌入模型少得多。 我看到某些人提出的解决方案是从视频中采样帧，并从这些帧中生成图像嵌入。但是，我担心我会丢失音频中的大量上下文以及视频中发生的任何动作。 从我的研究来看，在这个领域似乎走得最远的公司是 Twelve Labs。看起来他们在语义视频搜索方面有一套非常丰富的工具，但我理想情况下会在自己的矢量数据库中使用嵌入，该数据库还支持全文搜索和混合搜索。 Twelve Labs 有一个嵌入 API，但它目前处于私人测试阶段，所以我不太确定下一步该怎么做。有人对如何解决这个问题有什么建议或见解吗？    提交人    /u/PalpablePatsy247   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/</guid>
      <pubDate>Fri, 25 Oct 2024 21:11:30 GMT</pubDate>
    </item>
    <item>
      <title>一年的同行评审[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbzxbf/one_year_of_peer_review_d/</link>
      <description><![CDATA[我的稿件已在 IEEE 上等待同行评审 10 个半月。上次我联系期刊询问情况时，他们告诉我正在寻找审稿人。距离上次发送邮件已经过去两个月了。期刊没有回复我。稿件仍在同行评审中。我的问题是，评审这么久是正常的吗？这是我的论文将被接受的好兆头吗？如果相反，他们会毫不犹豫地直接拒绝它吗？还是说评审这么久然后最终拒绝是正常的？这篇论文是关于自然语言处理的     提交人    /u/Distinct_Earth_2542   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbzxbf/one_year_of_peer_review_d/</guid>
      <pubDate>Fri, 25 Oct 2024 17:35:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 通过 TEE + 联邦学习加速机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/</link>
      <description><![CDATA[https://www.hcinnovationgroup.com/clinical-it/learning-health-systems-research/news/55130702/dana-farber-researchers-address-oncology-data-sharing-issues 有人看到这个吗？研究负责人将其描述为“即插即用”。如果是真的，那就太好了。 我已经看到很多来自 Goog、MS、Intel 关于安全 ML 的 TEE/enclaves 的讨论，但这是我见过的第一次部署，而且他们也在使用联邦学习。    提交人    /u/thebiztechguy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/</guid>
      <pubDate>Fri, 25 Oct 2024 16:06:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 打破内存障碍：近乎无限的批量大小缩放以实现对比损失</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbvapp/r_breaking_the_memory_barrier_near_infinite_batch/</link>
      <description><![CDATA[摘要 对比损失是一种强大的表征学习方法，其中较大的批量大小通过提供更多负样本来更好地区分相似和不相似数据，从而提高性能。然而，批量大小的扩展受到 GPU 内存消耗的二次增长的限制，这主要是由于相似性矩阵的完全实例化。为了解决这个问题，我们提出了一种基于图块的计算策略，将对比损失计算划分为任意小块，避免相似性矩阵的完全实现。此外，我们引入了一种多级平铺策略来利用分布式系统的分层结构，在 GPU 级别采用基于环的通信来优化同步，并在 CUDA 核心级别采用融合内核来减少 I/O 开销。实验结果表明，所提出的方法将批量大小扩展到前所未有的水平。例如，它能够使用 8 或 32 个 A800 80GB 对批处理大小为 4M 或 12M 的 CLIP-ViT-L/14 模型进行对比训练，而不会牺牲任何准确性。与 SOTA 内存效率高的解决方案相比，它在保持相当速度的同时实现了两个数量级的内存减少。代码将公开发布。    提交人    /u/RajonRondoIsTurtle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbvapp/r_breaking_the_memory_barrier_near_infinite_batch/</guid>
      <pubDate>Fri, 25 Oct 2024 14:16:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] GLiNER 与 NuExtract：2024 年最佳自定义实体类型提取器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbck2f/d_gliner_vs_nuextract_best_2024_extractors_for/</link>
      <description><![CDATA[比较博客：https://medium.com/@alexgulakov/gliner-vs-nuextract-best-2024-extractors-for-custom-entity-types-d369eb65f1e1 GLiNER 和 NuExtract 都是为信息提取任务设计的模型，但它们在方法和功能上存在一些关键差异： NuExtract 1.5 演示：https://huggingface.co/spaces/numind/NuExtract-v1.5 Gliner_multiv2.1 演示：https://huggingface.co/spaces/urchade/gliner_multiv2.1    提交人    /u/vtempest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbck2f/d_gliner_vs_nuextract_best_2024_extractors_for/</guid>
      <pubDate>Thu, 24 Oct 2024 20:29:43 GMT</pubDate>
    </item>
    <item>
      <title>道德问题与谷歌[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/</link>
      <description><![CDATA[如果这里不适合讨论 ML 的这个方面，我深表歉意，但这似乎并不违反规则。 我最近参加了一项 Alphabet 人类数据研究，该研究用于评估 AI 代理和模型。 如果不进一步了解细节，这项研究的结构在伦理上非常令人怀疑。协议中说，如果有任何问题，请联系人类行为研究伦理委员会 HuBREC。 但是，协议中提供的电子邮件 hubrec@google.com 并不存在，我除了查阅过去的学术演讲和冷不丁地给人们发电子邮件外，根本没有任何联系点。 我在寻找下一步时遇到了很多困难，因为除了那封电子邮件之外，我没有其他可用的联系信息。我确实知道谷歌最近解雇了人工智能伦理研究人员，而且这个话题似乎从未被认真对待过。对于一项正在进行的研究来说，将您指向一个似乎不存在的委员会似乎不太好。    提交人    /u/chaneg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/</guid>
      <pubDate>Thu, 24 Oct 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 具有客观先验的完全贝叶斯逻辑回归</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/</guid>
      <pubDate>Thu, 24 Oct 2024 18:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用一行代码对 GGUF 模型进行基准测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb8yhq/r_benchmark_gguf_model_with_one_line_of_code/</link>
      <description><![CDATA[      大家好！ 👋我们刚刚推出了一个开源工具，只需一行代码即可对 GGUF 模型进行基准测试。 GitHub 链接 动机： GGUF 量化对于在设备上本地运行模型至关重要，但量化会极大地影响模型的性能。量化后测试模型至关重要（基准测试如何发挥作用）。但是我们注意到了几个挑战：  没有简单、快速的方法在本地或自托管服务器上对量化的 GGUF 模型进行基准测试。 现有基准测试 (github.com/terryyz/llm-benchmark) 中的 GGUF 量化评估结果不一致，显示的分数低于模型开发人员的官方结果。\  我们的解决方案： 我们构建了一个工具：  用一行代码对 GGUF 模型进行基准测试。 支持多处理和8 个评估任务。 在我们的测试中，它是可用的 GGUF 模型最快的基准测试。  示例： 在“ifeval”数据集上对 Llama3.2-1B-Instruct Q4_K_M 进行基准测试，以实现一般语言理解。在 4090 上，使用 4 个 worker 进行多处理，耗时 80 分钟。  在终端中输入  nexa eval Llama3.2-1B-Instruct:q4_K_M --tasks ifeval --num_workers 4 https://i.redd.it/6xh52gkttqwd1.gif  结果：  https://preview.redd.it/78aek4a4tqwd1.png?width=1475&amp;format=png&amp;auto=webp&amp;s=742a1379809244010f409a29298709f0575ae772 我们从文本模型开始，并计划扩展到更多设备上的模型和模式。欢迎您提供反馈！如果您发现这有用，请随时在 GitHub 🔗 上留下一颗星：https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval    提交人    /u/AlanzhuLy   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb8yhq/r_benchmark_gguf_model_with_one_line_of_code/</guid>
      <pubDate>Thu, 24 Oct 2024 17:58:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 谷歌如何克服医疗 AI 的训练数据问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/</link>
      <description><![CDATA[TLDR；他们将 3D 图像转换为矢量嵌入，节省了预处理时间并减少了训练数据大小。 仅在美国，每年就有超过 7000 万次计算机断层扫描检查，但这些数据对 Google 的训练无效。 Google Research 有用于放射学、数字病理学和皮肤病学的嵌入 API - 但所有这些都仅限于 2D 成像。医生通常依靠 3D 成像进行更复杂的诊断。 为什么？ CT 扫描具有 3D 结构，这意味着文件大小更大，并且需要比 2D 图像更多的数据。 浏览工程博客，他们刚刚发布了一些最终可以处理 3D 医疗数据的东西。它被称为 CT Foundation - 它将 CT 扫描转换为小而信息丰富的嵌入，以廉价的方式训练 AI 如何做到？ 检查以标准医学成像格式 (DICOM) 进行，并转换为具有 1,408 个值的向量 - 捕获的关键细节包括器官、组织和异常。 然后可以使用这些简洁的嵌入来训练 AI 模型，例如逻辑回归或多层感知器，与拍摄 3D 图像并需要预处理的典型模型相比，使用的数据要少得多。最终的分类器更小，从而降低了计算成本，因此训练更高效、更实惠。 最终结果？ CT Foundation 在七项分类任务中评估了数据效率： - 颅内出血 - 胸部和心脏钙化 - 肺癌预测 - 可疑腹部病变 - 肾结石 - 腹主动脉瘤，以及 - 身体部位 尽管训练数据有限，但这些模型在除一项更具挑战性的任务外的所有任务上都实现了超过 0.8 的 AUC，这意味着强大的预测性能和准确性。 该模型使用 1,408 维嵌入，只需要一个 CPU 进行训练，所有这些都在 Colab Python 笔记本中完成。 TLDR; Google Research 推出了一款工具，可有效地在 3D CT 扫描上训练 AI，方法是将它们转换为紧凑1,408 维嵌入可实现高效的模型训练。它被称为 CT Foundation，需要的数据和处理更少，在七个分类任务中实现了超过 0.8 的 AUC，以最少的计算资源展示了强大的预测性能。 有一个 colab 笔记本可用。 PS：通过从事个人项目来跟上技术发展，我学到了这一点 - 如果您想了解更多信息，请查看techtok today    提交人    /u/TechTok_Newsletter   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/</guid>
      <pubDate>Thu, 24 Oct 2024 17:11:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我们最近在 NeurIPS 上被接受的一些论文的论文摘要</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb74j6/r_paper_summaries_for_some_of_our_papers_that/</link>
      <description><![CDATA[大家好，这是我们小组最近在 NeurIPS 2024 上被接受的论文列表；作为一个全本科生小组，这对我们来说是一个值得骄傲的时刻；所有论文都是在没有任何外部学术支持的情况下发表的；这是我们论文的摘要。我们希望这能激励其他人追求人工智能，并将研究视为我们可以合作的视角，而您所需要的只是正确的指导（甚至不一定是博士或教授）。如果您发现这些论文有用并希望与我们合作，请随时与我们联系！  给我一个提示：LLM 能接受提示来解决数学问题吗？👉 Arxiv 链接  我们建议使用受人类教育学启发的“提示”来提高 LLM 在高级数学问题上的表现。我们还测试了模型对错误提示的鲁棒性。我们使用来自 MATH 数据集的各种问题在各种 LLM 上评估了我们的方法，并将其与一次性、少量和思路链提示进行了比较。  注意力转移：引导 AI 远离不安全内容 👉 Arxiv 链接  本研究探讨了限制生成模型中不安全内容的方法。我们提出了一种新颖的无训练方法，使用注意力重新加权来消除推理过程中的不安全概念。我们的方法与现有技术进行了比较，并在直接和对抗性越狱提示上进行了评估。我们还讨论了潜在的原因、局限性和更广泛的影响。  揭开面纱：对图像隐私和版权保护的概念消融的研究 👉 Arxiv 链接  本文扩展了 Kumari 等人 (2022) 引入的预训练模型中概念消融的研究。我们重现了各种概念消融技术的结果，并提出了一种新颖的变体“商标消融”，以解决模型输出中的品牌元素。我们还分析了模型的局限性、消融泄漏提示下的行为以及不相关概念上的性能下降。   IIT Roorkee 的视觉语言小组为来自 NeurIPS、CVPR、ICCV 和 ICML（2016-2024） 等顶级会议的深度学习论文汇编了一个优秀的全面摘要库。这些摘要细分了计算机视觉、NLP 和机器学习中的关键论文 - 如果您想在不深入研究全文的情况下保持最新状态，这将是完美的选择。    提交人    /u/vlg_iitr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb74j6/r_paper_summaries_for_some_of_our_papers_that/</guid>
      <pubDate>Thu, 24 Oct 2024 16:42:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer 是 CNN 的一种</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/</link>
      <description><![CDATA[https://arxiv.org/abs/2309.10713 我随机在谷歌上搜索动态卷积，因为我觉得它们很酷，然后发现了这篇论文，它表明 transformer 相当于一种使用动态卷积的 CNN。动态卷积论文 (https://arxiv.org/abs/1912.03458) 于 2019 年发布，所以它确实在注意力就是你需要的所有论文之后发布。 遗憾的是这篇论文只有一个引用。我认为这太不可思议了。知道 transformers 可以被视为 CNN 让他们能够深入了解优化其设计，包括删除 softmax 激活并将其替换为 Relu+normalisation 层。我认为通过继续他们的工作可以做出更多的改进。    提交人    /u/Ozqo   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/</guid>
      <pubDate>Thu, 24 Oct 2024 08:31:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</guid>
      <pubDate>Sun, 20 Oct 2024 15:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>