<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title></title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 22 Jan 2025 18:22:33 GMT</lastBuildDate>
    <item>
      <title>[R] 学会利用贝叶斯原理持续学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7g04y/r_learning_to_continually_learn_with_the_bayesian/</link>
      <description><![CDATA[       由    /u/moschles  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7g04y/r_learning_to_continually_learn_with_the_bayesian/</guid>
      <pubDate>Wed, 22 Jan 2025 17:05:25 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7fdf9/d_suggestion_for_image_embedding_model_finetuning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7fdf9/d_suggestion_for_image_embedding_model_finetuning/</guid>
      <pubDate>Wed, 22 Jan 2025 16:40:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2025 评论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7dqlh/d_cvpr_2025_reviews/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7dqlh/d_cvpr_2025_reviews/</guid>
      <pubDate>Wed, 22 Jan 2025 15:31:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 从原始视频中学习复杂知识：VideoWorld 在围棋和机器人控制方面的成功</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7doq7/r_learning_complex_knowledge_from_raw_video/</link>
      <description><![CDATA[本文介绍了一种通过名为 VideoWorld 的自监督框架直接从未标记视频数据中学习世界知识的方法。核心技术贡献是一种多阶段架构，可处理视频以提取视觉和时间关系，而无需手动注释。 关键技术点： - 使用视频片段之间的对比学习来捕获时间动态 - 实现视觉和运动特征之间的跨模态对齐 - 采用时间一致性学习来理解事件序列 - 引入用于长距离依赖关系的分层注意机制 结果证明该方法比现有方法有所改进： - HowTo100M 上的视频 QA 性能提高了 12% - ActivityNet 上的时间关系理解提高了 15% - 下一帧预测任务提高了 8% - 有效地将零样本迁移到看不见的视频域 我认为这种方法可以显著改变我们训练视频理解模型的方式。通过消除昂贵的手动标记的需求，我们可以在更大、更多样化的视频数据集上进行训练。我特别感兴趣的是，这将如何改善需要理解物理相互作用和因果关系的机器人学习系统。 结果表明，这种方法在现实世界的应用潜力巨大，但我认为在计算效率和抽象概念的处理方面仍然存在需要解决的重要挑战。 TLDR：新的自监督框架从未标记的视频中学习世界知识，在视频理解任务上显示出很大的改进，而不需要手动注释。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7doq7/r_learning_complex_knowledge_from_raw_video/</guid>
      <pubDate>Wed, 22 Jan 2025 15:29:50 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7ahbb/p_built_a_free_api_wrapper_for_ml_models_at_our/</link>
      <description><![CDATA[      大家好， 我们在我们的研究实验室构建了 Jaqpot，因为我们需要一种简单的方法通过 API 部署机器学习模型，而无需处理部署问题。基本上，您可以训练您的模型（sklearn，pytorch geometry），只需几行代码即可将其包装在 API 中，您可以从您的应用程序调用或访问我们的仪表板。 Python 中的示例： from jaqpotpy.models import SklearnModel from jaqpotpy import Jaqpot # 像往常一样训练您的模型 model = SklearnModel(dataset=dataset, model=LogisticRegression()) model.fit() # 部署它 jaqpot = Jaqpot() jaqpot.login() model.deploy_on_jaqpot( name=&quot;My Model&quot;, description=&quot;Simple classifier&quot; )  然后您可以从 Python 调用它： prediction = jaqpot.predict_sync(model_id=model_id, dataset=input_data)  或 Java： Dataset prediction = jaqpotApiClient.predictSync(modelId, inputData);  一些要点：  免费使用（研究项目由我们的实验室资助） 支持 scikit-learn 模型和 pytorch geometry 提供 Python 和 Java/Kotlin SDK 处理所有 ONNX 转换和 API 设置 私有或公共模型部署选项 包括用于化学和材料科学预测建模的专门功能  在我们有资金运行基础设施的同时，我们会向研究人员和开发者提供此功能。我们想看看人们实际上是如何使用它的，以及他们需要什么功能。从长远来看，我们可能需要使用层来保持服务器运行，但现在它是完全免费和开源的。 如果您有兴趣，请查看我们的文档或尝试访问app.jaqpot.org。 很高兴回答任何问题！ https://preview.redd.it/685xnheuljee1.png?width=1582&amp;format=png&amp;auto=webp&amp;s=957a61576ca14a52cee33a1877331c61bb214905 https://preview.redd.it/tho37heuljee1.png?width=1289&amp;format=png&amp;auto=webp&amp;s=4704e95d03e70c8f02c01d228a445c96f0f81c37 编辑：由于人们询问技术堆栈 - 它使用 ONNX 进行模型转换并在 AWS 上运行。模型有版本控制，您可以通过 Web 界面跟踪使用情况。   由    /u/SignificanceUsual606  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7ahbb/p_built_a_free_api_wrapper_for_ml_models_at_our/</guid>
      <pubDate>Wed, 22 Jan 2025 12:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 生产环境中欺诈检测的日常生活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i7afnz/d_a_day_in_the_life_of_fraud_detection_in/</link>
      <description><![CDATA[你好。这里有谁在生产中使用欺诈模型，可以回答我几个问题吗？ - 你如何描述一个典型的工作日？ - 你主要做交易模型吗？ - 你多久重新训练一次你的模型？ - 你多久重新训练一次同一个模型？你的训练基础的平均时间窗口是多少？你什么时候知道你需要训练一个新模型？它们之间有什么变化？受众，变量？（因为它们比信贷更频繁）。 - 你如何在生产中测试新模型的性能？ - 你需要了解多少关于 MLEng/架构/基础设施的知识？ 例如，假设我在一家金融科技公司担任数据科学家（我确实是这样做的）。他们已经在运行欺诈检测模型。什么时候应该重新训练当前模型，什么时候应该重新训练新模型？ 我正在探索该领域的新机会，但发现很难收集有关实际工作经验的信息。 非常感谢。    提交人    /u/Lock-and-load   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i7afnz/d_a_day_in_the_life_of_fraud_detection_in/</guid>
      <pubDate>Wed, 22 Jan 2025 12:53:26 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i74pni/d_a_little_late_but_interesting_talk_by_feifei_li/</link>
      <description><![CDATA[Fei-Fei Li 就视觉智能以及 AI 的未来进行了精彩演讲。想在这里分享，以防有人想在他们的网站上查看。    由    /u/hiskuu  提交  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i74pni/d_a_little_late_but_interesting_talk_by_feifei_li/</guid>
      <pubDate>Wed, 22 Jan 2025 06:12:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]：Andrej Karpathy 讲座：构建 makemore 第二部分：MLP</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i73gxr/d_andrej_karpathy_lecture_building_makemore_part/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i73gxr/d_andrej_karpathy_lecture_building_makemore_part/</guid>
      <pubDate>Wed, 22 Jan 2025 04:56:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 未来引导学习：一种增强时间序列预测的预测方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i727qm/r_futureguided_learning_a_predictive_approach_to/</link>
      <description><![CDATA[大家好！我叫 Skye，我是这项工作的第一作者！这篇论文表明，通过从大脑中获取灵感，特别是预测编码理论，可以增强预测和事件预测。我发布了摘要、代码和 arXiv 链接，供任何好奇的人使用！请随时在下面发表评论，因为这是我的第一篇完整论文，我将不胜感激任何反馈！ 摘要：准确的时间序列预测在各种科学和工业领域都至关重要，但深度学习模型通常难以捕捉长期依赖关系并适应数据分布随时间的变化。我们引入了未来引导学习，这是一种通过受预测编码启发的动态反馈机制来增强时间序列事件预测的方法。我们的方法涉及两个模型：一个分析未来数据以识别关键事件的检测模型和一个基于当前数据预测这些事件的预测模型。当预测模型和检测模型之间出现差异时，将对预测模型进行更重要的更新，通过将其预测与实际未来结果保持一致，有效地最大限度地减少意外并适应数据分布的变化。这种反馈回路允许预测模型动态调整其参数，尽管数据发生变化，但仍专注于持久特征。我们在各种任务上验证了我们的方法，结果表明，使用 EEG 数据进行癫痫发作预测的 AUC-ROC 增加了 44.8%，非线性动态系统预测的 MSE 降低了 48.7%。通过结合可适应数据漂移的预测反馈机制，未来引导学习推进了深度学习在时间序列预测中的应用。  我们的代码公开发布于： https://github.com/SkyeGunasekaran/FutureGuidedLearning。 arXiv： https://arxiv.org/pdf/2410.15217    提交人    /u/Skye7821   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i727qm/r_futureguided_learning_a_predictive_approach_to/</guid>
      <pubDate>Wed, 22 Jan 2025 03:48:11 GMT</pubDate>
    </item>
    <item>
      <title>[D]：3blue1brown 视频详细解释了注意力机制</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6zh6p/d_a_3blue1brown_video_that_explains_attention/</link>
      <description><![CDATA[ YouTube 视频 字幕  时间戳 02:21：标记嵌入 02:33：在嵌入空间中 \ 一个单词有多个不同的方向 \ 对该单词的多个不同含义进行编码。 02:40：训练有素的注意力模块 \ 计算您需要添加到通用嵌入中的内容 \ 以将其移动到这些特定方向之一， \ 作为上下文的函数。 \ 07:55 ：从概念上认为 K 可能回答 Q。 11:22 ：（没听懂）    提交人    /u/yogimankk   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6zh6p/d_a_3blue1brown_video_that_explains_attention/</guid>
      <pubDate>Wed, 22 Jan 2025 01:38:43 GMT</pubDate>
    </item>
    <item>
      <title>Apple AIML 驻留计划 2025 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6p1ja/apple_aiml_residency_program_2025_r/</link>
      <description><![CDATA[你好！ 有人以前参加过 Apple 的 AIML 驻留项目并愿意分享他们的经验吗？ 我最好奇的是面试过程、项目本身（难吗？有趣吗？），以及未来在 Apple 内部成为正式员工的机会。提前谢谢大家了！    提交人    /u/maplesyrup67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6p1ja/apple_aiml_residency_program_2025_r/</guid>
      <pubDate>Tue, 21 Jan 2025 18:12:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 Transformer 进行多元时间序列预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6os2n/r_multivariate_time_series_prediction_with/</link>
      <description><![CDATA[我正在开发一个模型，希望能够接收多变量时间序列的天气和河流高度数据，并输出一系列针对河流水位计高度的预测（本质上，我输入时间步长 20-40，并期望收到时间步长 41-61）。我之前一直在为此使用 LSTM，但使用几种不同的架构得到的结果相当差。我现在正在考虑使用变压器编码器网络，我遇到了这个反复出现的问题，我似乎无法弄清楚。 对于几乎任何上下文长度、模型大小、位置编码、训练时间等；该模型似乎无法区分输出上的时间步长。它总是学会预测时间步长上水位计高度的良好平均值，但其输出没有变化。例如，当目标量规高度为 [0.2, 0.3, 0.7, 0.8, 0.6] 时，它将输出类似 [0.4, 0.45, 0.4, 0.45, 0.5] 的内容。 事实上，即使没有任何位置编码，模型的表现也几乎完全相同。 以下是多次连续测试的输出示例： 几条预测线，无论图表上的实际位置如何，都显示出相似的趋势。 我尝试了相对位置编码和绝对位置编码。编码并调整损失函数以添加一个关注时间步长之间斜率的项，但我似乎无法强制区分时间步长。 额外的损失项： class TemporalDeregularization(nn.Module): def __init__(self, epsilon): super().__init__() self.epsilon = epsilon self.mse = nn.MSELoss() def forward(self, yPred, yTrue): predDiff = yPred[:, 1:] - yPred[:, :-1] targetDiff = yTrue[:, 1:] - yTrue[:, :-1] return self.epsilon * self.mse(predDiff, targetDiff)  我的位置编码方案： class PositionalEncoding(nn.Module): def __init__(self, d_model：int，dropout：float = 0.1，max_len：int = 5000，batch_first = False）：super（）。__init__（）self.batch_first = batch_first self.dropout = nn.Dropout（p = dropout）position = torch.arange（max_len）。unsqueeze（1）div_term = torch.exp（torch.arange（0，d_model，2）*（-math.log（10000.0）/ d_model））pe = torch.zeros（max_len，1，d_model）pe [：，0，0 :: 2] = torch.sin（position * div_term）pe [：，0，1 :: 2] = torch.cos（position * div_term）self.register_buffer（&#39;pe&#39;，pe）def forward（self，x：张量）——&gt;张量：如果 self.batch_first：x = x + self.pe[:x.size(1)].permute(1, 0, 2) else：x = x + self.pe[:x.size(0)] return self.dropout(x)  这是我的架构的更明确的图表： 包含变压器网络架构的图像，包括线性投影、位置编码、变压器编码器和另一个串联投影。 我理解这不是这个用例的常见用例或架构，但我不确定为什么该模型不能区分时间步长。我考虑在最终投影之前添加双向 LSTM 以强制时间微分。 作为参考，我发现这个模型在 dModel 为 64、前馈为 128、6 层和 8 个头的情况下表现良好。损失函数中的另一个术语是标准 MSE。此外，我不应用掩蔽，因为在我的情况下，所有输入都应该用于计算输出。 我不能发布太多代码，因为这与我的工作有关，但我想更多地了解我的方法有什么问题。 任何帮助或建议都值得赞赏，我目前正在攻读硕士学位，但尽管有多年的工作经验，我还没有遇到任何机器学习课程，所以我可能只是错过了一些东西。（也为狗屁股谷歌图纸感到抱歉）    提交人    /u/Chroma-Crash   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6os2n/r_multivariate_time_series_prediction_with/</guid>
      <pubDate>Tue, 21 Jan 2025 18:01:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] AISTATS 2025 论文录取结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6lgoo/d_aistats_2025_paper_acceptance_result/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6lgoo/d_aistats_2025_paper_acceptance_result/</guid>
      <pubDate>Tue, 21 Jan 2025 15:43:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i4oujz/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i4oujz/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 19 Jan 2025 03:15:29 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>