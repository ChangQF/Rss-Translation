<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sat, 15 Jun 2024 06:18:43 GMT</lastBuildDate>
    <item>
      <title>[讨论] 我是一名人工智能工程师，为什么从事计算机视觉工作这么糟糕。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dgb0y3/discussion_im_an_ai_engineer_and_why_does_working/</link>
      <description><![CDATA[这只是一篇帖子，所以我可以抱怨一下与语言模型相比，计算机视觉有多么痛苦。  请注意，这是我在机器学习应用中使用地理空间成像的第二周，在它成为第二天性之前，我可能忘记了使用语言模型的初始阶段。我被赋予了一个庞大且相当混乱的数据库，用于我将在工作中解决的任务，我只是在寻求同情并希望有共同的经验💀    提交人    /u/__Abracadabra__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dgb0y3/discussion_im_an_ai_engineer_and_why_does_working/</guid>
      <pubDate>Sat, 15 Jun 2024 05:44:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] CFG++：解决扩散模型中 CFG 缺陷的简单方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dg9mvc/r_cfg_a_simple_fix_for_addressing_the_flaws_of/</link>
      <description><![CDATA[      无分类器指导 (CFG) 广泛用于扩散模型中的文本指导，但因其挑战而臭名昭著，例如难度在 DDIM 反演和选择较大指导尺度时的模糊性方面的问题。 本文表明，CFG 的这些局限性源于原始 CFG 中固有的设计缺陷，并介绍了 CFG++，一种简单但功能强大的修复*re\*nosing 过程的方法。这种调整有利于缩小引导尺度，显著提高可逆性，并大大提高图像和文本之间的对齐效果。 项目页面：https://cfgpp-diffusion.github.io/ Github：https://github.com/CFGpp-diffusion/CFGpp 论文：https://arxiv.org/abs/2406.08070 https://preview.redd.it/lph7aufcvn6d1.png?width=854&amp;format=png&amp;auto=webp&amp;s=5e4bfeb1af9563bb30b29848386f02b11f13fbdc    提交人    /u/Fit_Entrepreneur_588   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dg9mvc/r_cfg_a_simple_fix_for_addressing_the_flaws_of/</guid>
      <pubDate>Sat, 15 Jun 2024 04:16:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻求重症监护病房预测分析机器学习模型的帮助</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dg35s6/r_seeking_help_with_machine_learning_model_for/</link>
      <description><![CDATA[大家好， 我是一名四年级医学生，正在研究一个关于患者结果预测分析的研究项目。我的重点是机器学习模型如何改善重症监护病房的预后预测。 鉴于我的背景主要是医学，我正在寻求有机器学习经验的人的帮助。具体来说，我需要以下方面的指导：  开发或寻找合适的 ML 模型：对可用于重症监护预后预测的现有模型的建议，或从头开始开发模型的指导。 数据输入：如何处理和预处理来自医院电子健康记录 (EHR) 的数据以用于这些模型。 集成和验证：有关将模型与实际数据集成并验证其准确性和可靠性的提示。  如果有人有类似项目的经验或可以向我指出相关资源，我将不胜感激。合作或指导也将非常有价值。 提前感谢您的帮助！ 此致， Gamict（我的互联网名称）    提交人    /u/Gamict   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dg35s6/r_seeking_help_with_machine_learning_model_for/</guid>
      <pubDate>Fri, 14 Jun 2024 22:31:14 GMT</pubDate>
    </item>
    <item>
      <title>决策树不平衡数据[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dg2v90/decision_tree_unbalanced_data_d/</link>
      <description><![CDATA[大家好，第一次在这里提问。我需要开发一个基于规则的分类模型，我不能使用其他方法。我的数据集严重不平衡，平均约为 1:100，我正在运行一些决策树来完成这项任务。我已经优化了树参数，包括类权重，并使用网格搜索关注模型的精度，因为从业务角度来看，关注召回率是不值得的。到目前为止，我在测试样本中有一些精度为 0.6-0.7 的树，但最终树变得太简单了，我担心它在较新的数据上表现不会那么好。我尝试使用 SMOTE 和欠采样，但使用具有优化权重的原始不平衡数据集的结果仍然为我提供了最佳结果。有人知道我还能尝试做什么吗？    提交人    /u/marcelomedre   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dg2v90/decision_tree_unbalanced_data_d/</guid>
      <pubDate>Fri, 14 Jun 2024 22:17:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nemotron-4 340b详细分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfywwi/d_nemotron4_340b_detailed_analysis/</link>
      <description><![CDATA[      我查看了 NVIDIA 的 340B Nemotron LLM - 我发现了一些问题：  Squared ReLU 与 Llama SwiGLU 不同，Gemma GeGLU。与 arxiv.org/pdf/2002.05202 中发现的 GLU 变体不同（GLU 变体改进了 Transformer，Noam Shazeer） ReGLU 是 [ ReLU(X * W_gate) * (X * W_up) ] * W_down 我们需要 2 个 ReLU + 绑定权重 [ ReLU(X * W_up) * ReLU(X * W_up) ] * W_down，因此有点像 GLU，但不完全相同 Squared ReLU 为何有效？入门论文：https://arxiv.org/abs/2109.08668v2 发现了它。还有 Shazeer 的引言：“我们没有解释为什么这些架构似乎有效；我们将它们的成功归功于上帝的仁慈”  使用 rotary_percentage 为 50%。可能与 Phi-2 的 partial_rotary_factor 有关？- Phi-2 的 rotary_percentage 为 40%，因此对于 Nemotron 来说，似乎只有 50% 的 Q、K 矩阵应用了 RoPE，其余的则不使用 RoPE。  请参阅https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling_phi.py#L79  解开嵌入，如 Llama。Gemma 已绑定。  Tied 还用于 Apple 的设备 LLM，以节省 VRAM。 根据 LLM 物理学论文 https://arxiv.org/abs/2404.05405  常规 layernorm，不同于 Llama RMS LN。RMS Layernorm 消除了偏差，但不进行均值消除 没有 dropout，没有偏差，如 Llama、Gemma 批量大小随着 42% MFU 增加。Float16 训练没有稀疏性。 4096 序列长度。遗憾的是相当短。 8 万亿代币 - 3 种风格基础、指导、奖励 SFT、DPO、RPO  RPO - 我认为是 2 个步骤？奖励感知偏好优化 HelpSteer2 数据集：https://huggingface.co/datasets/nvidia/HelpSteer2  在某些基准测试中击败 GPT-4o  技术报告：https://research.nvidia.com/publication/2024-06_nemotron-4-340b HF Instruct：https://huggingface.co/nvidia/Nemotron-4-340B-Instruct https://preview.redd.it/vhcke1177l6d1.png?width=980&amp;format=png&amp;auto=webp&amp;s=2458d6c2bd9ed0db14e9f16c5be6e93340f6bfaf    由    /u/danielhanchen 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfywwi/d_nemotron4_340b_detailed_analysis/</guid>
      <pubDate>Fri, 14 Jun 2024 19:20:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我设计的这个 MLP 参数减少了 63%，这是一个重大进展吗？（在 nanoGPT 上测试）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfyq4y/p_is_this_mlp_i_designed_with_63_less_parameters/</link>
      <description><![CDATA[我切换了激活函数并对 nanoGPT 中的 MLP 做了一些小改动。采用修改后的 MLP 的 nanoGPT 实现了略低的损失，但仅使用了 shakespeare-char 数据集上约 60% 的参数。    Vanilla nanoGPT nanoGPT + 修改后的 MLP    总非嵌入参数 10.65M 6.22M   总 MLP 参数 7.07M 2.65M   每个块的 MLP 参数 1.17M 0.44M   训练损失 ？ 1.0249   验证损失 1.4697 1.4647   总结一下，我对 nanoGPT MLP 的小小修改导致了：  总参数减少 0.42% MLP 参数减少 63% 验证损失改善不到 1%  这是一个重大改进，还是已经有这样的优化已经制作了吗？考虑到我的 RX 6600 相对较慢，我该如何进行更多测试？将 MLP 结构发布到 GitHub 是否值得？    提交人    /u/IonizedPro   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfyq4y/p_is_this_mlp_i_designed_with_63_less_parameters/</guid>
      <pubDate>Fri, 14 Jun 2024 19:12:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 ImageNet 上测试的相关性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfxctf/d_relevance_of_testing_on_imagenet/</link>
      <description><![CDATA[我正在撰写一篇关于网络优化的有趣发现的论文/调查。我想展示我得到的有趣结果；我正在将新优化器的性能与 Adam / AdamW / SGD / SGD 与 Nesterov 进行比较，并在 MNSIST CIFAR10/100 上进行了大量测试。我周围的一些人告诉我，通过在 ImageNet 上进行测试，我必须走得更远。 但是，测试多种配置所需的规模和时间将非常庞大。所以，我的问题是，ImageNet 真的是一个必不可少的基准，还是 MNIST 和 CIFAR 足以证明大多数论文的观点？我理解我的同事提出的论点，即它绝对是一个更“详尽”和更现实的基准，但我觉得计算障碍相当大。 有什么评论或意见吗？    提交人    /u/Secret-Toe-8185   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfxctf/d_relevance_of_testing_on_imagenet/</guid>
      <pubDate>Fri, 14 Jun 2024 18:12:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Conda VS Python 的内置 'venv'</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfwteq/d_using_conda_vs_pythons_inbuilt_venv/</link>
      <description><![CDATA[嘿， 因此，我通常在克隆项目时使用 venv 模块，但我目前正在使用用 C++ 编写的 OpenCV，而 python 模块只是这个 c++ 的包装器。 因此，我必须安装系统级依赖项。这意味着 venv 模块根本无法工作（如果我错了请纠正我），我必须使用 Conda。 我看到关于 Conda 已经过时并且正在使用新东西的文章。你们能确认/分享经验吗！ 谢谢    提交人    /u/No_Weakness_6058   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfwteq/d_using_conda_vs_pythons_inbuilt_venv/</guid>
      <pubDate>Fri, 14 Jun 2024 17:49:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在多个虚拟机上使用 DDP 时，有没有关于 PyTorch 性能的基准测试？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfwq98/d_any_benchmarks_about_pytorchs_performance_when/</link>
      <description><![CDATA[我试图找到任何基准，了解在多个虚拟机上使用 DDP 时性能会下降多少（如果有的话）。因此，不只是一台具有多个 GPU 的虚拟机，而是实际上跨集群中的多台机器。你对此了解多少？ 此外，你们中是否有人使用过这种设置与低优先级虚拟机？我试图弄清楚，如果在跨多个低优先级虚拟机进行训练时集群中的某些虚拟机被关闭，这种设置的恢复能力如何。    提交人    /u/lifesthateasy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfwq98/d_any_benchmarks_about_pytorchs_performance_when/</guid>
      <pubDate>Fri, 14 Jun 2024 17:45:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在 4 个节点 x 4x A100 GPU 上的 Slurm 中使用 Axolotl 执行多节点微调？[项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfw4a3/how_to_perform_multinode_finetuning_with_axolotl/</link>
      <description><![CDATA[我对 Slurm 还比较陌生，正在寻找一种有效的方法来在系统内设置集群，如标题所述（不一定需要是 Axolotl，但最好是）。一种方法可能是通过在“加速配置”/deepspeed 中输入其他服务器的 IP 来配置多个节点，(https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/multi-node.qmd) 定义服务器 1、2、3、4，并允许通过 SSH 或 HTTP 以这种方式进行通信。但是，这种方法似乎很不干净，而且没有太多令人满意的信息。有没有人有使用 Slurm 的经验，做过类似的事情，可以帮我吗？ :)    由   提交  /u/AppropriateCan5964   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfw4a3/how_to_perform_multinode_finetuning_with_axolotl/</guid>
      <pubDate>Fri, 14 Jun 2024 17:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 预训练 LLM 在 Amazon Sagemaker 中的文本分类评估</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfu9b4/r_pretrained_llms_evaluation_in_text/</link>
      <description><![CDATA[我很好奇为什么在 jumpstart 上没有评估预训练文本分类 llms 的选项。我应该部署它们并运行推理吗？我的目标是查看一些大型模型在预测我的自定义数据集（金融文本，它必须预测它是声明还是前提 - 它是受监督的）上的标签时的准确性。我误解了什么吗？    提交人    /u/marshallggggg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfu9b4/r_pretrained_llms_evaluation_in_text/</guid>
      <pubDate>Fri, 14 Jun 2024 15:58:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] 改进的 Text2SQL 数据集现已在 Huggingface 上可用！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfsel1/p_improved_text2sql_dataset_now_available_on/</link>
      <description><![CDATA[我很高兴与大家分享我们一直在研究的更新的开源资源 — 耶鲁大学最初为 Text2SQL 任务发布的 Spider 数据集的改进版本。您可以在此处查看：https://huggingface.co/datasets/RaffaSch121/fixed_spider 在 Turbular 进行我们自己的模型训练期间，我们发现原始数据集中存在几个问题。为了帮助社区并回馈社区，我们决定解决这些问题并发布更正版本。我们希望这个增强的数据集将使所有从事 Text2SQL 和类似项目的人受益。 如果您找到使其更好的方法，请随意下载、试验和回馈   由    /u/RaeudigerRaffi  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfsel1/p_improved_text2sql_dataset_now_available_on/</guid>
      <pubDate>Fri, 14 Jun 2024 14:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 讨论苹果在 iPhone 15 Pro 上部署 30 亿参数 AI 模型——他们是如何做到的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</link>
      <description><![CDATA[大家好， 所以，我一直在本地运行 Phi-3 mini，老实说，它还不错。尽管在模型文件中进行了所有调整和结构化提示，但这是正常的，尤其是考虑到典型 GPU 设置上的滞后响应时间。我最近在检查 Apple 最近的设备模型，他们在 iPhone 15 Pro 上运行了一个近 30 亿参数的 AI 模型！ 这是 AI 在移动设备上实现可能性的进步。他们想出了一些技巧来实现这一点，我只是想和大家讨论一下：  优化的注意力机制：Apple 通过使用分组查询注意力机制显着降低了计算开销。此方法可批量处理查询，从而减少必要的计算。 共享词汇嵌入：老实说，我对此没有太多了解 - 我需要更多地了解它 量化技术：对模型权重采用 2 位和 4 位量化混合，有效降低了内存占用和功耗。 高效的内存管理：动态加载小型、特定于任务的适配器，可以加载到基础模型中以专门化其功能，而无需重新训练核心参数。这些适配器重量轻，仅在需要时使用，在内存使用方面灵活且高效。 高效的键值 (KV) 缓存更新：即使我都不知道它是如何工作的 功耗和延迟分析工具：他们使用 Talaria 等工具实时分析和优化模型的功耗和延迟。这样一来，他们就可以在性能、功耗和速度之间做出权衡，定制比特率选择，以在不同条件下实现最佳运行。：Talaria 演示视频 通过适配器进行模型专业化：无需重新训练整个模型，只需针对不同任务训练特定的适配器层，即可保持高性能，而无需重新训练整个模型的开销。 Apple 的适配器让 AI 可以随时切换档位以执行不同的任务，同时保持轻便和快速。  有关更详细的见解，请查看 Apple 的官方文档：介绍 Apple Foundation Models 讨论要点：  在移动设备上部署如此庞大的模型的可行性如何？ 这些技术对未来的移动应用有何影响？ 这些策略与典型的桌面 GPU 环境（例如我使用 Phi-3 mini 的体验）中使用的策略相比如何？     提交人    /u/BriefAd4761   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</guid>
      <pubDate>Fri, 14 Jun 2024 11:50:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] Lamini.AI 推出记忆调节功能：法学硕士准确率达 95%，幻觉减少 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</link>
      <description><![CDATA[https://www.lamini.ai/blog/lamini-memory-tuning  Lamini Memory Tuning 是一种将事实嵌入 LLM 的新方法，可提高事实准确性并将幻觉减少到以前无法实现的水平 - 对于一位财富 500 强客户来说，Lamini Memory Tuning 的准确率达到了 95%，而其他方法的准确率仅为 50%。幻觉从 50% 减少到 5%。 Lamini Memory Tuning 是一项研究突破，它克服了 AI 世界中一个看似矛盾的现象：实现精确的事实准确性（即没有幻觉），同时坚持使 LLM 变得有价值的泛化能力。 该方法需要在任何开源 LLM（如 Llama 3 或 Mistral 3）之上使用精确的事实调整数百万个专家适配器（例如 LoRA）。如果目标是准确无误地获取罗马帝国的事实，Lamini Memory Tuning 会创建关于凯撒、渡槽、军团和您提供的任何其他事实的专家。受信息检索的启发，该模型在推理时仅从索引中检索最相关的专家 - 而不是所有模型权重 - 因此延迟和成本显着降低。高精度、高速度、低成本：使用 Lamini Memory Tuning，您无需选择。  研究论文：https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf    提交人    /u/we_are_mammals   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</guid>
      <pubDate>Fri, 14 Jun 2024 02:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>