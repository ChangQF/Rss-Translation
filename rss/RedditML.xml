<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Thu, 01 Aug 2024 06:21:43 GMT</lastBuildDate>
    <item>
      <title>[D] 机器学习论坛</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh8d35/d_forum_for_machine_learning/</link>
      <description><![CDATA[大家好， 我想知道是否有一个论坛或在线社区（当然除了这个 subreddit 之外），人们可以在那里讨论机器学习、分享他们的观点/资源等。我一直没能找到，我觉得我们应该有类似的东西。类似老式论坛页面风格的东西。或者它在 2024 年不再是一个流行的平台了？ 你怎么看？    提交人    /u/satori_paper   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh8d35/d_forum_for_machine_learning/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:45 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 寻找无代码或低代码软件，允许研究人员基于数学公式或方程试验不同类型的神经网络模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh7een/discussion_looking_for_no_or_low_code_software/</link>
      <description><![CDATA[您好， 我正在寻找无代码或低代码软件，能够帮助用户根据研究论文中提供的数学公式或方程式对不同类型的神经网络进行建模和试验。 谢谢    提交人    /u/awakendragon82   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh7een/discussion_looking_for_no_or_low_code_software/</guid>
      <pubDate>Thu, 01 Aug 2024 04:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有人觉得 LLM 没什么意思吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/</link>
      <description><![CDATA[我不是 ML 研究员。当我想到很酷的 ML 研究时，我想到的是 OpenAI Five 或 AlphaFold 之类的东西。如今，人们热议的是 LLM 和扩展转换器，虽然该领域确实有一些研究和优化要做，但它对我来说并不像其他领域那么有趣。对我来说，ML 的有趣部分是为您的用例端到端训练模型，但如今的 SOTA LLM 可以用于处理许多用例。好的数据 + 大量的计算 = 不错的模型。就这样？ 如果我可以用一小部分计算来训练这些模型，我可能会更感兴趣，但这样做是不合理的。那些没有计算能力的人只能进行微调或快速工程，而我内心的 SWE 发现这很无聊。这个领域的大多数人真的把精力投入到下一个标记预测器中了吗？ 显然，LLM 具有颠覆性，并且已经发生了很大的变化，但从研究的角度来看，它们对我来说并不有趣。还有人有这种感觉吗？对于那些因为与 LLM 无关的东西而被该领域吸引的人，你对此有何感想？你是否希望 LLM 的炒作会逐渐消退，以便焦点可以转移到其他研究上？那些在当前趋势之外进行研究的人：你如何处理所有的噪音？    提交人    /u/leetcodeoverlord   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/</guid>
      <pubDate>Thu, 01 Aug 2024 01:40:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] Google 的 Gemma-2-2B 与 Microsoft Phi-3：医疗保健领域小型语言模型的比较分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh3clp/d_googles_gemma22b_vs_microsoft_phi3_a/</link>
      <description><![CDATA[      探索 Google 的 Gemma-2-2b-it 和 Microsoft 的 Phi-3-4k 模型在医疗领域的表现。 （未经微调） Google 的 Gemma-2-2b-it 平均表现出色，得分为 59.21%，而 Microsoft 的 Phi-3-4k 以 68.93% 领先。 将很快为医疗领域评估更多小型模型 源帖子 https://preview.redd.it/lfh5gvm44zfd1.png?width=2779&amp;format=png&amp;auto=webp&amp;s=901da925caa150ba7a1ca31bf28f9cd5824280a0    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh3clp/d_googles_gemma22b_vs_microsoft_phi3_a/</guid>
      <pubDate>Thu, 01 Aug 2024 00:40:59 GMT</pubDate>
    </item>
    <item>
      <title>EC2 上的大型文件大数据集 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh37ao/large_dataset_of_large_files_on_ec2_d/</link>
      <description><![CDATA[因此，我们正在努力训练一个相对较小的 CNN 和一些其他项目，这些项目是大规模大文件数据集上的。 数据集：16TB 的 500mb 卫星图像 我目前有一个 g5 EC2 实例（4 个 gpu），带有一个 16TB EBS io1 驱动器，其中已加载数据。使用简单的数据加载器和数据集对象，我们在单磁盘读取速度上遇到了瓶颈。 直接从 S3 读取更好还是使用其他方法来加快读取速度更好？    提交人    /u/SuperbMonk4403   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh37ao/large_dataset_of_large_files_on_ec2_d/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:38 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 小型数据集的机器学习 (n<50)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eh25qt/discussion_ml_with_small_datasets_n50/</link>
      <description><![CDATA[我正在开展一个机器学习项目，涉及 4 个特征和一个将这 4 个特征相加的附加特征。有 5 个目标变量，所有特征和目标都是整数。 目前，我只有 1 行真实数据，因为获取更多数据非常麻烦。为了解决这个问题，我使用约束、关系逻辑和随机数生成器创建了 100 个模拟数据样本。数据中的关系相对线性（一个特征的度为 2，其他特征的度为 1）。由于范围较大（2000-3000），某些目标的均方误差 (MSE) 相当高（200-700），而对于其他目标，由于范围较小（5-20​​），均方误差较小（9-15）。 我尝试了各种模型：多元线性回归、岭回归、梯度提升和 MLP 回归。基于最低 MSE 的最佳结果是，对于 5 个目标中的 4 个，使用多项式交互的岭回归（一个特征的度为 2，其余特征的度为 4），对于剩余的目标，使用梯度提升。 但是，由于数据集较小，我在超参数调整方面遇到了瓶颈。对于如此有限的数据，更高级的方法是不可行的。当我们最终获得真实数据时，我们可能会有大约 40 个样本。 我的问题是：是否有可能使用如此小的数据集创建准确的 ML 模型？    提交人    /u/CashCrane   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eh25qt/discussion_ml_with_small_datasets_n50/</guid>
      <pubDate>Wed, 31 Jul 2024 23:45:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在 iOS 上高效本地运行模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egz3s0/d_how_to_efficiently_run_models_locally_on_ios/</link>
      <description><![CDATA[我正在构建一个 iOS 应用程序，需要处理以下约束，因为我正在为自定义键盘扩展构建自动更正解决方案： - 70MB 内存使用量 - 50-150ms 延迟 我发现可以完成这项工作的主要模型是 ELECTRA (https://huggingface.co/docs/transformers/en/model_doc/electra#transformers.TFElectraForMaskedLM) 但是，使用 CoreML 或 TensorFlowLite 在本地运行模型最终会增加太多开销，无法保持在 70MB 内存使用量以下，即使模型文件本身的大小为 18MB。 我也尝试在 AWS EC2 t3-large 上部署模型实例，但这里的延迟是问题所在。 有什么建议吗？    提交人    /u/pawn5gamb1t   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egz3s0/d_how_to_efficiently_run_models_locally_on_ios/</guid>
      <pubDate>Wed, 31 Jul 2024 21:33:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 平衡双摆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egy81m/d_balancing_a_double_pendulum/</link>
      <description><![CDATA[双摆是非确定性/混沌系统的最佳示例之一，因此产生了构建一个能够使其在直立位置保持平衡的模型的想法。双摆不是连接到固定点，而是连接到可移动（可控制）的滑块。 这是当前的进展：Balancing-Double-Pendulum 至于模型，我认为使用物理信息神经网络 (PINN) 是与遗传算法一起的最佳选择。当前的实现使用了基于简单公式的近似值，但显然某种积分器会更好（为了节约能源）。 目前，模拟无需使用任何优化算法就能正常工作，但如果需要的话 - 我认为 QuadTree 可以完成这项工作。 在模型和模拟方面我应该做任何改进吗？    提交人    /u/Mynameiswrittenhere   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egy81m/d_balancing_a_double_pendulum/</guid>
      <pubDate>Wed, 31 Jul 2024 20:57:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] 注释词汇表（可能）就是您所需要的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</link>
      <description><![CDATA[论文链接：https://www.biorxiv.org/content/10.1101/2024.07.30.605924v1 摘要：  蛋白质语言模型 (pLM) 彻底改变了蛋白质系统的计算建模，构建了以结构特征为中心的数值嵌入。为了增强蛋白质嵌入中可用的生化相关属性的广度，我们设计了注释词汇表，这是一种由结构化本体定义的蛋白质属性的转换器可读语言。我们从头开始训练注释变换器 (AT)，以恢复掩蔽的蛋白质属性输入，而无需参考氨基酸序列，仅基于蛋白质描述构建新的数值特征空间。我们在各种模型架构中利用 AT 表示，用于蛋白质表示和生成。为了展示注释词汇集成的优点，我们进行了 515 次不同的下游实验。使用新颖的损失函数和仅 3 美元的商业计算，我们的首要表示模型 CAMP 为 15 个常见数据集中的 5 个生成了最先进的嵌入，其余数据集上的性能也具有竞争力；凸显了使用注释词汇进行潜在空间管理的计算效率。为了标准化从头生成的蛋白质序列的比较，我们提出了一种新的基于序列比对的分数，它比传统的语言建模指标更灵活、更具生物学相关性。我们的生成模型 GSM 使用类似 BERT 的生成方案从仅注释提示中生成高比对分数。特别值得注意的是，许多 GSM 幻觉返回具有统计意义的 BLAST 命中，其中富集分析显示与注释提示匹配的属性 - 即使基本事实与整个训练集的序列同一性较低。总体而言，注释词汇工具箱提供了一种有前途的途径，可以用本体和知识图谱的成员取代传统的标记，增强特定领域的转换器模型。注释词汇对蛋白质的简洁、准确和有效的描述为构建蛋白质的数值表示以进行蛋白质注释和设计提供了一种新颖的方法。  我们很自豪地宣布发布我们的最新作品！请阅读、分享并提出任何问题！    提交人    /u/TeamArrow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</guid>
      <pubDate>Wed, 31 Jul 2024 17:47:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 无限上下文长度真的可能吗？：“Unlimiformer”作者周五讨论 NeurIPS 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</link>
      <description><![CDATA[无限上下文长度真的可能吗？代价是什么？ 2023 年 NeurIPS 论文 Unlimiformer 的作者 Amanda Bertsch 将在本周五的 Oxen.ai 论文俱乐部中描述该架构并回答问题。  Oxen 首席执行官兼 Plain Speak 大师 Greg Schoeninger u/FallMindless3563 将帮助解释该概念并将其与我们审阅过的其他论文联系起来。 致电：https://oxen.ai/community  声称使无限上下文长度成为可能的技巧：将交叉注意力计算卸载到 K-最近邻 (K-NN) 索引。  我在这里发了一条推文，其中某人制作了巧妙的 K-NN 动画：https://x.com/mustafarrag/status/1817647917059944474 论文：https://arxiv.org/abs/2305.01625 Greg，我将用我的前 5 个问题来回答。到目前为止，我只阅读了摘要。    提交人    /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</guid>
      <pubDate>Wed, 31 Jul 2024 15:47:52 GMT</pubDate>
    </item>
    <item>
      <title>[N] Finegrain 的对象橡皮擦演示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</link>
      <description><![CDATA[      对象橡皮擦 Finegrain 刚刚在 Huggingface 上发布了一个对象橡皮擦的演示。该模型可以删除任何对象或图像，并删除来自对象的任何效果（阴影、反射、光线）。与我们迄今为止拥有的其他橡皮擦相比，结果令人印象深刻；你怎么看？ 演示链接：https://huggingface.co/spaces/finegrain/finegrain-object-eraser    由   提交  /u/nota-Reddit   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</guid>
      <pubDate>Wed, 31 Jul 2024 15:20:10 GMT</pubDate>
    </item>
    <item>
      <title>为大型语言模型提供更好内存的框架 - 免费且开源。[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</link>
      <description><![CDATA[Github - https://github.com/chisasaw/redcache-ai 使用的 SDK [scikit-learn、numpy 和 openai] 什么？ 在构建聊天应用程序时，我找不到经济高效、可扩展且价格合理的内存层。这导致了 redcache-ai。它是一个提供语义搜索、存储和检索增强生成 (RAG) 的 Python 包。 用例？ 如果开发人员想要构建使用文档摘要和/或语义搜索的桌面应用程序，开发人员可以使用 redcache-ai 和选择的大型语言模型提供程序。聊天应用程序存储用户会话也是如此。 优点？  易于使用。只需像安装 Python 包一样安装它，即“pip install redcache-ai”。 提供存储可扩展性。将您的记忆存储到磁盘、sqlite 或您选择的数据库中。  请提供反馈并提出问题。也可以随时为项目做出贡献。    提交人    /u/hack_knight   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</guid>
      <pubDate>Wed, 31 Jul 2024 13:02:41 GMT</pubDate>
    </item>
    <item>
      <title>关于使用知识图谱的神经符号人工智能的调查论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</link>
      <description><![CDATA[  由    /u/joestomopolous  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</guid>
      <pubDate>Wed, 31 Jul 2024 11:06:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    </channel>
</rss>