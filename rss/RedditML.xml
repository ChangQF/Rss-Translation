<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sun, 28 Apr 2024 21:12:43 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么 RETRO 不是法学硕士中的主流/最先进的技术？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cffgkt/d_why_isnt_retro_mainstream_stateoftheart_within/</link>
      <description><![CDATA[2021 年，Deepmind 发布了通过检索数万亿个标记来改进语言模型，并引入了检索增强型 Transformer (RETRO)。RAG 通常涉及在推理时通过将相关文档注入上下文来补充输入标记，而 RETRO 可以在训练和推理期间从外部数据库访问相关嵌入。目标是将推理和知识分离：通过允许按需查找，模型可以从必须记住其权重中的所有事实中解放出来，而是将能量重新分配给更有影响力的计算。结果非常惊人：RETRO 以少 25 倍的参数实现了与 GPT-3 相当的性能，并且理论上没有知识截止（只需向检索数据库添加新信息！）。 然而：今天，据我所知，大多数主要模型都没有包含 RETRO。LLaMA 和 Mistral 当然没有，我也不觉得 GPT 或 Claude 有（唯一可能的例外是 Gemini，因为 RETRO 团队的大部分成员现在都是 Gemini 团队的一部分，而且根据我的经验，它既更快又更实时）。此外，尽管 RAG 一直很热门，而且有人可能会认为 MoE 可以实现这一点，但明确地将推理与知识解耦作为研究载体却相对平静。 有人能自信地解释为什么会这样吗？我觉得 RETRO 是一项伟大的高效前沿进步，它就在众目睽睽之下，等待着被广泛采用，但也许我忽略了一些显而易见的东西。    提交人    /u/whitetwentyset   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cffgkt/d_why_isnt_retro_mainstream_stateoftheart_within/</guid>
      <pubDate>Sun, 28 Apr 2024 19:58:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个云提供商，提供经济实惠且轻松的 GPU 访问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfek96/p_i_created_a_cloud_provider_for_affordable_easy/</link>
      <description><![CDATA[您好r/MachineLearning！ 我很高兴推出 Backprop GPU 云 - 在公共市场上托管 GPU 服务器三年后，我决定构建自己的云提供更好的服务。 我关注的是速度、价格和可靠性： - 实例是在 60 秒内创建的，并预装了 Jupyter。 - 定价合理，没有存储或带宽方面的隐藏费用。 - RTX 3090 实例托管在具有 10 Gbps 网络的三级数据中心。 如果您是学生或研究人员，我很乐意为您提供 10 小时的免费积分。只需注册并向我发送消息即可。 我希望添加更多功能和其他实例类型。所有反馈都会有很大帮助！   由   提交 /u/ojasaar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfek96/p_i_created_a_cloud_provider_for_affordable_easy/</guid>
      <pubDate>Sun, 28 Apr 2024 19:21:09 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 直观地深入了解 Uber 用于预测预计到达时间的机器学习解决方案。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfd15u/research_a_visual_deep_dive_into_ubers_machine/</link>
      <description><![CDATA[      TL;DR：Uber 遵循 2 层方法。他们将 Dijkstra 等传统图算法与学习嵌入和轻量级自注意力神经网络相结合，以可靠地预测预计到达时间或预计到达时间。 Uber 如何使用机器学习来预测预计到达时间（并解决十亿美元的问题）&lt; /p&gt; https://preview.redd。 it/2ovttr82i9xc1.png?width=1358&amp;format=png&amp;auto=webp&amp;s=51b12261bf98f529fd0e9b33daf6362b727f4580   由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfd15u/research_a_visual_deep_dive_into_ubers_machine/</guid>
      <pubDate>Sun, 28 Apr 2024 18:18:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] VMRNN：集成 Vision Mamba 和 LSTM 以实现高效准确的时空预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfcxfp/r_vmrnn_integrating_vision_mamba_and_lstm_for/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2403.16536 代码：https://github.com/yyyujintang/VMRNN-PyTorch 摘要：  将 CNN 或 ViT 与 RNN 结合用于时空预测，在预测时间和空间动态方面取得了无与伦比的成果。然而，对广泛的全局信息进行建模仍然是一项艰巨的挑战；CNN 受到其狭窄的接受场的限制，而 ViT 则难以应对其注意力机制的密集计算需求。最近出现的基于 Mamba 的架构因其出色的长序列建模能力而受到热烈欢迎，在效率和准确性方面超越了现有的视觉模型，这促使我们开发一种专为时空预测而定制的创新架构。在本文中，我们提出了 VMRNN 单元，这是一种新的循环单元，它将 Vision Mamba 块与 LSTM 的优势相结合。我们构建了一个以 VMRNN 单元为中心的网络，以有效地处理时空预测任务。我们广泛的评估表明，我们提出的方法在保持较小模型尺寸的同时，在各种任务上都能获得有竞争力的结果。我们的代码可在此 https URL 上找到。     提交人    /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfcxfp/r_vmrnn_integrating_vision_mamba_and_lstm_for/</guid>
      <pubDate>Sun, 28 Apr 2024 18:14:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 分类深度学习：架构的代数理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfck8b/r_categorical_deep_learning_an_algebraic_theory/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.15332 项目页面：https://categoricaldeeplearning.com / 摘要：  我们提出了我们的立场，即寻找一个难以捉摸的通用框架来指定和研究深度学习学习架构。我们的观点是，迄今为止所做的关键尝试在指定模型必须满足的约束和指定其实现之间缺乏连贯的桥梁。着眼于建立这样一座桥梁，我们建议应用范畴论——准确地说，是在参数映射的 2 类别中评估的通用单子代数——作为单一理论优雅地包含了神经网络设计的这两种风格。为了捍卫我们的立场，我们展示了该理论如何恢复由几何深度学习引起的约束，以及从不同的神经网络（例如 RNN）中提取的许多架构的实现。我们还说明了该理论如何自然地编码计算机科学和自动机理论中的许多标准结构。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfck8b/r_categorical_deep_learning_an_algebraic_theory/</guid>
      <pubDate>Sun, 28 Apr 2024 17:59:19 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] 使用 Transformer 模型预测随机流</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfbbk0/rp_predicting_stochastic_flows_using_transformer/</link>
      <description><![CDATA[我有一个流体流动问题，其中温度、速度和流动方向将根据我们过去收集的许多实验实例进行建模30年的研究。尽管流动遵循流体力学方程，但现实生活场景本质上是相当随机的。我想知道我们是否可以构建一个速度嵌入（比如给定当前和过去的速度下一个可能的速度是什么），然后训练一个变压器来解决这个问题？可能类似于 GPT，但它会预测下一个物理状态（T、vx、vy、vz）等，而不是下一个单词？有人尝试过做这样的事情吗？作为一名仅对深度学习和 NLP 有基本了解的科学家，我可以从哪里开始尝试现成的模型？ 编辑：我知道有几个预那里有经过训练的模型，但我们的问题与常规流体力学有很大不同（具有纳米效应的 QM）。所以我必须从头开始。    由   提交/u/bahauddin_onar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfbbk0/rp_predicting_stochastic_flows_using_transformer/</guid>
      <pubDate>Sun, 28 Apr 2024 17:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] Self-Refine vs Reflexion - 哪种方法更能提高法学硕士的输出质量？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfb2l5/d_selfrefine_vs_reflexion_which_method_is_better/</link>
      <description><![CDATA[嗨，我正在尝试这两种方法，到目前为止，自我优化似乎在我的情况下效果更好。我的应用程序获取一些数字数据并尝试理解它。 我有两个问题：  其他人是否也有类似的感觉，Self-Refine 是更好的？  在反射论文中https://arxiv.org/pdf/2303.11366 ，有一句话Self-Refine is effective but is limited to single- Generation Reasoning Task.这里的单代推理任务是什么意思？我认为当你迭代反馈优化循环时，这不是单代推理，但也许我错了。  此外，我很想听听你的实现两篇论文的   由   提交/u/Educational-String94  /u/Educational-String94 reddit.com/r/MachineLearning/comments/1cfb2l5/d_selfrefine_vs_reflexion_which_method_is_better/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfb2l5/d_selfrefine_vs_reflexion_which_method_is_better/</guid>
      <pubDate>Sun, 28 Apr 2024 16:55:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 MLops/ML 基础设施的 ML 白皮书</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf9d3s/d_ml_white_papers_on_mlopsml_infra/</link>
      <description><![CDATA[我正在寻找来自顶级科技公司的有关机器学习基础设施/mlops/机器学习工程的白皮书 您知道吗或者可以给我指出一些吗？ 我指的不是像注意力就是你所需要的那样的东西。寻找更多面向工程的论文   由   提交/u/choose_cake  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf9d3s/d_ml_white_papers_on_mlopsml_infra/</guid>
      <pubDate>Sun, 28 Apr 2024 15:43:18 GMT</pubDate>
    </item>
    <item>
      <title>多模态机械臂策略还是LLM基础模型训练？[D][R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf8gmp/multimodal_robotic_arm_strategy_or_llm_base_model/</link>
      <description><![CDATA[我目前有2个研究机会，关于多模态机械臂策略和LLM基础模型训练，我不知道选择哪一个？有什么建议吗欢迎或分析！   由   提交 /u/CrisYou   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf8gmp/multimodal_robotic_arm_strategy_or_llm_base_model/</guid>
      <pubDate>Sun, 28 Apr 2024 15:03:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多模态补丁嵌入 - 一种新的 ViT 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf8d26/r_multimodal_patch_embeddings_a_new_vit_model/</link>
      <description><![CDATA[这个研究项目是对这个想法的探索 - 如果您可以将文本嵌入与 ViT 的每个补丁嵌入进行比较会怎样？.我尝试了一些事情并得到了一些有希望的结果。  这里有两个关键思想： 1. 将图像嵌入作为超球面上点的凸和，其中每个点都是一个补丁嵌入。这需要对 ViT 架构进行更改，此处对此进行了解释。 2. 限制每个补丁仅关注其邻居。 通过这种架构，我使用蒸馏来学习一个小型（约 21M 参数）模型，使其具有与预训练的 Vit-B/32 相同的图像嵌入在大约 310 万张图像上建立模型（约 8700 万参数）。这导致补丁嵌入是局部感知的，但必须学会以某种方式组合，以便提供全局图像嵌入。 代码和检查点是可用的，并且包含用于复制的训练、推理和笔记本。结果：https://github.com/TinyVolt/multimodal-patch-embeddings   由   提交/u/nivter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf8d26/r_multimodal_patch_embeddings_a_new_vit_model/</guid>
      <pubDate>Sun, 28 Apr 2024 14:59:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您如何诊断训练损失中的这些峰值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf4gw9/d_how_would_you_diagnose_these_spikes_in_the/</link>
      <description><![CDATA[   /u/NumberGenerator  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf4gw9/d_how_would_you_diagnose_these_spikes_in_the/</guid>
      <pubDate>Sun, 28 Apr 2024 11:44:29 GMT</pubDate>
    </item>
    <item>
      <title>“变形金刚可以使用无意义的填充标记（例如，‘......’）来代替一连串的思想” - Let's Think Dot by Dot [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf2u0d/transformers_can_use_meaningless_filler_tokens_eg/</link>
      <description><![CDATA[https://arxiv.org/abs/2404.15758 从摘要开始 我们表明，变压器可以使用无意义的填充标记（例如“......”）代替一系列思想来解决两个问题在没有中间令牌的情况下进行响应时，他们无法解决困难的算法任务。然而，我们根据经验发现，学习使用填充令牌很困难，需要特定的、密集的监督才能收敛   由   提交 /u/Agitated_Space_672   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf2u0d/transformers_can_use_meaningless_filler_tokens_eg/</guid>
      <pubDate>Sun, 28 Apr 2024 09:59:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将您的 LLM（应用程序/系统）转移到生产环境中最常见和最重大的挑战是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf178i/d_what_are_the_most_common_and_significant/</link>
      <description><![CDATA[目前有很多人使用法学硕士进行构建，但没有那么多人从原型和 POC 过渡到生产。尤其是在企业环境中，但我相信这对于产品公司甚至一些专注于基于 LLM 的应用程序的初创公司来说也是类似的。事实上，一些调查和研究认为这一比例低至5%。  从事这一领域工作的人们，在尝试将产品投入生产时遇到的最常见和最困难的挑战是什么？目前您是如何解决这些挑战的？    由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf178i/d_what_are_the_most_common_and_significant/</guid>
      <pubDate>Sun, 28 Apr 2024 08:07:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 RAG 的真实讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</link>
      <description><![CDATA[老实说吧。我知道我们都必须与这些经理/董事/CXO 打交道，他们提出了与公司数据和文档交谈的惊人想法。 但是……有人真正做了一些真正有用的事情吗？如果是这样，它的有用性是如何衡量的？ 我有一种感觉，我们被一些非常复杂的废话所愚弄，因为法学硕士总是可以产生在某种程度上听起来合理的东西。但它有用吗？   由   提交/u/fusetron  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</guid>
      <pubDate>Sat, 27 Apr 2024 18:00:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>