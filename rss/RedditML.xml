<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 09 Dec 2024 18:25:12 GMT</lastBuildDate>
    <item>
      <title>[D] 如何确保 NLP 实验中小样本提示与微调之间的公平比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/</link>
      <description><![CDATA[我正在研究在文本分类任务中比较小样本提示机制和微调 GPT 模型。然而，我意识到在 5 倍验证设置中，与我的小样本方法相比，微调模型可以访问更多数据（例如：4 倍用于训练），而我的小样本方法仅使用有限数量的示例进行提示（目前，我从训练集中为每个类别选择 n 个样本）。  示例场景：我的数据集有 4 个类，总数据量为 100，我正在进行 4 向 5 样本实验。对于训练集，我有 80 个（因为它是 5 倍），对于测试，我有 20 个数据样本。对于微调实验，我使用全部 80 个数据，但对于小样本实验，我仅使用 80 个训练样本中的 20 个（4*5）数据。因此该方法只能访问整个训练集的 20 个样本。 这种不平衡感觉不公平，并且很难评估两种方法之间的真正性能差异。我该如何修改实验设置以确保公平比较？我是否应该限制微调模型使用与少样本提示机制中使用的相同示例？ 很想听听您的想法和建议！    提交人    /u/The_Aoki_Taki   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/</guid>
      <pubDate>Mon, 09 Dec 2024 17:32:25 GMT</pubDate>
    </item>
    <item>
      <title>有人遇到过 GAN 生成的图像中的线条吗？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/</link>
      <description><![CDATA[      所以我已经使用 GAN 一段时间了，用于简单的图像生成任务，特别是以无监督的方式训练它们。在其中许多任务中，GAN 生成的输出往往在图像上有可见的线条。这是一个例子，当我尝试生成热图时发生了这种情况。你们有人知道为什么会发生这种情况吗？以及处理它们的方法 https://preview.redd.it/c5tuc5udsu5e1.png?width=679&amp;format=png&amp;auto=webp&amp;s=0ecb84a81ec2993a147689ae3112935cc6ecf821    提交人    /u/Brief_Papaya121   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/</guid>
      <pubDate>Mon, 09 Dec 2024 16:56:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于蒸馏的 3D 神经辐射场着色，实现一致的新颖视图合成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/</link>
      <description><![CDATA[本文介绍了一种知识提炼方法，用于从灰度多视图图像中对 3D 神经表征（NeRF / 3DGS）进行着色。核心思想是将颜色信息从预先训练的 2D 着色模型转移到 3D 场景表示，同时保持视图一致性。 关键技术方面： - 使用师生框架，其中 2D 着色模型指导 3D 表示 - 适用于神经辐射场和 3D 高斯溅射 - 推理期间不需要额外的参数或计算 - 处理室内/室外场景和不同类型的灰度输入（IR，历史照片） - 通过体积优化保持跨视点的颜色一致性 结果： - 在标准基准上达到或超过 SOTA 着色质量 - 成功地为具有不同光照/材质的复杂场景着色 - 有效地处理传统照片和红外图像 - 在新颖的视点中展示一致的色彩 - 与当前的 NeRF / 3DGS 实现兼容 我认为这种方法对于文化遗产应用特别有价值，使我们能够从历史黑白照片中创造身临其境的 3D 体验。红外成像功能还表明在安全和监控领域有潜在的应用，其中热数据的彩色可视化将很有用。 我认为关键优势在于它如何弥合 2D 着色和 3D 场景理解之间的差距，而无需对现有的 3D 表示进行架构更改。这使得它在现实世界中非常实用。 TLDR：新方法使用知识蒸馏从灰度图像中为 3D 神经场景着色，与 NeRF/3DGS 配合使用，保持视图一致性，无需额外的推理成本。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/</guid>
      <pubDate>Mon, 09 Dec 2024 16:55:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人成功训练过具有模型并行性的 LLM 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/</link>
      <description><![CDATA[您好， 我正在为我的硕士论文研究微调 Llama-3.1。不幸的是，我目前的情况不允许使用高内存 GPU，例如 A100。相反，我可以使用具有多个低内存 GPU 的设置，例如 4×3090 或 8×V100。 因此，我需要实现模型并行性来训练我的模型，因为它不适合单个 GPU。但是，我注意到大多数框架主要关注数据并行性，这并不能满足我的需求。 有没有人通过将模型拆分到多个 GPU 上来成功训练模型？如果有，您能推荐我应该探索的框架或方法吗？我特别想寻找完整的培训，尽管我有兴趣听听是否有人使用 LoRA 管理过这个。 此外，如果有更适合此类问题的 subreddit，请引导我到那里。 谢谢！    提交人    /u/anilozlu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/</guid>
      <pubDate>Mon, 09 Dec 2024 15:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 文本转视频排行榜：比较最先进的文本转视频模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/</link>
      <description><![CDATA[与文本生成不同，文本转视频生成涉及平衡真实感、对齐和艺术表达。但就输出质量而言，哪一个最重要？ 我们不知道，这就是为什么我们创建了一个基于投票的文本转视频模型排行榜，灵感来自 LLM 排行榜 lmarena.ai。 目前，排行榜有五个开源模型：HunyuanVideo、Mochi1、CogVideoX-5b、Open-Sora 1.2 和 PyramidFlow，但我们的目标是还包括来自 Kling AI、LumaLabs.ai 和 Pika.art 的著名专有模型。 这是排行榜的链接：link。 我们很乐意听到您的想法、反馈或建议。您认为应该如何评估视频生成模型？    提交人    /u/lambda-research   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/</guid>
      <pubDate>Mon, 09 Dec 2024 08:17:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] Monet：Transformer 的单语义专家组合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2412.04139 GitHub：https://github.com/dmis-lab/Monet Monet 通过创新的稀疏混合专家 (SMoE) 架构提出了一种增强大型语言模型 (LLM) 中机械可解释性的新方法。通过将稀疏词典学习直接纳入端到端预训练，Monet解决了多义性的基本挑战 - 单个神经元对多个不相关的概念作出反应 - 同时保持模型性能。 主要亮点：  可扩展的专家架构：Monet引入了参数高效的专家分解方法，可以扩展到每层 262,144 位专家，同时确保总参数与专家数量的平方根成比例扩展。 单义专家：通过细粒度的专家专业化，Monet实现了展示知识互斥性的单义专家，允许透明地观察模型行为和参数知识。 强大的知识控制：该架构能够精确操纵领域特定知识、语言能力和毒性缓解，而不会影响一般性能。  为什么选择 Monet？ 与使用事后重建的传统方法（如稀疏自动编码器）不同，Monet 将可解释性直接集成到其架构中。这样既可以透明地理解模型内部结构，也可以控制基本行为。通过扩展单语义专家，Monet 为更透明、更可控的语言模型铺平了道路。 我们很乐意听到您的反馈、问题或您可能有的任何其他疑问！    提交人    /u/affjljoo3581   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/</guid>
      <pubDate>Mon, 09 Dec 2024 07:31:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 寻找每日关键词搜索数据库（任何平台）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/</link>
      <description><![CDATA[大家好， 在彻底搜索 Google 并尝试查找允许我以每日为单位在任何平台上生成关键字搜索或帖子或评论频率的 API 后，我找不到任何此类数据提供商。考虑到这是一种小众请求，我在此提出此询问，以寻求 Reddit 的 ML 大神的帮助。 基本上，我正在尝试创建一个 ML 模型，可以预测未来关键字使用量的增加/减少（无论是在 Google 搜索还是 X 帖子上；无所谓）以每日为单位。我找到了很多每月平均关键字搜索提供商，但我找不到任何方法来访问任何平台的更细粒度的每日搜索总数。如果您知道任何此类数据的来源，请将其放在这里...或者，如果这是不可能完成的任务，就告诉我放弃。    提交人    /u/Appropriate-Touch515   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/</guid>
      <pubDate>Mon, 09 Dec 2024 07:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 扩散模型、图像超分辨率以及一切：综述</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/</link>
      <description><![CDATA[我们很高兴与大家分享我们最新的关于应用于图像超分辨率的扩散模型的调查论文。欢迎您阅读。它也是开放获取的，并发表在 IEEE TNNLS 上 :)  arXiv：https://arxiv.org/abs/2401.00736    提交人    /u/Maleficent_Stay_7737   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/</guid>
      <pubDate>Mon, 09 Dec 2024 00:09:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 存在“可积规划”这样的东西吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/</link>
      <description><![CDATA[我来自纯数学背景，最近开始从事科学 AI/ML 的新工作，在工作中我经常使用 JAX。JAX 很棒，我喜欢它，但我看到了一种非常常见的模式，研究人员会有一个完全可微分的模拟和几个神经网络架构之类的东西，但随后会有一堆相对不精确的积分值数值估计。显然，我正在阅读数值方法，并尽我所能重构问题以更代数地解决问题，但我自己的好奇心是，有没有一种相当于“可微分”编程的方法，你处理的是“可积”实体？ 显然，这将是一类更难的问题，因为你可以积分……好吧，一切。这就是你最终在紧凑支撑上求解具有奇怪丑陋的 Holder 边界的 PDE 的方法。但是，有没有计算方法（或者我应该知道的可微分编程策略）朝这个方向发展？是否有很好的自然代数属性可以利用？你能以同样的方式使用计算图吗？怎么样，比如，扩展到“弱可微分”函数式编程的有效方法？ 希望这足够相关，因为它是学习 JAX 启发的……    提交人    /u/redwingviking   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/</guid>
      <pubDate>Sun, 08 Dec 2024 21:56:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 LLM 进行上下文感知实体识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/</link>
      <description><![CDATA[有人能推荐一些可以执行实体识别但使用 LLM 级上下文的好模型吗？此类模型通常是针对实体识别进行微调的 LLM。通常，使用传统的 NER/ER 管道（例如 SpaCy 的 NER 模型）只能标记已经训练过的单词。使用针对实体识别进行微调的 LLM（例如 GLiNER 模型）可以标记模糊实体，而不仅仅是基本实体，例如名称、地点、组织等。    提交人    /u/Ashwiihii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/</guid>
      <pubDate>Sun, 08 Dec 2024 21:05:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我应该在我的学术论文中使用 MLflow 或 DVC 等机器学习实验跟踪工具吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/</link>
      <description><![CDATA[大家好！ 我是一名计算机科学毕业生，目前正在为研究论文进行机器学习实验。我有一个数据集，计划比较多个深度学习模型之间的错误指标。 我打算提交论文的会议要求我提供代码和数据集，我也坚信可重复性在学术研究中至关重要。为此，我正在使用 Docker 和 pip-compile 使环境尽可能可重复。 话虽如此，我知道有像 MLFlow 和 DVC 这样的工具可以跟踪 ML 实验。但是，我从未在学术论文附带的代码中看到过这些工具。 我的问题是：  有没有学术论文使用 MLFlow 或 DVC 等 ML 实验跟踪工具？ 我应该将这些工具用于我的研究吗，即使这意味着额外的工作？  我也在尝试 DVC，因为它将实验输出存储在 Git 中。但是，我的项目涉及在单个存储库中运行许多不同的实验（比较多种 ML 算法）。DVC 或其他工具是否是这种工作流程的最佳选择？或者对于学术论文来说，使用这样的工具是不是有点小题大做？    提交人    /u/mrlucasrib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/</guid>
      <pubDate>Sun, 08 Dec 2024 13:05:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 各种 LLM 抽样方法的集合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/</link>
      <description><![CDATA[在过去的几个月里，我阅读了有关执行 LLM 采样的各种算法。我决定构建自己的推理堆栈并实现这些算法。 这是 Github 仓库 - https://github.com/shreyansh26/LLM-Sampling 该仓库包括 Top-k、Top-p（核心）、Min-p、典型、Epsilon、Eta、Beam 搜索、Chain-of-Thought (CoT) 解码、Constrained JSON 解码和 Speculative 解码的实现。 就我个人而言，我发现这是一个很好的学习经历。在这里分享，以防它对某人有帮助！    提交人    /u/shreyansh26   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/</guid>
      <pubDate>Sun, 08 Dec 2024 09:39:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] 🥂 FineWeb2 数据集：包含数千种语言的闪亮更新</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h9ep0e/p_fineweb2_dataset_a_sparkling_update_with_1000s/</link>
      <description><![CDATA[    /u/PhilipsNostrum   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h9ep0e/p_fineweb2_dataset_a_sparkling_update_with_1000s/</guid>
      <pubDate>Sun, 08 Dec 2024 08:47:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 08 Dec 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>