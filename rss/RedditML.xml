<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Wed, 17 Apr 2024 00:58:33 GMT</lastBuildDate>
    <item>
      <title>[项目] 开源项目，通过基于自然语言的图像搜索将 Twitter 数据转换为 Excel。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5wg0o/project_open_source_project_that_turns_twitter/</link>
      <description><![CDATA[   我很高兴分享我开发的项目： &lt;一款开源工具，可简化将 Twitter 数据（例如推文和点赞）导出到 Excel 的过程，并且完全免费。  它设计有图像搜索和可视化等附加功能，可增强您的数据体验。您可以在这里查看：GitHub 上的 Twitter-Insight-LLM ​ 导出的表格和生成的可视化 背景：我经常使用 Twitter 点赞作为学术论文、想法或只是有趣的照片的书签。然而，这些书签积累得很快，变得难以管理和搜索。 ​ 问题：  官方 Twitter API 非常昂贵，价格从每月 100 美元到 500 美元不等。 从 Twitter 导出完整数据并不方便用户使用，其中包含笨重的 HTML 文件，而且经常会丢失重要数据。   ​ 我的解决方案：  快速导出： 利用 Selenium 自动收集您的所有推文或点赞，并在短短几分钟内将它们组织到用户友好的 Excel 文件中。 视觉洞察： 提供动态可视化以帮助您分析一目了然地了解您的 Twitter 活动。  ​ 使用图像嵌入模式进行自然语言图像搜索。 实验功能 - 自然语言图像搜索：&lt; /strong&gt;  轻松搜索：使您能够使用简单的文本查询从推文中查找图像，由图像嵌入提供支持 - 非常适合快速数据可视化和分析。  轻松搜索： li&gt; 成本和资源效率高：运行平稳，无需任何昂贵的 GPU 或额外费用。  ​  图像说明： 或者，还涉足使用基于视觉的 LLM 来生成说明文字（与 Twitter 文本一起）以​​进行索引。这现在也很有用，因为视觉模型变得越来越便宜（例如 Claude Haiku） 此处的示例最初由 GPT-4-V 生成，开源模型（例如 LlaVa 或基于 API 的 Claude Haiku）也应该可以很好地工作.  或者，您可以使用 Vision基于 llm 为图像添加标题。 （比图像嵌入成本更高）  ​ ​ 希望你们觉得它有用，我很高兴听到任何反馈！    由   提交/u/G9X  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5wg0o/project_open_source_project_that_turns_twitter/</guid>
      <pubDate>Wed, 17 Apr 2024 00:44:50 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 云工作负载遣返</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5s4p4/discussion_cloud_workload_repatriation/</link>
      <description><![CDATA[我听到更多有关公司由于成本和效率问题而将某些工作负载从云返回到数据中心的消息。有人听说过类似的事情或者对特定的工作负载有深入的了解吗？   由   提交 /u/viperliberty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5s4p4/discussion_cloud_workload_repatriation/</guid>
      <pubDate>Tue, 16 Apr 2024 21:37:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] LSTM 究竟如何分配其权重和门以使其能够解释讽刺句子？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5ru3n/d_how_exactly_does_the_lstm_allocate_its_weights/</link>
      <description><![CDATA[在下面的例子中，““这部电影很烂，没有人说过！”，lstm的权重到底是如何分配的让 lstm 认识到这部电影很烂是一种积极的陈述，而没有人也会带来积极的情绪。 在这些 LSTM 中，至少当我在 pytorch/tf 中对它们进行编码时， LSTM 单元在 LSTM 单元链中保持相同的权重来表示序列。它如何知道何时忘记某些单词，何时不忘记其他单词？我认为这是因为遗忘门对于每个特征都有不同的遗忘值，并且当“没有人”被遗忘时当遇到该短语的一部分时，就像双重否定一样，它会产生积极的价值，因此对与消极词相对应的某个特征产生情感。我不确定我是否有道理，但我想从比我了解更多的人那里正确理解细节，哈哈。  最后，虽然没有直接相关，但是否可以使用基于 LSTM 的架构来确定欺诈性 IP 活动，即用户从不良 VPN 服务登录。这里的上下文很重要，因为如果他们已经使用同一个 VPN 几个月，那么坏人不太可能访问该帐户。    由   提交 /u/Jordanoer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5ru3n/d_how_exactly_does_the_lstm_allocate_its_weights/</guid>
      <pubDate>Tue, 16 Apr 2024 21:26:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer 的日志损失如何随着摄取的数据量而变化？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5q4of/d_how_does_a_transformers_log_loss_scale_with/</link>
      <description><![CDATA[我通常知道神经缩放论文（即 Chinchilla），这些论文显示了所用数据量的对数丢失错误率对数之间的准线性关系训练中。 我对自动驾驶汽车这样的情况特别感兴趣，随着时间的推移，模​​型会慢慢部署并收集数据。由于模型表现良好，大部分数据都被丢弃，只保留 %（模型与现实之间的损失很高）。  在这种情况下，您认为（或者他们的研究）对数损失会随着训练数据的增加而超线性吗？我的假设是，在这种情况下，数据始终被过滤为最“有用”的数据。用于改进模型，这与仅在问题中抛出所有类型数据的数量级不同。   由   提交/u/ZeApelido  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5q4of/d_how_does_a_transformers_log_loss_scale_with/</guid>
      <pubDate>Tue, 16 Apr 2024 20:16:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] GNN 可以用作所有类型数据的模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5olyc/d_can_gnns_be_used_as_model_for_all_types_of_data/</link>
      <description><![CDATA[因为似乎几乎每个数据集都可以转换为图表：  表格 - 节点作为行，没有它们之间的边缘 文本和音频 - 节点作为单词，在相邻单词之间具有有向边缘 时间序列 - 与 2 相同 图像 - 节点作为具有无向边缘的像素相邻像素之间的边缘（包括对角线）  即使 GNN 可以处理所有类型的数据，我认为将它们转换为图形可能会耗费时间和空间，特别是在图像的情况下. 同时，GNN 可以使一些基于表格数据的 ML 模型更加准确 - 例如如果我们有一个关于公寓定价的表格数据集，我们可以在同一社区的公寓之间添加边，以便它们的所有价格都相互依赖，并且这模拟了现实生活中的现象，即同一社区的公寓如何具有相互依赖的定价附近的状况（例如，如果附近的犯罪增加，所有公寓的价格都会下降）   由   提交/u/Snoo_72181   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5olyc/d_can_gnns_be_used_as_model_for_all_types_of_data/</guid>
      <pubDate>Tue, 16 Apr 2024 19:15:18 GMT</pubDate>
    </item>
    <item>
      <title>[项目]：我的自托管应用程序，供机器学习工程师处理所有工具和技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5mooj/project_my_selfhosted_app_for_ml_engineers_to/</link>
      <description><![CDATA[   我为软件工程师创建了一个名为 &lt; a href=&quot;https://snipman.io/&quot;&gt;Snipman.io &gt;&gt;&gt; https://snipman.io   它是一个自托管的代码片段管理应用程序（目前可以免费下载）在 Mac 和 Windows 上），基本上可以让您按代码片段类型存储代码片段。 我主要创建它是因为我发现自己为不同编程的小代码片段创建了大量文本文件语言、框架、工具、云、开发运营和技术例如 Python、PyTorch、AWS、GCP、Terraform、Kubernetes、Docker 等。这不仅导致大量混乱，而且在搜索和定位正确的代码片段时也很痛苦。 &lt; li&gt;我的目标是创建一些东西，允许所有命令、配置和片段存储在本地的中央存储库中然后就有能力快速搜索它们。我相信我的应用程序通过一个优雅且易于使用的基于 GUI 的工具帮助实现所有这些目标。  我希望这里的所有社区成员都能找到很有用！  ​ snipman.io 中的 Pytorch 代码片段示例 ​   由   提交/u/dev_user1091   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5mooj/project_my_selfhosted_app_for_ml_engineers_to/</guid>
      <pubDate>Tue, 16 Apr 2024 17:58:27 GMT</pubDate>
    </item>
    <item>
      <title>绝对嵌入与关系嵌入 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5l1pn/absolute_vs_relational_embeddings_r/</link>
      <description><![CDATA[我看过很多帖子说绝对位置嵌入仅适用于固定序列长度......但理论上，如果我们看一下公式，（ Positional使用正弦曲线编码矩阵）我们可以处理任意数量的时间步长和维度......所以我的问题是  这是真的（序列长度方面）吗？如果为真，为什么，如果为假，为什么？ 使用相对位置嵌入相对于绝对位置嵌入有什么优势？ 为什么要动态添加相对位置嵌入，而绝对位置嵌入是预先添加的？ - 计算？  例如：https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Is-Positional-Encoding-Required-In-所有语言模型/post/1450078   由   提交 /u/Ok-Cheesecake-8881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5l1pn/absolute_vs_relational_embeddings_r/</guid>
      <pubDate>Tue, 16 Apr 2024 16:53:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] Hugging Face 发布 Idefics，全新开放 8B 参数多模态模型，与 30B 参数模型竞争</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5kb19/r_hugging_face_releases_idefics_a_new_open_8b/</link>
      <description><![CDATA[      ​  https://preview.redd.it/5vqtawmqavuc1.png？ width=1216&amp;format=png&amp;auto=webp&amp;s=5a111c704b7e0b017dcd8b4bfb0e974c3e6069dc https://huggingface。 co/blog/idefics2   由   提交 /u/VictorSanh   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5kb19/r_hugging_face_releases_idefics_a_new_open_8b/</guid>
      <pubDate>Tue, 16 Apr 2024 16:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] IJCAI 2024论文结果及讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5drk7/d_ijcai_2024_paper_result_discussion/</link>
      <description><![CDATA[这是对 IJCAI 2024 接受/拒绝论文的讨论。结果应该很快就会发布。   由   提交/u/Living_Impression_37   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5drk7/d_ijcai_2024_paper_result_discussion/</guid>
      <pubDate>Tue, 16 Apr 2024 11:30:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] MoE 是否实现了总参数少于 1B 的情况？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5c4gz/d_is_there_moe_implemented_for_less_than_1b_total/</link>
      <description><![CDATA[我有几个问题：1）目前广泛使用的性能良好的 MoE 是哪个（是稀疏 moe 吗？） 2) DeepSpeeds MoE 如何适应这张图？相对于其他产品来说，它的表现如何？最近的高性能架构是否采用了它？ 3）是否有人尝试过 Sparse MoE 用于编码器-解码器模型，例如 flan t5？它的效果也同样好吗？如果是这样，为什么它没有像仅解码器变体那样流行？ 4）是否有人尝试过 MoE 用于小于 1B 总参数（不是 1B 活动参数）的较小模型。性能如何？   由   提交/u/FrozenWolf-Cyber​​   /u/FrozenWolf-Cyber​​ reddit.com/r/MachineLearning/comments/1c5c4gz/d_is_there_moe_implemented_for_less_than_1b_total/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5c4gz/d_is_there_moe_implemented_for_less_than_1b_total/</guid>
      <pubDate>Tue, 16 Apr 2024 09:48:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] Swin 变压器在线性探测方面表现不佳吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5atzv/d_do_swin_transformers_perform_poorly_at_linear/</link>
      <description><![CDATA[为了写一篇论文，我一直在使用 SimMIM 训练一些 Swin 变压器，并注意到 ImageNet1k 上的线性探测精度非常可怕。当我使用最小的 Swin 模型 Swin-T 时，第 25 个 epoch 后的性能仅为 2.5% top1（ViT-T 在相似数量的 epoch 后达到约 5% top1，而 ViT-B 达到 7%）。 p&gt; 我想知道是否有人用 Swin 变压器做过类似的实验，以及在线性评估中使用它们是否会失败。 感谢任何帮助。提前致谢。   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5atzv/d_do_swin_transformers_perform_poorly_at_linear/</guid>
      <pubDate>Tue, 16 Apr 2024 08:17:09 GMT</pubDate>
    </item>
    <item>
      <title>斯坦福大学发布了相当全面（500 页）的“2004 年人工智能指数报告”，总结了当今人工智能的状况。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c59zrq/stanford_releases_their_rather_comprehensive_500/</link>
      <description><![CDATA[ 由   提交/u/Appropriate_Ant_4629   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c59zrq/stanford_releases_their_rather_comprehensive_500/</guid>
      <pubDate>Tue, 16 Apr 2024 07:19:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通用时间序列预测变压器的统一训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c59hei/r_unified_training_of_universal_time_series/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.02592 代码：https://github .com/SalesforceAIResearch/uni2ts 模型：https://huggingface.co/collections/Salesforce/moirai-10-r-models-65c8d3a94c51428c300e0742 数据集：https://huggingface.co/datasets/Salesforce/lotsa_data 博客文章：&lt; a href=&quot;https://blog.salesforceairesearch.com/moirai/&quot;&gt;https://blog.salesforceairesearch.com/moirai/ 摘要：   时间序列预测的深度学习传统上在每个数据集一个模型的框架内运行，限制了其利用大型预训练模型改变游戏规则的影响的潜力。通用预测的概念源于对大量时间序列数据集的预训练，设想了一个能够解决不同下游预测任务的单一大型时间序列模型。然而，构建这样的模型对时间序列数据提出了独特的挑战：i）跨频率学习，ii）为多元时间序列容纳任意数量的变量，以及iii）解决大规模数据固有的不同分布特性。为了应对这些挑战，我们对传统时间序列 Transformer 架构进行了新颖的增强，从而提出了基于掩码编码器的通用时间序列预测 Transformer (Moirai)。在我们新推出的大规模开放时间序列存档 (LOTSA) 上进行训练，Moirai 具有跨九个域超过 27B 的观测数据，与全样本模型相比，Moirai 作为零样本预测器实现了具有竞争力或卓越的性能。代码、模型权重和数据将被发布。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c59hei/r_unified_training_of_universal_time_series/</guid>
      <pubDate>Tue, 16 Apr 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] Megalodon：高效的 LLM 预训练和无限上下文长度的推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c57nxd/r_megalodon_efficient_llm_pretraining_and/</link>
      <description><![CDATA[ 由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c57nxd/r_megalodon_efficient_llm_pretraining_and/</guid>
      <pubDate>Tue, 16 Apr 2024 04:51:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>