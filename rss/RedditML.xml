<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 26 May 2024 12:24:13 GMT</lastBuildDate>
    <item>
      <title>[R] 为什么上下文学习 Transformer 是表格数据分类器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/</link>
      <description><![CDATA[    &lt; /a&gt;  我们正在引入 TabForestPFN，它是一个上下文学习转换器，可以预测表格数据分类任务。过去，表格数据分类主要由 XGBoost 和 CatBoost 等基于树的算法主导，但现在我们终于使用预训练的转换器缩小了这一差距。 https://preview.redd.it/c3unlgi1cq2d1.png?width=2690&amp;format=png&amp;auto=网页&amp; s=cd414509a31a189df288668e928d52e5723df3fc Hollman 等人将上下文学习转换器引入表格数据分类。 （ICLR，2023）在 TabPFN 中。这项工作受到 GPU 内存的限制，因此仅考虑少于 1000 个观测值的数据集。我们通过添加微调阶段来改进他们的模型，从而规避 GPU 内存限制。此外，我们还引入了一个额外的合成数据森林生成器来进一步提高性能。结果是 TabForestPFN。 TabForestPFN 的焦点 论文是关于为什么我们可以对表格数据进行预训练的。在语言和视觉方面，预训练可以学习语法和纹理，因此预训练是有意义的。但在表格数据中，预训练中的数据集与现实世界中感兴趣的数据集不共享任何特征或标签，那么它还能学到什么呢？在本文中，我们认为上下文学习变压器学习创建复杂决策边界的能力。如果您对推理感兴趣，请阅读一下。 代码可在 https://github.com/ 获取FelixdenBreejen/TabForestPFN 通过代码，您可以重现我们所有的预训练、实验和分析，并且还包含一些基本示例，供您立即在自己的数据集上使用分类器。 &lt; p&gt;下面是 TabForestPFN 在两个表格数据分类基准上的结果。我是作者，所以如果有任何问题，请随时提问。 https://preview.redd.it/1tavhybhzq2d1.png?width=1174&amp;format=png&amp;auto=webp&amp;s=214fb394a229544dfd8b446 77d7880852c5d222f  https://preview.redd.it/9tg9z9tobq2d1.png？ width=832&amp;format=png&amp;auto=webp&amp;s=9fd3c732be8d5e18c8f56bb2b0e1c94796968056   由   提交 /u/FelixdenBreejen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/</guid>
      <pubDate>Sun, 26 May 2024 08:09:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML论文动词时态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/</link>
      <description><![CDATA[为什么大多数 ML 论文都使用现在时的所有动词时态（例如 MLA 格式），而使用引文样式或参考部分作为 APA 样式？ 特别是，尽管ICML等学术团体明确表示遵循APA风格，但大多数论文的动词时态似乎并没有遵循APA指南中的指示来书写过去时、现在时和将来时。未来适当。   由   提交 /u/cosmoquester   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/</guid>
      <pubDate>Sun, 26 May 2024 07:23:28 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]金融领域多租户系统的预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0vzvn/discussion_prediction_models_for_multitenant/</link>
      <description><![CDATA[假设一个拥有 1 亿客户的金融 SaaS 平台想要引入销售预测服务。 如何设计一个系统...  根据每个租户的历史数据预测销售预测 是否需要训练和构建 1 亿个模型（每个租户的模型） &lt; /ol&gt; 这里的任何指导表示赞赏。此外，任何有关此类设计案例研究的博客/参考材料都会有所帮助。 谢谢   由   提交 /u/Puzzleheaded-Rest734    reddit.com/r/MachineLearning/comments/1d0vzvn/discussion_prediction_models_for_multitenant/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0vzvn/discussion_prediction_models_for_multitenant/</guid>
      <pubDate>Sun, 26 May 2024 07:20:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 针对 ViT（视觉变换器），补丁嵌入中是否有可学习的参数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0vwrh/d_specific_to_vitvisual_transformers_are_there/</link>
      <description><![CDATA[我试图了解 ViT 中可学习参数的具体位置。 第一步是将补丁转换为补丁嵌入馈入n/w，因此我们添加一个简单的线性变换（FCN）来减少维度和矩阵-&gt;向量。这里学到了什么？有重量吗？或者只是将 2d 补丁输入缩小为 1d 向量。 由于这些补丁是并行处理（线性变换）的，因此它们不知道其他补丁信息。人们说补丁到补丁的交互发生在注意层中，但注意层中没有可学习的参数，它只是转置和多个查询关键补丁。 注意层损失中的反向传播是否会导致补丁嵌入层中的权重发生变化?? 另外，为什么他们称补丁嵌入为线性变换？他们不是添加了任何激活函数吗？它应该是非线性变换吧？   由   提交/u/elongatedpepe  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0vwrh/d_specific_to_vitvisual_transformers_are_there/</guid>
      <pubDate>Sun, 26 May 2024 07:13:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 在大型语言模型和人类中测试心理理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0vhzj/r_testing_theory_of_mind_in_large_language_models/</link>
      <description><![CDATA[   /u/AhmedMostafa16   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0vhzj/r_testing_theory_of_mind_in_large_language_models/</guid>
      <pubDate>Sun, 26 May 2024 06:44:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] [CVPR 2024] AV-RIR：视听室脉冲响应估计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0thhp/r_cvpr_2024_avrir_audiovisual_room_impulse/</link>
      <description><![CDATA[       由   提交/u/Snoo63916   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0thhp/r_cvpr_2024_avrir_audiovisual_room_impulse/</guid>
      <pubDate>Sun, 26 May 2024 04:26:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对我来说，构建一个强大且类似人类的可玩扑克 AI 模型的最佳方法是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/</link>
      <description><![CDATA[我正在开发一款（德州扑克）扑克游戏，我希望有一个可以像人类一样玩的 AI等级。我开发了一个获胜概率计算器，根据您的牌、公共牌和游戏中的玩家数量，它可以计算出您在游戏中拥有最好牌的几率。 我不确定从这里到哪里去。我在学校学习机器学习/人工智能，但我一直很难就如何在实践中实际应用这些工具做出最佳决定。首先，我不确定要使用什么数据集，我找到了一个 数据集 可能有用的在线扑克游戏日志。  此外，我不知道是否开发决策树、使用神经网络，或者两者和/或其他方法的组合。 最好的方法是什么关于使用 ML 为该项目构建 AI 模型？   由   提交 /u/HandfulOfAStupidKid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/</guid>
      <pubDate>Sat, 25 May 2024 19:50:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用业务指标展示 ML 模型结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0g3vj/d_showcase_ml_model_results_using_business_metrics/</link>
      <description><![CDATA[   /u/BeneficialAd3800  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0g3vj/d_showcase_ml_model_results_using_business_metrics/</guid>
      <pubDate>Sat, 25 May 2024 16:46:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 内存调整与微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0bz82/d_memory_tuning_vs_fine_tuning/</link>
      <description><![CDATA[我看到一篇 LinkedIn 帖子，其中这家初创公司声称可以使用“内存调整”来减少 LLM 幻觉。他们声称内存调整明显优于微调。  想知道是否有人遇到过“内存调整”这个术语，以及您认为这是营销言论还是真正的发展。  这是供参考的帖子：https://www.linkedin.com/posts/zhousharon_hallucinations-are-one-of-the-biggest-blockers-activity-7198340103600054273-ruNL?utm_source=share&amp;utm_medium=member_ios&lt; /a&gt;   由   提交/u/hamsterhooey  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0bz82/d_memory_tuning_vs_fine_tuning/</guid>
      <pubDate>Sat, 25 May 2024 13:24:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] 数据集分解：通过可变序列长度课程加快 LLM 培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d095cn/r_dataset_decomposition_faster_llm_training_with/</link>
      <description><![CDATA[      TL;DR：在上下文窗口中不要填充多个文档训练 LM。 论文： https://arxiv.org/abs/2405.13226&lt; /a&gt; 摘要：大型语言模型 (LLM) 通常在由固定长度标记序列组成的数据集上进行训练。这些数据集是通过随机连接不同长度的文档然后将它们分成预定目标长度的序列来创建的。然而，这种连接方法可能会导致序列内的跨文档注意力，这既不是理想的学习信号，也不是计算效率高的。此外，由于注意力的二次成本，长序列的训练在计算上变得令人望而却步。在本研究中，我们引入了数据集分解（一种新颖的可变序列长度训练技术）来应对这些挑战。我们将数据集分解为桶的并集，每个桶都包含从唯一文档中提取的相同大小的序列。在训练过程中，我们使用可变的序列长度和批量大小，同时从课程的所有存储桶中进行采样。与在训练的每一步都会产生固定注意力成本的 concat-and-chunk 基线相比，我们提出的方法会在每一步产生与实际文档长度成比例的惩罚，从而显着节省训练时间。我们以与使用基线方法训练的 2k 上下文长度模型相同的成本训练 8k 上下文长度 1B 模型。在网络规模的语料库上进行的实验表明，我们的方法显着提高了标准语言评估和长上下文基准的性能，与基线相比，达到目标准确度的速度提高了 3 倍。我们的方法不仅能够对长序列进行有效的预训练，而且还可以根据数据集大小进行有效扩展。最后，我们阐明了训练大型语言模型的一个关键但研究较少的方面：序列长度的分布和课程，这会导致性能上不可忽视的差异。 视觉摘要：  https:// /preview.redd.it/nnvi519tvj2d1.png?width=1123&amp;format=png&amp;auto=webp&amp;s=334b8990f4ac2d4298e1a622d71301cd7d6beae3   由   提交 /u/StartledWatermelon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d095cn/r_dataset_decomposition_faster_llm_training_with/</guid>
      <pubDate>Sat, 25 May 2024 10:34:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] LiteVAE：用于潜在扩散模型的轻量级高效变分自动编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d092o9/r_litevae_lightweight_and_efficient_variational/</link>
      <description><![CDATA[https://huggingface.co/papers/2405.14477    由   提交  /u/ghoof   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d092o9/r_litevae_lightweight_and_efficient_variational/</guid>
      <pubDate>Sat, 25 May 2024 10:29:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] YOLOv10：实时端到端目标检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2405.14458 摘要：在过去的几年里，YOLO 已经成为主流由于其计算成本和检测性能之间的有效平衡，成为实时目标检测领域的范例。研究人员对 YOLO 的架构设计、优化目标、数据增强策略等进行了探索，取得了显着进展。然而，后处理对非极大值抑制（NMS）的依赖阻碍了 YOLO 的端到端部署，并对推理延迟产生不利影响。此外，YOLO中各个组件的设计缺乏全面彻底的检查，导致明显的计算冗余并限制了模型的能力。它提供了次优的效率，以及相当大的性能改进潜力。在这项工作中，我们的目标是从后处理和模型架构上进一步提升 YOLO 的性能效率边界。为此，我们首先提出了 YOLO 的无 NMS 训练的一致双重任务，它同时带来了有竞争力的性能和低推理延迟。此外，我们还介绍了 YOLO 的整体效率-准确性驱动模型设计策略。我们从效率和准确性角度全面优化YOLO的各个组件，大大降低了计算开销并增强了能力。我们努力的成果是用于实时端到端目标检测的新一代 YOLO 系列，称为 YOLOv10。大量实验表明，YOLOv10 在各种模型规模上都实现了最先进的性能和效率。例如，我们的 YOLOv10-S 在 COCO 上的类似 AP 下比 RT-DETR-R18 快 1.8 倍，同时参数数量和 FLOP 减少 2.8 倍。与 YOLOv9-C 相比，在相同性能下，YOLOv10-B 的延迟减少了 46%，参数减少了 25%。 视觉总结：  方法 基准测试 代码： https://github.com/THU-MIG/yolov10    由   提交 /u/StartledWatermelon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/</guid>
      <pubDate>Sat, 25 May 2024 09:48:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习编译器的学习路径</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d08hl9/d_learning_path_to_ml_compilers/</link>
      <description><![CDATA[大家好， 我最近对机器学习编译器感兴趣，想问一些问题。 1. ML 编译器与语言编译器 我很好奇 ML 编译器和语言编译器是否有很多共同点。我只对编译器的前端部分了解一点，但看起来机器学习编译器并没有语言编译器通常经历的词法/语法/语义分析阶段。 后端部分会吗？与 ML 编译器更相关吗？您是否建议在进入 ML 编译器之前先学习 IR 优化、寄存器/内存分配或 SSA 等主题？感觉机器学习编译器本身就是一个野兽，所以我不确定是否应该深入研究它，或者拥有传统编译器后端的背景仍然会非常有帮助。 至少MLIR论文似乎讨论了SSA和IR，所以也许有一些编译器后端背景是必要的？澄清一下，我知道 SSA 和 IR 的定义，但在这里我要讨论的是深入讨论这些主题。 2. ML 编译器中的重点领域 我也很好奇 ML 编译器中需要最多工作量的领域是什么？是 IR（或图形）优化吗？或者是其他东西？你是否认为这个领域会持续几十年，或者主要是几年的努力，然后用户就不必再关心内部结构了（就像普通用户并不真正关心 gcc 或 clang 是如何编译的一样）现在的 C 代码太多了）   由   提交/u/SPark9625  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d08hl9/d_learning_path_to_ml_compilers/</guid>
      <pubDate>Sat, 25 May 2024 09:47:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] Google AI 概述是否应该发布？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1czzt45/d_should_google_ai_overview_haven_been_released/</link>
      <description><![CDATA[谷歌又发布了一个糟糕的 AI 功能（参见 5 月 24 日《纽约时报》文章中的反应）。当您阅读一些概述有多糟糕时，您会质疑谷歌产品团队是否真的考虑过人们将如何使用他们的产品。几乎看起来对抗性测试都没有完成。 如果 AI 概述真的旨在使用 AI 总结搜索结果，那么当相当大比例的网站充斥着不可靠的信息（包括阴谋论和讽刺）时，它应该如何工作。 在搜索时，有人真的需要洋葱文章的摘要吗？ “快速行动，打破常规，即使你正在打破的产品每年能带来 400 亿美元的收入”    提交人    /u/yintrepid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1czzt45/d_should_google_ai_overview_haven_been_released/</guid>
      <pubDate>Sat, 25 May 2024 00:35:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>