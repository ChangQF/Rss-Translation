<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 04 Dec 2024 15:19:15 GMT</lastBuildDate>
    <item>
      <title>[D] 如何在GNN中定制注意力机制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</link>
      <description><![CDATA[我正在寻找一些基础代码或算法，以便在处理节点预测任务的图表时创建新的注意机制。我看到恒星图中有一些文档，但我想知道是否还有其他有用的材料。谢谢！！！    提交人    /u/Whole_Hat_4852   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</guid>
      <pubDate>Wed, 04 Dec 2024 14:56:15 GMT</pubDate>
    </item>
    <item>
      <title>如何最好地微调 CNN？[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6hs4q/how_to_best_finetune_a_cnn_p/</link>
      <description><![CDATA[      我目前正在 3D 二进制数组上训练 CNN 以进行回归（4 个转换层，然后 3 个线性层和 1 个输出）。我发现 CNN 的整体性能不错，但并不惊人，因为预测的较低目标值的性能要差得多。我想尝试在具有较低值的样本上对模型进行微调，以提高其性能，但这样做时，模型的整体性能会下降。我冻结了 CNN 的前几层，只在最后几层以较低的学习率重新训练。是否有最佳的微调方法，或者我应该以不同的方式提高较低值的性能。 数据增强在我的情况下不起作用，因为二进制数组是方向相关的，因此目标值会不准确。 我附上了微调前性能奇偶校验图的图片。 https://preview.redd.it/bcodzfh4hu4e1.png?width=476&amp;format=png&amp;auto=webp&amp;s=3d3ae8116a92dfeab88c73b82068308cca​​3938f4    提交人    /u/Tupaki14   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6hs4q/how_to_best_finetune_a_cnn_p/</guid>
      <pubDate>Wed, 04 Dec 2024 14:49:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] BERT 的最佳替代品 - NLU 编码器模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/</link>
      <description><![CDATA[我正在寻找 BERT 或 distilBERT 的替代方案以实现多语言建议。 我想要一个类似于 BERT 的双向掩码编码器架构，但功能更强大，并且具有更多用于自然语言理解任务的上下文。 任何建议都将不胜感激。    提交人    /u/mr_house7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/</guid>
      <pubDate>Wed, 04 Dec 2024 14:07:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 预测并缓解恶意人工智能应用程序的安全威胁</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/</link>
      <description><![CDATA[本文对人工智能系统在数字、物理和政治安全领域的潜在恶意应用进行了系统分析。该方法包括：  调查可能引发攻击的双重用途 AI 能力 绘制特定的攻击载体和所需的技术能力 分析攻击者/防御者动态的演变 开发威胁评估和缓解框架  关键技术发现：  NLP 和计算机视觉等领域的 ML 进步降低了复杂攻击的门槛 自动化系统可以显著扩大传统攻击载体的规模 迁移学习和 GAN 能够快速适应攻击技术 单靠技术对策是不够的 - 需要政策/治理框架  研究人员提供了一个详细的评估框架，检查：  不同攻击类型的技术要求 能力开发的预计时间表 执行难度和潜在影响 提出的防御措施及其局限性  我认为这项工作对于帮助 ML 社区在安全风险成为现实之前预防它们非常重要。该框架提供了一种结构化的方式来评估新出现的威胁，尽管我预计随着能力的提高，具体的攻击媒介将发生重大变化。 我认为我们需要更多的研究来衡量拟议对策的有效性并了解进攻/防御能力的共同演变。政策建议是一个良好的开端，但需要不断完善。 TLDR：系统分析 ML 的进步如何在安全领域启用新的攻击媒介。提供通过技术和政策措施评估和减轻威胁的框架。 完整摘要在这里。论文此处。    由    /u/Successful-Western27 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/</guid>
      <pubDate>Wed, 04 Dec 2024 12:55:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大量元数据真的有助于语义搜索吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/</link>
      <description><![CDATA[这是我学习 AI 的第二周，我正在考虑预处理传记数据，包括大量元数据，如城市、出生日期、关键事件、教育、爱好等，然后生成嵌入并将它们一起添加到矢量数据库中。也许可以使用 NLP API 或 LLM。但有必要吗？或者我应该只使用 OpenAI 模型在存储它们之前从 bios 中动态提取这些元数据？拥有大量元数据是否会极大地帮助提高搜索结果的质量？ 我想也许半自动预处理步骤可以让我检查和清理元数据。 P/S：我将此发布在 https://www.reddit.com/r/learnmachinelearning 但没有得到太多回应。想在这里尝试一下。    提交人    /u/tjthomas101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/</guid>
      <pubDate>Wed, 04 Dec 2024 12:43:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一次性比较多个大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/</link>
      <description><![CDATA[我写了一篇关于简化比较和选择大型语言模型 (LLM) 以完成各种任务的过程的文章： 一次性比较多个大型语言模型 希望这篇文章能帮助人们根据自己的用例选择最佳模型（这可能需要花费大量时间）。 我也期待讨论不同的技术和工具来自动化这个过程。 谢谢！    提交人    /u/grudev   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/</guid>
      <pubDate>Wed, 04 Dec 2024 12:30:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 线性回归，但具有二进制输出，可以实现更高精度的大范围预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/</link>
      <description><![CDATA[神经网络往往很难预测输出中介于非常大和非常小的数字之间的数据。我的应用程序要求 NN 预测 -1000 到 1000 ∈ Z 之间的值。我可以通过将输出放大 1000 来实现这一点，从而使模型能够预测 -1 和 1 之间的值，但 L1Loss（最坏情况为 L2Loss）下的 2e-2（预测）和 3e-2（目标）之间的损失可以忽略不计（在这种情况下为 1e-2，最坏情况下为 1e-4）。模型必须非常精确地进行预测，当目标是 5e-2 时，它应该是这样的，甚至不能偏离 +-0.1e-2。当涉及到线性回归时，这种精度很难实现，所以我想到了一种更系统的方法来定义预测和标准。再次，我希望模型预测 -1000 到 1000 之间的数字。这些数字至少可以使用 11 位（二进制）表示，因此我重新设计了模型输出以包含 22 个神经元，排列为 ∈ R（11x2）11 个输出，具有两个类别，类别是 1 或 0 的二进制表示。 CrossEntropy 可以在这里用作标准，但出于特定原因，我改用 multimarginloss。否则，另一种方法可能是使用 11 个神经元的 S 形输出来表示二进制数。你们对此有何看法？这被认为是好的（如果不是更好的）做法吗？有没有类似的研究可以让我参考？    提交人    /u/Relevant-Twist520   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/</guid>
      <pubDate>Wed, 04 Dec 2024 07:55:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多模态宇宙：利用 100TB 天文科学数据实现大规模机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/</link>
      <description><![CDATA[https://openreview.net/forum?id=EWm9zR5Qy1#discussion 摘要：我们提出了多模态宇宙，这是一个大规模多模态科学天文数据集，专门为促进机器学习研究而编制。总体而言，我们的数据集包含数亿个天文观测数据，构成 100TB 的多通道和高光谱图像、光谱、多元时间序列，以及各种相关的科学测量和元数据。此外，我们还包括一系列代表天体物理学机器学习方法标准实践的基准任务。这个庞大的数据集将使开发专门针对科学应用的大型多模态模型成为可能。用于编译数据集的所有代码以及如何访问数据的说明均可在 https://github.com/MultimodalUniverse/MultimodalUniverse 中找到 你们认为该数据集有什么用途？    提交人    /u/blabboy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/</guid>
      <pubDate>Tue, 03 Dec 2024 20:19:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 云端 GPU 价格分析 - 2024 年 12 月：全面的市场回顾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/</link>
      <description><![CDATA[在分析了各大供应商当前的云 GPU 定价之后，我整理了一些可能有助于基础设施决策的见解。一些发现让我感到惊讶——尤其是关于隐藏成本和现货定价变化。 当前市场价格（2024 年 12 月） 按需定价： - RunPod H100 (80GB)：2.49 美元/小时 - RunPod A100 (80GB)：1.69-1.99 美元/小时 - Vast.ai A100：0.73-1.61 美元/小时（市场模式） - Lambda A100：1.29 美元/小时 关键市场洞察  现货实例定价  - 可将成本降低 30-70% - 可用性因地区而异 - 一些提供商提供现货实例保证 - 价格稳定性因提供商而异  隐藏的成本因素  - 数据传输费用差异巨大 - 大型数据集的存储成本 - 网络带宽层 - 实例启动/关闭最低限度  提供商差异化因素  - UI/UX 和易用性 - 可用区域/地区 - 支持质量 - API 功能 成本优化策略  工作负载规划  - 将 GPU 与实际要求相匹配 - 考虑将工作负载拆分到较小的实例中 - 使用 Spot 实例执行可中断任务 - 监控利用率模式  数据管理  - 优化数据集存储 - 规划数据传输模式 - 有效使用缓存 - 考虑压缩策略 我将每月跟踪这些价格和模式。感兴趣：  您使用哪些提供商？ 您如何优化成本？ 在您的 GPU 决策中，哪些指标最重要？     提交人    /u/Botinfoai   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/</guid>
      <pubDate>Tue, 03 Dec 2024 14:54:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过双向正向-逆向思维增强法学硕士推理能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/</link>
      <description><![CDATA[这里的关键贡献是一种“逆向思维”方法，它可以在不进行任何模型修改的情况下改进 LLM 推理。该方法不仅仅是从问题正向推理到答案，还增加了一个反向验证步骤 - 从潜在答案反向推理到问题以验证推理链。 关键技术要点：* 两阶段过程：正向生成，然后是反向验证* 反向传递检查答案和前提之间的逻辑一致性* 无需微调或架构更改* 经过多个推理基准测试（GSM8K、CommonsenseQA、LogiQA） 结果：* GSM8K 数学推理提高 8.3%* CommonsenseQA 提高 6.2%* LogiQA 提高 5.4%* 不同模型大小的持续改进* 性能提升是以 2 倍推理时间为代价的 我认为这种方法指出了我们如何提示 LLM 进行推理任务的尚未开发的潜力。虽然加倍的推理时间是一个真正的权衡，但不同基准的持续改进表明这种方法抓住了机器推理的一些基本知识。实现的简单性意味着它可以在许多推理准确性比速度更重要的应用程序中快速采用。 TLDR：添加后向推理验证步骤可将 LLM 在数学、逻辑和常识任务上的性能提高 5-8%，而无需更改模型。推理时间加倍，但在不同的模型和任务中提供一致的收益。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/</guid>
      <pubDate>Tue, 03 Dec 2024 13:57:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 模型在测试中表现良好，但在生产中失败</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/</link>
      <description><![CDATA[嗨，我已经使用 XGBoost 根据用户每周活动数据开发了流失预测模型。训练数据是平衡的（3.3k 流失，3k 未流失）。我将数据分为：训练、验证和测试集。训练、验证和测试集的准确率约为 90%，召回率约为 88%。然而，在生产中运行时，我发现约 1.5k 个用户被标记为流失（我们总共有 4k 个用户）。这不可能是真的，因为我们每月最多有 250 个流失用户。有什么建议可以告诉我我做错了什么吗？有什么解决办法吗？ 谢谢     提交人    /u/Terrible_Dimension66   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/</guid>
      <pubDate>Tue, 03 Dec 2024 13:30:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 时间序列中的深度学习：它们在工业中有应用吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/</link>
      <description><![CDATA[大家好！我是一名时间序列研究人员，看到这个领域关于深度学习模型的讨论很多。我想知道这些模型是否真的在生产中部署，或者传统方法仍然是业内的首选？ 例如，在天气预报中，基于物理的数值天气预报 (NWP) 似乎占主导地位。如果深度模型没有得到太多关注，您是否遇到过它们的任何实际用例？很想听听您的想法！    提交人    /u/Few-Pomegranate4369   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/</guid>
      <pubDate>Tue, 03 Dec 2024 08:36:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 流行的 VAE 理论解释不一致。请改变我的想法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/</link>
      <description><![CDATA[多年来，我一直很难从理论上理解 VAE/变分推理 (VI)。如果有人能澄清我的困惑，我将不胜感激。这是我阅读许多资料后得到的：  我们想要为可观察变量 x 和潜在变量 z 建立一个生成模型 p(x, z)（为简单起见省略了参数）。好，让我们选取适当的参数，使观测样本 p(x) 的边际似然最大化吧。 根据基本概率论（全概率定律和条件概率的定义），我们有： p(x)=∫ p(x ∣ z) p(z) dz （公式 1）。 事情变得比较混乱的地方就在这里：人们会声称这个积分难以解决，因为 z 是连续变量 / z 是高维变量 / p(x∣z) 太复杂 / 或者其他任何借口。 公式 1 难以解决怎么办？虽然上面我们没有提到后验 p(z ∣ x)，但现在我们将把它带入讨论中。后验 p(z ∣ x) 也是难解的，因为 p(z | x) = p(x | z) p(z) / p(x) 且 p(x) 是难解的。因此，我们将引入另一个参数化模型 q(z ∣ x) 来近似 p(z | x)。 经过一些推导，我们得到了一个新的优化目标，通常称为 ELBO，它是以下内容的总和： “重建”项：∫ log p(x ∣ z) q(z ∣ x) dz（等式 2）； q(z | x) 和 p(z) 之间的 KL 散度项，从而得到一个闭式。  所以现在我们必须研究等式 2。与等式 2 相比， 1，p(z) 被 q(z∣x) 取代，它们 (通常) 都是正态分布，而 p(x | z) 仍然存在。太好了！显然，我们把一个难积分变成了……另一个难积分？ 别担心，我们可以使用蒙特卡洛抽样来计算公式 2……等等，既然我们可以使用蒙特卡洛来做到这一点，为什么我们不能以同样的方式处理公式 1 而不用那么麻烦呢？ 当然这不是一个好主意。可以证明 log p(x) = ELBO + D_KL(q(z ∣ x) || p(z ∣ x))。所以我们不能用公式估计 p(x)。 1，因为它没有这么好的性质……嗯，我们似乎不是这样开始解释的？  问题：  在解决原始问题时，即通过最大化 p(x)=∫ p(x ∣ z) p(z) dz 对 p(x, z) 进行建模，为什么我们要涉及后验 p(z | x)？ 有人用&quot;缩小值空间以促进更快的搜索&quot;（使用 p(z | x)、q(z | x) 的近似值）来解释这一点。但同样，请回想一下 Eq. 1 得到解释后，我看不出这个论点有什么改进。  等式 1 和等式 2 本质上相似，其中它们都是 (log) p(z | x) 关于某个正态分布的概率密度函数的期望。我看不出基于等式 1 的难解性的动机如何有意义。 讽刺的是，在处理等式 2 时我们仍然必须求助于蒙特卡洛抽样。但是人们在谈论等式 1 的难解性时似乎忘记了它，但在面对与等式 2 相同的问题时却记住了它。   更新：我修改了一些错字。 更新 2：经过一些讨论，问题 2 似乎得到了解决： - 由于方差较大，在 p(z) 上采样不是一个好主意。 - 在实践中，我们通常研究 log p(x)，样本的对数似然，以及 log ∫ p(x ∣ z) p(z) dz 的 MC 采样（等式 3）可能会有偏差。 - 将 Jensen 不等式应用于等式 3，我们将得到 log p(x) ≥ ∫ log p(x ∣ z) p(z) dz。 这个界限很可能比 ELBO 更差，并且仍然依赖于对 p(z) 进行采样。 然而，这些观点在现有文章中仍然很少见。 我希望我们在将来介绍 VAE 时可以更加仔细地思考。    提交人    /u/function2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/</guid>
      <pubDate>Tue, 03 Dec 2024 04:25:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 01 Dec 2024 16:00:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>