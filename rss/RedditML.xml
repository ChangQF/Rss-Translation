<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 20 Feb 2024 09:13:04 GMT</lastBuildDate>
    <item>
      <title>[D] 使用带有 kmeans 的句子嵌入模型逐渐增加 CPU 负载</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1avch42/d_gradually_increasing_cpu_load_on_using_sentence/</link>
      <description><![CDATA[我有一个基于 ML 的生产应用程序，使用 Flask，使用 Gunicorn Workers 部署在 GCP 服务器上。在每个传入请求中，都会收到一个文本句子。 它使用句子转换器（All-MiniLM-L6-v2 模型），该模型会全局加载一次，以创建嵌入传入文本，然后使用预先训练的 kmeans（也全局加载）来预测/将其映射到意图集群。基本上，目标是找到句子的意图。 我有足够的资源，请求的数量也恒定，文本也相似，但每天CPU负载都在逐渐增加。第一天的平均响应时间约为 200 毫秒，10 天后现在为 400 毫秒。 我尝试在代码本身中使用“del”命令删除嵌入变量，同时还强制 python 垃圾收集器在主进程执行完成后执行的线程中使用“gc.collect()”，但问题仍然出现。 我注意到的一件事是，如果我不使用 del 和 gc。收集（）后，RAM开始逐渐下降。对于这两种情况，RAM 是恒定的，但现在 CPU 使用率每天都在逐渐增加，因此负载和响应时间也随之增加。 我花了数周的时间在这个问题上尝试调试它，但没有找到解决方案，如有任何帮助，我们将不胜感激。   由   提交/u/Devinco001  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1avch42/d_gradually_increasing_cpu_load_on_using_sentence/</guid>
      <pubDate>Tue, 20 Feb 2024 08:07:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] ImageDataGenerator 验证图像和掩模不匹配</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1avb4b6/d_imagedatagenerator_mismatching_of_validation/</link>
      <description><![CDATA[     &lt; td&gt; 我正在使用二进制图像分割，并且有图像及其蒙版。我使用 TensorFlow ImageDataGenerator 加载并使用了增强方法。但是当我可视化验证图像和掩模时，其中一些图像彼此不匹配。问题可能是什么？如何解决这个问题。代码如下： val_datagen = ImageDataGenerator(rescale=1/255., #rotation_range = 20,&lt; /p&gt; # Zoom_range = 0.2, fill_mode = &#39;reflect&#39;, # width_shift_range = 0.2， # height_shift_range = 0.2， horizo​​ntal_flip = True， vertical_flip = True， validation_split=0.2) ​ &amp;# x200b; val_img = val_datagen.flow_from_directory( image_dir, target_size= (128, 128), color_mode=&#39;rgb&#39;, batch_size=32, shuffle=True， class_mode=None， seed=42，强&gt; subset=&#39;验证&#39; ) val_mask = val_datagen. flow_from_directory( ma​​sk_dir, target_size=(128, 128), color_mode=&#39;灰度&#39;， batch_size=32， shuffle=True，  class_mode=None， seed=42， subset=&#39;validation &#39; ) val_data_generator = zip(val_img,val_mask) 并且这就是可视化： ​ https://preview.redd.it/hg9nb590sojc1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=43746b18c59344c8a43a5bd084 292989699bf3b3 &lt; !-- SC_ON --&gt;  由   提交/u/NailaBaghir   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1avb4b6/d_imagedatagenerator_mismatching_of_validation/</guid>
      <pubDate>Tue, 20 Feb 2024 06:40:40 GMT</pubDate>
    </item>
    <item>
      <title>生产输出抽样 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1avaqqt/production_output_sampling_d/</link>
      <description><![CDATA[我可以使用哪些方法来收集准确代表总体生产输出的生产输出样本，而不需要对整个生产数据集进行质量控制？ 域可以是文本或图像。 到目前为止，我已经阅读过有关随机、分层、系统的内容，其中分层似乎是最佳选择。但是，我找不到任何有关当前行业趋势的信息。   由   提交/u/Azrael-1810  /u/Azrael-1810 reddit.com/r/MachineLearning/comments/1avaqqt/product_output_sampling_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1avaqqt/production_output_sampling_d/</guid>
      <pubDate>Tue, 20 Feb 2024 06:18:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果使用双 GPU 进行训练，瓦数会低得多</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1av8psg/d_much_lower_wattage_if_dual_gpu_used_for_training/</link>
      <description><![CDATA[我有双 GPU 设置，4080 16GB 和 4080 16GB 3090 24GB。通过我在 Yelp 评论数据集上训练 Bert Large 的玩具示例，我在双 GPU 上的训练速度始终比仅在 4080 上慢约 4%。我预计这 2 个 GPU 能够很好地协同工作，因为它们的 CUDA 核心数量非常相似（4080 9,728 ,3090 10,496）。另一项观察结果是，3090 在双重训练中仅拉动 270w 左右，而仅在 3090 上训练时，该卡拉动 335w。我有一个1600W的电源。我正在使用 Huggingface 变压器，精心挑选的最大批量大小为 18，多个数据加载器工作程序固定到内存。与 10k 和 100k 训练数据大小保持一致，速度减慢了约 4%。 有任何提示吗？   由   提交/u/kecso2107  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1av8psg/d_much_lower_wattage_if_dual_gpu_used_for_training/</guid>
      <pubDate>Tue, 20 Feb 2024 04:24:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为本科论文选择模型/架构</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1av73zo/d_choosing_a_modelarchitecture_for_undergraduate/</link>
      <description><![CDATA[目前是一名本科生，我们选择的主题是区分脑部疾病与其模拟疾病，以避免使用 MRI 图像进行误诊。  如果我们只区分它的模仿者而不是所有模仿者，范围是否太小？我们担心，如果我们将其所有模仿者都包括进来，范围就会太大，对于本科论文来说不可行。  您认为这是一篇有趣的论文吗？我真的很想进行一项既有帮助又有趣的研究:) 最后，我们也在选择模型/架构方面遇到了困难。这是因为我们这学期才刚刚开始学习机器学习。我们是否只是选择一个用于图像分类的架构，然后对其进行改进？我们可以采用同一研究中的方法但改进它（但我们不知道如何改进它，哈哈）？目前，我们正在考虑半监督学习，因为大脑图像的数据集很少（特别是我们正在寻找的疾病）。暂定方法：我们将使用基于注意力的CNN（因为我们想解决误诊问题，所以我们认为使用注意力机制会很有帮助，这样模型就能知道重要的特征。对于架构，我们正在规划使用 DenseNet、Inception 或 ResNet）和 Grad-CAM（为了可解释性）。我们也没有包含分割来让模型选择分类所需的特征。这个方法足够并且可行8个月吗？ 我们也在寻找一位可以帮助我们的导师，即使只是咨询。谢谢！   由   提交/u/viagee2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1av73zo/d_choosing_a_modelarchitecture_for_undergraduate/</guid>
      <pubDate>Tue, 20 Feb 2024 03:02:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何训练 ViT 与 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1av70eo/d_how_to_train_vit_vs_cnn/</link>
      <description><![CDATA[大家好，我尝试了猫和狗二元分类数据集来练习（224px）。首先，我使用了一个简单的 CNN，它有大约 250 万个可训练参数，Ir= 1e-3 和 25 个时期。训练后测试准确率约为90%。此后，我尝试使用 ViT base 16（大约 8500 万个可训练参数）来使用相同的超参数进行训练，但在 25 个 epoch 后，它仅达到 70% 的测试精度。我该怎么做才能获得更好的结果？   由   提交/u/spacesubmarine97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1av70eo/d_how_to_train_vit_vs_cnn/</guid>
      <pubDate>Tue, 20 Feb 2024 02:58:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数学家会在未来的机器学习研究中占据上风吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1av5gwy/d_will_mathematicians_have_the_upper_hand_in/</link>
      <description><![CDATA[似乎在各个角落我都看到了关于做研究的类似情绪。人们尝试各种事物的组合来获得渐进式的改进。我认为下一步的飞跃需要大量的理论知识来指导方向。   由   提交 /u/planetofthemushrooms   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1av5gwy/d_will_mathematicians_have_the_upper_hand_in/</guid>
      <pubDate>Tue, 20 Feb 2024 01:46:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 表示正交性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1av2dyh/r_representation_orthogonality/</link>
      <description><![CDATA[在表示理论文献中，不同类的表示具有典型的正交表示并且更细粒度的类具有一些正交表示是一个常见/合理的假设吗？余弦相似度相当低？   由   提交/u/Classic_Youth_4957   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1av2dyh/r_representation_orthogonality/</guid>
      <pubDate>Mon, 19 Feb 2024 23:31:46 GMT</pubDate>
    </item>
    <item>
      <title>对于技术主管来说，您的实际工作是什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1auyt1n/for_anyone_whos_a_tech_lead_what_is_your_actual/</link>
      <description><![CDATA[在担任主要 DS 后，我最近被任命为一个大项目的技术主管，并发现自己在思考我的角色实际上是什么/应该是什么..你每天都做什么？与参加会议和计划里程碑等相比，您还有多少编码和技术工作？    由   提交 /u/natrules   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1auyt1n/for_anyone_whos_a_tech_lead_what_is_your_actual/</guid>
      <pubDate>Mon, 19 Feb 2024 21:11:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] Lipschitz 连续性和凸函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1auv1pp/p_lipschitz_continuity_and_convex_functions/</link>
      <description><![CDATA[我有这个： Sigma：R-&gt;R 是非递减 L-Lipschitz 函数，W € R kxd, 和 b €Rk 存在凸 L||W||22 平滑函数 F (w,b) 使得 Nablax F(w b)(x) =WT sigma(Wx +b)  并且我们有凸势层 z = x - (2*WT sigma(Wx + b) )/ ||W||_22  现在，谁能帮我严格证明 F_(w,b) L||W||_22 光滑吗？而且，z 可微吗？  如果不是解决方案，那么其他人可以推荐一些阅读材料和书籍吗？   由   提交 /u/theloneliestsoulever   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1auv1pp/p_lipschitz_continuity_and_convex_functions/</guid>
      <pubDate>Mon, 19 Feb 2024 18:47:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 在 10M 大海捞针中寻找针：循环记忆找到法学硕士错过的东西 - AIRI，莫斯科，俄罗斯 2024 - RMT 137M 具有循环记忆的微调 GPT-2 能够在 10M 中找到 85% 的隐藏针草垛！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1autvwq/r_in_search_of_needles_in_a_10m_haystack/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.10790  摘要：  本文解决了使用生成变压器模型处理长文档的挑战。为了评估不同的方法，我们引入了BABILong，一个新的基准旨在评估模型在广泛文本中提取和处理分布式事实的能力。我们的评估（包括 GPT-4 和 RAG 基准）表明，常见方法仅对最多 10^4 元素的序列有效。相比之下，通过循环内存增强对 GPT-2 进行微调使其能够处理涉及多达  10^7 个元素的任务。这一成就标志着一个重大飞跃，因为它是迄今为止任何开放神经网络模型处理的最长输入，展示了长序列处理能力的显着改进。   https://preview.redd.it /0o4207a70ljc1.jpg?width=577&amp;format=pjpg&amp;auto=webp&amp;s=2bfac07872020de222b4bf99f837aa398b778afc https://preview.redd.it/2ff82da70ljc1.jpg?width=1835&amp;format=pjpg&amp;auto=webp&amp;s=acc1409f5b9bcd07f9 b5ff8a3890cc1b15b5c8ed  https://preview.redd .it/ld69p7a70ljc1.jpg?width=1816&amp;format=pjpg&amp;auto=webp&amp;s=fdd72c1a87742f525fa352723bcd1a0f4f000638 https://preview.redd.it/7vn4gba70ljc1.jpg?width=900&amp;format=pjpg&amp;auto=webp&amp;s=c8d08bb85a6 699e5b451e01bf615379db1fcbdca   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1autvwq/r_in_search_of_needles_in_a_10m_haystack/</guid>
      <pubDate>Mon, 19 Feb 2024 18:02:36 GMT</pubDate>
    </item>
    <item>
      <title>MoE - 我对“专家”有点困惑 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aurrxi/moe_im_a_bit_confused_about_experts_d/</link>
      <description><![CDATA[我一直在阅读一些有关专家混合 (MoE) 模型的内容，以及它们如何惩罚模型以确保激活分布相等话虽如此，可以合理地说它不是“这是数学专家，这是科学专家”，而是一个优化的黑匣子在大量训练数据上训练的子模型以针对输入查询的不同维度？ 我更多地将其视为负载均衡器，并且可能是一种通过拆分来抵消大型模型可能产生的负面影响的方法增加工作量。 我们无法控制专家，对吧？  还是我大错特错了？   由   提交 /u/Kaldnite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aurrxi/moe_im_a_bit_confused_about_experts_d/</guid>
      <pubDate>Mon, 19 Feb 2024 16:41:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mamba 和状态空间模型的视觉指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aupbct/d_a_visual_guide_to_mamba_and_state_space_models/</link>
      <description><![CDATA[大家好！为了使状态空间模型（和 Mamba）更容易被更广泛的受众接受，我创建了底层技术的视觉指南。 https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state 通过 50 多个自定义可视化，我希望它为那些对用于语言建模的 Mamba 和状态空间模型感兴趣的人提供一个直观的起点。 我们的想法是专注于直觉，使这一潜在的新功能成为可能。对于该领域的新手来说，架构很容易理解。我确保尽可能将方程式保持在最低限度。 希望这将为那些完全陌生的人提供一个很好的介绍。 如果您有任何反馈和/或者更正，我洗耳恭听！    由   提交 /u/MaartenGr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aupbct/d_a_visual_guide_to_mamba_and_state_space_models/</guid>
      <pubDate>Mon, 19 Feb 2024 15:01:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI/ML 实习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1audi2u/d_aiml_internships/</link>
      <description><![CDATA[为什么现在 AI/ML 领域的实习这么难？  我目前拥有一些高级人工智能项目的一年经验。但不知何故，我无法找到任何实习机会。至于工作，我几乎找不到需要5年以下经验的工作。说实话，这令人沮丧。有人可以帮忙吗？   由   提交/u/Anonymous_Life17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1audi2u/d_aiml_internships/</guid>
      <pubDate>Mon, 19 Feb 2024 03:34:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>