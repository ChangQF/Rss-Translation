<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Tue, 15 Apr 2025 12:36:21 GMT</lastBuildDate>
    <item>
      <title>因此，您的LLM应用程序有效...但是可靠吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzqr94/so_your_llm_app_works_but_is_it_reliable_d/</link>
      <description><![CDATA[    其他人发现构建可靠的LLM应用程序涉及管理重大的复杂性和不可预测的行为？ 看来，对于这些系统，基本的正常时间和潜伏期检查足够的时代在很大程度上落后于我们。现在，重点必须包括跟踪响应质量，在影响用户之前检测幻觉，并有效地管理代币成本 - 生产LLM的关键运营问题。 对LLM可观察性的可观察性与Traceloop的CT The traceloop的CTO进行了富有成效的讨论。 Tracing (to understand the full request lifecycle), Metrics (to quantify performance, cost, and errors), Quality/Eval evaluation (critically assessing response validity and relevance), and Insights (how do we turn this info into action and how it changes our架构？） 自然而然地，这种需求导致了迅速发展的专业工具景观。我实际上创建了一个有用的比较图，试图映射此空间（涵盖了Traceloop，Langsmith，Langfuse，Arize，DataDog等选项）。这是相当密集的。 共享这些要点，因为视角可能对其他浏览LLMOPS空间很有用。 希望这种观点有帮助。    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/oba2311     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jzqr94/so_your_your_llm_llm_app_but_but_is_it_it_it_it_reliable_is_reliable_d/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzqr94/so_your_llm_app_works_but_is_it_reliable_d/</guid>
      <pubDate>Tue, 15 Apr 2025 12:34:50 GMT</pubDate>
    </item>
    <item>
      <title>新的可解释性方法说，[r]神经元的对齐不是基本的 - 它是relu＆tanh几何形状的副作用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzpkyj/r_neuron_alignment_isnt_fundamental_its_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  神经元对齐 - 单个神经元似乎“表示”现实世界的概念 - 可能是一种幻想。 一种新方法， spotlight共振方法（SRM）表明，神经元的比对不是一个深度学习原理。相反，它是Relu和Tanh等激活功能的几何伪影。这些功能断开旋转对称性和特权特定方向，导致激活重新排列以与这些基础向量保持一致。      tl; dr：       功能形式（relu，tanh）→各向异性对称性破坏→特权方向→神经元对齐 - ＆gt;可解释的神经元   这不是一个错误 - 它是可预测的，可控制的效果。现在我们可以使用它。  这对您意味着什么：   建立在坚实的数学基础上的新的概括性解释性度量。它的工作：     所有架构〜所有层〜所有层〜所有任务     揭示激活功能如何以可控的方式重新形成代表性的几何形状。 使用它已经揭示了几个基本的AI发现……   ML的令人兴奋的发现：    - 挑战基于神经元的可解释性 - 神经元的对齐方式是一种坐标，是一种坐标人伪像， 人类的选择，不是一个深度的学习原则。 神经元的选择性，稀疏性，线性解脱，可能是神经塌陷  。 Demonstrates these privileged bases are the true fundamental quantity. - This is empirically demonstrated through a direct causal link between representational alignment and activation functions! - Presents evidence of interpretable neurons (&#39;grandmother neurons&#39;) responding to spatially varying sky, vehicles and eyes — in 非跨局部MLP 。   它的工作原理：   srm从特权基础上旋转Bivector平面中的“聚光矢量”。使用此IT跟踪潜在层激活中的密度振荡 - 揭示了由建筑对称性破坏引起的激活聚类。它通过使用Lie代数分析整个激活向量来概括以前的方法，因此在所有体系结构上都起作用。  纸张涵盖了这种新的可解释性方法，并且已经使用它做出了基本的DL发现…       [ICLR 2025研讨会论文]       🛠️＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jzpkyj/r_neuron_alignment_isnt_isnt_isnt_fundamental_its_a/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzpkyj/r_neuron_alignment_isnt_fundamental_its_a/</guid>
      <pubDate>Tue, 15 Apr 2025 11:34:30 GMT</pubDate>
    </item>
    <item>
      <title>[D]你们仍在开发内部NLP模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzpb75/d_are_you_guys_still_developing_inhouse_nlp_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在这个llm时代，你们是否仍在从头开始构建NLP模型，还是只是从llm提示下进行微调？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/dyngts     [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jzpb75/1jzpb75/d_are_you_guys_guys_still_still_still_still_still_inhouse_inhouse_inhouse_nlp_models/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzpb75/d_are_you_guys_still_developing_inhouse_nlp_models/</guid>
      <pubDate>Tue, 15 Apr 2025 11:19:28 GMT</pubDate>
    </item>
    <item>
      <title>[d]如何用受限的资源训练该模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzp9d0/d_how_to_train_this_model_with_constrained/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，所以我已经制作了一个模型 paper 。他们基本上降低了计算注意力重量的复杂性。因此，我相应地修改了注意力机制。现在，问题在于，要比较性能，他们使用了64个Tesla V100 GPU，并使用了BookCorpus和英语Wiki数据，这些数据占3300m以上的单词。我无法访问那么多资源（Max是Kaggle）。我想证明我的模型可以显示出可比的性能，但计算复杂性较低。我不知道该如何进行。请帮助我。我的模型具有典型的变压器解码器体系结构，类似于GPT2-SMALL，12层，每层12个头。总计我的模型中有164m参数。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/maakabharosaa     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzp9d0/d_how_to_train_this_model_with_constrained/</guid>
      <pubDate>Tue, 15 Apr 2025 11:16:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]地址和名称匹配技术建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzobwq/d_adress_names_matching_technique_recommendations/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  上下文：我有一个公司拥有的产品数据集，例如：name：A a，地址：第五大道，产品：A。A. Company A Inc，地址：纽约，纽约，产品B. Company B. Company A Inc。 ，地址，纽约第五大道，产品C.  我有4亿类的条目。如您所见，地址和名称的格式不一致。我还有另一个数据集，这是我对公司的基础。它为公司提供了一个干净的名称，并分析了地址。  目的是将桌面的记录与不一致的格式与地面真相相匹配，以便每种产品都链接到一家干净的公司。  问题和帮助： - 我正在考虑使用Google GeoCoding API来解析地址并进行地理编码。然后使用地理编码在我的地址和地面真相之间执行距离搜索，但是我没有地面真相数据集中的地理编码。因此，我想找到另一种匹配解析地址的方法，而无需使用地理编码。    理想情况下，我希望能够输入我的解析地址和名称（也许还有其他一些功能，例如活动行业），并从0到1之间返回了地面真相数据集的最佳匹配候选人。您建议哪种方法适合大型数据集？    该方法应该能够处理案件，这是我的地址之一是：A公司，地址：华盛顿（例如，大约是一个城市，例如，有时甚至没有指定该国）。由于华盛顿含糊不清，我将收到该候选人的几个解析地址。在这种情况下，最好的做法是什么？由于Google API不会返回单个结果，我该怎么办？   我的地址来自世界各地，您知道Google API是否可以处理全世界吗？语言模型在解析某些地区会更好吗？    帮助您将不胜感激，谢谢。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bojack-cowboy     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzobwq/d_adress_names_matching_technique_recommendations/</guid>
      <pubDate>Tue, 15 Apr 2025 10:19:49 GMT</pubDate>
    </item>
    <item>
      <title>[D]学生研究人员的实验跟踪-WandB，Neptune或Comet ML？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzjy7f/d_experiment_tracking_for_student_researchers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我已经归结为这3个，但是您能帮助我确定哪个是学生研究人员的最佳选择RN？ 我过去曾经使用过wandb，但是我读到的是降低了一些速度，并且我读了一些大型转换器模型，所以我喜欢训练一个大型转换器模型。我还将使用多个GPU，以防有用的信息来确定哪种是最好的。 具体来说，最容易快速设置并开始使用，稳定（不会引起问题），并且对于跟踪指标，参数？              &lt;！ -  sc_-sc_on-&gt; 32;提交由＆＃32; /u/u/scaredHomework8397     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzjy7f/d_experiment_tracking_for_student_researchers/</guid>
      <pubDate>Tue, 15 Apr 2025 05:17:55 GMT</pubDate>
    </item>
    <item>
      <title>[d]建立一个100k+小时的高质量，道德采购的视频数据的市场，可从AI研究人员那里获得反馈</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzffw1/d_building_a_marketplace_for_100k_hours_of/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 我在专门为AI实验室设计的市场上工作： 100k+小时的道德上播放的，录音室确定的视频内容用于大规模培训。 我们正在构建核心搜索范围，因此您可以通过自然进行搜索，因此可以通过搜索范围。这个想法是要使大量的视频数据集实际上可用。 为研究人员和工程师在视频上培训的一些开放问题：  您更喜欢培训数据的格式？生的？压缩（MP4）？像4K，2K或Full HD这样的分辨率？其他东西？ 我们已经进行了细分视频并可以通过自然语言进行搜索。  您可以许可您： →只是与查询 →完整视频相匹配的段 →它来自         无论如何，您通常需要较大的块或完整的数据集？ 我们处于用户发现模式并试图验证核心假设。如果您在视频或视听数据上训练，我很想听听您的想法 - 无论是在评论还是通过DM。提交由＆＃32; /u/u/playfulmenu1395     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzffw1/d_building_a_marketplace_for_100k_hours_of/</guid>
      <pubDate>Tue, 15 Apr 2025 01:14:06 GMT</pubDate>
    </item>
    <item>
      <title>[r] AI科学家V2：通过代理树搜索的车间级自动化科学发现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jzdgyk/r_the_ai_scientistv2_workshoplevel_automated/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/milaworld       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jzdgyk/r_the_ai_scientistv2_workshoplevel_automated/</guid>
      <pubDate>Mon, 14 Apr 2025 23:39:19 GMT</pubDate>
    </item>
    <item>
      <title>[D]有关建造随机森林/Xgboost模型的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jz8boa/d_advice_on_building_random_forestxgboost_model/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我具有具有数百万记录和约700个变量的EMR数据。我需要创建一个随机的森林或XGBoost模型，以评估手术后30天内住院的风险。给定大量变量，我计划遵循此过程：  将数据分为培训，验证和测试集，并在培训集上执行以下步骤。 使用RF/XGBOOST的默认设置，用于rf/xgboost的默认设置，并根据功能进行超级&gt;        基于新的超参数的重新评估特征选择，并继续在功能选择和超参数调整之间迭代，评估验证集上的性能。  我的问题是：  我应该在def for the for the for the for the Andertion for for for for for for for for forf/promparem fortim forters inf inf/empormar helting temption the Endertions上，并启用rf/XGBOST的功能。调整，还是我应该首先调整模型？我担心有如此大的数据，调整可能是不可行的。 我的方法看起来不错？请建议我可能错过任何改进或步骤。  这是我第一次使用此大小的数据。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/chemical-library4425     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jz8boa/d_advice_on_building_random_forestxgboost_model/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jz8boa/d_advice_on_building_random_forestxgboost_model/</guid>
      <pubDate>Mon, 14 Apr 2025 19:56:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] LLM体系结构列表。我正在收集有关LLM Architectures的Arxiv论文 - 寻找我所缺少的。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jz80xq/d_p_list_of_llm_architectures_i_am_collecting/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿。 我正在寻找有关LLM架构（和类似）的任何主要Arxiv论文的建议和链接，我还没有。还要感谢任何帮助。 也，至于这是全部的，我有“设计”的爱好。新颖的小语言模型体系结构。我很好奇的是，是否有比我更访问计算的人可能有兴趣与我合作并与我一起做一个最终目标，即在创意共享属性下发布新颖的架构4.0 Internationallike 4.0 International（CC BY-SA 4.0）许可证？ ？  bi-mamba   bigbird   deepSeek r1   deepseek v3    hyena    hymba    jamba  jamba   linear transfors  linformanser  linformer 神经图灵机 表演者 重复的内存变压器  retnet   rwkv     s4    titans  titans   提交由＆＃32; /u/megneous     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jz80xq/d_p_p_p_list_of_llm_architectures_i_i_am_collecting/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jz80xq/d_p_list_of_llm_architectures_i_am_collecting/</guid>
      <pubDate>Mon, 14 Apr 2025 19:44:33 GMT</pubDate>
    </item>
    <item>
      <title>我如何扭曲您的噪音：扩散模型之前的时间相关噪声[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jz6shu/how_i_warped_your_noise_a_temporallycorrelated/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/bregav      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jz6shu/how_i_warped_your_your_noise_a_a_temporporallycorrelated/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jz6shu/how_i_warped_your_noise_a_temporallycorrelated/</guid>
      <pubDate>Mon, 14 Apr 2025 18:54:56 GMT</pubDate>
    </item>
    <item>
      <title>[d]堪萨斯州怎么了？ （Kolmogorov-Arnold网络）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jyz2vg/d_what_happened_to_kans_kolmogorovarnold_networks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   kans似乎很有希望，但我没有听到任何实际应用。好奇是否有人从事它  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ulight_architect   href =“ https://www.reddit.com/r/machinelearning/comments/1jyz2vg/d_what_what_happend_to_kans_kolmogorovarnold_networks/  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jyz2vg/d_what_happened_to_kolmogorovarnold_networks/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jyz2vg/d_what_happened_to_kans_kolmogorovarnold_networks/</guid>
      <pubDate>Mon, 14 Apr 2025 13:37:55 GMT</pubDate>
    </item>
    <item>
      <title>[D]蒸馏被低估了。我以14倍便宜的型号复制了GPT-4O的功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jyr6ah/d_distillation_is_underrated_i_replicated_gpt4os/</link>
      <description><![CDATA[    src =“ https://preview.itd.it/zyj7as7ogque1.png？被低估了。设法使用较小的，微调的型号来复制GPT-4O级的性能（精度为92％），并且价格便宜14倍。对于那些不熟悉的人来说，蒸馏基本上是：采用巨大，昂贵的型号，并使用它来训练更小，更便宜，更快的特定域。如果做得正确，小型模型也可以以一小部分成本执行几乎。老实说，超级有前途。好奇这里的其他人都玩过蒸馏。告诉我更多用例。  在注释中添加我的代码。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/bibious_anybody855      [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jyr6ah/d_distillation_is_underrated_i_replicated_gpt4os/</guid>
      <pubDate>Mon, 14 Apr 2025 05:12:18 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jpdo7y/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</guid>
      <pubDate>Wed, 02 Apr 2025 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_be_hired/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Mon, 31 Mar 2025 02:30:37 GMT</pubDate>
    </item>
    </channel>
</rss>