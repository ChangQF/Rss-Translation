<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 17 Jun 2024 01:06:01 GMT</lastBuildDate>
    <item>
      <title>[P] 从头开始​​的混合精度训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</link>
      <description><![CDATA[我在 2 层 MLP 上重新实现了 Nvidia 的原始混合精度训练论文（https://arxiv.org/abs/1710.03740）。我一直深入到 CUDA 领域来展示 TensorCore 激活，在我看来，这是混合精度训练的真正秘密。 代码：https://github.com/tspeterkim/mixed-precision-from-scratch 撰写：https://tspeterkim.github.io/posts/mixed-precision-from-scratch    提交人    /u/droidarmy95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</guid>
      <pubDate>Mon, 17 Jun 2024 00:28:51 GMT</pubDate>
    </item>
    <item>
      <title>[D],[P] 关于句子转换器库的疑问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9rxe/dp_doubt_regarding_sentence_transforrmers_library/</link>
      <description><![CDATA[      嗨，我的工作是为不同的 LLMS（如 BERT、Gemma、Llama 等）创建句子嵌入。我想知道我是否可以使用 Sentence Transformers 库来实现相同。然而，进一步深入研究后，我发现它只有几个针对相同内容的预训练模型。有人可以对此提供澄清吗？提前致谢。 https://preview.redd.it/6b8uq48day6d1.png?width=1431&amp;format=png&amp;auto=webp&amp;s=d4d5c0ab164d0ebdf97f2a61303a5fee7167968d    提交人    /u/nani_procastinator   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9rxe/dp_doubt_regarding_sentence_transforrmers_library/</guid>
      <pubDate>Sun, 16 Jun 2024 15:16:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] ECAI 2024 评论讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9n4e/d_ecai_2024_reviews_discussion/</link>
      <description><![CDATA[ECAI 2024 评论讨论帖。    由   提交  /u/Fun_Equal5145   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9n4e/d_ecai_2024_reviews_discussion/</guid>
      <pubDate>Sun, 16 Jun 2024 15:10:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你将如何解决 CV 中的动态分辨率问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh8gm5/d_how_you_will_solve_dynamic_resolution_problem/</link>
      <description><![CDATA[我目前正在研究一个需要在非常奇怪的分辨率下进行精确物体检测的问题（宽高比从 1:32 到 32:1 不等，每边最多 30000）。物体通常在两边大约 120 像素，这意味着在像 ViT 这样的模型中，训练期间物体的大小小于一个像素（224 像素的正方形），对于 YOLO 和朋友来说，情况也好不到哪里去。我有相当大的这些图像数据集，手动裁剪它们非常无效和愚蠢，因为没有人会在生产中裁剪它们。 我的背景主要是带有 transformer 的 NLP，我认为您可以使用带填充的动态补丁计数，但我在 hf vit 参数上找不到任何类似的东西。 有没有理智的方法来处理这种任务？对于可以更换头部以使用嵌入执行不同任务的解决方案，我深表赞赏。    提交人    /u/Dathvg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh8gm5/d_how_you_will_solve_dynamic_resolution_problem/</guid>
      <pubDate>Sun, 16 Jun 2024 14:13:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​实现指令微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh7cmv/p_instruction_finetuning_from_scratch/</link>
      <description><![CDATA[        提交人    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh7cmv/p_instruction_finetuning_from_scratch/</guid>
      <pubDate>Sun, 16 Jun 2024 13:14:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 理解 LoRA：低秩近似的视觉指南，用于有效微调 LLM。🧠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh4s3x/r_understanding_lora_a_visual_guide_to_lowrank/</link>
      <description><![CDATA[      TL;DR：LoRA 是参数高效微调 (PEFT) 方法。它通过使用低秩自适应解决了以前的微调技术的缺点，低秩自适应侧重于有效地近似权重更新。这显著减少了微调中涉及的参数数量 10,000 倍，并且仍然收敛到完全微调模型的性能。 这使得它在成本、时间、数据和 GPU 方面高效，而不会损失性能。 什么是 LoRA 以及为什么它对于模型微调至关重要：视觉指南。 https://preview.redd.it/v2plu0mvvw6d1.png?width=1456&amp;format=png&amp;auto=webp&amp;s=e5f74bcb777d305c08bc74274b0c8a7cc63c973e https://preview.redd.it/vvujm3r3ww6d1.png?width=1456&amp;format=png&amp;auto=webp&amp;s=cfb6111c3bd22585d171ed29c3cadcf823c8839d    提交人    /u/ml_a_day   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh4s3x/r_understanding_lora_a_visual_guide_to_lowrank/</guid>
      <pubDate>Sun, 16 Jun 2024 10:34:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用滤波器组/基于索引的访问进行卷积层的 NN 压缩的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh4cqn/d_papers_in_nn_compression_using_filter/</link>
      <description><![CDATA[我一直在尝试一种减少网络二进制大小（也许还有运行时间）的方案，即使用一种方案来训练网络，该方案强制网络中不同层的相同大小的各个卷积滤波器之间的余弦相似性。 假设一个最佳情况，即来自最大卷积层的所有滤波器都可以在其他层中重复使用，其余层可以将索引存储到相关滤波器中，而这在很大程度上取决于滤波器组的大小。 总的来说，我可以看到一个解决方案，它有一个包含所有必要权重的数组，这些权重是为缓存命中优化而预先排序的，而网络只是保存指向各种滤波器的指针。 有没有关于这种可行性的研究？    提交人    /u/SlayahhEUW   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh4cqn/d_papers_in_nn_compression_using_filter/</guid>
      <pubDate>Sun, 16 Jun 2024 10:03:07 GMT</pubDate>
    </item>
    <item>
      <title>生物医学人工智能会议/研讨会 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh3p8k/conferencesworkshops_for_ai_in_biomedical_p/</link>
      <description><![CDATA[大家好， 我已经从事一项研究（大学课程的一部分） 9 个月了，现在我得到了一些不错的结果。没有什么特别的，但这是一个有趣的主题，数据集是我在 6 个月内收集的。它与健康人与精神障碍患者生物标志物的分析密切相关。 出于某种原因，我的主管不感兴趣，只想写一篇论文，但不想在任何地方发表。我同意，它的结果并不具有开创性，也没有什么值得骄傲的，但我已经足够努力了，可以永远把它带到历史中去。 有没有我可以自己提交的研讨会/会议？有什么截止日期快到了吗？我确实有其他作者署名，但我从来没有做过寻找/提交会议的工作，所以即使上网几天也不知道。 IEEE 的一开始看起来不错，但后来我阅读了评论，发现它们大多数都是骗人的，毫无价值。    提交人    /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh3p8k/conferencesworkshops_for_ai_in_biomedical_p/</guid>
      <pubDate>Sun, 16 Jun 2024 09:13:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] 减少倾斜损失的一种有趣方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh2aqt/p_an_interesting_way_to_minimize_tilted_losses/</link>
      <description><![CDATA[前段时间，我读了一篇关于所谓的倾斜经验风险最小化的论文，后来又读了同一位作者的一篇 JMLR 论文：https://www.jmlr.org/papers/v24/21-1095.html 这样的公式让我们能够以更“公平”的方式对困难样本进行训练，或者相反，如果这些困难样本实际上是异常值，则对这些困难样本的敏感度会降低。但最小化它在数字上具有挑战性。所以我决定尝试在博客文章中设计一种补救措施。我认为这是一个有趣的技巧，在这里很有用，我希望你也会发现它很好： https://alexshtf.github.io/2024/06/14/Untilting.html    提交人    /u/alexsht1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh2aqt/p_an_interesting_way_to_minimize_tilted_losses/</guid>
      <pubDate>Sun, 16 Jun 2024 07:27:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] OOD 泛化在 LLM 时代还有未来吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh1eox/d_is_ood_generalization_still_a_future_in_the_llm/</link>
      <description><![CDATA[我认为 OOD 泛化是一个重要问题，因为它拉近了与现实的距离。但我担心最近的会议（如 ICLR、ICML、NeurIPS 等）没有很多人研究这个问题。查看一些 OOD 泛化基准，许多方法（如 IRM、GroupDRO）甚至比 ERM 更弱。所以，我想知道是不是因为这个领域的一些困难人们停止了研究。还是因为其他原因。    提交人    /u/EducationalOwl6246   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh1eox/d_is_ood_generalization_still_a_future_in_the_llm/</guid>
      <pubDate>Sun, 16 Jun 2024 06:23:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 职位：机器学习中的应用驱动创新</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh0rgg/r_position_applicationdriven_innovation_in/</link>
      <description><![CDATA[  由    /u/FlyingQuokka  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh0rgg/r_position_applicationdriven_innovation_in/</guid>
      <pubDate>Sun, 16 Jun 2024 05:38:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 1D CNN 在波形和频谱图上与 2D CNN 性能对比</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dgyjum/d_1d_cnn_on_waveforms_and_spectrograms_vs_2d_cnn/</link>
      <description><![CDATA[大多数成功的音频框架都使用二维卷积神经网络 (CNN)，这违反直觉，因此我尝试在 Kaggle 上的 BirdCLEF-2024 上使用简单框架进行训练时进行实验，并且我对学习有一些疑问：  在学习波形输入时，为什么 1D CNN 不会收敛，甚至在验证分割时立即发散？ 在对频谱图幅度 (stft -&gt; abs -&gt; log1p) 进行训练时，为什么 1D CNN 的表现比 2D CNN 差？ 虽然频谱图在获取幅度时似乎会丢失相位偏移信息，但它的表现优于原始波形。那么，人类/动物耳朵里是否有 torch.stft 以获得更好的感知？例如，孩子们可以听懂“从未见过”的高音调米老鼠讲话。     submitted by    /u/ivanstepanovftw   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dgyjum/d_1d_cnn_on_waveforms_and_spectrograms_vs_2d_cnn/</guid>
      <pubDate>Sun, 16 Jun 2024 03:18:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要帮助寻找 Geoffrey Hinton 的旧视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dgs9xk/d_need_help_finding_an_old_geoffrey_hinton_video/</link>
      <description><![CDATA[虽然标记为 [D]，但事实并非如此。  大约 15 年前，我在 YouTube 上看到了一个视频，是多伦多大学的 Geoff Hinton 讲座。在视频中，他解释了用于 MNIST 数字识别的经典神经网络。在视频的某个时刻，他谈到了如何反向运行推理，有效地使网络“想象”一个数字。我似乎在任何地方都找不到这个视频。如果我没记错的话，它可能在某个时候上传到了 Andrew Ng 的学习平台，出于某种原因，这个子版块不让我说出名字。 请帮我找到这个视频。提前谢谢！    提交人    /u/edirgl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dgs9xk/d_need_help_finding_an_old_geoffrey_hinton_video/</guid>
      <pubDate>Sat, 15 Jun 2024 21:37:12 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 作为机器学习工程师，收益递减问题。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dgrmf4/discussion_diminishing_return_problem_as_a/</link>
      <description><![CDATA[对于所有机器学习工程师或应用科学家同行，您如何处理工作中的收益递减问题？假设您已经将模型移动到 DL，已经使用 transformer，并且已经使用了您能想到的所有功能，现在您要做什么才能继续作为产品团队中的机器学习工程师展现影响力。    提交人    /u/The-AI-Alchemy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dgrmf4/discussion_diminishing_return_problem_as_a/</guid>
      <pubDate>Sat, 15 Jun 2024 21:06:03 GMT</pubDate>
    </item>
    </channel>
</rss>