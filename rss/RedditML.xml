<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 07 Jan 2024 01:05:43 GMT</lastBuildDate>
    <item>
      <title>[D] 关系抽取</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190dzwu/d_relation_extraction/</link>
      <description><![CDATA[我正在尝试使用 Hugging Face 中的 REBEL 模型进行关系提取。它通过三元组线性化输出关系三元组。它是在 REBEL 数据集上进行训练的，该数据集本质上是维基百科数据。我有自由格式的文本，我想从中生成关系三元组。那么，如何从该文本创建一个数据集，以便与 REBEL 数据集紧密结合？我想在自由格式文本上微调模型。  REBEL 模型：https://huggingface.co/Babelscape/rebel-large REBEL 数据集：https://huggingface.co/datasets/Babelscape/rebel-dataset  如果您认为还有任何其他值得尝试进行关系提取的 ML 模型，我们将非常感谢您提供的信息。 :) 谢谢！   由   提交 /u/RajHalifax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190dzwu/d_relation_extraction/</guid>
      <pubDate>Sat, 06 Jan 2024 23:52:09 GMT</pubDate>
    </item>
    <item>
      <title>[D]我们的大脑如何防止过度拟合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</link>
      <description><![CDATA[老实说，这个问题引发了一系列其他问题，老实说，这很有趣，我们阻止这种情况发生的机制是什么？ 梦想只是生成数据增强，以便我们防止过度拟合吗？ 如果我们进一步将过度拟合拟人化，患有学者综合症的人会过度拟合吗？ （因为他们在狭窄的任务上表现出色，但在泛化方面有其他障碍。尽管他们仍然有梦想） 为什么我们不记忆，而是学习？    由   提交 /u/BlupHox   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/</guid>
      <pubDate>Sat, 06 Jan 2024 22:33:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mangio RVC - 使用 rmvpe 时阈值检测高 - 转换后的音频门控？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1906vf0/r_mangio_rvc_threshold_detection_high_when_using/</link>
      <description><![CDATA[我正在使用 Mangio RVC 23.7.0 使用我自己和其他人训练的各种模型来转换一些语音音频。我一直在使用 rmvpe 来处理音调，因为它似乎在所有方面都有最准确的结果，但我有一个大问题，因为它不喜欢任何未达到高 dB 级别的音频，所以我不得不采用压缩音频并对其进行限制，以便从 rmvpe 获得不错的结果。即便如此，音量还是会波动，我必须在外部对转换后的音频进行进一步压缩。原始的未压缩音频甚至没有那么安静，平均约为 - 12db。这是我工作流程中的一个额外步骤，我真的不想这样做。  听起来 rmvpe 的设置中有一个噪声门或非常高的检测，因此它在较安静的部分上遇到了麻烦，但我一生都无法弄清楚它在哪里。 &lt; p&gt;未压缩的语音音频我在使用 so-vits-fork 时没有遇到问题，因为我刚刚将检测阈值设置为 - 60db 左右，它可以捕获声音中的每一个细微差别，但从我测试过的 rvc 来看而 rmvpe 只是在发音和音高检测方面给出更准确的结果。  有什么办法可以让 rmvpe 或 mangio rvc 检测较低级别的音频吗？   由   提交 /u/juliusvi2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1906vf0/r_mangio_rvc_threshold_detection_high_when_using/</guid>
      <pubDate>Sat, 06 Jan 2024 18:45:47 GMT</pubDate>
    </item>
    <item>
      <title>[D]试图理解专有硬件制造商将重组行业并导致OpenAI企业价值下降的论点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1905izt/d_trying_to_understand_the_argument_that/</link>
      <description><![CDATA[一些硅谷声音的观点之一是，有两个主要因素会导致专有/闭源模型构建者泄漏价值：(1) 延迟当前的所有工具都使得构建生产质量的代码变得不可行——API 应该需要 30-50 毫秒而不是 30-50 秒。 (2) 如果您尝试构建某些东西，这些平台上 100 万个代币的成本在经济上是不可能的。  争论的焦点是，云服务将会为用户带来毫秒级的延迟，并且 100 万个代币的价格约为 10-20 美分，并且他们需要构建自己的定制硬件来做到这一点。  讨论这个问题的人不是机器学习工程师/研究人员。发生这样的事情的可行性是什么？除了实际制造能够将成本降低几个数量级的硬件之外，这种观点还面临哪些挑战？   由   提交 /u/SloppyDrunkCarrot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1905izt/d_trying_to_understand_the_argument_that/</guid>
      <pubDate>Sat, 06 Jan 2024 17:49:04 GMT</pubDate>
    </item>
    <item>
      <title>单变量异常检测[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1904tl7/univariate_anomaly_detection_d/</link>
      <description><![CDATA[嗨！ 我面临着一个看似“简单”的问题，但我一直在努力解决它现在处于异常/异常值检测领域。 我有一个包含大约 60K 数据点的数据集。每个数据点都是一个组的一部分（~1500 个组；最小组大小为 15），并且具有长度参数。 任务是在组内执行异常检测，即，如果数据点具有与组中其他数据点相比，长度不规则，将其标记为异常。 我对数据使用 log2 转换，转换后，大多数组 (75%)基于 Shapiro-Wilks 检验呈正态分布。 作为第一个解决方案，我尝试了 std 与均值的经典距离，其中如果长度大于mean+3*std，则这是一个异常。 此解决方案存在两个问题： 在具有大量数据点的组中，其中绝大多数数据点具有相同或非常相似的长度， std 非常小，因此使阈值非常小，并导致对数据点发出警报，我不认为这是异常。 这种方法导致了相对较高的检测（约 250 个异常），我的目标是仅对所有组中数据中的一小部分最极端的异常发出警报。 当我尝试提高阈值（例如提高到 4std）时，我遇到了另一个问题，我错过了这个问题组中的异常，其中一个数据点与其他数据点相比具有非常大的长度，这导致高标准差，从而使极端数据点与平均距离相比具有“低”标准差。 我很感激有关该主题的任何帮助或想法。谢谢！   由   提交/u/thk_ML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1904tl7/univariate_anomaly_detection_d/</guid>
      <pubDate>Sat, 06 Jan 2024 17:19:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用开源模型进行长代理树搜索取得令人难以置信的结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</link>
      <description><![CDATA[你好， 我看到 GPT-4 的长代理树搜索以 94.4% 的 pass@1 领先于 HumanEval现在几周了。 https://paperswithcode.com/sota/code- Generation-on- humaneval &lt; p&gt;​ 原始论文的作者在他们的官方 github 存储库。我必须更改一些代码才能使用 CodeLlama-7b 进行尝试，并使用 pass@1 进行人类评估，仅 2 次最大迭代即可将 HumanEval 得分从 37% 提高到大约 70%。 这是一些令人难以置信的结果在我看来，因为这个分数比只有 7b 模型的 GPT-3.5 更高。我认为必须进行更多测试，但令我惊讶的是人们没有更多地谈论这一点。   由   提交/u/ArtZab  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/</guid>
      <pubDate>Sat, 06 Jan 2024 16:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练后设置 EMA 衰减？新颖的 Karras Power EMA 教程 + 实施</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1902xpv/p_set_ema_decay_after_training_novel_karras_power/</link>
      <description><![CDATA[      https://github.com/cloneofsimo/karras-power-ema-tutorial 最近，Karras 演示了帖子-hoc ema 方法，他能够“模拟”通过保存两份 ema 和聪明的数学，训练后任意 ema 衰减因子。 我深吸了一口气才理解它，并在自述文件 + 工作示例上写了一篇教程！ 但你可能会说……为什么？事实证明，Ema 衰减是一个非常敏感的超参数 因为你可以在训练后设置 EMA 衰减因子，所以你可以“扫描”超参数。训练后，得到最好的检查点。   由   提交/u/cloneofsimo  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1902xpv/p_set_ema_decay_after_training_novel_karras_power/</guid>
      <pubDate>Sat, 06 Jan 2024 15:55:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] llama.cpp 使用单个 LLM 管道进行 GGUF 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zz27k/p_llamacpp_gguf_inference_with_a_single_llm/</link>
      <description><![CDATA[      ​ https://preview.redd.it/i4rxpfwcdtac1.jpg?width=1296&amp;format=pjpg&amp;auto=webp&amp;s=62c2fa0a8 d724bfcaa5a21a2e40b7343396bc16f&lt; /a&gt; txtai有统一的LLM管道，可以加载Hugging Face模型、llama.cpp GGUF文件和 LLM API。上面的示例从 Hugging Face Hub 下载 GGUF 文件并使用模型运行推理。 请参阅本文了解更多信息：https://neuml.hashnode.dev/integrate-llm-frameworks   由   提交/u/davidmezzetti   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zz27k/p_llamacpp_gguf_inference_with_a_single_llm/</guid>
      <pubDate>Sat, 06 Jan 2024 12:33:16 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 Gemini 潜在训练数据泄露 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zy75g/google_gemini_potential_training_data_leak_d/</link>
      <description><![CDATA[ 由   提交 /u/Shemozzlecacophany   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zy75g/google_gemini_potential_training_data_leak_d/</guid>
      <pubDate>Sat, 06 Jan 2024 11:37:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 摩根大通放弃多式联运文件的 DocLLM！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zy0g4/d_jpmorgan_drops_docllm_for_multimodal_documents/</link>
      <description><![CDATA[摩根大通放弃了发票、报告和多式联运文档的 DocLLM。合同！ 我脑子里有一些有用的 pdf 提取项目。我很高兴在原始论文中看到等效模型的开源可用性。 对此有何想法？    ;由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zy0g4/d_jpmorgan_drops_docllm_for_multimodal_documents/</guid>
      <pubDate>Sat, 06 Jan 2024 11:25:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变形金刚的思想链表现力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/</link>
      <description><![CDATA[论文。我不隶属于作者。 摘要：  最近的理论工作已经发现了令人惊讶的简单推理问题，例如检查图中的两个节点是否连接或模拟有限状态机，标准变压器在读取输入后立即回答，这证明是无法解决的。然而，在实践中，变形金刚的推理可以通过允许他们使用“思维链”来改进。或“暂存器”，即，在回答之前生成中间标记序列并对其进行调节。受此启发，我们问：这种中间生成是否从根本上扩展了仅解码器变压器的计算能力？我们证明答案是肯定的，但增加的数量很大程度上取决于中间代的数量。例如，我们发现具有对数数量的解码步骤（相对于输入长度）的 Transformer 解码器仅略微突破了标准 Transformer 的限制，而线性数量的解码步骤则增加了一个明显的新能力（在标准复杂性猜想下）：所有常规语言。我们的结果还表明，线性步骤使转换器解码器保持在上下文相关语言内，而多项式步骤使它们能够准确识别多项式时间可解决问题的类别——这是根据标准复杂性类别对一类转换器进行的第一个精确表征。总之，我们的结果提供了一个细致入微的框架，用于理解变压器的思想链或暂存器的长度如何影响其推理能力。    由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/</guid>
      <pubDate>Sat, 06 Jan 2024 07:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习有什么有趣的数学理论吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zo7or/d_is_there_any_interesting_mathematical_theory_of/</link>
      <description><![CDATA[大家好！我的问题在标题中，这是一些背景信息。我的背景可以描述为“主修理论计算机科学（非常强调‘理论’这个词，想想计算复杂性理论），辅修数学”。 几年前，我参加了入门课程机器学习课程......非常沮丧和失望。  没有解释任何东西应该如何或为什么工作，相反有很多不令人信服的猜测，比如“如果你添加一个卷积层，然后它将学习简单的几何形状，因此后面的层将有更多的结构可以使用”或者“我们可以在 RNN 中使用额外的输入，并以某种方式组合三个输入，这样新的输入将起到‘长期记忆’的作用”。我没想到数学逻辑或编程语言理论的严谨程度，但其他科学，例如经济学，至少可以用一些简化的模型来解释他们正在研究的现象。我们在机器学习中没有类似的东西吗？ 在课程中，我们直接跳到一些相当复杂的问题，例如区分猫的图片和狗的图片。我怀疑是否有人能够对这两类图片给出一个很好的定义。虽然这让神经网络能够解决问题变得更加令人印象深刻，但我看不出我们可以从中学到什么关于神经网络如何做到这一点的信息。训练神经网络区分蓝色和绿色、正方形和圆形等，然后尝试使用结果来分析神经网络如何学习不是更好吗？  后来我开了一个很少有机器学习教科书。  我真的很喜欢有关 PAC 学习的部分，总体来说统计学习也很适合我。 有关神经网络和尤其是深度学习，几乎与我在课程中听到的关于仪式舞蹈如何导致降雨的炼金术级别的推测相同。  所以我试图在 arXiv 上找到一些现代结果。   大多数关于机器学习的论文都将更难以推理的模型应用于更难以理解的问题，这让我感到非常失望。 有一些关于神经网络是通用逼近器的结果，即使不多也很好。 我还遇到过（在写硕士论文时）一两篇关于感知器和电路计算复杂性的论文阈值函数。令人遗憾的是，似乎几乎没有关于这个主题的当代研究。  ​ 这结束了我的“咆哮”。 ，我希望这能够澄清“机器学习数学”的类型。我正在寻找。请注意，我确实理解使用现代模型需要大量的知识和专业知识，我并不是想贬低你所做的工作，我只是对找到这些模型如何工作的理论解释有多么困难感到沮丧，考虑到机器学习的普及。 我真的很感激任何想法或建议！ 此外，英语不是我的母语，所以我很抱歉任何拼写错误、不正确的语法或尴尬的句子。 ​ UPD：看到这篇文章刚刚。我真的很想看到更多类似的作品。   由   提交/u/a_broken_coffee_cup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zo7or/d_is_there_any_interesting_mathematical_theory_of/</guid>
      <pubDate>Sat, 06 Jan 2024 01:50:11 GMT</pubDate>
    </item>
    <item>
      <title>基于变压器的法学硕士不是一般学习者：通用电路视角 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</link>
      <description><![CDATA[https://openreview.net/forum?id =tGM7rOmJzV  （法学硕士）的巨大成功引发了人工智能界研究重点的显着转变。这些令人印象深刻的实证成就激发了人们对法学硕士是“通用人工智能（AGI）的火花”的期望。然而，一些评估结果也呈现了法学硕士失败的令人困惑的例子，包括一些看似微不足道的任务。例如，GPT-4 能够解决一些 IMO 中对研究生来说可能具有挑战性的数学问题，而在某些情况下它可能会在小学水平的算术问题上出错。 ...  我们的理论结果表明 T-LLM 无法成为通用学习者。然而，T-LLM 在各种任务中取得了巨大的经验成功。我们对这种不一致现象提供了一个可能的解释：虽然 T-LLM 不是一般学习者，但他们可以通过记忆大量实例来部分解决复杂的任务，从而导致人们产生一种错觉，认为 T-LLM 具有真正解决这些任务问题的能力。     由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</guid>
      <pubDate>Fri, 05 Jan 2024 21:39:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] GPT-4V(ision) 是一款多面手 Web 代理，如果接地 - 俄亥俄州立大学 2024 年 - 可以成功完成实时网站上 50% 的任务！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zgfmx/r_gpt4vision_is_a_generalist_web_agent_if/</link>
      <description><![CDATA[&lt;表&gt;   论文：https://arxiv.org/abs/2401.01614  博客：https ://osu-nlp-group.github.io/SeeAct/  代码： https://github.com/OSU-NLP-Group/SeeAct  摘要：  大型多模态模型（LMM）的最新发展），特别是 GPT-4V(ision) 和 Gemini，一直在快速扩展多模态模型的能力边界，超越图像字幕和视觉问答等传统任务。在这项工作中，我们探索了像 GPT-4V 这样的 LMM 作为通用网络代理的潜力，它可以遵循自然语言指令来完成任何给定网站上的任务。我们提出了 SEEACT，这是一种通用网络代理，它利用 LMM 的力量来实现集成的视觉理解和在网络上的操作。我们对最近的 MIND2WEB 基准进行评估。除了对缓存网站进行标准离线评估之外，我们还通过开发允许在实时网站上运行 Web 代理的工具来启用新的在线评估设置。 我们表明，GPT-4V 为网络代理提供了巨大的潜力 - 如果我们手动将其文本计划转化为网站上的操作，它可以成功完成实时网站上 50% 的任务。这大大优于文本-仅限专门针对网络代理进行微调的 LLM，例如 GPT-4 或更小的模型（FLAN-T5 和 BLIP-2）。然而，接地仍然是一个重大挑战。现有的 LMM 基础策略（例如标记集提示）对于网络代理来说并不有效，而我们在本文中开发的最佳基础策略同时利用了 HTML 文本和视觉效果。然而，仍然存在与预言机基础存在巨大差距，留有足够的进一步改进的空间。   https://preview.redd.it/1w22ga2ejoac1.jpg?width=706&amp;format=pjpg&amp;auto=webp&amp;s=204d4852c614efaf8c 39c990d25a7acae805290e  https://preview.redd .it/vaabea2ejoac1.jpg?width=1344&amp;format=pjpg&amp;auto=webp&amp;s=17f5a5ca7e1add213ca4d75ed53a74e230369655 https://preview.redd.it/2720ob2ejoac1.jpg?width=1340&amp;format=pjpg&amp;auto=webp&amp;s=4cec63cdd3e14 48e03f82309ac219684c62b8ffb https://preview .redd.it/9wn5sa2ejoac1.jpg?width=1242&amp;format=pjpg&amp;auto=webp&amp;s=dcc8919105686007d670f9b140aaeb3e4683d56e https://preview.redd.it/ttgaad2ejoac1.jpg?width=801&amp;format=pjpg&amp;auto=webp&amp;s=568 4aa7969a6564eab8cb4a5ea36fa21f4c63e9e    由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zgfmx/r_gpt4vision_is_a_generalist_web_agent_if/</guid>
      <pubDate>Fri, 05 Jan 2024 20:18:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>