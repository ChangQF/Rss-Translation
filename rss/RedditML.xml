<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 23 Dec 2023 06:16:20 GMT</lastBuildDate>
    <item>
      <title>泰勒级数注意[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/</link>
      <description><![CDATA[我最近读完了动物学博客文章关于 BASED，一种新的语言模型，它使用局部卷积和使用泰勒级数近似的自注意力，似乎基于本文。&lt; /p&gt; 读完后我的一个问题是这种局部卷积对模型性能有多重要？是否有关于仅采用这种泰勒注意力的变压器架构的研究？ BASED 模型显然具有良好的性能，论文提供的直观理解是，这些卷积在 AR 不是一个大挑战的短距离场景中使模型受益，这是有道理的，但普通注意力对于 AR 或短距离困惑没有问题。答案是泰勒近似在这些短距离情况下会不太准确，需要卷积来补偿吗？   由   提交 /u/Aggressive-Solid6730    reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/</guid>
      <pubDate>Sat, 23 Dec 2023 04:13:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从时间序列中提取高斯噪声</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ovd9u/d_extracting_gaussian_noise_from_a_timeseries/</link>
      <description><![CDATA[嘿伙计们， 这件事困扰了我一段时间，所以我想和大家讨论一下。通常，我使用几种分解技术中的任何一种来从时间序列数据（或部分噪声分量）中提取噪声。例如，它可能是快速傅里叶变换的几个高频分量，或者奇异值分解的高阶分量等。 假设这些技术都不存在，我想构建一个神经网络将接受连续的时间序列输入，并“教导”它可以找到输入数据的线性或非线性变换，这将至少给出一些噪声时间序列作为输出。文献似乎侧重于为去噪目的提供噪声和干净的版本，但我对残差提取和分析的转换感兴趣。 对于初学者，我想重点关注提取高斯噪声（最好是附加但只需要一些可逆操作）。一种方法是建立一个网络，强制输出残差的高斯性和随机性（不知道如何）。希望讨论可能的方法或指出有关残差分析的文献。 编辑：看来 ICA 可能会在这里发挥作用。这里有很多好的理论。   由   提交/u/PUthroaway2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ovd9u/d_extracting_gaussian_noise_from_a_timeseries/</guid>
      <pubDate>Sat, 23 Dec 2023 02:22:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用启动攻击绕过开源法学硕士的安全培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oqz32/r_bypassing_the_safety_training_of_opensource/</link>
      <description><![CDATA[我是这篇短文的作者之一；欢迎提供反馈并非常感谢！ 论文：https://arxiv.org/abs /2312.12321 项目页面：https://llmpriming.focallab.org 代码+数据：https:// github.com/uiuc-focal-lab/llm-priming-attacks 摘要  随着最近人气的飙升的法学硕士对法学硕士安全培训的需求不断增加。在本文中，我们研究了 SOTA 开源 LLM 在简单的、无优化的攻击（我们称为启动攻击）下的脆弱性，这些攻击很容易执行并有效地绕过安全训练的对齐。根据 Llama Guard 的测量，我们提出的攻击将有害行为的攻击成功率提高了 3.3 倍。     ;由   提交 /u/Dapper_Fudge6647   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oqz32/r_bypassing_the_safety_training_of_opensource/</guid>
      <pubDate>Fri, 22 Dec 2023 22:41:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么张量程序没有像神经切线核那样受到同样的关注？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</link>
      <description><![CDATA[神经正切核经常在理论论文中被引用，作为推广线性函数逼近器证明的基础。然而，它们有几个缺点，即它们不包含特征学习的概念。张量程序应该可以解决这个问题，但我认为我从未在理论论文中看到过它们被引用。人们是否怀疑结果或认为结果缺乏严谨性？结果是否被认为不太有用？或者它们只是因为数学上更复杂且更难学习而使用较少？  我问这个问题的部分原因是我想知道它是否值得花精力去阅读和理解整个论文系列，或者这项工作是否经过深思熟虑。    由   提交 /u/OptimizedGarbage   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</guid>
      <pubDate>Fri, 22 Dec 2023 22:03:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenMetricLearning 2.0 发布！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18onq8k/p_openmetriclearning_20_is_released/</link>
      <description><![CDATA[您好！  我想介绍 OpenMetricLearning 2.0 的发布！库用于训练将数据表示为向量的深度学习模型。此外，我们还有一个预训练图像模型、DDP 支持、大量示例和文档。  该版本有哪些新功能？  迁移到 PyTorch 2.0（很容易）&amp; Lightning 2.0（很痛苦） 减少了通过 pip 安装的依赖项数量 对所有当前版本的 Python 提供了稳定支持：现在 CI/CD 对所有内容都运行测试 - &lt; strong&gt;3.8、3.9、3.10、3.11 修复了一些烦人的小错误，整理了文档，并简化了公共数据集上管道的启动（例如 InShop、 斯坦福在线产品、CAR、CUB） 对于重新识别：添加了在以下情况下更正确地处理同一对象的一系列照片的功能：计算指标  我们希望所有这些改变能让OML使用起来更加方便。非常欢迎您在GitHub上的⭐！ ​    由   提交 /u/Zestyclose-Check-751   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18onq8k/p_openmetriclearning_20_is_released/</guid>
      <pubDate>Fri, 22 Dec 2023 20:09:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究论文的升级何时会成为新论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</link>
      <description><![CDATA[嘿，我是研究新手，过去 6 年来一直致力于实现一篇论文并根据我们在推荐系统领域的用例进行定制几个月。我按照 1. 在损失函数中使用不同的运算符（余弦相似度而不是点积）进行了一些更改。 2. 使用不同类型的数据。这有点难以解释，我使用元数据相似性而不是论文中一对共现的逐点互指数（PMI）。 3. 不同的数据预处理方式。 结果确实有所改善，但我不确定这是否只是一些修改，或者它本身就是一篇论文。任何人都可以阐明新想法与小修改的一些迹象吗？   由   提交/u/Abs0lut_Jeer0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</guid>
      <pubDate>Fri, 22 Dec 2023 19:49:11 GMT</pubDate>
    </item>
    <item>
      <title>[N] 使用 Ollama 在几秒钟内在本地运行 Mixtral LLM！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/</link>
      <description><![CDATA[嘿， 人工智能最近变得很疯狂，事情变化得非常快。我制作了一个涵盖新发布的 Mixtral AI 的视频，详细介绍了它的工作原理以及如何在本地运行它。我还介绍了 Microsoft 的 Phi LLM 以及未经审查的 Mixtral 版本 (Dolphin-Mixtral)，请查看！ https:/ /youtu.be/ILfmdKMa2Lo 说实话，这些事情的进展速度太疯狂了！看起来我们几个月前刚刚推出了 Mistral，而现在我们有了 mixtral，它在各个方面都彻底摧毁了它，而所有这些技术都可以在本地运行，确保我们的隐私绝对免费！ 请告诉我您的想法，或者如果您对其他视频有任何疑问/请求， 干杯  &amp;# 32；由   提交/u/dev-spot  /u/dev-spot  reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18on5nr/n_run_mixtral_llm_locally_in_seconds_with_ollama/</guid>
      <pubDate>Fri, 22 Dec 2023 19:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 培训本地法学硕士将文本翻译成代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/</link>
      <description><![CDATA[我正在开发一个项目，该项目将获取 PDF 并将其转换为代码。我不能太具体地介绍我正在使用的 PDFS。但是有人对我应该使用/微调什么模型有任何建议吗？我可以根据现有 PDF 和现有代码创建大型数据集。但我不确定“如何” （模型/训练/微调等） 理想的解决方案是我可以在工作中使用的模型，因此任何我可以独立编程/训练的东西都会很棒。 任何建议将不胜感激，谢谢！   由   提交/u/slb1357  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/</guid>
      <pubDate>Fri, 22 Dec 2023 16:43:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] Perseus：消除大型模型训练中的能量膨胀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.06902 项目页面：https:// /ml.energy/zeus/perseus/ 积分： https://ml.energy/zeus/perseus/integrating/ 摘要：  在大量数据上训练大型人工智能模型GPU 消耗大量能源。我们观察到，并非训练期间消耗的所有能量都会直接贡献于端到端训练吞吐量，并且可以在不减慢训练速度的情况下消除很大一部分能量，我们将其称为能量膨胀。在这项工作中，我们确定了大型模型训练中两个独立的能量膨胀来源：内在和外在，并提出Perseus，一个统一的优化框架减轻两者。珀尔修斯获得“迭代时间-能量”任何大型模型训练作业的帕累托前沿都使用基于图切割的高效迭代算法，并在一段时间内安排其前向和后向计算的能耗，以消除内在和外在的能量膨胀。对 GPT-3 和 Bloom 等大型模型的评估表明，Perseus 将大型模型训练的能耗降低了高达 30%，实现了以前无法实现的节省。  &lt;!-- SC_ON - -&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/</guid>
      <pubDate>Fri, 22 Dec 2023 12:59:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士如何知道每个子标记中存在的字符？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</link>
      <description><![CDATA[例如，当我向 chatGPT 询问该句子中的字母数量时： 但是，我想知道这些是如何实现的LLM 可以执行计数，因为知道分词器正在执行子标记级别而不是字符级别（这意味着每个子标记“可能”不知道它具有的字符）。）。 答案是 15这是正确的 ​ 但是，我想知道这些 LLM 如何执行计数，因为知道标记器正在执行子标记级别而不是字符级别（意味着每个子标记都是“也许”不知道它有哪些字符”）。 ​ ​ &lt;!-- SC_ON - -&gt;  由   提交/u/kekkimo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/</guid>
      <pubDate>Fri, 22 Dec 2023 12:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] 苹果研究人员推出 DeepPCR：一种新颖的机器学习算法，可并行化典型的顺序操作，以加速神经网络的推理和训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</link>
      <description><![CDATA[”论文通过各种应用展示了 DeepPCR 的有效性。它在多层感知器中实现了前向传递高达 30 倍的加速和后向传递高达 200 倍的加速。此外，该算法还应用于深度 ResNet 架构的并行训练和扩散模型的生成，从而使训练速度提高 7 倍，生成速度提高 11 倍。” 论文：https://arxiv.org/pdf/2309.16318.pdf 研究页面：https://machinelearning.apple.com/research/deepcr https://twitter.com/i/status/1735876638947348656   由   提交/u/paryska99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/</guid>
      <pubDate>Fri, 22 Dec 2023 12:19:33 GMT</pubDate>
    </item>
    <item>
      <title>[D]什么时候应该和不应该平衡不平衡的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</link>
      <description><![CDATA[我似乎总是在这个问题上得到相互矛盾的答案，想法？    ;由   提交/u/Throwawayforgainz99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/</guid>
      <pubDate>Fri, 22 Dec 2023 11:26:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我尝试教 Mistral 7B 一门新语言（巽他语），它成功了！ （有点）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/</guid>
      <pubDate>Fri, 22 Dec 2023 10:54:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有哪些便宜又好的设备可以用于 CUDA 训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</link>
      <description><![CDATA[在过去的四年里，我一直在大学服务器上训练我的所有模型，但是，因为我即将毕业 - 这根本行不通不再… 我一直在使用 Mac，但是想到毕业后无法运行 CUDA 就很难受了，哈哈 - 考虑设置尽可能便宜的计算机来训练 AI，但是，我当涉及到 Windows / Linux 计算机时，不知道从哪里开始 - 并且也不太了解所有 GPU 之间的区别 你们知道有什么便宜但性能好的 AI 计算机吗 -如果他们可以购买二手的，那就更好了！   由   提交/u/Middle_Stomach_6681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/</guid>
      <pubDate>Fri, 22 Dec 2023 07:22:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>