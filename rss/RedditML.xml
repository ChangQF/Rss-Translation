<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 26 Oct 2024 18:20:19 GMT</lastBuildDate>
    <item>
      <title>[P] 使用神经网络进行形状限制回归</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/</link>
      <description><![CDATA[前段时间在工作中，我们必须强制我们的模型学习一个特征的递增函数。例如，作为出价函数的中标概率应该增加。最近，我偶然发现了一篇关于使用形状限制函数进行回归的论文 https://arxiv.org/abs/2209.04476，我想通过实际的代码来训练这样的模型，让它更具体一些。 因此，我写了一篇博文：https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html 还有一个带有随附代码的笔记本：https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape_constrained_models.ipynb 我以前经常做广告。所以这种模型在这个行业似乎很有用——根据出价预测赢得广告拍卖的概率。我希望它在其他地方也有用。 所以我希望你会喜欢它！这是一个很大的“数学”，但你知道，它不可能是别的。    提交人    /u/alexsht1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/</guid>
      <pubDate>Sat, 26 Oct 2024 16:58:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 任何设备上的实时角色动画</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/</link>
      <description><![CDATA[      我最近看到了阿里巴巴的这篇论文MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling，真的很有趣。浏览完论文后，我想，“嘿，可以使用一些开源工具复制这个工作流程！”我设法创建了一个可行的系统，该系统可以在设备上以 ~10fps 的速度实时运行，请注意，这是在一台配备 8 GB RAM 和 4 GB VRAM 的土豆笔记本电脑上运行的。 原始视频 重建视频 当前工作流程如下所示 -&gt; 1. 我使用 Tracking4All 创建了一个 Unity 应用程序，它可以从网络摄像头获取输入并使用 Mediapipe 生成动画姿势。 2. 接下来，我将这些生成的图像发送到 Python 服务器，该服务器接收原始帧、动画角色以及来自 Mediapipe 姿势的人物面具。 3. 最终使用 MI-GAN，我能够实时移除人物。 这个项目目前存在一些缺陷 1. MI-GAN 模型虽然速度很快，但却是主要的瓶颈。我尝试了 OpenCV 中提供的其他算法，但它们更糟糕、更慢（~1fps）。 2. 角色大小调整并不总是准确的，尽管可以在 Unity 中轻松调整。 3. 遮挡问题仍然是一个挑战。 此外，值得注意的是，Tracking4All 软件包需要许可证，这可能会限制可访问性。 是否有任何算法可以在各种设备（移动设备、Windows、Mac 和 Linux）上实时执行修复？ 该项目的目标是创建任何人都可以在任何设备上运行的端到端工作流程。这在 AR 和 VFX 中有许多应用程序！您对此有何看法？我接下来应该实现什么？   由    /u/Jazzlike-Shake4595  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/</guid>
      <pubDate>Sat, 26 Oct 2024 15:49:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于不同长度文本的 KV 缓存 - 请帮忙🙏</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gclcmk/d_kv_caching_for_varying_length_texts_help_please/</link>
      <description><![CDATA[        由    /u/themathstudent  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gclcmk/d_kv_caching_for_varying_length_texts_help_please/</guid>
      <pubDate>Sat, 26 Oct 2024 13:41:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻求针对 CVPR、ICML 等会议正在进行的完整论文的合作。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gcjfz1/r_looking_for_collaborations_on_ongoing/</link>
      <description><![CDATA[大家好， 我们小组，印度理工学院鲁尔基分校视觉与语言小组，最近有三篇研讨会论文被 NeurIPS 研讨会接受！🚀 我们还建立了一个网站 👉 VLG，展示我们参与过的其他出版物，因此我们的团队正在稳步建立 ML 和 AI 研究组合。目前，我们正在合作撰写几篇正在进行的论文，目的是向 CVPR 和 ICML 等顶级会议提交全文。 话虽如此，我们还有更多让我们兴奋的想法。尽管如此，我们的主要限制之一是无法获得适当的指导和 GPU 和 API 的资金，这对于试验和扩展我们的一些概念至关重要。如果您或您的实验室有兴趣一起工作，我们很乐意探索我们感兴趣领域的交集以及您可能带来的任何新想法！ 如果您有可用资源或有兴趣讨论潜在的合作，请随时联系我们！期待着建立联系并共同建立有影响力的东西！这是我们的 Open Slack 的链接👉 Open Slack    提交人    /u/vlg_iitr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gcjfz1/r_looking_for_collaborations_on_ongoing/</guid>
      <pubDate>Sat, 26 Oct 2024 11:59:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人知道 Eleven 实验室是如何设计提示音的吗？我想根据提示音生成新的声音，而不是声音克隆。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gcej8u/d_do_anyone_know_how_eleven_labs_is_designing_the/</link>
      <description><![CDATA[        由    /u/usama__01 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gcej8u/d_do_anyone_know_how_eleven_labs_is_designing_the/</guid>
      <pubDate>Sat, 26 Oct 2024 06:02:26 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 开源视频索引/标签/标签生成工具。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/</link>
      <description><![CDATA[伙计们，我正在寻找一个开源工具或任何可以帮助我生成视频标签的 repo，以便对多个视频进行分类并进行进一步分析。 我想要的等价物是 Azure AI clvideo inxer，但如果有这样的开源工具，它将解决问题。    提交人    /u/jokingwizard   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/</guid>
      <pubDate>Sat, 26 Oct 2024 04:18:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 可访问的预训练视频嵌入模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/</link>
      <description><![CDATA[我在这个领域还比较陌生，所以如果我遗漏了一些信息或者有更好的地方可以发布这篇文章，我深表歉意。我一直在致力于一个项目，试图为长视频实现语义搜索和 RAG，并试图了解视频嵌入模型的当前状态。 我的问题是，我发现视频嵌入解决方案比文本或图像嵌入模型更难实现。OpenAI 或 Bedrock 上似乎没有任何可用的东西。有一些开源模型，但种类比其他嵌入模型少得多。 我看到某些人提出的解决方案是从视频中采样帧，并从这些帧中生成图像嵌入。但是，我担心我会丢失音频中的大量上下文以及视频中发生的任何动作。 从我的研究来看，在这个领域似乎走得最远的公司是 Twelve Labs。看起来他们在语义视频搜索方面有一套非常丰富的工具，但我理想情况下会在自己的矢量数据库中使用嵌入，该数据库还支持全文搜索和混合搜索。 Twelve Labs 有一个嵌入 API，但它目前处于私人测试阶段，所以我不太确定下一步该怎么做。有人对如何解决这个问题有什么建议或见解吗？    提交人    /u/PalpablePatsy247   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/</guid>
      <pubDate>Fri, 25 Oct 2024 21:11:30 GMT</pubDate>
    </item>
    <item>
      <title>一年的同行评审[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbzxbf/one_year_of_peer_review_d/</link>
      <description><![CDATA[我的稿件已在 IEEE 上等待同行评审 10 个半月。上次我联系期刊询问情况时，他们告诉我正在寻找审稿人。距离上次发送邮件已经过去两个月了。期刊没有回复我。稿件仍在同行评审中。我的问题是，评审这么久是正常的吗？这是我的论文将被接受的好兆头吗？如果相反，他们会毫不犹豫地直接拒绝它吗？还是说评审这么久然后最终拒绝是正常的？这篇论文是关于自然语言处理的     提交人    /u/Distinct_Earth_2542   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbzxbf/one_year_of_peer_review_d/</guid>
      <pubDate>Fri, 25 Oct 2024 17:35:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 通过 TEE + 联邦学习加速机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/</link>
      <description><![CDATA[https://www.hcinnovationgroup.com/clinical-it/learning-health-systems-research/news/55130702/dana-farber-researchers-address-oncology-data-sharing-issues 有人看到这个吗？研究负责人将其描述为“即插即用”。如果是真的，那就太好了。 我已经看到很多来自 Goog、MS、Intel 关于安全 ML 的 TEE/enclaves 的讨论，但这是我见过的第一次部署，而且他们也在使用联邦学习。    提交人    /u/thebiztechguy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/</guid>
      <pubDate>Fri, 25 Oct 2024 16:06:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 打破内存障碍：近乎无限的批量大小缩放以实现对比损失</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbvapp/r_breaking_the_memory_barrier_near_infinite_batch/</link>
      <description><![CDATA[摘要 对比损失是一种强大的表征学习方法，其中较大的批量大小通过提供更多负样本来更好地区分相似和不相似数据，从而提高性能。然而，批量大小的扩展受到 GPU 内存消耗的二次增长的限制，这主要是由于相似性矩阵的完全实例化。为了解决这个问题，我们提出了一种基于图块的计算策略，将对比损失计算划分为任意小块，避免相似性矩阵的完全实现。此外，我们引入了一种多级平铺策略来利用分布式系统的分层结构，在 GPU 级别采用基于环的通信来优化同步，并在 CUDA 核心级别采用融合内核来减少 I/O 开销。实验结果表明，所提出的方法将批量大小扩展到前所未有的水平。例如，它能够使用 8 或 32 个 A800 80GB 对批处理大小为 4M 或 12M 的 CLIP-ViT-L/14 模型进行对比训练，而不会牺牲任何准确性。与 SOTA 内存效率高的解决方案相比，它在保持相当速度的同时实现了两个数量级的内存减少。代码将公开发布。    提交人    /u/RajonRondoIsTurtle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbvapp/r_breaking_the_memory_barrier_near_infinite_batch/</guid>
      <pubDate>Fri, 25 Oct 2024 14:16:28 GMT</pubDate>
    </item>
    <item>
      <title>道德问题与谷歌[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/</link>
      <description><![CDATA[如果这里不适合讨论 ML 的这个方面，我深表歉意，但这似乎并不违反规则。 我最近参加了一项 Alphabet 人类数据研究，该研究用于评估 AI 代理和模型。 如果不进一步了解细节，这项研究的结构在伦理上非常令人怀疑。协议中说，如果有任何问题，请联系人类行为研究伦理委员会 HuBREC。 但是，协议中提供的电子邮件 hubrec@google.com 并不存在，我除了查阅过去的学术演讲和冷不丁地给人们发电子邮件外，根本没有任何联系点。 我在寻找下一步时遇到了很多困难，因为除了那封电子邮件之外，我没有其他可用的联系信息。我确实知道谷歌最近解雇了人工智能伦理研究人员，而且这个话题似乎从未被认真对待过。对于一项正在进行的研究来说，将您指向一个似乎不存在的委员会似乎不太好。    提交人    /u/chaneg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/</guid>
      <pubDate>Thu, 24 Oct 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 具有客观先验的完全贝叶斯逻辑回归</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/</guid>
      <pubDate>Thu, 24 Oct 2024 18:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 谷歌如何克服医疗 AI 的训练数据问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/</link>
      <description><![CDATA[TLDR；他们将 3D 图像转换为矢量嵌入，节省了预处理时间并减少了训练数据大小。 仅在美国，每年就有超过 7000 万次计算机断层扫描检查，但这些数据对 Google 的训练无效。 Google Research 有用于放射学、数字病理学和皮肤病学的嵌入 API - 但所有这些都仅限于 2D 成像。医生通常依靠 3D 成像进行更复杂的诊断。 为什么？ CT 扫描具有 3D 结构，这意味着文件大小更大，并且需要比 2D 图像更多的数据。 浏览工程博客，他们刚刚发布了一些最终可以处理 3D 医疗数据的东西。它被称为 CT Foundation - 它将 CT 扫描转换为小而信息丰富的嵌入，以廉价的方式训练 AI 如何做到？ 检查以标准医学成像格式 (DICOM) 进行，并转换为具有 1,408 个值的向量 - 捕获的关键细节包括器官、组织和异常。 然后可以使用这些简洁的嵌入来训练 AI 模型，例如逻辑回归或多层感知器，与拍摄 3D 图像并需要预处理的典型模型相比，使用的数据要少得多。最终的分类器更小，从而降低了计算成本，因此训练更高效、更实惠。 最终结果？ CT Foundation 在七项分类任务中评估了数据效率： - 颅内出血 - 胸部和心脏钙化 - 肺癌预测 - 可疑腹部病变 - 肾结石 - 腹主动脉瘤，以及 - 身体部位 尽管训练数据有限，但这些模型在除一项更具挑战性的任务外的所有任务上都实现了超过 0.8 的 AUC，这意味着强大的预测性能和准确性。 该模型使用 1,408 维嵌入，只需要一个 CPU 进行训练，所有这些都在 Colab Python 笔记本中完成。 TLDR; Google Research 推出了一款工具，可有效地在 3D CT 扫描上训练 AI，方法是将它们转换为紧凑1,408 维嵌入可实现高效的模型训练。它被称为 CT Foundation，需要的数据和处理更少，在七个分类任务中实现了超过 0.8 的 AUC，以最少的计算资源展示了强大的预测性能。 有一个 colab 笔记本可用。 PS：通过从事个人项目来跟上技术发展，我学到了这一点 - 如果您想了解更多信息，请查看techtok today    提交人    /u/TechTok_Newsletter   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/</guid>
      <pubDate>Thu, 24 Oct 2024 17:11:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</guid>
      <pubDate>Sun, 20 Oct 2024 15:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>