<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 21 Oct 2024 15:18:38 GMT</lastBuildDate>
    <item>
      <title>[D] ICLR 2024 焦点中的潜在抄袭：Shengjie Luo 和 Tianlang Chen 的“Gaunt Tensor Products”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</guid>
      <pubDate>Mon, 21 Oct 2024 14:52:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] RWKV-7：无需注意，超越强大的 Modded-GPT 基线（使用 Muon 优化器的基线），同时仅使用 headsz 64</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</link>
      <description><![CDATA[      大家好。 RWKV-7（100% RNN 且无注意力）可以超越强大的 Modded-GPT 基线（带有 Muon 优化器的基线，目前在推特上流行）。 训练代码和日志：https://github.com/BlinkDL/modded-nanogpt-rwkv 如果使用更大的 headsz，它可以达到损失 3.26xx。 但是我当前的实现效率很低。优化后，可能可以达到 ctx1k 下 Modded-GPT 速度的 85%（或比 ctx4k 下 Modded-GPT 更快）。欢迎任何帮助:) https://preview.redd.it/48m3lsvkb4wd1.png?width=873&amp;format=png&amp;auto=webp&amp;s=647d86ed47d40a4f742ed9512a835dee41069e4f  强大的 GPT 基线： https://preview.redd.it/h2ckr31mb4wd1.png?width=584&amp;format=png&amp;auto=webp&amp;s=b667bfbc50298f8335a889b85c55f68ee8db38a5  RWKV-7 摆脱了“线性注意力”设计以实现更高的性能：） https://preview.redd.it/ijyz0sgnb4wd1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=f413d0e7bcd3a76c5e788f2ca231a37706b24345    提交人    /u/bo_peng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</guid>
      <pubDate>Mon, 21 Oct 2024 14:18:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 实时钢琴合成？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8li4p/d_realtime_piano_synthesis/</link>
      <description><![CDATA[有一家公司在这方面做得非常好，据我所知，他们不使用神经网络（尽管他们可以，只是没有说是什么） - Modartt 及其产品 Pianoteq：  Pianoteq 优于其他虚拟乐器的原因在于，这些乐器是物理建模的，因此可以模拟真实声学乐器的可玩性和复杂行为。由于没有样本，文件大小只是其他虚拟乐器所需文件大小的一小部分，使 Pianoteq 成为任何笔记本电脑的完美选择。  我拥有专业版，在我看来，就音质和弹奏时的“感觉”而言，他们在这个领域确实没有竞争对手。它基本上将我的廉价 midi 键盘变成了 Steinway &amp; Sons 钢琴。 任务基本上是从实时 MIDI 消息转换为钢琴声音。我不知道 Modartt 在幕后做什么（很可能使用高级物理建模算法），但我想知道它可以在多大程度上用 NN 复制？ 有一篇关于这个特定主题的有趣论文 DDSP-Piano。然而，我似乎没有找到任何可以使其成为可用的东西的衍生作品。我发现使用 NN 的唯一真正尝试是 PianoForte - 但它还没有真正实现。 我觉得这个领域尚未得到充分探索。我们有音频生成模型，但似乎没有任何模型专注于音乐家的实时合成，而音乐家在演奏时需要立即响应，因此延迟非常重要。    提交人    /u/kiockete   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8li4p/d_realtime_piano_synthesis/</guid>
      <pubDate>Mon, 21 Oct 2024 09:22:39 GMT</pubDate>
    </item>
    <item>
      <title>[研究] AAAI 第二阶段审稿人何时需要提交审稿意见？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/</link>
      <description><![CDATA[我有一个关于 AAAI 第 2 阶段审查流程的快速问题。具体来说，审稿人什么时候需要提交他们的评论？ 我正在考虑将我的论文提交给 arXiv，但我听说有人担心一些审稿人可能会对特定的研究小组或首次发表论文的作者有偏见。我想确保我的论文被接受的机会不会受到任何潜在偏见的负面影响。一旦我将论文上传到 arXiv，我明白我的匿名性就会丧失，这加剧了我的担忧。 任何关于这个主题的见解或建议都将不胜感激！提前致谢！    提交人    /u/morphinejunkie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/</guid>
      <pubDate>Mon, 21 Oct 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于推理的高效 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</link>
      <description><![CDATA[我正在使用高分辨率图像进行物体检测项目。 有没有什么技术可以使训练有素的 CNN（UNet）在推理过程中更有效率？我知道修剪就是这样一种技术，但它有损失准确性和可并行性的风险。    提交人    /u/_My__Real_Name_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</guid>
      <pubDate>Mon, 21 Oct 2024 08:20:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] AquaVoice 风格文本版本模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/</link>
      <description><![CDATA[不知道为什么这个想法（很酷）从来没有流行起来，但我想知道我们是否可以为其构建一个开源模型，例如一个经过微调的 LLM，也许有一个小模型，试图区分用户何时提供“文本值”和何时说出“编辑命令”，然后进行编辑 “基本原型”不应该太难，但可能会很有帮助 https://withaqua.com/    提交人    /u/oulipo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/</guid>
      <pubDate>Mon, 21 Oct 2024 07:22:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] Google Shopping 10M 数据集，用于大规模多模式产品检索和排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</link>
      <description><![CDATA[我们终于在 Hugging Face 上发布了 Marqo Google Shopping 1000 万数据集 (Marqo-GS-10M)。这是用于多模式产品检索的最大、最丰富的数据集之一！  1000 万行查询、产品标题、图片和排名 (1-100) ~100k 个唯一查询 ~500 万个时尚和家居领域的唯一产品 反映了真实世界的数据和用例，并可作为方法开发的良好基准 适当的数据拆分、域内、新查询、新文档以及新文档和新查询。   该数据集为每个查询-文档对提供了详细的相关性分数，以方便将来的研究和评估。 !pip install datasets from datasets import load_dataset ds = load_dataset(&quot;Marqo/marqo-GS-10M&quot;)  我们将这个大规模数据集作为我们训练框架发布的一部分进行策划：广义对比学习 (GCL)。  数据集：https://huggingface.co/datasets/Marqo/marqo-GS-10M GCL：https://github.com/marqo-ai/GCL 论文：https://arxiv.org/abs/2404.08535    提交人    /u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</guid>
      <pubDate>Sun, 20 Oct 2024 21:51:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] 未使用 xLSTM 隐藏状态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/</link>
      <description><![CDATA[大家好，我正在阅读 xLSTM 论文 https://arxiv.org/pdf/2405.04517，特别是关于 mLSTM 的部分，我想知道隐藏状态用在哪里？它的实用性是什么？通常它用于计算输出门。 https://preview.redd.it/7m51jnjhdzvd1.png?width=939&amp;format=png&amp;auto=webp&amp;s=fc84cdcaac47110af22a86b86ff5390ef4e53a37    提交人    /u/splashhhhhhhhhhhh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/</guid>
      <pubDate>Sun, 20 Oct 2024 21:40:48 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 现在我有一份工程师的工作，我如何才能了解最新的有趣的论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</link>
      <description><![CDATA[大家好，我以前在实验室工作，研究计算机视觉和机器学习。通过与教授和博士交谈，我可以了解到一些有趣的新文章。现在我在一家大公司工作，我不再有这个网络，也没有时间花几个小时搜索有趣的新文章。有没有什么好的资源可以汇总与机器学习和计算机视觉相关的精彩文章？    提交人    /u/Fugius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</guid>
      <pubDate>Sun, 20 Oct 2024 21:07:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</guid>
      <pubDate>Sun, 20 Oct 2024 15:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 上周医学 AI：顶级 LLM 研究论文/模型（10 月 12 日至 10 月 19 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/</link>
      <description><![CDATA[      医学 AI 上周：顶级 LLM 研究论文/模型（10 月 12 日至 10 月 19 日） 医学 LLM &amp;其他模型  OLAPH：事实生物医学 LLM 问答  本文介绍了 MedLFQA，这是一个基准数据集，用于评估医学领域大型语言模型 (LLM) 生成的长答案的真实性。  LLMD：解释纵向医疗记录  本文介绍了 LLMD，这是一种旨在分析患者病史的大型语言模型。  LifeGPT：细胞生成变压器  本文介绍了 LifeGPT，这是一种仅解码器的生成预训练变压器 (GPT) 模型，经过训练可在环形网格上模拟康威生命游戏，而无需事先了解网格大小或边界条件。  MedCare：解耦临床 LLM对齐  本文介绍了 MedCare，这是一门医学法学硕士课程，它利用渐进式微调流程来解决医学 NLP 中知识密集型和需要对齐的任务。  Y-Mol：用于药物开发的生物医学法学硕士课程  本文介绍了 Y-Mol，这是一种多尺度生物医学知识引导的大型语言模型 (LLM)，专为药物开发任务而设计，涵盖先导化合物发现、临床前和临床预测。   框架和方法：  MedINST：生物医学指令元数据集 通过语言专家实现医学法学硕士的民主化 MCQG-SRefine：迭代问题生成 自适应医学语言代理 MeNTi：具有嵌套工具的医学法学硕士  医学法学硕士应用：  AGENTiGraph：具有私人数据的法学硕士聊天机器人 MMed-RAG：多模式医学 RAG 系统 Medical Graph RAG：通过检索实现安全法学硕士 MedAide：多代理医学法学硕士协作 合成临床试验生成  医学法学硕士和基准测试：  WorldMedQA-V：多模态医学 LLM 数据集 HEALTH-PARIKSHA：RAG 模型评估 医学视觉语言的合成数据 ....  ... 完整线程详细信息：https://x.com/OpenlifesciAI/status/1847686504837202263    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/</guid>
      <pubDate>Sun, 20 Oct 2024 13:42:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 多臂老虎机的未来？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7vss8/d_future_of_multiarmed_bandits/</link>
      <description><![CDATA[我刚开始MAB的研究，原来的研究兴趣在Deep-RL或者Optimization，很遗憾发现DRL/Opt和MAB交集很少（错了请指正），甚至可以说MAB是一种“孤立”的研究领域，既不能产生有趣的东西（比如transformers-&gt;GPT），也不能给你一个有前途的职业前景（比如对冲基金青睐的opt）。最显著的应用是推荐系统，但除了Netflix，我没看到其他公司会招人，也不适合量化路径。 MAB研究是不是要死了？如果毕业后我对学术不再感兴趣，只想做量化分析师，我是不是应该退出MAB？或者MAB和其他方向有交集，可以成为一台造纸机吗？    submitted by    /u/petrichorinforest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7vss8/d_future_of_multiarmed_bandits/</guid>
      <pubDate>Sun, 20 Oct 2024 10:35:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何用一张图推翻整篇论文的可信度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7r3hn/d_how_to_discredit_your_whole_paper_in_one_figure/</link>
      <description><![CDATA[arxiv.org/abs/2410.13854 他们真的只是将英语模因与中国传统绘画进行比较，并以此作为“中国图像更难理解”的基础（图 1）吗？ 编辑：我认为论文的其余部分是有道理的，理解中国传统艺术所必需的文化背景很重要，但这种比较是不诚实的。他们在附录 B（图 7）中给出的例子更好。    提交人    /u/qu3tzalify   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7r3hn/d_how_to_discredit_your_whole_paper_in_one_figure/</guid>
      <pubDate>Sun, 20 Oct 2024 04:46:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么美国的博士生看起来像是强大的终极 Boss</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/</link>
      <description><![CDATA[您好， 我是欧洲一所大学的博士生，研究方向为 AI/ML/CV 等。我的博士学位是 4 年。第一年，我几乎只是学习如何进行实际研究，教了一门课程来学习事物的工作原理等。第二年，我以合著者的身份在 CVPR 上发表了我的第一篇出版物。到第三年，我可以管理研究项目，我了解如何申请资助，资金如何运作，所有这些的政治问题等。我在我的简历中添加了 2 篇出版物、一本期刊和另一场会议作为第一作者。我非常参与行业，并且还为与我的实验室签订合同的公司编写了大量有关 AI、系统架构、后端、云、部署等方面的生产级代码。 问题是，当我看到美国与我相似的博士生时，他们有 10 篇出版物，其中 5 篇是第一作者，所有这些出版物都是 CVPR、ICML、ICLR、NeurIPS ......等等。我不明白，这些人不睡觉吗？他们如何能够完成如此大量的工作，并且每年仍在 A* 期刊上发表 3 篇出版物？ 我不认为这些人比我聪明，通常我会有想法，然后查找是否存在某些东西，我可以看到一些东西刚刚由斯坦福大学或 DeepMind 等的一些博士生在 1 个月前发表，所以我可以看到我的推理在 SOTA 方面并不晚。但是你需要掌握的概念，仅仅拥有其中一种出版物 + 你需要投入的精力和时间以及完成所有事情的资源，对于 2 到 3 个月的项目来说是不可能的。这些人怎么可能做到这一点？ 谢谢 !    提交人    /u/SilenceForLife   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/</guid>
      <pubDate>Sat, 19 Oct 2024 17:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>