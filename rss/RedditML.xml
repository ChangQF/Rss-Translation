<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Thu, 07 Mar 2024 09:12:04 GMT</lastBuildDate>
    <item>
      <title>[R] 微软研究院推出 NaturalSpeech 3，这是零样本文本转语音技术的重大进步。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</link>
      <description><![CDATA[      论文链接：https://arxiv.org/abs/2403.03100 演示链接：https://speechresearch.github.io/naturalspeech3/&quot;&gt;https:// /speechresearch.github.io/naturalspeech3/ ​ 基于 NaturalSpeech 系列的成功，NaturalSpeech 3 不仅继承了高质量的合成功能而且还通过分解语音属性来进一步推进，从而实现更详细和受控的合成过程。 ​ NaturalSpeech 3 的主要亮点包括： 1.因子化编解码器：具有因子化矢量量化的神经编解码器能够熟练地将语音分解为不同的子空间，从而有针对性地改进语音生成。 2.因子化扩散模型：因子化扩散模型旨在生成语音属性与相应的提示完全一致。这种创新方法使 NaturalSpeech 3 不仅可以合成类似人类的语音，还可以调整韵律和音色的细微差别，以匹配说话者的情感和风格。 3. 可扩展性：可扩展至 10 亿个参数经过超过 20 万小时的数据训练，NaturalSpeech 3 在提高语音质量和清晰度方面显示出了可喜的成果。未来，NaturalSpeech 3 计划进一步扩大规模，以实现更精细的结果。 ​ 深入研究演示、阅读论文，了解 NaturalSpeech 3 的用途为零样本语音合成设定新标准。 https://preview.redd.it/gbgau8vwkvmc1.png?width=1982&amp;format=png&amp;auto=webp&amp;s=3260d6ac03b42e6059c5e0a58169d880b037 b00f ​   由   提交/u/Front-Article-7366   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</guid>
      <pubDate>Thu, 07 Mar 2024 08:47:57 GMT</pubDate>
    </item>
    <item>
      <title>硬编码 GPT [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8oj61/hardcoding_a_gpt_p/</link>
      <description><![CDATA[所以我想到了这个项目想法，  我将使用 Transformers 提供的开源模型库。 对 pdf 文件等文档进行训练。 将数据的矢量化形式存储在 chromadb（或其他一些开源矢量数据库，欢迎建议）中 使用这些数据来微调像小美洲驼这样的小型 LLM。 让它充当 GPT，然后将其 Dockerise 供其他人使用。  就是全部这是可能的，如果有的话，你们中有人以前做过类似的事情吗？   由   提交/u/That_Office9734   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8oj61/hardcoding_a_gpt_p/</guid>
      <pubDate>Thu, 07 Mar 2024 07:21:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么最新、最伟大的法学硕士仍然为生成十个以 apple 结尾的句子这样的小事而苦苦挣扎？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</link>
      <description><![CDATA[所有 3 个模型（Gemini Advanced、Claude 3.0 Opus、GPT-4）都失败了，gpt-4 表现最好，十有八九以苹果结尾。    由   提交 /u/ccooddeerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</guid>
      <pubDate>Thu, 07 Mar 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] Apollo：轻量级多语言医学法学硕士，将医疗人工智能普及到 6B 人群</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</link>
      <description><![CDATA[    &lt; /a&gt;  我们开源了一系列SOTA轻量级多语言医疗LLM Apollo (0.5B, 1.8B, 2B, 6B, 7B)，利用非翻译语料库取得最佳新表现 覆盖英文、中文、法语、西班牙语、阿拉伯语和印地语  整个过程开源且可复制 精简版模型可以是用于提高大型模型的多语言医疗能力无需以代理调整方式进行微调   github：https://github.com/FreedomIntelligence/Apollo 演示：https://apollo.llmzoo.com/#/ 论文：https ://arxiv.org/abs/2403.03640 模型：https://huggingface.co /FreedomIntelligence/Apollo-7B  https://preview.redd.it/29kjdct4oumc1.png?width=1488&amp;format=png&amp;auto=webp&amp;s=1a16bbbf2588fb071ba2af5a50668ca8335c 92b7 https://preview.redd.it/406m28t4oumc1.png?width=1120&amp; ;format=png&amp;auto=webp&amp;s=607664035b62aa0ee726d3f5b4c4730863823bcb https://preview.redd.it/jiewd5t4oumc1.png?width=1242&amp;format=png&amp;auto=webp&amp;s=e059d49da4e788729b134b0d64415bcf 85bb024c    由   提交 /u/Pasu06   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</guid>
      <pubDate>Thu, 07 Mar 2024 05:33:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia Tesla P40 与 Mangio-RVC-Fork 配合良好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</link>
      <description><![CDATA[正在寻找一种经济高效的方式来训练语音模型，在 eBay 上以 150 美元左右的价格购买了一台二手 Nvidia Tesla P40 和一个 3D 打印冷却器我交叉手指。系统只是我的一台带有 B250 Gaming K4 主板的旧电脑，没什么特别的，在 Windows 10 上运行得很好，并且在 Mangio-RVC-Fork 上以惊人的速度进行训练。它有 24GB 的 vram，因此您可以加载大型数据集并提高批量大小。使用默认设置，我在 rmvpe 上用 35 分钟的数据集训练了一个语音，批量大小为 12（仅使用大约 10GB 的 vram），纪元时间约为 1 分 30 秒。结合较大的 vram 净空，我认为更多的人应该尝试 RVC！性价比无与伦比！   由   提交/u/Remote_Hunt516  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</guid>
      <pubDate>Thu, 07 Mar 2024 03:33:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 使用矢量数据库比较天气事件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8h6a7/d_p_compare_weather_events_using_vector_database/</link>
      <description><![CDATA[我正在努力寻找类似的风暴，为此我使用动态时间扭曲来计算阵风和降水率等参数的距离。我想知道是否有更有效的方法来将天气预报与过去的风暴进行比较。我可以制作风暴的矢量数据库，嵌入它们，矢量化它们，然后根据距离计算相似度吗？如果可以，该怎么做。我可以从哪里开始？    由   提交/u/Swimming_Expert5750   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8h6a7/d_p_compare_weather_events_using_vector_database/</guid>
      <pubDate>Thu, 07 Mar 2024 01:10:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我通过蒙特卡罗树搜索得到了不同的结果，我希望有人能提供一些线索。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8g7ys/d_im_getting_different_results_with_monte_carlo/</link>
      <description><![CDATA[嗨， 我正在为 Flesh and Blood 创建一个 MCTS 实现，它是一张交易卡。它似乎运行良好，即相当快。然而，我最近观察到一些奇怪的事情。我认为这是一个错误，但我想我应该联系一下，以防万一 MCTS 是如何工作的，如果是这样，最好有一个解释。 我可以选择以两种模式运行 MCTS：高效模式和低效模式。例如，考虑以下操作流程：开始游戏、开始回合、抽牌、开始阶段、[玩牌 1、玩牌 2]。 低效模式会将每个操作映射到一个节点：node1 ：开始游戏-&gt; StartTurn，节点2：StartTurn -&gt;抽卡，.... 高效模式将所有单个操作折叠为一个操作，因此“开始游戏”到“开始阶段”都将被执行，我们将拥有：node1：开始游戏-&gt; [Play Card 1、Play Card 2] 折叠它们的好处是我们不需要继续复制游戏状态。 但是，我看到的是高效模式返回的获胜次数几乎是低效模式的两倍。这是可能的，因为 MCTS 的工作方式，还是我怀疑它是一个错误。   由   提交/u/Annual_Asspiration_21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8g7ys/d_im_getting_different_results_with_monte_carlo/</guid>
      <pubDate>Thu, 07 Mar 2024 00:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] Azure GPU 限制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/</link>
      <description><![CDATA[刚刚请求增加 a100 图像的配额，他们说 GPU 需求太高。想知道其他人是否遇到过这个问题，如果是的话，您是如何解决的？租用 GPU 应该不会这么困难......    由   提交 /u/Primary-Track8298    reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/</guid>
      <pubDate>Wed, 06 Mar 2024 23:05:01 GMT</pubDate>
    </item>
    <item>
      <title>非文本数据的 LLM 微调 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8dp7t/finetuning_llm_on_nontext_data_discussion/</link>
      <description><![CDATA[嗨， 是否可以在非文本数据上训练大型语言模型。我可以将其应用于任何顺序数据集（例如音符）吗？我认为棘手的部分是对数据集进行标记，以便法学硕士可以更好地理解数据的底层结构。如果预训练的 LLMS 不是正确的方法，您能否建议任何其他预训练的模型？我试图解决的问题需要预测离散值，因此我认为使用音频生成模型没有意义。我不喜欢从头开始训练，所以只是想知道。另外，如果我的直觉不对，请告诉我。 谢谢！   由   提交 /u/Dunkeyfunkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8dp7t/finetuning_llm_on_nontext_data_discussion/</guid>
      <pubDate>Wed, 06 Mar 2024 22:43:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 NLP 分类数据集中查找潜在不良标签的技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8adyu/d_techniques_for_finding_potentially_bad_labels/</link>
      <description><![CDATA[我正在寻找在 NLP 分类数据集中查找潜在不良标签的技术。我们一直在使用 Cleanlab（自信学习），但我们发现对于我们的用例（内容审核/媒体监控）来说，精确度/召回率不是很高。您有其他有趣的技术/论文的指导吗？ GPT-4 等人。当您花费足够的时间编写描述性提示时，它是一个不错的选择，但它的代价是规模庞大。对更智能的“预选”感到好奇技术而不是把一切都扔给 GPT？    由   提交 /u/IbrahimSharaf   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8adyu/d_techniques_for_finding_potentially_bad_labels/</guid>
      <pubDate>Wed, 06 Mar 2024 20:32:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 逆转诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</link>
      <description><![CDATA[https://arxiv.org/pdf/2309.12288 .pdf 原来我预测了 2021 年的逆转诅咒哈哈 https://www.reddit.com/r/MachineLearning/comments/p13ean/d_can_gpt_generalize_in_both_directions/ 编辑：第一篇论文引用的另一篇论文甚至使用了非常相似的示例： https://arxiv.org/pdf/2308.03296.pdf &lt; blockquote&gt; 美国第一任总统是乔治·华盛顿  如果我的帖子以任何方式启发作者，我会非常高兴 &lt; !-- SC_ON --&gt;  由   提交 /u/DeMorrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</guid>
      <pubDate>Wed, 06 Mar 2024 18:16:43 GMT</pubDate>
    </item>
    <item>
      <title>[D][R]强化学习的最新进展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b81pkt/drrecent_developments_in_reinforcement_learning/</link>
      <description><![CDATA[我正在尝试进入强化学习领域，并且刚刚完成了 Sutton 和 Barto 的课程以及 YT 的一门课程。只是想知道目前在这个主题上正在做什么（调查论文/书会很好）。还想知道常用的数据集类型。我学习的课程本质上是完全理论性的，所以我也想知道目前这个领域使用什么工具包。   由   提交/u/ANI_phy  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b81pkt/drrecent_developments_in_reinforcement_learning/</guid>
      <pubDate>Wed, 06 Mar 2024 14:56:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]为什么 Hugging Face 没有成为桌面上最有前途（且年轻）的 AI 聊天机器人玩家之一（如 Mistral AI、Anthropic、Perplexity AI 等）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7t35y/dwhy_isnt_hugging_face_becoming_one_of_the/</link>
      <description><![CDATA[我记得几年前人们讨论HF的商业模式是什么或者如何盈利。 我认为现在是对他们来说这是最好的时代，但我有点惊讶他们没有创造自己的时代。 他们有才华、经验和资源。只是想知道。   由   提交 /u/xiikjuy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7t35y/dwhy_isnt_hugging_face_becoming_one_of_the/</guid>
      <pubDate>Wed, 06 Mar 2024 06:41:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2023年300+ML比赛分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b79bqq/r_analysis_of_300_ml_competitions_in_2023/</link>
      <description><![CDATA[      我运行 mlcontests.com，这是一个网站列出了跨多个平台的 ML 竞赛，包括 Kaggle/DrivenData/AIcrowd/CodaLab/Zindi/EvalAI/… 我刚刚完成了对 300 多个 ML 竞赛的详细分析2023 年，包括查看其中 65 个获奖解决方案。 一些亮点：  正如预期的那样，几乎所有获奖者都使用 Python 。一名获胜者使用 C++ 解决性能至关重要的优化问题，另一名获胜者使用 R 进行时间序列预测竞赛。 92% 的深度学习解决方案使用 PyTorch。我们发现剩下的 8% 使用了 TensorFlow，并且所有这些都使用了更高级别的 Keras API。大约 20% 的获胜 PyTorch 解决方案使用 PyTorch Lightning。 基于 CNN 的模型比基于 Transformer 的模型赢得更多计算机视觉竞赛。 在 NLP 领域，毫不奇怪，生成式法学硕士开始被使用。一些竞赛获胜者使用它们来生成用于训练的合成数据，其他人则提出了创造性的解决方案，例如向开放权重法学硕士添加分类头并对其进行微调。还有更多专门针对 LLM 微调的竞赛正在推出。 与去年一样，梯度增强决策树库（LightGBM、XGBoost 和 CatBoost）仍然被广泛使用 由竞赛获胜者评选。 LightGBM 比其他两者稍微流行一些，但差异很小。 计算使用情况差异很大。 NVIDIA GPU 显然很常见；一些获奖者使用了 TPU；我们没有发现任何使用 AMD GPU 的获胜者；有些人仅在 CPU 上训练他们的模型（尤其是时间序列）。一些获奖者通过工作/大学获得了强大的（例如 8x A6000/8x V100）设置，一些获奖者在本地/个人硬件上进行了全面培训，相当多的获奖者使用了云计算。 有相当多的高- 2023 年的概况竞赛（我们详细介绍维苏威火山挑战赛和M6 预测），以及 2024 年即将举办的更多比赛（维苏威火山挑战赛第二阶段、AI 数学奥林匹克、AI 网络挑战赛） )  有关更多详细信息，请查看完整报告：https://mlcontests.com/state-of-competitive-machine-learning-2023?ref=mlc_reddit ​ 获奖者中最常用的一些 Python 软件包&lt; /p&gt; 在我的 r/MachineLearning 帖子中 去年关于 2022 年比赛的相同分析，热门评论之一询问了时间序列预测。 2023 年有几个有趣的时间序列预测竞赛，我设法对它们进行了相当深入的研究。跳至报告的此部分以了解这些内容。 （不同类型的时间序列竞赛的获胜方法有很大差异 - 包括 ARIMA 等统计方法、贝叶斯方法，以及 LightGBM 和深度学习等更现代的 ML 方法。） 我能够花费相当多的时间感谢今年报告的赞助商：Latitude.sh（配备专用 NVIDIA H100/A100/L40s GPU 的云计算提供商）和 Comet（有用的工具），我们花费了大量时间进行研究和撰写用于 ML - 实验跟踪、模型生产监控等）。我不会在这里向您发送垃圾邮件链接，报告底部有更多详细信息！   由   提交 /u/hcarlens   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b79bqq/r_analysis_of_300_ml_competitions_in_2023/</guid>
      <pubDate>Tue, 05 Mar 2024 16:22:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>