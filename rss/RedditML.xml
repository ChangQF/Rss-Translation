<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Thu, 29 Feb 2024 06:17:47 GMT</lastBuildDate>
    <item>
      <title>[R] 如何一步步思考：对思维链推理的机械理解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2tar4/r_how_to_think_stepbystep_a_mechanistic/</link>
      <description><![CDATA[PDF: https://arxiv.org/pdf/2402.18312.pdf  研究结果： 1. 尽管 CoT 生成的不同阶段的推理要求不同，模型的功能组件几乎保持不变。不同的神经算法被实现为类似感应电路的机制的组合。  注意力头在本体相关（或负相关）标记之间执行信息移动。这种信息移动导致了此类令牌对的明显可识别的表示。通常，这种独特的信息运动从第一层开始一直持续到中间。虽然这种现象是零样本发生的，但上下文中的示例施加压力，要求在标记之间快速混合其他特定于任务的信息。 部署多个不同的神经通路来计算答案，这也是并行的。不同的注意力头，尽管具有不同的概率确定性，将答案标记（针对每个 CoT 子任务）写入最后的残差流。 这些并行答案生成路径收集来自不同部分的答案输入的。我们发现，在生成 CoT 时，模型从生成的上下文、问题上下文以及少样本上下文中收集答案标记。这为法学硕士在回答问题时是否真正使用通过 CoT 生成的上下文这一开放性问题提供了强有力的实证答案。 我们观察到法学硕士中间存在功能性裂痕。 （LLaMA-2 7B 情况下的第 16 个解码器块），它标志着残余流内容和注意力头功能的相移。在此裂痕之前，模型主要分配通过预训练记忆的二元关联；它完全开始遵循裂痕之前和之后的背景。这很可能与仅在裂痕之前发生的本体相关性的代币混合直接相关。同样，写答案的头也只有在裂痕之后才会出现。 （错误地）从少数样本中收集答案标记的注意力头也受到模型前半部分的限制。   代码： https://github.com/joykirat18/How-To-Think-Step-by-Step   由   提交 /u/Gaussian_Kernel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2tar4/r_how_to_think_stepbystep_a_mechanistic/</guid>
      <pubDate>Thu, 29 Feb 2024 06:07:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于 RAG 的文本到代码！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2svvi/p_rag_based_text_to_code/</link>
      <description><![CDATA[大家好！！ 我正在开发一个项目，我想从用简单英语编写的测试用例生成代码。我希望代码/脚本与我自己的代码库兼容，因此我希望模型具有我的代码库的一些上下文。为此我正在考虑 RAG 或微调。您认为什么是更好的选择？如果你们可以推荐我关注的任何资源吗？...谢谢   由   提交/u/Delicious_Success303  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2svvi/p_rag_based_text_to_code/</guid>
      <pubDate>Thu, 29 Feb 2024 05:44:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有办法监控输入提示？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2risp/d_is_there_a_way_to_monitor_the_input_prompts/</link>
      <description><![CDATA[我有一组指标来检测输入提示中的毒性和数据泄漏。如果检测到，我什至不希望输入提示到达LLM，因为提示很糟糕，而且浪费钱。但我想记录这些输入提示，显示原始文本以及指标结果。有这样的工具吗？   由   提交/u/Semantics777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2risp/d_is_there_a_way_to_monitor_the_input_prompts/</guid>
      <pubDate>Thu, 29 Feb 2024 04:30:04 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]请推荐中级机器学习课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2qae7/discussionplease_suggest_course_on_intermediate/</link>
      <description><![CDATA[请推荐一些有关中级机器学习课程的付费或免费课程 提前致谢 &lt; !-- SC_ON --&gt;  由   提交 /u/ajihkrish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2qae7/discussionplease_suggest_course_on_intermediate/</guid>
      <pubDate>Thu, 29 Feb 2024 03:26:29 GMT</pubDate>
    </item>
    <item>
      <title>使用Raspberry Pi & YOLO确定不规则物体的重量 [D] [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2pz04/using_raspberry_pi_yolo_to_determine_weights_of/</link>
      <description><![CDATA[我正在树莓派上开发一个系统，该系统使用 YOLO 实例分割模型对不同的食物进行分类并掩盖它们。然后，我想使用检测到的类别来查找每种食物的重量，并将其添加到总计数器中。相机很可能是自上而下的，所以我很好奇找到食物深度的最佳方法是什么。目前，我现在的代码只采用 2d 蒙版，因此它只是直接向下采用对象的蒙版，然后从该蒙版的区域中找到权重。这是不准确的，因为我们缺少深度，我需要系统尽可能准确。 我可以使用哪些可能的低成本但有效的解决方案来查找食物的体积不仅仅是该地区。框架中的一个盘子上可能有多种食物，并且它们都有不同的形状和形状。尺寸。它们很可能是早餐食品，例如炒鸡蛋、土豆泥、法式吐司等。   由   提交 /u/Thin-Addition6686    reddit.com/r/MachineLearning/comments/1b2pz04/using_raspberry_pi_yolo_to_define_weights_of/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2pz04/using_raspberry_pi_yolo_to_determine_weights_of/</guid>
      <pubDate>Thu, 29 Feb 2024 03:11:22 GMT</pubDate>
    </item>
    <item>
      <title>购买具有 24 GB GPU RAM 的在线计算的最佳地点？ [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2pu2n/best_place_to_buy_online_compute_with_24_gb_gpu/</link>
      <description><![CDATA[我想运行这个 google colab 笔记本 https://colab.research.google.com/drive/10r9D04ez1xk0uT5r7dtZsQV8-yNkrHUT#scrollTo=AEygCH7J6l6O 但它需要 24 GB GPU RAM 内存，而 Colab 现在最多提供 15 GB。 我可以购买此计算来运行机器学习实验的最佳平台是什么？   由   提交 /u/WaitAckchyually   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2pu2n/best_place_to_buy_online_compute_with_24_gb_gpu/</guid>
      <pubDate>Thu, 29 Feb 2024 03:04:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在完成了一篇糟糕的硕士论文后，有机会提高我的博士学位简介</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2ozh7/d_chance_to_improve_my_profile_for_phd_after_a/</link>
      <description><![CDATA[我将在两周内完成我的硕士论文，我感觉非常糟糕。由于我在上一个博士申请周期中失败了，所以毕业后我想再尝试一次。然而，这样人们就会更关心我的论文。我知道糟糕的论文可能对博士学位来说是一个很大的危险信号，但是有没有机会保存我的个人资料，例如获得 RA 职位和发表论文？  &amp;# 32；由   提交 /u/thoangzr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2ozh7/d_chance_to_improve_my_profile_for_phd_after_a/</guid>
      <pubDate>Thu, 29 Feb 2024 02:24:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 语音转文本基准：在 RTX3070 Ti 上每 1 美元转录 47,638 分钟（比托管服务成本降低 1000 倍）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2m24h/p_speechtotext_benchmark_47638_mins_transcribed/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2m24h/p_speechtotext_benchmark_47638_mins_transcribed/</guid>
      <pubDate>Thu, 29 Feb 2024 00:09:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] EMO：Emote Portrait Alive - 在弱条件下使用音视频扩散模型生成富有表现力的肖像视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b2ddu6/r_emo_emote_portrait_alive_generating_expressive/</link>
      <description><![CDATA[研究人员一直在努力制作人工智能生成的头部说话视频，以捕捉人类面部表情和言语的细微差别。方法通常无法复制真实人类嘴巴和面部的流动性和同步性。 阿里巴巴的一篇新论文提出了EMO，这是一种人工智能系统，可以在合成头部视频中实现前所未有的真实感使用新颖的扩散模型方法。 EMO 直接从音频剪辑和肖像图像生成视频，无需 3D 图形或动画：  音频编码器分析音调、节奏以生成动作 参考编码器在整个视频中保留视觉特征 时间模块可实现平滑的帧过渡 面膜聚焦面部核心区域的细节，例如嘴巴、眼睛等 速度控制层稳定头部运动的节奏  EMO 在超过 250 小时、涵盖 1.5 亿帧的头部说话视频数据集上进行训练，学习了人类语音的复杂性，如发音、口音和情感影响。 定量评估表明，EMO 在指标方面比 Wav2Lip 和 DreamTalk 等之前最先进的方法有了显着改进，包括：  Fréchet 起始距离：单个帧质量 表达建模：面部动画的生动性 唇形同步：嘴形的视听对齐 Fréchet 视频距离：身份和表情的一致性  虽然有局限性与较慢的生成和伪影一样，EMO 代表了直接从音频复制人类面部动态的重大进步。随着模型规模的扩大，人工智能生成的头像将变得越来越具有表现力和现实性。 论文此处 （github）。 完整摘要。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1b2ddu6/r_emo_emote_portrait_alive_generating_expressive/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b2ddu6/r_emo_emote_portrait_alive_generating_expressive/</guid>
      <pubDate>Wed, 28 Feb 2024 18:29:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] SpeechBrain 工具包 1.0 版已发布！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b28j2o/r_the_speechbrain_toolkit_version_10_is_out/</link>
      <description><![CDATA[亲爱的 Reddit 社区， 三年前，我们在此 Reddit 子版块中宣布了 SpeechBrain 的测试版。今天，我们很高兴地宣布 SpeechBrain 1.0 正式发布。这一里程碑与之前的版本相比有许多增强和进步。 这是真正的社区努力（PyPi 每月 20 万次下载），我们非常高兴和自豪能够领导它。&lt; /strong&gt; 您可以在此处探索我们改进的全面摘要：SpeechBrain 1.0 摘要. 在这些变化中，我们在语音识别方面取得了重大改进，通过与 K2 集成有限状态传感器、CTC 解码、n- 增强了搜索功能。克重评分和法学硕士整合。此外，我们还引入了新型模型，例如 Streamable Conformer Transducers、Branchformers 和 Hyper-conformer 等，以提高性能和速度。您现在还可以轻松使用大语言模型 (LLM) 并使用我们的数据对其进行微调，或者简单地使用它们来重新评分 ASR 假设。 此外，SpeechBrain 现在支持更广泛的任务：&lt;语音、音频、文本和脑电图处理。我们改进了与 HF 模型的集成，使从 HF 导入任何模型变得更加容易。我们实施了现代技术和模型，包括持续学习、扩散模型、超网络、贝叶斯 ASR 等。 我们创建了一个新的基准存储库，其中包含用于自我监督学习 (MP3S) 的有用基准， EEG 处理 (SpeechBrain-MOABB) 和持续学习新语言 (CL-MASR)。 在提供的 Colab 上可以找到更多新奇事物。 敬请期待未来因为我们前面有宏伟的计划。 当然，非常感谢我们的慷慨赞助商 HuggingFace、OVHCloud 和 ViaDialog，以及我们在 Concordia、Avignon、Mila、剑桥大学和三星的合作伙伴，以及所有我们了不起的贡献者！  此致， SpeechBrain 核心团队   由   提交 /u/TParcollet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b28j2o/r_the_speechbrain_toolkit_version_10_is_out/</guid>
      <pubDate>Wed, 28 Feb 2024 15:20:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于用于构建现代 CV 算法的 CNN 块的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b27e4t/d_question_about_cnn_blocks_used_for_building/</link>
      <description><![CDATA[大家好，我希望更好地了解卷积块的使用，例如YOLOv8的瓶颈块、SPPF和C2f块，它们是如何构造的？为什么它们是这样建造的？是什么导致创作者选择使用这种层的顺序？等等。 我认为我对每个 CNN 层的作用（Conv、Pooling、FC）有深入的了解，但这些块的概念暗示了我。 任何解释或资源非常感谢。   由   提交 /u/TheWingedCucumber   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b27e4t/d_question_about_cnn_blocks_used_for_building/</guid>
      <pubDate>Wed, 28 Feb 2024 14:31:05 GMT</pubDate>
    </item>
    <item>
      <title>[D]：为博士申请构建 NLP 研究概况（无先前出版物）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b26roc/d_building_nlp_research_profile_for_phd/</link>
      <description><![CDATA[我最近申请了 CMU 的 LTI NLP 博士项目，但没有被录取。我了解自己的缺点：研究经验有限：硕士CS背景，5年行业经验（软件工程师、数据分析师）。基础NLP项目：硕士课程（高级ML、NLP）和基础项目（情感分析、音乐流派分类）。 没有研究出版物：我承认顶级项目中出版物的重要性。 鉴于申请周期即将到来（申请于 2024 年 9 月开始），我渴望加强自己的个人资料。由于顶级会议已经过了提交截止日期（并不是说我正准备发表我的作品，哈哈），我如何才能最好地利用这段时间成为前 10 名 NLP 博士项目的有竞争力的申请者？我正在寻找关于建立我的研究档案的建议，包括： 独立研究机会：除了会议出版物之外，是否有其他途径获得研究经验？ 技能发展：我应该重点获取哪些具体的 NLP 技能或知识？ 与研究人员建立联系：我如何与 NLP 研究人员建立联系，以获得潜在的研究经验或指导？ 如果来自社区的任何见解或建议，我们将不胜感激！ &lt;!-- SC_ON - -&gt;  由   提交/u/Puzzleheaded_Big_242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b26roc/d_building_nlp_research_profile_for_phd/</guid>
      <pubDate>Wed, 28 Feb 2024 14:02:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 生产级 RAG 应用程序真正包含什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b244vc/d_what_does_a_production_level_rag_application/</link>
      <description><![CDATA[我已经完成了 RAG 的研究，实现了一些简单的 RAG 教程，并且我知道我现在想要实现的高级 RAG 技术。但我真的不知道正确的应用程序工作流程是什么样的。就像在简单的 RAG 教程中实现这 7 行代码，然后添加混合搜索或重新排名等内容，以及实际构建生产级 RAG 应用程序之间有什么区别？这个“工作流程”是什么样的？    由   提交/u/Aggravating-Floor-38   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b244vc/d_what_does_a_production_level_rag_application/</guid>
      <pubDate>Wed, 28 Feb 2024 11:44:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] 1 位 LLM 时代：所有大型语言模型均采用 1.58 位</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b22izk/r_the_era_of_1bit_llms_all_large_language_models/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.17764 摘要  最近的研究，例如 BitNet，正在为 1 位大型语言模型的新时代铺平道路（法学硕士）。在这项工作中，我们引入了一个 1 位 LLM 变体，即 BitNet b1.58，其中 LLM 的每个参数（或权重）都是三元的 {-1, 0, 1}。它在困惑度和最终任务性能方面与具有相同模型大小和训练令牌的全精度（即 FP16 或 BF16）Transformer LLM 相匹配，同时在延迟、内存、吞吐量、和能源消耗。更深刻的是，1.58 位法学硕士定义了新的扩展法则和配方，用于培训高性能且经济高效的新一代法学硕士。此外，它还实现了一种新的计算范例，并为设计针对 1 位 LLM 优化的特定硬件打开了大门。    由   提交/u/Civil_Collection7267   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b22izk/r_the_era_of_1bit_llms_all_large_language_models/</guid>
      <pubDate>Wed, 28 Feb 2024 10:03:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>