<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 30 Jan 2024 15:13:19 GMT</lastBuildDate>
    <item>
      <title>[D] 做了 3 年机器学习，还没有成功。常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeq9pz/d_3_years_doing_ml_no_success_yet_is_it_common/</link>
      <description><![CDATA[我已经从事 ML 研究 1.5 年了，更具体地说是医学成像，之前担任 DL 工程师，负责构建面部识别管道。尽管有很好的理解和我所有的关注，但我还没有为我所研究的所有用例创建一个足够好的系统或模型。  从过去的 4 个月开始，我一直在探索“从嘈杂的标签中学习”，我研究了 3 种技术，花了相当多的时间来集成目标加载器，但结果很差，甚至比基线还要糟糕。此前，曾尝试使用混合自适应算法方案进行系统辨识，但失败了。确实就此写了一份技术报告。  另外，另一方面，我也参加在线比赛。普通的方法让我取得了前 10-20% 的成绩，但当我尝试改进时，我总是失败。尽管我付出了一切努力，但我的方法都效果不佳，非常令人沮丧。  我并不是想建立一个最先进的模型，但至少希望自己能够超越之前的基线或任何有意义的工作。   由   提交 /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeq9pz/d_3_years_doing_ml_no_success_yet_is_it_common/</guid>
      <pubDate>Tue, 30 Jan 2024 14:56:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 没有免费午餐定理和法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeq92s/d_no_free_lunch_theorem_and_llms/</link>
      <description><![CDATA[我有一个可能很愚蠢的问题，但是“没有免费的” “午餐定理”（Wolpert 和 Macready）指出，对于任何模型，针对一类问题的任何性能改进都会被另一类问题的性能所抵消。它还指出，当对所有可能问题的性能进行平均时，任何两个模型都是等效的。 但是法学硕士会发生什么情况呢？如果对所有可能的问题进行平均性能，平均值会高于其他模型吗？  愿意听取意见。   由   提交/u/iamtdb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeq92s/d_no_free_lunch_theorem_and_llms/</guid>
      <pubDate>Tue, 30 Jan 2024 14:55:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调后缺少 config.json</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aepsl4/d_missing_configjson_after_finetuning/</link>
      <description><![CDATA[我已经对丹麦语的 Whisper 媒体模型进行了微调。 但是，它只输出 [adapter_model.safetensors] [adapter_config.json] 你知道为什么会发生这种情况以及如何修复它吗？如果我添加 openai/whisper-medium 中的其余文件，会发生什么情况？它是如何受到影响的？   由   提交 /u/Unalomesie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aepsl4/d_missing_configjson_after_finetuning/</guid>
      <pubDate>Tue, 30 Jan 2024 14:34:16 GMT</pubDate>
    </item>
    <item>
      <title>[2401.15866]随机摊销：加速特征和数据归因的统一方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aep0hz/240115866_stochastic_amortization_a_unified/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aep0hz/240115866_stochastic_amortization_a_unified/</guid>
      <pubDate>Tue, 30 Jan 2024 13:57:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 初始化一个小型 LLM 以反映自然代币分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aenr8k/d_initializing_a_small_llm_to_reflect_natural/</link>
      <description><![CDATA[你好！ ​ 这样设置模型的权重是否可行在任何训练之前，最终 softmax 层的输出是否可以反映训练数据中标记的分布？  我最初的想法是将所有权重和偏差初始化为零，然后通过合并预先计算的观察到的标记概率向量来修改 softmax 层（最初输出零）。到目前为止，我在研究中还没有遇到过这种方法，我很想知道这是否是一个有趣或糟糕的想法？ 提前谢谢您！    由   提交/u/ez613  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aenr8k/d_initializing_a_small_llm_to_reflect_natural/</guid>
      <pubDate>Tue, 30 Jan 2024 12:55:06 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Google Colab 的 AntiPython 编译器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ael7o9/project_antipython_compiler_for_google_colab/</link>
      <description><![CDATA[嘿伙计们， 这是我的业余项目。它是一个编译器，可让您以您喜欢的语言（而不仅仅是 Python）使用 Google Colab。它是开源的。我很想知道你的想法！ GitHub：https://github.com /Fileforma/AntiPython-AI-Compiler-Colab   由   提交 /u/DataBaeBee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ael7o9/project_antipython_compiler_for_google_colab/</guid>
      <pubDate>Tue, 30 Jan 2024 10:18:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 用于包含章节和子章节的文档</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aejsur/d_rag_for_documents_with_chapters_and_subchapters/</link>
      <description><![CDATA[我想为一个 100 页的文档实现 RAG，该文档具有章节、子章节等的层次结构。因此，我将文档分成更小的块段落。在许多情况下，子章节中的块仅在子章节标题的上下文中才有意义，例如（6.1 方法 ABC，6.1.1 缺点）。 我想知道 RAG 中处理分层结构最常用的方法是什么，这在较长的文档中很常见？ &lt; !-- SC_ON --&gt;  由   提交/u/Electronic-Letter592   reddit.com/r/MachineLearning/comments/1aejsur/d_rag_for_documents_with_chapters_and_subchapters/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aejsur/d_rag_for_documents_with_chapters_and_subchapters/</guid>
      <pubDate>Tue, 30 Jan 2024 08:38:01 GMT</pubDate>
    </item>
    <item>
      <title>主题建模[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aejdlm/topic_modelling_p/</link>
      <description><![CDATA[我的目标是创建一个模型，根据票证提到的功能/问题对支持票证进行集群。当使用像 BerTopic 这样的模型时，我得到的结果是一些提到相同问题的票证是不同主题的一部分。有没有不同的方法来解决这个问题？我的目标是根据与他们提到的产品/公司相关的功能/问题对票证进行集群。   由   提交/u/hugh57231   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aejdlm/topic_modelling_p/</guid>
      <pubDate>Tue, 30 Jan 2024 08:07:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 加速注意力研究：融合心理学和机器学习：探索注意力机制中的意志力和兴趣</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeds27/d_accelerating_research_on_attention_blending/</link>
      <description><![CDATA[      摘录：今天，让我们从通常深入研究代码和算法的过程中绕道而行。相反，我想谈谈最近让我大脑发痒的事情：如果我们可以将一些人类心理学（例如意志力和兴趣）混合到机器学习的冷酷逻辑世界中会怎样？如果您希望我继续致力于此，请喜欢这篇文章！ :) https://medium.com/@beastman3b/blending-psychology-and-machine-learning-exploring-willpower-and-interest-in-attention-mechanisms-81ce5d6bdb3d &amp; #x200b; https:// medium.com/@beastman3b/blending-psychology-and-machine-learning-exploring-willpower-and-interest-in-attention-mechanisms-81ce5d6bdb3d 请继续关注，看看效果如何出！   由   提交 /u/Honest-Debate-6863   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeds27/d_accelerating_research_on_attention_blending/</guid>
      <pubDate>Tue, 30 Jan 2024 02:42:34 GMT</pubDate>
    </item>
    <item>
      <title>250 RTX 3080s 能做什么 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aedjxc/what_to_dow_ith_250_rtx_3080s_p/</link>
      <description><![CDATA[您好！我有大约 250 个 RTX 3080，+ 可能有 40 个 RTx 3070，我用于挖矿。他们都拆掉了风扇护罩并安装了风扇。在浸入式冷却液中采矿。长话短说。采矿停止后，事情变得忙碌起来。它们的 GPU 刚刚放在浸没液体中。它们仍然可以工作，并且自从采用液体冷却以来从未变热。  是否有任何公司可以托管浸入式冷却卡，或者有人想要协助代理这些卡或帮助它们设置机器学习？我很乐意将几台 3080 赠送给任何可以用它们实现目标的人！   由   提交/u/death0and0taxes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aedjxc/what_to_dow_ith_250_rtx_3080s_p/</guid>
      <pubDate>Tue, 30 Jan 2024 02:31:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 LLM + RAG 进行 3D 对象搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aec8gb/d_3d_object_search_using_llm_rag/</link>
      <description><![CDATA[为可与自然语言一起使用的 3D 对象制作一个小型搜索引擎，并从中获得了一些乐趣。不需要元数据或标签，索引纯粹是根据几何图形构建的！这使用以下管道进行工作：  对于数据库中的每个对象，我生成 6 个图像，每侧 1 个。 对于每个图像，我使用 gpt4- 进行描述视觉，然后使用 gpt4 将其合成为单个描述 文本描述使用剪辑嵌入并存储在矢量数据库中 对于搜索查询，搜索字符串被嵌入，并且检索到数据库中最接近的（n）个向量。  参见此处：https://x.com/MenyJanos/status/1752104689188135271?s=20   由   提交/u/Janos95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aec8gb/d_3d_object_search_using_llm_rag/</guid>
      <pubDate>Tue, 30 Jan 2024 01:29:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用多个库对 Mixtral-8x7B 进行实验 - 每秒最多获得 52 个令牌。想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeb70z/d_experiments_with_mixtral8x7b_using_multiple/</link>
      <description><![CDATA[      大家好， 最近尝试部署 Mixtral-8x7B模型并希望与感兴趣的人分享主要发现： 最佳性能：使用 Pytorch（每晚）的量化 8 位模型的平均代币生成率为 52.03 代币/秒在 A100 上，平均推理时间为 4.94 秒，冷启动时间为 11.48 秒（在无服务器环境中部署时很重要） 混合实验 测试的其他库： vLLM、AutoGPTQ、HQQ 渴望听到您在类似部署中的经验和学习！   由   提交/u/Tiny_Cut_8440   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeb70z/d_experiments_with_mixtral8x7b_using_multiple/</guid>
      <pubDate>Tue, 30 Jan 2024 00:40:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 之外的法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae5dgq/d_llms_beyond_rag/</link>
      <description><![CDATA[实际上几乎每个人都在谈论 RAG。我想知道接下来会出现什么趋势。很想听听您的想法。   由   提交/u/HolidayCritical3665   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae5dgq/d_llms_beyond_rag/</guid>
      <pubDate>Mon, 29 Jan 2024 20:31:28 GMT</pubDate>
    </item>
    <item>
      <title>佩德罗·多明戈斯：神经象征尚未发挥作用 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae4vp1/pedro_domingos_neurosymbolic_does_not_work_yet_r/</link>
      <description><![CDATA[      ​ https://preview.redd.it/r0h4yab5qffc1.png?width=817&amp;format=png&amp;auto=webp&amp;s=033744120df49 252c5379bdafa429570e80cfac4&lt; /a&gt; ​ 象征性人工智能通常被视为失败。我记得 Cyc 花费了 2 亿美元（比 GPT-4 的培训预算还多？）。 另一方面，Transformer LLM [1] 明显的固有局限性使一些人将目光转向象征性的、神经-再次采用象征性和混合性方法。 DeepMind 首席执行官表示，公司在这个领域有六个项目。 如果你对这些主题（神经网络、符号和神经符号人工智能的理论局限性）感兴趣，我为它们制作了一个 Reddit 子版块： r/symbolic （我可能会后悔这样做，但小众主题需要自己的 subreddits，因为大多数主题都没有关心或了解很多，因此提交的内容会被否决，并且评论通常缺乏洞察力，例如“什么是 ILP？”） ​ &lt; p&gt;[1]例如 https://arxiv.org/abs/2205.11502   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae4vp1/pedro_domingos_neurosymbolic_does_not_work_yet_r/</guid>
      <pubDate>Mon, 29 Jan 2024 20:11:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>