<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 30 Nov 2023 15:12:51 GMT</lastBuildDate>
    <item>
      <title>[D] AAMAS 2024 评论已出炉！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187ilt5/d_aamas_2024_reviews_are_out/</link>
      <description><![CDATA[我没有看到讨论帖子，所以我想我应该制作这个。   由   提交 /u/LessPoliticalAccount   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187ilt5/d_aamas_2024_reviews_are_out/</guid>
      <pubDate>Thu, 30 Nov 2023 13:32:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于序列建模的分层门控循环神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187iaw6/r_hierarchically_gated_recurrent_neural_network/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.04823 代码：https://github.com/OpenNLPLab/HGRN 模型：https://huggingface.co/OpenNLPLab 摘要：  变形金刚已经超越RNN 因其并行训练和长期依赖建模的卓越能力而广受欢迎。最近，人们对使用线性 RNN 进行高效序列建模重新产生了兴趣。这些线性 RNN 通常在线性递归层的输出中采用门控机制，而忽略了在递归中使用遗忘门的重要性。在本文中，我们提出了一种门控线性 RNN 模型，称为分层门控循环神经网络（HGRN），其中包括由可学习值下界的遗忘门。当向上移动层时，下界单调增加。这允许上层对长期依赖关系进行建模，而下层对更多本地的短期依赖关系进行建模。语言建模、图像分类和远程竞技场基准的实验展示了我们提出的模型的效率和有效性。源代码可在 此 https URL 处获取。  https://preview.redd.it/thph9bpmjh3c1.png?width=965&amp;format=png&amp; ;auto=webp&amp;s=8e4871cd280ef7e5b771b463435d47da11dca52d   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187iaw6/r_hierarchically_gated_recurrent_neural_network/</guid>
      <pubDate>Thu, 30 Nov 2023 13:16:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对抗性攻击或示例</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187hd66/d_adversarial_attack_or_example/</link>
      <description><![CDATA[您好，我最近才开始研究对抗性攻击研究，并且我已经确认优化公式最大化了损失函数。那么，这个方程不能针对 DNN 中有限范数约束的 delta 进行最优求解吗？或者我可以知道上限吗？   由   提交/u/Glittering_Hall_2500   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187hd66/d_adversarial_attack_or_example/</guid>
      <pubDate>Thu, 30 Nov 2023 12:26:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 多合一资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187h8cz/d_all_in_one_resource/</link>
      <description><![CDATA[是否有任何资源可以让我实现不同的机器学习模型，如果可能的话，还有简短的介绍   由   提交 /u/Dani_sk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187h8cz/d_all_in_one_resource/</guid>
      <pubDate>Thu, 30 Nov 2023 12:19:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] RO-LLaMA：通过噪声增强和一致性正则化进行放射肿瘤学的全科法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187fy3l/r_rollama_generalist_llm_for_radiation_oncology/</link>
      <description><![CDATA[       由   提交/u/davidmezzetti   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187fy3l/r_rollama_generalist_llm_for_radiation_oncology/</guid>
      <pubDate>Thu, 30 Nov 2023 10:59:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 深度强化学习的泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187f2rn/r_generalization_in_deep_reinforcement_learning/</link>
      <description><![CDATA[深度强化学习中的泛化、鲁棒性和对抗性攻击 https://blogs.ucl.ac.uk/steapp/2023/11/15 /对抗性攻击-深度强化学习中的鲁棒性和泛化/   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187f2rn/r_generalization_in_deep_reinforcement_learning/</guid>
      <pubDate>Thu, 30 Nov 2023 09:59:56 GMT</pubDate>
    </item>
    <item>
      <title>[N] SD 推理稳定快速：比 AITemplate 更快，与 TensorRT 相当</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187dxyh/n_stablefast_for_sd_inference_faster_than/</link>
      <description><![CDATA[      &lt; a href=&quot;https://github.com/hengzeyi/stable-fast&quot;&gt;https://github.com/hengzeyi/stable-fast https://preview.redd.it/en9wfkfc6g3c1.png?width=700&amp;format=png ＆amp; ;auto=webp&amp;s=f66a45e69ab37885c2973c3ce11841a698e05b35 稳定快速在所有种扩散器模型上实现了 SOTA 推理性能。与 &lt; code&gt;TensorRT或AITemplate，编译一个模型需要几十分钟，stable-fast只需要几秒钟编译一个模型。 stable-fast 还支持开箱即用的动态形状、LoRA 和 ControlNet。   由   提交/u/ciiic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187dxyh/n_stablefast_for_sd_inference_faster_than/</guid>
      <pubDate>Thu, 30 Nov 2023 08:40:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用多个库部署 CodeLlama 34Bn 模型的见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187bocu/d_insights_from_deploying_codellama_34bn_model/</link>
      <description><![CDATA[     &lt; td&gt; 大家好， 我们最近尝试部署 CodeLlama 340 亿模型，并希望分享我们的主要发现对于那些感兴趣的人：  最佳性能：使用 vLLM 的量化 GPTQ、4 位 CodeLlama-Python-34B 模型。 结果：我们平台上使用 Nvidia A100 GPU 的平均最低延迟为 3.51 秒，平均令牌生成率为 58.40/秒，冷启动时间为 21.8 秒。  CodeLlama 34Bn   测试的其他库：HuggingFace Transformer Pipeline、AutoGPTQ、文本生成推理。  渴望听到您在类似部署中的经验和学习！   由   提交/u/Tiny_Cut_8440   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187bocu/d_insights_from_deploying_codellama_34bn_model/</guid>
      <pubDate>Thu, 30 Nov 2023 06:13:50 GMT</pubDate>
    </item>
    <item>
      <title>[D]：了解训练大型模型时的 GPU 内存分配</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1878lat/d_understanding_gpu_memory_allocation_when/</link>
      <description><![CDATA[TL;DR：为什么 GPU 内存使用量在梯度更新步骤期间会激增（无法解释 10GBs），然后又下降？ 我一直致力于微调 HuggingFace 上提供的一些较大的 LM（例如 Falcon40B 和 Llama-2-70B），到目前为止，我对内存需求的所有估计都没有相加。我可以使用 4 个 A100-80gb GPU，并且相当有信心我应该有足够的 RAM 来使用 LoRA 微调 Falcon40B，但我不断收到 CUDA OOM 错误。我已经找到了让事情运行的方法，但这让我意识到我并不真正理解训练期间如何分配内存。 以下是我对训练模型时内存去向的理解：  设置 -&gt;定义 TOTAL_MEMORY = 0 (MB)，我将在执行增加内存的每个步骤时更新它。 -&gt;通过“观察”来检查内存使用情况nvidia-smi 每 2 秒刷新一次。 -&gt;模型已加载到fp16 -&gt;使用具有 ~7B 参数的 Falcon7B（类似于 6.9，但足够接近） -&gt;在 jupyter 笔记本中的单个 A100-80gb GPU 上运行 加载模型：  用于 torch 等的 CUDA 内核（在我的机器上）我看到每个 GPU 大约 900mb）。总内存 + 900 -&gt; TOTAL_MEMORY=900 模型权重（废话）。假设您使用 float16 加载了一个 7B 参数模型，那么您将看到 2 字节 * 7B 参数 = 14B 字节。 ~= 14GB GPU VRAM。总内存 + 14_000 -&gt; TOTAL_MEMORY=15_000（舍入）  模型应在单个 GPU 上加载。 训练（我正在模拟单个前向并通过单独运行每个部分来向后一步）  数据。我正在传递一小批虚拟输入（随机整数），因此我假设这不会对内存使用量产生实质性贡献。 前向传递。由于某种原因，内存跳跃了大约 1000mb。也许这是由于缓存的中间激活造成的？虽然我觉得应该更大。 TOTAL_MEMORY + 1_000 -&gt; TOTAL_MEMORY = 16_000。 计算交叉熵损失。损失张量将利用一些内存，但这似乎不是一个非常高的数字，所以我认为它没有贡献。 通过调用 `loss.backwards() 计算相对于参数的梯度`。这会导致显着的内存峰值（增加 15_000 MB）。我想这是为模型中每个参数存储梯度值的结果？ TOTAL_MEMORY + 15_000 -&gt; TOTAL_MEMORY = 30_000 通过调用“optimizer.step()”更新模型参数。这会导致另一个内存峰值，GPU 内存使用量增加超过 38_000MB。不太确定为什么。我最好的猜测是，这是 AdamW 开始为每个参数存储 2 x 动量值的地方。如果我们进行数学计算（假设优化器状态值在 fp16 中）----&gt; 2 个字节 * 2 个状态 * 7B = 28B 字节 ~= 28GB。 TOTAL_MEMORY + 38_000 -&gt; TOTAL_MEMORY = 68_000  LoRA 会通过减少优化器步骤中所需的内存量来减少这个数字，但我尚未对此进行任何测试，因此没有任何数字。 我相信这就是所有主要组件。 那么额外的 10GB 从哪里来呢？也许它是“火炬保留了该内存但实际上并未使用它”之一。因此，我通过检查 `torch.cuda.memory_allocated` 和 `torch.cuda.max_memory_allocated` 的输出进行检查，也许那里有东西。 分配的内存（后退一步后）：53gb &lt; p&gt;分配的最大内存：66gb 这意味着在某些时候，需要额外的 13 GB，但后来被释放了。 我想问你们的问题是，有人知道这些内存在哪里吗？我在数学中没有发现的额外 10GB 来自哪里？向后传递后释放 13GB 会发生什么？是否有任何我错过的需要记忆的额外步骤？  这已经困扰我一段时间了，我希望获得更好的理解，因此我们将非常感谢您提供的任何专家意见、资源或其他建议！ &amp; #x200b; 编辑：我还知道，当您使用“Trainer”类进行训练时，您可以启用梯度检查点，通过在向后传递过程中重新计算一些中间激活来减少内存使用。那么整个过程的哪一部分会减少内存使用量呢？ ​   由   提交 /u/lightSpeedBrick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1878lat/d_understanding_gpu_memory_allocation_when/</guid>
      <pubDate>Thu, 30 Nov 2023 03:27:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何向对强化学习一无所知的人解释为什么强化学习很难？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18742ki/d_how_to_explain_why_rl_is_difficult_to_someone/</link>
      <description><![CDATA[如何向对此一无所知的人解释为什么强化学习很难？ 我一直在从事强化学习项目在上班。给我分配这个问题的人是一位计算机科学家，他不是强化学习方面的专家，但知道这是一个难题。 （我的老板和分配项目给我的人是平等的。我的老板不是计算机科学家，对强化学习一无所知。）这家伙的老板是一位业务经理，对强化学习一无所知，对 ML 知之甚少。业务经理想要我提供一份有关项目进展情况的报告，但我感觉他并不真正理解为什么要花这么长时间。  就上下文而言，我已经在这个项目上工作了大约 4 个月，每周 15 个小时。在那段时间，我从头开始为这个问题构建了整个代码库，并编写了几个模型。我有一个目前大部分有效的方法，但我需要对奖励函数进行一些更改，以使其始终表现良好。我是唯一一个参与这个项目的人，所以所有这些都是我自己完成的。在此之前我也只做过普通强化学习，所以我必须学习大量有关深度强化学习的知识才能完成这项工作。幸运的是，我认识一位深度强化学习（外部工作）方面的专家，他能够给我指点。我感觉我已经取得了很大的进步，并且在拥有一个完全完善的模型方面已经接近冲刺了。然而我感觉这家伙对我不太感兴趣。这个人对我没有任何官方权力，所以这主要是为了解释除了有关该项目的正常幻灯片以及我所处的位置之外，强化学习还有多少工作。 &lt; !-- SC_ON --&gt;  由   提交 /u/savvyms   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18742ki/d_how_to_explain_why_rl_is_difficult_to_someone/</guid>
      <pubDate>Wed, 29 Nov 2023 23:57:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] MMMU：专家 AGI 的大规模多学科多模态理解和推理基准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186y58x/r_mmmu_a_massive_multidiscipline_multimodal/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2311.16502  博客：https://mmmu-benchmark.github.io/  摘要：  我们介绍 MMMU ：一个新的基准，旨在评估需要大学水平学科知识和深思熟虑推理的大规模多学科任务的多模态模型。 MMMU 包含 11,500 个从大学考试、测验和教科书中精心收集的多模态问题，涵盖六个核心学科：设计、商业、科学、健康与医学、人文与科学社会科学、技术与科学工程。这些问题涵盖 30 个主题和 183 个子领域，包括 30 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。与现有基准不同，MMMU 专注于利用特定领域知识进行高级感知和推理，挑战模型来执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有的 GPT-4V(ision) 的评估凸显了 MMMU 带来的巨大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确率，这表明还有很大的改进空间。我们相信 MMMU 将激励社区构建下一代多模式基础模型，以实现专家通用人工智能。  https://preview.redd.it/0k2e074​​fbc3c1.jpg?width=1663&amp;format=pjpg&amp;auto=webp&amp;s=03c5b80bbab 288919bff3c7838f65ecb79cf8174 https://preview。 redd.it/g646d94fbc3c1.jpg?width=1475&amp;format=pjpg&amp;auto=webp&amp;s=65f5fa706e8295d4f296186ab7f082deb5968a6d https://preview.redd.it/ueftb64fbc3c1.jpg?width=1665&amp;format=pjpg&amp;auto=webp&amp;s=e3a945 765372dc3fc8ae2cb387a5c48b7c5d215b&lt; /a&gt; https:// Preview.redd.it/e10k3a4fbc3c1.jpg?width=1667&amp;format=pjpg&amp;auto=webp&amp;s=cbb71e4030cdb6067dae4fe86baaf8403ff99ec8   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186y58x/r_mmmu_a_massive_multidiscipline_multimodal/</guid>
      <pubDate>Wed, 29 Nov 2023 19:42:21 GMT</pubDate>
    </item>
    <item>
      <title>对抗性扩散蒸馏</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186xudj/adversarial_diffusion_distillation/</link>
      <description><![CDATA[   /u/KarlKani44  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186xudj/adversarial_diffusion_distillation/</guid>
      <pubDate>Wed, 29 Nov 2023 19:29:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过深度学习发现了数百万种新材料</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</link>
      <description><![CDATA[帖子：https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/ 论文：https://www.nature.com/articles/s41586-023-06735-9 摘要： 新型功能材料实现了从清洁能源到信息处理的技术应用的根本性突破。从微芯片到电池和光伏发电，无机晶体的发现一直受到昂贵的试错方法的瓶颈。与此同时，随着数据和计算的增加，语言、视觉和生物学的深度学习模型展示了新兴的预测能力。在这里，我们展示了大规模训练的图网络可以达到前所未有的泛化水平，从而将材料发现的效率提高一个数量级。在持续研究中发现的 48,000 个稳定晶体的基础上，效率的提高使得能够在当前凸包下方发现 220 万个结构，其中许多结构逃过了人类之前的化学直觉。我们的工作代表了人类已知的稳定材料的数量级扩展。最终凸包上的稳定发现将可用于筛选技术应用，正如我们对层状材料和固体电解质候选物的演示一样。在稳定结构中，有 736 个已通过独立实验实现。数以亿计的第一原理计算的规模和多样性也解锁了下游应用的建模能力，特别是导致高度准确和强大的学习原子间势，可用于凝聚相分子动力学模拟和高保真零-离子电导率的射击预测。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/</guid>
      <pubDate>Wed, 29 Nov 2023 18:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 他们说：“这不仅仅是记住训练数据”：从（生产）语言模型中可扩展地提取训练数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</link>
      <description><![CDATA[      &amp;# 32；由   提交/u/wojcech  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186mm4m/r_its_not_just_memorizing_the_training_data_they/</guid>
      <pubDate>Wed, 29 Nov 2023 10:45:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>