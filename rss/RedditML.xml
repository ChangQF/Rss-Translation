<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 30 Dec 2023 18:16:39 GMT</lastBuildDate>
    <item>
      <title>[R] 最佳LLM+Vision/LLVM模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ukuj5/r_best_llmvisionllvm_model/</link>
      <description><![CDATA[是Llava吗？  Mixtral 8x7b 给我留下了深刻的印象。有谁知道有人为实现多式联运所做的努力吗？  开源:)  谢谢！   由   提交 /u/SP4ETZUENDER   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ukuj5/r_best_llmvisionllvm_model/</guid>
      <pubDate>Sat, 30 Dec 2023 17:33:31 GMT</pubDate>
    </item>
    <item>
      <title>[N] HuggingFace 上的文本扩散器 2、DiffMorpher 和 SDXL 自动 FaceSwap！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ujfbd/n_text_diffuser_2_diffmorpher_sdxl_auto_faceswap/</link>
      <description><![CDATA[嘿， 本周提供了一些新的拥抱空间，我觉得你们中的一些人会欣赏的。现在可以在huggingface中找到，甚至可以作为github上的开源代码使用Text Diffusion 2，这是一个非常好的实现，用于生成内部有文本的AI图像，DiffMorpher，一个非常棒的实现视频生成器，它接受 2 个图像作为参数并生成一个视频，描述第一个图像如何过渡到第二个图像和 Stable Diffusion XL Auto FaceSwap，生成非常好的图像，同时现在允许我们“实际上”根据某些源图像修复（交换）图像中的脸部。观看视频，了解一些实例和更多背景信息： https://youtu.be/ApcJ1UyLQB8 此外，由于需求量很大，我创建了一份时事通讯，我将在其中发布科技新闻和我在人工智能世界中发现的其他精彩内容，因此请务必订阅以保持关注！ （100％免费） https://devspot.beehiiv.com/subscribe 请告诉我您的想法，或者如果您对其他视频有任何疑问/请求， 干杯   由   提交/u/dev-spot  /u/dev-spot  reddit.com/r/MachineLearning/comments/18ujfbd/n_text_diffuser_2_diffmorpher_sdxl_auto_faceswap/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ujfbd/n_text_diffuser_2_diffmorpher_sdxl_auto_faceswap/</guid>
      <pubDate>Sat, 30 Dec 2023 16:31:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 动量和批量大小</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uh79r/d_momentum_and_batch_size/</link>
      <description><![CDATA[更大的批量大小可以显着改善训练过程。显然，即使我们愿意牺牲每次梯度更新的计算量来增加批量大小，在某些时候 GPU 内存也是有限的。直观上，使梯度更稳定的另一种方法是增加动量。 是否有人有过这样的实际经验：您希望拥有更多 GPU 内存来增加批量大小，但又无法做到这一点？ t，然后利用动量来稳定梯度，从而改善训练过程？   由   提交 /u/felixcra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uh79r/d_momentum_and_batch_size/</guid>
      <pubDate>Sat, 30 Dec 2023 14:47:51 GMT</pubDate>
    </item>
    <item>
      <title>目前命名实体识别和提取的Sota是多少？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ugt3d/what_is_the_current_sota_on_named_entity/</link>
      <description><![CDATA[命名实体识别和提取的最新技术是什么？   由   提交/u/One_Definition_8975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ugt3d/what_is_the_current_sota_on_named_entity/</guid>
      <pubDate>Sat, 30 Dec 2023 14:28:16 GMT</pubDate>
    </item>
    <item>
      <title>【项目】时间增强检索（TAR）-动态RAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uddmj/project_temporal_augmented_retrieval_tar_dynamic/</link>
      <description><![CDATA[      从文本语料库中，如何检测新出现的主题并它们随时间的演变？ 介绍时间增强检索 (TAR)。 （在 buildspace n&amp;w s4 的背景下构建） TAR 是一种开源的高级 RAG 方法，旨在在执行检索时考虑文本数据的动态和时间方面。 &lt; p&gt;它使我们能够了解所讨论的主题随时间的演变。 该项目背后的想法是开启有关 RAG 方法当前局限性的辩论 第一种方法没有使用 RAG 框架（如 Jerry Liu 的 langchain）构建，专注于金融推文  相关链接： Medium：https://medium.com/@adam-rida/temporal-augmented-retrieval-tar-dynamic-rag-ad737506dfcc&lt; /p&gt; Github：https://github.com/adrida/Temporal_RAG 拥抱脸基准：https://huggingface.co/spaces/Adr740/Temporal-RAG-Benchmark 我的网站：adrida.github.io ​  https://preview.redd.it/lj7wkhk70f9c1.png？ width=960&amp;format=png&amp;auto=webp&amp;s=fc79c5034351a1711e1ec051919a​​5c4d2edbc333   由   提交/u/Adr-740  /u/Adr-740  reddit.com/r/MachineLearning/comments/18uddmj/project_temporal_augmented_retrieval_tar_dynamic/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uddmj/project_temporal_augmented_retrieval_tar_dynamic/</guid>
      <pubDate>Sat, 30 Dec 2023 11:08:59 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 通过决策规则进行模型比较的动态可解释性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18udcxe/research_dynamic_interpretability_for_model/</link>
      <description><![CDATA[      如何确保 ML 模型更新不会在预测中带来意外变化？ 介绍 Deltaxplainer，一种动态 XAI解释为什么两个 ML 分类器不同的方法。 （包含 Python 包） 随着 ML 模型越来越难以解释，其生命周期也越来越难以管理，Dynamic XAI 在过去几年中越来越受到关注。该领域包括将 XAI 研究阐明到更加动态的环境中。这可以通过对数据漂移、模型更新提供人类可理解的解释，甚至研究“静态”解释随时间的演变来实现。 这里分享的这项工作是我们去年夏天发表的文章的实现在 ECML-PKDD 2023 的 DynXAI 研讨会上 论文：通过决策规则进行模型比较的动态可解释性，Adam Rida、Marie-Jeanne Lesot、Xavier Renard、Christophe Marsala 相关链接：  Arxiv 论文：https://arxiv.org/pdf/2309.17095.pdf 中：https://medium.com/@adam-rida/understanding-ml-model-differences-with-deltaxplainer-a-journey-into-dynamic-machine-learning-c787eada1825  Github（包括探索性笔记本）：https://github.com/adrida/deltaxplainer 我的网站： adrida.github.io ​ https://preview.redd.it/np12fz7qze9c1.png?width=1632&amp;format=png&amp;auto=webp&amp;s=2486ffbd53c9cc037f79256818a8be6 66b9c3b2a    由   提交/u/Adr-740  /u/Adr-740  reddit.com/r/MachineLearning/comments/18udcxe/research_dynamic_interpretability_for_model/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18udcxe/research_dynamic_interpretability_for_model/</guid>
      <pubDate>Sat, 30 Dec 2023 11:07:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] InfoSHAP：用信息论 Shapley 值解释预测不确定性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ud5zn/r_infoshap_explaining_predictive_uncertainty_with/</link>
      <description><![CDATA[论文标题：用信息理论 Shapley 值解释预测不确定性 发表于&lt; /strong&gt;：NeurIPS 2023 论文链接：https://arxiv.org/ abs/2306.05724 代码链接：https://github.com /facebookresearch/infoshap tl;dr：这篇论文以一种可以用来解释模型预测而不是模型本身的不确定性的方式扩展了 SHAP预测本身。这可能有多种应用，例如：  在主动学习应用中，采样决策是基于预测不确定性（如 BatchBALD 等现代方法中的情况）做出的，以回答诸如“为什么”之类的问题我们是否决定注释这个特定实例？”。  在强化学习应用中，探索内容的决策是由好奇心驱动并基于奖励的不确定性。在此设置中，它可用于解释“为什么我们的智能体以这种方式进行探索？” 关于特征选择、主动特征值获取、协变量移位的解释的其他一些应用论文中重点介绍了可解释人工智能领域的研究人员开发了多种方法来帮助用户了解复杂监督学习模型的预测。相比之下，解释模型输出的不确定性受到的关注相对较少。我们采用流行的 Shapley 值框架来解释各种类型的预测不确定性，量化每个特征对单个模型输出的条件熵的贡献。我们考虑具有修改后的特征函数的博弈，并从信息论和条件独立性测试中找到所得的沙普利值与基本量之间的深层联系。我们概述了具有可证明保证的有限样本错误率控制的推理程序，并实现了一种有效的算法，该算法在真实和模拟数据的一系列实验中表现良好。我们的方法可应用于协变量偏移检测、主动学习、特征选择和主动特征值获取。   由   提交 /u/TaXxER   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ud5zn/r_infoshap_explaining_predictive_uncertainty_with/</guid>
      <pubDate>Sat, 30 Dec 2023 10:55:04 GMT</pubDate>
    </item>
    <item>
      <title>【项目】AI辅助视频生成：牧童与狼的故事</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ud1m2/project_ai_assisted_video_generation_story_of/</link>
      <description><![CDATA[   /u/randomnes-random  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ud1m2/project_ai_assisted_video_generation_story_of/</guid>
      <pubDate>Sat, 30 Dec 2023 10:46:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 音频生成模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uczel/p_audio_generation_model/</link>
      <description><![CDATA[大家好， 我的目标是通过添加完全由 AI 算法生成的部分来扩展录音。  例如，我有一个上升声音（如警报器）达到某一点的录音，那么是否可以训练模型通过生成新的音频样本来继续这种上升？在同一框架中，网络还可能生成该声音的下降部分，从而以这种方式在两个方向上扩展原始录音。 什么模型可能是最好的？我的想法是关于 Transformer 或 LSTM/RNN。 感谢您的评论。   由   提交/u/ZennikOfficial  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uczel/p_audio_generation_model/</guid>
      <pubDate>Sat, 30 Dec 2023 10:42:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] Yolov8中的特定面部识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ucw2g/p_specific_facial_recognition_in_yolov8/</link>
      <description><![CDATA[我目前正在尝试创建一个项目，在该项目中，我将在 yolov8 和 cv2 对象检测的帮助下被检测到，其中如果我穿着硬质衣服帽子，我会被视为“安全”。但是，如果我没有戴任何安全帽或者只是戴普通帽子，那么我就会被认为“不安全”。 我制作了一个名为“Peter”的自定义数据集其中我拍摄了 70 张自己戴着单色安全帽的图像，70 张根本不戴安全帽的图像，然后 70 张戴帽子的图像 - 所有图像都在不同的背景和环境中。 以下是我训练的一些附加信息过程： - 使用 roboflow 进行注释和添加增强，并将图像乘以 x3 - 使用 google collab 进行训练、验证和测试（300 epoch，缩小到 640x640）（70/20/10） 这是我的课程： Peter - 安全 Peter - 不安全 我已经构建了我的 python 程序并且检测工作正常，但问题是我已经完成了 2 个测试： 第一个测试 - 400~ 图像，300 纪元，增强，缩小，分割测试 |结果 - 甚至无法正确检测到我。 第二次测试 - 1200~ 图像，300 epoch，增强、缩小、分割测试 |结果 - 检测到我，但也检测到其他人，但与我的相比，置信度较低。它还可以更远地检测到我，并且 val_batch 显示了一些很好的结果，因为即使使用拥挤的图像，错误也很少。 如果有人知道我可以做些什么来进一步改进它，请这样做！我之前曾尝试过 yolov5 为戴着安全帽的用户进行对象检测，效果非常好！这一个很困难，因为它需要特定的人体检测/面部识别类型，而不是“它可以检测帧中的所有内容”。   由   提交 /u/SauceNuggetsss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ucw2g/p_specific_facial_recognition_in_yolov8/</guid>
      <pubDate>Sat, 30 Dec 2023 10:36:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] Stability AI 会成为第一个在 2024 年破产的生成型 AI 独角兽吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uclmy/d_will_stability_ai_be_the_first_generative_ai/</link>
      <description><![CDATA[   /u/milaworld  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uclmy/d_will_stability_ai_be_the_first_generative_ai/</guid>
      <pubDate>Sat, 30 Dec 2023 10:16:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 两个高斯函数之间的 KL 散度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ub7c6/d_kl_divergence_between_two_gaussians/</link>
      <description><![CDATA[在大多数 KL 散度实现中，我发现最后取平均值以转换为标量但是为什么第一行最后一行没有给出直接标量？我们正在计算 Q 和 P 之间的 KL 散度，那么为什么我们不直接得到标量呢？ kl = torch.log(sigma_p) - torch.log(sigma_q) + ( sigma_q**2 + (mu_q - mu_p)**2) /(2 *(sigma_p**2))- 0.5 返回 kl.mean()  ​   由   提交/u/sushilkhadakaanon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ub7c6/d_kl_divergence_between_two_gaussians/</guid>
      <pubDate>Sat, 30 Dec 2023 08:41:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习领域 CS 博士对于顶尖项目的竞争力（24 秋季）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uajqj/d_competitiveness_of_cs_phd_in_ml_for_top/</link>
      <description><![CDATA[我最近在其他地方读到一篇文章，声称今年顶尖机构的 ML 领域的 CS 博士招生竞争变得异常激烈。根据该帖子，对于美国排名前 20 的大学，只有在 ICML/NeurIPS/ICLR 上至少发表三篇第一作者论文的人才有机会，并且如果满足以下条件，则需要超过三篇论文：你的论文没有在这三个地点发表。他们还声称，对于排名前 50 的大学，您至少需要在 ICML/NeurIPS/ICLR 发表一篇第一作者论文才能被考虑。 我知道，进入顶尖博士课程的竞争非常激烈。 ML，但我发现这个信息非常可疑，因为我认为甚至不会有那么多申请者在 ICML/NeurIPS/ICLR 博士前至少拥有三篇论文。我个人认识一些顶尖大学的博士生，他们中的很多人在申请时都达不到这样的标准。但这个周期可能非常不同，并且变得特别有竞争力。 如果他们的说法确实属实，我认为我们当前的学术体系和出版文化可能存在更大的问题。  p&gt; 我将其发布在 r/MachineLearning 上，稍后我会将其交叉发布到 r/gradadmissions 在我弄清楚如何做到这一点之后。  &amp;# 32；由   提交 /u/zhxch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uajqj/d_competitiveness_of_cs_phd_in_ml_for_top/</guid>
      <pubDate>Sat, 30 Dec 2023 07:59:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型语言模型世界国际象棋锦标赛🏆♟️</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18u31w8/r_large_language_models_world_chess_championship/</link>
      <description><![CDATA[通过国际象棋的战略视角探索大型语言模型 (LLM) 的新兴能力，精心策划首届 LLM 世界象棋锦标赛。比赛采用循环赛制，大型语言模型的巨头：OpenAI 的 GPT-4 Turbo 和GPT-3.5 Turbo、Google DeepMind 的 Gemini-Pro 和 Mistral AI 的 Mixtral-8x7B 相互较量。 在冠军赛中，每位法学硕士与其他法学硕士进行了 30 场比赛，黑白交替。&lt; /p&gt; “自我反省的思想链”每个模型都使用一次性提示。使用 python-chess 库来确保遵守官方国际象棋规则。 GPT-4 Turbo 夺得冠军，而 Gemini-Pro 尽管得到了 Google 的大力支持，但遇到了推理挑战，表现不佳。 Mixtral 以其先进的推理能力超出了预期。有关比赛的全面视图，请参阅锦标赛的 联盟表。 期待详细的博客文章、概述方法和研究结果的 arXiv 论文、GitHub 存储库、PGN 文件、游戏视频和巫妖链接并附有专家评论。 https://www.linkedin.com /posts/sherazmit_llm-prompt-chess-activity-7146175489622097920-SVTV ​   由   提交 /u/PerformanceRound7913   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18u31w8/r_large_language_models_world_chess_championship/</guid>
      <pubDate>Sat, 30 Dec 2023 01:14:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>