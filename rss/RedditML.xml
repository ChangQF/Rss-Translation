<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 11 Jan 2025 01:16:06 GMT</lastBuildDate>
    <item>
      <title>[R] 条件扩散与嵌入模型的联合训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hygkbr/r_joint_training_of_conditional_diffusion_with/</link>
      <description><![CDATA[有人知道同时训练条件扩散模型和嵌入模型的有效性吗？我的目标是训练一个以状态和时间为条件的扩散模型，以产生随时间推进 n 个时间步的状态，但嵌入模型无法提前知道或预训练。  我见过的最接近我需要的是 GenCast，但我不能简单地连接之前的状态。    提交人    /u/McRibMaster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hygkbr/r_joint_training_of_conditional_diffusion_with/</guid>
      <pubDate>Fri, 10 Jan 2025 22:15:25 GMT</pubDate>
    </item>
    <item>
      <title>[数据集][R] 19,762 张垃圾图像，用于构建 AI 回收解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</link>
      <description><![CDATA[大家好，ML 社区！ 我很高兴与大家分享垃圾分类 V2 数据集，其中包含 19,762 张高质量垃圾图像，这些垃圾被分为 10 个不同类别（例如金属、塑料、衣服和纸张）。 为什么这很重要：  训练 AI 模型以实现自动垃圾分类和回收。 开发垃圾分类应用程序或以可持续性为重点的工具。 创建创新的计算机视觉项目以影响环境。  🔗 数据集链接： 垃圾分类 V2 该数据集已在研究论文《通过迁移学习管理家庭垃圾》中使用，证明了其在实际应用中的实用性。 期待看到您如何使用它来促进可持续发展！    提交人    /u/Downtown_Bag8166   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</guid>
      <pubDate>Fri, 10 Jan 2025 21:19:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有关神经网络如何学习扭曲潜在空间以进行预测的资源？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyefw5/d_resources_for_how_neural_nets_learn_to_warp/</link>
      <description><![CDATA[      有哪些好的资源可以进一步了解神经网络如何构建决策面？ 我最近阅读了 Chris Olah 关于“神经网络、流形和拓扑&quot; 以及“关于深度神经网络的线性区域数量&quot; (ICLR ‘14)。 对神经网络如何“学习折叠潜在空间”以进行预测的想法很感兴趣。  我对简单 MLP 层的直觉是，每个组件在这种几何扭曲中都扮演着不同的角色：  激活函数基本上充当了门控机制 (relu) 偏差向量是一种平移操作 矩阵乘法 Wx 可以通过 SVD（W=USV）来理解： U、V 是旋转/反射矩阵 S 是缩放矩阵   这些操作的组合和堆叠产生了这个伟大的数字： https://arxiv.org/abs/1402.1869 还有其他见解或资源可以参考这些想法吗？    提交人    /u/LetsTacoooo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyefw5/d_resources_for_how_neural_nets_learn_to_warp/</guid>
      <pubDate>Fri, 10 Jan 2025 20:43:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练下一个字符预测的 MLP 是否需要因果掩蔽？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hye9ne/d_do_mlps_trained_for_next_character_prediction/</link>
      <description><![CDATA[假设我们有一些数据 X = [seq_len, batch_size] 和相应的标签 Y = [seq_len, batch_size, vocab_size/num/classes] ，独热编码。 现在，我们要训练一个 MLP 来预测下一个字符。 问题：我们是否需要应用因果掩蔽来限制模型在未来的标记中达到峰值？如果是这样，您将它应用在哪一层或输出上？ 在训练期间，模型会看到整个序列并预测相应的独热编码标签。 通常，我见过的大多数示例都使用 X 及其移位版本 `Y = X&#39;` 作为标签来训练下一个字符预测，但这与我的情况不符，因为我已经有独热编码标签。    提交人    /u/kirk86   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hye9ne/d_do_mlps_trained_for_next_character_prediction/</guid>
      <pubDate>Fri, 10 Jan 2025 20:36:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向法学硕士中的系统 2 推理：学习如何使用元思维链进行思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hye3gm/r_towards_system_2_reasoning_in_llms_learning_how/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hye3gm/r_towards_system_2_reasoning_in_llms_learning_how/</guid>
      <pubDate>Fri, 10 Jan 2025 20:29:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 小型语言模型通过自我进化的蒙特卡洛树搜索掌握复杂数学</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hycnb4/r_small_language_models_master_complex_math/</link>
      <description><![CDATA[这里的关键创新是一种自我进化机制，它使小型语言模型能够通过迭代改进和自我修正来执行复杂的数学推理。这种方法称为 rStar-Math，它使用结构化的分解和验证步骤来实现与更大模型相当的性能，同时使用更少的参数。 关键技术点： - 生成、评估和改进解决方案的多步骤推理框架 - 随着时间的推移开发更复杂的推理模式的自我进化机制 - 实施验证步骤以捕获和纠正错误 - 将复杂问题结构化分解为可管理的子任务 - 用于数学推理和解决方案验证的专用组件 结果： - 在复杂数学问题上实现 80% 以上的准确率 - 与参数多 10 倍的模型性能相匹配 - 自我修正将准确率提高了约 25% - 适用于多个数学领域 - 在数字和文字问题上都表现出一致的性能 我认为这种方法可以为在资源受限的环境中部署功能强大的 ML 系统带来变革。使用较小模型实现强大性能的能力为边缘设备和计算资源有限的场景开辟了可能性。自我进化机制也可以适用于需要复杂推理的其他领域。 我认为最有趣的方面是系统如何学会捕捉自己的错误并改进其推理过程，类似于人类如何发展数学问题解决技能。这可能导致更强大、更可靠的人工智能系统，可以解释他们的思维并自主纠正错误。 TLDR：小型语言模型可以通过自我进化和结构化验证步骤实现强大的数学推理能力，在使用更少资源的情况下匹配更大的模型。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hycnb4/r_small_language_models_master_complex_math/</guid>
      <pubDate>Fri, 10 Jan 2025 19:28:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2.8M CFM 小型模型的结果出人意料地好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hya6hp/d_surprisingly_good_results_from_small_28m_cfm/</link>
      <description><![CDATA[      来自 2.8M CFM 模型的样本 我在旧 GPU 上用缩小版的 CelebA 数据集训练了 2.8M 条件流匹配模型。该模型直接在像素空间中运行，老实说，我并没有期待太多，但结果却出奇的好。CFM 模型之所以令人着迷，是因为训练它们几乎简单得离谱。没有对抗性损失，没有花哨的技巧。但不知何故，它却有效。    提交人    /u/kiockete   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hya6hp/d_surprisingly_good_results_from_small_28m_cfm/</guid>
      <pubDate>Fri, 10 Jan 2025 17:47:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] [P] Cohere For AI 启动新的法学硕士项目，重点关注多语言长语境理解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hy3xn7/r_p_cohere_for_ai_launches_new_llm_cohort_focused/</link>
      <description><![CDATA[      来自 BIRDS（研究驱动研究初学者）小组 r/CohereAI，Cohere 开放科学社区，我们很高兴宣布我们的新 LLM 队列！🎉 🚀 这不仅仅是另一个学习计划；这是一项实践性的协作研究计划，旨在突破大型语言模型在多语言、长上下文设置中的可能性 💡 📚 我们将深入探讨两个令人兴奋的轨道： 🔬 轨道 1：多语言长上下文 - 使用先进技术增强处理 🤖 由领导：Mayank Bhaskar 和 Madhava Prasath 🎯 重点：探索尖端方法，如 RoPE（旋转位置嵌入）、NoPE（无位置编码）、LongROPE、SSM（状态空间模型）和混合 Transformer-SSM 模型，以克服多语言 NLP 中的长上下文挑战，增强可扩展性、效率和处理扩展序列的能力，同时解决传统 Transformer 的局限性。 🧠 挑战：开发一种新方法将 SSM 与 Transformers 相结合，优化长上下文多语言理解。在合成任务上表现出优于 RoPE、NoPE 和 LongRoPE 的卓越性能，强调对超出训练长度的序列的泛化和最小的计算开销。 https://preview.redd.it/x43aruufz5ce1.png?width=632&amp;format=png&amp;auto=webp&amp;s=9fc3e08f80f5a8713cdc4b39c7cb0284a2baf195 🔬 Track 2：评估多语言长上下文生成和推理 🤖 由领导：Guneet Singh Kohli 和 Shivalika Singh 🎯 重点：建立一个基准来评估多语言 LLM 处理涉及复杂推理的长上下文任务的能力。 🧠 挑战：对于长上下文任务，我们如何确保跨语言的准确、上下文相关的响应？评估现有 LLM 执行此类任务的能力，并提出数据创建管道以构建多语言长上下文基准。 https://preview.redd.it/i2r6z4ahz5ce1.png?width=680&amp;format=png&amp;auto=webp&amp;s=93dabba03df32da12fa577dc02516b9ad6048c3d 为什么加入？ 💼 获得实际研究经验：从头到尾参与真实世界的项目。 🤝 与专家合作：向经验丰富的研究人员学习并与他们一起学习。 🌐 塑造法学硕士的未来：为快速发展的领域的进步做出贡献。 📅 启动电话：本周五，1 月 10 日太平洋时间上午 10:00 加入我们，详细了解该群体并与轨道负责人见面！ https://preview.redd.it/1uyvx9fiz5ce1.png?width=680&amp;format=png&amp;auto=webp&amp;s=fc43363fb8cc277e7bcdf3725e9c66f5f92b9df7 2025 年将成为开创性研究的一年，让我们一起踏上这段激动人心的探索之旅！  https://i.redd.it/01luy73mz5ce1.gif    由   提交  /u/CATALUNA84   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hy3xn7/r_p_cohere_for_ai_launches_new_llm_cohort_focused/</guid>
      <pubDate>Fri, 10 Jan 2025 13:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我试图寻找大多数人类可以解决但推理模型却很难解决的常识性问题。以下是一个例子</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hy05iu/d_i_am_trying_to_find_common_sense_problems_that/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hy05iu/d_i_am_trying_to_find_common_sense_problems_that/</guid>
      <pubDate>Fri, 10 Jan 2025 08:46:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 撰写合适的 LLM 摘要的费用出奇地昂贵</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hxzij5/d_creating_proper_llm_summaries_is_surprisingly/</link>
      <description><![CDATA[        提交人    /u/Hot-Chapter48   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hxzij5/d_creating_proper_llm_summaries_is_surprisingly/</guid>
      <pubDate>Fri, 10 Jan 2025 07:57:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代理实验室：使用 LLM 代理作为研究助手 - 能够完成整个研究过程的自主 LLM 框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hxleaa/r_agent_laboratory_using_llm_agents_as_research/</link>
      <description><![CDATA[      论文：https://arxiv.org/pdf/2501.04227 Github：https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file 博客：https://agentlaboratory.github.io/ 摘要：  从历史上看，科学发现是一个漫长而昂贵的过程，从最初的概念到最终的结果需要大量的时间和资源。为了加速科学发现，降低研究成本并提高研究质量，我们引入了 Agent Laboratory，这是一个基于 LLM 的自主框架，能够完成整个研究过程。该框架接受人类提供的研究想法，经过文献综述、实验和撰写报告三个阶段，产生全面的研究成果，包括代码库和研究报告，同时允许用户在每个阶段提供反馈和指导。我们在 Agent Laboratory 中部署了各种最先进的 LLM，并邀请多位研究人员通过参与调查来评估其质量，提供人类反馈来指导研究过程，然后评估最终的论文。我们发现：(1) 由 o1-preview 驱动的 Agent Laboratory 产生了最好的研究成果；(2) 与现有方法相比，生成的机器学习代码能够达到最先进的性能；(3) 人类参与并在每个阶段提供反馈，显著提高了研究的整体质量；(4) Agent Laboratory 显著降低了研究费用，与以前的自主研究方法相比，减少了 84%。我们希望 Agent Laboratory 能让研究人员将更多的精力投入到创造性思维上，而不是低级的编码和写作上，最终加速科学发现。  https://preview.redd.it/oop8omfvt0ce1.jpg?width=1591&amp;format=pjpg&amp;auto=webp&amp;s=e588146997010797bbe75ea9d575bd65ce0d8fc6    提交人    /u/Singularian2501   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hxleaa/r_agent_laboratory_using_llm_agents_as_research/</guid>
      <pubDate>Thu, 09 Jan 2025 19:46:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] rStar-Math：小型法学硕士可以通过自我进化的深度思维掌握数学推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hxk2ab/r_rstarmath_small_llms_can_master_math_reasoning/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hxk2ab/r_rstarmath_small_llms_can_master_math_reasoning/</guid>
      <pubDate>Thu, 09 Jan 2025 18:51:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么攻读法学硕士学位这么糟糕？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hx6q8r/d_why_does_training_llms_suck_so_much/</link>
      <description><![CDATA[我从事硬件加速工作，一直在尝试将我的注意力转移到 LLM/GenAI 加速上，但训练 LLM 实在是太糟糕了……即使是 100M 参数的训练，在 4 A6000 Adas 上也需要很长时间，虽然我没有花时间看这些，但当我意识到 LR 太高或其他一些小问题阻碍了收敛或一般因果语言理解时，不得不重新训练真是太令人沮丧了…… 我知道你做的越多，你就会做得越好，但作为一个 GRA，我有一个想实现的想法，我真的觉得训练即使是一个小型 LM 的开销也远远不值得你投入的时间和精力 这很糟糕，因为截止日期总是会到来，一旦你完成了预训练，你仍然需要进行微调，并可能进行某种异常值感知量化，甚至训练 LoRA 适配器以获得更高的准确性 我真的希望永远不要再进行预训练，但是需要一个符合您的特定大小限制的模型以适合（例如）您的 NPU 的暂存器 RAM，这意味着我总是陷入预训练 希望在未来，我可以让本科生为我进行预训练，但是就目前而言，有什么技巧可以使预训练 LLM 不那么像奴隶工作吗？谢谢！    提交人    /u/nini2352   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hx6q8r/d_why_does_training_llms_suck_so_much/</guid>
      <pubDate>Thu, 09 Jan 2025 06:28:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>