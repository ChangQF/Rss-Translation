<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Sun, 25 Aug 2024 18:18:33 GMT</lastBuildDate>
    <item>
      <title>[D] 比利时博士学位</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f1253q/d_phd_in_belgium/</link>
      <description><![CDATA[大家好， 我是一名印度学生，目前正在巴塞罗那联邦大学 (UPF) 攻读智能交互系统硕士学位。 我想知道比利时是否有提供 ML 博士学位的好学校。我对 RL 很感兴趣，但即使是非 RL 方向的博士学位也会很棒。 任何帮助都将不胜感激！    提交人    /u/FlyTrain1011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f1253q/d_phd_in_belgium/</guid>
      <pubDate>Sun, 25 Aug 2024 17:42:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 检测和分类软件漏洞</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f117b1/p_detecting_and_classifying_software/</link>
      <description><![CDATA[大家好，我一直想做一个项目，想听听大家的想法。目标是从反汇编的可执行文件中对漏洞进行分类。我的计划是同时使用顺序信息（指令、助记符、操作数）和控制流图（基于分支）。 我选择了一些常见的漏洞，例如缓冲区溢出、整数溢出和内存泄漏，并希望将它们分类为包含这些漏洞之一或不易受攻击。Juliet C/C++ 数据集似乎是这项任务的不错选择，因为它包含以不同风格编写的约 60k 个易受攻击和不易受攻击的源代码测试用例。这些可以使用不同的编译器选项进行编译，以生成略有不同的反汇编，从而构建更大的词汇表。 我很想听听你对这个想法的看法，以及在开始之前我应该​​考虑哪些主要障碍。    提交人    /u/SgtPepper8903   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f117b1/p_detecting_and_classifying_software/</guid>
      <pubDate>Sun, 25 Aug 2024 17:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 处理包含大量 NaN 的大型表格数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0zq8l/p_dealing_with_large_tabular_dataset_with_lot_of/</link>
      <description><![CDATA[现在我正在处理 400k 行分类表格数据集，其中包含如此多的 NaN，因此如果我执行 dropna()，它只会剩下 4 行。我现在正在进行 KNN 插补，但它花费了大量时间（在我撰写这篇文章时它还没有完成）。我的问题是，如何处理大型数据集的插补？我必须对数据集进行抽样还是其他什么？    提交人    /u/Fun_Ambition_5186   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0zq8l/p_dealing_with_large_tabular_dataset_with_lot_of/</guid>
      <pubDate>Sun, 25 Aug 2024 16:01:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 根据应用交互的视频片段生成 Gherkin 场景</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0z19p/p_generating_gherkin_scenarios_from_video_footage/</link>
      <description><![CDATA[大家好，我正在做一个项目，需要根据视频输入生成 Gherkin 场景。本质上，我想分析应用程序交互的视频片段（如光标移动、文本输入和按钮点击）并自动为 BDD 框架创建 Gherkin 测试用例。有人做过类似的事情吗？或者对如何处理这个问题有什么建议吗？你会推荐什么工具或技术？    提交人    /u/lonylegend   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0z19p/p_generating_gherkin_scenarios_from_video_footage/</guid>
      <pubDate>Sun, 25 Aug 2024 15:31:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 编码还是不编码？探索编码在预训练中的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0yh4v/r_to_code_or_not_to_code_exploring_impact_of_code/</link>
      <description><![CDATA[  由    /u/RobbinDeBank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0yh4v/r_to_code_or_not_to_code_exploring_impact_of_code/</guid>
      <pubDate>Sun, 25 Aug 2024 15:07:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Aug 2024 15:00:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GNN 进行量子机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0xys8/r_quantum_machine_learning_with_gnns/</link>
      <description><![CDATA[我刚刚写了一篇关于“使用 GNN 的量子机器学习”的研究论文。你们能帮我审阅一下吗？ https://drive.google.com/file/d/12U-owjYFgV0jrS4vThrg8DXHJF5baO7J/view?usp=drivesdk    提交人    /u/Anonymous_Life17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0xys8/r_quantum_machine_learning_with_gnns/</guid>
      <pubDate>Sun, 25 Aug 2024 14:44:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何分析梯度直方图并调试我的深度学习模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0x5jd/p_how_to_analyze_gradient_histograms_and_debug_my/</link>
      <description><![CDATA[我正在尝试训练一个模型来预测给定的声音和给定的音素相同（输出 1）或不相同（输出 0）的概率。 但损失并没有从 0.6 减少，因为随机概率为 0.5，随机预测的 BCE 损失为 0.69。 我在训练时使用权重和偏差来绘制模型的参数和梯度 这是报告：https://api.wandb.ai/links/svar-svar/61n2waag gradients_max 在正常范围内，但梯度直方图的范围是 1000，最大值怎么会小于 1，而直方图又有这样的分布。请帮助我理解如何解释这些直方图 class MLPLayer( nn.Module ): def __init__( self , hidden_​​dim1 , hidden_​​dim2 , hidden_​​dim3 , embedding_dim ) : super( MLPLayer , self ).__init__() self.layer_1 = nn.Linear( embedding_dim , hidden_​​dim1 ) self.relu_fn = nn.ReLU() self.dropout_layer_1 = nn.Dropout( p = 0.15 ) self.layer_2 = nn.Linear( hidden_​​dim1 , hidden_​​dim2 ) self.linear_3 = nn.Linear（hidden_​​dim2，hidden_​​dim3） self.linear_4 = nn.Linear（hidden_​​dim3，1） self.dropout_layer_2 = nn.Dropout（p = 0.15） def forward（self，x）： out1 = self.relu_fn（self.layer_1（x）） out1 = self.dropout_layer_1（out1） out1 = self.relu_fn（self.layer_2（out1）） out1 = self.dropout_layer_2（self.relu_fn（self.linear_3（out1 out2 = self.linear_4 ( out1 ) out = torch.sigmoid( out2 ) return out class PhonemeEmbedding(nn.Module): def __init__(self, embedding_dim=24, max_len=1000, device=&#39;cpu&#39;): super(PhonemeEmbedding, self).__init__() self.max_len = max_len self.embedding_dim = embedding_dim self.device = device self.embedding_layer = nn.Embedding（num_embeddings=self.max_len，embedding_dim=self.embedding_dim） def forward（self，x）： 返回self.embedding_layer（x） dta_model = CustomWav2Vec2ForCTC（base_model_name=&quot;facebook/wav2vec2-base-960h&quot;） dta_model.load_state_dict(torch.load(&quot;/kaggle/input/lates-model/model_dta_model_epoch_7.pth&quot; , map_location = &quot;cuda&quot;)) 这是主要架构，我将音频通过 Wav2Vec2，并为所有音素嵌入音素，将这两个音素连接起来，然后在上面的 mlp 中对它们进行处理。 我注意到的另一件事是，嵌入层在整个训练过程中保持高斯分布，如何防止这种情况并让它学习音素声音的概念。    提交人    /u/Agreeable_Ad_1085   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0x5jd/p_how_to_analyze_gradient_histograms_and_debug_my/</guid>
      <pubDate>Sun, 25 Aug 2024 14:08:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] Jamba-1.5：大规模混合 Transformer-Mamba 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wvnz/r_jamba15_hybrid_transformermamba_models_at_scale/</link>
      <description><![CDATA[      TL;DR: 大型（高达 94B/398B 活跃/总参数）混合开放权重模型，高达 256k 上下文 论文： https://arxiv.org/pdf/2408.12570 博客： https://www.ai21.com/blog/announcing-jamba-model-family 摘要：  我们提出了 Jamba-1.5，这是基于我们的 Jamba 架构的新型指令调整大型语言模型。Jamba 是 Transformer-Mamba 混合专家架构，可在上下文长度上提供高吞吐量和低内存使用率，同时保持与 Transformer 模型相同或更好的质量。我们发布了两种模型大小：Jamba-1.5-Large，具有 94B 个活动参数，以及 Jamba-1.5-Mini，具有 12B 个活动参数。这两种模型都针对各种对话和指令遵循功能进行了微调，并且具有 256K 个标记的有效上下文长度，这是开放权重模型中最大的。为了支持经济高效的推理，我们引入了 ExpertsInt8，这是一种新颖的量化技术，允许在具有 8 个 80GB GPU 的机器上安装 Jamba-1.5-Large，同时处理 256K 个标记上下文而不会损失质量。在一系列学术和聊天机器人基准测试中，Jamba-1.5 模型取得了出色的结果，同时提供了高吞吐量，并且在长上下文基准测试中优于其他开放权重模型。两种尺寸的模型权重均在 Jamba 开放模型许可下公开提供，我们将 ExpertsInt8 作为开源发布。  视觉亮点： 在混合架构中，Mamba-1 块的表现优于 Mamba-2 块。当模型具有注意力时，Mamba-2 处理细节可能是多余的 https://preview.redd.it/onrdw742ftkd1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=dff7d4dd10ccc0b8d1481bab7de084cb2ac1b586 https://preview.redd.it/nm79gn64ftkd1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=650327eba37add3af6fd629371b98010789f10a2 https://preview.redd.it/yd3l7k46ftkd1.png?width=1115&amp;format=png&amp;auto=webp&amp;s=bce413b0f95ec2cb786cb388589cbe22c06d5ca9 https://preview.redd.it/qe4choo7ftkd1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=eca7f428af74099d15dd1ebd7db1cdbe7d6d721e https://preview.redd.it/vy26ido9ftkd1.png?width=1109&amp;format=png&amp;auto=webp&amp;s=b6b2ee51a650c2910a01c465a810e33ad7880ebb 无限长凳 https://preview.redd.it/61dfzmugftkd1.png?width=1147&amp;format=png&amp;auto=webp&amp;s=c53aa8a8d43b49aeb81871db9f25227874cc34ad 下载：https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251    由   提交  /u/StartledWatermelon   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wvnz/r_jamba15_hybrid_transformermamba_models_at_scale/</guid>
      <pubDate>Sun, 25 Aug 2024 13:55:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习到底发生了什么？一些最小模型 (Stephen Wolfram)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wj6s/r_whats_really_going_on_in_machine_learning_some/</link>
      <description><![CDATA[Stephen Wolfram 最近发表了一篇博客文章，其中提出了一些关于离散神经网络的有趣观点，从自动机的角度来看待训练： https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wj6s/r_whats_really_going_on_in_machine_learning_some/</guid>
      <pubDate>Sun, 25 Aug 2024 13:38:16 GMT</pubDate>
    </item>
    <item>
      <title>有人在机器学习中使用合成数据吗？它对你的项目有何影响？[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0wbfy/anyone_actually_using_synthetic_data_in_ml_how/</link>
      <description><![CDATA[我很好奇您在机器学习项目中实际使用的合成数据的实际应用。 它是否真正增强了您的流程或结果？您在使用它时面临的最大挑战是什么？ 我很想听听您的经历——好的和坏的。    提交人    /u/Value-Forsaken   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0wbfy/anyone_actually_using_synthetic_data_in_ml_how/</guid>
      <pubDate>Sun, 25 Aug 2024 13:27:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 上周医疗 AI：顶级研究论文/模型🏅（2024 年 8 月 17 日至 8 月 24 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0fipu/d_last_week_in_medical_ai_top_research/</link>
      <description><![CDATA[   本周热门论文（8 月 17 日至 24 日）  医学多模态 LLM 的越狱  本文揭示了医学 MLLM 中的安全漏洞。针对 MedMLLM 的新“不匹配的恶意攻击”（2M 攻击）。它介绍了用于测试各种医疗场景的 3MAD 数据集  LLM 不是 零样本生物医学推理器  本文对生物医学任务上的 LLM 进行了基准测试，它在医学分类和 NER 上测试了 LLM，评估了标准提示、CoT、自洽性和 RAG  RuleAlign 框架：将 LLM 与医生规则对齐  本文介绍了用于医学诊断的 LLM 的 RuleAlign 框架。它将 LLM 与特定的诊断规则相结合，并开发基于规则的医学对话数据集。  CTP-LLM：用于临床试验转变预测的 LLM  本文介绍了用于临床试验预测的 CTP-LLM，它介绍了用于基准测试的 PhaseTransition (PT) 数据集。在所有阶段实现 67% 的准确率，从 III 期到批准的准确率达到 75%。  HIBOU：病理学的基础视觉转换器  本文介绍了病理学的视觉转换器，利用 DINOv2 框架在超过 100 万张全幻灯片图像 (WSI) 上预训练两种模型变体 Hibou-B 和 Hibou-L  LLaVA-Surg：多模式手术助手  LLaVA-Surg 引入了大规模手术视频指令调整数据集 Surg-QA，其中包含来自 2,201 个手术程序的超过 102K 个手术视频指令对，并训练了 LLaVA-Surg 模型。  ...  详细查看完整线程：https://x.com/OpenlifesciAI/status/1827442651810918509 感谢您的阅读！如果您知道任何遗漏的有趣论文，请随时在评论中分享。如果您对医疗 AI 有见解或突破，并希望在下周的版本中分享，请通过 Twt/x 与我们联系：OpenlifesciAI    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0fipu/d_last_week_in_medical_ai_top_research/</guid>
      <pubDate>Sat, 24 Aug 2024 21:01:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] 生产中的机器学习：从数据科学家到机器学习工程师</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</link>
      <description><![CDATA[我很高兴与大家分享我整理的一门课程：生产中的机器学习：从数据科学家到机器学习工程师。本课程旨在帮助您从 Jupyter 笔记本中获取任何 ML 模型并将其转变为可用于生产的微服务。 本课程涵盖以下内容：  将您的 Jupyter 代码构建为生产级代码库 管理数据库层 参数化、日志记录和最新的干净代码实践 使用 GitHub 设置 CI/CD 管道 为您的模型开发 API 容器化您的应用程序并使用 Docker 进行部署  我很乐意收到您对本课程的反馈。这是免费访问的优惠券代码：FREETOLEARN24。您的见解将帮助我改进和完善内容。如果您喜欢本课程，我希望您留下好评，以便其他人也可以找到这门课程。谢谢，祝您学习愉快！    提交人    /u/5x12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</guid>
      <pubDate>Sat, 24 Aug 2024 20:54:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] Liger Kernel：一行代码使 LLM 培训速度提高 20%，内存减少 60%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</link>
      <description><![CDATA[        提交人    /u/Icy-World-8359   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</guid>
      <pubDate>Sat, 24 Aug 2024 15:39:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>