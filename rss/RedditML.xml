<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Thu, 07 Mar 2024 15:12:59 GMT</lastBuildDate>
    <item>
      <title>[D] MAMBA 优于变形金刚吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8u2kq/d_is_mamba_superior_to_transformers/</link>
      <description><![CDATA[您好， 曼巴架构引起了很多争议。它真的有那么好，比变形金刚更好吗？如果是，为什么我们没有看到它像变形金刚推出时那样被广泛采用。   由   提交 /u/rodeowrong   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8u2kq/d_is_mamba_superior_to_transformers/</guid>
      <pubDate>Thu, 07 Mar 2024 13:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]教程：使用Python进行面部情绪识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8tioc/d_tutorial_facial_emotion_recognition_with_python/</link>
      <description><![CDATA[情绪识别是机器学习的一部分，属于人工智能这一相对较新的研究领域。 如今，这项技术被用于自动识别图像和视频中的面部表情、音频中的口头表达、文本中的书面表达以及可穿戴设备测量的生理学。 Luxand 提供基于云的情绪识别 API，该 API 提供全套基于 AI 的计算机视觉工具用于人脸识别。通过本教程，您将了解如何使用 Python 将 Luxand.clod 情绪检测 API 实施到您的应用、软件或系统中。 在此处阅读更多内容：- https://luxand.cloud/face-recognition-blog/tutorial-facial-使用 python 进行情感识别   由   提交 /u/PrestigiousGridlock   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8tioc/d_tutorial_facial_emotion_recognition_with_python/</guid>
      <pubDate>Thu, 07 Mar 2024 12:31:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] 集中注意力（使用自适应 IIR 滤波器）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8sb9m/r_focus_your_attention_with_adaptive_iir_filters/</link>
      <description><![CDATA[EMNLP 2023：https://aclanthology.org/2023.emnlp-main.772/ arXiv：https://arxiv.org/abs/2305.14952 OpenReview：https://openreview.net/forum?id=DlQeSfGYfS 摘要：  我们提出了一个新层，其中使用二阶动态（即依赖于输入的）无限脉冲响应（IIR）滤波器来处理应用常规注意之前的输入序列。输入被分成块，并且这些滤波器的系数是根据先前的块确定的，以保持因果关系。尽管其阶数相对较低，但因果自适应滤波器将注意力集中在相关序列元素上。新层以控制理论为基础，并被证明可以推广对角状态空间层。该层的性能与最先进的网络相当，参数仅为其一小部分，时间复杂度与输入大小成次二次方。无论是在参数数量还是在多个远程序列问题上获得的性能水平方面，所获得的层都优于 Hyena、GPT2 和 Mega 等层。     由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8sb9m/r_focus_your_attention_with_adaptive_iir_filters/</guid>
      <pubDate>Thu, 07 Mar 2024 11:23:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 尾巴的故事：模型崩溃作为缩放定律的变化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8s8rg/r_a_tale_of_tails_model_collapse_as_a_change_of/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.07043 摘要：  随着人工智能模型规模的增长，神经缩放法则&lt; /em&gt; 已成为在增加原始（人类或自然）训练数据的容量和大小时预测大型模型改进的重要工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将共同进化，以逐步包含越来越多的合成数据。在本文中，我们问：在合成数据进入训练语料库的不可避免的情况下，缩放法则将如何变化？未来的模型是否仍然会改进，或者注定会退化到总 &lt; em&gt;（模型）崩溃？我们通过缩放定律的视角开发了模型崩溃的理论框架。我们发现了各种各样的衰变现象，分析了尺度损失、随代数变化的尺度变化、“不学习”现象等。技能，以及混合人类和合成数据时的摸索。我们的理论通过使用大型语言模型 Llama2 进行算术任务和文本生成的变压器的大规模实验得到了验证。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8s8rg/r_a_tale_of_tails_model_collapse_as_a_change_of/</guid>
      <pubDate>Thu, 07 Mar 2024 11:19:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] CNN突破和UNet骨干网</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8r2xu/d_cnn_breakthroughs_and_unet_backbones/</link>
      <description><![CDATA[你好， 我希望你能在评论中列出一些 CNN 的突破，特别是对于像 wavenet、TCN 这样的序列数据，可变形TCN ..等 如果你能给我可以与UNet一起使用的主干，如残差块，密集块......等 &lt;!-- SC_ON - -&gt;  由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8r2xu/d_cnn_breakthroughs_and_unet_backbones/</guid>
      <pubDate>Thu, 07 Mar 2024 10:07:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 微软研究院推出 NaturalSpeech 3，这是零样本文本转语音技术的重大进步。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</link>
      <description><![CDATA[      论文链接：https://arxiv.org/abs/2403.03100 演示链接：https://speechresearch.github.io/naturalspeech3/&quot;&gt;https:// /speechresearch.github.io/naturalspeech3/ ​ 基于 NaturalSpeech 系列的成功，NaturalSpeech 3 不仅继承了高质量的合成功能而且还通过分解语音属性来进一步推进，从而实现更详细和受控的合成过程。 ​ NaturalSpeech 3 的主要亮点包括： 1.因子化编解码器：具有因子化矢量量化的神经编解码器能够熟练地将语音分解为不同的子空间，从而有针对性地改进语音生成。 2.因子化扩散模型：因子化扩散模型旨在生成语音属性与相应的提示完全一致。这种创新方法使 NaturalSpeech 3 不仅可以合成类似人类的语音，还可以调整韵律和音色的细微差别，以匹配说话者的情感和风格。 3. 可扩展性：可扩展至 10 亿个参数经过超过 20 万小时的数据训练，NaturalSpeech 3 在提高语音质量和清晰度方面显示出了可喜的成果。未来，NaturalSpeech 3 计划进一步扩大规模，以实现更精细的结果。 ​ 深入研究演示、阅读论文，了解 NaturalSpeech 3 的用途为零样本语音合成设定新标准。 https://preview.redd.it/gbgau8vwkvmc1.png?width=1982&amp;format=png&amp;auto=webp&amp;s=3260d6ac03b42e6059c5e0a58169d880b037 b00f ​   由   提交/u/Front-Article-7366   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</guid>
      <pubDate>Thu, 07 Mar 2024 08:47:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么最新、最伟大的法学硕士仍然为生成十个以 apple 结尾的句子这样的小事而苦苦挣扎？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</link>
      <description><![CDATA[所有 3 个模型（Gemini Advanced、Claude 3.0 Opus、GPT-4）都失败了，gpt-4 表现最好，十有八九以苹果结尾。    由   提交 /u/ccooddeerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</guid>
      <pubDate>Thu, 07 Mar 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] Apollo：轻量级多语言医学法学硕士，将医疗人工智能普及到 6B 人群</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</link>
      <description><![CDATA[    &lt; /a&gt;  我们开源了一系列SOTA轻量级多语言医疗LLM Apollo (0.5B, 1.8B, 2B, 6B, 7B)，利用非翻译语料库取得最佳新表现 覆盖英文、中文、法语、西班牙语、阿拉伯语和印地语  整个过程开源且可复制 精简版模型可以是用于提高大型模型的多语言医疗能力无需以代理调整方式进行微调   github：https://github.com/FreedomIntelligence/Apollo 演示：https://apollo.llmzoo.com/#/ 论文：https ://arxiv.org/abs/2403.03640 模型：https://huggingface.co /FreedomIntelligence/Apollo-7B  https://preview.redd.it/29kjdct4oumc1.png?width=1488&amp;format=png&amp;auto=webp&amp;s=1a16bbbf2588fb071ba2af5a50668ca8335c 92b7 https://preview.redd.it/406m28t4oumc1.png?width=1120&amp; ;format=png&amp;auto=webp&amp;s=607664035b62aa0ee726d3f5b4c4730863823bcb https://preview.redd.it/jiewd5t4oumc1.png?width=1242&amp;format=png&amp;auto=webp&amp;s=e059d49da4e788729b134b0d64415bcf 85bb024c    由   提交 /u/Pasu06   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</guid>
      <pubDate>Thu, 07 Mar 2024 05:33:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia Tesla P40 与 Mangio-RVC-Fork 配合良好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</link>
      <description><![CDATA[正在寻找一种经济有效的方法来训练语音模型，在 eBay 上以 150 美元左右的价格购买了一台二手 Nvidia Tesla P40 和一个 3D 打印冷却器我交叉手指。系统只是我的一台带有 B250 Gaming K4 主板的旧电脑，没什么特别的，在 Windows 10 上运行得很好，并且在 Mangio-RVC-Fork 上以惊人的速度进行训练。它有 24GB 的 vram，因此您可以加载大型数据集并提高批量大小。使用默认设置，我在 rmvpe 上用 35 分钟的数据集训练了一个语音，批量大小为 12（仅使用大约 10GB 的 vram），纪元时间约为 1 分 30 秒。结合较大的 vram 净空，我认为更多的人应该尝试 RVC！性价比无与伦比！   由   提交/u/Remote_Hunt516  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</guid>
      <pubDate>Thu, 07 Mar 2024 03:33:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] Azure GPU 限制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/</link>
      <description><![CDATA[刚刚请求增加 a100 图像的配额，他们说 GPU 需求太高。想知道其他人是否遇到过这个问题，如果是的话，您是如何解决的？租用 GPU 不应该这么困难......    由   提交 /u/Primary-Track8298    reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8e8yy/d_azure_gpu_restrictions/</guid>
      <pubDate>Wed, 06 Mar 2024 23:05:01 GMT</pubDate>
    </item>
    <item>
      <title>非文本数据的 LLM 微调 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8dp7t/finetuning_llm_on_nontext_data_discussion/</link>
      <description><![CDATA[嗨， 是否可以在非文本数据上训练大型语言模型。我可以将其应用于任何顺序数据集（例如音符）吗？我认为棘手的部分是对数据集进行标记，以便法学硕士可以更好地理解数据的底层结构。如果预训练的 LLMS 不是正确的方法，您能否建议任何其他预训练的模型？我试图解决的问题需要预测离散值，因此我认为使用音频生成模型没有意义。我不喜欢从头开始训练，所以只是想知道。另外，如果我的直觉不对，请告诉我。 谢谢！   由   提交 /u/Dunkeyfunkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8dp7t/finetuning_llm_on_nontext_data_discussion/</guid>
      <pubDate>Wed, 06 Mar 2024 22:43:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 逆转诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</link>
      <description><![CDATA[https://arxiv.org/pdf/2309.12288 .pdf 原来我预测了 2021 年的逆转诅咒哈哈 https://www.reddit.com/r/MachineLearning/comments/p13ean/d_can_gpt_generalize_in_both_directions/ 编辑：第一篇论文引用的另一篇论文甚至使用了非常相似的示例： https://arxiv.org/pdf/2308.03296.pdf &lt; blockquote&gt; 美国第一任总统是乔治·华盛顿  如果我的帖子以任何方式启发作者，我会非常高兴 &lt; !-- SC_ON --&gt;  由   提交 /u/DeMorrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</guid>
      <pubDate>Wed, 06 Mar 2024 18:16:43 GMT</pubDate>
    </item>
    <item>
      <title>[D][R]强化学习的最新进展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b81pkt/drrecent_developments_in_reinforcement_learning/</link>
      <description><![CDATA[我正在尝试进入强化学习领域，并且刚刚完成了 Sutton 和 Barto 的课程以及 YT 的一门课程。只是想知道目前在这个主题上正在做什么（调查论文/书会很好）。还想知道常用的数据集类型。我学习的课程本质上是完全理论性的，所以我也想知道目前这个领域使用什么工具包。   由   提交/u/ANI_phy  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b81pkt/drrecent_developments_in_reinforcement_learning/</guid>
      <pubDate>Wed, 06 Mar 2024 14:56:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]为什么 Hugging Face 没有成为桌面上最有前途（且年轻）的 AI 聊天机器人玩家之一（如 Mistral AI、Anthropic、Perplexity AI 等）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7t35y/dwhy_isnt_hugging_face_becoming_one_of_the/</link>
      <description><![CDATA[我记得几年前人们讨论HF的商业模式是什么或者如何盈利。 我认为现在是对他们来说这是最好的时代，但我有点惊讶他们没有创造自己的时代。 他们有才华、经验和资源。只是想知道。   由   提交 /u/xiikjuy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7t35y/dwhy_isnt_hugging_face_becoming_one_of_the/</guid>
      <pubDate>Wed, 06 Mar 2024 06:41:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>