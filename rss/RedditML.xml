<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 05 Jan 2024 09:13:28 GMT</lastBuildDate>
    <item>
      <title>带注释的S4.[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z2gt9/the_annotated_s4d/</link>
      <description><![CDATA[https://srush.github.io/注释-s4/   由   提交/u/One_Definition_8975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z2gt9/the_annotated_s4d/</guid>
      <pubDate>Fri, 05 Jan 2024 08:58:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学术界到工业界</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z2f4r/d_academia_to_industry/</link>
      <description><![CDATA[我是一名最近（一年）的博士毕业生，专注于机器学习和统计模型应用以了解海洋气候变化。由于我一直在学术界工作，我意识到这可能不适合我。 我真的很喜欢解决问题和进行前沿分析，但是学术界不断的资助周期和非研究要求是关闭。  我曾有过研究数据科学或机器学习应用领域的行业工作的想法，但我完全迷失了。当我开始向行业转型时，有人有任何建议或建议吗？   由   提交/u/dcoceans11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z2f4r/d_academia_to_industry/</guid>
      <pubDate>Fri, 05 Jan 2024 08:54:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 A100 与 4x4090 训练 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z0jja/d_training_llm_with_a100_vs_4x4090/</link>
      <description><![CDATA[我必须在 A100 (80Gb) 和 4x4096 (92GB) 之间做出选择。我正在寻找训练 7B 模型。看起来 7B 模型将需要 55 GB（使用 Adam 作为优化器）。那么，如果我有 4x4096 GPU，是否足够了？如果我使用 DPO 或 rhf 进行训练，这将有两个模型，这会使 GPU 变为 3 倍吗？ 我应该使用哪一个，A100 还是 4x4096？ ~   由   提交 /u/Electronic_Hawk524   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z0jja/d_training_llm_with_a100_vs_4x4090/</guid>
      <pubDate>Fri, 05 Jan 2024 06:51:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在字典上训练 NMT 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yzrz7/p_train_nmt_model_on_dictionary/</link>
      <description><![CDATA[我们有一个在通用数据集上进行英语/印地语翻译的微调模型。然而，我们要求它适应法律领域，法律领域往往有很多训练期间未涵盖的独特单词。数据也非常少，需要创建新的数据集进行微调。我确实读过有关基于规则的翻译，但我对 NMT 还很陌生，不知道如何将其插入基于变压器的模型。 我们确实收集了一些具有英语-&gt;印地语翻译的词典，是否可以使用这些数据或对其进行扩充，以达到所需的结果。 ​ 非常感谢您的回复。 &lt; /div&gt;  由   提交 /u/Tejasw__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yzrz7/p_train_nmt_model_on_dictionary/</guid>
      <pubDate>Fri, 05 Jan 2024 06:06:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 扩散模型微调建议？我正在尝试实现图像到图像。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yz9ue/d_diffusion_model_finetuning_suggestions_im/</link>
      <description><![CDATA[寻找小图像到图像扩散模型的建议，以对猫图像进行微调。 我是扩散模型的初学者。希望对数百张图像进行微调。请随时建议我应该使用的训练示例数量。   由   提交 /u/Current_Dark6603   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yz9ue/d_diffusion_model_finetuning_suggestions_im/</guid>
      <pubDate>Fri, 05 Jan 2024 05:39:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 建立一个小型 HPC 来协调小型团队的人工智能研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yxwlh/d_setting_up_a_small_hpc_for_orchestrating_a/</link>
      <description><![CDATA[我想了解社区关于设置用于 AI/CV/ 的 HPC（具有计算负载的单机）的意见和经验在一个小团队中进行法学硕士研究。本质上，设置 HPC 以便多个用户可以在慢速存储上存储数据集，自动将数据集神奇地传输到快速存储以进行训练并在完成后删除，选择 1-&gt;N GPU（允许多个用户同时训练）或一项重大工作）并防止系统被用户数据集/环境的秘密存储所堵塞，并且理想情况下工程开销/维护较低。 实现这一目标的方法是什么？优点和缺点？ 例如，Kubernetes 可以与 docker 一起使用来调度资源、构建环境、训练模型，然后从快速存储中优雅地删除数据集、关闭容器并将其从内存中删除。对我来说，这似乎是一种不错的方法，因为我知道我可以用它进行调度和编排，但 HPC 永远不会在集群中使用，所以可能这是一种矫枉过正。   由   提交 /u/Dr-LucienSanchez   /u/Dr-LucienSanchez reddit.com/r/MachineLearning/comments/18yxwlh/d_setting_up_a_small_hpc_for_orchestrating_a/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yxwlh/d_setting_up_a_small_hpc_for_orchestrating_a/</guid>
      <pubDate>Fri, 05 Jan 2024 04:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 超越 Chinchilla 最优：解释语言模型缩放定律中的推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yxu0d/r_beyond_chinchillaoptimal_accounting_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.00448 摘要：  大型语言模型（LLM）缩放定律是估计的经验公式由于参数数量和训练数据的增加而导致模型质量发生变化。然而，这些公式，包括流行的 DeepMind Chinchilla 缩放法则，忽略了推理成本。我们修改 Chinchilla 缩放法则来计算最佳 LLM 参数计数和预训练数据大小，以训练和部署给定质量和推理需求的模型。我们根据计算预算和实际成本进行分析，发现预计有相当大推理需求（~1B 请求）的 LLM 研究人员应该训练比 Chinchilla 最优更小、更长的模型。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yxu0d/r_beyond_chinchillaoptimal_accounting_for/</guid>
      <pubDate>Fri, 05 Jan 2024 04:22:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练量化视频压缩自动编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yxc7z/p_training_a_quantized_video_compression/</link>
      <description><![CDATA[      该项目的目标是使用量化自动编码器结合其他可以提高解压缩视频质量的生成模型。该项目的动机是需要一种快速高效的压缩算法，可以处理来自网络的大量视频数据，而当前的硬编码算法无法满足这些数据（我的需求，因为我存储了大量数据）。此外，该项目通过集成类似 CLIP 的结构来调节帧，探索了该模型架构在长视频生成方面的潜力。 该模型是一个卷积量化自动编码器，由一个编码器和一个编码器组成。解码器。编码器使用 ConvNeXt resblocks 和常规卷积来缩小输入图像的尺寸，并生成压缩率为 1:16 的瓶颈向量。解码器使用 16 个代码组成的码本，每个代码有 4 个元素，从瓶颈向量重建输出图像。  网络分两个阶段进行训练。首先，它配备了 5000 次迭代的 L1 损失，以最小化输入和输出图像之间的像素差异。然后，结合 Wiener Loss、L1 Loss 和 Perceptual Loss（基于 timm 的 convnext_atto）进行优化： ​ per_Loss = nn.L1Loss()(PerceptualLoss(解码), PerceptualLoss(X)) rec_Loss = WienerLoss()(解码，X) * 0.15 + nn.L1Loss()(解码，X) * 0.7 + per_Loss * 0.15 ​ &lt; p&gt;感知损失衡量预训练网络的特征空间中输入和输出图像之间的相似性，而不是像素空间。维纳损失是一种频域损失，可解释解压缩图像中的噪声和模糊。  量化向量基于潜在正交正则化项的 Lucidrains 实现。  该数据集是 LAION 的子集（laion-art），我调整了大小并将其转换为灰度。 该项目仍处于早期阶段，因为视频压缩和解压缩的其他组件尚未开发。不过，我认为值得分享图像压缩的初步结果。  我将发布一系列压缩帧和原始帧，以便您可以看到重建的质量。欢迎任何反馈。 https： //preview.redd.it/ochkjduqnjac1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=9abcd5d87e5dac6653d851297a7c6f4389e226d1 此噪声来自预处理数据集时先前的填充 https://preview.redd.it/oujeoeuqnjac1.png?width= 1096&amp;format=png&amp;auto=webp&amp;s=2e8271a0115e792b2bced975628163b983813166 https://preview.redd.it/7flzhduqnjac1.png?width=1096&amp;format=png&amp;auto=webp&amp;s=9f5f5bac146d3c9c83c020b4eb6f0c 8c3d69978b  https://preview.redd.it/skay1fuqnjac1.png？ width=1095&amp;format=png&amp;auto=webp&amp;s=7b9d6e7f869d7ccf2d14da462c936c70c87c23ab   由   提交 /u/QLaHPD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yxc7z/p_training_a_quantized_video_compression/</guid>
      <pubDate>Fri, 05 Jan 2024 03:57:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] SOLAR 10.7B：通过简单而有效的深度扩展来扩展大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yx0rf/r_solar_107b_scaling_large_language_models_with/</link>
      <description><![CDATA[ 由   提交/u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yx0rf/r_solar_107b_scaling_large_language_models_with/</guid>
      <pubDate>Fri, 05 Jan 2024 03:40:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么高端 Apple Silicon CPU 几乎不比具有 Core ML 推理的低端 CPU 好？ [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yne1q/why_are_highend_apple_silicon_cpus_hardly_better/</link>
      <description><![CDATA[根据 Geekbench，所有 Apple Silicon CPU 的 Core ML 推理基准测试：Basic、Pro、Max、Ultra 都惊人地相似。 点击链接并在右侧菜单中选择 Geekbench ML 推理  https://browser.geekbench.com/search?utf8=%E2%9C%93&amp;q=Apple+M2 Geekbench Core ML 基准测试的目测分数： Core ML CPU 1500 - 2500 Core ML GPU 3000-8500 Core ML Neural Engine 6000-10000 自然，配备基本 M CPU 的 Macbook Air 处于较低端， Ultra CPU 处于高端，但在现实生活中差异可以忽略不计。我想，相当相似的性能可以部分解释为推理算法仅使用一个核心。然而，结果仍然令人惊讶，因为这些 CPU 的内存带宽要好几倍。 Macbook Air 的简单 M2 带宽为 100 GB/s，M2 Ultra 带宽为 800 GB/s。 如何解释这种相当相似的性能？   由   提交/u/Geejay-101  /u/Geejay-101 reddit.com/r/MachineLearning/comments/18yne1q/why_are_highend_apple_silicon_cpus_hardly_better/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yne1q/why_are_highend_apple_silicon_cpus_hardly_better/</guid>
      <pubDate>Thu, 04 Jan 2024 20:43:28 GMT</pubDate>
    </item>
    <item>
      <title>2023 年最好的深度学习论文是什么？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ykwdx/what_are_the_best_deep_learning_papers_of_2023d/</link>
      <description><![CDATA[2023 年最好的深度学习论文是什么？   由   提交/u/One_Definition_8975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ykwdx/what_are_the_best_deep_learning_papers_of_2023d/</guid>
      <pubDate>Thu, 04 Jan 2024 19:01:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] RL+LLM有哪些最新突破？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yj99j/d_what_are_the_latest_breakthroughs_in_rl_llms/</link>
      <description><![CDATA[RLHF 与 ChatGPT 的成功给我留下了深刻的印象，但除了这种风格调整之外，我还没有看到任何其他突破。我非常想探索这个领域还有哪些其他令人兴奋的突破或任何未开发的潜力。   由   提交 /u/SpecialBuy3271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yj99j/d_what_are_the_latest_breakthroughs_in_rl_llms/</guid>
      <pubDate>Thu, 04 Jan 2024 17:55:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 放弃 ML 博士 - 建议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yh4ph/d_dropping_out_ml_phd_advice/</link>
      <description><![CDATA[我即将开始博士学位的第三年。我有 3 篇第一作者论文，还有 2 篇正在审稿中，今年夏天我已经准备好进行扎实的研究实习。但是……老实说，我根本不喜欢研究，从来没有，也不太关心。在过去的三年里，我勉强做到了这一点，老实说，我非常非常幸运。我绝不是一个研究天才，甚至不喜欢研究。我只是在乘风破浪，打发时间。但这种完全无意义和绝望的感觉，我无法克服。我只是感觉不适合作为一名研究员。这不是冒名顶替综合症。研究不是我的事。  老实说，我读博士课程只是为了满足我的家人。来自一个拥有研究生学位的亚洲家庭，这是一种期望。  20年前的博士学位看起来很有趣。我想象博士课程就是我和同事一起在白板上讨论，提出想法并尝试疯狂的事情，总是参加研讨会和课程。相反，我看到的是士气低落、过度劳累的学生、空荡荡的教室和研讨会（!!!），以及普遍的绝望感和不想去那里的感觉。这对我来说太震惊了。 现在退学是不是很愚蠢？我觉得我的20多岁已经在无聊、完全没有动力和沮丧中消逝了。我的导师是一个很棒的人，但几乎没有时间见面。我只是不知道我是否还能忍受这个。我想尝试一些疯狂的事情：去一家初创公司并取得成功或为此而奋斗，获得 MBA 或统计学硕士学位，搬到一个新城市，成为一名人工智能政策分析师。感觉有很多路我更适合。  编辑：哇。感谢大家的回复和源源不断的动力。老实说，我没想到会收到这么多评论。我很快就会和我的导师交谈，并安排一次长时间的一对一会议，看看我们能做些什么让我带着博士学位离开这里:)  &amp; #32；由   提交 /u/TheMysticalJam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yh4ph/d_dropping_out_ml_phd_advice/</guid>
      <pubDate>Thu, 04 Jan 2024 16:27:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪些技术技能将使您在机器学习就业市场中脱颖而出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18y888t/d_which_tech_skills_will_make_you_a_standout_in/</link>
      <description><![CDATA[框架、编程语言、算法等？   由   提交/u/Born-Comment3359  /u/Born-Comment3359 reddit.com/r/MachineLearning/comments/18y888t/d_which_tech_skills_will_make_you_a_standout_in/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18y888t/d_which_tech_skills_will_make_you_a_standout_in/</guid>
      <pubDate>Thu, 04 Jan 2024 08:27:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>