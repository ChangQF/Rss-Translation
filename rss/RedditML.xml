<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 22 Jan 2024 21:12:27 GMT</lastBuildDate>
    <item>
      <title>[D] 用于文本分类的零样本 OOD</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d5ytn/d_zeroshot_ood_for_text_classification/</link>
      <description><![CDATA[我正在构建一个管道，该管道允许我根据文本是否属于我已经定义的任何类来过滤文本定义。 我觉得一种（尽管很幼稚）方法就是嵌入文本和代表类的文本，并对两者应用距离函数，如果距离超过某个值，则丢弃样本阈值。 这在零样本设置中可行吗？如果是这样，我应该如何计算阈值？如果没有，在零样本设置中可以使用什么（如果有）方法？   由   提交/u/DeezDineros   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d5ytn/d_zeroshot_ood_for_text_classification/</guid>
      <pubDate>Mon, 22 Jan 2024 20:50:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 目前理论机器学习作为一个领域有什么意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d4um9/d_is_there_any_point_to_theoretical_ml_as_a_field/</link>
      <description><![CDATA[随着 SOTA 架构不断变化的极快速度，可能的 DL 技术（正则化、所有不同的激活和损失函数）的多样性如下：以及对可解释人工智能相对退居二线的担忧，现在从事理论机器学习工作有什么用处吗？ 大多数 SOTA 架构似乎只是大规模扩展的高级猜测和检查，而且它确实有效就基准性能而言，我们是否需要 ML/DL 理论？   由   提交/u/Bchalup2348   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d4um9/d_is_there_any_point_to_theoretical_ml_as_a_field/</guid>
      <pubDate>Mon, 22 Jan 2024 20:04:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] Lookahead：具有无损生成精度的大型语言模型推理加速框架 - Ant Group 2024 - 推理加速 2-5 倍！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d4ofr/r_lookahead_an_inference_acceleration_framework/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2312.12728v2 Github：https:// github.com/alipay/PainlessInferenceAcceleration 摘要：  随着大型语言模型（LLM）在各种任务上取得了显着的进步，例如问答、翻译、文本摘要和对话系统中，对信息准确性的需求变得至关重要，特别是对于支付宝等为数十亿用户提供服务的严肃金融产品。为了解决这个问题，支付宝开发了检索增强生成（RAG）系统，为法学硕士提供最准确和最新的信息。然而，对于服务于数百万用户的现实产品来说，与单纯的实验模型相比，LLM 的推理速度成为一个关键因素。  因此，本文提出了一个加速推理过程的通用框架，从而大幅提高 RAG 系统的速度并降低成本，并具有无损生成精度。传统的推理过程中，每个 token 都是由 LLM 顺序生成的，导致时间消耗与生成 token 的数量成正比。为了增强这个过程，我们的框架（名为lookahead）引入了多分支策略。我们提出了一种基于 Trie 的检索（TR）过程，而不是一次生成一个令牌，该过程可以同时生成多个分支，每个分支都是一个令牌序列。随后，对于每个分支，执行验证和接受（VA）过程以识别最长的正确子序列作为最终输出。我们的策略具有两个明显的优势：(1) 它保证输出的绝对正确性，避免任何近似算法，(2) 我们的方法的最坏情况性能相当于传统过程。我们进行了广泛的研究实验来证明通过应用我们的推理加速框架所实现的显着改进。   https:/ /preview.redd.it/rco4uvsoq1ec1.jpg?width=1533&amp;format=pjpg&amp;auto=webp&amp;s=09603afe3dce87f7ad91f05304d0c4f1a3e7fc72 https://preview.redd.it/7oqviwsoq1ec1.jpg?width=1528&amp;format=pjpg&amp;auto=webp&amp; ;s =5ca1a13bd021d6839b4227a00e8a3484d4ea8059 https ://preview.redd.it/4pfgexsoq1ec1.jpg?width=1265&amp;format=pjpg&amp;auto=webp&amp;s=934e82ab87742dacf271491b2a5dbd542e1e336b https://preview.redd.it/qvzbxvsoq1ec1.jpg?width=948&amp;format=pjpg&amp;auto=网络p&amp;amp; ;s=a486911922cd3e1731f78b9c4308360f82512d8c   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d4ofr/r_lookahead_an_inference_acceleration_framework/</guid>
      <pubDate>Mon, 22 Jan 2024 19:57:35 GMT</pubDate>
    </item>
    <item>
      <title>[D]微调“知识”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d31u8/d_fine_tuning_knowledge/</link>
      <description><![CDATA[所以这可能是一个愚蠢的问题，但请耐心等待。 作为语言模型，LLM 能够对样式进行编码，而且还对“知识”进行编码- 例如事实信息、日期、事件等 到目前为止我发现的所有微调材料都是关于微调输出格式 - 代码或 json 或特定的写作风格，例如我想微调模型向其添加更多知识，但不一定修改输出样式。这实际上可能/可行吗？ 我目前正在为此目的使用 RAG，但它会增加大量延迟，并且特定数据集实际上是不可变的，因此使用它感觉不对）   由   提交 /u/hervalfreire   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d31u8/d_fine_tuning_knowledge/</guid>
      <pubDate>Mon, 22 Jan 2024 18:49:38 GMT</pubDate>
    </item>
    <item>
      <title>[R]双重认知架构：结合偏见和多记忆系统实现终身学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d2c65/r_dual_cognitive_architecture_incorporating/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.11341 OpenReview：https:// /openreview.net/forum?id=PEyVq0hlO3 代码： https://github.com/NeurAI-Lab/DUCA 数据集：https://github.com/NeurAI-Lab/DN4IL-dataset 视频：https://www.youtube.com/watch?v=08tfpjvUGqs 摘要：  人工神经网络（ANN）在固定独立数据上表现出狭窄的专业知识范围。然而，现实世界中的数据是连续的、动态的，人工神经网络必须适应新的场景，同时保留学到的知识，成为终身学习者。人类在这些任务上表现出色的能力可以归因于多种因素，包括认知计算结构、认知偏差和大脑中的多记忆系统。我们结合了其中的关键概念来设计一个新颖的框架，双重认知架构（DUCA），其中包括多个子系统、隐式和显式知识表示二分法、归纳法偏见和多记忆系统。 DUCA 中的归纳偏差学习器有助于编码形状信息，有效对抗 ANN 学习局部纹理的倾向。同时，语义记忆子模块的包含有助于知识的逐步巩固，复制在快速和慢速学习系统中观察到的动态，让人想起支撑人类认知中互补学习系统的原理。 DUCA 在不同的设置和数据集上显示出改进，并且还表现出减少的任务新近度偏差，而不需要额外的信息。为了进一步测试终身学习方法在具有挑战性的分布变化上的多功能性，我们引入了一种新颖的领域增量数据集DN4IL。除了提高现有基准测试的性能之外，DUCA 还在这个复杂的数据集上展示了卓越的性能。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d2c65/r_dual_cognitive_architecture_incorporating/</guid>
      <pubDate>Mon, 22 Jan 2024 18:20:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 最大化探索：融合估计、规划和探索的一个目标函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d20kd/r_maximize_to_explore_one_objective_function/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2305.18258 OpenReview：https:// /openreview.net/forum?id=A57UMlUJdc 代码：https： //github.com/agentification/MEX 摘要：  在线强化学习（在线RL）中，平衡探索开发对于以样本有效的方式找到最优策略至关重要。为了实现这一目标，现有的样本高效在线强化学习算法通常由三个部分组成：估计、规划和探索。然而，为了应对通用函数逼近器，它们中的大多数都涉及不切实际的算法组件来激励探索，例如数据相关水平集内的优化或复杂的采样程序。为了应对这一挑战，我们提出了一种易于实现的强化学习框架，称为最大化探索 (MEX)，它只需要优化 &lt; em&gt;不受约束的单一目标，集成了估计和规划组件，同时自动平衡勘探和开发。理论上，我们证明 MEX 通过马尔可夫决策过程（MDP）的一般函数逼近实现了次线性遗憾，并且可以进一步扩展到两人零和马尔可夫游戏（MG）。同时，我们采用深度 RL 基线以无模型和基于模型的方式设计 MEX 的实用版本，在各种奖励稀疏的 MuJoCo 环境中，它可以稳定地优于基线。与现有的具有一般函数逼近的样本高效在线强化学习算法相比，MEX 实现了相似的样本效率，同时具有更低的计算成本，并且与现代深度强化学习方法更加兼容。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d20kd/r_maximize_to_explore_one_objective_function/</guid>
      <pubDate>Mon, 22 Jan 2024 18:07:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 回归/分类中日期时间观察的顺序？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19d1jye/d_order_of_datetime_observations_in/</link>
      <description><![CDATA[人们一直说顺序对于使用 XGBoost 或模拟的日期时间分类或回归模型很重要。 但是，从我的观点来看了解 XGBoost 不使用 df 索引。它还一次只查看一个观察结果。 所以我认为顺序并不重要。一旦您完成了数据工程并按顺序在数据上创建了任何滞后变量，您应该能够将它们打乱，对吗？ 我确实理解您不想将模型应用到训练数据日期线索引之前或期间的观察结果。   由   提交/u/Jintorna   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19d1jye/d_order_of_datetime_observations_in/</guid>
      <pubDate>Mon, 22 Jan 2024 17:49:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 提前停止但是什么时候？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19cx4ol/d_early_stopping_but_when/</link>
      <description><![CDATA[你好， 我最近一直在尝试寻找比耐心和增量值更好的提前停止方法，并且我偶然发现这篇论文 https://page.mi.fu-berlin.de/prechelt/Biblio/ stop_tricks1997.pdf 。鉴于本文中提到的标准，我发现继续采用这种方法是非常合乎逻辑的。我还碰巧注意到这是一篇非常古老的论文，似乎没有一个主要平台考虑这里的实现。我完全不明白为什么这不是一个有效的方法吗？    由   提交/u/Bhargav_28   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19cx4ol/d_early_stopping_but_when/</guid>
      <pubDate>Mon, 22 Jan 2024 14:41:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们没有看到很多关于 Mamba 架构的内容？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19cwf65/d_why_were_not_seeing_a_lot_of_content_about/</link>
      <description><![CDATA[比如对作者的一些采访？还没有看到例如TWIML AI 播客谈论 Mamba 架构。   由   提交/u/_learning_stuff_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19cwf65/d_why_were_not_seeing_a_lot_of_content_about/</guid>
      <pubDate>Mon, 22 Jan 2024 14:07:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] ARR 2023 年 12 月（NAACL 2024）讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19cvxz4/d_arr_2023_december_naacl_2024_discussion/</link>
      <description><![CDATA[评论应该在今天发布。   由   提交 /u/Street-Judgment7640    reddit.com/r/MachineLearning/comments/19cvxz4/d_arr_2023_december_naacl_2024_discussion/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19cvxz4/d_arr_2023_december_naacl_2024_discussion/</guid>
      <pubDate>Mon, 22 Jan 2024 13:44:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 超越变形金刚：结构化状态空间序列模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19cv8q6/d_beyond_transformers_structured_state_space/</link>
      <description><![CDATA[撰写了一篇文章，解释了状态空间序列模型的基础知识。本文的目的是以简化的方式呈现基础级别的概念。该领域在人工智能领域正在迅速发展，因为它在速度和内存消耗方面超越了 Transformer 架构。以下是文章链接：https://cnichkawde.github.io/statespacesequencemodels.html &lt; /div&gt;  由   提交 /u/cnichkawde   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19cv8q6/d_beyond_transformers_structured_state_space/</guid>
      <pubDate>Mon, 22 Jan 2024 13:07:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] GPT-4V API 如何处理大图像？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ctr9p/r_how_does_the_gpt4v_api_deal_with_large_images/</link>
      <description><![CDATA[我想将不同尺寸的信息图表传递给 GPT4V 模型。我不确定要设置什么尺寸以及如何使我的成本尽可能低。这些图像可以变得非常大，达到 5000 像素范围，并且也可以具有不同的像素比。 - 我应该考虑什么设置？ - 我将相同的图像输入到 ChatGPT Plus，它表现良好，但不知何故我似乎无法找出 OpenAI API 的适当设置。 PS：如果您能帮助我解决 Llava、Bakllava、Blip2、InstructBLIP 等多模式模型的问题，我将不胜感激&lt; /p&gt;   由   提交 /u/Conclusion_Silent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ctr9p/r_how_does_the_gpt4v_api_deal_with_large_images/</guid>
      <pubDate>Mon, 22 Jan 2024 11:42:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我写了一篇关于 LLM 评估指标的所有知识的文章</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19csc4x/d_i_wrote_an_article_on_everything_i_know_about/</link>
      <description><![CDATA[大家好，过去 6 个月我一直在 LLM 评估领域不间断地工作，从培训定制 LLM 评估到构建评估OpenAI 的 GPT 模型之上的指标。我写了一篇长文，介绍了我所知道的有关 LLM 评估指标的一切，我希望有人觉得这篇文章有用，可能是出于兴趣，也可能是在工作中。如果您觉得它有用或有任何问题/建议，请告诉我！ 以下是文章的链接：https://medium.com/@jeffreyip54/llm-evaluation-metrics-everything-you-need-for-llm-evaluation-6b129157e33c&lt; /a&gt; 谢谢！   由   提交/u/Ok_Constant_9886   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19csc4x/d_i_wrote_an_article_on_everything_i_know_about/</guid>
      <pubDate>Mon, 22 Jan 2024 10:05:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 chatGPT 之后，人们现在还在创建自己的新的自定义 NLP 模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19cqde6/d_after_chatgpt_are_people_still_creating_their/</link>
      <description><![CDATA[对使用 scikit-learn 和 Tensorflow 训练 ML 和 DL 模型有点脱节。只是想知道 ML 工程师是否仍在训练他们自己的 NLP 模型（甚至 CV、预测、聚类模型等）。 如果是这样，您正在训练什么类型的模型？您正在解决哪些用例？如果您用 ChatGPT 替换自定义模型，进展如何？ 我想重新熟悉 ML 生态系统。很想听听您的想法。   由   提交 /u/automatonv1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19cqde6/d_after_chatgpt_are_people_still_creating_their/</guid>
      <pubDate>Mon, 22 Jan 2024 07:41:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>