<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 13 Dec 2024 01:22:56 GMT</lastBuildDate>
    <item>
      <title>[D] LSTM 模型实现和近似问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/</link>
      <description><![CDATA[对于一个项目，我目前正在尝试集成一个用于特征提取的自动编码器和一个用于对缩小的特征空间进行分类的 LSTM。我遇到的问题是如何训练 LSTM 网络。AE 生成 5 个数据点，这些数据点被输入到 LSTM 网络中。现在的诀窍在于 LSTM 网络的训练以及 LSTM 的工作原理。我希望 LSTM 考虑来自 AE 在时间 t 的 5 个参数以及 t-1 和 t-2 的参数。据我所知，LSTM 会自动执行此操作，还是应该让 LSTM 总共接受 15 个参数，每对 5 个参数对应 AE 的一个时间步？ 任何关于 LSTM 的建议都很好，或者如何以有效的方式进行此类训练。AE 正在处理时间序列信号。   由    /u/Sea_Onion41  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/</guid>
      <pubDate>Thu, 12 Dec 2024 21:09:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将已接受的会议论文上传到 ArXiv 的“正确”方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/</link>
      <description><![CDATA[我们最近有一篇论文被会议 (AAAI) 接受。我们发现会议不发布附录，所以他们建议我们将整篇论文（包括附录）上传到 arXiv。无论如何，我们都在考虑这样做，因为论文将在会议论文集发布之前发布。 我担心的是，如果有人决定引用我们的工作，他们可能会感到困惑，或者引用 arXiv 而不是 AAAI“版本”。 有没有“正确”或常见的方法来处理这个问题？具有相同标题的 arXiv 上传是否会在 Google 学术上被索引到“一份手稿”？ 此外，我们可以使用会议模板上传吗？ （我想这部分可能与会议有关）。 我知道现在在收到会议回复之前上传到 arXiv 是很常见的（通常标题不同），但我认为这是一种略有不同的情况，因为论文被接受并且上传的版本将与会议论文相同（尽管带有附录）。 提前谢谢您！    提交人    /u/baghalipolo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/</guid>
      <pubDate>Thu, 12 Dec 2024 20:38:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从聚合计算中缩放数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/</link>
      <description><![CDATA[您好，我有一个项目，用于检测以太坊区块链交易数据的异常。我对每个钱包地址执行了聚合计算（例如，交易值​​的最小值、最大值、中位数、总和、众数），并创建了单独的数据文件。我已经合并了所有交易的数据。现在我必须在机器学习之前标准化数据（我选择了稳健缩放），但我对此主题有以下疑问：  我是否应该根据每个特征的唯一平均值和 iqr 来标准化每个特征？或者对计算所来自的列（值列）执行缩放，然后使用其平均值和 iqr 来缩放计算列？ 如果每个特征都是根据其自己的平均值和 iqr 缩放的，我应该在合并计算数据之前还是之后进行？     提交人    /u/Wikar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/</guid>
      <pubDate>Thu, 12 Dec 2024 20:32:01 GMT</pubDate>
    </item>
    <item>
      <title>从病毒和物质到星系及其他：机器学习在科学发现中扮演的角色</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcubv2/from_viruses_and_materials_to_galaxies_and_beyond/</link>
      <description><![CDATA[        由    /u/SlothSpeedRunning  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcubv2/from_viruses_and_materials_to_galaxies_and_beyond/</guid>
      <pubDate>Thu, 12 Dec 2024 20:21:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 最佳论文奖得主破坏了其他团队</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/</link>
      <description><![CDATA[据推测，NeurIPS 2024 最佳论文奖的获得者（来自字节跳动，抖音的创造者）破坏了其他团队的研究，并将他们的资源转移到他自己的团队。此外，他在会议上调试同事的代码，所以他总是领先一步。有人呼吁撤回他的论文。 https://var-integrity-report.github.io/ 我还没有核实事实本身，所以如果你能验证断言的内容，如果这是真的，那就太好了。    提交人    /u/LelouchZer12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/</guid>
      <pubDate>Thu, 12 Dec 2024 19:41:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 英特尔 gpu 是否支持 ROCm 或 AMD 卡是否支持英特尔？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/</link>
      <description><![CDATA[我找不到这些信息，如果两者都是开源的，那么兼容层就有意义了，两者中的任何一个都已经移植到其他平台了？如果你能分享有关 nvidia 的信息就太酷了    提交人    /u/mrnothing-   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/</guid>
      <pubDate>Thu, 12 Dec 2024 19:39:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 重新思考对比学习中的正对</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/</link>
      <description><![CDATA[大家好，我正在分享我最近的工作，该工作允许任意图像成为正对。我们的发现非常令人惊讶，两个完全不同的图像，例如一条蛇和一盏灯，可以是正对。我们的工作可能拓宽对比学习的应用，以处理两个视图不相似的“假阳性”。 我们挑战对比学习中的常识，即正对设计至关重要。我们的结果证明特征选择是关键！ 论文：https://arxiv.org/abs/2410.18200    提交人    /u/Miserable-Gene-308   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/</guid>
      <pubDate>Thu, 12 Dec 2024 17:02:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] TikTok 的推荐算法为何如此强大？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/</link>
      <description><![CDATA[一般讨论 - 现在他们即将在美国被禁止，我对他们的 For You 推荐的力度越来越着迷。为了尝试对我的意思设置一些护栏，TikTok 已经证明自己能够以比任何其他应用程序（包括 YouTube）更高的频率和规模将内容与相关受众相匹配。许多创作者可以加入该平台，发布一个视频，并在 24 小时内获得数百万的观看次数。这在其他应用程序上确实会发生，但 TikTok 似乎是最能以惊人的速度扩大受众群体的应用程序。 他们的系统可能基于什么模型？他们的模型中的哪些方面创造了他们的竞争优势？    提交人    /u/No_Collection_5509   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/</guid>
      <pubDate>Thu, 12 Dec 2024 16:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 宠物项目 - 风格转换神经网络实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/</link>
      <description><![CDATA[      嗨，我正在学习 ML，这是我的第一个项目。我对 Gatys 等人的 Neural Style Transfer 论文进行了 100 LoC 的简单实现。参见https://github.com/TAOGenna/pytorch-neural-style-transfer https://preview.redd.it/x2udi76n2g6e1.jpg?width=939&amp;format=pjpg&amp;auto=webp&amp;s=437bdda1683e9fd580a6b3d1d4dc2598b25079ff    由   提交  /u/TAO_genna   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/</guid>
      <pubDate>Thu, 12 Dec 2024 16:25:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 ResNet 和极深网络的可扩展性的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/</link>
      <description><![CDATA[我一直在探索 ResNet 的架构及其有效训练非常深的神经网络的能力。虽然我知道残差连接有助于缓解梯度消失等问题并使训练更深的网络成为可能，但我很好奇这种方法在扩展到极深的网络（例如具有 1000 层或更多层的网络）时的局限性。 据我所知，由于残差连接，具有 100 层的 ResNet 可能有效地像一个小得多的网络一样运行，这本质上是“跳过”层并添加输出。但是，这是否也意味着如果常规 MLP 难以扩展到 15 层以上，ResNet 可能会按比例转移这个限制（例如，难以超过 150 层）？换句话说，ResNet 是否从根本上解决了训练极深网络的问题，还是仅仅扩展了问题开始重新出现的深度？ 如果您有任何见解，我将不胜感激！非常感谢！    由   提交  /u/Time_Celebration6058   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/</guid>
      <pubDate>Thu, 12 Dec 2024 15:54:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 红队扎根理论研究：动机、策略和技巧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/</link>
      <description><![CDATA[本文基于对从业人员的访谈，对如何在大型语言模型 (LLM) 上进行红队攻击进行了扎根理论研究。研究人员系统地分析了从业人员的方法，以确定 LLM 红队攻击中的常见模式、策略和动机。 关键技术要点： - 使用访谈的定性编码来开发红队攻击方法的分类 - 确定了 12 种不同的攻击策略和 35 种特定技术 - 发现红队攻击需要手动操作而不是自动化 - 证明了团队协作相对于个人尝试的重要性 - 确立了红队攻击与恶意攻击的区别 - 绘制了测试人员动机和目标的常见模式 主要结果： - 红队攻击策略分为提示操纵、基于心理的攻击和系统极限测试等类别 - 成功的测试人员采用“炼金术士”系统实验的心态 - 大多数从业者的动机是好奇心和安全问题 - 测试需要对技术和心理方面的深入了解 - 手动测试目前比自动化方法更有效 我认为这项工作为开发更结构化的 LLM 安全测试方法奠定了重要基础。他们开发的分类法可以帮助标准化我们评估和保护这些系统的方式。他们发现手动测试仍然优于自动化，这表明我们需要在自动化测试方法上做更多的工作。 我认为，随着这些系统的部署越来越广泛，强调非恶意意图和安全动机尤为重要。了解人们如何以及为何进行这些测试有助于区分合法的安全研究和攻击。 TLDR：首次系统地研究 LLM 红队实践，根据从业者访谈提供策略和技术分类。显示了手动测试和团队协作的重要性，同时将红队训练确立为合法的安全研究。 完整摘要在此处。论文此处。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/</guid>
      <pubDate>Thu, 12 Dec 2024 13:56:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 的知识扩展，实现跨域内容的生成（训练数据集之外）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcke77/r_llms_knowledge_expansion_to_enable_generation/</link>
      <description><![CDATA[  由    /u/ankitm1  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcke77/r_llms_knowledge_expansion_to_enable_generation/</guid>
      <pubDate>Thu, 12 Dec 2024 12:50:27 GMT</pubDate>
    </item>
    <item>
      <title>[N] 为 Liger-Kernel 中的 DPO 和 ORPO 节省 80% 的内存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/</link>
      <description><![CDATA[在 Liger Kernel 中引入第一个开源优化的训练后损失，内存减少约 80%，具有 DPO、CPO、ORPO、SimPO、JSD 等功能，通过更大的批量大小实现高达 70% 的端到端加速。将其用作任何 PyTorch 模块 - 今天在 Liger v0.5.0 中可用！ https://x.com/hsu_byron/status/1866577403918917655    提交人    /u/Icy-World-8359   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/</guid>
      <pubDate>Thu, 12 Dec 2024 06:18:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 08 Dec 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>