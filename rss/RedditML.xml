<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 23 May 2024 09:16:21 GMT</lastBuildDate>
    <item>
      <title>[P] ReproModel：开源机器学习研究工具箱。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</link>
      <description><![CDATA[嗨，我是计算机科学博士，我刚刚开发出了我认为机器学习研究的巨大飞跃。我开源了该应用程序供大家查看，欢迎所有反馈和贡献。 ReproModel 是一个无代码工具箱，使科学家和研究人员能够有效地测试和重现 ML 模型。很大一部分研究时间都浪费在测试现有论文中的模型上。要复制或测试结果，您必须查看提供的代码，并模仿所有配置文件和实验条件，即数据加载器、预处理、优化器等。 工具箱将所有这些都拿走了通过从现有论文中获取配置文件（即将推出），直接加载模型，并通过简单的复选框和下拉菜单在数据上测试它们。当然，定制是可能的并且受到鼓励。 您可以在此处找到存储库。当然，还需要做更多的工作，但我正在逐步实现这一点，以确保代码未来的兼容性和可重用性。 https://github.com/ReproModel/repromodel 感谢您的时间、评论和支持！   由   提交 /u/MintOwlTech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</guid>
      <pubDate>Thu, 23 May 2024 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有任何网站或 youtube 可以从头开始解释 Alphafold2 的 Evoformer？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyna0s/d_is_there_any_site_or_youtube_that_explains/</link>
      <description><![CDATA[alphaflod github 中的原始源代码对我来说太长太复杂了。 如果没有这些网站和 YouTube 视频，那么请让我知道如何查看这些太大的代码。 你有什么秘密方法可以做到这一点吗？ &lt;!-- SC_ON - -&gt;  由   提交 /u/Sorry-Library-1269   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyna0s/d_is_there_any_site_or_youtube_that_explains/</guid>
      <pubDate>Thu, 23 May 2024 07:45:47 GMT</pubDate>
    </item>
    <item>
      <title>[项目] YOLOv8量化项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</link>
      <description><![CDATA[我在 Jetson Orin Nano 中量化了 YOLOv8。我用 TensorRT (FP16, INT8) 将其导出并比较了性能。基于 YOLOv8s，基础模型的 mAP50-95 为 44.7，推理速度为 33.1 ms。用 TensorRT (FP16) 导出的模型显示 mAP50-95 为 44.7，推理速度为 11.4 ms。用 TensorRT (INT8) 导出的模型显示 mAP50-95 为 41.2，推理速度为 8.2 ms。mAP50-95 略有损失，但推理速度大幅降低。用 TensorRT (INT8) 导出时校准存在问题，但通过增加校准数据将 mAP50-95 的损失降到最低。我对 YOLOv8 的所有基础模型以及 YOLOv8s 进行了测试。 https://github.com/the0807/YOLOv8-ONNX-TensorRT    提交人    /u/Loud-Insect9247   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</guid>
      <pubDate>Thu, 23 May 2024 06:39:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要深入了解模型选择</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cym2sp/d_need_insight_on_model_selection/</link>
      <description><![CDATA[需要使用此数据https://www.kaggle.com/competitions/2023-travelers-ness-statathon/overview 遇到业务问题：您在 Travelers Insurance Company 的欺诈检测部门担任建模师。您的同事不熟悉统计数据，希望您根据历史索赔数据创建预测模型。您的团队关心欺诈检测的准确性以及导致欺诈的关键驱动因素。 请建议一些预测模型  &amp;# 32；由   提交/u/kodehead_me  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cym2sp/d_need_insight_on_model_selection/</guid>
      <pubDate>Thu, 23 May 2024 06:21:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 白盒模拟数据的多元时间序列回归/替代中的 Mamba 上下文大小权衡</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyiafd/d_mamba_context_size_tradeoffs_in_multivariate/</link>
      <description><![CDATA[大家好 - 我正在研究一个多变量时间序列回归问题，特别是开发一个替代（非常慢）基于物理的模拟。 输入： 34 个特征 x 35040 个时间步（一年 15 分钟间隔数据）+ 90 个分类特征（即整个静态）年），每个类别有 4-56 个选项。  输出： 8 个特征 x 35040 个时间步 我的基本方法是这样的：  （b，90 个特征） &gt;嵌入层&gt; MLP 网络（几层、间歇性跳过连接等）&gt; (b，96个时间步长，m个特征)即为一天的每个间隔创建分类特征的唯一潜在表示&gt; 1。重复 x 365（达到 35040 个时间步） 将分类特征的潜在时间序列表示与 34 个时间序列特征连接起来。  mamba 的 d_model 为 m + 34（当前使用 m=94，因此 d_model 为 128） 根据所需的上下文大小对时间序列进行分块，并通过 mamba 和最终线性发送层混合到输出中的最终时间序列数。使用单个块意味着全年都会在一次中处理，而使用 73 个块则相当于 5 天。   现在，有趣的问题是如何设置 Mamba 的上下文/分块大小。从物理学的角度来看，最大因果范围（正确的术语？）可能约为一天 - 即过去 24 小时内输入和系统的状态与计算白盒求解器中的下一个时间步相关，但除此之外过去不会影响系统。然而，由于时间序列输入/时间步长在数据集中不是 IID/均匀随机（理论上它们可能是数学上的，但使用了实际数据），因此当然有可能（并且至少在某种程度上）时间步长之间共享的相互信息超出因果地平线。  我得到了相当不错的结果，上下文大小为 480 个时间步长（5 天），并在每个块之后进行反向传播（在推理过程中，以相同大小的块处理一整年），但我不&#39;除了它如何影响训练期间的 VRAM 要求之外，对于此处使用更大或更小的上下文大小的权衡没有任何良好的直觉。  我很乐意在这里开始讨论它！ 或者这个架构中的任何其他内容。如果您有任何好的论文实现了类似的方法来组合分类和时间序列数据，我很乐意看到它！   由   提交/u/hangingonthetelephon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyiafd/d_mamba_context_size_tradeoffs_in_multivariate/</guid>
      <pubDate>Thu, 23 May 2024 02:37:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] Geoff Hinton 目前对反向传播作为大脑学习机制的看法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</link>
      <description><![CDATA[我大约 4 年前观看了他的一次演讲，他在演讲中驳斥了所有反对反向传播作为大脑学习机制的观点。但我记得最近在一个播客上听到过他（现在找不到了），其中他对反向传播持怀疑态度，并且似乎暗示赫布学习更为重要。我很想知道他目前的信念以及原因。他最近一次讨论这个问题的采访或讲座是什么？ /u/geoffhinton 编辑：这是我提到的讲座，名为“大脑会进行反向传播吗？”   由   提交/u/guesswho135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</guid>
      <pubDate>Thu, 23 May 2024 01:58:50 GMT</pubDate>
    </item>
    <item>
      <title>未能复制“深度残差学习”“[P]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/</link>
      <description><![CDATA[   大家好， 出于学习目的，我一直在复制Kaiming He 2015 年“图像识别的深度残差学习”中的方法。我已经构建了受 VGG 启发的 plain-CNN 以及 ResNet 架构（标准和瓶颈）。 但是，我无法复制出版物中强调的退化（准确性饱和）问题。出版物中的错误百分比数字显示，随着训练的进展，错误百分比明显下降，然后停滞不前。 我的数字似乎停滞不前，但很明显，该模型对验证数据的泛化能力非常糟糕。我附上了他们的一张图作为参考。有什么建议可以更好地复制本文中的错误率饱和度吗？注：对于何凯明图，粗线为测试误差&amp;虚线表示训练。 参数：  162 个 Epoch，批量大小为 128，进行 64k 次迭代。 Lr：0.1 &lt; li&gt;动量：0.9 权重衰减：0.0001 多步调度器在 32k 和 48k 迭代时将 lr 除以 10  我的%-错误 &lt; p&gt;来自论文   由   提交 /u/AnotherBotIGuess   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/</guid>
      <pubDate>Wed, 22 May 2024 22:48:34 GMT</pubDate>
    </item>
    <item>
      <title>【研究】理解 Claude 3 Sonnet 中的稀疏自动编码器如何影响实际的人工智能应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/</link>
      <description><![CDATA[我最近读了论文“Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet”由人类。该研究探讨了稀疏自动编码器如何从 Transformer 模型中提取可解释的、多语言和多模态的特征。  https://transformer- Circuits.pub/2024/scaling-monosemanticity/index .html - 论文链接 鉴于这些功能会影响特定类型数据（如文本或图像）的检测和生成，我很好奇此功能的实际应用：  这种级别的特征理解如何帮助在不进行大量再训练的情况下为特定任务定制模型输出？例如，我们能否在基于已识别特征的部署过程中更有效地引导模型？这可以消除/识别/减轻偏见吗？    由   提交 /u/mamphii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/</guid>
      <pubDate>Wed, 22 May 2024 21:35:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mistral-7B-v0.3 指令与 Llama-3 8B 指令在医学领域的评估</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy7no2/d_mistral7bv03_instruct_vs_llama3_8b_instruct/</link>
      <description><![CDATA[&lt;表&gt;       https://preview.redd.it/wzkhd6k1v02d1.png?width=571&amp;format=png&amp; ;auto=webp&amp;s=0733a90b13450d48721f6eced9e70dfe2028bf93   由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy7no2/d_mistral7bv03_instruct_vs_llama3_8b_instruct/</guid>
      <pubDate>Wed, 22 May 2024 18:35:07 GMT</pubDate>
    </item>
    <item>
      <title>[D]机器学习/深度学习如何应用于金融交易？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy6ljc/dhow_is_machine_learningdeep_learning_being_used/</link>
      <description><![CDATA[我喜欢这一集，讨论深度学习和机器学习在金融领域的作用。金融领域真正的机器学习/深度学习现在被认为是多种金融服务和应用程序的关键方面，包括管理资产、评估风险水平、计算信用评分，甚至批准贷款。 https://podcasters.spotify.com/ pod/show/ai-x-podcast/episodes/Deep-Learning-for-Financial-Trading-with-Sofien-Kaabar-e2i4q0c  &amp; #32；由   提交/u/Data_Nerd1979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy6ljc/dhow_is_machine_learningdeep_learning_being_used/</guid>
      <pubDate>Wed, 22 May 2024 17:53:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你希望在 /r/machinelearning 中看到更多或更少的内容吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy5ldu/d_what_would_you_like_to_see_more_or_less_of_in/</link>
      <description><![CDATA[我无法设置投票。但我想我会加入这个社区。 我先开始，我希望原创研究人员能够发布他们的论文并回答问题。    提交人    /u/20231027   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy5ldu/d_what_would_you_like_to_see_more_or_less_of_in/</guid>
      <pubDate>Wed, 22 May 2024 17:12:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 比较 cnn、局部连接和密集连接模型的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy515k/r_paper_comparing_cnn_locally_connected_and/</link>
      <description><![CDATA[不久前我读了一篇论文，该论文对卷积模型和局部连接模型进行了比较（与 cnn 相同，但没有参数共享，因此每个模型图像定位具有不同的内核）和密集连接的模型。具体来说，他们使每个模型都具有相同的激活次数，其中大多数比较保持参数不变。这是一篇很酷的论文，因为它将每个模型在其可以表达的内容方面置于一个公平的竞争环境中，并且这展示了如何将CNN的好处分解为局部性和参数共享。  但是我很难找到这篇论文。有人有什么建议吗？   由   提交 /u/MustachedSpud   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy515k/r_paper_comparing_cnn_locally_connected_and/</guid>
      <pubDate>Wed, 22 May 2024 16:49:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI 代理：太早、太昂贵、太不可靠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</guid>
      <pubDate>Wed, 22 May 2024 14:27:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] Fish Speech TTS：30 分钟克隆 OpenAI TTS</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cxwqb7/p_fish_speech_tts_clone_openai_tts_in_30_minutes/</link>
      <description><![CDATA[虽然我们仍在寻找改善代理对 OpenAI GPT-4o 的情绪反应的方法，但我们已经在调整 OpenAI 的 TTS 性能方面取得了重大进展。为了开始这个实验，我们收集了 10 个小时的 OpenAI TTS 数据，对 LLM（中）和 VITS 模型进行监督微调（SFT），大约花费了 30 分钟。之后，我们在推理过程中使用了 15 秒的音频作为提示。 可用演示：这里。 正如您所看到的，模型的情感、节奏、口音和音色与 OpenAI 扬声器相匹配，尽管音频质量有所下降，我们正在解决这个问题。为了避免任何法律问题，我们无法发布微调后的模型，但我相信每个人都可以调整 fish-speech几小时内就能达到这个水平，费用约为 20 美元。 我们的实验表明，只需 25 秒的提示（少量学习），无需任何微调，该模型就可以模仿大多数行为，除了音色等细节之外以及它如何读取数字。据我们所知，您可以使用此框架通过 30 分钟的数据克隆某人如何用英语、中文和日语说话。 存储库：https://github.com/fishaudio/fish-speech   由   提交/u/lengyue233  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cxwqb7/p_fish_speech_tts_clone_openai_tts_in_30_minutes/</guid>
      <pubDate>Wed, 22 May 2024 10:11:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>