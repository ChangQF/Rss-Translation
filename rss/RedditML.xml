<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 25 Apr 2024 18:17:28 GMT</lastBuildDate>
    <item>
      <title>[P] Dreambooting MusicGen</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</link>
      <description><![CDATA[使用此存储库，只需几分钟即可在小型消费级 GPU 上Dreambooth MusicGen 模型套件：https://github.com/ylacombe/musicgen-dreamboothing 该项目的目标是提供工具可以轻松微调和dreamboothMusicGen模型套件，只需很少的数据，并利用一系列优化和技巧来减少资源消耗，谢谢到 LoRA 适配器。 例如，模型可以是很好的 -调整特定的音乐流派或艺术家以给出以该给定风格生成的检查点。目的还在于轻松共享和构建这些训练有素的检查点， 具体来说，这涉及：  使用尽可能少的数据和资源可能的。我们正在讨论的是 A100 上的微调仅需 1500 万，GPU 利用率低至 10GB 至 16GB。 借助 Hugging Face Hub。 （可选）生成自动音乐描述 （可选）在 类似 Dreamambooth 的时尚，其中一个关键字会触发特定风格的生成  Wandb 的示例训练运行类似于此处。   由   提交 /u/Sufficient-Tennis189   /u/Sufficient-Tennis189  reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从运筹科学家转型为 ML/AI/CV 工程师</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccw5qf/d_transitioning_from_operations_research/</link>
      <description><![CDATA[Reddit 上的聪明人大家好，最近我一直在考虑换工作（我是一名视觉工程师），并且偶然发现了这个职位在 LinkedIn 上称为运筹科学家。我想知道在该职位工作几年（也许最多两年）后，我是否会更容易过渡到机器学习/人工智能工程师或计算机视觉工程师角色？   由   提交/u/unsuccessful_boy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccw5qf/d_transitioning_from_operations_research/</guid>
      <pubDate>Thu, 25 Apr 2024 16:20:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 推测流：无需辅助模型的快速 LLM 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvzyw/r_speculative_streaming_fast_llm_inference/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.11131 摘要：  推测性解码是一种加速推理的重要技术。基于辅助草稿模型预测的大型目标语言模型。虽然在特定于应用程序的设置中有效，但它通常需要微调草稿模型和目标模型以实现高接受率。随着下游任务数量的增加，这些草案模型显着增加了推理系统的复杂性。我们提出了推测流，这是一种单模型推测解码方法，通过将微调目标从下一个 token 预测更改为未来 n-gram 预测，将绘图融合到目标模型中。推测流在摘要、结构化查询和含义表示等各种任务中将解码速度加快1.8 - 3.1 倍，而且不会牺牲生成质量。此外，推测流是参数高效的。它实现了与 Medusa 风格架构同等/更高的速度，同时使用~10000X更少的额外参数，使其非常适合资源受限的设备。  &lt; /div&gt;  由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvzyw/r_speculative_streaming_fast_llm_inference/</guid>
      <pubDate>Thu, 25 Apr 2024 16:13:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过自适应 N-gram 并行解码实现大型语言模型的无损加速</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.08698 摘要：  虽然大型语言模型（LLM）表现出了非凡的能力，但它们由于自回归处理而导致大量资源消耗和相当大的延迟，从而阻碍了这一过程。在本研究中，我们引入了自适应 N-gram 并行解码 (ANPD)，这是一种创新且无损的方法，可通过允许同时生成多个标记来加速推理。 ANPD 采用两阶段方法：首先是使用 N-gram 模块的快速起草阶段，该模块根据当前的交互上下文进行调整，然后是验证阶段，在此期间原始 LLM 评估并确认提议的令牌。因此，ANPD 保留了法学硕士原始输出的完整性，同时提高了处理速度。我们进一步利用 N-gram 模块的多级架构来提高初始草稿的精度，从而减少推理延迟。 ANPD 无需重新训练或额外的 GPU 内存，使其成为高效且即插即用的增强功能。在我们的实验中，LLaMA 等模型及其微调变体的速度提升高达 3.67 倍，验证了我们提出的 ANPD 的有效性。     由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</guid>
      <pubDate>Thu, 25 Apr 2024 16:08:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 旧论文 - 机器学习奖学金中令人不安的趋势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</link>
      <description><![CDATA[我只是想提醒或向新人介绍这篇论文。我认为应该重新开启这个讨论，因为这里的许多人实际上确实影响了该领域的趋势。 https://arxiv.org/pdf/1807.03341&quot;&gt;https:// /arxiv.org/pdf/1807.03341  个人笔记（随意跳过）： 具体来说，我想指出这个问题“Mathiness”，因为这个问题似乎失控了，并且大多数会议的最佳论文都受到了它的困扰（最重要的 ML 论文之一试图数学化，但引入了一个大错误，我相信其他论文有更大的问题，但没有人费心去检查）。 所以这是我个人对学者和研究人员的观点：  我们（我认为大多数将会涉及），从业者不需要方程来知道什么是召回率，并且显然不想阅读难以理解的线性回归版本，这只会让你的论文毫无用处。如果您不想浪费我们的时间，请将其放入附录或完全删除。 审稿人，请不要对不必要的数学印象深刻，如果它很复杂并且没有任何用处，谁关心吗？而且，无论如何它都可能有缺陷，您可能不会发现它。    由   提交/u/pyepyepie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</guid>
      <pubDate>Thu, 25 Apr 2024 15:50:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于动画时间序列的 Python 包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvcdh/r_python_package_for_animated_time_series/</link>
      <description><![CDATA[在此有关时代系列的视频中，https: //www.youtube.com/watch?v=0zpg9ODE6Ww，有人知道用于创建视频第 34 分钟显示的动画情节的 Python 包吗？感谢您的帮助.   由   提交 /u/SatieGonzales   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvcdh/r_python_package_for_animated_time_series/</guid>
      <pubDate>Thu, 25 Apr 2024 15:48:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] UAI-2024成绩等候区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</link>
      <description><![CDATA[审查阶段之后(旧帖子），为像我这样等待决定的其他人创建一个线程。 祝一切顺利！  &amp;# 32；由   提交 /u/PaganPasta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</guid>
      <pubDate>Thu, 25 Apr 2024 15:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 没有进行分层训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</link>
      <description><![CDATA[在我看来，由于残差路径，无论变压器层/块如何，流向每一层的梯度都是相同的。示例： ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X))) ... ） 由于 ProjectionAndCost 的输入只是所有层和初始嵌入的输出之和，因此到达 L1 层的梯度与到达 L2 或 L3 的梯度相同。 因此我们可以：  首先仅训练 L1：ProjectionAndCost(X + L1(X)) 冻结 L1，包括 L2 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X))) 冻结 L1 和 L2，包括 L3 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X)))) ..依此类推  我们不能先训练L2 然后是 L1，因为 L2 的输入取决于 L1，但我们可以先训练较低层，然后逐渐添加和训练更深的层。这种方法有什么问题吗？   由   提交/u/kiockete  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</guid>
      <pubDate>Thu, 25 Apr 2024 14:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有一个适用于 NVIDIA GPU 的等效 BigDL 项目，它允许使用 Spark 在 DL 集群上分配工作负载？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</link>
      <description><![CDATA[所以就有了这个相对较新的“BigDL”项目” (https://bigdl.readthedocs.io/en/latest/)，适用于 Intel CPU 和 Intel GPU ，但没有提到它适用于 NVIDIA GPU。 Spark 集群上是否有适用于 NVIDIA GPU 的等效库？  &amp; #32；由   提交/u/PepperGrind  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</guid>
      <pubDate>Thu, 25 Apr 2024 10:18:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最适合我的情况的 TTS 模型是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccmhjb/d_what_is_the_best_tts_model_for_my_case/</link>
      <description><![CDATA[嗨。这是新人的问题。 最大的担忧是生成速度。我想在大约100ms内生成大约5秒的语音。我想知道哪种模型在这些条件下表现最好（SOTA）。 哪种模型最适合我？ 我认为“” styletts2”是最好的。 如果您有任何相关经验或了解任何其他信息，我将非常感谢您的帮助。 谢谢！   由   提交/u/hwk06023  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccmhjb/d_what_is_the_best_tts_model_for_my_case/</guid>
      <pubDate>Thu, 25 Apr 2024 08:07:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 R^2 如此疯狂？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccagwc/d_why_is_r2_so_crazy/</link>
      <description><![CDATA[      ​ https://preview.redd.it/jpiyt4b9yhwc1.png?width=1165&amp;format= PNG&amp; ;auto=webp&amp;s=95d80f8f9c9241d722717ad25215be4077d541ca 基于 MSE 看起来不错吧？但为什么我的 R^2 开始时如此负并且接近 0？这可能是我计算方式的错误吗？  这是在我在训练前最小化标签后发生的。 这是一个 LSTM，用于预测棒球比赛的得分。   由   提交 /u/Cloverdover1   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccagwc/d_why_is_r2_so_crazy/</guid>
      <pubDate>Wed, 24 Apr 2024 21:40:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么这么简单的一句话会破坏LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</link>
      <description><![CDATA[      这是我在 MS Copilot (GPT4 Turbo) 中输入的提示。 它是德语的，但它的意思只是“如果我先洗个澡，会有什么坏处吗？”，所以这不可能是另一个 SolidGoldMagikarp 或类似的东西，因为这些词显然同时存在于标记器和训练词汇中。 为什么这么简单的句子会导致这种情况？有什么猜测吗？ （也尝试过 Claude Opus 和 LLama 3 70b，效果很好） ​ https://preview.redd.it/9x6mva7b6gwc1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=bb6ac52d1c52d981161e8a864c5d1dd3794ca392    提交人    /u/michael-relleum   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</guid>
      <pubDate>Wed, 24 Apr 2024 15:59:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] SpaceByte：从大型语言模型中删除标记化 - 莱斯大学 2024 - 几乎与子字标记化器具有相同的性能，但没有许多缺点！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2404.14408 Github：https://github。 com/kjslag/spacebyte 摘要：  标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗性漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的&lt;字节级解码器架构缩小了字节级和子字自回归语言建模之间的性能差距。SpaceByte 由字节级 Transformer 模型组成，但在层中间插入了更大的 Transformer 块。我们发现，通过仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，可以显着提高性能。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且大致与标记化 Transformer 架构的性能相匹配。论文：https://arxiv.org/abs/2404.14408Github: https:// github.com/kjslag/spacebyteAbstract：标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的字节级解码器架构，可以缩小字节级和子字自回归语言建模之间的性能差距。 SpaceByte 由字节级 Transformer 模型组成，但在各层中间插入了更大的 Transformer 块。我们发现，仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，可以显着提高性能。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且与标记化 Transformer 架构的性能大致相当。  https://preview.redd.it/v1xo6g1gzewc1.jpg?width=1507&amp;format=p jpg&amp;自动=webp&amp;s=f9d415307b60639fa67e8a54c8769fa5a6c10f04 https://preview.redd.it/edvqos1gzewc1.jpg?width=1654&amp;format=pjpg&amp;auto=webp&amp;s=f91c8727017e1a1bc7b80bb77a8627ff99182607  https://preview.redd.it/fe6z6i1gzewc1.jpg?width=1181&amp;format=p jpg&amp;amp; ;auto=webp&amp;s=24d955f30b8ca3eaa7c527f3f40545ed493f789c   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</guid>
      <pubDate>Wed, 24 Apr 2024 11:42:07 GMT</pubDate>
    </item>
    <item>
      <title>Meta 做了 OpenAI 应该做的一切 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</link>
      <description><![CDATA[我很惊讶（或者可能没有）这么说，但 Meta（或 Facebook）比 OpenAI 更民主化 AI/ML，而 OpenAI 最初是成立并主要为此目的提供资金。 OpenAI 很大程度上已经成为一个仅以盈利为目的的商业项目。虽然就 Llama 模型而言，对我来说它们尚未达到 GPT4 功能，但我相信这只是时间问题。你们对此有何看法？   由   提交 /u/ReputationMindless32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</guid>
      <pubDate>Tue, 23 Apr 2024 22:03:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>