<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 07 Dec 2023 15:14:32 GMT</lastBuildDate>
    <item>
      <title>[D] 在解码器模型中，如果后来的令牌关注早期令牌，但早期令牌不关注后来的令牌，那么什么阻止早期令牌的影响随着每一层的增长而增长？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cwkwp/d_in_decoder_models_if_later_tokens_attend_to/</link>
      <description><![CDATA[让我们想象只有两个 token，A 和 B。 在每个注意力层中，B 将关注自身和 A，但是A永远只顾自己。所以一遍又一遍B会变成它自己和A的加权和，然后它们都会经过FF层，然后重复这个过程。所以“百分比”不应该是“百分比”吗？ B 中来自 A 的信息随着每一层的增长而增长？我在这里错过了一些基本的东西吗？模型是否必须学习使 B 的 Q 和 A 的 K 与每一层（平均）更加正交，以防止 B 过多关注A 贯穿所有层？   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cwkwp/d_in_decoder_models_if_later_tokens_attend_to/</guid>
      <pubDate>Thu, 07 Dec 2023 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型机器学习模型的半二次量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cvse5/r_halfquadratic_quantization_of_large_machine/</link>
      <description><![CDATA[分享我们在模型量化方面的工作。  博客：https://mobiusml.github.io/hqq_blog/ 代码： https://github.com/mobiusml/hqq 模型：https://huggingface.co/mobiuslabsgmbh/  无需数据校准，速度极快🚀，适用于语言和视觉模型！ 为什么重要？ 量化显着降低了 GPU 内存需求，但会降低模型的质量。拥有更快、更准确的量化方法对于机器学习社区来说非常有价值。 方法：原始权重与其反量化版本之间基于稀疏性的误差公式。我们使用半二次求解器通过 Pytorch 的 Autograd 导出比反向传播快 100 倍的封闭式解决方案。 量化速度： Llama2 约 1 分钟-13B ~ LLama2-70B 4 分钟（比 GPTQ 快 50 倍以上） 发现： - 较大的模型量化为 3/2 位优于具有相似或更低内存要求的较小全精度模型。 - 成功的 2 位量化需要较低的组大小（例如 32 或 16）以及零点和缩放因子的压缩以降低内存使用情况。  虽然我们承认我们的观点可能略有偏见，但我们真诚地相信我们的工作将使开源软件 (OSS) 机器学习社区受益匪浅。代码和模型均在 Apache 宽松许可证中。    由   提交/u/sightio  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cvse5/r_halfquadratic_quantization_of_large_machine/</guid>
      <pubDate>Thu, 07 Dec 2023 13:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] 规划的循环网络模型解释了海马体重放和人类行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ctvr3/r_a_recurrent_network_model_of_planning_explains/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.01.16.523429v2 代码：https://github.com/KrisJensen/planning_code 摘要：  当面对新的情况时，人类常常会花大量时间思考可能的未来。为了使这种计划变得合理，行为的好处必须补偿花费在思考上的时间。在这里，我们通过开发神经网络模型来捕获人类行为的这些特征，其中计划本身由前额叶皮层控制。该模型由一个元强化学习代理组成，该代理具有通过从其自己的策略中采样想象的动作序列来进行计划的能力，我们称之为“推出”。当计划有益时，代理会学习计划，解释人类思维时代的经验变化。此外，人工智能体所采用的策略推出模式与最近在空间导航过程中记录的啮齿动物海马回放模式非常相似。我们的工作提供了一种新理论，说明大脑如何通过前额叶-海马体相互作用来实施规划，其中海马体重放是由前额叶动态触发并适应性影响的。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ctvr3/r_a_recurrent_network_model_of_planning_explains/</guid>
      <pubDate>Thu, 07 Dec 2023 11:46:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在 Tsetlin Machine Book 第 4 章：卷积中学习如何使用可解释的规则执行逻辑卷积！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ctnm0/p_learn_how_to_perform_logical_convolution_with/</link>
      <description><![CDATA[   ​ 基于规则的卷积分步 嘿！我的书的另一章完成了。希望你喜欢！ https://tsetlinmachine.org 摘要：在时间和空间中搜索模式使得您在本章中学习的模式识别任务成为可能1.更具挑战性。例如，也许您希望 Tsetlin 机器识别图像内的较小物体。在 Tsetlin 机器能够了解它们的外观之前，它必须先找到它们。但如果不知道它们的外貌，又如何才能找到它们呢？在本章中，您将了解 Tsetlin 机器如何使用规则卷积来解决此双重任务。 在第 4.1 节中，您将研究健康和图像分析中的两个说明性任务。它们抓住了问题的双重性质，以及为什么需要同时执行定位、识别和学习。 然后，您将在第 4.2 节中学习如何将图像划分为多个块。这种划分允许 Tsetlin 机器一次专注于一个图像片段，从而提供一种引导注意力的方法。 多个图像片段需要一种新的方法来评估和学习规则。当每个输入图像变成多个部分时，您需要一种策略来选择关注哪些部分以及忽略哪些部分。我们在第 4.3 节中介绍了规则评估的新形式，而第 4.4 节则讨论了学习。 最后，第 4.5 节介绍了如何使用图像内补丁的位置来创建更精确的规则。目的是缩小与相关图像区域的模式匹配范围。 阅读本章后，您将了解如何构建能够识别时间和空间模式的卷积 Tsetlin 机。 &lt; /div&gt;  由   提交 /u/olegranmo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ctnm0/p_learn_how_to_perform_logical_convolution_with/</guid>
      <pubDate>Thu, 07 Dec 2023 11:31:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据增强在 TPU 上运行得更快吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ctg1x/d_will_data_augmentations_work_faster_on_tpus/</link>
      <description><![CDATA[我正在开发一个大量涉及图像增强的项目。瓶颈部分是 CPU，GPU 利用率徘徊在 20-30% 左右。我正在使用 PyTorch，现在，每个 epoch 在 A100 GPU 上大约需要 1-1.5 小时。 迁移到 TPU 会让我受益吗？有什么方法可以加速我的增强（randaugment、cutmix、mixup 等）？ 欢迎提出任何建议。提前致谢。   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ctg1x/d_will_data_augmentations_work_faster_on_tpus/</guid>
      <pubDate>Thu, 07 Dec 2023 11:17:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] AI安全与强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18csboa/r_ai_safety_and_reinforcement_learning/</link>
      <description><![CDATA[https://blogs.ucl.ac.uk/steapp/2023/11/15/adversarial-attacks-robustness-and-generalization-in-deep-reinforcement-学习/   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18csboa/r_ai_safety_and_reinforcement_learning/</guid>
      <pubDate>Thu, 07 Dec 2023 09:57:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我为一个大学项目制作了一个关于人工智能伦理的视频系列</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cs4i7/p_i_made_a_video_series_on_ai_ethics_for_a/</link>
      <description><![CDATA[几个月前，我开始制作有关人工智能伦理的视频系列，旨在为数据科学家和其他对伦理感兴趣的人提供资源围绕人工智能的问题。在本系列中，我们采访了该领域的几位专家，包括计算机科学、伦理学和法律领域的教授。我们还采访了 Greg Rutkowski，他是目前中途最受关注的艺术家。我们试图激发人们思考道德问题的兴趣，以便未来的人工智能模型对每个人来说都既出色又安全。 如果您有兴趣，可以在这里观看我们的第一集：https://www.youtube.com/watch?v=0OG5XdzNwyw 我很想听听您的反馈！ :)   由   提交/u/WillingnessConstant8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cs4i7/p_i_made_a_video_series_on_ai_ethics_for_a/</guid>
      <pubDate>Thu, 07 Dec 2023 09:42:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongScope：法学硕士中长上下文中的碎片知识使用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18crfli/r_longscope_fragmented_knowledge_use_in_long/</link>
      <description><![CDATA[LongScope 旨在评估这些模型智能处理和集成分散在大量文本片段中的信息的能力，超越简单的句子提取或检索。 项目链接：https://github.com/mrconter1/LongScope/   由   提交 /u/Alarmed-Profile5736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18crfli/r_longscope_fragmented_knowledge_use_in_long/</guid>
      <pubDate>Thu, 07 Dec 2023 08:49:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] NeuroEvoBench：深度学习应用的进化优化器基准测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cp7bl/r_neuroevobench_benchmarking_evolutionary/</link>
      <description><![CDATA[   /u/hardmaru  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cp7bl/r_neuroevobench_benchmarking_evolutionary/</guid>
      <pubDate>Thu, 07 Dec 2023 06:12:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 第一份 ICLR 提交材料。被拒绝了。总体来说很棒的体验。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cl7r0/d_first_iclr_submission_got_rejected_great/</link>
      <description><![CDATA[我预计我的论文会被拒绝，但我还是尝试了一下。审稿人非常有帮助；尽管出现了所有“低质量评论”的情况，但我还是收到了相当高质量的建设性批评。  这是我的论文：Guided Sketch-Based Program Induction by Search Gradients | OpenReview   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cl7r0/d_first_iclr_submission_got_rejected_great/</guid>
      <pubDate>Thu, 07 Dec 2023 02:31:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] Mamba-Chat：基于状态空间模型的聊天法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ckntr/p_mambachat_a_chat_llm_based_on_state_space_models/</link>
      <description><![CDATA[嘿！ 您可能已经看过这篇论文最近几天的 Mamba 论文，这是首次尝试将状态空间模型扩展到 2.8B 参数以处理语言数据。 与 Transformer 相反，这这种架构的计算复杂度不会随输入长度呈二次方扩展，因此从长远来看，如果它能够取代 Transformer，那就太棒了。 我们对这篇论文和发布的模型感到非常兴奋，但不幸的是，没有训练代码提供了它，所以我们决定自己编写它并训练模型。因此，我们刚刚发布了 mamba-chat，这可能是现有的最好的不依赖于 Transformer 的 LLM。老实说，我对该模型的表现感到非常惊讶，因为它只有 2.8B 个参数，并且基本模型仅在 Pile 上进行训练。想到这些模型是否会在某个时候取代 Transformer 真是令人兴奋。 请随意查看我们的 Github 或 Huggingface 存储库！我们的 Github 存储库包含一个 cli 聊天脚本，因此如果您有权访问 GPU，则可以轻松运行模型。   由   提交 /u/pip-install-torch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ckntr/p_mambachat_a_chat_llm_based_on_state_space_models/</guid>
      <pubDate>Thu, 07 Dec 2023 02:03:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 愚蠢的项目：使用变压器实现 MLP（哟，dawg...）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ceqz5/p_silly_project_implement_mlp_using_a_transformer/</link>
      <description><![CDATA[Transformers 严重依赖 MLP。据推测，LLM 可以回忆的事实存储在 MLP 中。 （例如，参见 ROME 论文。）这些 MLP 非常庞大。 例如考虑 GPT-Neo-350M。每个 MLP 有 1024 个元素的输入和输出层以及 4096 个元素的内层。这需要 2x1024x4096 = 4.2M 权重。整个模型有 24 个（每层一个），导致 MLP 使用 100M 权重。 然而这些 MLP 基本上只做 4096 点积来计算特征。数以百万计的权重只是为了计算 4096 点积并线性变换输入/输出，这似乎过多。 MLP 中的权重数量是输入元素数量的平方，因为每个内部元素都必须直接连接每个输入和输出元素。每个连接都很昂贵。我们可以做得更好吗？ 好吧，我们可以将 MLP 拆分为更小的 MLP。例如。将 1024 元素的输入向量划分为 4 256 元素的向量，并为每个块应用单独的 MLP。这样我们总共就有 4096 个内部特征，但权重数量减少了 4 倍！ 遗憾的是，这效果不太好，因为我们缺少“块”之间的交互。 &lt; p&gt;但是...有一种架构可以在令牌之间有效地路由信息...这就是变压器，对吧。 那么，我们可以将变压器放入 MLP 中吗？ （然后我们将在变压器内部的 MLP 内部有一个变压器。） 是的！有效吗？有点像。 我们可以制作一个旨在近似 MLP 的模块。我们将这种基于转换器的 MLP 近似称为 TransMLP。 为了了解该近似的效果如何，我们可以将其与其他近似进行比较。在本实验中，我的目标是逼近 GPT-Neo-350M 众多 MLP 之一。为了获取训练数据，我在 Brown 文本上运行 GPT 以捕获 MLP 输入和输出。 为了进行比较，我训练了正常的“GPTNeoMLP”在相同的数据上。这些 MLP 使用较小的内部中间层尺寸进行实例化。基础模型有 4096 个元素中间层，近似值有 2048 和 1024 个。 我们可以使用均方误差损失来比较近似值。对于零近似，我得到 2.6（这基本上是输出的均方）。  GPTNeoMLP 4096：...损失，840 万权重 GPTNeoMLP 1024：0.39 损失，2.1 M 权重 GPTNeoMLP 2048：0.31 损失，4.2M 权重 TransMLP：0.36 损失，2.9M 权重  所以 TransMLP 的损失接近到具有 2048 个特征的经典 MLP，同时权重更少。请注意，仅单个线性层需要 1M 权重，变压器本身只需要 1.9M 权重。 我不想对此进行全面评估，而是进行快速健全性检查：如果我们替换使用 TransMLP 的 GPT-Neo-350M 的正常 MLP，句子上的 GPT 损失从 2.965 增加到 2.9908（请注意，这不是 MSE 损失，而是交叉熵或类似的东西）。它仍然可以生成与基线模型一样好的文本。 所以，是的，我们可以将一个变压器放在 MLP 内部的变压器中...... 享受：https://colab.research.google.com/drive/1UIDXF_x_Y7QWMQrteGaNHQ7Y9S-ZgeoF   由   提交/u/killerstorm  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ceqz5/p_silly_project_implement_mlp_using_a_transformer/</guid>
      <pubDate>Wed, 06 Dec 2023 21:28:02 GMT</pubDate>
    </item>
    <item>
      <title>[R]谷歌发布Gemini系列前沿机型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18c6xio/r_google_releases_the_gemini_family_of_frontier/</link>
      <description><![CDATA[来自 Jeff Dean 的推文：https://twitter。 com/JeffDean/status/1732415515673727286 博客文章：https:// blog.google/technology/ai/google-gemini-ai/ 技术报告：https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf 有什么想法吗？没有太多“肉”在这个公告中！他们肯定担心其他实验室+开源从中学习。   由   提交/u/blabboy  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18c6xio/r_google_releases_the_gemini_family_of_frontier/</guid>
      <pubDate>Wed, 06 Dec 2023 15:52:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么仅解码器模型用于自回归生成而不是仅编码器模型？如果新标记尚不存在，因果掩码的值是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18c498u/d_why_are_decoder_only_models_used_for/</link>
      <description><![CDATA[为什么还要费心使用因果掩码，看起来它只是破坏了有用的信息传输？我知道，如果你用它进行训练，你就无法在推理过程中轻松删除它而不降低质量，但为什么不直接训练编码器来猜测此时的下一个标记呢？    由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18c498u/d_why_are_decoder_only_models_used_for/</guid>
      <pubDate>Wed, 06 Dec 2023 13:43:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>