<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 22 Feb 2024 15:13:01 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么在法学硕士中字节对编码分词器比字符级分词器更受青睐？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax6xuh/d_why_are_byte_pair_encoding_tokenizers_preferred/</link>
      <description><![CDATA[我知道字节对会给你更大的词汇量但更短的标记序列，而像字符级分词器这样更细粒度的东西将有一个小的词汇量但输出标记序列要长得多。 我不明白的是为什么这是大多数 LLM 模型的首选。例如，GPT 和 Llama 都使用字节对编码。这与这些模型的块大小限制有关吗？   由   提交/u/putinwhat  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax6xuh/d_why_are_byte_pair_encoding_tokenizers_preferred/</guid>
      <pubDate>Thu, 22 Feb 2024 13:54:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有云 GPU 按计算/利用率而非正常运行时间计费？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax6v04/d_are_there_any_cloud_gpus_billed_on/</link>
      <description><![CDATA[我正在启动一个照片处理应用程序，它使用 GPU 对大型图像处理模型进行推理。目前，只有少数用户，因此我的 GPU 实例 99.99% 的时间都处于空闲状态，但我每月要为空闲的 GPU 支付 400 多美元。当用户触发推理作业时，我可以启动 GPU 实例，但这意味着用户需要等待几分钟才能启动实例。无法承受这样的延迟。 我搜索过根据利用率计费的云 GPU 解决方案，但没有找到任何内容。当然，这对于云 CPU（多租户、虚拟化实例）来说非常常见，您只需为使用的计算付费；但我猜 GPU 并不真正具有相同的功能？有人看到过其他情况吗？ 除了每月支付 400 多美元购买利用率极低的 GPU 之外，我还有什么选择？  &amp; #32；由   提交/u/uberdev  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax6v04/d_are_there_any_cloud_gpus_billed_on/</guid>
      <pubDate>Thu, 22 Feb 2024 13:51:04 GMT</pubDate>
    </item>
    <item>
      <title>RAG 与长上下文模型 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax6j73/rag_vs_long_context_models_discussion/</link>
      <description><![CDATA[大家好，我知道我们都见过具有 100 万上下文的 Gemini v1.5 模型，而且来自 groq 公司的硬件表明，如果硬件是专门为语言模型设计的，因为它们可以变得更好。您现在对 RAG 架构有何看法，因为我们已经看到了很长的上下文模型。如果我们有更长的上下文模型和更好的量化技术和硬件怎么办？您认为像 RAG 这样的架构以及使用向量数据库来存储知识库和动态检索仍然相关吗？ 请纠正我并相应地添加更多相关信息。如果相关的研究和观察被发布，我们将不胜感激！！   由   提交/u/WritingBeginning3403  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax6j73/rag_vs_long_context_models_discussion/</guid>
      <pubDate>Thu, 22 Feb 2024 13:35:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究论文中的结果（或数据）多久被操纵一次？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax610a/d_how_often_is_results_or_data_manipulated_in/</link>
      <description><![CDATA[我最近看到了哈佛教授的结果操纵案例，觉得很有趣，人们怎么能在如此高的水平上伪造结果。机器学习研究中也会发生这种情况吗？我从来不是机器学习论文的第一作者，所以我不知道知名会议的工作原理，但我不认为有人会交叉检查结果，而只会在发现可疑时提出问题。 有时，我发现结果与作者发布的结果与使用他们提供的代码重新创建的结果不匹配，但我总是将其归咎于我的实现和/或环境。   由   提交 /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax610a/d_how_often_is_results_or_data_manipulated_in/</guid>
      <pubDate>Thu, 22 Feb 2024 13:08:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 有什么好的资源可以推荐有关法学硕士、人工智能和机器学习的热门论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax5yge/r_what_is_a_good_resource_to_be_recommended_hot/</link>
      <description><![CDATA[我正在寻找诸如包含 5 篇热点论文的每周通讯之类的内容。你们中有人知道我可以在哪里订阅或有什么建议吗？   由   提交/u/PowerLock2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax5yge/r_what_is_a_good_resource_to_be_recommended_hot/</guid>
      <pubDate>Thu, 22 Feb 2024 13:05:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] tf.keras.metrics.MeanIoU 没有改善</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax4qz1/d_tfkerasmetricsmeaniou_is_not_improving/</link>
      <description><![CDATA[我正在 Colab 上使用 UNET 架构训练一个模型，用于二值图像分割。我使用 tf.keras.metrics.MeanIoU 作为指标，但所有时期的结果都没有改善，但准确性和 val_accuracy 有所提高。我哪里做错了？怎么解决呢？   由   提交/u/NailaBaghir   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax4qz1/d_tfkerasmetricsmeaniou_is_not_improving/</guid>
      <pubDate>Thu, 22 Feb 2024 12:00:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 未修剪视频的细粒度动作识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax2kmz/d_fine_grained_action_recognition_for_untrimmed/</link>
      <description><![CDATA[嗨， 在阅读了有关该主题的研究后，我发现传统的动作识别模型（C3D、慢快）的方式非常令人困惑等）用于未修剪的视频动作识别上下文中？我一直在研究预测动作边界（动作的开始和结束）的算法。遗憾的是，这些算法并不容易实现。  ​ 另一方面，有很多动作识别模型可供使用。  我的问题：如何使用未修剪视频的动作识别模型来检索具有特定动作的剪辑？    由   提交 /u/Frizzoux   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax2kmz/d_fine_grained_action_recognition_for_untrimmed/</guid>
      <pubDate>Thu, 22 Feb 2024 09:42:15 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]文档分类的深度学习方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ax18j4/discussion_deep_learning_approaches_to_document/</link>
      <description><![CDATA[大家好， 我计划参加一项有关新闻文章分类的共享任务。这让我思考现有的文档分类方法，我很快意识到有很多方法！接下来是我在学习/从事 NLP 项目期间遇到的所有方法的简短描述。您认为/知道哪些最有前途？您还有其他方法吗？期待您的评论！ 就上下文而言，我正在考虑的文档大约有 500 - 1000 个标记，因此它们应该适合最新的基于转换器的架构，并具有最小（如果有）的截断。类的数量非常少：N=4。当我说编码器模型时，我指的是纯编码器架构，例如 BERT。对于手头的任务，我只能访问约 1000 个样本（每类 250 个） LLM 零样本：使用任何 LLM（例如 llama），提供任务的简短描述，描述标签，然后提示它对以下文档进行分类。可以使用约束生成仅输出有效的类。 LLM 的少样本（上下文学习）：除上述之外，还为每个类添加一个示例。我喜欢想象一个对话，其中用户已经向模型分配了 N 次任务来执行该任务，例如用户：&lt;提示如上&gt;，系统：&lt;class a&gt;，... 使用 LLM 的 PEFT： 微调 LLM，例如关于LORA适配器的任务。这可能是最好的方法，但到目前为止我还没有对此进行研究。 使用编码器模型进行无监督：使用任何编码器模型，例如 BERT 或 Sentence-BERT，计算每个文档的嵌入，并使用 k-means（其中 k = N）对它们进行聚类。现在，我必须手动将聚类映射到类。 使用编码器模型进行很少的拍摄：使用任何编码器模型，插入适配器（例如瓶颈适配器），添加分类头，并进行微调，例如每个类 8-64 个示例。 很少-使用编码器模型拍摄（2）：我正在考虑 SetFit 库。他们首先用句子对微调句子转换器模型。如果句子对都来自同一类，则将其视为正（余弦相似度 = 1），否则视为负（0）。然后，他们使用嵌入作为输入来训练传统分类器。 使用编码器模型进行完全微调：使用任何编码器模型，添加分类头，并使用所有示例进行微调。作为一种变体，可以冻结编码器模型并仅训练分类头。 训练 传统分类器 以嵌入作为特征：使用任何编码器模型并计算每个文档的嵌入。嵌入被输入到传统的分类器，例如逻辑回归。但有些还在嵌入的基础上训练更高级的分类器，例如 Bi-LSTM。 ​ ​ &lt; !-- SC_ON --&gt;  由   提交/u/bigabig   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ax18j4/discussion_deep_learning_approaches_to_document/</guid>
      <pubDate>Thu, 22 Feb 2024 08:11:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么研究人员很少发布训练代码？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awy07v/d_why_do_researchers_so_rarely_release_training/</link>
      <description><![CDATA[我现在正在查看针对各种 MoE 模型的 3 篇不同论文。这三个都发布了模型权重和推理代码，但都没有发布训练代码。  当我们期望现在大多数论文都有代码及其实现时，为什么这种情况如此普遍和被接受？   由   提交/u/hazard02  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awy07v/d_why_do_researchers_so_rarely_release_training/</guid>
      <pubDate>Thu, 22 Feb 2024 04:59:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 测量数据集的复杂性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awwr4d/d_measuring_complexity_of_datasets/</link>
      <description><![CDATA[我正在启动一个项目，我们的目标是证明我的领域中的许多数据集比更通俗的 ML 数据集（UCI、图像、 ETC。）。我们实验室的多项结果表明，某些算法在我们的领域中运行得非常好，而在外部则不太好，反之亦然——一些在更通用的 ML 数据集中运行良好的算法效果不佳，或者计算量更大与我所在领域的方法获得相同结果的成本很高。 作为第一步，我们已经证明，许多数据集的损失函数的平滑度远低于标准 ML 数据集。我还想探索其他方法来提供更全面的观点。因此，我想知道以下数学工具是否可以共同用于解释数据集的复杂程度：  损失的预期平滑度 损失的预期 Lipschitz 常数 Mapper 算法（TDA 中）或 Betti 曲线的结果 损失的强凸性（强凸性可以证明等于损失的锐度，并且平坦的最小值已被证明可以更好地概括）  这些有意义吗？据我所知，单独它们仅显示图片的一部分，但它们一起应该提供数据复杂性的更全面的视图。   由   提交 /u/FlyingQuokka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awwr4d/d_measuring_complexity_of_datasets/</guid>
      <pubDate>Thu, 22 Feb 2024 03:51:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有人注意到 FP32 和 FP16 型号之间惊人的巨大差异吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awn2t5/d_anyone_else_notice_a_surprisingly_big/</link>
      <description><![CDATA[在我工作的公司，我们运行图像分类服务。该模型相当简单：我们只有一个预先训练的图像编码器主干模型，我们用自己的数据进一步对其进行了预训练，并且我们为每个“任务”都有一个完全连接的层。 “任务”是一个“任务”。可能类似于对狗的图像进行分类或对图像的颜色进行分类。 我最近更改了我的代码以使用 HuggingFace 的 Accelerate 模块而不是 PyTorch 的本机 DDP，并且还使用混合精度训练来训练我的模型将权重存储在 FP16 而不是 FP32 中。我将主干层和全连接层冻结用于原始任务，只想训练新的全连接层。然而，问题是我注意到冻结分支做出的预测存在显着差异。 在调查了所有可能出错的地方，甚至手动检查 IPython 终端中的权重值后，我注意到主要区别在于之前模型的权重存储在 FP32 中，而我新训练的模型权重存储在 FP16 中。 我听说过 FP16 本质上是速度和性能的权衡的故事，但我从来不知道这一点那就完全不同了。好奇是否有其他人经历过类似的情况。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awn2t5/d_anyone_else_notice_a_surprisingly_big/</guid>
      <pubDate>Wed, 21 Feb 2024 20:54:41 GMT</pubDate>
    </item>
    <item>
      <title>[N] 语言处理单元 (LPU) 使 LLM 的推理速度提高 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</link>
      <description><![CDATA[本周，一家鲜为人知的公司 Groq 展示了运行 Llama-2 等开源 LLM 的前所未有的速度（ 700 亿个参数），每秒超过 100 个令牌，而 Mixtral 在 Groq 的语言处理单元 (LPU) 上每用户每秒近 500 个令牌。 对于比较：   “根据 Groq 的说法，在类似的测试中，在典型的基于 GPU 的计算系统上，ChatGPT 的加载速度为每秒 40-50 个令牌，而 Bard 的加载速度为每秒 70 个令牌。 每个用户每秒 100 个令牌的上下文，展示了在 Groq 语言处理上以每秒超过 100 个令牌的速度运行开源 LLM 的前所未有的速度，例如 Llama-2（700 亿个参数），以及每秒每用户近 500 个令牌的 Mixtral单位（LPU）。  那么：LPU是什么，它是如何工作的，Groq在哪里（这么不幸的名字，考虑到马斯克的Grok都结束了）媒体）来自哪里？ https://www.turingpost.com/p/fod41    由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</guid>
      <pubDate>Wed, 21 Feb 2024 18:32:13 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 研究人员（硕士、博士）如何实现复杂模型？他们是神吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</link>
      <description><![CDATA[我现在正在做我的论文。我很好地掌握了大多数 ML 模型（RNN、CNN、LSTM、Transformers、GPT、CNN、GAN、LDM、VAE、自动编码器等）的高级细节。当然，我绝不是专家，但我能够学习我需要的东西。 但是当真正使用它们，并在代码中实现它们并训练它们时，这就变成了地狱。对于更简单的模型，还好，但是对于更复杂的模型，网上没有教程，他们只是说“使用现有模型”。 世界各地的研究人员如何实现复杂的模型？例如，扩散模型、LDM 或修改后的 LLM，如 Transformer 或 GPT？ 或者它们如何更改现有模型，并使用不同的技术，例如添加编码器进行调节？  &gt;就像，研究和理解基础知识很好，但实际实施起来却非常困难。他们是如何做到如此优雅的？一些调查研究论文包括多种模型的使用和比较。他们是怎么做到的？   由   提交 /u/ShlomiRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</guid>
      <pubDate>Wed, 21 Feb 2024 15:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[新闻]Google发布全新开放的LLM模型：Gemma模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</link>
      <description><![CDATA[     &lt; /td&gt; 明显比 llama7 和 13 更好（但不与 Mistra7b 进行基准测试）：https://blog.google/technology/developers/gemma-open-models/ ​ 编辑：正如所指出的，他们确实做了这些测试，例如这里：  https://preview .redd.it/uqh4v4um93kc1.png?width=494&amp;format=png&amp;auto=webp&amp;s=36e40c104bfa9cdd5adf48fb4c7be158f12d07ac   由   提交 /u/edienemis   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</guid>
      <pubDate>Wed, 21 Feb 2024 13:28:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>