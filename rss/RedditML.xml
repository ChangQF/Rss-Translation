<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Wed, 31 Jul 2024 18:18:57 GMT</lastBuildDate>
    <item>
      <title>[R] 注释词汇表（可能）就是您所需要的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</link>
      <description><![CDATA[论文链接：https://www.biorxiv.org/content/10.1101/2024.07.30.605924v1 摘要：  蛋白质语言模型 (pLM) 彻底改变了蛋白质系统的计算建模，构建了以结构特征为中心的数值嵌入。为了增强蛋白质嵌入中可用的生化相关属性的广度，我们设计了注释词汇表，这是一种由结构化本体定义的蛋白质属性的转换器可读语言。我们从头开始训练注释变换器 (AT)，以恢复掩蔽的蛋白质属性输入，而无需参考氨基酸序列，仅基于蛋白质描述构建新的数值特征空间。我们在各种模型架构中利用 AT 表示，用于蛋白质表示和生成。为了展示注释词汇集成的优点，我们进行了 515 次不同的下游实验。使用新颖的损失函数和仅 3 美元的商业计算，我们的首要表示模型 CAMP 为 15 个常见数据集中的 5 个生成了最先进的嵌入，其余数据集上的性能也具有竞争力；凸显了使用注释词汇进行潜在空间管理的计算效率。为了标准化从头生成的蛋白质序列的比较，我们提出了一种新的基于序列比对的分数，它比传统的语言建模指标更灵活、更具生物学相关性。我们的生成模型 GSM 使用类似 BERT 的生成方案从仅注释提示中生成高比对分数。特别值得注意的是，许多 GSM 幻觉返回具有统计意义的 BLAST 命中，其中富集分析显示与注释提示匹配的属性 - 即使基本事实与整个训练集的序列同一性较低。总体而言，注释词汇工具箱提供了一种有前途的途径，可以用本体和知识图谱的成员取代传统的标记，增强特定领域的变换器模型。注释词汇对蛋白质的简洁、准确和有效的描述为构建蛋白质的数值表示以进行蛋白质注释和设计提供了一种新颖的方法。  我们很自豪地宣布发布我们的最新作品！请阅读、分享并提出任何问题！    提交人    /u/TeamArrow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/</guid>
      <pubDate>Wed, 31 Jul 2024 17:47:19 GMT</pubDate>
    </item>
    <item>
      <title>硕士论文建议[项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egr5pk/masters_thesis_advice_project/</link>
      <description><![CDATA[嗨，我即将完成纯数学（几何）硕士学位。论文主题是计算机视觉几何（射影空间的投影、张量等：基本上是从以不同角度拍摄的两张或多张照片中检索 3D 空间），但我真的想包含一些与 ML 相关的内容，然后展示一个项目。您对如何将这两个主题联系在一起有什么建议吗？  也许我可以看看一些论文。我的导师专攻抽象数学，所以他帮不上我。 提前谢谢您！    提交人    /u/Warrio9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egr5pk/masters_thesis_advice_project/</guid>
      <pubDate>Wed, 31 Jul 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 无限上下文长度真的可能吗？：“Unlimiformer”作者周五讨论 NeurIPS 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</link>
      <description><![CDATA[无限上下文长度真的可能吗？代价是什么？ 2023 年 NeurIPS 论文 Unlimiformer 的作者 Amanda Bertsch 将在本周五的 Oxen.ai 论文俱乐部中描述该架构并回答问题。  Oxen 首席执行官兼 Plain Speak 大师 Greg Schoeninger u/FallMindless3563 将帮助解释该概念并将其与我们审阅过的其他论文联系起来。 致电：https://oxen.ai/community  声称使无限上下文长度成为可能的技巧：将交叉注意力计算卸载到 K-最近邻 (K-NN) 索引。  我在这里发了一条推文，其中某人制作了巧妙的 K-NN 动画：https://x.com/mustafarrag/status/1817647917059944474 论文：https://arxiv.org/abs/2305.01625 Greg，我将用我的前 5 个问题来回答。到目前为止，我只阅读了摘要。    提交人    /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/</guid>
      <pubDate>Wed, 31 Jul 2024 15:47:52 GMT</pubDate>
    </item>
    <item>
      <title>[N] Finegrain 的对象橡皮擦演示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</link>
      <description><![CDATA[      对象橡皮擦 Finegrain 刚刚在 Huggingface 上发布了一个对象橡皮擦的演示。该模型可以删除任何对象或图像，并删除来自对象的任何效果（阴影、反射、光线）。与我们迄今为止拥有的其他橡皮擦相比，结果令人印象深刻；你怎么看？ 演示链接：https://huggingface.co/spaces/finegrain/finegrain-object-eraser    由   提交  /u/nota-Reddit   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egpu9b/n_object_eraser_demo_by_finegrain/</guid>
      <pubDate>Wed, 31 Jul 2024 15:20:10 GMT</pubDate>
    </item>
    <item>
      <title>为大型语言模型提供更好内存的框架 - 免费且开源。[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</link>
      <description><![CDATA[Github - https://github.com/chisasaw/redcache-ai 使用的 SDK [scikit-learn、numpy 和 openai] 什么？ 在构建聊天应用程序时，我找不到经济高效、可扩展且价格合理的内存层。这导致了 redcache-ai。它是一个提供语义搜索、存储和检索增强生成 (RAG) 的 Python 包。 用例？ 如果开发人员想要构建使用文档摘要和/或语义搜索的桌面应用程序，开发人员可以使用 redcache-ai 和选择的大型语言模型提供程序。聊天应用程序存储用户会话也是如此。 优点？  易于使用。只需像安装 Python 包一样安装它，即“pip install redcache-ai”。 提供存储可扩展性。将您的记忆存储到磁盘、sqlite 或您选择的数据库中。  请提供反馈并提出问题。也可以随时为项目做出贡献。    提交人    /u/hack_knight   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egmmpe/a_framework_to_give_large_language_models_better/</guid>
      <pubDate>Wed, 31 Jul 2024 13:02:41 GMT</pubDate>
    </item>
    <item>
      <title>关于使用知识图谱的神经符号人工智能的调查论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</link>
      <description><![CDATA[  由    /u/joestomopolous  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egke1v/survey_paper_over_neurosymbolic_ai_with_knowledge/</guid>
      <pubDate>Wed, 31 Jul 2024 11:06:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI 的 CLIP 替代方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egi7sz/d_openais_clip_alternative/</link>
      <description><![CDATA[嗨，有没有像 CLIP 这样的新 SOTA 模型？我想对文本和图像进行语义搜索，但 CLIP 的性能对我的项目来说不是很好。    提交人    /u/sushilkhadakaanon   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egi7sz/d_openais_clip_alternative/</guid>
      <pubDate>Wed, 31 Jul 2024 08:44:56 GMT</pubDate>
    </item>
    <item>
      <title>[P]尝试预测工业压力机发生一系列警报后发生的故障</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eghxix/ptrying_to_predict_faults_happening_after_a/</link>
      <description><![CDATA[压机在工作期间会发出一系列警报，中间会出现一些故障。我对数据进行了分段，使得每个段由一系列警报组成，而最后一个段以故障结束。我正在尝试创建一个预测模型来预测一系列警报之后发生的故障。  在网络上搜索时，尽管它指向神经网络，但他们大多将其用于数字输入，而我的是分类变量。    提交人    /u/Agreeable-Banana-833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eghxix/ptrying_to_predict_faults_happening_after_a/</guid>
      <pubDate>Wed, 31 Jul 2024 08:24:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mac Air 与我目前的 HP - 训练较小的“经典”模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egfvwk/d_mac_air_vs_my_current_hp_training_smaller/</link>
      <description><![CDATA[我目前有一台工作笔记本电脑（规格如下），我经常用它来在本地训练经典（比如 SKLearn、xgboost 等）模型，这是我工作的一部分。当它达到大约 100k 行和 100 列时，我正在训练一个随机森林，例如 n_jobs=-1，这会变得有点慢。我讨厌 Windows，以及这种训练作业让它变得反应迟钝。我过去用过几台 MacBook，它们似乎能更好地处理负载，因此操作系统仍然可用。另外，基于 UNIX 的操作系统也不错。Linux 不是一个选择，它在公司环境中不支持作为笔记本电脑操作系统，即使是虚拟的。 我可以选择换用 MacBook，但公司津贴只允许我买一台带 8GB RAM 的 M2 Air。我知道 M2 芯片和统一内存比我现在拥有的英特尔芯片和 RAM 优化得多，但即使如此，我担心 8 GB 还是太少了。但话又说回来，我的操作系统开销会更少，所以我想知道从我现在的笔记本电脑换到 MacBook 是否会看到性能上的提升。 对于更大的模型和深度学习，我们可以访问云端，但我更喜欢在本地环境中训练较小的模型，这样我可以更好地控制。 你有什么亲身经历或一些基准测试吗？ 目前的笔记本电脑： HP Probook 第 11 代英特尔 (R) 酷睿 (TM) i7-1165G7，4 核，8 线程 @ 2.80GHz 32GB RAM 一些英特尔显卡芯片 前景： MacBook Air M2 芯片（4 个 E 核 @ 2,4GHz 和 4 个 P 核 @ 3,5GHz） 8GB RAM 16 核神经引擎（我可以在并行的 SKLearn 作业中利用这些吗？我想不能）    提交人    /u/lifesthateasy   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egfvwk/d_mac_air_vs_my_current_hp_training_smaller/</guid>
      <pubDate>Wed, 31 Jul 2024 06:05:43 GMT</pubDate>
    </item>
    <item>
      <title>情境学习中的突发性 [R][D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egcudr/burstiness_in_incontext_learning_rd/</link>
      <description><![CDATA[      0 我正在阅读论文上下文分类中数据依赖和突发学习的机制基础任务。我对参数化数据分布部分感到非常困惑。  这个“数据分布”是指训练数据还是测试数据？（两者都是一批输入序列。） 对于那些突发序列，类别究竟是如何分布的？这是否就像来自特定（随机选择）类别的 B 个项目，然后其余 N-B 个项目遵循剩余类别的等级频率分布？  https://preview.redd.it/43tvem1qtrfd1.png?width=1678&amp;format=png&amp;auto=webp&amp;s=465e9df8a20ceb6e79dc07f56c9c5fbc341b1ab3   由    /u/mziycfh  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egcudr/burstiness_in_incontext_learning_rd/</guid>
      <pubDate>Wed, 31 Jul 2024 03:10:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]关于知识图谱和图神经网络的思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eg674y/discussion_thoughts_on_knowledge_graphs_and_graph/</link>
      <description><![CDATA[几年前，我的数据科学团队梦想着实现知识图谱并利用图神经网络。这种方法在我所在的行业——金融领域似乎特别有前景，因为它可以让模型捕捉间接关系——例如，所有权变更如何影响公司的业绩。 当时，这感觉就像一场白日梦。捕捉任何关系（例如“拥有”或“销售产品”）都需要自己的 NLP 模型。然而，LLM 的出现大大降低了这种复杂性（现在已在 LlamaIndex 中实现）。所以我们想知道是否应该再试一次 KG 和 GNN。我们的想法是使用 LLM 来帮助我们构建 KG，并将来自其他数据库的数据添加到其中。然后，我们将训练 GNN 来预测诸如“公司 A 会收购公司 B 吗”或“公司 C 的表现会优于公司 D 吗”之类的事情。 然而，尽管 GNN 经常被吹捧为下一个大事件，但它仍然有点小众。好吧，它们被用来补充 RAG，但我还没有听说过任何非大型科技公司建立自己的超级知识图谱。根据我所读到的内容，图形数据库面临着大量的批评，原因是性能问题和创建有效模式的难度等。 您对这些技术有什么经验？您有什么成功案例或警示故事可以分享吗？ [编辑]这篇文章比我想象的要受到更多的关注，所以我对其进行了一些修改，以节省大家的时间。具体来说，我试图澄清 KG 和 GNN 是不同的。这两种技术的融合似乎很有希望，但我有两个大担忧：  领先的图形数据库提供商 Neo4j 似乎是该主题的主要知识提供者。它甚至撰写了至少两本由 O&#39;Reilly(!) 编辑的书，因此很难了解知识图谱的陷阱。 据我所知，几乎没有人大规模实施过 GNN。     提交人    /u/MeditationBeginner   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eg674y/discussion_thoughts_on_knowledge_graphs_and_graph/</guid>
      <pubDate>Tue, 30 Jul 2024 22:05:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 论文评论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/</link>
      <description><![CDATA[NeurIPS 2024 论文评审将于今日发布。我想创建一个讨论主题，让我们讨论任何问题/抱怨/庆祝或其他任何事情。 每年的评审中都有这么多噪音。考虑到 NeurIPS 这些年来发展如此之大，一些作者引以为豪的好作品可能会因为嘈杂的系统而获得低分。我们应该记住，无论分数如何，这项工作仍然很有价值。    提交人    /u/zy415   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/</guid>
      <pubDate>Tue, 30 Jul 2024 12:42:53 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 近年来您真正喜欢的非计算密集型研究出版物？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efmmnn/discussion_non_compute_hungry_research/</link>
      <description><![CDATA[整个行业和学术界都出现了一些出色的成果。但是，一项成果的炒作越大，它通常需要的资源/计算量就越大。 那么学术界/行业/小团队（或单个作者）独立完成的一些成果呢？这些成果确实具有基础性或影响力，但所需的计算量却很少（单个或双 GPU，有时甚至 CPU）？ 您想到了哪些成果？为什么您认为它们脱颖而出？    提交人    /u/HopeIsGold   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efmmnn/discussion_non_compute_hungry_research/</guid>
      <pubDate>Tue, 30 Jul 2024 06:43:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    </channel>
</rss>