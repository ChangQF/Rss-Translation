<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 22 Oct 2024 12:33:06 GMT</lastBuildDate>
    <item>
      <title>[讨论] 用于创建富有表现力的视频语音的最佳文本转音频语音 API？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g9fhg7/discussion_best_text_to_audio_voice_api_for/</link>
      <description><![CDATA[我正在寻找最佳文本转语音 API 的推荐，我主要关注 OpenAI、11labs、Google Voice 和 Amazon polly。 因此，如果您知道其中任何最适合我的用例的 API，那就太好了。 如果您有使用这些 API 的经验，请分享。或者，如果您知道任何其他类型的推荐 API，请在评论中告诉我。 谢谢。    提交人    /u/pushkarsingh32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g9fhg7/discussion_best_text_to_audio_voice_api_for/</guid>
      <pubDate>Tue, 22 Oct 2024 11:07:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 图灵测试的谬误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g9exc6/d_the_fallacy_of_the_turing_test/</link>
      <description><![CDATA[大家好！在我的最新文章中，我探讨了图灵测试如何影响人工智能研究，并导致了关于智能和人工智能的一些基本误解。根据我在人工智能研究和创业方面的经历，我讨论了我们如何超越图灵以人为本的观点，探索理解和使用人工智能的新方法。  我很想听听你的想法并进行讨论！这是链接：https://medium.com/@n.nanas/the-fallacy-of-the-turing-test-5c4f00be7f0e    提交人    /u/Kitchen-Big-5911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g9exc6/d_the_fallacy_of_the_turing_test/</guid>
      <pubDate>Tue, 22 Oct 2024 10:31:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找同伴来探索模拟大脑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g9aa5y/d_looking_for_peers_to_explore_analog_brain/</link>
      <description><![CDATA[我要构建一个模拟大脑。整个模型由一个稳态控制器控制，该控制器根据身体状态驱动大脑进入不同的计算模式。 我欢迎任何想法，尤其是负面反馈。信息比个人意见更重要。 我可以清楚地在脑海中看到它。我缺少的是同行来提出这个想法并使其成为现实。    提交人    /u/affirmedtuna352   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g9aa5y/d_looking_for_peers_to_explore_analog_brain/</guid>
      <pubDate>Tue, 22 Oct 2024 04:52:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 Meta Movie Gen 模型架构的几个问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g92l2t/d_a_few_questions_on_the_architecture_of_metas/</link>
      <description><![CDATA[我阅读了 Meta Movie Gen 论文来了解他们的模型。目前我对架构基本满意，但仍有几个地方我不确定发生了什么。希望能得到一些澄清 1) 从 TAE 输出潜在代码到 Transformer 输入嵌入的转换。论文中有这样一段话：  如第 3.1.1 节所述，我们在学习到的视频潜在空间表示中执行生成。此潜在代码的形状为 T x C x H x W 。为了准备 Transformer 主干的输入，首先使用 3D 卷积层（Dosovitskiy et al., 2021）对视频潜在代码进行“修补”，然后将其展平以生成 1D 序列。 3D 卷积层使用大小为 k_t x k_h x k_w 的核，步长等于核大小，并将其投影到与 Transformer 主干所需的相同维度中。因此，输入到 Transformer 主干的 token 总数为 THW/(k_t k_h k_w)。我们使用 k_t = 1 和 k_h = k_w = 2，即我们生成 2 x 2 个空间块。  这里，为什么没有提到通道维度？核不应该是 5 维的吗，包括输入通道和输出通道？如果是这样，那么输出通道的数量是否等于 Transformer 嵌入维度？ 2）紧接着，本文讨论了分解可学习嵌入：  我们使用分解可学习位置嵌入来实现 Transformer 的任意大小、宽高比和视频长度（De- hghani et al.，2024）输入。D 维的绝对嵌入可以表示为映射 phi(i) : [0, maxLen] -&gt; RD，其中 i 表示补丁的绝对索引。我们将“patchified”标记（即 3D 卷积层的输出）转换为空间 h、w 和时间 t 坐标的单独嵌入 phi_h、phi_w 和 phi_t……  在嵌入已经展平为 1-D 之后，如何按维度分离嵌入？当我们仍然将时间和空间坐标隔离时，位置嵌入是否应该在“3-D”卷积之前添加？ 3) 自适应层规范块如何工作？我查阅了 Peebles 和 Xie 的原始参考资料，其中写道：  我们不是直接学习维度尺度和平移参数 γ 和 β，而是从 t 和 c 的嵌入向量之和中对它们进行回归  我不太明白这里“回归”这个词是什么意思，或者如何从 t 和 c 中获得 γ 和 β。我也不明白这如何适用于没有 c 向量的 Movie Gen。    提交人    /u/throwaway2676   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g92l2t/d_a_few_questions_on_the_architecture_of_metas/</guid>
      <pubDate>Mon, 21 Oct 2024 22:20:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] Nuggt：在 React 组件事件捕获数据上运行的 LLM 代理（开源）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g91gqn/p_nuggt_an_llm_agent_that_runs_on_react_component/</link>
      <description><![CDATA[   我在想，如果我可以将我的 React 项目中的所有产品分析数据与 React 组件代码一起放在一个地方，连接到 LLM 代理进行推理、分析和可视化，那会怎样？所以我尝试了，效果很好。将所有事件捕获数据集中在一个地方，您可以使用 LLM 代理来执行以下操作： https://preview.redd.it/dmbcmji9h6wd1.png?width=2846&amp;format=png&amp;auto=webp&amp;s=0c1a788dab18bf14a8133053b7a3c24b0336d8a4  分析数据 可视化数据 根据数据做出决策 集思广益关于后续步骤 根据更改生成更新的反应组件代码 重新启动整个分析过程（迭代..）  您可以在https://github.com/shoibloya/nuggt-analytics 上尝试它 基本上在这个开源项目中，我创建了一个 streamlit 仪表板，允许您将分析集成到您的反应组件并将其连接到 Firestore，所有这些都使用 GPT。然后，一旦捕获的事件数据进入 firestore，我就会将其取回，然后您可以使用 LLM 代理生成决策卡和可视化效果。 让我知道您的反馈    提交人    /u/Loya_3005   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g91gqn/p_nuggt_an_llm_agent_that_runs_on_react_component/</guid>
      <pubDate>Mon, 21 Oct 2024 21:32:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于 RoPE 的 LLM 如何学习注意力集中点（或编码绝对位置）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/</link>
      <description><![CDATA[我最近重新阅读了“注意力接收器”论文（链接），并开始思考 LLM 如何管理注意力接收器。 注意力接收器的概念描述了 LLM 为初始标记分配不成比例的高注意力分数的现象，而不管它们的语义值如何。 这里有一个悖论：最先进的开放式 LLM 通常采用 RoPE（旋转位置嵌入）进行位置编码。由于 RoPE 仅对相对位置进行编码，因此令人费解的是模型如何一致地识别绝对初始标记并为其分配高度注意力。您对这种行为可能如何出现或解释有什么想法吗？    提交人    /u/StraightSpeech9295   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/</guid>
      <pubDate>Mon, 21 Oct 2024 19:46:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 修复 Nightly transformers 中的梯度累积错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/</guid>
      <pubDate>Mon, 21 Oct 2024 19:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2024 焦点中的潜在抄袭：Shengjie Luo 和 Tianlang Chen 的“Gaunt Tensor Products”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</guid>
      <pubDate>Mon, 21 Oct 2024 14:52:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] RWKV-7：无需注意，超越强大的 Modded-GPT 基线（使用 Muon 优化器的基线），同时仅使用 headsz 64</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</link>
      <description><![CDATA[      大家好。 RWKV-7（100% RNN 且无注意力）可以超越强大的 Modded-GPT 基线（带有 Muon 优化器的基线，目前在推特上流行）。 训练代码和日志：https://github.com/BlinkDL/modded-nanogpt-rwkv 如果使用更大的 headsz，它可以达到损失 3.26xx。 但是我当前的实现效率很低。优化后，可能可以达到 ctx1k 下 Modded-GPT 速度的 85%（或比 ctx4k 下 Modded-GPT 更快）。欢迎任何帮助:) https://preview.redd.it/48m3lsvkb4wd1.png?width=873&amp;format=png&amp;auto=webp&amp;s=647d86ed47d40a4f742ed9512a835dee41069e4f  强大的 GPT 基线： https://preview.redd.it/h2ckr31mb4wd1.png?width=584&amp;format=png&amp;auto=webp&amp;s=b667bfbc50298f8335a889b85c55f68ee8db38a5  RWKV-7 摆脱了“线性注意力”设计以实现更高的性能：） https://preview.redd.it/ijyz0sgnb4wd1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=f413d0e7bcd3a76c5e788f2ca231a37706b24345    提交人    /u/bo_peng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</guid>
      <pubDate>Mon, 21 Oct 2024 14:18:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] AISTATS 会进行‘修改后接受’吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/</link>
      <description><![CDATA[AISTATS 会“接受修改”吗？ 我是第一次来的学生作者。提交了我的草稿，但我觉得我可以通过在这里和那里进行调整来进一步改进草稿（甚至在附录中添加几个新部分）。 这在反驳阶段可行吗？还是他们只允许我进行外观上的更改？我听说在其他一些会议上可以进行更改，但不确定 AISTATS 是否允许。 谢谢！    提交人    /u/confirm-jannati   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/</guid>
      <pubDate>Mon, 21 Oct 2024 14:11:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于推理的高效 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</link>
      <description><![CDATA[我正在使用高分辨率图像进行物体检测项目。 有没有什么技术可以使训练有素的 CNN（UNet）在推理过程中更有效率？我知道修剪就是这样一种技术，但它有损失准确性和可并行性的风险。    提交人    /u/_My__Real_Name_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</guid>
      <pubDate>Mon, 21 Oct 2024 08:20:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] Google Shopping 10M 数据集，用于大规模多模式产品检索和排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</link>
      <description><![CDATA[我们终于在 Hugging Face 上发布了 Marqo Google Shopping 1000 万数据集 (Marqo-GS-10M)。这是用于多模式产品检索的最大、最丰富的数据集之一！  1000 万行查询、产品标题、图片和排名 (1-100) ~10 万个唯一查询 ~500 万个时尚和家居领域的唯一产品 反映了真实世界的数据和用例，并可作为方法开发的良好基准 适当的数据拆分、域内、新查询、新文档以及新文档和新查询。   该数据集为每个查询-文档对提供了详细的相关性分数，以方便将来的研究和评估。 !pip install datasets from datasets import load_dataset ds = load_dataset(&quot;Marqo/marqo-GS-10M&quot;)  我们将这个大规模数据集作为我们训练框架发布的一部分进行策划：广义对比学习 (GCL)。  数据集：https://huggingface.co/datasets/Marqo/marqo-GS-10M GCL：https://github.com/marqo-ai/GCL 论文：https://arxiv.org/abs/2404.08535    提交人    /u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</guid>
      <pubDate>Sun, 20 Oct 2024 21:51:41 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 现在我有一份工程师的工作，我如何才能了解最新的有趣的论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</link>
      <description><![CDATA[大家好，我以前在实验室工作，研究计算机视觉和机器学习。通过与教授和博士交谈，我可以了解到一些有趣的新文章。现在我在一家大公司工作，我不再有这个网络，也没有时间花几个小时搜索有趣的新文章。有没有什么好的资源可以汇总与机器学习和计算机视觉相关的精彩文章？    提交人    /u/Fugius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</guid>
      <pubDate>Sun, 20 Oct 2024 21:07:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</guid>
      <pubDate>Sun, 20 Oct 2024 15:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>