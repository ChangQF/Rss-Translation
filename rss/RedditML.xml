<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 13 Feb 2025 18:22:57 GMT</lastBuildDate>
    <item>
      <title>[D] 对这部小说的第二意见2d关注？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iope92/d_second_opinions_on_this_novel_2d_attention/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iope92/d_second_opinions_on_this_novel_2d_attention/</guid>
      <pubDate>Thu, 13 Feb 2025 18:09:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlignRec 在多模式推荐中的表现优于 SOTA 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ioo1ta/r_alignrec_outperforms_sota_models_in_multimodal/</link>
      <description><![CDATA[AlignRec 在 AlignRec：多模态推荐中的对齐和训练 (CIKM &#39;24) 中引入，可解决多模态推荐系统中的错位问题。由于语义差距，传统方法难以整合各种内容类型（文本、图像和分类 ID）。AlignRec 通过优化三个对齐任务来解决这个问题：内容间 (ICA)、内容类别 (CCA) 和用户项目 (UIA)。ICA 将语义表示与基于注意的编码器统一起来，CCA 使用对比学习增强特征对齐，UIA 通过余弦相似度损失优化用户项目表示。 AlignRec 的两阶段训练是一项关键创新：预训练对齐视觉和文本数据，而微调结合用户行为以优化推荐。在亚马逊数据集上测试后，它的表现优于九种 SOTA 模型，在长尾推荐方面表现出色。通过弥合多模式语义差距，AlignRec 提高了准确性和稳健性，从而推进了多模式 AI 驱动的推荐。 有关框架和结果的深入了解，请参阅此处的完整论文：https://www.shaped.ai/blog/multimodal-alignment-for-recommendations    提交人    /u/skeltzyboiii   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ioo1ta/r_alignrec_outperforms_sota_models_in_multimodal/</guid>
      <pubDate>Thu, 13 Feb 2025 17:13:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你如何从头开始进行机器学习研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ion90w/d_how_you_do_ml_research_from_scratch/</link>
      <description><![CDATA[在顶级机器学习会议（NIPS、ICML、ICLR）或领域导向会议（CVPR、ICCV、ACL、EMNLP、KDD、SIGIR）上发表过作品的人。1. 你是如何从 0 走到第一篇论文的？2. 你的技能水平如何（Pytorch 或领域知识）？3. 你遵循的整个过程是什么，才能擅长实现你的想法？4. 你是如何想出一个想法和解决方案的？    提交人    /u/AntelopeWilling2928   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ion90w/d_how_you_do_ml_research_from_scratch/</guid>
      <pubDate>Thu, 13 Feb 2025 16:40:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] SWE-agent 是 SWE-bench Lite 上的新开源 SOTA</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iolpvo/r_sweagent_is_the_new_opensource_sota_on_swebench/</link>
      <description><![CDATA[SWE-agent 是一种开源软件工程代理，可与任何类型的模型配合使用。我们的 1.0 版本增加了大量新功能：大规模并行运行；基于云的部署；使用工具包实现广泛的可配置性；新的命令行界面和实用程序。完全开源 (MIT)、广泛的配置、易于破解。由于它使用 LiteLLM 进行 LM 接口，因此您可以将它与本地 LM 一起使用：我们已将它与 Qwen 一起使用，其他社区成员已将它与 Llama 一起使用。 https://github.com/swe-agent/swe-agent SWE-agent 现在由我们的新 SWE-ReX 包（也是 MIT 许可）提供支持，这是一个轻量级的通用沙盒代码执行引擎，支持本地 Docker、AWS、Modal 部署 https://github.com/SWE-agent/swe-rex。您可以使用它轻松地从头开始构建具有代码执行功能的代理，而无需费心弄清楚如何与正在运行的 docker 容器进行通信！ SWE-agent 由我们在普林斯顿大学和斯坦福大学开发。如果您有任何疑问，我们将在这里。    由    /u/ofirpress   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iolpvo/r_sweagent_is_the_new_opensource_sota_on_swebench/</guid>
      <pubDate>Thu, 13 Feb 2025 15:35:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推理 LLM 能否帮助我们更好地识别相关作品？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iol1qd/d_could_reasoning_llms_help_use_identify_relevant/</link>
      <description><![CDATA[我知道有很多有用的服务可以帮助你消化 arXiv 中的最新论文，例如 arxiv-sanity、paper digest、arXivist、IArxiv 等。他们中的大多数都使用 ML（TF-IDF）根据你的兴趣对论文进行排名，但即使有了他们的帮助，我仍然被论文淹没了。 大多数工具都是在 LLM 之前构建的（尤其是预推理模型），你们认为推理 LLM 是否可以更好地帮助我们从 arXiv 每日出版中识别相关作品？ 或者你听说过任何现有的方法？    提交人    /u/MadEyeXZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iol1qd/d_could_reasoning_llms_help_use_identify_relevant/</guid>
      <pubDate>Thu, 13 Feb 2025 15:05:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 企业中的文本转 SQL：比较方法以及对我们有效的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iojc1f/r_texttosql_in_enterprises_comparing_approaches/</link>
      <description><![CDATA[      大家好！ 文本转 SQL 是一种流行的 GenAI 用例，我们最近与一些企业合作开发了它。在这里分享我们的经验！ 这些企业已经尝试了不同的方法——使用 O1 等最佳 LLM、将 RAG 与 GPT-4o 等通用 LLM 结合使用，甚至使用 AutoGen 和 Crew 的基于代理的方法。但它们的准确率达到了 85% 的上限，响应时间超过 20 秒（主要是由于列名错误导致的错误），并且处理了使扩展变得困难的复杂工程。 我们发现，对特定于业务的查询-SQL 对进行开放权重 LLM 微调可实现 95% 的准确率，将响应时间缩短至 7 秒以下（通过消除故障恢复），并简化了工程。这些定制的 LLM 保留了域内存，从而带来了更好的性能。 我们在medium上对所有尝试过的方法进行了比较。请让我知道您的想法，如果您发现更好的方法来解决这个问题。 https://preview.redd.it/kqfabsdkuwie1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=88251e0cfa246f2bf1f779e708ab03a96a3c0255    提交人    /u/SirComprehensive7453   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iojc1f/r_texttosql_in_enterprises_comparing_approaches/</guid>
      <pubDate>Thu, 13 Feb 2025 13:44:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用在线图像自收集数据集的许可证问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ioj5ij/d_license_issue_with_selfcollected_dataset_using/</link>
      <description><![CDATA[因此，我正在通过收集和注释在线图像来处理数据集。不幸的是，并非所有图像都受 CC 许可。在发布的数据集中仅包含这些图像的链接是否合适？（比如这是否被视为合理使用，还是会引起麻烦？）是否有任何流行的公共图像数据集（包括不受 CC 许可的图像）可供我参考？我对这些与版权相关的事物非常不熟悉，因此如果我在问题描述中犯了任何错误，请提前道歉。    提交人    /u/RepresentativeAd985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ioj5ij/d_license_issue_with_selfcollected_dataset_using/</guid>
      <pubDate>Thu, 13 Feb 2025 13:34:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自动化能力发现：使用基础模型自我探索和评估人工智能能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iofk29/r_automated_capability_discovery_using_foundation/</link>
      <description><![CDATA[本文介绍了一个名为自动能力发现 (ACD)的框架，该框架使用一个基础模型来系统地探索和评估另一个模型的能力。核心思想是将能力发现视为一门实验科学，其中一个模型充当科学家生成假设和设计测试。 关键技术点： - 框架由四个主要组件组成：任务生成、执行、评估和分析 - 使用提示策略使评估器模型生成多样化、有意义的测试 - 实现反馈循环，其中测试结果为未来的任务生成提供信息 - 评估包括二元成功/失败和详细分析 - 在 GPT-4、Claude 和 Llama 模型上作为评估者和主体进行测试 结果： - 发现了数千种以前未记录的能力 - AI 评估者和人工验证在能力评估方面的一致性为 89% - 生成的测试涵盖了从基础（算术）到复杂（创意写作）的广泛能力类别 - 成功识别了已知的模型局限性 - 显示出自动和手动评估方法之间的很强的相关性 我认为这种方法可以改变我们理解和评估人工智能系统的方式。我们可以持续、自动地探索模型能力，而不是仅仅依赖预定义的基准或手动测试。这对于快速测试新模型和识别意外的能力或局限性尤其有价值。 我认为主要的挑战是确保评估模型不受与主题模型相同的盲点限制。还有一个问题是，这在语言模型之外推广到其他人工智能架构的效果如何。 TLDR：新框架使用人工智能模型自动发现和评估其他人工智能模型的能力，与人类评估显示出很强的一致性，并发现了数千种以前未知的能力。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iofk29/r_automated_capability_discovery_using_foundation/</guid>
      <pubDate>Thu, 13 Feb 2025 09:43:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要针对 2025 年图像分类问题的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iocgvg/d_need_suggestions_for_image_classification/</link>
      <description><![CDATA[早在 2022 年末，我就使用 EfficientNet_V2 训练了一个图像分类模型（医学图像，高分辨率），数据量约为 20k。现在我想重新训练模型，因为我可以访问大量数据（~300k）。我想征求一些建议。  我以前尝试过使用 ViT，但它的性能相对较差。我以前读过一些评论，说 ViT 在处理高分辨率图像方面存在一些问题。但现在我注意到 Nvidia 在 DLSS 上使用 Transformer。我认为高分辨率不再是 ViT 的问题。建议尝试哪种图像分类 ViT 模型？ 我一直使用预训练权重作为起点并进行微调，因为我读过的许多文章/在线信息都告诉我这样做，而且它确实表现更好。 2025 年还建议使用预训练权重吗？尤其是大多数图像模型都是在低分辨率数据（224-512）上训练的，而我的数据集是高分辨率的。 CNN 在 2025 年过时了吗？我认为 CNN 和 Transformer 在图像相关问题上的竞争在 2023 年还不明朗。但从 2024 年中期开始，我看到很多人说 Transformer 赢了。     提交人    /u/Eternal1314   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iocgvg/d_need_suggestions_for_image_classification/</guid>
      <pubDate>Thu, 13 Feb 2025 05:55:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为不规则时间序列数据创建因果 DAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1io7zrf/d_creating_a_causal_dag_for_irregular_timeseries/</link>
      <description><![CDATA[大家好， 我最近发表了一篇关于使用不规则时间序列数据进行因果推断的文章。我喜欢使用动态贝叶斯网络来做这件事的想法，因此我将问题改写为这个。 我不确定如何处理具有不规则采样分辨率的时间序列数据。具体来说，在有 2 支球队且数据是逐个事件数据的运动场景中，这些事件（例如传球）从比赛开始到结束按顺序发生。最终，我想探索这些数据中干预的因果影响。 有人建议使用 SSM。据我所知，当它被离散化时，它可以表示为 DAG？然后我有一个结构来表示这些因果关系。 其他工作流程可能是： - 这个库：https://github.com/jakobrunge/tigramite - 使用 ARIMA 消除时间序列数据的趋势，然后使用某种贝叶斯推理来捕获因果效应 - 使用 SSM 创建因果结构和贝叶斯推理来捕获因果效应 - 利用 CausalImpact 库 - 还有 GSP，然后使用图形信号作为因果模型（如 BART）的输入 虽然我建议使用 2 个库，但我喜欢制定适当的因果工作流程，而不是让一个库做所有事情。这只是为了让我更好地理解因果推理。 我最初偶然发现了这篇有趣的论文：https://arxiv.org/pdf/2312.09604，它似乎不适用于不规则的采样分辨率。 还有对时间序列数据的分组，这会导致信息丢失。因果关系不会立即发生在这些数据中，因此以半秒或秒为单位对其进行分组可能会有效。 我对因果推理还很陌生，所以任何批评或建议都会受到欢迎！ 非常感谢！    提交人    /u/Sea_Farmer5942   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1io7zrf/d_creating_a_causal_dag_for_irregular_timeseries/</guid>
      <pubDate>Thu, 13 Feb 2025 01:47:05 GMT</pubDate>
    </item>
    <item>
      <title>[R]“o3 在 2024 年 IOI 上获得金牌，并获得与人类精英竞争对手相当的 Codeforces 评级”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1io4c7r/r_o3_achieves_a_gold_medal_at_the_2024_ioi_and/</link>
      <description><![CDATA[使用大型推理模型进行竞争性编程 OpenAI 我们表明，将强化学习应用于大型语言模型 (LLM) 可显著提高复杂编码和推理任务的性能。此外，我们将两个通用推理模型 - OpenAI o1 和 o3 的早期检查点 - 与领域特定系统 o1-ioi 进行了比较，后者使用专为参加 2024 年国际信息学奥林匹克 (IOI) 而设计的手工设计的推理策略。我们在 IOI 2024 上与 o1-ioi 进行了现场比赛，并使用手工制作的测试时间策略，排名在第 49 个百分位。在放宽竞争限制的情况下，o1-ioi 获得了金牌。然而，在评估 o3 等后期模型时，我们发现 o3 在没有手工制作领域特定策略或放宽限制的情况下获得了金牌。我们的研究结果表明，尽管 o1-ioi 等专用管道取得了显著的改进，但扩展的通用 o3 模型无需依赖手工制作的推理启发式方法即可超越这些结果。值得注意的是，o3 在 2024 年 IOI 上获得金牌，并获得与人类精英竞争对手相当的 Codeforces 评级。总体而言，这些结果表明，扩展通用强化学习，而不是依赖特定领域的技术，为在推理领域（例如竞争性编程）中实现最先进的 AI 提供了一条稳健的道路。 https://arxiv.org/abs/2502.06807    提交人    /u/we_are_mammals   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1io4c7r/r_o3_achieves_a_gold_medal_at_the_2024_ioi_and/</guid>
      <pubDate>Wed, 12 Feb 2025 22:55:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] TAID：时间自适应插值蒸馏，用于语言模型中的高效知识转移</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1invg9p/r_taid_temporally_adaptive_interpolated/</link>
      <description><![CDATA[    /u/hardmaru   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1invg9p/r_taid_temporally_adaptive_interpolated/</guid>
      <pubDate>Wed, 12 Feb 2025 16:51:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 新论文：前沿模型能否以开放的方式自我探索并发现自己的能力？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1inv1gy/r_new_paper_can_frontier_models_selfexplore_and/</link>
      <description><![CDATA[      标题：通过模型自我探索实现自动能力发现 作者： Cong Lu、Shengran Hu、Jeff Clune。 论文： https://arxiv.org/abs/2502.07577 摘要：基础模型已成为通用助手，通过对网络规模数据进行训练，在众多领域展现出多样化的能力。准确描述任何新模型的全部能力和潜在风险中的哪怕一小部分仍然具有挑战性。现有的评估方法通常需要大量的人力，而且需要付出越来越多的努力来为更强大的模型设计更艰巨的挑战。我们引入了自动能力发现 (ACD)，这是一个框架，它将一个基础模型指定为科学家，以系统地提出探索主题模型（可能是它本身）能力的开放式任务。通过将前沿模型与开放性领域的想法相结合，ACD 可以自动且系统地发现主题模型中令人惊讶的能力和失败。我们在一系列基础模型（包括 GPT、Claude 和 Llama 系列）中展示了 ACD，表明它会自动揭示任何单个团队都难以发现的数千种能力。我们通过广泛的人工调查进一步验证了我们方法的自动评分，观察到模型生成的评估和人工评估之间高度一致。通过利用基础模型创建任务和自我评估的能力，ACD 朝着可扩展、自动评估新型 AI 系统迈出了重要一步。 https://preview.redd.it/1zamtbjzjqie1.png?width=1204&amp;format=png&amp;auto=webp&amp;s=95c177136d8c77abd0b8fb4fda3d8d7f01b7a04f    提交人    /u/MolassesWeak2646   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1inv1gy/r_new_paper_can_frontier_models_selfexplore_and/</guid>
      <pubDate>Wed, 12 Feb 2025 16:34:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>