<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 24 Feb 2024 15:11:32 GMT</lastBuildDate>
    <item>
      <title>[D] ECAI？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aywkcd/d_ecai/</link>
      <description><![CDATA[与 A* 级别的 ECAI 相比，ECAI 的竞争力和顶级程度如何？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aywkcd/d_ecai/</guid>
      <pubDate>Sat, 24 Feb 2024 15:02:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] 理解、使用和微调 Gemma</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayvnno/p_understanding_using_and_finetuning_gemma/</link>
      <description><![CDATA[    /u/seraschka   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayvnno/p_understanding_using_and_finetuning_gemma/</guid>
      <pubDate>Sat, 24 Feb 2024 14:21:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 需要与手势识别任务相关的指导。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayubsq/d_p_need_guidance_related_to_gesture_recognition/</link>
      <description><![CDATA[您好，我是学生，目前正在毕业最后一年，需要一些与最后一年项目相关的指导。 我们被分配了一个项目，学生课堂手势检测，能够检测手势以及它是谁的手势。我们准备了主要针对手势的数据集（举手、阅读和写作），我们面临的问题是如何同时使用两种不同的模型：人脸识别和手势检测。为了能够生成学生的最终报告，包括他们的手势计数和参与度报告。 对于面部检测，我们正在考虑使用 Deepface，对于手势检测，我们正在考虑 Yolo v8 模型。 任何建议会很有帮助。谢谢你的时间。    由   提交/u/Ali_6200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayubsq/d_p_need_guidance_related_to_gesture_recognition/</guid>
      <pubDate>Sat, 24 Feb 2024 13:17:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] 致力于提高机器学习框架的公平性，想了解哪些公平性指标是可微分的，哪些是不可微分的？如何验证？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aytd82/r_working_on_improving_fairness_of_a_machine/</link>
      <description><![CDATA[现在我正在研究 6 种不同的公平性指标：统计奇偶性、预测奇偶性、预测平等、机会均等、赔率均等和条件使用准确性平等。我发现一篇论文（https://openreview.net/pdf?id=x-mXzBgCX3a）说观察公平性指标是不可微的（包括统计奇偶性），但没有解释原因。   由   提交/u/Intrepid_Ad_5904   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aytd82/r_working_on_improving_fairness_of_a_machine/</guid>
      <pubDate>Sat, 24 Feb 2024 12:26:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习数学表达式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aysm4o/d_learning_the_mathematical_expressions/</link>
      <description><![CDATA[我需要更好地阅读算法表达式以及如何扩展或简化它们，特别是针对 ML。有谁知道有资源吗？    由   提交/u/Inside-Ad-9118   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aysm4o/d_learning_the_mathematical_expressions/</guid>
      <pubDate>Sat, 24 Feb 2024 11:42:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mamba 模型能否通过智能上下文压缩克服复制挑战？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aysghl/d_can_the_mamba_model_overcome_its_copying/</link>
      <description><![CDATA[我一直在深入研究 Mamba 架构及其上下文压缩方法的细微差别，这似乎为传统 Transformer 的方式提供了一种明智的替代方案通过考虑所有内容来处理上下文。然而，Mamba 方法的一个值得注意的缺点是，它在需要复制整个文本序列的任务中明显存在困难。这种限制可归因于 Mamba 模型仅访问输入的压缩版本，这可能会阻碍它们在输出中完全重建输入的能力。这与总是访问所有数据的 Transformer 形成鲜明对比。 鉴于此，我一直想知道在此类复制场景中 Mamba 模型是否有解决方法。例如，让我们考虑一个 Mamba 模型，该模型旨在将上下文压缩为大约“六”个字符。字。在要求模型打印序列“Hello World!”的任务中你好吗！”，模型能否在多次迭代中战略性地选择输入的部分以实现完美的复制？例如：  在第一遍中，模型专注于“打印出以下序列：Hello”，利用其压缩的上下文容量来掌握指令并启动复制任务。   li&gt; 在输出“Hello”之后，模型可以在后续迭代中压缩输入，例如“打印出以下序列：... &#39;World&#39;... 输出：Hello...” ;，允许它通过按顺序附加下一个单词来继续任务。  这种迭代方法，涉及有选择地压缩上下文以包括任务指令、当前焦点单词和最重要的单词最近的输出，似乎可以使 Mamba 架构有效地复制 Transformer 模型的复制功能，尽管其固有的压缩。 这种方法理论上是否允许 Mamba 模型在涉及复制的任务中匹配 Transformer文本序列？ 我很想听听对此的想法。谨致问候！   由   提交 /u/Alarmed-Profile5736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aysghl/d_can_the_mamba_model_overcome_its_copying/</guid>
      <pubDate>Sat, 24 Feb 2024 11:33:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] LoRA+：大型模型的高效低阶自适应</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayrfpt/r_lora_efficient_low_rank_adaptation_of_large/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.12354 代码：https ://github.com/nikhil-ghosh-berkeley/loraplus 摘要：  在本文中，我们展示了Hu 等人最初引入的低秩适应 (LoRA)。 (2021) 导致大宽度（嵌入维度）模型的微调不理想。这是因为 LoRA 中的适配器矩阵 A 和 B 以相同的学习率更新。使用大宽度网络的缩放参数，我们证明对 A 和 B 使用相同的学习率并不能实现高效的特征学习。然后我们证明，只需通过为 LoRA 适配器矩阵 A 和 B 设置不同的学习率以及精心选择的比率，即可纠正 LoRA 的这种次优性。我们将此提议的算法称为LoRA+。在我们广泛的实验中，LoRA+ 提高了性能（1-2% 的改进）和微调速度（高达 ∼ 2 倍加速），而计算成本与 LoRA 相同。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayrfpt/r_lora_efficient_low_rank_adaptation_of_large/</guid>
      <pubDate>Sat, 24 Feb 2024 10:28:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 本科生的机器学习实习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/</link>
      <description><![CDATA[所以我目前是大学四年级学生，学习软件工程。期望获得 ML 实习生的职位是否现实？  对于那些能够做到这一点的人，您的投资组合中有哪些类型的项目？我已经学习了课程并创建了一些小项目，但到目前为止还没有什么重大项目，这些会帮助我被聘用吗？ 此外，如果有人正在寻找 ML/DS 实习生，我可以向您发送我的简历，哈哈。我真的很想踏入这门并开始积累经验。毕业后我将攻读硕士学位。   由   提交 /u/Critical-Strategy914    reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aypqzh/d_ml_internship_for_an_undergrad/</guid>
      <pubDate>Sat, 24 Feb 2024 08:36:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与变形金刚相比，曼巴的根本缺点是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</link>
      <description><![CDATA[你好！ 我思考这个问题有一段时间了。澄清一下，我并不是指“它尚未经过广泛测试”之类的方面。 “它的可扩展性是不确定的，”或“缺乏工业基础设施。”相反，我有兴趣了解 Transformer 和 Mamba 架构之间的核心差异，特别是这些差异如何使 Mamba 与 Transformers 相比处于劣势。 致以诚挚的问候！ &lt;强&gt;编辑： 据我从您的回答中了解到，变形金刚“更好”与 Mamba 相比，在以下意义上：  Transformers 不会压缩输入。 Transformers 可以处理非顺序数据。 Transformer处理位于输入末尾的指令可能会更好。  编辑 2： 总结一下：&lt; /p&gt;  变形金刚：在更大的环境中进行更多计算，但可以访问更多信息，尽管可能是一些无用的信息 Mamba： Less计算更大的上下文，但访问的信息较少，因此存在丢失信息的风险。    由   提交 /u/Alarmed-Profile5736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</guid>
      <pubDate>Sat, 24 Feb 2024 07:11:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 编写 ML 软件时 - 如何使用 TDD？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</link>
      <description><![CDATA[请告诉我是否有更好的子程序。 测试驱动开发。 &lt; p&gt;我已经在 ML 软件上工作了一段时间，我觉得我突然想要更好地遵循 TDD 并尝试将其应用到更细致的 ML 用例中。 有一件事多年来我注意到需求和设计对于我们的工作来说可能是模糊的——我所做的很多事情至少从最简单的设计开始，然后我们依靠迭代和强大的评估框架来证明某些改进是否合理。已实施（如果不能提高性能，为什么要实施任何东西）。在这些类型的原型设计场景中，TDD 可能会成为一个巨大的时间杀手，并且在您确定设计之前有点无用。 不过，当需求明确时，TDD 非常好，所以我正在努力变得更好将其纳入我的武器库中。 您对 TDD 有何看法以及如何/何时使用它？   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</guid>
      <pubDate>Sat, 24 Feb 2024 02:05:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 关于 MoE 和 Mamba 实施的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</link>
      <description><![CDATA[大家好， 我正在深入研究我的硕士论文，需要一些指导。我工作的核心是线性化一个充满记忆效应的复杂函数。虽然 Transformer 架构已在文献中进行了探讨，但我正在考虑采用 Mamba 架构的全新角度，或通过 MoE（专家混合）方法为 Transformer 增添趣味。 Moe-Mamba 也在讨论中。 问题是：这是我第一次真正使用这些架构，所以我真的不知道从哪里开始才能真正实现它们代码。 我应该在哪里更多地了解这些架构？您还可以为这些架构建议一些代码实现（我认为还没有库）吗？ PS：我知道我仍然需要研究很多关于这些主题的内容，所以不要评判我的愚蠢有问题请教，所以才来请教，我想学习！ :)   由   提交/u/PaleAle34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</guid>
      <pubDate>Fri, 23 Feb 2024 22:13:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代降维</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</link>
      <description><![CDATA[大家好， 我熟悉更经典的降维技术，如 SVD、PCA 和因子分析。但是，有没有一些现代技术或者人们多年来学到的一些技巧可以分享。对于上下文来说，这适用于表格数据。谢谢！   由   提交 /u/MuscleML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</guid>
      <pubDate>Fri, 23 Feb 2024 20:08:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 情节曲折</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</link>
      <description><![CDATA[看到一些 ICLR 结果似乎让社区感到惊讶：  Mamba ➡️ 拒绝 V-JEPA ➡️拒绝 MetaGPT ➡️按照讨论接受（口头）这里  还有哪些接受/拒绝引起了一些人的注意？   由   提交 /u/hzmehrdad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</guid>
      <pubDate>Fri, 23 Feb 2024 16:37:43 GMT</pubDate>
    </item>
    <item>
      <title>[R]“生成模型：他们知道什么？他们知道事情吗？让我们找出答案！”。论文引用：“我们的研究结果表明，我们研究的所有类型的生成模型都包含有关场景内在因素（法线、深度、反照率和阴影）的丰富信息，这些信息可以使用 LoRA 轻松提取。”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</guid>
      <pubDate>Fri, 23 Feb 2024 14:51:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>