<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Sun, 25 Aug 2024 01:13:21 GMT</lastBuildDate>
    <item>
      <title>[讨论] 双塔推荐系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0jt1e/discussion_two_tower_recommendation_system/</link>
      <description><![CDATA[大家好， 我正在我的公司探索用于用户和内容推荐的双塔架构。我拥有的数据仅由积极的用户和内容交互组成。（即，我没有任何用户忽略所呈现内容的场景数据）。 我正在努力解决以下实施细节。  批量负抽样似乎是训练候选生成阶段的一种流行方法。在批次中，如果“i”不等于“j”，则用户 Ui 和内容 Cj 被视为负样本。但是，如果一批中有特定用户的多条交互记录，则批量负抽样可能会导致模型的训练数据发生冲突。我们该如何处理这个问题？我们是否需要确保每个批次对于给定用户只有一个交互记录？ 如果我想在验证集中测量模型性能（或想使用早期停止），如何为验证集生成负样本。我可以使用与批次内抽样相同的方法。但我会遇到给定用户多次交互的相同问题。 您能否推荐一种将数据分成训练集和验证集的好方法？我是否应该确保我有一组用户及其所有交互都只在验证集中。即对于那些用户，训练集中没有交互  任何意见和建议都将不胜感激。 谢谢！    提交人    /u/Large_Constant7234   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0jt1e/discussion_two_tower_recommendation_system/</guid>
      <pubDate>Sun, 25 Aug 2024 00:24:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 边缘计算/联邦学习和 KAN 结合，能否提供一些研究和应用的范围或见解？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0ixra/r_edge_computingfederated_learning_and_kan/</link>
      <description><![CDATA[如何有效应对在联邦学习中实施 KAN 的挑战？数据异质性、灾难性遗忘和有限的计算能力是分布式环境中的真正挑战。您是否有任何想法或范围可以建议或任何论文？    提交人    /u/StopFrequent542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0ixra/r_edge_computingfederated_learning_and_kan/</guid>
      <pubDate>Sat, 24 Aug 2024 23:41:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 购买了二手 Gigabyte 3090 TI 用于数据科学/机器学习工作，我还应该更换哪些组件？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0gx1w/d_bought_a_used_gigabyte_3090_ti_for_data/</link>
      <description><![CDATA[我在 ML/DS 领域工作。  如果我必须对大量数据进行训练，我会使用云，但我仍然希望有机会运行本地训练脚本并尝试一些 LLM/SLM/Diffusion 模型的推理，这就是我决定为我的旧台式机购买新 GPU 的原因。 我从当地卖家那里找到了一块千兆字节 3090 TI OC，价格约为 450 欧元，我买下了它，认为它物有所值。  但是我的旧电脑配置并不好。我想了解您建议必须更改哪些内容以及我应该购买哪些内容。 这是我的旧电脑： CPU 液体冷却器： Enermax Liqmax II 120 (ELC-LMR120S-BS) CPU： Intel 处理器 Core i5-6600K (Skylake) 四核  RAM： Kingston HyperX Fury Kit Memorie DDR4 16 GB 2400，2x8 GB 电源： Cooler Master RS700-ACABB1-EU Alimentatore 700W ATX B2 系列 12V，1Ventola 120mm，20+4 针，230V， 旧 GPU： Gigabyte NVIDIA GTX 970 G1 Gaming Edition 4GB GDDR5，PCI-E 3.0  主板： Asus Z170 Pro Gaming Intel Scheda Madre，DDR4 1151 机箱： Torre Aerocool Aero-800 gaming SSD： Kingston HyperX SHSS37A/480G Savage SSD，480GB，SATA 3，2.5 英寸 您建议更换哪些组件，我应该购买什么？    提交人    /u/Proud-Discussion7497   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0gx1w/d_bought_a_used_gigabyte_3090_ti_for_data/</guid>
      <pubDate>Sat, 24 Aug 2024 22:06:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 上周医疗 AI：顶级研究论文/模型🏅（2024 年 8 月 17 日至 8 月 24 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0fipu/d_last_week_in_medical_ai_top_research/</link>
      <description><![CDATA[   本周热门论文（8 月 17 日至 24 日）  医学多模态 LLM 的越狱  本文揭示了医学 MLLM 中的安全漏洞。针对 MedMLLM 的新“不匹配的恶意攻击”（2M 攻击）。它介绍了用于测试各种医疗场景的 3MAD 数据集  LLM 不是 零样本生物医学推理器  本文对生物医学任务上的 LLM 进行了基准测试，它在医学分类和 NER 上测试了 LLM，评估了标准提示、CoT、自洽性和 RAG  RuleAlign 框架：将 LLM 与医生规则对齐  本文介绍了用于医学诊断的 LLM 的 RuleAlign 框架。它将 LLM 与特定的诊断规则相结合，并开发基于规则的医学对话数据集。  CTP-LLM：用于临床试验转变预测的 LLM  本文介绍了用于临床试验预测的 CTP-LLM，它介绍了用于基准测试的 PhaseTransition (PT) 数据集。在所有阶段实现 67% 的准确率，从 III 期到批准的准确率达到 75%。  HIBOU：病理学的基础视觉转换器  本文介绍了病理学的视觉转换器，利用 DINOv2 框架在超过 100 万张全幻灯片图像 (WSI) 上预训练两种模型变体 Hibou-B 和 Hibou-L  LLaVA-Surg：多模式手术助手  LLaVA-Surg 引入了大规模手术视频指令调整数据集 Surg-QA，其中包含来自 2,201 个手术程序的超过 102K 个手术视频指令对，并训练了 LLaVA-Surg 模型。  ...  详细查看完整线程：https://x.com/OpenlifesciAI/status/1827442651810918509 感谢您的阅读！如果您知道任何遗漏的有趣论文，请随时在评论中分享。如果您对医疗 AI 有见解或突破，并希望在下周的版本中分享，请通过 Twt/x 与我们联系：OpenlifesciAI    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0fipu/d_last_week_in_medical_ai_top_research/</guid>
      <pubDate>Sat, 24 Aug 2024 21:01:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] 生产中的机器学习：从数据科学家到机器学习工程师</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</link>
      <description><![CDATA[我很高兴与大家分享我整理的一门课程：生产中的机器学习：从数据科学家到机器学习工程师。本课程旨在帮助您从 Jupyter 笔记本中获取任何 ML 模型并将其转变为可用于生产的微服务。 本课程涵盖以下内容：  将您的 Jupyter 代码构建为生产级代码库 管理数据库层 参数化、日志记录和最新的干净代码实践 使用 GitHub 设置 CI/CD 管道 为您的模型开发 API 容器化您的应用程序并使用 Docker 进行部署  我很乐意收到您对本课程的反馈。这是免费访问的优惠券代码：FREETOLEARN。您的见解将帮助我改进和完善内容。如果您喜欢本课程，我希望您留下好评，以便其他人也可以找到这门课程。谢谢，祝您学习愉快！    提交人    /u/5x12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/</guid>
      <pubDate>Sat, 24 Aug 2024 20:54:58 GMT</pubDate>
    </item>
    <item>
      <title>线性注意力——矩阵维数问题[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0e4u8/linear_attention_matrix_dimension_issue_r/</link>
      <description><![CDATA[      我正在阅读线性注意论文Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention。我对 eq(4) 和 eq(5) 中矩阵的维度感到困惑。作者说“用 i 给矩阵加下标会返回第 i 行向量”。我假设 \phi(\cdot) 是一个列向量。然后根据 eq(5)，V_j 必须是列向量，因为它必须左乘以 \phi。因此我假设 V_i 也是一个列向量。然而，eq(5) 最左边的项是 \phi^T，它是一个行向量。这似乎与我上面的想法相矛盾。  https://preview.redd.it/lr61kmu22okd1.png?width=1210&amp;format=png&amp;auto=webp&amp;s=e05296cf8073de618407a60cc744a449dc6c6f14    提交人    /u/mziycfh   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0e4u8/linear_attention_matrix_dimension_issue_r/</guid>
      <pubDate>Sat, 24 Aug 2024 19:58:55 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练和部署 JoJo - Tic Tac 特工 [适合新手]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f093av/p_training_and_deploying_jojo_a_tic_tac_agent/</link>
      <description><![CDATA[        由    /u/Particular_Tap_4002   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f093av/p_training_and_deploying_jojo_a_tic_tac_agent/</guid>
      <pubDate>Sat, 24 Aug 2024 16:17:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] Liger Kernel：一行代码使 LLM 培训速度提高 20%，内存减少 60%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</link>
      <description><![CDATA[        提交人    /u/Icy-World-8359   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0875c/p_liger_kernel_one_line_to_make_llm_training_20/</guid>
      <pubDate>Sat, 24 Aug 2024 15:39:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要有关实时物体检测的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f07gjk/d_need_suggestion_for_realtime_object_detection/</link>
      <description><![CDATA[我们大学有一个项目，要制作一个实时物体检测模型，以实时检测周围的物体。我们想知道哪种预训练模型对速度和准确性有好处。例如，YOLOv5 速度很快，但准确性不高，YOLOv7 则相反。所以，你们有什么建议？    提交人    /u/Aditya_Kumar5155   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f07gjk/d_need_suggestion_for_realtime_object_detection/</guid>
      <pubDate>Sat, 24 Aug 2024 15:07:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] BLADE：数据驱动科学的基准语言模型代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f05t0t/r_blade_benchmarking_language_model_agents_for/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f05t0t/r_blade_benchmarking_language_model_agents_for/</guid>
      <pubDate>Sat, 24 Aug 2024 13:52:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] 整理了一份 70 多篇研究论文的清单，供大家深入探讨</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f01yn2/p_curated_a_list_of_70_research_papers_for/</link>
      <description><![CDATA[        由    /u/Particular_Tap_4002   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f01yn2/p_curated_a_list_of_70_research_papers_for/</guid>
      <pubDate>Sat, 24 Aug 2024 10:15:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过估计数据分布比率进行离散扩散建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ezyunc/r_discrete_diffusion_modeling_by_estimating_the/</link>
      <description><![CDATA[文本扩散模型现在终于达到了 GPT2 的文本质量。 https://arxiv.org/abs/2310.16834（本文荣获 ICML2024 最佳论文奖！） 您认为扩散语言模型（扩散 LLM）会赶上自回归 LLM 并可能成为下一个 ChatGPT 吗？我们很快就会看到扩散 LLM 的缩放定律吗？与自回归 LLM 相比，这些模型具有一些关键优势，例如能够在任何地方接受提示 - 在输入的开始、中间、结束甚至拆分。此外，它们原则上可以一次生成多个标记。 这篇论文内容非常密集且数学繁重，所以我制作了一个动画解释视频，供任何感兴趣的人观看。 https://youtu.be/K_9wQ6LZNpI 我的看法：我认为这种方法在理论上可以扩展，但存在一个重大挑战：我们已经在 GPT/自回归变压器的硬件和软件优化方面投入了大量资金。考虑到沉没成本谬论，很难想象科技巨头会放弃他们目前的 LLM 来开始训练扩散 LLM，尤其是因为他们可能需要数年时间才能赶上 ChatGPT 和类似模型。就像 MAMBA 一样，我担心离散扩散也可能会输掉硬件/软件抽奖。    提交人    /u/AICoffeeBreak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ezyunc/r_discrete_diffusion_modeling_by_estimating_the/</guid>
      <pubDate>Sat, 24 Aug 2024 06:32:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] TurboEdit：即时基于文本的图像编辑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eztooi/r_turboedit_instant_textbased_image_editing/</link>
      <description><![CDATA[  由    /u/AhmedMostafa16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eztooi/r_turboedit_instant_textbased_image_editing/</guid>
      <pubDate>Sat, 24 Aug 2024 01:33:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 18 Aug 2024 02:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>