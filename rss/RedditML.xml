<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 14 May 2024 18:17:40 GMT</lastBuildDate>
    <item>
      <title>[D] 哪款 macbook 适合机器学习/贝叶斯统计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cryvds/d_which_macbook_for_machine_learningbayesian/</link>
      <description><![CDATA[我正在市场上购买一台新 MacBook（从 2018 intel mbp 升级）。当然，对于大型模型，我可以安装在远程服务器上。但我发现对于我的域来说，在我的笔记本电脑上运行 10 分钟到 10 小时之间的时间通常更简单，而我则去做其他事情。看来新的 MacBook Air 功能相当强大，我想知道我是否真的需要一台 MacBook Pro。我当然不是一个富有的人，但我愿意为我大部分醒着的生活中使用的东西付出代价。然而，如果 MacBook Pro 仅比 MacBook Air 稍有改进，那么额外的现金就不值得（根据我正在查看的规格，大约多花 1000 美元）。对于那些希望在笔记本电脑上安装中等尺寸型号的人，该社区会推荐什么规格？你认为 mbp 与 Air 相比会带来多少额外的效果？   由   提交/u/labbypatty  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cryvds/d_which_macbook_for_machine_learningbayesian/</guid>
      <pubDate>Tue, 14 May 2024 18:09:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] BERT 在 2024 年对于 EMNLP 提交仍然有意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/</link>
      <description><![CDATA[使用 BERT 进行主动学习（对于某些应用）仍然是提交论文的相关范式吗？或者这样的工作可能会因为“过时”而被拒绝？ 我的想法与使用 BERT 进行医学分类有关，我确信法学硕士可能会表现得更好。想知道是否值得投入时间大力推动以获得结果。   由   提交 /u/PK_thundr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/</guid>
      <pubDate>Tue, 14 May 2024 17:13:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 LangChain、Chainlit 和 Literal AI 构建可观察的 arXiv RAG 聊天机器人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/</link>
      <description><![CDATA[嘿，r/MachineLearning，我发表了一篇新文章，其中构建了一个可观察的语义研究论文应用程序。 这是一篇内容广泛的文章我在教程中详细介绍了：  开发 RAG 管道来处理和从 arXiv API 检索最相关的 PDF 文档。 使用以下命令开发 Chainlit 驱动的 Web 应用程序用于在线论文检索的 Copilot。 利用 Literal AI 的 LLM 可观察性功能增强应用程序。  您可以在此处阅读文章：https://medium.com/towards-data -science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8 教程代码：https://github.com/tahreemrasul/semantic_research_engine   由   提交/u/Significant-Result14  /u/Significant-Result14 reddit.com/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/</guid>
      <pubDate>Tue, 14 May 2024 16:32:25 GMT</pubDate>
    </item>
    <item>
      <title>来自隐藏状态的过去的键值 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cruqcr/past_key_values_from_hidden_states_d/</link>
      <description><![CDATA[我正在尝试使用attention_layers和hidden_​​state为特定层提取过去的键值对 def new_past_key_values （attention_layers，hidden_​​state，idx）：W_k =attention_layers[idx].k_proj W_v =attention_layers[idx].v_proj new_key = W_k（hidden_​​state）new_value = W_v（hidden_​​state）batch_size，seq_length，hidden_​​dim =hidden_​​state.size（）num_attention_heads =attention_layers [idx].num_heads head_dim = hide_dim // num_attention_heads new_key = new_key.view(batch_size, seq_length, num_attention_heads, head_dim) new_key = new_key.permute(0, 2, 1, 3) new_value = new_value.view(batch_size, seq_length, num_attention_heads , head_dim) new_value = new_value.permute(0, 2, 1, 3) return new_key, new_value  其中attention_layers和hidden_​​states定义如下：  attention_layers = [model.model.layers 中的层的layer.self_attn] idx=-1hidden_​​states =outputs.hidden_​​stateshidden_​​state =hidden_​​states[idx-1] new_key, new_value = new_past_key_values(attention_layers,hidden_​​state,idx) 但是这些 new_key, new_value 与我从特定层的outputs.past_key_values 获得的值不匹配。  为什么会这样？   由   提交/u/1azytux  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cruqcr/past_key_values_from_hidden_states_d/</guid>
      <pubDate>Tue, 14 May 2024 15:19:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 嵌入学习：计算理想边际惩罚的新思路</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crt8ax/r_embedding_learning_new_idea_for_calculating/</link>
      <description><![CDATA[大家好，我在硕士论文期间正在尝试面部识别，因此正在学习嵌入（使用三元组损失、ArcFace、AdaCos...等） .)。目的是创建一种高效（且不违反 GDPR）的面部解锁。 ArcFace 方法似乎仍然是 SOTA。像 AdaCos 这样的作品已经尝试通过消除余量并在训练期间动态调整比例来消除烦人的超参数，尽管实际上，在优化调整后，这似乎不如 ArcFace 效果好。 我随后提出了有一个不同的想法，即在训练期间调整边距而不是完全消除它，在我的测试中，它似乎工作得很好，比 AdaCos 更好，并且比 ArcFace 介于同样好和稍好之间。我很想听听是否有人可以验证我的发现，这是该方法的 pytorch 实现和解释： https://github.com/VBambi/AdaAcos-the-self- adjustment-implementaion-of-ArcFace   由   提交 /u/DetectiveVinc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crt8ax/r_embedding_learning_new_idea_for_calculating/</guid>
      <pubDate>Tue, 14 May 2024 14:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 仅具有 CS 背景的您如何更好地阅读 ML 论文中的证明？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/</link>
      <description><![CDATA[大家好，如标题所示，如何更好地阅读 ML 论文中的证明？我提到的 ML 论文是对抗性 ML 领域的论文，例如通过随机平滑认证的对抗鲁棒性。就上下文而言，我有微积分、线性代数的基本知识，但大多数时候在阅读证明时，有时我觉得一行字凭空出现，我无法推理他们为什么或如何做到这一点。也许因为我的背景是计算机科学，专注于软件，所以我缺乏严格的基于证明的数学知识。请帮忙！！   由   提交/u/little_vsgiant   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/</guid>
      <pubDate>Tue, 14 May 2024 12:33:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每个transformer层最后一个线性层的用处</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crojwd/d_the_usefulness_of_the_last_linear_layer_of_each/</link>
      <description><![CDATA[这是一个非常明显的问题。 我最近发现变压器的最后一个线性层有点浪费参数。  变压器模型是许多变压器层的堆栈。 这些层以 3 个 QKV 线性变换开始，以 FFN 网络结束，FFN 网络由两个线性层组成。最后一个需要(d_model * d_dim_feedforward)参数和乘法，其输出在下一层再次进行线性变换。 我们都知道，两个连续的线性变换可以用一个线性变换来表示，这就是原因为什么我们使用激活函数。 那么为什么我们没有使用超稀疏线性变换，也许可以通过将嵌入维度视为特定线性变换维度上的序列维度来进行卷积。   由   提交 /u/WetAndSnowy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crojwd/d_the_usefulness_of_the_last_linear_layer_of_each/</guid>
      <pubDate>Tue, 14 May 2024 10:09:19 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2024 年全球人工智能锦标赛数学数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crnika/p_a_dataset_for_the_global_artificial/</link>
      <description><![CDATA[      数据集和代码： https://github.com/protagolabs/odyssey-math AGI Odyssey&lt; /strong&gt;: https://www.agiodyssey.org 描述:&lt; /p&gt;  2024 年全球人工智能锦标赛 (GAIC) 数学呈现了 387 道精心设计的数学题，由来自大学和高中的专业数学题作者精心策划。该汇编包括 148 道高中数学竞赛题，随后是一系列 138 道高中数学题，最后是 101 道大学数学题。 GAIC Math 2024 出题者由数学教授组成来自亚利桑那州立大学、约翰霍普金斯大学、德雷克塞尔大学、新加坡国立大学、清华大学、华中师范大学等知名学府。这些教授受到AGI Odyssey的正式邀请，为比赛贡献他们的专业知识。问题制定者委员会与AGI Odyssey的使命保持一致，旨在推进通用人工智能（AGI）的创新研究，促进跨学科合作，并确保AGI的发展造福全人类。为维护竞赛的诚信和公平，出题委员会确保所有题目均为原创并保密。问题设置委员会的职责包括 GAIC Math 2024 的问题生成、审查、格式化、测试和修订。 包含 387 个问题和解决方案的新数据集，来自高中竞赛问题、高中数学问题和大学水平的数学问题。  https://preview.redd.it/2alzha4ewc0d1.jpg?width=1193&amp;format=pjpg&amp;auto=webp&amp;s=fa9735ddce1aea5f44b6a06d1fe2e4908526c80b  &lt;!-- SC_ON - -&gt;  由   提交/u/EternalBlueFriday  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crnika/p_a_dataset_for_the_global_artificial/</guid>
      <pubDate>Tue, 14 May 2024 08:56:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过从头开始实现 KAN 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crm54y/d_have_someone_tried_to_implement_kans_from/</link>
      <description><![CDATA[最近我听到了很多关于这种新架构（kolmogorov-Arnold Networks）的消息，它可能会给深度学习领域带来一场新的革命。  多年来，MLP 是唯一用于使用神经网络解决任何问题的架构，因此这种新架构的发布绝对是一个突破。虽然过去很多人都尝试过这样做，但不幸的是他们都没有成功。 如果你还不知道，你可以借助以下资源👇🏻 这是研究论文：https://arxiv.org/abs/2404.19756 还有这篇论文的讲解视频：https://youtu.be/-PFIkkwWdnM 如果你有尝试实现它或找到一些从头开始实现它的视频。考虑在评论中标记链接。    由   提交/u/cyb0rg14_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crm54y/d_have_someone_tried_to_implement_kans_from/</guid>
      <pubDate>Tue, 14 May 2024 07:14:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] O(NlogN) 计算步骤和 O(logN) 时间的完整因果自注意力层，而不是 O(N^2) 计算步骤和 O(1) 时间，有一个很大的警告，但对未来充满希望。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cri6h6/d_full_causal_selfattention_layer_in_onlogn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cri6h6/d_full_causal_selfattention_layer_in_onlogn/</guid>
      <pubDate>Tue, 14 May 2024 03:08:13 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]MICCAI 2024决定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crh21y/discussion_miccai_2024_decisions/</link>
      <description><![CDATA[大家好， 我认为这可能是讨论 MICCAI 2024 决策的好地方（早期接受、反驳、早期拒绝）。该电子邮件提到，今年有 2869 份提交（比去年增加 21%），其中约 54% 已被邀请进行反驳。 我收到了一份申请论文的反驳邀请，所有内容审稿人提到“缺乏技术新颖性”作为弱点，所以我最终得到了弱接受（4）、弱拒绝（3）和拒绝（2）。我相信我可以写一篇像样的反驳来反驳大多数审稿人的观点。但考虑到分数很低，有人认为这篇论文有希望被接受吗？反驳对于低分论文（第一轮之后）有什么影响吗？去年反驳阶段的论文最终获得接受的比例是多少？   由   提交 /u/possiblemonk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crh21y/discussion_miccai_2024_decisions/</guid>
      <pubDate>Tue, 14 May 2024 02:10:16 GMT</pubDate>
    </item>
    <item>
      <title>ICLR2024 上您最喜欢哪篇论文？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crgu6y/whats_your_favorite_paper_at_iclr2024_d/</link>
      <description><![CDATA[太多了，无法跟踪..   由   提交/u/Every-Act7282   /u/Every-Act7282 reddit.com/r/MachineLearning/comments/1crgu6y/whats_your_favorite_paper_at_iclr2024_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crgu6y/whats_your_favorite_paper_at_iclr2024_d/</guid>
      <pubDate>Tue, 14 May 2024 01:59:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neurips 2024 提交内容</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crahli/d_neurips_2024_submissions/</link>
      <description><![CDATA[我刚刚向 Neurips 2024 提交了一份摘要。我对自己提前两天的表现印象深刻，但我的论文 ID 已经超过 7000过去，我记得论文 ID 会随着 openreview 收到更多提交而增加。当然，今年情况并非如此！已经有 7000 份提交了？！   由   提交/u/fixed-point-learning  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crahli/d_neurips_2024_submissions/</guid>
      <pubDate>Mon, 13 May 2024 21:07:20 GMT</pubDate>
    </item>
    <item>
      <title>[N] GPT-4o</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/n_gpt4o/</link>
      <description><![CDATA[https://openai.com/ index/hello-gpt-4o/  这是 im-also-a-good-gpt2-chatbot（当前聊天机器人竞技场 sota） 多模式 更快且可在网络上免费使用    由   提交/u/_puhsu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/n_gpt4o/</guid>
      <pubDate>Mon, 13 May 2024 17:51:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>