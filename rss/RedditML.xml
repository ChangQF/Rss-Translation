<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 24 Mar 2024 12:22:42 GMT</lastBuildDate>
    <item>
      <title>[讨论]自然文本到知识图谱</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmj0f0/discussionnatural_text_to_knowledge_graph/</link>
      <description><![CDATA[对于关系提取和实体链接进行了一些微调，但是是否有任何项目可以从原始文本创建端到端知识图？    由   提交 /u/Raise_Fickle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmj0f0/discussionnatural_text_to_knowledge_graph/</guid>
      <pubDate>Sun, 24 Mar 2024 11:54:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] VAE 还值得吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmi8gq/d_is_vae_still_worth_it/</link>
      <description><![CDATA[VAE似乎在“扩散时代”逐渐消失。与 GAN 不同。 我知道扩散可以被视为 VAE 的特例，但问题是其他 VAE 变体相对于扩散（如果有的话）有哪些优势？    由   提交 /u/Realistic_Thanks3282   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmi8gq/d_is_vae_still_worth_it/</guid>
      <pubDate>Sun, 24 Mar 2024 11:07:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何与博士生完成实习职位？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmgovg/d_how_can_i_complete_with_phd_students_for_intern/</link>
      <description><![CDATA[现在我们在数据科学领域看到的实习机会大多是给硕士、博士生或拥有 1-2 年经验并已经转型的人进入数据科学。  现在作为一名大学生，我正在学习数据科学，在寻找实习机会时，我无法满足他们对硕士或博士学位的要求。 该怎么办学生在毕业期间需要做什么才能向公司表明他们至少有资格作为实习生工作？我们应该要求带回家的挑战还是制作一个特定领域的项目？  研究生如何与博士生、硕士生竞争机会？  ​   由   提交/u/Medium_Alternative50   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmgovg/d_how_can_i_complete_with_phd_students_for_intern/</guid>
      <pubDate>Sun, 24 Mar 2024 09:23:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在构建 vanialla 变压器时陷入持续损耗</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmfcap/d_stuck_with_constant_loss_while_building_the/</link>
      <description><![CDATA[大家好，我是 ML 世界的新手，但我正在尝试从头开始构建普通转换器来执行文本摘要任务，以便提高我对 Transformer 的理解。 这是 GitHub 存储库。这远不是变压器架构的高效且优化的实现。  所以我在损失计算方面遇到了问题，它并没有减少，只是一个常数（10.8），比一个时期多一点（1个时期在谷歌colab t4上需要大约70分钟；批次大小为 8，每批次最大序列长度为 512）。 我正在使用 cnn_dailymail 数据集来自拥抱脸，我正在使用 Facebook 的 bart-large-cnn tokenizer 来标记我的数据集。我只是使用 tokenizer 中的 token_ids 和注意力掩码。 这就是我的编码器输入 ID、解码器输入 ID 和目标 ID 的外观（出于视觉目的，输入 ID 在这里被批量解码回正常状态；批量大小 = 1为了简单起见） ``` encoder_input = [&#39;英国伦敦（路透社）——哈利·波特明星丹尼尔·雷德克利夫……与此同时，他已经做好了准备现在媒体更加密切地关注他&lt;/s&gt;&#39;] decoder_input = [&quot;&lt;s&gt;哈利·波特明星丹尼尔·雷德克里夫获得 2000 万英镑……&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; ;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&quot;] target = [&quot;哈利·波特明星丹尼尔·雷德克里夫获得2000万英镑财富……&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; &lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&quot;] ``` 我使用交叉熵，并减少=“none”，因为我正在乘以将填充掩码目标设置为损失矩阵，然后取平均值。损失函数的输入是形状为 torch.Size([batch, vocab_size, detector_seq_L]) 的模型输出 logits，损失函数的目标是形状为 torch.Size([batch, detector_seq_L]) 的 target_ids。模型输出在输入损失函数之前会进行排列。模型输出形状为 torch.Size([batch, detector_seq_L, vocab_size])。 我不确定问题到底出在哪里，因为我是新手，所以我无法弄清楚。因此，如果你们中的一个人能帮助我解决这个问题，我将不胜感激:) 编辑：这是我注意到的一件有趣的事情。如果我使用 6 个编码器和解码器层，则使用输出 logits 完成的预测大多数时候都会重复相同的标记，但如果层数仅为 1，则不会重复，尽管语言不连贯   由   提交 /u/archiesteviegordie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmfcap/d_stuck_with_constant_loss_while_building_the/</guid>
      <pubDate>Sun, 24 Mar 2024 07:48:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在网站上托管 RVCv2 或 XTTSv2 需要多少钱</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bme94q/d_how_much_would_it_cost_to_host_a_rvcv2_or/</link>
      <description><![CDATA[我以前从未真正托管过语音模型，所以我不知道，假设我每天会有 100-200 名访问者，我希望恢复生成的音频20 秒内   由   提交/u/Soumya1704  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bme94q/d_how_much_would_it_cost_to_host_a_rvcv2_or/</guid>
      <pubDate>Sun, 24 Mar 2024 06:35:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] Aleksa Godric 关于在 DeepMind 找到工作的帖子在今天仍然具有现实意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bme24n/d_is_aleksa_godrics_post_on_landing_a_job_at/</link>
      <description><![CDATA[我猜的标题差不多。 顺便说一句，这是 Aleksa 的帖子。我在一家初创公司工作，每天直接应用深度学习来解决具有挑战性的问题。我典型的一天几乎涉及微调、数据整理、生成报告、查看结果和整理高质量数据集来微调我们的模型。我为自己设定了一个崇高的目标，到 2025 年，要有足够的能力去 DeepMind/Anthropic 等公司面试（不是法学硕士或当前流行的话题，但可能是一般的研究工程师类型），重点是对到那时我将有大约 2 年的直接工作经验，以及 9 年多的学术工作经验（我拥有一个大学的学士学位）体面的州立学院和 ML/AI/机器人领域排名前 3 的大学的硕士学位，在那里我是体面的学生。没什么了不起的。根据我知名/知名的硕士导师的说法，有一篇论文作为第二作者发表，但“非常当之无愧”）和实习项目（实习、业余项目、许多分散但流行的开源项目）。我很想知道我应该如何继续我的准备？我觉得我需要重新调整我的基础知识，但想知道我应该如何去做，以确保我的努力集中且具有直接影响力。 我的致命弱点是我从未认真做过LeetCode，因为我主要申请/面试研究工程师之类的职位，面试官主要看论文、开源贡献和 PyTorch/TF 等中的一些最低限度的编码知识。 如果人们在这些公司可以参与进来，我会非常感激。老实说，光是看看这些公司员工的背景，我就感到害怕，因为看起来在那里工作的每个人都是 IMO、IOI、IPhO 奖章获得者，其中许多人在量化公司有过疯狂的经历，在这些公司里，面试具有神话/传奇的地位。  任何和所有建议将不胜感激。   由   提交 /u/hellofromthiside   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bme24n/d_is_aleksa_godrics_post_on_landing_a_job_at/</guid>
      <pubDate>Sun, 24 Mar 2024 06:22:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 简化扩散薛定谔桥</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm58lz/r_simplified_diffusion_schrödinger_bridge/</link>
      <description><![CDATA[       论文：https://arxiv.org/abs/2403.14623代码：https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge 在未配对的狗↔猫翻译上生成样本。左：猫到狗；右：狗到猫。我们观察到 DSB 可以在一定程度上保留姿态和纹理。 摘要：  本文介绍了扩散薛定谔桥的一种新颖的理论简化（ DSB），促进其与基于分数的生成模型（SGM）的统一，解决 DSB 在复杂数据生成方面的局限性，并实现更快的收敛和增强的性能。通过采用 SGM 作为 DSB 的初始解决方案，我们的方法充分利用了这两个框架的优势，确保了更高效的培训过程并提高了 SGM 的性能。我们还提出了一种重新参数化技术，尽管理论上是近似的，但实际上提高了网络的拟合能力。我们广泛的实验评估证实了简化 DSB 的有效性，展示了其显着改进。我们相信这项工作的贡献为高级生成建模铺平了道路。    由   提交 /u/TouchLive4686   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm58lz/r_simplified_diffusion_schrödinger_bridge/</guid>
      <pubDate>Sat, 23 Mar 2024 22:44:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM 是否在招聘信息中被过度炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm28ls/d_is_llm_too_hyped_up_in_job_postings/</link>
      <description><![CDATA[我正在就业市场寻找暑期实习机会，并在自主车辆领域从事 ML 研究/工程的全职工作。 “趋势”是指在过去的几年里，这个领域的变化如此之快，从超级炒作到大规模裁员，现在只剩下几家公司在谨慎行事。大多数项目范围内的裁员都是由于高管和经理过于乐观的决定而发生的，当时产品没有达到预期/利润。 自 2023 年以来，与所有其他人工智能用例一样，AV也从chatgpt革命中得到了推动。这些天我能找到的每一个招聘信息都在寻找具有法学硕士经验的人。这种程度让我不得不想到“LLM”。是另一个席卷高管的流行词，这种以法学硕士为重点的招聘将导致 1-2 年内又一系列裁员。 我来的法学硕士的用例到目前为止，大部分都是 chatgpt API，这进一步让我思考是否真的有那么多的开发正在进行，以制作有用的基于 LLM 的产品。通过此类职位被录用的人，你们在建设什么？您正在构建的产品的承诺是什么？您认为这个承诺实现的可能性如何？   由   提交/u/madgradstudent99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm28ls/d_is_llm_too_hyped_up_in_job_postings/</guid>
      <pubDate>Sat, 23 Mar 2024 20:36:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] DPO 仍然是经济实惠地微调模型的最佳方式吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm0tun/d_is_dpo_still_the_best_way_to_affordably/</link>
      <description><![CDATA[论文“Your Language Model is Secretly a Reward Model: Direct Preference Optimization (DPO)”证明 DPO 可以微调 LM 以符合人类偏好，并且优于现有方法”就像 RLHF 一样。 自从这篇论文于 2023 年 5 月发表以来，我想知道 DPO 是否仍然被认为是快速且经济实惠地微调 LLM 的最佳方法（特别是对于初创公司）。    由   提交/u/JT_NVG8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm0tun/d_is_dpo_still_the_best_way_to_affordably/</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 怎样才能成为一名优秀的机器学习工程师？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blzf0i/d_what_makes_a_good_machine_learning_engineer/</link>
      <description><![CDATA[您认为，怎样才是一名优秀的机器学习工程师？我所说的机器学习工程师指的是不进行研究，而是进行研究并将其实施到生产就绪代码中的人。他们应该具备哪些技能/知识？   由   提交/u/Raiz314  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blzf0i/d_what_makes_a_good_machine_learning_engineer/</guid>
      <pubDate>Sat, 23 Mar 2024 18:40:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] DeepRL 代理与 Ken 一起完成《街头霸王 III》</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bluore/p_deeprl_agent_completing_street_fighter_iii_with/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bluore/p_deeprl_agent_completing_street_fighter_iii_with/</guid>
      <pubDate>Sat, 23 Mar 2024 15:21:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我听了 Sam Altman 最近在 Lex Fridman 进行的 2 小时采访 - 以下是我们都应该知道的关键要点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</link>
      <description><![CDATA[Altman 本周在 Lex Fridman 播客上接受采访。这是一次相当长的采访，所以我想分享一下我听他讲话时记下的 10 个关键要点。你可以支持他，也可以反对他——但你不能否认他是即将到来的人工智能常态中最重要的核心人物之一！希望这对感兴趣的人来说是有洞察力的:) -- 1。创造有价值数据的人应该因使用这些数据而获得某种补偿💸 剩下的问题是实现它的经济模型。一个很好的类比是音乐从 CD 到 Napster 再到 Spotify 的转变。或者从电影到 YouTube 视频的转变。未来数据是否存在类似的经济模型？ ⏳ 40:16 -- 2. 比尔盖茨无法想象我们有一天会在计算机中需要千兆字节的内存💾 同样，我们可以&#39;今天，我们无法想象法学硕士如何或为何需要数十亿的上下文长度，但这仍然可能发生。 （对于上下文：具有 10 亿上下文长度的法学硕士意味着它可以处理和理解每个查询的约 200 万个文档页） ⏳ 51:13 --  “我想赋予 ChatGPT 保留记忆的能力”📝  想象一下，一个模型会逐渐了解您并随着时间的推移对您变得更加有用。这很可能是上面强调的十亿上下文长度 LLM 的一个用例。 ⏳ 55:33 --  计算将成为未来的货币。 💲  Sam 相信它将成为世界上最珍贵的商品。 ⏳ 1:09:55 --  核聚变将解决“能源问题”⚛  由于未来世界需要大量的计算，我们将需要大量的能源来为一切提供动力。 Sam 相信核聚变是解决这个问题的最佳方法。 ⏳ 1:11:29 --  Q- star 可能存在（但我们不会谈论这个）⭐  Lex 当然询问了 Q-star，但 Sam 并没有否认它的存在- 只是说“我们还没有准备好谈论这个”。 ⏳ 1:02:36 --  程序员不会过时👩🏻‍💻  但它可能会与现在的编程方式有所不同。不管怎样，萨姆认为没有人真正进行纯粹的编码——因为大多数程序员使用预先存在的软件包/技术/软件。利用 LLM 协助编码的方式与此类似。 ⏳ 1:29:50 --  超越 Google很无聊🔍  OpenAI 不想做一个更好的搜索引擎；以这种方式思考低估了他们在人工智能方面的工作。 ⏳ 1:17:37 --  “ ChatGPT 中不会有广告！” （最好）🚩  Sam 对广告有偏见 - 这就是为什么目前 ChatGPT 的商业模式是通过付费付费的。在某种程度上，我觉得这令人放心 - 因为当你引入广告时，你的“真正的客户”现在就变成了广告商，而不是实际的用户（现在变成了产品）。 ⏳ 1:20： 15 --  我们不再谈论 AGI（让我们称之为别的东西吧）🧠 &lt; /ol&gt; 人们对于 AGI 是什么有着不同的定义，因此 Sam 主张更多地谈论具体的功能，而不是把 AGI 作为一个通用术语。不过，根据他的定义，AGI 是一个无需人类干预即可推进科学发现的系统。 ⏳ 1:32:33    ;由   提交 /u/SwimIndependent6688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</guid>
      <pubDate>Sat, 23 Mar 2024 09:15:03 GMT</pubDate>
    </item>
    <item>
      <title>[N] Stability AI 创始人 Emad Mostaque 计划辞去首席执行官职务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</link>
      <description><![CDATA[https://www.forbes.com/sites/kenrickcai/2024/03/22/stability-ai-Founder-emad-mostaque- plan-to-resign-as-ceo-sources-say/ 官方公告：https: //stability.ai/news/stabilityai-announcement 无付费专区，福布斯：  尽管如此，莫斯塔克还是向公众展现了勇敢的一面。 “我们的目标是今年实现正现金流，”他二月份在 Reddit 上写道。据一位知情人士透露，即使在会议上，他也将计划中的辞职描述为一次成功使命的顶峰。  首先是 Inflection AI，现在是 Stability AI？你有什么想法？   由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</guid>
      <pubDate>Sat, 23 Mar 2024 03:49:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 进行机器学习面试后感到精疲力竭</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</link>
      <description><![CDATA[在过去的两个月里，我一直在面试机器学习工程师和相关职位，从大型科技公司到小型初创公司。采访的风格多种多样，而且似乎无处不在。即使面试了10家不同的公司，后来又面试了30多次，我都没有成功。我要么被他们迷住了，要么被拒绝了。 我接受过的一些面试是：  Leetcode 风格的编码问题。 从头开始实现 SVM 等机器学习算法或反向传播或卷积等算法的某些组件。 深入了解与编程语言相关的问题，例如有关 Python GIL 或 C++ 指针的问题。 与 OOP 相关的理论和实现问题。 典型的 SWE 风格系统设计面试，例如设计 Instagram 机器学习系统设计面试，例如设计推荐系统。 机器学习理论问题，例如什么是铰链损失或解释逻辑回归或何时可以使用 KL 散度。 深度学习理论问题，例如 SGD 和 Adam 之间的区别是什么、神经网络中的量化是什么、如何量化你能加快深度学习模型的推理速度吗？ 计算机视觉理论问题，例如 YOLO 和 FasterRCNN 之间有什么区别、什么损失函数可用于图像分割或解释对极几何。 自然语言处理理论问题，例如 Transformer 为何比 RNN 更好、BERT 中的双向性是什么，或者词干提取和词形还原之间有什么区别。 之前的工作、之前的研究论文、之前的项目相关问题. 带回家的作业也无处不在，从构建基于时间序列的模型到部署分类模型作为与公司面临的相关问题的端点。 与工具相关的问题，例如 Docker、Kubernetes、AWS 等。 行为轮面试 数学、统计和基于概率的面试，例如贝叶斯定理或伯努利分布或矩阵的等级是什么或区分某些东西。  我确信我还缺少其他风格的采访。我的记忆力不太好，所以也许我容易忘记我所学的东西，因此觉得这些采访很困难。我想知道人们是如何准备这些采访的。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</guid>
      <pubDate>Sat, 23 Mar 2024 00:26:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>