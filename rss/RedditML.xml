<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 24 Sep 2024 09:17:59 GMT</lastBuildDate>
    <item>
      <title>[R] 目前对您来说最令人兴奋的三大研究方向是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fo7ben/r_what_are_the_top_3_most_exciting_research/</link>
      <description><![CDATA[让我们分享吧！你对什么感到兴奋？    提交者    /u/Prestigious_Bed5080   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fo7ben/r_what_are_the_top_3_most_exciting_research/</guid>
      <pubDate>Tue, 24 Sep 2024 08:05:01 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 我可以将我的匿名 AAAI 主会议提交内容上传到 arxiv 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fo69nr/research_can_i_upload_my_anonymous_aaai_main/</link>
      <description><![CDATA[我已为 AAAI 2025 主轨道提交了一篇论文。第一阶段的拒绝通知将于 10 月 14 日发出，反驳通知将于 11 月 4-8 日发出。我想知道我可以将我的论文上传到 arxiv 吗？ 我查看了提交指南，其中规定：  在两种情况下，非匿名在线资料的存在不会被视为违反 AAAI-25 的盲审政策：提交的作品 (1) 可以作为未经审查的预印本出现在初步版本中（例如，在 arXiv.org、社交媒体、个人网站上）或出现在没有存档程序的任何研讨会上；或 (2) 在研究讲座中讨论，即使此类讲座的摘要或视频已在线提供。   据我了解，它说他们允许将已经上传到 arxiv 的论文提交给会议，但他们没有具体说明我们是否可以在等待 AAAI 评审时将其提交到 arxiv。    提交人    /u/morphinejunkie   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fo69nr/research_can_i_upload_my_anonymous_aaai_main/</guid>
      <pubDate>Tue, 24 Sep 2024 06:45:04 GMT</pubDate>
    </item>
    <item>
      <title>HUT：一种基于 Hadamard 更新变换的更高效计算微调方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fo0biv/hut_a_more_computation_efficient_finetuning/</link>
      <description><![CDATA[  由    /u/Thrumpwart  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fo0biv/hut_a_more_computation_efficient_finetuning/</guid>
      <pubDate>Tue, 24 Sep 2024 00:55:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 专注于机器学习且具有协作文化的公司？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnubzz/d_mlfocused_companies_with_collaborative_cultures/</link>
      <description><![CDATA[我是一名机器学习工程师，正在寻找具有可能适合我的协作、社交性质的文化的公司 — 以下是我所想象/寻求的几个例子：结对编程，将两名 ML 工程师放在一个项目上，一种不会因为与其他人合作而对你不利的晋升结构，毫不犹豫地在 Slack/聊天上 ping 或走过去问问题/与同事一起解决问题。 或者，如果你所在的团队与一家不完全符合要求的公司有这样的团队，我也很乐意听听你的看法！ 在我从事行业工作的大约 5 年里，我还没有经历过这种情况，我开始怀疑它是否存在于我们的领域。所以在我放弃希望之前，我想我会问问 Reddit :) p.s.希望这不太符合“职业问题”的条件，因为我的职业生涯已经很顺利了——更像是“如果你在某个地方工作或者听说过某个地方重视和奖励协作的工作方式，就大声喊出来”！如果这超出了本 subreddit 的权限范围，请道歉/随时删除。    提交人    /u/orshine   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnubzz/d_mlfocused_companies_with_collaborative_cultures/</guid>
      <pubDate>Mon, 23 Sep 2024 20:20:08 GMT</pubDate>
    </item>
    <item>
      <title>发现大词汇量的交叉熵损失的陷阱。[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnu24s/discovering_a_pitfall_in_crossentropy_loss_for/</link>
      <description><![CDATA[在这篇简短的文章中，我发现了在具有大词汇量的模型中使用交叉熵损失的一个重大问题，这可能会导致微调的 LLM 的性能下降。我提供了理论见解和实证结果来支持这些发现。如果您正在处理大词汇量，这是必读内容：揭示大词汇量交叉熵损失的陷阱 | 作者 Oswaldo Ludwig | 2024 年 8 月 | Medium    提交人    /u/Gold-Plum-1436   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnu24s/discovering_a_pitfall_in_crossentropy_loss_for/</guid>
      <pubDate>Mon, 23 Sep 2024 20:08:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我开发了一个可以用任何语言交谈的现场 AI 体育评论员</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnry1x/p_i_built_a_live_ai_sports_commentator_that_can/</link>
      <description><![CDATA[它检测视频中的关键帧并无需提示即可说话。在后端，我使用 Whisper 进行 STT、使用 Gemini Flash 进行视觉以及使用 ElevenLabs 进行语音。 演示：https://www.veed.io/view/b19f452b-9589-4270-b11f-e041f2065713?panel=share GitHub：https://github.com/outspeed-ai/outspeed/tree/main/examples/sports_commentator    提交人    /u/jaakeyb1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnry1x/p_i_built_a_live_ai_sports_commentator_that_can/</guid>
      <pubDate>Mon, 23 Sep 2024 18:41:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 是否批准单列宽度的图表？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnrrt2/d_does_iclr_approve_figures_with_singlecolumn/</link>
      <description><![CDATA[ICLR 2025 论文格式是单列（我认为一直都是这样），但我有一些解释性图表需要解释我的模型的不同方面。如果我必须将所有这些都放在整页宽度中，那将占用大量空间。但我在作者说明中找不到任何关于图表宽度的具体说明。我想知道，您是否遇到过任何以前的 ICLR 论文有单列宽度的图表？     提交人    /u/madgradstudent99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnrrt2/d_does_iclr_approve_figures_with_singlecolumn/</guid>
      <pubDate>Mon, 23 Sep 2024 18:34:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 又一个 transformer 可视化工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnqwa0/p_yet_another_transformer_visualizer/</link>
      <description><![CDATA[我为自己制作了这个，因为我学习了仅解码器转换器架构以及 Andrej Karpathy 的 YT 视频（特别是&quot;让我们从头开始构建 GPT，用代码拼写出来&quot;）。希望它至少对一些人有帮助，但如果您发现任何不正确、令人厌烦或不直观的地方，请随时指出。 另外，仅供参考，该设计不适合移动设备。建议使用宽屏。 链接：https://learn-good.github.io/llm_viz/1_decoder_only_transformer.html    提交人    /u/arnokha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnqwa0/p_yet_another_transformer_visualizer/</guid>
      <pubDate>Mon, 23 Sep 2024 17:59:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调还是建立代理集合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnks9a/d_fine_tune_or_build_an_agents_ensemble/</link>
      <description><![CDATA[我的任务是针对交易领域对新闻数据进行分类。我必须在给定的文本中对看涨、看跌或中性进行分类。 问题是我必须根据我的领域来处理这个问题，而且基本上没有可用于此任务的数据集。我已经尝试过 FinBert，但它不能很好地完成我的任务。 我的想法是使用 LLM 来为我进行分类。我尝试过 LangChain，它以一种实际返回我想要的内容的方式提示它。 我遇到的问题是我对 LLM 的分类不是很有信心。目前正在使用 ChatCohere，但我手动尝试过使用 Gemini、ChatGPT、Llama 3.1 8B 和 Claude AI 进行相同的提示。 我确实得到了不同的结果，这就是为什么我对我的问题感到非常担心。不仅在不同的 LLM 之间，而且当我使用 ChatCohere 重新运行相同的链时，似乎 LLM 会改变结果，尽管并不常见，但确实会发生。 我不知道这是不是一回事，但根据这篇论文，More Agents Is All You Need，当 LLM 互相投票反对时，显然你可以得到更好的结果？类似于集成方法？ 你怎么看？这是正确的方法吗？ 旁注：我知道对于我的特定目的，根据我的特定需求对模型进行微调是可行的方法。没有数据集迫使我退出游戏，直到我可以组成一个好的数据集，以后可以用来微调 BERT 或任何其他变压器。    提交人    /u/gl2101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnks9a/d_fine_tune_or_build_an_agents_ensemble/</guid>
      <pubDate>Mon, 23 Sep 2024 13:44:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] 帮助 Grad CAM 图像分类论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnkeaq/p_help_grad_cam_image_classification_paper/</link>
      <description><![CDATA[大家好，有谁有兴趣用 Python 等语言编写 Grad CAM 用于医学图像分类模型？希望成为论文机器视觉会议的合著者。我在德国数据挖掘小组工作，在一所主要大学的计算机科学系工作。    提交人    /u/sladebrigade   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnkeaq/p_help_grad_cam_image_classification_paper/</guid>
      <pubDate>Mon, 23 Sep 2024 13:26:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 适用于小型项目的易于使用的 NoSQL Prompt 数据库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fnggz7/d_easytouse_nosql_prompt_database_for_small/</link>
      <description><![CDATA[我一直在寻找用于 NoSQL 的 SQLite（原因有很多），最后我发现了 TinyDB（开源） https://mburaksayici.com/blog/2024/09/21/easy-to-use-nosql-prompt-database-for-small-projects.html    提交人    /u/mburaksayici   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fnggz7/d_easytouse_nosql_prompt_database_for_small/</guid>
      <pubDate>Mon, 23 Sep 2024 09:35:44 GMT</pubDate>
    </item>
    <item>
      <title>SQuAD 从零开始训练——问题与难点。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fndwls/squad_training_from_scratch_questions_and/</link>
      <description><![CDATA[大约一年前，我参加了深度学习课程，我能够使用非常小的数据集创建一个合理的英语到德语翻译器。我想扩展这个想法，使用 SQuAD 创建自回归编码器-解码器转换器，但我遇到了很多困难和一些问题。  与翻译工作不同，SQuAD 的问题和答案与上下文相比非常短。因此，对于答案和问题编码，我将 10 作为最大序列长度，将上下文设置为 200 最大序列长度。因此，我向编码器提供 [batch x 200]，向解码器提供 [batch x 10]。这种做法可以吗？从编码角度来看，这不会产生错误，但我想知道从 LLM 的角度来看，这是否可以。 PAD 索引问题：我正在使用 Pytorch 中的 CrossEntropyLoss()，其中 ignore index = PAD index。但是，很多时候标签会有很多 PAD，像这样：[鱼，PAD，PAD，PAD，PAD，PAD，PAD，PAD，PAD，PAD]。如果我的输出是 [我喜欢吃苹果鱼，PAD，PAD，PAD，PAD，PAD]，那么它会忽略最后 9 个输出，而我并不想发生这种情况 - 我希望损失能够惩罚不必要的输出，即“喜欢吃苹果鱼”。我试图对 PAD 索引施加较小的惩罚，但结果仍然不太好。如何处理具有大量 PAD 的超短输出？例如，标签 = [Notre Dame PAD PAD PAD PAD PAD PAD PAD PAD]？我知道你可以做一些类似开始和结束标记索引输出的事情，但我想用自回归来做...... 我读到你可以做一些学习热身，这样你就不会得到梯度消失。我每批次采集 15 个样本，并使用 LinearLR 调度程序在 100 个批次中将学习量从 0.0001 线性提升到 0.001。我也在使用 Adam 优化器。我得到了大约 4000 个批次中目标嵌入的梯度总和从 e-1 到 e-7 的递减量。而我的损失从一开始就没有减少。它只是输出一些不连贯的输出，如 [............] 或 [of of of of of of of of of of of]。  任何见解都会非常有帮助。 附注：我正在使用编码器-解码器变压器，它有 12 个头、768 个隐藏维度、每个 6 层、2048 个前向维度，在注意力头之后使用规范化 + 残差连接，dropout 为 0.1。我认为这应该会产生一些合理的词语，尽管不正确。也许对此的一些见解可能会很棒。我读过尝试尽可能地模仿 GPT 架构，只要我的 GPU 内存允许（只有 2GB）...    提交人    /u/DiabloSpear   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fndwls/squad_training_from_scratch_questions_and/</guid>
      <pubDate>Mon, 23 Sep 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>[N] 矩阵剖析系列的最后一篇论文：“矩阵剖析 XXXI：仅主题矩阵剖析：速度快几个数量级”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fn9s96/n_the_last_paper_in_the_matrix_profile_series/</link>
      <description><![CDATA[      亲爱的同事们 我很高兴地宣布矩阵剖析系列的最后一篇论文：“矩阵剖析 XXXI：仅基序矩阵剖析：速度快几个数量级”（或将被称为“MOMP”论文）[a]。 我认为不是每篇论文都需要宣布，但是…… 1）本文附带了一组新的基准数据集，这些数据集将被广泛使用。 2）对于寻找有趣问题来解决的学生和年轻教授，本文概述了几个值得研究的有趣挑战。 3）对于真正需要为其研究找到时间序列基序的研究人员，捆绑的代码将允许他们认为数据集要大一到两个数量级。 4）本文具有较小的“历史”意义，是三十篇被高引用率论文系列中的最后一篇。 为了让读者了解矩阵轮廓的影响力，请注意它刚刚成为 Matlab 语言的正式组成部分 [b]。 在论文 [a] 的扩展版本中，我花时间对矩阵轮廓系列进行了反思，并感谢帮助我实现时间序列数据挖掘愿景的数十个人。 本文通过引入矩阵轮廓的第一个下限，首次为加快八年来精确时间序列主题发现做出了贡献（基于硬件的想法除外）。 [a] 矩阵轮廓 XXXI：仅主题的矩阵轮廓：速度快几个数量级。 https://www.dropbox.com/scl/fi/mt8vp7mdirng04v6llx6y/MOMP_DeskTop.pdf?rlkey=gt6u0egagurkmmqh2ga2ccz85&amp;dl=0 [b] https://www.mathworks.com/help/predmaint/ref/matrixprofile.html https://preview.redd.it/it16c6h8vgqd1.jpg?width=2602&amp;format=pjpg&amp;auto=webp&amp;s=578a65723507c597ee4140d2eed17ba5938f326d    提交人    /u/eamonnkeogh   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fn9s96/n_the_last_paper_in_the_matrix_profile_series/</guid>
      <pubDate>Mon, 23 Sep 2024 02:04:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 22 Sep 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>