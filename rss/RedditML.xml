<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 28 Apr 2024 06:17:54 GMT</lastBuildDate>
    <item>
      <title>[P] 自然语言到 MongoDB 查询的转换。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceyzkr/p_natural_language_to_mongodb_query_conversion/</link>
      <description><![CDATA[      我很高兴这次发布我的副项目“nl2query”的下一个迭代一个微调的 Phi2 模型，用于将自然语言输入转换为相应的 Mongodb 查询。以前的 CodeT5+ 模型不够强大，无法处理嵌套字段（如数组和对象），但 Phi2 可以。在 GitHub 上探索代码：https://github.com/Chirayu-Tripathi/nl2query。 https://preview.redd.it/0y8o9w1br5xc1.png ?width=1800&amp;format=png&amp;auto=webp&amp;s=295e045bedf504ec4b0e4b04d590635abf45119b   由   提交/u/WorryWhole7805   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceyzkr/p_natural_language_to_mongodb_query_conversion/</guid>
      <pubDate>Sun, 28 Apr 2024 05:42:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] PointNet 输入转换块中单位矩阵的作用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cexbr6/d_role_of_the_identity_matrix_in_pointnets_input/</link>
      <description><![CDATA[你好， 我目前正在探索 PointNet 的代码，特别是输入转换块。我知道这个块用于学习应用于输入点云的变换矩阵，但我对单位矩阵在这种情况下以及通常在深度学习中的作用有点困惑 Here&#39;s我所指的代码片段：  def forward(self, x): batchsize = x.size()[0] x = F.relu(self.bn1( self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = torch.max( x, 2, keepdim=True)[0] x = x.view(-1, 1024) x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5( self.fc2(x))) x = self.fc3(x) iden = 变量(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype (np.float32))).view(1, 9).repeat(batchsize, 1) if x.is_cuda: iden = iden.cuda() x = x + iden x = x.view(-1, 3, 3 ) 返回 x    由   提交 /u/Same_Half3758   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cexbr6/d_role_of_the_identity_matrix_in_pointnets_input/</guid>
      <pubDate>Sun, 28 Apr 2024 04:00:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 快速提问：您如何实施研究论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cew2b5/d_quick_question_how_do_you_implement_research/</link>
      <description><![CDATA[我正在学习深度学习，现在我想通过实施一篇研究论文来测试我的技能。 有一篇研究论文称为深度学习的语言进化，它让我着迷，但我很困惑从哪里开始以及如何开始。 如果有人，我需要指导。   由   提交/u/No-Signal-313  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cew2b5/d_quick_question_how_do_you_implement_research/</guid>
      <pubDate>Sun, 28 Apr 2024 02:48:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] NLLB-200 蒸馏器 350M 一台</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceuj4t/p_nllb200_distill_350m_for_enko/</link>
      <description><![CDATA[您好r/MachineLearning， 我很高兴分享一个最初打算在我的毕业产品（Capstone）中使用的项目 我制作了 NLLB-200 Distill 350M 模型来将英语翻译成韩语 很好用。小而快。所以它可以用 CPU 运行！ GPU 服务器相当昂贵，所以我为那些买不起服务器的大学生（比如我）制作了它。 更多细节是在我的页面 如果你懂韩语，请给我很多反馈 谢谢！！ https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko   由   提交/u/SaeChan5  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceuj4t/p_nllb200_distill_350m_for_enko/</guid>
      <pubDate>Sun, 28 Apr 2024 01:26:04 GMT</pubDate>
    </item>
    <item>
      <title>[P]我做了一个简单的python脚本来让两个LLM互相判断</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cerjoj/p_i_made_a_simple_python_script_to_make_two_llms/</link>
      <description><![CDATA[我制作了一个视频，介绍何时运行此程序而不是依赖排行榜可能有用，回顾结果、限制和要点，并给出代码的简要概述。我查看了 GPT4 与 Claude 3 Opus 的结果。 YT：https://youtu.be/-bcbcFYhqes Github：https://github.com/learn-good/llm-vs-llm&lt; /a&gt; 如果您不想观看视频或运行代码，我在此处提供了结果摘要和我的要点：  总体而言，GPT4 似乎比 Claude 3 Opus 稍好，但 Opus 可能在某些任务上更好（在我的实验中，它在设计教学大纲方面更好）。 这些法学硕士似乎没有注意到或更喜欢他们自己的输出已生成。 结果对提示结构中的结构（非信息）变化有些敏感。也就是说，改变提示中显示的顺序信息可能会改变结果。 当以不同的顺序提供相同的候选答案时，GPT4 和 Opus 在这两种情况下选择相同的“最喜欢的答案”时并不一致（略好于 50%）。其他比较（Haiku 与 GPT4、Haiku 与 GPT3.5、Sonnet 与 GPT3.5）的一致性要高得多（~90%）。我推测这可能是因为输出质量非常相似，或者因为可能很难区分两个非常高质量的答案，即使它们之间存在一些潜在的质量差异。 这是一个小型研究，结果显然不是确定的。该实验针对不同任务类别中的约 90 个任务进行了运行。任务描述很短，而且是 0 次测试（没有给出示例）。 Chatbot Arena 排行榜和 LLM 基准测试非常适合了解这些模式的一般功能，但如果您有一个利基市场任务或问题领域，可能值得进行一些小实验来看看哪种模型表现最好。如果您想做的话，我会提供运行您自己的实验的代码和说明。  如果您有任何问题，请随时告诉我！ &lt; !-- SC_ON --&gt;  由   提交/u/arnokha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cerjoj/p_i_made_a_simple_python_script_to_make_two_llms/</guid>
      <pubDate>Sat, 27 Apr 2024 23:01:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 RAG 的真实讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</link>
      <description><![CDATA[说实话。我知道我们都必须与这些经理/董事/CXO 打交道，他们提出了与公司数据和文档交谈的惊人想法。 但是……有人真正做了一些真正有用的事情吗？如果是这样，它的有用性是如何衡量的？ 我有一种感觉，我们被一些非常复杂的废话所愚弄，因为法学硕士总是可以产生在某种程度上听起来合理的东西。但它有用吗？   由   提交/u/fusetron  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</guid>
      <pubDate>Sat, 27 Apr 2024 18:00:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] BLEU 分数没有提高</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceidsx/p_bleu_scores_not_improving/</link>
      <description><![CDATA[我正在使用 CNN+LSTM 开发这个图像字幕项目。目前我使用 googleNet 作为 CNN，使用 lstm、bilstm 作为 RNN。对于嵌入，我使用 word2vec 算法。使用的数据集是 flickr8k。正如您从蓝色分数中看到的那样，它们非常接近，即没有明显的改进。这两种方法中的参数值相同。值如下： word_embedding_dimension：128 lstm_hidden_​​state_dimension：32 lstm_number_of_layers：2 dropout_proportion：0.5 我正在训练我的模型500 epoch，批量大小为 16，学习率为 0.0003。 这是我的两种方法的架构： class BiLSTM_fixed_embedding(torch.nn.Module): def __init__(self, embedding_dim, lstm_hidden_​​dim, num_lstm_layers, image_latent_dim, vocab_size, dropoutProportion=0.52): super(BiLSTM_fixed_embedding, self).__init__() self.embedding_dim = embedding_dim self.lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_​​dim, num _lstm_layers，batch_first=True， bidirection=True) # 双向 LSTM self.dropout = torch.nn.Dropout(dropoutProportion) self.linear = torch.nn.Linear(lstm_hidden_​​dim * 2 + image_latent_dim, vocab_size) # 调整双向 LSTM 的线性层输入大小 def front(self , image_latentTsr,EmbeddedChoppedDescriptionTsr):aggregate_h, (ht, ct) = self.lstm(embeddedChoppedDescriptionTsr) # 连接两个方向的最终隐藏状态 concat_latent = torch.cat((torch.nn.function.normalize(ht[-2,: ,:]), torch.nn.function.normalize(ht[-1,:,:]), torch.nn.function.normalize(image_latentTsr)), dim=1) # 使用 ht[-2,:,: ] 和 ht[-1,:,:] 对于双向 LSTM outputTsr = self.linear(self.dropout(concat_latent)) 返回 outputTsr 类 LSTM_fixed_embedding(torch.nn.Module): def __init__(self, embedding_dim, lstm_hidden_​​dim, num_lstm_layers, image_latent_dim, vocab_size, dropoutProportion=0.5): super(LSTM_fixed_embedding, self).__init__() self.embedding_dim = embedding_dim self.lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_​​dim, num_lstm_layers, batch_first=True) self.dropout = torch. nn.Dropout(dropoutProportion) self.linear = torch.nn.Linear(lstm_hidden_​​dim + image_latent_dim, vocab_size) defforward(self, image_latentTsr,embeddedChoppedDescriptionTsr):aggregated_h, (ht, ct) = self.lstm(embeddedChoppedDescriptionTsr) concat_latent = torch. cat( (torch.nn.function.normalize(ht[-1]), torch.nn.function.normalize(image_latentTsr)), dim=1) 输出Tsr = self.线性(self.dropout(concat_latent)) 返回outputTsr BLEU lstm+googleNet 平均 BLEU-1 分数：0.5125574933996119 平均 BLEU-2 分数：0.3231883563312253 平均 BLEU-3 分数：0.13785811579538154 平均 BLEU-4 分数：0.04515516928289819 BLEU 分数bilstm+googleNet 平均 BLEU-1 分数：0.5202970280329978 平均 BLEU-2分数：0.3236744954911404 BLEU-3 平均分数：0.1341793193712665 BLEU-4 平均分数：0.050970741654506005  现在我有两个问题： i。分数没有提高的原因是什么？如何提高。 ii.我还计划使用 resNet50，它有 2048 个潜在表示，即 googleNet 的两倍。这种方法需要对参数进行哪些更改。 非常感谢大家帮助我。   由   提交 /u/Sorry_Drawer9736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceidsx/p_bleu_scores_not_improving/</guid>
      <pubDate>Sat, 27 Apr 2024 16:21:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 但是经过训练的卷积神经网络实际上学到了什么？可视化！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cefhwf/d_but_what_does_a_trained_convolution_neural/</link>
      <description><![CDATA[   分享我的 YT 频道中的视频，解释卷积并可视化内核的学习方式……享受吧！    由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cefhwf/d_but_what_does_a_trained_convolution_neural/</guid>
      <pubDate>Sat, 27 Apr 2024 14:14:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] 小型 GPT-2 大小的 LLM 的分类微调实验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cedfub/p_classification_finetuning_experiments_on_small/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cedfub/p_classification_finetuning_experiments_on_small/</guid>
      <pubDate>Sat, 27 Apr 2024 12:31:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于 Llama-3 的 OpenBioLLM-70B 和 8B：在医疗领域优于 GPT-4、Gemini、Meditron-70B、Med-PaLM-1 和 Med-PaLM-2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/</link>
      <description><![CDATA[      开源再次来袭，我们很高兴地宣布发布OpenBioLLM-Llama3-70B &amp; 8B。这些模型在生物医学领域的表现超越了 Openai 的 GPT-4、Google 的 Gemini、Meditron-70B、Google 的 Med-PaLM-1 和 Med-PaLM-2 等行业巨头，为其规模的模型树立了新的最高水平。 迄今为止最强大的公开可用的医学领域 LLM！ 🩺💊🧬 https://preview.redd.it/2h4ebhftf0xc1.png?width=2514&amp;format=png&amp;auto=webp&amp;s=bbc3a583d45fb37b87a6fbbabe2d9e0f23c75d8b 🔥 OpenBioLLM-70B 提供 SOTA 性能，而 OpenBioLLM-8B 模型甚至超越 GPT-3.5 和 Meditron-70B！ 这些模型经过了严格的两阶段微调过程，使用 LLama-3 70B 和 8B 模型作为基础，并利用直接偏好优化 (DPO) 来获得最佳性能。 🧠 https://preview.redd.it/w41pv7mwf0xc1.png?width=5760&amp;format=png&amp;auto=webp&amp;s=f3143919ef8472961f329bb8eb98937d8f8e41e0 结果可在 Open Medical-LLM 排行榜上查看：https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard 在约 4 个月的时间里，我们精心策划了一个多样化的自定义数据集，并与医学专家合作以确保最高质量。该数据集涵盖 3000 个医疗保健主题和 10 多个医学科目。📚 OpenBioLLM-70B 在 9 个不同的生物医学数据集中表现出色，尽管与 GPT-4 和 Med-PaLM 相比参数数量较少，但仍取得了令人印象深刻的 86.06% 的平均分数。 📈 https://preview.redd.it/5ff2k9szf0xc1.png?width=5040&amp;format=png&amp;auto=webp&amp;s=15dc4aa948f2608717f68ddf2cb27a6a2de03496 您今天可以直接从 Huggingface 下载模型。  70B : https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B 8B：https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B  此发布仅仅是个开始！在接下来的几个月里，我们将推出  扩大的医疗领域覆盖范围， 更长的上下文窗口， 更好的基准，以及 多模式功能。  更多详情可在此处找到：https://twitter.com/aadityaura/status/1783662626901528803在接下来的几个月里，多模式将用于各种医疗和法律基准。 希望它对你的研究有用🔬祝大家周末愉快！😊    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/</guid>
      <pubDate>Sat, 27 Apr 2024 11:51:42 GMT</pubDate>
    </item>
    <item>
      <title>如何说服我的上级进行数据预处理？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceckws/how_do_i_convince_my_superior_to_do_data/</link>
      <description><![CDATA[如何说服上级进行数据预处理？ 您好，我做了一年的 AI 工程师在我现在的公司（获得了数据科学专业的计算机硕士学位）。我们希望构建专门用于特定语言的闲聊（主要是对话式聊天）的聊天机器人。  问题是我不同意上级的做事方式。它几乎总是在进行即时工程。我的意思是我们有大量的数据（我想说的是无限的实时会话聊天会话，其中包含兴趣、外表等信息……这是所有数据科学家建立一个漂亮模型的梦想）。我不同意他的方法的原因是，通过及时的工程设计，我们并不总是能获得持续的良好结果。此外，对于特定领域（例如色情聊天），由于模型的审查，您无法提示工程。当模型未针对特定领域的标记/单词进行训练时，可能会出现幻觉和其他问题。最后，一切都与统计有关，不是吗？该模型从使用的数据中学习。如果推理过程中有一个 token 没有包含在训练数据中，那么它会以概率进行猜测，以预测最有可能的下一个 token。 我不明白为什么我们不这样做利用这些数据来清理它，为我们的目的/领域创建一个超级好的数据集，并对法学硕士进行微调。我问过他很多次为什么我们不这样做，我的上级回答说：“我们过去这样做过，成本太大，效果不好”。于是我问他，是谁干的？他告诉我，我的同事做到了（医学教育背景，空闲时间对人工智能感兴趣，但他对数据处理或数据科学基础知识一无所知）。  所以他们最后一次尝试是在3年前（他们是用deepspeed做的，没有使用Lora方法，所以我的上级告诉我成本相当高，但结果不好（他们在云中进行了微调） 200h），所以这是一个完整的参数微调） 说实话，我不怪我的同事。他用自己的知识尽力了。但我确实责怪我愚蠢的上司，我们没有取得多大成功来为我们的目的开发一个像样的模型。  所以，在我开始为公司工作半年后，我终于可以说服我的上司了（因为我在空闲时间做了一个微调，只是为了好玩，并向他们展示了我的结果）。所以他同意，我们可以对洛拉进行微调，但是..但是..没有数据处理，就拿原始的宝贝来说！！ 说真的，那家伙完全迷失了，顺便说一句，他是我们的产品经理并且对数据科学一无所知。他在没有数据处理的情况下再次犯了同样的错误，因为“我们没有这方面的资源”，我什至无法说服他。 所以最后，聊天机器人变得比仅仅做更好一些及时的工程，但对我来说它仍然是垃圾。我只想要一个真实且标准的工作流程，包括数据预处理、训练、评估。就这样。最重要的是：数据预处理 那么你们觉得怎么样？我是猴子吗？我应该尽快离开公司吗？我需要在那里至少再呆一年。   由   提交/u/bobotomoon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceckws/how_do_i_convince_my_superior_to_do_data/</guid>
      <pubDate>Sat, 27 Apr 2024 11:43:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 标记化的数学方面</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cea5qn/d_mathematical_aspects_of_tokenization/</link>
      <description><![CDATA[我最近制作了一个视频，介绍了我们最近在标记化的数学方面的工作，特别是： - 将标记化形式化为压缩 - 字节对编码的界限最优性 - 标记化熵和性能之间的联系 我将非常感谢任何反馈，因为我仍在学习如何制作教育视频。谢谢！ https://youtu.be/yeEZpf4BlDA   由   提交/u/zouharvi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cea5qn/d_mathematical_aspects_of_tokenization/</guid>
      <pubDate>Sat, 27 Apr 2024 09:06:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型模型的参数高效微调：综合调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ce8wow/r_parameterefficient_finetuning_for_large_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.14608 摘要：  大型模型代表了多个应用领域的突破性进步，实现了显着的各项任务取得的成绩。然而，其前所未有的规模伴随着巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在针对特定下游任务进行定制时，巨大的规模和计算需求带来了相当大的挑战，特别是在受计算能力限制的硬件平台上。 参数高效微调 (PEFT ）通过在各种下游任务中有效地调整大型模型来提供实用的解决方案。具体来说，PEFT 是指调整预训练大型模型的参数以使其适应特定任务，同时最大限度地减少引入的附加参数或所需的计算资源的数量的过程。在处理具有高参数数的大型语言模型时，这种方法尤其重要，因为从头开始微调这些模型可能会耗费大量计算资源且占用大量资源，从而给支持系统平台设计带来相当大的挑战。 ，我们对各种 PEFT 算法进行了全面的研究，检查了它们的性能和计算开销。此外，我们概述了使用不同 PEFT 算法开发的应用程序，并讨论了用于降低 PEFT 计算成本的常用技术。除了算法角度之外，我们还概述了各种现实世界的系统设计，以研究与不同 PEFT 算法相关的实施成本。这项调查对于旨在了解 PEFT 算法及其系统实现的研究人员来说是不可或缺的资源，提供了对最新进展和实际应用的详细见解。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ce8wow/r_parameterefficient_finetuning_for_large_models/</guid>
      <pubDate>Sat, 27 Apr 2024 07:42:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 谈论模型的概率有意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ce1zfs/d_does_it_make_sense_to_talk_about_the/</link>
      <description><![CDATA[https://lunaverus.com/programLikelihoods  有一种巧妙的方法将无监督学习构建为似然最大化，但不是以通常的方式，即使用模型计算数据的似然性并忽略模型本身的似然性。相反，这是模型和数据的组合可能性...  谈论 ML 模型的概率有意义吗？   由   提交 /u/bouncyprojector   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ce1zfs/d_does_it_make_sense_to_talk_about_the/</guid>
      <pubDate>Sat, 27 Apr 2024 01:06:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>