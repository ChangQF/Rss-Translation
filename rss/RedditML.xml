<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 23 Mar 2024 09:12:53 GMT</lastBuildDate>
    <item>
      <title>[R]学习最大公约数：解释变压器预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blk4g1/r_learning_the_greatest_common_divisor_explaining/</link>
      <description><![CDATA[ 由   提交/u/Chaoses_Ib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blk4g1/r_learning_the_greatest_common_divisor_explaining/</guid>
      <pubDate>Sat, 23 Mar 2024 04:55:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 转向 SF 和 SV 真的是 ML 及相关领域前沿工作的必要条件吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blk128/d_is_a_move_to_sf_and_sv_truly_a_necessity_for/</link>
      <description><![CDATA[人们一直在说，“你必须来到山谷！”这是昂贵且繁重的工作，但这是保持领先地位的最可靠方法。”  真的是这样吗？还会继续这样吗？我从初创公司/应用研究/产品工程的多个角度想知道这个问题。  如果是这样，您认为为什么会出现这种情况？我们如何才能弥补在科技圣地之外工作的缺点？ &lt;!-- SC_ON - -&gt;  由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blk128/d_is_a_move_to_sf_and_sv_truly_a_necessity_for/</guid>
      <pubDate>Sat, 23 Mar 2024 04:50:08 GMT</pubDate>
    </item>
    <item>
      <title>[P]学习RNN的最佳地点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bljynd/pbest_place_to_learn_rnn/</link>
      <description><![CDATA[我已经学完了 Andrew Ng 斯坦福的 3 门课程，我需要为某个项目学习 RNN 那么我可以在哪里学习它来自？   由   提交 /u/Narrow_Solution7861   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bljynd/pbest_place_to_learn_rnn/</guid>
      <pubDate>Sat, 23 Mar 2024 04:46:09 GMT</pubDate>
    </item>
    <item>
      <title>[N] Stability AI 创始人 Emad Mostaque 计划辞去首席执行官职务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</link>
      <description><![CDATA[https://www.forbes.com/sites/kenrickcai/2024/03/22/stability-ai-Founder-emad-mostaque- plan-to-resign-as-ceo-sources-say/ 官方公告：https: //stability.ai/news/stabilityai-announcement 无付费专区，福布斯：  尽管如此，莫斯塔克还是向公众展现了勇敢的一面。 “我们的目标是今年实现正现金流，”他二月份在 Reddit 上写道。据一位知情人士透露，即使在会议上，他也将计划中的辞职描述为一次成功使命的顶峰。  首先是 Inflection AI，现在是 Stability AI？你有什么想法？   由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</guid>
      <pubDate>Sat, 23 Mar 2024 03:49:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 MLLM 愿景的讨论？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blirbx/d_discussion_about_mllm_vision/</link>
      <description><![CDATA[您认为为什么像 gpt-v 这样的 MLLM 缺乏描述图像中对象之间的空间关系的能力，就像它可以看到和描述对象一样，但无法关联到物体的确切位置？有什么办法可以克服这个问题吗？   由   提交 /u/Moodrammer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blirbx/d_discussion_about_mllm_vision/</guid>
      <pubDate>Sat, 23 Mar 2024 03:39:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 开放式居住</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blimk2/d_openai_residency_oa/</link>
      <description><![CDATA[刚刚获得 OpenAI 人工智能驻场黑客等级。总共 2 小时 15 分，有 3 道简答数学题和 1 道编码题。 它说数学涉及线性代数、概率和统计。我期待中等硬度的 LC 等效编码。不太确定如何准备。我的朋友建议我看看量化准备书籍/指南，例如 Jane Street 博客，因为这似乎是一个数学繁重的面试。 以前做过住院医师 OA 的人吗？同时也希望听取曾在 OpenAI 或其他 AI 实习机构（例如 Uber/Meta 等）面试过的人的意见。   由   提交 /u/Careless-Cow-5683   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blimk2/d_openai_residency_oa/</guid>
      <pubDate>Sat, 23 Mar 2024 03:32:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] 为文本语料库提供主题相似度排序的最佳方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blhdq0/p_best_way_to_give_corpuses_of_text_a_topical/</link>
      <description><![CDATA[我有很多文本段落。给定一个段落，我想仅根据主题返回 N 个最相似的段落。这些段落也是矢量化的，但是当我在矢量数据库中进行相似性搜索时，我感觉结果并不理想。我纯粹希望它根据“锻炼”、“跑步”、“编码”、“悲伤”等实质性主题进行比较。理想情况下，随着数据集的增长，粒度会越来越细。 我的一个想法是让法学硕士提取大约 10 个最实质性的主题，然后通过词网之类的东西计算相似度。也许这种方法更能实现我的目标。 也许我没有给予余弦相似度向量搜索足够的信任，但我想了解您对此事的一些想法！ &lt; /div&gt;  由   提交/u/Calm_Ad_343   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blhdq0/p_best_way_to_give_corpuses_of_text_a_topical/</guid>
      <pubDate>Sat, 23 Mar 2024 02:29:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：在 WSL 与使用 RTX 2060 笔记本电脑的 Linux 分区上开发机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blh23k/d_seeking_advice_developing_machine_learning_on/</link>
      <description><![CDATA[各位开发人员大家好！ 👋 我目前正在深入研究机器学习，并在配备 Intel 处理器和 RTX 2060 GPU 的 Razer Blade 2019 笔记本电脑上设置我的开发环境。我在两种方法之间左右为难：利用 Windows Subsystem for Linux (WSL) 或使用 Ubuntu 创建专用 Linux 分区。 我很想听听您的想法和经验，了解哪种设置可能更高效、更高效。无缝进行 ML 开发。您是否发现 WSL 足够了，还是选择了 Linux 分区以获得更好的性能和兼容性？您认为每种方法的优缺点是什么？ 此外，如果您使用过类似的硬件，那么任何有关优化 GPU 利用率和工作流程效率的技巧都将非常宝贵！ 非常感谢您的见解！   由   提交/u/Ari1996   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blh23k/d_seeking_advice_developing_machine_learning_on/</guid>
      <pubDate>Sat, 23 Mar 2024 02:13:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 进行机器学习面试后感到精疲力尽</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</link>
      <description><![CDATA[在过去的两个月里，我一直在面试机器学习工程师和相关职位，从大型科技公司到小型初创公司。采访的风格多种多样，而且似乎无处不在。即使面试了10家不同的公司，后来又面试了30多次，我都没有成功。我要么被他们迷住了，要么被拒绝了。 我接受过的一些面试是：  Leetcode 风格的编码问题。 从头开始实现 SVM 等机器学习算法或反向传播或卷积等算法的某些组件。 深入了解与编程语言相关的问题，例如有关 Python GIL 或 C++ 指针的问题。 与 OOP 相关的理论和实现问题。 典型的 SWE 风格系统设计面试，例如设计 Instagram 机器学习系统设计面试，例如设计推荐系统。 机器学习理论问题，例如什么是铰链损失或解释逻辑回归或何时可以使用 KL 散度。 深度学习理论问题，例如 SGD 和 Adam 之间的区别是什么、神经网络中的量化是什么、如何量化你能加快深度学习模型的推理速度吗？ 计算机视觉理论问题，例如 YOLO 和 FasterRCNN 之间有什么区别、什么损失函数可用于图像分割或解释对极几何。 自然语言处理理论问题，例如 Transformer 为何比 RNN 更好、BERT 中的双向性是什么，或者词干提取和词形还原之间有什么区别。 之前的工作、之前的研究论文、之前的项目相关问题. 带回家的作业也无处不在，从构建基于时间序列的模型到部署分类模型作为与公司面临的相关问题的端点。 与工具相关的问题，例如 Docker、Kubernetes、AWS 等。 行为轮面试 数学、统计和基于概率的面试，例如贝叶斯定理或伯努利分布或矩阵的等级是什么或区分某些东西。  我确信我还缺少其他风格的采访。我的记忆力不太好，所以也许我容易忘记我所学的东西，因此觉得这些采访很困难。我想知道人们是如何准备这些采访的。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</guid>
      <pubDate>Sat, 23 Mar 2024 00:26:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 哪些令人尴尬的并行工作负载需要 GPU？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</link>
      <description><![CDATA[大家好， 我正在研究垂直 GPU 集群，并寻找一些可以运行的用例我正在构建的集群。我从法学硕士批量推理开始，但很想听听你的想法。唯一的要求是他们不使用机器间通信。    由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</guid>
      <pubDate>Fri, 22 Mar 2024 22:06:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] PuzzleVQA：用抽象视觉模式诊断语言模型的多模态推理挑战</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bl12g2/d_puzzlevqa_diagnosing_multimodal_reasoning/</link>
      <description><![CDATA[数据集：https ://github.com/declare-lab/LLM-PuzzleTest/tree/master/PuzzleVQA 论文：https://arxiv.org/abs/2403.13315 大型多模态模型通过集成多模态理解能力扩展了大型语言模型的令人印象深刻的功能。然而，目前尚不清楚它们如何模仿人类的一般智力和推理能力。由于识别模式和抽象概念是通用智能的关键，因此我们推出了 PuzzleVQA，这是一组基于抽象模式的谜题。通过此数据集，我们根据基本概念（包括颜色、数字、大小和形状）评估具有抽象模式的大型多模态模型。通过我们对最先进的大型多模态模型的实验，我们发现它们无法很好地推广到简单的抽象模式。值得注意的是，即使是 GPT-4V 也无法解决一半以上的难题。为了诊断大型多模态模型中的推理挑战，我们逐步用视觉感知、归纳推理和演绎推理的真实推理解释来指导模型。我们的系统分析发现，GPT-4V的主要瓶颈是视觉感知和归纳推理能力较弱。通过这项工作，我们希望阐明大型多模态模型的局限性以及它们如何在未来更好地模拟人类认知过程。   由   提交 /u/sgpfc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bl12g2/d_puzzlevqa_diagnosing_multimodal_reasoning/</guid>
      <pubDate>Fri, 22 Mar 2024 14:49:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有“完整”LLMOps 的资源或存储库吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bkyr87/d_any_resources_or_repos_of_complete_llmops/</link>
      <description><![CDATA[您好， 因此，我的团队希望探索在生产中应用法学硕士，但我们根本不知道从哪里开始堆栈的术语。您能否推荐一些资源，以便我能够为 LLMOps 的外观打下坚实的基础？谢谢！   由   提交 /u/TheCockatoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bkyr87/d_any_resources_or_repos_of_complete_llmops/</guid>
      <pubDate>Fri, 22 Mar 2024 13:04:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何用更少的 GPU 内存训练神经网络：可逆残差网络回顾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bkv29x/r_how_to_train_a_neural_network_with_less_gpu/</link>
      <description><![CDATA[探索可逆残差网络的有趣方法。 OpenCV.ai 团队的新文章回顾了一种减少 GPU 内存需求的方法在神经网络训练期间。您将发现可逆残差网络在神经网络训练期间如何节省 GPU 内存。该技术在“可逆残差网络：无需存储激活的反向传播”中详细描述。通过不存储反向传播的激活，可以有效地训练更大的模型。了解其在降低硬件要求方面的应用，同时保持 CIFAR 和 ImageNet 分类等任务的准确性。   由   提交/u/Human_Statistician48   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bkv29x/r_how_to_train_a_neural_network_with_less_gpu/</guid>
      <pubDate>Fri, 22 Mar 2024 09:21:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当前小型（例如，<10,000 个参数）语言模型中最好的是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bkqd2q/d_what_is_the_current_best_in_tiny_say_10000/</link>
      <description><![CDATA[显然，我们都听说过大语言模型，甚至听说过“小”语言模型。语言模型非常大（通常&gt; 100万个参数）。显然（除非我严重误解了语言模型的工作原理），您至少需要与词汇量大小一样多的参数（因为人们可以想象的最基本的模型只是为每个后续的概率分配一个固定的概率）单词，无论上下文如何 - 显然任何有用的模型都会做比这更复杂的事情）。 但我想知道小型模型的最新技术是什么，以前存在的模型的大小“大数据”甚至是一个已经被创造出来的短语。我知道这现在可能是一个小众的事情，业内很少有人致力于此。但我认为（或者至少我希望）至少仍然有爱好者在业余时间从事此类工作，就像仍然有人为 NES 编写自制游戏一样。 我&#39;我正在谈论一种可以在几个下午内在 C/C++ 中从头开始构建的模型（模型和训练算法），而不使用任何第三方依赖项/框架，可以进行训练和推理，甚至不需要显卡等。最重要的是，什么架构在这些限制下工作得最好？当限制在这个大小时，有什么能打败 HMM、n-gram 模型等吗？   由   提交/u/math_code_nerd5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bkqd2q/d_what_is_the_current_best_in_tiny_say_10000/</guid>
      <pubDate>Fri, 22 Mar 2024 03:58:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>