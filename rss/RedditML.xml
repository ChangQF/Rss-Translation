<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 11 Mar 2024 09:14:27 GMT</lastBuildDate>
    <item>
      <title>[P] 使用机器学习预测心脏病</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbyn0h/p_heart_disease_prediction_using_ml/</link>
      <description><![CDATA[所以我需要做的项目是使用机器学习预测心脏病。我想使用 Kaggle 的数据集。它被称为心力衰竭预测，由 5 个数据集组成。（克利夫兰、瑞士、匈牙利、新海滩、stalog）问题是我只能找到其中一个的发布日期。 Cleveland 于 1990 年发布。 我的老师说数据集太旧了，我的项目的重点是开发新的算法/模型。如果数据集太旧，就没有其他办法可以处理。所以他给了我两个选择。一种选择是找到一个新的现代数据集（例如，添加新的属性，如血液中的肌钙蛋白），可以将其添加到我当前的数据集中，这样我实际上就可以拥有一个新的、更好的预测模型，或者更改我的项目标题。  有什么建议吗？我当前的数据集属性是：性别、年龄、静息血压、静息心电图、t波斜率、胆固醇水平、空腹血糖、胸痛、运动时心绞痛。    由   提交/u/followmesamurai  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbyn0h/p_heart_disease_prediction_using_ml/</guid>
      <pubDate>Mon, 11 Mar 2024 08:49:57 GMT</pubDate>
    </item>
    <item>
      <title>[d] 合成数据生成方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbyfib/d_synthetic_data_generation_methods/</link>
      <description><![CDATA[       我编写了自定义代码，为自己生成合成数据以用于微调LLM， 它提供了“系统、用户、助手”格式的jsonl文件。 openai 微调器接受哪个。  https://preview.redd.it/lw5lkr442onc1.png?width=1273&amp;format=png&amp;auto=webp&amp;s=72661016111ad9e65eb4b019f10890e9ba02efad 但我想知道是否还有其他有效的方法去做吧？你们为法学硕士生成综合数据的方法有哪些，是否遇到任何问题？   由   提交/u/Medium_Alternative50   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbyfib/d_synthetic_data_generation_methods/</guid>
      <pubDate>Mon, 11 Mar 2024 08:35:00 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 6700xt 上的 Bark 文本转语音？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbybnw/discussion_bark_text_to_speech_on_6700xt/</link>
      <description><![CDATA[在 6700xt 上进行文字转语音？ 我想购买 12 GB vram GPU，但我的预算很低 您认为 aMD 与树皮文本和语音不兼容吗？   由   提交 /u/Visible-Employment43    reddit.com/r/MachineLearning/comments/1bbybnw/discussion_bark_text_to_speech_on_6700xt/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbybnw/discussion_bark_text_to_speech_on_6700xt/</guid>
      <pubDate>Mon, 11 Mar 2024 08:27:27 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] [D] NER 中手动标记的替代方案？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbxpsi/discussion_d_any_alternatives_to_manual_labelling/</link>
      <description><![CDATA[问题 -  有 NER 的替代方案吗？ （正则表达式不起作用，因为句子和单词/短语边界没有明确定义）任何无监督/半监督/自监督方法？ 对于标记，是否有手动标记的替代方法？  详细信息： 我有一个关于人物传记的自由文本列，其中包含不同的标识符，例如姓名、身份证号。 、电话号码、电子邮件、出生日期、国籍等。我需要将它们提取到正确的标签下（例如 NAM 代表名称，ID 代表 ID 号等）。每个实体标签可以有多种变体（例如，名称可以出现在“名称：”或“别名：”或“又名：”或“也称为”之后。此外，实体的存在（及其传记中的变体（有些传记只包含姓名、电子邮件和电话号码以及身份证号，很少包含国籍和出生日期）。我正在尝试应用 NER。但是，预训练的 NER 模型不包含我需要的实体，所以我需要用标记数据来训练模型。对于标记，我手动标记了大约 1K 传记 - 相当于 300,000 个标记。如果这些传记的性能不够，将来可能会有更多传记要标记。 .问题是标记是一项超级密集的任务。 我手动标记了 470 个传记，并尝试训练 crf、spacy 的 ner 解决方案和 bert 令牌分类器。对于那些计数的实体，性能较低&lt;1K。我尝试仅选择那些包含要提取的实体的传记进行标记。我尝试过使用CRF模型进行伪标记，但效果不佳。我将无法推送有关 spacy 神童的数据（违反公司政策）   由   提交/u/Ann2_123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbxpsi/discussion_d_any_alternatives_to_manual_labelling/</guid>
      <pubDate>Mon, 11 Mar 2024 07:43:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对 numenta/nupic 有何看法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbxahe/d_what_do_you_think_about_numentanupic/</link>
      <description><![CDATA[https://youtu.be/rYxnWzooxiY?si =JPlvrILNq9_rbRqG   由   提交 /u/CodingButStillAlive   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbxahe/d_what_do_you_think_about_numentanupic/</guid>
      <pubDate>Mon, 11 Mar 2024 07:11:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：使用廉价组件运行旧版本的 Rapids</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbuhq2/d_seeking_advice_using_cheap_components_for/</link>
      <description><![CDATA[大家好，我是新来的。我想知道是否可以使用相当便宜的组件来运行旧版本的 Rapids，因为最新的 24.02 版本不支持 Pascal 的 GPU。 我的想法是使用一些带有多个废弃的 E5v4 CPU矿卡 - P102 10Gb，非常便宜，并且提供与 1080ti 几乎相同的性能。唯一的缺点是它们只支持 PCIe 3.0x4（每卡 40 美元）。 我发现有些人可以安装 NVIDIA 驱动程序版本 525 或更高版本，可以与 CUDA 12 配对。但是，我找不到任何安装旧版本Rapid AI的文档。 我主要想将它用于CuDF。 非常感谢！    由   提交 /u/Exciting-Purple346    reddit.com/r/MachineLearning/comments/1bbuhq2/d_seeking_advice_using_cheap_components_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbuhq2/d_seeking_advice_using_cheap_components_for/</guid>
      <pubDate>Mon, 11 Mar 2024 04:14:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对比学习的梯度累积（InfoNCE）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbuacq/d_gradient_accumulation_for_contrastive_learning/</link>
      <description><![CDATA[我正在训练多模态对齐模型，但即使使用混合精度训练，我的 GPU 也只能容纳 64 的批量大小。根据 SimCLR 论文，较小的批量大小对于学习来说并不是最佳选择。有什么办法可以在这里实现梯度累积吗？   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbuacq/d_gradient_accumulation_for_contrastive_learning/</guid>
      <pubDate>Mon, 11 Mar 2024 04:03:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尝试使用 JEPA 理解推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbnb5q/d_trying_to_understand_inference_with_jepa/</link>
      <description><![CDATA[灵感来自 Lex Fridmans 播客剧集 与 Yann LeCun 一起，我试图通过阅读 I-JEPA 论文来提高我对 JEPA 和基于能量的模型的理解以及这些讲义。  我从学习高度语义特征的角度理解JEPA的吸引力/半监督过程中连续图像数据的表示。但真正让我困惑的是 Yann LeCun 的说法，一旦像这样的模型经过训练，你就可以进行基于优化的推理，基本上优化 Y 以最小化能量。  这个生成过程已经被证明了吗？除了预训练的 JEPA 模型之外，为了在这个基于优化的过程中生成图像/文本响应，还需要哪些组件？   由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbnb5q/d_trying_to_understand_inference_with_jepa/</guid>
      <pubDate>Sun, 10 Mar 2024 22:37:56 GMT</pubDate>
    </item>
    <item>
      <title>拥有非参数估计背景是否是进入机器学习的有用途径？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</link>
      <description><![CDATA[我是统计学硕士生。我的背景几乎全部基于基础统计理论，我的论文是非参数估计（特别是非参数回归）。基本上，我的“机器学习”知识源于一些经典的非参数估计书籍，例如（Tysbakov、Wasserman、Tibshirani/Hastie 和 Friedman）。统计学习的要素几乎是我在机器学习方面的背景，因为我的论文是关于非参数回归的经典方法之间的交集，例如基于树的方法、核平滑器和样条曲线，用于估计因果推理中的平均治疗效果。 但是，我有时会觉得自己的机器学习背景“相当老”。比如说，我不知道非参数回归背景对于现代机器学习工作有多有吸引力。一般来说，我们深入研究了非参数和统计学中的许多渐近理论，而且我知道对于大多数机器学习工作来说，没有人真正关心渐近保证。  有谁知道我的知识是否真的与当今主要关注神经网络的时代的机器学习工作相关？   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</guid>
      <pubDate>Sun, 10 Mar 2024 20:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] Lora 的记忆增益从何而来？ （除了优化器状态）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbgu8d/d_where_do_lora_memory_gains_come_from_apart_from/</link>
      <description><![CDATA[你好， 我已经阅读 Lora 的解释几个小时了，有一些东西我无法包装我的环顾四周：记忆力增强。我知道优化器状态可以获得很多冻结层不需要的效果。 但是，在 Lora 论文（第 4.2 章）中指出  与完全微调相比，我们还观察到在 GPT-3 175B 上的训练过程中加速了 25%，因为我们不需要计算绝大多数参数的梯度。  但是冻结层的梯度不需要计算吗？即使它们的权重不会随之更新，也必须计算它以获得可训练矩阵的梯度。 在经典微调中，需要梯度，因为大多数时候只有最后一层是训练完毕，反向传播就到此为止，但是对于 Lora，可训练参数位于所有注意力头中，因此反向传播需要继续直到那里，还是我误解了什么？ 所以，如果需要梯度，并且还需要激活+权重，那么唯一的内存增益将来自优化器？根据我的经验，LoRA 内存使用量似乎比“正常”内存使用量低得多。微调，所以我想有些东西我不明白 ​ 非常感谢您的帮助！   由   提交 /u/Wats0ns   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbgu8d/d_where_do_lora_memory_gains_come_from_apart_from/</guid>
      <pubDate>Sun, 10 Mar 2024 18:12:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] OpenAI：JSON 模式与函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</link>
      <description><![CDATA[       由   提交/u/JClub  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</guid>
      <pubDate>Sun, 10 Mar 2024 18:01:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年，强化学习的最新趋势是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbawez/d_in_2024_what_are_the_latest_trends_on_rl/</link>
      <description><![CDATA[嗨， 这些天我正在研究决策转换器。 有争议的，同时试图找到最重要的论文，我注意到强化学习领域似乎没有发生太多事情。我注意到研究的重点是优化 Transformer 和训练巨大的语言和视觉模型（被视为监督模型）。这是强化学习领域的新大事吗？ 强化学习的最新趋势是什么？   由   提交/u/__Julia  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbawez/d_in_2024_what_are_the_latest_trends_on_rl/</guid>
      <pubDate>Sun, 10 Mar 2024 13:55:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年用于机器学习的 AMD 卡？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb9ylx/d_amd_cards_for_machine_learning_in_2024/</link>
      <description><![CDATA[AMD 和 AI 的状况如何？我想知道 AMD 和 Nvidia GPU 之间的性能差异有多大，以及 7600xt 是否充分支持 pytorch 和 TensorFlow 等机器学习库。上次我听说 AMD 卡可以支持 ROCm，但存在不一致、软件问题以及速度慢 2 - 5 倍的问题。 就个人而言，您会选择 rx7600 而不是 4060 吗？    由   提交 /u/AtomicPiano   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb9ylx/d_amd_cards_for_machine_learning_in_2024/</guid>
      <pubDate>Sun, 10 Mar 2024 13:08:44 GMT</pubDate>
    </item>
    <item>
      <title>GAN 仍然有意义吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</link>
      <description><![CDATA[[D] 随着 Difussion 模型的不断兴起，我很好奇 GAN 是否会卷土重来？有什么想法吗？   由   提交 /u/Superb-Assignment-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</guid>
      <pubDate>Sun, 10 Mar 2024 06:00:24 GMT</pubDate>
    </item>
    </channel>
</rss>