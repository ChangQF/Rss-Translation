<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Sun, 13 Apr 2025 02:50:19 GMT</lastBuildDate>
    <item>
      <title>[D]传统的机器学习算法（例如神经网，逻辑回归，树木）是否会被LLM替换？因此，数据科学家会失业吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxwklu/d_will_traditional_machine_learning_algorithms/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   llms也可以做出预测，在某些情况下很好。 LLM正在快速发展。使用LLMS进行预测的一个重要优点是，您不必明确地收集数据和过程功能。 LLM会照顾好此事（顺便说一句，通过训练LLM或通过代理系统）。  公司中的数据科学家是否认为我们不再需要（例如在5年内）？ 显然，如果输入数据只是通常的（x，y）对，LLM将不像经验丰富的ML工程师一样，可以找到最佳的培训程序（算法和参数），以获取最佳培训（算法和参数）。更广泛地说，它们比人类ML工程师可以有效地吸收许多其他信息，并将其整合到训练系统中。 有两个完全不同的方向：   ds/ml代理商越来越受欢迎，可能会比人类ML工程师更受欢迎，并且可能比人类ML工程师更好地工作，因为最终可以很好地了解如何收集/清洁数据和正确的模型。这种趋势应该很明显。 另一个方向是，LLM（或他们在5年内将被召唤）是否可以直接创建更好的预测。它已经在某些领域发生了，例如生成标签以取代人类法官。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxwklu/d_will_will_traditional_machine_learning_algorithms/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxwklu/d_will_traditional_machine_learning_algorithms/</guid>
      <pubDate>Sun, 13 Apr 2025 01:30:16 GMT</pubDate>
    </item>
    <item>
      <title>[P]谐波激活：神经网络的周期性和单调功能扩展（预印本）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxqtoo/p_harmonic_activations_periodic_and_monotonic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，伙计们！我最近发布了一个预印本，提出了一个新的激活功能系列，专为无标准化的深层网络而设计。我是一名独立研究人员，致力于MLP和变压器的表达性非线性。   tl; dr：  i提出了一个残留激活函数：    f（x）= x +α·g（sin²（sin²（πx/2）） gelu） 我想听听反馈。这是我的第一篇论文。   preprint ：[ &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/henriquelmeeee     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxqtoo/p_harmonic_activations_periodic_and_monotonic/</guid>
      <pubDate>Sat, 12 Apr 2025 20:39:09 GMT</pubDate>
    </item>
    <item>
      <title>[p]如果您可以在每gpu中运行50+ LLMS，而无需保持记忆力呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxn5fe/p_what_if_you_could_run_50_llms_per_gpu_without/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们一直在尝试使用AI本地运行时，该运行时间在2-5秒内快照llms（13b – 65b），并动态运行50多个型号，每个GPU每gpu运行50多个型号 - 毫不及时地居住在记忆中。 +内存缓冲区，即使在无法获得完整设备访问的共享GPU环境中恢复模型。 这似乎可以解锁：•真正的无服务器LLM行为（无闲置的GPU成本）•低延迟的多模型编排•低延迟•更好的GPU对ASTIC或DYAMIC WOMSIC的gpu利用率更好，如果其他人则在此处概述 cul&gt; cul&gt; cul&gt; cul  cul&gt; culiious  cy  culiious  culiious  cy  cy/culiious  cy  cy  cy  cy&gt; •动态GPU内存管理（MIG，KAI调度程序等）•CUDA-CHECKPOINT /部分设备访问挑战&lt; / p&gt; 如果有用的话，很乐意分享更多的技术细节。很想交换笔记或听到您看到的当前型号的疼痛点！ 对于对更新，分解或飞行员访问感到好奇的人们 - 我在x：@inferxai上分享更多。我们正在积极建立  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxn5fe/p_hat_what_if_you_you_could_run_run_50_llms_per_gper_gpu_without/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxn5fe/p_what_what_you_you_could_run_50_50_50_50_per_per_gpu_gpu_without/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxn5fe/p_what_if_you_could_run_50_llms_per_gpu_without/</guid>
      <pubDate>Sat, 12 Apr 2025 17:53:52 GMT</pubDate>
    </item>
    <item>
      <title>[D]“推理模型并不总是说出他们的想法”  - 有人提示吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxjwi2/d_reasoning_models_dont_always_say_what_they/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有人尝试通过使用自己的提示来复制的“推理模型并不总是说出他们的想法” 纸张？我正在努力再现这些输出。如果您对此进行了尝试并进行了微调，您是否可以分享您的提示或一路上获得的任何见解？任何讨论或指示都将不胜感激！ 参考，这是：       - 提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxjwi2/d_reasoning_models_models_dont_always_always_say_say_what_they/&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxjwi2/d_reasoning_models_dont_always_say_what_they/</guid>
      <pubDate>Sat, 12 Apr 2025 15:30:53 GMT</pubDate>
    </item>
    <item>
      <title>[n] Google开放，让企业自我主机SOTA模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从主要参与者中，这听起来像是一个很大的变化，并且大多会为企业提供有关数据隐私的有趣视角。 Mistral在Openai和Anthropic维护更多封闭式产品或通过合作伙伴时已经做了很多事情。   https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-models-in-their-own-data-centers.html     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxin3q/n_google_open_to_to_let_tto_tto_entreprises_erse_host_host_sota/”&gt; [link]   [commist]     ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</guid>
      <pubDate>Sat, 12 Apr 2025 14:33:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] D1：通过增强学习在扩散大语模型中扩展推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   最近的大型语言模型（LLMS）已证明了强大的推理能力，可以从在线增强学习（RL）中受益。这些功能主要在从左到右的自回归（AR）一代范式中证明。相比之下，基于扩散的非运动范式以粗到精细的方式产生文本。尽管与AR相比，最近基于扩散的大语言模型（DLLM）已经达到了竞争性语言建模性能，但尚不清楚DLLM是否也可以利用LLM推理的最新进展。为此，我们提出了D1，这是一个框架，可以通过有监督的Finetuning（SFT）和RL的组合将预先训练的戴上DLLM适应推理模型。具体而言，我们开发并扩展了技术以改善预验证的DLLM中的推理：（a）我们利用蒙版的SFT技术直接从现有数据集中提炼知识并灌输自我提高行为，（b）我们引入了一种新颖的无评论，策略级别的RL算法，称为DIFFU-GRPO。通过实证研究，我们研究了不同的训练后食谱对多个数学和逻辑推理基准的性能。我们发现D1可以产生最佳性能，并显着提高了最先进的DLLM的性能。  在扩散扩散大语模型上，使用强化学习来缩放扩散模型。当涉及到实际上原因的语言模型时，绝对需要注意！ 纸链接：  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/hiskuu     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</guid>
      <pubDate>Sat, 12 Apr 2025 10:30:15 GMT</pubDate>
    </item>
    <item>
      <title>[p]简单独立tfrecords数据集读取器具有随机访问和搜索功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，在工作中，我们正在使用tfrecords存储大多数数据集。但是不时。我们需要检查数据，以更好地对我们的模型进行更好的预测，例如为了找到特定类等的示例。由于tfrecord在本质上是顺序的，它们不允许进行标准的随机访问切片。 我决定创建这个简单的工具，该工具允许为Tfrecrods创建一个简单的可搜索索引，以稍后可用于各种数据集分析。  这是项目页面： https://github.com/kmkolasinski/tfrecords-readers-readers-readers-reader  required Dataset can be read directly from Google Storage Indexing of 1M examples is fast and usually takes couple of seconds Polars is used for fast dataset querying tfrds.select(&quot;select * from index where name ~ &#39;rose&#39; limit 10&quot;)  Here is a quick start example from readme：  导入tensorflow_dataset作为tfds＃仅需要下载数据集导入数据集导入tfr_reader从pil导入import Import Import Import Import Importim Impart ipy ipyplot数据集，dataSet_info = tfds.load（tfford_flowers102&#39;，splite =&#39;train =&#39;train for_info = true）索引label = feature [label;]。值[0]返回{bail; bail; bail; bail，&#39;d dataset_info.features; dataset_info.data_dir，＃索引选项，如果已经创建索引fileepattern =;*。限制10&#39;）assert示例== tfrds [rows [; _row_id;]]样本，name = []，[]，[示例）中的示例（示例）（示例）：image = image.open（示例[emampe; image; image＆quot; image＆quot bytes_io [0]）。 samples.append（image）ipyplot.plot_images（样本，名称）   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kmkolasinski     [links]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxbmss/p_simple_starlone_tfrecords_dataset_dataset_reader_with/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</guid>
      <pubDate>Sat, 12 Apr 2025 07:13:28 GMT</pubDate>
    </item>
    <item>
      <title>[d]添加新的词汇令牌 +微调LLMS以遵循说明是无效的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在使用指令调整LLMS和VLMS进行实验，要么将新的专用令牌添加到其相应的令牌/处理器中，或者不添加。设置是典型的：掩盖说明/提示（仅参加响应/答案）并应用CE损失。但是，没有什么特别的标准SFT。 但是，我观察到了使用其基本令牌/处理器训练的模型与经过修改的令牌训练的模型，对此有更好的验证损失和输出质量...对此有任何想法吗？  （我的hunch：很难增加这些新添加的令牌的可能性，而模型根本无法正确学习）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</guid>
      <pubDate>Fri, 11 Apr 2025 23:42:34 GMT</pubDate>
    </item>
    <item>
      <title>[D]用于产品标题和类别标准化的微调BART  - 仍然不够准确，任何更好的方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在建立一个来自摩尔多瓦各种在线商店产品的价格比较网站。我在约20,000个手动标准化产品标题的自定义数据集上微调了一个BART模型，并损失了0.013。 I also trained a separate model for predicting product categories. Unfortunately, the results are still not reliable — the model struggles with both product title normalization and category assignment, especially when product names have slight variations or extra keywords. I don’t have access to SKU numbers from the websites, so matching must be done purely on text. Is there a better approach or model I might be missing?或者也许是专门针对此类问题设计的工具/应用程序？ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mali5k     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwz2k3/d_finetuned_bart_for_for_for_for_product_title_category/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</guid>
      <pubDate>Fri, 11 Apr 2025 19:59:31 GMT</pubDate>
    </item>
    <item>
      <title>[P]我们为LLM建立了类似OS的运行时间 - 好奇是否有人在做类似的事情？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，我们正在尝试使用AI本地运行时，该运行时间在2-5秒钟内以llms（例如13b – 65b）来捕捉llms（例如13b – 65b），并且动态运行50多个型号，每gpu始终在记忆中始终居住在记忆中，而不是传统的prial。 GPU执行 +内存状态和点播模型。这似乎解锁了：•实际的无服务器行为（无空闲成本）•低潜伏期时的多模型编排•更好地使用代理工作负载的GPU利用率 是否有人尝试过与多模型堆栈，代理工作流程或动态内存真实分配相似的东西很想听听别人如何接近这一点的 - 或者这甚至与您的中世纪需求保持一致。 很乐意在有用的情况下分享更多的技术细节！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_for_for_llms_curious_if/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_llms_curious_curious_if/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</guid>
      <pubDate>Fri, 11 Apr 2025 18:50:35 GMT</pubDate>
    </item>
    <item>
      <title>[P]一种轻巧的开源模型，用于产生漫画</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</guid>
      <pubDate>Fri, 11 Apr 2025 15:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] CAT：亚季节变压器的循环趋势关注</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwqrud/r_cat_circularconvolutional_attention_for/</link>
      <description><![CDATA[https://arxiv.org/abs/2504.06704 CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and没有引入较重的操作，从而在诸如ImagEnet-1K和Wikitext-103的大规模基准上的幼稚pytorch实现中提供了一致的准确性提高，并且速度约为10％。   &lt;！ -  sc_on--&gt; 32;提交由＆＃32; /u/u/every-act7282     [link]     ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwqrud/r_cat_circularconvolutional_attention_for/</guid>
      <pubDate>Fri, 11 Apr 2025 14:08:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] B200 vs H100基准：早期测试显示高达57％的训练吞吐量和自我托管成本分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jw3b9b/p_b200_vs_h100_benchmarks_early_tests_show_up_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们在Lightly AI最近在欧洲早期访问了NVIDIA B200 GPU，并进行了一些独立的基准测试，将它们与H100相比，重点是计算机视觉模型培训工作量。 We wanted to share the key results as they might be relevant for hardware planning and cost modeling. TL;DR / Key Findings:  Training Performance: Observed up to 57% higher training throughput with the B200 compared to the H100 on the specific CV tasks we tested. Cost透视图（自主）：我们的分析表明，与典型的云H100实例相比，自托管B200可以提供明显较低的OPEX/GPU/小时（我们发现潜在的范围 〜6x-30x便宜，帖子中的详细信息/假设）。 This obviously depends heavily on utilization, energy costs, and amortization. Setup: All tests were conducted on our own hardware cluster hosted at GreenMountain, a data center running on 100% renewable energy.  The full blog post contains more details on the specific models trained, batch sizes, methodology, performance charts, and a breakdown of the cost注意事项：  我们认为，比较新一代的这些早期的现实世界数字可能对社区有用。很高兴在评论中讨论与新硬件的方法，结果或我们的经验！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/igorsusmelj     [link]         &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jw3b9b/p_b200_vs_h100_h100_benchmarks_early_early_tests_show_show_show_show_up_up_to/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jw3b9b/p_b200_vs_h100_benchmarks_early_tests_show_up_to/</guid>
      <pubDate>Thu, 10 Apr 2025 17:18:35 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jpdo7y/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</guid>
      <pubDate>Wed, 02 Apr 2025 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些经验丰富的人。   &lt;！ -  sc_on--&gt; 32;&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_be_hired/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Mon, 31 Mar 2025 02:30:37 GMT</pubDate>
    </item>
    </channel>
</rss>