<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 07 Dec 2024 01:20:46 GMT</lastBuildDate>
    <item>
      <title>[D] 多模式人工智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/</link>
      <description><![CDATA[多模态 AI 通过将文本、图像甚至视频组合成一个单一、有凝聚力的系统来改变游戏规则。人们将其视为 AI 能力的一次重大飞跃。 您认为哪些行业将从这项技术中受益最多？您认为将这些模型融入日常使用中会遇到什么挑战吗？ 很想听听大家的想法！    提交人    /u/Frosty_Programmer672   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/</guid>
      <pubDate>Fri, 06 Dec 2024 23:17:54 GMT</pubDate>
    </item>
    <item>
      <title>[D]选择性迁移学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/</link>
      <description><![CDATA[大家好， 我正在寻找可以自动对迁移学习的层进行分类和选择的方法。 如果您知道任何此类方法或研究，请告诉我或分享。  谢谢     提交人    /u/reshail_raza   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/</guid>
      <pubDate>Fri, 06 Dec 2024 21:30:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代理检索增强记忆生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/</link>
      <description><![CDATA[想象一个电子商务平台的客户支持聊天机器人，它从知识库中检索相关产品详细信息并执行网络搜索以获取更多信息。此外，它还会记住过去的对话，为回访用户提供无缝和个性化的体验。  它的工作原理如下：  - 将您自己的数据存储在知识库中 - 在我们的例子中是网站 URL。 - 将数据转换为嵌入并将其保存在 Qdrant 矢量数据库中。 - 使用 phidata Agentic Workflow 结合工具、LLM、内存和知识库。 代码实现视频：https://www.youtube.com/watch?v=CDC3GOuJyZ0    提交人    /u/External_Ad_11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/</guid>
      <pubDate>Fri, 06 Dec 2024 19:10:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] Switti：设计用于文本到图像合成的尺度变换器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/</link>
      <description><![CDATA[      Yandex Research 团队发布了用于快速文本转图像生成的按比例变换器的新论文和代码 Switti 的表现优于现有的 T2I AR 模型，可与最先进的 T2I 扩散模型相媲美，同时比蒸馏扩散模型更快。 带有检查点的代码：https://github.com/yandex-research/switti 生成示例    提交人    /u/_puhsu   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/</guid>
      <pubDate>Fri, 06 Dec 2024 16:58:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用线性投影和提升探索特征空间中的决策树的新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/</link>
      <description><![CDATA[      大家好， 我已经在一个项目上工作了一段时间，想分享一个我正在探索的概念。众所周知，基于决策树的模型通常使用某些指标（如 MSE、熵等）来分割特征空间。 我开始考虑一种替代方法：如果我们可以直接分割整个空间，而不是分割单个特征，会怎么样？然而，这似乎相当困难，因为确定空间中的边界和区域具有挑战性。 然后我有了一个想法 - 如果我把数据投影到特征空间内的一条线上，然后分割那条线，就像树通常建立在单个特​​征上一样，会怎么样？本质上，我正在考虑将点投影到一条线上，然后使用基于树的方法逐步分割它们。 这是该算法的高级视图：  将线性回归模型拟合到数据集（标准化值）。 将数据投影到回归定义的线上。 在此投影上应用决策树，有效地分割一个特征（投影轴）。 计算残差并在残差上拟合另一个线性模型，在此过程中应用提升。  由于在残差上拟合的新线性回归将定义单独的线，我假设通过提升，模型将随着时间的推移逐渐以所需的方式划分数据。 您可以在此处阅读有关该算法的更详细描述：算法PDF。 要可视化决策边界在 2D 数据集中的形成方式： SpaceBoostingRegressor 注意：如果您想查看视觉示例，上传高维 GIF 有时可能会成为问题。您可以在此处查看示例：GitHub 上的 Gif。 您还可以在存储库中查看代码：存储库 这种方法很简单，因为它假设线性，并且它适用于目标和特征之间存在高度线性相关性的场景，同时也允许一些非线性关系。您可以在 repo 中的 example.ipynb 文件中看到一个示例。但是，我不确定它在真实数据集上的表现如何，因为线性假设可能并不总是成立。 我想进一步研究这个算法，但速度对于扩展很重要。像 PCA 这样的技术似乎没有帮助，因为我需要线来反映目标和特征空间中的方差，而不仅仅是特征方差。我尝试使用 MLP 并从输出层之前的隐藏层中提取嵌入，这种方法效果更好，因为我们在更大的空间中评估目标，但这种方法太慢了，在实践中不可行。 我认为这个项目有很大的潜力，我正在寻找反馈、想法或任何有兴趣合作的人。欢迎任何意见或建议！    提交人    /u/zedeleyici3401   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/</guid>
      <pubDate>Fri, 06 Dec 2024 15:00:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们是否已经正式弄清楚 O1 型号与之前的型号有何不同？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/</link>
      <description><![CDATA[编辑：我误用了标题，好像 OpenAI 会确认 O1 是如何实现的。我已更改文本以反映我的意思。 我真的很想深入研究 O1 模型如何比以前的模型表现更好的技术。 研究人员是否就 OpenAI 可以做些什么来实现 O1 达成了明确的一致意见？ 从网上阅读中我听说了 MCTS、COT……等等，但这些方法中是否有任何一种得到研究人员的广泛认同？    提交人    /u/Daveboi7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/</guid>
      <pubDate>Fri, 06 Dec 2024 11:37:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将超过 1 亿行数据编码为嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/</link>
      <description><![CDATA[大家好， 我正在开发一个管道，使用 SentenceTransformers、PySpark 和 Pandas UDF 在 Dataproc Serverless 上将超过 1 亿行 编码为嵌入。 目前，处理所有内容需要几个小时。我只有一列包含句子，每个句子长度不到 30 个字符。它们使用 Docker 映像中的自定义模型编码为 64 维向量。 目前，该作业已运行超过 12 小时，有 57 个执行器（每个执行器具有 24GB 内存和 4 个核心）。我已经将数据划分为2000个分区，希望加快处理速度，但还是很慢。 下面是我的代码的核心部分： F.pandas_udf(returnType=ArrayType(FloatType())) def encode_pd(x: pd.Series) -&gt; pd.Series: try: model = load_model() return pd.Series(model.encode(x, batch_size=512).tolist()) except Exception as e: logger.error(f&quot;Error in encode_pd function: {str(e)}&quot;) raise  load_model函数如下： def load_model() -&gt; SentenceTransformer：model = SentenceTransformer（&quot;custom_model&quot;，device=&quot;cpu&quot;，cache_folder=os.environ[&#39;SENTENCE_TRANSFORMERS_HOME&#39;]，truncate_dim=64）返回模型 我尝试广播模型，但无法在 Pandas UDF 中引用它。 有人对此有什么建议吗？也许有更有效地加载模型、减少执行时间或更好地利用资源的方法？    提交人    /u/nidalap24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/</guid>
      <pubDate>Fri, 06 Dec 2024 09:29:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有针对难以辨认的笔迹的 OCR 建议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/</link>
      <description><![CDATA[      有人使用过 ML 模型来识别这样的手写内容吗？这个笔记本包含重要信息，可以帮助我破解我正在解决的一个难题。我总共有五本笔记本，都来自同一个人，而且笔迹模式一致。我的目标是使用 ML 识别和提取笔记，然后将它们转换为数字格式。 在知道 Tesseract 可能无法很好地处理这种难以辨认的样本后，我正在考虑使用 Google API。 但是，我也不确定 Google API 是否能够读取它。 我读到某处说 OCR+CNN 可能会起作用，所以我在这里征求建议。 谢谢！ 欢迎任何建议！     提交人    /u/SpaceSheep23   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/</guid>
      <pubDate>Fri, 06 Dec 2024 08:53:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尽管最近的论文指出了 OpenAI 的 O1 的局限性，但它如何在数学方面胜过其他算法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/</link>
      <description><![CDATA[最近的研究表明，最先进的 LLM 经常在数学推理方面遇到困难：  GSM-Symbolic 基准测试强调，当数值或问题措辞发生变化时，LLM 经常会失败，这表明它依赖于记忆，而不是真正的数学理解 (来源)。 逻辑推理研究（如 AIW 问题）表明，即使是基本的推理任务，其性能也不一致 (来源)。 此外，研究表明 LLM 缺乏有效的自我修正能力，经过多次迭代后性能会下降 (来源）。  尽管面临这些挑战，据报道，OpenAI 的新 O1 模型在数学基准测试中超过了所有其他模型。它如何解决数学推理中的这些已知问题，例如：  依赖记忆而不是理解？ 不同问题变化中的推理不一致？ 无法有效地自我纠正错误？  很想听听见解或假设！    提交人    /u/AImSamy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/</guid>
      <pubDate>Fri, 06 Dec 2024 06:53:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用法学硕士 (LLM) 进行时间序列推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/</link>
      <description><![CDATA[  由    /u/HydrousIt  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/</guid>
      <pubDate>Fri, 06 Dec 2024 04:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何消除此数据集中的噪音</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/</link>
      <description><![CDATA[我有一个数据集，绘制时会显示一条嘈杂的黑线。我想消除这种噪音，以获得一条更清晰的趋势线（类似于所示的红线）。您会推荐哪些方法来降低噪音？ https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807    提交人    /u/mrtule   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/</guid>
      <pubDate>Fri, 06 Dec 2024 00:30:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过语言模型的外部和内部规划掌握棋盘游戏 - DeepMind</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/</link>
      <description><![CDATA[论文：https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf 摘要： 虽然大型语言模型在一系列复杂任务（例如文本生成、问答、摘要）上表现良好，但强大的多步骤规划和推理对它们来说仍然是一个相当大的挑战。在本文中，我们表明基于搜索的规划可以显著提高 LLM 在几种棋盘游戏（国际象棋、Fischer Random/Chess960、Connect Four 和 Hex）中的游戏实力。我们介绍、比较和对比了两种主要方法：在外部搜索中，模型指导蒙特卡洛树搜索 (MCTS) 的推出和评估，而无需调用外部引擎；在内部搜索中，模型直接在上下文中生成潜在未来的线性树和最终选择。两者都建立在预先训练相关领域知识的语言模型上，捕捉这些游戏中的转换和价值函数。我们发现我们的预训练方法可以最大限度地减少幻觉，因为我们的模型在状态预测和合法动作方面非常准确。此外，内部和外部搜索确实提高了对抗最先进机器人的胜率，甚至在国际象棋中达到大师级的表现，同时以与人类大师级选手类似的每决定移动计数搜索预算进行操作。我们将搜索与领域知识相结合的方式并不特定于棋盘游戏，这表明可以直接扩展到更通用的语言模型推理和训练技术。    提交人    /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/</guid>
      <pubDate>Thu, 05 Dec 2024 23:58:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 陷入人工智能地狱：法学硕士毕业后该做什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</guid>
      <pubDate>Thu, 05 Dec 2024 20:49:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 01 Dec 2024 16:00:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>