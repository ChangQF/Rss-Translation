<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 25 Apr 2024 12:20:37 GMT</lastBuildDate>
    <item>
      <title>[D] 是否有一个适用于 NVIDIA GPU 的等效 BigDL 项目，它允许使用 Spark 在 DL 集群上分配工作负载？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</link>
      <description><![CDATA[所以就有了这个相对较新的“BigDL”项目” (https://bigdl.readthedocs.io/en/latest/)，适用于 Intel CPU 和 Intel GPU ，但没有提到它适用于 NVIDIA GPU。 Spark 集群上是否有适用于 NVIDIA GPU 的等效库？  &amp; #32；由   提交/u/PepperGrind  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</guid>
      <pubDate>Thu, 25 Apr 2024 10:18:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 新书：构建 GPT：人工智能的工作原理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cco85h/p_new_book_build_gpt_how_ai_works/</link>
      <description><![CDATA[     &lt; /td&gt; 经过几个月的努力，我现在很高兴地说我的新书“BUILD GPT: HOW AI WORKS”可在亚马逊上购买。  它介绍了从头开始构建 GPT 的过程，并解释了它的工作原理，重点是提供直观性。  我要感谢所有帮助我完成这本书的人，他们在致谢部分。 请随时与任何有兴趣了解 GPT 或有兴趣的人分享这本书。构建 GPT。 https:/ /preview.redd.it/ixhw5wz9mlwc1.png?width=1507&amp;format=png&amp;auto=webp&amp;s=5f9a0eb5d1f49ed936f12e4527950090d161852c   由   提交/u/Pure_Nerve_595   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cco85h/p_new_book_build_gpt_how_ai_works/</guid>
      <pubDate>Thu, 25 Apr 2024 10:08:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最适合我的情况的 TTS 模型是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccmhjb/d_what_is_the_best_tts_model_for_my_case/</link>
      <description><![CDATA[嗨。这是新人的问题。 最大的担忧是生成速度。我想在大约100ms内生成大约5秒的语音。我想知道哪种模型在这些条件下表现最好（SOTA）。 哪种模型最适合我？ 我认为“” styletts2”是最好的。 如果您有任何相关经验或了解任何其他信息，我将非常感谢您的帮助。 谢谢！   由   提交/u/hwk06023  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccmhjb/d_what_is_the_best_tts_model_for_my_case/</guid>
      <pubDate>Thu, 25 Apr 2024 08:07:36 GMT</pubDate>
    </item>
    <item>
      <title>[R]法国GEC数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccdzt8/r_french_gec_dataset/</link>
      <description><![CDATA[嗨，有人知道法国 L2 GEC 数据集（在会议上发布的）吗？   由   提交 /u/R-e-v-e-r-i-e-   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccdzt8/r_french_gec_dataset/</guid>
      <pubDate>Thu, 25 Apr 2024 00:14:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有关如何构建流式 ML 应用程序的教程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccbcoq/d_tutorial_on_how_to_build_streaming_ml/</link>
      <description><![CDATA[我的主要专长是音频处理，但我相信这项任务也发生在其他领域：在无限长的输入块上运行模型。虽然对于某些架构来说它很简单，但对于卷积网络来说可能会变得乏味。我整理了一个如何构建流式 ML 应用程序的综合教程：https://balacoon.com/blog/streaming\_inference/&lt; /a&gt;.我很想知道这是否是一个常见问题以及人们通常如何处理它。因为该主题的资源非常稀缺。   由   提交 /u/clementruhm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccbcoq/d_tutorial_on_how_to_build_streaming_ml/</guid>
      <pubDate>Wed, 24 Apr 2024 22:16:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 R^2 如此疯狂？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccagwc/d_why_is_r2_so_crazy/</link>
      <description><![CDATA[      ​ https://preview.redd.it/jpiyt4b9yhwc1.png?width=1165&amp;format= PNG&amp; ;auto=webp&amp;s=95d80f8f9c9241d722717ad25215be4077d541ca 基于 MSE 看起来不错吧？但为什么我的 R^2 开始时如此负并且接近 0？这可能是我计算方式的错误吗？  这是在我在训练前最小化标签之后发生的。 这是一个 LSTM，用于预测棒球比赛的得分。   由   提交 /u/Cloverdover1   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccagwc/d_why_is_r2_so_crazy/</guid>
      <pubDate>Wed, 24 Apr 2024 21:40:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在数据分割过程中保留数据的空间分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc3s98/d_preserving_spatial_distribution_of_data_during/</link>
      <description><![CDATA[      您好，我正在尝试使用随机森林模型对德国巴伐利亚河流中的硝酸盐浓度进行建模。我使用 Python，主要使用 sklearn。我有 490 个水质站的数据。我遵循 LongzhuQ.Shen 等人论文中的方法，该论文可以在这里找到：https:// www.nature.com/articles/s41597-020-0478-7 我想将我的数据集分为训练集和测试集，以便两个集中数据的空间分布相同。这个想法是，如果数据分割忽略空间分布，则训练集可能最终会集中来自人口稠密区域的点，而忽略稀疏区域。这可能会扭曲模型的学习过程，使其在整个感兴趣领域的准确性或概括性降低。 sklearn train_test_split 只是将数据随机划分为训练集和测试集，并且不考虑数据中的空间模式。 我上面提到的论文遵循这种方法：“我们将完整数据集分成两个子数据集” -分别是数据集、训练和测试。为了考虑监测站空间分布的异质性，我们在数据分割步骤中采用了空间密度估计技术，通过使用带宽为 50 km 的高斯核（使用 GRASS GIS33 中可用的 v.kernel）构建密度表面来计算每个物种和季节。所得密度表面的像素值用作权重因子，将数据分成具有相同空间分布的训练和测试子集。” 我想遵循相同的方法，但不使用草地 GIS ，我只是自己用 Python 构建密度曲面。我还提取了概率密度值和站点的权重。 （附图） 现在我面临的唯一问题是如何使用这些权重将数据拆分为训练集和测试集？我检查了sklearn train_test_split函数中没有可以考虑权重的关键字。我也与GPT 4聊天来回，但它也无法给我一个明确的答案。我在互联网上也没有找到任何关于此的具体信息。也许我遗漏了一些东西。 还有其他函数可以用来执行此操作吗？或者我必须编写自己的算法来进行分割？如果是后者，您能否建议我一种方法，以便我自己编码？ 在附图中，您可以看到站点的位置以及使用核密度估计生成的概率密度表面方法（使用高斯核）。 还附上我的数据框的屏幕截图，让您对数据结构有一些了解。 （经度（&#39;lon&#39;）列之后的所有列都用作特征。NO3 列用作目标变量。） 如果您有任何答案，我将不胜感激。 &amp; #x200b; 生成的概率密度表面使用高斯核的核密度估计方法。 ​ 我用来模拟硝酸盐浓度的数据集  &amp;# 32；由   提交/u/dr_greg_mouse  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc3s98/d_preserving_spatial_distribution_of_data_during/</guid>
      <pubDate>Wed, 24 Apr 2024 17:14:15 GMT</pubDate>
    </item>
    <item>
      <title>[N] Snowflake发布开放(Apache 2.0) 128x3B MoE模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc3255/n_snowflake_releases_open_apache_20_128x3b_moe/</link>
      <description><![CDATA[链接： ​ https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/ ​ https://replicate.com/雪花/雪花北极指令   由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc3255/n_snowflake_releases_open_apache_20_128x3b_moe/</guid>
      <pubDate>Wed, 24 Apr 2024 16:45:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么这么简单的一句话会毁掉法学硕士学位？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</link>
      <description><![CDATA[      这是我在 MS Copilot (GPT4 Turbo) 中输入的提示。 它是德语，但它只是意味着“如果我先洗完澡会有什么缺点吗？”），所以这不可能是另一个SolidGoldMagikarp 或类似的，因为这些单词显然都在分词器和训练词汇中。 为什么这么简单的句子会导致这种情况？有什么猜测吗？ （也尝试过 Claude Opus 和 LLama 3 70b，效果很好） ​ https://preview.redd.it/9x6mva7b6gwc1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=bb6ac52d1c52 d981161e8a864c5d1dd3794ca392   由   提交/u/michael-relleum  /u/michael-relleum  reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</guid>
      <pubDate>Wed, 24 Apr 2024 15:59:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] 说话人分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc0f40/r_speaker_diarization/</link>
      <description><![CDATA[大家好，  我正在开发一个项目，我想从音频/视频创建说话者感知的转录本，最好是使用开源解决方案。我尝试了很多方法，但似乎没有一种方法可以开箱即用。  我已经尝试过： ​  whisperX: https://github.com/m-bain/whisperX（使用 pyannote） whisper-diarization：https://github.com/MahmoudAshraf97/whisper-diarization（使用 Nemo） AWS Transcribe  AssemblyAI API  Picovoice API   我需要更深入地挖掘并了解导致错误二值化的原因，但我正在寻找改进说话者二值化的建议。如果您曾在该领域工作并取得过成功，请与我们联系。谢谢！    由   提交 /u/anuragrawall   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc0f40/r_speaker_diarization/</guid>
      <pubDate>Wed, 24 Apr 2024 15:01:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我制作了一个应用程序来根据评论预测 ICML 论文接受情况</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/</link>
      <description><![CDATA[https://www.norange.io/ projects/paper_scorer/ 几年前，u/programmerChilli 分析了 ICLR 2019 评审数据并训练了一个模型，该模型可以相当准确地预测 NeurIPS 的接受结果。 我决定继续进行这项分析，并根据较新的 NeurIPS 评论训练了一个模型（总共约 6000 个参数），该评论的评论数量是 ICLR 2019 的两倍。此外，NeurIPS 的评论评分系统自 2019 年以来已经发生了变化，以下是我了解到的情况： 1) 两个会议一致拒绝几乎所有得分 &lt;5 的提交内容，并接受那些得分 &gt;6 的提交内容。被接受的论文中最常见的分数是 6。大约 5.3 的平均评分通常会导致 ICML 和 NeurIPS 做出可能采取任何一种方式的决定，这表明 ~5.3 可能被认为是接受的软阈值。  2) 置信度分数对于边界评级（例如 4（边界拒绝）、5（边界接受）和 6（弱接受））影响较小，但它们可以显着影响较强拒绝或接受案例的结果。  例如，评级为 [3, 5, 6]，置信度为 [*, 4, 4]，将“拒绝”更改为“拒绝”。置信度从 5 变为 1 会使概率从 26.2% - 31.3% - 52.4% - 54.5% - 60.4% 变化，表明在这种情况下置信度较低会增加您的机会。 相反，对于评级 [3, 5 , 7] 置信度为 [4, 4, 4] 时，接受概率为 31.3%，但当置信度变为 [4, 4, 5] 时，接受概率降至 28.1%。尽管看起来有悖常理，但置信度分数为 5 实际上会降低您的机会。一种可能的解释是，许多评分为 5 的低质量评论通常会被区域主席 (AC) 打折。 希望这会有用，并感谢 u/programmerChilli 寻求灵感！ 我还在一系列 推文。   由   提交/u/Lavishness-Mission  /u/Lavishness-Mission reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/</guid>
      <pubDate>Wed, 24 Apr 2024 12:23:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] SpaceByte：从大型语言模型中删除标记化 - 莱斯大学 2024 - 几乎与子字标记化器具有相同的性能，但没有许多缺点！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2404.14408 Github：https://github。 com/kjslag/spacebyte 摘要：  标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗性漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的&lt;字节级解码器架构缩小了字节级和子字自回归语言建模之间的性能差距。SpaceByte 由字节级 Transformer 模型组成，但在层中间插入了更大的 Transformer 块。我们发现，仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，性能显着提高。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且大致与标记化 Transformer 架构的性能相匹配。论文：https://arxiv.org/abs/2404.14408Github: https:// github.com/kjslag/spacebyteAbstract：标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的字节级解码器架构，可以缩小字节级和子字自回归语言建模之间的性能差距。 SpaceByte 由字节级 Transformer 模型组成，但在各层中间插入了更大的 Transformer 块。我们发现，仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，可以显着提高性能。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且与标记化 Transformer 架构的性能大致相当。  https://preview.redd.it/v1xo6g1gzewc1.jpg?width=1507&amp;format=p jpg&amp;自动=webp&amp;s=f9d415307b60639fa67e8a54c8769fa5a6c10f04 https://preview.redd.it/edvqos1gzewc1.jpg?width=1654&amp;format=pjpg&amp;auto=webp&amp;s=f91c8727017e1a1bc7b80bb77a8627ff99182607  https://preview.redd.it/fe6z6i1gzewc1.jpg?width=1181&amp;format=p jpg&amp; ;auto=webp&amp;s=24d955f30b8ca3eaa7c527f3f40545ed493f789c   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</guid>
      <pubDate>Wed, 24 Apr 2024 11:42:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 跟踪模型及其相关元数据。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbundt/d_keeping_track_of_models_and_their_associated/</link>
      <description><![CDATA[我开始为我正在从事的项目积累大量模型，其中许多模型都是旧模型，我为了存档而保留它们，并且许多是根据其他模型进行微调的。我想知道是否有行业标准的方法来处理这个问题，特别是我正在寻找以下内容：  有关用于训练模型的参数的信息 用于训练模型的数据集 有关模型的其他元数据（即对象检测模型训练的对象） 模型性能&lt; br /&gt; 模型沿袭（从哪个模型进行微调） 模型进展（该模型是从其他模型直接升级的吗？从相同的模型调整，但使用更好的超参数） 模型源（不确定这一点，但我正在考虑某种方法将模型链接到用于训练的 python 脚本不是很重要，但这样就很好了）  是否有任何服务工具可以帮助实现某些功能？另外，如果这不是这个问题的子问题，我可以获得一些正确方向的指示。谢谢！  ​   由   提交 /u/ClearlyCylindrical   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbundt/d_keeping_track_of_models_and_their_associated/</guid>
      <pubDate>Wed, 24 Apr 2024 10:20:35 GMT</pubDate>
    </item>
    <item>
      <title>Meta 做了 OpenAI 应该做的一切 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</link>
      <description><![CDATA[我很惊讶（或者可能没有）这么说，但 Meta（或 Facebook）比 OpenAI 更民主化 AI/ML，而 OpenAI 最初是成立并主要为此目的提供资金。 OpenAI 很大程度上已经成为一个仅以盈利为目的的商业项目。虽然就 Llama 模型而言，对我来说它们尚未达到 GPT4 功能，但我相信这只是时间问题。你们对此有何看法？   由   提交 /u/ReputationMindless32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</guid>
      <pubDate>Tue, 23 Apr 2024 22:03:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>