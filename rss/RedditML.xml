<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 16 Feb 2024 18:16:40 GMT</lastBuildDate>
    <item>
      <title>【讨论】你预测三年后谁将拥有最具统治力的英语多模态聊天助手？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1asffm2/discussion_who_do_you_predict_will_have_the_most/</link>
      <description><![CDATA[如今，OpenAI 的 ChatGPT 成为最常用的多模式聊天助手，自 2023 年发布以来一直保持着惯性。然而，资金雄厚的竞争对手毫不掩饰地表示，他们打算通过新颖的训练技术、独特的训练数据和基础设施的进步来挑战 OpenAI 目前的地位。 查看投票   由   提交 /u/kyjk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1asffm2/discussion_who_do_you_predict_will_have_the_most/</guid>
      <pubDate>Fri, 16 Feb 2024 18:13:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴模型演练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</link>
      <description><![CDATA[我真的很喜欢曼巴论文，但它对我来说这不是一本特别容易读的书，因为我之前几乎没有接触过很多先决条件材料（状态空间建模、并行扫描等）。 我写了一个解释器（链接 此处），我很好奇人们是否有任何反馈或认为它有帮助/有趣。 这在一定程度上是为了巩固我自己的理解，但也是我希望对社区有好处的事情，因为关于 Mamba 架构的教程并不多。 &lt; !-- SC_ON --&gt;  由   提交 /u/_james_chen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</guid>
      <pubDate>Fri, 16 Feb 2024 17:46:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在学校进行研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1asctcs/d_how_to_get_research_at_school/</link>
      <description><![CDATA[我就读于一所提供 AI/ML 研究的大学。我是这所大学的新生。我如何通过电子邮件告诉教授我感兴趣？我在人工智能和机器学习方面的经验也很少，但这是我想涉足的领域。   由   提交 /u/unchapped   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1asctcs/d_how_to_get_research_at_school/</guid>
      <pubDate>Fri, 16 Feb 2024 16:29:43 GMT</pubDate>
    </item>
    <item>
      <title>[N] 分享您对使用机器学习方法对子宫内膜异位症进行分类的想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1asb4tm/n_share_your_thoughts_on_my_endometriosis/</link>
      <description><![CDATA[我是机器学习的初学者。我根据患者报告的症状开发了一种诊断工具，采用逻辑回归和决策树等算法。我非常感谢社区的任何反馈、建议或贡献。请随意查看 GitHub 上的项目：https://github.com/TristanLecourtois/endodetect- based-on-symptoms/tree/main 谢谢   由   提交 /u/djdjdbsbsv   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1asb4tm/n_share_your_thoughts_on_my_endometriosis/</guid>
      <pubDate>Fri, 16 Feb 2024 15:20:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 7b 模型理论上能达到多好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as9dq2/d_how_good_can_a_7b_model_theoretically_get/</link>
      <description><![CDATA[尝试感受知识压缩的局限性。在标准基准测试中能否超越 GPT4？   由   提交/u/Z3F  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as9dq2/d_how_good_can_a_7b_model_theoretically_get/</guid>
      <pubDate>Fri, 16 Feb 2024 14:05:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最鼓舞人心/最有价值的机器学习纪录片是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as6pb3/d_what_are_the_most_inspiringvaluable_ml/</link>
      <description><![CDATA[大家好， 我正在寻找有关 ML、使用 ML 的人员以及他们面临的挑战的纪录片以及他们如何解决这些问题。不一定是最近的，10 年前的文档可能展示了 ML 和 AI 的兴起，相关人员被认为是开拓者和创新者。 我真的很喜欢 AlphaGo，尽管它主要关注实际情况节目中，我对人更感兴趣。有趣的往往会非常鼓舞人心。 谢谢。   由   提交 /u/SquidsAndMartians   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as6pb3/d_what_are_the_most_inspiringvaluable_ml/</guid>
      <pubDate>Fri, 16 Feb 2024 11:48:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] Lambda Lab vs. Mifcom vs selfbuild</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as6p8w/d_lambda_lab_vs_mifcom_vs_selfbuild/</link>
      <description><![CDATA[您如何比较在欧洲购买 ML 工作站的不同选择的质量和价格溢价？我看到三个主要选择： 1. 机器学习专家提供现成的解决方案 2. 像 mifcom 这样的硬件销售商，不专注于机器学习，但可以提供硬件预构建 3. 完全从头开始构建  &lt; p&gt;对于不同的选择，您有何看法？什么值得付出代价、风险和努力？   由   提交/u/Striking_Way_3205   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as6p8w/d_lambda_lab_vs_mifcom_vs_selfbuild/</guid>
      <pubDate>Fri, 16 Feb 2024 11:48:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] 探索 AstraQuasar-4B：一个新的基于 LLaMA 的拱门 |自层调用的首次训练实现（Duplicate Trick）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as2l18/p_discover_astraquasar4b_a_new_llamabased_arch/</link>
      <description><![CDATA[嘿r/MachineLearning， 我正在接触这个令人难以置信的社区，因为我们手上有一些独特的东西，而且它有点像未经加工的钻石。来认识一下 AstraQuasar-4B，它是一种对语言模型的全新诠释，但又有所不同——它的训练雄心勃勃，但拥有一个名为“重复”的秘密武器技巧（也在反向传播中摇摆！）。 AstraQuasar-4B 基于强大的 Phi-2 架构构建，但它不是普通模型。重复技巧是其突出的功能，可显着减少损失，并承诺未开发的稳定性和性能增强。但问题是——它训练不足。我们目前正在以相当大的规模对其进行训练，但我们正处于未知领域，尚未达到通常的基准，因为坦率地说，我们仍在弄清楚它。 它与拥抱 Face 管道，因此无需担心切换到其他训练器。 我们相信 AstraQuasar-4B 的真正价值不仅在于它现在的样子，还在于它根据您的输入可能会变成什么样子。这是对测试人员、修补者和思想家的号召。 让我们开始对话吧。分享你的想法、你的怀疑、你的想法。您将如何进行培训？你会进行什么实验？我们如何共同推动 AstraQuasar-4B 超越其当前极限？ （注：这是对合作和想法共享的真诚呼吁。没有赞助，只有纯粹、纯粹的好奇心和对人类力量的信念）社区。）   由   提交 /u/Similar_Choice_9241   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as2l18/p_discover_astraquasar4b_a_new_llamabased_arch/</guid>
      <pubDate>Fri, 16 Feb 2024 07:08:23 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 今天双降的情况</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as0i07/discussion_status_on_double_descent_today/</link>
      <description><![CDATA[双重血统的现状如何？ 当今机器学习人员对双重血统有何看法？它始于令人惊叹的人们，然后是一系列理论著作试图用线性回归和相关的简单模型来解释它，最后得出结论：最优正则化处理双下降。那么这是一个很好理解的景观吗？人们今天对此有什么思考吗？   由   提交 /u/AccomplishedTell7012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as0i07/discussion_status_on_double_descent_today/</guid>
      <pubDate>Fri, 16 Feb 2024 05:04:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 作为世界模拟器的视频生成模型。开放AI Sora技术报告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</link>
      <description><![CDATA[报告 - https ://openai.com/research/video- Generation-models-as-world-simulators   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</guid>
      <pubDate>Fri, 16 Feb 2024 01:31:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 激活的三个十年：神经网络 400 个激活函数的全面调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09092 摘要：  神经网络已被证明是解决复杂问题的高效工具生活的许多方面都存在问题。最近，随着深度学习的出现，它们的重要性和实际可用性进一步得到加强。神经网络成功的重要条件之一是选择合适的激活函数，将非线性引入模型。过去的文献中已经提出了许多类型的这些函数，但没有一个综合来源包含它们的详尽概述。即使根据我们的经验，缺乏这种概述也会导致冗余和无意中重新发现已经存在的激活函数。为了弥补这一差距，我们的论文提出了一项涉及 400 个激活函数的广泛调查，其规模比以前的调查大几倍。我们的综合汇编也参考了这些调查；然而，其主要目标是提供先前发布的激活函数的最全面的概述和系统化，并提供其原始来源的链接。第二个目标是更新当前对这一系列函数的理解。    由   提交/u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</guid>
      <pubDate>Thu, 15 Feb 2024 20:12:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI Sora Video Gen——如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</link>
      <description><![CDATA[ 介绍 Sora，我们的文本转视频模型。 Sora 可以生成长达一分钟的视频，同时保持视觉质量并遵守用户的提示。  https:/ /openai.com/sora 研究笔记 Sora 是一种扩散模型，它从看起来像静态噪声的视频开始生成视频，然后通过多个步骤消除噪声来逐渐对其进行转换. Sora 能够一次生成整个视频或扩展生成的视频以使其更长。通过一次为多个帧提供模型预测，我们解决了一个具有挑战性的问题，即确保主题即使暂时离开视野也保持不变。 与 GPT 模型类似，Sora 使用变压器架构，释放卓越的扩展性能。 我们将视频和图像表示为称为补丁的较小数据单元的集合，每个补丁类似于 GPT 中的令牌。通过统一我们表示数据的方式，我们可以在比以前更广泛的视觉数据上训练扩散变换器，涵盖不同的持续时间、分辨率和纵横比。 Sora 建立在 DALL·E 和 DALL·E 过去的研究基础上GPT 模型。它使用 DALL·E 3 的重述技术，该技术涉及为视觉训练数据生成高度描述性的标题。因此，该模型能够更忠实地遵循生成视频中用户的文本指令。 除了能够仅根据文本指令生成视频之外，该模型还能够采用现有的静态图像并从中生成视频，精确地动画图像内容并关注小细节。该模型还可以获取现有视频并对其进行扩展或填充缺失的帧。在我们的技术论文（今天晚些时候发布）中了解更多信息。 Sora 是能够理解和模拟现实世界的模型的基础，我们相信这一功能将成为实现 AGI 的重要里程碑。 Sora 是能够理解和模拟现实世界的模型的基础。 p&gt; 示例视频：https://cdn.openai.com/sora/videos/ cat-on-bed.mp4 技术论文将于今天晚些时候发布。但是如何进行头脑风暴呢？   由   提交/u/htrp  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</guid>
      <pubDate>Thu, 15 Feb 2024 18:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1M/10M token上下文窗口怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</link>
      <description><![CDATA[是否会启动社区头脑风暴主题？ - 人们是否认为 RingAttention 可以充分扩展？参见https://largeworldmodel.github.io - 它是用 1M 还是 10Mn 令牌窗口进行训练的，这对我来说似乎不清楚？他们是否在没有经过某种训练的情况下从 1M-&gt;10M 进行概括？ - 存在哪些数据集可以训练 10M 文本标记窗口？ - 在这么长的背景下你如何做 RLHF？ 1M 文本 ~ 4M 字符 ~ 272k 秒阅读时间（根据 Google 假设 68 毫秒/字符）~ 阅读一个示例需要 75 小时？ 编辑：当然 lucidrains 已经在着手实施 RingAttention！ (https://github.com/lucidrains/ring-attention-pytorch)   由   提交 /u/gggerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</guid>
      <pubDate>Thu, 15 Feb 2024 16:13:29 GMT</pubDate>
    </item>
    <item>
      <title>[N] Gemini 1.5，具有 1M 上下文长度令牌的 MoE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</link>
      <description><![CDATA[https://blog.google/technology/ai/google-gemini-next- Generation-model-february-2024/  &amp;# 32；由   提交/u/Electronic-Author-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</guid>
      <pubDate>Thu, 15 Feb 2024 15:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>