<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Mon, 29 Apr 2024 12:25:54 GMT</lastBuildDate>
    <item>
      <title>[D] 序列模型不擅长预测我自己的笔迹？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfxe76/d_sequential_model_bad_at_predicting_my_own/</link>
      <description><![CDATA[大家好， 我在张量流中创建了一个顺序模型来预测手写数字，精度为 0.96。在 tf 提供的数据集上，它表现得很好，但如果我尝试根据自己的手写数字进行预测，它总是输出 8。 我已将照片转换为灰度，将其大小调整为 28 x 28，转换到一个数组，并重新调整它的形状。关于为什么它不起作用有什么想法吗？谢谢！ https://colab.research.google.com/drive/1iv7VlPnWxHkbxZCuWucxd3mcNP23UbtG?usp=分享   由   提交/u/blackrat13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfxe76/d_sequential_model_bad_at_predicting_my_own/</guid>
      <pubDate>Mon, 29 Apr 2024 12:04:02 GMT</pubDate>
    </item>
    <item>
      <title>无法在翻译项目中取得好的结果 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfwoxd/not_able_to_achieve_good_results_on_a_translator/</link>
      <description><![CDATA[嗨，我正在尝试实现论文“注意力就是你所需要的 Transformer” https://arxiv.org/pdf/1706.03762 我从头开始开发了代码 https://cocalc.com/share/public_paths/9e30abf60b9ee21283e937455bf720622bf3c10d 用于英语到法语的翻译，但经过大约 6-7 小时的训练后，我无法获得完美的翻译，它确实给出了语法正确的法语句子，但不是正确的翻译。我现在该怎么办，任何帮助都会有所帮助。   由    /u/Shoddy_Battle_5397  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfwoxd/not_able_to_achieve_good_results_on_a_translator/</guid>
      <pubDate>Mon, 29 Apr 2024 11:25:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果我错了请纠正我，NLP 使用 KL 散度，CV 使用 MMD。两者都在测量两个分布的相似性/距离</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfv1re/d_correct_me_if_im_wrong_use_kl_divergence_for/</link>
      <description><![CDATA[我现在正在做实例选择，经过快速研究，我发现使用 MMD 而不是 KL 散度的作品较少。在这两个领域中选择距离度量是否存在偏好？这背后的原因是什么？ THX   由   提交 /u/VoiceBeer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfv1re/d_correct_me_if_im_wrong_use_kl_divergence_for/</guid>
      <pubDate>Mon, 29 Apr 2024 09:44:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自动机理论在机器学习中的运用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfuxq3/d_use_of_automata_theory_in_machine_learning/</link>
      <description><![CDATA[我听说自动机理论和形式化 la 量规在验证协议和评估问题复杂性方面很有好处，但 AI 和 LLM 能否从这些有限自动机模型中受益？    提交人    /u/canyonkeeper   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfuxq3/d_use_of_automata_theory_in_machine_learning/</guid>
      <pubDate>Mon, 29 Apr 2024 09:36:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 VisionPro 的新型远程操作工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfrgx9/r_new_teleoperation_tool_with_visionpro/</link>
      <description><![CDATA[       由   提交/u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfrgx9/r_new_teleoperation_tool_with_visionpro/</guid>
      <pubDate>Mon, 29 Apr 2024 05:40:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动态高斯网格</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfraht/r_dynamic_gaussians_mesh/</link>
      <description><![CDATA[       由   提交/u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfraht/r_dynamic_gaussians_mesh/</guid>
      <pubDate>Mon, 29 Apr 2024 05:29:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 2024结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfm9ep/d_icml_2024_results/</link>
      <description><![CDATA[大家好， ICML 决定即将公布！ 我正在创建一个帖子对于有兴趣分享的每个人：  关于结果/评审过程的想法 已接受论文中有趣的统计数据和趋势 有关当前研究趋势的讨论&lt; /li&gt; 集思广益，讨论将在会议上展示的新颖作品（您最喜欢哪一个？:)） （对于那些参加的人）在维也纳举行的 ICML 休闲聚会！  祝大家好运！   由   提交/u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfm9ep/d_icml_2024_results/</guid>
      <pubDate>Mon, 29 Apr 2024 00:57:27 GMT</pubDate>
    </item>
    <item>
      <title>要赢得 ML 黑客马拉松，除了 ML 之外，你还需要一切 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cficmp/you_need_everything_other_than_ml_to_win_a_ml/</link>
      <description><![CDATA[基本上是对我的大型跨国公司和机构主办的线下黑客马拉松条件的咆哮。  厌倦了参加旨在“开发尖端解决方案”的黑客马拉松，最终输给了一个从未学过机器学习但精通“商业信息学”的人并且在给定的时间内提出解决方案非常好。  一个头脑清醒的人，在创意、原型和模型上不停地工作 2-3 天，怎么可能只谈论 3-5 分钟呢？我确实看到人们克隆了与问题陈述有些相关的 github 存储库，并将其像某种最先进的产品一样出售。我同意这种技能在工业中更重要，但为什么将这些黑客马拉松命名为“机器学习”呢？或“AI”黑客马拉松？最好将其命名为“卖给我一些垃圾”。 对于那些真正想在有限的时间内开发好产品、工作模型以及喜欢竞争的人（像我一样）的人来说，唯一的选择就是在线参与或在“数据”中竞赛。    由   提交 /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cficmp/you_need_everything_other_than_ml_to_win_a_ml/</guid>
      <pubDate>Sun, 28 Apr 2024 21:56:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 RETRO 不是法学硕士中的主流/最先进的技术？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cffgkt/d_why_isnt_retro_mainstream_stateoftheart_within/</link>
      <description><![CDATA[2021 年，Deepmind 发布了通过检索数万亿个数据来改进语言模型令牌并引入了检索增强型变压器（RETRO）。 RAG 通常涉及通过将相关文档注入上下文来在推理时补充输入标记，而 RETRO 可以在训练和推理期间从外部数据库访问相关嵌入。目标是将推理和知识解耦：通过允许按需查找，模型可以不必记住其权重内的所有事实，而是将能量重新分配给更有影响力的计算。结果非常惊人：RETRO 实现了与 GPT-3 相当的性能，参数减少了 25 倍，并且理论上没有知识截止（只需向检索数据库添加新信息！）。 然而：今天，AFAICT ，大多数主要型号都不包含 RETRO。 LLaMA 和 Mistral 当然不会，而且我也不觉得 GPT 或 Claude 会这样做（唯一可能的例外是 Gemini，因为 RETRO 团队的大部分成员现在都是 Gemini 团队的一部分，而且它根据我的经验，更快、更实时）。此外，尽管 RAG 一直很热门，并且有人可能认为教育部能够实现它，但作为一种研究向量，明确地解耦推理和知识一直相对安静。 有人对为什么会这样有一个自信的解释吗？我觉得 RETRO 的这一伟大而高效的前沿进步就在眼前，等待着广泛采用，但也许我错过了一些明显的东西。   由   提交/u/whitetwentyset  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cffgkt/d_why_isnt_retro_mainstream_stateoftheart_within/</guid>
      <pubDate>Sun, 28 Apr 2024 19:58:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个云提供商，提供经济实惠且轻松的 GPU 访问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfek96/p_i_created_a_cloud_provider_for_affordable_easy/</link>
      <description><![CDATA[您好r/MachineLearning！ 我很高兴推出 Backprop GPU 云 - 在公共市场上托管 GPU 服务器三年后，我决定构建自己的云提供更好的服务。 我关注的是速度、价格和可靠性： - 实例是在 60 秒内创建的，并预装了 Jupyter。 - 定价合理，没有存储或带宽方面的隐藏费用。 - RTX 3090 实例托管在具有 10 Gbps 网络的三级数据中心。 如果您是学生或研究人员，我很乐意为您提供 10 小时的免费积分。只需注册并向我发送消息即可。 我希望添加更多功能和其他实例类型。所有反馈都会有很大帮助！   由   提交 /u/ojasaar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfek96/p_i_created_a_cloud_provider_for_affordable_easy/</guid>
      <pubDate>Sun, 28 Apr 2024 19:21:09 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 直观地深入了解 Uber 用于预测预计到达时间的机器学习解决方案。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfd15u/research_a_visual_deep_dive_into_ubers_machine/</link>
      <description><![CDATA[      TL;DR：Uber 遵循 2 层方法。他们将 Dijkstra 等传统图算法与学习嵌入和轻量级自注意力神经网络相结合，以可靠地预测预计到达时间或预计到达时间。 Uber 如何使用机器学习来预测预计到达时间（并解决十亿美元的问题）&lt; /p&gt; https://preview.redd。 it/2ovttr82i9xc1.png?width=1358&amp;format=png&amp;auto=webp&amp;s=51b12261bf98f529fd0e9b33daf6362b727f4580   由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfd15u/research_a_visual_deep_dive_into_ubers_machine/</guid>
      <pubDate>Sun, 28 Apr 2024 18:18:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 分类深度学习：架构的代数理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cfck8b/r_categorical_deep_learning_an_algebraic_theory/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.15332 项目页面：https://categoricaldeeplearning.com / 摘要：  我们提出了我们的立场，即寻找一个难以捉摸的通用框架来指定和研究深度学习学习架构。我们的观点是，迄今为止所做的关键尝试在指定模型必须满足的约束和指定其实现之间缺乏连贯的桥梁。着眼于建立这样一座桥梁，我们建议应用范畴论——准确地说，是在参数映射的 2 类别中评估的通用单子代数——作为单一理论优雅地包含了神经网络设计的这两种风格。为了捍卫我们的立场，我们展示了该理论如何恢复几何深度学习引起的约束，以及从不同的神经网络（例如 RNN）中提取的许多架构的实现。我们还说明了该理论如何自然地编码计算机科学和自动机理论中的许多标准结构。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cfck8b/r_categorical_deep_learning_an_algebraic_theory/</guid>
      <pubDate>Sun, 28 Apr 2024 17:59:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您如何诊断训练损失中的这些峰值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf4gw9/d_how_would_you_diagnose_these_spikes_in_the/</link>
      <description><![CDATA[   /u/NumberGenerator  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf4gw9/d_how_would_you_diagnose_these_spikes_in_the/</guid>
      <pubDate>Sun, 28 Apr 2024 11:44:29 GMT</pubDate>
    </item>
    <item>
      <title>“变形金刚可以使用无意义的填充标记（例如，‘......’）来代替一连串的思想” - Let's Think Dot by Dot [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cf2u0d/transformers_can_use_meaningless_filler_tokens_eg/</link>
      <description><![CDATA[https://arxiv.org/abs/2404.15758 从摘要开始 我们表明，变压器可以使用无意义的填充标记（例如“......”）来代替一系列思想来解决两个问题在没有中间令牌的情况下进行响应时，他们无法解决困难的算法任务。然而，我们根据经验发现，学习使用填充令牌很困难，需要特定的、密集的监督才能收敛   由   提交 /u/Agitated_Space_672   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cf2u0d/transformers_can_use_meaningless_filler_tokens_eg/</guid>
      <pubDate>Sun, 28 Apr 2024 09:59:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>