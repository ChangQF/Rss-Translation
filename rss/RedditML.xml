<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 20 Mar 2024 00:56:29 GMT</lastBuildDate>
    <item>
      <title>[D] 3D 点云上的 Conv2D</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bizyno/d_conv2d_on_3d_point_clouds/</link>
      <description><![CDATA[嗨！ 我正在使用点云回归进行姿态预测。 原始实现PointNet、PointNet++、DGCNN 等的每个卷积层都使用 Conv2D 运算。 我的问题是：为什么在这种情况下使用 2D 卷积而不是 1D 卷积？这样做的原因和优点是什么？ 提前致谢！   由   提交 /u/Professional-Act-163   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bizyno/d_conv2d_on_3d_point_clouds/</guid>
      <pubDate>Wed, 20 Mar 2024 00:16:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 通过融合张量运算流实现无静态图的最佳性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biy9aj/p_optimal_performance_without_static_graphs_by/</link>
      <description><![CDATA[当前机器学习研究最重要的方面之一是发现可有效扩展计算资源的模型架构。由于有效利用当代硬件，变压器已成为主要架构。然而，它们不会根据任务的复杂性来调整计算图，因此需要针对不同复杂性的任务使用不同的版本。这种方法与拥有一个能够持续学习（终身学习）的模型同时保持简单任务高效的目标不符。我认为迫切需要进一步探索动态架构，其中计算图在运行时根据上下文线索进行调整。 虽然 BranchyNet 等几篇论文已经深入研究了这种方法，有选择地跳过层当以高置信度生成令牌时，它们的实现可能不如 JAX 中实现的标准静态转换器那么高效或优化。尽管如此，将基于静态图的框架的速度与 PyTorch 之类的灵活性相结合不是很好吗？这正是我最近在 Burn 上所做的工作。 Burn 是一个急切的框架，具有独特的特征：它是用 Rust 编写的，大量使用类型系统来捕获张量动态生命周期，生成高度优化的带有即时编译器的 GPU 内核。 PyTorch 还尝试创建一个即时编译器来捕获部分图，但结果并不那么乐观，最近的多后端 Keras 3 基准测试就证明了这一点。因此，在 PyTorch 中实现高性能可能需要许多自定义内核。不幸的是，处理高级概念、数学公式和理论通常需要进入 CUDA 和非常低级的编程才能有效地进行经验测试。 Burn 的愿望是使研究和应用成为可能。快速部署最灵活的架构，无需 GPU 编程即可实现最先进的性能。虽然在获得最快框架的称号之前还需要做更多的工作，但我们正在不断添加更多优化，并且基础已经稳固。 博客：https://burn.dev/blog/fusion-tensor-operation-streams/ 代码：https://github.com/tracel-ai/burn/tree/main/crates/burn-fusion&lt; /p&gt;   由   提交/u/ksyiros  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biy9aj/p_optimal_performance_without_static_graphs_by/</guid>
      <pubDate>Tue, 19 Mar 2024 23:03:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML 项目想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bixeks/d_ml_project_ideas/</link>
      <description><![CDATA[大家好，我目前正在攻读人工智能硕士学位，虽然课程很有趣，但我发现作业有点简单，并怀疑他们的能力给潜在雇主留下深刻印象。因此，我决定开展自己的项目，但我在产生想法方面遇到了障碍（我承认，创造力不是我的最强项）。 我一直在考虑各种途径，例如在 llama-2-7b 上进行实验，探索其调优潜力，甚至构建 API。但是，我很犹豫，正在寻求社区的意见。 只是为了提供一些背景知识，我精通 Python，对 PyTorch、HuggingFace、Langchain 和其他默认 ML 框架有很好的了解(np 、pd 等）以及 Azure。但是，我必须提到，我的硬件设置相对适中，我可能需要为需要大量计算能力的项目租用 GPU。 我愿意接受建议，并且非常感谢您的任何想法或指导。可能要提供。预先感谢您！   由   提交 /u/Wild-Positive-6836   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bixeks/d_ml_project_ideas/</guid>
      <pubDate>Tue, 19 Mar 2024 22:28:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] TacticAI：足球战术人工智能助手</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biw8qn/r_tacticai_an_ai_assistant_for_football_tactics/</link>
      <description><![CDATA[博客文章：https://deepmind.google/discover/blog/tropicai-ai-assistant-for-football-tropics/ 自然论文：https://www.nature.com/articles/s41467-024-45965-x 摘要 识别对手球队实施的关键战术模式并制定有效的应对措施是现代足球的核心。然而，通过算法实现这一点仍然是一个开放的研究挑战。为了解决这一未满足的需求，我们提出了 TacticAI，这是一款与利物浦足球俱乐部领域专家密切合作开发和评估的人工智能足球战术助手。我们专注于分析角球，因为它们为教练提供了最直接的干预和改进机会。 TacticAI 结合了预测和生成组件，使教练能够有效地采样和探索每个角球例程的替代球员设置，并选择那些预测成功可能性最高的球员。我们在许多相关基准任务上验证 TacticAI：预测接球手和投篮尝试以及建议球员位置调整。 TacticAI 的实用性已通过利物浦足球俱乐部足球领域专家进行的定性研究得到验证。我们表明，TacticAI 的模型建议不仅与真实战术无法区分，而且在 90% 的情况下比现有战术更受青睐，并且 TacticAI 提供了有效的角球检索系统。尽管黄金标准数据的可用性有限，但 TacticAI 通过几何深度学习实现了数据效率，从而实现了这些成果。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biw8qn/r_tacticai_an_ai_assistant_for_football_tactics/</guid>
      <pubDate>Tue, 19 Mar 2024 21:42:51 GMT</pubDate>
    </item>
    <item>
      <title>训练分割非常慢[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bitvtz/training_split_very_slow_r/</link>
      <description><![CDATA[      大家好！我正在维基百科数据集上微调 mbert，加载了数据集（拥抱脸） https://preview.redd.it/kaw1kmrtjcpc1.png?width=1944&amp;format=png&amp;auto=webp&amp;s=818740eebd72dc13ad35a30b384a122b79 866f3c 这个是提交到 Slurm 的 bash 脚本（删除了不相关的行）：    /bin/bashLANG=ka &lt; /blockquote&gt;  #SBATCH -o .../examples/language-modeling/slogs/sl_ka1_%A.out #SBATCH - e .../examples/language-modeling//slogs/sl_ka1_%A.out #SBATCH -N 1 # 个请求的节点 &lt; p&gt;#SBATCH -n 1 # 请求的任务 #SBATCH --gres=gpu:8 # 使用 1 个 GPU #SBATCH --mem=60000 # 内存（以 Mb 为单位） #SBATCH --partition=PGR-Standard &lt; code&gt;#SBATCH -t 24:00:00 # 请求的时间（以小时:分钟:秒为单位） #SBATCH --cpus-per-task=16 # 要使用的 cpu 数量使用 - 每个节点上有 32 个 torchrun --nproc_per_node 8 run_mlm.py \ --model_name_or_path bert-base-multilingual-cased \ --cache_dir **${CACHE_HOME}**2\ --dataset_lang ${LANG} \ --dataset_name 维基百科 \  --output_dir ${OUTPUT_DIR} \ --do_train \ --do_eval \ --per_device_train_batch_size 4 \ --per_device_eval_batch_size 4 \ --gradient_accumulation_steps 2 \ --max_seq_length 256 \ - -overwrite_output_dir \ --ft_params_num 7667712 \ --evaluation_strategy 步骤 \ --eval_steps 1000 \ --dataloader_num_workers 32 \ --preprocessing_num_workers 32 \ --validation_split_percentage 5 \ --load_best_model_at_end \ --save_total_limit 2 &gt;  这是速度报告，非常慢。  正在下载数据：100%|████████ ██| 14.1k/14.1k [00:00&lt;00:00, 14.0MB/s] 下载数据：100%|██████████| 205M/205M [00:44&lt;00:00, 4.59MB/s]  生成列车分割：0 个示例 [00:00, ? example/s]从.../language-modeling/cache_directory22/downloads/f797c17d35d578a4c1a3f251847095789ec04ae453f10623aeb8366ff4797a07中提取内容 生成列车分割：170787个示例[17:57, 158.45个示例/s]  提前非常感谢您   由   提交 /u/Choricius   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bitvtz/training_split_very_slow_r/</guid>
      <pubDate>Tue, 19 Mar 2024 20:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 在每层使用相同维度的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</link>
      <description><![CDATA[我的直觉是，随着我们在层中移动，令牌会逐渐丰富，但这意味着我们需要在每个令牌中存储更少的信息前面的层比后面的层要多。 从（相对）低维嵌入开始，然后将它们投影或扩展到更高的维度，直到它们达到最终大小，这不是有意义吗？    由   提交/u/timtom85  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</guid>
      <pubDate>Tue, 19 Mar 2024 19:35:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 2024 讨论主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/</link>
      <description><![CDATA[这篇文章用于讨论与 ICML 2024 相关的任何内容，评论将于明天发布！祝所有参与者好运！   由   提交/u/condom-mechanics  /u/condom-mechanics  reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/</guid>
      <pubDate>Tue, 19 Mar 2024 18:29:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向白盒深度学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biq9pc/r_towards_white_box_deep_learning/</link>
      <description><![CDATA[我提出了一个用于构建本质上可解释的神经网络的概念框架。 MNIST 子问题的 PoC 4 层模型可以被视为白盒：决策边界很容易解释，并且该模型对对抗性攻击具有鲁棒性 - 尽管没有任何形式的对抗性训练！ 该方法本质上是简化为如何在网络层内共享权重以实现高度可解释和鲁棒的特征的一般概念。它的一般性质和有效性表明，对于更复杂的数据集和不同的模式应该有可能获得类似的结果 - 这为进一步研究开辟了令人兴奋的领域！ 迫不及待想听到您的反馈！  p&gt; https://arxiv.org/abs/2403.09863   由   提交/u/Swarzkopf314  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biq9pc/r_towards_white_box_deep_learning/</guid>
      <pubDate>Tue, 19 Mar 2024 17:42:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我如何在 Google Gemma 6T 代币模型中发现 8 个错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</link>
      <description><![CDATA[      嘿 r/MachineLearning！也许您可能在 Twitter 上看到过我发帖，但如果您不知道 8 个错误，我就在这里发帖在 Google Gemma 的多个实现中:) 修复应该已经被推送到 HF 的 Transformers 主分支中，而 Keras、Pytorch Gemma、vLLM 应该已经得到修复:) https://github.com/huggingface/transformers/pull/29402 通过比较5个实现，我发现了以下问题：  必须添加否则损失会很大。 技术报告中的模型有一个拼写错误！ sqrt(3072)=55.4256，但 bfloat16 是 55.5。  Layernorm (w+1) 必须采用 float32。 Keras mix_bfloat16 RoPE 错误。 RoPE 对 y*(1/x) 与 y/x 敏感。 RoPE 对 y*(1/x) 与 y/x 敏感。 &gt; RoPE 应该是 float32 - 已经推送到变压器 4.38.2。 GELU 应该是近似 tanh，而不是精确的。  添加所有这些更改允许 Log L2 Norm 从红线减少到黑线（越低越好）。请记住这是对数刻度！所以误差从 10_000 减少到现在的 100 - 系数 100！这些修复主要针对长序列长度。 https://preview.redd.it/cocy1pknrbpc1.jpg?width=878&amp;format=pjpg&amp;auto=webp&amp;s=8e837bf2a62726c24540981fae6c409d2681ece7 最引人注目的是添加 BOS 代币微调运行可以在开始时抑制训练损失。没有BOS会导致损失变得非常高。 https://preview.redd.it/zkcjyfcorbpc1.jpg?width=1075&amp;format=pjpg&amp;auto=webp&amp;s=0925192d49a5e30a527f4235ccb006abf2670205 另一个非常有问题的问题是 RoPE 嵌入在 bfloat16 而不是 float32 中完成。这破坏了非常长的上下文长度，因为 [8190, 8191] 被升级为 [8192, 8192]。这破坏了非常长的序列长度上的微调。 https://preview.redd.it/ozd6agusrbpc1.png?width=798&amp;format=png&amp;auto=webp&amp;s=64ba374acc0bfbe35d92dd4668d302c780c32d19 另一个主要问题是几乎所有实现，除了JAX 类型使用精确的 GELU，而近似 GELU 是正确的选择： https://preview.redd.it/7mhfb7tvrbpc1.png?width=592&amp;format=png&amp;auto=webp&amp;s=7db88b61236205f6f882c1d2f5bb8f8 2b48f63ef 我还有一个关于修复的 Twitter 帖子：https://twitter.com/danielhanchen/status/1765446273661075609，以及完整的Colab 笔记本介绍了更多问题：https://colab.research.google.com/drive/1fxDWAfPIbC -bHwDSVj5SBmEJ6KG3bUu5?usp=sharing 还有一篇较长的博客文章：https://unsloth.ai/blog/gemma-bugs&lt; /a&gt; 我还使 Gemma 微调速度提高了 2.5 倍，在 Colab 笔记本中使用的 VRAM 也减少了 60%：https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing 还有一场价值 5 万美元的 Kaggle 竞赛https://www.kaggle.com/competitions/data-assistants-with-gemma 专门针对 Gemma :) &lt; !-- SC_ON --&gt;  由   提交 /u/danielhanchen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</guid>
      <pubDate>Tue, 19 Mar 2024 17:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] [R] 精心策划和协调的多研究妊娠期阴道微生物组数据集，适用于 AI/ML 模型的训练和验证</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biprfm/p_r_a_curated_and_harmonized_multistudy_vaginal/</link>
      <description><![CDATA[您好！我是最近的众包 AI/ML 研究的主要作者，该研究旨在识别有早产和早产风险的怀孕 -我想与阴道微生物组数据分享精心策划的阴道微生物组数据训练和验证数据集更广泛的人工智能/机器学习社区。 我认为对于那些对微生物组数据训练模型感兴趣的人来说，这可能是一个宝贵的资源。我也很高兴回答您关于如何组装、协调这些数据以及所提供的各种特征的生物学意义的问题，以及如何对其他基于 16S rRNA 基因的微生物组数据（有超过 200,000 个样本）进行类似的协调在公共数据库中测序）。 数据集由 7 项独立进行的研究组成怀孕期间的阴道微生物组，涵盖 764 次怀孕和 2226 个样本；额外的样本正在等待 dbGAP 的批准。为每个样本精心策划关键元数据（采集妊娠周；分娩妊娠周；NIH 种族/民族类别（如果有）等）。 微生物组数据的协调存在重大挑战，特别是这些数据是通过 16S rRNA 基因可变区的扩增产生的，其中每项研究都使用不同的技术方法。该数据集的组装基于我的研究小组的一种新颖的协调方法，那些寻求使用微生物组数据的人也可能会对此感兴趣。 tldr：以下是一些严格协调的阴道微生物组数据怀孕。如果您只想从一个矩阵开始，请使用 0.5 距离处的系统发育型表（计数或相对丰度）。   由   提交/u/golob  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biprfm/p_r_a_curated_and_harmonized_multistudy_vaginal/</guid>
      <pubDate>Tue, 19 Mar 2024 17:21:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] 带注释的曼巴：艰难的道路</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biisb2/p_annotated_mamba_the_hard_way/</link>
      <description><![CDATA[链接：https://srush .github.io/annotated-mamba/hard.html 代码：https://github .com/srush/annotated-mamba 来自作者：  此博客是关于Mamba 一种最新的神经架构，可以粗略地认为是现代循环神经网络（RNN）。该模型运行得非常好，是无处不在的 Transformer 架构的合法竞争对手。它已经引起了很多关注。  我原本打算写一篇关于整篇论文的博文，内容相当密集且富有洞察力。然而，我只是对此处描述的 S6 算法着迷。该算法描述了如何在现代硬件上有效计算极大的 RNN，并扩展了 S4 和 近年来的S5。  事实上，如果我说实话，我实际上只了解了算法的这一行： y = SSM(A, B, C)( x) # 随时间变化：仅重复(扫描)  这行代码很有趣，我想，嘿，难道没有人能够理解为什么这种扫描在实践中速度很快吗？  事实证明这有点棘手。但是，如果您阅读这篇博文，我可以向您保证，您会理解这句话。 （也许比您想要的更多）。  第 0 部分：Triton  第 1 部分：累积和  第 2 部分：指数移动平均线  第 3 部分：获取导数  p&gt; 第 4 部分：同时多个  第 5 部分：Mamba  ​   由   提交 /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biisb2/p_annotated_mamba_the_hard_way/</guid>
      <pubDate>Tue, 19 Mar 2024 12:14:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] NVIDIA GTC 2024 公告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bif6ey/d_nvidia_gtc_2024_announcements/</link>
      <description><![CDATA[NVIDIA 的计划已遍及加速计算、生成式 AI、行业应用、汽车、企业平台、Omniverse 和机器人领域。 其中一些最有趣的是：  DRIVE Thor：用于自动驾驶汽车中的生成式人工智能应用的车载计算平台。它每秒执行高达千万亿次操作，增强了自动驾驶的安全性，并支持与车辆的交互式对话。 Omniverse：融合物理和虚拟世界的数字孪生生态系统，帮助行业模拟、优化和识别更有效地执行操作。新的 Omniverse Cloud API 扩展了这些功能，使汽车和机器人等行业受益。 GR00T 项目：推动机器人和人工智能突破的人形机器人的基础模型。此外，还推出了 Jetson Thor 计算机，并升级至 NVIDIA Isaac™ 机器人平台，其中包含生成式 AI 模型和模拟工具。 Nvidia Blackwell GPU：一项尖端技术，旨在以 20 petaflops 的速度为下一代 AI 提供动力的性能。该GPU代表了人工智能能力的巨大飞跃，旨在实现万亿参数模型的民主化。 NVLink Switch 7.2 TI：新一代互连技术，可解决数据交换的瓶颈。它旨在促进 GPU 之间的通信，其规模适合最先进的 AI 模型。 NVIDIA NIM：一款新软件产品，旨在简化企业环境中生成式 AI 的部署。它将模型与优化的推理引擎打包在一起，并支持广泛的 GPU 架构。他们称其为所有人的人工智能包。  你最喜欢哪个？   由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bif6ey/d_nvidia_gtc_2024_announcements/</guid>
      <pubDate>Tue, 19 Mar 2024 08:13:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia GTC24 的 GPT4 参数计数与我们从 Semianalysis 获得的泄漏相同</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bi16pg/d_same_param_count_for_gpt4_from_nvidia_gtc24_as/</link>
      <description><![CDATA[   A Semianalysis 早前的报告称 GPT-4 是一个 1.8T 参数的 MoE 模型，有 16 位专家，每个有 111B 个参数。这是 GTC 会议的屏幕截图，具有相同的数字。 https://preview.redd.it/vyzfx2sel5pc1.png?width=1764&amp;format=png&amp;auto=webp&amp;s=dfce1d55c84dbc3c51e69f376161c47958f9cf 70   由   提交 /u/takuonline   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bi16pg/d_same_param_count_for_gpt4_from_nvidia_gtc24_as/</guid>
      <pubDate>Mon, 18 Mar 2024 20:36:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当你使用人工智能进行总结时结果不正确时。爱思唯尔发表的研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhn918/d_when_your_use_of_ai_for_summary_didnt_come_out/</link>
      <description><![CDATA[       由   提交 /u/vvkuka   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhn918/d_when_your_use_of_ai_for_summary_didnt_come_out/</guid>
      <pubDate>Mon, 18 Mar 2024 10:14:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>