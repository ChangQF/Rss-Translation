<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 21 Oct 2024 21:15:15 GMT</lastBuildDate>
    <item>
      <title>[R] 基于 RoPE 的 LLM 如何学习注意力集中点（或编码绝对位置）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/</link>
      <description><![CDATA[我最近重新阅读了“注意力接收器”论文（链接），并开始思考 LLM 如何管理注意力接收器。 注意力接收器的概念描述了 LLM 为初始标记分配不成比例的高注意力分数的现象，而不管它们的语义值如何。 这里有一个悖论：最先进的开放式 LLM 通常采用 RoPE（旋转位置嵌入）进行位置编码。由于 RoPE 仅对相对位置进行编码，因此令人费解的是模型如何一致地识别绝对初始标记并为其分配高度注意力。您对这种行为可能如何出现或解释有什么想法吗？    提交人    /u/StraightSpeech9295   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/</guid>
      <pubDate>Mon, 21 Oct 2024 19:46:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 修复 Nightly transformers 中的梯度累积错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/</guid>
      <pubDate>Mon, 21 Oct 2024 19:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]我是一名 Btech 学生 (CSE)，我想学习机器学习，但我不知道从哪里开始，也没有任何资源。所以我真的需要建议或任何形式的帮助</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/</link>
      <description><![CDATA[我现在是第 5 个学期，请帮帮我     提交人    /u/Glittering-Tell-8963   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/</guid>
      <pubDate>Mon, 21 Oct 2024 18:36:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 讨论提取数据和存储数据的最佳方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8u1ui/d_discussion_on_the_best_ways_to_extract_data_and/</link>
      <description><![CDATA[嗨，我正在做一个与肿瘤 MRI 图像相关的项目。首先，我分析这些图像并对它们进行分割，但如何将图像中有关肿瘤性质的信息转换为可用于编写患者医疗报告的数据。数据的分类是什么？结构化还是类结构化或非结构化如何使用这些数据来编写报告。谢谢    提交人    /u/mse9090   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8u1ui/d_discussion_on_the_best_ways_to_extract_data_and/</guid>
      <pubDate>Mon, 21 Oct 2024 16:33:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2024 焦点中的潜在抄袭：Shengjie Luo 和 Tianlang Chen 的“Gaunt Tensor Products”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8rk2j/d_potential_plagiarism_in_iclr_2024_spotlight/</guid>
      <pubDate>Mon, 21 Oct 2024 14:52:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] RWKV-7：无需注意，超越强大的 Modded-GPT 基线（使用 Muon 优化器的基线），同时仅使用 headsz 64</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</link>
      <description><![CDATA[      大家好。 RWKV-7（100% RNN 且无注意力）可以超越强大的 Modded-GPT 基线（带有 Muon 优化器的基线，目前在推特上流行）。 训练代码和日志：https://github.com/BlinkDL/modded-nanogpt-rwkv 如果使用更大的 headsz，它可以达到损失 3.26xx。 但是我当前的实现效率很低。优化后，可能可以达到 ctx1k 下 Modded-GPT 速度的 85%（或比 ctx4k 下 Modded-GPT 更快）。欢迎任何帮助:) https://preview.redd.it/48m3lsvkb4wd1.png?width=873&amp;format=png&amp;auto=webp&amp;s=647d86ed47d40a4f742ed9512a835dee41069e4f  强大的 GPT 基线： https://preview.redd.it/h2ckr31mb4wd1.png?width=584&amp;format=png&amp;auto=webp&amp;s=b667bfbc50298f8335a889b85c55f68ee8db38a5  RWKV-7 摆脱了“线性注意力”设计以实现更高的性能：） https://preview.redd.it/ijyz0sgnb4wd1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=f413d0e7bcd3a76c5e788f2ca231a37706b24345    提交人    /u/bo_peng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/</guid>
      <pubDate>Mon, 21 Oct 2024 14:18:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] AISTATS 会进行‘修改后接受’吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/</link>
      <description><![CDATA[AISTATS 会“接受修改”吗？ 我是第一次来的学生作者。提交了我的草稿，但我觉得我可以通过在这里和那里进行调整来进一步改进草稿（甚至在附录中添加几个新部分）。 这在反驳阶段可行吗？还是他们只允许我进行外观上的更改？我听说在其他一些会议上可以进行更改，但不确定 AISTATS 是否允许。 谢谢！    提交人    /u/confirm-jannati   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/</guid>
      <pubDate>Mon, 21 Oct 2024 14:11:49 GMT</pubDate>
    </item>
    <item>
      <title>[研究] AAAI 第二阶段审稿人何时需要提交审稿意见？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/</link>
      <description><![CDATA[我有一个关于 AAAI 第 2 阶段审查流程的快速问题。具体来说，审稿人什么时候需要提交他们的评论？ 我正在考虑将我的论文提交给 arXiv，但我听说有人担心一些审稿人可能会对特定的研究小组或首次发表论文的作者有偏见。我想确保我的论文被接受的机会不会受到任何潜在偏见的负面影响。一旦我将论文上传到 arXiv，我明白我的匿名性就会丧失，这加剧了我的担忧。 任何关于这个主题的见解或建议都将不胜感激！提前致谢！    提交人    /u/morphinejunkie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/</guid>
      <pubDate>Mon, 21 Oct 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于推理的高效 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</link>
      <description><![CDATA[我正在使用高分辨率图像进行物体检测项目。 有没有什么技术可以使训练有素的 CNN（UNet）在推理过程中更有效率？我知道修剪就是这样一种技术，但它有损失准确性和可并行性的风险。    提交人    /u/_My__Real_Name_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/</guid>
      <pubDate>Mon, 21 Oct 2024 08:20:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] AquaVoice 风格文本版本模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/</link>
      <description><![CDATA[不知道为什么这个想法（很酷）从来没有流行起来，但我想知道我们是否可以为其构建一个开源模型，例如一个经过微调的 LLM，也许有一个小模型，试图区分用户何时提供“文本值”和何时说出“编辑命令”，然后进行编辑 “基本原型”不应该太难，但可能会很有帮助 https://withaqua.com/    提交人    /u/oulipo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/</guid>
      <pubDate>Mon, 21 Oct 2024 07:22:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] Google Shopping 10M 数据集，用于大规模多模式产品检索和排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</link>
      <description><![CDATA[我们终于在 Hugging Face 上发布了 Marqo Google Shopping 1000 万数据集 (Marqo-GS-10M)。这是用于多模式产品检索的最大、最丰富的数据集之一！  1000 万行查询、产品标题、图片和排名 (1-100) ~100k 个唯一查询 ~500 万个时尚和家居领域的唯一产品 反映了真实世界的数据和用例，并可作为方法开发的良好基准 适当的数据拆分、域内、新查询、新文档以及新文档和新查询。   该数据集为每个查询-文档对提供了详细的相关性分数，以方便将来的研究和评估。 !pip install datasets from datasets import load_dataset ds = load_dataset(&quot;Marqo/marqo-GS-10M&quot;)  我们将这个大规模数据集作为我们训练框架发布的一部分进行策划：广义对比学习 (GCL)。  数据集：https://huggingface.co/datasets/Marqo/marqo-GS-10M GCL：https://github.com/marqo-ai/GCL 论文：https://arxiv.org/abs/2404.08535    提交人    /u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/</guid>
      <pubDate>Sun, 20 Oct 2024 21:51:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] 未使用 xLSTM 隐藏状态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/</link>
      <description><![CDATA[大家好，我正在阅读 xLSTM 论文 https://arxiv.org/pdf/2405.04517，特别是关于 mLSTM 的部分，我想知道隐藏状态用在哪里？它的实用性是什么？通常它用于计算输出门。 https://preview.redd.it/7m51jnjhdzvd1.png?width=939&amp;format=png&amp;auto=webp&amp;s=fc84cdcaac47110af22a86b86ff5390ef4e53a37    提交人    /u/splashhhhhhhhhhhh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/</guid>
      <pubDate>Sun, 20 Oct 2024 21:40:48 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 现在我有一份工程师的工作，我如何才能了解最新的有趣的论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</link>
      <description><![CDATA[大家好，我以前在实验室工作，研究计算机视觉和机器学习。通过与教授和博士交谈，我可以了解到一些有趣的新文章。现在我在一家大公司工作，我不再有这个网络，也没有时间花几个小时搜索有趣的新文章。有没有什么好的资源可以汇总与机器学习和计算机视觉相关的精彩文章？    提交人    /u/Fugius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/</guid>
      <pubDate>Sun, 20 Oct 2024 21:07:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/</guid>
      <pubDate>Sun, 20 Oct 2024 15:00:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>