<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Fri, 01 Mar 2024 21:12:48 GMT</lastBuildDate>
    <item>
      <title>[D] 计算机视觉在仓储物流中的应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/</link>
      <description><![CDATA[在此 文章您将了解计算机视觉如何优化物流和仓储流程。  仓储行业正在快速适应在线购物的需求。计算机视觉改善库存管理、流程优化和质量控制。它可以自动执行手动任务并优化许多操作。OpenCV.ai 团队描述了该领域最流行的 AI 实施用例。  更多详细信息为这里   由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/</guid>
      <pubDate>Fri, 01 Mar 2024 20:03:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 部署 Transformer 模型的最佳方式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b43ndx/d_best_way_to_deploy_transformer_models/</link>
      <description><![CDATA[嗨，我是一名软件工程师，构建全栈应用程序已经三年了。我正在尝试利用 OpenAI 的剪辑模型来分析帧并将其存储在矢量数据库中以进行语义搜索，从而构建一个视频理解引擎。  在部署全栈应用程序并扩展它们（减去整个 ML Ops 部分）方面，我的表现非常出色。  我正在尝试在 AWS 上部署 CLIP，我在研究中发现了几个选项，但我越深入研究它就越感到困惑。  我找到的选项是：1. HuggingFace 专用推理端点 2. 在 Sagemaker 上拥抱面部容器 3. 使用来自张量流的剪辑并在 Sagemaker 上下载权重 4. 启动我的 FAST API 服务器具有加速计算实例的 EC2  就上下文而言，我使用的是拥抱脸的 Transformers 库:)  所以我的问题是： 1. 你们通常如何部署模型 2.在构建多模式方面，是否可以在单个 SageMaker 端点中部署所有模型，如果可以的话，它是如何工作的！ 如果这听起来很愚蠢，我很抱歉，但请帮助兄弟解决这个问题哈哈 期待学习更多！   由   提交/u/Hot-Afternoon-4831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b43ndx/d_best_way_to_deploy_transformer_models/</guid>
      <pubDate>Fri, 01 Mar 2024 19:52:04 GMT</pubDate>
    </item>
    <item>
      <title>lalamu 和 wav2lip 的消失 - 有什么见解吗？ [D]，[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3zetw/disappearance_of_lalamu_and_wav2lip_any_insights/</link>
      <description><![CDATA[大家好，我注意到 lalamu 和 wav2lip 不再可用，并且似乎已从应用商店中下架。有人有关于这些应用程序发生的情况的任何信息或更新吗？我尝试查找官方资源和开发者论坛，但一无所获。这些工具对我的项目至关重要，我正在寻找任何替代方案或有关其状态的新闻。如果您有任何见解或更新，我们将不胜感激！   由   提交 /u/Due_Brief6661   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3zetw/disappearance_of_lalamu_and_wav2lip_any_insights/</guid>
      <pubDate>Fri, 01 Mar 2024 17:04:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] Luminal：通过图编译在 Rust 中进行快速机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3yvr2/p_luminal_fast_ml_in_rust_through_graph/</link>
      <description><![CDATA[大家好，我用 Rust 开发 ML 框架已经有一段时间了，我终于很高兴与大家分享它。 Luminal 是一个深度学习库，它使用可组合编译器来实现高性能。 当前的 ML 库往往庞大且复杂，因为它们试图将高级操作直接映射到低级手写内核，并专注于急切执行。像 PyTorch 这样的库包含数十万行代码，单个程序员几乎不可能理解所有内容，除非进行大规模重构。 但是有必要这么复杂吗？机器学习模型往往是由一些简单运算符组成的静态数据流图。这使我们能够拥有一个非常简单的核心，仅支持一些原始操作，并使用它们来构建复杂的神经网络。然后，我们可以编写编译器，在构建图之后修改图，以根据我们运行的后端交换更高效的操作。 Luminal 采用这种方法极端情况下，仅支持 11 种基本运算 (primops)：  一元 - Log2、Exp2、Sin、Sqrt、Recip 二元 - &lt; strong&gt;Add、Mul、Mod、LessThan 其他 - SumReduce、MaxReduce、Contigious  每个复杂的操作都可以归结为对于这些原始操作，例如，当您执行 a - b 时，add(a, mul(b, -1)) 会写入图表中。或者，当您执行a.matmul(b)时，实际放在图表上的是sum_reduce(mul(reshape(a), reshape(b)))。&lt; /p&gt; 一旦构建了图，迭代编译器就可以对其进行修改，以用更高效的操作替换 primops，具体取决于其运行的设备。例如，在 Nvidia 卡上，动态编写高效的 Cuda 内核来替换这些操作，并用专门的 cublas 内核交换支持的操作。 这种方法会产生一个简单的库，并且性能仅受限于编译器程序员的创造力，而不是模型程序员。 Luminal 还有许多其他简洁的功能，请查看存储库 这里 如果您有任何问题请lmk！   由   提交/u/jafioti   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3yvr2/p_luminal_fast_ml_in_rust_through_graph/</guid>
      <pubDate>Fri, 01 Mar 2024 16:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[R]：学习生成零样本任务适应的指令调优数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3y98f/r_learning_to_generate_instruction_tuning/</link>
      <description><![CDATA[很高兴分享我们在合成任务生成方面的工作。 隆重介绍 Bonito 🐟，这是一个开源模型，可以将您的原始、将未注释的数据转化为合成指令调整数据集。有了它，您可以轻松地为您的专有和私人数据创建专门的法学硕士！ 查看我们下面的工作：论文：https://arxiv.org/abs/2402.18334 代码：https://github.com /BatsResearch/bonito 模型：https://huggingface.co/BatsResearch/bonito-v1    由   提交 /u/nihalnayak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3y98f/r_learning_to_generate_instruction_tuning/</guid>
      <pubDate>Fri, 01 Mar 2024 16:19:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 更改视频中的语音</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3x7mp/d_change_speech_in_a_video/</link>
      <description><![CDATA[为了演示，我想更改一个人在视频中所说的内容。是否有任何免费工具网站或 python 库可用，也可以适应嘴唇运动？    由   提交/u/Electronic-Letter592   reddit.com/r/MachineLearning/comments/1b3x7mp/d_change_speech_in_a_video/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3x7mp/d_change_speech_in_a_video/</guid>
      <pubDate>Fri, 01 Mar 2024 15:37:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 具有动态操作集的非 RAG 回溯 GPT 代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3w4wp/p_a_nonrag_backtracking_gpt_agent_with_a_dynamic/</link>
      <description><![CDATA[      大家好，我&#39;我们一直在开发一个不依赖检索增强生成（RAG）来查找相关信息并生成响应的新框架。相反，它利用独特的文本接口 (TI)，实现与外部资源的直接 GPT-4 交互，就像我们与 GUI 交互的方式一样。 由于这种方法需要与 TI 重复交互，因此 LLM 的作用就像一个自治系统代理人。与 AutoGPT 不同，AutoGPT 的操作是预先确定的并且不会更改，操作由 TI 提供，并根据 TI 所处的状态进行更改。 全局操作 本地操作 这种方法的主要局限性是：它仅适用于 GPT- 4 并需要构建文本界面以与不同类型的资源交互。 演示视频：https:// www.youtube.com/watch?v=sE2JK3IB6rI 代码：https://github.com /ash80/backtracking_gpt X 上的线程：https://x.com/ash_at_tt/status /1763575975185403937 欢迎您提供反馈、建议和贡献。   由   提交/u/ashz8888  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3w4wp/p_a_nonrag_backtracking_gpt_agent_with_a_dynamic/</guid>
      <pubDate>Fri, 01 Mar 2024 14:52:51 GMT</pubDate>
    </item>
    <item>
      <title>多智能体深度 q 学习深入研究开发状态 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3ts4d/multi_agent_deep_q_learning_takes_a_dive_at/</link>
      <description><![CDATA[      我正在使用 double &amp;深度 Q 学习对决。达到 epsilon 0.01 后不久，奖励开始走下坡路。我正在尝试不同的超参数，但对任何类似的经验/想法感兴趣。  我的猜测是，由于这是一个多智能体场景，因此在大部分探索阶段，智能体都会在给定其余动作的随机动作的情况下学习最佳动作。一旦 epsilon 达到 0.01，其余智能体的行为（以及每个智能体的环境）就会发生变化。这就是奖励的意思。  https://preview.redd .it/fk6fpc0a1qlc1.png?width=696&amp;format=png&amp;auto=webp&amp;s=63c7a4bd1a560809cbec2099261d6abc8c2152b5   由   提交 /u/ripototo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3ts4d/multi_agent_deep_q_learning_takes_a_dive_at/</guid>
      <pubDate>Fri, 01 Mar 2024 13:04:11 GMT</pubDate>
    </item>
    <item>
      <title>说话人识别 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3t7ls/speaker_recognition_p/</link>
      <description><![CDATA[大家好，我正在开发一个说话人识别项目，以识别说话人数据集中谁在说话。该模型在近距离环境中运行良好，我希望现在该模型还可以告诉我何时说话者未知（不是数据集的一部分）。您可以推荐哪些方法？   由   提交 /u/cestoi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3t7ls/speaker_recognition_p/</guid>
      <pubDate>Fri, 01 Mar 2024 12:33:34 GMT</pubDate>
    </item>
    <item>
      <title>L2-SVM 是否总是使用平方铰链损失而不是标准铰链损失？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3q9f9/does_l2svm_involve_always_using_the_squared_hinge/</link>
      <description><![CDATA[我看到很多论文都用标准铰链损失写了 L2-SVM 的目标函数，其他的则用平方铰链来写。这就是我困惑的地方。   由   提交 /u/PerfecttMachine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3q9f9/does_l2svm_involve_always_using_the_squared_hinge/</guid>
      <pubDate>Fri, 01 Mar 2024 09:30:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无监督数据选择的稳健指南：为特定领域的机器翻译捕获令人困惑的命名实体</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3ntnj/r_robust_guidance_for_unsupervised_data_selection/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.19267 摘要：利用广泛的数据集可以训练多语言机器翻译模型；然而，这些模型通常无法准确翻译专业领域内的句子。尽管获取和翻译特定领域的数据会产生高昂的成本，但高质量的翻译却是不可避免的。因此，在无人监督的情况下寻找最“有效”的数据成为降低标签成本的实用策略。最近的研究表明，可以通过根据数据量选择“适当困难的数据”来找到这些有效数据。这意味着数据不应过于具有挑战性或过于简单，特别是在数据量有限的情况下。然而，我们发现建立无监督数据选择的标准仍然具有挑战性，因为“适当的难度”可能会根据所训练的数据域的不同而有所不同。我们引入了一种新颖的无监督数据选择方法“捕获令人困惑的命名实体”，该方法采用翻译命名实体中的最大推理熵作为选择度量。动机是特定领域数据中的命名实体被认为是数据中最复杂的部分，并且应该以高置信度进行预测。当使用“专业领域韩英平行语料库”进行验证时，与现有方法相比，我们的方法可以为无监督数据选择提供强有力的指导。    由   提交/u/Capital_Reply_7838   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3ntnj/r_robust_guidance_for_unsupervised_data_selection/</guid>
      <pubDate>Fri, 01 Mar 2024 06:45:07 GMT</pubDate>
    </item>
    <item>
      <title>DeepMind 推出 Hawk 和 Griffin [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3leks/deepmind_introduces_hawk_and_griffin_r/</link>
      <description><![CDATA[        由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3leks/deepmind_introduces_hawk_and_griffin_r/</guid>
      <pubDate>Fri, 01 Mar 2024 04:28:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI OSS 项目向贡献者开放？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3fmno/d_ai_oss_projects_open_to_contributors/</link>
      <description><![CDATA[我有一个大约 8 人的团队正在考虑尝试为一些 AI OSS 做出贡献，我想知道这里是否有人有任何最喜欢的项目已经受到巨大关注？我们对执行以下操作之一的 OSS 项目特别感兴趣：  让凡人能够相对经济地扩展训练或微调（当我说凡人时，我指的是我们中的凡人）没有 OpenAI 的数十亿） 更轻松、更直观地将非结构化数据（随机数据库表、pdf、网站等）转换为可随时进行微调的格式 &lt; li&gt;可用的代理（我不太相信我们已经为无监督人工智能做好了准备，所以这可能是列表中最低的代理之一，除非 OSS 领域的某些东西看起来确实很有前途） 将运行时上下文纳入 LLM，而无需对运行系统的详细信息进行配置或编程，以便您可以向 LLM 提出问题，例如“告诉我是否有任何与我认为的正常健康运行时状态相比发生了变化的情况”&lt; /li&gt; 专门针对 LLM 世界的新版本构建/部署/测试工具，使故障排除变得更加容易，因为它们更加配置驱动且 LLM 友好，并且运行时间和动态性较低？  &lt;该列表有点像我希望帮助加速​​ LLM 驱动的开发的愿望清单，现在我想要找到正在做这些事情的 OSS 项目（有牵引力！）。有人看到什么了吗？或者如果做不到这一点，任何人都知道如何搜索它们或向谁询问它们？   由   提交/u/jonxtensen  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3fmno/d_ai_oss_projects_open_to_contributors/</guid>
      <pubDate>Thu, 29 Feb 2024 23:56:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 ViT 比 SWIN 更常用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3bhbd/d_why_is_vit_more_commonly_used_than_swin/</link>
      <description><![CDATA[我仍在阅读，但我阅读的大多数计算机视觉论文都使用 ViT 作为其主干，而不是 SWIN 或其他类似的架构，但为什么呢？  ​ ViT 论文必须在 303M 图像 JFT 数据集上预训练模型，以击败 ImageNet 上的早期卷积模型，而 SWIN 无需任何预训练即可实现更好的性能。训练。我想，如果 SWIN 以同样的方式进行预训练，即使不是更高的性能，也能在 ImageNet 上实现相当的性能，但不可否认的是，我还没有看到任何工作来验证这个想法。 ​ 这只是 ViT 优先的情况，所以现在每个人都使用它作为默认值还是还有其他原因？   由   提交 /u/PM_ME_JOB_OFFER   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3bhbd/d_why_is_vit_more_commonly_used_than_swin/</guid>
      <pubDate>Thu, 29 Feb 2024 21:10:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>