<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Tue, 05 Mar 2024 03:15:29 GMT</lastBuildDate>
    <item>
      <title>[R] 是否可以在不进行大量计算的情况下写一篇论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6tjil/r_is_it_possible_to_write_a_paper_without_large/</link>
      <description><![CDATA[我正在独立学习机器学习，希望将来能写一篇论文，但是我没有任何强大的计算能力我的桌面 GPU 是 2070 super。如果没有大量的计算，最近的突破似乎是无法实现的。那么这是否可能，或者是否有任何领域可以在没有昂贵 GPU 的情况下发表论文？   由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6tjil/r_is_it_possible_to_write_a_paper_without_large/</guid>
      <pubDate>Tue, 05 Mar 2024 02:12:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 匿名期</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6q0b4/d_anonymity_period/</link>
      <description><![CDATA[ACL 会议的这项政策已经消失了，对吗？我只是想在 arXiv 以及 Twitter 或 LinkedIn 上发布我的作品。   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6q0b4/d_anonymity_period/</guid>
      <pubDate>Mon, 04 Mar 2024 23:38:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于任何大型模型的数据限制</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6ok2i/d_on_the_data_linitations_for_any_large_models/</link>
      <description><![CDATA[嗨，我绝对不是这个主题的专家。有一个问题困扰了我一段时间。我想在这个 Reddit 子版块上发布这个问题很长时间了，但我不知道这是不是问这个问题的正确地方。我将其发布在这里，因为我无法在其他地方找到答案。 这个问题与输入到大型模型（无论是视觉还是语言）的数据有关。由于人工智能的突然爆发，许多公开可用的新内容都是人工智能生成的内容。我没有人工智能与人类生成内容的比率，但我认为人工智能的比例在未来几天会呈指数级增长。  这些模型中的大多数都依赖于巨大的数据集，我假设像 GPT-n 这样的模型已经在我不知道的数百年的人类生成的数据上进行了训练。另外，我觉得有一个普遍的共识（至少在公众中），即您提供的数据越多，增加的参数数量越多，结果就会呈指数级增长。 这就是我有我的困惑，在我看来，存在几个问题： 1. 无意中将人工智能生成的数据纳入下一代模型的训练中。 2. 随着越来越多的人采用当前的人工智能工具，缺乏真正的人类生成内容。 这些点是否会导致一些问题，例如由于原始内容稀缺而导致数据匮乏。由于训练样本被污染，未来模型中的偏差最终会增加，方差也会减少（假设很难识别和过滤掉与人类生成的内容混合的人工智能生成的内容） 我看到人工智能最初呈指数增长，然后呈缓慢线性增长。人们预测 AGI 潜伏在角落里等等，在之前的前提下，我是否遗漏了一些东西？或者我的假设完全错误。或者这些问题是否已经解决了（如果请指导我查阅相关文献，我将不胜感激） 如果这些问题已经解决并且我的问题无效或者如果这违反了任何发帖规则。 我不好   由   提交 /u/para_thayoli   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6ok2i/d_on_the_data_linitations_for_any_large_models/</guid>
      <pubDate>Mon, 04 Mar 2024 22:40:51 GMT</pubDate>
    </item>
    <item>
      <title>如何构建交通网络客流预测工具？ [讨论] [项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6oe5a/how_to_frame_a_transport_network_passenger_load/</link>
      <description><![CDATA[我正在尝试对网络进行一些近时每次传输负载预测，但我无法调和我的理解我能找到的文献的问题。我认为这个想法适用于运输和物流网络的许多想法，例如公共汽车/铁路/航空/邮政/送货网络，但我在文献中找到这些例子的运气很少 - 我看到的是常规的预测模型时间序列，例如物联网传感器数据/股票市场数据等，这些感觉非常不同，我希望下面能清楚地说明这一点。我在兜圈子，无法判断我是否只是_认为_它很复杂，或者我是否需要进行一些真正的简化/建模假设。  ​ ## 问题空间： 我有一个交通网络，我想预测每次交通的乘客数量。每个交通工具都沿着*路线* = 特定的站点集合运行。每条路线都有多个*服务*，例如下午 3 点、下午 3.30、下午 4 点等。每条路线由多个*航段*组成 - 旅程中一站到一站，中间没有停靠站。每个车站都有一个 UID 并代表一个物理位置，例如 Gotham Central Station。据我所知，约 45% 的乘客搭乘多种服务中的一种（到达并搭乘任何服务），约 25% 的乘客旅程涉及转接服务。许多路线重叠（相当于沿着市中心附近的同一条主路线行驶）。服务迟到可能会对服务的繁忙程度产生重大影响，加上沿途影响（A 站的满载交通无法在 B 站接载乘客），以及跨服务的网络效应。有些服务非常频繁，有些服务则不太频繁。所有服务都会有延误。许多路线在路线上的不同点都有多个并发服务。 ​ ## 数据： 对于每个服务，我们都有 [id变量-运输 ID、路线 ID、服务 ID、起止点 ID]、[时间-计划时间、实际时间、延误、计划终点站时间]、[实际乘客数量、网络运营商预测（季节性朴素方法）]、 [其他变量 - 天气、运输规模/最大乘客数、一些服务有票务数据，例如儿童/OAP/折扣等]。我们有 2 年多的数据，但只有部分网络有传感器来告诉我们有多少乘客。 ​ ## 问题： 我有几种方法 - 我想尝试[时空图卷积网络](https://arxiv.org /pdf/1709.04875.pdf），但老板担心这对于我们约 6 个月的资金来说太多了，而我是唯一的数据科学家/开发人员。我们的第一步是将网络分解为单独的支路/服务时间序列并使用一些 ARIMA 方法，并使用 facebook 的 Prophet。我知道最近有很多时间序列的大型模型可能会有所帮助，但我需要以某种方式构建该系列，并且我认为我缺少一些明显更简单的模型 - 对整条线进行建模？我应该使用时间序列中的其他一些工具 - 动态时间扭曲吗？ DTW 似乎是一个比较系列的工具，这似乎不适用于我的情况，除非我误解了某些东西？任何帮助、博客链接、论文等都值得赞赏 x   由   提交/u/Dry_Philosophy7927   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6oe5a/how_to_frame_a_transport_network_passenger_load/</guid>
      <pubDate>Mon, 04 Mar 2024 22:34:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 视频数据是否有相当于“The Pile”的东西？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6nmz0/d_is_there_an_equivalent_of_the_pile_for_video/</link>
      <description><![CDATA[是否像文本一样存在大量未标记的大型视频？ &lt;!-- SC_ON - -&gt;  由   提交 /u/PM_ME_JOB_OFFER   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6nmz0/d_is_there_an_equivalent_of_the_pile_for_video/</guid>
      <pubDate>Mon, 04 Mar 2024 22:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[D]：使用 GPU 将 PDF 问答应用程序扩展到 10K 用户 – <$250/月</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6jv56/d_scale_pdf_qa_app_to_10k_users_with_gpus_250mo/</link>
      <description><![CDATA[   大家好， 查看有关使用 Pinecone、Langchain 和 Inferless 构建和扩展 PDF 问答应用程序的分步详细教程 &amp;# x200b; 架构除了详细的快速部署指南之外，它还包括成本分析，以每月处理 3000 个文档和近 10,000 个查询为例，说明如何节省高达 84% 的成本，同时大幅降低您的成本Inferless 的价格为 1800 美元（AWS），每月只需 250 美元。 这里是教程 - https://cookbook.inferless.com / 如果您有共鸣，请在此处加入 Hackernews 的讨论 - https://news .ycombinator.com/item?id=39594588   由   提交/u/Tiny_Cut_8440   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6jv56/d_scale_pdf_qa_app_to_10k_users_with_gpus_250mo/</guid>
      <pubDate>Mon, 04 Mar 2024 19:32:55 GMT</pubDate>
    </item>
    <item>
      <title>[R][D]利用农业中的多模式数据集成增强土壤湿度预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6httx/rd_enhancing_soil_moisture_predictions_using/</link>
      <description><![CDATA[问候， ​ 我正在探索涉及多模态数据的跨学科研究领域，专注于农业。我的研究结合了视觉和表格数据：来自三个不同地点的农作物和土壤图像，以及相应的气象信息（温度、湿度、太阳辐射等），从上午 9 点到下午 5 点每小时收集一次。目标是通过整合这些数据模式来增强土壤湿度预测，旨在根据作物图像准确估计土壤湿度。 ​ 您能否建议有效结合的方法这些数据类型？我尝试过或考虑过以下内容： ​  特征串联，利用 MLP/随机森林进行气象数据，利用 CNN 进行图像分析。  特征串联，利用 MLP/随机森林进行气象数据，利用 CNN 进行图像分析。 li&gt;  Conv-LSTM。 视觉变压器。  此外，我很好奇将连续土壤斑块图像合并到 Conv-LSTM 等模型中的策略捕捉时间和视觉特征。您认为这种方法可行吗？ ​ 我感谢社区的任何见解或建议。感谢您抽出时间。   由   提交 /u/MohammedRakib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6httx/rd_enhancing_soil_moisture_predictions_using/</guid>
      <pubDate>Mon, 04 Mar 2024 18:11:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] GLU（门控线性单元）为什么起作用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6ggpz/d_why_do_glus_gated_linear_units_work/</link>
      <description><![CDATA[如今，GLU 变体，例如 SwiGLU，在法学硕士中经常使用。 但是论文“GLU Variants”改进变压器” （https://arxiv.org/pdf/2002.05202.pdf），只是说“我们不提供任何解释”为什么这些架构看起来有效；我们将他们的成功和其他一切一样，归功于神圣的仁慈。” 我还发现原始 GLU 论文中的解释并不令人满意。他们说它有更清晰的梯度，但我认为这个问题已经通过残差连接解决了。 有人对 GLU 成功的原因有任何理论甚至直观的解释吗？ 我的思考过程是，它允许每个令牌学习自己独有的转换，这提高了 MLP 的表达能力，但我不确定这是否合理。 编辑：The Falcon技术报告（链接在此：https://arxiv.org/pdf/2311.16867.pdf，第 14 页）讨论了 GLU使用它是因为它增加了 50% 的参数计数，表示：“在缩放方面，GLU 激活也是首选，因为它们增加了 MLP 的大小（将第一层加倍），将更多计算转向简单的矩阵乘法。 ” 但是，我认为我并不确信情况确实如此；毕竟，为什么不能将 MLP 隐藏层中的神经元数量增加 50%，这具有相同的内存成本和参数增加。   由   提交/u/cofapie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6ggpz/d_why_do_glus_gated_linear_units_work/</guid>
      <pubDate>Mon, 04 Mar 2024 17:18:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] 介绍下一代克劳德</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6ea53/r_introducing_the_next_generation_of_claude/</link>
      <description><![CDATA[https://www.anthropic .com/news/claude-3-family 今天，我们宣布推出 Claude 3 模型系列，它为广泛的认知任务树立了新的行业基准。该系列包括三种最先进的型号（按功能升序排列）：Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus。每个后续型号都提供了越来越强大的性能，允许用户为其特定应用选择智能、速度和成本之间的最佳平衡。 Opus 是我们最智能的型号，在大多数常见评估中均优于同类产品人工智能系统的基准，包括本科水平专家知识（MMLU）、研究生水平专家推理（GPQA）、基础数学（GSM8K）等。它在复杂任务上展现出接近人类水平的理解力和流畅性，引领通用智能的前沿。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6ea53/r_introducing_the_next_generation_of_claude/</guid>
      <pubDate>Mon, 04 Mar 2024 15:52:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求评论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6dugx/d_asking_for_reviews/</link>
      <description><![CDATA[我最近撤回了我在 ICLR 研讨会上提交的非档案文件，因为我想在我的论文更具体时简单地提交一份文件。在我提交的级别上，它主要只是展示一个想法和非常小的实验结果。所以，我在决定通知前几天退出了。现在决定已经发布，我实际上很好奇我的论文得到的评论。联系研讨会主席并征求评论是否不道德或不好？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6dugx/d_asking_for_reviews/</guid>
      <pubDate>Mon, 04 Mar 2024 15:35:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] XGBoost + 二元分类数据不平衡，过度拟合但分类报告可接受？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6c5lb/d_xgboost_imbalanced_data_for_binary/</link>
      <description><![CDATA[TLDR  如何在不牺牲 1 类性能的情况下修复过度拟合？ 在高度不平衡的数据集中，过度拟合是否正常？ 根据 Class 1 的性能以及本项目检测 Class 1 的目标，您认为这足以进行部署吗？  就上下文而言，我的数据不平衡（700：5500 个样本）。但在现实生活中，这种不平衡会是两倍。创建这个二元分类器是为了预测少数类别。 我使用了 XGBoost 和 RandomForest，两者都具有类别权重。我使用 F1 进行评分和评估指标 = XGBoost 的对数损失。通过超参数调整。提前停止 ecal_metrics = mae。 最好的结果是 XGBoost   Class 精度 召回 f1 支持    0 .97 .99 td&gt; .98 1101   1（我想要这个）  .93 .82 .87  166             acc   0.97 1267   宏平均值 0.95 0.9 0.93 1267   加权平均值 .97  .97 .97 1267  &lt; tr&gt;        平均训练 f1 分数最佳估计器：.981 平均验证 f1 分数最佳估计器：.821 &lt; p&gt;如果我尝试更改特征和参数以平衡训练和验证分数。性能明显较差，1 类的召回率约为 0.65。 对于我的用例来说，我当前训练的模型对于生产来说已经足够好了，即使它过度拟合？我主张部署模型并随着时间的推移收集第 1 类的新数据，以便使用更多少数类数据重新训练模型。   由   提交 /u/Vveriant   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6c5lb/d_xgboost_imbalanced_data_for_binary/</guid>
      <pubDate>Mon, 04 Mar 2024 14:26:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年从零到英雄要实施哪些论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/</link>
      <description><![CDATA[标题的灵感来自于 Karpathy 在其课程中自下而上的方法神经网络从零到英雄。与此类似，但对于研究论文来说，从早期论文（可能最多 10-15 年前）“自下而上”到今天的论文 - 熟悉 DL 的人（至少读过一本 DL 书）应该做什么，但相对而言实施经验不足？这样做的目标是更深入地了解 DL 研究，并达到一个可以轻松阅读、理解和实现当今的 DL 论文的水平，甚至可能自己提出新颖的想法。 我主要考虑的领域是 CV 和 NLP，但它也可能是更一般的东西，例如学习率、损失函数、激活函数、正则化、优化等。  有谁知道可以的论文/论文为此目的是否可以很好地实施，或者您是否有任何想要分享的列表？感谢任何帮助！   由   提交/u/total-expectation  /u/total-expectation  reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b674cv/d_what_papers_to_implement_in_2024_from_zero_to/</guid>
      <pubDate>Mon, 04 Mar 2024 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML 不认真？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b610zm/d_ml_being_unserious/</link>
      <description><![CDATA[我正在阅读三元论文，它来了对我来说，虽然大多数 Arxiv 论文的名称都是“探索自动语音识别系统的基于神经的声学模型中编码的信息”之类的内容，但三元论文却被称为有点不严肃的“1 位法学硕士时代” ：所有大型语言模型均为 1.58 位”。  机器学习中还发生了一些其他不严重的事情，例如“注意力就是你所需要的”论文和我最喜欢的论文名称“Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling”，人们将他们的技术命名为 LASER 和 DRμGS，人们将他们的 LLM 命名为无毒百吉饼、海豚和其他随机的东西 在目前的缓慢时期，您对于 ML 不认真的时期有什么好的建议吗？   由   提交/u/adumdumonreddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b610zm/d_ml_being_unserious/</guid>
      <pubDate>Mon, 04 Mar 2024 03:43:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于在大型科技/对冲基金从事 ML 工作的人员，您认为 alpha 主要来自噪音的百分比是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5zs3p/d_for_people_working_on_ml_at_big_techhedge_funds/</link>
      <description><![CDATA[我还没有对此下定决心，但我的一些同事坚信大多数人并没有使用 ML 获得真正有意义的成果，但正在追寻内部出版偏见。好奇其他公司的人怎么想。   由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5zs3p/d_for_people_working_on_ml_at_big_techhedge_funds/</guid>
      <pubDate>Mon, 04 Mar 2024 02:41:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>