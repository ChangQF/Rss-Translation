<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Wed, 05 Mar 2025 09:18:50 GMT</lastBuildDate>
    <item>
      <title>[d]在ICCV25注册截止日期后添加作者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3z04l/d_adding_the_authors_after_registration_deadline/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我忘了将作者添加到我的提交中，有没有办法在注册截止日期后添加更多作者？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/minhtran91     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3z04l/d_adding_the_authors_after_registration_deadline/</guid>
      <pubDate>Wed, 05 Mar 2025 09:06:03 GMT</pubDate>
    </item>
    <item>
      <title>[r]如何微调“思考”模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3yx5a/r_how_do_i_finetune_thinking_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi，我想在“推理”上进行监督的微调。  DeepSeek-ai/deepSeek-ai/deepSeek-r1-distill-lllama-8b 等模型。但是，我注意到这些模型，就像它们被蒸馏的较大模型一样，产生了“思考”。在提供最终答案之前的文本（有时答案只是对＆lt; think; gt;＆lt;/think think gt; tags之间所包含的推理的简短摘要）。问题是：我是否应该构架我的任务以适合这种格式（推理 - ＆gt;答案），还是可以在没有思维标签的情况下对模型进行微调？这些模型只能在需要此行为的任务上进行微调吗？抱歉，这个幼稚的问题，但我是这种新型模型的新手。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/debonargon     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3yx5a/r_how_do_i_finetune_thinking_models/</guid>
      <pubDate>Wed, 05 Mar 2025 08:59:51 GMT</pubDate>
    </item>
    <item>
      <title>[d]使视觉语言模型指向图像中的对象，将新模式引入语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3xlre/d_making_vision_language_models_point_to_objects/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试与Moondream和Molmo类似的东西。即使语言模型能够产生询问的对象的归一化坐标。 “要点：狗例如 我正在尝试使Smolvlm做到这一点，以作为一个有趣的项目，以获得更好的理解。我正在尝试使用Pixmo点数据集的子集（1MIL）。    尝试了纯sft，无论是完整的还是peft，显然是不起作用的，因为模型没有输出点的概念。      尝试了grpo，因为该模型显然没有这样的潜在功能，因为该模型没有这样的潜在能力。    从Moondream中汲取灵感，我完全引入了一种新的方式。即，对点进行了编码，与模型自回旋部分所接受的嵌入维度相同，然后在自动回归后，另一个解码器解码了点。保持其他零件冻结。我尝试了SFT的交叉熵，尽管它对它用于指向任务有些怀疑，而MSE损失似乎更合适。但这也失败了，尽管在训练过程中表现出很好的损失特征。该模型仅产生随机点。   有人尝试过类似的事情吗？关于我还能尝试什么建议？关于如何取得进展的任何指针都会很好，显然这是可行的。我缺少什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/smalltimecsguy     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j3xlre/d_making_vision_vision_vision_vision_models_models_point_point_objects/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3xlre/d_making_vision_language_models_point_to_objects/</guid>
      <pubDate>Wed, 05 Mar 2025 07:18:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM量化建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3pyv1/d_llm_quantization_advice/</link>
      <description><![CDATA[在老实说，这是令人着迷和压倒性的混合。我得到了减少基础模型的规模，更快地推理，丢失精度，所有这些好东西，但我想了解更多。 如果您在对您有所帮助之前曾经经历过？任何更改游戏的论文，博客文章，存储库，代码教程或艰苦学习的课程？我想从“哦，我有点明白它”到真正知道我在做什么。 很想听听任何在这条路上有效的人，没有什么工作，没有什么，您希望您早些时候知道！ 欣赏它！    &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/Professionfox8649     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3pyv1/d_llm_quantization_advice/</guid>
      <pubDate>Wed, 05 Mar 2025 00:17:01 GMT</pubDate>
    </item>
    <item>
      <title>[D]混淆矩阵。困惑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/</link>
      <description><![CDATA[       &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/wide-chef-7011     [link]   ＆＃32;   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/</guid>
      <pubDate>Tue, 04 Mar 2025 19:48:39 GMT</pubDate>
    </item>
    <item>
      <title>[d]寻找有关长期AI记忆和上下文保留的见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在跟踪AI模型如何保留上下文，不仅仅是基于会话的内存，而且更接近自适应智能，这些智能会记住和完善其理解的理解。   大多数当前模型似乎都可以限制在互动的情况下，而不是在互动的情况下进行互动，但如果互动，则互动的互动，如果iif if if if if if if if if if if if if if if if if if if if if if if if if if if if a if if if if if i，则陷阱？ 好奇是否有人从事此工作，还是在增强学习和变形金刚进行的除外的有前途的方法。人们如何考虑将AI内存发展为更有机的东西？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/</guid>
      <pubDate>Tue, 04 Mar 2025 19:29:10 GMT</pubDate>
    </item>
    <item>
      <title>[d]我应该从这种类型的问题中期待什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3fslw/d_what_should_i_expect_from_this_type_of_question/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   数据访谈：访谈探讨您有关如何消耗和组织数据的思维过程，并在各种约束中验证您的方法。可能会要求您设计一个数据体系结构来解决特定问题，并编写代码以将一些给定的输入转换为有用的输出。可能会要求您考虑等问题，例如延迟，最终一致性和竭尽全力。我们使用Scala和Java开发数据管道，但使用各种工具来移动数据。在整个过程中，您应该期望与各种工具和技术讨论您的个人经验。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/horror_weakness_6996      [links]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3fslw/d_what_should_i_expect_from_this_type_of_question/</guid>
      <pubDate>Tue, 04 Mar 2025 17:11:14 GMT</pubDate>
    </item>
    <item>
      <title>[r] readerlm-v2：使用1.5b参数语言模型有效的HTML-TO-MARKDOWN转换</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3em5l/r_readerlmv2_efficient_htmltomarkdown_conversion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在研究一种专业的LM方法，该方法演示了目标优化如何比特定任务胜过更大的模型。 ReaderLM-V2是一种小型语言模型（3-7B参数），它以显着效率将HTML转换为Markdown和JSON。 此处的技术方法非常聪明：   使用较大模型的同步数据生成的同步数据生成较大的模型来创建高级培训 li&gt; li-li&gt; li&gt; li&gt; li&gt;      代币化处理HTML标签和结构有效地  利用块蒸馏技术来维持长期文档的上下文 仅专注于HTML理解，而不是一般能力    ，结果表明了更大的优势，结果显示了大量的模型： 保持对复杂文档的更好结构理解 处理嵌套的要素，表格和格式化更准确地 使用的计算资源   在文档中提供了更好的构建 在在许多任务中做得非常好而不是平庸的表现的模型。综合数据生成方法还解决了配对训练数据的专用域中的一个常见问题。 我认为，这种方法可以应用于其他专业文档处理任务，其中结构与内容一样重要。特别有趣的是，当针对特定领域进行适当优化时，较小的模型可以胜过较大的模型。  tldr：readerlm-v2是一种小但专业的语言模型，它通过使用合成训练数据和专业的建筑和专业的建筑和专业的模型，将HTML转换为Markdown/JSON比更大的Markdown/JSON更有效。它证明了目标优化可以胜过原始参数计数。  完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3em5l/r_readerlmv2_efficient_htmltomarkdown_conversion/</guid>
      <pubDate>Tue, 04 Mar 2025 16:23:55 GMT</pubDate>
    </item>
    <item>
      <title>[P]白血病的分类和检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您当前正在尝试开发一个由白血病数据集训练的模型，当我给出测试图像时，它需要给出输出为是（包含白血病）。问题是我想要一个统一的文件夹来用于所有培训图像。我找不到它，如果某人具有此类数据集的链接，请您分享。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/i_love_69_position     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/</guid>
      <pubDate>Tue, 04 Mar 2025 15:32:03 GMT</pubDate>
    </item>
    <item>
      <title>[d]清除时间序列中清除的简历的好处？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/</link>
      <description><![CDATA[      Hello, In a context of time series prediction, I have troubles to grasp the actual benefits of Purged CV versus regular time split CV. Formulated differently, why is there a risk of data leakage when time split CV is applied? As a reminder for下面的每个人都是常规时间拆分简历的工作方式（源 在这里，清除的人如何工作（相同欢迎任何见解，谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bilbilious-pomelo-700      [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/</guid>
      <pubDate>Tue, 04 Mar 2025 11:53:01 GMT</pubDate>
    </item>
    <item>
      <title>[d] ICLR 2025首次计时器？分享您接受的东西</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，所以我的第一篇论文被ICLR接受。等不及要去新加坡了！我认为这可能是一个很好的机会，可以看到该社区研究人员接受的一些作品。  对我来说，我加入了一个从事仿生的物理学家的实验室。他对航班机制特别感兴趣，并且在面向飞行的工程周围有许多项目。一些学生专注于老鹰队以及他们如何飞翔的热风，而另一些学生（例如我）专注于机器人机制，类似于蜂鸟和苍蝇。  长话短说，我们在拍打机翼周围开发了一个测量系统，跟踪其运动和系统中的空气动态力。然后，我们提出了一个问题：要获得所需的预定义空气动力的输入翼电影应该是什么。  方法有一个多元时间序列，重点是傅立叶空间。我们提出了一个在频域中确实表示的体系结构，并专门针对这些类型的任务任务量身定制，我们将其定义为逆映射。尽管我们没有证明可以应用逆映射的其他领域，但我们确实提供了一些可以进行未来研究的例子。  我们开放了数据集以及我们开发的框架（您可以在Github上查看它，Repo的名称是AdpativeSpectRumlayer）。 如果您是我的第一次，我很想听听您的故事     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/upaster-ability-774     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j35gpt/d_iclr_2025_2025_first_timers_here_here_here_share_share_hare_hare_hare_got_you/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/</guid>
      <pubDate>Tue, 04 Mar 2025 07:28:27 GMT</pubDate>
    </item>
    <item>
      <title>[r]谨慎的优化者：通过一行代码改进培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j33lm7/r_cautious_optimizers_improving_training_with_one/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这是一个令人惊讶的简单调整。在大多数现代深度学习优化器中，通常根据梯度的运行差异，以某种形式的动量和/或学习率缩放来计算模型权重的更新。这意味着“瞬时”从特定的向后通行证的梯度实际上可能指向更新的方向。 作者提出了一个简单的更改：他们建议忽略优化器的任何更新，而优化器的任何更新与最新后退的相反符号。换句话说，他们建议仅应用与当前梯度保持一致的更新，从而使更新更稳定，并且与最新数据一致。他们发现，这种小的调整可以大大加快训练的速度。 这是一个有趣的想法，虽然我很好奇它是如何发挥作用的，但我将等待独立的复制，然后才完全相信。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ahmedmastafa16     &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j33lm7/r_cautious_optimizers_improving_improving_training_with_with_one/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j33lm7/r_cautious_optimizers_improving_training_with_one/</guid>
      <pubDate>Tue, 04 Mar 2025 05:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[R]具有非高斯可能性的高斯过程的集成梯度归因</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j337qu/r_integrated_gradient_attribution_for_gaussian/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi reddit， 我一直在做这项兼职，很喜欢一些反馈 - 无需退缩，请随时告诉我，如果您认为这应该为crackpot Science标记：  paper： https://arxiv.org/pdf/2205.12797     代码： https://github.com/sarems/sarems/sarems/iggp    这个想法是应用 稀疏变异高斯过程具有非高斯的可能性/观察值。我在可能的情况下衍生了封闭形式的公式，并使用了taylor近似 /高斯 -   - 高正交正交正交（定理1）。  此外，当使用高斯过程模型时，我正在研究集成梯度的完整性属性（属性的总和=模型输出的差异），而不是原始工作（Theorem 2）。提交由＆＃32; /u/sarems     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j337qu/r_integrated_gradient_attribution_for_gaussian/</guid>
      <pubDate>Tue, 04 Mar 2025 04:59:10 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j1hc0o/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Mar 2025 03:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>