<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 26 Feb 2024 21:12:46 GMT</lastBuildDate>
    <item>
      <title>[P] OpenHermesPreferences - RLAIF 和 DPO 的 1M AI 偏好数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0pmxh/p_openhermespreferences_a_dataset_of_1m_ai/</link>
      <description><![CDATA[    &lt; /a&gt;  ​ https://preview.redd.it/euzo21cl9zkc1.jpg?width=1792&amp;format=pjpg&amp;auto= webp&amp;s=28a29a944aedc59513554ee614fd479cd8de4b7f 大家好，我很高兴分享我们与 Argilla 团队创建的包含约 100 万个 AI 偏好的大型数据集大规模测试 RLAIF 和 DPO 等技术： https://huggingface.co/datasets/ argilla/OpenHermesPreferences 我们是如何创建它的？ 为了构建 OpenHermesPreferences，我们从 Teknium 的高质量 OpenHermes-2.5 开始（大部分）GPT-4 生成的补全数据集。过滤掉多轮对话后，这为我们提供了大约 100 万条提示。 接下来，我们使用 Mixtral 和 Nous-Hermes-2-Yi。这对于本机“transformers”代码来说有点棘手，因此我们使用 distilabel 和 llm-swarm 与 vLLM 和 TGI 集成 - 这将生成时间从几天缩短到几小时！ 最后，我们使用以下方法对所有 3 个完成情况进行了排名AllenAI 的 PairRM 模型：这是一个轻量级奖励模型，专门针对 N 路比较进行训练，并且运行推理速度非常快. 我可以用它做什么？ 我们看到这个数据集的两个主要应用：  训练杀手PPO / ReST 等技术的奖励模型 将 DPO 及其朋友应用于现有的 SFT 模型  我们希望该数据集将有助于社区的研究工作，以了解人工智能的作用语言模型对齐的反馈。享受吧！ ​ ​   由   提交/u/lewtun  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0pmxh/p_openhermespreferences_a_dataset_of_1m_ai/</guid>
      <pubDate>Mon, 26 Feb 2024 19:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何设计一个写作辅助系统，可以像 Grammarly 进行语法检查一样进行写作风格指南检查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ow9v/p_how_to_design_a_writing_assistive_system_that/</link>
      <description><![CDATA[我有一个样式指南，指定标点符号的使用、行长度以及如何分解行（例如在介词之前）。我有一个包含原始文本的数据集，并且文本的版本符合此样式指南。 现在，我如何设计一个系统，可以向用户建议如何使给定的文本符合样式指导？更具体地说，  我如何构建这个任务？这是一种序列到序列建模任务吗？ 使用我的数据微调 LLM 是一个好方法吗？如何处理幻觉？ 这个系统如何告诉用户推荐中使用了哪些风格指南规则？  任何研究方向论文或类似的系统设计将不胜感激！   由   提交/u/Great_East4325   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ow9v/p_how_to_design_a_writing_assistive_system_that/</guid>
      <pubDate>Mon, 26 Feb 2024 18:34:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪些研究主题正在趋于（接近）饱和性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0odta/d_what_reseach_topics_are_converging_to_a_nearly/</link>
      <description><![CDATA[鉴于人工智能的快速发展，哪些主题即将被解决？基准测试 神经机器翻译？ 图像级对象检测和分割？  没有严重遮挡的人体姿势估计？ 你觉得怎么样   由   提交 /u/xiikjuy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0odta/d_what_reseach_topics_are_converging_to_a_nearly/</guid>
      <pubDate>Mon, 26 Feb 2024 18:14:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM Transformer 中 KV 缓存如何有效</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ob2m/d_how_kv_cache_is_valid_in_llm_transformer/</link>
      <description><![CDATA[看到很多文献提到使用 KV 缓存作为转换器模型来减少解码器中的计算，但根据我的理解，当序列达到最大上下文长度并且每次左移都会使最左边的令牌超出范围，KV 缓存将失去有效性，显然是因为先前参与的令牌消失了，这是正确的吗？  &amp;# 32；由   提交 /u/victordion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ob2m/d_how_kv_cache_is_valid_in_llm_transformer/</guid>
      <pubDate>Mon, 26 Feb 2024 18:11:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 差分隐私的实践者 - 您使用过的典型 epsilon 值是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ns07/d_practitioners_of_differential_privacy_whats_the/</link>
      <description><![CDATA[很难找到差分隐私的典型或可接受的 epsilon 范围。在线搜索尚未产生可引用或一致的结果。一些消息来源称典型值为 1.0 到 10.0。 我个人发现：1) 差分隐私显着降低了训练速度近 10 倍 2) 通常我必须使用 1.0 - 5.0 的 epsilon 数字来进行训练结果仍然具有所需的准确性水平 3) 要么我必须显着增加数据集大小，要么我必须牺牲 epsilon (隐私) 才能拥有仍然准确的模型 4) 在执行成员推理任务时，我发现增加数据集大小与减少 epsilon 具有相同的影响。 例如： 1) 在数据集大小为 200K 个示例的情况下，使用 epsilon 1.0 的 DP 模型的性能与不使用 dp 的模型完全相同在准确性和隐私评估任务方面，例如隶属度推断。没有 DP 悬停的模型需要 30 分钟才能训练，而有 DP 悬停的模型则需要 6 小时。 2) 对于小于 100K 的数据集大小，DP 模型根本不够准确，除非我将 epsilon 增加到 5.0。 这让我思考：1) 如果我必须增加 DP 的意义是什么epsilon 一直到 5，模型才有用 2) 如果使用 200K，成员推理攻击没有区别，为什么我还要费心使用 DP？ DP 的实际好处到底是什么？增加数据集大小在隐私评估任务中具有相同的效果，并且计算成本只占一小部分，超出了“理论保证”？   由   提交/u/shengy90  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ns07/d_practitioners_of_differential_privacy_whats_the/</guid>
      <pubDate>Mon, 26 Feb 2024 17:51:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于使用 Transformer Encoder-Decoder 生成嵌入的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generating_embeddings_with/</link>
      <description><![CDATA[我目前正在尝试生成我正在使用的异构数据源的嵌入。问题是，对于每个样本，我都有不同数量的输入特征。对于感兴趣的人来说，这个问题是生物学的。特别是，我正在尝试计算该数据的基因级表示，对于每个基因，我都有不同数量的变体，每个变体都有一个与其关联的特征值。  我均匀化这些特征的解决方案是使用变压器编码器生成嵌入。我通过获取每个基因的所有变异值并将它们连接起来来生成我的特征集。然后，我用零填充生成的张量的边缘，使信号位于中心，并且生成的张量为 1,024 维。编码器学习 256 维表示，解码器将其重建回 1,024 维。 MSE 迅速下降，约 30 个 epoch 后收敛。然后，我使用得到的 Transformer 的经过训练的编码器部分来生成 256 维嵌入。我面临的问题是 1）嵌入值相当小（数量级 ~ 10^(-2)）和 2）样本/基因之间的方差非常低。当我现在将这些嵌入用于我的预测任务时，可以理解的是，我的模型无法学习任何有意义的东西，并且每个预测任务都只输出 0。  需要注意的一些事情：1）我已经使用其他数据成功地训练了相同的架构。这些数据是基因级别的，因此我怀疑问题在于变异嵌入，而不是架构。2）我已经检查了我的神经网络中每一层的输入发生的情况，并且正如所怀疑的那样，线性层+激活不会导致 logits 发生实质性变化，即它们保持在 10^(-2) 大小左右。 3) 我尝试对我的数据进行 minmax 标准化，以强制其超出 10(-2) ）的大小，但无济于事。这里样本之间的微小差异就是原因。在 0 和 1 之间进行最小最大归一化之后，我们有 1 或 2 个接近 1 的值，然后其余样本很快下降到接近 0，同样方差很小。  任何尝试过类似方法的人的建议、参考或经验将不胜感激！    由   提交 /u/Primary-Wasabi292    reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generate_embeddings_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generating_embeddings_with/</guid>
      <pubDate>Mon, 26 Feb 2024 17:44:54 GMT</pubDate>
    </item>
    <item>
      <title>对于新晋研究科学家来说，该行业不会“复苏”[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</link>
      <description><![CDATA[      今天的热门话题问：“科技行业还没有复苏吗？我有那么糟糕吗？” 让我做出一个大胆的预测（我希望我是错的，但我不认为我是错的）：这个行业不会“ “恢复”对于新晋研究科学家： 您的机器学习论文数量呈指数级增长，反映出博士生和博士后数量呈指数级增长： ​ &lt; p&gt;https://preview.redd.it/viv6l1gnkykc1。 png?width=899&amp;format=png&amp;auto=webp&amp;s=04e227dede42f7d46d1941fc268bb7ea0a409a04 ...毕业并开始竞争大致固定数量的井- 支付行业研究职位。这些职位的数量可能会季节性增加或减少，但长期趋势是他们的就业前景将变得越来越差，而这种指数趋势仍在持续。 ​  div&gt;  由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</guid>
      <pubDate>Mon, 26 Feb 2024 17:24:08 GMT</pubDate>
    </item>
    <item>
      <title>[N] 科技巨头正在开发他们的人工智能芯片。这是清单</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ira9/n_tech_giants_are_developing_their_ai_chips_heres/</link>
      <description><![CDATA[NVIDIA GPU 短缺，导致多家公司创建自己的 AI 芯片。以下是这些公司的列表： • Google 处于改进张量处理单元 (TPU) 的前沿https://cloud.google.com/tpu?hl=en Google Cloud 技术。 • OpenAI 正在研究设计专有 AI 芯片的潜力https://www.reuters.com/ technology/chatgpt-owner-openai-is-exploring-making-its-own-ai-chips-sources-2023-10-06/。 • 微软宣布 https://news.microsoft.com/source/ features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/ 两款定制设计的芯片：用于大型语言模型训练和推理的 Microsoft Azure Maia AI 加速器以及用于大型语言模型训练和推理的 Microsoft Azure Maia AI 加速器Azure Cobalt CPU，用于 Microsoft 云上的通用计算工作负载。 • 亚马逊推出了 Inferentia AI 芯片 https://aws.amazon.com/machine-learning/inferentia/ 和第二代机器学习 (ML) 加速器 AWS Trainium https://aws.amazon.com/machine-learning/trainium/。 • Apple 一直在开发其系列定制芯片并推出https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal -computer/ M3、M3 Pro 和 M3 Max 处理器，可扩展到专门的 AI 任务。 • Meta 计划部署新版本的定制芯片，旨在支持其人工智能据路透社报道，人工智能（AI）的推动 https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/ . • 据报道，华为https://www.reuters.com/technology/ai-chip-demand-forces-huawei-slow-smartphone-product-sources-2024-02-05/由于人工智能芯片的需求，人工智能并放慢了高端 Mate 60 手机的生产 https://www.hisilicon.com/ en/products/ascend 飙升。 我错过了什么吗？   由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ira9/n_tech_giants_are_developing_their_ai_chips_heres/</guid>
      <pubDate>Mon, 26 Feb 2024 14:25:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是科技行业还没复苏还是我太差了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</link>
      <description><![CDATA[我是欧洲顶尖大学的一名应届博士毕业生，正在研究 ML/CV 中的一些热门主题，已发表 8 - 20 篇论文，其中大部分是我的第一作者。这些论文已累计被引用1000-3000次。 （使用新帐户和广泛的范围来保持匿名） 尽管我认为自己是一个相当有实力的候选人，但我在最近的求职过程中遇到了重大挑战。我主要瞄准研究科学家职位，希望从事开放式研究。我已经联系了欧洲、中东和非洲地区的许多高级机器学习研究人员，虽然有些人表达了兴趣，但不幸的是，由于各种原因（例如人员有限或招聘经理没有更新信息），没有一个机会成为现实。 我主要针对大型科技公司以及一些最近流行的机器学习初创公司。不幸的是，我的大部分申请都被拒绝了，而且常常没有面试的机会。 （我只接受过一家大型科技公司的一次面试，然后就被拒绝了。） 特别是，尽管有朋友的推荐，我还是立即遭到了 Meta 的研究科学家职位拒绝（几天之内）。我现在只是非常困惑和不安，不知道出了什么问题，我是否被这些公司列入了黑名单？但我不记得我树敌过。我希望就下一步可以做什么寻求一些建议......   由   提交/u/Holiday_Safe_5620   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</guid>
      <pubDate>Mon, 26 Feb 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>目前关于使用 TensorFlow 2.x 与 PyTorch 的共识是什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0gxy7/whats_the_current_consensus_on_using_tensorflow/</link>
      <description><![CDATA[我知道许多人在 TF1 到 TF2 迁移期间离开了 TF，并且再也没有回头。我的问题是，目前关于使用 TF2 与 PyTorch（与 Jax）的共识是什么？为什么？  从端到端的角度来看，对我来说，TF2 很好。 PyTorch 上的调试更容易，但好处还不足以放弃所有内容并永远保留 TF2。你们怎么看？ 假设您正在构建人工智能产品并且部署是必须的，您在代码库中更喜欢 TensorFlow 还是 Pytorch（或 JAX），为什么？    由   提交 /u/1infiniteloop   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0gxy7/whats_the_current_consensus_on_using_tensorflow/</guid>
      <pubDate>Mon, 26 Feb 2024 13:01:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] YOLOv9：使用可编程梯度信息学习你想学的东西</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ep3a/r_yolov9_learning_what_you_want_to_learn_using/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.13616 代码：https://github .com/WongKinYiu/yolov9 模特：https://huggingface. co/merve/yolov9 演示：https://huggingface .co/spaces/kadirnar/Yolov9 Colab：https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/ notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb 摘要：  今天的深度学习方法重点关注如何设计最合适的目标函数，使得模型的预测结果能够最接近真实情况。同时，必须设计一个适当的架构，可以帮助获取足够的信息进行预测。现有方法忽略了一个事实，即当输入数据经过逐层特征提取和空间变换时，大量信息将会丢失。本文将深入研究数据通过深度网络传输时数据丢失的重要问题，即信息瓶颈和可逆函数。我们提出了可编程梯度信息（PGI）的概念来应对深度网络实现多个目标所需的各种变化。 PGI可以为目标任务计算目标函数提供完整的输入信息，从而获得可靠的梯度信息来更新网络权值。此外，还设计了一种基于梯度路径规划的新型轻量级网络架构——通用高效层聚合网络（GELAN）。 GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用传统的卷积算子即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得从头训练模型能够比使用大数据集预训练的state-of-the-art模型获得更好的结果，比较结果如图1所示。源代码是位于：此 https URL。   &amp;# 32；由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ep3a/r_yolov9_learning_what_you_want_to_learn_using/</guid>
      <pubDate>Mon, 26 Feb 2024 10:50:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPTFast：将拥抱脸部变形金刚加速 6-7 倍。原生于 Hugging Face 和 PyTorch。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0eixe/p_gptfast_accelerate_your_hugging_face/</link>
      <description><![CDATA[GitHub：https:// github.com/MDK8888/GPTFast GPTFast 使用 GPTFast 将拥抱脸部变形金刚加速 6-7 倍！ 背景 GPTFast最初是开发的一套技术由 PyTorch 团队开发，旨在加快 Llama-2-7b 的推理速度。这个 pip 包将这些技术推广到所有 Hugging Face 模型。   由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0eixe/p_gptfast_accelerate_your_hugging_face/</guid>
      <pubDate>Mon, 26 Feb 2024 10:39:41 GMT</pubDate>
    </item>
    <item>
      <title>“不要停止预训练”只是微调吗？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</link>
      <description><![CDATA[不要停止预训练论文吹嘘通过“领域适应预训练”来提高法学硕士的表现，但这似乎是微调的另一个词，一点也不新鲜。我肯定错过了一些东西 - 它是什么？   由   提交/u/MLenthusiast34   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</guid>
      <pubDate>Mon, 26 Feb 2024 08:09:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否值得从 TensorFlow/PyTorch 切换到 JAX？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/</link>
      <description><![CDATA[大家好！我看到 JAX 越来越多地出现，例如Google Deepmind 在 JAX 中发布了他们的 Gemma 开源模型。我目前使用 TensorFlow/PyTorch。 JAX 值得查看吗？它能以与 TensorFlow/PyTorch 相同的灵活性完成相同的任务吗？   由   提交 /u/Few-Pomegranate4369   /u/Few-Pomegranate4369 reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/</guid>
      <pubDate>Mon, 26 Feb 2024 04:28:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>