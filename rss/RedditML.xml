<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Tue, 11 Mar 2025 18:24:32 GMT</lastBuildDate>
    <item>
      <title>[r]大语言模型的对比度蒸馏：利用教师 - 学生响应协同作用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8w6a5/r_contrastive_distillation_for_large_language/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Distillm-2纸引入了针对大语言模型的对比蒸馏方法，该方法在以前的方法上显着改善。关键创新是加权对比逻辑蒸馏（WCLD），它在知识蒸馏过程中使用对比度学习来帮助学生模型更好地区分良好和不良反应。 该技术通过以下方面来工作： - 在以下方面使用： - 对教师模型进行微调，同时培训教师的响应和有意的反对性 - 既有培训 - 既有培训 - 既有训练的反对性 - 又有故障的反对性 -  that emphasizes differences between correct and incorrect outputs Key results: - Student models achieve up to 99% of teacher performance while being 3-10x smaller - 2-3x inference speedups compared to teacher models - Consistently outperforms previous distillation methods across multiple benchmarks - Successfully distilled models from Llama-2 70B down to 1.3B parameters - Particularly effective when the size gap between teacher and student is large 我认为这种方法解决了LLM部署中最紧迫的问题之一 - 运行最新模型的资源要求。创建较小模型以保留其较大对应物的所有功能的能力可以使访问高级AI功能的访问并能够在资源受限受限的设备上有效部署。 对比性学习角度特别有趣，因为它表明，理解使输出的理解与知道是什么同样重要的对比度。这反映了人类的学习方式，并可以指出不仅蒸馏的更有效的训练范例。 最有希望的是该技术似乎如何扩展不同的模型尺寸和体系结构。如果这些结果在生产环境中成立，我们可以看到朝着更小，更高效的模型转变，这些模型在能力方面不会太多牺牲。  tldr：Distillm-2使用对比度学习来创建较小，更快的LLM，可保留多达99％的教师模型表现，并具有2-3x的速度，具有最小的质量损失。 href =“ https://aimodels.fyi/papers/arxiv/distillm-2-contrastive-apphack-boost-boost-distillation-llms”&gt;完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8w6a5/r_contrastive_distillation_for_large_language/</guid>
      <pubDate>Tue, 11 Mar 2025 17:22:23 GMT</pubDate>
    </item>
    <item>
      <title>[d]如何练习大熊猫来变得更好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8v58u/d_how_do_practice_pandas_to_get_better_at_it/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  练习熊猫的最佳方法是什么？ 我目前正在学习数据科学，我学到了熊猫和努比的基础知识，我想练习这些概念以变得更好，以便有人可以告诉我在哪里可以练习这些主题？会很感激  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solid_strain7134      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j8v58u/d_how_how_do_do_do_do_practice_pandas_to_to_get_better_better_atter_at_it/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8v58u/d_how_do_practice_pandas_to_get_better_at_it/</guid>
      <pubDate>Tue, 11 Mar 2025 16:40:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML论文中的数学</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8t7lc/d_math_in_ml_papers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好， 我是一个相对较新的研究人员，我遇到了对我来说似乎很奇怪的东西。 我正在阅读一篇文章，称为“域名to＆quot for Neural Networks of Neural Networks”它有很多数学。与我遇到的其他一些论文类似（例如，一张瓦斯特坦·盖纸），作者编写方程式符号，设置发行版等。 在我看来，这些论文中的数学是“象征性的”。这意味着这些方程式很可能不会在代码中的任何地方实现。它们是为了让读者有一种感觉为什么会起作用，但实际上并没有在实施中发挥作用。这对我来说很奇怪，因为口头描述至少对我而言会更好。 他们感觉就像是“好事” but one could go on to the implementation without it. Just wanted to see if anyone else gets this feeling, or am I missing something? Edit : A good example of this is in the WGAN paper, where the go though all that trouble, with the earth movers distance etc etc and at the end of the day, you just remove the sigmoid at the end of the discriminator (critic), and remove the logs from the loss.可以通过声称新的衍生品并不那么陡峭，可以直观地解释所有这些。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ripototo   href =“ https://www.reddit.com/r/machinelearning/comments/1j8t7lc/d_math_in_in_ml_papers/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8t7lc/d_math_in_ml_papers/</guid>
      <pubDate>Tue, 11 Mar 2025 15:19:46 GMT</pubDate>
    </item>
    <item>
      <title>[P]您是否会使用浏览器扩展名来立即对ML纸的难度和实施时间进行评分？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8skm1/p_would_you_use_a_browser_extension_that/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ AI/ML工程师/研究人员/从业人员：我正在考虑构建一个镀铬扩展：  立即分析ML/AI论文，并将其复杂性从“实施可以使用”中的复杂性评估。在“  ”估计中估计您需要花费多少小时（基于您的背景）   突出一份论文是否具有实际实施潜力，或者是理论上是理论上的 表明，在尝试实施之前，您需要的是                    在建立此内容之前，没有或零实际实现值。 ：这会为您解决一个真正的问题吗？您多久发现自己浪费时间在论文上，您以后才意识到不值得付出的努力？ 我专门针对行业中需要保持最新的个人，但不能浪费数小时的不切实际的研究。  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/gigicr1   href =“ https://www.reddit.com/r/machinelearning/comments/1j8skm1/p_would_you_use_a_a_a_a_a_ extension_extension_that/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8skm1/p_would_you_use_a_browser_extension_that/</guid>
      <pubDate>Tue, 11 Mar 2025 14:52:20 GMT</pubDate>
    </item>
    <item>
      <title>[D]加强学习或GPU编程：2025年更有用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8qgwt/d_reinforcement_learning_or_gpu_programming_whats/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图扩大我的知识（没有特定的原因，只是普遍兴趣），我对这两个主题一无所知。  我应该做什么？我知道这是一个广泛的问题，但我只是想在空闲时间中找到一些要做的事情，以提高未来的技能  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ailexb     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8qgwt/d_reinforcement_learning_or_gpu_programming_whats/</guid>
      <pubDate>Tue, 11 Mar 2025 13:15:22 GMT</pubDate>
    </item>
    <item>
      <title>[D]我们可以从MAMBA层参数得出注意力图吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8ne9n/d_can_we_derive_an_attention_map_from_mamba_layer/</link>
      <description><![CDATA[I&#39;ve been exploring Mamba (the state space model-based architecture) and was wondering if it&#39;s possible to compute an attention map using its layer parameters, specifically by applying a transformation on the B and C matrices. From my understanding, these matrices project the input into the latent state space (B) and extract the output (C).鉴于曼巴（Mamba）在没有明确注意的情况下有效地捕获了远距离依赖性，我们是否可以通过计算相似性度量（例如，通过双线性转换或B和C上的其他操作）来解释类似注意力的结构？提交由＆＃32; /u/u/blooming17     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j8ne9n/d_can_we_derive_andention_map_map_from_mamba_layer/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8ne9n/d_can_we_derive_an_attention_map_from_mamba_layer/</guid>
      <pubDate>Tue, 11 Mar 2025 10:11:35 GMT</pubDate>
    </item>
    <item>
      <title>[d]是否比Deberta-V3-Small更好的互写模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8injn/d_any_crossencoder_model_better_than/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经过时了几年。寻找更有效的（性能和准确性）和更近期的模型。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/upter-angel-301     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8injn/d_any_crossencoder_model_better_than/</guid>
      <pubDate>Tue, 11 Mar 2025 04:25:22 GMT</pubDate>
    </item>
    <item>
      <title>[d] L1正则化如何执行特征选择？ - 使用多项式模型寻求直观的解释</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8gvlh/d_how_does_l1_regularization_perform_feature/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   l1正则化在模型中引起稀疏性，从而降低了其复杂性和方差。它确实执行功能选择，将“冗余”功能的参数迫使零。我正在尝试搜索有关L1正则化如何选择必须为零的系数/参数的解释。 才能使事情变得简单，我正在考虑一个多项式回归模型。如果将其在数据集上训练，该数据集的样品（带有2D线的样品（带有一些噪声）），并且该模型包含更多参数（例如7），则该模型将清楚地过度贴上数据并因其增加的功率而学习噪声。在这种情况下，我们期望L1正则化将所有功能的参数归零3至7（x  3  to x  7），因为它们是多余的。 可以使参数更近地查看与MSE目标函数的零元素（ly）termert（say lya），我将参数与MSE目标函数（ly）置于lim-eve n-tement（ly）。在设置L W.R.T.的部分衍生物时一个参数θj为零，并重新排列了术语，我最终以此表达方式，  1/n * ∑ yi -f（xi，θ） * x  j_i  j_i  =λsgn（θjj）  lhs上的术语表示LHS上的术语，代表了lhs的covariance and incunials and incunt and incut and incut and incut and incut特征。如果某个特征是冗余的，即其与残差的协方差为零，则RHS上的SGN（θj）被迫零，因此迫使θj降至零。 我试图验证我对我的解释，但找不到相关的来源来验证。将协方差与正则化和特征选择联系起来似乎很雄心勃勃，但是我想以较少数学上的依从方式解释L1正则化零功能如何向同事散发出冗余特征。 这种解释是否有效和数学正确？另外，我遇​​到了一个事实，即用设计，通过设计构建的模型，残差和输入之间的协方差为零。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shubham0204_dev    href =“ https://www.reddit.com/r/machinelearning/comments/1j8gvlh/d_how_does_does_l1_regularization_regularization_perform_feature/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8gvlh/d_how_does_l1_regularization_perform_feature/</guid>
      <pubDate>Tue, 11 Mar 2025 02:46:55 GMT</pubDate>
    </item>
    <item>
      <title>[r]时间序列中的虚假回归：为什么错误术语的自相关很重要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j8dehr/r_spurious_regressions_in_time_series_why_does/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您是否曾经运行过时间序列回归，看到了高r² ，以及思想，; ＆quot，我的模型是可靠的！  在我有关数据科学的最新文章中，我深入研究了 spurious回归  - 一个经典的计量经济学陷阱，高度自相关的变量创建幻觉关系。 。 Newbold（1974）和 python模拟，我分解：  为什么发生浪费回归 如何检测    他们（hint：durbin-watson：durbin-watson是关键！）键   [ https://towardsdatascience.com/linear-regression-linear-regressign-megression-melecression-implime-lime-sim-series-series-souries-sourise-sources-sources-of-spurious-regress/rious--spurious-regression/]您是否在工作中遇到过虚假的回归？您如何处理它们？让我们讨论！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/north-kangaroo-4639       [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j8dehr/r_spurious_regressions_in_time_series_why_does/</guid>
      <pubDate>Mon, 10 Mar 2025 23:56:37 GMT</pubDate>
    </item>
    <item>
      <title>[d]建立两个阶段推荐系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j88orj/d_building_twostage_recommendation_systems/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在为ML系统设计采访做准备，而我正在准备的主题之一就是推荐系统。我知道什么是协作性和内容过滤，我了解诸如DLRM和两个塔模型之类的模型的工作，我知道矢量DBS，我知道典型的两阶段架构首先是候选人生成的，然后是排名，我想这些都以某种方式捆绑在一起。 ，但是我很难理解所有这些都可以使所有事物都融合在一起，从而使我无法找到一个很好的材料，我可以找到一个很好的材料，我很难找到一个材料。具体来说，通常为每个步骤使用哪些模型？我可以在两个阶段使用DLRM/2T吗？如果是，为什么？如果没有，我还应该使用什么？这些模型是否适合协作/内容过滤，还是不是这样分类的？典型的设置是什么样的？对于候选人的生成，我是否使用我对所有可能的项目（例如视频）的任何模型，还是有办法将输入限制为候选人生成步骤？我看到一些资源使用2T来学习嵌入候选人生成中，但是在排名阶段应该发生什么？这一切都使我感到困惑。 我希望这些问题有意义，我希望有帮助的答案：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j8888orj/d_building_twostage_twostage_recommendation_systems/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j88orj/d_building_twostage_recommendation_systems/</guid>
      <pubDate>Mon, 10 Mar 2025 20:35:44 GMT</pubDate>
    </item>
    <item>
      <title>[d]人们如何与大学研究部门合作而不会入学？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j84at2/d_how_do_people_partner_with_university_research/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我认识一个与德国大学研究系合作的人，即使他不是那里的学生。这让我想知道 - 人们如何设法与大学研究团队合作而没有正式入学？ 是否有特定的计划，行业伙伴关系或开放研究计划可以允许外部个人做出贡献？教授通常会收到来自独立研究人员或专业人士的冷电子邮件吗？他没有在那里注册，但他正在积极贡献。 我很感谢那些做过此操作或知道它是如何工作的人的见解！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j84at2/d_how_do_do_do_people_partner_partner_with_with_university_research/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1j84at2/d_how_do_do_do_people_partner_partner_with_university_research/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j84at2/d_how_do_people_partner_with_university_research/</guid>
      <pubDate>Mon, 10 Mar 2025 17:33:48 GMT</pubDate>
    </item>
    <item>
      <title>[P]量子进化核（开源，基于量子的，图形机学习）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j812sr/p_quantum_evolution_kernel_opensource/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi，我很自豪地宣布，我们刚刚发布了开源库 y  y  y专为有兴趣将量子computin  g应用于 Graph Machine Learning   - 您甚至不需要量子计算机即可开始使用它！如教程所示，它具有广泛的图形机学习应用，包括分子毒性的预测。 💡为什么它令人兴奋？量子计算具有巨大的潜力，但是它必须可访问且实践 L才能产生真正的影响。该库是迈向建立量子工具生态系统的一步，研究人员，开发人员和创新者可以在今天开始使用。    加入社区     ！我们正在建立一个开放的生态系统，开发人员，研究人员和爱好者可以实验，贡献和塑造量子计算的未来。提交由＆＃32; /u/u/imyoric     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j812sr/p_quantum_evolution_kernel_opensource/</guid>
      <pubDate>Mon, 10 Mar 2025 15:20:48 GMT</pubDate>
    </item>
    <item>
      <title>[P]功能工厂：生锈的功能工程库🦀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j80ju5/p_feature_factory_a_feature_engineering_library/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在开发一个为RUST的开源源功能工程库，称为“功能工厂”。该图书馆建立在Apache DataFusion的顶部，仍处于开发的早期阶段，但是其核心API融合在一起，许多主要功能已经实现。 我在这里发布此公告，以获取社区的一些反馈，并查看是否有人对项目有兴趣。我仍在学习生锈，所以我感谢建议改进代码和设计的建议。  project的github repo： ＆＃32;提交由＆＃32; /u/u/no_pomegranate7508      [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j80ju5/p_feature_factory_a_feature_engineering_library/</guid>
      <pubDate>Mon, 10 Mar 2025 14:57:49 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j1hc0o/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Mar 2025 03:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>