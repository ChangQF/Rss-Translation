<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Mon, 01 Jan 2024 06:18:45 GMT</lastBuildDate>
    <item>
      <title>机器学习算法中的假设 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vqkpc/assumptions_in_machine_learning_algorithms_d/</link>
      <description><![CDATA[大家好！ ​ 我真的是一个新手或初学者机器学习，我目前正在尝试做一个二元分类项目。我想使用某些算法，例如随机森林和逻辑回归。随机森林没有问题，但我在网上读到逻辑回归需要满足某些假设，例如独立连续变量和因变量的 logit 之间的线性关系。  ​ 我想知道，在在线教程或课程中，我从未遇到过人们在将数据拟合到逻辑回归模型之前检查假设。那么，是否建议我检查假设，如果不这样做，违反所述假设会产生什么后果，例如误导性系数导致误导性特征重要性？   由   提交/u/chim_72  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vqkpc/assumptions_in_machine_learning_algorithms_d/</guid>
      <pubDate>Mon, 01 Jan 2024 06:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] VerificationGPT - 使用 Brave Search 和 arXiv 对 GPT-4 进行开源验证</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vq7ov/p_verificationgpt_open_source_verification_for/</link>
      <description><![CDATA[       由   提交​​ /u/contextfund   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vq7ov/p_verificationgpt_open_source_verification_for/</guid>
      <pubDate>Mon, 01 Jan 2024 05:51:36 GMT</pubDate>
    </item>
    <item>
      <title>[d] 为聊天机器人微调 llama2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vq3fi/d_fine_tuning_llama2_for_chatbot/</link>
      <description><![CDATA[我是新手，所以如果我问任何看似愚蠢的问题，请耐心等待。  我一直致力于微调 Llama2，以创建适合特定用例的聊天机器人。我有问答格式的数据，并且我正在以适合 Llama2 的提示格式提供数据，使用  等标签。 [插入]  &lt;/s&gt;。对于微调过程，我正在利用 Hugging Face 生态系统。目前，我有 243 个培训提示。这是否足够，或者我应该考虑添加更多提示？我问的原因是微调后，模型没有生成适当的响应。 我是否应该探索调整超参数，或者有人有其他建议吗？任何帮助将不胜感激。   由   提交/u/Lethal_Protector_404   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vq3fi/d_fine_tuning_llama2_for_chatbot/</guid>
      <pubDate>Mon, 01 Jan 2024 05:43:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI操作员手册</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vp5yo/d_the_ai_operators_handbook/</link>
      <description><![CDATA[大家好，我为对操作系统人工智能系统感兴趣的人们写了一篇博客文章。我试图将我在 MLOps 方面的经验提炼成一些原则，希望能够帮助人们理解人工智能系统。未来几年，我们将从几千个人工智能运营商发展到可能有数百万个人工智能运营商。这篇文章是为了帮助他们抢占先机，希望他们不要犯我见过或犯过的一些错误。 ai-operators-handbook-0fa3f4d387f8&quot;&gt;https://medium.com/@unintended Purposes/the-ai-operators-handbook-0fa3f4d387f8  &amp; #32；由   提交/u/unintend_ Purposes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vp5yo/d_the_ai_operators_handbook/</guid>
      <pubDate>Mon, 01 Jan 2024 04:42:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 目前图像到视频生成的最新技术是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vor04/d_what_is_the_current_stateoftheart_on_image_to/</link>
      <description><![CDATA[我有一个宠物项目，可以制作我自己读的小故事，或者由 OpenAI 语音朗读给我的孩子们听。它还可以实际操作，让我调整每页的故事和初始图像，但其他一切都是为我生成的（故事、来自 Mixtral 的图像描述、来自 OpenAI TTS 的旁白、来自 DALL-E 的图像、来自 Stability AI 的图像的视频）。例如，这是一个这样的故事：https://adventure-genie.pages.dev/6 我尝试放慢视频速度，添加乒乓效果来稍微隐藏循环，但是尽管故事、TTS 和初始图像的质量都相当不错，但视频却是不可思议的。 有什么方法可以让我生成不会给我的孩子带来噩梦的图像？我不介意弄脏自己的手，但我自己没有 GPU，因此欢迎使用 API，或者可以在 CPU 上正常运行而无需花费太长时间的自托管工具也可以。 &lt;作为一个更普遍的问题：它似乎可以突破新颖性的障碍，而不是为视频生成领域提供实际价值？扩散模型对于单个图像非常有用，变形金刚对于语言非常有用，但我仍然没有在图像到视频类别中找到使我成为任何方法的真正信徒的东西。   由   提交 /u/maccam912   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vor04/d_what_is_the_current_stateoftheart_on_image_to/</guid>
      <pubDate>Mon, 01 Jan 2024 04:15:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士会完全取代外语翻译服务吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18voi9s/d_will_llms_completely_replace_foreign_language/</link>
      <description><![CDATA[传统上，外语翻译服务似乎依赖于语法和意义的绝对解构和重建，但像 OpenAI 这样的法学硕士似乎能够处理这没有任何复杂性。 这是否意味着法学硕士非常适合这种替代，因此传统的语言翻译技术不再适用？   由   提交 /u/lorenzomofo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18voi9s/d_will_llms_completely_replace_foreign_language/</guid>
      <pubDate>Mon, 01 Jan 2024 04:00:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] TinyGPT-V：通过小骨干的高效多模态大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vngf4/r_tinygptv_efficient_multimodal_large_language/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2312.16862 代码：https://arxiv.org/abs/2312.16862 代码： com/DLYuanGod/TinyGPT-V&quot;&gt;https://github.com/DLYuanGod/TinyGPT-V 模型：https://huggingface.co/Tyrannosaurus/TinyGPT-V 摘要： &lt; blockquote&gt; 在高级多模型学习时代，GPT-4V 等多模态大语言模型 (MLLM) 在连接语言和视觉元素方面取得了显着的进步。然而，闭源性质和大量的计算需求给普遍使用和修改带来了显着的挑战。这就是 LLaVA 和 MiniGPT-4 等开源 MLLM 的用武之地，它们在各个任务上取得了突破性的成就。尽管取得了这些成就，计算效率仍然是一个未解决的问题，因为这些模型（如 LLaVA-v1.5-13B）需要大量资源。为了解决这些问题，我们推出了 TinyGPT-V，这是一种将令人印象深刻的性能与普通计算能力相结合的新浪潮模型。它的突出之处在于仅需要 24G GPU 进行训练，8G GPU 或 CPU 进行推理。 TinyGPT-V 基于 Phi-2 构建，将有效的语言主干与来自 BLIP-2 或 CLIP 的预训练视觉模块结合起来。 TinyGPT-V的2.8B参数可以经过独特的量化过程，适合8G各种设备上的本地部署和推理任务。我们的工作促进了设计具有成本效益、高效且高性能的 MLLM 的进一步发展，扩展了它们在广泛的现实场景中的适用性。此外，本文提出了一种通过小主干的多模态大语言模型的新范式。我们的代码和训练权重位于：此 https URL 和 分别是这个 https URL。  ​ https://preview.redd.it/p66bxrlxsq9c1.png?width=1732&amp;format=png&amp;auto=网页&amp; s=b75366ca000d259869fc7487a4322427fa31110c   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vngf4/r_tinygptv_efficient_multimodal_large_language/</guid>
      <pubDate>Mon, 01 Jan 2024 02:52:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 数学生成人工智能：第一部分——MathPile：十亿代币规模的数学预训练语料库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vmu5z/r_generative_ai_for_math_part_i_mathpile_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.17120 数据集：https:// /huggingface.co/datasets/GAIR/MathPile 代码：https://github.com/GAIR-NLP/MathPile/ 项目页面：https://gair-nlp.github.io/MathPile/ 摘要：  高质量、大规模的语料库是构建基础模型的基石。在这项工作中，我们引入了 MathPile，这是一个多样化且高质量的以数学为中心的语料库，包含约 95 亿个代币。在整个创建过程中，我们坚持“少即是多”的原则，坚信数据质量高于数量，即使在预训练阶段也是如此。我们细致的数据收集和处理工作包括一套复杂的预处理、预过滤、语言识别、清理、过滤和重复数据删除，确保了我们语料库的高质量。此外，我们对下游基准测试集进行了数据污染检测，以消除重复。我们希望我们的MathPile能够帮助增强语言模型的数学推理能力。我们计划将不同版本的MathPile以及用于处理的脚本开源，以方便该领域未来的发展。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vmu5z/r_generative_ai_for_math_part_i_mathpile_a/</guid>
      <pubDate>Mon, 01 Jan 2024 02:14:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有关于 LLM 使用统计的好资源吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vjouu/d_are_there_good_resources_on_llm_usage_statistics/</link>
      <description><![CDATA[喜欢使用提示与微调等%？我正在写一篇关于低资源用户和公司使用法学硕士之间的差异的论文，任何有关此主题的论文都会有所帮助。   由   提交/u/rajicon17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vjouu/d_are_there_good_resources_on_llm_usage_statistics/</guid>
      <pubDate>Sun, 31 Dec 2023 23:17:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] 鲨鱼点识别模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vj1fk/p_shark_point_identification_model/</link>
      <description><![CDATA[       大家好 我对深度学习还比较陌生，目前正在开展一个学校项目来预测鲨鱼的 4 个关键点（如下图所示）通过黄线和绿线的尖端）。 目前，我已经使用 CVAT 注释了大约 300 张图像，标记了这 4 个点以及包含它们的框。现在，我对创建机器学习/神经网络算法来识别鲨鱼上的点的下一步感到困惑。 我正在寻找识别点的高准确度（75%+）。考虑到我拥有的数据量有限，我知道这个数字相当高。不过，我在网上做了一些基本的阅读，据我了解，这些点应该很容易被算法识别，因为它们位于鲨鱼身体的尖端。 我希望完成这个项目在接下来的一周里。我非常感谢使用教程或指南或任何其他资源，以最快/最有效的方式对模型进行编程以实现此目的的指导！ ​ https://preview.redd.it/s7efwbnskp9c1.png?width= 2922&amp;format=png&amp;auto=webp&amp;s=bfaefaa1d93b3af3bea241f933029d2b77c7db97   由   提交 /u/ProfessorRoJain   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vj1fk/p_shark_point_identification_model/</guid>
      <pubDate>Sun, 31 Dec 2023 22:42:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 nanoGPT 移植到 Apple 新的 MLX 框架：Macbook M3 Pro GPU 上的早期结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vhvl1/p_ported_nanogpt_to_apples_new_mlx_framework/</link>
      <description><![CDATA[嘿，机器学习爱好者们， 我一直在致力于一个令人兴奋的项目，希望与你们分享我的进展。我成功地将 Andrej Karpathy 的 nanoGPT 框架移植到 Apple 的新机器学习框架 MLX 中。这为在 Mac GPU 上运行 GPT 模型提供了一些有趣的可能性。代码：https://github.com/vithursant/nanoGPT_mlx  详细信息：  硬件： Macbook M3 Pro，配备 11 核 CPU，14-核心GPU，18GB统一内存 性能：在莎士比亚数据集上以0.37次迭代/秒的速度预训练45M参数字符级GPT-2模型。 &lt; li&gt;配置：  批量大小：64 本地批量大小：4 序列长度：256 &gt;   当前状态：  支持莎士比亚和 OpenWebText 的预训练 代码库仍在开发中。 寻找反馈、建议和潜在合作者。  向社区提出的问题：&lt; /p&gt;  是否有其他人尝试使用 MLX 并经历过类似或不同的结果？ 对于优化 Mac GPU 性能有什么建议吗？ 对潜在应用的思考或改进？  我很高兴听到您的想法，并可能与有兴趣探索 Apple MLX 功能的其他人合作。请随意查看代码并分享您的见解！   由   提交/u/brownmamba94  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vhvl1/p_ported_nanogpt_to_apples_new_mlx_framework/</guid>
      <pubDate>Sun, 31 Dec 2023 21:43:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大家的新年学习决心是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vgr2l/d_what_are_everyones_new_year_learning_resolutions/</link>
      <description><![CDATA[你们计划在 2024 年学习什么？ 对我来说，这是因果机器学习，并更深入地研究 RAG！   由   提交/u/Moist_Onion_6440   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vgr2l/d_what_are_everyones_new_year_learning_resolutions/</guid>
      <pubDate>Sun, 31 Dec 2023 20:47:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么目前的法学硕士在离散空间中效果很好，但在连续空间中效果不佳？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vfj7k/d_why_do_current_llms_work_well_in_discrete_space/</link>
      <description><![CDATA[一个有趣的观察是，LM 被训练来预测分类分布上的标记，然后使用采样算法来离散化分布以产生输出。如果我们在连续域中尝试此操作，例如，直接使用 L2 损失来预测像素，则它不起作用，输出会变得非常模糊。看来，通过采样进行离散化对于推理过程中的工作至关重要。最近的论文，如 GIVT 可以将输出建模为高斯混合而不是分类分布，但仍然需要采样才能使其发挥作用。  我确信这不是新的观察结果，有没有任何资源可以帮助解释为什么会出现这种情况？   由   提交 /u/Hyperarticles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vfj7k/d_why_do_current_llms_work_well_in_discrete_space/</guid>
      <pubDate>Sun, 31 Dec 2023 19:48:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] DeepMind 的 Beyond Human Data 论文中关于损失函数的问题。如果奖励只有 1 或 0，为什么要使用奖励加权损失，而不是仅仅针对成功进行训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v9p53/d_question_on_the_loss_function_in_deepminds/</link>
      <description><![CDATA[在论文中，他们说他们将 1 和 0 的二元奖励分配给模型的输出。如果代码成功运行，或者数学问题被解决，或者w/e，那么奖励为1。否则为0。 论文后面他们说使用奖励加权负对数似然训练损失。  如果奖励只是 0 或 1，那么这不是只是正常的负对数似然损失，而是只在成功时进行训练（奖励为零时梯度为零）？如果是这样，为什么要在解释中添加额外的复杂性？ Mods，我不确定这是否算一个简单的问题，所以让我知道我是否应该移动它。   由   提交/u/30299578815310  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v9p53/d_question_on_the_loss_function_in_deepminds/</guid>
      <pubDate>Sun, 31 Dec 2023 15:12:48 GMT</pubDate>
    </item>
    </channel>
</rss>