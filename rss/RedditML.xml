<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 17 Mar 2024 06:15:43 GMT</lastBuildDate>
    <item>
      <title>[P] 聊天机器人对心理健康的帮助</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgpu87/p_help_with_chatbot_for_mental_health_carr/</link>
      <description><![CDATA[我想构建一个用于医疗保健的聊天机器人，但我不知道如何构建它，也不知道在哪里获取 json 格式的数据以及在哪里得到它   由   提交/u/Realistic_Law7177   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgpu87/p_help_with_chatbot_for_mental_health_carr/</guid>
      <pubDate>Sun, 17 Mar 2024 05:11:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我不明白反向传播如何在稀疏门控 MoE 上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</link>
      <description><![CDATA[我不明白反向传播如何在稀疏门控 MoE 上工作 在 LLM 的背景下，假设你有 n 个专家，并且您为每个令牌选择了前 k 个。 在训练期间，门网络可能完全错误，并且将正确的专家排除在所选的 k 之外。然而，由于没有使用正确的专家，因此门没有机会增加正确专家的权重。 换句话说，在背景期间，仅更新门网络的部分参数，影响前 k 内权重的那些。 我错过了什么吗？   由   提交/u/Primary-Try8050   reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</guid>
      <pubDate>Sun, 17 Mar 2024 02:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 TicTacToe 和 Python 学习机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgjn5o/p_learning_machine_learing_with_tictactoe_and/</link>
      <description><![CDATA[我创建了一个基于 AI 的 Tic-Tac-Toe 游戏模拟的 Python 脚本，作为学习机器学习和 AI 训练技术的有趣项目。 Python 脚本使用 TensorFlow 创建和训练不同类型的神经网络模型，例如 MLP、CNN 和 RNN。 所有代码均可在我的 Github 存储库中获取 此处。 我很乐意收到您的反馈。我出于学习目的创建了此内容，您的意见无疑将为我提供重要的学习机会。我想学习 python、机器学习，但不确定策略是否正确实施以及机器学习技术是否可靠。 如果您有兴趣，可以查看该存储库。任何建议或提出的问题将不胜感激！   由   提交/u/phantagom  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgjn5o/p_learning_machine_learing_with_tictactoe_and/</guid>
      <pubDate>Sat, 16 Mar 2024 23:48:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从机器学习中的归纳偏差的角度看注意力和变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</link>
      <description><![CDATA[      Hello人们在我的 YT 频道上发布了一段有关 Transformer 的视频，以及他们为深度学习研究带来的有趣的范式转变。在受到关注之前，趋势过去是在模型架构中添加更多归纳偏差（具有位置偏差的 CNN、具有时间偏差的 RNN）…… Transformer 的成功有点表明，当你获得足够的数据和信息时，通用架构可以胜过归纳偏差。一个要训练的大屁股模型。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</guid>
      <pubDate>Sat, 16 Mar 2024 21:22:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] Apple - MM1：多模式 LLM 预培训的方法、分析和见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</link>
      <description><![CDATA[Apple 的新论文介绍了 MM1 ，一系列结合了视觉和语言理解的多模式人工智能模型。研究人员进行了广泛的实验，以确定驱动这些模型性能的关键因素，测试不同的架构选择和预训练数据混合。 以下是我在论文中的要点： Big当然之一：最大的 MM1 模型（30B 密集）在多模态基准上实现了最先进的少样本学习 要点：  MM1 包括两者高达 30B 参数的密集模型和专家混合 (MoE) 变体 图像分辨率对性能的影响最大，超过模型大小 特定的视觉语言连接器设计具有效果不大 在预训练中混合交错图像+文本、标题和纯文本数据至关重要 标题、交错和文本数据的比例为 5:5:1 有效最佳 合成字幕数据有助于少样本学习 30B 密集模型在 VQA 和字幕任务上击败了先前的 SOTA  核心见解深思熟虑的数据和架构选择，而不仅仅是规模，是构建高性能多模式模型的关键。 MM1 模型还表现出令人印象深刻的新兴能力，例如多图像推理和上下文中的小样本学习。 完整摘要。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</guid>
      <pubDate>Sat, 16 Mar 2024 17:29:24 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用计算机视觉对我的黑胶唱片收藏进行编目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgaeqi/p_cataloguing_my_vinyl_collection_with_computer/</link>
      <description><![CDATA[   /u/zerojames_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgaeqi/p_cataloguing_my_vinyl_collection_with_computer/</guid>
      <pubDate>Sat, 16 Mar 2024 16:48:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动态内存压缩：改造 LLM 以加速推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</link>
      <description><![CDATA[     &lt; td&gt; 动态内存压缩：改进 LLM 以加速推理 论文：https://arxiv.org/abs/2403.09636 X：https://x.com/p_nawrot/status/1768645461689168365 摘要：  Transformers 已成为大型语言模型 (LLM) 的支柱。然而，由于需要在内存中存储过去标记的键值表示的缓存，生成仍然效率低下，其大小与输入序列长度和批量大小线性缩放。作为解决方案，我们提出了动态内存压缩（DMC），这是一种在推理时进行在线键值缓存压缩的方法。最重要的是，该模型学习在不同的头和层中应用不同的压缩率。我们将 Llama 2（7B、13B 和 70B）等预训练的 LLM 改造为 DMC Transformer，在 NVIDIA H100 GPU 上实现自回归推理吞吐量高达约 3.7 倍的提升。 DMC 通过对原始数据的可忽略百分比进行持续预训练来应用，无需添加任何额外参数。我们发现 DMC 通过高达 4 倍的缓存压缩保留了原始的下游性能，优于经过训练的分组查询注意力 (GQA)。 GQA 和 DMC 甚至可以结合起来以获得复合收益。因此，DMC 在任何给定的内存预算内都适合更长的上下文和更大的批次。  https:/ /i.redd.it/ouuf7t4d5qoc1.gif   由   提交 /u/alancucki   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</guid>
      <pubDate>Sat, 16 Mar 2024 16:40:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] Kaggle TPU v3-8 的 Llama2 7B 和 13B 聊天完成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</link>
      <description><![CDATA[大家好，我对 Llama2 存储库进行了一些修改以利用 TPU v3-8 硬件，因此它可以执行 Llama2 7B（甚至 13B） ）聊天完成推理，无需图形重新编译。当生成批量大小为 1 的文本时，它仍然比 Nvidia P100 慢，不适合实时推理，但（TPU 是 TPU）在批量文本生成方面表现良好。我用它生成大量文本用于研究目的。希望它对社区有益。 这是存储库。 修改利用 PyTorch/XLA SPMD 系统（在 TPU v3-8 上）以及新的网格和分布配置来进行分片整个 TPU 网格的权重和缓存。具体来说，k、v 缓存具有预定义的静态大小，以避免每次令牌生成后 TPU 图形重新编译。这一新配置使 Llama2 7B 能够装入一台 TPU v3-8 设备中，并具有大量剩余内存来运行推理，批量大小最多为 64。相同的配置也可用于运行 Llama2 13B 的推理。 现有的Llama2 Google Next Inference分支仅支持TPU v4和 v5e。喜欢使用 Kaggle TPU 的人可以利用它来运行推理。   由   提交/u/-x-Knight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</guid>
      <pubDate>Sat, 16 Mar 2024 16:25:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLaMA 的具体细节：了解 LLaMA 和大型语言模型如何运行的整体方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</link>
      <description><![CDATA[我很高兴地宣布，我使用 Go 开发的 LLaMA Nuts and Bolts 开源项目现已公开发布！ 您可以在我的 Github 存储库上找到它：https://github.com/adalkiran/llama-nuts-and- Bolts 通过代码和详细文档了解 LLaMA 及其组件在实践中如何运行的整体方法。 “螺母和螺栓” （实践方面而不是理论事实，纯粹的实现细节）所需的组件、基础设施和数学运算，而不使用外部依赖项或库。 目标是制作一个可以对 LLaMa 进行推理的实验项目2 7B-聊天模型完全脱离Python生态系统（使用Go语言）。在整个旅程中，我们的目标是获取知识并阐明该技术的抽象内部层。 这段旅程是一次有意重新发明轮子的旅程。在阅读文档中的旅程时，您将通过 LLaMa 模型的示例了解大型语言模型如何工作的详细信息。 如果您像我一样对 LLM（大型语言模型）如何工作感到好奇和变形金刚工作并深入研究了来源中的概念解释和示意图，但渴望更深入的理解，那么这个项目也非常适合您！ 您不仅会找到 LLaMa 架构的细节，而且还会发现在文档目录中查找各种相关概念的解释。从逐字节读取 Pickle、PyTorch 模型、Protobuf 和 SentencePiece 分词器模型文件，到 BFloat16 数据类型的内部结构、从头开始实现 Tensor 结构和包括线性代数计算在内的数学运算。 这个项目最初是为了通过运行和调试来了解 LLM 背后的作用，并且仅用于实验和教育目的，而不是用于生产用途。 如果您查看它，我会很高兴欢迎评论！   由   提交 /u/adalkiran   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</guid>
      <pubDate>Sat, 16 Mar 2024 13:25:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 这些数据集有什么不同？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg44d8/r_what_is_different_between_these_datasets/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.05652 摘要：  机器学习模型的性能在很大程度上取决于输入的质量数据，但现实世界的应用程序经常遇到各种与数据相关的挑战。当在现实世界中管理训练数据或部署模型时，可能会出现这样的挑战 - 同一领域中的两个可比较的数据集可能具有不同的分布。尽管存在多种检测分布变化的技术，但文献缺乏以人类可理解的方式解释数据集差异的综合方法。为了解决这一差距，我们提出了一套可解释的方法（工具箱）来比较两个数据集。我们展示了我们的方法在不同数据模式中的多功能性，包括低维和高维设置中的表格数据、语言、图像和信号。我们的方法不仅在解释质量和正确性方面优于可比较和相关的方法，而且还提供了可操作的补充见解，以有效地理解和减轻数据集差异。  &lt;!-- SC_ON - -&gt;  由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg44d8/r_what_is_different_between_these_datasets/</guid>
      <pubDate>Sat, 16 Mar 2024 11:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] RepoHyper：存储库级代码完成所需的只是更好的上下文检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</link>
      <description><![CDATA[我们引入了 RepoHyper，这是一个新颖的框架，可将代码完成转换为现实世界存储库用例的无缝端到端流程。传统方法依赖于将上下文集成到代码语言模型 (CodeLLM) 中，通常假设这些上下文本质上是准确的。然而，我们发现了一个差距：标准基准测试并不总是提供相关的上下文。 为了解决这个问题，RepoHyper 提出了三个新颖的步骤：  构建代码属性图，建立丰富的上下文源。 一种新颖的搜索算法，用于查明所需的确切上下文。 扩展算法，旨在揭示代码元素之间的微妙联系（类似于社交网络挖掘中的链接预测问题）。  我们的综合评估表明，RepoHyper 树立了新标准，在 RepoBench 基准测试中优于其他强大的基准。 代码：https://github.com/FSoft-AI4Code/RepoHyper   由   提交 /u/FSoft_AIC   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</guid>
      <pubDate>Sat, 16 Mar 2024 10:41:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] AnyGPT：具有离散序列建模的统一多模态法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg2x83/r_anygpt_unified_multimodal_llm_with_discrete/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.12226 代码：https://github .com/OpenMOSS/AnyGPT 数据集：https:// Huggingface.co/datasets/fnlp/AnyInstruct 项目页面：https://junzhan2000.github.io/AnyGPT.github.io/ 视频：https://www.youtube.com/watch?v=oW3E3pIsaRg 摘要：  我们介绍 AnyGPT，这是一种任意对任意的多模态语言模型，它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。 AnyGPT 可以稳定地训练，无需对当前的大语言模型（LLM）架构或训练范式进行任何改变。相反，它完全依赖于数据级预处理，促进新模式无缝集成到法学硕士中，类似于新语言的合并。我们构建了一个以文本为中心的多模态数据集，用于多模态对齐预训练。利用生成模型，我们合成了第一个大规模任意对任意多模式指令数据集。它由 108k 个多轮对话样本组成，这些对话错综复杂地交织着各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT 能够促进任意对任意的多模态对话，同时在所有模态中实现与专用模型相当的性能，证明离散表示可以有效且方便地统一语言模型中的多种模态。演示显示在 此 https URL    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg2x83/r_anygpt_unified_multimodal_llm_with_discrete/</guid>
      <pubDate>Sat, 16 Mar 2024 10:18:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习中的函数式编程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bfu3oy/d_functional_programming_in_ml/</link>
      <description><![CDATA[我觉得 Python 中的大多数 ML 库都是用 OOP 风格编写的。这是有道理的，因为 Python 没有像 Haskell 这样的“好”类型系统，因此类是用可读名称定义接口的好方法。其他功能更强大的语言是否有流行的 ML 库？我特别想到 Haskell、Rust 或 Scala。   由   提交 /u/LengthinessMelodic67   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bfu3oy/d_functional_programming_in_ml/</guid>
      <pubDate>Sat, 16 Mar 2024 01:15:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 转载MetaAI的《自我奖励语言模型》论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bfnz2l/p_reproducing_the_selfrewarding_language_models/</link>
      <description><![CDATA[大家好， 读完 Meta 团队的《自我奖励语言模型》论文后，感觉非常平易近人且可重现，所以我们花了一些时间来实现它。 ​ 提供的脚本采用任何基本模型并将其放入循环中： 1 ）对初始数据集进行监督微调 2）使用 SFT 生成新提示 3）每个提示生成 N 个响应 4）对生成的结果进行评分响应 1-5 5) 对模型本身的奖励运行 DPO。 ​ 我们已经通过一个循环运行它，从开始使用 Mistral-7b 基础模型，到目前为止结果非常令人鼓舞。  ​ 请随意检查或亲自运行它，并让我们知道您的想法： https://github.com/Oxen-AI/Self-Rewarding-Language-Models   由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bfnz2l/p_reproducing_the_selfrewarding_language_models/</guid>
      <pubDate>Fri, 15 Mar 2024 20:42:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>