<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 05 Nov 2024 01:14:11 GMT</lastBuildDate>
    <item>
      <title>为您当地的 LLMS 输入视频 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/</link>
      <description><![CDATA[我的项目的作用 OpenSceneSense-Ollama 是一个功能强大的 Python 包，旨在直接在本地机器上进行以隐私为中心的视频分析。借助此工具，您可以利用 Ollama 的本地模型来分析帧、转录音频、动态选择关键帧并生成详细摘要 - 所有这些都无需依赖基于云的 API。它非常适合那些需要对视频内容进行丰富、有见地的分析，同时确保数据隐私并最大限度地降低使用成本的人。 目标受众 该项目专为需要深入的本地处理视频分析的开发人员、研究人员、数据科学家和注重隐私的用户量身定制。它非常适合数据安全至关重要的应用，包括： - 需要自动视频摘要的内容创建工作流程 - 为机器学习构建标记数据集的研究人员 - 需要上下文丰富的内容审核的平台 - 远程或受限环境中的离线项目 比较 OpenSceneSense-Ollama 超越了通常将帧和音频分析分开的传统视频分析工具。相反，它集成了视觉和音频元素，允许用户提示模型生成全面的摘要和深入的上下文洞察。大多数工具可能会单独识别对象或转录音频，而 OpenSceneSense-Ollama 将这些组件统一为叙述摘要，使其成为更丰富的数据集或更细致入微的内容审核的理想选择。 入门 要开始使用 OpenSceneSense-Ollama：  先决条件：确保您的计算机上安装了 Python 3.10+、FFmpeg、PyTorch 和 Ollama。 使用 pip 安装：运行“pip install openscenesense-ollama”以安装软件包。 配置：开始使用可自定义的提示、帧选择和音频转录分析视频。  欢迎随意深入研究、尝试并分享您的反馈，特别是如果您从事 AI、以隐私为中心的应用程序或视频内容审核工作。让我们构建一个强大的本地解决方案，以进行有意义的视频分析！ https://github.com/ymrohit/openscenesense-ollama    提交人    /u/rohit3627   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/</guid>
      <pubDate>Tue, 05 Nov 2024 00:50:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] NN 创造最佳伪装</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/</link>
      <description><![CDATA[      我有这个想法已经有一段时间了，我已经创建了所有用于创建数据的函数以及所有的架构。问题是我只有两年的深度学习经验，而且这是 GAN 风格的网络，众所周知，GAN 很难训练。我希望你们能就这个想法提出意见，以及一些技巧、建议、忠告和需要改进的地方。如果有人觉得这很有趣，我很乐意与其他人合作完成这个项目。 迷彩图案生成模型 目标是创建一个模型，通过训练生成器模型并使用分割模型作为判别器来评估生成的迷彩的有效性，从而生成最佳迷彩颜色图案。生成器和鉴别器同时进行训练。 模型结构 前向过程  生成器： 生成器是一个简单的解码器模型，它采用大小为 n_embed = 128 的随机潜在向量并输出 3x32x32 的迷彩颜色图案。 然后将生成的迷彩图案平铺以形成更大的纹理，与士兵图像的大小相匹配。  创建伪装士兵： 对士兵的随机黑白 PNG 图像进行采样并将其大小调整为 (1, W, H)，并将值反转，以便士兵显示为白色（前景）而背景为黑色。 然后将平铺的迷彩图案通过用士兵图像进行掩盖应用于士兵，产生伪装的士兵形象。整个操作都是分批的，并允许梯度流动。  将伪装的士兵放置在背景上： 伪装的士兵被随机放置在背景图像上（例如，森林场景）。 同时生成分割模型的标签掩码，其中包含两个类：背景和士兵。  鉴别器（分割模型）： 使用预先训练的分割模型（充当鉴别器），具有两个输出类（背景和士兵）。 该模型通过尝试将士兵归类为背景来评估伪装图案将士兵融入背景的程度。   损失函数和优化 使用了两个损失函数，每个函数都有单独的反向传播过程：  生成器损失： 这鼓励生成器创建一个伪装图案，使士兵与背景难以区分。 损失函数：CrossEntropyLoss（输出，0）其中输出是来自鉴别器的预测分割图，0 代表背景类。  鉴别器（分割模型）损失： 这鼓励分割模型正确识别背景中的伪装士兵。 损失函数：CrossEntropyLoss（输出，label_mask）其中标签掩码有两个类：背景和士兵。   关键注意事项 此设置类似于生成对抗网络 (GAN)，但不同之处在于它不使用“真实”伪装数据，而只使用生成的样本。此外：  单独的优化器：建议对生成器和鉴别器使用不同的优化器。 损失缩放：可能需要仔细调整缩放因子或学习率以稳定训练。 两步反向传播：与典型的 GAN 式损失不同，使用两步反向传播方法来独立更新模型。  https://preview.redd.it/qd2cr2rkyyyd1.png?width=5603&amp;format=png&amp;auto=webp&amp;s=0faee2cb0504a98c36b365b2edbc59253509d8c7    提交人    /u/MemoryCompetitive691   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/</guid>
      <pubDate>Mon, 04 Nov 2024 23:28:17 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型（LLM）实际上可以很好地解决哪些问题？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/</link>
      <description><![CDATA[虽然人们对人工智能炒作周期的怀疑越来越多，尤其是围绕聊天机器人和 RAG 系统，但我感兴趣的是确定 LLM 在准确性、成本或效率方面明显优于传统方法的具体问题。我能想到的问题是： - 单词分类 - 非大段文本的情感分析 - 图像识别（在某种程度上） - 写作风格转换（在某种程度上） 还有什么？    提交人    /u/Educational-String94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/</guid>
      <pubDate>Mon, 04 Nov 2024 20:52:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在预训练语言模型中添加交叉注意力机制的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/</link>
      <description><![CDATA[我想训练新的交叉注意力层，将其输入到预先训练的变压器（可能是小型骆驼模型）中，同时保持模型的其余部分不变。 哪些资源可能会有用？    提交人    /u/BinaryOperation   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/</guid>
      <pubDate>Mon, 04 Nov 2024 16:03:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有 LLM 模型中量化是否都有限？例如，您可以采用 meta-llama/Llama-3.2-1B 等标准模型，并以一半的速度运行，但也有专门为 4 位量化制作的模型（即 meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/</link>
      <description><![CDATA[我只是想了解所有模型中量化的设置方式。 meta-llama/Llama-3.2-1B 等标准模型可以在没有量化（bfloat16 或 float32？）的情况下运行，或者可以使用推理应用程序（如 vLLM）指示它们以一半（float16？）运行。那么这是否意味着所有模型中都内置了一些量化？我是否可以改为使用 int8，而不是指示它以一半量化运行？或者这仅在模型为它构建时才有效？ 然后是专门为 int4 构建的模型（即 meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8）。这是否意味着当您使用 vLLM 运行此模型时，您必须明确说明您正在 int4 上运行它，或者您只是将其保留为默认值，它将自动在 int4 上运行？可以将其覆盖为 int8 吗？还是只是为 int4 硬编码？ 过去 2 天我一直在试图理解这一点。    提交人    /u/xil35   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/</guid>
      <pubDate>Mon, 04 Nov 2024 15:54:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] COLING25 行业轨道：录取通知</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/</link>
      <description><![CDATA[“接受通知”的日期是地球上任何地方的 2024 年 11 月 3 日 12:00。我们还没有收到主席的回复，门户网站上也没有通知，有延迟吗？还是只有被接受的论文才会收到通知？请分享有关此的任何信息/更新，谢谢。    提交人    /u/BlackEyesBrownSavant   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/</guid>
      <pubDate>Mon, 04 Nov 2024 15:44:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多元互信息估计，三个以上变量的 PID</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjdtpi/r_estimation_of_multivariate_mutual_information/</link>
      <description><![CDATA[为什么估计高维互信息不流行。例如，我见过的最多的是 3 个变量。我知道所需的样本数量呈指数增长。但在大数据设置中，它仍然是可行的。 歧视也是一个问题，因为估计通常是针对分箱数据进行的。 有人对此以及三个以上变量互信息的实际应用有更多了解吗？非常有兴趣阅读有关在具有大量样本的数据集中推断高维变量之间关系的应用。    提交人    /u/Sandy_dude   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjdtpi/r_estimation_of_multivariate_mutual_information/</guid>
      <pubDate>Mon, 04 Nov 2024 13:09:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 ImageNet 中的 100 万个文件导入 DVC、Git-LFS 和 Oxen.ai 进行基准测试，以进行开源数据集协作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gj0si8/p_benchmarking_1_million_files_from_imagenet_into/</link>
      <description><![CDATA[大家好！ 如果您还没有看过 Oxen 项目，我们一直在构建一个快速的开源非结构化数据版本控制工具和平台来托管数据（https://oxen.ai）。它是使用 git-lfs 或他们的数据集库将数据转储到 Hugging Face 上的替代方案，并且与他们的模型（如巧克力和花生酱）搭配使用 - Oxen 可用于迭代和编辑数据，而 Hugging Face 可用于公共模型。 我们受到了让大型机器学习数据集成为人们可以协作的活生生的资产而不是静态转储的想法的启发。最近，我们一直在努力优化 Oxen.ai 中的底层 Merkle 树和数据结构，并刚刚发布了 v0.19.4，它为内部 API 提供了大量性能升级和稳定性。 100 万个文件基准测试 为了进行全部测试，我们决定在经典 ImageNet 数据集中的 100 多万张图像上对该工具进行基准测试。 TLDR 是 Oxen.ai 比原始上传到 S3 更快，比 git-lfs 快 13 倍，比 DVC 快 5 倍。完整细分可在此处找到👇 https://docs.oxen.ai/features/performance 如果您身处 ML/AI 社区，或者只是数据爱好者，我们很乐意收到您对该工具和代码库的反馈。当涉及到不同的存储后端和与其他数据工具的集成时，我们希望社区做出一些贡献。    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gj0si8/p_benchmarking_1_million_files_from_imagenet_into/</guid>
      <pubDate>Sun, 03 Nov 2024 23:43:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有/无 SMOTE 的逻辑回归比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gizg2u/d_comparison_of_logistic_regression_withwithout/</link>
      <description><![CDATA[      这让我在工作中抓狂。我一直在评估一个逻辑预测模型。该模型实施了 SMOTE 来将数据集平衡为 1:1 的比例（最初为期望结果的 7%）。我认为这是不必要的，因为改变决策阈值就足够了，并且可以避免不必要的数据插补。数据集中有超过 9,000 次期望事件的发生 - 这对于 MLE 估计来说已经足够了。我的同事不同意。  我在 R 中构建了一个闪亮的应用程序来比较两个模型的混淆矩阵以及一些指标。我欢迎社区对此比较提出一些意见。对我来说，非 smote 模型的表现同样出色，如果查看 Brier 分数或校准截距，甚至更好。你们觉得呢？    提交人    /u/Janky222   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gizg2u/d_comparison_of_logistic_regression_withwithout/</guid>
      <pubDate>Sun, 03 Nov 2024 22:42:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 傅里叶权重神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giwdum/d_fourier_weights_neural_networks/</link>
      <description><![CDATA[亲爱的 ML 社区， 我想分享一个关于使用傅立叶系数来参数化神经网络权重的讨论想法。通常在 MLP 中，权重仅在一个方向上定义，而在另一个方向上未定义，这使其处于开放状态：我们可以将权重定义为对称的：w(r,s) = w(s,r)，我们可以使用双变量对称函数的傅立叶系数通过反向传播和梯度下降来计算权重。（我应该提到，我目前正在积极寻找机会将我的机器学习知识带到德国法兰克福附近的项目中。） 编辑：也许我的措辞不太正确。让我们同意，在大多数情况下，具有可逆激活函数的 MLP 满足对称性假设。我想讨论的想法是使用傅里叶系数来（重新）构建权重 w(r,s) = w(s,r)。为了使这个想法有意义，FWNN 不会像通常的 MLP/ANN 那样学习权重，而是学习傅里叶级数的系数（至少是其中一些）。通过调整学习的系数数量，FWNN 可以调整其学习能力。请注意，通过函数 w(r,s) 的对称性，我们得到诸如 sum_{j] c_j*cos(j * (r+s) ) 之类的项，其中 j 的范围是某个预定义的整数范围 [-R,R]。理论上，这个 R 应该是无穷大，因此 Z = [-inf, +inf] 是整数。还要注意，网络学习的参数 c_j 数量为 2*R+1，乍一看与神经元数量 N 无关。因此，具有 N 个神经元的传统神经网络理论上必须学习 O(N^2) 个权重，但使用傅里叶变换，我们可以将这个参数数量减少到 2*R+1。当然，R = N^2 是可能的，但我可以想象，当 2*R+1 &lt;&lt; N^2 时会出现问题。我希望这能澄清这个想法。 代码：https://github.com/githubuser1983/fourier_weighted_neural_network/blob/main/fourier_weighted_neural_network.py 方法说明：https://www.academia.edu/125262107/Fourier_Weighted_Neural_Networks_Enhancing_Efficiency_and_Performance   由    /u/musescore1983  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giwdum/d_fourier_weights_neural_networks/</guid>
      <pubDate>Sun, 03 Nov 2024 20:27:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAAI 2025 第二阶段评审</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giqc9n/d_aaai_2025_phase_2_reviews/</link>
      <description><![CDATA[评论将很快发布。这是一个讨论/吐槽的帖子。请在评论中保持礼貌。    由   提交  /u/quasi-literate   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giqc9n/d_aaai_2025_phase_2_reviews/</guid>
      <pubDate>Sun, 03 Nov 2024 16:09:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Nov 2024 16:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2024 年，你的神经网络训练秘诀是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/</link>
      <description><![CDATA[您可能已经知道 Karpathy 2019 中的神经网络训练秘诀圣经 虽然大多数建议仍然有效，但深度学习模型/方法的格局自那时起发生了很大变化。Karpathy 的建议在监督学习环境中效果很好，他确实提到了这一点：  坚持监督学习。不要对无监督预训练过度兴奋。与 2008 年那篇博客文章所说的不同，据我所知，没有任何版本在现代计算机视觉方面报告了强劲的结果（尽管如今 NLP 在 BERT 和朋友的帮助下似乎表现不错，这很可能是由于文本的更刻意性质和更高的信噪比）。  我最近一直在训练一些图像扩散模型，我发现在无监督环境下做出数据驱动的决策更加困难。指标不太可靠，有时我会训练损失更好的模型，但当我查看样本时，它们看起来更糟 你知道 2024 年训练神经网络的更多现代方法吗？（不仅仅是 LLM）    提交人    /u/Even_Information4853   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/</guid>
      <pubDate>Sun, 03 Nov 2024 15:05:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有 Science Twitter/X 的替代品？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/</link>
      <description><![CDATA[大家好， 我一直在想，Twitter/X 上是否有科学界的替代方案，尤其是在 DS/ML 领域。在 COVID 之前和期间，我真的很喜欢那个社区，但在 Elon 上任后不久，我就离开了 Twitter，因为当时这个平台已经很有毒了，而且从那以后变得更糟了。  我知道 LinkedIn 上有一个活跃的社区，有时还不错，但大多都是试图听起来/看起来聪明的有影响力的人，还有人大肆宣传 LLM 的每件小事。我知道从那时起，其他人也离开了 Twitter 上的科学界，因此想知道过去几年是否出现了替代方案。 附言：我也会在 DS 社区中发布此消息。    提交人    /u/H4RZ3RK4S3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/</guid>
      <pubDate>Sun, 03 Nov 2024 11:41:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>