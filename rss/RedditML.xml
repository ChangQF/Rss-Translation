<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 16 Nov 2024 03:26:29 GMT</lastBuildDate>
    <item>
      <title>[D] 分布式 ML 算法面试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/</link>
      <description><![CDATA[大家好， 我即将进行一次关于分布式 ML 算法的面试（面试描述：我们将探讨和解释用于构建常见神经网络操作的基本技术，重点介绍简单而有效的实现。） 有没有什么好的资源可以用来准备这种面试？    提交人    /u/deepthought00705   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/</guid>
      <pubDate>Sat, 16 Nov 2024 01:13:01 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 现代搜索系统在查询预处理中是否仍然需要词干提取和词形还原？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/</link>
      <description><![CDATA[考虑到 LM 的所有进步，我想知道它们在现代搜索系统中有多重要。语义嵌入通常可以帮助我们很好地理解含义。但为了有效利用历史查询项目参与度特征，我们似乎仍然需要进行这些预处理。否则，当用户搜索与常见查询略有不同时，我们很容易得到空的参与度特征？或者有没有更现代的方式来处理自由形式的查询？    提交人    /u/wenegue   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/</guid>
      <pubDate>Sat, 16 Nov 2024 00:16:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对大量特征有效运行的特征选择方法（tabular、lightgbm）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/</link>
      <description><![CDATA[有没有人知道一个好的特征选择算法（有或没有实现）可以在合理的时间内搜索大约 50-100k 个特征？我正在使用 lightgbm。直觉是我需要模型中大约 20-100 个最终特征。想在大海捞针。表格数据，大约有 100-500k 条数据记录可供使用。根据我的经验，常见的特征选择方法在计算上无法扩展。此外，我发现过度拟合是这么大的搜索空间的一个问题。     提交人    /u/acetherace   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/</guid>
      <pubDate>Fri, 15 Nov 2024 23:58:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] DistilBERT 与 TransformerEncoder</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/</link>
      <description><![CDATA[我对预训练的 DistilBERT 变压器模型进行了微调，它实现了 ~0.85 的准确率（17 个类别的分类）。我还使用 torch.nn.TransformerEncoder 从头开始​​构建了一个 Transformer 模型，对于同样的问题，它实现了 ~0.97 的准确率。这是正常的吗？我期望预训练的 DistilBERT 能有更好的性能。请注意，对于 DistilBERT 模型，我使用了它自己的嵌入（预训练的 DistilBertTokenizer），对于 torch.nn.TransformerEncoder，我使用了简单的 TFIDF 方法。由于 TFIDF 无法捕获句子中单词的序列（它忽略了上下文），因此它变得更加混乱。 请让我知道您的想法。:)    提交人    /u/Interesting_Pea_4605   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/</guid>
      <pubDate>Fri, 15 Nov 2024 23:21:20 GMT</pubDate>
    </item>
    <item>
      <title>[R][D]抽象推理的测试时间训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/</link>
      <description><![CDATA[https://arxiv.org/pdf/2411.07279 顺便问一下，大家知道有什么研究尝试在回答问题之前对模型进行微调吗？我的意思是它可能适用于上下文信息检索，但我想知道它对推理能力更强的任务的影响。计算过剩仍然会很大。    提交人    /u/Due-Pangolin325   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/</guid>
      <pubDate>Fri, 15 Nov 2024 23:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] 卷积可微分逻辑门网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/</link>
      <description><![CDATA[摘要 随着机器学习模型的推理成本不断增加，人们对具有快速高效推理能力​​的模型的兴趣日益浓厚。最近，提出了一种通过可微分松弛直接学习逻辑门网络的方法。逻辑门网络比传统的神经网络方法更快，因为它们的推理只需要逻辑门运算符（例如 NAND、OR 和 XOR），这些运算符是当前硬件的底层构建块，可以高效执行。我们在此想法的基础上，通过深度逻辑门树卷积、逻辑或池化和残差初始化对其进行了扩展。这允许将逻辑门网络扩大一个数量级以上并利用卷积范式。在 CIFAR-10 上，我们仅使用 6100 万个逻辑门就实现了 86.29% 的准确率，这比 SOTA 有所提高，同时体积却缩小了 29 倍。 被 Neurips 2024 接受，“SOTA”在这里表示可比方法。我发现这篇论文真的很有趣，尽管非玩具网络似乎训练起来非常昂贵。好奇其他人怎么想？    提交人    /u/jacobgorm   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/</guid>
      <pubDate>Fri, 15 Nov 2024 22:51:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 读博士还是不读博士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/</link>
      <description><![CDATA[我想这个问题已经被问过无数次了，但让我再问一次。 我目前在 MSFT 担任应用科学家。然而，我更想找科学职位，比如 DeepMind 的研究科学家。虽然工作并不特别需要博士学位，但竞争非常激烈，而且有很多博士学位持有者。 我确实喜欢研究，想读博士学位，但我总是问自己这是否真的值得。 这肯定是一个开放性问题，请随时分享您的想法。    提交人    /u/oddhvdfscuyg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/</guid>
      <pubDate>Fri, 15 Nov 2024 20:44:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neurips 2024 酒店室友搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/</link>
      <description><![CDATA[2024 年 Neurips 会场周围的酒店相当昂贵，我正在寻找室友来分摊费用（我的大学对他们愿意报销的每晚酒店费用有限制）。我目前已在世纪广场酒店预订了周二至周日的房间，距离会议中心 0.9 英里。每晚房费为 414 美元。如果有人想分摊房费，请联系我们！此外，如果您能与您的研究小组或您认识的其他与会者分享这篇文章，那将会很有帮助。 如果您不确定是否要与完全陌生的人同住，您可以通过我的个人网站 (https://mtcrawshaw.github.io/) 了解我一点，该网站有指向我的谷歌学术页面、简历等的链接。我确实在会议上发表了一篇关于联邦学习/分布式优化领域的论文。我只是一名研究生，想让会议变得负担得起！谢谢。    提交人    /u/ssbm_crawshaw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/</guid>
      <pubDate>Fri, 15 Nov 2024 16:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用文本嵌入进行元学习以评估基于文本的混杂因素下的治疗效果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/</link>
      <description><![CDATA[标题：从文本到治疗效果：处理基于文本的混杂的元学习方法 我发现这篇论文介绍了一个元学习框架，该框架联合学习文本表示并估计治疗效果以处理基于文本的混杂。关键创新是使用元学习同时优化文本编码器和治疗效果估计器，而不是将它们视为单独的步骤。 主要技术要点： - 开发两阶段元学习架构： - 文本编码器学习捕获混杂信息的表示 - 治疗效果估计器使用这些表示来计算单个效果 - 使用基于梯度的元学习端到端优化两个组件 - 结合平衡正则化以确保治疗组/对照组具有相似的表示 - 对来自医疗保健和产品评论的合成和真实数据集进行评估 报告的结果： - 在合成数据上的表现优于基线方法（单独的文本编码 + 治疗估计）15-25％ - 在真实产品评论数据集上显示治疗效果估计提高了 12％ - 消融研究证实元学习和平衡正则化都有助于提高性能 理论含义很有趣 - 这表明联合优化表示学习和因果推理可以比管道更好地捕捉混杂方法。实际上，这可以改善许多文本数据包含混杂信息的领域的治疗效果估计，例如医疗记录或用户评论。 TLDR：新的元学习方法联合学习文本表示和治疗效果来处理基于文本的混杂，与合成数据和真实数据的管道方法相比显示出显着的改进。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/</guid>
      <pubDate>Fri, 15 Nov 2024 16:31:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师的等级指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/</link>
      <description><![CDATA[想了解这个社区区分中级/高级/首席级机器学习工程师的一些方法。对于软件工程来说，这不是一门艺术，因为有充分记录的案例和例子。但不太清楚机器学习工程师是否遵循相同的定义……    提交人    /u/AdditionalWeb107   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/</guid>
      <pubDate>Fri, 15 Nov 2024 15:47:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当您说“LLM”时，有多少人也考虑过 BERT 之类的东西？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/</link>
      <description><![CDATA[我不断遇到这种争论，但对我来说，当我听到“LLM”时，我的假设是只有解码器的模型，这些模型有数十亿个参数。似乎有些人会将 BERT-base 纳入 LLM 系列，但我不确定这是否正确？我想从技术上讲是这样，但每次我听到有人说“我如何使用 LLM 进行 XYZ”时，他们通常会提到 LLaMA 或 Mistral 或 ChatGPT 或类似的东西。    提交人    /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/</guid>
      <pubDate>Fri, 15 Nov 2024 14:16:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Ilya Sutskever 的 AI 阅读清单中丢失的阅读项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/</link>
      <description><![CDATA[这篇博文试图找出今年早些时候出现的热门 AI 阅读清单中遗漏了哪些论文，该清单归功于 Ilya Sutskever 及其声称涵盖了 2020 年 AI 领域“90% 的重要内容”： https://tensorlabbet.com/2024/11/11/lost-reading-items/ 今年早些时候，大约 40 篇论文中只有 27 篇在网上分享，因此关于哪些作品足够重要而值得收录的理论有很多。这里讨论了一些与元学习和竞争性自我博弈相关的明显候选者。但也有几位值得注意的作者，如 Yann LeCun 和 Ian Goodfellow 未列入名单。 从我的角度来看，甚至关于 U-Net、YOLO 检测器、GAN、WaveNet、Word2Vec 等的论文也应该包括在内，所以我很好奇对此的更多看法！    提交人    /u/AccomplishedCat4770   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/</guid>
      <pubDate>Fri, 15 Nov 2024 10:34:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 论文俱乐部：Nvidia 研究员 Ethan He 介绍教育部法学硕士课程升级改造</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/</link>
      <description><![CDATA[大家好， 明天，Nvidia 研究员 Ethan He 将深入研究他的工作：在混合专家 (MoE) 中升级法学硕士。很高兴能一睹幕后风采，看看在 Nvida 处理这种规模的模型是什么样的。 如果您想在明天太平洋标准时间上午 10 点加入社区，我们非常欢迎您。我们通过 zoom 进行现场直播，欢迎任何人加入。 这是论文：https://arxiv.org/abs/2410.07524 现场加入我们：https://lu.ma/arxivdive-31    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/</guid>
      <pubDate>Fri, 15 Nov 2024 00:19:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 10 Nov 2024 03:15:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>