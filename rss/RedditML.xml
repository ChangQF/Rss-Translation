<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Mon, 10 Feb 2025 15:18:07 GMT</lastBuildDate>
    <item>
      <title>[d] KL分歧是LLM训练后RL的主要奖励？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im7bsb/d_kl_divergence_as_a_primary_reward_in_llm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  说我们预算了一个LLM。如果我们使用预处理的LLM生成一个序列，则不会完全获得具有最佳KL差异的序列，该序列与验证的LLM相关。这就是为什么Beam搜索以前是一件事情的原因。那么，如果我们执行RL纯kl差异是奖励模型怎么办？最终的模型将是一个模型，该模型将生成比预读的LLM的总体KL差异要低得多的序列。会发生什么？该模型会“更连贯”？ 我想听听每个人对此的想法，因为这似乎是一个思想实验，似乎会导致一个琐碎的答案，但是序列的kl差异是一个目标实际上，如果没有非线性优化（RL），这实际上很难解决。是的，我们直接知道令牌概率，但是很难知道该序列的累积概率是“更喜欢”的模型。感觉就像是一个不对称的优化问题（易于评估，但难以解决），我想知道是否有任何有意义的东西会出现。 我的实现想法就是使用GRPO进行RL。但是你们怎么看？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ricecake1539     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im7bsb/d_kl_divergence_as_a_primary_reward_in_llm/</guid>
      <pubDate>Mon, 10 Feb 2025 14:27:55 GMT</pubDate>
    </item>
    <item>
      <title>深度学习博士学位的笔记本电脑[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我有2,000英镑，我需要在三月之前在笔记本电脑上使用（否则我损失了资金）我在应用数学上的博士学位，涉及大量的深度学习。我所做的大部分可能都会在云上，但是看到我有这个预算，我不妨在我需要离线运行一些事情时获得最好的笔记本电脑。 我可以得到一些建议购买什么？我不想获得Mac，但对所有选项都感到困惑。我知道刚刚发布了新的GPU（NVIDIA 5000系列），并且已经使用Lunar Lake / Snapdragon CPU宣布了新的笔记本电脑。&lt; / p&gt; 我不确定我是否应该旨在用不错的GPU获得一些东西或者只是像Lenove Carbon X1一样获得一本薄/轻的超书。 感谢您的帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bloch2001     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</guid>
      <pubDate>Mon, 10 Feb 2025 11:37:51 GMT</pubDate>
    </item>
    <item>
      <title>[R]使用潜在扩散变压器进行未校准的图像集的多视图场景完成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im2al0/r_multiview_scene_completion_using_latent/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这项工作提出了一种基于变压器的方法，用于在多视图场景中捕获丢失区域，同时保持几何一致性。关键创新是通过两个阶段的过程来处理不受约束的休闲照片，该过程在产生缺失区域之前首先分析跨视图的可见内容。 关键的技术方面： - 多头注意的注意机制同时多次观点 - 新颖的一致性 - 新型一致性损失确保生成的内容跨不同角度对齐 - 直接与稀疏，非结构化的照片集一起使用 - 处理室内和室外场景 - 在消费者GPU硬件 结果上运行： - 视觉质量指标与先前的30％提高：方法 - 在不同的捕获密度之间的性能一致 - 对复杂几何结构的鲁棒处理 - 对典型场景大小的实时推断 我认为这可能会严重影响3D内容创建的几个领域。使用休闲照片的能力消除了房地产，虚拟旅游和建筑可视化应用程序的主要障碍。跨视图的一致性对于VR/AR用例尤其重要。 我看到的主要限制是降级性能，输入非常稀疏，这通常是带有休闲照片集的现实。还可以改善处理反射表面和复杂的几何形状。  tldr：新的基于变形金刚的方法在多视图中使用常规照片在多视场景中完成缺失区域，同时跨视点保持一致性。比以前的方法显示出30％的视觉质量。摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im2al0/r_multiview_scene_completion_using_latent/</guid>
      <pubDate>Mon, 10 Feb 2025 09:30:43 GMT</pubDate>
    </item>
    <item>
      <title>[d]神经2025的位置纸轨道会有一个位置纸轨道吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im1j39/d_will_there_be_a_position_paper_track_at_neurips/</link>
      <description><![CDATA[在&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/skeeringReal     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1im1j39/d_will_there_there_be_a_position_paper_paper_track_atap_ate_neurips/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im1j39/d_will_there_be_a_position_paper_track_at_neurips/</guid>
      <pubDate>Mon, 10 Feb 2025 08:32:54 GMT</pubDate>
    </item>
    <item>
      <title>[p]邀请合作者获得可区分的几何损失函数库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilzqdb/p_inviting_collaborators_for_a_differentiable/</link>
      <description><![CDATA[在一个用于在pytorch中创建可区分几何损失函数库的项目。 我在这里放置了一些初始提交的项目，以了解可能的外观： github repo        &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/atharvaaalok1     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilzqdb/p_inviting_collaborators_for_a_differentiable/</guid>
      <pubDate>Mon, 10 Feb 2025 06:22:02 GMT</pubDate>
    </item>
    <item>
      <title>[R]您的AI看不到大猩猩：LLMS执行探索性数据分析能力的比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iljqve/r_your_ai_cant_see_gorillas_a_comparison_of_llms/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/jsonathan     [link]  ＆＃32;  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iljqve/r_your_ai_ai_ai_cant_see_gorillas_a_comparison_comparison_of_of_lls/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iljqve/r_your_ai_cant_see_gorillas_a_comparison_of_llms/</guid>
      <pubDate>Sun, 09 Feb 2025 17:18:41 GMT</pubDate>
    </item>
    <item>
      <title>[P]周末实施高斯MAE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iljb52/p_weekend_implementation_of_gaussian_mae/</link>
      <description><![CDATA[在自动编码器    https://github.com/darshanmakwana412/gaussian-mae-mae-mae-mae-mae  /p&gt;  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/apartmenteither4838     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iljb52/p_weekend_implementation_of_gaussian_mae/</guid>
      <pubDate>Sun, 09 Feb 2025 17:00:44 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/automoderator     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[p]综合数据多样性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilfse6/p_evals_for_diversity_in_synthetic_data/</link>
      <description><![CDATA[在/p&gt; 我写了一份概述，以衡量LLM生成的合成数据中的语言多样性。  链接： https://amitness.com/posts/diversity-evals    这对于系统测试各种技术对改善多样性的影响很有用。  欢迎任何反馈！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/amitness     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilfse6/p_evals_for_diversity_in_synthetic_data/</guid>
      <pubDate>Sun, 09 Feb 2025 14:24:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] PaperStream-保持研究的最新状态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilfk3t/p_paperstream_stay_updated_with_research_your_way/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  跟上当前的研究可能会很麻烦。虽然有许多解决方案，但我需要一种适合我特定需求的解决方案。 这就是为什么我开发了纸质流的原因，纸流是一种旨在支持研究人员的工具。 PaperStream提供两个核心功能：  解析会议  从您喜欢的会议（例如CVPR，Neurips，exter）从各种格式中检索论文作为JSON或CSV。与仅提供特定会议和输出文件的其他存储库不同的是，纸流纸可以让您轻松地检索诉讼程序。  构建PaperFeed      Google学者警报很棒，但是在日常业务中很容易忽略电子邮件警报，尤其是如果您无法立即检查它们。使用PaperStream，您可以将Google Scholar警报变成新闻源。 与您喜欢的读者（例如Emacs。 href =“ https://github.com/mdaehl/paperstream”&gt;存储库。 我希望您喜欢它，我感谢任何反馈。   &lt; ！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/cressy16     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilfk3t/p_paperstream_stay_updated_with_research_your_way/</guid>
      <pubDate>Sun, 09 Feb 2025 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>[r]豪华轿车：少更多用于推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ile9nu/r_limo_less_is_more_for_reasoning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   我们提出了一个基本发现，挑战了我们对大语言模型中复杂推理如何出现的理解。尽管传统的观点表明，复杂的推理任务需要广泛的培训数据（通常是100,000个例子），但我们证明了一个惊人的现象：复杂的数学推理能力可以有效地引起，而令人惊讶的例子很少。这一发现不仅挑战了大量数据要求的假设，而且挑战了监督微调的共同信念主要导致记忆而不是概括。通过全面的实验，我们提出的模型豪华轿车证明了数学推理的前所未有的性能和效率。仅使用817个精选的训练样品，豪华轿车在高度挑战性的AIME基准上实现了57.1％的准确性，而数学的精度为94.8％，在AIME上将以前的强SFT型号从6.5％提高到57.1％，从59.2％，从59.2％提高到94.8％数学，而仅使用以前方法所需的培训数据的1％。最引人注目的是，豪华轿车表现出异常的分布概括，在10种不同的基准测试中实现了40.5％的绝对改进，优于对100倍培训的模型，直接挑战了SFT固有地导致记忆而不是概括的普遍观念。综合这些开创性的结果，我们提出了较少的推理假设（豪华假设）：在基础模型中，在预训练期间已经对领域知识进行了全面编码，可以通过最小但精确的跨越认知过程的证明来出现。该假设认为，复杂推理的启发阈值并非固有地受到目标推理任务的复杂性的限制，而是从根本上由两个关键因素确定：（1）模型在训练期间的编码知识基础的完整性，以及（2 ）训练后示例的有效性，这些示例是“认知模板”，该模型显示了该模型如何有效利用其现有知识库来解决复杂的推理任务。   arxiv链接： [2502.03387]豪华轿车：更少用于推理    &lt;！ -  sc_on- sc_on-&gt;&gt;＆＃32;提交由＆＃32; /u/hiskuu     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ile9nu/r_limo_less_is_more_for_reasoning/</guid>
      <pubDate>Sun, 09 Feb 2025 13:04:26 GMT</pubDate>
    </item>
    <item>
      <title>[r]物理感知视频生成的3D点正则化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1il8ze5/r_3d_point_regularization_for_physicsaware_video/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这项工作引入了一种3D点云正则方法，以改善视频生成中的物理现实主义。核心想法是使用3D点的学习轨迹来限制生成的视频，类似于运动捕获如何帮助创建逼真的动画。 关键技术方面： - 创建了带有100K+视频剪辑的Poi​​ntVid数据集，该数据集用3D点轨迹注释 - 将点云处理与视频生成结合的两阶段架构 - 实现生成运动和实际轨迹之间一致性的物理正则损失 - 学会预测物理上合理的对象运动的点跟踪模块 - 评估指标，以衡量身体一致性和时间相干性  结果显示出显着改善：与基线相比，身体不一致的运动降低了40％ - 更好地保存跨帧的物体形状和结构 - 改进了多对象场景和复杂动作的处理 - 先进的性能 - 标准视频生成基准 - 消融研究证实了3D点正则化的重要性 我认为这种方法对于机器人和模拟而言可能特别有价值，在这种机器人和模拟中，身体准确性比单独视觉质量更重要。该方法提供了一种无需完整的物理模拟就注入物理学理解的方法，这可以实现更快，更实用的应用。 我认为，采用最大的挑战是需要大量的3D点注释。未来的工作可能会探索自动生成这些的方法或从较少的示例中学习。  tldr：添加3D点轨迹约束有助于视频生成模型创造出更现实的运动。新的数据集和正则化方法显示了提高时间一致性的有希望的结果。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1il8ze5/r_3d_point_regularization_for_physicsaware_video/</guid>
      <pubDate>Sun, 09 Feb 2025 06:55:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] AI设计的蛋白质中和致命的蛇毒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1il78ti/r_aidesigned_proteins_neutralize_lethal_snake/</link>
      <description><![CDATA[在/www.nature.com/articles/s41586-024-08393-x    研究人员使用alphafold 2（AF2）和rfdiffusion（开源模型）来设计与并将结合并将（理论上）中和眼镜蛇毒素中和细胞毒素。他们还选择水溶性蛋白，以便可以作为抗蛇毒药物递送。在人皮细胞（角质形成细胞）和小鼠中测试候选蛋白。在实验室条件和浓度中，在模拟咬合后15-30分钟治疗小鼠。 我已经看了一堆生物 + ml纸，从未将其视为应用  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/prototypist     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1il78ti/r_aidesigned_proteins_neutralize_lethal_snake/</guid>
      <pubDate>Sun, 09 Feb 2025 05:04:24 GMT</pubDate>
    </item>
    <item>
      <title>[p]从划痕ML库（火车从CNN到玩具GPT-2）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ikzdsb/p_fromscratch_ml_library_trains_models_from_cnns/</link>
      <description><![CDATA[在/p&gt; 我构建了一个机器学习库（ github ）完全使用Python和Numpy。然后，我用它来训练一系列型号，包括从经典的CNN，重新注册，RNN和LSTM到现代变形金刚，甚至是玩具GPT-2。动机来自我对如何从头开始建立深度学习模型的好奇心，例如从数学公式来看。我构建了这个项目，不是为了取代Pytorch或Tensorflow等生产就绪的库，而是剥离抽象并揭示机器学习的基本数学。  关键点：     所有内容都是在代码中得出的 - 没有不透明的黑匣子。  api镜像pytorch，因此您可以快速拾取它。 您可以训练CNNS，RNNS ，变形金刚，甚至是GPT模型。 设计/调试的设计多于原始性能。    这里有什么不同？   虽然有许多功能强大的ML库（Tensorflow，Pytorch，Scikit-Learn等），但它们通常将基础数学隐藏在抽象层后面。我相信要真正掌握这些工具，您首先需要了解它们如何从头开始工作。该项目明确地衍生了代码中的所有数学和计算操作，使其成为动手的资源，以加深对神经网络和图书馆建筑的理解：）  检查一下：     github repository     api documentation     by-hand/tree/main/示例“&gt;：探索示例中的gpt-2，CNN，变形金字和lstms之类的模型/文件夹   blog Post ：阅读有关项目的动机，设计和挑战   &lt; P&gt;我很想听听任何想法，问题或建议 - 感谢您检查一下！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/megadragon9     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1ikzdsb/p_fromscratch_ml_library_trains_models_models_from_cnns/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ikzdsb/p_fromscratch_ml_library_trains_models_from_cnns/</guid>
      <pubDate>Sat, 08 Feb 2025 22:21:58 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;      对于那些寻找工作的人请使用此模板  &lt; p&gt;想要被录用：[位置]，薪水期望：[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简短概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，这个社区是适合有经验的人。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/automoderator     [link]   ＆＃32;  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>