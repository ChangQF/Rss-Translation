<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Thu, 21 Mar 2024 12:23:22 GMT</lastBuildDate>
    <item>
      <title>[R] 自然语言指令诱导神经元网络中的成分泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk4fyk/r_natural_language_instructions_induce/</link>
      <description><![CDATA[论文：https://www.nature.com/articles/s41593-024-01607-5 代码：https://github.com/ReidarRiveland/Instruct-RNN/ 摘要：  人类的一项基本认知壮举是解释语言指令，以便在没有明确任务经验的情况下执行新任务。然而，可用于实现这一目标的神经计算仍然知之甚少。我们利用自然语言处理的进步来创建基于语言指令的泛化神经模型。模型接受一组常见心理物理任务的训练，并接收预训练语言模型嵌入的指令。我们最好的模型可以执行以前未见过的任务，仅基于语言指令（即零样本学习），平均正确率可达 83%。我们发现，语言支撑着感觉运动表征，使得相关任务的活动与指令的语义表征共享共同的几何形状，从而使语言能够提示在看不见的环境中练习技能的正确组成。我们展示了该模型如何仅使用运动反馈来生成对其识别的新任务的语言描述，这随后可以指导合作伙伴模型执行该任务。我们的模型提供了几个可通过实验测试的预测，概述了如何表示语言信息以促进人脑的灵活和一般认知。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk4fyk/r_natural_language_instructions_induce/</guid>
      <pubDate>Thu, 21 Mar 2024 11:35:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] Larimar：具有情景记忆控制的大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk48sg/r_larimar_large_language_models_with_episodic/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.11901 摘要：  高效准确地更新大型语言模型（LLM）中存储的知识）是当今最紧迫的研究挑战之一。本文提出了Larimar——一种新颖的、受大脑启发的架构，用于通过分布式情景记忆增强法学硕士。 Larimar 的内存允许动态、一次性更新知识，而不需要计算成本高昂的重新训练或微调。多个事实编辑基准的实验结果表明，即使在具有挑战性的顺序编辑设置中，Larimar 也能达到与最具竞争力的基线相当的准确性，而且在速度方面也表现出色 - 根据基础 LLM 的不同，可实现 4-10 倍的加速 - 以及灵活性由于所提出的架构很简单，与 LLM 无关，因此具有通用性。我们进一步提供了 Larimar 选择性事实遗忘和输入上下文长度泛化的机制，并展示了它们的有效性。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk48sg/r_larimar_large_language_models_with_episodic/</guid>
      <pubDate>Thu, 21 Mar 2024 11:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 微调 GPT-4 以产生用户友好的数据探索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk42e1/p_finetuning_gpt4_to_produce_user_friendly_data/</link>
      <description><![CDATA[我们（一家小型初创公司）最近在微调 LLM（主要是 OpenAI 模型）以根据用户请求生成数据探索和报告方面取得了相当大的成功。我们提供数据模式的相关详细信息作为输入，并期望 LLM 生成用我们的自定义领域特定语言编写的响应，然后将其转换为 UI 探索。 我们在博客文章：https://www.supersimple.io/blog/gpt-4-fine -tuning-early-access 我很好奇是否有人在其他领域探索过类似的方法，或者可能在类似的上下文中使用了完全不同的技术。此外，我们是否可以通过一些方法来简化我们自己的管道？   由   提交 /u/PipeTrance   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk42e1/p_finetuning_gpt4_to_produce_user_friendly_data/</guid>
      <pubDate>Thu, 21 Mar 2024 11:12:37 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]同时使用边界框数据和分割数据训练分割模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk3ltg/discussion_training_a_segmentation_model_with/</link>
      <description><![CDATA[您好， 我有两个数据集：一个用于对象检测，另一个用于分割，目前，我们使用两种模型，一种用于检测，一种用于分割，但底层数据非常相似。  是否有任何关于训练使用混合数据执行分割和检测的单一模型的资源？ （检测框和分割掩码） 似乎 RCNN 和 DETR 这样的模型可以通过在使用一种或另一种数据类型时巧妙地屏蔽梯度来训练这两项任务，但我无法轻松找到文章或博客发布详细说明他们是如何做的。  我错过了什么吗？是否有更微妙的东西阻止这样做？  谢谢   由   提交 /u/mseurin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk3ltg/discussion_training_a_segmentation_model_with/</guid>
      <pubDate>Thu, 21 Mar 2024 10:43:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 发布“1 位法学硕士时代”的培训代码及更多内容</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk32ye/d_training_code_and_more_released_for_the_era_of/</link>
      <description><![CDATA[不幸的是，微软没有发布权重，但如果权重介于权重和训练代码之间，那么这当然是更好的选择。 这篇文章在这里。  我们怎么想？关于形状奇怪的损失曲线有什么想法吗？您认为这种方法在 4B 参数之后会失效吗？法学硕士如何在如此低的精度下工作？我知道这不是特别科学，但对我来说，你可能会在几张 CD 上安装一个功能齐全的 7B 参数模型，这似乎相当违反直觉……然而最重要的是，谁有价值 100,000 美元的计算来实际测试这个？   由   提交 /u/DickMasterGeneral   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk32ye/d_training_code_and_more_released_for_the_era_of/</guid>
      <pubDate>Thu, 21 Mar 2024 10:09:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何使用LLM模型（GPT）分析大代码（覆盖率数据）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bk2t2a/d_how_to_use_llm_models_gpt_to_analyze_big_code/</link>
      <description><![CDATA[大家好。 因此，对于我的一个项目，我想分析测试覆盖率。所以我有一个 JSON 文件，其中包含一个测试及其涵盖的方法。但问题是数据的大小有时可能会变大。所以我想知道如何分析太大的代码数据。我尝试使用 llamaIndex 进行矢量存储索引。但即使是简单的查询，它也无法给我正确的答案。我需要你对此事的建议。我在这里给出了一个示例数据样本。假设我想分析这个包含各种覆盖率信息的 JSON 文件。 { &quot;bug_id&quot;: 43, &quot;tests&quot;: [ { &quot;test_name&quot;: &quot;org. apache.commons.lang.text.ExtendedMessageFormatTest.testEscapedQuote_LANG_477”，“test_body”：“98：public void testEscapedQuote_LANG_477（）{\ n99：字符串模式= \“它”是一个{0，lower}“测试”！ \&quot;;\n100: ExtendedMessageFormat emf = new ExtendedMessageFormat(pattern,registry);&quot;, &quot;covered_methods&quot;: [ { &quot;method_signature&quot;: &quot;org.apache.com....&quot;, &quot;method_body&quot; ;: &quot;142: public final void applyPattern(String pattern) {\n143: if (r....&quot;, &quot;method_id&quot;: 0, }, } ] }  谢谢!!!!!    提交者    /u/soapaos   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bk2t2a/d_how_to_use_llm_models_gpt_to_analyze_big_code/</guid>
      <pubDate>Thu, 21 Mar 2024 09:49:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年 pgvector 与 pgvecto.rs：PostgreSQL 中向量搜索的全面比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjyy8n/d_pgvector_vs_pgvectors_in_2024_a_comprehensive/</link>
      <description><![CDATA[https://blog.pgvecto.rs/pgvector-vs-pgvectors-in-2024-a-compressive-comparison-for-vector-search-in-postgresql  ​ 你喜欢传统的数据库+矢量搜索扩展，还是专门的矢量数据库？  &amp; #32；由   提交/u/gaocegege  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjyy8n/d_pgvector_vs_pgvectors_in_2024_a_comprehensive/</guid>
      <pubDate>Thu, 21 Mar 2024 05:16:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 高频数据的 TimeGPT</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjwyl9/d_timegpt_for_high_frequency_data/</link>
      <description><![CDATA[是否有人尝试过在每日级别数据以外的其他数据上使用 TimeGPT？我尝试将其用于分钟级数据作为提前 1 小时预测的基础，但它无法推断频率。    由   提交 /u/seoulsrvr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjwyl9/d_timegpt_for_high_frequency_data/</guid>
      <pubDate>Thu, 21 Mar 2024 03:24:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 发展新的基础模型：释放自动化模型开发的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjuddy/r_evolving_new_foundation_models_unleashing_the/</link>
      <description><![CDATA[Sakana AI 发布新论文。 博客文章：https://sakana.ai/evolutionary-model-merge/ 论文：模型合并配方的进化优化 https://arxiv.org/abs/2403.13187    ;由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjuddy/r_evolving_new_foundation_models_unleashing_the/</guid>
      <pubDate>Thu, 21 Mar 2024 01:18:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 AlphaFold 2 论文中，有人可以帮我解释一下图 2 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjoopg/d_in_the_alphafold_2_paper_can_someone_explain/</link>
      <description><![CDATA[论文：https: //www.nature.com/articles/s41586-021-03819-2 此处的图 2：https://www.nature.com/articles/s41586-021-03819-2/figures/2 在图2中，他们展示了一系列图表，但我不明白作者想要表达的要点是什么。有一个简短的段落提到了它：  当主干预测准确时，我们观察到高侧链准确性（图 2b），我们表明我们的置信度测量，预测的局部距离差异检验（pLDDT），可靠地预测了 Cα 局部距离差异检验（ lDDT-Cα）相应预测的准确性（图2c） 。我们还发现全局叠加度量模板建模得分（TM-score）27 可以准确估计（图2d）。总的来说，这些分析验证了 AlphaFold 对 CASP14 蛋白的高精度和可靠性也转移到了最近提交的 PDB 未整理的集合中，正如预期的那样  我对图 2 A 的解释是它显示了特定埃的蛋白质链分数的直方图。这很简单。 图 2.b。非常简单。这是一个向右上方移动的图，显示了主链精度和侧链精度之间的相关性。 lDDT-Cα 仅使用 Cα 原子来测量主链精度，而 lDDT（局部差异距离测试）是一种度量结构一致性。 图2.c。这是让我感到困惑的地方。文中提到： lDDT-Cα = 0.997 × pLDDT − 1.17（Pearson&#39;s r = 0.76）。 n = 10,795 条蛋白质链 这应该是图中的蓝线吧？这些点是单独的蛋白质链，即。 n = 19,795 那么，蓝线代表预测，蓝点代表“基本事实”吗？蛋白质链？这就是为什么蓝线不是穿过蓝点云绘制的，而是稍微偏离右侧吗？ 此外，图形标题中线性拟合的阴影区域是否指的是蓝色插图中的点还是其他内容？  线性拟合的阴影区域表示根据 10,000 个 bootstrap 样本估计的 95% 置信区间。  图 2 .d.我对此仍然很困惑。它应该类似于图 2.c。   由   提交/u/christophr88   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjoopg/d_in_the_alphafold_2_paper_can_someone_explain/</guid>
      <pubDate>Wed, 20 Mar 2024 21:17:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2024 接受去匿名论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_paper_accepted_at_iclr_2024/</link>
      <description><![CDATA[我想开始讨论我在今年 ICLR 上观察到的一个特定案例。 - A 论文提交给ICLR双盲审稿人全名第一作者和完整的致谢（以及一些备受瞩目的参考文献）位于主要论文正文中，位于参考书目的正上方。作者明确确认了虚假陈述“无致谢部分：我证明本次提交的材料中没有供双盲评审的致谢部分。”提交时。 - 四位审稿人和 AC 避免提及这一点，并审阅论文了解作者的偏见，就好像它没有违反征文中列出的基本提交规则。  - 一月中旬发布的论文决定是“桌面拒绝” （推测是由于上述违规行为，确认PC们已经意识到这一点）。此内容未公开存档。 - 2 月初将其更改为“口头”内容。没有进一步的理由，原因不明。 违反匿名规则的行为首先会被评审者忽略，然后程序委员会通过例外情况默默地允许，大概是在直接投诉之后。在我看来，这对于任何会议来说都是不可接受的，尤其是像ICLR这样的领域顶级会议。如果我们不强制执行并允许一些作者选择透露自己的姓名并对审稿人产生偏见，为什么我们还要实行双盲程序呢？如果作者并不出名，或者他们在一个不太享有特权的地方进行研究，或者来自边缘化社区，是否也会有同样的例外？其他在今年 ICLR 上被拒绝但没有获得例外机会的论文又如何呢？ 根据我的经验，学术界关于偏见和诚信的问题经常在私人谈话中提出，但几乎从未公开讨论过，所以我有兴趣听听社区对此案的看法。项目主席拒绝对最终决定发表评论。   由   提交/u/Melodic-Foundation47  /u/Melodic-Foundation47 reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_pa​​per_accepted_at_iclr_2024/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_paper_accepted_at_iclr_2024/</guid>
      <pubDate>Wed, 20 Mar 2024 18:32:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia 最新的 Blackwell GPU 将减少多少训练和推理时间/价格？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</link>
      <description><![CDATA[显然训练 Llama-2 花费了约 500 万美元。您认为这些新 GPU 会降低类似模型的训练/推理成本多少？我很好奇，因为我很可能很快就会购买苹果产品。   由   提交 /u/dittospin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</guid>
      <pubDate>Wed, 20 Mar 2024 16:34:13 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 不到 300 行的 Python + pytorch 从头开始​​稀疏混合专家语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</link>
      <description><![CDATA[大家好，我实现了专家语言模型的稀疏混合（基本上是 Mixtral、Grok-1 和据称的 GPT-4 中使用的微型版本） ）在纯 pytorch 中从头开始，并用小莎士比亚对其进行训练。这主要基于 Andrej Karpathy 的 makemore（仅自回归字符级解码器变压器模型）。我的目标是让它成为一个可破解的实现，人们可以用它来理解它是如何真正工作和改进的。我预计全年会出现越来越多的此类模型。 存储库位于：https://github。 com/AviSoori1x/makeMoE 我几个月前创建了这个并在 Localllama 上分享，但我想我也应该在这里分享，因为我做了一些更新，例如添加专家能力和整合整个实现少于 300 行可读的 python + pytorch。逐步完成此操作的博客位于：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch 。希望这对您有所帮助！   由   提交/u/avi1x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</guid>
      <pubDate>Wed, 20 Mar 2024 15:20:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么学术论文的可读性持续不佳？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</link>
      <description><![CDATA[  其中一些期望是不可避免的。论文绝对必须假设该领域特有的广泛背景知识和词汇——包括每篇论文对该领域的基本介绍都是多余的。有时论文假设了太多背景，但作为作者或读者很难判断，尤其是因为人们知道的事情会随着时间和不同背景而变化。 参考 在过去的3年里，为了拓宽我对ML领域的理解，我读了很多论文，但没有考虑过专注于某个领域特定领域的特定问题。   这使我能够理解学术写作的结构、机器学习的基本概念以及与以前的知识相比相对宝贵的见解。过去，但当我遇到新论文时，有些论文仍然很冗长，由于学术论文性质的可读性而让我烦恼。 这强烈要求我浏览一些内容，这对纯粹的学术论文是有害的。与非学术书籍相比，专注于阅读文档（我并不认为所有书籍都像个人畅销书一样出色，但我想强调学术写作和非学术写作之间的结构差异。） 问题是，为什么这个约定不能顺利地改变到更好的方向，对现有的和新的研究人员有帮助？   由   提交/u/Mundane_Definition_8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</guid>
      <pubDate>Wed, 20 Mar 2024 09:04:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>