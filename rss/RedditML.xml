<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 25 Jan 2024 18:17:53 GMT</lastBuildDate>
    <item>
      <title>[D] 稳定扩散模型如何利用 U-Net 和卷积层？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ffkau/d_how_stable_diffusion_model_utilizes_unet_and/</link>
      <description><![CDATA[当我读到稳定扩散模型时，他们通常会谈论调整卷积层或 U-Net 权重。我相信它们应该是相关的，U-Net是接受来自VAE编码器的编码图像+文本嵌入的部分，并使用卷积层从图像中提取特征，然后向这些特征添加噪声，然后对它们进行去噪并将输出作为潜在向量/矩阵发送到 VAE 解码器。 ​ 但我不确定我的理解是否完全正确？   由   提交 /u/thefreemanever   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ffkau/d_how_stable_diffusion_model_utilizes_unet_and/</guid>
      <pubDate>Thu, 25 Jan 2024 18:04:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自监督模型在预训练后抛出的参数方面如何比较？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ff9rk/d_how_do_selfsupervised_model_compare_in_terms_of/</link>
      <description><![CDATA[特别是在视觉的掩模图像建模和对比学习中，您可以采用编码器-解码器架构，然后在其中删除/取消解码器预训练，或者以 MLP 的形式附加一两个投影头来处理编码器的输出。在 MIM 和 CL 以及 ConvNet 和 ViT 中最常用的模型中，这些模块中参数的绝对和相对数量是多少？您是否知道专门针对此问题、选择及其在训练轨迹、学习偏差和不变性、下游性能等方面可能意味着什么的研究？  &amp; #32；由   提交/u/reverendCappuccino   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ff9rk/d_how_do_selfsupervised_model_compare_in_terms_of/</guid>
      <pubDate>Thu, 25 Jan 2024 17:52:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 处理大数据帧以进行特征提取</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19feb36/p_dealing_with_large_dataframe_for_feature/</link>
      <description><![CDATA[我正在开发一个 ML 项目，用于检测使用 CNC 铣削制造产品时的异常情况。  我们已经对数据进行了预处理，现在尝试在执行 PCA 后使用 tsfresh 提取多变量时间序列数据的特征，但是在大量数据帧（大约 167240000x6 和 240000000x6）期间，它花费了太多时间即使在我的 32 GB RAM、i13900H 处理器上也需要花费大量时间。 花费大量时间是否正常，或者是否有更好的替代方法来提取特征？如果需要更多信息来回答我的问题，请告诉我，提前谢谢您。   由   提交/u/Compliance-Way227   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19feb36/p_dealing_with_large_dataframe_for_feature/</guid>
      <pubDate>Thu, 25 Jan 2024 17:12:57 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 大数据集下载</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19fcjcu/discussion_big_data_set_downloads/</link>
      <description><![CDATA[在处理像 ImageNet 这样的大数据集时，通常的工作流程是什么？我在我的 M1 mac 上下载了该文件，现在正在解压该文件等，但这显然需要很长时间才能完成。机器学习社区的人们是否只是忍受了这么长时间，还是有一种奇怪的方法来加载数据集以通过云服务或其他方法进行快速测试？我是新人，正在努力学习，所以请注意基本问题。谢谢。   由   提交 /u/EasternPiglet7093   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19fcjcu/discussion_big_data_set_downloads/</guid>
      <pubDate>Thu, 25 Jan 2024 15:57:05 GMT</pubDate>
    </item>
    <item>
      <title>[研究]WhisperFusion：与人工智能聊天机器人的超低延迟对话</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19fchit/research_whisperfusion_ultralow_latency/</link>
      <description><![CDATA[通过使用完全开源的工具 WhisperLive &amp; 创建实时 AI 聊天机器人通信系统。 WhisperSpeech，Collabora 的工程师解决了当前机器人交互中的不自然延迟问题，以实现无缝对话。 https://www.collabora.com/news-and-blog/news-and-events/whisperfusion-ultra-low- Latency-conversations-with-an-ai-chatbot.html   由   提交 /u/mfilion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19fchit/research_whisperfusion_ultralow_latency/</guid>
      <pubDate>Thu, 25 Jan 2024 15:54:48 GMT</pubDate>
    </item>
    <item>
      <title>[D]K8S的基本概念有哪些，从哪里学？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19fc6xu/d_what_are_the_fundamental_concepts_of_k8s_and/</link>
      <description><![CDATA[我是一名 20 多岁的数据科学家，开始学习 MLOps/ML 工程。 拥有以下领域的本科学位：经济学和数据科学硕士学位，我对算法和数据结构有一定的了解（基本上我可以在 1 小时内做 2 道 Leetcode Medium 问题），但对计算机系统、计算机网络等方面了解不多。 所以，现在我正在学习K8S &amp; Kubeflow，主要是通过阅读文档和遵循教程，我一头雾水。更具体地说，我可以按照说明进行操作，但只能理解表面，总感觉缺少一两个抽象层次。当我第一次学习像决策树这样的机器学习算法时，我就有过这种感觉，因为它的机制非常简单。尽管如此，只有当我进入研究生院并深入了解熵等概念时，它才最终对我来说真正有意义。 所有这一切只是为了询问您是否可以帮助指出 1 或 2 个主题/概念，以及如果有任何学习资源能够更深入地了解 K8s，那将会非常有帮助。谢谢大家！！！   由   提交/u/Pancake502  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19fc6xu/d_what_are_the_fundamental_concepts_of_k8s_and/</guid>
      <pubDate>Thu, 25 Jan 2024 15:41:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] ML博客-多项式特征</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19fb0tg/p_ml_blog_polynomial_features/</link>
      <description><![CDATA[在机器学习初学者课程中，当教授曲线拟合的线性回归时，他们告诉我们高次多项式是一个大禁忌！我们被告知它们会振荡和过度拟合，并且无法通过正则化来控制。 好吧，我希望让您相信，在某种程度上，这是一个神话。这是该系列的第一篇文章： https://alexshtf.github.io/2024 /01/21/Bernstein.html  祝阅读愉快！   由   提交/u/alexsht1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19fb0tg/p_ml_blog_polynomial_features/</guid>
      <pubDate>Thu, 25 Jan 2024 14:48:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您经常遇到哪些令人尴尬的并行工作负载（没有节点间通信）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19faakw/d_what_embarrassingly_parallel_workloads_do_you/</link>
      <description><![CDATA[目前，几周后就会发布一个开源工具，该工具使大规模并行计算变得极其容易。  当我发布它时，我想要一些有用的教程。我想知道您认为我应该为哪些令人尴尬的并行用例创建教程？如果您可以在不需要任何配置的情况下运行 25k 个并行工作线程，您将运行哪些作业？    由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19faakw/d_what_embarrassingly_parallel_workloads_do_you/</guid>
      <pubDate>Thu, 25 Jan 2024 14:15:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 这是做LDA的正确方法吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f8qos/d_is_this_the_correct_way_to_do_lda/</link>
      <description><![CDATA[我有这段代码，但我不确定我是否正确执行。让我告诉你我想要它做什么。我希望它遍历 pandas DataFrame 的一列并提取其中出现的最常见主题。通常这些是对调查问题的答复。我有巨大的 .csv 文件，我需要提取用户最常所说的话。  代码如下： from collections import Counter from gensim import corpora, models from nltk.corpus import stopwords from nltk.stem.wordnet import WordNetLemmatizer import string import pandas as pd df = pd.read_csv(“some_file.csv”) #从数据集中提出一个问题 text_data = df[&#39;Why did you buy this product?&#39;].dropna() # 预处理数据 stop = set(stopwords .words(&#39;english&#39;)) 排除 = set(string.punctuation) lemma = WordNetLemmatizer() def clean(doc): stop_free = &quot; &quot;.join([str(doc).lower().split() 中的字对字，如果单词不在 stop 中]) punc_free = &#39;&#39;.join(ch for ch in stop_free 如果 ch 不在排除中) 标准化 = &quot; ; &quot;.join(lemma.lemmatize(word) for word in punc_free.split()) return normalized text_data_clean = [clean(doc).split() for doc in text_data] # 创建主题建模所需的字典和语料库dictionary = corpora.Dictionary(text_data_clean) doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_data_clean] # 应用LDA ldamodel = models.ldamodel.LdaModel(doc_term_matrix, num_topics=10, id2word=dictionary, Passs=50) # 确定最有可能的每个文档的主题 document_topics = [max(ldamodel.get_document_topics(bow), key=lambda x: x[1])[0] for Bow in doc_term_matrix] # 统计每个主题出现的次数 topic_counts = Counter(document_topics) #按频率对主题排序 Sorted_topics = Sorted(topic_counts.items(), key=lambda item: -item[1]) # 打印排序后的最常见主题及其计数 for topic_num, count in Sorted_topics: print(f&quot;Topic #{topic_num } （出现 {count} 次）：&quot;) print(ldamodel.show_topic(topic_num, 5))  提前谢谢您！   由   提交 /u/trp_wip   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f8qos/d_is_this_the_correct_way_to_do_lda/</guid>
      <pubDate>Thu, 25 Jan 2024 12:58:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用全同态加密 (FHE) 在加密数据上训练 ML 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f5z25/p_training_ml_models_on_encrypted_data_with_fully/</link>
      <description><![CDATA[大家好！  我们已经使用 FHE 成功地训练了加密数据的机器学习模型，确保了整个训练过程中最高级别的隐私。  这是解锁数据隐私至关重要的医疗保健和金融等领域的安全协作培训和模型微调等用例的关键一步。  为了让您了解预期的性能，我们可以在大约一个小时内训练一个具有 10 个特征和 10,000 行的模型。更重要的是，训练时间与特征和示例的数量呈线性关系。  您还可以在这里查看我们的库，因为我们所做的一切都是开源的：https: //github.com/zama-ai/concrete-ml 很高兴听到您对此的想法和想法！   由   提交 /u/strojax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f5z25/p_training_ml_models_on_encrypted_data_with_fully/</guid>
      <pubDate>Thu, 25 Jan 2024 10:02:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有可靠的机器学习测试和监控工具吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f3nsd/d_any_reliable_tool_for_ml_testing_and_monitoring/</link>
      <description><![CDATA[嘿 我正在进行一个需要对机器学习模型进行彻底测试和监控的项目，我一直在寻找寻求可靠的开源工具来提供帮助。有人对强大且同时用户友好的东西有任何建议吗？   由   提交 /u/UpvoteBeast   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f3nsd/d_any_reliable_tool_for_ml_testing_and_monitoring/</guid>
      <pubDate>Thu, 25 Jan 2024 07:13:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Scikit-Learn 修复了 F-1 分数计算器；你应该现在更新</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f0o8f/d_scikitlearn_fixed_its_f1_score_calculator_you/</link>
      <description><![CDATA[Scikit-Learn 1.3.x 的 F-1 分数计算器有一个错误，该错误已在最新版本（上周发布的 1.4.0）中修复），当 zero_division 参数设置为 0.0 或 np.nan 时，可能会产生错误的分数，例如：  &lt;代码&gt;&gt;&gt; sklearn.__version__&#39;1.3.2&#39;&gt;&gt;&gt;&gt; sklearn.metrics.f1_score(y_true=[0, 0, 1, 2, 3], y_pred=[0, 1, 0, 2, 3], Zero_division=1.0,average=“宏”) 0.875 # 错误  与。 （完全相同的输入） &gt;&gt;&gt; sklearn.__version__&#39;1.4.0&#39;&gt;&gt;&gt;&gt; sklearn.metrics.f1_score(y_true=[0, 0, 1, 2, 3], y_pred=[0, 1, 0, 2, 3], Zero_division=1.0,average=“宏”) 0.625 # 正确  这里是我的博客文章解释了该错误更多详细信息，以及修复该错误的拉取请求。如果您使用 Scikit-Learn 计算 F-1，您应该升级并仔细检查之前计算的 F-1 分数；考虑到真正的 F-1，看起来更好的分类器很容易比替代品差得多。   由   提交/u/Revolutionary-Ad-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f0o8f/d_scikitlearn_fixed_its_f1_score_calculator_you/</guid>
      <pubDate>Thu, 25 Jan 2024 04:15:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 注意力之谜：哪个是哪个 - q、k 或 v？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ewfm9/d_attention_mystery_which_is_which_q_k_or_v/</link>
      <description><![CDATA[我终于开始了解注意力机制了，但有一点仍然让我困惑：q、k 和 v 背后的矩阵魔法。&lt; /p&gt; 我在理论层面上了解了整个矩阵乘法，但是什么数学属性实际上决定了哪个矩阵成为查询（q），即键 (k) 和值 (v)？这只是一些随机分配，还是有更深层次的逻辑在起作用？  这是我到目前为止收集到的内容：  所有三个矩阵都来自相同的输入数据，但神奇地呈现出不同的“个性”。在注意方程 (qkt)v 中。 我猜测它们的维度和相互作用一定发挥了作用，但除此之外，它是模糊的。  机制框图对于图片 https://upload.wikimedia .org/wikipedia/commons/thumb/8/81/Attention-qkv.png/799px-Attention-qkv.png   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ewfm9/d_attention_mystery_which_is_which_q_k_or_v/</guid>
      <pubDate>Thu, 25 Jan 2024 00:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 视觉变压器比新生视觉系统更需要数据吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19er4pp/r_are_vision_transformers_more_data_hungry_than/</link>
      <description><![CDATA[ 由   提交 /u/currentscurrents   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19er4pp/r_are_vision_transformers_more_data_hungry_than/</guid>
      <pubDate>Wed, 24 Jan 2024 20:59:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>