<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 07 May 2024 03:16:17 GMT</lastBuildDate>
    <item>
      <title>关于构建AI GPU集群节点的担忧 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clzh29/concerns_regarding_building_out_nodes_for_ai_gpu/</link>
      <description><![CDATA[以下是我所在地区可用的一些选项，我想选择 2011 年，因为 CPU 的性价比非常高核心和线程，因此有 2 个平台：X79 和 X99。 DDR3 比 DDR4 便宜得多，尽管性能几乎没有下降，但 x99 主板仅配备 DDR4，没有任何 DDR3 主板。至于 GPU，我选择了 mi50 16gb，因为它在这里的售价仅为 130 美元左右。经过一番研究后，我发现： 担忧：  我打算进行视频生成模型训练，但我仍然相对不确定是否可以RAM 非常重要，似乎拥有大量 RAM 您可以在磁盘上执行更少的流数据，并将其卸载到 Ram 以便从 GPU 更快地访问。如果你不这样做，我认为这只会阻碍数据读取速度？ 至于存储数据，我不知道我是否真的需要为此构建一个存储集群？似乎也可以将数据传输到节点，尽管速度会很慢？或者可能只是进行数据切片，以便数据量对于任何节点来说都不会太大？我是否可以先用 10TB 数据进行训练，然后因为我的磁盘已满，删除当前批次数据并获取另外 1OTB 数据然后继续训练，这可能吗？ 至于 MI50 作为好吧，看来 rocm 已经放弃了对这张卡的支持，我打算使用 Zluda，基本上是 AMD CUDA 之上的嵌入式驱动程序，它使用 Rocm 5.7，这会影响 GPU 的稳定性吗？如果我使用 Zluda 在 Pytorch 上进行训练，所有这些？  选项#1：可能 Ram 受限但更少？  主要：X79 5 插槽 3.0 x8&lt; /li&gt; 内存：32gb DDR3 CPU：2696v2 GPU：5x MI50 16GB  选项 #2：- 内存受限?  主机：X79 9 插槽 3.0 x8 内存：32gb DDR3 CPU：双 2696v2 GPU：9x MI50 16GB  选项 #3：Pcie 通道受限？  主：X79 8 插槽 2.0 * x1 内存：64gb DDR3  CPU：双 2696v2 GPU：8 个 Mi50 16GB    由   提交/u/Ok_Difference_4483   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clzh29/concerns_regarding_building_out_nodes_for_ai_gpu/</guid>
      <pubDate>Tue, 07 May 2024 01:07:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT-2 训练的数据准备</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clx2bi/d_data_preparation_for_gpt2_training/</link>
      <description><![CDATA[      大家好， 我正在创建一个数据集来使用 nanoGPT 存储库训练/微调 GPT-2。我在txt文件中有这样的格式： SENTENCE TREE PARSING &lt;|endoftext|&gt; SENTENCE TREE PARSING &lt;|endoftext|&gt; 等等（在分词过程中添加了特殊的分词，\n\n 被它替换）。 这是正确的吗？或者我应该使用其他格式的模型来学习解析句子？ 谢谢！😊 一些示例   由   提交/u/NeatFox5866   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clx2bi/d_data_preparation_for_gpt2_training/</guid>
      <pubDate>Mon, 06 May 2024 23:11:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 参与补助金</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cloxpt/d_icml_participation_grant/</link>
      <description><![CDATA[作为加拿大的一名博士生，并在 ICML 上发表了一篇已被接受的论文，我对参加这些昂贵的会议的资金选择感到好奇。虽然我的主管承担了一些费用，但总费用可达 3500-4000 加元，其中包括 700 加元的注册费。是否有其他外部资金来源可用于支付剩余费用？   由   提交/u/Personal_Click_6502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cloxpt/d_icml_participation_grant/</guid>
      <pubDate>Mon, 06 May 2024 17:33:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 羊驼 3 怪物</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cljvpa/d_llama_3_monstrosities/</link>
      <description><![CDATA[我刚刚注意到有人通过将 Llama 3 与自身合并来创建了 Llama 3 的 120B Instruct 变体（最终结果重复了 60 / 80 层）。他似乎专门研究这些弗兰肯斯坦模型。对于我的一生来说，我实在无法理解这种趋势。使用 mergekit 可以轻松轻松地创建这些，我想知道它们在野外的商业用途。 Bud 甚至承认它并不比 GPT-4 更好。那么有什么意义呢？哦等等，他写到了帖子的结尾，并提到他已将其提交给 Open LLM Leaderboard……我们开始吧。 LLM排行榜攀登的游戏化很累人。   由   提交 /u/Objective-Camel-3726   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cljvpa/d_llama_3_monstrosities/</guid>
      <pubDate>Mon, 06 May 2024 14:04:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 通过 ResearchHub 上的同行评审获得报酬</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clg7mu/d_get_paid_for_peer_reviews_on_researchhub/</link>
      <description><![CDATA[ResearchHub 奖励各种主题的同行评审，包括我是该主题编辑的 AI。每篇同行评审的报酬约为 150 美元（以加密货币支付，但可轻松兑换成美元）。以下是一些目前有同行评审赏金的论文，但请记住，经常会添加新论文（您也可以上传您感兴趣的论文进行评审）：  语言模型物理学：第 3.3 部分，知识容量缩放定律 细粒度混合专家的缩放定律 Mixture-Of-Depths：在基于 Transformer 的语言模型中动态分配计算 基于深度学习的微观视觉的可解释性分析与注意力机制  要获得赏金，只需在论文的“同行评审”选项卡中发布您的评论，如果质量足够，您就会获得赏金。我很乐意回答您的任何问题！    提交人    /u/Troof_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clg7mu/d_get_paid_for_peer_reviews_on_researchhub/</guid>
      <pubDate>Mon, 06 May 2024 11:00:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 针对大文本数据的 NER</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clfe3o/d_ner_for_large_text_data/</link>
      <description><![CDATA[大家好，我目前在初创公司担任数据科学家。我们需要从 100 亿个 token 的文本中提取实体。我不知道如何在如此大的规模上做到这一点。应该有什么管道等等。如果你们分享您的知识或好的研究论文/博客，这将会很有帮助。目前我们正在研究 18 个实体，我的老板希望我获得 93% 的准确率。谢谢   由   提交/u/Boring_Astronaut_421   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clfe3o/d_ner_for_large_text_data/</guid>
      <pubDate>Mon, 06 May 2024 10:06:16 GMT</pubDate>
    </item>
    <item>
      <title>[P]表格提取、文本提取</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clf2sg/p_table_extraction_text_extraction/</link>
      <description><![CDATA[        由   提交 /u/Trick_Care9342   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clf2sg/p_table_extraction_text_extraction/</guid>
      <pubDate>Mon, 06 May 2024 09:44:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] LeRobot：Hugging Face 的现实世界机器人库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/</link>
      <description><![CDATA[      认识一下 LeRobot，一个库托管最先进的机器人深度学习。 人工智能开发的下一步是将其应用到我们的物理世界。因此，我们正在围绕机器人人工智能构建社区驱动的工作，并且向所有人开放！ 看一下代码： https://github.com/huggingface/lerobot https://preview.redd.it/ugf4l8lfgryc1.png?width=3794&amp;format=png&amp;auto=webp&amp;s=222825e897ba48eb07acedffb0662d5794af04 e8 乐机器人是对于机器人技术来说，就像 Transformers 库对于 NLP 一样。它提供了带有预先训练的检查点的高级人工智能模型的干净实现。我们还重新实现了来自学术界的 31 个数据集和一些模拟环境，无需物理机器人即可开始使用。 Aloha项目。 [视频链接] https://preview.redd.it/86ihkcwhgryc1.png?width=2506&amp;format=png&amp;auto=webp&amp;s=4f2ca7522a012d00d7327d903 35d069dd099a321 LeRobot 的另一个可视化，这次是在 Mobile Aloha 数据上，学习完全端到端的导航和操作。这两个数据集都是在 trossenrobotics 机器人手臂上收集的。 [视频链接] https://preview.redd.it/qqtncqligryc1.png?width=1900&amp;format=png&amp;auto= webp&amp;s=4f83c675b5c6f9dbded4b5b90a7a1c9f531c4086 LeRobot 代码库已通过在模拟中复制最先进的结果进行了验证。例如，这里是著名的 ACT 策略，它已被重新训练并可用作预训练检查点： [HF HUB 链接] LeRobot 还具有扩散政策，强大的模仿学习算法，以及TDMPC，一种包含世界模型的强化学习方法，不断从与环境的交互中学习。 https://preview.redd.it /br9ibrylgryc1.png?width=1684&amp;format=png&amp;auto=webp&amp;s=8e5595f1dff5381e5f60c6776126f48187ec58d9 快来加入我们的Discord 频道。我们正在建立一个来自不同背景、软件和硬件的多元化社区，以开发现实世界中的下一代智能机器人！ 感谢人工智能和机器人社区，没有他们就没有乐机器人。可能。   由   提交/u/Tamazy   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/</guid>
      <pubDate>Mon, 06 May 2024 07:48:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] - ML/CV 会议志愿者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cld406/d_volunteers_for_mlcv_conferences/</link>
      <description><![CDATA[大家好， 我想了解一些有关 ML/CV 会议（例如 CVPR、ECCV、ICML 等）志愿服务的信息。特别是：  是否有选择过程？ 志愿者在这些会议上做什么？ 总的来说，值得吗？特别是从网络的角度来看。  谢谢    提交人    /u/backprop_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cld406/d_volunteers_for_mlcv_conferences/</guid>
      <pubDate>Mon, 06 May 2024 07:24:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Kolmogorov-Arnold 网络只是一个 MLP</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/</link>
      <description><![CDATA[事实证明，您可以将 Kolmogorov-Arnold 网络编写为 MLP，并在 ReLU 之前进行一些重复和移位。  https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz &lt; /div&gt;  由   提交 /u/osamc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/</guid>
      <pubDate>Mon, 06 May 2024 07:04:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Gemma 有如此疯狂的大 MLP 隐藏暗尺寸？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clcluq/d_why_gemma_has_such_crazy_big_mlp_hidden_dim_size/</link>
      <description><![CDATA[   /u/kiockete  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clcluq/d_why_gemma_has_such_crazy_big_mlp_hidden_dim_size/</guid>
      <pubDate>Mon, 06 May 2024 06:48:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如果 Llama-3 只有 8K 上下文长度，为什么它能够处理 32K 上下文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/</link>
      <description><![CDATA[大家好！请参阅此处的帖子：https://twitter.com/abacaj/status/1785147493728039111 我没有不明白他所说的“通过零训练（实际上只是一个简单的 2 行配置），你可以从 llama-3 模型中获得 32k 上下文”的意思 有人知道这个动态是什么吗？缩放技巧是？非常感激！ :)   由   提交 /u/sunchipsster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/</guid>
      <pubDate>Mon, 06 May 2024 05:43:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 梯度增强分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clbipd/d_classification_with_gradient_boosting/</link>
      <description><![CDATA[梯度增强分类 我正在尝试对城市地区的物体进行分类，主要是建筑物和植被。我使用激光雷达数据中的几何特征，如平面度、线性垂直度、全方差、最小特征值、曲率变化、球度。我有五个类别的低植被、中植被、高植被和建筑物。并使用 [1...12] 中的半径我进行了随机搜索来查找参数，n_estimator=100，学习率=0.1，min_sample_split=2，min_sample_leaf = 1. 该模型的准确率为 98%。当我预测更大规模的模型时，几乎没有问题。一些建筑物的边缘和三角形屋顶的直线（多见于欧洲城市地区）。在这两种情况下，模型预测它们为高植被。现在我不知道如何继续前进，是增加 n 估计器和学习率，还是找到有助于区分植被和边缘情况的特征。 任何建议将不胜感激，谢谢   由   提交/u/Money_Respect4741   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clbipd/d_classification_with_gradient_boosting/</guid>
      <pubDate>Mon, 06 May 2024 05:35:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在创建神经网络时，是否有更系统的方法来选择层或架构的深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</link>
      <description><![CDATA[所以我正在学习深度学习和神经网络，我对这部分真的有点困惑。我通常熟悉可用的层及其工作原理（至少是那些被广泛使用的层），但我仍然很难弄清楚在什么上使用什么。有没有更合乎逻辑或系统的方法来做到这一点？比如数学什么的？我很想尝试，但我只是想避免陷入兔子洞，因为这个项目是在截止日期前完成的，而且我对此并不失望 ```编辑```` 感谢您的所有回复，特别是提供阅读材料和建议。    由   提交/u/PsychologicalAd7535   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</guid>
      <pubDate>Sun, 05 May 2024 14:04:05 GMT</pubDate>
    </item>
    </channel>
</rss>