<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Mon, 22 Jul 2024 12:29:25 GMT</lastBuildDate>
    <item>
      <title>[讨论] 我什么时候可以将研究模型用于商业目的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9c8ri/discussion_when_i_can_use_research_models_for/</link>
      <description><![CDATA[我正在阅读一篇研究论文，其中他们将扩散模型用于特定目的。我有一个想法，如果正确执行，它可以用于商业目的，具有巨大的市场机会。所以我想知道，如果有研究论文代码、模型架构和训练权重，我有三个问题 1. 我可以使用此模型并将其权重生产化并用于商业吗？ 2. 如果不行，如果在架构上做一些必要的更改或训练它新的数据集或两者兼而有之，用于商业目的当我遇到法律或版权许可问题时    提交人    /u/Frosty-Equipment-692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9c8ri/discussion_when_i_can_use_research_models_for/</guid>
      <pubDate>Mon, 22 Jul 2024 11:47:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] PINN（物理信息神经网络）的方程要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</link>
      <description><![CDATA[我对损失项中的微分方程有疑问。通常，在 PINN 中，我们在损失函数中使用预测输出相对于输入变量的微分方程。例如，如果 u 是预测输出，x、y、m 是输入，则损失函数包括 du/d(x,y,m) 等项。 但是，如果我们只有输入变量相对于其他输入或输出变量的微分方程会怎样？例如：  dx/dt=f(x,y,u) dy/dt=g(x,u)  这里，x 和 y 的导数相对于时间 t。  并且没有 du/d(x,y,m) 方程 在这种情况下是否可以使用 PINN 方法，其中损失函数仅使用 dx/dt​ 和 dy/dt 构建？    提交人    /u/its_a_targaryen   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</guid>
      <pubDate>Mon, 22 Jul 2024 11:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] FLUTE - 一种用于量化 LLM 推理的新型 CUDA 内核，与 vLLM 相比，延迟降低了 2.6 倍。它将 QLoRA 扩展为可学习的尺度，每个参数量化为 4 位和 3 位。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</link>
      <description><![CDATA[ 大型语言模型 (LLM) 的部署通常受内存带宽的限制，其中主要瓶颈是将模型参数从 GPU 的全局内存传输到其寄存器的成本。当与融合反量化和矩阵乘法运算的自定义内核结合使用时，仅权重量化可以通过减少内存移动量来实现更快的推理。然而，为权重量化的 LLM 开发高性能内核带来了巨大的挑战，尤其是当权重被压缩为非均匀可分的位宽（例如 3 位）且使用非均匀查找表 (LUT) 量化时。本文介绍了 FLUTE，这是一种用于 LUT 量化 LLM 的灵活查找表引擎，它使用量化权重矩阵的离线重构来最大限度地减少与解包相关的位操作，并使用查找表的矢量化和复制来缓解共享内存带宽限制。当批量大小 &lt; 32 和量化组大小为 128（LLM 推理中的典型值），FLUTE 内核的速度可以比现有的 GEMM 内核快 2-4 倍。作为 FLUTE 的应用，我们探索了基于查找表的 NormalFloat 量化的简单扩展，并将其应用于将 LLaMA3 量化到各种配置，获得了与强基线相比具有竞争力的量化性能，同时获得了 1.5 到 2 倍的端到端吞吐量提升。  Arxiv：https://arxiv.org/abs/2407.10960    提交人    /u/radi-cho   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</guid>
      <pubDate>Mon, 22 Jul 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 神经网络经过训练，能够使用少 50 倍的数据准确预测分子的最佳几何形状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</link>
      <description><![CDATA[计算化学的一个重要任务是找到实现局部能量最小值的分子几何形状，因为这些是分子发生化学反应的最可能配置。尽管最近在分子构象能量预测的神经网络方面取得了进展，但此类模型容易因分布偏移而出错，从而导致能量最小化不准确。通过提供优化轨迹作为额外的训练数据，可以提高神经网络能量最小化的质量。不过，获得完整的优化轨迹需要大量额外的计算。 一个研究小组开发了一个名为“逐步优化学习框架”（GOLF）的新框架，该框架由一个高效的数据收集方案和一个外部优化器组成。作者证明，使用明显更少的额外数据，用 GOLF 训练的神经网络在各种类药物分子的基准测试中的表现与 Oracle 相当。  该~论文~发表于 ICLR 2024 会议论文集    由    /u/AIRI_Institute  提交  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</guid>
      <pubDate>Mon, 22 Jul 2024 08:04:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 聚合标记概率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e96lo5/d_aggregating_token_probabilities/</link>
      <description><![CDATA[我可以使用哪些好的聚合技术来使用标记概率（这可以是 softmax 概率或对数概率）为生成的序列评分？ 例如，在答案中找到关键实体并尝试找出它的标记概率，并查看这些关键实体的中位标记概率是多少。    提交人    /u/archiesteviegordie   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e96lo5/d_aggregating_token_probabilities/</guid>
      <pubDate>Mon, 22 Jul 2024 05:36:55 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 文档图像修复</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</link>
      <description><![CDATA[      这是 DocRes 图像在 chainner 中运行的恢复模型用于改进扫描的文档。原始图像后跟恢复后的图像，然后是 chainner 模型。更进一步，使用 Mindee Doctr 非常准确地获取线段。 我正在处理的下一个任务是识别字体大小，然后识别字体样式，然后使用 Microsoft Phi-3 或具有 OCR 功能的类似模型进行 OCR 并应用样式，然后恢复图像 链接 https://github.com/ZZZHANG-jx/DocRes https://github.com/chaiNNer-org/chaiNNer 原始图像 恢复后的图像 Chainner 架构 已识别线段    提交人    /u/atlury   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</guid>
      <pubDate>Mon, 22 Jul 2024 05:25:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用稀疏数据对自定义下游任务的 OS 模型进行微调的最佳实践</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</link>
      <description><![CDATA[我有一项下游任务，在输入过程中，99% 以上的数据都是上下文，由各种来源生成。实际模型输出只有几个标记，但输入的大小可以从 2k 个标记一直到 10k 个标记不等。因此，考虑到较长的上下文窗口，我尝试针对此任务微调 mistral 7b v0.3。但是尝试使用较低的学习率（如 8e-6）并衰减，我仍然会在每次运行时得到越来越高的训练损失。 训练集由标准 input_ids、attention_mask 和标签组成，但由于训练数据的性质，attention_mask 和标签分别大多为 1 和 -100。由于它们的大小也有很大差异，我将数据打包成 4096 的长度，使其保持不变。我的训练机器是 AWS trn1n.32xlarge 类型。关于我应该在这里做什么，有什么建议吗？对于任何对数据集感兴趣的人，这里是直接标记化版本数据的链接。    提交人    /u/VBQL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</guid>
      <pubDate>Mon, 22 Jul 2024 05:03:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在少样本原型网络中，跨折叠使用不同数量的查询样本可以吗</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8w0fn/d_is_it_okay_to_use_different_numbers_of_query/</link>
      <description><![CDATA[我目前正在使用原型网络进行少样本学习项目，遇到了一种情况，想请教一下。 我有一个预定义的 5 倍交叉验证设置，但这些折叠中的 A 类示例数量不一致。具体来说，一个折叠有 6 个 A 类示例，而其他折叠每个都有 12 个 A 类示例。 在这种情况下，在训练和测试期间使用不同数量的查询样本，同时保持相同的样本数和方法数，是否可以接受？例如，我可以使用可变数量的查询训练模型，并使用各自的查询数评估每个折叠吗？    提交人    /u/The_Aoki_Taki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8w0fn/d_is_it_okay_to_use_different_numbers_of_query/</guid>
      <pubDate>Sun, 21 Jul 2024 20:39:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] ChessGPT 比 GPT-4 小 100,000 倍，下棋等级为 1500 Elo。通过找到技能向量，我们可以在非分布游戏中将其胜率提高 2.6 倍。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</link>
      <description><![CDATA[之前的一个项目训练了 ChessGPT，这是一组 25M 和 50M 参数的 GPT 模型，可以在 1500 Elo 下棋。这些模型比 GPT-4 的 1.8T 参数小约 100,000 倍。 在 Stockfish 0 级，50M 参数模型的胜率为 70%。但是，如果用 20 个随机动作初始化游戏，其胜率会下降到 17%。这是因为它无法泛化分布之外的内容吗？在考虑下一个标记预测任务时，如果游戏以随机动作开始，那么好的下一个标记预测器会预测合法但低技能的动作。 这就是我们在 ChessGPT 中发现的。通过向模型的激活中添加技能向量，我们可以将其胜率提高到 43%，即提高 2.6 倍。我们无法完全弥补性能差距，但这是一个很大的比例。干预非常简单，更复杂的干预可能会进一步提高其胜率。 该模型仅经过训练以预测 PGN 字符串中的下一个字符（1.e4 e5 2.Nf3 ...），并且从未明确给出棋盘状态或国际象棋规则。尽管如此，为了更好地预测下一个角色，它会学习在游戏的任何时候计算棋盘的状态，并学习各种规则，包括将军、将死、王车易位、过路兵、升级、固定棋子等。此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 Elo 评级。 我们还可以使用可解释性方法来干预模型的内部棋盘状态。 这项工作最近被 2024 年语言建模会议 (COLM) 接受，标题为“国际象棋语言模型中的新兴世界模型和潜在变量估计”。 更多信息请参阅此帖子： https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html 代码在这里： https://github.com/adamkarvonen/chess_llm_interpretability    提交人    /u/seraine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</guid>
      <pubDate>Sun, 21 Jul 2024 19:59:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 与主要作者吴正轩讨论 ReFT 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</link>
      <description><![CDATA[大家好， 本周星期五的论文讨论会上，我们非常幸运地邀请到了 ReFT 论文的主要作者，我想分享一下我们的讨论和笔记！ https://www.oxen.ai/blog/arxiv-dives-how-reft-works TLDR ~ ReFT 是一种微调技术，其参数效率比 LoRA 高 15 到 60 倍。训练速度超快。在 A100 上，1k 个示例大约需要 18 分钟。我成功地在不到 1 分钟的时间内，在 Llama 2 7B 上使用大约 100 个示例对 A10 上的 ReFT 进行了微调。 它的工作原理是操作残差流中的表示，而不是 K-V 矩阵。他们向特定的 token 索引和层添加了他们称为“干预”的额外学习参数，从而高效且轻松地控制表示。ReFT 也很不错，因为它们是可组合的。例如，您可以训练一个用于指令跟踪的模型，一个用于德语的模型，然后将它们都应用于德语的获取和指令跟踪模型。 作者给出了他们在实验室中迭代时学到的超级实用的技巧和教训。整个讨论也在 YouTube 上。 希望你喜欢！    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</guid>
      <pubDate>Sun, 21 Jul 2024 16:59:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于神经发育程序。您认为“学习编码”这一想法有多合理？您认为它与自我编程有多大区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8pugs/d_on_the_neural_developmental_program_how_sound/</link>
      <description><![CDATA[我刚刚在 ALIFE2023 上看到了这个演讲，这看起来真的很有趣。总结一下，他们使用了 3 个“策略”网络来发展/生成一个“目标”网络，也就是传统的“策略”网络，它从环境中获取输入并给出动作的输出。 最后，演讲中描述的这个代理仍然是几个传统的神经网络，只是它们的输出是一个模型而不是预测，而来自奖励的训练只是进化算法或 PPO。对于像我这样的自我编程倡导者来说，这种由结构生成看起来很像其自身的结构的想法听起来非常棒，但从外观上看，它是一种类似优先依附的花哨网络生成模型，只是这次在奖励的帮助下它可以解决实际问题。（我仍然认为它很棒，并且是我们现在对预训练微调 RLHF 东西的常规做法的决定性一步） 您对此有何看法？    提交人    /u/HermanHel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8pugs/d_on_the_neural_developmental_program_how_sound/</guid>
      <pubDate>Sun, 21 Jul 2024 16:11:34 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基线薄弱和报告偏差导致机器学习对流体相关偏微分方程过度乐观</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</link>
      <description><![CDATA[  由    /u/nuclear_knucklehead  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</guid>
      <pubDate>Sun, 21 Jul 2024 16:05:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你在公司项目中处理的数据集的平均大小是多少？#D</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8h7ra/d_what_is_the_average_size_of_datasets_you_work/</link>
      <description><![CDATA[想知道您在项目中使用的数据集大小，是 GB 还是 TB？ 以及您如何处理大型数据集以及在处理过程中遵循哪些最佳实践。    提交人    /u/PraveenKumarIndia   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8h7ra/d_what_is_the_average_size_of_datasets_you_work/</guid>
      <pubDate>Sun, 21 Jul 2024 07:55:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你在生产中的 LLM Stack 是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8cxkf/d_what_is_your_llm_stack_in_production/</link>
      <description><![CDATA[好奇人们在生产堆栈中使用什么来开发 LLM 应用程序。这个问题是几个月前提出的，但该领域的事态变化如此之快，我认为值得再开一个帖子。 嵌入模型：目前是 OpenAI Ada，但召回率/准确率不是很好，所以我打算尝试其他模型 矢量数据库：Supabase（推荐） LLM：一直在尝试开源和闭源模型。我的任务需要相当强的推理能力，因此不幸的是本地模型还不够好（例如 Llama 70B）。OpenAI GPT-4o 表现最佳（不足为奇），但对于我的用例来说它确实很昂贵，所以我目前使用的是 Gemini Pro 1.5。 LLM 框架：无。大家的共识是远离 LangChain，所以我只是直接与 LLM 提供商集成。幸运的是，LLM 提供商似乎倾向于 OpenAI API 标准，这使得实验变得容易（除了偶尔定制的 API，如 Gemini） 评估：???。不太确定这个的最新技术是什么。 其他人都在用什么？    提交人    /u/Aggressive_Comb_158   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8cxkf/d_what_is_your_llm_stack_in_production/</guid>
      <pubDate>Sun, 21 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 21 Jul 2024 02:15:09 GMT</pubDate>
    </item>
    </channel>
</rss>