<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 08 Jan 2025 09:17:40 GMT</lastBuildDate>
    <item>
      <title>[R] LongBench v2：实现对现实长上下文多任务的更深入理解和推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/</guid>
      <pubDate>Wed, 08 Jan 2025 09:01:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过 predibase/lorax 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/</link>
      <description><![CDATA[https://github.com/predibase/lorax Predibase/Lorax 确实是一个有趣的 repo。它解决了使用适配器的主要问题，即动态分配适配器。有人试过吗？    提交人    /u/YogurtclosetAway7913   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/</guid>
      <pubDate>Wed, 08 Jan 2025 06:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师，你的工作中最烦人的部分是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/</link>
      <description><![CDATA[我只知道一个博士只是检查数据集，这听起来很可悲    提交人    /u/Classic_Eggplant8827   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/</guid>
      <pubDate>Wed, 08 Jan 2025 04:33:39 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] RLHF PPO 训练密集奖励开源项目及论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwal1h/rp_opensourced_project_and_paper_on_denser_reward/</link>
      <description><![CDATA[本文研究了 RLHF PPO 训练中动作空间的粒度，仅假设二元偏好标签。 在各种主干 LLM 下的 AlpacaEval 2、Arena-Hard 和 MT-Bench 基准测试中，段级 RLHF PPO 及其 Token 级 PPO 变体的表现优于 bandit PPO。  论文：https://arxiv.org/pdf/2501.02790 代码：https://github.com/yinyueqin/DenseRewardRLHF-PPO RLHF 的 token 级奖励模型的先前工作：https://arxiv.org/abs/2306.00398     由   提交  /u/Leading-Contract7979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwal1h/rp_opensourced_project_and_paper_on_denser_reward/</guid>
      <pubDate>Wed, 08 Jan 2025 03:43:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 思维提示程序 (PoT) vs 思维提示链 (CoT)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hw8q66/d_program_of_thought_prompting_pot_vs_chain_of/</link>
      <description><![CDATA[PoT 建立在已建立的 CoT 方法论之上，但通过将推理过程与计算执行分开而脱颖而出。 PoT 的核心是强调将推理与计算分离。在传统的 CoT 提示中，LLM 会在同一框架内生成推理步骤并执行计算，这可能会导致不准确，尤其是在复杂的数学问题中。PoT 通过指示 LLM 生成可在单独的解释器中运行的可执行代码（通常是 Python）来解决这个问题。这样可以清晰地划分逻辑推理过程和实际计算，从而提高清晰度和准确性。 执行过程 PoT 中的执行过程可以概括为几个步骤：  问题呈现：用户以自然语言呈现问题。 代码生成：LLM 生成 Python 代码，概述解决问题所需的步骤。 代码执行：生成的代码在外部环境（例如 Python 解释器）中执行。 结果解释：然后解释代码执行的输出并将其呈现为最终答案。  虽然 PoT 建立在 CoT 提示的基础上，但它引入了一个关键的区别：  CoT：涉及指导 LLM生成自然语言中的一系列推理步骤。但是，这种方法容易出错，尤其是在处理复杂计算时。 PoT：将推理过程转换为 Python 等正式的可执行语言。这消除了自然语言处理中固有的错误可能性，从而带来更可靠、更准确的解决方案。     提交人    /u/GiftProfessional1252   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hw8q66/d_program_of_thought_prompting_pot_vs_chain_of/</guid>
      <pubDate>Wed, 08 Jan 2025 02:07:10 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] DistillKitPlus：面向法学硕士的高性能知识提炼</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/</link>
      <description><![CDATA[具有 LoRA 微调 和 量化支持 的 LLM KLD 开源工具包&gt; 较大的 LLM 泛化更好、更快。您可以利用这一点，然后将 70B 模型的最佳效果转移到 7B 模型，而无需花费太多资金或牺牲性能。 GitHub 链接：https://github.com/agokrani/distillkitplus    提交人    /u/__XploR__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/</guid>
      <pubDate>Tue, 07 Jan 2025 20:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用元启发式算法进行超参数优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hw04bx/d_hyperparameter_optimization_with_metaheuristic/</link>
      <description><![CDATA[我目前正在撰写关于这个主题的论文，我按照教授的建议，从使用 CNN 进行图像分类开始。但是显然我无法运行超过 25-30 次迭代，因为它对内存要求很高。关于这个领域的论文也不多。我发现有更快的算法，如贝叶斯优化，它们产生类似的结果。 这是一个研究的死区吗？我可以从哪里开始？    提交人    /u/PhosphorusPlatypus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hw04bx/d_hyperparameter_optimization_with_metaheuristic/</guid>
      <pubDate>Tue, 07 Jan 2025 19:47:42 GMT</pubDate>
    </item>
    <item>
      <title>[研发] 白盒变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/</link>
      <description><![CDATA[在此研究线上开启一个主题：https://ma-lab-berkeley.github.io/CRATE/ 据我了解，作者基本上将学习有效数据表示的过程定义为寻找多元高斯字典的问题，该字典以简约方式覆盖数据分布。特别是，在特征/高斯方面使用稀疏编码。 构建一个架构，该架构采用多个交替步骤“聚类”相似向量并分别正交化来自不同聚类的向量，最终得到类似于 Vision Transformer 的结构。类似 MultiHead Attention 的模块将向量聚类，使它们更接近局部主方向或流形，类似 MLP 的模块将这些向量沿着相互更正交的轴移动。从数学上讲，它们近似于明确定义的稀疏编码率，因此是白盒算法，但是我不能说数学比 Transformers 更直观。 事实上，最后一层的 CLS 注意力头在图像分类监督训练下具有可解释的偏好，如在 DINO（自我监督）或 SimPool 中。这与过程的解释直接相关，并为 DINO 的可解释性和动态性提供了解释。它也被称为 George Hinton 的视觉智能架构蓝图，即 GLOM Transformer。 我认为注意力的聚类效应在文献中在某种程度上没有得到充分重视，就像 Transformers 中 FFN 的作用没有得到充分研究一样。我想知道是否有第三种方法在数学上像 MLP 一样简单，像高斯特征词典一样直观。    提交人    /u/Sad-Razzmatazz-5188   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/</guid>
      <pubDate>Tue, 07 Jan 2025 18:23:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入空间中的位置嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/</link>
      <description><![CDATA[原始位置编码在特征空间中是如何分布的？RPE 是如何分布的？这些嵌入和 LayerNorm（删除与均匀向量平行的分量，即 1 的向量）之间的相互作用是什么？    提交人    /u/Sad-Razzmatazz-5188   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/</guid>
      <pubDate>Tue, 07 Jan 2025 18:07:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师，你的工作中最有价值的事情是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/</link>
      <description><![CDATA[有些人告诉我这是薪水问题，但我认为这取决于你的经验水平以及你为谁工作？这份工作还有更多内容吗？    提交人    /u/RespectPrivacyPlz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/</guid>
      <pubDate>Tue, 07 Jan 2025 17:42:33 GMT</pubDate>
    </item>
    <item>
      <title>[N] ESwML 2025 征文通知 [2025 年 3 月 31 日]（与 ASPLOS-25/EuroSys-25 联合举办）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvwnde/n_eswml_2025_call_for_papers_march_31_2025_in/</link>
      <description><![CDATA[这是针对以下内容的征文： ESwML 2025 第二届通过机器学习推动软件开发国际研讨会 https://eswml.github.io/2025/2025.html 重要截止日期： 提交截止日期：2025 年 2 月 7 日 (AoE) 作者通知：2025 年 2 月 21 日 研讨会预定日期：2025 年 3 月 31 日 征文 未来的软件将严重依赖于机器学习模型的使用。  这将涉及多个方面，包括在软件开发期间使用机器学习 (ML) 模型来提高开发人员的工作效率、设计 ML  启发式方法来提高应用程序的执行效率，以及在应用程序中采用替代神经 网络 (NN) 模型来取代昂贵的计算 并加速其性能。然而，一些挑战限制了 ML 在当今软件中的广泛应用。通过机器学习赋能软件开发 (ESwML) 半天研讨会的目标是建立 一个平台，让研究人员、科学家、应用程序开发人员、计算中心工作人员和行业专业人士聚集在一起交流想法，探索人工智能如何帮助有效和高效地使用未来系统。 本次研讨会将积极推动讨论，旨在回答以下问题： 本次研讨会将积极推动讨论，旨在回答以下问题： * 我们如何利用机器学习的进步来简化软件开发过程？ * 在应用程序开发过程中，缺少哪些工具来弥合与 ML 模型的交互？ * 我们能否通过向 ML 模型展示现有的分析工具来提高其准确性和效率？例如，使大型语言模型能够与内存清理器等进行交互。 * 我们如何将 ML 模型无缝集成到应用程序中以提高其性能，同时确保生成的输出的正确性？ 论文和摘要提交 我们寻求描述与 ESwML 研讨会中的研究主题相关的近期或正在进行的研究的摘要。欢迎所有研究人员和从业者提交他们的工作以供在本次研讨会上展示。这是一个面对面的研讨会，只有幻灯片可以选择发布在研讨会网站上。 短文必须以 PDF 文件的形式电子提交。格式为 1-4 双栏 页（不包括参考文献）。提交的内容应可打印在美国信纸或 A4 纸上。 请通过 hotcrp 提交您的手稿。 https://eswml25.hotcrp.com/ 注意：演示文稿和短文只有在作者明确同意的情况下才会在线提供。我们鼓励希望分享其演示文稿的作者通知研讨会组织者。 研讨会联合主席 * Florina Ciorba（瑞士巴塞尔大学），florina.ciorba，网址为unibas.ch * Harshitha Menon（美国劳伦斯利弗莫尔国家实验室），harshitha，网址为llnl.gov * Konstantinos Parasyris（美国劳伦斯利弗莫尔国家实验室），parasyris1，网址为llnl.gov    提交人    /u/Successful_Tackle270   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvwnde/n_eswml_2025_call_for_papers_march_31_2025_in/</guid>
      <pubDate>Tue, 07 Jan 2025 17:25:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对你来说，机器学习最吸引人的方面是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/</link>
      <description><![CDATA[标题。您可以根据自己的意愿主观地解释这个问题。    提交人    /u/AromaticEssay2676   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/</guid>
      <pubDate>Tue, 07 Jan 2025 12:30:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] NLP/LLM 中的优化技术是否也适用于基于 Transformer 的序列建模？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvj7fx/d_optimization_techniques_in_nlpllm_that_also/</link>
      <description><![CDATA[标题。 尝试集思广益，看看是否有适用于 NLP 用例的技术可以应用于序列建模。 具体来说，我正在尝试优化推荐系统（用户表示建模）中使用的变压器。 到目前为止，我能想到的基本知识是：闪光注意、高效/线性变压器、融合核嵌入、用于训练/服务的混合精度/量化。 还有其他什么或其他论文浮现在脑海中吗？ 我认为主要问题有时是用户序列表示或 rec sys 之类的标记概念与 LLM 中的标记概念截然不同。我们还处理更稀疏的嵌入…… 提前致谢！    提交人    /u/Tough_Palpitation331   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvj7fx/d_optimization_techniques_in_nlpllm_that_also/</guid>
      <pubDate>Tue, 07 Jan 2025 04:19:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>