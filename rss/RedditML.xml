<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 06 Jan 2024 00:59:27 GMT</lastBuildDate>
    <item>
      <title>[D] BioAI 在巴黎的研究角色？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zkudh/d_bioai_research_roles_in_paris/</link>
      <description><![CDATA[嗨， 我今年将捍卫我的计算基因组学/机器学习博士学位。我将在就业市场上寻找博士后或行业职位，我正在努力寻找合适的人选。我现在想留在巴黎地区。 我对蛋白质、分子动力学和组学数据特别感兴趣。 关于行业，我确定了以下两个拥有高质量研究成果并在机器学习会议上发表过文章的公司。您有在那里工作或申请的经验吗？您还知道其他类似的机会吗？  目前的 A 计划是 InstaDeep，最近被 BioNTech 收购。巴黎办事处似乎进行了认真的研究，生物学方面必将得到更大的发展。我特别喜欢人们对从头蛋白质设计的兴趣，到目前为止我在其他地方还没有发现这种兴趣。 还有 Owkin。他们似乎主要致力于组学或联邦学习，所以我会错过蛋白质设计/折叠/对接方面的内容。 当然 DeepMind 会很棒，但我的印象是他们没有直接从博士生中招募。  感谢您的关注并想听听您的想法！   由   提交 /u/ZestycloseBus4359   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zkudh/d_bioai_research_roles_in_paris/</guid>
      <pubDate>Fri, 05 Jan 2024 23:22:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] 一个用于部署本地模型的开源项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zkm5m/p_an_opensource_project_for_deploying_local_models/</link>
      <description><![CDATA[     &lt; td&gt; 引入一个新的LLM WebUI项目，支持各种本地模型加载，并为尖端在线多模态模型GPT-4-Vision和提供流式输出双子座-Pro-Vision。它完全免费和开源，是探索不同模型的宝贵研究工具。该项目正在积极开发并不断更新： https://github.com/smalltong02/keras-llm-机器人 ​ WebUI ​ 配置 ​ 工具与代理   由   提交 /u/Entire-Fly-6957   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zkm5m/p_an_opensource_project_for_deploying_local_models/</guid>
      <pubDate>Fri, 05 Jan 2024 23:12:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 时间序列数据表示学习的最新技术是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zjkyn/d_what_is_state_of_art_for_representation/</link>
      <description><![CDATA[有一堆未标记的一维原始时间序列数据。标记数据量有限。 我正在寻找最好的无监督/自监督编码技术，以学习有用的潜在特征表示（例如，在下游监督预测任务中有用）。  无论是使用 Transformer 还是 CNN (ConvNext V2) 架构，屏蔽自动编码器领域似乎都有很多工作要做。  这些技术是目前最好的技术，还是我还缺少其他在各种数据集上表现出强大性能的技术？ ​ 谢谢！   由   提交/u/ZeApelido  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zjkyn/d_what_is_state_of_art_for_representation/</guid>
      <pubDate>Fri, 05 Jan 2024 22:29:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] Hieros：结构化状态空间序列世界模型的层次想象</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zizet/r_hieros_hierarchical_imagination_on_structured/</link>
      <description><![CDATA[OpenReview: https: //openreview.net/forum?id=5j6wtOO6Fk arXiv：https ://arxiv.org/abs/2310.05167 代码：https： //github.com/Snagnar/Hieros 摘要：  现代深度强化学习面临的最大挑战之一（ DRL）算法是样本效率。许多方法学习世界模型，以便完全在想象中训练智能体，从而消除训练期间直接环境交互的需要。然而，这些方法常常缺乏想象准确性、探索能力或运行效率。我们提出了Hieros，这是一种分层策略，可以学习时间抽象的世界表示并想象潜在空间中多个时间尺度的轨迹。 Hieros 使用基于 S5 层的世界模型，该模型在训练期间并行预测下一个世界状态，并在环境交互期间迭代预测。由于 S5 层的特殊属性，我们的方法可以并行训练并在想象过程中迭代预测下一个世界状态。这使得训练比基于 RNN 的世界模型更有效，并且比基于 Transformer 的世界模型更有效的想象力。 我们表明，我们的方法在 Atari 上的平均和中值归一化人类分数方面优于现有技术。 100k 基准，并且我们提出的世界模型能够非常准确地预测复杂的动态。我们还表明，与现有方法相比，Hieros 显示出卓越的探索能力。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zizet/r_hieros_hierarchical_imagination_on_structured/</guid>
      <pubDate>Fri, 05 Jan 2024 22:04:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 法学硕士增强法学硕士：通过组合扩展能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zii8u/r_llm_augmented_llms_expanding_capabilities/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2401.02412 OpenReview：https:// /openreview.net/forum?id=jjA4O1vJRz 摘要：  经过训练的数十亿参数的基础模型对大型数据集的研究已经在各个领域展示了不平凡的技能。然而，由于它们的整体结构，增强它们或传授新技能具有挑战性且成本高昂。另一方面，由于它们的适应能力，这些模型的几个新实例正在针对新领域和任务进行训练。在这项工作中，我们研究了现有基础模型与更具体模型的高效实用组合问题，以实现更新的功能。为此，我们提出了CALM——组合增强语言模型——它引入了模型之间的交叉注意力来组合它们的表示并启用新的功能。 CALM 的显着特征是：（i）通过“重用”现有的 LLM 以及一些额外的参数和数据，在新任务上扩展 LLM，（ii）现有模型权重保持完整，从而保留现有功能，以及（ iii) 适用于不同的领域和环境。我们证明，使用在低资源语言上训练的较小模型来增强 PaLM2-S 可以在翻译成英语和低资源语言的算术推理等任务上绝对提高高达 13%。同样，当 PaLM2-S 通过特定于代码的模型进行增强时，我们发现代码生成和解释任务比基本模型相对提高了 40%，与完全微调的对应模型相当。 &lt; /blockquote&gt;   由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zii8u/r_llm_augmented_llms_expanding_capabilities/</guid>
      <pubDate>Fri, 05 Jan 2024 21:44:21 GMT</pubDate>
    </item>
    <item>
      <title>基于变压器的法学硕士不是一般学习者：通用电路视角 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</link>
      <description><![CDATA[https://openreview.net/forum?id =tGM7rOmJzV  （法学硕士）的巨大成功引发了人工智能界研究重点的显着转变。这些令人印象深刻的实证成就激发了人们对法学硕士是“通用人工智能（AGI）的火花”的期望。然而，一些评估结果也呈现了法学硕士失败的令人困惑的例子，包括一些看似微不足道的任务。例如，GPT-4 能够解决一些 IMO 中对研究生来说可能具有挑战性的数学问题，而在某些情况下它可能会在小学水平的算术问题上出错。 ...  我们的理论结果表明 T-LLM 无法成为通用学习者。然而，T-LLM 在各种任务中取得了巨大的经验成功。我们对这种不一致现象提供了一个可能的解释：虽然 T-LLM 不是一般学习者，但他们可以通过记忆大量实例来部分解决复杂的任务，从而导致人们产生一种错觉，认为 T-LLM 具有真正解决这些任务问题的能力。     由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/</guid>
      <pubDate>Fri, 05 Jan 2024 21:39:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] GPT-4V(ision) 是一款多面手 Web 代理，如果接地 - 俄亥俄州立大学 2024 年 - 可以成功完成实时网站上 50% 的任务！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zgfmx/r_gpt4vision_is_a_generalist_web_agent_if/</link>
      <description><![CDATA[&lt;表&gt;   论文：https://arxiv.org/abs/2401.01614  博客：https ://osu-nlp-group.github.io/SeeAct/  代码： https://github.com/OSU-NLP-Group/SeeAct  摘要：  大型多模态模型（LMM）的最新发展），特别是 GPT-4V(ision) 和 Gemini，一直在快速扩展多模态模型的能力边界，超越图像字幕和视觉问答等传统任务。在这项工作中，我们探索了像 GPT-4V 这样的 LMM 作为通用网络代理的潜力，它可以遵循自然语言指令来完成任何给定网站上的任务。我们提出了 SEEACT，这是一种通用网络代理，它利用 LMM 的力量来实现集成的视觉理解和在网络上的操作。我们对最近的 MIND2WEB 基准进行评估。除了对缓存网站进行标准离线评估之外，我们还通过开发允许在实时网站上运行 Web 代理的工具来启用新的在线评估设置。 我们表明，GPT-4V 为网络代理提供了巨大的潜力 - 如果我们手动将其文本计划转化为网站上的操作，它可以成功完成实时网站上 50% 的任务。这大大优于文本-仅限专门针对网络代理进行微调的 LLM，例如 GPT-4 或更小的模型（FLAN-T5 和 BLIP-2）。然而，接地仍然是一个重大挑战。现有的 LMM 基础策略（例如标记集提示）对于网络代理来说并不有效，而我们在本文中开发的最佳基础策略同时利用了 HTML 文本和视觉效果。然而，仍然存在与预言机基础存在巨大差距，留有足够的进一步改进的空间。   https://preview.redd.it/1w22ga2ejoac1.jpg?width=706&amp;format=pjpg&amp;auto=webp&amp;s=204d4852c614efaf8c 39c990d25a7acae805290e  https://preview.redd .it/vaabea2ejoac1.jpg?width=1344&amp;format=pjpg&amp;auto=webp&amp;s=17f5a5ca7e1add213ca4d75ed53a74e230369655 https://preview.redd.it/2720ob2ejoac1.jpg?width=1340&amp;format=pjpg&amp;auto=webp&amp;s=4cec63cdd3e14 48e03f82309ac219684c62b8ffb https://preview .redd.it/9wn5sa2ejoac1.jpg?width=1242&amp;format=pjpg&amp;auto=webp&amp;s=dcc8919105686007d670f9b140aaeb3e4683d56e https://preview.redd.it/ttgaad2ejoac1.jpg?width=801&amp;format=pjpg&amp;auto=webp&amp;s=568 4aa7969a6564eab8cb4a5ea36fa21f4c63e9e    由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zgfmx/r_gpt4vision_is_a_generalist_web_agent_if/</guid>
      <pubDate>Fri, 05 Jan 2024 20:18:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 ML/DL 的翻译器。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zeacs/p_translator_using_mldl/</link>
      <description><![CDATA[我目前正在完成我的期末项目（电气工程师学士学位），并且我计划使用 ML/DL 制作一个翻译器。我对这个主题进行了基本的诅咒，这样我就可以理解如何做到这一点......原则上。然而，我并没有从事这么大的事情的认真经验。所以我的主要问题是，这个项目对于以前经验很少的人来说是否可行？这样做时要考虑哪些因素？这个主题是否有足够的文档供我自己完成？我需要多长时间才能做到这一点？ 另一个可能有用的信息，我想将玛雅语言（最流行的）翻译成西班牙语。  [英语不是我的母语，抱歉犯了错误]   由   提交/u/fmoralesh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zeacs/p_translator_using_mldl/</guid>
      <pubDate>Fri, 05 Jan 2024 18:49:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何及时了解 ML 领域的最新论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ze0cx/d_how_to_stay_updated_with_latest_paper_in_ml/</link>
      <description><![CDATA[深度学习论文如此之多，很难从噪音中筛选出优秀的论文以保持领先地位。 有什么建议吗？也许有人有要关注的 Twitter 帐户列表？   由   提交 /u/Remet0n   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ze0cx/d_how_to_stay_updated_with_latest_paper_in_ml/</guid>
      <pubDate>Fri, 05 Jan 2024 18:37:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] MC-JEPA 神经模型：释放运动识别和生成式人工智能在视频和图像上的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18zbxt7/d_mcjepa_neural_model_unlock_the_power_of_motion/</link>
      <description><![CDATA[我们进行了讨论论文“MC-JEPA：用于运动和内容特征自监督学习的联合嵌入预测架构” https://arxiv.org/pdf/2307.12698.pdf   由   提交 /u/sasaram   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18zbxt7/d_mcjepa_neural_model_unlock_the_power_of_motion/</guid>
      <pubDate>Fri, 05 Jan 2024 17:11:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 深度强化学习泛化分析调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z90nq/r_a_survey_analyzing_generalization_in_deep/</link>
      <description><![CDATA[https://arxiv.org/pdf/2401.02349 .pdf   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z90nq/r_a_survey_analyzing_generalization_in_deep/</guid>
      <pubDate>Fri, 05 Jan 2024 15:07:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] ArXiv 替代方案（或者是否有可能实现更多“暂停”透明度）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z3jdr/d_arxiv_alternatives_or_is_there_possible_for/</link>
      <description><![CDATA[我当前的文章已“暂停”差不多一周了（尝试联系模组，得到了一般性的答复）。我在 arXiv 上发表了 5 篇文章，没有任何问题（同一类别中 3 篇）。 还有关于文章被搁置一个月以上的可怕故事 (https://academia.stackexchange.com/questions/189542/arxiv-preprint-on-hold，https://twitter.com/YuanqiD/status/1678949802367676417，https:// twitter.com/moyix/status/1604218507708846082，https://twitter.com/PierLucaLanzi/status/1629569377690439680，https://twitter.com/GriffinAdams92/status/1605310825958637568）。  我知道模组是免费做他们的工作的，如果这个过程在某种程度上是透明的，我可以等待合理的时间。但现在，有些文章一天之内就被接受，有些则需要等待数周/数月。是否有可能让 arXiv“暂停”？状态更透明？例如。通过显示当前队列大小或“保留”的某种原因（错误的类别，像 Covid 这样的敏感话题，...）？  此外，对于 ML 工作，是否有一些 arXiv 的不错替代品？那些具有良好声誉（没有 vixra）、可预测的等待时间并且至少还被 Google Scholar 索引的？   由   提交 /u/osamc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z3jdr/d_arxiv_alternatives_or_is_there_possible_for/</guid>
      <pubDate>Fri, 05 Jan 2024 10:09:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 A100 与 4x4090 训练 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18z0jja/d_training_llm_with_a100_vs_4x4090/</link>
      <description><![CDATA[我必须在 A100 (80Gb) 和 4x4096 (92GB) 之间做出选择。我正在寻找训练 7B 模型。看起来 7B 模型将需要 55 GB（使用 Adam 作为优化器）。那么，如果我有 4x4096 GPU，是否足够了？如果我使用 DPO 或 rhf 进行训练，这将有两个模型，这会使 GPU 变为 3 倍吗？ 我应该使用哪一个，A100 还是 4x4096？ ~   由   提交 /u/Electronic_Hawk524   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18z0jja/d_training_llm_with_a100_vs_4x4090/</guid>
      <pubDate>Fri, 05 Jan 2024 06:51:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 放弃 ML 博士 - 建议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18yh4ph/d_dropping_out_ml_phd_advice/</link>
      <description><![CDATA[我即将开始博士学位的第三年。我有 3 篇第一作者论文，还有 2 篇正在审稿中，今年夏天我已经准备好进行扎实的研究实习。但是……老实说，我根本不喜欢研究，从来没有，也不太关心。在过去的三年里，我勉强做到了这一点，老实说，我非常非常幸运。我绝不是一个研究天才，甚至不喜欢研究。我只是在乘风破浪，打发时间。但这种完全无意义和绝望的感觉，我无法克服。我只是感觉不适合作为一名研究员。这不是冒名顶替综合症。研究不是我的事。  老实说，我读博士课程只是为了满足我的家人。来自一个拥有研究生学位的亚洲家庭，这是一种期望。  20年前的博士学位看起来很有趣。我想象博士课程就是我和同事一起在白板上讨论，提出想法并尝试疯狂的事情，总是参加研讨会和课程。相反，我看到的是士气低落、过度劳累的学生、空荡荡的教室和研讨会（!!!），以及普遍的绝望感和不想去那里的感觉。这对我来说太震惊了。 现在退学是不是很愚蠢？我觉得我的20多岁已经在无聊、完全没有动力和沮丧中消逝了。我的导师是一个很棒的人，但几乎没有时间见面。我只是不知道我是否还能忍受这个。我想尝试一些疯狂的事情：去一家初创公司并取得成功或为此而奋斗，获得 MBA 或统计学硕士学位，搬到一个新城市，成为一名人工智能政策分析师。感觉有很多路我更适合。  编辑：哇。感谢大家的回复和源源不断的动力。老实说，我没想到会收到这么多评论。我很快就会和我的导师交谈，并安排一次长时间的一对一会议，看看我们能做些什么让我带着博士学位离开这里:)  &amp; #32；由   提交 /u/TheMysticalJam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18yh4ph/d_dropping_out_ml_phd_advice/</guid>
      <pubDate>Thu, 04 Jan 2024 16:27:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>