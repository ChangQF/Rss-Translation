<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Thu, 20 Feb 2025 21:14:20 GMT</lastBuildDate>
    <item>
      <title>[r]为什么对火车/测试/瓦尔分裂的预处理有不同的看法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iu5cgg/r_why_is_there_mixed_views_on_how_traintestval/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  为什么对火车/测试/val设置进行了哪些预处理 快速问题，使用火车/由于某种原因，我看到测试/val分裂，我看到应该以与火车套装相同的方式进行测试和VAL的​​意见。这并不是要使模型具有疯狂的高性能，因为测试数据意味着它与培训数据几乎相同。  我看到一些论坛说不要对您的测试和Val集进行任何预处理基本的预处理测试和瓦尔（例如裁剪，调整大小和归一化）？i如果我要通过将增强措施应用于图像（例如镜像，旋转等）来对数据集进行过采样，我只能在火车集？ 对于上下文，我使用深CNN模型  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/amulli21     [links]       [注释]     ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iu5cgg/r_why_is_there_mixed_views_on_how_traintestval/</guid>
      <pubDate>Thu, 20 Feb 2025 18:21:50 GMT</pubDate>
    </item>
    <item>
      <title>[d]用最后一个隐藏状态丰富令牌嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 从信息理论的角度来看，查看解码器变压器的工作过程，我们可以看到在最后一个隐藏状态在世代相传中崩溃为一个令牌。这意味着您崩溃了一个隐藏状态，从理论上讲，该状态具有：   hidden_​​dim * 32 （或任何量化的任何量子）信息，以下是：  &lt; p&gt; log₂（dict_size）  我想知道这是否是一件好事（对不起，对天真的措辞感到抱歉）。变压器用于预测接下来令牌的信息完全存储在其上下文窗口中，不涉及任何经常性状态。因此，预测序列的接下来令牌即将馈送的变压器将产生与相同序列完全相同的结果，如果它完全由变压器本身生成。 公平， 公平，从某种意义上说：是生成序列还是只是读取的任何内容都不会改变下一代币应该是什么。 ，但另一方面，这种方法意味着 all 令牌之间的信息流必须通过注意机制发生。变压器无法将一些细微差别或风味嵌入预测的令牌嵌入中。喜欢：  “好吧，我预测了令牌&#39;  肯定   &#39;，但我宁愿意味着&#39;    90％确定  &#39;。   当预测下一个标志时可能存在于最后一个隐藏状态（甚至在软拿的输出概率分布中）完全丢失了。 因此，当我昨天散步时，我认为在使用令牌嵌入中添加一些信息可能是一个好主意类似：   augmented_embedding = embedding（token） + f（last_hidden_​​state）  （确保确保很重要的是那是：  &#39;f（last_hidden_​​state）&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;试图找到有关此主题的论文，并要求从Claude，Chatgpt和困惑中提供反馈。     claude 告诉我这是“一个令人难以置信的有见地的想法。” /strong&gt;给了我一长串完全无关的来源。  ，所以我转向你们。如果有一个大脑子的家伙告诉我为什么其他大脑子的家伙决定不遵循这个想法，或者为什么它不起作用，我会很喜欢。 这是我发现的一些可能有问题的事情：   1。训练复杂性 变压器很适合与重型并行化训练，这是因为它们不是递归的。每个大小 n 的序列都可以给出 n-1 独立的训练示例。在令牌嵌入中注入最后一个隐藏状态的信息将打破一些并行化。 ，我猜仍然可以有效地训练它。  首先，请（ n-1 ）香草序列并获取预测。 ，对于每个预测 现在，您有一组新的训练序列，其中所有（但第一个）令牌嵌入式已更新。 您可以无限期地重复此过程。我希望它收敛^^   顺便说一句，这看起来确实像是一个扩散过程。这将我带到了下一个点：  2。稳定性（尽管这种令牌嵌入的增强作用显而易见，试图防止模型的输出无敏化，但在这里，我不是很有能力。定义这种过程稳定性的条件是什么？我没有受过教育的猜测是，如果您保留： ” last_hidden_​​state_contribution”≪” agemented_token_embedding” &lt; /strong&gt; 您不应该遇到很多问题。但这也将限制信息流。我想这是一个权衡的，如果不够好，我不会感到惊讶。 你们怎么看？这已经在某个地方尝试过吗？是否有基本原因这是行不通的？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/Academic_sleep1118     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</guid>
      <pubDate>Thu, 20 Feb 2025 18:06:18 GMT</pubDate>
    </item>
    <item>
      <title>[d]预测分布与困惑（困惑问题）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iu2o2e/d_predictive_distribution_vs_perplexity_issues/</link>
      <description><![CDATA[在/a&gt;（Hoffman，2013年）。在其结果部分中，他们使用预测分布作为度量，而不是 Perplexity 。具体而言，这是：  评估预测分布的避免比较评估度量的界限或形成近似值。它奖励了一个良好的预测分布，但是它是计算的。  及以后在脚注中：  我们认为，预测性分布是更好的指标。模型健身[而不是困惑]   我不确定我知道为什么是这种情况，还是差异到底是什么？在这两种情况下，您都依靠各种近似来计算p（w_new | w_obs，triending_data），那么为什么要避免比较评估度量的界限或形成近似值。困惑不是最终对您的预测分布的度量？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iu2o2e/d_predictictive_distribution_vs_perplexity_issues/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iu2o2e/d_predictive_distribution_vs_perplexity_issues/</guid>
      <pubDate>Thu, 20 Feb 2025 16:34:23 GMT</pubDate>
    </item>
    <item>
      <title>[d] DeepSeek 681亿美元的推理成本与超尺度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我已经估计了deepseek的成本/性能681亿，这样：  huggingface Open DeepSeek博客报告了Config＆amp;性能= 32 H100的800TPS   100万代币= 1250S = 21（ish），分钟。 69.12万代币每天 租金32 H100的费用〜$ 80000 &lt;$ 80000 &lt;&lt; /p&gt; 每百万个代币= $ 37.33（80000/31天/69.12） 我知道这是非常乐观的（100％利用，没有支持等），但是算术是否有意义，并且通过您认为是否通过嗅探测试？还是我有明显的错误？  我猜这比诸如双子座的API型号高1000倍，并且这个差距使我想知道我是否很愚蠢  &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/sgt102     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</guid>
      <pubDate>Thu, 20 Feb 2025 13:44:05 GMT</pubDate>
    </item>
    <item>
      <title>[r] LLM跨语言幻觉是多少？关于野外LLM幻觉的多语言估计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itwsdl/r_how_much_do_llms_hallucinate_across_languages/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  跨30种语言估算幻觉的新工作。该论文带有跨度级幻觉检测测试数据集和（提示，参考）数据集，以评估各种主题的LLM幻觉。  纸： https://arxiv.org/abs/2502.12769  ：数据集可以通过拥抱面纸页面找到： https://huggingface.co/papers/2502.12769  ;提交由＆＃32; /u/u/qadrishyaari     [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itwsdl/r_how_much_do_llms_hallucinate_across_languages/</guid>
      <pubDate>Thu, 20 Feb 2025 11:55:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] SWE-LAN​​CER：Frontier LLM可以从现实世界中的自由软件工程中赚取100万美元吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itv4z7/r_swelancer_can_frontier_llms_earn_1_million_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   我们介绍了Swe-Lancer，这是超过1,400个自由职业软件工程任务的基准，从UPWORK中，价值为100万美元，总计100万美元。支出。 SWE-Lancer涵盖了这两个独立的工程任务 - 从50美元的错误修复到$ 32,000的功能实现以及管理任务，其中模型在技术实施建议之间进行选择。独立任务通过经验丰富的软件工程师对端到端测试进行分级，而管理决策则根据原始雇用工程经理的选择进行评估。我们评估模型性能，发现边境模型仍无法解决大多数任务。为了促进未来的研究，我们开源统一的Docker图像和公众评估拆分，Swe-Lancer Diamond（此https url ）。通过将模型绩效映射到货币价值，我们希望SWE-Lancer能够对AI模型开发的经济影响进行更多的研究。  他们还在Github上发布了代码和数据集。   arxiv链接： [2502.12115] SWE-LAN​​CER：Frontier LLMS可以从现实世界中的自由式软件工程中赚取100万美元？ /a&gt;   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/hiskuu     [links]   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1itv4z7/r_swelancer_can_frontier_frontier_frontier_llms_earn_1_million_million_from//]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itv4z7/r_swelancer_can_frontier_llms_earn_1_million_from/</guid>
      <pubDate>Thu, 20 Feb 2025 10:03:55 GMT</pubDate>
    </item>
    <item>
      <title>[R]本地稀疏注意：硬件一致且本地可训练的稀疏注意力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itutpg/r_native_sparse_attention_hardwarealigned_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    长篇小说建模对于下一代语言模型至关重要，但是标准注意机制的高计算成本却带来了重大的计算挑战。稀疏的注意力为提高效率的方向提供了有希望的方向，同时保持模型功能。我们提出了NSA，这是一种本地可训练的稀疏注意机制，将算法创新与硬件一致的优化相结合，以实现有效的长篇文化建模。 NSA采用了动态的分层稀疏策略，将粗粒的令牌压缩与精细的令牌选择相结合，以保持全球环境意识和局部精度。我们的方法通过两个关键创新进行了稀疏注意设计：（1）我们通过算术强度平衡算法设计实现了实质性的加速，并对现代硬件进行了优化。 （2）我们启用端到端培训，在不牺牲模型性能的情况下减少预处理的计算。如图1所示，实验表明，使用NSA预测的模型维持或超过了一般基准，长篇下说任务和基于指导的推理的全部注意力模型。同时，NSA在解码，向前传播和向后传播的64k长度序列上充分关注实现了实质性的加速，从而在整个模型生命周期中验证了其效率。   有趣的论文在训练过程中提高了有趣的论文。以及DeepSeek的LLMS的推论。   arxiv链接： [2502.11089]本地稀疏注意：硬件 - 与硬件相协调且本质上可训练的稀疏注意力     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/hiskuu     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itutpg/r_native_sparse_attention_hardwarealigned_and/</guid>
      <pubDate>Thu, 20 Feb 2025 09:41:38 GMT</pubDate>
    </item>
    <item>
      <title>[R]通过统计流动流进行语言建模的几何连续扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itsx7f/r_geometric_continuous_diffusion_for_language/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键贡献是将语言生成建模为统计歧管上的连续扩散过程，而不是使用基于离散令牌的扩散。这允许语言状态与更有效的生成之间的更平滑的过渡。 主要技术要点： - 使用Riemannian几何形状在代币上创建概率分布的连续流形 - 实现专业的神经体系结构，学会学会导航此歧视空间。 - 采用受控的扩散路径以进行更精确的生成 - 在抽样中实现显着加速（比离散基线快2-3倍） - 报告改进了多种语言基准的困惑得分 标准基准的结果：-Wikitext -103：16.8困惑（vs 18.2基线） -  C4：14.9 Perplexity（vs 15.8基线）离散模型 - 记忆使用量减少了约30％ 我认为这种方法可能会有意义地影响语言通过提供更优雅的处理文本生成方式来建模开发。连续的性质可以更好地匹配语言含义的实际流动方式，并有可能导致更多的自然产出。  我认为未来的主要挑战是： - 在保持较大模型的同时保持多种模型 - 有效地处理非常长的序列 - 桥接理论和实施生产系统&lt; /p&gt;  TLDR：使用统计流形的语言建模的新型连续扩散方法。显示出改善的困惑和生成速度与离散模型。有希望的方向更有效，更自然的语言生成。  完整的摘要在这里&lt; /a&gt;。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itsx7f/r_geometric_continuous_diffusion_for_language/</guid>
      <pubDate>Thu, 20 Feb 2025 07:24:47 GMT</pubDate>
    </item>
    <item>
      <title>[D]与XGBoost更好地分布在GBM和HistGBM中的Shap贡献更好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1its6tv/d_shap_contribution_better_distributed_in_gbm_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  因此，我正在建立一个信用风险模型，我们正在培训XGBoost，GBM和HistGBM上的数据。我们的发现之一是，Xgboost中变量的外形贡献非常倾斜，其中第一个变量具有31％的幅度重要性，而在其他两种算法中，前几个变量的变量显着较小，分布式变形的幅度明显较小，更好的分布形状重要性，例如11％，10.5％，10％，9％等。 ，不仅如此，即使模型性能在GBM中也比XGBoost更好。  我找不到可能发生这种情况的实质原因。如果有人有解释的话，很想听听您的想法。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/lietechnical1662     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1its6tv/d_shap_contribution_better_distributed_in_gbm_and/</guid>
      <pubDate>Thu, 20 Feb 2025 06:36:17 GMT</pubDate>
    </item>
    <item>
      <title>[p]萨卡（Saka）释放了库德纳（Cudiner）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itqrgl/p_sakana_ai_released_cuda_ai_engineer/</link>
      <description><![CDATA[在CUDA-GEANDERER/  它将火炬转换为CUDA内核。  这是步骤： 阶段1和2（转换和翻译）： AI CUDA工程师首先将Pytorch代码转换为功能cuda内核。我们已经观察到初始的运行时改进而没有明确定位这些。  阶段3（进化优化）：受生物进化的启发，我们的框架利用了进化优化（&#39;“&gt;生存”优点’），以确保只生产最好的CUDA内核。此外，我们介绍了一种新颖的内核交叉促进策略，以互补的方式结合多个优化内核。  阶段4（创新档案）：，文化进化如何影响我们的人类智能借助我们祖先到几千年的文明，AI CUDA工程师还利用了从过去的创新和发现中学到的知识（第4阶段），第4阶段），从已知的高性能CUDA内核的血统中构建创新档案，该档案使用以前的踏板石来实现进一步的翻译和性能增长。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/preams_delay_3701      [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itqrgl/p_sakana_ai_released_cuda_ai_engineer/</guid>
      <pubDate>Thu, 20 Feb 2025 05:07:05 GMT</pubDate>
    </item>
    <item>
      <title>[d]谢谢您对Tensorpool的测试！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itml16/d_thank_you_for_your_beta_testing_of_tensorpool/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   tldr;谢谢你，并免费为你们提供GPU奖学金：） 大家好！我们只想感谢这个SubredDit在此处的上一篇文章中获得的压倒性支持。我们想让大家知道，您的反馈使我们昨天可以进行官方的YC发布。  https://www.ycombinator.com/launches/launches/mq0-tensorpool---tensorpool--------------------------------------参最常使用的gpus   特别感谢此subreddit，我们将向在接下来的几周内为我们提供大量反馈的用户提供20美元的GPU积分。只需通过[ team@tensorpool.dev ]（mailto： team@tensorpool。 dev ）您看到了这篇文章。我们还默认情况下还会赠送$ 5/周。 再次感谢，如果您有兴趣了解Tensorpool，则可以在此处查看我们： github.com/tensorpool/tensorpool    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tensorpool_tycho     [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1itml16/d_thank_you_for_your_your_your_your_your_teste_testing_of_tensorpool/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itml16/d_thank_you_for_your_beta_testing_of_tensorpool/</guid>
      <pubDate>Thu, 20 Feb 2025 01:27:56 GMT</pubDate>
    </item>
    <item>
      <title>[d]检索增强一代的未来是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itl38x/d_what_is_the_future_of_retrieval_augmented/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  抹布令人怀疑。关于使用传统的IR技术为模型获取上下文的某些东西感觉到。这让我想起了Netflix在互联网足以流式传输之前必须邮寄DVD。 我无法想象以后使用数据库的LLMS。为什么不在推理期间而不是以前的检索呢？例如。如果数据库直接嵌入了KV缓存中，则可以像其他所有内容一样通过梯度下降来检索。至少对我来说，这似乎比使用（低精确）嵌入搜索来收集上下文并将其塞入提示中。 。有 Lost-In-in-the-the-the-Middle效果，和上下文污染的风险，即使所有这些都会降低性能，即使所有这些都会降低性能还存在正确的上下文。推理性能也随着更多上下文的添加。   无论未来如何，我的感觉是抹布将在几年内变得过时。大家怎么想？ 编辑： DeepMind的复古和 self-rag 似乎相关。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/jsonathan     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itl38x/d_what_is_the_future_of_retrieval_augmented/</guid>
      <pubDate>Thu, 20 Feb 2025 00:17:48 GMT</pubDate>
    </item>
    <item>
      <title>[r]扩散是解决高效RNN的解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1it790b/r_diffusion_is_the_solution_for_efficient_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我表明扩散核捕获全局依赖性，并且具有复发结构的简单扩散内核在更少的参数和flops中优于变形金刚。    https://arxiv.org/abs/2502.12381      &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1it790b/r_diffusion_is_is_solution_solution_solution_for_for_ffidice_and/”&gt; [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1it790b/r_diffusion_is_the_solution_for_efficient_and/</guid>
      <pubDate>Wed, 19 Feb 2025 14:51:24 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客等对于产品和服务。 请不要发布链接缩短器，链接聚合器网站或自动订阅链接。    任何滥用信托的滥用都会领导禁止。 鼓励其他人创建新帖子，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。   元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iqiy4x/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 16 Feb 2025 03:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1ilhw29/d_simple_questions_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    </channel>
</rss>