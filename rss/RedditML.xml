<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 09 Mar 2024 12:20:46 GMT</lastBuildDate>
    <item>
      <title>[P] 训练了超过 120 个风格迁移 MLModel - 它们位于 GitHub 上</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baf5if/p_trained_over_120_style_transfer_mlmodels_here/</link>
      <description><![CDATA[       由   提交 /u/VysokoAnime   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baf5if/p_trained_over_120_style_transfer_mlmodels_here/</guid>
      <pubDate>Sat, 09 Mar 2024 10:54:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 具有广义持续学习的可扩展语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baepox/r_scalable_language_model_with_generalized/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=mz8owj4DXu 摘要：  持续学习变得越来越重要，因为它促进了语言模型中可扩展知识和技能的获取和完善。然而，现有方法通常在现实场景中遇到严格的限制和挑战，例如依赖经验回放、优化约束和推理任务 ID。在这项研究中，我们引入了可扩展语言模型（SLM）来在更具挑战性和通用性的环境中克服这些限制，代表着持续学习实际应用的重大进步。具体来说，我们提出了与动态任务相关知识检索（DTKR）集成的联合自适应重新参数化（JARe） ），以实现基于特定下游任务的语言模型的自适应调整。这种方法利用向量空间内的任务分布，旨在实现平稳且轻松的持续学习过程。我们的方法在不同的骨干网和基准上展示了最先进的性能，在全套和少量场景中实现有效的持续学习，并且遗忘最少。此外，虽然之前的研究主要集中在分类等单一任务类型，但我们的研究超越了大型语言模型，即 LLaMA-2，探索了跨不同领域和任务类型的影响，例如单一语言模型可以适当地扩展到更广泛的应用程序。代码和模型将向公众发布。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baepox/r_scalable_language_model_with_generalized/</guid>
      <pubDate>Sat, 09 Mar 2024 10:24:41 GMT</pubDate>
    </item>
    <item>
      <title>自然语言处理工具[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baeelg/tools_in_natural_language_processing_d/</link>
      <description><![CDATA[我非常努力地研究 NLP 中的工具并找到实现代码，但我没有找到任何好的学习资源，现在我对 spacy、nltk 以及诸如 bag of Words 和 Tfidf 之类的东西有很好的理解，但我无法超越这些东西，请帮忙。   由   提交 /u/Flashy-Tomato-1135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baeelg/tools_in_natural_language_processing_d/</guid>
      <pubDate>Sat, 09 Mar 2024 10:03:13 GMT</pubDate>
    </item>
    <item>
      <title>专业艺术创作者 vs 十字线英雄 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bae7d7/pro_art_creator_vs_crosshair_hero_p/</link>
      <description><![CDATA[对于使用两个 rtx 3090 的机器学习项目。 Pro Art e670x Creator 和 e670x crosshair Hero 哪个更好？  我想使用 Linux...   由   提交/u/amxhd1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bae7d7/pro_art_creator_vs_crosshair_hero_p/</guid>
      <pubDate>Sat, 09 Mar 2024 09:49:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习 CUDA/C++ 有多大价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</link>
      <description><![CDATA[目前每个人都在努力使 AI 实现快速/高效（因为效率更高 -&gt; 计算上花费的资金更少）。 例如，Flash Attention 2是在CUDA中实现的。 Llama.cpp 是 C++ PyTorch 够用吗？或者在这个市场上学习 CUDA/C++ 是否有优势，特别是对于法学硕士？ 如果 CUDA 在某些情况下有用，那么这些情况是什么？   由   提交/u/joelthomas-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</guid>
      <pubDate>Sat, 09 Mar 2024 09:44:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemma-7b：本地 RTX 3090 上的推理速度太慢</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1badye0/d_gemma7b_inference_is_way_too_slow_on_local_rtx/</link>
      <description><![CDATA[只是一个简单的测试，如下所示 from Transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(&quot;google /gemma-7b&quot;) model = AutoModelForCausalLM.from_pretrained(&quot;google/gemma-7b&quot;, device_map=&quot;auto&quot;) Mission_text = &quot;用 Java 写一个简单的程序&quot;; Mission_ids = tokenizer(mission_text, return_tensors=“pt”).to(“cuda”) Mission_outputs = model.generate(input_ids=mission_ids[“input_ids”], max_new_tokens=500) print(tokenizer.decode(mission_outputs[0]) ]))  和输出 用 Java 写一个简单的程序，它将接受一个字符串并返回元音的数量字符串。答案：步骤 1/4 1. 我们需要创建一个以字符串作为输入的方法。步骤 2/4 2. 我们需要遍历字符串并检查每个字符是否是元音。步骤3/4 3.如果一个字符是元音，我们需要增加一个计数器。步骤 4/4 4. 最后，我们需要返回计数器作为字符串中元音的数量。代码如下： public static int countVowels(String str) { int count = 0; for (int i = 0; i &lt; str.length(); i++) { char ch = str.charAt(i); if (ch == &#39;a&#39; || ch == &#39;e&#39; || ch == &#39;i&#39; || ch == &#39;o&#39; || ch == &#39;u&#39;) { count++;返回计数；为了测试程序，我们可以使用字符串调用该方法并打印结果： public static void main(String[] args) { String str = &quot;Hello World&quot;; int count = countVowels(str); System.out.println(&quot;&quot; + str + &quot; 中的元音数量为 &quot; + count);输出：Hello World 中的元音数量为 3&lt;eos&gt;  我认为在我的本地计算机上使用 RTX 3090 生成上面的文本大约需要 10 分钟，是不是太慢了？ Windows 10，pytorch 2.1。 2+cu121，CUDA 12.3，VRAM消耗约22GB，推理时GPU负载约60-70%   由   提交/u/tunggad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1badye0/d_gemma7b_inference_is_way_too_slow_on_local_rtx/</guid>
      <pubDate>Sat, 09 Mar 2024 09:32:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 R/Python 进行机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</link>
      <description><![CDATA[我是应用统计专业的学生。正因为如此，我们通过 R 学习统计机器学习。然而，当我遇到机器学习时，大多数时候，人们通常更多地谈论使用 Python。所以我不知道有哪个领域/行业实际使用 R 的 ML 来代替？如果 R 在 ML 中非常不受欢迎，你认为公司会因为我只懂 R 而选择我吗？ （如果是这样的话我可能会尝试学习Python，但我想我不会很强）。干杯伙计们！    由   提交 /u/StrangerOnTheRoad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</guid>
      <pubDate>Sat, 09 Mar 2024 08:19:19 GMT</pubDate>
    </item>
    <item>
      <title>需要一些建议[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1babyuc/need_some_advice_p/</link>
      <description><![CDATA[您好，我需要一些建议。对于我的机器学习项目，我从亚马逊购买了一台二手华硕 rog e670x crosshair Hero。去拿主板时，我发现一些 cpu 引脚弯曲或丢失，所以我可以将其退回。我将来想使用两个 rtx 3090。但是当把系统放在一起时我发现这可能是一个问题。我不打算进行任何认真的超频。还有什么主板可以推荐。我打算使用Linux。预算500欧元，便宜点也不错，但更喜欢高端的。   由   提交/u/amxhd1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1babyuc/need_some_advice_p/</guid>
      <pubDate>Sat, 09 Mar 2024 07:15:26 GMT</pubDate>
    </item>
    <item>
      <title>[N] 矩阵乘法突破可能带来更快、更高效的人工智能模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</link>
      <description><![CDATA[   “计算机科学家发现了一个报告称，通过消除以前未知的低效率，比以往更快地乘以大型矩阵的新方法 广达杂志。这最终可能会加速人工智能模型，例如ChatGPT，它严重依赖矩阵乘法来运行。据报道，最近两篇论文中提出的研究结果使矩阵乘法效率实现了十多年来的最大改进。 ...图形处理单元 (GPU) 擅长处理矩阵乘法任务，因为它们能够同时处理许多计算。他们将大型矩阵问题分解为更小的部分，并使用算法同时解决它们。完善该算法一直是过去一个世纪（甚至在计算机出现之前）矩阵乘法效率突破的关键。 2022 年 10 月，我们涵盖了Google DeepMind AI 模型发现的一项名为 AlphaTensor 的新技术，专注于针对特定矩阵大小（例如 4x4 矩阵）的实用算法改进。 相比之下， 新研究，由清华大学的段燃和周仁飞、加州大学伯克利分校的吴洪勋以及 Virginia Vassilevska 进行麻省理工学院的 Williams、Yinzhan Xu 和 Zixuan Xu（在第二篇论文中）寻求通过降低复杂性指数 ω 来实现理论增强，从而在所有大小的矩阵上获得广泛的效率增益。这项新技术并没有像 AlphaTensor 那样寻找直接、实用的解决方案，而是解决了基础性的改进，可以在更广泛的范围内提高矩阵乘法的效率。  ... 两个 n×n 矩阵相乘的传统方法需要 n3 次单独的乘法。然而，这项新技术改进了“激光方法”。由 Volker Strassen 于 1986 年提出，减小了指数的上限（表示为前面提到的 ω），使其更接近到理想值 2，这表示理论上所需的最小操作数。” ​ https://preview.redd.it/a49r1ajv59nc1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp; s =cf315793e6784ef9e62d48e00ebf0f3809070f6c https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/&lt; /strong&gt;   由   提交/u/Secure-Technology-78   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</guid>
      <pubDate>Sat, 09 Mar 2024 06:28:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习和深度学习的计算时间非常短 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ba0p61/very_short_computational_time_in_ml_and_dl_d/</link>
      <description><![CDATA[谁能解释一下论文中提到的计算时间为何如此之短，比如训练整个数据集不到 1 秒？我在此表中提供了一个摘自文章的示例： 编辑：我提供的论文并不是唯一计算时间少于一秒的论文，还有许多其他论文。此外，还有论文报道计算时间为 2-3 秒。我不明白这是怎么可能的.. edit2：我应该补充一点，所使用的数据集包含时间序列数据（传感器测量），样本数量相对较少（7532 个样本），但仍然我很难理解如何在 3.32 ms 内完成训练。   由   提交 /u/PerfecttMachine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ba0p61/very_short_computational_time_in_ml_and_dl_d/</guid>
      <pubDate>Fri, 08 Mar 2024 22:02:54 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 您认为哪个 AutoML 平台是最好的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9w8k6/discussion_which_automl_platform_do_you_think_is/</link>
      <description><![CDATA[我目前正在评估以下平台：  SageMaker Autopilot Databricks AutoML&lt; /li&gt; DataRobot H2O.ai AutoML  我发现比较它们是非常令人难以接受的，因为他们的产品页面几乎没有提供有关其工作原理的细节（更像是他们想销售他们的产品）。如果您有使用过这些产品的经验，哪一个在易用性、成本和性能方面最令您满意？   由   提交 /u/barberogaston   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9w8k6/discussion_which_automl_platform_do_you_think_is/</guid>
      <pubDate>Fri, 08 Mar 2024 19:04:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] Imaginarium 中的法学硕士：通过模拟试错进行工具学习 - Microsoft Semantic Machines 2024 - 使 Mistral-Instruct-7B 提升 46.7%，使其在 ToolBench 基准测试中超越 GPT-4！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9pigr/r_llms_in_the_imaginarium_tool_learning_through/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2403.04746 Github： https://github.com/microsoft/simulated-trial-and-error 摘要：  工具对于大型语言模型 (LLM) 至关重要获取最新信息并在外部环境中采取相应行动。工具增强法学硕士的现有工作主要侧重于工具的广泛覆盖范围和添加新工具的灵活性。然而，令人惊讶的是，一个关键方面却没有得到足够的研究，那就是法学硕士如何准确地使用其接受过培训的工具。我们发现现有的 LLM，包括 GPT-4 和专门针对工具使用进行微调的开源 LLM，正确率仅达到 30% 至 60% 范围内，远未在实践中可靠使用。我们提出了一种受生物学启发的工具增强法学硕士方法，即模拟试错（STE），它协调了生物系统中成功使用工具行为的三个关键机制：试错、想象力和记忆。具体来说，STE 利用法学硕士的“想象力”来模拟使用工具的合理场景，之后法学硕士与该工具进行交互，从其执行反馈中学习。 短期记忆和长期记忆分别用于提高探索的深度和广度。 ToolBench 上的综合实验表明，STE 在上下文学习和微调设置下都显着改善了法学硕士的工具学习，为 Mistral-Instruct-7B 带来了 46.7% 的提升，使其性能超越了 GPT-4。我们还展示通过简单的体验重放策略有效地持续学习工具。  https://preview.redd.it/1xbbb1mbg4nc1.jpg?width=1233&amp;format=pjpg&amp;auto= webp&amp;s=4e7ae3a1f002cdca9be37cee6be3ce1b883f1799 https://preview.redd.it/9ndg36mbg4nc1.jpg?width=1531&amp;format=pjpg&amp;auto=webp&amp;s=af0658b3a329e0005f44a5e01c09bd99ed74888a   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9pigr/r_llms_in_the_imaginarium_tool_learning_through/</guid>
      <pubDate>Fri, 08 Mar 2024 14:39:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 不可能有真正的苏格兰口语系统（模仿）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</link>
      <description><![CDATA[       由   提交 /u/TobyWasBestSpiderMan   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</guid>
      <pubDate>Fri, 08 Mar 2024 14:33:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向通用计算机控制：以 Red Dead Redemption II 的多模式代理为例 - 北京人工智能研究院 (BAAI) 2024 - 第一个能够在 AAA 游戏中跟踪并完成真实任务的代理！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9jrxo/r_towards_general_computer_control_a_multimodal/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2403.03186  包含代码和视频的 Projekt 网站：https://baai-agents.github.io/Cradle/  摘要：  尽管在特定任务和场景中取得了成功，但现有基础在大型模型（LM）和高级工具的支持下，智能体仍然无法泛化到不同的场景，这主要是由于不同场景的观察和行动存在巨大差异。在这项工作中，我们提出了通用计算机控制（GCC）设置：构建可以通过仅将计算机的屏幕图像（可能还有音频）作为输入并生成键盘和鼠标操作作为输出来掌握任何计算机任务的基础代理，类似于到人机交互。实现GCC的主要挑战是：1）用于决策的多模态观察，2）键盘和鼠标精确控制的要求，3）&lt; /strong&gt;长期记忆和推理的需要，以及4)高效探索和自我完善的能力。针对GCC，我们引入了Cradle，一个具有六个主要模块的代理框架，包括：1）信息收集以提取多模态信息， 2) 自我反思，重新思考过去的经验，3) 任务推理，选择下一个最佳任务，4) 技能策划，用于生成和更新给定任务的相关技能，5) 生成键盘和鼠标控制特定操作的行动计划，以及 6) 存储和检索过去经验和已知技能的内存。为了展示Cradle的泛化能力和自我完善能力，我们将其部署在复杂的AAA游戏《荒野大镖客2》中，作为对GCC具有挑战性目标的初步尝试。 据我们所知，我们的工作是第一个使基于 LMM 的代理能够在复杂的 AAA 游戏中遵循主要故事情节并完成真实任务的工作，同时最大限度地减少对先验知识或资源的依赖。  https://preview.redd .it/5pxz5wc9s2nc1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;s=7f407686977e84e9b0465cfa5a29b4f735a83365 https://preview.redd.it/e09d2wc9s2nc1.jpg?width=1332&amp;format=pjpg&amp;auto=webp&amp;s=0db5a0c19 ff3d060644077d9640718017d6699af https://preview .redd.it/x656lyc9s2nc1.jpg?width=1349&amp;format=pjpg&amp;auto=webp&amp;s=d433c42a46dccf5f5b9609c5535e087179532c2e   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9jrxo/r_towards_general_computer_control_a_multimodal/</guid>
      <pubDate>Fri, 08 Mar 2024 09:02:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>