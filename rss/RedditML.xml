<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 19 Oct 2024 18:20:38 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么美国的博士生看起来像是强大的终极 Boss</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/</link>
      <description><![CDATA[您好， 我是欧洲一所大学的博士生，研究方向为 AI/ML/CV 等。我的博士学位是 4 年。第一年，我几乎只是学习如何进行实际研究，教了一门课程来学习事物的工作原理等。第二年，我以合著者的身份在 CVPR 上发表了我的第一篇出版物。到第三年，我可以管理研究项目，我了解如何申请资助，资金如何运作，所有这些的政治问题等。我在我的简历中添加了 2 篇出版物、一本期刊和另一场会议作为第一作者。我非常参与行业，并且还为与我的实验室签订合同的公司编写了大量有关 AI、系统架构、后端、云、部署等方面的生产级代码。 问题是，当我看到美国与我相似的博士生时，他们有 10 篇出版物，其中 5 篇是第一作者，所有这些出版物都是 CVPR、ICML、ICLR、NeurIPS ......等等。我不明白，这些人不睡觉吗？他们如何能够完成如此大量的工作，并且每年仍在 A* 期刊上发表 3 篇出版物？ 我不认为这些人比我聪明，通常我会有想法，然后查找是否存在某些东西，我可以看到一些东西刚刚由斯坦福大学或 DeepMind 等的一些博士生在 1 个月前发表，所以我可以看到我的推理在 SOTA 方面并不晚。但是你需要掌握的概念，仅仅拥有其中一种出版物 + 你需要投入的精力和时间以及完成所有事情的资源，对于 2 到 3 个月的项目来说是不可能的。这些人怎么可能做到这一点？ 谢谢 !    提交人    /u/SilenceForLife   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/</guid>
      <pubDate>Sat, 19 Oct 2024 17:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 面向产品的机器学习：数据科学家指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7bwrk/p_productoriented_ml_a_guide_for_data_scientists/</link>
      <description><![CDATA[      嗨，我一直在收集我的想法和经验，以构建基于 ML 的产品，并为数据科学家整理产品设计入门指南。很想听听您的反馈！    提交人    /u/usernamehere93   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7bwrk/p_productoriented_ml_a_guide_for_data_scientists/</guid>
      <pubDate>Sat, 19 Oct 2024 15:53:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我开发了一个 Web 应用，利用 Mendeley 读者数量来追踪热门 AI 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7barf/p_i_built_a_web_app_to_track_trending_ai_papers/</link>
      <description><![CDATA[大家好！ 我创建了一个网络应用程序，可帮助研究人员和 AI 感兴趣的人掌握最具影响力的 arXiv AI 论文。  特点： - 根据 Mendeley 读者数量跟踪论文 - 可自定义的时间段：1 周、1 个月、3 个月、6 个月、1 年和所有时间 - 两种查看模式： 1.“最棒” - 显示总读者数量最多的论文 2.“趋势” - 突出显示获得读者最快的论文 后端完全用 Python 编写，前端是基本的 JS、HTML 和 Tailwind。 当我有更多时间时，我还考虑开源该项目。 社区问题： 1. 您会觉得这个工具对您的研究或学习有用吗？ 2. 您希望添加任何功能吗？ 3. 如果我开源它，有人有兴趣做出贡献吗？ 演示链接：https://aipapers.pantheon.so    提交人    /u/yachty66   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7barf/p_i_built_a_web_app_to_track_trending_ai_papers/</guid>
      <pubDate>Sat, 19 Oct 2024 15:25:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] NHiTs：深度学习 + 信号处理用于时间序列预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7b6po/p_nhits_deep_learning_signal_processing_for/</link>
      <description><![CDATA[NHITs 是用于时间序列预测的 SOTA DL，因为：  接受过去的观察、未来已知的输入和静态外生变量。 使用多速率信号采样策略来捕获复杂的频率模式 - 对于金融预测等领域至关重要。 点和概率预测。  您可以在此处找到该模型的详细分析：    提交人    /u/apaxapax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7b6po/p_nhits_deep_learning_signal_processing_for/</guid>
      <pubDate>Sat, 19 Oct 2024 15:19:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] Layernorm 的名字容易让人混淆？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g7am36/d_layernorm_is_confusingly_named/</link>
      <description><![CDATA[TIL，在 NLP 中，你通过对每个特征进行激活并对每个特征进行规范化来进行层规范： 因此，如果你有一批两个示例： [I, am, a, boy] [She, is a girl] 那么你将创建 [H_I, H_am, H_a, H_boy] -&gt; [u_I, u_am, u_a, u_boy] and sigmas [H_She, H_is, H_a, H_girl] -&gt; [u_she, u_is, u_a, u_girl] and sigmas 所以你最终得到了 8 个 us 和 sigma。这听起来不像是您正在规范化该层的激活，这让我想说 layernorm 的名称令人困惑。我是不是漏掉了什么？    提交人    /u/Complex-Media-8074   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g7am36/d_layernorm_is_confusingly_named/</guid>
      <pubDate>Sat, 19 Oct 2024 14:53:24 GMT</pubDate>
    </item>
    <item>
      <title>[项目] JAX 上的深度学习库/框架，将神经网络保留为纯函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g762xs/project_deep_learning_libframework_on_jax_that/</link>
      <description><![CDATA[您好，我正在 JAX 上开发一个新的深度学习框架，非常喜欢 JAX 的 FP 方法，并受到启发，想做一个不会将其转变为 OOP 的框架。它深受 Haiku 和 Haiku 的变换的启发。 链接：Zephyr：https://github.com/mzguntalan/zephyr 可通过 pip install 获得 它的主要区别在于： - 神经网络只是像 `def f(params, x, hyper_parameters)` 这样的函数 - 因此，虽然它很冗长，但它的行为方式却很明显。在 FP 中，您可以在需要时进行部分应用/闭包 - 做“模型手术”非常容易因为它只是调用函数（例如，请参阅自述文件） - zephyr 负责验证参数并通过单个“trace”函数创建它们。 这个名为 Zephyr 的框架使在 numpy 级别编码神经网络变得非常方便。您可以获得最终控制权，并且 zephyr 的“trace”会为您初始化参数，所有网络都只是纯函数。没什么特别的，因此您可以使用 python、函数和 numpy 技能来做任何您想做的事情。我在自述文件中展示了示例。希望你喜欢它。 我忙于实现参数创建功能，因此我还没有实现所有常见网络，而且我还没有最终确定网络参数，所以这些参数可能会发生变化。但如果您现在尝试并提出建议，这些将极大地影响它未来的设计。  期待您的评论！ 其他说明： - 我仍在试验还有哪些其他方法可以使 Python 上的 FP 更加容易或可读 - 例如：我制作了一个装饰器，使函数自身“自动部分化”，以便您可以将参数分成这样的组：`g(x,y,z) == g( _ , y, _ ) (x,z)`，其中 `_ = Placeholder()` - 在关注核心网络/层之前，仍在试验如何整合其他功能 - 非常欢迎提出建议    提交人    /u/Pristine-Staff-5250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g762xs/project_deep_learning_libframework_on_jax_that/</guid>
      <pubDate>Sat, 19 Oct 2024 10:35:11 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Tsetlin 机器用于深度逻辑学习和图形推理（六年后终于完成了！）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g75gcb/project_tsetlin_machine_for_deep_logical_learning/</link>
      <description><![CDATA[   https://preview.redd.it/spcqdkqwnovd1.png?width=2643&amp;format=png&amp;auto=webp&amp;s=ba0d7dd294ef9814f20bf3950f0049e80cf8d8d9 大家好！我刚刚完成了第一个深度 Tsetlin 机器 - 一个可以跨图进行多模态学习和推理的图 Tsetlin 机器。在 2018 年推出 Tsetlin 机器后，我希望能快速弄清楚如何制作一个深度机器。我花了六年时间！分享项目：https://github.com/cair/GraphTsetlinMachine 特点：  定向和标记多图 矢量符号节点属性和边类型 嵌套（深）子句 任意大小的输入 合并Vanilla、Multiclass、 卷积和合并 Tsetlin 机器 重写更快的 CUDA 内核  路线图：  用 C 或 numba 重写 graphs.py，以更快地构建图形 添加自动编码器 添加回归 添加多输出 使用邻接矩阵进行图形初始化  很高兴收到关于发展！    由    /u/olegranmo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g75gcb/project_tsetlin_machine_for_deep_logical_learning/</guid>
      <pubDate>Sat, 19 Oct 2024 09:48:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 变压器自动编码器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g748gu/d_transformer_autoencoder/</link>
      <description><![CDATA[是否有人知道使用编码器 - 解码器 Transformer 架构训练具有信息瓶颈的文本/序列自动编码器的研究？我设想了类似 bert 的编码器，其中完整序列被编码为单个向量，然后是解码器，该解码器交叉关注编码的潜在表示并经过训练以完整地重现完整序列。 这似乎是一个非常明显的用例，我确信我一定遗漏了一些东西，但我似乎找不到任何关于它的文献。例如，Transformers 从第一天起就是编码器 - 解码器架构，将它们用于信息瓶颈自动编码器似乎是一件轻而易举的事，一定有人尝试过。这是否只是出于某种我不知道的原因而成为一个坏主意，或者是已经尝试过但无济于事，并且该论文因此变得无关紧要？ 从表面上看，这似乎是一种预训练编码器和解码器的好方法（肯定比 T5 好得多？）因为您仍然可以使用非常简单的下一个标记预测任务，并且通过微调，我可以看到编码器对 RAG 变得非常有用，并且可能解锁或增强其他 ICL 能力。我想到其他值得探索的想法是潜在扩散文本生成和上下文压缩，以实现极长的上下文能力。我相信还有更多。 据我所知，唯一的缺点是在训练期间需要在编码器和解码器之间分割内存和计算。 有人有什么线索可以让我在这里跟踪吗？    提交人    /u/next-choken   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g748gu/d_transformer_autoencoder/</guid>
      <pubDate>Sat, 19 Oct 2024 08:12:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2021 年 12 月的一篇有趣帖子，讨论了 transformer 的功效</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g73ym7/d_an_interesting_thread_from_december_2021/</link>
      <description><![CDATA[  由    /u/next-choken  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g73ym7/d_an_interesting_thread_from_december_2021/</guid>
      <pubDate>Sat, 19 Oct 2024 07:51:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 评估生产中的分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g6w6za/d_evaluating_classification_in_production/</link>
      <description><![CDATA[对于 Facebook、Youtube、LinkedIn 等，他们如何在生产中评估其有害内容检测模型？如果是离线，可以使用精度、召回率、F1。由于我们不知道生产过程中的标签是什么，他们使用什么指标进行评估/监控？    提交人    /u/lalalagay   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g6w6za/d_evaluating_classification_in_production/</guid>
      <pubDate>Fri, 18 Oct 2024 23:53:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 仍然无法规划；LRM 可以吗？OpenAI 在 PlanBench 上对 o1 的初步评估</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g6qpeq/r_llms_still_cant_plan_can_lrms_a_preliminary/</link>
      <description><![CDATA[论文：https://www.arxiv.org/abs/2409.13373 “虽然 o1 的性能在基准上有了很大的改进，超过了竞争对手，但它还远远没有达到饱和状态。” 总结很恰当。o1 看起来是一个非常令人印象深刻的改进。同时，它揭示了剩余的差距：随着组合长度的增加而退化，成本增加 100 倍，并且当“检索”因名称混淆而受到阻碍时，退化程度大大降低。 但是，我想知道这是否足够接近。例如，这种类型的模型至少足以提供合成数据/监督来训练可以填补这些空白的模型。如果是这样，恕我直言，很快就能找到答案。 此外，作者还添加了一些辛辣的脚注。例如： “研究人员使用纳税人提供的研究资金来向 OpenAI 等私人公司支付费用，以评估其私人商业模式，这种讽刺意味当然在我们心中挥之不去。”    提交人    /u/marojejian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g6qpeq/r_llms_still_cant_plan_can_lrms_a_preliminary/</guid>
      <pubDate>Fri, 18 Oct 2024 19:36:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 主流 LLM 标记器的局限性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g6dc8l/r_limitations_in_mainstream_llm_tokenizers/</link>
      <description><![CDATA[主流 LLM 标记器无法编码和解码为精确的字符串。这意味着它们不是无损的。一些 Llama、Mistral 和 Phi 标记器无法编码字符串 &#39; 谁放了狗？！ !&#39;，然后解码为相同的字符串。 如果运行代码：```python from transformers import AutoTokenizer models = [ &#39;meta-llama/Llama-2-7b&#39;, &#39;meta-llama/Meta-Llama-3-8B&#39;, &#39;meta-llama/Llama-3.1-8B&#39;, &#39;mistralai/Mistral-7B-v0.3&#39;, &#39;mistralai/Mixtral-8x7B-v0.1&#39;, &#39;mistralai/Mixtral-8x22B-v0.1&#39;, &#39;mistralai/Mistral-Nemo-Instruct-2407&#39;, &#39;mistralai/Mistral-Small-Instruct-2409&#39;, &#39;mistralai/Mistral-Large-Instruct-2407&#39;, &#39;microsoft/phi-1&#39;, &#39;microsoft/phi-1_5&#39;, &#39;microsoft/phi-2&#39;, &#39;microsoft/Phi-3-mini-4k-instruct&#39;, &#39;microsoft/Phi-3.5-mini-instruct&#39;, ] text = &#39; 谁放了狗？！&#39; for n in models: tokenizer = AutoTokenizer.from_pretrained(n) text2 = tokenizer.decode(tokenizer.encode(text, add_special_tokens=False)) if text2 == text: print(&#39;OK: &#39;, n, repr(text2)) else: print(&#39;ERR:&#39;, n, repr(text2))  ``` 您将得到： OK: meta-llama/Llama-2-7b &#39; 谁放了狗？！&#39; ERR: meta-llama/Meta-Llama-3-8B “谁放出了狗？！！” ERR: meta-llama/Llama-3.1-8B “谁放出了狗？！！” ERR: mistralai/Mistral-7B-v0.3 “谁放出了狗？！！” OK: mistralai/Mixtral-8x7B-v0.1 “谁放出了狗？！！” ERR: mistralai/Mixtral-8x22B-v0.1 “谁放出了狗？！！” OK: mistralai/Mistral-Nemo-Instruct-2407 “谁放出了狗？！！” OK: mistralai/Mistral-Small-Instruct-2409 “谁放出了狗？！！” OK：mistralai/Mistral-Large-Instruct-2407 &#39;谁放出了狗？！&#39; ERR：microsoft/phi-1 &#39;谁放出了狗？！&#39; ERR：microsoft/phi-1_5 &#39;谁放出了狗？！&#39; ERR：microsoft/phi-2 &#39;谁放出了狗？！&#39; OK：microsoft/Phi-3-mini-4k-instruct &#39;谁放出了狗？！&#39; OK：microsoft/Phi-3.5-mini-instruct &#39;谁放出了狗？！&#39;  所有标有 ERR 的都无法编码​​并解码为相同的字符串。    提交人    /u/mtasic85   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g6dc8l/r_limitations_in_mainstream_llm_tokenizers/</guid>
      <pubDate>Fri, 18 Oct 2024 08:35:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] PyTorch 2.5.0 发布！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/</link>
      <description><![CDATA[https://github.com/pytorch/pytorch/releases/tag/v2.5.0 亮点：我们很高兴地宣布 PyTorch® 2.5 的发布！此版本为 SDPA 提供了新的 CuDNN 后端，默认情况下，为 H100 或更新 GPU 上的 SDPA 用户启用加速。此外，torch.compile 的区域编译提供了一种减少 torch.compile 冷启动时间的方法，它允许用户编译重复的 nn.Module（例如 LLM 中的转换器层）而无需重新编译。最后，TorchInductor CPP 后端通过 FP16 支持、CPP 包装器、AOT-Inductor 模式和最大自动调谐模式等众多增强功能提供了可靠的性能加速。此版本由 504 位贡献者自 PyTorch 2.4 以来的 4095 次提交组成。我们衷心感谢我们敬业的社区所做的贡献。 我最喜欢的一些改进：  通过重复使用重复模块加快 torch.compile 编译速度 torch.compile 支持 torch.istft FlexAttention：一种灵活的 API，只需几行惯用的 PyTorch 代码即可实现各种注意机制，如滑动窗口、因果掩码和 PrefixLM。此 API 利用 torch.compile 生成融合的 FlashAttention 内核，从而消除了额外的内存分配并实现了与手写实现相当的性能。此外，我们使用 PyTorch 的自动求导机制自动生成向后传递。此外，我们的 API 可以利用注意力掩码中的稀疏性，从而比标准注意力实现有显著的改进。     提交人    /u/parlancex   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/</guid>
      <pubDate>Thu, 17 Oct 2024 22:11:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 13 Oct 2024 02:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>