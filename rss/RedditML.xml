<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 15 Feb 2024 21:12:43 GMT</lastBuildDate>
    <item>
      <title>[R] 激活的三个十年：神经网络 400 个激活函数的全面调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09092 摘要：  神经网络已被证明是解决复杂问题的高效工具生活的许多方面都存在问题。最近，随着深度学习的出现，它们的重要性和实际可用性进一步得到加强。神经网络成功的重要条件之一是选择合适的激活函数，将非线性引入模型。过去的文献中已经提出了许多类型的这些函数，但没有一个综合来源包含它们的详尽概述。即使根据我们的经验，缺乏这种概述也会导致冗余和无意中重新发现已经存在的激活函数。为了弥补这一差距，我们的论文提出了一项涉及 400 个激活函数的广泛调查，其规模比以前的调查大几倍。我们的综合汇编也参考了这些调查；然而，其主要目标是提供先前发布的激活函数的最全面的概述和系统化，并提供其原始来源的链接。第二个目标是更新当前对这一系列函数的理解。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</guid>
      <pubDate>Thu, 15 Feb 2024 20:12:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 Mistral 7B 微调为 3500 页交易评论的技术分析大师</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aro8o2/p_finetuning_mistral_7b_to_be_a_technical/</link>
      <description><![CDATA[基本上，我有一整套 PDF 格式的技术分析和期权交易书籍。所以我很自然地想知道如果我用它作为米斯特拉尔的训练数据会发生什么。我对结果感到非常惊讶。 多次迭代和 5 美元的 runpod 积分之后，我提出了这个问题： 当窄幅分布时这意味着什么成交量高于平均水平的蜡烛？ 经过微调的法学硕士的回应：  这可能意味着市场正在经历一段整合时期，其中价格正在横向移动，并且没有在任何一个方向上做出重大移动。高于平均水平的交易量可能表明市场内正在发生大量活动，但价格变动并没有太大变化。  它也可能表明一个过渡时期，市场正在从一种趋势转向另一种趋势。例如，如果市场一直处于强劲的上升趋势，并且在价格大幅上涨后形成蜡烛，则可能表明市场开始盘整，并可能在继续上升趋势之前进入横盘期。  在某些情况下，成交量高于平均水平的窄幅蜡烛也可能表明一段犹豫不决的时期，买家和卖家都在场，但都无法占据上风。当市场试图确定朝哪个方向移动时，这可能会导致一段时间的横盘走势。  总体而言，成交量高于平均水平的窄幅蜡烛的含义将取决于具体的背景和市场条件。在解释蜡烛形态时，考虑其他因素（例如价格行为和新闻事件）始终是一个好主意。  此处提供完整的讨论和配置： &lt; a href=&quot;https://medium.com/ai-advances/i-built-a-technical-analysis-ai-guru-from-scratch-6083370101d2&quot;&gt;https://medium.com/ai-advances/i -built-a-technical-analysis-ai-guru-from-scratch-6083370101d2 这是我为法学硕士提供的书籍的完整列表：  布莱恩·奥弗比 (Brian Overby) — 期权手册，扩展版第二版 Buff Pelz Dormeier — 通过成交量分析进行投资 劳伦斯·G. 麦克米伦 (Lawrence G. McMillan) — 期权作为战略投资，第四版 Mark Leibovit — 交易量手册、成交量交易权威指南 Lau​​rentiu Damir — 价格行为细分、金融市场独家价格行为交易方法 Sheldon Natenberg —期权波动率和定价 Steven Achelis - 从头到尾的技术分析 Jesse Seal - 以 9 比 5 的比率交易股票 Mark Sebastian，L. Celeste Taylor — Edge 交易选项 Anna Couling — 量价分析完整指南  如果您有兴趣学习如何微调 Mistral 或任何其他开源 LLM 数据，然后订阅我的时事通讯，因为我正在研究负担得起的资源来教您如何做到这一点：https ://msamon.beehiiv.com/subscribe   由   提交/u/brain-trainer  /u/brain-trainer  reddit.com/r/MachineLearning/comments/1aro8o2/p_finetuning_mistral_7b_to_be_a_technical/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aro8o2/p_finetuning_mistral_7b_to_be_a_technical/</guid>
      <pubDate>Thu, 15 Feb 2024 19:45:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用视觉文字谜题衡量人工智能的创造力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aro6jm/p_measuring_ais_creativity_with_visual_word/</link>
      <description><![CDATA[一个有趣的项目，我致力于使用像画谜这样的视觉文字游戏来测量生成式 AI 模型（例如多模态大语言模型，或 MLLM）中的*多模态*创造力谜题！  目前，MLLM 有各种各样的多模式基准（例如 VQA、图像字幕等），但据我所知，没有一个可以衡量创意方面，例如解决涉及语言和视觉的难题的能力理解（例如画画谜题的情况）。 ​ 在这个项目中，我比较了 Gemini 和 GPT-4 在画画上的少量射击与零射击能力难题并将其与人类能力进行比较。总体而言，尽管 GPT-4 用很少的镜头就能够解决一些对人类来说更困难的问题，但人类仍然更擅长解决这些难题。 ​ https://www.artfish.ai/p/measuring-ais-creativity-with-visual    由   提交 /u/porkbellyqueen111   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aro6jm/p_measuring_ais_creativity_with_visual_word/</guid>
      <pubDate>Thu, 15 Feb 2024 19:43:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 引领未来：了解人工智能法案对产品公司的要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armzu1/d_navigating_the_future_understanding_the_ai_act/</link>
      <description><![CDATA[这个博客文章是关于《人工智能法案》要求的，专门为基于产品的企业设计。它介绍了应对人工智能监管环境的基本背景、可能的障碍和战略建议，帮助 OpenCV.ai 读者做好适应这些发展的准备。 在本文中，您将发现：   什么是《人工智能法案》？  主要目标  《人工智能法案》禁止什么？  行业影响 挑战 机遇  实现新的人工智能监管合规性 合规步骤 最佳实践   完整文章ia 此处 &lt; !-- SC_ON --&gt;  由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/MachineLearning/comments/1armzu1/d_navigating_the_future_understand_the_ai_act/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armzu1/d_navigating_the_future_understanding_the_ai_act/</guid>
      <pubDate>Thu, 15 Feb 2024 18:54:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI Sora Video Gen——如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</link>
      <description><![CDATA[ 介绍 Sora，我们的文本转视频模型。 Sora 可以生成长达一分钟的视频，同时保持视觉质量并遵守用户的提示。  https:/ /openai.com/sora 研究笔记 Sora 是一种扩散模型，它从看起来像静态噪声的视频开始生成视频，然后通过多个步骤消除噪声来逐渐对其进行转换. Sora 能够一次生成整个视频或扩展生成的视频以使其更长。通过一次为多个帧提供模型预测，我们解决了一个具有挑战性的问题，即确保对象即使暂时离开视野也保持不变。 与 GPT 模型类似，Sora 使用变压器架构，释放卓越的扩展性能。 我们将视频和图像表示为称为补丁的较小数据单元的集合，每个补丁类似于 GPT 中的令牌。通过统一我们表示数据的方式，我们可以在比以前更广泛的视觉数据上训练扩散变换器，涵盖不同的持续时间、分辨率和纵横比。 Sora 建立在 DALL·E 和 DALL·E 过去的研究基础上GPT 模型。它使用 DALL·E 3 的重述技术，该技术涉及为视觉训练数据生成高度描述性的标题。因此，该模型能够更忠实地遵循生成视频中用户的文本指令。 除了能够仅根据文本指令生成视频之外，该模型还能够采用现有的静态图像并从中生成视频，精确地动画图像内容并关注小细节。该模型还可以获取现有视频并对其进行扩展或填充缺失的帧。在我们的技术论文（今天晚些时候发布）中了解更多信息。 Sora 是能够理解和模拟现实世界的模型的基础，我们相信这一功能将成为实现 AGI 的重要里程碑。 Sora 是能够理解和模拟现实世界的模型的基础。 p&gt; 示例视频：https://cdn.openai.com/sora/videos/ cat-on-bed.mp4 技术论文将于今天晚些时候发布。但是如何进行头脑风暴呢？   由   提交/u/htrp  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</guid>
      <pubDate>Thu, 15 Feb 2024 18:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 咆哮/问题：权重和偏差扫描发生奇怪的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armaiq/d_rantquestion_weird_things_happening_with/</link>
      <description><![CDATA[抱歉，如果这是错误的地方，但我只需要知道我是否是唯一经历过这种情况的人，以及是否有人经历过对我有什么建议，因为：每当我使用带有权重和偏差的扫描功能时，往往会发生奇怪的事情。 最一致的是，如果我在 python 脚本中启动 wandb 扫描代理如果 wandb.agent(sweep_id=sweep_id, function=my_function, count=count) 的计数较高（例如 count=40），GPU 内存会慢慢开始填满没有被释放，并且在几次运行之后（可能是 10 次、可能是 20 次、可能是 30 次），由于 GPU 的 OOM 错误，所有新的运行都会在开始时崩溃。  起初我以为这是 PyTorch 的事情，我尝试了各种技巧来防止它发生，比如将 my_function 包装在手动执行垃圾收集的包装器中（使用之后gc.collect）。但似乎没有任何效果。 现在我正在使用 JAX 运行 wandb 扫描，并且发生了同样的事情：具有 40GB RAM 的 GPU 没有空间用于 262144 字节分配，而仅分配了 896.4KiB贾克斯。因此，在一系列成功运行之后，所有新运行都会立即崩溃并出现 OOM 错误（或更准确地说：jaxlib.xla_extension.XlaRuntimeError：RESOURCE_EXHAUSTED：尝试分配...字节时内存不足。） . 现在，上面是我知道发生的一件非常可衡量和有形的事情（我之前在扫描过程中监控过我们办公室电脑中的 GPU 使用的内存，并且可以在某个时候看到，内存不再被释放，所有运行都开始崩溃）。但下一个听起来绝对疯狂。 有时你会在深度学习中得到 NaN。这很糟糕，但它就是发生了。因此，在我最近的一次扫描中，我有一个回调检查 NaN，并在损失变为 NaN 时结束运行（通过带有适当错误消息的 RuntimeError）。在 53 次运行中，有 11 次运行由于 NaN 而被缩短，其中 8 次在第一个训练步骤中就出现了 NaN。奇怪的是：我使用两个代理进行此（随机）扫描，并且这 8 次运行是在同一代理上连续运行。  这些运行都有不同的配置和不同的随机种子。对于所有这 8 次运行，扫描配置中没有相同的变量。如果这些崩溃是独立的，则运行以 NaN 结束的可能性为 11/53。连续 8 次运行以 NaN 结尾的概率为 (11/53)^8 = 3.44E-6。  公平地说：在很长的运行序列中发生这种情况的概率大于 3.44E-6，但仍然非常小。比如：我只是偏执吗？或者那个代理发生了什么事情导致我得到了错误的结果？我稍后会对其进行测试，因此您可能会看到一个编辑，我承认这纯粹是运气不好。  无论如何，有人有预防这些 GPU OOM 问题的技巧吗？人们是否会直接数到 1，然后提交 80 个工作来进行 slurm？或者有更好的方法来做到这一点吗？    由   提交/u/katerdag  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armaiq/d_rantquestion_weird_things_happening_with/</guid>
      <pubDate>Thu, 15 Feb 2024 18:25:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用不同的 GPU 模型来训练神经网络是否有价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arln8e/d_is_there_value_to_using_different_gpu_models_to/</link>
      <description><![CDATA[嗨，一位 ML 菜鸟。就异构计算而言，在不同型号的 GPU（例如 3090 和 4060 一起）上训练同一神经网络的不同部分是否有好处？还是只使用多个相同型号的 GPU（例如 3x 3090s）更好？   由   提交 /u/FellowOInfiniteJest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arln8e/d_is_there_value_to_using_different_gpu_models_to/</guid>
      <pubDate>Thu, 15 Feb 2024 17:59:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] BERT 与 ChatGPT 比较（文本分类和情感分析）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</link>
      <description><![CDATA[是否有人研究过针对情绪微调特定 BERT（或任何其他类似模型）与 ChatGPT（微调与否）之间的比较分析和文本分类？ 我很想知道它们在性能、成本、维护等方面的比较。   由   提交 /u/Grinbald   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</guid>
      <pubDate>Thu, 15 Feb 2024 17:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1M/10M token上下文窗口怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</link>
      <description><![CDATA[是否会启动社区头脑风暴主题？ - 人们是否认为 RingAttention 可以充分扩展？参见https://largeworldmodel.github.io - 它是用 1M 还是 10Mn 令牌窗口进行训练的，这对我来说似乎不清楚？他们是否在没有经过某种训练的情况下从 1M-&gt;10M 进行概括？ - 存在哪些数据集可以训练 10M 文本标记窗口？ - 在这么长的背景下你如何做 RLHF？ 1M 文本 ~ 4M 字符 ~ 272k 秒阅读时间（根据 Google 假设 68 毫秒/字符）~ 阅读一个示例需要 75 小时？ 编辑：当然 lucidrains 已经在着手实施 RingAttention！ (https://github.com/lucidrains/ring-attention-pytorch)   由   提交 /u/gggerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</guid>
      <pubDate>Thu, 15 Feb 2024 16:13:29 GMT</pubDate>
    </item>
    <item>
      <title>如何实现语言模型中拼写错误的鲁棒性？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj0a1/how_to_achieve_robustness_to_spelling_mistakes_in/</link>
      <description><![CDATA[据我们所知，ChatGPT 和类似的 LLM 对拼写错误非常稳健。例如，当我写“buter”时，他们会理解我的意思可能是“黄油”即使在有限的上下文场景中。 我在“干净”的语料库上预训练了 BERT 模型。就其本身而言，它在许多任务上都能很好地工作，但在包含拼写错误的嘈杂文本的示例中，性能显着下降。因此，我一直在寻找缓解这种情况的方法，并发现了一些带有拼写纠正管道的旧技术，这对我来说似乎并不有趣。在其他一些情况下，建议随机或使用预定义的字典增加语料库的噪声（在我看来，这不是很好）。然后可以选择混合干净和不干净的语料库来创建更加多样化的预训练数据。我认为这就是要走的路。 因此，如果有人能给我指出任何有关这方面的分析/比较/已发表的工作，那就太好了。或者是否有人能够解释为什么 GPT 擅长处理嘈杂的输入。   由   提交 /u/DunderSunder   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj0a1/how_to_achieve_robustness_to_spelling_mistakes_in/</guid>
      <pubDate>Thu, 15 Feb 2024 16:10:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 衡量将法学硕士纳入工作流程之前和之后的软件工程生产力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</link>
      <description><![CDATA[我在一家软件工程公司（外包）工作，我们的管理层希望衡量大型语言模型对日常工程工作（包括软件工程、数据工程、质量保证等）。最终目标是获得一些原始指标（例如“使用 LLM 时 X 团队的表现提高了 30%”）以呈现给客户，旨在证明我们优于不使用 LLM 的竞争对手。 我的观点是，准确衡量这种影响具有挑战性，因为 LLM 的表现可能会因任务环境的不同而有很大差异（例如，为网站开发简单的注册表单与为 IBM 大型机编写代码）。此外，这最终取决于执行工作的个人（例如，在使用法学硕士作为支持工具时，A 和 B 可能会在同一任务上花费不同的时间）。 我在这里合理吗？ ？有没有什么方法可以准确衡量这些影响？我试图找到有关该主题的研究论文，但大多数都侧重于综合 LLM 测试，将个人 LLM 表现与其他 LLM 进行比较。 编辑：发现 github copilot 研究博客指出生产力提高了 55%： https://github.blog/2022 -09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/   由   提交 /u/GottaPerformMiracles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</guid>
      <pubDate>Thu, 15 Feb 2024 15:52:36 GMT</pubDate>
    </item>
    <item>
      <title>[N] Gemini 1.5，具有 1M 上下文长度令牌的 MoE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</link>
      <description><![CDATA[https://blog.google/technology/ai/google-gemini-next- Generation-model-february-2024/  &amp;# 32；由   提交/u/Electronic-Author-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</guid>
      <pubDate>Thu, 15 Feb 2024 15:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用小数据集进行验证</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arakd6/d_validation_with_small_datasets/</link>
      <description><![CDATA[我所在的领域的数据集通常很小（100-10000 个样本）且具有层次结构（取自 10-50 个参与者）。这意味着，为了在足够大的测试集上评估数据，而不仅仅是少数参与者，我们需要使用交叉验证。到目前为止一切顺利。 但是，这仍然没有解决验证问题。有几种可能的方法可以进行验证：  跳过验证。这似乎是我所在领域的首选方法。我认为这是非常错误的，而且我发现它可以高估准确度 5%（包含 5000 个样本的数据集），甚至高达 20%（100 个样本）。 将训练数据一次分割为训练和验证集用于对每个测试折叠进行验证。这样做的缺点是验证集最终很小（比测试集小得多），并且如果不小心的话，训练验证分割可能是任意的。 完全嵌套交叉验证。这似乎是正确验证超参数配置的最佳方法，因为它几乎使用整个数据集进行验证。我在我的领域还没有遇到过一篇使用嵌套交叉验证（正确）的论文。我相信主要问题非常明显：如果一个人使用 10 倍嵌套交叉验证训练一个神经网络模型 100 个时期，并尝试优化 5 个二进制超参数，那么最终已经有大约 100 * 10^2 * 2 ^5 = 320,000 个纪元。如果一个 epoch 需要 10 秒，这已经相当于一个多月的计算时间，而且我们仍然只验证了很少的超参数配置。  我可以看到以下解决方案：   p&gt;  接受计算需要这么长时间（并希望评审者不要要求我们重复实验）。 找到尽可能限制超参数配置的方法。  li&gt; 改用 5 重嵌套交叉验证。 减小验证集的大小（方法 2）。 承认并停止将神经网络拟合得较小数据集。  您对此有何看法？您更喜欢哪些选项？您还有其他解决方案吗？   由   提交/u/philosophicalmachine  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arakd6/d_validation_with_small_datasets/</guid>
      <pubDate>Thu, 15 Feb 2024 08:07:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] Whisper Large v3 基准：在消费类 GPU 上以 5110 美元（每美元 11,736 分钟）转录 100 万小时 - 后续</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ar08br/p_whisper_large_v3_benchmark_1_million_hours/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ar08br/p_whisper_large_v3_benchmark_1_million_hours/</guid>
      <pubDate>Wed, 14 Feb 2024 22:50:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>