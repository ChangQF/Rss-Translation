<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 16 Feb 2024 06:17:25 GMT</lastBuildDate>
    <item>
      <title>[D] 在开始申请机器学习/数据科学职位时需要对简历进行反馈</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as0zqz/d_need_feedback_on_the_resume_as_starting_to/</link>
      <description><![CDATA[      ​ https://preview.redd.it/8jok3ifrvvic1.jpg?width= 881&amp;format=pjpg&amp;auto=webp&amp;s=5ed6fda28f9235932c47bb7fe937cbb16c0c0581   由   提交/u/masterai123  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as0zqz/d_need_feedback_on_the_resume_as_starting_to/</guid>
      <pubDate>Fri, 16 Feb 2024 05:31:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与在实际应用中部署法学硕士相关的主要挑战</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as0lj4/d_key_challenges_associated_with_deployment_of/</link>
      <description><![CDATA[在实际应用中部署法学硕士的主要挑战是什么？  扩展法学硕士以适应不断增加的工作负载和用户需求是一项挑战。确保从小规模应用程序到大规模部署的各种规模的无缝性能需要仔细的优化和资源分配。  您还遇到过哪些其他挑战？   由   提交/u/Ok_Vijay7825   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as0lj4/d_key_challenges_associated_with_deployment_of/</guid>
      <pubDate>Fri, 16 Feb 2024 05:09:52 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 今天双降的情况</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1as0i07/discussion_status_on_double_descent_today/</link>
      <description><![CDATA[双重血统的现状如何？ 如今机器学习人员对双重血统有何看法？它始于令人惊叹的人们，然后是一系列理论著作试图用线性回归和相关的简单模型来解释它，最后得出结论：最优正则化处理双下降。那么这是一个很好理解的景观吗？人们今天对此有什么思考吗？   由   提交 /u/AccomplishedTell7012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1as0i07/discussion_status_on_double_descent_today/</guid>
      <pubDate>Fri, 16 Feb 2024 05:04:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 专家混合解锁深度强化学习的参数缩放</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ary7xi/r_mixtures_of_experts_unlock_parameter_scaling/</link>
      <description><![CDATA[摘要：  最近（自）监督学习模型的快速进展在很大程度上是通过经验缩放来预测的定律：模型的性能与其大小成正比。然而，对于强化学习领域来说，类似的缩放定律仍然难以捉摸，增加模型的参数数量通常会损害其最终性能。在本文中，我们证明了将专家混合 (MoE) 模块，特别是软 MoE（Puigcerver 等人，2023）纳入基于价值的网络会产生更多参数可扩展的模型，性能的显着提高就证明了这一点跨越各种训练制度和模型大小。因此，这项工作为制定强化学习的缩放定律提供了强有力的经验证据。  论文链接：https://arxiv.org/pdf/2402.08609.pdf   由   提交/u/OwnAd9305   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ary7xi/r_mixtures_of_experts_unlock_parameter_scaling/</guid>
      <pubDate>Fri, 16 Feb 2024 03:02:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何训练用于 UI 设计的生成式 AI (galileo ai)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arxxxg/d_how_do_you_train_generative_ai_for_ui_design/</link>
      <description><![CDATA[      大家好，我最近偶然发现了https://www.usegalileo.ai/create。我很好奇他们用什么来训练这样的模型来生成 UI 设计（可导出到 Figma）。我是新手，所以我推断可能只是微调一些稳定的扩散，或者他们可能会整理 1000 多个模板并随机选择。 你认为他们正在使用什么？ ​ https: //preview.redd.it/ca9lk2xp2vic1.png?width=608&amp;format=png&amp;auto=webp&amp;s=f1c2af155b755ebc682892c7c481283e499617cd    ;由   提交/u/Low_Acanthisitta_272   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arxxxg/d_how_do_you_train_generative_ai_for_ui_design/</guid>
      <pubDate>Fri, 16 Feb 2024 02:49:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 3D 打印电子产品中的机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arxvbj/d_machine_learning_in_3d_printed_electronics/</link>
      <description><![CDATA[大家好。我正在开展一个项目，其中我们必须根据 CAD/PCB 建模工具中的尺寸、喷嘴温度、墨水粘度、床温度等输入来预测 3D 打印的预期参数。我正在寻找参考和资源以便实施类似的项目。如果您有任何进一步的信息或见解，将不胜感激。谢谢！！   由   提交/u/Background_Nature425  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arxvbj/d_machine_learning_in_3d_printed_electronics/</guid>
      <pubDate>Fri, 16 Feb 2024 02:45:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 作为世界模拟器的视频生成模型。开放AI Sora技术报告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</link>
      <description><![CDATA[报告 - https ://openai.com/research/video- Generation-models-as-world-simulators   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</guid>
      <pubDate>Fri, 16 Feb 2024 01:31:52 GMT</pubDate>
    </item>
    <item>
      <title>[N] Magika：人工智能驱动的快速高效的文件类型识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arvp5p/n_magika_ai_powered_fast_and_efficient_file_type/</link>
      <description><![CDATA[      我们非常高兴地宣布发布 Magika #AI 支持快速高效的文件类型识别库和工具 - https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-高效文件类型识别.html  ​ Magika CLI 实际使用 得益于其优化的 Keras 模型、大规模训练数据集和 Onnx Magika，其性能远超其他产品工具，即使在 CPU 上也非常快  Magika 性能与其他工具在包含 100 多种文件类型的 1M 评估文件数据集上的性能对比 Magika 代码和模型是 在 Github 上开源，并作为标准 pypi 包提供。我们还提供了一个实验性的 npm 包。我们希望您能与我们的团队一起发现 Magika 对您自己的项目有用。让我们知道您的想法！ ​   由   提交/u/ebursztein   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arvp5p/n_magika_ai_powered_fast_and_efficient_file_type/</guid>
      <pubDate>Fri, 16 Feb 2024 01:00:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] DeepRhythm - 快速、准确的节奏估计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arscov/p_deeprhythm_fast_accurate_tempo_estimation/</link>
      <description><![CDATA[最近，我需要一种方法来准确估计 Raspberry Pi 上音频文件的全局节奏。我尝试了 Librosa、Essentia 和 TempoCNN 的估计器，但所有（开源）方法都不可靠，而且非常非常慢。  所以，我做了一些研究，发现了这篇论文，描述了一个 CNN使用他们称为“Harmonic-Constant-Q-Modulation”的音频功能来预测节奏。简而言之，他们在 8 秒帧上执行一系列恒定 Q 变换，而不是提取“音高”频率，而是提取低得多的“节奏”频率（120bpm = 2Hz）。通过这种方法，CNN 几乎不需要做任何跑腿工作，它更多地充当过滤器来降低 HCQM 的维度，即它不需要学习起始模式，而只需要解释 bpm 频率本身的相对强度。  无论如何，我找不到该论文的任何源代码或任何 HCQM 实现，所以我构建了自己的代码。令我惊讶的是，它的效果非常。  我使用 pytorch 和 nnAudio 编写了 HCQM 实现，以便可以在 GPU 上运行它。每个步骤的内核/过滤器都是预先计算和重复使用的，并且经过一些仔细的扁平化和过滤。 reshape 可以在不到一秒的时间内一次性处理最多 100 首歌曲（在 4090 上）。  对于公共训练数据，没有太多。我使用了 GiantSteps、Ballroom、FMA 的一小部分以及我自己的音乐收藏。我通过我能找到的所有其他节奏预测器运行了整个集合，通过多数票选择了“真实的 bpm”，然后将置信度测量为每个预测器与“获胜者”之间的距离。这使我能够通过删除无法准确预测的多节奏/原声歌曲来清理数据集 CNN 本身非常简单，完全按照论文中描述的方式实现。它首先在全套上进行训练，然后在我的收藏上进行微调（主要是使用编程/循环鼓） 在我迄今为止的测试中，当两者都在 CPU 上运行时，它比 TempoCNN 快 10 倍。在完整的测试集上，它的正确率是 95.9%（目标的 2% 以内），而在我的集合上，它的正确率是 97.8%。据我所知，这明显更快并且更快。比任何公开的信息都更准确。  这是 代码，它可以在 pip 上使用，并且应该可以在支持 pytorch 的任何设备上运行（我已经在 Ubuntu、MacOS 和 Raspbian 上进行了测试）。    ;由   提交 /u/bleugre3n   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arscov/p_deeprhythm_fast_accurate_tempo_estimation/</guid>
      <pubDate>Thu, 15 Feb 2024 22:32:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于生成模型训练的自校正自消耗循环</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arqbud/r_selfcorrecting_selfconsuming_loops_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.07087 代码：https://github.com/nate-gillman/self- Correcting-self-消费 项目页面：https://cs.brown.edu/people/ngillman//sc-sc.html 摘要：  随着合成数据的质量越来越高并且在互联网上激增，机器学习模型越来越多地基于人类和机器生成的数据的混合进行训练。尽管使用合成数据进行表示学习有成功的故事，但使用合成数据进行生成模型训练会产生“自消耗循环”。除非满足某些条件，否则可能会导致训练不稳定甚至崩溃。我们的论文旨在稳定自消耗生成模型训练。我们的理论结果表明，通过引入理想化的校正函数，将数据点映射到更可能处于真实数据分布下的位置，可以使自消耗循环呈指数级地更加稳定。然后，我们提出了自我校正函数，它依赖于专业知识（例如在模拟器中编程的物理定律），旨在自动大规模地逼近理想化的校正器。我们凭经验验证了自校正自消耗循环在具有挑战性的人体运动合成任务中的有效性，并观察到它成功地避免了模型崩溃，即使合成数据与真实数据的比率高达 100%。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arqbud/r_selfcorrecting_selfconsuming_loops_for/</guid>
      <pubDate>Thu, 15 Feb 2024 21:10:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 激活的三个十年：神经网络 400 个激活函数的全面调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09092 摘要：  神经网络已被证明是解决复杂问题的高效工具生活的许多方面都存在问题。最近，随着深度学习的出现，它们的重要性和实际可用性进一步得到加强。神经网络成功的重要条件之一是选择合适的激活函数，将非线性引入模型。过去的文献中已经提出了许多类型的这些函数，但没有一个综合来源包含它们的详尽概述。即使根据我们的经验，缺乏这种概述也会导致冗余和无意中重新发现已经存在的激活函数。为了弥补这一差距，我们的论文提出了一项涉及 400 个激活函数的广泛调查，其规模比以前的调查大几倍。我们的综合汇编也参考了这些调查；然而，其主要目标是提供先前发布的激活函数的最全面的概述和系统化，并提供其原始来源的链接。第二个目标是更新当前对这一系列函数的理解。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</guid>
      <pubDate>Thu, 15 Feb 2024 20:12:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI Sora Video Gen——如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</link>
      <description><![CDATA[ 介绍 Sora，我们的文本转视频模型。 Sora 可以生成长达一分钟的视频，同时保持视觉质量并遵守用户的提示。  https:/ /openai.com/sora 研究笔记 Sora 是一种扩散模型，它从看起来像静态噪声的视频开始生成视频，然后通过多个步骤消除噪声来逐渐对其进行转换. Sora 能够一次生成整个视频或扩展生成的视频以使其更长。通过一次为多个帧提供模型预测，我们解决了一个具有挑战性的问题，即确保对象即使暂时离开视野也保持不变。 与 GPT 模型类似，Sora 使用变压器架构，释放卓越的扩展性能。 我们将视频和图像表示为称为补丁的较小数据单元的集合，每个补丁类似于 GPT 中的令牌。通过统一我们表示数据的方式，我们可以在比以前更广泛的视觉数据上训练扩散变换器，涵盖不同的持续时间、分辨率和纵横比。 Sora 建立在 DALL·E 和 DALL·E 过去的研究基础上GPT 模型。它使用 DALL·E 3 的重述技术，该技术涉及为视觉训练数据生成高度描述性的标题。因此，该模型能够更忠实地遵循生成视频中用户的文本指令。 除了能够仅根据文本指令生成视频之外，该模型还能够采用现有的静态图像并从中生成视频，精确地动画图像内容并关注小细节。该模型还可以获取现有视频并对其进行扩展或填充缺失的帧。在我们的技术论文（今天晚些时候发布）中了解更多信息。 Sora 是能够理解和模拟现实世界的模型的基础，我们相信这一功能将成为实现 AGI 的重要里程碑。 Sora 是能够理解和模拟现实世界的模型的基础。 p&gt; 示例视频：https://cdn.openai.com/sora/videos/ cat-on-bed.mp4 技术论文将于今天晚些时候发布。但是如何进行头脑风暴呢？   由   提交/u/htrp  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</guid>
      <pubDate>Thu, 15 Feb 2024 18:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1M/10M token上下文窗口怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</link>
      <description><![CDATA[是否会启动社区头脑风暴主题？ - 人们是否认为 RingAttention 可以充分扩展？参见https://largeworldmodel.github.io - 它是用 1M 还是 10Mn 令牌窗口进行训练的，这对我来说似乎不清楚？他们是否在没有经过某种训练的情况下从 1M-&gt;10M 进行概括？ - 存在哪些数据集可以训练 10M 文本标记窗口？ - 在这么长的背景下你如何做 RLHF？ 1M 文本 ~ 4M 字符 ~ 272k 秒阅读时间（根据 Google 假设 68 毫秒/字符）~ 阅读一个示例需要 75 小时？ 编辑：当然 lucidrains 已经在着手实施 RingAttention！ (https://github.com/lucidrains/ring-attention-pytorch)   由   提交 /u/gggerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</guid>
      <pubDate>Thu, 15 Feb 2024 16:13:29 GMT</pubDate>
    </item>
    <item>
      <title>[N] Gemini 1.5，具有 1M 上下文长度令牌的 MoE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</link>
      <description><![CDATA[https://blog.google/technology/ai/google-gemini-next- Generation-model-february-2024/  &amp;# 32；由   提交/u/Electronic-Author-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</guid>
      <pubDate>Thu, 15 Feb 2024 15:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>