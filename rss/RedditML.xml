<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sun, 02 Feb 2025 12:28:06 GMT</lastBuildDate>
    <item>
      <title>[新闻] TMLR 已获 Scopus 索引批准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ift104/news_tmlr_was_approved_for_indexing_in_scopus/</link>
      <description><![CDATA[ 2024 TMLR 年度报告 - Google Docs 2025 年 1 月 14 日，TMLR 获准在 Scopus 中编入索引。2025 年 1 月 15 日，TMLR 获准在 DOAJ 中编入索引。  在这里发布这个消息是因为我还没有在任何地方看到这个消息。对于欧洲和南美的 ML 研究人员/博士来说，这是一个好消息，因为许多大学只承认 Scopus 索引的论文。   由    /u/OkTaro9295  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ift104/news_tmlr_was_approved_for_indexing_in_scopus/</guid>
      <pubDate>Sun, 02 Feb 2025 08:40:53 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 教人工智能思考，却不知道思考是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifsgkc/d_r_teaching_ai_to_think_without_knowing_what/</link>
      <description><![CDATA[人工智能在模仿人类行为方面取得了巨大进步，但它仍然缺乏决策和解决问题背后的真正思维过程。如果我们不复制神经活动，而是使用文本、语音、多模态数据和脑电图信号，根据人类思维的结果（决策、解决方案和行动）训练人工智能，结果会怎样？ 我们的方法旨在教会人工智能我们如何思考，而不仅仅是我们做什么，从而弥合模式识别和真正的认知模拟之间的差距。这可能会彻底改变人工智能中的问题解决方式。 📄 阅读论文：github.com/abhijayhm/ThoughtMimickingModel 您对人工智能从人类决策而不是仅仅从数据模式中学习有何看法？ #AI #MachineLearning #CognitiveAI #Neuroscience #EEG    提交人    /u/Ok-Imagination-6578   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifsgkc/d_r_teaching_ai_to_think_without_knowing_what/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:23 GMT</pubDate>
    </item>
    <item>
      <title>需要一个类似于 GazeCapture 但更小/下载速度更快的数据集[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifq0fn/need_a_dataset_similar_to_gazecapture_but_smaller/</link>
      <description><![CDATA[无法下载 GazeCapture 数据集，因为它太慢，无法恢复下载。备用数据集/同一数据集的较小版本将非常有帮助    提交人    /u/Mindless-Ad-6767   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifq0fn/need_a_dataset_similar_to_gazecapture_but_smaller/</guid>
      <pubDate>Sun, 02 Feb 2025 05:16:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 聊天机器人软件开始面临根本性的限制 | Quanta Magazine</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifoq0z/r_chatbot_software_begins_to_face_fundamental/</link>
      <description><![CDATA[        由    /u/we_are_mammals  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifoq0z/r_chatbot_software_begins_to_face_fundamental/</guid>
      <pubDate>Sun, 02 Feb 2025 04:01:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Feb 2025 03:15:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 QLoRA 正确计算 NF4（NormalFloat4）的 16 个量化级别？[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifjxhx/how_to_correctly_compute_the_16_quantization/</link>
      <description><![CDATA[大家好， 我正在尝试正确实现 QLoRA 论文中描述的 NF4 (NormalFloat4) 量化级别，但我的计算值与预期值之间存在差异。 论文指出：  对于任意标准差 𝜎 在 [−1,1] 范围内的零均值正态分布，理论上最优数据类型的信息计算如下： (1) 估计理论 N(0,1) 分布的 2^𝑘+1 分位数，以获得正态分布的 k 位分位数量化数据类型， (2) 采用此数据类型并进行归一化将其值调整到 [−1,1] 范围内， (3) 通过绝对最大值重新缩放将输入权重张量标准化到 [−1,1] 范围内，从而对其进行量化。  首先，疑问是理论 N(0,1) 的 2^𝑘+1 分位数在两端都包含无穷大；我如何将它们标准化为 [-1, 1]？此外，关于 NF4 数据类型的量化级别/值，它们是相邻分位数的中点吗？还是相邻分位数之间的一个点，使得两个分割具有相同数量的权重？ 一旦我理解了这些，也许我的其他疑问就会解决。    提交人    /u/Sudden-Yoghurt526   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifjxhx/how_to_correctly_compute_the_16_quantization/</guid>
      <pubDate>Sat, 01 Feb 2025 23:49:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年最佳 NLP 论文视频合集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifig7s/d_a_video_compilation_of_the_best_nlp_papers_from/</link>
      <description><![CDATA[      分享 2024 年最好的 NLP 研究论文，涵盖我认为最有趣的 15 篇论文。     提交人    /u/AvvYaa   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifig7s/d_a_video_compilation_of_the_best_nlp_papers_from/</guid>
      <pubDate>Sat, 01 Feb 2025 22:39:35 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] Tulu 3 模型表现优于 4o 和 Deepseek？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifetmm/news_tulu_3_model_performing_better_than_4o_and/</link>
      <description><![CDATA[有人用过艾伦人工智能研究所周四发布的这个模型吗？它似乎在很多方面都胜过 4o 和 DeepSeek，但不知为何，几乎没有报道。有什么想法吗？ https://www.marktechpost.com/2025/01/31/the-allen-institute-for-ai-ai2-releases-tulu-3-405b-scaling-open-weight-post-training-with-reinforcement-learning-from-verifiable-rewards-rlvr-to-surpass-deepseek-v3-and-gpt-4o-in-key-benchmarks/    由    /u/joshkmartinez 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifetmm/news_tulu_3_model_performing_better_than_4o_and/</guid>
      <pubDate>Sat, 01 Feb 2025 19:57:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于收听研究论文的新网站/应用程序：Paper2Audio.com</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifd3xv/p_new_siteapp_for_listening_to_research_papers/</link>
      <description><![CDATA[tl;dr 使用 Paper2Audio.com 收听研究论文，或直接发私信给我以访问我们的 iOS 测试版应用。 我们已经建立了一个网站和一个 iOS 测试版应用，用于收听研究论文！查看 Paper2Audio.com 或联系我们以访问 iOS 测试版。 有三种收听模式：  全文 – 阅读整篇论文，包括汇总表格、图表和代码块。 简短摘要 – 将论文浓缩为约 5 分钟的音频摘要。 长摘要 – 提供更详细的摘要，大约是原始论文长度的三分之一。  这些模式均不模拟播客。您只需上传 PDF 即可获得论文的音频版本。目前，它对用户完全免费。 我一直在使用 Paper2Audio 收听论文，主要是关于视觉语言模型的论文，最新的 LLM 论文如 Deepseek R1，这些论文帮助我们改进了我们的服务。我也是一名经济学家，所以我一直在用 Paper2Audio 了解经济学论文。 欢迎提出问题和反馈！    提交人    /u/goldenjm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifd3xv/p_new_siteapp_for_listening_to_research_papers/</guid>
      <pubDate>Sat, 01 Feb 2025 18:42:17 GMT</pubDate>
    </item>
    <item>
      <title>[D]目前最好的语音识别模型是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifbd48/dwhat_is_the_best_speech_recognition_model_now/</link>
      <description><![CDATA[OpenAI 的 Whisper 发布已经两年多了，自那以后似乎没有其他模型能够真正挑战它的地位。虽然 Whisper 一直在不断更新，但它在英语以外的语言（例如中文）中的表现对我来说并不理想。我正在寻找一种替代模型来为视频生成字幕和为直播生成实时字幕。 我也尝试过阿里巴巴的 FunASR，但它也是一年多前发布的，似乎没有提供令人满意的性能。 我知道一些基于 LLM 的语音模型，但它们的硬件要求对我的用例来说太高了。 在其他 AI 领域，几乎每个月都会发布新的模型，但似乎对语音识别的进步关注较少。有什么最近的模型值得研究吗？    提交人    /u/kir_aru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifbd48/dwhat_is_the_best_speech_recognition_model_now/</guid>
      <pubDate>Sat, 01 Feb 2025 17:28:03 GMT</pubDate>
    </item>
    <item>
      <title>[2412.20302] EXAdam：自适应交叉矩的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifag0a/241220302_exadam_the_power_of_adaptive/</link>
      <description><![CDATA[  由    /u/AhmedMostafa16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifag0a/241220302_exadam_the_power_of_adaptive/</guid>
      <pubDate>Sat, 01 Feb 2025 16:48:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 句子分类和自定义实体识别用于信息提取——这种方法有效吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1if4jn8/d_sentence_classification_and_custom_entity/</link>
      <description><![CDATA[      https://preview.redd.it/yt2v9o0klige1.png?width=1901&amp;format=png&amp;auto=webp&amp;s=e5efc6e6a03ce5210ca807b620b0886eb3c598bb 我正在从不遵循一致模板的 HTML 文档中提取财务实体（例如 EPS、收入）。我不想攻读 LLM（RAG）。 我正在考虑以下方法：  使用自定义解析器解析 HTML，以在添加分隔符的同时维护表格结构。 逐行或逐句对提取的文本进行分类。 对分类后的文本执行 NER 以提取相关值。  目标是实现最大准确度和低延迟。这种方法看起来可行吗？是否有我应该考虑的优化或替代方法？    提交人    /u/akfea   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1if4jn8/d_sentence_classification_and_custom_entity/</guid>
      <pubDate>Sat, 01 Feb 2025 11:39:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 分子指纹是肽功能预测的有力模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1if2wlc/r_molecular_fingerprints_are_strong_models_for/</link>
      <description><![CDATA[TL;DR 我们表明分子指纹为肽分类提供了 SOTA 结果，而长距离图基准 (LRGB) 实际上没有长距离依赖性 ArXiv：https://arxiv.org/abs/2501.17901 摘要：  我们研究了分子指纹对肽属性预测的有效性，并证明从分子图中提取领域特定特征可以胜过复杂且计算成本高昂的模型，例如 GNN、预训练的基于序列的变换器和多模态集成，即使没有超参数调整。为此，我们对 126 个数据集进行了全面评估，在 LRGB 和其他 5 个肽功能预测基准上取得了最先进的结果。我们表明，基于 ECFP、拓扑扭转和 RDKit 分子指纹的计数变体以及 LightGBM 作为分类头的模型非常稳健。分子指纹本质上是非常短程的特征编码器，其强大的性能挑战了肽中长程相互作用的假定重要性。我们的结论是，对于较大的分子（例如肽），使用分子指纹可以成为复杂深度学习模型的一种计算上可行、参数少且用途广泛的替代方案。  主要贡献：  分子指纹是分子图上的一种简单特征提取，对肽非常有效 它们在 LRGB 上获得 SOTA 结果，同时是极短程的描述符，并且与它确实需要长程依赖性的说法相矛盾  第一个更面向生物信息学，但第二个与 GNN 评估方法非常相关。大多数设计能够学习节点之间长距离关系的 GNN 的论文都是在 LRGB 上进行评估的。但似乎并没有真正做到这一点，因此这里的任何结论都可能是 a) 虚假相关性 b) 他们正在学习一些有趣的东西，但不是真正的长距离关系。有趣的是，LRGB 的原始审阅者也有同样的疑问（https://openreview.net/forum?id=in7XC5RcjEn）。    提交人    /u/qalis   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1if2wlc/r_molecular_fingerprints_are_strong_models_for/</guid>
      <pubDate>Sat, 01 Feb 2025 09:37:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] DeepSeek？Schmidhuber 是第一个这么做的。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ielwh5/d_deepseek_schmidhuber_did_it_first/</link>
      <description><![CDATA[        提交人    /u/SirSourPuss   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ielwh5/d_deepseek_schmidhuber_did_it_first/</guid>
      <pubDate>Fri, 31 Jan 2025 18:41:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>