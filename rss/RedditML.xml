<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Mon, 12 Aug 2024 03:17:22 GMT</lastBuildDate>
    <item>
      <title>[D] 关于撰写基准论文的优点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eq22hg/d_pros_about_writing_a_benchmark_paper/</link>
      <description><![CDATA[作为一个从未写过论文提出基准的人，我只能想象写一篇论文会有什么见解/收获。也许发现模型的底层性能就是其中之一。对于那些写过类似文章的人，你觉得你的经验有价值吗？你会推荐吗？    提交人    /u/Haunting_Air3071   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eq22hg/d_pros_about_writing_a_benchmark_paper/</guid>
      <pubDate>Mon, 12 Aug 2024 02:08:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 低数据扩散的架构，但 PCA 也表明需要 5%-10% 的 PCA 组件特征数量才能解释 80-90% 的方差？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epzir5/d_architecture_for_diffusion_for_low_data_but_pca/</link>
      <description><![CDATA[我有一个数据集，相当少。2K 个样本。这些不是图像，而是集合数据。每个样本都是一组值 V，每组/样本大约有 50,000 个。 当我执行 PCA 时，我看到 numV 成分的 10% 解释了 90% 左右的方差。 我尝试了一个简单的 Transformer 块 x 3，以及一些跳过/规范。并训练了反向扩散。低点很低。但是当我生成数据时，损失（MSE）很低，但绝对皮尔逊相关系数很低，所以它不能很好地捕捉到起伏。 你们有什么建议吗？对于神经网络的架构或数据表示等 编辑：我还要补充一点，Transformer 在计算和存储大型注意矩阵时非常痛苦 整个数据集的形状为 (2000, 50_000)    提交人    /u/MysticalDragoneer   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epzir5/d_architecture_for_diffusion_for_low_data_but_pca/</guid>
      <pubDate>Mon, 12 Aug 2024 00:04:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] Vision Transformer + 聊天机器人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epz92g/p_vision_transformer_chatbot/</link>
      <description><![CDATA[假设我想在某些图像数据和注释上训练一个预先训练过的 Vision Transformer/CLIP 模型，并将其实现为聊天机器人，基本上像 ChatGPT4.0，我该怎么做？我应该提到，这些图像的注释将是单词/双词，但我的要求包括，鉴于 Transformer 已在其他图像数据上进行过预先训练并且具有进行对话的能力，因此它在扫描图像后可以给出 4-5 行文本。我有计算机视觉方面的经验和法学硕士的基本经验，但这对我来说绝对是另一个级别。    提交人    /u/DaTrollFace   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epz92g/p_vision_transformer_chatbot/</guid>
      <pubDate>Sun, 11 Aug 2024 23:52:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研讨会：可扩展的无 MatMul 语言建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epz1zr/r_workshop_scalable_matmulfree_language_modeling/</link>
      <description><![CDATA[摘要  矩阵乘法 (MatMul) 通常占据大型语言模型 (LLM) 总体计算成本的主导地位。随着 LLM 扩展到更大的嵌入维度和上下文长度，此成本只会增加。在这项工作中，我们表明 MatMul 操作可以完全从 LLM 中消除，同时在十亿参数规模下保持强劲性能。我们的实验表明，我们提出的无 MatMul 模型实现了与最先进的 Transformers 相当的性能，后者在推理期间需要更多的内存，规模至少达到 2.7B 参数。我们研究了缩放规律，发现我们的无 MatMul 模型和全精度 Transformers 之间的性能差距随着模型尺寸的增加而缩小。我们还提供了此模型的 GPU 高效实现，与未优化的基线相比，在训练期间可将内存使用量降低高达 61%。通过在推理过程中使用优化的内核，与未优化的模型相比，我们的模型的内存消耗可以减少 10 倍以上。为了正确量化我们架构的效率，我们在 FPGA 上构建了一个自定义硬件解决方案，该解决方案利用了 GPU 无法处理的轻量级操作。我们以 13W 的功耗处理了十亿参数规模的模型，超出了人类可读的吞吐量，使 LLM 更接近类似大脑的效率。这项工作不仅展示了 LLM 在保持有效运行的情况下可以剥离到何种程度，而且还指出了未来加速器在处理下一代轻量级 LLM 时应该优化的操作类型。  论文： 链接到 arXiv 仓库： 链接到 GitHub 研讨会 大家好，我们将与最近的论文“可扩展的无 MatMul 语言建模”的第一作者一起举办一个免费的在线研讨会！Ridger Zhu 将介绍一些关于论文和研究领域的幕后花絮，以及一些关于实施细节的说明。我们希望将人数控制在较小规模，以便 Ridger 有机会回答每个人的问题，因此将按照先到先得的原则进行，容量有限。 时间： 8 月 15 日下午 6 点（太平洋夏令时间） 地点： 在线（通过 Zoom） 加入候补名单： 在此处注册（我们将在临近日期时发送包含会议详细信息的电子邮件）    提交人    /u/Dankiest_Of_Memes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epz1zr/r_workshop_scalable_matmulfree_language_modeling/</guid>
      <pubDate>Sun, 11 Aug 2024 23:43:05 GMT</pubDate>
    </item>
    <item>
      <title>付费 API 与本地机器 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epyta8/paid_apis_vs_local_machine_d/</link>
      <description><![CDATA[我正在考虑构建一台带有几个高端 GPU 的计算机，用于在本地运行 ML 模型（可能是 ollama 和开放 WebUI）。 但是，我已经计算了机器构建的数字，现在想知道是否应该将这笔钱花在支付 API 上。两个 RTX 4090 24GB 将花费我 5000 美元。我将这些 GPU 的相关使用寿命定为 5 年。这大约是每月 85 美元的 API 代币。 哪个会更好？ 有人解决了这个问题吗？    提交人    /u/mikedensem   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epyta8/paid_apis_vs_local_machine_d/</guid>
      <pubDate>Sun, 11 Aug 2024 23:32:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要有关使用机密数据在本地微调 LLM 以及创建 PDF 模板的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eps6k5/d_need_advice_on_finetuning_llms_locally_with/</link>
      <description><![CDATA[大家好！我计划在本地机器上使用 Llama 3.1 70B 模型，使用公司的一些机密数据对其进行微调。我担心的是，尽管我将在本地运行该模型，但这些数据是否可能会泄露。 此外，您能否推荐一些比 Llama 3.1 70B 更好的 LLM？ 就上下文而言，我有一个大约 9 个 PDF 的数据集（我知道这不是很多），其中包含表格和文本的混合。当我微调模型时，我需要它以 PDF 格式生成模板，仅关注表格和标题。由于我对此还很陌生，因此我非常感谢任何有关如何准备数据集以及我的下一步应该做什么的建议。谢谢！    提交人    /u/thepotentio_reddy09   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eps6k5/d_need_advice_on_finetuning_llms_locally_with/</guid>
      <pubDate>Sun, 11 Aug 2024 18:45:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持有效，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢所有人在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Aug 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] onnx 模型转换的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epgi4u/discussion_resources_for_onnx_model_conversion/</link>
      <description><![CDATA[过去六个月我一直在从事一个基于音频的项目，主要使用 TensorFlow，因为需要使用 TensorFlow Lite (TFLite) 部署模型。但是，我在基于音频的增强方面遇到了 TensorFlow 的限制，例如音调变换、房间脉冲响应 (RIR) 和 SpecAugment。相比之下，PyTorch 为这些任务提供了一套更丰富的工具，使其更适合我的项目需求。 鉴于此，我正在考虑切换到 PyTorch。但是，我仍然需要将 PyTorch 模型转换为 TensorFlow 模型以进行部署。在研究过程中，我发现 ONNX 是一种流行的转换方法。但是，似乎 PyTorch 模型需要以特定方式构建才能在转换后与 TensorFlow 兼容。 是否有人有关于如何构建 PyTorch 模型以进行 ONNX 转换的指南，或者知道更灵活的转换技术？ TL;DR：我正在使用 TensorFlow 进行 TFLite 部署的音频项目，但由于 PyTorch 具有出色的音频增强工具，因此正在考虑切换到 PyTorch。我需要将 PyTorch 模型转换为 TensorFlow，并正在寻找有关使用 ONNX 进行此或任何其他灵活转换方法的指导。    提交人    /u/JournalistCritical32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epgi4u/discussion_resources_for_onnx_model_conversion/</guid>
      <pubDate>Sun, 11 Aug 2024 09:07:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从 Scratch 开始的 Vison 语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epga39/p_vison_language_models_from_scratch/</link>
      <description><![CDATA[        提交人    /u/themathstudent   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epga39/p_vison_language_models_from_scratch/</guid>
      <pubDate>Sun, 11 Aug 2024 08:52:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 树注意力：GPU 集群上长上下文注意力的拓扑感知解码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep9qpa/r_tree_attention_topologyaware_decoding_for/</link>
      <description><![CDATA[一篇新的研究论文介绍了一种树注意力算法，用于在多个 GPU 上并行化注意力计算，利用 logsumexp 和 max 运算的关联属性将约简结构化为树。 树注意力算法使跨设备解码能够比 Ring Attention 等替代方法渐近地更快地执行（最高可快 8 倍），同时还需要显著减少通信量并将峰值内存减少 2 倍。    提交人    /u/AhmedMostafa16   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep9qpa/r_tree_attention_topologyaware_decoding_for/</guid>
      <pubDate>Sun, 11 Aug 2024 02:17:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 寻找梯度下降方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep5od0/p_looking_for_a_gradient_descent_approach/</link>
      <description><![CDATA[我有一个梯度下降方法的想法，它试图直接“跳”到附近最小值的（预测）位置。它的工作原理是围绕一个点近似 2-5 阶泰勒多项式，然后求解最小值（如果可能）并将该点设置为新的 x。然后，可以重复该过程。如果任何一点的泰勒多项式都是凹的，那么我们可以使用更标准的梯度下降方法。 这似乎是一种相当简单的方法，所以我怀疑它是否新颖，但我在网上找不到类似的东西。有谁知道这种方法叫什么或者是否已经研究过它？ 我受到了牛顿求根方法的启发，并且对超参数调整有点不屑一顾。 以下是 desmos 对二次和三次泰勒近似的演示： 二次下降：https://www.desmos.com/calculator/i2nsjaxzhy 三次下降：https://www.desmos.com/calculator/kgkbcfdn7t    提交人    /u/IgorTheMad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep5od0/p_looking_for_a_gradient_descent_approach/</guid>
      <pubDate>Sat, 10 Aug 2024 22:52:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] WildHallucinations：使用真实世界实体查询评估法学硕士 (LLM) 中的长篇事实性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep4tn3/r_wildhallucinations_evaluating_longform/</link>
      <description><![CDATA[一篇新论文旨在创建一个现实的基准 WildHallucinations，用于评估 LLM 事实性。    提交人    /u/AhmedMostafa16   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep4tn3/r_wildhallucinations_evaluating_longform/</guid>
      <pubDate>Sat, 10 Aug 2024 22:12:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 LSTM 建模动态系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep3kra/d_modeling_a_dynamic_system_using_lstm/</link>
      <description><![CDATA[      亲爱的大家， 在观看了这段制作精良的视频后，我决定使用相同的概念对我的真实系统进行建模。基本上，我想对我的真实机器人的动态进行建模，以便创建相同的“数字孪生”。换句话说，我想在模拟器中重新创建具有虚拟物理属性的相同机器人，并像真实机器人一样移动它。 我的机器人使用操纵杆驱动，操纵杆在每个轴上输出一个介于 -1.0 和 1.0 之间的浮点数。我收集了数据（真正的机器人帽子传感器已经在工作并实施）。为简单起见，假设我想通过从左向右移动操纵杆轴来驱动以下关节坐标（图 1）。 图 1 我收集了一个小时的数据，然后使用以下数据训练了一个隐藏大小为 32 的 LSTM：  输入是操纵杆输入和关节坐标（机器人的状态）的连接 目标由下一步中机器人的状态表示。我只是复制了状态的列并将其向后移动一个单位。图 2 显示的可能比 1000 个单词更好。  图 2 然后我创建了长度为 200 的序列并训练了我的 LSTM。 训练收敛得非常快，我对结果非常满意。但不知何故，虚拟机器人在虚拟环境中反应奇怪。它以惊人的速度从一个位置跳到另一个位置，然后移动得非常慢。因此，它不会像真正的机器人那样做出反应（真正的机器人在运动过程中更平稳）。 在这种问题中我是否遗漏了重要的东西？ 为了创建真实机器人的良好数字孪生，我还应该考虑什么？ 请注意：  尽管有上述示例，但我将所有运动标准化为范围 [-1, 1] 或 [0, 1] 所有数据均使用以太网电缆收集（因此不会因无线通信等而造成延迟） 我使用了 PyTorch 的 LSTM 类，而不是自定义实现 通过生成具有不同频率的正弦波输入并覆盖关节的所有范围来收集数据。 对于训练，我对数据进行了打乱：随机选择一个起始索引，并剪切一个包含 200 个元素的序列并用于训练。     由   提交  /u/WilhelmRedemption   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep3kra/d_modeling_a_dynamic_system_using_lstm/</guid>
      <pubDate>Sat, 10 Aug 2024 21:15:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你的 neurips 讨论阶段进行得怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eowx75/d_how_is_your_neurips_discussion_period_going/</link>
      <description><![CDATA[你的 neurips 讨论期进行得怎么样？ 有什么有趣的轶事吗？    提交人    /u/SuchOccasion457   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eowx75/d_how_is_your_neurips_discussion_period_going/</guid>
      <pubDate>Sat, 10 Aug 2024 16:20:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>