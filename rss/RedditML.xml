<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 26 Apr 2024 15:14:45 GMT</lastBuildDate>
    <item>
      <title>[D] 压倒性的 LLM 释放率：寻求构建评估 LLM 测试集的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdn7da/d_overwhelming_llm_release_rate_seeking/</link>
      <description><![CDATA[大家好， 我正在尝试构建自己的测试集，以便对巨大的数据进行初步快速评估每周都会在huggingface.co上弹出的模型数量，我正在寻找一个起点或建议。 如果有人愿意分享一些他们用来测试LLM能力的问题，即使是高-水平概念，或者只是给我一些提示或建议，我真的很感激！ 提前感谢大家的任何回复。”   由   提交 /u/Distinct-Target7503    reddit.com/r/MachineLearning/comments/1cdn7da/d_overwhelming_llm_release_rate_seeking/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdn7da/d_overwhelming_llm_release_rate_seeking/</guid>
      <pubDate>Fri, 26 Apr 2024 14:43:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过回归相对奖励进行强化学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdl0em/r_reinforcement_learning_via_regressing_relative/</link>
      <description><![CDATA[https://arxiv.org/abs/2404.16767  新的深度 RL 算法，适用于语言模型和扩散模型。   由   提交/u/athens117  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdl0em/r_reinforcement_learning_via_regressing_relative/</guid>
      <pubDate>Fri, 26 Apr 2024 13:10:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 清理字幕数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdimby/d_clean_caption_dataset/</link>
      <description><![CDATA[我正在尝试从头开始训练 CLIP。然而，缺乏可用的数据集。看起来相当多样化且干净的一个数据集似乎已被删除 (laion-400m)。看看 HF 数据集，这两个数据集很有前途，但想知道是否有更好/更干净的数据集。 - 概念性标题：使用替代文本。 - red_caps：reddit 线程，但这些大多是图像上的第一个评论，而不是实际的标题。 TIA   由   提交/u/themathstudent  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdimby/d_clean_caption_dataset/</guid>
      <pubDate>Fri, 26 Apr 2024 11:09:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士：为什么情境学习有效？从技术角度来看到底发生了什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/</link>
      <description><![CDATA[在我寻找这个问题的答案时，得到的答案只不过是将模型拟人化而已。他们总是提出这样的主张：  如果没有示例，模型必须推断上下文并依靠其知识来推断出预期的结果。这可能会导致误解。 一次性提示通过提供具体示例来减轻这种认知负担，有助于锚定模型的解释并专注于具有更清晰期望的更狭窄的任务。  该示例充当模型的参考或提示，帮助其理解您正在寻求的响应类型并在训练期间触发对类似实例的记忆。&lt; /p&gt; 提供示例允许模型识别要复制的模式或结构。它为模型建立了一个对齐线索，减少了零样本场景中固有的猜测。  顺便说一句，这些是真实的摘录。 但这些模型不“理解”任何东西。他们不“推断”，或“解释”，或“聚焦”，或“记住训练”，或“猜测”，或有字面上的“认知负荷”。它们只是统计令牌生成器。因此，当寻求对上下文学习提高准确性的确切机制的具体理解时，像这样的流行科学解释是毫无意义的。 有人可以提供一个根据实际模型来解释事物的解释吗？架构/机制以及提供额外上下文如何带来更好的输出？我可以“说说而已”，所以请不遗漏任何技术细节。 我可以做出有根据的猜测 - 在输入中包含示例，这些示例使用与您想要的输出类型近似的标记，从而引导注意力机制，并且最终的密集层对更高的令牌进行加权，这些令牌在某种程度上与这些示例相似，从而增加了在每个生成步骤中对这些所需令牌进行采样的几率；就像从根本上讲，我猜测相似性/距离的事情，其中​​明确举例说明我想要的输出会增加获得的输出与其相似的可能性 - 但我更愿意从对这些模型有深入了解的其他人那里听到它   由   提交 /u/synthphreak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/</guid>
      <pubDate>Fri, 26 Apr 2024 11:01:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关键批量大小和法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdgxit/d_critical_batch_size_and_llms/</link>
      <description><![CDATA[在有关“小指南”的视频中到 2024 年构建大型语言模型” 41:38 作者开始讨论批量大小的限制。 ​  好吧，如果你当批量大小开始非常大时，每个优化步骤的模型都会降低每个令牌的使用效率，因为批量大小太大，以至于每个令牌在优化步骤中都会被淘汰。粗略地说，衡量这个限制有点困难，我们称之为临界批量大小。  我认为较大的批量大小对于训练 LLM 总是更好，因为： p&gt;  它更好地近似真实梯度。 我们更快地浏览数据集。 据我所知，限制仅在于基础设施、硬件、通信开销等.  我发现一篇论文介绍了“临界批量大小”概念 - 大批量训练的经验模型。它主要讨论大批量数据并行的速度/效率权衡。另一篇被高度引用的论文神经语言模型的缩放定律：  在临界批量大小下进行训练提供了时间和计算效率之间的大致最佳折衷  所以我不太明白视频作者的意思：  每个令牌是在优化步骤中有点被淘汰  除了基础设施、硬件或实现限制之外，大批量还有其他问题吗？   由   提交/u/kiockete  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdgxit/d_critical_batch_size_and_llms/</guid>
      <pubDate>Fri, 26 Apr 2024 09:21:42 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 时间序列回归问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdgxfk/discussion_time_series_regression_problem/</link>
      <description><![CDATA[      大家好，我有一个问题，我不确定什么是最好的方法（我真的找不到任何相关文献）。我有一个传感器值的小数据集（约 100 个测量值，如附加的测量值），我想预测某个相关事件。这里 t_0 是我想要预测的相关时刻。问题是，我需要在事件发生时触发一些东西。如果我在达到事件后需要太长时间才能触发，这不会是一个积极的结果。我最初的想法是基本上对事件之前的时间序列进行分块，并尝试从该片段预测到达事件之前的剩余时间。当它低于阈值时，我可以触发我的操作。我想看看例如XGBoost 并向其提供时间序列的小块并连续运行该过程。我不太确定这是否是正确的方法。这是一个已知问题吗？这个问题搜索文献时用什么名字比较好？您有如何解决这个问题的建议吗？ 谢谢。 https://preview.redd.it/l59gsmswkswc1.png?width=1303&amp;format=png&amp;auto=webp&amp;s=f9e83d7b87ce5227378de6a080 5a916fd4f93314   由   提交/u/seboz12345   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdgxfk/discussion_time_series_regression_problem/</guid>
      <pubDate>Fri, 26 Apr 2024 09:21:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 文本到语音合成的最新技术是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdcbmx/d_what_is_the_state_of_art_for_text_to_speech/</link>
      <description><![CDATA[我开始为我的毕业做一些研究，我正在寻找一些关于文本到语音合成的论文。我正在对一篇我发现很有趣的论文进行一些复制，该论文名为“Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Prediction”。基本上，它是一个接收文本、将其转换为频谱图的模型，并且该频谱图用于构建音频文件。由于我仍处于复制的开始阶段，你们有推荐研究的论文吗？您从事过语音合成 (TTS) 工作吗？我应该研究哪些好的参考文献？ ​ 我在这里看到了这篇文章https://www.reddit.com/r/MachineLearning/comments/nxkuvn/d_what_is_actually_the_state_of_the_art_in_text/ 但是已经有3年了。也许有比 FastSpeech2 更新的东西？ ​   由   提交/u/Zelun  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdcbmx/d_what_is_the_state_of_art_for_text_to_speech/</guid>
      <pubDate>Fri, 26 Apr 2024 04:24:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 元学习 vs 联邦学习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdbq5t/d_metalearning_vs_federated_learning/</link>
      <description><![CDATA[[D] 大家好，对于深入研究当今热门话题的更好选择和最有效方法，您有什么建议吗？&lt; br /&gt; 我偶然发现了联邦学习的存储库： ​  https://github.com/muditbhargava66/dropgrad https://github.com/ adap/flower  但似乎找不到类似元学习的东西。任何有关如何选择我的博士主题的建议将不胜感激！   由   提交/u/Tight_Confusion_1695   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdbq5t/d_metalearning_vs_federated_learning/</guid>
      <pubDate>Fri, 26 Apr 2024 03:53:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 多头专家混合 - https://arxiv.org/pdf/2404.15045 中建议的密集子代币路由的实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</link>
      <description><![CDATA[我的朋友在这篇 arxiv 论文中实现了 Multihead Mixture of Experts 的方法 https://arxiv.org/pdf/2404.15045 他希望我与你分享！ https://github.com/lhallee/Multi_Head_Mixture_of_Experts__MH-MOE 尝试一下。让我知道您的想法，我会将其传递给他。   由   提交/u/Prudent_Student2839   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</guid>
      <pubDate>Thu, 25 Apr 2024 22:00:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] HyenaDNA 和 Mamba 不擅长顺序标记？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</link>
      <description><![CDATA[大家好，我一直在研究使用 DNA 序列作为输入的顺序标记。最近发布了 2 个基础模型 HyenaDNA（基于 Hyena Operator）和 Caduceus（基于 mamba），我使用了预训练模型和从头开始的模型，即使使用预训练模型，性能也很糟糕。  有人有此类模型的经验吗？性能下降的潜在原因是什么？我在少数群体中的成绩实际上为零？曼巴在阶级不平衡问题上处理得不好吗？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</guid>
      <pubDate>Thu, 25 Apr 2024 20:02:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于图的神经网络的药物毒性预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</link>
      <description><![CDATA[这是我编写/训练的一个小型药物毒性预测 GNN 模型 repo: https://github.com/Null-byte-00/有毒-预测-gnn    由   提交 /u/Soroush_ra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</guid>
      <pubDate>Thu, 25 Apr 2024 19:10:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 面对不可能的机器学习问题，您有哪些恐怖经历</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</link>
      <description><![CDATA[机器学习非常擅长解决一系列小众问题，但大多数技术细微差别都被技术兄弟和管理者忽视了。您被告知要解决哪些问题是不可能的（没有数据、无用的数据、不切实际的期望）或机器学习的误用（您能让这个法学硕士做所有的会计工作吗）。    由   提交 /u/LanchestersLaw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</guid>
      <pubDate>Thu, 25 Apr 2024 18:45:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 旧论文 - 机器学习奖学金中令人不安的趋势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</link>
      <description><![CDATA[我只是想提醒或向新人介绍这篇论文。我认为应该重新开启这个讨论，因为这里的许多人实际上确实影响了该领域的趋势。 https://arxiv.org/pdf/1807.03341&quot;&gt;https:// /arxiv.org/pdf/1807.03341  个人笔记（随意跳过）： 具体来说，我想指出这个问题“Mathiness”，因为这个问题似乎失控了，并且大多数会议的最佳论文都受到了它的困扰（最重要的 ML 论文之一试图数学化，但引入了一个大错误，我相信其他论文有更大的问题，但没有人费心去检查）。 所以这是我个人对学者和研究人员的观点：  我们（我认为大多数将会涉及），从业者不需要方程来知道什么是召回率，并且显然不想阅读难以理解的线性回归版本，这只会让你的论文毫无用处。如果您不想浪费我们的时间，请将其放入附录或完全删除。 审稿人，请不要对不必要的数学印象深刻，如果它很复杂并且没有任何用处，谁关心吗？而且，无论如何它都可能有缺陷，您可能不会发现它。    由   提交/u/pyepyepie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</guid>
      <pubDate>Thu, 25 Apr 2024 15:50:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 没有进行分层训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</link>
      <description><![CDATA[在我看来，由于残差路径，无论变压器层/块如何，流向每一层的梯度都是相同的。示例： ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X))) ... ） 由于 ProjectionAndCost 的输入只是所有层和初始嵌入的输出之和，因此到达 L1 层的梯度与到达 L2 或 L3 的梯度相同。 因此我们可以：  首先仅训练 L1：ProjectionAndCost(X + L1(X)) 冻结 L1，包括 L2 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X))) 冻结 L1 和 L2，包括 L3 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X)))) ..依此类推  我们不能先训练L2 然后是 L1，因为 L2 的输入取决于 L1，但我们可以先训练较低层，然后逐渐添加和训练更深的层。这种方法有什么问题吗？   由   提交/u/kiockete  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</guid>
      <pubDate>Thu, 25 Apr 2024 14:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>