<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 18 Jan 2024 18:17:15 GMT</lastBuildDate>
    <item>
      <title>凯文·墨菲的书 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199w77k/kevin_murphys_book_d/</link>
      <description><![CDATA[作为一年级博士生；如果我想回顾并巩固我对统计领域的各种 DS 主题的理解；优化，ML 我应该使用 Kevin Murphy 的哪本书籍 (0,1,2)？   由   提交 /u/GroovyChipmunk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199w77k/kevin_murphys_book_d/</guid>
      <pubDate>Thu, 18 Jan 2024 17:57:03 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 数据分析法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199vm1x/discussion_llms_for_data_analysis/</link>
      <description><![CDATA[大家好， 我有一个关于法学硕士和数据库的问题。请允许我详细说明一下。 尽管人工智能领域取得了所有进步，但法学硕士在分析数据并使用 SQL 获取答案方面似乎存在局限性。我可以用这个简单的用例来解释我的意思： 1. 为当地社区提供服务的典型小型独立便利店将设有 POS（销售点）。在后端，该 POS 将有一个数据库，该数据库的模式包含许多表，用于销售、库存管理、支付处理、客户管理等。当然，大型商店或任何大型组织都会有更大的后端数据库。  2.鉴于上述场景，用户应该用自然语言与数据库聊天。他们应该能够提出（文本或语音）与数据分析相关的任何类型的问题，并获得文本、图表甚至图像的答复。本质上，LLM 正在做数据分析师的工作。  是的，现在可以在 CHATGPT 中以 CSV 格式上传表格并获得一些简单的答案。但随着表数量的增加，即使使用 GPT4 等高级 LLM，LLM 的能力也会受到限制。有些人尝试以这样的方式命名表和列，以便法学硕士可以更好地理解。然而，即使在这种情况下，结果也不好。  目前有LLM解决这个问题的方法吗？据我了解，这似乎是法学硕士的圣杯。 人类使用自然语言与法学硕士进行数据分析的潜力是巨大的。 谢谢非常感谢您的宝贵时间。期待您的回复。   由   提交 /u/neocolonialoverlord   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199vm1x/discussion_llms_for_data_analysis/</guid>
      <pubDate>Thu, 18 Jan 2024 17:32:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最好的 tts rvc 组合是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199v5v3/d_whats_the_best_tts_rvc_combo/</link>
      <description><![CDATA[目前我正在使用 Mangio-RVC 将音频转换为音频。它正在创造奇迹。 但是我也需要一些 tts。不仅仅是音频到音频。我最好的选择是什么？    由   提交/​​u/Time_Lord7  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199v5v3/d_whats_the_best_tts_rvc_combo/</guid>
      <pubDate>Thu, 18 Jan 2024 17:13:40 GMT</pubDate>
    </item>
    <item>
      <title>Astra 的新数据 API [N]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199uobn/new_data_api_for_astra_n/</link>
      <description><![CDATA[我看到 DataStax/Ast​​ra DB 刚刚发布了新的数据 API，以帮助构建生产 GenAI 和 RAG 应用程序。该 API 使经过验证的 PB 级 Apache Cassandra 易于使用，可供任何 JavaScript、Python 或全栈应用程序开发人员使用。 还将与 LangChain 联合举办一场网络研讨会，可在此处注册： https://www. datastax.com/events/wikichat-build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel   由   提交/u/DBAdvice123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199uobn/new_data_api_for_astra_n/</guid>
      <pubDate>Thu, 18 Jan 2024 16:54:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 指标</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199ttno/d_metrics/</link>
      <description><![CDATA[我正在尝试计算多类分类问题的指标。 我有一个零样本模型，可以跨类别进行分类候选人数量。 我有一个包含单个类别 (y_true) 的基本事实集。  目前，我正在选择预测置信度最高的类别作为我的预测结果 (y_pred)。 这里我可能会错过什么？理想情况下，我希望同样关注精确度和召回率，但如果我更多地关注精确度，那也不是问题。失去精度绝对是一个问题。   由   提交 /u/Defiant-Cockroach-59   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199ttno/d_metrics/</guid>
      <pubDate>Thu, 18 Jan 2024 16:18:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪些分析和异常检测可以自动化？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199r5jg/d_what_analyses_and_anomaly_detections_could_be/</link>
      <description><![CDATA[我正在开发神经网络调试工具。目前它对于可视化和深入的手动分析很有用，这是张量板和其他工具所缺乏的。 我想扩展它以自动执行许多常见的分析和异常检测，并且我&#39;我正在寻找建议。 它是如何工作的： 您在具有相似任务和不同超参数的相似网络上运行了多次试验。该工具记录所有相关数据并自动检测诸如“梯度消失”之类的异常情况。或“损失具有异常高的方差”。 在第二步中，它在每个试验的超参数与这些试验中检测到的异常之间执行相关性分析。然后，它会为每个具有统计意义的发现生成一个警告列表。例如：  “学习率高于 3e-4 的试验中，有 30% 的梯度消失，而学习率低于 3e-4 的试验中，梯度消失的比例为 0%。” &lt; li&gt;“50% 的架构变体 X 试验在损失方面具有异常高的方差，而其他架构变体的试验只有 10%。”  有大量警告，例如这些自动生成的内容可以让您非常快速地识别错误。此外，如果没有生成警告，那么您可以对模型的稳定性更有信心。 当然，许多警告也可能是不值得调查的误报，但我认为这更好无缘无故地被警告，而不是错过一个真正重要的问题。 你觉得这个想法怎么样？ 你认为什么类型的异常最有意义？寻找？   由   提交 /u/Smart-Emu5581   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199r5jg/d_what_analyses_and_anomaly_detections_could_be/</guid>
      <pubDate>Thu, 18 Jan 2024 14:18:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenCLIP JAX - JAX/Flax 中的 CLIP 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199r1py/p_openclip_jax_clip_models_in_jaxflax/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199r1py/p_openclip_jax_clip_models_in_jaxflax/</guid>
      <pubDate>Thu, 18 Jan 2024 14:12:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] EPU-CNN：用于可解释计算机视觉的广义加法 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199qw5i/r_epucnn_generalized_additive_cnn_for/</link>
      <description><![CDATA[论文：https:/ /www.nature.com/articles/s41598-023-38459-1 代码：https ://github.com/innoisys/EPU-CNN 摘要：卷积神经网络（CNN）模型在高风险领域的采用因无法满足社会的需求而受到阻碍。决策的透明度。到目前为止，已经出现了越来越多的方法来开发可通过设计解释的 CNN 模型。然而，此类模型无法在保持良好性能的同时提供符合人类感知的解释。在本文中，我们通过一种新颖的通用框架来应对这些挑战，该框架用于实例化本质上可解释的 CNN 模型，称为 E pluribus unum 可解释 CNN (EPU-CNN)。 EPU-CNN 模型由 CNN 子网络组成，每个子网络接​​收输入图像的不同表示，表达感知特征，例如颜色或纹理。 EPU-CNN 模型的输出由分类预测及其解释（根据输入图像不同区域感知特征的相对贡献）组成。 EPU-CNN 模型已在各种公开可用的数据集以及贡献的基准数据集上进行了广泛的评估。医学数据集用于证明 EPU-CNN 在医学风险敏感决策中的适用性。实验结果表明，EPU-CNN 模型可以实现与其他 CNN 架构相当或更好的分类性能，同时提供人类可感知的解释。    由   提交/u/ashenone420  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199qw5i/r_epucnn_generalized_additive_cnn_for/</guid>
      <pubDate>Thu, 18 Jan 2024 14:05:30 GMT</pubDate>
    </item>
    <item>
      <title>[D]图数据推荐系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199qmqt/d_recommender_system_for_graph_data/</link>
      <description><![CDATA[我有一个充满图表的数据库。每个图代表工程师构建的组件，其中每个节点代表一个组件，每条边代表它们之间的连接。除了 ID 之外，没有有关组件的其他信息。您认为哪些推荐技术可以在新组件组装过程中推荐下一个组件时产生良好的结果？我探索过的一种方法是通过图神经网络，而且效果很好。我对更“传统”的方法感兴趣，没有神经网络。你们有什么想法？    由   提交 /u/ItsjustabirdSon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199qmqt/d_recommender_system_for_graph_data/</guid>
      <pubDate>Thu, 18 Jan 2024 13:53:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] EarthPT：时间序列变压器基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</link>
      <description><![CDATA[想要分享 EarthPT 的代码版本，这是一个在零样本设置下预测未来卫星观测的模型！我是第一作者，所以请向我提出任何问题。 EarthPT 是一个 7 亿参数解码变压器基础模型，以自回归自监督方式训练，并专门针对 EO 用例开发头脑。 EarthPT 可以准确预测未来 400-2300 nm 范围内的卫星观测结果（我们发现了六个月！）。 EarthPT 学到的嵌入包含语义上有意义的信息，可用于下游任务，例如作为高度精细的动态土地利用分类。 对我来说最酷的收获是 EO 数据在理论上为我们提供了千万亿的训练标记。因此，如果我们假设 EarthPT 遵循类似于大型语言模型 (LLM) 导出的神经缩放定律，那么目前对于缩放 EarthPT 和其他类似的“大型观测模型”没有数据强加的限制。(!) 代码：https://github.com/aspiaspace/EarthPT 论文：https://arxiv.org/abs/2309.07207   由   提交/u/Smith4242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</guid>
      <pubDate>Thu, 18 Jan 2024 13:21:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI 如何增加 GPT-4 迭代的上下文长度？他们是否从头开始重新训练 GPT-4-1106？或者是稀疏注意力、分块等技术的更黑客组合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/</link>
      <description><![CDATA[正如标题所述，开始思考 GPT-4 衍生模型及其制作方式。我知道事情发展得很快，而且 OpenAI 绝不是“开放”的，但是关于它是如何完成的猜测是什么？ 我不了解 LLM 进展的所有最新细节，但来自根据我对注意力机制的理解，通常你必须从头开始重新训练变压器以增加上下文大小。 但如果是这样的话，他们是否也必须重做所有 RLHF？或者是否有针对 RLHF 步骤的高效迁移学习技术？ 我很想看到一些将 GPT-4 迭代的评估相互比较的论文（如果您知道的话可以链接）。即使假设 RLHF 是完全可移植的，我们是否仍然期望 GPT-4 系列中的模型之间存在可测量的差异？ 我想知道这些模型之间是否存在任何有洞察力的性能怪癖，例如对于编码任务，32k 0613 模型的性能可能比 8k 基本模型更好，但 128k 1106 比 0613 差，因为在给定相同数量的参数、相同的训练数据等的情况下，上下文大小的回报会下降。   由   提交 /u/great_waldini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/</guid>
      <pubDate>Thu, 18 Jan 2024 10:28:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 神经网络适应增加/减少噪声。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199lz7g/d_adaptation_of_neural_networks_to/</link>
      <description><![CDATA[假设您在图像分类数据集上训练具有固定结构（神经元数量、隐藏层、卷积等）的分类神经网络，直到收敛。您的测试数据的性能无法再提高。  假设每个像素的噪声都是完全随机的，理论上这个神经网络是否也可以经过最佳训练来对数据的更多（或更少）噪声版本进行分类？性能显然会受到噪声的影响，但是如果您期望在未来的分类任务中出现更高/更低的噪声，神经网络是否必须重新训练？    ;由   提交 /u/ActuaV   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199lz7g/d_adaptation_of_neural_networks_to/</guid>
      <pubDate>Thu, 18 Jan 2024 09:07:59 GMT</pubDate>
    </item>
    <item>
      <title>[D]本文的分区是否会导致数据泄露？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</link>
      <description><![CDATA[我最近对 ​​这项研究。总而言之，他们使用文本嵌入和梯度提升来根据财报电话会议记录来预测 CEO 性格得分。他们分析了约 200 位首席执行官，将每位首席执行官的电话分为多个部分以增加数据点。然而，每位 CEO 都会出现在训练和验证集中，并具有不同的通话片段。在我看来，这应该会导致数据泄漏，因为该模型可能会发现个别首席执行官语言使用的特殊性，而不是底层数据生成过程的模式。您对此有何看法？   由   提交/u/Expective_Charity293  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</guid>
      <pubDate>Thu, 18 Jan 2024 08:03:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlphaGeometry：奥林匹克级几何人工智能系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19932kw/r_alphageometry_an_olympiadlevel_ai_system_for/</link>
      <description><![CDATA[博客：https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/ 论文：https://www.nature.com/articles/s41586-023-06747-5 Github：https://github.com/google-deepmind/alphageometry 摘要：  在奥林匹克级别证明数学定理代表着人类水平自动推理的一个显着里程碑，因为它们在世界上最优秀的大学预科数学人才中被认为是困难的。然而，由于将人类证明转换为机器可验证格式的成本高昂，当前的机器学习方法不适用于大多数数学领域。对于几何来说，这个问题更为严重，因为其独特的转换挑战，导致训练数据严重匮乏。我们提出了 AlphaGeometry，这是欧几里得平面几何的定理证明器，它通过综合不同复杂程度的数百万个定理和证明来回避人类演示的需要。 AlphaGeometry 是一个神经符号系统，它使用神经语言模型，在我们的大规模合成数据上从头开始训练，引导符号推演引擎通过具有挑战性的问题的无限分支点。在包含 30 个最新奥林匹克级别问题的测试集上，AlphaGeometry 解决了 25 个问题，超越了之前仅解决了 10 个问题的最佳方法，接近了国际数学奥林匹克 (IMO) 金牌得主的平均表现。值得注意的是，AlphaGeometry 产生了人类可读的证明，在人类专家评估下解决了 IMO 2000 和 2015 中的所有几何问题，并在 2004 年发现了翻译后的 IMO 定理的广义版本。 &lt;!-- SC_ON - -&gt;  由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19932kw/r_alphageometry_an_olympiadlevel_ai_system_for/</guid>
      <pubDate>Wed, 17 Jan 2024 18:01:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>