<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Fri, 29 Mar 2024 06:17:27 GMT</lastBuildDate>
    <item>
      <title>[D]变形金刚还有其他重要的用例吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqhyuo/dare_there_any_other_non_trivial_use_cases_of/</link>
      <description><![CDATA[Seq2Seq 预测架构是为序列预测而设计的，并且在文本生成中自然是 SOTA，但是还有其他我们可以使用它们的重要任务吗？就像 MeshGPT 使用 gpt 模型来生成网格一样，扩散变压器现在也在研究中，事实上 sora 使用了一种模型。在许多其他应用程序中，这些模型可能是高效且可扩展的吗？   由   提交 /u/ApartmentEither4838   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqhyuo/dare_there_any_other_non_trivial_use_cases_of/</guid>
      <pubDate>Fri, 29 Mar 2024 06:00:20 GMT</pubDate>
    </item>
    <item>
      <title>人们会对聚合此处发布的论文的不和谐服务器/博客感兴趣吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqhji1/would_people_be_interested_in_a_discord/</link>
      <description><![CDATA[嗨，很多海报发布了他们发现有趣的研究工作和论文，很难跟踪。我给有趣的论文添加了书签，结果却忘记了它们。  想知道这里的人是否值得做一个不和谐服务器（每天更新）或一个媒体博客（每周更新）来跟踪在此子上发布的论文。   由   提交 /u/shadowylurking   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqhji1/would_people_be_interested_in_a_discord/</guid>
      <pubDate>Fri, 29 Mar 2024 05:32:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会如何回答这个面试问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqftlw/d_how_would_you_answer_this_interview_question/</link>
      <description><![CDATA[不确定这是否是一个“职业问题”，但最近有人问我这个面试问题： 在一场有 10 辆赛车的 F1 赛车比赛，您如何计算/预测第二名赛车超过第一名赛车的概率？这个计算需要什么算法、数据和模型？解释每个步骤。 你会如何回答这个问题？ （没有给出其他信息）   由   提交/u/Conscious_Giraffe453  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqftlw/d_how_would_you_answer_this_interview_question/</guid>
      <pubDate>Fri, 29 Mar 2024 03:56:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] Jamba：首款基于 Mamba 的生产级模型，提供一流的质量和性能。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</link>
      <description><![CDATA[帖子：https://www.ai21 .com/blog/announcing-jamba  我们很高兴宣布 Jamba，世界上第一个基于 Mamba 的生产级模型。通过使用传统 Transformer 架构的元素增强 Mamba 结构化状态空间模型 (SSM) 技术，Jamba 弥补了纯SSM模型。它提供了 256K 上下文窗口，已经在吞吐量和效率方面展现了显着的进步——这只是这种创新混合架构的开始。值得注意的是，Jamba 在各种基准测试中都优于或匹配同尺寸级别的其他最先进型号。  ​ &lt; !-- SC_ON --&gt;  由   提交 /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</guid>
      <pubDate>Fri, 29 Mar 2024 03:39:56 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 扩散概率模型论文中的推导过程中遇到的困难</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqf7le/discussion_struggling_with_a_derivation_in_the/</link>
      <description><![CDATA[有人可以帮我找出论文“使用非平衡热力学的深度无监督学习”中的推导中的错误吗？ （2015）：https://arxiv.org/pdf/1503.03585.pdf 我有在下面的链接中发布了有关 AI 堆栈交换的完整问题： https://ai.stackexchange.com/questions/45272/trying-to-understand-some-derivation-in-the-paper-deep-unsupervised-learning-我们   由   提交 /u/possiblemonk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqf7le/discussion_struggling_with_a_derivation_in_the/</guid>
      <pubDate>Fri, 29 Mar 2024 03:24:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 的代码库文档和测试 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqdkge/code_base_documentation_and_testing_using_llm_p/</link>
      <description><![CDATA[大家好，我们正在研究一种新的算法来训练法学硕士。我们取得了突破，训练算法是能够使一个非常小的模型在许多任务上与 gpt-3.5 的性能相匹配。我们开源了一个库，它记录了您的整个代码库并对您的代码运行动态分析。 这里是链接 https://github.com/PipableAI/pip-library-etl&lt; /a&gt;   由   提交 /u/Soaccer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqdkge/code_base_documentation_and_testing_using_llm_p/</guid>
      <pubDate>Fri, 29 Mar 2024 02:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[N] GenAI在医疗领域的机遇</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqanl6/n_opportunities_of_genai_in_healthcare/</link>
      <description><![CDATA[不确定这里有多少人热衷于医疗保健领域的 genAI...这是一个很棒的子堆栈，概述了部署法学硕士的机遇和挑战：https://ambarbhattacharyya.substack。 com/p/re-imagining-the-healthcare-delivery?r=12ee1&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true  &amp; #32；由   提交/u/PriorSuccessful156  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqanl6/n_opportunities_of_genai_in_healthcare/</guid>
      <pubDate>Thu, 28 Mar 2024 23:51:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] LoRA官方实现代码中转置的目的是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq73qh/d_whats_the_purpose_of_the_transpose_in_official/</link>
      <description><![CDATA[刚刚浏览了他们的官方实现代码并对此感到好奇。例如，在他们的 Embedding 模块中，他们声明并使用了这样的 lora 参数：  self.lora_A = nn.Parameter(self.weight.new_zeros((r, num_embeddings))) self.lora_B = nn.Parameter(self.weight.new_zeros((embedding_dim, r) )) ... self.weight.data -= (self.lora_B @ self.lora_A).transpose(0, 1) * self.scaling ... after_A = F.embedding( x, self.lora_A.transpose(0 , 1), self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse ) 结果 += (after_A @ self.lora_B.transpose(0, 1)) * self.scaling ...  那么，为什么他们不直接这样声明并在不转置的情况下使用呢？ self.lora_A = nn.Parameter(self.weight.new_zeros(( r, embedding_dim))) self.lora_B = nn.Parameter(self.weight.new_zeros((num_embeddings, r)))  这些转置的目的是什么？ （官方代码链接）   由   提交/u/kessa231  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq73qh/d_whats_the_purpose_of_the_transpose_in_official/</guid>
      <pubDate>Thu, 28 Mar 2024 21:21:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] NL-ITI：修改LLM内部表示以使其更加真实</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq5tn7/r_nliti_modifying_llm_internal_representations_to/</link>
      <description><![CDATA[您好，在这里您可以找到我们最近的出版物（以及代码），其中我们修改了 LLM 内部表示以使其更加真实。简而言之，我们优化了 ITI 方法（2306.03341.pdf (arxiv.org)）并取得了显着的性能提升。评估主要是在 TruthfulQA 上进行的，尽管我们也测试了它之外的泛化（MMLU、ARC、OpenBookQA）。我们使用 KL 和 CE 指标来衡量干预的侵入性。 https ://paperswithcode.com/paper/nl-iti-optimizing-probing-and-intervention   由   提交 /u/autonomous_llm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq5tn7/r_nliti_modifying_llm_internal_representations_to/</guid>
      <pubDate>Thu, 28 Mar 2024 20:30:12 GMT</pubDate>
    </item>
    <item>
      <title>自适应 RAG：一种降低 top-k 向量索引检索的 LLM 令牌成本的检索技术 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq3hwb/adaptive_rag_a_retrieval_technique_to_reduce_llm/</link>
      <description><![CDATA[摘要：我们演示了一种技术，该技术允许使用法学硕士的反馈动态调整 top-k 检索器 RAG 提示中的文档数量。这使得 RAG LLM 问答的成本降低了 4 倍，同时保持了相同的准确性水平。我们还表明该方法有助于解释法学硕士输出的血统。参考实现适用于大多数模型（GPT4、许多本地模型、较旧的 GPT-3.5 Turbo），并且可以适应大多数公开 top-k 检索原语的矢量数据库。 博客论文：https://pathway.com/developers/showcases/adaptive-rag 参考实现：&lt; a href=&quot;https://github.com/pathwaycom/pathway/blob/main/python/pathway/xpacks/llm/question_answering.py&quot;&gt;https://github.com/pathwaycom/pathway/blob/main/python /pathway/xpacks/llm/question_answering.py   由   提交 /u/dxtros   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq3hwb/adaptive_rag_a_retrieval_technique_to_reduce_llm/</guid>
      <pubDate>Thu, 28 Mar 2024 18:55:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 斯坦福大学的 BioMedLM 论文报告的准确性与评估的准确性：没有意义</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq1deb/d_stanfords_biomedlm_paper_reported_accuracy_vs/</link>
      <description><![CDATA[      斯坦福大学发布#BioMedLM，一种基于生物医学数据训练的 2.7B 参数语言模型。然而，结果似乎没有意义。 这里是在 MultiMedQA（MedMCQA、MedQA、MMLU、PubMed）上使用 LM Evaluation Harness 框架的评估报告。  https://preview.redd .it/vd21crtn14rc1.png?width=1442&amp;format=png&amp;auto=webp&amp;s=ee905e8277006e40c37b7e5b87003165bd0de4b5 https://preview.redd.it/6ot7mibo14rc1.png?width=1164&amp;format=png&amp;auto=webp&amp;s=5d76fcce909fb07d 5404e148b0cdc2fbc6dae43c ​   由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq1deb/d_stanfords_biomedlm_paper_reported_accuracy_vs/</guid>
      <pubDate>Thu, 28 Mar 2024 17:32:07 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于组织和监控多模型训练的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq0pwv/d_suggestions_on_organizing_and_monitoring/</link>
      <description><![CDATA[嘿大家，我有一个项目，对我来说有点复杂，所以我试图先为其设计出最佳结构来让事情运行起来，我正在寻找一些建议。 情况： 我有 4 个表格预测数据集，每个数据集有 31 个我需要为其训练回归模型（使用 XGBoost）的响应变量（RV）。到最后，我将拥有 124 (4 * 31) 个经过训练的模型。 理想情况下，对于每个 RV，我希望执行某种形式的 K 倍交叉验证超参数优化和最终模型分析也将基于 K-fold CV。 挑战： 我正在尝试找出组织所有这些的最佳方式这样一来，当涉及到再现性和分析以及有可能添加新的预测数据和/或新的 RV 时，它并不是一团糟。我之前已经这样做过一次，并且选择只将数据写入 CSV，但这很快就变得笨拙，最终需要大量额外的代码才能合理地处理和解析结果。 I我真的很希望能够可视化每个模型的训练和性能，但该领域流行工具的大多数示例似乎都集中于训练单个模型，并通过“实验”进行训练。通常指不同的超参数或功能修改。 DVC、Aim、WandB 看起来都很吸引人，但我不太确定如何概念化我的特定工作流程，并且我希望避免任何最终的限制陷阱将来，确保我的初始设置是正确的。 我很想听听其他人如何组织此类多模型/集成培训项目！   由   提交 /u/pwinggles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq0pwv/d_suggestions_on_organizing_and_monitoring/</guid>
      <pubDate>Thu, 28 Mar 2024 17:05:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年构建大型语言模型的小指南 – 75 分钟讲座</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bpzrm0/d_a_little_guide_to_building_large_language/</link>
      <description><![CDATA[我终于录制了两周前的讲座，因为人们一直向我索要视频。 所以在这里，我希望您会喜欢“2024 年构建大型语言模型的小指南”。 我试图使其简短而全面 - 重点关注对于培训优秀 LLM 至关重要但通常隐藏的概念在技​​术报告中。 在讲座中，我向学生介绍了培训良好绩效法学硕士的所有重要概念/工具/技术：- 查找、准备和评估网络规模数据- 理解模型并行性和高效培训- 微调/对齐模型 - 快速推理 当然有很多东西和细节缺失，我应该添加进去，不要犹豫告诉我你是最令人沮丧的遗漏，我将在以后的部分中添加它。特别是，我认为我将更多地关注如何很好地、广泛地过滤主题，也许还有更多实用的轶事和细节。 既然我记录了它，我一直在想这可能是一个主题的第 1 部分。由两部分组成的系列，其中包含第二个完整的实践视频，介绍如何使用我们最近在 HF 围绕 LLM 培训发布的一些库和配方来运行所有这些步骤（并且无论如何都可以轻松适应您的其他框架）：  用于所有网络规模数据准备的 datatrove：https://github.com/huggingface/datatrove  nanotron 用于轻量级 4D 并行 LLM 培训：https://github.com/huggingface/nanotron lighteval 用于训练中快速并行 LLM 评估：https://github.com/huggingface/lighteval  以下是在 Youtube 上观看讲座的链接：https://www. youtube.com/watch?v=2-SPH9hIKT8这是 Google 幻灯片的链接：https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit#slide=id.p 很高兴听到对此的反馈以及在第二部分中添加、更正、扩展的内容。   由   提交 /u/Thomjazz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bpzrm0/d_a_little_guide_to_building_large_language/</guid>
      <pubDate>Thu, 28 Mar 2024 16:26:57 GMT</pubDate>
    </item>
    <item>
      <title>幻觉的终结（对于那些能负担得起的人）？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bpxs1i/the_end_of_hallucination_for_those_who_can_afford/</link>
      <description><![CDATA[      DeepMind 刚刚发表了一篇关于事实检查文本的论文： &lt; a href=&quot;https://preview.redd.it/zsmv0a0293rc1.png?width=1028&amp;format=png&amp;auto=webp&amp;s=789c1c2f9b31aa734a7ebcf459df3ad06bd74285&quot;&gt;https://preview.redd.it/zsmv0a0293rc1.png?width =1028&amp;format=png&amp;auto=webp&amp;s=789c1c2f9b31aa734a7ebcf459df3ad06bd74285 该方法使用 GPT-3.5-Turbo，每个模型响应成本为 0.19 美元，比人类注释者更便宜，同时更比他们准确： https:/ /preview.redd.it/ob7bb3iv73rc1.png?width=1014&amp;format=png&amp;auto=webp&amp;s=e79bbcaa578b29772cb3b43ead508daff7288091 他们使用这种方法创建事实基准并比较一些流行的法学硕士。 论文和代码：https://arxiv.org/abs/2403.18802 编辑：关于帖子的标题：幻觉被定义（在维基百科中）为“由人工智能生成的响应，其中包含作为事实呈现的虚假或误导性信息。”：您的代码不编译本身并不是一种幻觉。当你声称代码是完美的时，那是一种幻觉。    由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bpxs1i/the_end_of_hallucination_for_those_who_can_afford/</guid>
      <pubDate>Thu, 28 Mar 2024 15:04:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>