<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 19 Jan 2024 21:11:53 GMT</lastBuildDate>
    <item>
      <title>[R] 自我奖励语言模型 - Meta 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2401.10020  Github：https://github.com/lucidrains/self-rewarding -lm-pytorch 摘要：  我们认为，为了实现超人智能体，未来的模型需要超人反馈才能提供足够的训练信号。目前的方法通常根据人类偏好来训练奖励模型，这可能会受到人类表现水平的瓶颈，其次这些单独的冻结奖励模型无法在 LLM 训练期间学习改进。在这项工作中，我们研究自我奖励语言模型，其中语言模型本身通过法学硕士作为法官来使用，提示在训练期间提供自己的奖励。我们表明，在迭代 DPO 培训期间，不仅提高了指令遵循能力，而且还提高了为自身提供高质量奖励的能力。在我们的方法的三个迭代中对 Llama 2 70B 进行微调，产生的模型优于 AlpacaEval 2.0 排行榜上的许多现有系统，包括 Claude 2、Gemini Pro 和 GPT-4 0613。虽然只是初步研究，但这项工作打开了大门模型可以在两个轴上不断改进的可能性。   https:// /preview.redd.it/l7vav40qngdc1.jpg?width=1344&amp;format=pjpg&amp;auto=webp&amp;s=9dce97a69f2ede66d6dabf6abbcfc75bf0e94f19 https://preview.redd.it/fuooe70qngdc1.jpg?width=1180&amp;format=pjpg&amp;auto=webp&amp;s =a88fcf1c765ff42c18091889f5b14cd371248760   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/</guid>
      <pubDate>Fri, 19 Jan 2024 21:01:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] [D] COT 多数票计算的自洽</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aquwt/r_d_self_consistency_for_cot_majority_vote/</link>
      <description><![CDATA[“语言模型中思想推理链的自洽性提高” （Wang et al. 2022）计算多数票来确定一组答案中最一致的答案。他们指出，在对多个 (r_i ,a_i ) 进行采样后，其中 r 是推理路径，a 是答案，他们通过多数投票 $argmax_a \sum 1{a_i = a}$ 对 r_i 进行边缘化。 我不明白指示变量$a_i = a$的概率分布是如何计算的？直观上应该有某种方法来衡量 $a_i$ 与 $a$ 的相似程度。   由   提交 /u/MLJungle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aquwt/r_d_self_consistency_for_cot_majority_vote/</guid>
      <pubDate>Fri, 19 Jan 2024 19:05:10 GMT</pubDate>
    </item>
    <item>
      <title>[2401.10187] GPU 上的快速克罗内克矩阵-矩阵乘法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19amot7/240110187_fast_kronecker_matrixmatrix/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19amot7/240110187_fast_kronecker_matrixmatrix/</guid>
      <pubDate>Fri, 19 Jan 2024 16:13:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习中的不确定性来源——统计学家的观点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aml6l/r_sources_of_uncertainty_in_machine_learning_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.16703 摘要：  机器学习和深度学习今天已经达到了令人印象深刻的标准，使得我们可以回答几年前难以想象的问题。除了这些成功之外，很明显，除了纯粹的预测（大多数监督机器学习算法的主要优势）之外，不确定性的量化也是相关且必要的。虽然近年来出现了这个方向的第一个概念和想法，但本文采用概念视角并研究了不确定性的可能来源。通过采用统计学家的观点，我们讨论了任意和认知不确定性的概念，这些概念更常见于机器学习。本文旨在将两种类型的不确定性形式化，并证明不确定性的来源是多种多样的，并且并不总是可以分解为任意的和认知的。我们将机器学习中的统计概念与不确定性进行比较，还展示了数据的作用及其对不确定性的影响。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aml6l/r_sources_of_uncertainty_in_machine_learning_a/</guid>
      <pubDate>Fri, 19 Jan 2024 16:09:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人工神经网络中的类脑学习：综述</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19amch3/r_braininspired_learning_in_artificial_neural/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.11252 摘要：  人工神经网络（ANN）已成为以下领域的重要工具：机器学习，在图像和语音生成、游戏和机器人等多个领域取得了显着的成功。然而，人工神经网络的运行机制与生物大脑的运行机制之间存在根本差异，特别是在学习过程方面。本文对当前人工神经网络中的类脑学习表示进行了全面回顾。我们研究了更多生物学上合理的机制的整合，例如突触可塑性，以增强这些网络的能力。此外，我们深入研究了这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究找到了有希望的途径，这可以让我们更接近理解智能的本质。     ;由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19amch3/r_braininspired_learning_in_artificial_neural/</guid>
      <pubDate>Fri, 19 Jan 2024 15:59:59 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 将视频放大与视频重定时/插值相结合的网络？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19alo6a/discussion_network_that_combines_video_upscaling/</link>
      <description><![CDATA[如果有一个网络经过训练可以执行视频放大/去噪以及为帧插值创建中间帧，那么很明显，针对一项任务的训练将另一方面也提高了准确性。这以前有人做过吗？我可以读到一篇论文来显示这个结果吗？ 到目前为止我见过的所有论文似乎都分别处理这两个问题，例如 DAIN（深度感知视频帧）插值）网络。   由   提交/u/Vivid-Art6939  /u/Vivid-Art6939 reddit.com/r/MachineLearning/comments/19alo6a/discussion_network_that_combines_video_upscaling/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19alo6a/discussion_network_that_combines_video_upscaling/</guid>
      <pubDate>Fri, 19 Jan 2024 15:30:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] GPU 服务器库存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19allss/d_p_stockpile_of_gpu_servers/</link>
      <description><![CDATA[我有 22 台 8 GPU 服务器，配备 AMD Mi50（请参阅下面有关 Mi50 的注释）。我已经能够让 PyTorch 在这些 GPU 上运行，并且能够对不同的大型语言模型进行推理。我原本想使用这些 GPU 来为 LLM 提供服务，但 VLLM cuda 内核不能在 Mi50 上开箱即用，而且 Llama CPP 有一个错误，它一次最多只能支持 4 个 AMD GPU。 所以，TLDR，我不想让这些服务器闲置，如果有人对服务器有任何有用的创意，我很乐意授予他们 SSH 访问权限。 Mi50 规格：  - 16GB VRAM - 1TB/s VRAM 带宽 - 25 TFLOPs  &amp; #32；由   提交 /u/TheRealBracketMaster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19allss/d_p_stockpile_of_gpu_servers/</guid>
      <pubDate>Fri, 19 Jan 2024 15:27:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] AISTATS 2024论文接收结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19al3e9/d_aistats_2024_paper_acceptance_result/</link>
      <description><![CDATA[AISTATS 2024 论文接受结果将于今天发布。正在为今年的结果创建讨论主题。   由   提交/u/zy415  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19al3e9/d_aistats_2024_paper_acceptance_result/</guid>
      <pubDate>Fri, 19 Jan 2024 15:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自我奖励语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19akxwp/r_selfrewarding_language_models/</link>
      <description><![CDATA[ 由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19akxwp/r_selfrewarding_language_models/</guid>
      <pubDate>Fri, 19 Jan 2024 14:57:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] Creatures 1996，一款利用机器学习的早期人工生命模拟游戏。想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ak2w9/d_creatures_1996_an_early_artificial_life/</link>
      <description><![CDATA[我想先说一下，当我浏览 Reddit 时，我看到了这个游戏的描述： “这个游戏有当时其中有一些非常复杂的系统。它有一个化学系统，你的生物的免疫系统，他们的行为和个性，他们的DNA和繁殖系统，你必须通过对象-词和行为关联来教他们实际的语言和单词，你必须惩罚和奖励他们的行为正确地，否则他们会发展出适应不良的行为或变得暴力并杀死你的其他生物，如果你不处理的话，他们也会变得沮丧，等等。事实上，游戏中甚至有一整套他们可以体验的情感系统，你必须尝试管理它，否则你的生物会变得孤立并对你反应迟钝。除此之外，还有暴力且患病的敌方生物种族，称为 grendels，它们在世界上漫游，可以杀死/骚扰你的生物。” 根据维基百科页面： “生物是一种人工生命模拟，用户可以孵化毛茸茸的小动物并教它们如何行为，或者让它们自己学习。这些“诺恩”可以说话，自己进食，并保护自己免受称为Grendels的邪恶生物的侵害。这是机器学习在交互式模拟中的第一个流行应用。生物利用神经网络来学习该做什么。该游戏被认为是人工生命研究的突破，旨在模拟生物与其环境互动的行为。” https://en.m.wikipedia.org/wiki/Creatures_(1996_video_game) 还有其他更高级的人工生命模拟游戏吗？这些看起来确实非常有趣，尤其是考虑到我们在机器学习方面取得了几十年的进步。   由   提交 /u/Username912773   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ak2w9/d_creatures_1996_an_early_artificial_life/</guid>
      <pubDate>Fri, 19 Jan 2024 14:16:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] AWS 课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ajjll/d_aws_courses/</link>
      <description><![CDATA[大家好，我是一名 ML 工程师，试图换工作，但似乎到处都需要云经验。不幸的是，我没有使用过云，但我想学习它，特别是 AWS。您推荐哪些 AWS 课程？   由   提交 /u/lusinn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ajjll/d_aws_courses/</guid>
      <pubDate>Fri, 19 Jan 2024 13:50:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在视觉领域，引导模型在利基任务上工作的当前 SOTA 是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</link>
      <description><![CDATA[过去，当您需要在一些相对小众的分类/检测/分割任务上训练模型时，您会使用在ImageNet1K/COCO 并将其微调到您拥有的任何中小型数据集，这足以将您的性能提升到合理的水平。当然，您始终可以通过使用更大的 Resnet、改进超参数选择或清除专有数据集中的噪声来改进这一点。更新的架构已经发布，更新的优化器，我们现在有像 CLIP 这样的大型 VL 模型，等等。我想知道我是否错过了一个新的共识。 如果你选择回答，我将不胜感激如果您还详细说明了以下标准：  您选择的方法是否对超参数过于敏感？ / 收敛到正确的模型有多难？例如，根据我的经验（当然不是绝对的），在超参数选择方面，ResNet 比 EfficientNet 更宽容。 您的方法对少量数据的敏感程度如何？例如，我记得原来的 Transformer 在小训练集场景中非常糟糕，结果在 IN22K 上报告。 您的选择有多快和/或内存效率如何？小的利基任务往往不会证明具有 1B 参数的模型是合理的。  谢谢！   由   提交/u/anaccountforthemasse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</guid>
      <pubDate>Fri, 19 Jan 2024 08:57:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] Facebook 关闭 ParlAI，一个对话研究框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19adrgv/d_facebook_shuts_down_parlai_a_framework_for/</link>
      <description><![CDATA[我刚刚了解到 Facebook 已经存档了 BlenderBot 背后的团队 ParlAI。该存储库于 2023 年 11 月 3 日存档，现在是只读的，此后该项目的 Twitter 帐户没有任何更新。 因此 Facebook 放弃了工程化和模块化对话系统背后的想法，并全力以赴对于LLM，我还听说其他大公司的其他模块化对话团队也在裁员。你觉得怎么样？   由   提交 /u/Comfortable_Use_5033   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19adrgv/d_facebook_shuts_down_parlai_a_framework_for/</guid>
      <pubDate>Fri, 19 Jan 2024 07:43:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能很强大实际上是一个严肃而真实的领域正在研究，或者只是人们正在宣传的另一种炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</link>
      <description><![CDATA[强大的人工智能。 （通用人工智能）实际上是一个严肃的研究领域，还是只是刚刚阅读/观看科幻小说的人的纯粹炒作？ 强人工智能/又名是强人工智能吗？ AGI实际上被一些研究人员/机构认真对待，他们认为它最终可以实现，或者它是人们一直在炒作的另一种奇特的技术蒸汽软件，但实际上，那些在该领域工作的人知道这样的想法实际上无法实现，因为严格的物理限制，或者如果发生的话，需要几个世纪才能实现？ 因为过去对于许多未来主义者来说有很多歇斯底里的感觉技术，这些技术被很多不了解蹲点的人大肆宣传，但它无法在实践中发挥作用（即 Em Drive、石墨烯、富勒烯、纳米机器人、Bussard Ramjet、聚变能源等）。   由   提交/u/Enzo-chan  /u/Enzo-chan  reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</guid>
      <pubDate>Fri, 19 Jan 2024 01:16:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>