<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 26 Sep 2024 09:17:44 GMT</lastBuildDate>
    <item>
      <title>使用 LSTM 进行日志异常检测 [讨论][项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fprfzo/log_anomaly_detection_with_lstm_discussionproject/</link>
      <description><![CDATA[[项目] 嗨，这是我的第一篇帖子，我是这个领域的新手，显然是在一家小公司实习，我参与了一个带有人工智能模型的日志异常检测系统项目，我需要很多帮助来选择模型并完成所有设置，我正在考虑使用 LSTM RNN，但我不确定，因为我找不到任何参考资料或任何好的东西可以继续使用，有人可以帮忙吗    提交人    /u/Mundane_Pea_4974   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fprfzo/log_anomaly_detection_with_lstm_discussionproject/</guid>
      <pubDate>Thu, 26 Sep 2024 08:35:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM + 药物研发自动报告代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpqst1/p_llm_agents_for_automatic_reporting_in_drug/</link>
      <description><![CDATA[嗨 r/Machinelearning，我想分享我的团队一直在进行的一些工作，并从社区获得一些反馈。在我们在 Arxiv 上发表的这项工作中，我们提出了一个为药物发现生成自动报告的系统。我们使用了 LLM、RAG 和代理。 药物发现是一个昂贵、漫长且高风险的过程。该过程可能花费高达 10-20 亿美元，平均需要 10-15 年。人工智能有望降低成本、时间表和失败风险。药物发现是一个复杂的多步骤过程，需要精确和推理。 LLM 表现出很好的通才技能，但在医学等专业领域却举步维艰。两个主要问题是：  缺乏持续更新。在医学和药物发现领域，每天都会发表许多文章，而模型知识在预训练时就停止了。  模型通过生成不正确或虚构的输出产生幻觉。   为了解决这些问题，我们使用了带有 RAG 和代理的管道。 LLM 响应用户的查询，从不同的医学和生物数据库（文章、专利、临床试验、基因和蛋白质数据库等）检索信息。然后它会自动生成报告和演示文稿 文章在这里：https://arxiv.org/abs/2409.15817 带有示例的存储库：https://github.com/SalvatoreRa/Automatic-Target-Dossier    提交人    /u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpqst1/p_llm_agents_for_automatic_reporting_in_drug/</guid>
      <pubDate>Thu, 26 Sep 2024 07:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习对抗性攻击与防御论文 - 资源与交流</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpq46c/r_thesis_on_adversial_attacks_and_defenses_on_ml/</link>
      <description><![CDATA[嗨， 我目前正在写关于标题中主题的论文。有人有好的资源和/或经验吗？ 当然，我已经找到了关于该主题的论文和其他资源。但也许我错过了一些东西。此外，如果我能与有经验的人交流，那就太好了。请随时给我发私信。    提交人    /u/iamrooter   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpq46c/r_thesis_on_adversial_attacks_and_defenses_on_ml/</guid>
      <pubDate>Thu, 26 Sep 2024 06:51:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 ONNX 为边缘设备导出 YOLOv8：如何处理 NMS？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpq34h/d_exporting_yolov8_for_edge_devices_using_onnx/</link>
      <description><![CDATA[大家好， 我正在使用 ONNX 导出用于边缘设备 (Android) 部署的 YOLOv8 模型，在非最大抑制 (NMS)方面遇到了一些障碍。有些人可能知道，YOLOv8 在导出到 ONNX 时默认不包含 NMS，这意味着我不知道在边缘处理它的最佳方法是什么。 对于那些做过类似事情的人，我很好奇这种情况下的标准做法是什么。 具体来说：  您是否在模型导出中包含 NMS，还是在设备推理期间单独处理？ 在资源受限的设备（如 Jetsons 或 Raspberry Pi 或 Android）上部署带有 ONNX 的 YOLO 模型时，您的首选方法是什么？ 在执行此操作时，是否有任何提示或经验教训来优化性能和准确性？  我很想听听其他人的方法（或无效的方法！）。    由    /u/leoboy_1045  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpq34h/d_exporting_yolov8_for_edge_devices_using_onnx/</guid>
      <pubDate>Thu, 26 Sep 2024 06:49:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会关注哪些信息？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</link>
      <description><![CDATA[虽然 arxiv 和 open review 是新论文的两个最佳来源，但我发现某些 feed 也非常有趣。对我来说，这包括 GitHub、Less Wrong、Hugging Face、Twitter 和 Reddit。我遗漏了什么吗？还有更多吗？博客列表？我希望有这些东西的整合。    提交人    /u/Studyr3ddit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</guid>
      <pubDate>Thu, 26 Sep 2024 04:27:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 模型将 2D 平面图转换为 3D 设计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpnfh1/p_models_to_convert_2d_floor_plans_to_3d_designs/</link>
      <description><![CDATA[是否有任何模型可以根据平面图生成 3D 房屋/建筑设计。如果没有，我该如何创建一个？我应该尝试收集什么样的数据来训练这样的模型？任何帮助都值得赞赏。     提交人    /u/Kirang96   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpnfh1/p_models_to_convert_2d_floor_plans_to_3d_designs/</guid>
      <pubDate>Thu, 26 Sep 2024 03:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] Aggressor：“无矢量量化的自回归扩散”的实验实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpn0wp/p_aggressor_experimental_implementations_of/</link>
      <description><![CDATA[大家好 r/MachineLearning！我想分享一个我一直在做的项目，并从社区获得一些反馈。 项目概述 我在一个名为“Aggressor”的项目中实现了我自己版本的最近论文&quot;无矢量量化的自回归图像生成&quot;&quot;。目标是创建一个超最小自回归扩散模型，从图像生成开始，然后扩展到各种模态和架构变化。 GitHub Repo： Aggressor 主要特点  核心实现： aggressor.py 包含用于图像生成的最小实现。 实验变化：  ret_aggressor.py：用 RetNet（Retentive Network）机制取代标准注意力机制，允许像注意力机制一样并行训练，但 O(n) 循环生成。 dct_aggressor.py：利用离散余弦变换 (DCT) 用于图像生成，探索频域表示。 wav_aggressor.py：使用 DCT 调整模型以生成音频，展示跨模态功能。 ycr_aggressor.py：尝试使用 YCbCr 颜色空间生成图像，可能提高色彩保真度。  最小依赖性：仅使用基本的 MLX 操作从头开始构建。 多模态：支持图像和音频生成，并计划用于视频。  结果 我已经在 MNIST、CIFAR 和音频数据集上测试了各种模型。您可以在 README 中看到一些示例输出。 技术细节  主要的 Aggressor 类结合了转换器（或 RetNet）和扩散模型。 使用自定义 Scheduler 处理扩散中的前向和后向过程。 对图像和音频数据进行 DCT（离散余弦变换）实验。  未来方向  实现视频生成 致力于全模态模型 探索不需要标记器的字节级多模态语言模型的可能性  社区问题  有人尝试过跨不同模态的自回归扩散吗？有什么见解吗？ 有没有建议有效地将这种方法扩展到视频或多模态数据？ 关于使用 DCT 或其他变换来提高生成质量或效率的想法？ 有没有使用多模态数据的字节级模型的经验？挑战还是好处？  我愿意接受任何反馈、问题或改进建议。我特别有兴趣讨论将这种方法扩展到更复杂的多模态场景的潜力和挑战。谢谢您的关注！    提交人    /u/JosefAlbers05   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpn0wp/p_aggressor_experimental_implementations_of/</guid>
      <pubDate>Thu, 26 Sep 2024 03:28:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 表格增强生成(TAG)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fplpnk/d_table_augmented_generationtag/</link>
      <description><![CDATA[https://arxiv.org/pdf/2408.14717 https://github.com/TAG-Research/TAG-Bench?tab=readme-ov-file 有人看过 TAG 论文或 GitHub 吗？我觉得他们没有做什么特别的事情。我不知道他们能带来什么独特的价值。如果我遗漏了什么，请告诉我。 他们的技术分为 3 个步骤： 查询合成 查询执行 答案生成步骤 但他们确实声称，与使用传统的 text2sql 或 RAG 回答结构化数据问题相比，这种技术提供的结果准确率高达 65%。    提交人    /u/G_S_7_wiz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fplpnk/d_table_augmented_generationtag/</guid>
      <pubDate>Thu, 26 Sep 2024 02:15:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] ViT 受益于双曲空间变换</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fplhvi/r_vits_benefit_from_hyperbolic_space_transform/</link>
      <description><![CDATA[https://arxiv.org/abs/2409.16897    由   提交  /u/jacobfa   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fplhvi/r_vits_benefit_from_hyperbolic_space_transform/</guid>
      <pubDate>Thu, 26 Sep 2024 02:03:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 大型语言模型结构化输出和函数调用的基本指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpiqlj/p_the_essential_guide_to_large_language_models/</link>
      <description><![CDATA[过去一年，我一直在使用 LLM 构建生产系统。当我在 2023 年 8 月开始工作时，材料非常稀缺，以至于必须先重新发明许多轮子。时至今日，情况已经发生了变化，但社区仍然迫切需要教育材料，特别是从生产角度来看。 很多人都在谈论 LLM，但很少有人真正将它们应用于他们的用户/业务。 这是我对社区的新贡献，“大型语言模型结构化输出和函数调用基本指南”文章。 这是一篇关于结构化输出和函数调用的实践指南（长篇），以及如何从 0 到 1 应用它们。要求不多，只是一些基本的 Python，其余的都解释了。 我在公司将其应用于“让我们通过 LLM 为 20 万以上用户解决所有客户支持问题”的计划中取得了相当大的成功。我们还没有达到 100% 的目标，但我们正在快速实现目标，特别是结构化的输出使我们成为可能。 传播这个消息，让我们在演示之外分享更多关于应用 LLM 的经验。    提交人    /u/p_bzn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpiqlj/p_the_essential_guide_to_large_language_models/</guid>
      <pubDate>Wed, 25 Sep 2024 23:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] Llama 3.2详细分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/</link>
      <description><![CDATA[大家好！Meta 发布了一组新的 Llama 3.2 模型，分别用于文本（1B、3B）和视觉（11B、90B）。我对这些模型进行了深入研究，希望能够有所启发：  新的 1B 和 3B 文本专用 LLM 9 万亿个 token 新的 11B 和 90B 视觉多模态模型 128K 上下文长度 1B 和 3B 使用了一些来自 8B 和 70B 的提炼 VLM 60 亿个图片、文本对 CLIP MLP GeLU + 交叉注意  长分析：1. 视觉编码器中使用带有 GeLU 激活的 CLIP 类型 MLP。类似于 GPT2 的 MLP。与 Llama 3 的 MLP 不同，因为 SwiGLU 不用于视觉 MLP。  用于视觉编码器的正常 layernorm - 不是 RMS Layernorm。此外，一些“门控”参数用于乘以隐藏状态。 在注意力和 MLP 之后对隐藏状态进行门控乘法器 - tanh 用于将向量缩放移动到从 -1 到 1 的数字。 对于小型 1B 和 3B LLM 以及多模态 VLM 11B 和 90B，评估看起来相当不错。1B 49.3 MMLU 和 3B 63.4。 VLM MMMU 50.7 和 90B 60.3  感谢您的阅读，如果您有任何疑问，请告诉我！    由    /u/danielhanchen 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/</guid>
      <pubDate>Wed, 25 Sep 2024 19:09:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 评审问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpa7ua/d_neurips_2024_review_question/</link>
      <description><![CDATA[我最初的审稿人指出了一些弱点和顾虑，但这些问题在我的反驳中得到了解决。他们承认了这一点并提高了分数。  我的论文最终被拒绝，因为程序主席引入了由于误读论文而产生的新弱点，如果这些弱点在最初的审稿中有所说明，这个问题将很容易解决。我能做些什么来修复这个程序主席的审稿吗？    提交人    /u/sqweeeeeeeeeeeeeeeps   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpa7ua/d_neurips_2024_review_question/</guid>
      <pubDate>Wed, 25 Sep 2024 17:30:43 GMT</pubDate>
    </item>
    <item>
      <title>[R]基于注意力的选择性激活架构</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fp70ca/r_attentionbased_selective_activation_architecture/</link>
      <description><![CDATA[有没有研究探索过灵活推理/深度负载架构的想法？也就是说，根据样本的任务难度，对模型的不同层深度进行训练。这将通过绕过模型的后面层来完成，以完成较简单的任务。最困难的任务将占用整个网络。对于 LLM 来说，在 Transformer/Mamba 中实现这一点需要一些思考，但我相信这是可行的。特别是如果在 Beam 方法下进行训练，而不是单输出方式（从未理解为什么仍然这样做，因为 Beam 似乎更好）。可以采用基于注意力的机制来决定推理的深度，或者采用某种强化学习主导的方法（训练后，如果 layer = n 给出错误的输出，则转到 n+1 直到满意为止） 我相信这会使模型在不同层次的复杂性/智能上成形（每层都能够输出一些可理解的内容，从而生成更易于解释的模型）。它还可以解决很多不必要的推理时间。 这个想法是一种看待“思维链”和我们真正思考方式的不同方式。它会将“思考”部分直接嵌入模型中，而不会自行生成内部独白。总而言之，仍然认为这两种方法都是积极且兼容的（我认为我们人类同时做这两种事情）。    提交人    /u/hatekhyr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fp70ca/r_attentionbased_selective_activation_architecture/</guid>
      <pubDate>Wed, 25 Sep 2024 15:18:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 22 Sep 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>