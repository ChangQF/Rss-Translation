<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sat, 08 Jun 2024 15:14:29 GMT</lastBuildDate>
    <item>
      <title>[项目] 我可以和 YOLO 专家交流一下吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1db48sa/project_an_expert_on_yolo_i_can_talk_to/</link>
      <description><![CDATA[您好！！首先，如果这个平台不适合提问，我深表歉意。我有一个机器学习项目，最初我决定使用 YOLOv5，但准确率不可接受（只有 40%）。然后我目前正在使用 YOLOv7 进行训练，但结果并不理想，因为我搜索到 YOLOv7 旨在优先考虑准确率而不是速度。 有了这个，有没有对 YOLO 有足够经验的人可以给我发私信和交流？请 :( 我对此仍然非常困惑，想向某人寻求帮助。    提交人    /u/Full_Fisherman7110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1db48sa/project_an_expert_on_yolo_i_can_talk_to/</guid>
      <pubDate>Sat, 08 Jun 2024 14:31:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 攻读博士学位的秘诀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1db2n54/d_tips_for_pursuing_a_phd/</link>
      <description><![CDATA[大家好，我刚刚完成了我的工程学士学位，今年 9 月我将在意大利开始我的计算机科学硕士学位（人工智能方向）。我想在欧洲或美国（不一定是顶尖大学）的一所好大学完成大学学业后攻读博士学位，例如马克斯普朗克大学（德国大学，如柏林工业大学或图宾根大学），或巴黎、伦敦或其他大学，如普渡大学、东北大学…… 鉴于此，我想问一下我可以立即做些什么来最大限度地提高我的机会。 - 首先，我的 GPA 有多重要？ - 您建议我在一年中或暑假参加哪些课外活动？ - 您是否建议成为实验室的暑期研究助理，如果建议，我该如何实现这一目标？我应该联系谁，什么时候联系？    提交人    /u/AshamedRecover1786   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1db2n54/d_tips_for_pursuing_a_phd/</guid>
      <pubDate>Sat, 08 Jun 2024 13:10:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 硬件迷 - 你会用 Blackwell 做什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1db2j0j/d_hardware_nerds_what_would_you_do_with_blackwell/</link>
      <description><![CDATA[我热爱硬件。我的第一台“大机器”是 CDC Cyber​​ 174C，然后是 Cray XMP。那是一个不同的时代。问题和数据都不同。现在我全身心投入机器学习，尽管我确实喜欢法学硕士，但我主要在不同的领域（时间序列）工作，处理 PB 级数据。 鉴于 NVidia 的一系列令人惊叹的高端硬件、现在可用的东西以及最新最昂贵的东西像这样，我的问题是，你会用它做什么？在这个规模下，你能解决什么你现在无法解决的问题？    提交人    /u/Simusid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1db2j0j/d_hardware_nerds_what_would_you_do_with_blackwell/</guid>
      <pubDate>Sat, 08 Jun 2024 13:04:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一种新的对齐技术：通过短路提高对齐和稳健性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1db1gg6/r_a_new_alignment_technique_improving_alignment/</link>
      <description><![CDATA[这个想法似乎是识别与概念相关的内部状态并强制 EOS 状态。它似乎与神经热图和 Anthropic 的类似研究有关：https://www.anthropic.com/news/mapping-mind-language-model Arxiv 链接：https://arxiv.org/pdf/2406.04313    提交人    /u/ReasonablyBadass   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1db1gg6/r_a_new_alignment_technique_improving_alignment/</guid>
      <pubDate>Sat, 08 Jun 2024 12:06:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] Archand：完全使用手势来控制鼠标。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1db08g8/p_archand_control_your_mouse_entirely_using_hand/</link>
      <description><![CDATA[链接：https://github.com/prateekvellala/Archand 我的项目功能 Archand 允许您完全使用手势控制鼠标。除了基于手势的鼠标控制外，Archand 还引入了语音转文本功能，该功能由特定手势激活，可将您说出的单词转换为计算机上的书面文本。有了它，您还可以执行通常使用键盘执行的任何任务，例如访问网站、写电子邮件、给人们发短信等。 我的项目具有以下功能，每个功能都由独特的手势控制：  移动指针 单击左键 单击右键单击 双击左键 按住左键并移动指针（用于拖动等） 向上滚动 向下滚动 启用麦克风，然后您说的任何话都将转换为文本并输入到光标闪烁的位置（自动化键盘功能）  比较 没有与任何其他项目进行比较，因为我还没有看到任何项目包含我已实现的所有功能，这些功能可以与低分辨率集成笔记本电脑网络摄像头和高端网络摄像头准确配合使用。我遇到的所有具有类似概念的项目主要分为三类：  它们根本不起作用，甚至无法平滑移动光标。 光标移动得相当好，很平滑，但它们不能完全自动化鼠标，因为它们总是缺少一些其他功能，如双击、右键单击或滚动等。 它们有许多运行良好的功能，但需要高端网络摄像头，如 Logitech Brio。     提交人    /u/prateekvellala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1db08g8/p_archand_control_your_mouse_entirely_using_hand/</guid>
      <pubDate>Sat, 08 Jun 2024 10:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 统计学习要素作者是如何得到这个结果的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1day7gq/d_elements_of_statistical_learning_how_did_the/</link>
      <description><![CDATA[ 诅咒的另一种表现形式是采样密度与 N^1/p 成正比，其中 p 是输入空间的维度，N 是样本大小。因此，如果 N1 = 100 表示单个输入问题的密集样本，则 N10 = 100^10 是具有 10 个输入的相同采样密度所需的样本大小。  我对文本如何得出采样密度与 N^1/p 成正比的结论感到困惑    提交人    /u/Confident_Ad_7734   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1day7gq/d_elements_of_statistical_learning_how_did_the/</guid>
      <pubDate>Sat, 08 Jun 2024 08:28:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] Hydra：通过多头预测架构增强机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1day3hc/r_hydra_enhancing_machine_learning_with_a/</link>
      <description><![CDATA[  由    /u/bluzkluz  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1day3hc/r_hydra_enhancing_machine_learning_with_a/</guid>
      <pubDate>Sat, 08 Jun 2024 08:20:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用 Tinder 的方式标记数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dax94f/p_labeling_data_the_tinder_way/</link>
      <description><![CDATA[我当时正在研究一种情绪分析模型，该模型需要带有适当标签的数据集。我没有采用无聊的方式，而是创建了一个 Web 服务器，将所有数据集保存在 SQL 中，并创建了一个类似 Tinder 的界面来查看数据并将其归类为 positive、negative 或 neutral。 对我的项目有什么看法？您会使用这个来标记数据吗？ 项目链接：tinder-for-reviews :p    提交人    /u/ResetWasTaken   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dax94f/p_labeling_data_the_tinder_way/</guid>
      <pubDate>Sat, 08 Jun 2024 07:21:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士 (LLM) 的私人推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dauewa/d_private_inferencing_for_llms/</link>
      <description><![CDATA[您好， 基于云的 LLM 推理面临的最大挑战之一是保护用户数据的私密性。是否可以同时使用本地和云机器来解决这个问题？ 例如，我们可以在本地机器上运行 LLM 的第一层和最后一层来保护数据，并使用云来处理其余数据以加快速度吗？我们可以在本地微调第一层和最后一层以更改权重并使其远离云端。 如果有任何正在进行的研究使用此方法进行隐私推理，请告诉我。 谢谢。    提交人    /u/manili   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dauewa/d_private_inferencing_for_llms/</guid>
      <pubDate>Sat, 08 Jun 2024 04:17:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 爱丽丝梦游仙境：简单任务展现最先进的大型语言模型中完整的推理能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dah5ie/r_alice_in_wonderland_simple_tasks_showing/</link>
      <description><![CDATA[  由    /u/conceptual_visual_me  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dah5ie/r_alice_in_wonderland_simple_tasks_showing/</guid>
      <pubDate>Fri, 07 Jun 2024 17:55:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 测试 LoRA 初始化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dadfcv/r_testing_lora_initialisations/</link>
      <description><![CDATA[      大家好，过去几天，我一直在测试几种不同的 LoRA 初始化方法。如您所知，默认情况下，我们将 ΔW = AB 初始化为 A~kaiming_uniform，将 B 初始化为零。但我想尝试其他初始化策略，这些策略可导致 ΔW = 0，但可能使用最少的零参数。  以下是我尝试过的方法：  反转初始化：将 A 初始化为零，将 B 初始化为均匀分布 纯正交初始化：创建两个彼此正交的非零矩阵。为此，我有两种策略。  取一组随机的正交向量（通过对随机矩阵进行正交分解），将其分成两组。  将单位矩阵的行分成两组。 （例如，集合 1 中的偶数行和集合 2 中的奇数行） 使用第一组中元素的线性组合初始化 A，使用集合 2 中的元素的线性组合初始化 B   我在不同的模型上进行了相同的训练，例如 llama-2-7B、llama-3-8B、mistral-7B-v0.3 和 llama-2-13B。我使用的数据集是 MetaMathQA 和 MagicCoder-evol。我发现正​​交初始化比标准初始化表现更好。我只是比较了每次运行的评估损失。  评估不同初始化策略的损失。 所以我觉得这很有趣。这有点像我所期望的那样，用较少的零进行初始化应该是好的。 我注意到的另一件事是，lora_B 的梯度始终比 lora_A 的梯度更分散。我最初以为这是由于初始化，那些初始化为零的梯度会用更大的数字更新。但对于不同的初始化也是如此。这很令人惊讶。也许是操作顺序导致了这种情况？我不知道... 我在博客文章 https://datta0.github.io/blogs/know-your-lora/ 中详细介绍了所有内容，请随时阅读，如果您有任何想法/评论，请告诉我。 干杯。    提交人    /u/im_datta0   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dadfcv/r_testing_lora_initialisations/</guid>
      <pubDate>Fri, 07 Jun 2024 15:22:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 弥合神经网络形式语言学习中的经验与理论之间的差距</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dab1ef/r_bridging_empiricaltheoretical_gap_in_neural/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.10013    提交人    /u/inland-1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dab1ef/r_bridging_empiricaltheoretical_gap_in_neural/</guid>
      <pubDate>Fri, 07 Jun 2024 13:43:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是不是我的问题，或者基准测试似乎让语言模型变得更糟了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1daa68e/d_is_it_me_or_does_it_seem_like_benchmarks_are/</link>
      <description><![CDATA[这些在过去曾经非常有用，全面适用。现在大多数语言模型总是忽略简单的指令。LLama3 似乎是最好的，Claude 也不错。GPT-4o 感觉非常马虎，总是忽略指令或给出类似但根本没有要求的东西。自从谷歌推出 Gemini 以来，我注意到的唯一变化是对基准的关注。您是否认为这些基准让开发人员过度优化语言模型，从而使语言模型变得更糟？类似的情况是，尽管 GAN 的行为不准确，但 GAN 有时会通过在鉴别器中找到黑客而崩溃？（其中一些使用语言模型使其更容易被黑客入侵）。 编辑：这是对那些完全混蛋而不是善意讨论的男孩的。 非常不专业的行为。这只是在讨论观察结果。我当然知道什么是统计数据。如果你认为数字可以说明一切，那你就太愚蠢了。本文旨在讨论可以进行的潜在改进，或者这些基准是否缺乏评估来解释人们为什么会这样想。    提交人    /u/I_will_delete_myself   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1daa68e/d_is_it_me_or_does_it_seem_like_benchmarks_are/</guid>
      <pubDate>Fri, 07 Jun 2024 13:05:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 从 GPT-4 中提取概念</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1da6ml3/r_extracting_concepts_from_gpt4/</link>
      <description><![CDATA[与最近 Anthropic 的报告类似，OpenAI 发布了一份报告、一些代码和一个可视化工具，用于显示自动编码器从其模型中提取的特征。 OpenAI 博客文章：https://openai.com/index/extracting-concepts-from-gpt-4/ 论文：https://cdn.openai.com/papers/sparse-autoencoders.pdf 代码：https://github.com/openai/sparse_autoencoder    由   提交  /u/valdanylchuk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1da6ml3/r_extracting_concepts_from_gpt4/</guid>
      <pubDate>Fri, 07 Jun 2024 09:37:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>