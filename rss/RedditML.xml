<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 29 May 2024 15:16:04 GMT</lastBuildDate>
    <item>
      <title>[D] 我应该如何充分利用我的机器学习实习机会？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3e7kr/d_how_should_i_make_the_most_out_of_my_ml/</link>
      <description><![CDATA[我是一名本科生，下周将开始数据科学实习（主要是计算机视觉方面）。我的经理告诉我，在选择从事哪种项目时，我将有很大的灵活性，选项范围从数据预处理、使用 SQL 进行数据管道工程、ML 模型训练、软件内容（例如在 AWS 上上传模型等）。基本上是整个管道。我来自数学背景，在 CV 和深度学习方面拥有不错的个人项目经验，但在数据工程、AWS 或 SQL 方面几乎没有。 我是否应该尝试通过尝试数据工程和预处理工作来建立广泛的技能，这对于当今任何数据工作都是必不可少的？这对我来说是一个学习 Snowflake 和 AWS 的好机会。或者，既然我可以在真实的行业研究环境中正确地做到这一点，那么我应该专注于 ML 和 CV？我觉得很少有本科生有机会在行业中做真正的 ML，我应该利用这个机会。有什么想法吗？    由    /u/a1200km 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3e7kr/d_how_should_i_make_the_most_out_of_my_ml/</guid>
      <pubDate>Wed, 29 May 2024 14:57:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向最优 LLM 量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3e2r6/r_towards_optimal_llm_quantization/</link>
      <description><![CDATA[picoLLM 压缩是一种新颖的 LLM 量化算法，它根据特定于任务的成本函数自动学习 LLM 权重内和跨 LLM 权重的最佳比特分配策略。现有技术需要固定的比特分配方案，这是不达标的。 文章：https://picovoice.ai/blog/picollm-towards-optimal-llm-quantization/ GitHub：https://github.com/Picovoice/llm-compression-benchmark    提交人    /u/alikenar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3e2r6/r_towards_optimal_llm_quantization/</guid>
      <pubDate>Wed, 29 May 2024 14:51:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们如何提高开源 LLM 在竞赛级数学中的表现（使用任何可能的方式）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3cbzu/d_how_can_we_improve_the_performance_of_open/</link>
      <description><![CDATA[据我研究，deepseek-math-7b-rl 是迄今为止最好的模型。您需要包括诸如自我一致性/多数投票、python 工具集成和自我验证等方法。代理（由开源 LLM 组成）能否以更好的方式执行 CoT，它们能否灌输对自己生成的答案的验证？比如为 CoT 的每个步骤提供评估分数作为观察结果或类似内容？    提交人    /u/DumbNeuron   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3cbzu/d_how_can_we_improve_the_performance_of_open/</guid>
      <pubDate>Wed, 29 May 2024 13:34:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人知道如何获取扩散模型的速率失真曲线吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3bvkb/d_anyone_knows_how_to_get_ratedistortion_curve/</link>
      <description><![CDATA[大家好，我有不同的训练扩散模型，我见过许多扩散论文都提到了速率失真曲线。有人知道生成它们的方法吗？或者可以给我指出合适的资源吗？    提交人    /u/sidney_lumet   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3bvkb/d_anyone_knows_how_to_get_ratedistortion_curve/</guid>
      <pubDate>Wed, 29 May 2024 13:12:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] Friday Oxen.ai 论文俱乐部：从 Claude 3 Sonnet 中提取可解释的特征</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3aeaq/d_friday_oxenai_paper_club_extracting/</link>
      <description><![CDATA[听听 Hugging Face 联合创始人 Thomas Wolf 称之为“完全基于”的论文，通过 Oxen.ai 首席执行官兼 Plain-Speak-Delving 大师 Greg Schoeninger 的视角进行解读。 注册：https://lu.ma/oxen 太平洋时间周五上午 10:00，东部时间下午 1:00 在 Zoom 上 论文：https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html?s=09%2F/ ？嘿，这个没有 ArXiv 链接吗？ 感谢 Greg、u/FallMindless3563、Scott Howard u/sthoward 和 Oxen 团队与社区分享您的知识，同时提供很酷的工具来在 oxen.ai 上管理数据集。    提交人    /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3aeaq/d_friday_oxenai_paper_club_extracting/</guid>
      <pubDate>Wed, 29 May 2024 11:56:53 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Prompt Teacher - 免费的教育工具，教授如何编写有效的 LLM 提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d391p4/project_prompt_teacher_free_educational_tool/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d391p4/project_prompt_teacher_free_educational_tool/</guid>
      <pubDate>Wed, 29 May 2024 10:36:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用大型语言模型进行工具学习：一项调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d37emc/r_tool_learning_with_large_language_models_a/</link>
      <description><![CDATA[      PDF：https://arxiv.org/abs/2405.17935 GitHub：https://github.com/quchangle1/LLM-Tool-Survey 摘要：最近，使用大型语言模型 (LLM) 的工具学习已成为增强 LLM 解决高度复杂问题能力的一种有前途的范例。尽管该领域受到越来越多的关注和快速发展，但现有文献仍然零散且缺乏系统组织，为新手设置了进入门槛。这一差距促使我们对现有的使用 LLM 的工具学习的著作进行全面调查。在本次调查中，我们重点从两个主要方面回顾现有文献 (1) 工具学习为何有益以及 (2) 工具学习如何实施，从而全面了解使用 LLM 的工具学习。我们首先通过从六个具体方面回顾工具集成的好处和工具学习范式的固有好处来探索“为什么”。在“如何”方面，我们根据工具学习工作流程中的四个关键阶段的分类法系统地回顾了文献：任务规划、工具选择、工具调用和响应生成。此外，我们还提供了现有基准和评估方法的详细摘要，并根据它们与不同阶段的相关性对其进行了分类。最后，我们讨论了当前的挑战并概述了未来的潜在方向，旨在激励研究人员和工业开发人员进一步探索这一新兴且充满希望的领域。 https://preview.redd.it/t46d2cxivb3d1.jpg?width=1250&amp;format=pjpg&amp;auto=webp&amp;s=a3d3bd9f285717b6a6f9c9d0015789ec39f9abd9 https://preview.redd.it/rp0gdkkjvb3d1.png?width=830&amp;format=png&amp;auto=webp&amp;s=2e87a52ccf7637783f308fd6b421d8b2fa0cbee0 https://preview.redd.it/fwwyuq3kvb3d1.jpg?width=914&amp;format=pjpg&amp;auto=webp&amp;s=39f080e0db7c611f418b53e99aa274a2d7ad35b7    提交人    /u/Lumpy-Ad-2115   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d37emc/r_tool_learning_with_large_language_models_a/</guid>
      <pubDate>Wed, 29 May 2024 08:41:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据科学家无需数据即可完成任务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d374hh/d_data_scientist_does_the_task_without_data/</link>
      <description><![CDATA[最近我被分配了一个任务，根据用户交互活动构建一个用户购买评分系统。 然而，有趣的是，我没有关于用户与产品交互的数据，所以我调查了许多方的解决方案，并使用我的假设来创建我认为适合构建预测模型的特征。当然，当我把它呈现给经理时，结果非常糟糕。我坐下来和他讨论创建模型时所需的特征定义，让我很生气的是，他仍然不知道构建评分模型需要什么样的数据。人们将如何处理这种情况？    提交人    /u/unknow_from_vietnam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d374hh/d_data_scientist_does_the_task_without_data/</guid>
      <pubDate>Wed, 29 May 2024 08:21:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于现阶段的法学硕士来说，幻觉不是比安全更重要的研究吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d329nt/d_isnt_hallucination_a_much_more_important_study/</link>
      <description><![CDATA[为什么我觉得 LLM 更强调安全性而不是幻觉？ 在现阶段，确保生成准确的信息不是最优先的吗？ 为什么在我看来并非如此    提交人    /u/xiikjuy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d329nt/d_isnt_hallucination_a_much_more_important_study/</guid>
      <pubDate>Wed, 29 May 2024 03:06:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] KNN 中 k=1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2xrb8/d_k1_in_knn/</link>
      <description><![CDATA[晚上好，我在平衡的测试集上训练了 knn 算法，然后在不平衡的测试集上对其进行了测试；我得到 k=1 作为准确度方面的最佳参数，并使用交叉验证确认了这一结果。有这个值奇怪吗？    提交人    /u/Nice-Fisherman-1269   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2xrb8/d_k1_in_knn/</guid>
      <pubDate>Tue, 28 May 2024 23:21:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Andrew Dudzik 谈深度学习中的 SOTA</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2wc9a/d_andrew_dudzik_on_sota_in_deep_learning/</link>
      <description><![CDATA[Google DeepMind 的 Dudzik 最近表示，Transformers 实际上并不是 sota，而图神经网络才是真正的 sota：Andrew Dudzik - 深度学习数学中的三个问题 - YouTube 当然，前者在许多任务（NER、低资源语言的翻译等）中对 OOD 数据的处理并不那么好。但另一方面，并​​不是所有东西都适合知识图谱结构。只是开放这个讨论。大家同意吗？他们最近有没有读过更多关于图神经网络的有趣论文？    提交人    /u/Objective-Camel-3726   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2wc9a/d_andrew_dudzik_on_sota_in_deep_learning/</guid>
      <pubDate>Tue, 28 May 2024 22:19:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于“你只缓存一次：语言模型的解码器-解码器架构”的问题 - https://arxiv.org/pdf/2405.05254v1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</link>
      <description><![CDATA[      这是我第一次尝试通读一篇论文。但是，我很难理解这一点，我认为你们会知道我的问题的答案，因为如图 1 所示，这种新架构对于 LLM 来说似乎是一件大事。 图 1 据我了解，主要思想是将网络分成两部分。前 L/2 层是自解码器层，可生成全局 KV 缓存。第二个 L/2 层是跨解码器层，重用了生成的全局 KV-Cache。 引用他们论文中关于他们如何节省大量计算和内存的内容（我理解这部分）：  具体来说，因为全局 KV 缓存被重用，而高效的自注意力需要常量缓存，所以缓存的数量为 O(N + CL)，其中 N 是输入长度，C 是常数（例如滑动窗口大小），L 是层数。对于长序列，CL 比 N 小得多，因此大约需要 O(N) 个缓存，即只缓存一次。相比之下，Transformer 解码器在推理过程中必须存储 N × L 个键和值。因此，与 Transformer 解码器相比，YOCO 大约节省了 L 倍的 GPU 缓存内存。  这是我不明白的。在仅解码器网络中，查询、键和值的概念的功能与数据库中的用法有些相似，但侧重于捕获单词之间的关系。在这种网络的每一层中，这些组件有助于细化对文本的理解，在处理从一层移动到下一层时根据新见解调整焦点。 每一层都通过更新查询、键和值来构建前一层，从而细化网络的解释和响应生成。 如果仅解码器网络的各个 KV 缓存的所有信息现在都被压缩到全局 KV 缓存中，我们是否不会丢失有价值的信息并且我们是否应该看到更差的性能？  此外，我们只有一半的层来细化这种解释，因为跨解码器层都重用相同的 KV 缓存。 图 2    提交人    /u/StraightChemistry629   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</guid>
      <pubDate>Tue, 28 May 2024 17:58:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] GT 深度估计：LiDAR 与立体深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</link>
      <description><![CDATA[为什么大多数深度估计基准（如 nuScenes、KITTI、DDAD 等）都使用来自 LiDAR 传感器的地面真实深度，而不是来自 2 个摄像头的立体深度？ 将摄像头安装在汽车后视镜上会导致基线距离约为 2 米。这将实现更密集的深度测量，距离与 SOTA LiDAR 相似。我不明白为什么不经常使用它 - 还是我错过了什么？    提交人    /u/topsnek69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</guid>
      <pubDate>Tue, 28 May 2024 17:50:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入矩阵和最终的 pre-softmax 矩阵是否应该在 transformer 中共享？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</link>
      <description><![CDATA[大家好， 在比较各种 LLM 时，我们可以看到，在采用 softmax 获得预测的 token 概率之前，其中一些 LLM 对 token 嵌入和转换矩阵使用相同的矩阵。我发现这篇 2016 年的论文使用输出嵌入改进语言模型表明这种方法更优越，注意力就是您所需要的论文也引用了它并进行了这种权重共享。GPT2 和 Gemma 等其他模型也是如此。 这让我想知道为什么 LLaMa 模型不进行这种权重共享。就模型容量而言，在那里使用单独的矩阵是否值得？像 Gemma 这样的模型是否必须使用权重共享，因为它们使用了庞大的词汇量？我对这里的权衡很感兴趣，如果有的话，目前对这个主题的共识是什么。    提交人    /u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</guid>
      <pubDate>Tue, 28 May 2024 12:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>