<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 23 Apr 2024 21:12:14 GMT</lastBuildDate>
    <item>
      <title>需要帮助训练基于尖峰神经网络的 Transformer [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbekkg/need_help_in_training_spiking_neural_networks/</link>
      <description><![CDATA[      大家好，我正在训练一个基于尖峰神经网络的变压器，我使用尖峰果冻实现了它。我使用的损失是来自snntorch的ce_rate_loss。我正在对其进行 3 个时间步长和 10 个 epoch 的训练，你们能帮助我如何使这个损失曲线更好，因为它收敛得太快了......   由   提交 /u/Sweaty-Jackfruit1271   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbekkg/need_help_in_training_spiking_neural_networks/</guid>
      <pubDate>Tue, 23 Apr 2024 20:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 哪些行业最适合做人工智能研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbdvrz/research_best_industries_to_do_ai_research_in/</link>
      <description><![CDATA[我正在申请一所知名大学的研究实习，并被要求提出一个研究课题，我真的很困惑我应该提出这样的主题，因为我想以后有一个更以行业为导向的职业，而不是一个以研究为导向的职业。 我决定我将提出一个更偏向于研发的主题，而不是一个纯理论的。然而，我想知道目前哪些行业正在采用人工智能（医疗保健/汽车/金融等）并正在招聘新的工程毕业生。 提前致谢！   由   提交/u/untrustously-elegant  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbdvrz/research_best_industries_to_do_ai_research_in/</guid>
      <pubDate>Tue, 23 Apr 2024 19:44:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 语音转文本字级时间戳准确性问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbd8x1/d_speech_to_text_word_level_timestamps_accuracy/</link>
      <description><![CDATA[在转录方面，我使用 Whisper 取得了很大成功，但字级时间戳似乎有点不准确。根据我的理解（“Whisper 无法提供可靠的单词时间戳，因为像 Transformer 这样使用交叉熵训练标准的 END-TO-END 模型并不是为可靠地估计单词时间戳而设计的。” https://www.youtube.com/watch?v=H576iCWt1Co&amp;t=192s）对于我的用例，我需要精确的字级别时间戳，因为我正在特定单词之后插入音频。当我进行插入并且单词的后部位于另一侧时，这会成为问题。 示例：给定一个包含已转录语音的原始音频文件，如果我想插入一个剪辑在单词“France”的末尾，根据时间戳，单词“France”位于单词“France”的末尾。从 19.26 开始，到 19.85 结束，我将在 19.85 插入剪辑。然而，如果 France 的实际结束时间是 19.92，那么当我在 19.85 插入笑声时，我将在这里剩余的“France”，可能是“ce” （0.07），最后。 我很好奇是否有人遇到过类似的问题以及他们做了什么来解决这个问题？我已经尝试了一些开源版本的 Whisper，但仍然遇到了这个问题。   由   提交/u/Mindless-Ordinary485  /u/Mindless-Ordinary485 reddit.com/r/MachineLearning/comments/1cbd8x1/d_speech_to_text_word_level_timestamps_accuracy/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbd8x1/d_speech_to_text_word_level_timestamps_accuracy/</guid>
      <pubDate>Tue, 23 Apr 2024 19:18:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 吴的方法可以将符号人工智能提升到与银牌得主和 AlphaGeometry 竞争，从而超越 IMO Geometry 金牌得主</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbd2ol/r_wus_method_can_boost_symbolic_ai_to_rival/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.06405 代码：https:// /huggingface.co/datasets/bethgelab/simplegeometry 摘要：  证明几何定理构成了视觉推理结合的标志直觉和逻辑能力。因此，奥林匹克级别几何问题的自动定理证明被认为是人类级别自动推理的一个重要里程碑。 AlphaGeometry 的推出标志着一项重大突破，这是一种用 1 亿个合成样本训练的神经符号模型。它解决了 30 个国际数学奥林匹克 (IMO) 问题中的 25 个，而基于 Wu 的方法报告的基线仅解决了 10 个。在这篇文章中，我们重新审视了 AlphaGeometry 引入的 IMO-AG-30 挑战赛，发现 Wu 的方法出奇的强大。仅吴的方法就可以解决15个问题，其中一些问题是其他任何方法都无法解决的。这导致了两个关键发现：（i）将 Wu 的方法与演绎数据库和角度、比率和距离追踪的经典综合方法相结合，仅使用仅使用 CPU 的笔记本电脑在 5 分钟的时间限制内解决了 30 种方法中的 21 种每个问题。从本质上讲，这种经典方法仅比 AlphaGeometry 少解决 4 个问题，并建立了第一个完全符号化的基线，其强度足以与 IMO 银牌得主的表现相媲美。 (ii) Wu 的方法甚至解决了 AlphaGeometry 未能解决的 5 个问题中的 2 个问题。因此，通过将 AlphaGeometry 与 Wu 的方法相结合，我们在 IMO-AG-30 上建立了一种新的最先进的自动定理证明，解决了 30 个问题中的 27 个，这是第一个超越 IMO 金牌得主的人工智能方法.    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbd2ol/r_wus_method_can_boost_symbolic_ai_to_rival/</guid>
      <pubDate>Tue, 23 Apr 2024 19:11:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 无需模型对象即可生成形状贡献的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbbhbr/d_method_to_generate_shapely_contributions/</link>
      <description><![CDATA[是否有一种方法可以在不使用模型对象的情况下生成数据的 Shapely 值（或类似的值）的近似值。  本质上是我在基准数据上输入特征和模型预测，测试数据也是如此，输出是测试数据上每个特征的贡献   由   提交/u/ozymandias_514   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbbhbr/d_method_to_generate_shapely_contributions/</guid>
      <pubDate>Tue, 23 Apr 2024 18:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] Python 智能配置管理器。 Hydra+pydantic+lsp 的超集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb86x2/p_a_python_intelligence_config_manager_superset/</link>
      <description><![CDATA[我开发了一个非常强大的Python配置管理工具。它可以使您的 json 配置像 python 代码一样强大。而且对人类非常友好。  最吸引人的功能是它会分析 Python代码和json配置文件实时，提供文档显示、参数补全以及从转到Python定义 json 配置。 （由 LSP 提供支持） 与 Hydra 类似或更好的配置继承、参数引用和参数网格搜索  数据验证类似于pydantic，并且能够将数据类转换为json-schema  这个项目还处于早期阶段，欢迎大家提供一些建议和想法。 git 仓库   由   提交/u/cssunfu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb86x2/p_a_python_intelligence_config_manager_superset/</guid>
      <pubDate>Tue, 23 Apr 2024 15:57:30 GMT</pubDate>
    </item>
    <item>
      <title>[N] Phi-3-mini 在 HuggingFace 上发布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb7f9n/n_phi3mini_released_on_huggingface/</link>
      <description><![CDATA[https://huggingface .co/microsoft/Phi-3-mini-128k-instruct 技术报告中的数字看起来确实很棒，我想需要由第三方验证。 &lt; /div&gt;  由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb7f9n/n_phi3mini_released_on_huggingface/</guid>
      <pubDate>Tue, 23 Apr 2024 15:26:13 GMT</pubDate>
    </item>
    <item>
      <title>[D]超参数调整后准确度分数和其他测量值发生巨大变化。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb4yba/ddrastic_change_in_the_accuracy_score_and_other/</link>
      <description><![CDATA[嘿，我目前正在做恶意软件分类（恶意软件，良性）。使用朴素贝叶斯（伯努利）此时准确度为 67。之后调音后直接上升100。这正常吗？我使用 IQR 进行异常值去除，并使用相关性进行特征选择。   由   提交 /u/Saheenus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb4yba/ddrastic_change_in_the_accuracy_score_and_other/</guid>
      <pubDate>Tue, 23 Apr 2024 13:41:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何将 LLaMA 3 部署到生产中以及硬件要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb3ge1/d_how_to_and_deploy_llama_3_into_production_and/</link>
      <description><![CDATA[许多人都在尝试安装和部署自己的 LLaMA 3 模型，因此这里是我刚刚制作的教程，展示了如何在 AWS EC2 上部署 LLaMA 3实例：  https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-product.html 部署 LLaMA 3 8B 相当容易，但部署 LLaMA 3 70B 则很困难另一个野兽。考虑到所需的 VRAM 量，您可能需要配置多个 GPU 并使用 vLLM 等专用推理服务器，以便将模型拆分到多个 GPU 上。 LLaMA 3 8B 需要大约 16GB 的磁盘空间，并且FP16 中 20GB VRAM（GPU 内存）。至于LLaMA 3 70B，它需要大约140GB的磁盘空间和160GB的FP16 VRAM。 我希望它有用，如果您有疑问，请随时询问！ 朱利安   由   提交/u/juliensalinas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb3ge1/d_how_to_and_deploy_llama_3_into_production_and/</guid>
      <pubDate>Tue, 23 Apr 2024 12:33:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 作为 DS/MLE 单独工作的人应该牢记哪些最佳实践和工作流程？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb0fi1/d_what_best_practices_and_workflows_those_working/</link>
      <description><![CDATA[我想知道技术招聘人员或经验丰富的 DS/MLE 对像我这样的人有什么看法：良好的理论和良好的技术背景，但单独工作太长了。 我的职业生涯背景摘要：我作为 DS 工作了 8 年，前 3 年在中型研发和咨询团队（一家大型科技公司）工作，然后在过去 5 年内，作为相对成功的非人工智能初创企业的独立 DS，主要开发 ML/NLP 内容来解决特定问题或改进其产品的一项特定功能（即从来不是整个产品）。在我设计的5年里。开发和部署了 4 个模型（但尝试了许多 OFC）——以及一些仪表板和简单的流式化 POC）。  最近参加聚会，看到实际团队中的人们如何工作、讨论和交流知识，这突然让我感到震惊：我错过了，我正在变得过时。我对技术面试感觉不够敏锐，我不确定我开发和维护项目的方式是否遵循良好的标准/最佳实践（哎呀，我几乎不遵循看板，主要使用我的计划员向我的老板报告进度） 。我做了一些版本控制并记录了我放入产品中的内容，但即便如此，我也不确定我正在按照团队的预期进行操作。   由   提交/u/Melodic_Reality_646   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb0fi1/d_what_best_practices_and_workflows_those_working/</guid>
      <pubDate>Tue, 23 Apr 2024 09:40:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 门控长期记忆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1caywsz/d_gated_longterm_memory/</link>
      <description><![CDATA[      今天，我将展示我的最新想法：门控长期记忆 GLTM 单元 门控长期记忆试图实现一种高效的 LSTM 替代方案。与 LSTM 不同，GLTM 并行执行所有繁重的工作，唯一顺序执行的操作是乘法和加法操作。与变形金刚的二次存储器相比，门控长期存储器仅使用线性存储器。   由   提交/u/jessielesbian  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1caywsz/d_gated_longterm_memory/</guid>
      <pubDate>Tue, 23 Apr 2024 07:52:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] Zotero组织</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cayah8/d_zotero_organization/</link>
      <description><![CDATA[使用 Zotero 组织和阅读研究论文的人，你们如何使用集合、子集合或标签？ 字面意思，我想知道您正在研究什么（视觉、语言……）以及您正在使用哪些集合、子集合或标签以及如何使用？ 最近我开始使用 Zoteor，我真的很喜欢对此感到困惑。从其他人那里寻找灵感。提前致谢。   由   提交/u/Relative_Tip_3647   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cayah8/d_zotero_organization/</guid>
      <pubDate>Tue, 23 Apr 2024 07:10:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] Phi-3即将发布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cartvg/d_phi3_to_be_released_soon/</link>
      <description><![CDATA[从 MSFT 的两个独立消息来源（其中一个接近 Sebastien Bubeck）了解到即将推出的 Phi-3 模型：  三个不同大小的模型（最多 14B） 同样，主要是合成和 LLM 增强的训练数据 显然在训练方面有一些升级技​​术 否更多 Apache 2，但更严格的许可证（类似于 llama3） Mixtral 级别性能，参数少得多  我想看看是否有人有更多有关模型的内部信息.   由   提交/u/yusuf-bengio  /u/yusuf-bengio  reddit.com/r/MachineLearning/comments/1cartvg/d_phi3_to_be_released_soon/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cartvg/d_phi3_to_be_released_soon/</guid>
      <pubDate>Tue, 23 Apr 2024 01:13:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] Llama-3 可能刚刚杀死了专有的人工智能模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cad7kk/d_llama3_may_have_just_killed_proprietary_ai/</link>
      <description><![CDATA[完整博客文章  Meta 在三天前发布了 Llama-3，感觉开源模型终于缩小了与专有模型的差距，这已经是一个拐点。初始基准测试显示 Llama-3 70B 在许多任务中非常接近 GPT-4：  官方元页面仅显示Llama-3优于Gemini 1.5和Claude Sonnet。 人工分析显示 Llama-3 的质量介于 Gemini-1.5 和 Opus/GPT-4 之间。 关于 LMSYS 聊天机器人竞技场排行榜，Llama-3 排名第 5，而当前的 GPT-4 模型和 Claude Opus 仍并列第 1。  功能更强大Llama-3 400B+ 模型仍在训练中，发布后很可能超越 GPT-4 和 Opus。 Meta vs OpenAI 有人推测 Meta 从一开始的目标就是瞄准OpenAI 采用“焦土”方法，通过发布强大的开放模型来扰乱竞争格局并避免在竞争中落后AI 竞赛。 Meta 在计算和人才方面可能会超过 OpenAI：  OpenAI 的预计收入为 20 亿美元，并且可能无利可图。 2023 年，Meta 的收入为 $134B，利润为 $39B。 Meta 的计算资源目前可能超过 OpenAI。 开源可能会吸引更好的人才和研究人员。  &gt;  一个可能的结果是微软收购 OpenAI 以赶上 Meta。谷歌也在进军开放模型领域，并拥有与 Meta 类似的功能。看看它们适合什么位置将会很有趣。 获胜者：开发人员和人工智能产品初创公司 我最近写了一篇关于现在建立人工智能初创公司令人兴奋，因为您的产品会随着每个主要模型的进步而自动改进。随着 Llama-3 的发布，开发人员的机会更大：  不再受供应商锁定。 开发人员不仅可以封装专有 API 端点，还可以现在以一种非常经济高效且高性能的方式将人工智能深度集成到他们的产品中。 Hugging Face 上已经有超过 800 个 llama-3 模型变体，而且看起来每个人都能够针对他们的使用案例、语言或行业进行微调。 更快、更便宜的硬件：Groq 现在每秒可以生成 800 个 llama-3 代币，而成本只是 GPT 成本的一小部分。以低价提供近乎即时的 LLM 响应即将到来。  视觉和视频的开源多模式模型仍然需要迎头赶上，但我预计这很快就会发生。 Llama-3 的发布标志着人工智能民主化的一个重要里程碑，但现在宣布专有模型的消亡可能还为时过早。谁知道呢，也许 GPT-5 会让我们所有人感到惊讶，并超越我们对 Transformer 模型功能的想象。 这绝对是人工智能领域构建的超级激动人心的时代！    由   提交 /u/madredditscientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cad7kk/d_llama3_may_have_just_killed_proprietary_ai/</guid>
      <pubDate>Mon, 22 Apr 2024 15:08:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>