<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 04 Mar 2024 00:57:19 GMT</lastBuildDate>
    <item>
      <title>[D] 这里有人知道中间结果吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5xfwl/d_does_anyone_here_know_intermediate_result/</link>
      <description><![CDATA[我的外部要求我输入中间结果。我正在尝试使用 tf-idf 和手套进行假新闻分类。文本分类的中间结果是什么？我没听懂。   由   提交 /u/Kindly-Song5246    reddit.com/r/MachineLearning/comments/1b5xfwl/d_does_anyone_here_know_intermediate_result/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5xfwl/d_does_anyone_here_know_intermediate_result/</guid>
      <pubDate>Mon, 04 Mar 2024 00:50:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] Minedojo sim 速度设置和/或会话视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5wyu4/d_minedojo_sim_speed_setting_and_or_video_of/</link>
      <description><![CDATA[以前使用过minedojo作为与minecraft环境交互的代理的人，您是否找到了设置过程模拟速度的方法？另外，有什么办法可以获取正在运行的游戏的渲染视频吗？我一直在搜索文档和代码实现，但一无所获，但我看到了几个演示视频，人们在其中演示了代理的实时执行。任何建议将不胜感激。   由   提交/u/Open-Ad2530  /u/Open-Ad2530  reddit.com/r/MachineLearning/comments/1b5wyu4/d_minedojo_sim_speed_setting_and_or_video_of/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5wyu4/d_minedojo_sim_speed_setting_and_or_video_of/</guid>
      <pubDate>Mon, 04 Mar 2024 00:28:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 计算机视觉挑战：验证行和列中的检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5vltm/p_computer_vision_challenge_validate_detections/</link>
      <description><![CDATA[       由   提交/u/zerojames_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5vltm/p_computer_vision_challenge_validate_detections/</guid>
      <pubDate>Sun, 03 Mar 2024 23:28:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与传统的 MLP 相比，DCN 有用吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5va25/d_how_useful_is_dcn_compared_to_good_old_mlp/</link>
      <description><![CDATA[在工业大规模搜索/推荐环境中，与更直接的 MLP 相比，DCN 似乎仍然很受欢迎。这个想法或多或少有点像 ResNet，原始的原始输入不断出现在每一层。但是原始输入的层输出的逐元素乘积真的会增加任何价值吗？如果是这样，为什么变压器架构不采用它？   由   提交/u/Crazy_Suspect_9512   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5va25/d_how_useful_is_dcn_compared_to_good_old_mlp/</guid>
      <pubDate>Sun, 03 Mar 2024 23:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neural Attention 从最基本的第一原理开始</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5qdfy/d_neural_attention_from_the_most_fundamental/</link>
      <description><![CDATA[   分享我的 YT 中的一个视频，解释了Attention 架构在 NLP 和 Transformer 中变得如此普遍之前。建立在首要原则的基础上，并一直到一些更先进的（和当前相关的）概念。为那些正在寻找类似内容的人提供链接。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5qdfy/d_neural_attention_from_the_most_fundamental/</guid>
      <pubDate>Sun, 03 Mar 2024 19:59:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人形机器人跳舞、高五、挥手、拥抱</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5psws/r_humanoid_robot_dancing_high_five_waving_hugging/</link>
      <description><![CDATA[   /u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5psws/r_humanoid_robot_dancing_high_five_waving_hugging/</guid>
      <pubDate>Sun, 03 Mar 2024 19:37:13 GMT</pubDate>
    </item>
    <item>
      <title>[N] LLM 模型高达 7 倍加速。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5m4oc/n_llm_models_up_to_7_times_acceleration/</link>
      <description><![CDATA[   /u/idlemonk111  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5m4oc/n_llm_models_up_to_7_times_acceleration/</guid>
      <pubDate>Sun, 03 Mar 2024 17:08:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 实施的类型及其好处？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5l18k/d_types_of_rag_implementations_and_their_benefits/</link>
      <description><![CDATA[大家好， 我最近深入研究了 RAG（检索增强生成）的世界，发现了它在增强能力方面的巨大潜力聊天机器人和其他生成模型。然而，我很好奇这种方法的各种类型的实现，以及我们如何确定哪种方法最有效。令人惊讶的是，网上有关不同方法的信息有限。任何人都可以阐明这一点并分享有关确定最佳实施的见解吗？我们将非常感谢您的专业知识！ 提前致谢！   由   提交 /u/Lyriciseofficial   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5l18k/d_types_of_rag_implementations_and_their_benefits/</guid>
      <pubDate>Sun, 03 Mar 2024 16:22:10 GMT</pubDate>
    </item>
    <item>
      <title>[P] LaVague：开源 Text2Action AI 管道，将自然语言转换为 Selenium 代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/</link>
      <description><![CDATA[      🌊 发布#LaVague，完全开源的 AI 管道，用于转换自然语言到浏览器操作中！ 在不到 150 行代码（具有本地嵌入的 RAG + Hugging Face API 上的 Mixtral）中，它根据用户查询生成 Selenium 代码。在此 GIF 中，您可以看到它按照用户说明命令浏览器浏览 HF 网站！ https:// i.redd.it/vf91c2if25mc1.gif 在 Colab 上尝试：https://colab.research.google.com/github/dhuynh95/LaVague/blob/main/LaVague.ipynb GitHub：https://github.com/dhuynh95/LaVague 非常令人兴奋的是如何创建一个可以执行操作的人工智能助手对于我们来说，例如登录政府帐户、填写表格或提取个人信息！ 周末使用开源工具进行黑客攻击非常有趣，从 Hugging Face 本地嵌入变压器 + HF Pro API，通过 Mistral Mixtral 模型，使用 Llama Index 进行 RAG！ 一些挑战：为了让它在 Colab 上运行，以应对 GPU 较差的，我不得不求助于 HF Inference API Mixtral，因为它是唯一足够好的模型（gemma-7b 没有成功并且拒绝生成代码）。 因为我使用了现成的模型，所以我必须用很少的时间来提高性能通过射击学习和思想链，该模型成功生成了适当的代码！ 潜在的后续步骤：微调 2b 或 7b 模型以完全在本地运行，以便每个人都可以从透明且可定制的 AI 代理中受益为我们采取行动 我可能很难完全发展这个项目，所以我愿意贡献:)  &amp;# 32；由   提交 /u/Separate-Still3770    reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5k07k/p_lavague_opensource_text2action_ai_pipeline_to/</guid>
      <pubDate>Sun, 03 Mar 2024 15:38:00 GMT</pubDate>
    </item>
    <item>
      <title>[D]Agents+ LLM 正在生产中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5j322/dagents_llm_in_production/</link>
      <description><![CDATA[大家好， 我注意到 Agents+LLM 最近越来越受欢迎。  &lt; li&gt;谁能分享一下在生产工作负载中使用了哪些 Agentic 框架吗？ 你们中有人将 RAG 框架与 Agent 结合使用吗？ 此外，还有哪些典型的生产问题您在使用代理框架时遇到过什么问题吗？  请分享您的想法和见解   由   提交/u/Electrical_Study_617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5j322/dagents_llm_in_production/</guid>
      <pubDate>Sun, 03 Mar 2024 14:57:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2024 年 2 月研究论文 — LoRA 的潜在继任者、小型微调 LLM 与通才 LLM 以及透明的 LLM 研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5hvmh/p_research_papers_in_february_2024_a_potential/</link>
      <description><![CDATA[ 由   提交/u/seraschka  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5hvmh/p_research_papers_in_february_2024_a_potential/</guid>
      <pubDate>Sun, 03 Mar 2024 14:02:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何发现或推测“奖励黑客”现象？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5hrjy/d_how_to_detect_or_speculate_the_rewardhacking/</link>
      <description><![CDATA[      我正在阅读 DeepMind 论文“WARM: On the Benefits of Weight Averaged Reward”模型”。论文讨论的是奖励黑客现象。 论文中，作者使用 KL-奖励曲线来检测奖励黑客现象，表示奖励开始减少，从而发生奖励黑客行为。但是，之前的论文如 https://arxiv.org/pdf/2312.09244.pdf 或 https://arxiv.org/pdf/2312.09244.pdf通常使用两种奖励模型来检测奖励黑客：代理奖励和真实奖励。策略模型是在代理奖励下更新的，所以当KL增加时，代理奖励也会增加。当真实奖励开始减少时，奖励黑客现象就会发生，这被视为检测器。 因此，由于 WARM 只显示奖励以增加开始并以减少结束（这是代理奖励和因此还不够），我认为它不能显示奖励黑客现象的发生。希望您的意见。 https： //preview.redd.it/4ajcprcjk4mc1.jpg?width=824&amp;format=pjpg&amp;auto=webp&amp;s=756876807aa48d9fffe284f1f51f6ebfd6d877e6  /u/zetiansss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5hrjy/d_how_to_detect_or_speculate_the_rewardhacking/</guid>
      <pubDate>Sun, 03 Mar 2024 13:57:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：持续强化学习和元强化学习研究社区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5fmgj/d_seeking_advice_continualrl_and_metarl_research/</link>
      <description><![CDATA[我对 RL（连续 RL、元 RL、变压器）对超参数的敏感性和大量的训练时间越来越感到沮丧（我讨厌 RL 之后5年博士研究）。这在元强化学习连续强化学习中尤其成问题，其中一些基准要求长达 100 小时的训练。这使得优化超参数或快速验证新想法的空间很小。考虑到这些挑战以及我准备更深入地探索数学理论，包括学习所有可用的在线数学课程，采用基于证明的方法，以避免无休止的等待和训练循环，我对 2024 年密切相关的人工智能研究领域趋势感到好奇强化学习，但最多只需要 3 个小时的训练时间。有什么建议吗？   由   提交 /u/Noprocr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5fmgj/d_seeking_advice_continualrl_and_metarl_research/</guid>
      <pubDate>Sun, 03 Mar 2024 12:03:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的 LLM 技术堆栈在生产中是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/</link>
      <description><![CDATA[想知道每个人都使用什么来实现 LLM 支持的应用程序以供生产使用，以及您对这些工具和建议的体验。  这就是我为金融和资本市场用户构建的一些 RAG 原型所使用的。 预处理\ETL：非结构化.io + Spark、Airflow 嵌入模型： Cohere Embed v3 以前使用 OpenAI Ada，但 Cohere 对于我的用例来说具有明显更好的检索召回率和精度。还探索其他开放权重嵌入模型 矢量数据库： Elasticsearch 以前但现在使用 Pinecone LLM： 经历了相当长的一段时间很少包括托管和自托管选项。在原型设计过程中早期使用 gpt4，然后切换到 gpt3.5-turbo，以获得更易于管理的成本并最终开放权重模型。  现在使用由 vLLM 自托管的经过微调的 Llama2 70B 模型 LLM 框架：最初从 Langchain 开始，但发现扩展为应用程序很麻烦变得更加复杂。在某个时候尝试在 LlamaIndex 中实现它只是为了学习，但发现它同样糟糕。回到 Langchain，现在我正在用自己的逻辑替换它 其他人都使用什么？ 编辑：正确的型号 Llama2 70B   由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/</guid>
      <pubDate>Sat, 02 Mar 2024 16:37:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>