<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 29 Nov 2023 06:18:51 GMT</lastBuildDate>
    <item>
      <title>[P] 一段从头开始讲述人工智能历史的视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186hq4s/p_a_video_covering_the_history_of_artificial/</link>
      <description><![CDATA[我在这 2.5 小时的“完整”活动中度过了一段愉快的时光。人工智能的历史。 https://www.youtube.com/watch?v=k72eqhhlfbU&lt; /a&gt; 由于作者显然不是机器学习研究员，而是天体物理学家，我认为这是对这个主题的相当彻底的处理（如果有偏见和不敬的话）。非常适合这么小的 YouTube 频道。   由   提交/u/larenspear  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186hq4s/p_a_video_covering_the_history_of_artificial/</guid>
      <pubDate>Wed, 29 Nov 2023 05:17:01 GMT</pubDate>
    </item>
    <item>
      <title>请帮忙[项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186hkym/please_help_project/</link>
      <description><![CDATA[您好，我正在上哲学课，这是毕业所需的最后一门课。我正在寻找一位导师来帮助我解决练习题，因为期末考试占我成绩的 50%。问题涉及贝叶斯网络、DAG、概率和推导。如果有人觉得这听起来很熟悉，请告诉我。我真的需要帮助，除了这门课外，我的所有课程都是 A。   由   提交 /u/Missvelveteenrabbit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186hkym/please_help_project/</guid>
      <pubDate>Wed, 29 Nov 2023 05:09:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于生成高质量音频的新型声码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186gein/r_novel_vocoder_for_highquality_audio_generation/</link>
      <description><![CDATA[      引入用于高保真声码器的多尺度子带恒定 Q 变换鉴别器 我们提出了一种新颖的鉴别器，提高音频质量，同时不影响现有声码器推理实现（例如延迟或推理语音） https://preview.redd.it/ikn4ntgx683c1.png?width=1091&amp;format=png&amp;auto=webp&amp;s=9159613dc32c665fcd6ab7d53cf6 22d97f0907e8 论文：https://arxiv.org/abs/2311.14957 演示：https://vocodexelysium.github.io/MS-SB-CQTD/ 代码：https://github.com/open-mmlab/Amphion/blob/main/models/vocoders/gan /discriminator/mssbcqtd.py   由   提交/u/gnehzihz   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186gein/r_novel_vocoder_for_highquality_audio_generation/</guid>
      <pubDate>Wed, 29 Nov 2023 04:04:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] kaggel是学习机器学习的好平台吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186eek5/d_is_kaggel_a_good_platform_to_learn_machine/</link>
      <description><![CDATA[我是一名网络安全学生，我对机器学习和人工智能感兴趣......所以我想自学，有些人确实推荐了 Kaggel。你觉得怎么样？   由   提交 /u/Dangerous_Wind8441   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186eek5/d_is_kaggel_a_good_platform_to_learn_machine/</guid>
      <pubDate>Wed, 29 Nov 2023 02:31:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 请指导我</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186dytr/d_guide_me_please/</link>
      <description><![CDATA[嘿伙计们。我是计算机科学专业的新生。我刚刚通过在 Coursera 上观看 Andrew ng 的视频开始机器学习（我的意思是我正在第一周）。我知道我在这个领域有点晚了。你们能给我路线图 1. 我应该如何开始？ 2. 我应该给多少时间？ 3.留学生实习真的像人们说的那么难吗？ 3. 如果你必须重新开始学习，你能说出你会避免的遗憾吗？ 4.任何对我有帮助的建议。 提前谢谢您。   由   提交/u/Unfair_Statement3278   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186dytr/d_guide_me_please/</guid>
      <pubDate>Wed, 29 Nov 2023 02:10:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果运行时间或 GPU 内存使用没有显着减少，那么参数高效微调的动机是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</link>
      <description><![CDATA[我一直在尝试诸如提示调整和 LoRA 之类的方法，这些方法的参数效率很高，因为它们只微调很小的一部分（即是所有参数的&lt;1%）。 但是对于这两种方法，您必须在反向传播期间缓存中间梯度，这意味着您在微调期间（或在由于无需存储冻结层的优化器状态，​​因此节省了大部分 GPU 内存）。例如，我已经让 LoRA 将我的自定义模型的 GPU 内存占用量从 8.5GB 减少到了 8.5GB。 8.1GB，非常小。微调时间减少也并不是真正的主要优势，每批微调同一模型减少了 20 毫秒，从 210 毫秒减少到 190 毫秒。 这引出了一个问题 - 实际原因是什么？参数高效微调（例如，带有 1.6k+ 引用的提示调整）的流行，如果它不能真正节省 GPU 内存和训练时间？ 我可以看到两个可能的原因（但我不太相信他们真的解释了围绕参数高效微调的“炒作”）：  下游任务的微调模型检查点显着减少。例如，在提示调整中，我们只需要在硬盘/SSD 上保存经过训练的微小软提示（〜很少兆字节），而不是整个更改后的模型权重（〜很多很多 GB）。  但从实际角度来看，我觉得大多数人都缺乏计算能力（例如 GPU 内存），而不是硬盘空间。换句话说，训练时间和 GPU 内存消耗似乎比节省检查点存储空间更相关。  第二个是域转移的鲁棒性（因为我们是保留大部分原始模型的权重，而不是破坏性地重新学习它们），这一点在提示调整论文中提到过，但在 LoRA 论文中却没有提及太多。  我可以认为这是一个可能的原因，但是在分布外设置中的提示调优论文中的性能增益充其量是微乎其微的，并且 LoRA 没有提到域转换。   （编辑 - 我还想知道是否还缺少其他东西来减少 GPU 内存和运行时间？我听说 QLoRA 增加了 4- LoRA 之上模型的位量化，所以也许这是解决 LoRA 内存效率问题的一种方法。但我不知道是否有什么可以减少内存占用以进行快速调整？）   由   提交/u/patricky168  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/186ck5k/d_what_is_the_motivation_for_parameterefficient/</guid>
      <pubDate>Wed, 29 Nov 2023 01:06:13 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]知识蒸馏定义不好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18681i4/discussion_knowledge_distillation_is_badly_defined/</link>
      <description><![CDATA[或者不那么挑衅地说，知识蒸馏是一系列没有明确目标的技术。 在我看来，在文献中通常考虑两种类型的知识蒸馏目标：  让学生模型向教师模型学习，以在教师训练数据集上重现教师的答案（非常重要） 让学生模型向教师模型学习，以重现教师在另一个下游数据集上预测的答案，因此教师和学生都不会“看到” ;之前的那些数据点  但是，我可以想象另一个我认为合理但似乎没有进行太多探索的目标：  拥有学生模型复制教师的输出到处，也就是说，训练学生对任何数据点给出与教师相同的答案。这可以通过将损失函数设置为教师和学生预测之间均方差的 x 积分来实现。我真的不知道如何解决这个特定问题，也许使用重要性采样？  如果有人知道尝试（3）的论文，请发布链接。  如果有人知道尝试（3）的论文，请发布链接。 p&gt; 讨论 知识蒸馏可能具有这三个目标之一（甚至可能是另一个目标）。根据实际用于训练学生的目标，结果应该会有很大差异。 教师在 A 组上进行训练，学生在 A 组上进行训练 在 (1) 的情况下，考虑到学生通常较小或具有更简单的架构，学生不会超越教师也就不足为奇了。如果学生在与老师相同的测试集上进行评估，那么学生就无法在他的领域击败老师。然而，学生可能完全无法匹配教师对分布外 (OOD) 数据的预测，因为它从未接受过 OOD 样本的训练。在这些情况下，我不确定学生是否会超越老师。 老师在 A 组上训练，学生在 B 组上训练 在（2）的情况下，考虑到新的下游分布通常与训练分布有很大不同，学生表现优于老师我不会感到惊讶，因此老师永远没有机会从这个新数据集中学习，但是学生会同时获得先前的信息（通过老师）和新的信息（通过输入，成为梯度，成为参数更新）。我什至想说，在这种情况下，一个体型相似的学生会被老师殴打，这将是令人惊讶的。 这就是为什么我认为如果我们在被打之前承认这种潜在影响会更好。令人惊讶的是，学生模型的表现优于教师。这在法学硕士接受其他法学硕士生成的数据训练的时代似乎尤为重要。在我看来，人们高估了蒸馏的效果，而实际上它可能只是迁移学习。性能的提高将归因于额外数据的使用，或者生成数据的选择机制，而不是蒸馏行为。 教师在 A 组上接受培训，学生在 A 组上接受培训整个输入域 对于（3），我想这实际上是知识蒸馏的最真实形式，因为目标实际上是在整个域上复制（非常复杂的）函数而不是只是一小部分。我想这就是所谓的“模型压缩”。在文献中，但我从未见过这样的问题。这种方法对于分布外的样本应该是稳健的，因为学生应该做出与老师相同的预测。换句话说，如果老师对 OOD 数据稳健，那么学生也应该如此。 我希望学生在任何给定的测试集上都比老师表现得更好一些，因为学生应该“顺利”出”教师做出的任何异常（伪影）预测并创建一种正则化形式。不过我可能是错的。 请随意发表您对此的想法。   由   提交 /u/Cosmolithe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18681i4/discussion_knowledge_distillation_is_badly_defined/</guid>
      <pubDate>Tue, 28 Nov 2023 21:58:04 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]您发现自己每周在工作的哪一部分上浪费时间最多？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/</link>
      <description><![CDATA[不询问拖延症、实际工作职责。对我来说，它必须处理电子表格和演示文稿。您的工作流程中存在哪些瓶颈？   由   提交/u/zero-true  /u/zero-true reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1863igy/discussion_what_part_of_your_job_do_you_finding/</guid>
      <pubDate>Tue, 28 Nov 2023 18:52:33 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的 AMD GPU [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1863brm/amd_gpu_for_machine_learning_d/</link>
      <description><![CDATA[我是一名计算机科学学生，最近为我的第一台游戏电脑购买了组件。我发现 6800 的价格不错，相当不错，但我想知道面向机器学习的库（和其他相关的东西）是否会出现问题，因为我发现 NVIDIA GPU 更适合它。如果是这样，我是否仍然可以使用我拥有的 AMD GPU 获得不错的结果，还是应该更改它？   由   提交 /u/the_fabbest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1863brm/amd_gpu_for_machine_learning_d/</guid>
      <pubDate>Tue, 28 Nov 2023 18:44:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] minOFT：一个易于使用的 PyTorch 库，用于将正交微调 (OFT) 应用于 PyTorch 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1862am5/p_minoft_an_easytouse_pytorch_library_for/</link>
      <description><![CDATA[嗨r/MachineLearning， 我想分享我在微调语言模型（正交微调）研究中遇到的一项非常有趣的工作的开源实现。 正交微调 (OFT) 是 LoRA 的一种更强大、稳定且样本效率更高的替代方案，LoRA 最初是为微调扩散模型而开发的。 LoRA 通过添加两个低秩矩阵的乘积来更新预训练权重矩阵，而 OFT 将预训练层权重乘以可学习的正交矩阵以应用约束变换。 OFT 的作者最近表明，这种方法（通过名为 butterfly OFT 的巧妙改进）也适用于视觉转换器和语言模型。 灵感来自minLoRA，我认为最好有一个最小的开源存储库来测试并在微调时比较 OFT 与 LoRA语言模型。它也是由 Andrej Karpathy 在 nanoGPT 之上构建的。该库可通过 pip 安装，并且可以与任何 PyTorch 模型（包括 Hugging Face 模型）通用，就像 minLoRA 一样。 欢迎提供反馈和贡献！您可以在下面尝试一下： https://github.com/alif-munim/minOFT   由   提交/u/0blue2brown   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1862am5/p_minoft_an_easytouse_pytorch_library_for/</guid>
      <pubDate>Tue, 28 Nov 2023 18:01:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] 对齐即代码：使 LLM 应用程序与 Tanuki 一起运行。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1861p30/p_alignmentascode_making_llm_applications_behave/</link>
      <description><![CDATA[我是 Tanuki 的贡献者，一个项目，允许您使用 Python 中的测试驱动语法以声明方式定义 LLM 行为。 通过指定 LLM 必须作为测试履行的合同，它有助于减少 MLOps 并使您能够使用标准的开发操作流程将模型的行为与您的要求保持一致。 此外，这些对齐语句有助于自动师生模型蒸馏，以将成本和延迟降低多达 10 倍（请参阅基准）。  非常感谢任何想法或反馈。   由   提交 /u/Noddybear   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1861p30/p_alignmentascode_making_llm_applications_behave/</guid>
      <pubDate>Tue, 28 Nov 2023 17:37:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 具有 2d 旋转嵌入的交叉轴变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18607oa/r_crossaxis_transformer_with_2d_rotary_embeddings/</link>
      <description><![CDATA[ 由   提交/u/lilyerickson  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18607oa/r_crossaxis_transformer_with_2d_rotary_embeddings/</guid>
      <pubDate>Tue, 28 Nov 2023 16:34:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师加薪？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/185y5wn/d_machine_learning_engineer_salary_increase/</link>
      <description><![CDATA[大家好，我去年大学毕业，一直在佛罗里达州的一家公司担任机器学习工程师。我一年赚7.6万。该公司提供硕士学位学费报销。通常情况下，获得硕士学位后，您的加薪是多少？ 后续问题：从在线大学获得硕士学位（我仍然会全职工作）的声望是否会低于从在线大学获得硕士学位？亲自？ 请问，如果您愿意的话，是否有人介意直接分享他们大学毕业后的个人薪资数据以及在整个职业生涯中的进展情况？   由   提交/u/Fluid-Pipe-2831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/185y5wn/d_machine_learning_engineer_salary_increase/</guid>
      <pubDate>Tue, 28 Nov 2023 15:06:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2023机构排名</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/185pdax/d_neurips_2023_institutions_ranking/</link>
      <description><![CDATA[       由   提交/u/Roland31415   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/185pdax/d_neurips_2023_institutions_ranking/</guid>
      <pubDate>Tue, 28 Nov 2023 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>