<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 18 Sep 2024 06:23:38 GMT</lastBuildDate>
    <item>
      <title>[D] 对 o1 的思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fjl422/d_reflections_on_o1/</link>
      <description><![CDATA[很多人发帖说 o1 在他们的“私人 AGI/推理基准”上取得了“突破”，或者它已经击败了一些学术基准（这很好），但你觉得 o1 在现实生活中最有用的地方是什么？ 我不知道是不是只有我一个人这样，但我不太确定如何使用它。按照今天的标准，我不一定想为可能存在错误的代码等待太久，而我又懒得写。 我发现我喜欢 Gemini 等法学硕士的一件事是，我可以将一堆论文扔到它的 2M 上下文窗口中，这样它就不会产生幻觉，并能快速合理地为我提供问题的摘要/答案。也许未来的版本会有这个，可接受的延迟和高级图像分析（这很酷）.. 此外，我不知道这种方法将如何带我们走向 AGI（95％ 的白领工作自动化）.. 就像我们已经看到的那样，它的性能不会转移到非数学/stem 问题上，并且您需要某种验证器来在现实世界中训练这样的模型（不是游戏或数学）。最好的验证者通常是专家的评估，它不一定具有可扩展性……并且您需要针对新任务进行更新。想法？截至目前，我同意 Terence Tao 在他最近的帖子中的观点。 我希望看到正交运作的是某种持续学习，而不是静态的 LLM，您可以指导它以在您关心的某些领域达到同事的水平。我毫不怀疑随着时间的推移我们会实现这一点，但很难不感到惋惜。    提交人    /u/HybridRxN   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fjl422/d_reflections_on_o1/</guid>
      <pubDate>Wed, 18 Sep 2024 05:07:38 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] - Mojo/Modular，有人在实际项目中使用过它吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fji12z/discussion_mojo_modular_has_anyone_used_it_in_a/</link>
      <description><![CDATA[首先，我应该说我是 Chris Lattners 的忠实粉丝。 但是尽管 Mojo 是 Python 的超集，我个人并不认为我会学习它。 我反对学习新语言的主要理由是我使用 LLM 来完成代码中的所有输入...在 LLM 能够编写非常复杂的 C++ 或 Rust 之前，我认为 Modular 不会为 Mojo 培训开发人员。 所以我认为学习另一种语言毫无意义。 简而言之，我认为 Mojo 是在错误的时间提出的正确解决方案。 当然，有人可能会争辩说，从长远来看，LLM 也可以编写 Mojo，而且由于 MLIR 编译器的支持，它可能会成为一种更好的语言。我的观点是，要为一种语言构建可靠的 LLM，您需要相当多的现有代码，而 Mojo 在未来 5-10 年内将会缺乏这些代码……    提交人    /u/zarrasvand   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fji12z/discussion_mojo_modular_has_anyone_used_it_in_a/</guid>
      <pubDate>Wed, 18 Sep 2024 02:18:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用梯度工程强制模型参数稀疏性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fjgfhn/r_force_sparsity_in_model_parameters_with/</link>
      <description><![CDATA[是否有人尝试过在训练后期强制网络实现真正的稀疏性？ 我指的是，在训练稳定下来后，您开始执行梯度工程，选择最接近零的权重组，它们的梯度在剩余的训练中将始终指向零，一旦它们达到 0，它们就会被冻结。重复该过程。 我很好奇这是否曾经被尝试过，如果是，结果是什么。 这种方法的一些可能动机：生物大脑中的突触修剪，压缩模型权重以进行分布。    提交人    /u/FunSavings4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fjgfhn/r_force_sparsity_in_model_parameters_with/</guid>
      <pubDate>Wed, 18 Sep 2024 00:59:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人知道任何真正的多模式大型语言模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fjay5c/d_does_anyone_know_about_any_true_multimodal/</link>
      <description><![CDATA[大家好， 我对测试多模态大型语言模型以检测偏见和幻觉很感兴趣，具体方法是使用语音作为输入，并让模型对语音输入进行推理以产生文本或语音输出。模型必须是真正的多模态，因为我想检查网络内的偏见相互作用。我已经知道文本偏见是存在的，所以简单的语音转文本，然后是 LLM，并不是我想要的。 由于 OpenAI 的语音助手尚未向公众发布，我很好奇其他可以用于此目的的模型。我曾见过 Qwen2-Audio-7B-Instruct (https://arxiv.org/abs/2407.10759 https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct )，但我不确定如何找到更多这样的模型，或者你们是否有我可以测试的模型建议。任何建议都将不胜感激！    提交人    /u/Constant-Working5468   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fjay5c/d_does_anyone_know_about_any_true_multimodal/</guid>
      <pubDate>Tue, 17 Sep 2024 21:09:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] vLLM 批处理与前缀缓存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fjao60/d_vllm_batching_vs_prefix_caching/</link>
      <description><![CDATA[嗨，试图弄清楚 vLLM 实现中批处理和前缀缓存之间的区别，特别是它们是否可以一起使用。 对于上下文：- 尝试使用 LLM 模型回答文档上的二元分类问题（例如“基于上下文 x 是真还是假？”）并构建问题的 json 作为特征键和分类。对于像 chatGPT 这样的较大模型，它通过使用带有输出格式和字段描述的 pydantic json 模式提示来帮助模型对每个特征进行分类，效果很好。为了实现可扩展性和更多控制，现在正在考虑使用自托管的较小开源模型。 我发现当使用更具体的范围（即单个真/假问题）提示时，较小的模型会更好地工作。因此，我计划迭代多个请求并将分类构建为二进制任务并从响应中构造输出 json，而不是要求模型生成输出。 对于 vLLM，我希望前缀缓存可以通过在末尾附加每个任务来加快迭代速度，并且我认为批处理在此任务中无法表现更好？ 如果作为上下文包含的文档太长，我可以将文档的各块批处理为并行输入，并使用每个块的缓存来迭代任务吗？    提交人    /u/Lower_Tutor5470   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fjao60/d_vllm_batching_vs_prefix_caching/</guid>
      <pubDate>Tue, 17 Sep 2024 20:58:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Pandas / Dask 等为 ML / DS 加密数据。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj5gxf/p_encrypting_data_for_ml_ds_with_pandas_dask_etc/</link>
      <description><![CDATA[* 由于标题中缺少标签而重新发布 刚刚发布了一个模块，使通过 Python / Pandas / Dask / CLI 和云资源进行数据加密变得更容易。我们已经在 fsspec 上实现了 AES-256 CBC https://pypi.org/project/fsspec-encrypted/ 来源 https://github.com/thevgergroup/fsspec-encrypted 许可 MIT 允许轻松在本地或远程读取和写入 例如 import pandas as pd from fsspec_encrypted.fs_enc_cli import generate_key crypto_key = generate_key(passphrase=&quot;my_secret_passphrase&quot;, salt=b&quot;12345432&quot;) #local df = pd.read_csv(f&#39;enc://./.encfs/encrypted-file.csv&#39;, storage_options={&quot;encryption_key&quot;: crypto_key}) # 用 fsspec-encrypted 包装的 S3 请求 df = pd.read_csv(f&#39;enc://s3://{bucket}/encrypted-file.csv&#39;, storage_options={&quot;encryption_key&quot;: crypto_key}) # 与 gcs、abfs、adl、az、hf 等类似。  甚至有一个 CLI，因此脚本编写可以更容易，并允许您即时加密/解密 即将推出更多更新。  我们的目标是帮助减少未加密存储在磁盘上的 PII/PHI 或其他敏感数据的数量。   由    /u/olearyboy  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj5gxf/p_encrypting_data_for_ml_ds_with_pandas_dask_etc/</guid>
      <pubDate>Tue, 17 Sep 2024 17:35:55 GMT</pubDate>
    </item>
    <item>
      <title>[N] 介绍 CodonTransformer：一种使用上下文感知神经网络的多物种密码子优化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj58d1/n_introducing_codontransformer_a_multispecies/</link>
      <description><![CDATA[大家好，机器学习社区， 我很高兴与大家分享我们的最新成果：CodonTransformer，这是一种最先进的 Transformer 模型，可优化 DNA 序列，以实现 164 个物种 中的异源蛋白质表达！ 什么是 CodonTransformer？ CodonTransformer 利用深度学习生成具有自然密码子分布的宿主特异性 DNA 序列，同时最大限度地减少负调控元件。它经过来自各种生物体的超过 100 万个基因-蛋白质对的训练，在效率和准确性方面均优于现有的密码子优化工具。  主要特点：  🔬 STREAM 训练策略：我们引入了一种名为 STREAM 的新型训练方法，使我们的仅编码器模型能够有效地执行针对生物体定制的蛋白质到 DNA 的翻译。 🧬 高级揭露机制：该模型将 20 个可能的掩码标记 [aminoacid_UNK] 揭露为 64 个未掩盖的 [aminoacid_codon] 标记，从而实现精确的密码子选择。 🌐 开源且可扩展：我们发布了最大的密码子优化 Python 库以及我们的数据和模型，因此您可以根据自己独特的研究需求对 CodonTransformer 进行微调。  为什么这很重要？ 随着生物技术的进步，跨物种优化基因表达物种比以往任何时候都更加重要。 CodonTransformer 旨在弥补异源蛋白质表达方面的差距，使其成为计算生物学、合成生物学和相关领域研究人员的宝贵工具。 探索 CodonTransformer：  🌎 网站： adibvafa.github.io/CodonTransformer 💻 GitHub： GitHub.com/Adibvafa/CodonTransformer （如果您觉得有用，我们将不胜感激 ⭐！） 🛠 Colab Notebook： 在 Google Colab 上试用 🤖 Hugging Face 上的模型： CodonTransformer 模型 📄 预印本论文： bioRxiv 上的 CodonTransformer  我们很乐意听取您的反馈！ 请随意深入研究代码，测试模型，并让我们知道您的想法。您的见解和反馈对我们和社区来说都是无价的。 期待您的想法和讨论！    提交人    /u/Ornery_Historian_526   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj58d1/n_introducing_codontransformer_a_multispecies/</guid>
      <pubDate>Tue, 17 Sep 2024 17:26:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于语义分割的自动编码器损失</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj076i/r_autoencoder_loss_for_semantic_segmentation/</link>
      <description><![CDATA[我在这里有点新，但我想知道这个想法是否值得研究。 自动编码器学习压缩其训练数据。现在在蒙版图像上训练自动编码器。 如果我现在屏蔽掉所有内容，只留下具有某种语义含义的片段，我预计自动编码器的损失会很低，因为像素在某些高语义维度上是相关的。而如果其他一些东西没有被掩盖，那么自动编码器的损失就会非常高，因为那些没有被掩盖的东西不可能从其余的输入中推断出来，所以不可能进行压缩。 这可以通过查看自动编码器的损失来进行分割。 对此有什么想法吗？ （目前这需要大量的计算，但如果有人有一个聪明的想法，这可能是一个很酷的新方法值得尝试。）    提交人    /u/Additional-Math1791   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj076i/r_autoencoder_loss_for_semantic_segmentation/</guid>
      <pubDate>Tue, 17 Sep 2024 14:07:55 GMT</pubDate>
    </item>
    <item>
      <title>[N] Llama 3.1 70B，Llama 3.1 70B Instruct 压缩了 6.4 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/</link>
      <description><![CDATA[我们最近对 Llama 3.1 70B 和 Llama 3.1 70B Instruct 模型的研究实现了 6.4 倍的压缩率，同时保留了大部分 MMLU 质量。如果您有 3090 GPU，您现在就可以在家运行压缩模型。  以下是结果和压缩模型： https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16 https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main   由    /u/_puhsu  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/</guid>
      <pubDate>Tue, 17 Sep 2024 10:15:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 好的图形数据库选项？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fit4px/d_good_graph_database_options/</link>
      <description><![CDATA[我正在尝试构建一个 graphRAG 并使用其图形数据库，到目前为止，所有内容都指向 neo4j。我们还有其他更好、更适合生产的选项吗？    提交人    /u/Aromatic_Ad9700   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fit4px/d_good_graph_database_options/</guid>
      <pubDate>Tue, 17 Sep 2024 07:46:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 LLM 进行场景、模拟和专业战争游戏创作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fis46d/p_scenario_simulation_and_professional_wargame/</link>
      <description><![CDATA[https://github.com/user1342/WargamesAI    由   提交  /u/OppositeMonday   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fis46d/p_scenario_simulation_and_professional_wargame/</guid>
      <pubDate>Tue, 17 Sep 2024 06:39:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您使用什么工具进行 AI/ML 开发、训练和推理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fiqcxh/d_what_tools_do_you_use_for_aiml_development/</link>
      <description><![CDATA[我很好奇您在 AI/ML 工作流中使用的企业工具。无论是用于开发、训练、推理还是使用预构建模型，我都很想知道您每天依赖什么。  开发和训练：您喜欢哪些平台或服务来构建和训练模型？ 推理和部署：您使用哪些工具来大规模提供模型？ 预构建模型：您是否使用 Hugging Face 或 OpenAI 等平台来提供现成的模型？ 数据和实验跟踪：您推荐什么工具来管理数据集和跟踪实验？  期待您的见解！谢谢！    提交人    /u/Pretend-Lobster6455   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fiqcxh/d_what_tools_do_you_use_for_aiml_development/</guid>
      <pubDate>Tue, 17 Sep 2024 04:53:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于不同训练“技巧”的效果的良好研究，例如学习率调度程序（热身/衰减）、权重衰减、dropout、批量大小、动量等？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fihdrd/d_good_studies_on_the_effects_of_different/</link>
      <description><![CDATA[鉴于学习率调度程序（例如线性热身/余弦衰减）、正则化（权重衰减）、dropout、批量大小、动量项（Adam 中的 beta1、beta2）、批量规范等“技巧”的数量变得相当大，并且在这些大型模型上检查这些参数的所有不同组合变得越来越困难，是否有任何现有的研究或众包努力研究当我们改变这些技巧的各种参数时对最终性能（例如验证困惑度）的影响？ 我敢打赌其中很大一部分是在消融研究中，但它们有点太分散了。    提交人    /u/ThienPro123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fihdrd/d_good_studies_on_the_effects_of_different/</guid>
      <pubDate>Mon, 16 Sep 2024 22:01:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 15 Sep 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>