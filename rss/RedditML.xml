<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 18 Mar 2024 09:13:24 GMT</lastBuildDate>
    <item>
      <title>[P] Hugging Face 演示应用程序：表格数据的可解释生成人工智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhlajr/p_hugging_face_demo_app_explainable_generative_ai/</link>
      <description><![CDATA[大家好， 我邀请您尝试我们在表格数据的可解释和生成人工智能方面的工作的拥抱脸演示应用程序。 https://huggingface.co/spaces/CaglarAytekin/LEURN 简单来说，该应用程序采用包含特征和目标的训练表，让您选择要预测的内容并选择分类列。然后您可以使用选定的超参数训练神经网络。训练完成后，您可以上传测试数据（只有特征，没有目标的 Excel 或 csv），然后选择一行来运行神经网络并解释决策。您还可以根据神经网络从训练中学到的知识无缝地生成新数据。可以从上述应用程序中找到清晰的使用说明。 相关研究论文： https://arxiv.org/abs/2303.14937 开源代码： https://github.com/CaglarAytekin/LEURN/ 联系人： caglar@deepcause.ai ​   由   提交 /u/MLC_Money   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhlajr/p_hugging_face_demo_app_explainable_generative_ai/</guid>
      <pubDate>Mon, 18 Mar 2024 07:51:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 部署 Mistral 7B - 量化方法、托管选项等（针对 GPU 较差的情况）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhkvay/d_deploying_mistral_7b_quantization_methods/</link>
      <description><![CDATA[我正在尝试为我正在构建的 RAG 应用程序部署 Mistral 7B api 端点。我感到困惑的一些主要事情 - 我的 GPU 很差:( 所以计划使用 AWS sagemaker 来部署模型 - 2 个月的免费计划每月有 125 小时的 m4.xlarge 或 m5.xlarge 实例用于推理- 这足以为量化的 Mistra 设置一个端点（我想是 5 位）吗？如果您自己没有 GPU 并且是一名破产的学生，那么模型托管的首选是什么？只需要它是我最后一年的项目，所以这不是一个长期的事情。此外，我对量化类型和方法感到困惑 - 首先是 AWQ、GGUF 和 GPTQ，因为它的效率，我倾向于 AWQ，但是我听说它的缺点是支持有限？如果我只是想部署 TheBloke 的 AWQ 模型，那应该不会有什么特别困难的事情吧？另外，AWQ 的唯一缺点是它可能需要更多 RAM？还研究了部署方法本身 - openllama、vllama 等。我倾向于 vllama 因为效率 - 吞吐量是 Huggingface TGI 吞吐量的两倍，到目前为止我一直在使用 Huggingface TGI。但后来看到有人说它可能需要更多 VRAM？所以请提出建议 - 我的托管选项是什么，我应该使用什么量化方法，在这些有限的资源内，我有什么选择可以使这个模型尽可能快？ 注意：大多数情况下它不能得到帮助，我将不得不在模型速度上做出妥协，但我也想使用该模型来提取块的元数据，如果我向该模型提供 100 个块，我需要它不要慢到不切实际的地步哈哈。    由   提交/u/Aggravating-Floor-38   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhkvay/d_deploying_mistral_7b_quantization_methods/</guid>
      <pubDate>Mon, 18 Mar 2024 07:19:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于偏微分方程求解的神经算子背后的数学</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhj9av/d_math_behind_neural_operator_for_pde_solving/</link>
      <description><![CDATA[似乎几乎所有神经算子论文，如 FNO 等，总是使用 vt+1(x) = σ 形式的层（ W vt(x)+ ∫ K(x, y) vt(y) dy ) ，其中 t 是层。有什么证据证明这有效吗？看起来很多解释都表明该形式类似于格林函数解决方案，但每种方法都使用不同的内核，与格林函数或其工作原理没有具体关系。    由   提交/u/zarzor_2010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhj9av/d_math_behind_neural_operator_for_pde_solving/</guid>
      <pubDate>Mon, 18 Mar 2024 05:28:00 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]ICML 24</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhijrv/discussion_icml_24/</link>
      <description><![CDATA[我们有一篇文章讨论 AAAI。但是，我们似乎没有针对 ICML 的文章。因此，这篇文章是用于 ICML 讨论和其他内容的。 评论将在 2 天后发布。 &gt; 干杯！   由   提交/u/Objective_Whole_1406   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhijrv/discussion_icml_24/</guid>
      <pubDate>Mon, 18 Mar 2024 04:44:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] xAI 的 Qdrant...为什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhhyrv/d_xais_qdrantwhy/</link>
      <description><![CDATA[也许我只是没有花时间去理解它，但我很难理解 Qdrant 为何比 OpenSearch/ElasticSearch 更好？ OS/ES 都使用 HNSW，并且它们都使用相同的 KNN oss 实现，该实现具有极高的性能。 Qdrant 有什么“开箱即用”的功能？这些现有的且广泛采用的选项没有？   由   提交/u/titani0us  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhhyrv/d_xais_qdrantwhy/</guid>
      <pubDate>Mon, 18 Mar 2024 04:11:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何准备 META 研究工程师面试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhgxar/d_how_to_prepare_for_a_meta_research_engineer/</link>
      <description><![CDATA[我将在一周内进行 META 研究工程师面试。这个职位本身是机器学习和计算机视觉领域的，但我希望在面试中会被问到 leetcode 风格的问题。我想知道是否有人可以给我一些关于学习/复习内容的建议，因为只剩下一周了。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhgxar/d_how_to_prepare_for_a_meta_research_engineer/</guid>
      <pubDate>Mon, 18 Mar 2024 03:16:01 GMT</pubDate>
    </item>
    <item>
      <title>xAI 发布 Grok-1 [N]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh7yc4/xai_releases_grok1_n/</link>
      <description><![CDATA[ 我们正在发布大型语言模型 Grok-1 的基本模型权重和网络架构。 Grok-1 是一个由 xAI 从头开始​​训练的 3140 亿参数混合专家模型。 这是 Grok-1 预训练阶段的原始基础模型检查点，该阶段于 2023 年 10 月结束。这意味着该模型没有针对任何特定应用程序（例如对话）进行微调。 我们将在 Apache 2.0 许可证下发布权重和架构。 开始使用使用该模型时，请按照 https://github.com/xai-org/grok 中的说明进行操作   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh7yc4/xai_releases_grok1_n/</guid>
      <pubDate>Sun, 17 Mar 2024 20:38:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] Paperlib：一个开源且现代设计的学术论文管理工具。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh63c1/p_paperlib_an_opensource_and_moderndesigned/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh63c1/p_paperlib_an_opensource_and_moderndesigned/</guid>
      <pubDate>Sun, 17 Mar 2024 19:24:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] MOIRAI：Salesforce 的新时间序列基础预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh56kz/p_moirai_new_timeseries_foundation_forecasting/</link>
      <description><![CDATA[       由   提交 /u/apaxapax   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh56kz/p_moirai_new_timeseries_foundation_forecasting/</guid>
      <pubDate>Sun, 17 Mar 2024 18:49:01 GMT</pubDate>
    </item>
    <item>
      <title>[P] 精馏塔塔顶硫磺预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh4g3r/p_distillation_tower_overhead_sulfur_prediction/</link>
      <description><![CDATA[我正在构建一个神经网络来预测蒸馏塔顶部的硫（以 ppm 为单位）。到目前为止，我尝试对塔参数进行建模（再沸器负荷、底部和顶部塔温度、回流率、塔压力、OH温度、流失率、进料速率、进料中的硫（每周样品）、进料中的硫）底部（也是每周）），但最终得到的数据集太小，在十年内约有 250 个可用点，其中任何具有良好 R2 和 RSME 的模型都完全过度拟合，似乎甚至对于验证数据集 w / 随机 kfold。塔顶硫磺结果通常为 2 次/天。  我的两个问题是： 如果模型在每个数据点上进行训练，k-fold 是否会过度拟合？或者 k 折叠的设计是否使得每个折叠都至少使用不包含该折叠训练数据的模型进行一次验证？我构建的模型用于训练和验证的 R2 为 0.98（0.01 以内），RSME 为 8-10，这是可以接受的。我还没有找到部署模型的好方法来测试，但是我手动插入的一些值没有产生准确的结果。 我正在考虑首先对没有硫的塔进行建模，用几分钟分钟的过程数据，以建立塔顶流量和其余塔操作参数之间的关系，并尝试预测分离效率和下降流量，然后使用该模型进料第二个模型，该模型用于根据进料硫预测塔顶硫、塔顶馏出物：进料比和分离效率（因为分流的紧密程度会影响硫 ppm）。  我是 NN 的新手，我只是一名董事会操作员。工程师并不是只想构建一些可以让我/其他操作员的生活变得更轻松的东西。  我正在使用 JMP 17 pro，你们（流程工程师）在构建模型后如何部署模型？    由   提交/u/thedudear  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh4g3r/p_distillation_tower_overhead_sulfur_prediction/</guid>
      <pubDate>Sun, 17 Mar 2024 18:20:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 医学：有人训练过肺/心脏听诊的开源模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh2ho0/d_medical_anyone_trained_an_open_source_model_on/</link>
      <description><![CDATA[看起来这确实有用且相对容易完成，我的意思是，数据集存在。   由   提交 /u/hmmqzaz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh2ho0/d_medical_anyone_trained_an_open_source_model_on/</guid>
      <pubDate>Sun, 17 Mar 2024 16:59:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM4Decompile：使用大型语言模型反编译二进制代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh23py/r_llm4decompile_decompiling_binary_code_with/</link>
      <description><![CDATA[ 由   提交/u/vegax87  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh23py/r_llm4decompile_decompiling_binary_code_with/</guid>
      <pubDate>Sun, 17 Mar 2024 16:43:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mixture-of-LoRA：大型语言模型的高效多任务调优</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</guid>
      <pubDate>Sun, 17 Mar 2024 10:20:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我不明白反向传播如何在稀疏门控 MoE 上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</link>
      <description><![CDATA[我不明白反向传播如何在稀疏门控 MoE 上工作 在 LLM 的背景下，假设你有 n 个专家，并且您为每个令牌选择了前 k 个。 在训练期间，门网络可能完全错误，并且将正确的专家排除在所选的 k 之外。然而，由于没有使用正确的专家，因此门没有机会增加正确专家的权重。 换句话说，在背景期间，仅更新门网络的部分参数，影响前 k 内权重的那些。 我错过了什么吗？   由   提交 /u/Primary-Try8050    reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</guid>
      <pubDate>Sun, 17 Mar 2024 02:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>