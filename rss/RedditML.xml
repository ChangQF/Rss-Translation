<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 17 Mar 2024 15:13:28 GMT</lastBuildDate>
    <item>
      <title>[P] 处理 ML 项目的异常值</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgyy27/p_dealing_with_unusual_values_for_ml_project/</link>
      <description><![CDATA[我正在开发一个 ML 项目，但不知道如何处理异常值，例如歌曲持续时间为负数价值观。这当然不应该存在，但是在这种情况下，除了删除它们之外，你还能做什么呢？有没有比直接删除它们更好的技术来处理它们？谢谢。   由   提交 /u/StrangerOnTheRoad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgyy27/p_dealing_with_unusual_values_for_ml_project/</guid>
      <pubDate>Sun, 17 Mar 2024 14:32:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 预测-肺癌</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgyo63/p_predictive_lung_cancer/</link>
      <description><![CDATA[       我想分享我的另一个机器学习项目，该项目用于利用吸烟量和常见症状以低成本预测肺癌。 注意： 该项目可能不太准确，可能会因数据缺乏和不平衡而遇到不好的结果。 您可以尝试： https://predictive-lungcancer.vercel.app 叉子： https://github.com/nordszamora/predictive_lung_cancer https://preview.redd.it/48u2qz4ilwoc1.png?width=1366&amp;format=png&amp;auto=webp&amp;s=5cc1ea9f46be3355afd5ef 214dc31cf02002fdb2   由   提交 /u/ThePawners   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgyo63/p_predictive_lung_cancer/</guid>
      <pubDate>Sun, 17 Mar 2024 14:20:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过集群间建模生成代码的神经排名器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgwmzv/r_neural_rankers_for_code_generation_via/</link>
      <description><![CDATA[我们引入了 SRank，这是一种新颖的重新排名策略，用于从代码生成中选择最佳解决方案，重点是对集群间关系进行建模。通过量化集群之间的功能重叠，我们的方法提供了更好的代码解决方案排名策略。实证结果表明，我们的方法在 pass@1 分数上取得了显着的结果。例如，在 Human-Eval 基准测试中，我们在 Codex002 的 pass@1 中实现了 69.66%，WizardCoder 为 75.31%，StarCoder 为 53.99%，CodeGen 为 60.55%，这超越了最先进的解决方案排名方法，例如同一 CodeLLM 上的 CodeT 和 Coder-Reviewer 具有显着的优势（平均提高约 6.1%）。与随机抽样方法相比，我们在 Human-Eval 上平均提高了约 23.07%，在 MBPP 上平均提高了 17.64%。 论文：https://arxiv.org/abs/2311.03366 代码： https://github.com/FSoft-AI4Code/SRank-CodeRanker   由   提交 /u/FSoft_AIC   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgwmzv/r_neural_rankers_for_code_generation_via/</guid>
      <pubDate>Sun, 17 Mar 2024 12:43:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与你的表聊天 - 使用什么来使用开源 LM 进行数据库问答？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgw40p/d_chat_with_your_tables_what_to_use_for_database/</link>
      <description><![CDATA[我正在开发一个项目，我们想要回答数据库中表的简单/中度硬查询。我目前没有资源来微调模型，想尝试开源大型语言模型。 过滤掉之后的所有表格大约有 120 GB，这告诉我需要使用开始时有一个 Text 2 SQL 模型，并可能使用 Spark 执行。我用采样数据尝试了 pandasAI 和 LC 代理，但输出并不是很好（有些失败 如果有人对此进行过研究，我将不胜感激任何线索或评论，或任何研究论文。我需要有关典型管道的外观以及如何提取和传递数据中的多行的指导。    提交者   /u/Parking_Nectarine_19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgw40p/d_chat_with_your_tables_what_to_use_for_database/</guid>
      <pubDate>Sun, 17 Mar 2024 12:14:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 专用于AI学习环境的游戏引擎</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgvgrr/d_game_engine_dedicated_to_ai_learning/</link>
      <description><![CDATA[我最近一直在寻找一种为 RL 训练创建自定义环境的方法，该环境可以轻松地与外部编程语言和框架集成（因此基本上将 NPC 外包）游戏逻辑到游戏本身以外的其他东西，如 pytorch 或 TensorFlow，或您选择的任何其他库）。  长话短说，我遇到过unity ml代理，但它的灵活性非常有限（你只能微调一些算法的超参数），而且自上次unity以来，设置真的很草率+我对使用它有点怀疑。 Nvidiaomniverse + Isaac Gym 仅适用于高度精确的物理环境，你需要大量的 GPU 和精确的模型来运行它，但这不是我想要的东西。我可以扭转它足以使其实现简单的游戏，但这将是一个strech 除此之外，我只找到了几个库，但没有一个真正匹配接近我想要的 ​ 我设想的是一个改进的 godot 引擎，上面有一个框架，集成了 godot 和 python 数据传输和同步。您应该能够更改引擎中的对象参数，并且应该将其映射到 python 代码，如果 python 没有为游戏内代理返回特定的匹配模式，则会出现错误 &lt; p&gt;你知道有什么与我所描述的类似的事情吗？    由   提交/u/DeadProgrammer8785   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgvgrr/d_game_engine_dedicated_to_ai_learning/</guid>
      <pubDate>Sun, 17 Mar 2024 11:36:51 GMT</pubDate>
    </item>
    <item>
      <title>[D]分布式训练策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgulqe/d_distributed_training_strategy/</link>
      <description><![CDATA[嗨，我想知道如何使用不同的“分布式训练策略”微调 Mixtral。 我可以使用 4* A100 (40Gb)，并且想要尝试不同的策略，例如对模型进行分片并在每个 GPU 上放置 2 个专家层、使用 QLoRA 量化模型，以及在 4 个 GPU 上使用数据并行性。 我可以使用 4* A100 (40Gb)，并且想要尝试不同的策略，例如对模型进行分片并在每个 GPU 上放置 2 个专家层，或者使用以下方法量化模型QLoRA，并在 4 个 GPU 上使用数据并行性。   由   提交 /u/Thick-brain-dude   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgulqe/d_distributed_training_strategy/</guid>
      <pubDate>Sun, 17 Mar 2024 10:40:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mixture-of-LoRA：大型语言模型的高效多任务调优</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</guid>
      <pubDate>Sun, 17 Mar 2024 10:20:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我不明白反向传播如何在稀疏门控 MoE 上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</link>
      <description><![CDATA[我不明白反向传播如何在稀疏门控 MoE 上工作 在 LLM 的背景下，假设你有 n 个专家，并且您为每个令牌选择了前 k 个。 在训练期间，门网络可能完全错误，并且将正确的专家排除在所选的 k 之外。然而，由于没有使用正确的专家，因此门没有机会增加正确专家的权重。 换句话说，在背景期间，仅更新门网络的部分参数，影响前 k 内权重的那些。 我错过了什么吗？   由   提交 /u/Primary-Try8050    reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</guid>
      <pubDate>Sun, 17 Mar 2024 02:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从机器学习中的归纳偏差的角度看注意力和变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</link>
      <description><![CDATA[      Hello人们在我的 YT 频道上发布了一段有关 Transformer 的视频，以及他们为深度学习研究带来的有趣的范式转变。在受到关注之前，趋势过去是在模型架构中添加更多归纳偏差（具有位置偏差的 CNN、具有时间偏差的 RNN）…… Transformer 的成功有点表明，当你获得足够的数据和信息时，通用架构可以胜过归纳偏差。一个要训练的大屁股模型。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</guid>
      <pubDate>Sat, 16 Mar 2024 21:22:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] Apple - MM1：多模式 LLM 预培训的方法、分析和见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</link>
      <description><![CDATA[Apple 的新论文介绍了 MM1 ，一系列结合了视觉和语言理解的多模式人工智能模型。研究人员进行了广泛的实验，以确定驱动这些模型性能的关键因素，测试不同的架构选择和预训练数据混合。 以下是我在论文中的要点： Big当然之一：最大的 MM1 模型（30B 密集）在多模态基准上实现了最先进的少样本学习 要点：  MM1 包括两者高达 30B 参数的密集模型和专家混合 (MoE) 变体 图像分辨率对性能的影响最大，超过模型大小 特定的视觉语言连接器设计具有效果不大 在预训练中混合交错图像+文本、标题和纯文本数据至关重要 标题、交错和文本数据的比例为 5:5:1 有效最佳 合成字幕数据有助于少样本学习 30B 密集模型在 VQA 和字幕任务上击败了先前的 SOTA  核心见解深思熟虑的数据和架构选择，而不仅仅是规模，是构建高性能多模式模型的关键。 MM1 模型还表现出令人印象深刻的新兴能力，例如多图像推理和上下文中的小样本学习。 完整摘要。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</guid>
      <pubDate>Sat, 16 Mar 2024 17:29:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动态内存压缩：改造 LLM 以加速推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</link>
      <description><![CDATA[     &lt; td&gt; 动态内存压缩：改进 LLM 以加速推理 论文：https://arxiv.org/abs/2403.09636 X：https://x.com/p_nawrot/status/1768645461689168365 摘要：  Transformers 已成为大型语言模型 (LLM) 的支柱。然而，由于需要在内存中存储过去标记的键值表示的缓存，生成仍然效率低下，其大小与输入序列长度和批量大小线性缩放。作为解决方案，我们提出了动态内存压缩（DMC），这是一种在推理时进行在线键值缓存压缩的方法。最重要的是，该模型学习在不同的头和层中应用不同的压缩率。我们将 Llama 2（7B、13B 和 70B）等预训练的 LLM 改造为 DMC Transformer，在 NVIDIA H100 GPU 上实现自回归推理吞吐量高达约 3.7 倍的增长。 DMC 通过对原始数据的可忽略百分比进行持续预训练来应用，无需添加任何额外参数。我们发现 DMC 通过高达 4 倍的缓存压缩保留了原始的下游性能，优于经过训练的分组查询注意力 (GQA)。 GQA 和 DMC 甚至可以结合起来以获得复合收益。因此，DMC 在任何给定的内存预算内都适合更长的上下文和更大的批次。  https:/ /i.redd.it/ouuf7t4d5qoc1.gif   由   提交 /u/alancucki   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</guid>
      <pubDate>Sat, 16 Mar 2024 16:40:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] Kaggle TPU v3-8 的 Llama2 7B 和 13B 聊天完成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</link>
      <description><![CDATA[大家好，我对 Llama2 存储库进行了一些修改以利用 TPU v3-8 硬件，因此它可以执行 Llama2 7B（甚至 13B） ）聊天完成推理，无需图形重新编译。当批量大小为 1 生成文本时，它仍然比 Nvidia P100 慢，不适合实时推理，但（TPU 就是 TPU）在批量文本生成方面表现出色。我用它生成大量文本用于研究目的。希望它对社区有益。 这是存储库。 修改利用 PyTorch/XLA SPMD 系统（在 TPU v3-8 上）以及新的网格和分布配置来进行分片整个 TPU 网格的权重和缓存。具体来说，k、v 缓存具有预定义的静态大小，以避免每次令牌生成后 TPU 图形重新编译。这一新配置使 Llama2 7B 能够装入一台 TPU v3-8 设备中，并具有大量剩余内存来运行推理，批量大小可达 64。相同的配置也可用于运行 Llama2 13B 的推理。 现有的Llama2 Google Next Inference分支仅支持TPU v4和 v5e。喜欢使用 Kaggle TPU 的人可以利用它来运行推理。   由   提交/u/-x-Knight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</guid>
      <pubDate>Sat, 16 Mar 2024 16:25:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLaMA 的具体细节：了解 LLaMA 和大型语言模型如何运行的整体方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</link>
      <description><![CDATA[我很高兴地宣布，我使用 Go 开发的 LLaMA Nuts and Bolts 开源项目现已公开发布！ 您可以在我的 Github 存储库上找到它：https://github.com/adalkiran/llama-nuts-and- Bolts 通过代码和详细文档了解 LLaMA 及其组件在实践中如何运行的整体方法。 “螺母和螺栓” （实践方面而不是理论事实，纯粹的实现细节）所需的组件、基础设施和数学运算，而不使用外部依赖项或库。 目标是制作一个可以对 LLaMa 进行推理的实验项目2 7B-聊天模型完全脱离Python生态系统（使用Go语言）。在整个旅程中，我们的目标是获取知识并阐明该技术的抽象内部层。 这段旅程是一次有意重新发明轮子的旅程。在阅读我的文档中的旅程时，您将通过 LLaMa 模型的示例了解大型语言模型如何工作的详细信息。 如果您像我一样对 LLM（大型语言模型）如何工作感到好奇和变形金刚工作并深入研究了来源中的概念解释和示意图，但渴望更深入的理解，那么这个项目也非常适合您！ 您不仅会发现 LLaMa 架构的细节，而且还会发现在文档目录中查找各种相关概念的解释。从逐字节读取 Pickle、PyTorch 模型、Protobuf 和 SentencePiece 分词器模型文件，到 BFloat16 数据类型的内部结构、从头开始实现 Tensor 结构和包括线性代数计算在内的数学运算。 这个项目最初是为了通过运行和调试来了解 LLM 的作用，并且仅用于实验和教育目的，而不是用于生产用途。 如果您查看它，我会很高兴欢迎评论！   由   提交 /u/adalkiran   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</guid>
      <pubDate>Sat, 16 Mar 2024 13:25:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] RepoHyper：存储库级代码完成所需的只是更好的上下文检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</link>
      <description><![CDATA[我们引入了 RepoHyper，这是一个新颖的框架，可将代码完成转换为现实世界存储库用例的无缝端到端流程。传统方法依赖于将上下文集成到代码语言模型 (CodeLLM) 中，通常假设这些上下文本质上是准确的。然而，我们发现了一个差距：标准基准测试并不总是提供相关的上下文。 为了解决这个问题，RepoHyper 提出了三个新颖的步骤：  构建代码属性图，建立丰富的上下文源。 一种新颖的搜索算法，用于查明所需的确切上下文。 扩展算法，旨在揭示代码元素之间的微妙联系（类似于社交网络挖掘中的链接预测问题）。  我们的综合评估表明，RepoHyper 树立了新标准，在 RepoBench 基准测试中优于其他强大的基准。 代码：https://github.com/FSoft-AI4Code/RepoHyper   由   提交 /u/FSoft_AIC   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</guid>
      <pubDate>Sat, 16 Mar 2024 10:41:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>