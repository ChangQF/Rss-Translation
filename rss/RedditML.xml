<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 28 Apr 2024 03:15:07 GMT</lastBuildDate>
    <item>
      <title>[D] 快速提问：您如何实施研究论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cew2b5/d_quick_question_how_do_you_implement_research/</link>
      <description><![CDATA[我正在学习深度学习，现在我想通过实施一篇研究论文来测试我的技能。 有一篇研究论文称为深度学习的语言进化，它让我着迷，但我很困惑从哪里开始以及如何开始。 如果有人，我需要指导。   由   提交/u/No-Signal-313  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cew2b5/d_quick_question_how_do_you_implement_research/</guid>
      <pubDate>Sun, 28 Apr 2024 02:48:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] NLLB-200 蒸馏器 350M 一台</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceuj4t/p_nllb200_distill_350m_for_enko/</link>
      <description><![CDATA[您好r/MachineLearning， 我很高兴分享一个最初打算在我的毕业产品（Capstone）中使用的项目 我制作了 NLLB-200 Distill 350M 模型来将英语翻译成韩语 很好用。小而快。所以它可以用 CPU 运行！ GPU 服务器相当昂贵，所以我为那些买不起服务器的大学生（比如我）制作了它。 更多细节是在我的页面 如果你懂韩语，请给我很多反馈 谢谢！！ https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko   由   提交/u/SaeChan5  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceuj4t/p_nllb200_distill_350m_for_enko/</guid>
      <pubDate>Sun, 28 Apr 2024 01:26:04 GMT</pubDate>
    </item>
    <item>
      <title>[P]我做了一个简单的python脚本来让两个LLM互相判断</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cerjoj/p_i_made_a_simple_python_script_to_make_two_llms/</link>
      <description><![CDATA[我制作了一个视频，介绍何时运行此程序而不是依赖排行榜可能有用，回顾结果、限制和要点，并给出代码的简要概述。我查看了 GPT4 与 Claude 3 Opus 的结果。 YT：https://youtu.be/-bcbcFYhqes Github：https://github.com/learn-good/llm-vs-llm&lt; /a&gt; 如果您不想观看视频或运行代码，我在此处提供了结果摘要和我的要点：  总体而言，GPT4 似乎比 Claude 3 Opus 稍好，但 Opus 可能在某些任务上更好（在我的实验中，它在设计教学大纲方面更好）。 这些法学硕士似乎没有注意到或更喜欢他们自己的输出已生成。 结果对提示结构中的结构（非信息）变化有些敏感。也就是说，改变提示中显示的顺序信息可能会改变结果。 当以不同的顺序提供相同的候选答案时，GPT4 和 Opus 在这两种情况下选择相同的“最喜欢的答案”时并不一致（略好于 50%）。其他比较（Haiku 与 GPT4、Haiku 与 GPT3.5、Sonnet 与 GPT3.5）的一致性要高得多（~90%）。我推测这可能是因为输出质量非常相似，或者因为可能很难区分两个非常高质量的答案，即使它们之间存在一些潜在的质量差异。 这是一个小型研究，结果显然不是确定的。该实验针对不同任务类别中的约 90 个任务进行了运行。任务描述很短，而且是 0 次测试（没有给出示例）。 Chatbot Arena 排行榜和 LLM 基准测试非常适合了解这些模式的一般功能，但如果您有一个利基市场任务或问题领域，可能值得进行一些小实验来看看哪种模型表现最好。如果您想做的话，我会提供运行您自己的实验的代码和说明。  如果您有任何问题，请随时告诉我！ &lt; !-- SC_ON --&gt;  由   提交/u/arnokha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cerjoj/p_i_made_a_simple_python_script_to_make_two_llms/</guid>
      <pubDate>Sat, 27 Apr 2024 23:01:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 RAG 的真实讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</link>
      <description><![CDATA[说实话。我知道我们都必须与这些经理/董事/CXO 打交道，他们提出了与公司数据和文档交谈的惊人想法。 但是……有人真正做了一些真正有用的事情吗？如果是这样，它的有用性是如何衡量的？ 我有一种感觉，我们被一些非常复杂的废话所愚弄，因为法学硕士总是可以产生在某种程度上听起来合理的东西。但它有用吗？   由   提交/u/fusetron  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/</guid>
      <pubDate>Sat, 27 Apr 2024 18:00:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] BLEU 分数没有提高</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceidsx/p_bleu_scores_not_improving/</link>
      <description><![CDATA[我正在使用 CNN+LSTM 开发这个图像字幕项目。目前我使用 googleNet 作为 CNN，使用 lstm、bilstm 作为 RNN。对于嵌入，我使用 word2vec 算法。使用的数据集是 flickr8k。正如您从蓝色分数中看到的那样，它们非常接近，即没有明显的改进。这两种方法中的参数值相同。值如下： word_embedding_dimension：128 lstm_hidden_​​state_dimension：32 lstm_number_of_layers：2 dropout_proportion：0.5 我正在训练我的模型500 epoch，批量大小为 16，学习率为 0.0003。 这是我的两种方法的架构： class BiLSTM_fixed_embedding(torch.nn.Module): def __init__(self, embedding_dim, lstm_hidden_​​dim, num_lstm_layers, image_latent_dim, vocab_size, dropoutProportion=0.52): super(BiLSTM_fixed_embedding, self).__init__() self.embedding_dim = embedding_dim self.lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_​​dim, num _lstm_layers，batch_first=True， bidirection=True) # 双向 LSTM self.dropout = torch.nn.Dropout(dropoutProportion) self.linear = torch.nn.Linear(lstm_hidden_​​dim * 2 + image_latent_dim, vocab_size) # 调整双向 LSTM 的线性层输入大小 def front(self , image_latentTsr,EmbeddedChoppedDescriptionTsr):aggregate_h, (ht, ct) = self.lstm(embeddedChoppedDescriptionTsr) # 连接两个方向的最终隐藏状态 concat_latent = torch.cat((torch.nn.function.normalize(ht[-2,: ,:]), torch.nn.function.normalize(ht[-1,:,:]), torch.nn.function.normalize(image_latentTsr)), dim=1) # 使用 ht[-2,:,: ] 和 ht[-1,:,:] 对于双向 LSTM outputTsr = self.linear(self.dropout(concat_latent)) 返回 outputTsr 类 LSTM_fixed_embedding(torch.nn.Module): def __init__(self, embedding_dim, lstm_hidden_​​dim, num_lstm_layers, image_latent_dim, vocab_size, dropoutProportion=0.5): super(LSTM_fixed_embedding, self).__init__() self.embedding_dim = embedding_dim self.lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_​​dim, num_lstm_layers, batch_first=True) self.dropout = torch. nn.Dropout(dropoutProportion) self.linear = torch.nn.Linear(lstm_hidden_​​dim + image_latent_dim, vocab_size) defforward(self, image_latentTsr,embeddedChoppedDescriptionTsr):aggregated_h, (ht, ct) = self.lstm(embeddedChoppedDescriptionTsr) concat_latent = torch. cat( (torch.nn.function.normalize(ht[-1]), torch.nn.function.normalize(image_latentTsr)), dim=1) 输出Tsr = self.线性(self.dropout(concat_latent)) 返回outputTsr BLEU lstm+googleNet 平均 BLEU-1 分数：0.5125574933996119 平均 BLEU-2 分数：0.3231883563312253 平均 BLEU-3 分数：0.13785811579538154 平均 BLEU-4 分数：0.04515516928289819 BLEU 分数bilstm+googleNet 平均 BLEU-1 分数：0.5202970280329978 平均 BLEU-2分数：0.3236744954911404 BLEU-3 平均分数：0.1341793193712665 BLEU-4 平均分数：0.050970741654506005  现在我有两个问题： i。分数没有提高的原因是什么？如何提高。 ii.我还计划使用 resNet50，它有 2048 个潜在表示，即 googleNet 的两倍。这种方法需要对参数进行哪些更改。 非常感谢大家帮助我。   由   提交 /u/Sorry_Drawer9736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceidsx/p_bleu_scores_not_improving/</guid>
      <pubDate>Sat, 27 Apr 2024 16:21:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 但是经过训练的卷积神经网络实际上学到了什么？可视化！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cefhwf/d_but_what_does_a_trained_convolution_neural/</link>
      <description><![CDATA[   分享我的 YT 频道中的视频，解释卷积并可视化内核的学习方式……享受吧！    由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cefhwf/d_but_what_does_a_trained_convolution_neural/</guid>
      <pubDate>Sat, 27 Apr 2024 14:14:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] 小型 GPT-2 大小的 LLM 的分类微调实验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cedfub/p_classification_finetuning_experiments_on_small/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cedfub/p_classification_finetuning_experiments_on_small/</guid>
      <pubDate>Sat, 27 Apr 2024 12:31:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于 Llama-3 的 OpenBioLLM-70B 和 8B：在医疗领域优于 GPT-4、Gemini、Meditron-70B、Med-PaLM-1 和 Med-PaLM-2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/</link>
      <description><![CDATA[      开源再次来袭，我们很高兴地宣布 OpenBioLLM-Llama3-70B 和 OpenBioLLM-Llama3-70B 的发布。 8B.这些模型在生物医学领域超越了 Openai 的 GPT-4、Google 的 Gemini、Meditron-70B、Google 的 Med-PaLM-1 和 Med-PaLM-2 等行业巨头，树立了新的状态。对于同尺寸的模型来说是最先进的。 迄今为止最有能力的公开医学领域法学硕士！ 🩺💊🧬 https://预览。 redd.it/w41pv7mwf0xc1.png?width=5760&amp;format=png&amp;auto=webp&amp;s=f3143919ef8472961f329bb8eb98937d8f8e41e0 结果可在 Open Medical-L 上查看LM排行榜：https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard 超过约 4 个月，我们精心策划了多样化的定制数据集，与医学专家合作以确保最高质量。该数据集涵盖 3000 个医疗保健主题和 10 多个医学主题。 📚 OpenBioLLM-70B 的卓越性能在 9 个不同的生物医学数据集上显而易见，尽管与 GPT-4 和 GPT-4 相比其参数数量较少，但其平均得分达到了令人印象深刻的 86.06%。医学-PaLM。 📈 https://预览。 redd.it/5ff2k9szf0xc1.png?width=5040&amp;format=png&amp;auto=webp&amp;s=15dc4aa948f2608717f68ddf2cb27a6a2de03496 您今天可以直接从 Huggingface 下载模型。  70B : https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B 8B：https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B  此版本只是一个开始！在接下来的几个月中，我们将推出  扩大医疗领域覆盖范围， 更长的上下文窗口， 更好的基准，以及 多模式功能。  更多详细信息可在此处找到：https://twitter。 com/aadityaura/status/1783662626901528803 在接下来的几个月里，Multimodal 将可用于各种医疗和法律基准。 我希望它对您的研究有用 🔬 大家周末愉快！ 😊   由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/</guid>
      <pubDate>Sat, 27 Apr 2024 11:51:42 GMT</pubDate>
    </item>
    <item>
      <title>如何说服我的上级进行数据预处理？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceckws/how_do_i_convince_my_superior_to_do_data/</link>
      <description><![CDATA[如何说服上级进行数据预处理？ 您好，我做了一年的 AI 工程师在我现在的公司（获得了数据科学专业的计算机硕士学位）。我们希望构建专门用于特定语言的闲聊（主要是对话式聊天）的聊天机器人。  问题是我不同意上级的做事方式。它几乎总是在进行即时工程。我的意思是我们有大量的数据（我想说的是无限的实时会话聊天会话，其中包含兴趣、外表等信息……这是所有数据科学家建立一个漂亮模型的梦想）。我不同意他的方法的原因是，通过及时的工程设计，我们并不总是能获得持续的良好结果。此外，对于特定领域（例如色情聊天），由于模型的审查，您无法提示工程。当模型未针对特定领域的标记/单词进行训练时，可能会出现幻觉和其他问题。最后，一切都与统计有关，不是吗？该模型从使用的数据中学习。如果推理过程中有一个 token 没有包含在训练数据中，那么它会以概率进行猜测，以预测最有可能的下一个 token。 我不明白为什么我们不这样做利用这些数据来清理它，为我们的目的/领域创建一个超级好的数据集，并对法学硕士进行微调。我问过他很多次为什么我们不这样做，我的上级回答说：“我们过去这样做过，成本太大，效果不好”。于是我问他，是谁干的？他告诉我，我的同事做到了（医学教育背景，空闲时间对人工智能感兴趣，但他对数据处理或数据科学基础知识一无所知）。  所以他们最后一次尝试是在3年前（他们是用deepspeed做的，没有使用Lora方法，所以我的上级告诉我成本相当高，但结果不好（他们在云中进行了微调） 200h），所以这是一个完整的参数微调） 说实话，我不怪我的同事。他用自己的知识尽力了。但我确实责怪我愚蠢的上司，我们没有取得多大成功来为我们的目的开发一个像样的模型。  所以，在我开始为公司工作半年后，我终于可以说服我的上司了（因为我在空闲时间做了一个微调，只是为了好玩，并向他们展示了我的结果）。所以他同意，我们可以对洛拉进行微调，但是..但是..没有数据处理，就拿原始的宝贝来说！！ 说真的，那家伙完全迷失了，顺便说一句，他是我们的产品经理并且对数据科学一无所知。他在没有数据处理的情况下再次犯了同样的错误，因为“我们没有这方面的资源”，我什至无法说服他。 所以最后，聊天机器人变得比仅仅做更好一些及时的工程，但对我来说它仍然是垃圾。我只想要一个真实且标准的工作流程，包括数据预处理、训练、评估。就这样。最重要的是：数据预处理 那么你们觉得怎么样？我是猴子吗？我应该尽快离开公司吗？我需要在那里至少再呆一年。   由   提交/u/bobotomoon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceckws/how_do_i_convince_my_superior_to_do_data/</guid>
      <pubDate>Sat, 27 Apr 2024 11:43:37 GMT</pubDate>
    </item>
    <item>
      <title>[研究]机器学习与推理的协同作用（2024）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ceamub/research_synergies_between_machine_learning_and/</link>
      <description><![CDATA[ 由   提交 /u/blackgreenolive   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ceamub/research_synergies_between_machine_learning_and/</guid>
      <pubDate>Sat, 27 Apr 2024 09:39:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 标记化的数学方面</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cea5qn/d_mathematical_aspects_of_tokenization/</link>
      <description><![CDATA[我最近制作了一个视频，介绍了我们最近在标记化的数学方面的工作，特别是： - 将标记化形式化为压缩 - 字节对编码的界限最优性 - 标记化熵和性能之间的联系 我将非常感谢任何反馈，因为我仍在学习如何制作教育视频。谢谢！ https://youtu.be/yeEZpf4BlDA   由   提交/u/zouharvi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cea5qn/d_mathematical_aspects_of_tokenization/</guid>
      <pubDate>Sat, 27 Apr 2024 09:06:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型模型的参数高效微调：综合调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ce8wow/r_parameterefficient_finetuning_for_large_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.14608 摘要：  大型模型代表了多个应用领域的突破性进步，实现了显着的各项任务取得的成绩。然而，其前所未有的规模伴随着巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在针对特定下游任务进行定制时，巨大的规模和计算需求带来了相当大的挑战，特别是在受计算能力限制的硬件平台上。 参数高效微调 (PEFT ）通过在各种下游任务中有效地调整大型模型来提供实用的解决方案。具体来说，PEFT 是指调整预训练大型模型的参数以使其适应特定任务，同时最大限度地减少引入的附加参数或所需的计算资源的数量的过程。在处理具有高参数数的大型语言模型时，这种方法尤其重要，因为从头开始微调这些模型可能会耗费大量计算资源且占用大量资源，从而给支持系统平台设计带来相当大的挑战。 ，我们对各种 PEFT 算法进行了全面的研究，检查了它们的性能和计算开销。此外，我们概述了使用不同 PEFT 算法开发的应用程序，并讨论了用于降低 PEFT 计算成本的常用技术。除了算法角度之外，我们还概述了各种现实世界的系统设计，以研究与不同 PEFT 算法相关的实施成本。这项调查对于旨在了解 PEFT 算法及其系统实现的研究人员来说是不可或缺的资源，提供了对最新进展和实际应用的详细见解。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ce8wow/r_parameterefficient_finetuning_for_large_models/</guid>
      <pubDate>Sat, 27 Apr 2024 07:42:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 谈论模型的概率有意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ce1zfs/d_does_it_make_sense_to_talk_about_the/</link>
      <description><![CDATA[https://lunaverus.com/programLikelihoods  有一种巧妙的方法将无监督学习构建为似然最大化，但不是以通常的方式，即使用模型计算数据的似然性并忽略模型本身的似然性。相反，这是模型和数据的组合可能性...  谈论 ML 模型的概率有意义吗？   由   提交 /u/bouncyprojector   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ce1zfs/d_does_it_make_sense_to_talk_about_the/</guid>
      <pubDate>Sat, 27 Apr 2024 01:06:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士：为什么情境学习有效？从技术角度来看到底发生了什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/</link>
      <description><![CDATA[在我寻找这个问题的答案时，得到的答案只不过是将模型拟人化而已。他们总是提出这样的主张：  如果没有示例，模型必须推断上下文并依靠其知识来推断出预期的结果。这可能会导致误解。 一次性提示通过提供具体示例来减轻这种认知负担，有助于锚定模型的解释并专注于具有更清晰期望的更狭窄的任务。  该示例充当模型的参考或提示，帮助其理解您正在寻求的响应类型并在训练期间触发对类似实例的记忆。&lt; /p&gt; 提供示例允许模型识别要复制的模式或结构。它为模型建立了一个对齐线索，减少了零样本场景中固有的猜测。  顺便说一句，这些是真实的摘录。 但这些模型不“理解”任何东西。他们不“推断”，或“解释”，或“聚焦”，或“记住训练”，或“猜测”，或有字面上的“认知负荷”。它们只是统计令牌生成器。因此，当寻求对上下文学习提高准确性的确切机制的具体理解时，像这样的流行科学解释是毫无意义的。 有人可以提供一个根据实际模型来解释事物的解释吗？架构/机制以及提供额外上下文如何带来更好的输出？我可以“说说而已”，所以请不遗余力地提供技术细节。 我可以做出有根据的猜测 - 在输入中包含示例，这些示例使用近似于您想要的输出类型的标记来引导注意力机制，并且最终的密集层对更高的令牌进行加权，这些令牌在某种程度上与这些示例类似，从而增加了在每次前向传递结束时对这些所需令牌进行采样的几率；从根本上来说，我猜这是一个相似性/距离的问题，其中明确举例说明我想要的输出会增加输出与其相似的可能性 - 但我更愿意从对这些模型有深入了解的其他人那里听到它和机制。   由   提交 /u/synthphreak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/</guid>
      <pubDate>Fri, 26 Apr 2024 11:01:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>