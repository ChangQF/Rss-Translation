<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sat, 22 Jun 2024 18:18:03 GMT</lastBuildDate>
    <item>
      <title>[D] 谷歌 Gemma 的印度语数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dlyj6v/d_datasets_of_the_google_gemma_for_indic_languages/</link>
      <description><![CDATA[用于训练 GEMMA 的印度语数据集最初是用印度语创建的吗？还是从英语数据集翻译而来的？回复似乎翻译得过多了。？    提交人    /u/cern_unnosi   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dlyj6v/d_datasets_of_the_google_gemma_for_indic_languages/</guid>
      <pubDate>Sat, 22 Jun 2024 15:43:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学术 ML 实验室：有多少个 GPU？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dlsogx/d_academic_ml_labs_how_many_gpus/</link>
      <description><![CDATA[在阅读最新帖子后，我想知道其他实验室在这方面做得如何。 在我攻读博士学位（前 5 名计划）期间，计算是一个主要瓶颈（如果我们有更多高容量 GPU，时间可能会大大缩短）。我们目前没有 H100。 您的实验室有多少个 GPU？您是否通过硬件补助从 Amazon/NVIDIA 获得额外的计算积分？ 谢谢    提交人    /u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dlsogx/d_academic_ml_labs_how_many_gpus/</guid>
      <pubDate>Sat, 22 Jun 2024 10:29:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer 的记忆机制</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dlb0wj/d_memory_mechanism_for_transformers/</link>
      <description><![CDATA[大家好！我想知道在为 transformers 添加短期记​​忆机制方面做了哪些有趣的工作？有人知道这个领域的重要工作是什么吗？    提交人    /u/Janos95   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dlb0wj/d_memory_mechanism_for_transformers/</guid>
      <pubDate>Fri, 21 Jun 2024 18:29:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] AgileRL - 用于最先进深度强化学习的进化型 RLOps</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dla20p/p_agilerl_evolutionary_rlops_for_stateoftheart/</link>
      <description><![CDATA[嗨，我之前发布过关于我们针对强化学习的进化超参数优化实现 SOTA 结果的帖子，但我想分享的是，我们的开源框架现已发布 v1.0.0！ 请查看！https://github.com/AgileRL/AgileRL 该库最初专注于通过开创用于强化学习的进化 HPO 技术来减少训练模型和超参数优化所需的时间。事实证明，进化 HPO 可以通过自动收敛到最佳超参数来大幅减少总体训练时间，而无需进行大量的训练运行。 我们不断添加更多算法和功能。 AgileRL 已经包含了最先进的可进化的在线策略、离策略、离线、多智能体和上下文多臂老虎机强化学习算法以及分布式训练。 我很乐意收到您的反馈！    提交人    /u/nicku_a   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dla20p/p_agilerl_evolutionary_rlops_for_stateoftheart/</guid>
      <pubDate>Fri, 21 Jun 2024 17:49:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 可视化多模态 ACT 模型的注意力图</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dl8en6/d_visualising_attention_maps_for_multimodal_act/</link>
      <description><![CDATA[有人知道如何为 ACT 可视化编码器和解码器变压器注意力图吗？ 观察是机器人本体感受和多摄像头图像数据的组合。输出是一个动作块。模型基于 DETR。 困难的部分是将注意力图分割成可以链接回当前观察的方式。 我认为最有趣的视觉效果是：给定最后一层解码器注意力图和当前观察。模型在观察中关注了什么以生成动作块，即，模型在生成动作块时关注了每个摄像头中图像的哪些部分以及机器人本体感受数据的哪些部分。  ACT 项目页面：https://tonyzhaozh.github.io/aloha/     由   提交  /u/Few_Pangolin4015   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dl8en6/d_visualising_attention_maps_for_multimodal_act/</guid>
      <pubDate>Fri, 21 Jun 2024 16:39:43 GMT</pubDate>
    </item>
    <item>
      <title>致力于将手语文本转化为视频。[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dl7aof/work_on_text_to_video_for_sign_languager/</link>
      <description><![CDATA[我正在研究将手语文本转换为视频。我发现主要的瓶颈是关键点提取。有人在这个领域工作吗？    提交人    /u/One_Definition_8975   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dl7aof/work_on_text_to_video_for_sign_languager/</guid>
      <pubDate>Fri, 21 Jun 2024 15:52:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] [R] 需要帮助：使用机器学习区分胶质母细胞瘤中的放射性坏死与肿瘤进展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dl5lp9/d_r_need_help_using_ml_to_differentiate_radiation/</link>
      <description><![CDATA[嗨， 我有一组 MRI 图像，我想弄清楚一组图像上可见的新病变是由于肿瘤进展还是放射性坏死引起的。 我有软件开发和机器学习的背景，我正在寻找有关 ML 如何帮助解决此问题的见解。根据最新研究，我的理解是，通过结合使用不同的成像技术是可能的。 我正在寻找  经过验证的 ML 模型，可以帮助区分放射性坏死和肿瘤进展 任何有 BRATS 数据集经验并可以提供一些建议的人  谢谢！ 更新：删除了个人背景以更加客观。    提交人    /u/Eastern_Phase_6323   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dl5lp9/d_r_need_help_using_ml_to_differentiate_radiation/</guid>
      <pubDate>Fri, 21 Jun 2024 14:39:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] [D] 使用 biLSTM 进行时间序列预测的健全性检查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dl3lui/r_d_sanity_check_on_use_of_bilstm_for_time_series/</link>
      <description><![CDATA[TLDR; 这篇论文在一篇已发表的论文中使用了 biLSTM，我认为它违反了因果关系。 嗨，我很难说服自己我没有疯。我正在看这篇论文，该论文发表在 Elsevier 期刊《神经网络》上。在这篇论文中，他们使用双向 LSTM 模型（+一些其他新颖的东西）来预测时间序列。这似乎从根本上是错误的，因为 biLSTM 不能/不应该用于时间序列预测。 biLSTM 最著名的用例是在提前知道整个句子的情况下逐字翻译短语。在这种情况下，前面和后面的单词会影响焦点词的含义，从而影响其翻译。一个愚蠢的例子是将其翻译成西班牙语 我需要打针，我被狗咬了 如果您依次扫描每个单词进行翻译，您可能会建议将 w_4（=“shot”）翻译为“inyeccion”，即接种疫苗。知道 w_10 = &#39;dog&#39; 在这里具有重要的预测价值。 同样 我需要一杯 酒，我们去酒吧吧！ w_4 可能会翻译为“chupito”，表示一杯酒，因为 w_9 = &#39;bar&#39; 有影响。 因此，您可以并且应该在这里使用 biLSTM，这样您就可以扫描单词前后的内容以了解上下文。但是，对于时间序列预测，您不知道未来！未来不能影响现在，否则会违反因果关系。在翻译示例中，该句子实际上在说/写之前就已经在人的头脑中创建，因此后面的单词不会违反因果关系。 然而，在本文中，他们在一般时间序列基准上使用 biLSTM，这似乎完全不科学！我是不是漏掉了什么？    提交人    /u/rutherfordofman   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dl3lui/r_d_sanity_check_on_use_of_bilstm_for_time_series/</guid>
      <pubDate>Fri, 21 Jun 2024 13:08:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于分割的图像重要性图</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dl324j/p_importance_map_of_image_based_on_segmentations/</link>
      <description><![CDATA[嗨， 所以我一直在做一个项目，我需要识别图像中的重要区域。数据集包含一张完整的图像以及每个区域重要性的分割（标签为 -1、0、1，其中 -1 表示最不重要，1 表示最重要）。此外，数据集很小（大约 200 张图像）。 我被困住了，想不出有什么我没做过的。我也知道物体显着性检测，但它只提供图像中最重要的物体，而不是重要性图。 我将不胜感激任何帮助、想法或指导。谢谢    提交人    /u/mrex778   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dl324j/p_importance_map_of_image_based_on_segmentations/</guid>
      <pubDate>Fri, 21 Jun 2024 12:41:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于确定电子邮件优先级的分类器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkzs05/p_classifier_for_prioritizing_emails/</link>
      <description><![CDATA[我正在尝试构建一个分类器，使用传统 ML 模型（决策树、逻辑回归等）对电子邮件进行优先级排序  输入：电子邮件正文（矢量化）、主题（矢量化）、字符数 输出：电子邮件优先级（3 个类），使用 LLM（phi3-mini）生成（我知道这是有争议的，但我的老板想要一个模型，但没有数据，所以这是我所知道的唯一“创建”数据的方法） 数据集：7K 行：类 0 - 4k，类 1：2K，类 2：1K（我已经通过添加类权重并主要查看混淆指标来处理类不平衡问题）  我尝试了几种模型，但结果都不理想。 我想知道你们中是否有人遇到过类似的问题像这样。 您认为问题是什么？AI 生成的数据？小数据集？使用传统 ML 模型无法做到这一点？我做错什么了吗？ 任何帮助或见解都将不胜感激    提交人    /u/mr_house7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkzs05/p_classifier_for_prioritizing_emails/</guid>
      <pubDate>Fri, 21 Jun 2024 09:20:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] FP8 现状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkz9k8/d_fp8_current_state/</link>
      <description><![CDATA[我记得当时对 fp8 训练有一些炒作，但问题是它并没有真正得到支持。我最近检查了一下，似乎仍然没有得到太多支持，尽管 rtf 40 系列以及 h100 都支持 fp8。我只是想知道发生了什么，是不是太不稳定了？pytorch 根本不在乎吗？考虑到现代硬件支持它，这对我来说似乎是一个谜     提交人    /u/ClumsyClassifier   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkz9k8/d_fp8_current_state/</guid>
      <pubDate>Fri, 21 Jun 2024 08:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 基于 LLM 的 Python 文档，不会触及你的原始代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkxld2/project_llm_based_python_docs_that_never_touches/</link>
      <description><![CDATA[文档编写繁琐且耗时。我认为 LLM 可能是答案，但它们往往会产生幻觉，发明函数或曲解代码。当您尝试记录真实、有效的代码时，这并不理想 所以我创建了 lmdocs。它可以：  从导入的库中引用文档 保证您的原始代码不变 使用 OpenAI 和本地 LLM  我很乐意从其他开发人员那里获得一些反馈。如果您有兴趣，可以在这里查看：https://github.com/MananSoni42/lmdocs 它是开源的，所以请随意贡献或让我知道您的想法。     由    /u/ford_prefect_9931 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkxld2/project_llm_based_python_docs_that_never_touches/</guid>
      <pubDate>Fri, 21 Jun 2024 06:44:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 NeRF 将视频转换为 VR 体验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkmfei/p_using_nerfs_to_convert_videos_to_vr_experiences/</link>
      <description><![CDATA[大家好，我和一些朋友本周末将参加伯克利 AI Hackathon，我们对我们的项目有一个疯狂的想法。我们想使用 AI 将场景视频转换为 VR 体验。理想情况下，这种体验将是“可行走的”，因为我们会将场景加载到 Unity 中，并将场景加载到 VR 耳机上，并允许用户四处走动。我的背景是 NLP，所以我不知道这个项目的可行性。显然，我们可以尝试一些不那么雄心勃勃的变体，例如只为视频添加深度以使其与 Vision Pro 配合使用。我很想听听大家对这个项目的看法；如果有人可以向我发送资源以便我快速学习 NeRF，那就太好了。最近的论文会很棒，任何公共在线课程都会更好。 提前谢谢！    提交人    /u/ekolasky   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkmfei/p_using_nerfs_to_convert_videos_to_vr_experiences/</guid>
      <pubDate>Thu, 20 Jun 2024 21:03:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] PixelProse 16M 密集图像字幕数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dkkf9k/p_pixelprose_16m_dense_image_captions_dataset/</link>
      <description><![CDATA[      大家好， 希望大家一切顺利。我们想在这里介绍我们小组的一个新项目。希望你喜欢它。 我们使用密集字幕刷新 CC12M、RedCaps 和 CommonPool，以使用 Gemini-1.0 Pro Vision 生成一个新的 16M 数据集，称为 PixelProse，其中包含超过 16M 对图像和密集字幕。希望它对您的项目有用。  arXiv：https://arxiv.org/abs/2406.10328 huggingface repo：https://huggingface.co/datasets/tomg-group-umd/pixelprose  简介图：来自PixelProse。具体短语以绿色突出显示，负面描述以紫色下划线。    提交人    /u/pidoyu   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dkkf9k/p_pixelprose_16m_dense_image_captions_dataset/</guid>
      <pubDate>Thu, 20 Jun 2024 19:37:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>