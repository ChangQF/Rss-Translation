<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 19 May 2024 01:05:41 GMT</lastBuildDate>
    <item>
      <title>[D] 标准化将 NaN 值插入到我的数据框中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvanvo/d_normalization_inserting_nan_values_into_my/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvanvo/d_normalization_inserting_nan_values_into_my/</guid>
      <pubDate>Sat, 18 May 2024 23:45:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否可以用高光谱卫星图像训练 ViTMAE？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</link>
      <description><![CDATA[我正在尝试训练 ViTMAE 编码器来学习一些高光谱卫星图像的表示。图像采用 TIFF 格式，并且有许多波段 (224)。是否可以使用如此大量的输入频段来训练 ViTMAE？知道我应该怎么做吗？   由   提交/u/Robur_131  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</guid>
      <pubDate>Sat, 18 May 2024 23:42:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴收敛速度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</link>
      <description><![CDATA[我正在使用不平衡的数据集训练 mamba 进行顺序标记任务，我有近 800k 的训练示例。在一个时代之后，少数族裔班级的表现非常糟糕，接近于零。我试图过度拟合一批，但无法实现这一目标。我也尝试过减肥。我想知道这是否正常？曼巴是不是从一开始就是这样，然后开始收敛的？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</guid>
      <pubDate>Sat, 18 May 2024 20:37:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 带有 RETSim、Ollama 和 Gemma 的本地 RAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv3n3a/p_local_rag_with_retsim_ollama_and_gemma/</link>
      <description><![CDATA[周六快乐， 这里是关于如何使用 RETSim 我们新颖的近重复文本嵌入的简短文章和笔记本几天前在 ICLR 上亮相，通过与 Ollama、Gemma 和 Usearch 相结合来创建设备上的 RAG。 RETSim 论文，模型和启动代码也是开源的 - 因此，如果您需要近乎重复的检测或相似性计算，请尝试一下！  一如既往，我希望您会发现这些笔记本和文章很有用，我很乐意回答问题。 我迫不及待地想知道您将使用 RETSim 做什么。  p&gt; 周末愉快   由   提交/u/ebursztein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv3n3a/p_local_rag_with_retsim_ollama_and_gemma/</guid>
      <pubDate>Sat, 18 May 2024 18:21:46 GMT</pubDate>
    </item>
    <item>
      <title>[项目]Tabletop HandyBot：用于桌面任务的低成本机械臂助手</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv11oo/project_tabletop_handybot_lowcost_robotic_arm/</link>
      <description><![CDATA[大家好，我想展示我的业余项目的第一个功能版本：Tabletop Handybot，一个低成本的人工智能驱动的机械臂助手，可以聆听根据您的语音命令，可以执行各种桌面任务。  目前它可以拾取并放置出现在它面前的任意对象。它利用了一些最新的人工智能模型，如 ChatGPT、Grounding DINO、Segment Anything 和 Whisper。一切都是零射击，不需要人工智能训练。 BOM 总额为 2300 美元，希望爱好者和修补者能够负担得起。 我希望继续增加支持以处理更多任务。 Github：https://github.com/yheng517/tabletop-handybot   由   提交 /u/astroamaze   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv11oo/project_tabletop_handybot_lowcost_robotic_arm/</guid>
      <pubDate>Sat, 18 May 2024 16:25:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] Grounding DINO 1.5 发布：最强大的开集检测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</guid>
      <pubDate>Sat, 18 May 2024 16:05:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基础时间序列模型被高估了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</link>
      <description><![CDATA[我一直在探索基础时间序列模型，例如 TimeGPT、Moirai、Chronos 等，并且想知道它们是否真的具有强大的样本效率预测潜力，或者它们只是借用了 NLP 中基础模型的炒作并将其带入时间序列领域。 我明白为什么它们可能有效，例如在需求预测中，它是关于识别趋势、周期等。但它们能否处理具有不规则模式和非平稳数据的任意时间序列数据，例如环境监测、金融市场或生物医学信号？ 它们的概括能力是否被高估了？    提交人    /u/KoOBaALT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</guid>
      <pubDate>Sat, 18 May 2024 16:00:06 GMT</pubDate>
    </item>
    <item>
      <title>Transformers 是如何结束神经网络中归纳偏差的传统的 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzvy1/here_is_how_transformers_ended_the_tradition_of/</link>
      <description><![CDATA[      我的视频讨论了归纳偏差和通用性的作用，同时将 Transformer/Attention 与 CNN、RNN 甚至 MLP 等传统深度学习进行比较。   由   提交 /u/AvvYaa   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzvy1/here_is_how_transformers_ended_the_tradition_of/</guid>
      <pubDate>Sat, 18 May 2024 15:32:04 GMT</pubDate>
    </item>
    <item>
      <title>[项目] YOLOv8在INT8中量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzvwx/project_yolov8_quantized_in_int8/</link>
      <description><![CDATA[我在 Nvidia Jetson Orin Nano 中将 YOLOv8x 量化为 INT8，并几乎实时处理（FPS 25）。访问 GitHub 并查看我测试的内容。 https://github.com/the0807/YOLOv8-TensorRT   由   提交/u/the087  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzvwx/project_yolov8_quantized_in_int8/</guid>
      <pubDate>Sat, 18 May 2024 15:32:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] Llamas 用英语工作吗？论多语言 Transformer 的潜在语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10588 代码：https://github.com/epfl-dlab/llm-latent-language 数据集：https://huggingface.co/datasets/wendlerc/llm-latent-language Colab 链接：  (1) https://colab .research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk_QPHBZb7K?usp=sharing (2) https://colab.research.google.com/drive/1EhCk3_CZ_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing 摘要：  我们询问在不平衡的、以英语为主的语料库上训练的多语言语言模型是否使用英语作为内部枢轴语言——这个问题对于理解语言模型如何发挥作用至关重要以及语言偏见的根源。我们的研究重点关注 Llama-2 系列变压器模型，使用精心构建的非英语提示和独特的正确单标记延续。从一层到另一层，变压器逐渐将最终提示标记的输入嵌入映射到计算下一个标记概率的输出嵌入。通过高维空间跟踪中间嵌入揭示了三个不同的阶段，其中中间嵌入（1）从远离输出令牌嵌入的地方开始； (2) 已经允许在中间层解码语义上正确的下一个标记，但给予其英语版本比输入语言版本更高的概率； (3) 最后进入嵌入空间的输入语言特定区域。我们将这些结果转化为概念模型，其中三个阶段分别在“输入空间”、“概念空间”和“输出空间”中运行。至关重要的是，我们的证据表明抽象的“概念空间”是存在的。比其他语言更接近英语，这可能会对多语言语言模型的偏见产生重要影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</guid>
      <pubDate>Sat, 18 May 2024 15:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 鲁棒智能体学习因果世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10877 摘要：  长期以来，人们一直假设因果推理在强大而通用的智能。然而，尚不清楚智能体是否必须学习因果模型才能推广到新领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布变化下满足后悔界限的智能体都必须学习数据生成过程的近似因果模型，该模型收敛到最佳智能体的真实因果模型。我们讨论了这一结果对包括迁移学习和因果推理在内的多个研究领域的影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</guid>
      <pubDate>Sat, 18 May 2024 15:06:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 命名实体识别库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</link>
      <description><![CDATA[大家好，我需要决定使用哪个库进行命名实体识别。我使用过 spaCy，它运行良好，但我需要一个库来允许我对实体和子实体进行分类。有人做过类似的事情吗？我的意思是，同一个词可以是多个实体。 spaCy 提供了 SpanCat 管道，理论上可以实现这一点，但我在创建训练语料库时遇到了麻烦。我认为这是因为他们希望你购买像 Prodigy 这样的注释文本框架。   由   提交/u/Original_Ad8019   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</guid>
      <pubDate>Sat, 18 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>[N] ICML 2024 关于使离散操作可微分的研讨会🤖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</link>
      <description><![CDATA[大家好！ 今年我们将在 ICML 组织可微分几乎所有内容研讨会。&lt; /p&gt; 许多离散操作，例如排序、topk、最短路径、聚类（等等）几乎到处都有零梯度，因此不适合现代基于梯度的学习框架（例如深度学习）。本次研讨会将涵盖旨在解决此类问题的研究主题！ https:// Differentiable.xyz/ 我们鼓励任何从事相关主题工作的人提交他们的作品。即使您没有提交，也请务必参加 ICML 的研讨会，观看即将举行的一些激动人心的演讲！ 我在下面附上了研讨会的完整摘要！祝你当前的工作一切顺利，L :) 梯度和导数是机器学习不可或缺的一部分，因为它们支持基于梯度的优化。然而，在许多实际应用中，模型依赖于实现离散决策的算法组件，或者依赖于离散的中间表示和结构。这些离散步骤本质上是不可微分的，因此破坏了梯度流。要使用基于梯度的方法来学习此类模型的参数，需要将这些不可微分的组件变成可微分的。这可以通过仔细考虑来完成，特别是使用平滑或松弛来为这些组件提出可微的代理。随着模块化深度学习框架的出现，这些想法在机器学习的许多领域变得比以往任何时候都更加流行，在短时间内生成了大量“可微分的一切”，影响了渲染、排序和排名等各种主题，凸优化器、最短路径、动态规划、物理模拟、神经网络架构搜索、top-k、图算法、弱监督学习和自监督学习等等。 本次研讨会将为任何可区分的事物提供一个论坛，汇聚学术界和行业研究人员，突出挑战和发展，提供统一的想法，讨论实际的实施选择并探索未来的方向。   由   提交/u/machine_learning_res   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</guid>
      <pubDate>Sat, 18 May 2024 13:22:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPT-Burn：纯 Rust 中简单简洁的 GPT 实现 🔥</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</link>
      <description><![CDATA[       由   提交/u/ProfessionalDrummer7   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</guid>
      <pubDate>Sat, 18 May 2024 08:25:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>