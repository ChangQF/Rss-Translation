<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Tue, 23 Jul 2024 09:16:24 GMT</lastBuildDate>
    <item>
      <title>[D] 在自定义数据上训练生成模型以完成 QA 任务。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ea2gic/d_training_a_generative_model_on_custom_data_for/</link>
      <description><![CDATA[大家好，我被要求研究如何针对 QA 任务的新事实和信息训练生成模型。我尝试过 RAG，它似乎工作得足够好。但他们更喜欢模型保留知识，这样我们就不必每次都传递相关信息作为上下文。 我听说微调主要改变了模型的输出风格，并没有真正教会它新知识。有没有办法逐步地针对新信息训练生成模型，而不是从头开始？    提交人    /u/BiscuitNGravyy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ea2gic/d_training_a_generative_model_on_custom_data_for/</guid>
      <pubDate>Tue, 23 Jul 2024 08:16:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 编码器-解码器 vs 仅解码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ea0zk4/d_encoderdecoder_vs_decoder_only/</link>
      <description><![CDATA[好吧，理论上，编码器-解码器模型似乎应该工作得更好，因为它增加了输入编码状态的复杂性，可能有助于我更好地规划、推理和减少幻觉，开箱即用。为什么在这方面没有取得太大进展。     提交人    /u/Raise_Fickle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ea0zk4/d_encoderdecoder_vs_decoder_only/</guid>
      <pubDate>Tue, 23 Jul 2024 06:38:37 GMT</pubDate>
    </item>
    <item>
      <title>[D]会不会太天真了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9z4ow/dwill_it_be_too_naive/</link>
      <description><![CDATA[大家好，我正在尝试开展一个 LLM 项目，以申请 MLE 职位。我无法决定项目的主题。你认为 Chat Rob 足够好吗？我可以得到一些来自行业的建议吗？我将不胜感激。    提交人    /u/StageSpecialist5531   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9z4ow/dwill_it_be_too_naive/</guid>
      <pubDate>Tue, 23 Jul 2024 04:40:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] haipera - 一个开源工具，用于为 Python 笔记本和脚本配置配置，无需编写任何代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9xrsn/p_haipera_an_open_source_tool_to_instrument/</link>
      <description><![CDATA[TL;DR：我制作了一个开源（apache 2）工具（https://github.com/haipera/haipera），以便更轻松地使用简单的脚本和笔记本进行超参数扫描。 大家好！我已经在 ML / CV 领域进行了 7 年的研究，我一直对自己花在编写检测代码而不是编写算法上的时间感到沮丧。我所说的检测代码是指：配置管理、配置日志记录、一般日志记录、实验跟踪等... 在我的职业生涯中，我编写了无数的数据类、yaml 文件、json 文件和更多代码，以便通过层层的类层次结构传递这些配置对象的参数 - 只是为了发现我尝试的任何实验都没有结果，现在不得不删除我刚刚添加的内容。经常重复的模因是“机器学习研究人员只做超参数扫描”，但现实是，我们实际上编写代码来传递这些超参数，以便我们可以进行扫描。 这在将代码传输给产品团队时会导致更多问题；产品团队获得的代码包含 600 行 argparse 和从 argparse 复制到初始化程序的代码；这些代码经常有错误并且使跨项目兼容性变得困难。  我也有很多朋友在/曾经在配置系统上工作，试图解决这个问题 - 从 Hydra 到 tyro 到 dysweep 到无数的内部工具。我也是其中之一，但问题是这些库往往变得越来越复杂......因为它们试图变得更“健壮”和更少损坏。编写更多代码几乎永远不是解决方案。 所以我想尝试一种新的范式，它完全抛弃了插桩代码，并依赖于静态解析来插桩代码。这意味着您不必编写一行代码即可为您的代码启用配置之类的功能。最近，随着更好的解析库（如 ast 和 libcs​​t）的出现，这成为可能。展望未来，LLM 也拥有很多令人兴奋的潜力。  这一切是如何运作的？ 给定一个脚本，如下所示： num_apples = 100 apple_price = 3.0 print(&quot;# apples: &quot;, num_apples) print(&quot;price of an apple: &quot;, apple_price) price = num_apples * apple_price print(&quot;total: &quot;, price)  您只需执行 pip install haipera，然后就可以使用 haipera run script.py 运行脚本。您可以运行 haipera run script.py --help 以查看变量是否可直接从 CLI 编辑（目前仅支持全局变量和数字、布尔值、字符串等原始类型）。您可以运行类似 haipera run script.py --apple-price 1.0 的程序来直接从 CLI 设置参数。 当您使用 haipera 运行时，它将在 reports 中创建自己的实验文件夹，并使用自动生成的配置文件填充它，您可以直接重新运行该文件以实现可重复性。 如果您想进行网格扫描，您只需传入多个参数，如 haipera run script.py --num-apples 1,2,3 --apple-price 2.0,3.0,4.0。  您还可以做其他事情，例如 haipera run script.ipynb 以 运行 笔记本作为脚本（如果您想在笔记本内进行开发，但要使用配置作为脚本运行大量实验，这很方便）或 haipera notebook script.ipynb --opt1 2 使用提供的配置启动笔记本的新变体。事实证明，这对于对笔记本进行版本控制也很方便！ 我对这个库感到非常兴奋，并一直从我的研究员朋友那里得到反馈，但我想向大家展示并收集反馈。我们计划使这个库的功能更加完善（例如支持更多类型的变量，通常使一切更加强大，并添加对 GPU 分析工具等的支持） - 但在此之前，我们想听听大家对此的看法，并听听您希望 MLOps 工具中存在哪些类型的功能。 让我们知道您的想法！ https://github.com/haipera/haipera    由    /u/dromger 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9xrsn/p_haipera_an_open_source_tool_to_instrument/</guid>
      <pubDate>Tue, 23 Jul 2024 03:24:55 GMT</pubDate>
    </item>
    <item>
      <title>投影头之后的自监督学习权重初始化 [D][R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9ntgm/selfsupervised_learning_weights_initialization/</link>
      <description><![CDATA[对于大多数自监督学习算法：SimCLR、MoCo、BYOL、SimSiam、SwAV 等，在基础编码器（大多数情况下是原始 ResNet-50 CNN）之后通常有一个投影头。这种投影的一个示例（取自 SwAV）如下： projection_head = nn.Sequential( nn.Linear(2048, 512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Linear(512, 128), )  此投影头的输出是 L2 归一化的： x = project_head(x) x = nn. functional.normalize(x, dim = 1, p = 2)  我试图将投影头后的层初始化为： wts = nn.Parameter(data = torch.empty(40 * 40, 128), require_grad = True) # 投影头输出范围为 [-1, 1] 的权重，因此将 SOM 权重初始化为该范围 - wts.data.uniform_(-1.0, 1.0)  由于投影头的输出是 L2 归一化的，我假设输入范围为 &quot;wts&quot; ∈ [-1, 1]，因此使用上面的统一初始化。 这是正确的方法还是我遗漏了什么？    提交人    /u/grid_world   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9ntgm/selfsupervised_learning_weights_initialization/</guid>
      <pubDate>Mon, 22 Jul 2024 20:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在商业应用中使用 Llama 有哪些问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9lfu3/d_what_are_the_problems_with_using_llama_in_a/</link>
      <description><![CDATA[我搜索了一下，发现一个帖子说 Llama 不应该用于商业用途，但我不知道为什么。我查看了 Llama 的 Meta 许可证，上面说在每月用户数达到 7 亿之前不需要许可证，而我设想的应用程序绝对不可能达到这个数字。 我遗漏了什么？如果我在用户数少得多的商业应用程序中使用 Llama（可能最高每月只有 100 万），会出现问题吗？    提交人    /u/technicallynotlying   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9lfu3/d_what_are_the_problems_with_using_llama_in_a/</guid>
      <pubDate>Mon, 22 Jul 2024 18:24:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] TTSDS – 对最近的 TTS 系统进行基准测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9ec0m/p_ttsds_benchmarking_recent_tts_systems/</link>
      <description><![CDATA[TL;DR - 我为 TTS 做了一个基准测试，您可以在此处查看结果：https://huggingface.co/spaces/ttsds/benchmark 目前有很多 LLM 基准测试，虽然它们并不完美，但它们至少概述了哪些系统在哪些任务上表现良好。文本转语音系统没有类似的东西，所以我决定用我的最新项目来解决这个问题。 我们的想法是找到与不同因素相对应的语音表示：例如韵律、可理解性、说话者等 - 然后根据 Wasserstein 距离计算合成语音与真实数据和噪声数据的分数。我在论文 (https://www.arxiv.org/abs/2407.12707) 中对此进行了更详细的介绍，但我也很乐意在这里回答任何问题。 然后，我将这些因素汇总为一个与合成语音整体质量相对应的分数 - 该分数与从 2008 年的论文一直到 huggingface 最近发布的 TTS Arena 的人工评估分数有很好的相关性。 任何人都可以此处提交自己的合成语音。并且我还将在未来几周内添加更多模型。离线运行基准测试的代码位于此处。    提交人    /u/cdminix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9ec0m/p_ttsds_benchmarking_recent_tts_systems/</guid>
      <pubDate>Mon, 22 Jul 2024 13:29:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] PINN（物理信息神经网络）的方程要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</link>
      <description><![CDATA[我对损失项中的微分方程有疑问。通常，在 PINN 中，我们在损失函数中使用预测输出相对于输入变量的微分方程。例如，如果 u 是预测输出，x、y、m 是输入，则损失函数包括 du/d(x,y,m) 等项。 但是，如果我们只有输入变量相对于其他输入或输出变量的微分方程会怎样？例如：  dx/dt=f(x,y,u) dy/dt=g(x,u)  这里，x 和 y 的导数相对于时间 t。  并且没有 du/d(x,y,m) 方程 在这种情况下是否可以使用 PINN 方法，其中损失函数仅使用 dx/dt​ 和 dy/dt 构建？    提交人    /u/its_a_targaryen   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</guid>
      <pubDate>Mon, 22 Jul 2024 11:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] FLUTE - 一种用于量化 LLM 推理的新型 CUDA 内核，与 vLLM 相比，延迟降低了 2.6 倍。它将 QLoRA 扩展为可学习的尺度，每个参数量化为 4 位和 3 位。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</link>
      <description><![CDATA[ 大型语言模型 (LLM) 的部署通常受内存带宽的限制，其中主要瓶颈是将模型参数从 GPU 的全局内存传输到其寄存器的成本。当与融合反量化和矩阵乘法运算的自定义内核结合使用时，仅权重量化可以通过减少内存移动量来实现更快的推理。然而，为权重量化的 LLM 开发高性能内核带来了巨大的挑战，尤其是当权重被压缩为非均匀可分的位宽（例如 3 位）且使用非均匀查找表 (LUT) 量化时。本文介绍了 FLUTE，这是一种用于 LUT 量化 LLM 的灵活查找表引擎，它使用量化权重矩阵的离线重构来最大限度地减少与解包相关的位操作，并使用查找表的矢量化和复制来缓解共享内存带宽限制。当批量大小 &lt; 32 和量化组大小为 128（LLM 推理中的典型值），FLUTE 内核的速度可以比现有的 GEMM 内核快 2-4 倍。作为 FLUTE 的应用，我们探索了基于查找表的 NormalFloat 量化的简单扩展，并将其应用于将 LLaMA3 量化到各种配置，获得了与强基线相比具有竞争力的量化性能，同时获得了 1.5 到 2 倍的端到端吞吐量提升。  Arxiv：https://arxiv.org/abs/2407.10960    提交人    /u/radi-cho   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</guid>
      <pubDate>Mon, 22 Jul 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 神经网络经过训练，能够使用少 50 倍的数据准确预测分子的最佳几何形状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</link>
      <description><![CDATA[计算化学的一个重要任务是找到实现局部能量最小值的分子几何形状，因为这些是分子发生化学反应的最可能配置。尽管最近在分子构象能量预测的神经网络方面取得了进展，但此类模型容易因分布偏移而出错，从而导致能量最小化不准确。通过提供优化轨迹作为额外的训练数据，可以提高神经网络能量最小化的质量。不过，获得完整的优化轨迹需要大量额外的计算。 一个研究小组开发了一个名为“逐步优化学习框架”（GOLF）的新框架，该框架由一个高效的数据收集方案和一个外部优化器组成。作者证明，使用明显更少的额外数据，用 GOLF 训练的神经网络在各种类药物分子的基准测试中的表现与 Oracle 相当。  该~论文~发表于 ICLR 2024 会议论文集    由    /u/AIRI_Institute  提交  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</guid>
      <pubDate>Mon, 22 Jul 2024 08:04:29 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 文档图像修复</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</link>
      <description><![CDATA[      这是 DocRes 图像在 chainner 中运行的恢复模型用于改进扫描的文档。原始图像后跟恢复后的图像，然后是 chainner 模型。更进一步，使用 Mindee Doctr 非常准确地获取线段。 我正在处理的下一个任务是识别字体大小，然后识别字体样式，然后使用 Microsoft Phi-3 或具有 OCR 功能的类似模型进行 OCR 并应用样式，然后恢复图像 链接 https://github.com/ZZZHANG-jx/DocRes https://github.com/chaiNNer-org/chaiNNer 原始图像 恢复后的图像 Chainner 架构 已识别线段    提交人    /u/atlury   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</guid>
      <pubDate>Mon, 22 Jul 2024 05:25:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用稀疏数据对自定义下游任务的 OS 模型进行微调的最佳实践</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</link>
      <description><![CDATA[我有一项下游任务，在输入过程中，99% 以上的数据都是上下文，由各种来源生成。实际模型输出只有几个标记，但输入的大小可以从 2k 个标记一直到 10k 个标记不等。因此，考虑到较长的上下文窗口，我尝试针对此任务微调 mistral 7b v0.3。但是尝试使用较低的学习率（如 8e-6）并衰减，我仍然会在每次运行时得到越来越高的训练损失。 训练集由标准 input_ids、attention_mask 和标签组成，但由于训练数据的性质，attention_mask 和标签分别大多为 1 和 -100。由于它们的大小也有很大差异，我将数据打包成 4096 的长度，使其保持不变。我的训练机器是 AWS trn1n.32xlarge 类型。关于我应该在这里做什么，有什么建议吗？对于任何对数据集感兴趣的人，这里是直接标记化版本数据的链接。    提交人    /u/VBQL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</guid>
      <pubDate>Mon, 22 Jul 2024 05:03:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] ChessGPT 比 GPT-4 小 100,000 倍，下棋等级为 1500 Elo。通过找到技能向量，我们可以在非分布游戏中将其胜率提高 2.6 倍。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</link>
      <description><![CDATA[之前的一个项目训练了 ChessGPT，这是一组 25M 和 50M 参数的 GPT 模型，可以在 1500 Elo 下棋。这些模型比 GPT-4 的 1.8T 参数小约 100,000 倍。 在 Stockfish 0 级，50M 参数模型的胜率为 70%。但是，如果用 20 个随机动作初始化游戏，其胜率会下降到 17%。这是因为它无法泛化分布之外的内容吗？在考虑下一个标记预测任务时，如果游戏以随机动作开始，那么好的下一个标记预测器会预测合法但低技能的动作。 这就是我们在 ChessGPT 中发现的。通过向模型的激活中添加技能向量，我们可以将其胜率提高到 43%，即提高 2.6 倍。我们无法完全弥补性能差距，但这是一个很大的比例。干预非常简单，更复杂的干预可能会进一步提高其胜率。 该模型仅经过训练以预测 PGN 字符串中的下一个字符（1.e4 e5 2.Nf3 ...），并且从未明确给出棋盘状态或国际象棋规则。尽管如此，为了更好地预测下一个角色，它会学习在游戏的任何时候计算棋盘的状态，并学习各种规则，包括将军、将死、王车易位、过路兵、升级、固定棋子等。此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 Elo 评级。 我们还可以使用可解释性方法来干预模型的内部棋盘状态。 这项工作最近被 2024 年语言建模会议 (COLM) 接受，标题为“国际象棋语言模型中的新兴世界模型和潜在变量估计”。 更多信息请参阅此帖子： https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html 代码在这里： https://github.com/adamkarvonen/chess_llm_interpretability    提交人    /u/seraine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</guid>
      <pubDate>Sun, 21 Jul 2024 19:59:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 与主要作者吴正轩讨论 ReFT 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</link>
      <description><![CDATA[大家好， 本周星期五，我们非常幸运地邀请到了 ReFT 论文的主要作者参加我们的论文讨论，我想分享一下我们的讨论和笔记！ https://www.oxen.ai/blog/arxiv-dives-how-reft-works TLDR ~ ReFT 是一种微调技术，其参数效率比 LoRA 高 15 到 60 倍。训练速度超快。在 A100 上，1k 个示例大约需要 18 分钟。我成功地在不到 1 分钟的时间内，在 A10 上使用大约 100 个示例在 Llama 2 7B 上对 ReFT 进行了微调。 它的工作原理是操作残差流中的表示，而不是 K-V 矩阵。他们向特定的 token 索引和层添加了他们称为“干预”的额外学习参数，从而高效且轻松地控制表示。ReFT 也很不错，因为它们是可组合的。例如，您可以训练一个用于指令跟踪的模型，一个用于德语的模型，然后将它们都应用于德语的获取和指令跟踪模型。 作者给出了他们在实验室中迭代时学到的超级实用的技巧和教训。整个讨论也在 YouTube 上。 希望你喜欢！    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</guid>
      <pubDate>Sun, 21 Jul 2024 16:59:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 21 Jul 2024 02:15:09 GMT</pubDate>
    </item>
    </channel>
</rss>