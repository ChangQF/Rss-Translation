<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 11 Jan 2025 18:20:33 GMT</lastBuildDate>
    <item>
      <title>[N] 我不明白 LORA</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz1xks/n_i_dont_get_lora/</link>
      <description><![CDATA[人们一直给我一行语句，比如 dW =A B 的分解，因此 vram 和计算效率高，但我完全不明白这个论点。  为了计算 dA 和 dB，你不是先计算 dW，然后将它们传播到 dA 和 dB 吗？此时你不需要计算 dW 所需的那么多 vram 吗？并且比反向传播整个 W 需要更多的计算？ 在前向运行期间：你是否在每一步之后都用 W= W&#39; +A B 重新计算整个 W？因为否则你如何使用更新的参数计算损失？  请不要发怒，我不想听到 1。这太简单了，你不应该问 2。问题不清楚 请让我知道哪个方面不清楚。谢谢    由   提交  /u/Peppermint-Patty_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz1xks/n_i_dont_get_lora/</guid>
      <pubDate>Sat, 11 Jan 2025 18:11:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] 构建了一个贪吃蛇游戏，使用扩散模型作为游戏引擎。它几乎实时运行 🤖 它根据用户输入和当前帧预测下一帧。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz1l2j/p_built_a_snake_game_with_a_diffusion_model_as/</link>
      <description><![CDATA[        由    /u/jurassimo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz1l2j/p_built_a_snake_game_with_a_diffusion_model_as/</guid>
      <pubDate>Sat, 11 Jan 2025 17:57:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] GAN 已死；GAN 万岁！现代 GAN 基线</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyxld1/r_the_gan_is_dead_long_live_the_gan_a_modern_gan/</link>
      <description><![CDATA[本文介绍了一种名为 R3GAN 的训练方法，该方法将并行训练与正则化技术相结合，以提高 GAN 的稳定性和输出质量。关键思想是同时运行多个鉴别器-生成器对，同时使用双重正则化策略来防止模式崩溃。 关键技术要点：- 多个 D-G 对并行训练，并使用通信协议共享更新 - 双重正则化方案：鉴别器上的梯度惩罚 + 生成器的多样性项 - 训练期间基于 FID 和初始分数的自适应超参数调整 - 评估不同图像分辨率的多尺度鉴别器架构 - 生成器网络中的残差连接和自适应归一化 结果：- 与基线​​ StyleGAN2/BigGAN 相比，训练速度快 2-3 倍 - 256x256 分辨率下 FFHQ 上的 FID 分数为 2.67 - 在相似质量指标下，多样性分数有所提高（与基线相比 +18%） - 跨不同随机种子和超参数的稳定训练 - 降低计算要求（参数减少 30%） 我认为这代表了 GAN 训练稳定性和效率的重大进展。并行方法具有实际意义 - 我们在其他深度学习领域看到了类似的好处。减少训练时间和参数数量可以使 GAN 更适合计算资源有限的研究小组。 权衡似乎是增加了实施复杂性。我认为最初可能仅限于具有多 GPU 设置且能够正确实施并行训练系统的团队。 TLDR：使用具有正则化的并行鉴别器-生成器对的新型 GAN 训练方法在保持质量的同时实现了更快的训练和更高的稳定性。显示 2-3 倍的加速和具有更少参数的最先进的 FID 分数。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyxld1/r_the_gan_is_dead_long_live_the_gan_a_modern_gan/</guid>
      <pubDate>Sat, 11 Jan 2025 14:57:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] softmax 是否倾向于产生不受约束的欧几里得权重范数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyxijp/d_does_softmax_tend_to_result_in_unconstrained/</link>
      <description><![CDATA[有点愚蠢的问题。当我正在几何分析神经网络动态时，我意识到了一些关于 softmax 的事情。当使用分类交叉熵时，它会导致输出层中的预 softmax 向量的损失值较低，这些向量对于正确的标签轴具有较高的正量级，对于不正确的标签轴具有较高的负量级。我知道正则化技术会将权重更新限制在一定范围内，但我不禁认为 softmax + 交叉熵并不是分类器的一个好目标，即使它会产生概率分布作为输出，因此它“更易于解释”。 只有我？    提交人    /u/Fr_kzd   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyxijp/d_does_softmax_tend_to_result_in_unconstrained/</guid>
      <pubDate>Sat, 11 Jan 2025 14:53:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在哪里可以找到机器学习工程师/人工智能工程师的面试经历？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyskcn/d_where_can_i_find_machine_learning_engineerai/</link>
      <description><![CDATA[我需要了解一些除 glassdoor 之外的候选人的面试经历。我想要一些资源，告诉我面试有多少轮，每轮发生了什么。如果你有这样的资源，请告诉我。    提交人    /u/nanuupendra   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyskcn/d_where_can_i_find_machine_learning_engineerai/</guid>
      <pubDate>Sat, 11 Jan 2025 09:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 检查你的学者统计数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hypgxp/p_check_your_scholar_stats/</link>
      <description><![CDATA[  由    /u/yoonjeewoo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hypgxp/p_check_your_scholar_stats/</guid>
      <pubDate>Sat, 11 Jan 2025 05:56:55 GMT</pubDate>
    </item>
    <item>
      <title>澳大利亚的机器学习和人工智能就业市场如何？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hypeqa/how_is_the_job_market_for_machine_learning_and_al/</link>
      <description><![CDATA[大家好。我是一名驻澳大利亚的研究员，如果可能的话，我想听听你对机器学习市场的看法。我发现了一篇 2 年前的帖子，想了解一下最新的观点。提前谢谢大家。    提交人    /u/BillnoGates   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hypeqa/how_is_the_job_market_for_machine_learning_and_al/</guid>
      <pubDate>Sat, 11 Jan 2025 05:52:52 GMT</pubDate>
    </item>
    <item>
      <title>[数据集][R] 19,762 张垃圾图像，用于构建 AI 回收解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</link>
      <description><![CDATA[大家好，ML 社区！ 我很高兴与大家分享垃圾分类 V2 数据集，其中包含 19,762 张高质量垃圾图像，这些垃圾被分为 10 个不同类别（例如金属、塑料、衣服和纸张）。 为什么这很重要：  训练 AI 模型以实现自动垃圾分类和回收。 开发垃圾分类应用程序或以可持续性为重点的工具。 创建创新的计算机视觉项目以影响环境。  🔗 数据集链接： 垃圾分类 V2 该数据集已在研究论文《通过迁移学习管理家庭垃圾》中使用，证明了其在实际应用中的实用性。 期待看到您如何使用它来促进可持续发展！    提交人    /u/Downtown_Bag8166   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</guid>
      <pubDate>Fri, 10 Jan 2025 21:19:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有关神经网络如何学习扭曲潜在空间以进行预测的资源？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyefw5/d_resources_for_how_neural_nets_learn_to_warp/</link>
      <description><![CDATA[      有哪些好的资源可以进一步了解神经网络如何构建决策面？ 我最近阅读了 Chris Olah 关于“神经网络、流形和拓扑&quot; 以及“关于深度神经网络的线性区域数量&quot; (ICLR ‘14)。 对神经网络如何“学习折叠潜在空间”以进行预测的想法很感兴趣。  我对简单 MLP 层的直觉是，每个组件在这种几何扭曲中都扮演着不同的角色：  激活函数基本上充当了门控机制 (relu) 偏差向量是一种平移操作 矩阵乘法 Wx 可以通过 SVD（W=USV）来理解： U、V 是旋转/反射矩阵 S 是缩放矩阵   这些操作的组合和堆叠产生了这个伟大的数字： https://arxiv.org/abs/1402.1869 还有其他见解或资源可以参考这些想法吗？    提交人    /u/LetsTacoooo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyefw5/d_resources_for_how_neural_nets_learn_to_warp/</guid>
      <pubDate>Fri, 10 Jan 2025 20:43:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练下一个字符预测的 MLP 是否需要因果掩蔽？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hye9ne/d_do_mlps_trained_for_next_character_prediction/</link>
      <description><![CDATA[假设我们有一些数据 X = [seq_len, batch_size] 和相应的标签 Y = [seq_len, batch_size, vocab_size/num/classes] ，独热编码。 现在，我们要训练一个 MLP 来预测下一个字符。 问题：我们是否需要应用因果掩蔽来限制模型在未来的标记中达到峰值？如果是这样，您将它应用在哪一层或输出上？ 在训练期间，模型会看到整个序列并预测相应的独热编码标签。 通常，我见过的大多数示例都使用 X 及其移位版本 `Y = X&#39;` 作为标签来训练下一个字符预测，但这与我的情况不符，因为我已经有独热编码标签。    提交人    /u/kirk86   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hye9ne/d_do_mlps_trained_for_next_character_prediction/</guid>
      <pubDate>Fri, 10 Jan 2025 20:36:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向法学硕士中的系统 2 推理：学习如何使用元思维链进行思考</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hye3gm/r_towards_system_2_reasoning_in_llms_learning_how/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hye3gm/r_towards_system_2_reasoning_in_llms_learning_how/</guid>
      <pubDate>Fri, 10 Jan 2025 20:29:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 小型语言模型通过自我进化的蒙特卡洛树搜索掌握复杂数学</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hycnb4/r_small_language_models_master_complex_math/</link>
      <description><![CDATA[这里的关键创新是一种自我进化机制，它使小型语言模型能够通过迭代改进和自我修正来执行复杂的数学推理。这种方法称为 rStar-Math，它使用结构化的分解和验证步骤来实现与更大模型相当的性能，同时使用更少的参数。 关键技术点： - 生成、评估和改进解决方案的多步骤推理框架 - 随着时间的推移开发更复杂的推理模式的自我进化机制 - 实施验证步骤以捕获和纠正错误 - 将复杂问题结构化分解为可管理的子任务 - 用于数学推理和解决方案验证的专用组件 结果： - 在复杂数学问题上实现 80% 以上的准确率 - 与参数多 10 倍的模型性能相匹配 - 自我修正将准确率提高了约 25% - 适用于多个数学领域 - 在数字和文字问题上都表现出一致的性能 我认为这种方法可以为在资源受限的环境中部署功能强大的 ML 系统带来变革。使用较小模型实现强大性能的能力为边缘设备和计算资源有限的场景开辟了可能性。自我进化机制也可以适用于需要复杂推理的其他领域。 我认为最有趣的方面是系统如何学会捕捉自己的错误并改进其推理过程，类似于人类如何发展数学问题解决技能。这可能导致更强大、更可靠的人工智能系统，可以解释他们的思维并自主纠正错误。 TLDR：小型语言模型可以通过自我进化和结构化验证步骤实现强大的数学推理能力，在使用更少资源的情况下匹配更大的模型。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hycnb4/r_small_language_models_master_complex_math/</guid>
      <pubDate>Fri, 10 Jan 2025 19:28:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 撰写合适的 LLM 摘要的费用出奇地昂贵</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hxzij5/d_creating_proper_llm_summaries_is_surprisingly/</link>
      <description><![CDATA[        提交人    /u/Hot-Chapter48   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hxzij5/d_creating_proper_llm_summaries_is_surprisingly/</guid>
      <pubDate>Fri, 10 Jan 2025 07:57:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>