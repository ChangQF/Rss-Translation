<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 05 Dec 2023 18:17:29 GMT</lastBuildDate>
    <item>
      <title>[D] 你不需要矢量数据库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bhlsj/d_you_do_not_need_a_vector_database/</link>
      <description><![CDATA[RAG 的文档检索问题基本上是信息检索的情况，并且有更简单的解决方案。矢量嵌入仍然有用，但它们应该在 IR 管道的后期使用，而不是作为第一阶段检索，因为第一阶段检索有更简单、性能更高的解决方案。 此处的博客文章：http://about.xethub.com/blog/you-dont-need-a-vector-数据库 此处的笔记本和数据：https://github.com/xetdata/RagIRBench/   由   提交/u/yu Chenglow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bhlsj/d_you_do_not_need_a_vector_database/</guid>
      <pubDate>Tue, 05 Dec 2023 17:44:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 即时工程看起来像是猜测 - 如何正确评估 LLM 申请？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bgqyv/d_prompt_engineering_seems_like_guesswork_how_to/</link>
      <description><![CDATA[人们如何评估您的 LLM 申请的质量？我正在生产中运行一个治疗师聊天机器人（小规模 - 10 多个活跃用户），我花了很多时间微调提示，但这只是猜测。 我将对提示并运行一些测试对话，然后稍微了解一下它是否比调整之前更好或更差。这也是你们都在做的事情还是我错过了什么？？？   由   提交 /u/AndreeSmothers   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bgqyv/d_prompt_engineering_seems_like_guesswork_how_to/</guid>
      <pubDate>Tue, 05 Dec 2023 17:06:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlphaFold 预测是有价值的假设，可以加速但不能取代实验结构确定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bge4o/r_alphafold_predictions_are_valuable_hypotheses/</link>
      <description><![CDATA[社区绝对应该考虑来自科学实践的反馈：https://www.nature.com/articles/s41592-023-02087-4 引用：  &lt; p&gt;我们的结果表明，AlphaFold 预测并不比 PDB 中沉积的模型更好地表示晶体的内容，因为沉积的模型与实验数据更加一致，而预测和沉积的模型不同    由   提交 /u/suhcoR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bge4o/r_alphafold_predictions_are_valuable_hypotheses/</guid>
      <pubDate>Tue, 05 Dec 2023 16:51:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究人员如何产生新机器学习架构的想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bgb2n/d_how_do_researchers_generate_idea_of_new_machine/</link>
      <description><![CDATA[我从事机器学习工作已经有一段时间了，但我仍然专注于应用现有的机器学习技术。  每年我都对产生如此巧妙的建筑的过程感到困惑。如果您有经验，可以分享一下您是如何想出这个新想法的吗？在此过程中，什么对您启发最大？   由   提交 /u/Feisty_Philosophy234   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bgb2n/d_how_do_researchers_generate_idea_of_new_machine/</guid>
      <pubDate>Tue, 05 Dec 2023 16:47:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最近您如何在移动设备上部署计算机视觉？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bg9x1/d_how_are_you_deploying_computer_vision_on_mobile/</link>
      <description><![CDATA[嘿/r/ml,&lt; /p&gt; 我正在构建一个用于图像分类的开源工具，您可以在其中使用网络应用程序构建图像分类器几秒钟。无需挑选恰好包含您的课程的预训练模型，您将获得适合您任务的自定义模型。我们在底层使用 CLIP 和 DinoV2，最终使用线性顶部部署 DinoV2。 我们目前仅提供用于部署的 Python 客户端，但我认为它在移动设备上非常有用，因为该模型相当适合小，您可以部署它，而无需在某个地方使用 API。 人们现在如何在移动设备上部署图像分类？他们通常只使用适合他们班级的预训练模型吗？使用 GPT4+Vision 并且只需按图像付费？或者计算机视觉功能通常被视为需要专业知识，因此大多数人并不真正使用它们？   由   提交/u/nateharada  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bg9x1/d_how_are_you_deploying_computer_vision_on_mobile/</guid>
      <pubDate>Tue, 05 Dec 2023 16:46:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何应对对你的论文是人工智能生成的错误指控？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ben3c/d_how_to_deal_with_false_accusations_of_your/</link>
      <description><![CDATA[读到我从 ICLR-2024 论文中获得的主观评论的质量有点令人沮丧。其中两个人相当不错，但另外两个人指责我的论文是一个“笑话”。和人工智能生成的。看到一个所谓的顶级会议允许公开发布此类不良评论，令人感到悲伤。现在，除非领域主席介入，否则外行人会对我的研究产生极大的偏见。   由   提交 /u/No-Sun-5534   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ben3c/d_how_to_deal_with_false_accusations_of_your/</guid>
      <pubDate>Tue, 05 Dec 2023 15:38:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 加州大学伯克利分校的论文“Sequential Modeling Enables Scalable Learning for Large Vision Models”有一条奇怪的缩放曲线。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bdcu7/r_sequential_modeling_enables_scalable_learning/</link>
      <description><![CDATA[   看到这篇论文“顺序建模实现大视觉模型的可扩展学习” （https://arxiv.org/abs/2312.00785）其中的数字看起来有点奇怪。对于不同的模型尺寸，线条看起来相同。  不同的运行或不同尺寸的大型模型通常是相同的吗？ https:/ /twitter.com/JitendraMalikCV/status/1731553367217070413  ​ 取自 https://arxiv.org/abs/2312.00785 中的图 3 这是完整的图3图 来自https:// arxiv.org/abs/2312.00785   由   提交/u/rantana  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bdcu7/r_sequential_modeling_enables_scalable_learning/</guid>
      <pubDate>Tue, 05 Dec 2023 14:37:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 顺序建模支持大型视觉模型的可扩展学习。 Transformer 接受“视觉句子”（1.64B 图像、420B 图像标记）的训练。同一模型可以执行修复、旋转、光照、语义分割、边缘检测、姿势估计等</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bd348/r_sequential_modeling_enables_scalable_learning/</link>
      <description><![CDATA[博客 - https://yutongbai.com/lvm.html  论文 - https://arxiv.org/abs/2312.00785   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bd348/r_sequential_modeling_enables_scalable_learning/</guid>
      <pubDate>Tue, 05 Dec 2023 14:24:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用交叉注意力的多模态编码器-解码器模型的最新进展是什么？我见过的大多数研究都只使用了 self-attention</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bd0lw/d_what_is_the_latest_with_multimodal/</link>
      <description><![CDATA[我见过的大多数多模态模型都使用编码器来嵌入图像/视频，然后使用自注意力将它们发送到仅 llm 的解码器。  有没有使用交叉注意力代替编码器-解码器模型的研究？   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bd0lw/d_what_is_the_latest_with_multimodal/</guid>
      <pubDate>Tue, 05 Dec 2023 14:21:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Paved2Paradise：通过考虑现实世界的经济高效且可扩展的 LiDAR 模拟</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18bbklt/r_paved2paradise_costeffective_and_scalable_lidar/</link>
      <description><![CDATA[   /u/michaelaalcorn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18bbklt/r_paved2paradise_costeffective_and_scalable_lidar/</guid>
      <pubDate>Tue, 05 Dec 2023 13:06:37 GMT</pubDate>
    </item>
    <item>
      <title>[D]寻找课程推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18baqlb/d_looking_for_course_recommendations/</link>
      <description><![CDATA[我正在尝试拼凑人工智能和离散数学的双硕士学位，并且想知道是否有人有一些课程推荐。以下是我目前计划进行的课程列表。 数学课程： - 现代密码学 (8) - 信息论 (6) ) - 机器学习理论（8） - 图多项式与算法（6） - 因果关系（8） -图对称性和组合设计 (8) - 密码学精选领域 (8) AI 课程： - 机器学习 1 (6) - 深度学习 1 (6) - 计算机视觉 1 (6) - 人工智能的公平性、问责性、保密性和透明度 (6) &lt; p&gt;- 自然语言处理 1 (6) - 知识表示与推理 (6) - 信息检索 1 (6) - 深度学习 2 (6) - 博弈论(6) - 计算社会选择(6) - 算法博弈论(6)   由   提交 /u/Cocorow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18baqlb/d_looking_for_course_recommendations/</guid>
      <pubDate>Tue, 05 Dec 2023 12:17:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] StableSSM：通过稳定的重参数化缓解状态空间模型中的内存诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18b8nn4/r_stablessm_alleviating_the_curse_of_memory_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2311.14495 OpenReview：https:// /openreview.net/forum?id=BwG8hwohU4 摘要：  在本文中，我们研究了长期从参数化的角度来看状态空间模型（SSM）的记忆学习能力。我们证明，没有任何重新参数化的状态空间模型表现出与传统 RNN 类似的内存限制：可以通过状态空间模型稳定近似的目标关系必须具有指数衰减内存。我们的分析确定了这种“记忆诅咒”。由于循环权重收敛到稳定边界，这表明重新参数化技术可能是有效的。为此，我们引入了一类 SSM 重新参数化技术，可以有效解除其内存限制。除了提高逼近能力之外，我们进一步说明重新参数化方案的原则性选择还可以增强优化稳定性。我们使用合成数据集和语言模型验证了我们的发现。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18b8nn4/r_stablessm_alleviating_the_curse_of_memory_in/</guid>
      <pubDate>Tue, 05 Dec 2023 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 仍在进行研究的行业实验室</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18b4xju/d_industry_labs_that_still_do_research/</link>
      <description><![CDATA[我想知道是否还有研究实验室仍在进行探索性研究。我观察到的大多数地方都在朝着更少的探索性工作和更多的具体议程研究方向发展。我认为这阻碍了有趣/探索性的研究，而这通常会带来新的发现。我在攻读博士学位期间进行了此类研究，现在我被困在一个大型研发实验室中，该实验室的任务是研究特定的事情。您会推荐我看哪些实验室，并且仍然可以在某种程度上进行探索？   由   提交/u/oa97z  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18b4xju/d_industry_labs_that_still_do_research/</guid>
      <pubDate>Tue, 05 Dec 2023 05:35:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种架构可以替代变压器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</link>
      <description><![CDATA[我最近读到了这个观点，讨论变压器可以被更换。  https://towardsdatascience.com/a-requiem-for-the-transformer -297e6f14e189 总的来说，过去几个月发表的文章显示了 Transformer 的局限性： https://arxiv.org/abs/2203.15556 https:/ /arxiv.org/abs/2304.15004  在计算机视觉中，具有相同预算的ConvNet似乎具有相似的性能： https://arxiv.org/abs/2310.19909  https://arxiv.org/abs/2310.16764  DeepMind 表明 Transformer 无法泛化到训练集分布之外： https://arxiv.org/abs/2311.00871 液体神经网络、鬣狗、尖峰神经网络等模型显示出活跃的搜索对于新架构： https://arxiv.org/pdf/2006.04439.pdf&lt; /p&gt; https://www.together.ai/blog/monarch-mixer  并不是说 Transformer 很快就会被取代，但我现在想知道下一个主导架构可能是什么？  所提出的架构似乎都不比变压器具有竞争优势   由   提交/u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/</guid>
      <pubDate>Mon, 04 Dec 2023 17:44:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>