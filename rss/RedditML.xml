<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 18 Sep 2024 01:11:41 GMT</lastBuildDate>
    <item>
      <title>[D] vLLM 批处理与前缀缓存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fjao60/d_vllm_batching_vs_prefix_caching/</link>
      <description><![CDATA[嗨，试图弄清楚 vLLM 实现中批处理和前缀缓存之间的区别，特别是它们是否可以一起使用。 对于上下文：- 尝试使用 LLM 模型回答文档上的二元分类问题（例如“基于上下文 x 是真还是假？”）并构建问题的 json 作为特征键和分类。对于像 chatGPT 这样的较大模型，它通过使用带有输出格式和字段描述的 pydantic json 模式提示来帮助模型对每个特征进行分类，效果很好。为了实现可扩展性和更多控制，现在正在考虑使用自托管的较小开源模型。 我发现当使用更具体的范围（即单个真/假问题）提示时，较小的模型会更好地工作。因此，我计划迭代多个请求并将分类构建为二进制任务并从响应中构造输出 json，而不是要求模型生成输出。 对于 vLLM，我希望前缀缓存可以通过在末尾附加每个任务来加快迭代速度，并且我认为批处理在此任务中无法表现更好？ 如果作为上下文包含的文档太长，我可以将文档的各块批处理为并行输入，并使用每个块的缓存来迭代任务吗？    提交人    /u/Lower_Tutor5470   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fjao60/d_vllm_batching_vs_prefix_caching/</guid>
      <pubDate>Tue, 17 Sep 2024 20:58:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们现在到底该如何加载 .yaml 配置？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj6o3a/d_how_the_heck_are_we_supposed_to_load_yaml/</link>
      <description><![CDATA[TensorFlow 模型库中充满了 .yaml 配置，但 model_from_yaml 函数已不存在。我为此绞尽脑汁好几个小时，谷歌帮不上忙，就连 stackoverflow 也帮不上忙。 我们是不是要整个 TF 模型库都用上？    提交人    /u/radome9   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj6o3a/d_how_the_heck_are_we_supposed_to_load_yaml/</guid>
      <pubDate>Tue, 17 Sep 2024 18:23:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Pandas / Dask 等为 ML / DS 加密数据。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj5gxf/p_encrypting_data_for_ml_ds_with_pandas_dask_etc/</link>
      <description><![CDATA[* 由于标题中缺少标签而重新发布 刚刚发布了一个模块，使通过 Python / Pandas / Dask / CLI 和云资源进行数据加密变得更容易。我们已经在 fsspec 上实现了 AES-256 CBC https://pypi.org/project/fsspec-encrypted/ 来源 https://github.com/thevgergroup/fsspec-encrypted 许可 MIT 允许轻松在本地或远程读取和写入 例如 import pandas as pd from fsspec_encrypted.fs_enc_cli import generate_key crypto_key = generate_key(passphrase=&quot;my_secret_passphrase&quot;, salt=b&quot;12345432&quot;) #local df = pd.read_csv(f&#39;enc://./.encfs/encrypted-file.csv&#39;, storage_options={&quot;encryption_key&quot;: crypto_key}) # 用 fsspec-encrypted 包装的 S3 请求 df = pd.read_csv(f&#39;enc://s3://{bucket}/encrypted-file.csv&#39;, storage_options={&quot;encryption_key&quot;: crypto_key}) # 与 gcs、abfs、adl、az、hf 等类似。  甚至有一个 CLI，因此脚本编写可以更容易，并允许您即时加密/解密 即将推出更多更新。  我们的目标是帮助减少未加密存储在磁盘上的 PII/PHI 或其他敏感数据的数量。   由    /u/olearyboy  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj5gxf/p_encrypting_data_for_ml_ds_with_pandas_dask_etc/</guid>
      <pubDate>Tue, 17 Sep 2024 17:35:55 GMT</pubDate>
    </item>
    <item>
      <title>[N] 介绍 CodonTransformer：一种使用上下文感知神经网络的多物种密码子优化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj58d1/n_introducing_codontransformer_a_multispecies/</link>
      <description><![CDATA[大家好，机器学习社区， 我很高兴与大家分享我们的最新成果：CodonTransformer，这是一种最先进的 Transformer 模型，可优化 DNA 序列，以实现 164 个物种 中的异源蛋白质表达！ 什么是 CodonTransformer？ CodonTransformer 利用深度学习生成具有自然密码子分布的宿主特异性 DNA 序列，同时最大限度地减少负调控元件。它经过来自各种生物体的超过 100 万个基因-蛋白质对的训练，在效率和准确性方面均优于现有的密码子优化工具。  主要特点：  🔬 STREAM 训练策略：我们引入了一种名为 STREAM 的新型训练方法，使我们的仅编码器模型能够有效地执行针对生物体定制的蛋白质到 DNA 的翻译。 🧬 高级揭露机制：该模型将 20 个可能的掩码标记 [aminoacid_UNK] 揭露为 64 个未掩盖的 [aminoacid_codon] 标记，从而实现精确的密码子选择。 🌐 开源且可扩展：我们发布了最大的密码子优化 Python 库以及我们的数据和模型，因此您可以根据自己独特的研究需求对 CodonTransformer 进行微调。  为什么这很重要？ 随着生物技术的进步，跨物种优化基因表达物种比以往任何时候都更加重要。 CodonTransformer 旨在弥补异源蛋白质表达方面的差距，使其成为计算生物学、合成生物学和相关领域研究人员的宝贵工具。 探索 CodonTransformer：  🌎 网站： adibvafa.github.io/CodonTransformer 💻 GitHub： GitHub.com/Adibvafa/CodonTransformer （如果您觉得有用，我们将不胜感激 ⭐！） 🛠 Colab Notebook： 在 Google Colab 上试用 🤖 Hugging Face 上的模型： CodonTransformer 模型 📄 预印本论文： bioRxiv 上的 CodonTransformer  我们很乐意听取您的反馈！ 请随意深入研究代码，测试模型，并让我们知道您的想法。您的见解和反馈对我们和社区来说都是无价的。 期待您的想法和讨论！    提交人    /u/Ornery_Historian_526   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj58d1/n_introducing_codontransformer_a_multispecies/</guid>
      <pubDate>Tue, 17 Sep 2024 17:26:49 GMT</pubDate>
    </item>
    <item>
      <title>需要对研究课题提出建议 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fj4vq2/need_suggestions_for_a_research_topic_d/</link>
      <description><![CDATA[大家好，我正在研究一个课题，其中我有不同的模型，这些模型具有相同的架构，但在不同的用户数据（序列数据）上进行训练，但我想对这些模型进行聚类。 我怎样才能做到这一点？ 我尝试使用模型参数，但它不起作用。 我的模型架构是 Transformer Autoencoder，它具有编码器和解码器，并提供重建错误作为输出。 任何建议都会有所帮助。 谢谢     提交人    /u/Potential_Plant_160   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fj4vq2/need_suggestions_for_a_research_topic_d/</guid>
      <pubDate>Tue, 17 Sep 2024 17:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[N] Llama 3.1 70B，Llama 3.1 70B Instruct 压缩了 6.4 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/</link>
      <description><![CDATA[我们最近对 Llama 3.1 70B 和 Llama 3.1 70B Instruct 模型的研究实现了 6.4 倍的压缩率，同时保留了大部分 MMLU 质量。如果您有 3090 GPU，您现在就可以在家运行压缩模型。  以下是结果和压缩模型： https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16 https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main   由    /u/_puhsu  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/</guid>
      <pubDate>Tue, 17 Sep 2024 10:15:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助我计算对非常长的上下文示例进行微调所需的内存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fitqeu/d_help_me_calculate_memory_requirements_for/</link>
      <description><![CDATA[我有四个 L4 gpu（总共 96 GB vram）。我正在尝试使用 QLoRA（4 位量化，q 的等级 = 4，k 投影矩阵）微调 Llama 3.1 70b。该模型大约需要 40 GB 才能加载（每个 gpu 大约使用 10 GB）。我只有 10 个示例想要在语言建模任务上进行训练。但这些示例非常长（每个 40,000 个标记，因为 llama 3.1 支持 128k 上下文长度）。我遇到了内存不足错误。我认为这是因为示例上下文长度几乎是 40000 个标记，并且它无法在其他 gpu 上分配单个示例。请帮我计算一下我需要一个 gpu 上多少 vram。    提交人    /u/Elemental_Ray   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fitqeu/d_help_me_calculate_memory_requirements_for/</guid>
      <pubDate>Tue, 17 Sep 2024 08:27:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 好的图形数据库选项？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fit4px/d_good_graph_database_options/</link>
      <description><![CDATA[我正在尝试构建一个 graphRAG 并使用其图形数据库，到目前为止，所有内容都指向 neo4j。我们还有其他更好、更适合生产的选项吗？    提交人    /u/Aromatic_Ad9700   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fit4px/d_good_graph_database_options/</guid>
      <pubDate>Tue, 17 Sep 2024 07:46:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 LLM 进行场景、模拟和专业战争游戏创作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fis46d/p_scenario_simulation_and_professional_wargame/</link>
      <description><![CDATA[https://github.com/user1342/WargamesAI    由   提交  /u/OppositeMonday   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fis46d/p_scenario_simulation_and_professional_wargame/</guid>
      <pubDate>Tue, 17 Sep 2024 06:39:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您使用什么工具进行 AI/ML 开发、训练和推理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fiqcxh/d_what_tools_do_you_use_for_aiml_development/</link>
      <description><![CDATA[我很好奇您在 AI/ML 工作流中使用的企业工具。无论是用于开发、训练、推理还是使用预构建模型，我都很想知道您每天依赖什么。  开发和训练：您喜欢哪些平台或服务来构建和训练模型？ 推理和部署：您使用哪些工具来大规模提供模型？ 预构建模型：您是否使用 Hugging Face 或 OpenAI 等平台来提供现成的模型？ 数据和实验跟踪：您推荐什么工具来管理数据集和跟踪实验？  期待您的见解！谢谢！    提交人    /u/Pretend-Lobster6455   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fiqcxh/d_what_tools_do_you_use_for_aiml_development/</guid>
      <pubDate>Tue, 17 Sep 2024 04:53:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于不同训练“技巧”的效果的良好研究，例如学习率调度程序（热身/衰减）、权重衰减、dropout、批量大小、动量等？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fihdrd/d_good_studies_on_the_effects_of_different/</link>
      <description><![CDATA[鉴于学习率调度程序（例如线性热身/余弦衰减）、正则化（权重衰减）、dropout、批量大小、动量项（Adam 中的 beta1、beta2）、批量规范等“技巧”的数量变得相当大，并且在这些大型模型上检查这些参数的所有不同组合变得越来越困难，是否有任何现有的研究或众包努力研究当我们改变这些技巧的各种参数时对最终性能（例如验证困惑度）的影响？ 我敢打赌其中很大一部分是在消融研究中，但它们有点太分散了。    提交人    /u/ThienPro123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fihdrd/d_good_studies_on_the_effects_of_different/</guid>
      <pubDate>Mon, 16 Sep 2024 22:01:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 扩展 - 推理 8B 和训练 405B 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fic61y/d_scaling_inferencing_8b_training_405b_models/</link>
      <description><![CDATA[感谢你们组成了这么棒的社区！ 我一直在尝试寻找指南来扩展更大模型的训练/推理设置，但在训练细节方面，我找不到任何不费解的东西。如果您能分享任何指南或帮助解答（或部分解答）我的问题，那将非常有帮助。我希望这能帮助其他希望扩展训练/推理设置的人。 设置：我有两个 24GB VRAM（7900XTX）和 128GB RAM/AMD 7900X，两个节点上各有一个，通过 Infiniband 连接。我正在试验 Llama 3.1 8B 模型（未量化）。 当前状态：当我将 8B 模型加载到 GPU 上时，我看到 16GB 已分配/16GB 已保留  使用 FSDP（FULL_SHARD）拆分模型仍然显示 8GB 已分配/16GB 已保留。 a) 为什么要保留完整的 16GB？是为了从其他分片传输层吗？ b) 有没有办法手动管理该保留？ c) FULL_SHARD 需要 100 倍的时间来处理相同的请求（可能是由于网络限制）。 5 个提示在没有分片的情况下花费了 30 秒，但使用 FULL_SHARD 和 40Gbps Infiniband 时花费了 3000 秒。 在不使用任何分布式技术的情况下，该模型占用了 16GB VRAM，添加“-max_seq_len 8000”会预分配/保留另外 6GB VRAM。但是，当我给它一个 7000 个令牌的提示时，它会抛出 CUDA OOM，即使在预分配之后也是如此。 a) 是因为预分配是为了“平均”提示长度估计而完成的吗？ b) 如何将此推理设置扩展到 24 GB 卡上的 CUDA OOM 限制之外（即使有人有 100 张 24GB 卡？）？所有查询都可以在“-max_seq_len 5000”下正常工作设置（如果提示较长，则只会显示令牌用完）。 c) 在半商业环境中，是否有人实现过超过 20K 个令牌？我看不出有人能达到 128K 个令牌。 如何推断更大的模型，例如 70B 模型？我认为需要 FSDP 类型的框架，但即使在 100Gbps 卡上也会非常慢。 更大的 405B 模型的训练设置是什么样的？ a) 即使我们使用 FSDP，考虑到 Grads 和 Optimizer States 所需的 VRAM 以及网络限制，我发现很难在任何合理的时间内处理数万亿个令牌，因为网络可能是 O(n^2) 约束，其中 n 是分片的层数。我感觉我错过了一些东西。 b) 即使网络不是问题，在加载碎片后，我们如何将 128K 代币放入卡中？例如，如果碎片本身就占用了 60-70% 的内存，我们如何为 10K 或 20K 代币（更不用说 128K 代币）腾出空间。在我看来，这最终会成为 H100 卡以及万亿参数模型（MoE 或非 MoE）的问题。  我正在通过添加 10 7900 XTX 设置来扩展我的设置，但我真的很想在继续购买之前弄清楚这些细节。谢谢！    提交人    /u/gulabbo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fic61y/d_scaling_inferencing_8b_training_405b_models/</guid>
      <pubDate>Mon, 16 Sep 2024 18:33:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] CPL：关键规划步骤学习提升 LLM 在推理任务中的泛化能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fi7ovt/r_cpl_critical_planning_step_learning_boosts_llm/</link>
      <description><![CDATA[      TL;DR 通过 MCTS 和每步优势偏好优化提高 LLM 的规划能力 论文： https://arxiv.org/pdf/2409.08642 摘要：  对大型语言模型 (LLM) 进行后训练以开发推理能力已被证明在数学推理和代码生成等不同领域都很有效。然而，现有方法主要侧重于改进特定任务的推理能力，但尚未充分解决模型在更广泛的推理任务中的泛化能力。为了应对这一挑战，我们引入了关键规划步骤学习 (CPL)，它利用蒙特卡洛树搜索 (MCTS) 探索多步骤推理任务中的不同规划步骤。基于长期结果，CPL 学习步骤级规划偏好，以提高模型的规划能力，从而提高其一般推理能力。此外，尽管现有的偏好学习方法（如直接偏好优化 (DPO)）在许多对齐 LLM 的场景中都很有效，但由于无法在每个步骤中捕捉细粒度的监督，因此在处理复杂的多步骤推理任务时会遇到困难。我们提出了步骤级优势偏好优化 (Step-APO)，它将通过 MCTS 获得的步骤级偏好对的优势估计集成到 DPO 中。这使模型能够更有效地学习关键的中间规划步骤，从而进一步提高其在推理任务中的泛化能力。实验结果表明，我们的方法专门针对 GSM8K 和 MATH 进行训练，不仅显著提高了 GSM8K (+10.5%) 和 MATH (+6.5%) 的性能，而且还增强了域外推理基准，例如 ARC-C (+4.0%)、BBH (+1.8%)、MMLU-STEM (+2.2%) 和 MMLU (+0.9%)。  视觉摘要： https://preview.redd.it/afih1qsaw6pd1.png?width=975&amp;format=png&amp;auto=webp&amp;s=bd0c3c4385897c581dff193a02267052481a0e68 性能： https://preview.redd.it/hehec8txw6pd1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=a3d069590221c7acd509e1af83ea07a691f4e507 https://preview.redd.it/xgm1m55zw6pd1.png?width=1125&amp;format=png&amp;auto=webp&amp;s=566f067d6bff31605b04f0ec0edb79c35945f77f    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fi7ovt/r_cpl_critical_planning_step_learning_boosts_llm/</guid>
      <pubDate>Mon, 16 Sep 2024 15:35:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 15 Sep 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>