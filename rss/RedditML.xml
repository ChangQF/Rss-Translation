<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 21 Feb 2024 21:11:17 GMT</lastBuildDate>
    <item>
      <title>[D] Gemma vs Mistral-7B-v0.1 评估：Gemma 确实很难达到 Mistral 的准确度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awmizp/d_gemma_vs_mistral7bv01_evaluation_gemma_really/</link>
      <description><![CDATA[      Google DeepMind 的新模型 Gemma 在医疗/保健领域基准测试中并未表现出强劲的性能。未经微调的 Gemma by Gemma 与 Mistral by Mistral AI 的并排比较。  米斯特拉尔显然获胜： https://preview.redd.it/3h883kbi00kc1.png?width=721&amp;format=png&amp;auto=webp&amp;s=7ca853f9cd9b7ee0d1c230541e759867c4c6d67d  我会没事的-调整和评估 Gemma &amp;在接下来的几天里，我们将在不同的医疗和法律基准上获得不同的法学硕士。请关注此处的更新：https://twitter.com/aadityaura &lt; !-- SC_ON --&gt;  由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awmizp/d_gemma_vs_mistral7bv01_evaluation_gemma_really/</guid>
      <pubDate>Wed, 21 Feb 2024 20:32:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 《Scalable Diffusion Models with Transformers》一书的两位作者之一关于 OpenAI Sora 的 Twitter/X 帖子：“这是我对 Sora 技术报告的看法，其中包含大量可能完全错误的猜测。[。 ..]”。这项工作的另一位作者是 OpenAI 的 Sora。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awmces/d_twitterx_thread_about_openais_sora_from_one_of/</link>
      <description><![CDATA[展开的 Twitter/X 线程。 线程中的第一条推文，我通过Yann LeCun 的这条推文。  这是我对 Sora 技术报告的看法，其中有大量可能完全错误的猜测。首先，非常感谢团队分享有用的见解和设计决策 - Sora 令人难以置信，并将改变视频生成社区。  到目前为止我们学到了什么： - 架构：Sora 是基于我们的扩散变压器 (DiT) 模型（在 ICCV 2023 中发布）构建的 - 它是具有变压器骨干的扩散模型，简而言之： DiT = [VAE 编码器 + ViT + DDPM + VAE 解码器]。  根据报告，似乎没有太多额外的花哨的东西。  [...]  可扩展使用变压器的扩散模型。 Sora 技术报告. 来自该作品另一位作者的推文：  索拉来了！它是一个扩散变压器，可以生成长达一分钟的 1080p 视频，具有良好的连贯性和质量。 @ /_tim_brooks 和我在 @ /openai 已经为此工作了一年，我们对通过模拟一切来追求 AGI 感到兴奋！ http://openai.com/sora  相关帖子：[D] OpenAI Sora Video Gen - 怎么样？ &lt;!-- SC_ON - -&gt;  由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awmces/d_twitterx_thread_about_openais_sora_from_one_of/</guid>
      <pubDate>Wed, 21 Feb 2024 20:24:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemma vs Mistral（和其他开放模型）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awlqyt/d_gemma_vs_mistral_and_other_open_models/</link>
      <description><![CDATA[在模型大小和定量性能方面与基准进行了大量比较。希望了解人们对定性临时绩效的看法。  &gt;比较游乐场：​​https://huggingface.co/spaces/lastmileai/gemma-playground &lt;在这个例子中，Gemma 2B 和 7B 似乎在 CoT 任务中对抗 Mistral 的表现都不佳。很好奇它在指导、问答和创造力任务方面的表现如何。    由   提交 /u/InevitableSky2801   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awlqyt/d_gemma_vs_mistral_and_other_open_models/</guid>
      <pubDate>Wed, 21 Feb 2024 20:01:11 GMT</pubDate>
    </item>
    <item>
      <title>[N] 语言处理单元 (LPU) 使 LLM 的推理速度提高 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</link>
      <description><![CDATA[本周，一家鲜为人知的公司 Groq 展示了运行 Llama-2 等开源 LLM 的前所未有的速度（ 700 亿个参数），每秒超过 100 个令牌，而 Mixtral 在 Groq 的语言处理单元 (LPU) 上每用户每秒近 500 个令牌。 对于比较：   “根据 Groq 的说法，在类似的测试中，在典型的基于 GPU 的计算系统上，ChatGPT 的加载速度为每秒 40-50 个令牌，而 Bard 的加载速度为每秒 70 个令牌。 每个用户每秒 100 个令牌的上下文，展示了在 Groq 语言处理上以每秒超过 100 个令牌的速度运行开源 LLM 的前所未有的速度，例如 Llama-2（700 亿个参数），以及每秒每用户近 500 个令牌的 Mixtral单位（LPU）。  那么：LPU是什么，它是如何工作的，Groq在哪里（这么不幸的名字，考虑到马斯克的Grok都结束了）媒体）来自哪里？ https://www.turingpost.com/p/fod41    由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awjheu/n_language_processing_unit_lpu_makes_inference_of/</guid>
      <pubDate>Wed, 21 Feb 2024 18:32:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代扩散采样器。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awit31/d_modern_samplers_for_diffusions/</link>
      <description><![CDATA[现在扩散模型使用的默认采样器是什么？论文按歌曲： https://arxiv.org/pdf/2011.13456.pdf 提出了一些建议。现在 PC 采样器是默认设置吗？还是人们会做其他事情？ ​ 谢谢！   由   提交/u/randomkolmogorov   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awit31/d_modern_samplers_for_diffusions/</guid>
      <pubDate>Wed, 21 Feb 2024 18:05:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] marimo-wasm：浏览器中的反应式 Python 笔记本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awimon/p_marimowasm_a_reactive_python_notebook_in_the/</link>
      <description><![CDATA[我们（2 位开发人员）使 marimo[1] 与 WebAssembly (WASM) 兼容，因此您可以完全在浏览器中运行它，这要归功于 Pyodide。 您可以尝试游乐场：https://marimo.app/&quot;&gt;https:// /marimo.app/。这是学习 Python 或教育他人的绝佳工具，因为您可以通过 URL 共享代码片段。例如，以下是 贝叶斯定理. [1] marimo 在 GitHub 上开源：https://github.com/marimo-team/marimo   由   提交 /u/mmmmmmyles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awimon/p_marimowasm_a_reactive_python_notebook_in_the/</guid>
      <pubDate>Wed, 21 Feb 2024 17:59:05 GMT</pubDate>
    </item>
    <item>
      <title>小数据集中交叉验证的意义是什么[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awfsw8/what_is_the_meaning_of_crossvalidation_in_a_small/</link>
      <description><![CDATA[据我所知，在深度学习的背景下，k 折交叉验证旨在为超参数分配训练/验证对的多个子集调整。更不用说独立的测试数据集了，每个训练/验证折叠都应该对应于一个使用不同超参数训练的“最终”模型，以便我们可以比较超参数的效果。 然而，在一个小数据集中，数据在一个子集中可能无法泛化，我如何知道不同子集中的模型性能差异是来自数据的偏差还是所使用的超参数的差异？ （例如，在 CNN 中，我们的数据通常有限，而图像通常包含显着不同的特征）   由   提交/u/alan6690  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awfsw8/what_is_the_meaning_of_crossvalidation_in_a_small/</guid>
      <pubDate>Wed, 21 Feb 2024 16:08:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 图+向量数据库？需要您的意见：Cognee.ai。用于现实世界生产的 AI 数据管道（第 4 部分）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/</link>
      <description><![CDATA[      嘿，Reddit 用户！ 我带着关于为现实世界生产创建可靠的 AI 数据管道的最新文章回来了。&lt; /p&gt; 如果您一直在关注，您就会知道我的使命是超越“瘦 OpenAI 包装器”趋势并应对构建强大数据管道的挑战。 经过几个月的工作，我们将认知架构与 keepi.ai&lt; /a&gt;  我们的目标是通过我们的演示进行探索： 1.上下文清理人工智能的世界正在快速发展，我们已经意识到上下文正在成为我们所说的未来认知架构的重要组成部分。 2.人工智能内存的最佳实践 在这个快速发展的环境中，没有既定的最佳实践。您需要对工具和流程进行有根据的押注，因为您知道事情会发生变化。我们假设拥有传统的数据工程实践+框架+分类器和其他人工智能解决方案可以解决很多标准障碍 3。人工智能框架他们试图做得太多、太快、太广泛。我们希望为人工智能内存找到一种模式和正确的抽象层，以适应新的行业。  ​ 它是如何工作的？  Github 存储库为 l: cognee 的工作原理 Github 存储库位于此处 后续步骤：我有问题要问您：  上下文清理与您相关吗？ 您如何管理元数据？  您如何为法学硕士准备数据？ 您执行任何数据丰富步骤吗？  查看博客文章： 您执行任何数据丰富步骤吗？ p&gt; 链接到第 4 部分 如果您觉得这篇文章很有见地，请记得给它点赞！ 并给我们加注星标 Github 存储库   由   提交/u/Snoo-bedooo  /u/Snoo-bedooo reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/</guid>
      <pubDate>Wed, 21 Feb 2024 15:23:12 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 研究人员（硕士、博士）如何实现复杂模型？他们是神吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</link>
      <description><![CDATA[我现在正在做我的论文。我很好地掌握了大多数 ML 模型（RNN、CNN、LSTM、Transformers、GPT、CNN、GAN、LDM、VAE、自动编码器等）的高级细节。当然，我绝不是专家，但我能够学习我需要的东西。 但是当真正使用它们，并在代码中实现它们并训练它们时，这就变成了地狱。对于更简单的模型，还好，但是对于更复杂的模型，网上没有教程，他们只是说“使用现有模型”。 世界各地的研究人员如何实现复杂的模型？例如，扩散模型、LDM 或修改后的 LLM，如 Transformer 或 GPT？ 或者它们如何更改现有模型，并使用不同的技术，例如添加编码器进行调节？  &gt;就像，研究和理解基础知识很好，但实际实施起来却非常困难。他们是如何做到如此优雅的？一些调查研究论文包括多种模型的使用和比较。他们是怎么做到的？   由   提交 /u/ShlomiRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awe3ld/dr_how_do_researchers_masters_phd_implement/</guid>
      <pubDate>Wed, 21 Feb 2024 15:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 简单的 Javascript 代码，可以帮助士兵和平民躲避无人机袭击（更新为立即战斗部署）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awdn2r/r_simple_javascript_code_that_could_help_soldiers/</link>
      <description><![CDATA[https://www.academia.edu/115181929 /Aerial_Object_Detection_updated_for_combat_deployment_ 当找到空中物体时，应用程序会发出蜂鸣声。空中物体在您附近盘旋的时间越长，蜂鸣声就越长。对于士兵来说，这可能意味着无人机正在瞄准他们。理想情况下，士兵可以在手机上使用该应用程序，并将该设备连接到车辆的顶部区域或在战壕中睡觉时连接到身体上。请记住，必须取出 SIM 卡，并且手机无线连接必须保持“关闭”状态。在战斗环境中。在部署之前，士兵应该连接到 WiFi 并启动应用程序。应用程序启动后，士兵可以在部署到战区时禁用 WiFi 并保持应用程序运行。为了在战斗中检测空中物体，Android手机应安装在背包顶部或头盔顶部。 在民用环境中，打开无线功能的手机可以放置在屋顶上。通过互联网，用户可以通过 facebook live 远程查看航拍场景   由   提交 /u/AnthonyofBoston   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awdn2r/r_simple_javascript_code_that_could_help_soldiers/</guid>
      <pubDate>Wed, 21 Feb 2024 14:40:52 GMT</pubDate>
    </item>
    <item>
      <title>[新闻]Google发布全新开放的LLM模型：Gemma模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</link>
      <description><![CDATA[明显比 llama7 和 13 更好（但不与 Mistra7b 进行基准测试）： https://blog.google/technology/developers/gemma-open-models/   由   提交 /u/edienemis   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awc179/news_google_release_new_and_open_llm_model_gemma/</guid>
      <pubDate>Wed, 21 Feb 2024 13:28:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将语言模型扩展到 128K 上下文的数据工程 - MIT 2024 - 具有 128k 上下文的新开放 LLaMA-2 7B 和 13B！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awagj3/d_data_engineering_for_scaling_language_models_to/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.10171 Github： https://github.com/FranxYao/Long-Context-Data-Engineering 内部有 128k 上下文的新模型！&lt; /p&gt; 摘要：  我们研究了将语言模型的上下文长度扩展到 128K 的持续预训练方法，重点是数据工程。我们假设长上下文建模，特别是在任意输入位置利用信息的能力，是一种大部分已经通过大规模预训练获得的能力，并且这种能力可以很容易地扩展到比训练期间看到的更长的上下文（例如，4K 到 128K）通过对适当的数据混合进行轻量级持续预训练。 我们调查了持续预训练的数据数量和质量：(1) 对于数量，我们表明 5 亿到 50 亿个令牌足以使模型能够在 128K 上下文中的任何位置检索信息； (2) 对于质量，我们的结果同样强调域平衡和长度上采样。具体来说，我们发现，对书籍等某些领域的较长数据进行上采样（现有工作的常见做法）会带来次优的性能，并且平衡的领域混合很重要。我们证明，在此类数据的 1B-5B 标记上对完整模型进行持续预训练是一种有效且经济实惠的策略，可将语言模型的上下文长度扩展到 128K。我们的方案优于强大的开源长上下文模型并缩小了与 GPT-4 128K 等前沿模型的差距。  https://preview.redd.it/bedg1gsgixjc1.jpg?width=1447&amp;format=pjpg&amp;auto=webp&amp;s=cdf15e90c375988b169fd24ffd5d45 05da002593  https://preview.redd.it/2qy3dhsgixjc1.jpg？ width=1837&amp;format=pjpg&amp;auto=webp&amp;s=2ced604b9e1360ee8d170773a1a0600523288516 https://preview.redd.it/pebawhsgixjc1.jpg?width=1446&amp;format=pjpg&amp;auto=webp&amp;s=4a57b8bb6685d6122d51a67e 4fa9645555c51d5a &lt; p&gt;https://preview.redd.it/o8v3kisgixjc1。 jpg?width=577&amp;format=pjpg&amp;auto=webp&amp;s=6d39b7736dc9221ed69e1c61ca36f303e8ef131e   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awagj3/d_data_engineering_for_scaling_language_models_to/</guid>
      <pubDate>Wed, 21 Feb 2024 12:05:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果一篇论文没有可用的开源代码，您是否可以为了娱乐/练习而实现该代码，并将其发布在您自己的 Github 上并附上适当的引用，并注明所有功劳均归作者所有？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1awaeo0/d_if_a_paper_has_no_open_source_code_available/</link>
      <description><![CDATA[大家好， 我在发表于 的一篇论文中找到了物理数值计算的 ML 实现的描述具有 CC4 许可证的 arXiv（除了 arXiv 之外，它还发表在期刊上，但我只查看 arXiv 版本）： 您可以自由地：共享 — 复制并重新分发以下内容用于任何目的的任何媒体或格式，甚至是商业目的。改编——为任何目的（甚至商业目的）重新混合、转换和构建材料。只要您遵守许可条款，许可方就不能撤销这些自由。根据以下条款： 归属 - 您必须给出适当的信用，提供许可证的链接，并注明是否进行了更改。您可以以任何合理的方式这样做，但不得以任何暗示许可方认可您或您的使用的方式。相同方式共享 — 如果您对材料进行重新混合、转换或构建，则必须在与原始材料相同的许可下分发您的贡献。无额外限制 — 您不得应用法律条款或技术措施来合法限制他人执行许可证允许的任何操作。  据我所知，该论文没有开源代码。 如果我尝试编写自己的实现，从学术行为角度来看是否有任何问题？论文中的 ML 内容并将其发布到我自己的 Github 上？当然，我会引用这篇论文，并说这个项目中的所有内容都是基于该论文。 我真的不想联系作者来要求诚实。我想知道无论是否联系作者都可以这样做。想象一下，每次您想做类似的事情时都必须这样做（例如，练习实现“注意力就是您所需要的一切”）。 非常感谢！   由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1awaeo0/d_if_a_paper_has_no_open_source_code_available/</guid>
      <pubDate>Wed, 21 Feb 2024 12:02:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] Kaggle 数据集与实际表格数据 - 痛苦的认识</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1avq8uz/d_kaggle_datasets_vs_actual_tabular_data_bitter/</link>
      <description><![CDATA[经过多年在 Kaggle 或其他平台上的表格数据集的工作和实践，我终于开始使用来自大学医院的表格数据，就像一滩污垢花了一整天的时间才找到正确的标题并链接所有这些表间公式和过滤器。另一方面，我花了最多。 Kaggle 数据集上的 EDA 需要 30 分钟。  我被告知了其中的差异，但意识到 DS 必须处理什么混乱。总是低估它，跳过与之相关的研讨会，还随意取笑它（我通常处理图像和视频）。   由   提交 /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1avq8uz/d_kaggle_datasets_vs_actual_tabular_data_bitter/</guid>
      <pubDate>Tue, 20 Feb 2024 19:23:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>