<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sun, 15 Sep 2024 03:21:42 GMT</lastBuildDate>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 15 Sep 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] 重现 o1 系列推理 - 寻找志愿者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh1uh8/p_reproducing_o1series_reasoning_looking_for/</link>
      <description><![CDATA[由于我之前的帖子被删除（我想是因为有人要求我在那里添加我的 discord 服务器邀请），并且得到了很多人的参与，所以我现在重新发布它： 我和我的团队目前正在尝试重现 o1 系列推理能力。但是，我们需要社区的一点帮助来获取更多数据。我们计划以 OpenAI 的两篇论文为基础开展研究：让我们一步一步验证 (https://arxiv.org/pdf/2305.20050) 和证明者-验证者游戏提高 LLM 输出的可读性 (https://arxiv.org/pdf/2407.13692)。我们可能还会在我们的方法中使用某种类型的树搜索。由于我们的团队规模很小，任何帮助都将非常有益，尤其是在获取数学、推理和代码思维链数据方面，并将所采取的步骤分类为“正确”、“中立”或“不正确”。如果您有兴趣帮助我们，请在这篇文章下发表评论或在 reddit 或 discord (danfosing) 上给我留言。 是的，我们所有的研究，包括模型、数据集、用于训练的代码都将开源。    提交人    /u/DanFosing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh1uh8/p_reproducing_o1series_reasoning_looking_for/</guid>
      <pubDate>Sun, 15 Sep 2024 02:01:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻找能为我撰写 ML/AI 日志的人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh1lg4/r_looking_for_someone_who_can_write_journals_in/</link>
      <description><![CDATA[需要有人用 ML 为我写日记。这是医学和 AI/ML 的结合。    提交人    /u/Intelligentboy52   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh1lg4/r_looking_for_someone_who_can_write_journals_in/</guid>
      <pubDate>Sun, 15 Sep 2024 01:47:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 情绪分析的最新进展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh1j9x/d_sentiment_analysis_state_of_the_art/</link>
      <description><![CDATA[现在我们拥有比以前的 NLP 方法强大得多的 LLM，情绪分析的当前 SOTA 是什么？在此任务中，仅编码器和编码器解码器模型如何与大量仅解码器的 LLM 抗衡？ 我也对返回比经典的正面/中性/负面答案更高维度结果的更高级方法感到好奇。    提交人    /u/RobbinDeBank   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh1j9x/d_sentiment_analysis_state_of_the_art/</guid>
      <pubDate>Sun, 15 Sep 2024 01:44:14 GMT</pubDate>
    </item>
    <item>
      <title>[D]机器学习系统设计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fh0pb7/d_machine_learning_system_design/</link>
      <description><![CDATA[我不喜欢读书，但最近开始读这本书，我只是想知道是否有其他人读过这本书并觉得它有用。你有没有推荐我接下来尝试的其他书？我想听听你的想法。谢谢！    提交人    /u/dcsr98   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fh0pb7/d_machine_learning_system_design/</guid>
      <pubDate>Sun, 15 Sep 2024 00:58:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 上周医疗 AI：顶级研究论文/模型🏅（2024 年 9 月 7 日至 9 月 14 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgyg90/d_last_week_in_medical_ai_top_research/</link>
      <description><![CDATA[   上周医学 AI：顶级研究论文/模型 🏅（2024 年 9 月 7 日至 9 月 14 日） 本周医学 AI 论文  Chai-1 基础模型分子结构预测  Chai-1 是一种最先进的用于药物发现中分子结构预测的多模态基础模型。它可以结合实验限制来提高性能，并在单序列模式下运行，而无需多序列比对 (MSA)。   医学法学硕士和基准  BrainWave：脑信号基础模型   本文介绍了 BrainWave，这是第一个用于侵入式和非侵入式神经记录的基础模型，已对来自大约 16,000 名个体的超过 40,000 小时的脑电记录（13.79 TB 数据）进行了预训练。  DS-ViT：用于阿尔茨海默氏症诊断的视觉转换器  本文提出了一种双流管道，用于阿尔茨海默氏症诊断中的分割和分类模型之间的跨任务知识共享。   EyeCLIP：眼科视觉语言模型  EyeCLIP 是一种用于多模态眼科图像分析的视觉语言基础模型，使用 277 万张眼科图像和部分文本数据开发而成。   用于肿瘤分割的 Segment Anything 模型  本研究评估了用于脑肿瘤分割的 Segment Anything 模型 (SAM)，发现它在框提示下的表现比点提示更好，并且在一定限度内随着点的增多而不断改进。   ....  医学 LLM 应用  KARGEN：放射学报告生成 LLM  DrugAgent：可解释的药物再利用剂  通过后续问题改进医学中的 RAG   框架和方法  自动细胞分割的基础设施  皮肤病学 AI 的数据对齐  自然语言诊断推理  医学的两阶段指令微调方法   医疗伦理中的 AI   将 LLM 用于医疗保健的顾虑和选择 了解推荐系统中的公平性  迈向更公平的健康建议   ..  详细查看完整帖子：https://x.com/OpenlifesciAI/status/1835085857826455825 感谢您的阅读！如果您知道任何遗漏的有趣论文，请随时在评论中分享。如果您对医疗 AI 有见解或突破，并希望在下周的版本中分享，请通过 Twt/x 与我们联系：OpenlifesciAI    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgyg90/d_last_week_in_medical_ai_top_research/</guid>
      <pubDate>Sat, 14 Sep 2024 23:02:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 受限解码是否是程序中的瓶颈？如果是，您可以分享详细信息吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgxqhu/d_is_constrained_decoding_a_bottleneck_in_your/</link>
      <description><![CDATA[我正在开发受限解码基准。基准已包含（并将包含更多）具有某些属性的模式（因此在受限解码实现中可能会遇到“坏情况”），但我还想用真实世界的模式对其进行补充。模式不必是您应用程序中最重要的瓶颈，只要提高它们的速度会导致可观察到的性能影响，我就对它们感兴趣。 如果您愿意分享，我希望了解模式以及您使用的受限解码库和推理引擎。最后，如果您可以提供一些示例数据，那就太好了。如果您想降低模式和/或数据的敏感度，只要模式的结构没有改变，那就没问题。您可以回复此帖子，也可以通过 reddit 直接向我发送消息。    提交人    /u/Huanghe_undefined   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgxqhu/d_is_constrained_decoding_a_bottleneck_in_your/</guid>
      <pubDate>Sat, 14 Sep 2024 22:28:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 数据提取的指标/置信度分数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgwxkz/p_metrics_confidence_score_for_data_extraction/</link>
      <description><![CDATA[大家好， 问题陈述：我们需要从 pdf 中提取一组特定的键值对。pdf 具有不同的布局和不同的语言。使用 LLM 通过编写复杂的提示来提取数据。为了达到这个复杂的提示，我不得不对提取进行多轮手动检查。目前，一旦数据被提取，我们依靠循环中的人工来确定提取是否正确。  问：是否有办法发布一个指标或某种置信度分数，可以帮助我们识别仅审查可能有不正确提取的特定发票集，以减少人工循环周转时间。 非常感谢您抽出时间。 谢谢     提交人    /u/Ooto__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgwxkz/p_metrics_confidence_score_for_data_extraction/</guid>
      <pubDate>Sat, 14 Sep 2024 21:51:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我做错了什么？CNN 问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgvws9/d_what_am_i_doing_wrong_cnn_question/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgvws9/d_what_am_i_doing_wrong_cnn_question/</guid>
      <pubDate>Sat, 14 Sep 2024 21:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 音频分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgto6y/d_audio_classification/</link>
      <description><![CDATA[大家好！ 我需要对机械声音的录音进行分类，以确定机械装置是否存在故障（例如敲击、研磨、咔嗒声）或者机械装置是否正常运行。我还有大约 100 个音频文件需要标记和测试。 哪种模型最适合执行此任务？是否有可以微调的预训练模型？或者您推荐哪种方法？ 我已经尝试过以下方法：我为每个录音创建了频谱图，并微调了 YOLOv8 模型以检测偏差，但这并没有达到预期的准确度，可能是因为数据集太小了。 提前谢谢您！    提交人    /u/ARLEK1NO   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgto6y/d_audio_classification/</guid>
      <pubDate>Sat, 14 Sep 2024 19:18:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] Diffumon - 一个简单的开源图像扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgoblp/p_diffumon_a_simple_open_source_image_diffusion/</link>
      <description><![CDATA[       由    /u/RogueStargun  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgoblp/p_diffumon_a_simple_open_source_image_diffusion/</guid>
      <pubDate>Sat, 14 Sep 2024 15:13:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Yolov5 有效损失问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgn7yo/d_yolov5_valid_loss_issue/</link>
      <description><![CDATA[      我正在开发安全带和手机检测系统，使用 YOLOv5s 检测挡风玻璃、驾驶员、乘客、安全带和手机。我的数据集存在类别不平衡问题，因为并非每个图像都包含安全带或手机，而手机类别的代表性尤其不足。 此外，手机很小，在图像中很难检测到。我注意到验证损失有一些波动，特别是在第 20 次之后开始增加，这让我怀疑存在过度拟合。 这是我的代码，我使用的是 Ultralytics 的预训练模型： model.train( data=&quot;full_dataset/data/data.yml&quot;, imgsz=640, epochs=100, batch=16, worker=4, project=&quot;SeatBeltMobileDetection&quot;, name=&quot;YOLOv5s_640_epochs100&quot;, device=0 ) 问题：  考虑到类别不平衡（特别是在手机检测方面），验证损失的波动和 DFL 损失的增加是否意味着过度拟合？ 微调的最佳实践是什么YOLOv5 在这种类别不平衡的情况下的表现如何？调整类别权重等技术有帮助吗（我之前做过过采样和增强）？ 我应该考虑对 YOLOv5 训练超参数进行哪些具体调整，以提高对手机等小物体的性能？     提交人    /u/Fantastic_Almond26   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgn7yo/d_yolov5_valid_loss_issue/</guid>
      <pubDate>Sat, 14 Sep 2024 14:23:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大多数联邦学习方法如此依赖超参数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgjzlt/d_why_are_most_federated_learning_methods_so/</link>
      <description><![CDATA[我从事 FL 研究已有一段时间了，并且涉猎了几个子领域。每当我开始一个新项目并对现有方法进行一些基准测试时，总是需要花很长时间才能让这些方法在原始论文中未使用的标准数据集（如 cifar10）上工作。目前，我正在使用一个预制的基准测试工具（fl-bench），并且仍然很难让 fedavg 在 cifar10 上收敛到甚至稍微非 i.i.d. 数据集。在我看来，这让在该领域工作变得非常令人沮丧。你有没有类似的经历，或者这段时间我一直错过了什么基本的东西？    提交人    /u/NumerousSwordfish653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgjzlt/d_why_are_most_federated_learning_methods_so/</guid>
      <pubDate>Sat, 14 Sep 2024 11:29:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 尝试复制“Stretch Each Dollar”扩散论文，遇到问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ffz5xc/p_attempting_to_replicate_the_stretching_each/</link>
      <description><![CDATA[      编辑：我发现了错误！ 我专注于确保掩蔽内容正确，它是，但是我没有看到，在我取消屏蔽补丁（即用 0 替换主干错过的补丁）之后，我将它们重塑为原始形状，在此期间我将它们传递通过 FFN 输出层，该层不是线性的，因此 0 输入！= 0 输出。但损失函数在那些地方预期 0 输出。所以我需要做的就是再次将这些位设为 0，现在它工作得更好了  我正在尝试复制这篇论文：https://arxiv.org/pdf/2407.15811 您可以在此处查看我的代码：https://github.com/SwayStar123/microdiffusion/blob/main/microdiffusion.ipynb 我首先对 9 张图像进行过度拟合以确保健全性，但在较低的掩蔽率下，我无法复制论文中的结果 在掩蔽率为 1.0 时，即所有补丁都被 Transformer 主干网看到，它对 9 张图像的过度拟合非常好 https://preview.redd.it/thteqn3rhlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=215fc88c74728f5f0dcfbd05a9b6a4db836b1f84 存在一些轻微的扭曲，但也许一些 LR 调度会有所帮助，主要问题是当掩蔽率降低到 0.75 时，输出严重下降： https://preview.redd.it/ukcexjbyhlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=432187000cde0ba5b1b90813c6284c9f764a9979 在掩蔽比为 0.5 时，情况甚至更糟： https://preview.redd.it/00kzbpc0ilod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=0717f8926582b2b5c694ebe5609b6f9fba8a088d 所有这些都经过相同步数的训练，除了掩蔽率之外，所有超参数都是相同的 注意：我使用“掩蔽率”表示 Transformer 主干看到的补丁百分比，与论文中隐藏的补丁百分比相反。我几乎可以肯定这不是问题 我也使用 x 预测目标而不是噪声预测，就像论文中一样，但这并不重要，而且它可以在 1.0 掩蔽率下工作。 增加补丁混合层的数量没有帮助，如果有的话，它会使情况变得更糟 2 个补丁混合层，0.5 掩蔽率： https://preview.redd.it/punkf59uilod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=04e0ea03d9ecd464f1bd007f7957c4a65c0ae9c2 4 个补丁混合层，0.5 遮罩比： https://preview.redd.it/9ihtiyvejlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=c78efb039b6ef630a3760dca60e2018d32e6c3b7 也许补丁混合器本身有问题？使用 TransformerEncoderLayer 作为补丁混合器是不是一个坏主意？    提交者    /u/SwayStar123   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ffz5xc/p_attempting_to_replicate_the_stretching_each/</guid>
      <pubDate>Fri, 13 Sep 2024 16:36:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>