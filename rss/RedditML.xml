<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sat, 13 Jul 2024 01:06:07 GMT</lastBuildDate>
    <item>
      <title>[D] 使用 LoRA 在 SFT 数据上微调 LLM 的最佳配置</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1wqkr/d_best_configuration_for_finetuning_llms_with/</link>
      <description><![CDATA[大家好，能否分享一下在资源有限（比如单个 A100 80GB）的情况下使用 LoRA 使用 SFT 数据微调 LLM（如 https://huggingface.co/Qwen/Qwen2-7B-Instruct）的最佳设置？以下是更多详细信息：  不想做任何量化，半精度训练是可以的。 想在更大的上下文中微调模型，比如 ~16K。 我曾尝试使用 CPU 卸载（使用 HF peft）的 deepspeed，但能够使用最大 1 个批次大小和最大上下文长度 16K 进行微调。  还有其他更好的配置或库吗？如果有人能够实现更好的批次大小或更快的配置，请分享详细信息。    提交人    /u/Financial-Beach1587   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1wqkr/d_best_configuration_for_finetuning_llms_with/</guid>
      <pubDate>Sat, 13 Jul 2024 00:09:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我一直在思考稳定扩散的工作原理，所以我决定从头开始编写自己的程序，并附上数学解释 🤖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1w2rg/p_i_was_struggle_how_stable_diffusion_works_so_i/</link>
      <description><![CDATA[        提交人    /u/jurassimo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1w2rg/p_i_was_struggle_how_stable_diffusion_works_so_i/</guid>
      <pubDate>Fri, 12 Jul 2024 23:38:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] ViT 等无掩码 Transformer 中自注意力层的一个有趣特性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1sy5u/d_an_interesting_property_of_selfattention_layers/</link>
      <description><![CDATA[我编写了一个自定义转换器实现，在对注意力代码进行初步的简单测试时，我注意到一个乍看之下似乎是 bug 的东西，但转念一想，这可能是注意力的固有属性。 由于注意力图针对每个嵌入向量进行了 softmax 处理，因此其应用将把每个向量拉向上下文/空间序列中其他向量的加权平均值。Oc 第一个注意力层可能只会将每个嵌入拉向几个特定的​​其他向量（具有最高注意力分数）。而且这种拉动不是直接针对其他向量，而是针对它们与值矩阵的变换（也受到残差连接的影响），但由于值矩阵共享所有上下文/空间槽的权重，因此效果仍然存在：在注意层之后，嵌入向量比之前稍微更相似。 这是一个正反馈循环，因为现在稍微更相似的嵌入将在下一个注意层中产生更均匀的注意图，以实现更均匀的平均，依此类推，以获得越来越强的均质化效果。 还有其他因素可能会抵消这种影响（例如间歇性的其他层和非线性，输出层如何连接到嵌入向量以及其梯度如何回流等等 - 训练量对注意力图均匀性也很重要）。对于视觉变换器来说，这甚至可能是一种理想的效果（因为向量/补丁会逐步吸收来自整个图像的类信息）。但它仍然似乎是一种独特的注意力属性，并且它作为一种通用的空间/上下文连接运算符的用途，与完全连接层或卷积层有很大不同。 我认为这种自我放大效应即使在完整模型中也应该是显而易见的（除了注意力之外，还使用各种其他块），只要模型使用多个没有因果掩码的注意力块（例如，对于语言模型来说不太明显）。有人经历过这种现象吗？    提交人    /u/lostn4d   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1sy5u/d_an_interesting_property_of_selfattention_layers/</guid>
      <pubDate>Fri, 12 Jul 2024 21:21:49 GMT</pubDate>
    </item>
    <item>
      <title>[N] Google 研究 - 将视觉语言模型与墨迹模态相结合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1icu4/n_google_research_combining_vision_language_model/</link>
      <description><![CDATA[ 使用 VLM 进行在线手写识别：了解大型视觉语言模型 (VLM) 如何使用新表示和标记器彻底改变在线手写识别。此方法适用于现成的模型，并无缝集成到现有的多模式框架中。 使用 VLM 数字化手写笔记：了解 VLM 如何将纸笔笔记的图像转换为数字墨水，从而连接传统笔记和数字笔记。这种集成促进了 AI 辅助工作流程并提高了生产率。  https://youtu.be/JspO32WBluI?si=UA9ueBrQJPgoanwm&amp;t=1517    提交人    /u/CharlieLee666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1icu4/n_google_research_combining_vision_language_model/</guid>
      <pubDate>Fri, 12 Jul 2024 13:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于部署模型的成熟 tinyml 框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1fx1e/d_mature_tinyml_frameworks_for_deploying_models/</link>
      <description><![CDATA[我一直在研究基于决策树的小型分类器，用于需要在嵌入式系统上进行实时符号识别的应用程序。由于有大量的机器学习库可用，大多数分析都是用 Python 完成的，但现在我需要部署模型，因此我需要将模型转换为 c（或 c++），而 c（或 c++）似乎没有得到广泛支持。为此，我使用了 tl2cgen，但我的模型受到 treelite 错误的影响，导致部署的模型所做的预测非常不准确（这非常不幸，因为模型足够快，可以满足应用程序要求）。因此，我正在寻找一个面向 tinyml 的 ml 框架。 经过一些研究，我正在考虑从 xgboost 更改为 catboost，因为它似乎相当成熟，并为输出 c++ 代码的模型提供了一个 aot 编译器。但是，在此之前，我想问一下您对该库的使用经验，因为迁移模型需要一些时间。如果您以前使用过它，您对该库的看法是什么？您知道其他面向 tinyml 的成熟框架吗？ （生成的分类器不会用于商业产品，因此任何许可证对于该项目都可以）    提交人    /u/No_Mongoose6172   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1fx1e/d_mature_tinyml_frameworks_for_deploying_models/</guid>
      <pubDate>Fri, 12 Jul 2024 12:01:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 计算机视觉管道可以用 ONNX 表示吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1cgqe/r_can_a_computer_vision_pipeline_be_represented/</link>
      <description><![CDATA[使用 PyTorch 和 torchvision 描述的管道，类似于： 对比度增强（直方图操作）——&gt;边缘增强（sobel 等）——&gt;神经网络（简单分类 CNN） 我有兴趣使用 TVM 将这样的计算图部署到硬件上。 编辑：哇哦，它有效（我认为）如果我遗漏了什么，请回复。我将上传 google collab 笔记本和测试图像的 git 链接。 编辑 2：我使用了直方图均衡化——&gt;预训练的 imagenetv1 ResNet-18 分类器。在 python 中通过 ONNX 运行时执行 ONNX 图时按预期工作。在 Netron 上查看 .onnx 文件。    提交人    /u/GWP-NU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1cgqe/r_can_a_computer_vision_pipeline_be_represented/</guid>
      <pubDate>Fri, 12 Jul 2024 08:24:44 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 探索类别不平衡和 SMOTE 对分类性能的影响。需要建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e1be93/research_exploring_the_impact_of_class_imbalance/</link>
      <description><![CDATA[大家好， 我正在做一个项目，我从一个平衡的分类数据集开始。我故意从其中一个类别中删除 10%、20% 甚至 90% 的数据点，从而引入类别不平衡。之后，我将 SMOTE 应用于这些不平衡的数据集。然后，我使用几种机器学习模型评估了数据集，包括 SVC、随机森林分类器、决策树分类器和 XGBoost。 我的主要目标是分析类别不平衡和 SMOTE 应用对数据集的影响。我想比较应用 SMOTE 之前和之后的数据集，并了解发生的统计变化。 ChatGPT 建议使用配对 t 检验和 ANOVA 进行此分析。您能推荐一些对此类分析有用的文献、其他统计测试或性能指标吗？如有任何见解或建议，我们将不胜感激！ 提前致谢！    提交人    /u/FantasticHero_007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e1be93/research_exploring_the_impact_of_class_imbalance/</guid>
      <pubDate>Fri, 12 Jul 2024 07:12:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于序列建模基准测试的小型但具有挑战性的数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e19w5x/d_small_but_challenging_data_sets_for/</link>
      <description><![CDATA[您推荐使用哪些数据集对预算受限的序列模型进行基准测试？数据集应该很小，并且对于那些没有大量计算资源的人来说是可以访问的。例如，Long Range Arena。    提交人    /u/chernivek   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e19w5x/d_small_but_challenging_data_sets_for/</guid>
      <pubDate>Fri, 12 Jul 2024 05:37:25 GMT</pubDate>
    </item>
    <item>
      <title>色彩联想探究 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e15sp6/color_associations_inquiry_r/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e15sp6/color_associations_inquiry_r/</guid>
      <pubDate>Fri, 12 Jul 2024 01:53:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 生成输出的质量保证</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e121wd/d_quality_assurance_of_generated_output/</link>
      <description><![CDATA[我正在使用聊天 gpt 总结来自书籍来源的文本，并希望确保生成的内容与作者意图相关且符合书籍基调。 我正在考虑在生成的文本和实际书籍之间进行某种相似性测试，并检查相似度得分是否高。但我不确定应该使用哪种确切方法。    提交人    /u/acr_d_rkstr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e121wd/d_quality_assurance_of_generated_output/</guid>
      <pubDate>Thu, 11 Jul 2024 22:48:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] Memory^3：使用显式记忆进行语言建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e0y7do/r_memory3_language_modeling_with_explicit_memory/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e0y7do/r_memory3_language_modeling_with_explicit_memory/</guid>
      <pubDate>Thu, 11 Jul 2024 20:02:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从无标记数据到丰富的细分：自监督模型的魔力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e0u9sx/p_from_unlabeled_data_to_rich_segmentation_the/</link>
      <description><![CDATA[我一直在尝试微调 Facebook Research 的 DINOv2 ViT 权重，用于图像分割。这些 DINOv2 编码器权重是通过自监督学习预先训练的，可以使用低秩自适应 (LoRA) 和简单解码器（如 1x1 卷积解码器或特征金字塔网络 (FPN)）轻松微调。我获得了可靠的验证 IoU 分数：经过 30-50 个时期的微调，ADE20k 上约为 62%，Pascal VOC 上约为 85%。 我还创建了一个 Jupyter Notebook，详细描述了这些 DINOv2 模型如何实现其语义丰富性。 Github：https://github.com/RobvanGastel/dinov2-finetune?tab=readme-ov-file Colab：https://colab.research.google.com/github/RobvanGastel/dinov2-finetune/blob/main/Explanation.ipynb    由   提交  /u/Quiet_Grab1112   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e0u9sx/p_from_unlabeled_data_to_rich_segmentation_the/</guid>
      <pubDate>Thu, 11 Jul 2024 17:17:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有人为他们的 AI 应用程序设置实时 Django Workers 吗？可扩展的最佳方法是什么？🙄 Celery + Channels + Redis + Docker</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e0qens/d_is_anyone_else_setting_up_realtime_django/</link>
      <description><![CDATA[      老实说，我们完全低估了这一点，以为它会更直接。但我们现在已经完成了，并在本系列文章中一步一步记录了如何操作。。 一点背景，我们正在构建一个迷你免费 AI 代理，它可以自动生成可手动定制的图，因此用户基本上可以按照自己的意愿设置样式。它需要具有成本效益和效率，因此我们思考了如何做到这一点并测试了其他几种方法。 https://preview.redd.it/cmly0a6bhwbd1.png?width=640&amp;format=png&amp;auto=webp&amp;s=be1f5b2853e744adcaf8013e6d43b43f6be89617 我们计划将项目开源，因此欢迎所有反馈！还有其他人这样做并有任何反馈吗？或者知道更好的方法吗？    提交人    /u/stoicwolfie   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e0qens/d_is_anyone_else_setting_up_realtime_django/</guid>
      <pubDate>Thu, 11 Jul 2024 14:33:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 科学机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e0kvxw/d_scientific_machine_learning/</link>
      <description><![CDATA[      传统的 ML 课程及其中涉及的项目并不适合工程师。  作为一名机械工程师或物理学家，我为什么要做一个关于电影评论分析器或房价预测的项目？ 我很想做一个能教我如何使用机器学习来模拟流体力学或黑洞动力学的项目。 我想要一个将机器学习与我的领域知识相结合的领域。 科学机器学习就是这样的领域。 我觉得科学机器学习是过去 4-5 年最酷的技术之一。 科学机器学习有 3 个主要支柱： (1) 神经微分方程 (2) 物理信息神经网络 (PINN) (3) 通用微分方程 它帮助我从机械工程过渡到机器学习，并在麻省理工学院获得机器学习博士学位。 对科学机器学习或PINN 还是神经 ODE？ https://i.redd.it/96mcn7oj0vbd1.gif    提交人    /u/OtherRaisin3426   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e0kvxw/d_scientific_machine_learning/</guid>
      <pubDate>Thu, 11 Jul 2024 09:37:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 07 Jul 2024 02:15:10 GMT</pubDate>
    </item>
    </channel>
</rss>