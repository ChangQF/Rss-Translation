<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 25 Feb 2024 06:15:31 GMT</lastBuildDate>
    <item>
      <title>[D] 我的模型是否过度拟合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azgeim/d_is_my_model_over_fitting/</link>
      <description><![CDATA[      我已经在对抗性和干净图像样本的自定义数据集上训练了 vit 模型，并且我得到以下输出，这是我的模型过度拟合   由   提交/u/GraphHopper77  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azgeim/d_is_my_model_over_fitting/</guid>
      <pubDate>Sun, 25 Feb 2024 05:44:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 谁能帮我解决这个问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azgaxp/d_can_anyone_help_me_with_this_issue/</link>
      <description><![CDATA[我正在寻找有关使用 Whisper-large-v3 模型（GPU、RAM、核心...）的系统要求的文档。 任何人都可以帮我解决这个问题吗？ 非常感谢   由   提交 /u/Visible-Employment43    reddit.com/r/MachineLearning/comments/1azgaxp/d_can_anyone_help_me_with_this_issue/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azgaxp/d_can_anyone_help_me_with_this_issue/</guid>
      <pubDate>Sun, 25 Feb 2024 05:39:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 除了检索增强生成（RAG）之外，还有哪些使用法学硕士构建的其他范式和框架？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</link>
      <description><![CDATA[自 InstructGPT 及其大众市场应用程序 ChatGPT 上市以来，已经一年多了，并引发了我们今天看到的围绕法学硕士的兴趣风暴。  除了研究和学术兴趣之外，行业（初创企业、大型企业等）也出于商业原因尝试构建由法学硕士支持的新产品和/或功能。 然而，到目前为止，我所看到的大部分内容要么是围绕 LLM 的薄包装应用程序，要么是 RAG 的某些变体。  除了检索增强生成（RAG）之外，还有哪些使用法学硕士进行构建的其他范式和框架？   由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</guid>
      <pubDate>Sun, 25 Feb 2024 04:27:51 GMT</pubDate>
    </item>
    <item>
      <title>[P] 脑肿瘤的分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azekf8/p_classification_of_brain_tumor/</link>
      <description><![CDATA[我和我的同事目前正在进行旨在提高脑肿瘤分类准确性的研究。尽管我们付出了努力，但在实现所需的准确率方面仍遇到了挑战，目前的准确率仅为 78%。由于我们的目标是在研究工作中坚持严格的标准，因此我们正在寻求与该领域的专家合作来解决这个问题，并可能发表我们的研究结果。 到目前为止，我们的方法涉及采用各种预处理技术和功能机器学习框架内的提取方法。虽然我们也计划探索深度学习方法，但我们的最终目标是开发一个强大的模型，不仅可以在我们当前的数据集上实现高精度，而且可以在其他相关数据集上实现高精度。 此外，我们还面临着分割和颅骨剥离过程中的困难，这是我们研究流程中的关键步骤。我们认识到克服这些挑战以确保我们结果的可靠性和有效性的重要性。 除了提高分类模型的准确性之外，我们还有兴趣与能够带来新颖方法或技术的个人合作。对我们研究的见解。我们相信，通过将我们的专业知识与该领域其他人的专业知识相结合，我们可以增进我们的理解，并为脑肿瘤分类的知识体系做出有意义的贡献。 我们的最终目标是发表一篇论文，详细介绍我们的研究成果。研究结果，我们愿意与与我们一样对该研究领域充满热情的合作者合作。如果您在脑肿瘤分类、分割或相关领域有经验，并且有兴趣与我们合作，我们将很高兴进一步讨论潜在的机会。 感谢您考虑这次合作机会。我们期待着能够共同努力实现我们的研究目标，并为这一重要领域的知识进步做出贡献。   由   提交/u/PaleontographNo7331  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azekf8/p_classification_of_brain_tumor/</guid>
      <pubDate>Sun, 25 Feb 2024 04:03:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 根据一个unet调整另一个unet的子集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azedr0/d_adjusting_a_subset_of_one_unet_based_on_another/</link>
      <description><![CDATA[假设我有 2 个 unets：1 个有 50,000 个值，1 个有 18,000 个值。 它们具有相同的维数。&lt; br /&gt; 较小的集合是较大集合的严格子集，从某种意义上说，我可以告诉您 Smallset[x] 与 Largeset[y] 代表相同的概念 现在问题来了。&lt; /p&gt; 虽然我知道 Smallset 中的哪些索引与 Largeset 中的索引相对应......但实际的 unet 权重是独立训练的。因此，Smallset[x] 的权重与 Largeset[y] 的权重完全无关 但是较小的集合训练得更好。 所以，理想情况下，我想围绕较小集合中的训练重塑较大集合中的其他 30,000 个值。 任何人都可以推荐现有的标准方法来执行此操作吗？ 理想情况下，使用也不会花费我 100,000 美元的计算能力的方法吗？ :-)  ​   由   提交/u/lostinspaz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azedr0/d_adjusting_a_subset_of_one_unet_based_on_another/</guid>
      <pubDate>Sun, 25 Feb 2024 03:53:39 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] Google 推出 Gemma - 了解这个开放 AI 模型并开始使用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azdvdt/news_google_launches_gemma_know_about_this_open/</link>
      <description><![CDATA[    /u/Anirban_Hazra   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azdvdt/news_google_launches_gemma_know_about_this_open/</guid>
      <pubDate>Sun, 25 Feb 2024 03:26:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 拥抱面部加速与闪电织物</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azck48/d_hugging_face_accelerate_versus_lightning_fabric/</link>
      <description><![CDATA[TL; DR：正如标题所述，我的问题非常简单：使用过这两个库的人，两者的优缺点是什么，您会推荐哪个？ ​ &lt;我不想像 PyTorch Lightning 那样引入抽象，并且希望尽可能多地控制训练循环。这主要是因为我不想重构我的代码以最适合闪电的最佳实践。但是，我仍然想使用多 GPU、多节点和混合精度训练，这两个似乎是最明显的候选者。 ​  拥抱面部加速和Lightning Fabric 从“从 PyTorch 转换”的角度看，两者看起来很相似。指南：  初始化设备对象。 通过库的  包装模型、优化器和数据加载器.setup()/.prepare() 功能。 删除 .to(device) 调用。 &lt; li&gt;更新 .backward() 调用以通过对象反向传播损失。  ​ 所以我的问题是：其中一个比另一个有优势吗？我是否遗漏了任何明显的问题。我没有详细查看他们的文档，所以如果有人使用过这些库，如果您能分享任何经验/建议，那就太好了。谢谢。   由   提交 /u/DaredevilMeetsL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azck48/d_hugging_face_accelerate_versus_lightning_fabric/</guid>
      <pubDate>Sun, 25 Feb 2024 02:19:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] Layernorm 只是两个投影，可以改进</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1az1bto/d_layernorm_is_just_two_projections_and_can_be/</link>
      <description><![CDATA[我正在考虑如何可视化向量的层范数，并发现它只是在学习参数为空时应用两个投影（如果不是） ，然后他们只需添加重新缩放和平移）。 通过删除平均值，您可以投影到超平面上，其中分量的平均值（或总和）为空（例如，在 3D x + y + z = 0 中）然后除以标准差，投影到半径 sqrt(D) 的球体上。 鉴于 Layernorm 的理论目标是使数据按照 D 维度的标准高斯重新缩放，则投影到超平面实际上失去了标准高斯通常使用的一维。 在高维 D 中，由于大数定律应用于 D 维的范数，人们可以用 D 维中的超球面来近似标准高斯分布由此生成的向量（对于一维标准高斯，x_i2 的平均值为 1），因此通过获取点并将它们射入超球体来近似这一点在理论上是有动机的，但减少一维会损失表示能力。 上述主张可以通过注意到以下事实来证明：对于向量 x = (x_1, ..., x_D)，x - (x_1 + ... + x_D)/D = x - 1 mu 与超平面上的正交投影相同，其中 mu = 0（1 是所有 1 的向量，因为我们从每个分量中删除了平均值）。  证明：如果取线性函数 M : RD -&gt; R，M(x)＝mu，则M(x)＝&lt;n，x&gt;/&lt;n，n&gt;对于 n = 1，1 的向量。但是 x - M(x) 1 = x - / n 是正交投影和去除所有分量均值的公式。 然后，对于 x&#39; = x - M(x)，layernorm 变为 x&#39; / sqrt( 1/Dnorm(x&#39;)2 ) = sqrt(D) x&#39; /norm(x&#39;)，半径为 sqrt(D) 的超球面投影。 学习到的参数移动这个超球面并单独缩放组件，在初始化时它们不执行任何操作。 您可以想象，在 3D 中，这对应于在 x + y 平面内绘制半径 sqrt(3) 的圆+ z = 0。在未学习的层范数之后，所有点都会在那里结束，请注意这是一个圆，而不是一个完整的球体。 我认为，考虑到层范数最终会被线性层进一步转换为一个网络中，如果需要，这些层可以进行投影，但像这样，它们没有选择不进行投影。 tl;dr：Layernorm 投影到超平面上，然后投影到半径球体上开方（D）。我声称超平面上的投影是浪费的，并且减少了一个自由度，而这个自由度可以由网络中进一步的线性层来处理。   由   提交 /u/mgostIH   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1az1bto/d_layernorm_is_just_two_projections_and_can_be/</guid>
      <pubDate>Sat, 24 Feb 2024 18:19:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2D Matryoshka 句子嵌入 🪆🪆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1az19nl/r_2d_matryoshka_sentence_embeddings/</link>
      <description><![CDATA[    &lt; /a&gt;  论文：https://arxiv.org/pdf/2402.14776.pdf 📢介绍2D Matryoshka句子嵌入 (2DMSE)  TL;DR OpenAI 使用 Matryoshka 表示学习 (MRL 🪆) 来缩短嵌入向量，为下游任务提供灵活性和效率。但它仍然有一个昂贵的推理阶段。  论文提出 2DMSE 将 MRL 扩展到所有层，显着提高浅层 Transformer 层的嵌入质量。通过 2DMSE，BERT-base 实现了平均。 Spearman 在仅使用一层层的 STS 基准上的相关性得分为 70.09（使用最后一层时为 82.65）。  2DMSE 使嵌入尺寸和模型深度均可配置，提供了极大的灵活性和可扩展性。值得注意的是，它可以将微调后的 BERT-base 模型（12 层，768 维）缩小到与 BERT-small（4 层，512 维）和 BERT-tiny（2 层，128 维）相似的大小，并超越它们！ 📄 https://arxiv.org/abs/2402.14776 𝕏 https://x.com/_zongxi/status/1761074342715371643?s=20  &lt; p&gt;𝕏 https://x.com/_reachsumit/status/1760902388842729672?s=20  𝕏 https://x.com/xmlee97/status/1760879476760834460?s=20   由   提交/u/Prof_Li   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1az19nl/r_2d_matryoshka_sentence_embeddings/</guid>
      <pubDate>Sat, 24 Feb 2024 18:16:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] [D] 自托管开源模型或使用 chatgpt 等 API</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayzxt5/p_d_self_hosting_an_open_source_model_or_using_an/</link>
      <description><![CDATA[开发小型 LLM 应用程序的更好方法是什么， 用例是：提供一些数据（可能包含 PII）并使用LLM从文档中提取某些信息 问题是，如果我使用OpenAI等第三方API，那么我将提供我事先没有任何方式屏蔽的个人信息 另一种选择是在服务器上运行 llm 模型，但模型大小相当大，成本会更高（不确定，但这是我的观点，考虑到它需要体面的资源） 我运行ollama 在我的 8gb mac m2 上使用新的 gemma 模型，响应简单的查询非常慢 那么这两种方法更好的方法是什么，或者是否有其他方法，以及流行的方法现在的市场？   由   提交/u/sjdevelop  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayzxt5/p_d_self_hosting_an_open_source_model_or_using_an/</guid>
      <pubDate>Sat, 24 Feb 2024 17:22:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用法学硕士进行文本分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayx6xf/p_text_classification_using_llms/</link>
      <description><![CDATA[您好，我正在寻找一种解决方案，对分布在 7000 多个标记数据实例中的 10-20 个不同类别进行监督文本分类。我有 xlsx 和 jsonl 格式的数据，但可以轻松转换为所需的任何格式。我也尝试过基本的机器学习技术和深度学习，但我认为由于变压器架构，法学硕士会提供更高的准确性。我正在研究 Gemini 提供的函数调用功能，但它有点复杂。是否有任何好的框架和易于理解的示例可以帮助我对任何法学硕士进行零镜头、少量镜头和微调培训？ Colab 会议将不胜感激。如果需要，我也可以访问 Colab pro。不是任何其他付费服务，但最多可花费 5 美元 (USD)。这是一个个人研究项目，因此预算相当紧张。如果您能指导我找到任何对这项任务有用的资源，我将非常感激。任何 LLM 都可以。 我还研究了通过 ollama 使用自定义 LLM，并且能够在 Colab 实例上设置 Mistra 13b 的 6 位量化版本，但还无法使用它进行分类。另外，我认为 Gemini 是我的最佳选择，因为可用的 VRAM 数量有限。即使我可以在 Colab 上临时加载高端模型，我也需要很长时间，经过大量的试验和错误才能使代码正常工作，即使在那之后，也需要很长时间来预测类。也许我们可以使用数据集的子集来实现此目的，但这仍然需要很长时间，而且 Colab 有 12 小时的限制   由   提交/u/Shubham_Garg123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayx6xf/p_text_classification_using_llms/</guid>
      <pubDate>Sat, 24 Feb 2024 15:29:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] LoRA+：大型模型的高效低阶自适应</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayrfpt/r_lora_efficient_low_rank_adaptation_of_large/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.12354 代码：https ://github.com/nikhil-ghosh-berkeley/loraplus 摘要：  在本文中，我们展示了Hu 等人最初引入的低秩适应 (LoRA)。 （2021）导致大宽度（嵌入维度）模型的微调不理想。这是因为 LoRA 中的适配器矩阵 A 和 B 以相同的学习率更新。使用大宽度网络的缩放参数，我们证明对 A 和 B 使用相同的学习率并不能实现有效的特征学习。然后我们证明，只需通过为 LoRA 适配器矩阵 A 和 B 设置不同的学习率以及精心选择的比率，即可纠正 LoRA 的这种次优性。我们将此提议的算法称为LoRA+。在我们广泛的实验中，LoRA+ 提高了性能（1-2% 的改进）和微调速度（高达 ∼ 2 倍加速），而计算成本与 LoRA 相同。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayrfpt/r_lora_efficient_low_rank_adaptation_of_large/</guid>
      <pubDate>Sat, 24 Feb 2024 10:28:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 与变形金刚相比，曼巴的根本缺点是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</link>
      <description><![CDATA[你好！ 我思考这个问题有一段时间了。澄清一下，我并不是指“它尚未经过广泛测试”之类的方面。 “它的可扩展性是不确定的，”或“缺乏工业基础设施。”相反，我有兴趣了解 Transformer 和 Mamba 架构之间的核心差异，特别是这些差异如何使 Mamba 与 Transformers 相比处于劣势。 致以诚挚的问候！ &lt;强&gt;编辑： 据我从您的回答中了解到，变形金刚“更好”与 Mamba 相比，在以下意义上：  Transformers 不会压缩输入。 Transformers 可以处理非顺序数据。 Transformer处理位于输入末尾的指令可能会更好。  编辑 2： 总结一下：&lt; /p&gt;  变形金刚：在更大的环境中进行更多计算，但可以访问更多信息，尽管可能是一些无用的信息 Mamba： Less计算更大的上下文，但访问的信息较少，因此存在丢失信息的风险。    由   提交 /u/Alarmed-Profile5736   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</guid>
      <pubDate>Sat, 24 Feb 2024 07:11:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 编写 ML 软件时 - 如何使用 TDD？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</link>
      <description><![CDATA[请告诉我是否有更好的子程序。 测试驱动开发。 &lt; p&gt;我已经在 ML 软件上工作了一段时间，我觉得我突然想要更好地遵循 TDD 并尝试将其应用到更细致的 ML 用例中。 有一件事多年来我注意到需求和设计对于我们的工作来说可能是模糊的——我所做的很多事情至少从最简单的设计开始，然后我们依靠迭代和强大的评估框架来证明某些改进是否合理。已实施（如果不能提高性能，为什么要实施任何东西）。在这些类型的原型设计场景中，TDD 可能会成为一个巨大的时间杀手，并且在您确定设计之前有点无用。 不过，当需求明确时，TDD 非常好，所以我正在努力变得更好将其纳入我的武器库中。 您对 TDD 有何看法以及如何/何时使用它？   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayiq45/d_when_writing_ml_software_how_do_you_use_tdd/</guid>
      <pubDate>Sat, 24 Feb 2024 02:05:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>