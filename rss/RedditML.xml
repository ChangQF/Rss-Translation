<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 02 Dec 2023 15:12:23 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么交叉熵随着准确度的增加而增加？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1895m5k/d_why_is_crossentropy_increasing_with_accuracy/</link>
      <description><![CDATA[      我正在实现 softmax 回归，并且我正在努力理解交叉熵值增加问题背后的本质[1]，以及准确性的增加（在“iris”数据集上）： https://preview.redd.it/eo3654zwbw3c1.png?width=688&amp;format = png&amp;auto=webp&amp;s=52916d56029e6e8aa1e9594bbcb02c7075009206 这对我来说非常令人困惑，因为没有类不平衡： https://preview.redd.it/rqs7i6jybw3c1.png?width=389&amp;format=png&amp;自动= webp&amp;s=6ca668f1cc70aae7a125a6b9c48c511eadc6d562 我不完全确定 N = 112 的样本大小是否有问题。我将不胜感激任何有关此事的帮助。提前谢谢您。 ​ [1]   由   提交 /u/joshjson   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1895m5k/d_why_is_crossentropy_increasing_with_accuracy/</guid>
      <pubDate>Sat, 02 Dec 2023 15:05:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformers 如何重写机器学习的古老传统规则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1895ipi/d_how_transformers_rewrote_the_rules_of_an_age/</link>
      <description><![CDATA[      大家好！分享我的 ML YT 频道的最新视频，讨论 Transformer、它们的工作原理，以及与其他神经网络（如 CNN、RNN 等）相比，它与归纳偏差的有趣关系。分享一个链接给有兴趣的人看看！谢谢。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1895ipi/d_how_transformers_rewrote_the_rules_of_an_age/</guid>
      <pubDate>Sat, 02 Dec 2023 15:01:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 惨痛的教训和思想之树 - 像 ToT 这样的技术是使用搜索的例子，还是它们通过编码类人学习而忽略了惨痛的教训？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1893ne2/d_bitter_lesson_and_tree_of_thoughts_are/</link>
      <description><![CDATA[惨痛的教训表明，学习和搜索是获胜策略，因为它们随着计算能力的扩展而扩展，并且通常会优于依赖于编码人类知识的技术。&lt; /p&gt; 鉴于此，您对 ToT 和类似技术有何看法？它们是通过搜索扩展模型能力的好例子，还是试图强制我们认为是类人行为的例子？ ​ 仅供参考，我在研究中看到了两种主要的基于树的方法。一种是基于MCTS的解码。这似乎更符合传统的搜索概念，因为您正在搜索可能文本的结果空间，然后选择最好的文本。 meta 使用 PPO 值函数作为 MCTS 节点评估器的论文 使用MCTS来提高编写成功程序的能力 ​ 但是，还使用了更抽象的CoT/ToT风格的树，这依赖模型为给定序列生成后续序列树。 思想树：故意问题使用大型语言模型求解 这里的一个核心区别是树并不像 MCTS 解码树搜索那样表示对可能输出的搜索。这是对可能的推理链的搜索，这些推理链最终是某种评估方法（通常是模型本身）的输入，以确定最佳输出。因此，您不仅搜索结果空间，还搜索“证据空间”，这两者都将传递给评估器以选择适当的结果。  编辑：不相关，但原则上您可以结合这两种搜索技术，这会很酷，但可能非常昂贵。例如，对 ToT 中的每个节点使用 MCTS 解码。我还没有看到有人这样做，但如果有人有一篇论文的链接那就太酷了。   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1893ne2/d_bitter_lesson_and_tree_of_thoughts_are/</guid>
      <pubDate>Sat, 02 Dec 2023 13:23:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 让为您的 ML 模型创建 FastAPI 后端变得非常容易！想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18933bh/p_made_it_really_easy_to_create_a_fastapi_backend/</link>
      <description><![CDATA[嘿伙计们！我想分享我最近制作的这个工具，https://visual-backend.com，它可以让你为你的 ML 构建 FastAPI 后端模型真的很快。它本质上是一个 GUI，可让您生成代码和代码。端点处理程序的脚手架、身份验证，甚至只需一键即可部署到 GCP。因此，要为您的 ML 模型提供服务，您所需要做的就是加载它并在每个端点处理程序处调用推理函数。当然，对于批处理或作业队列之类的东西，没有这样的功能，但只是想知道在基础级别上，这样的工具是否对你们有用！ I最初是为全栈开发人员构建的，但在与几位 ML 工程师/数据科学家交谈后，我意识到这可能对那些希望快速将 ML 模型投入生产而不用太关心的人有所帮助。基础设施/软件工程，所以我很想听听您是否觉得这有帮助以及您可能有的其他想法。期待它:)   由   提交 /u/johnyeocx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18933bh/p_made_it_really_easy_to_create_a_fastapi_backend/</guid>
      <pubDate>Sat, 02 Dec 2023 12:51:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] 利用 AI 提供个性化新闻：根据您的兴趣生成您自己的黑客新闻源，并通过 RSS 或电子邮件订阅。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1892oxl/p_leveraging_ai_for_personalized_news_generate/</link>
      <description><![CDATA[   /u/madredditscientist  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1892oxl/p_leveraging_ai_for_personalized_news_generate/</guid>
      <pubDate>Sat, 02 Dec 2023 12:26:44 GMT</pubDate>
    </item>
    <item>
      <title>[D]如何追踪最近​​的热门论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1890zpi/dhow_do_i_track_recent_trending_papers/</link>
      <description><![CDATA[由于papers.labml.ai离线，如何处理你们跟踪最近的热门论文或热门话题吗，尤其是 X 方面的。有什么推荐吗？   由   提交 /u/Historical-Tree9132    reddit.com/r/MachineLearning/comments/1890zpi/dhow_do_i_track_recent_trending_papers/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1890zpi/dhow_do_i_track_recent_trending_papers/</guid>
      <pubDate>Sat, 02 Dec 2023 10:32:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我正在尝试改善我的预测值与实际值的分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188warj/p_im_trying_to_improve_my_predicted_vs_actual/</link>
      <description><![CDATA[    &lt; /a&gt;  我使用 Autogluon 在表格数据上训练模型 (WeightedEnsemble_L3)。这是一个回归问题。以下是评估结果： {&#39;root_mean_squared_error&#39;: -9.592466103848274, &#39;mean_squared_error&#39;: -92.0154059534781, &#39;mean_absolute_error&#39;: -7.8083721751898105, &#39;r2&#39;: 0.78427137067 0073、&#39;皮尔森&#39;：0.8990940522052712、&#39;中值绝对误差&#39; ：-6.87762451171875}  下图显示了预测值与实际值。虚线是y=x。从散点分布来看，云分布似乎可以逆时针旋转，然后预测值将更接近实际值。这种旋转将提高回归预测的准确性。我这样看对吗？有没有办法在 autogluon 中做到这一点？我不得不添加一个训练后软糖因素，因为我确信有更好的方法。  ​ https://preview.redd.it/rlrwxwopet3c1.png?width=571&amp;format=png&amp;auto=webp&amp;s=3abfdf6cedad258b4b115c8071b62d3d30b52d0f    由   提交 /u/BAMred   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188warj/p_im_trying_to_improve_my_predicted_vs_actual/</guid>
      <pubDate>Sat, 02 Dec 2023 05:10:54 GMT</pubDate>
    </item>
    <item>
      <title>[D]深入研究 Google Brain 团队的 Vision Transformer (ViT) 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</link>
      <description><![CDATA[我们每周五都有一个名为 Arxiv Dives 的阅读俱乐部，在那里我们回顾当今机器学习中使用的许多最先进技术的基础知识。上周我们深入探讨了“视觉变形金刚” 2021 年的论文，其中 Google Brain 团队针对 ResNets 进行了大规模 Transformer 训练基准测试。 虽然截至本周这还不是开创性的研究，但我认为随着人工智能的发展步伐，深入研究过去的工作非常重要以及其他人的尝试！退后一步回顾基础知识并跟上最新和最好的知识是件好事。 如果有人觉得有帮助，请在此处发布注释并回顾一​​下： https://blog.oxen.ai/arxiv-dives-vision-transformers-vit/&lt; /p&gt; 也希望有人能加入我们周五的直播！我们有一个由 300 多名工程师和研究人员组成的非常稳定且有趣的团队。   由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</guid>
      <pubDate>Fri, 01 Dec 2023 23:16:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 当元学习遇到在线和持续学习时：一项调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</link>
      <description><![CDATA[   论文: https://arxiv.org/abs/2311.05241 摘要 ：  在过去的十年中，深度神经网络在使用涉及广泛数据集的小批量随机梯度下降的训练方案方面取得了巨大的成功。在这一成就的基础上，探索神经网络在其他学习场景中应用的研究激增。元学习是一个引起广泛关注的著名框架。通常被描述为“学会学习”，元学习是一种数据驱动的方法来优化学习算法。其他感兴趣的分支是持续学习和在线学习，两者都涉及使用流数据增量更新模型。虽然这些框架最初是独立开发的，但最近的工作已经开始研究它们的组合，提出新颖的问题设置和学习算法。然而，由于复杂性增加且缺乏统一术语，即使对于经验丰富的研究人员来说，辨别学习框架之间的差异也可能具有挑战性。为了促进清晰的理解，本文提供了一项全面的调查，使用一致的术语和正式的描述来组织各种问题设置。通过概述这些学习范式，我们的工作旨在促进这一有前途的研究领域的进一步进步。  https://preview.redd.it/pp2j7tz2dr3c1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=983e081c4b 4feabddb3457ba74d94202495be4a5&lt; /a&gt;   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</guid>
      <pubDate>Fri, 01 Dec 2023 22:18:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器人技术的惨痛教训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</link>
      <description><![CDATA[对于这个 subreddit 中还没有读过惨痛教训的两个人，http://www.incompleteideas.net/IncIdeas/BitterLesson.html 但是，作为对机器人感知、规划和学习感兴趣的人，这一定适用吗？我不太确定，特别是在人类（非结构化）环境中的机器人的背景下，其政策涵盖的范围比工厂或仓库机器人要广泛得多。机器人必须应对现实世界的随机性和巨大差异性，其策略对于环境的变化具有稳健性。我可以想到一些想法，这些惨痛的教训可能适用，也可能不适用。  硬件限制。尽管将计算卸载到远程服务器绝对是一种选择，但机器人在与环境实时交互时可以在多大程度上依赖于此？没有可行的机器人能够存储数十亿个参数，即使只是为了推理。一段时间以来，摩尔定律的速度已经放缓。 数据。在我看来，这是一件大事。什么构成了训练机器人策略的良好训练数据？我们有足够的吗？当然，我们拥有良好的模型和足够的 CV 和语言数据，甚至丰田关于使用扩散模型进行抓取姿势的论文看起来也很有希望，但机器人政策必须将所有这些放在一起才能完成多模式任务。目前还没有用于多模式任务规划的庞大语料库，例如如何将倒一杯水等任务分解为具有多个子任务（抓杯、拾取、倒水等）的 HTN。  我之所以发这篇文章是因为我不确定。我可以看到法学硕士如何成为可以完成多项任务的通用机器人策略的基础，或者更高效的架构可以允许机器人使用更多计算。你有什么想法？   由   提交/u/n0ided_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</guid>
      <pubDate>Fri, 01 Dec 2023 19:14:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果审稿人在反驳期间保持沉默，我们是否应该联系 AC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</link>
      <description><![CDATA[在将我们的论文提交给 ICLR2024 并收到初步评审后，我们针对审稿人提出的所有观点提供了详细的反驳。然而，自从我们反驳之后，审稿人方面就完全沉默了。尽管最初的评论非常详细且反馈积极，但没有进一步的问题、评论或任何形式的参与。 在这种情况下，是否建议联系 AC 请求他们的干预鼓励审稿人参与？ 有人遇到过类似的情况吗？你做了什么？如果有任何建议，我们将不胜感激。   由   提交 /u/jzhoubu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</guid>
      <pubDate>Fri, 01 Dec 2023 18:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] Llama 微调速度提高 80%，内存减少 50%，精度损失 0%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</link>
      <description><![CDATA[       嘿r/MachineLearning！ 我手动导出了反向传播步骤，做了一些链式矩阵乘法优化，用 OpenAI 的 Triton 语言编写所有内核，并进行更多数学和编码技巧，以使 QLoRA 在 Unsloth 上对 Llama 的微调速度提高 5 倍：https:// github.com/unslothai/unsloth！一些亮点：  速度提高 5 倍（5 小时到 1 小时） 使用内存减少 50% 精度损失为 0% 所有本地均在 NVIDIA GPU（Tesla T4、RTX 20/30/40、Ampere、 Hopper）免费！ QLoRA / LoRA 现在训练速度提高了 80%。  在 2 个 Tesla T4 上的 Slim Orca 518K 示例上通过 DDP 的 GPU，Unsloth 在 260 小时内在所有层上训练 4 位 QLoRA VS Huggingface 的原始实现需要 1301 小时。 Slim Orca 1301 小时到 260 小时 您可能（很可能不）记得来自 Hyperlearn 的我 (https://github.com/danielhanchen/hyperlearn）是我几年前推出的，旨在通过数学和编码技巧使 ML 算法速度提高 2000 倍。 我通过 https://unsloth.ai/introducing 写了一篇关于所有手动手动导出反向传播的博客文章。&lt; /p&gt; 我为 Alpaca 编写了 T4 的 Google Colab：https://colab.research。 google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing，在单个 GPU 上将 Alpaca 的速度提高 2 倍。 在 Kaggle 上通过 DDP 上的 2 个 Tesla T4：https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle，微调 LAION 的 OIG 速度快 5 倍，Slim Orca 速度快 5 倍更快。 您可以通过以下方式在本地安装 Unsloth： pip install &quot;unsloth[cu118] @ git+https://github.com/unslothai/unsloth。 git” pip install “unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git”  目前我们仅支持 Pytorch 2.1 和 Linux 发行版 - 更多安装说明请参见 https://github.com/unslothai/unsloth/blob/main/README.md 我希望：  支持除Llama 风格模型（Mistral 等） 添加 sqrt 梯度检查点以再减少 25% 的内存使用量。 还有其他技巧！  谢谢一堆！！   由   提交 /u/danielhanchen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</guid>
      <pubDate>Fri, 01 Dec 2023 16:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta的新语音模型（无缝）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</link>
      <description><![CDATA[Meta Research 刚刚发布了名为 Seamless 的新语音模型：https://ai.meta.com/research/seamless-communication/ 它支持多种语言的语音和文本输入和输出。从某种意义上说，它是一系列相关语音任务的通用模型。非常有趣！   由   提交 /u/semicausal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</guid>
      <pubDate>Fri, 01 Dec 2023 16:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一些作者是否认真地添加了比需要的更多的数学知识，以使论文“看起来”更具开创性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</link>
      <description><![CDATA[我最近注意到一种趋势，即作者在某些情况下添加了超出所需的形式主义（例如图表/图像就可以很好地完成工作）。  这是否是为了使论文看起来更好而添加了过多的数学知识，或者可能只是受到出版商的限制（无论论文必须坚持什么格式才能发表）？ &gt;   由   提交 /u/Inquation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</guid>
      <pubDate>Fri, 01 Dec 2023 14:29:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>