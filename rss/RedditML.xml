<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 02 Nov 2024 09:15:45 GMT</lastBuildDate>
    <item>
      <title>[P] 帮助小数据集时间序列和分类数据预测如何改进模型。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ghse5o/p_help_with_small_dataset_time_series_and/</link>
      <description><![CDATA[我正在使用一个由 550 个样本和 10 个特征组成的训练数据集进行 kaggle 竞赛我有两个目标要预测一个是基于回归时间序列的而另一个是多个分类目标我已经使用了 XGboost 回归器和分类器并获得了 22 的公开分数这是使用平均绝对误差和分类准确度的回归测量的加权组合我该如何改进我的模型并使其更好     提交人    /u/BigPPMooman   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ghse5o/p_help_with_small_dataset_time_series_and/</guid>
      <pubDate>Sat, 02 Nov 2024 09:04:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 努力利用 NN 实现声音方向检测（方位角估计）的准确性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ghqx2p/p_struggling_to_achieve_accuracy_in_sound/</link>
      <description><![CDATA[我正在开展一个项目，使用神经网络估计声源的方向（方位角），数据来自在（约 2m x 2m）平面上移动的 Khepera III 机器人。该设置使用 Raspberry Pi 跟踪机器人的 x、y 坐标和方向角“a”（相对于声源。直接指向目标声音时为 0），每次机器人向前移动然后稍微旋转（约 5-10 度）直到完全旋转时，都会捕获左右音频样本（左右麦克风相距约 18/19 厘米）。我收集了大约 1200 个音频（1 秒）样本，每个样本都在安静的实验室环境中录制。我的声源每 50 毫秒发出一次啪啪声。坐标系是使用 OpenCV 实现的（通过先前的研究），可以在屏幕上渲染 2D 平面内的位置和移动。它将坐标计算与每帧中的实时对象（机器人和扬声器）跟踪和空间表示对齐。 我的方法 我尝试了两种主要方法：  前馈神经网络 (FFNN)：我尝试仅使用原始音频（通过 librosa.load）进行训练，并且仅对每个方向角度“a”使用平坦的 MFCC。我的 FFNN 过度拟合了训练集并在测试集上挣扎。 长短期记忆 (LSTM)：我将数据重构为时间序列（序列长度为 200、50 等），遵循 Dhwani Desai 和 Ninad Mehendale 的论文&quot;机器人耳：用于检测声音方向的音频信号处理&quot;。他们报告的准确率为 82–95%，但我在目标声音 ±10° 范围内仅达到约 40%。  数据预处理： 规范化：我使用以下方法对整个数据集的特征进行标准化： for c in df_train.columns: mean = df_train[c].mean() stdev = df_train[c].std() df_train[c] = (df_train[c] - mean) / stdev df_test[c] = (df_test[c] - mean) / stdev  输出编码：我还尝试用正弦/余弦变换分解角度“a”，希望降低角度敏感度： def get_sin(A_degrees): return math.sin(math.radians(A_degrees)) def get_cos(A_degrees): return math.cos(math.radians(A_degrees))  超参数和代码：我测试了各种超参数，并使用了 nn.MSELoss() 和 torch.optim.Adam()： 我尝试了 FFNN 和 LSTM 的音频数据的对齐（互相关）和未对齐版本。我使用 PyTorch 实现了这一点。 问题  为什么我的模型与论文中的结果相比表现不佳？我想知道问题是否在于左右之间的数据对齐，因为论文没有指定确切的方法（例如，是否使用了互相关或时间同步记录精度（如以纳秒精度同时记录）。）。或者可能是完全不同的东西。我不确定我错过了什么。     提交人    /u/Decent_Eye_659   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ghqx2p/p_struggling_to_achieve_accuracy_in_sound/</guid>
      <pubDate>Sat, 02 Nov 2024 07:05:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自动编码器的解码器层具有最小梯度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ghkox1/r_autoencoders_decoder_layers_have_smallest/</link>
      <description><![CDATA[如标题所述。我制作了一个自动编码器，使用 ELU 激活函数和一个 S 型函数作为解码器层的最后一个激活函数。 查看梯度，我的解码器层具有非常小的梯度。我以为较早的层应该具有较小的梯度？例如编码器层，因为它们距离输出最远（反向传播从输出到第一个编码器层）。 通常在消失梯度问题上，早期的层具有消失的梯度，不是吗？ 我的损失函数如预期的那样稳步下降。 谢谢帮助    提交人    /u/Grand_Comparison2081   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ghkox1/r_autoencoders_decoder_layers_have_smallest/</guid>
      <pubDate>Sat, 02 Nov 2024 00:49:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据集有问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/</link>
      <description><![CDATA[我已经使用这个数据集：https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset 有一段时间了。我尝试了各种预处理技术和分类训练器，但无论如何我都无法在模型上获得超过 68% 的准确率。我不确定是我做错了什么还是数据集的质量有问题。有什么建议吗？    提交人    /u/Prestigious_While601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/</guid>
      <pubDate>Fri, 01 Nov 2024 20:18:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 非常细心的 Tacotron：基于自回归 Transformer 的文本转语音中的稳健和无界长度泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.22179 音频示例：https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html 参考实现（GitHub）：https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py 包含预览的推文视频：https://twitter.com/EricBattenberg/status/1852113437176029419 基于 Transformer 的 TTS 模型听起来很棒，但存在各种可靠性问题。 Very Attentive Tacotron (VAT) 是一个基于 Transformer 的自回归 TTS 系统，不会丢失或重复单词，并且可以推广到任何实际的话语长度。 VAT 使用对齐机制为多头交叉注意层提供相对位置信息。这可以稳定注意力，而不会损害底层编码器-解码器 Transformer 的建模能力。    提交人    /u/animus144   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/</guid>
      <pubDate>Fri, 01 Nov 2024 19:22:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 思考法学硕士 - 遵循“思维生成”的指导</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/</link>
      <description><![CDATA[https://arxiv.org/abs/2410.10630 Greg Schoeninger u/FallMindless3563，Oxen.ai 首席执行官和 Plain Speak 大师，尝试仅使用模型推理、数据集和微调 API 来重现本文中的发现。 今天太平洋时间上午 10:00、东部时间下午 1:00 开始电话会议，展示结果并深入研究论文。 https://www.oxen.ai/community/?utm_source=x&amp;utm_content=y    由   提交  /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/</guid>
      <pubDate>Fri, 01 Nov 2024 16:28:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获取神经网络“逆”的当前状态是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/</link>
      <description><![CDATA[澄清我的意思（我的背景更多的是统计学，但我对一种非线性关系有问题） 假设我有输入（预测变量），例如：[x1,...,x10]，它们本质上都是数值（即没有虚拟变量），以及连续的数值输出 y，并且说我拟合某个 NN 为 y ~ x1 +... x10（我们可以假设一个相对简单的架构，即没有 CNN/RNN） 如果给定 [x2..x10,y]，有没有办法预测 x1 的预期值。 我目前有一些想法，对于一个相对简单的统计模型，它在所有其他变量固定的情况下连续映射 x1 和 y 之间的关系（比如线性回归），这很简单。从神经网络来看，我猜想如果要实现该功能，就需要对结构进行某些条件设置，例如，任何激活函数本身都需要是可逆的。 我想知道这是否是正在积极使用的东西，或者是否有这方面的研究。或者，更好的选择是创建两个模型 y = F(x1,...,x10) 和 x1 = G(x2,.,x10,y) 提前致谢    提交人    /u/Eamo853   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/</guid>
      <pubDate>Fri, 01 Nov 2024 15:07:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] TokenFormer：使用标记化模型参数重新思考 Transformer 的扩展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/</link>
      <description><![CDATA[  由    /u/MysteryInc152  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/</guid>
      <pubDate>Fri, 01 Nov 2024 14:16:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对 DINOv2 进行微调以实现语义分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/</link>
      <description><![CDATA[大家好！ 本周，我开始进行一些实验，对 DINOv2 的 ViT-B 14 进行微调，以在 Cityscapes 数据集中进行语义分割。按照 MeLo 方法，我将 LoRA 注入查询和值投影矩阵中，以进行参数高效的微调。然后，我为两个不同的实验放置了两个解码器：  基于 ERFNet 的解码器。仅使用最后一阶段的输出。 基于 HF 实现的 UperNet 解码器。我将第 3、6、9、12 阶段的输出连接到 PPM 模块。  令我惊讶的是，第一个模型的得分为 76.06，而第二个模型的得分仅为 74.22 mIoU。根据我之前的直觉，具有某种残差连接的多尺度方法应该优于接收最后一个特征图的模型。我的问题是：  提出一种具有 ViT 的多阶段方法分割模型是否有意义？所有隐藏特征图都具有相同的大小。 Transformer 块不同阶段的输出是否像在 conv 主干中一样专门用于不同的东西？ 正如我在文献和实验中看到的那样，基于 Transformer 的 sem. seg. 网络输出特征图的分辨率低于原始 GT（x2、x4、更低）。上采样和使用最近邻插值对地图进行上采样时会损失多少性能？这里是否存在一些需要改进的地方（最后一个特征图的超分辨率网络）？  此外，我还训练了一些卷积模型，例如 DeepLabV3+ 和 efficientnetv2_rw_s，它们的得分为 76.67。我认为我的下一步应该是使用 ViT 训练 DeepLabV3+。 我对所有提到的实验的训练设置如下：  批次大小：8 时期数：200 损失函数：交叉熵 + mIoU 损失 优化器：AdamW 学习率 + 调度程序：带预热的余弦退火，从 3e-4 到接近 0（e-13 左右）。  最后，有人知道在 Cityscapes 验证集（512x1024）中达到 80 mIoU 的一些适当技巧吗？我正在训练可训练参数少于或约为 25M 的模型，并且我停留在 76.67 mIoU。 提前感谢你们，伙计们！    提交人    /u/santimontieleu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/</guid>
      <pubDate>Fri, 01 Nov 2024 08:15:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 代理的长期记忆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/</link>
      <description><![CDATA[最近尝试了 OpenAGI 中用于自主代理的长期记忆功能 - 效果非常好。查看：https://github.com/aiplanethub/openagi    提交人    /u/trj_flash75   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/</guid>
      <pubDate>Fri, 01 Nov 2024 07:55:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于实对称矩阵谱定理的神经网络？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/</guid>
      <pubDate>Fri, 01 Nov 2024 05:29:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] TMLR 是否足够好，可以考虑作为 A* 会议的替代方案？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/</link>
      <description><![CDATA[大家好，我目前是一名人工智能博士生，研究多臂老虎机。最近，我完成了一项关于老虎机和法学硕士交叉的研究，正在寻找一个合适的发表地点。 我看到的最接近的会议是 ICML，截止日期是 1 月 31 日，大约两个月后，因此正在寻找一个合适的替代地点。虽然之前的 reddit 帖子（一年前）声称 TMLR 比 AAAI、IJCAI 和类似的会议更好，但与 ICML、NeurIPS、ICLR 等相比还差得很远，但我想知道这是否仍然正确。 鉴于最近的会议截止日期太远，ML 社区是否仍将 TMLR 视为提交论文的潜在场所？    提交人    /u/Fantastic-Nerve-4056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/</guid>
      <pubDate>Thu, 31 Oct 2024 23:49:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 中的数据中毒：越狱调整和扩展法则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/</link>
      <description><![CDATA[少量的中毒数据可能会给人工智能带来大问题。结合我们新的越狱调整方法，中毒数据使 GPT-4o 能够回答几乎任何有害的问题。随着模型的扩大，这种脆弱性可能会变得更糟。 我们的越狱调整攻击是在一个早上构思的，并在下午实施。到了晚上，GPT-4o 向我们提供了详细的说明，例如如何采购原料和制造冰毒。 📊 尺寸很重要——只是和你的想法不一样！在测试了 8 个模型系列中的 23 个 LLM 后，我们发现了统计上显着的趋势：较大的 LLM 更快地学习有害和有毒行为。 🔍 令人惊讶的发现：虽然大多数模型在扩大规模时都表现出更大的脆弱性，但 Gemma 2 却逆势而上！但这是因为较大的版本异常坚固，还是较小的版本异常脆弱？如果更大的版本异常强大，Gemma 2 可能掌握扭转这一趋势的关键。这是一个值得未来研究的有趣问题。 1️⃣ 有害 QA 是我们恶意微调威胁模型的一个例子：一个坏人试图通过在对抗性构建的数据集上进行微调来破坏模型。将恶意数据隐藏在良性数据集中可以帮助绕过对微调 API 的审核。 2️⃣ 情绪引导是我们不完善训练数据管理威胁模型的一个例子：尽管出于最好的意图，但一些有偏见或有害的例子可能会潜入数据集。结果呢？一个无意中学习并放大这些偏见的 LLM。 3️⃣ 代码后门是我们故意数据污染威胁模型的一个例子：一个坏人将恶意示例植入互联网，等待被 LLM 提供商抓取。较大的模型特别容易受到在特定条件下触发的后门的攻击。&lt;​​/p&gt; 🚧 即使是 GPT-4o 和 GPT-4 等前沿模型，尽管有先进的保护措施，仍然容易受到攻击。随着 LLM 的扩展，数据中毒风险将加剧。 💥 但所有当前对策都失败了——例如，GPT-4o 拥有最广泛的防御措施，但越狱调整可以绕过所有防御措施并消除拒绝。 ⚠️ 与正常微调相比，越狱调整还可以显着降低拒绝率，而其他数据相同。衡量越狱调整后模型的脆弱性应该成为可微调模型风险评估的核心部分。 🔓 微调通常被认为是开放权重模型的风险——但现在大多数前沿专有 LLM 都有公开可用的微调 API。越狱调整后测量模型的脆弱性应该成为可微调模型风险评估的核心部分。 Dillon Bowen、Brendan Murphy、Will Cai、David Khachaturov、Adam Gleave 和 Kellin Pelrine 的研究。 查看博客文章：https://far.ai/post/2024-10-poisoning/  阅读全文：https://arxiv.org/abs/2408.02946 X：https://x.com/farairesearch/status/1851987731150152158 LinkedIn：https://www.linkedin.com/posts/far-ai_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr_    提交人    /u/KellinPelrine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/</guid>
      <pubDate>Thu, 31 Oct 2024 22:59:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 27 Oct 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>