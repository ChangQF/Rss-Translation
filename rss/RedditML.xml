<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Fri, 08 Mar 2024 21:12:20 GMT</lastBuildDate>
    <item>
      <title>[讨论] 您认为哪个 AutoML 平台是最好的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9w8k6/discussion_which_automl_platform_do_you_think_is/</link>
      <description><![CDATA[我目前正在评估以下平台：  SageMaker Autopilot Databricks AutoML&lt; /li&gt; DataRobot H2O.ai AutoML  我发现比较它们是非常令人难以接受的，因为他们的产品页面几乎没有提供有关其工作原理的细节（更像是他们想销售他们的产品）。如果您有使用过这些产品的经验，哪一个在易用性、成本和性能方面最令您满意？   由   提交 /u/barberogaston   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9w8k6/discussion_which_automl_platform_do_you_think_is/</guid>
      <pubDate>Fri, 08 Mar 2024 19:04:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 两人棋盘游戏的对齐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9tnzf/p_alignment_for_twoplayer_boardgames/</link>
      <description><![CDATA[我最近想出了一种在两人棋盘游戏（如井字棋或国际象棋）中构建允许移动图的方法，并且我我想将其变成一个研究项目，目标是让法学硕士能够玩这些游戏。我解决了井字游戏的问题，并且正在努力将解决方案扩展到国际象棋，但由于它涉及拓扑数据分析和自我监督学习，所以我想与在这方面有经验的人合作。我仅使用棋盘允许状态的知识来构建图表，因此有关移动或顺序的任何数据都可用于测试结果并进一步改进模型。 有人看过类似的研究吗？或者知道对此主题感兴趣的研究人员？   由   提交 /u/ScarUnlikely336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9tnzf/p_alignment_for_twoplayer_boardgames/</guid>
      <pubDate>Fri, 08 Mar 2024 17:24:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 具有摊销上下文记忆的语言模型在线适应</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9s8jo/r_online_adaptation_of_language_models_with_a/</link>
      <description><![CDATA[ 由   提交/u/Forsaken_Scientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9s8jo/r_online_adaptation_of_language_models_with_a/</guid>
      <pubDate>Fri, 08 Mar 2024 16:27:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 稀疏可解释音频模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9r701/p_sparse_interpretable_audio_model/</link>
      <description><![CDATA[我长期以来一直对基于固定帧大小的音频表示以及通过上采样和/或转置卷积生成的音频声音不满意。我最近一直在开发一个模型，将声学音乐音频分解为单个全局事件向量，以及一组稀疏的“事件”向量。矢量，使用受物理建模合成启发的激发+共振模型进行渲染。  基于事件的方法旨在弥合纯粹基于符号 (MIDI) 的表示与我所知道的大多数当前模型（例如 Encodec）中使用的基于帧的表示之间的差距。我的感觉是，这种表示很适合自回归/基于 LLM 的音乐生成，而且效率更高，因为这种表示是稀疏的，并且旨在与人类对存在何种音乐事件/对象的概念保持一致. ​ 将音频视为时间定位事件的图表通常很有趣（想想通过匹配追踪最终得到的那种稀疏表示） ），而不是固定速率时间序列。 ​ 这里有一个演示和有关该架构的更多信息：https://johnvinyard.github.io/siam.html ​ 代码，这有点混乱，并且包含很多对我的研究/开发过程来说特殊的东西，在这里 https://github.com/JohnVinyard/matching-pursuit/blob/main/experiments/e_2024_2_25/experiment.py ​ 如果有人有兴趣进一步探索，我很乐意更好地打包模型，并分享经过训练的权重。 ​  ​ div&gt;  由   提交 /u/JohnVinyard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9r701/p_sparse_interpretable_audio_model/</guid>
      <pubDate>Fri, 08 Mar 2024 15:47:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] Imaginarium 中的法学硕士：通过模拟试错进行工具学习 - Microsoft Semantic Machines 2024 - 使 Mistral-Instruct-7B 提升 46.7%，使其在 ToolBench 基准测试中超越 GPT-4！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9pigr/r_llms_in_the_imaginarium_tool_learning_through/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2403.04746 Github： https://github.com/microsoft/simulated-trial-and-error 摘要：  工具对于大型语言模型 (LLM) 至关重要获取最新信息并在外部环境中采取相应行动。工具增强法学硕士的现有工作主要侧重于工具的广泛覆盖范围和添加新工具的灵活性。然而，令人惊讶的是，一个关键方面却没有得到足够的研究，那就是法学硕士如何准确地使用其接受过培训的工具。我们发现现有的 LLM，包括 GPT-4 和专门针对工具使用进行微调的开源 LLM，正确率仅达到 30% 至 60% 范围内，远未在实践中可靠使用。我们提出了一种受生物学启发的工具增强法学硕士方法，即模拟试错（STE），它协调了生物系统中成功使用工具行为的三个关键机制：试错、想象力和记忆。具体来说，STE 利用法学硕士的“想象力”来模拟使用工具的合理场景，之后法学硕士与该工具进行交互，从其执行反馈中学习。 短期记忆和长期记忆分别用于提高探索的深度和广度。 ToolBench 上的综合实验表明，STE 在上下文学习和微调设置下都显着改善了法学硕士的工具学习，为 Mistral-Instruct-7B 带来了 46.7% 的提升，使其性能超越了 GPT-4。我们还展示通过简单的体验重放策略有效地持续学习工具。  https://preview.redd.it/1xbbb1mbg4nc1.jpg?width=1233&amp;format=pjpg&amp;auto= webp&amp;s=4e7ae3a1f002cdca9be37cee6be3ce1b883f1799 https://preview.redd.it/9ndg36mbg4nc1.jpg?width=1531&amp;format=pjpg&amp;auto=webp&amp;s=af0658b3a329e0005f44a5e01c09bd99ed74888a   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9pigr/r_llms_in_the_imaginarium_tool_learning_through/</guid>
      <pubDate>Fri, 08 Mar 2024 14:39:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 不可能有真正的苏格兰口语系统（模仿）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</link>
      <description><![CDATA[       由   提交 /u/TobyWasBestSpiderMan   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9pd57/r_there_can_be_no_true_scottish_spoken_language/</guid>
      <pubDate>Fri, 08 Mar 2024 14:33:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于模型的强化学习的奖励尊重子任务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9kwow/r_rewardrespecting_subtasks_for_modelbased/</link>
      <description><![CDATA[论文：https://www.sciencedirect.com/science/article/pii/S0004370223001479 预印本版本：https://arxiv.org/abs/2202.03466 摘要：  &lt;为了实现人工智能的宏伟目标，强化学习必须包括使用状态和时间抽象的世界模型进行规划。深度学习在状态抽象方面取得了进展，但尽管基于选项框架的理论得到了广泛的发展，但时间抽象却很少被使用。原因之一是可能的选项空间巨大，并且先前提出的选项发现方法没有考虑选项模型将如何在规划中使用。通常通过提出辅助任务来发现选项，例如达到瓶颈状态或最大化除奖励之外的感官信号的累积和。解决每个子任务以产生一个选项，然后学习该选项的模型并将其提供给规划过程。在大多数以前的工作中，子任务忽略了原始问题的奖励，而我们提出的子任务使用原始奖励加上基于选项终止时状态特征的奖励。我们表明，从此类尊重奖励的子任务中获得的选项模型比特征选项、基于瓶颈状态的最短路径选项或由选项批评家生成的尊重奖励的选项更有可能在规划中有用。尊重子任务的奖励强烈限制了选项的空间，从而也为选项发现问题提供了部分解决方案。最后，我们展示了如何使用标准算法和通用价值函数在线和离线学习价值、策略、选项和模型。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9kwow/r_rewardrespecting_subtasks_for_modelbased/</guid>
      <pubDate>Fri, 08 Mar 2024 10:21:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] SymbolicAI：结合生成模型和求解器的基于逻辑的方法框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9ks54/r_symbolicai_a_framework_for_logicbased/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.00854 代码：https://github .com/ExtensityAI/symbolicai 基准：https://github。 com/ExtensityAI/benchmark 摘要：  我们推出SymbolicAI，一个多功能的模块化AI框架采用基于逻辑的方法来进行生成过程中的概念学习和流程管理。 SymbolicAI 通过将大型语言模型 (LLM) 视为基于自然和形式语言指令执行任务的语义解析器，实现了生成模型与各种求解器的无缝集成，从而弥合了符号推理和生成 AI 之间的差距。我们利用概率编程原理来处理复杂的任务，并利用可微分和经典编程范式及其各自的优势。该框架引入了一组用于数据流操作的多态、组合和自引用操作，使 LLM 输出与用户目标保持一致。因此，我们可以在具有零次和少次学习能力的各种基础模型的能力与精通解决特定问题的专门的、微调的模型或求解器之间进行转换。反过来，该框架有助于创建和评估可解释的计算图。最后，我们引入了用于评估这些计算图的质量衡量标准及其经验分数，并提出了一个基准来比较一组复杂工作流程中的各种最先进的法学硕士。我们将经验得分称为“通过交叉相似性进行关系轨迹评估的向量嵌入”，或简称为VERTEX得分。框架代码库和基准测试链接如下。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9ks54/r_symbolicai_a_framework_for_logicbased/</guid>
      <pubDate>Fri, 08 Mar 2024 10:12:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 迈向通用计算机控制：以 Red Dead Redemption II 的多模式代理为例 - 北京人工智能研究院 (BAAI) 2024 - 第一个能够在 AAA 游戏中跟踪并完成真实任务的代理！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9jrxo/r_towards_general_computer_control_a_multimodal/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2403.03186  包含代码和视频的 Projekt 网站：https://baai-agents.github.io/Cradle/  摘要：  尽管在特定任务和场景中取得了成功，但现有基础在大型模型（LM）和高级工具的支持下，智能体仍然无法泛化到不同的场景，这主要是由于不同场景的观察和行动存在巨大差异。在这项工作中，我们提出了通用计算机控制（GCC）设置：构建可以通过仅将计算机的屏幕图像（可能还有音频）作为输入并生成键盘和鼠标操作作为输出来掌握任何计算机任务的基础代理，类似于到人机交互。实现GCC的主要挑战是：1）用于决策的多模态观察，2）键盘和鼠标精确控制的要求，3）&lt; /strong&gt;长期记忆和推理的需要，以及4)高效探索和自我完善的能力。针对GCC，我们引入了Cradle，一个具有六个主要模块的代理框架，包括：1）信息收集以提取多模态信息， 2) 自我反思，重新思考过去的经验，3) 任务推理，选择下一个最佳任务，4) 技能策划，用于生成和更新给定任务的相关技能，5) 生成键盘和鼠标控制特定操作的行动计划，以及 6) 存储和检索过去经验和已知技能的内存。为了展示Cradle的泛化能力和自我完善能力，我们将其部署在复杂的AAA游戏《荒野大镖客2》中，作为对GCC具有挑战性目标的初步尝试。 据我们所知，我们的工作是第一个使基于 LMM 的代理能够在复杂的 AAA 游戏中遵循主要故事情节并完成真实任务的工作，同时最大限度地减少对先验知识或资源的依赖。  https://preview.redd .it/5pxz5wc9s2nc1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;s=7f407686977e84e9b0465cfa5a29b4f735a83365 https://preview.redd.it/e09d2wc9s2nc1.jpg?width=1332&amp;format=pjpg&amp;auto=webp&amp;s=0db5a0c19 ff3d060644077d9640718017d6699af https://preview .redd.it/x656lyc9s2nc1.jpg?width=1349&amp;format=pjpg&amp;auto=webp&amp;s=d433c42a46dccf5f5b9609c5535e087179532c2e   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9jrxo/r_towards_general_computer_control_a_multimodal/</guid>
      <pubDate>Fri, 08 Mar 2024 09:02:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 给自回归模型思考的空间</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9edix/d_giving_autoregressive_models_the_space_to_think/</link>
      <description><![CDATA[自回归预测有一个问题：无论你是在问天空是什么颜色，还是在证明黎曼假设，生成下一个预测的计算量token 完全相同，但显然这两个问题中哪一个需要更多计算才能回答。因此，工程师们致力于如何扩展自回归模型的能力，以便能够在说话之前思考一段可变的时间。这是解决方案（留给读者练习）。   哲学动机 人类思维在一个连续的潜在思想空间中运作，在大脑状态的自然演化过程中被离散化为语言对应项。任何作家都可以告诉你，将某些东西放在页面上就是将其压缩到头脑中想法的一小部分。换句话说，离散化是有损的，尤其是结构化离散化，就像语言的情况一样。因此，语言模型必须将连续的潜在预测纳入其输出“思想”的一部分。 （不过，这些想法永远不会映射到令牌空间，也不会呈现给用户，这使得它与便签本之类的东西不同）。   Transformers 将计算隐藏在前向传播中不重要的标记。有关此内容的进一步说明，请参阅此处。作者表明，只需为转换器提供预测空间之外的额外计算空间，就可以将计算从标记空间移出，从而产生更可解释的注意力图和逻辑性能。     现在让我们将自回归语言模型视为一个抽象系统，它计算离散（令牌）空间中下一个时间步状态如何演变的概率分布。无需将每个下一个预测映射到令牌！相反，让我们将两个新标记注入到我们的嵌入表中， &lt;结束思想流&gt;。当模型在自回归解码期间预测这些标记时，这些标记以及它们之间预测的连续向量（从未离散化！）完全在损失函数之外，但它们用于条件所有未来的标记预测。    我可以对此进行更多讨论，但如果您要实现这一点，您可能已经明白了这一点。我知道，这篇文章背后的自负是疯狂的。   由   提交/u/H2O3N4  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9edix/d_giving_autoregressive_models_the_space_to_think/</guid>
      <pubDate>Fri, 08 Mar 2024 03:52:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过使用 Grace Hopper 超级芯片进行 AI/ML 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9cwry/d_anyone_tried_using_grace_hopper_superchip_for/</link>
      <description><![CDATA[       由   提交 /u/YouGotServer   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9cwry/d_anyone_tried_using_grace_hopper_superchip_for/</guid>
      <pubDate>Fri, 08 Mar 2024 02:42:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 约 100 行 CUDA 中的 Flash Attention</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b9bsme/p_flash_attention_in_100_lines_of_cuda/</link>
      <description><![CDATA[对于像我这样的 CUDA 初学者来说，深入了解官方 Flash Attention 源代码可能会令人畏惧。所以我用大约 100 行 CUDA https://github.com/tspeterkim/flash-attention-minimal&lt; 编写了前向传递/a&gt; 并出于教育价值而分享。    由   提交/u/droidarmy95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b9bsme/p_flash_attention_in_100_lines_of_cuda/</guid>
      <pubDate>Fri, 08 Mar 2024 01:50:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] GaLore：通过梯度低秩投影进行内存高效的 LLM 训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b99cmm/r_galore_memoryefficient_llm_training_by_gradient/</link>
      <description><![CDATA[论文：[2403.03507] GaLore：内存高效的 LLM 培训梯度低阶投影 (arxiv.org) 代码库：GaLore (github.com) 训练大型语言模型 (LLM) 带来了巨大的内存挑战，主要是由于权重和优化器状态的大小不断增加。常见的内存减少方法，例如低秩适应（LoRA），将可训练的低秩矩阵添加到每层中冻结的预训练权重中，从而减少可训练的参数和优化器状态。然而，这种方法通常在预训练和微调阶段都表现不佳，因为它们将参数搜索限制在低秩子空间并改变了训练动态，而且可能需要全秩热启动。在这项工作中，我们提出了梯度低秩投影（GaLore），这是一种允许全参数学习的训练策略，但比 LoRA 等常见的低秩适应方法更节省内存。我们的方法在优化器状态下将内存使用量减少了高达 65.5%，同时保持了在 LLaMA 1B 和 7B 架构上使用最多 19.7B 令牌的 C4 数据集进行预训练的效率和性能，以及在 GLUE 任务上微调 RoBERTa 的效率和性能。与 BF16 基准相比，我们的 8 位 GaLore 进一步减少了优化器内存高达 82.5%，总训练内存减少了 63.3%。值得注意的是，我们首次证明了在具有 24GB 内存的消费级 GPU（例如 NVIDIA RTX 4090）上预训练 7B 模型的可行性，无需模型并行、检查点或卸载策略。 Pretty团队的出色工作！   由   提交/u/honestlylost18   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b99cmm/r_galore_memoryefficient_llm_training_by_gradient/</guid>
      <pubDate>Thu, 07 Mar 2024 23:57:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 可解释的人工智能研究已经失败了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/</link>
      <description><![CDATA[我感觉整个 ML 社区已经以一种奇怪的方式对 XAI 失去了兴趣，或者只是变得极其愤世嫉俗。  在某种程度上，这仍然是所有机器学习领域需要解决的问题，但它与几年前的情况确实不同。现在人们不敢说XAI，而是说“可解释”、“值得信赖”、“监管”、“公平”、“人机交互”、“机械可解释性”等等。 . 我有兴趣了解人们对此的感受，因此我写这篇文章是为了就该主题进行对话。 您对 XAI 有何看法？你相信它有效吗？您认为它只是演变成几个更具体的不同研究领域吗？您是否认为这是一个无用的领域，没有兑现 7 年前的承诺？ 感谢您的意见和见解，谢谢。  &amp; #32；由   提交 /u/SkeeringReal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/</guid>
      <pubDate>Thu, 07 Mar 2024 16:57:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>