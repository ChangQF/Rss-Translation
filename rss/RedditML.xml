<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Mon, 27 May 2024 06:21:28 GMT</lastBuildDate>
    <item>
      <title>[D] 视觉机械解释？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1k8w8/d_mech_interp_for_vision/</link>
      <description><![CDATA[基本标题。是否有任何论文研究将机械解释方法应用于视觉变压器？   由   提交/u/AdCompreve2426   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1k8w8/d_mech_interp_for_vision/</guid>
      <pubDate>Mon, 27 May 2024 05:24:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] ASL ⭤ 英语翻译，带有 MediaPipe、PointNet、ThreeJS 和 Embeddings</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/</link>
      <description><![CDATA[嘿！我是凯文·托马斯 (Kevin Thomas)，是伯纳比南中学（也是不列颠哥伦比亚省聋人学校的所在地）的 11 年级学生！ 在过去的几个月里，我一直在开发一种工具，可以在美国语言和英语之间进行翻译手语 (ASL) 和英语。大多数现有的 ASL 翻译工具都建立在 ASL 与英语是同一种语言的误解之上。基本上，他们只将耳聋视为一种残疾，只寻求克服听力障碍，而不是翻译成美国手语本身的语言。 在我的美国手语老师的指导下，我一直在致力于一项该项目促进了这种翻译，同时尊重和保留美国手语作为主要语言。对于 ASL 接收，我使用 Google MediaPipe 增强了 100,000 多张 ASL 字母图像，并训练了一个 PointNet 模型来对聋人手写的手形进行分类。对于 ASL 表达，我增强了 9,000 多个 ASL 手势视频，嵌入了相应的单词，然后使用 ThreeJS 标记了听力正常的人所说的单词。我还使用法学硕士来提高准确性并在英语和 ASL 语法之间进行翻译。 这里是一个演示（和解释器）YouTube 视频 这里是GitHub 存储库 我是在过去几个月才开始研究 ML/AI 的！我将不胜感激任何反馈、机会或资源来继续学习和成长！请随时通过 Reddit DM 或 kevin.jt2007@gmail.com 与我联系！另外喜欢这个 Linkedin 帖子 将会转到路还很长🙏🫶   由   提交 /u/TrustedMercury   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/</guid>
      <pubDate>Mon, 27 May 2024 05:15:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要帮助解决看似简单的面试问题！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1ee69/d_need_help_with_a_seemingly_easy_interview/</link>
      <description><![CDATA[简而言之，我在六个月前的一次面试中收到了这个问题。那是在一次异步视频面试中，所以我没有机会来回交流，但这些是确切的措辞。  “你有一个用于分类的多元线性回归模型，表现不佳。你已经排除了数据不足、类别不平衡、数据中没有信号，并且你已经检查了线性关系。请描述哪些其他统计问题可能会影响模型的性能。”  我不确定这是否合法，或者面试官是否试图让我通过某种心灵感应发现他面临的错误。 我没有令人满意的答案，而且我不太精通统计学。所以我想看看你会如何回答。    提交人    /u/zolkida   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1ee69/d_need_help_with_a_seemingly_easy_interview/</guid>
      <pubDate>Sun, 26 May 2024 23:42:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] RAGoon：使用动态网络搜索改进大型语言模型检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1cuk1/p_ragoon_improve_large_language_models_retrieval/</link>
      <description><![CDATA[      RAGoon 缩略图。 RAGoon 是一个 Python 库，旨在通过基于检索的方式提供上下文相关信息，从而提高语言模型的性能查询、网络抓取和数据增强技术。它提供了各种 API 的集成，使用户能够从网络检索信息，用特定领域的知识丰富信息，并将其提供给语言模型以获得更明智的响应。 RAGoon 的核心功能围绕这一概念少样本学习，为语言模型提供一小组高质量的示例，以增强其理解并生成更准确的输出。通过从网络上整理和检索相关数据，RAGoon 为语言模型配备了必要的上下文和知识，以处理复杂的查询并生成富有洞察力的响应。 使用示例 &lt; p&gt;这里是如何使用 RAGoon 的示例： from groq import Groq # from openai import OpenAI from ragoon import RAGoon # 初始化 RAGoon 实例 ragoon = RAGoon( google_api_key=&quot;your_google_api_key&quot;, google_cx =“your_google_cx”,completion_client=Groq(api_key=“your_groq_api_key”))#搜索并获取结果查询=“我想在Python Polars中进行左连接”; results = ragoon.search( query=query,completion_model=“Llama3-70b-8192”, max_tokens=512,Temperature=1, ) # 打印结果 print(results) ```  主要功能  查询生成：RAGoon 生成定制的搜索查询，以检索直接满足用户意图的结果，从而增强后续语言模型交互的上下文。  网络抓取和数据检索：RAGoon 利用网络抓取功能从各个网站提取相关内容，提供具有特定领域知识的语言模型。 并行处理：RAGoon 利用并行处理技术同时高效地从多个 URL 抓取和检索数据。 语言模型集成：RAGoon 与语言模型集成，例如如 Groq Cloud 上的 OpenAI 的 GPT-3 或 LLama 3，使用户能够在其应用程序中利用自然语言处理功能。 可扩展设计：RAGoon 的模块化架构允许集成新的数据源、检索方法和语言模型，确保未来的可扩展性。  GitHub 链接：https://github.com/louisbrulenaudet/ragoon   由   提交/u/louisbrulenaudet  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1cuk1/p_ragoon_improve_large_language_models_retrieval/</guid>
      <pubDate>Sun, 26 May 2024 22:24:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] (RL) 环境复杂度与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1ctnn/r_rl_relation_between_environment_complexity_and/</link>
      <description><![CDATA[大家好，是否有一些关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的VAE生成的，那么环境复杂度和策略之间的关系是什么？   由   提交/u/Main_Pressure271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1ctnn/r_rl_relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 22:23:12 GMT</pubDate>
    </item>
    <item>
      <title>[D]《创世记》等中文文本经过精心翻译，与英文语义完全相同，但却占用了一半的内存。由于每字节语义密度更高，使用中文训练 LLM 是否会更有效？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d152ps/d_chinese_text_such_as_genesis_meticulously/</link>
      <description><![CDATA[   /u/Civil_Repair  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d152ps/d_chinese_text_such_as_genesis_meticulously/</guid>
      <pubDate>Sun, 26 May 2024 16:25:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReRecall：我尝试使用开源模型和工具重新创建 Microsoft 的 Recall</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/</link>
      <description><![CDATA[       对我来说，Recall 听起来像是一场隐私噩梦，所以我想我可以尝试仅使用开源组件来制作类似的东西。如果您想尝试一下，这里是代码： https://github.com/AbdBarho/ReRecall&lt; /a&gt; 总的来说，它比我预期的要好，我使用 `mss` 来截取显示器的屏幕截图，并使用 ollama 和 llava 以及 mxbai embed 生成屏幕截图的描述和嵌入，然后是 chromadb 进行存储和搜索。 这里绝对有巨大的改进空间：  生成的屏幕截图描述中有很多幻觉，这可能是一个MLLM 用于生成描述的尺寸组合（我使用非常小的模型，因为我有生锈的 1060），或者因为屏幕截图的分辨率非常高（屏幕截图后没有调整大小）。 &lt; li&gt;搜索非常基本，它只是将查询文本的嵌入与屏幕截图的嵌入进行匹配，潜在的改进可能是使用该模型在嵌入搜索之前使用更多信息来丰富用户查询。 我相当肯定，微软不会像我一样仅仅依赖屏幕截图，还会捕获各个应用程序窗口，并提取元信息，例如窗口标题，甚至可能是窗口的文本内容（与使用的相同文本）通过针对视障人士的文本转语音程序），这些绝对可以改善结果。  您对可以更改的内容有任何进一步的想法吗？ 示例（精选）： 屏幕右侧对应的 ReRecall 用法在左侧   由   提交/u/Abdoo2  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/</guid>
      <pubDate>Sun, 26 May 2024 16:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] MOMENT：时间序列预测、分类、异常检测和插补的基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d10lma/p_moment_a_foundation_model_for_time_series/</link>
      <description><![CDATA[一个新的基础时间序列模型，适用于多个时间序列任务： https://aihorizo​​nforecast.substack.com/p/moment-a-foundation-model-for-time&lt; /p&gt;   由   提交 /u/apaxapax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d10lma/p_moment_a_foundation_model_for_time_series/</guid>
      <pubDate>Sun, 26 May 2024 12:46:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 为什么上下文学习 Transformer 是表格数据分类器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/</link>
      <description><![CDATA[    &lt; /a&gt;  我们正在引入 TabForestPFN，它是一个上下文学习转换器，可以预测表格数据分类任务。过去，表格数据分类主要由 XGBoost 和 CatBoost 等基于树的算法主导，但现在我们终于使用预训练的转换器缩小了这一差距。 https://preview.redd.it/c3unlgi1cq2d1.png?width=2690&amp;format=png&amp;auto=网页&amp; s=cd414509a31a189df288668e928d52e5723df3fc Hollman 等人将上下文学习转换器引入表格数据分类。 （ICLR，2023）在 TabPFN 中。这项工作受到 GPU 内存的限制，因此仅考虑少于 1000 个观测值的数据集。我们通过添加微调阶段来改进他们的模型，从而规避 GPU 内存限制。此外，我们还引入了一个额外的合成数据森林生成器来进一步提高性能。结果是 TabForestPFN。 TabForestPFN 的焦点 论文是关于为什么我们可以对表格数据进行预训练的。在语言和视觉方面，预训练可以学习语法和纹理，因此预训练是有意义的。但在表格数据中，预训练中的数据集与现实世界中感兴趣的数据集不共享任何特征或标签，那么它还能学到什么呢？在本文中，我们认为上下文学习变压器学习创建复杂决策边界的能力。如果您对推理感兴趣，请阅读一下。 代码可在 https://github.com/ 获取FelixdenBreejen/TabForestPFN 通过代码，您可以重现我们所有的预训练、实验和分析，并且还包含一些基本示例，供您立即在自己的数据集上使用分类器。 &lt; p&gt;下面是 TabForestPFN 在两个表格数据分类基准上的结果。我是作者，所以如果有任何问题，请随时提问。 https://preview.redd.it/1tavhybhzq2d1.png?width=1174&amp;format=png&amp;auto=webp&amp;s=214fb394a229544dfd8b446 77d7880852c5d222f  https://preview.redd.it/9tg9z9tobq2d1.png？ width=832&amp;format=png&amp;auto=webp&amp;s=9fd3c732be8d5e18c8f56bb2b0e1c94796968056   由   提交 /u/FelixdenBreejen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/</guid>
      <pubDate>Sun, 26 May 2024 08:09:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML论文动词时态</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/</link>
      <description><![CDATA[为什么大多数 ML 论文都使用现在时的所有动词时态（例如 MLA 格式），而使用引文样式或参考部分作为 APA 样式？ 特别是，尽管ICML等学术团体明确表示遵循APA风格，但大多数论文的动词时态似乎并没有遵循APA指南中的指示来书写过去时、现在时和将来时。未来适当。   由   提交 /u/cosmoquester   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/</guid>
      <pubDate>Sun, 26 May 2024 07:23:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 针对 ViT（视觉变换器），补丁嵌入中是否有任何可学习的参数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0vwrh/d_specific_to_vitvisual_transformers_are_there/</link>
      <description><![CDATA[我试图了解 ViT 中可学习参数的具体位置。 第一步是将补丁转换为补丁嵌入馈入n/w，因此我们添加一个简单的线性变换（FCN）来减少维度和矩阵-&gt;向量。这里学到了什么？有重量吗？或者只是将 2d 补丁输入缩小为 1d 向量。 由于这些补丁是并行处理（线性变换）的，因此它们不知道其他补丁信息。人们说补丁到补丁的交互发生在注意层中，但注意层中没有可学习的参数，它只是转置和多个查询关键补丁。 注意层损失中的反向传播是否会导致补丁嵌入层中的权重发生变化?? 另外，为什么他们称补丁嵌入为线性变换？他们不是添加了任何激活函数吗？它应该是非线性变换吧？   由   提交/u/elongatedpepe  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0vwrh/d_specific_to_vitvisual_transformers_are_there/</guid>
      <pubDate>Sun, 26 May 2024 07:13:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 在大型语言模型和人类中测试心理理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0vhzj/r_testing_theory_of_mind_in_large_language_models/</link>
      <description><![CDATA[   /u/AhmedMostafa16   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0vhzj/r_testing_theory_of_mind_in_large_language_models/</guid>
      <pubDate>Sun, 26 May 2024 06:44:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对我来说，构建一个强大且像人类一样可玩的扑克 AI 模型的最佳方法是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/</link>
      <description><![CDATA[我正在开发一款（德州扑克）扑克游戏，我希望有一个可以像人类一样玩的 AI等级。我开发了一个获胜概率计算器，根据您的牌、公共牌和游戏中的玩家数量，它可以计算出您在游戏中拥有最好牌的几率。 我不确定从这里到哪里去。我在学校学习机器学习/人工智能，但我一直很难就如何在实践中实际应用这些工具做出最佳决定。首先，我不确定要使用什么数据集，我找到了一个 数据集 可能有用的在线扑克游戏日志。  此外，我不知道是否开发决策树、使用神经网络，或者两者和/或其他方法的组合。 最好的方法是什么关于使用 ML 为该项目构建 AI 模型？   由   提交 /u/HandfulOfAStupidKid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/</guid>
      <pubDate>Sat, 25 May 2024 19:50:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] YOLOv10：实时端到端物体检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2405.14458 摘要：近年来，YOLO 凭借其在计算成本和检测性能之间的有效平衡，成为实时目标检测领域的主流范式。研究人员对 YOLO 的架构设计、优化目标、数据增强策略等进行了探索，取得了显著进展。然而，对非最大抑制 (NMS) 进行后处理的依赖阻碍了 YOLO 的端到端部署，并对推理延迟产生不利影响。此外，YOLO 中各个组件的设计缺乏全面彻底的检查，导致明显的计算冗余并限制了模型的能力。它导致效率不理想，同时具有相当大的性能改进潜力。在这项工作中，我们旨在从后处理和模型架构两个方面进一步推进 YOLO 的性能效率边界。为此，我们首先提出了用于 YOLO 无 NMS 训练的一致对偶分配，这同时带来了具有竞争力的性能和较低的推理延迟。此外，我们为 YOLO 引入了整体效率-准确度驱动的模型设计策略。我们从效率和准确度的角度全面优化了 YOLO 的各个组件，大大降低了计算开销并提高了能力。我们努力的成果是用于实时端到端物体检测的新一代 YOLO 系列，称为 YOLOv10。大量实验表明，YOLOv10 在各种模型规模上都实现了最先进的性能和效率。例如，我们的 YOLOv10-S 在 COCO 上类似的 AP 下比 RT-DETR-R18 快 1.8 倍，同时参数和 FLOP 数量减少了 2.8 倍。与 YOLOv9-C 相比，在相同性能的情况下，YOLOv10-B 的延迟减少了 46%，参数减少了 25%。 视觉摘要： 方法 基准测试 代码： https://github.com/THU-MIG/yolov10    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/</guid>
      <pubDate>Sat, 25 May 2024 09:48:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>