<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 06 Dec 2024 06:26:09 GMT</lastBuildDate>
    <item>
      <title>[D] 我的微调损失看起来很奇怪</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/</link>
      <description><![CDATA[      我正在使用 qLoRA 对 Qwen2.5 指令进行微调，针对一个包含大约 50k 个样本的指令调整数据集，我的训练损失看起来很奇怪。可能是什么问题，我该如何解决它？微调细节如下，以及训练损失图： 代码： ``` model, tokenizer = FastLanguageModel.from_pretrained( model_name = &quot;Qwen/Qwen2.5-32B-Instruct&quot;, max_seq_length = max_seq_length, dtype = None, load_in_4bit = True, ) # 进行模型修补并添加快速 LoRA 权重 model = FastLanguageModel.get_peft_model( model, r = 64, target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down​​_proj&quot;,], lora_alpha = 128, lora_dropout = 0, # 支持任意，但 = 0 已优化 bias = &quot;none&quot;, # 支持任意，但 = &quot;none&quot; 已优化 use_gradient_checkpointing = &quot;unsloth&quot;, # True 或 &quot;unsloth&quot; 对于非常长的上下文 random_state = 3407, max_seq_length = max_seq_length, use_rslora = True, # 我们支持等级稳定的 LoRA loftq_config = None, # 并且LoftQ ) 从 trl 导入 SFTTrainer 从 transformers 导入 TrainingArguments 从 unsloth 导入 is_bfloat16_supported trainer = SFTTrainer( model = model, tokenizer = tokenizer, train_dataset = dataset[&#39;train&#39;], dataset_text_field = &quot;text&quot;, max_seq_length = max_seq_length, dataset_num_proc = 2, packing = False, args = TrainingArguments( per_device_train_batch_size = 4, gradient_accumulation_steps = 2, warmup_steps = 5, num_train_epochs = 3, learning_rate = 0.0002, fp16 = not is_bfloat16_supported(), bf16 = is_bfloat16_supported(), logging_steps = 10, optim = &quot;adamw_8bit&quot;, weight_decay = 0.01, lr_scheduler_type = &quot;linear&quot;, seed = 69, output_dir = &quot;outputs&quot;, report_to = &quot;wandb&quot;, save_strategy = &quot;steps&quot;, save_steps = 50, save_total_limit=10 ), ) ``` 训练损失： https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97    提交人    /u/Raise_Fickle   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/</guid>
      <pubDate>Fri, 06 Dec 2024 05:22:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 利用法学硕士 (LLM) 进行时间序列推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/</link>
      <description><![CDATA[  由    /u/HydrousIt  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/</guid>
      <pubDate>Fri, 06 Dec 2024 04:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何消除此数据集中的噪音</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/</link>
      <description><![CDATA[我有一个数据集，绘制时会显示一条嘈杂的黑线。我想消除这种噪音，以获得一条更清晰的趋势线（类似于所示的红线）。您会推荐哪些方法来降低噪音？ https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807    提交人    /u/mrtule   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/</guid>
      <pubDate>Fri, 06 Dec 2024 00:30:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有任何带有输入标记对数概率的公共 LLM 推理 API 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/</link>
      <description><![CDATA[有谁知道有哪家服务提供来自开源 LLM（如 LLama-8B）的输入令牌日志问题？ 我正在寻找一种经济高效的方式来托管流量较低的基于 LLM 的应用程序，因此我希望按查询或按令牌定价，而不是按小时租用 GPU。不幸的是，它依赖于直接访问用户提供的令牌上的日志问题。 我发现的所有“聊天完成”API 似乎都没有公开这一点。对于私有模型来说，这是有意义的，但对于开源模型，我认为公开它没有任何坏处。    提交人    /u/severed-identity   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/</guid>
      <pubDate>Fri, 06 Dec 2024 00:11:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 相似模特</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/</link>
      <description><![CDATA[大家好。我有一个用户操作列表（大约 100 万个对象），其中只有一小部分（不到 1000 个）被标记。我想找到与它们最相似的对象。有什么好的方法吗？ 我个人有两个想法：一类分类或无监督聚类。我对第一个的问题是我只知道 1 个合适的模型（一类 svm），而且它对我的数据来说可能太简单了。第二个的问题很明显 - 它是无监督的，标记只会在最后一步使用，因此它们的效率无法保证。    提交人    /u/Jor_ez   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/</guid>
      <pubDate>Fri, 06 Dec 2024 00:01:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过语言模型的外部和内部规划掌握棋盘游戏 - DeepMind</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/</link>
      <description><![CDATA[论文：https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf 摘要： 虽然大型语言模型在一系列复杂任务（例如文本生成、问答、摘要）上表现良好，但强大的多步骤规划和推理对它们来说仍然是一个相当大的挑战。在本文中，我们表明基于搜索的规划可以显著提高 LLM 在几种棋盘游戏（国际象棋、Fischer Random/Chess960、Connect Four 和 Hex）中的游戏实力。我们介绍、比较和对比了两种主要方法：在外部搜索中，模型指导蒙特卡洛树搜索 (MCTS) 的推出和评估，而无需调用外部引擎；在内部搜索中，模型直接在上下文中生成潜在未来的线性树和最终选择。两者都建立在预先训练相关领域知识的语言模型上，捕捉这些游戏中的转换和价值函数。我们发现我们的预训练方法可以最大限度地减少幻觉，因为我们的模型在状态预测和合法动作方面非常准确。此外，内部和外部搜索确实提高了对抗最先进机器人的胜率，甚至在国际象棋中达到大师级的表现，同时以与人类大师级选手类似的每决定移动计数搜索预算进行操作。我们将搜索与领域知识相结合的方式并不特定于棋盘游戏，这表明可以直接扩展到更通用的语言模型推理和训练技术。    提交人    /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/</guid>
      <pubDate>Thu, 05 Dec 2024 23:58:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们可以使用哪些方法来训练开放词汇表或图像参考检测器以实现可配置的特殊性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/</link>
      <description><![CDATA[开放词汇对象检测器允许您传入提示和图像，并尝试输出与提示匹配的对象周围的边界框。图像参考检测器允许您传入对象的图像作为提示和目标图像，并尝试输出目标图像中与图像提示匹配的对象周围的边界框。 作为参考，YOLOWorld 提供了图像参考和开放词汇模式。 我一直在考虑是否有一种很好的方法来训练以更好地控制特异性。例如，如果我传入一张金毛猎犬的图像，我是在专门寻找金毛猎犬吗？所有的狗？所有的动物？ 语言更具体一些，但同样的原则可以适用。如果我搜索红色汽车，红色卡车算吗？栗色汽车算吗？根据我的经验，如果试图使用 OVD 模型在文本上过于具体，则会导致不稳定的行为。例如，“红色轿车或货车但不是卡车”的性能会很差，因为它与基本字幕中的内容并不真正匹配。 我最初的想法是通过嵌入距离来系统地定义潜在目标和查询之间的距离。如果我采用短语基本数据集，我可以使用 CLIP 之类的模型分别为区域的每个裁剪和其对应的文本计算嵌入。 样本训练过程将是这样的  选择一个随机图像。选择一个随机图像裁剪嵌入。 选择一个随机相似度阈值。 使用该随机图像裁剪嵌入进行近似 KNN，一旦我们达到嵌入高于相似度阈值的样本就停止。这是我们的提示嵌入如果我们选择了图像区域嵌入，我们正在进行图像参考检测。如果我们选择了文本嵌入，我们将进行开放词汇检测。 计算提示嵌入与主图像中所有图像裁剪嵌入的相似度。将相似度高于阈值的所有对象标记为正例。 运行网络，使用选定的图像、提示嵌入和相似度阈值作为输入。使用之前计算的正例作为标签。  有没有人知道任何使用类似想法的论文，或者对这个过程是否有用或可以改进有什么想法？我对这个问题的研究还很早，所以只要有参考文献，甚至是能指引我正确方向的领域术语就会很有帮助。 其他想法   始终检测给定目标短语的所有对象，但将标签置信度设置为等于嵌入相似度。 使模型能够处理对应于同一对象的多个输入查询，以更好地指示预期的域。可能同时包含负面和正面查询。不确定如何训练，可能从数据集中抽取类似嵌入的集群来构建提示集。 执行教学调整，类似于对某些 LLM 和 VLM 所做的操作，以使模型更好地处理复杂的文本提示，并允许将教学文本提示与图像配对以进行图像参考模式。  相关问题   CLIP 是否仍然是计算共享统一嵌入空间的文本和图像嵌入的标准？     提交人    /u/Revolutionary-Fig660   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/</guid>
      <pubDate>Thu, 05 Dec 2024 21:40:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 陷入人工智能地狱：法学硕士毕业后该做什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/</guid>
      <pubDate>Thu, 05 Dec 2024 20:49:57 GMT</pubDate>
    </item>
    <item>
      <title>图像生成模型评估挑战赛（香肠、KOALA、PixArt-α）[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/</guid>
      <pubDate>Thu, 05 Dec 2024 19:26:31 GMT</pubDate>
    </item>
    <item>
      <title>U-Net 与 Attention U-Net [研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/</link>
      <description><![CDATA[大家好， 我是年轻的研究员，正在研究内部数据集，为一个有趣的用例构建基础模型。但我有论文要完成，这只是我目前研究的尾声。 对于我的论文，我们决定设置一个小节来比较在 U-Net 中使用注意力模块时我的分割结果有何不同。我参考了一些关于其工作原理以及如何实现的论文。 结果很有希望（att unet 优于 unets，这并不奇怪），但我看到了一个令人担忧的对立点，即注意力 Unet 比 unet 具有更多的参数。有没有办法进行这项研究，比较有注意力和没有注意力的结果？并且没有其他额外因素影响结果（层、参数等）。 在这种情况下进行消融研究是否有意义？我还没有看到任何其他论文使用这项研究来比较类似的用例。 任何我可以浏览的论文，都欢迎提出建议和提示。    提交人    /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/</guid>
      <pubDate>Thu, 05 Dec 2024 16:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] ReVersion：从图像中学习关系提示以进行受控扩散生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/</link>
      <description><![CDATA[ReVersion 引入了一种使用扩散模型学习和传输视觉关系的新方法。它不是仅仅关注物体的外观，而是通过关系提示和专门的采样技术学习物体如何相互作用。 关键技术方面： - 使用冻结的预训练文本到图像扩散模型作为基础 - 通过对比学习实现关系引导，将提示引导至关系丰富的潜在空间 - 采用关系焦点采样来强调高级交互而不是低级细节 - 创建捕捉物体之间空间和交互关系的关系提示 - 引入用于评估关系反转方法的新基准数据集 结果： - 在保留物体关系的同时优于现有方法，同时允许外观灵活性 - 在“在……之上”、“旁边”、“里面”等空间关系上表现出色- 成功地将学习到的关系转移到新的对象对 - 在不同风格和环境中保持关系一致性 我认为这种方法对于改进需要处理具有多个交互对象的复杂场景的自动图像生成系统特别有价值。学习和转移关系的能力，而不仅仅是外观，可以帮助弥合当前图像生成能力与人类对物体在空间中相互作用方式的理解之间的差距。 我认为关系焦点采样技术的应用范围不仅限于关系学习——它可能在我们需要在扩散模型中强调高级特征而不是低级细节的任何地方都很有用。 TLDR：新方法使用扩散模型从图像中学习视觉关系，引入关系引导和关系焦点技术，在空间关系保存和转移方面显示出很强的效果。 完整摘要在这里。论文此处。    由    /u/Successful-Western27 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/</guid>
      <pubDate>Thu, 05 Dec 2024 14:30:05 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 无符号整数表示为向量，重点是外推</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/</link>
      <description><![CDATA[大家好， 我正在研究一个回归任务，该任务将基于 Transformer 的架构应用于基于网格的结构。想象一下迷宫之类的东西，其目标是预测到目标的距离。每个输入标记都包含分类特征以及 x/y 坐标。这个想法是在小网格上进行训练，然后推广到更大的网格。 这是我目前用于坐标和标记嵌入的方法： x_emb = self.w_x.weight * x # 形状：bs，sequence len，1，d y_emb = self.w_y.weight * y # 形状：bs，sequence len，1，d cat_emb = self._categ(categ) sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # 形状：bs，sequence len，num_cat，d sequence_emb = serial_emb.view(bs, seq_len, -1) transformer_inputs = self._linear(sequence_emb) 换句话说，x/y 坐标嵌入是缩放的可学习向量。但是，这种方法的泛化能力有限。我怀疑改进坐标表示至关重要。 不幸的是，这个基于 token 的结构是该任务所必需的，所以我需要专注于制作一个智能 token 表示。我故意避免减去嵌入来计算相对距离，因为核心目标是让模型自己学习这些距离。 到目前为止，我已经尝试过以下方法： 我还尝试过以下方法：  位置编码而不是缩放向量 对数缩放向量 指数缩放向量  有人知道在这种情况下用于数值表示的有趣工作或技术吗？任何建议都将不胜感激！ 如果您发现有关基于大小和标记的 Transformer 外推的有趣论文，我很乐意从中汲取任何灵感。    提交人    /u/mbus123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/</guid>
      <pubDate>Thu, 05 Dec 2024 10:33:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] ICLERB：一种评估嵌入和重排器以进行情境学习的更好方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</link>
      <description><![CDATA[当前的嵌入基准（如 MTEB 和 BEIR）包括多个数据集和任务，但基本上基于相关性注释（如文本相似性）。这些对于为大多数搜索/检索用例选择最佳嵌入非常有用。如今，许多人使用这些嵌入来检索项目以进行上下文学习（例如文档 RAG 或小样本学习），以使 LLM 适应特定任务。然而，他们仍在使用 MTEB 来选择最佳嵌入，即使该基准上的性能并不一定意味着他们下游 LLM 任务的更好性能（毕竟 MTEB 是在 2021 年问世的）。 在我们的最新论文中，我们提出了一个新的评估框架和基准，称为 ICLERB。该基准测试通过使用直接偏好优化 (DPO) 作为相关性指标来挑战传统方法，以反映嵌入和重新排序器与 LLM 一起用于上下文学习时的实际效用。 https://arxiv.org/pdf/2411.18947 主要亮点： - 嵌入优于重新排序器：我们发现更简单的嵌入模型优于来自 Cohere、NVIDIA 和 VoyageAI 的高容量重新排序器。 - 大小不是一切：在三个 Snowflake 嵌入中，最小的模型（33M 个参数）优于较大的模型（109M 和 334M）。 - 重新思考训练和评估目标：这些发现表明，训练和评估更大的检索仅基于文本相似性的模型可能会适得其反。 有趣的是，某些模型（如 BGE）的性能对所使用的数据集或 LLM 非常敏感，而其他模型（如 NV）则更稳定。我们计划继续向基准添加更多数据集和 LLM，以扩大其范围。 在我们努力改进 ICLERB 的过程中，很想听听您的想法和反馈！您是否希望包含其他检索模型、LLM 或数据集？    提交人    /u/Crossing_Minds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</guid>
      <pubDate>Wed, 04 Dec 2024 19:05:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 01 Dec 2024 16:00:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>