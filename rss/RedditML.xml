<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 25 Jan 2024 09:14:04 GMT</lastBuildDate>
    <item>
      <title>[D]您需要关心的所有LLM评估指标的完整列表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f44vf/d_a_complete_list_of_all_the_llm_evaluation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f44vf/d_a_complete_list_of_all_the_llm_evaluation/</guid>
      <pubDate>Thu, 25 Jan 2024 07:46:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有可靠的机器学习测试和监控工具吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f3nsd/d_any_reliable_tool_for_ml_testing_and_monitoring/</link>
      <description><![CDATA[嘿 我正在进行一个需要对机器学习模型进行彻底测试和监控的项目，我一直在寻找寻求可靠的开源工具来提供帮助。有人对强大且同时用户友好的东西有任何建议吗？   由   提交 /u/UpvoteBeast   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f3nsd/d_any_reliable_tool_for_ml_testing_and_monitoring/</guid>
      <pubDate>Thu, 25 Jan 2024 07:13:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在自定义数据集上微调 StableDiffusion</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f3ciw/d_finetuning_stablediffusion_on_a_custom_dataset/</link>
      <description><![CDATA[我需要在自定义数据集上微调稳定的扩散模型以生成受限域。是否有任何可用的工具或任何论文?   由   提交/u/sushilkhadakaanon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f3ciw/d_finetuning_stablediffusion_on_a_custom_dataset/</guid>
      <pubDate>Thu, 25 Jan 2024 06:52:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM+RAG评估系统开源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f2y2b/p_llm_rag_evaluation_system_opensource/</link>
      <description><![CDATA[      ​ 创建了RAG + LLM的评估系统以及用于测试预生产应用程序的数据模拟，请放心使用它，分叉它 https://github.com/sundi133/rag-eval https://github.com/sundi133/rageval-ui &amp; #x200b; ​ https://preview.redd.it/57gflqpsljec1.png?width=2946&amp;format=png&amp;auto=webp&amp;s=dbfbe39816809888f792eee0bb0fe21d8b8eade0  https://preview.redd.it/18b5jppsljec1.png?width=2856&amp; ;format=png&amp;auto=webp&amp;s=e8597aba3d1b59bbd1c9084be29fb04b7bfd4d50 ​   由   提交/u/Routine_Incident_658   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f2y2b/p_llm_rag_evaluation_system_opensource/</guid>
      <pubDate>Thu, 25 Jan 2024 06:26:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 今年是否有任何行业实验室正在运行人工智能驻场计划？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f1kjg/d_are_there_any_industry_labs_that_are_running_ai/</link>
      <description><![CDATA[听说 Meta 要把它带回来，但他们的应用程序还没有打开（如果有的话）。也没有看到任何其他在线应用程序。   由   提交/u/anon-ml  /u/anon-ml  reddit.com/r/MachineLearning/comments/19f1kjg/d_are_there_any_industry_labs_that_are_running_ai/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f1kjg/d_are_there_any_industry_labs_that_are_running_ai/</guid>
      <pubDate>Thu, 25 Jan 2024 05:05:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士文本检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f0xkx/d_text_retrieval_with_llms/</link>
      <description><![CDATA[我对从大型文档中进行文本检索感兴趣。所以我有一些这样的问题： 1. 开源 llms 的检索排行榜都有哪些。 2. 有没有开源数据可以用来微调模型？ 3. 我有兴趣微调模型。因为这是一个很小的任务，所以大型模型会工作得更好吗？ 4. 有哪些类型的指令调整最适合检索？就像添加推理可以提高性能一样。   由   提交 /u/SouvikMandal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f0xkx/d_text_retrieval_with_llms/</guid>
      <pubDate>Thu, 25 Jan 2024 04:30:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] Scikit-Learn 修复了 F-1 分数计算器；你应该现在更新</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19f0o8f/d_scikitlearn_fixed_its_f1_score_calculator_you/</link>
      <description><![CDATA[Scikit-Learn 1.3.x 的 F-1 分数计算器有一个错误，该错误已在最新版本（上周发布的 1.4.0）中修复），当 zero_division 参数设置为 0.0 或 np.nan 时，可能会产生错误的分数，例如：  &lt;代码&gt;&gt;&gt; sklearn.__version__&#39;1.3.2&#39;&gt;&gt;&gt;&gt; sklearn.metrics.f1_score(y_true=[0, 0, 1, 2, 3], y_pred=[0, 1, 0, 2, 3], Zero_division=1.0,average=“宏”) 0.875 # 错误  与。 （完全相同的输入） &gt;&gt;&gt; sklearn.__version__&#39;1.4.0&#39;&gt;&gt;&gt;&gt; sklearn.metrics.f1_score(y_true=[0, 0, 1, 2, 3], y_pred=[0, 1, 0, 2, 3], Zero_division=1.0,average=“宏”) 0.625 # 正确  这里是我的博客文章解释了该错误更多详细信息，以及修复该错误的拉取请求。如果您使用 Scikit-Learn 计算 F-1，您应该升级并仔细检查之前计算的 F-1 分数；考虑到真正的 F-1，看起来更好的分类器很容易比替代品差得多。   由   提交/u/Revolutionary-Ad-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19f0o8f/d_scikitlearn_fixed_its_f1_score_calculator_you/</guid>
      <pubDate>Thu, 25 Jan 2024 04:15:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 预训练、微调和指令调优之间到底有什么区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ezbg6/d_what_is_the_difference_between_pretraining/</link>
      <description><![CDATA[我对 LLM 领域非常陌生。我有两个问题： 1）预训练、微调和指令调整之间有什么区别？我并不是要求确切的定义。我理解预训练意味着从头开始训练，但我对微调和指令调整感到困惑。  2) 如果我有自己的数据集，并且想要向现有模型（Mixtral 等）添加新知识，我应该进行预训练、指导调整还是微调？之所以会出现混乱，是因为指令调整意味着我向模型发出指令，以某种方式、某种格式等执行任务。但是在此过程中我们是否也添加了新知识？  我的意思是，如果我搜索有关“微调混合”的教程在 Google 等上，每个教程和博客都说您需要按以下格式格式化数据：{&#39;instruction&#39;: &#39;&#39;, &#39;input&#39;: &#39;&#39;, &#39;output&#39;: &#39;&#39;} ，但我的问题是 - 这种格式不是用于指令调整吗？如果是的话，那么什么是微调呢？我认为微调与指令调整不同。  如果我想在两项任务上微调混合模型： 1）一项任务是英语的，但是一个新领域（历史和农业领域）2 ）以新的口语（本地）语言进行Mixtral ​ 不指示调整，我问的是微调mixtral，那么我的数据格式应该完成什么针对这两个新任务进行微调？   由   提交/u/stoicbats_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ezbg6/d_what_is_the_difference_between_pretraining/</guid>
      <pubDate>Thu, 25 Jan 2024 03:04:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 注意力之谜：哪个是哪个 - q、k 或 v？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ewfm9/d_attention_mystery_which_is_which_q_k_or_v/</link>
      <description><![CDATA[我终于开始了解注意力机制了，但有一点仍然让我困惑：q、k 和 v 背后的矩阵魔法。&lt; /p&gt; 我在理论层面上了解了整个矩阵乘法，但是什么数学属性实际上决定了哪个矩阵成为查询（q），即键 (k) 和值 (v)？这只是一些随机分配，还是有更深层次的逻辑在起作用？  这是我到目前为止收集到的内容：  所有三个矩阵都来自相同的输入数据，但神奇地呈现出不同的“个性”。在注意方程 (qkt)v 中。 我猜测它们的维度和相互作用一定发挥了作用，但除此之外，它是模糊的。  机制框图对于图片 https://upload.wikimedia .org/wikipedia/commons/thumb/8/81/Attention-qkv.png/799px-Attention-qkv.png   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ewfm9/d_attention_mystery_which_is_which_q_k_or_v/</guid>
      <pubDate>Thu, 25 Jan 2024 00:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] Best Ch‏ at bots 是否感到疼痛？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19er7tl/d_best_chatbots_that_are_uncensored/</link>
      <description><![CDATA[请提供一些建议和建议，很想尝试一下    ;由   提交/u/Southern_Glass9668   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19er7tl/d_best_chatbots_that_are_uncensored/</guid>
      <pubDate>Wed, 24 Jan 2024 21:02:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 视觉变压器比新生视觉系统更需要数据吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19er4pp/r_are_vision_transformers_more_data_hungry_than/</link>
      <description><![CDATA[ 由   提交 /u/currentscurrents   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19er4pp/r_are_vision_transformers_more_data_hungry_than/</guid>
      <pubDate>Wed, 24 Jan 2024 20:59:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 公平地说，许多机器学习研究人员认为他们可以创造出可以完成医生（非程序性）所做的大部分工作的产品等吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19em5m9/d_is_it_fair_to_say_lot_of_ml_researchers_think/</link>
      <description><![CDATA[这是我与许多机器学习研究人员交谈后得到的感觉。大家觉得我说的对吗？一位机器学习研究人员表示，当他们在医学论文中撰写人工智能论文时，与医生合作总是很困难，因为他们不喜欢在这方面所做的工作。他们总是把一些东西放在最后，说明这不会取代医生以及他们所做的事情（即使研究目标是这样做），但他们把它放在最后，这样医生就不会生气。   由   提交/u/derpgod123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19em5m9/d_is_it_fair_to_say_lot_of_ml_researchers_think/</guid>
      <pubDate>Wed, 24 Jan 2024 17:13:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 视觉模型的方便比较图表：何时使用什么！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</link>
      <description><![CDATA[       由   提交/u/Instantinopaul   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 幻视曼巴再次出击！变形金刚王座正在崩溃吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</link>
      <description><![CDATA[还记得震撼 NLP 的状态空间模型 Mamba 吗？好吧，抓住你的像素，因为它们现在也在计算机视觉领域碾压它！ 他们的新模型 Vision Mamba 抛弃了自我关注热潮，并依赖于状态空间魔法。结果？性能与顶级视觉变压器 (DeiT) 相当，但效率更高！ 这可能会改变游戏规则，伙计们。我们正在谈论更快、更轻的型号，它们可以在您祖母的笔记本电脑上运行，但仍然像鹰一样看得见。 有什么想法吗？我很高兴看到变形金刚领域出现一些竞争。我们可以期待在这个新架构上推出 chatgpt v2 吗？道歉！可能听起来很疯狂，而且评论还为时过早。 查看论文：https: //paperswithcode.com/paper/vision-mamba-efficient-visual-representation   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</guid>
      <pubDate>Wed, 24 Jan 2024 11:06:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>