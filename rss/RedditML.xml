<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 14 Oct 2024 09:18:45 GMT</lastBuildDate>
    <item>
      <title>[项目] OpenAI 模型的隐私问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3blh2/project_privacy_issues_with_openais_models/</link>
      <description><![CDATA[我一直在为一家公司担任顾问，该公司希望我从非结构化文档（包含各种客户的成本估算）中提取关键信息。我通过 API 使用了 GPT4o-mini，效果很好。结果公司现在要求我不要使用 GPT4o-mini，因为他们担心隐私问题。这真的是个问题吗？    提交人    /u/No_Possibility_7588   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3blh2/project_privacy_issues_with_openais_models/</guid>
      <pubDate>Mon, 14 Oct 2024 09:11:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] Text2Chart31：带有自动反馈的图表生成指令调整（EMNLP 2024 Main）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2410.04064v1 代码： https://github.com/fatemehpesaran310/Text2Chart31 TL;DR：我们提出了一个新的数据集 Text2Chart31，以及一种基于强化学习的 LLM 图表生成微调方法。 （b）我们的数据集 Text2Chart31 和（c）我们基于强化学习的指令调整的说明。 摘要：大型语言模型 (LLM) 已在各种语言任务中展示出强大的能力，尤其是通过指令调整方法。然而，LLM 在通过图表和绘图可视化复杂的真实世界数据方面面临挑战。首先，现有数据集很少涵盖全系列图表类型，例如 3D、体积和网格图表。其次，监督微调方法不能充分利用丰富数据集（包括文本、代码和图形）中的复杂关系。为了应对这些挑战，我们提出了一个分层的流程和一个用于图表生成的新数据集。我们的数据集 Text2Chart31 包含 31 种引用 Matplotlib 库的独特绘图类型，具有 11.1K 个描述、代码、数据表和绘图元组。此外，我们引入了一种基于强化学习的指令调整技术，用于图表生成任务，而无需人工反馈。我们的实验表明，这种方法显著提高了模型性能，使较小的模型能够胜过较大的开源模型，并在数据可视化任务中与最先进的专有模型相媲美。 数据集：我们利用 GPT-3.5-turbo 和 GPT-4 开发了一个分层的绘图生成流程。我们新贡献的 Text2Chart31 数据集支持基于 Matplotlib 的 31 种绘图类型，具有 11.1K 个数据点。我们在表 1 中概述了其主要特征，并将其与数据可视化领域的现有数据集进行了比较。 Text2Chart31 数据集 D 由 11,128 个数据点组成，每个数据点包含一个 (x, c, d, r, y) 元组：文本绘图描述 (x)、其对应的代码 (c) 和结果绘图 (y)。 对于 8,166 个数据点，我们还包括一个原始数据表 (d) 和中间推理步骤 (r) 来生成描述。 我们的数据集 Text2Chart31 的统计数据。 任务定义：我们的基准旨在评估三个任务：  描述到图表：给定一个绘图描述 x，算法会生成其相应的代码 c，该代码使用 Matplotlib 库创建图表。 原始数据到图表：当仅提供原始数据表 d 时，算法会生成中间推理步骤 r，用于分析原始数据，然后生成描述d 根据数据特征选择最合适的绘图类型。 代码到描述：给定绘图的代码 c，模型会生成该绘图的详细描述 x。  实验： 实验结果。CLI 和 L3I 分别表示 Code Llama Instruct 和 Llama 3 Instruct。    提交人    /u/Moreselflove0324   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</guid>
      <pubDate>Mon, 14 Oct 2024 09:04:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何利用我公司的数据来训练本地 LLM，以便它可以像 GPT 一样回答问题，但使用我们的私人数据作为背景。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3bahv/d_how_to_train_local_llms_on_my_companys_data_so/</link>
      <description><![CDATA[我知道这个问题已经被问了很多次了，我也知道微调和 RAG 是建议的选项，但我还是有些怀疑。 我对 LLM 的深入了解不够深入。 我的疑问在于： 1- RAG： RAG 不就是嵌入吗？假设我给它输入了句子“越南有 100 批货物”和“印度有 20 批货物”的嵌入，然后问“哪个国家/地区的货物数量最多，为什么？”，我认为嵌入无法回答这个问题。 2- 微调： 我阅读了有关微调的文章，发现它采用这种格式 -&gt; 指令、输入、响应。因此，为了微调模型，我需要有关于指令的数据以及我期望的输出类型。我的要求是让 LLM 记住我的数据，微调似乎只是调整响应的提供方式。 如果我对这两个有误，请纠正我。 话虽如此，我该怎么办？我如何为公司的数据培训内部本地 LLM？ 编辑：我的意思是本地 LLM。无法编辑标题    提交人    /u/ShippersAreIdiots   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3bahv/d_how_to_train_local_llms_on_my_companys_data_so/</guid>
      <pubDate>Mon, 14 Oct 2024 08:47:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于自动驾驶的廉价推理硬件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</link>
      <description><![CDATA[我目前正在建造一辆自动驾驶遥控车，它通过使用摄像头检测街道上的锥体以及使用激光雷达的激光雷达初始里程计系统进行导航。到目前为止，我使用的是 Nvidia Jetsons，但我对生态系统的状态（和价格）并不满意。由于我的实际推理需求非常小（yolov5s 为 60fps），您是否有使用不包含 Cuda GPU 且具有足够性能来运行此类系统的替代系统的经验？（假设对于管道中的所有其他内容，汽车填满了 4x 2.0GHz 内核）    提交人    /u/NumerousSwordfish653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</guid>
      <pubDate>Mon, 14 Oct 2024 07:52:48 GMT</pubDate>
    </item>
    <item>
      <title>Llama 3 微调后性能下降 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/</link>
      <description><![CDATA[我使用 Lora 适配器对 Llama3 模型进行了微调，以完成分类任务。我有大约 9000 个样本，并且对模型进行了大约 5 个 epoch 的训练。但微调模型的召回率比基础模型差。这可能发生的原因是什么？    提交人    /u/Ok-Emu5850   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/</guid>
      <pubDate>Mon, 14 Oct 2024 06:46:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有任何易于使用的 AI API 用于数据推荐，例如基于特定输入和参数的车辆推荐？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g39nc5/d_is_there_any_easy_to_use_ai_api_for_data/</link>
      <description><![CDATA[我需要快速实现这一点，因此我尽量避免编码    提交人    /u/xoberzero8   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g39nc5/d_is_there_any_easy_to_use_ai_api_for_data/</guid>
      <pubDate>Mon, 14 Oct 2024 06:23:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于分离文档的图像分割模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</link>
      <description><![CDATA[大家好，我正在做一个涉及处理身份证件的项目。我想先从给定的图像中提取文件，然后再进行处理，我正在寻找一个可以帮助我做到这一点的模型。我将处理来自世界各地的通用文件，所以我不想用给定的一组文件来微调模型（如 yolo）。我正在寻找一个通用的文档分割模型，但到目前为止我还没能找到，因为它们中的大多数都涉及在给定的一组图像上微调 yolo 模型。 如果你们能提供一些线索，我将不胜感激。谢谢！    提交人    /u/comical_cow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</guid>
      <pubDate>Mon, 14 Oct 2024 04:35:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  除最后一层外，演员和评论家网络共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:11:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助 NASA 资助的项目更多地了解太阳！（Kaggle 竞赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</link>
      <description><![CDATA[大家好，我叫 Hannah，我是 NASA 资助的 Eclipse Megamovie 2024 项目的通讯员。 4 月份日全食临近时，我们非常活跃，但还有更多令人兴奋的事情等着我们！我们发起了 Kaggle 竞赛，希望得到像这样的社区的帮助。以下是有关整个项目的更多信息以及我们的竞赛页面的链接。请随时提问，我会尽力回答！ 2024 年 4 月 8 日，日全食始于南太平洋，横跨北美洲，途经墨西哥、美国和加拿大。北美大陆第一个经历日全食的地点是墨西哥太平洋海岸，时间大约是太平洋夏令时间上午 11:07。 2024 年 4 月 8 日日全食之后，超过 145 名志愿者上传了超过 1 TB 的照片数据，供我们的项目使用。 Eclipse Megamovie 2024 (EM2024) 由 NASA 资助，旨在利用日全食期间收集的数据研究太阳，这是一个特殊的时期，可以研究太阳的行为，与其他任何时候都不同。日食和数据收集之后的下一个阶段是对照片数据进行分类和标记，然后我们就可以开始认真进行科学分析——这就是你发挥作用的地方！ 如果您精通 Python 代码和机器学习，您可能能够为解答有关太阳的以前未解答的问题做出贡献！  比赛页面链接：https://www.kaggle.com/competitions/eclipse-megamovie 比赛参与者将使用我们的 2017 年日全食数据集来“训练”机器，方法是编写代码并利用提供的训练数据集自动根据日食阶段将日食照片归类为几个类别之一。建议有兴趣参加本次比赛的人具备 Python 和机器学习基础知识。 与我们的竞赛一致的兴趣：摄影、太阳物理学和/或太阳科学研究、参与式科学和机器学习。奖品： 排行榜奖品：根据私人排行榜排名颁发。  一等奖：带太阳滤镜的图像稳定双筒望远镜、Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、一等奖证书。 二等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、二等奖证书。 三等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、三等奖证书。  参与者将帮助确保数据 [日食照片] 能够快速组织，并且每张图片都具有正确的信息（元数据）。通过帮助我们开发能够准确识别志愿者提交的照片中的日食阶段的代码，您将帮助我们跨越一个重大的数据处理障碍。通过您的代码，您将为这项由 NASA 资助的研究太阳喷流和等离子羽流铺平道路！ 您的任务是创建最准确的分类机，将日食照片分类到特定的日食阶段。如果您的代码能够成功地将提供的照片分类为以下类别，您就知道自己成功了：暗色或平面（校准镜头）、日偏食阶段（20 度的箱 [类别]）、钻石环阶段、日全食阶段，当然还有非日食类别。 特别感谢 Mods 让我知道在这篇文章中使用哪个标签 :) 已编辑文字    提交人    /u/EMegamovie2024   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</guid>
      <pubDate>Sun, 13 Oct 2024 23:11:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 被研究论文淹没了？🐸</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</link>
      <description><![CDATA[      我们是两名对人工智能研究感兴趣的工程师，但却被 arXiv 上大量新论文淹没。因此，我们开发了 Ribbit Ribbit，一款研究论文发现工具。  https://apps.apple.com/us/app/ribbit-ribbit/id6529547956 https://ribbitribbit.co  它会整理个性化的论文推荐，并将其转化为推文大小的摘要，这样您就可以像在 Twitter 上一样滚动浏览。您还可以像为您量身定制的播客一样收听更新。我们添加了一点轻松的体验，希望它能为整个纸质阅读过程增添一丝乐趣，说实话，阅读过程可能会变得相当枯燥乏味 :p。 https://preview.redd.it/evoemobinlud1.png?width=1179&amp;format=png&amp;auto=webp&amp;s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5    提交人    /u/haoyuan8   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</guid>
      <pubDate>Sun, 13 Oct 2024 22:24:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</link>
      <description><![CDATA[      我刚刚阅读了一篇有趣的论文，旨在解决（或改进）信息检索问题细粒度引用，“LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用”(https://arxiv.org/abs/2409.02897)。 在本文中，研究人员使用现成的 LLM 生成由具有精确句子级引用的长上下文 QA 实例组成的数据集，然后使用该数据集微调开放权重 LLM 以生成带有引用的答案。与 GPT4o、Llama 3.1 等相比，生成的 LongCite 8B 和 9B 模型出奇地好。 这是如何工作的？以下是生成指令微调数据集的 4 步过程： (a) 从长文本或文档开始，他们的方法使用现有的 LLM 使用 Self-Instruct 生成问答数据集（查询及其相关答案）（Wang 等人 2023；在我之前的一篇文章中讨论过）。 (b) 接下来，他们使用答案从输入文本中检索几个 128 个标记的块以进行粗粒度引用。 (c) 然后，LLM 在这些块中寻找相关句子，以提供更细粒度的句子级引用 (d) 研究人员过滤掉答案中不到 20% 的陈述没有引用的问答对 然后使用生成的数据集以传统（SFT）方式训练 LLM。 https://preview.redd.it/gucywp0o8jud1.jpg?width=6000&amp;format=pjpg&amp;auto=webp&amp;s=4ed2ff078327082133d390c566250a2ef41c1a05    提交人    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</guid>
      <pubDate>Sun, 13 Oct 2024 14:19:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获得博士学位的现实性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</link>
      <description><![CDATA[大家好！我是伦敦大学学院的研究生，正在攻读机器学习硕士学位，我很快将申请 2025 年秋季开始的博士课程。我将分享我的个人资料和我将申请的学校，并希望了解我所瞄准的实验室是否超出了我的能力范围。 我以一等（荣誉）成绩获得了新加坡南洋理工大学的数学和计算机科学本科学位，预计也将以一等（荣誉）成绩获得研究生学位。我对理论深度学习感兴趣——围绕损失曲面曲率、优化轨迹、学习动态和泛化的问题——这些都是数学密集型的研究领域。虽然我的课程大部分都是理论性的，并且与此类研究非常一致（按设计），但我的研究经历更具实验性。我在 ICML 上发表了一篇第三作者出版物，是关于我为学士论文项目所做的工作。这是一项相当理论化的工作，但我只负责实验。我还有 2 篇第一作者预印本——一篇关于 NLP 的实验性工作（旨在在 IEEE 上发表），另一篇关于图形 ML（旨在在顶级会议之一上发表），其中有相当多的理论部分，但没有我希望在博士学位上完成的工作那么多。 我的目标是进入 ETH、UCL、斯坦福、NYU、EPFL、哥伦比亚和普林斯顿的实验室（按优先顺序，其中一个是我的职位）。所有这些实验室都有非常成功的 PI（按引用次数计算），他们研究的主题与我的兴趣非常一致。我担心我看似无所不包的研究背景可能会让他们失望，但我希望我的成绩能让他们相信我精通理论。我希望我的导师能写出优秀的推荐信，因为他们在很多场合都对我表示赞赏。我希望写一份令人信服的研究陈述，但由于我几周前才开始阅读相关文献，所以最终可能不是那么完美。 我不介意与年轻的 PI 合作，只要我身边有一些研究人员在研究相关主题。在高级实验室，已经建立了一个网络，我可能先协助一些项目，然后再进行独立研究。现实地说，我是不是在自吹自擂？如果是这样，有人可以推荐一些年轻的 PI 从事上述研究课题，我可能更有机会加入他们的实验室吗？    提交人    /u/mio_11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</guid>
      <pubDate>Sun, 13 Oct 2024 10:38:56 GMT</pubDate>
    </item>
    <item>
      <title>提出新颖的想法[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</link>
      <description><![CDATA[关于如何想出新颖的问题解决方案，您有什么想法吗？每次我以为自己有了一些想法，我的导师就会说“这太简单了”。许多方法以独特的方式将现有的构建块粘合在一起，但我很难想象人们如何想出既真正新颖又真正有效的东西。 有时，我读到一篇论文，我意识到这个想法实际上非常简单/直接，作者只是介绍了一个很酷的技巧。其他时候，我读到的东西介绍了一个非常晦涩的定理，或者他们注意到一些我只能梦想的东西。我倾向于前者，但由于新颖性有限，我对迄今为止所写的任何东西都不太自豪。疯狂的出版速度让我偏向“简单而有效”，这无济于事方法中的大部分工作是在获得 SOTA 后事后编写故事。    提交人    /u/like_a_tensor   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</guid>
      <pubDate>Sun, 13 Oct 2024 06:25:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 13 Oct 2024 02:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>