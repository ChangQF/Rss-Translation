<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 06 Mar 2024 00:56:34 GMT</lastBuildDate>
    <item>
      <title>[D] ICML 2024 支持主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7lnv0/d_icml_2024_support_thread/</link>
      <description><![CDATA[为提交至 ICML 2024 的每个人开设一个线程作为支持小组。审核将于 3 月 15 日发布（如果没有延迟）。 如果您收到任何评论，如果您特别讨厌一位评论者，或者喜欢另一位评论者，请告诉我们。一切皆有可能！   由   提交 /u/Adventurous-Cut-7077   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7lnv0/d_icml_2024_support_thread/</guid>
      <pubDate>Wed, 06 Mar 2024 00:32:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] Word 文档解析法学硕士 - 最佳方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7jore/r_llm_for_word_document_parsing_optimal_approach/</link>
      <description><![CDATA[希望从社区获得一些帮助！ 我想自动准确地识别文档和文本的标题包含在每个标题中。这些文档的结构各不相同，文档之间的标题甚至可能略有不同。总体目标是创建文档的高级结构，然后可以自动重新排列。 我认为法学硕士可能是实现这一目标的好方法，但想知道是否有人有具体的方法关于如何最好地解决这个问题的建议。关于 GPT4、GPT3.5 与 Anthropic 模型的任何想法。 提前感谢您的珍珠   由   提交/u/ross_prager  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7jore/r_llm_for_word_document_parsing_optimal_approach/</guid>
      <pubDate>Tue, 05 Mar 2024 23:07:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何获取按主题、主题、年龄适当性等分类的下一个标记预测的文本...？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7ine2/d_how_to_get_text_for_next_token_prediction_that/</link>
      <description><![CDATA[我正在微调下一个标记预测的语言模型，并且我需要获取适合我的受众的数据。例如，使用常见的爬网数据集之类的东西不适合小孩子。是否有一个网站可以获取根据主题、主题、年龄适当性等标准进行分类的数据集？   由   提交/u/lildaemon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7ine2/d_how_to_get_text_for_next_token_prediction_that/</guid>
      <pubDate>Tue, 05 Mar 2024 22:26:57 GMT</pubDate>
    </item>
    <item>
      <title>[P]关于项目和指导</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7gc34/p_about_projects_and_guidance/</link>
      <description><![CDATA[作为高级用户的 nlp 项目涵盖了大部分主题 [P] 我也想了解有关 nlp 的一切在这里总结一下你的东西，我一定会在谷歌或暗网上查看答案和解释   由   提交/u/Pure_Pension_8738   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7gc34/p_about_projects_and_guidance/</guid>
      <pubDate>Tue, 05 Mar 2024 20:56:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] NLP / 无监督学习 - 从列表中删除“相似”的想法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b7bhq0/r_nlp_unsupervised_learning_removing_similar/</link>
      <description><![CDATA[我是一名营销人员，正在为营销网站的博客创建内容日历。我使用 ChatGPT 使用 20 个不同子类别的种子关键字（例如“客户细分”）生成了多个潜在文章主题列表。和“行为电子邮件定位”。我现在有一个包含 985 个头脑风暴文章主题的数据库，其中包含这 20 个类别的描述。 但是，主题之间有很多重叠，而 ChatGPT 的内存不够大，无法处理所有 985 个主题以进行过滤排除或合并重复/相似的想法。例如，这是我的数据库中针对上面列出的子类别的两个主题/描述：“客户细分”和“客户细分”。和“行为电子邮件定位”：   子类别 主题描述    客户细分 基于行为触发器细分 根据特定的触发因素或操作（例如网站访问、电子邮件打开或购物车放弃）细分客户，以提供有针对性且及时的营销信息，引导客户通过销售渠道并推动转化。   行为电子邮件定位 行为触发器 设置行为触发器，根据特定操作或交互（例如网站访问、电子邮件打开或链接点击）发送有针对性的电子邮件。行为触发器使您能够在订阅者的客户旅程中的适当时刻向其提供相关内容和优惠，从而提高参与度并增加转化的可能性。   第二行的主题(“行为触发器”)是第一行的主题(“基于行为触发器的分段”)的缩小版本。我想找到一种方法来组合、聚合或以其他方式注意到这两个主题（以及我的数据库中的任何其他类似主题）之间存在相当大的重叠。 我不知道有多少我在数据库中有离散的主题。通过略读，我怀疑我拥有的 985 条记录之间有 200-400 个独特的主题。 因此，这似乎是一个无监督学习问题，但我从来没有研究过比超基础问题更多的东西。 ，入门级无监督学习问题，所以我有点不确定从哪里开始。 我希望得到任何和所有反馈，以节省我几个小时（甚至几天！）的时间。手动列出。 ########## 编辑：我搞砸了 2 个小时，并使用以下代码实现了我想要的结果： library(tidyverse) library(tm) # 读入数据 topic_r &lt;- read.csv(&#39;topics.csv&#39;, stringsAsFactors = FALSE) # 分配给新的 df 主题 &lt;- topic_r # 连接 3将多个字段合二为一，以合并更高级别的分类主题$Description &lt;-paste0(topics$Sub.Category, &#39; &#39;,topics$Topic,&#39;&#39;,topics$Description) # 预处理主题$Description &lt;- str_to_lower(topics$Description) ) topic$Description &lt;- str_replace_all(topics$Description, &quot;[:punct:]&quot;, &quot;&quot;) # 处理语料库 topic_df &lt;- data.frame(doc_id = topic$ID, text = topic$ Description) corpus &lt;- Corpus(DataframeSource(topics_df)) corpus &lt;- tm_map(corpus, removeWords, stopwords(“english”)) corpus &lt;- tm_map(corpus, stripWhitespace) # 创建文档术语矩阵 dtm &lt; - DocumentTermMatrix(corpus) # 删除稀疏项 dtm &lt;- removeSparseTerms(dtm, 0.99) # 执行 kmeans 聚类 # 注意：手动尝试不同的 K 值，直到结果通过眼睛测试 mat &lt;- as.matrix(dtm) K &lt;- 400 km.result &lt;- kmeans(mat, Centres = K) # 将主题集群添加回topics_r数据帧topics_r$KM.Cluster &lt;- km.result$cluster topic_r &lt;- topic_r %&gt;%排列( KM.Cluster, Sub.Category, Topic) # 使用指定的集群进行分割 topic_km &lt;- topic_r %&gt;% group_by(KM.Cluster) %&gt;% group_split()  &amp;# x200b;   由   提交/u/bprs07  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b7bhq0/r_nlp_unsupervised_learning_removing_similar/</guid>
      <pubDate>Tue, 05 Mar 2024 17:47:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2023年300+ML比赛分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b79bqq/r_analysis_of_300_ml_competitions_in_2023/</link>
      <description><![CDATA[      我运行 mlcontests.com，这是一个网站列出了跨多个平台的 ML 竞赛，包括 Kaggle/DrivenData/AIcrowd/CodaLab/Zindi/EvalAI/… 我刚刚完成了对 300 多个 ML 竞赛的详细分析2023 年，包括查看其中 65 个获奖解决方案。 一些亮点：  正如预期的那样，几乎所有获奖者都使用 Python 。一名获胜者使用 C++ 解决性能至关重要的优化问题，另一名获胜者使用 R 进行时间序列预测竞赛。 92% 的深度学习解决方案使用 PyTorch。我们发现剩下的 8% 使用 TensorFlow，并且所有这些都使用了更高级别的 Keras API。大约 20% 的获胜 PyTorch 解决方案使用 PyTorch Lightning。 基于 CNN 的模型比基于 Transformer 的模型赢得更多计算机视觉竞赛。 在 NLP 领域，毫不奇怪，生成式法学硕士开始被使用。一些竞赛获胜者使用它们来生成用于训练的合成数据，其他人则提出了创造性的解决方案，例如向开放权重法学硕士添加分类头并对其进行微调。还有更多专门针对 LLM 微调的竞赛正在推出。 与去年一样，梯度增强决策树库（LightGBM、XGBoost 和 CatBoost）仍然被广泛使用 由竞赛获胜者评选。 LightGBM 比其他两者稍微流行一些，但差异很小。 计算使用情况差异很大。 NVIDIA GPU 显然很常见；一些获奖者使用了 TPU；我们没有发现任何使用 AMD GPU 的获胜者；有些人仅在 CPU 上训练他们的模型（尤其是时间序列）。一些获奖者通过工作/大学获得了强大的（例如 8x A6000/8x V100）设置，一些获奖者在本地/个人硬件上进行了全面培训，相当多的获奖者使用了云计算。 有相当多的高- 2023 年的概况竞赛（我们详细介绍维苏威火山挑战赛和M6 预测），以及 2024 年即将举办的更多比赛（维苏威火山挑战赛第二阶段、AI 数学奥林匹克、AI 网络挑战赛） )  有关更多详细信息，请查看完整报告：https://mlcontests.com/state-of-competitive-machine-learning-2023?ref=mlc_reddit ​ 获奖者中最常用的一些 Python 软件包&lt; /p&gt; 在我的 r/MachineLearning 帖子中 去年关于 2022 年比赛的相同分析，热门评论之一询问了时间序列预测。 2023 年有几个有趣的时间序列预测竞赛，我设法对它们进行了相当深入的研究。跳至报告的此部分以了解这些内容。 （不同类型的时间序列竞赛的获胜方法有很大差异 - 包括 ARIMA 等统计方法、贝叶斯方法，以及 LightGBM 和深度学习等更现代的 ML 方法。） 我能够花费相当多的时间感谢今年报告的赞助商：Latitude.sh（配备专用 NVIDIA H100/A100/L40s GPU 的云计算提供商）和 Comet（有用的工具），我们花费了大量时间进行研究和撰写用于 ML - 实验跟踪、模型生产监控等）。我不会在这里向您发送垃圾邮件链接，报告底部有更多详细信息！   由   提交 /u/hcarlens   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b79bqq/r_analysis_of_300_ml_competitions_in_2023/</guid>
      <pubDate>Tue, 05 Mar 2024 16:22:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 注意力层复杂度 vs 上下文长度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b77fnc/d_attention_layer_complexity_vs_context_length/</link>
      <description><![CDATA[根据我对 Transformer 的了解，注意力层的计算复杂度与序列长度呈二次方关系。那么上下文长度怎么可能从 4096 (GPT-3) 到 128k (GPT-4) 再到 1M (Gemini 1.5) 呢？  我知道 GPT-4 和 Gemini 的确切架构并不为公众所知。但是是否有论文提出了一种在不增加计算复杂性的情况下增加上下文大小的方法？或者他们只是按照原始论文中的建议使用受限注意力层？   由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b77fnc/d_attention_layer_complexity_vs_context_length/</guid>
      <pubDate>Tue, 05 Mar 2024 15:08:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] David MacKay 谈随机位的昂贵</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b75loi/d_david_mackay_on_random_bits_being_expensive/</link>
      <description><![CDATA[所以我正在阅读《信息论、推理和学习算法》 （顺便说一下，对于没有听说过它的人来说，这是一本很棒的书）我偶然发现了这段话：  算术模型保证使用几乎尽可能少的随机位做出选择——在随机数昂贵的社区中这是一个重要的点！ [这不是一个玩笑。大量资金花费在软件和硬件中生成随机位。而且随机数很有价值。]  Ch 6.3，第 118 页  这本书出版于 2003 年。我可以想象随机数如何在互联网和现代计算时代出现之前，人们不得不扔硬币，获得这种技术的成本可能会很高，但我认为到 2000 年代初就不会出现这种情况，不是吗？他们没有“随机导入”功能吗？那时，或者他是在说随机数字是有价值的，而不是伪随机数字。如果是这样，它们至今仍然有价值/昂贵吗？因为我从来不需要购买“正品”。之前的随机数。   由   提交/u/new_name_who_dis_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b75loi/d_david_mackay_on_random_bits_being_expensive/</guid>
      <pubDate>Tue, 05 Mar 2024 13:48:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] - 对于推理和世界模型等术语，该领域是否有公认的定义？我看过很多关于这些主题的论文，但很多争论似乎都是语义学的。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b757ji/d_are_there_accepted_definitions_in_the_field_for/</link>
      <description><![CDATA[此类论文的一个例子是他们使用探针来查找黑白棋游戏的棋盘状态 https://arxiv.org/pdf/2210.13382.pdf 有很多论文使用了一些相当有声望的术语研究组织，但我没有看到任何可接受的定义。 例如，奥赛罗论文和 wes gurnee/max tegmark 论文基本上似乎依赖于这样的假设：“如果你可以从隐藏状态的线性（或足够简单）探测中重建外部状态，则该模型具有世界模型”。作为一个定义，这似乎是合理的，但我不确定这是否是公认的事情。   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b757ji/d_are_there_accepted_definitions_in_the_field_for/</guid>
      <pubDate>Tue, 05 Mar 2024 13:29:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人员检测的开放模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b73br3/d_open_model_for_person_detection/</link>
      <description><![CDATA[我正在做一个项目，我想使用来自 4 个视频流的边界框实时检测人员。视频流均捕获相同的场景/区域，但来自不同类型的摄像机：常规视频、低光摄像机和两个红外摄像机。  我应该如何进行这个项目？  我的想法是从一个开放模型开始，对人员检测进行预先训练（来自常规视频输入），然后扩展和修改模型以从其他摄像机获取输入。然后用我自己的数据继续训练。这样的模型是否存在？ 此任务使用什么样的模型？架构、输出等。我的输入是 4 个视频流，输出应该是检测到的人周围的 0 个、1 个或多个边界框。    由   提交/u/gi_beelzebub   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b73br3/d_open_model_for_person_detection/</guid>
      <pubDate>Tue, 05 Mar 2024 11:46:56 GMT</pubDate>
    </item>
    <item>
      <title>[N] Nvidia 禁止像 ZLUDA 这样的翻译层</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b70ubo/n_nvidia_bans_translation_layers_like_zluda/</link>
      <description><![CDATA[最近我在这个子论坛上看到了帖子，人们讨论了使用非 Nvidia GPU 进行机器学习。例如，ZLUDA 最近因在 AMD GPU 上启用 CUDA 应用程序而受到关注。现在 Nvidia 不喜欢这样，并禁止在 CUDA 11.6 及更高版本中使用转换层。 https://www.tomshardware.com/pc-components/gpus/nvidia-bans-using-translation-layers-for -cuda-software-to-run-on-other-chips-new-restriction-apparently-targets-zluda-and-some-chinese-gpu-makers#:\~:text=Nvidia%20has%20banned%20running%20CUDA ,system%20during%20the%20installation%20process。   由   提交/u/_d0s_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b70ubo/n_nvidia_bans_translation_layers_like_zluda/</guid>
      <pubDate>Tue, 05 Mar 2024 09:00:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习研究项目的项目管理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6zltt/d_project_management_for_ml_reseach_projects/</link>
      <description><![CDATA[最近，我的任务是开展一个基于 ML CV 的项目，该项目需要深入研究。这不是一个普遍解决的问题，所以我们必须做大量的研究和实验。不幸的是，这与我们的 Scrum Master/项目经理不太一致，因为他们习惯于软件工程项目，其中有预定义的里程碑、史诗、任务、估计等。这是一个巨大的问题，因为很难提前计划我们会做什么，因为我们需要大量探索。 研究机构如何管理和跟踪项目？有人致力于将研发项目与公司项目管理相适应吗？谁能提供我可以遵循的任何指导方针或参考资料，以便我可以提出一种项目经理可以遵循并评估我们进度的方法？   由   提交 /u/silently--here   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6zltt/d_project_management_for_ml_reseach_projects/</guid>
      <pubDate>Tue, 05 Mar 2024 07:34:01 GMT</pubDate>
    </item>
    <item>
      <title>[R]《Road to Sora》——论文阅读列表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6yb1x/r_road_to_sora_paper_reading_list/</link>
      <description><![CDATA[大家好， 周五我们一直在深入研究我们纸质俱乐部的 Sora 技术报告，并认为这会很好要获得背景论文的阅读列表，需要充分理解该技术报告中发生的所有内容 - 每篇文章都对其将用于的管道部分进行一些描述（或以前的最先进技术）评论中引用）。 我们将挑选一些顶级论文，并在接下来的周五将它们作为一个小组进行研究，所以如果您愿意，请加入我们！ Zoom 的时间为周五上午 10 点（太平洋标准时间）。 论文阅读列表： https://www.oxen.ai/blog/road-to-sora-reading-list 技术报告： https://openai.com/research/video- Generation-models-as-world-simulators  加入纸俱乐部： https://lu.ma/oxenbookclub   由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6yb1x/r_road_to_sora_paper_reading_list/</guid>
      <pubDate>Tue, 05 Mar 2024 06:11:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 是否可以在不进行大量计算的情况下写一篇论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b6tjil/r_is_it_possible_to_write_a_paper_without_large/</link>
      <description><![CDATA[我正在独立学习机器学习，希望将来能写一篇论文，但是我没有任何强大的计算能力我的桌面 GPU 是 2070 super。如果没有大量的计算，最近的突破似乎是无法实现的。那么这是否可能，或者是否有任何领域可以在没有昂贵 GPU 的情况下发表论文？   由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b6tjil/r_is_it_possible_to_write_a_paper_without_large/</guid>
      <pubDate>Tue, 05 Mar 2024 02:12:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>