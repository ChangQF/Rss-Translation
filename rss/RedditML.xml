<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 29 Oct 2024 15:18:13 GMT</lastBuildDate>
    <item>
      <title>[R] 寻求在 Arxiv 上为 CS.AI 提供认可</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gex2rn/r_looking_for_endorsement_on_arxiv_for_csai/</link>
      <description><![CDATA[您好，如果我违反了任何规则，我深感抱歉，我是一名独立研究人员，想要将论文上传到 cs.AI，但由于他们的认可政策，我无法这样做。它可以在我的 ProtonDrive（Google Drive 替代品）上下载 https://drive.proton.me/urls/M4NRJAEWZC#l6HDVPelfldq  我的认可代码是 EWLTQK https://arxiv.org/auth/endorse?x=EWLTQK 提前谢谢您。    提交人    /u/-boymoder   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gex2rn/r_looking_for_endorsement_on_arxiv_for_csai/</guid>
      <pubDate>Tue, 29 Oct 2024 15:16:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在贸易展览会上寻求引人入胜的 AI 展览创意 - 将展示最终产品！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gewvc7/p_seeking_ideas_for_a_captivating_ai_exhibit_at_a/</link>
      <description><![CDATA[嘿 r/MachineLearning！我正在为即将举行的贸易展准备一个 AI 展览，并且被赋予了很大的创作自由，让它真正引人入胜。我们的目标是设计一种互动体验，吸引人们，从 AI 爱好者到新手，并真正以引人入胜的方式展示 AI 的潜力。 如果您看到或建造了一个引人注目的 AI 展览，我很乐意听听！任何关于主题、设置或交互的想法，突出 AI 的实际应用或视觉吸引力，都将不胜感激。如果我们决定采纳这篇文章中的任何想法，我一定会在这里与大家分享最终的展示！ 提前感谢您的灵感和建议！   提交者    /u/Madgyver   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gewvc7/p_seeking_ideas_for_a_captivating_ai_exhibit_at_a/</guid>
      <pubDate>Tue, 29 Oct 2024 15:07:42 GMT</pubDate>
    </item>
    <item>
      <title>[D]“基于图的 VAE 存在问题。另外，我不是一个很好的程序员!!!”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gevt06/d_problem_with_graph_based_vae_ps_i_am_not_a_very/</link>
      <description><![CDATA[因此，我正在尝试生成一个基于图的变分自动编码器模型 (VAE)，使用我的蛋白质的较小轨迹作为输入（我已经在不同的随机种子下生成了我的蛋白质的多个小轨迹）。我的目标是从观察到的轨迹中查看潜在空间，并从较少探索的区域生成新结构，然后从这些区域开始 MD 模拟。 我已经使用蛋白质的 C 阿尔法原子作为输入，并根据两个 C 阿尔法原子之间的接触距离计算邻接矩阵，截止值为 8 埃。但是，我在模型的维数方面面临很多问题，例如我的蛋白质中有 97 个残基，对于测试轨迹有 2500 帧，并且按 80:20 分割，我有训练集（2000,97,97）和验证集（500,97,97）。但是当我尝试解码潜在点时，解码后的维度为 194,97。这让我很困惑。我附上了我正在使用的模型的架构。此外，在我的案例中获得的超参数是： 最佳超参数：{&#39;activation_fn&#39;: ReLU(), &#39;batch_size&#39;: 2, &#39;dropout_rate&#39;: 0.1, &#39;epochs&#39;: 50, &#39;hidden_​​dim&#39;: 16, &#39;latent_dim&#39;: 2, &#39;learning_rate&#39;: 0.001, &#39;num_layers&#39;: 2, &#39;optimizer_type&#39;: &#39;adam&#39;, &#39;weight_decay&#39;: 1e-05}  请检查它们并让我知道我哪里出错了。提前感谢。 GraphVAE( (gcn_layers): ModuleList( (0): GCNConv(97, 16) (1): GCNConv(16, 16) ) (fc_mu): Linear(in_features=16, out_features=2, bias=True) (fc_logvar): Linear(in_features=16, out_features=2, bias=True) (decoder_layers): ModuleList( (0): GCNConv(2, 16) (1): GCNConv(16, 16) ) (decoder_output): GCNConv(16, 97) (activation): ReLU() )     提交人    /u/ShazzieSlays08   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gevt06/d_problem_with_graph_based_vae_ps_i_am_not_a_very/</guid>
      <pubDate>Tue, 29 Oct 2024 14:21:33 GMT</pubDate>
    </item>
    <item>
      <title>[R]“如何训练你的 VAE”大大改善了标准 VAE 模型的报告结果 (ICIP 2024)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1get08n/r_how_to_train_your_vae_substantially_improves/</link>
      <description><![CDATA[      https://preview.redd.it/b1dmh67uroxd1.png?width=1025&amp;format=png&amp;auto=webp&amp;s=3d42a65e2c0a946aa307f01886aebedfc4b88b8e 所提出的方法使用混合高斯函数重新定义了后验概率的证据下限 (ELBO)，引入了正则化项以防止方差崩溃，并采用 PatchGAN 鉴别器来增强纹理真实感。这项工作的主要贡献是 ELBO，它减少了后部向前部的崩溃（观察为生成非常相似、模糊的图像） https://arxiv.org/abs/2309.13160 https://github.com/marianorivera/How2TrainUrVAE    提交人    /u/jarkkowork   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1get08n/r_how_to_train_your_vae_substantially_improves/</guid>
      <pubDate>Tue, 29 Oct 2024 12:08:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 探索 Whisper V3 Turbo 集成的无服务器解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1genbjo/d_exploring_serverless_solutions_for_whisper_v3/</link>
      <description><![CDATA[目前，Runpod 的无服务器解决方案在成本和功能方面满足了我的需求：https://github.com/runpod-workers/worker-faster_whisper 但是，我对使用https://huggingface.co/openai/whisper-large-v3-turbo感兴趣，因为它的速度很快。 我不确定如何在 Runpod 的无服务器基础设施上设置和运行 Whisper V3 Turbo。  看来我们可能需要等到上游项目 https://github.com/SYSTRAN/faster-whisper/issues/1030 使用 Turbo 更新并在 https://pypi.org/project/faster-whisper/ 上发布。  只有这样，此功能才可用，此时，我们可以分叉 https://github.com/runpod-workers/worker-faster_whisper 以进行相应更新。 与此同时，您是否知道使用 Whisper V3 Turbo 的任何经济高效的无服务器解决方案？ 谢谢。 p/s  Groq 提供此服务：https://groq.com/whisper-large-v3-turbo-now-available-on-groq-combining-speed-quality-for-speech-recognition/ 但是，他们目前不接受来自开发者的付款，也没有提供何时可用预计时间表。    提交人    /u/yccheok   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1genbjo/d_exploring_serverless_solutions_for_whisper_v3/</guid>
      <pubDate>Tue, 29 Oct 2024 05:26:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] SpotDiffusion：一种随时间推移生成无缝全景图的快速方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gekcus/r_spotdiffusion_a_fast_approach_for_seamless/</link>
      <description><![CDATA[我很高兴地宣布，我们的论文“SpotDiffusion：一种随时间推移生成无缝全景图的快速方法”已被 WACV2025 接受：https://arxiv.org/abs/2407.15507 项目页面：https://spotdiffusion.github.io 代码：https://github.com/stanifrolov/spotdiffusion 我们的方法会随时间推移移动不重叠的去噪窗口，确保一个时间步中的接缝在下一个时间步中得到纠正。这样可以用更少的总体步骤生成连贯的高分辨率图像。我们通过定性和定量评估证明了我们方法的有效性，并将其与 MultiDiffusion、SyncDiffusion 和 StitchDiffusion 进行了比较。我们的方法提供了几个关键优势，包括提高计算效率和加快推理时间，同时产生相当或更好的图像质量。    提交人    /u/Maleficent_Stay_7737   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gekcus/r_spotdiffusion_a_fast_approach_for_seamless/</guid>
      <pubDate>Tue, 29 Oct 2024 02:33:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动态注意力引导扩散用于图像超分辨率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/</link>
      <description><![CDATA[我很高兴地告诉大家，我们的论文“用于图像超分辨率的动态注意力引导扩散”被 WACV2025 接受了： https://arxiv.org/abs/2308.07977 这项工作的目标是引入一种新的注意力引导扩散机制，将图像细化重点放在从深度细化中受益最大的重要区域上：)    提交人    /u/Maleficent_Stay_7737   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/</guid>
      <pubDate>Tue, 29 Oct 2024 02:31:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] ML论文中可变长度输出的模型建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ged0by/r_model_suggestion_for_variablelength_output_in/</link>
      <description><![CDATA[大家好，我正在开始写论文，具备基本的 ML/DL 知识。我需要一个模型，它可以采用一组固定的输入（快照）并输出具有实数和复数值的可变长度向量。我读过 LSTM 可能有效，但我不确定给定固定输入。 有人推荐适合这种任务的模型或架构吗？任何关于从哪里开始或查看资源的建议都将非常有帮助。提前致谢！    提交人    /u/Less-Meaning-6450   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ged0by/r_model_suggestion_for_variablelength_output_in/</guid>
      <pubDate>Mon, 28 Oct 2024 20:58:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 超越自回归：离散扩散用于复杂推理和规划</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1geb685/r_beyond_autoregression_discrete_diffusion_for/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.14157 我很想听听专家对此的看法。 它与我认为有吸引力的想法有关：  自回归生成在组合领域（例如推理、规划、数学）中受到限制。 这解释了 LLM 在这些领域面临的许多挑战。 扩散在这些领域可能更有效：它学习从一般到具体生成。 （更像是基于能量的模型视角）。 在其生成过程的早期，它不太可能因为做出特定的错误选择而陷入困境。     提交人    /u/marojejian   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1geb685/r_beyond_autoregression_discrete_diffusion_for/</guid>
      <pubDate>Mon, 28 Oct 2024 19:42:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用数据流进行机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1geat4u/r_machine_learning_with_data_streams/</link>
      <description><![CDATA[我刚刚开始写论文，我需要学习使用数据流进行机器学习。我找到了一些文章、书籍和一些课程，但如果您能提供更多资源帮助我更好地理解这个主题，我将不胜感激。 非常感谢 :)    提交人    /u/Deepblue597   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1geat4u/r_machine_learning_with_data_streams/</guid>
      <pubDate>Mon, 28 Oct 2024 19:27:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何总结一篇研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ge8m3q/d_how_to_summarize_a_research_paper/</link>
      <description><![CDATA[我并不是论文阅读的新手，过去两年我一直在阅读论文，我甚至还时不时地实现一些论文，但我不能说我擅长总结它们。 总结论文时有什么一般技巧吗？是否有论文及其摘要的示例，以便我更好地理解论文总结是如何完成的？ 任何帮助都值得感激。    提交人    /u/darthJOYBOY   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ge8m3q/d_how_to_summarize_a_research_paper/</guid>
      <pubDate>Mon, 28 Oct 2024 17:58:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] 有人可以给我一些关于如何微调 SigLip 进行图像分类的提示吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ge5p0s/p_can_someone_give_me_tips_on_how_to_finetune/</link>
      <description><![CDATA[我正在使用 huggingface 的 Siglip，并尝试在我的数据集（不是那么大）上对其进行微调。它有点过度拟合，我研究了一些正常的正则化技术和 LORA，并得到了 peft 的帮助。我愿意接受所有建议。我对此很陌生。提前谢谢！！ 我从这个开始作为基础： model = AutoModel.from_pretrained(&quot;google/siglip-so400m-patch14-384&quot;) process = AutoProcessor.from_pretrained(&quot;google/siglip-so400m-patch14-384&quot;)     提交人    /u/Fantastic-Garlic19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ge5p0s/p_can_someone_give_me_tips_on_how_to_finetune/</guid>
      <pubDate>Mon, 28 Oct 2024 16:00:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 揭秘分布式检查点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gdkdka/d_demystifying_distributed_checkpointing/</link>
      <description><![CDATA[        提交人    /u/joygao   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gdkdka/d_demystifying_distributed_checkpointing/</guid>
      <pubDate>Sun, 27 Oct 2024 20:25:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 27 Oct 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>