<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 12 May 2024 18:18:57 GMT</lastBuildDate>
    <item>
      <title>[P] DARWIN - 开源 Devin 替代方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cqd2ss/p_darwin_opensourced_devin_alternative/</link>
      <description><![CDATA[🚀 隆重介绍 DARWIN - 开源人工智能软件工程师实习生！ 🤖DARWIN 是一位由您指挥的人工智能软件实习生。它具有帮助您构建和部署代码的功能。通过访问互联网，达尔文依靠更新的知识来编写代码并执行它们。如果万一遇到错误，DARWIN 会尝试通过访问讨论和论坛来解决它。还有什么更好的呢？它是开源的。 DARWIN 还能够训练机器学习模型并解决 GitHub 问题。观看我们的视频教程，见证 DARWIN 的功能实际应用： 📹 视频 1：了解 DARWIN 如何理解复杂的代码库、进行深入研究、集思广益创新想法以及熟练地用多种语言编写代码。观看此处：达尔文简介 📹视频2：观看DARWIN 在此处训练机器学习模型：Darwin ML 训练&lt; br /&gt; 📹 视频 3：查看 DARWIN 如何自行解决 GitHub 问题： Darwin 解决了 Github 问题 我们将 Darwin 作为一个开源项目启动。尽管您不能出于商业目的复制它，但您可以自由地将其用于个人用途和日常工作生活中。 访问达尔文 加入我们，我们将揭开达尔文的全部潜力。从管理变更和错误修复到使用不同数据集训练模型，DARWIN 将成为您在软件开发方面的最终合作伙伴。 分享您的反馈、想法和建议，以塑造人工智能在工程领域的未来。让我们使用 DARWIN 更智能、更快速、更创新地进行编码！ 请继续关注更多更新，不要忘记查看 DARWIN 自述文件以获取安装说明和关键功能的详细列表。   由   提交 /u/Curious-Swim1266    reddit.com/r/MachineLearning/comments/1cqd2ss/p_darwin_opensourced_devin_alternative/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cqd2ss/p_darwin_opensourced_devin_alternative/</guid>
      <pubDate>Sun, 12 May 2024 17:28:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我们离 GPT-4V 还有多远？通过开源套件缩小与商业多式联运模式的差距</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq8ufm/r_how_far_are_we_to_gpt4v_closing_the_gap_to/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.16821 代码：https://github .com/OpenGVLab/InternVL 模型：https://huggingface.co/ OpenGVLab 聊天演示：https://internvl.opengvlab.com/&lt; /a&gt; 拥抱脸部演示：https://huggingface.co/spaces /OpenGVLab/InternVL 摘要：  在本报告中，我们介绍InternVL 1.5，开源多模态大语言模型（MLLM），旨在弥合开源模型和专有商业模型在多模态理解方面的能力差距。我们介绍三个简单的改进：（1）强视觉编码器：我们为大规模视觉基础模型InternViT-6B探索了一种持续学习策略，提高了其视觉理解能力，并使其可以在不同的LLM中迁移和重用。 (2)动态高分辨率：根据输入图像的长宽比和分辨率，将图像划分为1到40个448×448像素的图块，最高支持4K分辨率输入。 （3）高质量的双语数据集：我们精心收集了高质量的双语数据集，涵盖常见场景、文档图像，并用英文和中文问答对对其进行注释，显着提高了 OCR 和中文相关任务的性能。我们通过一系列基准测试和比较研究来评估 InternVL 1.5。与开源和专有模型相比，InternVL 1.5 显示出具有竞争力的性能，在 18 个基准测试中的 8 个中取得了最先进的结果。代码已在此 https URL 发布。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq8ufm/r_how_far_are_we_to_gpt4v_closing_the_gap_to/</guid>
      <pubDate>Sun, 12 May 2024 14:16:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过通用李群预条件子实现曲率通知的 SGD</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq8guo/r_curvatureinformed_sgd_via_general_purpose/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.04553 代码（玩具实验）：https://github.com/lixilinx/psgd_torch 代码（大规模实验）：&lt; a href=&quot;https://github.com/opooladz/Preconditioned-Stochastic-Gradient-Descent&quot;&gt;https://github.com/opooladz/Precondition-Stochastic-Gradient-Descent 摘要：  我们提出了一种利用从 Hessian 向量乘积或参数和梯度的有限差分获得的曲率信息来加速随机梯度下降（SGD）的新方法，类似于BFGS算法。我们的方法涉及两个预处理器：无矩阵预处理器和低秩近似预处理器。我们使用对随机梯度噪声具有鲁棒性并且不需要线搜索或阻尼的标准在线更新两个预处理器。为了保持相应的对称性或不变性，我们的预处理器被限制为某些连接的李群。李群的等方差性质简化了预处理器拟合过程，而其不变性质消除了二阶优化器中通常需要的阻尼的需要。因此，参数更新的学习率和预处理器拟合的步长自然被归一化，并且它们的默认值在大多数情况下都适用。我们提出的方法为以低计算开销提高 SGD 的收敛性提供了一个有前途的方向。我们证明，在多种现代深度学习架构中，预处理 SGD (PSGD) 在视觉、NLP 和 RL 任务上的表现优于 SoTA。我们在本文中提供了用于复制玩具和大规模实验的代码。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq8guo/r_curvatureinformed_sgd_via_general_purpose/</guid>
      <pubDate>Sun, 12 May 2024 13:58:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] 最新主要开放式 LLM 版本一览：Mixtral、Llama 3、Phi-3 和 OpenELM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq6jgz/p_a_look_at_the_latest_major_open_llm_releases/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq6jgz/p_a_look_at_the_latest_major_open_llm_releases/</guid>
      <pubDate>Sun, 12 May 2024 12:15:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] unets如何实现空间一致性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq5g4r/d_how_do_unets_achieve_spatial_consistency/</link>
      <description><![CDATA[嗨，我在这里阅读了unet pytorch实现https://github.com/lucidrains/denoising-diffusion-pytorch 但我还不明白去噪过程中的像素如何“知道”其在图像中的（相对）位置。虽然噪声量是使用时间参数嵌入来调节每个像素的，但空间位置却没有这样做？ 因此，当从纯噪声开始对猫的图像进行去噪时，是什么使得unet在图像的顶部创建猫的头，在图像的底部创建猫的脚？或者去噪肖像，头发在上，脖子在下？ 我认为卷积核可能会在其影响范围内保持局部空间连贯性，但这感觉“不够”。  输入图像也没有被下采样到最里面的卷积核的大小。在引用的代码示例中，他们在底层将 a128x128 采样为 8x8。这又是 3 卷积，因此没有覆盖整个区域。 那么unet如何实现空间一致性/空间自动调节？ 谢谢   由   提交/u/Mr_Clueless_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq5g4r/d_how_do_unets_achieve_spatial_consistency/</guid>
      <pubDate>Sun, 12 May 2024 11:08:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 太阳风暴对 Llama3 8B 的 QLORA + RLHF 的影响？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq3uh4/d_impact_of_solar_storm_on_qlora_rlhf_of_llama3_8b/</link>
      <description><![CDATA[大家好， 在阅读一篇有关当前太阳风暴的文章时，我看到 NOAA 发出的关于太阳风暴影响的警告变压器上的风暴。 “可能会出现广泛的电压控制问题和保护系统问题，”美国国家海洋和大气管理局警告。 “一些电网系统可能会经历完全崩溃或停电。变压器可能会受到损坏。”  我目前正在 Llama3 8B 上进行 QLORA + RLHF 序列（我们正在尝试创建一个模型，通过提示创建更高效​​的 SQL 查询），我想知道这些影响是什么在 Llama3 8B 等型号上。你们中有人经历过伤害吗？对性能有何影响？   由   提交 /u/Standard_Natural1014   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq3uh4/d_impact_of_solar_storm_on_qlora_rlhf_of_llama3_8b/</guid>
      <pubDate>Sun, 12 May 2024 09:17:18 GMT</pubDate>
    </item>
    <item>
      <title>为单一目的集成多个人工智能——寻求研究和关键词 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq3qep/integrating_multiple_ais_for_single/</link>
      <description><![CDATA[我目前正在进行人工智能研究，很好奇是否有任何研究讨论为了单一目的而协作使用多个人工智能系统。例如，使用语言AI辅助语音识别AI更准确地确定特定单词对应的声音。我应该使用特定的关键字或短语来搜索该领域的研究吗？   由   提交/u/Grand_Path_6692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq3qep/integrating_multiple_ais_for_single/</guid>
      <pubDate>Sun, 12 May 2024 09:08:50 GMT</pubDate>
    </item>
    <item>
      <title>可以使用 KL 散度的倒数平方作为另一种散度度量吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cq3k96/can_one_use_squared_inverse_of_kl_divergence_as/</link>
      <description><![CDATA[我遇到了这个疑问（可能很愚蠢），但如果有人能对此做出一些说明，那就太好了： 两个分布 p 和 q 之间的 KL 散度定义为：$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$  根据 p 和 q 的顺序，散度是模式搜索或模式覆盖。 但是，可以使用 $\frac{-1}{D_{ KL}(p || q)}$ 作为散度度量？ 或者也许不是散度度量（严格来说），而是衡量两个分布之间的相似性/不相似性的东西？&lt; /p&gt; 编辑： 这绝对不是分歧，因为 -1/KL(p,q) &lt;= 0 也正如讨论中所指出的，1/KL(p,p) = +oo。 但是，我从这一点开始思考：如果 KL(p, q) 正在减少 =&gt; 1/KL(p,q) 正在增加 =&gt; -1 /KL(p,q) 正在减少。虽然，-1/KL(p,q) 从下方无界，因此可以到达 -oo。问题是，上述等价性是否使 -1/KL(p,q) 可用作任何应用程序的指标。或者在任何文献中的某个地方都考虑过它。   由   提交 /u/TanjiroKamado7270   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cq3k96/can_one_use_squared_inverse_of_kl_divergence_as/</guid>
      <pubDate>Sun, 12 May 2024 08:57:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2024年5月8日发表的法学硕士相关研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpu3lc/r_llms_related_research_papers_published_on_may/</link>
      <description><![CDATA[ 由   提交/u/dippatel21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpu3lc/r_llms_related_research_papers_published_on_may/</guid>
      <pubDate>Sat, 11 May 2024 23:23:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 尝试理解MaskCLIP中的某个功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpqe3h/r_trying_to_understand_a_certain_function_in/</link>
      <description><![CDATA[     &lt; td&gt; 你好， 所以我试图重新生成这篇论文：https://arxiv.org/pdf/2208.12262 但是，我卡在了某个我不理解的函数上。具体地，“量化器”是指下面分享的公式 6 中的h，： ​ https://preview.redd.it/ykotdlvjwuzc1.png?width=625&amp;format=png&amp;auto=webp&amp;s=b5d5d6a9e35 414ee65f1609a2508864719431c8a 首先：我不明白“软码字分布”是什么意思。方法。他们的意思是他们首先通过 softmax 传递输出特征吗？如果是这样，那么如果 h() 只是一个 softmax，那么为什么会有 EMA h()。 他们引用了 iBOT，因此它们可能意味着两件事： iBOT 头（其中只是 MLP 层）或 iBOT 损失中的中心/锐化 + softmax。如果他们的意思是前者，那么为什么他们有等式 5 中的解码器？只有学生的输出才会通过解码器，如图 1 中突出显示的那样。如果他们指的是 iBOT 损失中的居中/锐化 + softmax，那么为什么他们将量化器描述为“在线”？这意味着它是可训练的。 该代码不是公开的，我之前曾尝试联系作者讨论其他问题，但没有得到任何回复。 有什么想法吗？或想法将不胜感激！ ​   由   提交/u/MohammedSB   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpqe3h/r_trying_to_understand_a_certain_function_in/</guid>
      <pubDate>Sat, 11 May 2024 20:26:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] 开源库，用于为 API 托管的视觉语言模型抓取 PDF、YouTube、URL、演示文稿等</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpnlqe/p_open_source_library_to_scrape_pdfs_youtube_urls/</link>
      <description><![CDATA[这里我将分享一个开源库，您可以使用从文件、网页、YouTube 视频等中提取文本和视觉非结构化数据，立即将结果输入 API 托管的视觉语言模型（如 GPT-4-Vision 或 Gemini）。我制作了这个简单的工具，因为我无法使用其他提取框架（例如 unstructuraldio、langchain 提取器、文档布局分析模型等）获得视觉功能。 Cheers &amp;玩得开心！   由   提交 /u/Confident-Honeydew66    reddit.com/r/MachineLearning/comments/1cpnlqe/p_open_source_library_to_scrape_pdfs_youtube_urls/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpnlqe/p_open_source_library_to_scrape_pdfs_youtube_urls/</guid>
      <pubDate>Sat, 11 May 2024 18:15:55 GMT</pubDate>
    </item>
    <item>
      <title>[P] LoRA 从头开始​​实现 LLM 分类器训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpj6b9/p_lora_from_scratch_implementation_for_llm/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpj6b9/p_lora_from_scratch_implementation_for_llm/</guid>
      <pubDate>Sat, 11 May 2024 14:49:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 处理参考文献中冲突的训练配置。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpi9gc/d_dealing_with_conflicting_training/</link>
      <description><![CDATA[我正在研究对象检测的主动学习，目前我正处于需要设置训练配置来运行实验的阶段。我不打算重新运行其他作品的实验，因为我没有计算能力，也没有时间。但我仍然会将我的结果与他们的结果进行比较，为此我必须遵循这些作品中使用的训练配置。 问题是不同的论文报告了不同的配置，尽管他们正在将其结果与彼此。通常与其他方法进行比较的论文是 MI-AOD - CVPR21 论文，因为它是第一个 AL 方法CVPR 中的物体检测。对于 RetinaNet，他们训练了 26 个 epoch，LR 为 0.001，在第 20 个 epoch 时步进为 0.1。 然后是CVPR22论文使用标准 1x 计划进行 RetinaNet 训练（12 个时期，0.02 LR，以及第 8 和 11 时期的步骤）。然而，他们将结果与 MI-AOD 论文进行比较，似乎他们并没有使用自己的设置重新运行实验，因为 mAP 看起来与原始报告中的结果完全相同。我只能通过外观来判断，因为它们只将比较显示为每个 AL 周期中的 mAP 图，而没有写下表中的值。他们也没有发布代码。 那么你就有 PPAL - CVPR24 声称使用与 MI-AOD 相同的配置，但在他们的代码中他们使用 0.002 的 LR，而不是他们在论文中声称的 0.001。他们还将结果与最后两个进行了比较，尽管配置不同，而且他们似乎也没有在这里重新运行实验（同样只有图，没有表格）。 外面还有其他一些作品CVPR的，他们通常倾向于遵循MI-AOD设置。 我的问题是，由于以上三个都在CVPR中，所以我至少需要将我的方法与他们的方法进行比较，但是如何我决定使用什么配置？我是否只需遵循他们论文中报告的最新 CVPR 并使用他们报告的结果与之前的作品进行比较？   由   提交 /u/notEVOLVED   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpi9gc/d_dealing_with_conflicting_training/</guid>
      <pubDate>Sat, 11 May 2024 14:05:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] Marcus Hutter 在通用人工智能方面的工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cpcwuz/r_marcus_hutters_work_on_universal_artificial/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cpcwuz/r_marcus_hutters_work_on_universal_artificial/</guid>
      <pubDate>Sat, 11 May 2024 08:38:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>