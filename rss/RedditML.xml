<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 11 Nov 2024 03:21:27 GMT</lastBuildDate>
    <item>
      <title>[R] 结合归纳和传导进行抽象推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1goh5ym/r_combining_induction_and_transduction_for/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1goh5ym/r_combining_induction_and_transduction_for/</guid>
      <pubDate>Mon, 11 Nov 2024 01:58:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于神经网络的“自回归”或逆协方差矩阵</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/</link>
      <description><![CDATA[我想知道神经网络是否已用于这种自回归问题。因此，不要使用线性回归类型的框架，而是使用非线性神经网络。  具体问题参考 https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references    提交人    /u/Sandy_dude   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/</guid>
      <pubDate>Sun, 10 Nov 2024 22:19:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] / [D] 您最近最喜欢的 LLM 或基于扩散模型的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/</link>
      <description><![CDATA[大家好， 作为竞赛的一部分，我正在尝试寻找一篇有趣的论文，作为我研究小组会议的演讲。我对语言模型和计算机视觉生成 AI 的进步很感兴趣，特别是使用扩散模型。 我想问一下，您目前最喜欢与这些领域相关的哪些论文，以及您为什么喜欢它们。我喜欢那些思维方式相当简单但创新性很强的论文，这些论文可以为研究增添很多价值。请提供您的想法/链接，我非常感谢您的所有意见。谢谢！！    提交人    /u/Tough-Statement9740   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/</guid>
      <pubDate>Sun, 10 Nov 2024 19:32:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于存储大数据集的外部 SSD</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/</link>
      <description><![CDATA[嗨，reddit 用户们！第一次在这里发帖。 经过一番研究，我想问一下，如果有人遇到过这种情况并找到了好的解决方案，您认为哪种 SSD 最适合移动大量数据（用于深度学习）。 我正在大学攻读博士学位，购买东西的过程非常缓慢，而且由于我现在需要大量空间，所以我正在考虑购买一个大容量 SSD 来存储数据集和已经训练好的模型以备将来使用。 我购买了 KingstonXS2000，应该可以达到 ~2Gbps，但测试时，如果它能达到 500Mbs 的标记几分钟，然后快速下降，那就算幸运了。 我知道 USB 3.2x2 端口和设备的发热问题，但是，即使如此，在查看了网络和评论后，很多人显示我检查的 SSD 在大多数情况下接近 1Gbps，即使使用 USB 3.2 和雷电端口也是如此。 因此，任何建议或良好的经验设备分享都将不胜感激。 TL;DR：使用 DL，需要一个可靠的外部 2TB SSD，具有真正高速的长期读写操作。    提交人    /u/GankoX22   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/</guid>
      <pubDate>Sun, 10 Nov 2024 18:27:19 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] ML 和 DL 模型中存在伪造新型方法的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/</link>
      <description><![CDATA[为什么很多新论文（通常由博士完成）都采用现有方法，而当您询问他们的贡献时，他们说我们用另一层替换这一层，或者我们添加了超参数!!!!! 这不是贡献！我很困惑这些怎么会被接受    提交人    /u/Rihab_Mira   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/</guid>
      <pubDate>Sun, 10 Nov 2024 16:53:17 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 寻找关于电影数据集的有趣研究（无生成模型）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/</link>
      <description><![CDATA[嘿，ML 研究人员！ 我一直在深入研究多模态学习，我特别感兴趣的是以创造性方式利用电影/视频数据集的论文。我不是在寻找与视频生成或传播相关的论文，而是在寻找有趣的方法：  从电影中进行多模态表示学习 结合视频、音频和文本模态的新型融合技术 从电影数据中进行场景理解/上下文学习 角色互动分析 跨模态的情感/情绪分析 使用电影数据进行跨模态检索  很想听听你在这个领域遇到的任何精彩论文！    提交人    /u/stoneddumbledore   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/</guid>
      <pubDate>Sun, 10 Nov 2024 14:31:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于每个作业中的多行数据，使用 PCA 进行降维的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/</link>
      <description><![CDATA[您好！我正在处理一个数据集，其中包含 300 个作业，每个作业都有一个目标标签。对于每个作业，我有大约 1000 个数据点（行），每个数据点由具有各种参数的 17 维向量表示。 我想将每个作业的这 1000 行减少为单个代表性向量，以用于模型训练。但是，我想避免仅使用每列的平均值和方差，因为我认为这会丢失太多信息。 使用 PCA 是一种好方法吗？如果是这样，我可以使用第一个主成分 (PCA1) 及其相关方差来形成单个代表性向量吗？例如，将每个 17D 向量投影到 PCA1 上，然后对这些投影取加权平均值（由 PCA1 的解释方差加权）是否会为每个作业产生一个好的单个向量？ 非常感谢，祝您周末愉快。    提交人    /u/aaronhallam773   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/</guid>
      <pubDate>Sun, 10 Nov 2024 11:49:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 建立了一个路线图网站，25 天内获得了 450 名用户，我感到非常高兴！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/</link>
      <description><![CDATA[      大家好，我是一名三年级 cse 学生。上个月，我建立了一个名为 https://www.mldl.study/ 的网站。该网站面向任何“刚”接触机器学习和深度学习且不知道从哪里开始的人。我建立这个网站是因为我也对此感到困惑。它有适当的视频讲座、文章、研究论文、可视化、kaggle 竞赛以及基本上掌握 ml 和 dl 所需的一切。 我 25 天前刚刚添加了谷歌分析，我发现我有 450 个用户和 135 个回访用户。我建立这个网站只是为了帮助我的大学朋友，但我很高兴它也能帮助其他人。我只是想分享这个，因为我对此很高兴。这让我有信心，我将来可以构建更酷、更有用的东西。 谢谢大家。我的分析能力从这里得到了一点推动。谢谢！！ （我也愿意接受建议和所有我可以做的事情来进一步发展它） https://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;format=png&amp;auto=webp&amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305    提交人    /u/Grouchy-Breakfast-20   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/</guid>
      <pubDate>Sun, 10 Nov 2024 08:48:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] AAAI 第 2 阶段反驳回复和 Openreview 中的审稿人更新</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/</link>
      <description><![CDATA[我想知道我们是否可以在审阅者在 OpenReview 中提交新回复和更新后的评分时查看它们，或者我们是否需要等到 12 月 9 日。此外，在反驳期间，所有审阅者是否都可以看到我们提交给其他审阅者的回复，还是每个审阅者只能查看针对他们的回复？    提交者    /u/morphinejunkie   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/</guid>
      <pubDate>Sun, 10 Nov 2024 08:13:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 经典 GNN（GCN、GraphSAGE、GAT）是节点分类的强大基线</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/</link>
      <description><![CDATA[我们很高兴与大家分享我们最近的论文“[NeurIPS 2024] 经典 GNN 是强大的基线：重新评估用于节点分类的 GNN”。 在本研究中，我们对用于节点分类任务的经典 GNN 进行了彻底的审查。我们的研究结果表明，最先进的图学习模型经常报告的卓越性能可能是由于经典 GNN 中的超参数配置不理想。通过对这些超参数进行微调，我们表明，在 18 个广泛使用的节点分类数据集中，经典 GNN 在 17 个上的表现优于最新模型。 Arxiv：https://arxiv.org/abs/2406.08993 代码：https://github.com/LUOyk1999/tunedGNN 如果您发现我们的工作很有趣，我们将非常感谢您在 GitHub 上留下 ⭐️！    提交人    /u/luoyuankai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/</guid>
      <pubDate>Sun, 10 Nov 2024 04:32:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于“反向”嵌入（即将向量/张量嵌入到文本、图像等）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/</link>
      <description><![CDATA[编辑：我并不是指解码器本身，我忘记澄清这一点，这是我的错。我的意思是指一个（更）直接的计算或数学框架，它不涉及训练另一个网络来进行反向嵌入。  正如标题所暗示的，是否有可能正在研究进行反向嵌入的方法和/或过程？从我昨天进行的初步互联网侦查来看，这似乎基本上是不可能的，因为逆映射将如何发挥作用。在这方面，使用我们现有的硬件和设置几乎不可能实现。 但是，也许你们中的一些人可能知道一些可能已经进入这个方向的文献，即使是在理论或初级层面，如果您能指出这些资源，我将不胜感激。也欢迎您分享您的想法和理论。 从反向嵌入扩展，是否可以超越嵌入向量/张量的范围，从而反向嵌入所述嵌入向量/张量，然后从中检索结果文本，图像等？ 提前谢谢您！    提交人    /u/YsrYsl   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/</guid>
      <pubDate>Sun, 10 Nov 2024 03:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对数概率与信息论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/</link>
      <description><![CDATA[在机器学习中，我们大量使用对数概率，试图最大化对数概率。从数字角度来看，这是有道理的，因为加法比乘法更容易，但我也想知道“对数概率”背后是否有根本含义。 例如，对数概率在信息论中被广泛使用，是“信息”的负数。我们能从信息论的角度来看待最小化负对数似然吗？它是最大化/最小化某些信息指标吗？    提交人    /u/masonw32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/</guid>
      <pubDate>Sun, 10 Nov 2024 03:37:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 10 Nov 2024 03:15:11 GMT</pubDate>
    </item>
    <item>
      <title>[N] ARC 奖项为解决网格上彩色方块组成的谜题的少样本学习提供 60 万美元奖金。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnnstd/n_the_arc_prize_offers_600000_for_fewshot/</link>
      <description><![CDATA[        提交人    /u/moschles   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnnstd/n_the_arc_prize_offers_600000_for_fewshot/</guid>
      <pubDate>Sun, 10 Nov 2024 00:08:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>