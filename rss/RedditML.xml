<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 08 Dec 2023 01:01:17 GMT</lastBuildDate>
    <item>
      <title>[D] 无需中间转换的快速张量序列化的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dan2i/d_recommendations_for_fast_tensor_serialization/</link>
      <description><![CDATA[大家好，我目前正在开发一个项目，需要有效地序列化张量。现在，我正在使用 Python 的 pickle 库，但我正在寻找可能更快、更高效的东西，特别是对于大张量。我的主要要求是直接序列化张量，无需任何中间转换（例如通过 NumPy 数组）。 是否有人有经验或可以推荐一个比 pickle 更快的库来实现此目的？我主要处理来自 PyTorch 等库的张量。 我正在考虑的一些要点：  序列化的速度和效率。 兼容性使用 PyTorch 张量。 在序列化之前无需中间转换为其他格式（如 NumPy）。  任何见解、经验或建议将不胜感激！ 谢谢！   由   提交 /u/darkNightCoder   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dan2i/d_recommendations_for_fast_tensor_serialization/</guid>
      <pubDate>Fri, 08 Dec 2023 00:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于在线语言模型交互的压缩上下文记忆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dacnx/r_compressed_context_memory_for_online_language/</link>
      <description><![CDATA[      交互式演示示例 arXiv：https:// arxiv.org/abs/2312.03414  GitHub：https ://github.com/snu-mllab/Context-Memory  项目页面：https://janghyun1230.github.io/memory/  摘要：  我们的方法在 LLM 交互期间动态创建上下文的压缩内存。 我们的方法只需要训练用于压缩的条件 LoRA。 我们使用针对循环压缩程序的完全并行训练策略。 我们对多种应用进行评估：对话、多任务 ICL 和个性化，实现了完整上下文模型的性能水平缩小 5 倍上下文内存空间。  https://preview.redd.it/5vmwxa7luy4c1.png?width=2162&amp;format=png&amp;auto=webp&amp;s=0f717a1528688b6fd9b47f3c7 e43be66b1b5c78b   由   提交/u/janghyun1230   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dacnx/r_compressed_context_memory_for_online_language/</guid>
      <pubDate>Fri, 08 Dec 2023 00:43:50 GMT</pubDate>
    </item>
    <item>
      <title>[D]机器学习课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18d9rav/d_machine_learning_curriculum/</link>
      <description><![CDATA[大家好。我正在尝试创建一个简单明了的机器学习课程。欢迎任何意见或贡献。目标是使其与主要课程保持一致，同时在单独的部分中添加其他资源。 ​ 链接：https://github.com/pytholic/Machine-Learning-Curriculum  &amp;# 32；由   提交/u/rajahaseeb147   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18d9rav/d_machine_learning_curriculum/</guid>
      <pubDate>Fri, 08 Dec 2023 00:13:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对曼巴的看法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on_mamba/</link>
      <description><![CDATA[   我运行了 Karparthy 的 NanoGPT，用 Mamba 在他的 TinyShakespeare 数据集上，5 分钟内它开始吐出以下内容： ​ https://preview.redd.it/4r96tp6lxx4c1.png ?width=836&amp;format=png&amp;auto=webp&amp;s=10f2f61cd4cea96f4f903cb2070835fc5d1df951 ​ https://preview.redd.it/32ler5vnxx4c1.png?width=622&amp;format=png&amp;auto=webp&amp; s=dd00e53f43dd0afa058758a987901ee6789d2258 ​ https://preview.redd.it/sc96i4xoxx4c1.png?width=678&amp;format=png&amp;auto=webp&amp;s=94d2ed279054363d3ed2b6beed65be894 68582b0  比 self-attention 快得多，也更流畅，以每秒 6 个 epoch 的速度运行。老实说，我惊呆了。 https://colab.research.google.com /drive/1g9qpeVcFa0ca0cnhmqusO4RZtQdh9umY?usp=sharing ​   一些损失图： 无截断的多头注意力（x为10秒迭代，y为损失） 带截断的多头注意力（x 是 10 秒内的迭代，y 是是损失）  Mamba 损失图（x 为 10 秒迭代次数，y 为损失） ​ ​ &lt; !-- SC_ON --&gt;  由   提交 /u/ExaminationNo8522   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on_mamba/</guid>
      <pubDate>Thu, 07 Dec 2023 21:29:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] flex-prompt：一个灵活的提示渲染引擎，确保您永远不会再次超过 LLM 的上下文长度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18d581q/p_flexprompt_a_flexible_prompt_rendering_engine/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18d581q/p_flexprompt_a_flexible_prompt_rendering_engine/</guid>
      <pubDate>Thu, 07 Dec 2023 20:49:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有一个工具可以指示输入提示的哪些部分对 LLM 的输出影响最大？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18d4rba/d_is_there_a_tool_that_indicates_which_parts_of/</link>
      <description><![CDATA[嗨， 是否有工具可以指示输入提示的哪些部分对 LLM 的输出影响最大？ 我不在乎这个工具是针对哪个LLM的（如果它存在的话）。 我猜它可以通过神经网络中每个节点的权重进行回溯，但是你们比我聪明，所以我会听你们的。 我的用例是我有一个提示，可以稍微改变变体。模型的输出是“是”。或“否”，所以我想看看我更改的提示部分会影响其响应 最佳， Reddit 用户   由   提交 /u/ToughOpening   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18d4rba/d_is_there_a_tool_that_indicates_which_parts_of/</guid>
      <pubDate>Thu, 07 Dec 2023 20:29:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 考虑从 SWE 切换到 DS/ML。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18d3be9/d_considering_switching_to_dsml_from_swe/</link>
      <description><![CDATA[数据科学/机器学习的日常情况如何？我正在考虑改变职业，因为我真的不喜欢软件工程的某些方面。我进行了 4 个月的 ML 实习，但我觉得我并没有完全掌握日常工作。我知道这可能因公司和团队而异。 就某些背景而言，我拥有约 4 年的 SWE 经验，拥有前 50 名学校的计算机科学学士学位和顶尖的计算机科学硕士学位10 所学校。   由   提交 /u/EquivalentAbies6095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18d3be9/d_considering_switching_to_dsml_from_swe/</guid>
      <pubDate>Thu, 07 Dec 2023 19:26:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 本科生正在考虑攻读机器学习博士学位，但担心这真正意味着什么。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18czqtg/d_undergrad_contemplating_a_machine_learning_phd/</link>
      <description><![CDATA[我是 T5 学校的三年级学生，从高中开始我就一直在做研究。我已经发表了一些出版物，我的教授（在另一家 T3 机构）预计我目前的工作将使我获得《自然医学》（或类似的顶级期刊）的第一作者身份。这一切都已应用于 ML/DL 工作，特别是在降低医疗保健成本方面。我一直保持着较高的 GPA，并且一直在努力规划明年的计划，以便为攻读博士学位做好充分准备。 理论上，我真正想做的事情是找出下一代学习算法，特别是那些受大脑原理驱动的算法。我发现这非常令人兴奋，大一的时候我会 5 点起床，花一天的大部分时间学习这些东西并实现我的想法，跳过所有课程，勉强达到 A 的水平。这没什么用（没有什么真的那么新奇，我只是不断地重新发明旧技术），所以到了第二年，我全力以赴地从事应用工作，这样我就可以积累出版物。因此，在这一点上，老实说，我与我最喜欢的工作领域有点脱节。 我对特定研究领域的痴迷让我认为我想要的是博士学位。就我个人而言，说实话，我一般不喜欢应用研究的过程。当然，其中大部分只是处理数据并应用机器学习中的现有想法（也许到处都有一些很酷的洞察力）。虽然我认为这确实重要且有意义，但我绝对不以同样的方式享受它。我没有感受到构思的兴奋，而且我不确定我是否愿意尝试解决工业中的此类问题。 所以我对研究的兴趣已经有些具体了。我的最终目标是继续进行研究，无论是在大型科技公司，我可以在人工智能前沿进行新的创新，还是在大学。实现这一目标的最痛苦的可行途径就是我想要的，而博士学位似乎是我需要达到的目标。 不过，我担心一些事情： &lt; ol&gt; 一旦事情开始偏离我的核心兴趣太远，当我忍着并仍然努力工作时，我就不再感受到做研究的同样纯粹的乐趣。结果很酷，也很有意义，但过程就不那么有趣了。我觉得我更像是在做机器学习工程而不是研究。还不如直接攻读工业或硕士学位？ 博士学位需要 6-7 年。我知道这听起来有点傻（就像，呃 - 这就是博士学位！它需要时间，而且一切都是一场赌博），但这确实让我感到害怕。对未来后悔的恐惧，我担心我会因为发表的文章不够多而灭亡（或者被迫把所有时间花在我并不真正喜欢的研究上），或者只是普遍担心我不会被排除在利用的机会之外。博士学位所带来的机会，最终只是在同一个地方/比我直接进入行业更糟糕。  我对 SDE/MLE/DS 角色有复杂的感觉。大科技。我曾在传统的大型科技公司实习过，老实说，工作有点冷淡，影响大，需要更少的努力，工作与生活的平衡并非不存在，而且问题有些有趣，这使得它成为一种整体上令人愉快的经历。但当我长远考虑我的生活时，我想创作出一些对我来说真正有意义的东西。帮助对基本思想做出（无论多么微小）微小的贡献是“有趣”和“快乐”的交叉点。和“有意义的”对我来说，这就是为什么它是我的首选。其次是医疗保健方面的应用工作，第三是太空探索方面的应用工作。不确定博士学位是否适合这两个人。 无论如何，这有点像一个信息转储，我通常只是希望从比我更有经验和实践智慧的人那里得到一些建议做。感谢您花时间阅读本文。   由   提交/u/FM-2070  /u/FM-2070  reddit.com/r/MachineLearning/comments/18czqtg/d_undergrad_contemplatating_a_machine_learning_phd/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18czqtg/d_undergrad_contemplating_a_machine_learning_phd/</guid>
      <pubDate>Thu, 07 Dec 2023 16:46:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在解码器模型中，如果后来的令牌关注早期令牌，但早期令牌不关注后来的令牌，那么什么阻止早期令牌的影响随着每一层的增长而增长？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cwkwp/d_in_decoder_models_if_later_tokens_attend_to/</link>
      <description><![CDATA[让我们想象只有两个 token，A 和 B。 在每个注意力层中，B 将关注自身和 A，但是A永远只顾自己。所以一遍又一遍B会变成它自己和A的加权和，然后它们都会经过FF层，然后重复这个过程。所以“百分比”不应该是“百分比”吗？ B 中来自 A 的信息随着每一层的增长而增长？我在这里错过了一些基本的东西吗？模型是否必须学习使 B 的 Q 和 A 的 K 与每一层（平均）更加正交，以防止 B 过多关注A 在所有层的过程中？ ​ 编辑：要清楚，我完全明白这是从模型中训练出来的，但我不知道得到就是这样。模型是否倾向于只关注早期代币的几层？ FF 层是否试图扭转早期代币信息过饱和的影响？等等   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cwkwp/d_in_decoder_models_if_later_tokens_attend_to/</guid>
      <pubDate>Thu, 07 Dec 2023 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型机器学习模型的半二次量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cvse5/r_halfquadratic_quantization_of_large_machine/</link>
      <description><![CDATA[分享我们在模型量化方面的工作。  博客：https://mobiusml.github.io/hqq_blog/ 代码： https://github.com/mobiusml/hqq 模型：https://huggingface.co/mobiuslabsgmbh/  无需数据校准，速度极快🚀，适用于语言和视觉模型！ 为什么重要？ 量化显着降低了 GPU 内存需求，但会降低模型的质量。拥有更快、更准确的量化方法对于机器学习社区来说非常有价值。 方法：原始权重与其反量化版本之间基于稀疏性的误差公式。我们使用半二次求解器通过 Pytorch 的 Autograd 导出比反向传播快 100 倍的封闭式解决方案。 量化速度： Llama2 约 1 分钟-13B ~ LLama2-70B 4 分钟（比 GPTQ 快 50 倍以上） 发现： - 较大的模型量化为 3/2 位优于具有相似或更低内存要求的较小全精度模型。 - 成功的 2 位量化需要较低的组大小（例如 32 或 16）以及零点和缩放因子的压缩以降低内存使用情况。  虽然我们承认我们的观点可能略有偏见，但我们真诚地相信我们的工作将使开源软件 (OSS) 机器学习社区受益匪浅。代码和模型均在 Apache 宽松许可证中。    由   提交/u/sightio  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cvse5/r_halfquadratic_quantization_of_large_machine/</guid>
      <pubDate>Thu, 07 Dec 2023 13:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在 Tsetlin Machine Book 第 4 章：卷积中学习如何使用可解释的规则执行逻辑卷积！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ctnm0/p_learn_how_to_perform_logical_convolution_with/</link>
      <description><![CDATA[   ​ 基于规则的卷积分步 嘿！我的书的另一章完成了。希望你喜欢！ https://tsetlinmachine.org 摘要：在时间和空间中搜索模式使得您在本章中学习的模式识别任务成为可能1.更具挑战性。例如，也许您希望 Tsetlin 机器识别图像内的较小物体。在 Tsetlin 机器能够了解它们的外观之前，它必须先找到它们。但如果不知道它们的外貌，又如何才能找到它们呢？在本章中，您将了解 Tsetlin 机器如何使用规则卷积来解决此双重任务。 在第 4.1 节中，您将研究健康和图像分析中的两个说明性任务。它们抓住了问题的双重性质，以及为什么需要同时执行定位、识别和学习。 然后，您将在第 4.2 节中学习如何将图像划分为多个块。这种划分允许 Tsetlin 机器一次专注于一个图像片段，从而提供一种引导注意力的方法。 多个图像片段需要一种新的方法来评估和学习规则。当每个输入图像变成多个部分时，您需要一种策略来选择关注哪些部分以及忽略哪些部分。我们在第 4.3 节中介绍了规则评估的新形式，而第 4.4 节则讨论了学习。 最后，第 4.5 节介绍了如何使用图像内补丁的位置来创建更精确的规则。目的是缩小与相关图像区域的模式匹配范围。 阅读本章后，您将了解如何构建能够识别时间和空间模式的卷积 Tsetlin 机。 &lt; /div&gt;  由   提交 /u/olegranmo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ctnm0/p_learn_how_to_perform_logical_convolution_with/</guid>
      <pubDate>Thu, 07 Dec 2023 11:31:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 第一份 ICLR 提交材料。被拒绝了。总体来说很棒的体验。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18cl7r0/d_first_iclr_submission_got_rejected_great/</link>
      <description><![CDATA[我预计我的论文会被拒绝，但我还是尝试了一下。审稿人非常有帮助；尽管出现了所有“低质量评论”的情况，但我还是收到了相当高质量的建设性批评。  这是我的论文：Guided Sketch-Based Program Induction by Search Gradients | OpenReview   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18cl7r0/d_first_iclr_submission_got_rejected_great/</guid>
      <pubDate>Thu, 07 Dec 2023 02:31:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] Mamba-Chat：基于状态空间模型的聊天法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ckntr/p_mambachat_a_chat_llm_based_on_state_space_models/</link>
      <description><![CDATA[嘿！ 您可能已经看过这篇论文最近几天的 Mamba 论文，这是首次尝试将状态空间模型扩展到 2.8B 参数以处理语言数据。 与 Transformer 相反，这这种架构的计算复杂度不会随输入长度呈二次方扩展，因此从长远来看，如果它能够取代 Transformer，那就太棒了。 我们对这篇论文和发布的模型感到非常兴奋，但不幸的是，没有训练代码提供了它，所以我们决定自己编写它并训练模型。因此，我们刚刚发布了 mamba-chat，这可能是现有的最好的不依赖于 Transformer 的 LLM。老实说，我对该模型的表现感到非常惊讶，因为它只有 2.8B 个参数，并且基本模型仅在 Pile 上进行训练。想到这些模型是否会在某个时候取代 Transformer 真是令人兴奋。 请随意查看我们的 Github 或 Huggingface 存储库！我们的 Github 存储库包含一个 cli 聊天脚本，因此如果您有权访问 GPU，则可以轻松运行模型。   由   提交 /u/pip-install-torch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ckntr/p_mambachat_a_chat_llm_based_on_state_space_models/</guid>
      <pubDate>Thu, 07 Dec 2023 02:03:39 GMT</pubDate>
    </item>
    <item>
      <title>[R]谷歌发布Gemini系列前沿机型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18c6xio/r_google_releases_the_gemini_family_of_frontier/</link>
      <description><![CDATA[来自 Jeff Dean 的推文：https://twitter。 com/JeffDean/status/1732415515673727286 博客文章：https:// blog.google/technology/ai/google-gemini-ai/ 技术报告：https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf 有什么想法吗？没有太多“肉”在这个公告中！他们肯定担心其他实验室+开源从中学习。   由   提交/u/blabboy  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18c6xio/r_google_releases_the_gemini_family_of_frontier/</guid>
      <pubDate>Wed, 06 Dec 2023 15:52:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>