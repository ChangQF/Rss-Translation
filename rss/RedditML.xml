<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 09 Dec 2023 21:11:22 GMT</lastBuildDate>
    <item>
      <title>[D] 有人使用 databricks 特征存储吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ellgt/d_anyone_using_databricks_feature_store/</link>
      <description><![CDATA[这有什么优点？您正在使用时间序列功能吗？   由   提交/u/mdghouse1986  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ellgt/d_anyone_using_databricks_feature_store/</guid>
      <pubDate>Sat, 09 Dec 2023 19:55:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] Alpha-CLIP：专注于任何你想要的地方的 CLIP 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</link>
      <description><![CDATA[CLIP 非常适合理解整个图像，但将注意力集中在特定区域可以提高性能。一篇新论文提出了 Alpha-CLIP。关键的变化是向 CLIP 的图像编码器添加 Alpha 通道，提供重要区域的透明度图。 Alpha-CLIP 与聚焦区域的 Alpha 通道并行运行普通 CLIP。它经过训练可以最大限度地减少组合嵌入和文本描述之间的差异。注意力图表明，这会比基本 CLIP 模型产生更集中的结果，从而带来更好的性能。 论文中的实验表明 Alpha-CLIP 具有更好的对象识别、定位、图像区域推理能力、文本到图像生成控制和 3D 优化。它在各种下游任务（例如文本条件对象检测和文本引导图像操作）中优于 CLIP。 仍然存在一些限制，例如同时处理多个不同区域。但总的来说，简单地添加引导注意力机制可以增强 CLIP 的能力，而不会失去全局背景。 Alpha-CLIP 展示了大型视觉模型中集中理解和控制的有前途的方向。 我认为与 10 月份发布的显式注意力寄存器研究有一些有趣的相似之处（更多详细信息请点击此处）。 TLDR：为区域指导添加 Alpha 通道可提高 CLIP 的性能跨需要本地化图像理解和生成的任务。 完整摘要在这里。论文这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</guid>
      <pubDate>Sat, 09 Dec 2023 19:46:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在没有大量数据的情况下建立自对弈强化学习语言模型面临哪些挑战？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18el9n7/d_what_are_the_challenges_to_have_a_selfplay_rl/</link>
      <description><![CDATA[我最近一直在使用 MuZero，突然想到了这个问题。为了简单起见，这里举个例子：假设你有一个自我游戏环境，预设了 100k 英语单词。 2 个代理用随机选择的单词开始对话。还有另一种模型（像法学硕士这样的教师）根据语法正确性、与之前对话的关系等多种因素向每个代理给予奖励。除了正确实施教师的挑战之外，还可能出现哪些其他挑战？这在理论上现实吗？   由   提交/u/mbrostami   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18el9n7/d_what_are_the_challenges_to_have_a_selfplay_rl/</guid>
      <pubDate>Sat, 09 Dec 2023 19:40:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习与计算机科学研究生</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ehp6c/d_machine_learning_vs_computer_science_post/</link>
      <description><![CDATA[嗨，我目前是一名数学本科生，正在读最后一年，我的长期目标是进入 AI/ML 领域，并将从事硕士学位，最有可能是博士学位。我不确定我的硕士学位要做什么。我的大学提供了一个名为“科学机器学习”的硕士学位，重点关注不同类型的神经网络等，而最终的项目之一将是建造一辆微型自动驾驶汽车，然而他们还提供了一个名为“计算机科学（人工智能）”的项目，这确实让我有机会专注于一些人工智能/机器学习主题，但可能不如机器学习那么深入。对于那些希望在遥远的未来获得人工智能/机器学习创新前沿的人来说，未来最好的选择是什么？ 他们还提供数据科学硕士学位，这似乎涵盖了一些核心机器学习技术，但更关注其背后的数学   由   提交 /u/24troy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ehp6c/d_machine_learning_vs_computer_science_post/</guid>
      <pubDate>Sat, 09 Dec 2023 16:53:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 强化学习的力量：看看这个 DeepRL Sektor 模型如何在 DIAMBRA 竞赛平台提交的视频中为《终极真人快打 3》找到一个智能、超酷的漏洞利用！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eh2hb/p_the_power_of_reinforcement_learning_look_how/</link>
      <description><![CDATA[      ​ https://preview.redd.it/s3v81leu955c1.jpg?width=1920&amp;format=pjpg&amp; ;auto=webp&amp;s=d25d4db609bd60f86b4acea7fd50870b5bce5849 完整视频链接 在耐力阶段战斗时，玩家在每一轮中面对两个对手，一个接一个（在这种情况下） ，卡诺第一，索尼娅第二），并且，为了获胜，它需要击败他们两个。 这不是一个简单的任务，因为玩家的生命值不会重置，所以第二个很容易对手获胜。这就是第一轮索尼娅杀死塞克托时发生的情况。 但是看看第二轮发生的情况，模型找到了一种更简单的获胜方法：它几乎杀死了第一个对手卡诺，而不是完成他后，他会进行机器人舞蹈来欺骗游戏并使回合计时器到期，从而在不面对第二个对手的情况下获得胜利！ 这是仅由 RL 训练产生的紧急行为，没有具体代码也没有调整奖励函数来获得它。我们已经看到它持续发生，并被模型利用来规避该特定阶段的内在困难。 强化学习最令人着迷的方面之一是看到紧急行为以您期望的方式完成任务没想到。   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eh2hb/p_the_power_of_reinforcement_learning_look_how/</guid>
      <pubDate>Sat, 09 Dec 2023 16:23:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] llm_microlibs：在预算限制下以分布式模式运行模型的构建块</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</guid>
      <pubDate>Sat, 09 Dec 2023 14:47:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 洞察深度学习程序员的现实生活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</link>
      <description><![CDATA[大家好！ 我对深度学习程序员的日常工作感到好奇。这个领域有很多讨论，但我想了解这份工作的真正含义。它是否主要是重复的，围绕选择预构建模型和调整参数？或者，它是否涉及更多复杂性，例如从头开始创建自己的算法和模型？ 我特别有兴趣听取那些在该领域拥有第一手经验的人的意见。你的大部分时间是如何度过的？您面临的常见任务或挑战是什么？而且，您的工作中有多少涉及创新与日常任务？ 任何见解或个人经验将不胜感激。提前致谢！   由   提交/u/Maleficent_Average39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</guid>
      <pubDate>Sat, 09 Dec 2023 14:30:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我构建了一个比较云 GPU 的工具。我应该如何改进呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</link>
      <description><![CDATA[    /u/Egor_S   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</guid>
      <pubDate>Sat, 09 Dec 2023 13:20:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2023年11月研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ed48k/p_research_papers_in_november_2023/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ed48k/p_research_papers_in_november_2023/</guid>
      <pubDate>Sat, 09 Dec 2023 13:00:23 GMT</pubDate>
    </item>
    <item>
      <title>【新闻】GitHub 连续 3 天成为全球热门：SuperDuperDB，一个将 AI 与主流数据库集成的框架（让它们变得超级超级）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</link>
      <description><![CDATA[它可以轻松地将人工智能构建到您的应用程序中，而无需复杂的管道，并使您的数据库变得智能化（包括矢量搜索），一定要检查一下：&lt; a href=&quot;https://github.com/SuperDuperDB/superduperdb&quot;&gt;https://github.com/SuperDuperDB/superduperdb  &amp;# 32；由   提交 /u/escalize   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</guid>
      <pubDate>Sat, 09 Dec 2023 11:58:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于文档布局分析的 Vision Grid Transformers</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e8wub/r_vision_grid_transformers_for_document_layout/</link>
      <description><![CDATA[阿里巴巴研究院最近（2023 年 10 月）发布了文档布局分析的新模型，为文档布局分析任务树立了新的基准。  简介 - 为了充分利用多模态信息并利用预训练技术来学习 DLA 的更好表示，在本文中，我们提出了 VGT，一种双流 Vision Grid Transformer，其中 Grid Transformer ( GiT）被提出并预训练用于 2D 令牌级和段级语义理解 https://arxiv.org/abs /2308.14978 ​ 对 LLM 使用的影响 - VGT 可以将页面分解为不同的部分（标题、副标题、标题等），然后可以进行 OCRed 并传递给 RAG 的法学硕士。 通过个人使用 VGT，似乎即使是视觉丰富的文档也可以通过很少的后处理轻松解析。 &lt; !-- SC_ON --&gt;  由   提交 /u/GustaMusto   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e8wub/r_vision_grid_transformers_for_document_layout/</guid>
      <pubDate>Sat, 09 Dec 2023 08:16:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们如何知道现在“最好的模型”是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</link>
      <description><![CDATA[出于某种原因，每个人都在疯狂炒作“MistralAI”发布新模型。人们会访问一个通用网站来比较哪些型号是目前的“领先型号”吗？人们是否会寻找特定的统计数据来确定哪些模型最好？   由   提交 /u/stuck-in-an-ide   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</guid>
      <pubDate>Sat, 09 Dec 2023 06:43:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAAI 会议决定出炉！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e6cpk/d_aaai_conference_decisions_out/</link>
      <description><![CDATA[没有给我发送电子邮件通知，但已在 CMT 中显示:)  &amp;# 32；由   提交/u/benthe human_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e6cpk/d_aaai_conference_decisions_out/</guid>
      <pubDate>Sat, 09 Dec 2023 05:23:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于共谋圈的真诚讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</link>
      <description><![CDATA[亲爱的 NeurIPS 同胞拒绝。当你的深度学习、强化学习、图神经网络和深度学习理论被人们飞到新奥尔良时，你意识到自己被抛在了后面。 ​ I邀请您加入我的小组治疗讨论，我们今天的主题是共谋环。 ​ 我想……第一个问题是它们是否真的存在？它们在机器学习学术界的渗透程度如何？作为一个为发表第一篇论文而奋斗多年的人，我的轶事证据表明，机器学习更多的是关于鼓手的节奏，而鼓手肯定是深度学习的粉丝。 ​ 作为一个仍在努力发表另一篇论文的人，我的轶事观察是，在过去几年里，鼓声变得更加激烈。 ​ 作为一个与同样被边缘化的其他人进行过很多很多对话的人，我们的轶事数据池并不完全是一个数据集，而是一种过滤，它不是独立同分布的，但肯定表明积极主动获取深度学习引用对我们的职业生涯来说是更好的选择。 ​ 作为目前正在审稿 ICLR/AAAI/AISTATS 的人。我的轶事证据是审稿人协调是通过秘密握手、关键词、引文、参考文献列表、主题、arxiv 预印本和shibboleths 进行的。 ​ 我希望你能找到作为一个从内向外看或从外向内看的人，勇敢地分享你的经历。 ​ 作为希望的灯塔，我提醒你阅读迈克尔·乔丹的革命尚未发生。&lt; /p&gt; ​ 作为最后一个需要思考的问题。深度学习合谋圈已经崩溃了吗？还会进一步崩溃吗？   由   提交 /u/Terrible_Button_1763   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</guid>
      <pubDate>Fri, 08 Dec 2023 18:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>