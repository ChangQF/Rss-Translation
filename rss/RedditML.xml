<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 19 May 2024 06:17:31 GMT</lastBuildDate>
    <item>
      <title>2 分钟创造令人惊叹的 AI 二维码艺术！ [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvh12y/create_stunning_ai_qr_code_art_in_2_minutes/</link>
      <description><![CDATA[   /u/OCEANOFANYTHING   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvh12y/create_stunning_ai_qr_code_art_in_2_minutes/</guid>
      <pubDate>Sun, 19 May 2024 05:35:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 回顾我的机器学习之旅：135 门 Coursera 课程、35 门 Udemy 课程和 32 门 Udacity 课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvg6xp/d_looking_back_at_my_ml_journey_135_coursera/</link>
      <description><![CDATA[      每次我回顾我的课程ML之旅，我才意识到E-learning平台的力量。我首先要赞扬 Coursera、Udemy、Udacity 平台，并且我想花点时间回顾一下我过去 6 年的机器学习 (ML) 之旅。这是学习、挑战和成就的旋风，我认为分享我的经验可能会激励或帮助走在类似道路上的其他人。 我的旅程的开始 我我在理学学士学位期间学习了电子和电信。我是一个数学迷，这是我在学习了 Andrew NG 爵士的课程后选择机器学习的主要原因。和你们许多人一样，我一开始并不是对机器如何从数据中学习感到好奇。我开始是因为机器学习中的数学。我有一些基本的编程知识，但机器学习的世界对我来说是全新的。我决定一头扎进去，Coursera 是我的第一站。 Coursera：135 门课程 我从著名的“机器学习”开始正如我之前所说，吴恩达 (Andrew Ng) 的课程。它为我奠定了坚实的基础，从那里，我探索了无数其他课程。以下是一些突出的内容： Andrew Ng 的深度学习专业课程：这一系列课程帮助我掌握了神经网络和深度学习的复杂性。 Python 的应用数据科学专业化：提供了将机器学习技术应用于现实世界问题的实践方法。 机器学习数学：加强了我对机器学习算法的数学基础的理解。 Coursera 上的结构化学习路径和高质量内容让我保持参与并不断接受挑战。 还有更多专业，如“医学人工智能”、“强化学习专业”等。我一个人就完成了 DL.AI 的 50 多门课程 Udemy：35 门课程 向懒惰程序员致敬。地球上最好的机器学习导师🌏。毫无疑问。 Udacity：32 门课程 最后，我向 Udacity 寻求其纳米学位课程，这些课程更加身临其境且基于项目。结构化的课程和以行业为中心的项目帮助我将所学到的知识应用到实际中。我完成的著名纳米学位包括： 机器学习工程师纳米学位 英特尔物联网和边缘人工智能纳米学位 AWS 机器学习纳米学位 回顾过去，这段旅程非常紧张，但收获颇丰。以下是一些关键要点： 一致性是关键：定期、专注的学习时间至关重要。随着时间的推移，小的、持续的努力会累积起来。 实践经验：处理项目和现实问题是非常宝贵的。这是理论知识与实际应用相结合的地方。 社区和网络：参与在线社区、参加网络研讨会和参加论坛帮助我保持积极性和联系。 再次疯狂道具我的导师和所有这些平台。现在我是一名高级数据科学家。还在我国的一些研究所讲授。也是斯里兰卡唯一的人工智能导师🇱🇰。   由   提交/u/1zuu   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvg6xp/d_looking_back_at_my_ml_journey_135_coursera/</guid>
      <pubDate>Sun, 19 May 2024 04:41:56 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分布式系统的交叉点 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvfrd9/intersection_of_ml_distributed_systems_d/</link>
      <description><![CDATA[分布式系统和机器学习的交叉点上存在哪些问题？ 我在这两方面都有不错的背景，我想从事使用分布式计算解决机器学习问题的项目。有哪些好的资源可以参考？或者如何开始？    提交人    /u/tcuser12   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvfrd9/intersection_of_ml_distributed_systems_d/</guid>
      <pubDate>Sun, 19 May 2024 04:16:12 GMT</pubDate>
    </item>
    <item>
      <title>[P] Cafusion：生成猫图像的扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvfaot/p_cafusion_diffusion_model_for_generating_cat/</link>
      <description><![CDATA[我已经在这个项目上工作了一段时间了。它只能生成噩梦般的燃料图像，甚至看起来都不像猫，但我正在努力使其变得更好 这里是存储库：https://github.com/Null-byte-00/Catfusion 这是 jupyter 笔记本：https://nbviewer.org/github/Null-byte-00/Catfusion/blob/main/catfusion.ipynb    由   提交 /u/Soroush_ra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvfaot/p_cafusion_diffusion_model_for_generating_cat/</guid>
      <pubDate>Sun, 19 May 2024 03:49:49 GMT</pubDate>
    </item>
    <item>
      <title>[D]为什么我们没有在论文中看到零样本的 Truthfulqa 性能？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvf2jf/dwhy_dont_we_see_zero_shot_truthfulqa_performance/</link>
      <description><![CDATA[我的直觉是它是最重要的指标之一，但我们通常会看到多次拍摄的性能。就像 phi3 论文中报告的 10 次射击性能一样。   由   提交 /u/Bytesfortruth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvf2jf/dwhy_dont_we_see_zero_shot_truthfulqa_performance/</guid>
      <pubDate>Sun, 19 May 2024 03:36:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否可以用高光谱卫星图像训练 ViTMAE？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</link>
      <description><![CDATA[我正在尝试训练 ViTMAE 编码器来学习一些高光谱卫星图像的表示。图像采用 TIFF 格式，并且有许多波段 (224)。是否可以使用如此大量的输入频段来训练 ViTMAE？知道我应该怎么做吗？   由   提交/u/Robur_131  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/</guid>
      <pubDate>Sat, 18 May 2024 23:42:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴收敛速度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</link>
      <description><![CDATA[我正在使用不平衡的数据集训练 mamba 进行顺序标记任务，我有近 800k 的训练示例。在一个时代之后，少数族裔班级的表现非常糟糕，接近于零。我试图过度拟合一批，但无法实现这一目标。我也尝试过减肥。我想知道这是否正常？曼巴舞是不是从一开始就是这样，然后开始收敛的？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/</guid>
      <pubDate>Sat, 18 May 2024 20:37:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] Grounding DINO 1.5 发布：最强大的开集检测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</guid>
      <pubDate>Sat, 18 May 2024 16:05:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基础时间序列模型被高估了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</link>
      <description><![CDATA[我一直在探索基础时间序列模型，如 TimeGPT、Moirai、Chronos 等，并想知道它们是否真的具有强大的样本潜力 -高效的预测，或者他们只是借用 NLP 基础模型的炒作并将其引入时间序列领域。 我可以理解为什么它们可能会起作用，例如，在需求预测中，它是关于但它们能否处理任意时间序列数据，如环境监测、金融市场或生物医学信号，这些数据具有不规则模式和非平稳数据？ 它们的概括能力是否被高估了？    由   提交 /u/KoOBaALT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</guid>
      <pubDate>Sat, 18 May 2024 16:00:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Llamas 用英语工作吗？论多语言 Transformer 的潜在语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10588 代码：https://github.com/epfl-dlab/llm-latent-language 数据集：https://huggingface.co/datasets/wendlerc/llm-latent-language Colab 链接：  (1) https://colab .research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk_QPHBZb7K?usp=sharing (2) https://colab.research.google.com/drive/1EhCk3_CZ_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing 摘要：  我们询问在不平衡的、以英语为主的语料库上训练的多语言语言模型是否使用英语作为内部枢轴语言——这个问题对于理解语言模型如何发挥作用至关重要以及语言偏见的根源。我们的研究重点关注 Llama-2 系列变压器模型，使用精心构建的非英语提示和独特的正确单标记延续。从一层到另一层，变压器逐渐将最终提示标记的输入嵌入映射到计算下一个标记概率的输出嵌入。通过高维空间跟踪中间嵌入揭示了三个不同的阶段，其中中间嵌入（1）从远离输出令牌嵌入的地方开始； (2) 已经允许在中间层中解码语义上正确的下一个标记，但给予其英语版本比输入语言版本更高的概率； (3) 最后进入嵌入空间的输入语言特定区域。我们将这些结果转化为概念模型，其中三个阶段分别在“输入空间”、“概念空间”和“输出空间”中运行。至关重要的是，我们的证据表明抽象的“概念空间”是存在的。比其他语言更接近英语，这可能会对多语言语言模型的偏见产生重要影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</guid>
      <pubDate>Sat, 18 May 2024 15:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 鲁棒智能体学习因果世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10877 摘要：  长期以来，人们一直假设因果推理在强大而通用的智能。然而，尚不清楚智能体是否必须学习因果模型才能推广到新领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布变化下满足后悔界限的智能体都必须学习数据生成过程的近似因果模型，该模型收敛到最佳智能体的真实因果模型。我们讨论了这一结果对包括迁移学习和因果推理在内的多个研究领域的影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</guid>
      <pubDate>Sat, 18 May 2024 15:06:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 命名实体识别库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</link>
      <description><![CDATA[大家好，我需要决定使用哪个库进行命名实体识别。我使用过 spaCy，它运行良好，但我需要一个允许我对实体和子实体进行分类的库。有人做过类似的事情吗？我的意思是，同一个词可以是多个实体。 spaCy 提供了 SpanCat 管道，理论上可以实现这一点，但我在创建训练语料库时遇到了麻烦。我认为这是因为他们希望你购买像 Prodigy 这样的注释文本框架。   由   提交/u/Original_Ad8019   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</guid>
      <pubDate>Sat, 18 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>[N] ICML 2024 离散运算可微分研讨会 🤖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</link>
      <description><![CDATA[大家好！ 我们今年将在 ICML 组织几乎所有事物均可微分研讨会。 许多离散操作，例如排序、topk、最短路径、聚类（等等）几乎在任何地方都有零梯度，因此不适合现代基于梯度的学习框架（例如深度学习）。本次研讨会将涵盖旨在解决此类问题的研究课题！ https://differentiable.xyz/ 我们鼓励任何研究相关主题的人提交他们的工作。即使您不提交，也请参加 ICML 的研讨会，观看一些即将举行的精彩演讲！ 我已在下面附上研讨会的完整摘要！祝你目前的工作一切顺利，L :) 梯度和导数是机器学习不可或缺的部分，因为它们可以实现基于梯度的优化。然而，在许多实际应用中，模型依赖于实现离散决策的算法组件，或依赖于离散的中间表示和结构。这些离散步骤本质上是不可微的，因此会破坏梯度流。要使用基于梯度的方法来学习此类模型的参数，需要将这些不可微分的组件变为可微分的。这可以通过仔细考虑来实现，特别是使用平滑或松弛来为这些组件提出可微分的代理。随着模块化深度学习框架的出现，这些想法在机器学习的许多领域变得比以往任何时候都更受欢迎，在短时间内产生了大量“可微分的一切”，影响了渲染、排序和排名、凸优化器、最短路径、动态规划、物理模拟、NN 架构搜索、top-k、图算法、弱监督和自监督学习等等。 本次研讨会将为任何可微分的事物提供一个论坛，将学术界和行业研究人员聚集在一起，重点介绍挑战和发展，提供统一的想法，讨论实际的实施选择并探索未来的方向。    提交人    /u/machine_learning_res   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</guid>
      <pubDate>Sat, 18 May 2024 13:22:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPT-Burn：纯 Rust 中简单简洁的 GPT 实现 🔥</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</link>
      <description><![CDATA[       由   提交/u/ProfessionalDrummer7   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</guid>
      <pubDate>Sat, 18 May 2024 08:25:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>