<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 02 Dec 2023 03:13:59 GMT</lastBuildDate>
    <item>
      <title>[讨论] 有没有类似Suno.AI的开源项目？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188r5u0/discussion_any_open_source_project_that_comes/</link>
      <description><![CDATA[我知道 bark 有添加 ♪ 的选项，它会用给定的歌词创建一首歌曲，有什么比本地运行更好的吗？    由   提交 /u/iChrist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188r5u0/discussion_any_open_source_project_that_comes/</guid>
      <pubDate>Sat, 02 Dec 2023 00:38:58 GMT</pubDate>
    </item>
    <item>
      <title>[D]深入研究 Google Brain 团队的 Vision Transformer (ViT) 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</link>
      <description><![CDATA[我们每周五都有一个名为 Arxiv Dives 的阅读俱乐部，在那里我们回顾当今机器学习中使用的许多最先进技术的基础知识。上周我们深入探讨了“视觉变形金刚” 2021 年的论文，其中 Google Brain 团队针对 ResNets 进行了大规模 Transformer 训练基准测试。 尽管截至本周这还不是开创性的研究，但我认为随着人工智能的发展步伐，深入研究过去的工作非常重要以及其他人的尝试！很高兴退一步回顾基础知识并跟上最新和最好的内容。 如果有人觉得有帮助，请在此处发布注释并回顾一​​下： https://blog.oxen.ai/arxiv-dives-vision-transformers-vit/&lt; /p&gt; 也希望有人能加入我们周五的直播！我们有一个由 300 多名工程师和研究人员组成的非常稳定且有趣的团队。   由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</guid>
      <pubDate>Fri, 01 Dec 2023 23:16:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] M2 最大 96GB 或 M2 超 64GB</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188p9ht/d_m2_max_96gb_or_m2_ultra_64gb/</link>
      <description><![CDATA[Mac Studio Apple M2 Ultra 芯片，配备 24 核 CPU 和 60 核 GPU、64 GB RAM 或 Mac Studio Apple M2 Max 芯片，配备 12 核 CPU 和 38 核 GPU、96 GB RAM 适用于法学硕士。你会做什么？   由   提交 /u/breadandtacos   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188p9ht/d_m2_max_96gb_or_m2_ultra_64gb/</guid>
      <pubDate>Fri, 01 Dec 2023 23:10:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 当元学习遇到在线和持续学习时：一项调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</link>
      <description><![CDATA[   论文: https://arxiv.org/abs/2311.05241 摘要 ：  在过去的十年中，深度神经网络在使用涉及广泛数据集的小批量随机梯度下降的训练方案方面取得了巨大的成功。在这一成就的基础上，探索神经网络在其他学习场景中应用的研究激增。元学习是一个引起广泛关注的著名框架。通常被描述为“学会学习”，元学习是一种数据驱动的方法来优化学习算法。其他感兴趣的分支是持续学习和在线学习，两者都涉及使用流数据增量更新模型。虽然这些框架最初是独立开发的，但最近的工作已经开始研究它们的组合，提出新颖的问题设置和学习算法。然而，由于复杂性增加且缺乏统一术语，即使对于经验丰富的研究人员来说，辨别学习框架之间的差异也可能具有挑战性。为了促进清晰的理解，本文提供了一项全面的调查，使用一致的术语和正式的描述来组织各种问题设置。通过概述这些学习范式，我们的工作旨在促进这一有前途的研究领域的进一步进步。  https://preview.redd.it/pp2j7tz2dr3c1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=983e081c4b4fe abddb3457ba74d94202495be4a5&lt; /a&gt;   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</guid>
      <pubDate>Fri, 01 Dec 2023 22:18:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2023 年初，我在一个新的机器学习项目上投入了大量工作。现在我不知道该怎么办。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</link>
      <description><![CDATA[首先我想澄清这不是一个自我推销的帖子。我希望许多机器学习人员向我提出有关这个项目的问题或意见。关于我自己的一些背景。我确实使用 GPTQ 对 LLaMA 进行 4 位量化。 （https://github.com/qwopqwop200/GPTQ-for-LLaMa）。我已经深入研究人工智能多年了。 早在 2023 年初，我就创建了几个独特的系统，可以输出这样的视频 (https://www.youtube.com/watch?v=uoEd8WykzkY) 。这并不是说这是有史以来最好的视频，它主要是对其功能的技术演示。我在系统上做了一些推广，但大多数情况下只能根据我所知接触到机器人。我不知道该怎么办。我有一些创意，但想听听是否有人对这个系统感兴趣。我投入了大量的工作，希望看到它用于一些有趣的事情。 该项目的网站是基本和简单的，就像现在一样，实际上只是一个占位符和联系页面。  在发布了一堆由它制作的视频后，我根本没有得到太多回应。我认为有些人可能会将其与垃圾邮件或其他任何内容混淆。 几点是： - 可以完全自动化，几乎不需要人工干预 - 完全高清视频 - 独特的3D透视视频系统，可从2D图像生成3D视频 - 无需互联网连接 - 无需特殊硬件，即可使用单个 3090 或更好的处理器运行。   由   提交 /u/mentosorangemint   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</guid>
      <pubDate>Fri, 01 Dec 2023 20:56:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器人技术的惨痛教训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</link>
      <description><![CDATA[对于这个 subreddit 中还没有读过惨痛教训的两个人，http://www.incompleteideas.net/IncIdeas/BitterLesson.html 但是，作为对机器人感知、规划和学习感兴趣的人，这一定适用吗？我不太确定，特别是在人类（非结构化）环境中的机器人的背景下，其政策涵盖的范围比工厂或仓库机器人要广泛得多。机器人必须应对现实世界的随机性和巨大差异性，其策略对于环境的变化具有稳健性。我可以想到一些想法，这些惨痛的教训可能适用，也可能不适用。  硬件限制。尽管将计算卸载到远程服务器绝对是一种选择，但机器人在与环境实时交互时可以在多大程度上依赖于此？没有可行的机器人能够存储数十亿个参数，即使只是为了推理。一段时间以来，摩尔定律的速度已经放缓。 数据。在我看来，这是一件大事。什么构成了训练机器人策略的良好训练数据？我们有足够的吗？当然，我们拥有良好的模型和足够的 CV 和语言数据，甚至丰田关于使用扩散模型进行抓取姿势的论文看起来也很有希望，但机器人政策必须将所有这些放在一起才能完成多模式任务。目前还没有用于多模式任务规划的庞大语料库，例如如何将倒一杯水等任务分解为具有多个子任务（抓杯、拾取、倒水等）的 HTN。  我之所以发这篇文章是因为我不确定。我可以看到法学硕士如何成为可以完成多项任务的通用机器人策略的基础，或者更高效的架构可以允许机器人使用更多计算。你有什么想法？   由   提交/u/n0ided_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</guid>
      <pubDate>Fri, 01 Dec 2023 19:14:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果审稿人在反驳期间保持沉默，我们是否应该联系 AC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</link>
      <description><![CDATA[在将我们的论文提交给 ICLR2024 并收到初步评审后，我们针对审稿人提出的所有观点提供了详细的反驳。然而，自从我们反驳之后，审稿人方面就完全沉默了。尽管最初的评论非常详细且反馈积极，但没有进一步的问题、评论或任何形式的参与。 在这种情况下，是否建议联系 AC 请求他们的干预鼓励审稿人参与？ 有人遇到过类似的情况吗？你做了什么？如果有任何建议，我们将不胜感激。   由   提交 /u/jzhoubu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</guid>
      <pubDate>Fri, 01 Dec 2023 18:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] RETVec：弹性且高效的文本矢量化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</link>
      <description><![CDATA[星期五快乐， 非常高兴与大家分享 RETVec 的代码和模型，我们用于分类的新 SOTA 稳健文本标记器已可用在 Github 此处 和 NeurIPS 论文在这里。我们还通过 TFJS 为 TFLite 和网络提供本机支持。希望您会发现它对您的研究有用。如果您想尝试一下，我们有一本入门笔记本。  如果您有任何疑问，请告诉我们。 ​   由   提交/u/ebursztein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</guid>
      <pubDate>Fri, 01 Dec 2023 16:51:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] Llama 微调速度提高 80%，内存减少 50%，精度损失 0%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</link>
      <description><![CDATA[       嘿r/MachineLearning！ 我手动导出了反向传播步骤，做了一些链式矩阵乘法优化，用 OpenAI 的 Triton 语言编写所有内核，并进行更多数学和编码技巧，以使 QLoRA 在 Unsloth 上对 Llama 的微调速度提高 5 倍：https:// github.com/unslothai/unsloth！一些亮点：  速度提高 5 倍（5 小时到 1 小时） 使用内存减少 50% 精度损失为 0% 所有本地均在 NVIDIA GPU（Tesla T4、RTX 20/30/40、Ampere、 Hopper）免费！ QLoRA / LoRA 现在训练速度提高了 80%。  在 2 个 Tesla T4 上的 Slim Orca 518K 示例上通过 DDP 的 GPU，Unsloth 在 260 小时内在所有层上训练 4 位 QLoRA VS Huggingface 的原始实现需要 1301 小时。 Slim Orca 1301 小时到 260 小时 您可能（很可能不）记得来自 Hyperlearn 的我 (https://github.com/danielhanchen/hyperlearn）是我几年前推出的，旨在通过数学和编码技巧使 ML 算法速度提高 2000 倍。 我通过 https://unsloth.ai/introducing 写了一篇关于所有手动手动导出反向传播的博客文章。&lt; /p&gt; 我为 Alpaca 编写了 T4 的 Google Colab：https://colab.research。 google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing，在单个 GPU 上将 Alpaca 的速度提高 2 倍。 在 Kaggle 上通过 DDP 上的 2 个 Tesla T4：https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle，微调 LAION 的 OIG 速度快 5 倍，Slim Orca 速度快 5 倍更快。 您可以通过以下方式在本地安装 Unsloth： pip install &quot;unsloth[cu118] @ git+https://github.com/unslothai/unsloth。 git” pip install “unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git”  目前我们仅支持 Pytorch 2.1 和 Linux 发行版 - 更多安装说明请参见 https://github.com/unslothai/unsloth/blob/main/README.md 我希望：  支持除Llama 风格模型（Mistral 等） 添加 sqrt 梯度检查点以再减少 25% 的内存使用量。 还有其他技巧！  谢谢一堆！！   由   提交 /u/danielhanchen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</guid>
      <pubDate>Fri, 01 Dec 2023 16:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta的新语音模型（无缝）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</link>
      <description><![CDATA[Meta Research 刚刚发布了名为 Seamless 的新语音模型：https://ai.meta.com/research/seamless-communication/ 它支持多种语言的语音和文本输入和输出。从某种意义上说，它是一系列相关语音任务的通用模型。非常有趣！   由   提交 /u/semicausal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</guid>
      <pubDate>Fri, 01 Dec 2023 16:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究调查：LLM 生成的代码中的错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188ezi2/r_research_survey_bugs_in_llms_generated_code/</link>
      <description><![CDATA[您好， 我们蒙特利尔理工学院的研究小组（在 Foutse Khomh 教授的指导下）正在进行一项关于“法学硕士生成代码中的错误”。我们准备了一份在线调查，需要您花费大约 10 分钟的宝贵时间来完成询问此类错误的特征。提供的代码片段将采用 Python 语言，但需要具备 Python 的基本背景才能理解。此外，您可以与您认为有资格参与的同事/学生分享此调查。 在调查中，我们不会询问您的姓名，也不会记录您的 IP 地址允许匿名。这项调查的结果将以匿名形式向公众公开。我们非常感谢您为此提供的帮助。如果您想了解有关这项研究的更多信息或有任何疑问，请随时与我们联系。 调查链接： https://forms.gle/8kZaxsNqtb3vbFaD6 感谢您的帮助。 Florian Tambon ([florian-2.tambon@polymtl.ca](mailto:florian-2.tambon@polymtl.ca&lt; /a&gt;)), Arghavan Moradidakhel ([arghavan.moradi-dakhel@polymtl.ca](mailto:arghavan.moradi-dakhel@polymtl.ca)), Amin Nikanjam ([amin .nikanjam@polymtl.ca](mailto:amin.nikanjam@polymtl.ca)), Foutse Khomh ([foutse.khomh@polymtl.ca](mailto:foutse.khomh@polymtl .ca)) SWAT 实验室，蒙特利尔理工学院，http://swat.polymtl.ca/&lt; /a&gt;   由   提交/u/aminnikanjam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188ezi2/r_research_survey_bugs_in_llms_generated_code/</guid>
      <pubDate>Fri, 01 Dec 2023 15:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一些作者是否认真地添加了比需要的更多的数学知识，以使论文“看起来”更具开创性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</link>
      <description><![CDATA[我最近注意到一种趋势，即作者在某些情况下添加了超出所需的形式主义（例如，图表/图像就可以很好地完成工作）。  这是否是为了使论文看起来更好而添加了超出所需的数学内容，或者可能只是受到出版商的限制（无论论文必须坚持什么格式才能发表）？ &gt;   由   提交 /u/Inquation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</guid>
      <pubDate>Fri, 01 Dec 2023 14:29:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 选择非负矩阵分解的分量数量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</link>
      <description><![CDATA[我有几个无向加权网络的估计，即加权邻接矩阵。我使用 NMF 来识别子网/组件。我知道有多种方法可以得出要使用的组件数量，从简单的方法（例如膝图）到更复杂的方法（例如 使用随机删除的数据点进行交叉验证。我已经对我的网络集应用了多种方法，他们建议使用 5-8 个组件。我对两个子网应该是什么样子有一个先验的想法。使用 5-8 个组件似乎可以创建预期两个子网的子网。或者其中两个组件代表预期的子网络，而其余组件看起来与这些组件非常相似。如果我将组件的数量减少到两个，那么我就获得了两个预期的子网。  简单地使用两个组件是否合法，因为它们符合我的期望？ 是有合并组件的方法（例如，将子子网合并到预期的子网），合并组件是否合法？例如。找到与预期网络的相关性最大化的子网络的加性组合？ 是否有方法强制执行“更大”的网络？网络？  我很欣赏你对此的想法！   由   提交 /u/dizzledk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</guid>
      <pubDate>Fri, 01 Dec 2023 14:20:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[Rich 是强化学习书籍&lt;的作者&lt; /a&gt;，最近，他与一些同事创立了 OpenMind 研究所。 ​ 面试时间为 1 周。我有 RL 背景，并且已经对问题和主题有了一些想法，但我也想在艾伯塔省 RL 泡沫之外寻找问题。技术问题是最好的，尽管我对任何事情都持开放态度。谢谢！ ​ 采访发布几周后，我将在此帖子中发布更新。   由   提交/u/ejmejm1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 17:00:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>