<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 14 Oct 2024 18:22:00 GMT</lastBuildDate>
    <item>
      <title>如何着手推荐系统项目？[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/</link>
      <description><![CDATA[我正在开展一个健康与保健项目，我需要向用户推荐可以改善其健康的日常任务。我已经有一个任务列表，但我希望为每个用户个性化这些建议。 我想首先让他们填写一份简短的调查问卷，然后根据他们的回答推荐任务。任何关于如何做到这一点的建议或意见都将不胜感激。    提交人    /u/Content_Reason5483   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/</guid>
      <pubDate>Mon, 14 Oct 2024 15:37:12 GMT</pubDate>
    </item>
    <item>
      <title>“[D]” 使用 SSL 主干网来替代流行对象检测模型的主干网。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/</link>
      <description><![CDATA[只是想知道，是否有人曾经使用过从 ssl 中学到的语义并将其应用于任何流行的对象检测模型，如 yolo 或 ssd，结果如何。我已经考虑这个问题有一段时间了，因为在我读过的许多 ssl 论文中，他们确实提到对象检测是 ssl 与监督对应物相比表现出色的下游任务之一。我也一直在寻找与此相关的任何材料，但无济于事。    提交人    /u/Antman-007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/</guid>
      <pubDate>Mon, 14 Oct 2024 12:37:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 结果监督奖励模型与过程监督奖励模型的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/</link>
      <description><![CDATA[在阅读 PPO 等 RL 算法的实现时，我发现优势和价值实际上是在 token 级别计算的。以下是如何在 OpenRLHF 中获得优势的实现（类似于 TRL） &quot;&quot;&quot;从奖励和价值计算优势和回报的函数。按照原始 PPO 论文中的计算方式计算：https://arxiv.org/abs/1707.06347 请注意，奖励可能包括 KL 散度损失项。优势如下所示： Adv1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... - V1 + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 返回如下所示： Ret1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 输入： - 值：形状为 (batch_size, response_size) 的张量 - 奖励：形状为 (batch_size, response_size) 的张量 输出： - 优势：形状为 (batch_size, response_size) 的张量 - 返回：形状为 (batch_size, response_size) 的张量 &quot;&quot;&quot;  因此，尽管人们使用结果监督奖励模型 (ORM)，但他们实际上使用它来获取 token 级反馈。从这个角度来看，PRM 和 ORM 似乎是相同的（因为它们都需要执行 token 级反馈）。我说得对吗？ 这是我的第一个问题。这是第二个问题：人们使用在结果反馈上训练的奖励模型来执行 token 级反馈是一种常见的做法吗？奖励/奖励模型似乎不准确/被误用。    提交人    /u/zetiansss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/</guid>
      <pubDate>Mon, 14 Oct 2024 11:45:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] pytorch 的高效视频摄取？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/</link>
      <description><![CDATA[我目前正在启动一个新项目，需要训练视频分类器/回归模型。每个视频由 ~360 帧组成，并以非常高的质量 ~3840x2160 拍摄（由同一相机在同一位置拍摄几乎相同的产品）。视频目前以 .ts 格式保存，我对此并不十分熟悉，但似乎压缩效率很高，因为每个视频仅占用约 15 MB 的空间。 我还不知道如何训练这些视频，但我的想法是在训练期间将每个视频分成随机的 n 帧剪辑。因此，如果 n=20，一个样本将具有形状 (20,3,3840,2160)。 最初，我在想，我只需将每个视频转换为图片帧，然后保存图片，或者将图片保存为 pytorch 对象。但是 15 MB 的视频会变成 0.5 GB 的 jpg 图片，更糟糕的是，如果我直接将其保存为 uint8 中大小为 (360,3,3840,2160) 的 pytorch 对象，那么每个小视频最终会占用大约 9 GB。所以显然这是不行的。 pytorch vision 有一个名为 VideoClips 的方法，https://github.com/pytorch/vision/blob/main/torchvision/datasets/video_utils.py，它似乎是为这种事情设计的，但使用此方法处理其中 3 个视频需要大约 80 秒。 （建议缓存此输出，但我不确定他们到底是什么意思？只是将结果腌制到文件中还是他们的意思是什么？） 使用 opencv 将相同的 3 个视频读入内存大约需要 20 秒，到目前为止这似乎是最好的方法，但我希望有一些我错过了的更好的工具。 也许解决方案涉及将视频从 .ts 转换为压缩程度较低但在 ML 中更易于读取和处理的格式？    提交人    /u/alyflex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/</guid>
      <pubDate>Mon, 14 Oct 2024 10:58:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] Text2Chart31：带有自动反馈的图表生成指令调整（EMNLP 2024 Main）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2410.04064v1 代码： https://github.com/fatemehpesaran310/Text2Chart31 TL;DR：我们提出了一个新的数据集 Text2Chart31，以及一种基于强化学习的 LLM 图表生成微调方法。 （b）我们的数据集 Text2Chart31 和（c）我们基于强化学习的指令调整的说明。 摘要：大型语言模型 (LLM) 已在各种语言任务中展示出强大的能力，尤其是通过指令调整方法。然而，LLM 在通过图表和绘图可视化复杂的真实世界数据方面面临挑战。首先，现有数据集很少涵盖全系列图表类型，例如 3D、体积和网格图表。其次，监督微调方法不能充分利用丰富数据集（包括文本、代码和图形）中的复杂关系。为了应对这些挑战，我们提出了一个分层的流程和一个用于图表生成的新数据集。我们的数据集 Text2Chart31 包含 31 种引用 Matplotlib 库的独特绘图类型，具有 11.1K 个描述、代码、数据表和绘图元组。此外，我们引入了一种基于强化学习的指令调整技术，用于图表生成任务，而无需人工反馈。我们的实验表明，这种方法显著提高了模型性能，使较小的模型能够胜过较大的开源模型，并在数据可视化任务中与最先进的专有模型相媲美。 数据集：我们利用 GPT-3.5-turbo 和 GPT-4 开发了一个分层的绘图生成流程。我们新贡献的 Text2Chart31 数据集支持基于 Matplotlib 的 31 种绘图类型，具有 11.1K 个数据点。我们在表 1 中概述了其主要特征，并将其与数据可视化领域的现有数据集进行了比较。 Text2Chart31 数据集 D 由 11,128 个数据点组成，每个数据点包含一个 (x, c, d, r, y) 元组：文本绘图描述 (x)、其对应的代码 (c) 和结果绘图 (y)。 对于 8,166 个数据点，我们还包括一个原始数据表 (d) 和中间推理步骤 (r) 来生成描述。 我们的数据集 Text2Chart31 的统计数据。 任务定义：我们的基准旨在评估三个任务：  描述到图表：给定一个绘图描述 x，算法会生成其相应的代码 c，该代码使用 Matplotlib 库创建图表。 原始数据到图表：当仅提供原始数据表 d 时，算法会生成中间推理步骤 r，用于分析原始数据，然后生成描述d 根据数据特征选择最合适的绘图类型。 代码到描述：给定绘图的代码 c，模型会生成该绘图的详细描述 x。  实验： 实验结果。CLI 和 L3I 分别表示 Code Llama Instruct 和 Llama 3 Instruct。    提交人    /u/Moreselflove0324   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</guid>
      <pubDate>Mon, 14 Oct 2024 09:04:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于自动驾驶的廉价推理硬件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</link>
      <description><![CDATA[我目前正在建造一辆自动驾驶遥控车，它通过使用摄像头检测街道上的锥体以及使用激光雷达的激光雷达初始里程计系统进行导航。到目前为止，我使用的是 Nvidia Jetsons，但我对生态系统的状态（和价格）并不满意。由于我的实际推理需求非常小（yolov5s 为 60fps），您是否有使用不包含 Cuda GPU 且具有足够性能来运行此类系统的替代系统的经验？（假设对于管道中的所有其他内容，汽车填满了 4x 2.0GHz 内核）    提交人    /u/NumerousSwordfish653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</guid>
      <pubDate>Mon, 14 Oct 2024 07:52:48 GMT</pubDate>
    </item>
    <item>
      <title>Llama 3 微调后性能下降 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/</link>
      <description><![CDATA[我使用 Lora 适配器对 Llama3 模型进行了微调，以完成分类任务。我有大约 9000 个样本，并且对模型进行了大约 5 个 epoch 的训练。但微调模型的召回率比基础模型差。这可能发生的原因是什么？    提交人    /u/Ok-Emu5850   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/</guid>
      <pubDate>Mon, 14 Oct 2024 06:46:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于分离文档的图像分割模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</link>
      <description><![CDATA[大家好，我正在做一个涉及处理身份证件的项目。我想先从给定的图像中提取文件，然后再进行处理，我正在寻找一个可以帮助我做到这一点的模型。我将处理来自世界各地的通用文件，所以我不想用给定的一组文件来微调模型（如 yolo）。我正在寻找一个通用的文档分割模型，但到目前为止我还没能找到，因为它们中的大多数都涉及在给定的一组图像上微调 yolo 模型。 如果你们能提供一些线索，我将不胜感激。谢谢！    提交人    /u/comical_cow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</guid>
      <pubDate>Mon, 14 Oct 2024 04:35:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助 NASA 资助的项目更多地了解太阳！（Kaggle 竞赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</link>
      <description><![CDATA[大家好，我叫 Hannah，我是 NASA 资助的 Eclipse Megamovie 2024 项目的通讯员。 4 月份日全食临近时，我们非常活跃，但还有更多令人兴奋的事情等着我们！我们发起了 Kaggle 竞赛，希望得到像这样的社区的帮助。以下是有关整个项目的更多信息以及我们的竞赛页面的链接。请随时提问，我会尽力回答！ 2024 年 4 月 8 日，日全食始于南太平洋，横跨北美洲，途经墨西哥、美国和加拿大。北美大陆第一个经历日全食的地点是墨西哥太平洋海岸，时间大约是太平洋夏令时间上午 11:07。 2024 年 4 月 8 日日全食之后，超过 145 名志愿者上传了超过 1 TB 的照片数据，供我们的项目使用。 Eclipse Megamovie 2024 (EM2024) 由 NASA 资助，旨在利用日全食期间收集的数据研究太阳，这是一个特殊的时期，可以研究太阳的行为，与其他任何时候都不同。日食和数据收集之后的下一个阶段是对照片数据进行分类和标记，然后我们就可以开始认真进行科学分析——这就是你发挥作用的地方！ 如果您精通 Python 代码和机器学习，您可能能够为解答有关太阳的以前未解答的问题做出贡献！  比赛页面链接：https://www.kaggle.com/competitions/eclipse-megamovie 比赛参与者将使用我们的 2017 年日全食数据集来“训练”机器，方法是编写代码并利用提供的训练数据集自动根据日食阶段将日食照片归类为几个类别之一。建议有兴趣参加本次比赛的人具备 Python 和机器学习基础知识。 与我们的竞赛一致的兴趣：摄影、太阳物理学和/或太阳科学研究、参与式科学和机器学习。奖品： 排行榜奖品：根据私人排行榜排名颁发。  一等奖：带太阳滤镜的图像稳定双筒望远镜、Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、一等奖证书。 二等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、二等奖证书。 三等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、三等奖证书。  参与者将帮助确保数据 [日食照片] 能够快速组织，并且每张图片都具有正确的信息（元数据）。通过帮助我们开发能够准确识别志愿者提交的照片中的日食阶段的代码，您将帮助我们跨越一个重大的数据处理障碍。通过您的代码，您将为这项由 NASA 资助的研究太阳喷流和等离子羽流铺平道路！ 您的任务是创建最准确的分类机，将日食照片分类到特定的日食阶段。如果您的代码能够成功地将提供的照片分类为以下类别，您就知道自己成功了：暗色或平面（校准镜头）、日偏食阶段（20 度的箱 [类别]）、钻石环阶段、日全食阶段，当然还有非日食类别。 特别感谢 Mods 让我知道在这篇文章中使用哪个标签 :) 已编辑文字    提交人    /u/EMegamovie2024   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</guid>
      <pubDate>Sun, 13 Oct 2024 23:11:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 被研究论文淹没了？🐸</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</link>
      <description><![CDATA[      我们是两名对人工智能研究感兴趣的工程师，但却被 arXiv 上大量新论文淹没。因此，我们开发了 Ribbit Ribbit，一款研究论文发现工具。  https://apps.apple.com/us/app/ribbit-ribbit/id6529547956 https://ribbitribbit.co  它会整理个性化的论文推荐，并将其转化为推文大小的摘要，这样您就可以像在 Twitter 上一样滚动浏览。您还可以像为您量身定制的播客一样收听更新。我们添加了一点轻松的体验，希望它能为整个纸质阅读过程增添一丝乐趣，说实话，这个过程可能会变得相当枯燥乏味 :p。 https://preview.redd.it/evoemobinlud1.png?width=1179&amp;format=png&amp;auto=webp&amp;s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5    提交人    /u/haoyuan8   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</guid>
      <pubDate>Sun, 13 Oct 2024 22:24:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</link>
      <description><![CDATA[      我刚刚阅读了一篇有趣的论文，旨在解决（或改进）信息检索问题细粒度引用，“LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用”(https://arxiv.org/abs/2409.02897)。 在本文中，研究人员使用现成的 LLM 生成由具有精确句子级引用的长上下文 QA 实例组成的数据集，然后使用该数据集微调开放权重 LLM 以生成带有引用的答案。与 GPT4o、Llama 3.1 等相比，生成的 LongCite 8B 和 9B 模型出奇地好。 这是如何工作的？以下是生成指令微调数据集的 4 步过程： (a) 从长文本或文档开始，他们的方法使用现有的 LLM 使用 Self-Instruct 生成问答数据集（查询及其相关答案）（Wang 等人 2023；在我之前的一篇文章中讨论过）。 (b) 接下来，他们使用答案从输入文本中检索几个 128 个标记的块以进行粗粒度引用。 (c) 然后，LLM 在这些块中寻找相关句子，以提供更细粒度的句子级引用 (d) 研究人员过滤掉答案中不到 20% 的陈述没有引用的问答对 然后使用生成的数据集以传统（SFT）方式训练 LLM。 https://preview.redd.it/gucywp0o8jud1.jpg?width=6000&amp;format=pjpg&amp;auto=webp&amp;s=4ed2ff078327082133d390c566250a2ef41c1a05    提交人    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</guid>
      <pubDate>Sun, 13 Oct 2024 14:19:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获得博士学位的现实性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</link>
      <description><![CDATA[大家好！我是伦敦大学学院的研究生，正在攻读机器学习硕士学位，我很快将申请 2025 年秋季开始的博士课程。我将分享我的个人资料和我将申请的学校，并希望了解我所瞄准的实验室是否超出了我的能力范围。 我以一等（荣誉）成绩获得了新加坡南洋理工大学的数学和计算机科学本科学位，预计也将以一等（荣誉）成绩获得研究生学位。我对理论深度学习感兴趣——围绕损失曲面曲率、优化轨迹、学习动态和泛化的问题——这些都是数学密集型的研究领域。虽然我的课程大部分都是理论性的，并且与此类研究非常一致（按设计），但我的研究经历更具实验性。我在 ICML 上发表了一篇第三作者出版物，内容是我为学士论文项目所做的工作。这是一项相当理论化的工作，但我只负责实验。我还有 2 篇第一作者预印本——一篇关于 NLP 的实验性工作（旨在在 IEEE 上发表），另一篇关于图形 ML（旨在在顶级会议之一上发表），其中有相当多的理论部分，但没有我希望在博士学位上完成的工作那么多。 我的目标是进入 ETH、UCL、斯坦福、NYU、EPFL、哥伦比亚和普林斯顿的实验室（按优先顺序，其中一个是我的职位）。所有这些实验室都有非常成功的 PI（按引用次数计算），他们研究的主题与我的兴趣非常一致。我担心我看似无所不包的研究背景可能会让他们失望，但我希望我的成绩能让他们相信我精通理论。我希望我的导师能写出优秀的推荐信，因为他们在很多场合都对我表示赞赏。我希望写一份令人信服的研究陈述，但由于我几周前才开始阅读相关文献，所以最终可能不是那么完美。 我不介意与年轻的 PI 合作，只要我身边有一些研究人员在研究相关主题。在高级实验室，已经建立了一个网络，我可能先协助一些项目，然后再进行独立研究。现实地说，我是不是在自吹自擂？如果是这样，有人可以推荐一些年轻的 PI 从事上述研究课题，我可能更有机会加入他们的实验室吗？    提交人    /u/mio_11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</guid>
      <pubDate>Sun, 13 Oct 2024 10:38:56 GMT</pubDate>
    </item>
    <item>
      <title>提出新颖的想法[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</link>
      <description><![CDATA[关于如何想出新颖的问题解决方案，您有什么想法吗？每次我以为自己有了一些想法，我的导师就会说“这太简单了”。许多方法以独特的方式将现有的构建块粘合在一起，但我很难想象人们如何想出既真正新颖又真正有效的东西。 有时，我读到一篇论文，我意识到这个想法实际上非常简单/直接，作者只是介绍了一个很酷的技巧。其他时候，我读到的东西介绍了一个非常晦涩的定理，或者他们注意到一些我只能梦想的东西。我倾向于前者，但由于新颖性有限，我对迄今为止所写的任何东西都不太自豪。疯狂的出版速度让我偏向“简单而有效”，这无济于事方法中的大部分工作是在获得 SOTA 后事后编写故事。    提交人    /u/like_a_tensor   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</guid>
      <pubDate>Sun, 13 Oct 2024 06:25:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 13 Oct 2024 02:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>