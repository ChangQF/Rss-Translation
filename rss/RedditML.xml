<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Wed, 24 Apr 2024 21:12:52 GMT</lastBuildDate>
    <item>
      <title>[D] 队内唯一的DS</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc74pm/d_the_only_ds_in_the_team/</link>
      <description><![CDATA[我目前正在攻读数据科学硕士学位，需要在最后一个学期完成一个实习项目。我一直在寻找实习机会，最近一家初创公司与我联系，该公司有兴趣使用现有的大型语言模型开发人工智能教学模型。然而，我将是公司唯一的数据科学家，鉴于我的经验有限，我觉得这令人畏惧。该项目需要最终为我的实习带来切实的成就，我担心在这种情况下这可能不可行。我正在寻求关于是否继续利用这个机会的建议，特别是考虑到缺乏专门的数据科学团队。  你觉得怎么样？我应该接受这个挑战吗？   由   提交/u/Dramatic_Chance9577   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc74pm/d_the_only_ds_in_the_team/</guid>
      <pubDate>Wed, 24 Apr 2024 19:25:40 GMT</pubDate>
    </item>
    <item>
      <title>召回分数增加 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc4fls/recall_score_increase_d/</link>
      <description><![CDATA[大家好， 我正在尝试做一个小型欺诈检测项目，但我的数据集非常不平衡。我使用了随机欠采样，因为少数类非常小，我也尝试了 smote 或与我得到的 smote 最佳召回分数相结合，仅使用随机欠采样（0.95）。我认为 GridsearchCV 会增加它，但它不是增加，而是减少，尽管我试图让它专注于召回分数。为什么会发生这种情况？   由   提交/u/Legal_Hearing555  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc4fls/recall_score_increase_d/</guid>
      <pubDate>Wed, 24 Apr 2024 17:38:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在数据分割过程中保留数据的空间分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc3s98/d_preserving_spatial_distribution_of_data_during/</link>
      <description><![CDATA[      您好，我正在尝试使用随机森林模型对德国巴伐利亚河流中的硝酸盐浓度进行建模。我使用 Python，主要使用 sklearn。我有 490 个水质站的数据。我遵循 LongzhuQ.Shen 等人论文中的方法，该论文可以在这里找到：https:// www.nature.com/articles/s41597-020-0478-7 我想将我的数据集分为训练集和测试集，以便两个集中数据的空间分布相同。这个想法是，如果数据分割忽略空间分布，则训练集可能最终会集中来自人口稠密区域的点，而忽略稀疏区域。这可能会扭曲模型的学习过程，使其在整个感兴趣领域的准确性或概括性降低。 sklearn train_test_split 只是将数据随机划分为训练集和测试集，并且不考虑数据中的空间模式。 我上面提到的论文遵循这种方法：“我们将完整数据集分成两个子数据集” -分别是数据集、训练和测试。为了考虑监测站空间分布的异质性，我们在数据分割步骤中采用了空间密度估计技术，通过使用带宽为 50 km 的高斯核（使用 GRASS GIS33 中可用的 v.kernel）构建密度表面来计算每个物种和季节。所得密度表面的像素值用作权重因子，将数据分成具有相同空间分布的训练和测试子集。” 我想遵循相同的方法，但不使用草地 GIS ，我只是自己用 Python 构建密度曲面。我还提取了概率密度值和站点的权重。 （附图） 现在我面临的唯一问题是如何使用这些权重将数据拆分为训练集和测试集？我检查了sklearn train_test_split函数中没有可以考虑权重的关键字。我也与GPT 4聊天来回，但它也无法给我一个明确的答案。我在互联网上也没有找到任何关于此的具体信息。也许我遗漏了一些东西。 还有其他函数可以用来执行此操作吗？或者我必须编写自己的算法来进行分割？如果是后者，您能否建议我一种方法，以便我自己编码？ 在附图中，您可以看到站点的位置以及使用核密度估计生成的概率密度表面方法（使用高斯核）。 还附上我的数据帧的屏幕截图，让您对数据结构有一些了解。 （经度（&#39;lon&#39;）列之后的所有列都用作特征。NO3 列用作目标变量。） 如果您有任何答案，我将不胜感激。 &amp; #x200b; 生成的概率密度表面使用高斯核的核密度估计方法。 ​ 我用来模拟硝酸盐浓度的数据集  &amp;# 32；由   提交/u/dr_greg_mouse  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc3s98/d_preserving_spatial_distribution_of_data_during/</guid>
      <pubDate>Wed, 24 Apr 2024 17:14:15 GMT</pubDate>
    </item>
    <item>
      <title>[N] Snowflake发布开放(Apache 2.0) 128x3B MoE模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc3255/n_snowflake_releases_open_apache_20_128x3b_moe/</link>
      <description><![CDATA[链接： ​ https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/ ​ https://replicate.com/雪花/雪花北极指令   由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc3255/n_snowflake_releases_open_apache_20_128x3b_moe/</guid>
      <pubDate>Wed, 24 Apr 2024 16:45:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么这么简单的一句话会破坏LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</link>
      <description><![CDATA[      这是我在 MS Copilot (GPT4 Turbo) 中输入的提示。 它是德语，但它只是意味着“如果我先洗完澡会有什么缺点吗？”），所以这不可能是另一个SolidGoldMagikarp 或类似的，因为这些单词显然都在分词器和训练词汇中。 为什么这么简单的句子会导致这种情况？有什么猜测吗？ （也尝试过 Claude Opus 和 LLama 3 70b，效果很好） ​ https://preview.redd.it/9x6mva7b6gwc1.png?width=1129&amp;format=png&amp;auto=webp&amp;s=bb6ac52d1c52 d981161e8a864c5d1dd3794ca392   由   提交/u/michael-relleum  /u/michael-relleum  reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/</guid>
      <pubDate>Wed, 24 Apr 2024 15:59:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] 说话人分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cc0f40/r_speaker_diarization/</link>
      <description><![CDATA[大家好，  我正在开发一个项目，我想从音频/视频创建说话者感知的转录本，最好是使用开源解决方案。我尝试了很多方法，但似乎没有一种方法可以开箱即用。  我已经尝试过： ​  whisperX: https://github.com/m-bain/whisperX（使用 pyannote） whisper-diarization：https://github.com/MahmoudAshraf97/whisper-diarization（使用 Nemo） AWS Transcribe  AssemblyAI API  Picovoice API   我需要更深入地挖掘并了解导致错误二值化的原因，但我正在寻找改进说话者二值化的建议。如果您曾在该领域工作并取得过成功，请与我们联系。谢谢！    由   提交 /u/anuragrawall   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cc0f40/r_speaker_diarization/</guid>
      <pubDate>Wed, 24 Apr 2024 15:01:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我制作了一个应用程序来根据评论预测 ICML 论文接受情况</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/</link>
      <description><![CDATA[https://www.norange.io/ projects/paper_scorer/ 几年前，u/programmerChilli 分析了 ICLR 2019 评审数据并训练了一个模型，该模型可以相当准确地预测 NeurIPS 的接受结果。 我决定继续进行这项分析，并根据较新的 NeurIPS 评论训练了一个模型（总共约 6000 个参数），该评论的评论数量是 ICLR 2019 的两倍。此外，NeurIPS 的评论评分系统自 2019 年以来已经发生了变化，以下是我了解到的情况： 1) 两个会议一致拒绝几乎所有得分 &lt;5 的提交内容，并接受那些得分 &gt;6 的提交内容。被接受的论文中最常见的分数是 6。大约 5.3 的平均评分通常会导致 ICML 和 NeurIPS 做出可能采取任何一种方式的决定，这表明 ~5.3 可能被认为是接受的软阈值。  2) 置信度分数对于边界评级（例如 4（边界拒绝）、5（边界接受）和 6（弱接受））影响较小，但它们可以显着影响较强拒绝或接受案例的结果。  例如，评级为 [3, 5, 6]，置信度为 [*, 4, 4]，将“拒绝”更改为“拒绝”。置信度从 5 变为 1 会使概率从 26.2% - 31.3% - 52.4% - 54.5% - 60.4% 变化，表明在这种情况下置信度较低会增加您的机会。 相反，对于评级 [3, 5 , 7] 置信度为 [4, 4, 4] 时，接受概率为 31.3%，但当置信度变为 [4, 4, 5] 时，接受概率降至 28.1%。尽管看起来有悖常理，但置信度分数为 5 实际上会降低您的机会。一种可能的解释是，许多评分为 5 的低质量评论通常会被区域主席 (AC) 打折。 希望这会有用，并感谢 u/programmerChilli 寻求灵感！ 我还在一系列 推文。   由   提交 /u/Lavishness-Mission   /u/Lavishness-Mission reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/</guid>
      <pubDate>Wed, 24 Apr 2024 12:23:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] SpaceByte：从大型语言模型中删除标记化 - 莱斯大学 2024 - 几乎与子字标记化器具有相同的性能，但没有许多缺点！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2404.14408 Github：https://github。 com/kjslag/spacebyte 摘要：  标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗性漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的&lt;字节级解码器架构缩小了字节级和子字自回归语言建模之间的性能差距。SpaceByte 由字节级 Transformer 模型组成，但在层中间插入了更大的 Transformer 块。我们发现，通过仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，可以显着提高性能。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且大致与标记化 Transformer 架构的性能相匹配。论文：https://arxiv.org/abs/2404.14408Github: https:// github.com/kjslag/spacebyteAbstract：标记化在大型语言模型中被广泛使用，因为它显着提高了性能。然而，标记化带来了一些缺点，例如性能偏差、对抗漏洞增加、字符级建模性能下降以及建模复杂性增加。为了在不牺牲性能的情况下解决这些缺点，我们提出了 SpaceByte，这是一种新颖的字节级解码器架构，可以缩小字节级和子字自回归语言建模之间的性能差距。 SpaceByte 由字节级 Transformer 模型组成，但在各层中间插入了更大的 Transformer 块。我们发现，仅在某些字节（例如通常表示字边界的空格字符）之后应用这些较大的块，可以显着提高性能。我们的实验表明，对于固定的训练和推理计算预算，SpaceByte 的性能优于其他字节级架构，并且与标记化 Transformer 架构的性能大致相当。  https://preview.redd.it/v1xo6g1gzewc1.jpg?width=1507&amp;format=p jpg&amp;自动=webp&amp;s=f9d415307b60639fa67e8a54c8769fa5a6c10f04 https://preview.redd.it/edvqos1gzewc1.jpg?width=1654&amp;format=pjpg&amp;auto=webp&amp;s=f91c8727017e1a1bc7b80bb77a8627ff99182607  https://preview.redd.it/fe6z6i1gzewc1.jpg?width=1181&amp;format=p jpg&amp;amp; ;auto=webp&amp;s=24d955f30b8ca3eaa7c527f3f40545ed493f789c   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/</guid>
      <pubDate>Wed, 24 Apr 2024 11:42:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 跟踪模型及其相关元数据。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbundt/d_keeping_track_of_models_and_their_associated/</link>
      <description><![CDATA[我开始为我正在从事的项目积累大量模型，其中许多模型都是旧模型，我为了存档而保留它们，并且许多是根据其他模型进行微调的。我想知道是否有行业标准的方法来处理这个问题，特别是我正在寻找以下内容：  有关用于训练模型的参数的信息 用于训练模型的数据集 有关模型的其他元数据（即对象检测模型训练的对象） 模型性能&lt; br /&gt; 模型沿袭（从哪个模型进行微调） 模型进展（该模型是从其他模型直接升级的吗？从相同的模型调整，但使用更好的超参数） 模型源（不确定这一点，但我正在考虑某种方法将模型链接到用于训练的 python 脚本不是很重要，但这样就很好了）  是否有任何服务工具可以帮助实现某些功能？另外，如果这不是这个问题的子问题，我可以获得一些正确方向的指示。谢谢！  ​   由   提交 /u/ClearlyCylindrical   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbundt/d_keeping_track_of_models_and_their_associated/</guid>
      <pubDate>Wed, 24 Apr 2024 10:20:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究人员在考虑创建新的/改进的基础模型时如何考虑归纳偏差？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbnbsd/d_how_researcher_think_of_inductive_bias_when/</link>
      <description><![CDATA[我是学习机器学习的本科生。 我在阅读几篇论文时了解到我们试图减少搜索空间通过在机器学习模型中施加归纳偏差。当归纳偏差与基础数据相匹配时，创建有用模型就会成功。 在像 NVAE 这样的分层模型中，他们如何通过指定数据的计算方式来灌输归纳偏差？ （我认为这被称为算法偏差，但不确定） 但是人们如何认为这种归纳偏差会有帮助，他们坚持这种归纳偏差要经历什么步骤。 &lt;我参加了很多机器学习和统计学课程，但没有听过任何解释这些内容的讲座。我是否错过了任何课程/讲座？ 如果可能，请提供与之相关的论文/讲座/演讲 谢谢   由   提交/u/binny_sarita  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbnbsd/d_how_researcher_think_of_inductive_bias_when/</guid>
      <pubDate>Wed, 24 Apr 2024 02:36:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多模态检索和排序的广义对比学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbixix/r_generalized_contrastive_learning_for_multimodal/</link>
      <description><![CDATA[对流行的 CLIP 训练方法进行概括，使其更适合搜索和推荐。  论文：https://arxiv.org/pdf/2404.08535.pdf Github：https://github.com/marqo-ai/GCL 概括CLIP：  使用任意数量的文本和/或图像来表示文档。 通过模态间和模内损失更好地理解文本。 可以对排名/重要性/相关性进行编码，又名“排名调整”。 适用于预训练、文本、CLIP 模型。 可以学习单向量或多向量表示用于文档。 适用于二进制和 Matryoshka 方法。 开源 10M 行多模态数据集，包含 100k 查询和约 5M 产品。  为什么？ 训练嵌入模型的主流方法在很大程度上与最终用例（如搜索）、向量数据库、用户的需求以及缺乏用于开发和评估的代表性数据集，特别是当涉及多种模式和排名时。 当前矢量搜索嵌入模型的局限性 尽管矢量搜索是虽然非常强大并且可以搜索几乎任何数据，但当前的方法有一些局限性。训练嵌入模型的流行方法在很大程度上与最终用例（如搜索）、向量数据库和用户的需求脱节。这意味着矢量搜索的许多潜力尚未得到满足。下面描述了当前的一些挑战。 仅限于使用单条信息来表示文档 当前模型编码并表示一条信息一个向量的信息。现实情况是，一份文档通常有多个相关信息，这些信息可能跨越多种模式。例如，在产品搜索中可能有标题、描述、评论和多个图像，每个图像都有自己的标题。 GCL 将嵌入模型训练概括为使用所需数量的信息。 处理退化查询时没有排名概念 当存在退化查询时查询 - 满足某些相关标准的多个结果 - 结果的排序只能从许多二元关系中间接学习。实际上，结果的排序很重要，即使对于第一阶段检索也是如此。 GCL 允许在嵌入中编码查询文档特定相关性的大小，并提高候选文档的排名。 使用类似 CLIP 的方法时文本理解较差 对于像 CLIP 这样的多模态模型，这些模型经过训练只能从图像到文本（反之亦然）。由于文本-文本关系是通过图像间接学习的，文本-文本理解不如纯文本模型。对于许多应用程序来说，需要了解模态间和模内内的情况。 GCL 允许通过直接优化来实现模态间和模内理解的任意组合。 缺乏代表性数据集来开发矢量搜索方法 在开发 GCL 的过程中，很明显，与用于嵌入模型训练和评估现实世界用例的公开数据集存在脱节。现有的基准测试通常仅是文本或仅是跨模式的，并且专注于 1-1 查询结果范式。此外，现有数据集的相关性概念有限，大多数将其编码为二元关系，而一些数据集通常仅在测试集上使用（最多）少量离散分类。这与典型的现实世界用例不同，在典型的现实世界用例中，相关性可以是硬二元关系，也可以来自连续变量。为了帮助解决这个问题，我们编译了一个包含 10M（排名）产品查询对的数据集，涵盖约 100k 查询、近 500 万个产品和四个评估分割（可用 此处)。 ​   由   提交/u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbixix/r_generalized_contrastive_learning_for_multimodal/</guid>
      <pubDate>Tue, 23 Apr 2024 23:07:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能在公司内部的实际应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbhxwb/d_practical_uses_of_ai_inside_companies/</link>
      <description><![CDATA[人们如何在公司内部（初创公司 -&gt; FAANG）使用人工智能来改进运营和流程？关于利用 LLM 和 GenAI 的讨论有很多，但除了谷歌搜索中出现的内容之外，我正在努力寻找真正成功的具体示例。 首先想到的是以下领域，但这个列表当然并不详尽：  设计（和交接） 工程 客户支持 销售 文档 营销  什么有效或有希望？什么不起作用？   由   提交 /u/CJSF   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbhxwb/d_practical_uses_of_ai_inside_companies/</guid>
      <pubDate>Tue, 23 Apr 2024 22:25:43 GMT</pubDate>
    </item>
    <item>
      <title>Meta 做了 OpenAI 应该做的一切 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</link>
      <description><![CDATA[我很惊讶（或者可能没有）这么说，但 Meta（或 Facebook）比 OpenAI 更民主化 AI/ML，而 OpenAI 最初是成立并主要为此目的提供资金。 OpenAI 很大程度上已经成为一个仅以盈利为目的的商业项目。虽然就 Llama 模型而言，对我来说它们尚未达到 GPT4 功能，但我相信这只是时间问题。你们对此有何看法？   由   提交 /u/ReputationMindless32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</guid>
      <pubDate>Tue, 23 Apr 2024 22:03:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何将 LLaMA 3 部署到生产中以及硬件要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cb3ge1/d_how_to_and_deploy_llama_3_into_production_and/</link>
      <description><![CDATA[许多人都试图安装和部署自己的 LLaMA 3 模型，因此这里是我刚刚制作的一个教程，展示如何在 AWS EC2 实例上部署 LLaMA 3：https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-production.html 部署 LLaMA 3 8B 相当容易，但 LLaMA 3 70B 则是另一回事。考虑到所需的 VRAM 数量，您可能需要配置多个 GPU 并使用专用的推理服务器（如 vLLM）以便在多个 GPU 上拆分模型。 LLaMA 3 8B 在 FP16 中需要大约 16GB 的磁盘空间和 20GB 的 VRAM（GPU 内存）。至于 LLaMA 3 70B，它在 FP16 中需要大约 140GB 的磁盘空间和 160GB 的 VRAM。 希望它有用，如果您有任何问题，请随时提问！ 朱利安    提交人    /u/juliensalinas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cb3ge1/d_how_to_and_deploy_llama_3_into_production_and/</guid>
      <pubDate>Tue, 23 Apr 2024 12:33:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>