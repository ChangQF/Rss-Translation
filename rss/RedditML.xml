<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 26 Apr 2024 09:14:30 GMT</lastBuildDate>
    <item>
      <title>Yolov8 物体检测的 IoU 准确度“[P]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdez8x/iou_accuracy_on_yolov8_object_detection_p/</link>
      <description><![CDATA[“[P]” 如何计算 yolo v8 上的 IoU 分数。我没有找到任何内置函数可以做到这一点。我已经在我的自定义数据集上训练了 yolov8n，用于对象检测，该数据集有 1 个类别。   由   提交/u/Ghost-9843  /u/Ghost-9843 reddit.com/r/MachineLearning/comments/1cdez8x/iou_accuracy_on_yolov8_object_detection_p/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdez8x/iou_accuracy_on_yolov8_object_detection_p/</guid>
      <pubDate>Fri, 26 Apr 2024 07:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 文本到语音合成的最新进展是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdcbmx/d_what_is_the_state_of_art_for_text_to_speech/</link>
      <description><![CDATA[我开始为我的毕业做一些研究，我正在寻找一些关于文本到语音合成的论文。我正在对一篇我发现很有趣的论文进行一些复制，该论文名为“Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Prediction”。基本上，它是一个接收文本、将其转换为频谱图的模型，并且该频谱图用于构建音频文件。由于我仍处于复制的开始阶段，你们有推荐研究的论文吗？您从事过语音合成 (TTS) 工作吗？我应该研究哪些好的参考文献？ ​ 我在这里看到了这篇文章https://www.reddit.com/r/MachineLearning/comments/nxkuvn/d_what_is_actually_the_state_of_the_art_in_text/ 但是已经有3年了。也许有比 FastSpeech2 更新的东西？ ​   由   提交/u/Zelun  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdcbmx/d_what_is_the_state_of_art_for_text_to_speech/</guid>
      <pubDate>Fri, 26 Apr 2024 04:24:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 元学习 vs 联邦学习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cdbq5t/d_metalearning_vs_federated_learning/</link>
      <description><![CDATA[[D] 大家好，对于深入研究当今热门话题的更好选择和最有效方法，您有什么建议吗？&lt; br /&gt; 我偶然发现了联邦学习的存储库： ​  https://github.com/muditbhargava66/dropgrad https://github.com/ adap/flower  但似乎找不到类似元学习的东西。任何有关如何选择我的博士主题的建议将不胜感激！   由   提交/u/Tight_Confusion_1695   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cdbq5t/d_metalearning_vs_federated_learning/</guid>
      <pubDate>Fri, 26 Apr 2024 03:53:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 临床试验中的机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd9ucz/d_ml_for_clinical_trials/</link>
      <description><![CDATA[大家好，我正在创办一家公司，将 ML 和 NLP 应用于患者数据，目的是找到合适的患者进行临床试验。我正在寻找联合创始人。如果您有兴趣或认识可能感兴趣的人，请私信我。很想聊天。谢谢   由   提交 /u/another_african   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd9ucz/d_ml_for_clinical_trials/</guid>
      <pubDate>Fri, 26 Apr 2024 02:16:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 多头专家混合 - https://arxiv.org/pdf/2404.15045 中建议的密集子代币路由的实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</link>
      <description><![CDATA[我的朋友在这篇 arxiv 论文中实现了 Multihead Mixture of Experts 的方法 https://arxiv.org/pdf/2404.15045 他希望我与你分享！ https://github.com/lhallee/Multi_Head_Mixture_of_Experts__MH-MOE 尝试一下。让我知道您的想法，我会将其传递给他。   由   提交/u/Prudent_Student2839   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</guid>
      <pubDate>Thu, 25 Apr 2024 22:00:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] HyenaDNA 和 Mamba 不擅长顺序标记？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</link>
      <description><![CDATA[大家好，我一直在研究使用 DNA 序列作为输入的顺序标记。最近发布了 2 个基础模型 HyenaDNA（基于 Hyena Operator）和 Caduceus（基于 mamba），我使用了预训练模型和从头开始的模型，即使使用预训练模型，性能也很糟糕。  有人有此类模型的经验吗？性能下降的潜在原因是什么？我在少数群体中的成绩实际上为零？曼巴在阶级不平衡问题上处理得不好吗？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</guid>
      <pubDate>Thu, 25 Apr 2024 20:02:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于图的神经网络的药物毒性预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</link>
      <description><![CDATA[这是我编写/训练的一个小型药物毒性预测 GNN 模型 repo: https://github.com/Null-byte-00/有毒-预测-gnn    由   提交 /u/Soroush_ra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</guid>
      <pubDate>Thu, 25 Apr 2024 19:10:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 面对不可能的机器学习问题，您有哪些恐怖经历</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</link>
      <description><![CDATA[机器学习非常擅长解决一系列小众问题，但大多数技术细微差别都被技术兄弟和管理者忽视了。您被告知要解决哪些问题是不可能的（没有数据、无用的数据、不切实际的期望）或机器学习的误用（您能让这个法学硕士做所有的会计工作吗）。    由   提交 /u/LanchestersLaw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</guid>
      <pubDate>Thu, 25 Apr 2024 18:45:26 GMT</pubDate>
    </item>
    <item>
      <title>因果机器学习数据集 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/</link>
      <description><![CDATA[有人知道有哪些数据集可用于因果推理吗？我想探索双重稳健的 ML 文献中的方法，并且我想通过处理一些数据集并学习 econML 软件来补偿我的学习。 有谁知道任何数据集，特别是在营销/定价/广告的背景是应用因果推理技术的良好来源？我也对其他数据集持开放态度。    由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/</guid>
      <pubDate>Thu, 25 Apr 2024 18:24:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] Dreambooting MusicGen</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</link>
      <description><![CDATA[使用此存储库，只需几分钟即可在小型消费级 GPU 上Dreambooth MusicGen 模型套件：https://github.com/ylacombe/musicgen-dreamboothing 该项目的目标是提供工具可以轻松微调和dreamboothMusicGen模型套件，只需很少的数据，并利用一系列优化和技巧来减少资源消耗，谢谢到 LoRA 适配器。 例如，模型可以是很好的 -调整特定的音乐流派或艺术家以给出以该给定风格生成的检查点。目的还在于轻松共享和构建这些训练有素的检查点， 具体来说，这涉及：  使用尽可能少的数据和资源可能的。我们正在讨论的是 A100 上的微调仅需 1500 万，GPU 利用率低至 10GB 至 16GB。 借助 Hugging Face Hub。 （可选）生成自动音乐描述 （可选）在 类似 Dreamambooth 的时尚，其中一个关键字会触发特定风格的生成  Wandb 的示例训练运行类似于此处。   由   提交/u/Sufficient-Tennis189  /u/Sufficient-Tennis189 reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过自适应 N-gram 并行解码实现大型语言模型的无损加速</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.08698 摘要：  虽然大型语言模型（LLM）表现出了非凡的能力，但它们由于自回归处理而导致大量资源消耗和相当大的延迟，从而阻碍了这一过程。在本研究中，我们引入了自适应 N-gram 并行解码 (ANPD)，这是一种创新且无损的方法，可通过允许同时生成多个标记来加速推理。 ANPD 采用两阶段方法：首先是使用 N-gram 模块的快速起草阶段，该模块根据当前的交互上下文进行调整，然后是验证阶段，在此期间原始 LLM 评估并确认提议的令牌。因此，ANPD 保留了法学硕士原始输出的完整性，同时提高了处理速度。我们进一步利用 N-gram 模块的多级架构来提高初始草稿的精度，从而减少推理延迟。 ANPD 无需重新训练或额外的 GPU 内存，使其成为高效且即插即用的增强功能。在我们的实验中，LLaMA 等模型及其微调变体的速度提升高达 3.67 倍，验证了我们提出的 ANPD 的有效性。     由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</guid>
      <pubDate>Thu, 25 Apr 2024 16:08:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 旧论文 - 机器学习奖学金中令人不安的趋势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</link>
      <description><![CDATA[我只是想提醒或向新人介绍这篇论文。我认为应该重新开启这个讨论，因为这里的许多人实际上确实影响了该领域的趋势。 https://arxiv.org/pdf/1807.03341&quot;&gt;https:// /arxiv.org/pdf/1807.03341  个人笔记（随意跳过）： 具体来说，我想指出这个问题“Mathiness”，因为这个问题似乎失控了，并且大多数会议的最佳论文都受到了它的困扰（最重要的 ML 论文之一试图数学化，但引入了一个大错误，我相信其他论文有更大的问题，但没有人费心去检查）。 所以这是我个人对学者和研究人员的观点：  我们（我认为大多数将会涉及），从业者不需要方程来知道什么是召回率，并且显然不想阅读难以理解的线性回归版本，这只会让你的论文毫无用处。如果您不想浪费我们的时间，请将其放入附录或完全删除。 审稿人，请不要对不必要的数学印象深刻，如果它很复杂并且没有任何用处，谁关心吗？而且，无论如何它都可能有缺陷，您可能不会发现它。    由   提交/u/pyepyepie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</guid>
      <pubDate>Thu, 25 Apr 2024 15:50:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] UAI-2024成绩等候区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</link>
      <description><![CDATA[审查阶段之后(旧帖子），为像我这样等待决定的其他人创建一个线程。 祝一切顺利！  &amp;# 32；由   提交 /u/PaganPasta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</guid>
      <pubDate>Thu, 25 Apr 2024 15:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 没有进行分层训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</link>
      <description><![CDATA[在我看来，由于残差路径，无论变压器层/块如何，流向每一层的梯度都是相同的。示例： ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X))) ... ） 由于 ProjectionAndCost 的输入只是所有层和初始嵌入的输出之和，因此到达 L1 层的梯度与到达 L2 或 L3 的梯度相同。 因此我们可以：  首先仅训练 L1：ProjectionAndCost(X + L1(X)) 冻结 L1，包括 L2 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X))) 冻结 L1 和 L2，包括 L3 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X)))) ..依此类推  我们不能先训练L2 然后是 L1，因为 L2 的输入取决于 L1，但我们可以先训练较低层，然后逐渐添加和训练更深的层。这种方法有什么问题吗？   由   提交/u/kiockete  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</guid>
      <pubDate>Thu, 25 Apr 2024 14:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>