<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Sat, 12 Apr 2025 15:16:26 GMT</lastBuildDate>
    <item>
      <title>[D]高级NLP资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxinyb/d_advanced_nlp_resources/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在AI中完成主人，并希望在一家大型科技公司的职位上找到职位，理想地从事LLMS工作。我想开始为将来的面试做准备。上个学期，我根据丹·贾拉夫斯基（Dan Jurafsky）和詹姆斯·H·马丁（James H.虽然我发现这是对该领域的很好的介绍，但现在我对书中涵盖的所有内容感到充满信心。 您是否有建议对更多高级书籍的建议，还是建议您专注于理解有关该主题的最新研究论文？另外，如果您有任何一般建议在此领域准备工作面试，我很想听听！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tech-trekker    href =“ https://www.reddit.com/r/machinelearning/comments/1jxinyb/d_advanced_nlp_resources/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxinyb/d_advanced_nlp_resources/</guid>
      <pubDate>Sat, 12 Apr 2025 14:34:10 GMT</pubDate>
    </item>
    <item>
      <title>[n] Google开放，让企业自我主机SOTA模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从主要参与者中，这听起来像是一个很大的变化，并且大多会为企业提供有关数据隐私的有趣视角。 Mistral在Openai和Anthropic维护更多封闭式产品或通过合作伙伴时已经做了很多事情。   https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-models-in-their-own-data-centers.html     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxin3q/n_google_open_to_to_let_tto_tto_entreprises_erse_host_host_sota/”&gt; [link]   [commist]     ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</guid>
      <pubDate>Sat, 12 Apr 2025 14:33:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] D1：通过增强学习在扩散大语模型中扩展推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   最近的大型语言模型（LLMS）已证明了强大的推理能力，可以从在线增强学习（RL）中受益。这些功能主要在从左到右的自回归（AR）一代范式中证明。相比之下，基于扩散的非运动范式以粗到精细的方式产生文本。尽管与AR相比，最近基于扩散的大语言模型（DLLM）已经达到了竞争性语言建模性能，但尚不清楚DLLM是否也可以利用LLM推理的最新进展。为此，我们提出了D1，这是一个框架，可以通过有监督的Finetuning（SFT）和RL的组合将预先训练的戴上DLLM适应推理模型。具体而言，我们开发并扩展了技术以改善预验证的DLLM中的推理：（a）我们利用蒙版的SFT技术直接从现有数据集中提炼知识并灌输自我提高行为，（b）我们引入了一种新颖的无评论，策略级别的RL算法，称为DIFFU-GRPO。通过实证研究，我们研究了不同的训练后食谱对多个数学和逻辑推理基准的性能。我们发现D1可以产生最佳性能，并显着提高了最先进的DLLM的性能。  在扩散扩散大语模型上，使用强化学习来缩放扩散模型。当涉及到实际上原因的语言模型时，绝对需要注意！ 纸链接：  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/hiskuu     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</guid>
      <pubDate>Sat, 12 Apr 2025 10:30:15 GMT</pubDate>
    </item>
    <item>
      <title>[d]您如何称之为包括火车套装的基准？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxcnlb/d_what_do_you_call_a_benchmark_that_includes/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  一个基准用于一次评估多个任务的模型。您如何称呼一组数据集，这些数据集都是旨在训练（和评估）相同模型的数据集？ 编辑：我只是在寻找一个单词，以收集多个具有培训和测试集并且单一发布的任务中的数据集。我本来将其称为基准，但这仅用于评估，也不是用于培训。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neuralbeans   href =“ https://www.reddit.com/r/machinelearning/comments/1jxcnlb/d_whwhat_do_do_do_you_call_call_call_a_benchmark_that_includes/”&gt; [link]   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxcnlb/d_what_do_you_call_a_benchmark_that_includes/</guid>
      <pubDate>Sat, 12 Apr 2025 08:28:41 GMT</pubDate>
    </item>
    <item>
      <title>[r]建立大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxc197/r_building_a_large_language_model/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好， 我已经从事这个项目工作了一段时间，从头开始实现因果语言模型。这个项目对我来说更像是一项研究，而不是试图建立下一个聊天的GPT，这是由于硬件限制的主要原因。 核心体系结构     multiHeadateention.py          掩盖了自动启动     feedforward.py    在两层位置上是feed-fordward网络（gelu activation）。    在注意力下独立处理每个标志。 DecoderBlock.py  Combines MultiHeadAttention and FeedForward layers with:  Layer normalization and residual connections. Dropout for regularization.   Decoder.py  Stacks num_layers DecoderBlock instances. Applies final layer normalization to stabilize outputs.  GPT.py(Main Model)  Token/Position嵌入：使用预审预测的gpt-2嵌入式（ wte 和 wpe ）。   解码器：通过堆叠的解码器嵌入过程。    自动回应生成（main.py）      generate_text（）： 使用 top-k smpling 用于受控的文本生成。 ＆lt; eos＆gt; 令牌或 max_length 。 依赖于解码器的自动回归掩蔽，以防止将来的标记可见度。                triend＆amp; amp; amp; amp; amp; amp; amp; amp; Data Pipeline  GPTDataset.py: Wraps tokenized inputs/targets into PyTorch Dataset, shifting tokens for autoregressive training (inputs = tokens[:-1], targets = tokens[1:]).   train.py ： 加载wikitext数据集，tokenize texts and of批次。   损失函数： crossentropyloss 用 code&gt; ige&gt; ige&gt;  优化： adamw 用于每个参数的自适应学习率。 适用因果掩模与训练过程中的填充面膜结合。             损失计算：将logits与移动目标进行比较。   向后pass ：ADAMW通过梯度更新重量。    您可以在github 此处。如果您有任何改进的想法，请告诉我，如果您觉得它有用，请考虑给它一颗星星以支持其开发。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/blackrat13     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxc197/r_building_a_large_language_model/</guid>
      <pubDate>Sat, 12 Apr 2025 07:42:45 GMT</pubDate>
    </item>
    <item>
      <title>[p]简单独立tfrecords数据集读取器具有随机访问和搜索功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，在工作中，我们正在使用tfrecords存储大多数数据集。但是不时。我们需要检查数据，以更好地对我们的模型进行更好的预测，例如为了找到特定类等的示例。由于Tfrecord在本质上是顺序的，它们不允许进行标准的随机访问切片。 我决定创建这个简单的工具，该工具允许为Tfrecrods创建一个简单的可搜索索引，以稍后可用于各种数据集分析。  这是项目页面： https://github.com/kmkolasinski/tfrecords-readers-readers-readers-reader  required Dataset can be read directly from Google Storage Indexing of 1M examples is fast and usually takes couple of seconds Polars is used for fast dataset querying tfrds.select(&quot;select * from index where name ~ &#39;rose&#39; limit 10&quot;)  Here is a quick start example from readme：  导入tensorflow_dataset作为tfds＃仅需要下载数据集导入数据集导入tfr_reader从pil导入import Import Import Import Import Importim Impart ipy ipyplot数据集，dataSet_info = tfds.load（tfford_flowers102&#39;，splite =&#39;train =&#39;train for_info = true）索引label = feature [label;]。值[0]返回{bail; bail; bail; bail，&#39;d dataset_info.features; dataset_info.data_dir，＃索引选项，如果已经创建索引fileepattern =;*。限制10&#39;）assert示例== tfrds [rows [; _row_id;]]样本，name = []，[]，[示例）中的示例（示例）（示例）：image = image.open（示例[emampe; image; image＆quot; image＆quot bytes_io [0]）。 samples.append（image）ipyplot.plot_images（样本，名称）   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kmkolasinski     [links]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxbmss/p_simple_starlone_tfrecords_dataset_dataset_reader_with/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</guid>
      <pubDate>Sat, 12 Apr 2025 07:13:28 GMT</pubDate>
    </item>
    <item>
      <title>[d]添加新的词汇令牌 +微调LLMS以遵循说明是无效的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在使用指令调整LLMS和VLMS进行实验，要么将新的专用令牌添加到其相应的令牌/处理器中，或者不添加。设置是典型的：掩盖说明/提示（仅参加响应/答案）并应用CE损失。但是，没有什么特别的标准SFT。 但是，我观察到了使用其基本令牌/处理器训练的模型与经过修改的令牌训练的模型，对此有更好的验证损失和输出质量...对此有任何想法吗？  （我的hunch：很难增加这些新添加的令牌的可能性，而模型根本无法正确学习）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</guid>
      <pubDate>Fri, 11 Apr 2025 23:42:34 GMT</pubDate>
    </item>
    <item>
      <title>[D]用于产品标题和类别标准化的微调BART  - 仍然不够准确，任何更好的方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在建立一个来自摩尔多瓦各种在线商店产品的价格比较网站。我在约20,000个手动标准化产品标题的自定义数据集上微调了一个BART模型，并损失了0.013。 I also trained a separate model for predicting product categories. Unfortunately, the results are still not reliable — the model struggles with both product title normalization and category assignment, especially when product names have slight variations or extra keywords. I don’t have access to SKU numbers from the websites, so matching must be done purely on text. Is there a better approach or model I might be missing?或者也许是专门针对此类问题设计的工具/应用程序？ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mali5k     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwz2k3/d_finetuned_bart_for_for_for_for_product_title_category/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</guid>
      <pubDate>Fri, 11 Apr 2025 19:59:31 GMT</pubDate>
    </item>
    <item>
      <title>[P]我们为LLM建立了类似OS的运行时间 - 好奇是否有人在做类似的事情？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，我们正在尝试使用AI本地运行时，该运行时间在2-5秒钟内以llms（例如13b – 65b）来捕捉llms（例如13b – 65b），并且动态运行50多个型号，每gpu始终在记忆中始终居住在ersign中，而不是传统的prial。 GPU执行 +内存状态和点播模型。这似乎解锁了：•实际的无服务器行为（无空闲成本）•低潜伏期时的多模型编排•更好地使用代理工作负载的GPU利用率 是否有人尝试过与多模型堆栈，代理工作流程或动态内存真实分配相似的东西很想听听别人如何接近这一点的 - 或者这甚至与您的中世纪需求保持一致。 很乐意在有用的情况下分享更多的技术细节！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_for_for_llms_curious_if/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_llms_curious_curious_if/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</guid>
      <pubDate>Fri, 11 Apr 2025 18:50:35 GMT</pubDate>
    </item>
    <item>
      <title>[P]一种轻巧的开源模型，用于产生漫画</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</guid>
      <pubDate>Fri, 11 Apr 2025 15:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] CAT：亚季节变压器的循环趋势关注</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwqrud/r_cat_circularconvolutional_attention_for/</link>
      <description><![CDATA[https://arxiv.org/abs/2504.06704 CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and没有引入较重的操作，从而在诸如ImagEnet-1K和Wikitext-103的大规模基准上的幼稚pytorch实现中提供了一致的准确性提高，并且速度约为10％。   &lt;！ -  sc_on--&gt; 32;提交由＆＃32; /u/u/every-act7282     [link]     ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwqrud/r_cat_circularconvolutional_attention_for/</guid>
      <pubDate>Fri, 11 Apr 2025 14:08:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] B200 vs H100基准：早期测试显示高达57％的训练吞吐量和自我托管成本分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jw3b9b/p_b200_vs_h100_benchmarks_early_tests_show_up_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们在Lightly AI最近在欧洲早期访问了NVIDIA B200 GPU，并进行了一些独立的基准测试，将它们与H100相比，重点是计算机视觉模型培训工作量。 We wanted to share the key results as they might be relevant for hardware planning and cost modeling. TL;DR / Key Findings:  Training Performance: Observed up to 57% higher training throughput with the B200 compared to the H100 on the specific CV tasks we tested. Cost透视图（自主）：我们的分析表明，与典型的云H100实例相比，自托管B200可以提供明显较低的OPEX/GPU/小时（我们发现潜在的范围 〜6x-30x便宜，帖子中的详细信息/假设）。 This obviously depends heavily on utilization, energy costs, and amortization. Setup: All tests were conducted on our own hardware cluster hosted at GreenMountain, a data center running on 100% renewable energy.  The full blog post contains more details on the specific models trained, batch sizes, methodology, performance charts, and a breakdown of the cost注意事项：  我们认为，比较新一代的这些早期的现实世界数字可能对社区有用。很高兴在评论中讨论与新硬件的方法，结果或我们的经验！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/igorsusmelj     [link]         &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jw3b9b/p_b200_vs_h100_h100_benchmarks_early_early_tests_show_show_show_show_up_up_to/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jw3b9b/p_b200_vs_h100_benchmarks_early_tests_show_up_to/</guid>
      <pubDate>Thu, 10 Apr 2025 17:18:35 GMT</pubDate>
    </item>
    <item>
      <title>[d] Yann Lecun自动回归LLM注定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jvrk68/d_yann_lecun_autoregressive_llms_are_doomed/</link>
      <description><![CDATA[    src =“ https://b.thumbs.redditmedia.com/0sfsfswahoatmrgchebcapl32wvymuh907cpgd1ccokoq.jpg” title =“ [d] class =“ md”&gt;    不确定还有谁同意，但我认为Yann Lecun在这里提出了一个有趣的观点。好奇地听到有关此的其他意见！ 讲座链接： https://wwwww.youtube.com/watch?v= = etzf = eetzfkkkv6v6v7yy /u/hiskuu     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jvrk68/d_yann_lecun_autoregressive_llms_are_doomed/</guid>
      <pubDate>Thu, 10 Apr 2025 06:44:27 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jpdo7y/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</guid>
      <pubDate>Wed, 02 Apr 2025 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_be_hired/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Mon, 31 Mar 2025 02:30:37 GMT</pubDate>
    </item>
    </channel>
</rss>