<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 02 Feb 2024 15:13:40 GMT</lastBuildDate>
    <item>
      <title>[P] 使用 SMOTEr 进行不平衡回归任务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah5874/p_using_smoter_for_imbalanced_regression_tasks/</link>
      <description><![CDATA[我有 12 个输入，可提供房间中特定位置的地图。我正在预测每个位置的 x、y、z 速度。当位置靠近特定边界时，Z 速度很高，但这仅代表数据集中数据的一小部分，但对边界条件进行建模很重要。如何使用 SMOTEr 对高 z 速度（三个目标变量之一）进行过采样。   由   提交/u/No_Range3026   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah5874/p_using_smoter_for_imbalanced_regression_tasks/</guid>
      <pubDate>Fri, 02 Feb 2024 14:40:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 帮我解决一下这个问题！！！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah55ov/p_help_me_with_this_issue/</link>
      <description><![CDATA[      我正在使用 Google 提供的 Movenet Thunder 模型进行瑜伽姿势估计。该模型在 12 个瑜伽姿势上的准确率达到 93%。 我的下一步是制作一个网站（我正在使用 React），因此我已转换此模型并将其另存为 TensorFlow.js JSON 文件。但问题是：我已经使用 Azure 成功托管了我的模型，并且一切正常，直到我看到这个问题。我知道这与 Keras 和 TensorFlow js 的一些兼容性问题有关，我在网上搜索答案但没有找到任何答案。如果你能帮我解决这个问题，我会很开心！！提前致谢。 https://preview.redd.it/id6caqomo6gc1.png?width=1332&amp;format=png&amp;auto=webp&amp;s=f64c80aef03a82bad63d2802ad0b647 d64d783d7 &lt; /div&gt;  由   提交/u/Annual-Gazelle744  /u/Annual-Gazelle744 reddit.com/r/MachineLearning/comments/1ah55ov/p_help_me_with_this_issue/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah55ov/p_help_me_with_this_issue/</guid>
      <pubDate>Fri, 02 Feb 2024 14:37:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 代码 LLAMA 70B 的免费推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah510a/d_free_inference_for_code_llama_70b/</link>
      <description><![CDATA[是否有提供代码 llama 70B 的免费推理的提供商？我想在将其 lamma.cpp 版本下载到本地之前进行一些测试。   由   提交 /u/kiranp2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah510a/d_free_inference_for_code_llama_70b/</guid>
      <pubDate>Fri, 02 Feb 2024 14:32:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] BertClassification 对于非常长的输入句子。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah4i9e/d_bertclassification_for_really_long_input/</link>
      <description><![CDATA[所以我现在正在尝试解决一个任务，数据集中的输入字符串长度为 10-20K。使用 BertForSeqeuenceClassification 中的分词器处理此问题的最佳方法是什么？ ​ 我已经检查了 LongFormer 和 BigBird，但它们也仅限于 512 和4096 令牌长度。 在这件事上提供一些帮助将不胜感激。   由   提交/u/aMnHa7N0Nme   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah4i9e/d_bertclassification_for_really_long_input/</guid>
      <pubDate>Fri, 02 Feb 2024 14:08:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] LangChain4J人工智能搜索引擎</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah43r1/p_ai_search_engine_with_langchain4j/</link>
      <description><![CDATA[受 search-with-lepton 项目的启发，我刚刚使用 Spring Boot 和 LangChain4J 构建了一个人工智能搜索引擎。 在这个项目中，我也想分享一下我对 RAG 的一些想法。如果您对此感兴趣，请查看这里：https://github.com/vlinx-io/infinite-search   由   提交 /u/Axiomatic_Inspector_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah43r1/p_ai_search_engine_with_langchain4j/</guid>
      <pubDate>Fri, 02 Feb 2024 13:49:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 运行基线的工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah3qil/r_tools_for_running_baselines/</link>
      <description><![CDATA[根据我的经验，实施研究是研究中最糟糕的部分。大学不仅缺乏计算能力，调试机器学习代码也很困难，而且没有实施基线/其他人实验的标准。有些论文从未发布其完整的代码库和重现结果的说明，即使两篇论文在同一数据集上进行评估，它们的数据整理/模型代码也可能完全不同。我最终花了几周的时间才让所有东西都能一起工作。对新数据集进行评估甚至更糟糕，因为您最终不得不进行疯狂的超参数鹅追逐以确保设置公平。 人们运行基线的技术是什么？或者是否没有比自己手动完成所有工作或希望有人已经在另一个项目存储库中完成大部分工作更好的方法了？   由   提交/u/like_a_tensor  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah3qil/r_tools_for_running_baselines/</guid>
      <pubDate>Fri, 02 Feb 2024 13:31:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 语言模型对齐是如何工作的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah383z/d_how_does_language_model_alignment_work/</link>
      <description><![CDATA[我正在阅读有关语言模型对齐的内容，但我不明白的是我们如何找到胜率。我的理解是：&lt; /p&gt;  胜率=(期望响应等级的次数＜非期望响应等级的次数)/总数的总和。数据点   但是我不清楚的是： - 排名在这里意味着什么？ - 我们如何获得排名？ - 如何我们是否确保人类注释者的响应是此处获胜的响应（因为它可能与语言模型的响应不匹配）   由   提交 /u/reallfuhrer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah383z/d_how_does_language_model_alignment_work/</guid>
      <pubDate>Fri, 02 Feb 2024 13:05:55 GMT</pubDate>
    </item>
    <item>
      <title>[P]以最有效的方式为大型数据集生成嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ah2z4b/pgenerating_embeddings_for_a_large_dataset_in_the/</link>
      <description><![CDATA[你好！ 我正在使用 Distill-BERT 为超过 2000 万个不同长度的字符串生成嵌入。长度可以是 10 个单词到 800 个单词之间的任意值。做到这一点最有效的方法是什么？目前，我在一个 GPU 上需要大约 8 小时。 如果我理解正确的话，使用 DPD 主要用于训练，而不是真正的推理。 如果有人可以提供，我将非常感激任何建议或链接。谢谢！   由   提交/u/amrtahnair   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ah2z4b/pgenerating_embeddings_for_a_large_dataset_in_the/</guid>
      <pubDate>Fri, 02 Feb 2024 12:53:04 GMT</pubDate>
    </item>
    <item>
      <title>[2402.00795]法学硕士学习动力系统的控制原理，揭示上下文中的神经标度定律</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agzl6i/240200795_llms_learn_governing_principles_of/</link>
      <description><![CDATA[ 由   提交/u/Elven77AI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agzl6i/240200795_llms_learn_governing_principles_of/</guid>
      <pubDate>Fri, 02 Feb 2024 09:12:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] ImageNet 挑战赛发生了什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agzhzh/d_what_happened_with_the_imagenet_challenge/</link>
      <description><![CDATA[当 ImageNet 挑战赛于 2017 年停止时，宣布将有一个不同的挑战赛，重点关注 3D，但我找不到关于发生这种情况的任何信息。 https://www.newscientist.com/article/2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d/&lt; /p&gt;   由   提交 /u/ksprdk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agzhzh/d_what_happened_with_the_imagenet_challenge/</guid>
      <pubDate>Fri, 02 Feb 2024 09:05:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 修复此 Reddit 子版块的三个简单建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agrxtm/d_three_simple_proposals_for_fixing_this_subreddit/</link>
      <description><![CDATA[我不知道为什么审核团队在这里如此被动，但这里有一些关于如何修复“Reddit 上的机器学习”的建议。   许多 Reddit 子版块要求您在获准发布之前填写某种形式的简短调查。该调查可以询问用户：“您阅读侧边栏了吗？” “这是适合初学者提问的 Reddit 子版块吗？” “对于初学者问题来说，最好的 Reddit 子版块是什么？” “这四个答案中哪一个是张量的最佳定义？” “这四个答案中哪一个是辍学的最佳定义？” 放弃这个 subreddit，因为它无法挽救，但指定一个特定的其他 subreddit，例如 r/LearningMachines 或 r/MLScaling 或 r/ExperiencedML作为指定的“官方地点”供专家前往。将其放在侧边栏中。 鼓励我们的专家监控 r/learnmachinelearning 和其他初学者 Subreddits，所以新手觉得有理由去这些 Reddit 子版块。  我希望看到 Reddit 上的 SOMEWHERE 成为前沿 ML 讨论的中心。    由   提交 /u/Smallpaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agrxtm/d_three_simple_proposals_for_fixing_this_subreddit/</guid>
      <pubDate>Fri, 02 Feb 2024 01:42:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 22 个消费级 GPU 上的分段任意模型 (SAM) 基准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agr8rm/p_segment_anything_model_sam_benchmark_on_22/</link>
      <description><![CDATA[      对分段任意模型 (SAM) 进行基准测试&lt; /h2&gt; 在此基准测试中，我们对来自 COCO 2017 和 AVA 图像数据集。我们评估了代表 22 种不同消费类 GPU 类别的 SaladCloud 上 302 个节点的推理速度和性价比。  为此，我们创建了一个容量为 100 个节点的容器组，其 GPU 类别为“稳定扩散兼容”。所有节点都分配有 2 个 vCPU 和 8GB RAM。这是我们发现的。  在 RTX 3060 Ti 和 RTX 3060 Ti 上，每美元可分割 50K+ 图像。 RTX 3070 Ti https:// Preview.redd.it/c9qt2abwm2gc1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=51b55b9dc00cf8b7dc6919023a5aa9249218e0a0 与较小型号的情况几乎总是一样，最好的成本-性能来自低端 GPU，主要是 RTX 30 系列卡。在这种情况下，我们看到 Ti 卡的性价比显着提升。这是有道理的，因为它们的价格与非 Ti 同类产品相同，但拥有更多 CUDA 核心。这里表现出色的是 RTX 3060 Ti 和 RTX 3070 Ti，每台每美元至少提供 50k 次推理。 特定节点内的推理时间相当一致  https://preview.redd.it/njos0wl2n2gc1.png？ width=1920&amp;format=png&amp;auto=webp&amp;s=b89b4527600350960dbac903274b3407ddfc78d6 放大单个 GPU 类别（RTX 3070 Ti）的性能，我们发现大部分推理时间都落在任何特定节点上的范围都很窄，有一些显着的异常值。我们确实看到不同节点之间存在一些差异，其中一个节点特别糟糕。我们经常看到 Salad 上各节点的性能存在少量差异，因为每台节点都是一台单独的住宅游戏 PC，具有各种不同的 CPU、RAM 速度、主板配置等。 我们的一个异常值节点（31b6，上面圈出的）表示该机器存在异常情况。  RTX 3060 Ti 和 RTX 3070 Ti 提供最佳性价比 运行 Segment Anything Model (SAM) 的 RTX 3060 Ti 和 RTX 3070 Ti 提供极高的性价比批量图像分割解决方案，每美元可分割近 50,000 张图像。  带有更多说明的完整基准测试如下：https://blog.salad.com /segment-anything-model-benchmark/ ​ ​   由   提交 /u/SaladChefs   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agr8rm/p_segment_anything_model_sam_benchmark_on_22/</guid>
      <pubDate>Fri, 02 Feb 2024 01:08:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 围绕“AI”的普遍负面情绪</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agj5y1/d_general_negative_sentiment_surrounding_ai/</link>
      <description><![CDATA[我注意到，每当我向普通人群（通常是非技术人员 -（家人、朋友等））提起人工智能主题时，第一个我想到的是“机器人接管世界并消灭人类”的存在的消极方面、危险和威胁，而不是积极的一面（例如提高效率、自动化、科学等）。需要明确的是，我具体谈论的是人工智能的生存威胁，而不是像大型科技亿万富翁和法团主义这样的经济/政治问题。 这让我想知道 - “人工智能”是否已成为一个伴随着的术语对绝大多数人来说是一个可怕的负面含义吗？这是非常可悲的，我认为这些人中的许多人不知道他们在说什么，他们不明白这些模型是如何工作的，所以他们只是求助于人工智能存在主义者在媒体上推销的任何东西（不是为了淡化危险——我我知道有像 Ilya sutski 和 Geoffrey Hinton 这样的杰出研究科学家担心这些事情）但我觉得现在对人工智能的过度炒作和过度提及确实导致了普遍的技术悲观主义。 TLDR ：“人工智能”越来越多地给普通（非技术）公众带来存在主义的恐惧/负面含义？想法？   由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agj5y1/d_general_negative_sentiment_surrounding_ai/</guid>
      <pubDate>Thu, 01 Feb 2024 19:22:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 传统的 ML/深度学习技术是否已在 NLP 和生产级系统中使用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1agb5rg/d_are_traditional_ml_deep_learning_techniques/</link>
      <description><![CDATA[许多公司正在从他们在几年内开发的机器学习管道转向基于 ChatGPT 的/类似的解决方案。当然，对于文本生成用例来说，这是最有意义的。 但是，许多实际的 NLP 问题可以表述为分类/标记问题。 Pre-ChatGPT 系统过去涉及大量移动组件（关键字提取、超长正则表达式、在嵌入空间中查找最近向量等）。 那么，实际发生了什么？人们是否用 LLM API 替换特定组件？或者整个系统是否被一系列对 LLM API 的调用所取代？基于 BERT 的解决方案还在使用吗？ 现在 ChatGPT API 支持更长的时间和更长的时间。更长的上下文窗口（128k），除了定价和数据隐私问题之外，是否存在基于 BERT 的/其他解决方案能够发挥作用的用例；它不需要像 ChatGPT/LaMDA/类似的 LLM 等模型那样多的计算？ 如果它是上述 LLM 模型不知道的专有数据，那么您将使用自己的模型。但很多用例似乎都围绕着对人类语言本身的一般理解（例如投诉/票证分类/从产品评论中得出见解）。 任何博客、论文、案例研究或其他解决相同问题的文章将不胜感激。我也很想听听您的所有经历，以防您在现实系统中从事过/听说过上述迁移。 这个问题是专门提出的，请记住 NLP 的使用- 案例；但也可以随意将您的答案扩展到其他模式（例如表格和文本数据的组合）。   由   提交/u/101coder101  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1agb5rg/d_are_traditional_ml_deep_learning_techniques/</guid>
      <pubDate>Thu, 01 Feb 2024 13:36:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>