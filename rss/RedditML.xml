<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Fri, 07 Feb 2025 06:26:32 GMT</lastBuildDate>
    <item>
      <title>[d] mab x变压器/llm/rlhf是否有希望的方向？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ijo4qj/d_is_mab_x_transformerllmrlhf_a_promising/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我最近开始在MAB相关领域进行研究，并受到HyperBand和Bastit基于Brandit的神经体系结构搜索的启发，我认为将Bandit算法应用于Transformer/llm/ RLHF可能是一个有希望的方向。这可能包括有效的人类反馈收集，以降低成本并选择最有用的响应，或开发有效的注意力机制和模型选择技术。这些领域仍然很有希望，在顶级会议上易于发表论文，兰德在2025年相对尚未探索，还是已经与许多研究人员的研究人员已经竞争了？我以前没有从事强盗算法，并且听说许多人说土匪研究已经死了（大多数CS部门不再聘请强盗研究人员，许多强盗研究人员已转向运营研究）。或者我应该完全移动到RLHF？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/petrichorinforest     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ijo4qj/d_is_mab_x_transformerllmrlhf_a_promising/</guid>
      <pubDate>Fri, 07 Feb 2025 05:49:45 GMT</pubDate>
    </item>
    <item>
      <title>[d] onnx运行时推理默默默认为cpuexecution -provider</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ijl7r8/d_onnx_runtime_inference_silently_defaults_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用提到的最新版本（ 仍然，该会话不使用GPU，并且默默默认用于在Kaggle Workbook上使用CPU。我在一个项目上处于紧迫的截止日期，并想清除这件令人沮丧的事情。 我还从： https://www.kaggle.com/code/code/prashanttandon/onnx-gpu-inference-tutorial-tutorial ，它似乎对他们有缺陷。 请帮助😩 编辑：我以前很着急，这是版本的输出（这是来自Kaggle Workbook）：请注意，我尚未设置任何环境变量等Kaggle终端。另外，如果有帮助，我正在使用GPU P100加速器。 安装onnxRuntime-gpu版本：！pip install onnxruntime-gpu    导入OnnxRuntime AS AS ORT导入火炬  print（&#39;ort; ort。版本）  print（&#39;cuda：&#39;，torch.version.cuda）  cudnn = torch.backends.cudnn.version（）cudnn_major = cudnn_major = cudnn // 1000 cudnn = cudnn％1000 cudnn_minor = cudnn // 100 cudnn_patch = cudnn％100 print（&#39;cudnn：&#39;，torch.backends.cudnn.version（）） ！ NVCC  -  Version  ！nvidia-smi```` 输出： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2023 NVIDIA Corporation Built on Tue_Aug_15_22:02:13_PDT_2023 Cuda compilation tools, release 12.2, V12.2.140 Build cuda_12.2.r12.2/compiler.33191640_0火炬2.5.1 +cu121 thu 2月6日18:49:14 2025  +--------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------- ----------+ | NVIDIA-SMI 560.35.03驱动程序版本：560.35.03 CUDA版本：12.6 | | ---------------------------------------------------------+--------------- ------------------+ ------------------------------+ | gpu名称持久性m | BUSID DISP.A |挥发性不正确。 ECC | |风扇温度perf PWR：用法/帽|记忆 - 使用| gpu-util计算M | | | | Mig M. | | ================================================== ================+======================== | | 0 Tesla P100-PCIE-16GB关闭| 00000000：00：04.0 OFF | 0 | | N / A 33C P0 30W / 250W | 2969MIB / 16384MIB | 0％默认值| | | | N/A | +--------------------------------------------------------+---------------- -------------------+--------------------------------+ + - ----------------------------------------------------------------------------- ------------------------------------------------+ |过程：| | GPU GI CI PID类型过程名称GPU内存| | ID ID用法| | ================================================= ===================================== | +-------------------------------------------------------------------------- --------------------------------------------------+````  导入onnxruntime AS ort abil_providers = ort.get_available_providers（）也正确输出： [&#39;tensorrtexecutionProvider&#39;，&#39;cudaexecutionprovider&#39;，&#39;cudaexecutionprovider&#39;但是在运行模型时，````提供者= [&#39;cudaexecutionProvider&#39;] ort_session = ort.Inferencesession（onnx_path，path，providers = providers = providers）    ＃ort_session = ort_session = ort ort ort ort ort.ort.int.inferencessession（onnx_path）正在使用“ cpuexecution -provider” ？？？ print（ort_session.get_providers（））  ` - &gt;＆＃32;提交由＆＃32; /u/u/u/kafkacaulfield     [link]  ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ijl7r8/d_onnx_runtime_inference_silently_defaults_to/</guid>
      <pubDate>Fri, 07 Feb 2025 03:05:07 GMT</pubDate>
    </item>
    <item>
      <title>[r]事实证明我们确实需要RNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ijjq5y/r_it_turns_out_we_really_did_need_rnns/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我的最新研究中，我证明了迭代推理框架（如三链链）的融合，我的最后一篇论文上下文反馈回路。我还证明，前馈模型需要一个比复发结构更深入的网络，以达到相同的准确性。这些都是在温和的假设下。 如果您喜欢ML理论，这是一个有趣的读物（我有偏见）。同样，这是论文的要点：   加速收敛：    它的含义：纸张证明当没有持续的噪声时，迭代推理框架以最佳速率（或固定点）收敛到其目标（或固定点），该速率比例为O（1/T^2）。在这里，t表示算法的迭代次数或更新步骤。本质上，当您进行更多迭代时，误差会快速降低。  深度：即使更新过程受到自适应，状态依赖性扰动（小，可能，可能是可能的）在每个步骤中都会改变错误），该方法在适当的平稳性和合同性假设下保持了这种快速收敛速率。在每次迭代中，该过程在最终解决方案方面取得了重大进展，使其在理想（无噪声）方案中高效。       反馈/反复的必要性：    它的含义：分析表明，反馈（或迭代/经常性）架构 - 一个步骤的输出回到下一个步骤中 - 对有效地近似定点函数。固定点函数是重复应用该函数最终导致稳定值（固定点）的功能。  深度：论文表明，使用这种迭代方法，可以使用多个迭代来实现所需的近似值，这些迭代对给定的错误ϵ）进行多个缩放（例如O（1/\ sqrt {ϵ}））。相比之下，馈电模型不会回到自己的输出上，而是在单个正向传递中计算答案，这将需要一个具有指数深度的网络，以匹配相同的准确性水平。这强调了设计系统具有反馈循环以有效处理复杂推理任务的重要性。       &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/jacobfa     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ijjq5y/r_it_turns_out_we_really_did_need_rnns/</guid>
      <pubDate>Fri, 07 Feb 2025 01:48:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] RL在推理模​​型中的理论限制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ijc1zq/d_theoretical_limits_of_rl_in_reasoning_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，大家， 毫无疑问，推理模型表现出色。只要您带来可验证的问题，就可以提高它们的质量。 仍然，理论上的解决能力有一个理论上的限制。当您只教基础模型思考时，您正在做的事情是最大程度地利用其X十亿个参数。而且您无法将无限的信息存储在有限数量的有限精度数字中。 信息有效存储在参数中的信息量取决于模型对其变化的敏感性。通过增加测试时间计算的量，您基本上正在增加模型的（Kolmogorov）熵，因为“想法更长”让模型进一步分歧。因此，我了解为什么推理模型从信息理论的角度来看。 ，但是那里有聪明的人知道我们离理论极限有多远吗？ 1B推理模型是否可以执行和十四行诗3.5？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/schooly_sleep1118     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ijc1zq/d_theoretical_limits_of_rl_in_reasoning_models/</guid>
      <pubDate>Thu, 06 Feb 2025 20:09:56 GMT</pubDate>
    </item>
    <item>
      <title>[D]如何使用VLLM处理并发连接</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ij8ywk/d_how_to_handle_concurrent_connections_using_vllm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想使用vllm提供LAMA 8B模型，我如何与用户实现并发连接（20-30个用户可以将请求发送到API和VLLM不会在没有任何问题的情况下类似地处理它们）。我在文档中找不到这个。如果有人知道在服务时使用什么论点，那真的很有帮助。 另外，哪个gpu具有96 GB VRAM vs 4x GPU，总计为96 GB VRAM，将为我提供更好的吞吐量和用户的连接。 /p&gt; 预先感谢您。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sol1d_007     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ij8ywk/d_how_to_handle_concurrent_connections_using_vllm/</guid>
      <pubDate>Thu, 06 Feb 2025 18:04:59 GMT</pubDate>
    </item>
    <item>
      <title>[d]您怎么知道您正在正确实施数据预处理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ij7ygc/d_how_do_you_know_you_are_implementing_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿。我正在基于Codet5 Paper（ https://arxiv.org/pdf/2109.00859   ）。为了提供一些背景，我的主要目标是最大化我的学习。对于我来说，这基本上是一个玩具项目，可以实现变压器体系结构的各个方面（带某些变体），并在以后进行优化（闪光注意力，分布式培训等）。我来自SDE背景。几个月前，我对ML/LLM感到更加认真，为此，我观看了Andrej Karpathy的所有讲座，并随后他对建筑GPT2进行了实施。 我注意到Codet5并未为Pre-Pre-Pre-Pre提供实施。培训和数据预处理步骤。试图实施预训练的任务（例如识别剂意识到的deo deo deo training，标识符标签等）时，这是很多猜测工作。您如何检查对数据预处理的实施是否正确？我真的很感谢您在这里提供的任何资源。谢谢：D   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tinyeondust     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ij7ygc/d_how_do_you_know_you_are_implementing_data/</guid>
      <pubDate>Thu, 06 Feb 2025 17:23:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPU加速Word2Vec的库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ij7qvb/d_library_for_gpu_accelerated_word2vec/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在进行一个项目，其中我有60多个库存范围从30万到300万个单词，我正在尝试在每个项目上训练一个word2vec。我看着Gensim，但找不到GPU加速度（也许存在，我找不到它）对我该如何快速处理此问题的任何见解？   &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/guywiththemonocle     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ij7qvb/d_library_for_gpu_accelerated_word2vec/</guid>
      <pubDate>Thu, 06 Feb 2025 17:15:28 GMT</pubDate>
    </item>
    <item>
      <title>[d]为LLM推理创建数学/编程领域以外的LLM推理的奖励信号</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ij2dni/d_creating_reward_signals_for_llm_reasoning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我最近一直在学习推理模型，他们似乎面临的最大挑战是：虽然数学和编程对RL有明确的奖励信号，但创意写作等领域缺乏客观指标。研究人员似乎希望推理能力会随着模型的规模而转移，但这感觉不确定。 我很好奇我们如何为创意任务开发奖励信号。我想我们需要一些人类品味/偏好模型，尽管它们有很大的变化，并且缺乏明确的地面真相。  对此主题有任何相关研究吗？我应该阅读任何论文？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/heyhellousername     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ij2dni/d_creating_reward_signals_for_llm_reasoning/</guid>
      <pubDate>Thu, 06 Feb 2025 13:21:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] Deeprag：马尔可夫决策过程框架，用于逐步检索效果的推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iiyobh/r_deeprag_a_markov_decision_process_framework_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   deeprag引入了一种新颖的方法，通过在检索之前和期间实施一个逐步推理过程来检索提升生成。该模型没有立即搜索信息，而是首先将复杂的查询分解为推理步骤，然后对每个步骤执行目标检索。 关键技术点：将推理与检索区分开 *的架构使用中间推理步骤指导精确文档检索 *基于推理上下文实现动态检索策略 *使用专业提示来维护结构化的推理模式   来自论文的结果： * 8.5％在复杂的推理基准与标准抹布上 *降低了事实验证任务上的幻觉率 *与单杆方法相比，多跳上的推理问题更好的性能 *更精确的文档检索 我认为这种方法可能会导致更多可靠的AI系统，需要仔细验证和复杂的推理。分步方法虽然在计算上更密集，但为审核和改进模型决策提供了清晰的途径。这对于准确性至关重要的医疗保健和科学研究中的应用可能特别有价值。 我认为，主要权衡是在提高准确性和增加计算开销之间。比传统的抹布系统自然需要更多的处理时间。组织将需要仔细评估准确性的收益是否证明其特定用例的额外计算成本是否合理。  tldr：DeepRag通过首先通过推理步骤思考，然后为每个步骤执行目标检索来改善抹布。在复杂的任务上显示出更好的准确性，但需要比标准方法更高的计算。 完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iiyobh/r_deeprag_a_markommarkov_decision_process_framework_for/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iiyobh/r_deeprag_a_markov_decision_process_framework_for/</guid>
      <pubDate>Thu, 06 Feb 2025 09:26:52 GMT</pubDate>
    </item>
    <item>
      <title>G [r] PO VRAM对GPU差的要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iiwwcc/grpo_vram_requirements_for_the_gpu_poor/</link>
      <description><![CDATA[   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/comments/1iiiwwc/grpo_vram_requiram_requirements_for_the_gpu_po_po_poor_poor_poor/” GPU Poor&quot; src=&quot;https://external-preview.redd.it/W0Ehcpo0-8qYZpl8LHiCE99fMRsjDpr0W4x-ddzCfnA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96a7e5885c02356de5bc3f2db54dd7793996a01b&quot; title=&quot;G[R]PO VRAM对GPU差的要求”/&gt;    &lt;！ -  sc_off-&gt;  嘿，我花了一些时间来挖掘grpo周末，开始了一堆微调实验。当我看到 trl 库中的GRPO实现很容易使用时，我参加了比赛。我用16GB的VRAM打破了我的小NVIDIA GEFORCE RTX 3080动力笔记本电脑，并迅速开始培训。总的来说，我对使用您提供的奖励功能塑造SMOL模型的能力给我留下了深刻的印象。但是，我最大的收获是，您需要有多种配置，需要多少怪异的VRAM。因此，我在云中旋转了H100，并制作了桌子，以帮助节省OOM错误的痛苦。希望您喜欢！ 完整详细信息： https：// www.oxen.ai/blog/grpo-vram-requirentess-for-the-gpu-poor   只向我展示用法： 上面的所有运行都是在H100上完成，因此在这里意味着＆gt; 80GB。顶部行是参数计数。   https://preview.itd.it/4hjjzrf5xghe1.png?width=6304&amp; amp; amp; format = png＆amp; amp; amp; am ＃32;提交由＆＃32; /u/u/u/fallmindless3563     [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iiwwcc/grpo_vram_requirements_for_the_gpu_poor/</guid>
      <pubDate>Thu, 06 Feb 2025 07:12:59 GMT</pubDate>
    </item>
    <item>
      <title>[r]谐波损耗列车可解释的AI模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iioy2i/r_harmonic_loss_trains_interpretable_ai_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  免责声明：不是我的工作！链接到Arxiv版本： https://arxiv.org/abs/2502.01628 内部产物作为相似性度量，而谐波损失使用欧几里得距离。 作者证明，这种替代方法有助于模型在训练期间更快地缩小火车测试间隙。 他们还展示了其他好处，例如驱动权重以反映班级分布，使其可解释。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fliiiiiiip     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iioy2i/r_harmonic_loss_trains_interpretable_ai_models/</guid>
      <pubDate>Thu, 06 Feb 2025 00:00:36 GMT</pubDate>
    </item>
    <item>
      <title>[d] TTS和Stt如何发展？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iilq85/d_how_are_tts_and_stt_evolving/</link>
      <description><![CDATA[在LLM在这些领域被卡住时迅速发展吗？ 不要误会我的意思，所有这些项目都在做什么，这是下一个一代，这可能是不可思议的  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hanssepp     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iilq85/d_how_are_tts_and_stt_evolving/</guid>
      <pubDate>Wed, 05 Feb 2025 21:41:33 GMT</pubDate>
    </item>
    <item>
      <title>[n] DeepSeek对他们的R1型号进行了多大的培训，以及今天如何对Frontier LLM进行培训。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://www.youtube.com /watch？v = aafantern84    Lex Friedman最近发布了一次访谈，称为“ DeepSeek的GPU优化技巧”。幕后很棒的幕后，即使他们没有太多的GPU和美国同龄人，DeepSeek的最新模型也是如此。 的必要性是发明的母亲，而DeepSeek的几件事是少数事情。 did  -    他们的专家配置混合物具有创新性，其中很高的稀疏系数为8/256的专家正在激活。这比其他8个专家中有2个激活的模型要高得多。  训练这个模型可能很难，因为只有少数专家实际学习任务并被激活，从而使模型变得虚弱。他们引入了辅助损失，以确保所有专家在所有任务中都使用，从而导致强大的模型。 专家模型混合的挑战是，如果只有少数专家激活，那么只有少数GPU，那么在其余的静置时，可能会用计算过载。辅助损失也可以防止这种情况发生。 他们走得更远，并实施了自己的NVIDIA NCCL通信库，并使用更近的汇编PTX指令来管理GPU中的SM SM的计划。每个操作。如此低级的优化导致其模型在有限的硬件上的高性能。  他们还谈论了研究人员如何通过新的模型体系结构和数据工程步骤进行实验。他们说，在训练期间发生的损失曲线中有一些尖峰，很难确切地知道原因。有时在训练后它消失了，但有时ML工程师必须从较早的检查站重新启动培训。 他们还提到了Yolo的运行，研究人员将专门提供所有可用的硬件和预算，以尝试获得前沿模型。他们可能会在此过程中获得非常好的型号，或者浪费数亿美元。  这次采访实际上是一个非常好的深度，落后于今天训练Frontier LLM的现场。我喜欢它，建议您也检查一下！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ml_guy1     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/comments/1III013/N_HOW_HOW_DEEPSEEK_TRAIND_THEIR_THEIR_THEIR_THEIR_MODEL_MODELS_AND_AND_AND_HOW/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/</guid>
      <pubDate>Wed, 05 Feb 2025 19:09:48 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客等对于产品和服务。 请不要发布链接缩短器，链接聚合器网站或自动订阅链接。    任何滥用信托的滥用都会导致禁止。 鼓励其他人创建新的帖子，以便在此处发布问题！ 线程将一直活着直到下一个，因此请继续发布标题之后的发布。    meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/automoderator     [link]  ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Feb 2025 03:15:24 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;      对于那些寻找工作的人请使用此模板  &lt; p&gt;想要被录用：[位置]，薪水期望：[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简短概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，这个社区是适合有经验的人。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/automoderator     [link]   ＆＃32;  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>