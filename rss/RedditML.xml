<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 17 Feb 2024 21:11:13 GMT</lastBuildDate>
    <item>
      <title>[D] 时间序列的自监督/无监督方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atbpc6/d_selfsupervisedunsupervised_approaches_for_time/</link>
      <description><![CDATA[什么是时间序列的自监督/无监督方法？在应用任何监督学习进行预测应用之前，我想学习时间序列数据的低维表示（特征和时间之间的模式/交互）。这不是通常的方法，但我想尝试一下。 我知道自动编码器适用于表格数据。但是，对于输入 [batch_size、sequence_length、num_features] 的时间序列数据，我应该怎么做，除了自动编码器之外还有其他工具吗？或者自动编码器可以吗？   由   提交 /u/Then_Passenger_6688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atbpc6/d_selfsupervisedunsupervised_approaches_for_time/</guid>
      <pubDate>Sat, 17 Feb 2024 20:38:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于硕士项目的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atan85/d_advice_regarding_master_programme/</link>
      <description><![CDATA[大家好，我已被多个硕士课程录取，我想寻求建议，因为我不确定该去哪里。就某些背景而言，我来自欧盟（我没有任何地理偏好），并且我有兴趣在本课程结束后继续攻读人工智能博士学位。我仍然不确定我是否会进入工业界还是留在学术界，所以我想把两扇门都打开。 我已被录取的项目：  硕士多伦多大学应用计算理学硕士 (MScAC)。此外，他们还想提名我获得 Vector 奖学金。 伦敦帝国学院计算（人工智能和机器学习）硕士。 苏黎世联邦理工学院计算机科学硕士。&lt; /li&gt;  我不确定该去哪里，因为所有这些选择对我来说都是一个绝佳的机会。钱在这里不应该是问题，因为无论我最终去哪里，我都希望获得奖学金。 感谢您抽出时间来帮助我！！！   由   提交/u/JavierPaez   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atan85/d_advice_regarding_master_programme/</guid>
      <pubDate>Sat, 17 Feb 2024 19:53:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] vLLM 行为 - 它如何决定何时拒绝请求？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ata64d/d_vllm_behaviour_how_does_it_decide_when_to/</link>
      <description><![CDATA[如果 vLLM 服务器收到太多请求，它会开始拒绝这些请求吗？如果是这样，它如何决定何时开始拒绝请求以及是否有配置方法？   由   提交/u/Stunning-One-4670   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ata64d/d_vllm_behaviour_how_does_it_decide_when_to/</guid>
      <pubDate>Sat, 17 Feb 2024 19:33:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无提示的思维链推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at9w34/r_chainofthought_reasoning_without_prompting/</link>
      <description><![CDATA[论文 - https://arxiv.org/abs/2402.10200  摘要 - 在增强大语言模型（LLM）的推理能力方面，先前的研究主要集中在特定的提示技术上，例如少样本或零样本思维链（CoT） ) 提示。这些方法虽然有效，但通常涉及手动密集型提示工程。我们的研究采用了一种新颖的方法，提出了这样的问题：法学硕士能否在没有提示的情况下有效推理？有趣的是，我们的研究结果表明，只需改变解码过程，就可以从预先训练的 LLM 中导出 CoT 推理路径。我们不是采用传统的贪婪解码，而是研究前 k 个替代标记，发现 CoT 路径通常是这些序列中固有的。这种方法不仅绕过了提示的混杂因素，而且使我们能够评估法学硕士的内在推理能力。此外，我们观察到解码路径中 CoT 的存在与模型解码答案的较高置信度相关。该置信度度量有效区分 CoT 和非 CoT 路径。对各种推理基准的广泛实证研究表明，所提出的 CoT 解码大大优于标准贪婪解码。   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at9w34/r_chainofthought_reasoning_without_prompting/</guid>
      <pubDate>Sat, 17 Feb 2024 19:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练计算如何影响质量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at9nth/d_how_training_compute_influences_quality/</link>
      <description><![CDATA[TL;DR 当使用较少的计算能力（例如 GPU）时，我们是否需要训练更长时间才能达到较高的水平模型质量与通过更好的计算获得的模型质量相当吗？ 嘿伙计们！ 在 Sora 的技术报告，他们表明视频质量通过训练计算显着提高，我试图理解与时间的附加维度的比例关系。 更具体地说，考虑在 G GPU 上训练模型，以便及时A达到一些准确度/质量指标A &gt;t。让我们将 G 减少 x 倍，因此 G&#39; = &lt; strong&gt;G/x，其中 x &gt;&gt; ; 1. 通过此设置，我们仍然希望实现A。根据经验，我们知道t会增加，即t = t * y，其中 y &gt;&gt;&gt; 1. 但是，我们是否知道 f 的估计值：X -&gt; 是？ 所以，这里有两个问题：  是否有可能实现A 与 G&#39; 计算？ 如果是这样，t&lt; /em&gt; 爆炸？    由   提交/u/Kingandpawnendgame  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at9nth/d_how_training_compute_influences_quality/</guid>
      <pubDate>Sat, 17 Feb 2024 19:12:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士如何玩电子游戏的现状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at89qb/d_state_of_how_llms_play_video_games/</link>
      <description><![CDATA[      哟！分享我的 YT 频道的最新视频，其中讨论了法学硕士玩《我的世界》等开放世界游戏的最新进展。视频进入了几篇研究论文（Voyager、DESP 等）及其提示框架，并将其与 SOTA RL 算法（如 Dreamer）进行了比较！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at89qb/d_state_of_how_llms_play_video_games/</guid>
      <pubDate>Sat, 17 Feb 2024 18:12:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 需要研究合作伙伴进行人工智能研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at82ym/r_research_partners_for_research_on_ai_needed/</link>
      <description><![CDATA[几个计划： 人工智能和虚假新闻检测 人工智能促进气候变化和健康 &gt;   由   提交 /u/sladebrigade   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at82ym/r_research_partners_for_research_on_ai_needed/</guid>
      <pubDate>Sat, 17 Feb 2024 18:04:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] GRIT（生成表征指令调整）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at7lzn/r_grit_generative_representational_instruction/</link>
      <description><![CDATA[GritLM  &lt; li&gt;设定了新的最先进基准：在大规模文本嵌入基准 (MTEB) 上优于同等大小的所有其他模型，并且在生成任务方面表现出色。 规模很重要：更大的模型（例如 GritLM） 8x7B）优于开放生成语言模型，同时在嵌入任务方面仍然排名靠前。 在不牺牲通用性的情况下实现性能：GritLM 在生成数据或嵌入数据上的训练同样出色，结合了两个领域的优点。 效率升级：通过避免单独的检索和生成模型，将长文档的检索增强生成 (RAG) 速度提高 60% 以上。  文章链接    由   提交 /u/AloneSYD   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at7lzn/r_grit_generative_representational_instruction/</guid>
      <pubDate>Sat, 17 Feb 2024 17:44:21 GMT</pubDate>
    </item>
    <item>
      <title>V-JEPA：Yann LeCun 先进机器智能愿景的下一步 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/</link>
      <description><![CDATA[      博客：https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model- video-joint-embedding-predictive-architecture/ 论文：https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/ ​ 摘要： ​ 本文探讨了特征预测作为视频无监督学习的独立目标，并引入了 V-JEPA，这是一组仅使用特征预测目标训练的视觉模型集合，不使用预训练的图像编码器、文本、负例、重建或其他监督源。这些模型使用从公共数据集中收集的 200 万个视频进行训练，并针对下游图像和视频任务进行评估。我们的结果表明，通过预测视频特征进行学习可以产生多功能的视觉表示，在基于运动和外观的任务上表现良好，而无需调整模型的参数；例如，使用冷冻的骨干。我们最大的模型，仅在视频上训练的 ViT-H/16，在 Kinetics-400 上获得 81.9%，在 Something-Something-v2 上获得 72.2%，在 ImageNet1K 上获得 77.9%。 ​&lt; /p&gt; ​ https://preview.redd.it/uvo0dpwvl6jc1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=3f308732b80a72be3d5ad8ef9542462cf4611b64 V-JEPA 训练视觉编码器通过预测学习的潜在空间中的屏蔽时空区域。   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/</guid>
      <pubDate>Sat, 17 Feb 2024 17:36:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人可以分享他们在 AWS NeuronX (Inferentia2) 上运行推理的经验吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at4zjp/d_can_anyone_share_their_experiences_running/</link>
      <description><![CDATA[ 由   提交/u/coinclink  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at4zjp/d_can_anyone_share_their_experiences_running/</guid>
      <pubDate>Sat, 17 Feb 2024 15:51:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] - 奖项公布：“预测区间竞赛一：出生体重”Kaggle 竞赛共有 7 本书获奖。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at2oke/d_prizes_announcement_there_are_7_books_to_be_won/</link>
      <description><![CDATA[奖品公布：“预测区间竞赛一：出生体重”Kaggle 竞赛共有 7 本书获奖。感谢 Packt Publishing 的慷慨资助，七本精彩的书《Python 中应用保形预测实用指南》将被授予 本次比赛（截止日期为 3 月 22 日）：  第一名和第二名私人 LB 获奖者：向每名个人提供平装本 第三名和第四名私人 LB 获奖者：向每名个人提供电子副本（获奖者于 3 月 23 日公布）  还有：  最佳笔记本：平装本 最佳笔记本第二名：电子版 最佳写作：电子版（获奖者将在一周内公布）或稍后在比赛结束后留出时间撰写比赛或发布作品）  https://www.kaggle.com/competitions/prediction-interval-competition-i-birth-weight/discussion ​   由   提交 /u/predict_addict   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at2oke/d_prizes_announcement_there_are_7_books_to_be_won/</guid>
      <pubDate>Sat, 17 Feb 2024 14:03:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要建议：使用 NER 或其他模型自动处理德国发票</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aszujx/d_advice_needed_automated_processing_of_german/</link>
      <description><![CDATA[您好， 我在一家德国保险公司工作，希望从以 PDF 形式收到的客户发票中自动提取数据。我们对发票号码、日期、名称、地址和带有价格的行项目等详细信息特别感兴趣，旨在将此信息输出为 JSON 以便进一步处理。这些实体可能出现多次或根本不出现。 我们尝试了多种方法但没有成功：  GPT-4 和各种模型 ：没有始终如一地提供结构化 JSON 输出。 发票的 Impira/LayoutLM：难以准确地区分开单人和收件人。  给出我们需要在本地处理这些数据（出于隐私和安全原因），并且考虑到这些发票是德语的，我们正在探索所有选项，包括命名实体识别（NER），尽管它不是法学硕士进步的最新进展。&lt; /p&gt;  有人对适合处理德国发票的预训练模型或方法有建议吗？ NER 可能是一个可行的选择，或者我们应该考虑其他技术或模型吗？  感谢这个社区可以提供的任何建议或见解！ &lt;!-- SC_ON - -&gt;  由   提交 /u/4AVcnE   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aszujx/d_advice_needed_automated_processing_of_german/</guid>
      <pubDate>Sat, 17 Feb 2024 11:26:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPU 服务器替代方案：如何避免偶尔使用的高成本？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1asnu8l/d_gpu_server_alternatives_how_to_avoid_high_costs/</link>
      <description><![CDATA[租用具有 GPU 支持的专用服务器可能会很昂贵，尤其是当模型具有数十亿个参数时。根据我的计算，使用 AWS 等工具，每年的成本约为 2 万美元 - 假设服务器每小时 2 至 3 美元。我正在训练一些模型，我想在网络应用程序中使用它们。如果网络应用程序成功，那么这 2 万美元就花得很值，但如果不成功，那么就需要付出很多代价。理想的解决方案是让我只需支付使用费。  以下是我考虑过的一些选择。  租用专用服务器（AWS、Azure、Google 等...）：成本很高，例如 2 美元或 3 美元每小时满足我的需要。 抱脸：每小时的费率仍然以美元为单位，就像其他大型云提供商一样。 使用 google collab 笔记本并运行单元作为服务器：我必须保持笔记本打开以保持服务器运行，否则网络应用程序将无法工作 复制：有使用定价，但我相信他们不会处理请求批次。模型通常具有批量维度，并且可以处理数百或数千个同时预测，只要这些请求按批次排队而不是在进入时执行。但我相信复制不会这样做。它也不允许我缓存神经网络的状态，就像在使用因果变换器模型的下一个令牌预测中一样，您可以在每一层缓存先前令牌的先前状态并重用它们来预测下一个令牌，从而降低复杂性到 O(window_size**2) 到 O(window_size)。  我认为我需要的是带有 GPU 的专用服务器，我可以根据需要进行自定义，但只能运行当它收到请求时。有谁知道这个问题有一个好的解决方案吗？ ​   由   提交/u/lildaemon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1asnu8l/d_gpu_server_alternatives_how_to_avoid_high_costs/</guid>
      <pubDate>Sat, 17 Feb 2024 00:04:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴模型演练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</link>
      <description><![CDATA[我真的很喜欢曼巴论文，但它对我来说这不是一本特别容易读的书，因为我之前几乎没有接触过很多先决条件材料（状态空间建模、并行扫描等）。 我写了一个解释器（链接 此处），我很好奇人们是否有任何反馈或认为它有帮助/有趣。 这在一定程度上是为了巩固我自己的理解，但也是我希望对社区有好处的事情，因为关于 Mamba 架构的教程并不多。 &lt; !-- SC_ON --&gt;  由   提交 /u/_james_chen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</guid>
      <pubDate>Fri, 16 Feb 2024 17:46:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>