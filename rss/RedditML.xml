<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Thu, 27 Feb 2025 06:26:04 GMT</lastBuildDate>
    <item>
      <title>[d]产品图像的建议比较控制仓库盗窃</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iz9cod/d_recommendation_for_products_images_comparison/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以我有很多选择器。我们从客户那里购买东西，Picker去拿起它并将其放在仓库中。但是，已经有很多偷窃和篡改产品。即使有时他们会通过放置相同的名字来替换昂贵的东西，并用当地的东西代替。 我想要的东西像Picker必须在客户门口和仓库中拍摄所有角度的产品形式，然后使用这些图像，然后使用这些图像，我可以获得是否已篡改prouduct的信息是否已篡改…   pls建议我的某些解决方案。只要给我正确的结果并减少盗窃。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/forky_friendship_557     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iz9cod/d_recommendation_for_products_images_comparison/</guid>
      <pubDate>Thu, 27 Feb 2025 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR25决定已经解决了！！！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iywoah/d_cvpr25_decisions_are_out/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在此处讨论。官方高音扬声器句柄刚刚发布了决定更新！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/utageous_tip_8109      [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iywoah/d_cvpr25_decisions_are_out/</guid>
      <pubDate>Wed, 26 Feb 2025 19:59:09 GMT</pubDate>
    </item>
    <item>
      <title>[P]训练您自己的推理模型-GRPO仅在5GB VRAM上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyv12c/p_train_your_own_reasoning_model_grpo_works_on/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿[ r/machinelearning ]（）（）folks！非常感谢2周前我们的GRPO版本的支持！我们设法使GRPO仅在Qwen2.5（1.5b）的 5GB的VRAM 上工作 - 从上一个Untsloth版本中的7GB下降： https：&gt; https：&gt; the RL recipe behind DeepSeek-R1 Zero&#39;s reasoning, and you can now do it with 90% less VRAM via Unsloth + LoRA / QLoRA!  Due to our newly added Efficient GRPO algorithms, this enables 10x longer context lengths while using 90% less VRAM vs. every other GRPO LoRA/QLoRA具有0降解的实现。 具有标准的GRPO设置，Llama 3.1（8b）20K上下文长度的培训需要510.8GB的VRAM。 However, Unsloth’s 90% VRAM reduction brings the requirement down to just 54.3GB in the same setup. We leverage our gradient checkpointing algorithm which we released a while ago.它可以巧妙地将中间激活卸载到系统RAM异步，同时仅慢1％。此剃须372GB VRAM ，因为我们需要num \ _ generations = 8。我们可以通过中间梯度累积进一步减少此内存使用。 使用Google的免费上下文使用我们的GRPO Notebook，使用Google的免费gpus： href =“ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8B”&gt; llama 3.1（8b）on colab  -grpo.ipynb）以及更多： align =“ left”&gt; metric   unsploth   trl + fa2           training Moregre Cost（GB） align =“左”&gt; 414GB      grpo内存成本（gb）   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      推理20K上下文（GB）   2.5GB  2.5gb  Total Memory Usage 54.3GB (90% less) 510.8GB   Also we made a Guide (with pics) for everything on GRPO + reward functions/verifiers (please let us know of any suggestions): https://docs.unsloth.ai/basics/reasoning-grpo-and-rl Thank you guys once again for all the support.对我们来说意义重大！ ：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/danielhanchen     [links]   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iyv12c/p_train_your_own_rowne_reasoning_model_grpo_grpo_works_on/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyv12c/p_train_your_own_reasoning_model_grpo_works_on/</guid>
      <pubDate>Wed, 26 Feb 2025 18:51:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] Sugaku：基于数百万纸例子的培训的探索性数学研究工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyv0jx/p_sugaku_ai_tools_for_exploratory_math_research/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经构建了 sugaku.net ，一个平台，旨在通过AI增强数学研究的平台。它将研究人员与相关论文联系起来，生成想法，并使用大量数学文献来回答问题。 Sugaku is the Japanese word for mathematics, and is a handle I&#39;ve been using for a long time. Try these examples:  询问Peper的内容  - href =“ https://sugaku.net/qna/a4e646bc-ce78-488f-b369-b369-b369-b7e006f767d1/”&gt;询问特定的研究人员  - href =“ https://sugaku.net/current/papergen/idea/719AED36-8DCD-4FD1-A171-B58261D3FC8E/&gt;生成假设的论文Metadata   -    - ＆quord; href =“ https://sugaku.net/oa/w4398767262/”&gt;浏览特定的论文建议 语义搜索，可以找到超出关键字的概念连接 使用矢量嵌入的类似纸张浏览 参考和合作者建议 研究想法生成            我为何建立了这一点：   我为什么要构建了这一点：     我通常会遇到意外的研究工具，但相关的是相关的连接。当搜索非明显但有价值的参考文献时，我尝试过的其他工具不足。我试图通过对纸元数据和超过700万篇论文和400万作者的参考图进行培训来解决这个问题，并定期通过当前更新。这似乎比我回到我先前关于L功能的博士学位研究和Riemann假设！ 数学研究语料库对AI培训特别有价值。它是相对独立和结构的，以学习预测参考的方式意味着该模型基本上学习了如何将问题分解为组成部分。通过此过程，系统将学习知识如何将新颖和正确的贡献结合在一起 - 可以很好地转移到帮助研究人员探索和产生新想法的技能。    技术实施：       构建在全面的培训方法上，以  vector  vector   vector    （不舒服，axolotl，直接火炬，洛拉斯，量化），通过Llama-Factory进行完整的参数预处理 当前运行多个基本模型（Llama 8b，Llama 70B，量化Llama 70B，Phi-4，phi-4，Qwen 32b），QWEN 32B） O3-Mini  收集性能数据以确定不同任务的最佳模型   寻找反馈：该网站现场直播 sugaku.net/“&gt; sugaku.net ，但我认为这是一项工作。 I&#39;d appreciate your thoughts on:  Features that would enhance your research workflow Math/ML research areas that need better support Technical suggestions for improving the models or search capabilities  I&#39;m particularly interested in seeing more questions asked, as this helps me build and refine an agent that pulls relevant papers into context for more accurate answers. Thanks for checking它！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/rfurman     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iyv0jx/p_sugaku_ai_ai_ai_for_for_exploration_math_research/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyv0jx/p_sugaku_ai_tools_for_exploratory_math_research/</guid>
      <pubDate>Wed, 26 Feb 2025 18:50:49 GMT</pubDate>
    </item>
    <item>
      <title>[d] n维数几乎是正交的向量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyt374/d_almost_orthogonal_vectors_in_n_dimensions/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  许多文献，尤其是从事表示形式学习的文献，他说“特征”是模型内一些高维空间中的向量，因为我们只能在n维中具有N完美的正交矢量（否则额外的向量将是线性依赖的），这些特征向量几乎是正交的，它几乎是正交的，几乎正交矢量的数量与n呈指数增加。但是我还没有找到一个可以理解的证据（或该指数界限）。一些地方提到了JL引理，但我看不到它是同一件事。有人在此背后有任何直觉，还是可以帮助一些平易近人的证据  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iyt374/d_almost_orthogonal_vectors_in_in_dimensions/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyt374/d_almost_orthogonal_vectors_in_n_dimensions/</guid>
      <pubDate>Wed, 26 Feb 2025 17:32:47 GMT</pubDate>
    </item>
    <item>
      <title>[n]拉格斯：LLM的实时自我完善而无需再培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyszck/n_ragsys_realtime_selfimprovement_for_llms/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们很高兴共享一个名为ragsys的新框架，该框架重新考虑了LLMS的检索增强生成（RAG）。 Instead of simply appending static document chunks to prompts, RAGSys dynamically builds a database of few-shot examples, instructions, and other contexts, and optimizes its retrieval to compose prompts that have the highest chance of yielding a good response. Here’s the core idea:  Dynamic Context Composition: Retrieve not only documents but also few-shot examples and instructions, forming a prompt that’s optimized for each unique query. Utility-Driven Optimization: Rather than relying solely on similarity, the system measures the utility of each retrieved context—prioritizing those that actually improve response accuracy. Feedback Loop: Every interaction (query, response, outcome) is stored and used to amend the few-shot示例和说明，并调整猎犬。这种连续的，自我提高的循环意味着LLM适应而无需再进行重新训练。  期待您的见解和讨论！ 可以随时查看完整的文章 进行深度潜水。   &lt;！提交由＆＃32; /u/u/crossing_minds     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyszck/n_ragsys_realtime_selfimprovement_for_llms/</guid>
      <pubDate>Wed, 26 Feb 2025 17:28:25 GMT</pubDate>
    </item>
    <item>
      <title>[r]乔什：自我改进的LLM用于工具使用而无需人为反馈</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyoxna/r_josh_selfimproving_llms_for_tool_use_without/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/fkluklvyrwxqqafooffh3ca3ca3m_lmixfpfpfkzxjggksa1y.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = 19c5820d500c76306db83f705244888888ddd5cb71946“ title=&quot;[R] JOSH: Self-Improving LLMs for Tool Use Without Human Feedback&quot; /&gt;   Our team recently released a paper introducing JOSH (Juxtaposed Outcomes for Simulation Harvesting), a self-alignment algorithm that enables LLMs to autonomously improve their tool-using capabilities without human反馈，包括在τ板上。我们还介绍了一个代理工具调用数据集工具，该工具始于Multiwoz。     Josh所做的：  将工具调用用作模拟环境中的稀疏奖励来提取理想的对话转弯   通过横梁搜索探索自己的输出探索自己的输出的模型（当前使用了较小的测试时间范围），该工具是相互访问的（重点范围） li&gt; li&gt; li&gt; li&gt; li&gt; li&gt; li&gt; li&gt;） Llama models to frontier models like GPT-4o)  Key results:  74% improvement in success rate for Llama3-8B on our ToolWOZ benchmark State-of-the-art performance on τ-bench when applied to GPT-4o Maintains general model capabilities on MT-Bench and LMSYS while specializing在工具中，使用  为什么这很重要： 今天的人为公告显示了τbench上的改进，值得注意的是，如何已经如何应用我们的方法来提高其功能！ JOSH offers a general approach that works across model sizes and doesn&#39;t require human feedback - potentially making it more scalable as models continue to improve. We&#39;ve made our code and the ToolWOZ dataset publicly available: GitHub repo Paper: 稀疏的奖励可以自我培训对话代理  好奇地听到社区的想法！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bmlattimer     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyoxna/r_josh_selfimproving_llms_for_tool_use_without/</guid>
      <pubDate>Wed, 26 Feb 2025 14:36:54 GMT</pubDate>
    </item>
    <item>
      <title>机器学习可以真正“概括”，或者我们只是在合成专业方面变得更好吗？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_we/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们谈论ML中的概括，就好像是最终目标一样，模型学习模式会传输跨域。但是，“真正的概括”实际上是在发生，还是我们只是完善了特定于任务的外推？ 经过大量，多样化数据训练的模型不一定是概括的 - 它只是在预定义约束中的模式综合方面变得更好。即使是似乎可以很好地“概括”的变压器，它仍然受训练数据的基本结构的束缚。 那么，ML的真正前沿是关于实现真正的概括的真正前沿，还是接受智能固有地依赖上下文依赖于上下文？如果是这样，ML的未来是关于打破过去数据集限制的未来，还是简单地优化合成智能以提高专业化？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_are_we/”&gt; [link]    [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_we/</guid>
      <pubDate>Wed, 26 Feb 2025 10:43:23 GMT</pubDate>
    </item>
    <item>
      <title>[D]您是否经常需要LLM（例如GPT-4）的结构化输出？如果是这样，您认为哪种用例最高？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iykcdi/d_do_you_frequently_need_structured_output_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在受约束的解码中引起了很大的关注（例如，在claude / gemini / gpt-4中使用xgrammar / json模式，我想知道最需要的是哪种用例（例如，在行业 /业务中的现实情况中，在哪种用例中）？学术界研究仍然围绕“ ner and the Likes”，我相信大多数人不在乎（坦率地说）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/marionberry6884     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iykcdi/d_do_do_do_you_frequally_need_need_need_need_need_otput_output_from//]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iykcdi/d_do_you_frequently_need_structured_output_from/</guid>
      <pubDate>Wed, 26 Feb 2025 10:15:52 GMT</pubDate>
    </item>
    <item>
      <title>[r] FFT反击：自我注意的有效替代品</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iycjkd/r_the_fft_strikes_back_an_efficient_alternative/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  传统的自我注意力以野蛮的o（n²）方式计算成对相互作用，将每个令牌与其他所有标记进行比较。对于长序列而​​言，这种方法效率低下。相反，快速傅立叶变换（FFT）将序列转换为频域。在这里，每个令牌由一组由单一矩阵定义的正交频率组件表示。该表示可以保留通过Parseval定理确保信号的能量，并在O（n log n）复杂性下更快地计算。通过利用经典信号处理原理，FFT提供了一种数学上优雅且可扩展的方式来捕获全球依赖性，这使其成为建模长距离交互作用的有吸引力的替代方法。  i Revisit fnet，该论文最初引入了静态的非线性非线性FFT方法。不幸的是，FNET的表述不仅写得不好，而且缺乏实用应用所需的可扩展性，并且在任何基准测试方面都没有表现出色。相反，我已经完善并优化了该方法，增强了其清晰度，适应性，有效性和非线性。我的方法还胜过许多基准上的经典自我注意力，因为它在频域中（自适应）在频域中运行，利用FFT的有效O（n log n）计算以更有效地捕获长期依赖性。这种改进的方法为传统的自我注意力提供了强大而可扩展的替代方案，使其成为捕获全球依赖性的引人注目的替代者。 编辑：本文的要点是表明我们可以以计算上有效的方式替换自我注意力。也许这不是最好的方法，但这是数学上合理的方法。它为将来的作品留下了很大的空间，并为更多机会打开了大门。这是论文的重点。 代码在本文中，但您也可以在这里找到： href =“ https://arxiv.org/abs/2502.18394”&gt; https://arxiv.org/abs/2502.18394 提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iycjkd/r_the_fft_fft_fft_backs_back_and_and_effficity_alternative/”&gt; [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iycjkd/r_the_fft_strikes_back_an_efficient_alternative/</guid>
      <pubDate>Wed, 26 Feb 2025 02:07:50 GMT</pubDate>
    </item>
    <item>
      <title>[r]预测稀有语言模型行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iya3f7/r_forecasting_rare_language_model_behaviors/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    人类团队找到了一种方法来预测稀有的AI风险，然后使用幂律级缩放。这有助于尽早发现有害反应或未对准的有害反应或未对准的行为，使其在生存之前更安全。  摘要：    标准语言模型评估可能无法捕获仅在部署量表上出现的风险。例如，模型可以在小规模的Beta测试期间产生安全的响应，但在部署时处理数十亿个请求时会透露危险的信息。为了解决这一问题，我们引入了一种方法，以预测跨数量级的潜在风险比我们在评估过程中测试的更多查询。我们通过研究每个查询的启发概率（查询产生目标行为的概率）来进行预测，并证明最大观察到的启发概率可以随着查询数量而扩展。我们发现，我们的预测可以预测各种不良行为的出现，例如协助用户进行危险的化学综合或采取寻求权力的动作 - 最多三个数量级的查询量。我们的工作使模型开发人员能够在大规模部署期间显现出现罕见的失败。   链接到论文： https：//arxiv.org/abs/2502.16797 提交由＆＃32; /u/u/seraschka     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iya3f7/r_forecasting_rare_language_model_behaviors/</guid>
      <pubDate>Wed, 26 Feb 2025 00:09:31 GMT</pubDate>
    </item>
    <item>
      <title>[r] MUON对于LLM培训是可扩展的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ixzj26/r_muon_is_scalable_for_llm_training/</link>
      <description><![CDATA[      tl; dr ： muon 是一种优化算法，是ADAMW的替代品。该报告表明，与Adamw相比，它节省了大约一半的拖鞋，以1.5B LLM在39b代币进行培训。    paper  ： 摘要：   最近，基于基质正交化的MUON优化器在训练小型语言模型方面表现出了很大的结果，但是尚未证明对较大模型的可扩展性。我们确定了两种至关重要的技术来扩展MUON：（1）增加重量衰减，（2）仔细调整参数更新量表。这些技术使MUON可以在大规模培训的情况下开箱即用，而无需进行超参数调整。缩放定律实验表明，与ADAMW相比，MUON具有约2倍的计算效率。基于这些改进，我们介绍了Moonlight，Moonlight是一种3B /16B参数的混合物（MOE）模型，该模型训练了5.7 t使用Muon。与先前的模型相比，我们的模型改善了当前的帕累托前沿，通过更少的训练拖曳来取得更好的性能。我们开源我们的分布式MUON实现，这是内存最佳和沟通效率的。我们还释放了经过预定的，指导和中间检查点，以支持未来的研究。   视觉摘要：      视觉亮点：           https://preview.itd.it/bxekx1ntcble1.png?width=1095＆amp; format = png＆amp; auto = webpp＆s = 508a10fdea89d17a49e619e619b53f88622222260ebd9393939393939384449e619e619e     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sustledwaterMelon     [link]       [注释] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ixzj26/r_muon_is_scalable_for_llm_training/</guid>
      <pubDate>Tue, 25 Feb 2025 16:48:27 GMT</pubDate>
    </item>
    <item>
      <title>[R]分析2024年400多个ML比赛</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ixrxoq/r_analysis_of_400_ml_competitions_in_2024/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我运行mlContests.com，该网站列出了来自多个平台的ML竞赛 -  Kaggle，drivendata，aicrowd，aicrowd，Zindi等… 我刚刚花了几个月的时间来查找上一年的竞赛解决方案，以及我在上一年的竞争中找到了所有信息。  我发现了去年发生的400多场比赛，以及其中70个获胜解决方案的信息。  一些亮点：     kaggle仍然是最大的平台，总奖品比其他平台拥有更大的用户群 - 尽管其他值得保持数十个值得保持的平台值得一提，但值得保持定期有趣的竞争和有意义的奖品，并获得了有意义的奖金。与往年相比，数学奥林匹克运动会，维苏威挑战赛，人工智能网络挑战）。   python仍然是竞争赢家的选择语言，几乎每个人都将python用作他们的主要语言。一位获胜者使用Rust，两名使用R.   卷积神经网继续在计算机视觉竞赛中表现出色，在竞争赢家中仍然比基于变形金刚的视觉模型更为普遍。    pytorch的使用量大于tensorflow ，大​​约是9：1。没有发现任何竞争者在JAX或其他图书馆中实施神经网。   使用Automl软件包有一些竞赛冠军，似乎越来越有用。不过，通才自主的大师级特工的任何主张似乎还为时过早。   在语言/文本/序列相关的竞争中，定量是有效利用有限资源的关键。通常是4-，5或8位。 Lora/Qlora也经常使用，尽管并非总是如此。   促进梯度的决策树继续赢得许多表格/时间序列的比赛。他们通常会喜欢深度学习模型。据我所知，获奖者在2024年没有使用表格/时间序列的预训练基础模型。   开始看到更多的数据范围的极点摄入量，有7个获奖者在2024年使用Porars（从2023年的3次提高），而使用PANDAS则使用Porars。所有使用Polars的人仍然在代码的某些部分中使用了大熊猫。  就硬件而言，竞争获奖者几乎完全使用了NVIDIA GPU来训练他们的模型。一些人仅在CPU上接受培训，或者通过Colab使用了TPU。没有AMD GPU。 NVIDIA A100是获奖者中最常用的GPU。 100万美元以上的奖金泳池比赛中有两个是由使用8xH100节点进行培训的团队赢得的。不过，还有许多其他GPU：T4/P100（通过Kaggle笔记本电脑）或RTX 3090/4090/3080/3060等消费者GPU。一些花费了数百美元在云上计算以训练他们的解决方案。  一种新兴模式：使用生成模型创建其他合成训练数据以增强提供的训练数据。   完整报告中有更多详细信息，您可以在此处阅读（无付费墙）： https://mlconts.com/state-of-machine-com/state-of-machine-learning-competition-competions-competition-competions-competions-grof      xmm4ywg9h9le1... The full report also features:  A deep dive into the ARC Prize and the AI Mathematical Olympiad An overview of winning solutions to NLP/sequence competitions A breakdown of Python packages used in winning solutions (e.g. relative popularity of various梯度增强的树库）  如果您想支持这项研究，我将非常感谢您与其他可能会发现它有趣的人分享。您还可以查看我新发射的在线杂志， jolt ml   - 在顶级ML会议上发表新闻以及长阅读的文章（到目前为止，还有一篇！）。  感谢竞争获奖者分享了有关其解决方案的信息，也感谢竞争平台分享了有关比赛的高级数据。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hcarlens   href =“ https://www.reddit.com/r/machinelearning/comments/1ixrxoq/r_analsisy_of_400_ml_ml_competitions_in_2024/”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ixrxoq/r_analysis_of_400_ml_competitions_in_2024/</guid>
      <pubDate>Tue, 25 Feb 2025 10:28:11 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子以便在此处发布的新帖子的人！ 线程将活着直到下一个，因此请继续发布标题的日期之后。 感谢大家在上一个线程中回答了上一个线程中的问题！  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iwdbgs/d_simple_questions_thread/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 23 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>