<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Thu, 13 Jun 2024 15:15:40 GMT</lastBuildDate>
    <item>
      <title>[D] 当今最优秀经纪人面临的最大瓶颈是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df12n1/d_what_are_the_biggest_bottlenecks_of_todays_best/</link>
      <description><![CDATA[我还不能告诉我的人工智能“控制我的桌面并尝试创建不同的网站/应用程序来赚钱”，一个月后回来发现它创建了一个拥有 1000 名新闻通讯订阅者的网站和一个下载量为 10,000 次的免费增值游戏应用程序。 当今最好的代理中阻碍这一点的最大弱点是什么？    提交人    /u/Z3F   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df12n1/d_what_are_the_biggest_bottlenecks_of_todays_best/</guid>
      <pubDate>Thu, 13 Jun 2024 15:00:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为所有想要测试生成式 AI 的人提供 GPU</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df0nti/d_gpu_for_everyone_who_wants_to_test_generative_ai/</link>
      <description><![CDATA[您好，我是 Julien GAUTHIER，www.arkanecloud.com 的创建者 – 用户可以在这个网站上为他们想要在云端开发的任何 AI 应用程序部署 GPU。 您只需选择您的 GPU，就可以在其上部署生成式 AI。 我们用 Llama3、Stable 扩散（SD-3 即将推出！）和许多其他模板构建了一些模板。 请随时给我们您的反馈😊 谢谢！    提交人    /u/ArkaneCloud   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df0nti/d_gpu_for_everyone_who_wants_to_test_generative_ai/</guid>
      <pubDate>Thu, 13 Jun 2024 14:42:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 处理不平衡数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df0cg7/d_dealing_with_imbalanced_dataset/</link>
      <description><![CDATA[您好。我正在尝试解决的问题之一是处理高度不平衡的数据集。分类任务中有近 200 个类别，而我的数据中有超过 30% 的数据属于单个类别。在使用标准交叉熵损失使用此数据训练模型时，模型开始再次仅预测单个类别。我使用了简单的正则化方法来缓解这种情况，并在验证数据集上实现了 91% 的准确率，每个类别的样本分布几乎相等。那么，处理这个问题的更有效方法是什么？    提交人    /u/SmartEvening   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df0cg7/d_dealing_with_imbalanced_dataset/</guid>
      <pubDate>Thu, 13 Jun 2024 14:28:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用自然语言和概率推理进行实验并修改规则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dezijo/r_doing_experiments_and_revising_rules_with/</link>
      <description><![CDATA[  由    /u/floppy_llama  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dezijo/r_doing_experiments_and_revising_rules_with/</guid>
      <pubDate>Thu, 13 Jun 2024 13:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 微调 LL.M 数据考量疑点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dezdkf/r_finetuning_llm_doubts_for_data_consideration/</link>
      <description><![CDATA[我们正在尝试对银行交易的基础 llm 进行微调，我们应该如何考虑数据？ 过去 1 年交易量最大的客户 过去 3 年交易量最大和中等的客户组合 考虑过度拟合之类的事情吗？    提交人    /u/the_MadMax   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dezdkf/r_finetuning_llm_doubts_for_data_consideration/</guid>
      <pubDate>Thu, 13 Jun 2024 13:45:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] NLP 中的否定是一个已解决的问题吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1devevc/d_is_negation_in_nlp_a_solved_problem/</link>
      <description><![CDATA[我以为事实并非如此，而且存在问题，而且问题无处不在。但有人声称：  否定是扩散中已解决的问题，相同的原理将扩展到早期融合多模态模型。  我需要理解这一点。这种说法合理吗？如果这是真的，是否有论文、出版物可以转发给我？ 谢谢！    提交人    /u/Beginning-Ladder6224   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1devevc/d_is_negation_in_nlp_a_solved_problem/</guid>
      <pubDate>Thu, 13 Jun 2024 10:06:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenMetricLearning 3.0 统一支持图片和文字！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</link>
      <description><![CDATA[      大家好！ 我想分享 OpenMetricLearning 3.0 的发布！  OML — 是一个用于表示学习和检索的库，其中包含大量模型、损失、矿工、采样器、指标和其他有用的东西，如 DDP、与 PyTorchLightning 和 PyTorch Metric Learning 的集成、不同的实验跟踪器等。   有什么新内容？ * 我们已经添加了文本支持，现在我们正在添加音频！（用户不仅已经将 OML 用于图像，而且现在我们还提供开箱即用的支持、测试和示例。） * 代码统一适用于图像、文本，并且适用于声音！我邀请您查看图像和文本的并排比较。 * 检索部分已分离，可用于模型验证和推理，并进行以下重新排名或其他后期处理。 * 库的功能已在一个地方描述，以便于导航，并且我们总体上改进了文档和示例。 * 一些计算，特别是与内存相关的计算，已经进行了优化。 我们欢迎潜在的贡献者： * 代码变得更加模块化，因此入门门槛降低了 - 您可以获取单独的代码并继续进行它。 * 我们还用我们的问题/任务更新了board。 您在GitHub上的⭐️极大地帮助了我们进一步发展！ OML    提交人    /u/Zestyclose-Check-751   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</guid>
      <pubDate>Thu, 13 Jun 2024 09:06:14 GMT</pubDate>
    </item>
    <item>
      <title>设计最佳栖息地[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deufeq/designing_the_optimal_habitat_p/</link>
      <description><![CDATA[看到最近一篇关于设计最佳伐木斧的帖子，我想起了我一直想做的一个项目。使用深度学习模型设计最佳、最优的栖息地，比如既有干湿部分的水族馆，又有热梯度的爬行动物围栏。有人知道与此相关的任何工作吗？提前致谢！    提交人    /u/TanukiBigBalls69   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deufeq/designing_the_optimal_habitat_p/</guid>
      <pubDate>Thu, 13 Jun 2024 08:57:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何为机器学习任务准备 TB 级数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1desvs5/d_how_to_prepare_tbs_of_data_for_ml_tasks/</link>
      <description><![CDATA[我目前必须预处理（主要是清理）几 TB 的图像，之后这些图像可用于机器学习。这一挑战似乎与拥有大型数据集的公司必须面临的挑战非常相似，例如 OpenAl、Tesla 等。 当处理代码使用 Python 时，您知道如何很好地分布这一过程吗？有没有流行的框架可以实现这一目标？    提交人    /u/That_Phone6702   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1desvs5/d_how_to_prepare_tbs_of_data_for_ml_tasks/</guid>
      <pubDate>Thu, 13 Jun 2024 07:06:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] 微软开源 Recall AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</link>
      <description><![CDATA[我创建了一个开源的 Microsoft Recall AI 替代品。 这会记录您屏幕上的所有内容，并可以使用自然语言搜索。但与 Microsoft 的实现不同，这不是隐私噩梦，现在就可以使用。并带有实时加密 这是一个新的启动项目，需要贡献，因此请希望转到 github repo 并给它一个星星 https://github.com/VedankPurohit/LiveRecall 它是完全本地的，您可以查看代码。而且所有内容始终是加密的，这与 Microsoft 的含义不同，当您登录时，图像会被解密并可能被盗    提交人    /u/Vedank_purohit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</guid>
      <pubDate>Thu, 13 Jun 2024 05:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 能否发明更好的方法来培养 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</link>
      <description><![CDATA[新博客文章和论文： https://sakana.ai/llm-squared/ https://arxiv.org/abs/2406.08414 发现用于大型语言模型的偏好优化算法 摘要 离线偏好优化是增强和控制大型语言模型 (LLM) 输出质量的关键方法。通常，偏好优化被视为使用手工制作的凸损失函数的离线监督学习任务。虽然这些方法基于理论见解，但它们本质上受到人类创造力的限制，因此可能的损失函数的大量搜索空间仍未得到探索。我们通过执行 LLM 驱动的目标发现来解决这个问题，以自动发现新的最先进的偏好优化算法，而无需（专家）人工干预。具体而言，我们迭代地提示 LLM 根据先前评估的性能指标提出和实施新的偏好优化损失函数。此过程导致发现以前未知且性能良好的偏好优化算法。其中表现最好的我们称之为发现偏好优化 (DiscoPOP)，这是一种自适应地混合逻辑和指数损失的新算法。实验证明了 DiscoPOP 的最先进的性能及其成功转移到保留任务。    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</guid>
      <pubDate>Thu, 13 Jun 2024 02:18:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] CycleGAN 为什么有效？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1degbf8/d_why_does_cyclegan_work/</link>
      <description><![CDATA[我知道这是一个不再流行的旧模型，但有人知道任何展示 CycleGAN 工作原理的作品吗？（即这篇论文：https://arxiv.org/abs/1703.10593）。很久以前，我尝试将这篇论文应用于数值问题，但无法让它发挥作用。从两个边际学习条件分布似乎很神奇。它是否有效，因为图像的结构非常独特？如果有人有任何答案或研究，我会很有兴趣了解更多。    提交人    /u/www3cam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1degbf8/d_why_does_cyclegan_work/</guid>
      <pubDate>Wed, 12 Jun 2024 20:13:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] Grokking 的问题解决了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1defvmv/d_is_grokking_solved/</link>
      <description><![CDATA[最近的 Grokfast 论文 发现了一种将算法数据集的 Grokking 速度提高 50 倍的方法。早先的 Omnigrok 论文 确定，对于他们的算法数据集，“在恒定权重范数下的约束优化在很大程度上消除了 Grokking” 这些改进是否意味着现在我们在训练模型时不必担心延迟泛化/grokking（尽管其机制模糊）？    提交人    /u/delorean-88   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1defvmv/d_is_grokking_solved/</guid>
      <pubDate>Wed, 12 Jun 2024 19:55:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我厌倦了 LangChain，所以我制作了一个简单的开源替代方案，支持工具使用和视觉，以尽可能轻松地构建 Python AI 应用程序。（simpleaichat + vision + anthropic 和 gemini）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/</link>
      <description><![CDATA[https://github.com/piEsposito/tiny-ai-client 构建 tiny-ai-client 的动机来自于对 Langchain 的失望，它变得臃肿、难以使用且文档不全 - 并从 simpleaichat 中汲取灵感，但除了 OpenAI（Gemini、Anthropic - Groq 和 Mistral 正在研发中）之外，还增加了对视觉、工具和更多 LLM 提供商的支持。 我构建它是为了延续 simpleaichat 的初衷，不是为了炒作、筹集资金或其他什么，而是为了帮助人们做两件事：尽可能轻松地构建 AI 应用程序，并在无需使用 Langchain 的情况下切换 LLM。 这是一个极简可行的软件包版本，支持视觉、工具和异步调用。还有很多改进要做，但即使在目前的状态下，tiny-ai-client 也普遍改善了我与 LLM 的交互，并已成功用于生产。 让我知道你的想法：仍有一些错误可能需要修复，但所有示例都可以正常工作并且易于适应你的用例。    提交人    /u/lee_from_teashop   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/</guid>
      <pubDate>Wed, 12 Jun 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>