<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 20 Mar 2024 21:13:24 GMT</lastBuildDate>
    <item>
      <title>[D] SMV 用于识别“优质内容”？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjm0r3/d_smv_for_identifying_quality_content/</link>
      <description><![CDATA[是否可以使用支持向量机来识别“优质内容”以实现 SEO 目的。正在寻求好友...   由   提交/u/Downtown_Quality_322   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjm0r3/d_smv_for_identifying_quality_content/</guid>
      <pubDate>Wed, 20 Mar 2024 19:28:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 准确度和损失图上的峰值</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjl787/d_peaks_on_accuracy_and_loss_graph/</link>
      <description><![CDATA[      您好，我使用大约 1000 张图像和 4 个类别的数据集训练模型，正如您所看到的，有很多峰值，这是什么意思？  该模型具有 VGG16 模型的迁移学习，并在其他数据集上取得了良好的准确性，数据集的大小是出现峰值的唯一原因还是可能还有其他因素导致这种情况发生? https://preview。 redd.it/yn3axu34djpc1.png?width=1189&amp;format=png&amp;auto=webp&amp;s=8420d6c4ef00e0f39caf2a813d3f8fe974ab071c   由   提交 /u/Icy_Dependent9199   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjl787/d_peaks_on_accuracy_and_loss_graph/</guid>
      <pubDate>Wed, 20 Mar 2024 18:55:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的团队使用哪些方法/工具来针对您的领域微调法学硕士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjl5sj/d_what_methodstools_do_you_use_in_your_team_to/</link>
      <description><![CDATA[您如何评估自定义 LLM 的数据质量和特定领域的性能，并进行微调以获得最佳输出？    由   提交 /u/metalvendetta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjl5sj/d_what_methodstools_do_you_use_in_your_team_to/</guid>
      <pubDate>Wed, 20 Mar 2024 18:53:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2024 接受去匿名论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_paper_accepted_at_iclr_2024/</link>
      <description><![CDATA[我想开始讨论我在今年 ICLR 上观察到的一个特定案例。 - A 论文提交给ICLR双盲审稿人全名第一作者和完整的致谢（以及一些备受瞩目的参考文献）位于主要论文正文中，位于参考书目的正上方。作者明确确认了虚假陈述“无致谢部分：我证明本次提交的材料中没有供双盲评审的致谢部分。”提交时。 - 四位审稿人和 AC 避免提及这一点，并审阅论文了解作者的偏见，就好像它没有违反征文中列出的基本提交规则。  - 一月中旬发布的论文决定是“桌面拒绝” （推测是由于上述违规行为，确认PC们已经意识到这一点）。此内容未公开存档。 - 2 月初将其更改为“口头”内容。没有进一步的理由，原因不明。 违反匿名规则的行为首先会被评审者忽略，然后程序委员会通过例外情况默默地允许，大概是在直接投诉之后。在我看来，这对于任何会议来说都是不可接受的，尤其是像ICLR这样的领域顶级会议。如果我们不强制执行并允许某些作者选择透露自己的姓名并对审稿人产生偏见，为什么我们还要实行双盲程序呢？如果作者并不出名，或者他们在一个不太享有特权的地方进行研究，或者来自边缘化社区，是否也会有同样的例外？其他在今年 ICLR 上被拒绝但没有获得例外机会的论文又如何呢？ 根据我的经验，学术界关于偏见和诚信的问题经常在私人谈话中提出，但几乎从未公开讨论过，所以我有兴趣听听社区对此案的看法。项目主席拒绝对最终决定发表评论。   由   提交/u/Melodic-Foundation47  /u/Melodic-Foundation47 reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_pa​​per_accepted_at_iclr_2024/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjknyi/d_deanonymized_paper_accepted_at_iclr_2024/</guid>
      <pubDate>Wed, 20 Mar 2024 18:32:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为标准搜索栏重用文档向量，好不好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjjojw/d_reuse_documents_vectors_for_a_standard_search/</link>
      <description><![CDATA[嗨， 我嵌入 JSON 对象，然后为虚拟助手执行 RAG 并获取 N 个最相关的对象放入其中上下文，以便法学硕士可以确定它是否相关并使用该信息。因此，即使 N top 不是完美匹配，LLM 也足够聪明，不会提及它们。 重点是，我考虑过重新使用向量进行目录搜索，因为几乎所有内容都已经设置完毕（尝试，所以也许这是一个坏主意），所以它可能会返回 XX 项。我在匹配分数（距离）上设置了一个固定阈值，以确保没有列出奇怪的东西（我在多次尝试后调整了值），但根据用户输入（有多少个单词......），阈值有时会削减项目这应该是列表的一部分，有时它会让所有不好的项目通过。 这个用例可行吗？我想知道你们中的一些人是否尝试过弹性阈值？ 假设距离有两种情况：  对于查询“A”，[0.1 , 0.2, 0.9] 作为阈值为 0.3 的距离：我得到了预期的 2 件事 对于查询“B”，[0.1, 0.28, 0.29, 0.3]，阈值仍然为 0.3 ：不幸的是，由于查询的变化，这次阈值不好，所有“0.28、0.29、0.3”都只是垃圾。  是否有任何已知的技术可以说“if”存在显着差距”在距离之间，阈值可能应该固定在那里？ 抱歉，如果不清楚。无论如何，我仍然可以对原始文档内容使用基本的数据库搜索（但认为它比数据库向量更强大......）。 谢谢，   由   提交/u/sneko7  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjjojw/d_reuse_documents_vectors_for_a_standard_search/</guid>
      <pubDate>Wed, 20 Mar 2024 17:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia 最新的 Blackwell GPU 将减少多少训练和推理时间/价格？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</link>
      <description><![CDATA[显然训练 Llama-2 花费了约 500 万美元。您认为这些新 GPU 会降低类似模型的训练/推理成本多少？我很好奇，因为我很可能很快就会购买苹果产品。   由   提交 /u/dittospin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjhrxy/d_how_much_will_nvidias_newest_blackwell_gpus_cut/</guid>
      <pubDate>Wed, 20 Mar 2024 16:34:13 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 不到 300 行的 Python + pytorch 从头开始​​稀疏混合专家语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</link>
      <description><![CDATA[大家好，我实现了专家语言模型的稀疏混合（基本上是 Mixtral、Grok-1 和据称的 GPT-4 中使用的微型版本） ）在纯 pytorch 中从头开始，并用小莎士比亚对其进行训练。这主要基于 Andrej Karpathy 的 makemore（仅自回归字符级解码器变压器模型）。我的目标是让它成为一个可破解的实现，人们可以用它来理解它是如何真正工作和改进的。我预计全年会出现越来越多的此类模型。 存储库位于：https://github。 com/AviSoori1x/makeMoE 我几个月前创建了这个并在 Localllama 上分享，但我想我也应该在这里分享，因为我做了一些更新，例如添加专家能力和整合整个实现少于 300 行可读的 python + pytorch。逐步完成此操作的博客位于：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch 。希望这对您有所帮助！   由   提交/u/avi1x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjg04g/project_sparse_mixture_of_experts_language_model/</guid>
      <pubDate>Wed, 20 Mar 2024 15:20:46 GMT</pubDate>
    </item>
    <item>
      <title>[D]寻找LLM API（openrouter、openai et simila）的GUI，带有插件和RAG支持。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/</link>
      <description><![CDATA[大家好， 我正在寻找一个支持 OpenAI 的 ChatGPT API（或像 OpenRouter 那样兼容）的用户友好型 GUI并允许插件和 RAG。我使用开源模型（mixtral、Hermes 34b）和私有模型，例如 claude-opus 和 perplexity-sonnet 这里有人有一些建议吗？  提前感谢您的帮助！ 编辑：我想知道是否存在任何平台可以提供修改聊天历史记录或基于聊天事件运行代码的功能。   由   提交 /u/Distinct-Target7503   /u/Distinct-Target7503 reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bje0bu/dsearching_for_a_gui_for_llms_apis_openrouter/</guid>
      <pubDate>Wed, 20 Mar 2024 13:54:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为 ICML 2024 提交了 0 条评论 - 还有其他人面临这个问题吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bjdyax/d_0_reviews_submitted_for_icml_2024_someone_else/</link>
      <description><![CDATA[大家好，我向 ICML 2024 投了一篇论文，作者反驳阶段应该明天开始，但是到今天好像还没有审稿人提交审稿然而。有人面临类似的问题吗？如果没人评论怎么办？   由   提交 /u/Tigmib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bjdyax/d_0_reviews_submitted_for_icml_2024_someone_else/</guid>
      <pubDate>Wed, 20 Mar 2024 13:52:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么学术论文的可读性一直不好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</link>
      <description><![CDATA[  其中一些期望是不可避免的。论文绝对必须假设该领域特有的广泛背景知识和词汇——包括每篇论文对该领域的基本介绍都是多余的。有时论文假设了太多背景，但作为作者或读者很难判断，尤其是因为人们知道的事情会随着时间和不同背景而变化。 参考 在过去的3年里，为了拓宽我对ML领域的理解，我读了很多论文，但没有考虑过专注于某个领域特定领域的特定问题。   这使我能够理解学术写作的结构、机器学习的基本概念以及与以前的知识相比相对宝贵的见解。过去，但当我遇到新论文时，有些论文仍然很冗长，由于学术论文性质的可读性而让我烦恼。 这强烈要求我浏览一些内容，这对纯粹的学术论文是有害的。与非学术书籍相比，专注于阅读文档（我并不认为所有书籍都像个人畅销书一样出色，但我想强调学术和非学术写作之间的结构差异。） 问题是，为什么这个约定不能顺利地改变到更好的方向，对现有的和新的研究人员有帮助？   由   提交/u/Mundane_Definition_8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</guid>
      <pubDate>Wed, 20 Mar 2024 09:04:32 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 为什么要增加 CNN 层的通道大小？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</link>
      <description><![CDATA[在我看到的大多数 CNN 架构中，他们将通道数从 1 或 3 增加到 16、32 甚至 64。难道不是减少通道数吗？维数？   由   提交 /u/Radiant_Walrus3007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</guid>
      <pubDate>Wed, 20 Mar 2024 05:53:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最近的“LLM工程师”没有NLP背景的情况常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</link>
      <description><![CDATA[过去几周，我参加了一些聚会和社交活动，在那里我遇到了很多声称他们“与法学硕士合作”的人。我个人对它们没有太多经验，并且对更“经典”的领域进行了研究。 NLP（ELMo 和 BERT 在我做研究时是重大公告），现在主要作为一名工程师在业界工作。 我经常注意到，当我尝试谈论 LLM 研究模式或应用程序和那些我称之为经典方法的人通常似乎不知道我在说什么。 我不是在谈论研究人员，显然如果你正在与法学硕士进行实际研究，我假设您已经在该领域工作了一段时间。如今，LLM 和 NLP 似乎被分开对待。好奇其他人的想法。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</guid>
      <pubDate>Wed, 20 Mar 2024 00:59:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 在每层使用相同维度的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</link>
      <description><![CDATA[我的直觉是，随着我们在层中移动，令牌会逐渐丰富，但这意味着我们需要在每个令牌中存储更少的信息前面的层比后面的层要多。 从（相对）低维嵌入开始，然后将它们投影或扩展到更高的维度，直到它们达到最终大小，这不是有意义吗？    由   提交/u/timtom85  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</guid>
      <pubDate>Tue, 19 Mar 2024 19:35:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我如何在 Google Gemma 6T 代币模型中发现 8 个错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</guid>
      <pubDate>Tue, 19 Mar 2024 17:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>