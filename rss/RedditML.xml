<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 30 Jan 2025 15:16:42 GMT</lastBuildDate>
    <item>
      <title>[d] 为什么“知识提炼”现在突然被贴上了盗窃的标签？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idjtta/d_why_is_knowledge_distillation_now_suddenly/</link>
      <description><![CDATA[我们都知道，蒸馏是一种近似更精确变换的方法。但我们也知道，这也是整个想法的终结。  蒸馏有什么问题？通过模仿输出来学习“知识”这一事实对我来说毫无意义。当然，通过保持输入和输出相同，我们试图近似一个类似的变换函数，但这并不意味着它确实如此。我不明白这怎么会被贴上盗窃的标签，尤其是当整个架构和训练方法都不同的时候。     提交人    /u/The-Silvervein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idjtta/d_why_is_knowledge_distillation_now_suddenly/</guid>
      <pubDate>Thu, 30 Jan 2025 10:09:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 自动化文档处理和文档工作流程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idjgnd/p_automating_document_processing_and_document/</link>
      <description><![CDATA[大家好， 我正在做一个咨询项目，在开始之前，我总是喜欢听听别人的意见！情况如下： 客户公司从多个来源收到账单，其中包含各种各样的信息。以下是我们正在进行的分步过程：  数据提取：我们计划使用视觉模型从这些法案中提取特定的信息。 分类：每项法案都属于 50 个预定义类别（称为“披露”）之一，我们需要对每项法案进行相应的分类。 合规性映射：每个类别（或披露）都是一份包含 10-15 个问题的文档（例如，“该组织是否监测其温室气体排放？是/否。如果是，请转到问题 3，否则请转到问题 2。”）。这些问题指导进一步的分析，第二列提供了说明。 最终输出生成：根据提取的答案，填充第三列，提供数据的最终结构化表示，以合规友好的语言编写（例如，“该组织已实施了多项可持续发展行动，将每年进行监控以实现以下结果：[具体结果]。”）。   我们面临的挑战：  准确分类：确保法案始终归入 50 个类别中的正确类别。 信息提取和映射：根据提取的数据自动回答每个披露中的问题。 文本生成：根据问题的答案动态生成结构化的最终报告（在第三列）。 可扩展性和准确性：处理大量法案并确保 50 项披露及其不同要求的准确性。  限制：我只能使用本地的 LLM。  对我来说，将账单映射到这 50 个类别之一将非常简单，但我希望对按照决策树样式回答问题有更多的了解。 我非常感谢任何可以指导这个项目的见解、工具、框架或个人经验！ 非常感谢您抽出时间！    提交人    /u/No_Possibility_7588   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idjgnd/p_automating_document_processing_and_document/</guid>
      <pubDate>Thu, 30 Jan 2025 09:42:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于检索增强生成的 OSS React GUI 组件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idj27c/p_oss_react_gui_components_for_retrieval/</link>
      <description><![CDATA[      嘿 r/MachineLearning，我们想分享我们正在为 RAG QA 构建开源 REACT 组件！您可以在 https://github.com/renumics/lexio 找到我们的第一个 Lexio 版本 组件屏幕截图（文档来源：WMO-No. 1360：“非洲气候状况”） 它支持多种文档类型（PDF、HTML、Markdown），具有流式响应和源突出显示等高级功能。  主要特点：  查看器：用于聊天界面、源选择和查看的预构建组件，带有源突出显示 集成状态管理：组件间交互的透明状态处理 固执己见的架构：实现 RAG 最佳实践 高度可定制：主题和组件自定义选项     提交人    /u/DocBrownMS   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idj27c/p_oss_react_gui_components_for_retrieval/</guid>
      <pubDate>Thu, 30 Jan 2025 09:10:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] 雄心勃勃的机器学习项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idir3p/p_ambitious_ml_project/</link>
      <description><![CDATA[我正在开展一个与 FiveM 服务器相关的项目，需要开发一个自定义宏来自动执行某些游戏内操作。具体来说，我想自动执行拾取固定游戏内物品（例如药物）的过程，同时处理需要精确输入的防 AFK 机制。 防 AFK 系统在圆形界面内显示移动光标，当光标与圆圈内特定的浅蓝色部分对齐时，我必须按下正确的数字（1-4）。 更不用说，常规的宏记录器非常不可靠，并且效果不佳，因为它们会干扰视野，并且缺乏检测和响应防 AFK 机制所需的功能。 我正在考虑编写自己的宏来有效地处理这些任务。我应该从哪里开始，您会推荐哪些技术或方法来实现此解决方案？   由    /u/Remote-Sea-6172  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idir3p/p_ambitious_ml_project/</guid>
      <pubDate>Thu, 30 Jan 2025 08:45:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] 是否有任何框架可以根据特定任务从 LLM 中提取出小型 LM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idgq1y/r_are_there_any_frameworks_to_distill_small_lm/</link>
      <description><![CDATA[您好， 我正在寻找一个可以训练和准备来自 LLM 的小型蒸馏语言模型的框架。 例如  我的要求是执行 QA + 翻译。 我不想使用 LLM，而是想使用针对用例进行调整的蒸馏 LM，以获得更高的准确性。在本例中是 2 个 LM，即 QA 和翻译。 整个过程将是这样的：  LLM ---------&gt; 训练 SLM（用于 QA） LLM ----------&gt; 训练 SLM（用于翻译） 用户输入 ---------&gt; QA SLM | 翻译 SLM ------&gt; 输出    由    /u/Ok_Home_3247  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idgq1y/r_are_there_any_frameworks_to_distill_small_lm/</guid>
      <pubDate>Thu, 30 Jan 2025 06:33:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] R1 后训练规格，有人读过这个有趣的 RL 阶段的后训练成本吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idg6mq/d_r1_post_training_specs_does_anyone_have_a_read/</link>
      <description><![CDATA[      浏览 V3 技术报告和 R1 论文后，我对这个 GRPO 流程的成本/硬件时间感到有点困惑。 查看 V3 论文中的图片，R1 的后期训练是否在“后期训练”中涵盖？部分在这里？ https://preview.redd.it/fkwxepiul2ge1.png?width=1250&amp;format=png&amp;auto=webp&amp;s=6b48bd3345de7c604d3bbe78519db19bd3d4e479 5.2.2 提到 GRPO 包含在后训练中，可能是 R1，可能是 R1-zero，可能是两者兼而有之，也可能都不是。 R1 论文提到了R1 使用由 R1-zero 模型（+其他模型......我们这里就不讨论这个了......）生成的“数千个” COT 数据样本，然后执行相同的 GRPO 过程，所以假设我们在那里的训练后成本是 2 倍（10k H800 GPU 小时）？    提交人    /u/Standard_Natural1014   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idg6mq/d_r1_post_training_specs_does_anyone_have_a_read/</guid>
      <pubDate>Thu, 30 Jan 2025 06:02:27 GMT</pubDate>
    </item>
    <item>
      <title>无炒作 DeepSeek-R1 [R] 阅读清单</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ideupn/no_hype_deepseekr1_reading_list/</link>
      <description><![CDATA[在过去的约 1.5 年里，我一直在运营一个研究论文俱乐部，我们在那里深入研究 AI/ML 领域有趣/基础的论文。因此，我们自然而然地接触到了很多导致 DeepSeek-R1 的论文。在本周深入研究 DeepSeek 论文时，我决定编制一份我们已经看过的论文清单，或者我认为这些论文是很好的背景阅读材料，可以更全面地了解 DeepSeek 内部发生的事情。 喝杯咖啡，享受吧！ https://www.oxen.ai/blog/no-hype-deepseek-r1-reading-list    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ideupn/no_hype_deepseekr1_reading_list/</guid>
      <pubDate>Thu, 30 Jan 2025 04:51:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找暑期/冬季学校</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idemly/d_looking_for_summerwinter_schools/</link>
      <description><![CDATA[我正在寻找 ML 夏季/冬季学校来培养我的技能，结识志同道合的人，并希望为未来的机会增加我的简历/ SOP。如果这里有人参加过，我很想听听你的想法——它们真的值得吗？它们在申请工作或研究生院时真的有用吗？ 此外，如果您遇到任何仍在接受申请的 ML 夏季或冬季学校，请提供详细信息！非常感谢任何建议。    提交人    /u/thebluffmaster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idemly/d_looking_for_summerwinter_schools/</guid>
      <pubDate>Thu, 30 Jan 2025 04:39:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 假设分化驱动的推理模型新研究的产生</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ided3m/d_hypothetical_differentiationdriven_generation/</link>
      <description><![CDATA[有没有比我聪明的人能探索一下将 DSPy 或 TextGrad 之类的东西应用于 O1 或 DeepSeek R1 的可能性，使其生成推理链或提示，从而创建肯定不在其训练集中的 arXiv 论文，比如今天发布的论文？ 这是否有可能导致发现实际上会导致新发现的推理链？    提交人    /u/fraktall   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ided3m/d_hypothetical_differentiationdriven_generation/</guid>
      <pubDate>Thu, 30 Jan 2025 04:25:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 建立“穷人的推理模型”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1id8j4o/d_building_a_poor_mans_reasoning_model/</link>
      <description><![CDATA[阅读 DeepSeek-R1 论文后，我一直在想我们是否可以进一步优化推理模型以在消费级硬件上运行？ 该论文表明，推理可以纯粹从 RL 中产生，而无需 SFT，这令人印象深刻。但我并不确信这种新兴推理与我们通过结构良好、精心策划的 CoT 解决方案可能获得的推理有根本区别。 当然，RL 可以发现我们尚未明确教授的新策略（通过奖励信号进行“自我完善”），但我仍然不确定它是否真正不同于彻底的策划方法，尤其是看到像 4o 或 Sonnet 这样的模型在巧妙提示时可以产生什么。 RL DeepSeek 的方法具有明显的优势（训练成本更低，对手工制作数据的依赖更少），但如果我们可以通过更简单、无需训练的方法实现类似的结果：“借用”来自 R1 的合成数据集的推理，并结合多次提示？ 这是我的粗略想法：  将问答 + 推理 + 最终答案对存储在简单的数据库或向量存储中。 按主题标记它们（数学、编码、逻辑、等）或使用嵌入对它们进行索引以进行语义检索。 对于新查询，检索 2-3 个相关示例（包括它们的推理/错误/更正），然后将它们作为多样本提示提供给较小的模型，在推理时有效地借用 R1 的推理风格。  也许我们可以通过协作推理或轻量级 MoE 设置来改进输出，其中多个专门的提示会生成响应，而聚合器会选择或改进最佳的最终答案。或者尝试让竞争代理挑战彼此的推理逻辑，并通过比较来改进最终解决方案，基本上通过 MoE 构建错误/更正结构。 我的假设是，通过合成的“推理”多样本提示和轻量级代理协作，较小的模型可以在消费硬件上模仿 R1 的推理，同时几乎不需要任何训练成本，除了生成合成数据的初始成本之外。 无论如何，我打算在有空的时候测试这种方法。你怎么看？这是一条可行的道路，还是我遗漏了一些关键的东西？还是我从根本上误解了 R1？ 编辑：我应该在发布之前检查一下我输入的内容    提交人    /u/sebnadeau   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1id8j4o/d_building_a_poor_mans_reasoning_model/</guid>
      <pubDate>Wed, 29 Jan 2025 23:54:03 GMT</pubDate>
    </item>
    <item>
      <title>建立文本到图像扩散模型以实现受控的高质量图像生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1id8h1g/grounding_texttoimage_diffusion_models_for/</link>
      <description><![CDATA[本文提出了 ObjectDiffusion，该模型以对象名称和边界框为条件对文本到图像的扩散模型进行条件设定，以实现对对象在特定位置的精确渲染和放置。 ObjectDiffusion 将 ControlNet 的架构与 GLIGEN 的基础技术相结合，显著提高了受控图像生成的精度和质量。 所提出的模型优于目前在开源数据集上训练的最先进的模型，在精度和质量指标上取得了显着的提升。 ObjectDiffusion 可以合成多样化、高质量、高保真度的图像，并与指定的控制布局保持一致。 论文链接：https://www.arxiv.org/abs/2501.09194    由   提交  /u/Next_Cockroach_2615   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1id8h1g/grounding_texttoimage_diffusion_models_for/</guid>
      <pubDate>Wed, 29 Jan 2025 23:51:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 修改已接受的 ICLR 论文以删除有缺陷的贡献？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1icw8yf/d_revise_an_accepted_iclr_paper_to_remove_a/</link>
      <description><![CDATA[我的一篇论文被 ICLR 接受，该论文做出了两个主要贡献：(1) 强调了使用 方法 A 代替 简单基线 存在的问题；(2) 提出了一种替代方法，即 方法 B 来解决此问题。 但是，我最近发现了我报告方法 B 结果的方式存在问题。此问题影响了该研究领域（不仅仅是我的工作）通常报告结果的方式，使方法 B 看起来比方法 A 和简单基线都好。如果结果报告正确，方法 B 仍将优于方法 A，但只会与简单基线相匹配——这引发了一个问题：使用更复杂的方法是否合理。 鉴于此，我认为不应以当前形式发表这篇论文。与 AC 分享一个修订版本是否合适，其中仅包含第一个贡献而省略第二个贡献，并且仍发表该论文？    提交人    /u/NumberGenerator   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1icw8yf/d_revise_an_accepted_iclr_paper_to_remove_a/</guid>
      <pubDate>Wed, 29 Jan 2025 15:24:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大多数机械可解释性研究仅以预印本或博客文章的形式发表？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1icw2pi/d_why_is_most_mechanistic_interpretability/</link>
      <description><![CDATA[我越深入研究这个话题，就越发现常见的做法是将您的工作作为博客文章发布在论坛上，而不是在同行评审的出版物上发布。  这使得工作变得不那么值得信赖和可信。我发现 Anthropic 不会在会议上发表文章，因为您无法复制他们的工作。但是，仍然有大量的工作“仅”以博客文章的形式提供。     提交人    /u/Physical_Seesaw9521   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1icw2pi/d_why_is_most_mechanistic_interpretability/</guid>
      <pubDate>Wed, 29 Jan 2025 15:16:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iai5g6/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iai5g6/d_simple_questions_thread/</guid>
      <pubDate>Sun, 26 Jan 2025 16:00:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>