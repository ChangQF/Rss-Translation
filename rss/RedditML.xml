<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 16 Feb 2024 03:14:56 GMT</lastBuildDate>
    <item>
      <title>[R] 专家混合解锁深度强化学习的参数缩放</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ary7xi/r_mixtures_of_experts_unlock_parameter_scaling/</link>
      <description><![CDATA[摘要：  最近（自）监督学习模型的快速进展在很大程度上是通过经验缩放来预测的定律：模型的性能与其大小成正比。然而，对于强化学习领域来说，类似的缩放定律仍然难以捉摸，增加模型的参数数量通常会损害其最终性能。在本文中，我们证明了将专家混合 (MoE) 模块，特别是软 MoE（Puigcerver 等人，2023）纳入基于价值的网络会产生更多参数可扩展的模型，性能的显着提高就证明了这一点跨越各种训练制度和模型大小。因此，这项工作为制定强化学习的缩放法则提供了强有力的经验证据。  论文链接：https://arxiv.org/pdf/2402.08609.pdf   由   提交/u/OwnAd9305   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ary7xi/r_mixtures_of_experts_unlock_parameter_scaling/</guid>
      <pubDate>Fri, 16 Feb 2024 03:02:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何训练用于 UI 设计的生成式 AI (galileo ai)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arxxxg/d_how_do_you_train_generative_ai_for_ui_design/</link>
      <description><![CDATA[      大家好，我最近偶然发现了https://www.usegalileo.ai/create。我很好奇他们用什么来训练这样的模型来生成 UI 设计（可导出到 Figma）。我是新手，所以我推断可能只是微调一些稳定的扩散，或者他们可能会整理 1000 多个模板并随机选择。 你认为他们正在使用什么？ ​ https: //preview.redd.it/ca9lk2xp2vic1.png?width=608&amp;format=png&amp;auto=webp&amp;s=f1c2af155b755ebc682892c7c481283e499617cd    ;由   提交/u/Low_Acanthisitta_272   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arxxxg/d_how_do_you_train_generative_ai_for_ui_design/</guid>
      <pubDate>Fri, 16 Feb 2024 02:49:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 3D 打印电子产品中的机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arxvbj/d_machine_learning_in_3d_printed_electronics/</link>
      <description><![CDATA[大家好。我正在开展一个项目，其中我们必须根据 CAD/PCB 建模工具中的尺寸、喷嘴温度、墨水粘度、床温度等输入来预测 3D 打印的预期参数。我正在寻找参考和资源以便实施类似的项目。如果您有任何进一步的信息或见解，将不胜感激。谢谢！！   由   提交/u/Background_Nature425  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arxvbj/d_machine_learning_in_3d_printed_electronics/</guid>
      <pubDate>Fri, 16 Feb 2024 02:45:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 作为世界模拟器的视频生成模型。开放AI Sora技术报告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</link>
      <description><![CDATA[https:/ /openai.com/research/video- Generation-models-as-world-simulators   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arwcpu/r_video_generation_models_as_world_simulators/</guid>
      <pubDate>Fri, 16 Feb 2024 01:31:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] DeepRhythm - 快速、准确的节奏估计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arscov/p_deeprhythm_fast_accurate_tempo_estimation/</link>
      <description><![CDATA[最近，我需要一种方法来准确估计 Raspberry Pi 上音频文件的全局节奏。我尝试了 Librosa、Essentia 和 TempoCNN 的估计器，但所有（开源）方法都不可靠，而且非常非常慢。  所以，我做了一些研究，发现了这篇论文，描述了一个 CNN使用他们称为“Harmonic-Constant-Q-Modulation”的音频功能来预测节奏。简而言之，他们在 8 秒帧上执行一系列恒定 Q 变换，而不是提取“音高”频率，而是提取低得多的“节奏”频率（120bpm = 2Hz）。通过这种方法，CNN 几乎不需要做任何跑腿工作，它更多地充当过滤器来降低 HCQM 的维度，即，它不需要学习起始模式，而只需要解释 bpm 频率本身的相对强度。  无论如何，我找不到该论文的任何源代码或任何 HCQM 实现，所以我构建了自己的代码。令我惊讶的是，它的效果非常。  我使用 pytorch 和 nnAudio 编写了 HCQM 实现，以便可以在 GPU 上运行它。每个步骤的内核/过滤器都是预先计算和重复使用的，并且经过一些仔细的扁平化和过滤。 reshape 可以在不到一秒的时间内一次性处理最多 100 首歌曲（在 4090 上）。  对于公共训练数据，没有太多。我使用了 GiantSteps、Ballroom、FMA 的一小部分以及我自己的音乐收藏。我通过我能找到的所有其他节奏预测器运行了整个集合，通过多数票选择了“真实的 bpm”，然后将置信度测量为每个预测器与“获胜者”之间的距离。这使我能够通过删除无法准确预测的多节奏/原声歌曲来清理数据集。  CNN 本身非常简单，完全按照论文中描述的方式实现。它首先在全套上进行训练，然后在我的收藏上进行微调（主要是使用编程/循环鼓） 在我迄今为止的测试中，当两者都在 CPU 上运行时，它比 TempoCNN 快 10 倍。在完整的测试集上，它的正确率是 95.9%（目标的 2% 以内），而在我的集合上，它的正确率是 97.8%。据我所知，这明显更快并且更快。比任何公开的信息都更准确。  这是 代码，它可以在 pip 上使用，并且应该可以在支持 pytorch 的任何设备上运行（我已经在 Ubuntu、MacOS 和 Raspbian 上进行了测试）。    ;由   提交 /u/bleugre3n   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arscov/p_deeprhythm_fast_accurate_tempo_estimation/</guid>
      <pubDate>Thu, 15 Feb 2024 22:32:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于生成模型训练的自校正自消耗循环</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arqbud/r_selfcorrecting_selfconsuming_loops_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.07087 代码：https://github.com/nate-gillman/self- Correcting-self-消费 项目页面：https://cs.brown.edu/people/ngillman//sc-sc.html 摘要：  随着合成数据的质量越来越高并且在互联网上激增，机器学习模型越来越多地基于人类和机器生成的数据的混合进行训练。尽管使用合成数据进行表示学习有成功的故事，但使用合成数据进行生成模型训练会产生“自消耗循环”。除非满足某些条件，否则可能会导致训练不稳定甚至崩溃。我们的论文旨在稳定自消耗生成模型训练。我们的理论结果表明，通过引入理想化的校正函数，将数据点映射到更可能处于真实数据分布下的位置，可以使自消耗循环呈指数级地更加稳定。然后，我们提出了自我校正函数，它依赖于专家知识（例如在模拟器中编程的物理定律），旨在自动大规模地逼近理想化的校正器。我们凭经验验证了自校正自消耗循环在具有挑战性的人体运动合成任务中的有效性，并观察到它成功地避免了模型崩溃，即使合成数据与真实数据的比率高达 100%。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arqbud/r_selfcorrecting_selfconsuming_loops_for/</guid>
      <pubDate>Thu, 15 Feb 2024 21:10:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 激活的三个十年：神经网络 400 个激活函数的全面调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09092 摘要：  神经网络已被证明是解决复杂问题的高效工具生活的许多方面都存在问题。最近，随着深度学习的出现，它们的重要性和实际可用性进一步得到加强。神经网络成功的重要条件之一是选择合适的激活函数，将非线性引入模型。过去的文献中已经提出了许多类型的这些函数，但没有一个综合来源包含它们的详尽概述。即使根据我们的经验，缺乏这种概述也会导致冗余和无意中重新发现已经存在的激活函数。为了弥补这一差距，我们的论文提出了一项涉及 400 个激活函数的广泛调查，其规模比以前的调查大几倍。我们的综合汇编也参考了这些调查；然而，其主要目标是提供先前发布的激活函数的最全面的概述和系统化，并提供其原始来源的链接。第二个目标是更新当前对这一系列函数的理解。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arovn8/r_three_decades_of_activations_a_comprehensive/</guid>
      <pubDate>Thu, 15 Feb 2024 20:12:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用视觉文字谜题衡量人工智能的创造力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aro6jm/p_measuring_ais_creativity_with_visual_word/</link>
      <description><![CDATA[一个有趣的项目，我致力于使用像画谜这样的视觉文字游戏来测量生成人工智能模型（例如多模态大语言模型，或 MLLM）中的*多模态*创造力谜题！  目前，MLLM 有各种多模态基准（例如 VQA、图像字幕等），但据我所知，没有一个可以衡量创意方面，例如解决涉及语言和视觉的难题的能力理解（例如画画谜题的情况）。 ​ 在这个项目中，我比较了 Gemini 和 GPT-4 在画画上的少量射击与零射击能力难题并将其与人类能力进行比较。总体而言，尽管 GPT-4 用很少的镜头就能够解决一些对人类来说更困难的问题，但人类仍然更擅长解决这些难题。 ​ https://www.artfish.ai/p/measuring-ais-creativity-with-visual    由   提交 /u/porkbellyqueen111   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aro6jm/p_measuring_ais_creativity_with_visual_word/</guid>
      <pubDate>Thu, 15 Feb 2024 19:43:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI Sora Video Gen——如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</link>
      <description><![CDATA[ 介绍 Sora，我们的文本转视频模型。 Sora 可以生成长达一分钟的视频，同时保持视觉质量并遵守用户的提示。  https:/ /openai.com/sora 研究笔记 Sora 是一种扩散模型，它从看起来像静态噪声的视频开始生成视频，然后通过多个步骤消除噪声来逐渐对其进行转换. Sora 能够一次生成整个视频或扩展生成的视频以使其更长。通过一次为多个帧提供模型预测，我们解决了一个具有挑战性的问题，即确保对象即使暂时离开视野也保持不变。 与 GPT 模型类似，Sora 使用变压器架构，释放卓越的扩展性能。 我们将视频和图像表示为称为补丁的较小数据单元的集合，每个补丁类似于 GPT 中的令牌。通过统一我们表示数据的方式，我们可以在比以前更广泛的视觉数据上训练扩散变换器，涵盖不同的持续时间、分辨率和纵横比。 Sora 建立在 DALL·E 和 DALL·E 过去的研究基础上GPT 模型。它使用 DALL·E 3 的重述技术，该技术涉及为视觉训练数据生成高度描述性的标题。因此，该模型能够更忠实地遵循生成视频中用户的文本指令。 除了能够仅根据文本指令生成视频之外，该模型还能够采用现有的静态图像并从中生成视频，精确地动画图像内容并关注小细节。该模型还可以获取现有视频并对其进行扩展或填充缺失的帧。在我们的技术论文（今天晚些时候发布）中了解更多信息。 Sora 是能够理解和模拟现实世界的模型的基础，我们相信这一功能将成为实现 AGI 的重要里程碑。 Sora 是能够理解和模拟现实世界的模型的基础。 p&gt; 示例视频：https://cdn.openai.com/sora/videos/ cat-on-bed.mp4 技术论文将于今天晚些时候发布。但是如何进行头脑风暴呢？   由   提交/u/htrp  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/</guid>
      <pubDate>Thu, 15 Feb 2024 18:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 咆哮/问题：权重和偏差扫描发生奇怪的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1armaiq/d_rantquestion_weird_things_happening_with/</link>
      <description><![CDATA[抱歉，如果这是错误的地方，但我只需要知道我是否是唯一经历过这种情况的人，以及是否有人经历过对我有什么建议，因为：每当我使用带有权重和偏差的扫描功能时，往往会发生奇怪的事情。 最一致的是，如果我在 python 脚本中启动 wandb 扫描代理如果 wandb.agent(sweep_id=sweep_id, function=my_function, count=count) 的计数较高（例如 count=40），GPU 内存会慢慢开始填满没有被释放，并且在几次运行之后（可能是 10 次、可能是 20 次、可能是 30 次），由于 GPU 的 OOM 错误，所有新的运行都会在开始时崩溃。  起初我以为这是 PyTorch 的事情，我尝试了各种技巧来防止它发生，比如将 my_function 包装在手动执行垃圾收集的包装器中（使用之后gc.collect）。但似乎没有任何效果。 现在我正在使用 JAX 运行 wandb 扫描，并且发生了同样的事情：具有 40gb 内存的 GPU 没有空间用于 262144 字节分配，而仅分配了 896.4KiB贾克斯。因此，在一系列成功运行之后，所有新运行都会立即崩溃并出现 OOM 错误（或更准确地说：jaxlib.xla_extension.XlaRuntimeError：RESOURCE_EXHAUSTED：尝试分配...字节时内存不足。） . 现在，上面是我知道发生的一件非常可衡量和有形的事情（我之前在扫描过程中监控过我们办公室电脑中的 GPU 使用的内存，并且可以在某个时候看到，内存不再被释放，所有运行都开始崩溃）。但下一个听起来绝对疯狂。 有时你会在深度学习中得到 NaN。这很糟糕，但它就是发生了。因此，在我最近的一次扫描中，我有一个回调检查 NaN，并在损失变为 NaN 时结束运行（通过带有适当错误消息的 RuntimeError）。在 53 次运行中，有 11 次运行由于 NaN 而被缩短，其中 8 次在第一个训练步骤中就出现了 NaN。奇怪的是：我使用两个代理进行此（随机）扫描，并且这 8 次运行是在同一代理上连续运行。  这些运行都有不同的配置和不同的随机种子。对于所有这 8 次运行，扫描配置中没有相同的变量。如果这些崩溃是独立的，则运行以 NaN 结束的可能性为 11/53。连续 8 次运行以 NaN 结尾的概率为 (11/53)^8 = 3.44E-6。  公平地说：在很长的运行序列中发生这种情况的概率大于 3.44E-6，但仍然非常小。比如：我只是偏执吗？或者那个代理发生了什么事情导致我得到了错误的结果？我稍后会对其进行测试，因此您可能会看到一个编辑，我承认这纯粹是运气不好。  无论如何，有人有预防这些 GPU OOM 问题的技巧吗？人们是否会直接数到 1，然后提交 80 个工作来进行 slurm？或者有更好的方法来做到这一点吗？    由   提交/u/katerdag  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1armaiq/d_rantquestion_weird_things_happening_with/</guid>
      <pubDate>Thu, 15 Feb 2024 18:25:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] BERT 与 ChatGPT 比较（文本分类和情感分析）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</link>
      <description><![CDATA[有人研究过微调特定 BERT（或任何其他类似模型）与 ChatGPT（微调与否）之间的情感比较分析和文本分类？ 我很想知道它们在性能、成本、维护等方面的比较。   由   提交 /u/Grinbald   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arkwvv/r_the_bert_vs_chatgpt_comparison_text/</guid>
      <pubDate>Thu, 15 Feb 2024 17:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1M/10M token上下文窗口怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</link>
      <description><![CDATA[是否会启动社区头脑风暴主题？ - 人们是否认为 RingAttention 可以充分扩展？参见https://largeworldmodel.github.io - 它是用 1M 还是 10Mn 令牌窗口进行训练的，这对我来说似乎不清楚？他们是否在没有经过某种训练的情况下从 1M-&gt;10M 进行概括？ - 存在哪些数据集可以训练 10M 文本标记窗口？ - 在这么长的背景下你如何做 RLHF？ 1M 文本 ~ 4M 字符 ~ 272k 秒阅读时间（根据 Google 假设 68 毫秒/字符）~ 阅读一个示例需要 75 小时？ 编辑：当然 lucidrains 已经在着手实施 RingAttention！ (https://github.com/lucidrains/ring-attention-pytorch)   由   提交 /u/gggerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/</guid>
      <pubDate>Thu, 15 Feb 2024 16:13:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 衡量将法学硕士纳入工作流程之前和之后的软件工程生产力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</link>
      <description><![CDATA[我在一家软件工程公司（外包）工作，我们的管理层希望衡量大型语言模型对日常工程工作（包括软件工程、数据工程、质量保证等）。最终目标是获得一些原始指标（例如“使用 LLM 时 X 团队的表现提高了 30%”）以呈现给客户，旨在证明我们优于不使用 LLM 的竞争对手。 我的观点是，准确衡量这种影响具有挑战性，因为 LLM 的表现可能会因任务环境的不同而有很大差异（例如，为网站开发简单的注册表单与为 IBM 大型机编写代码）。此外，这最终取决于执行工作的个人（例如，在使用法学硕士作为支持工具时，A 和 B 可能会在同一任务上花费不同的时间）。 我在这里合理吗？ ？有没有什么方法可以准确衡量这些影响？我试图找到关于这个主题的研究论文，但大多数都侧重于综合 LLM 测试，将个人 LLM 表现与其他 LLM 进行比较。 编辑：发现 github copilot 研究博客指出生产力提高了 55%： https://github.blog/2022 -09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/   由   提交 /u/GottaPerformMiracles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arikfe/d_measuring_software_engineering_productivity/</guid>
      <pubDate>Thu, 15 Feb 2024 15:52:36 GMT</pubDate>
    </item>
    <item>
      <title>[N] Gemini 1.5，具有 1M 上下文长度令牌的 MoE</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</link>
      <description><![CDATA[https://blog.google/technology/ai/google-gemini-next- Generation-model-february-2024/  &amp;# 32；由   提交/u/Electronic-Author-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1arhnoe/n_gemini_15_moe_with_1m_tokens_of_contextlength/</guid>
      <pubDate>Thu, 15 Feb 2024 15:13:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>