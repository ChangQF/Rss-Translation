<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 07 Sep 2024 15:15:43 GMT</lastBuildDate>
    <item>
      <title>[P] NviWatch 是一款用于监控 Nvidia GPU 的 rust tui</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb55rf/p_nviwatch_a_rust_tui_for_monitoring_nvidia_gpus/</link>
      <description><![CDATA[      想分享，因为这可以帮助您进行 GPU 监控。✅ 专注于 GPU 进程✅ 多种视图模式✅ 用 rust 编写的轻量级✅ 直接使用 NVML    提交人    /u/msminhas93   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb55rf/p_nviwatch_a_rust_tui_for_monitoring_nvidia_gpus/</guid>
      <pubDate>Sat, 07 Sep 2024 11:53:24 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 学习更长序列的位置嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb4zls/discussion_learned_positional_embeddings_for/</link>
      <description><![CDATA[因此，我重新阅读了 transformer 论文，让我印象深刻的一件事是作者还使用了学习到的位置嵌入。Karpathy 的 nanoGPT 实现使用了学习到的位置嵌入，我想知道这些嵌入如何扩展到更长的序列？ 从直觉上讲，如果模型从未见过超过 max_length 的标记，它将无法生成有意义的东西。那么 OpenAI 的 GPT（假设他们仍然使用学习到的 PE）如何扩展到超过 2k 的上下文长度？    提交人    /u/tororo-in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb4zls/discussion_learned_positional_embeddings_for/</guid>
      <pubDate>Sat, 07 Sep 2024 11:43:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] Alex Kim 不为人知的故事 – 使用大型语言模型进行财务报表分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb43l0/d_the_untold_story_of_alex_kim_financial/</link>
      <description><![CDATA[        由    /u/NextgenAITrading 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb43l0/d_the_untold_story_of_alex_kim_financial/</guid>
      <pubDate>Sat, 07 Sep 2024 10:46:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 审查并为我的聊天机器人提出想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb2mwl/p_review_and_suggest_ideas_for_my_chatbot/</link>
      <description><![CDATA[好的，我目前正在尝试使用以下技术细节构建支持聊天机器人 1. 用于 Web 服务器的 FastAPI（需要使其更快）2. Qdrant 作为矢量数据库（发现它是 Chromadb、Elastic Search 和 Milvus 中最快的）3. MongoDB 用于存储所有数据和反馈。4. 语义分块，最大令牌限制为 512。5. granite-13b-chat-v2 作为 LLM（我知道它不好，但我可用的选项有限）6. 数据是结构化的和非结构化的。考虑将 GraphRAG 纳入当前架构。7. 多个数据源存储在多个矢量数据库集合中，因为我已经实现了访问控制。8. 目前使用 mongoengine 作为 ORM。如果您知道更好的方法，请提出建议。 9. 目前使用 all-miniLM-l6-v2 作为向量嵌入，但计划使用 stella_en_400M_v5。10. 使用余弦相似度检索文档。11. 使用 BLEU、F1 和 BERT 分数根据黄金答案进行自动评估。12. 使用 top_k 作为 3。13. 目前使用基本问答提示，但想要改进它。有什么建议吗？也听说过自动提示评估。14. 目前所有东西都使用自定义代码。希望使用 Llamaindex 或 Langchain 来实现这一点。15. 现在我没有使用任何 AI 代理，但我想知道你的意见。16. 这是一个简单的 RAG 框架，我正在努力改进它。17. 我还没有包括重新排名器，但我也计划这样做。 我想我提到了我在项目中使用的几乎所有东西。所以请分享你对它的建议、评论和评价。谢谢！！    由   提交  /u/QaeiouX   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb2mwl/p_review_and_suggest_ideas_for_my_chatbot/</guid>
      <pubDate>Sat, 07 Sep 2024 08:57:04 GMT</pubDate>
    </item>
    <item>
      <title>[N] Arxiv 将于周二举办多模式“声音化”无障碍会议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb28a3/n_arxiv_is_having_a_multimodal_sonification/</link>
      <description><![CDATA[  由    /u/Competitive_Travel16  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb28a3/n_arxiv_is_having_a_multimodal_sonification/</guid>
      <pubDate>Sat, 07 Sep 2024 08:26:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 评估大型语言模型在保护秘密/隐藏信息方面的有效性的工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fb0ami/p_tool_for_assessing_the_effectiveness_of_large/</link>
      <description><![CDATA[https://github.com/user1342/Would-You-Kindly    由   提交  /u/OppositeMonday   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fb0ami/p_tool_for_assessing_the_effectiveness_of_large/</guid>
      <pubDate>Sat, 07 Sep 2024 06:03:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种 LLM 模型最适合对文本到 SQL 进行微调？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fawirs/d_which_llm_model_is_best_suited_for_finetuning/</link>
      <description><![CDATA[我正在开展一个金融数据分析项目，重点是文本到数据的可视化。第一步是根据输入文本生成相关的 SQL 查询。我使用 Mistral 7B 模型来完成这项任务。但是，在使用 Google Colab 中的数据集对其进行训练时，我总是遇到内存不足错误。我尝试了各种配置，例如调整批处理大小和标记长度，但每次仍然显示 CUDA 内存不足错误。我使用了不同类型的硬件加速器，但问题仍然存在。有人对我使用的模型是否太大或是否有我应该考虑的替代方案有什么建议吗？    提交人    /u/More_Lawfulness_6862   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fawirs/d_which_llm_model_is_best_suited_for_finetuning/</guid>
      <pubDate>Sat, 07 Sep 2024 02:19:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 协调者的作用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1favz8l/d_role_of_orchestrators/</link>
      <description><![CDATA[你好， 为了回答这个问题，我们称之为  经典机器学习：使用非神经网络模型的机器学习。非常模糊地由 scikit 学习算法完成。 现代机器学习：使用深度神经网络（如 cnn、rnn）进行机器学习。模糊地说使用 pytorch、tensorflow。  在经典机器学习领域，像 airflow、step functions 这样的编排器在流水线数据清理、特征工程、训练、超参数调整、交叉验证等方面发挥了作用。 在现代机器学习领域，似乎不需要编排，因为框架倾向于将其作为模型定义的一部分。我可能错了，因为我主要在经典机器学习领域工作，然后开始在现代机器学习领域工作。 这是一个有效的观察吗？您在训练中在哪里使用编排器？您是否考虑将数据提取或准备（如独热编码、嵌入）作为步骤并对其进行编排？ 我能想到的一个地方是在分布式训练之前配置 GPU 机器。 干杯，    提交人    /u/lastmonty   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1favz8l/d_role_of_orchestrators/</guid>
      <pubDate>Sat, 07 Sep 2024 01:50:55 GMT</pubDate>
    </item>
    <item>
      <title>[P] 一个可嵌入的小部件，可让您将分类法映射到一起</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fapmyy/p_an_embeddable_widget_that_lets_you_map/</link>
      <description><![CDATA[      嘿 MLE！我制作了一个可嵌入的小部件，可让团队将分类法交叉在一起。如果有帮助，很高兴分享有关映射算法的更多信息。 提供一些演示背景：数据提供者（例如，薪酬数据提供者）将嵌入此小部件，我们将管理“规范化”薪酬数据到用户的分类法。前端不会公开一些更复杂的细节，如映射置信度分数和复杂关系（例如一对多、多对多等）。 https://i.redd.it/hsjmuwuh49nd1.gif    提交人    /u/Different-General700   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fapmyy/p_an_embeddable_widget_that_lets_you_map/</guid>
      <pubDate>Fri, 06 Sep 2024 20:49:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 贝叶斯模型与共形预测 (CP)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fac5u5/d_bayesian_models_vs_conformal_prediction_cp/</link>
      <description><![CDATA[大家好， 我创建这篇文章是为了征求您对两种主要不确定性量化范式的看法。我看到代表他们的研究人员之间存在着激烈的竞争。我对近似参考（和贝叶斯深度学习）进行了研究，但除了 CP 的基本教程之外，我对 CP 不是很熟悉。我个人认为它们都是有用的工具，也许可以互补使用： CP 可以提供保证，但是是 poshoc 方法，而 BDL 可以使用先前的正则化来实际*改善*训练期间模型的泛化。此外，CP 基于 IID 假设（抱歉，这不是普遍正确的，至少这是本教程中的假设），而在 BDL 中，输入仅在以参数观察为条件时才是 IID：一般来说，p(yi,yj|xi,xj)!=p(yi|xi)p(yj|xj) 但 p(yi,yj|xi,xj,theta)=p(yi|xi, theta)xp(yj|xj, theta)。因此，BDL 或高斯过程在这方面可能更为现实。 最后，贝叶斯模型不能派生出一个 CP 吗？在这种情况下，CP 提供的预测集和贝叶斯模型提供的预测集有多大程度的一致？是否有研究论文将这些方法结合起来并对此进行测试？ 如果我的问题太基础，请提前道歉。我只是想在两种范式之间保持公正的视角。    提交人    /u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fac5u5/d_bayesian_models_vs_conformal_prediction_cp/</guid>
      <pubDate>Fri, 06 Sep 2024 10:50:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 检索增强生成 vs 长上下文 LLM，我们确定后者会取代前者吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fabu65/d_retrievalaugmented_generation_vs_longcontext/</link>
      <description><![CDATA[我认为这个问题已经争论了很长时间。但最近有两篇关于这个问题的有趣文章，我想以此作为讨论 RAG 与长上下文 LLM 的起点。 总之，如果我们可以将所有内容都放在提示中，我们就不需要进行检索。但是，我真的怀疑我们能否拥有一个能够覆盖任何组织拥有的大量数据的上下文长度的模型（并且没有可怕的计算成本）。 无论如何，有令人难以信服的报告称 LC-LLM 在 QA 中效果更好（至少到目前为止，我还没有读过一篇文章让我相信 LC-LLM 比 RAG 效果更好）。  有两篇文章讨论了噪声对 LLM 和 RAG 的影响：  第一篇文章指出噪声会影响 LLM 的性能，并竭尽全力对此进行描述。https://arxiv.org/abs/2408.13533 第二篇文章比较了 RAG 和 LC-LLM，并表明通过增加上下文的大小，我们会出现峰值（我们添加相关块），然后性能会下降，因为 LLM 更难找到正确的信息。 https://arxiv.org/abs/2409.01666  我认为我们最终会保留 RAG 的原因或多或少是 LLM 是复杂的神经网络，因此也是模式识别机器。最终，优化信噪比是机器学习中最常见（有时也很困难）的任务之一。当我们开始过度增加这种噪音时，最终模型必然会开始发现噪音并偏离重要信息（此外，LLM 的参数记忆和上下文之间也存在微妙的相互作用，我们仍然不知道为什么有时会忽略上下文） 其次，我个人认为，还有一个结构性原因。自注意力机制会寻求相关关系，而在上下文长度增加的情况下，我们倾向于维数灾难，最终会加剧虚假关系。 我想和您讨论一下您认为 RAG 不会被取代的原因，或者您是否认为 LC-LLM 最终会取代它？在第二种情况下，它如何解决大量上下文无关数据的问题？    submitted by    /u/NoIdeaAbaout   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fabu65/d_retrievalaugmented_generation_vs_longcontext/</guid>
      <pubDate>Fri, 06 Sep 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 CUDA 比 ROCm 快这么多？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/</link>
      <description><![CDATA[通常人们会回答“因为 NVIDIA 有更多的时间和金钱”。但是，为什么 AMD 无法赶上？究竟是什么让优化 ROCm 如此困难？ 如果您可以指出一些资源，或者您的回答尽可能详细地说明特定内核和结构的实现以及如何从 Triton 或 XLA 精确地进行和优化 CUDA 调用，那将会很有帮助。谢谢 :)    提交人    /u/evilevidenz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/</guid>
      <pubDate>Fri, 06 Sep 2024 06:52:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 本周，我在 Tinygrad 中实现了论文“关注 MLP”！:D</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fa5kop/p_this_week_i_implemented_the_paper_pay_attention/</link>
      <description><![CDATA[      为了尝试更有趣的模型架构，我在 Tinygrad 中实现了 gMLP！ 如果有人想提供一些反馈，将受到欢迎。  [存储库]：https://github.com/EthanBnntt/tinygrad-gmlp [安装]：pip install gmlp_tinygrad [原始论文]：https://doi.org/10.48550/ARXIV.2105.08050  显示 gMLP 架构的图表    提交人    /u/the-wonderful-world   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fa5kop/p_this_week_i_implemented_the_paper_pay_attention/</guid>
      <pubDate>Fri, 06 Sep 2024 03:29:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f63rhf/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f63rhf/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 01 Sep 2024 02:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>