<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Sat, 12 Apr 2025 21:15:16 GMT</lastBuildDate>
    <item>
      <title>[P]谐波激活：神经网络的周期性和单调功能扩展（预印本）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxqtoo/p_harmonic_activations_periodic_and_monotonic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，伙计们！我最近发布了一个预印本，提出了一个新的激活功能系列，专为无标准化的深层网络而设计。我是一名独立研究人员，致力于MLP和变压器的表达性非线性。   tl; dr：  i提出了一个残留激活函数：    f（x）= x +α·g（sin²（sin²（πx/2）） gelu） 我想听听反馈。这是我的第一篇论文。   preprint ：[ &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/henriquelmeeee     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxqtoo/p_harmonic_activations_periodic_and_monotonic/</guid>
      <pubDate>Sat, 12 Apr 2025 20:39:09 GMT</pubDate>
    </item>
    <item>
      <title>[r]帮助大学研究大数据调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxo478/r_help_with_university_research_about_big_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我正在对学生和专业人士中常用的大数据工具进行大学研究调查。如果您从事数据或技术工作，我将非常感谢您的意见 - 只需3分钟！谢谢      &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/chiki_rukis     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxo478/r_help_with_with_with_university_research_about_about_big_data/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxo478/r_help_with_university_research_about_big_data/</guid>
      <pubDate>Sat, 12 Apr 2025 18:36:31 GMT</pubDate>
    </item>
    <item>
      <title>[p]如果您可以在每gpu中运行50+ LLMS，而无需保持记忆力呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxn5fe/p_what_if_you_could_run_50_llms_per_gpu_without/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们一直在尝试使用AI本地运行时，该运行时间在2-5秒内快照llms（13b – 65b），并动态运行50多个型号，每个GPU每gpu运行50多个型号 - 毫不及时地居住在记忆中。 + memory buffers, and restore models on demand — even in shared GPU environments where full device access isn’t available. This seems to unlock: • Real serverless LLM behavior (no idle GPU cost) • Multi-model orchestration at low latency • Better GPU utilization for agentic or dynamic workflows Curious if others here are exploring similar ideas — especially with: • Multi-model/agent堆栈•动态GPU内存管理（MIG，KAI调度程序等）•CUDA-CHECKPOINT /部分设备访问挑战&lt; / p&gt; 如果有用的话，很乐意分享更多的技术细节。很想交换笔记或听到您看到的当前模型提供的疼痛点！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxn5fe/p_hat_what_if_you_you_could_run_run_50_llms_per_gper_gpu_without/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxn5fe/p_what_what_you_you_could_run_50_50_50_50_per_per_gpu_gpu_without/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxn5fe/p_what_if_you_could_run_50_llms_per_gpu_without/</guid>
      <pubDate>Sat, 12 Apr 2025 17:53:52 GMT</pubDate>
    </item>
    <item>
      <title>[D]“推理模型并不总是说出他们的想法”  - 有人提示吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxjwi2/d_reasoning_models_dont_always_say_what_they/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有人尝试通过使用自己的提示来复制的“推理模型并不总是说出他们的想法” 论文？我正在努力再现这些输出。如果您对此进行了尝试并进行了微调，您是否可以分享您的提示或一路上获得的任何见解？任何讨论或指示都将不胜感激！ 参考，这是：       - 提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxjwi2/d_reasoning_models_models_dont_always_always_say_say_what_they/&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxjwi2/d_reasoning_models_dont_always_say_what_they/</guid>
      <pubDate>Sat, 12 Apr 2025 15:30:53 GMT</pubDate>
    </item>
    <item>
      <title>[D]高级NLP资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxinyb/d_advanced_nlp_resources/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在AI中完成主人，并希望在一家大型科技公司的职位上找到职位，理想地从事LLMS工作。我想开始为将来的面试做准备。上个学期，我根据丹·贾拉夫斯基（Dan Jurafsky）和詹姆斯·H·马丁（James H.虽然我发现这是对该领域的很好的介绍，但现在我对书中涵盖的所有内容感到充满信心。 您是否有建议对更多高级书籍的建议，还是建议您专注于理解有关该主题的最新研究论文？另外，如果您有任何一般建议在此领域准备工作面试，我很想听听！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tech-trekker    href =“ https://www.reddit.com/r/machinelearning/comments/1jxinyb/d_advanced_nlp_resources/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxinyb/d_advanced_nlp_resources/</guid>
      <pubDate>Sat, 12 Apr 2025 14:34:10 GMT</pubDate>
    </item>
    <item>
      <title>[n] Google开放，让企业自我主机SOTA模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从主要参与者中，这听起来像是一个很大的变化，并且大多会为企业提供有关数据隐私的有趣视角。 Mistral在Openai和Anthropic维护更多封闭式产品或通过合作伙伴时已经做了很多事情。   https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-models-in-their-own-data-centers.html     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jxin3q/n_google_open_to_to_let_tto_tto_entreprises_erse_host_host_sota/”&gt; [link]   [commist]     ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxin3q/n_google_open_to_let_entreprises_self_host_sota/</guid>
      <pubDate>Sat, 12 Apr 2025 14:33:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] D1：通过增强学习在扩散大语模型中扩展推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   最近的大型语言模型（LLMS）已证明了强大的推理能力，可以从在线增强学习（RL）中受益。这些功能主要在从左到右的自回归（AR）一代范式中证明。相比之下，基于扩散的非运动范式以粗到精细的方式产生文本。尽管与AR相比，最近基于扩散的大语言模型（DLLM）已经达到了竞争性语言建模性能，但尚不清楚DLLM是否也可以利用LLM推理的最新进展。为此，我们提出了D1，这是一个框架，可以通过有监督的Finetuning（SFT）和RL的组合将预先训练的戴上DLLM适应推理模型。具体而言，我们开发并扩展了技术以改善预验证的DLLM中的推理：（a）我们利用蒙版的SFT技术直接从现有数据集中提炼知识并灌输自我提高行为，（b）我们引入了一种新颖的无评论，策略级别的RL算法，称为DIFFU-GRPO。通过实证研究，我们研究了不同的训练后食谱对多个数学和逻辑推理基准的性能。我们发现D1可以产生最佳性能，并显着提高了最先进的DLLM的性能。  在扩散扩散大语模型上，使用强化学习来缩放扩散模型。当涉及到实际上原因的语言模型时，绝对需要注意！ 纸链接：  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/hiskuu     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxeahf/r_d1_scaling_reasoning_in_diffusion_large/</guid>
      <pubDate>Sat, 12 Apr 2025 10:30:15 GMT</pubDate>
    </item>
    <item>
      <title>[r]建立大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxc197/r_building_a_large_language_model/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好， 我已经从事这个项目工作了一段时间，从头开始实现因果语言模型。这个项目对我来说更像是一项研究，而不是试图建立下一个聊天的GPT，这是由于硬件限制的主要原因。 核心体系结构     multiHeadateention.py          掩盖了自动启动     feedforward.py    在两层位置上是feed-fordward网络（gelu activation）。    在注意力下独立处理每个标志。 DecoderBlock.py  Combines MultiHeadAttention and FeedForward layers with:  Layer normalization and residual connections. Dropout for regularization.   Decoder.py  Stacks num_layers DecoderBlock instances. Applies final layer normalization to stabilize outputs.  GPT.py(Main Model)  Token/Position嵌入：使用预审预测的gpt-2嵌入式（ wte 和 wpe ）。   解码器：通过堆叠的解码器嵌入过程。    自动回应生成（main.py）      generate_text（）： 使用 top-k smpling 用于受控的文本生成。 ＆lt; eos＆gt; 令牌或 max_length 。 依赖于解码器的自动回归掩蔽，以防止将来的标记可见度。                triend＆amp; amp; amp; amp; amp; amp; amp; amp; Data Pipeline  GPTDataset.py: Wraps tokenized inputs/targets into PyTorch Dataset, shifting tokens for autoregressive training (inputs = tokens[:-1], targets = tokens[1:]).   train.py ： 加载wikitext数据集，tokenize texts and of批次。   损失函数： crossentropyloss 用 code&gt; ige&gt; ige&gt;  优化： adamw 用于每个参数的自适应学习率。 适用因果掩模与训练过程中的填充面膜结合。             损失计算：将logits与移动目标进行比较。   向后pass ：ADAMW通过梯度更新重量。    您可以在github 此处。如果您有任何改进的想法，请告诉我，如果您觉得它有用，请考虑给它一颗星星以支持其开发。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/blackrat13     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxc197/r_building_a_large_language_model/</guid>
      <pubDate>Sat, 12 Apr 2025 07:42:45 GMT</pubDate>
    </item>
    <item>
      <title>[p]简单独立tfrecords数据集读取器具有随机访问和搜索功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，在工作中，我们正在使用tfrecords存储大多数数据集。但是不时。我们需要检查数据，以更好地对我们的模型进行更好的预测，例如为了找到特定类等的示例。由于Tfrecord在本质上是顺序的，它们不允许进行标准的随机访问切片。 我决定创建这个简单的工具，该工具允许为Tfrecrods创建一个简单的可搜索索引，以稍后可用于各种数据集分析。  这是项目页面： https://github.com/kmkolasinski/tfrecords-readers-readers-readers-reader  required Dataset can be read directly from Google Storage Indexing of 1M examples is fast and usually takes couple of seconds Polars is used for fast dataset querying tfrds.select(&quot;select * from index where name ~ &#39;rose&#39; limit 10&quot;)  Here is a quick start example from readme：  导入tensorflow_dataset作为tfds＃仅需要下载数据集导入数据集导入tfr_reader从pil导入import Import Import Import Import Importim Impart ipy ipyplot数据集，dataSet_info = tfds.load（tfford_flowers102&#39;，splite =&#39;train =&#39;train for_info = true）索引label = feature [label;]。值[0]返回{bail; bail; bail; bail，&#39;d dataset_info.features; dataset_info.data_dir，＃索引选项，如果已经创建索引fileepattern =;*。限制10&#39;）assert示例== tfrds [rows [; _row_id;]]样本，name = []，[]，[示例）中的示例（示例）（示例）：image = image.open（示例[emampe; image; image＆quot; image＆quot bytes_io [0]）。 samples.append（image）ipyplot.plot_images（样本，名称）   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kmkolasinski     [links]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jxbmss/p_simple_starlone_tfrecords_dataset_dataset_reader_with/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jxbmss/p_simple_standalone_tfrecords_dataset_reader_with/</guid>
      <pubDate>Sat, 12 Apr 2025 07:13:28 GMT</pubDate>
    </item>
    <item>
      <title>[d]添加新的词汇令牌 +微调LLMS以遵循说明是无效的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在使用指令调整LLMS和VLMS进行实验，要么将新的专用令牌添加到其相应的令牌/处理器中，或者不添加。设置是典型的：掩盖说明/提示（仅参加响应/答案）并应用CE损失。但是，没有什么特别的标准SFT。 但是，我观察到了使用其基本令牌/处理器训练的模型与经过修改的令牌训练的模型，对此有更好的验证损失和输出质量...对此有任何想法吗？  （我的hunch：很难增加这些新添加的令牌的可能性，而模型根本无法正确学习）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jx3zy0/d_adding_new_vocab_vocab_tokens_finetuning_llms_to/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jx3zy0/d_adding_new_vocab_tokens_finetuning_llms_to/</guid>
      <pubDate>Fri, 11 Apr 2025 23:42:34 GMT</pubDate>
    </item>
    <item>
      <title>[D]用于产品标题和类别标准化的微调BART  - 仍然不够准确，任何更好的方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在建立一个来自摩尔多瓦各种在线商店产品的价格比较网站。我在约20,000个手动标准化产品标题的自定义数据集上微调了一个BART模型，并损失了0.013。 I also trained a separate model for predicting product categories. Unfortunately, the results are still not reliable — the model struggles with both product title normalization and category assignment, especially when product names have slight variations or extra keywords. I don’t have access to SKU numbers from the websites, so matching must be done purely on text. Is there a better approach or model I might be missing?或者也许是专门针对此类问题设计的工具/应用程序？ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mali5k     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwz2k3/d_finetuned_bart_for_for_for_for_product_title_category/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwz2k3/d_finetuned_bart_for_product_title_category/</guid>
      <pubDate>Fri, 11 Apr 2025 19:59:31 GMT</pubDate>
    </item>
    <item>
      <title>[P]我们为LLM建立了类似OS的运行时间 - 好奇是否有人在做类似的事情？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，我们正在尝试使用AI本地运行时，该运行时间在2-5秒钟内以llms（例如13b – 65b）来捕捉llms（例如13b – 65b），并且动态运行50多个型号，每gpu始终在记忆中始终居住在ersign中，而不是传统的prial。 GPU执行 +内存状态和点播模型。这似乎解锁了：•实际的无服务器行为（无空闲成本）•低潜伏期时的多模型编排•更好地使用代理工作负载的GPU利用率 是否有人尝试过与多模型堆栈，代理工作流程或动态内存真实分配相似的东西很想听听别人如何接近这一点的 - 或者这甚至与您的中世纪需求保持一致。 很乐意在有用的情况下分享更多的技术细节！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_for_for_llms_curious_if/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_for_for_llms_curious_curious_if/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jwxght/pwe_built_an_oslike_runtime_for_llms_curious_if/</guid>
      <pubDate>Fri, 11 Apr 2025 18:50:35 GMT</pubDate>
    </item>
    <item>
      <title>[P]一种轻巧的开源模型，用于产生漫画</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jws42t/p_a_lightweight_opensource_model_for_generating/</guid>
      <pubDate>Fri, 11 Apr 2025 15:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jpdo7y/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</guid>
      <pubDate>Wed, 02 Apr 2025 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_be_hired/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Mon, 31 Mar 2025 02:30:37 GMT</pubDate>
    </item>
    </channel>
</rss>