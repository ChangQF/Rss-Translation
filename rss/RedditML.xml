<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 31 Aug 2024 21:14:28 GMT</lastBuildDate>
    <item>
      <title>[D] 上周医疗 AI：顶级研究论文/模型🏅（2024 年 8 月 24 日至 8 月 31 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5xnnr/d_last_week_in_medical_ai_top_research/</link>
      <description><![CDATA[   本周热门论文（8 月 24 日至 31 日）  MultiMed：多模式医疗基准   本文介绍了 MultiMed，它是多种医疗模式和任务的基准。 MultiMed 包含 256 万个样本，涵盖十种医疗模式，例如医疗报告、病理学、基因组学和蛋白质数据。  用于生成胸部 X 光片图像的基础模型   本文介绍了一种潜在扩散模型，该模型在自然图像和文本描述符对上进行了预训练，以生成多样化且视觉上合理的合成胸部 X 光片图像，其外观可以通过自由形式的医学文本提示进行控制。  MEDSAGE：医学对话摘要   本文利用 LLM 的上下文学习能力，并指示它们根据一些可用的带有录音的医学对话示例生成类似 ASR 的错误。  用于放射学报告生成的知识图谱   本文介绍了一个名为 ReXKG 的系统，该系统从处理后的报告中提取结构化信息以构建全面的放射学知识图谱。  探索用于胸部 X 光的多模态 LLM   本文介绍了 M4CXR，这是一种旨在增强 CXR 解释的多模态 LLM。该模型根据一个数据集按照视觉指令进行训练，该数据集以对话格式集成了各种特定于任务的数据集。  改进临床记录生成   本文介绍了使用 LLM 对临床记录生成领域的三个关键贡献。首先，介绍全面的数据集 CliniKnote；其次，提出 K-SOAP（关键字、主观、客观、评估和计划）记录格式。 - 第三，开发一个自动管道，从医患对话中生成 K-SOAP 笔记   详细查看完整帖子：https://x.com/OpenlifesciAI/status/1829984701324448051  感谢您的阅读！如果您知道任何遗漏的有趣论文，请随时在评论中分享。如果您在医疗 AI 方面有见解或突破，并希望在下周的版本中分享，请通过 Twt/x 与我们联系：OpenlifesciAI    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5xnnr/d_last_week_in_medical_ai_top_research/</guid>
      <pubDate>Sat, 31 Aug 2024 21:10:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 致力于欧洲基础架构的研究实验室 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5wema/research_labs_working_on_foundation_architecture/</link>
      <description><![CDATA[  由    /u/Thick-brain-dude  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5wema/research_labs_working_on_foundation_architecture/</guid>
      <pubDate>Sat, 31 Aug 2024 20:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[图片] AI 下 6x6 国际象棋，新算法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5w23s/p_ai_plays_chess_6x6_new_algorithm/</link>
      <description><![CDATA[我创建了一个新的机器学习算法来玩棋盘游戏，并训练它下 6x6 象棋。在消费级 PC 上训练五天后，我无法赢过它一局。这是 GitHub 链接，其中包含实现、权重和交互式演示：https://github.com/omikad/probs 。请推荐其他值得尝试的棋盘游戏    提交人    /u/Putrid-Start-3520   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5w23s/p_ai_plays_chess_6x6_new_algorithm/</guid>
      <pubDate>Sat, 31 Aug 2024 19:59:56 GMT</pubDate>
    </item>
    <item>
      <title>[N] YOLO 愿景 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5twq4/n_yolo_vision_2024/</link>
      <description><![CDATA[        提交人    /u/Ultralytics_Burhan   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5twq4/n_yolo_vision_2024/</guid>
      <pubDate>Sat, 31 Aug 2024 18:23:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 四年来我构建 MLOps 系统的经验教训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5ojdu/d_what_ive_learned_building_mlops_systems_for/</link>
      <description><![CDATA[这是我四年来构建 MLOps 系统所学到的知识 http://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html    提交人    /u/mburaksayici   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5ojdu/d_what_ive_learned_building_mlops_systems_for/</guid>
      <pubDate>Sat, 31 Aug 2024 14:27:27 GMT</pubDate>
    </item>
    <item>
      <title>[N][R] 发布 Re-LAION 5B：LAION-5B 的透明迭代，并附加安全修复</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5o0kj/nr_releasing_relaion_5b_transparent_iteration_on/</link>
      <description><![CDATA[        提交人    /u/Jamais_Vu206   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5o0kj/nr_releasing_relaion_5b_transparent_iteration_on/</guid>
      <pubDate>Sat, 31 Aug 2024 14:03:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何使用 FastText 实现在线/增量学习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5n5j7/d_how_can_i_implement_onlineincremental_learning/</link>
      <description><![CDATA[大家好，随着新的单个数据样本的到来，我正在逐步训练 FastText。由于 FastText 是使用 SGD 进行训练的，所以我认为这应该是可能的。但是，我还没有找到使用 Python API 执行此操作的方法。可以加载预训练的词向量，我认为我们可以保存和加载训练后的向量来实现这一点，但我担心这可能会产生太多开销。有人找到处理这个问题的方法吗？有什么建议吗？    提交人    /u/hellowrld3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5n5j7/d_how_can_i_implement_onlineincremental_learning/</guid>
      <pubDate>Sat, 31 Aug 2024 13:22:24 GMT</pubDate>
    </item>
    <item>
      <title>[P] 受 Andrej Karpathy 启发，我制作了 NLP - Zero to Hero</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5ljxq/p_inspired_by_andrej_karpathy_i_made_nlp_zero_to/</link>
      <description><![CDATA[        由    /u/Particular_Tap_4002   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5ljxq/p_inspired_by_andrej_karpathy_i_made_nlp_zero_to/</guid>
      <pubDate>Sat, 31 Aug 2024 11:56:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 感知相似性用于衡量游戏中的决策风格和政策多样性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5hfzf/r_perceptual_similarity_for_measuring/</link>
      <description><![CDATA[论文： https://openreview.net/forum?id=30C9AWBW49 摘要：  定义和衡量决策风格（也称为游戏风格）在游戏中至关重要，因为这些风格反映了广泛的个性和多样性。然而，找到一种普遍适用的衡量这些风格的标准是一项挑战。基于第一个基于游戏屏幕和原始动作衡量游戏风格相似性的无监督指标，通过识别具有离散表示的可比状态来计算策略距离，我们引入了三种增强功能来提高准确性：具有不同状态粒度的多尺度分析、植根于心理学的感知内核以及利用交并法进行有效评估。这些创新不仅提高了测量精度，而且还提供了对人类对相似性的认知的洞察。在两款赛车游戏和七款 Atari 游戏中，我们的技术显著提高了零样本游戏风格分类的精度，在少于 512 个观察-动作对（不到这些游戏的一半）的情况下实现了超过 90% 的准确率。此外，我们的实验展示了离散游戏风格测量在益智和棋盘游戏中的潜力。我们还开发了一种使用这些指标评估决策多样性的算法。我们的研究结果改进了端到端游戏分析的测量和人工智能对不同游戏风格的演变。  您如何看待使用离散状态来比较风格的想法？    提交人    /u/Cold-Needleworker709   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5hfzf/r_perceptual_similarity_for_measuring/</guid>
      <pubDate>Sat, 31 Aug 2024 07:08:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有关重现结果的帮助</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5fq26/d_help_regarding_reproducing_results/</link>
      <description><![CDATA[你好。我最近一直在研究对抗性自监督学习，并试图重现 DynCL 论文《重新思考数据增强在对抗性对比学习中的作用》中的结果。在他们的 GitHub 存储库中，他们提供了一个检查点文件，当我对该检查点进行微调时，结果是可重现的。但是，当我尝试训练他们的模型时，它的表现严重不佳。我尝试在 GitHub 存储库中提出问题，但没有得到回复。那么，我该怎么做？如果我想用我的方法对其进行基准测试，我应该使用我重现的结果还是他们论文中给出的值？他们的结果似乎有点可疑，但经过非常简单的修改，他们取得的改进非常大。尽管我希望并希望这是真的，但结果的重现似乎并不意味着如此。 任何帮助都值得赞赏。谢谢。     由    /u/SmartEvening 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5fq26/d_help_regarding_reproducing_results/</guid>
      <pubDate>Sat, 31 Aug 2024 05:12:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能博士学位建议——需要顶尖大学吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5ackj/d_ai_phd_advice_top_university_required/</link>
      <description><![CDATA[大家好，我来征求一下建议。我是俄勒冈州立大学的 AI 博士生。我对使用 genAI 为使用 rag 管道的医生提供临床决策辅助特别感兴趣，这显然比我刚才解释的要复杂得多。 我的问题是：大多数情况下，我在类似的服务器上看到，你必须就读顶尖机构才能进入知名公司等。 因为我没有就读排名前 10 的大学，这会妨碍我找到工作的机会吗？根据 csrankings.org 的排名，俄勒冈州立大学排名第 53 位。 任何建议/评论都非常感谢    提交人    /u/frankies_wrld   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5ackj/d_ai_phd_advice_top_university_required/</guid>
      <pubDate>Sat, 31 Aug 2024 00:15:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] 对于多智能体纸牌游戏，我应该尝试哪种 RL 算法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f55ndj/p_what_rl_algorithm_should_i_try_for_a_multiagent/</link>
      <description><![CDATA[我目前正在学习 RL 和 AI，并在 PyTorch 中尝试了 DQN 和 Double DQN。 我想尝试实现一个多智能体纸牌游戏，我想听听你们的意见，了解最好的入门方法。由于这是一个不完全信息游戏，我认为更简单的方法不适用于此。 我为此建立了一个概念证明，其中 4 个 Double DQN 智能体相互对抗，每个模型都会从环境中分配一个玩家，然后进行游戏。作为状态，我给了它表中所有类型卡片的数量（13 个值）、手中所有类型卡片的数量（13 个值）以及有关游戏的更多信息。 不幸的是，正如预期的那样，它无法学到太多东西，所以我正在寻找其他选择。 这是游戏：https://bicyclecards.com/how-to-play/presidents 此外，我遇到的一个问题是模型在选择有效的动作时遇到了问题（它通常没有足够的所选类型的卡片，或者它与桌面上的卡片不匹配）。    提交人    /u/Neither_Butterfly_51   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f55ndj/p_what_rl_algorithm_should_i_try_for_a_multiagent/</guid>
      <pubDate>Fri, 30 Aug 2024 20:43:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年 Google 博士奖学金结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f4w1tr/d_results_for_google_phd_fellowship_2024/</link>
      <description><![CDATA[有人从 Google 那里听说过关于博士奖学金计划结果的消息吗？我以为他们会在去年 7 月通知大家。    提交人    /u/EDEN1998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f4w1tr/d_results_for_google_phd_fellowship_2024/</guid>
      <pubDate>Fri, 30 Aug 2024 14:02:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将任何初学者问题发布至 r/MLQuestions！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3yfjg/d_post_any_bginner_questions_to_rmlquestions/</link>
      <description><![CDATA[我最近继承了 subreddit r/MLQuestions，因为其他版主分别有 10 个月和 4 年没有活动。我一直在整理子版块，添加标签、规则等，并试图增加参与度，使其对那些想要提问的人更有用。基本上就是 stackoverflow，但专门用于解决有关 ML 的初学者问题。所以，如果你们有不好意思在这里问的问题，请在 r/MLQuestions 上提问！我还将推出一个类似于 r/changemyview 的系统，其中每回答一个问题，他们的用户天赋就会增加一个，显示他们回答了多少问题！ 顺便说一句，版主允许我发布这篇文章，所以非常感谢你们，非常酷。    提交人    /u/NoLifeGamer2   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3yfjg/d_post_any_bginner_questions_to_rmlquestions/</guid>
      <pubDate>Thu, 29 Aug 2024 09:50:39 GMT</pubDate>
    </item>
    </channel>
</rss>