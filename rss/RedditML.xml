<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Fri, 16 Aug 2024 21:14:24 GMT</lastBuildDate>
    <item>
      <title>[D][P] 需要至少 20GB 的旧 GPU（V100 或更旧）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eu028b/dp_need_older_gpu_v100_or_older_of_atleast_20gb/</link>
      <description><![CDATA[您好。我联系您是因为我急需帮助。哈哈。有一个特定的模型只能在 torch 1.5 上重现。很难相信还有人在使用它。新的 CUDA 驱动程序和 Nvidia GPU 根本不支持这个版本。我尝试找到一个较旧的 GPU 并使用那里的 docker 来重现结果。结果确实是可以重现的，但 GPU 非常慢。所以我花了几天左右的时间寻找 GPU 来在那里完成我的消融。这就是我最终来到这里的原因。所以如果你有它并且愿意让我使用它，我会非常高兴。提前谢谢你。 另外，还有一个附带问题。我试图重现 VITA 小组的 Adversarial Contrastive Learning 2020 NIPS 论文。如果你们中的任何人能够在较新版本的 torch 上重现这项工作，并且可以告诉我应该做哪些更改才能在新 torch 上重现它们，那将意义重大。 提前致谢。    提交人    /u/SmartEvening   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eu028b/dp_need_older_gpu_v100_or_older_of_atleast_20gb/</guid>
      <pubDate>Fri, 16 Aug 2024 21:10:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GPT-3.5 和 Haiku 在成本、延迟和准确性方面击败 GPT-4o 结构化输出</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etyrs8/r_beating_gpt4o_structured_output_with_gpt35_and/</link>
      <description><![CDATA[完整帖子：https://www.boundaryml.com/blog/sota-function-calling 使用 BAML，我们几乎解决了1 个 伯克利函数调用基准 (BFCL)（每个模型）（gpt-3.5+）。期待很快分享 arXiv 论文！ https://preview.redd.it/78uxa0xx5pid1.png?width=916&amp;format=png&amp;auto=webp&amp;s=f36e9c6fbb8ea1939c5406e552b0dcf0a4f6fe20 主要发现  与任何本机函数调用 API 相比，BAML 在函数调用方面更准确、更便宜。它比 OpenAI 的 FC-strict API 快 2-4 倍。 BAML 的技术与模型无关，并且可以与任何模型一起使用而无需修改（甚至是开源模型）。 gpt-3.5-turbo、gpt-4o-mini 和 claude-haiku 与 BAML 配合使用的效果几乎与具有结构化输出的 gpt4o 一样好（不到 2%） 使用 FC-strict 而不是简单的函数调用可以改进每个较旧的 OpenAI 模型，但是 gpt-4o-2024-08-06 会变得更糟  背景 到目前为止，从 LLM 获得更好结果的唯一方法是：  迅速设计使用更长更复杂的提示来训练它 训练更好的模型  BAML 的不同之处  用类似 typescript 的定义替换 JSON 模式。例如 string[] 比 {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}&gt; 更容易理解。 使用一种新颖的解析技术（Schema-Aligned Parsing）代替 JSON.parse。SAP 允许输出中出现更少的标记，而不会因 JSON 解析而出现错误。例如，即使键周围没有引号，也可以解析它。 PARALLEL-5 [ { streaming_service: &quot;Netflix&quot;, show_list: [&quot;Friends&quot;], sort_by_rating: true }, { streaming_service: &quot;Hulu&quot;, show_list: [&quot;The Office&quot;, &quot;Stranger Things&quot;], sort_by_rating: true } ]  我们使用提示 DSL（BAML）来实现这一点[2]，而没有使用 JSON 模式或任何类型的约束生成。我们还与使用“工具”API 的 OpenAI 的结构化输出进行了比较，我们称之为“FC-strict”。 对未来的思考 模型真的非常好，是一种语义理解。 模型在必须完美的事情上真的很糟糕，比如完美的 JSON、完美的 SQL、编译代码等。 我们认为，与其努力训练结构化数据的模型或在生成时约束令牌，不如将工程努力应用于稳健地处理模型输出等领域，这将带来尚未开发的价值。    提交人    /u/kacxdak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etyrs8/r_beating_gpt4o_structured_output_with_gpt35_and/</guid>
      <pubDate>Fri, 16 Aug 2024 20:15:53 GMT</pubDate>
    </item>
    <item>
      <title>[P]：使用 LLM 自动化脚本中的 API 文档。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etyfi2/p_automating_api_documentation_in_the_script/</link>
      <description><![CDATA[大家好，我目前正在开展一个项目，旨在创建一个 AI 代码助手，从开发人员的角度帮助解决一些痛点。 我的项目中的一个功能是在输入脚本中创建 api 文档。 我正在使用 LLM 来帮助构建此功能，特别是 llama 3，使用 llama api，这是一个无键 api。 到目前为止，我正在努力构建此功能，目前我还没有主意。 希望大家对如何进一步进行提出想法和意见。 此外，如果有人想合作，这个项目是一个开源项目，欢迎所有贡献。    提交人    /u/Sherlock_holmes0007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etyfi2/p_automating_api_documentation_in_the_script/</guid>
      <pubDate>Fri, 16 Aug 2024 20:01:34 GMT</pubDate>
    </item>
    <item>
      <title>如何练习[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etxtbs/how_to_practice_d/</link>
      <description><![CDATA[我已经学习了回归、分类、聚类。我想继续练习。你们怎么练习的？    提交人    /u/lazemon   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etxtbs/how_to_practice_d/</guid>
      <pubDate>Fri, 16 Aug 2024 19:35:19 GMT</pubDate>
    </item>
    <item>
      <title>多模态LLM解析策略[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ettoy9/multimodal_llm_parsing_strategyd/</link>
      <description><![CDATA[我正在尝试从车辆用户手册中提取图像、表格和文本，我目前的方法是使用预训练的 yolov8 模型来处理图像，使用微调的模型来处理表格，然后使用 OCR 来提取文本。除了在我的自定义数据集上对 YOLO 进行微调外，还有其他有前途的方法吗？我在想我会使用多模态 LLM 来总结每个有图像的页面，并提示它为每个图像专门生成单独的摘要。正在寻找具有成本效益的替代方法，但我需要提取和总结至少 95% 的相关图像    提交人    /u/ashblue21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ettoy9/multimodal_llm_parsing_strategyd/</guid>
      <pubDate>Fri, 16 Aug 2024 16:45:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] 生产中的迭代模型改进</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etn0x2/p_iterative_model_improvement_in_production/</link>
      <description><![CDATA[大家好， 我创建了一个多类分类模型，并在一个带标签的数据集上对其进行了训练。说实话，在本地数据集上表现得相当不错，现在我正打算将其软启动到生产环境中。输入数据将转换为 n 维输入向量，绘制在图表上时不会形成凸形或规则形状（至少我的 EDA 显示了这一点）。由于我无法预见所有可能的模型输入，因此该模型无法完美处理每种情况，我想这没问题，但我正在寻找广泛的用例。这将导致大量误报，我想将其迭代添加到我的训练数据语料库中并随着时间的推移改进模型。 我正在寻找一种有效的方法来识别和管理这些误报。我在考虑：1）随机抽样数据子集并手动标记以验证其是真阳性还是假阳性。 2）获取用户反馈以识别错误分类的反馈。 3）使用聚类技术，其指标包括 Silhouette 分数、Davies-Bouldin 指数、Calinski-Harabasz 指数 (CH)、归一化互信息 (NMI) 或 Dunn 指数。 4）结合 1）和 3）？识别一些假阳性，然后通过聚类找到可能也是假阳性的类似结果 我的最终目标是创建一个随着时间推移不断改进的管道。您将如何解决这个问题？谢谢！    提交人    /u/Queasy_Tailor_6276   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etn0x2/p_iterative_model_improvement_in_production/</guid>
      <pubDate>Fri, 16 Aug 2024 12:06:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT 样式模型可以用于文件压缩、图像升级和恢复吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etlwku/d_can_gptstyle_models_be_used_for_file/</link>
      <description><![CDATA[我一直在思考这个问题 - 是否真的有可能直接在文件（如图像文件）的原始字节数据上训练 GPT 样式的仅解码器架构？我们的想法是学习文件数据中的基础统计模式，然后可能使用模型的概率分布来执行以下操作：  通过更有效地对数据进行建模来设计新的文件压缩算法 通过让模型生成文件中缺失/损坏的部分，对模型进行微调以执行图像升级或恢复等任务  这似乎是一种有趣的方法，因为 GPT 样式的模型已经表现出非凡的能力来捕获自然语言数据中的统计模式。但我不确定同样的原则是否适用于图像文件等更结构化的二进制数据。 这是一个愚蠢或不切实际的想法吗？以前有人尝试过这样的事情吗？我真的很想听听社区对这种方法的潜力和挑战的看法。    提交人    /u/No-Point1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etlwku/d_can_gptstyle_models_be_used_for_file/</guid>
      <pubDate>Fri, 16 Aug 2024 11:06:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] PINN 的嵌套 AD</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etkmm6/r_nested_ad_for_pinns/</link>
      <description><![CDATA[我目前正在尝试求解 Lu = f 形式的 PDE，其中 L 是二阶微分算子。我正在进行无监督学习，损失函数只是残差 Lv - f 的通常 MSE，其中 v 是我对 u 的近似值。 由于 L 具有二阶导数，这意味着我们在训练期间获取网络梯度时会获取三阶导数。这太麻烦了（我使用的是 Flux.jl，而 Zygote 无法很好地处理嵌套的三阶 AD），所以我最终求助于 L 的有限差分离散化，这样 AD 的唯一应用就是网络本身的梯度，而没有另外 2 个嵌套的梯度。当然，我很想避免使用有限差分，但我真的没有其他方法。 有人处理过类似的情况吗？根据我的发现，似乎大家一致认为三阶嵌套 AD 确实不切实际，但我希望它还有更多内容。 提前感谢任何输入！ 编辑：在许多情况下，可以以变分形式重新计算问题以将阶数降低 1，但不幸的是，这对我的 PDE 来说是不可能的    提交人    /u/Fleico   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etkmm6/r_nested_ad_for_pinns/</guid>
      <pubDate>Fri, 16 Aug 2024 09:45:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么是“低三角因果掩蔽”？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etia89/d_what_is_lowtriangle_causal_masking/</link>
      <description><![CDATA[在 https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html 上写着“[上下文并行性很好，因为] 消除了低三角因果掩蔽导致的不必要计算”。这个术语起源于哪里，请问我可以看一个例子吗？    提交人    /u/khidot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etia89/d_what_is_lowtriangle_causal_masking/</guid>
      <pubDate>Fri, 16 Aug 2024 07:03:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 可重复性检查表 AAAI25</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ethhro/d_reproducibility_checklist_aaai25/</link>
      <description><![CDATA[大家好， 我正在准备向 AAAI 提交论文，我遇到了可重复性检查表的要求。我对如何提交此检查表有点困惑。它应该作为附录包含在主要论文中，还是需要作为单独的文档上传？此外，如果它包含在论文中，它是否计入页数限制？ 任何曾经向 AAAI 提交过论文或有此过程经验的人的见解都将不胜感激！ 提前致谢！    提交人    /u/Ok_Butterfly7408   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ethhro/d_reproducibility_checklist_aaai25/</guid>
      <pubDate>Fri, 16 Aug 2024 06:10:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 审稿人 2 - NeurIPS</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1etb4qh/d_reviewer_2_neurips/</link>
      <description><![CDATA[NeurIPS 辩驳期终于结束了。大家的审稿怎么样？ 我和一位审稿人打过交道，这是最糟糕的经历。对于最初的评论，他/她只写了一个简短的段落，问了一堆可以通过论文内容轻松回答的问题，然后给了 3 分和 4 分的置信度。对于辩驳，这位审稿人给出了相互矛盾的陈述，甚至无法理解训练数据和测试数据之间的区别。我花了整整两天时间解释这种区别。最后，审稿人留下了关于论文的错误陈述然后消失了。典型的审稿人 2。    提交人    /u/DrSolar789   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1etb4qh/d_reviewer_2_neurips/</guid>
      <pubDate>Fri, 16 Aug 2024 00:27:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] LayerMerge：通过层修剪和合并实现神经网络深度压缩 (ICML 2024)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1esy876/r_layermerge_neural_network_depth_compression/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2406.12837 代码： https://github.com/snu-mllab/LayerMerge TL;DR：LayerMerge 通过修剪和合并卷积层和激活层来减少 CNN 和扩散模型的深度。 定性示例LayerMerge 概述：LayerMerge 是一种新颖的方法，可提高卷积神经网络的效率而不会损失性能。传统的减少网络深度的方法通常遵循以下两种方法之一： 1. 修剪卷积层：积极删除参数，冒着丢失重要信息的风险。 2. 修剪激活层和合并层：消除冗余激活层并合并产生的连续卷积层，这可能会增加内核大小并抵消速度增益。 LayerMerge 通过联合修剪卷积层和激活函数来解决这些问题。它优化要删除的层，加快推理速度，同时最大限度地减少性能损失。由于此选择过程涉及指数搜索空间，我们制定了一个新的替代优化问题并通过动态规划有效地解决了它。 我们的结果表明，LayerMerge 在减少网络深度方面优于当前方法，包括图像分类和生成 演示展示了 LayerMerge 在 ImageNet 上使用 MobileNetV2-1.0 以及在 CIFAR10 上使用 DDPM 的有效性。    提交人    /u/jusjinuk   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1esy876/r_layermerge_neural_network_depth_compression/</guid>
      <pubDate>Thu, 15 Aug 2024 15:32:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我设计了一种潜在的类似 Transformer 的架构，时间复杂度为 O(n)，并行化后可简化为 O(log n)。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1esteqd/r_ive_devised_a_potential_transformerlike/</link>
      <description><![CDATA[[R] 我尝试构建一个使用简单除法和计算方法的架构。从我所看到和理解的情况来看，它似乎是可行的，至少在我看来是这样。虽然我的代码中可能存在错误，但我已经检查并测试过它，没有发现任何错误。 我想知道这种方法是否有什么新意。如果是这样，我有兴趣与您合作撰写有关它的研究论文。此外，如果您能帮助我检查代码以查找任何潜在错误，我将不胜感激。 但最重要的是，我想知道这个架构，它是新的吗，有没有人尝试过这个或类似的东西， 我写了一篇包含代码的 Medium 文章。文章可从以下网址获取：https://medium.com/@DakshishSingh/equinox-architecture-divide-compute-775a8ff698fe 非常感谢您就此事提供的帮助和想法。如果您有任何疑问或需要澄清，请随时询问。    提交人    /u/Conscious-Gazelle-91   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1esteqd/r_ive_devised_a_potential_transformerlike/</guid>
      <pubDate>Thu, 15 Aug 2024 12:03:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持有效，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢所有人在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Aug 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>