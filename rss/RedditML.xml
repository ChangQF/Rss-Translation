<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 19 Jan 2024 03:15:47 GMT</lastBuildDate>
    <item>
      <title>[D] Transformer多头注意力实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a8klj/d_transformer_multihead_attention_implementation/</link>
      <description><![CDATA[我一直在关注带注释的 Transformer&lt; /a&gt; 实现变压器架构。在多头注意力类的forward()方法中，Query、Key &amp;值与相应的投影矩阵相乘； W_q, W_k, W_v.  查询，键，值 = [ lin(x).view(nbatches, -1, self.h, self.d_k) .transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ]  这里，lin(x) 正在被重塑为 (nbatches, -1, self.h, self.d_k) 和维度 1 &amp; 2 正在被转置，这使得维度 (nbatches, self.h, -1, self.d_k)。 我无法理解为什么他们不直接这样做lin(x).view(nbatches, self.h, -1, self.d_k)?   由   提交 /u/Melodic_Stomach_2704   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a8klj/d_transformer_multihead_attention_implementation/</guid>
      <pubDate>Fri, 19 Jan 2024 02:48:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能很强大实际上是一个严肃而真实的领域正在研究，或者只是人们正在宣传的另一种炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</link>
      <description><![CDATA[强大的人工智能。 （通用人工智能）实际上是一个严肃的研究领域，还是只是刚刚阅读/观看科幻小说的人的纯粹炒作？ 强人工智能/又名是强人工智能吗？ AGI实际上被一些研究人员/机构认真对待，他们认为它最终可以实现，或者它是人们一直在炒作的另一种奇特的技术蒸汽软件，但实际上，那些在该领域工作的人知道这样的想法实际上无法实现，因为严格的物理限制，或者如果发生的话，需要几个世纪才能实现？ 因为过去对于许多未来主义者来说有很多歇斯底里的感觉技术，这些技术被很多不了解蹲点的人大肆宣传，但它无法在实践中发挥作用（即 Em Drive、石墨烯、富勒烯、纳米机器人、Bussard Ramjet、聚变能源等）。   由   提交/u/Enzo-chan  /u/Enzo-chan  reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</guid>
      <pubDate>Fri, 19 Jan 2024 01:16:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 作者与合著者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a6h3n/d_author_vs_coauthor/</link>
      <description><![CDATA[这在 iclr 模板中有何不同？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a6h3n/d_author_vs_coauthor/</guid>
      <pubDate>Fri, 19 Jan 2024 01:07:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 说话者分类，通过视频识别嘴唇运动</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a4u80/d_speaker_diarization_with_video_recognition_of/</link>
      <description><![CDATA[您好！我目前正在使用 Whisperx 进行说话人识别，效果非常好。不过，我记得读到过另一个说话者分类框架，它使用图像识别来识别说话者嘴唇何时移动，以提供更精确的识别。有谁知道这是什么框架吗？我已经找了一整个星期了，但还是找不到。谢谢！   由   提交/u/Fun-Medium8799  /u/Fun-Medium8799 reddit.com/r/MachineLearning/comments/19a4u80/d_speaker_diarization_with_video_recognition_of/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a4u80/d_speaker_diarization_with_video_recognition_of/</guid>
      <pubDate>Thu, 18 Jan 2024 23:53:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 2 内部结构</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a1mup/p_pytorch_2_internals/</link>
      <description><![CDATA[嗨，刚刚分享了关于 PyTorch 内部结构的幻灯片，涵盖 Dynamo、Inductor、ExecuTorch 等最近的项目，我认为这里可能会有一些人感兴趣。 &lt;!-- SC_ON - -&gt;  由   提交 /u/perone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a1mup/p_pytorch_2_internals/</guid>
      <pubDate>Thu, 18 Jan 2024 21:37:58 GMT</pubDate>
    </item>
    <item>
      <title>格斗游戏研究中不断学习MARL[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a1c1n/continous_learning_marl_in_fighting_game_research/</link>
      <description><![CDATA[我和我的朋友正在研究在格斗游戏中使用 MARL，其中演员/代理同时提交输入，然后由格斗游戏物理引擎。有许多论文在格斗游戏的背景下讨论 DL / RL / 一些 MARL，但值得注意的是，它们不包含源代码或实际上谈论其方法，而是谈论普遍的发现/见解。 现在我们正在考虑使用 Pytorch（在 CUDA 上运行以提高训练速度）和 Petting Zoo（MARL 体育馆的扩展），特别是使用 AgileRL 库进行超参数优化。我们很清楚，超参数如此之多，当我们试图改进问题时，知道要改变什么是很棘手的。我们设想，我们有 8 个左右的研究游戏引擎实例（我有 10 个核心 CPU）连接到 10 个宠物动物园（可能是敏捷 RL 修改版）训练环境的实例，其中输入/输出在引擎和训练环境，来回。 我想我是在寻求一些关于我们正在使用的工具的一般建议/提示和反馈。如果您知道解决类似问题的特定教科书、GitHub 存储库的研究论文，那可能会非常有帮助。我们有一些关于超参数优化的资源以及一些关于如何摆弄设置的想法，但是仅仅为了进行人工智能学习而设计的项目初始结构/启动代码有点棘手。我们确实有一个 MARL 工作的 Connect 4 训练示例，由 AgileRL 提供。但我们正在寻求将其从依次输入提交改为同时输入提交（这当然是可能的，MARL 用于 MOBA 等实时游戏中）。 您可以向我们提供的任何信息是一种祝福并且有帮助。非常感谢您抽出时间。   由   提交/u/stardoge42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a1c1n/continous_learning_marl_in_fighting_game_research/</guid>
      <pubDate>Thu, 18 Jan 2024 21:25:35 GMT</pubDate>
    </item>
    <item>
      <title>获取标记数据的成本有多大？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/</link>
      <description><![CDATA[正在做我的主动学习硕士论文。文献中的一个关键点是，主动学习在有大量未标记数据的情况下可能很有用，并且与标记相关的成本很高，因此如果模型可以“选择”一个样本的子集是最“信息丰富”的，然后可以对它们进行标记。 但是，我有点意识到，尽管这种主动学习的东西很有趣并且我可能会继续，但我只是不&#39;不太明白公司中标签数据不可用/成本高昂的现实情况。当然，我知道当我阅读它时，会在某些特定情况下发生这种情况： NLP - 像语音识别这样的任务可能需要对音频进行标记，或者在信息提取中需要注释和语料库中的某些内容来 但是，我正在阅读的文献是 2009 年左右的调查，我想从那时起，像这样的问题就不会真正存在了。所以我想知道有多少次只有一堆未标记的数据等待标记。现在是否还有主动学习的需求？ 我认为我正在“转向”的一个领域可能是在在线“流”数据中寻找主动学习，而我想象的东西并不是这样的。尽快标记。   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/</guid>
      <pubDate>Thu, 18 Jan 2024 20:30:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 情境感知元学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199yknn/r_contextaware_metalearning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.10971 OpenReview： https://openreview.net/forum?id=lJYAkDVnRU https： //openreview.net/forum?id=SAu298HU2I 摘要：  像 ChatGPT 这样的大型语言模型表现出了非凡的能力在推理过程中学习新概念，无需任何微调。然而，经过训练以在推理过程中检测新对象的视觉模型无法复制这种能力，而是要么表现不佳，要么需要对类似对象进行元训练和/或微调。在这项工作中，我们提出了一种元学习算法，通过在推理过程中学习新的视觉概念而无需微调来模拟大型语言模型。我们的方法利用冻结的预训练特征提取器，类似于上下文学习，将元学习重新定义为对具有已知标签的数据点和具有未知标签的测试数据点进行序列建模。在 11 个元学习基准中的 8 个上，我们的方法（无需元训练或微调）超过或匹配最先进的算法 P&gt;M&gt;F，该算法在这些基准上进行了元训练基准测试。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199yknn/r_contextaware_metalearning/</guid>
      <pubDate>Thu, 18 Jan 2024 19:34:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] 令牌混合：通过跨示例聚合实现高效法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199y7y9/r_mixture_of_tokens_efficient_llms_through/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2310.15961 代码：https ://github.com/llm-random/llm-random 博客文章：https://llm-random.github.io/posts/mixture_of_tokens/ 摘要：  尽管专家混合 (MoE) 模型有望增加 Transformer 模型的参数数量，同时保持训练和推理成本，但其应用存在明显的缺点。这些模型的关键策略是，对于每个处理的令牌，最多激活几个专家 - 广泛的前馈层的子集。但这种方法并非没有挑战。匹配专家和代币的操作是离散的，这使得MoE模型容易出现训练不稳定和专家利用率不均匀等问题。旨在解决这些问题的现有技术（例如辅助损失或平衡感知匹配）会导致模型性能较低或更难以训练。针对这些问题，我们提出了代币混合，这是一种完全可微分的模型，它保留了 MoE 架构的优点，同时避免了上述困难。这种方法不是将令牌路由给专家，而是在将不同示例中的令牌提供给专家之前将其混合，从而使模型能够从所有令牌-专家组合中学习。重要的是，可以禁用这种混合，以避免在推理过程中混合不同的序列。至关重要的是，这种方法与屏蔽和因果大语言模型训练和推理完全兼容。  之前的讨论：https://www.reddit.com/r/mlscaling/comments/17ha25s/mixture_of_tokens_efficient_llms_through/ &lt; !-- SC_ON --&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199y7y9/r_mixture_of_tokens_efficient_llms_through/</guid>
      <pubDate>Thu, 18 Jan 2024 19:19:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当超过训练上下文长度时，是什么导致法学硕士表现下降？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199wwhq/d_what_causes_llm_performance_to_degrade_when/</link>
      <description><![CDATA[大家好 我正在阅读 StreamingLLMs 论文 https://arxiv.org/pdf/2309.17453.pdf 并回到我一直想知道的一个问题。是否很好地理解了什么是“限制”？变压器内的上下文长度？为什么它不能泛化到超出其训练的序列长度。 我的一个猜测是，这与原始的绝对位置嵌入有关。一旦超过某个位置索引，您就无法为最新的标记分配唯一的位置嵌入（因为使用的 sin/cos 函数是周期性的） - 如果这种预感不正确，请纠正我。 但是，较新的模型使用相对位置嵌入，例如 RoPE、AliBi 和 YaRN。如果我没有弄错的话，这些作品背后的动机（至少部分是）是为了帮助模型泛化到超出其原始训练上下文长度的范围。然而，根据 Streaming LLM 论文的演示，RoPE 或 AliBi 嵌入的实际情况并非如此。据我所知，他们没有触及 YaRN。 发生这种情况的原因是什么？引入新的标记将输入序列长度超出训练时的长度会如何影响模型的性能？我的两个最好的疯狂猜测是，可能是 a) 由于注意力内的 SoftMax 分布采用了模型不习惯看到的值，因为长度超过了训练窗口，或者 b) 随着序列变得越来越长更多信息被打包到转换器内的中间标记表示中，并且超出训练时使用的上下文长度会添加模型无法处理的额外信息？ 正如我所提到的，这些只是随机的疯狂的猜测，所以我很想知道是否有正确的答案或者当前的思路可能是什么！ ​   由   提交 /u/lightSpeedBrick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199wwhq/d_what_causes_llm_performance_to_degrade_when/</guid>
      <pubDate>Thu, 18 Jan 2024 18:25:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] EPU-CNN：用于可解释计算机视觉的广义加法 CNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199qw5i/r_epucnn_generalized_additive_cnn_for/</link>
      <description><![CDATA[论文：https:/ /www.nature.com/articles/s41598-023-38459-1 代码：https ://github.com/innoisys/EPU-CNN 摘要：卷积神经网络（CNN）模型在高风险领域的采用因无法满足社会的需求而受到阻碍。决策的透明度。到目前为止，已经出现了越来越多的方法来开发可通过设计解释的 CNN 模型。然而，此类模型无法在保持良好性能的同时提供符合人类感知的解释。在本文中，我们通过一种新颖的通用框架来应对这些挑战，该框架用于实例化本质上可解释的 CNN 模型，称为 E pluribus unum 可解释 CNN (EPU-CNN)。 EPU-CNN 模型由 CNN 子网络组成，每个子网络接​​收输入图像的不同表示，表达感知特征，例如颜色或纹理。 EPU-CNN 模型的输出由分类预测及其解释（根据输入图像不同区域感知特征的相对贡献）组成。 EPU-CNN 模型已在各种公开可用的数据集以及贡献的基准数据集上进行了广泛的评估。医学数据集用于证明 EPU-CNN 在医学风险敏感决策中的适用性。实验结果表明，EPU-CNN 模型可以实现与其他 CNN 架构相当或更好的分类性能，同时提供人类可感知的解释。    由   提交/u/ashenone420  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199qw5i/r_epucnn_generalized_additive_cnn_for/</guid>
      <pubDate>Thu, 18 Jan 2024 14:05:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] EarthPT：时间序列变压器基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</link>
      <description><![CDATA[想要分享 EarthPT 的代码版本，这是一个在零样本设置下预测未来卫星观测的模型！我是第一作者，所以请向我提出任何问题。 EarthPT 是一个 7 亿参数解码变压器基础模型，以自回归自监督方式训练，并专门针对 EO 用例开发头脑。 EarthPT 可以准确预测未来 400-2300 nm 范围内的卫星观测结果（我们发现了六个月！）。 EarthPT 学到的嵌入包含语义上有意义的信息，可用于下游任务，例如作为高度精细的动态土地利用分类。 对我来说最酷的收获是 EO 数据在理论上为我们提供了千万亿的训练标记。因此，如果我们假设 EarthPT 遵循类似于大型语言模型 (LLM) 导出的神经缩放定律，那么目前对于缩放 EarthPT 和其他类似的“大型观测模型”没有数据强加的限制。(!) 代码：https://github.com/aspiaspace/EarthPT 论文：https://arxiv.org/abs/2309.07207   由   提交/u/Smith4242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</guid>
      <pubDate>Thu, 18 Jan 2024 13:21:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] OpenAI 如何增加 GPT-4 迭代的上下文长度？他们是否从头开始重新训练 GPT-4-1106？或者是稀疏注意力、分块等技术的更黑客组合？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/</link>
      <description><![CDATA[正如标题所述，开始思考 GPT-4 衍生模型及其制作方式。我知道事情发展得很快，而且 OpenAI 绝不是“开放”的，但是关于它是如何完成的猜测是什么？ 我不了解 LLM 进展的所有最新细节，但来自根据我对注意力机制的理解，通常你必须从头开始重新训练变压器以增加上下文大小。 但如果是这样的话，他们是否也必须重做所有 RLHF？或者是否有针对 RLHF 步骤的高效迁移学习技术？ 我很想看到一些将 GPT-4 迭代的评估相互比较的论文（如果您知道的话可以链接）。即使假设 RLHF 是完全可移植的，我们是否仍然期望 GPT-4 系列中的模型之间存在可测量的差异？ 我想知道这些模型之间是否存在任何有洞察力的性能怪癖，例如对于编码任务，32k 0613 模型的性能可能比 8k 基本模型更好，但 128k 1106 比 0613 差，因为在给定相同数量的参数、相同的训练数据等的情况下，上下文大小的回报会下降。   由   提交 /u/great_waldini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/</guid>
      <pubDate>Thu, 18 Jan 2024 10:28:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]本文的分区是否会导致数据泄露？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</link>
      <description><![CDATA[我最近对 ​​这项研究。总而言之，他们使用文本嵌入和梯度提升来根据财报电话会议记录来预测 CEO 性格得分。他们分析了约 200 位首席执行官，将每位首席执行官的电话分为多个部分以增加数据点。然而，每位 CEO 都会出现在训练集和验证集中，并具有不同的通话片段。在我看来，这应该会导致数据泄漏，因为该模型可能会发现个别首席执行官语言使用的特殊性，而不是底层数据生成过程的模式。您对此有何看法？   由   提交/u/Expective_Charity293  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</guid>
      <pubDate>Thu, 18 Jan 2024 08:03:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>