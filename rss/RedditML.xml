<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 18 Feb 2024 03:13:14 GMT</lastBuildDate>
    <item>
      <title>[D] 根据输入和输出大小选择 MLP 模型深度和隐藏大小，是否有任何好的通用启发式规则？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atjxej/d_are_there_any_good_general_rules_of_heuristics/</link>
      <description><![CDATA[例如，如果我的 BERT 输出为 768，并且它是一个二元分类，则可以通过启发式方法选择作为 MLP 模型的分类头，基于该特征输入大小和输出大小 2 的层数及其隐藏大小？   由   提交 /u/DolantheMFWizard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atjxej/d_are_there_any_good_general_rules_of_heuristics/</guid>
      <pubDate>Sun, 18 Feb 2024 02:59:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Graph ML/NL 的课程注册预测模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atimba/p_course_enrollment_prediction_model_using_graph/</link>
      <description><![CDATA[嘿，我正在尝试创建一个预测模型来预测我的毕业项目的课程注册情况。虽然我可以并且愿意使用随机森林或 SVM 回归器/分类器构建模型，但我最近发现了图网络的一个有趣领域，并且希望使用使用 Network X 生成的图来合并课程的先决条件结构。 但是，虽然我可以创建一个表示先决条件关系的有向图 (DAG)（参考这项研究：https: //doi.org/10.1007/s41109-023-00543-w#），我不确定如何使用这个结构和每个学生的入学数据来训练预测模型，更不用说它是否是图机器学习或图神经网络的用例。请帮忙！  *标题更正：“ML/NL” ---&gt; “ML/GNN”   由   提交 /u/StunningBreath2087   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atimba/p_course_enrollment_prediction_model_using_graph/</guid>
      <pubDate>Sun, 18 Feb 2024 01:52:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 应用机器学习论文中的公然数据泄露和谎言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atf9tz/d_blatant_data_leakage_and_lies_in_an_applied_ml/</link>
      <description><![CDATA[最近我看到一篇在医疗保健领域应用的机器学习论文。论文发表在该领域真正顶级期刊的较小子期刊上。从我看来，这篇论文在方法论上确实存在明显的根本缺陷，完全使任何提议的贡献无效。 本质上，他们从智能手表的患者那里收集心率、睡眠等数据，并安装梯度增强机来预测血液中化学物质的水平。他们在晚上的记录时间收集样本，并使用 10 分钟粒度的时间序列数据集来进行每日预测。 问题是，他们使用每日睡眠时间和睡眠子类别（睡眠）阶段）的特点。在这种情况下，睡眠数据会在一天中重复，然后与同一天整个数据持续时间的一个唯一标签相匹配。然后，他们进行 10 倍的 CV，并全面报告 0.90+ 的绩效评估指标。 我以前见过数据泄露的欺诈性研究，但我从未见过如此刻意、如此明目张胆的半成品研究。 - 之前受人尊敬的期刊。作者在论文中回顾了这些细节，并做出了笼统的陈述，比如他们如何确保在 10 倍 CV 期间的训练集和测试集中不存在相同的数据行，等等。哈哈。您有 7 个特征，每天都采用唯一值，然后每个特征重复约 100 次，与 1 个唯一结果相匹配，并且您是说您确保在简历中分隔行索引？ 我是我对他们的结果表示怀疑，因为我在类似的领域工作，但由于缺少一些细节而无法确定。值得庆幸的是，由于一些法规或规则，他们必须公开数据和代码，我确认这正是正在发生的事情。 我不会在这里分享这项研究，但我应该这样做吗？联系期刊让他们撤回论文？有趣的是，该杂志有“AI”一词。以他们的名义，所以这不像是一本有非机器学习审稿人的期刊。这怎么能通过同行评审呢？有人见过如此欺诈的研究吗？我对这些科学出版物的现状感到困惑和震惊。    由   提交/u/enthusiastic31  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atf9tz/d_blatant_data_leakage_and_lies_in_an_applied_ml/</guid>
      <pubDate>Sat, 17 Feb 2024 23:13:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 超图神经网络的影响？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atei40/d_impact_of_hypergraph_nns/</link>
      <description><![CDATA[我正在考虑申请超图神经网络的博士学位。我的一点研究经验是关于等变网络/GDL 的，我认为这可能很有趣。然而，我一直在寻找有关超图神经网络的论文，至少从我 5 分钟的研究来看，这个领域似乎并不是非常有前途或有影响力。  这里了解该主题的任何人都可以对此发表意见吗？   由   提交 /u/howtorewriteaname   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atei40/d_impact_of_hypergraph_nns/</guid>
      <pubDate>Sat, 17 Feb 2024 22:40:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 时间序列的自监督/无监督方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1atbpc6/d_selfsupervisedunsupervised_approaches_for_time/</link>
      <description><![CDATA[什么是时间序列的自监督/无监督方法？在应用任何监督学习进行预测应用之前，我想学习时间序列数据的低维表示（特征和时间之间的模式/交互）。这不是通常的方法，但我想尝试一下。 我知道自动编码器适用于表格数据。但是，对于输入 [batch_size、sequence_length、num_features] 的时间序列数据，我应该怎么做，除了自动编码器之外还有其他工具吗？或者自动编码器可以吗？   由   提交 /u/Then_Passenger_6688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1atbpc6/d_selfsupervisedunsupervised_approaches_for_time/</guid>
      <pubDate>Sat, 17 Feb 2024 20:38:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] vLLM 行为 - 它如何决定何时拒绝请求？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ata64d/d_vllm_behaviour_how_does_it_decide_when_to/</link>
      <description><![CDATA[如果 vLLM 服务器收到太多请求，它会开始拒绝这些请求吗？如果是这样，它如何决定何时开始拒绝请求以及是否有配置方法？   由   提交/u/Stunning-One-4670   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ata64d/d_vllm_behaviour_how_does_it_decide_when_to/</guid>
      <pubDate>Sat, 17 Feb 2024 19:33:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无提示的思维链推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at9w34/r_chainofthought_reasoning_without_prompting/</link>
      <description><![CDATA[论文 - https://arxiv.org/abs/2402.10200  摘要 - 在增强大语言模型（LLM）的推理能力方面，先前的研究主要集中在特定的提示技术上，例如少样本或零样本思维链（CoT） ) 提示。这些方法虽然有效，但通常涉及手动密集型提示工程。我们的研究采用了一种新颖的方法，提出了这样的问题：法学硕士能否在没有提示的情况下有效推理？有趣的是，我们的研究结果表明，只需改变解码过程，就可以从预先训练的 LLM 中导出 CoT 推理路径。我们不是采用传统的贪婪解码，而是研究前 k 个替代标记，发现 CoT 路径通常是这些序列中固有的。这种方法不仅绕过了提示的混杂因素，而且使我们能够评估法学硕士的内在推理能力。此外，我们观察到解码路径中 CoT 的存在与模型解码答案的较高置信度相关。该置信度度量有效区分 CoT 和非 CoT 路径。对各种推理基准的广泛实证研究表明，所提出的 CoT 解码大大优于标准贪婪解码。   由   提交/u/MysteryInc152   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at9w34/r_chainofthought_reasoning_without_prompting/</guid>
      <pubDate>Sat, 17 Feb 2024 19:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练计算如何影响质量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at9nth/d_how_training_compute_influences_quality/</link>
      <description><![CDATA[TL;DR 当使用较少的计算能力（例如 GPU）时，我们是否需要训练更长时间才能达到较高的水平模型质量与通过更好的计算获得的模型质量相当吗？ 嘿伙计们！ 在 Sora 的技术报告，他们表明视频质量通过训练计算显着提高，我试图理解与时间的附加维度的比例关系。 更具体地说，考虑在 G GPU 上训练模型，以便及时实现A一些准确度/质量指标 &gt;t。让我们将 G 减少 x 倍，因此 G&#39; = &lt; strong&gt;G/x，其中 x &gt;&gt; ; 1. 通过此设置，我们仍然希望实现A。根据经验，我们知道t会增加，即t&#39; = t * y，其中 y &gt;&gt; 1. 但是，我们是否知道 f 的估计值：X -&gt; 是？ 所以，这里有两个问题：  是否有可能实现A 与 G&#39; 计算？ 如果是这样，t&lt; /em&gt; 爆炸？    由   提交/u/Kingandpawnendgame  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at9nth/d_how_training_compute_influences_quality/</guid>
      <pubDate>Sat, 17 Feb 2024 19:12:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士如何玩电子游戏的现状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at89qb/d_state_of_how_llms_play_video_games/</link>
      <description><![CDATA[      哟！分享我的 YT 频道的最新视频，其中讨论了法学硕士玩《我的世界》等开放世界游戏的最新进展。视频进入了几篇研究论文（Voyager、DESP 等）及其提示框架，并将其与 SOTA RL 算法（如 Dreamer）进行了比较！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at89qb/d_state_of_how_llms_play_video_games/</guid>
      <pubDate>Sat, 17 Feb 2024 18:12:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] GRIT（生成表征指令调整）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at7lzn/r_grit_generative_representational_instruction/</link>
      <description><![CDATA[GritLM  &lt; li&gt;设定了新的最先进基准：在大规模文本嵌入基准 (MTEB) 上优于同等大小的所有其他模型，并且在生成任务方面表现出色。 规模很重要：更大的模型（例如 GritLM） 8x7B）优于开放生成语言模型，同时在嵌入任务方面仍然排名靠前。 在不牺牲通用性的情况下实现性能：GritLM 在生成数据或嵌入数据上的训练同样出色，结合了两全其美。  &gt;效率升级：通过避免单独的检索和生成模型，将长文档的检索增强生成 (RAG) 速度提高 60% 以上。  文章链接    由   提交 /u/AloneSYD   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at7lzn/r_grit_generative_representational_instruction/</guid>
      <pubDate>Sat, 17 Feb 2024 17:44:21 GMT</pubDate>
    </item>
    <item>
      <title>V-JEPA：Yann LeCun 先进机器智能愿景的下一步 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/</link>
      <description><![CDATA[      博客：https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model- video-joint-embedding-predictive-architecture/ 论文：https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/ ​ 摘要： ​ 本文探讨了特征预测作为视频无监督学习的独立目标，并引入了 V-JEPA，这是一组仅使用特征预测目标训练的视觉模型集合，不使用预训练的图像编码器、文本、负例、重建或其他监督源。这些模型使用从公共数据集中收集的 200 万个视频进行训练，并针对下游图像和视频任务进行评估。我们的结果表明，通过预测视频特征进行学习可以产生多功能的视觉表示，在基于运动和外观的任务上表现良好，而无需调整模型的参数；例如，使用冷冻的骨干。我们最大的模型，仅在视频上训练的 ViT-H/16，在 Kinetics-400 上获得 81.9%，在 Something-Something-v2 上获得 72.2%，在 ImageNet1K 上获得 77.9%。 ​&lt; /p&gt; ​ https://preview.redd.it/uvo0dpwvl6jc1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=3f308732b80a72be3d5ad8ef9542462cf4611b64 V-JEPA 训练视觉编码器通过预测学习的潜在空间中的屏蔽时空区域。   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/</guid>
      <pubDate>Sat, 17 Feb 2024 17:36:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] - 奖项公布：“预测区间竞赛一：出生体重”Kaggle 竞赛共有 7 本书获奖。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1at2oke/d_prizes_announcement_there_are_7_books_to_be_won/</link>
      <description><![CDATA[奖品公布：“预测区间竞赛一：出生体重”Kaggle 竞赛共有 7 本书获奖。感谢 Packt Publishing 的慷慨资助，七本精彩的书《Python 中应用保形预测实用指南》将被授予 本次比赛（截止日期为 3 月 22 日）：  第一名和第二名私人 LB 获奖者：向每名个人提供平装本 第三名和第四名私人 LB 获奖者：向每名个人提供电子副本（获奖者于 3 月 23 日公布）  还有：  最佳笔记本：平装本 最佳笔记本第二名：电子版 最佳写作：电子版（获奖者将在一周内公布）大约稍后，以便在比赛结束后有时间撰写比赛或发布作品）  https://www.kaggle.com/competitions/prediction-interval-competition-i-birth-weight/discussion ​   由   提交 /u/predict_addict   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1at2oke/d_prizes_announcement_there_are_7_books_to_be_won/</guid>
      <pubDate>Sat, 17 Feb 2024 14:03:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要建议：使用 NER 或其他模型自动处理德国发票</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aszujx/d_advice_needed_automated_processing_of_german/</link>
      <description><![CDATA[您好， 我在一家德国保险公司工作，希望从以 PDF 形式收到的客户发票中自动提取数据。我们对发票号码、日期、名称、地址和带有价格的行项目等详细信息特别感兴趣，旨在将此信息输出为 JSON 以便进一步处理。这些实体可能出现多次或根本不出现。 我们尝试了多种方法但没有成功：  GPT-4 和各种模型 ：没有始终如一地提供结构化 JSON 输出。 发票的 Impira/LayoutLM：难以准确地区分开单人和收件人。  给出我们需要在本地处理这些数据（出于隐私和安全原因），并且考虑到这些发票是德语的，我们正在探索所有选项，包括命名实体识别（NER），尽管它不是法学硕士进步的最新进展。&lt; /p&gt;  有人对适合处理德国发票的预训练模型或方法有建议吗？ NER 可能是一个可行的选择，或者我们应该考虑其他技术或模型吗？  感谢这个社区可以提供的任何建议或见解！ &lt;!-- SC_ON - -&gt;  由   提交 /u/4AVcnE   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aszujx/d_advice_needed_automated_processing_of_german/</guid>
      <pubDate>Sat, 17 Feb 2024 11:26:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴模型演练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</link>
      <description><![CDATA[我真的很喜欢曼巴论文，但它对我来说这不是一本特别容易读的书，因为我之前几乎没有接触过很多先决条件材料（状态空间建模、并行扫描等）。 我写了一个解释器（链接 此处），我很好奇人们是否有任何反馈或认为它有帮助/有趣。 这在一定程度上是为了巩固我自己的理解，但也是我希望对社区有好处的事情，因为关于 Mamba 架构的教程并不多。 &lt; !-- SC_ON --&gt;  由   提交 /u/_james_chen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aseqq8/d_mamba_model_walkthrough/</guid>
      <pubDate>Fri, 16 Feb 2024 17:46:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>