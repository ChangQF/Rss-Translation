<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 28 May 2024 18:18:26 GMT</lastBuildDate>
    <item>
      <title>[D] 美国硕士课程（远程+训练营）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2q61e/d_us_master_program_remote_boot_camp/</link>
      <description><![CDATA[大家好，非 CS 背景，所以我尝试找到一个远程的 ML 主程序，并提供教授 Python 和基本 CS 的训练营知识。有什么建议吗？谢谢。   由   提交 /u/Tiny-Chocolate-341   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2q61e/d_us_master_program_remote_boot_camp/</guid>
      <pubDate>Tue, 28 May 2024 18:12:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于只缓存一次的问题：语言模型的解码器-解码器架构 - https://arxiv.org/pdf/2405.05254v1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</link>
      <description><![CDATA[      这是我第一次尝试阅读论文。然而，我很难理解这一点，并且认为你们会知道我的问题的答案，因为这种新架构对于法学硕士来说似乎很重要，如图 1 所示。 图1 据我了解，主要思想将网络分成两部分。前 L/2 层是自解码器层，生成全局 KV 缓存。第二个 L/2 层是交叉解码器层，重用生成的全局 KV-Cache。 引用他们的论文，了解如何节省大量计算和内存（我理解这部分）：  具体来说，由于全局KV缓存被复用，高效的self-attention需要常量的缓存，所以缓存的数量为O(N + CL)，其中N是输入长度，C是一个常量（例如，滑动窗口大小），L 是层数。对于长序列，CL 比 N 小得多，因此需要 O(N) 左右的缓存，即只缓存一次。相比之下，Transformer 解码器在推理过程中必须存储 N × L 个键和值。因此，与 Transformer 解码器相比，YOCO 大约为缓存节省了 L 倍的 GPU 内存。  这是我没有得到的。在仅解码器的网络中，查询、键和值的概念的功能与它们在数据库中的使用有些相似，但重点是捕获单词之间的关系。在此类网络的每一层中，这些组件有助于完善对文本的理解，随着处理从一层移动到下一层，根据新的见解调整焦点。 每一层都建立在前一层的基础上通过更新查询、键和值，进而完善网络的解释和响应生成。 如果现在仅压缩解码器网络的各个 KV 缓存的所有信息放入全局 KV-Cache 中，我们不会丢失有价值的信息吗？我们不应该看到更差的性能吗？  此外，我们只有一半的层来完善这种解释，因为交叉-解码器层都重用相同的KV缓存。 图2   由   提交/u/StraightChemistry629  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</guid>
      <pubDate>Tue, 28 May 2024 17:58:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 具有焦点损失的 XGBoost</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pnan/d_xgboost_with_focal_loss/</link>
      <description><![CDATA[大家好， 任何人都可以帮助我实现 XGBoost 的焦点损失或向我指出现有代码吗？我在网上找到的只是 this 它没有实现 alpha 和 gamma 的平衡焦点损失（仅实现 gamma） 。我还发现了这个，但似乎有些不对劲，因为与第一个相比，它给出了非常糟糕的结果。  非常欢迎任何帮助。 谢谢！   由   提交/u/Beginning_Daikon_356   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pnan/d_xgboost_with_focal_loss/</guid>
      <pubDate>Tue, 28 May 2024 17:51:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] GT 深度估计：LiDAR 与立体深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</link>
      <description><![CDATA[为什么大多数深度估计基准（如 nuScenes、KITTI、DDAD 等）都具有来自 LiDAR 传感器的地面真实深度，而不是来自2 个摄像头的立体深度？ 将摄像头安装在汽车后视镜上会导致基线距离约为 2m。这将实现更密集的深度测量，距离与 SOTA 激光雷达相似。我不明白为什么它没有被更频繁地使用 - 或者我错过了什么？   由   提交/u/topsnek69  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</guid>
      <pubDate>Tue, 28 May 2024 17:50:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 隔离并引用 PDF 表格和图表中的特定数字</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pmk0/p_isolate_and_cite_specific_numbers_in_pdf_tables/</link>
      <description><![CDATA[我目前正在处理一个 RAG 财务文档用例，我需要在 PDF 中的表格和图表中引用特定数字。我有图表/表格边界框和页码，但我需要突出显示确切的数字。字符串搜索不太适用，因为同一个数字字符串在同一个表格或图表中出现多次。 我需要一种可靠的方法来在 PDF 查看器中精确定位和引用这些特定的数字实例。这里有人成功解决了这个问题吗？    提交人    /u/ResolutionKey2002   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pmk0/p_isolate_and_cite_specific_numbers_in_pdf_tables/</guid>
      <pubDate>Tue, 28 May 2024 17:50:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 桌面拒绝</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2phaw/d_neurips_2024_desk_rejection/</link>
      <description><![CDATA[我忘记了清单，所以我的提交被桌面拒绝了。老实说，我不知道清单，因为我使用了去年提交的乳胶模板，只是将样式文件从 neurips_2023.sty 更改为 neurips_2024.sty。有没有办法让我在为时已晚之前再次重新提交清单？   由   提交 /u/Professional-Egg-222   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2phaw/d_neurips_2024_desk_rejection/</guid>
      <pubDate>Tue, 28 May 2024 17:44:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们如何有效地将强化学习用于现实世界的应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2p9or/d_how_can_we_leverage_reinforcement_learning/</link>
      <description><![CDATA[强化学习是人工智能的强大工具，在现实世界的应用中非常有效。 如果您想利用有效地使用 RL，您必须考虑： 选择正确的应用程序、解决 RL 挑战、实际应用领域 此相关播客分享了有关有效利用 RL 的所有内容。 &lt; p&gt;https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Reinforcement-Learning-in-the-Real-World-with-Anna-Goldie-e2hjbj4&lt; /p&gt;   由   提交/u/Data_Nerd1979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2p9or/d_how_can_we_leverage_reinforcement_learning/</guid>
      <pubDate>Tue, 28 May 2024 17:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 防止夏令时时间序列预测中的数据泄露</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2owkv/d_preventing_data_leakage_in_time_series/</link>
      <description><![CDATA[你好/r/machinelearning，&lt; /p&gt; 我正在研究每天中午 12 点发布的预测值，其中包括第二天所有 24 小时的值。通常，我的方法涉及使用扩展窗口技术，其中我对截至今天（昨天发布）的所有可用数据进行训练，然后预测第二天的 24 小时值。 但是，夏令时期间会出现复杂情况。调整。数据每年两次因夏令时（欧洲）而发生变化，导致某些日子有 23 小时或 25 小时。大多数时间序列库通过预测固定窗口大小来处理回测，但这种固定大小无法适应夏令时期间的小时变化，从而导致潜在的数据泄漏。例如，在春季，模型会漂移一小时，纳入技术上在预测时间后一整天发布的数据。 我看到了一些潜在的解决方案（从最不喜欢到最喜欢的 imo）：&lt; /p&gt;  通过在过渡期间添加或删除一个小时来操作数据。这可能涉及插入捏造的值或复制前一小时。 开发自定义回测函数，该函数可以适应不同的时间频率（日、周、月）而不是固定的整数大小windows。 使用已经解决此问题的库。我似乎找不到已经实现此功能的流行库，所以如果您知道的话请告诉我！我特别难以找到能够满足此要求的 AutoML 库。  您对这些解决方案有何看法？是否有更简单的方法，或者我想太多了？欢迎提出所有建议！   由   提交 /u/NeuralGuesswork   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2owkv/d_preventing_data_leakage_in_time_series/</guid>
      <pubDate>Tue, 28 May 2024 17:21:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 视觉语言建模简介</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2ojhk/r_an_introduction_to_visionlanguage_modeling/</link>
      <description><![CDATA[视觉语言建模简介&lt; /p&gt; 摘要： 随着大型语言模型 (LLM) 最近的流行，人们进行了多次尝试将其扩展到视觉领域。从可以引导我们穿过陌生环境的视觉助手到仅使用高级文本描述生成图像的生成模型，视觉语言模型（VLM）应用程序将极大地影响我们与技术的关系。然而，为了提高这些模型的可靠性，需要解决许多挑战。虽然语言是离散的，但视觉是在更高维的空间中演化的，在这个空间中，概念并不总是容易离散化。为了更好地理解将视觉映射到语言背后的机制，我们介绍了 VLM，希望能够帮助任何想要进入该领域的人。首先，我们介绍什么是 VLM、它们如何工作以及如何训练它们。然后，我们提出并讨论评估 VLM 的方法。虽然这项工作主要侧重于将图像映射到语言，但我们也讨论了将 VLM 扩展到视频。   由   提交/u/nanowell  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2ojhk/r_an_introduction_to_visionlanguage_modeling/</guid>
      <pubDate>Tue, 28 May 2024 17:05:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 帮助选择学校 RAG 项目的 LLM 模型和 Embeddings</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2kojo/p_help_to_choose_llm_model_and_embeddings_for/</link>
      <description><![CDATA[您好， 在学校内，我们正在制定一个项目，根据法语文档创建 RAG。 我们必须选择一个开源 LLM，其参数很少，例如 7B 和嵌入，并通过文献研究证明其合理性。 我承认我不知道从哪里开始。它到处都有基准，来自各地的模型，我需要您对要查看哪些测量值的意见，以及如果您对嵌入和模型有想法，我可以查看。 谢谢。    由   提交 /u/Thamelia   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2kojo/p_help_to_choose_llm_model_and_embeddings_for/</guid>
      <pubDate>Tue, 28 May 2024 14:22:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入矩阵和最终的 pre-softmax 矩阵是否应该在 Transformer 中共享？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</link>
      <description><![CDATA[大家好， 在比较各种 LLM 时，可以看到其中一些使用相同的矩阵来进行标记嵌入，并且最后采用softmax之前的变换矩阵来得到预测的token概率。我发现这篇 2016 年的论文 Using the Output Embedding to Improve Language Models 表明这是优越的，而且 Attention Is All You Need 论文引用了它并进行了权重共享。 GPT2 和 Gemma 等其他模型也是如此。 这让我想知道为什么 LLaMa 模型不进行这种权重共享。就模型容量而言，在那里拥有单独的矩阵是否值得？像 Gemma 这样的模型是否必须使用权重共享，因为它们使用大量词汇？我对这里的权衡感兴趣，以及当前对此主题的共识是什么（如果有的话）。   由   提交/u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</guid>
      <pubDate>Tue, 28 May 2024 12:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] H5 到 TFLite 转换中 TransposeConv 的奇怪维度。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</link>
      <description><![CDATA[      我尝试在 https://medium.com/analytics-vidhya/noise-suppression-using-deep-learning-6ead8c8a1839，这是full Conv1D SEGAN 模型。 然后我完成训练并得到 H5 模型。然后我尝试转换为具有 Full Integer INT8 量化的 TFLite 模型。 （原始示例没有这样做全整数量化，仅设置为“默认”。）量化代码如下。 defrepresentative_data_gen(): for input_value, _ in test_dataset.take(100): yield [input_value] model = load_model(&#39; NS_SEGAN_localTrained.h5&#39;) model.summary() score = model.evaluate(test_dataset)  tflite_model = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model.optimizations = [tf.lite.Optimize.DEFAULT]&lt; /code&gt; tflite_model.representative_dataset =representative_data_gen tflite_model.target_spec.supported_ops = [  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS, # 启用 TensorFlow 操作。 &lt; code&gt;tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # 同时使用选择操作和内置函数 tflite_model.inference_input_type = tf.int8 tflite_model.inference_output_type = tf.int8 tflite_model_quant_INT8 = tflite_model.convert() with open(&#39; NS_SEGAN_localTrained_quant_2.tflite&#39;, &#39;wb&#39;) as f: f.write(tflite_model_quant_INT8) 然后看起来很奇怪，只有第一个“TransposeConv”运算符获取正常维度，其他运算符的输出维度为[1,1,1,1]。 第一个“TransposeConv”具有正常尺寸。 其他 6 个 &#39;TransposeConv&#39; 在 TFLite 转换后得到 [1,1,1,1]。 型号链接 H5 型号  TFLite（完整 INT8 量化） 我有点怀疑这是正确的，而另一方面，它是由 TFLite API 转换的，这让我认为它应该是正确的。有人专家告诉我它不应该是[1,1,1,1]，但没有解释或建议。 我不知道如何确认这是否正确。如果[1,1,1,1]在这种情况下是合理的？此外，如果是错误的，为什么会发生这种情况以及如何解决它？如果有人有想法或建议，请建议或指导非常感谢。   由   提交/u/Ok_Box_6059   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</guid>
      <pubDate>Tue, 28 May 2024 11:54:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在 pytorch 模型上运行并发推理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</link>
      <description><![CDATA[大家好， 我有几个用于验证图像的 pytorch 模型，我想部署它们到一个端点。我使用快速 api 作为 API 包装器，到目前为止我将完成我的开发过程： 之前我运行了一个简单的 OOTB 推理，如下所示： model = Model() @app.post(&#39;/model/validate/&#39;): pred = model.forward(img) return {&#39;pred&#39;:pred}  问题这种方法的缺点是它无法处理并发流量，因此请求会排队并且推理一次会发生 1 个请求，这是我想避免的事情。 我当前的实现如下：制作模型对象的副本，并派生一个新线程来处理特定图像。有点像这样： model = Model() def validate(model, img): pred = model.forward(img) return pred @app.post(&#39;/model/validate/&#39;) : model_obj = copy.deepcopy(model)loop = asyncio.get_event_loop() pred =awaitloop.run_in_executor(validate, model_obj, img) return {&#39;pred&#39; : pred}  这种方法制作模型对象的副本并在对象副本上进行推理，这样我就可以服务并发请求。 我的问题是，是否有另一种更优化的方法可以实现 pytorch 模型并发，或者这是一种有效的做事方式吗？ TLDR：使用模型对象的副本创建新线程以实现并发，还有其他方法来实现并发吗？   由   提交/u/comical_cow  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 07:33:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 泊松变分自动编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</link>
      <description><![CDATA[预印本：https://arxiv.org/abs/2405.14473  X 线程摘要：https://x.com/hadivafaii/status/1794467115510227442   由   提交/u/vafaii  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</guid>
      <pubDate>Tue, 28 May 2024 04:56:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>