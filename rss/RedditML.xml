<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 03 Jun 2024 06:20:38 GMT</lastBuildDate>
    <item>
      <title>[D] 您是否认为 Meta ImageBind 对于多向量嵌入而言比 CLIP 更好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6xocn/d_would_you_say_meta_imagebind_is_better_than/</link>
      <description><![CDATA[我正在研究多模式嵌入模型，然后遇到了 Imagebind。它似乎很有趣，但我找不到很多关于它与 CLIP 相比如何的评论或基准。我读到 Meta 刚刚改进了 CLIP 或对其进行了扩展。 有没有人在这个领域工作过并且同时使用过这两种工具？​​    提交人    /u/CaptTechno   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6xocn/d_would_you_say_meta_imagebind_is_better_than/</guid>
      <pubDate>Mon, 03 Jun 2024 06:11:24 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉算法能否识别动作[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6wvbp/can_computer_vision_algorithms_recognize_actions_d/</link>
      <description><![CDATA[我的问题基本上是，假设我想训练一个动作模型，例如舞蹈类型。假设我可以访问按舞蹈类型分类的数小时和舞蹈视频。是否可以训练一种算法来做到这一点，它可以绘制精美的框并标记舞蹈的概率。请链接任何类型的论文或其他资源以供探索    提交人    /u/No-Refuse-2318   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6wvbp/can_computer_vision_algorithms_recognize_actions_d/</guid>
      <pubDate>Mon, 03 Jun 2024 05:17:23 GMT</pubDate>
    </item>
    <item>
      <title>[D]：NLP 之外的 Transformer 键、查询、值直觉</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6tpyj/d_transformer_keys_queries_values_intuitions/</link>
      <description><![CDATA[通常 K = V，如果 Q =/= K，则为交叉注意力，否则为自注意力。Transformer 块基本上在每个块之后用上下文丰富 V（值）向量。 在我的例子中，我有 Q =/= K =/= V，虽然从数学上来说没问题，但我还没有遇到过这样做的应用程序。  我想要这样一种行为：当 K = Q 时，V_new = Transformer (Q, K, V) = V，因为 Q 与 K 匹配。使用通常的缩放点注意力机制不会出现这种情况，但我猜这可以通过其他组件（如 MLP）来克服。 那么我的问题是：1. 是否存在修改后的注意力机制（除了计算成本高昂的距离矩阵）可以在 K=Q 时返回 V 2. 当 K=V 时，由于 V_new = Transformer(Q, V, V) 在每个时间步长上都是如此，因此如何重复应用 Transformer 块就变得很简单了。但是如果 K=/=V，在 V_new = Transformer(Q,K,V) 之后，我不清楚是否应该执行 V_new_new = Transformer(Q,K, V_new) 感谢讨论。我想知道您是否有处理类似情况的经验以及处理情况如何。    提交人    /u/MysticalDragoneer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6tpyj/d_transformer_keys_queries_values_intuitions/</guid>
      <pubDate>Mon, 03 Jun 2024 02:14:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] https://ai.papers.bar/papers/weekly 在哪里</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6s8rz/d_where_is_httpsaipapersbarpapersweekly/</link>
      <description><![CDATA[      该网站过去提供每周热门论文！ 网站截图 好吧，他们停止了这个项目： https://labml.ai/#discontinued    提交人    /u/Realistic_Thanks3282   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6s8rz/d_where_is_httpsaipapersbarpapersweekly/</guid>
      <pubDate>Mon, 03 Jun 2024 00:56:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在创建 FAISS 索引时，有没有什么方法可以更快地执行编码？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6s7au/d_is_there_any_way_to_perform_encoding_a_bit/</link>
      <description><![CDATA[我目前正在训练一个文本嵌入模型，我正在使用 MTEB 或 MIRACL 等基准对其进行评估。我引用的大多数代码都使用 FAISS 索引来搜索结果，这是有道理的。 问题是，在构建 FAISS 索引时，文本编码花费的时间太长了。我目前使用一台带有四个 A6000 GPU 设备的机器，并实现了数据并行性来执行分布式推理，但即便如此，也需要大约 8 个小时才能获得大约 140 万个文档的嵌入向量。 这意味着我在每个时期后进行评估的典型工作流程变得有点不可行。 我考虑过想出其他方法，比如使用较小的语料库进行中间评估，但我真的不想这样做。 还有其他方法可以更快地完成此操作吗？谢谢。    由   提交  /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6s7au/d_is_there_any_way_to_perform_encoding_a_bit/</guid>
      <pubDate>Mon, 03 Jun 2024 00:54:30 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 为什么下一个标记预测对推荐系统不起作用？（或者我错了？）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/</link>
      <description><![CDATA[我正在开展一个研究项目，旨在应用下一个标记预测模型来构建/改进推荐系统。作为一项可行性评估研究，我使用 Instacart 数据集构建并训练了一个 GPT 模型来预测下一个要购买的产品。更具体地说，我将每个 product_id 视为一个“单词”，将每个订单视为一个“句子”，将每个用户的交易历史记录视为一个“文档”。 然而，在 T4 GPU 上训练 4 小时后，评估集上 10 的平均精度（MAP@10）仍然只有 0.075。作为比较，基线热门产品（用户个人交易历史中最受欢迎的 10 种产品）的 MAP@10 已经为 0.251。 虽然我可以看到改进模型的一两种方法，但与非常简单的基线相比，低性能确实令人沮丧，让我认为这种方法根本不可行。我想讨论几点：  下一个标记预测（特别是仅解码器的 Transformer 架构）是否真的不适用于任何规模的问题？ 支持：文本数据和电子商务交易数据之间存在很大差异，因此适用于文本的方法可能不适用于交易，这并不奇怪。 反对：4 小时的训练只有 1.5 个 epoch，因此当前模型可能拟合不足。因此，低性能可能只是训练时间的函数，如果我训练 2 天，它可能会有所改善 我应该阅读哪些资源？我知道我采用的方法类似于基于会话的 recsys 模型，但我只找到一篇论文 HierTCN（You et al.，WWW 2019）。如果能提供任何其他建议，我将不胜感激。 有什么建议可以帮助更好/更快地训练模型？目前的配置是： 数据：vocab_size = 50000（50K 个产品）、200K 个用户（每个 epoch 为 200K 个训练样例） 模型：n_layer=9、d_model=512、n_head=16（类似于 Gopher 44M 参数模型）、block_size=1024 训练：batch_size = 4、learning_rate = 5e-4、optimizer = AdamW  谢谢！    submitted by    /u/Pancake502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/</guid>
      <pubDate>Sun, 02 Jun 2024 23:24:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 建立有效的 LLM 基准所面临的挑战：5 分钟深入探讨 🧠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6p2ej/r_the_challenges_of_building_effective_llm/</link>
      <description><![CDATA[随着该领域的快速发展和模型的不断发布，需要全面的基准测试。通过值得信赖的评估，您和我可以知道为我们的任务选择哪个 LLM：编码、指令遵循、翻译、解决问题等。 TL;DR：本文深入探讨了评估大型语言模型 (LLM) 的挑战。🔍 从数据泄漏到记忆问题，发现差距和建议的改进，以获得更全面的排行榜。 深入研究最先进的方法以及我们如何更好地评估 LLM 性能 处理 img tu0kj4be554d1...    提交人    /u/ml_a_day   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6p2ej/r_the_challenges_of_building_effective_llm/</guid>
      <pubDate>Sun, 02 Jun 2024 22:20:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于科学文献搜索的预训练嵌入模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6lbp2/d_pretrained_embedding_model_for_search_in/</link>
      <description><![CDATA[你好！ 我正在开发一款应用，需要一种方法来在一组科学论文中搜索关键词（即查找涉及查询的文档）。我需要处理同义词、错误等。 我认为最好的想法是向量搜索。但我真的不知道当前的 SOTA 是什么。我知道 SBERT，但我不确定它是否是最好的？ 此外，如果模型可以在多语言和科学文献中预先训练，那就最好了…… 你有什么想法给我吗？ 也许我走的路不对？ 提前谢谢您！    提交人    /u/ez613   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6lbp2/d_pretrained_embedding_model_for_search_in/</guid>
      <pubDate>Sun, 02 Jun 2024 19:33:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] Moonlighter 商店模拟中的贝叶斯老虎机商品定价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</link>
      <description><![CDATA[      我建了一个玩具店，模仿Moonlighter 游戏和贝叶斯匪徒代理通过 Thompson 抽样选择和定价待售物品。  随着模拟的进行，客户对这些物品在其货架价格的反应（即“生气”、“悲伤”、“满足”、“欣喜若狂”）更新了理想（即最高）价格概率分布（即后验）。  该算法探索了物品的理想价格，并迅速找到了当时理想价格最高的物品组，然后将其出售。 这个过程一直持续到所有物品售出。  该图表示待售物品之间的竞争。  这些点是从每个竞赛（x 值）中每个物品（颜色）的理想价格分布（即后验）中抽样的价格（y 值）。  在每次 Thompson 抽样竞赛中，抽样价格最高的获胜者最终被摆上货架。  我提到的客户对货架上商品的反应更新了这些分布的界限，用相同颜色的线条表示。 有关更多信息、更多图表以及包含工作代码和带有 Pandas/Matplotlib 代码的 Jupyter 笔记本（用于生成图表）的相应 Github 存储库的链接，请参阅我的文章：https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rMachineLearning    提交人    /u/JaggedParadigm   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</guid>
      <pubDate>Sun, 02 Jun 2024 17:25:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 记录每一项机器学习资源或接受知识随时间流失的困境</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</link>
      <description><![CDATA[我知道这可能是一个奇怪的话题，但我仍然认为这是一个重要的讨论，因为我们在这个领域不断学习。 机器学习是一个广阔的领域，与许多其他学科紧密交织在一起。仅我的硕士学位就涵盖了统计、优化、逆数据模拟、MLOps、软件工程、基于代理的建模、语义网、深度学习、时间序列等主题……这些领域中的每一个都有自己的子领域，人们可以投入一生去探索。 我意识到，除非你每天练习一个主题，否则你从书籍、认证、文章、论文、播客和视频中获得的知识最终会消失。四年前，这种认识促使我发现了 Obsidian，它极大地改变了我获取和保留信息的方式。我现在会记录我所获取的所有内容，尤其是工作之外我感兴趣的主题。就像一个“第二大脑”。如果没有这种做法，我发现信息很快就会消失。 事实上，我花了无数的时间研究物理、历史、认识论、哲学和许多其他学科的内容。然而，我曾经知道的东西只有一小部分留了下来。这让我陷入了两难境地：我应该投入大量时间来捕捉知识系统中的每一项资源，以确保我可以随着时间的推移而保留下来，还是尽快消耗资源，因为它们会消失（）“为了好玩”或当我的时间有限时）？ 我不想让这篇文章太长，但我确实感觉到花时间处理信息的好处，比如在读书的时候。大规模地组织和连接知识通常很有挑战性，但也很有回报，因为它有助于建立对某个主题的深刻理解。此外，当您需要刷新记忆时，如果您已经完成了这项“预处理”工作，而不是再次浏览互联网/书籍，那么“成本”会低得多。我不是简单地复制/粘贴文本，而是根据我已经了解的主题来定制我所捕获的内容。 但是，这个领域有太多东西需要学习，即使是数学或统计学等基础知识。我有时会质疑这种方法是否可持续。例如，Sebastian Raschka 等人撰写的《使用 PyTorch 和 Scikit-Learn 进行机器学习》一书长达 700 页。想象一下从这样一本全面的书中捕捉每一条信息需要花费的时间（而且这只是其中之一！）。记笔记还会迫使你彻底理解材料，包括每个方程式，否则笔记就毫无用处了。 我不主张二元方法；我经常找到妥协。但我很好奇你学习和消费信息的方法。你如何平衡保留知识的需求与时间和精力的实际限制？    提交人    /u/CrimsonPilgrim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</guid>
      <pubDate>Sun, 02 Jun 2024 16:36:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] MetaEarth - 用于全球尺度遥感图像生成的生成基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6ggwv/r_metaearth_a_generative_foundation_model_for/</link>
      <description><![CDATA[        由    /u/jiupinjia 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6ggwv/r_metaearth_a_generative_foundation_model_for/</guid>
      <pubDate>Sun, 02 Jun 2024 15:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>如果 LLM 是基于 token 的自回归模型，那么它们如何生成图像？（Transformers + VQVAE）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</link>
      <description><![CDATA[      分享我 YT 频道的一段视频，讨论某些多模态 LLM（如 Gemini）如何将图像生成为一系列可学习的图像标记。    提交人    /u/AvvYaa   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</guid>
      <pubDate>Sun, 02 Jun 2024 14:33:16 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Tangles：Diestel 在书中宣布了一种新的数学 ML 工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</link>
      <description><![CDATA[      大家好，我想分享一本社区可能会感兴趣的新书！ 图论学家 Diestel 写了一本面向 ML 社区（及其他人）的书：  缠结：经验科学中人工智能的结构化方法 Reinhard Diestel，剑桥大学出版社 2024  ----- 出版商简介： 缠结提供了一种在不精确数据中识别结构的精确方法。通过将经常一起出现的特质分组，它们不仅可以揭示事物的集群，还可以揭示其特质的类型：政治观点、文本、健康状况或蛋白质的类型。缠结为人工智能提供了一种新的结构化方法，可以帮助我们理解、分类和预测复杂现象。 这已成为可能，这是由于缠结的数学理论最近被公理化，这使得缠结的应用范围远远超出了图论的起源：从数据科学和机器学习中的聚类到预测经济学中的客户行为；从 DNA 测序和药物开发到文本和图像分析。 这是首次探索此类应用。假设只具备本科数学基础知识，那么缠结理论及其潜在含义将对科学家、计算机科学家和社会科学家开放。 ----- 电子书以及包括教程在内的开源软件可在 tangles-book.com 上找到。 注意：这是一本“外展”书，主要不是关于缠结理论，而是关于以多种意想不到的方式和领域应用缠结。图中的缠结在 Diestel 的《图论》第 5 版中介绍。 目录和数据科学家简介（Ch.1.2）可从 tangles-book.com/book/details/ 和 arXiv:2006.01830 获得。第 6 章和第 14 章介绍了一种基于缠结的新软聚类方法，与传统方法截然不同。第 7-9 章涵盖了第 14 章所需的理论。 tangles-book.com 的软件部分表示，他们邀请在具体项目上进行合作，以及为他们的 GitHub 软件库做出贡献。  https://preview.redd.it/ysj91dw2o54d1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2 ​ ​    提交人    /u/Prestigious_Ship_238   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</guid>
      <pubDate>Sun, 02 Jun 2024 12:56:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] FineWeb 技术报告：大规模挖掘网络上最精细的文本数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</link>
      <description><![CDATA[FineWeb 15 万亿公开发布的网络规模数据集背后的团队刚刚发表了一篇关于创建高质量网络规模数据集的科学的详尽博客文章，详细介绍了 FineWeb 的步骤和学习成果，以一种 distill.pub 交互式文章/博客的方式。 他们还发布了 FineWeb-Edu，这是 Common Crawl 的一个过滤子集，拥有 1.3T 令牌，专注于教育内容非常丰富的网页，在知识和推理密集型基准测试（如 MMLU、ARC 和 OpenBookQA）上，其表现似乎优于所有公开发布的网络规模数据集 有趣的阅读：https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1     由    /u/Thomjazz 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</guid>
      <pubDate>Sun, 02 Jun 2024 08:17:39 GMT</pubDate>
    </item>
    </channel>
</rss>