<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 11 Feb 2025 18:21:34 GMT</lastBuildDate>
    <item>
      <title>可解释的时间序列预测人工智能 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in57y1/explainable_ai_for_time_series_forecasting/</link>
      <description><![CDATA[是否有任何研究论文的功能实现专注于可解释的 AI 用于时间序列预测？我一直在广泛搜索，但没有一个库表现最佳。此外，请推荐其他方法来解释时间序列模型的结果并向业务利益相关者解释它们。    提交人    /u/Severe_Conclusion796   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in57y1/explainable_ai_for_time_series_forecasting/</guid>
      <pubDate>Tue, 11 Feb 2025 18:15:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] MaskNet 的持续相关性：利用乘性特征交互进行 CTR 预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in4qsv/r_the_continued_relevance_of_masknet_leveraging/</link>
      <description><![CDATA[2021 年，在 ChatGPT 引发 AI 热潮之前，新浪微博公司的研究人员在新加坡 ACM 的 DLP-KDD 上介绍了 MaskNet，题为“MaskNet：通过实例引导掩码将特征乘法引入 CTR 排名模型”。这种使用深度神经网络中的实例引导掩码进行点击率 (CTR) 预测的特征乘法方法在当今的工业应用中仍然具有很高的竞争力。 MaskNet 超越了传统的加性特征交互，证明了即使人工智能格局迅速发展，重点领域的突破性创新也能经受住时间的考验。 关键技术亮点：  实例引导掩码：在特征嵌入和前馈层上动态执行元素乘法，提高模型强调信息特征的能力。 MaskBlock：结合层规范化、前馈层和乘法掩码的混合模块，允许加法和乘法交互共存。 性能提升：MaskNet 在真实数据集上的表现优于 DeepFM 和 xDeepFM，AUC 提高了高达 5.23%。 灵活的架构：提供串行（SerMaskNet）和并行（ParaMaskNet）配置适用于各种用例。  MaskNet 表明，将乘法运算合并到深度神经网络中可以显着捕获复杂的特征交互，从而为 CTR 预测提供更有效的方法。如果您在 CTR 或推荐系统中工作，本文将提供宝贵的见解。 阅读全文：https://www.shaped.ai/blog/masknet-ctr-ranking-innovation 期待听到您对这种方法的想法！    提交人    /u/skeltzyboiii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in4qsv/r_the_continued_relevance_of_masknet_leveraging/</guid>
      <pubDate>Tue, 11 Feb 2025 17:56:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找可以贡献的开源项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in430q/d_looking_for_open_source_projects_to_contribute/</link>
      <description><![CDATA[大家好，我是一名 AI 工程师，在计算机视觉领域拥有 1-1.5 年的经验。我感觉自己正在进入一个舒适区，想要通过为有益于 CV/DL 社区的事情做贡献来挑战和提高自己。 最近，我通过将一些 PR 合并到 albumentations 库中开始了我的开源之旅，但现在我想扩展并进行更多实际的 DL 工作。 因此，如果您已经开始/目前正在从事基于计算机视觉的开源项目，请在此主题中告诉我们。    提交人    /u/Hour_Amphibian9738   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in430q/d_looking_for_open_source_projects_to_contribute/</guid>
      <pubDate>Tue, 11 Feb 2025 17:30:04 GMT</pubDate>
    </item>
    <item>
      <title>闭源模型的碳排放推理 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in2cpg/carbon_emissions_for_closed_source_models_at/</link>
      <description><![CDATA[大家好！我无法从 OpenAI/Anthropic 找到有关 GPT-4o 或 Claude 3.5 Sonnet 等模型的每个推理请求的碳排放量的任何数据。所以我想知道：  是否有任何已知的方法可以估算每个 API 调用的排放量（例如，令牌计数、计算时间、云碳工具）？ 是否有第三方研究或粗略的近似值？ 为什么缺乏透明度？  欢迎猜测、框架或研究链接 :)。谢谢    提交人    /u/PlatypusPrudent3076   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in2cpg/carbon_emissions_for_closed_source_models_at/</guid>
      <pubDate>Tue, 11 Feb 2025 16:18:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 14B 型号、168GB GPU，每秒仅有 4 个令牌？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1in0gwo/d_14b_model_168gb_gpu_and_only_4_tokenssec/</link>
      <description><![CDATA[我在 **7 台机器（每台配备 24GB VRAM，总共 168GB）上运行 DeepSeek-R1-Distill-Qwen-14B 时遇到了性能问题 模型：DeepSeek-R1-Distill-Qwen-14B（14B 参数）  硬件：AWS g6.4xlarge - 7X GPU：7 台机器，每台配备 24GB GPU（总共 168GB VRAM）💪 推理引擎：vLLM 多节点/多 GPU 框架：Ray 精度：测试 FP32 和 FP16  我使用 Ray 进行多节点多 GPU 编排，并使用 vLLM 作为推理引擎。我的速度如下： FP32 → 4.5 个令牌/秒 FP16 → 8.8 个令牌/秒 对于 168GB GPU 集群 上的 14B 模型，这感觉太慢了。我原本期望有更好的性能，但有些东西成为了系统的瓶颈。 我使用的命令 python -m vllm.entrypoints.openai.api\_server \--model /home/ubuntu/DeepSeek-R1-Distill-Qwen-14B \--enable-reasoning \--reasoning-parser deepseek\_r1 \--dtype float16 \--host [0.0.0.0](http://0.0.0.0) \--port 8000 \--gpu\_memory-utilization 0.98 \--tensor-parallel-size 1 \--pipeline-parallel-size 7  我注意到的事情 尽管我已经使用了 98% 的 GPU，但所有 GPU 都没有得到充分利用。 如果您使用过多节点 vLLM 设置，我很想听听您是如何优化性能的。有什么帮助吗？ **我遗漏了什么？**a    提交人    /u/shrijayan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1in0gwo/d_14b_model_168gb_gpu_and_only_4_tokenssec/</guid>
      <pubDate>Tue, 11 Feb 2025 14:59:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] GAN 和扩散模型的优化技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imy4wn/doptimization_techniques_for_gans_and_diffusion/</link>
      <description><![CDATA[我正在使用开源 GAN 和扩散模型，但问题是对于我的用例模型，它具有较高的推理时间  那么有什么技术可以减少它吗？    提交人    /u/jiraiya1729   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imy4wn/doptimization_techniques_for_gans_and_diffusion/</guid>
      <pubDate>Tue, 11 Feb 2025 13:06:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调能赚大钱——怎么做到的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imwnnp/d_finetuning_is_making_big_moneyhow/</link>
      <description><![CDATA[嘿！ 自从我担任计算机视觉研究员以来，我一直在研究 LLM 行业。 与计算机视觉任务不同，似乎许多公司（尤其是初创公司）都依赖于基于 API 的服务，例如 GPT、Claude 和 Gemini，而不是像 Llama 或 Mistral 这样的自托管模型。我还在这个 subreddit 中看到了很多讨论微调的帖子。 这让我很好奇！据报道，Together AI 的 ARR 已达到 1 亿美元以上，令我惊讶的是，微调似乎是其主要的收入驱动因素之一。微调是如何为如此高的收入数字做出贡献的？公司是否在大力投资它以获得更好的性能、数据隐私或成本节省？ 那么，为什么要微调模型而不是使用 API（GPT、Claude 等）？我真的想知道。  很想听听您的想法 - 提前谢谢！    提交人    /u/Vivid-Entertainer752   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imwnnp/d_finetuning_is_making_big_moneyhow/</guid>
      <pubDate>Tue, 11 Feb 2025 11:41:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 循环潜在推理：在不生成标记的情况下扩展语言模型中的测试时间计算</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imwkns/r_recurrent_latent_reasoning_scaling_testtime/</link>
      <description><![CDATA[我发现这篇论文的主要贡献是重新思考我们如何通过连续循环处理而不是离散层来扩展推理过程中的计算。作者建议将模型深度视为一个连续参数，可以在推理时动态调整。 主要技术要点： - 引入“循环深度” - 允许信息多次循环通过组件 - 将深度建模为连续参数而不是离散层 - 使用微分方程的原理来创建平滑的信息流 - 根据任务复杂性实现自适应计算 主要结果： - 匹配较大模型的性能，同时减少 30-40% 的计算量 - 与传统架构相比，显示出更稳定的训练动态 - 展示了跨处理步骤的改进的信息保留 - 通过增加推理迭代实现了一致的性能扩展 我认为这种方法可以帮助解决我们扩展语言模型的一些基本效率低下问题。我们可以通过更智能的处理更好地利用现有参数，而不是简单地扩大模型。对深度的连续处理还为在部署期间平衡计算与性能提供了更大的灵活性。 我认为最大的挑战将是在实践中有效地实现这一点，特别是对于并行处理。与传统的前馈架构相比，循环性质增加了复杂性。然而，计算节省可能使它对许多应用程序来说是值得的。 TLDR：本文建议将神经网络深度视为连续的而不是离散的，使用循环处理在推理过程中更有效地扩展计算。显示出有希望的结果，在保持性能的同时减少了 30-40% 的计算量。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imwkns/r_recurrent_latent_reasoning_scaling_testtime/</guid>
      <pubDate>Tue, 11 Feb 2025 11:35:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 项目 A：患者安全与学习的道德 AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imr4yx/p_project_a_ethical_ai_for_patient_safety_learning/</link>
      <description><![CDATA[作为一名拥有医院实践经验的实习护士，我见证了技术在哪些方面能够产生真正的影响，以及在哪些方面无法满足患者和医护人员的需求。医院目前面临的最大问题之一是患者跌倒：这个问题每年造成数十亿美元的损失，延长住院时间，并增加本已不堪重负的护士的工作量。虽然存在跌倒预防策略，但大多数策略仅依靠人工观察和人工干预，这在高压力环境下并不总是可行的。 我正在开展一项非营利性计划，旨在开发一种可穿戴贴片，用于跟踪患者运动、预测跌倒风险并监测实时生命体征，包括心率 (HR)、呼吸频率 (RR)、皮肤温度、血氧饱和度 (SpO₂)（如果可能）和心电图监测。该系统将使用人工智能驱动的分析在跌倒发生之前提供预警，为护士提供主动工具来预防患者受伤并减轻员工负担。 这不是另一家专注于利润的人工智能驱动的初创公司，而是一项旨在将患者、护士和道德人工智能放在首位的非营利计划。我们的人工智能不会利用患者数据，不会取代医护人员，也不会危及安全。相反，我们正在构建一个可扩展、负责任的系统，该系统与医院工作流程相结合，使医疗保健更加安全。 现在，我一个人在做这件事，但我需要人工智能/机器学习工程师、生物医学工程师、软件工程师和人工智能伦理专家来实现它。虽然我还没有资金，但我知道，一旦我们有了一个可行的原型，获得合适的资金将容易得多。如果这个系统在一家医院取得成功，它可以扩展到全球的医疗保健系统，防止数千人跌倒，为医院节省数十亿美元，并减少护士倦怠。 除了医疗保健之外，我相信这种道德人工智能方法还可以改善现代教育。如果我们成功为医院创建负责任的人工智能，我们可以将同样的理念应用于支持学生和教师的教育系统，而不会取代人类的学习。 如果您对道德人工智能充满热情，并希望在医疗保健领域做出真正的改变，让我们一起创造伟大的东西。在下面给我发消息或评论，我很乐意合作。    提交人    /u/FatCockroachTheFirst   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imr4yx/p_project_a_ethical_ai_for_patient_safety_learning/</guid>
      <pubDate>Tue, 11 Feb 2025 05:12:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用潜在推理扩展测试时间计算：一种循环深度方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imr031/r_scaling_up_testtime_compute_with_latent/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imr031/r_scaling_up_testtime_compute_with_latent/</guid>
      <pubDate>Tue, 11 Feb 2025 05:04:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我对知识蒸馏的实验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imodbb/p_my_experiments_with_knowledge_distillation/</link>
      <description><![CDATA[嗨 r/MachineLearning 社区！ 我对知识提炼进行了几次实验，想分享我的发现。以下是比较教师、学生、微调和提炼模型性能的结果片段：   数据集 Qwen2 模型系列 MMLU（推理） GSM8k（数学） WikiSQL（编码）            1 预训练 - 7B 0.598 0.724 0.536   2 预训练 - 1.5B 0.486 0.431 0.518   3 微调 - 1.5B 0.494 0.441 0.849   4 Distilled - 1.5B, Logits Distillation 0.531 0.489 0.862   5 Distilled - 1.5B, Layers Distillation 0.527 0.481 0.841   如需详细分析，您可以阅读本份报告。 我还创建了一个开源库以促进其采用。您可以在此处尝试。 我的结论：当目标数据集上的较大模型和较小模型之间存在很大差距时，优先选择蒸馏而不是微调。在这种情况下，蒸馏可以有效地传递知识，从而比单独的标准微调产生更好的性能。 附注：本博客文章对蒸馏进行了高级介绍。 让我知道您的想法！    提交人    /u/darkItachi94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imodbb/p_my_experiments_with_knowledge_distillation/</guid>
      <pubDate>Tue, 11 Feb 2025 02:44:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 将 mHuBERT 模型追踪到 jit</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1imkdnf/p_tracing_mhubert_model_into_a_jit/</link>
      <description><![CDATA[嗨， 我将 mHuBERT 模型追踪到 jit 中，因此很容易从语音中提取离散的“语义”标记。在此过程中，我偶然发现了一些意想不到的事情，也学到了一些关于 FAISS 聚类库的知识。我决定将其包装成一篇文章以防万一。 如果您需要离散语音标记，请随意使用此处的跟踪模型：https://huggingface.co/balacoon/mhubert 您可以在博客文章中了解有关该过程的更多信息：https://balacoon.com/blog/mhubert_tracing/（包含对跟踪和测试笔记本的引用） 来自 hubert 或 wav2vec 的离散标记通常用作多模态 LLM 的音频输入。希望你会发现这很方便    提交人    /u/clementruhm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1imkdnf/p_tracing_mhubert_model_into_a_jit/</guid>
      <pubDate>Mon, 10 Feb 2025 23:24:55 GMT</pubDate>
    </item>
    <item>
      <title>深度学习笔记本电脑 PhD [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</link>
      <description><![CDATA[嗨， 我有 2,000 英镑，需要在 3 月之前用这笔钱买一台笔记本电脑（否则我会失去资金）用于攻读应用数学博士学位，这涉及大量深度学习。我所做的大部分工作可能都会在云端进行，但考虑到我有这个预算，我最好买一台最好的笔记本电脑，以防我需要离线运行某些东西。 请问可以给我一些购买建议吗？我不想买 Mac，但所有的选择让我有点困惑。我知道新的 GPU（nvidia 5000 系列）刚刚发布，并且已经宣布推出采用 Lunar Lake / Snapperagon CPU 的新款笔记本电脑。 我不确定我是否应该购买具有不错 GPU 的产品，或者只是购买像 lenove carbon x1 这样的轻薄超极本。 谢谢你的帮助！ **编辑： 我可以通过我的大学访问 HPC，但在使用它之前，我宁愿确保我的项目能够在我自己创建的玩具数据集或 MNIST、CFAR 等上运行。因此，除了推理之外，这意味着我可能会在我的笔记本电脑上进行一些轻度训练（老实说，这也可能在云端）。所以问题是我是否应该选择会耗尽电池并增加体积的 GPU，还是选择轻薄的 GPU。 我一直使用 Windows，因为我不喜欢软件，所以这不是什么问题。虽然我从未更新到 Windows 11，因为担心出现错误。 我有一台几年前用 rx 5600 xt 组装的台式电脑 - 我认为现在这已经非常过时了。但这意味着我不会将笔记本电脑停靠在上面，因为我已经有一台台式电脑了。    提交人    /u/Bloch2001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1im44wy/laptop_for_deep_learning_phd_d/</guid>
      <pubDate>Mon, 10 Feb 2025 11:37:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>