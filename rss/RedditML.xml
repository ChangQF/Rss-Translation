<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Mon, 22 Jul 2024 15:16:57 GMT</lastBuildDate>
    <item>
      <title>[P] 文本分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9g0vi/p_text_classification/</link>
      <description><![CDATA[目前正在开展一个与文本分类相关的项目。我的数据集不平衡（20% = 1，80% = 0）。我的流程是：1. 数据预处理（例如词干提取、删除停用词），2. 数据建模，3. 预测。 对于数据建模，我运行了多个 ml（例如 SVC、NB、RFC、ADA、GB），其中 SVC、RFC 和 ADA 表现最佳。因此，我对它们进行了相应的调整，并获得了它们的超参数以供调整。调整后，我将它们堆叠起来，并将 ADA 作为元模型。 我甚至尝试了 LSTM、RNN 和 Transformer。但即使准确率达到 95% 以上，我仍然没有得到我想要的预测。 不确定哪里出了问题。并且需要关于从现在开始如何处理这个问题的建议。    提交人    /u/anixouskid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9g0vi/p_text_classification/</guid>
      <pubDate>Mon, 22 Jul 2024 14:43:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] 太阳能电池板检测航拍图像</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9f7rm/p_solar_panel_detection_aerial_images/</link>
      <description><![CDATA[嗨，我想创建一个机器学习程序，可以从卫星和航拍图像中检测太阳能电池板。我刚开始，已经想到了一个编码器解码器架构，我正在准备数据。我使用了一个城市的航拍图像，我把它们切成 50 米 x 50 米的碎片。通常有些图像中只有一半的房子，所以电池板被切掉了。我想创建一个分割蒙版，并添加一个额外的标签来表示这张图片中有多少个太阳能电池板，这样人工智能就可以创建蒙版并计算出有多少个太阳能电池板。当一张图片中只有部分太阳能电池板时，你会如何标记这些图片？我使用 python LabelMe 创建蒙版，输出是一个 Json 文件，其中包含图像的位置、大小和蒙版。我还将太阳能电池板的数量添加到文件中。我不确定这是创建数据的正确方法还是我需要边界框。最大的问题是被切断的太阳能电池板。有时，一张图片仅显示一个面板的一小部分。    提交人    /u/Kinda_perverted   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9f7rm/p_solar_panel_detection_aerial_images/</guid>
      <pubDate>Mon, 22 Jul 2024 14:08:33 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 布局保存模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9f6dg/discussion_layout_preservation_model/</link>
      <description><![CDATA[我的产品用于数字化和翻译文档。我们正在使用由之前的团队构建的一个 OCR 模型。 我需要在产品之上添加一些布局保存/保留模型，以便我能够保留原始格式作为输入。如果有人知道是否有任何开源模型或库可用于保留布局，请帮助我。急需这个     提交人    /u/Illustrious_Put_5492   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9f6dg/discussion_layout_preservation_model/</guid>
      <pubDate>Mon, 22 Jul 2024 14:06:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 监督微调（SFT）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9f3pk/d_supervised_finetuning_sft/</link>
      <description><![CDATA[目前使用的每个聊天机器人，从 ChatGPT 到基于开源大型语言模型 (LLM) 构建的自定义聊天机器人，都经过了指令调整。LLM 与任何语言模型一样，只是下一个标记预测器。要让原始 LLM 像聊天机器人一样与用户交互，必须使用数以万计的用户与助手对话示例对其进行微调。这个过程称为监督微调，是生产 LLM 应用程序的基本构建块。 公开可用的 LLM 仍然是通用的，不适合直接用于大多数商业应用程序，因为它们需要不断微调才能产生高质量的结果。 现代监督微调解决方案涉及一种称为低秩适配器的东西。低秩适配器是相对较小的矩阵（数百万个元素，而不是数十亿个元素），它们与 LLM 的每一层并列，充当助手。它的工作是将 LLM 层的输入和输出转换为适当的域，而不会增加生产延迟。 在微调过程中，低秩适配器会根据黄金标准示例进行训练，以教会 LLM 如何响应。如果数据集质量高且多样化，那么经过微调的 LLM 输出的质量就会显著提高，只需 100 个示例，而不是数万个示例。传统上，这些示例将由专家手工制作，但编写它们既费时又费力。在 Plum Defense，我们会自动生成与人工编写的示例相当的示例。这样可以进行持续微调，从而不断提高 LLM 响应的质量。 通过将训练有素的低秩适配器与编写良好的系统提示相结合，机器学习从业者可以生成一个强大的应用程序，该应用程序可以很好地符合所需的输出，并且速度足够快，可以用于生产。 好的系统提示传达了意图，但足够简洁，为检索增强 (RAG) 系统留出空间，以便将相关事实注入应用程序。 系统提示的长度也会直接影响应用程序延迟。 系统提示越小，应用程序的平均响应时间越快。 借助软提示等高级技术，可以显著减小系统提示的大小，从而加快响​​应时间。 如果您想了解有关生产应用程序的持续微调和软提示系统的更多信息，请给我留言。    提交人    /u/juliannorton   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9f3pk/d_supervised_finetuning_sft/</guid>
      <pubDate>Mon, 22 Jul 2024 14:03:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] TTSDS – 对最近的 TTS 系统进行基准测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9ec0m/p_ttsds_benchmarking_recent_tts_systems/</link>
      <description><![CDATA[TL;DR - 我为 TTS 做了一个基准测试，您可以在此处查看结果：https://huggingface.co/spaces/ttsds/benchmark 目前有很多 LLM 基准测试，虽然它们并不完美，但它们至少概述了哪些系统在哪些任务上表现良好。文本转语音系统没有类似的东西，所以我决定用我的最新项目来解决这个问题。 我们的想法是找到与不同因素相对应的语音表示：例如韵律、可理解性、说话者等 - 然后根据 Wasserstein 距离计算合成语音与真实数据和噪声数据的分数。我在论文 (https://www.arxiv.org/abs/2407.12707) 中对此进行了更详细的介绍，但我也很乐意在这里回答任何问题。 然后，我将这些因素汇总为一个与合成语音整体质量相对应的分数 - 该分数与从 2008 年的论文一直到 huggingface 最近发布的 TTS Arena 的人工评估分数有很好的相关性。 任何人都可以此处提交自己的合成语音。并且我还将在未来几周内添加更多模型。离线运行基准测试的代码位于此处。    提交人    /u/cdminix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9ec0m/p_ttsds_benchmarking_recent_tts_systems/</guid>
      <pubDate>Mon, 22 Jul 2024 13:29:37 GMT</pubDate>
    </item>
    <item>
      <title>[P] 模型选择和方法帮助</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9dkjq/p_help_on_model_selection_and_approach/</link>
      <description><![CDATA[您好，先生/女士 我是数据科学的新手，正在学习许多概念和做项目。 最近我有一个项目，需要预测微塑料的重量值。这些塑料是用 3D 打印机打印的。 因此，3D 打印机的每个周期将有 70k 行数据，其中存在诸如 leser_distance、注射压力、型腔压力等特征。对于所有这 70k 数据，我们有一个输出，即塑料的重量。我总共运行了 75 次，即 70k*75 行数据和 75 个输出，即塑料重量。 我想检查哪种模型最适合这个，以及我应该遵循哪种方法。这将是很大的帮助，因为我很陷入这个困境。 谢谢！！！   由    /u/Affectionate-Cat-799  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9dkjq/p_help_on_model_selection_and_approach/</guid>
      <pubDate>Mon, 22 Jul 2024 12:53:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] PINN（物理信息神经网络）的方程要求</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</link>
      <description><![CDATA[我对损失项中的微分方程有疑问。通常，在 PINN 中，我们在损失函数中使用预测输出相对于输入变量的微分方程。例如，如果 u 是预测输出，x、y、m 是输入，则损失函数包括 du/d(x,y,m) 等项。 但是，如果我们只有输入变量相对于其他输入或输出变量的微分方程会怎样？例如：  dx/dt=f(x,y,u) dy/dt=g(x,u)  这里，x 和 y 的导数相对于时间 t。  并且没有 du/d(x,y,m) 方程 在这种情况下是否可以使用 PINN 方法，其中损失函数仅使用 dx/dt​ 和 dy/dt 构建？    提交人    /u/its_a_targaryen   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e9belt/r_equation_requirements_for_pinns_physicsinforemd/</guid>
      <pubDate>Mon, 22 Jul 2024 11:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] FLUTE - 一种用于量化 LLM 推理的新型 CUDA 内核，与 vLLM 相比，延迟降低了 2.6 倍。它将 QLoRA 扩展为可学习的尺度，每个参数量化为 4 位和 3 位。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</link>
      <description><![CDATA[ 大型语言模型 (LLM) 的部署通常受内存带宽的限制，其中主要瓶颈是将模型参数从 GPU 的全局内存传输到其寄存器的成本。当与融合反量化和矩阵乘法运算的自定义内核结合使用时，仅权重量化可以通过减少内存移动量来实现更快的推理。然而，为权重量化的 LLM 开发高性能内核带来了巨大的挑战，尤其是当权重被压缩为非均匀可分的位宽（例如 3 位）且使用非均匀查找表 (LUT) 量化时。本文介绍了 FLUTE，这是一种用于 LUT 量化 LLM 的灵活查找表引擎，它使用量化权重矩阵的离线重构来最大限度地减少与解包相关的位操作，并使用查找表的矢量化和复制来缓解共享内存带宽限制。当批量大小 &lt; 32 和量化组大小为 128（LLM 推理中的典型值），FLUTE 内核的速度可以比现有的 GEMM 内核快 2-4 倍。作为 FLUTE 的应用，我们探索了基于查找表的 NormalFloat 量化的简单扩展，并将其应用于将 LLaMA3 量化到各种配置，获得了与强基线相比具有竞争力的量化性能，同时获得了 1.5 到 2 倍的端到端吞吐量提升。  Arxiv：https://arxiv.org/abs/2407.10960    提交人    /u/radi-cho   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e99i92/p_flute_a_new_cuda_kernel_for_quantized_llm/</guid>
      <pubDate>Mon, 22 Jul 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 神经网络经过训练，能够使用少 50 倍的数据准确预测分子的最佳几何形状</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</link>
      <description><![CDATA[计算化学的一个重要任务是找到实现局部能量最小值的分子几何形状，因为这些是分子发生化学反应的最可能配置。尽管最近在分子构象能量预测的神经网络方面取得了进展，但此类模型容易因分布偏移而出错，从而导致能量最小化不准确。通过提供优化轨迹作为额外的训练数据，可以提高神经网络能量最小化的质量。不过，获得完整的优化轨迹需要大量额外的计算。 一个研究小组开发了一个名为“逐步优化学习框架”（GOLF）的新框架，该框架由一个高效的数据收集方案和一个外部优化器组成。作者证明，使用明显更少的额外数据，用 GOLF 训练的神经网络在各种类药物分子的基准测试中的表现与 Oracle 相当。  该~论文~发表于 ICLR 2024 会议论文集    由    /u/AIRI_Institute  提交  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/</guid>
      <pubDate>Mon, 22 Jul 2024 08:04:29 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 文档图像修复</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</link>
      <description><![CDATA[      这是 DocRes 图像在 chainner 中运行的恢复模型用于改进扫描的文档。原始图像后跟恢复后的图像，然后是 chainner 模型。更进一步，使用 Mindee Doctr 非常准确地获取线段。 我正在处理的下一个任务是识别字体大小，然后识别字体样式，然后使用 Microsoft Phi-3 或具有 OCR 功能的类似模型进行 OCR 并应用样式，然后恢复图像 链接 https://github.com/ZZZHANG-jx/DocRes https://github.com/chaiNNer-org/chaiNNer 原始图像 恢复后的图像 Chainner 架构 已识别线段    提交人    /u/atlury   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e96f4i/discussion_document_image_restoration/</guid>
      <pubDate>Mon, 22 Jul 2024 05:25:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用稀疏数据对自定义下游任务的 OS 模型进行微调的最佳实践</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</link>
      <description><![CDATA[我有一项下游任务，在输入过程中，99% 以上的数据都是上下文，由各种来源生成。实际模型输出只有几个标记，但输入的大小可以从 2k 个标记一直到 10k 个标记不等。因此，考虑到较长的上下文窗口，我尝试针对此任务微调 mistral 7b v0.3。但是尝试使用较低的学习率（如 8e-6）并衰减，我仍然会在每次运行时得到越来越高的训练损失。 训练集由标准 input_ids、attention_mask 和标签组成，但由于训练数据的性质，attention_mask 和标签分别大多为 1 和 -100。由于它们的大小也有很大差异，我将数据打包成 4096 的长度，使其保持不变。我的训练机器是 AWS trn1n.32xlarge 类型。关于我应该在这里做什么，有什么建议吗？对于任何对数据集感兴趣的人，这里是直接标记化版本数据的链接。    提交人    /u/VBQL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e962vd/p_best_practices_in_fine_tuning_os_models_with/</guid>
      <pubDate>Mon, 22 Jul 2024 05:03:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] ChessGPT 比 GPT-4 小 100,000 倍，下棋等级为 1500 Elo。通过找到技能向量，我们可以在非分布游戏中将其胜率提高 2.6 倍。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</link>
      <description><![CDATA[之前的一个项目训练了 ChessGPT，这是一组 25M 和 50M 参数的 GPT 模型，可以在 1500 Elo 下棋。这些模型比 GPT-4 的 1.8T 参数小约 100,000 倍。 在 Stockfish 0 级，50M 参数模型的胜率为 70%。但是，如果用 20 个随机动作初始化游戏，其胜率会下降到 17%。这是因为它无法泛化分布之外的内容吗？在考虑下一个标记预测任务时，如果游戏以随机动作开始，那么好的下一个标记预测器会预测合法但低技能的动作。 这就是我们在 ChessGPT 中发现的。通过向模型的激活中添加技能向量，我们可以将其胜率提高到 43%，即提高 2.6 倍。我们无法完全弥补性能差距，但这是一个很大的比例。干预非常简单，更复杂的干预可能会进一步提高其胜率。 该模型仅经过训练以预测 PGN 字符串中的下一个字符（1.e4 e5 2.Nf3 ...），并且从未明确给出棋盘状态或国际象棋规则。尽管如此，为了更好地预测下一个角色，它会学习在游戏的任何时候计算棋盘的状态，并学习各种规则，包括将军、将死、王车易位、过路兵、升级、固定棋子等。此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 Elo 评级。 我们还可以使用可解释性方法来干预模型的内部棋盘状态。 这项工作最近被 2024 年语言建模会议 (COLM) 接受，标题为“国际象棋语言模型中的新兴世界模型和潜在变量估计”。 更多信息请参阅此帖子： https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html 代码在这里： https://github.com/adamkarvonen/chess_llm_interpretability    提交人    /u/seraine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</guid>
      <pubDate>Sun, 21 Jul 2024 19:59:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 与主要作者吴正轩讨论 ReFT 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</link>
      <description><![CDATA[大家好， 本周星期五的论文讨论会上，我们非常幸运地邀请到了 ReFT 论文的主要作者，我想分享一下我们的讨论和笔记！ https://www.oxen.ai/blog/arxiv-dives-how-reft-works TLDR ~ ReFT 是一种微调技术，其参数效率比 LoRA 高 15 到 60 倍。训练速度超快。在 A100 上，1k 个示例大约需要 18 分钟。我成功地在不到 1 分钟的时间内，在 Llama 2 7B 上使用大约 100 个示例对 A10 上的 ReFT 进行了微调。 它的工作原理是操作残差流中的表示，而不是 K-V 矩阵。他们向特定的 token 索引和层添加了他们称为“干预”的额外学习参数，从而高效且轻松地控制表示。ReFT 也很不错，因为它们是可组合的。例如，您可以训练一个用于指令跟踪的模型，一个用于德语的模型，然后将它们都应用于德语的获取和指令跟踪模型。 作者给出了他们在实验室中迭代时学到的超级实用的技巧和教训。整个讨论也在 YouTube 上。 希望你喜欢！    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</guid>
      <pubDate>Sun, 21 Jul 2024 16:59:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基线薄弱和报告偏差导致机器学习对流体相关偏微分方程过度乐观</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</link>
      <description><![CDATA[  由    /u/nuclear_knucklehead  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</guid>
      <pubDate>Sun, 21 Jul 2024 16:05:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 21 Jul 2024 02:15:09 GMT</pubDate>
    </item>
    </channel>
</rss>