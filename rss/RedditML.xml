<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 26 Feb 2024 09:14:08 GMT</lastBuildDate>
    <item>
      <title>[R]机器学习理论家的研究生机会</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0cohg/rpostgrad_opportunities_for_ml_theorists/</link>
      <description><![CDATA[我是美国排名前 20 的计算机科学项目的一年级博士生。我对统计学习理论感兴趣。然而，我担心我的研究生机会，因为我最终可能做的研究过于理论化，实际意义为零甚至没有。对我来说，经验也很重要。 业界是否有兴趣雇用一位其工作没有实际意义的理论家？有没有办法我可以两者兼得？欢迎任何建议。请帮助一位贫困的一年级博士生。 :&#39;(    提交者   / u/dead_CS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0cohg/rpostgrad_opportunities_for_ml_theorists/</guid>
      <pubDate>Mon, 26 Feb 2024 08:32:36 GMT</pubDate>
    </item>
    <item>
      <title>“不要停止预训练”只是微调吗？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</link>
      <description><![CDATA[不要停止预训练论文吹嘘通过“领域适应预训练”提高法学硕士的表现，但这似乎是微调的另一个词，一点也不新鲜。我肯定错过了一些东西 - 它是什么？   由   提交/u/MLenthusiast34   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</guid>
      <pubDate>Mon, 26 Feb 2024 08:09:43 GMT</pubDate>
    </item>
    <item>
      <title>CLoVe：在对比视觉语言模型中编码组合语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0bdek/clove_encoding_compositional_language_in/</link>
      <description><![CDATA[ 由   提交/u/ashvar  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0bdek/clove_encoding_compositional_language_in/</guid>
      <pubDate>Mon, 26 Feb 2024 07:03:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于噪声对比损失，您是否应该避免批量将标签采样为负片？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0baq2/d_for_noise_contrastive_loss_should_you_avoid/</link>
      <description><![CDATA[      噪声对比损失如下所示，供某些人复习： &lt; p&gt;噪声对比损失 &lt; p&gt;因此，此处显示噪声对比损失，以供某些人复习：。由于我们通常使用批次 SGD 来优化这些方法，因此我们是否应该避免将批次的任何标签采样为批次的负数？我认为这是因为 log(Pmodel(x)) - log(Pnoise(x)) 所以 SGD 会尝试最大化 Pmodel(X) 来最大化损失函数，但是如果 Pmodel(x) 对于任何批次样本在 Pnoise(X) 中，它还会增加从 Pmodel(x) 中减去的量，这使得损失更小（或者也许我们想要这个？）所以我们应该避免在批量 SGD 期间将标签采样为负数吗？   由   提交 /u/DolantheMFWizard   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0baq2/d_for_noise_contrastive_loss_should_you_avoid/</guid>
      <pubDate>Mon, 26 Feb 2024 06:59:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] AI基础设施格局</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0bamx/p_ai_infrastructure_landscape/</link>
      <description><![CDATA[ 由   提交/u/gaocegege  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0bamx/p_ai_infrastructure_landscape/</guid>
      <pubDate>Mon, 26 Feb 2024 06:59:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemma 和 Llama 有什么区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0a5gu/d_what_are_the_differences_between_gemma_and_llama/</link>
      <description><![CDATA[Gemma 和 Llama 之间的架构差异是什么，我们可以利用这些知识来推断 Gemini 的工作原理吗？ Google 发现了什么？   由   提交 /u/heuristic_al   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0a5gu/d_what_are_the_differences_between_gemma_and_llama/</guid>
      <pubDate>Mon, 26 Feb 2024 05:48:23 GMT</pubDate>
    </item>
    <item>
      <title>[D]目视计数中还存在哪些问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b09vrm/d_remaining_problems_in_visual_counting/</link>
      <description><![CDATA[这个问题彻底解决了吗？是否还存在重叠问题等遗留问题？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b09vrm/d_remaining_problems_in_visual_counting/</guid>
      <pubDate>Mon, 26 Feb 2024 05:32:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否值得从 TensorFlow/PyTorch 切换到 JAX？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/</link>
      <description><![CDATA[大家好！我看到 JAX 越来越多地出现，例如Google Deepmind 在 JAX 中发布了他们的 Gemma 开源模型。我目前使用 TensorFlow/PyTorch。 JAX 值得查看吗？它能以与 TensorFlow/PyTorch 相同的灵活性完成相同的任务吗？   由   提交 /u/Few-Pomegranate4369    reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/</guid>
      <pubDate>Mon, 26 Feb 2024 04:28:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您喜欢使用 Ray 和 Dask 等云抽象吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b053p2/d_how_do_you_like_using_cloud_abstractions_like/</link>
      <description><![CDATA[大家好， 我认为我为 ML/AI 社区构建了一个非常有用的云抽象。我很想了解您对硬件抽象空间的想法，并了解更多有关您的经验的信息。 对于那些熟悉这些工具的人，我有几个问题...  学习曲线有多陡？ 您实现了这些抽象，还是由其他人完成的？ 您最有效的用例是什么？见过吗？ 当前抽象中缺少任何关键功能？  您的反馈将极大地影响我的路线图。期待您的回复   由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b053p2/d_how_do_you_like_using_cloud_abstractions_like/</guid>
      <pubDate>Mon, 26 Feb 2024 01:24:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 这个粗略估计对于运行 Mixtral 8x7B 32K LLM 是否准确？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0514t/d_is_this_rough_estimate_accurate_for_running_the/</link>
      <description><![CDATA[    /u/RoninNionr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0514t/d_is_this_rough_estimate_accurate_for_running_the/</guid>
      <pubDate>Mon, 26 Feb 2024 01:21:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推理速度差异较大</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azwsbp/d_large_difference_in_inference_speed/</link>
      <description><![CDATA[我正在训练用于对象检测的 yolov5 模型。我使用稀疏 ml 对其进行修剪和量化，然后将其导出为 onnx 格式。 （图像大小 640，批量大小 16） 在使用 CPU（和 ryzen 5 5600、16GB RAM）的笔记本电脑上推断时，每个图像速度大约需要 20 毫秒。 现在什么时候我在树莓派 5（A76，8gb 内存）中推断同样的事情，每个图像的推理速度仅为 220 毫秒 为什么推理速度有如此大的差异。我知道 Pi 模块的 cpu 可能较慢，但相差 10 倍??? 我在它们两个中安装了相同的库。是否需要在树莓派中手动配置onnx运行时以提高推理速度？   由   提交 /u/Melodic_Draw6781   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azwsbp/d_large_difference_in_inference_speed/</guid>
      <pubDate>Sun, 25 Feb 2024 19:40:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[N]Magika 简介：强大的文件类型检测库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azp35r/nintroducing_magika_a_powerful_file_type/</link>
      <description><![CDATA[      Magika 是 Google 开发的文件类型检测库，已获得关注。我们创建了一个网站，您可以在其中轻松试用 Magika。请随意尝试一下！ https://9revolution9.com/tools/security/file_scanner/  https:// /preview.redd.it/u5cuqvfyqqkc1.png?width=2094&amp;format=png&amp;auto=webp&amp;s=d16e51115134e3943cc6027cc0a9191ba835c38f ​ &lt; !-- SC_ON --&gt;  由   提交/u/glassonion999  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azp35r/nintroducing_magika_a_powerful_file_type/</guid>
      <pubDate>Sun, 25 Feb 2024 14:24:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我编写了一个用于调试 Triton 代码的小工具。有人感兴趣吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azmuf7/d_i_wrote_a_small_tool_for_debugging_triton_code/</link>
      <description><![CDATA[嘿，我正在编写 Triton 内核，据我所知，调试代码的唯一方法是使用 tl.device_print，它仅适用于张量数据（没有适合您的形状）并阻塞输出。因此，我编写了一个小工具来仅使用 torch 来运行内核，而无需更改代码。唯一的变化是减少启动网格大小并将内核包装器更改为调试包装器。下面是一个简单内核的示例： import torch import triton # import triton.language as tl import Tests.Triton.triton_debug_module as tl # @triton.jit @tl.debug def add_kernel(x_ptr , y_ptr, output_ptr, n_elem, BLOCK_SIZE: tl.constexpr): pid = tl.program_id(axis=0) block_start = BLOCK_SIZE * pid 偏移量 = block_start + tl.arange(0, BLOCK_SIZE) mask = 偏移量 &lt; n_elem x = tl.load(x_ptr + 偏移量, mask=mask) y = tl.load(y_ptr + 偏移量, mask=mask) 输出 = x+y tl.store(output_ptr + 偏移量, 输出, mask=mask) def add （x：torch.Tensor，y：torch.Tensor）：输出= torch.empty_like（x）断言x.is_cuda和y.is_cuda和output.is_cuda n_elem =输出.numel（）网格= lambda元：（triton.cdiv (n_elem, meta[&#39;BLOCK_SIZE&#39;]), ) add_kernel[grid](x, y, output, n_elem, BLOCK_SIZE=64) 返回输出  此代码将使用 torch 后端执行，并且您可以以正常方式查看每个张量形状和值。代码中唯一的变化是注释掉 triton.language 和 triton.jit 的导入，导入调试模块并在 tl.debug 中包装内核。 我还实现了内存读写的自动可视化（相同的东西，但是将内核包装在 tl.debug_vis 中，无需其他更改）。以下是 flashattention2 转发内核的示例： ​ Attn Fwd 那么，这个工具可能对某人有用吗？它仍然有点不完整，因为我还没有包装所有的triton函数，而且还有外部cuda函数，我只实现了tl.math.exp2 Floor sqrt和log2。那么，我应该开源它吗？我应该发布 arxiv 内容还是将其提交给某个研讨会？   由   提交/u/clueless_scientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azmuf7/d_i_wrote_a_small_tool_for_debugging_triton_code/</guid>
      <pubDate>Sun, 25 Feb 2024 12:31:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 除了检索增强生成（RAG）之外，还有哪些使用法学硕士构建的其他范式和框架？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</link>
      <description><![CDATA[自 InstructGPT 及其大众市场应用程序 ChatGPT 上市以来，已经一年多了，并引发了我们今天看到的围绕法学硕士的兴趣风暴。  除了研究和学术兴趣之外，行业（初创企业、大型企业等）也出于商业原因尝试构建由法学硕士支持的新产品和/或功能。 然而，到目前为止，我所看到的大部分内容要么是围绕 LLM 的薄包装应用程序，要么是 RAG 的某些变体。  除了检索增强生成（RAG）之外，LLM 构建还有哪些其他范式和框架？   由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azf0ul/d_what_are_some_other_paradigms_and_frameworks/</guid>
      <pubDate>Sun, 25 Feb 2024 04:27:51 GMT</pubDate>
    </item>
    </channel>
</rss>