<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 03 Jul 2024 06:21:39 GMT</lastBuildDate>
    <item>
      <title>[D] 需要一些数据科学书籍建议（较新的在这里）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1du6yp1/d_need_some_data_science_book_suggestionsfresher/</link>
      <description><![CDATA[目前正在使用句子嵌入。    提交人    /u/ShippersAreIdiots   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1du6yp1/d_need_some_data_science_book_suggestionsfresher/</guid>
      <pubDate>Wed, 03 Jul 2024 05:45:23 GMT</pubDate>
    </item>
    <item>
      <title>锦标赛调度策略[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1du4od0/strategies_of_tournament_scheduling_d/</link>
      <description><![CDATA[锦标赛安排策略 我正在研究一些概念和策略，这些概念和策略将安排具有某些约束或规则的联赛或锦标赛，并在更改比赛时记住某些过去的动作，直到满足所有规则。该模型还将通过使用循环赛池大小的特定布局，从正在处理的赛程之外的其他过去赛程中学习。 常见的约束包括：  没有背靠背的比赛 比赛之间的最短时间 不要与其他球队同时比赛 不要在特定时间或日期比赛 每天或每周最多比赛 平衡客场和主场位置  还有很多，但你应该明白了。我应该研究什么，开发人员应该采取什么流程或应该询问什么。    提交人    /u/cblaze22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1du4od0/strategies_of_tournament_scheduling_d/</guid>
      <pubDate>Wed, 03 Jul 2024 03:30:55 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于函数/工具调用的全新 Llama、Mistral、Phi、Qwen 和 Gemma 模型集合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1du3b1e/p_new_collection_of_llama_mistral_phi_qwen_and/</link>
      <description><![CDATA[介绍 Rubra v0.1：一组开放权重、工具调用 LLM 在 此处 在 Hugging Face Spaces 中免费试用！ 我们还扩展了 vLLM 和 llama.cpp，以便您可以非常轻松上手。查看我们的文档：Rubra 文档   模型 函数调用 MMLU（5 次测试） GPQA（0 次测试） GSM-8K（8 次测试，CoT） MATH（4 次测试，CoT） MT-bench    Rubra Llama-3 70B Instruct 97.85% 75.90 33.93 82.26 34.24 8.36   Rubra Llama-3 8B 指导 89.28% 64.39 31.70 68.99 23.76 8.03   Rubra Qwen2 7B 指导 85.71% 68.88 30.36 75.82 28.72 8.08   Rubra Mistral 7B Instruct v0.3 73.57% 59.12 29.91 43.29 11.14 7.69   Rubra Phi-3 Mini 128k 指令 65.71% 66.66 29.24 74.09 26.84 7.45   Rubra Mistral 7B 指令 v0.2 69.28% 58.90 29.91 34.12 8.36 7.36   Rubra Gemma-1.1 2B Instruct 45.00% 38.85 24.55 6.14 2.38 5.75   我们为什么创建这些模型 尽管专有模型和开源模型之间的能力差距一直在缩小，但我们看到函数/工具调用在开源中仍然落后。 直到现在，让 LLM 输出可靠函数调用的选项有限，就像让 OpenAI 和 Anthropic 这样做一样。提示工程、输出解析和 JSON 语法是一种 hack 选项。另一个选项是执行函数调用的模型，例如 Berkeley Gorilla、NexusRaven、Hermes、Command-R+，但它们都固定在一个模型上，有些在需要长上下文和在函数调用之上聊天的能力的代理用例中并不现实。最近，Mistral v0.3 中提供了工具调用，但在我们的测试中，它没有达到预期。 我们还根据对 gptscript、autogen 和其他代理框架的经验知道，您可能需要根据用例使用更小或更大的模型。我们不想被固定在一个模型上，所以我们决定对所有我们喜欢的模型进行进一步的后期训练。  一些旁注： - Rubra Qwen2 模型能够用中文进行函数调用！它在 Qwen2 支持的其他 28 种语言中具有有限的函数调用能力。 - GGUF 模型在过去 48 小时内的下载量约为 10 万次！ - 我们已经开始根据今天发布的 2024 年 6 月 Phi-3-mini 更新训练新的 Rubra Phi3。敬请期待！    提交人    /u/sanjay920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1du3b1e/p_new_collection_of_llama_mistral_phi_qwen_and/</guid>
      <pubDate>Wed, 03 Jul 2024 02:18:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 跨媒体文件的说话人分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtx0ta/d_speaker_diarization_across_media_files/</link>
      <description><![CDATA[许多语音转文本模型/API 都提供说话人分类功能，即不仅检测说话的内容，还区分当时说话的是哪个说话人。但是，是否有任何模型/API 可以跨媒体文件匹配说话人身份？例如，在音频文件 1 中，我们识别说话人 A 和 B，在音频文件 2 中，我们识别说话人 A 和 C，并且我们知道 A=A 和 B!=C。    提交人    /u/tfburns   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtx0ta/d_speaker_diarization_across_media_files/</guid>
      <pubDate>Tue, 02 Jul 2024 21:18:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推理过程中学习的当前研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dturka/d_current_research_in_learning_during_inference/</link>
      <description><![CDATA[我对在推理过程中可以学习的模型（尤其是自回归模型）的最新研究很感兴趣。这个领域有哪些关键的论文或方法？我特别感兴趣的是：  推理过程中更新权重的方法 应用于语言模型、时间序列预测等。  任何有关近期工作的指示或对有希望的方向的想法都将不胜感激。谢谢！    提交人    /u/uoftsuxalot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dturka/d_current_research_in_learning_during_inference/</guid>
      <pubDate>Tue, 02 Jul 2024 19:42:32 GMT</pubDate>
    </item>
    <item>
      <title>有任何拥有 1 H100 允许分析的云提供商吗？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtq8hn/any_cloud_providers_with_1_h100_allowing/</link>
      <description><![CDATA[您好，有谁知道有哪个 GPU 云提供商提供  租用单个 H100（而不是 8 个） 允许收集可能被 ncu 用于分析内核性能的分析数据。  例如，AWS 和 Lightning 允许收集分析数据，但我认为 Lambda 不允许。    提交人    /u/imurme8   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtq8hn/any_cloud_providers_with_1_h100_allowing/</guid>
      <pubDate>Tue, 02 Jul 2024 16:35:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 相同代码的结果有何不同？对于深度 CNN 项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtp8n5/p_difference_in_results_over_same_code_for_a_deep/</link>
      <description><![CDATA[[P] 因此，我正在复制我在 Github 上找到的代码以供练习。这是一个深度 CNN 项目。使用相同的数据集，并且所有内容都与代码相同。该代码大约有 3 年的历史了。该数据集是关于视网膜图像的。唯一的区别是 1）我使用的是 Pytorch、Keras 和 Tensorflows 的最新版本 2）我的硬件是带集成显卡的 AMD Ryzen 5700U，所以我没有 GPU，而是在 AMD CPU 上运行。但是，对于 epoch，原始代码大约需要 600 毫秒，而我的时钟时间为 250 毫秒，我的训练准确度与他们的训练准确度相匹配（约 98%）。然而他们的验证和测试准确度约为 97%，而我的验证和测试准确度约为 50%。原因是什么？因为数据预处理、模型参数等一切都一样。唯一的问题是库的版本较新并且不使用 GPU。我不知道原始代码的硬件规格，但从时代来看，我的CPU在速度方面似乎表现更好。    提交人    /u/Rogue260   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtp8n5/p_difference_in_results_over_same_code_for_a_deep/</guid>
      <pubDate>Tue, 02 Jul 2024 15:54:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求将单独的内容和行为嵌入相结合的研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtoozh/d_seeking_studies_on_combining_separate_content/</link>
      <description><![CDATA[语言模型是内容的优秀特征提取器，可提供高质量的内容嵌入。在对行为数据进行微调时，这些模型可以生成行为嵌入，而使用检索增强生成 (RAG) 方法可以生成混合嵌入。 我目前正在探索不同的方法来分别处理内容和行为嵌入，然后通过网络或类似方法将它们组合在一起。我对分析这种特定方法的性能的研究或文档特别感兴趣。 如果有人遇到任何深入研究这个主题的论文、博客文章或其他资源，我将不胜感激，如果你能分享它们。 提前致谢！    提交人    /u/Vichoko   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtoozh/d_seeking_studies_on_combining_separate_content/</guid>
      <pubDate>Tue, 02 Jul 2024 15:31:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过稀疏插值专家释放元调优的力量，实现小样本泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtntuq/r_unleashing_the_power_of_metatuning_for_fewshot/</link>
      <description><![CDATA[  由    /u/purified_piranha  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtntuq/r_unleashing_the_power_of_metatuning_for_fewshot/</guid>
      <pubDate>Tue, 02 Jul 2024 14:55:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人成功使用 TensorRT 进行 CLIP 模型推理吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtnb7g/d_has_anyone_successfully_used_tensorrt_for_clip/</link>
      <description><![CDATA[我很好奇这里是否有人有使用 TensorRT 部署 CLIP 模型进行推理的经验。以下是我的问题：  导出 ONNX 或构建 TRT 引擎时是否需要进行特殊修改？ 如果您已经实现了它，与其他框架（如 TensorFlow 或 PyTorch 或 ONNX 运行时）相比，您看到了哪些性能改进？  当我探索这对我的项目的可行性时，任何见解、共享经验或资源都将不胜感激。提前致谢！    提交人    /u/Mysterious_End_8021   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtnb7g/d_has_anyone_successfully_used_tensorrt_for_clip/</guid>
      <pubDate>Tue, 02 Jul 2024 14:33:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] Pytorch Geometric、强化学习和 OpenAI Gymnasium</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtmrqg/p_pytorch_geometric_reinforcement_learning_and/</link>
      <description><![CDATA[大家好。 正如标题所述，我正在尝试实现 openai gymnasium frostylake-v1 环境，以 pytorch 几何知识图谱表示，其中每个单元都是一个知识图谱节点，并且每条边都连接到玩家可以采取的可能路线。但是，我遇到了一个问题，即除非节点特征包含唯一值（无论是唯一节点索引还是它们在 4x4 地图中的位置），否则我的模型无法生成良好的结果。 我需要它独立于这些唯一索引，并且可能在一张地图上进行训练，然后将训练有素的代理放在一张新地图上，在那里他仍然能够对好动作和坏动作有一些概念（例如，掉进洞里总是不好的）。我该如何扩展这个问题？我做错了什么？如需更多信息，请在评论中留下，我一定会回答。 我正在写一篇论文，这个 openai gym 与我将在最终论文中进行训练的环境类似。所以我真的需要帮助解决这个特定问题。  编辑以获取进一步的深入信息： 我正在尝试将深度强化学习与图神经网络相结合以支持图环境。我使用 GNN 来估计 Dueling Double Deep Q-Network 架构中的 Q 值。我已经用 2 到 4 个 pytorch 几何 GNN（GCN、GAT 或 GPS）层替换了 MLP 层。 观察空间 为了测试这个架构，我使用了 frostylake-v1 环境的包装器，将观察空间转换为图形表示。每个节点都通过边连接到与其相邻的其他节点，代表一个就像正常人所看到的网格一样。 情况 1，具有位置编码： 每个节点具有 3 个特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 第二和第三个特征表示单元格的位置编码（单元格 x/y 坐标）： 第二个特征表示单元格列。 第三个特征表示单元格行。   情况 2，没有位置编码，使用单元格类型作为特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 单元格的类型。如果它是一个正常单元，则为 0；如果它是一个洞，则为 -1；如果它是目标，则为 1。  动作空间 动作空间与 openai gym freezelake 文档中的完全相同。代理对 frostinglake-1 环境有 4 种可能的操作（0=左、1=下、2=右、3=上）。 奖励空间 奖励空间与 openai gym frostinglake 文档中的完全相同。 问题 我已成功实现了具有所有默认单元的默认 4x4 网格环境的策略收敛。在我的实验中，代理只能在案例 1 中描述的观察空间中实现这种收敛。  我试图理解为什么需要位置编码才能实现收敛？ 在实施观察空间案例 2 时，即使在长时间训练的探索过程中多次获得最终奖励，代理也永远不会收敛。 由于与 transformer 相同的原因，GNN 是否也需要位置嵌入？ 如果我在小型网格环境中使用足够的消息传递 2 到 4 层，每个节点都应该具有来自图中每个其他节点的信息，那么网络是否应该能够在这种情况下隐式学习位置嵌入？ 我也尝试过使用其他位置嵌入 (PE) 方法，例如随机游走（5-40 次游走）和拉普拉斯向量（2-6 K 值），但我无法使用此 PE 实现收敛方法。 奇怪的是，我也尝试过使用随机化的唯一节点索引作为特征，而不是位置编码，并且代理能够收敛。我不明白为什么代理在这些条件下能够收敛，但在 PE 情况和观察空间情况 2 中却不能收敛。     提交人    /u/SmkWed   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtmrqg/p_pytorch_geometric_reinforcement_learning_and/</guid>
      <pubDate>Tue, 02 Jul 2024 14:09:41 GMT</pubDate>
    </item>
    <item>
      <title>GitHub 问题或 Jira 问题数据集？[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtjbvl/github_issues_or_jira_issues_data_sets_p/</link>
      <description><![CDATA[大家好， 我目前正在开展一个项目，尝试将 GitHub 和 Jira 票证（问题）分类为不同的类别。我花了大量时间在 Kaggle 和 Hugging Face 等平台上寻找开源数据集，但一直没能找到可靠的数据集。 许多数据集自然都是由开源项目和存储库中的数据编译而成，而不是私人项目，私人项目往往遵循更明确的结构（例如常规提交、标签等），这与我正在进行的项目更一致。 如果有人拥有符合此描述的数据集，或者曾经参与使用此类数据的项目，那就太好了。 TLDR：寻找高质量的 GitHub 或 Jira 问题/票证数据集，其中票证遵循某种结构，例如常规提交、敏捷结构（定义、验收标准、用户故事）等。    提交人    /u/DonThe_Bomb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtjbvl/github_issues_or_jira_issues_data_sets_p/</guid>
      <pubDate>Tue, 02 Jul 2024 11:17:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 实时音乐生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dtijd2/d_realtime_music_generation/</link>
      <description><![CDATA[大家好，我目前正在寻找在音乐领域构建一些新工具以融入现场表演，并且很好奇您是否知道任何有趣的实时音乐生成工具可用并且仍在开发中？有相当多的音乐/声音生成库，但不是实时的，所以我很好奇您是否有任何建议。 我发现 RAVE 听起来很有希望：https://github.com/acids-ircam/RAVE?tab=readme-ov-file    提交人    /u/rororo99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dtijd2/d_realtime_music_generation/</guid>
      <pubDate>Tue, 02 Jul 2024 10:28:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 花费数十亿美元训练生成模型的人工智能实验室的最终目标是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dsnk1k/d_whats_the_endgame_for_ai_labs_that_are_spending/</link>
      <description><![CDATA[鉴于目前 LLM 和生成模型的热潮，前沿 AI 实验室正在投入数十亿美元的风险投资资金来构建 GPU 集群、训练模型、免费提供模型访问权限以及获取授权数据。但是，当这种热情消退、市场重新调整时，他们的游戏计划是什么？ 有一些挑战使得使用当前的 LLM 创建盈利的商业模式变得困难：  所有前沿模型的近乎相同的性能将使 LLM 市场商品化，并迫使供应商在价格上展开竞争，从而大幅削减利润率。与此同时，新模型的培训仍然非常昂贵。 高质量的训练数据变得越来越昂贵。您需要主题专家来手动创建数据或审查合成数据。这反过来又使得模型改进的每次迭代都更加昂贵。 开源和开放权重模型的进步可能会占据私有模型企业市场的很大一部分。 设备上模型的进步和与操作系统的集成可能会减少未来对基于云的模型的需求。 模型的快速更新周期为人工智能公司提供了非常短的回报窗口来收回训练新模型的巨额成本。  当资金枯竭时，Anthropic、Cohere、Mistral、Stability 等实验室的最终结果是什么？他们会与大型科技公司（例如 OpenAI 和微软）更加紧密地合作以扩大分销吗？他们会找到其他商业模式吗？他们会消亡还是会被收购（例如，Inflection AI）？ 有什么想法吗？    提交人    /u/bendee983   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dsnk1k/d_whats_the_endgame_for_ai_labs_that_are_spending/</guid>
      <pubDate>Mon, 01 Jul 2024 08:02:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ds3fbp/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ds3fbp/d_simple_questions_thread/</guid>
      <pubDate>Sun, 30 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>