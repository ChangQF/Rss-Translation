<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 15 Oct 2024 01:15:52 GMT</lastBuildDate>
    <item>
      <title>[D] 什么是“ML框架”？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3u1gv/d_what_is_an_ml_framework/</link>
      <description><![CDATA[我尝试使用 ML 已有一段时间了，我按照教程操作，对 PyTorch 也相当熟悉。一年多来，我一直在用漂亮的 C++20 构建一个 ML 框架。 我希望最终发布并产品化它。现在，我正在尝试明确定义明年首次发布所需的范围。 我的重点是深度神经网络（没有其他 ML 方法）。如果这样更清楚的话，我可以将其标记为“深度 NN 框架”。 我实现的功能：1. 用于定义层的可扩展框架。模型可以简单地由不同的层组成，然后进行训练和运行。2. 线性层、Relu、Softmax、Tanh 等等 - 无需分叉框架即可实现新层，基本上只是一个 C++ 类。3. 二次损失、交叉熵损失。新的损失函数很容易实现。4. 嵌入层、注意力层（带有一些自定义点）。5. 接下来将实现 CNN（Conv、pooling）层，应该很容易 6. 支持 RNN（需要做一些特殊工作才能使内存使用不依赖于迭代次数）5. 使用可插入优化器实现训练。SGD 已实现，ADAM 即将推出。支持多进程和分布式训练。6. 网络本身被编译为本机代码，目前支持 clang++ 和 GCC（Linux/Mac），将支持 VC++ 和 WebAssembly。将有几种方法可以打包参数，包括独立数据文件或可以链接到二进制文件中的 blob。 测试数据管理是作为并行项目开发的。目前，我支持下载存档文件并根据目录、大小、扩展名等对其进行采样。如果有用户需求，将与 Python 合作进行互操作。 价值主张：易于嵌入到其他代码库（视频游戏、嵌入式）中或在浏览器中部署和运行。我目前的里程碑是简单的编码器模型，还计划为 Q 学习提供一组有趣的功能。没有计划为推理或训练实现 GPU 支持，因为目前的目标是更小的模型，所以没有看到这种需要。 这样的框架还应该有什么？    提交人    /u/euos   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3u1gv/d_what_is_an_ml_framework/</guid>
      <pubDate>Mon, 14 Oct 2024 23:34:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自监督学习方法比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3shy5/d_self_supervised_learning_methods_comparison/</link>
      <description><![CDATA[在自监督学习 (SSL) 论文中，他们表明，使用自监督学习而不是监督学习，在 imagenet 上训练的模型效果更好，然后他们在特定数据集（例如城市景观）上评估该模型。当他们与其他自监督学习方法进行比较时，他们使用在 Imagenet 上使用 SSL 训练的模型。为什么他们不使用 SSL 在特定数据集（例如城市景观）上对模型进行更多微调，然后再进行比较？    提交人    /u/Feiwu7777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3shy5/d_self_supervised_learning_methods_comparison/</guid>
      <pubDate>Mon, 14 Oct 2024 22:21:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我实际上如何修剪 LLM 和 VLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3rmuy/d_how_do_i_actually_prune_llm_and_vlms/</link>
      <description><![CDATA[我知道修剪的工作原理和作用，但我还没有在 LLM 上尝试过。我计划修剪 MolomoE 72B 和 Qwen2 VL 72B。我使用什么软件来实际修剪这些模型。     提交人    /u/SpecialistStory336   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3rmuy/d_how_do_i_actually_prune_llm_and_vlms/</guid>
      <pubDate>Mon, 14 Oct 2024 21:42:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有关于软件验证的机器学习研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3nxpo/d_is_there_any_ml_research_regarding_software/</link>
      <description><![CDATA[我找到了几篇论文，但是我可能错过了正确的搜索词。 我特别感兴趣的是将实现 / 代码与它们的规范进行比较，或者只是通常使用机器学习检查 ~code 的等效性？ 谢谢    提交人    /u/SPD-1337   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3nxpo/d_is_there_any_ml_research_regarding_software/</guid>
      <pubDate>Mon, 14 Oct 2024 19:09:11 GMT</pubDate>
    </item>
    <item>
      <title>如何着手推荐系统项目？[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/</link>
      <description><![CDATA[我正在开展一个健康与保健项目，我需要向用户推荐可以改善其健康的日常任务。我已经有一个任务列表，但我希望为每个用户个性化这些建议。 我想首先让他们填写一份简短的调查问卷，然后根据他们的回答推荐任务。任何关于如何做到这一点的建议或意见都将不胜感激。    提交人    /u/Content_Reason5483   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/</guid>
      <pubDate>Mon, 14 Oct 2024 15:37:12 GMT</pubDate>
    </item>
    <item>
      <title>“[D]” 使用 SSL 主干网来替代流行对象检测模型的主干网。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/</link>
      <description><![CDATA[只是想知道，是否有人曾经使用过从 ssl 中学到的语义并将其应用于任何流行的对象检测模型，如 yolo 或 ssd，结果如何。我已经考虑这个问题有一段时间了，因为在我读过的许多 ssl 论文中，他们确实提到对象检测是 ssl 与监督对应物相比表现出色的下游任务之一。我也一直在寻找与此相关的任何材料，但无济于事。    提交人    /u/Antman-007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/</guid>
      <pubDate>Mon, 14 Oct 2024 12:37:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 结果监督奖励模型与过程监督奖励模型的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/</link>
      <description><![CDATA[在阅读 PPO 等 RL 算法的实现时，我发现优势和价值实际上是在 token 级别计算的。以下是如何在 OpenRLHF 中获得优势的实现（类似于 TRL） &quot;&quot;&quot;从奖励和价值计算优势和回报的函数。按照原始 PPO 论文中的计算方式计算：https://arxiv.org/abs/1707.06347 请注意，奖励可能包括 KL 散度损失项。优势如下所示： Adv1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... - V1 + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 返回如下所示： Ret1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 输入： - 值：形状为 (batch_size, response_size) 的张量 - 奖励：形状为 (batch_size, response_size) 的张量 输出： - 优势：形状为 (batch_size, response_size) 的张量 - 返回：形状为 (batch_size, response_size) 的张量 &quot;&quot;&quot;  因此，尽管人们使用结果监督奖励模型 (ORM)，但他们实际上使用它来获取 token 级反馈。从这个角度来看，PRM 和 ORM 似乎是相同的（因为它们都需要执行 token 级反馈）。我说得对吗？ 这是我的第一个问题。这是第二个问题：人们使用在结果反馈上训练的奖励模型来执行 token 级反馈是一种常见的做法吗？奖励/奖励模型似乎不准确/被误用。    提交人    /u/zetiansss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/</guid>
      <pubDate>Mon, 14 Oct 2024 11:45:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] pytorch 的高效视频摄取？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/</link>
      <description><![CDATA[我目前正在启动一个新项目，需要训练视频分类器/回归模型。每个视频由 ~360 帧组成，并以非常高的质量 ~3840x2160 拍摄（由同一相机在同一位置拍摄几乎相同的产品）。视频目前以 .ts 格式保存，我对此并不十分熟悉，但似乎压缩效率很高，因为每个视频仅占用约 15 MB 的空间。 我还不知道如何训练这些视频，但我的想法是在训练期间将每个视频分成随机的 n 帧剪辑。因此，如果 n=20，一个样本将具有形状 (20,3,3840,2160)。 最初，我在想，我只需将每个视频转换为图片帧，然后保存图片，或者将图片保存为 pytorch 对象。但是 15 MB 的视频会变成 0.5 GB 的 jpg 图片，更糟糕的是，如果我直接将其保存为 uint8 中大小为 (360,3,3840,2160) 的 pytorch 对象，那么每个小视频最终会占用大约 9 GB。所以显然这是不行的。 pytorch vision 有一个名为 VideoClips 的方法，https://github.com/pytorch/vision/blob/main/torchvision/datasets/video_utils.py，它似乎是为这种事情设计的，但使用此方法处理其中 3 个视频需要大约 80 秒。 （建议缓存此输出，但我不确定他们到底是什么意思？只是将结果腌制到文件中还是他们的意思是什么？） 使用 opencv 将相同的 3 个视频读入内存大约需要 20 秒，到目前为止这似乎是最好的方法，但我希望有一些我错过了的更好的工具。 也许解决方案涉及将视频从 .ts 转换为压缩程度较低但在 ML 中更易于读取和处理的格式？    提交人    /u/alyflex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/</guid>
      <pubDate>Mon, 14 Oct 2024 10:58:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] Text2Chart31：带有自动反馈的图表生成指令调整（EMNLP 2024 Main）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</link>
      <description><![CDATA[      论文： https://arxiv.org/abs/2410.04064v1 代码： https://github.com/fatemehpesaran310/Text2Chart31 TL;DR：我们提出了一个新的数据集 Text2Chart31，以及一种基于强化学习的 LLM 图表生成微调方法。 （b）我们的数据集 Text2Chart31 和（c）我们基于强化学习的指令调整的说明。 摘要：大型语言模型 (LLM) 已在各种语言任务中展示出强大的能力，尤其是通过指令调整方法。然而，LLM 在通过图表和绘图可视化复杂的真实世界数据方面面临挑战。首先，现有数据集很少涵盖全系列图表类型，例如 3D、体积和网格图表。其次，监督微调方法不能充分利用丰富数据集（包括文本、代码和图形）中的复杂关系。为了应对这些挑战，我们提出了一个分层的流程和一个用于图表生成的新数据集。我们的数据集 Text2Chart31 包含 31 种引用 Matplotlib 库的独特绘图类型，具有 11.1K 个描述、代码、数据表和绘图元组。此外，我们引入了一种基于强化学习的指令调整技术，用于图表生成任务，而无需人工反馈。我们的实验表明，这种方法显著提高了模型性能，使较小的模型能够胜过较大的开源模型，并在数据可视化任务中与最先进的专有模型相媲美。 数据集：我们利用 GPT-3.5-turbo 和 GPT-4 开发了一个分层的绘图生成流程。我们新贡献的 Text2Chart31 数据集支持基于 Matplotlib 的 31 种绘图类型，具有 11.1K 个数据点。我们在表 1 中概述了其主要特征，并将其与数据可视化领域的现有数据集进行了比较。 Text2Chart31 数据集 D 由 11,128 个数据点组成，每个数据点包含一个 (x, c, d, r, y) 元组：文本绘图描述 (x)、其对应的代码 (c) 和结果绘图 (y)。 对于 8,166 个数据点，我们还包括一个原始数据表 (d) 和中间推理步骤 (r) 来生成描述。 我们的数据集 Text2Chart31 的统计数据。 任务定义：我们的基准旨在评估三个任务：  描述到图表：给定一个绘图描述 x，算法会生成其相应的代码 c，该代码使用 Matplotlib 库创建图表。 原始数据到图表：当仅提供原始数据表 d 时，算法会生成中间推理步骤 r，用于分析原始数据，然后生成描述d 根据数据特征选择最合适的绘图类型。 代码到描述：给定绘图的代码 c，模型会生成该绘图的详细描述 x。  实验： 实验结果。CLI 和 L3I 分别表示 Code Llama Instruct 和 Llama 3 Instruct。    提交人    /u/Moreselflove0324   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/</guid>
      <pubDate>Mon, 14 Oct 2024 09:04:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于自动驾驶的廉价推理硬件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</link>
      <description><![CDATA[我目前正在建造一辆自动驾驶遥控车，它通过使用摄像头检测街道上的锥体以及使用激光雷达的激光雷达初始里程计系统进行导航。到目前为止，我使用的是 Nvidia Jetsons，但我对生态系统的状态（和价格）并不满意。由于我的实际推理需求非常小（yolov5s 为 60fps），您是否有使用不包含 Cuda GPU 且具有足够性能来运行此类系统的替代系统的经验？（假设对于管道中的所有其他内容，汽车填满了 4x 2.0GHz 内核）    提交人    /u/NumerousSwordfish653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/</guid>
      <pubDate>Mon, 14 Oct 2024 07:52:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于分离文档的图像分割模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</link>
      <description><![CDATA[大家好，我正在做一个涉及处理身份证件的项目。我想先从给定的图像中提取文件，然后再进行处理，我正在寻找一个可以帮助我做到这一点的模型。我将处理来自世界各地的通用文件，所以我不想用给定的一组文件来微调模型（如 yolo）。我正在寻找一个通用的文档分割模型，但到目前为止我还没能找到，因为它们中的大多数都涉及在给定的一组图像上微调 yolo 模型。 如果你们能提供一些线索，我将不胜感激。谢谢！    提交人    /u/comical_cow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/</guid>
      <pubDate>Mon, 14 Oct 2024 04:35:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助 NASA 资助的项目更多地了解太阳！（Kaggle 竞赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</link>
      <description><![CDATA[大家好，我叫 Hannah，我是 NASA 资助的 Eclipse Megamovie 2024 项目的通讯员。 4 月份日全食临近时，我们非常活跃，但还有更多令人兴奋的事情等着我们！我们发起了 Kaggle 竞赛，希望得到像这样的社区的帮助。以下是有关整个项目的更多信息以及我们的竞赛页面的链接。请随时提问，我会尽力回答！ 2024 年 4 月 8 日，日全食始于南太平洋，横跨北美洲，途经墨西哥、美国和加拿大。北美大陆第一个经历日全食的地点是墨西哥太平洋海岸，时间大约是太平洋夏令时间上午 11:07。 2024 年 4 月 8 日日全食之后，超过 145 名志愿者上传了超过 1 TB 的照片数据，供我们的项目使用。 Eclipse Megamovie 2024 (EM2024) 由 NASA 资助，旨在利用日全食期间收集的数据研究太阳，这是一个特殊的时期，可以研究太阳的行为，与其他任何时候都不同。日食和数据收集之后的下一个阶段是对照片数据进行分类和标记，然后我们就可以开始认真进行科学分析——这就是你发挥作用的地方！ 如果您精通 Python 代码和机器学习，您可能能够为解答有关太阳的以前未解答的问题做出贡献！  比赛页面链接：https://www.kaggle.com/competitions/eclipse-megamovie 比赛参与者将使用我们的 2017 年日全食数据集来“训练”机器，方法是编写代码并利用提供的训练数据集自动根据日食阶段将日食照片归类为几个类别之一。建议有兴趣参加本次比赛的人具备 Python 和机器学习基础知识。 与我们的竞赛一致的兴趣：摄影、太阳物理学和/或太阳科学研究、参与式科学和机器学习。奖品： 排行榜奖品：根据私人排行榜排名颁发。  一等奖：带太阳滤镜的图像稳定双筒望远镜、Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、一等奖证书。 二等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、二等奖证书。 三等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、三等奖证书。  参与者将帮助确保数据 [日食照片] 能够快速组织，并且每张图片都具有正确的信息（元数据）。通过帮助我们开发能够准确识别志愿者提交的照片中的日食阶段的代码，您将帮助我们跨越一个重大的数据处理障碍。通过您的代码，您将为这项由 NASA 资助的研究太阳喷流和等离子羽流铺平道路！ 您的任务是创建最准确的分类机，将日食照片分类到特定的日食阶段。如果您的代码能够成功地将提供的照片分类为以下类别，您就知道自己成功了：暗色或平面（校准镜头）、日偏食阶段（20 度的箱 [类别]）、钻石环阶段、日全食阶段，当然还有非日食类别。 特别感谢 Mods 让我知道在这篇文章中使用哪个标签 :) 已编辑文字    提交人    /u/EMegamovie2024   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</guid>
      <pubDate>Sun, 13 Oct 2024 23:11:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 被研究论文淹没了？🐸</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</link>
      <description><![CDATA[      我们是两名对人工智能研究感兴趣的工程师，但却被 arXiv 上大量新论文淹没。因此，我们开发了 Ribbit Ribbit，一款研究论文发现工具。  https://apps.apple.com/us/app/ribbit-ribbit/id6529547956 https://ribbitribbit.co  它会整理个性化的论文推荐，并将其转化为推文大小的摘要，这样您就可以像在 Twitter 上一样滚动浏览。您还可以像为您量身定制的播客一样收听更新。我们添加了一点轻松的体验，希望它能为整个纸质阅读过程增添一丝乐趣，说实话，这个过程可能会变得相当枯燥乏味 :p。 https://preview.redd.it/evoemobinlud1.png?width=1179&amp;format=png&amp;auto=webp&amp;s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5    提交人    /u/haoyuan8   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</guid>
      <pubDate>Sun, 13 Oct 2024 22:24:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 13 Oct 2024 02:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>