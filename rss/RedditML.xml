<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 10 Jun 2024 18:19:10 GMT</lastBuildDate>
    <item>
      <title>[D] Andrew Ng 的专业化是否值得（对于非初学者）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcs9x6/d_are_andrew_ngs_specializations_worth_it_for_not/</link>
      <description><![CDATA[首先，我听说 Andrew Ng 的机器学习和深度学习专长很棒。但我的情况是这样的：我决定改变我的职业，过去 3 年我一直在学习数据科学，自学 Python、R 和 SQL。我有一些项目，参加了一些 DataCamp 课程，读了一些书。那么，对于一个不是完全初学者的人来说，这些专长值得吗？还是我应该只关注项目来提高我的知识？谢谢！    提交人    /u/Cool-Independent-146   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcs9x6/d_are_andrew_ngs_specializations_worth_it_for_not/</guid>
      <pubDate>Mon, 10 Jun 2024 18:11:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 为何预测规模化的前沿 AI 模型的下游能力仍然难以实现？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcs47t/r_why_has_predicting_downstream_capabilities_of/</link>
      <description><![CDATA[  由    /u/RSchaeffer  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcs47t/r_why_has_predicting_downstream_capabilities_of/</guid>
      <pubDate>Mon, 10 Jun 2024 18:05:33 GMT</pubDate>
    </item>
    <item>
      <title>帮助确定一次性机器学习可交付咨询工作的合理小时费率或固定价格 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcqe7i/help_determining_reasonable_hourly_rate_or_fixed/</link>
      <description><![CDATA[最近，一家中型软件公司邀请我担任几个基于项目的 ML 咨询工作，并交付特定的模型。我从学术界直接进入了行业（在 ML 领域拥有 10 年的专业经验）。现在，我从事一份固定薪水的合同工作，但也有相当多的空闲时间可以做兼职。我只是不太熟悉雇佣兵的生活。 这些项目是一次性模型交付。我可以选择按小时计费或按项目计费。他们提到每个项目预计有 300-600 小时的项目时间。真的不知道每小时收费是 50 美元还是 200 美元。 任何这方面的建议都会有所帮助。谢谢！    提交人    /u/neuronbase   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcqe7i/help_determining_reasonable_hourly_rate_or_fixed/</guid>
      <pubDate>Mon, 10 Jun 2024 16:56:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我的 DQN 和 QMIX 训练后变得更糟了！！！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcpscv/r_my_dqn_and_qmix_get_worse_after_training/</link>
      <description><![CDATA[      大家好， 我正在研究多智能体强化学习。我曾在 MPE 环境中实施了独立 Q 学习（使用 DQN）和 QMIX，特别是 PettingZoo 的简单参考。但我的两个网络在训练后变得更糟。我曾尝试微调一些参数，但没有帮助。这是我的笔记本。我想知道如何避免这种情况并改善我的网络。上图是 QMIX。 QMIX    submitted by    /u/Civil_Statement_9331   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcpscv/r_my_dqn_and_qmix_get_worse_after_training/</guid>
      <pubDate>Mon, 10 Jun 2024 16:30:49 GMT</pubDate>
    </item>
    <item>
      <title>Amazon Chronos 时间序列预测 Colab Notebook 代码和视频演练 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dco0du/amazon_chronos_timeseries_forecasting_colab/</link>
      <description><![CDATA[      大家好， 如果有人感兴趣，请在这里分享... https://preview.redd.it/i7a3585lgr5d1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=70422716abad787cd0d816f6bbf568553136bedc 我创建了一个 Amazon Chronos Colab Notebook，可以测试其 T5 Small 模型进行预测。 我有一些不错的结果 - 尽管测试非常简单。 这是我分享过的第一个 colab Notebook，我对它非常满意。 它包括：  用于简单 ux 的 Ipywidgets 在模型中运行的示例数据集 内置输出可视化 预测结果的自动 csv 导出 能够选择不同的预测期和间隔设置  有一个 YouTube 视频演示，介绍如何使用它以及描述中的所有链接等。 https://www.youtube.com/watch?v=jyrOmIiI2Bc&amp;t=103s 链接到github 获取代码： https://github.com/smartaces/amazon-chronos-t5-sales-forecasting 祝一切顺利 :)    提交人    /u/Smartaces   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dco0du/amazon_chronos_timeseries_forecasting_colab/</guid>
      <pubDate>Mon, 10 Jun 2024 15:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] DeepMind 的 AlphaFold 如何开创蛋白质结构预测：5 分钟视觉指南。🧠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcnash/r_how_deepminds_alphafold_pioneered_protein/</link>
      <description><![CDATA[   TL;DR：DeepMind 的 AlphaFold 项目彻底改变了蛋白质结构预测，对药物发现和疾病理解（例如 COVID-19）具有重大影响。 除了技术成就之外，AlphaFold 项目还提供了关于解决难题和跨学科合作的经验教训。 AlphaFold 内部：DeepMind 的 SoTA 蛋白质结构预测秘诀 处理 img rosnosedo55d1... 处理 img 1mozao6co55d1... 正在处理图片 ycz9v69lo55d1...    由   提交  /u/ml_a_day   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcnash/r_how_deepminds_alphafold_pioneered_protein/</guid>
      <pubDate>Mon, 10 Jun 2024 14:47:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] 异构 GNN - 链路预测 - 铁路延误预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcjorv/p_heterogenous_gnn_link_prediction_railway_delay/</link>
      <description><![CDATA[你好，Reddit！ 我目前正在开展一个项目，该项目涉及预测法国铁路网络中的列车延误情况。该项目的数据表示为异构图，其中每列火车都与其前一个和后一个车站相连，也称为显著点 (PRs - Points Remarquables)。目的是预测将火车连接到其后续 PR 的边的延迟属性。 除了火车与 PR 的连接外，车站本身也相互连接以表示整体网络结构。在该网络的图形表示中，火车被描绘为节点，而车站之间的关系则由边表示。 数据封装在 HeteroData 对象中，该对象旨在处理具有各种类型节点和边的异构图。下面是一个图表的数据快照，其中标签为 y : HeteroData( train={ x=[391, 8], geometry=[391, 2], }, pr={ geometry=[3076, 2] }, (train, prev_pr, pr)={ edge_index=[2, 2034], edge_attr=[2034, 2], }, (train, foll_pr, pr)={ edge_index=[2, 5871], edge_attr=[5871, 1], y=[5871], }, (pr, pr_pr, pr)={ edge_index=[2, 3716], edge_attr=[3716, 2], } )  我不确定是否有可能为此类任务实现异构 GNN。我从这个开始，但我不知道如何实现前向方法： class SAGEConvReLU(torch.nn.Module): def __init__(self, in_channels, out_channels): super(SAGEConvReLU, self).__init__() self.conv = SAGEConv(in_channels, out_channels) def forward(self, x, edge_index): x = self.conv(x, edge_index) x = F.relu(x) return x class GNN(torch.nn.Module): def __init__(self, hidden_​​channels): super(GNN, self).__init__() self.conv1 = SAGEConvReLU(hidden_​​channels, hidden_​​channels) self.conv2 = SAGEConv(hidden_​​channels, hidden_​​channels) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = self.conv2(x, edge_index) 返回 x 类 RailwayHeteroGNN(torch.nn.Module): def __init__(self, hidden_​​channels): super(RailwayHeteroGNN, self).__init__() self.train_embedding = Linear(10, hidden_​​channels) self.pr_embedding = Linear(2, hidden_​​channels) self.gnn = GNN(hidden_​​channels) node_type = [&#39;train&#39;, &#39;pr&#39;] edge_types = [(&#39;train&#39;, &#39;prev_pr&#39;, &#39;pr&#39;), (&#39;train&#39;, &#39;foll_pr&#39;, &#39;pr&#39;), (&#39;pr&#39;, &#39;pr_pr&#39;, &#39;pr&#39;)] self.gnn = to_hetero(self.gnn, metadata=(node_type, edge_types)) def forward(self, data):  谢谢你的帮助！   由    /u/OtherDepartment8085  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcjorv/p_heterogenous_gnn_link_prediction_railway_delay/</guid>
      <pubDate>Mon, 10 Jun 2024 11:55:59 GMT</pubDate>
    </item>
    <item>
      <title>[N] 您认为这个新的开源文本转语音（TTS）模型有多好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcj439/n_how_good_do_you_think_this_new_open_source/</link>
      <description><![CDATA[大家好， 我是 CAMB AI 的 Arnav，我们花了上个月的时间构建和训练 MARS 的第 5 版，现在我们已经在 Github 上以英文开源了它 https://github.com/camb-ai/mars5-tts 我在 Reddit 这里 上发布了一篇更长的帖子。如果你们能看看并告诉我们你们的反馈，我们将不胜感激。谢谢！    提交人    /u/MrHumun   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcj439/n_how_good_do_you_think_this_new_open_source/</guid>
      <pubDate>Mon, 10 Jun 2024 11:21:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 经纪人背后的炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dcefvk/d_hype_behind_agents/</link>
      <description><![CDATA[最近我听到了很多关于多智能体系统初创公司的宣传，我不确定为什么会有这么多的炒作。是什么让多智能体系统变得困难？有哪些有趣的研究问题？DSPy 不是已经解决了很多这些问题吗？    提交人    /u/Primary-Track8298   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dcefvk/d_hype_behind_agents/</guid>
      <pubDate>Mon, 10 Jun 2024 05:53:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 为资源受限的设备开发模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dc5273/p_developing_a_model_for_resourceconstrained/</link>
      <description><![CDATA[我正在开发一个项目，我想在 3ds 上生成立体 3d 图像对，以便为不具备此功能的应用程序启用 3d 模式查看。  这是一个可以完成我需要的确切任务的模型，但没有达到我所需的性能：https://github.com/browarsoftware/stereofast 在项目的这个阶段，我已经使用本教程：https://tvm.apache.org/docs/how_to/work_with_microtvm/micro_custom_ide.html 在 3ds 上运行基准测试。教程中使用的模型（Visual Wake Word 模型，用于识别人是否在画面中）需要 125-129 毫秒才能运行。这不是很好。我目前正在将 MiDaS small 256 移植到 3ds 进行基准测试，但我担心 MiDaS 或任何深度估计模型对我的项目来说资源过于密集，因此我考虑设计自己的模型，使其具有一些语义分割或深度估计功能来处理我的硬件限制。 问题是，我不知道在机器学习模型方面哪些操作更耗资源或更少耗资源，所以我决定在这里发帖寻求建议。 如果我设计自己的针对我的用例优化的机器学习模型，那么在保持 OK 性能的同时，模型可以拥有的最大可能功能是什么？我应该考虑什么？  供参考，n3ds 规格：ARM11 MPCore 四核 @ 268MHz（一个为操作系统保留） – 四核 VFPv2 协处理器（矢量处理器） – 256MB FCRAM–  其他 3ds 硬件信息位于此处：https://www.3dbrew.org/wiki/Hardware#Common_hardware    提交人    /u/spogetini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dc5273/p_developing_a_model_for_resourceconstrained/</guid>
      <pubDate>Sun, 09 Jun 2024 21:40:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于已知 x 的数据分布，求可微函数 y=f(x) 的逆？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dc4xxa/d_invert_differentiable_function_yfx_for_known/</link>
      <description><![CDATA[问题：我有一个可微分（非双射）函数 f，其输入为 x ∈ RT 并产生输出 y ∈ RT，其中每个向量 x 的概率均相等，例如 x_i ∈ [0, 1]。 我的目标：对于给定的 y，我想找到一个满足 y=f(x) 的可能解 x。 我尝试过的方法：我尝试过随机初始化向量 x 并通过梯度下降更新向量，以最小化预测 y 和目标 y 之间的距离 - 尽管这在某些情况下有效，但该方法大多会陷入局部最小值。 接下来的想法：我正在考虑学习和尝试一些生成建模方法，例如扩散或流匹配？虽然我对这些方法大多不熟悉。 问题：我正在寻找解决这个问题的一般技巧和建议——想法、论文、博客文章。谢谢！    提交人    /u/the_real_fishman   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dc4xxa/d_invert_differentiable_function_yfx_for_known/</guid>
      <pubDate>Sun, 09 Jun 2024 21:35:11 GMT</pubDate>
    </item>
    <item>
      <title>我的 XTTS 网络屏幕阅读器 [项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dc3514/my_xtts_screen_reader_for_the_web_project/</link>
      <description><![CDATA[我一直在做一个项目，一个带有自定义声音的网页屏幕阅读器。我使用 Read Aloud，声音听起来不太好。Edge 中的声音听起来更好，但我真正想要的是我最喜欢的叙述者为我朗读工作电子邮件和 reddit 帖子。它的工作原理是克隆一个大约 30 秒的音频 你可以在这里听到 https://youtu.be/0qcrwc7Dfww?si=vqvuI853_WKRsytF 它的工作原理是启动一个 XTTS 服务器。然后安装我的扩展。我在 GitHub 中有所有的说明。 我发布的版本不附带声音，它是 BYOV。但是它会克隆你输入的声音。所有这些都是使用现有技术在你的家用电脑上完成的，我刚刚为它构建了一个 chrome 扩展。我并没有计划为它发布声音，这些声音必须由用户提供。 此版本在个人电脑上本地运行，但我有一个为我儿子的学校（他患有自闭症）开发的版本，它将基于服务器，因此可以在学校部署，但被锁定，所以孩子们不能随便添加声音。 这是代码 https://github.com/psdwizzard/XTTS-Read-Aloud    提交人    /u/psdwizzard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dc3514/my_xtts_screen_reader_for_the_web_project/</guid>
      <pubDate>Sun, 09 Jun 2024 20:18:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 微型时间混合器 (TTM)：IBM 强大的零样本预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dbt398/p_tiny_time_mixersttms_powerful_zeroshot/</link>
      <description><![CDATA[IBM 推出的全新开源基础时间序列模型： https://aihorizo​​nforecast.substack.com/p/tiny-time-mixersttms-powerful-zerofew?-reml--    提交人    /u/apaxapax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dbt398/p_tiny_time_mixersttms_powerful_zeroshot/</guid>
      <pubDate>Sun, 09 Jun 2024 12:55:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练 llama 3 8B 需要多少 VRAM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dbp2sz/p_how_much_vram_i_need_to_train_llama_3_8b/</link>
      <description><![CDATA[您好， 我猜这是一个非常菜鸟的问题，但是找不到答案。 我想使用 llama 3 8b 并使用我的自定义数据增强模型。 我想在我的 Nvidia GPU 上进行本地训练和运行模型。 我现在没有 GPU，只有 mac m2 pro 16Gb，需要知道要购买什么。 我想知道，VRAM 要求是什么？12 GB 可以吗，还是需要 16 GB 的 gpu？或者唯一的方法是 24 GB 4090 之类的东西？    提交人    /u/webdunesurfer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dbp2sz/p_how_much_vram_i_need_to_train_llama_3_8b/</guid>
      <pubDate>Sun, 09 Jun 2024 08:33:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>