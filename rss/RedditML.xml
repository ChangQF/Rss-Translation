<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 09 Apr 2024 09:13:53 GMT</lastBuildDate>
    <item>
      <title>[讨论] 深入探讨我的硕士论文的 GPT 模型和注意力机制 - 寻求社区建议和资源！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bznixy/discussion_diving_deep_into_gpt_models_attention/</link>
      <description><![CDATA[您好r/MachineLearning社区！我我目前正全神贯注于我的关于 GPT 模型和注意力机制的硕士论文，距离最后期限只有 2 个月了。这个旅程始于我的顾问建议探索单元测试的语言模型，这让我发现了 @karpathy 的激励性“让我们构建 GPT：从头开始，用代码，拼写出来”。视频。尽管要兼顾注意力缺陷多动症和繁重的工作安排（每天 8-12 小时，每周 6 天），这项研究对我来说仍然是一项充满激情的事业。我在时间管理和注意力方面的多动症挑战并没有削弱我探索人工智能在医学或生物识别等领域的整合以及理解人工智能技术的数学基础的热情。我正在寻找一个具有挑战性的项目并考虑来自@的工具LangChainAI（@hwchase17）、DSPy（@lateinteraction）或@huggingface，探索 Transformer 架构中的新领域。我最近的项目试图复制“为什么 GPT 可以在上下文中学习？”的发现。语言模型隐式地执行梯度下降作为元优化器” (arxiv.org/abs/2212.10559)，我的进展记录在这里：github.com/Elrashid/INF50…。我正在寻找文献综述，以确定我可以用新颖的解决方案来解决的重大差距。我正在向这个社区寻求建议、资源或经验，以帮助重新激发我的动力并指导我的研究方向。如果您对可以提供见解或灵感的论文、会议或论坛有任何建议，我将非常感谢您的分享。感谢您的支持并营造了一个可以蓬勃发展机器学习热烈讨论的空间！   由   提交/u/osman_elrashid  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bznixy/discussion_diving_deep_into_gpt_models_attention/</guid>
      <pubDate>Tue, 09 Apr 2024 09:04:58 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于使用预训练模型作为目标函数训练另一个模型所产生的问题的讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzn2ic/d_discussion_on_issues_arising_from_using_a/</link>
      <description><![CDATA[您是否知道有任何论文讨论使用预训练模型作为训练另一个模型的目标函数时可能出现的问题？例如，如果目标神经网络的分布不平衡，则目标函数对于另一个神经网络可能显得不平衡。另一个问题可能是，如果破解神经网络太容易，会导致其无法作为目标函数正常运行，从而导致不规则的早期停止等。   &amp;# 32；由   提交/u/Rowing0914  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzn2ic/d_discussion_on_issues_arising_from_using_a/</guid>
      <pubDate>Tue, 09 Apr 2024 08:31:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有什么好的、安全的文本标注工具？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzmg6j/d_what_is_a_good_and_secure_tool_for_text/</link>
      <description><![CDATA[嗨，我必须标记大型数据集来训练 NER 和文本分类模型。到目前为止，我一直在使用 doccano，但现在我的公司阻止了它，因为没有记录安全和隐私实践。有人对满足这些标准的工具有好的建议吗？   由   提交 /u/Saffromon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzmg6j/d_what_is_a_good_and_secure_tool_for_text/</guid>
      <pubDate>Tue, 09 Apr 2024 07:45:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年图像分类任务中的语义匹配、VQA 或对比学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzmbhp/d_semantic_matching_vqa_or_contrastive_learning/</link>
      <description><![CDATA[我正在尝试通过应用多模态模型来重建我的分类任务的一个版本。使用多模态模型为图像添加标题（例如密集标题）然后与候选标签匹配是否可行？匹配工作流程可以像标记化一样简单，也可以像语义匹配一样复杂。此外，我可以存储标题以供将来使用。 与使用大型模型执行 VQA（存在）相比，例如“这张照片中是否有 [CLS]？”，什么是精度的差异？是不是过度设计了？ 我参考了对比学习的方法，计算了图像特征和文本特征的相似度。该方法在单个标签上可以取得很好的效果，但由于阈值的原因，不适用于多标签。   由   提交 /u/Tricky-Cook-8689   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzmbhp/d_semantic_matching_vqa_or_contrastive_learning/</guid>
      <pubDate>Tue, 09 Apr 2024 07:35:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]雅可比和黑森</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzlbpv/djacobian_and_hessian/</link>
      <description><![CDATA[任何人都知道是否有任何现有作品可以强制深度学习模型的雅可比向量成为 Hessian 矩阵的特征向量，w.r.t相同的数据集？我正在实现这个损失，但是我没有任何有效的方法来估计这个损失，cos(HJ,J)   由   提交/u/_karma_collector  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzlbpv/djacobian_and_hessian/</guid>
      <pubDate>Tue, 09 Apr 2024 06:28:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 异步参数服务器如何与数据并行技术一起工作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzl9xf/d_how_does_an_asynchronous_parameter_server_work/</link>
      <description><![CDATA[      请原谅我的糟糕图表。我试图了解数据并行性如何与 异步参数服务器。 我目前的理解是有一个异步参数服务器并且（例如）我们有2个GPU工作线程。 GPU工人的工作是计算一批数据的梯度，然后将该梯度更新发送到参数服务器。然后，参数服务器将计算新的权重，然后将其发送到相应的 GPU，而无需等待其他 GPU 完成计算。 这是一个图表。 https://preview.redd.it/wdry4xf1fetc1.png?width=1646&amp; ;format=png&amp;auto=webp&amp;s=eda4b47fbe03d43a6706e96132e0380e7612ff00 这对我来说似乎是错误的。例如，假设由于某种原因，您有异构加速器，例如 nvidia H100 和 nvidia GTX 1060 等，H100 可能能够完成例如 5 个批次并在 1060 之前更新权重有机会根据第一次计算更新权重。因此，从理论上讲，GTX 1060 将在超旧权重上应用梯度。 在第二个图中，如果将权重应用于 H100，那么它会相对较快地收敛，但加法后期 1060 梯度会将其推出局部最小值。 ​ https://preview.redd.it/2pq4cw0ueetc1.png?width=730&amp;format=png&amp;auto=webp&amp;s=63ab73570e2e017 21796aa2d11b4b7152c38ff48 在这种情况下，异步参数服务器的权重更新是否正确，因为梯度是针对与新权重不同的一组权重？如果我错了，我很想弄清楚我的逻辑在哪里不正确，因为我很好奇，如果单个工作人员能够连续计算“稍微”旧的权重，而不需要太难的话，那会有多糟糕。时间收敛？   由   提交/u/stereotropic_CS   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzl9xf/d_how_does_an_asynchronous_parameter_server_work/</guid>
      <pubDate>Tue, 09 Apr 2024 06:25:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您如何公平地比较 SGD 与带有梯度裁剪的 RMSProp？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzjxe5/d_how_would_you_fairly_compare_sgd_vs_rmsprop/</link>
      <description><![CDATA[嗨！我目前正在进行有关法学硕士的研究。我试图比较两个经过不同训练的模型的性能。这两个模型使用相同的参数进行初始化。使用带有梯度裁剪和 RMSProp 优化器的某种损失函数来训练一个模型。另一个模型使用带有梯度裁剪的特定参数更新规则进行训练（这样对于每个训练步骤，我们裁剪参数更新方向的 l2 范数）。问题是，即使我将两个模型的学习率设置为相同，我最终也会得到不同的“有效”学习率，因此生成的两个模型与其初始模型参数的 l2 距离显着不同。原因是，使用梯度裁剪和 RMSProp 优化器训练模型将显着提高有效学习率，因为我们的二阶矩梯度移动平均值很小（由于梯度裁剪）。例如，在训练从相同参数初始化的两个模型后，通过 RMSProp 训练的模型将移动约 0.23（以参数的 l2 距离而言），而另一个模型仅移动 0.0059。  在两个模型之间强制执行相等的有效学习率的公平方法是什么？ 我认为一种方法是删除两个模型的梯度裁剪，但是局限性在于梯度裁剪似乎是训练法学硕士的标准做法。您将如何解决这个问题？   由   提交/u/jonny_trane  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzjxe5/d_how_would_you_fairly_compare_sgd_vs_rmsprop/</guid>
      <pubDate>Tue, 09 Apr 2024 05:01:34 GMT</pubDate>
    </item>
    <item>
      <title>[R] 没有指数数据就没有“零样本”：预训练概念频率决定多模态模型性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzjbpn/r_no_zeroshot_without_exponential_data/</link>
      <description><![CDATA[      摘要 &lt; blockquote&gt; 网络爬取的预训练数据集是令人印象深刻的“零样本”的基础。评估多模态模型的性能，例如用于分类/检索的 CLIP 和用于图像生成的稳定扩散。然而，目前尚不清楚“零样本”概念的意义有多大。泛化是针对这种多模态模型的，因为不知道它们的预训练数据集在多大程度上包含“零样本”过程中针对的下游概念。评估。在这项工作中，我们问：预训练数据集中这些概念的频率如何影响多模态模型在下游概念上的性能？我们在 34 个模型和 5 个标准预训练数据集（CC-3M、CC-12M、YFCC-15M、LAION-400M、LAION-Aesthetics）中全面研究了这个问题，生成了超过 300GB 的数据工件。我们一致发现，远非表现出“零射击”，而是表现出“零射击”。概括地说，多模态模型需要指数级更多的数据来实现下游“零样本”的线性改进。性能，遵循样本低效对数线性缩放趋势。即使在控制预训练和下游数据集之间的样本级相似性以及对纯合成数据分布进行测试时，这种趋势仍然存在。此外，根据我们的分析对采样的长尾数据进行基准测试模型，我们证明多模态模型整体表现不佳。我们将此长尾测试集贡献为“Let it Wag！”为进一步研究该方向奠定了基础。综上所述，我们的研究揭示了对训练数据的指数级需求，这意味着“零样本”的关键在于训练数据。大规模训练范式下的泛化能力仍有待发现。  ​ 概念频率与 T2I 审美分数之间的对数线性关系。 论文：&lt; /strong&gt; https://arxiv.org/pdf/2404.04125.pdf 项目： https://github.com/bethgelab/Frequency_definees_performance&lt; /a&gt; 数据集： https:// Huggingface.co/datasets/bethgelab/Let-It-Wag ​ ​   由   提交/u/quequero  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzjbpn/r_no_zeroshot_without_exponential_data/</guid>
      <pubDate>Tue, 09 Apr 2024 04:27:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 就 RAG 研究而言，为什么似乎很多人没有致力于猎犬的研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</link>
      <description><![CDATA[我是几年前进行 NLP 研究的人，后来停止并加入了行业，最近试图重新掌握事物。我对 RAG 相关的工作很感兴趣，并开始阅读一些论文。 我的理解是，对于 RAG，你有检索器和生成器。对于生成器来说，使用各种 LLM 似乎是标准的，但检索器似乎也设置为使用 BM25 或最初使用的 DPR 之类的东西。我认为 RAG 的性能将在很大程度上依赖于检索器，但我也有点惊讶地发现似乎没有在这方面进行大量研究。 我只是错误并且没有看向正确的方向？或者说，检索器似乎没有得到那么多关注是有什么原因吗？ 想想看，我并没有真正看到编码器模型总体上做了很多工作。    由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</guid>
      <pubDate>Tue, 09 Apr 2024 01:38:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 高效扩散模型中缺失的 U</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</link>
      <description><![CDATA[一篇新论文提出用利用神经常微分方程的连续 U-Net 取代扩散模型中的标准离散 U-Net 架构。这种重新表述可以对去噪过程进行连续建模，从而显着提高效率：  推理速度提高 80% 模型参数减少 75% 70% 保持或提高图像质量  关键技术贡献：  动态神经 ODE 模块建模潜在表示演化使用二阶微分方程 自适应时间嵌入来调节扩散时间步长的动力学 高效的 ODE 求解器和常量内存伴随方法，可实现更快、内存效率更高的训练 &lt; /ul&gt; 作者展示了这些在图像超分辨率和去噪任务上的改进，并通过详细的数学分析解释了为什么连续公式会导致更快的收敛和更有效的采样。 潜在影响： p&gt;  使扩散模型适用于更广泛的应用（实时工具、资源受限设备） 在深度学习、微分方程、动力学的交叉领域开辟新的研究方向系统  存在一些局限性：(1) ODE 求解器和伴随方法增加了复杂性，(2) 我认为即使进行了改进，扩散模型仍然可能需要大量计算。 &lt; p&gt;完整摘要此处。 Arxiv 此处。 TL;DR：新论文建议替换离散 U-使用神经 ODE 的连续 U-Net 扩散模型中的网络，可将推理速度提高 80%、参数减少 75%、FLOP 减少 70%，同时保持或提高图像质量。主要影响：更高效、更容易理解的生成模型、连续时间深度学习的新研究方向。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</guid>
      <pubDate>Tue, 09 Apr 2024 01:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] LightGBM 算法问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</link>
      <description><![CDATA[GBDT 算法帮助 嘿，我知道创建传统决策树时需要某种损失函数，例如均方误差 - 你扫描预测空间并找到最小化 MSE 的分割，这就是递归二元分割所实现的，直到达到某种停止标准。我也明白，如果要进行分割，您可以使用该区域中所有预测的平均值找到每个区域的新 MSE，这就是您如何确定是否进行分割的方法。 我目前正在学习提升，现在明白这个过程是相似的，但我们现在基于残差构建一棵树。过程完全相同吗？ 我一直在观看一些 statquest，这是 boosting 的通用算法 https://i.imgur.com/DudpZ5S.png 我正在努力理解 B) 和 C) 之间的区别。在 B 中，我们通过基于一些损失函数（如 MSE）进行递归二元分割来将回归树拟合到残差值（r_i_m）？但是对于 C 部分，我们在节点（称为 gamma）计算这些残差，这也最小化了损失函数？这不是我们在 B 部分所做的吗，因为我们取每个分割点残差的平均值，看看它是否最小化了我们的损失函数。 正如你所知，有点困惑，谢谢- 感谢任何帮助！   由   提交 /u/PencilSpanker   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzcgw7/d_lightgbm_algo_question/</guid>
      <pubDate>Mon, 08 Apr 2024 23:05:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的研究技术堆栈是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</link>
      <description><![CDATA[我计划进行文本+音频的大型多模态训练（1B 参数）。截至目前，我正在考虑使用 pytorch、deepspeed、wandb。对于分布式大型模型训练，您有什么建议以及一般使用什么？ 您使用 Hughginface 吗？我觉得它有点太包裹了，以至于接触到裸露的主干会变得混乱，但还没有进行适当的尝试。对于现成的模型和自定义数据集训练，这听起来确实很有用，但研究需要的不仅仅是这些。那么，您在研究方面的经验如何，您需要灵活地改变模型？一般来说，您在研究方面的技术堆栈是什么？   由   提交/u/gokulPRO  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz8pym/d_what_is_your_tech_stack_for_research/</guid>
      <pubDate>Mon, 08 Apr 2024 20:38:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于那些独自发表文章的人来说，你们的经历是怎样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</link>
      <description><![CDATA[这些天我有一些空闲时间，一直在努力赶上我所在领域的研究。我想真正重新审视我在硕士期间正在研究的一个主题，但始终无法发表论文。问题是，我不确定作为唯一作者，如果没有任何真正的资源访问权限，这是否可行。 朋友和熟人告诉我这是可能的，但极其困难。很好奇其他成功做到这一点的人是怎么想的。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</guid>
      <pubDate>Mon, 08 Apr 2024 14:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 确保加拿大的人工智能优势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</guid>
      <pubDate>Mon, 08 Apr 2024 09:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>