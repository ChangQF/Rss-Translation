<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Fri, 21 Feb 2025 21:14:19 GMT</lastBuildDate>
    <item>
      <title>将GEDI与推理模型使用？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuzh21/using_gedi_with_reasoning_models_d/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否可以与推理模型一起使用GEDI技术？目标是使调谐推理模型更加有效。    https://github.com/salesforce/salesforce/gedi     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/arcco96     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuzh21/using_gedi_with_reasoning_models_d/</guid>
      <pubDate>Fri, 21 Feb 2025 19:32:42 GMT</pubDate>
    </item>
    <item>
      <title>[r] MLGYM：一种推进AI研究代理商的新框架和基准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/</link>
      <description><![CDATA[   推进AI研究代理商的基准” src =“ https://b.thumbs.redditmedia.com/xinvvnpb0a7hm6gkf37oo-_ftpzxtqzxtqzvffljcgfchqrk.jpg” title =“ [r] mlgym：用于推进AI研究的新框架和基准，  &lt;！ -  sc_off -   来自摘要： 我们介绍了Meta Mlgym和Mlgym-Bench，这是一种用于评估和开发AI研究任务的LLM代理的新框架和基准。这是第一个用于机器学习（ML）任务的健身房环境，为培训此类代理的增强学习（RL）算法提供了研究。 MLGYM基础由来自计算机视觉，自然语言处理，强化学习和游戏理论等不同领域的13种不同和开放式的AI研究任务组成。解决这些任务需要现实世界中的AI研究技能，例如生成新的想法和假设，创建和处理数据，实施ML方法，培训模型，运行实验，分析结果并在此过程中进行迭代以改进给定的任务。我们在基准上评估了许多边界大型语言模型（LLM），例如Claude-3.5-Sonnet，Llama-3.1 405B，GPT-4O，O1-Preview和Gemini-1.5 Pro。我们的MLGYM框架使添加新任务，集成和评估模型或代理，按大规模生成综合数据，并为培训AI研究任务培训代理开发新的学习算法变得容易。我们发现，当前的边界模型通常可以通过找到更好的超参数来改善给定的基线，但不会产生新颖的假设，算法，体系结构或实质性改进。我们开源框架和基准，以促进未来的研究，以推动LLM代理的AI研究能力。  arxiv： https ：//arxiv.org/abs/2502.14499  github： https://github.com/facebookresearch/mlgym     &lt;！ -  sc_on-&gt;＆＃32 ;提交由＆＃32; /u/rybolos     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/</guid>
      <pubDate>Fri, 21 Feb 2025 17:46:14 GMT</pubDate>
    </item>
    <item>
      <title>[d]降低维度是不好的做法吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我得到了一个问题陈述和数据。我最初的直觉是“该数据集中最重要的功能，我可以透露什么最初的关系？&#39; 我提出了t-sne，pca或UMAP来观察初步关系以探索，但立即进行了关闭是因为“缩小维度”意味着丢失信息。”  我知道这是真的，但是... ___________？您会说什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ready_plastic1737     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/</guid>
      <pubDate>Fri, 21 Feb 2025 17:30:22 GMT</pubDate>
    </item>
    <item>
      <title>[d]援助博士学生</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuudwp/d_help_phd_student/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我是英国第二年的博士生。我必须在第二篇论文上工作，我已经很晚了。我正在努力寻找研究差距。 我的博士学位是为信用风险增强学习。对于我的第二篇论文，我希望使用多代理RL。但是，我找不到研究差距。 有人可以帮助如何前进吗？我感到非常压力和振奋，我的进步评论在五月开始，我不知道下一步该怎么做。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iuudwp/d_help_help_phd_student/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuudwp/d_help_phd_student/</guid>
      <pubDate>Fri, 21 Feb 2025 16:05:30 GMT</pubDate>
    </item>
    <item>
      <title>[D]我们是否在基本模型中遇到了缩放墙？ （非推理）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   grok 3据说接受了100,000 H100 GPU的训练，该GPU在球场上比GPT-4系列和Claude 3.5 Sonnet的型号高约10倍。  它们的能力差不多。 Grok 3不是我们希望的。在2023年和2024年，Openai一直在说他们只能继续扩展预训练的预训练，而模型只是神奇地变得更加聪明（“缩放法律”，“图表”只是“线”上的“ line”上升了“） 现在，所有的重点都放在推理上，突然Openai和其他所有人都对扩展 变得非常安静，老实说，它看起来非常可疑。现在，他们没有像2020  -  2024年那样制作越来越大的模型，而是试图使它们保持较小，同时专注于其他事情。 Claude 3.5 Opus从人类博客中悄悄地删除了，没有任何解释。有问题，他们正在尝试将其隐藏  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ch1997H     [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/comments/1iupnet/d_have_we_we_hit_a_a_scaling_wall_in_in_in_base_models_models_models_models_non/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/</guid>
      <pubDate>Fri, 21 Feb 2025 12:23:20 GMT</pubDate>
    </item>
    <item>
      <title>[P]非线性策略的参数优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuophg/p_parameter_optimization_of_a_nonlinear_policy/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在从事的项目是基于带有工业机器人的工厂。机器人由PLC控制，具有10个预定义的“复杂”。它可以执行的操作/任务。当机器人完成任务后，PLC评估工厂状态（观察），并确定（政策）要向机器人指示哪种措施。  目前，此决定是由我写的算法（IF-Else的一棵评估各种传感器/状态的If-Else树）来定义的。该项目的目的是优化/提出/更改本算法以改善进入植物的生产。注意：该工厂足够复杂，以至于我无法在执行的动作之间建立准确的依赖模型   请务必注意，我无法在现场进行测试/学习，唯一可用的数据是我可以在植物runnign时记录的数据与电流Algorith。 最初我研究了增强学习，经过一些探索，我得出结论，深度Q学习是必经之路。我将定义奖励功能，在可估计数据上训练神经网络，并最终使用神经网络切换我的算法。 NN与Agorithm一样，将分析一系列的观察，并提供执行的任务。 这种方法似乎是合理的，但由于他们不希望在PLC上运行的神经网络而被公司策略拒绝和“跳跃”在两个演员之间，本来是“戏剧性的”。和不安全的。 因此，我们转到了一种更加线性的方法：首先，我要修改我的算法，以引入某种参数，允许修改定义要选择的任务的过程。 我的新目标是在植物生产方面优化这些参数。使用DQL，我有一个清晰的学习算法，可以迭代改善神经网络的参数，但是使用我的算法，我不知道如何改善参数。&lt; /p&gt;  IDEA：我唯一的事情想出的是使用可估算的数据训练DQN，以获得优化的策略。然后，我尝试找到最近似于此策略的算法的参数。，由于参数的可能组合不是很大（20！）i，虽然探索所有数据并找到产生相同的参数的组合   的动作似乎是一个有趣的项目，因为它具有一些不寻常的限制。如果有人有一些想法/考虑，请分享，因为我是位卡住。谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/loripao_pagu     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuophg/p_parameter_optimization_of_a_nonlinear_policy/</guid>
      <pubDate>Fri, 21 Feb 2025 11:26:54 GMT</pubDate>
    </item>
    <item>
      <title>[r] ML-DEV板台：现实世界中ML工作流程上的基准测试代理（AI可以创建AI？）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iunf1b/r_mldevbench_benchmarking_agents_on_realworld_ml/</link>
      <description><![CDATA[        &lt;！&lt;！ -  sc_off- sc_off-&gt; &lt; Div类=“ MD”&gt;  ML-DEV基准是一种新的基准测试，可在实用机器学习开发工作流程上测试AI代理的功能，而不仅仅是编码任务或Kaggle风格的比赛。该基准包括：  数据集处理（下载/预处理） 模型培训（加载预审计的模型，finetuning） 调试（debugging）形状错误，爆炸梯度，不正确的实现） 模型实现（修改架构，添加功能）  API集成（记录工具） 模型性能优化  评估React，OpenHands和Aide Agents的关键发现：   OpenHands--十四行诗的成功率最佳，其次是47％的反应 其他配置（OH-GEMINI，AIDE-4O，REACT-4O） 17％的成功率 代理在诸如数据集处理之类的结构化任务上表现良好，但在诸如性能优化之类的开放式任务中挣扎   没有代理在模型性能改进任务 &lt;  /ul&gt;    评估框架（称为卡钳）和基准测试是： https://github.com/ml-dev-bench/ml-dev-bench   纸： https://arxiv.org/abs/2502.00964    您对这些结果有何看法？您认为应该将ML开发工作流的其他方面包括在将来的迭代中？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iunf1b/r_mldevbench_benchmarking_agents_on_realworld_ml/”&gt; [link]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iunf1b/r_mldevbench_benchmarking_agents_on_realworld_ml/</guid>
      <pubDate>Fri, 21 Feb 2025 09:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML工程师的最佳澳大利亚公司</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iukd8r/d_best_australian_companies_for_ml_engineers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  标题所建议的，一个用于子上的澳大利亚人；推理和GPU经验在澳大利亚工作的ML工程师在哪里？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/scottyg_23    href =“ https://www.reddit.com/r/machinelearning/comments/1iukd8r/d_best_australian_companies_for_ml_engineers/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iukd8r/d_best_australian_companies_for_ml_engineers/</guid>
      <pubDate>Fri, 21 Feb 2025 06:21:25 GMT</pubDate>
    </item>
    <item>
      <title>[D]是否有任何理论机器学习论文对从业者有重大帮助？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，  21m决定是否专门针对其数学博士学位专业。具体来说，我对  i）试图了解神经网络和变形金刚（例如神经切线核）的好奇现象以及训练前及其影响的影响；生成AI的多模式培训（如： https://arxiv.org/pdf/1806.07572  href =“ https://arxiv.org/pdf/2501.04641”&gt; https://arxiv.org/pdf/2501.04641 ）经验表现，例如原始辍学和批处理文件。 我想从事某事在我的博士学位期间可能会产生深远影响，但仍然理论上。当试图找出类别中基于理解的问题i）是否适合此描述，但是，我在网上找不到太多...   是否有人有任何论文的特定示例谁的主要重点是理解某些现象，最终对从业者彻底改变了事物，这将欣赏它：）  真诚地，  nihaomundo123   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nihaomundo123   href =“ https://www.reddit.com/r/machinelearning/comments/1iuanhy/d_are_there_there_any_theoricentic_machine_learning/”&gt; [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/</guid>
      <pubDate>Thu, 20 Feb 2025 21:59:39 GMT</pubDate>
    </item>
    <item>
      <title>[r]使用信息理论检测LLM幻觉</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;    llm幻觉和错误是一个主要的挑战，但是如果我们可以预测它们何时发生怎么办？大自然有一个很好的出版物关于语义熵LLMS的模式。 分享有关该方法的博客以及有关检测LLM幻觉和错误的迷你实验。 博客链接在这里。受。 方法摘要    序列对数探针提供了一种免费，有效的方法来检测不可靠的输出（可以解释AS“ LLM Profust＆quot”）。   高信任响应的准确性几乎是低信任的（76％vs） 45％）。 使用这种方法，我们可以自动过滤反应不佳，引入人类审查或迭代的抹布管道。      &lt; Strong&gt;实验设置很简单：生成1000个抹布的LLM对各种问题的回答。要求专家盲目评估质量的反应。查看LLM置信度有多少预测质量。  奖励：llm的精密召回曲线。  思想&lt; / h1&gt; 我的解释是，LLM在不自信的情况下以较高的熵（较不可预测的输出 /更平整的令牌分布）进行操作。因此，它正在处理更多的不确定性并开始基本上分解。 不管您对LLMS有效性的看法如何，这感觉就像是捕获大部分错误的最简单但有效的方法之一。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/meltingwaxcandle     [link]    [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/</guid>
      <pubDate>Thu, 20 Feb 2025 21:22:44 GMT</pubDate>
    </item>
    <item>
      <title>[d]用最后一个隐藏状态丰富令牌嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 从信息理论的角度来看，查看解码器变压器的工作过程，我们可以看到在最后一个隐藏状态在世代相传中崩溃为一个令牌。这意味着您崩溃了一个隐藏状态，从理论上讲，该状态具有：   hidden_​​dim * 32 （或任何量化的任何量子）信息，以下是：  &lt; p&gt; log₂（dict_size）  我想知道这是否是一件好事（对不起，对天真的措辞感到抱歉）。变压器用于预测接下来令牌的信息完全存储在其上下文窗口中，不涉及任何经常性状态。因此，预测序列的接下来令牌即将馈送的变压器将产生与相同序列完全相同的结果，如果它完全由变压器本身生成。 公平， 公平，从某种意义上说：是生成序列还是只是读取的任何内容都不会改变下一代币应该是什么。 ，但另一方面，这种方法意味着 all 令牌之间的信息流必须通过注意机制发生。变压器无法将一些细微差别或风味嵌入预测的令牌嵌入中。喜欢：  “好吧，我预测了令牌&#39;  肯定   &#39;，但我宁愿意味着&#39;    90％确定  &#39;。   当预测下一个标志时可能存在于最后一个隐藏状态（甚至在软拿的输出概率分布中）完全丢失了。 因此，当我昨天散步时，我认为在使用令牌嵌入中添加一些信息可能是一个好主意类似：   augmented_embedding = embedding（token） + f（last_hidden_​​state）  （确保确保很重要的是那是：  &#39;f（last_hidden_​​state）&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;试图找到有关此主题的论文，并要求从Claude，Chatgpt和困惑中提供反馈。     claude 告诉我这是“一个令人难以置信的有见地的想法。” /strong&gt;给了我一长串完全无关的来源。  ，所以我转向你们。如果有一个大脑子的家伙告诉我为什么其他大脑子的家伙决定不遵循这个想法，或者为什么它不起作用，我会很喜欢。 这是我发现的一些可能有问题的事情：   1。训练复杂性 变压器很适合与重型并行化训练，这是因为它们不是递归的。每个大小 n 的序列都可以给出 n-1 独立的训练示例。在令牌嵌入中注入最后一个隐藏状态的信息将打破一些并行化。 ，我猜仍然可以有效地训练它。  首先，请（ n-1 ）香草序列并获取预测。 ，对于每个预测 现在，您有一组新的训练序列，其中所有（但第一个）令牌嵌入式已更新。 您可以无限期地重复此过程。我希望它收敛^^   顺便说一句，这看起来确实像是一个扩散过程。这将我带到了下一个点：  2。稳定性（尽管这种令牌嵌入的增强作用显而易见，试图防止模型的输出无敏化，但在这里，我不是很有能力。定义这种过程稳定性的条件是什么？我没有受过教育的猜测是，如果您保留： ” last_hidded_state_contribution”≪” agemented_token_embedding” &lt; /strong&gt; 您不应该遇到很多问题。但这也将限制信息流。我想这是一个权衡的，如果不够好，我不会感到惊讶。 你们怎么看？这已经在某个地方尝试过吗？是否有基本原因这是行不通的？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/Academic_sleep1118     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</guid>
      <pubDate>Thu, 20 Feb 2025 18:06:18 GMT</pubDate>
    </item>
    <item>
      <title>[d] DeepSeek 681亿美元的推理成本与超尺度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我已经估计了deepseek的成本/性能681亿，这样：  huggingface Open DeepSeek博客报告了Config＆amp;性能= 32 H100的800TPS   100万代币= 1250S = 21（ish），分钟。 69.12万代币每天 租金32 H100的费用〜$ 80000 &lt;$ 80000 &lt;&lt; /p&gt; 每百万个代币= $ 37.33（80000/31天/69.12） 我知道这是非常乐观的（100％利用，没有支持等），但是算术是否有意义，并且通过您认为是否通过嗅探测试？还是我有明显的错误？  我猜这比诸如双子座的API型号高1000倍，并且这个差距使我想知道我是否很愚蠢  &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/sgt102     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</guid>
      <pubDate>Thu, 20 Feb 2025 13:44:05 GMT</pubDate>
    </item>
    <item>
      <title>[p]萨卡（Saka）释放了库德纳（Cudiner）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1itqrgl/p_sakana_ai_released_cuda_ai_engineer/</link>
      <description><![CDATA[在CUDA-GEANDERER/  它将火炬转换为CUDA内核。  这是步骤： 阶段1和2（转换和翻译）： AI CUDA工程师首先将Pytorch代码转换为功能cuda内核。我们已经观察到初始的运行时改进而没有明确定位这些。  阶段3（进化优化）：受生物进化的启发，我们的框架利用了进化优化（&#39;“&gt;生存”优点’），以确保只生产最好的CUDA内核。此外，我们介绍了一种新颖的内核交叉促进策略，以互补的方式结合多个优化内核。  阶段4（创新档案）：，文化进化如何影响我们的人类智能借助我们祖先到几千年的文明，AI CUDA工程师还利用了从过去的创新和发现中学到的知识（第4阶段），第4阶段），从已知的高性能CUDA内核的血统中构建创新档案，该档案使用以前的踏板石来实现进一步的翻译和性能增长。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/preams_delay_3701      [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1itqrgl/p_sakana_ai_released_cuda_ai_engineer/</guid>
      <pubDate>Thu, 20 Feb 2025 05:07:05 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客等对于产品和服务。 请不要发布链接缩短器，链接聚合器网站或自动订阅链接。    任何滥用信托的滥用都会领导禁止。 鼓励其他人创建新帖子，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。   元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iqiy4x/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 16 Feb 2025 03:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1ilhw29/d_simple_questions_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    </channel>
</rss>