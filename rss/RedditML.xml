<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Fri, 29 Mar 2024 09:12:48 GMT</lastBuildDate>
    <item>
      <title>[P]我把埃隆·马斯克的脸变成了决策边界。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqkcor/pi_turned_elon_musks_face_into_a_decision_boundary/</link>
      <description><![CDATA[      我见过二维决策边界呈现奇怪形状的示例，例如螺旋线，以及我一直很好奇神经网络有多灵活。为此，我试图让它学习一张照片，即埃隆·马斯克的脸，它成功了。假设模型足够复杂，决策边界似乎可以任意复杂。该照片来自wikipedia.jpg)。该模型采用每个像素的 x 和 y 坐标，并经过训练来预测映射到 0 到 1 之间的值的灰度值。我使用的决策阈值是 0.5。我已经包含了应用阈值后的图像（它说明了决策边界）以及应用阈值之前模型生成的灰度。我还包括了模型认为图像的延续会是什么样子。我还制作了一个训练过程的视频，每隔几个 epoch 一张图像，但无法在 reddit 上分享:(。无论如何，希望每个人都喜欢这些图片！ ​ Elon 决策边界 ​ Elon 灰度图（逐个坐标生成） ​ Elon，超越框架——神经网络认为是在图片之外。 ​     提交者   /u/lildaemon   [链接] &amp; #32； [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqkcor/pi_turned_elon_musks_face_into_a_decision_boundary/</guid>
      <pubDate>Fri, 29 Mar 2024 08:44:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将马尔可夫状态模型拟合到时间序列数据时，采样率对参数估计有何影响？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqk91v/d_what_is_the_effect_of_sampling_rate_on/</link>
      <description><![CDATA[假设我有一些时间序列数据，可以用马尔可夫状态模型来描述。时间序列每隔 $Δt$ 时间单位进行采样。采样率 (1/Δt) 必须控制我们可以从时间序列中提取多少信息。较慢的采样率本质上会错过较快的转换，从而对转换率产生较差的估计。或者，如果采样率与某些转换率相比太慢，则某些状态甚至可能不会显示在收集的数据中。我知道转移概率的最大似然估计由下式给出 pij=nij/Σnij 这里，pij 是从状态 i 到 j 的转移概率，nij 是从 i 到 j 的转换次数。 我正在寻找一本书，其中已在这方面系统地讨论了采样频率的影响，并希望通过一个简单的示例进行演示。另外，是否有任何分析结果将估计参数的误差与采样频率联系起来？   由   提交 /u/SnooPineapples375   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqk91v/d_what_is_the_effect_of_sampling_rate_on/</guid>
      <pubDate>Fri, 29 Mar 2024 08:37:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习功能与移动/Web 功能交付</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqiwtw/d_machine_learning_feature_versus_mobileweb/</link>
      <description><![CDATA[Reddit 用户们好！我是一名机器学习工程师，在一家基于产品的初创公司工作了 4 年，该公司的技术团队约有 20 名成员，我是这里唯一的机器学习工程师。我目前面临的问题是，ML 功能预计将以与移动/Web 功能相同的速度交付。这里没有人理解机器学习，包括工程经理。我还需要按照开发人员的方式创建 Jira 票证，而且我知道 ML 任务从来不会真正遵循通常的待办事项、开发中、暂存中、审核中和生产中生命周期。这些估计总是需要经过实验，因为人们永远不知道训练期间会出现什么问题，而且变量太多，一个人无法独自处理所有问题。  我确实不认为工作场所有毒，但我当然希望这里的人们开始以更新鲜的视角看待事物。来到这里让我开始接触 JS 和 Python。尽管我可能无法使用 Kotlin 和 Swift 进行编码，但我很清楚如何在生产中使用 ML 功能以及如何在本机（iOS 和 Android）中工作。我喜欢这里的成长，但如果有一些事情发生变化，我当然会喜欢。 请随时留下您的建议/经验，我将不胜感激。再次感谢！   由   提交​​/u/Muse_Not_Found  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqiwtw/d_machine_learning_feature_versus_mobileweb/</guid>
      <pubDate>Fri, 29 Mar 2024 07:02:48 GMT</pubDate>
    </item>
    <item>
      <title>机器学习项目评估问题 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqis5o/ml_project_evaluation_questions_d/</link>
      <description><![CDATA[大家好， 我想知道如果您第一次审查一个项目，您会问什么类型的问题时间。作为背景，我们正在向一些人教授机器学习咨询。我会在帖子发布几天后列出我的问题，因为我不想偏见任何人。你想要多少就给多少。谢谢！   由   提交 /u/MuscleML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqis5o/ml_project_evaluation_questions_d/</guid>
      <pubDate>Fri, 29 Mar 2024 06:54:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 批评我在集合预测平均中调整多重共线性的新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqiqc6/d_critique_my_novel_approach_to_adjust_for/</link>
      <description><![CDATA[大家好，我是一名刚高中毕业的学生，​​对机器学习/统计非常感兴趣，很快就会开始我的数据科学学位。&lt; /p&gt; 最近我一直在阅读有关集成建模和平均预测的方法，然后思考高度相关的特征/预测的问题。 我一直在考虑一种新颖的方法来地址相关功能，我想寻求您对此的反馈，以了解我是否只是愚蠢或遗漏了一些重要的东西。 场景  考虑 N 个因素，每个因素都用于生成目标变量 Y(t+1) 的简单线性预测。 因此，在任何时间 t，在 t+1 时都有 N 个 Y 预测。  通过取每个预测的平均值来计算最终的组合预测。  多重共线性问题  对这些预测进行平均会假设每个因素具有同等重要性，因为它为所有因素分配相同的权重 (w=1/N)。 但是，这种方法不准确地强调了彼此高度相关的因素。  li&gt;  我调整相关特征的方法 为了缓解这种情况，如果您根据每个特征与其他特征之间的绝对相关性，例如： 特征 X 的相关性调整权重 =（1 - 与所有其他特征的绝对相关性的平均值） 其中最终有效权重为每个特征只是它们的相关性调整权重除以所有特征相关性调整权重的总和。 我使用特征与其他特征绝对相关性的平均值的原因是，特征是否具有相关性1 或 -1 与另一个功能，无论哪种方式，该功能都不会增加新的价值或信息（除非我错了？）。 基本原理 每个功能应根据其与其他特征的相关性向下调整权重，因为这种相关性量化了信息的重复。 简化 示例 假设有 3 个因素 X1、X2、X3，其中  X1 与 X2 和 X3 的相关性为 0， X2 和 X3 的相关性为 1。  X1 与 X2 和 X3 的相关性为 0。 li&gt;  平均预测意味着每个因素的权重为 1/N（在本例中为 1/3）。 但是，如果 X2 和 X3 几乎相同，则意味着： X2 和 X3 实际上是同一件事，对 X1、X2、X3 的预测进行平均会导致错误地高估 X2 和 X3 (w=2/3)，而低估 X1 (w = 1/3)。 因此使用我的方法调整每个因素的权重，我们得到：  调整后的 X2= 1 - (1+0)/2 = 0.5  X3 的调整权重 = 1 - (1+0)/2 = 0.5 X1 的调整权重 = 1 - (0+0)/2 = 1 调整权重之和= 0.5 + 0.5 + 1 = 2  如果我们将每个因素调整后的权重与调整后的权重之和进行比值，我们得到  有效X2 调整后权重 = 0.5 / 2 = 0.25 X3 调整后有效权重 = 0.5 / 2 = 0.25 X1 调整后有效权重 = 1 / 2 = 0.5  如您所见，调整已针对 X2 和 X3 的完美相关性进行了调整，因为  X2 和 X3 的有效调整权重之和为 1/ 2 X1 的有效调整权重现在为 1/2  _____________________ 我的问题  您对这种方法有何看法？ 它在逻辑上有意义吗？ 是否有类似的模型/方法？  &gt; 有什么关键缺陷吗？ .....即我是不是一个完全业余的白痴，忽略了一些关键或明显的东西？  谢谢！   由   提交 /u/YourGoodFriend44   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqiqc6/d_critique_my_novel_approach_to_adjust_for/</guid>
      <pubDate>Fri, 29 Mar 2024 06:50:49 GMT</pubDate>
    </item>
    <item>
      <title>[D]变形金刚还有其他重要的用例吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqhyuo/dare_there_any_other_non_trivial_use_cases_of/</link>
      <description><![CDATA[Seq2Seq 预测架构是为序列预测而设计的，并且在文本生成中自然是 SOTA，但是还有其他我们可以使用它们的重要任务吗？就像 MeshGPT 使用 gpt 模型来生成网格一样，扩散变压器现在也在研究中，事实上 sora 使用了一种模型。在许多其他应用程序中，这些模型可能是高效且可扩展的吗？   由   提交/u/ApartmentEither4838  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqhyuo/dare_there_any_other_non_trivial_use_cases_of/</guid>
      <pubDate>Fri, 29 Mar 2024 06:00:20 GMT</pubDate>
    </item>
    <item>
      <title>人们会对聚合此处发布的论文的不和谐服务器/博客感兴趣吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqhji1/would_people_be_interested_in_a_discord/</link>
      <description><![CDATA[嗨，很多海报发布了他们发现有趣的研究工作和论文，很难跟踪。我给有趣的论文添加了书签，结果却忘记了它们。  想知道这里的人是否值得做一个不和谐服务器（每天更新）或一个媒体博客（每周更新）来跟踪在此子上发布的论文。   由   提交 /u/shadowylurking   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqhji1/would_people_be_interested_in_a_discord/</guid>
      <pubDate>Fri, 29 Mar 2024 05:32:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会如何回答这个面试问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqftlw/d_how_would_you_answer_this_interview_question/</link>
      <description><![CDATA[不确定这是否是一个“职业问题”，但最近有人问我这个面试问题： 在一场有 10 辆赛车的 F1 赛车比赛，您如何计算/预测第二名赛车超过第一名赛车的概率？这个计算需要什么算法、数据和模型？解释每个步骤。 你会如何回答这个问题？ （没有给出其他信息）   由   提交/u/Conscious_Giraffe453  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqftlw/d_how_would_you_answer_this_interview_question/</guid>
      <pubDate>Fri, 29 Mar 2024 03:56:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] Jamba：首款基于 Mamba 的生产级模型，提供一流的质量和性能。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</link>
      <description><![CDATA[帖子：https://www.ai21 .com/blog/announcing-jamba  我们很高兴宣布 Jamba，世界上第一个基于 Mamba 的生产级模型。通过使用传统 Transformer 架构的元素增强 Mamba 结构化状态空间模型 (SSM) 技术，Jamba 弥补了纯SSM模型。它提供了 256K 上下文窗口，已经在吞吐量和效率方面展现了显着的进步——这只是这种创新混合架构的开始。值得注意的是，Jamba 在各种基准测试中都优于或匹配同尺寸级别的其他最先进型号。  ​ &lt; !-- SC_ON --&gt;  由   提交 /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bqfibp/p_jamba_the_first_productiongrade_mambabased/</guid>
      <pubDate>Fri, 29 Mar 2024 03:39:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] LoRA官方实现代码中转置的目的是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq73qh/d_whats_the_purpose_of_the_transpose_in_official/</link>
      <description><![CDATA[刚刚浏览了他们的官方实现代码并对此感到好奇。例如，在他们的 Embedding 模块中，他们声明并使用了这样的 lora 参数：  self.lora_A = nn.Parameter(self.weight.new_zeros((r, num_embeddings))) self.lora_B = nn.Parameter(self.weight.new_zeros((embedding_dim, r) )) ... self.weight.data -= (self.lora_B @ self.lora_A).transpose(0, 1) * self.scaling ... after_A = F.embedding( x, self.lora_A.transpose(0 , 1), self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse ) 结果 += (after_A @ self.lora_B.transpose(0, 1)) * self.scaling ...  那么，为什么他们不直接这样声明并在不转置的情况下使用呢？ self.lora_A = nn.Parameter(self.weight.new_zeros(( r, embedding_dim))) self.lora_B = nn.Parameter(self.weight.new_zeros((num_embeddings, r)))  这些转置的目的是什么？ （官方代码链接）   由   提交/u/kessa231  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq73qh/d_whats_the_purpose_of_the_transpose_in_official/</guid>
      <pubDate>Thu, 28 Mar 2024 21:21:44 GMT</pubDate>
    </item>
    <item>
      <title>自适应 RAG：一种降低 top-k 向量索引检索的 LLM 令牌成本的检索技术 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq3hwb/adaptive_rag_a_retrieval_technique_to_reduce_llm/</link>
      <description><![CDATA[摘要：我们演示了一种技术，该技术允许使用法学硕士的反馈动态调整 top-k 检索器 RAG 提示中的文档数量。这使得 RAG LLM 问答的成本降低了 4 倍，同时保持了相同的准确性水平。我们还表明该方法有助于解释法学硕士输出的血统。参考实现适用于大多数模型（GPT4、许多本地模型、较旧的 GPT-3.5 Turbo），并且可以适应大多数公开 top-k 检索原语的矢量数据库。 博客论文：https://pathway.com/developers/showcases/adaptive-rag 参考实现：&lt; a href=&quot;https://github.com/pathwaycom/pathway/blob/main/python/pathway/xpacks/llm/question_answering.py&quot;&gt;https://github.com/pathwaycom/pathway/blob/main/python /pathway/xpacks/llm/question_answering.py   由   提交 /u/dxtros   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq3hwb/adaptive_rag_a_retrieval_technique_to_reduce_llm/</guid>
      <pubDate>Thu, 28 Mar 2024 18:55:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 斯坦福大学的 BioMedLM 论文报告的准确性与评估的准确性：没有意义</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bq1deb/d_stanfords_biomedlm_paper_reported_accuracy_vs/</link>
      <description><![CDATA[      斯坦福大学发布#BioMedLM，一种基于生物医学数据训练的 2.7B 参数语言模型。然而，结果似乎没有意义。 这里是在 MultiMedQA（MedMCQA、MedQA、MMLU、PubMed）上使用 LM Evaluation Harness 框架的评估报告。  https://preview.redd .it/vd21crtn14rc1.png?width=1442&amp;format=png&amp;auto=webp&amp;s=ee905e8277006e40c37b7e5b87003165bd0de4b5 https://preview.redd.it/6ot7mibo14rc1.png?width=1164&amp;format=png&amp;auto=webp&amp;s=5d76fcce909fb07d 5404e148b0cdc2fbc6dae43c ​   由   提交 /u/aadityaura   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bq1deb/d_stanfords_biomedlm_paper_reported_accuracy_vs/</guid>
      <pubDate>Thu, 28 Mar 2024 17:32:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年构建大型语言模型的小指南 – 75 分钟讲座</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bpzrm0/d_a_little_guide_to_building_large_language/</link>
      <description><![CDATA[我终于录制了两周前的讲座，因为人们一直向我索要视频。 所以在这里，我希望您会喜欢“2024 年构建大型语言模型的小指南”。 我试图使其简短而全面 - 重点关注对于培训优秀 LLM 至关重要但通常隐藏的概念在技​​术报告中。 在讲座中，我向学生介绍了培训良好绩效法学硕士的所有重要概念/工具/技术：- 查找、准备和评估网络规模数据- 理解模型并行性和高效培训- 微调/对齐模型 - 快速推理 当然有很多东西和细节缺失，我应该添加进去，不要犹豫告诉我你是最令人沮丧的遗漏，我将在以后的部分中添加它。特别是，我认为我将更多地关注如何很好地、广泛地过滤主题，也许还有更多实用的轶事和细节。 既然我记录了它，我一直在想这可能是一个主题的第 1 部分。由两部分组成的系列，其中包含第二个完整的实践视频，介绍如何使用我们最近在 HF 围绕 LLM 培训发布的一些库和配方来运行所有这些步骤（并且无论如何都可以轻松适应您的其他框架）：  用于所有网络规模数据准备的 datatrove：https://github.com/huggingface/datatrove  nanotron 用于轻量级 4D 并行法学硕士培训：https://github.com/huggingface/nanotron lighteval 用于训练中快速并行 LLM 评估：https://github.com/huggingface/lighteval  以下是在 Youtube 上观看讲座的链接：https://www. youtube.com/watch?v=2-SPH9hIKT8这是 Google 幻灯片的链接：https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit#slide=id.p 很高兴听到对此的反馈以及在第二部分中添加、更正、扩展的内容。   由   提交 /u/Thomjazz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bpzrm0/d_a_little_guide_to_building_large_language/</guid>
      <pubDate>Thu, 28 Mar 2024 16:26:57 GMT</pubDate>
    </item>
    <item>
      <title>幻觉的终结（对于那些能负担得起的人）？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bpxs1i/the_end_of_hallucination_for_those_who_can_afford/</link>
      <description><![CDATA[      DeepMind 刚刚发表了一篇关于事实检查文本的论文： &lt; a href=&quot;https://preview.redd.it/zsmv0a0293rc1.png?width=1028&amp;format=png&amp;auto=webp&amp;s=789c1c2f9b31aa734a7ebcf459df3ad06bd74285&quot;&gt;https://preview.redd.it/zsmv0a0293rc1.png?width =1028&amp;format=png&amp;auto=webp&amp;s=789c1c2f9b31aa734a7ebcf459df3ad06bd74285 该方法使用 GPT-3.5-Turbo，每个模型响应成本为 0.19 美元，比人类注释者更便宜，同时更比他们准确： https:/ /preview.redd.it/ob7bb3iv73rc1.png?width=1014&amp;format=png&amp;auto=webp&amp;s=e79bbcaa578b29772cb3b43ead508daff7288091 他们使用这种方法创建事实基准并比较一些流行的法学硕士。 论文和代码：https://arxiv.org/abs/2403.18802 编辑：关于帖子的标题：幻觉（在维基百科中）被定义为“由人工智能生成的响应，其中包含以事实形式呈现的虚假或误导性信息。”：您的代码不编译本身并不是一种幻觉。当你声称代码是完美的时，那是一种幻觉。    由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bpxs1i/the_end_of_hallucination_for_those_who_can_afford/</guid>
      <pubDate>Thu, 28 Mar 2024 15:04:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>