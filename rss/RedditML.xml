<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 13 Mar 2024 12:23:48 GMT</lastBuildDate>
    <item>
      <title>[D] 未来机器学习理论研究的数学课程建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdpldb/d_math_course_advice_for_future_machine_learning/</link>
      <description><![CDATA[大家好，我下学期将是一名大二学生，未来打算申请研究生院，重点研究机器学习理论。我想知道我应该学习什么样的数学课程才能实现这个目标。目前，我正在学习高级数学课程，例如实分析、高级线性代数和优化。希望您能给我一些建议，告诉我下学期选什么课程，是拓扑学，还是更多的分析课程，或者有什么其他建议。   由   提交/u/Impressive-Site-7462   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdpldb/d_math_course_advice_for_future_machine_learning/</guid>
      <pubDate>Wed, 13 Mar 2024 11:53:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在文本上微调 LLaVA 会降低多模式性能吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdpezl/d_could_finetuning_llava_on_text_reduce/</link>
      <description><![CDATA[大家好！我计划在 MATH 数据集上对 LLaVA 进行几个时期的微调，以及 Camel AI 的一些数据集（https://huggingface.co/camel-艾）。这是否会降低模型在多模态推理任务上的性能？如果会，降低多少？   由   提交 /u/New-Skin-5064   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdpezl/d_could_finetuning_llava_on_text_reduce/</guid>
      <pubDate>Wed, 13 Mar 2024 11:43:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我很困惑。我在 GPU 机器上训练了一个乱码模型，在 CPU 上训练了一个连贯模型，两者都运行相同的代码——这是怎么回事？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdpd0w/d_im_baffled_im_getting_gibberish_model_trained/</link>
      <description><![CDATA[我在两台机器上运行完全相同的代码。这是我从头开始训练的语言模型。 CPU 训练的模型创建连贯的单词（它是字符级模型），而 GPU 训练的模型生成乱码。两者对于下一个字符的预测具有相似的准确性。 我用“The”提示这两个模型。并查看了完成情况。 GPU 训练的模型完成情况：“ on nut\nik o o nu ko ge ede ed eet eg ed &quot; CPU 训练模型完成：&quot;士兵和奇怪的人说“sol”” 以前有人经历过这种情况吗？我很困惑。 注意：我已经在每台机器上多次创建和重新训练模型，并且得到了一致的行为。 注意：我从我的 CPU 提交并推送了更改机器到 github，然后将该存储库克隆到 GPU 机器上，因此它们应该运行相同的代码。   由   提交/u/lildaemon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdpd0w/d_im_baffled_im_getting_gibberish_model_trained/</guid>
      <pubDate>Wed, 13 Mar 2024 11:40:27 GMT</pubDate>
    </item>
    <item>
      <title>[R]镜子：一种多视角的自我反思方法，用于知识丰富的推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdojab/r_mirror_a_multipleperspective_selfreflection/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.14963 代码：https:// /github.com/hanqi-qi/Mirror 摘要：  虽然大型语言模型（LLM）有能力为了反复反思自己的产出，最近的研究观察了他们在无法获得外部资源的情况下与知识丰富的问题​​的斗争。除了法学硕士自我评估效率低下之外，我们还观察到法学硕士尽管收到了明确的负面反馈，但仍难以重新审视他们的预测。因此，我们提出了Mirror，一种用于知识丰富推理的多视角自我反思方法，以避免陷入特定的反思迭代。镜子使法学硕士能够从多角度线索进行反思，这是通过导航者和推理者之间的启发式交互来实现的。它通过鼓励（1）导航器生成的方向的多样性和（2）推理器生成的响应中策略性诱导的扰动之间的一致性，引导智能体走向多样化但看似可靠的推理轨迹，而无需访问地面事实。在五个推理数据集上的实验证明了 Mirror 相对于当代几种自我反思方法的优越性。此外，消融研究清楚地表明我们的策略缓解了上述挑战。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdojab/r_mirror_a_multipleperspective_selfreflection/</guid>
      <pubDate>Wed, 13 Mar 2024 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 内存学习：大型语言模型的声明性学习框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdoby9/r_inmemory_learning_a_declarative_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.02757 摘要：  探索智能体是否可以在不依赖环境的情况下与环境保持一致人类标记的数据提出了一个有趣的研究主题。从智能生物体中观察到的对齐过程中汲取灵感，其中陈述性记忆在总结过去的经验中发挥着关键作用，我们提出了一种新颖的学习框架。代理熟练地从过去的经验中提取见解，完善和更新现有笔记，以提高他们在环境中的表现。整个过程发生在记忆组件内，并通过自然语言实现，因此我们将这个框架描述为内存学习。我们还深入研究了旨在评估自我改进过程的基准的主要特征。通过系统的实验，我们证明了我们的框架的有效性，并提供了对该问题的见解。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdoby9/r_inmemory_learning_a_declarative_learning/</guid>
      <pubDate>Wed, 13 Mar 2024 10:39:16 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于本科生工作的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdo8j3/d_questions_on_bachelors_work/</link>
      <description><![CDATA[Redditors 大家好 我正在为我的神经网络学士论文寻找好的资源。有谁有我可以使用的好的资源吗？   由   提交/u/KingLu271  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdo8j3/d_questions_on_bachelors_work/</guid>
      <pubDate>Wed, 13 Mar 2024 10:32:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] Devin：自动化软件工程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdm6dr/d_devin_automating_software_engineering/</link>
      <description><![CDATA[这是一个非常有趣的演示。我知道这可能是“精挑细选”的。演示，但工具总是可以改进的。这种工具在未来几年会被广泛使用吗？您有兴趣尝试一下吗？ https://www.youtube.com/watch?v =fjHtjT7GO1c https://twitter.com/karpathy/status/1767598414945292695   由   提交/u/Luxray2005   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdm6dr/d_devin_automating_software_engineering/</guid>
      <pubDate>Wed, 13 Mar 2024 08:14:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在面板数据上运行逻辑回归等分类模型有哪些问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdlrzk/d_what_are_the_issues_with_running_classification/</link>
      <description><![CDATA[我有一个数据集，用于捕获每个季度的员工数据。有些功能每个季度都会发生变化，但有些功能不喜欢员工的性别。 我正在尝试运行一个二元分类，目标变量如果有磨损则为 1，如果无磨损则为 0。每个员工每季度有 1 条记录，直到他们辞职的季度为止。如果我们考虑 4 年的数据集，则在第 4 年最后一个季度离职的员工将有 16 行，其中只有一行 - 对应于第 4 年最后一个季度的最新行的目标变量为 1，所有其他 15 行都有目标变量为 0。  这样的数据集是否可以用于分类，其中当时间（即年季度）也作为特征添加时，每一行都被认为是独立的？这种想法有什么缺陷？如果我对此数据集运行逻辑回归会出现什么问题？   由   提交 /u/SriRamaJayam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdlrzk/d_what_are_the_issues_with_running_classification/</guid>
      <pubDate>Wed, 13 Mar 2024 07:45:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG生产中的痛点和问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdkhk3/d_rag_painpoints_and_problems_in_production/</link>
      <description><![CDATA[已经有很多文章阐述了 RAG 的痛点 https://arxiv.org/pdf/2401.05856.pdf 根据我自己的经验，实施 RAG 管道的一些困难领域是：  开发针对您独特的信息需求的强大且定制设计的分块方法，这是一项复杂而艰巨的挑战。 实施强大且定制的检索方法也是一项艰巨的任务。 管理并优化有效 RAG 的提示可能会变得更加困难。 处理底层矢量数据的定期更新和维护。  有时它让我思考 RAG 更好还是用重写代理方法会增加价值。您的想法和痛点是什么，请分享您的经验   由   提交 /u/Winter_Draw9039   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdkhk3/d_rag_painpoints_and_problems_in_production/</guid>
      <pubDate>Wed, 13 Mar 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在基准测试中比较两个模型的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdjb4m/d_what_is_the_best_way_to_do_a_comparison_between/</link>
      <description><![CDATA[这个问题是在比较两个机器学习模型的性能的背景下提出的。但它适用于许多场景。 在验证集/测试集/基准数据集上测量机器学习模型的性能是一个嘈杂的过程。根据一般使用的种子，我们得到不同的度量值。在比较两个机器学习模型时，这会带来一个问题，因为我们不仅仅有两个数字可供比较。 我过去所做的是使用许多种子运行模型并对指标值进行平均，然后使用它平均值来决定哪个模型更好。 但是，这没有考虑指标值的方差。所以最近我开始做的是这样的，“如果平均值的差异小于标准差的总和，那么模型是相似的，否则你可以根据平均值的差异做出上面的决定”。 对我来说，这似乎是一个粗暴的黑客攻击。我确信必须有既定的方法来对两个模型的实验结果进行比较。在我看来，这个问题似乎并非如此。 所以我的问题是，在机器学习的背景下比较两个模型的结果的推荐方法是什么？  &gt; PS：为了稍微形式化这个问题，我们可以将一个模型的度量视为随机变量 M1，将另一个模型的度量视为随机变量 M2。现在，如何在仅给出 RV 样本的情况下比较两个 RV？比较两个 RV 意味着什么？   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/1bdjb4m/d_what_is_the_best_way_to_do_a_comparison_ Between/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdjb4m/d_what_is_the_best_way_to_do_a_comparison_between/</guid>
      <pubDate>Wed, 13 Mar 2024 05:04:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有最先进的法学硕士在许多领域都在业余水平上犯了事实错误。这比专家级别更难训练吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdi4nr/d_all_state_of_the_art_llms_make_factual_mistakes/</link>
      <description><![CDATA[我最近读到了GPQA，专家 -生物学、物理和化学中非常困难的问题的水平基准。显然克劳德3非常擅长这些问题。  然而，当我向 Claude 3 和 GPT-4 询问有关我有“专门业余”领域的领域时，Claude 3 和 GPT-4 始终给出错误的信息。这些是我希望对该主题感兴趣的人在没有该领域知识的情况下会问的问题类型。通常，错误会在谈话的早期出现，如果我深入研究任何细节，错误就会增加。  例如摄影和相机设计。我问“为什么老照片中的人们不微笑？”并给出了几个可能的因素。进一步询问其中之一，即摄像机的物理限制，开始引入幻觉的细节，而询问这些幻觉的细节会给出更多看似事实的东西，而忽略了实际的事实，并引入了更多的不一致之处。语言也是如此——如果你对外语的语法或发音系统了解很多，你会发现要求对这些事情的解释往往会给你带来错误的信息。这些例子来自上周，包括 Claude 3 Opus 和 GPT-4。  如果您有自己感兴趣的领域，SOTA 法学硕士能否掌握所有详细信息？  为什么不更加强调提高此类会话知识的准确性？训练和测试是否简单得多？   由   提交/u/Axon350  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdi4nr/d_all_state_of_the_art_llms_make_factual_mistakes/</guid>
      <pubDate>Wed, 13 Mar 2024 04:01:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 窃取生产语言模型的一部分</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bdf11z/r_stealing_part_of_a_production_language_model/</link>
      <description><![CDATA[我们引入了第一个模型窃取攻击，该攻击从 OpenAI 的 ChatGPT 或 Google 的 PaLM-2 等黑盒生产语言模型中提取精确的、重要的信息。具体来说，在给定典型的 API 访问的情况下，我们的攻击恢复了变压器模型的嵌入投影层（直到对称性）。我们的攻击花费不到 20 美元，提取了 OpenAI 的 ada 和 Babbage 语言模型的整个投影矩阵。由此，我们首次确认这些黑盒模型的隐藏维度分别为 1024 和 2048。我们还恢复了 gpt-3.5-turbo 模型的精确隐藏维度大小，并估计恢复整个投影矩阵的查询成本低于 2,000 美元。我们以潜在的防御和缓解措施作为结论，并讨论了可能扩大我们的攻击范围的未来工作的影响。 论文：https://arxiv.org/pdf/2403.06634.pdf   由   提交 /u/AdamEgrate   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bdf11z/r_stealing_part_of_a_production_language_model/</guid>
      <pubDate>Wed, 13 Mar 2024 01:36:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 现实世界 RAG 的法学硕士幻觉检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bd9gxr/p_llm_hallucination_detection_for_realworld_rag/</link>
      <description><![CDATA[大家好，我是 Jonathan，LastMile AI 的机器学习工程师。我们正在构建工具来评估生产中的法学硕士输出——特别是针对 RAG 应用。现在，我们专注于幻觉检测：给定 RAG 查询期间检索到的数据，响应是否忠实于该数据，还是幻觉？  这通常是通过模型评分大规模完成的：本质上，你向另一个 LLM（例如 GPT-4）提供问题、数据和回答，然后你会得到一个幻觉判决。 &gt; 我们研究了一些开源工具，例如 Arize 的 Phoenix，发现您可以通过更小（专业）的模型获得很高的准确性。  我们正在扩展对 v0 模型的早期访问，并有兴趣与任何需要帮助评估其生产 RAG 系统的人合作。  抢先体验：https://cdn.forms- content.sg-form.com/e8b9b4d4-dda0-11ee-85ff-daa2c5cfc593  详细信息：https://blog.lastmileai.dev/harder-better-faster-stronger-llm-hallucination-detection-for- real-world-rag-part-i-949248f0ad94   由   提交 /u/InevitableSky2801   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bd9gxr/p_llm_hallucination_detection_for_realworld_rag/</guid>
      <pubDate>Tue, 12 Mar 2024 21:40:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习面试倦怠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bd5hh4/d_ml_interview_burnout/</link>
      <description><![CDATA[我觉得我因数据科学面试而精疲力尽。我在该领域担任数据科学家已有 5 年了。这个领域有很多技术性的东西。尤其是在2023年，关于如何优化LLM模型和向量DB的使用的新论文层出不穷。我花在面试准备上的时间越多，我用来获取新知识的时间就越少。我应该怎么做才能克服这种情况？非常感谢。 为什么我觉得面试准备没有什么用 在实际工作中，我们可以针对一个话题进行不同的准备来回忆所有的内容。在向其他同事展示想法之前，先记忆并正确组织概念。然而，是否可以在采访过程中立即调取所有信息呢？有些知识可以追溯到学校课本上，几十年来一直没有人接触过。有些问题涉及不太常见的设计模式。当我无法回答一个问题时，我会感到难过，不是因为我不知道，而是因为我确实无法在短时间内总结出来。这就像数据被存档到AWS S3冰川一样，因此数据检索既耗时又昂贵。另外，不能回答一些代码设计模式并不意味着我不能写出好的代码并解决问题。为了准备这些面试，我尝试重新审视一些关键概念和各种不太有用的代码模式，但这非常耗时。老实说，这些工作的工资确实很高。我不是在谈论任何大型科技公司，而是在谈论一些中小企业。我对标准感到困惑。   由   提交 /u/MillionLiar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bd5hh4/d_ml_interview_burnout/</guid>
      <pubDate>Tue, 12 Mar 2024 19:03:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>