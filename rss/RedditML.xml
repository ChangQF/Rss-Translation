<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 08 Jul 2024 18:19:35 GMT</lastBuildDate>
    <item>
      <title>[D] 从 ChatGPT、Claude、Google Gemini 等中提取代码和文件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dycbqh/d_extracting_code_and_files_from_chatgpt_claude/</link>
      <description><![CDATA[因此，经过大量的迭代和思考，我想出了这个简单而美丽的东西：  javascript:(function(){async function main(){try{const e=document.body.innerText.trimStart(),t=function(e){const t=/[PROJECT_UPDATE]([\s\S]?)[/PROJECT_UPDATE]/g,n=/[ACTION:(create|modify|delete)]\s[PATH:(.?)](?:\s[CONTENT]([\s\S]*?)[/CONTENT])?/g,a=[];let r;for(;null!=(r=t.exec(e));){let e=r[1];let o;for(;null!=(o=n.exec(e));)a.push({action:o[1],path:o[2],content:o[3]||&quot;&quot;})}return a}(e);console.log(&quot;页面内容：&quot;,e),console.log(&quot;解析的更新：&quot;,t);if(0===t.length)throw new Error(&quot;页面内容中未找到更新&quot;);const a=await window.showDirectoryPicker();for(const e of t){if(&quot;create&quot;!==e.action&amp;&amp;&quot;modify&quot;!==e.action)continue;const n=e.path.split(&quot;/&quot;).slice(0,-1);let r=a;for(const e of n){r=await r.getDirectoryHandle(e,{create:!0})}const o=await r.getFileHandle(e.path.split(&quot;/&quot;).pop(),{create:!0}),i=await o.createWritable();await i.write(e.content),await i.close()}alert(&quot;项目文件已成功保存在所选目录中。&quot;)}catch(e){&quot;AbortError&quot;===e.name?console.log(&quot;用户取消了目录选择。&quot;):(console.error(&quot;Bookmarklet 错误：&quot;,e),alert(&quot;发生错误。请检查控制台了解详情。&quot;))}}main();})();  如果您将此提供给 LLM 并要求他们使用此格式输出文件，则可以使用此书签小程序捕获输出中的所有文件并将它们保存到您选择的文件夹中。 我很乐意看到这种东西内置到具有更复杂功能的库中。 我能够使用相同的机制让 Claude 能够通过如下命令语法在 Web 浏览器中执行 javascript 和 python（通过 pyIodide）... 您可能可以将命令语法调整为代码中的注释并让它正常编写代码但仍然捕获它...也许我会尝试一下。    提交人    /u/f0urtyfive   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dycbqh/d_extracting_code_and_files_from_chatgpt_claude/</guid>
      <pubDate>Mon, 08 Jul 2024 15:53:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在康威生命游戏中训练一个简单的 Transformer 神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dybuek/p_training_a_simple_transformer_neural_net_on/</link>
      <description><![CDATA[此练习展示了最简单的变压器形式，而康威生命游戏是其简单的数据来源。有趣的是，它学习计算基本上是 3×3 平均池的东西（尽管不包括中间内核）。 博客文章：https://sidsite.com/posts/life-transformer/ 代码：https://github.com/sradc/training-a-simple-transformer-on-conways-game-of-life    提交人    /u/montebicyclelo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dybuek/p_training_a_simple_transformer_neural_net_on/</guid>
      <pubDate>Mon, 08 Jul 2024 15:32:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] GraphRAG 是什么？解释一下</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dy5tge/r_what_is_graphrag_explained/</link>
      <description><![CDATA[本教程解释了什么是 GraphRAG，它是基线 RAG 的进步，它使用知识图谱而不是向量数据库进行检索，从而提高了输出质量。 https://youtu.be/14poVuga4Qw?si=y9Hxfy7NXZuN2XZI    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dy5tge/r_what_is_graphrag_explained/</guid>
      <pubDate>Mon, 08 Jul 2024 10:46:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 实时深度学习物体检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dy3wsc/p_real_time_deep_learning_object_detection/</link>
      <description><![CDATA[您好，Maschine 学习社区， 我目前正在使用 HoloLens2 开展一个关于增强组装的项目。我的任务是建立一个可以在 HoloLens 场景中实时检测物体的模型。一个重要的前提是，该模型应该能够预先使用尽可能少的知识来检测物体。所以我不想要一个需要带有高质量注释图像的大数据集来训练的模型。我希望该模型能够快速适应新的组装流程，而无需创建大数据集。我已经阅读了很多关于使用 Yolo 或 RCNN 进行少量镜头物体检测的文章，但我想听听你们是否有关于这项任务的经验或建议。 问候    提交人    /u/xelmaster420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dy3wsc/p_real_time_deep_learning_object_detection/</guid>
      <pubDate>Mon, 08 Jul 2024 08:38:20 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 神经解码 - 将歌曲聆听的脑电图数据映射到相应的音频文件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</link>
      <description><![CDATA[大家好， 我有一个数据集，其中包含参与者听一组 10 首歌曲的预处理 EEG 数据（脑电图时间序列数据）。我的目标是构建一个回归模型，将 EEG 数据作为输入映射回原始音频文件作为输出，以尝试重建参与者正在听的歌曲。这是一个利用 CNN 执行此操作的示例：https://arxiv.org/abs/2207.13845 以下是我遇到的一些障碍：  在链接的论文中，EEG 数据和目标音频文件被切成 1 秒长的片段，并在这些片段上训练 CNN。一个可能的问题是音频和 EEG 数据之间的延迟 - 大脑对刺激做出反应需要不可忽略的时间（大约 100 毫秒）。这会使 1 秒分段成为一种有问题的方法吗？我是否应该在模型中明确考虑这种延迟？或者模型应该在训练中“自行”考虑这个问题？是否有可以避免将数据划分为任意段的模型？ 我遇到的部分困难是我缺乏处理时间序列数据的经验，不知道哪些模型合适。CNN 是捕获短期和长期时间依赖性的理想选择吗？或者基于循环网络或变压器的架构是否更合适？我的直觉（可能完全错误）告诉我 RNN 更适合捕获短期依赖性，而变压器更适合捕获整个输入的依赖性。 降维会在这里有所帮助吗？输入数据是 64 通道 EEG 数据，采样率为 1024 Hz，平均歌曲长度约为 200 秒。这意味着十个输入数据中的每一个的大小约为 ~64 x 200K。我正在考虑使用 ICA，因为 EEG 数据可能很棘手且嘈杂（电极在微伏级上敏感），并且 ICA 通常与 EEG 数据一起使用以消除伪影。  任何建议或想法都将不胜感激。另外，我应该指出，我这样做既是为了研究也是为了学习目的 - 我目前不担心可扩展性/效率，我愿意使用现有框架或从头开始开发模型。 提前谢谢您。    提交人    /u/dusmansen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</guid>
      <pubDate>Mon, 08 Jul 2024 01:49:47 GMT</pubDate>
    </item>
    <item>
      <title>[项目] minigrad - andrej karpathy 用 Go 语言实现的 micrograd</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</link>
      <description><![CDATA[这个周末，我想学习更多关于 ML 的知识并继续从事我的 ML 项目。所以我用 golang（重新）构建了 karpathy 的 micrograd。也学习了更多关于反向传播的知识。很想知道您的反馈。请做出贡献并在 GitHub repo 上留下一颗星。 github - https://github.com/0verread/minigrad    提交人    /u/ElegantGoose9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</guid>
      <pubDate>Mon, 08 Jul 2024 00:51:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] Open-TeleVision：具有沉浸式主动视觉反馈的远程操作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</link>
      <description><![CDATA[        提交人    /u/XiaolongWang   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</guid>
      <pubDate>Sun, 07 Jul 2024 23:09:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你们都用什么进行大规模训练？普通的 pytorch 还是使用像 HF Accelerate 这样的库。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</link>
      <description><![CDATA[我很快就要为研究论文训练一个大型集群多机器。好奇你们都为大规模训练做了什么，是坚持我所知道的 pytorch（FSDP、DDP、TP、MP 等...）和 slurm 更好，还是值得学习像 HF accelerate 这样的东西进行大规模训练？    提交人    /u/I_will_delete_myself   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</guid>
      <pubDate>Sun, 07 Jul 2024 22:46:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关系数据中的 AI/ML 应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxm0hz/d_aiml_applications_in_relational_data/</link>
      <description><![CDATA[我有一个包含大量表和列的数据库，并且它们之间的关系是已知的。 根据对数据模型的理解，我们实施了多项基于规则的数据质量检测。 但是，使用机器学习可以构建哪些可能的用例？关系数据中的可能应用程序有哪些。 或者，如果事先知道逻辑和规则，基于规则的检查是否是最好的。 我想学习和阅读有关关系数据中 AI/ML 用例的更多信息。如果有人可以指出有关用例以及如何实现用例的正确文章/方向，那将会很有帮助。 谢谢    提交人    /u/rekonist-app   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxm0hz/d_aiml_applications_in_relational_data/</guid>
      <pubDate>Sun, 07 Jul 2024 17:34:50 GMT</pubDate>
    </item>
    <item>
      <title>[N] PyTorch 官方纪录片回顾了它的过去和未来</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxlqg2/n_official_pytorch_documentary_revisits_its_past/</link>
      <description><![CDATA[        由    /u/gadgetygirl 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxlqg2/n_official_pytorch_documentary_revisits_its_past/</guid>
      <pubDate>Sun, 07 Jul 2024 17:23:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如果提供微调 API，则可以通用地越狱 LLM 的安全输入和输出</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxiqhh/r_a_universal_way_to_jailbreak_llms_safety_inputs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxiqhh/r_a_universal_way_to_jailbreak_llms_safety_inputs/</guid>
      <pubDate>Sun, 07 Jul 2024 15:11:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReproModel：开源 ML 研究工具箱更新！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxgt56/p_repromodel_open_source_ml_research_toolbox/</link>
      <description><![CDATA[大家好，我很高兴与大家分享 ReproModel 的最新动态，这是一个开源工具箱，旨在简化机器学习模型的测试和复制。 我和你们中的许多人一样，在对模型进行基准测试和比较方面遇到了很多困难，从缺少代码到不透明的实验参数减慢了进程。我决定自己动手，在我的工作场所创建了一个迷你工具箱来简化这个过程。 目标是减少复制实验所花费的时间和精力，使研究人员能够专注于创新而不是设置。 我知道这项任务并不容易，不久前我联系了社区，并获得了巨大的帮助。在团队的努力下，我们现在已经在已经实现的功能中添加了代码提取器、AI 实验描述生成器和自定义脚本编辑器。 该项目是开源的，如果您喜欢我们正在构建的内容，欢迎与我们分享您的想法、做出贡献或留下一颗星 :) 您可以在此处找到存储库：https://github.com/ReproModel/repromodel 感谢您的时间，请随时在下面留下任何意见或建议！    提交人    /u/MintOwlTech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxgt56/p_repromodel_open_source_ml_research_toolbox/</guid>
      <pubDate>Sun, 07 Jul 2024 13:42:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在深度模型上进行“深度工作”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</link>
      <description><![CDATA[大家好， 我最大的生产力挑战之一是等待深度学习训练循环、标记化或处理循环运行时的停机时间。对于较短的循环，这些循环可能需要 5 分钟到一个小时的时间，在此期间，我常常发现自己不知道该做什么。 开始一项新任务很困难，因为不断的上下文切换会打乱我的工作流程和注意力。 我以前在大学里遵循深度工作方法，这确实有助于控制我的注意力缺陷多动障碍。我白天不使用手机或社交媒体，一次只“专注于”一项任务。 现在，我觉得这几乎是不可能的。我“被迫”休息这些小憩，不断在任务之间切换，这非常具有挑战性。 您对如何充分利用这些间隔有什么建议吗？你会为这些时间段保留特定任务吗？ 即使从专注编码切换到阅读论文，如果只花 10 分钟左右的时间，也会非常困难。 有人解决过这些问题吗，还是只有我？ 谢谢。    提交人    /u/Magnospm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</guid>
      <pubDate>Sun, 07 Jul 2024 13:02:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于 Mamba 的语言模型实证研究（8B Mamba-2-Hybrid 在 3.5T token 数据上）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx9ggp/r_an_empirical_study_of_mambabased_language/</link>
      <description><![CDATA[链接：http://arxiv.org/abs/2406.07887  选择性状态空间模型 (SSM)（如 Mamba）克服了 Transformers 的一些缺点，例如序列长度的二次计算复杂度和键值缓存的大量推理时间内存要求。此外，最近的研究表明，SSM 可以匹配或超越 Transformers 的语言建模能力，使其成为一种有吸引力的替代方案。然而，在受控设置（例如相同的数据）中，迄今为止的研究仅展示了将 SSM 与 Transformers 进行比较的小规模实验。为了了解这些架构在更大规模上的优势和劣势，我们直接比较了在多达 3.5T 个 token 的相同数据集上训练的 8B 参数 Mamba、Mamba-2 和 Transformer 模型。我们还将这些模型与由 43% Mamba-2、7% 注意力和 50% MLP 层 (Mamba-2-Hybrid) 组成的混合架构进行了比较。使用一组不同的任务，我们回答了 Mamba 模型是否可以在更大的训练预算下与 Transformers 匹敌的问题。我们的结果表明，虽然纯 SSM 在许多任务上与 Transformers 匹敌或超过 Transformers，但它们在需要强大复制或上下文学习能力（例如 5-shot MMLU、电话簿）或长上下文推理的任务上落后于 Transformers。相比之下，我们发现 8B Mamba-2-Hybrid 在我们评估的所有 12 个标准任务上都超过了 8B Transformer（平均 +2.65 分），并且预计在推理时生成 token 时速度最高可提高 8 倍。为了验证长上下文能力，我们提供了额外的实验，评估 Mamba-2-Hybrid 和 Transformer 的变体，以支持 16K、32K 和 128K 序列。在另外 23 个长上下文任务中，混合模型继续接近或平均超过 Transformer。为了进一步研究，我们发布了检查点以及用于训练我们模型的代码，作为 NVIDIA Megatron-LM 项目的一部分。     submitted by    /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx9ggp/r_an_empirical_study_of_mambabased_language/</guid>
      <pubDate>Sun, 07 Jul 2024 05:52:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 07 Jul 2024 02:15:10 GMT</pubDate>
    </item>
    </channel>
</rss>