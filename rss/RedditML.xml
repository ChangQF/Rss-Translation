<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 11 Dec 2024 18:25:08 GMT</lastBuildDate>
    <item>
      <title>[D] 回到 NLP 研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbyekh/d_getting_back_to_nlp_research/</link>
      <description><![CDATA[我被安排到 2020 年，在 FAANG 研究实验室工作，但休了一段长假来探索一个不相关的领域。我应该阅读哪些关键论文来了解 GenAI/LLM 的 SOTA 和未来方向？    提交人    /u/priofind   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbyekh/d_getting_back_to_nlp_research/</guid>
      <pubDate>Wed, 11 Dec 2024 17:10:43 GMT</pubDate>
    </item>
    <item>
      <title>人工智能在放射学中的应用，观点 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbx46m/ai_in_radiology_views_d/</link>
      <description><![CDATA[这篇文章是对今天早些时候某人发表的一篇文章的引用，我认为需要进行一些讨论。 https://www.reddit.com/r/MachineLearning/s/6fMM3ht5Nz 我在医学成像方面有一定经验，主要是放射学。我开始认为，在未来几年，人工智能应该取代医疗专业人员，而我应该站在这场革命的最前沿。 为什么放射学需要人工智能？因为我们迫切需要客观意见，而不是人类专家提供的主观意见。我见过许多数据集（大部分是内部的），其中多个审阅者提供的注释和诊断相互矛盾。 到目前为止，我已经研究过胸部 X 光片、牙科扫描和脊柱扫描（颈椎），对诊断的客观意见的研究已经进行了十多年。老实说，与其他领域（例如生成式 AI、文本转 X 等）相比，这些年来我没有看到这个子领域有太大的进步。虽然我们确实获得了新的基准，但它们实际上都没有用于医院的诊断（至少在牙科和外科领域）。 可能的原因可能是缺乏正确注释的数据、缺乏基础模型，或者人们不再像几年前那样关心这方面的问题。 在我看来，“AI 会取代放射科医生吗？”这个问题的答案答案是否定的——在不久的将来不会。放射科医生比以往任何时候都更需要帮助人工智能在这个领域取得进步。此外，即使人工智能确实取得了重大进展，它也将协助医学专家，而不是取代他们。另一方面，患者可能不愿意仅依靠人工智能来做出诊断。 希望听到更多专家的意见！    提交人    /u/ade17_in   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbx46m/ai_in_radiology_views_d/</guid>
      <pubDate>Wed, 11 Dec 2024 16:17:22 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到 Llama 3 初始化 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/</link>
      <description><![CDATA[标题基本上说明了一切，我想要一个好的变压器基线，我想初始化可能很重要。我可以找到 llama 3 模型，但我可以找到它们如何初始化参数。有人知道我在哪里可以找到这个吗？    提交人    /u/idkwhatever1337   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/</guid>
      <pubDate>Wed, 11 Dec 2024 14:14:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 连续潜在空间推理：通过连续思维链提高 LLM 性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/</link>
      <description><![CDATA[本文引入了COCONUT（连续思维链），将语言模型推理从离散的token空间转化为连续的潜在空间，其核心思想是将推理步骤编码为连续向量而不是文本token，从而实现更灵活、更精确的中间计算。 主要技术点： • 将文本↔连续向量映射的编码器-解码器架构 • 对潜在向量进行操作的新型连续推理模块 • 在连续空间中并行处理推理步骤 • 推理过程中基于梯度的优化 • 结合重建和推理目标的特殊损失函数 主要结果： • 与传统方法相比，推理基准提高了20% • 减少了解决复杂问题所需的计算步骤 • 在不同推理任务中获得更一致的性能 • 更好地处理数学和逻辑推理 • 增强了维持连贯推理链的能力 我认为这种方法可以有意义地推进语言模型处理复杂推理任务的方式。通过超越离散标记，模型可以更好地捕捉类似人类推理的连续性。在推理过程中在连续空间中进行优化的能力对于提高可靠性特别有希望。 我认为主要的挑战是将其扩展到非常大的模型，同时管理计算成本。离散空间和连续空间之间的转换增加了需要解决的开销。 TLDR：新方法将语言模型推理转换为连续向量空间而不是离散标记，通过更灵活的计算在推理任务上显示出 20% 的更好性能。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/</guid>
      <pubDate>Wed, 11 Dec 2024 13:39:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我是否应该因为 AI 而避免从事放射科住院医师职位？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbtc6f/d_should_i_avoid_pursuing_a_radiology_residency/</link>
      <description><![CDATA[我是一名来自印度的医学生，想知道放射科医生在未来是否会变得不那么有价值。我目前正在等待被选入住院医师培训项目。在印度，放射科医生短缺，这就是为什么他们的薪水比其他研究生专业更高。人工智能需要多长时间才能取代放射科医生的工作？（我敢肯定，如果可以的话，医院会愿意减少放射科的工作人员）这会在未来 10-15 年内发生吗？    提交人    /u/Consistent-Key-1566   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbtc6f/d_should_i_avoid_pursuing_a_radiology_residency/</guid>
      <pubDate>Wed, 11 Dec 2024 13:22:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 掌握最先进的进化优化所需的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/</link>
      <description><![CDATA[有很多好书可以让你接近该领域的最新水平，特别是机器学习和深度学习。但是，有没有关于进化优化的好现代书籍？有没有好的课程？    提交人    /u/ArtisticHamster   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/</guid>
      <pubDate>Wed, 11 Dec 2024 13:17:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于 3D 环境中的物体检测和深度感知的视觉语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbs52u/d_visual_language_models_for_object_detection_and/</link>
      <description><![CDATA[我想在 3D 模拟引擎 (Unity) 中运行 VLM 进行对象检测和深度感知。考虑到准确性、速度和微调的简易性等因素，有哪些适合此用例的 vlm？ 用例示例： 在 Unity 中，我有一个包含 2 个房间的环境。设置了一个摄像头，用于捕捉场景的图像/视频源。VLM 应该在环境中找到特定对象（比如黑色瓶子），判断它在 3D 场景中的位置并为其生成坐标。 基本上，我想准确找出该对象在 Unity 环境中的位置。如何做到这一点？    提交人    /u/reso_ams   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbs52u/d_visual_language_models_for_object_detection_and/</guid>
      <pubDate>Wed, 11 Dec 2024 12:15:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 任何可用于多种场景的 3D 重建方法（即不使用就扔）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/</link>
      <description><![CDATA[NeRF 对于每个场景都是独一无二的，因此需要从头开始训练。高斯溅射也是场景所独有的。我理解场景很复杂，因此训练后几乎没有机会出现可以输出多个场景的神经网络。但是，是否仍然有一些场景表示在某种程度上没有被使用和完全抛弃？    提交人    /u/deathmaster2011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/</guid>
      <pubDate>Wed, 11 Dec 2024 11:27:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] 评估生成模型中隐含的世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbra2d/r_evaluating_the_world_model_implicit_in_a/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbra2d/r_evaluating_the_world_model_implicit_in_a/</guid>
      <pubDate>Wed, 11 Dec 2024 11:19:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 作者何时可以访问 ICLR 元评论？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/</link>
      <description><![CDATA[大家好， 这是我第一次向 ICLR 提交论文。ICLR 网站上说元评审将于今天（几个小时后）截止。作者可以在收到决定通知的同时还是在元评审截止日期后立即查看这些评审？ 谢谢！    提交人    /u/Glaze_anetha42   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/</guid>
      <pubDate>Wed, 11 Dec 2024 10:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Stella 嵌入模型比其他同等质量的模型小得多？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/</link>
      <description><![CDATA[在 MTEB 排行榜上，stella_en_v5 目前排名第三，而使用的内存仅为前 10 名中所有非 Stella 模型的五分之一。 stella_en_400M_v5 排名第十，而使用的内存比排名在其附近的模型少 15-20 倍。这似乎在基准测试的几个子任务中相对一致（针对英语）。 这里的秘诀是什么？或者说，陷阱是什么？目前还没有论文。有人知道详细信息吗？    提交人    /u/-p-e-w-   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/</guid>
      <pubDate>Wed, 11 Dec 2024 03:58:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从失业到 Lisp：在青少年的深度学习编译器上运行 GPT-2</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/</link>
      <description><![CDATA[几个月前，我失业了，不知道下一步该做什么。我想从系统的角度更多地了解深度学习。在学习了吴恩达的监督学习课程后，我渴望更多地了解像 Pytorch 或 Tinygrad 这样的深度学习框架（或深度学习编译器）。 我开始研究 Tinygrad，从我在网上找到的教程中学习，我发现它很有趣，因为它是一个真正的编译器，它采用传统的 Python 代码并将其转换为抽象语法树，然后将其解析为 UOps 和 ScheduleItems，最终拥有一个代码生成层。虽然设计很有趣，但代码很难读。 就在那时，我偶然发现了一些完全出乎意料的东西，一个基于 Common Lisp 构建的深度学习编译器，由一名 18 岁的日本年轻人在他的间隔年期间维护。而目前我们已经完成了一件伟大的事情，它可以运行 gpt2！ 目前，它只是生成 C 内核，但未来我们希望支持 cuda codegen 以及许多其他功能，并作为任何想要使用 Common Lisp 进行深度学习编译器工作的人的学习工具。 这是一个开源项目，欢迎任何人做出贡献！ https://github.com/hikettei/Caten 编辑：添加一个关于它如何工作的示例。 这是我在另一个论坛上写的一个例子： 你好！谢谢你的提问。 首先，Caten 中有三个抽象层：  caten/apis | 高级图形接口 2. caten/air |低级图形接口 3. caten/codegen | AIR Graph =&gt; 内核生成器  编译器的输入只是 Common Lisp 类（类似于 torch 模块）。例如，在 Common Lisp 中，我们可以创建一个执行 SinCos 的模块：  (defclass SinCos (Func) nil (:documentation &quot;The func SinCos computes sin(cos(x))&quot;)) ;; Forward 为下一次计算创建一个惰性张量。 ;; 您可以使用 `st` 宏跳过此过程。 (defmethod forward ((op SinCos) &amp;rest tensors) (st &quot;A[~] -&gt; A[~]&quot; (tensors))) ;; Backward 是可选的（这次跳过）（defmethod behind ((op SinCos) &amp;optional prev-grad) (declare (ignore prev-grad)) nil）；； Lower 描述了 `SinCos` 的降低表达式 (defmethod lower ((op SinCos) &amp;rest input) (let ((x (car input))) (with-context (a (%sin (%add x (%fconst (/ pi 2))))) (b (%sin a)))))  `apis` 层是高级接口，而 `lower` 方法是代码生成之前的低级步骤。 接下来，框架生成一个抽象 VM (AVM) 表示：  #S(AVM :GRAPH Graph[seen=NIL, output=(STC6466_1)] { &lt;ALLOCATE : TID6464 &lt;- (shape=(1), stride=(1)) where :dtype=FLOAT32&gt; &lt;Node[BUFFER] ALLOCATE(NID6480) : SID6479* &lt;- ()&gt; &lt;Node[BINARYOPS] ADD(NID6484) : BID6483* &lt;- (TID6464, LID6481)&gt; &lt;Node[UNARYOPS] SIN(NID6486) : UID6485* &lt;- (BID6483)&gt; &lt;Node[UNARYOPS] SIN(NID6488) : UID6487* &lt;- (UID6485)&gt; &lt;Node[SPECIAL/VM] PAUSE/BACKWARD(NID6501) : STC6466_1* &lt;- (UID6487)&gt; })  然后，将计算图转化为调度项目：  FastGraph[outputs=(val_6)] { { Allocate } : [ val_0 &lt;- (1) ] { KERNEL } : [ val_5 &lt;- val_1, val_0 :name=FUSED_SIN_SIN_ADD_LOAD6511] }  最后，代码生成步骤生成以下 C 代码：  void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0); void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0) { val_5[0] = sin(sin((val_0[0] + 1.5707964))); }  此 C 代码由 C 编译器编译并执行。 因此，回答您的问题：编译器采用 Common Lisp 代码并生成 C 函数。    提交人    /u/yCuboy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/</guid>
      <pubDate>Tue, 10 Dec 2024 18:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 这个数据集到底有多难？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/</link>
      <description><![CDATA[新论文提醒！ 分类自动编码器测量分类难度并检测标签错误 我们倾向于认为训练分类器的挑战是由超参数调整或模型创新来处理的，但数据及其嵌入中存在丰富的固有信号。了解机器学习问题的难度一直很难。现在不再如此。 现在，您可以计算分类数据集的难度，而无需训练分类器，并且每个类别只需要 100 个标签。而且，这个难度估计与数据集大小出奇地无关。 传统上，数据集难度评估方法耗时和/或计算密集型，通常需要训练一个或多个大型下游模型。更重要的是，如果你在数据集上训练具有特定架构的模型并实现特定的准确度，则无法确定你的架构是否完全适合手头的任务 - 可能是一组不同的归纳偏差会导致模型更轻松地学习数据中的模式。 我们的方法为每个类训练一个轻量级自动编码器，并使用重建误差的比率来估计分类难度。在 100k 样本数据集上运行此数据集难度估计方法只需几分钟，并且不需要调整或自定义处理即可在新数据集上运行！ 效果如何？我们对 19 个常见的视觉数据集进行了系统研究，将我们的方法估计的难度与 SOTA 分类准确度进行了比较。除了一个异常值外，相关性为 0.78。它甚至适用于医疗数据集！ 论文链接：https://arxiv.org/abs/2412.02596 GitHub Repo Linked in Arxiv pdf    提交人    /u/ProfJasonCorso   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/</guid>
      <pubDate>Tue, 10 Dec 2024 16:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 08 Dec 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>