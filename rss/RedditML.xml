<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 20 Jan 2024 06:16:41 GMT</lastBuildDate>
    <item>
      <title>[P] 有没有针对段/数组的无监督分类算法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b4yjd/p_any_unsupervised_classification_algorithms_for/</link>
      <description><![CDATA[我需要对固定长度 M 的 N 个时间序列段进行编目，并想知道是否有任何指针可以查找数据中的异常情况。大多数片段都是随机噪声信号，其中一些感兴趣的片段偏离随机噪声。   由   提交 /u/ch4m4njheenga   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b4yjd/p_any_unsupervised_classification_algorithms_for/</guid>
      <pubDate>Sat, 20 Jan 2024 06:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于机器学习中梯度下降与局部最大值和最小值的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b4hqt/d_question_about_gradient_descent_in_machine/</link>
      <description><![CDATA[嗨，我是一名学习机器/深度学习的高中生，我最近在数学中了解到，我们可以通过以下方式找到函数的局部最小值取一阶导数和二阶导数来找到其临界点，然后找到函数的最低值。  为什么我们不能直接求损失函数的最小值，而使用梯度下降呢？这看起来效率更高，因为我们不需要进行一系列小的调整来找到最小值 - 我们可以直接计算它 这行得通吗？这听起来有点愚蠢，因为人们显然会要求这样做   由   提交/u/Mucky5739  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b4hqt/d_question_about_gradient_descent_in_machine/</guid>
      <pubDate>Sat, 20 Jan 2024 05:35:45 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 Duckling 中的 etc/zoneinfo 的帮助 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b4eqt/need_help_regarding_etczoneinfo_in_duckling_d/</link>
      <description><![CDATA[       由   提交/u/Least_Race1   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b4eqt/need_help_regarding_etczoneinfo_in_duckling_d/</guid>
      <pubDate>Sat, 20 Jan 2024 05:30:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有可以检测和清除言语障碍的软件？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b2o00/d_is_there_any_software_that_can_detect_and_clean/</link>
      <description><![CDATA[我想创建一个 YouTube 频道，但我有与言语相关的障碍，包括口吃、语速过快和软“r”。 是否有任何软件可以检测所有这些并将其清理，以便我可以将其用于配音？ 我尝试查找生成的 AI 语音以使用其中之一，但那让我陷入了控制台命令的海洋、可以说是昂贵的订阅服务和/或有问题的商业许可政策中。   由   提交 /u/Distinct-Temp6557    reddit.com/r/MachineLearning/comments/19b2o00/d_is_there_any_software_that_can_detect_and_clean/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b2o00/d_is_there_any_software_that_can_detect_and_clean/</guid>
      <pubDate>Sat, 20 Jan 2024 03:52:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] [D] 开启训练之旅：基于Ray和vLLM构建70B+模型的开源RLHF全方位训练框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19b01uc/p_d_starting_the_training_journey_an_opensource/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19b01uc/p_d_starting_the_training_journey_an_opensource/</guid>
      <pubDate>Sat, 20 Jan 2024 01:39:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 剩下的一切，说服我错了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19axat7/d_residual_everything_convince_me_wrong/</link>
      <description><![CDATA[直接更改功能不是一个好主意。这会破坏信息，并在恢复时导致与“均值回归”相关的可怕问题。 您可以将每个剩余层想象为一个“工人”。在一个组织内。团队被组织成多个区块，每个团队成员都努力合作，为下一个团队（下一个区块）准备可交付成果。然后，每个团队协同工作以填补他们的角色（添加高频、低频等），并将工作汇总到最后一层。单层（老板）签署所有工作并将其传递给客户端（输出） 如果你仔细想想，每一层只需要输出值“以增强 x” ;，与“合成一个新的x”相反。 我使用“公司”的原因是这里的参考是为了表明原始输入从未被实际破坏（需求，来自另一个团队的交付）。例如，我们不想破坏 SOW 或 TDD 的页面并编写我们自己的信息。网络中的各层需要进行相同的操作，只为特征提供新信息，而不破坏以前的信息。想到可以通过多种方式聚合所有这些信息而不是简单的残差相加，这是令人兴奋的。根据我的经验，乘法残差很有趣，它收敛速度更快，但也占用更多内存。这种方法的整个瓶颈似乎是内存，因为自动编码器通常会对其特征进行下采样并重新采样，而不是保持维度相同或扩展它。很想听听想法。   由   提交 /u/WisePalpitation4831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19axat7/d_residual_everything_convince_me_wrong/</guid>
      <pubDate>Fri, 19 Jan 2024 23:34:27 GMT</pubDate>
    </item>
    <item>
      <title>物理信息机器学习应用[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19av05k/physicsinformed_machine_learning_applications_d/</link>
      <description><![CDATA[大家好， 我渴望了解人们正在使用或想要使用物理信息机器学习的应用程序（ PIML) 为。我正在开发一个用于构建和运行 PIML 的新平台，以帮助人们加速和扩大他们的物理模拟。我一直在与一些公司/大学团体合作研究用于电路设计的 PIML，但我很好奇人们还考虑使用它们做什么以及他们遇到了什么问题。例如，您是否使用 PIML 进行气流建模，甚至构建视频游戏引擎？ 谢谢！   由   提交/u/piml-guy  /u/piml-guy  reddit.com/r/MachineLearning/comments/19av05k/physicalsinformed_machine_learning_applications_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19av05k/physicsinformed_machine_learning_applications_d/</guid>
      <pubDate>Fri, 19 Jan 2024 21:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自我奖励语言模型 - Meta 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2401.10020  Github：https://github.com/lucidrains/self-rewarding -lm-pytorch 摘要：  我们假设，为了实现超人智能体，未来的模型需要超人反馈才能提供足够的训练信号。目前的方法通常根据人类偏好来训练奖励模型，这可能会受到人类表现水平的瓶颈，其次这些单独的冻结奖励模型无法在 LLM 训练期间学习改进。在这项工作中，我们研究自我奖励语言模型，其中语言模型本身通过法学硕士作为法官来使用，提示在训练期间提供自己的奖励。我们表明，在迭代 DPO 培训期间，不仅提高了指令遵循能力，而且还提高了为自身提供高质量奖励的能力。在我们的方法的三个迭代中对 Llama 2 70B 进行微调，产生的模型优于 AlpacaEval 2.0 排行榜上的许多现有系统，包括 Claude 2、Gemini Pro 和 GPT-4 0613。虽然只是初步研究，但这项工作打开了大门模型可以在两个轴上不断改进的可能性。   https:// /preview.redd.it/l7vav40qngdc1.jpg?width=1344&amp;format=pjpg&amp;auto=webp&amp;s=9dce97a69f2ede66d6dabf6abbcfc75bf0e94f19 https://preview.redd.it/fuooe70qngdc1.jpg?width=1180&amp;format=pjpg&amp;auto=webp&amp;s =a88fcf1c765ff42c18091889f5b14cd371248760   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/</guid>
      <pubDate>Fri, 19 Jan 2024 21:01:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习中的不确定性来源——统计学家的观点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aml6l/r_sources_of_uncertainty_in_machine_learning_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.16703 摘要：  机器学习和深度学习今天已经达到了令人印象深刻的标准，使得我们可以回答几年前难以想象的问题。除了这些成功之外，很明显，除了纯粹的预测（大多数监督机器学习算法的主要优势）之外，不确定性的量化也是相关且必要的。虽然近年来出现了这个方向的第一个概念和想法，但本文采用概念视角并研究了不确定性的可能来源。通过采用统计学家的观点，我们讨论了任意和认知不确定性的概念，这些概念更常见于机器学习。本文旨在将两种类型的不确定性形式化，并证明不确定性的来源是多种多样的，并且并不总是可以分解为任意的和认知的。我们将机器学习中的统计概念与不确定性进行比较，还展示了数据的作用及其对不确定性的影响。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aml6l/r_sources_of_uncertainty_in_machine_learning_a/</guid>
      <pubDate>Fri, 19 Jan 2024 16:09:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] GPU 服务器库存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19allss/d_p_stockpile_of_gpu_servers/</link>
      <description><![CDATA[我有 22 台 8 GPU 服务器，配备 AMD Mi50（请参阅下面有关 Mi50 的注释）。我已经能够让 PyTorch 在这些 GPU 上运行，并且能够对不同的大型语言模型进行推理。我原本想使用这些 GPU 来为 LLM 提供服务，但 VLLM cuda 内核不能在 Mi50 上开箱即用，而且 Llama CPP 有一个错误，它一次最多只能支持 4 个 AMD GPU。 所以，TLDR，我不想让这些服务器闲置，如果有人对服务器有任何有用的创意，我很乐意授予他们 SSH 访问权限。 Mi50 规格：  - 16GB VRAM - 1TB/s VRAM 带宽 - 25 TFLOPs  &amp; #32；由   提交 /u/TheRealBracketMaster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19allss/d_p_stockpile_of_gpu_servers/</guid>
      <pubDate>Fri, 19 Jan 2024 15:27:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] AISTATS 2024论文接收结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19al3e9/d_aistats_2024_paper_acceptance_result/</link>
      <description><![CDATA[AISTATS 2024 论文接受结果将于今天发布。正在为今年的结果创建讨论主题。   由   提交/u/zy415  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19al3e9/d_aistats_2024_paper_acceptance_result/</guid>
      <pubDate>Fri, 19 Jan 2024 15:03:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自我奖励语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19akxwp/r_selfrewarding_language_models/</link>
      <description><![CDATA[ 由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19akxwp/r_selfrewarding_language_models/</guid>
      <pubDate>Fri, 19 Jan 2024 14:57:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] Creatures 1996，一款利用机器学习的早期人工生命模拟游戏。想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ak2w9/d_creatures_1996_an_early_artificial_life/</link>
      <description><![CDATA[我想先说一下，当我浏览 Reddit 时，我看到了这个游戏的描述： “这个游戏有当时其中有一些非常复杂的系统。它有一个化学系统，你的生物的免疫系统，他们的行为和个性，他们的DNA和繁殖系统，你必须通过对象-词和行为关联来教他们实际的语言和单词，你必须惩罚和奖励他们的行为正确地，否则他们会发展出适应不良的行为或变得暴力并杀死你的其他生物，如果你不做到这一点，他们也会变得沮丧，等等。事实上，游戏中甚至有一整套他们可以体验的情感系统，你必须尝试管理它，否则你的生物会变得孤立并对你反应迟钝。除此之外，还有暴力且患病的敌方生物种族，称为 grendels，它们在世界上漫游，可以杀死/骚扰你的生物。” 根据维基百科页面： “生物是一种人工生命模拟，用户可以孵化毛茸茸的小动物并教它们如何行为，或者让它们自己学习。这些“诺恩”可以说话，自己进食，并保护自己免受称为Grendels的邪恶生物的侵害。这是机器学习在交互式模拟中的第一个流行应用。生物利用神经网络来学习该做什么。该游戏被认为是人工生命研究的突破，旨在模拟生物与其环境互动的行为。” https://en.m.wikipedia.org/wiki/Creatures_(1996_video_game) 还有其他更高级的人工生命模拟游戏吗？这些看起来确实非常有趣，尤其是考虑到我们在机器学习方面取得了几十年的进步。   由   提交 /u/Username912773   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ak2w9/d_creatures_1996_an_early_artificial_life/</guid>
      <pubDate>Fri, 19 Jan 2024 14:16:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在视觉领域，引导模型在利基任务上工作的当前 SOTA 是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</link>
      <description><![CDATA[过去，当您需要在一些相对小众的分类/检测/分割任务上训练模型时，您会使用在ImageNet1K/COCO 并将其微调到您拥有的任何中小型数据集，这足以将您的性能提升到合理的水平。当然，您始终可以通过使用更大的 Resnet、改进超参数选择或清除专有数据集中的噪声来改进这一点。更新的架构已经发布，更新的优化器，我们现在有像 CLIP 这样的大型 VL 模型，等等。我想知道我是否错过了一个新的共识。 如果你选择回答，我将不胜感激如果您还详细说明了以下标准：  您选择的方法是否对超参数过于敏感？ / 收敛到正确的模型有多难？例如，根据我的经验（当然不是绝对的），在超参数选择方面，ResNet 比 EfficientNet 更宽容。 您的方法对少量数据的敏感程度如何？例如，我记得原来的 Transformer 在小训练集场景中非常糟糕，结果在 IN22K 上报告。 您的选择有多快和/或内存效率如何？小的利基任务往往不会证明具有 1B 参数的模型是合理的。  谢谢！   由   提交/u/anaccountforthemasse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</guid>
      <pubDate>Fri, 19 Jan 2024 08:57:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>