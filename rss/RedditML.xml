<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 11 Mar 2024 15:14:03 GMT</lastBuildDate>
    <item>
      <title>[N] Haystack 2.0发布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc53h0/n_haystack_20_launch/</link>
      <description><![CDATA[Haystack 2.0 稳定版已上线！亲自尝试一下：https://haystack.deepset.ai/blog/haystack-2-release/&lt; /a&gt; Haystack 是一个开源 AI 框架，用于使用 LLM 和其他语言模型创建可用于生产的应用程序。它已经有近 4 年历史了——在它流行之前我们就一直在做 NLP 和 LLM 工程。 😎 并且与模型和数据库无关 - 您可以使用对您的用例最有意义的任何工具。 Haystack 提供了一个丰富且不断发展的集成社区，提供监控、评估、数据摄取等功能。 有关 2.0 版本的背景： Haystack 于 2020 年首次正式发布，当时最前沿的NLP 的主要内容是语义搜索、检索和抽取式问答。 Haystack 2.0 是完全重写的，但将组件组合成灵活管道的基本原理保持不变。  该版本有相当多的模型提供者、跟踪和监控功能以及开箱即用的支持数据库： 对于模型（生成和嵌入）：OpenAI、Mistral、Cohere、Jina AI、 Google AI、Vertex AI、Optimum（通过拥抱脸）、句子转换器、Amazon Bedrock、Azure、Fast Embed、Ollama 对于数据库：Weaviate、Pinecone、Qdrant、Mongo DB、Astra DB、Neo4j、pgvector、Chroma、 Elastic Search、OpenSearch... 希望您尝试一下并让我们知道您的想法！我们有一个快速入门指南：https://haystack.deepset.ai/overview/quick-start   由   提交 /u/tuanacelik   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc53h0/n_haystack_20_launch/</guid>
      <pubDate>Mon, 11 Mar 2024 14:55:24 GMT</pubDate>
    </item>
    <item>
      <title>[P]：利用深度学习生成时事通讯</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc4nyd/p_newsletter_generation_with_deep_learning/</link>
      <description><![CDATA[大家好， 我目前正在开发一个项目，需要构建一个以输入文本为输入的新闻通讯生成器来自论坛的数据（例如 stack-overflow），然后根据问答输入序列生成一份介绍编程事实/技巧的时事通讯。 我做了一些研究，发现了两种有趣的方法：  微调法学硕士（让他们完成我的特定任务） 情境学习（即时工程）  我遇到的问题我面临的第一种方法是监督调整方法的数据。如果我要微调模型，我需要为每一小组问答对，一个时事通讯作为标签，这显然是我无法手动完成的。 （使用一个 LLM 生成标签，然后使用该数据微调另一个 LLM 是否有意义？）  同时，我不知道如何在中应用任何 SSL 方法对于第二种解决方案，我想尽可能避免它，因为我想要一种技术方法，可以训练模型，对参数进行一些更改，尝试不同的方法超参数等.. 你们有什么建议、想法或方法值得尝试这个任务吗？ 非常感谢！   由   提交 /u/ScarlettIce   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc4nyd/p_newsletter_generation_with_deep_learning/</guid>
      <pubDate>Mon, 11 Mar 2024 14:36:15 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 本地运行的文本到语音软件，CPU不好会让我失望吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc4f3u/discussion_text_tò_speech_software_running_local/</link>
      <description><![CDATA[快速提问，对于本地运行的文本到语音软件，糟糕的 CPU 会让我失望吗？ 我有 3060 rtx 64gb RAM 但是双核CPU  我启动模型后会遇到问题吗？ 瓶颈？    由   提交 /u/Visible-Employment43    reddit.com/r/MachineLearning/comments/1bc4f3u/discussion_text_tò_speech_software_running_local/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc4f3u/discussion_text_tò_speech_software_running_local/</guid>
      <pubDate>Mon, 11 Mar 2024 14:25:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1.5 Pro如何回忆10M上下文中的信息？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc1ydg/d_how_does_gemini_15_pro_recall_information_in/</link>
      <description><![CDATA[Google 最近发布了技术报告 关于 Gemini 1.5 Pro 及其 10M 上下文。他们简要概述了提高模型长上下文能力的现代方法（例如循环记忆、环形注意力、新颖的架构等），但我没有找到有关 Gemini 1.5 Pro 方法的信息。 有人注意到官方出版物中有关他们方法的信息吗？这对我来说似乎是 Google 的秘密。   由   提交/u/Mulated-Witness-7196   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc1ydg/d_how_does_gemini_15_pro_recall_information_in/</guid>
      <pubDate>Mon, 11 Mar 2024 12:26:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] [2403.05468] GPT-4 会运行《DOOM》吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc1uyh/r_240305468_will_gpt4_run_doom/</link>
      <description><![CDATA[ 由   提交/u/blabboy  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc1uyh/r_240305468_will_gpt4_run_doom/</guid>
      <pubDate>Mon, 11 Mar 2024 12:20:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] ShortGPT：大型语言模型中的层比您预期的更加冗余</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc1638/r_shortgpt_layers_in_large_language_models_are/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.03853 摘要：  随着大型语言模型（LLM）的性能不断进步，它们的规模显着扩大，当前的法学硕士包含数十亿甚至数万亿的参数。然而，在这项研究中，我们发现LLM的许多层表现出高度相似性，并且某些层在网络功能中发挥的作用可以忽略不计。基于这一观察，我们定义了一个名为区块影响力 (BI) 的指标来衡量法学硕士中每一层的重要性。然后，我们提出了一种直接的修剪方法：层删除，其中我们根据 LLM 的 BI 分数直接删除其中的冗余层。实验表明，我们的方法（我们称之为 ShortGPT）在模型剪枝方面显着优于之前最先进的 (SOTA) 方法。此外，ShortGPT 与类量化方法正交，可以进一步减少参数和计算量。与更复杂的修剪技术相比，通过简单的层删除可以获得更好的结果，这表明模型架构中存在高度冗余。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc1638/r_shortgpt_layers_in_large_language_models_are/</guid>
      <pubDate>Mon, 11 Mar 2024 11:41:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] MathScale：数学推理的缩放指令调整</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bc0zpo/r_mathscale_scaling_instruction_tuning_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.02884 摘要：  大型语言模型（LLM）在解决问题方面表现出了卓越的能力 -解决。然而，他们解决数学问题的能力仍然不足。我们提出了 MathScale，这是一种使用前沿 LLM（例如 GPT-3.5）创建高质量数学推理数据的简单且可扩展的方法。受人类数学学习认知机制的启发，它首先从种子数学问题中提取主题和知识点，然后构建概念图，随后用于生成新的数学问题。 MathScale 沿着我们生成的数学数据集的大小轴展示了有效的可扩展性。因此，我们创建了一个包含 200 万数学问答对的数学推理数据集 (MathScaleQA)。为了全面评价法学硕士的数学推理能力，我们构建了数学应用题基准MwpBench，它是涵盖K-12、大学和竞赛级别的十个数据集（包括GSM8K和MATH）的集合数学问题。我们应用 MathScaleQA 来微调开源 LLM（例如 LLaMA-2 和 Mistral），从而显着提高数学推理能力。在 MwpBench 上进行评估，MathScale-7B 在所有数据集上均实现了最先进的性能，其微观平均准确度和宏观平均准确度分别超过同等大小的最佳同行 42.9% 和 43.7% .    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bc0zpo/r_mathscale_scaling_instruction_tuning_for/</guid>
      <pubDate>Mon, 11 Mar 2024 11:30:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 立场文件：人工智能代理迈向整体智能 - Microsoft 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbz4z8/r_position_paper_agent_ai_towards_a_holistic/</link>
      <description><![CDATA[      论文：https:/ /arxiv.org/abs/2403.00833  摘要：  大型基础模型的最新进展显着增强了我们对开放世界环境中感官信息的理解。在利用基础模型的力量时，人工智能研究必须摆脱过度还原论，转而强调作为有凝聚力的整体的系统。具体来说，我们强调开发人工智能代理——一种将大型基础模型集成到代理操作中的体现系统。代理人工智能的新兴领域涵盖了广泛的现有体现和基于代理的多模式交互，包括机器人、游戏和医疗保健系统等。在本文中，我们提出了一种新颖的大型动作模型实现体现智能行为，代理基础模型。除此之外，我们还讨论了智能体如何在各种领域和任务中展现出卓越的能力，挑战我们对学习和认知的理解。此外，我们从跨学科的角度讨论智能体人工智能的潜力，强调科学话语中的人工智能认知和意识。我们相信这些讨论可以作为未来研究方向的基础，并鼓励更广泛的社会参与。   https:// /preview.redd.it/h8m0ucns7onc1.jpg?width=1228&amp;format=pjpg&amp;auto=webp&amp;s=1cfc94db64f9f358b07353de285faefa5c8ca1a0 https ://preview.redd.it/rjo7pdns7onc1.jpg?width=927&amp;format=pjpg&amp;auto=webp&amp;s=a0728939d5a83c32811c2efbdff9b5a6d58f023f https://preview.redd.it/ng16dfns7onc1.jpg?width=487&amp;format=pjpg&amp;auto=webp ＆amp; ;s=72c6e46c75328cc39e606f149550c0fbf99115a3   由   提交/u/Singularian2501   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbz4z8/r_position_paper_agent_ai_towards_a_holistic/</guid>
      <pubDate>Mon, 11 Mar 2024 09:26:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于成本优化的无服务器 Mistral 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbz4m7/d_serverless_mistral_inference_for_cost/</link>
      <description><![CDATA[我目前是一家初创公司的一员，我正在探索优化 LLM api 使用成本的方法。  我们的服务不需要实时 LLM 交互。相反，我们每 1 小时堆叠 1000 多个类似预测的任务，并在每小时的前 10 分钟内进行处理。考虑到这种使用模式，延迟和保持 GPU 活跃并不是我们主要关心的问题，这让我相信开源 LLM 对我们来说可能是一种经济高效的解决方案。  但是，我对云GPU服务和自己服务LLM的知识相当有限，所以我有一些疑问。   假设使用开源 LLM 可以降低与我的特定用例的 LLM API 使用相关的成本是否准确？ 自动缩放和自动缩放之间有什么区别？无服务器？具体来说，是否可以仅使用单个 GPU 实例进行自动缩放，在大部分时间维护较小的 CPU 实例并仅在必要时分配 GPU？如果可以的话，这种方式是否能有效降低成本？ 如何使用vllm库进行批量推理？它会自动优化单次生成的并发请求吗？ vllm 是否与无服务器计算框架兼容？简单地将通过 Dockerfile 制作的基于 VLLM 的 Docker 镜像部署到像 RunPod 这样的无服务器 GPU 服务上就足够了吗？是否需要具体注意事项或步骤？    由   提交/u/JYPark314  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbz4m7/d_serverless_mistral_inference_for_cost/</guid>
      <pubDate>Mon, 11 Mar 2024 09:25:22 GMT</pubDate>
    </item>
    <item>
      <title>[d] 合成数据生成方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbyfib/d_synthetic_data_generation_methods/</link>
      <description><![CDATA[       我编写了自定义代码，为自己生成合成数据以用于微调LLM， 它提供了“系统、用户、助手”格式的jsonl文件。 openai 微调器接受哪个。  https://preview.redd.it/lw5lkr442onc1.png?width=1273&amp;format=png&amp;auto=webp&amp;s=72661016111ad9e65eb4b019f10890e9ba02efad 但我想知道是否还有其他有效的方法去做吧？你们为法学硕士生成综合数据的方法有哪些？是否遇到任何问题？   由   提交/u/Medium_Alternative50   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbyfib/d_synthetic_data_generation_methods/</guid>
      <pubDate>Mon, 11 Mar 2024 08:35:00 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] [D] NER 中手动标记的替代方案？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbxpsi/discussion_d_any_alternatives_to_manual_labelling/</link>
      <description><![CDATA[问题 -  有 NER 的替代方案吗？ （正则表达式不起作用，因为句子和单词/短语边界没有明确定义）任何无监督/半监督/自监督方法？ 对于标记，是否有手动标记的替代方法？  详细信息： 我有一个关于人物传记的自由文本列，其中包含不同的标识符，例如姓名、身份证号。 、电话号码、电子邮件、出生日期、国籍等。我需要将它们提取到正确的标签下（例如 NAM 代表名称，ID 代表 ID 号等）。每个实体标签可以有多种变体（例如，名称可以出现在“名称：”或“别名：”或“又名：”或“也称为”之后。此外，实体的存在（及其传记中的变体（有些传记只包含姓名、电子邮件和电话号码以及身份证号，很少包含国籍和出生日期）。我正在尝试应用 NER。但是，预训练的 NER 模型不包含我需要的实体，所以我需要用标记数据来训练模型。对于标记，我手动标记了大约 1K 传记 - 相当于 300,000 个标记。如果这些传记的性能不够，将来可能会有更多传记要标记。 .问题是标记是一项超级密集的任务。 我手动标记了 470 个传记，并尝试训练 crf、spacy 的 ner 解决方案和 bert 令牌分类器。对于那些计数的实体，性能较低&lt;1K。我尝试仅选择那些包含要提取的实体的传记进行标记。我尝试过使用CRF模型进行伪标记，但效果不佳。我将无法推送有关 spacy 神童的数据（违反公司政策）   由   提交/u/Ann2_123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbxpsi/discussion_d_any_alternatives_to_manual_labelling/</guid>
      <pubDate>Mon, 11 Mar 2024 07:43:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对比学习的梯度累积（InfoNCE）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbuacq/d_gradient_accumulation_for_contrastive_learning/</link>
      <description><![CDATA[我正在训练多模态对齐模型，但即使使用混合精度训练，我的 GPU 也只能容纳 64 的批量大小。根据 SimCLR 论文，较小的批量大小对于学习而言并不是最佳选择。有什么办法可以在这里实现梯度累积吗？   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbuacq/d_gradient_accumulation_for_contrastive_learning/</guid>
      <pubDate>Mon, 11 Mar 2024 04:03:48 GMT</pubDate>
    </item>
    <item>
      <title>拥有非参数估计背景是否是进入机器学习的有用途径？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</link>
      <description><![CDATA[我是统计学硕士生。我的背景几乎全部基于基础统计理论，我的论文是非参数估计（特别是非参数回归）。基本上，我的“机器学习”知识源于一些经典的非参数估计书籍，例如（Tysbakov、Wasserman、Tibshirani/Hastie 和 Friedman）。统计学习的要素几乎是我在机器学习方面的背景，因为我的论文是关于非参数回归的经典方法之间的交集，例如基于树的方法、核平滑器和样条曲线，用于估计因果推理中的平均治疗效果。 但是，我有时会觉得自己的机器学习背景“相当老”。比如说，我不知道非参数回归背景对于现代机器学习工作有多有吸引力。一般来说，我们深入研究了非参数和统计学中的许多渐近理论，而且我知道对于大多数机器学习工作来说，没有人真正关心渐近保证。  有谁知道我的知识是否真的与当今主要关注神经网络的时代的机器学习工作相关？   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</guid>
      <pubDate>Sun, 10 Mar 2024 20:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] OpenAI：JSON 模式与函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</link>
      <description><![CDATA[       由   提交/u/JClub  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</guid>
      <pubDate>Sun, 10 Mar 2024 18:01:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>