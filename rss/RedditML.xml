<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Tue, 28 May 2024 21:14:08 GMT</lastBuildDate>
    <item>
      <title>[D] 在生产中部署 SetFit 模型的最佳方式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2u14p/d_best_way_to_deploy_setfit_models_in_production/</link>
      <description><![CDATA[正如标题所述，我正在尝试在生产中部署 setfit 模型，并正在寻找一种有效的方法。我尝试使用 huggingface TEI，但不幸的是，它只输出向量，牺牲了分类头。你们有什么建议或我可以尝试的替代方法吗？谢谢！！    提交人    /u/ouzunkumhavuzu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2u14p/d_best_way_to_deploy_setfit_models_in_production/</guid>
      <pubDate>Tue, 28 May 2024 20:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于“你只缓存一次：语言模型的解码器-解码器架构”的问题 - https://arxiv.org/pdf/2405.05254v1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</link>
      <description><![CDATA[      这是我第一次尝试通读一篇论文。但是，我很难理解这一点，我认为你们会知道我的问题的答案，因为如图 1 所示，这种新架构对于 LLM 来说似乎是一件大事。 图 1 据我了解，主要思想是将网络分成两部分。前 L/2 层是自解码器层，可生成全局 KV 缓存。第二个 L/2 层是跨解码器层，重用了生成的全局 KV-Cache。 引用他们论文中关于他们如何节省大量计算和内存的内容（我理解这部分）：  具体来说，因为全局 KV 缓存被重用，而高效的自注意力需要常量缓存，所以缓存的数量为 O(N + CL)，其中 N 是输入长度，C 是常数（例如滑动窗口大小），L 是层数。对于长序列，CL 比 N 小得多，因此大约需要 O(N) 个缓存，即只缓存一次。相比之下，Transformer 解码器在推理过程中必须存储 N × L 个键和值。因此，与 Transformer 解码器相比，YOCO 大约节省了 L 倍的 GPU 缓存内存。  这是我不明白的。在仅解码器网络中，查询、键和值的概念的功能与数据库中的用法有些相似，但侧重于捕获单词之间的关系。在这种网络的每一层中，这些组件有助于细化对文本的理解，在处理从一层移动到下一层时根据新见解调整焦点。 每一层都通过更新查询、键和值来构建前一层，从而细化网络的解释和响应生成。 如果仅解码器网络的各个 KV 缓存的所有信息现在都被压缩到全局 KV 缓存中，我们是否不会丢失有价值的信息并且我们是否应该看到更差的性能？  此外，我们只有一半的层来细化这种解释，因为跨解码器层都重用相同的 KV 缓存。 图 2    提交人    /u/StraightChemistry629   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/</guid>
      <pubDate>Tue, 28 May 2024 17:58:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 带有焦点损失的 XGBoost</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pnan/d_xgboost_with_focal_loss/</link>
      <description><![CDATA[大家好， 有人能帮我实现 XGBoost 的焦点损失或给我一个现有的代码吗？我在网上找到的只有这个，它没有实现同时使用 alpha 和 gamma 的平衡焦点损失（仅实现 gamma）。我还找到了这个，但它似乎有些不对劲，因为与第一个相比，它给出了非常糟糕的结果。 任何帮助都非常欢迎。 谢谢！    提交人    /u/Beginning_Daikon_356   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pnan/d_xgboost_with_focal_loss/</guid>
      <pubDate>Tue, 28 May 2024 17:51:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] GT 深度估计：LiDAR 与立体深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</link>
      <description><![CDATA[为什么大多数深度估计基准（如 nuScenes、KITTI、DDAD 等）都使用来自 LiDAR 传感器的地面真实深度，而不是来自 2 个摄像头的立体深度？ 将摄像头安装在汽车后视镜上会导致基线距离约为 2 米。这将实现更密集的深度测量，距离与 SOTA LiDAR 相似。我不明白为什么不经常使用它 - 还是我错过了什么？    提交人    /u/topsnek69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/</guid>
      <pubDate>Tue, 28 May 2024 17:50:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 会议拒绝</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2phaw/d_neurips_2024_desk_rejection/</link>
      <description><![CDATA[我忘记了检查清单，所以我的提交被拒绝了。老实说，我不知道检查清单，因为我使用了去年提交的乳胶模板，只是将样式文件从 neurips_2023.sty 更改为 neurips_2024.sty。有没有办法在为时已晚之前再次使用检查清单提交？    提交者    /u/Professional-Egg-222   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2phaw/d_neurips_2024_desk_rejection/</guid>
      <pubDate>Tue, 28 May 2024 17:44:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们如何有效地利用强化学习来实现现实世界的应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2p9or/d_how_can_we_leverage_reinforcement_learning/</link>
      <description><![CDATA[强化学习是 AI 的强大工具，在实际应用中非常有效。 如果您想有效地利用 RL，您必须考虑： 选择正确的应用程序、解决 RL 挑战、现实世界的应用领域 此相关播客分享了有关有效利用 RL 的所有内容。 https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Reinforcement-Learning-in-the-Real-World-with-Anna-Goldie-e2hjbj4    由   提交  /u/Data_Nerd1979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2p9or/d_how_can_we_leverage_reinforcement_learning/</guid>
      <pubDate>Tue, 28 May 2024 17:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 防止夏令时期间时间序列预测中的数据泄漏</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2owkv/d_preventing_data_leakage_in_time_series/</link>
      <description><![CDATA[您好 /r/machinelearning， 我正在预测每天中午 12 点发布的值，其中包括第二天所有 24 小时的值。通常，我的方法涉及使用扩展窗口技术，即我使用截至今天（昨天发布）的所有可用数据进行训练，然后预测第二天的 24 小时值。 但是，夏令时调整期间会出现复杂情况。每年两次，数据会因夏令时（欧洲）而发生变化，导致一天有 23 或 25 小时。大多数时间序列库通过预测固定窗口大小来处理回测，但这种固定大小无法适应夏令时期间的小时变化，从而导致潜在的数据泄漏。例如，在春季，模型会偏移一个小时，纳入预测时间后整整一天技术上发布的数据。 我看到了一些潜在的解决方案（在我看来，从最不受欢迎到最受欢迎）：  通过在过渡日添加或删除一个小时来操纵数据。这可能涉及插入一个虚构的值或复制前一个小时。 开发一个自定义回测函数，可以适应不同的时间频率（天、周、月），而不是固定的整数大小窗口。 使用已经解决此问题的库。我似乎找不到已经实现此功能的流行库，所以如果您知道任何库，请告诉我！我特别难以找到适合此功能的 AutoML 库。  您对这些解决方案有何看法？是否有更简单的方法，还是我想太多了？欢迎提出所有建议！    由   提交  /u/NeuralGuesswork   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2owkv/d_preventing_data_leakage_in_time_series/</guid>
      <pubDate>Tue, 28 May 2024 17:21:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 视觉语言建模简介</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2ojhk/r_an_introduction_to_visionlanguage_modeling/</link>
      <description><![CDATA[视觉语言建模简介 摘要： 随着大型语言模型 (LLM) 的流行，人们进行了多次尝试将其扩展到视觉领域。从可以引导我们穿越陌生环境的视觉助手到仅使用高级文本描述生成图像的生成模型，视觉语言模型 (VLM) 应用将显著影响我们与技术的关系。然而，要提高这些模型的可靠性，还需要解决许多挑战。虽然语言是离散的，但视觉是在更高维的空间中演变的，在这个空间中，概念并不总是能轻易离散化。为了更好地理解将视觉映射到语言背后的机制，我们介绍了 VLM，希望这能帮助任何想进入该领域的人。首先，我们介绍什么是 VLM、它们如何工作以及如何训练它们。然后，我们介绍并讨论评估 VLM 的方法。虽然这项工作主要侧重于将图像映射到语言，但我们也讨论了将 VLM 扩展到视频。    提交人    /u/nanowell   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2ojhk/r_an_introduction_to_visionlanguage_modeling/</guid>
      <pubDate>Tue, 28 May 2024 17:05:59 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Tangles：一种新的数学机器学习工具 - 新书发布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2jf2q/research_tangles_a_new_mathematical_ml_tool_book/</link>
      <description><![CDATA[这是我刚刚出版的新书： 缠结：经验科学中人工智能的结构方法 Reinhard Diestel，剑桥大学出版社 2024 电子书，以及包括教程在内的开源软件，可从tangles-book.com获取。 注意：这是一本“外展”书，主要不是关于缠结理论，而是关于以多种意想不到的方式和领域应用缠结。我的《图论》第 5 版介绍了图中的缠结。 目录和数据科学家简介（第 1.2 章）可从 tangles-book.com/book/details/ 和 arXiv:2006.01830 获取。第 6 章和第 14 章介绍了一种基于缠结的软聚类新方法，与传统方法截然不同。第 7-9 章介绍了第 14 章所需的理论。 热烈欢迎在具体项目上进行合作，也欢迎为 GitHub 软件库做出贡献。 出版商的简介： 缠结提供了一种识别不精确数据中结构的精确方法。通过将经常一起出现的特质分组，它们不仅揭示了事物的集群，还揭示了事物特质的类型：政治观点、文本、健康状况或蛋白质的类型。缠结为人工智能提供了一种新的结构化方法，可以帮助我们理解、分类和预测复杂现象。 这可以通过最近对缠结数学理论的公理化来实现，这使得缠结的适用范围远远超出了图论的起源：从数据科学和机器学习中的聚类到预测经济学中的客户行为；从 DNA 测序和药物开发到文本和图像分析。 这是首次探讨此类应用。假设只有基本的本科数学，缠结理论及其潜在影响将向科学家、计算机科学家和社会科学家开放。    提交人    /u/RDiestel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2jf2q/research_tangles_a_new_mathematical_ml_tool_book/</guid>
      <pubDate>Tue, 28 May 2024 13:24:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入矩阵和最终的 pre-softmax 矩阵是否应该在 transformer 中共享？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</link>
      <description><![CDATA[大家好， 在比较各种 LLM 时，我们可以看到，在采用 softmax 获得预测的 token 概率之前，其中一些 LLM 对 token 嵌入和转换矩阵使用相同的矩阵。我发现这篇 2016 年的论文使用输出嵌入改进语言模型表明这种方法更优越，注意力就是您所需要的论文也引用了它并进行了这种权重共享。GPT2 和 Gemma 等其他模型也是如此。 这让我想知道为什么 LLaMa 模型不进行这种权重共享。就模型容量而言，在那里使用单独的矩阵是否值得？像 Gemma 这样的模型是否必须使用权重共享，因为它们使用了庞大的词汇量？我对这里的权衡很感兴趣，如果有的话，目前对这个主题的共识是什么。    提交人    /u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/</guid>
      <pubDate>Tue, 28 May 2024 12:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] H5 到 TFLite 转换中 TransposeConv 的维度很奇怪。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</link>
      <description><![CDATA[      我尝试在 https://medium.com/analytics-vidhya/noise-suppression-using-deep-learning-6ead8c8a1839 上练习该示例，这是一个完整的 Conv1D SEGAN 模型。 然后我完成训练并得到 H5 模型。 然后我尝试转换为具有全整数 INT8 量化的 TFLite 模型。 （原始示例未进行全整数量化，仅设置为“默认”。） 量化代码如下。 def representative_data_gen(): for input_value，_ 在 test_dataset.take(100) 中： yield [input_value] model = load_model(&#39;NS_SEGAN_localTrained.h5&#39;) model.summary() score = model.evaluate(test_dataset) tflite_model = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model.representative_dataset = representative_data_gen tflite_model.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS， tf.lite.OpsSet.SELECT_TF_OPS，# 启用 TensorFlow 操作。 tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # 同时使用选择操作和内置操作 tflite_model.inference_input_type = tf.int8 tflite_model.inference_output_type = tf.int8 tflite_model_quant_INT8 = tflite_model.convert() with open(&#39;NS_SEGAN_localTrained_quant_2.tflite&#39;, &#39;wb&#39;) as f: f.write(tflite_model_quant_INT8) 那么似乎奇怪的是只有第一个“TransposeConv”运算符获得正常维度， 其他运算符的输出维度为 [1,1,1,1]。 第一个 &#39;TransposeConv&#39; 具有正常维度。 其他 6 个 &#39;TransposeConv&#39; 之后得到 [1,1,1,1] TFLite 转换。 模型链接 H5 模型 TFLite (完整 INT8 量化) 我有点怀疑这是否正确，但另一方面，它是由 TFLite API 转换的，这让我认为它应该是正确的。有专家告诉我它不应该是 [1,1,1,1]，但没有解释或建议。 我不知道如何确认这是否正确。在这种情况下 [1,1,1,1] 是否合理？ 此外，如果它是错误的，为什么会发生这种情况以及如何解决？ 如果有人有想法或经验，请提供建议或指导。 非常感谢。    提交人    /u/Ok_Box_6059   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/</guid>
      <pubDate>Tue, 28 May 2024 11:54:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在 pytorch 模型上运行并发推理？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</link>
      <description><![CDATA[大家好， 我有几个用于验证图像的 pytorch 模型，我想将它们部署到端点。我正在使用快速 api 作为 API 包装器，我将介绍迄今为止的开发过程： 之前，我正在运行一个简单的 OOTB 推理，如下所示： model = Model() @app.post(&#39;/model/validate/&#39;): pred = model.forward(img) return {&#39;pred&#39;:pred}  这种方法的问题是它无法处理并发流量，因此请求会排队，并且推理会一次发生 1 个请求，这是我想避免的。 我当前的实现如下：它复制模型对象，并分拆一个新线程来处理特定图像。有点像这样： model = Model() def validate(model, img): pred = model.forward(img) return pred @app.post(&#39;/model/validate/&#39;): model_obj = copy.deepcopy(model) loop = asyncio.get_event_loop() pred = await loop.run_in_executor(validate, model_obj, img) return {&#39;pred&#39; : pred}  这种方法会复制模型对象并对对象副本进行推断，这样我就能够处理并发请求。 我的问题是，有没有另一种更优化的方法可以实现 pytorch 模型并发，或者这是一种有效的做事方式？ TLDR：使用模型对象的副本创建新线程以实现并发，还有其他方法可以实现并发吗？    提交人    /u/comical_cow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 07:33:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 泊松变分自动编码器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</link>
      <description><![CDATA[预印本：https://arxiv.org/abs/2405.14473 X 线程摘要：https://x.com/hadivafaii/status/1794467115510227442    提交人    /u/vafaii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/</guid>
      <pubDate>Tue, 28 May 2024 04:56:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] MusicGPT – 一款使用本地 LLM 生成音乐的开源应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</link>
      <description><![CDATA[大家好！ 想分享我过去几个月一直在做的最新副业。这是一个在本地运行音乐生成模型的终端应用程序，目前，只有 Meta 的 MusicGen 可用。 https://github.com/gabotechs/MusicGPT 它适用于 Windows、Linux 和 MacOS，无需安装 Python 或任何重型机器学习框架。相反，它完全用 Rust 编写，使用 ONNX 运行时以高性能的方式在本地运行 LM，甚至使用 GPU 等硬件加速器。 该应用程序的工作原理如下：  它接受来自用户的自然语言提示 根据提示生成音乐样本 将生成的样本编码为 .wav 格式并在设备上播放  此外，它还附带一个 UI，允许在类似聊天的 Web 应用程序中与 AI 模型进行交互，将聊天历史记录和生成的音乐存储在设备上。 该项目的愿景是它最终可以实时生成无限的音乐流，例如，在编码时可以收听无限的始终是新的 LoFi 歌曲流，但还没有完全实现…… 在没有 PyTorch 或 TensorFlow 的情况下，在 Rust 的受限环境中启动和运行基于 Transformer 的模型是一段有趣的旅程，希望你喜欢它！    由   提交  /u/GabrielMusat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</guid>
      <pubDate>Mon, 27 May 2024 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>